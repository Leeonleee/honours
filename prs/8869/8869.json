{
  "repo": "duckdb/duckdb",
  "pull_number": 8869,
  "instance_id": "duckdb__duckdb-8869",
  "issue_numbers": [
    "7887"
  ],
  "base_commit": "7208022cac983300712db555f39265eed04a655d",
  "patch": "diff --git a/.github/config/uncovered_files.csv b/.github/config/uncovered_files.csv\nindex 5a547f99691a..fabdde4f610b 100644\n--- a/.github/config/uncovered_files.csv\n+++ b/.github/config/uncovered_files.csv\n@@ -94,7 +94,7 @@ common/types/string_heap.cpp\t11\n common/types/time.cpp\t25\n common/types/timestamp.cpp\t19\n common/types/uuid.cpp\t3\n-common/types/validity_mask.cpp\t12\n+common/types/validity_mask.cpp\t13\n common/types/value.cpp\t180\n common/types/vector.cpp\t153\n common/value_operations/comparison_operations.cpp\t32\n@@ -768,7 +768,7 @@ storage/checkpoint/row_group_writer.cpp\t6\n storage/checkpoint/table_data_writer.cpp\t3\n storage/checkpoint/write_overflow_strings_to_disk.cpp\t5\n storage/checkpoint_manager.cpp\t9\n-storage/compression/bitpacking.cpp\t19\n+storage/compression/bitpacking.cpp\t23\n storage/compression/dictionary_compression.cpp\t2\n storage/compression/fsst.cpp\t11\n storage/compression/numeric_constant.cpp\t11\n@@ -806,6 +806,7 @@ storage/table/column_segment.cpp\t62\n storage/table/list_column_data.cpp\t28\n storage/table/row_group.cpp\t54\n storage/table/row_group_collection.cpp\t7\n+storage/table/row_version_manager.cpp\t6\n storage/table/scan_state.cpp\t7\n storage/table/standard_column_data.cpp\t2\n storage/table/struct_column_data.cpp\t24\ndiff --git a/extension/parquet/parquet_extension.cpp b/extension/parquet/parquet_extension.cpp\nindex 1dfcdc8e7744..f2192bf16055 100644\n--- a/extension/parquet/parquet_extension.cpp\n+++ b/extension/parquet/parquet_extension.cpp\n@@ -118,7 +118,7 @@ struct ParquetWriteBindData : public TableFunctionData {\n \tvector<LogicalType> sql_types;\n \tvector<string> column_names;\n \tduckdb_parquet::format::CompressionCodec::type codec = duckdb_parquet::format::CompressionCodec::SNAPPY;\n-\tidx_t row_group_size = RowGroup::ROW_GROUP_SIZE;\n+\tidx_t row_group_size = Storage::ROW_GROUP_SIZE;\n \n \t//! If row_group_size_bytes is not set, we default to row_group_size * BYTES_PER_ROW\n \tstatic constexpr const idx_t BYTES_PER_ROW = 1024;\ndiff --git a/src/common/enum_util.cpp b/src/common/enum_util.cpp\nindex 65f44eff5a79..560c31d3f728 100644\n--- a/src/common/enum_util.cpp\n+++ b/src/common/enum_util.cpp\n@@ -551,6 +551,8 @@ BindingMode EnumUtil::FromString<BindingMode>(const char *value) {\n template<>\n const char* EnumUtil::ToChars<BitpackingMode>(BitpackingMode value) {\n \tswitch(value) {\n+\tcase BitpackingMode::INVALID:\n+\t\treturn \"INVALID\";\n \tcase BitpackingMode::AUTO:\n \t\treturn \"AUTO\";\n \tcase BitpackingMode::CONSTANT:\n@@ -568,6 +570,9 @@ const char* EnumUtil::ToChars<BitpackingMode>(BitpackingMode value) {\n \n template<>\n BitpackingMode EnumUtil::FromString<BitpackingMode>(const char *value) {\n+\tif (StringUtil::Equals(value, \"INVALID\")) {\n+\t\treturn BitpackingMode::INVALID;\n+\t}\n \tif (StringUtil::Equals(value, \"AUTO\")) {\n \t\treturn BitpackingMode::AUTO;\n \t}\ndiff --git a/src/common/file_buffer.cpp b/src/common/file_buffer.cpp\nindex c8263cd544e7..01fa5d77f20a 100644\n--- a/src/common/file_buffer.cpp\n+++ b/src/common/file_buffer.cpp\n@@ -5,7 +5,7 @@\n #include \"duckdb/common/exception.hpp\"\n #include \"duckdb/common/file_system.hpp\"\n #include \"duckdb/common/helper.hpp\"\n-\n+#include \"duckdb/storage/storage_info.hpp\"\n #include <cstring>\n \n namespace duckdb {\ndiff --git a/src/common/types/validity_mask.cpp b/src/common/types/validity_mask.cpp\nindex 59b3bc608d7f..5ec42b77f1df 100644\n--- a/src/common/types/validity_mask.cpp\n+++ b/src/common/types/validity_mask.cpp\n@@ -1,4 +1,7 @@\n #include \"duckdb/common/types/validity_mask.hpp\"\n+#include \"duckdb/common/limits.hpp\"\n+#include \"duckdb/common/serializer/write_stream.hpp\"\n+#include \"duckdb/common/serializer/read_stream.hpp\"\n \n namespace duckdb {\n \n@@ -173,4 +176,57 @@ void ValidityMask::SliceInPlace(const ValidityMask &other, idx_t target_offset,\n #endif\n }\n \n+enum class ValiditySerialization : uint8_t { BITMASK = 0, VALID_VALUES = 1, INVALID_VALUES = 2 };\n+\n+void ValidityMask::Write(WriteStream &writer, idx_t count) {\n+\tauto valid_values = CountValid(count);\n+\tauto invalid_values = count - valid_values;\n+\tauto bitmask_bytes = ValidityMask::ValidityMaskSize(count);\n+\tauto need_u32 = count >= NumericLimits<uint16_t>::Maximum();\n+\tauto bytes_per_value = need_u32 ? sizeof(uint32_t) : sizeof(uint16_t);\n+\tauto valid_value_size = bytes_per_value * valid_values + sizeof(uint32_t);\n+\tauto invalid_value_size = bytes_per_value * invalid_values + sizeof(uint32_t);\n+\tif (valid_value_size < bitmask_bytes || invalid_value_size < bitmask_bytes) {\n+\t\tauto serialize_valid = valid_value_size < invalid_value_size;\n+\t\t// serialize (in)valid value indexes as [COUNT][V0][V1][...][VN]\n+\t\tauto flag = serialize_valid ? ValiditySerialization::VALID_VALUES : ValiditySerialization::INVALID_VALUES;\n+\t\twriter.Write(flag);\n+\t\twriter.Write<uint32_t>(MinValue<uint32_t>(valid_values, invalid_values));\n+\t\tfor (idx_t i = 0; i < count; i++) {\n+\t\t\tif (RowIsValid(i) == serialize_valid) {\n+\t\t\t\tif (need_u32) {\n+\t\t\t\t\twriter.Write<uint32_t>(i);\n+\t\t\t\t} else {\n+\t\t\t\t\twriter.Write<uint16_t>(i);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t} else {\n+\t\t// serialize the entire bitmask\n+\t\twriter.Write(ValiditySerialization::BITMASK);\n+\t\twriter.WriteData(const_data_ptr_cast(GetData()), bitmask_bytes);\n+\t}\n+}\n+\n+void ValidityMask::Read(ReadStream &reader, idx_t count) {\n+\tInitialize(count);\n+\t// deserialize the storage type\n+\tauto flag = reader.Read<ValiditySerialization>();\n+\tif (flag == ValiditySerialization::BITMASK) {\n+\t\t// deserialize the bitmask\n+\t\treader.ReadData(data_ptr_cast(GetData()), ValidityMask::ValidityMaskSize(count));\n+\t\treturn;\n+\t}\n+\tauto is_u32 = count >= NumericLimits<uint16_t>::Maximum();\n+\tauto is_valid = flag == ValiditySerialization::VALID_VALUES;\n+\tauto serialize_count = reader.Read<uint32_t>();\n+\tif (is_valid) {\n+\t\tSetAllInvalid(count);\n+\t}\n+\tfor (idx_t i = 0; i < serialize_count; i++) {\n+\t\tidx_t index = is_u32 ? reader.Read<uint32_t>() : reader.Read<uint16_t>();\n+\t\tSet(index, is_valid);\n+\t}\n+}\n+\n } // namespace duckdb\ndiff --git a/src/execution/operator/persistent/physical_batch_insert.cpp b/src/execution/operator/persistent/physical_batch_insert.cpp\nindex 5ccc6b7bb79a..8a57ece39891 100644\n--- a/src/execution/operator/persistent/physical_batch_insert.cpp\n+++ b/src/execution/operator/persistent/physical_batch_insert.cpp\n@@ -194,7 +194,7 @@ class BatchInsertGlobalState : public GlobalSinkState {\n \t\t}\n \t\tauto new_count = current_collection->GetTotalRows();\n \t\tauto batch_type =\n-\t\t    new_count < RowGroup::ROW_GROUP_SIZE ? RowGroupBatchType::NOT_FLUSHED : RowGroupBatchType::FLUSHED;\n+\t\t    new_count < Storage::ROW_GROUP_SIZE ? RowGroupBatchType::NOT_FLUSHED : RowGroupBatchType::FLUSHED;\n \t\tif (batch_type == RowGroupBatchType::FLUSHED && writer) {\n \t\t\twriter->WriteLastRowGroup(*current_collection);\n \t\t}\ndiff --git a/src/execution/operator/persistent/physical_insert.cpp b/src/execution/operator/persistent/physical_insert.cpp\nindex abb135781132..8e91ae202094 100644\n--- a/src/execution/operator/persistent/physical_insert.cpp\n+++ b/src/execution/operator/persistent/physical_insert.cpp\n@@ -482,7 +482,7 @@ SinkCombineResultType PhysicalInsert::Combine(ExecutionContext &context, Operato\n \n \tlock_guard<mutex> lock(gstate.lock);\n \tgstate.insert_count += append_count;\n-\tif (append_count < RowGroup::ROW_GROUP_SIZE) {\n+\tif (append_count < Storage::ROW_GROUP_SIZE) {\n \t\t// we have few rows - append to the local storage directly\n \t\tauto &table = gstate.table;\n \t\tauto &storage = table.GetStorage();\ndiff --git a/src/include/duckdb/common/constants.hpp b/src/include/duckdb/common/constants.hpp\nindex 6b977d64d40f..03bded7842f4 100644\n--- a/src/include/duckdb/common/constants.hpp\n+++ b/src/include/duckdb/common/constants.hpp\n@@ -58,21 +58,6 @@ struct DConstants {\n \tstatic constexpr const idx_t INVALID_INDEX = idx_t(-1);\n };\n \n-struct Storage {\n-\t//! The size of a hard disk sector, only really needed for Direct IO\n-\tconstexpr static int SECTOR_SIZE = 4096;\n-\t//! Block header size for blocks written to the storage\n-\tconstexpr static int BLOCK_HEADER_SIZE = sizeof(uint64_t);\n-\t// Size of a memory slot managed by the StorageManager. This is the quantum of allocation for Blocks on DuckDB. We\n-\t// default to 256KB. (1 << 18)\n-\tconstexpr static int BLOCK_ALLOC_SIZE = 262144;\n-\t//! The actual memory space that is available within the blocks\n-\tconstexpr static int BLOCK_SIZE = BLOCK_ALLOC_SIZE - BLOCK_HEADER_SIZE;\n-\t//! The size of the headers. This should be small and written more or less atomically by the hard disk. We default\n-\t//! to the page size, which is 4KB. (1 << 12)\n-\tconstexpr static int FILE_HEADER_SIZE = 4096;\n-};\n-\n struct LogicalIndex {\n \texplicit LogicalIndex(idx_t index) : index(index) {\n \t}\ndiff --git a/src/include/duckdb/common/serializer/memory_stream.hpp b/src/include/duckdb/common/serializer/memory_stream.hpp\nindex c4463f46a07b..16ee46f2331f 100644\n--- a/src/include/duckdb/common/serializer/memory_stream.hpp\n+++ b/src/include/duckdb/common/serializer/memory_stream.hpp\n@@ -1,7 +1,7 @@\n //===----------------------------------------------------------------------===//\n //                         DuckDB\n //\n-// duckdb/common/serializer/buffer_stream.hpp\n+// duckdb/common/serializer/memory_stream.hpp\n //\n //\n //===----------------------------------------------------------------------===//\ndiff --git a/src/include/duckdb/common/types/validity_mask.hpp b/src/include/duckdb/common/types/validity_mask.hpp\nindex 8dd91e6759cb..c3fcc25b8c96 100644\n--- a/src/include/duckdb/common/types/validity_mask.hpp\n+++ b/src/include/duckdb/common/types/validity_mask.hpp\n@@ -332,6 +332,9 @@ struct ValidityMask : public TemplatedValidityMask<validity_t> {\n \tDUCKDB_API string ToString(idx_t count) const;\n \n \tDUCKDB_API static bool IsAligned(idx_t count);\n+\n+\tvoid Write(WriteStream &writer, idx_t count);\n+\tvoid Read(ReadStream &reader, idx_t count);\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/storage/block.hpp b/src/include/duckdb/storage/block.hpp\nindex fa63d531fb12..b12bab29fe5a 100644\n--- a/src/include/duckdb/storage/block.hpp\n+++ b/src/include/duckdb/storage/block.hpp\n@@ -52,11 +52,11 @@ struct MetaBlockPointer {\n \tidx_t block_pointer;\n \tuint32_t offset;\n \n-\tbool IsValid() {\n+\tbool IsValid() const {\n \t\treturn block_pointer != DConstants::INVALID_INDEX;\n \t}\n-\tblock_id_t GetBlockId();\n-\tuint32_t GetBlockIndex();\n+\tblock_id_t GetBlockId() const;\n+\tuint32_t GetBlockIndex() const;\n \n \tvoid Serialize(Serializer &serializer) const;\n \tstatic MetaBlockPointer Deserialize(Deserializer &source);\ndiff --git a/src/include/duckdb/storage/compression/bitpacking.hpp b/src/include/duckdb/storage/compression/bitpacking.hpp\nindex 31e7a875d8f2..6b87e5bb14f9 100644\n--- a/src/include/duckdb/storage/compression/bitpacking.hpp\n+++ b/src/include/duckdb/storage/compression/bitpacking.hpp\n@@ -12,14 +12,7 @@\n \n namespace duckdb {\n \n-enum class BitpackingMode : uint8_t {\n-\tAUTO,\n-\n-\tCONSTANT,\n-\tCONSTANT_DELTA,\n-\tDELTA_FOR,\n-\tFOR\n-};\n+enum class BitpackingMode : uint8_t { INVALID, AUTO, CONSTANT, CONSTANT_DELTA, DELTA_FOR, FOR };\n \n BitpackingMode BitpackingModeFromString(const string &str);\n string BitpackingModeToString(const BitpackingMode &mode);\ndiff --git a/src/include/duckdb/storage/data_pointer.hpp b/src/include/duckdb/storage/data_pointer.hpp\nindex 23a8bf3c3941..314f27f015cd 100644\n--- a/src/include/duckdb/storage/data_pointer.hpp\n+++ b/src/include/duckdb/storage/data_pointer.hpp\n@@ -40,8 +40,8 @@ struct RowGroupPointer {\n \tuint64_t tuple_count;\n \t//! The data pointers of the column segments stored in the row group\n \tvector<MetaBlockPointer> data_pointers;\n-\t//! The versions information of the row group (if any)\n-\tshared_ptr<VersionNode> versions;\n+\t//! Data pointers to the delete information of the row group (if any)\n+\tvector<MetaBlockPointer> deletes_pointers;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/storage/metadata/metadata_manager.hpp b/src/include/duckdb/storage/metadata/metadata_manager.hpp\nindex 96300402a3ba..8c58a67289bf 100644\n--- a/src/include/duckdb/storage/metadata/metadata_manager.hpp\n+++ b/src/include/duckdb/storage/metadata/metadata_manager.hpp\n@@ -64,6 +64,7 @@ class MetadataManager {\n \tvoid Flush();\n \n \tvoid MarkBlocksAsModified();\n+\tvoid ClearModifiedBlocks(const vector<MetaBlockPointer> &pointers);\n \n \tidx_t BlockCount();\n \n@@ -82,6 +83,7 @@ class MetadataManager {\n \n \tvoid AddBlock(MetadataBlock new_block, bool if_exists = false);\n \tvoid AddAndRegisterBlock(MetadataBlock block);\n+\tvoid ConvertToTransient(MetadataBlock &block);\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/storage/metadata/metadata_reader.hpp b/src/include/duckdb/storage/metadata/metadata_reader.hpp\nindex 17fddbc69161..1acb60fe222d 100644\n--- a/src/include/duckdb/storage/metadata/metadata_reader.hpp\n+++ b/src/include/duckdb/storage/metadata/metadata_reader.hpp\n@@ -18,6 +18,7 @@ enum class BlockReaderType { EXISTING_BLOCKS, REGISTER_BLOCKS };\n class MetadataReader : public ReadStream {\n public:\n \tMetadataReader(MetadataManager &manager, MetaBlockPointer pointer,\n+\t               optional_ptr<vector<MetaBlockPointer>> read_pointers = nullptr,\n \t               BlockReaderType type = BlockReaderType::EXISTING_BLOCKS);\n \tMetadataReader(MetadataManager &manager, BlockPointer pointer);\n \t~MetadataReader() override;\n@@ -46,6 +47,7 @@ class MetadataReader : public ReadStream {\n \tMetadataHandle block;\n \tMetadataPointer next_pointer;\n \tbool has_next_block;\n+\toptional_ptr<vector<MetaBlockPointer>> read_pointers;\n \tidx_t index;\n \tidx_t offset;\n \tidx_t next_offset;\ndiff --git a/src/include/duckdb/storage/metadata/metadata_writer.hpp b/src/include/duckdb/storage/metadata/metadata_writer.hpp\nindex e245f97913b2..206451b77b51 100644\n--- a/src/include/duckdb/storage/metadata/metadata_writer.hpp\n+++ b/src/include/duckdb/storage/metadata/metadata_writer.hpp\n@@ -15,10 +15,10 @@ namespace duckdb {\n \n class MetadataWriter : public WriteStream {\n public:\n+\texplicit MetadataWriter(MetadataManager &manager,\n+\t                        optional_ptr<vector<MetaBlockPointer>> written_pointers = nullptr);\n \tMetadataWriter(const MetadataWriter &) = delete;\n \tMetadataWriter &operator=(const MetadataWriter &) = delete;\n-\n-\texplicit MetadataWriter(MetadataManager &manager);\n \t~MetadataWriter() override;\n \n public:\n@@ -27,6 +27,9 @@ class MetadataWriter : public WriteStream {\n \n \tBlockPointer GetBlockPointer();\n \tMetaBlockPointer GetMetaBlockPointer();\n+\tMetadataManager &GetManager() {\n+\t\treturn manager;\n+\t}\n \n protected:\n \tvirtual MetadataHandle NextHandle();\n@@ -41,6 +44,7 @@ class MetadataWriter : public WriteStream {\n \tMetadataManager &manager;\n \tMetadataHandle block;\n \tMetadataPointer current_pointer;\n+\toptional_ptr<vector<MetaBlockPointer>> written_pointers;\n \tidx_t capacity;\n \tidx_t offset;\n };\ndiff --git a/src/include/duckdb/storage/storage_info.hpp b/src/include/duckdb/storage/storage_info.hpp\nindex 38b8cce59505..77e02336517c 100644\n--- a/src/include/duckdb/storage/storage_info.hpp\n+++ b/src/include/duckdb/storage/storage_info.hpp\n@@ -23,6 +23,25 @@ struct FileHandle;\n #error Row group size should be cleanly divisible by vector size\n #endif\n \n+struct Storage {\n+\t//! The size of a hard disk sector, only really needed for Direct IO\n+\tconstexpr static int SECTOR_SIZE = 4096;\n+\t//! Block header size for blocks written to the storage\n+\tconstexpr static int BLOCK_HEADER_SIZE = sizeof(uint64_t);\n+\t// Size of a memory slot managed by the StorageManager. This is the quantum of allocation for Blocks on DuckDB. We\n+\t// default to 256KB. (1 << 18)\n+\tconstexpr static int BLOCK_ALLOC_SIZE = 262144;\n+\t//! The actual memory space that is available within the blocks\n+\tconstexpr static int BLOCK_SIZE = BLOCK_ALLOC_SIZE - BLOCK_HEADER_SIZE;\n+\t//! The size of the headers. This should be small and written more or less atomically by the hard disk. We default\n+\t//! to the page size, which is 4KB. (1 << 12)\n+\tconstexpr static int FILE_HEADER_SIZE = 4096;\n+\t//! The number of rows per row group (must be a multiple of the vector size)\n+\tconstexpr static const idx_t ROW_GROUP_SIZE = STANDARD_ROW_GROUPS_SIZE;\n+\t//! The number of vectors per row group\n+\tconstexpr static const idx_t ROW_GROUP_VECTOR_COUNT = ROW_GROUP_SIZE / STANDARD_VECTOR_SIZE;\n+};\n+\n //! The version number of the database storage format\n extern const uint64_t VERSION_NUMBER;\n \ndiff --git a/src/include/duckdb/storage/table/chunk_info.hpp b/src/include/duckdb/storage/table/chunk_info.hpp\nindex 25b669c29d5b..14ed981e3707 100644\n--- a/src/include/duckdb/storage/table/chunk_info.hpp\n+++ b/src/include/duckdb/storage/table/chunk_info.hpp\n@@ -46,8 +46,10 @@ class ChunkInfo {\n \tvirtual void CommitAppend(transaction_t commit_id, idx_t start, idx_t end) = 0;\n \tvirtual idx_t GetCommittedDeletedCount(idx_t max_count) = 0;\n \n-\tvirtual void Serialize(Serializer &serializer) const = 0;\n-\tstatic unique_ptr<ChunkInfo> Deserialize(Deserializer &deserializer);\n+\tvirtual bool HasDeletes() const = 0;\n+\n+\tvirtual void Write(WriteStream &writer) const;\n+\tstatic unique_ptr<ChunkInfo> Read(ReadStream &reader);\n \n public:\n \ttemplate <class TARGET>\n@@ -74,8 +76,8 @@ class ChunkConstantInfo : public ChunkInfo {\n public:\n \texplicit ChunkConstantInfo(idx_t start);\n \n-\tatomic<transaction_t> insert_id;\n-\tatomic<transaction_t> delete_id;\n+\ttransaction_t insert_id;\n+\ttransaction_t delete_id;\n \n public:\n \tidx_t GetSelVector(TransactionData transaction, SelectionVector &sel_vector, idx_t max_count) override;\n@@ -85,8 +87,10 @@ class ChunkConstantInfo : public ChunkInfo {\n \tvoid CommitAppend(transaction_t commit_id, idx_t start, idx_t end) override;\n \tidx_t GetCommittedDeletedCount(idx_t max_count) override;\n \n-\tvoid Serialize(Serializer &serializer) const override;\n-\tstatic unique_ptr<ChunkInfo> Deserialize(Deserializer &deserializer);\n+\tbool HasDeletes() const override;\n+\n+\tvoid Write(WriteStream &writer) const override;\n+\tstatic unique_ptr<ChunkInfo> Read(ReadStream &reader);\n \n private:\n \ttemplate <class OP>\n@@ -102,13 +106,13 @@ class ChunkVectorInfo : public ChunkInfo {\n \texplicit ChunkVectorInfo(idx_t start);\n \n \t//! The transaction ids of the transactions that inserted the tuples (if any)\n-\tatomic<transaction_t> inserted[STANDARD_VECTOR_SIZE];\n-\tatomic<transaction_t> insert_id;\n-\tatomic<bool> same_inserted_id;\n+\ttransaction_t inserted[STANDARD_VECTOR_SIZE];\n+\ttransaction_t insert_id;\n+\tbool same_inserted_id;\n \n \t//! The transaction ids of the transactions that deleted the tuples (if any)\n-\tatomic<transaction_t> deleted[STANDARD_VECTOR_SIZE];\n-\tatomic<bool> any_deleted;\n+\ttransaction_t deleted[STANDARD_VECTOR_SIZE];\n+\tbool any_deleted;\n \n public:\n \tidx_t GetSelVector(transaction_t start_time, transaction_t transaction_id, SelectionVector &sel_vector,\n@@ -130,8 +134,10 @@ class ChunkVectorInfo : public ChunkInfo {\n \tidx_t Delete(transaction_t transaction_id, row_t rows[], idx_t count);\n \tvoid CommitDelete(transaction_t commit_id, row_t rows[], idx_t count);\n \n-\tvoid Serialize(Serializer &serializer) const override;\n-\tstatic unique_ptr<ChunkInfo> Deserialize(Deserializer &deserializer);\n+\tbool HasDeletes() const override;\n+\n+\tvoid Write(WriteStream &writer) const override;\n+\tstatic unique_ptr<ChunkInfo> Read(ReadStream &reader);\n \n private:\n \ttemplate <class OP>\ndiff --git a/src/include/duckdb/storage/table/column_data.hpp b/src/include/duckdb/storage/table/column_data.hpp\nindex 1dd9ef05c4e4..c278b3a54855 100644\n--- a/src/include/duckdb/storage/table/column_data.hpp\n+++ b/src/include/duckdb/storage/table/column_data.hpp\n@@ -151,7 +151,7 @@ class ColumnData {\n \tvoid AppendTransientSegment(SegmentLock &l, idx_t start_row);\n \n \t//! Scans a base vector from the column\n-\tidx_t ScanVector(ColumnScanState &state, Vector &result, idx_t remaining);\n+\tidx_t ScanVector(ColumnScanState &state, Vector &result, idx_t remaining, bool has_updates);\n \t//! Scans a vector from the column merged with any potential updates\n \t//! If ALLOW_UPDATES is set to false, the function will instead throw an exception if any updates are found\n \ttemplate <bool SCAN_COMMITTED, bool ALLOW_UPDATES>\ndiff --git a/src/include/duckdb/storage/table/row_group.hpp b/src/include/duckdb/storage/table/row_group.hpp\nindex 637ce1537486..5b416ba1179e 100644\n--- a/src/include/duckdb/storage/table/row_group.hpp\n+++ b/src/include/duckdb/storage/table/row_group.hpp\n@@ -36,11 +36,12 @@ class Vector;\n struct ColumnCheckpointState;\n struct RowGroupPointer;\n struct TransactionData;\n-struct VersionNode;\n class CollectionScanState;\n class TableFilterSet;\n struct ColumnFetchState;\n struct RowGroupAppendState;\n+class MetadataManager;\n+class RowVersionManager;\n \n struct RowGroupWriteData {\n \tvector<unique_ptr<ColumnCheckpointState>> states;\n@@ -50,11 +51,6 @@ struct RowGroupWriteData {\n class RowGroup : public SegmentBase<RowGroup> {\n public:\n \tfriend class ColumnData;\n-\tfriend class VersionDeleteState;\n-\n-public:\n-\tstatic constexpr const idx_t ROW_GROUP_SIZE = STANDARD_ROW_GROUPS_SIZE;\n-\tstatic constexpr const idx_t ROW_GROUP_VECTOR_COUNT = ROW_GROUP_SIZE / STANDARD_VECTOR_SIZE;\n \n public:\n \tRowGroup(RowGroupCollection &collection, idx_t start, idx_t count);\n@@ -65,7 +61,7 @@ class RowGroup : public SegmentBase<RowGroup> {\n \t//! The RowGroupCollection this row-group is a part of\n \treference<RowGroupCollection> collection;\n \t//! The version info of the row_group (inserted and deleted tuple info)\n-\tshared_ptr<VersionNode> version_info;\n+\tshared_ptr<RowVersionManager> version_info;\n \t//! The column data of the row_group\n \tvector<shared_ptr<ColumnData>> columns;\n \n@@ -145,12 +141,17 @@ class RowGroup : public SegmentBase<RowGroup> {\n \n \tvoid NextVector(CollectionScanState &state);\n \n+\tidx_t DeleteRows(idx_t vector_idx, transaction_t transaction_id, row_t rows[], idx_t count);\n+\tRowVersionManager &GetOrCreateVersionInfo();\n+\n \t// Serialization\n \tstatic void Serialize(RowGroupPointer &pointer, Serializer &serializer);\n \tstatic RowGroupPointer Deserialize(Deserializer &deserializer);\n \n private:\n-\tChunkInfo *GetChunkInfo(idx_t vector_idx);\n+\tshared_ptr<RowVersionManager> &GetVersionInfo();\n+\tshared_ptr<RowVersionManager> &GetOrCreateVersionInfoPtr();\n+\n \tColumnData &GetColumn(storage_t c);\n \tidx_t GetColumnCount() const;\n \tvector<shared_ptr<ColumnData>> &GetColumns();\n@@ -158,18 +159,17 @@ class RowGroup : public SegmentBase<RowGroup> {\n \ttemplate <TableScanType TYPE>\n \tvoid TemplatedScan(TransactionData transaction, CollectionScanState &state, DataChunk &result);\n \n+\tvector<MetaBlockPointer> CheckpointDeletes(MetadataManager &manager);\n+\n+\tbool HasUnloadedDeletes() const;\n+\n private:\n \tmutex row_group_lock;\n \tmutex stats_lock;\n \tvector<MetaBlockPointer> column_pointers;\n \tunique_ptr<atomic<bool>[]> is_loaded;\n-};\n-\n-struct VersionNode {\n-\tunique_ptr<ChunkInfo> info[RowGroup::ROW_GROUP_VECTOR_COUNT];\n-\n-\tvoid SetStart(idx_t start);\n-\tidx_t GetCommittedDeletedCount(idx_t count);\n+\tvector<MetaBlockPointer> deletes_pointers;\n+\tatomic<bool> deletes_is_loaded;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/storage/table/row_version_manager.hpp b/src/include/duckdb/storage/table/row_version_manager.hpp\nnew file mode 100644\nindex 000000000000..0763513b767d\n--- /dev/null\n+++ b/src/include/duckdb/storage/table/row_version_manager.hpp\n@@ -0,0 +1,59 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/storage/table/row_version_manager.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/common/vector_size.hpp\"\n+#include \"duckdb/storage/table/chunk_info.hpp\"\n+#include \"duckdb/storage/storage_info.hpp\"\n+#include \"duckdb/common/mutex.hpp\"\n+\n+namespace duckdb {\n+\n+class MetadataManager;\n+struct MetaBlockPointer;\n+\n+class RowVersionManager {\n+public:\n+\texplicit RowVersionManager(idx_t start);\n+\n+\tidx_t GetStart() {\n+\t\treturn start;\n+\t}\n+\tvoid SetStart(idx_t start);\n+\tidx_t GetCommittedDeletedCount(idx_t count);\n+\n+\tidx_t GetSelVector(TransactionData transaction, idx_t vector_idx, SelectionVector &sel_vector, idx_t max_count);\n+\tidx_t GetCommittedSelVector(transaction_t start_time, transaction_t transaction_id, idx_t vector_idx,\n+\t                            SelectionVector &sel_vector, idx_t max_count);\n+\tbool Fetch(TransactionData transaction, idx_t row);\n+\n+\tvoid AppendVersionInfo(TransactionData transaction, idx_t count, idx_t row_group_start, idx_t row_group_end);\n+\tvoid CommitAppend(transaction_t commit_id, idx_t row_group_start, idx_t count);\n+\tvoid RevertAppend(idx_t start_row);\n+\n+\tidx_t DeleteRows(idx_t vector_idx, transaction_t transaction_id, row_t rows[], idx_t count);\n+\tvoid CommitDelete(idx_t vector_idx, transaction_t commit_id, row_t rows[], idx_t count);\n+\n+\tvector<MetaBlockPointer> Checkpoint(MetadataManager &manager);\n+\tstatic shared_ptr<RowVersionManager> Deserialize(MetaBlockPointer delete_pointer, MetadataManager &manager,\n+\t                                                 idx_t start);\n+\n+private:\n+\tmutex version_lock;\n+\tidx_t start;\n+\tunique_ptr<ChunkInfo> vector_info[Storage::ROW_GROUP_VECTOR_COUNT];\n+\tbool has_changes;\n+\tvector<MetaBlockPointer> storage_pointers;\n+\n+private:\n+\toptional_ptr<ChunkInfo> GetChunkInfo(idx_t vector_idx);\n+\tChunkVectorInfo &GetVectorInfo(idx_t vector_idx);\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/storage/table/update_segment.hpp b/src/include/duckdb/storage/table/update_segment.hpp\nindex c604b4b7cc52..cefa5d099066 100644\n--- a/src/include/duckdb/storage/table/update_segment.hpp\n+++ b/src/include/duckdb/storage/table/update_segment.hpp\n@@ -101,7 +101,7 @@ struct UpdateNodeData {\n };\n \n struct UpdateNode {\n-\tunique_ptr<UpdateNodeData> info[RowGroup::ROW_GROUP_VECTOR_COUNT];\n+\tunique_ptr<UpdateNodeData> info[Storage::ROW_GROUP_VECTOR_COUNT];\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/transaction/commit_state.hpp b/src/include/duckdb/transaction/commit_state.hpp\nindex 58bd6ccbff79..3b005c4dcfbc 100644\n--- a/src/include/duckdb/transaction/commit_state.hpp\n+++ b/src/include/duckdb/transaction/commit_state.hpp\n@@ -23,7 +23,7 @@ struct UpdateInfo;\n \n class CommitState {\n public:\n-\texplicit CommitState(ClientContext &context, transaction_t commit_id, optional_ptr<WriteAheadLog> log = nullptr);\n+\texplicit CommitState(transaction_t commit_id, optional_ptr<WriteAheadLog> log = nullptr);\n \n \toptional_ptr<WriteAheadLog> log;\n \ttransaction_t commit_id;\n@@ -35,9 +35,6 @@ class CommitState {\n \tunique_ptr<DataChunk> delete_chunk;\n \tunique_ptr<DataChunk> update_chunk;\n \n-private:\n-\tClientContext &context;\n-\n public:\n \ttemplate <bool HAS_LOG>\n \tvoid CommitEntry(UndoFlags type, data_ptr_t data);\n@@ -49,8 +46,6 @@ class CommitState {\n \tvoid WriteCatalogEntry(CatalogEntry &entry, data_ptr_t extra_data);\n \tvoid WriteDelete(DeleteInfo &info);\n \tvoid WriteUpdate(UpdateInfo &info);\n-\n-\tvoid AppendRowId(row_t rowid);\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/transaction/delete_info.hpp b/src/include/duckdb/transaction/delete_info.hpp\nindex 4b151a6345db..569d12f19c36 100644\n--- a/src/include/duckdb/transaction/delete_info.hpp\n+++ b/src/include/duckdb/transaction/delete_info.hpp\n@@ -11,12 +11,13 @@\n #include \"duckdb/common/constants.hpp\"\n \n namespace duckdb {\n-class ChunkVectorInfo;\n class DataTable;\n+class RowVersionManager;\n \n struct DeleteInfo {\n \tDataTable *table;\n-\tChunkVectorInfo *vinfo;\n+\tRowVersionManager *version_info;\n+\tidx_t vector_idx;\n \tidx_t count;\n \tidx_t base_row;\n \trow_t rows[1];\ndiff --git a/src/include/duckdb/transaction/duck_transaction.hpp b/src/include/duckdb/transaction/duck_transaction.hpp\nindex d245cc256bbe..8dc116a433ae 100644\n--- a/src/include/duckdb/transaction/duck_transaction.hpp\n+++ b/src/include/duckdb/transaction/duck_transaction.hpp\n@@ -11,12 +11,13 @@\n #include \"duckdb/transaction/transaction.hpp\"\n \n namespace duckdb {\n+class RowVersionManager;\n \n class DuckTransaction : public Transaction {\n public:\n \tDuckTransaction(TransactionManager &manager, ClientContext &context, transaction_t start_time,\n \t                transaction_t transaction_id);\n-\t~DuckTransaction();\n+\t~DuckTransaction() override;\n \n \t//! The start timestamp of this transaction\n \ttransaction_t start_time;\n@@ -49,7 +50,8 @@ class DuckTransaction : public Transaction {\n \n \tbool ChangesMade();\n \n-\tvoid PushDelete(DataTable &table, ChunkVectorInfo *vinfo, row_t rows[], idx_t count, idx_t base_row);\n+\tvoid PushDelete(DataTable &table, RowVersionManager &info, idx_t vector_idx, row_t rows[], idx_t count,\n+\t                idx_t base_row);\n \tvoid PushAppend(DataTable &table, idx_t row_start, idx_t row_count);\n \tUpdateInfo *CreateUpdateInfo(idx_t type_size, idx_t entries);\n \ndiff --git a/src/include/duckdb/transaction/local_storage.hpp b/src/include/duckdb/transaction/local_storage.hpp\nindex 1617c29a4d96..099507b5a727 100644\n--- a/src/include/duckdb/transaction/local_storage.hpp\n+++ b/src/include/duckdb/transaction/local_storage.hpp\n@@ -88,7 +88,7 @@ class LocalTableManager {\n class LocalStorage {\n public:\n \t// Threshold to merge row groups instead of appending\n-\tstatic constexpr const idx_t MERGE_THRESHOLD = RowGroup::ROW_GROUP_SIZE;\n+\tstatic constexpr const idx_t MERGE_THRESHOLD = Storage::ROW_GROUP_SIZE;\n \n public:\n \tstruct CommitState {\ndiff --git a/src/include/duckdb/transaction/undo_buffer.hpp b/src/include/duckdb/transaction/undo_buffer.hpp\nindex 1383229989e9..d0ae709635c1 100644\n--- a/src/include/duckdb/transaction/undo_buffer.hpp\n+++ b/src/include/duckdb/transaction/undo_buffer.hpp\n@@ -48,7 +48,6 @@ class UndoBuffer {\n \tvoid Rollback() noexcept;\n \n private:\n-\tClientContext &context;\n \tArenaAllocator allocator;\n \n private:\ndiff --git a/src/main/settings/settings.cpp b/src/main/settings/settings.cpp\nindex b2d9031ee684..ee42b92ac06a 100644\n--- a/src/main/settings/settings.cpp\n+++ b/src/main/settings/settings.cpp\n@@ -726,17 +726,12 @@ Value ForceCompressionSetting::GetSetting(ClientContext &context) {\n //===--------------------------------------------------------------------===//\n void ForceBitpackingModeSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {\n \tauto mode_str = StringUtil::Lower(input.ToString());\n-\tif (mode_str == \"none\") {\n-\t\tconfig.options.force_bitpacking_mode = BitpackingMode::AUTO;\n-\t} else {\n-\t\tauto mode = BitpackingModeFromString(mode_str);\n-\t\tif (mode == BitpackingMode::AUTO) {\n-\t\t\tthrow ParserException(\n-\t\t\t    \"Unrecognized option for force_bitpacking_mode, expected none, constant, constant_delta, \"\n-\t\t\t    \"delta_for, or for\");\n-\t\t}\n-\t\tconfig.options.force_bitpacking_mode = mode;\n+\tauto mode = BitpackingModeFromString(mode_str);\n+\tif (mode == BitpackingMode::INVALID) {\n+\t\tthrow ParserException(\"Unrecognized option for force_bitpacking_mode, expected none, constant, constant_delta, \"\n+\t\t                      \"delta_for, or for\");\n \t}\n+\tconfig.options.force_bitpacking_mode = mode;\n }\n \n void ForceBitpackingModeSetting::ResetGlobal(DatabaseInstance *db, DBConfig &config) {\ndiff --git a/src/storage/checkpoint/table_data_writer.cpp b/src/storage/checkpoint/table_data_writer.cpp\nindex 11302d17d957..4c7168bea832 100644\n--- a/src/storage/checkpoint/table_data_writer.cpp\n+++ b/src/storage/checkpoint/table_data_writer.cpp\n@@ -66,7 +66,6 @@ void SingleFileTableDataWriter::FinalizeTable(TableStatistics &&global_stats, Da\n \t\trow_group_serializer.End();\n \t}\n \n-\t// TODO: Serialize this:\n \tauto index_pointers = info->indexes.SerializeIndexes(table_data_writer);\n \n \t// Now begin the metadata as a unit\ndiff --git a/src/storage/compression/bitpacking.cpp b/src/storage/compression/bitpacking.cpp\nindex eced1241e405..ffc794326529 100644\n--- a/src/storage/compression/bitpacking.cpp\n+++ b/src/storage/compression/bitpacking.cpp\n@@ -23,8 +23,7 @@ static constexpr const idx_t BITPACKING_METADATA_GROUP_SIZE = STANDARD_VECTOR_SI\n \n BitpackingMode BitpackingModeFromString(const string &str) {\n \tauto mode = StringUtil::Lower(str);\n-\n-\tif (mode == \"auto\") {\n+\tif (mode == \"auto\" || mode == \"none\") {\n \t\treturn BitpackingMode::AUTO;\n \t} else if (mode == \"constant\") {\n \t\treturn BitpackingMode::CONSTANT;\n@@ -35,21 +34,21 @@ BitpackingMode BitpackingModeFromString(const string &str) {\n \t} else if (mode == \"for\") {\n \t\treturn BitpackingMode::FOR;\n \t} else {\n-\t\treturn BitpackingMode::AUTO;\n+\t\treturn BitpackingMode::INVALID;\n \t}\n }\n \n string BitpackingModeToString(const BitpackingMode &mode) {\n \tswitch (mode) {\n-\tcase (BitpackingMode::AUTO):\n+\tcase BitpackingMode::AUTO:\n \t\treturn \"auto\";\n-\tcase (BitpackingMode::CONSTANT):\n+\tcase BitpackingMode::CONSTANT:\n \t\treturn \"constant\";\n-\tcase (BitpackingMode::CONSTANT_DELTA):\n+\tcase BitpackingMode::CONSTANT_DELTA:\n \t\treturn \"constant_delta\";\n-\tcase (BitpackingMode::DELTA_FOR):\n+\tcase BitpackingMode::DELTA_FOR:\n \t\treturn \"delta_for\";\n-\tcase (BitpackingMode::FOR):\n+\tcase BitpackingMode::FOR:\n \t\treturn \"for\";\n \tdefault:\n \t\tthrow NotImplementedException(\"Unknown bitpacking mode: \" + to_string((uint8_t)mode) + \"\\n\");\n@@ -161,7 +160,7 @@ struct BitpackingState {\n \t\t// Don't delta encoding 1 value makes no sense\n \t\tif (compression_buffer_idx < 2) {\n \t\t\treturn;\n-\t\t};\n+\t\t}\n \n \t\t// TODO: handle NULLS here?\n \t\t// Currently we cannot handle nulls because we would need an additional step of patching for this.\n@@ -686,48 +685,57 @@ struct BitpackingScanState : public SegmentScanState {\n \t}\n \n \tvoid Skip(ColumnSegment &segment, idx_t skip_count) {\n-\t\twhile (skip_count > 0) {\n-\t\t\tif (current_group_offset + skip_count < BITPACKING_METADATA_GROUP_SIZE) {\n-\t\t\t\t// Skipping Delta FOR requires a bit of decoding to figure out the new delta\n-\t\t\t\tif (current_group.mode == BitpackingMode::DELTA_FOR) {\n-\t\t\t\t\t// if current_group_offset points into the middle of a\n-\t\t\t\t\t// BitpackingPrimitives::BITPACKING_ALGORITHM_GROUP_SIZE, we need to scan a few\n-\t\t\t\t\t// values before current_group_offset to align with the algorithm groups\n-\t\t\t\t\tidx_t extra_count = current_group_offset % BitpackingPrimitives::BITPACKING_ALGORITHM_GROUP_SIZE;\n-\n-\t\t\t\t\t// Calculate total offset and count to bitunpack\n-\t\t\t\t\tidx_t base_decompress_count = BitpackingPrimitives::RoundUpToAlgorithmGroupSize(skip_count);\n-\t\t\t\t\tidx_t decompress_count = base_decompress_count + extra_count;\n-\t\t\t\t\tidx_t decompress_offset = current_group_offset - extra_count;\n-\t\t\t\t\tbool skip_sign_extension = true;\n-\n-\t\t\t\t\tBitpackingPrimitives::UnPackBuffer<T>(data_ptr_cast(decompression_buffer),\n-\t\t\t\t\t                                      current_group_ptr + decompress_offset, decompress_count,\n-\t\t\t\t\t                                      current_width, skip_sign_extension);\n-\n-\t\t\t\t\tApplyFrameOfReference<T_S>(reinterpret_cast<T_S *>(decompression_buffer + extra_count),\n-\t\t\t\t\t                           current_frame_of_reference, skip_count);\n-\t\t\t\t\tDeltaDecode<T_S>(reinterpret_cast<T_S *>(decompression_buffer + extra_count),\n-\t\t\t\t\t                 static_cast<T_S>(current_delta_offset), skip_count);\n-\t\t\t\t\tcurrent_delta_offset = decompression_buffer[extra_count + skip_count - 1];\n-\n-\t\t\t\t\tcurrent_group_offset += skip_count;\n-\t\t\t\t} else {\n-\t\t\t\t\tcurrent_group_offset += skip_count;\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\t} else {\n-\t\t\t\tauto left_in_this_group = BITPACKING_METADATA_GROUP_SIZE - current_group_offset;\n-\t\t\t\tauto number_of_groups_to_skip = (skip_count - left_in_this_group) / BITPACKING_METADATA_GROUP_SIZE;\n-\n-\t\t\t\tcurrent_group_offset = 0;\n-\t\t\t\tbitpacking_metadata_ptr -= number_of_groups_to_skip * sizeof(bitpacking_metadata_encoded_t);\n+\t\tbool skip_sign_extend = true;\n \n+\t\tidx_t skipped = 0;\n+\t\twhile (skipped < skip_count) {\n+\t\t\t// Exhausted this metadata group, move pointers to next group and load metadata for next group.\n+\t\t\tif (current_group_offset >= BITPACKING_METADATA_GROUP_SIZE) {\n \t\t\t\tLoadNextGroup();\n+\t\t\t}\n \n-\t\t\t\tskip_count -= left_in_this_group;\n-\t\t\t\tskip_count -= number_of_groups_to_skip * BITPACKING_METADATA_GROUP_SIZE;\n+\t\t\tidx_t offset_in_compression_group =\n+\t\t\t    current_group_offset % BitpackingPrimitives::BITPACKING_ALGORITHM_GROUP_SIZE;\n+\n+\t\t\tif (current_group.mode == BitpackingMode::CONSTANT) {\n+\t\t\t\tidx_t remaining = skip_count - skipped;\n+\t\t\t\tidx_t to_skip = MinValue(remaining, BITPACKING_METADATA_GROUP_SIZE - current_group_offset);\n+\t\t\t\tskipped += to_skip;\n+\t\t\t\tcurrent_group_offset += to_skip;\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tif (current_group.mode == BitpackingMode::CONSTANT_DELTA) {\n+\t\t\t\tidx_t remaining = skip_count - skipped;\n+\t\t\t\tidx_t to_skip = MinValue(remaining, BITPACKING_METADATA_GROUP_SIZE - current_group_offset);\n+\t\t\t\tskipped += to_skip;\n+\t\t\t\tcurrent_group_offset += to_skip;\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tD_ASSERT(current_group.mode == BitpackingMode::FOR || current_group.mode == BitpackingMode::DELTA_FOR);\n+\n+\t\t\tidx_t to_skip =\n+\t\t\t    MinValue<idx_t>(skip_count - skipped,\n+\t\t\t                    BitpackingPrimitives::BITPACKING_ALGORITHM_GROUP_SIZE - offset_in_compression_group);\n+\t\t\t// Calculate start of compression algorithm group\n+\t\t\tif (current_group.mode == BitpackingMode::DELTA_FOR) {\n+\t\t\t\tdata_ptr_t current_position_ptr = current_group_ptr + current_group_offset * current_width / 8;\n+\t\t\t\tdata_ptr_t decompression_group_start_pointer =\n+\t\t\t\t    current_position_ptr - offset_in_compression_group * current_width / 8;\n+\n+\t\t\t\tBitpackingPrimitives::UnPackBlock<T>(data_ptr_cast(decompression_buffer),\n+\t\t\t\t                                     decompression_group_start_pointer, current_width,\n+\t\t\t\t                                     skip_sign_extend);\n+\n+\t\t\t\tT *decompression_ptr = decompression_buffer + offset_in_compression_group;\n+\t\t\t\tApplyFrameOfReference<T_S>(reinterpret_cast<T_S *>(decompression_ptr),\n+\t\t\t\t                           static_cast<T_S>(current_frame_of_reference), to_skip);\n+\t\t\t\tDeltaDecode<T_S>(reinterpret_cast<T_S *>(decompression_ptr), static_cast<T_S>(current_delta_offset),\n+\t\t\t\t                 to_skip);\n+\t\t\t\tcurrent_delta_offset = decompression_ptr[to_skip - 1];\n \t\t\t}\n+\n+\t\t\tskipped += to_skip;\n+\t\t\tcurrent_group_offset += to_skip;\n \t\t}\n \t}\n \n@@ -757,7 +765,6 @@ void BitpackingScanPartial(ColumnSegment &segment, ColumnScanState &state, idx_t\n \tbool skip_sign_extend = true;\n \n \tidx_t scanned = 0;\n-\n \twhile (scanned < scan_count) {\n \t\t// Exhausted this metadata group, move pointers to next group and load metadata for next group.\n \t\tif (scan_state.current_group_offset >= BITPACKING_METADATA_GROUP_SIZE) {\ndiff --git a/src/storage/data_table.cpp b/src/storage/data_table.cpp\nindex 7328280e4461..45549f62e161 100644\n--- a/src/storage/data_table.cpp\n+++ b/src/storage/data_table.cpp\n@@ -208,7 +208,7 @@ void DataTable::InitializeScanWithOffset(TableScanState &state, const vector<col\n }\n \n idx_t DataTable::MaxThreads(ClientContext &context) {\n-\tidx_t parallel_scan_vector_count = RowGroup::ROW_GROUP_VECTOR_COUNT;\n+\tidx_t parallel_scan_vector_count = Storage::ROW_GROUP_VECTOR_COUNT;\n \tif (ClientConfig::GetConfig(context).verify_parallelism) {\n \t\tparallel_scan_vector_count = 1;\n \t}\ndiff --git a/src/storage/local_storage.cpp b/src/storage/local_storage.cpp\nindex 7ad0157a0555..d09ae8651931 100644\n--- a/src/storage/local_storage.cpp\n+++ b/src/storage/local_storage.cpp\n@@ -102,7 +102,7 @@ void LocalTableStorage::WriteNewRowGroup() {\n }\n \n void LocalTableStorage::FlushBlocks() {\n-\tif (!merged_storage && row_groups->GetTotalRows() > RowGroup::ROW_GROUP_SIZE) {\n+\tif (!merged_storage && row_groups->GetTotalRows() > Storage::ROW_GROUP_SIZE) {\n \t\toptimistic_writer.WriteLastRowGroup(*row_groups);\n \t}\n \toptimistic_writer.FinalFlush();\ndiff --git a/src/storage/metadata/metadata_manager.cpp b/src/storage/metadata/metadata_manager.cpp\nindex 043aac606c81..f09c48d88a41 100644\n--- a/src/storage/metadata/metadata_manager.cpp\n+++ b/src/storage/metadata/metadata_manager.cpp\n@@ -34,6 +34,12 @@ MetadataHandle MetadataManager::AllocateHandle() {\n \tMetadataPointer pointer;\n \tpointer.block_index = free_block;\n \tauto &block = blocks[free_block];\n+\tif (block.block->BlockId() < MAXIMUM_BLOCK) {\n+\t\t// this block is a disk-backed block, yet we are planning to write to it\n+\t\t// we need to convert it into a transient block before we can write to it\n+\t\tConvertToTransient(block);\n+\t\tD_ASSERT(block.block->BlockId() >= MAXIMUM_BLOCK);\n+\t}\n \tD_ASSERT(!block.free_blocks.empty());\n \tpointer.index = block.free_blocks.back();\n \t// mark the block as used\n@@ -54,6 +60,23 @@ MetadataHandle MetadataManager::Pin(MetadataPointer pointer) {\n \treturn handle;\n }\n \n+void MetadataManager::ConvertToTransient(MetadataBlock &block) {\n+\t// pin the old block\n+\tauto old_buffer = buffer_manager.Pin(block.block);\n+\n+\t// allocate a new transient block to replace it\n+\tshared_ptr<BlockHandle> new_block;\n+\tauto new_buffer = buffer_manager.Allocate(Storage::BLOCK_SIZE, false, &new_block);\n+\n+\t// copy the data to the transient block\n+\tmemcpy(new_buffer.Ptr(), old_buffer.Ptr(), Storage::BLOCK_SIZE);\n+\n+\tblock.block = std::move(new_block);\n+\n+\t// unregister the old block\n+\tblock_manager.UnregisterBlock(block.block_id, false);\n+}\n+\n block_id_t MetadataManager::AllocateNewBlock() {\n \tauto new_block_id = GetNextBlockId();\n \n@@ -91,11 +114,11 @@ MetaBlockPointer MetadataManager::GetDiskPointer(MetadataPointer pointer, uint32\n \treturn MetaBlockPointer(block_pointer, offset);\n }\n \n-block_id_t MetaBlockPointer::GetBlockId() {\n+block_id_t MetaBlockPointer::GetBlockId() const {\n \treturn block_id_t(block_pointer & ~(idx_t(0xFF) << 56ULL));\n }\n \n-uint32_t MetaBlockPointer::GetBlockIndex() {\n+uint32_t MetaBlockPointer::GetBlockIndex() const {\n \treturn block_pointer >> 56ULL;\n }\n \n@@ -262,6 +285,22 @@ void MetadataManager::MarkBlocksAsModified() {\n \t}\n }\n \n+void MetadataManager::ClearModifiedBlocks(const vector<MetaBlockPointer> &pointers) {\n+\tfor (auto &pointer : pointers) {\n+\t\tauto block_id = pointer.GetBlockId();\n+\t\tauto block_index = pointer.GetBlockIndex();\n+\t\tauto entry = modified_blocks.find(block_id);\n+\t\tif (entry == modified_blocks.end()) {\n+\t\t\tthrow InternalException(\"ClearModifiedBlocks - Block id %llu not found in modified_blocks\", block_id);\n+\t\t}\n+\t\tauto &modified_list = entry->second;\n+\t\t// verify the block has been modified\n+\t\tD_ASSERT(modified_list && (1ULL << block_index));\n+\t\t// unset the bit\n+\t\tmodified_list &= ~(1ULL << block_index);\n+\t}\n+}\n+\n block_id_t MetadataManager::GetNextBlockId() {\n \treturn block_manager.GetFreeBlockId();\n }\ndiff --git a/src/storage/metadata/metadata_reader.cpp b/src/storage/metadata/metadata_reader.cpp\nindex a10b1132a838..fbc4dfb44e1d 100644\n--- a/src/storage/metadata/metadata_reader.cpp\n+++ b/src/storage/metadata/metadata_reader.cpp\n@@ -2,9 +2,14 @@\n \n namespace duckdb {\n \n-MetadataReader::MetadataReader(MetadataManager &manager, MetaBlockPointer pointer, BlockReaderType type)\n-    : manager(manager), type(type), next_pointer(FromDiskPointer(pointer)), has_next_block(true), index(0), offset(0),\n-      next_offset(pointer.offset), capacity(0) {\n+MetadataReader::MetadataReader(MetadataManager &manager, MetaBlockPointer pointer,\n+                               optional_ptr<vector<MetaBlockPointer>> read_pointers_p, BlockReaderType type)\n+    : manager(manager), type(type), next_pointer(FromDiskPointer(pointer)), has_next_block(true),\n+      read_pointers(read_pointers_p), index(0), offset(0), next_offset(pointer.offset), capacity(0) {\n+\tif (read_pointers) {\n+\t\tD_ASSERT(read_pointers->empty());\n+\t\tread_pointers->push_back(pointer);\n+\t}\n }\n \n MetadataReader::MetadataReader(MetadataManager &manager, BlockPointer pointer)\n@@ -57,6 +62,10 @@ void MetadataReader::ReadNextBlock() {\n \t\thas_next_block = false;\n \t} else {\n \t\tnext_pointer = FromDiskPointer(MetaBlockPointer(next_block, 0));\n+\t\tMetaBlockPointer next_block_pointer(next_block, 0);\n+\t\tif (read_pointers) {\n+\t\t\tread_pointers->push_back(next_block_pointer);\n+\t\t}\n \t}\n \tif (next_offset < sizeof(block_id_t)) {\n \t\tnext_offset = sizeof(block_id_t);\ndiff --git a/src/storage/metadata/metadata_writer.cpp b/src/storage/metadata/metadata_writer.cpp\nindex 130370e597fa..47bb8d1f16c2 100644\n--- a/src/storage/metadata/metadata_writer.cpp\n+++ b/src/storage/metadata/metadata_writer.cpp\n@@ -3,7 +3,9 @@\n \n namespace duckdb {\n \n-MetadataWriter::MetadataWriter(MetadataManager &manager) : manager(manager), capacity(0), offset(0) {\n+MetadataWriter::MetadataWriter(MetadataManager &manager, optional_ptr<vector<MetaBlockPointer>> written_pointers_p)\n+    : manager(manager), written_pointers(written_pointers_p), capacity(0), offset(0) {\n+\tD_ASSERT(!written_pointers || written_pointers->empty());\n }\n \n MetadataWriter::~MetadataWriter() {\n@@ -38,7 +40,8 @@ void MetadataWriter::NextBlock() {\n \n \t// write the block id of the new block to the start of the current block\n \tif (capacity > 0) {\n-\t\tStore<idx_t>(manager.GetDiskPointer(new_handle.pointer).block_pointer, BasePtr());\n+\t\tauto disk_block = manager.GetDiskPointer(new_handle.pointer);\n+\t\tStore<idx_t>(disk_block.block_pointer, BasePtr());\n \t}\n \t// now update the block id of the block\n \tblock = std::move(new_handle);\n@@ -46,6 +49,9 @@ void MetadataWriter::NextBlock() {\n \toffset = sizeof(idx_t);\n \tcapacity = MetadataManager::METADATA_BLOCK_SIZE;\n \tStore<idx_t>(-1, BasePtr());\n+\tif (written_pointers) {\n+\t\twritten_pointers->push_back(manager.GetDiskPointer(current_pointer));\n+\t}\n }\n \n void MetadataWriter::WriteData(const_data_ptr_t buffer, idx_t write_size) {\ndiff --git a/src/storage/single_file_block_manager.cpp b/src/storage/single_file_block_manager.cpp\nindex 998d7d448a53..e8d0d5f135e0 100644\n--- a/src/storage/single_file_block_manager.cpp\n+++ b/src/storage/single_file_block_manager.cpp\n@@ -240,8 +240,7 @@ void SingleFileBlockManager::LoadFreeList() {\n \t\t// no free list\n \t\treturn;\n \t}\n-\n-\tMetadataReader reader(GetMetadataManager(), free_pointer, BlockReaderType::REGISTER_BLOCKS);\n+\tMetadataReader reader(GetMetadataManager(), free_pointer, nullptr, BlockReaderType::REGISTER_BLOCKS);\n \tauto free_list_count = reader.Read<uint64_t>();\n \tfree_list.clear();\n \tfor (idx_t i = 0; i < free_list_count; i++) {\ndiff --git a/src/storage/storage_info.cpp b/src/storage/storage_info.cpp\nindex 582f138db1ec..d931608b449c 100644\n--- a/src/storage/storage_info.cpp\n+++ b/src/storage/storage_info.cpp\n@@ -2,7 +2,7 @@\n \n namespace duckdb {\n \n-const uint64_t VERSION_NUMBER = 59;\n+const uint64_t VERSION_NUMBER = 60;\n \n struct StorageVersionInfo {\n \tconst char *version_name;\ndiff --git a/src/storage/table/CMakeLists.txt b/src/storage/table/CMakeLists.txt\nindex 714eaa8d8129..449ba1f484e4 100644\n--- a/src/storage/table/CMakeLists.txt\n+++ b/src/storage/table/CMakeLists.txt\n@@ -11,6 +11,7 @@ add_library_unity(\n   persistent_table_data.cpp\n   row_group.cpp\n   row_group_collection.cpp\n+  row_version_manager.cpp\n   scan_state.cpp\n   standard_column_data.cpp\n   struct_column_data.cpp\ndiff --git a/src/storage/table/chunk_info.cpp b/src/storage/table/chunk_info.cpp\nindex fefdfdf58455..fa19b5021070 100644\n--- a/src/storage/table/chunk_info.cpp\n+++ b/src/storage/table/chunk_info.cpp\n@@ -2,6 +2,7 @@\n #include \"duckdb/transaction/transaction.hpp\"\n #include \"duckdb/common/serializer/serializer.hpp\"\n #include \"duckdb/common/serializer/deserializer.hpp\"\n+#include \"duckdb/common/serializer/memory_stream.hpp\"\n \n namespace duckdb {\n \n@@ -29,15 +30,19 @@ static bool UseVersion(TransactionData transaction, transaction_t id) {\n \treturn TransactionVersionOperator::UseInsertedVersion(transaction.start_time, transaction.transaction_id, id);\n }\n \n-unique_ptr<ChunkInfo> ChunkInfo::Deserialize(Deserializer &deserializer) {\n-\tauto type = deserializer.ReadProperty<ChunkInfoType>(100, \"type\");\n+void ChunkInfo::Write(WriteStream &writer) const {\n+\twriter.Write<ChunkInfoType>(type);\n+}\n+\n+unique_ptr<ChunkInfo> ChunkInfo::Read(ReadStream &reader) {\n+\tauto type = reader.Read<ChunkInfoType>();\n \tswitch (type) {\n \tcase ChunkInfoType::EMPTY_INFO:\n \t\treturn nullptr;\n \tcase ChunkInfoType::CONSTANT_INFO:\n-\t\treturn ChunkConstantInfo::Deserialize(deserializer);\n+\t\treturn ChunkConstantInfo::Read(reader);\n \tcase ChunkInfoType::VECTOR_INFO:\n-\t\treturn ChunkVectorInfo::Deserialize(deserializer);\n+\t\treturn ChunkVectorInfo::Read(reader);\n \tdefault:\n \t\tthrow SerializationException(\"Could not deserialize Chunk Info Type: unrecognized type\");\n \t}\n@@ -79,22 +84,23 @@ void ChunkConstantInfo::CommitAppend(transaction_t commit_id, idx_t start, idx_t\n \tinsert_id = commit_id;\n }\n \n+bool ChunkConstantInfo::HasDeletes() const {\n+\tbool is_deleted = insert_id >= TRANSACTION_ID_START || delete_id < TRANSACTION_ID_START;\n+\treturn is_deleted;\n+}\n+\n idx_t ChunkConstantInfo::GetCommittedDeletedCount(idx_t max_count) {\n \treturn delete_id < TRANSACTION_ID_START ? max_count : 0;\n }\n \n-void ChunkConstantInfo::Serialize(Serializer &serializer) const {\n-\tbool is_deleted = insert_id >= TRANSACTION_ID_START || delete_id < TRANSACTION_ID_START;\n-\tif (!is_deleted) {\n-\t\tserializer.WriteProperty(100, \"type\", ChunkInfoType::EMPTY_INFO);\n-\t\treturn;\n-\t}\n-\tserializer.WriteProperty(100, \"type\", type);\n-\tserializer.WriteProperty(200, \"start\", start);\n+void ChunkConstantInfo::Write(WriteStream &writer) const {\n+\tD_ASSERT(HasDeletes());\n+\tChunkInfo::Write(writer);\n+\twriter.Write<idx_t>(start);\n }\n \n-unique_ptr<ChunkInfo> ChunkConstantInfo::Deserialize(Deserializer &deserializer) {\n-\tauto start = deserializer.ReadProperty<idx_t>(200, \"start\");\n+unique_ptr<ChunkInfo> ChunkConstantInfo::Read(ReadStream &reader) {\n+\tauto start = reader.Read<idx_t>();\n \tauto info = make_uniq<ChunkConstantInfo>(start);\n \tinfo->insert_id = 0;\n \tinfo->delete_id = 0;\n@@ -218,6 +224,10 @@ void ChunkVectorInfo::CommitAppend(transaction_t commit_id, idx_t start, idx_t e\n \t}\n }\n \n+bool ChunkVectorInfo::HasDeletes() const {\n+\treturn any_deleted;\n+}\n+\n idx_t ChunkVectorInfo::GetCommittedDeletedCount(idx_t max_count) {\n \tif (!any_deleted) {\n \t\treturn 0;\n@@ -231,45 +241,41 @@ idx_t ChunkVectorInfo::GetCommittedDeletedCount(idx_t max_count) {\n \treturn delete_count;\n }\n \n-void ChunkVectorInfo::Serialize(Serializer &serializer) const {\n+void ChunkVectorInfo::Write(WriteStream &writer) const {\n \tSelectionVector sel(STANDARD_VECTOR_SIZE);\n \ttransaction_t start_time = TRANSACTION_ID_START - 1;\n \ttransaction_t transaction_id = DConstants::INVALID_INDEX;\n \tidx_t count = GetSelVector(start_time, transaction_id, sel, STANDARD_VECTOR_SIZE);\n \tif (count == STANDARD_VECTOR_SIZE) {\n \t\t// nothing is deleted: skip writing anything\n-\t\tserializer.WriteProperty(100, \"type\", ChunkInfoType::EMPTY_INFO);\n+\t\twriter.Write<ChunkInfoType>(ChunkInfoType::EMPTY_INFO);\n \t\treturn;\n \t}\n \tif (count == 0) {\n \t\t// everything is deleted: write a constant vector\n-\t\tserializer.WriteProperty(100, \"type\", ChunkInfoType::CONSTANT_INFO);\n-\t\tserializer.WriteProperty(200, \"start\", start);\n+\t\twriter.Write<ChunkInfoType>(ChunkInfoType::CONSTANT_INFO);\n+\t\twriter.Write<idx_t>(start);\n \t\treturn;\n \t}\n \t// write a boolean vector\n-\tserializer.WriteProperty(100, \"type\", ChunkInfoType::VECTOR_INFO);\n-\tserializer.WriteProperty(200, \"start\", start);\n-\tbool deleted_tuples[STANDARD_VECTOR_SIZE];\n-\tfor (idx_t i = 0; i < STANDARD_VECTOR_SIZE; i++) {\n-\t\tdeleted_tuples[i] = true;\n-\t}\n+\tChunkInfo::Write(writer);\n+\twriter.Write<idx_t>(start);\n+\tValidityMask mask(STANDARD_VECTOR_SIZE);\n+\tmask.Initialize(STANDARD_VECTOR_SIZE);\n \tfor (idx_t i = 0; i < count; i++) {\n-\t\tdeleted_tuples[sel.get_index(i)] = false;\n+\t\tmask.SetInvalid(sel.get_index(i));\n \t}\n-\tserializer.WriteProperty(201, \"deleted_tuples\", data_ptr_cast(deleted_tuples), sizeof(bool) * STANDARD_VECTOR_SIZE);\n+\tmask.Write(writer, STANDARD_VECTOR_SIZE);\n }\n \n-unique_ptr<ChunkInfo> ChunkVectorInfo::Deserialize(Deserializer &deserializer) {\n-\tauto start = deserializer.ReadProperty<idx_t>(200, \"start\");\n-\n+unique_ptr<ChunkInfo> ChunkVectorInfo::Read(ReadStream &reader) {\n+\tauto start = reader.Read<idx_t>();\n \tauto result = make_uniq<ChunkVectorInfo>(start);\n \tresult->any_deleted = true;\n-\tbool deleted_tuples[STANDARD_VECTOR_SIZE];\n-\tdeserializer.ReadProperty(201, \"deleted_tuples\", data_ptr_cast(deleted_tuples),\n-\t                          sizeof(bool) * STANDARD_VECTOR_SIZE);\n+\tValidityMask mask;\n+\tmask.Read(reader, STANDARD_VECTOR_SIZE);\n \tfor (idx_t i = 0; i < STANDARD_VECTOR_SIZE; i++) {\n-\t\tif (deleted_tuples[i]) {\n+\t\tif (mask.RowIsValid(i)) {\n \t\t\tresult->deleted[i] = 0;\n \t\t}\n \t}\ndiff --git a/src/storage/table/column_data.cpp b/src/storage/table/column_data.cpp\nindex dd3c53ac2076..773d0b1da73e 100644\n--- a/src/storage/table/column_data.cpp\n+++ b/src/storage/table/column_data.cpp\n@@ -87,7 +87,7 @@ void ColumnData::InitializeScanWithOffset(ColumnScanState &state, idx_t row_idx)\n \tstate.last_offset = 0;\n }\n \n-idx_t ColumnData::ScanVector(ColumnScanState &state, Vector &result, idx_t remaining) {\n+idx_t ColumnData::ScanVector(ColumnScanState &state, Vector &result, idx_t remaining, bool has_updates) {\n \tstate.previous_states.clear();\n \tif (state.version != version) {\n \t\tInitializeScanWithOffset(state, state.row_index);\n@@ -113,7 +113,8 @@ idx_t ColumnData::ScanVector(ColumnScanState &state, Vector &result, idx_t remai\n \t\tidx_t scan_count = MinValue<idx_t>(remaining, state.current->start + state.current->count - state.row_index);\n \t\tidx_t result_offset = initial_remaining - remaining;\n \t\tif (scan_count > 0) {\n-\t\t\tstate.current->Scan(state, scan_count, result, result_offset, scan_count == initial_remaining);\n+\t\t\tstate.current->Scan(state, scan_count, result, result_offset,\n+\t\t\t                    !has_updates && scan_count == initial_remaining);\n \n \t\t\tstate.row_index += scan_count;\n \t\t\tremaining -= scan_count;\n@@ -138,10 +139,14 @@ idx_t ColumnData::ScanVector(ColumnScanState &state, Vector &result, idx_t remai\n \n template <bool SCAN_COMMITTED, bool ALLOW_UPDATES>\n idx_t ColumnData::ScanVector(TransactionData transaction, idx_t vector_index, ColumnScanState &state, Vector &result) {\n-\tauto scan_count = ScanVector(state, result, STANDARD_VECTOR_SIZE);\n-\n-\tlock_guard<mutex> update_guard(update_lock);\n-\tif (updates) {\n+\tbool has_updates;\n+\t{\n+\t\tlock_guard<mutex> update_guard(update_lock);\n+\t\thas_updates = updates ? true : false;\n+\t}\n+\tauto scan_count = ScanVector(state, result, STANDARD_VECTOR_SIZE, has_updates);\n+\tif (has_updates) {\n+\t\tlock_guard<mutex> update_guard(update_lock);\n \t\tif (!ALLOW_UPDATES && updates->HasUncommittedUpdates(vector_index)) {\n \t\t\tthrow TransactionException(\"Cannot create index with outstanding updates\");\n \t\t}\n@@ -179,7 +184,7 @@ idx_t ColumnData::ScanCommitted(idx_t vector_index, ColumnScanState &state, Vect\n void ColumnData::ScanCommittedRange(idx_t row_group_start, idx_t offset_in_row_group, idx_t count, Vector &result) {\n \tColumnScanState child_state;\n \tInitializeScanWithOffset(child_state, row_group_start + offset_in_row_group);\n-\tauto scan_count = ScanVector(child_state, result, count);\n+\tauto scan_count = ScanVector(child_state, result, count, updates ? true : false);\n \tif (updates) {\n \t\tresult.Flatten(scan_count);\n \t\tupdates->FetchCommittedRange(offset_in_row_group, count, result);\n@@ -192,7 +197,7 @@ idx_t ColumnData::ScanCount(ColumnScanState &state, Vector &result, idx_t count)\n \t}\n \t// ScanCount can only be used if there are no updates\n \tD_ASSERT(!updates);\n-\treturn ScanVector(state, result, count);\n+\treturn ScanVector(state, result, count, false);\n }\n \n void ColumnData::Select(TransactionData transaction, idx_t vector_index, ColumnScanState &state, Vector &result,\n@@ -339,7 +344,7 @@ idx_t ColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &result) {\n \tstate.row_index = start + ((row_id - start) / STANDARD_VECTOR_SIZE * STANDARD_VECTOR_SIZE);\n \tstate.current = data.GetSegment(state.row_index);\n \tstate.internal_index = state.current->start;\n-\treturn ScanVector(state, result, STANDARD_VECTOR_SIZE);\n+\treturn ScanVector(state, result, STANDARD_VECTOR_SIZE, false);\n }\n \n void ColumnData::FetchRow(TransactionData transaction, ColumnFetchState &state, row_t row_id, Vector &result,\ndiff --git a/src/storage/table/list_column_data.cpp b/src/storage/table/list_column_data.cpp\nindex c455c3792851..36f6da70c504 100644\n--- a/src/storage/table/list_column_data.cpp\n+++ b/src/storage/table/list_column_data.cpp\n@@ -86,7 +86,7 @@ idx_t ListColumnData::ScanCount(ColumnScanState &state, Vector &result, idx_t co\n \tD_ASSERT(!updates);\n \n \tVector offset_vector(LogicalType::UBIGINT, count);\n-\tidx_t scan_count = ScanVector(state, offset_vector, count);\n+\tidx_t scan_count = ScanVector(state, offset_vector, count, false);\n \tD_ASSERT(scan_count > 0);\n \tvalidity.ScanCount(state.child_states[0], result, count);\n \n@@ -132,7 +132,7 @@ void ListColumnData::Skip(ColumnScanState &state, idx_t count) {\n \t// note that we only need to read the first and last entry\n \t// however, let's just read all \"count\" entries for now\n \tVector result(LogicalType::UBIGINT, count);\n-\tidx_t scan_count = ScanVector(state, result, count);\n+\tidx_t scan_count = ScanVector(state, result, count, false);\n \tif (scan_count == 0) {\n \t\treturn;\n \t}\ndiff --git a/src/storage/table/row_group.cpp b/src/storage/table/row_group.cpp\nindex 95f789b8ba71..7ebd8d0105b6 100644\n--- a/src/storage/table/row_group.cpp\n+++ b/src/storage/table/row_group.cpp\n@@ -16,15 +16,13 @@\n #include \"duckdb/transaction/duck_transaction.hpp\"\n #include \"duckdb/storage/table/append_state.hpp\"\n #include \"duckdb/storage/table/scan_state.hpp\"\n+#include \"duckdb/storage/table/row_version_manager.hpp\"\n #include \"duckdb/common/serializer/serializer.hpp\"\n #include \"duckdb/common/serializer/deserializer.hpp\"\n #include \"duckdb/common/serializer/binary_serializer.hpp\"\n \n namespace duckdb {\n \n-constexpr const idx_t RowGroup::ROW_GROUP_VECTOR_COUNT;\n-constexpr const idx_t RowGroup::ROW_GROUP_SIZE;\n-\n RowGroup::RowGroup(RowGroupCollection &collection, idx_t start, idx_t count)\n     : SegmentBase<RowGroup>(start, count), collection(collection) {\n \tVerify();\n@@ -42,7 +40,8 @@ RowGroup::RowGroup(RowGroupCollection &collection, RowGroupPointer &&pointer)\n \tfor (idx_t c = 0; c < columns.size(); c++) {\n \t\tthis->is_loaded[c] = false;\n \t}\n-\tthis->version_info = std::move(pointer.versions);\n+\tthis->deletes_pointers = std::move(pointer.deletes_pointers);\n+\tthis->deletes_is_loaded = false;\n \n \tVerify();\n }\n@@ -53,34 +52,12 @@ void RowGroup::MoveToCollection(RowGroupCollection &collection, idx_t new_start)\n \tfor (auto &column : GetColumns()) {\n \t\tcolumn->SetStart(new_start);\n \t}\n-\tif (version_info) {\n-\t\tversion_info->SetStart(new_start);\n-\t}\n-}\n-\n-void VersionNode::SetStart(idx_t start) {\n-\tidx_t current_start = start;\n-\tfor (idx_t i = 0; i < RowGroup::ROW_GROUP_VECTOR_COUNT; i++) {\n-\t\tif (info[i]) {\n-\t\t\tinfo[i]->start = current_start;\n-\t\t}\n-\t\tcurrent_start += STANDARD_VECTOR_SIZE;\n-\t}\n-}\n-\n-idx_t VersionNode::GetCommittedDeletedCount(idx_t count) {\n-\tidx_t deleted_count = 0;\n-\tfor (idx_t r = 0, i = 0; r < count; r += STANDARD_VECTOR_SIZE, i++) {\n-\t\tif (!info[i]) {\n-\t\t\tcontinue;\n-\t\t}\n-\t\tidx_t max_count = MinValue<idx_t>(STANDARD_VECTOR_SIZE, count - r);\n-\t\tif (max_count == 0) {\n-\t\t\tbreak;\n+\tif (!HasUnloadedDeletes()) {\n+\t\tauto &vinfo = GetVersionInfo();\n+\t\tif (vinfo) {\n+\t\t\tvinfo->SetStart(new_start);\n \t\t}\n-\t\tdeleted_count += info[i]->GetCommittedDeletedCount(max_count);\n \t}\n-\treturn deleted_count;\n }\n \n RowGroup::~RowGroup() {\n@@ -124,6 +101,11 @@ ColumnData &RowGroup::GetColumn(storage_t c) {\n \tthis->columns[c] =\n \t    ColumnData::Deserialize(GetBlockManager(), GetTableInfo(), c, start, column_data_reader, types[c], nullptr);\n \tis_loaded[c] = true;\n+\tif (this->columns[c]->count != this->count) {\n+\t\tthrow InternalException(\"Corrupted database - loaded column with index %llu at row start %llu, count %llu did \"\n+\t\t                        \"not match count of row group %llu\",\n+\t\t                        c, start, this->columns[c]->count, this->count.load());\n+\t}\n \treturn *columns[c];\n }\n \n@@ -265,7 +247,7 @@ unique_ptr<RowGroup> RowGroup::AlterType(RowGroupCollection &new_collection, con\n \n \t// set up the row_group based on this row_group\n \tauto row_group = make_uniq<RowGroup>(new_collection, this->start, this->count);\n-\trow_group->version_info = version_info;\n+\trow_group->version_info = GetOrCreateVersionInfoPtr();\n \tauto &cols = GetColumns();\n \tfor (idx_t i = 0; i < cols.size(); i++) {\n \t\tif (i == changed_idx) {\n@@ -304,7 +286,7 @@ unique_ptr<RowGroup> RowGroup::AddColumn(RowGroupCollection &new_collection, Col\n \n \t// set up the row_group based on this row_group\n \tauto row_group = make_uniq<RowGroup>(new_collection, this->start, this->count);\n-\trow_group->version_info = version_info;\n+\trow_group->version_info = GetOrCreateVersionInfoPtr();\n \trow_group->columns = GetColumns();\n \t// now add the new column\n \trow_group->columns.push_back(std::move(added_column));\n@@ -319,7 +301,7 @@ unique_ptr<RowGroup> RowGroup::RemoveColumn(RowGroupCollection &new_collection,\n \tD_ASSERT(removed_column < columns.size());\n \n \tauto row_group = make_uniq<RowGroup>(new_collection, this->start, this->count);\n-\trow_group->version_info = version_info;\n+\trow_group->version_info = GetOrCreateVersionInfoPtr();\n \t// copy over all columns except for the removed one\n \tauto &cols = GetColumns();\n \tfor (idx_t i = 0; i < cols.size(); i++) {\n@@ -566,45 +548,62 @@ void RowGroup::ScanCommitted(CollectionScanState &state, DataChunk &result, Tabl\n \t}\n }\n \n-ChunkInfo *RowGroup::GetChunkInfo(idx_t vector_idx) {\n-\tif (!version_info) {\n-\t\treturn nullptr;\n+shared_ptr<RowVersionManager> &RowGroup::GetVersionInfo() {\n+\tif (!HasUnloadedDeletes()) {\n+\t\t// deletes are loaded - return the version info\n+\t\treturn version_info;\n+\t}\n+\tlock_guard<mutex> lock(row_group_lock);\n+\t// double-check after obtaining the lock whether or not deletes are still not loaded to avoid double load\n+\tif (HasUnloadedDeletes()) {\n+\t\t// deletes are not loaded - reload\n+\t\tauto root_delete = deletes_pointers[0];\n+\t\tversion_info = RowVersionManager::Deserialize(root_delete, GetBlockManager().GetMetadataManager(), start);\n+\t\tdeletes_is_loaded = true;\n+\t}\n+\treturn version_info;\n+}\n+\n+shared_ptr<RowVersionManager> &RowGroup::GetOrCreateVersionInfoPtr() {\n+\tauto vinfo = GetVersionInfo();\n+\tif (!vinfo) {\n+\t\tlock_guard<mutex> lock(row_group_lock);\n+\t\tif (!version_info) {\n+\t\t\tversion_info = make_shared<RowVersionManager>(start);\n+\t\t}\n \t}\n-\treturn version_info->info[vector_idx].get();\n+\treturn version_info;\n+}\n+\n+RowVersionManager &RowGroup::GetOrCreateVersionInfo() {\n+\treturn *GetOrCreateVersionInfoPtr();\n }\n \n idx_t RowGroup::GetSelVector(TransactionData transaction, idx_t vector_idx, SelectionVector &sel_vector,\n                              idx_t max_count) {\n-\tlock_guard<mutex> lock(row_group_lock);\n-\n-\tauto info = GetChunkInfo(vector_idx);\n-\tif (!info) {\n+\tauto &vinfo = GetVersionInfo();\n+\tif (!vinfo) {\n \t\treturn max_count;\n \t}\n-\treturn info->GetSelVector(transaction, sel_vector, max_count);\n+\treturn vinfo->GetSelVector(transaction, vector_idx, sel_vector, max_count);\n }\n \n idx_t RowGroup::GetCommittedSelVector(transaction_t start_time, transaction_t transaction_id, idx_t vector_idx,\n                                       SelectionVector &sel_vector, idx_t max_count) {\n-\tlock_guard<mutex> lock(row_group_lock);\n-\n-\tauto info = GetChunkInfo(vector_idx);\n-\tif (!info) {\n+\tauto &vinfo = GetVersionInfo();\n+\tif (!vinfo) {\n \t\treturn max_count;\n \t}\n-\treturn info->GetCommittedSelVector(start_time, transaction_id, sel_vector, max_count);\n+\treturn vinfo->GetCommittedSelVector(start_time, transaction_id, vector_idx, sel_vector, max_count);\n }\n \n bool RowGroup::Fetch(TransactionData transaction, idx_t row) {\n \tD_ASSERT(row < this->count);\n-\tlock_guard<mutex> lock(row_group_lock);\n-\n-\tidx_t vector_index = row / STANDARD_VECTOR_SIZE;\n-\tauto info = GetChunkInfo(vector_index);\n-\tif (!info) {\n+\tauto &vinfo = GetVersionInfo();\n+\tif (!vinfo) {\n \t\treturn true;\n \t}\n-\treturn info->Fetch(transaction, row - vector_index * STANDARD_VECTOR_SIZE);\n+\treturn vinfo->Fetch(transaction, row);\n }\n \n void RowGroup::FetchRow(TransactionData transaction, ColumnFetchState &state, const vector<column_t> &column_ids,\n@@ -628,72 +627,23 @@ void RowGroup::FetchRow(TransactionData transaction, ColumnFetchState &state, co\n void RowGroup::AppendVersionInfo(TransactionData transaction, idx_t count) {\n \tidx_t row_group_start = this->count.load();\n \tidx_t row_group_end = row_group_start + count;\n-\tif (row_group_end > RowGroup::ROW_GROUP_SIZE) {\n-\t\trow_group_end = RowGroup::ROW_GROUP_SIZE;\n+\tif (row_group_end > Storage::ROW_GROUP_SIZE) {\n+\t\trow_group_end = Storage::ROW_GROUP_SIZE;\n \t}\n-\tlock_guard<mutex> lock(row_group_lock);\n-\n \t// create the version_info if it doesn't exist yet\n-\tif (!version_info) {\n-\t\tversion_info = make_shared<VersionNode>();\n-\t}\n-\tidx_t start_vector_idx = row_group_start / STANDARD_VECTOR_SIZE;\n-\tidx_t end_vector_idx = (row_group_end - 1) / STANDARD_VECTOR_SIZE;\n-\tfor (idx_t vector_idx = start_vector_idx; vector_idx <= end_vector_idx; vector_idx++) {\n-\t\tidx_t start = vector_idx == start_vector_idx ? row_group_start - start_vector_idx * STANDARD_VECTOR_SIZE : 0;\n-\t\tidx_t end =\n-\t\t    vector_idx == end_vector_idx ? row_group_end - end_vector_idx * STANDARD_VECTOR_SIZE : STANDARD_VECTOR_SIZE;\n-\t\tif (start == 0 && end == STANDARD_VECTOR_SIZE) {\n-\t\t\t// entire vector is encapsulated by append: append a single constant\n-\t\t\tauto constant_info = make_uniq<ChunkConstantInfo>(this->start + vector_idx * STANDARD_VECTOR_SIZE);\n-\t\t\tconstant_info->insert_id = transaction.transaction_id;\n-\t\t\tconstant_info->delete_id = NOT_DELETED_ID;\n-\t\t\tversion_info->info[vector_idx] = std::move(constant_info);\n-\t\t} else {\n-\t\t\t// part of a vector is encapsulated: append to that part\n-\t\t\tChunkVectorInfo *info;\n-\t\t\tif (!version_info->info[vector_idx]) {\n-\t\t\t\t// first time appending to this vector: create new info\n-\t\t\t\tauto insert_info = make_uniq<ChunkVectorInfo>(this->start + vector_idx * STANDARD_VECTOR_SIZE);\n-\t\t\t\tinfo = insert_info.get();\n-\t\t\t\tversion_info->info[vector_idx] = std::move(insert_info);\n-\t\t\t} else {\n-\t\t\t\tD_ASSERT(version_info->info[vector_idx]->type == ChunkInfoType::VECTOR_INFO);\n-\t\t\t\t// use existing vector\n-\t\t\t\tinfo = &version_info->info[vector_idx]->Cast<ChunkVectorInfo>();\n-\t\t\t}\n-\t\t\tinfo->Append(start, end, transaction.transaction_id);\n-\t\t}\n-\t}\n+\tauto &vinfo = GetOrCreateVersionInfo();\n+\tvinfo.AppendVersionInfo(transaction, count, row_group_start, row_group_end);\n \tthis->count = row_group_end;\n }\n \n void RowGroup::CommitAppend(transaction_t commit_id, idx_t row_group_start, idx_t count) {\n-\tD_ASSERT(version_info.get());\n-\tidx_t row_group_end = row_group_start + count;\n-\tlock_guard<mutex> lock(row_group_lock);\n-\n-\tidx_t start_vector_idx = row_group_start / STANDARD_VECTOR_SIZE;\n-\tidx_t end_vector_idx = (row_group_end - 1) / STANDARD_VECTOR_SIZE;\n-\tfor (idx_t vector_idx = start_vector_idx; vector_idx <= end_vector_idx; vector_idx++) {\n-\t\tidx_t start = vector_idx == start_vector_idx ? row_group_start - start_vector_idx * STANDARD_VECTOR_SIZE : 0;\n-\t\tidx_t end =\n-\t\t    vector_idx == end_vector_idx ? row_group_end - end_vector_idx * STANDARD_VECTOR_SIZE : STANDARD_VECTOR_SIZE;\n-\n-\t\tauto info = version_info->info[vector_idx].get();\n-\t\tinfo->CommitAppend(commit_id, start, end);\n-\t}\n+\tauto &vinfo = GetOrCreateVersionInfo();\n+\tvinfo.CommitAppend(commit_id, row_group_start, count);\n }\n \n void RowGroup::RevertAppend(idx_t row_group_start) {\n-\tif (!version_info) {\n-\t\treturn;\n-\t}\n-\tidx_t start_row = row_group_start - this->start;\n-\tidx_t start_vector_idx = (start_row + (STANDARD_VECTOR_SIZE - 1)) / STANDARD_VECTOR_SIZE;\n-\tfor (idx_t vector_idx = start_vector_idx; vector_idx < RowGroup::ROW_GROUP_VECTOR_COUNT; vector_idx++) {\n-\t\tversion_info->info[vector_idx].reset();\n-\t}\n+\tauto &vinfo = GetOrCreateVersionInfo();\n+\tvinfo.RevertAppend(row_group_start - this->start);\n \tfor (auto &column : columns) {\n \t\tcolumn->RevertAppend(row_group_start);\n \t}\n@@ -806,10 +756,24 @@ RowGroupWriteData RowGroup::WriteToDisk(PartialBlockManager &manager,\n }\n \n bool RowGroup::AllDeleted() {\n-\tif (!version_info) {\n+\tif (HasUnloadedDeletes()) {\n+\t\t// deletes aren't loaded yet - we know not everything is deleted\n+\t\treturn false;\n+\t}\n+\tauto &vinfo = GetVersionInfo();\n+\tif (!vinfo) {\n+\t\treturn false;\n+\t}\n+\treturn vinfo->GetCommittedDeletedCount(count) == count;\n+}\n+\n+bool RowGroup::HasUnloadedDeletes() const {\n+\tif (deletes_pointers.empty()) {\n+\t\t// no stored deletes at all\n \t\treturn false;\n \t}\n-\treturn version_info->GetCommittedDeletedCount(count) == count;\n+\t// return whether or not the deletes have been loaded\n+\treturn !deletes_is_loaded;\n }\n \n RowGroupPointer RowGroup::Checkpoint(RowGroupWriter &writer, TableStatistics &global_stats) {\n@@ -846,48 +810,30 @@ RowGroupPointer RowGroup::Checkpoint(RowGroupWriter &writer, TableStatistics &gl\n \t\tstate->WriteDataPointers(writer, serializer);\n \t\tserializer.End();\n \t}\n-\trow_group_pointer.versions = version_info;\n+\trow_group_pointer.deletes_pointers = CheckpointDeletes(writer.GetPayloadWriter().GetManager());\n \tVerify();\n \treturn row_group_pointer;\n }\n \n+vector<MetaBlockPointer> RowGroup::CheckpointDeletes(MetadataManager &manager) {\n+\tif (HasUnloadedDeletes()) {\n+\t\t// deletes were not loaded so they cannot be changed\n+\t\t// re-use them as-is\n+\t\tmanager.ClearModifiedBlocks(deletes_pointers);\n+\t\treturn deletes_pointers;\n+\t}\n+\tif (!version_info) {\n+\t\t// no version information: write nothing\n+\t\treturn vector<MetaBlockPointer>();\n+\t}\n+\treturn version_info->Checkpoint(manager);\n+}\n+\n void RowGroup::Serialize(RowGroupPointer &pointer, Serializer &serializer) {\n \tserializer.WriteProperty(100, \"row_start\", pointer.row_start);\n \tserializer.WriteProperty(101, \"tuple_count\", pointer.tuple_count);\n \tserializer.WriteProperty(102, \"data_pointers\", pointer.data_pointers);\n-\n-\t// Checkpoint deletes\n-\tauto versions = pointer.versions.get();\n-\n-\tif (!versions) {\n-\t\t// no version information: write nothing\n-\t\tserializer.WriteProperty(103, \"versions_count\", 0);\n-\t\treturn;\n-\t}\n-\t// first count how many ChunkInfo's we need to deserialize\n-\tidx_t chunk_info_count = 0;\n-\tidx_t idx_map[ROW_GROUP_VECTOR_COUNT];\n-\tfor (idx_t vector_idx = 0; vector_idx < RowGroup::ROW_GROUP_VECTOR_COUNT; vector_idx++) {\n-\t\tauto chunk_info = versions->info[vector_idx].get();\n-\t\tif (!chunk_info) {\n-\t\t\tcontinue;\n-\t\t}\n-\t\tidx_map[chunk_info_count++] = vector_idx;\n-\t}\n-\n-\t// now serialize the actual version information\n-\tserializer.WriteProperty(103, \"versions_count\", chunk_info_count);\n-\tif (chunk_info_count == 0) {\n-\t\treturn;\n-\t}\n-\tserializer.WriteList(104, \"versions\", chunk_info_count, [&](Serializer::List &list, idx_t i) {\n-\t\tauto vector_idx = idx_map[i];\n-\t\tauto chunk_info = versions->info[vector_idx].get();\n-\t\tlist.WriteObject([&](Serializer &obj) {\n-\t\t\tobj.WriteProperty(100, \"vector_index\", vector_idx);\n-\t\t\tobj.WriteProperty(101, \"chunk_info\", const_cast<const ChunkInfo *>(chunk_info));\n-\t\t});\n-\t});\n+\tserializer.WriteProperty(103, \"delete_pointers\", pointer.deletes_pointers);\n }\n \n RowGroupPointer RowGroup::Deserialize(Deserializer &deserializer) {\n@@ -895,26 +841,7 @@ RowGroupPointer RowGroup::Deserialize(Deserializer &deserializer) {\n \tresult.row_start = deserializer.ReadProperty<uint64_t>(100, \"row_start\");\n \tresult.tuple_count = deserializer.ReadProperty<uint64_t>(101, \"tuple_count\");\n \tresult.data_pointers = deserializer.ReadProperty<vector<MetaBlockPointer>>(102, \"data_pointers\");\n-\tresult.versions = nullptr;\n-\t// Deserialize Deletes\n-\tauto chunk_count = deserializer.ReadProperty<idx_t>(103, \"versions_count\");\n-\tif (chunk_count == 0) {\n-\t\treturn result;\n-\t}\n-\tauto version_info = make_shared<VersionNode>();\n-\tdeserializer.ReadList(104, \"versions\", [&](Deserializer::List &list, idx_t i) {\n-\t\tlist.ReadObject([&](Deserializer &obj) {\n-\t\t\tauto vector_index = obj.ReadProperty<idx_t>(100, \"vector_index\");\n-\t\t\tif (vector_index >= RowGroup::ROW_GROUP_VECTOR_COUNT) {\n-\t\t\t\tthrow Exception(\"In DeserializeDeletes, vector_index is out of range for the row group. Corrupted \"\n-\t\t\t\t                \"file?\");\n-\t\t\t}\n-\t\t\tversion_info->info[vector_index] = obj.ReadProperty<unique_ptr<ChunkInfo>>(101, \"chunk_info\");\n-\t\t});\n-\t});\n-\n-\tresult.versions = version_info;\n-\n+\tresult.deletes_pointers = deserializer.ReadProperty<vector<MetaBlockPointer>>(103, \"delete_pointers\");\n \treturn result;\n }\n \n@@ -934,14 +861,13 @@ void RowGroup::GetColumnSegmentInfo(idx_t row_group_index, vector<ColumnSegmentI\n class VersionDeleteState {\n public:\n \tVersionDeleteState(RowGroup &info, TransactionData transaction, DataTable &table, idx_t base_row)\n-\t    : info(info), transaction(transaction), table(table), current_info(nullptr),\n-\t      current_chunk(DConstants::INVALID_INDEX), count(0), base_row(base_row), delete_count(0) {\n+\t    : info(info), transaction(transaction), table(table), current_chunk(DConstants::INVALID_INDEX), count(0),\n+\t      base_row(base_row), delete_count(0) {\n \t}\n \n \tRowGroup &info;\n \tTransactionData transaction;\n \tDataTable &table;\n-\tChunkVectorInfo *current_info;\n \tidx_t current_chunk;\n \trow_t rows[STANDARD_VECTOR_SIZE];\n \tidx_t count;\n@@ -955,7 +881,6 @@ class VersionDeleteState {\n };\n \n idx_t RowGroup::Delete(TransactionData transaction, DataTable &table, row_t *ids, idx_t count) {\n-\tlock_guard<mutex> lock(row_group_lock);\n \tVersionDeleteState del_state(*this, transaction, table, this->start);\n \n \t// obtain a write lock\n@@ -976,6 +901,10 @@ void RowGroup::Verify() {\n #endif\n }\n \n+idx_t RowGroup::DeleteRows(idx_t vector_idx, transaction_t transaction_id, row_t rows[], idx_t count) {\n+\treturn GetOrCreateVersionInfo().DeleteRows(vector_idx, transaction_id, rows, count);\n+}\n+\n void VersionDeleteState::Delete(row_t row_id) {\n \tD_ASSERT(row_id >= 0);\n \tidx_t vector_idx = row_id / STANDARD_VECTOR_SIZE;\n@@ -983,26 +912,6 @@ void VersionDeleteState::Delete(row_t row_id) {\n \tif (current_chunk != vector_idx) {\n \t\tFlush();\n \n-\t\tif (!info.version_info) {\n-\t\t\tinfo.version_info = make_shared<VersionNode>();\n-\t\t}\n-\n-\t\tif (!info.version_info->info[vector_idx]) {\n-\t\t\t// no info yet: create it\n-\t\t\tinfo.version_info->info[vector_idx] =\n-\t\t\t    make_uniq<ChunkVectorInfo>(info.start + vector_idx * STANDARD_VECTOR_SIZE);\n-\t\t} else if (info.version_info->info[vector_idx]->type == ChunkInfoType::CONSTANT_INFO) {\n-\t\t\tauto &constant = info.version_info->info[vector_idx]->Cast<ChunkConstantInfo>();\n-\t\t\t// info exists but it's a constant info: convert to a vector info\n-\t\t\tauto new_info = make_uniq<ChunkVectorInfo>(info.start + vector_idx * STANDARD_VECTOR_SIZE);\n-\t\t\tnew_info->insert_id = constant.insert_id.load();\n-\t\t\tfor (idx_t i = 0; i < STANDARD_VECTOR_SIZE; i++) {\n-\t\t\t\tnew_info->inserted[i] = constant.insert_id.load();\n-\t\t\t}\n-\t\t\tinfo.version_info->info[vector_idx] = std::move(new_info);\n-\t\t}\n-\t\tD_ASSERT(info.version_info->info[vector_idx]->type == ChunkInfoType::VECTOR_INFO);\n-\t\tcurrent_info = &info.version_info->info[vector_idx]->Cast<ChunkVectorInfo>();\n \t\tcurrent_chunk = vector_idx;\n \t\tchunk_row = vector_idx * STANDARD_VECTOR_SIZE;\n \t}\n@@ -1016,11 +925,12 @@ void VersionDeleteState::Flush() {\n \t// it is possible for delete statements to delete the same tuple multiple times when combined with a USING clause\n \t// in the current_info->Delete, we check which tuples are actually deleted (excluding duplicate deletions)\n \t// this is returned in the actual_delete_count\n-\tauto actual_delete_count = current_info->Delete(transaction.transaction_id, rows, count);\n+\tauto actual_delete_count = info.DeleteRows(current_chunk, transaction.transaction_id, rows, count);\n \tdelete_count += actual_delete_count;\n \tif (transaction.transaction && actual_delete_count > 0) {\n \t\t// now push the delete into the undo buffer, but only if any deletes were actually performed\n-\t\ttransaction.transaction->PushDelete(table, current_info, rows, actual_delete_count, base_row + chunk_row);\n+\t\ttransaction.transaction->PushDelete(table, info.GetOrCreateVersionInfo(), current_chunk, rows,\n+\t\t                                    actual_delete_count, base_row + chunk_row);\n \t}\n \tcount = 0;\n }\ndiff --git a/src/storage/table/row_group_collection.cpp b/src/storage/table/row_group_collection.cpp\nindex 425fcde95d37..1b0d9b702e98 100644\n--- a/src/storage/table/row_group_collection.cpp\n+++ b/src/storage/table/row_group_collection.cpp\n@@ -339,7 +339,7 @@ bool RowGroupCollection::Append(DataChunk &chunk, TableAppendState &state) {\n \t\tauto current_row_group = state.row_group_append_state.row_group;\n \t\t// check how much we can fit into the current row_group\n \t\tidx_t append_count =\n-\t\t    MinValue<idx_t>(remaining, RowGroup::ROW_GROUP_SIZE - state.row_group_append_state.offset_in_row_group);\n+\t\t    MinValue<idx_t>(remaining, Storage::ROW_GROUP_SIZE - state.row_group_append_state.offset_in_row_group);\n \t\tif (append_count > 0) {\n \t\t\tcurrent_row_group->Append(state.row_group_append_state, chunk, append_count);\n \t\t\t// merge the stats\n@@ -393,7 +393,7 @@ void RowGroupCollection::FinalizeAppend(TransactionData transaction, TableAppend\n \tauto remaining = state.total_append_count;\n \tauto row_group = state.start_row_group;\n \twhile (remaining > 0) {\n-\t\tauto append_count = MinValue<idx_t>(remaining, RowGroup::ROW_GROUP_SIZE - row_group->count);\n+\t\tauto append_count = MinValue<idx_t>(remaining, Storage::ROW_GROUP_SIZE - row_group->count);\n \t\trow_group->AppendVersionInfo(transaction, append_count);\n \t\tremaining -= append_count;\n \t\trow_group = row_groups->GetNextSegment(row_group);\ndiff --git a/src/storage/table/row_version_manager.cpp b/src/storage/table/row_version_manager.cpp\nnew file mode 100644\nindex 000000000000..9fa7c46cb882\n--- /dev/null\n+++ b/src/storage/table/row_version_manager.cpp\n@@ -0,0 +1,228 @@\n+#include \"duckdb/storage/table/row_version_manager.hpp\"\n+#include \"duckdb/transaction/transaction_data.hpp\"\n+#include \"duckdb/storage/metadata/metadata_manager.hpp\"\n+#include \"duckdb/storage/metadata/metadata_reader.hpp\"\n+#include \"duckdb/storage/metadata/metadata_writer.hpp\"\n+#include \"duckdb/common/pair.hpp\"\n+\n+namespace duckdb {\n+\n+RowVersionManager::RowVersionManager(idx_t start) : start(start), has_changes(false) {\n+}\n+\n+void RowVersionManager::SetStart(idx_t new_start) {\n+\tlock_guard<mutex> l(version_lock);\n+\tthis->start = new_start;\n+\tidx_t current_start = start;\n+\tfor (idx_t i = 0; i < Storage::ROW_GROUP_VECTOR_COUNT; i++) {\n+\t\tif (vector_info[i]) {\n+\t\t\tvector_info[i]->start = current_start;\n+\t\t}\n+\t\tcurrent_start += STANDARD_VECTOR_SIZE;\n+\t}\n+}\n+\n+idx_t RowVersionManager::GetCommittedDeletedCount(idx_t count) {\n+\tlock_guard<mutex> l(version_lock);\n+\tidx_t deleted_count = 0;\n+\tfor (idx_t r = 0, i = 0; r < count; r += STANDARD_VECTOR_SIZE, i++) {\n+\t\tif (!vector_info[i]) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tidx_t max_count = MinValue<idx_t>(STANDARD_VECTOR_SIZE, count - r);\n+\t\tif (max_count == 0) {\n+\t\t\tbreak;\n+\t\t}\n+\t\tdeleted_count += vector_info[i]->GetCommittedDeletedCount(max_count);\n+\t}\n+\treturn deleted_count;\n+}\n+\n+optional_ptr<ChunkInfo> RowVersionManager::GetChunkInfo(idx_t vector_idx) {\n+\treturn vector_info[vector_idx].get();\n+}\n+\n+idx_t RowVersionManager::GetSelVector(TransactionData transaction, idx_t vector_idx, SelectionVector &sel_vector,\n+                                      idx_t max_count) {\n+\tlock_guard<mutex> l(version_lock);\n+\tauto chunk_info = GetChunkInfo(vector_idx);\n+\tif (!chunk_info) {\n+\t\treturn max_count;\n+\t}\n+\treturn chunk_info->GetSelVector(transaction, sel_vector, max_count);\n+}\n+\n+idx_t RowVersionManager::GetCommittedSelVector(transaction_t start_time, transaction_t transaction_id, idx_t vector_idx,\n+                                               SelectionVector &sel_vector, idx_t max_count) {\n+\tlock_guard<mutex> l(version_lock);\n+\tauto info = GetChunkInfo(vector_idx);\n+\tif (!info) {\n+\t\treturn max_count;\n+\t}\n+\treturn info->GetCommittedSelVector(start_time, transaction_id, sel_vector, max_count);\n+}\n+\n+bool RowVersionManager::Fetch(TransactionData transaction, idx_t row) {\n+\tlock_guard<mutex> lock(version_lock);\n+\tidx_t vector_index = row / STANDARD_VECTOR_SIZE;\n+\tauto info = GetChunkInfo(vector_index);\n+\tif (!info) {\n+\t\treturn true;\n+\t}\n+\treturn info->Fetch(transaction, row - vector_index * STANDARD_VECTOR_SIZE);\n+}\n+\n+void RowVersionManager::AppendVersionInfo(TransactionData transaction, idx_t count, idx_t row_group_start,\n+                                          idx_t row_group_end) {\n+\tlock_guard<mutex> lock(version_lock);\n+\thas_changes = true;\n+\tidx_t start_vector_idx = row_group_start / STANDARD_VECTOR_SIZE;\n+\tidx_t end_vector_idx = (row_group_end - 1) / STANDARD_VECTOR_SIZE;\n+\tfor (idx_t vector_idx = start_vector_idx; vector_idx <= end_vector_idx; vector_idx++) {\n+\t\tidx_t vector_start =\n+\t\t    vector_idx == start_vector_idx ? row_group_start - start_vector_idx * STANDARD_VECTOR_SIZE : 0;\n+\t\tidx_t vector_end =\n+\t\t    vector_idx == end_vector_idx ? row_group_end - end_vector_idx * STANDARD_VECTOR_SIZE : STANDARD_VECTOR_SIZE;\n+\t\tif (vector_start == 0 && vector_end == STANDARD_VECTOR_SIZE) {\n+\t\t\t// entire vector is encapsulated by append: append a single constant\n+\t\t\tauto constant_info = make_uniq<ChunkConstantInfo>(start + vector_idx * STANDARD_VECTOR_SIZE);\n+\t\t\tconstant_info->insert_id = transaction.transaction_id;\n+\t\t\tconstant_info->delete_id = NOT_DELETED_ID;\n+\t\t\tvector_info[vector_idx] = std::move(constant_info);\n+\t\t} else {\n+\t\t\t// part of a vector is encapsulated: append to that part\n+\t\t\toptional_ptr<ChunkVectorInfo> new_info;\n+\t\t\tif (!vector_info[vector_idx]) {\n+\t\t\t\t// first time appending to this vector: create new info\n+\t\t\t\tauto insert_info = make_uniq<ChunkVectorInfo>(start + vector_idx * STANDARD_VECTOR_SIZE);\n+\t\t\t\tnew_info = insert_info.get();\n+\t\t\t\tvector_info[vector_idx] = std::move(insert_info);\n+\t\t\t} else if (vector_info[vector_idx]->type == ChunkInfoType::VECTOR_INFO) {\n+\t\t\t\t// use existing vector\n+\t\t\t\tnew_info = &vector_info[vector_idx]->Cast<ChunkVectorInfo>();\n+\t\t\t} else {\n+\t\t\t\tthrow InternalException(\"Error in RowVersionManager::AppendVersionInfo - expected either a \"\n+\t\t\t\t                        \"ChunkVectorInfo or no version info\");\n+\t\t\t}\n+\t\t\tnew_info->Append(vector_start, vector_end, transaction.transaction_id);\n+\t\t}\n+\t}\n+}\n+\n+void RowVersionManager::CommitAppend(transaction_t commit_id, idx_t row_group_start, idx_t count) {\n+\tidx_t row_group_end = row_group_start + count;\n+\n+\tlock_guard<mutex> lock(version_lock);\n+\tidx_t start_vector_idx = row_group_start / STANDARD_VECTOR_SIZE;\n+\tidx_t end_vector_idx = (row_group_end - 1) / STANDARD_VECTOR_SIZE;\n+\tfor (idx_t vector_idx = start_vector_idx; vector_idx <= end_vector_idx; vector_idx++) {\n+\t\tidx_t vstart = vector_idx == start_vector_idx ? row_group_start - start_vector_idx * STANDARD_VECTOR_SIZE : 0;\n+\t\tidx_t vend =\n+\t\t    vector_idx == end_vector_idx ? row_group_end - end_vector_idx * STANDARD_VECTOR_SIZE : STANDARD_VECTOR_SIZE;\n+\n+\t\tauto info = vector_info[vector_idx].get();\n+\t\tinfo->CommitAppend(commit_id, vstart, vend);\n+\t}\n+}\n+\n+void RowVersionManager::RevertAppend(idx_t start_row) {\n+\tlock_guard<mutex> lock(version_lock);\n+\tidx_t start_vector_idx = (start_row + (STANDARD_VECTOR_SIZE - 1)) / STANDARD_VECTOR_SIZE;\n+\tfor (idx_t vector_idx = start_vector_idx; vector_idx < Storage::ROW_GROUP_VECTOR_COUNT; vector_idx++) {\n+\t\tvector_info[vector_idx].reset();\n+\t}\n+}\n+\n+ChunkVectorInfo &RowVersionManager::GetVectorInfo(idx_t vector_idx) {\n+\tif (!vector_info[vector_idx]) {\n+\t\t// no info yet: create it\n+\t\tvector_info[vector_idx] = make_uniq<ChunkVectorInfo>(start + vector_idx * STANDARD_VECTOR_SIZE);\n+\t} else if (vector_info[vector_idx]->type == ChunkInfoType::CONSTANT_INFO) {\n+\t\tauto &constant = vector_info[vector_idx]->Cast<ChunkConstantInfo>();\n+\t\t// info exists but it's a constant info: convert to a vector info\n+\t\tauto new_info = make_uniq<ChunkVectorInfo>(start + vector_idx * STANDARD_VECTOR_SIZE);\n+\t\tnew_info->insert_id = constant.insert_id;\n+\t\tfor (idx_t i = 0; i < STANDARD_VECTOR_SIZE; i++) {\n+\t\t\tnew_info->inserted[i] = constant.insert_id;\n+\t\t}\n+\t\tvector_info[vector_idx] = std::move(new_info);\n+\t}\n+\tD_ASSERT(vector_info[vector_idx]->type == ChunkInfoType::VECTOR_INFO);\n+\treturn vector_info[vector_idx]->Cast<ChunkVectorInfo>();\n+}\n+\n+idx_t RowVersionManager::DeleteRows(idx_t vector_idx, transaction_t transaction_id, row_t rows[], idx_t count) {\n+\tlock_guard<mutex> lock(version_lock);\n+\thas_changes = true;\n+\treturn GetVectorInfo(vector_idx).Delete(transaction_id, rows, count);\n+}\n+\n+void RowVersionManager::CommitDelete(idx_t vector_idx, transaction_t commit_id, row_t rows[], idx_t count) {\n+\tlock_guard<mutex> lock(version_lock);\n+\thas_changes = true;\n+\tGetVectorInfo(vector_idx).CommitDelete(commit_id, rows, count);\n+}\n+\n+vector<MetaBlockPointer> RowVersionManager::Checkpoint(MetadataManager &manager) {\n+\tif (!has_changes && !storage_pointers.empty()) {\n+\t\t// the row version manager already exists on disk and no changes were made\n+\t\t// we can write the current pointer as-is\n+\t\t// ensure the blocks we are pointing to are not marked as free\n+\t\tmanager.ClearModifiedBlocks(storage_pointers);\n+\t\t// return the root pointer\n+\t\treturn storage_pointers;\n+\t}\n+\t// first count how many ChunkInfo's we need to deserialize\n+\tvector<pair<idx_t, reference<ChunkInfo>>> to_serialize;\n+\tfor (idx_t vector_idx = 0; vector_idx < Storage::ROW_GROUP_VECTOR_COUNT; vector_idx++) {\n+\t\tauto chunk_info = vector_info[vector_idx].get();\n+\t\tif (!chunk_info) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tif (!chunk_info->HasDeletes()) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tto_serialize.emplace_back(vector_idx, *chunk_info);\n+\t}\n+\tif (to_serialize.empty()) {\n+\t\treturn vector<MetaBlockPointer>();\n+\t}\n+\n+\tstorage_pointers.clear();\n+\n+\tMetadataWriter writer(manager, &storage_pointers);\n+\t// now serialize the actual version information\n+\twriter.Write<idx_t>(to_serialize.size());\n+\tfor (auto &entry : to_serialize) {\n+\t\tauto &vector_idx = entry.first;\n+\t\tauto &chunk_info = entry.second.get();\n+\t\twriter.Write<idx_t>(vector_idx);\n+\t\tchunk_info.Write(writer);\n+\t}\n+\twriter.Flush();\n+\n+\thas_changes = false;\n+\treturn storage_pointers;\n+}\n+\n+shared_ptr<RowVersionManager> RowVersionManager::Deserialize(MetaBlockPointer delete_pointer, MetadataManager &manager,\n+                                                             idx_t start) {\n+\tif (!delete_pointer.IsValid()) {\n+\t\treturn nullptr;\n+\t}\n+\tauto version_info = make_shared<RowVersionManager>(start);\n+\tMetadataReader source(manager, delete_pointer, &version_info->storage_pointers);\n+\tauto chunk_count = source.Read<idx_t>();\n+\tD_ASSERT(chunk_count > 0);\n+\tfor (idx_t i = 0; i < chunk_count; i++) {\n+\t\tidx_t vector_index = source.Read<idx_t>();\n+\t\tif (vector_index >= Storage::ROW_GROUP_VECTOR_COUNT) {\n+\t\t\tthrow Exception(\"In DeserializeDeletes, vector_index is out of range for the row group. Corrupted file?\");\n+\t\t}\n+\t\tversion_info->vector_info[vector_index] = ChunkInfo::Read(source);\n+\t}\n+\tversion_info->has_changes = false;\n+\treturn version_info;\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/storage/table/update_segment.cpp b/src/storage/table/update_segment.cpp\nindex 125906e1cc77..3e1cab7d4791 100644\n--- a/src/storage/table/update_segment.cpp\n+++ b/src/storage/table/update_segment.cpp\n@@ -328,7 +328,7 @@ void UpdateSegment::FetchCommittedRange(idx_t start_row, idx_t count, Vector &re\n \tidx_t start_vector = start_row / STANDARD_VECTOR_SIZE;\n \tidx_t end_vector = (end_row - 1) / STANDARD_VECTOR_SIZE;\n \tD_ASSERT(start_vector <= end_vector);\n-\tD_ASSERT(end_vector < RowGroup::ROW_GROUP_VECTOR_COUNT);\n+\tD_ASSERT(end_vector < Storage::ROW_GROUP_VECTOR_COUNT);\n \n \tfor (idx_t vector_idx = start_vector; vector_idx <= end_vector; vector_idx++) {\n \t\tif (!root->info[vector_idx]) {\n@@ -1089,7 +1089,7 @@ void UpdateSegment::Update(TransactionData transaction, idx_t column_index, Vect\n \tidx_t vector_offset = column_data.start + vector_index * STANDARD_VECTOR_SIZE;\n \n \tD_ASSERT(idx_t(first_id) >= column_data.start);\n-\tD_ASSERT(vector_index < RowGroup::ROW_GROUP_VECTOR_COUNT);\n+\tD_ASSERT(vector_index < Storage::ROW_GROUP_VECTOR_COUNT);\n \n \t// first check the version chain\n \tUpdateInfo *node = nullptr;\ndiff --git a/src/transaction/cleanup_state.cpp b/src/transaction/cleanup_state.cpp\nindex 5dfa7f229793..cbdaa80a3ea2 100644\n--- a/src/transaction/cleanup_state.cpp\n+++ b/src/transaction/cleanup_state.cpp\n@@ -8,6 +8,7 @@\n #include \"duckdb/catalog/dependency_manager.hpp\"\n #include \"duckdb/storage/table/chunk_info.hpp\"\n #include \"duckdb/storage/table/update_segment.hpp\"\n+#include \"duckdb/storage/table/row_version_manager.hpp\"\n \n namespace duckdb {\n \n@@ -69,7 +70,7 @@ void CleanupState::CleanupDelete(DeleteInfo &info) {\n \n \tcount = 0;\n \tfor (idx_t i = 0; i < info.count; i++) {\n-\t\trow_numbers[count++] = info.vinfo->start + info.rows[i];\n+\t\trow_numbers[count++] = info.base_row + info.rows[i];\n \t}\n \tFlush();\n }\ndiff --git a/src/transaction/commit_state.cpp b/src/transaction/commit_state.cpp\nindex 6bba9c175706..6f844317dca1 100644\n--- a/src/transaction/commit_state.cpp\n+++ b/src/transaction/commit_state.cpp\n@@ -16,13 +16,14 @@\n #include \"duckdb/transaction/update_info.hpp\"\n #include \"duckdb/catalog/catalog_entry/scalar_macro_catalog_entry.hpp\"\n #include \"duckdb/catalog/catalog_entry/view_catalog_entry.hpp\"\n+#include \"duckdb/storage/table/row_version_manager.hpp\"\n #include \"duckdb/common/serializer/binary_deserializer.hpp\"\n #include \"duckdb/common/serializer/memory_stream.hpp\"\n \n namespace duckdb {\n \n-CommitState::CommitState(ClientContext &context, transaction_t commit_id, optional_ptr<WriteAheadLog> log)\n-    : log(log), commit_id(commit_id), current_table_info(nullptr), context(context) {\n+CommitState::CommitState(transaction_t commit_id, optional_ptr<WriteAheadLog> log)\n+    : log(log), commit_id(commit_id), current_table_info(nullptr) {\n }\n \n void CommitState::SwitchTable(DataTableInfo *table_info, UndoFlags new_op) {\n@@ -280,7 +281,7 @@ void CommitState::CommitEntry(UndoFlags type, data_ptr_t data) {\n \t\t\tWriteDelete(*info);\n \t\t}\n \t\t// mark the tuples as committed\n-\t\tinfo->vinfo->CommitDelete(commit_id, info->rows, info->count);\n+\t\tinfo->version_info->CommitDelete(info->vector_idx, commit_id, info->rows, info->count);\n \t\tbreak;\n \t}\n \tcase UndoFlags::UPDATE_TUPLE: {\n@@ -321,7 +322,7 @@ void CommitState::RevertCommit(UndoFlags type, data_ptr_t data) {\n \t\tauto info = reinterpret_cast<DeleteInfo *>(data);\n \t\tinfo->table->info->cardinality += info->count;\n \t\t// revert the commit by writing the (uncommitted) transaction_id back into the version info\n-\t\tinfo->vinfo->CommitDelete(transaction_id, info->rows, info->count);\n+\t\tinfo->version_info->CommitDelete(info->vector_idx, transaction_id, info->rows, info->count);\n \t\tbreak;\n \t}\n \tcase UndoFlags::UPDATE_TUPLE: {\ndiff --git a/src/transaction/duck_transaction.cpp b/src/transaction/duck_transaction.cpp\nindex f1451b3d780b..ceaaf26bec5d 100644\n--- a/src/transaction/duck_transaction.cpp\n+++ b/src/transaction/duck_transaction.cpp\n@@ -70,10 +70,12 @@ void DuckTransaction::PushCatalogEntry(CatalogEntry &entry, data_ptr_t extra_dat\n \t}\n }\n \n-void DuckTransaction::PushDelete(DataTable &table, ChunkVectorInfo *vinfo, row_t rows[], idx_t count, idx_t base_row) {\n+void DuckTransaction::PushDelete(DataTable &table, RowVersionManager &info, idx_t vector_idx, row_t rows[], idx_t count,\n+                                 idx_t base_row) {\n \tauto delete_info = reinterpret_cast<DeleteInfo *>(\n \t    undo_buffer.CreateEntry(UndoFlags::DELETE_TUPLE, sizeof(DeleteInfo) + sizeof(row_t) * count));\n-\tdelete_info->vinfo = vinfo;\n+\tdelete_info->version_info = &info;\n+\tdelete_info->vector_idx = vector_idx;\n \tdelete_info->table = &table;\n \tdelete_info->count = count;\n \tdelete_info->base_row = base_row;\ndiff --git a/src/transaction/rollback_state.cpp b/src/transaction/rollback_state.cpp\nindex 8edfb0b437be..b30124c113e3 100644\n--- a/src/transaction/rollback_state.cpp\n+++ b/src/transaction/rollback_state.cpp\n@@ -9,6 +9,7 @@\n #include \"duckdb/catalog/catalog_set.hpp\"\n #include \"duckdb/storage/data_table.hpp\"\n #include \"duckdb/storage/table/update_segment.hpp\"\n+#include \"duckdb/storage/table/row_version_manager.hpp\"\n \n namespace duckdb {\n \n@@ -30,7 +31,7 @@ void RollbackState::RollbackEntry(UndoFlags type, data_ptr_t data) {\n \tcase UndoFlags::DELETE_TUPLE: {\n \t\tauto info = reinterpret_cast<DeleteInfo *>(data);\n \t\t// reset the deleted flag on rollback\n-\t\tinfo->vinfo->CommitDelete(NOT_DELETED_ID, info->rows, info->count);\n+\t\tinfo->version_info->CommitDelete(info->vector_idx, NOT_DELETED_ID, info->rows, info->count);\n \t\tbreak;\n \t}\n \tcase UndoFlags::UPDATE_TUPLE: {\ndiff --git a/src/transaction/undo_buffer.cpp b/src/transaction/undo_buffer.cpp\nindex d89bf136969c..bf1717b70527 100644\n--- a/src/transaction/undo_buffer.cpp\n+++ b/src/transaction/undo_buffer.cpp\n@@ -11,12 +11,10 @@\n #include \"duckdb/transaction/rollback_state.hpp\"\n #include \"duckdb/common/pair.hpp\"\n \n-#include <unordered_map>\n-\n namespace duckdb {\n constexpr uint32_t UNDO_ENTRY_HEADER_SIZE = sizeof(UndoFlags) + sizeof(uint32_t);\n \n-UndoBuffer::UndoBuffer(ClientContext &context_p) : context(context_p), allocator(BufferAllocator::Get(context_p)) {\n+UndoBuffer::UndoBuffer(ClientContext &context_p) : allocator(BufferAllocator::Get(context_p)) {\n }\n \n data_ptr_t UndoBuffer::CreateEntry(UndoFlags type, idx_t len) {\n@@ -138,7 +136,7 @@ void UndoBuffer::Cleanup() {\n \n void UndoBuffer::Commit(UndoBuffer::IteratorState &iterator_state, optional_ptr<WriteAheadLog> log,\n                         transaction_t commit_id) {\n-\tCommitState state(context, commit_id, log);\n+\tCommitState state(commit_id, log);\n \tif (log) {\n \t\t// commit WITH write ahead log\n \t\tIterateEntries(iterator_state, [&](UndoFlags type, data_ptr_t data) { state.CommitEntry<true>(type, data); });\n@@ -149,7 +147,7 @@ void UndoBuffer::Commit(UndoBuffer::IteratorState &iterator_state, optional_ptr<\n }\n \n void UndoBuffer::RevertCommit(UndoBuffer::IteratorState &end_state, transaction_t transaction_id) {\n-\tCommitState state(context, transaction_id, nullptr);\n+\tCommitState state(transaction_id, nullptr);\n \tUndoBuffer::IteratorState start_state;\n \tIterateEntries(start_state, end_state, [&](UndoFlags type, data_ptr_t data) { state.RevertCommit(type, data); });\n }\n",
  "test_patch": "diff --git a/test/issues/fuzz/encode_string_data_crash.test b/test/issues/fuzz/encode_string_data_crash.test\nindex a52920a52720..56f062b3c85d 100644\n--- a/test/issues/fuzz/encode_string_data_crash.test\n+++ b/test/issues/fuzz/encode_string_data_crash.test\n@@ -24,7 +24,8 @@ SELECT sum(a) OVER (\n     PARTITION BY (\n SELECT c FROM id WHERE strings_with_null=a\n     ) ORDER BY a\n-  ) FROM strings;\n+  ) FROM strings\n+ORDER BY 1\n ----\n 10\n 30\ndiff --git a/test/sql/function/list/list_resize_types.test_slow b/test/sql/function/list/list_resize_types.test_slow\nindex f3fabc6e48f8..cec76bfaedf1 100644\n--- a/test/sql/function/list/list_resize_types.test_slow\n+++ b/test/sql/function/list/list_resize_types.test_slow\n@@ -2,6 +2,8 @@\n # description: test for list_resize() that take a long time\n # group: [list]\n \n+require vector_size 512\n+\n statement ok\n PRAGMA enable_verification;\n \ndiff --git a/test/sql/index/art/nodes/test_art_leaf_with_duplicates.test_slow b/test/sql/index/art/nodes/test_art_leaf_with_duplicates.test_slow\nindex 324c1f1784b4..3ead22902502 100644\n--- a/test/sql/index/art/nodes/test_art_leaf_with_duplicates.test_slow\n+++ b/test/sql/index/art/nodes/test_art_leaf_with_duplicates.test_slow\n@@ -12,12 +12,8 @@ statement ok\n CREATE INDEX i_index ON integers(i)\n \n # insert 4 elements\n-loop i 0 4\n-\n statement ok\n-INSERT INTO integers VALUES (1);\n-\n-endloop\n+INSERT INTO integers SELECT 1 FROM range(4)\n \n # verify counts\n \n@@ -51,12 +47,8 @@ SELECT COUNT(*) FROM integers\n 0\n \n # insert 1024 elements\n-loop i 0 1024\n-\n statement ok\n-INSERT INTO integers VALUES (1);\n-\n-endloop\n+INSERT INTO integers SELECT 1 FROM range(1024);\n \n # verify counts\n \n@@ -90,20 +82,12 @@ SELECT COUNT(*) FROM integers\n 0\n \n # insert 3000 elements: 1\n-loop i 0 3000\n-\n statement ok\n-INSERT INTO integers VALUES (1);\n-\n-endloop\n+INSERT INTO integers SELECT 1 FROM range(3000);\n \n # insert 1024 elements: 2\n-loop i 0 1024\n-\n statement ok\n-INSERT INTO integers VALUES (2);\n-\n-endloop\n+INSERT INTO integers SELECT 2 FROM range(1024);\n \n # verify counts\n \ndiff --git a/test/sql/parallelism/intraquery/test_aggregations_parallelism.test_slow b/test/sql/parallelism/intraquery/test_aggregations_parallelism.test_slow\nindex f6ab52ce1a1f..27dec21d2dfb 100644\n--- a/test/sql/parallelism/intraquery/test_aggregations_parallelism.test_slow\n+++ b/test/sql/parallelism/intraquery/test_aggregations_parallelism.test_slow\n@@ -26,13 +26,13 @@ create table bool (a bool);\n statement ok\n insert into bool select i from (values (True),(False)) tbl(i), range(5000);\n \n-query I\n+query I rowsort\n select regr_avgx(a, b) from t group by b%2;\n ----\n 4\n 5\n \n-query I\n+query I rowsort\n select regr_avgy(a, b) from t group by b%2;\n ----\n 49999\n@@ -87,7 +87,7 @@ from bool;\n ----\n 1\t0\n \n-query II\n+query II rowsort\n select approx_count_distinct(a), approx_count_distinct(b) from t group by b%2;\n ----\n 49874\t5\ndiff --git a/test/sql/parallelism/intraquery/test_parallel_nested_aggregates.test b/test/sql/parallelism/intraquery/test_parallel_nested_aggregates.test\nindex e27b81ac86e3..3ef81d107641 100644\n--- a/test/sql/parallelism/intraquery/test_parallel_nested_aggregates.test\n+++ b/test/sql/parallelism/intraquery/test_parallel_nested_aggregates.test\n@@ -24,7 +24,7 @@ create table t as select range a, range%10 b from range(100000);\n statement ok\n select first([a]) from t group by b%2;\n \n-query II\n+query II rowsort\n select min([a]), max([a]) from t group by b%2;\n ----\n [0]\t[99998]\ndiff --git a/test/sql/storage/buffer_manager/larger_than_memory_aggregate.test_slow b/test/sql/storage/buffer_manager/larger_than_memory_aggregate.test_slow\nnew file mode 100644\nindex 000000000000..27df807ed413\n--- /dev/null\n+++ b/test/sql/storage/buffer_manager/larger_than_memory_aggregate.test_slow\n@@ -0,0 +1,56 @@\n+# name: test/sql/storage/buffer_manager/larger_than_memory_aggregate.test_slow\n+# description: Test scanning a table and computing an aggregate over a table that exceeds buffer manager size\n+# group: [buffer_manager]\n+\n+# load the DB from disk\n+load __TEST_DIR__/larger_than_memory_aggregate.db\n+\n+statement ok\n+PRAGMA force_compression='uncompressed'\n+\n+statement ok\n+SET memory_limit='10000000b'\n+\n+statement ok\n+SET threads=1\n+\n+statement ok\n+CREATE TABLE test (a INTEGER, b INTEGER);\n+\n+statement ok\n+INSERT INTO test VALUES (11, 22), (13, 22), (12, 21), (NULL, NULL)\n+\n+# insert until 16777216 rows\n+loop i 0 22\n+\n+statement ok\n+INSERT INTO test SELECT * FROM test\n+\n+endloop\n+\n+query I\n+SELECT COUNT(*) FROM test\n+----\n+16777216\n+\n+query I\n+SELECT SUM(a) + SUM(b) FROM test\n+----\n+423624704\n+\n+loop i 0 2\n+\n+restart\n+\n+statement ok\n+SET memory_limit='10000000b'\n+\n+statement ok\n+SET threads=1\n+\n+query I\n+SELECT SUM(a) + SUM(b) FROM test\n+----\n+423624704\n+\n+endloop\ndiff --git a/test/sql/storage/compression/bitpacking/bitpacking_filter_pushdown.test b/test/sql/storage/compression/bitpacking/bitpacking_filter_pushdown.test\nindex 2ae8f0bcbf0f..47119dd2b479 100644\n--- a/test/sql/storage/compression/bitpacking/bitpacking_filter_pushdown.test\n+++ b/test/sql/storage/compression/bitpacking/bitpacking_filter_pushdown.test\n@@ -8,7 +8,7 @@ load __TEST_DIR__/test_bitpacking.db\n statement ok\n PRAGMA force_compression = 'bitpacking'\n \n-foreach bitpacking_mode delta_for for constant_delta constant\n+foreach bitpacking_mode auto delta_for for constant_delta constant\n \n statement ok\n PRAGMA force_bitpacking_mode='${bitpacking_mode}'\ndiff --git a/test/sql/storage/compression/bitpacking/bitpacking_mode.test b/test/sql/storage/compression/bitpacking/bitpacking_mode.test\nnew file mode 100644\nindex 000000000000..d2b210438f78\n--- /dev/null\n+++ b/test/sql/storage/compression/bitpacking/bitpacking_mode.test\n@@ -0,0 +1,29 @@\n+# name: test/sql/storage/compression/bitpacking/bitpacking_mode.test\n+# description: Test bitpacking mode\n+# group: [bitpacking]\n+\n+statement ok\n+PRAGMA force_compression = 'bitpacking'\n+\n+query I\n+SELECT current_setting('force_bitpacking_mode')\n+----\n+auto\n+\n+statement error\n+PRAGMA force_bitpacking_mode='xxx'\n+----\n+Unrecognized option\n+\n+\n+foreach mode auto constant constant_delta delta_for for\n+\n+statement ok\n+PRAGMA force_bitpacking_mode='${mode}'\n+\n+query I\n+SELECT current_setting('force_bitpacking_mode')='${mode}'\n+----\n+true\n+\n+endloop\ndiff --git a/test/sql/storage/delete/load_delete_modify.test b/test/sql/storage/delete/load_delete_modify.test\nnew file mode 100644\nindex 000000000000..3ab33d7f9999\n--- /dev/null\n+++ b/test/sql/storage/delete/load_delete_modify.test\n@@ -0,0 +1,49 @@\n+# name: test/sql/storage/delete/load_delete_modify.test\n+# description: Test loading a table with deletes and then performing an operation\n+# group: [delete]\n+\n+# load the DB from disk\n+load __TEST_DIR__/load_delete_modify.db\n+\n+statement ok\n+CREATE TABLE integers AS SELECT * FROM generate_series(0,599999) t(i);\n+\n+query I\n+DELETE FROM integers WHERE i%2=0\n+----\n+300000\n+\n+# LOAD -> ALTER\n+restart\n+\n+statement ok\n+ALTER TABLE integers ADD COLUMN k INTEGER\n+\n+query III\n+SELECT COUNT(*), COUNT(i), COUNT(k) FROM integers\n+----\n+300000\t300000\t0\n+\n+# LOAD -> UPDATE\n+restart\n+\n+statement ok\n+UPDATE integers SET k=i+1\n+\n+query IIII\n+SELECT COUNT(*), COUNT(i), COUNT(k), SUM(k) - SUM(i) FROM integers\n+----\n+300000\t300000\t300000\t300000\n+\n+# LOAD -> DELETE MORE\n+restart\n+\n+query I\n+DELETE FROM integers WHERE i%3=0\n+----\n+100000\n+\n+query IIII\n+SELECT COUNT(*), COUNT(i), COUNT(k), SUM(k) - SUM(i) FROM integers\n+----\n+200000\t200000\t200000\t200000\ndiff --git a/test/sql/storage/delete/repeated_deletes.test b/test/sql/storage/delete/repeated_deletes.test\nnew file mode 100644\nindex 000000000000..6ced746c9508\n--- /dev/null\n+++ b/test/sql/storage/delete/repeated_deletes.test\n@@ -0,0 +1,130 @@\n+# name: test/sql/storage/delete/repeated_deletes.test\n+# description: Test deletes with storage\n+# group: [delete]\n+\n+# load the DB from disk\n+load __TEST_DIR__/test_repeated_deletes.db\n+\n+statement ok\n+CREATE TABLE test (i INTEGER);\n+\n+statement ok\n+INSERT INTO test SELECT * FROM generate_series(0, 999);\n+\n+query I\n+DELETE FROM test WHERE i%2=0;\n+----\n+500\n+\n+restart\n+\n+query IIII\n+SELECT COUNT(*), SUM(i), MIN(i), MAX(i) FROM test;\n+----\n+500\t250000\t1\t999\n+\n+statement ok\n+INSERT INTO test SELECT * FROM generate_series(1000, 1099);\n+\n+query IIII\n+SELECT COUNT(*), SUM(i), MIN(i), MAX(i) FROM test;\n+----\n+600\t354950\t1\t1099\n+\n+query I\n+DELETE FROM test WHERE i%3=0;\n+----\n+200\n+\n+restart\n+\n+query IIII\n+SELECT COUNT(*), SUM(i), MIN(i), MAX(i) FROM test;\n+----\n+400\t236633\t1\t1099\n+\n+statement ok\n+INSERT INTO test SELECT * FROM generate_series(1000, 1999);\n+\n+query IIII\n+SELECT COUNT(*), SUM(i), MIN(i), MAX(i) FROM test;\n+----\n+1400\t1736133\t1\t1999\n+\n+restart\n+\n+query IIII\n+SELECT COUNT(*), SUM(i), MIN(i), MAX(i) FROM test;\n+----\n+1400\t1736133\t1\t1999\n+\n+query I\n+SELECT COUNT(*) FROM test WHERE i%7=0\n+----\n+200\n+\n+query I\n+DELETE FROM test WHERE i%7=0;\n+----\n+200\n+\n+restart\n+\n+query IIII\n+SELECT COUNT(*), SUM(i), MIN(i), MAX(i) FROM test;\n+----\n+1200\t1488228\t1\t1999\n+\n+query I\n+DELETE FROM test WHERE i%3=0;\n+----\n+285\n+\n+restart\n+\n+query IIII\n+SELECT COUNT(*), SUM(i), MIN(i), MAX(i) FROM test;\n+----\n+915\t1060800\t1\t1999\n+\n+statement ok\n+INSERT INTO test SELECT 1 FROM generate_series(0, 4000);\n+\n+statement ok\n+INSERT INTO test SELECT 2 FROM generate_series(0, 4000);\n+\n+query I\n+DELETE FROM test WHERE i=1;\n+----\n+4002\n+\n+restart\n+\n+query IIII\n+SELECT COUNT(*), SUM(i), MIN(i), MAX(i) FROM test;\n+----\n+4915\t1068801\t2\t1999\n+\n+query I\n+DELETE FROM test WHERE i=2;\n+----\n+4001\n+\n+restart\n+\n+query IIII\n+SELECT COUNT(*), SUM(i), MIN(i), MAX(i) FROM test;\n+----\n+914\t1060799\t5\t1999\n+\n+query I\n+DELETE FROM test;\n+----\n+914\n+\n+restart\n+\n+query IIII\n+SELECT COUNT(*), SUM(i), MIN(i), MAX(i) FROM test;\n+----\n+0\tNULL\tNULL\tNULL\ndiff --git a/test/sql/storage/store_deletes_large.test_slow b/test/sql/storage/delete/store_deletes_large.test_slow\nsimilarity index 87%\nrename from test/sql/storage/store_deletes_large.test_slow\nrename to test/sql/storage/delete/store_deletes_large.test_slow\nindex 92435ea9afb5..03a196cfdce3 100644\n--- a/test/sql/storage/store_deletes_large.test_slow\n+++ b/test/sql/storage/delete/store_deletes_large.test_slow\n@@ -1,8 +1,6 @@\n-# name: test/sql/storage/store_deletes_large.test_slow\n+# name: test/sql/storage/delete/store_deletes_large.test_slow\n # description: Test storing large deletes\n-# group: [storage]\n-\n-mode skip\n+# group: [delete]\n \n # load the DB from disk\n load __TEST_DIR__/test_store_deletes.db\n@@ -53,4 +51,4 @@ DELETE FROM test WHERE a%2=0\n query I\n SELECT COUNT(*) FROM test\n ----\n-500000\n\\ No newline at end of file\n+500000\ndiff --git a/test/sql/storage/test_store_deletes.test b/test/sql/storage/delete/test_store_deletes.test\nsimilarity index 92%\nrename from test/sql/storage/test_store_deletes.test\nrename to test/sql/storage/delete/test_store_deletes.test\nindex ad0011495965..60126582d325 100644\n--- a/test/sql/storage/test_store_deletes.test\n+++ b/test/sql/storage/delete/test_store_deletes.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/storage/test_store_deletes.test\n+# name: test/sql/storage/delete/test_store_deletes.test\n # description: Test deletes with storage\n-# group: [storage]\n+# group: [delete]\n \n # load the DB from disk\n load __TEST_DIR__/test_store_deletes.db\ndiff --git a/test/sql/storage/delete/test_unchanged_deletes.test b/test/sql/storage/delete/test_unchanged_deletes.test\nnew file mode 100644\nindex 000000000000..7e4dcb93e4e1\n--- /dev/null\n+++ b/test/sql/storage/delete/test_unchanged_deletes.test\n@@ -0,0 +1,37 @@\n+# name: test/sql/storage/delete/test_unchanged_deletes.test\n+# description: Test reloading unchanged deletes\n+# group: [delete]\n+\n+# load the DB from disk\n+load __TEST_DIR__/test_unchanged_deletes.db\n+\n+statement ok\n+CREATE TABLE integers AS FROM range(4) t(i)\n+\n+query I\n+DELETE FROM integers WHERE i%2=0\n+----\n+2\n+\n+query I\n+SELECT COUNT(*) FROM integers\n+----\n+2\n+\n+statement ok\n+CREATE TABLE integers2(i int);\n+\n+# repeatedly add elements and checkpoint\n+loop i 0 10\n+\n+query I\n+SELECT COUNT(*) FROM integers\n+----\n+2\n+\n+statement ok\n+INSERT INTO integers2 VALUES (${i});\n+\n+restart\n+\n+endloop\ndiff --git a/test/sql/storage/delete/test_unchanged_deletes_large.test b/test/sql/storage/delete/test_unchanged_deletes_large.test\nnew file mode 100644\nindex 000000000000..6d91b1f722cc\n--- /dev/null\n+++ b/test/sql/storage/delete/test_unchanged_deletes_large.test\n@@ -0,0 +1,75 @@\n+# name: test/sql/storage/delete/test_unchanged_deletes_large.test\n+# description: Test reloading unchanged deletes\n+# group: [delete]\n+\n+# load the DB from disk\n+load __TEST_DIR__/test_unchanged_deletes_large.db\n+\n+statement ok\n+CREATE TABLE integers AS SELECT * FROM generate_series(0,599999) t(i);\n+\n+query I\n+DELETE FROM integers WHERE i%2=0\n+----\n+300000\n+\n+query I\n+SELECT COUNT(*) FROM integers\n+----\n+300000\n+\n+statement ok\n+INSERT INTO integers VALUES (42);\n+\n+query I\n+SELECT COUNT(*) FROM integers\n+----\n+300001\n+\n+restart\n+\n+query I\n+SELECT COUNT(*) FROM integers\n+----\n+300001\n+\n+query I\n+DELETE FROM integers WHERE i%3=0\n+----\n+100001\n+\n+query I\n+SELECT COUNT(*) FROM integers\n+----\n+200000\n+\n+restart\n+\n+query I\n+SELECT COUNT(*) FROM integers\n+----\n+200000\n+\n+loop i 0 5\n+\n+restart\n+\n+statement ok\n+INSERT INTO integers VALUES (84)\n+\n+statement ok\n+checkpoint\n+\n+query I\n+SELECT COUNT(*) - ${i} FROM integers\n+----\n+200001\n+\n+restart\n+\n+query I\n+SELECT COUNT(*) - ${i} FROM integers\n+----\n+200001\n+\n+endloop\ndiff --git a/test/sql/storage/large_updates_deletes_persistent_segment.test_slow b/test/sql/storage/mix/large_updates_deletes_persistent_segment.test_slow\nsimilarity index 95%\nrename from test/sql/storage/large_updates_deletes_persistent_segment.test_slow\nrename to test/sql/storage/mix/large_updates_deletes_persistent_segment.test_slow\nindex b60db278e288..71f959697b85 100644\n--- a/test/sql/storage/large_updates_deletes_persistent_segment.test_slow\n+++ b/test/sql/storage/mix/large_updates_deletes_persistent_segment.test_slow\n@@ -1,6 +1,6 @@\n-# name: test/sql/storage/large_updates_deletes_persistent_segment.test_slow\n+# name: test/sql/storage/mix/large_updates_deletes_persistent_segment.test_slow\n # description: Test large updates/deletes/insertions on persistent segments\n-# group: [storage]\n+# group: [mix]\n \n # load the DB from disk\n load __TEST_DIR__/updates_deletes_persistent_segments.db\ndiff --git a/test/sql/storage/test_update_delete_mix.test_slow b/test/sql/storage/mix/test_update_delete_mix.test_slow\nsimilarity index 87%\nrename from test/sql/storage/test_update_delete_mix.test_slow\nrename to test/sql/storage/mix/test_update_delete_mix.test_slow\nindex 3a2cc0c8758e..0a91743367e6 100644\n--- a/test/sql/storage/test_update_delete_mix.test_slow\n+++ b/test/sql/storage/mix/test_update_delete_mix.test_slow\n@@ -1,6 +1,6 @@\n-# name: test/sql/storage/test_update_delete_mix.test_slow\n+# name: test/sql/storage/mix/test_update_delete_mix.test_slow\n # description: Test mix of updates and deletes with storage\n-# group: [storage]\n+# group: [mix]\n \n # load the DB from disk\n load __TEST_DIR__/test_mix.db\ndiff --git a/test/sql/storage/test_update_delete_string.test b/test/sql/storage/mix/test_update_delete_string.test\nsimilarity index 95%\nrename from test/sql/storage/test_update_delete_string.test\nrename to test/sql/storage/mix/test_update_delete_string.test\nindex a0c9a6f5fc9a..1c6c055e1faa 100644\n--- a/test/sql/storage/test_update_delete_string.test\n+++ b/test/sql/storage/mix/test_update_delete_string.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/storage/test_update_delete_string.test\n+# name: test/sql/storage/mix/test_update_delete_string.test\n # description: Test updates/deletes and strings\n-# group: [storage]\n+# group: [mix]\n \n # load the DB from disk\n load __TEST_DIR__/test_string_update.db\ndiff --git a/test/sql/storage/updates_deletes_big_table.test b/test/sql/storage/mix/updates_deletes_big_table.test\nsimilarity index 92%\nrename from test/sql/storage/updates_deletes_big_table.test\nrename to test/sql/storage/mix/updates_deletes_big_table.test\nindex 788572916791..dbac332d2a62 100644\n--- a/test/sql/storage/updates_deletes_big_table.test\n+++ b/test/sql/storage/mix/updates_deletes_big_table.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/storage/updates_deletes_big_table.test\n+# name: test/sql/storage/mix/updates_deletes_big_table.test\n # description: Test update/deletes on big table\n-# group: [storage]\n+# group: [mix]\n \n # load the DB from disk\n load __TEST_DIR__/updates_deletes_big_table.db\ndiff --git a/test/sql/storage/updates_deletes_persistent_segments.test b/test/sql/storage/mix/updates_deletes_persistent_segments.test\nsimilarity index 94%\nrename from test/sql/storage/updates_deletes_persistent_segments.test\nrename to test/sql/storage/mix/updates_deletes_persistent_segments.test\nindex 38fcaa979330..597886f22bd9 100644\n--- a/test/sql/storage/updates_deletes_persistent_segments.test\n+++ b/test/sql/storage/mix/updates_deletes_persistent_segments.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/storage/updates_deletes_persistent_segments.test\n+# name: test/sql/storage/mix/updates_deletes_persistent_segments.test\n # description: Test updates/deletes/insertions on persistent segments\n-# group: [storage]\n+# group: [mix]\n \n # load the DB from disk\n load __TEST_DIR__/updates_deletes_persistent_segments.db\ndiff --git a/test/sql/storage/parallel/batch_insert_filtered_row_groups.test_slow b/test/sql/storage/parallel/batch_insert_filtered_row_groups.test_slow\nindex beb49110270e..8f283107a255 100644\n--- a/test/sql/storage/parallel/batch_insert_filtered_row_groups.test_slow\n+++ b/test/sql/storage/parallel/batch_insert_filtered_row_groups.test_slow\n@@ -2,6 +2,8 @@\n # description: Test batch insert with small batches\n # group: [parallel]\n \n+require vector_size 512\n+\n require parquet\n \n load __TEST_DIR__/insert_mix_batches.db\ndiff --git a/test/sql/storage/parallel/insert_many_compressible_batches.test_slow b/test/sql/storage/parallel/insert_many_compressible_batches.test_slow\nindex 43bc3f7a0cd8..09b500e41d12 100644\n--- a/test/sql/storage/parallel/insert_many_compressible_batches.test_slow\n+++ b/test/sql/storage/parallel/insert_many_compressible_batches.test_slow\n@@ -2,6 +2,8 @@\n # description: Test writing many compressible batches\n # group: [parallel]\n \n+require vector_size 512\n+\n load __TEST_DIR__/insert_many_compressible_batches.db\n \n require parquet\ndiff --git a/test/sql/storage/parallel/insert_order_preserving_odd_sized_batches.test_slow b/test/sql/storage/parallel/insert_order_preserving_odd_sized_batches.test_slow\nindex f361725e5e47..bc1a2e8a864e 100644\n--- a/test/sql/storage/parallel/insert_order_preserving_odd_sized_batches.test_slow\n+++ b/test/sql/storage/parallel/insert_order_preserving_odd_sized_batches.test_slow\n@@ -2,6 +2,8 @@\n # description: Test parallel order-preserving insert\n # group: [parallel]\n \n+require vector_size 512\n+\n load __TEST_DIR__/insert_odd_sized_batches.db\n \n require parquet\ndiff --git a/test/sql/storage/test_reclaim_space_alter_type.test_slow b/test/sql/storage/reclaim_space/test_reclaim_space_alter_type.test_slow\nsimilarity index 89%\nrename from test/sql/storage/test_reclaim_space_alter_type.test_slow\nrename to test/sql/storage/reclaim_space/test_reclaim_space_alter_type.test_slow\nindex f2a1c4b659b0..c0e894a8129c 100644\n--- a/test/sql/storage/test_reclaim_space_alter_type.test_slow\n+++ b/test/sql/storage/reclaim_space/test_reclaim_space_alter_type.test_slow\n@@ -1,6 +1,6 @@\n-# name: test/sql/storage/test_reclaim_space_alter_type.test_slow\n+# name: test/sql/storage/reclaim_space/test_reclaim_space_alter_type.test_slow\n # description: Test that we reclaim space when altering the type of a column\n-# group: [storage]\n+# group: [reclaim_space]\n \n load __TEST_DIR__/test_reclaim_space.db\n \ndiff --git a/test/sql/storage/test_reclaim_space_drop.test_slow b/test/sql/storage/reclaim_space/test_reclaim_space_drop.test_slow\nsimilarity index 88%\nrename from test/sql/storage/test_reclaim_space_drop.test_slow\nrename to test/sql/storage/reclaim_space/test_reclaim_space_drop.test_slow\nindex 4f9af74f1400..81dc2326c948 100644\n--- a/test/sql/storage/test_reclaim_space_drop.test_slow\n+++ b/test/sql/storage/reclaim_space/test_reclaim_space_drop.test_slow\n@@ -1,6 +1,6 @@\n-# name: test/sql/storage/test_reclaim_space_drop.test_slow\n+# name: test/sql/storage/reclaim_space/test_reclaim_space_drop.test_slow\n # description: Test that we reclaim space when dropping tables\n-# group: [storage]\n+# group: [reclaim_space]\n \n load __TEST_DIR__/test_reclaim_space.db\n \ndiff --git a/test/sql/storage/test_reclaim_space_drop_column.test_slow b/test/sql/storage/reclaim_space/test_reclaim_space_drop_column.test_slow\nsimilarity index 88%\nrename from test/sql/storage/test_reclaim_space_drop_column.test_slow\nrename to test/sql/storage/reclaim_space/test_reclaim_space_drop_column.test_slow\nindex 7448f91a4670..2a00ce5eb3dc 100644\n--- a/test/sql/storage/test_reclaim_space_drop_column.test_slow\n+++ b/test/sql/storage/reclaim_space/test_reclaim_space_drop_column.test_slow\n@@ -1,6 +1,6 @@\n-# name: test/sql/storage/test_reclaim_space_drop_column.test_slow\n+# name: test/sql/storage/reclaim_space/test_reclaim_space_drop_column.test_slow\n # description: Test that we reclaim space when dropping and adding columns\n-# group: [storage]\n+# group: [reclaim_space]\n \n load __TEST_DIR__/test_reclaim_space.db\n \ndiff --git a/test/sql/storage/test_reclaim_space_update.test_slow b/test/sql/storage/reclaim_space/test_reclaim_space_update.test_slow\nsimilarity index 89%\nrename from test/sql/storage/test_reclaim_space_update.test_slow\nrename to test/sql/storage/reclaim_space/test_reclaim_space_update.test_slow\nindex 975a6164e4d4..2ff4af8901fe 100644\n--- a/test/sql/storage/test_reclaim_space_update.test_slow\n+++ b/test/sql/storage/reclaim_space/test_reclaim_space_update.test_slow\n@@ -1,6 +1,6 @@\n-# name: test/sql/storage/test_reclaim_space_update.test_slow\n+# name: test/sql/storage/reclaim_space/test_reclaim_space_update.test_slow\n # description: Test that we reclaim space when updating the values of a column\n-# group: [storage]\n+# group: [reclaim_space]\n \n load __TEST_DIR__/test_reclaim_space.db\n \ndiff --git a/test/sql/storage/test_reclaim_space_update_large_string.test b/test/sql/storage/reclaim_space/test_reclaim_space_update_large_string.test\nsimilarity index 89%\nrename from test/sql/storage/test_reclaim_space_update_large_string.test\nrename to test/sql/storage/reclaim_space/test_reclaim_space_update_large_string.test\nindex 914befc1b2a3..d15863f736d1 100644\n--- a/test/sql/storage/test_reclaim_space_update_large_string.test\n+++ b/test/sql/storage/reclaim_space/test_reclaim_space_update_large_string.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/storage/test_reclaim_space_update_large_string.test\n+# name: test/sql/storage/reclaim_space/test_reclaim_space_update_large_string.test\n # description: Test that we reclaim space when updating a large string\n-# group: [storage]\n+# group: [reclaim_space]\n \n load __TEST_DIR__/test_reclaim_space.db\n \ndiff --git a/test/sql/storage/test_buffer_manager.cpp b/test/sql/storage/test_buffer_manager.cpp\nindex 50a02833ff0a..d4cbb110381e 100644\n--- a/test/sql/storage/test_buffer_manager.cpp\n+++ b/test/sql/storage/test_buffer_manager.cpp\n@@ -8,50 +8,6 @@\n using namespace duckdb;\n using namespace std;\n \n-TEST_CASE(\"Test scanning a table and computing an aggregate over a table that exceeds buffer manager size\",\n-          \"[storage][.]\") {\n-\tduckdb::unique_ptr<MaterializedQueryResult> result;\n-\tauto storage_database = TestCreatePath(\"storage_test\");\n-\tauto config = GetTestConfig();\n-\n-\t// set the maximum memory to 10MB and force uncompressed so we actually use the memory\n-\tconfig->options.force_compression = CompressionType::COMPRESSION_UNCOMPRESSED;\n-\tconfig->options.maximum_memory = 10000000;\n-\tconfig->options.maximum_threads = 1;\n-\n-\tint64_t expected_sum;\n-\tValue sum;\n-\t// make sure the database does not exist\n-\tDeleteDatabase(storage_database);\n-\t{\n-\t\t// create a database and insert values\n-\t\tDuckDB db(storage_database, config.get());\n-\t\tConnection con(db);\n-\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test (a INTEGER, b INTEGER);\"));\n-\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test VALUES (11, 22), (13, 22), (12, 21), (NULL, NULL)\"));\n-\t\tuint64_t table_size = 2 * 4 * sizeof(int);\n-\t\tuint64_t desired_size = 10 * config->options.maximum_memory;\n-\t\texpected_sum = 11 + 12 + 13 + 22 + 22 + 21;\n-\t\t// grow the table until it exceeds 100MB\n-\t\twhile (table_size < desired_size) {\n-\t\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test SELECT * FROM test\"));\n-\t\t\ttable_size *= 2;\n-\t\t\texpected_sum *= 2;\n-\t\t}\n-\t\tsum = Value::BIGINT(expected_sum);\n-\t\t// compute the sum\n-\t\tresult = con.Query(\"SELECT SUM(a) + SUM(b) FROM test\");\n-\t\tREQUIRE(CHECK_COLUMN(result, 0, {sum}));\n-\t}\n-\tfor (idx_t i = 0; i < 2; i++) {\n-\t\tDuckDB db(storage_database, config.get());\n-\t\tConnection con(db);\n-\t\tresult = con.Query(\"SELECT SUM(a) + SUM(b) FROM test\");\n-\t\tREQUIRE(CHECK_COLUMN(result, 0, {sum}));\n-\t}\n-\tDeleteDatabase(storage_database);\n-}\n-\n TEST_CASE(\"Test storing a big string that exceeds buffer manager size\", \"[storage][.]\") {\n \tduckdb::unique_ptr<MaterializedQueryResult> result;\n \tauto storage_database = TestCreatePath(\"storage_test\");\ndiff --git a/test/sql/storage/large_repeated_updates.test_slow b/test/sql/storage/update/large_repeated_updates.test_slow\nsimilarity index 94%\nrename from test/sql/storage/large_repeated_updates.test_slow\nrename to test/sql/storage/update/large_repeated_updates.test_slow\nindex 20c883f65890..f4162d04489b 100644\n--- a/test/sql/storage/large_repeated_updates.test_slow\n+++ b/test/sql/storage/update/large_repeated_updates.test_slow\n@@ -1,6 +1,6 @@\n-# name: test/sql/storage/large_repeated_updates.test_slow\n+# name: test/sql/storage/update/large_repeated_updates.test_slow\n # description: Test repeated updates on big table\n-# group: [storage]\n+# group: [update]\n \n # load the DB from disk\n load __TEST_DIR__/updates_large_repeated.db\ndiff --git a/test/sql/storage/test_store_null_updates.test b/test/sql/storage/update/test_store_null_updates.test\nsimilarity index 89%\nrename from test/sql/storage/test_store_null_updates.test\nrename to test/sql/storage/update/test_store_null_updates.test\nindex b27f5e977444..70dae522fb37 100644\n--- a/test/sql/storage/test_store_null_updates.test\n+++ b/test/sql/storage/update/test_store_null_updates.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/storage/test_store_null_updates.test\n+# name: test/sql/storage/update/test_store_null_updates.test\n # description: Test updates with storage and null values\n-# group: [storage]\n+# group: [update]\n \n # load the DB from disk\n load __TEST_DIR__/test_store_updates.db\ndiff --git a/test/sql/storage/test_store_updates.test b/test/sql/storage/update/test_store_updates.test\nsimilarity index 87%\nrename from test/sql/storage/test_store_updates.test\nrename to test/sql/storage/update/test_store_updates.test\nindex 67f9acc738e4..5dcd140f3236 100644\n--- a/test/sql/storage/test_store_updates.test\n+++ b/test/sql/storage/update/test_store_updates.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/storage/test_store_updates.test\n+# name: test/sql/storage/update/test_store_updates.test\n # description: Test updates with storage\n-# group: [storage]\n+# group: [update]\n \n # load the DB from disk\n load __TEST_DIR__/test_store_updates.db\ndiff --git a/test/sql/storage_version/storage_version.db b/test/sql/storage_version/storage_version.db\nindex c78e89831957..87056483c68f 100644\nBinary files a/test/sql/storage_version/storage_version.db and b/test/sql/storage_version/storage_version.db differ\n",
  "problem_statement": "Insert into table fails with `INTERNAL Error: Cannot perform IO in in-memory database - CreateBlock!`\n### What happens?\r\n\r\n1. I opened duckdb cli and opened it on a persistent database. \r\n2. I created a table with a parquet file. \r\n`create or replace table test as select * from read_parquet('2023_06_07_1686176284392_0.parquet');`\r\n3. I added more data into the table from some more parquet files using \r\n`insert into  test select * from read_parquet('*.parquet');`\r\n4. The insert command fails with error :\r\n`Error: TransactionContext Error: Failed to commit: INTERNAL Error: Cannot perform IO in in-memory database - CreateBlock!`\r\n\r\nIngesting all files in one table with `create or replace table test as select * from read_parquet('*.parquet');` succeeds as well ingesting all the files individually.\r\n\r\n### To Reproduce\r\n\r\nRunning following commands should reproduce the issue:\r\n\r\n```sql\r\n.open file.db\r\nCREATE OR REPLACE TABLE test AS SELECT * FROM read_parquet('2023_06_07_1686176284392_0.parquet');\r\nINSERT INTO test SELECT * FROM read_parquet('*.parquet');\r\n```\r\n```console\r\nError: TransactionContext Error: Failed to commit: INTERNAL Error: Cannot perform IO in in-memory database - CreateBlock!\r\n```\r\nAttaching parquet files as well : \r\n[data.zip](https://github.com/duckdb/duckdb/files/11704459/data.zip)\r\n\r\n\r\n### OS:\r\n\r\nmacOS 13.4\r\n\r\n### DuckDB Version:\r\n\r\nv0.8 and latest as well\r\n\r\n### DuckDB Client:\r\n\r\nCLI\r\n\r\n### Full Name:\r\n\r\nAnshul Khandelwal\r\n\r\n### Affiliation:\r\n\r\nRill data\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\n- [X] I agree\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] I agree\n",
  "hints_text": "",
  "created_at": "2023-09-10T21:07:16Z"
}