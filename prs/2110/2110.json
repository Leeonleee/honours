{
  "repo": "duckdb/duckdb",
  "pull_number": 2110,
  "instance_id": "duckdb__duckdb-2110",
  "issue_numbers": [
    "2102"
  ],
  "base_commit": "979a798219262349e8c2c3d52ef203a04d6ee728",
  "patch": "diff --git a/src/execution/operator/join/physical_hash_join.cpp b/src/execution/operator/join/physical_hash_join.cpp\nindex dd4a6bf14c01..d9d7b7f2ea0f 100644\n--- a/src/execution/operator/join/physical_hash_join.cpp\n+++ b/src/execution/operator/join/physical_hash_join.cpp\n@@ -10,6 +10,8 @@\n \n namespace duckdb {\n \n+bool CanCacheType(const LogicalType &type);\n+\n PhysicalHashJoin::PhysicalHashJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left,\n                                    unique_ptr<PhysicalOperator> right, vector<JoinCondition> cond, JoinType join_type,\n                                    const vector<idx_t> &left_projection_map,\n@@ -29,6 +31,13 @@ PhysicalHashJoin::PhysicalHashJoin(LogicalOperator &op, unique_ptr<PhysicalOpera\n \tif (join_type != JoinType::ANTI && join_type != JoinType::SEMI && join_type != JoinType::MARK) {\n \t\tbuild_types = LogicalOperator::MapTypes(children[1]->GetTypes(), right_projection_map);\n \t}\n+\t// we avoid caching lists, since lists can be very large caching can have very negative effects\n+\tcan_cache = true;\n+\tfor (auto &type : types) {\n+\t\tif (!CanCacheType(type)) {\n+\t\t\tcan_cache = false;\n+\t\t}\n+\t}\n }\n \n PhysicalHashJoin::PhysicalHashJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left,\n@@ -169,6 +178,25 @@ class PhysicalHashJoinState : public PhysicalOperatorState {\n \tunique_ptr<JoinHashTable::ScanStructure> scan_structure;\n };\n \n+bool CanCacheType(const LogicalType &type) {\n+\tswitch (type.id()) {\n+\tcase LogicalTypeId::LIST:\n+\tcase LogicalTypeId::MAP:\n+\t\treturn false;\n+\tcase LogicalTypeId::STRUCT: {\n+\t\tauto &entries = StructType::GetChildTypes(type);\n+\t\tfor (auto &entry : entries) {\n+\t\t\tif (!CanCacheType(entry.second)) {\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\t\treturn true;\n+\t}\n+\tdefault:\n+\t\treturn true;\n+\t}\n+}\n+\n unique_ptr<PhysicalOperatorState> PhysicalHashJoin::GetOperatorState() {\n \tauto state = make_unique<PhysicalHashJoinState>(*this, children[0].get(), children[1].get(), conditions);\n \tstate->cached_chunk.Initialize(types);\n@@ -197,8 +225,8 @@ void PhysicalHashJoin::GetChunkInternal(ExecutionContext &context, DataChunk &ch\n #if STANDARD_VECTOR_SIZE >= 128\n \t\t\tif (state->cached_chunk.size() > 0) {\n \t\t\t\t// finished probing but cached data remains, return cached chunk\n-\t\t\t\tchunk.Reference(state->cached_chunk);\n-\t\t\t\tstate->cached_chunk.SetCardinality(0);\n+\t\t\t\tchunk.Move(state->cached_chunk);\n+\t\t\t\tstate->cached_chunk.Initialize(types);\n \t\t\t} else\n #endif\n \t\t\t    if (IsRightOuterJoin(join_type)) {\n@@ -208,13 +236,13 @@ void PhysicalHashJoin::GetChunkInternal(ExecutionContext &context, DataChunk &ch\n \t\t\treturn;\n \t\t} else {\n #if STANDARD_VECTOR_SIZE >= 128\n-\t\t\tif (chunk.size() < 64) {\n+\t\t\tif (can_cache && chunk.size() < 64) {\n \t\t\t\t// small chunk: add it to chunk cache and continue\n \t\t\t\tstate->cached_chunk.Append(chunk);\n \t\t\t\tif (state->cached_chunk.size() >= (STANDARD_VECTOR_SIZE - 64)) {\n \t\t\t\t\t// chunk cache full: return it\n-\t\t\t\t\tchunk.Reference(state->cached_chunk);\n-\t\t\t\t\tstate->cached_chunk.SetCardinality(0);\n+\t\t\t\t\tchunk.Move(state->cached_chunk);\n+\t\t\t\t\tstate->cached_chunk.Initialize(types);\n \t\t\t\t\treturn;\n \t\t\t\t} else {\n \t\t\t\t\t// chunk cache not full: probe again\ndiff --git a/src/include/duckdb/execution/operator/join/physical_hash_join.hpp b/src/include/duckdb/execution/operator/join/physical_hash_join.hpp\nindex 37ffc13f0488..908fde7d42a3 100644\n--- a/src/include/duckdb/execution/operator/join/physical_hash_join.hpp\n+++ b/src/include/duckdb/execution/operator/join/physical_hash_join.hpp\n@@ -34,6 +34,8 @@ class PhysicalHashJoin : public PhysicalComparisonJoin {\n \tvector<LogicalType> build_types;\n \t//! Duplicate eliminated types; only used for delim_joins (i.e. correlated subqueries)\n \tvector<LogicalType> delim_types;\n+\t//! Whether or not we can cache the chunk\n+\tbool can_cache;\n \n public:\n \tunique_ptr<GlobalOperatorState> GetGlobalState(ClientContext &context) override;\n",
  "test_patch": "diff --git a/test/sql/copy/parquet/parquet_2102.test_slow b/test/sql/copy/parquet/parquet_2102.test_slow\nnew file mode 100644\nindex 000000000000..fb9f11b18f7f\n--- /dev/null\n+++ b/test/sql/copy/parquet/parquet_2102.test_slow\n@@ -0,0 +1,69 @@\n+# name: test/sql/copy/parquet/parquet_2102.test_slow\n+# description: Missing Column Data After Adding Left Join To Query in DuckDB Version 0.2.8\n+# group: [parquet]\n+\n+require parquet\n+\n+require httpfs\n+\n+statement ok\n+CREATE TABLE view_one AS SELECT * FROM 'https://github.com/cwida/duckdb-data/releases/download/v1.0/issue2102_one.parquet';\n+\n+statement ok\n+CREATE TABLE view_two AS SELECT * FROM 'https://github.com/cwida/duckdb-data/releases/download/v1.0/issue2102_two.parquet';\n+\n+query I\n+SELECT COUNT(*) FROM view_one WHERE date IS NULL\n+----\n+6219\n+\n+statement ok\n+CREATE TABLE tbl1 AS SELECT one.id id, one.date date\n+FROM\n+\tview_one AS one\n+JOIN\n+\tview_two two ON two.id = one.id AND two.line = 1;\n+\n+query I\n+SELECT COUNT(*) FROM tbl1\n+----\n+691951\n+\n+query I\n+SELECT COUNT(*) FROM tbl1 WHERE date IS NULL\n+----\n+4742\n+\n+statement ok\n+CREATE TABLE tbl2 AS SELECT one.id id, one.date date\n+FROM\n+\tview_one AS one\n+LEFT JOIN\n+\tview_two two ON two.id = one.id AND two.line = 1;\n+\n+query I\n+SELECT COUNT(*) FROM tbl2\n+----\n+695434\n+\n+query I\n+SELECT COUNT(*) FROM tbl2 WHERE date IS NULL\n+----\n+6219\n+\n+statement ok\n+CREATE TABLE tbl3 AS SELECT one.id id, one.date date\n+FROM\n+\tview_one AS one\n+LEFT JOIN\n+\tview_two two ON two.id = one.id;\n+\n+query I\n+SELECT COUNT(*) FROM tbl3\n+----\n+768666\n+\n+query I\n+SELECT COUNT(*) FROM tbl3 WHERE date IS NULL\n+----\n+7124\n",
  "problem_statement": "Missing Column Data After Adding Left Join To Query in DuckDB Version 0.2.8\nAfter upgrading to DuckDB version 0.2.8, column data is missing after adding a LEFT JOIN to a query.  This behavior is not seen in DuckDB version 0.2.7.\r\n\r\nIf you run the following query (queryOne) on the sample set in the attachment, there are 6219 rows which have a null value for the date field.  This is the expected value.  This is seen in both DuckDB versions 0.2.7 and 0.2.8.\r\n\r\n```\r\nSELECT one.id AS id,\r\n    one.status AS schedule_status,\r\n    one.date AS date\r\nFROM \r\n    view_one AS one\r\n```\r\n\r\nIf you run a second query (queryTwo) that includes a left join, it is expected that there will be the same 6219 rows which have a null value for the date field.  When the second query is run on the sample data set in the attachment, DuckDB version 0.2.8 has 6260 rows with a null date field instead of 6219 rows.  DuckDB version 0.2.7 will return the correct values for the date field and will have 6219 rows with a null date field.\r\n\r\n```\r\nSELECT one.id AS id,\r\n    one.status AS schedule_status,\r\n    one.date AS date\r\nFROM \r\n    view_one AS one\r\nLEFT JOIN\r\n    view_two two ON two.id = one.id AND two.line = 1\r\n```\r\n\r\nI tested the sample Python script using Python version 3.9.5.\r\n\r\n[DuckDB Missing Data in Query for 0.2.8.zip](https://github.com/duckdb/duckdb/files/6941094/DuckDB.Missing.Data.in.Query.for.0.2.8.zip)\r\n\n",
  "hints_text": "Thanks for the report! I can confirm the issue. I will have a look.",
  "created_at": "2021-08-07T09:36:31Z"
}