You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Segfault when importing data from DataFrame
#### What happens?
When copying a DataFrame with string attributes I get a segmentation fault error.
If I transform the string attribute into a category then the program works fine. 

#### To Reproduce
Results in a segmentation fault:
```Python3
import duckdb
import pandas as pd

N = 1_000_000

# Create DataFrame without category attribute
df = pd.DataFrame({"city": ["Amsterdam", "New York", "London"] * N})
# Copy Dataframe to DuckDB
con = duckdb.connect()
con.register("df", df)
con.execute(f"""
    CREATE TABLE t1 AS SELECT * FROM df
"""
)
```

```sh
[1]    2369 segmentation fault  python3 segfault.py
```

Works perfectly fine:
```Python3
import duckdb
import pandas as pd

N = 1_000_000

# Create DataFrame without category attribute
df = pd.DataFrame({"city": ["Amsterdam", "New York", "London"] * N})
df['city'] = df['city'].astype('category') # This is the only change
# Copy Dataframe to DuckDB
con = duckdb.connect()
con.register("df", df)
con.execute(f"""
    CREATE TABLE t1 AS SELECT * FROM df
"""
)
```


#### Environment:
- OS: Mac BigSur 11.4
- DuckDB Version: '0.3.1-dev287'
- DuckDB Client: Python3

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16: </p>
17: 
18: ## DuckDB
19: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/docs/why_duckdb.html).
20: 
21: ## Installation
22: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
23: 
24: ## Data Import
25: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
26: 
27: ```sql
28: SELECT * FROM 'myfile.csv';
29: SELECT * FROM 'myfile.parquet';
30: ```
31: 
32: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
33: 
34: ## SQL Reference
35: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
36: 
37: ## Development
38: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
39: 
40: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
41: 
42: 
[end of README.md]
[start of tools/pythonpkg/src/vector_conversion.cpp]
1: #include "duckdb_python/vector_conversion.hpp"
2: #include "duckdb/common/string_util.hpp"
3: #include "duckdb/common/types/timestamp.hpp"
4: #include "utf8proc_wrapper.hpp"
5: 
6: namespace duckdb {
7: 
8: template <class T>
9: void ScanPandasColumn(py::array &numpy_col, idx_t stride, idx_t offset, Vector &out, idx_t count) {
10: 	auto src_ptr = (T *)numpy_col.data();
11: 	if (stride == sizeof(T)) {
12: 		FlatVector::SetData(out, (data_ptr_t)(src_ptr + offset));
13: 	} else {
14: 		auto tgt_ptr = (T *)FlatVector::GetData(out);
15: 		for (idx_t i = 0; i < count; i++) {
16: 			tgt_ptr[i] = src_ptr[stride / sizeof(T) * (i + offset)];
17: 		}
18: 	}
19: }
20: 
21: template <class T, class V>
22: void ScanPandasCategoryTemplated(py::array &column, idx_t offset, Vector &out, idx_t count) {
23: 	auto src_ptr = (T *)column.data();
24: 	auto tgt_ptr = (V *)FlatVector::GetData(out);
25: 	auto &tgt_mask = FlatVector::Validity(out);
26: 	for (idx_t i = 0; i < count; i++) {
27: 		if (src_ptr[i + offset] == -1) {
28: 			// Null value
29: 			tgt_mask.SetInvalid(i);
30: 		} else {
31: 			tgt_ptr[i] = src_ptr[i + offset];
32: 		}
33: 	}
34: }
35: 
36: template <class T>
37: void ScanPandasCategory(py::array &column, idx_t count, idx_t offset, Vector &out, string &src_type) {
38: 	if (src_type == "int8") {
39: 		ScanPandasCategoryTemplated<int8_t, T>(column, offset, out, count);
40: 	} else if (src_type == "int16") {
41: 		ScanPandasCategoryTemplated<int16_t, T>(column, offset, out, count);
42: 	} else if (src_type == "int32") {
43: 		ScanPandasCategoryTemplated<int32_t, T>(column, offset, out, count);
44: 	} else {
45: 		throw NotImplementedException("The Pandas type " + src_type + " for categorical types is not implemented yet");
46: 	}
47: }
48: 
49: template <class T>
50: void ScanPandasNumeric(PandasColumnBindData &bind_data, idx_t count, idx_t offset, Vector &out) {
51: 	ScanPandasColumn<T>(bind_data.numpy_col, bind_data.numpy_stride, offset, out, count);
52: 	auto &result_mask = FlatVector::Validity(out);
53: 	if (bind_data.mask) {
54: 		auto mask = (bool *)bind_data.mask->numpy_array.data();
55: 		for (idx_t i = 0; i < count; i++) {
56: 			auto is_null = mask[offset + i];
57: 			if (is_null) {
58: 				result_mask.SetInvalid(i);
59: 			}
60: 		}
61: 	}
62: }
63: 
64: template <class T>
65: bool ValueIsNull(T value) {
66: 	throw std::runtime_error("unsupported type for ValueIsNull");
67: }
68: 
69: template <>
70: bool ValueIsNull(float value) {
71: 	return !Value::FloatIsValid(value);
72: }
73: 
74: template <>
75: bool ValueIsNull(double value) {
76: 	return !Value::DoubleIsValid(value);
77: }
78: 
79: template <class T>
80: void ScanPandasFpColumn(T *src_ptr, idx_t count, idx_t offset, Vector &out) {
81: 	FlatVector::SetData(out, (data_ptr_t)(src_ptr + offset));
82: 	auto tgt_ptr = FlatVector::GetData<T>(out);
83: 	auto &mask = FlatVector::Validity(out);
84: 	for (idx_t i = 0; i < count; i++) {
85: 		if (ValueIsNull(tgt_ptr[i])) {
86: 			mask.SetInvalid(i);
87: 		}
88: 	}
89: }
90: 
91: template <class T>
92: static string_t DecodePythonUnicode(T *codepoints, idx_t codepoint_count, Vector &out) {
93: 	// first figure out how many bytes to allocate
94: 	idx_t utf8_length = 0;
95: 	for (idx_t i = 0; i < codepoint_count; i++) {
96: 		int len = Utf8Proc::CodepointLength(int(codepoints[i]));
97: 		D_ASSERT(len >= 1);
98: 		utf8_length += len;
99: 	}
100: 	int sz;
101: 	auto result = StringVector::EmptyString(out, utf8_length);
102: 	auto target = result.GetDataWriteable();
103: 	for (idx_t i = 0; i < codepoint_count; i++) {
104: 		Utf8Proc::CodepointToUtf8(int(codepoints[i]), sz, target);
105: 		D_ASSERT(sz >= 1);
106: 		target += sz;
107: 	}
108: 	result.Finalize();
109: 	return result;
110: }
111: 
112: void VectorConversion::NumpyToDuckDB(PandasColumnBindData &bind_data, py::array &numpy_col, idx_t count, idx_t offset,
113:                                      Vector &out) {
114: 	switch (bind_data.pandas_type) {
115: 	case PandasType::BOOLEAN:
116: 		ScanPandasColumn<bool>(numpy_col, bind_data.numpy_stride, offset, out, count);
117: 		break;
118: 	case PandasType::UTINYINT:
119: 		ScanPandasNumeric<uint8_t>(bind_data, count, offset, out);
120: 		break;
121: 	case PandasType::USMALLINT:
122: 		ScanPandasNumeric<uint16_t>(bind_data, count, offset, out);
123: 		break;
124: 	case PandasType::UINTEGER:
125: 		ScanPandasNumeric<uint32_t>(bind_data, count, offset, out);
126: 		break;
127: 	case PandasType::UBIGINT:
128: 		ScanPandasNumeric<uint64_t>(bind_data, count, offset, out);
129: 		break;
130: 	case PandasType::TINYINT:
131: 		ScanPandasNumeric<int8_t>(bind_data, count, offset, out);
132: 		break;
133: 	case PandasType::SMALLINT:
134: 		ScanPandasNumeric<int16_t>(bind_data, count, offset, out);
135: 		break;
136: 	case PandasType::INTEGER:
137: 		ScanPandasNumeric<int32_t>(bind_data, count, offset, out);
138: 		break;
139: 	case PandasType::BIGINT:
140: 		ScanPandasNumeric<int64_t>(bind_data, count, offset, out);
141: 		break;
142: 	case PandasType::FLOAT:
143: 		ScanPandasFpColumn<float>((float *)numpy_col.data(), count, offset, out);
144: 		break;
145: 	case PandasType::DOUBLE:
146: 		ScanPandasFpColumn<double>((double *)numpy_col.data(), count, offset, out);
147: 		break;
148: 	case PandasType::TIMESTAMP: {
149: 		auto src_ptr = (int64_t *)numpy_col.data();
150: 		auto tgt_ptr = FlatVector::GetData<timestamp_t>(out);
151: 		auto &mask = FlatVector::Validity(out);
152: 
153: 		for (idx_t row = 0; row < count; row++) {
154: 			auto source_idx = offset + row;
155: 			if (src_ptr[source_idx] <= NumericLimits<int64_t>::Minimum()) {
156: 				// pandas Not a Time (NaT)
157: 				mask.SetInvalid(row);
158: 				continue;
159: 			}
160: 			tgt_ptr[row] = Timestamp::FromEpochNanoSeconds(src_ptr[source_idx]);
161: 		}
162: 		break;
163: 	}
164: 	case PandasType::INTERVAL: {
165: 		auto src_ptr = (int64_t *)numpy_col.data();
166: 		auto tgt_ptr = FlatVector::GetData<interval_t>(out);
167: 		auto &mask = FlatVector::Validity(out);
168: 
169: 		for (idx_t row = 0; row < count; row++) {
170: 			auto source_idx = offset + row;
171: 			if (src_ptr[source_idx] <= NumericLimits<int64_t>::Minimum()) {
172: 				// pandas Not a Time (NaT)
173: 				mask.SetInvalid(row);
174: 				continue;
175: 			}
176: 			int64_t micro = src_ptr[source_idx] / 1000;
177: 			int64_t days = micro / Interval::MICROS_PER_DAY;
178: 			micro = micro % Interval::MICROS_PER_DAY;
179: 			int64_t months = days / Interval::DAYS_PER_MONTH;
180: 			days = days % Interval::DAYS_PER_MONTH;
181: 			interval_t interval;
182: 			interval.months = months;
183: 			interval.days = days;
184: 			interval.micros = micro;
185: 			tgt_ptr[row] = interval;
186: 		}
187: 		break;
188: 	}
189: 	case PandasType::VARCHAR:
190: 	case PandasType::OBJECT: {
191: 		auto src_ptr = (PyObject **)numpy_col.data();
192: 		auto tgt_ptr = FlatVector::GetData<string_t>(out);
193: 		auto &out_mask = FlatVector::Validity(out);
194: 		for (idx_t row = 0; row < count; row++) {
195: 			auto source_idx = offset + row;
196: 			py::str str_val;
197: 			PyObject *val = src_ptr[source_idx];
198: 			if (bind_data.pandas_type == PandasType::OBJECT && !PyUnicode_CheckExact(val)) {
199: 				if (val == Py_None) {
200: 					out_mask.SetInvalid(row);
201: 					continue;
202: 				}
203: 				if (py::isinstance<py::float_>(val) && std::isnan(PyFloat_AsDouble(val))) {
204: 					out_mask.SetInvalid(row);
205: 					continue;
206: 				}
207: 				if (!py::isinstance<py::str>(val)) {
208: 					py::gil_scoped_acquire acquire;
209: 					py::handle object_handle = val;
210: 					str_val = py::str(object_handle);
211: 					val = str_val.ptr();
212: 				}
213: 			}
214: 			// Python 3 string representation:
215: 			// https://github.com/python/cpython/blob/3a8fdb28794b2f19f6c8464378fb8b46bce1f5f4/Include/cpython/unicodeobject.h#L79
216: 			if (!PyUnicode_CheckExact(val)) {
217: 				out_mask.SetInvalid(row);
218: 				continue;
219: 			}
220: 			if (PyUnicode_IS_COMPACT_ASCII(val)) {
221: 				// ascii string: we can zero copy
222: 				tgt_ptr[row] = string_t((const char *)PyUnicode_DATA(val), PyUnicode_GET_LENGTH(val));
223: 			} else {
224: 				// unicode gunk
225: 				auto ascii_obj = (PyASCIIObject *)val;
226: 				auto unicode_obj = (PyCompactUnicodeObject *)val;
227: 				// compact unicode string: is there utf8 data available?
228: 				if (unicode_obj->utf8) {
229: 					// there is! zero copy
230: 					tgt_ptr[row] = string_t((const char *)unicode_obj->utf8, unicode_obj->utf8_length);
231: 				} else if (PyUnicode_IS_COMPACT(unicode_obj) && !PyUnicode_IS_ASCII(unicode_obj)) {
232: 					auto kind = PyUnicode_KIND(val);
233: 					switch (kind) {
234: 					case PyUnicode_1BYTE_KIND:
235: 						tgt_ptr[row] =
236: 						    DecodePythonUnicode<Py_UCS1>(PyUnicode_1BYTE_DATA(val), PyUnicode_GET_LENGTH(val), out);
237: 						break;
238: 					case PyUnicode_2BYTE_KIND:
239: 						tgt_ptr[row] =
240: 						    DecodePythonUnicode<Py_UCS2>(PyUnicode_2BYTE_DATA(val), PyUnicode_GET_LENGTH(val), out);
241: 						break;
242: 					case PyUnicode_4BYTE_KIND:
243: 						tgt_ptr[row] =
244: 						    DecodePythonUnicode<Py_UCS4>(PyUnicode_4BYTE_DATA(val), PyUnicode_GET_LENGTH(val), out);
245: 						break;
246: 					default:
247: 						throw std::runtime_error("Unsupported typekind for Python Unicode Compact decode");
248: 					}
249: 				} else if (ascii_obj->state.kind == PyUnicode_WCHAR_KIND) {
250: 					throw std::runtime_error("Unsupported: decode not ready legacy string");
251: 				} else if (!PyUnicode_IS_COMPACT(unicode_obj) && ascii_obj->state.kind != PyUnicode_WCHAR_KIND) {
252: 					throw std::runtime_error("Unsupported: decode ready legacy string");
253: 				} else {
254: 					throw std::runtime_error("Unsupported string type: no clue what this string is");
255: 				}
256: 			}
257: 		}
258: 		break;
259: 	}
260: 	case PandasType::CATEGORY: {
261: 		switch (out.GetType().InternalType()) {
262: 		case PhysicalType::UINT8:
263: 			ScanPandasCategory<uint8_t>(numpy_col, count, offset, out, bind_data.internal_categorical_type);
264: 			break;
265: 		case PhysicalType::UINT16:
266: 			ScanPandasCategory<uint16_t>(numpy_col, count, offset, out, bind_data.internal_categorical_type);
267: 			break;
268: 		case PhysicalType::UINT32:
269: 			ScanPandasCategory<uint32_t>(numpy_col, count, offset, out, bind_data.internal_categorical_type);
270: 			break;
271: 		default:
272: 			throw InternalException("Invalid Physical Type for ENUMs");
273: 		}
274: 		break;
275: 	}
276: 
277: 	default:
278: 		throw std::runtime_error("Unsupported type " + out.GetType().ToString());
279: 	}
280: }
281: 
282: static void ConvertPandasType(const string &col_type, LogicalType &duckdb_col_type, PandasType &pandas_type) {
283: 	if (col_type == "bool") {
284: 		duckdb_col_type = LogicalType::BOOLEAN;
285: 		pandas_type = PandasType::BOOLEAN;
286: 	} else if (col_type == "uint8" || col_type == "Uint8") {
287: 		duckdb_col_type = LogicalType::UTINYINT;
288: 		pandas_type = PandasType::UTINYINT;
289: 	} else if (col_type == "uint16" || col_type == "Uint16") {
290: 		duckdb_col_type = LogicalType::USMALLINT;
291: 		pandas_type = PandasType::USMALLINT;
292: 	} else if (col_type == "uint32" || col_type == "Uint32") {
293: 		duckdb_col_type = LogicalType::UINTEGER;
294: 		pandas_type = PandasType::UINTEGER;
295: 	} else if (col_type == "uint64" || col_type == "Uint64") {
296: 		duckdb_col_type = LogicalType::UBIGINT;
297: 		pandas_type = PandasType::UBIGINT;
298: 	} else if (col_type == "int8" || col_type == "Int8") {
299: 		duckdb_col_type = LogicalType::TINYINT;
300: 		pandas_type = PandasType::TINYINT;
301: 	} else if (col_type == "int16" || col_type == "Int16") {
302: 		duckdb_col_type = LogicalType::SMALLINT;
303: 		pandas_type = PandasType::SMALLINT;
304: 	} else if (col_type == "int32" || col_type == "Int32") {
305: 		duckdb_col_type = LogicalType::INTEGER;
306: 		pandas_type = PandasType::INTEGER;
307: 	} else if (col_type == "int64" || col_type == "Int64") {
308: 		duckdb_col_type = LogicalType::BIGINT;
309: 		pandas_type = PandasType::BIGINT;
310: 	} else if (col_type == "float32") {
311: 		duckdb_col_type = LogicalType::FLOAT;
312: 		pandas_type = PandasType::FLOAT;
313: 	} else if (col_type == "float64") {
314: 		duckdb_col_type = LogicalType::DOUBLE;
315: 		pandas_type = PandasType::DOUBLE;
316: 	} else if (col_type == "object") {
317: 		//! this better be castable to strings
318: 		duckdb_col_type = LogicalType::VARCHAR;
319: 		pandas_type = PandasType::OBJECT;
320: 	} else if (col_type == "string") {
321: 		duckdb_col_type = LogicalType::VARCHAR;
322: 		pandas_type = PandasType::VARCHAR;
323: 	} else if (col_type == "timedelta64[ns]") {
324: 		duckdb_col_type = LogicalType::INTERVAL;
325: 		pandas_type = PandasType::INTERVAL;
326: 	} else {
327: 		throw std::runtime_error("unsupported python type " + col_type);
328: 	}
329: }
330: 
331: void VectorConversion::BindPandas(py::handle original_df, vector<PandasColumnBindData> &bind_columns,
332:                                   vector<LogicalType> &return_types, vector<string> &names) {
333: 	// This performs a shallow copy that allows us to rename the dataframe
334: 	auto df = original_df.attr("copy")(false);
335: 	auto df_columns = py::list(df.attr("columns"));
336: 	auto df_types = py::list(df.attr("dtypes"));
337: 	auto get_fun = df.attr("__getitem__");
338: 	// TODO support masked arrays as well
339: 	// TODO support dicts of numpy arrays as well
340: 	if (py::len(df_columns) == 0 || py::len(df_types) == 0 || py::len(df_columns) != py::len(df_types)) {
341: 		throw std::runtime_error("Need a DataFrame with at least one column");
342: 	}
343: 
344: 	// check if names in pandas dataframe are unique
345: 	unordered_map<string, idx_t> pandas_column_names_map;
346: 	py::array column_attributes = df.attr("columns").attr("values");
347: 	for (idx_t col_idx = 0; col_idx < py::len(df_columns); col_idx++) {
348: 		auto column_name_py = py::str(df_columns[col_idx]);
349: 		pandas_column_names_map[column_name_py]++;
350: 		if (pandas_column_names_map[column_name_py] > 1) {
351: 			// If the column name is repeated we start adding _x where x is the repetition number
352: 			string column_name = column_name_py;
353: 			column_name += "_" + to_string(pandas_column_names_map[column_name_py] - 1);
354: 			auto new_column_name_py = py::str(column_name);
355: 			names.emplace_back(new_column_name_py);
356: 			column_attributes[py::cast(col_idx)] = new_column_name_py;
357: 			pandas_column_names_map[new_column_name_py]++;
358: 		} else {
359: 			names.emplace_back(column_name_py);
360: 		}
361: 	}
362: 
363: 	for (idx_t col_idx = 0; col_idx < py::len(df_columns); col_idx++) {
364: 		LogicalType duckdb_col_type;
365: 		PandasColumnBindData bind_data;
366: 		auto col_type = string(py::str(df_types[col_idx]));
367: 		if (col_type == "Int8" || col_type == "Int16" || col_type == "Int32" || col_type == "Int64") {
368: 			// numeric object
369: 			// fetch the internal data and mask array
370: 			bind_data.numpy_col = get_fun(df_columns[col_idx]).attr("array").attr("_data");
371: 			bind_data.mask = make_unique<NumPyArrayWrapper>(get_fun(df_columns[col_idx]).attr("array").attr("_mask"));
372: 			ConvertPandasType(col_type, duckdb_col_type, bind_data.pandas_type);
373: 		} else if (StringUtil::StartsWith(col_type, "datetime64[ns") || col_type == "<M8[ns]") {
374: 			// timestamp type
375: 			bind_data.numpy_col = get_fun(df_columns[col_idx]).attr("array").attr("_data");
376: 			bind_data.mask = nullptr;
377: 			duckdb_col_type = LogicalType::TIMESTAMP;
378: 			bind_data.pandas_type = PandasType::TIMESTAMP;
379: 		} else {
380: 			// regular type
381: 			auto column = get_fun(df_columns[col_idx]);
382: 			if (col_type == "category") {
383: 				// for category types, we create an ENUM type for string or use the converted numpy type for the rest
384: 				D_ASSERT(py::hasattr(column, "cat"));
385: 				D_ASSERT(py::hasattr(column.attr("cat"), "categories"));
386: 				auto categories = py::array(column.attr("cat").attr("categories"));
387: 				auto category_type = string(py::str(categories.attr("dtype")));
388: 				if (category_type == "object") {
389: 					// Let's hope the object type is a string.
390: 					bind_data.pandas_type = PandasType::CATEGORY;
391: 					auto enum_name = string(py::str(df_columns[col_idx]));
392: 					vector<string> enum_entries = py::cast<vector<string>>(categories);
393: 					D_ASSERT(py::hasattr(column.attr("cat"), "codes"));
394: 					duckdb_col_type = LogicalType::ENUM(enum_name, enum_entries);
395: 					bind_data.numpy_col = py::array(column.attr("cat").attr("codes"));
396: 					bind_data.mask = nullptr;
397: 					D_ASSERT(py::hasattr(bind_data.numpy_col, "dtype"));
398: 					bind_data.internal_categorical_type = string(py::str(bind_data.numpy_col.attr("dtype")));
399: 				} else {
400: 					bind_data.numpy_col = py::array(column.attr("to_numpy")());
401: 					bind_data.mask = nullptr;
402: 					auto numpy_type = bind_data.numpy_col.attr("dtype");
403: 					// for category types (non-strings), we use the converted numpy type
404: 					category_type = string(py::str(numpy_type));
405: 					ConvertPandasType(category_type, duckdb_col_type, bind_data.pandas_type);
406: 				}
407: 			} else {
408: 				bind_data.numpy_col = py::array(column.attr("to_numpy")());
409: 				bind_data.mask = nullptr;
410: 				ConvertPandasType(col_type, duckdb_col_type, bind_data.pandas_type);
411: 			}
412: 		}
413: 		D_ASSERT(py::hasattr(bind_data.numpy_col, "strides"));
414: 		bind_data.numpy_stride = bind_data.numpy_col.attr("strides").attr("__getitem__")(0).cast<idx_t>();
415: 		return_types.push_back(duckdb_col_type);
416: 		bind_columns.push_back(move(bind_data));
417: 	}
418: }
419: } // namespace duckdb
[end of tools/pythonpkg/src/vector_conversion.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: