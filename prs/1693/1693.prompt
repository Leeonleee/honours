You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Import database fails when table's name have '.' or '-'
**What does happen?**
Fails to import a database created via the export command. This happens when the exported table has some symbols in its name (like . or -).

**What should happen?**
Given that DuckDB accepts creating these tables it also should be able to load tables with symbol when using EXPORT + IMPORT.

**To Reproduce**
Steps to reproduce the behavior. Bonus points if those are only SQL queries.
1. `duckdb original_db`
2. CREATE TABLE "a-1"(id int);
3. INSERT INTO "a-1" VALUES (1);
4. EXPORT DATABASE 'exported';
5. .quit
6. `duckdb imported`
7. IMPORT DATABASE 'exported';
```
Error: Parser Error: syntax error at or near "-"
LINE 7: COPY a-1...
```

**Environment (please complete the following information):**
 - OS: macOS Big Sur 11.2.3
 - DuckDB Version: master

**Before submitting**
- [X] Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?
- [X] Have you tried this on the latest `master` branch? In case you cannot compile, you may find some binaries here: https://github.com/duckdb/duckdb/releases/tag/master-builds


</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
2: 
3: ![.github/workflows/main.yml](https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
6: 
7: 
8: ## Installation
9: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
10: 
11: ## Development
12: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
13: 
14: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
15: 
16: 
[end of README.md]
[start of src/catalog/catalog_entry/table_catalog_entry.cpp]
1: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/serializer.hpp"
7: #include "duckdb/main/connection.hpp"
8: #include "duckdb/main/database.hpp"
9: #include "duckdb/parser/constraints/list.hpp"
10: #include "duckdb/parser/parsed_data/alter_table_info.hpp"
11: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
12: #include "duckdb/planner/constraints/bound_unique_constraint.hpp"
13: #include "duckdb/planner/constraints/bound_check_constraint.hpp"
14: #include "duckdb/planner/expression/bound_constant_expression.hpp"
15: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
16: #include "duckdb/storage/storage_manager.hpp"
17: #include "duckdb/planner/binder.hpp"
18: 
19: #include "duckdb/execution/index/art/art.hpp"
20: #include "duckdb/parser/expression/columnref_expression.hpp"
21: #include "duckdb/planner/expression/bound_reference_expression.hpp"
22: #include "duckdb/parser/parsed_expression_iterator.hpp"
23: #include "duckdb/planner/expression_binder/alter_binder.hpp"
24: #include "duckdb/parser/keyword_helper.hpp"
25: 
26: #include <algorithm>
27: #include <sstream>
28: 
29: namespace duckdb {
30: 
31: void TableCatalogEntry::AddLowerCaseAliases(unordered_map<string, column_t> &name_map) {
32: 	unordered_map<string, column_t> extra_lowercase_names;
33: 	for (auto &entry : name_map) {
34: 		auto lcase = StringUtil::Lower(entry.first);
35: 		// check the lowercase name map if there already exists a lowercase version
36: 		if (extra_lowercase_names.find(lcase) == extra_lowercase_names.end()) {
37: 			// not yet: add the mapping
38: 			extra_lowercase_names[lcase] = entry.second;
39: 		} else {
40: 			// the lowercase already exists: set it to invalid index
41: 			extra_lowercase_names[lcase] = INVALID_INDEX;
42: 		}
43: 	}
44: 	// for any new lowercase names, add them to the original name map
45: 	for (auto &entry : extra_lowercase_names) {
46: 		if (entry.second != INVALID_INDEX) {
47: 			name_map[entry.first] = entry.second;
48: 		}
49: 	}
50: }
51: 
52: TableCatalogEntry::TableCatalogEntry(Catalog *catalog, SchemaCatalogEntry *schema, BoundCreateTableInfo *info,
53:                                      std::shared_ptr<DataTable> inherited_storage)
54:     : StandardEntry(CatalogType::TABLE_ENTRY, schema, catalog, info->Base().table), storage(move(inherited_storage)),
55:       columns(move(info->Base().columns)), constraints(move(info->Base().constraints)),
56:       bound_constraints(move(info->bound_constraints)), name_map(info->name_map) {
57: 	this->temporary = info->Base().temporary;
58: 	// add lower case aliases
59: 	AddLowerCaseAliases(name_map);
60: 	// add the "rowid" alias, if there is no rowid column specified in the table
61: 	if (name_map.find("rowid") == name_map.end()) {
62: 		name_map["rowid"] = COLUMN_IDENTIFIER_ROW_ID;
63: 	}
64: 	if (!storage) {
65: 		// create the physical storage
66: 		storage = make_shared<DataTable>(catalog->db, schema->name, name, GetTypes(), move(info->data));
67: 
68: 		// create the unique indexes for the UNIQUE and PRIMARY KEY constraints
69: 		for (idx_t i = 0; i < bound_constraints.size(); i++) {
70: 			auto &constraint = bound_constraints[i];
71: 			if (constraint->type == ConstraintType::UNIQUE) {
72: 				// unique constraint: create a unique index
73: 				auto &unique = (BoundUniqueConstraint &)*constraint;
74: 				// fetch types and create expressions for the index from the columns
75: 				vector<column_t> column_ids;
76: 				vector<unique_ptr<Expression>> unbound_expressions;
77: 				vector<unique_ptr<Expression>> bound_expressions;
78: 				idx_t key_nr = 0;
79: 				for (auto &key : unique.keys) {
80: 					D_ASSERT(key < columns.size());
81: 
82: 					unbound_expressions.push_back(make_unique<BoundColumnRefExpression>(
83: 					    columns[key].name, columns[key].type, ColumnBinding(0, column_ids.size())));
84: 
85: 					bound_expressions.push_back(make_unique<BoundReferenceExpression>(columns[key].type, key_nr++));
86: 					column_ids.push_back(key);
87: 				}
88: 				// create an adaptive radix tree around the expressions
89: 				auto art = make_unique<ART>(column_ids, move(unbound_expressions), true);
90: 				storage->AddIndex(move(art), bound_expressions);
91: 			}
92: 		}
93: 	}
94: }
95: 
96: bool TableCatalogEntry::ColumnExists(const string &name) {
97: 	return name_map.find(name) != name_map.end();
98: }
99: 
100: unique_ptr<CatalogEntry> TableCatalogEntry::AlterEntry(ClientContext &context, AlterInfo *info) {
101: 	D_ASSERT(!internal);
102: 	if (info->type != AlterType::ALTER_TABLE) {
103: 		throw CatalogException("Can only modify table with ALTER TABLE statement");
104: 	}
105: 	auto table_info = (AlterTableInfo *)info;
106: 	switch (table_info->alter_table_type) {
107: 	case AlterTableType::RENAME_COLUMN: {
108: 		auto rename_info = (RenameColumnInfo *)table_info;
109: 		return RenameColumn(context, *rename_info);
110: 	}
111: 	case AlterTableType::RENAME_TABLE: {
112: 		auto rename_info = (RenameTableInfo *)table_info;
113: 		auto copied_table = Copy(context);
114: 		copied_table->name = rename_info->new_table_name;
115: 		return copied_table;
116: 	}
117: 	case AlterTableType::ADD_COLUMN: {
118: 		auto add_info = (AddColumnInfo *)table_info;
119: 		return AddColumn(context, *add_info);
120: 	}
121: 	case AlterTableType::REMOVE_COLUMN: {
122: 		auto remove_info = (RemoveColumnInfo *)table_info;
123: 		return RemoveColumn(context, *remove_info);
124: 	}
125: 	case AlterTableType::SET_DEFAULT: {
126: 		auto set_default_info = (SetDefaultInfo *)table_info;
127: 		return SetDefault(context, *set_default_info);
128: 	}
129: 	case AlterTableType::ALTER_COLUMN_TYPE: {
130: 		auto change_type_info = (ChangeColumnTypeInfo *)table_info;
131: 		return ChangeColumnType(context, *change_type_info);
132: 	}
133: 	default:
134: 		throw InternalException("Unrecognized alter table type!");
135: 	}
136: }
137: 
138: static void RenameExpression(ParsedExpression &expr, RenameColumnInfo &info) {
139: 	if (expr.type == ExpressionType::COLUMN_REF) {
140: 		auto &colref = (ColumnRefExpression &)expr;
141: 		if (colref.column_name == info.old_name) {
142: 			colref.column_name = info.new_name;
143: 		}
144: 	}
145: 	ParsedExpressionIterator::EnumerateChildren(
146: 	    expr, [&](const ParsedExpression &child) { RenameExpression((ParsedExpression &)child, info); });
147: }
148: 
149: unique_ptr<CatalogEntry> TableCatalogEntry::RenameColumn(ClientContext &context, RenameColumnInfo &info) {
150: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
151: 	create_info->temporary = temporary;
152: 	bool found = false;
153: 	for (idx_t i = 0; i < columns.size(); i++) {
154: 		ColumnDefinition copy = columns[i].Copy();
155: 
156: 		create_info->columns.push_back(move(copy));
157: 		if (info.old_name == columns[i].name) {
158: 			D_ASSERT(!found);
159: 			create_info->columns[i].name = info.new_name;
160: 			found = true;
161: 		}
162: 	}
163: 	if (!found) {
164: 		throw CatalogException("Table does not have a column with name \"%s\"", info.name);
165: 	}
166: 	for (idx_t c_idx = 0; c_idx < constraints.size(); c_idx++) {
167: 		auto copy = constraints[c_idx]->Copy();
168: 		switch (copy->type) {
169: 		case ConstraintType::NOT_NULL:
170: 			// NOT NULL constraint: no adjustments necessary
171: 			break;
172: 		case ConstraintType::CHECK: {
173: 			// CHECK constraint: need to rename column references that refer to the renamed column
174: 			auto &check = (CheckConstraint &)*copy;
175: 			RenameExpression(*check.expression, info);
176: 			break;
177: 		}
178: 		case ConstraintType::UNIQUE: {
179: 			// UNIQUE constraint: possibly need to rename columns
180: 			auto &unique = (UniqueConstraint &)*copy;
181: 			for (idx_t i = 0; i < unique.columns.size(); i++) {
182: 				if (unique.columns[i] == info.old_name) {
183: 					unique.columns[i] = info.new_name;
184: 				}
185: 			}
186: 			break;
187: 		}
188: 		default:
189: 			throw CatalogException("Unsupported constraint for entry!");
190: 		}
191: 		create_info->constraints.push_back(move(copy));
192: 	}
193: 	auto binder = Binder::CreateBinder(context);
194: 	auto bound_create_info = binder->BindCreateTableInfo(move(create_info));
195: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
196: }
197: 
198: unique_ptr<CatalogEntry> TableCatalogEntry::AddColumn(ClientContext &context, AddColumnInfo &info) {
199: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
200: 	create_info->temporary = temporary;
201: 	for (idx_t i = 0; i < columns.size(); i++) {
202: 		create_info->columns.push_back(columns[i].Copy());
203: 	}
204: 	info.new_column.oid = columns.size();
205: 	create_info->columns.push_back(info.new_column.Copy());
206: 
207: 	auto binder = Binder::CreateBinder(context);
208: 	auto bound_create_info = binder->BindCreateTableInfo(move(create_info));
209: 	auto new_storage =
210: 	    make_shared<DataTable>(context, *storage, info.new_column, bound_create_info->bound_defaults.back().get());
211: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
212: 	                                      new_storage);
213: }
214: 
215: unique_ptr<CatalogEntry> TableCatalogEntry::RemoveColumn(ClientContext &context, RemoveColumnInfo &info) {
216: 	idx_t removed_index = INVALID_INDEX;
217: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
218: 	create_info->temporary = temporary;
219: 	for (idx_t i = 0; i < columns.size(); i++) {
220: 		if (columns[i].name == info.removed_column) {
221: 			D_ASSERT(removed_index == INVALID_INDEX);
222: 			removed_index = i;
223: 			continue;
224: 		}
225: 		create_info->columns.push_back(columns[i].Copy());
226: 	}
227: 	if (removed_index == INVALID_INDEX) {
228: 		if (!info.if_exists) {
229: 			throw CatalogException("Table does not have a column with name \"%s\"", info.removed_column);
230: 		}
231: 		return nullptr;
232: 	}
233: 	if (create_info->columns.empty()) {
234: 		throw CatalogException("Cannot drop column: table only has one column remaining!");
235: 	}
236: 	// handle constraints for the new table
237: 	D_ASSERT(constraints.size() == bound_constraints.size());
238: 	for (idx_t constr_idx = 0; constr_idx < constraints.size(); constr_idx++) {
239: 		auto &constraint = constraints[constr_idx];
240: 		auto &bound_constraint = bound_constraints[constr_idx];
241: 		switch (bound_constraint->type) {
242: 		case ConstraintType::NOT_NULL: {
243: 			auto &not_null_constraint = (BoundNotNullConstraint &)*bound_constraint;
244: 			if (not_null_constraint.index != removed_index) {
245: 				// the constraint is not about this column: we need to copy it
246: 				// we might need to shift the index back by one though, to account for the removed column
247: 				idx_t new_index = not_null_constraint.index;
248: 				if (not_null_constraint.index > removed_index) {
249: 					new_index -= 1;
250: 				}
251: 				create_info->constraints.push_back(make_unique<NotNullConstraint>(new_index));
252: 			}
253: 			break;
254: 		}
255: 		case ConstraintType::CHECK: {
256: 			// CHECK constraint
257: 			auto &bound_check = (BoundCheckConstraint &)*bound_constraint;
258: 			// check if the removed column is part of the check constraint
259: 			if (bound_check.bound_columns.find(removed_index) != bound_check.bound_columns.end()) {
260: 				if (bound_check.bound_columns.size() > 1) {
261: 					// CHECK constraint that concerns mult
262: 					throw CatalogException(
263: 					    "Cannot drop column \"%s\" because there is a CHECK constraint that depends on it",
264: 					    info.removed_column);
265: 				} else {
266: 					// CHECK constraint that ONLY concerns this column, strip the constraint
267: 				}
268: 			} else {
269: 				// check constraint does not concern the removed column: simply re-add it
270: 				create_info->constraints.push_back(constraint->Copy());
271: 			}
272: 			break;
273: 		}
274: 		case ConstraintType::UNIQUE: {
275: 			auto copy = constraint->Copy();
276: 			auto &unique = (UniqueConstraint &)*copy;
277: 			if (unique.index != INVALID_INDEX) {
278: 				if (unique.index == removed_index) {
279: 					throw CatalogException(
280: 					    "Cannot drop column \"%s\" because there is a UNIQUE constraint that depends on it",
281: 					    info.removed_column);
282: 				} else if (unique.index > removed_index) {
283: 					unique.index--;
284: 				}
285: 			}
286: 			create_info->constraints.push_back(move(copy));
287: 			break;
288: 		}
289: 		default:
290: 			throw InternalException("Unsupported constraint for entry!");
291: 		}
292: 	}
293: 
294: 	auto binder = Binder::CreateBinder(context);
295: 	auto bound_create_info = binder->BindCreateTableInfo(move(create_info));
296: 	auto new_storage = make_shared<DataTable>(context, *storage, removed_index);
297: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
298: 	                                      new_storage);
299: }
300: 
301: unique_ptr<CatalogEntry> TableCatalogEntry::SetDefault(ClientContext &context, SetDefaultInfo &info) {
302: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
303: 	bool found = false;
304: 	for (idx_t i = 0; i < columns.size(); i++) {
305: 		auto copy = columns[i].Copy();
306: 		if (info.column_name == copy.name) {
307: 			// set the default value of this column
308: 			copy.default_value = info.expression ? info.expression->Copy() : nullptr;
309: 			found = true;
310: 		}
311: 		create_info->columns.push_back(move(copy));
312: 	}
313: 	if (!found) {
314: 		throw BinderException("Table \"%s\" does not have a column with name \"%s\"", info.name, info.column_name);
315: 	}
316: 
317: 	for (idx_t i = 0; i < constraints.size(); i++) {
318: 		auto constraint = constraints[i]->Copy();
319: 		create_info->constraints.push_back(move(constraint));
320: 	}
321: 
322: 	auto binder = Binder::CreateBinder(context);
323: 	auto bound_create_info = binder->BindCreateTableInfo(move(create_info));
324: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
325: }
326: 
327: unique_ptr<CatalogEntry> TableCatalogEntry::ChangeColumnType(ClientContext &context, ChangeColumnTypeInfo &info) {
328: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
329: 	idx_t change_idx = INVALID_INDEX;
330: 	for (idx_t i = 0; i < columns.size(); i++) {
331: 		auto copy = columns[i].Copy();
332: 		if (info.column_name == copy.name) {
333: 			// set the default value of this column
334: 			change_idx = i;
335: 			copy.type = info.target_type;
336: 		}
337: 		create_info->columns.push_back(move(copy));
338: 	}
339: 	if (change_idx == INVALID_INDEX) {
340: 		throw BinderException("Table \"%s\" does not have a column with name \"%s\"", info.name, info.column_name);
341: 	}
342: 
343: 	for (idx_t i = 0; i < constraints.size(); i++) {
344: 		auto constraint = constraints[i]->Copy();
345: 		switch (constraint->type) {
346: 		case ConstraintType::CHECK: {
347: 			auto &bound_check = (BoundCheckConstraint &)*bound_constraints[i];
348: 			if (bound_check.bound_columns.find(change_idx) != bound_check.bound_columns.end()) {
349: 				throw BinderException("Cannot change the type of a column that has a CHECK constraint specified");
350: 			}
351: 			break;
352: 		}
353: 		case ConstraintType::NOT_NULL:
354: 			break;
355: 		case ConstraintType::UNIQUE: {
356: 			auto &bound_unique = (BoundUniqueConstraint &)*bound_constraints[i];
357: 			if (bound_unique.keys.find(change_idx) != bound_unique.keys.end()) {
358: 				throw BinderException(
359: 				    "Cannot change the type of a column that has a UNIQUE or PRIMARY KEY constraint specified");
360: 			}
361: 			break;
362: 		}
363: 		default:
364: 			throw InternalException("Unsupported constraint for entry!");
365: 		}
366: 		create_info->constraints.push_back(move(constraint));
367: 	}
368: 
369: 	auto binder = Binder::CreateBinder(context);
370: 	// bind the specified expression
371: 	vector<column_t> bound_columns;
372: 	AlterBinder expr_binder(*binder, context, name, columns, bound_columns, info.target_type);
373: 	auto expression = info.expression->Copy();
374: 	auto bound_expression = expr_binder.Bind(expression);
375: 	auto bound_create_info = binder->BindCreateTableInfo(move(create_info));
376: 	if (bound_columns.empty()) {
377: 		bound_columns.push_back(COLUMN_IDENTIFIER_ROW_ID);
378: 	}
379: 
380: 	auto new_storage =
381: 	    make_shared<DataTable>(context, *storage, change_idx, info.target_type, move(bound_columns), *bound_expression);
382: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
383: 	                                      new_storage);
384: }
385: 
386: ColumnDefinition &TableCatalogEntry::GetColumn(const string &name) {
387: 	auto entry = name_map.find(name);
388: 	if (entry == name_map.end() || entry->second == COLUMN_IDENTIFIER_ROW_ID) {
389: 		throw CatalogException("Column with name %s does not exist!", name);
390: 	}
391: 	return columns[entry->second];
392: }
393: 
394: vector<LogicalType> TableCatalogEntry::GetTypes() {
395: 	vector<LogicalType> types;
396: 	for (auto &it : columns) {
397: 		types.push_back(it.type);
398: 	}
399: 	return types;
400: }
401: 
402: vector<LogicalType> TableCatalogEntry::GetTypes(const vector<column_t> &column_ids) {
403: 	vector<LogicalType> result;
404: 	for (auto &index : column_ids) {
405: 		if (index == COLUMN_IDENTIFIER_ROW_ID) {
406: 			result.push_back(LOGICAL_ROW_TYPE);
407: 		} else {
408: 			result.push_back(columns[index].type);
409: 		}
410: 	}
411: 	return result;
412: }
413: 
414: void TableCatalogEntry::Serialize(Serializer &serializer) {
415: 	serializer.WriteString(schema->name);
416: 	serializer.WriteString(name);
417: 	D_ASSERT(columns.size() <= NumericLimits<uint32_t>::Maximum());
418: 	serializer.Write<uint32_t>((uint32_t)columns.size());
419: 	for (auto &column : columns) {
420: 		column.Serialize(serializer);
421: 	}
422: 	D_ASSERT(constraints.size() <= NumericLimits<uint32_t>::Maximum());
423: 	serializer.Write<uint32_t>((uint32_t)constraints.size());
424: 	for (auto &constraint : constraints) {
425: 		constraint->Serialize(serializer);
426: 	}
427: }
428: 
429: string TableCatalogEntry::ToSQL() {
430: 	std::stringstream ss;
431: 	ss << "CREATE TABLE " << KeywordHelper::WriteOptionallyQuoted(name) << "(";
432: 
433: 	// find all columns that have NOT NULL specified, but are NOT primary key columns
434: 	unordered_set<idx_t> not_null_columns;
435: 	unordered_set<idx_t> unique_columns;
436: 	unordered_set<idx_t> pk_columns;
437: 	unordered_set<string> multi_key_pks;
438: 	vector<string> extra_constraints;
439: 	for (auto &constraint : constraints) {
440: 		if (constraint->type == ConstraintType::NOT_NULL) {
441: 			auto &not_null = (NotNullConstraint &)*constraint;
442: 			not_null_columns.insert(not_null.index);
443: 		} else if (constraint->type == ConstraintType::UNIQUE) {
444: 			auto &pk = (UniqueConstraint &)*constraint;
445: 			vector<string> constraint_columns = pk.columns;
446: 			if (pk.columns.empty()) {
447: 				// no columns specified: single column constraint
448: 				if (pk.is_primary_key) {
449: 					pk_columns.insert(pk.index);
450: 				} else {
451: 					unique_columns.insert(pk.index);
452: 				}
453: 			} else {
454: 				// multi-column constraint, this constraint needs to go at the end after all columns
455: 				if (pk.is_primary_key) {
456: 					// multi key pk column: insert set of columns into multi_key_pks
457: 					for (auto &col : pk.columns) {
458: 						multi_key_pks.insert(col);
459: 					}
460: 				}
461: 				extra_constraints.push_back(constraint->ToString());
462: 			}
463: 		} else {
464: 			extra_constraints.push_back(constraint->ToString());
465: 		}
466: 	}
467: 
468: 	for (idx_t i = 0; i < columns.size(); i++) {
469: 		if (i > 0) {
470: 			ss << ", ";
471: 		}
472: 		auto &column = columns[i];
473: 		ss << KeywordHelper::WriteOptionallyQuoted(column.name) << " " << column.type.ToString();
474: 		bool not_null = not_null_columns.find(column.oid) != not_null_columns.end();
475: 		bool is_single_key_pk = pk_columns.find(column.oid) != pk_columns.end();
476: 		bool is_multi_key_pk = multi_key_pks.find(column.name) != multi_key_pks.end();
477: 		bool is_unique = unique_columns.find(column.oid) != unique_columns.end();
478: 		if (not_null && !is_single_key_pk && !is_multi_key_pk) {
479: 			// NOT NULL but not a primary key column
480: 			ss << " NOT NULL";
481: 		}
482: 		if (is_single_key_pk) {
483: 			// single column pk: insert constraint here
484: 			ss << " PRIMARY KEY";
485: 		}
486: 		if (is_unique) {
487: 			// single column unique: insert constraint here
488: 			ss << " UNIQUE";
489: 		}
490: 		if (column.default_value) {
491: 			ss << " DEFAULT(" << column.default_value->ToString() << ")";
492: 		}
493: 	}
494: 	// print any extra constraints that still need to be printed
495: 	for (auto &extra_constraint : extra_constraints) {
496: 		ss << ", ";
497: 		ss << extra_constraint;
498: 	}
499: 
500: 	ss << ");";
501: 	return ss.str();
502: }
503: 
504: unique_ptr<CreateTableInfo> TableCatalogEntry::Deserialize(Deserializer &source) {
505: 	auto info = make_unique<CreateTableInfo>();
506: 
507: 	info->schema = source.Read<string>();
508: 	info->table = source.Read<string>();
509: 	auto column_count = source.Read<uint32_t>();
510: 
511: 	for (uint32_t i = 0; i < column_count; i++) {
512: 		auto column = ColumnDefinition::Deserialize(source);
513: 		info->columns.push_back(move(column));
514: 	}
515: 	auto constraint_count = source.Read<uint32_t>();
516: 
517: 	for (uint32_t i = 0; i < constraint_count; i++) {
518: 		auto constraint = Constraint::Deserialize(source);
519: 		info->constraints.push_back(move(constraint));
520: 	}
521: 	return info;
522: }
523: 
524: unique_ptr<CatalogEntry> TableCatalogEntry::Copy(ClientContext &context) {
525: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
526: 	for (idx_t i = 0; i < columns.size(); i++) {
527: 		create_info->columns.push_back(columns[i].Copy());
528: 	}
529: 
530: 	for (idx_t i = 0; i < constraints.size(); i++) {
531: 		auto constraint = constraints[i]->Copy();
532: 		create_info->constraints.push_back(move(constraint));
533: 	}
534: 
535: 	auto binder = Binder::CreateBinder(context);
536: 	auto bound_create_info = binder->BindCreateTableInfo(move(create_info));
537: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
538: }
539: 
540: void TableCatalogEntry::SetAsRoot() {
541: 	storage->SetAsRoot();
542: }
543: 
544: void TableCatalogEntry::CommitAlter(AlterInfo &info) {
545: 	D_ASSERT(info.type == AlterType::ALTER_TABLE);
546: 	auto &alter_table = (AlterTableInfo &)info;
547: 	string column_name;
548: 	switch (alter_table.alter_table_type) {
549: 	case AlterTableType::REMOVE_COLUMN: {
550: 		auto &remove_info = (RemoveColumnInfo &)alter_table;
551: 		column_name = remove_info.removed_column;
552: 		break;
553: 	}
554: 	case AlterTableType::ALTER_COLUMN_TYPE: {
555: 		auto &change_info = (ChangeColumnTypeInfo &)alter_table;
556: 		column_name = change_info.column_name;
557: 		break;
558: 	}
559: 	default:
560: 		break;
561: 	}
562: 	if (column_name.empty()) {
563: 		return;
564: 	}
565: 	idx_t removed_index = INVALID_INDEX;
566: 	for (idx_t i = 0; i < columns.size(); i++) {
567: 		if (columns[i].name == column_name) {
568: 			D_ASSERT(removed_index == INVALID_INDEX);
569: 			removed_index = i;
570: 			continue;
571: 		}
572: 	}
573: 	D_ASSERT(removed_index != INVALID_INDEX);
574: 	storage->CommitDropColumn(removed_index);
575: }
576: 
577: void TableCatalogEntry::CommitDrop() {
578: 	storage->CommitDropTable();
579: }
580: 
581: } // namespace duckdb
[end of src/catalog/catalog_entry/table_catalog_entry.cpp]
[start of src/execution/operator/persistent/physical_export.cpp]
1: #include "duckdb/execution/operator/persistent/physical_export.hpp"
2: #include "duckdb/catalog/catalog.hpp"
3: #include "duckdb/transaction/transaction.hpp"
4: #include "duckdb/common/file_system.hpp"
5: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
6: #include "duckdb/common/string_util.hpp"
7: 
8: #include <algorithm>
9: #include <sstream>
10: 
11: namespace duckdb {
12: 
13: using std::stringstream;
14: 
15: static void WriteCatalogEntries(stringstream &ss, vector<CatalogEntry *> &entries) {
16: 	for (auto &entry : entries) {
17: 		ss << entry->ToSQL() << std::endl;
18: 	}
19: 	ss << std::endl;
20: }
21: 
22: static void WriteStringStreamToFile(FileSystem &fs, stringstream &ss, const string &path) {
23: 	auto ss_string = ss.str();
24: 	auto handle = fs.OpenFile(path, FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE_NEW,
25: 	                          FileLockType::WRITE_LOCK);
26: 	fs.Write(*handle, (void *)ss_string.c_str(), ss_string.size());
27: 	handle.reset();
28: }
29: 
30: static void WriteValueAsSQL(stringstream &ss, Value &val) {
31: 	if (val.type().IsNumeric()) {
32: 		ss << val.ToString();
33: 	} else {
34: 		ss << "'" << val.ToString() << "'";
35: 	}
36: }
37: 
38: static void WriteCopyStatement(FileSystem &fs, stringstream &ss, TableCatalogEntry *table, CopyInfo &info,
39:                                const CopyFunction &function) {
40: 	string table_file_path;
41: 	ss << "COPY ";
42: 	if (table->schema->name != DEFAULT_SCHEMA) {
43: 		table_file_path = fs.JoinPath(
44: 		    info.file_path, StringUtil::Format("%s.%s.%s", table->schema->name, table->name, function.extension));
45: 		ss << table->schema->name << ".";
46: 	} else {
47: 		table_file_path = fs.JoinPath(info.file_path, StringUtil::Format("%s.%s", table->name, function.extension));
48: 	}
49: 	ss << table->name << " FROM '" << table_file_path << "' (";
50: 	// write the copy options
51: 	ss << "FORMAT '" << info.format << "'";
52: 	if (info.format == "csv") {
53: 		// insert default csv options, if not specified
54: 		if (info.options.find("header") == info.options.end()) {
55: 			info.options["header"].push_back(Value::INTEGER(0));
56: 		}
57: 		if (info.options.find("delimiter") == info.options.end() && info.options.find("sep") == info.options.end() &&
58: 		    info.options.find("delim") == info.options.end()) {
59: 			info.options["delimiter"].push_back(Value(","));
60: 		}
61: 		if (info.options.find("quote") == info.options.end()) {
62: 			info.options["quote"].push_back(Value("\""));
63: 		}
64: 	}
65: 	for (auto &copy_option : info.options) {
66: 		ss << ", " << copy_option.first << " ";
67: 		if (copy_option.second.size() == 1) {
68: 			WriteValueAsSQL(ss, copy_option.second[0]);
69: 		} else {
70: 			// FIXME handle multiple options
71: 			throw NotImplementedException("FIXME: serialize list of options");
72: 		}
73: 	}
74: 	ss << ");" << std::endl;
75: }
76: 
77: void PhysicalExport::GetChunkInternal(ExecutionContext &context, DataChunk &chunk, PhysicalOperatorState *state) const {
78: 	auto &ccontext = context.client;
79: 	auto &fs = FileSystem::GetFileSystem(ccontext);
80: 
81: 	// gather all catalog types to export
82: 	vector<CatalogEntry *> schemas;
83: 	vector<CatalogEntry *> sequences;
84: 	vector<CatalogEntry *> tables;
85: 	vector<CatalogEntry *> views;
86: 	vector<CatalogEntry *> indexes;
87: 
88: 	Catalog::GetCatalog(ccontext).schemas->Scan(context.client, [&](CatalogEntry *entry) {
89: 		auto schema = (SchemaCatalogEntry *)entry;
90: 		if (schema->name != DEFAULT_SCHEMA) {
91: 			// export schema
92: 			schemas.push_back(schema);
93: 		}
94: 		schema->Scan(context.client, CatalogType::TABLE_ENTRY, [&](CatalogEntry *entry) {
95: 			if (entry->type == CatalogType::TABLE_ENTRY) {
96: 				tables.push_back(entry);
97: 			} else {
98: 				views.push_back(entry);
99: 			}
100: 		});
101: 		schema->Scan(context.client, CatalogType::SEQUENCE_ENTRY,
102: 		             [&](CatalogEntry *entry) { sequences.push_back(entry); });
103: 		schema->Scan(context.client, CatalogType::INDEX_ENTRY, [&](CatalogEntry *entry) { indexes.push_back(entry); });
104: 	});
105: 
106: 	// write the schema.sql file
107: 	// export order is SCHEMA -> SEQUENCE -> TABLE -> VIEW -> INDEX
108: 
109: 	stringstream ss;
110: 	WriteCatalogEntries(ss, schemas);
111: 	WriteCatalogEntries(ss, sequences);
112: 	WriteCatalogEntries(ss, tables);
113: 	WriteCatalogEntries(ss, views);
114: 	WriteCatalogEntries(ss, indexes);
115: 
116: 	WriteStringStreamToFile(fs, ss, fs.JoinPath(info->file_path, "schema.sql"));
117: 
118: 	// write the load.sql file
119: 	// for every table, we write COPY INTO statement with the specified options
120: 	stringstream load_ss;
121: 	for (auto &table : tables) {
122: 		WriteCopyStatement(fs, load_ss, (TableCatalogEntry *)table, *info, function);
123: 	}
124: 	WriteStringStreamToFile(fs, load_ss, fs.JoinPath(info->file_path, "load.sql"));
125: 	state->finished = true;
126: }
127: 
128: } // namespace duckdb
[end of src/execution/operator/persistent/physical_export.cpp]
[start of src/execution/physical_plan/plan_export.cpp]
1: #include "duckdb/execution/physical_plan_generator.hpp"
2: #include "duckdb/execution/operator/persistent/physical_export.hpp"
3: #include "duckdb/planner/operator/logical_export.hpp"
4: 
5: namespace duckdb {
6: 
7: unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalExport &op) {
8: 	auto export_node = make_unique<PhysicalExport>(op.types, op.function, move(op.copy_info), op.estimated_cardinality);
9: 	// plan the underlying copy statements, if any
10: 	if (!op.children.empty()) {
11: 		auto plan = CreatePlan(*op.children[0]);
12: 		export_node->children.push_back(move(plan));
13: 	}
14: 	return move(export_node);
15: }
16: 
17: } // namespace duckdb
[end of src/execution/physical_plan/plan_export.cpp]
[start of src/include/duckdb/execution/operator/persistent/physical_export.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/persistent/physical_export.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include <utility>
12: 
13: #include "duckdb/execution/physical_operator.hpp"
14: #include "duckdb/function/copy_function.hpp"
15: #include "duckdb/parser/parsed_data/copy_info.hpp"
16: 
17: namespace duckdb {
18: //! Parse a file from disk using a specified copy function and return the set of chunks retrieved from the file
19: class PhysicalExport : public PhysicalOperator {
20: public:
21: 	PhysicalExport(vector<LogicalType> types, CopyFunction function, unique_ptr<CopyInfo> info,
22: 	               idx_t estimated_cardinality)
23: 	    : PhysicalOperator(PhysicalOperatorType::EXPORT, move(types), estimated_cardinality),
24: 	      function(std::move(function)), info(move(info)) {
25: 	}
26: 
27: 	//! The copy function to use to read the file
28: 	CopyFunction function;
29: 	//! The binding info containing the set of options for reading the file
30: 	unique_ptr<CopyInfo> info;
31: 
32: public:
33: 	void GetChunkInternal(ExecutionContext &context, DataChunk &chunk, PhysicalOperatorState *state) const override;
34: };
35: 
36: } // namespace duckdb
[end of src/include/duckdb/execution/operator/persistent/physical_export.hpp]
[start of src/include/duckdb/planner/operator/logical_export.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/operator/logical_export.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/parser/parsed_data/copy_info.hpp"
12: #include "duckdb/planner/logical_operator.hpp"
13: #include "duckdb/function/copy_function.hpp"
14: 
15: namespace duckdb {
16: 
17: class LogicalExport : public LogicalOperator {
18: public:
19: 	LogicalExport(CopyFunction function, unique_ptr<CopyInfo> copy_info)
20: 	    : LogicalOperator(LogicalOperatorType::LOGICAL_EXPORT), function(function), copy_info(move(copy_info)) {
21: 	}
22: 	CopyFunction function;
23: 	unique_ptr<CopyInfo> copy_info;
24: 
25: protected:
26: 	void ResolveTypes() override {
27: 		types.push_back(LogicalType::BOOLEAN);
28: 	}
29: };
30: 
31: } // namespace duckdb
[end of src/include/duckdb/planner/operator/logical_export.hpp]
[start of src/planner/binder/statement/bind_export.cpp]
1: #include "duckdb/catalog/catalog.hpp"
2: #include "duckdb/parser/statement/export_statement.hpp"
3: #include "duckdb/planner/binder.hpp"
4: #include "duckdb/planner/operator/logical_export.hpp"
5: #include "duckdb/catalog/catalog_entry/copy_function_catalog_entry.hpp"
6: #include "duckdb/parser/statement/copy_statement.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb/main/database.hpp"
9: #include "duckdb/common/file_system.hpp"
10: #include "duckdb/planner/operator/logical_set_operation.hpp"
11: #include "duckdb/common/string_util.hpp"
12: #include <algorithm>
13: 
14: namespace duckdb {
15: 
16: BoundStatement Binder::Bind(ExportStatement &stmt) {
17: 	// COPY TO a file
18: 	auto &config = DBConfig::GetConfig(context);
19: 	if (!config.enable_copy) {
20: 		throw Exception("COPY TO is disabled by configuration");
21: 	}
22: 	BoundStatement result;
23: 	result.types = {LogicalType::BOOLEAN};
24: 	result.names = {"Success"};
25: 
26: 	// lookup the format in the catalog
27: 	auto &catalog = Catalog::GetCatalog(context);
28: 	auto copy_function = catalog.GetEntry<CopyFunctionCatalogEntry>(context, DEFAULT_SCHEMA, stmt.info->format);
29: 	if (!copy_function->function.copy_to_bind) {
30: 		throw NotImplementedException("COPY TO is not supported for FORMAT \"%s\"", stmt.info->format);
31: 	}
32: 
33: 	// gather a list of all the tables
34: 	vector<TableCatalogEntry *> tables;
35: 	Catalog::GetCatalog(context).schemas->Scan(context, [&](CatalogEntry *entry) {
36: 		auto schema = (SchemaCatalogEntry *)entry;
37: 		schema->Scan(context, CatalogType::TABLE_ENTRY, [&](CatalogEntry *entry) {
38: 			if (entry->type == CatalogType::TABLE_ENTRY) {
39: 				tables.push_back((TableCatalogEntry *)entry);
40: 			}
41: 		});
42: 	});
43: 
44: 	// now generate the COPY statements for each of the tables
45: 	auto &fs = FileSystem::GetFileSystem(context);
46: 	unique_ptr<LogicalOperator> child_operator;
47: 	for (auto &table : tables) {
48: 		auto info = make_unique<CopyInfo>();
49: 		// we copy the options supplied to the EXPORT
50: 		info->format = stmt.info->format;
51: 		info->options = stmt.info->options;
52: 		// set up the file name for the COPY TO
53: 		if (table->schema->name == DEFAULT_SCHEMA) {
54: 			info->file_path = fs.JoinPath(stmt.info->file_path,
55: 			                              StringUtil::Format("%s.%s", table->name, copy_function->function.extension));
56: 		} else {
57: 			info->file_path =
58: 			    fs.JoinPath(stmt.info->file_path, StringUtil::Format("%s.%s.%s", table->schema->name, table->name,
59: 			                                                         copy_function->function.extension));
60: 		}
61: 		info->is_from = false;
62: 		info->schema = table->schema->name;
63: 		info->table = table->name;
64: 
65: 		// generate the copy statement and bind it
66: 		CopyStatement copy_stmt;
67: 		copy_stmt.info = move(info);
68: 
69: 		auto copy_binder = Binder::CreateBinder(context);
70: 		auto bound_statement = copy_binder->Bind(copy_stmt);
71: 		if (child_operator) {
72: 			// use UNION ALL to combine the individual copy statements into a single node
73: 			auto copy_union =
74: 			    make_unique<LogicalSetOperation>(GenerateTableIndex(), 1, move(child_operator),
75: 			                                     move(bound_statement.plan), LogicalOperatorType::LOGICAL_UNION);
76: 			child_operator = move(copy_union);
77: 		} else {
78: 			child_operator = move(bound_statement.plan);
79: 		}
80: 	}
81: 
82: 	// try to create the directory, if it doesn't exist yet
83: 	// a bit hacky to do it here, but we need to create the directory BEFORE the copy statements run
84: 	if (!fs.DirectoryExists(stmt.info->file_path)) {
85: 		fs.CreateDirectory(stmt.info->file_path);
86: 	}
87: 
88: 	// create the export node
89: 	auto export_node = make_unique<LogicalExport>(copy_function->function, move(stmt.info));
90: 
91: 	if (child_operator) {
92: 		export_node->children.push_back(move(child_operator));
93: 	}
94: 
95: 	result.plan = move(export_node);
96: 	return result;
97: }
98: 
99: } // namespace duckdb
[end of src/planner/binder/statement/bind_export.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: