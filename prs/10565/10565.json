{
  "repo": "duckdb/duckdb",
  "pull_number": 10565,
  "instance_id": "duckdb__duckdb-10565",
  "issue_numbers": [
    "9443"
  ],
  "base_commit": "c22e7f117a3313fea7aba18ea372d30b3d8b3738",
  "patch": "diff --git a/src/common/arrow/arrow_wrapper.cpp b/src/common/arrow/arrow_wrapper.cpp\nindex be914f74dc92..d439d99079b1 100644\n--- a/src/common/arrow/arrow_wrapper.cpp\n+++ b/src/common/arrow/arrow_wrapper.cpp\n@@ -66,10 +66,16 @@ int ResultArrowArrayStreamWrapper::MyStreamGetSchema(struct ArrowArrayStream *st\n \tif (!stream->release) {\n \t\treturn -1;\n \t}\n+\tout->release = nullptr;\n \tauto my_stream = reinterpret_cast<ResultArrowArrayStreamWrapper *>(stream->private_data);\n \tif (!my_stream->column_types.empty()) {\n-\t\tArrowConverter::ToArrowSchema(out, my_stream->column_types, my_stream->column_names,\n-\t\t                              my_stream->result->client_properties);\n+\t\ttry {\n+\t\t\tArrowConverter::ToArrowSchema(out, my_stream->column_types, my_stream->column_names,\n+\t\t\t                              my_stream->result->client_properties);\n+\t\t} catch (std::runtime_error &e) {\n+\t\t\tmy_stream->last_error = ErrorData(e);\n+\t\t\treturn -1;\n+\t\t}\n \t\treturn 0;\n \t}\n \n@@ -89,8 +95,13 @@ int ResultArrowArrayStreamWrapper::MyStreamGetSchema(struct ArrowArrayStream *st\n \t\tmy_stream->column_types = result.types;\n \t\tmy_stream->column_names = result.names;\n \t}\n-\tArrowConverter::ToArrowSchema(out, my_stream->column_types, my_stream->column_names,\n-\t                              my_stream->result->client_properties);\n+\ttry {\n+\t\tArrowConverter::ToArrowSchema(out, my_stream->column_types, my_stream->column_names,\n+\t\t                              my_stream->result->client_properties);\n+\t} catch (std::runtime_error &e) {\n+\t\tmy_stream->last_error = ErrorData(e);\n+\t\treturn -1;\n+\t}\n \treturn 0;\n }\n \n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/arrow/test_9443.py b/tools/pythonpkg/tests/fast/arrow/test_9443.py\nnew file mode 100644\nindex 000000000000..206c07b8bc9b\n--- /dev/null\n+++ b/tools/pythonpkg/tests/fast/arrow/test_9443.py\n@@ -0,0 +1,28 @@\n+import duckdb\n+import pytest\n+\n+pq = pytest.importorskip(\"pyarrow.parquet\")\n+pa = pytest.importorskip(\"pyarrow\")\n+\n+from datetime import time\n+from pathlib import PurePosixPath\n+\n+\n+class Test9443(object):\n+    def test_9443(self, tmp_path, duckdb_cursor):\n+        arrow_table = pa.Table.from_pylist(\n+            [\n+                {\"col1\": time(1, 2, 3)},\n+            ]\n+        )  # col1: time64[us]\n+\n+        print(arrow_table)\n+\n+        temp_file = str(PurePosixPath(tmp_path.as_posix()) / \"test9443.parquet\")\n+        pq.write_table(arrow_table, temp_file)\n+\n+        sql = f'SELECT * FROM \"{temp_file}\"'\n+\n+        duckdb_cursor.execute(sql)\n+        with pytest.raises(Exception, match='Unsupported Arrow type TIME WITH TIME ZONE'):\n+            duckdb_cursor.fetch_record_batch()\n",
  "problem_statement": "Python crashes when reading Parquet files that contain columns of type time64[us]\n### What happens?\n\nWhen reading a Parquet file using the `fetch_record_batch` method, if it contains a column of (arrow) type `time64[us]`, the Python interpreter crashes with the following error:\r\n\r\n```\r\nFatal Python error: PyThreadState_Get: the function must be called with the GIL held, but the GIL is released (the current Python thread state is NULL)\r\nPython runtime state: initialized\r\n\r\nExtension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pyarrow.lib, pyarrow._hdfsio, pyarrow._parquet, pyarrow._fs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs (total: 20)\r\n```\r\n\r\nThis happens with versions `0.9.0`, `0.9.1` and `0.9.2.dev113`, and works without errors with `0.8.1`. Tested with Python 3.10 and 3.11.\n\n### To Reproduce\n\n```python\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport duckdb\r\nfrom datetime import time\r\nfrom tempfile import NamedTemporaryFile\r\n\r\narrow_table = pa.Table.from_pylist(\r\n    [\r\n        {\"col1\": time(1, 2, 3)},\r\n    ]\r\n)  # col1: time64[us]\r\n\r\nwith NamedTemporaryFile(\"wb\", suffix=\".parquet\") as temp_file:\r\n    pq.write_table(arrow_table, temp_file)\r\n    temp_file.seek(0)\r\n\r\n    sql = f'SELECT * FROM \"{temp_file.name}\"'\r\n\r\n    connection = duckdb.connect()\r\n    connection.execute(sql)\r\n\r\n    connection.fetch_record_batch()  # Fails here\r\n\r\n```\n\n### OS:\n\nApple M1 Pro\n\n### DuckDB Version:\n\n0.9.0\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nAlexander Malyga\n\n### Affiliation:\n\nRevolut\n\n### Have you tried this on the latest `main` branch?\n\nI have tested with a main build\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] Yes, I have\n",
  "hints_text": "I confirmed it can avoid the segmentation fault.\r\n```\r\n[masayuki@localhost duckdb]$ git diff\r\ndiff --git a/src/common/arrow/arrow_converter.cpp b/src/common/arrow/arrow_converter.cpp\r\nindex d57bcc471b..fc8b0345e7 100644\r\n--- a/src/common/arrow/arrow_converter.cpp\r\n+++ b/src/common/arrow/arrow_converter.cpp\r\n@@ -138,9 +138,7 @@ void SetArrowFormat(DuckDBArrowSchemaHolder &root_holder, ArrowSchema &child, co\r\n        case LogicalTypeId::DATE:\r\n                child.format = \"tdD\";\r\n                break;\r\n-#ifdef DUCKDB_WASM\r\n        case LogicalTypeId::TIME_TZ:\r\n-#endif\r\n        case LogicalTypeId::TIME:\r\n                child.format = \"ttu\";\r\n                break;\r\n```\r\n\r\nHowever, `results.read_next_batch().to_pandas()` raised this error.\r\n```\r\n(duckdb-pythonpkg) [masayuki@localhost duckdb]$ python ./test.py\r\nTraceback (most recent call last):\r\n  File \"./test.py\", line 26, in <module>\r\n    print(results.read_next_batch().to_pandas()) # Fails here\r\n  File \"pyarrow/array.pxi\", line 884, in pyarrow.lib._PandasConvertible.to_pandas\r\n  File \"pyarrow/table.pxi\", line 2683, in pyarrow.lib.RecordBatch._to_pandas\r\n  File \"pyarrow/table.pxi\", line 4192, in pyarrow.lib.Table._to_pandas\r\n  File \"/home/masayuki/.pyenv/versions/duckdb-pythonpkg/lib/python3.8/site-packages/pyarrow/pandas_compat.py\", line 774, in table_to_blockmanager\r\n    blocks = _table_to_blocks(options, table, categories, ext_columns_dtypes)\r\n  File \"/home/masayuki/.pyenv/versions/duckdb-pythonpkg/lib/python3.8/site-packages/pyarrow/pandas_compat.py\", line 1122, in _table_to_blocks\r\n    result = pa.lib.table_to_blocks(options, block_table, categories,\r\n  File \"pyarrow/table.pxi\", line 3115, in pyarrow.lib.table_to_blocks\r\n  File \"pyarrow/types.pxi\", line 88, in pyarrow.lib._datatype_to_pep3118\r\nValueError: hour must be in 0..23\r\n```\r\n\r\nTest code\r\n```\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport duckdb\r\nfrom datetime import time\r\nfrom tempfile import NamedTemporaryFile\r\n\r\narrow_table = pa.Table.from_pylist(\r\n    [\r\n        {\r\n            \"col1\": time(1, 2, 3),\r\n        },\r\n    ]\r\n)  # col1: time64[us]\r\n\r\nwith NamedTemporaryFile(\"wb\", suffix=\".parquet\") as temp_file:\r\n    pq.write_table(arrow_table, temp_file)\r\n\r\n    temp_file.seek(0)\r\n    arrow_table2 = pq.read_table(temp_file.name)\r\n\r\n    sql = f'SELECT * FROM \"{temp_file.name}\"'\r\n    connection = duckdb.connect()\r\n    connection.execute(sql)\r\n\r\n    results = connection.fetch_record_batch()\r\n    print(results.read_next_batch().to_pandas()) # Fails here\r\n```\r\nDoes anyone have an idea to fix this?",
  "created_at": "2024-02-09T23:01:38Z"
}