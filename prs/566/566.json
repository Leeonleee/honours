{
  "repo": "duckdb/duckdb",
  "pull_number": 566,
  "instance_id": "duckdb__duckdb-566",
  "issue_numbers": [
    "568"
  ],
  "base_commit": "cc5e3f29bb1b4ea1acf185b19b57b5b009b33067",
  "patch": "diff --git a/CMakeLists.txt b/CMakeLists.txt\nindex 15d7e0fed3d6..4235af2dba4b 100644\n--- a/CMakeLists.txt\n+++ b/CMakeLists.txt\n@@ -178,6 +178,7 @@ function(enable_unity_build UB_SUFFIX SOURCE_VARIABLE_NAME)\n \n   # Generate a unique filename for the unity build translation unit\n   set(unit_build_file ${CMAKE_CURRENT_BINARY_DIR}/ub_${UB_SUFFIX}.cpp)\n+  set(temp_unit_build_file ${CMAKE_CURRENT_BINARY_DIR}/ub_${UB_SUFFIX}.cpp.tmp)\n   # Exclude all translation units from compilation\n   set_source_files_properties(${files}\n                               PROPERTIES\n@@ -185,12 +186,32 @@ function(enable_unity_build UB_SUFFIX SOURCE_VARIABLE_NAME)\n                               true)\n \n   set(rebuild FALSE)\n+  # check if any of the source files have changed\n   foreach(source_file ${files})\n     if(${CMAKE_CURRENT_SOURCE_DIR}/${source_file} IS_NEWER_THAN\n        ${unit_build_file})\n       set(rebuild TRUE)\n     endif()\n   endforeach(source_file)\n+  # write a temporary file\n+  file(WRITE ${temp_unit_build_file} \"// Unity Build generated by CMake\\n\")\n+  foreach(source_file ${files})\n+    file(\n+      APPEND ${temp_unit_build_file}\n+      \"#line 0 \\\"${source_file}\\\"\\n#include <${CMAKE_CURRENT_SOURCE_DIR}/${source_file}>\\n\"\n+      )\n+  endforeach(source_file)\n+\n+  execute_process( COMMAND ${CMAKE_COMMAND} -E compare_files ${unit_build_file} ${temp_unit_build_file} RESULT_VARIABLE compare_result OUTPUT_VARIABLE bla ERROR_VARIABLE bla)\n+  if( compare_result EQUAL 0)\n+    # files are identical: do nothing\n+  elseif( compare_result EQUAL 1)\n+    # files are different: rebuild\n+    set(rebuild TRUE)\n+  else()\n+    # error while compiling: rebuild\n+    set(rebuild TRUE)\n+  endif()\n \n   if(${rebuild})\n     file(WRITE ${unit_build_file} \"// Unity Build generated by CMake\\n\")\ndiff --git a/Makefile b/Makefile\nindex 51cacd7ec3e9..893d03a71254 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -56,6 +56,13 @@ amalgamation:\n \tcmake $(GENERATOR) $(FORCE_COLOR) -DAMALGAMATION_BUILD=1 -DCMAKE_BUILD_TYPE=Release ../.. && \\\n \tcmake --build .\n \n+amaldebug:\n+\tmkdir -p build/amaldebug && \\\n+\tpython scripts/amalgamation.py && \\\n+\tcd build/amaldebug && \\\n+\tcmake $(GENERATOR) $(FORCE_COLOR) -DAMALGAMATION_BUILD=1 -DCMAKE_BUILD_TYPE=Debug ../.. && \\\n+\tcmake --build .\n+\n jdbc:\n \tmkdir -p build/jdbc && \\\n \tcd build/jdbc && \\\ndiff --git a/scripts/amalgamation.py b/scripts/amalgamation.py\nindex fc33fd60a394..46f6282e3ecd 100644\n--- a/scripts/amalgamation.py\n+++ b/scripts/amalgamation.py\n@@ -1,8 +1,10 @@\n # this script creates a single header + source file combination out of the DuckDB sources\n-import os, re, sys, pickle\n+import os, re, sys, shutil\n amal_dir = os.path.join('src', 'amalgamation')\n header_file = os.path.join(amal_dir, \"duckdb.hpp\")\n source_file = os.path.join(amal_dir, \"duckdb.cpp\")\n+temp_header = 'duckdb.hpp.tmp'\n+temp_source = 'duckdb.cpp.tmp'\n \n src_dir = 'src'\n include_dir = os.path.join('src', 'include')\n@@ -25,168 +27,145 @@\n # paths of where to look for files to compile and include to the final amalgamation\n compile_directories = [src_dir, fmt_dir, hll_dir, miniz_dir, re2_dir, utf8proc_dir, pg_query_dir]\n \n+# files always excluded\n+always_excluded = ['src/amalgamation/duckdb.cpp', 'src/amalgamation/duckdb.hpp']\n # files excluded from the amalgamation\n excluded_files = ['grammar.cpp', 'grammar.hpp', 'symbols.cpp', 'file_system.cpp']\n # files excluded from individual file compilation during test_compile\n excluded_compilation_files = excluded_files + ['gram.hpp', 'kwlist.hpp', \"duckdb-c.cpp\"]\n-# where to cache which files have already been compiled, only used for --compile --resume\n-cache_file = 'amalgamation.cache'\n \n \n linenumbers = False\n-compile = False\n-resume = False\n-\n-for arg in sys.argv:\n-\tif arg == '--compile':\n-\t\tcompile = True\n-\telif arg == '--resume':\n-\t\tresume = True\n-\telif arg == '--linenumbers':\n-\t\tlinenumbers = True\n-\telif arg == '--no-linenumbers':\n-\t\tlinenumbers = False\n-\n-if not resume:\n-\ttry:\n-\t\tos.remove(cache_file)\n-\texcept:\n-\t\tpass\n \n def get_includes(fpath, text):\n-\t# find all the includes referred to in the directory\n-\tinclude_statements = re.findall(\"(^[#]include[\\t ]+[\\\"]([^\\\"]+)[\\\"])\", text, flags=re.MULTILINE)\n-\tinclude_files = []\n-\t# figure out where they are located\n-\tfor included_file in [x[1] for x in include_statements]:\n-\t\tincluded_file = os.sep.join(included_file.split('/'))\n-\t\tfound = False\n-\t\tfor include_path in include_paths:\n-\t\t\tipath = os.path.join(include_path, included_file)\n-\t\t\tif os.path.isfile(ipath):\n-\t\t\t\tinclude_files.append(ipath)\n-\t\t\t\tfound = True\n-\t\t\t\tbreak\n-\t\tif not found:\n-\t\t\traise Exception('Could not find include file \"' + included_file + '\", included from file \"' + fpath + '\"')\n-\treturn ([x[0] for x in include_statements], include_files)\n+    # find all the includes referred to in the directory\n+    include_statements = re.findall(\"(^[#]include[\\t ]+[\\\"]([^\\\"]+)[\\\"])\", text, flags=re.MULTILINE)\n+    include_files = []\n+    # figure out where they are located\n+    for included_file in [x[1] for x in include_statements]:\n+        included_file = os.sep.join(included_file.split('/'))\n+        found = False\n+        for include_path in include_paths:\n+            ipath = os.path.join(include_path, included_file)\n+            if os.path.isfile(ipath):\n+                include_files.append(ipath)\n+                found = True\n+                break\n+        if not found:\n+            raise Exception('Could not find include file \"' + included_file + '\", included from file \"' + fpath + '\"')\n+    return ([x[0] for x in include_statements], include_files)\n \n def cleanup_file(text):\n-\t# remove all \"#pragma once\" notifications\n-\ttext = re.sub('#pragma once', '', text)\n-\treturn text\n+    # remove all \"#pragma once\" notifications\n+    text = re.sub('#pragma once', '', text)\n+    return text\n \n # recursively get all includes and write them\n written_files = {}\n \n def write_file(current_file, ignore_excluded = False):\n-\tglobal linenumbers\n-\tglobal written_files\n-\tif current_file.split(os.sep)[-1] in excluded_files and not ignore_excluded:\n-\t\t# file is in ignored files set\n-\t\treturn \"\"\n-\tif current_file in written_files:\n-\t\t# file is already written\n-\t\treturn \"\"\n-\twritten_files[current_file] = True\n-\n-\t# first read this file\n-\twith open(current_file, 'r') as f:\n-\t\ttext = f.read()\n-\n-\t(statements, includes) = get_includes(current_file, text)\n-\t# find the linenr of the final #include statement we parsed\n-\tif len(statements) > 0:\n-\t\tindex = text.find(statements[-1])\n-\t\tlinenr = len(text[:index].split('\\n'))\n-\n-\t\t# now write all the dependencies of this header first\n-\t\tfor i in range(len(includes)):\n-\t\t\tinclude_text = write_file(includes[i])\n-\t\t\tif linenumbers and i == len(includes) - 1:\n-\t\t\t\t# for the last include statement, we also include a #line directive\n-\t\t\t\tinclude_text += '\\n#line %d \"%s\"\\n' % (linenr, current_file)\n-\t\t\ttext = text.replace(statements[i], include_text)\n-\n-\t# add the initial line here\n-\tif linenumbers:\n-\t\ttext = '\\n#line 1 \"%s\"\\n' % (current_file,) + text\n-\tprint(current_file)\n-\t# now read the header and write it\n-\treturn cleanup_file(text)\n-\n-def try_compilation(fpath, cache):\n-\tif fpath in cache:\n-\t\treturn\n-\tprint(fpath)\n-\n-\tcmd = 'clang++ -std=c++11 -Wno-deprecated -Wno-writable-strings -S -MMD -MF dependencies.d -o deps.s ' + fpath + ' ' + ' '.join([\"-I\" + x for x in include_paths])\n-\tret = os.system(cmd)\n-\tif ret != 0:\n-\t\traise Exception('Failed compilation of file \"' + fpath + '\"!\\n Command: ' + cmd)\n-\tcache[fpath] = True\n-\twith open(cache_file, 'wb') as cf:\n-\t\tpickle.dump(cache, cf)\n-\n-def compile_dir(dir, cache):\n-\tfiles = os.listdir(dir)\n-\tfiles.sort()\n-\tfor fname in files:\n-\t\tif fname in excluded_compilation_files:\n-\t\t\tcontinue\n-\t\tfpath = os.path.join(dir, fname)\n-\t\tif os.path.isdir(fpath):\n-\t\t\tcompile_dir(fpath, cache)\n-\t\telif fname.endswith('.cpp') or fname.endswith('.hpp') or fname.endswith('.c') or fname.endswith('.cc'):\n-\t\t\ttry_compilation(fpath, cache)\n-\n-if compile:\n-\t# compilation pass only\n-\t# compile all files in the src directory (including headers!) individually\n-\ttry:\n-\t\twith open(cache_file, 'rb') as cf:\n-\t\t\tcache = pickle.load(cf)\n-\texcept:\n-\t\tcache = {}\n-\tfor cdir in compile_directories:\n-\t\tcompile_dir(cdir, cache)\n-\texit(0)\n-\n-\n-if not os.path.exists(amal_dir):\n-\tos.makedirs(amal_dir)\n-\n-# now construct duckdb.hpp from these headers\n-print(\"-----------------------\")\n-print(\"-- Writing duckdb.hpp --\")\n-print(\"-----------------------\")\n-with open(header_file, 'w+') as hfile:\n-\thfile.write(\"#pragma once\\n\")\n-\tfor fpath in main_header_files:\n-\t\thfile.write(write_file(fpath))\n+    global linenumbers\n+    global written_files\n+    if current_file in always_excluded:\n+        return \"\"\n+    if current_file.split(os.sep)[-1] in excluded_files and not ignore_excluded:\n+        # file is in ignored files set\n+        return \"\"\n+    if current_file in written_files:\n+        # file is already written\n+        return \"\"\n+    written_files[current_file] = True\n+\n+    # first read this file\n+    with open(current_file, 'r') as f:\n+        text = f.read()\n+\n+    (statements, includes) = get_includes(current_file, text)\n+    # find the linenr of the final #include statement we parsed\n+    if len(statements) > 0:\n+        index = text.find(statements[-1])\n+        linenr = len(text[:index].split('\\n'))\n+\n+        # now write all the dependencies of this header first\n+        for i in range(len(includes)):\n+            include_text = write_file(includes[i])\n+            if linenumbers and i == len(includes) - 1:\n+                # for the last include statement, we also include a #line directive\n+                include_text += '\\n#line %d \"%s\"\\n' % (linenr, current_file)\n+            text = text.replace(statements[i], include_text)\n+\n+    # add the initial line here\n+    if linenumbers:\n+        text = '\\n#line 1 \"%s\"\\n' % (current_file,) + text\n+    print(current_file)\n+    # now read the header and write it\n+    return cleanup_file(text)\n \n def write_dir(dir, sfile):\n-\tfiles = os.listdir(dir)\n-\tfiles.sort()\n-\tfor fname in files:\n-\t\tif fname in excluded_files:\n-\t\t\tcontinue\n-\t\tfpath = os.path.join(dir, fname)\n-\t\tif os.path.isdir(fpath):\n-\t\t\twrite_dir(fpath, sfile)\n-\t\telif fname.endswith('.cpp') or fname.endswith('.c') or fname.endswith('.cc'):\n-\t\t\tsfile.write(write_file(fpath))\n-\n-# now construct duckdb.cpp\n-print(\"------------------------\")\n-print(\"-- Writing duckdb.cpp --\")\n-print(\"------------------------\")\n-\n-# scan all the .cpp files\n-with open(source_file, 'w+') as sfile:\n-\tsfile.write('#include \"duckdb.hpp\"\\n\\n')\n-\tfor compile_dir in compile_directories:\n-\t\twrite_dir(compile_dir, sfile)\n-\t# for windows we write file_system.cpp last\n-\t# this is because it includes windows.h which contains a lot of #define statements that mess up the other code\n-\tsfile.write(write_file(os.path.join('src', 'common', 'file_system.cpp'), True))\n+    files = os.listdir(dir)\n+    files.sort()\n+    for fname in files:\n+        if fname in excluded_files:\n+            continue\n+        fpath = os.path.join(dir, fname)\n+        if os.path.isdir(fpath):\n+            write_dir(fpath, sfile)\n+        elif fname.endswith('.cpp') or fname.endswith('.c') or fname.endswith('.cc'):\n+            sfile.write(write_file(fpath))\n+\n+def copy_if_different(src, dest):\n+    if os.path.isfile(dest):\n+        # dest exists, check if the files are different\n+        with open(src, 'r') as f:\n+            source_text = f.read()\n+        with open(dest, 'r') as f:\n+            dest_text = f.read()\n+        if source_text == dest_text:\n+            return\n+    shutil.copyfile(src, dest)\n+\n+def generate_amalgamation(source_file, header_file):\n+    # now construct duckdb.hpp from these headers\n+    print(\"-----------------------\")\n+    print(\"-- Writing \" + header_file + \" --\")\n+    print(\"-----------------------\")\n+    with open(temp_header, 'w+') as hfile:\n+        hfile.write(\"#pragma once\\n\")\n+        for fpath in main_header_files:\n+            hfile.write(write_file(fpath))\n+\n+\n+    # now construct duckdb.cpp\n+    print(\"------------------------\")\n+    print(\"-- Writing \" + source_file + \" --\")\n+    print(\"------------------------\")\n+\n+    # scan all the .cpp files\n+    with open(temp_source, 'w+') as sfile:\n+        header_file_name = header_file.split(os.sep)[-1]\n+        sfile.write('#include \"' + header_file_name + '\"\\n\\n')\n+        for compile_dir in compile_directories:\n+            write_dir(compile_dir, sfile)\n+        # for windows we write file_system.cpp last\n+        # this is because it includes windows.h which contains a lot of #define statements that mess up the other code\n+        sfile.write(write_file(os.path.join('src', 'common', 'file_system.cpp'), True))\n+\n+    copy_if_different(temp_header, header_file)\n+    copy_if_different(temp_source, source_file)\n+\n+\n+\n+if __name__ == \"__main__\":\n+    for arg in sys.argv:\n+        if arg == '--linenumbers':\n+            linenumbers = True\n+        elif arg == '--no-linenumbers':\n+            linenumbers = False\n+        elif arg.startswith('--header='):\n+            header_file = os.path.join(*arg.split('=', 1)[1].split('/'))\n+        elif arg.startswith('--source='):\n+            source_file = os.path.join(*arg.split('=', 1)[1].split('/'))\n+    if not os.path.exists(amal_dir):\n+        os.makedirs(amal_dir)\n+\n+    generate_amalgamation(source_file, header_file)\ndiff --git a/src/include/duckdb/storage/block_manager.hpp b/src/include/duckdb/storage/block_manager.hpp\nindex ccc807b15714..2cbc1f910572 100644\n--- a/src/include/duckdb/storage/block_manager.hpp\n+++ b/src/include/duckdb/storage/block_manager.hpp\n@@ -19,6 +19,7 @@ class BlockManager {\n public:\n \tvirtual ~BlockManager() = default;\n \n+\tvirtual void StartCheckpoint() = 0;\n \t//! Creates a new block inside the block manager\n \tvirtual unique_ptr<Block> CreateBlock() = 0;\n \t//! Return the next free block id\ndiff --git a/src/include/duckdb/storage/checkpoint/table_data_writer.hpp b/src/include/duckdb/storage/checkpoint/table_data_writer.hpp\nindex d662ab3bfb68..8f3b1fa586e0 100644\n--- a/src/include/duckdb/storage/checkpoint/table_data_writer.hpp\n+++ b/src/include/duckdb/storage/checkpoint/table_data_writer.hpp\n@@ -24,12 +24,13 @@ class TableDataWriter {\n \tvoid WriteTableData(Transaction &transaction);\n \n private:\n-\tvoid AppendData(idx_t col_idx, Vector &data, idx_t count);\n+\tvoid AppendData(Transaction &transaction, idx_t col_idx, Vector &data, idx_t count);\n \n \tvoid CreateSegment(idx_t col_idx);\n-\tvoid FlushSegment(idx_t col_idx);\n+\tvoid FlushSegment(Transaction &transaction, idx_t col_idx);\n \n \tvoid WriteDataPointers();\n+\tvoid VerifyDataPointers();\n \n private:\n \tCheckpointManager &manager;\ndiff --git a/src/include/duckdb/storage/in_memory_block_manager.hpp b/src/include/duckdb/storage/in_memory_block_manager.hpp\nindex 1242959a5a81..ac565aea2749 100644\n--- a/src/include/duckdb/storage/in_memory_block_manager.hpp\n+++ b/src/include/duckdb/storage/in_memory_block_manager.hpp\n@@ -17,6 +17,9 @@ namespace duckdb {\n //! InMemoryBlockManager is an implementation for a BlockManager\n class InMemoryBlockManager : public BlockManager {\n public:\n+\tvoid StartCheckpoint() override {\n+\t\tthrow Exception(\"Cannot perform IO in in-memory database!\");\n+\t}\n \tunique_ptr<Block> CreateBlock() override {\n \t\tthrow Exception(\"Cannot perform IO in in-memory database!\");\n \t}\ndiff --git a/src/include/duckdb/storage/single_file_block_manager.hpp b/src/include/duckdb/storage/single_file_block_manager.hpp\nindex 6fac59a63228..960058489638 100644\n--- a/src/include/duckdb/storage/single_file_block_manager.hpp\n+++ b/src/include/duckdb/storage/single_file_block_manager.hpp\n@@ -26,6 +26,7 @@ class SingleFileBlockManager : public BlockManager {\n public:\n \tSingleFileBlockManager(FileSystem &fs, string path, bool read_only, bool create_new, bool use_direct_io);\n \n+\tvoid StartCheckpoint() override;\n \t//! Creates a new Block and returns a pointer\n \tunique_ptr<Block> CreateBlock() override;\n \t//! Return the next free block id\ndiff --git a/src/include/duckdb/storage/string_segment.hpp b/src/include/duckdb/storage/string_segment.hpp\nindex 12e873f4e75c..bbc5fba9745b 100644\n--- a/src/include/duckdb/storage/string_segment.hpp\n+++ b/src/include/duckdb/storage/string_segment.hpp\n@@ -76,7 +76,6 @@ class StringSegment : public UncompressedSegment {\n \n \t//! Rollback a previous update\n \tvoid RollbackUpdate(UpdateInfo *info) override;\n-\n protected:\n \tvoid Update(ColumnData &column_data, SegmentStatistics &stats, Transaction &transaction, Vector &update, row_t *ids,\n \t            idx_t count, idx_t vector_index, idx_t vector_offset, UpdateInfo *node) override;\n@@ -126,7 +125,6 @@ class StringSegment : public UncompressedSegment {\n \tidx_t RemainingSpace() {\n \t\treturn Storage::BLOCK_SIZE - dictionary_offset - max_vector_count * vector_size;\n \t}\n-\n private:\n \t//! The max string size that is allowed within a block. Strings bigger than this will be labeled as a BIG STRING and\n \t//! offloaded to the overflow blocks.\ndiff --git a/src/include/duckdb/storage/uncompressed_segment.hpp b/src/include/duckdb/storage/uncompressed_segment.hpp\nindex 4cc5aca54505..92853b07cb84 100644\n--- a/src/include/duckdb/storage/uncompressed_segment.hpp\n+++ b/src/include/duckdb/storage/uncompressed_segment.hpp\n@@ -87,6 +87,7 @@ class UncompressedSegment {\n \t\treturn std::min((idx_t)STANDARD_VECTOR_SIZE, tuple_count - vector_index * STANDARD_VECTOR_SIZE);\n \t}\n \n+\tvirtual void Verify(Transaction &transaction);\n protected:\n \tvirtual void Update(ColumnData &data, SegmentStatistics &stats, Transaction &transaction, Vector &update,\n \t                    row_t *ids, idx_t count, idx_t vector_index, idx_t vector_offset, UpdateInfo *node) = 0;\ndiff --git a/src/optimizer/rule/like_optimizations.cpp b/src/optimizer/rule/like_optimizations.cpp\nindex 3edcd0d41399..35d048dbca82 100644\n--- a/src/optimizer/rule/like_optimizations.cpp\n+++ b/src/optimizer/rule/like_optimizations.cpp\n@@ -31,7 +31,7 @@ unique_ptr<Expression> LikeOptimizationRule::Apply(LogicalOperator &op, vector<E\n \n \t// the constant_expr is a scalar expression that we have to fold\n \tif (!constant_expr->IsFoldable()) {\n-\t\treturn move(root->Copy());\n+\t\treturn root->Copy();\n \t}\n \n \tauto constant_value = ExpressionExecutor::EvaluateScalar(*constant_expr);\n@@ -39,19 +39,19 @@ unique_ptr<Expression> LikeOptimizationRule::Apply(LogicalOperator &op, vector<E\n \tstring patt_str = string(((string_t)constant_value.str_value).GetData());\n \n \tif (std::regex_match(patt_str, std::regex(\"[^%_]*[%]+\"))) {\n-\t\t//^^^^^^^^^^^^^^^Prefix LIKE pattern : [^%_]*[%]+, ignoring undescore\n+\t\t// Prefix LIKE pattern : [^%_]*[%]+, ignoring underscore\n \n-\t\treturn move(ApplyRule(root, PrefixFun::GetFunction(), patt_str));\n+\t\treturn ApplyRule(root, PrefixFun::GetFunction(), patt_str);\n \n \t} else if (std::regex_match(patt_str, std::regex(\"[%]+[^%_]*\"))) {\n-\t\t//^^^^^^^^^^^^^^^^^^^^^^^Suffix LIKE pattern: [%]+[^%_]*, ignoring undescore\n+\t\t// Suffix LIKE pattern: [%]+[^%_]*, ignoring underscore\n \n-\t\treturn move(ApplyRule(root, SuffixFun::GetFunction(), patt_str));\n+\t\treturn ApplyRule(root, SuffixFun::GetFunction(), patt_str);\n \n \t} else if (std::regex_match(patt_str, std::regex(\"[%]+[^%_]*[%]+\"))) {\n-\t\t//^^^^^^^^^^^^^^^^^^^^^Contains LIKE pattern: [%]+[^%_]*[%]+, ignoring undescore\n+\t\t// Contains LIKE pattern: [%]+[^%_]*[%]+, ignoring underscore\n \n-\t\treturn move(ApplyRule(root, ContainsFun::GetFunction(), patt_str));\n+\t\treturn ApplyRule(root, ContainsFun::GetFunction(), patt_str);\n \t}\n \n \treturn nullptr;\n@@ -67,5 +67,5 @@ unique_ptr<Expression> LikeOptimizationRule::ApplyRule(BoundFunctionExpression *\n \n \texpr->children[1] = make_unique<BoundConstantExpression>(Value(pattern));\n \n-\treturn move(expr->Copy());\n+\treturn expr->Copy();\n }\ndiff --git a/src/storage/checkpoint/table_data_reader.cpp b/src/storage/checkpoint/table_data_reader.cpp\nindex 43f7d06189ac..0dc32995a642 100644\n--- a/src/storage/checkpoint/table_data_reader.cpp\n+++ b/src/storage/checkpoint/table_data_reader.cpp\n@@ -9,6 +9,9 @@\n \n #include \"duckdb/planner/parsed_data/bound_create_table_info.hpp\"\n \n+#include \"duckdb/main/database.hpp\"\n+#include \"duckdb/main/client_context.hpp\"\n+\n using namespace duckdb;\n using namespace std;\n \n@@ -23,8 +26,10 @@ void TableDataReader::ReadTableData() {\n \tassert(columns.size() > 0);\n \n \t// load the data pointers for the table\n+\tidx_t table_count = 0;\n \tfor (idx_t col = 0; col < columns.size(); col++) {\n \t\tauto &column = columns[col];\n+\t\tidx_t column_count = 0;\n \t\tidx_t data_pointer_count = reader.Read<idx_t>();\n \t\tfor (idx_t data_ptr = 0; data_ptr < data_pointer_count; data_ptr++) {\n \t\t\t// read the data pointer\n@@ -38,11 +43,19 @@ void TableDataReader::ReadTableData() {\n \t\t\treader.ReadData(data_pointer.min_stats, 8);\n \t\t\treader.ReadData(data_pointer.max_stats, 8);\n \n+\t\t\tcolumn_count += data_pointer.tuple_count;\n \t\t\t// create a persistent segment\n \t\t\tauto segment = make_unique<PersistentSegment>(\n \t\t\t    manager.buffer_manager, data_pointer.block_id, data_pointer.offset, GetInternalType(column.type),\n \t\t\t    data_pointer.row_start, data_pointer.tuple_count, data_pointer.min_stats, data_pointer.max_stats);\n \t\t\tinfo.data[col].push_back(move(segment));\n \t\t}\n+\t\tif (col == 0) {\n+\t\t\ttable_count = column_count;\n+\t\t} else {\n+\t\t\tif (table_count != column_count) {\n+\t\t\t\tthrow Exception(\"Column length mismatch in table load!\");\n+\t\t\t}\n+\t\t}\n \t}\n }\ndiff --git a/src/storage/checkpoint/table_data_writer.cpp b/src/storage/checkpoint/table_data_writer.cpp\nindex aa47b9bbbff8..ee9749316bf1 100644\n--- a/src/storage/checkpoint/table_data_writer.cpp\n+++ b/src/storage/checkpoint/table_data_writer.cpp\n@@ -75,15 +75,17 @@ void TableDataWriter::WriteTableData(Transaction &transaction) {\n \t\t\tbreak;\n \t\t}\n \t\t// for each column, we append whatever we can fit into the block\n+\t\tidx_t chunk_size = chunk.size();\n \t\tfor (idx_t i = 0; i < table.columns.size(); i++) {\n \t\t\tassert(chunk.data[i].type == GetInternalType(table.columns[i].type));\n-\t\t\tAppendData(i, chunk.data[i], chunk.size());\n+\t\t\tAppendData(transaction, i, chunk.data[i], chunk_size);\n \t\t}\n \t}\n \t// flush any remaining data and write the data pointers to disk\n \tfor (idx_t i = 0; i < table.columns.size(); i++) {\n-\t\tFlushSegment(i);\n+\t\tFlushSegment(transaction, i);\n \t}\n+\tVerifyDataPointers();\n \tWriteDataPointers();\n }\n \n@@ -98,16 +100,16 @@ void TableDataWriter::CreateSegment(idx_t col_idx) {\n \t}\n }\n \n-void TableDataWriter::AppendData(idx_t col_idx, Vector &data, idx_t count) {\n+void TableDataWriter::AppendData(Transaction &transaction, idx_t col_idx, Vector &data, idx_t count) {\n \tidx_t offset = 0;\n-\twhile (offset < count) {\n+\twhile (count > 0) {\n \t\tidx_t appended = segments[col_idx]->Append(*stats[col_idx], data, offset, count);\n \t\tif (appended == count) {\n \t\t\t// appended everything: finished\n \t\t\treturn;\n \t\t}\n \t\t// the segment is full: flush it to disk\n-\t\tFlushSegment(col_idx);\n+\t\tFlushSegment(transaction, col_idx);\n \n \t\t// now create a new segment and continue appending\n \t\tCreateSegment(col_idx);\n@@ -116,7 +118,7 @@ void TableDataWriter::AppendData(idx_t col_idx, Vector &data, idx_t count) {\n \t}\n }\n \n-void TableDataWriter::FlushSegment(idx_t col_idx) {\n+void TableDataWriter::FlushSegment(Transaction &transaction, idx_t col_idx) {\n \tauto tuple_count = segments[col_idx]->tuple_count;\n \tif (tuple_count == 0) {\n \t\treturn;\n@@ -145,6 +147,33 @@ void TableDataWriter::FlushSegment(idx_t col_idx) {\n \tdata_pointers[col_idx].push_back(move(data_pointer));\n \t// write the block to disk\n \tmanager.block_manager.Write(*handle->node, block_id);\n+\n+\thandle.reset();\n+\tsegments[col_idx] = nullptr;\n+}\n+\n+void TableDataWriter::VerifyDataPointers() {\n+\t// verify the data pointers\n+\tidx_t table_count = 0;\n+\tfor (idx_t i = 0; i < data_pointers.size(); i++) {\n+\t\tauto &data_pointer_list = data_pointers[i];\n+\t\tidx_t column_count = 0;\n+\t\t// then write the data pointers themselves\n+\t\tfor (idx_t k = 0; k < data_pointer_list.size(); k++) {\n+\t\t\tauto &data_pointer = data_pointer_list[k];\n+\t\t\tcolumn_count += data_pointer.tuple_count;\n+\t\t}\n+\t\tif (segments[i]) {\n+\t\t\tcolumn_count += segments[i]->tuple_count;\n+\t\t}\n+\t\tif (i == 0) {\n+\t\t\ttable_count = column_count;\n+\t\t} else {\n+\t\t\tif (table_count != column_count) {\n+\t\t\t\tthrow Exception(\"Column count mismatch in data write!\");\n+\t\t\t}\n+\t\t}\n+\t}\n }\n \n void TableDataWriter::WriteDataPointers() {\ndiff --git a/src/storage/checkpoint_manager.cpp b/src/storage/checkpoint_manager.cpp\nindex 59ec3a4ae1bc..be531ba44598 100644\n--- a/src/storage/checkpoint_manager.cpp\n+++ b/src/storage/checkpoint_manager.cpp\n@@ -41,6 +41,7 @@ void CheckpointManager::CreateCheckpoint() {\n \tassert(!metadata_writer);\n \n \tauto transaction = database.transaction_manager->StartTransaction();\n+\tblock_manager.StartCheckpoint();\n \n \t//! Set up the writers for the checkpoints\n \tmetadata_writer = make_unique<MetaBlockWriter>(block_manager);\ndiff --git a/src/storage/single_file_block_manager.cpp b/src/storage/single_file_block_manager.cpp\nindex 8234ce07a412..8c0bd8d3c3fb 100644\n--- a/src/storage/single_file_block_manager.cpp\n+++ b/src/storage/single_file_block_manager.cpp\n@@ -105,22 +105,30 @@ void SingleFileBlockManager::LoadFreeList(BufferManager &manager) {\n \t}\n \tMetaBlockReader reader(manager, free_list_id);\n \tauto free_list_count = reader.Read<uint64_t>();\n+\tfree_list.clear();\n \tfree_list.reserve(free_list_count);\n \tfor (idx_t i = 0; i < free_list_count; i++) {\n \t\tfree_list.push_back(reader.Read<block_id_t>());\n \t}\n }\n \n+void SingleFileBlockManager::StartCheckpoint() {\n+\tused_blocks.clear();\n+}\n+\n block_id_t SingleFileBlockManager::GetFreeBlockId() {\n+\tblock_id_t block;\n \tif (free_list.size() > 0) {\n \t\t// free list is non empty\n \t\t// take an entry from the free list\n-\t\tblock_id_t block = free_list.back();\n+\t\tblock = free_list.back();\n \t\t// erase the entry from the free list again\n \t\tfree_list.pop_back();\n-\t\treturn block;\n+\t} else {\n+\t\tblock = max_block++;\n \t}\n-\treturn max_block++;\n+\tused_blocks.insert(block);\n+\treturn block;\n }\n \n block_id_t SingleFileBlockManager::GetMetaBlock() {\n@@ -133,7 +141,7 @@ unique_ptr<Block> SingleFileBlockManager::CreateBlock() {\n \n void SingleFileBlockManager::Read(Block &block) {\n \tassert(block.id >= 0);\n-\tused_blocks.insert(block.id);\n+\tassert(std::find(free_list.begin(), free_list.end(), block.id) == free_list.end());\n \tblock.Read(*handle, BLOCK_START + block.id * Storage::BLOCK_ALLOC_SIZE);\n }\n \n@@ -147,13 +155,24 @@ void SingleFileBlockManager::WriteHeader(DatabaseHeader header) {\n \theader.iteration = ++iteration_count;\n \theader.block_count = max_block;\n \t// now handle the free list\n-\tif (used_blocks.size() > 0) {\n+\tfree_list.clear();\n+\tfor(block_id_t i = 0; i < max_block; i++) {\n+\t\tif (used_blocks.find(i) == used_blocks.end()) {\n+\t\t\tfree_list.push_back(i);\n+\t\t}\n+\t}\n+\tif (free_list.size() > 0) {\n \t\t// there are blocks in the free list\n \t\t// write them to the file\n \t\tMetaBlockWriter writer(*this);\n+\t\tauto entry = std::find(free_list.begin(), free_list.end(), writer.block->id);\n+\t\tif (entry != free_list.end()) {\n+\t\t\tfree_list.erase(entry);\n+\t\t}\n \t\theader.free_list = writer.block->id;\n-\t\twriter.Write<uint64_t>(used_blocks.size());\n-\t\tfor (auto &block_id : used_blocks) {\n+\n+\t\twriter.Write<uint64_t>(free_list.size());\n+\t\tfor (auto &block_id : free_list) {\n \t\t\twriter.Write<block_id_t>(block_id);\n \t\t}\n \t\twriter.Flush();\n@@ -178,6 +197,7 @@ void SingleFileBlockManager::WriteHeader(DatabaseHeader header) {\n \thandle->Sync();\n \n \t// the free list is now equal to the blocks that were used by the previous iteration\n+\tfree_list.clear();\n \tfor (auto &block_id : used_blocks) {\n \t\tfree_list.push_back(block_id);\n \t}\ndiff --git a/src/storage/string_segment.cpp b/src/storage/string_segment.cpp\nindex 53c372f4e6c9..d212a0da5678 100644\n--- a/src/storage/string_segment.cpp\n+++ b/src/storage/string_segment.cpp\n@@ -182,6 +182,7 @@ string_location_t StringSegment::FetchStringLocation(data_ptr_t baseptr, int32_t\n \n string_t StringSegment::FetchStringFromDict(buffer_handle_set_t &handles, data_ptr_t baseptr, int32_t dict_offset) {\n \t// fetch base data\n+\tassert(dict_offset <= Storage::BLOCK_SIZE);\n \tstring_location_t location = FetchStringLocation(baseptr, dict_offset);\n \treturn FetchString(handles, baseptr, location);\n }\n@@ -371,6 +372,7 @@ void StringSegment::AppendData(SegmentStatistics &stats, data_ptr_t target, data\n \t\t\t\tmemcpy(dict_pos + sizeof(uint16_t), sdata[source_idx].GetData(), string_length + 1);\n \t\t\t}\n \t\t\t// place the dictionary offset into the set of vectors\n+\t\t\tassert(dictionary_offset <= Storage::BLOCK_SIZE);\n \t\t\tresult_data[target_idx] = dictionary_offset;\n \t\t}\n \t\tremaining_strings--;\ndiff --git a/src/storage/uncompressed_segment.cpp b/src/storage/uncompressed_segment.cpp\nindex 9465384e03b0..5a0034a377df 100644\n--- a/src/storage/uncompressed_segment.cpp\n+++ b/src/storage/uncompressed_segment.cpp\n@@ -18,6 +18,21 @@ UncompressedSegment::~UncompressedSegment() {\n \t}\n }\n \n+void UncompressedSegment::Verify(Transaction &transaction) {\n+#ifdef DEBUG\n+\tColumnScanState state;\n+\tInitializeScan(state);\n+\n+\tVector result(this->type);\n+\tfor(idx_t i = 0; i < this->tuple_count; i+= STANDARD_VECTOR_SIZE) {\n+\t\tidx_t vector_idx = i / STANDARD_VECTOR_SIZE;\n+\t\tidx_t count = std::min((idx_t) STANDARD_VECTOR_SIZE, tuple_count - i);\n+\t\tScan(transaction, state, vector_idx, result);\n+\t\tresult.Verify(count);\n+\t}\n+#endif\n+}\n+\n static void CheckForConflicts(UpdateInfo *info, Transaction &transaction, row_t *ids, idx_t count, row_t offset,\n                               UpdateInfo *&node) {\n \tif (info->version_number == transaction.transaction_id) {\ndiff --git a/tools/pythonpkg/setup.py b/tools/pythonpkg/setup.py\nindex 2c5822a8f86b..ab7c08bab56e 100755\n--- a/tools/pythonpkg/setup.py\n+++ b/tools/pythonpkg/setup.py\n@@ -17,13 +17,16 @@\n # make sure we are in the right directory\n os.chdir(os.path.dirname(os.path.realpath(__file__)))\n \n-if not os.path.exists('duckdb.cpp'):\n+# check if amalgamation exists\n+if os.path.isfile(os.path.join('..', '..', 'scripts', 'amalgamation.py')):\n     prev_wd = os.getcwd()\n-    os.chdir('../..')\n-    subprocess.Popen('python scripts/amalgamation.py'.split(' ')).wait()\n+    target_header = os.path.join(prev_wd, 'duckdb.hpp')\n+    target_source = os.path.join(prev_wd, 'duckdb.cpp')\n+    os.chdir(os.path.join('..', '..'))\n+    sys.path.append('scripts')\n+    import amalgamation\n+    amalgamation.generate_amalgamation(target_source, target_header)\n     os.chdir(prev_wd)\n-    shutil.copyfile('../../src/amalgamation/duckdb.hpp', 'duckdb.hpp')\n-    shutil.copyfile('../../src/amalgamation/duckdb.cpp', 'duckdb.cpp')\n \n \n toolchain_args = ['-std=c++11']\n@@ -66,7 +69,7 @@ def __str__(self):\n     url=\"https://www.duckdb.org\",\n     long_description = '',\n     install_requires=[ # these versions are still available for Python 2, newer ones aren't\n-         'numpy>=1.14', \n+         'numpy>=1.14',\n          'pandas>=0.23',\n     ],\n     packages=['duckdb_query_graph'],\ndiff --git a/tools/rpkg/configure b/tools/rpkg/configure\nindex 6d353abed215..edba220523c6 100755\n--- a/tools/rpkg/configure\n+++ b/tools/rpkg/configure\n@@ -1,12 +1,7 @@\n #!/bin/sh\n \n-if [ ! -f \"src/duckdb.cpp\" ]; then\n-\tif [ ! -f \"../../scripts/amalgamation.py\" ]; then\n-\t\techo \"Could find neither duckdb.cpp nor the build script\"\n-\t\texit 1\n-\tfi\n-\t(cd ../.. && python scripts/amalgamation.py)\n-\tcat ../../src/amalgamation/duckdb.cpp | sed 's|#include \"duckdb.hpp\"|#include \"duckdb.h\"|' > src/duckdb.cpp\n-\tcp ../../src/amalgamation/duckdb.hpp src/duckdb.h\n-\n+if [ ! -f \"../../scripts/amalgamation.py\" ]; then\n+\techo \"Could find neither the amalgamation build script\"\n+\texit 1\n fi\n+(cd ../.. && python scripts/amalgamation.py --source=tools/rpkg/src/duckdb.cpp --header=tools/rpkg/src/duckdb.h)\n",
  "test_patch": "diff --git a/scripts/test_compile.py b/scripts/test_compile.py\nnew file mode 100644\nindex 000000000000..786c4361eb57\n--- /dev/null\n+++ b/scripts/test_compile.py\n@@ -0,0 +1,53 @@\n+import os, sys, amalgamation, pickle\n+\n+# where to cache which files have already been compiled, only used for --compile --resume\n+cache_file = 'amalgamation.cache'\n+\n+resume = False\n+\n+for arg in sys.argv:\n+\tif arg == '--resume':\n+\t\tresume = True\n+\n+if not resume:\n+\ttry:\n+\t\tos.remove(cache_file)\n+\texcept:\n+\t\tpass\n+\n+def try_compilation(fpath, cache):\n+\tif fpath in cache:\n+\t\treturn\n+\tprint(fpath)\n+\n+\tcmd = 'clang++ -std=c++11 -Wno-deprecated -Wno-writable-strings -S -MMD -MF dependencies.d -o deps.s ' + fpath + ' ' + ' '.join([\"-I\" + x for x in amalgamation.include_paths])\n+\tret = os.system(cmd)\n+\tif ret != 0:\n+\t\traise Exception('Failed compilation of file \"' + fpath + '\"!\\n Command: ' + cmd)\n+\tcache[fpath] = True\n+\twith open(cache_file, 'wb') as cf:\n+\t\tpickle.dump(cache, cf)\n+\n+def compile_dir(dir, cache):\n+\tfiles = os.listdir(dir)\n+\tfiles.sort()\n+\tfor fname in files:\n+\t\tif fname in amalgamation.excluded_compilation_files:\n+\t\t\tcontinue\n+\t\tfpath = os.path.join(dir, fname)\n+\t\tif os.path.isdir(fpath):\n+\t\t\tcompile_dir(fpath, cache)\n+\t\telif fname.endswith('.cpp') or fname.endswith('.hpp') or fname.endswith('.c') or fname.endswith('.cc'):\n+\t\t\ttry_compilation(fpath, cache)\n+\n+# compilation pass only\n+# compile all files in the src directory (including headers!) individually\n+try:\n+\twith open(cache_file, 'rb') as cf:\n+\t\tcache = pickle.load(cf)\n+except:\n+\tcache = {}\n+\n+for cdir in amalgamation.compile_directories:\n+\tcompile_dir(cdir, cache)\n+\ndiff --git a/test/sql/storage/CMakeLists.txt b/test/sql/storage/CMakeLists.txt\nindex 15d275b59950..39190f6bff2b 100644\n--- a/test/sql/storage/CMakeLists.txt\n+++ b/test/sql/storage/CMakeLists.txt\n@@ -14,6 +14,7 @@ if(NOT WIN32 AND NOT SUN)\n                     test_store_alter.cpp\n                     test_views.cpp\n                     test_readonly.cpp\n+                    test_repeated_checkpoint.cpp\n                     test_storage_tpch.cpp\n                     test_storage_scan.cpp\n                     test_database_size.cpp)\n@@ -27,6 +28,7 @@ else()\n                     test_storage_sequences.cpp\n                     test_shutdown.cpp\n                     test_big_storage.cpp\n+                    test_repeated_checkpoint.cpp\n                     test_storage.cpp\n                     test_storage_defaults.cpp\n                     test_store_alter.cpp\ndiff --git a/test/sql/storage/test_repeated_checkpoint.cpp b/test/sql/storage/test_repeated_checkpoint.cpp\nnew file mode 100644\nindex 000000000000..487b4fdc8a36\n--- /dev/null\n+++ b/test/sql/storage/test_repeated_checkpoint.cpp\n@@ -0,0 +1,65 @@\n+#include \"catch.hpp\"\n+#include \"duckdb/common/file_system.hpp\"\n+#include \"test_helpers.hpp\"\n+#include \"duckdb/storage/storage_info.hpp\"\n+\n+#include <fstream>\n+\n+using namespace duckdb;\n+using namespace std;\n+\n+TEST_CASE(\"Test repeated load and checkpoint of storage\", \"[storage][.]\") {\n+\tunique_ptr<MaterializedQueryResult> result;\n+\tauto storage_database = TestCreatePath(\"repeated_load\");\n+\tauto csv_file = TestCreatePath(\"rload.csv\");\n+\tauto config = GetTestConfig();\n+\n+\tvector<string> model { \"M11\", \"F22\", \"U33\" };\n+\tvector<string> shop { \"www.goodshop.com\", \"www.badshop.com\" };\n+\tvector<string> name { \"Electronics  Something  One\", \"Electronics  Something  Two\", \"Electronics  Something  Three\", \"Electronics  Something  Four\", \"Electronics  Something  Five\", \"Electronics  Something  Six\", \"Electronics  Something  Seven\", \"Electronics  Something  Eight\", \"Electronics  Something  Nine\", \"Electronics  Something  Ten\"};\n+\tvector<string> brand { \"AAAAA\", \"BBBBB\", \"CCCC\", \"DDDDDD\", \"PPPP\" };\n+\tvector<string> color { \"violet\", \"indigo\", \"blue\", \"green\", \"yellow\", \"orange\", \"red\" };\n+\tidx_t row_count = 1000;\n+\n+\tDeleteDatabase(storage_database);\n+\tfor(idx_t counter = 0; counter < 100; counter++) {\n+\t\tDuckDB db(storage_database);\n+\t\tConnection con(db);\n+\n+\t\tif (counter > 0) {\n+\t\t\tresult = con.Query(\"SELECT COUNT(*) FROM pdata\");\n+\t\t\tREQUIRE(CHECK_COLUMN(result, 0, {Value::BIGINT(counter * row_count)}));\n+\t\t}\n+\t\t// generate the csv file\n+\t\tofstream csv_writer(csv_file);\n+\t\tfor(idx_t i = 0; i < row_count; i++) {\n+\t\t\tidx_t z = i + counter;\n+\t\t\tidx_t record_id = i + (row_count * counter);\n+\t\t\tcsv_writer << record_id << \"|\";\n+\t\t\tcsv_writer << i % 99 << \"|\";\n+\t\t\tcsv_writer << shop[z % 2] << \"|\";\n+\t\t\tcsv_writer << \"electronics\" << \"|\";\n+\t\t\tcsv_writer << name[z % 10] << \"|\";\n+\t\t\tcsv_writer << brand[z % 5] << \"|\";\n+\t\t\tcsv_writer << color[z % 7] << \"|\";\n+\t\t\tcsv_writer << model[z % 3] << \"|\";\n+\t\t\tcsv_writer << \"\\n\";\n+\t\t}\n+\t\tcsv_writer.close();\n+\t\t// create and load the table\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE IF NOT EXISTS pdata (record_id BIGINT PRIMARY KEY , price DOUBLE, shop VARCHAR, category VARCHAR, name VARCHAR, brand VARCHAR, color VARCHAR, model VARCHAR);\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"COPY pdata(record_id,price,shop,category,name,brand,color,model) FROM '\" + csv_file + \"' ( DELIMITER '|' );\"));\n+\t\tresult = con.Query(\"SELECT MIN(record_id), MIN(price), MIN(shop), MIN(category), MIN(name), MIN(brand), MIN(color), MIN(model) FROM pdata\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {0}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {0}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 2, {\"www.badshop.com\"}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 3, {\"electronics\"}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 4, {\"Electronics  Something  Eight\"}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 5, {\"AAAAA\"}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 6, {\"blue\"}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 7, {\"F22\"}));\n+\n+\t\tresult = con.Query(\"SELECT COUNT(*) FROM pdata\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {Value::BIGINT((counter + 1) * row_count)}));\n+\t}\n+}\n",
  "problem_statement": "DuckDB example C++ program crashes for persistent database\nI was checking out duckDB using the simple example program (on ubuntu 18),\r\n\r\n```\r\n#include \"duckdb.hpp\"\r\n  \r\nusing namespace duckdb;\r\n\r\nint main() {\r\n  DuckDB db(\"./data/pdata\");\r\n  Connection con(db);\r\n  con.Query(\"CREATE TABLE IF NOT EXISTS pdata (record_id BIGINT PRIMARY KEY , price DOUBLE, shop VARCHAR, category VARCHAR, name VARCHAR, brand VARCHAR, color VARCHAR, model VARCHAR);\");\r\n  auto result = con.Query(\"COPY pdata(record_id,price,shop,category,name,brand,color,model) FROM 'items.txt' ( DELIMITER '|' );\");\r\n  result->Print();\r\n}\r\n```\r\nI tried to load a few csv files , each with about 10,000 records. After the fourth run, it crashes saying\r\n\r\n```\r\nterminate called after throwing an instance of 'duckdb::Exception'\r\n  what():  Column length mismatch in table load!\r\nCommand terminated by signal 6\r\n```\r\nIncidentally , if I load a million records at a time, it still crashes after 5-6 iterations. \r\nMemory usage is not high.\r\nWhat am I doing wrong please ?\n",
  "hints_text": "",
  "created_at": "2020-04-16T09:45:11Z"
}