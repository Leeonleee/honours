You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
DuckDB example C++ program crashes for persistent database
I was checking out duckDB using the simple example program (on ubuntu 18),

```
#include "duckdb.hpp"
  
using namespace duckdb;

int main() {
  DuckDB db("./data/pdata");
  Connection con(db);
  con.Query("CREATE TABLE IF NOT EXISTS pdata (record_id BIGINT PRIMARY KEY , price DOUBLE, shop VARCHAR, category VARCHAR, name VARCHAR, brand VARCHAR, color VARCHAR, model VARCHAR);");
  auto result = con.Query("COPY pdata(record_id,price,shop,category,name,brand,color,model) FROM 'items.txt' ( DELIMITER '|' );");
  result->Print();
}
```
I tried to load a few csv files , each with about 10,000 records. After the fourth run, it crashes saying

```
terminate called after throwing an instance of 'duckdb::Exception'
  what():  Column length mismatch in table load!
Command terminated by signal 6
```
Incidentally , if I load a million records at a time, it still crashes after 5-6 iterations. 
Memory usage is not high.
What am I doing wrong please ?

</issue>
<code>
[start of README.md]
1: <img align="left" src="logo/duckdb-logo.png" height="120">
2: 
3: # DuckDB, the SQLite for Analytics
4: [![Travis](https://api.travis-ci.org/cwida/duckdb.svg?branch=master)](https://travis-ci.org/cwida/duckdb)
5: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
6: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
7: 
8: <br>
9: 
10: 
11: # Requirements
12: DuckDB requires [CMake](https://cmake.org) to be installed and a `C++11` compliant compiler. GCC 4.9 and newer, Clang 3.9 and newer and VisualStudio 2017 are tested on each revision.
13: 
14: ## Compiling
15: Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You may run `make unit` and `make allunit` to verify that your version works properly after making changes.
16: 
17: # Usage
18: A command line utility based on `sqlite3` can be found in either `build/release/duckdb_cli` (release, the default) or `build/debug/duckdb_cli` (debug).
19: 
20: # Embedding
21: As DuckDB is an embedded database, there is no database server to launch or client to connect to a running server. However, the database server can be embedded directly into an application using the C or C++ bindings. The main build process creates the shared library `build/release/src/libduckdb.[so|dylib|dll]` that can be linked against. A static library is built as well.
22: 
23: For examples on how to embed DuckDB into your application, see the [examples](https://github.com/cwida/duckdb/tree/master/examples) folder.
24: 
25: ## Benchmarks
26: After compiling, benchmarks can be executed from the root directory by executing `./build/release/benchmark/benchmark_runner`.
27: 
28: ## Standing on the Shoulders of Giants
29: DuckDB is implemented in C++ 11, should compile with GCC and clang, uses CMake to build and [Catch2](https://github.com/catchorg/Catch2) for testing. DuckDB uses some components from various Open-Source databases and draws inspiration from scientific publications. Here is an overview:
30: 
31: * Parser: We use the PostgreSQL parser that was [repackaged as a stand-alone library](https://github.com/lfittl/libpg_query). The translation to our own parse tree is inspired by [Peloton](https://pelotondb.io).
32: * Shell: We have adapted the [SQLite shell](https://sqlite.org/cli.html) to work with DuckDB.
33: * Tests: We use the [SQL Logic Tests from SQLite](https://www.sqlite.org/sqllogictest/doc/trunk/about.wiki) to test DuckDB.
34: * Query fuzzing: We use [SQLsmith](https://github.com/anse1/sqlsmith) to generate random queries for additional testing.
35: * Date Math: We use the date math component from [MonetDB](https://www.monetdb.org).
36: * SQL Window Functions: DuckDB's window functions implementation uses Segment Tree Aggregation as described in the paper "Efficient Processing of Window Functions in Analytical SQL Queries" by Viktor Leis, Kan Kundhikanjana, Alfons Kemper and Thomas Neumann.
37: * Execution engine: The vectorized execution engine is inspired by the paper "MonetDB/X100: Hyper-Pipelining Query Execution" by Peter Boncz, Marcin Zukowski and Niels Nes.
38: * Optimizer: DuckDB's optimizer draws inspiration from the papers "Dynamic programming strikes back" by Guido Moerkotte and Thomas Neumman as well as "Unnesting Arbitrary Queries" by Thomas Neumann and Alfons Kemper.
39: * Concurrency control: Our MVCC implementation is inspired by the paper "Fast Serializable Multi-Version Concurrency Control for Main-Memory Database Systems" by Thomas Neumann, Tobias Mühlbauer and Alfons Kemper.
40: * Regular Expression: DuckDB uses Google's [RE2](https://github.com/google/re2) regular expression engine.
41: 
42: ## Other pages
43: * [Continuous Benchmarking (CB™)](https://www.duckdb.org/benchmarks/index.html), runs TPC-H, TPC-DS and some microbenchmarks on every commit
[end of README.md]
[start of CMakeLists.txt]
1: cmake_minimum_required(VERSION 2.8.7)
2: # if(APPLE) # needs to be before project() SET(CMAKE_CXX_FLAGS -stdlib=libc++)
3: # SET(CMAKE_OSX_DEPLOYMENT_TARGET "10.7" CACHE STRING "Minimum OS X deployment
4: # version") endif()
5: 
6: project(DuckDB)
7: 
8: set(DUCKDB_MAJOR_VERSION 0)
9: set(DUCKDB_MINOR_VERSION 1)
10: set(DUCKDB_PATCH_VERSION 1)
11: set(DUCKDB_VERSION
12:     ${DUCKDB_MAJOR_VERSION}.${DUCKDB_MINOR_VERSION}.${DUCKDB_PATCH_VERSION})
13: 
14: find_package(Threads REQUIRED)
15: 
16: set(CMAKE_CXX_STANDARD 11)
17: set(CMAKE_CXX_STANDARD_REQUIRED ON)
18: set(CMAKE_CXX_EXTENSIONS OFF)
19: 
20: set(CMAKE_VERBOSE_MAKEFILE OFF)
21: set(CMAKE_POSITION_INDEPENDENT_CODE ON)
22: set(CMAKE_MACOSX_RPATH 1)
23: 
24: find_program(CCACHE_PROGRAM ccache)
25: if(CCACHE_PROGRAM)
26:   set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE "${CCACHE_PROGRAM}")
27: endif()
28: 
29: # Determine install paths
30: set(INSTALL_LIB_DIR lib CACHE PATH "Installation directory for libraries")
31: set(INSTALL_BIN_DIR bin CACHE PATH "Installation directory for executables")
32: set(INSTALL_INCLUDE_DIR
33:     include
34:     CACHE PATH "Installation directory for header files")
35: if(WIN32 AND NOT CYGWIN)
36:   set(DEF_INSTALL_CMAKE_DIR cmake)
37: else()
38:   set(DEF_INSTALL_CMAKE_DIR lib/cmake/DuckDB)
39: endif()
40: set(INSTALL_CMAKE_DIR
41:     ${DEF_INSTALL_CMAKE_DIR}
42:     CACHE PATH "Installation directory for CMake files")
43: set(DUCKDB_EXPORT_SET "DuckDBExports")
44: 
45: # Make relative install paths absolute
46: foreach(p
47:         LIB
48:         BIN
49:         INCLUDE
50:         CMAKE)
51:   set(var INSTALL_${p}_DIR)
52:   if(NOT IS_ABSOLUTE "${${var}}")
53:     set(${var} "${CMAKE_INSTALL_PREFIX}/${${var}}")
54:   endif()
55: endforeach()
56: 
57: set(M32_FLAG "")
58: if(FORCE_32_BIT)
59:   set(M32_FLAG " -m32 ")
60: endif()
61: 
62: option(FORCE_COLORED_OUTPUT
63:        "Always produce ANSI-colored output (GNU/Clang only)." FALSE)
64: if(${FORCE_COLORED_OUTPUT})
65:   if("${CMAKE_CXX_COMPILER_ID}" STREQUAL "GNU")
66:     add_compile_options(-fdiagnostics-color=always)
67:   elseif("${CMAKE_CXX_COMPILER_ID}" STREQUAL "Clang")
68:     add_compile_options(-fcolor-diagnostics)
69:   endif()
70: endif()
71: 
72: option(ENABLE_SANITIZER "Enable sanitizer." TRUE)
73: if(${ENABLE_SANITIZER})
74:   set(CXX_EXTRA_DEBUG "${CXX_EXTRA_DEBUG} -fsanitize=address")
75: endif()
76: 
77: option(EXPLICIT_EXCEPTIONS "Explicitly enable C++ exceptions." FALSE)
78: if(${EXPLICIT_EXCEPTIONS})
79:   set(CXX_EXTRA "${CXX_EXTRA} -fexceptions")
80: endif()
81: 
82: set(SUN FALSE)
83: if(${CMAKE_SYSTEM_NAME} STREQUAL "SunOS")
84:   set(CXX_EXTRA "${CXX_EXTRA} -mimpure-text")
85:   add_definitions(-DSUN=1)
86:   set(SUN TRUE)
87: endif()
88: 
89: execute_process(COMMAND git
90:                         log
91:                         -1
92:                         --format=%h
93:                 WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}
94:                 OUTPUT_VARIABLE GIT_COMMIT_HASH
95:                 OUTPUT_STRIP_TRAILING_WHITESPACE)
96: add_definitions(-DDUCKDB_SOURCE_ID="\""${GIT_COMMIT_HASH}"\"")
97: 
98: option(AMALGAMATION_BUILD
99:        "Build from the amalgamation files, rather than from the normal sources."
100:        FALSE)
101: 
102: option(JDBC_DRIVER "Build the DuckDB JDBC driver" FALSE)
103: 
104: option(TREAT_WARNINGS_AS_ERRORS "Treat warnings as errors" FALSE)
105: 
106: if(TREAT_WARNINGS_AS_ERRORS)
107:   message("Treating warnings as errors.")
108: endif()
109: 
110: if(NOT MSVC)
111:   set(CMAKE_CXX_FLAGS_DEBUG
112:       "${CMAKE_CXX_FLAGS_DEBUG} -g -O0 -DDEBUG -Wall ${M32_FLAG} ${CXX_EXTRA}")
113:   set(CMAKE_CXX_FLAGS_RELEASE
114:       "${CMAKE_CXX_FLAGS_RELEASE} -O3 -DNDEBUG ${M32_FLAG} ${CXX_EXTRA}")
115:   set(CMAKE_CXX_FLAGS_RELWITHDEBINFO "${CMAKE_CXX_FLAGS_RELEASE} -g")
116: 
117:   set(
118:     CXX_EXTRA_DEBUG
119:     "${CXX_EXTRA_DEBUG} -Wunused-variable -Wunused-const-variable -Werror=vla -Wnarrowing"
120:     )
121:   if(TREAT_WARNINGS_AS_ERRORS)
122:     set(CXX_EXTRA_DEBUG "${CXX_EXTRA_DEBUG} -Werror")
123:   endif()
124: 
125:   if("${CMAKE_CXX_COMPILER_ID}" STREQUAL "GNU"
126:      AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 8.0)
127:     set(CMAKE_CXX_FLAGS_DEBUG "${CMAKE_CXX_FLAGS_DEBUG} ${CXX_EXTRA_DEBUG}")
128:   elseif("${CMAKE_CXX_COMPILER_ID}" STREQUAL "Clang"
129:          AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0)
130:     set(CMAKE_CXX_FLAGS_DEBUG "${CMAKE_CXX_FLAGS_DEBUG} ${CXX_EXTRA_DEBUG}")
131:   else()
132:     message(WARNING "Please use a recent compiler for debug builds")
133:   endif()
134: else()
135:   set(CMAKE_CXX_WINDOWS_FLAGS
136:       "/wd4244 /wd4267 /wd4200 /wd26451 /wd26495 /D_CRT_SECURE_NO_WARNINGS")
137:   if(TREAT_WARNINGS_AS_ERRORS)
138:     set(CMAKE_CXX_WINDOWS_FLAGS "${CMAKE_CXX_WINDOWS_FLAGS} /WX")
139:   endif()
140:   # remove warning from CXX flags
141:   string(REGEX
142:          REPLACE "/W[0-4]"
143:                  ""
144:                  CMAKE_CXX_FLAGS
145:                  "${CMAKE_CXX_FLAGS}")
146:   # add to-be-ignored warnings
147:   set(
148:     CMAKE_CXX_FLAGS
149:     "${CMAKE_CXX_FLAGS} /wd4244 /wd4267 /wd4200 /wd26451 /wd26495 /D_CRT_SECURE_NO_WARNINGS"
150:     )
151: endif()
152: 
153: # todo use CHECK_CXX_COMPILER_FLAG(-fsanitize=address SUPPORTS_SANITIZER) etc.
154: 
155: set(CMAKE_C_FLAGS_DEBUG "${CMAKE_CXX_FLAGS_DEBUG}")
156: set(CMAKE_C_FLAGS_RELEASE "${CMAKE_CXX_FLAGS_RELEASE}")
157: set(CMAKE_C_FLAGS_RELWITHDEBINFO "${CMAKE_CXX_FLAGS_RELWITHDEBINFO}")
158: 
159: if(NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES)
160:   set(DEFAULT_BUILD_TYPE "Release")
161:   message(STATUS "Setting build type to '${DEFAULT_BUILD_TYPE}'.")
162:   set(CMAKE_BUILD_TYPE
163:       "${DEFAULT_BUILD_TYPE}"
164:       CACHE STRING "Choose the type of build." FORCE)
165: endif()
166: 
167: include_directories(src/include)
168: include_directories(third_party/fmt/include)
169: include_directories(third_party/hyperloglog)
170: include_directories(third_party/re2)
171: include_directories(third_party/miniz)
172: include_directories(third_party/utf8proc/include)
173: include_directories(third_party/miniparquet)
174: 
175: # todo only regenerate ub file if one of the input files changed hack alert
176: function(enable_unity_build UB_SUFFIX SOURCE_VARIABLE_NAME)
177:   set(files ${${SOURCE_VARIABLE_NAME}})
178: 
179:   # Generate a unique filename for the unity build translation unit
180:   set(unit_build_file ${CMAKE_CURRENT_BINARY_DIR}/ub_${UB_SUFFIX}.cpp)
181:   # Exclude all translation units from compilation
182:   set_source_files_properties(${files}
183:                               PROPERTIES
184:                               HEADER_FILE_ONLY
185:                               true)
186: 
187:   set(rebuild FALSE)
188:   foreach(source_file ${files})
189:     if(${CMAKE_CURRENT_SOURCE_DIR}/${source_file} IS_NEWER_THAN
190:        ${unit_build_file})
191:       set(rebuild TRUE)
192:     endif()
193:   endforeach(source_file)
194: 
195:   if(${rebuild})
196:     file(WRITE ${unit_build_file} "// Unity Build generated by CMake\n")
197:     foreach(source_file ${files})
198:       file(
199:         APPEND ${unit_build_file}
200:         "#line 0 \"${source_file}\"\n#include <${CMAKE_CURRENT_SOURCE_DIR}/${source_file}>\n"
201:         )
202:     endforeach(source_file)
203:   endif()
204: 
205:   # Complement list of translation units with the name of ub
206:   set(${SOURCE_VARIABLE_NAME}
207:       ${${SOURCE_VARIABLE_NAME}} ${unit_build_file}
208:       PARENT_SCOPE)
209: endfunction(enable_unity_build)
210: 
211: function(add_library_unity NAME MODE)
212:   set(SRCS ${ARGN})
213:   if(NOT DISABLE_UNITY)
214:     enable_unity_build(${NAME} SRCS)
215:   endif()
216:   add_library(${NAME} OBJECT ${SRCS})
217: endfunction()
218: 
219: function(disable_target_warnings NAME)
220:   if(MSVC)
221:     target_compile_options(${NAME} PRIVATE "/W0")
222:   elseif("${CMAKE_CXX_COMPILER_ID}" STREQUAL "Clang"
223:          OR "${CMAKE_CXX_COMPILER_ID}" STREQUAL "GNU")
224:     target_compile_options(${NAME} PRIVATE "-w")
225:   endif()
226: endfunction()
227: 
228: add_subdirectory(src)
229: add_subdirectory(third_party)
230: add_subdirectory(test)
231: add_subdirectory(tools)
232: if(NOT WIN32 AND NOT SUN)
233:   add_subdirectory(benchmark)
234: endif()
235: 
236: # Write the export set for build and install tree
237: install(EXPORT "${DUCKDB_EXPORT_SET}" DESTINATION "${INSTALL_CMAKE_DIR}")
238: export(EXPORT "${DUCKDB_EXPORT_SET}"
239:        FILE "${PROJECT_BINARY_DIR}/${DUCKDB_EXPORT_SET}.cmake")
240: 
241: # Only write the cmake package configuration if the templates exist
242: set(CMAKE_CONFIG_TEMPLATE "${CMAKE_SOURCE_DIR}/DuckDBConfig.cmake.in")
243: set(CMAKE_CONFIG_VERSION_TEMPLATE
244:     "${CMAKE_SOURCE_DIR}/DuckDBConfigVersion.cmake.in")
245: if(EXISTS ${CMAKE_CONFIG_TEMPLATE} AND EXISTS ${CMAKE_CONFIG_VERSION_TEMPLATE})
246: 
247:   # Configure cmake package config for the build tree
248:   set(CONF_INCLUDE_DIRS "${PROJECT_SOURCE_DIR}/src/include")
249:   configure_file(${CMAKE_CONFIG_TEMPLATE}
250:                  "${PROJECT_BINARY_DIR}/DuckDBConfig.cmake" @ONLY)
251: 
252:   # Configure cmake package config for the install tree
253:   file(RELATIVE_PATH
254:        REL_INCLUDE_DIR
255:        "${INSTALL_CMAKE_DIR}"
256:        "${INSTALL_INCLUDE_DIR}")
257:   set(CONF_INCLUDE_DIRS "\${DuckDB_CMAKE_DIR}/${REL_INCLUDE_DIR}")
258:   configure_file(
259:     ${CMAKE_CONFIG_TEMPLATE}
260:     "${PROJECT_BINARY_DIR}${CMAKE_FILES_DIRECTORY}/DuckDBConfig.cmake" @ONLY)
261: 
262:   # Configure cmake package version for build and install tree
263:   configure_file(${CMAKE_CONFIG_VERSION_TEMPLATE}
264:                  "${PROJECT_BINARY_DIR}/DuckDBConfigVersion.cmake" @ONLY)
265: 
266:   # Install the cmake package
267:   install(
268:     FILES "${PROJECT_BINARY_DIR}${CMAKE_FILES_DIRECTORY}/DuckDBConfig.cmake"
269:           "${PROJECT_BINARY_DIR}/DuckDBConfigVersion.cmake"
270:     DESTINATION "${INSTALL_CMAKE_DIR}")
271: endif()
[end of CMakeLists.txt]
[start of Makefile]
1: .PHONY: all opt unit clean debug release test unittest allunit docs doxygen format sqlite imdb
2: 
3: all: release
4: opt: release
5: unit: unittest
6: imdb: third_party/imdb/data
7: 
8: GENERATOR=
9: FORCE_COLOR=
10: WARNINGS_AS_ERRORS=
11: DISABLE_UNITY_FLAG=
12: ifeq ($(GEN),ninja)
13: 	GENERATOR=-G "Ninja"
14: 	FORCE_COLOR=-DFORCE_COLORED_OUTPUT=1
15: endif
16: ifeq (${TREAT_WARNINGS_AS_ERRORS}, 1)
17: 	WARNINGS_AS_ERRORS=-DTREAT_WARNINGS_AS_ERRORS=1
18: endif
19: ifeq (${DISABLE_UNITY}, 1)
20: 	DISABLE_UNITY_FLAG=-DDISABLE_UNITY=1
21: endif
22: 
23: clean:
24: 	rm -rf build
25: 
26: debug:
27: 	mkdir -p build/debug && \
28: 	cd build/debug && \
29: 	cmake $(GENERATOR) $(FORCE_COLOR) ${WARNINGS_AS_ERRORS} ${DISABLE_UNITY_FLAG} -DCMAKE_BUILD_TYPE=Debug ../.. && \
30: 	cmake --build .
31: 
32: release:
33: 	mkdir -p build/release && \
34: 	cd build/release && \
35: 	cmake $(GENERATOR) $(FORCE_COLOR) ${WARNINGS_AS_ERRORS} ${DISABLE_UNITY_FLAG} -DCMAKE_BUILD_TYPE=Release ../.. && \
36: 	cmake --build .
37: 
38: unittest: debug
39: 	build/debug/test/unittest
40: 	build/debug/tools/sqlite3_api_wrapper/test_sqlite3_api_wrapper
41: 
42: allunit: release # uses release build because otherwise allunit takes forever
43: 	build/release/test/unittest "*"
44: 
45: docs:
46: 	mkdir -p build/docs && \
47: 	doxygen Doxyfile
48: 
49: doxygen: docs
50: 	open build/docs/html/index.html
51: 
52: amalgamation:
53: 	mkdir -p build/amalgamation && \
54: 	python scripts/amalgamation.py && \
55: 	cd build/amalgamation && \
56: 	cmake $(GENERATOR) $(FORCE_COLOR) -DAMALGAMATION_BUILD=1 -DCMAKE_BUILD_TYPE=Release ../.. && \
57: 	cmake --build .
58: 
59: jdbc:
60: 	mkdir -p build/jdbc && \
61: 	cd build/jdbc && \
62: 	cmake $(GENERATOR) $(FORCE_COLOR) -DJDBC_DRIVER=1 -DCMAKE_BUILD_TYPE=Release ../.. && \
63: 	cmake --build .
64: 
65: jdbcdebug:
66: 	mkdir -p build/jdbcdebug && \
67: 	cd build/jdbcdebug && \
68: 	cmake $(GENERATOR) $(FORCE_COLOR) -DJDBC_DRIVER=1 -DENABLE_SANITIZER=FALSE -DCMAKE_BUILD_TYPE=Debug ../.. && \
69: 	cmake --build .
70: 
71: test_compile: # test compilation of individual cpp files
72: 	python scripts/amalgamation.py --compile
73: 
74: format:
75: 	python3 scripts/format.py
76: 
77: third_party/sqllogictest:
78: 	git clone --depth=1 https://github.com/cwida/sqllogictest.git third_party/sqllogictest
79: 
80: third_party/imdb/data:
81: 	wget -i "http://download.duckdb.org/imdb/list.txt" -P third_party/imdb/data
82: 
83: sqlite: release | third_party/sqllogictest
84: 	git --git-dir third_party/sqllogictest/.git pull
85: 	./build/release/test/unittest "[sqlitelogic]"
86: 
87: sqlsmith: debug
88: 	./build/debug/third_party/sqlsmith/sqlsmith --duckdb=:memory:
[end of Makefile]
[start of scripts/amalgamation.py]
1: # this script creates a single header + source file combination out of the DuckDB sources
2: import os, re, sys, pickle
3: amal_dir = os.path.join('src', 'amalgamation')
4: header_file = os.path.join(amal_dir, "duckdb.hpp")
5: source_file = os.path.join(amal_dir, "duckdb.cpp")
6: 
7: src_dir = 'src'
8: include_dir = os.path.join('src', 'include')
9: fmt_dir = os.path.join('third_party', 'fmt')
10: fmt_include_dir = os.path.join('third_party', 'fmt', 'include')
11: hll_dir = os.path.join('third_party', 'hyperloglog')
12: miniz_dir = os.path.join('third_party', 'miniz')
13: re2_dir = os.path.join('third_party', 're2')
14: pg_query_dir = os.path.join('third_party', 'libpg_query')
15: pg_query_include_dir = os.path.join('third_party', 'libpg_query', 'include')
16: 
17: utf8proc_dir = os.path.join('third_party', 'utf8proc')
18: utf8proc_include_dir = os.path.join('third_party', 'utf8proc', 'include')
19: 
20: # files included in the amalgamated "duckdb.hpp" file
21: main_header_files = [os.path.join(include_dir, 'duckdb.hpp'), os.path.join(include_dir, 'duckdb.h'), os.path.join(include_dir, 'duckdb', 'common', 'types', 'date.hpp'), os.path.join(include_dir, 'duckdb', 'common', 'types', 'timestamp.hpp'), os.path.join(include_dir, 'duckdb', 'main', 'appender.hpp'), os.path.join(include_dir, 'duckdb', 'main', 'client_context.hpp'), os.path.join(include_dir, 'duckdb', 'function', 'function.hpp'), os.path.join(include_dir, 'duckdb', 'function', 'table_function.hpp'), os.path.join(include_dir, 'duckdb', 'parser', 'parsed_data', 'create_table_function_info.hpp')]
22: 
23: # include paths for where to search for include files during amalgamation
24: include_paths = [include_dir, fmt_include_dir, hll_dir, re2_dir, miniz_dir, utf8proc_include_dir, utf8proc_dir, pg_query_include_dir, pg_query_dir]
25: # paths of where to look for files to compile and include to the final amalgamation
26: compile_directories = [src_dir, fmt_dir, hll_dir, miniz_dir, re2_dir, utf8proc_dir, pg_query_dir]
27: 
28: # files excluded from the amalgamation
29: excluded_files = ['grammar.cpp', 'grammar.hpp', 'symbols.cpp', 'file_system.cpp']
30: # files excluded from individual file compilation during test_compile
31: excluded_compilation_files = excluded_files + ['gram.hpp', 'kwlist.hpp', "duckdb-c.cpp"]
32: # where to cache which files have already been compiled, only used for --compile --resume
33: cache_file = 'amalgamation.cache'
34: 
35: 
36: linenumbers = False
37: compile = False
38: resume = False
39: 
40: for arg in sys.argv:
41: 	if arg == '--compile':
42: 		compile = True
43: 	elif arg == '--resume':
44: 		resume = True
45: 	elif arg == '--linenumbers':
46: 		linenumbers = True
47: 	elif arg == '--no-linenumbers':
48: 		linenumbers = False
49: 
50: if not resume:
51: 	try:
52: 		os.remove(cache_file)
53: 	except:
54: 		pass
55: 
56: def get_includes(fpath, text):
57: 	# find all the includes referred to in the directory
58: 	include_statements = re.findall("(^[#]include[\t ]+[\"]([^\"]+)[\"])", text, flags=re.MULTILINE)
59: 	include_files = []
60: 	# figure out where they are located
61: 	for included_file in [x[1] for x in include_statements]:
62: 		included_file = os.sep.join(included_file.split('/'))
63: 		found = False
64: 		for include_path in include_paths:
65: 			ipath = os.path.join(include_path, included_file)
66: 			if os.path.isfile(ipath):
67: 				include_files.append(ipath)
68: 				found = True
69: 				break
70: 		if not found:
71: 			raise Exception('Could not find include file "' + included_file + '", included from file "' + fpath + '"')
72: 	return ([x[0] for x in include_statements], include_files)
73: 
74: def cleanup_file(text):
75: 	# remove all "#pragma once" notifications
76: 	text = re.sub('#pragma once', '', text)
77: 	return text
78: 
79: # recursively get all includes and write them
80: written_files = {}
81: 
82: def write_file(current_file, ignore_excluded = False):
83: 	global linenumbers
84: 	global written_files
85: 	if current_file.split(os.sep)[-1] in excluded_files and not ignore_excluded:
86: 		# file is in ignored files set
87: 		return ""
88: 	if current_file in written_files:
89: 		# file is already written
90: 		return ""
91: 	written_files[current_file] = True
92: 
93: 	# first read this file
94: 	with open(current_file, 'r') as f:
95: 		text = f.read()
96: 
97: 	(statements, includes) = get_includes(current_file, text)
98: 	# find the linenr of the final #include statement we parsed
99: 	if len(statements) > 0:
100: 		index = text.find(statements[-1])
101: 		linenr = len(text[:index].split('\n'))
102: 
103: 		# now write all the dependencies of this header first
104: 		for i in range(len(includes)):
105: 			include_text = write_file(includes[i])
106: 			if linenumbers and i == len(includes) - 1:
107: 				# for the last include statement, we also include a #line directive
108: 				include_text += '\n#line %d "%s"\n' % (linenr, current_file)
109: 			text = text.replace(statements[i], include_text)
110: 
111: 	# add the initial line here
112: 	if linenumbers:
113: 		text = '\n#line 1 "%s"\n' % (current_file,) + text
114: 	print(current_file)
115: 	# now read the header and write it
116: 	return cleanup_file(text)
117: 
118: def try_compilation(fpath, cache):
119: 	if fpath in cache:
120: 		return
121: 	print(fpath)
122: 
123: 	cmd = 'clang++ -std=c++11 -Wno-deprecated -Wno-writable-strings -S -MMD -MF dependencies.d -o deps.s ' + fpath + ' ' + ' '.join(["-I" + x for x in include_paths])
124: 	ret = os.system(cmd)
125: 	if ret != 0:
126: 		raise Exception('Failed compilation of file "' + fpath + '"!\n Command: ' + cmd)
127: 	cache[fpath] = True
128: 	with open(cache_file, 'wb') as cf:
129: 		pickle.dump(cache, cf)
130: 
131: def compile_dir(dir, cache):
132: 	files = os.listdir(dir)
133: 	files.sort()
134: 	for fname in files:
135: 		if fname in excluded_compilation_files:
136: 			continue
137: 		fpath = os.path.join(dir, fname)
138: 		if os.path.isdir(fpath):
139: 			compile_dir(fpath, cache)
140: 		elif fname.endswith('.cpp') or fname.endswith('.hpp') or fname.endswith('.c') or fname.endswith('.cc'):
141: 			try_compilation(fpath, cache)
142: 
143: if compile:
144: 	# compilation pass only
145: 	# compile all files in the src directory (including headers!) individually
146: 	try:
147: 		with open(cache_file, 'rb') as cf:
148: 			cache = pickle.load(cf)
149: 	except:
150: 		cache = {}
151: 	for cdir in compile_directories:
152: 		compile_dir(cdir, cache)
153: 	exit(0)
154: 
155: 
156: if not os.path.exists(amal_dir):
157: 	os.makedirs(amal_dir)
158: 
159: # now construct duckdb.hpp from these headers
160: print("-----------------------")
161: print("-- Writing duckdb.hpp --")
162: print("-----------------------")
163: with open(header_file, 'w+') as hfile:
164: 	hfile.write("#pragma once\n")
165: 	for fpath in main_header_files:
166: 		hfile.write(write_file(fpath))
167: 
168: def write_dir(dir, sfile):
169: 	files = os.listdir(dir)
170: 	files.sort()
171: 	for fname in files:
172: 		if fname in excluded_files:
173: 			continue
174: 		fpath = os.path.join(dir, fname)
175: 		if os.path.isdir(fpath):
176: 			write_dir(fpath, sfile)
177: 		elif fname.endswith('.cpp') or fname.endswith('.c') or fname.endswith('.cc'):
178: 			sfile.write(write_file(fpath))
179: 
180: # now construct duckdb.cpp
181: print("------------------------")
182: print("-- Writing duckdb.cpp --")
183: print("------------------------")
184: 
185: # scan all the .cpp files
186: with open(source_file, 'w+') as sfile:
187: 	sfile.write('#include "duckdb.hpp"\n\n')
188: 	for compile_dir in compile_directories:
189: 		write_dir(compile_dir, sfile)
190: 	# for windows we write file_system.cpp last
191: 	# this is because it includes windows.h which contains a lot of #define statements that mess up the other code
192: 	sfile.write(write_file(os.path.join('src', 'common', 'file_system.cpp'), True))
[end of scripts/amalgamation.py]
[start of src/include/duckdb/storage/block_manager.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/block_manager.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/storage/block.hpp"
13: #include "duckdb/storage/storage_info.hpp"
14: 
15: namespace duckdb {
16: //! BlockManager is an abstract representation to manage blocks on DuckDB. When writing or reading blocks, the
17: //! BlockManager creates and accesses blocks. The concrete types implements how blocks are stored.
18: class BlockManager {
19: public:
20: 	virtual ~BlockManager() = default;
21: 
22: 	//! Creates a new block inside the block manager
23: 	virtual unique_ptr<Block> CreateBlock() = 0;
24: 	//! Return the next free block id
25: 	virtual block_id_t GetFreeBlockId() = 0;
26: 	//! Get the first meta block id
27: 	virtual block_id_t GetMetaBlock() = 0;
28: 	//! Read the content of the block from disk
29: 	virtual void Read(Block &block) = 0;
30: 	//! Writes the block to disk
31: 	virtual void Write(FileBuffer &block, block_id_t block_id) = 0;
32: 	//! Writes the block to disk
33: 	void Write(Block &block) {
34: 		Write(block, block.id);
35: 	}
36: 	//! Write the header; should be the final step of a checkpoint
37: 	virtual void WriteHeader(DatabaseHeader header) = 0;
38: };
39: } // namespace duckdb
[end of src/include/duckdb/storage/block_manager.hpp]
[start of src/include/duckdb/storage/checkpoint/table_data_writer.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/checkpoint/table_data_writer.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/storage/checkpoint_manager.hpp"
12: #include "duckdb/common/unordered_map.hpp"
13: 
14: namespace duckdb {
15: class UncompressedSegment;
16: class SegmentStatistics;
17: 
18: //! The table data writer is responsible for writing the data of a table to the block manager
19: class TableDataWriter {
20: public:
21: 	TableDataWriter(CheckpointManager &manager, TableCatalogEntry &table);
22: 	~TableDataWriter();
23: 
24: 	void WriteTableData(Transaction &transaction);
25: 
26: private:
27: 	void AppendData(idx_t col_idx, Vector &data, idx_t count);
28: 
29: 	void CreateSegment(idx_t col_idx);
30: 	void FlushSegment(idx_t col_idx);
31: 
32: 	void WriteDataPointers();
33: 
34: private:
35: 	CheckpointManager &manager;
36: 	TableCatalogEntry &table;
37: 
38: 	vector<unique_ptr<UncompressedSegment>> segments;
39: 	vector<unique_ptr<SegmentStatistics>> stats;
40: 
41: 	vector<vector<DataPointer>> data_pointers;
42: };
43: 
44: } // namespace duckdb
[end of src/include/duckdb/storage/checkpoint/table_data_writer.hpp]
[start of src/include/duckdb/storage/in_memory_block_manager.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/in_memory_block_manager.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/exception.hpp"
13: #include "duckdb/storage/block_manager.hpp"
14: 
15: namespace duckdb {
16: 
17: //! InMemoryBlockManager is an implementation for a BlockManager
18: class InMemoryBlockManager : public BlockManager {
19: public:
20: 	unique_ptr<Block> CreateBlock() override {
21: 		throw Exception("Cannot perform IO in in-memory database!");
22: 	}
23: 	block_id_t GetFreeBlockId() override {
24: 		throw Exception("Cannot perform IO in in-memory database!");
25: 	}
26: 	block_id_t GetMetaBlock() override {
27: 		throw Exception("Cannot perform IO in in-memory database!");
28: 	}
29: 	void Read(Block &block) override {
30: 		throw Exception("Cannot perform IO in in-memory database!");
31: 	}
32: 	void Write(FileBuffer &block, block_id_t block_id) override {
33: 		throw Exception("Cannot perform IO in in-memory database!");
34: 	}
35: 	void WriteHeader(DatabaseHeader header) override {
36: 		throw Exception("Cannot perform IO in in-memory database!");
37: 	}
38: };
39: } // namespace duckdb
[end of src/include/duckdb/storage/in_memory_block_manager.hpp]
[start of src/include/duckdb/storage/single_file_block_manager.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/single_file_block_manager.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/storage/block_manager.hpp"
13: #include "duckdb/storage/block.hpp"
14: #include "duckdb/common/file_system.hpp"
15: #include "duckdb/common/unordered_set.hpp"
16: 
17: namespace duckdb {
18: class BufferManager;
19: class FileBuffer;
20: 
21: //! SingleFileBlockManager is an implementation for a BlockManager which manages blocks in a single file
22: class SingleFileBlockManager : public BlockManager {
23: 	//! The location in the file where the block writing starts
24: 	static constexpr uint64_t BLOCK_START = Storage::FILE_HEADER_SIZE * 3;
25: 
26: public:
27: 	SingleFileBlockManager(FileSystem &fs, string path, bool read_only, bool create_new, bool use_direct_io);
28: 
29: 	//! Creates a new Block and returns a pointer
30: 	unique_ptr<Block> CreateBlock() override;
31: 	//! Return the next free block id
32: 	block_id_t GetFreeBlockId() override;
33: 	//! Return the meta block id
34: 	block_id_t GetMetaBlock() override;
35: 	//! Read the content of the block from disk
36: 	void Read(Block &block) override;
37: 	//! Write the given block to disk
38: 	void Write(FileBuffer &block, block_id_t block_id) override;
39: 	//! Write the header to disk, this is the final step of the checkpointing process
40: 	void WriteHeader(DatabaseHeader header) override;
41: 
42: 	//! Load the free list from the file
43: 	void LoadFreeList(BufferManager &manager);
44: 
45: private:
46: 	void Initialize(DatabaseHeader &header);
47: 
48: private:
49: 	//! The active DatabaseHeader, either 0 (h1) or 1 (h2)
50: 	uint8_t active_header;
51: 	//! The path where the file is stored
52: 	string path;
53: 	//! The file handle
54: 	unique_ptr<FileHandle> handle;
55: 	//! The buffer used to read/write to the headers
56: 	FileBuffer header_buffer;
57: 	//! The list of free blocks that can be written to currently
58: 	vector<block_id_t> free_list;
59: 	//! The list of blocks that are used by the current block manager
60: 	unordered_set<block_id_t> used_blocks;
61: 	//! The current meta block id
62: 	block_id_t meta_block;
63: 	//! The current maximum block id, this id will be given away first after the free_list runs out
64: 	block_id_t max_block;
65: 	//! The block id where the free list can be found
66: 	block_id_t free_list_id;
67: 	//! The current header iteration count
68: 	uint64_t iteration_count;
69: 	//! Whether or not the db is opened in read-only mode
70: 	bool read_only;
71: 	//! Whether or not to use Direct IO to read the blocks
72: 	bool use_direct_io;
73: };
74: } // namespace duckdb
[end of src/include/duckdb/storage/single_file_block_manager.hpp]
[start of src/include/duckdb/storage/string_segment.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/string_segment.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/storage/uncompressed_segment.hpp"
12: 
13: namespace duckdb {
14: class OverflowStringWriter {
15: public:
16: 	virtual ~OverflowStringWriter() {
17: 	}
18: 
19: 	virtual void WriteString(string_t string, block_id_t &result_block, int32_t &result_offset) = 0;
20: };
21: 
22: struct StringBlock {
23: 	block_id_t block_id;
24: 	idx_t offset;
25: 	idx_t size;
26: 	unique_ptr<StringBlock> next;
27: };
28: 
29: struct string_location_t {
30: 	string_location_t(block_id_t block_id, int32_t offset) : block_id(block_id), offset(offset) {
31: 	}
32: 	string_location_t() {
33: 	}
34: 	bool IsValid() {
35: 		return offset < Storage::BLOCK_SIZE && (block_id == INVALID_BLOCK || block_id >= MAXIMUM_BLOCK);
36: 	}
37: 	block_id_t block_id;
38: 	int32_t offset;
39: };
40: 
41: struct StringUpdateInfo {
42: 	sel_t count;
43: 	sel_t ids[STANDARD_VECTOR_SIZE];
44: 	block_id_t block_ids[STANDARD_VECTOR_SIZE];
45: 	int32_t offsets[STANDARD_VECTOR_SIZE];
46: };
47: 
48: typedef unique_ptr<StringUpdateInfo> string_update_info_t;
49: 
50: class StringSegment : public UncompressedSegment {
51: public:
52: 	StringSegment(BufferManager &manager, idx_t row_start, block_id_t block_id = INVALID_BLOCK);
53: 	~StringSegment() override;
54: 
55: 	//! The current dictionary offset
56: 	idx_t dictionary_offset;
57: 	//! The string block holding strings that do not fit in the main block
58: 	//! FIXME: this should be replaced by a heap that also allows freeing of unused strings
59: 	unique_ptr<StringBlock> head;
60: 	//! Blocks that hold string updates (if any)
61: 	unique_ptr<string_update_info_t[]> string_updates;
62: 	//! Overflow string writer (if any), if not set overflow strings will be written to memory blocks
63: 	unique_ptr<OverflowStringWriter> overflow_writer;
64: 
65: public:
66: 	void InitializeScan(ColumnScanState &state) override;
67: 
68: 	//! Fetch a single value and append it to the vector
69: 	void FetchRow(ColumnFetchState &state, Transaction &transaction, row_t row_id, Vector &result,
70: 	              idx_t result_idx) override;
71: 
72: 	//! Append a part of a vector to the uncompressed segment with the given append state, updating the provided stats
73: 	//! in the process. Returns the amount of tuples appended. If this is less than `count`, the uncompressed segment is
74: 	//! full.
75: 	idx_t Append(SegmentStatistics &stats, Vector &data, idx_t offset, idx_t count) override;
76: 
77: 	//! Rollback a previous update
78: 	void RollbackUpdate(UpdateInfo *info) override;
79: 
80: protected:
81: 	void Update(ColumnData &column_data, SegmentStatistics &stats, Transaction &transaction, Vector &update, row_t *ids,
82: 	            idx_t count, idx_t vector_index, idx_t vector_offset, UpdateInfo *node) override;
83: 
84: 	void FetchBaseData(ColumnScanState &state, idx_t vector_index, Vector &result) override;
85: 	void FetchUpdateData(ColumnScanState &state, Transaction &transaction, UpdateInfo *versions,
86: 	                     Vector &result) override;
87: 
88: private:
89: 	void AppendData(SegmentStatistics &stats, data_ptr_t target, data_ptr_t end, idx_t target_offset, Vector &source,
90: 	                idx_t offset, idx_t count);
91: 
92: 	//! Fetch all the strings of a vector from the base table and place their locations in the result vector
93: 	void FetchBaseData(ColumnScanState &state, data_ptr_t base_data, idx_t vector_index, Vector &result, idx_t count);
94: 
95: 	string_location_t FetchStringLocation(data_ptr_t baseptr, int32_t dict_offset);
96: 	string_t FetchString(buffer_handle_set_t &handles, data_ptr_t baseptr, string_location_t location);
97: 	//! Fetch a single string from the dictionary and returns it, potentially pins a buffer manager page and adds it to
98: 	//! the set of pinned pages
99: 	string_t FetchStringFromDict(buffer_handle_set_t &handles, data_ptr_t baseptr, int32_t dict_offset);
100: 
101: 	//! Fetch string locations for a subset of the strings
102: 	void FetchStringLocations(data_ptr_t baseptr, row_t *ids, idx_t vector_index, idx_t vector_offset, idx_t count,
103: 	                          string_location_t result[]);
104: 
105: 	void WriteString(string_t string, block_id_t &result_block, int32_t &result_offset);
106: 	string_t ReadString(buffer_handle_set_t &handles, block_id_t block, int32_t offset);
107: 	string_t ReadString(data_ptr_t target, int32_t offset);
108: 
109: 	void WriteStringMemory(string_t string, block_id_t &result_block, int32_t &result_offset);
110: 
111: 	void WriteStringMarker(data_ptr_t target, block_id_t block_id, int32_t offset);
112: 	void ReadStringMarker(data_ptr_t target, block_id_t &block_id, int32_t &offset);
113: 
114: 	//! Expand the string segment, adding an additional maximum vector to the segment
115: 	void ExpandStringSegment(data_ptr_t baseptr);
116: 
117: 	string_update_info_t CreateStringUpdate(SegmentStatistics &stats, Vector &update, row_t *ids, idx_t count,
118: 	                                        idx_t vector_offset);
119: 	string_update_info_t MergeStringUpdate(SegmentStatistics &stats, Vector &update, row_t *ids, idx_t count,
120: 	                                       idx_t vector_offset, StringUpdateInfo &update_info);
121: 
122: 	void MergeUpdateInfo(UpdateInfo *node, row_t *ids, idx_t update_count, idx_t vector_offset,
123: 	                     string_location_t string_locations[], nullmask_t original_nullmask);
124: 
125: 	//! The amount of bytes remaining to store in the block
126: 	idx_t RemainingSpace() {
127: 		return Storage::BLOCK_SIZE - dictionary_offset - max_vector_count * vector_size;
128: 	}
129: 
130: private:
131: 	//! The max string size that is allowed within a block. Strings bigger than this will be labeled as a BIG STRING and
132: 	//! offloaded to the overflow blocks.
133: 	static constexpr uint16_t STRING_BLOCK_LIMIT = 4096;
134: 	//! Marker used in length field to indicate the presence of a big string
135: 	static constexpr uint16_t BIG_STRING_MARKER = (uint16_t)-1;
136: 	//! Base size of big string marker (block id + offset)
137: 	static constexpr idx_t BIG_STRING_MARKER_BASE_SIZE = sizeof(block_id_t) + sizeof(int32_t);
138: 	//! The marker size of the big string
139: 	static constexpr idx_t BIG_STRING_MARKER_SIZE = BIG_STRING_MARKER_BASE_SIZE + sizeof(uint16_t);
140: };
141: 
142: } // namespace duckdb
[end of src/include/duckdb/storage/string_segment.hpp]
[start of src/include/duckdb/storage/uncompressed_segment.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/uncompressed_segment.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/storage/table/column_segment.hpp"
12: #include "duckdb/storage/block.hpp"
13: #include "duckdb/storage/storage_lock.hpp"
14: #include "duckdb/storage/table/scan_state.hpp"
15: 
16: namespace duckdb {
17: class BufferManager;
18: class ColumnData;
19: class Transaction;
20: 
21: struct ColumnAppendState;
22: struct UpdateInfo;
23: 
24: //! An uncompressed segment represents an uncompressed segment of a column residing in a block
25: class UncompressedSegment {
26: public:
27: 	UncompressedSegment(BufferManager &manager, TypeId type, idx_t row_start);
28: 	virtual ~UncompressedSegment();
29: 
30: 	//! The buffer manager
31: 	BufferManager &manager;
32: 	//! Type of the uncompressed segment
33: 	TypeId type;
34: 	//! The block id that this segment relates to
35: 	block_id_t block_id;
36: 	//! The size of a vector of this type
37: 	idx_t vector_size;
38: 	//! The maximum amount of vectors that can be stored in this segment
39: 	idx_t max_vector_count;
40: 	//! The current amount of tuples that are stored in this segment
41: 	idx_t tuple_count;
42: 	//! The starting row of this segment
43: 	idx_t row_start;
44: 	//! Version chains for each of the vectors
45: 	unique_ptr<UpdateInfo *[]> versions;
46: 	//! The lock for the uncompressed segment
47: 	StorageLock lock;
48: 
49: public:
50: 	virtual void InitializeScan(ColumnScanState &state) {
51: 	}
52: 	//! Fetch the vector at index "vector_index" from the uncompressed segment, storing it in the result vector
53: 	void Scan(Transaction &transaction, ColumnScanState &state, idx_t vector_index, Vector &result);
54: 	//! Fetch the vector at index "vector_index" from the uncompressed segment, throwing an exception if there are any
55: 	//! outstanding updates
56: 	void IndexScan(ColumnScanState &state, idx_t vector_index, Vector &result);
57: 
58: 	//! Fetch a single vector from the base table
59: 	void Fetch(ColumnScanState &state, idx_t vector_index, Vector &result);
60: 	//! Fetch a single value and append it to the vector
61: 	virtual void FetchRow(ColumnFetchState &state, Transaction &transaction, row_t row_id, Vector &result,
62: 	                      idx_t result_idx) = 0;
63: 
64: 	//! Append a part of a vector to the uncompressed segment with the given append state, updating the provided stats
65: 	//! in the process. Returns the amount of tuples appended. If this is less than `count`, the uncompressed segment is
66: 	//! full.
67: 	virtual idx_t Append(SegmentStatistics &stats, Vector &data, idx_t offset, idx_t count) = 0;
68: 
69: 	//! Update a set of row identifiers to the specified set of updated values
70: 	void Update(ColumnData &data, SegmentStatistics &stats, Transaction &transaction, Vector &update, row_t *ids,
71: 	            idx_t count, row_t offset);
72: 
73: 	//! Rollback a previous update
74: 	virtual void RollbackUpdate(UpdateInfo *info) = 0;
75: 	//! Cleanup an update, removing it from the version chain. This should only be called if an exclusive lock is held
76: 	//! on the segment
77: 	void CleanupUpdate(UpdateInfo *info);
78: 
79: 	//! Convert a persistently backed uncompressed segment (i.e. one where block_id refers to an on-disk block) to a
80: 	//! temporary in-memory one
81: 	void ToTemporary();
82: 
83: 	//! Get the amount of tuples in a vector
84: 	idx_t GetVectorCount(idx_t vector_index) {
85: 		assert(vector_index < max_vector_count);
86: 		assert(vector_index * STANDARD_VECTOR_SIZE <= tuple_count);
87: 		return std::min((idx_t)STANDARD_VECTOR_SIZE, tuple_count - vector_index * STANDARD_VECTOR_SIZE);
88: 	}
89: 
90: protected:
91: 	virtual void Update(ColumnData &data, SegmentStatistics &stats, Transaction &transaction, Vector &update,
92: 	                    row_t *ids, idx_t count, idx_t vector_index, idx_t vector_offset, UpdateInfo *node) = 0;
93: 	//! Fetch base table data
94: 	virtual void FetchBaseData(ColumnScanState &state, idx_t vector_index, Vector &result) = 0;
95: 	//! Fetch update data from an UpdateInfo version
96: 	virtual void FetchUpdateData(ColumnScanState &state, Transaction &transaction, UpdateInfo *version,
97: 	                             Vector &result) = 0;
98: 
99: 	//! Create a new update info for the specified transaction reflecting an update of the specified rows
100: 	UpdateInfo *CreateUpdateInfo(ColumnData &data, Transaction &transaction, row_t *ids, idx_t count,
101: 	                             idx_t vector_index, idx_t vector_offset, idx_t type_size);
102: };
103: 
104: } // namespace duckdb
[end of src/include/duckdb/storage/uncompressed_segment.hpp]
[start of src/optimizer/rule/like_optimizations.cpp]
1: #include "duckdb/optimizer/rule/like_optimizations.hpp"
2: 
3: #include "duckdb/execution/expression_executor.hpp"
4: #include "duckdb/planner/expression/bound_function_expression.hpp"
5: #include "duckdb/planner/expression/bound_constant_expression.hpp"
6: 
7: #include <regex>
8: 
9: using namespace duckdb;
10: using namespace std;
11: 
12: LikeOptimizationRule::LikeOptimizationRule(ExpressionRewriter &rewriter) : Rule(rewriter) {
13: 	// match on a FunctionExpression that has a foldable ConstantExpression
14: 	auto func = make_unique<FunctionExpressionMatcher>();
15: 	func->matchers.push_back(make_unique<ConstantExpressionMatcher>());
16: 	func->matchers.push_back(make_unique<ExpressionMatcher>());
17: 	func->policy = SetMatcher::Policy::SOME;
18: 	// we only match on LIKE ("~~")
19: 	func->function = make_unique<SpecificFunctionMatcher>("~~");
20: 	root = move(func);
21: }
22: 
23: unique_ptr<Expression> LikeOptimizationRule::Apply(LogicalOperator &op, vector<Expression *> &bindings,
24:                                                    bool &changes_made) {
25: 	auto root = (BoundFunctionExpression *)bindings[0];
26: 	auto constant_expr = (BoundConstantExpression *)bindings[1];
27: 	assert(root->children.size() == 2);
28: 	if (constant_expr->value.is_null) {
29: 		return make_unique<BoundConstantExpression>(Value(root->return_type));
30: 	}
31: 
32: 	// the constant_expr is a scalar expression that we have to fold
33: 	if (!constant_expr->IsFoldable()) {
34: 		return move(root->Copy());
35: 	}
36: 
37: 	auto constant_value = ExpressionExecutor::EvaluateScalar(*constant_expr);
38: 	assert(constant_value.type == constant_expr->return_type);
39: 	string patt_str = string(((string_t)constant_value.str_value).GetData());
40: 
41: 	if (std::regex_match(patt_str, std::regex("[^%_]*[%]+"))) {
42: 		//^^^^^^^^^^^^^^^Prefix LIKE pattern : [^%_]*[%]+, ignoring undescore
43: 
44: 		return move(ApplyRule(root, PrefixFun::GetFunction(), patt_str));
45: 
46: 	} else if (std::regex_match(patt_str, std::regex("[%]+[^%_]*"))) {
47: 		//^^^^^^^^^^^^^^^^^^^^^^^Suffix LIKE pattern: [%]+[^%_]*, ignoring undescore
48: 
49: 		return move(ApplyRule(root, SuffixFun::GetFunction(), patt_str));
50: 
51: 	} else if (std::regex_match(patt_str, std::regex("[%]+[^%_]*[%]+"))) {
52: 		//^^^^^^^^^^^^^^^^^^^^^Contains LIKE pattern: [%]+[^%_]*[%]+, ignoring undescore
53: 
54: 		return move(ApplyRule(root, ContainsFun::GetFunction(), patt_str));
55: 	}
56: 
57: 	return nullptr;
58: }
59: 
60: unique_ptr<Expression> LikeOptimizationRule::ApplyRule(BoundFunctionExpression *expr, ScalarFunction function,
61:                                                        string pattern) {
62: 	// replace LIKE by an optimized function
63: 	expr->function = function;
64: 
65: 	// removing "%" from the pattern
66: 	pattern.erase(std::remove(pattern.begin(), pattern.end(), '%'), pattern.end());
67: 
68: 	expr->children[1] = make_unique<BoundConstantExpression>(Value(pattern));
69: 
70: 	return move(expr->Copy());
71: }
[end of src/optimizer/rule/like_optimizations.cpp]
[start of src/storage/checkpoint/table_data_reader.cpp]
1: #include "duckdb/storage/checkpoint/table_data_reader.hpp"
2: #include "duckdb/storage/checkpoint/table_data_writer.hpp"
3: #include "duckdb/storage/meta_block_reader.hpp"
4: 
5: #include "duckdb/common/vector_operations/vector_operations.hpp"
6: #include "duckdb/common/types/null_value.hpp"
7: 
8: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
9: 
10: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
11: 
12: using namespace duckdb;
13: using namespace std;
14: 
15: TableDataReader::TableDataReader(CheckpointManager &manager, MetaBlockReader &reader, BoundCreateTableInfo &info)
16:     : manager(manager), reader(reader), info(info) {
17: 	info.data = unique_ptr<vector<unique_ptr<PersistentSegment>>[]>(
18: 	    new vector<unique_ptr<PersistentSegment>>[info.Base().columns.size()]);
19: }
20: 
21: void TableDataReader::ReadTableData() {
22: 	auto &columns = info.Base().columns;
23: 	assert(columns.size() > 0);
24: 
25: 	// load the data pointers for the table
26: 	for (idx_t col = 0; col < columns.size(); col++) {
27: 		auto &column = columns[col];
28: 		idx_t data_pointer_count = reader.Read<idx_t>();
29: 		for (idx_t data_ptr = 0; data_ptr < data_pointer_count; data_ptr++) {
30: 			// read the data pointer
31: 			DataPointer data_pointer;
32: 			data_pointer.min = reader.Read<double>();
33: 			data_pointer.max = reader.Read<double>();
34: 			data_pointer.row_start = reader.Read<idx_t>();
35: 			data_pointer.tuple_count = reader.Read<idx_t>();
36: 			data_pointer.block_id = reader.Read<block_id_t>();
37: 			data_pointer.offset = reader.Read<uint32_t>();
38: 			reader.ReadData(data_pointer.min_stats, 8);
39: 			reader.ReadData(data_pointer.max_stats, 8);
40: 
41: 			// create a persistent segment
42: 			auto segment = make_unique<PersistentSegment>(
43: 			    manager.buffer_manager, data_pointer.block_id, data_pointer.offset, GetInternalType(column.type),
44: 			    data_pointer.row_start, data_pointer.tuple_count, data_pointer.min_stats, data_pointer.max_stats);
45: 			info.data[col].push_back(move(segment));
46: 		}
47: 	}
48: }
[end of src/storage/checkpoint/table_data_reader.cpp]
[start of src/storage/checkpoint/table_data_writer.cpp]
1: #include "duckdb/storage/checkpoint/table_data_writer.hpp"
2: 
3: #include "duckdb/common/vector_operations/vector_operations.hpp"
4: #include "duckdb/common/types/null_value.hpp"
5: 
6: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
7: #include "duckdb/common/serializer/buffered_serializer.hpp"
8: 
9: #include "duckdb/storage/numeric_segment.hpp"
10: #include "duckdb/storage/string_segment.hpp"
11: #include "duckdb/storage/table/column_segment.hpp"
12: 
13: using namespace duckdb;
14: using namespace std;
15: 
16: class WriteOverflowStringsToDisk : public OverflowStringWriter {
17: public:
18: 	WriteOverflowStringsToDisk(CheckpointManager &manager);
19: 	~WriteOverflowStringsToDisk();
20: 
21: 	//! The checkpoint manager
22: 	CheckpointManager &manager;
23: 	//! Block handle use for writing to
24: 	unique_ptr<BufferHandle> handle;
25: 	//! The current block we are writing to
26: 	block_id_t block_id;
27: 	//! The offset within the current block
28: 	idx_t offset;
29: 
30: 	static constexpr idx_t STRING_SPACE = Storage::BLOCK_SIZE - sizeof(block_id_t);
31: 
32: public:
33: 	void WriteString(string_t string, block_id_t &result_block, int32_t &result_offset) override;
34: 
35: private:
36: 	void AllocateNewBlock(block_id_t new_block_id);
37: };
38: 
39: TableDataWriter::TableDataWriter(CheckpointManager &manager, TableCatalogEntry &table)
40:     : manager(manager), table(table) {
41: }
42: 
43: TableDataWriter::~TableDataWriter() {
44: }
45: 
46: void TableDataWriter::WriteTableData(Transaction &transaction) {
47: 	// allocate segments to write the table to
48: 	segments.resize(table.columns.size());
49: 	data_pointers.resize(table.columns.size());
50: 	for (idx_t i = 0; i < table.columns.size(); i++) {
51: 		auto type_id = GetInternalType(table.columns[i].type);
52: 		stats.push_back(make_unique<SegmentStatistics>(type_id, GetTypeIdSize(type_id)));
53: 		CreateSegment(i);
54: 	}
55: 
56: 	// now start scanning the table and append the data to the uncompressed segments
57: 	vector<column_t> column_ids;
58: 	for (auto &column : table.columns) {
59: 		column_ids.push_back(column.oid);
60: 	}
61: 	// initialize scan structures to prepare for the scan
62: 	TableScanState state;
63: 	table.storage->InitializeScan(transaction, state, column_ids);
64: 	//! get all types of the table and initialize the chunk
65: 	auto types = table.GetTypes();
66: 	DataChunk chunk;
67: 	chunk.Initialize(types);
68: 
69: 	while (true) {
70: 		chunk.Reset();
71: 		// now scan the table to construct the blocks
72: 		vector<TableFilter> mock;
73: 		table.storage->Scan(transaction, chunk, state, mock);
74: 		if (chunk.size() == 0) {
75: 			break;
76: 		}
77: 		// for each column, we append whatever we can fit into the block
78: 		for (idx_t i = 0; i < table.columns.size(); i++) {
79: 			assert(chunk.data[i].type == GetInternalType(table.columns[i].type));
80: 			AppendData(i, chunk.data[i], chunk.size());
81: 		}
82: 	}
83: 	// flush any remaining data and write the data pointers to disk
84: 	for (idx_t i = 0; i < table.columns.size(); i++) {
85: 		FlushSegment(i);
86: 	}
87: 	WriteDataPointers();
88: }
89: 
90: void TableDataWriter::CreateSegment(idx_t col_idx) {
91: 	auto type_id = GetInternalType(table.columns[col_idx].type);
92: 	if (type_id == TypeId::VARCHAR) {
93: 		auto string_segment = make_unique<StringSegment>(manager.buffer_manager, 0);
94: 		string_segment->overflow_writer = make_unique<WriteOverflowStringsToDisk>(manager);
95: 		segments[col_idx] = move(string_segment);
96: 	} else {
97: 		segments[col_idx] = make_unique<NumericSegment>(manager.buffer_manager, type_id, 0);
98: 	}
99: }
100: 
101: void TableDataWriter::AppendData(idx_t col_idx, Vector &data, idx_t count) {
102: 	idx_t offset = 0;
103: 	while (offset < count) {
104: 		idx_t appended = segments[col_idx]->Append(*stats[col_idx], data, offset, count);
105: 		if (appended == count) {
106: 			// appended everything: finished
107: 			return;
108: 		}
109: 		// the segment is full: flush it to disk
110: 		FlushSegment(col_idx);
111: 
112: 		// now create a new segment and continue appending
113: 		CreateSegment(col_idx);
114: 		offset += appended;
115: 		count -= appended;
116: 	}
117: }
118: 
119: void TableDataWriter::FlushSegment(idx_t col_idx) {
120: 	auto tuple_count = segments[col_idx]->tuple_count;
121: 	if (tuple_count == 0) {
122: 		return;
123: 	}
124: 
125: 	// get the buffer of the segment and pin it
126: 	auto handle = manager.buffer_manager.Pin(segments[col_idx]->block_id);
127: 
128: 	// get a free block id to write to
129: 	auto block_id = manager.block_manager.GetFreeBlockId();
130: 
131: 	// construct the data pointer, FIXME: add statistics as well
132: 	DataPointer data_pointer;
133: 	data_pointer.block_id = block_id;
134: 	data_pointer.offset = 0;
135: 	data_pointer.row_start = 0;
136: 	if (data_pointers[col_idx].size() > 0) {
137: 		auto &last_pointer = data_pointers[col_idx].back();
138: 		data_pointer.row_start = last_pointer.row_start + last_pointer.tuple_count;
139: 	}
140: 	data_pointer.tuple_count = tuple_count;
141: 	//! FIXME: Can't deal with strings yet
142: 	idx_t type_size = stats[col_idx]->type == TypeId::VARCHAR ? 0 : stats[col_idx]->type_size;
143: 	memcpy(&data_pointer.min_stats, stats[col_idx]->minimum.get(), type_size);
144: 	memcpy(&data_pointer.max_stats, stats[col_idx]->maximum.get(), type_size);
145: 	data_pointers[col_idx].push_back(move(data_pointer));
146: 	// write the block to disk
147: 	manager.block_manager.Write(*handle->node, block_id);
148: }
149: 
150: void TableDataWriter::WriteDataPointers() {
151: 	for (idx_t i = 0; i < data_pointers.size(); i++) {
152: 		// get a reference to the data column
153: 		auto &data_pointer_list = data_pointers[i];
154: 		manager.tabledata_writer->Write<idx_t>(data_pointer_list.size());
155: 		// then write the data pointers themselves
156: 		for (idx_t k = 0; k < data_pointer_list.size(); k++) {
157: 			auto &data_pointer = data_pointer_list[k];
158: 			manager.tabledata_writer->Write<double>(data_pointer.min);
159: 			manager.tabledata_writer->Write<double>(data_pointer.max);
160: 			manager.tabledata_writer->Write<idx_t>(data_pointer.row_start);
161: 			manager.tabledata_writer->Write<idx_t>(data_pointer.tuple_count);
162: 			manager.tabledata_writer->Write<block_id_t>(data_pointer.block_id);
163: 			manager.tabledata_writer->Write<uint32_t>(data_pointer.offset);
164: 			manager.tabledata_writer->WriteData(data_pointer.min_stats, 8);
165: 			manager.tabledata_writer->WriteData(data_pointer.max_stats, 8);
166: 		}
167: 	}
168: }
169: 
170: WriteOverflowStringsToDisk::WriteOverflowStringsToDisk(CheckpointManager &manager)
171:     : manager(manager), handle(nullptr), block_id(INVALID_BLOCK), offset(0) {
172: }
173: 
174: WriteOverflowStringsToDisk::~WriteOverflowStringsToDisk() {
175: 	if (offset > 0) {
176: 		manager.block_manager.Write(*handle->node, block_id);
177: 	}
178: }
179: 
180: void WriteOverflowStringsToDisk::WriteString(string_t string, block_id_t &result_block, int32_t &result_offset) {
181: 	if (!handle) {
182: 		handle = manager.buffer_manager.Allocate(Storage::BLOCK_ALLOC_SIZE);
183: 	}
184: 	// first write the length of the string
185: 	if (block_id == INVALID_BLOCK || offset + sizeof(uint32_t) >= STRING_SPACE) {
186: 		AllocateNewBlock(manager.block_manager.GetFreeBlockId());
187: 	}
188: 	result_block = block_id;
189: 	result_offset = offset;
190: 
191: 	// write the length field
192: 	auto string_length = string.GetSize();
193: 	*((uint32_t *)(handle->node->buffer + offset)) = string_length;
194: 	offset += sizeof(uint32_t);
195: 	// now write the remainder of the string
196: 	auto strptr = string.GetData();
197: 	uint32_t remaining = string_length + 1;
198: 	while (remaining > 0) {
199: 		uint32_t to_write = std::min((uint32_t)remaining, (uint32_t)(STRING_SPACE - offset));
200: 		if (to_write > 0) {
201: 			memcpy(handle->node->buffer + offset, strptr, to_write);
202: 
203: 			remaining -= to_write;
204: 			offset += to_write;
205: 			strptr += to_write;
206: 		}
207: 		if (remaining > 0) {
208: 			// there is still remaining stuff to write
209: 			// first get the new block id and write it to the end of the previous block
210: 			auto new_block_id = manager.block_manager.GetFreeBlockId();
211: 			*((block_id_t *)(handle->node->buffer + offset)) = new_block_id;
212: 			// now write the current block to disk and allocate a new block
213: 			AllocateNewBlock(new_block_id);
214: 		}
215: 	}
216: }
217: 
218: void WriteOverflowStringsToDisk::AllocateNewBlock(block_id_t new_block_id) {
219: 	if (block_id != INVALID_BLOCK) {
220: 		// there is an old block, write it first
221: 		manager.block_manager.Write(*handle->node, block_id);
222: 	}
223: 	offset = 0;
224: 	block_id = new_block_id;
225: }
[end of src/storage/checkpoint/table_data_writer.cpp]
[start of src/storage/checkpoint_manager.cpp]
1: #include "duckdb/storage/checkpoint_manager.hpp"
2: #include "duckdb/storage/block_manager.hpp"
3: #include "duckdb/storage/meta_block_reader.hpp"
4: 
5: #include "duckdb/common/serializer.hpp"
6: #include "duckdb/common/vector_operations/vector_operations.hpp"
7: #include "duckdb/common/types/null_value.hpp"
8: 
9: #include "duckdb/catalog/catalog.hpp"
10: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
11: #include "duckdb/catalog/catalog_entry/sequence_catalog_entry.hpp"
12: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
13: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
14: 
15: #include "duckdb/parser/parsed_data/create_schema_info.hpp"
16: #include "duckdb/parser/parsed_data/create_table_info.hpp"
17: #include "duckdb/parser/parsed_data/create_view_info.hpp"
18: 
19: #include "duckdb/planner/binder.hpp"
20: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
21: 
22: #include "duckdb/main/client_context.hpp"
23: #include "duckdb/main/database.hpp"
24: 
25: #include "duckdb/transaction/transaction_manager.hpp"
26: 
27: #include "duckdb/storage/checkpoint/table_data_writer.hpp"
28: #include "duckdb/storage/checkpoint/table_data_reader.hpp"
29: 
30: using namespace duckdb;
31: using namespace std;
32: 
33: // constexpr uint64_t CheckpointManager::DATA_BLOCK_HEADER_SIZE;
34: 
35: CheckpointManager::CheckpointManager(StorageManager &manager)
36:     : block_manager(*manager.block_manager), buffer_manager(*manager.buffer_manager), database(manager.database) {
37: }
38: 
39: void CheckpointManager::CreateCheckpoint() {
40: 	// assert that the checkpoint manager hasn't been used before
41: 	assert(!metadata_writer);
42: 
43: 	auto transaction = database.transaction_manager->StartTransaction();
44: 
45: 	//! Set up the writers for the checkpoints
46: 	metadata_writer = make_unique<MetaBlockWriter>(block_manager);
47: 	tabledata_writer = make_unique<MetaBlockWriter>(block_manager);
48: 
49: 	// get the id of the first meta block
50: 	block_id_t meta_block = metadata_writer->block->id;
51: 
52: 	vector<SchemaCatalogEntry *> schemas;
53: 	// we scan the schemas
54: 	database.catalog->schemas.Scan(*transaction,
55: 	                               [&](CatalogEntry *entry) { schemas.push_back((SchemaCatalogEntry *)entry); });
56: 	// write the actual data into the database
57: 	// write the amount of schemas
58: 	metadata_writer->Write<uint32_t>(schemas.size());
59: 	for (auto &schema : schemas) {
60: 		WriteSchema(*transaction, *schema);
61: 	}
62: 	// flush the meta data to disk
63: 	metadata_writer->Flush();
64: 	tabledata_writer->Flush();
65: 
66: 	// finally write the updated header
67: 	DatabaseHeader header;
68: 	header.meta_block = meta_block;
69: 	block_manager.WriteHeader(header);
70: }
71: 
72: void CheckpointManager::LoadFromStorage() {
73: 	block_id_t meta_block = block_manager.GetMetaBlock();
74: 	if (meta_block < 0) {
75: 		// storage is empty
76: 		return;
77: 	}
78: 
79: 	ClientContext context(database);
80: 	context.transaction.BeginTransaction();
81: 	// create the MetaBlockReader to read from the storage
82: 	MetaBlockReader reader(buffer_manager, meta_block);
83: 	uint32_t schema_count = reader.Read<uint32_t>();
84: 	for (uint32_t i = 0; i < schema_count; i++) {
85: 		ReadSchema(context, reader);
86: 	}
87: 	context.transaction.Commit();
88: }
89: 
90: //===--------------------------------------------------------------------===//
91: // Schema
92: //===--------------------------------------------------------------------===//
93: void CheckpointManager::WriteSchema(Transaction &transaction, SchemaCatalogEntry &schema) {
94: 	// write the schema data
95: 	schema.Serialize(*metadata_writer);
96: 	// then, we fetch the tables/views/sequences information
97: 	vector<TableCatalogEntry *> tables;
98: 	vector<ViewCatalogEntry *> views;
99: 	schema.tables.Scan(transaction, [&](CatalogEntry *entry) {
100: 		if (entry->type == CatalogType::TABLE) {
101: 			tables.push_back((TableCatalogEntry *)entry);
102: 		} else if (entry->type == CatalogType::VIEW) {
103: 			views.push_back((ViewCatalogEntry *)entry);
104: 		} else {
105: 			throw NotImplementedException("Catalog type for entries");
106: 		}
107: 	});
108: 	vector<SequenceCatalogEntry *> sequences;
109: 	schema.sequences.Scan(transaction,
110: 	                      [&](CatalogEntry *entry) { sequences.push_back((SequenceCatalogEntry *)entry); });
111: 
112: 	// write the sequences
113: 	metadata_writer->Write<uint32_t>(sequences.size());
114: 	for (auto &seq : sequences) {
115: 		WriteSequence(*seq);
116: 	}
117: 	// now write the tables
118: 	metadata_writer->Write<uint32_t>(tables.size());
119: 	for (auto &table : tables) {
120: 		WriteTable(transaction, *table);
121: 	}
122: 	// finally write the views
123: 	metadata_writer->Write<uint32_t>(views.size());
124: 	for (auto &view : views) {
125: 		WriteView(*view);
126: 	}
127: }
128: 
129: void CheckpointManager::ReadSchema(ClientContext &context, MetaBlockReader &reader) {
130: 	// read the schema and create it in the catalog
131: 	auto info = SchemaCatalogEntry::Deserialize(reader);
132: 	// we set create conflict to ignore to ignore the failure of recreating the main schema
133: 	info->on_conflict = OnCreateConflict::IGNORE;
134: 	database.catalog->CreateSchema(context, info.get());
135: 
136: 	// read the sequences
137: 	uint32_t seq_count = reader.Read<uint32_t>();
138: 	for (uint32_t i = 0; i < seq_count; i++) {
139: 		ReadSequence(context, reader);
140: 	}
141: 	// read the table count and recreate the tables
142: 	uint32_t table_count = reader.Read<uint32_t>();
143: 	for (uint32_t i = 0; i < table_count; i++) {
144: 		ReadTable(context, reader);
145: 	}
146: 	// finally read the views
147: 	uint32_t view_count = reader.Read<uint32_t>();
148: 	for (uint32_t i = 0; i < view_count; i++) {
149: 		ReadView(context, reader);
150: 	}
151: }
152: 
153: //===--------------------------------------------------------------------===//
154: // Views
155: //===--------------------------------------------------------------------===//
156: void CheckpointManager::WriteView(ViewCatalogEntry &view) {
157: 	view.Serialize(*metadata_writer);
158: }
159: 
160: void CheckpointManager::ReadView(ClientContext &context, MetaBlockReader &reader) {
161: 	auto info = ViewCatalogEntry::Deserialize(reader);
162: 
163: 	database.catalog->CreateView(context, info.get());
164: }
165: 
166: //===--------------------------------------------------------------------===//
167: // Sequences
168: //===--------------------------------------------------------------------===//
169: void CheckpointManager::WriteSequence(SequenceCatalogEntry &seq) {
170: 	seq.Serialize(*metadata_writer);
171: }
172: 
173: void CheckpointManager::ReadSequence(ClientContext &context, MetaBlockReader &reader) {
174: 	auto info = SequenceCatalogEntry::Deserialize(reader);
175: 
176: 	database.catalog->CreateSequence(context, info.get());
177: }
178: 
179: //===--------------------------------------------------------------------===//
180: // Table Metadata
181: //===--------------------------------------------------------------------===//
182: void CheckpointManager::WriteTable(Transaction &transaction, TableCatalogEntry &table) {
183: 	// write the table meta data
184: 	table.Serialize(*metadata_writer);
185: 	//! write the blockId for the table info
186: 	metadata_writer->Write<block_id_t>(tabledata_writer->block->id);
187: 	//! and the offset to where the info starts
188: 	metadata_writer->Write<uint64_t>(tabledata_writer->offset);
189: 	// now we need to write the table data
190: 	TableDataWriter writer(*this, table);
191: 	writer.WriteTableData(transaction);
192: }
193: 
194: void CheckpointManager::ReadTable(ClientContext &context, MetaBlockReader &reader) {
195: 	// deserialize the table meta data
196: 	auto info = TableCatalogEntry::Deserialize(reader);
197: 	// bind the info
198: 	Binder binder(context);
199: 	auto bound_info = binder.BindCreateTableInfo(move(info));
200: 
201: 	// now read the actual table data and place it into the create table info
202: 	auto block_id = reader.Read<block_id_t>();
203: 	auto offset = reader.Read<uint64_t>();
204: 	MetaBlockReader table_data_reader(buffer_manager, block_id);
205: 	table_data_reader.offset = offset;
206: 	TableDataReader data_reader(*this, table_data_reader, *bound_info);
207: 	data_reader.ReadTableData();
208: 
209: 	// finally create the table in the catalog
210: 	database.catalog->CreateTable(context, bound_info.get());
211: }
[end of src/storage/checkpoint_manager.cpp]
[start of src/storage/single_file_block_manager.cpp]
1: #include "duckdb/storage/single_file_block_manager.hpp"
2: #include "duckdb/storage/meta_block_writer.hpp"
3: #include "duckdb/storage/meta_block_reader.hpp"
4: #include "duckdb/common/exception.hpp"
5: 
6: using namespace duckdb;
7: using namespace std;
8: 
9: SingleFileBlockManager::SingleFileBlockManager(FileSystem &fs, string path, bool read_only, bool create_new,
10:                                                bool use_direct_io)
11:     : path(path), header_buffer(FileBufferType::MANAGED_BUFFER, Storage::FILE_HEADER_SIZE), read_only(read_only),
12:       use_direct_io(use_direct_io) {
13: 
14: 	uint8_t flags;
15: 	FileLockType lock;
16: 	if (read_only) {
17: 		assert(!create_new);
18: 		flags = FileFlags::READ;
19: 		lock = FileLockType::READ_LOCK;
20: 	} else {
21: 		flags = FileFlags::WRITE;
22: 		lock = FileLockType::WRITE_LOCK;
23: 		if (create_new) {
24: 			flags |= FileFlags::CREATE;
25: 		}
26: 	}
27: 	if (use_direct_io) {
28: 		flags |= FileFlags::DIRECT_IO;
29: 	}
30: 	// open the RDBMS handle
31: 	handle = fs.OpenFile(path, flags, lock);
32: 	if (create_new) {
33: 		// if we create a new file, we fill the metadata of the file
34: 		// first fill in the new header
35: 		header_buffer.Clear();
36: 		MainHeader *main_header = (MainHeader *)header_buffer.buffer;
37: 		main_header->version_number = VERSION_NUMBER;
38: 		// now write the header to the file
39: 		header_buffer.Write(*handle, 0);
40: 		header_buffer.Clear();
41: 
42: 		// write the database headers
43: 		// initialize meta_block and free_list to INVALID_BLOCK because the database file does not contain any actual
44: 		// content yet
45: 		DatabaseHeader *header = (DatabaseHeader *)header_buffer.buffer;
46: 		// header 1
47: 		header->iteration = 0;
48: 		header->meta_block = INVALID_BLOCK;
49: 		header->free_list = INVALID_BLOCK;
50: 		header->block_count = 0;
51: 		header_buffer.Write(*handle, Storage::FILE_HEADER_SIZE);
52: 		// header 2
53: 		header->iteration = 1;
54: 		header_buffer.Write(*handle, Storage::FILE_HEADER_SIZE * 2);
55: 		// ensure that writing to disk is completed before returning
56: 		handle->Sync();
57: 		// we start with h2 as active_header, this way our initial write will be in h1
58: 		active_header = 1;
59: 		max_block = 0;
60: 	} else {
61: 		MainHeader header;
62: 		// otherwise, we check the metadata of the file
63: 		header_buffer.Read(*handle, 0);
64: 		header = *((MainHeader *)header_buffer.buffer);
65: 		// check the version number
66: 		if (header.version_number != VERSION_NUMBER) {
67: 			throw IOException(
68: 			    "Trying to read a database file with version number %lld, but we can only read version %lld",
69: 			    header.version_number, VERSION_NUMBER);
70: 		}
71: 		// read the database headers from disk
72: 		DatabaseHeader h1, h2;
73: 		header_buffer.Read(*handle, Storage::FILE_HEADER_SIZE);
74: 		h1 = *((DatabaseHeader *)header_buffer.buffer);
75: 		header_buffer.Read(*handle, Storage::FILE_HEADER_SIZE * 2);
76: 		h2 = *((DatabaseHeader *)header_buffer.buffer);
77: 		// check the header with the highest iteration count
78: 		if (h1.iteration > h2.iteration) {
79: 			// h1 is active header
80: 			active_header = 0;
81: 			Initialize(h1);
82: 		} else {
83: 			// h2 is active header
84: 			active_header = 1;
85: 			Initialize(h2);
86: 		}
87: 	}
88: }
89: 
90: void SingleFileBlockManager::Initialize(DatabaseHeader &header) {
91: 	free_list_id = header.free_list;
92: 	meta_block = header.meta_block;
93: 	iteration_count = header.iteration;
94: 	max_block = header.block_count;
95: }
96: 
97: void SingleFileBlockManager::LoadFreeList(BufferManager &manager) {
98: 	if (read_only) {
99: 		// no need to load free list for read only db
100: 		return;
101: 	}
102: 	if (free_list_id == INVALID_BLOCK) {
103: 		// no free list
104: 		return;
105: 	}
106: 	MetaBlockReader reader(manager, free_list_id);
107: 	auto free_list_count = reader.Read<uint64_t>();
108: 	free_list.reserve(free_list_count);
109: 	for (idx_t i = 0; i < free_list_count; i++) {
110: 		free_list.push_back(reader.Read<block_id_t>());
111: 	}
112: }
113: 
114: block_id_t SingleFileBlockManager::GetFreeBlockId() {
115: 	if (free_list.size() > 0) {
116: 		// free list is non empty
117: 		// take an entry from the free list
118: 		block_id_t block = free_list.back();
119: 		// erase the entry from the free list again
120: 		free_list.pop_back();
121: 		return block;
122: 	}
123: 	return max_block++;
124: }
125: 
126: block_id_t SingleFileBlockManager::GetMetaBlock() {
127: 	return meta_block;
128: }
129: 
130: unique_ptr<Block> SingleFileBlockManager::CreateBlock() {
131: 	return make_unique<Block>(GetFreeBlockId());
132: }
133: 
134: void SingleFileBlockManager::Read(Block &block) {
135: 	assert(block.id >= 0);
136: 	used_blocks.insert(block.id);
137: 	block.Read(*handle, BLOCK_START + block.id * Storage::BLOCK_ALLOC_SIZE);
138: }
139: 
140: void SingleFileBlockManager::Write(FileBuffer &buffer, block_id_t block_id) {
141: 	assert(block_id >= 0);
142: 	buffer.Write(*handle, BLOCK_START + block_id * Storage::BLOCK_ALLOC_SIZE);
143: }
144: 
145: void SingleFileBlockManager::WriteHeader(DatabaseHeader header) {
146: 	// set the iteration count
147: 	header.iteration = ++iteration_count;
148: 	header.block_count = max_block;
149: 	// now handle the free list
150: 	if (used_blocks.size() > 0) {
151: 		// there are blocks in the free list
152: 		// write them to the file
153: 		MetaBlockWriter writer(*this);
154: 		header.free_list = writer.block->id;
155: 		writer.Write<uint64_t>(used_blocks.size());
156: 		for (auto &block_id : used_blocks) {
157: 			writer.Write<block_id_t>(block_id);
158: 		}
159: 		writer.Flush();
160: 	} else {
161: 		// no blocks in the free list
162: 		header.free_list = INVALID_BLOCK;
163: 	}
164: 	if (!use_direct_io) {
165: 		// if we are not using Direct IO we need to fsync BEFORE we write the header to ensure that all the previous
166: 		// blocks are written as well
167: 		handle->Sync();
168: 	}
169: 	// set the header inside the buffer
170: 	header_buffer.Clear();
171: 	*((DatabaseHeader *)header_buffer.buffer) = header;
172: 	// now write the header to the file, active_header determines whether we write to h1 or h2
173: 	// note that if active_header is h1 we write to h2, and vice versa
174: 	header_buffer.Write(*handle, active_header == 1 ? Storage::FILE_HEADER_SIZE : Storage::FILE_HEADER_SIZE * 2);
175: 	// switch active header to the other header
176: 	active_header = 1 - active_header;
177: 	//! Ensure the header write ends up on disk
178: 	handle->Sync();
179: 
180: 	// the free list is now equal to the blocks that were used by the previous iteration
181: 	for (auto &block_id : used_blocks) {
182: 		free_list.push_back(block_id);
183: 	}
184: 	used_blocks.clear();
185: }
[end of src/storage/single_file_block_manager.cpp]
[start of src/storage/string_segment.cpp]
1: #include "duckdb/storage/string_segment.hpp"
2: #include "duckdb/storage/buffer_manager.hpp"
3: #include "duckdb/storage/numeric_segment.hpp"
4: #include "duckdb/transaction/update_info.hpp"
5: #include "duckdb/common/vector_operations/vector_operations.hpp"
6: 
7: using namespace duckdb;
8: using namespace std;
9: 
10: StringSegment::StringSegment(BufferManager &manager, idx_t row_start, block_id_t block)
11:     : UncompressedSegment(manager, TypeId::VARCHAR, row_start) {
12: 	this->max_vector_count = 0;
13: 	this->dictionary_offset = 0;
14: 	// the vector_size is given in the size of the dictionary offsets
15: 	this->vector_size = STANDARD_VECTOR_SIZE * sizeof(int32_t) + sizeof(nullmask_t);
16: 	this->string_updates = nullptr;
17: 
18: 	this->block_id = block;
19: 	if (block_id == INVALID_BLOCK) {
20: 		// start off with an empty string segment: allocate space for it
21: 		auto handle = manager.Allocate(Storage::BLOCK_ALLOC_SIZE);
22: 		this->block_id = handle->block_id;
23: 
24: 		ExpandStringSegment(handle->node->buffer);
25: 	}
26: }
27: 
28: StringSegment::~StringSegment() {
29: 	while (head) {
30: 		manager.DestroyBuffer(head->block_id);
31: 		head = move(head->next);
32: 	}
33: }
34: 
35: void StringSegment::ExpandStringSegment(data_ptr_t baseptr) {
36: 	// clear the nullmask for this vector
37: 	auto mask = (nullmask_t *)(baseptr + (max_vector_count * vector_size));
38: 	mask->reset();
39: 
40: 	max_vector_count++;
41: 	if (versions) {
42: 		auto new_versions = unique_ptr<UpdateInfo *[]>(new UpdateInfo *[max_vector_count]);
43: 		memcpy(new_versions.get(), versions.get(), (max_vector_count - 1) * sizeof(UpdateInfo *));
44: 		new_versions[max_vector_count - 1] = nullptr;
45: 		versions = move(new_versions);
46: 	}
47: 
48: 	if (string_updates) {
49: 		auto new_string_updates = unique_ptr<string_update_info_t[]>(new string_update_info_t[max_vector_count]);
50: 		for (idx_t i = 0; i < max_vector_count - 1; i++) {
51: 			new_string_updates[i] = move(string_updates[i]);
52: 		}
53: 		new_string_updates[max_vector_count - 1] = 0;
54: 		string_updates = move(new_string_updates);
55: 	}
56: }
57: 
58: //===--------------------------------------------------------------------===//
59: // Scan
60: //===--------------------------------------------------------------------===//
61: void StringSegment::InitializeScan(ColumnScanState &state) {
62: 	// pin the primary buffer
63: 	state.primary_handle = manager.Pin(block_id);
64: }
65: 
66: //===--------------------------------------------------------------------===//
67: // Fetch base data
68: //===--------------------------------------------------------------------===//
69: void StringSegment::FetchBaseData(ColumnScanState &state, idx_t vector_index, Vector &result) {
70: 	// clear any previously locked buffers and get the primary buffer handle
71: 	auto handle = state.primary_handle.get();
72: 	state.handles.clear();
73: 
74: 	// fetch the data from the base segment
75: 	FetchBaseData(state, handle->node->buffer, vector_index, result, GetVectorCount(vector_index));
76: }
77: 
78: void StringSegment::FetchBaseData(ColumnScanState &state, data_ptr_t baseptr, idx_t vector_index, Vector &result,
79:                                   idx_t count) {
80: 	auto base = baseptr + vector_index * vector_size;
81: 
82: 	auto &base_nullmask = *((nullmask_t *)base);
83: 	auto base_data = (int32_t *)(base + sizeof(nullmask_t));
84: 	auto result_data = FlatVector::GetData<string_t>(result);
85: 
86: 	if (string_updates && string_updates[vector_index]) {
87: 		// there are updates: merge them in
88: 		auto &info = *string_updates[vector_index];
89: 		idx_t update_idx = 0;
90: 		for (idx_t i = 0; i < count; i++) {
91: 			if (update_idx < info.count && info.ids[update_idx] == i) {
92: 				// use update info
93: 				result_data[i] = ReadString(state.handles, info.block_ids[update_idx], info.offsets[update_idx]);
94: 				update_idx++;
95: 			} else {
96: 				// use base table info
97: 				result_data[i] = FetchStringFromDict(state.handles, baseptr, base_data[i]);
98: 			}
99: 		}
100: 	} else {
101: 		// no updates: fetch only from the string dictionary
102: 		for (idx_t i = 0; i < count; i++) {
103: 			result_data[i] = FetchStringFromDict(state.handles, baseptr, base_data[i]);
104: 		}
105: 	}
106: 	FlatVector::SetNullmask(result, base_nullmask);
107: }
108: 
109: //===--------------------------------------------------------------------===//
110: // Fetch update data
111: //===--------------------------------------------------------------------===//
112: void StringSegment::FetchUpdateData(ColumnScanState &state, Transaction &transaction, UpdateInfo *info,
113:                                     Vector &result) {
114: 	// fetch data from updates
115: 	auto handle = state.primary_handle.get();
116: 
117: 	auto result_data = FlatVector::GetData<string_t>(result);
118: 	auto &result_mask = FlatVector::Nullmask(result);
119: 	UpdateInfo::UpdatesForTransaction(info, transaction, [&](UpdateInfo *current) {
120: 		auto info_data = (string_location_t *)current->tuple_data;
121: 		for (idx_t i = 0; i < current->N; i++) {
122: 			auto string = FetchString(state.handles, handle->node->buffer, info_data[i]);
123: 			result_data[current->tuples[i]] = string;
124: 			result_mask[current->tuples[i]] = current->nullmask[current->tuples[i]];
125: 		}
126: 	});
127: }
128: 
129: //===--------------------------------------------------------------------===//
130: // Fetch strings
131: //===--------------------------------------------------------------------===//
132: void StringSegment::FetchStringLocations(data_ptr_t baseptr, row_t *ids, idx_t vector_index, idx_t vector_offset,
133:                                          idx_t count, string_location_t result[]) {
134: 	auto base = baseptr + vector_index * vector_size;
135: 	auto base_data = (int32_t *)(base + sizeof(nullmask_t));
136: 
137: 	if (string_updates && string_updates[vector_index]) {
138: 		// there are updates: merge them in
139: 		auto &info = *string_updates[vector_index];
140: 		idx_t update_idx = 0;
141: 		for (idx_t i = 0; i < count; i++) {
142: 			auto id = ids[i] - vector_offset;
143: 			while (update_idx < info.count && info.ids[update_idx] < id) {
144: 				update_idx++;
145: 			}
146: 			if (update_idx < info.count && info.ids[update_idx] == id) {
147: 				// use update info
148: 				result[i].block_id = info.block_ids[update_idx];
149: 				result[i].offset = info.offsets[update_idx];
150: 				update_idx++;
151: 			} else {
152: 				// use base table info
153: 				result[i] = FetchStringLocation(baseptr, base_data[id]);
154: 			}
155: 		}
156: 	} else {
157: 		// no updates: fetch strings from base vector
158: 		for (idx_t i = 0; i < count; i++) {
159: 			auto id = ids[i] - vector_offset;
160: 			result[i] = FetchStringLocation(baseptr, base_data[id]);
161: 		}
162: 	}
163: }
164: 
165: string_location_t StringSegment::FetchStringLocation(data_ptr_t baseptr, int32_t dict_offset) {
166: 	if (dict_offset == 0) {
167: 		return string_location_t(INVALID_BLOCK, 0);
168: 	}
169: 	// look up result in dictionary
170: 	auto dict_end = baseptr + Storage::BLOCK_SIZE;
171: 	auto dict_pos = dict_end - dict_offset;
172: 	auto string_length = *((uint16_t *)dict_pos);
173: 	string_location_t result;
174: 	if (string_length == BIG_STRING_MARKER) {
175: 		ReadStringMarker(dict_pos, result.block_id, result.offset);
176: 	} else {
177: 		result.block_id = INVALID_BLOCK;
178: 		result.offset = dict_offset;
179: 	}
180: 	return result;
181: }
182: 
183: string_t StringSegment::FetchStringFromDict(buffer_handle_set_t &handles, data_ptr_t baseptr, int32_t dict_offset) {
184: 	// fetch base data
185: 	string_location_t location = FetchStringLocation(baseptr, dict_offset);
186: 	return FetchString(handles, baseptr, location);
187: }
188: 
189: string_t StringSegment::FetchString(buffer_handle_set_t &handles, data_ptr_t baseptr, string_location_t location) {
190: 	if (location.block_id != INVALID_BLOCK) {
191: 		// big string marker: read from separate block
192: 		return ReadString(handles, location.block_id, location.offset);
193: 	} else {
194: 		if (location.offset == 0) {
195: 			return string_t(nullptr, 0);
196: 		}
197: 		// normal string: read string from this block
198: 		auto dict_end = baseptr + Storage::BLOCK_SIZE;
199: 		auto dict_pos = dict_end - location.offset;
200: 		auto string_length = *((uint16_t *)dict_pos);
201: 
202: 		auto str_ptr = (char *)(dict_pos + sizeof(uint16_t));
203: 		return string_t(str_ptr, string_length);
204: 	}
205: }
206: 
207: void StringSegment::FetchRow(ColumnFetchState &state, Transaction &transaction, row_t row_id, Vector &result,
208:                              idx_t result_idx) {
209: 	auto read_lock = lock.GetSharedLock();
210: 
211: 	idx_t vector_index = row_id / STANDARD_VECTOR_SIZE;
212: 	idx_t id_in_vector = row_id - vector_index * STANDARD_VECTOR_SIZE;
213: 	assert(vector_index < max_vector_count);
214: 
215: 	data_ptr_t baseptr;
216: 
217: 	// fetch a single row from the string segment
218: 	// first pin the main buffer if it is not already pinned
219: 	auto entry = state.handles.find(block_id);
220: 	if (entry == state.handles.end()) {
221: 		// not pinned yet: pin it
222: 		auto handle = manager.Pin(block_id);
223: 		baseptr = handle->node->buffer;
224: 		state.handles[block_id] = move(handle);
225: 	} else {
226: 		// already pinned: use the pinned handle
227: 		baseptr = entry->second->node->buffer;
228: 	}
229: 
230: 	auto base = baseptr + vector_index * vector_size;
231: 	auto &base_nullmask = *((nullmask_t *)base);
232: 	auto base_data = (int32_t *)(base + sizeof(nullmask_t));
233: 	auto result_data = FlatVector::GetData<string_t>(result);
234: 	auto &result_mask = FlatVector::Nullmask(result);
235: 
236: 	bool found_data = false;
237: 	// first see if there is any updated version of this tuple we must fetch
238: 	if (versions && versions[vector_index]) {
239: 		UpdateInfo::UpdatesForTransaction(versions[vector_index], transaction, [&](UpdateInfo *current) {
240: 			auto info_data = (string_location_t *)current->tuple_data;
241: 			// loop over the tuples in this UpdateInfo
242: 			for (idx_t i = 0; i < current->N; i++) {
243: 				if (current->tuples[i] == row_id) {
244: 					// found the relevant tuple
245: 					found_data = true;
246: 					result_data[result_idx] = FetchString(state.handles, baseptr, info_data[i]);
247: 					result_mask[result_idx] = current->nullmask[current->tuples[i]];
248: 					break;
249: 				} else if (current->tuples[i] > row_id) {
250: 					// tuples are sorted: so if the current tuple is > row_id we will not find it anymore
251: 					break;
252: 				}
253: 			}
254: 		});
255: 	}
256: 	if (!found_data && string_updates && string_updates[vector_index]) {
257: 		// there are updates: check if we should use them
258: 		auto &info = *string_updates[vector_index];
259: 		for (idx_t i = 0; i < info.count; i++) {
260: 			if (info.ids[i] == id_in_vector) {
261: 				// use the update
262: 				result_data[result_idx] = ReadString(state.handles, info.block_ids[i], info.offsets[i]);
263: 				found_data = true;
264: 				break;
265: 			} else if (info.ids[i] > id_in_vector) {
266: 				break;
267: 			}
268: 		}
269: 	}
270: 	if (!found_data) {
271: 		// no version was found yet: fetch base table version
272: 		result_data[result_idx] = FetchStringFromDict(state.handles, baseptr, base_data[id_in_vector]);
273: 	}
274: 	result_mask[result_idx] = base_nullmask[id_in_vector];
275: }
276: 
277: //===--------------------------------------------------------------------===//
278: // Append
279: //===--------------------------------------------------------------------===//
280: idx_t StringSegment::Append(SegmentStatistics &stats, Vector &data, idx_t offset, idx_t count) {
281: 	assert(data.type == TypeId::VARCHAR);
282: 	auto handle = manager.Pin(block_id);
283: 
284: 	idx_t initial_count = tuple_count;
285: 	while (count > 0) {
286: 		// get the vector index of the vector to append to and see how many tuples we can append to that vector
287: 		idx_t vector_index = tuple_count / STANDARD_VECTOR_SIZE;
288: 		if (vector_index == max_vector_count) {
289: 			// we are at the maximum vector, check if there is space to increase the maximum vector count
290: 			// as a heuristic, we only allow another vector to be added if we have at least 32 bytes per string
291: 			// remaining (32KB out of a 256KB block, or around 12% empty)
292: 			if (RemainingSpace() >= STANDARD_VECTOR_SIZE * 32) {
293: 				// we have enough remaining space to add another vector
294: 				ExpandStringSegment(handle->node->buffer);
295: 			} else {
296: 				break;
297: 			}
298: 		}
299: 		idx_t current_tuple_count = tuple_count - vector_index * STANDARD_VECTOR_SIZE;
300: 		idx_t append_count = std::min(STANDARD_VECTOR_SIZE - current_tuple_count, count);
301: 
302: 		// now perform the actual append
303: 		AppendData(stats, handle->node->buffer + vector_size * vector_index, handle->node->buffer + Storage::BLOCK_SIZE,
304: 		           current_tuple_count, data, offset, append_count);
305: 
306: 		count -= append_count;
307: 		offset += append_count;
308: 		tuple_count += append_count;
309: 	}
310: 	return tuple_count - initial_count;
311: }
312: 
313: void StringSegment::AppendData(SegmentStatistics &stats, data_ptr_t target, data_ptr_t end, idx_t target_offset,
314:                                Vector &source, idx_t offset, idx_t count) {
315: 	VectorData adata;
316: 	source.Orrify(count, adata);
317: 
318: 	auto sdata = (string_t *)adata.data;
319: 	auto &result_nullmask = *((nullmask_t *)target);
320: 	auto result_data = (int32_t *)(target + sizeof(nullmask_t));
321: 
322: 	idx_t remaining_strings = STANDARD_VECTOR_SIZE - (this->tuple_count % STANDARD_VECTOR_SIZE);
323: 	for (idx_t i = 0; i < count; i++) {
324: 		auto source_idx = adata.sel->get_index(offset + i);
325: 		auto target_idx = target_offset + i;
326: 		if ((*adata.nullmask)[source_idx]) {
327: 			// null value is stored as -1
328: 			result_data[target_idx] = 0;
329: 			result_nullmask[target_idx] = true;
330: 			stats.has_null = true;
331: 		} else {
332: 			assert(dictionary_offset < Storage::BLOCK_SIZE);
333: 			// non-null value, check if we can fit it within the block
334: 			idx_t string_length = sdata[source_idx].GetSize();
335: 			idx_t total_length = string_length + 1 + sizeof(uint16_t);
336: 
337: 			if (string_length > stats.max_string_length) {
338: 				stats.max_string_length = string_length;
339: 			}
340: 			// determine hwether or not the string needs to be stored in an overflow block
341: 			// we never place small strings in the overflow blocks: the pointer would take more space than the
342: 			// string itself we always place big strings (>= STRING_BLOCK_LIMIT) in the overflow blocks we also have
343: 			// to always leave enough room for BIG_STRING_MARKER_SIZE for each of the remaining strings
344: 			if (total_length > BIG_STRING_MARKER_BASE_SIZE &&
345: 			    (total_length >= STRING_BLOCK_LIMIT ||
346: 			     total_length + (remaining_strings * BIG_STRING_MARKER_SIZE) > RemainingSpace())) {
347: 				assert(RemainingSpace() >= BIG_STRING_MARKER_SIZE);
348: 				// string is too big for block: write to overflow blocks
349: 				block_id_t block;
350: 				int32_t offset;
351: 				// write the string into the current string block
352: 				WriteString(sdata[source_idx], block, offset);
353: 
354: 				dictionary_offset += BIG_STRING_MARKER_SIZE;
355: 				auto dict_pos = end - dictionary_offset;
356: 
357: 				// write a big string marker into the dictionary
358: 				WriteStringMarker(dict_pos, block, offset);
359: 
360: 				stats.has_overflow_strings = true;
361: 			} else {
362: 				// string fits in block, append to dictionary and increment dictionary position
363: 				assert(string_length < std::numeric_limits<uint16_t>::max());
364: 				dictionary_offset += total_length;
365: 				auto dict_pos = end - dictionary_offset;
366: 
367: 				// first write the length as u16
368: 				uint16_t string_length_u16 = string_length;
369: 				memcpy(dict_pos, &string_length_u16, sizeof(uint16_t));
370: 				// now write the actual string data into the dictionary
371: 				memcpy(dict_pos + sizeof(uint16_t), sdata[source_idx].GetData(), string_length + 1);
372: 			}
373: 			// place the dictionary offset into the set of vectors
374: 			result_data[target_idx] = dictionary_offset;
375: 		}
376: 		remaining_strings--;
377: 	}
378: }
379: 
380: void StringSegment::WriteString(string_t string, block_id_t &result_block, int32_t &result_offset) {
381: 	assert(strlen(string.GetData()) == string.GetSize());
382: 	if (overflow_writer) {
383: 		// overflow writer is set: write string there
384: 		overflow_writer->WriteString(string, result_block, result_offset);
385: 	} else {
386: 		// default overflow behavior: use in-memory buffer to store the overflow string
387: 		WriteStringMemory(string, result_block, result_offset);
388: 	}
389: }
390: 
391: void StringSegment::WriteStringMemory(string_t string, block_id_t &result_block, int32_t &result_offset) {
392: 	uint32_t total_length = string.GetSize() + 1 + sizeof(uint32_t);
393: 	unique_ptr<BufferHandle> handle;
394: 	// check if the string fits in the current block
395: 	if (!head || head->offset + total_length >= head->size) {
396: 		// string does not fit, allocate space for it
397: 		// create a new string block
398: 		idx_t alloc_size = std::max((idx_t)total_length, (idx_t)Storage::BLOCK_ALLOC_SIZE);
399: 		auto new_block = make_unique<StringBlock>();
400: 		new_block->offset = 0;
401: 		new_block->size = alloc_size;
402: 		// allocate an in-memory buffer for it
403: 		handle = manager.Allocate(alloc_size);
404: 		new_block->block_id = handle->block_id;
405: 		new_block->next = move(head);
406: 		head = move(new_block);
407: 	} else {
408: 		// string fits, copy it into the current block
409: 		handle = manager.Pin(head->block_id);
410: 	}
411: 
412: 	result_block = head->block_id;
413: 	result_offset = head->offset;
414: 
415: 	// copy the string and the length there
416: 	auto ptr = handle->node->buffer + head->offset;
417: 	memcpy(ptr, &string.length, sizeof(uint32_t));
418: 	ptr += sizeof(uint32_t);
419: 	memcpy(ptr, string.GetData(), string.length + 1);
420: 	head->offset += total_length;
421: }
422: 
423: string_t StringSegment::ReadString(buffer_handle_set_t &handles, block_id_t block, int32_t offset) {
424: 	assert(offset < Storage::BLOCK_SIZE);
425: 	if (block == INVALID_BLOCK) {
426: 		return string_t(nullptr, 0);
427: 	}
428: 	if (block < MAXIMUM_BLOCK) {
429: 		// read the overflow string from disk
430: 		// pin the initial handle and read the length
431: 		auto handle = manager.Pin(block);
432: 		uint32_t length = *((uint32_t *)(handle->node->buffer + offset));
433: 		uint32_t remaining = length + 1;
434: 		offset += sizeof(uint32_t);
435: 
436: 		// allocate a buffer to store the string
437: 		auto alloc_size = std::max((idx_t)Storage::BLOCK_ALLOC_SIZE, (idx_t)length + 1 + sizeof(uint32_t));
438: 		auto target_handle = manager.Allocate(alloc_size, true);
439: 		auto target_ptr = target_handle->node->buffer;
440: 		// write the length in this block as well
441: 		*((uint32_t *)target_ptr) = length;
442: 		target_ptr += sizeof(uint32_t);
443: 		// now append the string to the single buffer
444: 		while (remaining > 0) {
445: 			idx_t to_write = std::min((idx_t)remaining, (idx_t)(Storage::BLOCK_SIZE - sizeof(block_id_t) - offset));
446: 			memcpy(target_ptr, handle->node->buffer + offset, to_write);
447: 
448: 			remaining -= to_write;
449: 			offset += to_write;
450: 			target_ptr += to_write;
451: 			if (remaining > 0) {
452: 				// read the next block
453: 				block_id_t next_block = *((block_id_t *)(handle->node->buffer + offset));
454: 				handle = manager.Pin(next_block);
455: 				offset = 0;
456: 			}
457: 		}
458: 
459: 		auto final_buffer = target_handle->node->buffer;
460: 		handles.insert(make_pair(target_handle->block_id, move(target_handle)));
461: 		return ReadString(final_buffer, 0);
462: 	} else {
463: 		// read the overflow string from memory
464: 		// first pin the handle, if it is not pinned yet
465: 		BufferHandle *handle;
466: 		auto entry = handles.find(block);
467: 		if (entry == handles.end()) {
468: 			auto pinned_handle = manager.Pin(block);
469: 			handle = pinned_handle.get();
470: 
471: 			handles.insert(make_pair(block, move(pinned_handle)));
472: 		} else {
473: 			handle = entry->second.get();
474: 		}
475: 		return ReadString(handle->node->buffer, offset);
476: 	}
477: }
478: 
479: string_t StringSegment::ReadString(data_ptr_t target, int32_t offset) {
480: 	auto ptr = target + offset;
481: 	auto str_length = *((uint32_t *)ptr);
482: 	auto str_ptr = (char *)(ptr + sizeof(uint32_t));
483: 	return string_t(str_ptr, str_length);
484: }
485: 
486: void StringSegment::WriteStringMarker(data_ptr_t target, block_id_t block_id, int32_t offset) {
487: 	uint16_t length = BIG_STRING_MARKER;
488: 	memcpy(target, &length, sizeof(uint16_t));
489: 	target += sizeof(uint16_t);
490: 	memcpy(target, &block_id, sizeof(block_id_t));
491: 	target += sizeof(block_id_t);
492: 	memcpy(target, &offset, sizeof(int32_t));
493: }
494: 
495: void StringSegment::ReadStringMarker(data_ptr_t target, block_id_t &block_id, int32_t &offset) {
496: 	target += sizeof(uint16_t);
497: 	memcpy(&block_id, target, sizeof(block_id_t));
498: 	target += sizeof(block_id_t);
499: 	memcpy(&offset, target, sizeof(int32_t));
500: }
501: 
502: //===--------------------------------------------------------------------===//
503: // String Update
504: //===--------------------------------------------------------------------===//
505: string_update_info_t StringSegment::CreateStringUpdate(SegmentStatistics &stats, Vector &update, row_t *ids,
506:                                                        idx_t count, idx_t vector_offset) {
507: 	auto info = make_unique<StringUpdateInfo>();
508: 	info->count = count;
509: 	auto strings = FlatVector::GetData<string_t>(update);
510: 	auto &update_nullmask = FlatVector::Nullmask(update);
511: 	for (idx_t i = 0; i < count; i++) {
512: 		info->ids[i] = ids[i] - vector_offset;
513: 		// copy the string into the block
514: 		if (!update_nullmask[i]) {
515: 			WriteString(strings[i], info->block_ids[i], info->offsets[i]);
516: 		} else {
517: 			info->block_ids[i] = INVALID_BLOCK;
518: 			info->offsets[i] = 0;
519: 		}
520: 	}
521: 	return info;
522: }
523: 
524: string_update_info_t StringSegment::MergeStringUpdate(SegmentStatistics &stats, Vector &update, row_t *ids,
525:                                                       idx_t update_count, idx_t vector_offset,
526:                                                       StringUpdateInfo &update_info) {
527: 	auto info = make_unique<StringUpdateInfo>();
528: 
529: 	// perform a merge between the new and old indexes
530: 	auto strings = FlatVector::GetData<string_t>(update);
531: 	auto &update_nullmask = FlatVector::Nullmask(update);
532: 	auto pick_new = [&](idx_t id, idx_t idx, idx_t count) {
533: 		info->ids[count] = id;
534: 		if (!update_nullmask[idx]) {
535: 			WriteString(strings[idx], info->block_ids[count], info->offsets[count]);
536: 		} else {
537: 			info->block_ids[count] = INVALID_BLOCK;
538: 			info->offsets[count] = 0;
539: 		}
540: 	};
541: 	auto merge = [&](idx_t id, idx_t aidx, idx_t bidx, idx_t count) {
542: 		// merge: only pick new entry
543: 		pick_new(id, aidx, count);
544: 	};
545: 	auto pick_old = [&](idx_t id, idx_t bidx, idx_t count) {
546: 		// pick old entry
547: 		info->ids[count] = id;
548: 		info->block_ids[count] = update_info.block_ids[bidx];
549: 		info->offsets[count] = update_info.offsets[bidx];
550: 	};
551: 
552: 	info->count =
553: 	    merge_loop(ids, update_info.ids, update_count, update_info.count, vector_offset, merge, pick_new, pick_old);
554: 	return info;
555: }
556: 
557: //===--------------------------------------------------------------------===//
558: // Update Info
559: //===--------------------------------------------------------------------===//
560: void StringSegment::MergeUpdateInfo(UpdateInfo *node, row_t *ids, idx_t update_count, idx_t vector_offset,
561:                                     string_location_t base_data[], nullmask_t base_nullmask) {
562: 	auto info_data = (string_location_t *)node->tuple_data;
563: 
564: 	// first we copy the old update info into a temporary structure
565: 	sel_t old_ids[STANDARD_VECTOR_SIZE];
566: 	string_location_t old_data[STANDARD_VECTOR_SIZE];
567: 
568: 	memcpy(old_ids, node->tuples, node->N * sizeof(sel_t));
569: 	memcpy(old_data, node->tuple_data, node->N * sizeof(string_location_t));
570: 
571: 	// now we perform a merge of the new ids with the old ids
572: 	auto merge = [&](idx_t id, idx_t aidx, idx_t bidx, idx_t count) {
573: 		// new_id and old_id are the same, insert the old data in the UpdateInfo
574: 		assert(old_data[bidx].IsValid());
575: 		info_data[count] = old_data[bidx];
576: 		node->tuples[count] = id;
577: 	};
578: 	auto pick_new = [&](idx_t id, idx_t aidx, idx_t count) {
579: 		// new_id comes before the old id, insert the base table data into the update info
580: 		assert(base_data[aidx].IsValid());
581: 		info_data[count] = base_data[aidx];
582: 		node->nullmask[id] = base_nullmask[aidx];
583: 
584: 		node->tuples[count] = id;
585: 	};
586: 	auto pick_old = [&](idx_t id, idx_t bidx, idx_t count) {
587: 		// old_id comes before new_id, insert the old data
588: 		assert(old_data[bidx].IsValid());
589: 		info_data[count] = old_data[bidx];
590: 		node->tuples[count] = id;
591: 	};
592: 	// perform the merge
593: 	node->N = merge_loop(ids, old_ids, update_count, node->N, vector_offset, merge, pick_new, pick_old);
594: }
595: 
596: //===--------------------------------------------------------------------===//
597: // Update
598: //===--------------------------------------------------------------------===//
599: void StringSegment::Update(ColumnData &column_data, SegmentStatistics &stats, Transaction &transaction, Vector &update,
600:                            row_t *ids, idx_t count, idx_t vector_index, idx_t vector_offset, UpdateInfo *node) {
601: 	if (!string_updates) {
602: 		string_updates = unique_ptr<string_update_info_t[]>(new string_update_info_t[max_vector_count]);
603: 	}
604: 
605: 	// first pin the base block
606: 	auto handle = manager.Pin(block_id);
607: 	auto baseptr = handle->node->buffer;
608: 	auto base = baseptr + vector_index * vector_size;
609: 	auto &base_nullmask = *((nullmask_t *)base);
610: 
611: 	// fetch the original string locations and copy the original nullmask
612: 	string_location_t string_locations[STANDARD_VECTOR_SIZE];
613: 	nullmask_t original_nullmask = base_nullmask;
614: 	FetchStringLocations(baseptr, ids, vector_index, vector_offset, count, string_locations);
615: 
616: 	string_update_info_t new_update_info;
617: 	// next up: create the updates
618: 	if (!string_updates[vector_index]) {
619: 		// no string updates yet, allocate a block and place the updates there
620: 		new_update_info = CreateStringUpdate(stats, update, ids, count, vector_offset);
621: 	} else {
622: 		// string updates already exist, merge the string updates together
623: 		new_update_info = MergeStringUpdate(stats, update, ids, count, vector_offset, *string_updates[vector_index]);
624: 	}
625: 
626: 	// now update the original nullmask
627: 	auto &update_nullmask = FlatVector::Nullmask(update);
628: 	for (idx_t i = 0; i < count; i++) {
629: 		base_nullmask[ids[i] - vector_offset] = update_nullmask[i];
630: 	}
631: 
632: 	// now that the original strings are placed in the undo buffer and the updated strings are placed in the base table
633: 	// create the update node
634: 	if (!node) {
635: 		// create a new node in the undo buffer for this update
636: 		node = CreateUpdateInfo(column_data, transaction, ids, count, vector_index, vector_offset,
637: 		                        sizeof(string_location_t));
638: 
639: 		// copy the string location data into the undo buffer
640: 		node->nullmask = original_nullmask;
641: 		memcpy(node->tuple_data, string_locations, sizeof(string_location_t) * count);
642: 	} else {
643: 		// node in the update info already exists, merge the new updates in
644: 		MergeUpdateInfo(node, ids, count, vector_offset, string_locations, original_nullmask);
645: 	}
646: 	// finally move the string updates in place
647: 	string_updates[vector_index] = move(new_update_info);
648: }
649: 
650: void StringSegment::RollbackUpdate(UpdateInfo *info) {
651: 	auto lock_handle = lock.GetExclusiveLock();
652: 
653: 	idx_t new_count = 0;
654: 	auto &update_info = *string_updates[info->vector_index];
655: 	auto string_locations = (string_location_t *)info->tuple_data;
656: 
657: 	// put the previous NULL values back
658: 	auto handle = manager.Pin(block_id);
659: 	auto baseptr = handle->node->buffer;
660: 	auto base = baseptr + info->vector_index * vector_size;
661: 	auto &base_nullmask = *((nullmask_t *)base);
662: 	for (idx_t i = 0; i < info->N; i++) {
663: 		base_nullmask[info->tuples[i]] = info->nullmask[info->tuples[i]];
664: 	}
665: 
666: 	// now put the original values back into the update info
667: 	idx_t old_idx = 0;
668: 	for (idx_t i = 0; i < update_info.count; i++) {
669: 		if (old_idx >= info->N || update_info.ids[i] != info->tuples[old_idx]) {
670: 			assert(old_idx >= info->N || update_info.ids[i] < info->tuples[old_idx]);
671: 			// this entry is not rolled back: insert entry directly
672: 			update_info.ids[new_count] = update_info.ids[i];
673: 			update_info.block_ids[new_count] = update_info.block_ids[i];
674: 			update_info.offsets[new_count] = update_info.offsets[i];
675: 			new_count++;
676: 		} else {
677: 			// this entry is being rolled back
678: 			auto &old_location = string_locations[old_idx];
679: 			if (old_location.block_id != INVALID_BLOCK) {
680: 				// not rolled back to base table: insert entry again
681: 				update_info.ids[new_count] = update_info.ids[i];
682: 				update_info.block_ids[new_count] = old_location.block_id;
683: 				update_info.offsets[new_count] = old_location.offset;
684: 				new_count++;
685: 			}
686: 			old_idx++;
687: 		}
688: 	}
689: 
690: 	if (new_count == 0) {
691: 		// all updates are rolled back: delete the string update vector
692: 		string_updates[info->vector_index].reset();
693: 	} else {
694: 		// set the count of the new string update vector
695: 		update_info.count = new_count;
696: 	}
697: 	CleanupUpdate(info);
698: }
[end of src/storage/string_segment.cpp]
[start of src/storage/uncompressed_segment.cpp]
1: #include "duckdb/storage/uncompressed_segment.hpp"
2: #include "duckdb/common/exception.hpp"
3: #include "duckdb/common/types/vector.hpp"
4: #include "duckdb/transaction/update_info.hpp"
5: 
6: using namespace duckdb;
7: using namespace std;
8: 
9: UncompressedSegment::UncompressedSegment(BufferManager &manager, TypeId type, idx_t row_start)
10:     : manager(manager), type(type), block_id(INVALID_BLOCK), max_vector_count(0), tuple_count(0), row_start(row_start),
11:       versions(nullptr) {
12: }
13: 
14: UncompressedSegment::~UncompressedSegment() {
15: 	if (block_id >= MAXIMUM_BLOCK) {
16: 		// if the uncompressed segment had an in-memory segment, destroy it when the uncompressed segment is destroyed
17: 		manager.DestroyBuffer(block_id);
18: 	}
19: }
20: 
21: static void CheckForConflicts(UpdateInfo *info, Transaction &transaction, row_t *ids, idx_t count, row_t offset,
22:                               UpdateInfo *&node) {
23: 	if (info->version_number == transaction.transaction_id) {
24: 		// this UpdateInfo belongs to the current transaction, set it in the node
25: 		node = info;
26: 	} else if (info->version_number > transaction.start_time) {
27: 		// potential conflict, check that tuple ids do not conflict
28: 		// as both ids and info->tuples are sorted, this is similar to a merge join
29: 		idx_t i = 0, j = 0;
30: 		while (true) {
31: 			auto id = ids[i] - offset;
32: 			if (id == info->tuples[j]) {
33: 				throw TransactionException("Conflict on update!");
34: 			} else if (id < info->tuples[j]) {
35: 				// id < the current tuple in info, move to next id
36: 				i++;
37: 				if (i == count) {
38: 					break;
39: 				}
40: 			} else {
41: 				// id > the current tuple, move to next tuple in info
42: 				j++;
43: 				if (j == info->N) {
44: 					break;
45: 				}
46: 			}
47: 		}
48: 	}
49: 	if (info->next) {
50: 		CheckForConflicts(info->next, transaction, ids, count, offset, node);
51: 	}
52: }
53: 
54: void UncompressedSegment::Update(ColumnData &column_data, SegmentStatistics &stats, Transaction &transaction,
55:                                  Vector &update, row_t *ids, idx_t count, row_t offset) {
56: 	// can only perform in-place updates on temporary blocks
57: 	assert(block_id >= MAXIMUM_BLOCK);
58: 
59: 	// obtain an exclusive lock
60: 	auto write_lock = lock.GetExclusiveLock();
61: 
62: #ifdef DEBUG
63: 	// verify that the ids are sorted and there are no duplicates
64: 	for (idx_t i = 1; i < count; i++) {
65: 		assert(ids[i] > ids[i - 1]);
66: 	}
67: #endif
68: 
69: 	// create the versions for this segment, if there are none yet
70: 	if (!versions) {
71: 		this->versions = unique_ptr<UpdateInfo *[]>(new UpdateInfo *[max_vector_count]);
72: 		for (idx_t i = 0; i < max_vector_count; i++) {
73: 			this->versions[i] = nullptr;
74: 		}
75: 	}
76: 
77: 	// get the vector index based on the first id
78: 	// we assert that all updates must be part of the same vector
79: 	auto first_id = ids[0];
80: 	idx_t vector_index = (first_id - offset) / STANDARD_VECTOR_SIZE;
81: 	idx_t vector_offset = offset + vector_index * STANDARD_VECTOR_SIZE;
82: 
83: 	assert(first_id >= offset);
84: 	assert(vector_index < max_vector_count);
85: 
86: 	// first check the version chain
87: 	UpdateInfo *node = nullptr;
88: 	if (versions[vector_index]) {
89: 		// there is already a version here, check if there are any conflicts and search for the node that belongs to
90: 		// this transaction in the version chain
91: 		CheckForConflicts(versions[vector_index], transaction, ids, count, vector_offset, node);
92: 	}
93: 	Update(column_data, stats, transaction, update, ids, count, vector_index, vector_offset, node);
94: }
95: 
96: UpdateInfo *UncompressedSegment::CreateUpdateInfo(ColumnData &column_data, Transaction &transaction, row_t *ids,
97:                                                   idx_t count, idx_t vector_index, idx_t vector_offset,
98:                                                   idx_t type_size) {
99: 	auto node = transaction.CreateUpdateInfo(type_size, STANDARD_VECTOR_SIZE);
100: 	node->column_data = &column_data;
101: 	node->segment = this;
102: 	node->vector_index = vector_index;
103: 	node->prev = nullptr;
104: 	node->next = versions[vector_index];
105: 	if (node->next) {
106: 		node->next->prev = node;
107: 	}
108: 	versions[vector_index] = node;
109: 
110: 	// set up the tuple ids
111: 	node->N = count;
112: 	for (idx_t i = 0; i < count; i++) {
113: 		assert((idx_t)ids[i] >= vector_offset && (idx_t)ids[i] < vector_offset + STANDARD_VECTOR_SIZE);
114: 		node->tuples[i] = ids[i] - vector_offset;
115: 	};
116: 	return node;
117: }
118: 
119: void UncompressedSegment::Fetch(ColumnScanState &state, idx_t vector_index, Vector &result) {
120: 	auto read_lock = lock.GetSharedLock();
121: 
122: 	InitializeScan(state);
123: 	FetchBaseData(state, vector_index, result);
124: }
125: 
126: //===--------------------------------------------------------------------===//
127: // Scan
128: //===--------------------------------------------------------------------===//
129: void UncompressedSegment::Scan(Transaction &transaction, ColumnScanState &state, idx_t vector_index, Vector &result) {
130: 	auto read_lock = lock.GetSharedLock();
131: 
132: 	// first fetch the data from the base table
133: 	FetchBaseData(state, vector_index, result);
134: 	if (versions && versions[vector_index]) {
135: 		// if there are any versions, check if we need to overwrite the data with the versioned data
136: 		FetchUpdateData(state, transaction, versions[vector_index], result);
137: 	}
138: }
139: 
140: void UncompressedSegment::IndexScan(ColumnScanState &state, idx_t vector_index, Vector &result) {
141: 	if (vector_index == 0) {
142: 		// vector_index = 0, obtain a shared lock on the segment that we keep until the index scan is complete
143: 		state.locks.push_back(lock.GetSharedLock());
144: 	}
145: 	if (versions && versions[vector_index]) {
146: 		throw TransactionException("Cannot create index with outstanding updates");
147: 	}
148: 	FetchBaseData(state, vector_index, result);
149: }
150: 
151: //===--------------------------------------------------------------------===//
152: // Update
153: //===--------------------------------------------------------------------===//
154: void UncompressedSegment::CleanupUpdate(UpdateInfo *info) {
155: 	if (info->prev) {
156: 		// there is a prev info: remove from the chain
157: 		auto prev = info->prev;
158: 		prev->next = info->next;
159: 		if (prev->next) {
160: 			prev->next->prev = prev;
161: 		}
162: 	} else {
163: 		// there is no prev info: remove from base segment
164: 		info->segment->versions[info->vector_index] = info->next;
165: 		if (info->next) {
166: 			info->next->prev = nullptr;
167: 		}
168: 	}
169: }
170: 
171: //===--------------------------------------------------------------------===//
172: // ToTemporary
173: //===--------------------------------------------------------------------===//
174: void UncompressedSegment::ToTemporary() {
175: 	auto write_lock = lock.GetExclusiveLock();
176: 
177: 	if (block_id >= MAXIMUM_BLOCK) {
178: 		// conversion has already been performed by a different thread
179: 		return;
180: 	}
181: 	// pin the current block
182: 	auto current = manager.Pin(block_id);
183: 
184: 	// now allocate a new block from the buffer manager
185: 	auto handle = manager.Allocate(Storage::BLOCK_ALLOC_SIZE);
186: 	// now copy the data over and switch to using the new block id
187: 	memcpy(handle->node->buffer, current->node->buffer, Storage::BLOCK_SIZE);
188: 	this->block_id = handle->block_id;
189: }
[end of src/storage/uncompressed_segment.cpp]
[start of tools/pythonpkg/setup.py]
1: #!/usr/bin/env python
2: # -*- coding: utf-8 -*-
3: 
4: import os
5: import numpy
6: import sys
7: import subprocess
8: import shutil
9: import platform
10: 
11: 
12: import distutils.spawn
13: from setuptools import setup, Extension
14: from setuptools.command.sdist import sdist
15: 
16: 
17: # make sure we are in the right directory
18: os.chdir(os.path.dirname(os.path.realpath(__file__)))
19: 
20: if not os.path.exists('duckdb.cpp'):
21:     prev_wd = os.getcwd()
22:     os.chdir('../..')
23:     subprocess.Popen('python scripts/amalgamation.py'.split(' ')).wait()
24:     os.chdir(prev_wd)
25:     shutil.copyfile('../../src/amalgamation/duckdb.hpp', 'duckdb.hpp')
26:     shutil.copyfile('../../src/amalgamation/duckdb.cpp', 'duckdb.cpp')
27: 
28: 
29: toolchain_args = ['-std=c++11']
30: #toolchain_args = ['-std=c++11', '-Wall', '-O0', '-g']
31: 
32: if platform.system() == 'Darwin':
33:     toolchain_args.extend(['-stdlib=libc++', '-mmacosx-version-min=10.7'])
34: 
35: class get_pybind_include(object):
36:     """Helper class to determine the pybind11 include path
37:     The purpose of this class is to postpone importing pybind11
38:     until it is actually installed, so that the ``get_include()``
39:     method can be invoked. """
40: 
41:     def __init__(self, user=False):
42:         self.user = user
43: 
44:     def __str__(self):
45:         import pybind11
46:         return pybind11.get_include(self.user)
47: 
48: 
49: libduckdb = Extension('duckdb',
50:     include_dirs=[numpy.get_include(), '.', get_pybind_include(), get_pybind_include(user=True)],
51:     sources=['duckdb_python.cpp', 'duckdb.cpp'],
52:     extra_compile_args=toolchain_args,
53:     extra_link_args=toolchain_args,
54:     language='c++')
55: 
56: # Only include pytest-runner in setup_requires if we're invoking tests
57: if {'pytest', 'test', 'ptr'}.intersection(sys.argv):
58:     setup_requires = ['pytest-runner']
59: else:
60:     setup_requires = []
61: 
62: setup(
63:     name = "duckdb",
64:     description = 'DuckDB embedded database',
65:     keywords = 'DuckDB Database SQL OLAP',
66:     url="https://www.duckdb.org",
67:     long_description = '',
68:     install_requires=[ # these versions are still available for Python 2, newer ones aren't
69:          'numpy>=1.14',
70:          'pandas>=0.23',
71:     ],
72:     packages=['duckdb_query_graph'],
73:     include_package_data=True,
74:     setup_requires=setup_requires + ["setuptools_scm"] + ['pybind11>=2.4'],
75:     use_scm_version = {"root": "../..", "relative_to": __file__},
76:     tests_require=['pytest'],
77:     classifiers = [
78:         'Topic :: Database :: Database Engines/Servers',
79:         'Intended Audience :: Developers'
80:     ],
81:     ext_modules = [libduckdb],
82:     maintainer = "Hannes Muehleisen",
83:     maintainer_email = "hannes@cwi.nl"
84: )
[end of tools/pythonpkg/setup.py]
[start of tools/rpkg/configure]
1: #!/bin/sh
2: 
3: if [ ! -f "src/duckdb.cpp" ]; then
4: 	if [ ! -f "../../scripts/amalgamation.py" ]; then
5: 		echo "Could find neither duckdb.cpp nor the build script"
6: 		exit 1
7: 	fi
8: 	(cd ../.. && python scripts/amalgamation.py)
9: 	cat ../../src/amalgamation/duckdb.cpp | sed 's|#include "duckdb.hpp"|#include "duckdb.h"|' > src/duckdb.cpp
10: 	cp ../../src/amalgamation/duckdb.hpp src/duckdb.h
11: 
12: fi
[end of tools/rpkg/configure]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: