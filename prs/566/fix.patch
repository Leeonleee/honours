diff --git a/CMakeLists.txt b/CMakeLists.txt
index 15d7e0fed3d6..4235af2dba4b 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -178,6 +178,7 @@ function(enable_unity_build UB_SUFFIX SOURCE_VARIABLE_NAME)
 
   # Generate a unique filename for the unity build translation unit
   set(unit_build_file ${CMAKE_CURRENT_BINARY_DIR}/ub_${UB_SUFFIX}.cpp)
+  set(temp_unit_build_file ${CMAKE_CURRENT_BINARY_DIR}/ub_${UB_SUFFIX}.cpp.tmp)
   # Exclude all translation units from compilation
   set_source_files_properties(${files}
                               PROPERTIES
@@ -185,12 +186,32 @@ function(enable_unity_build UB_SUFFIX SOURCE_VARIABLE_NAME)
                               true)
 
   set(rebuild FALSE)
+  # check if any of the source files have changed
   foreach(source_file ${files})
     if(${CMAKE_CURRENT_SOURCE_DIR}/${source_file} IS_NEWER_THAN
        ${unit_build_file})
       set(rebuild TRUE)
     endif()
   endforeach(source_file)
+  # write a temporary file
+  file(WRITE ${temp_unit_build_file} "// Unity Build generated by CMake
")
+  foreach(source_file ${files})
+    file(
+      APPEND ${temp_unit_build_file}
+      "#line 0 \"${source_file}\"
#include <${CMAKE_CURRENT_SOURCE_DIR}/${source_file}>
"
+      )
+  endforeach(source_file)
+
+  execute_process( COMMAND ${CMAKE_COMMAND} -E compare_files ${unit_build_file} ${temp_unit_build_file} RESULT_VARIABLE compare_result OUTPUT_VARIABLE bla ERROR_VARIABLE bla)
+  if( compare_result EQUAL 0)
+    # files are identical: do nothing
+  elseif( compare_result EQUAL 1)
+    # files are different: rebuild
+    set(rebuild TRUE)
+  else()
+    # error while compiling: rebuild
+    set(rebuild TRUE)
+  endif()
 
   if(${rebuild})
     file(WRITE ${unit_build_file} "// Unity Build generated by CMake
")
diff --git a/Makefile b/Makefile
index 51cacd7ec3e9..893d03a71254 100644
--- a/Makefile
+++ b/Makefile
@@ -56,6 +56,13 @@ amalgamation:
 	cmake $(GENERATOR) $(FORCE_COLOR) -DAMALGAMATION_BUILD=1 -DCMAKE_BUILD_TYPE=Release ../.. && \
 	cmake --build .
 
+amaldebug:
+	mkdir -p build/amaldebug && \
+	python scripts/amalgamation.py && \
+	cd build/amaldebug && \
+	cmake $(GENERATOR) $(FORCE_COLOR) -DAMALGAMATION_BUILD=1 -DCMAKE_BUILD_TYPE=Debug ../.. && \
+	cmake --build .
+
 jdbc:
 	mkdir -p build/jdbc && \
 	cd build/jdbc && \
diff --git a/scripts/amalgamation.py b/scripts/amalgamation.py
index fc33fd60a394..46f6282e3ecd 100644
--- a/scripts/amalgamation.py
+++ b/scripts/amalgamation.py
@@ -1,8 +1,10 @@
 # this script creates a single header + source file combination out of the DuckDB sources
-import os, re, sys, pickle
+import os, re, sys, shutil
 amal_dir = os.path.join('src', 'amalgamation')
 header_file = os.path.join(amal_dir, "duckdb.hpp")
 source_file = os.path.join(amal_dir, "duckdb.cpp")
+temp_header = 'duckdb.hpp.tmp'
+temp_source = 'duckdb.cpp.tmp'
 
 src_dir = 'src'
 include_dir = os.path.join('src', 'include')
@@ -25,168 +27,145 @@
 # paths of where to look for files to compile and include to the final amalgamation
 compile_directories = [src_dir, fmt_dir, hll_dir, miniz_dir, re2_dir, utf8proc_dir, pg_query_dir]
 
+# files always excluded
+always_excluded = ['src/amalgamation/duckdb.cpp', 'src/amalgamation/duckdb.hpp']
 # files excluded from the amalgamation
 excluded_files = ['grammar.cpp', 'grammar.hpp', 'symbols.cpp', 'file_system.cpp']
 # files excluded from individual file compilation during test_compile
 excluded_compilation_files = excluded_files + ['gram.hpp', 'kwlist.hpp', "duckdb-c.cpp"]
-# where to cache which files have already been compiled, only used for --compile --resume
-cache_file = 'amalgamation.cache'
 
 
 linenumbers = False
-compile = False
-resume = False
-
-for arg in sys.argv:
-	if arg == '--compile':
-		compile = True
-	elif arg == '--resume':
-		resume = True
-	elif arg == '--linenumbers':
-		linenumbers = True
-	elif arg == '--no-linenumbers':
-		linenumbers = False
-
-if not resume:
-	try:
-		os.remove(cache_file)
-	except:
-		pass
 
 def get_includes(fpath, text):
-	# find all the includes referred to in the directory
-	include_statements = re.findall("(^[#]include[\t ]+[\"]([^\"]+)[\"])", text, flags=re.MULTILINE)
-	include_files = []
-	# figure out where they are located
-	for included_file in [x[1] for x in include_statements]:
-		included_file = os.sep.join(included_file.split('/'))
-		found = False
-		for include_path in include_paths:
-			ipath = os.path.join(include_path, included_file)
-			if os.path.isfile(ipath):
-				include_files.append(ipath)
-				found = True
-				break
-		if not found:
-			raise Exception('Could not find include file "' + included_file + '", included from file "' + fpath + '"')
-	return ([x[0] for x in include_statements], include_files)
+    # find all the includes referred to in the directory
+    include_statements = re.findall("(^[#]include[\t ]+[\"]([^\"]+)[\"])", text, flags=re.MULTILINE)
+    include_files = []
+    # figure out where they are located
+    for included_file in [x[1] for x in include_statements]:
+        included_file = os.sep.join(included_file.split('/'))
+        found = False
+        for include_path in include_paths:
+            ipath = os.path.join(include_path, included_file)
+            if os.path.isfile(ipath):
+                include_files.append(ipath)
+                found = True
+                break
+        if not found:
+            raise Exception('Could not find include file "' + included_file + '", included from file "' + fpath + '"')
+    return ([x[0] for x in include_statements], include_files)
 
 def cleanup_file(text):
-	# remove all "#pragma once" notifications
-	text = re.sub('#pragma once', '', text)
-	return text
+    # remove all "#pragma once" notifications
+    text = re.sub('#pragma once', '', text)
+    return text
 
 # recursively get all includes and write them
 written_files = {}
 
 def write_file(current_file, ignore_excluded = False):
-	global linenumbers
-	global written_files
-	if current_file.split(os.sep)[-1] in excluded_files and not ignore_excluded:
-		# file is in ignored files set
-		return ""
-	if current_file in written_files:
-		# file is already written
-		return ""
-	written_files[current_file] = True
-
-	# first read this file
-	with open(current_file, 'r') as f:
-		text = f.read()
-
-	(statements, includes) = get_includes(current_file, text)
-	# find the linenr of the final #include statement we parsed
-	if len(statements) > 0:
-		index = text.find(statements[-1])
-		linenr = len(text[:index].split('
'))
-
-		# now write all the dependencies of this header first
-		for i in range(len(includes)):
-			include_text = write_file(includes[i])
-			if linenumbers and i == len(includes) - 1:
-				# for the last include statement, we also include a #line directive
-				include_text += '
#line %d "%s"
' % (linenr, current_file)
-			text = text.replace(statements[i], include_text)
-
-	# add the initial line here
-	if linenumbers:
-		text = '
#line 1 "%s"
' % (current_file,) + text
-	print(current_file)
-	# now read the header and write it
-	return cleanup_file(text)
-
-def try_compilation(fpath, cache):
-	if fpath in cache:
-		return
-	print(fpath)
-
-	cmd = 'clang++ -std=c++11 -Wno-deprecated -Wno-writable-strings -S -MMD -MF dependencies.d -o deps.s ' + fpath + ' ' + ' '.join(["-I" + x for x in include_paths])
-	ret = os.system(cmd)
-	if ret != 0:
-		raise Exception('Failed compilation of file "' + fpath + '"!
 Command: ' + cmd)
-	cache[fpath] = True
-	with open(cache_file, 'wb') as cf:
-		pickle.dump(cache, cf)
-
-def compile_dir(dir, cache):
-	files = os.listdir(dir)
-	files.sort()
-	for fname in files:
-		if fname in excluded_compilation_files:
-			continue
-		fpath = os.path.join(dir, fname)
-		if os.path.isdir(fpath):
-			compile_dir(fpath, cache)
-		elif fname.endswith('.cpp') or fname.endswith('.hpp') or fname.endswith('.c') or fname.endswith('.cc'):
-			try_compilation(fpath, cache)
-
-if compile:
-	# compilation pass only
-	# compile all files in the src directory (including headers!) individually
-	try:
-		with open(cache_file, 'rb') as cf:
-			cache = pickle.load(cf)
-	except:
-		cache = {}
-	for cdir in compile_directories:
-		compile_dir(cdir, cache)
-	exit(0)
-
-
-if not os.path.exists(amal_dir):
-	os.makedirs(amal_dir)
-
-# now construct duckdb.hpp from these headers
-print("-----------------------")
-print("-- Writing duckdb.hpp --")
-print("-----------------------")
-with open(header_file, 'w+') as hfile:
-	hfile.write("#pragma once
")
-	for fpath in main_header_files:
-		hfile.write(write_file(fpath))
+    global linenumbers
+    global written_files
+    if current_file in always_excluded:
+        return ""
+    if current_file.split(os.sep)[-1] in excluded_files and not ignore_excluded:
+        # file is in ignored files set
+        return ""
+    if current_file in written_files:
+        # file is already written
+        return ""
+    written_files[current_file] = True
+
+    # first read this file
+    with open(current_file, 'r') as f:
+        text = f.read()
+
+    (statements, includes) = get_includes(current_file, text)
+    # find the linenr of the final #include statement we parsed
+    if len(statements) > 0:
+        index = text.find(statements[-1])
+        linenr = len(text[:index].split('
'))
+
+        # now write all the dependencies of this header first
+        for i in range(len(includes)):
+            include_text = write_file(includes[i])
+            if linenumbers and i == len(includes) - 1:
+                # for the last include statement, we also include a #line directive
+                include_text += '
#line %d "%s"
' % (linenr, current_file)
+            text = text.replace(statements[i], include_text)
+
+    # add the initial line here
+    if linenumbers:
+        text = '
#line 1 "%s"
' % (current_file,) + text
+    print(current_file)
+    # now read the header and write it
+    return cleanup_file(text)
 
 def write_dir(dir, sfile):
-	files = os.listdir(dir)
-	files.sort()
-	for fname in files:
-		if fname in excluded_files:
-			continue
-		fpath = os.path.join(dir, fname)
-		if os.path.isdir(fpath):
-			write_dir(fpath, sfile)
-		elif fname.endswith('.cpp') or fname.endswith('.c') or fname.endswith('.cc'):
-			sfile.write(write_file(fpath))
-
-# now construct duckdb.cpp
-print("------------------------")
-print("-- Writing duckdb.cpp --")
-print("------------------------")
-
-# scan all the .cpp files
-with open(source_file, 'w+') as sfile:
-	sfile.write('#include "duckdb.hpp"

')
-	for compile_dir in compile_directories:
-		write_dir(compile_dir, sfile)
-	# for windows we write file_system.cpp last
-	# this is because it includes windows.h which contains a lot of #define statements that mess up the other code
-	sfile.write(write_file(os.path.join('src', 'common', 'file_system.cpp'), True))
+    files = os.listdir(dir)
+    files.sort()
+    for fname in files:
+        if fname in excluded_files:
+            continue
+        fpath = os.path.join(dir, fname)
+        if os.path.isdir(fpath):
+            write_dir(fpath, sfile)
+        elif fname.endswith('.cpp') or fname.endswith('.c') or fname.endswith('.cc'):
+            sfile.write(write_file(fpath))
+
+def copy_if_different(src, dest):
+    if os.path.isfile(dest):
+        # dest exists, check if the files are different
+        with open(src, 'r') as f:
+            source_text = f.read()
+        with open(dest, 'r') as f:
+            dest_text = f.read()
+        if source_text == dest_text:
+            return
+    shutil.copyfile(src, dest)
+
+def generate_amalgamation(source_file, header_file):
+    # now construct duckdb.hpp from these headers
+    print("-----------------------")
+    print("-- Writing " + header_file + " --")
+    print("-----------------------")
+    with open(temp_header, 'w+') as hfile:
+        hfile.write("#pragma once
")
+        for fpath in main_header_files:
+            hfile.write(write_file(fpath))
+
+
+    # now construct duckdb.cpp
+    print("------------------------")
+    print("-- Writing " + source_file + " --")
+    print("------------------------")
+
+    # scan all the .cpp files
+    with open(temp_source, 'w+') as sfile:
+        header_file_name = header_file.split(os.sep)[-1]
+        sfile.write('#include "' + header_file_name + '"

')
+        for compile_dir in compile_directories:
+            write_dir(compile_dir, sfile)
+        # for windows we write file_system.cpp last
+        # this is because it includes windows.h which contains a lot of #define statements that mess up the other code
+        sfile.write(write_file(os.path.join('src', 'common', 'file_system.cpp'), True))
+
+    copy_if_different(temp_header, header_file)
+    copy_if_different(temp_source, source_file)
+
+
+
+if __name__ == "__main__":
+    for arg in sys.argv:
+        if arg == '--linenumbers':
+            linenumbers = True
+        elif arg == '--no-linenumbers':
+            linenumbers = False
+        elif arg.startswith('--header='):
+            header_file = os.path.join(*arg.split('=', 1)[1].split('/'))
+        elif arg.startswith('--source='):
+            source_file = os.path.join(*arg.split('=', 1)[1].split('/'))
+    if not os.path.exists(amal_dir):
+        os.makedirs(amal_dir)
+
+    generate_amalgamation(source_file, header_file)
diff --git a/src/include/duckdb/storage/block_manager.hpp b/src/include/duckdb/storage/block_manager.hpp
index ccc807b15714..2cbc1f910572 100644
--- a/src/include/duckdb/storage/block_manager.hpp
+++ b/src/include/duckdb/storage/block_manager.hpp
@@ -19,6 +19,7 @@ class BlockManager {
 public:
 	virtual ~BlockManager() = default;
 
+	virtual void StartCheckpoint() = 0;
 	//! Creates a new block inside the block manager
 	virtual unique_ptr<Block> CreateBlock() = 0;
 	//! Return the next free block id
diff --git a/src/include/duckdb/storage/checkpoint/table_data_writer.hpp b/src/include/duckdb/storage/checkpoint/table_data_writer.hpp
index d662ab3bfb68..8f3b1fa586e0 100644
--- a/src/include/duckdb/storage/checkpoint/table_data_writer.hpp
+++ b/src/include/duckdb/storage/checkpoint/table_data_writer.hpp
@@ -24,12 +24,13 @@ class TableDataWriter {
 	void WriteTableData(Transaction &transaction);
 
 private:
-	void AppendData(idx_t col_idx, Vector &data, idx_t count);
+	void AppendData(Transaction &transaction, idx_t col_idx, Vector &data, idx_t count);
 
 	void CreateSegment(idx_t col_idx);
-	void FlushSegment(idx_t col_idx);
+	void FlushSegment(Transaction &transaction, idx_t col_idx);
 
 	void WriteDataPointers();
+	void VerifyDataPointers();
 
 private:
 	CheckpointManager &manager;
diff --git a/src/include/duckdb/storage/in_memory_block_manager.hpp b/src/include/duckdb/storage/in_memory_block_manager.hpp
index 1242959a5a81..ac565aea2749 100644
--- a/src/include/duckdb/storage/in_memory_block_manager.hpp
+++ b/src/include/duckdb/storage/in_memory_block_manager.hpp
@@ -17,6 +17,9 @@ namespace duckdb {
 //! InMemoryBlockManager is an implementation for a BlockManager
 class InMemoryBlockManager : public BlockManager {
 public:
+	void StartCheckpoint() override {
+		throw Exception("Cannot perform IO in in-memory database!");
+	}
 	unique_ptr<Block> CreateBlock() override {
 		throw Exception("Cannot perform IO in in-memory database!");
 	}
diff --git a/src/include/duckdb/storage/single_file_block_manager.hpp b/src/include/duckdb/storage/single_file_block_manager.hpp
index 6fac59a63228..960058489638 100644
--- a/src/include/duckdb/storage/single_file_block_manager.hpp
+++ b/src/include/duckdb/storage/single_file_block_manager.hpp
@@ -26,6 +26,7 @@ class SingleFileBlockManager : public BlockManager {
 public:
 	SingleFileBlockManager(FileSystem &fs, string path, bool read_only, bool create_new, bool use_direct_io);
 
+	void StartCheckpoint() override;
 	//! Creates a new Block and returns a pointer
 	unique_ptr<Block> CreateBlock() override;
 	//! Return the next free block id
diff --git a/src/include/duckdb/storage/string_segment.hpp b/src/include/duckdb/storage/string_segment.hpp
index 12e873f4e75c..bbc5fba9745b 100644
--- a/src/include/duckdb/storage/string_segment.hpp
+++ b/src/include/duckdb/storage/string_segment.hpp
@@ -76,7 +76,6 @@ class StringSegment : public UncompressedSegment {
 
 	//! Rollback a previous update
 	void RollbackUpdate(UpdateInfo *info) override;
-
 protected:
 	void Update(ColumnData &column_data, SegmentStatistics &stats, Transaction &transaction, Vector &update, row_t *ids,
 	            idx_t count, idx_t vector_index, idx_t vector_offset, UpdateInfo *node) override;
@@ -126,7 +125,6 @@ class StringSegment : public UncompressedSegment {
 	idx_t RemainingSpace() {
 		return Storage::BLOCK_SIZE - dictionary_offset - max_vector_count * vector_size;
 	}
-
 private:
 	//! The max string size that is allowed within a block. Strings bigger than this will be labeled as a BIG STRING and
 	//! offloaded to the overflow blocks.
diff --git a/src/include/duckdb/storage/uncompressed_segment.hpp b/src/include/duckdb/storage/uncompressed_segment.hpp
index 4cc5aca54505..92853b07cb84 100644
--- a/src/include/duckdb/storage/uncompressed_segment.hpp
+++ b/src/include/duckdb/storage/uncompressed_segment.hpp
@@ -87,6 +87,7 @@ class UncompressedSegment {
 		return std::min((idx_t)STANDARD_VECTOR_SIZE, tuple_count - vector_index * STANDARD_VECTOR_SIZE);
 	}
 
+	virtual void Verify(Transaction &transaction);
 protected:
 	virtual void Update(ColumnData &data, SegmentStatistics &stats, Transaction &transaction, Vector &update,
 	                    row_t *ids, idx_t count, idx_t vector_index, idx_t vector_offset, UpdateInfo *node) = 0;
diff --git a/src/optimizer/rule/like_optimizations.cpp b/src/optimizer/rule/like_optimizations.cpp
index 3edcd0d41399..35d048dbca82 100644
--- a/src/optimizer/rule/like_optimizations.cpp
+++ b/src/optimizer/rule/like_optimizations.cpp
@@ -31,7 +31,7 @@ unique_ptr<Expression> LikeOptimizationRule::Apply(LogicalOperator &op, vector<E
 
 	// the constant_expr is a scalar expression that we have to fold
 	if (!constant_expr->IsFoldable()) {
-		return move(root->Copy());
+		return root->Copy();
 	}
 
 	auto constant_value = ExpressionExecutor::EvaluateScalar(*constant_expr);
@@ -39,19 +39,19 @@ unique_ptr<Expression> LikeOptimizationRule::Apply(LogicalOperator &op, vector<E
 	string patt_str = string(((string_t)constant_value.str_value).GetData());
 
 	if (std::regex_match(patt_str, std::regex("[^%_]*[%]+"))) {
-		//^^^^^^^^^^^^^^^Prefix LIKE pattern : [^%_]*[%]+, ignoring undescore
+		// Prefix LIKE pattern : [^%_]*[%]+, ignoring underscore
 
-		return move(ApplyRule(root, PrefixFun::GetFunction(), patt_str));
+		return ApplyRule(root, PrefixFun::GetFunction(), patt_str);
 
 	} else if (std::regex_match(patt_str, std::regex("[%]+[^%_]*"))) {
-		//^^^^^^^^^^^^^^^^^^^^^^^Suffix LIKE pattern: [%]+[^%_]*, ignoring undescore
+		// Suffix LIKE pattern: [%]+[^%_]*, ignoring underscore
 
-		return move(ApplyRule(root, SuffixFun::GetFunction(), patt_str));
+		return ApplyRule(root, SuffixFun::GetFunction(), patt_str);
 
 	} else if (std::regex_match(patt_str, std::regex("[%]+[^%_]*[%]+"))) {
-		//^^^^^^^^^^^^^^^^^^^^^Contains LIKE pattern: [%]+[^%_]*[%]+, ignoring undescore
+		// Contains LIKE pattern: [%]+[^%_]*[%]+, ignoring underscore
 
-		return move(ApplyRule(root, ContainsFun::GetFunction(), patt_str));
+		return ApplyRule(root, ContainsFun::GetFunction(), patt_str);
 	}
 
 	return nullptr;
@@ -67,5 +67,5 @@ unique_ptr<Expression> LikeOptimizationRule::ApplyRule(BoundFunctionExpression *
 
 	expr->children[1] = make_unique<BoundConstantExpression>(Value(pattern));
 
-	return move(expr->Copy());
+	return expr->Copy();
 }
diff --git a/src/storage/checkpoint/table_data_reader.cpp b/src/storage/checkpoint/table_data_reader.cpp
index 43f7d06189ac..0dc32995a642 100644
--- a/src/storage/checkpoint/table_data_reader.cpp
+++ b/src/storage/checkpoint/table_data_reader.cpp
@@ -9,6 +9,9 @@
 
 #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
 
+#include "duckdb/main/database.hpp"
+#include "duckdb/main/client_context.hpp"
+
 using namespace duckdb;
 using namespace std;
 
@@ -23,8 +26,10 @@ void TableDataReader::ReadTableData() {
 	assert(columns.size() > 0);
 
 	// load the data pointers for the table
+	idx_t table_count = 0;
 	for (idx_t col = 0; col < columns.size(); col++) {
 		auto &column = columns[col];
+		idx_t column_count = 0;
 		idx_t data_pointer_count = reader.Read<idx_t>();
 		for (idx_t data_ptr = 0; data_ptr < data_pointer_count; data_ptr++) {
 			// read the data pointer
@@ -38,11 +43,19 @@ void TableDataReader::ReadTableData() {
 			reader.ReadData(data_pointer.min_stats, 8);
 			reader.ReadData(data_pointer.max_stats, 8);
 
+			column_count += data_pointer.tuple_count;
 			// create a persistent segment
 			auto segment = make_unique<PersistentSegment>(
 			    manager.buffer_manager, data_pointer.block_id, data_pointer.offset, GetInternalType(column.type),
 			    data_pointer.row_start, data_pointer.tuple_count, data_pointer.min_stats, data_pointer.max_stats);
 			info.data[col].push_back(move(segment));
 		}
+		if (col == 0) {
+			table_count = column_count;
+		} else {
+			if (table_count != column_count) {
+				throw Exception("Column length mismatch in table load!");
+			}
+		}
 	}
 }
diff --git a/src/storage/checkpoint/table_data_writer.cpp b/src/storage/checkpoint/table_data_writer.cpp
index aa47b9bbbff8..ee9749316bf1 100644
--- a/src/storage/checkpoint/table_data_writer.cpp
+++ b/src/storage/checkpoint/table_data_writer.cpp
@@ -75,15 +75,17 @@ void TableDataWriter::WriteTableData(Transaction &transaction) {
 			break;
 		}
 		// for each column, we append whatever we can fit into the block
+		idx_t chunk_size = chunk.size();
 		for (idx_t i = 0; i < table.columns.size(); i++) {
 			assert(chunk.data[i].type == GetInternalType(table.columns[i].type));
-			AppendData(i, chunk.data[i], chunk.size());
+			AppendData(transaction, i, chunk.data[i], chunk_size);
 		}
 	}
 	// flush any remaining data and write the data pointers to disk
 	for (idx_t i = 0; i < table.columns.size(); i++) {
-		FlushSegment(i);
+		FlushSegment(transaction, i);
 	}
+	VerifyDataPointers();
 	WriteDataPointers();
 }
 
@@ -98,16 +100,16 @@ void TableDataWriter::CreateSegment(idx_t col_idx) {
 	}
 }
 
-void TableDataWriter::AppendData(idx_t col_idx, Vector &data, idx_t count) {
+void TableDataWriter::AppendData(Transaction &transaction, idx_t col_idx, Vector &data, idx_t count) {
 	idx_t offset = 0;
-	while (offset < count) {
+	while (count > 0) {
 		idx_t appended = segments[col_idx]->Append(*stats[col_idx], data, offset, count);
 		if (appended == count) {
 			// appended everything: finished
 			return;
 		}
 		// the segment is full: flush it to disk
-		FlushSegment(col_idx);
+		FlushSegment(transaction, col_idx);
 
 		// now create a new segment and continue appending
 		CreateSegment(col_idx);
@@ -116,7 +118,7 @@ void TableDataWriter::AppendData(idx_t col_idx, Vector &data, idx_t count) {
 	}
 }
 
-void TableDataWriter::FlushSegment(idx_t col_idx) {
+void TableDataWriter::FlushSegment(Transaction &transaction, idx_t col_idx) {
 	auto tuple_count = segments[col_idx]->tuple_count;
 	if (tuple_count == 0) {
 		return;
@@ -145,6 +147,33 @@ void TableDataWriter::FlushSegment(idx_t col_idx) {
 	data_pointers[col_idx].push_back(move(data_pointer));
 	// write the block to disk
 	manager.block_manager.Write(*handle->node, block_id);
+
+	handle.reset();
+	segments[col_idx] = nullptr;
+}
+
+void TableDataWriter::VerifyDataPointers() {
+	// verify the data pointers
+	idx_t table_count = 0;
+	for (idx_t i = 0; i < data_pointers.size(); i++) {
+		auto &data_pointer_list = data_pointers[i];
+		idx_t column_count = 0;
+		// then write the data pointers themselves
+		for (idx_t k = 0; k < data_pointer_list.size(); k++) {
+			auto &data_pointer = data_pointer_list[k];
+			column_count += data_pointer.tuple_count;
+		}
+		if (segments[i]) {
+			column_count += segments[i]->tuple_count;
+		}
+		if (i == 0) {
+			table_count = column_count;
+		} else {
+			if (table_count != column_count) {
+				throw Exception("Column count mismatch in data write!");
+			}
+		}
+	}
 }
 
 void TableDataWriter::WriteDataPointers() {
diff --git a/src/storage/checkpoint_manager.cpp b/src/storage/checkpoint_manager.cpp
index 59ec3a4ae1bc..be531ba44598 100644
--- a/src/storage/checkpoint_manager.cpp
+++ b/src/storage/checkpoint_manager.cpp
@@ -41,6 +41,7 @@ void CheckpointManager::CreateCheckpoint() {
 	assert(!metadata_writer);
 
 	auto transaction = database.transaction_manager->StartTransaction();
+	block_manager.StartCheckpoint();
 
 	//! Set up the writers for the checkpoints
 	metadata_writer = make_unique<MetaBlockWriter>(block_manager);
diff --git a/src/storage/single_file_block_manager.cpp b/src/storage/single_file_block_manager.cpp
index 8234ce07a412..8c0bd8d3c3fb 100644
--- a/src/storage/single_file_block_manager.cpp
+++ b/src/storage/single_file_block_manager.cpp
@@ -105,22 +105,30 @@ void SingleFileBlockManager::LoadFreeList(BufferManager &manager) {
 	}
 	MetaBlockReader reader(manager, free_list_id);
 	auto free_list_count = reader.Read<uint64_t>();
+	free_list.clear();
 	free_list.reserve(free_list_count);
 	for (idx_t i = 0; i < free_list_count; i++) {
 		free_list.push_back(reader.Read<block_id_t>());
 	}
 }
 
+void SingleFileBlockManager::StartCheckpoint() {
+	used_blocks.clear();
+}
+
 block_id_t SingleFileBlockManager::GetFreeBlockId() {
+	block_id_t block;
 	if (free_list.size() > 0) {
 		// free list is non empty
 		// take an entry from the free list
-		block_id_t block = free_list.back();
+		block = free_list.back();
 		// erase the entry from the free list again
 		free_list.pop_back();
-		return block;
+	} else {
+		block = max_block++;
 	}
-	return max_block++;
+	used_blocks.insert(block);
+	return block;
 }
 
 block_id_t SingleFileBlockManager::GetMetaBlock() {
@@ -133,7 +141,7 @@ unique_ptr<Block> SingleFileBlockManager::CreateBlock() {
 
 void SingleFileBlockManager::Read(Block &block) {
 	assert(block.id >= 0);
-	used_blocks.insert(block.id);
+	assert(std::find(free_list.begin(), free_list.end(), block.id) == free_list.end());
 	block.Read(*handle, BLOCK_START + block.id * Storage::BLOCK_ALLOC_SIZE);
 }
 
@@ -147,13 +155,24 @@ void SingleFileBlockManager::WriteHeader(DatabaseHeader header) {
 	header.iteration = ++iteration_count;
 	header.block_count = max_block;
 	// now handle the free list
-	if (used_blocks.size() > 0) {
+	free_list.clear();
+	for(block_id_t i = 0; i < max_block; i++) {
+		if (used_blocks.find(i) == used_blocks.end()) {
+			free_list.push_back(i);
+		}
+	}
+	if (free_list.size() > 0) {
 		// there are blocks in the free list
 		// write them to the file
 		MetaBlockWriter writer(*this);
+		auto entry = std::find(free_list.begin(), free_list.end(), writer.block->id);
+		if (entry != free_list.end()) {
+			free_list.erase(entry);
+		}
 		header.free_list = writer.block->id;
-		writer.Write<uint64_t>(used_blocks.size());
-		for (auto &block_id : used_blocks) {
+
+		writer.Write<uint64_t>(free_list.size());
+		for (auto &block_id : free_list) {
 			writer.Write<block_id_t>(block_id);
 		}
 		writer.Flush();
@@ -178,6 +197,7 @@ void SingleFileBlockManager::WriteHeader(DatabaseHeader header) {
 	handle->Sync();
 
 	// the free list is now equal to the blocks that were used by the previous iteration
+	free_list.clear();
 	for (auto &block_id : used_blocks) {
 		free_list.push_back(block_id);
 	}
diff --git a/src/storage/string_segment.cpp b/src/storage/string_segment.cpp
index 53c372f4e6c9..d212a0da5678 100644
--- a/src/storage/string_segment.cpp
+++ b/src/storage/string_segment.cpp
@@ -182,6 +182,7 @@ string_location_t StringSegment::FetchStringLocation(data_ptr_t baseptr, int32_t
 
 string_t StringSegment::FetchStringFromDict(buffer_handle_set_t &handles, data_ptr_t baseptr, int32_t dict_offset) {
 	// fetch base data
+	assert(dict_offset <= Storage::BLOCK_SIZE);
 	string_location_t location = FetchStringLocation(baseptr, dict_offset);
 	return FetchString(handles, baseptr, location);
 }
@@ -371,6 +372,7 @@ void StringSegment::AppendData(SegmentStatistics &stats, data_ptr_t target, data
 				memcpy(dict_pos + sizeof(uint16_t), sdata[source_idx].GetData(), string_length + 1);
 			}
 			// place the dictionary offset into the set of vectors
+			assert(dictionary_offset <= Storage::BLOCK_SIZE);
 			result_data[target_idx] = dictionary_offset;
 		}
 		remaining_strings--;
diff --git a/src/storage/uncompressed_segment.cpp b/src/storage/uncompressed_segment.cpp
index 9465384e03b0..5a0034a377df 100644
--- a/src/storage/uncompressed_segment.cpp
+++ b/src/storage/uncompressed_segment.cpp
@@ -18,6 +18,21 @@ UncompressedSegment::~UncompressedSegment() {
 	}
 }
 
+void UncompressedSegment::Verify(Transaction &transaction) {
+#ifdef DEBUG
+	ColumnScanState state;
+	InitializeScan(state);
+
+	Vector result(this->type);
+	for(idx_t i = 0; i < this->tuple_count; i+= STANDARD_VECTOR_SIZE) {
+		idx_t vector_idx = i / STANDARD_VECTOR_SIZE;
+		idx_t count = std::min((idx_t) STANDARD_VECTOR_SIZE, tuple_count - i);
+		Scan(transaction, state, vector_idx, result);
+		result.Verify(count);
+	}
+#endif
+}
+
 static void CheckForConflicts(UpdateInfo *info, Transaction &transaction, row_t *ids, idx_t count, row_t offset,
                               UpdateInfo *&node) {
 	if (info->version_number == transaction.transaction_id) {
diff --git a/tools/pythonpkg/setup.py b/tools/pythonpkg/setup.py
index 2c5822a8f86b..ab7c08bab56e 100755
--- a/tools/pythonpkg/setup.py
+++ b/tools/pythonpkg/setup.py
@@ -17,13 +17,16 @@
 # make sure we are in the right directory
 os.chdir(os.path.dirname(os.path.realpath(__file__)))
 
-if not os.path.exists('duckdb.cpp'):
+# check if amalgamation exists
+if os.path.isfile(os.path.join('..', '..', 'scripts', 'amalgamation.py')):
     prev_wd = os.getcwd()
-    os.chdir('../..')
-    subprocess.Popen('python scripts/amalgamation.py'.split(' ')).wait()
+    target_header = os.path.join(prev_wd, 'duckdb.hpp')
+    target_source = os.path.join(prev_wd, 'duckdb.cpp')
+    os.chdir(os.path.join('..', '..'))
+    sys.path.append('scripts')
+    import amalgamation
+    amalgamation.generate_amalgamation(target_source, target_header)
     os.chdir(prev_wd)
-    shutil.copyfile('../../src/amalgamation/duckdb.hpp', 'duckdb.hpp')
-    shutil.copyfile('../../src/amalgamation/duckdb.cpp', 'duckdb.cpp')
 
 
 toolchain_args = ['-std=c++11']
@@ -66,7 +69,7 @@ def __str__(self):
     url="https://www.duckdb.org",
     long_description = '',
     install_requires=[ # these versions are still available for Python 2, newer ones aren't
-         'numpy>=1.14', 
+         'numpy>=1.14',
          'pandas>=0.23',
     ],
     packages=['duckdb_query_graph'],
diff --git a/tools/rpkg/configure b/tools/rpkg/configure
index 6d353abed215..edba220523c6 100755
--- a/tools/rpkg/configure
+++ b/tools/rpkg/configure
@@ -1,12 +1,7 @@
 #!/bin/sh
 
-if [ ! -f "src/duckdb.cpp" ]; then
-	if [ ! -f "../../scripts/amalgamation.py" ]; then
-		echo "Could find neither duckdb.cpp nor the build script"
-		exit 1
-	fi
-	(cd ../.. && python scripts/amalgamation.py)
-	cat ../../src/amalgamation/duckdb.cpp | sed 's|#include "duckdb.hpp"|#include "duckdb.h"|' > src/duckdb.cpp
-	cp ../../src/amalgamation/duckdb.hpp src/duckdb.h
-
+if [ ! -f "../../scripts/amalgamation.py" ]; then
+	echo "Could find neither the amalgamation build script"
+	exit 1
 fi
+(cd ../.. && python scripts/amalgamation.py --source=tools/rpkg/src/duckdb.cpp --header=tools/rpkg/src/duckdb.h)
