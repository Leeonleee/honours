{
  "repo": "duckdb/duckdb",
  "pull_number": 2567,
  "instance_id": "duckdb__duckdb-2567",
  "issue_numbers": [
    "2538"
  ],
  "base_commit": "0385e028ab8b631a35ccf24ef9a02ab0d88399e3",
  "patch": "diff --git a/src/execution/operator/persistent/buffered_csv_reader.cpp b/src/execution/operator/persistent/buffered_csv_reader.cpp\nindex f39ec75cba14..7a95b06498e2 100644\n--- a/src/execution/operator/persistent/buffered_csv_reader.cpp\n+++ b/src/execution/operator/persistent/buffered_csv_reader.cpp\n@@ -140,6 +140,9 @@ void BufferedCSVReader::Initialize(const vector<LogicalType> &requested_types) {\n \tPrepareComplexParser();\n \tif (options.auto_detect) {\n \t\tsql_types = SniffCSV(requested_types);\n+\t\tif (sql_types.empty()) {\n+\t\t\tthrow Exception(\"Failed to detect column types from CSV: is the file a valid CSV file?\");\n+\t\t}\n \t\tif (cached_chunks.empty()) {\n \t\t\tJumpToBeginning(options.skip_rows, options.header);\n \t\t}\n@@ -624,6 +627,10 @@ void BufferedCSVReader::DetectCandidateTypes(const vector<LogicalType> &type_can\n \t\theader_row.Initialize(sql_types);\n \t\tparse_chunk.Copy(header_row);\n \n+\t\tif (header_row.size() == 0) {\n+\t\t\tcontinue;\n+\t\t}\n+\n \t\t// init parse chunk and read csv with info candidate\n \t\tInitParseChunk(sql_types.size());\n \t\tParseCSV(ParserMode::SNIFFING_DATATYPES);\n",
  "test_patch": "diff --git a/test/sql/copy/csv/broken/test.csv.zst b/test/sql/copy/csv/broken/test.csv.zst\nnew file mode 100644\nindex 000000000000..8fa3ffb7b63d\nBinary files /dev/null and b/test/sql/copy/csv/broken/test.csv.zst differ\ndiff --git a/test/sql/copy/csv/zstd_crash.test b/test/sql/copy/csv/zstd_crash.test\nnew file mode 100644\nindex 000000000000..96e8439a0e17\n--- /dev/null\n+++ b/test/sql/copy/csv/zstd_crash.test\n@@ -0,0 +1,6 @@\n+# name: test/sql/copy/csv/zstd_crash.test\n+# description: Test that reading a ZSTD file with auto-detect does not crash\n+# group: [csv]\n+\n+statement error\n+CREATE TABLE test_zst AS SELECT * FROM read_csv('test/sql/copy/csv/broken/test.csv.zst', AUTO_DETECT=TRUE);\n",
  "problem_statement": "read CSV compressed with zstd\nHello,\r\n\r\nout of curiosity I have tried to read zstd compressed CSV file:\r\n\r\n```\r\nD CREATE TABLE test_zst AS SELECT * FROM read_csv('test.csv.zst', AUTO_DETECT=TRUE);\r\n#\r\nfish: \u201c./duckdb\u201d terminated by signal SIGSEGV (Address boundary error)\r\n```\r\n\r\nAfter checking read_csv.cpp I did:\r\n\r\n```\r\nCREATE TABLE test_zst AS SELECT * FROM read_csv('test.csv.zst', AUTO_DETECT=TRUE, compression='zstd');\r\nError: Binder Error: read_csv currently only supports 'gzip' compression.\r\n```\r\nTwo enhancement suggestions:\r\n1. checking if the provided input looks like CSV/gzipped CSV to prevent such crashes\r\n2. adding zstd support for reading CSV files (not a high priority I guess)\r\n\r\nHope it helps\r\n\r\nDK \n",
  "hints_text": "Could you provide the zstd compressed file? In general a segfault should not happen, no matter what input you provide, so that is a bug for sure.\r\n\r\nAs for zstd support, we could consider adding this as we do ship zstd along with the Parquet reader. \nSorry for the delay. Sent by email with 744 bytes test.csv.zst as an attachment.",
  "created_at": "2021-11-09T20:47:54Z"
}