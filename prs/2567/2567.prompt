You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
read CSV compressed with zstd
Hello,

out of curiosity I have tried to read zstd compressed CSV file:

```
D CREATE TABLE test_zst AS SELECT * FROM read_csv('test.csv.zst', AUTO_DETECT=TRUE);
#
fish: “./duckdb” terminated by signal SIGSEGV (Address boundary error)
```

After checking read_csv.cpp I did:

```
CREATE TABLE test_zst AS SELECT * FROM read_csv('test.csv.zst', AUTO_DETECT=TRUE, compression='zstd');
Error: Binder Error: read_csv currently only supports 'gzip' compression.
```
Two enhancement suggestions:
1. checking if the provided input looks like CSV/gzipped CSV to prevent such crashes
2. adding zstd support for reading CSV files (not a high priority I guess)

Hope it helps

DK 

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16: </p>
17: 
18: ## DuckDB
19: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/docs/why_duckdb.html).
20: 
21: ## Installation
22: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
23: 
24: ## Data Import
25: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
26: 
27: ```sql
28: SELECT * FROM 'myfile.csv';
29: SELECT * FROM 'myfile.parquet';
30: ```
31: 
32: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
33: 
34: ## SQL Reference
35: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
36: 
37: ## Development
38: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
39: 
40: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
41: 
42: 
[end of README.md]
[start of src/execution/operator/persistent/buffered_csv_reader.cpp]
1: #include "duckdb/execution/operator/persistent/buffered_csv_reader.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/file_system.hpp"
5: #include "duckdb/common/string_util.hpp"
6: #include "duckdb/common/to_string.hpp"
7: #include "duckdb/common/types/cast_helpers.hpp"
8: #include "duckdb/common/vector_operations/unary_executor.hpp"
9: #include "duckdb/common/vector_operations/vector_operations.hpp"
10: #include "duckdb/function/scalar/strftime.hpp"
11: #include "duckdb/main/database.hpp"
12: #include "duckdb/parser/column_definition.hpp"
13: #include "duckdb/storage/data_table.hpp"
14: #include "utf8proc_wrapper.hpp"
15: #include "utf8proc.hpp"
16: #include "duckdb/parser/keyword_helper.hpp"
17: 
18: #include <algorithm>
19: #include <cctype>
20: #include <cstring>
21: #include <fstream>
22: 
23: namespace duckdb {
24: 
25: static string GetLineNumberStr(idx_t linenr, bool linenr_estimated) {
26: 	string estimated = (linenr_estimated ? string(" (estimated)") : string(""));
27: 	return to_string(linenr + 1) + estimated;
28: }
29: 
30: static bool StartsWithNumericDate(string &separator, const string_t &value) {
31: 	auto begin = value.GetDataUnsafe();
32: 	auto end = begin + value.GetSize();
33: 
34: 	//	StrpTimeFormat::Parse will skip whitespace, so we can too
35: 	auto field1 = std::find_if_not(begin, end, StringUtil::CharacterIsSpace);
36: 	if (field1 == end) {
37: 		return false;
38: 	}
39: 
40: 	//	first numeric field must start immediately
41: 	if (!StringUtil::CharacterIsDigit(*field1)) {
42: 		return false;
43: 	}
44: 	auto literal1 = std::find_if_not(field1, end, StringUtil::CharacterIsDigit);
45: 	if (literal1 == end) {
46: 		return false;
47: 	}
48: 
49: 	//	second numeric field must exist
50: 	auto field2 = std::find_if(literal1, end, StringUtil::CharacterIsDigit);
51: 	if (field2 == end) {
52: 		return false;
53: 	}
54: 	auto literal2 = std::find_if_not(field2, end, StringUtil::CharacterIsDigit);
55: 	if (literal2 == end) {
56: 		return false;
57: 	}
58: 
59: 	//	third numeric field must exist
60: 	auto field3 = std::find_if(literal2, end, StringUtil::CharacterIsDigit);
61: 	if (field3 == end) {
62: 		return false;
63: 	}
64: 
65: 	//	second literal must match first
66: 	if (((field3 - literal2) != (field2 - literal1)) || strncmp(literal1, literal2, (field2 - literal1)) != 0) {
67: 		return false;
68: 	}
69: 
70: 	//	copy the literal as the separator, escaping percent signs
71: 	separator.clear();
72: 	while (literal1 < field2) {
73: 		const auto literal_char = *literal1++;
74: 		if (literal_char == '%') {
75: 			separator.push_back(literal_char);
76: 		}
77: 		separator.push_back(literal_char);
78: 	}
79: 
80: 	return true;
81: }
82: 
83: string GenerateDateFormat(const string &separator, const char *format_template) {
84: 	string format_specifier = format_template;
85: 
86: 	//	replace all dashes with the separator
87: 	for (auto pos = std::find(format_specifier.begin(), format_specifier.end(), '-'); pos != format_specifier.end();
88: 	     pos = std::find(pos + separator.size(), format_specifier.end(), '-')) {
89: 		format_specifier.replace(pos, pos + 1, separator);
90: 	}
91: 
92: 	return format_specifier;
93: }
94: 
95: TextSearchShiftArray::TextSearchShiftArray() {
96: }
97: 
98: TextSearchShiftArray::TextSearchShiftArray(string search_term) : length(search_term.size()) {
99: 	if (length > 255) {
100: 		throw Exception("Size of delimiter/quote/escape in CSV reader is limited to 255 bytes");
101: 	}
102: 	// initialize the shifts array
103: 	shifts = unique_ptr<uint8_t[]>(new uint8_t[length * 255]);
104: 	memset(shifts.get(), 0, length * 255 * sizeof(uint8_t));
105: 	// iterate over each of the characters in the array
106: 	for (idx_t main_idx = 0; main_idx < length; main_idx++) {
107: 		uint8_t current_char = (uint8_t)search_term[main_idx];
108: 		// now move over all the remaining positions
109: 		for (idx_t i = main_idx; i < length; i++) {
110: 			bool is_match = true;
111: 			// check if the prefix matches at this position
112: 			// if it does, we move to this position after encountering the current character
113: 			for (idx_t j = 0; j < main_idx; j++) {
114: 				if (search_term[i - main_idx + j] != search_term[j]) {
115: 					is_match = false;
116: 				}
117: 			}
118: 			if (!is_match) {
119: 				continue;
120: 			}
121: 			shifts[i * 255 + current_char] = main_idx + 1;
122: 		}
123: 	}
124: }
125: 
126: BufferedCSVReader::BufferedCSVReader(FileSystem &fs_p, FileOpener *opener_p, BufferedCSVReaderOptions options_p,
127:                                      const vector<LogicalType> &requested_types)
128:     : fs(fs_p), opener(opener_p), options(move(options_p)), buffer_size(0), position(0), start(0) {
129: 	file_handle = OpenCSV(options);
130: 	Initialize(requested_types);
131: }
132: 
133: BufferedCSVReader::BufferedCSVReader(ClientContext &context, BufferedCSVReaderOptions options_p,
134:                                      const vector<LogicalType> &requested_types)
135:     : BufferedCSVReader(FileSystem::GetFileSystem(context), FileSystem::GetFileOpener(context), move(options_p),
136:                         requested_types) {
137: }
138: 
139: void BufferedCSVReader::Initialize(const vector<LogicalType> &requested_types) {
140: 	PrepareComplexParser();
141: 	if (options.auto_detect) {
142: 		sql_types = SniffCSV(requested_types);
143: 		if (cached_chunks.empty()) {
144: 			JumpToBeginning(options.skip_rows, options.header);
145: 		}
146: 	} else {
147: 		sql_types = requested_types;
148: 		ResetBuffer();
149: 		SkipRowsAndReadHeader(options.skip_rows, options.header);
150: 	}
151: 	InitParseChunk(sql_types.size());
152: }
153: 
154: void BufferedCSVReader::PrepareComplexParser() {
155: 	delimiter_search = TextSearchShiftArray(options.delimiter);
156: 	escape_search = TextSearchShiftArray(options.escape);
157: 	quote_search = TextSearchShiftArray(options.quote);
158: }
159: 
160: unique_ptr<FileHandle> BufferedCSVReader::OpenCSV(const BufferedCSVReaderOptions &options) {
161: 	this->compression = FileCompressionType::UNCOMPRESSED;
162: 	if (options.compression == "infer" || options.compression == "auto") {
163: 		this->compression = FileCompressionType::AUTO_DETECT;
164: 	} else if (options.compression == "gzip") {
165: 		this->compression = FileCompressionType::GZIP;
166: 	}
167: 
168: 	auto result = fs.OpenFile(options.file_path.c_str(), FileFlags::FILE_FLAGS_READ, FileLockType::NO_LOCK,
169: 	                          this->compression, this->opener);
170: 	plain_file_source = result->OnDiskFile() && result->CanSeek();
171: 	file_size = result->GetFileSize();
172: 	return result;
173: }
174: 
175: // Helper function to generate column names
176: static string GenerateColumnName(const idx_t total_cols, const idx_t col_number, const string &prefix = "column") {
177: 	int max_digits = NumericHelper::UnsignedLength(total_cols - 1);
178: 	int digits = NumericHelper::UnsignedLength(col_number);
179: 	string leading_zeros = string(max_digits - digits, '0');
180: 	string value = to_string(col_number);
181: 	return string(prefix + leading_zeros + value);
182: }
183: 
184: // Helper function for UTF-8 aware space trimming
185: static string TrimWhitespace(const string &col_name) {
186: 	utf8proc_int32_t codepoint;
187: 	auto str = reinterpret_cast<const utf8proc_uint8_t *>(col_name.c_str());
188: 	idx_t size = col_name.size();
189: 	// Find the first character that is not left trimmed
190: 	idx_t begin = 0;
191: 	while (begin < size) {
192: 		auto bytes = utf8proc_iterate(str + begin, size - begin, &codepoint);
193: 		D_ASSERT(bytes > 0);
194: 		if (utf8proc_category(codepoint) != UTF8PROC_CATEGORY_ZS) {
195: 			break;
196: 		}
197: 		begin += bytes;
198: 	}
199: 
200: 	// Find the last character that is not right trimmed
201: 	idx_t end;
202: 	end = begin;
203: 	for (auto next = begin; next < col_name.size();) {
204: 		auto bytes = utf8proc_iterate(str + next, size - next, &codepoint);
205: 		D_ASSERT(bytes > 0);
206: 		next += bytes;
207: 		if (utf8proc_category(codepoint) != UTF8PROC_CATEGORY_ZS) {
208: 			end = next;
209: 		}
210: 	}
211: 
212: 	// return the trimmed string
213: 	return col_name.substr(begin, end - begin);
214: }
215: 
216: static string NormalizeColumnName(const string &col_name) {
217: 	// normalize UTF8 characters to NFKD
218: 	auto nfkd = utf8proc_NFKD((const utf8proc_uint8_t *)col_name.c_str(), col_name.size());
219: 	const string col_name_nfkd = string((const char *)nfkd, strlen((const char *)nfkd));
220: 	free(nfkd);
221: 
222: 	// only keep ASCII characters 0-9 a-z A-Z and replace spaces with regular whitespace
223: 	string col_name_ascii = "";
224: 	for (idx_t i = 0; i < col_name_nfkd.size(); i++) {
225: 		if (col_name_nfkd[i] == '_' || (col_name_nfkd[i] >= '0' && col_name_nfkd[i] <= '9') ||
226: 		    (col_name_nfkd[i] >= 'A' && col_name_nfkd[i] <= 'Z') ||
227: 		    (col_name_nfkd[i] >= 'a' && col_name_nfkd[i] <= 'z')) {
228: 			col_name_ascii += col_name_nfkd[i];
229: 		} else if (StringUtil::CharacterIsSpace(col_name_nfkd[i])) {
230: 			col_name_ascii += " ";
231: 		}
232: 	}
233: 
234: 	// trim whitespace and replace remaining whitespace by _
235: 	string col_name_trimmed = TrimWhitespace(col_name_ascii);
236: 	string col_name_cleaned = "";
237: 	bool in_whitespace = false;
238: 	for (idx_t i = 0; i < col_name_trimmed.size(); i++) {
239: 		if (col_name_trimmed[i] == ' ') {
240: 			if (!in_whitespace) {
241: 				col_name_cleaned += "_";
242: 				in_whitespace = true;
243: 			}
244: 		} else {
245: 			col_name_cleaned += col_name_trimmed[i];
246: 			in_whitespace = false;
247: 		}
248: 	}
249: 
250: 	// don't leave string empty; if not empty, make lowercase
251: 	if (col_name_cleaned.empty()) {
252: 		col_name_cleaned = "_";
253: 	} else {
254: 		col_name_cleaned = StringUtil::Lower(col_name_cleaned);
255: 	}
256: 
257: 	// prepend _ if name starts with a digit or is a reserved keyword
258: 	if (KeywordHelper::IsKeyword(col_name_cleaned) || (col_name_cleaned[0] >= '0' && col_name_cleaned[0] <= '9')) {
259: 		col_name_cleaned = "_" + col_name_cleaned;
260: 	}
261: 	return col_name_cleaned;
262: }
263: 
264: void BufferedCSVReader::ResetBuffer() {
265: 	buffer.reset();
266: 	buffer_size = 0;
267: 	position = 0;
268: 	start = 0;
269: 	cached_buffers.clear();
270: }
271: 
272: void BufferedCSVReader::ResetStream() {
273: 	if (!file_handle->CanSeek()) {
274: 		// seeking to the beginning appears to not be supported in all compiler/os-scenarios,
275: 		// so we have to create a new stream source here for now
276: 		file_handle->Reset();
277: 	} else {
278: 		file_handle->Seek(0);
279: 	}
280: 	linenr = 0;
281: 	linenr_estimated = false;
282: 	bytes_per_line_avg = 0;
283: 	sample_chunk_idx = 0;
284: 	jumping_samples = false;
285: }
286: 
287: void BufferedCSVReader::InitParseChunk(idx_t num_cols) {
288: 	// adapt not null info
289: 	if (options.force_not_null.size() != num_cols) {
290: 		options.force_not_null.resize(num_cols, false);
291: 	}
292: 	if (num_cols == parse_chunk.ColumnCount()) {
293: 		parse_chunk.Reset();
294: 	} else {
295: 		parse_chunk.Destroy();
296: 
297: 		// initialize the parse_chunk with a set of VARCHAR types
298: 		vector<LogicalType> varchar_types(num_cols, LogicalType::VARCHAR);
299: 		parse_chunk.Initialize(varchar_types);
300: 	}
301: }
302: 
303: void BufferedCSVReader::JumpToBeginning(idx_t skip_rows = 0, bool skip_header = false) {
304: 	ResetBuffer();
305: 	ResetStream();
306: 	SkipRowsAndReadHeader(skip_rows, skip_header);
307: 	sample_chunk_idx = 0;
308: 	bytes_in_chunk = 0;
309: 	end_of_file_reached = false;
310: 	bom_checked = false;
311: }
312: 
313: void BufferedCSVReader::SkipRowsAndReadHeader(idx_t skip_rows, bool skip_header) {
314: 	for (idx_t i = 0; i < skip_rows; i++) {
315: 		// ignore skip rows
316: 		string read_line = file_handle->ReadLine();
317: 		linenr++;
318: 	}
319: 
320: 	if (skip_header) {
321: 		// ignore the first line as a header line
322: 		InitParseChunk(sql_types.size());
323: 		ParseCSV(ParserMode::PARSING_HEADER);
324: 	}
325: }
326: 
327: bool BufferedCSVReader::JumpToNextSample() {
328: 	// get bytes contained in the previously read chunk
329: 	idx_t remaining_bytes_in_buffer = buffer_size - start;
330: 	bytes_in_chunk -= remaining_bytes_in_buffer;
331: 	if (remaining_bytes_in_buffer == 0) {
332: 		return false;
333: 	}
334: 
335: 	// assess if it makes sense to jump, based on size of the first chunk relative to size of the entire file
336: 	if (sample_chunk_idx == 0) {
337: 		idx_t bytes_first_chunk = bytes_in_chunk;
338: 		double chunks_fit = (file_size / (double)bytes_first_chunk);
339: 		jumping_samples = chunks_fit >= options.sample_chunks;
340: 
341: 		// jump back to the beginning
342: 		JumpToBeginning(options.skip_rows, options.header);
343: 		sample_chunk_idx++;
344: 		return true;
345: 	}
346: 
347: 	if (end_of_file_reached || sample_chunk_idx >= options.sample_chunks) {
348: 		return false;
349: 	}
350: 
351: 	// if we deal with any other sources than plaintext files, jumping_samples can be tricky. In that case
352: 	// we just read x continuous chunks from the stream TODO: make jumps possible for zipfiles.
353: 	if (!plain_file_source || !jumping_samples) {
354: 		sample_chunk_idx++;
355: 		return true;
356: 	}
357: 
358: 	// update average bytes per line
359: 	double bytes_per_line = bytes_in_chunk / (double)options.sample_chunk_size;
360: 	bytes_per_line_avg = ((bytes_per_line_avg * (sample_chunk_idx)) + bytes_per_line) / (sample_chunk_idx + 1);
361: 
362: 	// if none of the previous conditions were met, we can jump
363: 	idx_t partition_size = (idx_t)round(file_size / (double)options.sample_chunks);
364: 
365: 	// calculate offset to end of the current partition
366: 	int64_t offset = partition_size - bytes_in_chunk - remaining_bytes_in_buffer;
367: 	auto current_pos = file_handle->SeekPosition();
368: 
369: 	if (current_pos + offset < file_size) {
370: 		// set position in stream and clear failure bits
371: 		file_handle->Seek(current_pos + offset);
372: 
373: 		// estimate linenr
374: 		linenr += (idx_t)round((offset + remaining_bytes_in_buffer) / bytes_per_line_avg);
375: 		linenr_estimated = true;
376: 	} else {
377: 		// seek backwards from the end in last chunk and hope to catch the end of the file
378: 		// TODO: actually it would be good to make sure that the end of file is being reached, because
379: 		// messy end-lines are quite common. For this case, however, we first need a skip_end detection anyways.
380: 		file_handle->Seek(file_size - bytes_in_chunk);
381: 
382: 		// estimate linenr
383: 		linenr = (idx_t)round((file_size - bytes_in_chunk) / bytes_per_line_avg);
384: 		linenr_estimated = true;
385: 	}
386: 
387: 	// reset buffers and parse chunk
388: 	ResetBuffer();
389: 
390: 	// seek beginning of next line
391: 	// FIXME: if this jump ends up in a quoted linebreak, we will have a problem
392: 	string read_line = file_handle->ReadLine();
393: 	linenr++;
394: 
395: 	sample_chunk_idx++;
396: 
397: 	return true;
398: }
399: 
400: void BufferedCSVReader::SetDateFormat(const string &format_specifier, const LogicalTypeId &sql_type) {
401: 	options.has_format[sql_type] = true;
402: 	auto &date_format = options.date_format[sql_type];
403: 	date_format.format_specifier = format_specifier;
404: 	StrTimeFormat::ParseFormatSpecifier(date_format.format_specifier, date_format);
405: }
406: 
407: bool BufferedCSVReader::TryCastValue(const Value &value, const LogicalType &sql_type) {
408: 	if (options.has_format[LogicalTypeId::DATE] && sql_type.id() == LogicalTypeId::DATE) {
409: 		date_t result;
410: 		string error_message;
411: 		return options.date_format[LogicalTypeId::DATE].TryParseDate(string_t(value.str_value), result, error_message);
412: 	} else if (options.has_format[LogicalTypeId::TIMESTAMP] && sql_type.id() == LogicalTypeId::TIMESTAMP) {
413: 		timestamp_t result;
414: 		string error_message;
415: 		return options.date_format[LogicalTypeId::TIMESTAMP].TryParseTimestamp(string_t(value.str_value), result,
416: 		                                                                       error_message);
417: 	} else {
418: 		Value new_value;
419: 		string error_message;
420: 		return value.TryCastAs(sql_type, new_value, &error_message, true);
421: 	}
422: }
423: 
424: struct TryCastDateOperator {
425: 	static bool Operation(BufferedCSVReaderOptions &options, string_t input, date_t &result, string &error_message) {
426: 		return options.date_format[LogicalTypeId::DATE].TryParseDate(input, result, error_message);
427: 	}
428: };
429: 
430: struct TryCastTimestampOperator {
431: 	static bool Operation(BufferedCSVReaderOptions &options, string_t input, timestamp_t &result,
432: 	                      string &error_message) {
433: 		return options.date_format[LogicalTypeId::TIMESTAMP].TryParseTimestamp(input, result, error_message);
434: 	}
435: };
436: 
437: template <class OP, class T>
438: static bool TemplatedTryCastDateVector(BufferedCSVReaderOptions &options, Vector &input_vector, Vector &result_vector,
439:                                        idx_t count, string &error_message) {
440: 	D_ASSERT(input_vector.GetType().id() == LogicalTypeId::VARCHAR);
441: 	bool all_converted = true;
442: 	UnaryExecutor::Execute<string_t, T>(input_vector, result_vector, count, [&](string_t input) {
443: 		T result;
444: 		if (!OP::Operation(options, input, result, error_message)) {
445: 			all_converted = false;
446: 		}
447: 		return result;
448: 	});
449: 	return all_converted;
450: }
451: 
452: bool TryCastDateVector(BufferedCSVReaderOptions &options, Vector &input_vector, Vector &result_vector, idx_t count,
453:                        string &error_message) {
454: 	return TemplatedTryCastDateVector<TryCastDateOperator, date_t>(options, input_vector, result_vector, count,
455: 	                                                               error_message);
456: }
457: 
458: bool TryCastTimestampVector(BufferedCSVReaderOptions &options, Vector &input_vector, Vector &result_vector, idx_t count,
459:                             string &error_message) {
460: 	return TemplatedTryCastDateVector<TryCastTimestampOperator, timestamp_t>(options, input_vector, result_vector,
461: 	                                                                         count, error_message);
462: }
463: 
464: bool BufferedCSVReader::TryCastVector(Vector &parse_chunk_col, idx_t size, const LogicalType &sql_type) {
465: 	// try vector-cast from string to sql_type
466: 	Vector dummy_result(sql_type);
467: 	if (options.has_format[LogicalTypeId::DATE] && sql_type == LogicalTypeId::DATE) {
468: 		// use the date format to cast the chunk
469: 		string error_message;
470: 		return TryCastDateVector(options, parse_chunk_col, dummy_result, size, error_message);
471: 	} else if (options.has_format[LogicalTypeId::TIMESTAMP] && sql_type == LogicalTypeId::TIMESTAMP) {
472: 		// use the timestamp format to cast the chunk
473: 		string error_message;
474: 		return TryCastTimestampVector(options, parse_chunk_col, dummy_result, size, error_message);
475: 	} else {
476: 		// target type is not varchar: perform a cast
477: 		string error_message;
478: 		return VectorOperations::TryCast(parse_chunk_col, dummy_result, size, &error_message, true);
479: 	}
480: }
481: 
482: enum class QuoteRule : uint8_t { QUOTES_RFC = 0, QUOTES_OTHER = 1, NO_QUOTES = 2 };
483: 
484: void BufferedCSVReader::DetectDialect(const vector<LogicalType> &requested_types,
485:                                       BufferedCSVReaderOptions &original_options,
486:                                       vector<BufferedCSVReaderOptions> &info_candidates, idx_t &best_num_cols) {
487: 	// set up the candidates we consider for delimiter and quote rules based on user input
488: 	vector<string> delim_candidates;
489: 	vector<QuoteRule> quoterule_candidates;
490: 	vector<vector<string>> quote_candidates_map;
491: 	vector<vector<string>> escape_candidates_map = {{""}, {"\\"}, {""}};
492: 
493: 	if (options.has_delimiter) {
494: 		// user provided a delimiter: use that delimiter
495: 		delim_candidates = {options.delimiter};
496: 	} else {
497: 		// no delimiter provided: try standard/common delimiters
498: 		delim_candidates = {",", "|", ";", "\t"};
499: 	}
500: 	if (options.has_quote) {
501: 		// user provided quote: use that quote rule
502: 		quote_candidates_map = {{options.quote}, {options.quote}, {options.quote}};
503: 	} else {
504: 		// no quote rule provided: use standard/common quotes
505: 		quote_candidates_map = {{"\""}, {"\"", "'"}, {""}};
506: 	}
507: 	if (options.has_escape) {
508: 		// user provided escape: use that escape rule
509: 		if (options.escape.empty()) {
510: 			quoterule_candidates = {QuoteRule::QUOTES_RFC};
511: 		} else {
512: 			quoterule_candidates = {QuoteRule::QUOTES_OTHER};
513: 		}
514: 		escape_candidates_map[static_cast<uint8_t>(quoterule_candidates[0])] = {options.escape};
515: 	} else {
516: 		// no escape provided: try standard/common escapes
517: 		quoterule_candidates = {QuoteRule::QUOTES_RFC, QuoteRule::QUOTES_OTHER, QuoteRule::NO_QUOTES};
518: 	}
519: 
520: 	idx_t best_consistent_rows = 0;
521: 	for (auto quoterule : quoterule_candidates) {
522: 		const auto &quote_candidates = quote_candidates_map[static_cast<uint8_t>(quoterule)];
523: 		for (const auto &quote : quote_candidates) {
524: 			for (const auto &delim : delim_candidates) {
525: 				const auto &escape_candidates = escape_candidates_map[static_cast<uint8_t>(quoterule)];
526: 				for (const auto &escape : escape_candidates) {
527: 					BufferedCSVReaderOptions sniff_info = original_options;
528: 					sniff_info.delimiter = delim;
529: 					sniff_info.quote = quote;
530: 					sniff_info.escape = escape;
531: 
532: 					options = sniff_info;
533: 					PrepareComplexParser();
534: 
535: 					JumpToBeginning(original_options.skip_rows);
536: 					sniffed_column_counts.clear();
537: 					if (!TryParseCSV(ParserMode::SNIFFING_DIALECT)) {
538: 						continue;
539: 					}
540: 
541: 					idx_t start_row = original_options.skip_rows;
542: 					idx_t consistent_rows = 0;
543: 					idx_t num_cols = 0;
544: 
545: 					for (idx_t row = 0; row < sniffed_column_counts.size(); row++) {
546: 						if (sniffed_column_counts[row] == num_cols) {
547: 							consistent_rows++;
548: 						} else {
549: 							num_cols = sniffed_column_counts[row];
550: 							start_row = row + original_options.skip_rows;
551: 							consistent_rows = 1;
552: 						}
553: 					}
554: 
555: 					// some logic
556: 					bool more_values = (consistent_rows > best_consistent_rows && num_cols >= best_num_cols);
557: 					bool single_column_before = best_num_cols < 2 && num_cols > best_num_cols;
558: 					bool rows_consistent =
559: 					    start_row + consistent_rows - original_options.skip_rows == sniffed_column_counts.size();
560: 					bool more_than_one_row = (consistent_rows > 1);
561: 					bool more_than_one_column = (num_cols > 1);
562: 					bool start_good = !info_candidates.empty() && (start_row <= info_candidates.front().skip_rows);
563: 
564: 					if (!requested_types.empty() && requested_types.size() != num_cols) {
565: 						continue;
566: 					} else if ((more_values || single_column_before) && rows_consistent) {
567: 						sniff_info.skip_rows = start_row;
568: 						sniff_info.num_cols = num_cols;
569: 						best_consistent_rows = consistent_rows;
570: 						best_num_cols = num_cols;
571: 
572: 						info_candidates.clear();
573: 						info_candidates.push_back(sniff_info);
574: 					} else if (more_than_one_row && more_than_one_column && start_good && rows_consistent) {
575: 						bool same_quote_is_candidate = false;
576: 						for (auto &info_candidate : info_candidates) {
577: 							if (quote.compare(info_candidate.quote) == 0) {
578: 								same_quote_is_candidate = true;
579: 							}
580: 						}
581: 						if (!same_quote_is_candidate) {
582: 							sniff_info.skip_rows = start_row;
583: 							sniff_info.num_cols = num_cols;
584: 							info_candidates.push_back(sniff_info);
585: 						}
586: 					}
587: 				}
588: 			}
589: 		}
590: 	}
591: }
592: 
593: void BufferedCSVReader::DetectCandidateTypes(const vector<LogicalType> &type_candidates,
594:                                              const map<LogicalTypeId, vector<const char *>> &format_template_candidates,
595:                                              const vector<BufferedCSVReaderOptions> &info_candidates,
596:                                              BufferedCSVReaderOptions &original_options, idx_t best_num_cols,
597:                                              vector<vector<LogicalType>> &best_sql_types_candidates,
598:                                              std::map<LogicalTypeId, vector<string>> &best_format_candidates,
599:                                              DataChunk &best_header_row) {
600: 	BufferedCSVReaderOptions best_options;
601: 	idx_t min_varchar_cols = best_num_cols + 1;
602: 
603: 	// check which info candidate leads to minimum amount of non-varchar columns...
604: 	for (const auto &t : format_template_candidates) {
605: 		best_format_candidates[t.first].clear();
606: 	}
607: 	for (auto &info_candidate : info_candidates) {
608: 		options = info_candidate;
609: 		vector<vector<LogicalType>> info_sql_types_candidates(options.num_cols, type_candidates);
610: 		std::map<LogicalTypeId, bool> has_format_candidates;
611: 		std::map<LogicalTypeId, vector<string>> format_candidates;
612: 		for (const auto &t : format_template_candidates) {
613: 			has_format_candidates[t.first] = false;
614: 			format_candidates[t.first].clear();
615: 		}
616: 
617: 		// set all sql_types to VARCHAR so we can do datatype detection based on VARCHAR values
618: 		sql_types.clear();
619: 		sql_types.assign(options.num_cols, LogicalType::VARCHAR);
620: 
621: 		// jump to beginning and skip potential header
622: 		JumpToBeginning(options.skip_rows, true);
623: 		DataChunk header_row;
624: 		header_row.Initialize(sql_types);
625: 		parse_chunk.Copy(header_row);
626: 
627: 		// init parse chunk and read csv with info candidate
628: 		InitParseChunk(sql_types.size());
629: 		ParseCSV(ParserMode::SNIFFING_DATATYPES);
630: 		for (idx_t row_idx = 0; row_idx <= parse_chunk.size(); row_idx++) {
631: 			bool is_header_row = row_idx == 0;
632: 			idx_t row = row_idx - 1;
633: 			for (idx_t col = 0; col < parse_chunk.ColumnCount(); col++) {
634: 				auto &col_type_candidates = info_sql_types_candidates[col];
635: 				while (col_type_candidates.size() > 1) {
636: 					const auto &sql_type = col_type_candidates.back();
637: 					// try cast from string to sql_type
638: 					Value dummy_val;
639: 					if (is_header_row) {
640: 						dummy_val = header_row.GetValue(col, 0);
641: 					} else {
642: 						dummy_val = parse_chunk.GetValue(col, row);
643: 					}
644: 					// try formatting for date types if the user did not specify one and it starts with numeric values.
645: 					string separator;
646: 					if (has_format_candidates.count(sql_type.id()) && !original_options.has_format[sql_type.id()] &&
647: 					    StartsWithNumericDate(separator, dummy_val.str_value)) {
648: 						// generate date format candidates the first time through
649: 						auto &type_format_candidates = format_candidates[sql_type.id()];
650: 						const auto had_format_candidates = has_format_candidates[sql_type.id()];
651: 						if (!has_format_candidates[sql_type.id()]) {
652: 							has_format_candidates[sql_type.id()] = true;
653: 							// order by preference
654: 							auto entry = format_template_candidates.find(sql_type.id());
655: 							if (entry != format_template_candidates.end()) {
656: 								const auto &format_template_list = entry->second;
657: 								for (const auto &t : format_template_list) {
658: 									const auto format_string = GenerateDateFormat(separator, t);
659: 									// don't parse ISO 8601
660: 									if (format_string.find("%Y-%m-%d") == string::npos) {
661: 										type_format_candidates.emplace_back(format_string);
662: 									}
663: 								}
664: 							}
665: 							//	initialise the first candidate
666: 							options.has_format[sql_type.id()] = true;
667: 							//	all formats are constructed to be valid
668: 							SetDateFormat(type_format_candidates.back(), sql_type.id());
669: 						}
670: 						// check all formats and keep the first one that works
671: 						StrpTimeFormat::ParseResult result;
672: 						auto save_format_candidates = type_format_candidates;
673: 						while (!type_format_candidates.empty()) {
674: 							//	avoid using exceptions for flow control...
675: 							auto &current_format = options.date_format[sql_type.id()];
676: 							if (current_format.Parse(dummy_val.str_value, result)) {
677: 								break;
678: 							}
679: 							//	doesn't work - move to the next one
680: 							type_format_candidates.pop_back();
681: 							options.has_format[sql_type.id()] = (!type_format_candidates.empty());
682: 							if (!type_format_candidates.empty()) {
683: 								SetDateFormat(type_format_candidates.back(), sql_type.id());
684: 							}
685: 						}
686: 						//	if none match, then this is not a value of type sql_type,
687: 						if (type_format_candidates.empty()) {
688: 							//	so restore the candidates that did work.
689: 							//	or throw them out if they were generated by this value.
690: 							if (had_format_candidates) {
691: 								type_format_candidates.swap(save_format_candidates);
692: 								if (!type_format_candidates.empty()) {
693: 									SetDateFormat(type_format_candidates.back(), sql_type.id());
694: 								}
695: 							} else {
696: 								has_format_candidates[sql_type.id()] = false;
697: 							}
698: 						}
699: 					}
700: 					// try cast from string to sql_type
701: 					if (TryCastValue(dummy_val, sql_type)) {
702: 						break;
703: 					} else {
704: 						col_type_candidates.pop_back();
705: 					}
706: 				}
707: 			}
708: 			// reset type detection, because first row could be header,
709: 			// but only do it if csv has more than one line (including header)
710: 			if (parse_chunk.size() > 0 && is_header_row) {
711: 				info_sql_types_candidates = vector<vector<LogicalType>>(options.num_cols, type_candidates);
712: 				for (auto &f : format_candidates) {
713: 					f.second.clear();
714: 				}
715: 				for (auto &h : has_format_candidates) {
716: 					h.second = false;
717: 				}
718: 			}
719: 		}
720: 
721: 		idx_t varchar_cols = 0;
722: 		for (idx_t col = 0; col < parse_chunk.ColumnCount(); col++) {
723: 			auto &col_type_candidates = info_sql_types_candidates[col];
724: 			// check number of varchar columns
725: 			const auto &col_type = col_type_candidates.back();
726: 			if (col_type == LogicalType::VARCHAR) {
727: 				varchar_cols++;
728: 			}
729: 		}
730: 
731: 		// it's good if the dialect creates more non-varchar columns, but only if we sacrifice < 30% of best_num_cols.
732: 		if (varchar_cols < min_varchar_cols && parse_chunk.ColumnCount() > (best_num_cols * 0.7)) {
733: 			// we have a new best_options candidate
734: 			best_options = info_candidate;
735: 			min_varchar_cols = varchar_cols;
736: 			best_sql_types_candidates = info_sql_types_candidates;
737: 			best_format_candidates = format_candidates;
738: 			best_header_row.Destroy();
739: 			auto header_row_types = header_row.GetTypes();
740: 			best_header_row.Initialize(header_row_types);
741: 			header_row.Copy(best_header_row);
742: 		}
743: 	}
744: 
745: 	options = best_options;
746: 	for (const auto &best : best_format_candidates) {
747: 		if (!best.second.empty()) {
748: 			SetDateFormat(best.second.back(), best.first);
749: 		}
750: 	}
751: }
752: 
753: void BufferedCSVReader::DetectHeader(const vector<vector<LogicalType>> &best_sql_types_candidates,
754:                                      const DataChunk &best_header_row) {
755: 	// information for header detection
756: 	bool first_row_consistent = true;
757: 	bool first_row_nulls = false;
758: 
759: 	// check if header row is all null and/or consistent with detected column data types
760: 	first_row_nulls = true;
761: 	for (idx_t col = 0; col < best_sql_types_candidates.size(); col++) {
762: 		auto dummy_val = best_header_row.GetValue(col, 0);
763: 		if (!dummy_val.is_null) {
764: 			first_row_nulls = false;
765: 		}
766: 
767: 		// try cast to sql_type of column
768: 		const auto &sql_type = best_sql_types_candidates[col].back();
769: 		if (!TryCastValue(dummy_val, sql_type)) {
770: 			first_row_consistent = false;
771: 		}
772: 	}
773: 
774: 	// update parser info, and read, generate & set col_names based on previous findings
775: 	if (((!first_row_consistent || first_row_nulls) && !options.has_header) || (options.has_header && options.header)) {
776: 		options.header = true;
777: 		unordered_map<string, idx_t> name_collision_count;
778: 		// get header names from CSV
779: 		for (idx_t col = 0; col < options.num_cols; col++) {
780: 			const auto &val = best_header_row.GetValue(col, 0);
781: 			string col_name = val.ToString();
782: 
783: 			// generate name if field is empty
784: 			if (col_name.empty() || val.is_null) {
785: 				col_name = GenerateColumnName(options.num_cols, col);
786: 			}
787: 
788: 			// normalize names or at least trim whitespace
789: 			if (options.normalize_names) {
790: 				col_name = NormalizeColumnName(col_name);
791: 			} else {
792: 				col_name = TrimWhitespace(col_name);
793: 			}
794: 
795: 			// avoid duplicate header names
796: 			const string col_name_raw = col_name;
797: 			while (name_collision_count.find(col_name) != name_collision_count.end()) {
798: 				name_collision_count[col_name] += 1;
799: 				col_name = col_name + "_" + to_string(name_collision_count[col_name]);
800: 			}
801: 
802: 			col_names.push_back(col_name);
803: 			name_collision_count[col_name] = 0;
804: 		}
805: 
806: 	} else {
807: 		options.header = false;
808: 		idx_t total_columns = parse_chunk.ColumnCount();
809: 		for (idx_t col = 0; col < total_columns; col++) {
810: 			string column_name = GenerateColumnName(total_columns, col);
811: 			col_names.push_back(column_name);
812: 		}
813: 	}
814: }
815: 
816: vector<LogicalType> BufferedCSVReader::RefineTypeDetection(const vector<LogicalType> &type_candidates,
817:                                                            const vector<LogicalType> &requested_types,
818:                                                            vector<vector<LogicalType>> &best_sql_types_candidates,
819:                                                            map<LogicalTypeId, vector<string>> &best_format_candidates) {
820: 	// for the type refine we set the SQL types to VARCHAR for all columns
821: 	sql_types.clear();
822: 	sql_types.assign(options.num_cols, LogicalType::VARCHAR);
823: 
824: 	vector<LogicalType> detected_types;
825: 
826: 	// if data types were provided, exit here if number of columns does not match
827: 	if (!requested_types.empty()) {
828: 		if (requested_types.size() != options.num_cols) {
829: 			throw InvalidInputException(
830: 			    "Error while determining column types: found %lld columns but expected %d. (%s)", options.num_cols,
831: 			    requested_types.size(), options.toString());
832: 		} else {
833: 			detected_types = requested_types;
834: 		}
835: 	} else if (options.all_varchar) {
836: 		// return all types varchar
837: 		detected_types = sql_types;
838: 	} else {
839: 		// jump through the rest of the file and continue to refine the sql type guess
840: 		while (JumpToNextSample()) {
841: 			InitParseChunk(sql_types.size());
842: 			// if jump ends up a bad line, we just skip this chunk
843: 			if (!TryParseCSV(ParserMode::SNIFFING_DATATYPES)) {
844: 				continue;
845: 			}
846: 			for (idx_t col = 0; col < parse_chunk.ColumnCount(); col++) {
847: 				vector<LogicalType> &col_type_candidates = best_sql_types_candidates[col];
848: 				while (col_type_candidates.size() > 1) {
849: 					const auto &sql_type = col_type_candidates.back();
850: 					//	narrow down the date formats
851: 					if (best_format_candidates.count(sql_type.id())) {
852: 						auto &best_type_format_candidates = best_format_candidates[sql_type.id()];
853: 						auto save_format_candidates = best_type_format_candidates;
854: 						while (!best_type_format_candidates.empty()) {
855: 							if (TryCastVector(parse_chunk.data[col], parse_chunk.size(), sql_type)) {
856: 								break;
857: 							}
858: 							//	doesn't work - move to the next one
859: 							best_type_format_candidates.pop_back();
860: 							options.has_format[sql_type.id()] = (!best_type_format_candidates.empty());
861: 							if (!best_type_format_candidates.empty()) {
862: 								SetDateFormat(best_type_format_candidates.back(), sql_type.id());
863: 							}
864: 						}
865: 						//	if none match, then this is not a column of type sql_type,
866: 						if (best_type_format_candidates.empty()) {
867: 							//	so restore the candidates that did work.
868: 							best_type_format_candidates.swap(save_format_candidates);
869: 							if (!best_type_format_candidates.empty()) {
870: 								SetDateFormat(best_type_format_candidates.back(), sql_type.id());
871: 							}
872: 						}
873: 					}
874: 
875: 					if (TryCastVector(parse_chunk.data[col], parse_chunk.size(), sql_type)) {
876: 						break;
877: 					} else {
878: 						col_type_candidates.pop_back();
879: 					}
880: 				}
881: 			}
882: 
883: 			if (!jumping_samples) {
884: 				if ((sample_chunk_idx)*options.sample_chunk_size <= options.buffer_size) {
885: 					// cache parse chunk
886: 					// create a new chunk and fill it with the remainder
887: 					auto chunk = make_unique<DataChunk>();
888: 					auto parse_chunk_types = parse_chunk.GetTypes();
889: 					chunk->Move(parse_chunk);
890: 					cached_chunks.push(move(chunk));
891: 				} else {
892: 					while (!cached_chunks.empty()) {
893: 						cached_chunks.pop();
894: 					}
895: 				}
896: 			}
897: 		}
898: 
899: 		// set sql types
900: 		for (auto &best_sql_types_candidate : best_sql_types_candidates) {
901: 			LogicalType d_type = best_sql_types_candidate.back();
902: 			if (best_sql_types_candidate.size() == type_candidates.size()) {
903: 				d_type = LogicalType::VARCHAR;
904: 			}
905: 			detected_types.push_back(d_type);
906: 		}
907: 	}
908: 
909: 	return detected_types;
910: }
911: 
912: vector<LogicalType> BufferedCSVReader::SniffCSV(const vector<LogicalType> &requested_types) {
913: 	for (auto &type : requested_types) {
914: 		// auto detect for blobs not supported: there may be invalid UTF-8 in the file
915: 		if (type.id() == LogicalTypeId::BLOB) {
916: 			return requested_types;
917: 		}
918: 	}
919: 
920: 	// #######
921: 	// ### dialect detection
922: 	// #######
923: 	BufferedCSVReaderOptions original_options = options;
924: 	vector<BufferedCSVReaderOptions> info_candidates;
925: 	idx_t best_num_cols = 0;
926: 
927: 	DetectDialect(requested_types, original_options, info_candidates, best_num_cols);
928: 
929: 	// if no dialect candidate was found, then file was most likely empty and we throw an exception
930: 	if (info_candidates.empty()) {
931: 		throw InvalidInputException(
932: 		    "Error in file \"%s\": CSV options could not be auto-detected. Consider setting parser options manually.",
933: 		    options.file_path);
934: 	}
935: 
936: 	// #######
937: 	// ### type detection (initial)
938: 	// #######
939: 	// type candidates, ordered by descending specificity (~ from high to low)
940: 	vector<LogicalType> type_candidates = {
941: 	    LogicalType::VARCHAR, LogicalType::TIMESTAMP,
942: 	    LogicalType::DATE,    LogicalType::TIME,
943: 	    LogicalType::DOUBLE,  /* LogicalType::FLOAT,*/ LogicalType::BIGINT,
944: 	    LogicalType::INTEGER, /*LogicalType::SMALLINT, LogicalType::TINYINT,*/ LogicalType::BOOLEAN,
945: 	    LogicalType::SQLNULL};
946: 	// format template candidates, ordered by descending specificity (~ from high to low)
947: 	std::map<LogicalTypeId, vector<const char *>> format_template_candidates = {
948: 	    {LogicalTypeId::DATE, {"%m-%d-%Y", "%m-%d-%y", "%d-%m-%Y", "%d-%m-%y", "%Y-%m-%d", "%y-%m-%d"}},
949: 	    {LogicalTypeId::TIMESTAMP,
950: 	     {"%Y-%m-%d %H:%M:%S.%f", "%m-%d-%Y %I:%M:%S %p", "%m-%d-%y %I:%M:%S %p", "%d-%m-%Y %H:%M:%S",
951: 	      "%d-%m-%y %H:%M:%S", "%Y-%m-%d %H:%M:%S", "%y-%m-%d %H:%M:%S"}},
952: 	};
953: 	vector<vector<LogicalType>> best_sql_types_candidates;
954: 	map<LogicalTypeId, vector<string>> best_format_candidates;
955: 	DataChunk best_header_row;
956: 	DetectCandidateTypes(type_candidates, format_template_candidates, info_candidates, original_options, best_num_cols,
957: 	                     best_sql_types_candidates, best_format_candidates, best_header_row);
958: 
959: 	// #######
960: 	// ### header detection
961: 	// #######
962: 	DetectHeader(best_sql_types_candidates, best_header_row);
963: 
964: 	// #######
965: 	// ### type detection (refining)
966: 	// #######
967: 	return RefineTypeDetection(type_candidates, requested_types, best_sql_types_candidates, best_format_candidates);
968: }
969: 
970: bool BufferedCSVReader::TryParseComplexCSV(DataChunk &insert_chunk, string &error_message) {
971: 	// used for parsing algorithm
972: 	bool finished_chunk = false;
973: 	idx_t column = 0;
974: 	vector<idx_t> escape_positions;
975: 	uint8_t delimiter_pos = 0, escape_pos = 0, quote_pos = 0;
976: 	idx_t offset = 0;
977: 
978: 	// read values into the buffer (if any)
979: 	if (position >= buffer_size) {
980: 		if (!ReadBuffer(start)) {
981: 			return true;
982: 		}
983: 	}
984: 	// start parsing the first value
985: 	start = position;
986: 	goto value_start;
987: value_start:
988: 	/* state: value_start */
989: 	// this state parses the first characters of a value
990: 	offset = 0;
991: 	delimiter_pos = 0;
992: 	quote_pos = 0;
993: 	do {
994: 		idx_t count = 0;
995: 		for (; position < buffer_size; position++) {
996: 			quote_search.Match(quote_pos, buffer[position]);
997: 			delimiter_search.Match(delimiter_pos, buffer[position]);
998: 			count++;
999: 			if (delimiter_pos == options.delimiter.size()) {
1000: 				// found a delimiter, add the value
1001: 				offset = options.delimiter.size() - 1;
1002: 				goto add_value;
1003: 			} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1004: 				// found a newline, add the row
1005: 				goto add_row;
1006: 			}
1007: 			if (count > quote_pos) {
1008: 				// did not find a quote directly at the start of the value, stop looking for the quote now
1009: 				goto normal;
1010: 			}
1011: 			if (quote_pos == options.quote.size()) {
1012: 				// found a quote, go to quoted loop and skip the initial quote
1013: 				start += options.quote.size();
1014: 				goto in_quotes;
1015: 			}
1016: 		}
1017: 	} while (ReadBuffer(start));
1018: 	// file ends while scanning for quote/delimiter, go to final state
1019: 	goto final_state;
1020: normal:
1021: 	/* state: normal parsing state */
1022: 	// this state parses the remainder of a non-quoted value until we reach a delimiter or newline
1023: 	position++;
1024: 	do {
1025: 		for (; position < buffer_size; position++) {
1026: 			delimiter_search.Match(delimiter_pos, buffer[position]);
1027: 			if (delimiter_pos == options.delimiter.size()) {
1028: 				offset = options.delimiter.size() - 1;
1029: 				goto add_value;
1030: 			} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1031: 				goto add_row;
1032: 			}
1033: 		}
1034: 	} while (ReadBuffer(start));
1035: 	goto final_state;
1036: add_value:
1037: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1038: 	// increase position by 1 and move start to the new position
1039: 	offset = 0;
1040: 	start = ++position;
1041: 	if (position >= buffer_size && !ReadBuffer(start)) {
1042: 		// file ends right after delimiter, go to final state
1043: 		goto final_state;
1044: 	}
1045: 	goto value_start;
1046: add_row : {
1047: 	// check type of newline (\r or \n)
1048: 	bool carriage_return = buffer[position] == '\r';
1049: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1050: 	finished_chunk = AddRow(insert_chunk, column);
1051: 	// increase position by 1 and move start to the new position
1052: 	offset = 0;
1053: 	start = ++position;
1054: 	if (position >= buffer_size && !ReadBuffer(start)) {
1055: 		// file ends right after newline, go to final state
1056: 		goto final_state;
1057: 	}
1058: 	if (carriage_return) {
1059: 		// \r newline, go to special state that parses an optional \n afterwards
1060: 		goto carriage_return;
1061: 	} else {
1062: 		// \n newline, move to value start
1063: 		if (finished_chunk) {
1064: 			return true;
1065: 		}
1066: 		goto value_start;
1067: 	}
1068: }
1069: in_quotes:
1070: 	/* state: in_quotes */
1071: 	// this state parses the remainder of a quoted value
1072: 	quote_pos = 0;
1073: 	escape_pos = 0;
1074: 	position++;
1075: 	do {
1076: 		for (; position < buffer_size; position++) {
1077: 			quote_search.Match(quote_pos, buffer[position]);
1078: 			escape_search.Match(escape_pos, buffer[position]);
1079: 			if (quote_pos == options.quote.size()) {
1080: 				goto unquote;
1081: 			} else if (escape_pos == options.escape.size()) {
1082: 				escape_positions.push_back(position - start - (options.escape.size() - 1));
1083: 				goto handle_escape;
1084: 			}
1085: 		}
1086: 	} while (ReadBuffer(start));
1087: 	// still in quoted state at the end of the file, error:
1088: 	error_message = StringUtil::Format("Error in file \"%s\" on line %s: unterminated quotes. (%s)", options.file_path,
1089: 	                                   GetLineNumberStr(linenr, linenr_estimated).c_str(), options.toString());
1090: 	return false;
1091: unquote:
1092: 	/* state: unquote */
1093: 	// this state handles the state directly after we unquote
1094: 	// in this state we expect either another quote (entering the quoted state again, and escaping the quote)
1095: 	// or a delimiter/newline, ending the current value and moving on to the next value
1096: 	delimiter_pos = 0;
1097: 	quote_pos = 0;
1098: 	position++;
1099: 	if (position >= buffer_size && !ReadBuffer(start)) {
1100: 		// file ends right after unquote, go to final state
1101: 		offset = options.quote.size();
1102: 		goto final_state;
1103: 	}
1104: 	if (StringUtil::CharacterIsNewline(buffer[position])) {
1105: 		// quote followed by newline, add row
1106: 		offset = options.quote.size();
1107: 		goto add_row;
1108: 	}
1109: 	do {
1110: 		idx_t count = 0;
1111: 		for (; position < buffer_size; position++) {
1112: 			quote_search.Match(quote_pos, buffer[position]);
1113: 			delimiter_search.Match(delimiter_pos, buffer[position]);
1114: 			count++;
1115: 			if (count > delimiter_pos && count > quote_pos) {
1116: 				error_message = StringUtil::Format(
1117: 				    "Error in file \"%s\" on line %s: quote should be followed by end of value, end "
1118: 				    "of row or another quote. (%s)",
1119: 				    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.toString());
1120: 				return false;
1121: 			}
1122: 			if (delimiter_pos == options.delimiter.size()) {
1123: 				// quote followed by delimiter, add value
1124: 				offset = options.quote.size() + options.delimiter.size() - 1;
1125: 				goto add_value;
1126: 			} else if (quote_pos == options.quote.size() &&
1127: 			           (options.escape.empty() || options.escape == options.quote)) {
1128: 				// quote followed by quote, go back to quoted state and add to escape
1129: 				escape_positions.push_back(position - start - (options.quote.size() - 1));
1130: 				goto in_quotes;
1131: 			}
1132: 		}
1133: 	} while (ReadBuffer(start));
1134: 	error_message = StringUtil::Format(
1135: 	    "Error in file \"%s\" on line %s: quote should be followed by end of value, end of row or another quote. (%s)",
1136: 	    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.toString());
1137: 	return false;
1138: handle_escape:
1139: 	escape_pos = 0;
1140: 	quote_pos = 0;
1141: 	position++;
1142: 	do {
1143: 		idx_t count = 0;
1144: 		for (; position < buffer_size; position++) {
1145: 			quote_search.Match(quote_pos, buffer[position]);
1146: 			escape_search.Match(escape_pos, buffer[position]);
1147: 			count++;
1148: 			if (count > escape_pos && count > quote_pos) {
1149: 				error_message = StringUtil::Format(
1150: 				    "Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)",
1151: 				    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.toString());
1152: 				return false;
1153: 			}
1154: 			if (quote_pos == options.quote.size() || escape_pos == options.escape.size()) {
1155: 				// found quote or escape: move back to quoted state
1156: 				goto in_quotes;
1157: 			}
1158: 		}
1159: 	} while (ReadBuffer(start));
1160: 	error_message =
1161: 	    StringUtil::Format("Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)",
1162: 	                       options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.toString());
1163: 	return false;
1164: carriage_return:
1165: 	/* state: carriage_return */
1166: 	// this stage optionally skips a newline (\n) character, which allows \r\n to be interpreted as a single line
1167: 	if (buffer[position] == '\n') {
1168: 		// newline after carriage return: skip
1169: 		start = ++position;
1170: 		if (position >= buffer_size && !ReadBuffer(start)) {
1171: 			// file ends right after newline, go to final state
1172: 			goto final_state;
1173: 		}
1174: 	}
1175: 	if (finished_chunk) {
1176: 		return true;
1177: 	}
1178: 	goto value_start;
1179: final_state:
1180: 	if (finished_chunk) {
1181: 		return true;
1182: 	}
1183: 	if (column > 0 || position > start) {
1184: 		// remaining values to be added to the chunk
1185: 		AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1186: 		finished_chunk = AddRow(insert_chunk, column);
1187: 	}
1188: 	// final stage, only reached after parsing the file is finished
1189: 	// flush the parsed chunk and finalize parsing
1190: 	if (mode == ParserMode::PARSING) {
1191: 		Flush(insert_chunk);
1192: 	}
1193: 
1194: 	end_of_file_reached = true;
1195: 	return true;
1196: }
1197: 
1198: bool BufferedCSVReader::TryParseSimpleCSV(DataChunk &insert_chunk, string &error_message) {
1199: 	// used for parsing algorithm
1200: 	bool finished_chunk = false;
1201: 	idx_t column = 0;
1202: 	idx_t offset = 0;
1203: 	vector<idx_t> escape_positions;
1204: 
1205: 	// read values into the buffer (if any)
1206: 	if (position >= buffer_size) {
1207: 		if (!ReadBuffer(start)) {
1208: 			return true;
1209: 		}
1210: 	}
1211: 	// start parsing the first value
1212: 	goto value_start;
1213: value_start:
1214: 	offset = 0;
1215: 	/* state: value_start */
1216: 	// this state parses the first character of a value
1217: 	if (buffer[position] == options.quote[0]) {
1218: 		// quote: actual value starts in the next position
1219: 		// move to in_quotes state
1220: 		start = position + 1;
1221: 		goto in_quotes;
1222: 	} else {
1223: 		// no quote, move to normal parsing state
1224: 		start = position;
1225: 		goto normal;
1226: 	}
1227: normal:
1228: 	/* state: normal parsing state */
1229: 	// this state parses the remainder of a non-quoted value until we reach a delimiter or newline
1230: 	do {
1231: 		for (; position < buffer_size; position++) {
1232: 			if (buffer[position] == options.delimiter[0]) {
1233: 				// delimiter: end the value and add it to the chunk
1234: 				goto add_value;
1235: 			} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1236: 				// newline: add row
1237: 				goto add_row;
1238: 			}
1239: 		}
1240: 	} while (ReadBuffer(start));
1241: 	// file ends during normal scan: go to end state
1242: 	goto final_state;
1243: add_value:
1244: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1245: 	// increase position by 1 and move start to the new position
1246: 	offset = 0;
1247: 	start = ++position;
1248: 	if (position >= buffer_size && !ReadBuffer(start)) {
1249: 		// file ends right after delimiter, go to final state
1250: 		goto final_state;
1251: 	}
1252: 	goto value_start;
1253: add_row : {
1254: 	// check type of newline (\r or \n)
1255: 	bool carriage_return = buffer[position] == '\r';
1256: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1257: 	finished_chunk = AddRow(insert_chunk, column);
1258: 	// increase position by 1 and move start to the new position
1259: 	offset = 0;
1260: 	start = ++position;
1261: 	if (position >= buffer_size && !ReadBuffer(start)) {
1262: 		// file ends right after delimiter, go to final state
1263: 		goto final_state;
1264: 	}
1265: 	if (carriage_return) {
1266: 		// \r newline, go to special state that parses an optional \n afterwards
1267: 		goto carriage_return;
1268: 	} else {
1269: 		// \n newline, move to value start
1270: 		if (finished_chunk) {
1271: 			return true;
1272: 		}
1273: 		goto value_start;
1274: 	}
1275: }
1276: in_quotes:
1277: 	/* state: in_quotes */
1278: 	// this state parses the remainder of a quoted value
1279: 	position++;
1280: 	do {
1281: 		for (; position < buffer_size; position++) {
1282: 			if (buffer[position] == options.quote[0]) {
1283: 				// quote: move to unquoted state
1284: 				goto unquote;
1285: 			} else if (buffer[position] == options.escape[0]) {
1286: 				// escape: store the escaped position and move to handle_escape state
1287: 				escape_positions.push_back(position - start);
1288: 				goto handle_escape;
1289: 			}
1290: 		}
1291: 	} while (ReadBuffer(start));
1292: 	// still in quoted state at the end of the file, error:
1293: 	throw InvalidInputException("Error in file \"%s\" on line %s: unterminated quotes. (%s)", options.file_path,
1294: 	                            GetLineNumberStr(linenr, linenr_estimated).c_str(), options.toString());
1295: unquote:
1296: 	/* state: unquote */
1297: 	// this state handles the state directly after we unquote
1298: 	// in this state we expect either another quote (entering the quoted state again, and escaping the quote)
1299: 	// or a delimiter/newline, ending the current value and moving on to the next value
1300: 	position++;
1301: 	if (position >= buffer_size && !ReadBuffer(start)) {
1302: 		// file ends right after unquote, go to final state
1303: 		offset = 1;
1304: 		goto final_state;
1305: 	}
1306: 	if (buffer[position] == options.quote[0] && (options.escape.empty() || options.escape[0] == options.quote[0])) {
1307: 		// escaped quote, return to quoted state and store escape position
1308: 		escape_positions.push_back(position - start);
1309: 		goto in_quotes;
1310: 	} else if (buffer[position] == options.delimiter[0]) {
1311: 		// delimiter, add value
1312: 		offset = 1;
1313: 		goto add_value;
1314: 	} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1315: 		offset = 1;
1316: 		goto add_row;
1317: 	} else {
1318: 		error_message = StringUtil::Format(
1319: 		    "Error in file \"%s\" on line %s: quote should be followed by end of value, end of "
1320: 		    "row or another quote. (%s)",
1321: 		    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.toString());
1322: 		return false;
1323: 	}
1324: handle_escape:
1325: 	/* state: handle_escape */
1326: 	// escape should be followed by a quote or another escape character
1327: 	position++;
1328: 	if (position >= buffer_size && !ReadBuffer(start)) {
1329: 		error_message = StringUtil::Format(
1330: 		    "Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)", options.file_path,
1331: 		    GetLineNumberStr(linenr, linenr_estimated).c_str(), options.toString());
1332: 		return false;
1333: 	}
1334: 	if (buffer[position] != options.quote[0] && buffer[position] != options.escape[0]) {
1335: 		error_message = StringUtil::Format(
1336: 		    "Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)", options.file_path,
1337: 		    GetLineNumberStr(linenr, linenr_estimated).c_str(), options.toString());
1338: 		return false;
1339: 	}
1340: 	// escape was followed by quote or escape, go back to quoted state
1341: 	goto in_quotes;
1342: carriage_return:
1343: 	/* state: carriage_return */
1344: 	// this stage optionally skips a newline (\n) character, which allows \r\n to be interpreted as a single line
1345: 	if (buffer[position] == '\n') {
1346: 		// newline after carriage return: skip
1347: 		// increase position by 1 and move start to the new position
1348: 		start = ++position;
1349: 		if (position >= buffer_size && !ReadBuffer(start)) {
1350: 			// file ends right after delimiter, go to final state
1351: 			goto final_state;
1352: 		}
1353: 	}
1354: 	if (finished_chunk) {
1355: 		return true;
1356: 	}
1357: 	goto value_start;
1358: final_state:
1359: 	if (finished_chunk) {
1360: 		return true;
1361: 	}
1362: 
1363: 	if (column > 0 || position > start) {
1364: 		// remaining values to be added to the chunk
1365: 		AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1366: 		finished_chunk = AddRow(insert_chunk, column);
1367: 	}
1368: 
1369: 	// final stage, only reached after parsing the file is finished
1370: 	// flush the parsed chunk and finalize parsing
1371: 	if (mode == ParserMode::PARSING) {
1372: 		Flush(insert_chunk);
1373: 	}
1374: 
1375: 	end_of_file_reached = true;
1376: 	return true;
1377: }
1378: 
1379: bool BufferedCSVReader::ReadBuffer(idx_t &start) {
1380: 	auto old_buffer = move(buffer);
1381: 
1382: 	// the remaining part of the last buffer
1383: 	idx_t remaining = buffer_size - start;
1384: 	idx_t buffer_read_size = INITIAL_BUFFER_SIZE;
1385: 	while (remaining > buffer_read_size) {
1386: 		buffer_read_size *= 2;
1387: 	}
1388: 	if (remaining + buffer_read_size > MAXIMUM_CSV_LINE_SIZE) {
1389: 		throw InvalidInputException("Maximum line size of %llu bytes exceeded!", MAXIMUM_CSV_LINE_SIZE);
1390: 	}
1391: 	buffer = unique_ptr<char[]>(new char[buffer_read_size + remaining + 1]);
1392: 	buffer_size = remaining + buffer_read_size;
1393: 	if (remaining > 0) {
1394: 		// remaining from last buffer: copy it here
1395: 		memcpy(buffer.get(), old_buffer.get() + start, remaining);
1396: 	}
1397: 	idx_t read_count = file_handle->Read(buffer.get() + remaining, buffer_read_size);
1398: 
1399: 	bytes_in_chunk += read_count;
1400: 	buffer_size = remaining + read_count;
1401: 	buffer[buffer_size] = '\0';
1402: 	if (old_buffer) {
1403: 		cached_buffers.push_back(move(old_buffer));
1404: 	}
1405: 	start = 0;
1406: 	position = remaining;
1407: 	if (!bom_checked) {
1408: 		bom_checked = true;
1409: 		if (read_count >= 3 && buffer[0] == '\xEF' && buffer[1] == '\xBB' && buffer[2] == '\xBF') {
1410: 			position += 3;
1411: 		}
1412: 	}
1413: 
1414: 	return read_count > 0;
1415: }
1416: 
1417: void BufferedCSVReader::ParseCSV(DataChunk &insert_chunk) {
1418: 	// if no auto-detect or auto-detect with jumping samples, we have nothing cached and start from the beginning
1419: 	if (cached_chunks.empty()) {
1420: 		cached_buffers.clear();
1421: 	} else {
1422: 		auto &chunk = cached_chunks.front();
1423: 		parse_chunk.Move(*chunk);
1424: 		cached_chunks.pop();
1425: 		Flush(insert_chunk);
1426: 		return;
1427: 	}
1428: 
1429: 	string error_message;
1430: 	if (!TryParseCSV(ParserMode::PARSING, insert_chunk, error_message)) {
1431: 		throw InvalidInputException(error_message);
1432: 	}
1433: }
1434: 
1435: bool BufferedCSVReader::TryParseCSV(ParserMode mode) {
1436: 	DataChunk dummy_chunk;
1437: 	string error_message;
1438: 	return TryParseCSV(mode, dummy_chunk, error_message);
1439: }
1440: 
1441: void BufferedCSVReader::ParseCSV(ParserMode mode) {
1442: 	DataChunk dummy_chunk;
1443: 	string error_message;
1444: 	if (!TryParseCSV(mode, dummy_chunk, error_message)) {
1445: 		throw InvalidInputException(error_message);
1446: 	}
1447: }
1448: 
1449: bool BufferedCSVReader::TryParseCSV(ParserMode parser_mode, DataChunk &insert_chunk, string &error_message) {
1450: 	mode = parser_mode;
1451: 
1452: 	if (options.quote.size() <= 1 && options.escape.size() <= 1 && options.delimiter.size() == 1) {
1453: 		return TryParseSimpleCSV(insert_chunk, error_message);
1454: 	} else {
1455: 		return TryParseComplexCSV(insert_chunk, error_message);
1456: 	}
1457: }
1458: 
1459: void BufferedCSVReader::AddValue(char *str_val, idx_t length, idx_t &column, vector<idx_t> &escape_positions) {
1460: 	if (length == 0 && column == 0) {
1461: 		row_empty = true;
1462: 	} else {
1463: 		row_empty = false;
1464: 	}
1465: 
1466: 	if (!sql_types.empty() && column == sql_types.size() && length == 0) {
1467: 		// skip a single trailing delimiter in last column
1468: 		return;
1469: 	}
1470: 	if (mode == ParserMode::SNIFFING_DIALECT) {
1471: 		column++;
1472: 		return;
1473: 	}
1474: 	if (column >= sql_types.size()) {
1475: 		throw InvalidInputException("Error on line %s: expected %lld values per row, but got more. (%s)",
1476: 		                            GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(),
1477: 		                            options.toString());
1478: 	}
1479: 
1480: 	// insert the line number into the chunk
1481: 	idx_t row_entry = parse_chunk.size();
1482: 
1483: 	str_val[length] = '\0';
1484: 
1485: 	// test against null string
1486: 	if (!options.force_not_null[column] && strcmp(options.null_str.c_str(), str_val) == 0) {
1487: 		FlatVector::SetNull(parse_chunk.data[column], row_entry, true);
1488: 	} else {
1489: 		auto &v = parse_chunk.data[column];
1490: 		auto parse_data = FlatVector::GetData<string_t>(v);
1491: 		if (!escape_positions.empty()) {
1492: 			// remove escape characters (if any)
1493: 			string old_val = str_val;
1494: 			string new_val = "";
1495: 			idx_t prev_pos = 0;
1496: 			for (idx_t i = 0; i < escape_positions.size(); i++) {
1497: 				idx_t next_pos = escape_positions[i];
1498: 				new_val += old_val.substr(prev_pos, next_pos - prev_pos);
1499: 
1500: 				if (options.escape.empty() || options.escape == options.quote) {
1501: 					prev_pos = next_pos + options.quote.size();
1502: 				} else {
1503: 					prev_pos = next_pos + options.escape.size();
1504: 				}
1505: 			}
1506: 			new_val += old_val.substr(prev_pos, old_val.size() - prev_pos);
1507: 			escape_positions.clear();
1508: 			parse_data[row_entry] = StringVector::AddStringOrBlob(v, string_t(new_val));
1509: 		} else {
1510: 			parse_data[row_entry] = string_t(str_val, length);
1511: 		}
1512: 	}
1513: 
1514: 	// move to the next column
1515: 	column++;
1516: }
1517: 
1518: bool BufferedCSVReader::AddRow(DataChunk &insert_chunk, idx_t &column) {
1519: 	linenr++;
1520: 
1521: 	if (row_empty) {
1522: 		row_empty = false;
1523: 		if (sql_types.size() != 1) {
1524: 			column = 0;
1525: 			return false;
1526: 		}
1527: 	}
1528: 
1529: 	if (column < sql_types.size() && mode != ParserMode::SNIFFING_DIALECT) {
1530: 		throw InvalidInputException("Error on line %s: expected %lld values per row, but got %d. (%s)",
1531: 		                            GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(), column,
1532: 		                            options.toString());
1533: 	}
1534: 
1535: 	if (mode == ParserMode::SNIFFING_DIALECT) {
1536: 		sniffed_column_counts.push_back(column);
1537: 
1538: 		if (sniffed_column_counts.size() == options.sample_chunk_size) {
1539: 			return true;
1540: 		}
1541: 	} else {
1542: 		parse_chunk.SetCardinality(parse_chunk.size() + 1);
1543: 	}
1544: 
1545: 	if (mode == ParserMode::PARSING_HEADER) {
1546: 		return true;
1547: 	}
1548: 
1549: 	if (mode == ParserMode::SNIFFING_DATATYPES && parse_chunk.size() == options.sample_chunk_size) {
1550: 		return true;
1551: 	}
1552: 
1553: 	if (mode == ParserMode::PARSING && parse_chunk.size() == STANDARD_VECTOR_SIZE) {
1554: 		Flush(insert_chunk);
1555: 		return true;
1556: 	}
1557: 
1558: 	column = 0;
1559: 	return false;
1560: }
1561: 
1562: void BufferedCSVReader::Flush(DataChunk &insert_chunk) {
1563: 	if (parse_chunk.size() == 0) {
1564: 		return;
1565: 	}
1566: 	// convert the columns in the parsed chunk to the types of the table
1567: 	insert_chunk.SetCardinality(parse_chunk);
1568: 	for (idx_t col_idx = 0; col_idx < sql_types.size(); col_idx++) {
1569: 		if (sql_types[col_idx].id() == LogicalTypeId::VARCHAR) {
1570: 			// target type is varchar: no need to convert
1571: 			// just test that all strings are valid utf-8 strings
1572: 			auto parse_data = FlatVector::GetData<string_t>(parse_chunk.data[col_idx]);
1573: 			for (idx_t i = 0; i < parse_chunk.size(); i++) {
1574: 				if (!FlatVector::IsNull(parse_chunk.data[col_idx], i)) {
1575: 					auto s = parse_data[i];
1576: 					auto utf_type = Utf8Proc::Analyze(s.GetDataUnsafe(), s.GetSize());
1577: 					if (utf_type == UnicodeType::INVALID) {
1578: 						string col_name = to_string(col_idx);
1579: 						if (col_idx < col_names.size()) {
1580: 							col_name = "\"" + col_names[col_idx] + "\"";
1581: 						}
1582: 						throw InvalidInputException("Error in file \"%s\" between line %llu and %llu in column \"%s\": "
1583: 						                            "file is not valid UTF8. Parser options: %s",
1584: 						                            options.file_path, linenr - parse_chunk.size(), linenr, col_name,
1585: 						                            options.toString());
1586: 					}
1587: 				}
1588: 			}
1589: 			insert_chunk.data[col_idx].Reference(parse_chunk.data[col_idx]);
1590: 		} else {
1591: 			string error_message;
1592: 			bool success;
1593: 			if (options.has_format[LogicalTypeId::DATE] && sql_types[col_idx].id() == LogicalTypeId::DATE) {
1594: 				// use the date format to cast the chunk
1595: 				success = TryCastDateVector(options, parse_chunk.data[col_idx], insert_chunk.data[col_idx],
1596: 				                            parse_chunk.size(), error_message);
1597: 			} else if (options.has_format[LogicalTypeId::TIMESTAMP] &&
1598: 			           sql_types[col_idx].id() == LogicalTypeId::TIMESTAMP) {
1599: 				// use the date format to cast the chunk
1600: 				success = TryCastTimestampVector(options, parse_chunk.data[col_idx], insert_chunk.data[col_idx],
1601: 				                                 parse_chunk.size(), error_message);
1602: 			} else {
1603: 				// target type is not varchar: perform a cast
1604: 				success = VectorOperations::TryCast(parse_chunk.data[col_idx], insert_chunk.data[col_idx],
1605: 				                                    parse_chunk.size(), &error_message);
1606: 			}
1607: 			if (!success) {
1608: 				string col_name = to_string(col_idx);
1609: 				if (col_idx < col_names.size()) {
1610: 					col_name = "\"" + col_names[col_idx] + "\"";
1611: 				}
1612: 
1613: 				if (options.auto_detect) {
1614: 					throw InvalidInputException("%s in column %s, between line %llu and %llu. Parser "
1615: 					                            "options: %s. Consider either increasing the sample size "
1616: 					                            "(SAMPLE_SIZE=X [X rows] or SAMPLE_SIZE=-1 [all rows]), "
1617: 					                            "or skipping column conversion (ALL_VARCHAR=1)",
1618: 					                            error_message, col_name, linenr - parse_chunk.size() + 1, linenr,
1619: 					                            options.toString());
1620: 				} else {
1621: 					throw InvalidInputException("%s between line %llu and %llu in column %s. Parser options: %s ",
1622: 					                            error_message, linenr - parse_chunk.size(), linenr, col_name,
1623: 					                            options.toString());
1624: 				}
1625: 			}
1626: 		}
1627: 	}
1628: 	parse_chunk.Reset();
1629: }
1630: } // namespace duckdb
[end of src/execution/operator/persistent/buffered_csv_reader.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: