You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Segment Fault Crash when select a view (which has IN statement)
For latest master branch (including 0.2.4 release), when I execute the following 2 sql, duckdb will crash with  segment fault error.

```
create view t1 as select * from (select 123 as a) t where a in (select 123 );

select * from t1
```


the call stack for this crash will be:

```
=================================================================
==70572==ERROR: AddressSanitizer: stack-buffer-underflow on address 0x7ffee1d00788 at pc 0x00010ec0ccba bp 0x7ffee1cfe630 sp 0x7ffee1cfe628
READ of size 8 at 0x7ffee1d00788 thread T0
    #0 0x10ec0ccb9 in duckdb::Binder::GenerateTableIndex() binder.cpp:221
    #1 0x10ec0cd48 in duckdb::Binder::GenerateTableIndex() binder.cpp:222
    #2 0x10e7815d9 in duckdb::PlanUncorrelatedSubquery(duckdb::Binder&, duckdb::BoundSubqueryExpression&, std::__1::unique_ptr<duckdb::LogicalOperator, std::__1::default_delete<duckdb::LogicalOperator> >&, std::__1::unique_ptr<duckdb::LogicalOperator, std::__1::default_delete<duckdb::LogicalOperator> >) plan_subquery.cpp:115
    #3 0x10e77be59 in duckdb::Binder::PlanSubquery(duckdb::BoundSubqueryExpression&, std::__1::unique_ptr<duckdb::LogicalOperator, std::__1::default_delete<duckdb::LogicalOperator> >&) plan_subquery.cpp:308
    #4 0x10e76e5b3 in duckdb::Binder::PlanSubqueries(std::__1::unique_ptr<duckdb::Expression, std::__1::default_delete<duckdb::Expression> >*, std::__1::unique_ptr<duckdb::LogicalOperator, std::__1::default_delete<duckdb::LogicalOperator> >*) plan_subquery.cpp:340
    #5 0x10e76d77a in duckdb::Binder::PlanFilter(std::__1::unique_ptr<duckdb::Expression, std::__1::default_delete<duckdb::Expression> >, std::__1::unique_ptr<duckdb::LogicalOperator, std::__1::default_delete<duckdb::LogicalOperator> >) plan_select_node.cpp:12
    #6 0x10e76f5b5 in duckdb::Binder::CreatePlan(duckdb::BoundSelectNode&) plan_select_node.cpp:30
    #7 0x10ec06a03 in duckdb::Binder::CreatePlan(duckdb::BoundQueryNode&) binder.cpp:109
    #8 0x10e9be930 in duckdb::Binder::CreatePlan(duckdb::BoundSubqueryRef&) plan_subqueryref.cpp:10
    #9 0x10ec08a38 in duckdb::Binder::CreatePlan(duckdb::BoundTableRef&) binder.cpp:157
    #10 0x10e76f0a6 in duckdb::Binder::CreatePlan(duckdb::BoundSelectNode&) plan_select_node.cpp:21
    #11 0x10ec06a03 in duckdb::Binder::CreatePlan(duckdb::BoundQueryNode&) binder.cpp:109
    #12 0x10ec05d4f in duckdb::Binder::Bind(duckdb::QueryNode&) binder.cpp:102
    #13 0x10e84da53 in duckdb::Binder::Bind(duckdb::SelectStatement&) bind_select.cpp:9
    #14 0x10ec02260 in duckdb::Binder::Bind(duckdb::SQLStatement&) binder.cpp:33
    #15 0x10ec2913a in duckdb::Planner::CreatePlan(duckdb::SQLStatement&) planner.cpp:29
    #16 0x10ec2ef98 in duckdb::Planner::CreatePlan(std::__1::unique_ptr<duckdb::SQLStatement, std::__1::default_delete<duckdb::SQLStatement> >) planner.cpp:160
    #17 0x11289d076 in duckdb::ClientContext::CreatePreparedStatement(duckdb::ClientContextLock&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::unique_ptr<duckdb::SQLStatement, std::__1::default_delete<duckdb::SQLStatement> >) client_context.cpp:153
    #18 0x112a29dc9 in duckdb::ClientContext::PrepareInternal(duckdb::ClientContextLock&, std::__1::unique_ptr<duckdb::SQLStatement, std::__1::default_delete<duckdb::SQLStatement> >)::$_0::operator()() const client_context.cpp:270
    #19 0x112a299f6 in decltype(std::__1::forward<duckdb::ClientContext::PrepareInternal(duckdb::ClientContextLock&, std::__1::unique_ptr<duckdb::SQLStatement, std::__1::default_delete<duckdb::SQLStatement> >)::$_0&>(fp)()) std::__1::__invoke<duckdb::ClientContext::PrepareInternal(duckdb::ClientContextLock&, std::__1::unique_ptr<duckdb::SQLStatement, std::__1::default_delete<duckdb::SQLStatement> >)::$_0&>(duckdb::ClientContext::PrepareInternal(duckdb::ClientContextLock&, std::__1::unique_ptr<duckdb::SQLStatement, std::__1::default_delete<duckdb::SQLStatement> >)::$_0&) type_traits:4361
    #20 0x112a29856 in void std::__1::__invoke_void_return_wrapper<void>::__call<duckdb::ClientContext::PrepareInternal(duckdb::ClientContextLock&, std::__1::unique_ptr<duckdb::SQLStatement, std::__1::default_delete<duckdb::SQLStatement> >)::$_0&>(duckdb::ClientContext::PrepareInternal(duckdb::ClientContextLock&, std::__1::unique_ptr<duckdb::SQLStatement, std::__1::default_delete<duckdb::SQLStatement> >)::$_0&) __functional_base:349
    #21 0x112a297b2 in std::__1::__function::__alloc_func<duckdb::ClientContext::PrepareInternal(duckdb::ClientContextLock&, std::__1::unique_ptr<duckdb::SQLStatement, std::__1::default_delete<duckdb::SQLStatement> >)::$_0, std::__1::allocator<duckdb::ClientContext::PrepareInternal(duckdb::ClientContextLock&, std::__1::unique_ptr<duckdb::SQLStatement, std::__1::default_delete<duckdb::SQLStatement> >)::$_0>, void ()>::operator()() functional:1527
    #22 0x112a23806 in std::__1::__function::__func<duckdb::ClientContext::PrepareInternal(duckdb::ClientContextLock&, std::__1::unique_ptr<duckdb::SQLStatement, std::__1::default_delete<duckdb::SQLStatement> >)::$_0, std::__1::allocator<duckdb::ClientContext::PrepareInternal(duckdb::ClientContextLock&, std::__1::unique_ptr<duckdb::SQLStatement, std::__1::default_delete<duckdb::SQLStatement> >)::$_0>, void ()>::operator()() functional:1651
    #23 0x112a42864 in std::__1::__function::__value_func<void ()>::operator()() const functional:1799
    #24 0x1128c0ec2 in std::__1::function<void ()>::operator()() const functional:2347
    #25 0x1128a81dc in duckdb::ClientContext::RunFunctionInTransactionInternal(duckdb::ClientContextLock&, std::__1::function<void ()> const&, bool) client_context.cpp:669
    #26 0x1128a77b6 in duckdb::ClientContext::PrepareInternal(duckdb::ClientContextLock&, std::__1::unique_ptr<duckdb::SQLStatement, std::__1::default_delete<duckdb::SQLStatement> >) client_context.cpp:269
    #27 0x1128a93af in duckdb::ClientContext::Prepare(std::__1::unique_ptr<duckdb::SQLStatement, std::__1::default_delete<duckdb::SQLStatement> >) client_context.cpp:280
    #28 0x1128ca2c8 in duckdb::Connection::Prepare(std::__1::unique_ptr<duckdb::SQLStatement, std::__1::default_delete<duckdb::SQLStatement> >) connection.cpp:80
    #29 0x10e030bf7 in sqlite3_prepare_v2 sqlite3_api_wrapper.cpp:163
    #30 0x10df523b4 in shell_exec shell.c:13106
    #31 0x10e0082cc in runOneSqlLine shell.c:20022
    #32 0x10df56ab0 in process_input shell.c:20122
    #33 0x10df1a19f in main shell.c:20906
    #34 0x7fff71b79cc8 in start (libdyld.dylib:x86_64+0x1acc8)

Address 0x7ffee1d00788 is located in stack of thread T0 at offset 8 in frame
    #0 0x10e76d5ff in duckdb::Binder::PlanFilter(std::__1::unique_ptr<duckdb::Expression, std::__1::default_delete<duckdb::Expression> >, std::__1::unique_ptr<duckdb::LogicalOperator, std::__1::default_delete<duckdb::LogicalOperator> >) plan_select_node.cpp:11
```



Seems this only happens when "create view" (not create table" && contains "IN" or "NOT IN".


</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
2: 
3: ![.github/workflows/main.yml](https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
6: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3901452.svg)](https://zenodo.org/record/3901452)
7: 
8: 
9: ## Installation
10: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
11: 
12: ## Development
13: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
14: 
15: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
16: 
17: 
[end of README.md]
[start of extension/tpch/dbgen/dbgen.cpp]
1: #include "dbgen/dbgen.hpp"
2: #include "dbgen/dbgen_gunk.hpp"
3: #include "tpch_constants.hpp"
4: 
5: #ifndef DUCKDB_AMALGAMATION
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/common/types/date.hpp"
8: #include "duckdb/parser/column_definition.hpp"
9: #include "duckdb/storage/data_table.hpp"
10: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
11: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
12: #include "duckdb/parser/parsed_data/create_table_info.hpp"
13: #include "duckdb/parser/constraints/not_null_constraint.hpp"
14: #include "duckdb/catalog/catalog.hpp"
15: #include "duckdb/planner/binder.hpp"
16: #endif
17: 
18: #define DECLARER /* EXTERN references get defined here */
19: 
20: #include "dbgen/dss.h"
21: #include "dbgen/dsstypes.h"
22: 
23: #include <cassert>
24: #include <mutex>
25: 
26: using namespace duckdb;
27: 
28: seed_t DBGenGlobals::Seed[MAX_STREAM + 1] = {
29:     {PART, 1, 0, 1},                           /* P_MFG_SD     0 */
30:     {PART, 46831694, 0, 1},                    /* P_BRND_SD    1 */
31:     {PART, 1841581359, 0, 1},                  /* P_TYPE_SD    2 */
32:     {PART, 1193163244, 0, 1},                  /* P_SIZE_SD    3 */
33:     {PART, 727633698, 0, 1},                   /* P_CNTR_SD    4 */
34:     {NONE, 933588178, 0, 1},                   /* text pregeneration  5 */
35:     {PART, 804159733, 0, 2},                   /* P_CMNT_SD    6 */
36:     {PSUPP, 1671059989, 0, SUPP_PER_PART},     /* PS_QTY_SD    7 */
37:     {PSUPP, 1051288424, 0, SUPP_PER_PART},     /* PS_SCST_SD   8 */
38:     {PSUPP, 1961692154, 0, SUPP_PER_PART * 2}, /* PS_CMNT_SD   9 */
39:     {ORDER, 1227283347, 0, 1},                 /* O_SUPP_SD    10 */
40:     {ORDER, 1171034773, 0, 1},                 /* O_CLRK_SD    11 */
41:     {ORDER, 276090261, 0, 2},                  /* O_CMNT_SD    12 */
42:     {ORDER, 1066728069, 0, 1},                 /* O_ODATE_SD   13 */
43:     {LINE, 209208115, 0, O_LCNT_MAX},          /* L_QTY_SD     14 */
44:     {LINE, 554590007, 0, O_LCNT_MAX},          /* L_DCNT_SD    15 */
45:     {LINE, 721958466, 0, O_LCNT_MAX},          /* L_TAX_SD     16 */
46:     {LINE, 1371272478, 0, O_LCNT_MAX},         /* L_SHIP_SD    17 */
47:     {LINE, 675466456, 0, O_LCNT_MAX},          /* L_SMODE_SD   18 */
48:     {LINE, 1808217256, 0, O_LCNT_MAX},         /* L_PKEY_SD    19 */
49:     {LINE, 2095021727, 0, O_LCNT_MAX},         /* L_SKEY_SD    20 */
50:     {LINE, 1769349045, 0, O_LCNT_MAX},         /* L_SDTE_SD    21 */
51:     {LINE, 904914315, 0, O_LCNT_MAX},          /* L_CDTE_SD    22 */
52:     {LINE, 373135028, 0, O_LCNT_MAX},          /* L_RDTE_SD    23 */
53:     {LINE, 717419739, 0, O_LCNT_MAX},          /* L_RFLG_SD    24 */
54:     {LINE, 1095462486, 0, O_LCNT_MAX * 2},     /* L_CMNT_SD    25 */
55:     {CUST, 881155353, 0, 9},                   /* C_ADDR_SD    26 */
56:     {CUST, 1489529863, 0, 1},                  /* C_NTRG_SD    27 */
57:     {CUST, 1521138112, 0, 3},                  /* C_PHNE_SD    28 */
58:     {CUST, 298370230, 0, 1},                   /* C_ABAL_SD    29 */
59:     {CUST, 1140279430, 0, 1},                  /* C_MSEG_SD    30 */
60:     {CUST, 1335826707, 0, 2},                  /* C_CMNT_SD    31 */
61:     {SUPP, 706178559, 0, 9},                   /* S_ADDR_SD    32 */
62:     {SUPP, 110356601, 0, 1},                   /* S_NTRG_SD    33 */
63:     {SUPP, 884434366, 0, 3},                   /* S_PHNE_SD    34 */
64:     {SUPP, 962338209, 0, 1},                   /* S_ABAL_SD    35 */
65:     {SUPP, 1341315363, 0, 2},                  /* S_CMNT_SD    36 */
66:     {PART, 709314158, 0, 92},                  /* P_NAME_SD    37 */
67:     {ORDER, 591449447, 0, 1},                  /* O_PRIO_SD    38 */
68:     {LINE, 431918286, 0, 1},                   /* HVAR_SD      39 */
69:     {ORDER, 851767375, 0, 1},                  /* O_CKEY_SD    40 */
70:     {NATION, 606179079, 0, 2},                 /* N_CMNT_SD    41 */
71:     {REGION, 1500869201, 0, 2},                /* R_CMNT_SD    42 */
72:     {ORDER, 1434868289, 0, 1},                 /* O_LCNT_SD    43 */
73:     {SUPP, 263032577, 0, 1},                   /* BBB offset   44 */
74:     {SUPP, 753643799, 0, 1},                   /* BBB type     45 */
75:     {SUPP, 202794285, 0, 1},                   /* BBB comment  46 */
76:     {SUPP, 715851524, 0, 1}                    /* BBB junk     47 */
77: };
78: double DBGenGlobals::dM = 2147483647.0;
79: tdef DBGenGlobals::tdefs[10] = {
80:     {"part.tbl", "part table", 200000, NULL, NULL, PSUPP, 0},
81:     {"partsupp.tbl", "partsupplier table", 200000, NULL, NULL, NONE, 0},
82:     {"supplier.tbl", "suppliers table", 10000, NULL, NULL, NONE, 0},
83:     {"customer.tbl", "customers table", 150000, NULL, NULL, NONE, 0},
84:     {"orders.tbl", "order table", 150000, NULL, NULL, LINE, 0},
85:     {"lineitem.tbl", "lineitem table", 150000, NULL, NULL, NONE, 0},
86:     {"orders.tbl", "orders/lineitem tables", 150000, NULL, NULL, LINE, 0},
87:     {"part.tbl", "part/partsupplier tables", 200000, NULL, NULL, PSUPP, 0},
88:     {"nation.tbl", "nation table", NATIONS_MAX, NULL, NULL, NONE, 0},
89:     {"region.tbl", "region table", NATIONS_MAX, NULL, NULL, NONE, 0},
90: };
91: 
92: static seed_t *Seed = DBGenGlobals::Seed;
93: seed_t seed_backup[MAX_STREAM + 1];
94: static bool first_invocation = true;
95: std::mutex dbgen_lock;
96: 
97: static tdef *tdefs = DBGenGlobals::tdefs;
98: 
99: namespace tpch {
100: 
101: struct UnsafeAppender {
102: 	UnsafeAppender(ClientContext &context, TableCatalogEntry *tbl) : context(context), tbl(tbl), col(0) {
103: 		vector<LogicalType> types;
104: 		for (idx_t i = 0; i < tbl->columns.size(); i++) {
105: 			types.push_back(tbl->columns[i].type);
106: 		}
107: 		chunk.Initialize(types);
108: 	}
109: 
110: 	void BeginRow() {
111: 		col = 0;
112: 	}
113: 
114: 	void EndRow() {
115: 		assert(col == chunk.ColumnCount());
116: 		chunk.SetCardinality(chunk.size() + 1);
117: 		if (chunk.size() == STANDARD_VECTOR_SIZE) {
118: 			Flush();
119: 		}
120: 	}
121: 
122: 	void Flush() {
123: 		if (chunk.size() == 0) {
124: 			return;
125: 		}
126: 		tbl->storage->Append(*tbl, context, chunk);
127: 		chunk.Reset();
128: 	}
129: 
130: 	template <class T> void AppendValue(T value) {
131: 		assert(col < chunk.ColumnCount());
132: 		FlatVector::GetData<T>(chunk.data[col])[chunk.size()] = value;
133: 		col++;
134: 	}
135: 
136: 	void AppendString(const char *value) {
137: 		AppendValue<string_t>(StringVector::AddString(chunk.data[col], value));
138: 	}
139: 
140: private:
141: 	ClientContext &context;
142: 	TableCatalogEntry *tbl;
143: 	DataChunk chunk;
144: 	idx_t col;
145: };
146: 
147: struct tpch_append_information {
148: 	unique_ptr<UnsafeAppender> appender;
149: };
150: 
151: void append_value(tpch_append_information &info, int32_t value) {
152: 	info.appender->AppendValue<int32_t>(value);
153: }
154: 
155: void append_string(tpch_append_information &info, const char *value) {
156: 	info.appender->AppendString(value);
157: }
158: 
159: void append_decimal(tpch_append_information &info, int64_t value) {
160: 	info.appender->AppendValue<int64_t>(value);
161: }
162: 
163: void append_date(tpch_append_information &info, string value) {
164: 	info.appender->AppendValue<int32_t>(Date::FromString(value));
165: }
166: 
167: void append_char(tpch_append_information &info, char value) {
168: 	char val[2];
169: 	val[0] = value;
170: 	val[1] = '\0';
171: 	append_string(info, val);
172: }
173: 
174: static void append_order(order_t *o, tpch_append_information *info) {
175: 	auto &append_info = info[ORDER];
176: 
177: 	// fill the current row with the order information
178: 	append_info.appender->BeginRow();
179: 	// o_orderkey
180: 	append_value(append_info, o->okey);
181: 	// o_custkey
182: 	append_value(append_info, o->custkey);
183: 	// o_orderstatus
184: 	append_char(append_info, o->orderstatus);
185: 	// o_totalprice
186: 	append_decimal(append_info, o->totalprice);
187: 	// o_orderdate
188: 	append_date(append_info, o->odate);
189: 	// o_orderpriority
190: 	append_string(append_info, o->opriority);
191: 	// o_clerk
192: 	append_string(append_info, o->clerk);
193: 	// o_shippriority
194: 	append_value(append_info, o->spriority);
195: 	// o_comment
196: 	append_string(append_info, o->comment);
197: 	append_info.appender->EndRow();
198: }
199: 
200: static void append_line(order_t *o, tpch_append_information *info) {
201: 	auto &append_info = info[LINE];
202: 
203: 	// fill the current row with the order information
204: 	for (DSS_HUGE i = 0; i < o->lines; i++) {
205: 		append_info.appender->BeginRow();
206: 		// l_orderkey
207: 		append_value(append_info, o->l[i].okey);
208: 		// l_partkey
209: 		append_value(append_info, o->l[i].partkey);
210: 		// l_suppkey
211: 		append_value(append_info, o->l[i].suppkey);
212: 		// l_linenumber
213: 		append_value(append_info, o->l[i].lcnt);
214: 		// l_quantity
215: 		append_value(append_info, o->l[i].quantity);
216: 		// l_extendedprice
217: 		append_decimal(append_info, o->l[i].eprice);
218: 		// l_discount
219: 		append_decimal(append_info, o->l[i].discount);
220: 		// l_tax
221: 		append_decimal(append_info, o->l[i].tax);
222: 		// l_returnflag
223: 		append_char(append_info, o->l[i].rflag[0]);
224: 		// l_linestatus
225: 		append_char(append_info, o->l[i].lstatus[0]);
226: 		// l_shipdate
227: 		append_date(append_info, o->l[i].sdate);
228: 		// l_commitdate
229: 		append_date(append_info, o->l[i].cdate);
230: 		// l_receiptdate
231: 		append_date(append_info, o->l[i].rdate);
232: 		// l_shipinstruct
233: 		append_string(append_info, o->l[i].shipinstruct);
234: 		// l_shipmode
235: 		append_string(append_info, o->l[i].shipmode);
236: 		// l_comment
237: 		append_string(append_info, o->l[i].comment);
238: 		append_info.appender->EndRow();
239: 	}
240: }
241: 
242: static void append_order_line(order_t *o, tpch_append_information *info) {
243: 	append_order(o, info);
244: 	append_line(o, info);
245: }
246: 
247: static void append_supp(supplier_t *supp, tpch_append_information *info) {
248: 	auto &append_info = info[SUPP];
249: 
250: 	append_info.appender->BeginRow();
251: 	// s_suppkey
252: 	append_value(append_info, supp->suppkey);
253: 	// s_name
254: 	append_string(append_info, supp->name);
255: 	// s_address
256: 	append_string(append_info, supp->address);
257: 	// s_nationkey
258: 	append_value(append_info, supp->nation_code);
259: 	// s_phone
260: 	append_string(append_info, supp->phone);
261: 	// s_acctbal
262: 	append_decimal(append_info, supp->acctbal);
263: 	// s_comment
264: 	append_string(append_info, supp->comment);
265: 	append_info.appender->EndRow();
266: }
267: 
268: static void append_cust(customer_t *c, tpch_append_information *info) {
269: 	auto &append_info = info[CUST];
270: 
271: 	append_info.appender->BeginRow();
272: 	// c_custkey
273: 	append_value(append_info, c->custkey);
274: 	// c_name
275: 	append_string(append_info, c->name);
276: 	// c_address
277: 	append_string(append_info, c->address);
278: 	// c_nationkey
279: 	append_value(append_info, c->nation_code);
280: 	// c_phone
281: 	append_string(append_info, c->phone);
282: 	// c_acctbal
283: 	append_decimal(append_info, c->acctbal);
284: 	// c_mktsegment
285: 	append_string(append_info, c->mktsegment);
286: 	// c_comment
287: 	append_string(append_info, c->comment);
288: 	append_info.appender->EndRow();
289: }
290: 
291: static void append_part(part_t *part, tpch_append_information *info) {
292: 	auto &append_info = info[PART];
293: 
294: 	append_info.appender->BeginRow();
295: 	// p_partkey
296: 	append_value(append_info, part->partkey);
297: 	// p_name
298: 	append_string(append_info, part->name);
299: 	// p_mfgr
300: 	append_string(append_info, part->mfgr);
301: 	// p_brand
302: 	append_string(append_info, part->brand);
303: 	// p_type
304: 	append_string(append_info, part->type);
305: 	// p_size
306: 	append_value(append_info, part->size);
307: 	// p_container
308: 	append_string(append_info, part->container);
309: 	// p_retailprice
310: 	append_decimal(append_info, part->retailprice);
311: 	// p_comment
312: 	append_string(append_info, part->comment);
313: 	append_info.appender->EndRow();
314: }
315: 
316: static void append_psupp(part_t *part, tpch_append_information *info) {
317: 	auto &append_info = info[PSUPP];
318: 	for (size_t i = 0; i < SUPP_PER_PART; i++) {
319: 		append_info.appender->BeginRow();
320: 		// ps_partkey
321: 		append_value(append_info, part->s[i].partkey);
322: 		// ps_suppkey
323: 		append_value(append_info, part->s[i].suppkey);
324: 		// ps_availqty
325: 		append_value(append_info, part->s[i].qty);
326: 		// ps_supplycost
327: 		append_decimal(append_info, part->s[i].scost);
328: 		// ps_comment
329: 		append_string(append_info, part->s[i].comment);
330: 		append_info.appender->EndRow();
331: 	}
332: }
333: 
334: static void append_part_psupp(part_t *part, tpch_append_information *info) {
335: 	append_part(part, info);
336: 	append_psupp(part, info);
337: }
338: 
339: static void append_nation(code_t *c, tpch_append_information *info) {
340: 	auto &append_info = info[NATION];
341: 
342: 	append_info.appender->BeginRow();
343: 	// n_nationkey
344: 	append_value(append_info, c->code);
345: 	// n_name
346: 	append_string(append_info, c->text);
347: 	// n_regionkey
348: 	append_value(append_info, c->join);
349: 	// n_comment
350: 	append_string(append_info, c->comment);
351: 	append_info.appender->EndRow();
352: }
353: 
354: static void append_region(code_t *c, tpch_append_information *info) {
355: 	auto &append_info = info[REGION];
356: 
357: 	append_info.appender->BeginRow();
358: 	// r_regionkey
359: 	append_value(append_info, c->code);
360: 	// r_name
361: 	append_string(append_info, c->text);
362: 	// r_comment
363: 	append_string(append_info, c->comment);
364: 	append_info.appender->EndRow();
365: }
366: 
367: static void gen_tbl(int tnum, DSS_HUGE count, tpch_append_information *info) {
368: 	order_t o;
369: 	supplier_t supp;
370: 	customer_t cust;
371: 	part_t part;
372: 	code_t code;
373: 
374: 	for (DSS_HUGE i = 1; count; count--, i++) {
375: 		row_start(tnum);
376: 		switch (tnum) {
377: 		case LINE:
378: 		case ORDER:
379: 		case ORDER_LINE:
380: 			mk_order(i, &o, 0);
381: 			append_order_line(&o, info);
382: 			break;
383: 		case SUPP:
384: 			mk_supp(i, &supp);
385: 			append_supp(&supp, info);
386: 			break;
387: 		case CUST:
388: 			mk_cust(i, &cust);
389: 			append_cust(&cust, info);
390: 			break;
391: 		case PSUPP:
392: 		case PART:
393: 		case PART_PSUPP:
394: 			mk_part(i, &part);
395: 			append_part_psupp(&part, info);
396: 			break;
397: 		case NATION:
398: 			mk_nation(i, &code);
399: 			append_nation(&code, info);
400: 			break;
401: 		case REGION:
402: 			mk_region(i, &code);
403: 			append_region(&code, info);
404: 			break;
405: 		}
406: 		row_stop_h(tnum);
407: 	}
408: }
409: 
410: string get_table_name(int num) {
411: 	switch (num) {
412: 	case PART:
413: 		return "part";
414: 	case PSUPP:
415: 		return "partsupp";
416: 	case SUPP:
417: 		return "supplier";
418: 	case CUST:
419: 		return "customer";
420: 	case ORDER:
421: 		return "orders";
422: 	case LINE:
423: 		return "lineitem";
424: 	case NATION:
425: 		return "nation";
426: 	case REGION:
427: 		return "region";
428: 	default:
429: 		return "";
430: 	}
431: }
432: 
433: struct RegionInfo {
434: 	static constexpr char *Name = "region";
435: 	static constexpr idx_t ColumnCount = 3;
436: 	static const char *Columns[];
437: 	static const LogicalType Types[];
438: };
439: const char *RegionInfo::Columns[] = {"r_regionkey", "r_name", "r_comment"};
440: const LogicalType RegionInfo::Types[] = {LogicalType(LogicalTypeId::INTEGER), LogicalType(LogicalTypeId::VARCHAR),
441:                                          LogicalType(LogicalTypeId::VARCHAR)};
442: 
443: struct NationInfo {
444: 	static constexpr char *Name = "nation";
445: 	static const char *Columns[];
446: 	static constexpr idx_t ColumnCount = 4;
447: 	static const LogicalType Types[];
448: };
449: const char *NationInfo::Columns[] = {"n_nationkey", "n_name", "n_regionkey", "n_comment"};
450: const LogicalType NationInfo::Types[] = {LogicalType(LogicalTypeId::INTEGER), LogicalType(LogicalTypeId::VARCHAR),
451:                                          LogicalType(LogicalTypeId::INTEGER), LogicalType(LogicalTypeId::VARCHAR)};
452: 
453: struct SupplierInfo {
454: 	static constexpr char *Name = "supplier";
455: 	static const char *Columns[];
456: 	static constexpr idx_t ColumnCount = 7;
457: 	static const LogicalType Types[];
458: };
459: const char *SupplierInfo::Columns[] = {"s_suppkey", "s_name",    "s_address", "s_nationkey",
460:                                        "s_phone",   "s_acctbal", "s_comment"};
461: const LogicalType SupplierInfo::Types[] = {
462:     LogicalType(LogicalTypeId::INTEGER), LogicalType(LogicalTypeId::VARCHAR),
463:     LogicalType(LogicalTypeId::VARCHAR), LogicalType(LogicalTypeId::INTEGER),
464:     LogicalType(LogicalTypeId::VARCHAR), LogicalType(LogicalTypeId::DECIMAL, 15, 2),
465:     LogicalType(LogicalTypeId::VARCHAR)};
466: 
467: struct CustomerInfo {
468: 	static constexpr char *Name = "customer";
469: 	static const char *Columns[];
470: 	static constexpr idx_t ColumnCount = 8;
471: 	static const LogicalType Types[];
472: };
473: const char *CustomerInfo::Columns[] = {"c_custkey", "c_name",    "c_address",    "c_nationkey",
474:                                        "c_phone",   "c_acctbal", "c_mktsegment", "c_comment"};
475: const LogicalType CustomerInfo::Types[] = {
476:     LogicalType(LogicalTypeId::INTEGER), LogicalType(LogicalTypeId::VARCHAR),
477:     LogicalType(LogicalTypeId::VARCHAR), LogicalType(LogicalTypeId::INTEGER),
478:     LogicalType(LogicalTypeId::VARCHAR), LogicalType(LogicalTypeId::DECIMAL, 15, 2),
479:     LogicalType(LogicalTypeId::VARCHAR), LogicalType(LogicalTypeId::VARCHAR)};
480: 
481: struct PartInfo {
482: 	static constexpr char *Name = "part";
483: 	static const char *Columns[];
484: 	static constexpr idx_t ColumnCount = 9;
485: 	static const LogicalType Types[];
486: };
487: const char *PartInfo::Columns[] = {"p_partkey", "p_name",      "p_mfgr",        "p_brand",  "p_type",
488:                                    "p_size",    "p_container", "p_retailprice", "p_comment"};
489: const LogicalType PartInfo::Types[] = {LogicalType(LogicalTypeId::INTEGER), LogicalType(LogicalTypeId::VARCHAR),
490:                                        LogicalType(LogicalTypeId::VARCHAR), LogicalType(LogicalTypeId::VARCHAR),
491:                                        LogicalType(LogicalTypeId::VARCHAR), LogicalType(LogicalTypeId::INTEGER),
492:                                        LogicalType(LogicalTypeId::VARCHAR), LogicalType(LogicalTypeId::DECIMAL, 15, 2),
493:                                        LogicalType(LogicalTypeId::VARCHAR)};
494: 
495: struct PartsuppInfo {
496: 	static constexpr char *Name = "partsupp";
497: 	static const char *Columns[];
498: 	static constexpr idx_t ColumnCount = 5;
499: 	static const LogicalType Types[];
500: };
501: const char *PartsuppInfo::Columns[] = {"ps_partkey", "ps_suppkey", "ps_availqty", "ps_supplycost", "ps_comment"};
502: const LogicalType PartsuppInfo::Types[] = {
503:     LogicalType(LogicalTypeId::INTEGER), LogicalType(LogicalTypeId::INTEGER), LogicalType(LogicalTypeId::INTEGER),
504:     LogicalType(LogicalTypeId::DECIMAL, 15, 2), LogicalType(LogicalTypeId::VARCHAR)};
505: 
506: struct OrdersInfo {
507: 	static constexpr char *Name = "orders";
508: 	static const char *Columns[];
509: 	static constexpr idx_t ColumnCount = 9;
510: 	static const LogicalType Types[];
511: };
512: const char *OrdersInfo::Columns[] = {"o_orderkey",      "o_custkey", "o_orderstatus",  "o_totalprice", "o_orderdate",
513:                                      "o_orderpriority", "o_clerk",   "o_shippriority", "o_comment"};
514: const LogicalType OrdersInfo::Types[] = {
515:     LogicalType(LogicalTypeId::INTEGER), LogicalType(LogicalTypeId::INTEGER),
516:     LogicalType(LogicalTypeId::VARCHAR), LogicalType(LogicalTypeId::DECIMAL, 15, 2),
517:     LogicalType(LogicalTypeId::DATE),    LogicalType(LogicalTypeId::VARCHAR),
518:     LogicalType(LogicalTypeId::VARCHAR), LogicalType(LogicalTypeId::INTEGER),
519:     LogicalType(LogicalTypeId::VARCHAR)};
520: 
521: struct LineitemInfo {
522: 	static constexpr char *Name = "lineitem";
523: 	static const char *Columns[];
524: 	static constexpr idx_t ColumnCount = 16;
525: 	static const LogicalType Types[];
526: };
527: const char *LineitemInfo::Columns[] = {"l_orderkey",    "l_partkey",       "l_suppkey",  "l_linenumber",
528:                                        "l_quantity",    "l_extendedprice", "l_discount", "l_tax",
529:                                        "l_returnflag",  "l_linestatus",    "l_shipdate", "l_commitdate",
530:                                        "l_receiptdate", "l_shipinstruct",  "l_shipmode", "l_comment"};
531: const LogicalType LineitemInfo::Types[] = {
532:     LogicalType(LogicalTypeId::INTEGER),        LogicalType(LogicalTypeId::INTEGER),
533:     LogicalType(LogicalTypeId::INTEGER),        LogicalType(LogicalTypeId::INTEGER),
534:     LogicalType(LogicalTypeId::INTEGER),        LogicalType(LogicalTypeId::DECIMAL, 15, 2),
535:     LogicalType(LogicalTypeId::DECIMAL, 15, 2), LogicalType(LogicalTypeId::DECIMAL, 15, 2),
536:     LogicalType(LogicalTypeId::VARCHAR),        LogicalType(LogicalTypeId::VARCHAR),
537:     LogicalType(LogicalTypeId::DATE),           LogicalType(LogicalTypeId::DATE),
538:     LogicalType(LogicalTypeId::DATE),           LogicalType(LogicalTypeId::VARCHAR),
539:     LogicalType(LogicalTypeId::VARCHAR),        LogicalType(LogicalTypeId::VARCHAR)};
540: 
541: template <class T> static void CreateTPCHTable(ClientContext &context, string schema, string suffix) {
542: 	auto info = make_unique<CreateTableInfo>();
543: 	info->schema = schema;
544: 	info->table = T::Name + suffix;
545: 	info->on_conflict = OnCreateConflict::ERROR_ON_CONFLICT;
546: 	info->temporary = false;
547: 	for (idx_t i = 0; i < T::ColumnCount; i++) {
548: 		info->columns.push_back(ColumnDefinition(T::Columns[i], T::Types[i]));
549: 		info->constraints.push_back(make_unique<NotNullConstraint>(i));
550: 	}
551: 	Binder binder(context);
552: 	auto bound_info = binder.BindCreateTableInfo(move(info));
553: 	auto &catalog = Catalog::GetCatalog(context);
554: 
555: 	catalog.CreateTable(context, bound_info.get());
556: }
557: 
558: void DBGenWrapper::CreateTPCHSchema(ClientContext &context, string schema, string suffix) {
559: 	CreateTPCHTable<RegionInfo>(context, schema, suffix);
560: 	CreateTPCHTable<NationInfo>(context, schema, suffix);
561: 	CreateTPCHTable<SupplierInfo>(context, schema, suffix);
562: 	CreateTPCHTable<CustomerInfo>(context, schema, suffix);
563: 	CreateTPCHTable<PartInfo>(context, schema, suffix);
564: 	CreateTPCHTable<PartsuppInfo>(context, schema, suffix);
565: 	CreateTPCHTable<OrdersInfo>(context, schema, suffix);
566: 	CreateTPCHTable<LineitemInfo>(context, schema, suffix);
567: }
568: 
569: void DBGenWrapper::LoadTPCHData(ClientContext &context, double flt_scale, string schema, string suffix) {
570: 	if (flt_scale == 0) {
571: 		return;
572: 	}
573: 	std::lock_guard<std::mutex> l(dbgen_lock);
574: 	// generate the actual data
575: 	DSS_HUGE rowcnt = 0;
576: 	DSS_HUGE i;
577: 	// all tables
578: 	table = (1 << CUST) | (1 << SUPP) | (1 << NATION) | (1 << REGION) | (1 << PART_PSUPP) | (1 << ORDER_LINE);
579: 	force = 0;
580: 	insert_segments = 0;
581: 	delete_segments = 0;
582: 	insert_orders_segment = 0;
583: 	insert_lineitem_segment = 0;
584: 	delete_segment = 0;
585: 	verbose = 0;
586: 	set_seeds = 0;
587: 	scale = 1;
588: 	updates = 0;
589: 
590: 	// check if it is the first invocation
591: 	if (first_invocation) {
592: 		// store the initial random seed
593: 		memcpy(seed_backup, Seed, sizeof(seed_t) * MAX_STREAM + 1);
594: 		first_invocation = false;
595: 	} else {
596: 		// restore random seeds from backup
597: 		memcpy(Seed, seed_backup, sizeof(seed_t) * MAX_STREAM + 1);
598: 	}
599: 	tdefs[PART].base = 200000;
600: 	tdefs[PSUPP].base = 200000;
601: 	tdefs[SUPP].base = 10000;
602: 	tdefs[CUST].base = 150000;
603: 	tdefs[ORDER].base = 150000 * ORDERS_PER_CUST;
604: 	tdefs[LINE].base = 150000 * ORDERS_PER_CUST;
605: 	tdefs[ORDER_LINE].base = 150000 * ORDERS_PER_CUST;
606: 	tdefs[PART_PSUPP].base = 200000;
607: 	tdefs[NATION].base = NATIONS_MAX;
608: 	tdefs[REGION].base = NATIONS_MAX;
609: 
610: 	children = 1;
611: 	d_path = NULL;
612: 
613: 	if (flt_scale < MIN_SCALE) {
614: 		int i;
615: 		int int_scale;
616: 
617: 		scale = 1;
618: 		int_scale = (int)(1000 * flt_scale);
619: 		for (i = PART; i < REGION; i++) {
620: 			tdefs[i].base = (DSS_HUGE)(int_scale * tdefs[i].base) / 1000;
621: 			if (tdefs[i].base < 1) {
622: 				tdefs[i].base = 1;
623: 			}
624: 		}
625: 	} else {
626: 		scale = (long)flt_scale;
627: 	}
628: 
629: 	load_dists();
630: 
631: 	/* have to do this after init */
632: 	tdefs[NATION].base = nations.count;
633: 	tdefs[REGION].base = regions.count;
634: 
635: 	auto &catalog = Catalog::GetCatalog(context);
636: 
637: 	auto append_info = unique_ptr<tpch_append_information[]>(new tpch_append_information[REGION + 1]);
638: 	memset(append_info.get(), 0, sizeof(tpch_append_information) * REGION + 1);
639: 	for (size_t i = PART; i <= REGION; i++) {
640: 		auto tname = get_table_name(i);
641: 		if (!tname.empty()) {
642: 			string full_tname = string(tname) + string(suffix);
643: 			auto tbl_catalog = catalog.GetEntry<TableCatalogEntry>(context, schema, full_tname);
644: 			append_info[i].appender = make_unique<UnsafeAppender>(context, tbl_catalog);
645: 		}
646: 	}
647: 
648: 	for (i = PART; i <= REGION; i++) {
649: 		if (table & (1 << i)) {
650: 			if (i < NATION) {
651: 				rowcnt = tdefs[i].base * scale;
652: 			} else {
653: 				rowcnt = tdefs[i].base;
654: 			}
655: 			// actually doing something
656: 			gen_tbl((int)i, rowcnt, append_info.get());
657: 		}
658: 	}
659: 	// flush any incomplete chunks
660: 	for (size_t i = PART; i <= REGION; i++) {
661: 		if (append_info[i].appender) {
662: 			append_info[i].appender->Flush();
663: 			append_info[i].appender.reset();
664: 		}
665: 	}
666: 
667: 	cleanup_dists();
668: }
669: 
670: string DBGenWrapper::GetQuery(int query) {
671: 	if (query <= 0 || query > TPCH_QUERIES_COUNT) {
672: 		throw SyntaxException("Out of range TPC-H query number %d", query);
673: 	}
674: 	return TPCH_QUERIES[query - 1];
675: }
676: 
677: string DBGenWrapper::GetAnswer(double sf, int query) {
678: 	if (query <= 0 || query > TPCH_QUERIES_COUNT) {
679: 		throw SyntaxException("Out of range TPC-H query number %d", query);
680: 	}
681: 	const char *answer;
682: 	if (sf == 0.01) {
683: 		answer = TPCH_ANSWERS_SF0_01[query - 1];
684: 	} else if (sf == 0.1) {
685: 		answer = TPCH_ANSWERS_SF0_1[query - 1];
686: 	} else if (sf == 1) {
687: 		answer = TPCH_ANSWERS_SF1[query - 1];
688: 	} else {
689: 		throw NotImplementedException("Don't have TPC-H answers for SF %llf!", sf);
690: 	}
691: 	return answer;
692: }
693: 
694: } // namespace tpch
[end of extension/tpch/dbgen/dbgen.cpp]
[start of src/catalog/catalog_entry/table_catalog_entry.cpp]
1: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/serializer.hpp"
7: #include "duckdb/main/connection.hpp"
8: #include "duckdb/main/database.hpp"
9: #include "duckdb/parser/constraints/list.hpp"
10: #include "duckdb/parser/parsed_data/alter_table_info.hpp"
11: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
12: #include "duckdb/planner/constraints/bound_unique_constraint.hpp"
13: #include "duckdb/planner/constraints/bound_check_constraint.hpp"
14: #include "duckdb/planner/expression/bound_constant_expression.hpp"
15: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
16: #include "duckdb/storage/storage_manager.hpp"
17: #include "duckdb/planner/binder.hpp"
18: 
19: #include "duckdb/execution/index/art/art.hpp"
20: #include "duckdb/parser/expression/columnref_expression.hpp"
21: #include "duckdb/planner/expression/bound_reference_expression.hpp"
22: #include "duckdb/parser/parsed_expression_iterator.hpp"
23: #include "duckdb/planner/expression_binder/alter_binder.hpp"
24: #include "duckdb/parser/keyword_helper.hpp"
25: 
26: #include <algorithm>
27: #include <sstream>
28: 
29: namespace duckdb {
30: 
31: void TableCatalogEntry::AddLowerCaseAliases(unordered_map<string, column_t> &name_map) {
32: 	unordered_map<string, column_t> extra_lowercase_names;
33: 	for (auto &entry : name_map) {
34: 		auto lcase = StringUtil::Lower(entry.first);
35: 		// check the lowercase name map if there already exists a lowercase version
36: 		if (extra_lowercase_names.find(lcase) == extra_lowercase_names.end()) {
37: 			// not yet: add the mapping
38: 			extra_lowercase_names[lcase] = entry.second;
39: 		} else {
40: 			// the lowercase already exists: set it to invalid index
41: 			extra_lowercase_names[lcase] = INVALID_INDEX;
42: 		}
43: 	}
44: 	// for any new lowercase names, add them to the original name map
45: 	for (auto &entry : extra_lowercase_names) {
46: 		if (entry.second != INVALID_INDEX) {
47: 			name_map[entry.first] = entry.second;
48: 		}
49: 	}
50: }
51: 
52: TableCatalogEntry::TableCatalogEntry(Catalog *catalog, SchemaCatalogEntry *schema, BoundCreateTableInfo *info,
53:                                      std::shared_ptr<DataTable> inherited_storage)
54:     : StandardEntry(CatalogType::TABLE_ENTRY, schema, catalog, info->Base().table), storage(move(inherited_storage)),
55:       columns(move(info->Base().columns)), constraints(move(info->Base().constraints)),
56:       bound_constraints(move(info->bound_constraints)), name_map(info->name_map) {
57: 	this->temporary = info->Base().temporary;
58: 	// add lower case aliases
59: 	AddLowerCaseAliases(name_map);
60: 	// add the "rowid" alias, if there is no rowid column specified in the table
61: 	if (name_map.find("rowid") == name_map.end()) {
62: 		name_map["rowid"] = COLUMN_IDENTIFIER_ROW_ID;
63: 	}
64: 	if (!storage) {
65: 		// create the physical storage
66: 		storage = make_shared<DataTable>(catalog->db, schema->name, name, GetTypes(), move(info->data));
67: 
68: 		// create the unique indexes for the UNIQUE and PRIMARY KEY constraints
69: 		for (idx_t i = 0; i < bound_constraints.size(); i++) {
70: 			auto &constraint = bound_constraints[i];
71: 			if (constraint->type == ConstraintType::UNIQUE) {
72: 				// unique constraint: create a unique index
73: 				auto &unique = (BoundUniqueConstraint &)*constraint;
74: 				// fetch types and create expressions for the index from the columns
75: 				vector<column_t> column_ids;
76: 				vector<unique_ptr<Expression>> unbound_expressions;
77: 				vector<unique_ptr<Expression>> bound_expressions;
78: 				idx_t key_nr = 0;
79: 				for (auto &key : unique.keys) {
80: 					D_ASSERT(key < columns.size());
81: 
82: 					unbound_expressions.push_back(make_unique<BoundColumnRefExpression>(
83: 					    columns[key].name, columns[key].type, ColumnBinding(0, column_ids.size())));
84: 
85: 					bound_expressions.push_back(make_unique<BoundReferenceExpression>(columns[key].type, key_nr++));
86: 					column_ids.push_back(key);
87: 				}
88: 				// create an adaptive radix tree around the expressions
89: 				auto art = make_unique<ART>(column_ids, move(unbound_expressions), true);
90: 				storage->AddIndex(move(art), bound_expressions);
91: 			}
92: 		}
93: 	}
94: }
95: 
96: bool TableCatalogEntry::ColumnExists(const string &name) {
97: 	return name_map.find(name) != name_map.end();
98: }
99: 
100: unique_ptr<CatalogEntry> TableCatalogEntry::AlterEntry(ClientContext &context, AlterInfo *info) {
101: 	D_ASSERT(!internal);
102: 	if (info->type != AlterType::ALTER_TABLE) {
103: 		throw CatalogException("Can only modify table with ALTER TABLE statement");
104: 	}
105: 	auto table_info = (AlterTableInfo *)info;
106: 	switch (table_info->alter_table_type) {
107: 	case AlterTableType::RENAME_COLUMN: {
108: 		auto rename_info = (RenameColumnInfo *)table_info;
109: 		return RenameColumn(context, *rename_info);
110: 	}
111: 	case AlterTableType::RENAME_TABLE: {
112: 		auto rename_info = (RenameTableInfo *)table_info;
113: 		auto copied_table = Copy(context);
114: 		copied_table->name = rename_info->new_table_name;
115: 		return copied_table;
116: 	}
117: 	case AlterTableType::ADD_COLUMN: {
118: 		auto add_info = (AddColumnInfo *)table_info;
119: 		return AddColumn(context, *add_info);
120: 	}
121: 	case AlterTableType::REMOVE_COLUMN: {
122: 		auto remove_info = (RemoveColumnInfo *)table_info;
123: 		return RemoveColumn(context, *remove_info);
124: 	}
125: 	case AlterTableType::SET_DEFAULT: {
126: 		auto set_default_info = (SetDefaultInfo *)table_info;
127: 		return SetDefault(context, *set_default_info);
128: 	}
129: 	case AlterTableType::ALTER_COLUMN_TYPE: {
130: 		auto change_type_info = (ChangeColumnTypeInfo *)table_info;
131: 		return ChangeColumnType(context, *change_type_info);
132: 	}
133: 	default:
134: 		throw InternalException("Unrecognized alter table type!");
135: 	}
136: }
137: 
138: static void RenameExpression(ParsedExpression &expr, RenameColumnInfo &info) {
139: 	if (expr.type == ExpressionType::COLUMN_REF) {
140: 		auto &colref = (ColumnRefExpression &)expr;
141: 		if (colref.column_name == info.old_name) {
142: 			colref.column_name = info.new_name;
143: 		}
144: 	}
145: 	ParsedExpressionIterator::EnumerateChildren(
146: 	    expr, [&](const ParsedExpression &child) { RenameExpression((ParsedExpression &)child, info); });
147: }
148: 
149: unique_ptr<CatalogEntry> TableCatalogEntry::RenameColumn(ClientContext &context, RenameColumnInfo &info) {
150: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
151: 	create_info->temporary = temporary;
152: 	bool found = false;
153: 	for (idx_t i = 0; i < columns.size(); i++) {
154: 		ColumnDefinition copy = columns[i].Copy();
155: 
156: 		create_info->columns.push_back(move(copy));
157: 		if (info.old_name == columns[i].name) {
158: 			D_ASSERT(!found);
159: 			create_info->columns[i].name = info.new_name;
160: 			found = true;
161: 		}
162: 	}
163: 	if (!found) {
164: 		throw CatalogException("Table does not have a column with name \"%s\"", info.name);
165: 	}
166: 	for (idx_t c_idx = 0; c_idx < constraints.size(); c_idx++) {
167: 		auto copy = constraints[c_idx]->Copy();
168: 		switch (copy->type) {
169: 		case ConstraintType::NOT_NULL:
170: 			// NOT NULL constraint: no adjustments necessary
171: 			break;
172: 		case ConstraintType::CHECK: {
173: 			// CHECK constraint: need to rename column references that refer to the renamed column
174: 			auto &check = (CheckConstraint &)*copy;
175: 			RenameExpression(*check.expression, info);
176: 			break;
177: 		}
178: 		case ConstraintType::UNIQUE: {
179: 			// UNIQUE constraint: possibly need to rename columns
180: 			auto &unique = (UniqueConstraint &)*copy;
181: 			for (idx_t i = 0; i < unique.columns.size(); i++) {
182: 				if (unique.columns[i] == info.old_name) {
183: 					unique.columns[i] = info.new_name;
184: 				}
185: 			}
186: 			break;
187: 		}
188: 		default:
189: 			throw CatalogException("Unsupported constraint for entry!");
190: 		}
191: 		create_info->constraints.push_back(move(copy));
192: 	}
193: 	Binder binder(context);
194: 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
195: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
196: }
197: 
198: unique_ptr<CatalogEntry> TableCatalogEntry::AddColumn(ClientContext &context, AddColumnInfo &info) {
199: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
200: 	create_info->temporary = temporary;
201: 	for (idx_t i = 0; i < columns.size(); i++) {
202: 		create_info->columns.push_back(columns[i].Copy());
203: 	}
204: 	info.new_column.oid = columns.size();
205: 	create_info->columns.push_back(info.new_column.Copy());
206: 
207: 	Binder binder(context);
208: 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
209: 	auto new_storage =
210: 	    make_shared<DataTable>(context, *storage, info.new_column, bound_create_info->bound_defaults.back().get());
211: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
212: 	                                      new_storage);
213: }
214: 
215: unique_ptr<CatalogEntry> TableCatalogEntry::RemoveColumn(ClientContext &context, RemoveColumnInfo &info) {
216: 	idx_t removed_index = INVALID_INDEX;
217: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
218: 	create_info->temporary = temporary;
219: 	for (idx_t i = 0; i < columns.size(); i++) {
220: 		if (columns[i].name == info.removed_column) {
221: 			D_ASSERT(removed_index == INVALID_INDEX);
222: 			removed_index = i;
223: 			continue;
224: 		}
225: 		create_info->columns.push_back(columns[i].Copy());
226: 	}
227: 	if (removed_index == INVALID_INDEX) {
228: 		if (!info.if_exists) {
229: 			throw CatalogException("Table does not have a column with name \"%s\"", info.removed_column);
230: 		}
231: 		return nullptr;
232: 	}
233: 	if (create_info->columns.empty()) {
234: 		throw CatalogException("Cannot drop column: table only has one column remaining!");
235: 	}
236: 	// handle constraints for the new table
237: 	D_ASSERT(constraints.size() == bound_constraints.size());
238: 	for (idx_t constr_idx = 0; constr_idx < constraints.size(); constr_idx++) {
239: 		auto &constraint = constraints[constr_idx];
240: 		auto &bound_constraint = bound_constraints[constr_idx];
241: 		switch (bound_constraint->type) {
242: 		case ConstraintType::NOT_NULL: {
243: 			auto &not_null_constraint = (BoundNotNullConstraint &)*bound_constraint;
244: 			if (not_null_constraint.index != removed_index) {
245: 				// the constraint is not about this column: we need to copy it
246: 				// we might need to shift the index back by one though, to account for the removed column
247: 				idx_t new_index = not_null_constraint.index;
248: 				if (not_null_constraint.index > removed_index) {
249: 					new_index -= 1;
250: 				}
251: 				create_info->constraints.push_back(make_unique<NotNullConstraint>(new_index));
252: 			}
253: 			break;
254: 		}
255: 		case ConstraintType::CHECK: {
256: 			// CHECK constraint
257: 			auto &bound_check = (BoundCheckConstraint &)*bound_constraint;
258: 			// check if the removed column is part of the check constraint
259: 			if (bound_check.bound_columns.find(removed_index) != bound_check.bound_columns.end()) {
260: 				if (bound_check.bound_columns.size() > 1) {
261: 					// CHECK constraint that concerns mult
262: 					throw CatalogException(
263: 					    "Cannot drop column \"%s\" because there is a CHECK constraint that depends on it",
264: 					    info.removed_column);
265: 				} else {
266: 					// CHECK constraint that ONLY concerns this column, strip the constraint
267: 				}
268: 			} else {
269: 				// check constraint does not concern the removed column: simply re-add it
270: 				create_info->constraints.push_back(constraint->Copy());
271: 			}
272: 			break;
273: 		}
274: 		case ConstraintType::UNIQUE: {
275: 			auto copy = constraint->Copy();
276: 			auto &unique = (UniqueConstraint &)*copy;
277: 			if (unique.index != INVALID_INDEX) {
278: 				if (unique.index == removed_index) {
279: 					throw CatalogException(
280: 					    "Cannot drop column \"%s\" because there is a UNIQUE constraint that depends on it",
281: 					    info.removed_column);
282: 				} else if (unique.index > removed_index) {
283: 					unique.index--;
284: 				}
285: 			}
286: 			create_info->constraints.push_back(move(copy));
287: 			break;
288: 		}
289: 		default:
290: 			throw InternalException("Unsupported constraint for entry!");
291: 		}
292: 	}
293: 
294: 	Binder binder(context);
295: 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
296: 	auto new_storage = make_shared<DataTable>(context, *storage, removed_index);
297: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
298: 	                                      new_storage);
299: }
300: 
301: unique_ptr<CatalogEntry> TableCatalogEntry::SetDefault(ClientContext &context, SetDefaultInfo &info) {
302: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
303: 	bool found = false;
304: 	for (idx_t i = 0; i < columns.size(); i++) {
305: 		auto copy = columns[i].Copy();
306: 		if (info.column_name == copy.name) {
307: 			// set the default value of this column
308: 			copy.default_value = info.expression ? info.expression->Copy() : nullptr;
309: 			found = true;
310: 		}
311: 		create_info->columns.push_back(move(copy));
312: 	}
313: 	if (!found) {
314: 		throw BinderException("Table \"%s\" does not have a column with name \"%s\"", info.name, info.column_name);
315: 	}
316: 
317: 	for (idx_t i = 0; i < constraints.size(); i++) {
318: 		auto constraint = constraints[i]->Copy();
319: 		create_info->constraints.push_back(move(constraint));
320: 	}
321: 
322: 	Binder binder(context);
323: 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
324: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
325: }
326: 
327: unique_ptr<CatalogEntry> TableCatalogEntry::ChangeColumnType(ClientContext &context, ChangeColumnTypeInfo &info) {
328: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
329: 	idx_t change_idx = INVALID_INDEX;
330: 	for (idx_t i = 0; i < columns.size(); i++) {
331: 		auto copy = columns[i].Copy();
332: 		if (info.column_name == copy.name) {
333: 			// set the default value of this column
334: 			change_idx = i;
335: 			copy.type = info.target_type;
336: 		}
337: 		create_info->columns.push_back(move(copy));
338: 	}
339: 	if (change_idx == INVALID_INDEX) {
340: 		throw BinderException("Table \"%s\" does not have a column with name \"%s\"", info.name, info.column_name);
341: 	}
342: 
343: 	for (idx_t i = 0; i < constraints.size(); i++) {
344: 		auto constraint = constraints[i]->Copy();
345: 		switch (constraint->type) {
346: 		case ConstraintType::CHECK: {
347: 			auto &bound_check = (BoundCheckConstraint &)*bound_constraints[i];
348: 			if (bound_check.bound_columns.find(change_idx) != bound_check.bound_columns.end()) {
349: 				throw BinderException("Cannot change the type of a column that has a CHECK constraint specified");
350: 			}
351: 			break;
352: 		}
353: 		case ConstraintType::NOT_NULL:
354: 			break;
355: 		case ConstraintType::UNIQUE: {
356: 			auto &bound_unique = (BoundUniqueConstraint &)*bound_constraints[i];
357: 			if (bound_unique.keys.find(change_idx) != bound_unique.keys.end()) {
358: 				throw BinderException(
359: 				    "Cannot change the type of a column that has a UNIQUE or PRIMARY KEY constraint specified");
360: 			}
361: 			break;
362: 		}
363: 		default:
364: 			throw InternalException("Unsupported constraint for entry!");
365: 		}
366: 		create_info->constraints.push_back(move(constraint));
367: 	}
368: 
369: 	Binder binder(context);
370: 	// bind the specified expression
371: 	vector<column_t> bound_columns;
372: 	AlterBinder expr_binder(binder, context, name, columns, bound_columns, info.target_type);
373: 	auto expression = info.expression->Copy();
374: 	auto bound_expression = expr_binder.Bind(expression);
375: 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
376: 	if (bound_columns.empty()) {
377: 		bound_columns.push_back(COLUMN_IDENTIFIER_ROW_ID);
378: 	}
379: 
380: 	auto new_storage =
381: 	    make_shared<DataTable>(context, *storage, change_idx, info.target_type, move(bound_columns), *bound_expression);
382: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
383: 	                                      new_storage);
384: }
385: 
386: ColumnDefinition &TableCatalogEntry::GetColumn(const string &name) {
387: 	auto entry = name_map.find(name);
388: 	if (entry == name_map.end() || entry->second == COLUMN_IDENTIFIER_ROW_ID) {
389: 		throw CatalogException("Column with name %s does not exist!", name);
390: 	}
391: 	return columns[entry->second];
392: }
393: 
394: vector<LogicalType> TableCatalogEntry::GetTypes() {
395: 	vector<LogicalType> types;
396: 	for (auto &it : columns) {
397: 		types.push_back(it.type);
398: 	}
399: 	return types;
400: }
401: 
402: vector<LogicalType> TableCatalogEntry::GetTypes(const vector<column_t> &column_ids) {
403: 	vector<LogicalType> result;
404: 	for (auto &index : column_ids) {
405: 		if (index == COLUMN_IDENTIFIER_ROW_ID) {
406: 			result.push_back(LOGICAL_ROW_TYPE);
407: 		} else {
408: 			result.push_back(columns[index].type);
409: 		}
410: 	}
411: 	return result;
412: }
413: 
414: void TableCatalogEntry::Serialize(Serializer &serializer) {
415: 	serializer.WriteString(schema->name);
416: 	serializer.WriteString(name);
417: 	D_ASSERT(columns.size() <= NumericLimits<uint32_t>::Maximum());
418: 	serializer.Write<uint32_t>((uint32_t)columns.size());
419: 	for (auto &column : columns) {
420: 		column.Serialize(serializer);
421: 	}
422: 	D_ASSERT(constraints.size() <= NumericLimits<uint32_t>::Maximum());
423: 	serializer.Write<uint32_t>((uint32_t)constraints.size());
424: 	for (auto &constraint : constraints) {
425: 		constraint->Serialize(serializer);
426: 	}
427: }
428: 
429: string TableCatalogEntry::ToSQL() {
430: 	std::stringstream ss;
431: 	ss << "CREATE TABLE " << KeywordHelper::WriteOptionallyQuoted(name) << "(";
432: 
433: 	// find all columns that have NOT NULL specified, but are NOT primary key columns
434: 	unordered_set<idx_t> not_null_columns;
435: 	unordered_set<idx_t> unique_columns;
436: 	unordered_set<idx_t> pk_columns;
437: 	unordered_set<string> multi_key_pks;
438: 	vector<string> extra_constraints;
439: 	for (auto &constraint : constraints) {
440: 		if (constraint->type == ConstraintType::NOT_NULL) {
441: 			auto &not_null = (NotNullConstraint &)*constraint;
442: 			not_null_columns.insert(not_null.index);
443: 		} else if (constraint->type == ConstraintType::UNIQUE) {
444: 			auto &pk = (UniqueConstraint &)*constraint;
445: 			vector<string> constraint_columns = pk.columns;
446: 			if (pk.columns.empty()) {
447: 				// no columns specified: single column constraint
448: 				if (pk.is_primary_key) {
449: 					pk_columns.insert(pk.index);
450: 				} else {
451: 					unique_columns.insert(pk.index);
452: 				}
453: 			} else {
454: 				// multi-column constraint, this constraint needs to go at the end after all columns
455: 				if (pk.is_primary_key) {
456: 					// multi key pk column: insert set of columns into multi_key_pks
457: 					for (auto &col : pk.columns) {
458: 						multi_key_pks.insert(col);
459: 					}
460: 				}
461: 				extra_constraints.push_back(constraint->ToString());
462: 			}
463: 		} else {
464: 			extra_constraints.push_back(constraint->ToString());
465: 		}
466: 	}
467: 
468: 	for (idx_t i = 0; i < columns.size(); i++) {
469: 		if (i > 0) {
470: 			ss << ", ";
471: 		}
472: 		auto &column = columns[i];
473: 		ss << KeywordHelper::WriteOptionallyQuoted(column.name) << " " << column.type.ToString();
474: 		bool not_null = not_null_columns.find(column.oid) != not_null_columns.end();
475: 		bool is_single_key_pk = pk_columns.find(column.oid) != pk_columns.end();
476: 		bool is_multi_key_pk = multi_key_pks.find(column.name) != multi_key_pks.end();
477: 		bool is_unique = unique_columns.find(column.oid) != unique_columns.end();
478: 		if (not_null && !is_single_key_pk && !is_multi_key_pk) {
479: 			// NOT NULL but not a primary key column
480: 			ss << " NOT NULL";
481: 		}
482: 		if (is_single_key_pk) {
483: 			// single column pk: insert constraint here
484: 			ss << " PRIMARY KEY";
485: 		}
486: 		if (is_unique) {
487: 			// single column unique: insert constraint here
488: 			ss << " UNIQUE";
489: 		}
490: 		if (column.default_value) {
491: 			ss << " DEFAULT(" << column.default_value->ToString() << ")";
492: 		}
493: 	}
494: 	// print any extra constraints that still need to be printed
495: 	for (auto &extra_constraint : extra_constraints) {
496: 		ss << ", ";
497: 		ss << extra_constraint;
498: 	}
499: 
500: 	ss << ");";
501: 	return ss.str();
502: }
503: 
504: unique_ptr<CreateTableInfo> TableCatalogEntry::Deserialize(Deserializer &source) {
505: 	auto info = make_unique<CreateTableInfo>();
506: 
507: 	info->schema = source.Read<string>();
508: 	info->table = source.Read<string>();
509: 	auto column_count = source.Read<uint32_t>();
510: 
511: 	for (uint32_t i = 0; i < column_count; i++) {
512: 		auto column = ColumnDefinition::Deserialize(source);
513: 		info->columns.push_back(move(column));
514: 	}
515: 	auto constraint_count = source.Read<uint32_t>();
516: 
517: 	for (uint32_t i = 0; i < constraint_count; i++) {
518: 		auto constraint = Constraint::Deserialize(source);
519: 		info->constraints.push_back(move(constraint));
520: 	}
521: 	return info;
522: }
523: 
524: unique_ptr<CatalogEntry> TableCatalogEntry::Copy(ClientContext &context) {
525: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
526: 	for (idx_t i = 0; i < columns.size(); i++) {
527: 		create_info->columns.push_back(columns[i].Copy());
528: 	}
529: 
530: 	for (idx_t i = 0; i < constraints.size(); i++) {
531: 		auto constraint = constraints[i]->Copy();
532: 		create_info->constraints.push_back(move(constraint));
533: 	}
534: 
535: 	Binder binder(context);
536: 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
537: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
538: }
539: 
540: void TableCatalogEntry::SetAsRoot() {
541: 	storage->SetAsRoot();
542: }
543: 
544: void TableCatalogEntry::CommitAlter(AlterInfo &info) {
545: 	D_ASSERT(info.type == AlterType::ALTER_TABLE);
546: 	auto &alter_table = (AlterTableInfo &)info;
547: 	string column_name;
548: 	switch (alter_table.alter_table_type) {
549: 	case AlterTableType::REMOVE_COLUMN: {
550: 		auto &remove_info = (RemoveColumnInfo &)alter_table;
551: 		column_name = remove_info.removed_column;
552: 		break;
553: 	}
554: 	case AlterTableType::ALTER_COLUMN_TYPE: {
555: 		auto &change_info = (ChangeColumnTypeInfo &)alter_table;
556: 		column_name = change_info.column_name;
557: 		break;
558: 	}
559: 	default:
560: 		break;
561: 	}
562: 	if (column_name.empty()) {
563: 		return;
564: 	}
565: 	idx_t removed_index = INVALID_INDEX;
566: 	for (idx_t i = 0; i < columns.size(); i++) {
567: 		if (columns[i].name == column_name) {
568: 			D_ASSERT(removed_index == INVALID_INDEX);
569: 			removed_index = i;
570: 			continue;
571: 		}
572: 	}
573: 	D_ASSERT(removed_index != INVALID_INDEX);
574: 	storage->CommitDropColumn(removed_index);
575: }
576: 
577: void TableCatalogEntry::CommitDrop() {
578: 	storage->CommitDropTable();
579: }
580: 
581: } // namespace duckdb
[end of src/catalog/catalog_entry/table_catalog_entry.cpp]
[start of src/catalog/default/default_views.cpp]
1: #include "duckdb/catalog/default/default_views.hpp"
2: #include "duckdb/parser/parser.hpp"
3: #include "duckdb/parser/statement/select_statement.hpp"
4: #include "duckdb/planner/binder.hpp"
5: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
6: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
7: 
8: namespace duckdb {
9: 
10: struct DefaultView {
11: 	const char *schema;
12: 	const char *name;
13: 	const char *sql;
14: };
15: 
16: static DefaultView internal_views[] = {
17:     {DEFAULT_SCHEMA, "pragma_database_list", "SELECT * FROM pragma_database_list()"},
18:     {DEFAULT_SCHEMA, "sqlite_master", "SELECT * FROM sqlite_master()"},
19:     {DEFAULT_SCHEMA, "sqlite_schema", "SELECT * FROM sqlite_master()"},
20:     {DEFAULT_SCHEMA, "sqlite_temp_master", "SELECT * FROM sqlite_master()"},
21:     {DEFAULT_SCHEMA, "sqlite_temp_schema", "SELECT * FROM sqlite_master()"},
22:     {"information_schema", "columns", "SELECT * FROM information_schema_columns()"},
23:     {"information_schema", "schemata", "SELECT * FROM information_schema_schemata()"},
24:     {"information_schema", "tables", "SELECT * FROM information_schema_tables()"},
25:     {nullptr, nullptr, nullptr}};
26: 
27: static unique_ptr<CreateViewInfo> GetDefaultView(const string &schema, const string &name) {
28: 	for (idx_t index = 0; internal_views[index].name != nullptr; index++) {
29: 		if (internal_views[index].schema == schema && internal_views[index].name == name) {
30: 			auto result = make_unique<CreateViewInfo>();
31: 			result->schema = schema;
32: 			result->sql = internal_views[index].sql;
33: 
34: 			Parser parser;
35: 			parser.ParseQuery(internal_views[index].sql);
36: 			D_ASSERT(parser.statements.size() == 1 && parser.statements[0]->type == StatementType::SELECT_STATEMENT);
37: 			result->query = unique_ptr_cast<SQLStatement, SelectStatement>(move(parser.statements[0]));
38: 			result->temporary = true;
39: 			result->internal = true;
40: 			result->view_name = name;
41: 			return result;
42: 		}
43: 	}
44: 	return nullptr;
45: }
46: 
47: DefaultViewGenerator::DefaultViewGenerator(Catalog &catalog, SchemaCatalogEntry *schema)
48:     : DefaultGenerator(catalog), schema(schema) {
49: }
50: 
51: unique_ptr<CatalogEntry> DefaultViewGenerator::CreateDefaultEntry(ClientContext &context, const string &entry_name) {
52: 	auto info = GetDefaultView(schema->name, entry_name);
53: 	if (info) {
54: 		Binder binder(context);
55: 		binder.BindCreateViewInfo(*info);
56: 
57: 		return make_unique_base<CatalogEntry, ViewCatalogEntry>(&catalog, schema, info.get());
58: 	}
59: 	return nullptr;
60: }
61: 
62: } // namespace duckdb
[end of src/catalog/default/default_views.cpp]
[start of src/include/duckdb/common/types.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/types.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/assert.hpp"
12: #include "duckdb/common/constants.hpp"
13: #include "duckdb/common/single_thread_ptr.hpp"
14: #include "duckdb/common/vector.hpp"
15: 
16: namespace duckdb {
17: 
18: class Serializer;
19: class Deserializer;
20: 
21: struct interval_t {
22: 	int32_t months;
23: 	int32_t days;
24: 	int64_t micros;
25: };
26: 
27: struct hugeint_t {
28: public:
29: 	uint64_t lower;
30: 	int64_t upper;
31: 
32: public:
33: 	hugeint_t() = default;
34: 	hugeint_t(int64_t value); // NOLINT: Allow implicit conversion from `int64_t`
35: 	hugeint_t(const hugeint_t &rhs) = default;
36: 	hugeint_t(hugeint_t &&rhs) = default;
37: 	hugeint_t &operator=(const hugeint_t &rhs) = default;
38: 	hugeint_t &operator=(hugeint_t &&rhs) = default;
39: 
40: 	string ToString() const;
41: 
42: 	// comparison operators
43: 	bool operator==(const hugeint_t &rhs) const;
44: 	bool operator!=(const hugeint_t &rhs) const;
45: 	bool operator<=(const hugeint_t &rhs) const;
46: 	bool operator<(const hugeint_t &rhs) const;
47: 	bool operator>(const hugeint_t &rhs) const;
48: 	bool operator>=(const hugeint_t &rhs) const;
49: 
50: 	// arithmetic operators
51: 	hugeint_t operator+(const hugeint_t &rhs) const;
52: 	hugeint_t operator-(const hugeint_t &rhs) const;
53: 	hugeint_t operator*(const hugeint_t &rhs) const;
54: 	hugeint_t operator/(const hugeint_t &rhs) const;
55: 	hugeint_t operator%(const hugeint_t &rhs) const;
56: 	hugeint_t operator-() const;
57: 
58: 	// bitwise operators
59: 	hugeint_t operator>>(const hugeint_t &rhs) const;
60: 	hugeint_t operator<<(const hugeint_t &rhs) const;
61: 	hugeint_t operator&(const hugeint_t &rhs) const;
62: 	hugeint_t operator|(const hugeint_t &rhs) const;
63: 	hugeint_t operator^(const hugeint_t &rhs) const;
64: 	hugeint_t operator~() const;
65: 
66: 	// in-place operators
67: 	hugeint_t &operator+=(const hugeint_t &rhs);
68: 	hugeint_t &operator-=(const hugeint_t &rhs);
69: 	hugeint_t &operator*=(const hugeint_t &rhs);
70: 	hugeint_t &operator/=(const hugeint_t &rhs);
71: 	hugeint_t &operator%=(const hugeint_t &rhs);
72: 	hugeint_t &operator>>=(const hugeint_t &rhs);
73: 	hugeint_t &operator<<=(const hugeint_t &rhs);
74: 	hugeint_t &operator&=(const hugeint_t &rhs);
75: 	hugeint_t &operator|=(const hugeint_t &rhs);
76: 	hugeint_t &operator^=(const hugeint_t &rhs);
77: };
78: 
79: struct string_t;
80: 
81: template <class T>
82: using child_list_t = std::vector<std::pair<std::string, T>>;
83: template <class T>
84: using buffer_ptr = single_thread_ptr<T>;
85: 
86: template <class T, typename... Args>
87: buffer_ptr<T> make_buffer(Args &&... args) {
88: 	return single_thread_make_shared<T>(std::forward<Args>(args)...);
89: }
90: 
91: struct list_entry_t {
92: 	list_entry_t() = default;
93: 	list_entry_t(uint64_t offset, uint64_t length) : offset(offset), length(length) {
94: 	}
95: 
96: 	uint64_t offset;
97: 	uint64_t length;
98: };
99: 
100: //===--------------------------------------------------------------------===//
101: // Internal Types
102: //===--------------------------------------------------------------------===//
103: 
104: // taken from arrow's type.h
105: enum class PhysicalType : uint8_t {
106: 	/// A NULL type having no physical storage
107: 	NA = 0,
108: 
109: 	/// Boolean as 1 bit, LSB bit-packed ordering
110: 	BOOL = 1,
111: 
112: 	/// Unsigned 8-bit little-endian integer
113: 	UINT8 = 2,
114: 
115: 	/// Signed 8-bit little-endian integer
116: 	INT8 = 3,
117: 
118: 	/// Unsigned 16-bit little-endian integer
119: 	UINT16 = 4,
120: 
121: 	/// Signed 16-bit little-endian integer
122: 	INT16 = 5,
123: 
124: 	/// Unsigned 32-bit little-endian integer
125: 	UINT32 = 6,
126: 
127: 	/// Signed 32-bit little-endian integer
128: 	INT32 = 7,
129: 
130: 	/// Unsigned 64-bit little-endian integer
131: 	UINT64 = 8,
132: 
133: 	/// Signed 64-bit little-endian integer
134: 	INT64 = 9,
135: 
136: 	/// 2-byte floating point value
137: 	HALF_FLOAT = 10,
138: 
139: 	/// 4-byte floating point value
140: 	FLOAT = 11,
141: 
142: 	/// 8-byte floating point value
143: 	DOUBLE = 12,
144: 
145: 	/// UTF8 variable-length string as List<Char>
146: 	STRING = 13,
147: 
148: 	/// Variable-length bytes (no guarantee of UTF8-ness)
149: 	BINARY = 14,
150: 
151: 	/// Fixed-size binary. Each value occupies the same number of bytes
152: 	FIXED_SIZE_BINARY = 15,
153: 
154: 	/// int32_t days since the UNIX epoch
155: 	DATE32 = 16,
156: 
157: 	/// int64_t milliseconds since the UNIX epoch
158: 	DATE64 = 17,
159: 
160: 	/// Exact timestamp encoded with int64 since UNIX epoch
161: 	/// Default unit millisecond
162: 	TIMESTAMP = 18,
163: 
164: 	/// Time as signed 32-bit integer, representing either seconds or
165: 	/// milliseconds since midnight
166: 	TIME32 = 19,
167: 
168: 	/// Time as signed 64-bit integer, representing either microseconds or
169: 	/// nanoseconds since midnight
170: 	TIME64 = 20,
171: 
172: 	/// YEAR_MONTH or DAY_TIME interval in SQL style
173: 	INTERVAL = 21,
174: 
175: 	/// Precision- and scale-based decimal type. Storage type depends on the
176: 	/// parameters.
177: 	// DECIMAL = 22,
178: 
179: 	/// A list of some logical data type
180: 	LIST = 23,
181: 
182: 	/// Struct of logical types
183: 	STRUCT = 24,
184: 
185: 	/// Unions of logical types
186: 	UNION = 25,
187: 
188: 	/// Dictionary-encoded type, also called "categorical" or "factor"
189: 	/// in other programming languages. Holds the dictionary value
190: 	/// type but not the dictionary itself, which is part of the
191: 	/// ArrayData struct
192: 	DICTIONARY = 26,
193: 
194: 	/// Map, a repeated struct logical type
195: 	MAP = 27,
196: 
197: 	/// Custom data type, implemented by user
198: 	EXTENSION = 28,
199: 
200: 	/// Fixed size list of some logical type
201: 	FIXED_SIZE_LIST = 29,
202: 
203: 	/// Measure of elapsed time in either seconds, milliseconds, microseconds
204: 	/// or nanoseconds.
205: 	DURATION = 30,
206: 
207: 	/// Like STRING, but with 64-bit offsets
208: 	LARGE_STRING = 31,
209: 
210: 	/// Like BINARY, but with 64-bit offsets
211: 	LARGE_BINARY = 32,
212: 
213: 	/// Like LIST, but with 64-bit offsets
214: 	LARGE_LIST = 33,
215: 
216: 	// DuckDB Extensions
217: 	VARCHAR = 200, // our own string representation, different from STRING and LARGE_STRING above
218: 	POINTER = 202,
219: 	HASH = 203,
220: 	INT128 = 204, // 128-bit integers
221: 
222: 	INVALID = 255
223: };
224: 
225: //===--------------------------------------------------------------------===//
226: // SQL Types
227: //===--------------------------------------------------------------------===//
228: enum class LogicalTypeId : uint8_t {
229: 	INVALID = 0,
230: 	SQLNULL = 1, /* NULL type, used for constant NULL */
231: 	UNKNOWN = 2, /* unknown type, used for parameter expressions */
232: 	ANY = 3,     /* ANY type, used for functions that accept any type as parameter */
233: 
234: 	BOOLEAN = 10,
235: 	TINYINT = 11,
236: 	SMALLINT = 12,
237: 	INTEGER = 13,
238: 	BIGINT = 14,
239: 	DATE = 15,
240: 	TIME = 16,
241: 	TIMESTAMP = 17,
242: 	DECIMAL = 18,
243: 	FLOAT = 19,
244: 	DOUBLE = 20,
245: 	CHAR = 21,
246: 	VARCHAR = 22,
247: 	BLOB = 24,
248: 	INTERVAL = 25,
249: 	UTINYINT = 26,
250: 	USMALLINT = 27,
251: 	UINTEGER = 28,
252: 	UBIGINT = 29,
253: 	HUGEINT = 50,
254: 	POINTER = 51,
255: 	HASH = 52,
256: 
257: 	STRUCT = 100,
258: 	LIST = 101
259: };
260: 
261: struct LogicalType {
262: 	LogicalType();
263: 	LogicalType(LogicalTypeId id); // NOLINT: Allow implicit conversion from `LogicalTypeId`
264: 	LogicalType(LogicalTypeId id, string collation);
265: 	LogicalType(LogicalTypeId id, uint8_t width, uint8_t scale);
266: 	LogicalType(LogicalTypeId id, child_list_t<LogicalType> child_types);
267: 	LogicalType(LogicalTypeId id, uint8_t width, uint8_t scale, string collation,
268: 	            child_list_t<LogicalType> child_types);
269: 
270: 	LogicalTypeId id() const {
271: 		return id_;
272: 	}
273: 	uint8_t width() const {
274: 		return width_;
275: 	}
276: 	uint8_t scale() const {
277: 		return scale_;
278: 	}
279: 	const string &collation() const {
280: 		return collation_;
281: 	}
282: 	const child_list_t<LogicalType> &child_types() const {
283: 		return child_types_;
284: 	}
285: 	PhysicalType InternalType() const {
286: 		return physical_type_;
287: 	}
288: 
289: 	bool operator==(const LogicalType &rhs) const {
290: 		return id_ == rhs.id_ && width_ == rhs.width_ && scale_ == rhs.scale_ && child_types_ == rhs.child_types_;
291: 	}
292: 	bool operator!=(const LogicalType &rhs) const {
293: 		return !(*this == rhs);
294: 	}
295: 
296: 	//! Serializes a LogicalType to a stand-alone binary blob
297: 	void Serialize(Serializer &serializer) const;
298: 	//! Deserializes a blob back into an LogicalType
299: 	static LogicalType Deserialize(Deserializer &source);
300: 
301: 	string ToString() const;
302: 	bool IsIntegral() const;
303: 	bool IsNumeric() const;
304: 	bool IsMoreGenericThan(LogicalType &other) const;
305: 	hash_t Hash() const;
306: 
307: 	static LogicalType MaxLogicalType(const LogicalType &left, const LogicalType &right);
308: 
309: 	//! Gets the decimal properties of a numeric type. Fails if the type is not numeric.
310: 	bool GetDecimalProperties(uint8_t &width, uint8_t &scale) const;
311: 
312: 	void Verify() const;
313: 
314: private:
315: 	LogicalTypeId id_;
316: 	uint8_t width_;
317: 	uint8_t scale_;
318: 	string collation_;
319: 
320: 	child_list_t<LogicalType> child_types_;
321: 	PhysicalType physical_type_;
322: 
323: private:
324: 	PhysicalType GetInternalType();
325: 
326: public:
327: 	static const LogicalType SQLNULL;
328: 	static const LogicalType BOOLEAN;
329: 	static const LogicalType TINYINT;
330: 	static const LogicalType UTINYINT;
331: 	static const LogicalType SMALLINT;
332: 	static const LogicalType USMALLINT;
333: 	static const LogicalType INTEGER;
334: 	static const LogicalType UINTEGER;
335: 	static const LogicalType BIGINT;
336: 	static const LogicalType UBIGINT;
337: 	static const LogicalType FLOAT;
338: 	static const LogicalType DOUBLE;
339: 	static const LogicalType DECIMAL;
340: 	static const LogicalType DATE;
341: 	static const LogicalType TIMESTAMP;
342: 	static const LogicalType TIME;
343: 	static const LogicalType VARCHAR;
344: 	static const LogicalType STRUCT;
345: 	static const LogicalType LIST;
346: 	static const LogicalType ANY;
347: 	static const LogicalType BLOB;
348: 	static const LogicalType INTERVAL;
349: 	static const LogicalType HUGEINT;
350: 	static const LogicalType HASH;
351: 	static const LogicalType POINTER;
352: 	static const LogicalType INVALID;
353: 
354: 	//! A list of all NUMERIC types (integral and floating point types)
355: 	static const vector<LogicalType> NUMERIC;
356: 	//! A list of all INTEGRAL types
357: 	static const vector<LogicalType> INTEGRAL;
358: 	//! A list of ALL SQL types
359: 	static const vector<LogicalType> ALL_TYPES;
360: };
361: 
362: string LogicalTypeIdToString(LogicalTypeId type);
363: 
364: LogicalType TransformStringToLogicalType(const string &str);
365: 
366: //! Returns the PhysicalType for the given type
367: template <class T>
368: PhysicalType GetTypeId() {
369: 	if (std::is_same<T, bool>()) {
370: 		return PhysicalType::BOOL;
371: 	} else if (std::is_same<T, int8_t>()) {
372: 		return PhysicalType::INT8;
373: 	} else if (std::is_same<T, int16_t>()) {
374: 		return PhysicalType::INT16;
375: 	} else if (std::is_same<T, int32_t>()) {
376: 		return PhysicalType::INT32;
377: 	} else if (std::is_same<T, int64_t>()) {
378: 		return PhysicalType::INT64;
379: 	} else if (std::is_same<T, uint8_t>()) {
380: 		return PhysicalType::UINT8;
381: 	} else if (std::is_same<T, uint16_t>()) {
382: 		return PhysicalType::UINT16;
383: 	} else if (std::is_same<T, uint32_t>()) {
384: 		return PhysicalType::UINT32;
385: 	} else if (std::is_same<T, uint64_t>()) {
386: 		return PhysicalType::UINT64;
387: 	} else if (std::is_same<T, hugeint_t>()) {
388: 		return PhysicalType::INT128;
389: 	} else if (std::is_same<T, uint64_t>()) {
390: 		return PhysicalType::HASH;
391: 	} else if (std::is_same<T, uintptr_t>()) {
392: 		return PhysicalType::POINTER;
393: 	} else if (std::is_same<T, float>()) {
394: 		return PhysicalType::FLOAT;
395: 	} else if (std::is_same<T, double>()) {
396: 		return PhysicalType::DOUBLE;
397: 	} else if (std::is_same<T, const char *>() || std::is_same<T, char *>()) {
398: 		return PhysicalType::VARCHAR;
399: 	} else if (std::is_same<T, interval_t>()) {
400: 		return PhysicalType::INTERVAL;
401: 	} else {
402: 		return PhysicalType::INVALID;
403: 	}
404: }
405: 
406: template <class T>
407: bool IsValidType() {
408: 	return GetTypeId<T>() != PhysicalType::INVALID;
409: }
410: 
411: //! The PhysicalType used by the row identifiers column
412: extern const LogicalType LOGICAL_ROW_TYPE;
413: extern const PhysicalType ROW_TYPE;
414: 
415: string TypeIdToString(PhysicalType type);
416: idx_t GetTypeIdSize(PhysicalType type);
417: bool TypeIsConstantSize(PhysicalType type);
418: bool TypeIsIntegral(PhysicalType type);
419: bool TypeIsNumeric(PhysicalType type);
420: bool TypeIsInteger(PhysicalType type);
421: 
422: template <class T>
423: bool IsIntegerType() {
424: 	return TypeIsIntegral(GetTypeId<T>());
425: }
426: 
427: bool ApproxEqual(float l, float r);
428: bool ApproxEqual(double l, double r);
429: 
430: } // namespace duckdb
[end of src/include/duckdb/common/types.hpp]
[start of src/include/duckdb/planner/binder.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/binder.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/unordered_map.hpp"
12: #include "duckdb/parser/column_definition.hpp"
13: #include "duckdb/parser/tokens.hpp"
14: #include "duckdb/planner/bind_context.hpp"
15: #include "duckdb/planner/bound_tokens.hpp"
16: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
17: #include "duckdb/planner/logical_operator.hpp"
18: #include "duckdb/planner/bound_statement.hpp"
19: 
20: namespace duckdb {
21: class BoundResultModifier;
22: class ClientContext;
23: class ExpressionBinder;
24: class LimitModifier;
25: class OrderBinder;
26: class TableCatalogEntry;
27: class ViewCatalogEntry;
28: 
29: struct CreateInfo;
30: struct BoundCreateTableInfo;
31: struct BoundCreateFunctionInfo;
32: struct CommonTableExpressionInfo;
33: 
34: struct CorrelatedColumnInfo {
35: 	ColumnBinding binding;
36: 	LogicalType type;
37: 	string name;
38: 	idx_t depth;
39: 
40: 	explicit CorrelatedColumnInfo(BoundColumnRefExpression &expr)
41: 	    : binding(expr.binding), type(expr.return_type), name(expr.GetName()), depth(expr.depth) {
42: 	}
43: 
44: 	bool operator==(const CorrelatedColumnInfo &rhs) const {
45: 		return binding == rhs.binding;
46: 	}
47: };
48: 
49: //! Bind the parsed query tree to the actual columns present in the catalog.
50: /*!
51:   The binder is responsible for binding tables and columns to actual physical
52:   tables and columns in the catalog. In the process, it also resolves types of
53:   all expressions.
54: */
55: class Binder {
56: 	friend class ExpressionBinder;
57: 	friend class RecursiveSubqueryPlanner;
58: 
59: public:
60: 	explicit Binder(ClientContext &context, Binder *parent = nullptr, bool inherit_ctes = true);
61: 
62: 	//! The client context
63: 	ClientContext &context;
64: 	//! A mapping of names to common table expressions
65: 	unordered_map<string, CommonTableExpressionInfo *> CTE_bindings;
66: 	//! The CTEs that have already been bound
67: 	unordered_set<CommonTableExpressionInfo *> bound_ctes;
68: 	//! The bind context
69: 	BindContext bind_context;
70: 	//! The set of correlated columns bound by this binder (FIXME: this should probably be an unordered_set and not a
71: 	//! vector)
72: 	vector<CorrelatedColumnInfo> correlated_columns;
73: 	//! The set of parameter expressions bound by this binder
74: 	vector<BoundParameterExpression *> *parameters;
75: 	//! Whether or not the bound statement is read-only
76: 	bool read_only;
77: 	//! Whether or not the statement requires a valid transaction to run
78: 	bool requires_valid_transaction;
79: 	//! Whether or not the statement can be streamed to the client
80: 	bool allow_stream_result;
81: 	//! The alias for the currently processing subquery, if it exists
82: 	string alias;
83: 	//! Macro parameter bindings (if any)
84: 	MacroBinding *macro_binding = nullptr;
85: 
86: public:
87: 	BoundStatement Bind(SQLStatement &statement);
88: 	BoundStatement Bind(QueryNode &node);
89: 
90: 	unique_ptr<BoundCreateTableInfo> BindCreateTableInfo(unique_ptr<CreateInfo> info);
91: 	void BindCreateViewInfo(CreateViewInfo &base);
92: 	SchemaCatalogEntry *BindSchema(CreateInfo &info);
93: 	SchemaCatalogEntry *BindCreateFunctionInfo(CreateInfo &info);
94: 
95: 	//! Check usage, and cast named parameters to their types
96: 	static void BindNamedParameters(unordered_map<string, LogicalType> &types, unordered_map<string, Value> &values,
97: 	                                QueryErrorContext &error_context, string &func_name);
98: 
99: 	unique_ptr<BoundTableRef> Bind(TableRef &ref);
100: 	unique_ptr<LogicalOperator> CreatePlan(BoundTableRef &ref);
101: 
102: 	//! Generates an unused index for a table
103: 	idx_t GenerateTableIndex();
104: 
105: 	//! Add a common table expression to the binder
106: 	void AddCTE(const string &name, CommonTableExpressionInfo *cte);
107: 	//! Find a common table expression by name; returns nullptr if none exists
108: 	CommonTableExpressionInfo *FindCTE(const string &name, bool skip = false);
109: 
110: 	bool CTEIsAlreadyBound(CommonTableExpressionInfo *cte);
111: 
112: 	void PushExpressionBinder(ExpressionBinder *binder);
113: 	void PopExpressionBinder();
114: 	void SetActiveBinder(ExpressionBinder *binder);
115: 	ExpressionBinder *GetActiveBinder();
116: 	bool HasActiveBinder();
117: 
118: 	vector<ExpressionBinder *> &GetActiveBinders();
119: 
120: 	void MergeCorrelatedColumns(vector<CorrelatedColumnInfo> &other);
121: 	//! Add a correlated column to this binder (if it does not exist)
122: 	void AddCorrelatedColumn(const CorrelatedColumnInfo &info);
123: 
124: 	string FormatError(ParsedExpression &expr_context, const string &message);
125: 	string FormatError(TableRef &ref_context, const string &message);
126: 	string FormatError(idx_t query_location, const string &message);
127: 
128: private:
129: 	//! The parent binder (if any)
130: 	Binder *parent;
131: 	//! The vector of active binders
132: 	vector<ExpressionBinder *> active_binders;
133: 	//! The count of bound_tables
134: 	idx_t bound_tables;
135: 	//! Whether or not the binder has any unplanned subqueries that still need to be planned
136: 	bool has_unplanned_subqueries = false;
137: 	//! Whether or not subqueries should be planned already
138: 	bool plan_subquery = true;
139: 	//! Whether CTEs should reference the parent binder (if it exists)
140: 	bool inherit_ctes = true;
141: 	//! The root statement of the query that is currently being parsed
142: 	SQLStatement *root_statement = nullptr;
143: 
144: private:
145: 	//! Bind the default values of the columns of a table
146: 	void BindDefaultValues(vector<ColumnDefinition> &columns, vector<unique_ptr<Expression>> &bound_defaults);
147: 
148: 	//! Move correlated expressions from the child binder to this binder
149: 	void MoveCorrelatedExpressions(Binder &other);
150: 
151: 	BoundStatement Bind(SelectStatement &stmt);
152: 	BoundStatement Bind(InsertStatement &stmt);
153: 	BoundStatement Bind(CopyStatement &stmt);
154: 	BoundStatement Bind(DeleteStatement &stmt);
155: 	BoundStatement Bind(UpdateStatement &stmt);
156: 	BoundStatement Bind(CreateStatement &stmt);
157: 	BoundStatement Bind(DropStatement &stmt);
158: 	BoundStatement Bind(AlterStatement &stmt);
159: 	BoundStatement Bind(TransactionStatement &stmt);
160: 	BoundStatement Bind(PragmaStatement &stmt);
161: 	BoundStatement Bind(ExplainStatement &stmt);
162: 	BoundStatement Bind(VacuumStatement &stmt);
163: 	BoundStatement Bind(RelationStatement &stmt);
164: 	BoundStatement Bind(ShowStatement &stmt);
165: 	BoundStatement Bind(CallStatement &stmt);
166: 	BoundStatement Bind(ExportStatement &stmt);
167: 	BoundStatement Bind(SetStatement &stmt);
168: 
169: 	unique_ptr<BoundQueryNode> BindNode(SelectNode &node);
170: 	unique_ptr<BoundQueryNode> BindNode(SetOperationNode &node);
171: 	unique_ptr<BoundQueryNode> BindNode(RecursiveCTENode &node);
172: 	unique_ptr<BoundQueryNode> BindNode(QueryNode &node);
173: 
174: 	unique_ptr<LogicalOperator> VisitQueryNode(BoundQueryNode &node, unique_ptr<LogicalOperator> root);
175: 	unique_ptr<LogicalOperator> CreatePlan(BoundRecursiveCTENode &node);
176: 	unique_ptr<LogicalOperator> CreatePlan(BoundSelectNode &statement);
177: 	unique_ptr<LogicalOperator> CreatePlan(BoundSetOperationNode &node);
178: 	unique_ptr<LogicalOperator> CreatePlan(BoundQueryNode &node);
179: 
180: 	unique_ptr<BoundTableRef> Bind(BaseTableRef &ref);
181: 	unique_ptr<BoundTableRef> Bind(CrossProductRef &ref);
182: 	unique_ptr<BoundTableRef> Bind(JoinRef &ref);
183: 	unique_ptr<BoundTableRef> Bind(SubqueryRef &ref, CommonTableExpressionInfo *cte = nullptr);
184: 	unique_ptr<BoundTableRef> Bind(TableFunctionRef &ref);
185: 	unique_ptr<BoundTableRef> Bind(EmptyTableRef &ref);
186: 	unique_ptr<BoundTableRef> Bind(ExpressionListRef &ref);
187: 
188: 	bool BindFunctionParameters(vector<unique_ptr<ParsedExpression>> &expressions, vector<LogicalType> &arguments,
189: 	                            vector<Value> &parameters, unordered_map<string, Value> &named_parameters,
190: 	                            string &error);
191: 
192: 	unique_ptr<LogicalOperator> CreatePlan(BoundBaseTableRef &ref);
193: 	unique_ptr<LogicalOperator> CreatePlan(BoundCrossProductRef &ref);
194: 	unique_ptr<LogicalOperator> CreatePlan(BoundJoinRef &ref);
195: 	unique_ptr<LogicalOperator> CreatePlan(BoundSubqueryRef &ref);
196: 	unique_ptr<LogicalOperator> CreatePlan(BoundTableFunction &ref);
197: 	unique_ptr<LogicalOperator> CreatePlan(BoundEmptyTableRef &ref);
198: 	unique_ptr<LogicalOperator> CreatePlan(BoundExpressionListRef &ref);
199: 	unique_ptr<LogicalOperator> CreatePlan(BoundCTERef &ref);
200: 
201: 	unique_ptr<LogicalOperator> BindTable(TableCatalogEntry &table, BaseTableRef &ref);
202: 	unique_ptr<LogicalOperator> BindView(ViewCatalogEntry &view, BaseTableRef &ref);
203: 	unique_ptr<LogicalOperator> BindTableOrView(BaseTableRef &ref);
204: 
205: 	BoundStatement BindCopyTo(CopyStatement &stmt);
206: 	BoundStatement BindCopyFrom(CopyStatement &stmt);
207: 
208: 	void BindModifiers(OrderBinder &order_binder, QueryNode &statement, BoundQueryNode &result);
209: 	void BindModifierTypes(BoundQueryNode &result, const vector<LogicalType> &sql_types, idx_t projection_index);
210: 
211: 	unique_ptr<BoundResultModifier> BindLimit(LimitModifier &limit_mod);
212: 	unique_ptr<Expression> BindFilter(unique_ptr<ParsedExpression> condition);
213: 	unique_ptr<Expression> BindOrderExpression(OrderBinder &order_binder, unique_ptr<ParsedExpression> expr);
214: 
215: 	unique_ptr<LogicalOperator> PlanFilter(unique_ptr<Expression> condition, unique_ptr<LogicalOperator> root);
216: 
217: 	void PlanSubqueries(unique_ptr<Expression> *expr, unique_ptr<LogicalOperator> *root);
218: 	unique_ptr<Expression> PlanSubquery(BoundSubqueryExpression &expr, unique_ptr<LogicalOperator> &root);
219: 
220: 	unique_ptr<LogicalOperator> CastLogicalOperatorToTypes(vector<LogicalType> &source_types,
221: 	                                                       vector<LogicalType> &target_types,
222: 	                                                       unique_ptr<LogicalOperator> op);
223: 
224: 	string FindBinding(const string &using_column, const string &join_side);
225: 	bool TryFindBinding(const string &using_column, const string &join_side, string &result);
226: };
227: 
228: } // namespace duckdb
[end of src/include/duckdb/planner/binder.hpp]
[start of src/include/duckdb/planner/expression/bound_subquery_expression.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/expression/bound_subquery_expression.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/enums/subquery_type.hpp"
12: #include "duckdb/planner/binder.hpp"
13: #include "duckdb/planner/bound_query_node.hpp"
14: #include "duckdb/planner/expression.hpp"
15: 
16: namespace duckdb {
17: 
18: class BoundSubqueryExpression : public Expression {
19: public:
20: 	explicit BoundSubqueryExpression(LogicalType return_type);
21: 
22: 	bool IsCorrelated() {
23: 		return binder->correlated_columns.size() > 0;
24: 	}
25: 
26: 	//! The binder used to bind the subquery node
27: 	unique_ptr<Binder> binder;
28: 	//! The bound subquery node
29: 	unique_ptr<BoundQueryNode> subquery;
30: 	//! The subquery type
31: 	SubqueryType subquery_type;
32: 	//! the child expression to compare with (in case of IN, ANY, ALL operators)
33: 	unique_ptr<Expression> child;
34: 	//! The comparison type of the child expression with the subquery (in case of ANY, ALL operators)
35: 	ExpressionType comparison_type;
36: 	//! The LogicalType of the subquery result. Only used for ANY expressions.
37: 	LogicalType child_type;
38: 	//! The target LogicalType of the subquery result (i.e. to which type it should be casted, if child_type <>
39: 	//! child_target). Only used for ANY expressions.
40: 	LogicalType child_target;
41: 
42: public:
43: 	bool HasSubquery() const override {
44: 		return true;
45: 	}
46: 	bool IsScalar() const override {
47: 		return false;
48: 	}
49: 	bool IsFoldable() const override {
50: 		return false;
51: 	}
52: 
53: 	string ToString() const override;
54: 
55: 	bool Equals(const BaseExpression *other) const override;
56: 
57: 	unique_ptr<Expression> Copy() override;
58: };
59: } // namespace duckdb
[end of src/include/duckdb/planner/expression/bound_subquery_expression.hpp]
[start of src/include/duckdb/planner/planner.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/planner.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/parser/sql_statement.hpp"
12: #include "duckdb/planner/binder.hpp"
13: #include "duckdb/planner/logical_operator.hpp"
14: 
15: namespace duckdb {
16: class ClientContext;
17: class PreparedStatementData;
18: 
19: //! The planner creates a logical query plan from the parsed SQL statements
20: //! using the Binder and LogicalPlanGenerator.
21: class Planner {
22: public:
23: 	explicit Planner(ClientContext &context);
24: 
25: 	void CreatePlan(unique_ptr<SQLStatement> statement);
26: 
27: 	unique_ptr<LogicalOperator> plan;
28: 	vector<string> names;
29: 	vector<LogicalType> types;
30: 	unordered_map<idx_t, vector<unique_ptr<Value>>> value_map;
31: 
32: 	Binder binder;
33: 	ClientContext &context;
34: 
35: 	bool read_only;
36: 	bool requires_valid_transaction;
37: 	bool allow_stream_result;
38: 
39: private:
40: 	void CreatePlan(SQLStatement &statement);
41: 	shared_ptr<PreparedStatementData> PrepareSQLStatement(unique_ptr<SQLStatement> statement);
42: 	void PlanPrepare(unique_ptr<SQLStatement> statement);
43: 	void PlanExecute(unique_ptr<SQLStatement> statement);
44: 
45: 	// void VerifyQuery(BoundSQLStatement &statement);
46: 	// void VerifyNode(BoundQueryNode &statement);
47: 	// void VerifyExpression(Expression &expr, vector<unique_ptr<Expression>> &copies);
48: 
49: 	// bool StatementRequiresValidTransaction(BoundSQLStatement &statement);
50: };
51: } // namespace duckdb
[end of src/include/duckdb/planner/planner.hpp]
[start of src/include/duckdb/planner/query_node/bound_recursive_cte_node.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/query_node/bound_recursive_cte_node.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/planner/binder.hpp"
12: #include "duckdb/planner/bound_query_node.hpp"
13: 
14: namespace duckdb {
15: 
16: //! Bound equivalent of SetOperationNode
17: class BoundRecursiveCTENode : public BoundQueryNode {
18: public:
19: 	BoundRecursiveCTENode() : BoundQueryNode(QueryNodeType::RECURSIVE_CTE_NODE) {
20: 	}
21: 
22: 	//! Keep track of the CTE name this node represents
23: 	string ctename;
24: 
25: 	bool union_all;
26: 	//! The left side of the set operation
27: 	unique_ptr<BoundQueryNode> left;
28: 	//! The right side of the set operation
29: 	unique_ptr<BoundQueryNode> right;
30: 
31: 	//! Index used by the set operation
32: 	idx_t setop_index;
33: 	//! The binder used by the left side of the set operation
34: 	unique_ptr<Binder> left_binder;
35: 	//! The binder used by the right side of the set operation
36: 	unique_ptr<Binder> right_binder;
37: 
38: public:
39: 	idx_t GetRootIndex() override {
40: 		return setop_index;
41: 	}
42: };
43: 
44: } // namespace duckdb
[end of src/include/duckdb/planner/query_node/bound_recursive_cte_node.hpp]
[start of src/include/duckdb/planner/query_node/bound_set_operation_node.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/query_node/bound_set_operation_node.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/enums/set_operation_type.hpp"
12: #include "duckdb/planner/binder.hpp"
13: #include "duckdb/planner/bound_query_node.hpp"
14: 
15: namespace duckdb {
16: 
17: //! Bound equivalent of SetOperationNode
18: class BoundSetOperationNode : public BoundQueryNode {
19: public:
20: 	BoundSetOperationNode() : BoundQueryNode(QueryNodeType::SET_OPERATION_NODE) {
21: 	}
22: 
23: 	//! The type of set operation
24: 	SetOperationType setop_type = SetOperationType::NONE;
25: 	//! The left side of the set operation
26: 	unique_ptr<BoundQueryNode> left;
27: 	//! The right side of the set operation
28: 	unique_ptr<BoundQueryNode> right;
29: 
30: 	//! Index used by the set operation
31: 	idx_t setop_index;
32: 	//! The binder used by the left side of the set operation
33: 	unique_ptr<Binder> left_binder;
34: 	//! The binder used by the right side of the set operation
35: 	unique_ptr<Binder> right_binder;
36: 
37: public:
38: 	idx_t GetRootIndex() override {
39: 		return setop_index;
40: 	}
41: };
42: 
43: } // namespace duckdb
[end of src/include/duckdb/planner/query_node/bound_set_operation_node.hpp]
[start of src/include/duckdb/planner/tableref/bound_crossproductref.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/tableref/bound_crossproductref.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/planner/binder.hpp"
12: #include "duckdb/planner/bound_tableref.hpp"
13: 
14: namespace duckdb {
15: 
16: //! Represents a cross product
17: class BoundCrossProductRef : public BoundTableRef {
18: public:
19: 	BoundCrossProductRef() : BoundTableRef(TableReferenceType::CROSS_PRODUCT) {
20: 	}
21: 
22: 	//! The binder used to bind the LHS of the cross product
23: 	unique_ptr<Binder> left_binder;
24: 	//! The binder used to bind the RHS of the cross product
25: 	unique_ptr<Binder> right_binder;
26: 	//! The left hand side of the cross product
27: 	unique_ptr<BoundTableRef> left;
28: 	//! The right hand side of the cross product
29: 	unique_ptr<BoundTableRef> right;
30: };
31: } // namespace duckdb
[end of src/include/duckdb/planner/tableref/bound_crossproductref.hpp]
[start of src/include/duckdb/planner/tableref/bound_joinref.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/tableref/bound_joinref.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/planner/binder.hpp"
12: #include "duckdb/common/enums/join_type.hpp"
13: #include "duckdb/planner/bound_tableref.hpp"
14: #include "duckdb/planner/expression.hpp"
15: 
16: namespace duckdb {
17: 
18: //! Represents a join
19: class BoundJoinRef : public BoundTableRef {
20: public:
21: 	BoundJoinRef() : BoundTableRef(TableReferenceType::JOIN) {
22: 	}
23: 
24: 	//! The binder used to bind the LHS of the join
25: 	unique_ptr<Binder> left_binder;
26: 	//! The binder used to bind the RHS of the join
27: 	unique_ptr<Binder> right_binder;
28: 	//! The left hand side of the join
29: 	unique_ptr<BoundTableRef> left;
30: 	//! The right hand side of the join
31: 	unique_ptr<BoundTableRef> right;
32: 	//! The join condition
33: 	unique_ptr<Expression> condition;
34: 	//! The join type
35: 	JoinType type;
36: };
37: } // namespace duckdb
[end of src/include/duckdb/planner/tableref/bound_joinref.hpp]
[start of src/include/duckdb/planner/tableref/bound_subqueryref.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/tableref/bound_subqueryref.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/planner/binder.hpp"
12: #include "duckdb/planner/bound_query_node.hpp"
13: #include "duckdb/planner/bound_tableref.hpp"
14: 
15: namespace duckdb {
16: 
17: //! Represents a cross product
18: class BoundSubqueryRef : public BoundTableRef {
19: public:
20: 	BoundSubqueryRef(unique_ptr<Binder> binder, unique_ptr<BoundQueryNode> subquery)
21: 	    : BoundTableRef(TableReferenceType::SUBQUERY), binder(move(binder)), subquery(move(subquery)) {
22: 	}
23: 
24: 	//! The binder used to bind the subquery
25: 	unique_ptr<Binder> binder;
26: 	//! The bound subquery node
27: 	unique_ptr<BoundQueryNode> subquery;
28: };
29: } // namespace duckdb
[end of src/include/duckdb/planner/tableref/bound_subqueryref.hpp]
[start of src/main/client_context.cpp]
1: #include "duckdb/main/client_context.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/serializer/buffered_deserializer.hpp"
5: #include "duckdb/common/serializer/buffered_serializer.hpp"
6: #include "duckdb/execution/physical_plan_generator.hpp"
7: #include "duckdb/main/database.hpp"
8: #include "duckdb/main/materialized_query_result.hpp"
9: #include "duckdb/main/query_result.hpp"
10: #include "duckdb/main/stream_query_result.hpp"
11: #include "duckdb/optimizer/optimizer.hpp"
12: #include "duckdb/parser/parser.hpp"
13: #include "duckdb/parser/expression/constant_expression.hpp"
14: #include "duckdb/parser/statement/drop_statement.hpp"
15: #include "duckdb/parser/statement/execute_statement.hpp"
16: #include "duckdb/parser/statement/explain_statement.hpp"
17: #include "duckdb/parser/statement/prepare_statement.hpp"
18: #include "duckdb/parser/statement/select_statement.hpp"
19: #include "duckdb/planner/operator/logical_execute.hpp"
20: #include "duckdb/planner/planner.hpp"
21: #include "duckdb/transaction/transaction_manager.hpp"
22: #include "duckdb/transaction/transaction.hpp"
23: #include "duckdb/storage/data_table.hpp"
24: #include "duckdb/main/appender.hpp"
25: #include "duckdb/main/relation.hpp"
26: #include "duckdb/parser/statement/relation_statement.hpp"
27: #include "duckdb/parallel/task_scheduler.hpp"
28: #include "duckdb/common/serializer/buffered_file_writer.hpp"
29: #include "duckdb/planner/pragma_handler.hpp"
30: #include "duckdb/common/to_string.hpp"
31: 
32: namespace duckdb {
33: 
34: class ClientContextLock {
35: public:
36: 	explicit ClientContextLock(mutex &context_lock) : client_guard(context_lock) {
37: 	}
38: 	~ClientContextLock() {
39: 	}
40: 
41: private:
42: 	lock_guard<mutex> client_guard;
43: };
44: 
45: ClientContext::ClientContext(shared_ptr<DatabaseInstance> database)
46:     : db(move(database)), transaction(db->GetTransactionManager(), *this), interrupted(false), executor(*this),
47:       temporary_objects(make_unique<SchemaCatalogEntry>(&db->GetCatalog(), TEMP_SCHEMA, true)), open_result(nullptr) {
48: 	std::random_device rd;
49: 	random_engine.seed(rd());
50: }
51: 
52: ClientContext::~ClientContext() {
53: 	Destroy();
54: }
55: 
56: unique_ptr<ClientContextLock> ClientContext::LockContext() {
57: 	return make_unique<ClientContextLock>(context_lock);
58: }
59: 
60: void ClientContext::Destroy() {
61: 	auto lock = LockContext();
62: 	if (transaction.HasActiveTransaction()) {
63: 		ActiveTransaction().active_query = MAXIMUM_QUERY_ID;
64: 		if (!transaction.IsAutoCommit()) {
65: 			transaction.Rollback();
66: 		}
67: 	}
68: 	CleanupInternal(*lock);
69: }
70: 
71: void ClientContext::Cleanup() {
72: 	auto lock = LockContext();
73: 	CleanupInternal(*lock);
74: }
75: 
76: unique_ptr<DataChunk> ClientContext::Fetch() {
77: 	auto lock = LockContext();
78: 	if (!open_result) {
79: 		// no result to fetch from
80: 		throw Exception("Fetch was called, but there is no open result (or the result was previously closed)");
81: 	}
82: 	try {
83: 		// fetch the chunk and return it
84: 		auto chunk = FetchInternal(*lock);
85: 		return chunk;
86: 	} catch (Exception &ex) {
87: 		open_result->error = ex.what();
88: 	} catch (...) {
89: 		open_result->error = "Unhandled exception in Fetch";
90: 	}
91: 	open_result->success = false;
92: 	CleanupInternal(*lock);
93: 	return nullptr;
94: }
95: 
96: string ClientContext::FinalizeQuery(ClientContextLock &lock, bool success) {
97: 	profiler.EndQuery();
98: 
99: 	executor.Reset();
100: 
101: 	string error;
102: 	if (transaction.HasActiveTransaction()) {
103: 		ActiveTransaction().active_query = MAXIMUM_QUERY_ID;
104: 		try {
105: 			if (transaction.IsAutoCommit()) {
106: 				if (success) {
107: 					// query was successful: commit
108: 					transaction.Commit();
109: 				} else {
110: 					// query was unsuccessful: rollback
111: 					transaction.Rollback();
112: 				}
113: 			}
114: 		} catch (Exception &ex) {
115: 			error = ex.what();
116: 		} catch (...) {
117: 			error = "Unhandled exception!";
118: 		}
119: 	}
120: 	return error;
121: }
122: 
123: void ClientContext::CleanupInternal(ClientContextLock &lock) {
124: 	if (!open_result) {
125: 		// no result currently open
126: 		return;
127: 	}
128: 
129: 	auto error = FinalizeQuery(lock, open_result->success);
130: 	if (open_result->success) {
131: 		// if an error occurred while committing report it in the result
132: 		open_result->error = error;
133: 		open_result->success = error.empty();
134: 	}
135: 
136: 	open_result->is_open = false;
137: 	open_result = nullptr;
138: 
139: 	this->query = string();
140: }
141: 
142: unique_ptr<DataChunk> ClientContext::FetchInternal(ClientContextLock &) {
143: 	return executor.FetchChunk();
144: }
145: 
146: shared_ptr<PreparedStatementData> ClientContext::CreatePreparedStatement(ClientContextLock &lock, const string &query,
147:                                                                          unique_ptr<SQLStatement> statement) {
148: 	StatementType statement_type = statement->type;
149: 	auto result = make_shared<PreparedStatementData>(statement_type);
150: 
151: 	profiler.StartPhase("planner");
152: 	Planner planner(*this);
153: 	planner.CreatePlan(move(statement));
154: 	D_ASSERT(planner.plan);
155: 	profiler.EndPhase();
156: 
157: 	auto plan = move(planner.plan);
158: 	// extract the result column names from the plan
159: 	result->read_only = planner.read_only;
160: 	result->requires_valid_transaction = planner.requires_valid_transaction;
161: 	result->allow_stream_result = planner.allow_stream_result;
162: 	result->names = planner.names;
163: 	result->types = planner.types;
164: 	result->value_map = move(planner.value_map);
165: 	result->catalog_version = Transaction::GetTransaction(*this).catalog_version;
166: 
167: 	if (enable_optimizer) {
168: 		profiler.StartPhase("optimizer");
169: 		Optimizer optimizer(planner.binder, *this);
170: 		plan = optimizer.Optimize(move(plan));
171: 		D_ASSERT(plan);
172: 		profiler.EndPhase();
173: 	}
174: 
175: 	profiler.StartPhase("physical_planner");
176: 	// now convert logical query plan into a physical query plan
177: 	PhysicalPlanGenerator physical_planner(*this);
178: 	auto physical_plan = physical_planner.CreatePlan(move(plan));
179: 	profiler.EndPhase();
180: 
181: 	result->plan = move(physical_plan);
182: 	return result;
183: }
184: 
185: unique_ptr<QueryResult> ClientContext::ExecutePreparedStatement(ClientContextLock &lock, const string &query,
186:                                                                 shared_ptr<PreparedStatementData> statement_p,
187:                                                                 vector<Value> bound_values, bool allow_stream_result) {
188: 	auto &statement = *statement_p;
189: 	if (ActiveTransaction().IsInvalidated() && statement.requires_valid_transaction) {
190: 		throw Exception("Current transaction is aborted (please ROLLBACK)");
191: 	}
192: 	auto &config = DBConfig::GetConfig(*this);
193: 	if (config.access_mode == AccessMode::READ_ONLY && !statement.read_only) {
194: 		throw Exception(StringUtil::Format("Cannot execute statement of type \"%s\" in read-only mode!",
195: 		                                   StatementTypeToString(statement.statement_type)));
196: 	}
197: 
198: 	// bind the bound values before execution
199: 	statement.Bind(move(bound_values));
200: 
201: 	bool create_stream_result = statement.allow_stream_result && allow_stream_result;
202: 
203: 	// store the physical plan in the context for calls to Fetch()
204: 	executor.Initialize(statement.plan.get());
205: 
206: 	auto types = executor.GetTypes();
207: 
208: 	D_ASSERT(types == statement.types);
209: 
210: 	if (create_stream_result) {
211: 		// successfully compiled SELECT clause and it is the last statement
212: 		// return a StreamQueryResult so the client can call Fetch() on it and stream the result
213: 		return make_unique<StreamQueryResult>(statement.statement_type, shared_from_this(), statement.types,
214: 		                                      statement.names, move(statement_p));
215: 	}
216: 	// create a materialized result by continuously fetching
217: 	auto result = make_unique<MaterializedQueryResult>(statement.statement_type, statement.types, statement.names);
218: 	while (true) {
219: 		auto chunk = FetchInternal(lock);
220: 		if (chunk->size() == 0) {
221: 			break;
222: 		}
223: #ifdef DEBUG
224: 		for (idx_t i = 0; i < chunk->ColumnCount(); i++) {
225: 			if (statement.types[i].id() == LogicalTypeId::VARCHAR) {
226: 				chunk->data[i].UTFVerify(chunk->size());
227: 			}
228: 		}
229: #endif
230: 		result->collection.Append(*chunk);
231: 	}
232: 	return move(result);
233: }
234: 
235: void ClientContext::InitialCleanup(ClientContextLock &lock) {
236: 	//! Cleanup any open results and reset the interrupted flag
237: 	CleanupInternal(lock);
238: 	interrupted = false;
239: }
240: 
241: vector<unique_ptr<SQLStatement>> ClientContext::ParseStatements(const string &query) {
242: 	auto lock = LockContext();
243: 	return ParseStatementsInternal(*lock, query);
244: }
245: 
246: vector<unique_ptr<SQLStatement>> ClientContext::ParseStatementsInternal(ClientContextLock &lock, const string &query) {
247: 	Parser parser;
248: 	parser.ParseQuery(query);
249: 
250: 	PragmaHandler handler(*this);
251: 	handler.HandlePragmaStatements(lock, parser.statements);
252: 
253: 	return move(parser.statements);
254: }
255: 
256: void ClientContext::HandlePragmaStatements(vector<unique_ptr<SQLStatement>> &statements) {
257: 	auto lock = LockContext();
258: 
259: 	PragmaHandler handler(*this);
260: 	handler.HandlePragmaStatements(*lock, statements);
261: }
262: 
263: unique_ptr<PreparedStatement> ClientContext::PrepareInternal(ClientContextLock &lock,
264:                                                              unique_ptr<SQLStatement> statement) {
265: 	auto n_param = statement->n_param;
266: 	auto statement_query = statement->query;
267: 	shared_ptr<PreparedStatementData> prepared_data;
268: 	auto unbound_statement = statement->Copy();
269: 	RunFunctionInTransactionInternal(
270: 	    lock, [&]() { prepared_data = CreatePreparedStatement(lock, statement_query, move(statement)); }, false);
271: 	prepared_data->unbound_statement = move(unbound_statement);
272: 	return make_unique<PreparedStatement>(shared_from_this(), move(prepared_data), move(statement_query), n_param);
273: }
274: 
275: unique_ptr<PreparedStatement> ClientContext::Prepare(unique_ptr<SQLStatement> statement) {
276: 	auto lock = LockContext();
277: 	// prepare the query
278: 	try {
279: 		InitialCleanup(*lock);
280: 		return PrepareInternal(*lock, move(statement));
281: 	} catch (std::exception &ex) {
282: 		return make_unique<PreparedStatement>(ex.what());
283: 	}
284: }
285: 
286: unique_ptr<PreparedStatement> ClientContext::Prepare(const string &query) {
287: 	auto lock = LockContext();
288: 	// prepare the query
289: 	try {
290: 		InitialCleanup(*lock);
291: 
292: 		// first parse the query
293: 		auto statements = ParseStatementsInternal(*lock, query);
294: 		if (statements.empty()) {
295: 			throw Exception("No statement to prepare!");
296: 		}
297: 		if (statements.size() > 1) {
298: 			throw Exception("Cannot prepare multiple statements at once!");
299: 		}
300: 		return PrepareInternal(*lock, move(statements[0]));
301: 	} catch (std::exception &ex) {
302: 		return make_unique<PreparedStatement>(ex.what());
303: 	}
304: }
305: 
306: unique_ptr<QueryResult> ClientContext::Execute(const string &query, shared_ptr<PreparedStatementData> &prepared,
307:                                                vector<Value> &values, bool allow_stream_result) {
308: 	auto lock = LockContext();
309: 	try {
310: 		InitialCleanup(*lock);
311: 	} catch (std::exception &ex) {
312: 		return make_unique<MaterializedQueryResult>(ex.what());
313: 	}
314: 	return RunStatementOrPreparedStatement(*lock, query, nullptr, prepared, &values, allow_stream_result);
315: }
316: 
317: unique_ptr<QueryResult> ClientContext::RunStatementInternal(ClientContextLock &lock, const string &query,
318:                                                             unique_ptr<SQLStatement> statement,
319:                                                             bool allow_stream_result) {
320: 	// prepare the query for execution
321: 	auto prepared = CreatePreparedStatement(lock, query, move(statement));
322: 	// by default, no values are bound
323: 	vector<Value> bound_values;
324: 	// execute the prepared statement
325: 	return ExecutePreparedStatement(lock, query, move(prepared), move(bound_values), allow_stream_result);
326: }
327: 
328: unique_ptr<QueryResult> ClientContext::RunStatementOrPreparedStatement(ClientContextLock &lock, const string &query,
329:                                                                        unique_ptr<SQLStatement> statement,
330:                                                                        shared_ptr<PreparedStatementData> &prepared,
331:                                                                        vector<Value> *values,
332:                                                                        bool allow_stream_result) {
333: 	this->query = query;
334: 
335: 	unique_ptr<QueryResult> result;
336: 	// check if we are on AutoCommit. In this case we should start a transaction.
337: 	if (transaction.IsAutoCommit()) {
338: 		transaction.BeginTransaction();
339: 	}
340: 	ActiveTransaction().active_query = db->GetTransactionManager().GetQueryNumber();
341: 	if (statement && query_verification_enabled) {
342: 		// query verification is enabled
343: 		// create a copy of the statement, and use the copy
344: 		// this way we verify that the copy correctly copies all properties
345: 		auto copied_statement = statement->Copy();
346: 		if (statement->type == StatementType::SELECT_STATEMENT) {
347: 			// in case this is a select query, we verify the original statement
348: 			string error = VerifyQuery(lock, query, move(statement));
349: 			if (!error.empty()) {
350: 				// query failed: abort now
351: 				FinalizeQuery(lock, false);
352: 				// error in verifying query
353: 				return make_unique<MaterializedQueryResult>(error);
354: 			}
355: 		}
356: 		statement = move(copied_statement);
357: 	}
358: 	// start the profiler
359: 	profiler.StartQuery(query);
360: 	try {
361: 		if (statement) {
362: 			result = RunStatementInternal(lock, query, move(statement), allow_stream_result);
363: 		} else {
364: 			auto &catalog = Catalog::GetCatalog(*this);
365: 			if (prepared->unbound_statement && catalog.GetCatalogVersion() != prepared->catalog_version) {
366: 				D_ASSERT(prepared->unbound_statement.get());
367: 				// catalog was modified: rebind the statement before execution
368: 				auto new_prepared = CreatePreparedStatement(lock, query, prepared->unbound_statement->Copy());
369: 				if (prepared->types != new_prepared->types) {
370: 					throw BinderException("Rebinding statement after catalog change resulted in change of types");
371: 				}
372: 				new_prepared->unbound_statement = move(prepared->unbound_statement);
373: 				prepared = move(new_prepared);
374: 			}
375: 			result = ExecutePreparedStatement(lock, query, prepared, *values, allow_stream_result);
376: 		}
377: 	} catch (StandardException &ex) {
378: 		// standard exceptions do not invalidate the current transaction
379: 		result = make_unique<MaterializedQueryResult>(ex.what());
380: 	} catch (std::exception &ex) {
381: 		// other types of exceptions do invalidate the current transaction
382: 		if (transaction.HasActiveTransaction()) {
383: 			ActiveTransaction().Invalidate();
384: 		}
385: 		result = make_unique<MaterializedQueryResult>(ex.what());
386: 	}
387: 	if (!result->success) {
388: 		// initial failures should always be reported as MaterializedResult
389: 		D_ASSERT(result->type != QueryResultType::STREAM_RESULT);
390: 		// query failed: abort now
391: 		FinalizeQuery(lock, false);
392: 		return result;
393: 	}
394: 	// query succeeded, append to list of results
395: 	if (result->type == QueryResultType::STREAM_RESULT) {
396: 		// store as currently open result if it is a stream result
397: 		this->open_result = (StreamQueryResult *)result.get();
398: 	} else {
399: 		// finalize the query if it is not a stream result
400: 		string error = FinalizeQuery(lock, true);
401: 		if (!error.empty()) {
402: 			// failure in committing transaction
403: 			return make_unique<MaterializedQueryResult>(error);
404: 		}
405: 	}
406: 	return result;
407: }
408: 
409: unique_ptr<QueryResult> ClientContext::RunStatement(ClientContextLock &lock, const string &query,
410:                                                     unique_ptr<SQLStatement> statement, bool allow_stream_result) {
411: 	shared_ptr<PreparedStatementData> prepared;
412: 	return RunStatementOrPreparedStatement(lock, query, move(statement), prepared, nullptr, allow_stream_result);
413: }
414: 
415: unique_ptr<QueryResult> ClientContext::RunStatements(ClientContextLock &lock, const string &query,
416:                                                      vector<unique_ptr<SQLStatement>> &statements,
417:                                                      bool allow_stream_result) {
418: 	// now we have a list of statements
419: 	// iterate over them and execute them one by one
420: 	unique_ptr<QueryResult> result;
421: 	QueryResult *last_result = nullptr;
422: 	for (idx_t i = 0; i < statements.size(); i++) {
423: 		auto &statement = statements[i];
424: 		bool is_last_statement = i + 1 == statements.size();
425: 		auto current_result = RunStatement(lock, query, move(statement), allow_stream_result && is_last_statement);
426: 		// now append the result to the list of results
427: 		if (!last_result) {
428: 			// first result of the query
429: 			result = move(current_result);
430: 			last_result = result.get();
431: 		} else {
432: 			// later results; attach to the result chain
433: 			last_result->next = move(current_result);
434: 			last_result = last_result->next.get();
435: 		}
436: 	}
437: 	return result;
438: }
439: 
440: void ClientContext::LogQueryInternal(ClientContextLock &, const string &query) {
441: 	if (!log_query_writer) {
442: 		return;
443: 	}
444: 	// log query path is set: log the query
445: 	log_query_writer->WriteData((const_data_ptr_t)query.c_str(), query.size());
446: 	log_query_writer->WriteData((const_data_ptr_t) "\n", 1);
447: 	log_query_writer->Flush();
448: }
449: 
450: unique_ptr<QueryResult> ClientContext::Query(unique_ptr<SQLStatement> statement, bool allow_stream_result) {
451: 	auto lock = LockContext();
452: 	if (log_query_writer) {
453: 		LogQueryInternal(*lock, statement->query.substr(statement->stmt_location, statement->stmt_length));
454: 	}
455: 
456: 	vector<unique_ptr<SQLStatement>> statements;
457: 	statements.push_back(move(statement));
458: 
459: 	return RunStatements(*lock, query, statements, allow_stream_result);
460: }
461: 
462: unique_ptr<QueryResult> ClientContext::Query(const string &query, bool allow_stream_result) {
463: 	auto lock = LockContext();
464: 	LogQueryInternal(*lock, query);
465: 
466: 	vector<unique_ptr<SQLStatement>> statements;
467: 	try {
468: 		InitialCleanup(*lock);
469: 		// parse the query and transform it into a set of statements
470: 		statements = ParseStatementsInternal(*lock, query);
471: 	} catch (std::exception &ex) {
472: 		return make_unique<MaterializedQueryResult>(ex.what());
473: 	}
474: 
475: 	if (statements.empty()) {
476: 		// no statements, return empty successful result
477: 		return make_unique<MaterializedQueryResult>(StatementType::INVALID_STATEMENT);
478: 	}
479: 
480: 	return RunStatements(*lock, query, statements, allow_stream_result);
481: }
482: 
483: void ClientContext::Interrupt() {
484: 	interrupted = true;
485: }
486: 
487: void ClientContext::EnableProfiling() {
488: 	auto lock = LockContext();
489: 	profiler.Enable();
490: }
491: 
492: void ClientContext::DisableProfiling() {
493: 	auto lock = LockContext();
494: 	profiler.Disable();
495: }
496: 
497: string ClientContext::VerifyQuery(ClientContextLock &lock, const string &query, unique_ptr<SQLStatement> statement) {
498: 	D_ASSERT(statement->type == StatementType::SELECT_STATEMENT);
499: 	// aggressive query verification
500: 
501: 	// the purpose of this function is to test correctness of otherwise hard to test features:
502: 	// Copy() of statements and expressions
503: 	// Serialize()/Deserialize() of expressions
504: 	// Hash() of expressions
505: 	// Equality() of statements and expressions
506: 	// Correctness of plans both with and without optimizers
507: 	// Correctness of plans both with and without parallelism
508: 
509: 	// copy the statement
510: 	auto select_stmt = (SelectStatement *)statement.get();
511: 	auto copied_stmt = unique_ptr_cast<SQLStatement, SelectStatement>(select_stmt->Copy());
512: 	auto unoptimized_stmt = unique_ptr_cast<SQLStatement, SelectStatement>(select_stmt->Copy());
513: 
514: 	BufferedSerializer serializer;
515: 	select_stmt->Serialize(serializer);
516: 	BufferedDeserializer source(serializer);
517: 	auto deserialized_stmt = SelectStatement::Deserialize(source);
518: 	// all the statements should be equal
519: 	D_ASSERT(copied_stmt->Equals(statement.get()));
520: 	D_ASSERT(deserialized_stmt->Equals(statement.get()));
521: 	D_ASSERT(copied_stmt->Equals(deserialized_stmt.get()));
522: 
523: 	// now perform checking on the expressions
524: #ifdef DEBUG
525: 	auto &orig_expr_list = select_stmt->node->GetSelectList();
526: 	auto &de_expr_list = deserialized_stmt->node->GetSelectList();
527: 	auto &cp_expr_list = copied_stmt->node->GetSelectList();
528: 	D_ASSERT(orig_expr_list.size() == de_expr_list.size() && cp_expr_list.size() == de_expr_list.size());
529: 	for (idx_t i = 0; i < orig_expr_list.size(); i++) {
530: 		// run the ToString, to verify that it doesn't crash
531: 		orig_expr_list[i]->ToString();
532: 		// check that the expressions are equivalent
533: 		D_ASSERT(orig_expr_list[i]->Equals(de_expr_list[i].get()));
534: 		D_ASSERT(orig_expr_list[i]->Equals(cp_expr_list[i].get()));
535: 		D_ASSERT(de_expr_list[i]->Equals(cp_expr_list[i].get()));
536: 		// check that the hashes are equivalent too
537: 		D_ASSERT(orig_expr_list[i]->Hash() == de_expr_list[i]->Hash());
538: 		D_ASSERT(orig_expr_list[i]->Hash() == cp_expr_list[i]->Hash());
539: 	}
540: 	// now perform additional checking within the expressions
541: 	for (idx_t outer_idx = 0; outer_idx < orig_expr_list.size(); outer_idx++) {
542: 		auto hash = orig_expr_list[outer_idx]->Hash();
543: 		for (idx_t inner_idx = 0; inner_idx < orig_expr_list.size(); inner_idx++) {
544: 			auto hash2 = orig_expr_list[inner_idx]->Hash();
545: 			if (hash != hash2) {
546: 				// if the hashes are not equivalent, the expressions should not be equivalent
547: 				D_ASSERT(!orig_expr_list[outer_idx]->Equals(orig_expr_list[inner_idx].get()));
548: 			}
549: 		}
550: 	}
551: #endif
552: 
553: 	// disable profiling if it is enabled
554: 	bool profiling_is_enabled = profiler.IsEnabled();
555: 	if (profiling_is_enabled) {
556: 		profiler.Disable();
557: 	}
558: 
559: 	// see below
560: 	auto statement_copy_for_explain = select_stmt->Copy();
561: 
562: 	unique_ptr<MaterializedQueryResult> original_result =
563: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT),
564: 	                                    copied_result =
565: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT),
566: 	                                    deserialized_result =
567: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT),
568: 	                                    unoptimized_result =
569: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT);
570: 
571: 	// execute the original statement
572: 	try {
573: 		auto result = RunStatementInternal(lock, query, move(statement), false);
574: 		original_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
575: 	} catch (std::exception &ex) {
576: 		original_result->error = ex.what();
577: 		original_result->success = false;
578: 		interrupted = false;
579: 	}
580: 
581: 	// check explain, only if q does not already contain EXPLAIN
582: 	if (original_result->success) {
583: 		auto explain_q = "EXPLAIN " + query;
584: 		auto explain_stmt = make_unique<ExplainStatement>(move(statement_copy_for_explain));
585: 		try {
586: 			RunStatementInternal(lock, explain_q, move(explain_stmt), false);
587: 		} catch (std::exception &ex) {
588: 			return "EXPLAIN failed but query did not (" + string(ex.what()) + ")";
589: 		}
590: 	}
591: 
592: 	// now execute the copied statement
593: 	try {
594: 		auto result = RunStatementInternal(lock, query, move(copied_stmt), false);
595: 		copied_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
596: 	} catch (std::exception &ex) {
597: 		copied_result->error = ex.what();
598: 		copied_result->success = false;
599: 		interrupted = false;
600: 	}
601: 	// now execute the deserialized statement
602: 	try {
603: 		auto result = RunStatementInternal(lock, query, move(deserialized_stmt), false);
604: 		deserialized_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
605: 	} catch (std::exception &ex) {
606: 		deserialized_result->error = ex.what();
607: 		deserialized_result->success = false;
608: 		interrupted = false;
609: 	}
610: 	// now execute the unoptimized statement
611: 	enable_optimizer = false;
612: 	try {
613: 		auto result = RunStatementInternal(lock, query, move(unoptimized_stmt), false);
614: 		unoptimized_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
615: 	} catch (std::exception &ex) {
616: 		unoptimized_result->error = ex.what();
617: 		unoptimized_result->success = false;
618: 		interrupted = false;
619: 	}
620: 	enable_optimizer = true;
621: 
622: 	if (profiling_is_enabled) {
623: 		profiler.Enable();
624: 	}
625: 
626: 	// now compare the results
627: 	// the results of all runs should be identical
628: 	vector<unique_ptr<MaterializedQueryResult>> results;
629: 	results.push_back(move(copied_result));
630: 	results.push_back(move(deserialized_result));
631: 	results.push_back(move(unoptimized_result));
632: 	vector<string> names = {"Copied Result", "Deserialized Result", "Unoptimized Result"};
633: 	for (idx_t i = 0; i < results.size(); i++) {
634: 		if (original_result->success != results[i]->success) {
635: 			string result = names[i] + " differs from original result!\n";
636: 			result += "Original Result:\n" + original_result->ToString();
637: 			result += names[i] + ":\n" + results[i]->ToString();
638: 			return result;
639: 		}
640: 		if (!original_result->collection.Equals(results[i]->collection)) {
641: 			string result = names[i] + " differs from original result!\n";
642: 			result += "Original Result:\n" + original_result->ToString();
643: 			result += names[i] + ":\n" + results[i]->ToString();
644: 			return result;
645: 		}
646: 	}
647: 
648: 	return "";
649: }
650: 
651: void ClientContext::RegisterFunction(CreateFunctionInfo *info) {
652: 	RunFunctionInTransaction([&]() {
653: 		auto &catalog = Catalog::GetCatalog(*this);
654: 		catalog.CreateFunction(*this, temporary_objects.get(), info);
655: 	});
656: }
657: 
658: void ClientContext::RunFunctionInTransactionInternal(ClientContextLock &lock, const std::function<void(void)> &fun,
659:                                                      bool requires_valid_transaction) {
660: 	if (requires_valid_transaction && transaction.HasActiveTransaction() &&
661: 	    transaction.ActiveTransaction().IsInvalidated()) {
662: 		throw Exception("Failed: transaction has been invalidated!");
663: 	}
664: 	// check if we are on AutoCommit. In this case we should start a transaction
665: 	if (transaction.IsAutoCommit()) {
666: 		transaction.BeginTransaction();
667: 	}
668: 	try {
669: 		fun();
670: 	} catch (StandardException &ex) {
671: 		if (transaction.IsAutoCommit()) {
672: 			transaction.Rollback();
673: 		}
674: 		throw;
675: 	} catch (std::exception &ex) {
676: 		if (transaction.IsAutoCommit()) {
677: 			transaction.Rollback();
678: 		} else {
679: 			ActiveTransaction().Invalidate();
680: 		}
681: 		throw;
682: 	}
683: 	if (transaction.IsAutoCommit()) {
684: 		transaction.Commit();
685: 	}
686: }
687: 
688: void ClientContext::RunFunctionInTransaction(const std::function<void(void)> &fun, bool requires_valid_transaction) {
689: 	auto lock = LockContext();
690: 	RunFunctionInTransactionInternal(*lock, fun, requires_valid_transaction);
691: }
692: 
693: unique_ptr<TableDescription> ClientContext::TableInfo(const string &schema_name, const string &table_name) {
694: 	unique_ptr<TableDescription> result;
695: 	RunFunctionInTransaction([&]() {
696: 		// obtain the table info
697: 		auto &catalog = Catalog::GetCatalog(*this);
698: 		auto table = catalog.GetEntry<TableCatalogEntry>(*this, schema_name, table_name, true);
699: 		if (!table) {
700: 			return;
701: 		}
702: 		// write the table info to the result
703: 		result = make_unique<TableDescription>();
704: 		result->schema = schema_name;
705: 		result->table = table_name;
706: 		for (auto &column : table->columns) {
707: 			result->columns.emplace_back(column.name, column.type);
708: 		}
709: 	});
710: 	return result;
711: }
712: 
713: void ClientContext::Append(TableDescription &description, DataChunk &chunk) {
714: 	RunFunctionInTransaction([&]() {
715: 		auto &catalog = Catalog::GetCatalog(*this);
716: 		auto table_entry = catalog.GetEntry<TableCatalogEntry>(*this, description.schema, description.table);
717: 		// verify that the table columns and types match up
718: 		if (description.columns.size() != table_entry->columns.size()) {
719: 			throw Exception("Failed to append: table entry has different number of columns!");
720: 		}
721: 		for (idx_t i = 0; i < description.columns.size(); i++) {
722: 			if (description.columns[i].type != table_entry->columns[i].type) {
723: 				throw Exception("Failed to append: table entry has different number of columns!");
724: 			}
725: 		}
726: 		table_entry->storage->Append(*table_entry, *this, chunk);
727: 	});
728: }
729: 
730: void ClientContext::TryBindRelation(Relation &relation, vector<ColumnDefinition> &result_columns) {
731: 	RunFunctionInTransaction([&]() {
732: 		// bind the expressions
733: 		Binder binder(*this);
734: 		auto result = relation.Bind(binder);
735: 		D_ASSERT(result.names.size() == result.types.size());
736: 		for (idx_t i = 0; i < result.names.size(); i++) {
737: 			result_columns.emplace_back(result.names[i], result.types[i]);
738: 		}
739: 	});
740: }
741: 
742: unique_ptr<QueryResult> ClientContext::Execute(const shared_ptr<Relation> &relation) {
743: 	auto lock = LockContext();
744: 	string query;
745: 	if (query_verification_enabled) {
746: 		// run the ToString method of any relation we run, mostly to ensure it doesn't crash
747: 		relation->ToString();
748: 		if (relation->IsReadOnly()) {
749: 			// verify read only statements by running a select statement
750: 			auto select = make_unique<SelectStatement>();
751: 			select->node = relation->GetQueryNode();
752: 			RunStatement(*lock, query, move(select), false);
753: 		}
754: 	}
755: 	auto &expected_columns = relation->Columns();
756: 	auto relation_stmt = make_unique<RelationStatement>(relation);
757: 	auto result = RunStatement(*lock, query, move(relation_stmt), false);
758: 	if (!result->success) {
759: 		return result;
760: 	}
761: 	// verify that the result types and result names of the query match the expected result types/names
762: 	if (result->types.size() == expected_columns.size()) {
763: 		bool mismatch = false;
764: 		for (idx_t i = 0; i < result->types.size(); i++) {
765: 			if (result->types[i] != expected_columns[i].type || result->names[i] != expected_columns[i].name) {
766: 				mismatch = true;
767: 				break;
768: 			}
769: 		}
770: 		if (!mismatch) {
771: 			// all is as expected: return the result
772: 			return result;
773: 		}
774: 	}
775: 	// result mismatch
776: 	string err_str = "Result mismatch in query!\nExpected the following columns: ";
777: 	for (idx_t i = 0; i < expected_columns.size(); i++) {
778: 		err_str += i == 0 ? "[" : ", ";
779: 		err_str += expected_columns[i].name + " " + expected_columns[i].type.ToString();
780: 	}
781: 	err_str += "]\nBut result contained the following: ";
782: 	for (idx_t i = 0; i < result->types.size(); i++) {
783: 		err_str += i == 0 ? "[" : ", ";
784: 		err_str += result->names[i] + " " + result->types[i].ToString();
785: 	}
786: 	err_str += "]";
787: 	return make_unique<MaterializedQueryResult>(err_str);
788: }
789: 
790: } // namespace duckdb
[end of src/main/client_context.cpp]
[start of src/planner/binder.cpp]
1: #include "duckdb/planner/binder.hpp"
2: 
3: #include "duckdb/parser/statement/list.hpp"
4: #include "duckdb/parser/query_node/select_node.hpp"
5: #include "duckdb/planner/bound_query_node.hpp"
6: #include "duckdb/planner/bound_tableref.hpp"
7: #include "duckdb/planner/expression.hpp"
8: #include "duckdb/planner/operator/logical_sample.hpp"
9: 
10: #include <algorithm>
11: 
12: namespace duckdb {
13: 
14: Binder::Binder(ClientContext &context, Binder *parent_p, bool inherit_ctes_p)
15:     : context(context), read_only(true), requires_valid_transaction(true), allow_stream_result(false), parent(parent_p),
16:       bound_tables(0), inherit_ctes(inherit_ctes_p) {
17: 	if (parent) {
18: 		// We have to inherit macro parameter bindings from the parent binder, if there is a parent.
19: 		macro_binding = parent->macro_binding;
20: 		if (inherit_ctes) {
21: 			// We have to inherit CTE bindings from the parent bind_context, if there is a parent.
22: 			bind_context.SetCTEBindings(parent->bind_context.GetCTEBindings());
23: 			bind_context.cte_references = parent->bind_context.cte_references;
24: 			parameters = parent->parameters;
25: 		}
26: 	}
27: }
28: 
29: BoundStatement Binder::Bind(SQLStatement &statement) {
30: 	root_statement = &statement;
31: 	switch (statement.type) {
32: 	case StatementType::SELECT_STATEMENT:
33: 		return Bind((SelectStatement &)statement);
34: 	case StatementType::INSERT_STATEMENT:
35: 		return Bind((InsertStatement &)statement);
36: 	case StatementType::COPY_STATEMENT:
37: 		return Bind((CopyStatement &)statement);
38: 	case StatementType::DELETE_STATEMENT:
39: 		return Bind((DeleteStatement &)statement);
40: 	case StatementType::UPDATE_STATEMENT:
41: 		return Bind((UpdateStatement &)statement);
42: 	case StatementType::RELATION_STATEMENT:
43: 		return Bind((RelationStatement &)statement);
44: 	case StatementType::CREATE_STATEMENT:
45: 		return Bind((CreateStatement &)statement);
46: 	case StatementType::DROP_STATEMENT:
47: 		return Bind((DropStatement &)statement);
48: 	case StatementType::ALTER_STATEMENT:
49: 		return Bind((AlterStatement &)statement);
50: 	case StatementType::TRANSACTION_STATEMENT:
51: 		return Bind((TransactionStatement &)statement);
52: 	case StatementType::PRAGMA_STATEMENT:
53: 		return Bind((PragmaStatement &)statement);
54: 	case StatementType::EXPLAIN_STATEMENT:
55: 		return Bind((ExplainStatement &)statement);
56: 	case StatementType::VACUUM_STATEMENT:
57: 		return Bind((VacuumStatement &)statement);
58: 	case StatementType::SHOW_STATEMENT:
59: 		return Bind((ShowStatement &)statement);
60: 	case StatementType::CALL_STATEMENT:
61: 		return Bind((CallStatement &)statement);
62: 	case StatementType::EXPORT_STATEMENT:
63: 		return Bind((ExportStatement &)statement);
64: 	case StatementType::SET_STATEMENT:
65: 		return Bind((SetStatement &)statement);
66: 	default:
67: 		throw NotImplementedException("Unimplemented statement type \"%s\" for Bind",
68: 		                              StatementTypeToString(statement.type));
69: 	}
70: }
71: 
72: unique_ptr<BoundQueryNode> Binder::BindNode(QueryNode &node) {
73: 	// first we visit the set of CTEs and add them to the bind context
74: 	for (auto &cte_it : node.cte_map) {
75: 		AddCTE(cte_it.first, cte_it.second.get());
76: 	}
77: 	// now we bind the node
78: 	unique_ptr<BoundQueryNode> result;
79: 	switch (node.type) {
80: 	case QueryNodeType::SELECT_NODE:
81: 		result = BindNode((SelectNode &)node);
82: 		break;
83: 	case QueryNodeType::RECURSIVE_CTE_NODE:
84: 		result = BindNode((RecursiveCTENode &)node);
85: 		break;
86: 	default:
87: 		D_ASSERT(node.type == QueryNodeType::SET_OPERATION_NODE);
88: 		result = BindNode((SetOperationNode &)node);
89: 		break;
90: 	}
91: 	return result;
92: }
93: 
94: BoundStatement Binder::Bind(QueryNode &node) {
95: 	auto bound_node = BindNode(node);
96: 
97: 	BoundStatement result;
98: 	result.names = bound_node->names;
99: 	result.types = bound_node->types;
100: 
101: 	// and plan it
102: 	result.plan = CreatePlan(*bound_node);
103: 	return result;
104: }
105: 
106: unique_ptr<LogicalOperator> Binder::CreatePlan(BoundQueryNode &node) {
107: 	switch (node.type) {
108: 	case QueryNodeType::SELECT_NODE:
109: 		return CreatePlan((BoundSelectNode &)node);
110: 	case QueryNodeType::SET_OPERATION_NODE:
111: 		return CreatePlan((BoundSetOperationNode &)node);
112: 	case QueryNodeType::RECURSIVE_CTE_NODE:
113: 		return CreatePlan((BoundRecursiveCTENode &)node);
114: 	default:
115: 		throw Exception("Unsupported bound query node type");
116: 	}
117: }
118: 
119: unique_ptr<BoundTableRef> Binder::Bind(TableRef &ref) {
120: 	unique_ptr<BoundTableRef> result;
121: 	switch (ref.type) {
122: 	case TableReferenceType::BASE_TABLE:
123: 		result = Bind((BaseTableRef &)ref);
124: 		break;
125: 	case TableReferenceType::CROSS_PRODUCT:
126: 		result = Bind((CrossProductRef &)ref);
127: 		break;
128: 	case TableReferenceType::JOIN:
129: 		result = Bind((JoinRef &)ref);
130: 		break;
131: 	case TableReferenceType::SUBQUERY:
132: 		result = Bind((SubqueryRef &)ref);
133: 		break;
134: 	case TableReferenceType::EMPTY:
135: 		result = Bind((EmptyTableRef &)ref);
136: 		break;
137: 	case TableReferenceType::TABLE_FUNCTION:
138: 		result = Bind((TableFunctionRef &)ref);
139: 		break;
140: 	case TableReferenceType::EXPRESSION_LIST:
141: 		result = Bind((ExpressionListRef &)ref);
142: 		break;
143: 	default:
144: 		throw Exception("Unknown table ref type");
145: 	}
146: 	result->sample = move(ref.sample);
147: 	return result;
148: }
149: 
150: unique_ptr<LogicalOperator> Binder::CreatePlan(BoundTableRef &ref) {
151: 	unique_ptr<LogicalOperator> root;
152: 	switch (ref.type) {
153: 	case TableReferenceType::BASE_TABLE:
154: 		root = CreatePlan((BoundBaseTableRef &)ref);
155: 		break;
156: 	case TableReferenceType::SUBQUERY:
157: 		root = CreatePlan((BoundSubqueryRef &)ref);
158: 		break;
159: 	case TableReferenceType::JOIN:
160: 		root = CreatePlan((BoundJoinRef &)ref);
161: 		break;
162: 	case TableReferenceType::CROSS_PRODUCT:
163: 		root = CreatePlan((BoundCrossProductRef &)ref);
164: 		break;
165: 	case TableReferenceType::TABLE_FUNCTION:
166: 		root = CreatePlan((BoundTableFunction &)ref);
167: 		break;
168: 	case TableReferenceType::EMPTY:
169: 		root = CreatePlan((BoundEmptyTableRef &)ref);
170: 		break;
171: 	case TableReferenceType::EXPRESSION_LIST:
172: 		root = CreatePlan((BoundExpressionListRef &)ref);
173: 		break;
174: 	case TableReferenceType::CTE:
175: 		root = CreatePlan((BoundCTERef &)ref);
176: 		break;
177: 	default:
178: 		throw Exception("Unsupported bound table ref type type");
179: 	}
180: 	// plan the sample clause
181: 	if (ref.sample) {
182: 		root = make_unique<LogicalSample>(move(ref.sample), move(root));
183: 	}
184: 	return root;
185: }
186: 
187: void Binder::AddCTE(const string &name, CommonTableExpressionInfo *info) {
188: 	D_ASSERT(info);
189: 	D_ASSERT(!name.empty());
190: 	auto entry = CTE_bindings.find(name);
191: 	if (entry != CTE_bindings.end()) {
192: 		throw BinderException("Duplicate CTE \"%s\" in query!", name);
193: 	}
194: 	CTE_bindings[name] = info;
195: }
196: 
197: CommonTableExpressionInfo *Binder::FindCTE(const string &name, bool skip) {
198: 	auto entry = CTE_bindings.find(name);
199: 	if (entry != CTE_bindings.end()) {
200: 		if (!skip || entry->second->query->node->type == QueryNodeType::RECURSIVE_CTE_NODE) {
201: 			return entry->second;
202: 		}
203: 	}
204: 	if (parent && inherit_ctes) {
205: 		return parent->FindCTE(name, name == alias);
206: 	}
207: 	return nullptr;
208: }
209: 
210: bool Binder::CTEIsAlreadyBound(CommonTableExpressionInfo *cte) {
211: 	if (bound_ctes.find(cte) != bound_ctes.end()) {
212: 		return true;
213: 	}
214: 	if (parent && inherit_ctes) {
215: 		return parent->CTEIsAlreadyBound(cte);
216: 	}
217: 	return false;
218: }
219: 
220: idx_t Binder::GenerateTableIndex() {
221: 	if (parent) {
222: 		return parent->GenerateTableIndex();
223: 	}
224: 	return bound_tables++;
225: }
226: 
227: void Binder::PushExpressionBinder(ExpressionBinder *binder) {
228: 	GetActiveBinders().push_back(binder);
229: }
230: 
231: void Binder::PopExpressionBinder() {
232: 	D_ASSERT(HasActiveBinder());
233: 	GetActiveBinders().pop_back();
234: }
235: 
236: void Binder::SetActiveBinder(ExpressionBinder *binder) {
237: 	D_ASSERT(HasActiveBinder());
238: 	GetActiveBinders().back() = binder;
239: }
240: 
241: ExpressionBinder *Binder::GetActiveBinder() {
242: 	return GetActiveBinders().back();
243: }
244: 
245: bool Binder::HasActiveBinder() {
246: 	return !GetActiveBinders().empty();
247: }
248: 
249: vector<ExpressionBinder *> &Binder::GetActiveBinders() {
250: 	if (parent) {
251: 		return parent->GetActiveBinders();
252: 	}
253: 	return active_binders;
254: }
255: 
256: void Binder::MoveCorrelatedExpressions(Binder &other) {
257: 	MergeCorrelatedColumns(other.correlated_columns);
258: 	other.correlated_columns.clear();
259: }
260: 
261: void Binder::MergeCorrelatedColumns(vector<CorrelatedColumnInfo> &other) {
262: 	for (idx_t i = 0; i < other.size(); i++) {
263: 		AddCorrelatedColumn(other[i]);
264: 	}
265: }
266: 
267: void Binder::AddCorrelatedColumn(const CorrelatedColumnInfo &info) {
268: 	// we only add correlated columns to the list if they are not already there
269: 	if (std::find(correlated_columns.begin(), correlated_columns.end(), info) == correlated_columns.end()) {
270: 		correlated_columns.push_back(info);
271: 	}
272: }
273: 
274: string Binder::FormatError(ParsedExpression &expr_context, const string &message) {
275: 	return FormatError(expr_context.query_location, message);
276: }
277: 
278: string Binder::FormatError(TableRef &ref_context, const string &message) {
279: 	return FormatError(ref_context.query_location, message);
280: }
281: 
282: string Binder::FormatError(idx_t query_location, const string &message) {
283: 	QueryErrorContext context(root_statement, query_location);
284: 	return context.FormatError(message);
285: }
286: 
287: } // namespace duckdb
[end of src/planner/binder.cpp]
[start of src/planner/binder/expression/bind_subquery_expression.cpp]
1: #include "duckdb/parser/expression/subquery_expression.hpp"
2: #include "duckdb/planner/binder.hpp"
3: #include "duckdb/planner/expression/bound_cast_expression.hpp"
4: #include "duckdb/planner/expression/bound_subquery_expression.hpp"
5: #include "duckdb/planner/expression_binder.hpp"
6: 
7: namespace duckdb {
8: 
9: class BoundSubqueryNode : public QueryNode {
10: public:
11: 	BoundSubqueryNode(unique_ptr<Binder> subquery_binder, unique_ptr<BoundQueryNode> bound_node,
12: 	                  unique_ptr<SelectStatement> subquery)
13: 	    : QueryNode(QueryNodeType::BOUND_SUBQUERY_NODE), subquery_binder(move(subquery_binder)),
14: 	      bound_node(move(bound_node)), subquery(move(subquery)) {
15: 	}
16: 
17: 	unique_ptr<Binder> subquery_binder;
18: 	unique_ptr<BoundQueryNode> bound_node;
19: 	unique_ptr<SelectStatement> subquery;
20: 
21: 	const vector<unique_ptr<ParsedExpression>> &GetSelectList() const override {
22: 		throw Exception("Cannot get select list of bound subquery node");
23: 	}
24: 
25: 	unique_ptr<QueryNode> Copy() override {
26: 		throw Exception("Cannot copy bound subquery node");
27: 	}
28: };
29: 
30: BindResult ExpressionBinder::BindExpression(SubqueryExpression &expr, idx_t depth) {
31: 	if (expr.subquery->node->type != QueryNodeType::BOUND_SUBQUERY_NODE) {
32: 		D_ASSERT(depth == 0);
33: 		// first bind the actual subquery in a new binder
34: 		auto subquery_binder = make_unique<Binder>(context, &binder);
35: 		auto bound_node = subquery_binder->BindNode(*expr.subquery->node);
36: 		// check the correlated columns of the subquery for correlated columns with depth > 1
37: 		for (idx_t i = 0; i < subquery_binder->correlated_columns.size(); i++) {
38: 			CorrelatedColumnInfo corr = subquery_binder->correlated_columns[i];
39: 			if (corr.depth > 1) {
40: 				// depth > 1, the column references the query ABOVE the current one
41: 				// add to the set of correlated columns for THIS query
42: 				corr.depth -= 1;
43: 				binder.AddCorrelatedColumn(corr);
44: 			}
45: 		}
46: 		if (expr.subquery_type != SubqueryType::EXISTS && bound_node->types.size() > 1) {
47: 			throw BinderException("Subquery returns %zu columns - expected 1", bound_node->types.size());
48: 		}
49: 		auto prior_subquery = move(expr.subquery);
50: 		expr.subquery = make_unique<SelectStatement>();
51: 		expr.subquery->node =
52: 		    make_unique<BoundSubqueryNode>(move(subquery_binder), move(bound_node), move(prior_subquery));
53: 	}
54: 	// now bind the child node of the subquery
55: 	if (expr.child) {
56: 		// first bind the children of the subquery, if any
57: 		string error = Bind(&expr.child, depth);
58: 		if (!error.empty()) {
59: 			return BindResult(error);
60: 		}
61: 	}
62: 	// both binding the child and binding the subquery was successful
63: 	D_ASSERT(expr.subquery->node->type == QueryNodeType::BOUND_SUBQUERY_NODE);
64: 	auto bound_subquery = (BoundSubqueryNode *)expr.subquery->node.get();
65: 	auto child = (BoundExpression *)expr.child.get();
66: 	auto subquery_binder = move(bound_subquery->subquery_binder);
67: 	auto bound_node = move(bound_subquery->bound_node);
68: 	LogicalType return_type =
69: 	    expr.subquery_type == SubqueryType::SCALAR ? bound_node->types[0] : LogicalType(LogicalTypeId::BOOLEAN);
70: 	if (return_type.id() == LogicalTypeId::UNKNOWN) {
71: 		throw BinderException("Could not determine type of parameters: try adding explicit type casts");
72: 	}
73: 
74: 	auto result = make_unique<BoundSubqueryExpression>(return_type);
75: 	if (expr.subquery_type == SubqueryType::ANY) {
76: 		// ANY comparison
77: 		// cast child and subquery child to equivalent types
78: 		D_ASSERT(bound_node->types.size() == 1);
79: 		auto compare_type = LogicalType::MaxLogicalType(child->expr->return_type, bound_node->types[0]);
80: 		child->expr = BoundCastExpression::AddCastToType(move(child->expr), compare_type);
81: 		result->child_type = bound_node->types[0];
82: 		result->child_target = compare_type;
83: 	}
84: 	result->binder = move(subquery_binder);
85: 	result->subquery = move(bound_node);
86: 	result->subquery_type = expr.subquery_type;
87: 	result->child = child ? move(child->expr) : nullptr;
88: 	result->comparison_type = expr.comparison_type;
89: 
90: 	return BindResult(move(result));
91: }
92: 
93: } // namespace duckdb
[end of src/planner/binder/expression/bind_subquery_expression.cpp]
[start of src/planner/binder/query_node/bind_recursive_cte_node.cpp]
1: #include "duckdb/parser/expression/constant_expression.hpp"
2: #include "duckdb/parser/expression_map.hpp"
3: #include "duckdb/parser/query_node/select_node.hpp"
4: #include "duckdb/parser/query_node/recursive_cte_node.hpp"
5: #include "duckdb/planner/binder.hpp"
6: #include "duckdb/planner/query_node/bound_recursive_cte_node.hpp"
7: #include "duckdb/planner/query_node/bound_select_node.hpp"
8: 
9: namespace duckdb {
10: 
11: unique_ptr<BoundQueryNode> Binder::BindNode(RecursiveCTENode &statement) {
12: 	auto result = make_unique<BoundRecursiveCTENode>();
13: 
14: 	// first recursively visit the recursive CTE operations
15: 	// the left side is visited first and is added to the BindContext of the right side
16: 	D_ASSERT(statement.left);
17: 	D_ASSERT(statement.right);
18: 
19: 	result->ctename = statement.ctename;
20: 	result->union_all = statement.union_all;
21: 	result->setop_index = GenerateTableIndex();
22: 
23: 	result->left_binder = make_unique<Binder>(context, this);
24: 	result->left = result->left_binder->BindNode(*statement.left);
25: 
26: 	// the result types of the CTE are the types of the LHS
27: 	result->types = result->left->types;
28: 	// names are picked from the LHS, unless aliases are explicitly specified
29: 	result->names = result->left->names;
30: 	for (idx_t i = 0; i < statement.aliases.size() && i < result->names.size(); i++) {
31: 		result->names[i] = statement.aliases[i];
32: 	}
33: 
34: 	// This allows the right side to reference the CTE recursively
35: 	bind_context.AddGenericBinding(result->setop_index, statement.ctename, result->names, result->types);
36: 
37: 	result->right_binder = make_unique<Binder>(context, this);
38: 
39: 	// Add bindings of left side to temporary CTE bindings context
40: 	result->right_binder->bind_context.AddCTEBinding(result->setop_index, statement.ctename, result->names,
41: 	                                                 result->types);
42: 	result->right = result->right_binder->BindNode(*statement.right);
43: 
44: 	// move the correlated expressions from the child binders to this binder
45: 	MoveCorrelatedExpressions(*result->left_binder);
46: 	MoveCorrelatedExpressions(*result->right_binder);
47: 
48: 	// now both sides have been bound we can resolve types
49: 	if (result->left->types.size() != result->right->types.size()) {
50: 		throw BinderException("Set operations can only apply to expressions with the "
51: 		                      "same number of result columns");
52: 	}
53: 
54: 	if (!statement.modifiers.empty()) {
55: 		throw NotImplementedException("FIXME: bind modifiers in recursive CTE");
56: 	}
57: 
58: 	return move(result);
59: }
60: 
61: } // namespace duckdb
[end of src/planner/binder/query_node/bind_recursive_cte_node.cpp]
[start of src/planner/binder/query_node/bind_select_node.cpp]
1: #include "duckdb/common/limits.hpp"
2: #include "duckdb/common/string_util.hpp"
3: #include "duckdb/execution/expression_executor.hpp"
4: #include "duckdb/main/config.hpp"
5: #include "duckdb/parser/expression/columnref_expression.hpp"
6: #include "duckdb/parser/expression/comparison_expression.hpp"
7: #include "duckdb/parser/expression/constant_expression.hpp"
8: #include "duckdb/parser/expression/subquery_expression.hpp"
9: #include "duckdb/parser/expression/table_star_expression.hpp"
10: #include "duckdb/parser/query_node/select_node.hpp"
11: #include "duckdb/parser/tableref/joinref.hpp"
12: #include "duckdb/planner/binder.hpp"
13: #include "duckdb/planner/expression_binder/constant_binder.hpp"
14: #include "duckdb/planner/expression_binder/group_binder.hpp"
15: #include "duckdb/planner/expression_binder/having_binder.hpp"
16: #include "duckdb/planner/expression_binder/order_binder.hpp"
17: #include "duckdb/planner/expression_binder/select_binder.hpp"
18: #include "duckdb/planner/expression_binder/where_binder.hpp"
19: #include "duckdb/planner/query_node/bound_select_node.hpp"
20: #include "duckdb/planner/expression_binder/aggregate_binder.hpp"
21: 
22: namespace duckdb {
23: unique_ptr<Expression> Binder::BindFilter(unique_ptr<ParsedExpression> condition) {
24: 	WhereBinder where_binder(*this, context);
25: 	return where_binder.Bind(condition);
26: }
27: 
28: unique_ptr<Expression> Binder::BindOrderExpression(OrderBinder &order_binder, unique_ptr<ParsedExpression> expr) {
29: 	// we treat the Distinct list as a order by
30: 	auto bound_expr = order_binder.Bind(move(expr));
31: 	if (!bound_expr) {
32: 		// DISTINCT ON non-integer constant
33: 		// remove the expression from the DISTINCT ON list
34: 		return nullptr;
35: 	}
36: 	D_ASSERT(bound_expr->type == ExpressionType::BOUND_COLUMN_REF);
37: 	return bound_expr;
38: }
39: 
40: unique_ptr<Expression> BindDelimiter(ClientContext &context, unique_ptr<ParsedExpression> delimiter,
41:                                      int64_t &delimiter_value) {
42: 	Binder new_binder(context);
43: 	ExpressionBinder expr_binder(new_binder, context);
44: 	expr_binder.target_type = LogicalType::UBIGINT;
45: 	auto expr = expr_binder.Bind(delimiter);
46: 	if (expr->IsFoldable()) {
47: 		//! this is a constant
48: 		Value value = ExpressionExecutor::EvaluateScalar(*expr).CastAs(LogicalType::BIGINT);
49: 		delimiter_value = value.GetValue<int64_t>();
50: 		return nullptr;
51: 	}
52: 	return expr;
53: }
54: 
55: unique_ptr<BoundResultModifier> Binder::BindLimit(LimitModifier &limit_mod) {
56: 	auto result = make_unique<BoundLimitModifier>();
57: 	if (limit_mod.limit) {
58: 		result->limit = BindDelimiter(context, move(limit_mod.limit), result->limit_val);
59: 	}
60: 	if (limit_mod.offset) {
61: 		result->offset = BindDelimiter(context, move(limit_mod.offset), result->offset_val);
62: 	}
63: 	return move(result);
64: }
65: 
66: void Binder::BindModifiers(OrderBinder &order_binder, QueryNode &statement, BoundQueryNode &result) {
67: 	for (auto &mod : statement.modifiers) {
68: 		unique_ptr<BoundResultModifier> bound_modifier;
69: 		switch (mod->type) {
70: 		case ResultModifierType::DISTINCT_MODIFIER: {
71: 			auto &distinct = (DistinctModifier &)*mod;
72: 			auto bound_distinct = make_unique<BoundDistinctModifier>();
73: 			for (auto &distinct_on_target : distinct.distinct_on_targets) {
74: 				auto expr = BindOrderExpression(order_binder, move(distinct_on_target));
75: 				if (!expr) {
76: 					continue;
77: 				}
78: 				bound_distinct->target_distincts.push_back(move(expr));
79: 			}
80: 			bound_modifier = move(bound_distinct);
81: 			break;
82: 		}
83: 		case ResultModifierType::ORDER_MODIFIER: {
84: 			auto &order = (OrderModifier &)*mod;
85: 			auto bound_order = make_unique<BoundOrderModifier>();
86: 			auto &config = DBConfig::GetConfig(context);
87: 			for (auto &order_node : order.orders) {
88: 				auto order_expression = BindOrderExpression(order_binder, move(order_node.expression));
89: 				if (!order_expression) {
90: 					continue;
91: 				}
92: 				auto type = order_node.type == OrderType::ORDER_DEFAULT ? config.default_order_type : order_node.type;
93: 				auto null_order = order_node.null_order == OrderByNullType::ORDER_DEFAULT ? config.default_null_order
94: 				                                                                          : order_node.null_order;
95: 				bound_order->orders.emplace_back(type, null_order, move(order_expression));
96: 			}
97: 			if (!bound_order->orders.empty()) {
98: 				bound_modifier = move(bound_order);
99: 			}
100: 			break;
101: 		}
102: 		case ResultModifierType::LIMIT_MODIFIER:
103: 			bound_modifier = BindLimit((LimitModifier &)*mod);
104: 			break;
105: 		default:
106: 			throw Exception("Unsupported result modifier");
107: 		}
108: 		if (bound_modifier) {
109: 			result.modifiers.push_back(move(bound_modifier));
110: 		}
111: 	}
112: }
113: 
114: void Binder::BindModifierTypes(BoundQueryNode &result, const vector<LogicalType> &sql_types, idx_t projection_index) {
115: 	for (auto &bound_mod : result.modifiers) {
116: 		switch (bound_mod->type) {
117: 		case ResultModifierType::DISTINCT_MODIFIER: {
118: 			auto &distinct = (BoundDistinctModifier &)*bound_mod;
119: 			if (distinct.target_distincts.empty()) {
120: 				// DISTINCT without a target: push references to the standard select list
121: 				for (idx_t i = 0; i < sql_types.size(); i++) {
122: 					distinct.target_distincts.push_back(
123: 					    make_unique<BoundColumnRefExpression>(sql_types[i], ColumnBinding(projection_index, i)));
124: 				}
125: 			} else {
126: 				// DISTINCT with target list: set types
127: 				for (auto &expr : distinct.target_distincts) {
128: 					D_ASSERT(expr->type == ExpressionType::BOUND_COLUMN_REF);
129: 					auto &bound_colref = (BoundColumnRefExpression &)*expr;
130: 					if (bound_colref.binding.column_index == INVALID_INDEX) {
131: 						throw BinderException("Ambiguous name in DISTINCT ON!");
132: 					}
133: 					D_ASSERT(bound_colref.binding.column_index < sql_types.size());
134: 					bound_colref.return_type = sql_types[bound_colref.binding.column_index];
135: 				}
136: 			}
137: 			for (auto &target_distinct : distinct.target_distincts) {
138: 				auto &bound_colref = (BoundColumnRefExpression &)*target_distinct;
139: 				auto sql_type = sql_types[bound_colref.binding.column_index];
140: 				if (sql_type.id() == LogicalTypeId::VARCHAR) {
141: 					target_distinct =
142: 					    ExpressionBinder::PushCollation(context, move(target_distinct), sql_type.collation(), true);
143: 				}
144: 			}
145: 			break;
146: 		}
147: 		case ResultModifierType::ORDER_MODIFIER: {
148: 			auto &order = (BoundOrderModifier &)*bound_mod;
149: 			for (auto &order_node : order.orders) {
150: 				auto &expr = order_node.expression;
151: 				D_ASSERT(expr->type == ExpressionType::BOUND_COLUMN_REF);
152: 				auto &bound_colref = (BoundColumnRefExpression &)*expr;
153: 				if (bound_colref.binding.column_index == INVALID_INDEX) {
154: 					throw BinderException("Ambiguous name in ORDER BY!");
155: 				}
156: 				D_ASSERT(bound_colref.binding.column_index < sql_types.size());
157: 				auto sql_type = sql_types[bound_colref.binding.column_index];
158: 				bound_colref.return_type = sql_types[bound_colref.binding.column_index];
159: 				if (sql_type.id() == LogicalTypeId::VARCHAR) {
160: 					order_node.expression =
161: 					    ExpressionBinder::PushCollation(context, move(order_node.expression), sql_type.collation());
162: 				}
163: 			}
164: 			break;
165: 		}
166: 		default:
167: 			break;
168: 		}
169: 	}
170: }
171: 
172: unique_ptr<BoundQueryNode> Binder::BindNode(SelectNode &statement) {
173: 	auto result = make_unique<BoundSelectNode>();
174: 	result->projection_index = GenerateTableIndex();
175: 	result->group_index = GenerateTableIndex();
176: 	result->aggregate_index = GenerateTableIndex();
177: 	result->window_index = GenerateTableIndex();
178: 	result->unnest_index = GenerateTableIndex();
179: 	result->prune_index = GenerateTableIndex();
180: 
181: 	// first bind the FROM table statement
182: 	result->from_table = Bind(*statement.from_table);
183: 
184: 	// bind the sample clause
185: 	if (statement.sample) {
186: 		result->sample_options = move(statement.sample);
187: 	}
188: 
189: 	// visit the select list and expand any "*" statements
190: 	vector<unique_ptr<ParsedExpression>> new_select_list;
191: 	for (auto &select_element : statement.select_list) {
192: 		if (select_element->GetExpressionType() == ExpressionType::STAR) {
193: 			// * statement, expand to all columns from the FROM clause
194: 			bind_context.GenerateAllColumnExpressions(new_select_list);
195: 		} else if (select_element->GetExpressionType() == ExpressionType::TABLE_STAR) {
196: 			auto table_star = (TableStarExpression *)select_element.get();
197: 			bind_context.GenerateAllColumnExpressions(new_select_list, table_star->relation_name);
198: 		} else {
199: 			// regular statement, add it to the list
200: 			new_select_list.push_back(move(select_element));
201: 		}
202: 	}
203: 	statement.select_list = move(new_select_list);
204: 
205: 	// create a mapping of (alias -> index) and a mapping of (Expression -> index) for the SELECT list
206: 	unordered_map<string, idx_t> alias_map;
207: 	expression_map_t<idx_t> projection_map;
208: 	for (idx_t i = 0; i < statement.select_list.size(); i++) {
209: 		auto &expr = statement.select_list[i];
210: 		result->names.push_back(expr->GetName());
211: 		ExpressionBinder::BindTableNames(*this, *expr);
212: 		if (!expr->alias.empty()) {
213: 			alias_map[expr->alias] = i;
214: 			result->names[i] = expr->alias;
215: 		}
216: 		projection_map[expr.get()] = i;
217: 		result->original_expressions.push_back(expr->Copy());
218: 	}
219: 	result->column_count = statement.select_list.size();
220: 
221: 	// first visit the WHERE clause
222: 	// the WHERE clause happens before the GROUP BY, PROJECTION or HAVING clauses
223: 	if (statement.where_clause) {
224: 		result->where_clause = BindFilter(move(statement.where_clause));
225: 	}
226: 
227: 	// now bind all the result modifiers; including DISTINCT and ORDER BY targets
228: 	OrderBinder order_binder({this}, result->projection_index, statement, alias_map, projection_map);
229: 	BindModifiers(order_binder, statement, *result);
230: 
231: 	vector<unique_ptr<ParsedExpression>> unbound_groups;
232: 	BoundGroupInformation info;
233: 	if (!statement.groups.empty()) {
234: 		// the statement has a GROUP BY clause, bind it
235: 		unbound_groups.resize(statement.groups.size());
236: 		GroupBinder group_binder(*this, context, statement, result->group_index, alias_map, info.alias_map);
237: 		for (idx_t i = 0; i < statement.groups.size(); i++) {
238: 
239: 			// we keep a copy of the unbound expression;
240: 			// we keep the unbound copy around to check for group references in the SELECT and HAVING clause
241: 			// the reason we want the unbound copy is because we want to figure out whether an expression
242: 			// is a group reference BEFORE binding in the SELECT/HAVING binder
243: 			group_binder.unbound_expression = statement.groups[i]->Copy();
244: 			group_binder.bind_index = i;
245: 
246: 			// bind the groups
247: 			LogicalType group_type;
248: 			auto bound_expr = group_binder.Bind(statement.groups[i], &group_type);
249: 			D_ASSERT(bound_expr->return_type.id() != LogicalTypeId::INVALID);
250: 
251: 			// push a potential collation, if necessary
252: 			bound_expr = ExpressionBinder::PushCollation(context, move(bound_expr), group_type.collation(), true);
253: 			result->groups.push_back(move(bound_expr));
254: 
255: 			// in the unbound expression we DO bind the table names of any ColumnRefs
256: 			// we do this to make sure that "table.a" and "a" are treated the same
257: 			// if we wouldn't do this then (SELECT test.a FROM test GROUP BY a) would not work because "test.a" <> "a"
258: 			// hence we convert "a" -> "test.a" in the unbound expression
259: 			unbound_groups[i] = move(group_binder.unbound_expression);
260: 			ExpressionBinder::BindTableNames(*this, *unbound_groups[i]);
261: 			info.map[unbound_groups[i].get()] = i;
262: 		}
263: 	}
264: 
265: 	// bind the HAVING clause, if any
266: 	if (statement.having) {
267: 		HavingBinder having_binder(*this, context, *result, info, alias_map);
268: 		ExpressionBinder::BindTableNames(*this, *statement.having, &alias_map);
269: 		result->having = having_binder.Bind(statement.having);
270: 	}
271: 
272: 	// after that, we bind to the SELECT list
273: 	SelectBinder select_binder(*this, context, *result, info);
274: 	vector<LogicalType> internal_sql_types;
275: 	for (idx_t i = 0; i < statement.select_list.size(); i++) {
276: 		LogicalType result_type;
277: 		auto expr = select_binder.Bind(statement.select_list[i], &result_type);
278: 		if (statement.aggregate_handling == AggregateHandling::FORCE_AGGREGATES && select_binder.BoundColumns()) {
279: 			if (select_binder.BoundAggregates()) {
280: 				throw BinderException("Cannot mix aggregates with non-aggregated columns!");
281: 			}
282: 			// we are forcing aggregates, and the node has columns bound
283: 			// this entry becomes a group
284: 			auto group_ref = make_unique<BoundColumnRefExpression>(
285: 			    expr->return_type, ColumnBinding(result->group_index, result->groups.size()));
286: 			result->groups.push_back(move(expr));
287: 			expr = move(group_ref);
288: 		}
289: 		result->select_list.push_back(move(expr));
290: 		if (i < result->column_count) {
291: 			result->types.push_back(result_type);
292: 		}
293: 		internal_sql_types.push_back(result_type);
294: 		if (statement.aggregate_handling == AggregateHandling::FORCE_AGGREGATES) {
295: 			select_binder.ResetBindings();
296: 		}
297: 	}
298: 	result->need_prune = result->select_list.size() > result->column_count;
299: 
300: 	// in the normal select binder, we bind columns as if there is no aggregation
301: 	// i.e. in the query [SELECT i, SUM(i) FROM integers;] the "i" will be bound as a normal column
302: 	// since we have an aggregation, we need to either (1) throw an error, or (2) wrap the column in a FIRST() aggregate
303: 	// we choose the former one [CONTROVERSIAL: this is the PostgreSQL behavior]
304: 	if (!result->groups.empty() || !result->aggregates.empty() || statement.having) {
305: 		if (statement.aggregate_handling == AggregateHandling::NO_AGGREGATES_ALLOWED) {
306: 			throw BinderException("Aggregates cannot be present in a Project relation!");
307: 		} else if (statement.aggregate_handling == AggregateHandling::STANDARD_HANDLING) {
308: 			if (select_binder.BoundColumns()) {
309: 				throw BinderException("column must appear in the GROUP BY clause or be used in an aggregate function");
310: 			}
311: 		}
312: 	}
313: 
314: 	// now that the SELECT list is bound, we set the types of DISTINCT/ORDER BY expressions
315: 	BindModifierTypes(*result, internal_sql_types, result->projection_index);
316: 	return move(result);
317: }
318: 
319: } // namespace duckdb
[end of src/planner/binder/query_node/bind_select_node.cpp]
[start of src/planner/binder/query_node/bind_setop_node.cpp]
1: #include "duckdb/parser/expression/columnref_expression.hpp"
2: #include "duckdb/parser/expression/constant_expression.hpp"
3: #include "duckdb/parser/expression_map.hpp"
4: #include "duckdb/parser/query_node/select_node.hpp"
5: #include "duckdb/parser/query_node/set_operation_node.hpp"
6: #include "duckdb/planner/binder.hpp"
7: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
8: #include "duckdb/planner/query_node/bound_set_operation_node.hpp"
9: #include "duckdb/planner/query_node/bound_select_node.hpp"
10: #include "duckdb/planner/expression_binder/order_binder.hpp"
11: 
12: namespace duckdb {
13: 
14: static void GatherAliases(BoundQueryNode &node, unordered_map<string, idx_t> &aliases,
15:                           expression_map_t<idx_t> &expressions) {
16: 	if (node.type == QueryNodeType::SET_OPERATION_NODE) {
17: 		// setop, recurse
18: 		auto &setop = (BoundSetOperationNode &)node;
19: 		GatherAliases(*setop.left, aliases, expressions);
20: 		GatherAliases(*setop.right, aliases, expressions);
21: 	} else {
22: 		// query node
23: 		D_ASSERT(node.type == QueryNodeType::SELECT_NODE);
24: 		auto &select = (BoundSelectNode &)node;
25: 		// fill the alias lists
26: 		for (idx_t i = 0; i < select.names.size(); i++) {
27: 			auto &name = select.names[i];
28: 			auto &expr = select.original_expressions[i];
29: 			// first check if the alias is already in there
30: 			auto entry = aliases.find(name);
31: 			if (entry != aliases.end()) {
32: 				// the alias already exists
33: 				// check if there is a conflict
34: 				if (entry->second != i) {
35: 					// there is a conflict
36: 					// we place "-1" in the aliases map at this location
37: 					// "-1" signifies that there is an ambiguous reference
38: 					aliases[name] = INVALID_INDEX;
39: 				}
40: 			} else {
41: 				// the alias is not in there yet, just assign it
42: 				aliases[name] = i;
43: 			}
44: 			// now check if the node is already in the set of expressions
45: 			auto expr_entry = expressions.find(expr.get());
46: 			if (expr_entry != expressions.end()) {
47: 				// the node is in there
48: 				// repeat the same as with the alias: if there is an ambiguity we insert "-1"
49: 				if (expr_entry->second != i) {
50: 					expressions[expr.get()] = INVALID_INDEX;
51: 				}
52: 			} else {
53: 				// not in there yet, just place it in there
54: 				expressions[expr.get()] = i;
55: 			}
56: 		}
57: 	}
58: }
59: 
60: unique_ptr<BoundQueryNode> Binder::BindNode(SetOperationNode &statement) {
61: 	auto result = make_unique<BoundSetOperationNode>();
62: 	result->setop_type = statement.setop_type;
63: 
64: 	// first recursively visit the set operations
65: 	// both the left and right sides have an independent BindContext and Binder
66: 	D_ASSERT(statement.left);
67: 	D_ASSERT(statement.right);
68: 
69: 	result->setop_index = GenerateTableIndex();
70: 
71: 	result->left_binder = make_unique<Binder>(context, this);
72: 	result->left = result->left_binder->BindNode(*statement.left);
73: 
74: 	result->right_binder = make_unique<Binder>(context, this);
75: 	result->right = result->right_binder->BindNode(*statement.right);
76: 
77: 	if (!statement.modifiers.empty()) {
78: 		// handle the ORDER BY/DISTINCT clauses
79: 
80: 		// we recursively visit the children of this node to extract aliases and expressions that can be referenced in
81: 		// the ORDER BY
82: 		unordered_map<string, idx_t> alias_map;
83: 		expression_map_t<idx_t> expression_map;
84: 		GatherAliases(*result, alias_map, expression_map);
85: 
86: 		// now we perform the actual resolution of the ORDER BY/DISTINCT expressions
87: 		OrderBinder order_binder({result->left_binder.get(), result->right_binder.get()}, result->setop_index,
88: 		                         alias_map, expression_map, statement.left->GetSelectList().size());
89: 		BindModifiers(order_binder, statement, *result);
90: 	}
91: 
92: 	result->names = result->left->names;
93: 
94: 	// move the correlated expressions from the child binders to this binder
95: 	MoveCorrelatedExpressions(*result->left_binder);
96: 	MoveCorrelatedExpressions(*result->right_binder);
97: 
98: 	// now both sides have been bound we can resolve types
99: 	if (result->left->types.size() != result->right->types.size()) {
100: 		throw BinderException("Set operations can only apply to expressions with the "
101: 		                      "same number of result columns");
102: 	}
103: 
104: 	// figure out the types of the setop result by picking the max of both
105: 	for (idx_t i = 0; i < result->left->types.size(); i++) {
106: 		auto result_type = LogicalType::MaxLogicalType(result->left->types[i], result->right->types[i]);
107: 		result->types.push_back(result_type);
108: 	}
109: 
110: 	// finally bind the types of the ORDER/DISTINCT clause expressions
111: 	BindModifierTypes(*result, result->types, result->setop_index);
112: 	return move(result);
113: }
114: 
115: } // namespace duckdb
[end of src/planner/binder/query_node/bind_setop_node.cpp]
[start of src/planner/binder/query_node/plan_subquery.cpp]
1: #include "duckdb/planner/binder.hpp"
2: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
3: #include "duckdb/planner/expression/bound_cast_expression.hpp"
4: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
5: #include "duckdb/planner/expression/bound_comparison_expression.hpp"
6: #include "duckdb/planner/expression/bound_constant_expression.hpp"
7: #include "duckdb/planner/expression/bound_reference_expression.hpp"
8: #include "duckdb/planner/expression/bound_subquery_expression.hpp"
9: #include "duckdb/planner/expression_iterator.hpp"
10: #include "duckdb/planner/binder.hpp"
11: #include "duckdb/planner/operator/list.hpp"
12: #include "duckdb/planner/subquery/flatten_dependent_join.hpp"
13: #include "duckdb/function/aggregate/distributive_functions.hpp"
14: 
15: namespace duckdb {
16: 
17: static unique_ptr<Expression> PlanUncorrelatedSubquery(Binder &binder, BoundSubqueryExpression &expr,
18:                                                        unique_ptr<LogicalOperator> &root,
19:                                                        unique_ptr<LogicalOperator> plan) {
20: 	D_ASSERT(!expr.IsCorrelated());
21: 	switch (expr.subquery_type) {
22: 	case SubqueryType::EXISTS: {
23: 		// uncorrelated EXISTS
24: 		// we only care about existence, hence we push a LIMIT 1 operator
25: 		auto limit = make_unique<LogicalLimit>(1, 0, nullptr, nullptr);
26: 		limit->AddChild(move(plan));
27: 		plan = move(limit);
28: 
29: 		// now we push a COUNT(*) aggregate onto the limit, this will be either 0 or 1 (EXISTS or NOT EXISTS)
30: 		auto count_star_fun = CountStarFun::GetFunction();
31: 		auto count_star = AggregateFunction::BindAggregateFunction(binder.context, count_star_fun, {}, nullptr, false);
32: 		auto idx_type = count_star->return_type;
33: 		vector<unique_ptr<Expression>> aggregate_list;
34: 		aggregate_list.push_back(move(count_star));
35: 		auto aggregate_index = binder.GenerateTableIndex();
36: 		auto aggregate =
37: 		    make_unique<LogicalAggregate>(binder.GenerateTableIndex(), aggregate_index, move(aggregate_list));
38: 		aggregate->AddChild(move(plan));
39: 		plan = move(aggregate);
40: 
41: 		// now we push a projection with a comparison to 1
42: 		auto left_child = make_unique<BoundColumnRefExpression>(idx_type, ColumnBinding(aggregate_index, 0));
43: 		auto right_child = make_unique<BoundConstantExpression>(Value::Numeric(idx_type, 1));
44: 		auto comparison =
45: 		    make_unique<BoundComparisonExpression>(ExpressionType::COMPARE_EQUAL, move(left_child), move(right_child));
46: 
47: 		vector<unique_ptr<Expression>> projection_list;
48: 		projection_list.push_back(move(comparison));
49: 		auto projection_index = binder.GenerateTableIndex();
50: 		auto projection = make_unique<LogicalProjection>(projection_index, move(projection_list));
51: 		projection->AddChild(move(plan));
52: 		plan = move(projection);
53: 
54: 		// we add it to the main query by adding a cross product
55: 		// FIXME: should use something else besides cross product as we always add only one scalar constant
56: 		auto cross_product = make_unique<LogicalCrossProduct>();
57: 		cross_product->AddChild(move(root));
58: 		cross_product->AddChild(move(plan));
59: 		root = move(cross_product);
60: 
61: 		// we replace the original subquery with a ColumnRefExpression referring to the result of the projection (either
62: 		// TRUE or FALSE)
63: 		return make_unique<BoundColumnRefExpression>(expr.GetName(), LogicalType::BOOLEAN,
64: 		                                             ColumnBinding(projection_index, 0));
65: 	}
66: 	case SubqueryType::SCALAR: {
67: 		// uncorrelated scalar, we want to return the first entry
68: 		// figure out the table index of the bound table of the entry which we want to return
69: 		auto bindings = plan->GetColumnBindings();
70: 		D_ASSERT(bindings.size() == 1);
71: 		idx_t table_idx = bindings[0].table_index;
72: 
73: 		// in the uncorrelated case we are only interested in the first result of the query
74: 		// hence we simply push a LIMIT 1 to get the first row of the subquery
75: 		auto limit = make_unique<LogicalLimit>(1, 0, nullptr, nullptr);
76: 		limit->AddChild(move(plan));
77: 		plan = move(limit);
78: 
79: 		// we push an aggregate that returns the FIRST element
80: 		vector<unique_ptr<Expression>> expressions;
81: 		auto bound = make_unique<BoundColumnRefExpression>(expr.return_type, ColumnBinding(table_idx, 0));
82: 		vector<unique_ptr<Expression>> first_children;
83: 		first_children.push_back(move(bound));
84: 		auto first_agg = AggregateFunction::BindAggregateFunction(
85: 		    binder.context, FirstFun::GetFunction(expr.return_type), move(first_children), nullptr, false);
86: 
87: 		expressions.push_back(move(first_agg));
88: 		auto aggr_index = binder.GenerateTableIndex();
89: 		auto aggr = make_unique<LogicalAggregate>(binder.GenerateTableIndex(), aggr_index, move(expressions));
90: 		aggr->AddChild(move(plan));
91: 		plan = move(aggr);
92: 
93: 		// in the uncorrelated case, we add the value to the main query through a cross product
94: 		// FIXME: should use something else besides cross product as we always add only one scalar constant and cross
95: 		// product is not optimized for this.
96: 		D_ASSERT(root);
97: 		auto cross_product = make_unique<LogicalCrossProduct>();
98: 		cross_product->AddChild(move(root));
99: 		cross_product->AddChild(move(plan));
100: 		root = move(cross_product);
101: 
102: 		// we replace the original subquery with a BoundColumnRefExpression referring to the first result of the
103: 		// aggregation
104: 		return make_unique<BoundColumnRefExpression>(expr.GetName(), expr.return_type, ColumnBinding(aggr_index, 0));
105: 	}
106: 	default: {
107: 		D_ASSERT(expr.subquery_type == SubqueryType::ANY);
108: 		// we generate a MARK join that results in either (TRUE, FALSE or NULL)
109: 		// subquery has NULL values -> result is (TRUE or NULL)
110: 		// subquery has no NULL values -> result is (TRUE, FALSE or NULL [if input is NULL])
111: 		// fetch the column bindings
112: 		auto plan_columns = plan->GetColumnBindings();
113: 
114: 		// then we generate the MARK join with the subquery
115: 		idx_t mark_index = binder.GenerateTableIndex();
116: 		auto join = make_unique<LogicalComparisonJoin>(JoinType::MARK);
117: 		join->mark_index = mark_index;
118: 		join->AddChild(move(root));
119: 		join->AddChild(move(plan));
120: 		// create the JOIN condition
121: 		JoinCondition cond;
122: 		cond.left = move(expr.child);
123: 		cond.right = BoundCastExpression::AddCastToType(
124: 		    make_unique<BoundColumnRefExpression>(expr.child_type, plan_columns[0]), expr.child_target);
125: 		cond.comparison = expr.comparison_type;
126: 		join->conditions.push_back(move(cond));
127: 		root = move(join);
128: 
129: 		// we replace the original subquery with a BoundColumnRefExpression referring to the mark column
130: 		return make_unique<BoundColumnRefExpression>(expr.GetName(), expr.return_type, ColumnBinding(mark_index, 0));
131: 	}
132: 	}
133: }
134: 
135: static unique_ptr<LogicalDelimJoin> CreateDuplicateEliminatedJoin(vector<CorrelatedColumnInfo> &correlated_columns,
136:                                                                   JoinType join_type) {
137: 	auto delim_join = make_unique<LogicalDelimJoin>(join_type);
138: 	for (idx_t i = 0; i < correlated_columns.size(); i++) {
139: 		auto &col = correlated_columns[i];
140: 		delim_join->duplicate_eliminated_columns.push_back(
141: 		    make_unique<BoundColumnRefExpression>(col.type, col.binding));
142: 		delim_join->delim_types.push_back(col.type);
143: 	}
144: 	return delim_join;
145: }
146: 
147: static void CreateDelimJoinConditions(LogicalDelimJoin &delim_join, vector<CorrelatedColumnInfo> &correlated_columns,
148:                                       vector<ColumnBinding> bindings, idx_t base_offset) {
149: 	for (idx_t i = 0; i < correlated_columns.size(); i++) {
150: 		auto &col = correlated_columns[i];
151: 		JoinCondition cond;
152: 		cond.left = make_unique<BoundColumnRefExpression>(col.name, col.type, col.binding);
153: 		cond.right = make_unique<BoundColumnRefExpression>(col.name, col.type, bindings[base_offset + i]);
154: 		cond.comparison = ExpressionType::COMPARE_EQUAL;
155: 		cond.null_values_are_equal = true;
156: 		delim_join.conditions.push_back(move(cond));
157: 	}
158: }
159: 
160: static unique_ptr<Expression> PlanCorrelatedSubquery(Binder &binder, BoundSubqueryExpression &expr,
161:                                                      unique_ptr<LogicalOperator> &root,
162:                                                      unique_ptr<LogicalOperator> plan) {
163: 	auto &correlated_columns = expr.binder->correlated_columns;
164: 	D_ASSERT(expr.IsCorrelated());
165: 	// correlated subquery
166: 	// for a more in-depth explanation of this code, read the paper "Unnesting Arbitrary Subqueries"
167: 	// we handle three types of correlated subqueries: Scalar, EXISTS and ANY
168: 	// all three cases are very similar with some minor changes (mainly the type of join performed at the end)
169: 	switch (expr.subquery_type) {
170: 	case SubqueryType::SCALAR: {
171: 		// correlated SCALAR query
172: 		// first push a DUPLICATE ELIMINATED join
173: 		// a duplicate eliminated join creates a duplicate eliminated copy of the LHS
174: 		// and pushes it into any DUPLICATE_ELIMINATED SCAN operators on the RHS
175: 
176: 		// in the SCALAR case, we create a SINGLE join (because we are only interested in obtaining the value)
177: 		// NULL values are equal in this join because we join on the correlated columns ONLY
178: 		// and e.g. in the query: SELECT (SELECT 42 FROM integers WHERE i1.i IS NULL LIMIT 1) FROM integers i1;
179: 		// the input value NULL will generate the value 42, and we need to join NULL on the LHS with NULL on the RHS
180: 		auto delim_join = CreateDuplicateEliminatedJoin(correlated_columns, JoinType::SINGLE);
181: 
182: 		// the left side is the original plan
183: 		// this is the side that will be duplicate eliminated and pushed into the RHS
184: 		delim_join->AddChild(move(root));
185: 		// the right side initially is a DEPENDENT join between the duplicate eliminated scan and the subquery
186: 		// HOWEVER: we do not explicitly create the dependent join
187: 		// instead, we eliminate the dependent join by pushing it down into the right side of the plan
188: 		FlattenDependentJoins flatten(binder, correlated_columns);
189: 
190: 		// first we check which logical operators have correlated expressions in the first place
191: 		flatten.DetectCorrelatedExpressions(plan.get());
192: 		// now we push the dependent join down
193: 		auto dependent_join = flatten.PushDownDependentJoin(move(plan));
194: 
195: 		// now the dependent join is fully eliminated
196: 		// we only need to create the join conditions between the LHS and the RHS
197: 		// fetch the set of columns
198: 		auto plan_columns = dependent_join->GetColumnBindings();
199: 
200: 		// now create the join conditions
201: 		CreateDelimJoinConditions(*delim_join, correlated_columns, plan_columns, flatten.delim_offset);
202: 		delim_join->AddChild(move(dependent_join));
203: 		root = move(delim_join);
204: 		// finally push the BoundColumnRefExpression referring to the data element returned by the join
205: 		return make_unique<BoundColumnRefExpression>(expr.GetName(), expr.return_type,
206: 		                                             plan_columns[flatten.data_offset]);
207: 	}
208: 	case SubqueryType::EXISTS: {
209: 		// correlated EXISTS query
210: 		// this query is similar to the correlated SCALAR query, except we use a MARK join here
211: 		idx_t mark_index = binder.GenerateTableIndex();
212: 		auto delim_join = CreateDuplicateEliminatedJoin(correlated_columns, JoinType::MARK);
213: 		delim_join->mark_index = mark_index;
214: 		// LHS
215: 		delim_join->AddChild(move(root));
216: 		// RHS
217: 		FlattenDependentJoins flatten(binder, correlated_columns);
218: 		flatten.DetectCorrelatedExpressions(plan.get());
219: 		auto dependent_join = flatten.PushDownDependentJoin(move(plan));
220: 
221: 		// fetch the set of columns
222: 		auto plan_columns = dependent_join->GetColumnBindings();
223: 
224: 		// now we create the join conditions between the dependent join and the original table
225: 		CreateDelimJoinConditions(*delim_join, correlated_columns, plan_columns, flatten.delim_offset);
226: 		delim_join->AddChild(move(dependent_join));
227: 		root = move(delim_join);
228: 		// finally push the BoundColumnRefExpression referring to the marker
229: 		return make_unique<BoundColumnRefExpression>(expr.GetName(), expr.return_type, ColumnBinding(mark_index, 0));
230: 	}
231: 	default: {
232: 		D_ASSERT(expr.subquery_type == SubqueryType::ANY);
233: 		// correlated ANY query
234: 		// this query is similar to the correlated SCALAR query
235: 		// however, in this case we push a correlated MARK join
236: 		// note that in this join null values are NOT equal for ALL columns, but ONLY for the correlated columns
237: 		// the correlated mark join handles this case by itself
238: 		// as the MARK join has one extra join condition (the original condition, of the ANY expression, e.g.
239: 		// [i=ANY(...)])
240: 		idx_t mark_index = binder.GenerateTableIndex();
241: 		auto delim_join = CreateDuplicateEliminatedJoin(correlated_columns, JoinType::MARK);
242: 		delim_join->mark_index = mark_index;
243: 		// LHS
244: 		delim_join->AddChild(move(root));
245: 		// RHS
246: 		FlattenDependentJoins flatten(binder, correlated_columns);
247: 		flatten.DetectCorrelatedExpressions(plan.get());
248: 		auto dependent_join = flatten.PushDownDependentJoin(move(plan));
249: 
250: 		// fetch the columns
251: 		auto plan_columns = dependent_join->GetColumnBindings();
252: 
253: 		// now we create the join conditions between the dependent join and the original table
254: 		CreateDelimJoinConditions(*delim_join, correlated_columns, plan_columns, flatten.delim_offset);
255: 		// add the actual condition based on the ANY/ALL predicate
256: 		JoinCondition compare_cond;
257: 		compare_cond.left = move(expr.child);
258: 		compare_cond.right = BoundCastExpression::AddCastToType(
259: 		    make_unique<BoundColumnRefExpression>(expr.child_type, plan_columns[0]), expr.child_target);
260: 		compare_cond.comparison = expr.comparison_type;
261: 		delim_join->conditions.push_back(move(compare_cond));
262: 
263: 		delim_join->AddChild(move(dependent_join));
264: 		root = move(delim_join);
265: 		// finally push the BoundColumnRefExpression referring to the marker
266: 		return make_unique<BoundColumnRefExpression>(expr.GetName(), expr.return_type, ColumnBinding(mark_index, 0));
267: 	}
268: 	}
269: }
270: 
271: class RecursiveSubqueryPlanner : public LogicalOperatorVisitor {
272: public:
273: 	explicit RecursiveSubqueryPlanner(Binder &binder) : binder(binder) {
274: 	}
275: 	void VisitOperator(LogicalOperator &op) override {
276: 		if (!op.children.empty()) {
277: 			root = move(op.children[0]);
278: 			VisitOperatorExpressions(op);
279: 			op.children[0] = move(root);
280: 			for (idx_t i = 0; i < op.children.size(); i++) {
281: 				VisitOperator(*op.children[i]);
282: 			}
283: 		}
284: 	}
285: 
286: 	unique_ptr<Expression> VisitReplace(BoundSubqueryExpression &expr, unique_ptr<Expression> *expr_ptr) override {
287: 		return binder.PlanSubquery(expr, root);
288: 	}
289: 
290: private:
291: 	unique_ptr<LogicalOperator> root;
292: 	Binder &binder;
293: };
294: 
295: unique_ptr<Expression> Binder::PlanSubquery(BoundSubqueryExpression &expr, unique_ptr<LogicalOperator> &root) {
296: 	D_ASSERT(root);
297: 	// first we translate the QueryNode of the subquery into a logical plan
298: 	// note that we do not plan nested subqueries yet
299: 	Binder sub_binder(context);
300: 	sub_binder.plan_subquery = false;
301: 	auto subquery_root = sub_binder.CreatePlan(*expr.subquery);
302: 	D_ASSERT(subquery_root);
303: 
304: 	// now we actually flatten the subquery
305: 	auto plan = move(subquery_root);
306: 	unique_ptr<Expression> result_expression;
307: 	if (!expr.IsCorrelated()) {
308: 		result_expression = PlanUncorrelatedSubquery(*this, expr, root, move(plan));
309: 	} else {
310: 		result_expression = PlanCorrelatedSubquery(*this, expr, root, move(plan));
311: 	}
312: 	// finally, we recursively plan the nested subqueries (if there are any)
313: 	if (sub_binder.has_unplanned_subqueries) {
314: 		RecursiveSubqueryPlanner plan(*this);
315: 		plan.VisitOperator(*root);
316: 	}
317: 	return result_expression;
318: }
319: 
320: void Binder::PlanSubqueries(unique_ptr<Expression> *expr_ptr, unique_ptr<LogicalOperator> *root) {
321: 	if (!*expr_ptr) {
322: 		return;
323: 	}
324: 	auto &expr = **expr_ptr;
325: 
326: 	// first visit the children of the node, if any
327: 	ExpressionIterator::EnumerateChildren(expr, [&](unique_ptr<Expression> &expr) { PlanSubqueries(&expr, root); });
328: 
329: 	// check if this is a subquery node
330: 	if (expr.expression_class == ExpressionClass::BOUND_SUBQUERY) {
331: 		auto &subquery = (BoundSubqueryExpression &)expr;
332: 		// subquery node! plan it
333: 		if (subquery.IsCorrelated() && !plan_subquery) {
334: 			// detected a nested correlated subquery
335: 			// we don't plan it yet here, we are currently planning a subquery
336: 			// nested subqueries will only be planned AFTER the current subquery has been flattened entirely
337: 			has_unplanned_subqueries = true;
338: 			return;
339: 		}
340: 		*expr_ptr = PlanSubquery(subquery, *root);
341: 	}
342: }
343: 
344: } // namespace duckdb
[end of src/planner/binder/query_node/plan_subquery.cpp]
[start of src/planner/binder/statement/bind_export.cpp]
1: #include "duckdb/catalog/catalog.hpp"
2: #include "duckdb/parser/statement/export_statement.hpp"
3: #include "duckdb/planner/binder.hpp"
4: #include "duckdb/planner/operator/logical_export.hpp"
5: #include "duckdb/catalog/catalog_entry/copy_function_catalog_entry.hpp"
6: #include "duckdb/parser/statement/copy_statement.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb/main/database.hpp"
9: #include "duckdb/common/file_system.hpp"
10: #include "duckdb/planner/operator/logical_set_operation.hpp"
11: 
12: #include <algorithm>
13: 
14: namespace duckdb {
15: 
16: BoundStatement Binder::Bind(ExportStatement &stmt) {
17: 	// COPY TO a file
18: 	auto &config = DBConfig::GetConfig(context);
19: 	if (!config.enable_copy) {
20: 		throw Exception("COPY TO is disabled by configuration");
21: 	}
22: 	BoundStatement result;
23: 	result.types = {LogicalType::BOOLEAN};
24: 	result.names = {"Success"};
25: 
26: 	// lookup the format in the catalog
27: 	auto &catalog = Catalog::GetCatalog(context);
28: 	auto copy_function = catalog.GetEntry<CopyFunctionCatalogEntry>(context, DEFAULT_SCHEMA, stmt.info->format);
29: 	if (!copy_function->function.copy_to_bind) {
30: 		throw NotImplementedException("COPY TO is not supported for FORMAT \"%s\"", stmt.info->format);
31: 	}
32: 
33: 	// gather a list of all the tables
34: 	vector<TableCatalogEntry *> tables;
35: 	Catalog::GetCatalog(context).schemas->Scan(context, [&](CatalogEntry *entry) {
36: 		auto schema = (SchemaCatalogEntry *)entry;
37: 		schema->Scan(context, CatalogType::TABLE_ENTRY, [&](CatalogEntry *entry) {
38: 			if (entry->type == CatalogType::TABLE_ENTRY) {
39: 				tables.push_back((TableCatalogEntry *)entry);
40: 			}
41: 		});
42: 	});
43: 
44: 	// now generate the COPY statements for each of the tables
45: 	auto &fs = FileSystem::GetFileSystem(context);
46: 	unique_ptr<LogicalOperator> child_operator;
47: 	for (auto &table : tables) {
48: 		auto info = make_unique<CopyInfo>();
49: 		// we copy the options supplied to the EXPORT
50: 		info->format = stmt.info->format;
51: 		info->options = stmt.info->options;
52: 		// set up the file name for the COPY TO
53: 		if (table->schema->name == DEFAULT_SCHEMA) {
54: 			info->file_path = fs.JoinPath(stmt.info->file_path,
55: 			                              StringUtil::Format("%s.%s", table->name, copy_function->function.extension));
56: 		} else {
57: 			info->file_path =
58: 			    fs.JoinPath(stmt.info->file_path, StringUtil::Format("%s.%s.%s", table->schema->name, table->name,
59: 			                                                         copy_function->function.extension));
60: 		}
61: 		info->is_from = false;
62: 		info->schema = table->schema->name;
63: 		info->table = table->name;
64: 
65: 		// generate the copy statement and bind it
66: 		CopyStatement copy_stmt;
67: 		copy_stmt.info = move(info);
68: 
69: 		Binder copy_binder(context);
70: 		auto bound_statement = copy_binder.Bind(copy_stmt);
71: 		if (child_operator) {
72: 			// use UNION ALL to combine the individual copy statements into a single node
73: 			auto copy_union =
74: 			    make_unique<LogicalSetOperation>(GenerateTableIndex(), 1, move(child_operator),
75: 			                                     move(bound_statement.plan), LogicalOperatorType::LOGICAL_UNION);
76: 			child_operator = move(copy_union);
77: 		} else {
78: 			child_operator = move(bound_statement.plan);
79: 		}
80: 	}
81: 
82: 	// try to create the directory, if it doesn't exist yet
83: 	// a bit hacky to do it here, but we need to create the directory BEFORE the copy statements run
84: 	if (!fs.DirectoryExists(stmt.info->file_path)) {
85: 		fs.CreateDirectory(stmt.info->file_path);
86: 	}
87: 
88: 	// create the export node
89: 	auto export_node = make_unique<LogicalExport>(copy_function->function, move(stmt.info));
90: 
91: 	if (child_operator) {
92: 		export_node->children.push_back(move(child_operator));
93: 	}
94: 
95: 	result.plan = move(export_node);
96: 	return result;
97: }
98: 
99: } // namespace duckdb
[end of src/planner/binder/statement/bind_export.cpp]
[start of src/planner/binder/tableref/bind_basetableref.cpp]
1: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
2: #include "duckdb/parser/tableref/basetableref.hpp"
3: #include "duckdb/parser/tableref/subqueryref.hpp"
4: #include "duckdb/parser/query_node/select_node.hpp"
5: #include "duckdb/planner/binder.hpp"
6: #include "duckdb/planner/tableref/bound_basetableref.hpp"
7: #include "duckdb/planner/tableref/bound_subqueryref.hpp"
8: #include "duckdb/planner/tableref/bound_cteref.hpp"
9: #include "duckdb/planner/operator/logical_get.hpp"
10: #include "duckdb/parser/statement/select_statement.hpp"
11: #include "duckdb/function/table/table_scan.hpp"
12: 
13: namespace duckdb {
14: 
15: unique_ptr<BoundTableRef> Binder::Bind(BaseTableRef &ref) {
16: 	QueryErrorContext error_context(root_statement, ref.query_location);
17: 	// CTEs and views are also referred to using BaseTableRefs, hence need to distinguish here
18: 	// check if the table name refers to a CTE
19: 	auto cte = FindCTE(ref.table_name, ref.table_name == alias);
20: 	if (cte) {
21: 		// Check if there is a CTE binding in the BindContext
22: 		auto ctebinding = bind_context.GetCTEBinding(ref.table_name);
23: 		if (!ctebinding) {
24: 			if (CTEIsAlreadyBound(cte)) {
25: 				throw BinderException("Circular reference to CTE \"%s\", use WITH RECURSIVE to use recursive CTEs",
26: 				                      ref.table_name);
27: 			}
28: 			// Move CTE to subquery and bind recursively
29: 			SubqueryRef subquery(unique_ptr_cast<SQLStatement, SelectStatement>(cte->query->Copy()));
30: 			subquery.alias = ref.alias.empty() ? ref.table_name : ref.alias;
31: 			subquery.column_name_alias = cte->aliases;
32: 			for (idx_t i = 0; i < ref.column_name_alias.size(); i++) {
33: 				if (i < subquery.column_name_alias.size()) {
34: 					subquery.column_name_alias[i] = ref.column_name_alias[i];
35: 				} else {
36: 					subquery.column_name_alias.push_back(ref.column_name_alias[i]);
37: 				}
38: 			}
39: 			return Bind(subquery, cte);
40: 		} else {
41: 			// There is a CTE binding in the BindContext.
42: 			// This can only be the case if there is a recursive CTE present.
43: 			auto index = GenerateTableIndex();
44: 			auto result = make_unique<BoundCTERef>(index, ctebinding->index);
45: 			auto b = ctebinding;
46: 			auto alias = ref.alias.empty() ? ref.table_name : ref.alias;
47: 			auto names = BindContext::AliasColumnNames(alias, b->names, ref.column_name_alias);
48: 
49: 			bind_context.AddGenericBinding(index, alias, names, b->types);
50: 			// Update references to CTE
51: 			auto cteref = bind_context.cte_references[ref.table_name];
52: 			(*cteref)++;
53: 
54: 			result->types = b->types;
55: 			result->bound_columns = move(names);
56: 			return move(result);
57: 		}
58: 	}
59: 	// not a CTE
60: 	// extract a table or view from the catalog
61: 	auto table_or_view = Catalog::GetCatalog(context).GetEntry(context, CatalogType::TABLE_ENTRY, ref.schema_name,
62: 	                                                           ref.table_name, false, error_context);
63: 	switch (table_or_view->type) {
64: 	case CatalogType::TABLE_ENTRY: {
65: 		// base table: create the BoundBaseTableRef node
66: 		auto table_index = GenerateTableIndex();
67: 		auto table = (TableCatalogEntry *)table_or_view;
68: 
69: 		auto scan_function = TableScanFunction::GetFunction();
70: 		auto bind_data = make_unique<TableScanBindData>(table);
71: 		auto alias = ref.alias.empty() ? ref.table_name : ref.alias;
72: 		vector<LogicalType> table_types;
73: 		vector<string> table_names;
74: 		for (auto &col : table->columns) {
75: 			table_types.push_back(col.type);
76: 			table_names.push_back(col.name);
77: 		}
78: 		table_names = BindContext::AliasColumnNames(alias, table_names, ref.column_name_alias);
79: 
80: 		auto logical_get =
81: 		    make_unique<LogicalGet>(table_index, scan_function, move(bind_data), table_types, table_names);
82: 		bind_context.AddBaseTable(table_index, alias, table_names, table_types, *logical_get);
83: 		return make_unique_base<BoundTableRef, BoundBaseTableRef>(table, move(logical_get));
84: 	}
85: 	case CatalogType::VIEW_ENTRY: {
86: 		// the node is a view: get the query that the view represents
87: 		auto view_catalog_entry = (ViewCatalogEntry *)table_or_view;
88: 		// We need to use a new binder for the view that doesn't reference any CTEs
89: 		// defined for this binder so there are no collisions between the CTEs defined
90: 		// for the view and for the current query
91: 		bool inherit_ctes = false;
92: 		Binder view_binder(context, this, inherit_ctes);
93: 		SubqueryRef subquery(unique_ptr_cast<SQLStatement, SelectStatement>(view_catalog_entry->query->Copy()));
94: 		subquery.alias = ref.alias.empty() ? ref.table_name : ref.alias;
95: 		subquery.column_name_alias =
96: 		    BindContext::AliasColumnNames(subquery.alias, view_catalog_entry->aliases, ref.column_name_alias);
97: 		// bind the child subquery
98: 		auto bound_child = view_binder.Bind(subquery);
99: 		D_ASSERT(bound_child->type == TableReferenceType::SUBQUERY);
100: 		// verify that the types and names match up with the expected types and names
101: 		auto &bound_subquery = (BoundSubqueryRef &)*bound_child;
102: 		if (bound_subquery.subquery->types != view_catalog_entry->types) {
103: 			throw BinderException("Contents of view were altered: types don't match!");
104: 		}
105: 		bind_context.AddSubquery(bound_subquery.subquery->GetRootIndex(), subquery.alias, subquery,
106: 		                         *bound_subquery.subquery);
107: 		return bound_child;
108: 	}
109: 	default:
110: 		throw InternalException("Catalog entry type");
111: 	}
112: }
113: } // namespace duckdb
[end of src/planner/binder/tableref/bind_basetableref.cpp]
[start of src/planner/binder/tableref/bind_crossproductref.cpp]
1: #include "duckdb/parser/tableref/crossproductref.hpp"
2: #include "duckdb/planner/binder.hpp"
3: #include "duckdb/planner/tableref/bound_crossproductref.hpp"
4: 
5: namespace duckdb {
6: 
7: unique_ptr<BoundTableRef> Binder::Bind(CrossProductRef &ref) {
8: 	auto result = make_unique<BoundCrossProductRef>();
9: 	result->left_binder = make_unique<Binder>(context, this);
10: 	result->right_binder = make_unique<Binder>(context, this);
11: 	auto &left_binder = *result->left_binder;
12: 	auto &right_binder = *result->right_binder;
13: 
14: 	result->left = left_binder.Bind(*ref.left);
15: 	result->right = right_binder.Bind(*ref.right);
16: 
17: 	bind_context.AddContext(move(left_binder.bind_context));
18: 	bind_context.AddContext(move(right_binder.bind_context));
19: 	MoveCorrelatedExpressions(left_binder);
20: 	MoveCorrelatedExpressions(right_binder);
21: 	return move(result);
22: }
23: 
24: } // namespace duckdb
[end of src/planner/binder/tableref/bind_crossproductref.cpp]
[start of src/planner/binder/tableref/bind_joinref.cpp]
1: #include "duckdb/parser/tableref/joinref.hpp"
2: #include "duckdb/planner/binder.hpp"
3: #include "duckdb/planner/expression_binder/where_binder.hpp"
4: #include "duckdb/planner/tableref/bound_joinref.hpp"
5: #include "duckdb/parser/expression/comparison_expression.hpp"
6: #include "duckdb/parser/expression/columnref_expression.hpp"
7: #include "duckdb/parser/expression/constant_expression.hpp"
8: #include "duckdb/parser/expression/conjunction_expression.hpp"
9: #include "duckdb/parser/expression/bound_expression.hpp"
10: 
11: namespace duckdb {
12: 
13: static unique_ptr<ParsedExpression> BindColumn(Binder &binder, ClientContext &context, const string &alias,
14:                                                const string &column_name) {
15: 	auto expr = make_unique_base<ParsedExpression, ColumnRefExpression>(column_name, alias);
16: 	ExpressionBinder expr_binder(binder, context);
17: 	auto result = expr_binder.Bind(expr);
18: 	return make_unique<BoundExpression>(move(result), nullptr);
19: }
20: 
21: static unique_ptr<ParsedExpression> AddCondition(ClientContext &context, Binder &left_binder, Binder &right_binder,
22:                                                  const string &left_alias, const string &right_alias,
23:                                                  const string &column_name) {
24: 	ExpressionBinder expr_binder(left_binder, context);
25: 	auto left = BindColumn(left_binder, context, left_alias, column_name);
26: 	auto right = BindColumn(right_binder, context, right_alias, column_name);
27: 	return make_unique<ComparisonExpression>(ExpressionType::COMPARE_EQUAL, move(left), move(right));
28: }
29: 
30: bool Binder::TryFindBinding(const string &using_column, const string &join_side, string &result) {
31: 	// for each using column, get the matching binding
32: 	auto bindings = bind_context.GetMatchingBindings(using_column);
33: 	if (bindings.empty()) {
34: 		return false;
35: 	}
36: 	// find the join binding
37: 	for (auto &binding : bindings) {
38: 		if (!result.empty()) {
39: 			string error = "Column name \"";
40: 			error += using_column;
41: 			error += "\" is ambiguous: it exists more than once on ";
42: 			error += join_side;
43: 			error += " side of join.\nCandidates:";
44: 			for (auto &binding : bindings) {
45: 				error += "\n\t";
46: 				error += binding;
47: 				error += ".";
48: 				error += using_column;
49: 			}
50: 			throw BinderException(error);
51: 		} else {
52: 			result = binding;
53: 		}
54: 	}
55: 	return true;
56: }
57: 
58: string Binder::FindBinding(const string &using_column, const string &join_side) {
59: 	string result;
60: 	if (!TryFindBinding(using_column, join_side, result)) {
61: 		throw BinderException("Column \"%s\" does not exist on %s side of join!", using_column, join_side);
62: 	}
63: 	return result;
64: }
65: 
66: static void AddUsingBindings(UsingColumnSet &set, UsingColumnSet *input_set, const string &input_binding) {
67: 	if (input_set) {
68: 		for (auto &entry : input_set->bindings) {
69: 			set.bindings.insert(entry);
70: 		}
71: 	} else {
72: 		set.bindings.insert(input_binding);
73: 	}
74: }
75: 
76: static void SetPrimaryBinding(UsingColumnSet &set, JoinType join_type, const string &left_binding,
77:                               const string &right_binding) {
78: 	switch (join_type) {
79: 	case JoinType::LEFT:
80: 	case JoinType::INNER:
81: 	case JoinType::SEMI:
82: 	case JoinType::ANTI:
83: 		set.primary_binding = left_binding;
84: 		break;
85: 	case JoinType::RIGHT:
86: 		set.primary_binding = right_binding;
87: 		break;
88: 	default:
89: 		break;
90: 	}
91: }
92: 
93: unique_ptr<BoundTableRef> Binder::Bind(JoinRef &ref) {
94: 	auto result = make_unique<BoundJoinRef>();
95: 	result->left_binder = make_unique<Binder>(context, this);
96: 	result->right_binder = make_unique<Binder>(context, this);
97: 	auto &left_binder = *result->left_binder;
98: 	auto &right_binder = *result->right_binder;
99: 
100: 	result->type = ref.type;
101: 	result->left = left_binder.Bind(*ref.left);
102: 	result->right = right_binder.Bind(*ref.right);
103: 
104: 	vector<unique_ptr<ParsedExpression>> extra_conditions;
105: 	if (ref.is_natural) {
106: 		// natural join, figure out which column names are present in both sides of the join
107: 		// first bind the left hand side and get a list of all the tables and column names
108: 		unordered_map<string, string> lhs_columns;
109: 		auto &lhs_binding_list = left_binder.bind_context.GetBindingsList();
110: 		for (auto &binding : lhs_binding_list) {
111: 			for (auto &column_name : binding.second->names) {
112: 				if (lhs_columns.find(column_name) == lhs_columns.end()) {
113: 					// new column candidate: add it to the set
114: 					lhs_columns[column_name] = binding.first;
115: 				} else {
116: 					// this column candidate appears multiple times on the left-hand side of the join
117: 					// this is fine ONLY if the column name does not occur in the right hand side
118: 					// replace the binding with an empty string
119: 					lhs_columns[column_name] = string();
120: 				}
121: 			}
122: 		}
123: 		// now bind the rhs
124: 		for (auto &column : lhs_columns) {
125: 			auto &column_name = column.first;
126: 			auto &left_binding = column.second;
127: 
128: 			auto left_using_binding = left_binder.bind_context.GetUsingBinding(column_name, left_binding);
129: 			auto right_using_binding = right_binder.bind_context.GetUsingBinding(column_name);
130: 
131: 			string right_binding;
132: 			// loop over the set of lhs columns, and figure out if there is a table in the rhs with the same name
133: 			if (!right_using_binding) {
134: 				if (!right_binder.TryFindBinding(column_name, "right", right_binding)) {
135: 					// no match found for this column on the rhs: skip
136: 					continue;
137: 				}
138: 			}
139: 			// found this column name in both the LHS and the RHS of this join
140: 			// add it to the natural join!
141: 			// first check if the binding is ambiguous on the LHS
142: 			if (!left_using_binding && left_binding.empty()) {
143: 				// binding is ambiguous on left or right side: throw an exception
144: 				string error_msg = "Column name \"" + column_name +
145: 				                   "\" is ambiguous: it exists more than once on the left side of the join.";
146: 				throw BinderException(FormatError(ref, error_msg));
147: 			}
148: 			// there is a match! create the join condition
149: 			extra_conditions.push_back(
150: 			    AddCondition(context, left_binder, right_binder, left_binding, right_binding, column_name));
151: 
152: 			UsingColumnSet set;
153: 			AddUsingBindings(set, left_using_binding, left_binding);
154: 			AddUsingBindings(set, right_using_binding, right_binding);
155: 			SetPrimaryBinding(set, ref.type, left_binding, right_binding);
156: 			left_binder.bind_context.RemoveUsingBinding(column_name, left_using_binding);
157: 			right_binder.bind_context.RemoveUsingBinding(column_name, right_using_binding);
158: 			bind_context.AddUsingBinding(column_name, move(set));
159: 		}
160: 		if (extra_conditions.empty()) {
161: 			// no matching bindings found in natural join: throw an exception
162: 			string error_msg = "No columns found to join on in NATURAL JOIN.\n";
163: 			error_msg += "Use CROSS JOIN if you intended for this to be a cross-product.";
164: 			// gather all left/right candidates
165: 			string left_candidates, right_candidates;
166: 			auto &rhs_binding_list = right_binder.bind_context.GetBindingsList();
167: 			for (auto &binding : lhs_binding_list) {
168: 				for (auto &column_name : binding.second->names) {
169: 					if (!left_candidates.empty()) {
170: 						left_candidates += ", ";
171: 					}
172: 					left_candidates += binding.first + "." + column_name;
173: 				}
174: 			}
175: 			for (auto &binding : rhs_binding_list) {
176: 				for (auto &column_name : binding.second->names) {
177: 					if (!right_candidates.empty()) {
178: 						right_candidates += ", ";
179: 					}
180: 					right_candidates += binding.first + "." + column_name;
181: 				}
182: 			}
183: 			error_msg += "\n   Left candidates: " + left_candidates;
184: 			error_msg += "\n   Right candidates: " + right_candidates;
185: 			throw BinderException(FormatError(ref, error_msg));
186: 		}
187: 	} else if (!ref.using_columns.empty()) {
188: 		// USING columns
189: 		D_ASSERT(!result->condition);
190: 
191: 		for (idx_t i = 0; i < ref.using_columns.size(); i++) {
192: 			auto &using_column = ref.using_columns[i];
193: 			string left_binding;
194: 			string right_binding;
195: 			auto left_using_binding = left_binder.bind_context.GetUsingBinding(using_column);
196: 			auto right_using_binding = right_binder.bind_context.GetUsingBinding(using_column);
197: 			if (!left_using_binding) {
198: 				left_binding = left_binder.FindBinding(using_column, "left");
199: 			} else {
200: 				left_binding = left_using_binding->primary_binding;
201: 			}
202: 			if (!right_using_binding) {
203: 				right_binding = right_binder.FindBinding(using_column, "right");
204: 			} else {
205: 				right_binding = right_using_binding->primary_binding;
206: 			}
207: 			extra_conditions.push_back(
208: 			    AddCondition(context, left_binder, right_binder, left_binding, right_binding, using_column));
209: 
210: 			UsingColumnSet set;
211: 			AddUsingBindings(set, left_using_binding, left_binding);
212: 			AddUsingBindings(set, right_using_binding, right_binding);
213: 			SetPrimaryBinding(set, ref.type, left_binding, right_binding);
214: 			left_binder.bind_context.RemoveUsingBinding(using_column, left_using_binding);
215: 			right_binder.bind_context.RemoveUsingBinding(using_column, right_using_binding);
216: 			bind_context.AddUsingBinding(using_column, move(set));
217: 		}
218: 	}
219: 	bind_context.AddContext(move(left_binder.bind_context));
220: 	bind_context.AddContext(move(right_binder.bind_context));
221: 	MoveCorrelatedExpressions(left_binder);
222: 	MoveCorrelatedExpressions(right_binder);
223: 	for (auto &condition : extra_conditions) {
224: 		if (ref.condition) {
225: 			ref.condition = make_unique<ConjunctionExpression>(ExpressionType::CONJUNCTION_AND, move(ref.condition),
226: 			                                                   move(condition));
227: 		} else {
228: 			ref.condition = move(condition);
229: 		}
230: 	}
231: 	if (ref.condition) {
232: 		WhereBinder binder(*this, context);
233: 		result->condition = binder.Bind(ref.condition);
234: 	}
235: 	D_ASSERT(result->condition);
236: 	return move(result);
237: }
238: 
239: } // namespace duckdb
[end of src/planner/binder/tableref/bind_joinref.cpp]
[start of src/planner/binder/tableref/bind_subqueryref.cpp]
1: #include "duckdb/parser/tableref/subqueryref.hpp"
2: #include "duckdb/planner/binder.hpp"
3: #include "duckdb/planner/tableref/bound_subqueryref.hpp"
4: 
5: namespace duckdb {
6: 
7: unique_ptr<BoundTableRef> Binder::Bind(SubqueryRef &ref, CommonTableExpressionInfo *cte) {
8: 	auto binder = make_unique<Binder>(context, this);
9: 	if (cte) {
10: 		binder->bound_ctes.insert(cte);
11: 	}
12: 	binder->alias = ref.alias;
13: 	auto subquery = binder->BindNode(*ref.subquery->node);
14: 	idx_t bind_index = subquery->GetRootIndex();
15: 	auto result = make_unique<BoundSubqueryRef>(move(binder), move(subquery));
16: 
17: 	bind_context.AddSubquery(bind_index, ref.alias, ref, *result->subquery);
18: 	MoveCorrelatedExpressions(*result->binder);
19: 	return move(result);
20: }
21: 
22: } // namespace duckdb
[end of src/planner/binder/tableref/bind_subqueryref.cpp]
[start of src/planner/planner.cpp]
1: #include "duckdb/planner/planner.hpp"
2: 
3: #include "duckdb/common/serializer.hpp"
4: #include "duckdb/main/client_context.hpp"
5: #include "duckdb/main/database.hpp"
6: #include "duckdb/parser/statement/pragma_statement.hpp"
7: #include "duckdb/parser/statement/prepare_statement.hpp"
8: #include "duckdb/main/prepared_statement_data.hpp"
9: #include "duckdb/planner/binder.hpp"
10: #include "duckdb/planner/expression/bound_parameter_expression.hpp"
11: #include "duckdb/planner/operator/logical_execute.hpp"
12: #include "duckdb/planner/operator/logical_prepare.hpp"
13: #include "duckdb/planner/expression_binder/constant_binder.hpp"
14: #include "duckdb/parser/statement/execute_statement.hpp"
15: #include "duckdb/execution/expression_executor.hpp"
16: #include "duckdb/transaction/transaction.hpp"
17: 
18: namespace duckdb {
19: 
20: Planner::Planner(ClientContext &context) : binder(context), context(context) {
21: }
22: 
23: void Planner::CreatePlan(SQLStatement &statement) {
24: 	vector<BoundParameterExpression *> bound_parameters;
25: 
26: 	// first bind the tables and columns to the catalog
27: 	context.profiler.StartPhase("binder");
28: 	binder.parameters = &bound_parameters;
29: 	auto bound_statement = binder.Bind(statement);
30: 	context.profiler.EndPhase();
31: 
32: 	this->read_only = binder.read_only;
33: 	this->requires_valid_transaction = binder.requires_valid_transaction;
34: 	this->allow_stream_result = binder.allow_stream_result;
35: 	this->names = bound_statement.names;
36: 	this->types = bound_statement.types;
37: 	this->plan = move(bound_statement.plan);
38: 
39: 	// set up a map of parameter number -> value entries
40: 	for (auto &expr : bound_parameters) {
41: 		// check if the type of the parameter could be resolved
42: 		if (expr->return_type.id() == LogicalTypeId::INVALID || expr->return_type.id() == LogicalTypeId::UNKNOWN) {
43: 			throw BinderException("Could not determine type of parameters: try adding explicit type casts");
44: 		}
45: 		auto value = make_unique<Value>(expr->return_type);
46: 		expr->value = value.get();
47: 		// check if the parameter number has been used before
48: 		if (value_map.find(expr->parameter_nr) == value_map.end()) {
49: 			// not used before, create vector
50: 			value_map[expr->parameter_nr] = vector<unique_ptr<Value>>();
51: 		} else if (value_map[expr->parameter_nr].back()->type() != value->type()) {
52: 			// used before, but types are inconsistent
53: 			throw BinderException("Inconsistent types found for parameter with index %llu", expr->parameter_nr);
54: 		}
55: 		value_map[expr->parameter_nr].push_back(move(value));
56: 	}
57: }
58: 
59: shared_ptr<PreparedStatementData> Planner::PrepareSQLStatement(unique_ptr<SQLStatement> statement) {
60: 	auto copied_statement = statement->Copy();
61: 	// create a plan of the underlying statement
62: 	CreatePlan(move(statement));
63: 	// now create the logical prepare
64: 	auto prepared_data = make_shared<PreparedStatementData>(copied_statement->type);
65: 	prepared_data->unbound_statement = move(copied_statement);
66: 	prepared_data->names = names;
67: 	prepared_data->types = types;
68: 	prepared_data->value_map = move(value_map);
69: 	prepared_data->read_only = this->read_only;
70: 	prepared_data->requires_valid_transaction = this->requires_valid_transaction;
71: 	prepared_data->allow_stream_result = this->allow_stream_result;
72: 	prepared_data->catalog_version = Transaction::GetTransaction(context).catalog_version;
73: 	return prepared_data;
74: }
75: 
76: void Planner::PlanExecute(unique_ptr<SQLStatement> statement) {
77: 	auto &stmt = (ExecuteStatement &)*statement;
78: 
79: 	// bind the prepared statement
80: 	auto entry = context.prepared_statements.find(stmt.name);
81: 	if (entry == context.prepared_statements.end()) {
82: 		throw BinderException("Prepared statement \"%s\" does not exist", stmt.name);
83: 	}
84: 
85: 	// check if we need to rebind the prepared statement
86: 	// this happens if the catalog changes, since in this case e.g. tables we relied on may have been deleted
87: 	auto prepared = entry->second;
88: 	auto &catalog = Catalog::GetCatalog(context);
89: 	bool rebound = false;
90: 	if (catalog.GetCatalogVersion() != entry->second->catalog_version) {
91: 		// catalog was modified: rebind the statement before running the execute
92: 		prepared = PrepareSQLStatement(entry->second->unbound_statement->Copy());
93: 		if (prepared->types != entry->second->types) {
94: 			throw BinderException("Rebinding statement \"%s\" after catalog change resulted in change of types",
95: 			                      stmt.name);
96: 		}
97: 		rebound = true;
98: 	}
99: 
100: 	// the bound prepared statement is ready: bind any supplied parameters
101: 	vector<Value> bind_values;
102: 	for (idx_t i = 0; i < stmt.values.size(); i++) {
103: 		ConstantBinder cbinder(binder, context, "EXECUTE statement");
104: 		cbinder.target_type = prepared->GetType(i + 1);
105: 		auto bound_expr = cbinder.Bind(stmt.values[i]);
106: 
107: 		Value value = ExpressionExecutor::EvaluateScalar(*bound_expr);
108: 		bind_values.push_back(move(value));
109: 	}
110: 	prepared->Bind(move(bind_values));
111: 	if (rebound) {
112: 		return;
113: 	}
114: 
115: 	// copy the properties of the prepared statement into the planner
116: 	this->read_only = prepared->read_only;
117: 	this->requires_valid_transaction = prepared->requires_valid_transaction;
118: 	this->allow_stream_result = prepared->allow_stream_result;
119: 	this->names = prepared->names;
120: 	this->types = prepared->types;
121: 	this->plan = make_unique<LogicalExecute>(move(prepared));
122: }
123: 
124: void Planner::PlanPrepare(unique_ptr<SQLStatement> statement) {
125: 	auto &stmt = (PrepareStatement &)*statement;
126: 	auto prepared_data = PrepareSQLStatement(move(stmt.statement));
127: 
128: 	auto prepare = make_unique<LogicalPrepare>(stmt.name, move(prepared_data), move(plan));
129: 	// we can prepare in read-only mode: prepared statements are not written to the catalog
130: 	this->read_only = true;
131: 	// we can always prepare, even if the transaction has been invalidated
132: 	// this is required because most clients ALWAYS invoke prepared statements
133: 	this->requires_valid_transaction = false;
134: 	this->allow_stream_result = false;
135: 	this->names = {"Success"};
136: 	this->types = {LogicalType::BOOLEAN};
137: 	this->plan = move(prepare);
138: }
139: 
140: void Planner::CreatePlan(unique_ptr<SQLStatement> statement) {
141: 	D_ASSERT(statement);
142: 	switch (statement->type) {
143: 	case StatementType::SELECT_STATEMENT:
144: 	case StatementType::INSERT_STATEMENT:
145: 	case StatementType::COPY_STATEMENT:
146: 	case StatementType::DELETE_STATEMENT:
147: 	case StatementType::UPDATE_STATEMENT:
148: 	case StatementType::CREATE_STATEMENT:
149: 	case StatementType::DROP_STATEMENT:
150: 	case StatementType::ALTER_STATEMENT:
151: 	case StatementType::TRANSACTION_STATEMENT:
152: 	case StatementType::EXPLAIN_STATEMENT:
153: 	case StatementType::VACUUM_STATEMENT:
154: 	case StatementType::RELATION_STATEMENT:
155: 	case StatementType::CALL_STATEMENT:
156: 	case StatementType::EXPORT_STATEMENT:
157: 	case StatementType::PRAGMA_STATEMENT:
158: 	case StatementType::SHOW_STATEMENT:
159: 	case StatementType::SET_STATEMENT:
160: 		CreatePlan(*statement);
161: 		break;
162: 	case StatementType::EXECUTE_STATEMENT:
163: 		PlanExecute(move(statement));
164: 		break;
165: 	case StatementType::PREPARE_STATEMENT:
166: 		PlanPrepare(move(statement));
167: 		break;
168: 	default:
169: 		throw NotImplementedException("Cannot plan statement of type %s!", StatementTypeToString(statement->type));
170: 	}
171: }
172: 
173: } // namespace duckdb
[end of src/planner/planner.cpp]
[start of src/storage/checkpoint_manager.cpp]
1: #include "duckdb/storage/checkpoint_manager.hpp"
2: #include "duckdb/storage/block_manager.hpp"
3: #include "duckdb/storage/meta_block_reader.hpp"
4: 
5: #include "duckdb/common/serializer.hpp"
6: #include "duckdb/common/vector_operations/vector_operations.hpp"
7: #include "duckdb/common/types/null_value.hpp"
8: 
9: #include "duckdb/catalog/catalog.hpp"
10: #include "duckdb/catalog/catalog_entry/macro_catalog_entry.hpp"
11: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
12: #include "duckdb/catalog/catalog_entry/sequence_catalog_entry.hpp"
13: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
14: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
15: 
16: #include "duckdb/parser/parsed_data/create_schema_info.hpp"
17: #include "duckdb/parser/parsed_data/create_table_info.hpp"
18: #include "duckdb/parser/parsed_data/create_view_info.hpp"
19: 
20: #include "duckdb/planner/binder.hpp"
21: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
22: 
23: #include "duckdb/main/client_context.hpp"
24: #include "duckdb/main/connection.hpp"
25: #include "duckdb/main/database.hpp"
26: 
27: #include "duckdb/transaction/transaction_manager.hpp"
28: 
29: #include "duckdb/storage/checkpoint/table_data_writer.hpp"
30: #include "duckdb/storage/checkpoint/table_data_reader.hpp"
31: #include "duckdb/main/config.hpp"
32: 
33: namespace duckdb {
34: 
35: CheckpointManager::CheckpointManager(DatabaseInstance &db) : db(db) {
36: }
37: 
38: void CheckpointManager::CreateCheckpoint() {
39: 	auto &config = DBConfig::GetConfig(db);
40: 	auto &storage_manager = StorageManager::GetStorageManager(db);
41: 	if (storage_manager.InMemory()) {
42: 		return;
43: 	}
44: 	// assert that the checkpoint manager hasn't been used before
45: 	D_ASSERT(!metadata_writer);
46: 
47: 	auto &block_manager = BlockManager::GetBlockManager(db);
48: 	block_manager.StartCheckpoint();
49: 
50: 	//! Set up the writers for the checkpoints
51: 	metadata_writer = make_unique<MetaBlockWriter>(db);
52: 	tabledata_writer = make_unique<MetaBlockWriter>(db);
53: 
54: 	// get the id of the first meta block
55: 	block_id_t meta_block = metadata_writer->block->id;
56: 
57: 	vector<SchemaCatalogEntry *> schemas;
58: 	// we scan the set of committed schemas
59: 	auto &catalog = Catalog::GetCatalog(db);
60: 	catalog.schemas->Scan([&](CatalogEntry *entry) { schemas.push_back((SchemaCatalogEntry *)entry); });
61: 	// write the actual data into the database
62: 	// write the amount of schemas
63: 	metadata_writer->Write<uint32_t>(schemas.size());
64: 	for (auto &schema : schemas) {
65: 		WriteSchema(*schema);
66: 	}
67: 	// flush the meta data to disk
68: 	metadata_writer->Flush();
69: 	tabledata_writer->Flush();
70: 
71: 	// write a checkpoint flag to the WAL
72: 	// this protects against the rare event that the database crashes AFTER writing the file, but BEFORE truncating the
73: 	// WAL we write an entry CHECKPOINT "meta_block_id" into the WAL upon loading, if we see there is an entry
74: 	// CHECKPOINT "meta_block_id", and the id MATCHES the head idin the file we know that the database was successfully
75: 	// checkpointed, so we know that we should avoid replaying the WAL to avoid duplicating data
76: 	auto wal = storage_manager.GetWriteAheadLog();
77: 	wal->WriteCheckpoint(meta_block);
78: 	wal->Flush();
79: 
80: 	if (config.checkpoint_abort == CheckpointAbort::DEBUG_ABORT_BEFORE_HEADER) {
81: 		throw IOException("Checkpoint aborted before header write because of PRAGMA checkpoint_abort flag");
82: 	}
83: 
84: 	// finally write the updated header
85: 	DatabaseHeader header;
86: 	header.meta_block = meta_block;
87: 	block_manager.WriteHeader(header);
88: 
89: 	if (config.checkpoint_abort == CheckpointAbort::DEBUG_ABORT_BEFORE_TRUNCATE) {
90: 		throw IOException("Checkpoint aborted before truncate because of PRAGMA checkpoint_abort flag");
91: 	}
92: 
93: 	// truncate the WAL
94: 	wal->Truncate(0);
95: 
96: 	// mark all blocks written as part of the metadata as modified
97: 	for (auto &block_id : metadata_writer->written_blocks) {
98: 		block_manager.MarkBlockAsModified(block_id);
99: 	}
100: 	for (auto &block_id : tabledata_writer->written_blocks) {
101: 		block_manager.MarkBlockAsModified(block_id);
102: 	}
103: }
104: 
105: void CheckpointManager::LoadFromStorage() {
106: 	auto &block_manager = BlockManager::GetBlockManager(db);
107: 	block_id_t meta_block = block_manager.GetMetaBlock();
108: 	if (meta_block < 0) {
109: 		// storage is empty
110: 		return;
111: 	}
112: 
113: 	Connection con(db);
114: 	con.BeginTransaction();
115: 	// create the MetaBlockReader to read from the storage
116: 	MetaBlockReader reader(db, meta_block);
117: 	uint32_t schema_count = reader.Read<uint32_t>();
118: 	for (uint32_t i = 0; i < schema_count; i++) {
119: 		ReadSchema(*con.context, reader);
120: 	}
121: 	con.Commit();
122: }
123: 
124: //===--------------------------------------------------------------------===//
125: // Schema
126: //===--------------------------------------------------------------------===//
127: void CheckpointManager::WriteSchema(SchemaCatalogEntry &schema) {
128: 	// write the schema data
129: 	schema.Serialize(*metadata_writer);
130: 	// then, we fetch the tables/views/sequences information
131: 	vector<TableCatalogEntry *> tables;
132: 	vector<ViewCatalogEntry *> views;
133: 	schema.Scan(CatalogType::TABLE_ENTRY, [&](CatalogEntry *entry) {
134: 		if (entry->type == CatalogType::TABLE_ENTRY) {
135: 			tables.push_back((TableCatalogEntry *)entry);
136: 		} else if (entry->type == CatalogType::VIEW_ENTRY) {
137: 			views.push_back((ViewCatalogEntry *)entry);
138: 		} else {
139: 			throw NotImplementedException("Catalog type for entries");
140: 		}
141: 	});
142: 	vector<SequenceCatalogEntry *> sequences;
143: 	schema.Scan(CatalogType::SEQUENCE_ENTRY,
144: 	            [&](CatalogEntry *entry) { sequences.push_back((SequenceCatalogEntry *)entry); });
145: 
146: 	vector<MacroCatalogEntry *> macros;
147: 	schema.Scan(CatalogType::SCALAR_FUNCTION_ENTRY, [&](CatalogEntry *entry) {
148: 		if (entry->type == CatalogType::MACRO_ENTRY) {
149: 			macros.push_back((MacroCatalogEntry *)entry);
150: 		}
151: 	});
152: 
153: 	// write the sequences
154: 	metadata_writer->Write<uint32_t>(sequences.size());
155: 	for (auto &seq : sequences) {
156: 		WriteSequence(*seq);
157: 	}
158: 	// now write the tables
159: 	metadata_writer->Write<uint32_t>(tables.size());
160: 	for (auto &table : tables) {
161: 		WriteTable(*table);
162: 	}
163: 	// now write the views
164: 	metadata_writer->Write<uint32_t>(views.size());
165: 	for (auto &view : views) {
166: 		WriteView(*view);
167: 	}
168: 	// finally write the macro's
169: 	metadata_writer->Write<uint32_t>(macros.size());
170: 	for (auto &macro : macros) {
171: 		WriteMacro(*macro);
172: 	}
173: }
174: 
175: void CheckpointManager::ReadSchema(ClientContext &context, MetaBlockReader &reader) {
176: 	auto &catalog = Catalog::GetCatalog(db);
177: 
178: 	// read the schema and create it in the catalog
179: 	auto info = SchemaCatalogEntry::Deserialize(reader);
180: 	// we set create conflict to ignore to ignore the failure of recreating the main schema
181: 	info->on_conflict = OnCreateConflict::IGNORE_ON_CONFLICT;
182: 	catalog.CreateSchema(context, info.get());
183: 
184: 	// read the sequences
185: 	uint32_t seq_count = reader.Read<uint32_t>();
186: 	for (uint32_t i = 0; i < seq_count; i++) {
187: 		ReadSequence(context, reader);
188: 	}
189: 	// read the table count and recreate the tables
190: 	uint32_t table_count = reader.Read<uint32_t>();
191: 	for (uint32_t i = 0; i < table_count; i++) {
192: 		ReadTable(context, reader);
193: 	}
194: 	// now read the views
195: 	uint32_t view_count = reader.Read<uint32_t>();
196: 	for (uint32_t i = 0; i < view_count; i++) {
197: 		ReadView(context, reader);
198: 	}
199: 	// finally read the macro's
200: 	uint32_t macro_count = reader.Read<uint32_t>();
201: 	for (uint32_t i = 0; i < macro_count; i++) {
202: 		ReadMacro(context, reader);
203: 	}
204: }
205: 
206: //===--------------------------------------------------------------------===//
207: // Views
208: //===--------------------------------------------------------------------===//
209: void CheckpointManager::WriteView(ViewCatalogEntry &view) {
210: 	view.Serialize(*metadata_writer);
211: }
212: 
213: void CheckpointManager::ReadView(ClientContext &context, MetaBlockReader &reader) {
214: 	auto info = ViewCatalogEntry::Deserialize(reader);
215: 
216: 	auto &catalog = Catalog::GetCatalog(db);
217: 	catalog.CreateView(context, info.get());
218: }
219: 
220: //===--------------------------------------------------------------------===//
221: // Sequences
222: //===--------------------------------------------------------------------===//
223: void CheckpointManager::WriteSequence(SequenceCatalogEntry &seq) {
224: 	seq.Serialize(*metadata_writer);
225: }
226: 
227: void CheckpointManager::ReadSequence(ClientContext &context, MetaBlockReader &reader) {
228: 	auto info = SequenceCatalogEntry::Deserialize(reader);
229: 
230: 	auto &catalog = Catalog::GetCatalog(db);
231: 	catalog.CreateSequence(context, info.get());
232: }
233: 
234: //===--------------------------------------------------------------------===//
235: // Macro's
236: //===--------------------------------------------------------------------===//
237: void CheckpointManager::WriteMacro(MacroCatalogEntry &macro) {
238: 	macro.Serialize(*metadata_writer);
239: }
240: 
241: void CheckpointManager::ReadMacro(ClientContext &context, MetaBlockReader &reader) {
242: 	auto info = MacroCatalogEntry::Deserialize(reader);
243: 
244: 	auto &catalog = Catalog::GetCatalog(db);
245: 	catalog.CreateFunction(context, info.get());
246: }
247: 
248: //===--------------------------------------------------------------------===//
249: // Table Metadata
250: //===--------------------------------------------------------------------===//
251: void CheckpointManager::WriteTable(TableCatalogEntry &table) {
252: 	// write the table meta data
253: 	table.Serialize(*metadata_writer);
254: 	//! write the blockId for the table info
255: 	metadata_writer->Write<block_id_t>(tabledata_writer->block->id);
256: 	//! and the offset to where the info starts
257: 	metadata_writer->Write<uint64_t>(tabledata_writer->offset);
258: 	// now we need to write the table data
259: 	TableDataWriter writer(db, table, *tabledata_writer);
260: 	writer.WriteTableData();
261: }
262: 
263: void CheckpointManager::ReadTable(ClientContext &context, MetaBlockReader &reader) {
264: 	// deserialize the table meta data
265: 	auto info = TableCatalogEntry::Deserialize(reader);
266: 	// bind the info
267: 	Binder binder(context);
268: 	auto bound_info = binder.BindCreateTableInfo(move(info));
269: 
270: 	// now read the actual table data and place it into the create table info
271: 	auto block_id = reader.Read<block_id_t>();
272: 	auto offset = reader.Read<uint64_t>();
273: 	MetaBlockReader table_data_reader(db, block_id);
274: 	table_data_reader.offset = offset;
275: 	TableDataReader data_reader(db, table_data_reader, *bound_info);
276: 	data_reader.ReadTableData();
277: 
278: 	// finally create the table in the catalog
279: 	auto &catalog = Catalog::GetCatalog(db);
280: 	catalog.CreateTable(context, bound_info.get());
281: }
282: 
283: } // namespace duckdb
[end of src/storage/checkpoint_manager.cpp]
[start of src/storage/wal_replay.cpp]
1: #include "duckdb/storage/write_ahead_log.hpp"
2: #include "duckdb/storage/data_table.hpp"
3: #include "duckdb/common/serializer/buffered_file_reader.hpp"
4: #include "duckdb/catalog/catalog_entry/macro_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
6: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb/main/connection.hpp"
9: #include "duckdb/main/database.hpp"
10: #include "duckdb/parser/parsed_data/alter_table_info.hpp"
11: #include "duckdb/parser/parsed_data/drop_info.hpp"
12: #include "duckdb/parser/parsed_data/create_schema_info.hpp"
13: #include "duckdb/parser/parsed_data/create_table_info.hpp"
14: #include "duckdb/parser/parsed_data/create_view_info.hpp"
15: #include "duckdb/planner/binder.hpp"
16: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
17: #include "duckdb/common/printer.hpp"
18: #include "duckdb/common/string_util.hpp"
19: 
20: namespace duckdb {
21: class ReplayState {
22: public:
23: 	ReplayState(DatabaseInstance &db, ClientContext &context, Deserializer &source)
24: 	    : db(db), context(context), source(source), current_table(nullptr), deserialize_only(false),
25: 	      checkpoint_id(INVALID_BLOCK) {
26: 	}
27: 
28: 	DatabaseInstance &db;
29: 	ClientContext &context;
30: 	Deserializer &source;
31: 	TableCatalogEntry *current_table;
32: 	bool deserialize_only;
33: 	block_id_t checkpoint_id;
34: 
35: public:
36: 	void ReplayEntry(WALType entry_type);
37: 
38: private:
39: 	void ReplayCreateTable();
40: 	void ReplayDropTable();
41: 	void ReplayAlter();
42: 
43: 	void ReplayCreateView();
44: 	void ReplayDropView();
45: 
46: 	void ReplayCreateSchema();
47: 	void ReplayDropSchema();
48: 
49: 	void ReplayCreateSequence();
50: 	void ReplayDropSequence();
51: 	void ReplaySequenceValue();
52: 
53: 	void ReplayCreateMacro();
54: 	void ReplayDropMacro();
55: 
56: 	void ReplayUseTable();
57: 	void ReplayInsert();
58: 	void ReplayDelete();
59: 	void ReplayUpdate();
60: 	void ReplayCheckpoint();
61: };
62: 
63: bool WriteAheadLog::Replay(DatabaseInstance &database, string &path) {
64: 	auto initial_reader = make_unique<BufferedFileReader>(database.GetFileSystem(), path.c_str());
65: 	if (initial_reader->Finished()) {
66: 		// WAL is empty
67: 		return false;
68: 	}
69: 	Connection con(database);
70: 	con.BeginTransaction();
71: 
72: 	// first deserialize the WAL to look for a checkpoint flag
73: 	// if there is a checkpoint flag, we might have already flushed the contents of the WAL to disk
74: 	ReplayState checkpoint_state(database, *con.context, *initial_reader);
75: 	checkpoint_state.deserialize_only = true;
76: 	try {
77: 		while (true) {
78: 			// read the current entry
79: 			WALType entry_type = initial_reader->Read<WALType>();
80: 			if (entry_type == WALType::WAL_FLUSH) {
81: 				// check if the file is exhausted
82: 				if (initial_reader->Finished()) {
83: 					// we finished reading the file: break
84: 					break;
85: 				}
86: 			} else {
87: 				// replay the entry
88: 				checkpoint_state.ReplayEntry(entry_type);
89: 			}
90: 		}
91: 	} catch (std::exception &ex) {
92: 		Printer::Print(StringUtil::Format("Exception in WAL playback during initial read: %s\n", ex.what()));
93: 		return false;
94: 	}
95: 	initial_reader.reset();
96: 	if (checkpoint_state.checkpoint_id != INVALID_BLOCK) {
97: 		// there is a checkpoint flag: check if we need to deserialize the WAL
98: 		auto &manager = BlockManager::GetBlockManager(database);
99: 		if (manager.IsRootBlock(checkpoint_state.checkpoint_id)) {
100: 			// the contents of the WAL have already been checkpointed
101: 			// we can safely truncate the WAL and ignore its contents
102: 			return true;
103: 		}
104: 	}
105: 
106: 	// we need to recover from the WAL: actually set up the replay state
107: 	BufferedFileReader reader(database.GetFileSystem(), path.c_str());
108: 	ReplayState state(database, *con.context, reader);
109: 
110: 	// replay the WAL
111: 	// note that everything is wrapped inside a try/catch block here
112: 	// there can be errors in WAL replay because of a corrupt WAL file
113: 	// in this case we should throw a warning but startup anyway
114: 	try {
115: 		while (true) {
116: 			// read the current entry
117: 			WALType entry_type = reader.Read<WALType>();
118: 			if (entry_type == WALType::WAL_FLUSH) {
119: 				// flush: commit the current transaction
120: 				con.Commit();
121: 				// check if the file is exhausted
122: 				if (reader.Finished()) {
123: 					// we finished reading the file: break
124: 					break;
125: 				}
126: 				// otherwise we keep on reading
127: 				con.BeginTransaction();
128: 			} else {
129: 				// replay the entry
130: 				state.ReplayEntry(entry_type);
131: 			}
132: 		}
133: 	} catch (std::exception &ex) {
134: 		// FIXME: this should report a proper warning in the connection
135: 		Printer::Print(StringUtil::Format("Exception in WAL playback: %s\n", ex.what()));
136: 		// exception thrown in WAL replay: rollback
137: 		con.Rollback();
138: 	}
139: 	return false;
140: }
141: 
142: //===--------------------------------------------------------------------===//
143: // Replay Entries
144: //===--------------------------------------------------------------------===//
145: void ReplayState::ReplayEntry(WALType entry_type) {
146: 	switch (entry_type) {
147: 	case WALType::CREATE_TABLE:
148: 		ReplayCreateTable();
149: 		break;
150: 	case WALType::DROP_TABLE:
151: 		ReplayDropTable();
152: 		break;
153: 	case WALType::ALTER_INFO:
154: 		ReplayAlter();
155: 		break;
156: 	case WALType::CREATE_VIEW:
157: 		ReplayCreateView();
158: 		break;
159: 	case WALType::DROP_VIEW:
160: 		ReplayDropView();
161: 		break;
162: 	case WALType::CREATE_SCHEMA:
163: 		ReplayCreateSchema();
164: 		break;
165: 	case WALType::DROP_SCHEMA:
166: 		ReplayDropSchema();
167: 		break;
168: 	case WALType::CREATE_SEQUENCE:
169: 		ReplayCreateSequence();
170: 		break;
171: 	case WALType::DROP_SEQUENCE:
172: 		ReplayDropSequence();
173: 		break;
174: 	case WALType::SEQUENCE_VALUE:
175: 		ReplaySequenceValue();
176: 		break;
177: 	case WALType::CREATE_MACRO:
178: 		ReplayCreateMacro();
179: 		break;
180: 	case WALType::DROP_MACRO:
181: 		ReplayDropMacro();
182: 		break;
183: 	case WALType::USE_TABLE:
184: 		ReplayUseTable();
185: 		break;
186: 	case WALType::INSERT_TUPLE:
187: 		ReplayInsert();
188: 		break;
189: 	case WALType::DELETE_TUPLE:
190: 		ReplayDelete();
191: 		break;
192: 	case WALType::UPDATE_TUPLE:
193: 		ReplayUpdate();
194: 		break;
195: 	case WALType::CHECKPOINT:
196: 		ReplayCheckpoint();
197: 		break;
198: 	default:
199: 		throw Exception("Invalid WAL entry type!");
200: 	}
201: }
202: 
203: //===--------------------------------------------------------------------===//
204: // Replay Table
205: //===--------------------------------------------------------------------===//
206: void ReplayState::ReplayCreateTable() {
207: 	auto info = TableCatalogEntry::Deserialize(source);
208: 	if (deserialize_only) {
209: 		return;
210: 	}
211: 
212: 	// bind the constraints to the table again
213: 	Binder binder(context);
214: 	auto bound_info = binder.BindCreateTableInfo(move(info));
215: 
216: 	auto &catalog = Catalog::GetCatalog(context);
217: 	catalog.CreateTable(context, bound_info.get());
218: }
219: 
220: void ReplayState::ReplayDropTable() {
221: 	DropInfo info;
222: 
223: 	info.type = CatalogType::TABLE_ENTRY;
224: 	info.schema = source.Read<string>();
225: 	info.name = source.Read<string>();
226: 	if (deserialize_only) {
227: 		return;
228: 	}
229: 
230: 	auto &catalog = Catalog::GetCatalog(context);
231: 	catalog.DropEntry(context, &info);
232: }
233: 
234: void ReplayState::ReplayAlter() {
235: 	auto info = AlterInfo::Deserialize(source);
236: 	if (deserialize_only) {
237: 		return;
238: 	}
239: 	auto &catalog = Catalog::GetCatalog(context);
240: 	catalog.Alter(context, info.get());
241: }
242: 
243: //===--------------------------------------------------------------------===//
244: // Replay View
245: //===--------------------------------------------------------------------===//
246: void ReplayState::ReplayCreateView() {
247: 	auto entry = ViewCatalogEntry::Deserialize(source);
248: 	if (deserialize_only) {
249: 		return;
250: 	}
251: 
252: 	auto &catalog = Catalog::GetCatalog(context);
253: 	catalog.CreateView(context, entry.get());
254: }
255: 
256: void ReplayState::ReplayDropView() {
257: 	DropInfo info;
258: 	info.type = CatalogType::VIEW_ENTRY;
259: 	info.schema = source.Read<string>();
260: 	info.name = source.Read<string>();
261: 	if (deserialize_only) {
262: 		return;
263: 	}
264: 	auto &catalog = Catalog::GetCatalog(context);
265: 	catalog.DropEntry(context, &info);
266: }
267: 
268: //===--------------------------------------------------------------------===//
269: // Replay Schema
270: //===--------------------------------------------------------------------===//
271: void ReplayState::ReplayCreateSchema() {
272: 	CreateSchemaInfo info;
273: 	info.schema = source.Read<string>();
274: 	if (deserialize_only) {
275: 		return;
276: 	}
277: 
278: 	auto &catalog = Catalog::GetCatalog(context);
279: 	catalog.CreateSchema(context, &info);
280: }
281: 
282: void ReplayState::ReplayDropSchema() {
283: 	DropInfo info;
284: 
285: 	info.type = CatalogType::SCHEMA_ENTRY;
286: 	info.name = source.Read<string>();
287: 	if (deserialize_only) {
288: 		return;
289: 	}
290: 
291: 	auto &catalog = Catalog::GetCatalog(context);
292: 	catalog.DropEntry(context, &info);
293: }
294: 
295: //===--------------------------------------------------------------------===//
296: // Replay Sequence
297: //===--------------------------------------------------------------------===//
298: void ReplayState::ReplayCreateSequence() {
299: 	auto entry = SequenceCatalogEntry::Deserialize(source);
300: 	if (deserialize_only) {
301: 		return;
302: 	}
303: 
304: 	auto &catalog = Catalog::GetCatalog(context);
305: 	catalog.CreateSequence(context, entry.get());
306: }
307: 
308: void ReplayState::ReplayDropSequence() {
309: 	DropInfo info;
310: 	info.type = CatalogType::SEQUENCE_ENTRY;
311: 	info.schema = source.Read<string>();
312: 	info.name = source.Read<string>();
313: 	if (deserialize_only) {
314: 		return;
315: 	}
316: 
317: 	auto &catalog = Catalog::GetCatalog(context);
318: 	catalog.DropEntry(context, &info);
319: }
320: 
321: void ReplayState::ReplaySequenceValue() {
322: 	auto schema = source.Read<string>();
323: 	auto name = source.Read<string>();
324: 	auto usage_count = source.Read<uint64_t>();
325: 	auto counter = source.Read<int64_t>();
326: 	if (deserialize_only) {
327: 		return;
328: 	}
329: 
330: 	// fetch the sequence from the catalog
331: 	auto &catalog = Catalog::GetCatalog(context);
332: 	auto seq = catalog.GetEntry<SequenceCatalogEntry>(context, schema, name);
333: 	if (usage_count > seq->usage_count) {
334: 		seq->usage_count = usage_count;
335: 		seq->counter = counter;
336: 	}
337: }
338: 
339: //===--------------------------------------------------------------------===//
340: // Replay Macro
341: //===--------------------------------------------------------------------===//
342: void ReplayState::ReplayCreateMacro() {
343: 	auto entry = MacroCatalogEntry::Deserialize(source);
344: 	if (deserialize_only) {
345: 		return;
346: 	}
347: 
348: 	auto &catalog = Catalog::GetCatalog(context);
349: 	catalog.CreateFunction(context, entry.get());
350: }
351: 
352: void ReplayState::ReplayDropMacro() {
353: 	DropInfo info;
354: 	info.type = CatalogType::MACRO_ENTRY;
355: 	info.schema = source.Read<string>();
356: 	info.name = source.Read<string>();
357: 	if (deserialize_only) {
358: 		return;
359: 	}
360: 
361: 	auto &catalog = Catalog::GetCatalog(context);
362: 	catalog.DropEntry(context, &info);
363: }
364: 
365: //===--------------------------------------------------------------------===//
366: // Replay Data
367: //===--------------------------------------------------------------------===//
368: void ReplayState::ReplayUseTable() {
369: 	auto schema_name = source.Read<string>();
370: 	auto table_name = source.Read<string>();
371: 	if (deserialize_only) {
372: 		return;
373: 	}
374: 	auto &catalog = Catalog::GetCatalog(context);
375: 	current_table = catalog.GetEntry<TableCatalogEntry>(context, schema_name, table_name);
376: }
377: 
378: void ReplayState::ReplayInsert() {
379: 	DataChunk chunk;
380: 	chunk.Deserialize(source);
381: 	if (deserialize_only) {
382: 		return;
383: 	}
384: 	if (!current_table) {
385: 		throw Exception("Corrupt WAL: insert without table");
386: 	}
387: 
388: 	// append to the current table
389: 	current_table->storage->Append(*current_table, context, chunk);
390: }
391: 
392: void ReplayState::ReplayDelete() {
393: 	DataChunk chunk;
394: 	chunk.Deserialize(source);
395: 	if (deserialize_only) {
396: 		return;
397: 	}
398: 	if (!current_table) {
399: 		throw Exception("Corrupt WAL: delete without table");
400: 	}
401: 
402: 	D_ASSERT(chunk.ColumnCount() == 1 && chunk.data[0].GetType() == LOGICAL_ROW_TYPE);
403: 	row_t row_ids[1];
404: 	Vector row_identifiers(LOGICAL_ROW_TYPE, (data_ptr_t)row_ids);
405: 
406: 	auto source_ids = FlatVector::GetData<row_t>(chunk.data[0]);
407: 	// delete the tuples from the current table
408: 	for (idx_t i = 0; i < chunk.size(); i++) {
409: 		row_ids[0] = source_ids[i];
410: 		current_table->storage->Delete(*current_table, context, row_identifiers, 1);
411: 	}
412: }
413: 
414: void ReplayState::ReplayUpdate() {
415: 	idx_t column_index = source.Read<column_t>();
416: 
417: 	DataChunk chunk;
418: 	chunk.Deserialize(source);
419: 	if (deserialize_only) {
420: 		return;
421: 	}
422: 	if (!current_table) {
423: 		throw Exception("Corrupt WAL: update without table");
424: 	}
425: 
426: 	vector<column_t> column_ids {column_index};
427: 	if (column_index >= current_table->columns.size()) {
428: 		throw Exception("Corrupt WAL: column index for update out of bounds");
429: 	}
430: 
431: 	// remove the row id vector from the chunk
432: 	auto row_ids = move(chunk.data.back());
433: 	chunk.data.pop_back();
434: 
435: 	// now perform the update
436: 	current_table->storage->Update(*current_table, context, row_ids, column_ids, chunk);
437: }
438: 
439: void ReplayState::ReplayCheckpoint() {
440: 	checkpoint_id = source.Read<block_id_t>();
441: }
442: 
443: } // namespace duckdb
[end of src/storage/wal_replay.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: