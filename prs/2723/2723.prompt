You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Python: In-memory duckdb keep increasing indefinitely
#### What happens?
I run a in-memory duckdb python (initialise it with a table of 200K records, memory~250MB after inserting those, id column as the primary key) and the process subscribe to a stream of update (pandas dataframe) which keep updating the table by **cursor.executemany("UPDATE TABLE set field1 = ?, field2= ? where id = ?", df.to_records())** for 500 records every second.

However, the memory of the python program keep increasing even there is no new records inserted ( I keep reusing the cursor for the updates)

If i comment out the **cursor.executemany** statement and just print out the dataframe. The memory doesn't increase while getting the update from the data stream.

Therefore, I am quite sure the memory increment is due to the update statement. I also set the memory limit by _PRAGMA memory_limit='1GB';_

Moreover, I got segmentation fault if i try to run a update-select (update a big table with 20k records from a table with 500 records) statement . If i just have say 5k records in that big table , then it runs fine.

#### To Reproduce
I will try to create a sample program later. But for now, wondering if I am doing anything wrong with the in-memory database.

#### Environment (please complete the following information):
 - OS: Windows 10
 - DuckDB Version: duckdb-0.3.0
 - DuckDB Client: Python

#### Before Submitting

- [x] **Have you tried this on the latest `master` branch?**
* **Python**: `pip install duckdb --upgrade --pre`
* **R**: `install.packages("https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz", repos = NULL)`
* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.

- [ ] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**


</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16: </p>
17: 
18: ## DuckDB
19: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/docs/why_duckdb.html).
20: 
21: ## Installation
22: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
23: 
24: ## Data Import
25: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
26: 
27: ```sql
28: SELECT * FROM 'myfile.csv';
29: SELECT * FROM 'myfile.parquet';
30: ```
31: 
32: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
33: 
34: ## SQL Reference
35: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
36: 
37: ## Development
38: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
39: 
40: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
41: 
42: 
[end of README.md]
[start of src/include/duckdb/storage/table/column_data.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/table/column_data.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/data_chunk.hpp"
12: #include "duckdb/storage/table/append_state.hpp"
13: #include "duckdb/storage/table/scan_state.hpp"
14: #include "duckdb/storage/statistics/base_statistics.hpp"
15: #include "duckdb/storage/data_pointer.hpp"
16: #include "duckdb/storage/table/persistent_table_data.hpp"
17: #include "duckdb/storage/statistics/segment_statistics.hpp"
18: #include "duckdb/storage/table/column_checkpoint_state.hpp"
19: #include "duckdb/common/mutex.hpp"
20: 
21: namespace duckdb {
22: class ColumnData;
23: class ColumnSegment;
24: class DatabaseInstance;
25: class RowGroup;
26: class TableDataWriter;
27: class Transaction;
28: 
29: struct DataTableInfo;
30: 
31: struct ColumnCheckpointInfo {
32: 	ColumnCheckpointInfo(CompressionType compression_type_p) : compression_type(compression_type_p) {};
33: 	CompressionType compression_type;
34: };
35: 
36: class ColumnData {
37: 	friend class ColumnDataCheckpointer;
38: 
39: public:
40: 	ColumnData(DataTableInfo &info, idx_t column_index, idx_t start_row, LogicalType type, ColumnData *parent);
41: 	virtual ~ColumnData();
42: 
43: 	//! Table info for the column
44: 	DataTableInfo &info;
45: 	//! The column index of the column, either within the parent table or within the parent
46: 	idx_t column_index;
47: 	//! The start row
48: 	idx_t start;
49: 	//! The type of the column
50: 	LogicalType type;
51: 	//! The parent column (if any)
52: 	ColumnData *parent;
53: 
54: public:
55: 	virtual bool CheckZonemap(ColumnScanState &state, TableFilter &filter) = 0;
56: 
57: 	DatabaseInstance &GetDatabase() const;
58: 	DataTableInfo &GetTableInfo() const;
59: 	virtual idx_t GetMaxEntry();
60: 
61: 	//! The root type of the column
62: 	const LogicalType &RootType() const;
63: 
64: 	//! Initialize a scan of the column
65: 	virtual void InitializeScan(ColumnScanState &state);
66: 	//! Initialize a scan starting at the specified offset
67: 	virtual void InitializeScanWithOffset(ColumnScanState &state, idx_t row_idx);
68: 	//! Scan the next vector from the column
69: 	virtual idx_t Scan(Transaction &transaction, idx_t vector_index, ColumnScanState &state, Vector &result);
70: 	virtual idx_t ScanCommitted(idx_t vector_index, ColumnScanState &state, Vector &result, bool allow_updates);
71: 	virtual void ScanCommittedRange(idx_t row_group_start, idx_t offset_in_row_group, idx_t count, Vector &result);
72: 	virtual idx_t ScanCount(ColumnScanState &state, Vector &result, idx_t count);
73: 	//! Select
74: 	virtual void Select(Transaction &transaction, idx_t vector_index, ColumnScanState &state, Vector &result,
75: 	                    SelectionVector &sel, idx_t &count, const TableFilter &filter);
76: 	virtual void FilterScan(Transaction &transaction, idx_t vector_index, ColumnScanState &state, Vector &result,
77: 	                        SelectionVector &sel, idx_t count);
78: 	virtual void FilterScanCommitted(idx_t vector_index, ColumnScanState &state, Vector &result, SelectionVector &sel,
79: 	                                 idx_t count, bool allow_updates);
80: 
81: 	//! Skip the scan forward by "count" rows
82: 	virtual void Skip(ColumnScanState &state, idx_t count = STANDARD_VECTOR_SIZE);
83: 
84: 	//! Initialize an appending phase for this column
85: 	virtual void InitializeAppend(ColumnAppendState &state);
86: 	//! Append a vector of type [type] to the end of the column
87: 	virtual void Append(BaseStatistics &stats, ColumnAppendState &state, Vector &vector, idx_t count);
88: 	virtual void AppendData(BaseStatistics &stats, ColumnAppendState &state, VectorData &vdata, idx_t count);
89: 	//! Revert a set of appends to the ColumnData
90: 	virtual void RevertAppend(row_t start_row);
91: 
92: 	//! Fetch the vector from the column data that belongs to this specific row
93: 	virtual idx_t Fetch(ColumnScanState &state, row_t row_id, Vector &result);
94: 	//! Fetch a specific row id and append it to the vector
95: 	virtual void FetchRow(Transaction &transaction, ColumnFetchState &state, row_t row_id, Vector &result,
96: 	                      idx_t result_idx);
97: 
98: 	virtual void Update(Transaction &transaction, idx_t column_index, Vector &update_vector, row_t *row_ids,
99: 	                    idx_t offset, idx_t update_count);
100: 	virtual void UpdateColumn(Transaction &transaction, const vector<column_t> &column_path, Vector &update_vector,
101: 	                          row_t *row_ids, idx_t update_count, idx_t depth);
102: 	virtual unique_ptr<BaseStatistics> GetUpdateStatistics();
103: 
104: 	virtual void CommitDropColumn();
105: 
106: 	virtual unique_ptr<ColumnCheckpointState> CreateCheckpointState(RowGroup &row_group, TableDataWriter &writer);
107: 	virtual unique_ptr<ColumnCheckpointState> Checkpoint(RowGroup &row_group, TableDataWriter &writer,
108: 	                                                     ColumnCheckpointInfo &checkpoint_info);
109: 
110: 	virtual void CheckpointScan(ColumnSegment *segment, ColumnScanState &state, idx_t row_group_start, idx_t count,
111: 	                            Vector &scan_vector);
112: 
113: 	virtual void DeserializeColumn(Deserializer &source);
114: 	static shared_ptr<ColumnData> Deserialize(DataTableInfo &info, idx_t column_index, idx_t start_row,
115: 	                                          Deserializer &source, const LogicalType &type, ColumnData *parent);
116: 
117: 	virtual void GetStorageInfo(idx_t row_group_index, vector<idx_t> col_path, vector<vector<Value>> &result);
118: 	virtual void Verify(RowGroup &parent);
119: 
120: 	static shared_ptr<ColumnData> CreateColumn(DataTableInfo &info, idx_t column_index, idx_t start_row,
121: 	                                           const LogicalType &type, ColumnData *parent = nullptr);
122: 	static unique_ptr<ColumnData> CreateColumnUnique(DataTableInfo &info, idx_t column_index, idx_t start_row,
123: 	                                                 const LogicalType &type, ColumnData *parent = nullptr);
124: 
125: protected:
126: 	//! Append a transient segment
127: 	void AppendTransientSegment(idx_t start_row);
128: 
129: 	//! Scans a base vector from the column
130: 	idx_t ScanVector(ColumnScanState &state, Vector &result, idx_t remaining);
131: 	//! Scans a vector from the column merged with any potential updates
132: 	//! If ALLOW_UPDATES is set to false, the function will instead throw an exception if any updates are found
133: 	template <bool SCAN_COMMITTED, bool ALLOW_UPDATES>
134: 	idx_t ScanVector(Transaction *transaction, idx_t vector_index, ColumnScanState &state, Vector &result);
135: 
136: protected:
137: 	//! The segments holding the data of this column segment
138: 	SegmentTree data;
139: 	//! The lock for the updates
140: 	mutex update_lock;
141: 	//! The updates for this column segment
142: 	unique_ptr<UpdateSegment> updates;
143: };
144: 
145: } // namespace duckdb
[end of src/include/duckdb/storage/table/column_data.hpp]
[start of src/include/duckdb/storage/table/list_column_data.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/table/list_column_data.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/storage/table/column_data.hpp"
12: #include "duckdb/storage/table/validity_column_data.hpp"
13: 
14: namespace duckdb {
15: 
16: //! List column data represents a list
17: class ListColumnData : public ColumnData {
18: public:
19: 	ListColumnData(DataTableInfo &info, idx_t column_index, idx_t start_row, LogicalType type,
20: 	               ColumnData *parent = nullptr);
21: 
22: 	//! The child-column of the list
23: 	unique_ptr<ColumnData> child_column;
24: 	//! The validity column data of the struct
25: 	ValidityColumnData validity;
26: 
27: public:
28: 	bool CheckZonemap(ColumnScanState &state, TableFilter &filter) override;
29: 
30: 	void InitializeScan(ColumnScanState &state) override;
31: 	void InitializeScanWithOffset(ColumnScanState &state, idx_t row_idx) override;
32: 
33: 	idx_t Scan(Transaction &transaction, idx_t vector_index, ColumnScanState &state, Vector &result) override;
34: 	idx_t ScanCommitted(idx_t vector_index, ColumnScanState &state, Vector &result, bool allow_updates) override;
35: 	idx_t ScanCount(ColumnScanState &state, Vector &result, idx_t count) override;
36: 
37: 	void Skip(ColumnScanState &state, idx_t count = STANDARD_VECTOR_SIZE) override;
38: 
39: 	void InitializeAppend(ColumnAppendState &state) override;
40: 	void Append(BaseStatistics &stats, ColumnAppendState &state, Vector &vector, idx_t count) override;
41: 	void RevertAppend(row_t start_row) override;
42: 	idx_t Fetch(ColumnScanState &state, row_t row_id, Vector &result) override;
43: 	void FetchRow(Transaction &transaction, ColumnFetchState &state, row_t row_id, Vector &result,
44: 	              idx_t result_idx) override;
45: 	void Update(Transaction &transaction, idx_t column_index, Vector &update_vector, row_t *row_ids, idx_t offset,
46: 	            idx_t update_count) override;
47: 	void UpdateColumn(Transaction &transaction, const vector<column_t> &column_path, Vector &update_vector,
48: 	                  row_t *row_ids, idx_t update_count, idx_t depth) override;
49: 	unique_ptr<BaseStatistics> GetUpdateStatistics() override;
50: 
51: 	void CommitDropColumn() override;
52: 
53: 	unique_ptr<ColumnCheckpointState> CreateCheckpointState(RowGroup &row_group, TableDataWriter &writer) override;
54: 	unique_ptr<ColumnCheckpointState> Checkpoint(RowGroup &row_group, TableDataWriter &writer,
55: 	                                             ColumnCheckpointInfo &checkpoint_info) override;
56: 
57: 	void DeserializeColumn(Deserializer &source) override;
58: 
59: 	void GetStorageInfo(idx_t row_group_index, vector<idx_t> col_path, vector<vector<Value>> &result) override;
60: 
61: private:
62: 	list_entry_t FetchListEntry(idx_t row_idx);
63: };
64: 
65: } // namespace duckdb
[end of src/include/duckdb/storage/table/list_column_data.hpp]
[start of src/include/duckdb/storage/table/standard_column_data.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/table/standard_column_data.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/storage/table/column_data.hpp"
12: #include "duckdb/storage/table/validity_column_data.hpp"
13: 
14: namespace duckdb {
15: 
16: //! Standard column data represents a regular flat column (e.g. a column of type INTEGER or STRING)
17: class StandardColumnData : public ColumnData {
18: public:
19: 	StandardColumnData(DataTableInfo &info, idx_t column_index, idx_t start_row, LogicalType type,
20: 	                   ColumnData *parent = nullptr);
21: 
22: 	//! The validity column data
23: 	ValidityColumnData validity;
24: 
25: public:
26: 	bool CheckZonemap(ColumnScanState &state, TableFilter &filter) override;
27: 
28: 	void InitializeScan(ColumnScanState &state) override;
29: 	void InitializeScanWithOffset(ColumnScanState &state, idx_t row_idx) override;
30: 
31: 	idx_t Scan(Transaction &transaction, idx_t vector_index, ColumnScanState &state, Vector &result) override;
32: 	idx_t ScanCommitted(idx_t vector_index, ColumnScanState &state, Vector &result, bool allow_updates) override;
33: 	idx_t ScanCount(ColumnScanState &state, Vector &result, idx_t count) override;
34: 
35: 	void InitializeAppend(ColumnAppendState &state) override;
36: 	void AppendData(BaseStatistics &stats, ColumnAppendState &state, VectorData &vdata, idx_t count) override;
37: 	void RevertAppend(row_t start_row) override;
38: 	idx_t Fetch(ColumnScanState &state, row_t row_id, Vector &result) override;
39: 	void FetchRow(Transaction &transaction, ColumnFetchState &state, row_t row_id, Vector &result,
40: 	              idx_t result_idx) override;
41: 	void Update(Transaction &transaction, idx_t column_index, Vector &update_vector, row_t *row_ids, idx_t offset,
42: 	            idx_t update_count) override;
43: 	void UpdateColumn(Transaction &transaction, const vector<column_t> &column_path, Vector &update_vector,
44: 	                  row_t *row_ids, idx_t update_count, idx_t depth) override;
45: 	unique_ptr<BaseStatistics> GetUpdateStatistics() override;
46: 
47: 	void CommitDropColumn() override;
48: 
49: 	unique_ptr<ColumnCheckpointState> CreateCheckpointState(RowGroup &row_group, TableDataWriter &writer) override;
50: 	unique_ptr<ColumnCheckpointState> Checkpoint(RowGroup &row_group, TableDataWriter &writer,
51: 	                                             ColumnCheckpointInfo &checkpoint_info) override;
52: 	void CheckpointScan(ColumnSegment *segment, ColumnScanState &state, idx_t row_group_start, idx_t count,
53: 	                    Vector &scan_vector) override;
54: 
55: 	void DeserializeColumn(Deserializer &source) override;
56: 
57: 	void GetStorageInfo(idx_t row_group_index, vector<idx_t> col_path, vector<vector<Value>> &result) override;
58: 
59: 	void Verify(RowGroup &parent) override;
60: 
61: private:
62: 	template <bool SCAN_COMMITTED, bool ALLOW_UPDATES>
63: 	void TemplatedScan(Transaction *transaction, ColumnScanState &state, Vector &result);
64: };
65: 
66: } // namespace duckdb
[end of src/include/duckdb/storage/table/standard_column_data.hpp]
[start of src/include/duckdb/storage/table/struct_column_data.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/table/struct_column_data.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/storage/table/column_data.hpp"
12: #include "duckdb/storage/table/validity_column_data.hpp"
13: 
14: namespace duckdb {
15: 
16: //! Struct column data represents a struct
17: class StructColumnData : public ColumnData {
18: public:
19: 	StructColumnData(DataTableInfo &info, idx_t column_index, idx_t start_row, LogicalType type,
20: 	                 ColumnData *parent = nullptr);
21: 
22: 	//! The sub-columns of the struct
23: 	vector<unique_ptr<ColumnData>> sub_columns;
24: 	//! The validity column data of the struct
25: 	ValidityColumnData validity;
26: 
27: public:
28: 	bool CheckZonemap(ColumnScanState &state, TableFilter &filter) override;
29: 	idx_t GetMaxEntry() override;
30: 
31: 	void InitializeScan(ColumnScanState &state) override;
32: 	void InitializeScanWithOffset(ColumnScanState &state, idx_t row_idx) override;
33: 
34: 	idx_t Scan(Transaction &transaction, idx_t vector_index, ColumnScanState &state, Vector &result) override;
35: 	idx_t ScanCommitted(idx_t vector_index, ColumnScanState &state, Vector &result, bool allow_updates) override;
36: 	idx_t ScanCount(ColumnScanState &state, Vector &result, idx_t count) override;
37: 
38: 	void InitializeAppend(ColumnAppendState &state) override;
39: 	void Append(BaseStatistics &stats, ColumnAppendState &state, Vector &vector, idx_t count) override;
40: 	void RevertAppend(row_t start_row) override;
41: 	idx_t Fetch(ColumnScanState &state, row_t row_id, Vector &result) override;
42: 	void FetchRow(Transaction &transaction, ColumnFetchState &state, row_t row_id, Vector &result,
43: 	              idx_t result_idx) override;
44: 	void Update(Transaction &transaction, idx_t column_index, Vector &update_vector, row_t *row_ids, idx_t offset,
45: 	            idx_t update_count) override;
46: 	void UpdateColumn(Transaction &transaction, const vector<column_t> &column_path, Vector &update_vector,
47: 	                  row_t *row_ids, idx_t update_count, idx_t depth) override;
48: 	unique_ptr<BaseStatistics> GetUpdateStatistics() override;
49: 
50: 	void CommitDropColumn() override;
51: 
52: 	unique_ptr<ColumnCheckpointState> CreateCheckpointState(RowGroup &row_group, TableDataWriter &writer) override;
53: 	unique_ptr<ColumnCheckpointState> Checkpoint(RowGroup &row_group, TableDataWriter &writer,
54: 	                                             ColumnCheckpointInfo &checkpoint_info) override;
55: 
56: 	void DeserializeColumn(Deserializer &source) override;
57: 
58: 	void GetStorageInfo(idx_t row_group_index, vector<idx_t> col_path, vector<vector<Value>> &result) override;
59: 
60: 	void Verify(RowGroup &parent) override;
61: };
62: 
63: } // namespace duckdb
[end of src/include/duckdb/storage/table/struct_column_data.hpp]
[start of src/include/duckdb/storage/table/update_segment.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/table/update_segment.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/storage/table/row_group.hpp"
12: #include "duckdb/storage/storage_lock.hpp"
13: #include "duckdb/storage/statistics/segment_statistics.hpp"
14: #include "duckdb/common/types/string_heap.hpp"
15: 
16: namespace duckdb {
17: class ColumnData;
18: class DataTable;
19: class Vector;
20: struct UpdateInfo;
21: struct UpdateNode;
22: 
23: class UpdateSegment {
24: public:
25: 	UpdateSegment(ColumnData &column_data);
26: 	~UpdateSegment();
27: 
28: 	ColumnData &column_data;
29: 
30: public:
31: 	bool HasUpdates() const;
32: 	bool HasUncommittedUpdates(idx_t vector_index);
33: 	bool HasUpdates(idx_t vector_index) const;
34: 	bool HasUpdates(idx_t start_row_idx, idx_t end_row_idx);
35: 	void ClearUpdates();
36: 
37: 	void FetchUpdates(Transaction &transaction, idx_t vector_index, Vector &result);
38: 	void FetchCommitted(idx_t vector_index, Vector &result);
39: 	void FetchCommittedRange(idx_t start_row, idx_t count, Vector &result);
40: 	void Update(Transaction &transaction, idx_t column_index, Vector &update, row_t *ids, idx_t offset, idx_t count,
41: 	            Vector &base_data);
42: 	void FetchRow(Transaction &transaction, idx_t row_id, Vector &result, idx_t result_idx);
43: 
44: 	void RollbackUpdate(UpdateInfo *info);
45: 	void CleanupUpdateInternal(const StorageLockKey &lock, UpdateInfo *info);
46: 	void CleanupUpdate(UpdateInfo *info);
47: 
48: 	unique_ptr<BaseStatistics> GetStatistics();
49: 	StringHeap &GetStringHeap() {
50: 		return heap;
51: 	}
52: 
53: private:
54: 	//! The lock for the update segment
55: 	StorageLock lock;
56: 	//! The root node (if any)
57: 	unique_ptr<UpdateNode> root;
58: 	//! Update statistics
59: 	SegmentStatistics stats;
60: 	//! Stats lock
61: 	mutex stats_lock;
62: 	//! Internal type size
63: 	idx_t type_size;
64: 	//! String heap, only used for strings
65: 	StringHeap heap;
66: 
67: public:
68: 	typedef void (*initialize_update_function_t)(UpdateInfo *base_info, Vector &base_data, UpdateInfo *update_info,
69: 	                                             Vector &update, const SelectionVector &sel);
70: 	typedef void (*merge_update_function_t)(UpdateInfo *base_info, Vector &base_data, UpdateInfo *update_info,
71: 	                                        Vector &update, row_t *ids, idx_t count, const SelectionVector &sel);
72: 	typedef void (*fetch_update_function_t)(transaction_t start_time, transaction_t transaction_id, UpdateInfo *info,
73: 	                                        Vector &result);
74: 	typedef void (*fetch_committed_function_t)(UpdateInfo *info, Vector &result);
75: 	typedef void (*fetch_committed_range_function_t)(UpdateInfo *info, idx_t start, idx_t end, idx_t result_offset,
76: 	                                                 Vector &result);
77: 	typedef void (*fetch_row_function_t)(transaction_t start_time, transaction_t transaction_id, UpdateInfo *info,
78: 	                                     idx_t row_idx, Vector &result, idx_t result_idx);
79: 	typedef void (*rollback_update_function_t)(UpdateInfo *base_info, UpdateInfo *rollback_info);
80: 	typedef idx_t (*statistics_update_function_t)(UpdateSegment *segment, SegmentStatistics &stats, Vector &update,
81: 	                                              idx_t offset, idx_t count, SelectionVector &sel);
82: 
83: private:
84: 	initialize_update_function_t initialize_update_function;
85: 	merge_update_function_t merge_update_function;
86: 	fetch_update_function_t fetch_update_function;
87: 	fetch_committed_function_t fetch_committed_function;
88: 	fetch_committed_range_function_t fetch_committed_range;
89: 	fetch_row_function_t fetch_row_function;
90: 	rollback_update_function_t rollback_update_function;
91: 	statistics_update_function_t statistics_update_function;
92: 
93: private:
94: 	void InitializeUpdateInfo(UpdateInfo &info, row_t *ids, const SelectionVector &sel, idx_t count, idx_t vector_index,
95: 	                          idx_t vector_offset);
96: };
97: 
98: struct UpdateNodeData {
99: 	unique_ptr<UpdateInfo> info;
100: 	unique_ptr<sel_t[]> tuples;
101: 	unique_ptr<data_t[]> tuple_data;
102: };
103: 
104: struct UpdateNode {
105: 	unique_ptr<UpdateNodeData> info[RowGroup::ROW_GROUP_VECTOR_COUNT];
106: };
107: 
108: } // namespace duckdb
[end of src/include/duckdb/storage/table/update_segment.hpp]
[start of src/storage/table/column_data.cpp]
1: #include "duckdb/storage/table/column_data.hpp"
2: #include "duckdb/storage/data_table.hpp"
3: #include "duckdb/storage/storage_manager.hpp"
4: #include "duckdb/storage/data_pointer.hpp"
5: #include "duckdb/storage/table/update_segment.hpp"
6: #include "duckdb/planner/table_filter.hpp"
7: #include "duckdb/common/vector_operations/vector_operations.hpp"
8: #include "duckdb/storage/table/struct_column_data.hpp"
9: #include "duckdb/storage/table/list_column_data.hpp"
10: #include "duckdb/storage/table/standard_column_data.hpp"
11: 
12: #include "duckdb/storage/table/column_data_checkpointer.hpp"
13: #include "duckdb/function/compression_function.hpp"
14: 
15: namespace duckdb {
16: 
17: ColumnData::ColumnData(DataTableInfo &info, idx_t column_index, idx_t start_row, LogicalType type, ColumnData *parent)
18:     : info(info), column_index(column_index), start(start_row), type(move(type)), parent(parent) {
19: }
20: 
21: ColumnData::~ColumnData() {
22: }
23: 
24: DatabaseInstance &ColumnData::GetDatabase() const {
25: 	return info.db;
26: }
27: 
28: DataTableInfo &ColumnData::GetTableInfo() const {
29: 	return info;
30: }
31: 
32: const LogicalType &ColumnData::RootType() const {
33: 	if (parent) {
34: 		return parent->RootType();
35: 	}
36: 	return type;
37: }
38: 
39: idx_t ColumnData::GetMaxEntry() {
40: 	auto last_segment = data.GetLastSegment();
41: 	return last_segment ? last_segment->start + last_segment->count : start;
42: }
43: 
44: void ColumnData::InitializeScan(ColumnScanState &state) {
45: 	state.current = (ColumnSegment *)data.GetRootSegment();
46: 	state.row_index = state.current ? state.current->start : 0;
47: 	state.internal_index = state.row_index;
48: 	state.initialized = false;
49: }
50: 
51: void ColumnData::InitializeScanWithOffset(ColumnScanState &state, idx_t row_idx) {
52: 	state.current = (ColumnSegment *)data.GetSegment(row_idx);
53: 	state.row_index = row_idx;
54: 	state.internal_index = state.current->start;
55: 	state.initialized = false;
56: }
57: 
58: idx_t ColumnData::ScanVector(ColumnScanState &state, Vector &result, idx_t remaining) {
59: 	if (!state.initialized) {
60: 		D_ASSERT(state.current);
61: 		state.current->InitializeScan(state);
62: 		state.internal_index = state.current->start;
63: 		state.initialized = true;
64: 	}
65: 	D_ASSERT(state.internal_index <= state.row_index);
66: 	if (state.internal_index < state.row_index) {
67: 		state.current->Skip(state);
68: 	}
69: 	D_ASSERT(state.current->type == type);
70: 	idx_t initial_remaining = remaining;
71: 	while (remaining > 0) {
72: 		D_ASSERT(state.row_index >= state.current->start &&
73: 		         state.row_index <= state.current->start + state.current->count);
74: 		idx_t scan_count = MinValue<idx_t>(remaining, state.current->start + state.current->count - state.row_index);
75: 		idx_t result_offset = initial_remaining - remaining;
76: 		state.current->Scan(state, scan_count, result, result_offset, scan_count == initial_remaining);
77: 
78: 		state.row_index += scan_count;
79: 		remaining -= scan_count;
80: 		if (remaining > 0) {
81: 			if (!state.current->next) {
82: 				break;
83: 			}
84: 			state.current = (ColumnSegment *)state.current->next.get();
85: 			state.current->InitializeScan(state);
86: 			state.segment_checked = false;
87: 			D_ASSERT(state.row_index >= state.current->start &&
88: 			         state.row_index <= state.current->start + state.current->count);
89: 		}
90: 	}
91: 	state.internal_index = state.row_index;
92: 	return initial_remaining - remaining;
93: }
94: 
95: template <bool SCAN_COMMITTED, bool ALLOW_UPDATES>
96: idx_t ColumnData::ScanVector(Transaction *transaction, idx_t vector_index, ColumnScanState &state, Vector &result) {
97: 	auto scan_count = ScanVector(state, result, STANDARD_VECTOR_SIZE);
98: 
99: 	lock_guard<mutex> update_guard(update_lock);
100: 	if (updates) {
101: 		if (!ALLOW_UPDATES && updates->HasUncommittedUpdates(vector_index)) {
102: 			throw TransactionException("Cannot create index with outstanding updates");
103: 		}
104: 		result.Normalify(scan_count);
105: 		if (SCAN_COMMITTED) {
106: 			updates->FetchCommitted(vector_index, result);
107: 		} else {
108: 			D_ASSERT(transaction);
109: 			updates->FetchUpdates(*transaction, vector_index, result);
110: 		}
111: 	}
112: 	return scan_count;
113: }
114: 
115: template idx_t ColumnData::ScanVector<false, false>(Transaction *transaction, idx_t vector_index,
116:                                                     ColumnScanState &state, Vector &result);
117: template idx_t ColumnData::ScanVector<true, false>(Transaction *transaction, idx_t vector_index, ColumnScanState &state,
118:                                                    Vector &result);
119: template idx_t ColumnData::ScanVector<false, true>(Transaction *transaction, idx_t vector_index, ColumnScanState &state,
120:                                                    Vector &result);
121: template idx_t ColumnData::ScanVector<true, true>(Transaction *transaction, idx_t vector_index, ColumnScanState &state,
122:                                                   Vector &result);
123: 
124: idx_t ColumnData::Scan(Transaction &transaction, idx_t vector_index, ColumnScanState &state, Vector &result) {
125: 	return ScanVector<false, true>(&transaction, vector_index, state, result);
126: }
127: 
128: idx_t ColumnData::ScanCommitted(idx_t vector_index, ColumnScanState &state, Vector &result, bool allow_updates) {
129: 	if (allow_updates) {
130: 		return ScanVector<true, true>(nullptr, vector_index, state, result);
131: 	} else {
132: 		return ScanVector<true, false>(nullptr, vector_index, state, result);
133: 	}
134: }
135: 
136: void ColumnData::ScanCommittedRange(idx_t row_group_start, idx_t offset_in_row_group, idx_t count, Vector &result) {
137: 	ColumnScanState child_state;
138: 	InitializeScanWithOffset(child_state, row_group_start + offset_in_row_group);
139: 	auto scan_count = ScanVector(child_state, result, count);
140: 	if (updates) {
141: 		result.Normalify(scan_count);
142: 		updates->FetchCommittedRange(offset_in_row_group, count, result);
143: 	}
144: }
145: 
146: idx_t ColumnData::ScanCount(ColumnScanState &state, Vector &result, idx_t count) {
147: 	if (count == 0) {
148: 		return 0;
149: 	}
150: 	// ScanCount can only be used if there are no updates
151: 	D_ASSERT(!updates);
152: 	return ScanVector(state, result, count);
153: }
154: 
155: void ColumnData::Select(Transaction &transaction, idx_t vector_index, ColumnScanState &state, Vector &result,
156:                         SelectionVector &sel, idx_t &count, const TableFilter &filter) {
157: 	idx_t scan_count = Scan(transaction, vector_index, state, result);
158: 	result.Normalify(scan_count);
159: 	ColumnSegment::FilterSelection(sel, result, filter, count, FlatVector::Validity(result));
160: }
161: 
162: void ColumnData::FilterScan(Transaction &transaction, idx_t vector_index, ColumnScanState &state, Vector &result,
163:                             SelectionVector &sel, idx_t count) {
164: 	Scan(transaction, vector_index, state, result);
165: 	result.Slice(sel, count);
166: }
167: 
168: void ColumnData::FilterScanCommitted(idx_t vector_index, ColumnScanState &state, Vector &result, SelectionVector &sel,
169:                                      idx_t count, bool allow_updates) {
170: 	ScanCommitted(vector_index, state, result, allow_updates);
171: 	result.Slice(sel, count);
172: }
173: 
174: void ColumnData::Skip(ColumnScanState &state, idx_t count) {
175: 	state.Next(count);
176: }
177: 
178: void ColumnScanState::NextInternal(idx_t count) {
179: 	if (!current) {
180: 		//! There is no column segment
181: 		return;
182: 	}
183: 	row_index += count;
184: 	while (row_index >= current->start + current->count) {
185: 		current = (ColumnSegment *)current->next.get();
186: 		initialized = false;
187: 		segment_checked = false;
188: 		if (!current) {
189: 			break;
190: 		}
191: 	}
192: 	D_ASSERT(!current || (row_index >= current->start && row_index < current->start + current->count));
193: }
194: 
195: void ColumnScanState::Next(idx_t count) {
196: 	NextInternal(count);
197: 	for (auto &child_state : child_states) {
198: 		child_state.Next(count);
199: 	}
200: }
201: 
202: void ColumnScanState::NextVector() {
203: 	Next(STANDARD_VECTOR_SIZE);
204: }
205: 
206: void ColumnData::Append(BaseStatistics &stats, ColumnAppendState &state, Vector &vector, idx_t count) {
207: 	VectorData vdata;
208: 	vector.Orrify(count, vdata);
209: 	AppendData(stats, state, vdata, count);
210: }
211: 
212: void ColumnData::InitializeAppend(ColumnAppendState &state) {
213: 	lock_guard<mutex> tree_lock(data.node_lock);
214: 	if (data.nodes.empty()) {
215: 		// no segments yet, append an empty segment
216: 		AppendTransientSegment(start);
217: 	}
218: 	auto segment = (ColumnSegment *)data.GetLastSegment();
219: 	if (segment->segment_type == ColumnSegmentType::PERSISTENT) {
220: 		// no transient segments yet
221: 		auto total_rows = segment->start + segment->count;
222: 		AppendTransientSegment(total_rows);
223: 		state.current = (ColumnSegment *)data.GetLastSegment();
224: 	} else {
225: 		state.current = (ColumnSegment *)segment;
226: 	}
227: 
228: 	D_ASSERT(state.current->segment_type == ColumnSegmentType::TRANSIENT);
229: 	state.current->InitializeAppend(state);
230: }
231: 
232: void ColumnData::AppendData(BaseStatistics &stats, ColumnAppendState &state, VectorData &vdata, idx_t count) {
233: 	idx_t offset = 0;
234: 	while (true) {
235: 		// append the data from the vector
236: 		idx_t copied_elements = state.current->Append(state, vdata, offset, count);
237: 		stats.Merge(*state.current->stats.statistics);
238: 		if (copied_elements == count) {
239: 			// finished copying everything
240: 			break;
241: 		}
242: 
243: 		// we couldn't fit everything we wanted in the current column segment, create a new one
244: 		{
245: 			lock_guard<mutex> tree_lock(data.node_lock);
246: 			AppendTransientSegment(state.current->start + state.current->count);
247: 			state.current = (ColumnSegment *)data.GetLastSegment();
248: 			state.current->InitializeAppend(state);
249: 		}
250: 		offset += copied_elements;
251: 		count -= copied_elements;
252: 	}
253: }
254: 
255: void ColumnData::RevertAppend(row_t start_row) {
256: 	lock_guard<mutex> tree_lock(data.node_lock);
257: 	// check if this row is in the segment tree at all
258: 	if (idx_t(start_row) >= data.nodes.back().row_start + data.nodes.back().node->count) {
259: 		// the start row is equal to the final portion of the column data: nothing was ever appended here
260: 		D_ASSERT(idx_t(start_row) == data.nodes.back().row_start + data.nodes.back().node->count);
261: 		return;
262: 	}
263: 	// find the segment index that the current row belongs to
264: 	idx_t segment_index = data.GetSegmentIndex(start_row);
265: 	auto segment = data.nodes[segment_index].node;
266: 	auto &transient = (ColumnSegment &)*segment;
267: 	D_ASSERT(transient.segment_type == ColumnSegmentType::TRANSIENT);
268: 
269: 	// remove any segments AFTER this segment: they should be deleted entirely
270: 	if (segment_index < data.nodes.size() - 1) {
271: 		data.nodes.erase(data.nodes.begin() + segment_index + 1, data.nodes.end());
272: 	}
273: 	segment->next = nullptr;
274: 	transient.RevertAppend(start_row);
275: }
276: 
277: idx_t ColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &result) {
278: 	D_ASSERT(row_id >= 0);
279: 	D_ASSERT(idx_t(row_id) >= start);
280: 	// perform the fetch within the segment
281: 	state.row_index = start + ((row_id - start) / STANDARD_VECTOR_SIZE * STANDARD_VECTOR_SIZE);
282: 	state.current = (ColumnSegment *)data.GetSegment(state.row_index);
283: 	state.internal_index = state.current->start;
284: 	return ScanVector(state, result, STANDARD_VECTOR_SIZE);
285: }
286: 
287: void ColumnData::FetchRow(Transaction &transaction, ColumnFetchState &state, row_t row_id, Vector &result,
288:                           idx_t result_idx) {
289: 	auto segment = (ColumnSegment *)data.GetSegment(row_id);
290: 
291: 	// now perform the fetch within the segment
292: 	segment->FetchRow(state, row_id, result, result_idx);
293: 	// merge any updates made to this row
294: 	lock_guard<mutex> update_guard(update_lock);
295: 	if (updates) {
296: 		updates->FetchRow(transaction, row_id, result, result_idx);
297: 	}
298: }
299: 
300: void ColumnData::Update(Transaction &transaction, idx_t column_index, Vector &update_vector, row_t *row_ids,
301:                         idx_t offset, idx_t update_count) {
302: 	lock_guard<mutex> update_guard(update_lock);
303: 	if (!updates) {
304: 		updates = make_unique<UpdateSegment>(*this);
305: 	}
306: 	Vector base_vector(type);
307: 	ColumnScanState state;
308: 	auto fetch_count = Fetch(state, row_ids[offset], base_vector);
309: 
310: 	base_vector.Normalify(fetch_count);
311: 	updates->Update(transaction, column_index, update_vector, row_ids, offset, update_count, base_vector);
312: }
313: 
314: void ColumnData::UpdateColumn(Transaction &transaction, const vector<column_t> &column_path, Vector &update_vector,
315:                               row_t *row_ids, idx_t update_count, idx_t depth) {
316: 	// this method should only be called at the end of the path in the base column case
317: 	D_ASSERT(depth >= column_path.size());
318: 	ColumnData::Update(transaction, column_path[0], update_vector, row_ids, 0, update_count);
319: }
320: 
321: unique_ptr<BaseStatistics> ColumnData::GetUpdateStatistics() {
322: 	lock_guard<mutex> update_guard(update_lock);
323: 	return updates ? updates->GetStatistics() : nullptr;
324: }
325: 
326: void ColumnData::AppendTransientSegment(idx_t start_row) {
327: 	auto new_segment = ColumnSegment::CreateTransientSegment(GetDatabase(), type, start_row);
328: 	data.AppendSegment(move(new_segment));
329: }
330: 
331: void ColumnData::CommitDropColumn() {
332: 	auto &block_manager = BlockManager::GetBlockManager(GetDatabase());
333: 	auto segment = (ColumnSegment *)data.GetRootSegment();
334: 	while (segment) {
335: 		if (segment->segment_type == ColumnSegmentType::PERSISTENT) {
336: 			auto block_id = segment->GetBlockId();
337: 			if (block_id != INVALID_BLOCK) {
338: 				block_manager.MarkBlockAsModified(block_id);
339: 			}
340: 		}
341: 		segment = (ColumnSegment *)segment->next.get();
342: 	}
343: }
344: 
345: unique_ptr<ColumnCheckpointState> ColumnData::CreateCheckpointState(RowGroup &row_group, TableDataWriter &writer) {
346: 	return make_unique<ColumnCheckpointState>(row_group, *this, writer);
347: }
348: 
349: void ColumnData::CheckpointScan(ColumnSegment *segment, ColumnScanState &state, idx_t row_group_start, idx_t count,
350:                                 Vector &scan_vector) {
351: 	segment->Scan(state, count, scan_vector, 0, true);
352: 	if (updates) {
353: 		scan_vector.Normalify(count);
354: 		updates->FetchCommittedRange(state.row_index - row_group_start, count, scan_vector);
355: 	}
356: }
357: 
358: unique_ptr<ColumnCheckpointState> ColumnData::Checkpoint(RowGroup &row_group, TableDataWriter &writer,
359:                                                          ColumnCheckpointInfo &checkpoint_info) {
360: 	// scan the segments of the column data
361: 	// set up the checkpoint state
362: 	auto checkpoint_state = CreateCheckpointState(row_group, writer);
363: 	checkpoint_state->global_stats = BaseStatistics::CreateEmpty(type);
364: 
365: 	if (!data.root_node) {
366: 		// empty table: flush the empty list
367: 		return checkpoint_state;
368: 	}
369: 	lock_guard<mutex> update_guard(update_lock);
370: 
371: 	ColumnDataCheckpointer checkpointer(*this, row_group, *checkpoint_state, checkpoint_info);
372: 	checkpointer.Checkpoint(move(data.root_node));
373: 
374: 	// replace the old tree with the new one
375: 	data.Replace(checkpoint_state->new_tree);
376: 
377: 	return checkpoint_state;
378: }
379: 
380: void ColumnData::DeserializeColumn(Deserializer &source) {
381: 	// load the data pointers for the column
382: 	idx_t data_pointer_count = source.Read<idx_t>();
383: 	for (idx_t data_ptr = 0; data_ptr < data_pointer_count; data_ptr++) {
384: 		// read the data pointer
385: 		DataPointer data_pointer;
386: 		data_pointer.row_start = source.Read<idx_t>();
387: 		data_pointer.tuple_count = source.Read<idx_t>();
388: 		data_pointer.block_pointer.block_id = source.Read<block_id_t>();
389: 		data_pointer.block_pointer.offset = source.Read<uint32_t>();
390: 		data_pointer.compression_type = source.Read<CompressionType>();
391: 		data_pointer.statistics = BaseStatistics::Deserialize(source, type);
392: 
393: 		// create a persistent segment
394: 		auto segment = ColumnSegment::CreatePersistentSegment(
395: 		    GetDatabase(), data_pointer.block_pointer.block_id, data_pointer.block_pointer.offset, type,
396: 		    data_pointer.row_start, data_pointer.tuple_count, data_pointer.compression_type,
397: 		    move(data_pointer.statistics));
398: 		data.AppendSegment(move(segment));
399: 	}
400: }
401: 
402: shared_ptr<ColumnData> ColumnData::Deserialize(DataTableInfo &info, idx_t column_index, idx_t start_row,
403:                                                Deserializer &source, const LogicalType &type, ColumnData *parent) {
404: 	auto entry = ColumnData::CreateColumn(info, column_index, start_row, type, parent);
405: 	entry->DeserializeColumn(source);
406: 	return entry;
407: }
408: 
409: void ColumnData::GetStorageInfo(idx_t row_group_index, vector<idx_t> col_path, vector<vector<Value>> &result) {
410: 	D_ASSERT(!col_path.empty());
411: 
412: 	// convert the column path to a string
413: 	string col_path_str = "[";
414: 	for (idx_t i = 0; i < col_path.size(); i++) {
415: 		if (i > 0) {
416: 			col_path_str += ", ";
417: 		}
418: 		col_path_str += to_string(col_path[i]);
419: 	}
420: 	col_path_str += "]";
421: 
422: 	// iterate over the segments
423: 	idx_t segment_idx = 0;
424: 	auto segment = (ColumnSegment *)data.GetRootSegment();
425: 	while (segment) {
426: 		vector<Value> column_info;
427: 		// row_group_id
428: 		column_info.push_back(Value::BIGINT(row_group_index));
429: 		// column_id
430: 		column_info.push_back(Value::BIGINT(col_path[0]));
431: 		// column_path
432: 		column_info.emplace_back(col_path_str);
433: 		// segment_id
434: 		column_info.push_back(Value::BIGINT(segment_idx));
435: 		// segment_type
436: 		column_info.emplace_back(type.ToString());
437: 		// start
438: 		column_info.push_back(Value::BIGINT(segment->start));
439: 		// count
440: 		column_info.push_back(Value::BIGINT(segment->count));
441: 		// compression
442: 		column_info.emplace_back(CompressionTypeToString(segment->function->type));
443: 		// stats
444: 		column_info.emplace_back(segment->stats.statistics ? segment->stats.statistics->ToString()
445: 		                                                   : string("No Stats"));
446: 		// has_updates
447: 		column_info.push_back(Value::BOOLEAN(updates ? true : false));
448: 		// persistent
449: 		// block_id
450: 		// block_offset
451: 		if (segment->segment_type == ColumnSegmentType::PERSISTENT) {
452: 			column_info.push_back(Value::BOOLEAN(true));
453: 			column_info.push_back(Value::BIGINT(segment->GetBlockId()));
454: 			column_info.push_back(Value::BIGINT(segment->GetBlockOffset()));
455: 		} else {
456: 			column_info.push_back(Value::BOOLEAN(false));
457: 			column_info.emplace_back();
458: 			column_info.emplace_back();
459: 		}
460: 
461: 		result.push_back(move(column_info));
462: 
463: 		segment_idx++;
464: 		segment = (ColumnSegment *)segment->next.get();
465: 	}
466: }
467: 
468: void ColumnData::Verify(RowGroup &parent) {
469: #ifdef DEBUG
470: 	D_ASSERT(this->start == parent.start);
471: 	auto root = data.GetRootSegment();
472: 	if (root) {
473: 		D_ASSERT(root != nullptr);
474: 		D_ASSERT(root->start == this->start);
475: 		idx_t prev_end = root->start;
476: 		while (root) {
477: 			D_ASSERT(prev_end == root->start);
478: 			prev_end = root->start + root->count;
479: 			if (!root->next) {
480: 				D_ASSERT(prev_end == parent.start + parent.count);
481: 			}
482: 			root = root->next.get();
483: 		}
484: 	} else {
485: 		if (type.InternalType() != PhysicalType::STRUCT) {
486: 			D_ASSERT(parent.count == 0);
487: 		}
488: 	}
489: #endif
490: }
491: 
492: template <class RET, class OP>
493: static RET CreateColumnInternal(DataTableInfo &info, idx_t column_index, idx_t start_row, const LogicalType &type,
494:                                 ColumnData *parent) {
495: 	if (type.InternalType() == PhysicalType::STRUCT) {
496: 		return OP::template Create<StructColumnData>(info, column_index, start_row, type, parent);
497: 	} else if (type.InternalType() == PhysicalType::LIST) {
498: 		return OP::template Create<ListColumnData>(info, column_index, start_row, type, parent);
499: 	} else if (type.id() == LogicalTypeId::VALIDITY) {
500: 		return OP::template Create<ValidityColumnData>(info, column_index, start_row, parent);
501: 	}
502: 	return OP::template Create<StandardColumnData>(info, column_index, start_row, type, parent);
503: }
504: 
505: shared_ptr<ColumnData> ColumnData::CreateColumn(DataTableInfo &info, idx_t column_index, idx_t start_row,
506:                                                 const LogicalType &type, ColumnData *parent) {
507: 	return CreateColumnInternal<shared_ptr<ColumnData>, SharedConstructor>(info, column_index, start_row, type, parent);
508: }
509: 
510: unique_ptr<ColumnData> ColumnData::CreateColumnUnique(DataTableInfo &info, idx_t column_index, idx_t start_row,
511:                                                       const LogicalType &type, ColumnData *parent) {
512: 	return CreateColumnInternal<unique_ptr<ColumnData>, UniqueConstructor>(info, column_index, start_row, type, parent);
513: }
514: 
515: } // namespace duckdb
[end of src/storage/table/column_data.cpp]
[start of src/storage/table/list_column_data.cpp]
1: #include "duckdb/storage/table/list_column_data.hpp"
2: #include "duckdb/storage/statistics/list_statistics.hpp"
3: 
4: namespace duckdb {
5: 
6: ListColumnData::ListColumnData(DataTableInfo &info, idx_t column_index, idx_t start_row, LogicalType type_p,
7:                                ColumnData *parent)
8:     : ColumnData(info, column_index, start_row, move(type_p), parent), validity(info, 0, start_row, this) {
9: 	D_ASSERT(type.InternalType() == PhysicalType::LIST);
10: 	auto &child_type = ListType::GetChildType(type);
11: 	// the child column, with column index 1 (0 is the validity mask)
12: 	child_column = ColumnData::CreateColumnUnique(info, 1, start_row, child_type, this);
13: }
14: 
15: bool ListColumnData::CheckZonemap(ColumnScanState &state, TableFilter &filter) {
16: 	// table filters are not supported yet for list columns
17: 	return false;
18: }
19: 
20: void ListColumnData::InitializeScan(ColumnScanState &state) {
21: 	ColumnData::InitializeScan(state);
22: 
23: 	// initialize the validity segment
24: 	ColumnScanState validity_state;
25: 	validity.InitializeScan(validity_state);
26: 	state.child_states.push_back(move(validity_state));
27: 
28: 	// initialize the child scan
29: 	ColumnScanState child_state;
30: 	child_column->InitializeScan(child_state);
31: 	state.child_states.push_back(move(child_state));
32: }
33: 
34: list_entry_t ListColumnData::FetchListEntry(idx_t row_idx) {
35: 	auto segment = (ColumnSegment *)data.GetSegment(row_idx);
36: 	ColumnFetchState fetch_state;
37: 	Vector result(type, 1);
38: 	segment->FetchRow(fetch_state, row_idx, result, 0);
39: 
40: 	// initialize the child scan with the required offset
41: 	auto list_data = FlatVector::GetData<list_entry_t>(result);
42: 	return list_data[0];
43: }
44: 
45: void ListColumnData::InitializeScanWithOffset(ColumnScanState &state, idx_t row_idx) {
46: 	if (row_idx == 0) {
47: 		InitializeScan(state);
48: 		return;
49: 	}
50: 	ColumnData::InitializeScanWithOffset(state, row_idx);
51: 
52: 	// initialize the validity segment
53: 	ColumnScanState validity_state;
54: 	validity.InitializeScanWithOffset(validity_state, row_idx);
55: 	state.child_states.push_back(move(validity_state));
56: 
57: 	// we need to read the list at position row_idx to get the correct row offset of the child
58: 	auto list_entry = FetchListEntry(row_idx);
59: 	auto child_offset = list_entry.offset;
60: 
61: 	D_ASSERT(child_offset <= child_column->GetMaxEntry());
62: 	ColumnScanState child_state;
63: 	if (child_offset < child_column->GetMaxEntry()) {
64: 		child_column->InitializeScanWithOffset(child_state, child_offset);
65: 	}
66: 	state.child_states.push_back(move(child_state));
67: }
68: 
69: idx_t ListColumnData::Scan(Transaction &transaction, idx_t vector_index, ColumnScanState &state, Vector &result) {
70: 	return ScanCount(state, result, STANDARD_VECTOR_SIZE);
71: }
72: 
73: idx_t ListColumnData::ScanCommitted(idx_t vector_index, ColumnScanState &state, Vector &result, bool allow_updates) {
74: 	return ScanCount(state, result, STANDARD_VECTOR_SIZE);
75: }
76: 
77: idx_t ListColumnData::ScanCount(ColumnScanState &state, Vector &result, idx_t count) {
78: 	if (count == 0) {
79: 		return 0;
80: 	}
81: 	// updates not supported for lists
82: 	D_ASSERT(!updates);
83: 
84: 	idx_t scan_count = ScanVector(state, result, count);
85: 	validity.ScanCount(state.child_states[0], result, count);
86: 
87: 	auto data = FlatVector::GetData<list_entry_t>(result);
88: 	auto first_entry = data[0];
89: 	auto last_entry = data[scan_count - 1];
90: 
91: #ifdef DEBUG
92: 	for (idx_t i = 1; i < scan_count; i++) {
93: 		D_ASSERT(data[i].offset == data[i - 1].offset + data[i - 1].length);
94: 	}
95: #endif
96: 	// shift all offsets so they are 0 at the first entry
97: 	for (idx_t i = 0; i < scan_count; i++) {
98: 		data[i].offset -= first_entry.offset;
99: 	}
100: 
101: 	D_ASSERT(last_entry.offset >= first_entry.offset);
102: 	idx_t child_scan_count = last_entry.offset + last_entry.length - first_entry.offset;
103: 	ListVector::Reserve(result, child_scan_count);
104: 
105: 	if (child_scan_count > 0) {
106: 		auto &child_entry = ListVector::GetEntry(result);
107: 		D_ASSERT(child_entry.GetType().InternalType() == PhysicalType::STRUCT ||
108: 		         state.child_states[1].row_index + child_scan_count <= child_column->GetMaxEntry());
109: 		child_column->ScanCount(state.child_states[1], child_entry, child_scan_count);
110: 	}
111: 
112: 	ListVector::SetListSize(result, child_scan_count);
113: 	return scan_count;
114: }
115: 
116: void ListColumnData::Skip(ColumnScanState &state, idx_t count) {
117: 	// skip inside the validity segment
118: 	validity.Skip(state.child_states[0], count);
119: 
120: 	// we need to read the list entries/offsets to figure out how much to skip
121: 	// note that we only need to read the first and last entry
122: 	// however, let's just read all "count" entries for now
123: 	auto data = unique_ptr<list_entry_t[]>(new list_entry_t[count]);
124: 	Vector result(type, (data_ptr_t)data.get());
125: 	idx_t scan_count = ScanVector(state, result, count);
126: 	if (scan_count == 0) {
127: 		return;
128: 	}
129: 
130: 	auto &first_entry = data[0];
131: 	auto &last_entry = data[scan_count - 1];
132: 	idx_t child_scan_count = last_entry.offset + last_entry.length - first_entry.offset;
133: 
134: 	// skip the child state forward by the child_scan_count
135: 	child_column->Skip(state.child_states[1], child_scan_count);
136: }
137: 
138: void ListColumnData::InitializeAppend(ColumnAppendState &state) {
139: 	// initialize the list offset append
140: 	ColumnData::InitializeAppend(state);
141: 
142: 	// initialize the validity append
143: 	ColumnAppendState validity_append_state;
144: 	validity.InitializeAppend(validity_append_state);
145: 	state.child_appends.push_back(move(validity_append_state));
146: 
147: 	// initialize the child column append
148: 	ColumnAppendState child_append_state;
149: 	child_column->InitializeAppend(child_append_state);
150: 	state.child_appends.push_back(move(child_append_state));
151: }
152: 
153: void ListColumnData::Append(BaseStatistics &stats_p, ColumnAppendState &state, Vector &vector, idx_t count) {
154: 	D_ASSERT(count > 0);
155: 	auto &stats = (ListStatistics &)stats_p;
156: 
157: 	vector.Normalify(count);
158: 	auto &list_validity = FlatVector::Validity(vector);
159: 
160: 	// construct the list_entry_t entries to append to the column data
161: 	auto input_offsets = FlatVector::GetData<list_entry_t>(vector);
162: 	auto start_offset = child_column->GetMaxEntry();
163: 	idx_t child_count = 0;
164: 
165: 	auto append_offsets = unique_ptr<list_entry_t[]>(new list_entry_t[count]);
166: 	for (idx_t i = 0; i < count; i++) {
167: 		if (list_validity.RowIsValid(i)) {
168: 			append_offsets[i].offset = start_offset + input_offsets[i].offset;
169: 			append_offsets[i].length = input_offsets[i].length;
170: 			child_count += input_offsets[i].length;
171: 		} else {
172: 			if (i > 0) {
173: 				append_offsets[i].offset = append_offsets[i - 1].offset + append_offsets[i - 1].length;
174: 			} else {
175: 				append_offsets[i].offset = start_offset;
176: 			}
177: 			append_offsets[i].length = 0;
178: 		}
179: 	}
180: #ifdef DEBUG
181: 	D_ASSERT(append_offsets[0].offset == start_offset);
182: 	for (idx_t i = 1; i < count; i++) {
183: 		D_ASSERT(append_offsets[i].offset == append_offsets[i - 1].offset + append_offsets[i - 1].length);
184: 	}
185: 	D_ASSERT(append_offsets[count - 1].offset + append_offsets[count - 1].length - append_offsets[0].offset ==
186: 	         child_count);
187: #endif
188: 
189: 	VectorData vdata;
190: 	vdata.validity = list_validity;
191: 	vdata.sel = &FlatVector::INCREMENTAL_SELECTION_VECTOR;
192: 	vdata.data = (data_ptr_t)append_offsets.get();
193: 
194: 	// append the list offsets
195: 	ColumnData::AppendData(stats, state, vdata, count);
196: 	// append the validity data
197: 	validity.AppendData(*stats.validity_stats, state.child_appends[0], vdata, count);
198: 	// append the child vector
199: 	if (child_count > 0) {
200: 		auto &child_vector = ListVector::GetEntry(vector);
201: 		child_column->Append(*stats.child_stats, state.child_appends[1], child_vector, child_count);
202: 	}
203: }
204: 
205: void ListColumnData::RevertAppend(row_t start_row) {
206: 	ColumnData::RevertAppend(start_row);
207: 	validity.RevertAppend(start_row);
208: 	auto column_count = GetMaxEntry();
209: 	if (column_count > start) {
210: 		// revert append in the child column
211: 		auto list_entry = FetchListEntry(column_count - 1);
212: 		child_column->RevertAppend(list_entry.offset + list_entry.length);
213: 	}
214: }
215: 
216: idx_t ListColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &result) {
217: 	throw NotImplementedException("List Fetch");
218: }
219: 
220: void ListColumnData::Update(Transaction &transaction, idx_t column_index, Vector &update_vector, row_t *row_ids,
221:                             idx_t offset, idx_t update_count) {
222: 	throw NotImplementedException("List Update is not supported.");
223: }
224: 
225: void ListColumnData::UpdateColumn(Transaction &transaction, const vector<column_t> &column_path, Vector &update_vector,
226:                                   row_t *row_ids, idx_t update_count, idx_t depth) {
227: 	throw NotImplementedException("List Update Column is not supported");
228: }
229: 
230: unique_ptr<BaseStatistics> ListColumnData::GetUpdateStatistics() {
231: 	return nullptr;
232: }
233: 
234: void ListColumnData::FetchRow(Transaction &transaction, ColumnFetchState &state, row_t row_id, Vector &result,
235:                               idx_t result_idx) {
236: 	// insert any child states that are required
237: 	// we need two (validity & list child)
238: 	// note that we need a scan state for the child vector
239: 	// this is because we will (potentially) fetch more than one tuple from the list child
240: 	if (state.child_states.empty()) {
241: 		auto child_state = make_unique<ColumnFetchState>();
242: 		state.child_states.push_back(move(child_state));
243: 	}
244: 	// fetch the list_entry_t and the validity mask for that list
245: 	auto segment = (ColumnSegment *)data.GetSegment(row_id);
246: 
247: 	// now perform the fetch within the segment
248: 	segment->FetchRow(state, row_id, result, result_idx);
249: 	validity.FetchRow(transaction, *state.child_states[0], row_id, result, result_idx);
250: 
251: 	auto &validity = FlatVector::Validity(result);
252: 	auto list_data = FlatVector::GetData<list_entry_t>(result);
253: 	auto &list_entry = list_data[result_idx];
254: 	auto original_offset = list_entry.offset;
255: 	// set the list entry offset to the size of the current list
256: 	list_entry.offset = ListVector::GetListSize(result);
257: 	if (!validity.RowIsValid(result_idx)) {
258: 		// the list is NULL! no need to fetch the child
259: 		D_ASSERT(list_entry.length == 0);
260: 		return;
261: 	}
262: 
263: 	// now we need to read from the child all the elements between [offset...length]
264: 	auto child_scan_count = list_entry.length;
265: 	if (child_scan_count > 0) {
266: 		auto child_state = make_unique<ColumnScanState>();
267: 		auto &child_type = ListType::GetChildType(result.GetType());
268: 		Vector child_scan(child_type, child_scan_count);
269: 		// seek the scan towards the specified position and read [length] entries
270: 		child_column->InitializeScanWithOffset(*child_state, original_offset);
271: 		D_ASSERT(child_type.InternalType() == PhysicalType::STRUCT ||
272: 		         child_state->row_index + child_scan_count <= child_column->GetMaxEntry());
273: 		child_column->ScanCount(*child_state, child_scan, child_scan_count);
274: 
275: 		ListVector::Append(result, child_scan, child_scan_count);
276: 	}
277: }
278: 
279: void ListColumnData::CommitDropColumn() {
280: 	validity.CommitDropColumn();
281: 	child_column->CommitDropColumn();
282: }
283: 
284: struct ListColumnCheckpointState : public ColumnCheckpointState {
285: 	ListColumnCheckpointState(RowGroup &row_group, ColumnData &column_data, TableDataWriter &writer)
286: 	    : ColumnCheckpointState(row_group, column_data, writer) {
287: 		global_stats = make_unique<ListStatistics>(column_data.type);
288: 	}
289: 
290: 	unique_ptr<ColumnCheckpointState> validity_state;
291: 	unique_ptr<ColumnCheckpointState> child_state;
292: 
293: public:
294: 	unique_ptr<BaseStatistics> GetStatistics() override {
295: 		auto stats = global_stats->Copy();
296: 		auto &list_stats = (ListStatistics &)*stats;
297: 		stats->validity_stats = validity_state->GetStatistics();
298: 		list_stats.child_stats = child_state->GetStatistics();
299: 		return stats;
300: 	}
301: 
302: 	void FlushToDisk() override {
303: 		ColumnCheckpointState::FlushToDisk();
304: 		validity_state->FlushToDisk();
305: 		child_state->FlushToDisk();
306: 	}
307: };
308: 
309: unique_ptr<ColumnCheckpointState> ListColumnData::CreateCheckpointState(RowGroup &row_group, TableDataWriter &writer) {
310: 	return make_unique<ListColumnCheckpointState>(row_group, *this, writer);
311: }
312: 
313: unique_ptr<ColumnCheckpointState> ListColumnData::Checkpoint(RowGroup &row_group, TableDataWriter &writer,
314:                                                              ColumnCheckpointInfo &checkpoint_info) {
315: 	auto validity_state = validity.Checkpoint(row_group, writer, checkpoint_info);
316: 	auto base_state = ColumnData::Checkpoint(row_group, writer, checkpoint_info);
317: 	auto child_state = child_column->Checkpoint(row_group, writer, checkpoint_info);
318: 
319: 	auto &checkpoint_state = (ListColumnCheckpointState &)*base_state;
320: 	checkpoint_state.validity_state = move(validity_state);
321: 	checkpoint_state.child_state = move(child_state);
322: 	return base_state;
323: }
324: 
325: void ListColumnData::DeserializeColumn(Deserializer &source) {
326: 	ColumnData::DeserializeColumn(source);
327: 	validity.DeserializeColumn(source);
328: 	child_column->DeserializeColumn(source);
329: }
330: 
331: void ListColumnData::GetStorageInfo(idx_t row_group_index, vector<idx_t> col_path, vector<vector<Value>> &result) {
332: 	col_path.push_back(0);
333: 	validity.GetStorageInfo(row_group_index, col_path, result);
334: 	col_path.back() = 1;
335: 	child_column->GetStorageInfo(row_group_index, col_path, result);
336: }
337: 
338: } // namespace duckdb
[end of src/storage/table/list_column_data.cpp]
[start of src/storage/table/row_group.cpp]
1: #include "duckdb/storage/table/row_group.hpp"
2: #include "duckdb/common/types/vector.hpp"
3: #include "duckdb/transaction/transaction.hpp"
4: #include "duckdb/common/exception.hpp"
5: #include "duckdb/storage/table/column_data.hpp"
6: #include "duckdb/storage/table/standard_column_data.hpp"
7: #include "duckdb/storage/table/update_segment.hpp"
8: #include "duckdb/common/chrono.hpp"
9: #include "duckdb/planner/table_filter.hpp"
10: #include "duckdb/execution/expression_executor.hpp"
11: #include "duckdb/storage/checkpoint/table_data_writer.hpp"
12: #include "duckdb/storage/meta_block_reader.hpp"
13: #include "duckdb/transaction/transaction_manager.hpp"
14: 
15: namespace duckdb {
16: 
17: constexpr const idx_t RowGroup::ROW_GROUP_VECTOR_COUNT;
18: constexpr const idx_t RowGroup::ROW_GROUP_SIZE;
19: 
20: RowGroup::RowGroup(DatabaseInstance &db, DataTableInfo &table_info, idx_t start, idx_t count)
21:     : SegmentBase(start, count), db(db), table_info(table_info) {
22: 
23: 	Verify();
24: }
25: 
26: RowGroup::RowGroup(DatabaseInstance &db, DataTableInfo &table_info, const vector<LogicalType> &types,
27:                    RowGroupPointer &pointer)
28:     : SegmentBase(pointer.row_start, pointer.tuple_count), db(db), table_info(table_info) {
29: 	// deserialize the columns
30: 	if (pointer.data_pointers.size() != types.size()) {
31: 		throw IOException("Row group column count is unaligned with table column count. Corrupt file?");
32: 	}
33: 	for (idx_t i = 0; i < pointer.data_pointers.size(); i++) {
34: 		auto &block_pointer = pointer.data_pointers[i];
35: 		MetaBlockReader column_data_reader(db, block_pointer.block_id);
36: 		column_data_reader.offset = block_pointer.offset;
37: 		this->columns.push_back(ColumnData::Deserialize(table_info, i, start, column_data_reader, types[i], nullptr));
38: 	}
39: 
40: 	// set up the statistics
41: 	for (auto &stats : pointer.statistics) {
42: 		auto stats_type = stats->type;
43: 		this->stats.push_back(make_shared<SegmentStatistics>(stats_type, move(stats)));
44: 	}
45: 	this->version_info = move(pointer.versions);
46: 
47: 	Verify();
48: }
49: 
50: RowGroup::~RowGroup() {
51: }
52: 
53: void RowGroup::InitializeEmpty(const vector<LogicalType> &types) {
54: 	// set up the segment trees for the column segments
55: 	for (idx_t i = 0; i < types.size(); i++) {
56: 		auto column_data = ColumnData::CreateColumn(GetTableInfo(), i, start, types[i]);
57: 		stats.push_back(make_shared<SegmentStatistics>(types[i]));
58: 		columns.push_back(move(column_data));
59: 	}
60: }
61: 
62: bool RowGroup::InitializeScanWithOffset(RowGroupScanState &state, idx_t vector_offset) {
63: 	auto &column_ids = state.parent.column_ids;
64: 	if (state.parent.table_filters) {
65: 		if (!CheckZonemap(*state.parent.table_filters, column_ids)) {
66: 			return false;
67: 		}
68: 	}
69: 
70: 	state.row_group = this;
71: 	state.vector_index = vector_offset;
72: 	state.max_row =
73: 	    this->start > state.parent.max_row ? 0 : MinValue<idx_t>(this->count, state.parent.max_row - this->start);
74: 	state.column_scans = unique_ptr<ColumnScanState[]>(new ColumnScanState[column_ids.size()]);
75: 	for (idx_t i = 0; i < column_ids.size(); i++) {
76: 		auto column = column_ids[i];
77: 		if (column != COLUMN_IDENTIFIER_ROW_ID) {
78: 			columns[column]->InitializeScanWithOffset(state.column_scans[i],
79: 			                                          start + vector_offset * STANDARD_VECTOR_SIZE);
80: 		} else {
81: 			state.column_scans[i].current = nullptr;
82: 		}
83: 	}
84: 	return true;
85: }
86: 
87: bool RowGroup::InitializeScan(RowGroupScanState &state) {
88: 	auto &column_ids = state.parent.column_ids;
89: 	if (state.parent.table_filters) {
90: 		if (!CheckZonemap(*state.parent.table_filters, column_ids)) {
91: 			return false;
92: 		}
93: 	}
94: 	state.row_group = this;
95: 	state.vector_index = 0;
96: 	state.max_row =
97: 	    this->start > state.parent.max_row ? 0 : MinValue<idx_t>(this->count, state.parent.max_row - this->start);
98: 	state.column_scans = unique_ptr<ColumnScanState[]>(new ColumnScanState[column_ids.size()]);
99: 	for (idx_t i = 0; i < column_ids.size(); i++) {
100: 		auto column = column_ids[i];
101: 		if (column != COLUMN_IDENTIFIER_ROW_ID) {
102: 			columns[column]->InitializeScan(state.column_scans[i]);
103: 		} else {
104: 			state.column_scans[i].current = nullptr;
105: 		}
106: 	}
107: 	return true;
108: }
109: 
110: unique_ptr<RowGroup> RowGroup::AlterType(ClientContext &context, const LogicalType &target_type, idx_t changed_idx,
111:                                          ExpressionExecutor &executor, TableScanState &scan_state,
112:                                          DataChunk &scan_chunk) {
113: 	Verify();
114: 
115: 	// construct a new column data for this type
116: 	auto column_data = ColumnData::CreateColumn(GetTableInfo(), changed_idx, start, target_type);
117: 
118: 	ColumnAppendState append_state;
119: 	column_data->InitializeAppend(append_state);
120: 
121: 	// scan the original table, and fill the new column with the transformed value
122: 	InitializeScan(scan_state.row_group_scan_state);
123: 
124: 	Vector append_vector(target_type);
125: 	auto altered_col_stats = make_shared<SegmentStatistics>(target_type);
126: 	while (true) {
127: 		// scan the table
128: 		scan_chunk.Reset();
129: 		ScanCommitted(scan_state.row_group_scan_state, scan_chunk, TableScanType::TABLE_SCAN_COMMITTED_ROWS);
130: 		if (scan_chunk.size() == 0) {
131: 			break;
132: 		}
133: 		// execute the expression
134: 		executor.ExecuteExpression(scan_chunk, append_vector);
135: 		column_data->Append(*altered_col_stats->statistics, append_state, append_vector, scan_chunk.size());
136: 	}
137: 
138: 	// set up the row_group based on this row_group
139: 	auto row_group = make_unique<RowGroup>(db, table_info, this->start, this->count);
140: 	row_group->version_info = version_info;
141: 	for (idx_t i = 0; i < columns.size(); i++) {
142: 		if (i == changed_idx) {
143: 			// this is the altered column: use the new column
144: 			row_group->columns.push_back(move(column_data));
145: 			row_group->stats.push_back(move(altered_col_stats));
146: 		} else {
147: 			// this column was not altered: use the data directly
148: 			row_group->columns.push_back(columns[i]);
149: 			row_group->stats.push_back(stats[i]);
150: 		}
151: 	}
152: 	row_group->Verify();
153: 	return row_group;
154: }
155: 
156: unique_ptr<RowGroup> RowGroup::AddColumn(ClientContext &context, ColumnDefinition &new_column,
157:                                          ExpressionExecutor &executor, Expression *default_value, Vector &result) {
158: 	Verify();
159: 
160: 	// construct a new column data for the new column
161: 	auto added_column = ColumnData::CreateColumn(GetTableInfo(), columns.size(), start, new_column.type);
162: 
163: 	auto added_col_stats = make_shared<SegmentStatistics>(new_column.type);
164: 	idx_t rows_to_write = this->count;
165: 	if (rows_to_write > 0) {
166: 		DataChunk dummy_chunk;
167: 
168: 		ColumnAppendState state;
169: 		added_column->InitializeAppend(state);
170: 		for (idx_t i = 0; i < rows_to_write; i += STANDARD_VECTOR_SIZE) {
171: 			idx_t rows_in_this_vector = MinValue<idx_t>(rows_to_write - i, STANDARD_VECTOR_SIZE);
172: 			if (default_value) {
173: 				dummy_chunk.SetCardinality(rows_in_this_vector);
174: 				executor.ExecuteExpression(dummy_chunk, result);
175: 			}
176: 			added_column->Append(*added_col_stats->statistics, state, result, rows_in_this_vector);
177: 		}
178: 	}
179: 
180: 	// set up the row_group based on this row_group
181: 	auto row_group = make_unique<RowGroup>(db, table_info, this->start, this->count);
182: 	row_group->version_info = version_info;
183: 	row_group->columns = columns;
184: 	row_group->stats = stats;
185: 	// now add the new column
186: 	row_group->columns.push_back(move(added_column));
187: 	row_group->stats.push_back(move(added_col_stats));
188: 
189: 	row_group->Verify();
190: 	return row_group;
191: }
192: 
193: unique_ptr<RowGroup> RowGroup::RemoveColumn(idx_t removed_column) {
194: 	Verify();
195: 
196: 	D_ASSERT(removed_column < columns.size());
197: 
198: 	auto row_group = make_unique<RowGroup>(db, table_info, this->start, this->count);
199: 	row_group->version_info = version_info;
200: 	row_group->columns = columns;
201: 	row_group->stats = stats;
202: 	// now remove the column
203: 	row_group->columns.erase(row_group->columns.begin() + removed_column);
204: 	row_group->stats.erase(row_group->stats.begin() + removed_column);
205: 
206: 	row_group->Verify();
207: 	return row_group;
208: }
209: 
210: void RowGroup::CommitDrop() {
211: 	for (idx_t column_idx = 0; column_idx < columns.size(); column_idx++) {
212: 		CommitDropColumn(column_idx);
213: 	}
214: }
215: 
216: void RowGroup::CommitDropColumn(idx_t column_idx) {
217: 	D_ASSERT(column_idx < columns.size());
218: 	columns[column_idx]->CommitDropColumn();
219: }
220: 
221: void RowGroup::NextVector(RowGroupScanState &state) {
222: 	state.vector_index++;
223: 	for (idx_t i = 0; i < state.parent.column_ids.size(); i++) {
224: 		auto column = state.parent.column_ids[i];
225: 		if (column == COLUMN_IDENTIFIER_ROW_ID) {
226: 			continue;
227: 		}
228: 		D_ASSERT(column < columns.size());
229: 		columns[column]->Skip(state.column_scans[i]);
230: 	}
231: }
232: 
233: bool RowGroup::CheckZonemap(TableFilterSet &filters, const vector<column_t> &column_ids) {
234: 	for (auto &entry : filters.filters) {
235: 		auto column_index = entry.first;
236: 		auto &filter = entry.second;
237: 		auto base_column_index = column_ids[column_index];
238: 
239: 		auto propagate_result = filter->CheckStatistics(*stats[base_column_index]->statistics);
240: 		if (propagate_result == FilterPropagateResult::FILTER_ALWAYS_FALSE ||
241: 		    propagate_result == FilterPropagateResult::FILTER_FALSE_OR_NULL) {
242: 			return false;
243: 		}
244: 	}
245: 	return true;
246: }
247: 
248: bool RowGroup::CheckZonemapSegments(RowGroupScanState &state) {
249: 	if (!state.parent.table_filters) {
250: 		return true;
251: 	}
252: 	auto &column_ids = state.parent.column_ids;
253: 	for (auto &entry : state.parent.table_filters->filters) {
254: 		D_ASSERT(entry.first < column_ids.size());
255: 		auto column_idx = entry.first;
256: 		auto base_column_idx = column_ids[column_idx];
257: 		bool read_segment = columns[base_column_idx]->CheckZonemap(state.column_scans[column_idx], *entry.second);
258: 		if (!read_segment) {
259: 			idx_t target_row =
260: 			    state.column_scans[column_idx].current->start + state.column_scans[column_idx].current->count;
261: 			D_ASSERT(target_row >= this->start);
262: 			D_ASSERT(target_row <= this->start + this->count);
263: 			idx_t target_vector_index = (target_row - this->start) / STANDARD_VECTOR_SIZE;
264: 			if (state.vector_index == target_vector_index) {
265: 				// we can't skip any full vectors because this segment contains less than a full vector
266: 				// for now we just bail-out
267: 				// FIXME: we could check if we can ALSO skip the next segments, in which case skipping a full vector
268: 				// might be possible
269: 				// we don't care that much though, since a single segment that fits less than a full vector is
270: 				// exceedingly rare
271: 				return true;
272: 			}
273: 			while (state.vector_index < target_vector_index) {
274: 				NextVector(state);
275: 			}
276: 			return false;
277: 		}
278: 	}
279: 
280: 	return true;
281: }
282: 
283: template <TableScanType TYPE>
284: void RowGroup::TemplatedScan(Transaction *transaction, RowGroupScanState &state, DataChunk &result) {
285: 	const bool ALLOW_UPDATES = TYPE != TableScanType::TABLE_SCAN_COMMITTED_ROWS_DISALLOW_UPDATES &&
286: 	                           TYPE != TableScanType::TABLE_SCAN_COMMITTED_ROWS_OMIT_PERMANENTLY_DELETED;
287: 	auto &table_filters = state.parent.table_filters;
288: 	auto &column_ids = state.parent.column_ids;
289: 	auto &adaptive_filter = state.parent.adaptive_filter;
290: 	while (true) {
291: 		if (state.vector_index * STANDARD_VECTOR_SIZE >= state.max_row) {
292: 			// exceeded the amount of rows to scan
293: 			return;
294: 		}
295: 		idx_t current_row = state.vector_index * STANDARD_VECTOR_SIZE;
296: 		auto max_count = MinValue<idx_t>(STANDARD_VECTOR_SIZE, state.max_row - current_row);
297: 
298: 		//! first check the zonemap if we have to scan this partition
299: 		if (!CheckZonemapSegments(state)) {
300: 			continue;
301: 		}
302: 		// second, scan the version chunk manager to figure out which tuples to load for this transaction
303: 		idx_t count;
304: 		SelectionVector valid_sel(STANDARD_VECTOR_SIZE);
305: 		if (TYPE == TableScanType::TABLE_SCAN_REGULAR) {
306: 			D_ASSERT(transaction);
307: 			count = state.row_group->GetSelVector(*transaction, state.vector_index, valid_sel, max_count);
308: 			if (count == 0) {
309: 				// nothing to scan for this vector, skip the entire vector
310: 				NextVector(state);
311: 				continue;
312: 			}
313: 		} else if (TYPE == TableScanType::TABLE_SCAN_COMMITTED_ROWS_OMIT_PERMANENTLY_DELETED) {
314: 			auto &transaction_manager = TransactionManager::Get(db);
315: 			auto lowest_active_start = transaction_manager.LowestActiveStart();
316: 			auto lowest_active_id = transaction_manager.LowestActiveId();
317: 
318: 			count = state.row_group->GetCommittedSelVector(lowest_active_start, lowest_active_id, state.vector_index,
319: 			                                               valid_sel, max_count);
320: 			if (count == 0) {
321: 				// nothing to scan for this vector, skip the entire vector
322: 				NextVector(state);
323: 				continue;
324: 			}
325: 		} else {
326: 			count = max_count;
327: 		}
328: 		if (count == max_count && !table_filters) {
329: 			// scan all vectors completely: full scan without deletions or table filters
330: 			for (idx_t i = 0; i < column_ids.size(); i++) {
331: 				auto column = column_ids[i];
332: 				if (column == COLUMN_IDENTIFIER_ROW_ID) {
333: 					// scan row id
334: 					D_ASSERT(result.data[i].GetType().InternalType() == ROW_TYPE);
335: 					result.data[i].Sequence(this->start + current_row, 1);
336: 				} else {
337: 					if (TYPE != TableScanType::TABLE_SCAN_REGULAR) {
338: 						columns[column]->ScanCommitted(state.vector_index, state.column_scans[i], result.data[i],
339: 						                               ALLOW_UPDATES);
340: 					} else {
341: 						D_ASSERT(transaction);
342: 						columns[column]->Scan(*transaction, state.vector_index, state.column_scans[i], result.data[i]);
343: 					}
344: 				}
345: 			}
346: 		} else {
347: 			// partial scan: we have deletions or table filters
348: 			idx_t approved_tuple_count = count;
349: 			SelectionVector sel;
350: 			if (count != max_count) {
351: 				sel.Initialize(valid_sel);
352: 			} else {
353: 				sel.Initialize(FlatVector::INCREMENTAL_SELECTION_VECTOR);
354: 			}
355: 			//! first, we scan the columns with filters, fetch their data and generate a selection vector.
356: 			//! get runtime statistics
357: 			auto start_time = high_resolution_clock::now();
358: 			if (table_filters) {
359: 				D_ASSERT(ALLOW_UPDATES);
360: 				for (idx_t i = 0; i < table_filters->filters.size(); i++) {
361: 					auto tf_idx = adaptive_filter->permutation[i];
362: 					auto col_idx = column_ids[tf_idx];
363: 					columns[col_idx]->Select(*transaction, state.vector_index, state.column_scans[tf_idx],
364: 					                         result.data[tf_idx], sel, approved_tuple_count,
365: 					                         *table_filters->filters[tf_idx]);
366: 				}
367: 				for (auto &table_filter : table_filters->filters) {
368: 					result.data[table_filter.first].Slice(sel, approved_tuple_count);
369: 				}
370: 			}
371: 			if (approved_tuple_count == 0) {
372: 				// all rows were filtered out by the table filters
373: 				// skip this vector in all the scans that were not scanned yet
374: 				D_ASSERT(table_filters);
375: 				result.Reset();
376: 				for (idx_t i = 0; i < column_ids.size(); i++) {
377: 					auto col_idx = column_ids[i];
378: 					if (col_idx == COLUMN_IDENTIFIER_ROW_ID) {
379: 						continue;
380: 					}
381: 					if (table_filters->filters.find(i) == table_filters->filters.end()) {
382: 						columns[col_idx]->Skip(state.column_scans[i]);
383: 					}
384: 				}
385: 				state.vector_index++;
386: 				continue;
387: 			}
388: 			//! Now we use the selection vector to fetch data for the other columns.
389: 			for (idx_t i = 0; i < column_ids.size(); i++) {
390: 				if (!table_filters || table_filters->filters.find(i) == table_filters->filters.end()) {
391: 					auto column = column_ids[i];
392: 					if (column == COLUMN_IDENTIFIER_ROW_ID) {
393: 						D_ASSERT(result.data[i].GetType().InternalType() == PhysicalType::INT64);
394: 						result.data[i].SetVectorType(VectorType::FLAT_VECTOR);
395: 						auto result_data = (int64_t *)FlatVector::GetData(result.data[i]);
396: 						for (size_t sel_idx = 0; sel_idx < approved_tuple_count; sel_idx++) {
397: 							result_data[sel_idx] = this->start + current_row + sel.get_index(sel_idx);
398: 						}
399: 					} else {
400: 						if (TYPE == TableScanType::TABLE_SCAN_REGULAR) {
401: 							D_ASSERT(transaction);
402: 							columns[column]->FilterScan(*transaction, state.vector_index, state.column_scans[i],
403: 							                            result.data[i], sel, approved_tuple_count);
404: 						} else {
405: 							D_ASSERT(!transaction);
406: 							columns[column]->FilterScanCommitted(state.vector_index, state.column_scans[i],
407: 							                                     result.data[i], sel, approved_tuple_count,
408: 							                                     ALLOW_UPDATES);
409: 						}
410: 					}
411: 				}
412: 			}
413: 			auto end_time = high_resolution_clock::now();
414: 			if (adaptive_filter && table_filters->filters.size() > 1) {
415: 				adaptive_filter->AdaptRuntimeStatistics(duration_cast<duration<double>>(end_time - start_time).count());
416: 			}
417: 			D_ASSERT(approved_tuple_count > 0);
418: 			count = approved_tuple_count;
419: 		}
420: 		result.SetCardinality(count);
421: 		state.vector_index++;
422: 		break;
423: 	}
424: }
425: 
426: void RowGroup::Scan(Transaction &transaction, RowGroupScanState &state, DataChunk &result) {
427: 	TemplatedScan<TableScanType::TABLE_SCAN_REGULAR>(&transaction, state, result);
428: }
429: 
430: void RowGroup::ScanCommitted(RowGroupScanState &state, DataChunk &result, TableScanType type) {
431: 	switch (type) {
432: 	case TableScanType::TABLE_SCAN_COMMITTED_ROWS:
433: 		TemplatedScan<TableScanType::TABLE_SCAN_COMMITTED_ROWS>(nullptr, state, result);
434: 		break;
435: 	case TableScanType::TABLE_SCAN_COMMITTED_ROWS_DISALLOW_UPDATES:
436: 		TemplatedScan<TableScanType::TABLE_SCAN_COMMITTED_ROWS_DISALLOW_UPDATES>(nullptr, state, result);
437: 		break;
438: 	case TableScanType::TABLE_SCAN_COMMITTED_ROWS_OMIT_PERMANENTLY_DELETED:
439: 		TemplatedScan<TableScanType::TABLE_SCAN_COMMITTED_ROWS_OMIT_PERMANENTLY_DELETED>(nullptr, state, result);
440: 		break;
441: 	default:
442: 		throw InternalException("Unrecognized table scan type");
443: 	}
444: }
445: 
446: ChunkInfo *RowGroup::GetChunkInfo(idx_t vector_idx) {
447: 	if (!version_info) {
448: 		return nullptr;
449: 	}
450: 	return version_info->info[vector_idx].get();
451: }
452: 
453: idx_t RowGroup::GetSelVector(Transaction &transaction, idx_t vector_idx, SelectionVector &sel_vector, idx_t max_count) {
454: 	lock_guard<mutex> lock(row_group_lock);
455: 
456: 	auto info = GetChunkInfo(vector_idx);
457: 	if (!info) {
458: 		return max_count;
459: 	}
460: 	return info->GetSelVector(transaction, sel_vector, max_count);
461: }
462: 
463: idx_t RowGroup::GetCommittedSelVector(transaction_t start_time, transaction_t transaction_id, idx_t vector_idx,
464:                                       SelectionVector &sel_vector, idx_t max_count) {
465: 	lock_guard<mutex> lock(row_group_lock);
466: 
467: 	auto info = GetChunkInfo(vector_idx);
468: 	if (!info) {
469: 		return max_count;
470: 	}
471: 	return info->GetCommittedSelVector(start_time, transaction_id, sel_vector, max_count);
472: }
473: 
474: bool RowGroup::Fetch(Transaction &transaction, idx_t row) {
475: 	D_ASSERT(row < this->count);
476: 	lock_guard<mutex> lock(row_group_lock);
477: 
478: 	idx_t vector_index = row / STANDARD_VECTOR_SIZE;
479: 	auto info = GetChunkInfo(vector_index);
480: 	if (!info) {
481: 		return true;
482: 	}
483: 	return info->Fetch(transaction, row - vector_index * STANDARD_VECTOR_SIZE);
484: }
485: 
486: void RowGroup::FetchRow(Transaction &transaction, ColumnFetchState &state, const vector<column_t> &column_ids,
487:                         row_t row_id, DataChunk &result, idx_t result_idx) {
488: 	for (idx_t col_idx = 0; col_idx < column_ids.size(); col_idx++) {
489: 		auto column = column_ids[col_idx];
490: 		if (column == COLUMN_IDENTIFIER_ROW_ID) {
491: 			// row id column: fill in the row ids
492: 			D_ASSERT(result.data[col_idx].GetType().InternalType() == PhysicalType::INT64);
493: 			result.data[col_idx].SetVectorType(VectorType::FLAT_VECTOR);
494: 			auto data = FlatVector::GetData<row_t>(result.data[col_idx]);
495: 			data[result_idx] = row_id;
496: 		} else {
497: 			// regular column: fetch data from the base column
498: 			columns[column]->FetchRow(transaction, state, row_id, result.data[col_idx], result_idx);
499: 		}
500: 	}
501: }
502: 
503: void RowGroup::AppendVersionInfo(Transaction &transaction, idx_t row_group_start, idx_t count,
504:                                  transaction_t commit_id) {
505: 	idx_t row_group_end = row_group_start + count;
506: 	lock_guard<mutex> lock(row_group_lock);
507: 
508: 	this->count += count;
509: 	D_ASSERT(this->count <= RowGroup::ROW_GROUP_SIZE);
510: 
511: 	// create the version_info if it doesn't exist yet
512: 	if (!version_info) {
513: 		version_info = make_unique<VersionNode>();
514: 	}
515: 	idx_t start_vector_idx = row_group_start / STANDARD_VECTOR_SIZE;
516: 	idx_t end_vector_idx = (row_group_end - 1) / STANDARD_VECTOR_SIZE;
517: 	for (idx_t vector_idx = start_vector_idx; vector_idx <= end_vector_idx; vector_idx++) {
518: 		idx_t start = vector_idx == start_vector_idx ? row_group_start - start_vector_idx * STANDARD_VECTOR_SIZE : 0;
519: 		idx_t end =
520: 		    vector_idx == end_vector_idx ? row_group_end - end_vector_idx * STANDARD_VECTOR_SIZE : STANDARD_VECTOR_SIZE;
521: 		if (start == 0 && end == STANDARD_VECTOR_SIZE) {
522: 			// entire vector is encapsulated by append: append a single constant
523: 			auto constant_info = make_unique<ChunkConstantInfo>(this->start + vector_idx * STANDARD_VECTOR_SIZE);
524: 			constant_info->insert_id = commit_id;
525: 			constant_info->delete_id = NOT_DELETED_ID;
526: 			version_info->info[vector_idx] = move(constant_info);
527: 		} else {
528: 			// part of a vector is encapsulated: append to that part
529: 			ChunkVectorInfo *info;
530: 			if (!version_info->info[vector_idx]) {
531: 				// first time appending to this vector: create new info
532: 				auto insert_info = make_unique<ChunkVectorInfo>(this->start + vector_idx * STANDARD_VECTOR_SIZE);
533: 				info = insert_info.get();
534: 				version_info->info[vector_idx] = move(insert_info);
535: 			} else {
536: 				D_ASSERT(version_info->info[vector_idx]->type == ChunkInfoType::VECTOR_INFO);
537: 				// use existing vector
538: 				info = (ChunkVectorInfo *)version_info->info[vector_idx].get();
539: 			}
540: 			info->Append(start, end, commit_id);
541: 		}
542: 	}
543: }
544: 
545: void RowGroup::CommitAppend(transaction_t commit_id, idx_t row_group_start, idx_t count) {
546: 	D_ASSERT(version_info.get());
547: 	idx_t row_group_end = row_group_start + count;
548: 	lock_guard<mutex> lock(row_group_lock);
549: 
550: 	idx_t start_vector_idx = row_group_start / STANDARD_VECTOR_SIZE;
551: 	idx_t end_vector_idx = (row_group_end - 1) / STANDARD_VECTOR_SIZE;
552: 	for (idx_t vector_idx = start_vector_idx; vector_idx <= end_vector_idx; vector_idx++) {
553: 		idx_t start = vector_idx == start_vector_idx ? row_group_start - start_vector_idx * STANDARD_VECTOR_SIZE : 0;
554: 		idx_t end =
555: 		    vector_idx == end_vector_idx ? row_group_end - end_vector_idx * STANDARD_VECTOR_SIZE : STANDARD_VECTOR_SIZE;
556: 
557: 		auto info = version_info->info[vector_idx].get();
558: 		info->CommitAppend(commit_id, start, end);
559: 	}
560: }
561: 
562: void RowGroup::RevertAppend(idx_t row_group_start) {
563: 	if (!version_info) {
564: 		return;
565: 	}
566: 	idx_t start_row = row_group_start - this->start;
567: 	idx_t start_vector_idx = (start_row + (STANDARD_VECTOR_SIZE - 1)) / STANDARD_VECTOR_SIZE;
568: 	for (idx_t vector_idx = start_vector_idx; vector_idx < RowGroup::ROW_GROUP_VECTOR_COUNT; vector_idx++) {
569: 		version_info->info[vector_idx].reset();
570: 	}
571: 	for (auto &column : columns) {
572: 		column->RevertAppend(row_group_start);
573: 	}
574: 	this->count = MinValue<idx_t>(row_group_start - this->start, this->count);
575: 	Verify();
576: }
577: 
578: void RowGroup::InitializeAppend(Transaction &transaction, RowGroupAppendState &append_state,
579:                                 idx_t remaining_append_count) {
580: 	append_state.row_group = this;
581: 	append_state.offset_in_row_group = this->count;
582: 	// for each column, initialize the append state
583: 	append_state.states = unique_ptr<ColumnAppendState[]>(new ColumnAppendState[columns.size()]);
584: 	for (idx_t i = 0; i < columns.size(); i++) {
585: 		columns[i]->InitializeAppend(append_state.states[i]);
586: 	}
587: 	// append the version info for this row_group
588: 	idx_t append_count = MinValue<idx_t>(remaining_append_count, RowGroup::ROW_GROUP_SIZE - this->count);
589: 	AppendVersionInfo(transaction, this->count, append_count, transaction.transaction_id);
590: }
591: 
592: void RowGroup::Append(RowGroupAppendState &state, DataChunk &chunk, idx_t append_count) {
593: 	// append to the current row_group
594: 	for (idx_t i = 0; i < columns.size(); i++) {
595: 		columns[i]->Append(*stats[i]->statistics, state.states[i], chunk.data[i], append_count);
596: 	}
597: 	state.offset_in_row_group += append_count;
598: }
599: 
600: void RowGroup::Update(Transaction &transaction, DataChunk &update_chunk, row_t *ids, idx_t offset, idx_t count,
601:                       const vector<column_t> &column_ids) {
602: #ifdef DEBUG
603: 	for (size_t i = offset; i < offset + count; i++) {
604: 		D_ASSERT(ids[i] >= row_t(this->start) && ids[i] < row_t(this->start + this->count));
605: 	}
606: #endif
607: 	for (idx_t i = 0; i < column_ids.size(); i++) {
608: 		auto column = column_ids[i];
609: 		D_ASSERT(column != COLUMN_IDENTIFIER_ROW_ID);
610: 		D_ASSERT(columns[column]->type.id() == update_chunk.data[i].GetType().id());
611: 		columns[column]->Update(transaction, column, update_chunk.data[i], ids, offset, count);
612: 		MergeStatistics(column, *columns[column]->GetUpdateStatistics());
613: 	}
614: }
615: 
616: void RowGroup::UpdateColumn(Transaction &transaction, DataChunk &updates, Vector &row_ids,
617:                             const vector<column_t> &column_path) {
618: 	D_ASSERT(updates.ColumnCount() == 1);
619: 	auto ids = FlatVector::GetData<row_t>(row_ids);
620: 
621: 	auto primary_column_idx = column_path[0];
622: 	D_ASSERT(primary_column_idx != COLUMN_IDENTIFIER_ROW_ID);
623: 	D_ASSERT(primary_column_idx < columns.size());
624: 	columns[primary_column_idx]->UpdateColumn(transaction, column_path, updates.data[0], ids, updates.size(), 1);
625: 	MergeStatistics(primary_column_idx, *columns[primary_column_idx]->GetUpdateStatistics());
626: }
627: 
628: unique_ptr<BaseStatistics> RowGroup::GetStatistics(idx_t column_idx) {
629: 	D_ASSERT(column_idx < stats.size());
630: 
631: 	lock_guard<mutex> slock(stats_lock);
632: 	return stats[column_idx]->statistics->Copy();
633: }
634: 
635: void RowGroup::MergeStatistics(idx_t column_idx, BaseStatistics &other) {
636: 	D_ASSERT(column_idx < stats.size());
637: 
638: 	lock_guard<mutex> slock(stats_lock);
639: 	stats[column_idx]->statistics->Merge(other);
640: }
641: 
642: RowGroupPointer RowGroup::Checkpoint(TableDataWriter &writer, vector<unique_ptr<BaseStatistics>> &global_stats) {
643: 	vector<unique_ptr<ColumnCheckpointState>> states;
644: 	states.reserve(columns.size());
645: 
646: 	// checkpoint the individual columns of the row group
647: 	for (idx_t column_idx = 0; column_idx < columns.size(); column_idx++) {
648: 		auto &column = columns[column_idx];
649: 		ColumnCheckpointInfo checkpoint_info {writer.GetColumnCompressionType(column_idx)};
650: 		auto checkpoint_state = column->Checkpoint(*this, writer, checkpoint_info);
651: 		D_ASSERT(checkpoint_state);
652: 
653: 		auto stats = checkpoint_state->GetStatistics();
654: 		D_ASSERT(stats);
655: 
656: 		global_stats[column_idx]->Merge(*stats);
657: 		states.push_back(move(checkpoint_state));
658: 	}
659: 
660: 	// construct the row group pointer and write the column meta data to disk
661: 	D_ASSERT(states.size() == columns.size());
662: 	RowGroupPointer row_group_pointer;
663: 	row_group_pointer.row_start = start;
664: 	row_group_pointer.tuple_count = count;
665: 	for (auto &state : states) {
666: 		// get the current position of the meta data writer
667: 		auto &meta_writer = writer.GetMetaWriter();
668: 		auto pointer = meta_writer.GetBlockPointer();
669: 
670: 		// store the stats and the data pointers in the row group pointers
671: 		row_group_pointer.data_pointers.push_back(pointer);
672: 		row_group_pointer.statistics.push_back(state->GetStatistics());
673: 
674: 		// now flush the actual column data to disk
675: 		state->FlushToDisk();
676: 	}
677: 	row_group_pointer.versions = version_info;
678: 	Verify();
679: 	return row_group_pointer;
680: }
681: 
682: void RowGroup::CheckpointDeletes(VersionNode *versions, Serializer &serializer) {
683: 	if (!versions) {
684: 		// no version information: write nothing
685: 		serializer.Write<idx_t>(0);
686: 		return;
687: 	}
688: 	// first count how many ChunkInfo's we need to deserialize
689: 	idx_t chunk_info_count = 0;
690: 	for (idx_t vector_idx = 0; vector_idx < RowGroup::ROW_GROUP_VECTOR_COUNT; vector_idx++) {
691: 		auto chunk_info = versions->info[vector_idx].get();
692: 		if (!chunk_info) {
693: 			continue;
694: 		}
695: 		chunk_info_count++;
696: 	}
697: 	// now serialize the actual version information
698: 	serializer.Write<idx_t>(chunk_info_count);
699: 	for (idx_t vector_idx = 0; vector_idx < RowGroup::ROW_GROUP_VECTOR_COUNT; vector_idx++) {
700: 		auto chunk_info = versions->info[vector_idx].get();
701: 		if (!chunk_info) {
702: 			continue;
703: 		}
704: 		serializer.Write<idx_t>(vector_idx);
705: 		chunk_info->Serialize(serializer);
706: 	}
707: }
708: 
709: shared_ptr<VersionNode> RowGroup::DeserializeDeletes(Deserializer &source) {
710: 	auto chunk_count = source.Read<idx_t>();
711: 	if (chunk_count == 0) {
712: 		// no deletes
713: 		return nullptr;
714: 	}
715: 	auto version_info = make_shared<VersionNode>();
716: 	for (idx_t i = 0; i < chunk_count; i++) {
717: 		idx_t vector_index = source.Read<idx_t>();
718: 		if (vector_index >= RowGroup::ROW_GROUP_VECTOR_COUNT) {
719: 			throw Exception("In DeserializeDeletes, vector_index is out of range for the row group. Corrupted file?");
720: 		}
721: 		version_info->info[vector_index] = ChunkInfo::Deserialize(source);
722: 	}
723: 	return version_info;
724: }
725: 
726: void RowGroup::Serialize(RowGroupPointer &pointer, Serializer &serializer) {
727: 	serializer.Write<uint64_t>(pointer.row_start);
728: 	serializer.Write<uint64_t>(pointer.tuple_count);
729: 	for (auto &stats : pointer.statistics) {
730: 		stats->Serialize(serializer);
731: 	}
732: 	for (auto &data_pointer : pointer.data_pointers) {
733: 		serializer.Write<block_id_t>(data_pointer.block_id);
734: 		serializer.Write<uint64_t>(data_pointer.offset);
735: 	}
736: 	CheckpointDeletes(pointer.versions.get(), serializer);
737: }
738: 
739: RowGroupPointer RowGroup::Deserialize(Deserializer &source, const vector<ColumnDefinition> &columns) {
740: 	RowGroupPointer result;
741: 	result.row_start = source.Read<uint64_t>();
742: 	result.tuple_count = source.Read<uint64_t>();
743: 
744: 	result.data_pointers.reserve(columns.size());
745: 	result.statistics.reserve(columns.size());
746: 
747: 	for (idx_t i = 0; i < columns.size(); i++) {
748: 		auto stats = BaseStatistics::Deserialize(source, columns[i].type);
749: 		result.statistics.push_back(move(stats));
750: 	}
751: 	for (idx_t i = 0; i < columns.size(); i++) {
752: 		BlockPointer pointer;
753: 		pointer.block_id = source.Read<block_id_t>();
754: 		pointer.offset = source.Read<uint64_t>();
755: 		result.data_pointers.push_back(pointer);
756: 	}
757: 	result.versions = DeserializeDeletes(source);
758: 	return result;
759: }
760: 
761: //===--------------------------------------------------------------------===//
762: // GetStorageInfo
763: //===--------------------------------------------------------------------===//
764: void RowGroup::GetStorageInfo(idx_t row_group_index, vector<vector<Value>> &result) {
765: 	for (idx_t col_idx = 0; col_idx < columns.size(); col_idx++) {
766: 		columns[col_idx]->GetStorageInfo(row_group_index, {col_idx}, result);
767: 	}
768: }
769: 
770: //===--------------------------------------------------------------------===//
771: // Version Delete Information
772: //===--------------------------------------------------------------------===//
773: class VersionDeleteState {
774: public:
775: 	VersionDeleteState(RowGroup &info, Transaction &transaction, DataTable *table, idx_t base_row)
776: 	    : info(info), transaction(transaction), table(table), current_info(nullptr),
777: 	      current_chunk(DConstants::INVALID_INDEX), count(0), base_row(base_row), delete_count(0) {
778: 	}
779: 
780: 	RowGroup &info;
781: 	Transaction &transaction;
782: 	DataTable *table;
783: 	ChunkVectorInfo *current_info;
784: 	idx_t current_chunk;
785: 	row_t rows[STANDARD_VECTOR_SIZE];
786: 	idx_t count;
787: 	idx_t base_row;
788: 	idx_t chunk_row;
789: 	idx_t delete_count;
790: 
791: public:
792: 	void Delete(row_t row_id);
793: 	void Flush();
794: };
795: 
796: idx_t RowGroup::Delete(Transaction &transaction, DataTable *table, row_t *ids, idx_t count) {
797: 	lock_guard<mutex> lock(row_group_lock);
798: 	VersionDeleteState del_state(*this, transaction, table, this->start);
799: 
800: 	// obtain a write lock
801: 	for (idx_t i = 0; i < count; i++) {
802: 		D_ASSERT(ids[i] >= 0);
803: 		D_ASSERT(idx_t(ids[i]) >= this->start && idx_t(ids[i]) < this->start + this->count);
804: 		del_state.Delete(ids[i] - this->start);
805: 	}
806: 	del_state.Flush();
807: 	return del_state.delete_count;
808: }
809: 
810: void RowGroup::Verify() {
811: #ifdef DEBUG
812: 	for (auto &column : columns) {
813: 		column->Verify(*this);
814: 	}
815: #endif
816: }
817: 
818: void VersionDeleteState::Delete(row_t row_id) {
819: 	D_ASSERT(row_id >= 0);
820: 	idx_t vector_idx = row_id / STANDARD_VECTOR_SIZE;
821: 	idx_t idx_in_vector = row_id - vector_idx * STANDARD_VECTOR_SIZE;
822: 	if (current_chunk != vector_idx) {
823: 		Flush();
824: 
825: 		if (!info.version_info) {
826: 			info.version_info = make_unique<VersionNode>();
827: 		}
828: 
829: 		if (!info.version_info->info[vector_idx]) {
830: 			// no info yet: create it
831: 			info.version_info->info[vector_idx] =
832: 			    make_unique<ChunkVectorInfo>(info.start + vector_idx * STANDARD_VECTOR_SIZE);
833: 		} else if (info.version_info->info[vector_idx]->type == ChunkInfoType::CONSTANT_INFO) {
834: 			auto &constant = (ChunkConstantInfo &)*info.version_info->info[vector_idx];
835: 			// info exists but it's a constant info: convert to a vector info
836: 			auto new_info = make_unique<ChunkVectorInfo>(info.start + vector_idx * STANDARD_VECTOR_SIZE);
837: 			new_info->insert_id = constant.insert_id.load();
838: 			for (idx_t i = 0; i < STANDARD_VECTOR_SIZE; i++) {
839: 				new_info->inserted[i] = constant.insert_id.load();
840: 			}
841: 			info.version_info->info[vector_idx] = move(new_info);
842: 		}
843: 		D_ASSERT(info.version_info->info[vector_idx]->type == ChunkInfoType::VECTOR_INFO);
844: 		current_info = (ChunkVectorInfo *)info.version_info->info[vector_idx].get();
845: 		current_chunk = vector_idx;
846: 		chunk_row = vector_idx * STANDARD_VECTOR_SIZE;
847: 	}
848: 	rows[count++] = idx_in_vector;
849: }
850: 
851: void VersionDeleteState::Flush() {
852: 	if (count == 0) {
853: 		return;
854: 	}
855: 	// delete in the current info
856: 	delete_count += current_info->Delete(transaction, rows, count);
857: 	// now push the delete into the undo buffer
858: 	transaction.PushDelete(table, current_info, rows, count, base_row + chunk_row);
859: 	count = 0;
860: }
861: 
862: } // namespace duckdb
[end of src/storage/table/row_group.cpp]
[start of src/storage/table/standard_column_data.cpp]
1: #include "duckdb/storage/table/standard_column_data.hpp"
2: #include "duckdb/storage/table/scan_state.hpp"
3: #include "duckdb/storage/table/update_segment.hpp"
4: #include "duckdb/storage/table/append_state.hpp"
5: #include "duckdb/storage/data_table.hpp"
6: #include "duckdb/planner/table_filter.hpp"
7: 
8: namespace duckdb {
9: 
10: StandardColumnData::StandardColumnData(DataTableInfo &info, idx_t column_index, idx_t start_row, LogicalType type,
11:                                        ColumnData *parent)
12:     : ColumnData(info, column_index, start_row, move(type), parent), validity(info, 0, start_row, this) {
13: }
14: 
15: bool StandardColumnData::CheckZonemap(ColumnScanState &state, TableFilter &filter) {
16: 	if (!state.segment_checked) {
17: 		if (!state.current) {
18: 			return true;
19: 		}
20: 		state.segment_checked = true;
21: 		auto prune_result = filter.CheckStatistics(*state.current->stats.statistics);
22: 		if (prune_result != FilterPropagateResult::FILTER_ALWAYS_FALSE) {
23: 			return true;
24: 		}
25: 		if (updates) {
26: 			auto update_stats = updates->GetStatistics();
27: 			prune_result = filter.CheckStatistics(*update_stats);
28: 			return prune_result != FilterPropagateResult::FILTER_ALWAYS_FALSE;
29: 		} else {
30: 			return false;
31: 		}
32: 	} else {
33: 		return true;
34: 	}
35: }
36: 
37: void StandardColumnData::InitializeScan(ColumnScanState &state) {
38: 	ColumnData::InitializeScan(state);
39: 
40: 	// initialize the validity segment
41: 	ColumnScanState child_state;
42: 	validity.InitializeScan(child_state);
43: 	state.child_states.push_back(move(child_state));
44: }
45: 
46: void StandardColumnData::InitializeScanWithOffset(ColumnScanState &state, idx_t row_idx) {
47: 	ColumnData::InitializeScanWithOffset(state, row_idx);
48: 
49: 	// initialize the validity segment
50: 	ColumnScanState child_state;
51: 	validity.InitializeScanWithOffset(child_state, row_idx);
52: 	state.child_states.push_back(move(child_state));
53: }
54: 
55: idx_t StandardColumnData::Scan(Transaction &transaction, idx_t vector_index, ColumnScanState &state, Vector &result) {
56: 	D_ASSERT(state.row_index == state.child_states[0].row_index);
57: 	auto scan_count = ColumnData::Scan(transaction, vector_index, state, result);
58: 	validity.Scan(transaction, vector_index, state.child_states[0], result);
59: 	return scan_count;
60: }
61: 
62: idx_t StandardColumnData::ScanCommitted(idx_t vector_index, ColumnScanState &state, Vector &result,
63:                                         bool allow_updates) {
64: 	D_ASSERT(state.row_index == state.child_states[0].row_index);
65: 	auto scan_count = ColumnData::ScanCommitted(vector_index, state, result, allow_updates);
66: 	validity.ScanCommitted(vector_index, state.child_states[0], result, allow_updates);
67: 	return scan_count;
68: }
69: 
70: idx_t StandardColumnData::ScanCount(ColumnScanState &state, Vector &result, idx_t count) {
71: 	auto scan_count = ColumnData::ScanCount(state, result, count);
72: 	validity.ScanCount(state.child_states[0], result, count);
73: 	return scan_count;
74: }
75: 
76: void StandardColumnData::InitializeAppend(ColumnAppendState &state) {
77: 	ColumnData::InitializeAppend(state);
78: 
79: 	ColumnAppendState child_append;
80: 	validity.InitializeAppend(child_append);
81: 	state.child_appends.push_back(move(child_append));
82: }
83: 
84: void StandardColumnData::AppendData(BaseStatistics &stats, ColumnAppendState &state, VectorData &vdata, idx_t count) {
85: 	ColumnData::AppendData(stats, state, vdata, count);
86: 
87: 	validity.AppendData(*stats.validity_stats, state.child_appends[0], vdata, count);
88: }
89: 
90: void StandardColumnData::RevertAppend(row_t start_row) {
91: 	ColumnData::RevertAppend(start_row);
92: 
93: 	validity.RevertAppend(start_row);
94: }
95: 
96: idx_t StandardColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &result) {
97: 	// fetch validity mask
98: 	if (state.child_states.empty()) {
99: 		ColumnScanState child_state;
100: 		state.child_states.push_back(move(child_state));
101: 	}
102: 	auto scan_count = ColumnData::Fetch(state, row_id, result);
103: 	validity.Fetch(state.child_states[0], row_id, result);
104: 	return scan_count;
105: }
106: 
107: void StandardColumnData::Update(Transaction &transaction, idx_t column_index, Vector &update_vector, row_t *row_ids,
108:                                 idx_t offset, idx_t update_count) {
109: 	ColumnData::Update(transaction, column_index, update_vector, row_ids, offset, update_count);
110: 	validity.Update(transaction, column_index, update_vector, row_ids, offset, update_count);
111: }
112: 
113: void StandardColumnData::UpdateColumn(Transaction &transaction, const vector<column_t> &column_path,
114:                                       Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t depth) {
115: 	if (depth >= column_path.size()) {
116: 		// update this column
117: 		ColumnData::Update(transaction, column_path[0], update_vector, row_ids, 0, update_count);
118: 	} else {
119: 		// update the child column (i.e. the validity column)
120: 		validity.UpdateColumn(transaction, column_path, update_vector, row_ids, update_count, depth + 1);
121: 	}
122: }
123: 
124: unique_ptr<BaseStatistics> StandardColumnData::GetUpdateStatistics() {
125: 	auto stats = updates ? updates->GetStatistics() : nullptr;
126: 	auto validity_stats = validity.GetUpdateStatistics();
127: 	if (!stats && !validity_stats) {
128: 		return nullptr;
129: 	}
130: 	if (!stats) {
131: 		stats = BaseStatistics::CreateEmpty(type);
132: 	}
133: 	stats->validity_stats = move(validity_stats);
134: 	return stats;
135: }
136: 
137: void StandardColumnData::FetchRow(Transaction &transaction, ColumnFetchState &state, row_t row_id, Vector &result,
138:                                   idx_t result_idx) {
139: 	// find the segment the row belongs to
140: 	if (state.child_states.empty()) {
141: 		auto child_state = make_unique<ColumnFetchState>();
142: 		state.child_states.push_back(move(child_state));
143: 	}
144: 	validity.FetchRow(transaction, *state.child_states[0], row_id, result, result_idx);
145: 	ColumnData::FetchRow(transaction, state, row_id, result, result_idx);
146: }
147: 
148: void StandardColumnData::CommitDropColumn() {
149: 	ColumnData::CommitDropColumn();
150: 	validity.CommitDropColumn();
151: }
152: 
153: struct StandardColumnCheckpointState : public ColumnCheckpointState {
154: 	StandardColumnCheckpointState(RowGroup &row_group, ColumnData &column_data, TableDataWriter &writer)
155: 	    : ColumnCheckpointState(row_group, column_data, writer) {
156: 	}
157: 
158: 	unique_ptr<ColumnCheckpointState> validity_state;
159: 
160: public:
161: 	unique_ptr<BaseStatistics> GetStatistics() override {
162: 		auto stats = global_stats->Copy();
163: 		stats->validity_stats = validity_state->GetStatistics();
164: 		return stats;
165: 	}
166: 
167: 	void FlushToDisk() override {
168: 		ColumnCheckpointState::FlushToDisk();
169: 		validity_state->FlushToDisk();
170: 	}
171: };
172: 
173: unique_ptr<ColumnCheckpointState> StandardColumnData::CreateCheckpointState(RowGroup &row_group,
174:                                                                             TableDataWriter &writer) {
175: 	return make_unique<StandardColumnCheckpointState>(row_group, *this, writer);
176: }
177: 
178: unique_ptr<ColumnCheckpointState> StandardColumnData::Checkpoint(RowGroup &row_group, TableDataWriter &writer,
179:                                                                  ColumnCheckpointInfo &checkpoint_info) {
180: 	auto validity_state = validity.Checkpoint(row_group, writer, checkpoint_info);
181: 	auto base_state = ColumnData::Checkpoint(row_group, writer, checkpoint_info);
182: 	auto &checkpoint_state = (StandardColumnCheckpointState &)*base_state;
183: 	checkpoint_state.validity_state = move(validity_state);
184: 	return base_state;
185: }
186: 
187: void StandardColumnData::CheckpointScan(ColumnSegment *segment, ColumnScanState &state, idx_t row_group_start,
188:                                         idx_t count, Vector &scan_vector) {
189: 	ColumnData::CheckpointScan(segment, state, row_group_start, count, scan_vector);
190: 
191: 	idx_t offset_in_row_group = state.row_index - row_group_start;
192: 	validity.ScanCommittedRange(row_group_start, offset_in_row_group, count, scan_vector);
193: }
194: 
195: void StandardColumnData::DeserializeColumn(Deserializer &source) {
196: 	ColumnData::DeserializeColumn(source);
197: 	validity.DeserializeColumn(source);
198: }
199: 
200: void StandardColumnData::GetStorageInfo(idx_t row_group_index, vector<idx_t> col_path, vector<vector<Value>> &result) {
201: 	ColumnData::GetStorageInfo(row_group_index, col_path, result);
202: 	col_path.push_back(0);
203: 	validity.GetStorageInfo(row_group_index, move(col_path), result);
204: }
205: 
206: void StandardColumnData::Verify(RowGroup &parent) {
207: #ifdef DEBUG
208: 	ColumnData::Verify(parent);
209: 	validity.Verify(parent);
210: #endif
211: }
212: 
213: } // namespace duckdb
[end of src/storage/table/standard_column_data.cpp]
[start of src/storage/table/struct_column_data.cpp]
1: #include "duckdb/storage/table/struct_column_data.hpp"
2: #include "duckdb/storage/statistics/struct_statistics.hpp"
3: 
4: namespace duckdb {
5: 
6: StructColumnData::StructColumnData(DataTableInfo &info, idx_t column_index, idx_t start_row, LogicalType type_p,
7:                                    ColumnData *parent)
8:     : ColumnData(info, column_index, start_row, move(type_p), parent), validity(info, 0, start_row, this) {
9: 	D_ASSERT(type.InternalType() == PhysicalType::STRUCT);
10: 	auto &child_types = StructType::GetChildTypes(type);
11: 	D_ASSERT(child_types.size() > 0);
12: 	// the sub column index, starting at 1 (0 is the validity mask)
13: 	idx_t sub_column_index = 1;
14: 	for (auto &child_type : child_types) {
15: 		sub_columns.push_back(
16: 		    ColumnData::CreateColumnUnique(info, sub_column_index, start_row, child_type.second, this));
17: 		sub_column_index++;
18: 	}
19: }
20: 
21: bool StructColumnData::CheckZonemap(ColumnScanState &state, TableFilter &filter) {
22: 	// table filters are not supported yet for struct columns
23: 	return false;
24: }
25: 
26: idx_t StructColumnData::GetMaxEntry() {
27: 	return sub_columns[0]->GetMaxEntry();
28: }
29: 
30: void StructColumnData::InitializeScan(ColumnScanState &state) {
31: 	D_ASSERT(state.child_states.empty());
32: 
33: 	state.row_index = 0;
34: 	state.current = nullptr;
35: 
36: 	// initialize the validity segment
37: 	ColumnScanState validity_state;
38: 	validity.InitializeScan(validity_state);
39: 	state.child_states.push_back(move(validity_state));
40: 
41: 	// initialize the sub-columns
42: 	for (auto &sub_column : sub_columns) {
43: 		ColumnScanState child_state;
44: 		sub_column->InitializeScan(child_state);
45: 		state.child_states.push_back(move(child_state));
46: 	}
47: }
48: 
49: void StructColumnData::InitializeScanWithOffset(ColumnScanState &state, idx_t row_idx) {
50: 	D_ASSERT(state.child_states.empty());
51: 
52: 	state.row_index = row_idx;
53: 	state.current = nullptr;
54: 
55: 	// initialize the validity segment
56: 	ColumnScanState validity_state;
57: 	validity.InitializeScanWithOffset(validity_state, row_idx);
58: 	state.child_states.push_back(move(validity_state));
59: 
60: 	// initialize the sub-columns
61: 	for (auto &sub_column : sub_columns) {
62: 		ColumnScanState child_state;
63: 		sub_column->InitializeScanWithOffset(child_state, row_idx);
64: 		state.child_states.push_back(move(child_state));
65: 	}
66: }
67: 
68: idx_t StructColumnData::Scan(Transaction &transaction, idx_t vector_index, ColumnScanState &state, Vector &result) {
69: 	auto scan_count = validity.Scan(transaction, vector_index, state.child_states[0], result);
70: 	auto &child_entries = StructVector::GetEntries(result);
71: 	for (idx_t i = 0; i < sub_columns.size(); i++) {
72: 		sub_columns[i]->Scan(transaction, vector_index, state.child_states[i + 1], *child_entries[i]);
73: 	}
74: 	return scan_count;
75: }
76: 
77: idx_t StructColumnData::ScanCommitted(idx_t vector_index, ColumnScanState &state, Vector &result, bool allow_updates) {
78: 	auto scan_count = validity.ScanCommitted(vector_index, state.child_states[0], result, allow_updates);
79: 	auto &child_entries = StructVector::GetEntries(result);
80: 	for (idx_t i = 0; i < sub_columns.size(); i++) {
81: 		sub_columns[i]->ScanCommitted(vector_index, state.child_states[i + 1], *child_entries[i], allow_updates);
82: 	}
83: 	return scan_count;
84: }
85: 
86: idx_t StructColumnData::ScanCount(ColumnScanState &state, Vector &result, idx_t count) {
87: 	auto scan_count = validity.ScanCount(state.child_states[0], result, count);
88: 	auto &child_entries = StructVector::GetEntries(result);
89: 	for (idx_t i = 0; i < sub_columns.size(); i++) {
90: 		sub_columns[i]->ScanCount(state.child_states[i + 1], *child_entries[i], count);
91: 	}
92: 	return scan_count;
93: }
94: 
95: void StructColumnData::InitializeAppend(ColumnAppendState &state) {
96: 	ColumnAppendState validity_append;
97: 	validity.InitializeAppend(validity_append);
98: 	state.child_appends.push_back(move(validity_append));
99: 
100: 	for (auto &sub_column : sub_columns) {
101: 		ColumnAppendState child_append;
102: 		sub_column->InitializeAppend(child_append);
103: 		state.child_appends.push_back(move(child_append));
104: 	}
105: }
106: 
107: void StructColumnData::Append(BaseStatistics &stats, ColumnAppendState &state, Vector &vector, idx_t count) {
108: 	vector.Normalify(count);
109: 
110: 	// append the null values
111: 	validity.Append(*stats.validity_stats, state.child_appends[0], vector, count);
112: 
113: 	auto &struct_validity = FlatVector::Validity(vector);
114: 
115: 	auto &struct_stats = (StructStatistics &)stats;
116: 	auto &child_entries = StructVector::GetEntries(vector);
117: 	for (idx_t i = 0; i < child_entries.size(); i++) {
118: 		if (!struct_validity.AllValid()) {
119: 			// we set the child entries of the struct to NULL
120: 			// for any values in which the struct itself is NULL
121: 			child_entries[i]->Normalify(count);
122: 
123: 			auto &child_validity = FlatVector::Validity(*child_entries[i]);
124: 			child_validity.Combine(struct_validity, count);
125: 		}
126: 		sub_columns[i]->Append(*struct_stats.child_stats[i], state.child_appends[i + 1], *child_entries[i], count);
127: 	}
128: }
129: 
130: void StructColumnData::RevertAppend(row_t start_row) {
131: 	validity.RevertAppend(start_row);
132: 	for (auto &sub_column : sub_columns) {
133: 		sub_column->RevertAppend(start_row);
134: 	}
135: }
136: 
137: idx_t StructColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &result) {
138: 	// fetch validity mask
139: 	auto &child_entries = StructVector::GetEntries(result);
140: 	// insert any child states that are required
141: 	for (idx_t i = state.child_states.size(); i < child_entries.size() + 1; i++) {
142: 		ColumnScanState child_state;
143: 		state.child_states.push_back(move(child_state));
144: 	}
145: 	// fetch the validity state
146: 	idx_t scan_count = validity.Fetch(state.child_states[0], row_id, result);
147: 	// fetch the sub-column states
148: 	for (idx_t i = 0; i < child_entries.size(); i++) {
149: 		sub_columns[i]->Fetch(state.child_states[i + 1], row_id, *child_entries[i]);
150: 	}
151: 	return scan_count;
152: }
153: 
154: void StructColumnData::Update(Transaction &transaction, idx_t column_index, Vector &update_vector, row_t *row_ids,
155:                               idx_t offset, idx_t update_count) {
156: 	validity.Update(transaction, column_index, update_vector, row_ids, offset, update_count);
157: 	auto &child_entries = StructVector::GetEntries(update_vector);
158: 	for (idx_t i = 0; i < child_entries.size(); i++) {
159: 		sub_columns[i]->Update(transaction, column_index, *child_entries[i], row_ids, offset, update_count);
160: 	}
161: }
162: 
163: void StructColumnData::UpdateColumn(Transaction &transaction, const vector<column_t> &column_path,
164:                                     Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t depth) {
165: 	// we can never DIRECTLY update a struct column
166: 	if (depth >= column_path.size()) {
167: 		throw InternalException("Attempting to directly update a struct column - this should not be possible");
168: 	}
169: 	auto update_column = column_path[depth];
170: 	if (update_column == 0) {
171: 		// update the validity column
172: 		validity.UpdateColumn(transaction, column_path, update_vector, row_ids, update_count, depth + 1);
173: 	} else {
174: 		if (update_column > sub_columns.size()) {
175: 			throw InternalException("Update column_path out of range");
176: 		}
177: 		sub_columns[update_column - 1]->UpdateColumn(transaction, column_path, update_vector, row_ids, update_count,
178: 		                                             depth + 1);
179: 	}
180: }
181: 
182: unique_ptr<BaseStatistics> StructColumnData::GetUpdateStatistics() {
183: 	// check if any child column has updates
184: 	auto stats = BaseStatistics::CreateEmpty(type);
185: 	auto &struct_stats = (StructStatistics &)*stats;
186: 	stats->validity_stats = validity.GetUpdateStatistics();
187: 	for (idx_t i = 0; i < sub_columns.size(); i++) {
188: 		auto child_stats = sub_columns[i]->GetUpdateStatistics();
189: 		if (child_stats) {
190: 			struct_stats.child_stats[i] = move(child_stats);
191: 		}
192: 	}
193: 	return stats;
194: }
195: 
196: void StructColumnData::FetchRow(Transaction &transaction, ColumnFetchState &state, row_t row_id, Vector &result,
197:                                 idx_t result_idx) {
198: 	// fetch validity mask
199: 	auto &child_entries = StructVector::GetEntries(result);
200: 	// insert any child states that are required
201: 	for (idx_t i = state.child_states.size(); i < child_entries.size() + 1; i++) {
202: 		auto child_state = make_unique<ColumnFetchState>();
203: 		state.child_states.push_back(move(child_state));
204: 	}
205: 	// fetch the validity state
206: 	validity.FetchRow(transaction, *state.child_states[0], row_id, result, result_idx);
207: 	// fetch the sub-column states
208: 	for (idx_t i = 0; i < child_entries.size(); i++) {
209: 		sub_columns[i]->FetchRow(transaction, *state.child_states[i + 1], row_id, *child_entries[i], result_idx);
210: 	}
211: }
212: 
213: void StructColumnData::CommitDropColumn() {
214: 	validity.CommitDropColumn();
215: 	for (auto &sub_column : sub_columns) {
216: 		sub_column->CommitDropColumn();
217: 	}
218: }
219: 
220: struct StructColumnCheckpointState : public ColumnCheckpointState {
221: 	StructColumnCheckpointState(RowGroup &row_group, ColumnData &column_data, TableDataWriter &writer)
222: 	    : ColumnCheckpointState(row_group, column_data, writer) {
223: 		global_stats = make_unique<StructStatistics>(column_data.type);
224: 	}
225: 
226: 	unique_ptr<ColumnCheckpointState> validity_state;
227: 	vector<unique_ptr<ColumnCheckpointState>> child_states;
228: 
229: public:
230: 	unique_ptr<BaseStatistics> GetStatistics() override {
231: 		auto stats = make_unique<StructStatistics>(column_data.type);
232: 		D_ASSERT(stats->child_stats.size() == child_states.size());
233: 		stats->validity_stats = validity_state->GetStatistics();
234: 		for (idx_t i = 0; i < child_states.size(); i++) {
235: 			stats->child_stats[i] = child_states[i]->GetStatistics();
236: 			D_ASSERT(stats->child_stats[i]);
237: 		}
238: 		return move(stats);
239: 	}
240: 
241: 	void FlushToDisk() override {
242: 		validity_state->FlushToDisk();
243: 		for (auto &state : child_states) {
244: 			state->FlushToDisk();
245: 		}
246: 	}
247: };
248: 
249: unique_ptr<ColumnCheckpointState> StructColumnData::CreateCheckpointState(RowGroup &row_group,
250:                                                                           TableDataWriter &writer) {
251: 	return make_unique<StructColumnCheckpointState>(row_group, *this, writer);
252: }
253: 
254: unique_ptr<ColumnCheckpointState> StructColumnData::Checkpoint(RowGroup &row_group, TableDataWriter &writer,
255:                                                                ColumnCheckpointInfo &checkpoint_info) {
256: 	auto checkpoint_state = make_unique<StructColumnCheckpointState>(row_group, *this, writer);
257: 	checkpoint_state->validity_state = validity.Checkpoint(row_group, writer, checkpoint_info);
258: 	for (auto &sub_column : sub_columns) {
259: 		checkpoint_state->child_states.push_back(sub_column->Checkpoint(row_group, writer, checkpoint_info));
260: 	}
261: 	return move(checkpoint_state);
262: }
263: 
264: void StructColumnData::DeserializeColumn(Deserializer &source) {
265: 	validity.DeserializeColumn(source);
266: 	for (auto &sub_column : sub_columns) {
267: 		sub_column->DeserializeColumn(source);
268: 	}
269: }
270: 
271: void StructColumnData::GetStorageInfo(idx_t row_group_index, vector<idx_t> col_path, vector<vector<Value>> &result) {
272: 	col_path.push_back(0);
273: 	validity.GetStorageInfo(row_group_index, col_path, result);
274: 	for (idx_t i = 0; i < sub_columns.size(); i++) {
275: 		col_path.back() = i + 1;
276: 		sub_columns[i]->GetStorageInfo(row_group_index, col_path, result);
277: 	}
278: }
279: 
280: void StructColumnData::Verify(RowGroup &parent) {
281: #ifdef DEBUG
282: 	ColumnData::Verify(parent);
283: 	validity.Verify(parent);
284: 	for (auto &sub_column : sub_columns) {
285: 		sub_column->Verify(parent);
286: 	}
287: #endif
288: }
289: 
290: } // namespace duckdb
[end of src/storage/table/struct_column_data.cpp]
[start of src/storage/table/update_segment.cpp]
1: #include "duckdb/storage/table/update_segment.hpp"
2: #include "duckdb/transaction/update_info.hpp"
3: #include "duckdb/storage/table/column_data.hpp"
4: #include "duckdb/storage/statistics/numeric_statistics.hpp"
5: #include "duckdb/transaction/transaction.hpp"
6: #include "duckdb/storage/statistics/string_statistics.hpp"
7: #include "duckdb/storage/statistics/validity_statistics.hpp"
8: 
9: namespace duckdb {
10: 
11: static UpdateSegment::initialize_update_function_t GetInitializeUpdateFunction(PhysicalType type);
12: static UpdateSegment::fetch_update_function_t GetFetchUpdateFunction(PhysicalType type);
13: static UpdateSegment::fetch_committed_function_t GetFetchCommittedFunction(PhysicalType type);
14: static UpdateSegment::fetch_committed_range_function_t GetFetchCommittedRangeFunction(PhysicalType type);
15: 
16: static UpdateSegment::merge_update_function_t GetMergeUpdateFunction(PhysicalType type);
17: static UpdateSegment::rollback_update_function_t GetRollbackUpdateFunction(PhysicalType type);
18: static UpdateSegment::statistics_update_function_t GetStatisticsUpdateFunction(PhysicalType type);
19: static UpdateSegment::fetch_row_function_t GetFetchRowFunction(PhysicalType type);
20: 
21: UpdateSegment::UpdateSegment(ColumnData &column_data) : column_data(column_data), stats(column_data.type) {
22: 	auto physical_type = column_data.type.InternalType();
23: 
24: 	this->type_size = GetTypeIdSize(physical_type);
25: 
26: 	this->initialize_update_function = GetInitializeUpdateFunction(physical_type);
27: 	this->fetch_update_function = GetFetchUpdateFunction(physical_type);
28: 	this->fetch_committed_function = GetFetchCommittedFunction(physical_type);
29: 	this->fetch_committed_range = GetFetchCommittedRangeFunction(physical_type);
30: 	this->fetch_row_function = GetFetchRowFunction(physical_type);
31: 	this->merge_update_function = GetMergeUpdateFunction(physical_type);
32: 	this->rollback_update_function = GetRollbackUpdateFunction(physical_type);
33: 	this->statistics_update_function = GetStatisticsUpdateFunction(physical_type);
34: }
35: 
36: UpdateSegment::~UpdateSegment() {
37: }
38: 
39: void UpdateSegment::ClearUpdates() {
40: 	stats.Reset();
41: 	root.reset();
42: 	heap.Destroy();
43: }
44: 
45: //===--------------------------------------------------------------------===//
46: // Update Info Helpers
47: //===--------------------------------------------------------------------===//
48: Value UpdateInfo::GetValue(idx_t index) {
49: 	auto &type = segment->column_data.type;
50: 
51: 	switch (type.id()) {
52: 	case LogicalTypeId::VALIDITY:
53: 		return Value::BOOLEAN(((bool *)tuple_data)[index]);
54: 	case LogicalTypeId::INTEGER:
55: 		return Value::INTEGER(((int32_t *)tuple_data)[index]);
56: 	default:
57: 		throw NotImplementedException("Unimplemented type for UpdateInfo::GetValue");
58: 	}
59: }
60: 
61: void UpdateInfo::Print() {
62: 	Printer::Print(ToString());
63: }
64: 
65: string UpdateInfo::ToString() {
66: 	auto &type = segment->column_data.type;
67: 	string result = "Update Info [" + type.ToString() + ", Count: " + to_string(N) +
68: 	                ", Transaction Id: " + to_string(version_number) + "]\n";
69: 	for (idx_t i = 0; i < N; i++) {
70: 		result += to_string(tuples[i]) + ": " + GetValue(i).ToString() + "\n";
71: 	}
72: 	if (next) {
73: 		result += "\nChild Segment: " + next->ToString();
74: 	}
75: 	return result;
76: }
77: 
78: void UpdateInfo::Verify() {
79: #ifdef DEBUG
80: 	for (idx_t i = 1; i < N; i++) {
81: 		D_ASSERT(tuples[i] > tuples[i - 1] && tuples[i] < STANDARD_VECTOR_SIZE);
82: 	}
83: #endif
84: }
85: 
86: //===--------------------------------------------------------------------===//
87: // Update Fetch
88: //===--------------------------------------------------------------------===//
89: static void MergeValidityInfo(UpdateInfo *current, ValidityMask &result_mask) {
90: 	auto info_data = (bool *)current->tuple_data;
91: 	for (idx_t i = 0; i < current->N; i++) {
92: 		result_mask.Set(current->tuples[i], info_data[i]);
93: 	}
94: }
95: 
96: static void UpdateMergeValidity(transaction_t start_time, transaction_t transaction_id, UpdateInfo *info,
97:                                 Vector &result) {
98: 	auto &result_mask = FlatVector::Validity(result);
99: 	UpdateInfo::UpdatesForTransaction(info, start_time, transaction_id,
100: 	                                  [&](UpdateInfo *current) { MergeValidityInfo(current, result_mask); });
101: }
102: 
103: template <class T>
104: static void MergeUpdateInfo(UpdateInfo *current, T *result_data) {
105: 	auto info_data = (T *)current->tuple_data;
106: 	if (current->N == STANDARD_VECTOR_SIZE) {
107: 		// special case: update touches ALL tuples of this vector
108: 		// in this case we can just memcpy the data
109: 		// since the layout of the update info is guaranteed to be [0, 1, 2, 3, ...]
110: 		memcpy(result_data, info_data, sizeof(T) * current->N);
111: 	} else {
112: 		for (idx_t i = 0; i < current->N; i++) {
113: 			result_data[current->tuples[i]] = info_data[i];
114: 		}
115: 	}
116: }
117: 
118: template <class T>
119: static void UpdateMergeFetch(transaction_t start_time, transaction_t transaction_id, UpdateInfo *info, Vector &result) {
120: 	auto result_data = FlatVector::GetData<T>(result);
121: 	UpdateInfo::UpdatesForTransaction(info, start_time, transaction_id,
122: 	                                  [&](UpdateInfo *current) { MergeUpdateInfo<T>(current, result_data); });
123: }
124: 
125: static UpdateSegment::fetch_update_function_t GetFetchUpdateFunction(PhysicalType type) {
126: 	switch (type) {
127: 	case PhysicalType::BIT:
128: 		return UpdateMergeValidity;
129: 	case PhysicalType::BOOL:
130: 	case PhysicalType::INT8:
131: 		return UpdateMergeFetch<int8_t>;
132: 	case PhysicalType::INT16:
133: 		return UpdateMergeFetch<int16_t>;
134: 	case PhysicalType::INT32:
135: 		return UpdateMergeFetch<int32_t>;
136: 	case PhysicalType::INT64:
137: 		return UpdateMergeFetch<int64_t>;
138: 	case PhysicalType::UINT8:
139: 		return UpdateMergeFetch<uint8_t>;
140: 	case PhysicalType::UINT16:
141: 		return UpdateMergeFetch<uint16_t>;
142: 	case PhysicalType::UINT32:
143: 		return UpdateMergeFetch<uint32_t>;
144: 	case PhysicalType::UINT64:
145: 		return UpdateMergeFetch<uint64_t>;
146: 	case PhysicalType::INT128:
147: 		return UpdateMergeFetch<hugeint_t>;
148: 	case PhysicalType::FLOAT:
149: 		return UpdateMergeFetch<float>;
150: 	case PhysicalType::DOUBLE:
151: 		return UpdateMergeFetch<double>;
152: 	case PhysicalType::INTERVAL:
153: 		return UpdateMergeFetch<interval_t>;
154: 	case PhysicalType::VARCHAR:
155: 		return UpdateMergeFetch<string_t>;
156: 	default:
157: 		throw NotImplementedException("Unimplemented type for update segment");
158: 	}
159: }
160: 
161: void UpdateSegment::FetchUpdates(Transaction &transaction, idx_t vector_index, Vector &result) {
162: 	auto lock_handle = lock.GetSharedLock();
163: 	if (!root) {
164: 		return;
165: 	}
166: 	if (!root->info[vector_index]) {
167: 		return;
168: 	}
169: 	// FIXME: normalify if this is not the case... need to pass in count?
170: 	D_ASSERT(result.GetVectorType() == VectorType::FLAT_VECTOR);
171: 
172: 	fetch_update_function(transaction.start_time, transaction.transaction_id, root->info[vector_index]->info.get(),
173: 	                      result);
174: }
175: 
176: //===--------------------------------------------------------------------===//
177: // Fetch Committed
178: //===--------------------------------------------------------------------===//
179: static void FetchCommittedValidity(UpdateInfo *info, Vector &result) {
180: 	auto &result_mask = FlatVector::Validity(result);
181: 	MergeValidityInfo(info, result_mask);
182: }
183: 
184: template <class T>
185: static void TemplatedFetchCommitted(UpdateInfo *info, Vector &result) {
186: 	auto result_data = FlatVector::GetData<T>(result);
187: 	MergeUpdateInfo<T>(info, result_data);
188: }
189: 
190: static UpdateSegment::fetch_committed_function_t GetFetchCommittedFunction(PhysicalType type) {
191: 	switch (type) {
192: 	case PhysicalType::BIT:
193: 		return FetchCommittedValidity;
194: 	case PhysicalType::BOOL:
195: 	case PhysicalType::INT8:
196: 		return TemplatedFetchCommitted<int8_t>;
197: 	case PhysicalType::INT16:
198: 		return TemplatedFetchCommitted<int16_t>;
199: 	case PhysicalType::INT32:
200: 		return TemplatedFetchCommitted<int32_t>;
201: 	case PhysicalType::INT64:
202: 		return TemplatedFetchCommitted<int64_t>;
203: 	case PhysicalType::UINT8:
204: 		return TemplatedFetchCommitted<uint8_t>;
205: 	case PhysicalType::UINT16:
206: 		return TemplatedFetchCommitted<uint16_t>;
207: 	case PhysicalType::UINT32:
208: 		return TemplatedFetchCommitted<uint32_t>;
209: 	case PhysicalType::UINT64:
210: 		return TemplatedFetchCommitted<uint64_t>;
211: 	case PhysicalType::INT128:
212: 		return TemplatedFetchCommitted<hugeint_t>;
213: 	case PhysicalType::FLOAT:
214: 		return TemplatedFetchCommitted<float>;
215: 	case PhysicalType::DOUBLE:
216: 		return TemplatedFetchCommitted<double>;
217: 	case PhysicalType::INTERVAL:
218: 		return TemplatedFetchCommitted<interval_t>;
219: 	case PhysicalType::VARCHAR:
220: 		return TemplatedFetchCommitted<string_t>;
221: 	default:
222: 		throw NotImplementedException("Unimplemented type for update segment");
223: 	}
224: }
225: 
226: void UpdateSegment::FetchCommitted(idx_t vector_index, Vector &result) {
227: 	auto lock_handle = lock.GetSharedLock();
228: 
229: 	if (!root) {
230: 		return;
231: 	}
232: 	if (!root->info[vector_index]) {
233: 		return;
234: 	}
235: 	// FIXME: normalify if this is not the case... need to pass in count?
236: 	D_ASSERT(result.GetVectorType() == VectorType::FLAT_VECTOR);
237: 
238: 	fetch_committed_function(root->info[vector_index]->info.get(), result);
239: }
240: 
241: //===--------------------------------------------------------------------===//
242: // Fetch Range
243: //===--------------------------------------------------------------------===//
244: static void MergeUpdateInfoRangeValidity(UpdateInfo *current, idx_t start, idx_t end, idx_t result_offset,
245:                                          ValidityMask &result_mask) {
246: 	auto info_data = (bool *)current->tuple_data;
247: 	for (idx_t i = 0; i < current->N; i++) {
248: 		auto tuple_idx = current->tuples[i];
249: 		if (tuple_idx < start) {
250: 			continue;
251: 		} else if (tuple_idx >= end) {
252: 			break;
253: 		}
254: 		auto result_idx = result_offset + tuple_idx - start;
255: 		result_mask.Set(result_idx, info_data[i]);
256: 	}
257: }
258: 
259: static void FetchCommittedRangeValidity(UpdateInfo *info, idx_t start, idx_t end, idx_t result_offset, Vector &result) {
260: 	auto &result_mask = FlatVector::Validity(result);
261: 	MergeUpdateInfoRangeValidity(info, start, end, result_offset, result_mask);
262: }
263: 
264: template <class T>
265: static void MergeUpdateInfoRange(UpdateInfo *current, idx_t start, idx_t end, idx_t result_offset, T *result_data) {
266: 	auto info_data = (T *)current->tuple_data;
267: 	for (idx_t i = 0; i < current->N; i++) {
268: 		auto tuple_idx = current->tuples[i];
269: 		if (tuple_idx < start) {
270: 			continue;
271: 		} else if (tuple_idx >= end) {
272: 			break;
273: 		}
274: 		auto result_idx = result_offset + tuple_idx - start;
275: 		result_data[result_idx] = info_data[i];
276: 	}
277: }
278: 
279: template <class T>
280: static void TemplatedFetchCommittedRange(UpdateInfo *info, idx_t start, idx_t end, idx_t result_offset,
281:                                          Vector &result) {
282: 	auto result_data = FlatVector::GetData<T>(result);
283: 	MergeUpdateInfoRange<T>(info, start, end, result_offset, result_data);
284: }
285: 
286: static UpdateSegment::fetch_committed_range_function_t GetFetchCommittedRangeFunction(PhysicalType type) {
287: 	switch (type) {
288: 	case PhysicalType::BIT:
289: 		return FetchCommittedRangeValidity;
290: 	case PhysicalType::BOOL:
291: 	case PhysicalType::INT8:
292: 		return TemplatedFetchCommittedRange<int8_t>;
293: 	case PhysicalType::INT16:
294: 		return TemplatedFetchCommittedRange<int16_t>;
295: 	case PhysicalType::INT32:
296: 		return TemplatedFetchCommittedRange<int32_t>;
297: 	case PhysicalType::INT64:
298: 		return TemplatedFetchCommittedRange<int64_t>;
299: 	case PhysicalType::UINT8:
300: 		return TemplatedFetchCommittedRange<uint8_t>;
301: 	case PhysicalType::UINT16:
302: 		return TemplatedFetchCommittedRange<uint16_t>;
303: 	case PhysicalType::UINT32:
304: 		return TemplatedFetchCommittedRange<uint32_t>;
305: 	case PhysicalType::UINT64:
306: 		return TemplatedFetchCommittedRange<uint64_t>;
307: 	case PhysicalType::INT128:
308: 		return TemplatedFetchCommittedRange<hugeint_t>;
309: 	case PhysicalType::FLOAT:
310: 		return TemplatedFetchCommittedRange<float>;
311: 	case PhysicalType::DOUBLE:
312: 		return TemplatedFetchCommittedRange<double>;
313: 	case PhysicalType::INTERVAL:
314: 		return TemplatedFetchCommittedRange<interval_t>;
315: 	case PhysicalType::VARCHAR:
316: 		return TemplatedFetchCommittedRange<string_t>;
317: 	default:
318: 		throw NotImplementedException("Unimplemented type for update segment");
319: 	}
320: }
321: 
322: void UpdateSegment::FetchCommittedRange(idx_t start_row, idx_t count, Vector &result) {
323: 	D_ASSERT(count > 0);
324: 	if (!root) {
325: 		return;
326: 	}
327: 	D_ASSERT(result.GetVectorType() == VectorType::FLAT_VECTOR);
328: 
329: 	idx_t end_row = start_row + count;
330: 	idx_t start_vector = start_row / STANDARD_VECTOR_SIZE;
331: 	idx_t end_vector = (end_row - 1) / STANDARD_VECTOR_SIZE;
332: 	D_ASSERT(start_vector <= end_vector);
333: 	D_ASSERT(end_vector < RowGroup::ROW_GROUP_VECTOR_COUNT);
334: 
335: 	for (idx_t vector_idx = start_vector; vector_idx <= end_vector; vector_idx++) {
336: 		if (!root->info[vector_idx]) {
337: 			continue;
338: 		}
339: 		idx_t start_in_vector = vector_idx == start_vector ? start_row - start_vector * STANDARD_VECTOR_SIZE : 0;
340: 		idx_t end_in_vector =
341: 		    vector_idx == end_vector ? end_row - end_vector * STANDARD_VECTOR_SIZE : STANDARD_VECTOR_SIZE;
342: 		D_ASSERT(start_in_vector < end_in_vector);
343: 		D_ASSERT(end_in_vector > 0 && end_in_vector <= STANDARD_VECTOR_SIZE);
344: 		idx_t result_offset = ((vector_idx * STANDARD_VECTOR_SIZE) + start_in_vector) - start_row;
345: 		fetch_committed_range(root->info[vector_idx]->info.get(), start_in_vector, end_in_vector, result_offset,
346: 		                      result);
347: 	}
348: }
349: 
350: //===--------------------------------------------------------------------===//
351: // Fetch Row
352: //===--------------------------------------------------------------------===//
353: static void FetchRowValidity(transaction_t start_time, transaction_t transaction_id, UpdateInfo *info, idx_t row_idx,
354:                              Vector &result, idx_t result_idx) {
355: 	auto &result_mask = FlatVector::Validity(result);
356: 	UpdateInfo::UpdatesForTransaction(info, start_time, transaction_id, [&](UpdateInfo *current) {
357: 		auto info_data = (bool *)current->tuple_data;
358: 		// FIXME: we could do a binary search in here
359: 		for (idx_t i = 0; i < current->N; i++) {
360: 			if (current->tuples[i] == row_idx) {
361: 				result_mask.Set(result_idx, info_data[i]);
362: 				break;
363: 			} else if (current->tuples[i] > row_idx) {
364: 				break;
365: 			}
366: 		}
367: 	});
368: }
369: 
370: template <class T>
371: static void TemplatedFetchRow(transaction_t start_time, transaction_t transaction_id, UpdateInfo *info, idx_t row_idx,
372:                               Vector &result, idx_t result_idx) {
373: 	auto result_data = FlatVector::GetData<T>(result);
374: 	UpdateInfo::UpdatesForTransaction(info, start_time, transaction_id, [&](UpdateInfo *current) {
375: 		auto info_data = (T *)current->tuple_data;
376: 		// FIXME: we could do a binary search in here
377: 		for (idx_t i = 0; i < current->N; i++) {
378: 			if (current->tuples[i] == row_idx) {
379: 				result_data[result_idx] = info_data[i];
380: 				break;
381: 			} else if (current->tuples[i] > row_idx) {
382: 				break;
383: 			}
384: 		}
385: 	});
386: }
387: 
388: static UpdateSegment::fetch_row_function_t GetFetchRowFunction(PhysicalType type) {
389: 	switch (type) {
390: 	case PhysicalType::BIT:
391: 		return FetchRowValidity;
392: 	case PhysicalType::BOOL:
393: 	case PhysicalType::INT8:
394: 		return TemplatedFetchRow<int8_t>;
395: 	case PhysicalType::INT16:
396: 		return TemplatedFetchRow<int16_t>;
397: 	case PhysicalType::INT32:
398: 		return TemplatedFetchRow<int32_t>;
399: 	case PhysicalType::INT64:
400: 		return TemplatedFetchRow<int64_t>;
401: 	case PhysicalType::UINT8:
402: 		return TemplatedFetchRow<uint8_t>;
403: 	case PhysicalType::UINT16:
404: 		return TemplatedFetchRow<uint16_t>;
405: 	case PhysicalType::UINT32:
406: 		return TemplatedFetchRow<uint32_t>;
407: 	case PhysicalType::UINT64:
408: 		return TemplatedFetchRow<uint64_t>;
409: 	case PhysicalType::INT128:
410: 		return TemplatedFetchRow<hugeint_t>;
411: 	case PhysicalType::FLOAT:
412: 		return TemplatedFetchRow<float>;
413: 	case PhysicalType::DOUBLE:
414: 		return TemplatedFetchRow<double>;
415: 	case PhysicalType::INTERVAL:
416: 		return TemplatedFetchRow<interval_t>;
417: 	case PhysicalType::VARCHAR:
418: 		return TemplatedFetchRow<string_t>;
419: 	default:
420: 		throw NotImplementedException("Unimplemented type for update segment fetch row");
421: 	}
422: }
423: 
424: void UpdateSegment::FetchRow(Transaction &transaction, idx_t row_id, Vector &result, idx_t result_idx) {
425: 	if (!root) {
426: 		return;
427: 	}
428: 	idx_t vector_index = (row_id - column_data.start) / STANDARD_VECTOR_SIZE;
429: 	if (!root->info[vector_index]) {
430: 		return;
431: 	}
432: 	idx_t row_in_vector = row_id - vector_index * STANDARD_VECTOR_SIZE;
433: 	fetch_row_function(transaction.start_time, transaction.transaction_id, root->info[vector_index]->info.get(),
434: 	                   row_in_vector, result, result_idx);
435: }
436: 
437: //===--------------------------------------------------------------------===//
438: // Rollback update
439: //===--------------------------------------------------------------------===//
440: template <class T>
441: static void RollbackUpdate(UpdateInfo *base_info, UpdateInfo *rollback_info) {
442: 	auto base_data = (T *)base_info->tuple_data;
443: 	auto rollback_data = (T *)rollback_info->tuple_data;
444: 	idx_t base_offset = 0;
445: 	for (idx_t i = 0; i < rollback_info->N; i++) {
446: 		auto id = rollback_info->tuples[i];
447: 		while (base_info->tuples[base_offset] < id) {
448: 			base_offset++;
449: 			D_ASSERT(base_offset < base_info->N);
450: 		}
451: 		base_data[base_offset] = rollback_data[i];
452: 	}
453: }
454: 
455: static UpdateSegment::rollback_update_function_t GetRollbackUpdateFunction(PhysicalType type) {
456: 	switch (type) {
457: 	case PhysicalType::BIT:
458: 		return RollbackUpdate<bool>;
459: 	case PhysicalType::BOOL:
460: 	case PhysicalType::INT8:
461: 		return RollbackUpdate<int8_t>;
462: 	case PhysicalType::INT16:
463: 		return RollbackUpdate<int16_t>;
464: 	case PhysicalType::INT32:
465: 		return RollbackUpdate<int32_t>;
466: 	case PhysicalType::INT64:
467: 		return RollbackUpdate<int64_t>;
468: 	case PhysicalType::UINT8:
469: 		return RollbackUpdate<uint8_t>;
470: 	case PhysicalType::UINT16:
471: 		return RollbackUpdate<uint16_t>;
472: 	case PhysicalType::UINT32:
473: 		return RollbackUpdate<uint32_t>;
474: 	case PhysicalType::UINT64:
475: 		return RollbackUpdate<uint64_t>;
476: 	case PhysicalType::INT128:
477: 		return RollbackUpdate<hugeint_t>;
478: 	case PhysicalType::FLOAT:
479: 		return RollbackUpdate<float>;
480: 	case PhysicalType::DOUBLE:
481: 		return RollbackUpdate<double>;
482: 	case PhysicalType::INTERVAL:
483: 		return RollbackUpdate<interval_t>;
484: 	case PhysicalType::VARCHAR:
485: 		return RollbackUpdate<string_t>;
486: 	default:
487: 		throw NotImplementedException("Unimplemented type for uncompressed segment");
488: 	}
489: }
490: 
491: void UpdateSegment::RollbackUpdate(UpdateInfo *info) {
492: 	// obtain an exclusive lock
493: 	auto lock_handle = lock.GetExclusiveLock();
494: 
495: 	// move the data from the UpdateInfo back into the base info
496: 	D_ASSERT(root->info[info->vector_index]);
497: 	rollback_update_function(root->info[info->vector_index]->info.get(), info);
498: 
499: 	// clean up the update chain
500: 	CleanupUpdateInternal(*lock_handle, info);
501: }
502: 
503: //===--------------------------------------------------------------------===//
504: // Cleanup Update
505: //===--------------------------------------------------------------------===//
506: void UpdateSegment::CleanupUpdateInternal(const StorageLockKey &lock, UpdateInfo *info) {
507: 	D_ASSERT(info->prev);
508: 	auto prev = info->prev;
509: 	prev->next = info->next;
510: 	if (prev->next) {
511: 		prev->next->prev = prev;
512: 	}
513: }
514: 
515: void UpdateSegment::CleanupUpdate(UpdateInfo *info) {
516: 	// obtain an exclusive lock
517: 	auto lock_handle = lock.GetExclusiveLock();
518: 	CleanupUpdateInternal(*lock_handle, info);
519: }
520: 
521: //===--------------------------------------------------------------------===//
522: // Check for conflicts in update
523: //===--------------------------------------------------------------------===//
524: static void CheckForConflicts(UpdateInfo *info, Transaction &transaction, row_t *ids, idx_t count, row_t offset,
525:                               UpdateInfo *&node) {
526: 	if (!info) {
527: 		return;
528: 	}
529: 	if (info->version_number == transaction.transaction_id) {
530: 		// this UpdateInfo belongs to the current transaction, set it in the node
531: 		node = info;
532: 	} else if (info->version_number > transaction.start_time) {
533: 		// potential conflict, check that tuple ids do not conflict
534: 		// as both ids and info->tuples are sorted, this is similar to a merge join
535: 		idx_t i = 0, j = 0;
536: 		while (true) {
537: 			auto id = ids[i] - offset;
538: 			if (id == info->tuples[j]) {
539: 				throw TransactionException("Conflict on update!");
540: 			} else if (id < info->tuples[j]) {
541: 				// id < the current tuple in info, move to next id
542: 				i++;
543: 				if (i == count) {
544: 					break;
545: 				}
546: 			} else {
547: 				// id > the current tuple, move to next tuple in info
548: 				j++;
549: 				if (j == info->N) {
550: 					break;
551: 				}
552: 			}
553: 		}
554: 	}
555: 	CheckForConflicts(info->next, transaction, ids, count, offset, node);
556: }
557: 
558: //===--------------------------------------------------------------------===//
559: // Initialize update info
560: //===--------------------------------------------------------------------===//
561: void UpdateSegment::InitializeUpdateInfo(UpdateInfo &info, row_t *ids, const SelectionVector &sel, idx_t count,
562:                                          idx_t vector_index, idx_t vector_offset) {
563: 	info.segment = this;
564: 	info.vector_index = vector_index;
565: 	info.prev = nullptr;
566: 	info.next = nullptr;
567: 
568: 	// set up the tuple ids
569: 	info.N = count;
570: 	for (idx_t i = 0; i < count; i++) {
571: 		auto idx = sel.get_index(i);
572: 		auto id = ids[idx];
573: 		D_ASSERT(idx_t(id) >= vector_offset && idx_t(id) < vector_offset + STANDARD_VECTOR_SIZE);
574: 		info.tuples[i] = id - vector_offset;
575: 	};
576: }
577: 
578: static void InitializeUpdateValidity(UpdateInfo *base_info, Vector &base_data, UpdateInfo *update_info, Vector &update,
579:                                      const SelectionVector &sel) {
580: 	auto &update_mask = FlatVector::Validity(update);
581: 	auto tuple_data = (bool *)update_info->tuple_data;
582: 
583: 	if (!update_mask.AllValid()) {
584: 		for (idx_t i = 0; i < update_info->N; i++) {
585: 			auto idx = sel.get_index(i);
586: 			tuple_data[i] = update_mask.RowIsValidUnsafe(idx);
587: 		}
588: 	} else {
589: 		for (idx_t i = 0; i < update_info->N; i++) {
590: 			tuple_data[i] = true;
591: 		}
592: 	}
593: 
594: 	auto &base_mask = FlatVector::Validity(base_data);
595: 	auto base_tuple_data = (bool *)base_info->tuple_data;
596: 	if (!base_mask.AllValid()) {
597: 		for (idx_t i = 0; i < base_info->N; i++) {
598: 			base_tuple_data[i] = base_mask.RowIsValidUnsafe(base_info->tuples[i]);
599: 		}
600: 	} else {
601: 		for (idx_t i = 0; i < base_info->N; i++) {
602: 			base_tuple_data[i] = true;
603: 		}
604: 	}
605: }
606: 
607: struct UpdateSelectElement {
608: 	template <class T>
609: 	static T Operation(UpdateSegment *segment, T element) {
610: 		return element;
611: 	}
612: };
613: 
614: template <>
615: string_t UpdateSelectElement::Operation(UpdateSegment *segment, string_t element) {
616: 	return element.IsInlined() ? element : segment->GetStringHeap().AddString(element);
617: }
618: 
619: template <class T>
620: static void InitializeUpdateData(UpdateInfo *base_info, Vector &base_data, UpdateInfo *update_info, Vector &update,
621:                                  const SelectionVector &sel) {
622: 	auto update_data = FlatVector::GetData<T>(update);
623: 	auto tuple_data = (T *)update_info->tuple_data;
624: 
625: 	for (idx_t i = 0; i < update_info->N; i++) {
626: 		auto idx = sel.get_index(i);
627: 		tuple_data[i] = update_data[idx];
628: 	}
629: 
630: 	auto base_array_data = FlatVector::GetData<T>(base_data);
631: 	auto base_tuple_data = (T *)base_info->tuple_data;
632: 	for (idx_t i = 0; i < base_info->N; i++) {
633: 		base_tuple_data[i] =
634: 		    UpdateSelectElement::Operation<T>(base_info->segment, base_array_data[base_info->tuples[i]]);
635: 	}
636: }
637: 
638: static UpdateSegment::initialize_update_function_t GetInitializeUpdateFunction(PhysicalType type) {
639: 	switch (type) {
640: 	case PhysicalType::BIT:
641: 		return InitializeUpdateValidity;
642: 	case PhysicalType::BOOL:
643: 	case PhysicalType::INT8:
644: 		return InitializeUpdateData<int8_t>;
645: 	case PhysicalType::INT16:
646: 		return InitializeUpdateData<int16_t>;
647: 	case PhysicalType::INT32:
648: 		return InitializeUpdateData<int32_t>;
649: 	case PhysicalType::INT64:
650: 		return InitializeUpdateData<int64_t>;
651: 	case PhysicalType::UINT8:
652: 		return InitializeUpdateData<uint8_t>;
653: 	case PhysicalType::UINT16:
654: 		return InitializeUpdateData<uint16_t>;
655: 	case PhysicalType::UINT32:
656: 		return InitializeUpdateData<uint32_t>;
657: 	case PhysicalType::UINT64:
658: 		return InitializeUpdateData<uint64_t>;
659: 	case PhysicalType::INT128:
660: 		return InitializeUpdateData<hugeint_t>;
661: 	case PhysicalType::FLOAT:
662: 		return InitializeUpdateData<float>;
663: 	case PhysicalType::DOUBLE:
664: 		return InitializeUpdateData<double>;
665: 	case PhysicalType::INTERVAL:
666: 		return InitializeUpdateData<interval_t>;
667: 	case PhysicalType::VARCHAR:
668: 		return InitializeUpdateData<string_t>;
669: 	default:
670: 		throw NotImplementedException("Unimplemented type for update segment");
671: 	}
672: }
673: 
674: //===--------------------------------------------------------------------===//
675: // Merge update info
676: //===--------------------------------------------------------------------===//
677: template <class F1, class F2, class F3>
678: static idx_t MergeLoop(row_t a[], sel_t b[], idx_t acount, idx_t bcount, idx_t aoffset, F1 merge, F2 pick_a, F3 pick_b,
679:                        const SelectionVector &asel) {
680: 	idx_t aidx = 0, bidx = 0;
681: 	idx_t count = 0;
682: 	while (aidx < acount && bidx < bcount) {
683: 		auto a_index = asel.get_index(aidx);
684: 		auto a_id = a[a_index] - aoffset;
685: 		auto b_id = b[bidx];
686: 		if (a_id == b_id) {
687: 			merge(a_id, a_index, bidx, count);
688: 			aidx++;
689: 			bidx++;
690: 			count++;
691: 		} else if (a_id < b_id) {
692: 			pick_a(a_id, a_index, count);
693: 			aidx++;
694: 			count++;
695: 		} else {
696: 			pick_b(b_id, bidx, count);
697: 			bidx++;
698: 			count++;
699: 		}
700: 	}
701: 	for (; aidx < acount; aidx++) {
702: 		auto a_index = asel.get_index(aidx);
703: 		pick_a(a[a_index] - aoffset, a_index, count);
704: 		count++;
705: 	}
706: 	for (; bidx < bcount; bidx++) {
707: 		pick_b(b[bidx], bidx, count);
708: 		count++;
709: 	}
710: 	return count;
711: }
712: 
713: struct ExtractStandardEntry {
714: 	template <class T, class V>
715: 	static T Extract(V *data, idx_t entry) {
716: 		return data[entry];
717: 	}
718: };
719: 
720: struct ExtractValidityEntry {
721: 	template <class T, class V>
722: 	static T Extract(V *data, idx_t entry) {
723: 		return data->RowIsValid(entry);
724: 	}
725: };
726: 
727: template <class T, class V, class OP = ExtractStandardEntry>
728: static void MergeUpdateLoopInternal(UpdateInfo *base_info, V *base_table_data, UpdateInfo *update_info,
729:                                     V *update_vector_data, row_t *ids, idx_t count, const SelectionVector &sel) {
730: 	auto base_id = base_info->segment->column_data.start + base_info->vector_index * STANDARD_VECTOR_SIZE;
731: #ifdef DEBUG
732: 	// all of these should be sorted, otherwise the below algorithm does not work
733: 	for (idx_t i = 1; i < count; i++) {
734: 		auto prev_idx = sel.get_index(i - 1);
735: 		auto idx = sel.get_index(i);
736: 		D_ASSERT(ids[idx] > ids[prev_idx] && ids[idx] >= row_t(base_id) &&
737: 		         ids[idx] < row_t(base_id + STANDARD_VECTOR_SIZE));
738: 	}
739: #endif
740: 
741: 	// we have a new batch of updates (update, ids, count)
742: 	// we already have existing updates (base_info)
743: 	// and potentially, this transaction already has updates present (update_info)
744: 	// we need to merge these all together so that the latest updates get merged into base_info
745: 	// and the "old" values (fetched from EITHER base_info OR from base_data) get placed into update_info
746: 	auto base_info_data = (T *)base_info->tuple_data;
747: 	auto update_info_data = (T *)update_info->tuple_data;
748: 
749: 	// we first do the merging of the old values
750: 	// what we are trying to do here is update the "update_info" of this transaction with all the old data we require
751: 	// this means we need to merge (1) any previously updated values (stored in update_info->tuples)
752: 	// together with (2)
753: 	// to simplify this, we create new arrays here
754: 	// we memcpy these over afterwards
755: 	T result_values[STANDARD_VECTOR_SIZE];
756: 	sel_t result_ids[STANDARD_VECTOR_SIZE];
757: 
758: 	idx_t base_info_offset = 0;
759: 	idx_t update_info_offset = 0;
760: 	idx_t result_offset = 0;
761: 	for (idx_t i = 0; i < count; i++) {
762: 		auto idx = sel.get_index(i);
763: 		// we have to merge the info for "ids[i]"
764: 		auto update_id = ids[idx] - base_id;
765: 
766: 		while (update_info_offset < update_info->N && update_info->tuples[update_info_offset] < update_id) {
767: 			// old id comes before the current id: write it
768: 			result_values[result_offset] = update_info_data[update_info_offset];
769: 			result_ids[result_offset++] = update_info->tuples[update_info_offset];
770: 			update_info_offset++;
771: 		}
772: 		// write the new id
773: 		if (update_info_offset < update_info->N && update_info->tuples[update_info_offset] == update_id) {
774: 			// we have an id that is equivalent in the current update info: write the update info
775: 			result_values[result_offset] = update_info_data[update_info_offset];
776: 			result_ids[result_offset++] = update_info->tuples[update_info_offset];
777: 			update_info_offset++;
778: 			continue;
779: 		}
780: 
781: 		/// now check if we have the current update_id in the base_info, or if we should fetch it from the base data
782: 		while (base_info_offset < base_info->N && base_info->tuples[base_info_offset] < update_id) {
783: 			base_info_offset++;
784: 		}
785: 		if (base_info_offset < base_info->N && base_info->tuples[base_info_offset] == update_id) {
786: 			// it is! we have to move the tuple from base_info->ids[base_info_offset] to update_info
787: 			result_values[result_offset] = base_info_data[base_info_offset];
788: 		} else {
789: 			// it is not! we have to move base_table_data[update_id] to update_info
790: 			result_values[result_offset] = UpdateSelectElement::Operation<T>(
791: 			    base_info->segment, OP::template Extract<T, V>(base_table_data, update_id));
792: 		}
793: 		result_ids[result_offset++] = update_id;
794: 	}
795: 	// write any remaining entries from the old updates
796: 	while (update_info_offset < update_info->N) {
797: 		result_values[result_offset] = update_info_data[update_info_offset];
798: 		result_ids[result_offset++] = update_info->tuples[update_info_offset];
799: 		update_info_offset++;
800: 	}
801: 	// now copy them back
802: 	update_info->N = result_offset;
803: 	memcpy(update_info_data, result_values, result_offset * sizeof(T));
804: 	memcpy(update_info->tuples, result_ids, result_offset * sizeof(sel_t));
805: 
806: 	// now we merge the new values into the base_info
807: 	result_offset = 0;
808: 	auto pick_new = [&](idx_t id, idx_t aidx, idx_t count) {
809: 		result_values[result_offset] = OP::template Extract<T, V>(update_vector_data, aidx);
810: 		result_ids[result_offset] = id;
811: 		result_offset++;
812: 	};
813: 	auto pick_old = [&](idx_t id, idx_t bidx, idx_t count) {
814: 		result_values[result_offset] = base_info_data[bidx];
815: 		result_ids[result_offset] = id;
816: 		result_offset++;
817: 	};
818: 	// now we perform a merge of the new ids with the old ids
819: 	auto merge = [&](idx_t id, idx_t aidx, idx_t bidx, idx_t count) {
820: 		pick_new(id, aidx, count);
821: 	};
822: 	MergeLoop(ids, base_info->tuples, count, base_info->N, base_id, merge, pick_new, pick_old, sel);
823: 
824: 	base_info->N = result_offset;
825: 	memcpy(base_info_data, result_values, result_offset * sizeof(T));
826: 	memcpy(base_info->tuples, result_ids, result_offset * sizeof(sel_t));
827: }
828: 
829: static void MergeValidityLoop(UpdateInfo *base_info, Vector &base_data, UpdateInfo *update_info, Vector &update,
830:                               row_t *ids, idx_t count, const SelectionVector &sel) {
831: 	auto &base_validity = FlatVector::Validity(base_data);
832: 	auto &update_validity = FlatVector::Validity(update);
833: 	MergeUpdateLoopInternal<bool, ValidityMask, ExtractValidityEntry>(base_info, &base_validity, update_info,
834: 	                                                                  &update_validity, ids, count, sel);
835: }
836: 
837: template <class T>
838: static void MergeUpdateLoop(UpdateInfo *base_info, Vector &base_data, UpdateInfo *update_info, Vector &update,
839:                             row_t *ids, idx_t count, const SelectionVector &sel) {
840: 	auto base_table_data = FlatVector::GetData<T>(base_data);
841: 	auto update_vector_data = FlatVector::GetData<T>(update);
842: 	MergeUpdateLoopInternal<T, T>(base_info, base_table_data, update_info, update_vector_data, ids, count, sel);
843: }
844: 
845: static UpdateSegment::merge_update_function_t GetMergeUpdateFunction(PhysicalType type) {
846: 	switch (type) {
847: 	case PhysicalType::BIT:
848: 		return MergeValidityLoop;
849: 	case PhysicalType::BOOL:
850: 	case PhysicalType::INT8:
851: 		return MergeUpdateLoop<int8_t>;
852: 	case PhysicalType::INT16:
853: 		return MergeUpdateLoop<int16_t>;
854: 	case PhysicalType::INT32:
855: 		return MergeUpdateLoop<int32_t>;
856: 	case PhysicalType::INT64:
857: 		return MergeUpdateLoop<int64_t>;
858: 	case PhysicalType::UINT8:
859: 		return MergeUpdateLoop<uint8_t>;
860: 	case PhysicalType::UINT16:
861: 		return MergeUpdateLoop<uint16_t>;
862: 	case PhysicalType::UINT32:
863: 		return MergeUpdateLoop<uint32_t>;
864: 	case PhysicalType::UINT64:
865: 		return MergeUpdateLoop<uint64_t>;
866: 	case PhysicalType::INT128:
867: 		return MergeUpdateLoop<hugeint_t>;
868: 	case PhysicalType::FLOAT:
869: 		return MergeUpdateLoop<float>;
870: 	case PhysicalType::DOUBLE:
871: 		return MergeUpdateLoop<double>;
872: 	case PhysicalType::INTERVAL:
873: 		return MergeUpdateLoop<interval_t>;
874: 	case PhysicalType::VARCHAR:
875: 		return MergeUpdateLoop<string_t>;
876: 	default:
877: 		throw NotImplementedException("Unimplemented type for uncompressed segment");
878: 	}
879: }
880: 
881: //===--------------------------------------------------------------------===//
882: // Update statistics
883: //===--------------------------------------------------------------------===//
884: unique_ptr<BaseStatistics> UpdateSegment::GetStatistics() {
885: 	lock_guard<mutex> stats_guard(stats_lock);
886: 	return stats.statistics->Copy();
887: }
888: 
889: idx_t UpdateValidityStatistics(UpdateSegment *segment, SegmentStatistics &stats, Vector &update, idx_t offset,
890:                                idx_t count, SelectionVector &sel) {
891: 	auto &mask = FlatVector::Validity(update);
892: 	auto &validity = (ValidityStatistics &)*stats.statistics;
893: 	if (!mask.AllValid() && !validity.has_null) {
894: 		for (idx_t i = 0; i < count; i++) {
895: 			auto idx = offset + i;
896: 			if (!mask.RowIsValid(idx)) {
897: 				validity.has_null = true;
898: 				break;
899: 			}
900: 		}
901: 	}
902: 	sel.Initialize((sel_t *)(FlatVector::INCREMENTAL_VECTOR + offset));
903: 	return count;
904: }
905: 
906: template <class T>
907: idx_t TemplatedUpdateNumericStatistics(UpdateSegment *segment, SegmentStatistics &stats, Vector &update, idx_t offset,
908:                                        idx_t count, SelectionVector &sel) {
909: 	auto update_data = FlatVector::GetData<T>(update);
910: 	auto &mask = FlatVector::Validity(update);
911: 
912: 	if (mask.AllValid()) {
913: 		for (idx_t i = 0; i < count; i++) {
914: 			auto idx = offset + i;
915: 			NumericStatistics::Update<T>(stats, update_data[idx]);
916: 		}
917: 		sel.Initialize((sel_t *)(FlatVector::INCREMENTAL_VECTOR + offset));
918: 		return count;
919: 	} else {
920: 		idx_t not_null_count = 0;
921: 		sel.Initialize(STANDARD_VECTOR_SIZE);
922: 		for (idx_t i = 0; i < count; i++) {
923: 			auto idx = offset + i;
924: 			if (mask.RowIsValid(idx)) {
925: 				sel.set_index(not_null_count++, idx);
926: 				NumericStatistics::Update<T>(stats, update_data[idx]);
927: 			}
928: 		}
929: 		return not_null_count;
930: 	}
931: }
932: 
933: idx_t UpdateStringStatistics(UpdateSegment *segment, SegmentStatistics &stats, Vector &update, idx_t offset,
934:                              idx_t count, SelectionVector &sel) {
935: 	auto update_data = FlatVector::GetData<string_t>(update);
936: 	auto &mask = FlatVector::Validity(update);
937: 	if (mask.AllValid()) {
938: 		for (idx_t i = 0; i < count; i++) {
939: 			auto idx = offset + i;
940: 			((StringStatistics &)*stats.statistics).Update(update_data[idx]);
941: 			if (!update_data[idx].IsInlined()) {
942: 				update_data[idx] = segment->GetStringHeap().AddString(update_data[idx]);
943: 			}
944: 		}
945: 		sel.Initialize(FlatVector::INCREMENTAL_SELECTION_VECTOR);
946: 		return count;
947: 	} else {
948: 		idx_t not_null_count = 0;
949: 		sel.Initialize(STANDARD_VECTOR_SIZE);
950: 		for (idx_t i = 0; i < count; i++) {
951: 			auto idx = offset + i;
952: 			if (mask.RowIsValid(idx)) {
953: 				sel.set_index(not_null_count++, idx);
954: 				((StringStatistics &)*stats.statistics).Update(update_data[idx]);
955: 				if (!update_data[idx].IsInlined()) {
956: 					update_data[idx] = segment->GetStringHeap().AddString(update_data[idx]);
957: 				}
958: 			}
959: 		}
960: 		return not_null_count;
961: 	}
962: }
963: 
964: UpdateSegment::statistics_update_function_t GetStatisticsUpdateFunction(PhysicalType type) {
965: 	switch (type) {
966: 	case PhysicalType::BIT:
967: 		return UpdateValidityStatistics;
968: 	case PhysicalType::BOOL:
969: 	case PhysicalType::INT8:
970: 		return TemplatedUpdateNumericStatistics<int8_t>;
971: 	case PhysicalType::INT16:
972: 		return TemplatedUpdateNumericStatistics<int16_t>;
973: 	case PhysicalType::INT32:
974: 		return TemplatedUpdateNumericStatistics<int32_t>;
975: 	case PhysicalType::INT64:
976: 		return TemplatedUpdateNumericStatistics<int64_t>;
977: 	case PhysicalType::UINT8:
978: 		return TemplatedUpdateNumericStatistics<uint8_t>;
979: 	case PhysicalType::UINT16:
980: 		return TemplatedUpdateNumericStatistics<uint16_t>;
981: 	case PhysicalType::UINT32:
982: 		return TemplatedUpdateNumericStatistics<uint32_t>;
983: 	case PhysicalType::UINT64:
984: 		return TemplatedUpdateNumericStatistics<uint64_t>;
985: 	case PhysicalType::INT128:
986: 		return TemplatedUpdateNumericStatistics<hugeint_t>;
987: 	case PhysicalType::FLOAT:
988: 		return TemplatedUpdateNumericStatistics<float>;
989: 	case PhysicalType::DOUBLE:
990: 		return TemplatedUpdateNumericStatistics<double>;
991: 	case PhysicalType::INTERVAL:
992: 		return TemplatedUpdateNumericStatistics<interval_t>;
993: 	case PhysicalType::VARCHAR:
994: 		return UpdateStringStatistics;
995: 	default:
996: 		throw NotImplementedException("Unimplemented type for uncompressed segment");
997: 	}
998: }
999: 
1000: //===--------------------------------------------------------------------===//
1001: // Update
1002: //===--------------------------------------------------------------------===//
1003: static idx_t SortSelectionVector(SelectionVector &sel, idx_t count, row_t *ids) {
1004: 	D_ASSERT(count > 0);
1005: 
1006: 	bool is_sorted = true;
1007: 	for (idx_t i = 1; i < count; i++) {
1008: 		auto prev_idx = sel.get_index(i - 1);
1009: 		auto idx = sel.get_index(i);
1010: 		if (ids[idx] <= ids[prev_idx]) {
1011: 			is_sorted = false;
1012: 			break;
1013: 		}
1014: 	}
1015: 	if (is_sorted) {
1016: 		// already sorted: bailout
1017: 		return count;
1018: 	}
1019: 	// not sorted: need to sort the selection vector
1020: 	SelectionVector sorted_sel(count);
1021: 	for (idx_t i = 0; i < count; i++) {
1022: 		sorted_sel.set_index(i, sel.get_index(i));
1023: 	}
1024: 	std::sort(sorted_sel.data(), sorted_sel.data() + count, [&](sel_t l, sel_t r) { return ids[l] < ids[r]; });
1025: 	// eliminate any duplicates
1026: 	idx_t pos = 1;
1027: 	for (idx_t i = 1; i < count; i++) {
1028: 		auto prev_idx = sorted_sel.get_index(i - 1);
1029: 		auto idx = sorted_sel.get_index(i);
1030: 		D_ASSERT(ids[idx] >= ids[prev_idx]);
1031: 		if (ids[prev_idx] != ids[idx]) {
1032: 			sorted_sel.set_index(pos++, idx);
1033: 		}
1034: 	}
1035: #ifdef DEBUG
1036: 	for (idx_t i = 1; i < pos; i++) {
1037: 		auto prev_idx = sorted_sel.get_index(i - 1);
1038: 		auto idx = sorted_sel.get_index(i);
1039: 		D_ASSERT(ids[idx] > ids[prev_idx]);
1040: 	}
1041: #endif
1042: 
1043: 	sel.Initialize(sorted_sel);
1044: 	D_ASSERT(pos > 0);
1045: 	return pos;
1046: }
1047: 
1048: void UpdateSegment::Update(Transaction &transaction, idx_t column_index, Vector &update, row_t *ids, idx_t offset,
1049:                            idx_t count, Vector &base_data) {
1050: 	// obtain an exclusive lock
1051: 	auto write_lock = lock.GetExclusiveLock();
1052: 
1053: 	update.Normalify(count);
1054: 
1055: 	// update statistics
1056: 	SelectionVector sel;
1057: 	{
1058: 		lock_guard<mutex> stats_guard(stats_lock);
1059: 		count = statistics_update_function(this, stats, update, offset, count, sel);
1060: 	}
1061: 	if (count == 0) {
1062: 		return;
1063: 	}
1064: 
1065: 	// subsequent algorithms used by the update require row ids to be (1) sorted, and (2) unique
1066: 	// this is usually the case for "standard" queries (e.g. UPDATE tbl SET x=bla WHERE cond)
1067: 	// however, for more exotic queries involving e.g. cross products/joins this might not be the case
1068: 	// hence we explicitly check here if the ids are sorted and, if not, sort + duplicate eliminate them
1069: 	count = SortSelectionVector(sel, count, ids);
1070: 	D_ASSERT(count > 0);
1071: 
1072: 	// create the versions for this segment, if there are none yet
1073: 	if (!root) {
1074: 		root = make_unique<UpdateNode>();
1075: 	}
1076: 
1077: 	// get the vector index based on the first id
1078: 	// we assert that all updates must be part of the same vector
1079: 	auto first_id = ids[sel.get_index(0)];
1080: 	idx_t vector_index = (first_id - column_data.start) / STANDARD_VECTOR_SIZE;
1081: 	idx_t vector_offset = column_data.start + vector_index * STANDARD_VECTOR_SIZE;
1082: 
1083: 	D_ASSERT(idx_t(first_id) >= column_data.start);
1084: 	D_ASSERT(vector_index < RowGroup::ROW_GROUP_VECTOR_COUNT);
1085: 
1086: 	// first check the version chain
1087: 	UpdateInfo *node = nullptr;
1088: 
1089: 	if (root->info[vector_index]) {
1090: 		// there is already a version here, check if there are any conflicts and search for the node that belongs to
1091: 		// this transaction in the version chain
1092: 		auto base_info = root->info[vector_index]->info.get();
1093: 		CheckForConflicts(base_info->next, transaction, ids, count, vector_offset, node);
1094: 
1095: 		// there are no conflicts
1096: 		// first, check if this thread has already done any updates
1097: 		auto node = base_info->next;
1098: 		while (node) {
1099: 			if (node->version_number == transaction.transaction_id) {
1100: 				// it has! use this node
1101: 				break;
1102: 			}
1103: 			node = node->next;
1104: 		}
1105: 		if (!node) {
1106: 			// no updates made yet by this transaction: initially the update info to empty
1107: 			node = transaction.CreateUpdateInfo(type_size, count);
1108: 			node->segment = this;
1109: 			node->vector_index = vector_index;
1110: 			node->N = 0;
1111: 			node->column_index = column_index;
1112: 
1113: 			// insert the new node into the chain
1114: 			node->next = base_info->next;
1115: 			if (node->next) {
1116: 				node->next->prev = node;
1117: 			}
1118: 			node->prev = base_info;
1119: 			base_info->next = node;
1120: 		}
1121: 		base_info->Verify();
1122: 		node->Verify();
1123: 
1124: 		// now we are going to perform the merge
1125: 		merge_update_function(base_info, base_data, node, update, ids, count, sel);
1126: 
1127: 		base_info->Verify();
1128: 		node->Verify();
1129: 	} else {
1130: 		// there is no version info yet: create the top level update info and fill it with the updates
1131: 		auto result = make_unique<UpdateNodeData>();
1132: 
1133: 		result->info = make_unique<UpdateInfo>();
1134: 		result->tuples = unique_ptr<sel_t[]>(new sel_t[STANDARD_VECTOR_SIZE]);
1135: 		result->tuple_data = unique_ptr<data_t[]>(new data_t[STANDARD_VECTOR_SIZE * type_size]);
1136: 		result->info->tuples = result->tuples.get();
1137: 		result->info->tuple_data = result->tuple_data.get();
1138: 		result->info->version_number = TRANSACTION_ID_START - 1;
1139: 		result->info->column_index = column_index;
1140: 		InitializeUpdateInfo(*result->info, ids, sel, count, vector_index, vector_offset);
1141: 
1142: 		// now create the transaction level update info in the undo log
1143: 		auto transaction_node = transaction.CreateUpdateInfo(type_size, count);
1144: 		InitializeUpdateInfo(*transaction_node, ids, sel, count, vector_index, vector_offset);
1145: 
1146: 		// we write the updates in the
1147: 		initialize_update_function(transaction_node, base_data, result->info.get(), update, sel);
1148: 
1149: 		result->info->next = transaction_node;
1150: 		result->info->prev = nullptr;
1151: 		transaction_node->next = nullptr;
1152: 		transaction_node->prev = result->info.get();
1153: 		transaction_node->column_index = column_index;
1154: 
1155: 		transaction_node->Verify();
1156: 		result->info->Verify();
1157: 
1158: 		root->info[vector_index] = move(result);
1159: 	}
1160: }
1161: 
1162: bool UpdateSegment::HasUpdates() const {
1163: 	return root.get() != nullptr;
1164: }
1165: 
1166: bool UpdateSegment::HasUpdates(idx_t vector_index) const {
1167: 	if (!HasUpdates()) {
1168: 		return false;
1169: 	}
1170: 	return root->info[vector_index].get();
1171: }
1172: 
1173: bool UpdateSegment::HasUncommittedUpdates(idx_t vector_index) {
1174: 	if (!HasUpdates(vector_index)) {
1175: 		return false;
1176: 	}
1177: 	auto read_lock = lock.GetSharedLock();
1178: 	auto entry = root->info[vector_index].get();
1179: 	if (entry->info->next) {
1180: 		return true;
1181: 	}
1182: 	return false;
1183: }
1184: 
1185: bool UpdateSegment::HasUpdates(idx_t start_row_index, idx_t end_row_index) {
1186: 	if (!HasUpdates()) {
1187: 		return false;
1188: 	}
1189: 	auto read_lock = lock.GetSharedLock();
1190: 	idx_t base_vector_index = start_row_index / STANDARD_VECTOR_SIZE;
1191: 	idx_t end_vector_index = end_row_index / STANDARD_VECTOR_SIZE;
1192: 	for (idx_t i = base_vector_index; i <= end_vector_index; i++) {
1193: 		if (root->info[i]) {
1194: 			return true;
1195: 		}
1196: 	}
1197: 	return false;
1198: }
1199: 
1200: } // namespace duckdb
[end of src/storage/table/update_segment.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: