You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
IsFlushed() assertion fail
### What happens?

Assertion is triggered, see repro.

### To Reproduce

```
~/git-sandbox/duckdb (main)$ ./build/duckdb foodb
v1.0.1-dev3542 22040decf7
Enter ".help" for usage hints.
D create table foo  (d double);
D begin;
D insert into foo select random() from generate_series(1, 1000000) g(g);
D delete from foo;
D commit;
terminate called after throwing an instance of 'duckdb::InternalException'
  what():  {"exception_type":"INTERNAL","exception_message":"Assertion triggered in file \"/home/heikki/git-sandbox/duckdb/src/storage/table/column_checkpoint_state.cpp\" on line 32: IsFlushed() || Exception::UncaughtException()"}
Aborted
```

Backtrace:
```
(rr) bt
#0  0x00007f5fe96a6b0c in ?? () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007f5fe96584e2 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#2  0x00007f5fe96414ed in abort () from /lib/x86_64-linux-gnu/libc.so.6
#3  0x00007f5fe98a1a3d in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6
#4  0x00007f5fe98b2f9a in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00007f5fe98a14e3 in __cxa_call_terminate () from /lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00007f5fe98b2830 in __gxx_personality_v0 () from /lib/x86_64-linux-gnu/libstdc++.so.6
#7  0x00007f5fea3450d9 in ?? () from /lib/x86_64-linux-gnu/libgcc_s.so.1
#8  0x00007f5fea3457b1 in _Unwind_RaiseException () from /lib/x86_64-linux-gnu/libgcc_s.so.1
#9  0x00007f5fe98b320b in __cxa_throw () from /lib/x86_64-linux-gnu/libstdc++.so.6
#10 0x000055d2c67d0a7a in duckdb::DuckDBAssertInternal (condition=false, condition_name=0x55d2cfa28ac0 "IsFlushed() || Exception::UncaughtException()", 
    file=0x55d2cfa289e0 "/home/heikki/git-sandbox/duckdb/src/storage/table/column_checkpoint_state.cpp", linenr=32) at /home/heikki/git-sandbox/duckdb/src/common/assert.cpp:14
#11 0x000055d2c85002cc in duckdb::PartialBlockForCheckpoint::~PartialBlockForCheckpoint (this=0x50b000105de0, __in_chrg=<optimized out>)
    at /home/heikki/git-sandbox/duckdb/src/storage/table/column_checkpoint_state.cpp:32
#12 0x000055d2c85004e4 in duckdb::PartialBlockForCheckpoint::~PartialBlockForCheckpoint (this=0x50b000105de0, __in_chrg=<optimized out>)
    at /home/heikki/git-sandbox/duckdb/src/storage/table/column_checkpoint_state.cpp:33
#13 0x000055d2c71c5dbb in std::default_delete<duckdb::PartialBlock>::operator() (this=0x5040000a8df8, __ptr=0x50b000105de0) at /usr/include/c++/12/bits/unique_ptr.h:95
#14 0x000055d2c71be6d7 in std::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock> >::~unique_ptr (this=0x5040000a8df8, __in_chrg=<optimized out>)
    at /usr/include/c++/12/bits/unique_ptr.h:396
#15 0x000055d2c71b885d in duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true>::~unique_ptr (this=0x5040000a8df8, __in_chrg=<optimized out>)
    at /home/heikki/git-sandbox/duckdb/src/include/duckdb/common/unique_ptr.hpp:13
#16 0x000055d2c896ef13 in std::pair<unsigned long const, duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true> >::~pair (this=0x5040000a8df0, 
    __in_chrg=<optimized out>) at /usr/include/c++/12/bits/stl_pair.h:185
#17 0x000055d2c896ef57 in std::__new_allocator<std::_Rb_tree_node<std::pair<unsigned long const, duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true> > > >::destroy<std::pair<unsigned long const, duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true> > > (this=0x50f000000ad0, __p=0x5040000a8df0)
    at /usr/include/c++/12/bits/new_allocator.h:181
#18 0x000055d2c895c713 in std::allocator_traits<std::allocator<std::_Rb_tree_node<std::pair<unsigned long const, duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true> > > > >::destroy<std::pair<unsigned long const, duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true> > > (__a=..., __p=0x5040000a8df0)
    at /usr/include/c++/12/bits/alloc_traits.h:535
#19 0x000055d2c89440cd in std::_Rb_tree<unsigned long, std::pair<unsigned long const, duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true> >, std::_Select1st<std::pair<unsigned long const, duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true> > >, std::less<unsigned long>, std::allocator<std::pair<unsigned long const, duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true> > > >::_M_destroy_node (this=0x50f000000ad0, __p=0x5040000a8dd0)
    at /usr/include/c++/12/bits/stl_tree.h:623
#20 0x000055d2c8924e1f in std::_Rb_tree<unsigned long, std::pair<unsigned long const, duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true> >, std::_Select1st<std::pair<unsigned long const, duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true> > >, std::less<unsigned long>, std::allocator<std::pair<unsigned long const, duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true> > > >::_M_drop_node (this=0x50f000000ad0, __p=0x5040000a8dd0)
    at /usr/include/c++/12/bits/stl_tree.h:631
#21 0x000055d2c88fa732 in std::_Rb_tree<unsigned long, std::pair<unsigned long const, duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true> >, std::_Select1st<std::pair<unsigned long const, duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true> > >, std::less<unsigned long>, std::allocator<std::pair<unsigned long const, duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true> > > >::_M_erase (this=0x50f000000ad0, __x=0x5040000a8dd0)
    at /usr/include/c++/12/bits/stl_tree.h:1937
#22 0x000055d2c88c47fe in std::_Rb_tree<unsigned long, std::pair<unsigned long const, duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true> >, std::_Select1st<std::pair<unsigned long const, duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true> > >, std::less<unsigned long>, std::allocator<std::pair<unsigned long const, duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true> > > >::~_Rb_tree (this=0x50f000000ad0, __in_chrg=<optimized out>)
    at /usr/include/c++/12/bits/stl_tree.h:984
#23 0x000055d2c889498f in std::multimap<unsigned long, duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true>, std::less<unsigned long>, std::allocator<std::pair<unsigned long const, duckdb::unique_ptr<duckdb::PartialBlock, std::default_delete<duckdb::PartialBlock>, true> > > >::~multimap (this=0x50f000000ad0, __in_chrg=<optimized out>)
    at /usr/include/c++/12/bits/stl_multimap.h:301
--Type <RET> for more, q to quit, c to continue without paging--
#24 0x000055d2c8834f3f in duckdb::PartialBlockManager::~PartialBlockManager (this=0x50f000000a90, __in_chrg=<optimized out>)
    at /home/heikki/git-sandbox/duckdb/src/storage/partial_block_manager.cpp:50
#25 0x000055d2c8834fae in duckdb::PartialBlockManager::~PartialBlockManager (this=0x50f000000a90, __in_chrg=<optimized out>)
    at /home/heikki/git-sandbox/duckdb/src/storage/partial_block_manager.cpp:50
#26 0x000055d2c88f8075 in std::default_delete<duckdb::PartialBlockManager>::operator() (this=0x5100000027d0, __ptr=0x50f000000a90) at /usr/include/c++/12/bits/unique_ptr.h:95
#27 0x000055d2c88c35f5 in std::unique_ptr<duckdb::PartialBlockManager, std::default_delete<duckdb::PartialBlockManager> >::~unique_ptr (this=0x5100000027d0, __in_chrg=<optimized out>)
    at /usr/include/c++/12/bits/unique_ptr.h:396
#28 0x000055d2c8894425 in duckdb::unique_ptr<duckdb::PartialBlockManager, std::default_delete<duckdb::PartialBlockManager>, true>::~unique_ptr (this=0x5100000027d0, 
    __in_chrg=<optimized out>) at /home/heikki/git-sandbox/duckdb/src/include/duckdb/common/unique_ptr.hpp:13
#29 0x000055d2c8830ff1 in duckdb::OptimisticDataWriter::~OptimisticDataWriter (this=0x5100000027c8, __in_chrg=<optimized out>)
    at /home/heikki/git-sandbox/duckdb/src/storage/optimistic_data_writer.cpp:18
#30 0x000055d2c87e4757 in duckdb::LocalTableStorage::~LocalTableStorage (this=0x510000002750, __in_chrg=<optimized out>) at /home/heikki/git-sandbox/duckdb/src/storage/local_storage.cpp:72
#31 0x000055d2c898d035 in std::_Destroy<duckdb::LocalTableStorage> (__pointer=0x510000002750) at /usr/include/c++/12/bits/stl_construct.h:151
#32 0x000055d2c898cef9 in std::allocator_traits<std::allocator<void> >::destroy<duckdb::LocalTableStorage> (__p=0x510000002750) at /usr/include/c++/12/bits/alloc_traits.h:648
#33 0x000055d2c898bf6b in std::_Sp_counted_ptr_inplace<duckdb::LocalTableStorage, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose (this=0x510000002740)
    at /usr/include/c++/12/bits/shared_ptr_base.h:613
#34 0x000055d2c5aac50d in std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release_last_use (this=0x510000002740) at /usr/include/c++/12/bits/shared_ptr_base.h:175
#35 0x000055d2c5aac35c in std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release_last_use_cold (this=0x510000002740) at /usr/include/c++/12/bits/shared_ptr_base.h:199
#36 0x000055d2c5aa3706 in std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release (this=0x510000002740) at /usr/include/c++/12/bits/shared_ptr_base.h:353
#37 0x000055d2c5aacaac in std::__shared_count<(__gnu_cxx::_Lock_policy)2>::~__shared_count (this=0x7f5fe7064f68, __in_chrg=<optimized out>)
    at /usr/include/c++/12/bits/shared_ptr_base.h:1071
#38 0x000055d2c888a2d7 in std::__shared_ptr<duckdb::LocalTableStorage, (__gnu_cxx::_Lock_policy)2>::~__shared_ptr (this=0x7f5fe7064f60, __in_chrg=<optimized out>)
    at /usr/include/c++/12/bits/shared_ptr_base.h:1524
#39 0x000055d2c88e3d39 in std::__shared_ptr<duckdb::LocalTableStorage, (__gnu_cxx::_Lock_policy)2>::reset (this=0x50400008fce0) at /usr/include/c++/12/bits/shared_ptr_base.h:1642
#40 0x000055d2c88b2677 in duckdb::shared_ptr<duckdb::LocalTableStorage, true>::reset (this=0x50400008fce0)
    at /home/heikki/git-sandbox/duckdb/src/include/duckdb/common/shared_ptr_ipp.hpp:159
#41 0x000055d2c87f04cc in duckdb::LocalStorage::Commit (this=0x50b00003af80) at /home/heikki/git-sandbox/duckdb/src/storage/local_storage.cpp:487
#42 0x000055d2c899f0b5 in duckdb::DuckTransaction::WriteToWAL (this=0x5120000694c0, db=..., commit_state=...) at /home/heikki/git-sandbox/duckdb/src/transaction/duck_transaction.cpp:188
#43 0x000055d2c8994dd5 in duckdb::DuckTransactionManager::CommitTransaction (this=0x51200003cdc0, context=..., transaction_p=...)
    at /home/heikki/git-sandbox/duckdb/src/transaction/duck_transaction_manager.cpp:242
#44 0x000055d2c89a4c23 in duckdb::MetaTransaction::Commit (this=0x511000045640) at /home/heikki/git-sandbox/duckdb/src/transaction/meta_transaction.cpp:124
#45 0x000055d2c89aa50b in duckdb::TransactionContext::Commit (this=0x51600004f798) at /home/heikki/git-sandbox/duckdb/src/transaction/transaction_context.cpp:46
#46 0x000055d2cb7a3feb in duckdb::PhysicalTransaction::GetData (this=0x50d000094180, context=..., chunk=..., input=...)
    at /home/heikki/git-sandbox/duckdb/src/execution/operator/helper/physical_transaction.cpp:52
#47 0x000055d2c7bba066 in duckdb::PipelineExecutor::GetData (this=0x51500136eb80, chunk=..., input=...) at /home/heikki/git-sandbox/duckdb/src/parallel/pipeline_executor.cpp:466
#48 0x000055d2c7bba82a in duckdb::PipelineExecutor::FetchFromSource (this=0x51500136eb80, result=...) at /home/heikki/git-sandbox/duckdb/src/parallel/pipeline_executor.cpp:492
#49 0x000055d2c7bb576f in duckdb::PipelineExecutor::Execute (this=0x51500136eb80, max_chunks=50) at /home/heikki/git-sandbox/duckdb/src/parallel/pipeline_executor.cpp:203
#50 0x000055d2c7ba12e1 in duckdb::PipelineTask::ExecuteTask (this=0x5060002ffd80, mode=duckdb::TaskExecutionMode::PROCESS_PARTIAL)
    at /home/heikki/git-sandbox/duckdb/src/parallel/pipeline.cpp:40
#51 0x000055d2c7b8644b in duckdb::ExecutorTask::Execute (this=0x5060002ffd80, mode=duckdb::TaskExecutionMode::PROCESS_PARTIAL)
    at /home/heikki/git-sandbox/duckdb/src/parallel/executor_task.cpp:32
#52 0x000055d2c7b97f3a in duckdb::Executor::ExecuteTask (this=0x51400004a040, dry_run=false) at /home/heikki/git-sandbox/duckdb/src/parallel/executor.cpp:575
#53 0x000055d2c7692bb2 in duckdb::ClientContext::ExecuteTaskInternal (this=0x51600004f590, lock=..., result=..., dry_run=false)
    at /home/heikki/git-sandbox/duckdb/src/main/client_context.cpp:556
#54 0x000055d2c76f1611 in duckdb::PendingQueryResult::ExecuteTaskInternal (this=0x51300000b800, lock=...) at /home/heikki/git-sandbox/duckdb/src/main/pending_query_result.cpp:68
#55 0x000055d2c76f1b84 in duckdb::PendingQueryResult::ExecuteInternal (this=0x51300000b800, lock=...) at /home/heikki/git-sandbox/duckdb/src/main/pending_query_result.cpp:75
#56 0x000055d2c76f2464 in duckdb::PendingQueryResult::Execute (this=0x51300000b800) at /home/heikki/git-sandbox/duckdb/src/main/pending_query_result.cpp:95
#57 0x000055d2c76f528d in duckdb::PreparedStatement::Execute (this=0x512000109540, values=..., allow_stream_result=false)
--Type <RET> for more, q to quit, c to continue without paging--
    at /home/heikki/git-sandbox/duckdb/src/main/prepared_statement.cpp:85
#58 0x000055d2c5a8e9cd in duckdb_shell_sqlite3_print_duckbox (pStmt=0x50c000001480, max_rows=40, max_width=0, null_value=0x7f5fe770015c "", columnar=0)
    at /home/heikki/git-sandbox/duckdb/tools/sqlite3_api_wrapper/sqlite3_api_wrapper.cpp:249
#59 0x000055d2c5a1585c in exec_prepared_stmt (pArg=0x7f5fe7700040, pStmt=0x50c000001480) at /home/heikki/git-sandbox/duckdb/tools/shell/shell.c:12750
#60 0x000055d2c5984d7e in shell_exec (pArg=0x7f5fe7700040, zSql=0x50d000000380 "commit;", pzErrMsg=0x7f5fe7040460) at /home/heikki/git-sandbox/duckdb/tools/shell/shell.c:13085
#61 0x000055d2c5a21e16 in runOneSqlLine (p=0x7f5fe7700040, zSql=0x50d000000380 "commit;", in=0x0, startline=5) at /home/heikki/git-sandbox/duckdb/tools/shell/shell.c:19271
#62 0x000055d2c59887b2 in process_input (p=0x7f5fe7700040) at /home/heikki/git-sandbox/duckdb/tools/shell/shell.c:19382
#63 0x000055d2c594e9b0 in main (argc=2, argv=0x7ffec14ba958) at /home/heikki/git-sandbox/duckdb/tools/shell/shell.c:20204
```


### OS:

Debian, x64

### DuckDB Version:

Built from sources, commit 22040decf7ad0d87b449a87e6cb805978e34a5c1

### DuckDB Client:

duckdb's own CLI

### Full Name:

Heikki Linnakangas

### Affiliation:

Neon

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a source build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://www.duckdb.org/docs/installation) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/storage/local_storage.cpp]
1: #include "duckdb/transaction/local_storage.hpp"
2: #include "duckdb/execution/index/art/art.hpp"
3: #include "duckdb/storage/table/append_state.hpp"
4: #include "duckdb/storage/write_ahead_log.hpp"
5: #include "duckdb/common/vector_operations/vector_operations.hpp"
6: #include "duckdb/storage/table/row_group.hpp"
7: #include "duckdb/transaction/duck_transaction.hpp"
8: #include "duckdb/planner/table_filter.hpp"
9: #include "duckdb/storage/partial_block_manager.hpp"
10: 
11: #include "duckdb/storage/table/column_checkpoint_state.hpp"
12: #include "duckdb/storage/table_io_manager.hpp"
13: #include "duckdb/storage/table/scan_state.hpp"
14: 
15: namespace duckdb {
16: 
17: LocalTableStorage::LocalTableStorage(ClientContext &context, DataTable &table)
18:     : table_ref(table), allocator(Allocator::Get(table.db)), deleted_rows(0), optimistic_writer(table),
19:       merged_storage(false) {
20: 	auto types = table.GetTypes();
21: 	auto data_table_info = table.GetDataTableInfo();
22: 	row_groups = make_shared_ptr<RowGroupCollection>(
23: 	    data_table_info, TableIOManager::Get(table).GetBlockManagerForRowData(), types, MAX_ROW_ID, 0);
24: 	row_groups->InitializeEmpty();
25: 
26: 	data_table_info->GetIndexes().BindAndScan<ART>(context, *data_table_info, [&](ART &art) {
27: 		if (art.GetConstraintType() != IndexConstraintType::NONE) {
28: 			// unique index: create a local ART index that maintains the same unique constraint
29: 			vector<unique_ptr<Expression>> unbound_expressions;
30: 			unbound_expressions.reserve(art.unbound_expressions.size());
31: 			for (auto &expr : art.unbound_expressions) {
32: 				unbound_expressions.push_back(expr->Copy());
33: 			}
34: 			indexes.AddIndex(make_uniq<ART>(art.GetIndexName(), art.GetConstraintType(), art.GetColumnIds(),
35: 			                                art.table_io_manager, std::move(unbound_expressions), art.db));
36: 		}
37: 		return false;
38: 	});
39: }
40: 
41: LocalTableStorage::LocalTableStorage(ClientContext &context, DataTable &new_dt, LocalTableStorage &parent,
42:                                      idx_t changed_idx, const LogicalType &target_type,
43:                                      const vector<column_t> &bound_columns, Expression &cast_expr)
44:     : table_ref(new_dt), allocator(Allocator::Get(new_dt.db)), deleted_rows(parent.deleted_rows),
45:       optimistic_writer(new_dt, parent.optimistic_writer), optimistic_writers(std::move(parent.optimistic_writers)),
46:       merged_storage(parent.merged_storage) {
47: 	row_groups = parent.row_groups->AlterType(context, changed_idx, target_type, bound_columns, cast_expr);
48: 	parent.row_groups.reset();
49: 	indexes.Move(parent.indexes);
50: }
51: 
52: LocalTableStorage::LocalTableStorage(DataTable &new_dt, LocalTableStorage &parent, idx_t drop_idx)
53:     : table_ref(new_dt), allocator(Allocator::Get(new_dt.db)), deleted_rows(parent.deleted_rows),
54:       optimistic_writer(new_dt, parent.optimistic_writer), optimistic_writers(std::move(parent.optimistic_writers)),
55:       merged_storage(parent.merged_storage) {
56: 	row_groups = parent.row_groups->RemoveColumn(drop_idx);
57: 	parent.row_groups.reset();
58: 	indexes.Move(parent.indexes);
59: }
60: 
61: LocalTableStorage::LocalTableStorage(ClientContext &context, DataTable &new_dt, LocalTableStorage &parent,
62:                                      ColumnDefinition &new_column, ExpressionExecutor &default_executor)
63:     : table_ref(new_dt), allocator(Allocator::Get(new_dt.db)), deleted_rows(parent.deleted_rows),
64:       optimistic_writer(new_dt, parent.optimistic_writer), optimistic_writers(std::move(parent.optimistic_writers)),
65:       merged_storage(parent.merged_storage) {
66: 	row_groups = parent.row_groups->AddColumn(context, new_column, default_executor);
67: 	parent.row_groups.reset();
68: 	indexes.Move(parent.indexes);
69: }
70: 
71: LocalTableStorage::~LocalTableStorage() {
72: }
73: 
74: void LocalTableStorage::InitializeScan(CollectionScanState &state, optional_ptr<TableFilterSet> table_filters) {
75: 	if (row_groups->GetTotalRows() == 0) {
76: 		throw InternalException("No rows in LocalTableStorage row group for scan");
77: 	}
78: 	row_groups->InitializeScan(state, state.GetColumnIds(), table_filters.get());
79: }
80: 
81: idx_t LocalTableStorage::EstimatedSize() {
82: 	// count the appended rows
83: 	idx_t appended_rows = row_groups->GetTotalRows() - deleted_rows;
84: 
85: 	// get the (estimated) size of a row (no compressions, etc.)
86: 	idx_t row_size = 0;
87: 	auto &types = row_groups->GetTypes();
88: 	for (auto &type : types) {
89: 		row_size += GetTypeIdSize(type.InternalType());
90: 	}
91: 
92: 	// get the index size
93: 	idx_t index_sizes = 0;
94: 	indexes.Scan([&](Index &index) {
95: 		D_ASSERT(index.IsBound());
96: 		index_sizes += index.Cast<BoundIndex>().GetInMemorySize();
97: 		return false;
98: 	});
99: 
100: 	// return the size of the appended rows and the index size
101: 	return appended_rows * row_size + index_sizes;
102: }
103: 
104: void LocalTableStorage::WriteNewRowGroup() {
105: 	if (deleted_rows != 0) {
106: 		// we have deletes - we cannot merge row groups
107: 		return;
108: 	}
109: 	optimistic_writer.WriteNewRowGroup(*row_groups);
110: }
111: 
112: void LocalTableStorage::FlushBlocks() {
113: 	if (!merged_storage && row_groups->GetTotalRows() > Storage::ROW_GROUP_SIZE) {
114: 		optimistic_writer.WriteLastRowGroup(*row_groups);
115: 	}
116: 	optimistic_writer.FinalFlush();
117: }
118: 
119: ErrorData LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, RowGroupCollection &source,
120:                                              TableIndexList &index_list, const vector<LogicalType> &table_types,
121:                                              row_t &start_row) {
122: 	// only need to scan for index append
123: 	// figure out which columns we need to scan for the set of indexes
124: 	auto columns = index_list.GetRequiredColumns();
125: 	// create an empty mock chunk that contains all the correct types for the table
126: 	DataChunk mock_chunk;
127: 	mock_chunk.InitializeEmpty(table_types);
128: 	ErrorData error;
129: 	source.Scan(transaction, columns, [&](DataChunk &chunk) -> bool {
130: 		// construct the mock chunk by referencing the required columns
131: 		for (idx_t i = 0; i < columns.size(); i++) {
132: 			mock_chunk.data[columns[i]].Reference(chunk.data[i]);
133: 		}
134: 		mock_chunk.SetCardinality(chunk);
135: 		// append this chunk to the indexes of the table
136: 		error = DataTable::AppendToIndexes(index_list, mock_chunk, start_row);
137: 		if (error.HasError()) {
138: 			return false;
139: 		}
140: 		start_row += UnsafeNumericCast<row_t>(chunk.size());
141: 		return true;
142: 	});
143: 	return error;
144: }
145: 
146: void LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, TableAppendState &append_state,
147:                                         idx_t append_count, bool append_to_table) {
148: 	auto &table = table_ref.get();
149: 	if (append_to_table) {
150: 		table.InitializeAppend(transaction, append_state);
151: 	}
152: 	ErrorData error;
153: 	if (append_to_table) {
154: 		// appending: need to scan entire
155: 		row_groups->Scan(transaction, [&](DataChunk &chunk) -> bool {
156: 			// append this chunk to the indexes of the table
157: 			error = table.AppendToIndexes(chunk, append_state.current_row);
158: 			if (error.HasError()) {
159: 				return false;
160: 			}
161: 			// append to base table
162: 			table.Append(chunk, append_state);
163: 			return true;
164: 		});
165: 	} else {
166: 		auto data_table_info = table.GetDataTableInfo();
167: 		auto &index_list = data_table_info->GetIndexes();
168: 		error = AppendToIndexes(transaction, *row_groups, index_list, table.GetTypes(), append_state.current_row);
169: 	}
170: 	if (error.HasError()) {
171: 		// need to revert all appended row ids
172: 		row_t current_row = append_state.row_start;
173: 		// remove the data from the indexes, if there are any indexes
174: 		row_groups->Scan(transaction, [&](DataChunk &chunk) -> bool {
175: 			// append this chunk to the indexes of the table
176: 			try {
177: 				table.RemoveFromIndexes(append_state, chunk, current_row);
178: 			} catch (std::exception &ex) { // LCOV_EXCL_START
179: 				error = ErrorData(ex);
180: 				return false;
181: 			} // LCOV_EXCL_STOP
182: 
183: 			current_row += UnsafeNumericCast<row_t>(chunk.size());
184: 			if (current_row >= append_state.current_row) {
185: 				// finished deleting all rows from the index: abort now
186: 				return false;
187: 			}
188: 			return true;
189: 		});
190: 		if (append_to_table) {
191: 			table.RevertAppendInternal(NumericCast<idx_t>(append_state.row_start));
192: 		}
193: 
194: 		// we need to vacuum the indexes to remove any buffers that are now empty
195: 		// due to reverting the appends
196: 		table.VacuumIndexes();
197: 		error.Throw();
198: 	}
199: 	if (append_to_table) {
200: 		table.FinalizeAppend(transaction, append_state);
201: 	}
202: }
203: 
204: OptimisticDataWriter &LocalTableStorage::CreateOptimisticWriter() {
205: 	auto writer = make_uniq<OptimisticDataWriter>(table_ref.get());
206: 	optimistic_writers.push_back(std::move(writer));
207: 	return *optimistic_writers.back();
208: }
209: 
210: void LocalTableStorage::FinalizeOptimisticWriter(OptimisticDataWriter &writer) {
211: 	// remove the writer from the set of optimistic writers
212: 	unique_ptr<OptimisticDataWriter> owned_writer;
213: 	for (idx_t i = 0; i < optimistic_writers.size(); i++) {
214: 		if (optimistic_writers[i].get() == &writer) {
215: 			owned_writer = std::move(optimistic_writers[i]);
216: 			optimistic_writers.erase_at(i);
217: 			break;
218: 		}
219: 	}
220: 	if (!owned_writer) {
221: 		throw InternalException("Error in FinalizeOptimisticWriter - could not find writer");
222: 	}
223: 	optimistic_writer.Merge(*owned_writer);
224: }
225: 
226: void LocalTableStorage::Rollback() {
227: 	for (auto &writer : optimistic_writers) {
228: 		writer->Rollback();
229: 	}
230: 	optimistic_writers.clear();
231: 	optimistic_writer.Rollback();
232: }
233: 
234: //===--------------------------------------------------------------------===//
235: // LocalTableManager
236: //===--------------------------------------------------------------------===//
237: optional_ptr<LocalTableStorage> LocalTableManager::GetStorage(DataTable &table) {
238: 	lock_guard<mutex> l(table_storage_lock);
239: 	auto entry = table_storage.find(table);
240: 	return entry == table_storage.end() ? nullptr : entry->second.get();
241: }
242: 
243: LocalTableStorage &LocalTableManager::GetOrCreateStorage(ClientContext &context, DataTable &table) {
244: 	lock_guard<mutex> l(table_storage_lock);
245: 	auto entry = table_storage.find(table);
246: 	if (entry == table_storage.end()) {
247: 		auto new_storage = make_shared_ptr<LocalTableStorage>(context, table);
248: 		auto storage = new_storage.get();
249: 		table_storage.insert(make_pair(reference<DataTable>(table), std::move(new_storage)));
250: 		return *storage;
251: 	} else {
252: 		return *entry->second.get();
253: 	}
254: }
255: 
256: bool LocalTableManager::IsEmpty() {
257: 	lock_guard<mutex> l(table_storage_lock);
258: 	return table_storage.empty();
259: }
260: 
261: shared_ptr<LocalTableStorage> LocalTableManager::MoveEntry(DataTable &table) {
262: 	lock_guard<mutex> l(table_storage_lock);
263: 	auto entry = table_storage.find(table);
264: 	if (entry == table_storage.end()) {
265: 		return nullptr;
266: 	}
267: 	auto storage_entry = std::move(entry->second);
268: 	table_storage.erase(entry);
269: 	return storage_entry;
270: }
271: 
272: reference_map_t<DataTable, shared_ptr<LocalTableStorage>> LocalTableManager::MoveEntries() {
273: 	lock_guard<mutex> l(table_storage_lock);
274: 	return std::move(table_storage);
275: }
276: 
277: idx_t LocalTableManager::EstimatedSize() {
278: 	lock_guard<mutex> l(table_storage_lock);
279: 	idx_t estimated_size = 0;
280: 	for (auto &storage : table_storage) {
281: 		estimated_size += storage.second->EstimatedSize();
282: 	}
283: 	return estimated_size;
284: }
285: 
286: void LocalTableManager::InsertEntry(DataTable &table, shared_ptr<LocalTableStorage> entry) {
287: 	lock_guard<mutex> l(table_storage_lock);
288: 	D_ASSERT(table_storage.find(table) == table_storage.end());
289: 	table_storage[table] = std::move(entry);
290: }
291: 
292: //===--------------------------------------------------------------------===//
293: // LocalStorage
294: //===--------------------------------------------------------------------===//
295: LocalStorage::LocalStorage(ClientContext &context, DuckTransaction &transaction)
296:     : context(context), transaction(transaction) {
297: }
298: 
299: LocalStorage::CommitState::CommitState() {
300: }
301: 
302: LocalStorage::CommitState::~CommitState() {
303: }
304: 
305: LocalStorage &LocalStorage::Get(DuckTransaction &transaction) {
306: 	return transaction.GetLocalStorage();
307: }
308: 
309: LocalStorage &LocalStorage::Get(ClientContext &context, AttachedDatabase &db) {
310: 	return DuckTransaction::Get(context, db).GetLocalStorage();
311: }
312: 
313: LocalStorage &LocalStorage::Get(ClientContext &context, Catalog &catalog) {
314: 	return LocalStorage::Get(context, catalog.GetAttached());
315: }
316: 
317: void LocalStorage::InitializeScan(DataTable &table, CollectionScanState &state,
318:                                   optional_ptr<TableFilterSet> table_filters) {
319: 	auto storage = table_manager.GetStorage(table);
320: 	if (storage == nullptr) {
321: 		return;
322: 	}
323: 	storage->InitializeScan(state, table_filters);
324: }
325: 
326: void LocalStorage::Scan(CollectionScanState &state, const vector<storage_t> &column_ids, DataChunk &result) {
327: 	state.Scan(transaction, result);
328: }
329: 
330: void LocalStorage::InitializeParallelScan(DataTable &table, ParallelCollectionScanState &state) {
331: 	auto storage = table_manager.GetStorage(table);
332: 	if (!storage) {
333: 		state.max_row = 0;
334: 		state.vector_index = 0;
335: 		state.current_row_group = nullptr;
336: 	} else {
337: 		storage->row_groups->InitializeParallelScan(state);
338: 	}
339: }
340: 
341: bool LocalStorage::NextParallelScan(ClientContext &context, DataTable &table, ParallelCollectionScanState &state,
342:                                     CollectionScanState &scan_state) {
343: 	auto storage = table_manager.GetStorage(table);
344: 	if (!storage) {
345: 		return false;
346: 	}
347: 	return storage->row_groups->NextParallelScan(context, state, scan_state);
348: }
349: 
350: void LocalStorage::InitializeAppend(LocalAppendState &state, DataTable &table) {
351: 	table.InitializeIndexes(context);
352: 	state.storage = &table_manager.GetOrCreateStorage(context, table);
353: 	state.storage->row_groups->InitializeAppend(TransactionData(transaction), state.append_state);
354: }
355: 
356: void LocalStorage::Append(LocalAppendState &state, DataChunk &chunk) {
357: 	// append to unique indices (if any)
358: 	auto storage = state.storage;
359: 	idx_t base_id =
360: 	    NumericCast<idx_t>(MAX_ROW_ID) + storage->row_groups->GetTotalRows() + state.append_state.total_append_count;
361: 	auto error = DataTable::AppendToIndexes(storage->indexes, chunk, NumericCast<row_t>(base_id));
362: 	if (error.HasError()) {
363: 		error.Throw();
364: 	}
365: 
366: 	//! Append the chunk to the local storage
367: 	auto new_row_group = storage->row_groups->Append(chunk, state.append_state);
368: 	//! Check if we should pre-emptively flush blocks to disk
369: 	if (new_row_group) {
370: 		storage->WriteNewRowGroup();
371: 	}
372: }
373: 
374: void LocalStorage::FinalizeAppend(LocalAppendState &state) {
375: 	state.storage->row_groups->FinalizeAppend(state.append_state.transaction, state.append_state);
376: }
377: 
378: void LocalStorage::LocalMerge(DataTable &table, RowGroupCollection &collection) {
379: 	auto &storage = table_manager.GetOrCreateStorage(context, table);
380: 	if (!storage.indexes.Empty()) {
381: 		// append data to indexes if required
382: 		row_t base_id = MAX_ROW_ID + NumericCast<row_t>(storage.row_groups->GetTotalRows());
383: 		auto error = storage.AppendToIndexes(transaction, collection, storage.indexes, table.GetTypes(), base_id);
384: 		if (error.HasError()) {
385: 			error.Throw();
386: 		}
387: 	}
388: 	storage.row_groups->MergeStorage(collection);
389: 	storage.merged_storage = true;
390: }
391: 
392: OptimisticDataWriter &LocalStorage::CreateOptimisticWriter(DataTable &table) {
393: 	auto &storage = table_manager.GetOrCreateStorage(context, table);
394: 	return storage.CreateOptimisticWriter();
395: }
396: 
397: void LocalStorage::FinalizeOptimisticWriter(DataTable &table, OptimisticDataWriter &writer) {
398: 	auto &storage = table_manager.GetOrCreateStorage(context, table);
399: 	storage.FinalizeOptimisticWriter(writer);
400: }
401: 
402: bool LocalStorage::ChangesMade() noexcept {
403: 	return !table_manager.IsEmpty();
404: }
405: 
406: bool LocalStorage::Find(DataTable &table) {
407: 	return table_manager.GetStorage(table) != nullptr;
408: }
409: 
410: idx_t LocalStorage::EstimatedSize() {
411: 	return table_manager.EstimatedSize();
412: }
413: 
414: idx_t LocalStorage::Delete(DataTable &table, Vector &row_ids, idx_t count) {
415: 	auto storage = table_manager.GetStorage(table);
416: 	D_ASSERT(storage);
417: 
418: 	// delete from unique indices (if any)
419: 	if (!storage->indexes.Empty()) {
420: 		storage->row_groups->RemoveFromIndexes(storage->indexes, row_ids, count);
421: 	}
422: 
423: 	auto ids = FlatVector::GetData<row_t>(row_ids);
424: 	idx_t delete_count = storage->row_groups->Delete(TransactionData(0, 0), table, ids, count);
425: 	storage->deleted_rows += delete_count;
426: 	return delete_count;
427: }
428: 
429: void LocalStorage::Update(DataTable &table, Vector &row_ids, const vector<PhysicalIndex> &column_ids,
430:                           DataChunk &updates) {
431: 	auto storage = table_manager.GetStorage(table);
432: 	D_ASSERT(storage);
433: 
434: 	auto ids = FlatVector::GetData<row_t>(row_ids);
435: 	storage->row_groups->Update(TransactionData(0, 0), ids, column_ids, updates);
436: }
437: 
438: void LocalStorage::Flush(DataTable &table, LocalTableStorage &storage) {
439: 	if (storage.is_dropped) {
440: 		return;
441: 	}
442: 	if (storage.row_groups->GetTotalRows() <= storage.deleted_rows) {
443: 		return;
444: 	}
445: 	idx_t append_count = storage.row_groups->GetTotalRows() - storage.deleted_rows;
446: 
447: 	table.InitializeIndexes(context);
448: 
449: 	TableAppendState append_state;
450: 	table.AppendLock(append_state);
451: 	transaction.PushAppend(table, NumericCast<idx_t>(append_state.row_start), append_count);
452: 	if ((append_state.row_start == 0 || storage.row_groups->GetTotalRows() >= MERGE_THRESHOLD) &&
453: 	    storage.deleted_rows == 0) {
454: 		// table is currently empty OR we are bulk appending: move over the storage directly
455: 		// first flush any outstanding blocks
456: 		storage.FlushBlocks();
457: 		// now append to the indexes (if there are any)
458: 		// FIXME: we should be able to merge the transaction-local index directly into the main table index
459: 		// as long we just rewrite some row-ids
460: 		if (table.HasIndexes()) {
461: 			storage.AppendToIndexes(transaction, append_state, append_count, false);
462: 		}
463: 		// finally move over the row groups
464: 		table.MergeStorage(*storage.row_groups, storage.indexes);
465: 	} else {
466: 		// check if we have written data
467: 		// if we have, we cannot merge to disk after all
468: 		// so we need to revert the data we have already written
469: 		storage.Rollback();
470: 		// append to the indexes and append to the base table
471: 		storage.AppendToIndexes(transaction, append_state, append_count, true);
472: 	}
473: 
474: 	// possibly vacuum any excess index data
475: 	table.VacuumIndexes();
476: }
477: 
478: void LocalStorage::Commit() {
479: 	// commit local storage
480: 	// iterate over all entries in the table storage map and commit them
481: 	// after this, the local storage is no longer required and can be cleared
482: 	auto table_storage = table_manager.MoveEntries();
483: 	for (auto &entry : table_storage) {
484: 		auto table = entry.first;
485: 		auto storage = entry.second.get();
486: 		Flush(table, *storage);
487: 		entry.second.reset();
488: 	}
489: }
490: 
491: void LocalStorage::Rollback() {
492: 	// rollback local storage
493: 	// after this, the local storage is no longer required and can be cleared
494: 	auto table_storage = table_manager.MoveEntries();
495: 	for (auto &entry : table_storage) {
496: 		auto storage = entry.second.get();
497: 		if (!storage) {
498: 			continue;
499: 		}
500: 		storage->Rollback();
501: 
502: 		entry.second.reset();
503: 	}
504: }
505: 
506: idx_t LocalStorage::AddedRows(DataTable &table) {
507: 	auto storage = table_manager.GetStorage(table);
508: 	if (!storage) {
509: 		return 0;
510: 	}
511: 	return storage->row_groups->GetTotalRows() - storage->deleted_rows;
512: }
513: 
514: void LocalStorage::DropTable(DataTable &table) {
515: 	auto storage = table_manager.GetStorage(table);
516: 	if (!storage) {
517: 		return;
518: 	}
519: 	storage->is_dropped = true;
520: }
521: 
522: void LocalStorage::MoveStorage(DataTable &old_dt, DataTable &new_dt) {
523: 	// check if there are any pending appends for the old version of the table
524: 	auto new_storage = table_manager.MoveEntry(old_dt);
525: 	if (!new_storage) {
526: 		return;
527: 	}
528: 	// take over the storage from the old entry
529: 	new_storage->table_ref = new_dt;
530: 	table_manager.InsertEntry(new_dt, std::move(new_storage));
531: }
532: 
533: void LocalStorage::AddColumn(DataTable &old_dt, DataTable &new_dt, ColumnDefinition &new_column,
534:                              ExpressionExecutor &default_executor) {
535: 	// check if there are any pending appends for the old version of the table
536: 	auto storage = table_manager.MoveEntry(old_dt);
537: 	if (!storage) {
538: 		return;
539: 	}
540: 	auto new_storage = make_shared_ptr<LocalTableStorage>(context, new_dt, *storage, new_column, default_executor);
541: 	table_manager.InsertEntry(new_dt, std::move(new_storage));
542: }
543: 
544: void LocalStorage::DropColumn(DataTable &old_dt, DataTable &new_dt, idx_t removed_column) {
545: 	// check if there are any pending appends for the old version of the table
546: 	auto storage = table_manager.MoveEntry(old_dt);
547: 	if (!storage) {
548: 		return;
549: 	}
550: 	auto new_storage = make_shared_ptr<LocalTableStorage>(new_dt, *storage, removed_column);
551: 	table_manager.InsertEntry(new_dt, std::move(new_storage));
552: }
553: 
554: void LocalStorage::ChangeType(DataTable &old_dt, DataTable &new_dt, idx_t changed_idx, const LogicalType &target_type,
555:                               const vector<column_t> &bound_columns, Expression &cast_expr) {
556: 	// check if there are any pending appends for the old version of the table
557: 	auto storage = table_manager.MoveEntry(old_dt);
558: 	if (!storage) {
559: 		return;
560: 	}
561: 	auto new_storage = make_shared_ptr<LocalTableStorage>(context, new_dt, *storage, changed_idx, target_type,
562: 	                                                      bound_columns, cast_expr);
563: 	table_manager.InsertEntry(new_dt, std::move(new_storage));
564: }
565: 
566: void LocalStorage::FetchChunk(DataTable &table, Vector &row_ids, idx_t count, const vector<column_t> &col_ids,
567:                               DataChunk &chunk, ColumnFetchState &fetch_state) {
568: 	auto storage = table_manager.GetStorage(table);
569: 	if (!storage) {
570: 		throw InternalException("LocalStorage::FetchChunk - local storage not found");
571: 	}
572: 
573: 	storage->row_groups->Fetch(transaction, chunk, col_ids, row_ids, count, fetch_state);
574: }
575: 
576: TableIndexList &LocalStorage::GetIndexes(DataTable &table) {
577: 	auto storage = table_manager.GetStorage(table);
578: 	if (!storage) {
579: 		throw InternalException("LocalStorage::GetIndexes - local storage not found");
580: 	}
581: 	return storage->indexes;
582: }
583: 
584: void LocalStorage::VerifyNewConstraint(DataTable &parent, const BoundConstraint &constraint) {
585: 	auto storage = table_manager.GetStorage(parent);
586: 	if (!storage) {
587: 		return;
588: 	}
589: 	storage->row_groups->VerifyNewConstraint(parent, constraint);
590: }
591: 
592: } // namespace duckdb
[end of src/storage/local_storage.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: