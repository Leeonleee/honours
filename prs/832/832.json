{
  "repo": "duckdb/duckdb",
  "pull_number": 832,
  "instance_id": "duckdb__duckdb-832",
  "issue_numbers": [
    "1"
  ],
  "base_commit": "c95b7154ed4794b25f09f6af9b7ec96e4362a8c1",
  "patch": "diff --git a/src/common/operator/cast_operators.cpp b/src/common/operator/cast_operators.cpp\nindex 47613c1fa438..e6b2b6586a46 100644\n--- a/src/common/operator/cast_operators.cpp\n+++ b/src/common/operator/cast_operators.cpp\n@@ -163,24 +163,23 @@ template <> float Cast::Operation(double input) {\n template <class T> static T try_cast_string(string_t input) {\n \tT result;\n \tif (!TryCast::Operation<string_t, T>(input, result)) {\n-\t\tthrow ConversionException(\"Could not convert string '%s' to %s\", input.GetData(), TypeIdToString(GetTypeId<T>()).c_str());\n+\t\tthrow ConversionException(\"Could not convert string '%s' to %s\", input.GetData(),\n+\t\t                          TypeIdToString(GetTypeId<T>()).c_str());\n \t}\n \treturn result;\n }\n \n-\n-\n template <class T> static T try_strict_cast_string(string_t input) {\n \tT result;\n \tif (!TryCast::Operation<string_t, T>(input, result, true)) {\n-\t\tthrow ConversionException(\"Could not convert string '%s' to %s\", input.GetData(), TypeIdToString(GetTypeId<T>()).c_str());\n+\t\tthrow ConversionException(\"Could not convert string '%s' to %s\", input.GetData(),\n+\t\t                          TypeIdToString(GetTypeId<T>()).c_str());\n \t}\n \treturn result;\n }\n \n struct IntegerCastOperation {\n-\ttemplate<class T, bool NEGATIVE>\n-\tstatic bool HandleDigit(T &result, uint8_t digit) {\n+\ttemplate <class T, bool NEGATIVE> static bool HandleDigit(T &result, uint8_t digit) {\n \t\tif (NEGATIVE) {\n \t\t\tif (result < (NumericLimits<T>::Minimum() + digit) / 10) {\n \t\t\t\treturn false;\n@@ -195,8 +194,7 @@ struct IntegerCastOperation {\n \t\treturn true;\n \t}\n \n-\ttemplate<class T>\n-\tstatic bool HandleExponent(T &result, int64_t exponent) {\n+\ttemplate <class T> static bool HandleExponent(T &result, int64_t exponent) {\n \t\tdouble dbl_res = result * pow(10, exponent);\n \t\tif (dbl_res < NumericLimits<T>::Minimum() || dbl_res > NumericLimits<T>::Maximum()) {\n \t\t\treturn false;\n@@ -205,17 +203,16 @@ struct IntegerCastOperation {\n \t\treturn true;\n \t}\n \n-\ttemplate<class T>\n-\tstatic bool Finalize(T &result) {\n+\ttemplate <class T> static bool Finalize(T &result) {\n \t\treturn true;\n \t}\n };\n \n-template <class T, bool NEGATIVE, bool ALLOW_EXPONENT, class OP=IntegerCastOperation>\n+template <class T, bool NEGATIVE, bool ALLOW_EXPONENT, class OP = IntegerCastOperation>\n static bool IntegerCastLoop(const char *buf, idx_t len, T &result, bool strict) {\n \tidx_t start_pos = NEGATIVE || *buf == '+' ? 1 : 0;\n \tidx_t pos = start_pos;\n-\twhile(pos < len) {\n+\twhile (pos < len) {\n \t\tif (!std::isdigit((unsigned char)buf[pos])) {\n \t\t\t// not a digit!\n \t\t\tif (buf[pos] == '.') {\n@@ -231,7 +228,7 @@ static bool IntegerCastLoop(const char *buf, idx_t len, T &result, bool strict)\n \t\t\t\t// make sure everything after the period is a number\n \t\t\t\tpos++;\n \t\t\t\tidx_t start_digit = pos;\n-\t\t\t\twhile(pos < len) {\n+\t\t\t\twhile (pos < len) {\n \t\t\t\t\tif (!std::isdigit((unsigned char)buf[pos++])) {\n \t\t\t\t\t\treturn false;\n \t\t\t\t\t}\n@@ -242,7 +239,7 @@ static bool IntegerCastLoop(const char *buf, idx_t len, T &result, bool strict)\n \t\t\t}\n \t\t\tif (std::isspace((unsigned char)buf[pos])) {\n \t\t\t\t// skip any trailing spaces\n-\t\t\t\twhile(++pos < len) {\n+\t\t\t\twhile (++pos < len) {\n \t\t\t\t\tif (!std::isspace((unsigned char)buf[pos])) {\n \t\t\t\t\t\treturn false;\n \t\t\t\t\t}\n@@ -279,9 +276,10 @@ static bool IntegerCastLoop(const char *buf, idx_t len, T &result, bool strict)\n \treturn pos > start_pos;\n }\n \n-template <class T, bool ALLOW_EXPONENT = true, class OP=IntegerCastOperation> static bool TryIntegerCast(const char *buf, idx_t len, T &result, bool strict) {\n+template <class T, bool ALLOW_EXPONENT = true, class OP = IntegerCastOperation>\n+static bool TryIntegerCast(const char *buf, idx_t len, T &result, bool strict) {\n \t// skip any spaces at the start\n-\twhile(len > 0 && std::isspace(*buf)) {\n+\twhile (len > 0 && std::isspace(*buf)) {\n \t\tbuf++;\n \t\tlen--;\n \t}\n@@ -302,7 +300,7 @@ template <> bool TryCast::Operation(string_t input, bool &result, bool strict) {\n \tauto input_data = input.GetData();\n \tauto input_size = input.GetSize();\n \n-\tswitch(input_size) {\n+\tswitch (input_size) {\n \tcase 1: {\n \t\tchar c = std::tolower(*input_data);\n \t\tif (c == 't' || (!strict && c == '1')) {\n@@ -433,7 +431,7 @@ template <> bool CheckDoubleValidity(double value) {\n \n template <class T> static bool TryDoubleCast(const char *buf, idx_t len, T &result, bool strict) {\n \t// skip any spaces at the start\n-\twhile(len > 0 && std::isspace(*buf)) {\n+\twhile (len > 0 && std::isspace(*buf)) {\n \t\tbuf++;\n \t\tlen--;\n \t}\n@@ -657,7 +655,8 @@ struct HugeintToStringCast {\n \t\t\t// we want to avoid doing as many divisions as possible\n \t\t\t// for that reason we start off doing a division by a large power of ten that uint64_t can hold\n \t\t\t// (100000000000000000) - this is the third largest\n-\t\t\t// the reason we don't use the largest is because that can result in an overflow inside the division function\n+\t\t\t// the reason we don't use the largest is because that can result in an overflow inside the division\n+\t\t\t// function\n \t\t\tuint64_t remainder;\n \t\t\tvalue = Hugeint::DivModPositive(value, 100000000000000000ULL, remainder);\n \n@@ -668,7 +667,7 @@ struct HugeintToStringCast {\n \n \t\t\tint format_length = startptr - ptr;\n \t\t\t// pad with zero\n-\t\t\tfor(int i = format_length; i < 17; i++) {\n+\t\t\tfor (int i = format_length; i < 17; i++) {\n \t\t\t\t*--ptr = '0';\n \t\t\t}\n \t\t}\n@@ -917,16 +916,17 @@ template <> string_t CastFromBlob::Operation(string_t input, Vector &vector) {\n }\n \n void CastFromBlob::ToHexString(string_t input, string_t &output) {\n-\tconst char hexa_table[] = {'0','1','2','3','4','5','6','7','8','9','A','B','C','D','E','F'};\n+\tconst char hexa_table[] = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F'};\n \tidx_t input_size = input.GetSize();\n \tassert(output.GetSize() == (input_size * 2 + 2));\n \tauto input_data = input.GetData();\n-\tauto hexa_data  = output.GetData();\n+\tauto hexa_data = output.GetData();\n \t// hex identifier\n-\thexa_data[0] = '\\\\'; hexa_data[1] = 'x';\n+\thexa_data[0] = '\\\\';\n+\thexa_data[1] = 'x';\n \thexa_data += 2;\n-\tfor(idx_t idx = 0; idx < input_size; ++idx) {\n-\t\thexa_data[idx * 2]     = hexa_table[(input_data[idx] >> 4) & 0x0F];\n+\tfor (idx_t idx = 0; idx < input_size; ++idx) {\n+\t\thexa_data[idx * 2] = hexa_table[(input_data[idx] >> 4) & 0x0F];\n \t\thexa_data[idx * 2 + 1] = hexa_table[input_data[idx] & 0x0F];\n \t}\n \toutput.Finalize();\n@@ -935,7 +935,7 @@ void CastFromBlob::ToHexString(string_t input, string_t &output) {\n void CastFromBlob::FromHexToBytes(string_t input, string_t &output) {\n \tidx_t in_size = input.GetSize();\n \t// amount of hex chars must be even\n-\tif((in_size % 2) != 0) {\n+\tif ((in_size % 2) != 0) {\n \t\tthrow OutOfRangeException(\"Hex string must have an even number of bytes.\");\n \t}\n \n@@ -947,21 +947,20 @@ void CastFromBlob::FromHexToBytes(string_t input, string_t &output) {\n \tauto out_data = output.GetData();\n \tidx_t out_size = output.GetSize();\n \tassert(out_size == (in_size / 2));\n-\tidx_t out_idx=0;\n+\tidx_t out_idx = 0;\n \n \tidx_t num_hex_per_byte = 2;\n \tuint8_t hex[2];\n \n-\tfor(idx_t in_idx = 0; in_idx < in_size; in_idx+=2, ++out_idx) {\n-\t\tfor(idx_t hex_idx = 0; hex_idx < num_hex_per_byte; ++hex_idx) {\n+\tfor (idx_t in_idx = 0; in_idx < in_size; in_idx += 2, ++out_idx) {\n+\t\tfor (idx_t hex_idx = 0; hex_idx < num_hex_per_byte; ++hex_idx) {\n \t\t\tuint8_t int_ch = in_data[in_idx + hex_idx];\n-\t\t\tif(int_ch >= (uint8_t)'0' && int_ch <= (uint8_t)'9') {\n+\t\t\tif (int_ch >= (uint8_t)'0' && int_ch <= (uint8_t)'9') {\n \t\t\t\t// numeric ascii chars: '0' to '9'\n \t\t\t\thex[hex_idx] = int_ch & 0X0F;\n-\t\t\t}\n-\t\t\telse if((int_ch >= (uint8_t)'A' && int_ch <= (uint8_t)'F') ||\n-\t\t\t\t\t(int_ch >= (uint8_t)'a' && int_ch <= (uint8_t)'f')) {\n-\t\t\t\t\t// hex chars: ['A':'F'] or ['a':'f']\n+\t\t\t} else if ((int_ch >= (uint8_t)'A' && int_ch <= (uint8_t)'F') ||\n+\t\t\t           (int_ch >= (uint8_t)'a' && int_ch <= (uint8_t)'f')) {\n+\t\t\t\t// hex chars: ['A':'F'] or ['a':'f']\n \t\t\t\t// transforming char into an integer in the range of 10 to 15\n \t\t\t\thex[hex_idx] = ((int_ch & 0X0F) - 1) + 10;\n \t\t\t} else {\n@@ -983,7 +982,7 @@ template <> string_t CastToBlob::Operation(string_t input, Vector &vector) {\n \tauto input_data = input.GetData();\n \tstring_t result;\n \t// Check by a hex string\n-\tif(input_size >= 2 && input_data[0] == '\\\\' && input_data[1] == 'x') {\n+\tif (input_size >= 2 && input_data[0] == '\\\\' && input_data[1] == 'x') {\n \t\tauto output = StringVector::EmptyString(vector, (input_size - 2) / 2);\n \t\tCastFromBlob::FromHexToBytes(input, output);\n \t\tresult = output;\n@@ -1046,8 +1045,7 @@ struct HugeIntCastData {\n };\n \n struct HugeIntegerCastOperation {\n-\ttemplate<class T, bool NEGATIVE>\n-\tstatic bool HandleDigit(T &result, uint8_t digit) {\n+\ttemplate <class T, bool NEGATIVE> static bool HandleDigit(T &result, uint8_t digit) {\n \t\tif (NEGATIVE) {\n \t\t\tif (result.intermediate < (NumericLimits<int64_t>::Minimum() + digit) / 10) {\n \t\t\t\t// intermediate is full: need to flush it\n@@ -1068,13 +1066,13 @@ struct HugeIntegerCastOperation {\n \t\treturn true;\n \t}\n \n-\ttemplate<class T>\n-\tstatic bool HandleExponent(T &result, int64_t exponent) {\n+\ttemplate <class T> static bool HandleExponent(T &result, int64_t exponent) {\n \t\tresult.Flush();\n \t\tif (exponent < -38 || exponent > 38) {\n \t\t\t// out of range for exact exponent: use double and convert\n \t\t\tdouble dbl_res = Hugeint::Cast<double>(result.hugeint) * pow(10, exponent);\n-\t\t\tif (dbl_res < Hugeint::Cast<double>(NumericLimits<hugeint_t>::Minimum()) || dbl_res > Hugeint::Cast<double>(NumericLimits<hugeint_t>::Maximum())) {\n+\t\t\tif (dbl_res < Hugeint::Cast<double>(NumericLimits<hugeint_t>::Minimum()) ||\n+\t\t\t    dbl_res > Hugeint::Cast<double>(NumericLimits<hugeint_t>::Maximum())) {\n \t\t\t\treturn false;\n \t\t\t}\n \t\t\tresult.hugeint = Hugeint::Convert(dbl_res);\n@@ -1090,15 +1088,15 @@ struct HugeIntegerCastOperation {\n \t\t}\n \t}\n \n-\ttemplate<class T>\n-\tstatic bool Finalize(T &result) {\n+\ttemplate <class T> static bool Finalize(T &result) {\n \t\treturn result.Flush();\n \t}\n };\n \n template <> bool TryCast::Operation(string_t input, hugeint_t &result, bool strict) {\n \tHugeIntCastData data;\n-\tif (!TryIntegerCast<HugeIntCastData, true, HugeIntegerCastOperation>(input.GetData(), input.GetSize(), data, strict)) {\n+\tif (!TryIntegerCast<HugeIntCastData, true, HugeIntegerCastOperation>(input.GetData(), input.GetSize(), data,\n+\t                                                                     strict)) {\n \t\treturn false;\n \t}\n \tresult = data.hugeint;\n@@ -1215,8 +1213,7 @@ template <> bool Cast::Operation(hugeint_t input) {\n \treturn result;\n }\n \n-template<class T>\n-static T hugeint_cast_to_numeric(hugeint_t input) {\n+template <class T> static T hugeint_cast_to_numeric(hugeint_t input) {\n \tT result;\n \tif (!TryCast::Operation<hugeint_t, T>(input, result)) {\n \t\tthrow OutOfRangeException(\"Failed to cast from hugeint: value is out of range\");\ndiff --git a/src/common/types.cpp b/src/common/types.cpp\nindex 882a6cb750dd..66863a8a0ae1 100644\n--- a/src/common/types.cpp\n+++ b/src/common/types.cpp\n@@ -372,24 +372,24 @@ bool SQLType::IsMoreGenericThan(SQLType &other) const {\n \tswitch (id) {\n \tcase SQLTypeId::SMALLINT:\n \t\tswitch (other.id) {\n+\t\tcase SQLTypeId::BOOLEAN:\n \t\tcase SQLTypeId::TINYINT:\n-\t\tcase SQLTypeId::SMALLINT:\n-\t\tcase SQLTypeId::INTEGER:\n \t\t\treturn true;\n \t\tdefault:\n \t\t\treturn false;\n \t\t}\n \tcase SQLTypeId::INTEGER:\n \t\tswitch (other.id) {\n+\t\tcase SQLTypeId::BOOLEAN:\n \t\tcase SQLTypeId::TINYINT:\n \t\tcase SQLTypeId::SMALLINT:\n-\t\tcase SQLTypeId::INTEGER:\n \t\t\treturn true;\n \t\tdefault:\n \t\t\treturn false;\n \t\t}\n \tcase SQLTypeId::BIGINT:\n \t\tswitch (other.id) {\n+\t\tcase SQLTypeId::BOOLEAN:\n \t\tcase SQLTypeId::TINYINT:\n \t\tcase SQLTypeId::SMALLINT:\n \t\tcase SQLTypeId::INTEGER:\n@@ -399,6 +399,18 @@ bool SQLType::IsMoreGenericThan(SQLType &other) const {\n \t\t}\n \tcase SQLTypeId::HUGEINT:\n \t\tswitch (other.id) {\n+\t\tcase SQLTypeId::BOOLEAN:\n+\t\tcase SQLTypeId::TINYINT:\n+\t\tcase SQLTypeId::SMALLINT:\n+\t\tcase SQLTypeId::INTEGER:\n+\t\tcase SQLTypeId::BIGINT:\n+\t\t\treturn true;\n+\t\tdefault:\n+\t\t\treturn false;\n+\t\t}\n+\tcase SQLTypeId::FLOAT:\n+\t\tswitch (other.id) {\n+\t\tcase SQLTypeId::BOOLEAN:\n \t\tcase SQLTypeId::TINYINT:\n \t\tcase SQLTypeId::SMALLINT:\n \t\tcase SQLTypeId::INTEGER:\n@@ -407,12 +419,15 @@ bool SQLType::IsMoreGenericThan(SQLType &other) const {\n \t\tdefault:\n \t\t\treturn false;\n \t\t}\n+\t\treturn false;\n \tcase SQLTypeId::DOUBLE:\n \t\tswitch (other.id) {\n+\t\tcase SQLTypeId::BOOLEAN:\n \t\tcase SQLTypeId::TINYINT:\n \t\tcase SQLTypeId::SMALLINT:\n \t\tcase SQLTypeId::INTEGER:\n \t\tcase SQLTypeId::BIGINT:\n+\t\tcase SQLTypeId::FLOAT:\n \t\t\treturn true;\n \t\tdefault:\n \t\t\treturn false;\ndiff --git a/src/execution/operator/persistent/buffered_csv_reader.cpp b/src/execution/operator/persistent/buffered_csv_reader.cpp\nindex 2ebd590d4ffb..a92c07e59df7 100644\n--- a/src/execution/operator/persistent/buffered_csv_reader.cpp\n+++ b/src/execution/operator/persistent/buffered_csv_reader.cpp\n@@ -4,8 +4,8 @@\n #include \"duckdb/common/file_system.hpp\"\n #include \"duckdb/common/gzip_stream.hpp\"\n #include \"duckdb/common/string_util.hpp\"\n-#include \"duckdb/common/vector_operations/vector_operations.hpp\"\n #include \"duckdb/common/vector_operations/unary_executor.hpp\"\n+#include \"duckdb/common/vector_operations/vector_operations.hpp\"\n #include \"duckdb/execution/operator/persistent/physical_copy_from_file.hpp\"\n #include \"duckdb/function/scalar/strftime.hpp\"\n #include \"duckdb/main/database.hpp\"\n@@ -36,7 +36,7 @@ static string GenerateColumnName(const idx_t total_cols, const idx_t col_number,\n \n static string GetLineNumberStr(idx_t linenr, bool linenr_estimated) {\n \tstring estimated = (linenr_estimated ? string(\" (estimated)\") : string(\"\"));\n-\treturn std::to_string(linenr) + estimated;\n+\treturn std::to_string(linenr + 1) + estimated;\n }\n \n TextSearchShiftArray::TextSearchShiftArray() {\n@@ -70,14 +70,16 @@ TextSearchShiftArray::TextSearchShiftArray(string search_term) : length(search_t\n \t}\n }\n \n-BufferedCSVReader::BufferedCSVReader(ClientContext &context, BufferedCSVReaderOptions options, vector<SQLType> requested_types)\n-\t: options(options), buffer_size(0), position(0), start(0) {\n+BufferedCSVReader::BufferedCSVReader(ClientContext &context, BufferedCSVReaderOptions options,\n+                                     vector<SQLType> requested_types)\n+    : options(options), buffer_size(0), position(0), start(0) {\n \tsource = OpenCSV(context, options);\n \tInitialize(requested_types);\n }\n \n-BufferedCSVReader::BufferedCSVReader(BufferedCSVReaderOptions options, vector<SQLType> requested_types, unique_ptr<istream> ssource)\n-\t: options(options), source(move(ssource)), buffer_size(0), position(0), start(0) {\n+BufferedCSVReader::BufferedCSVReader(BufferedCSVReaderOptions options, vector<SQLType> requested_types,\n+                                     unique_ptr<istream> ssource)\n+    : options(options), source(move(ssource)), buffer_size(0), position(0), start(0) {\n \tInitialize(requested_types);\n }\n \n@@ -90,7 +92,7 @@ void BufferedCSVReader::Initialize(vector<SQLType> requested_types) {\n \n \tPrepareComplexParser();\n \tInitParseChunk(sql_types.size());\n-\tSkipHeader();\n+\tSkipHeader(options.skip_rows, options.header);\n }\n \n void BufferedCSVReader::PrepareComplexParser() {\n@@ -99,6 +101,17 @@ void BufferedCSVReader::PrepareComplexParser() {\n \tquote_search = TextSearchShiftArray(options.quote);\n }\n \n+void BufferedCSVReader::ConfigureSampling() {\n+\tif (options.sample_size > STANDARD_VECTOR_SIZE) {\n+\t\tthrow ParserException(\"Chunk size (%d) cannot be bigger than STANDARD_VECTOR_SIZE (%d)\",\n+\t\t\t                    options.sample_size, STANDARD_VECTOR_SIZE);\n+\t} else if (options.sample_size < 1) {\n+\t\tthrow ParserException(\"Chunk size cannot be smaller than 1.\");\n+\t}\n+\tSAMPLE_CHUNK_SIZE = options.sample_size;\n+\tMAX_SAMPLE_CHUNKS = options.num_samples;\n+}\n+\n unique_ptr<istream> BufferedCSVReader::OpenCSV(ClientContext &context, BufferedCSVReaderOptions options) {\n \tif (!FileSystem::GetFileSystem(context).FileExists(options.file_path)) {\n \t\tthrow IOException(\"File \\\"%s\\\" not found\", options.file_path.c_str());\n@@ -123,15 +136,15 @@ unique_ptr<istream> BufferedCSVReader::OpenCSV(ClientContext &context, BufferedC\n \treturn result;\n }\n \n-void BufferedCSVReader::SkipHeader() {\n-\tfor (idx_t i = 0; i < options.skip_rows; i++) {\n+void BufferedCSVReader::SkipHeader(idx_t skip_rows, bool skip_header) {\n+\tfor (idx_t i = 0; i < skip_rows; i++) {\n \t\t// ignore skip rows\n \t\tstring read_line;\n \t\tgetline(*source, read_line);\n \t\tlinenr++;\n \t}\n \n-\tif (options.header) {\n+\tif (skip_header) {\n \t\t// ignore the first line as a header line\n \t\tstring read_line;\n \t\tgetline(*source, read_line);\n@@ -182,18 +195,22 @@ void BufferedCSVReader::InitParseChunk(idx_t num_cols) {\n \tparse_chunk.Initialize(varchar_types);\n }\n \n-void BufferedCSVReader::JumpToBeginning() {\n+void BufferedCSVReader::JumpToBeginning(idx_t skip_rows, bool skip_header) {\n \tResetBuffer();\n \tResetStream();\n \tResetParseChunk();\n-\tSkipHeader();\n+\tSkipHeader(skip_rows, skip_header);\n }\n \n bool BufferedCSVReader::JumpToNextSample() {\n-\tif (source->eof() || sample_chunk_idx >= MAX_SAMPLE_CHUNKS) {\n+\tif (end_of_file_reached || sample_chunk_idx >= MAX_SAMPLE_CHUNKS) {\n \t\treturn false;\n \t}\n \n+\t// adjust the value of bytes_in_chunk, based on current state of the buffer\n+\tidx_t remaining_bytes_in_buffer = buffer_size - start;\n+\tbytes_in_chunk -= remaining_bytes_in_buffer;\n+\n \t// update average bytes per line\n \tdouble bytes_per_line = bytes_in_chunk / (double)SAMPLE_CHUNK_SIZE;\n \tbytes_per_line_avg = ((bytes_per_line_avg * sample_chunk_idx) + bytes_per_line) / (sample_chunk_idx + 1);\n@@ -213,10 +230,6 @@ bool BufferedCSVReader::JumpToNextSample() {\n \t\treturn true;\n \t}\n \n-\t// adjust the value of bytes_in_chunk, based on current state of the buffer\n-\tidx_t remaining_bytes_in_buffer = buffer_size - start;\n-\tbytes_in_chunk -= remaining_bytes_in_buffer;\n-\n \t// if none of the previous conditions were met, we can jump\n \tidx_t partition_size = (idx_t)round(file_size / (double)MAX_SAMPLE_CHUNKS);\n \n@@ -274,33 +287,64 @@ bool BufferedCSVReader::TryCastValue(Value value, SQLType sql_type) {\n \treturn false;\n }\n \n+bool BufferedCSVReader::TryCastVector(Vector &parse_chunk_col, idx_t size, SQLType sql_type) {\n+\ttry {\n+\t\t// try vector-cast from string to sql_type\n+\t\tVector dummy_result(GetInternalType(sql_type));\n+\t\tif (options.has_date_format && sql_type == SQLTypeId::DATE) {\n+\t\t\t// use the date format to cast the chunk\n+\t\t\tUnaryExecutor::Execute<string_t, date_t, true>(parse_chunk_col, dummy_result, size, [&](string_t input) {\n+\t\t\t\treturn options.date_format.ParseDate(input);\n+\t\t\t});\n+\t\t} else if (options.has_timestamp_format && sql_type == SQLTypeId::TIMESTAMP) {\n+\t\t\t// use the date format to cast the chunk\n+\t\t\tUnaryExecutor::Execute<string_t, timestamp_t, true>(\n+\t\t\t    parse_chunk_col, dummy_result, size,\n+\t\t\t    [&](string_t input) { return options.timestamp_format.ParseTimestamp(input); });\n+\t\t} else {\n+\t\t\t// target type is not varchar: perform a cast\n+\t\t\tVectorOperations::Cast(parse_chunk_col, dummy_result, SQLType::VARCHAR, sql_type, size, true);\n+\t\t}\n+\t} catch (const Exception &e) {\n+\t\treturn false;\n+\t}\n+\treturn true;\n+}\n+\n+void BufferedCSVReader::PrepareCandidateSets() {\n+\tif (options.has_delimiter) {\n+\t\tdelim_candidates = {options.delimiter};\n+\t}\n+\tif (options.has_quote) {\n+\t\tquote_candidates_map = {{options.quote}, {options.quote}, {options.quote}};\n+\t}\n+\tif (options.has_escape) {\n+\t\tif (options.escape == \"\") {\n+\t\t\tquoterule_candidates = {QuoteRule::QUOTES_RFC};\n+\t\t} else {\n+\t\t\tquoterule_candidates = {QuoteRule::QUOTES_OTHER};\n+\t\t}\n+\t\tescape_candidates_map[static_cast<uint8_t>(quoterule_candidates[0])] = {options.escape};\n+\t}\n+}\n+\n vector<SQLType> BufferedCSVReader::SniffCSV(vector<SQLType> requested_types) {\n-\t// TODO: sniff for uncommon (UTF-8) delimiter variants in first lines and add them to the list\n-\tconst vector<string> delim_candidates = {\",\", \"|\", \";\", \"\\t\"};\n-\tconst vector<QuoteRule> quoterule_candidates = {QuoteRule::QUOTES_RFC, QuoteRule::QUOTES_OTHER,\n-\t\t\t\t\t\t\t\t\t\t\t\t\tQuoteRule::NO_QUOTES};\n-\t// quote candiates depend on quote rule\n-\tconst vector<vector<string>> quote_candidates_map = {{\"\\\"\"}, {\"\\\"\", \"'\"}, {\"\"}};\n-\t// escape candiates also depend on quote rule.\n-\t// Note: RFC-conform escapes are handled automatically, and without quotes no escape char is required\n-\tconst vector<vector<string>> escape_candidates_map = {{\"\"}, {\"\\\\\"}, {\"\"}};\n+\tConfigureSampling();\n+\tPrepareCandidateSets();\n \n+\tBufferedCSVReaderOptions original_options = options;\n \tvector<BufferedCSVReaderOptions> info_candidates;\n \tidx_t best_consistent_rows = 0;\n \tidx_t best_num_cols = 0;\n \n-\t// if requested_types were provided, use them already in dialect detection\n-\t// TODO: currently they only serve to solve the edge case of trailing empty delimiters,\n-\t// however, they could be used to solve additional ambigious scenarios.\n-\tsql_types = requested_types;\n-\t// TODO: add a flag to indicate that no option actually worked and default will be used (RFC-4180)\n+\tJumpToBeginning(0, false);\n \tfor (QuoteRule quoterule : quoterule_candidates) {\n \t\tvector<string> quote_candidates = quote_candidates_map[static_cast<uint8_t>(quoterule)];\n \t\tfor (const auto &quote : quote_candidates) {\n \t\t\tfor (const auto &delim : delim_candidates) {\n \t\t\t\tvector<string> escape_candidates = escape_candidates_map[static_cast<uint8_t>(quoterule)];\n \t\t\t\tfor (const auto &escape : escape_candidates) {\n-\t\t\t\t\tBufferedCSVReaderOptions sniff_info = options;\n+\t\t\t\t\tBufferedCSVReaderOptions sniff_info = original_options;\n \t\t\t\t\tsniff_info.delimiter = delim;\n \t\t\t\t\tsniff_info.quote = quote;\n \t\t\t\t\tsniff_info.escape = escape;\n@@ -339,7 +383,9 @@ vector<SQLType> BufferedCSVReader::SniffCSV(vector<SQLType> requested_types) {\n \t\t\t\t\tbool more_than_one_column = (num_cols > 1);\n \t\t\t\t\tbool start_good = info_candidates.size() > 0 && (start_row <= info_candidates.front().skip_rows);\n \n-\t\t\t\t\tif ((more_values || single_column_before) && rows_consistent) {\n+\t\t\t\t\tif (requested_types.size() > 0 && requested_types.size() != num_cols) {\n+\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t} else if ((more_values || single_column_before) && rows_consistent) {\n \t\t\t\t\t\tsniff_info.skip_rows = start_row;\n \t\t\t\t\t\tsniff_info.num_cols = num_cols;\n \t\t\t\t\t\tbest_consistent_rows = consistent_rows;\n@@ -365,21 +411,25 @@ vector<SQLType> BufferedCSVReader::SniffCSV(vector<SQLType> requested_types) {\n \t\t}\n \t}\n \n-\t// then, file was most likely empty and we can do no more\n+\t// if not dialect candidate was found, then file was most likely empty and we default to RFC-4180 dialect\n \tif (info_candidates.size() < 1) {\n \t\tif (requested_types.size() == 0) {\n \t\t\t// no types requested and no types/names could be deduced: default to a single varchar column\n \t\t\tcol_names.push_back(\"col0\");\n \t\t\trequested_types.push_back(SQLType::VARCHAR);\n \t\t}\n+\n+\t\t// back to normal\n+\t\toptions = original_options;\n+\t\tJumpToBeginning(0, false);\n \t\treturn requested_types;\n \t}\n \n \t// type candidates, ordered by descending specificity (~ from high to low)\n-\tvector<SQLType> type_candidates = {SQLType::VARCHAR, SQLType::TIMESTAMP, SQLType::DATE,\n-\t\t\t\t\t\t\t\t\t   SQLType::TIME,    SQLType::DOUBLE,    /*SQLType::FLOAT,*/ SQLType::BIGINT,\n-\t\t\t\t\t\t\t\t\t   SQLType::INTEGER, /* SQLType::SMALLINT, */  /*SQLType::TINYINT,*/ SQLType::BOOLEAN,\n-\t\t\t\t\t\t\t\t\t   SQLType::SQLNULL};\n+\tvector<SQLType> type_candidates = {SQLType::VARCHAR, SQLType::TIMESTAMP,\n+\t                                   SQLType::DATE,    SQLType::TIME,\n+\t                                   SQLType::DOUBLE,  /* SQLType::FLOAT,*/ SQLType::BIGINT,\n+\t                                   SQLType::INTEGER, /*SQLType::SMALLINT, SQLType::TINYINT,*/ SQLType::BOOLEAN};\n \n \t// check which info candiate leads to minimum amount of non-varchar columns...\n \tBufferedCSVReaderOptions best_options;\n@@ -395,7 +445,7 @@ vector<SQLType> BufferedCSVReader::SniffCSV(vector<SQLType> requested_types) {\n \t\tInitParseChunk(sql_types.size());\n \n \t\t// detect types in first chunk\n-\t\tJumpToBeginning();\n+\t\tJumpToBeginning(options.skip_rows, false);\n \t\tParseCSV(ParserMode::SNIFFING_DATATYPES);\n \t\tfor (idx_t row = 0; row < parse_chunk.size(); row++) {\n \t\t\tfor (idx_t col = 0; col < parse_chunk.column_count(); col++) {\n@@ -427,7 +477,7 @@ vector<SQLType> BufferedCSVReader::SniffCSV(vector<SQLType> requested_types) {\n \t\t\t}\n \t\t}\n \n-\t\t// it's good if the dialect creates more non-varchar columns, but only if we sacrifice < 40% of best_num_cols.\n+\t\t// it's good if the dialect creates more non-varchar columns, but only if we sacrifice < 30% of best_num_cols.\n \t\tif (varchar_cols < min_varchar_cols && parse_chunk.column_count() > (best_num_cols * 0.7)) {\n \t\t\t// we have a new best_info candidate\n \t\t\tbest_options = info_candidate;\n@@ -438,53 +488,69 @@ vector<SQLType> BufferedCSVReader::SniffCSV(vector<SQLType> requested_types) {\n \n \toptions = best_options;\n \n-\t// if data types were provided, exit here if number of columns does not match\n-\t// TODO: we could think about postponing this to see if the csv happens to contain a superset of requested columns\n-\tif (requested_types.size() > 0 && requested_types.size() != options.num_cols) {\n-\t\tthrow ParserException(\"Error while determining column types: found %lld columns but expected %d\", options.num_cols,\n-\t\t\t\t\t\t\t  requested_types.size());\n-\t}\n-\n \t// sql_types and parse_chunk have to be in line with new info\n \tsql_types.clear();\n \tsql_types.assign(options.num_cols, SQLType::VARCHAR);\n \tInitParseChunk(sql_types.size());\n \n-\t// jump through the rest of the file and continue to refine the sql type guess\n-\twhile (JumpToNextSample()) {\n-\t\t// if jump ends up a bad line, we just skip this chunk\n-\t\ttry {\n-\t\t\tParseCSV(ParserMode::SNIFFING_DATATYPES);\n-\t\t} catch (const ParserException &e) {\n-\t\t\tcontinue;\n+\tvector<SQLType> detected_types;\n+\n+\t// if data types were provided, exit here if number of columns does not match\n+\tif (requested_types.size() > 0) {\n+\t\tif (requested_types.size() != options.num_cols) {\n+\t\t\tthrow ParserException(\"Error while determining column types: found %lld columns but expected %d\",\n+\t\t\t                      options.num_cols, requested_types.size());\n+\t\t} else {\n+\t\t\tdetected_types = requested_types;\n \t\t}\n-\t\tfor (idx_t col = 0; col < parse_chunk.column_count(); col++) {\n-\t\t\tvector<SQLType> &col_type_candidates = best_sql_types_candidates[col];\n-\t\t\twhile (col_type_candidates.size() > 1) {\n-\t\t\t\ttry {\n+\t} else {\n+\t\t// jump through the rest of the file and continue to refine the sql type guess\n+\t\twhile (JumpToNextSample()) {\n+\t\t\t// if jump ends up a bad line, we just skip this chunk\n+\t\t\ttry {\n+\t\t\t\tParseCSV(ParserMode::SNIFFING_DATATYPES);\n+\t\t\t} catch (const ParserException &e) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tfor (idx_t col = 0; col < parse_chunk.column_count(); col++) {\n+\t\t\t\tvector<SQLType> &col_type_candidates = best_sql_types_candidates[col];\n+\t\t\t\twhile (col_type_candidates.size() > 1) {\n \t\t\t\t\tconst auto &sql_type = col_type_candidates.back();\n-\t\t\t\t\t// try vector-cast from string to sql_type\n-\t\t\t\t\tparse_chunk.data[col];\n-\t\t\t\t\tVector dummy_result(GetInternalType(sql_type));\n-\t\t\t\t\tVectorOperations::Cast(parse_chunk.data[col], dummy_result, SQLType::VARCHAR, sql_type,\n-\t\t\t\t\t\t\t\t\t\t   parse_chunk.size(), true);\n-\t\t\t\t\tbreak;\n-\t\t\t\t} catch (const Exception &e) {\n-\t\t\t\t\tcol_type_candidates.pop_back();\n+\t\t\t\t\tif (TryCastVector(parse_chunk.data[col], parse_chunk.size(), sql_type)) {\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tcol_type_candidates.pop_back();\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t}\n+\n+\t\t// set sql types\n+\t\tfor (idx_t col = 0; col < best_sql_types_candidates.size(); col++) {\n+\t\t\tSQLType d_type = best_sql_types_candidates[col].back();\n+\t\t\tdetected_types.push_back(d_type);\n+\t\t}\n \t}\n \n+\t// if all rows are of type string, we will currently make the assumption there is no header.\n+\t// TODO: Do some kind of string-distance based constistency metic between first row and others\n+\t/*bool all_types_string = true;\n+\tfor (idx_t col = 0; col < parse_chunk.column_count(); col++) {\n+\t    const auto &col_type = best_sql_types_candidates[col].back();\n+\t    all_types_string &= (col_type == SQLType::VARCHAR);\n+\t}*/\n+\n \t// information for header detection\n \tbool first_row_consistent = true;\n-\tbool first_row_nulls = true;\n+\tbool first_row_nulls = false;\n \n \t// parse first row again with knowledge from the rest of the file to check\n \t// whether first row is consistent with the others or not.\n-\tJumpToBeginning();\n+\tJumpToBeginning(options.skip_rows, false);\n \tParseCSV(ParserMode::SNIFFING_DATATYPES);\n-\tif (parse_chunk.size() > 0) {\n+\tif (parse_chunk.size() > 1) {\n+\t\tfirst_row_nulls = true;\n+\n \t\tfor (idx_t col = 0; col < parse_chunk.column_count(); col++) {\n \t\t\tauto dummy_val = parse_chunk.GetValue(col, 0);\n \t\t\t// try cast as SQLNULL\n@@ -494,24 +560,15 @@ vector<SQLType> BufferedCSVReader::SniffCSV(vector<SQLType> requested_types) {\n \t\t\t\tfirst_row_nulls = false;\n \t\t\t}\n \t\t\t// try cast to sql_type of column\n-\t\t\tvector<SQLType> &col_type_candidates = best_sql_types_candidates[col];\n-\t\t\tconst auto &sql_type = col_type_candidates.back();\n+\t\t\tconst auto &sql_type = detected_types[col];\n \t\t\tif (!TryCastValue(dummy_val, sql_type)) {\n \t\t\t\tfirst_row_consistent = false;\n \t\t\t}\n \t\t}\n \t}\n \n-\t// if all rows are of type string, we will currently make the assumption there is no header.\n-\t// TODO: Do some kind of string-distance based constistency metic between first row and others\n-\t/*bool all_types_string = true;\n-\tfor (idx_t col = 0; col < parse_chunk.column_count(); col++) {\n-\t\tconst auto &col_type = best_sql_types_candidates[col].back();\n-\t\tall_types_string &= (col_type == SQLType::VARCHAR);\n-\t}*/\n-\n \t// update parser info, and read, generate & set col_names based on previous findings\n-\tif (!first_row_consistent || first_row_nulls) {\n+\tif (((!first_row_consistent || first_row_nulls) && !options.has_header) || (options.has_header && options.header)) {\n \t\toptions.header = true;\n \t\tvector<string> t_col_names;\n \t\tfor (idx_t col = 0; col < parse_chunk.column_count(); col++) {\n@@ -542,34 +599,8 @@ vector<SQLType> BufferedCSVReader::SniffCSV(vector<SQLType> requested_types) {\n \t\t}\n \t}\n \n-\t// set sql types\n-\tvector<SQLType> detected_types;\n-\tfor (idx_t col = 0; col < best_sql_types_candidates.size(); col++) {\n-\t\tSQLType d_type = best_sql_types_candidates[col].back();\n-\n-\t\tif (requested_types.size() > 0) {\n-\t\t\tSQLType r_type = requested_types[col];\n-\n-\t\t\t// check if the detected types are in line with the provided types\n-\t\t\tif (r_type != d_type) {\n-\t\t\t\tif (r_type.IsMoreGenericThan(d_type)) {\n-\t\t\t\t\td_type = r_type;\n-\t\t\t\t} else {\n-\t\t\t\t\tthrow ParserException(\n-\t\t\t\t\t\t\"Error while sniffing data type for column '%s': Requested column type %s, detected type %s\",\n-\t\t\t\t\t\tcol_names[col].c_str(), SQLTypeToString(r_type).c_str(), SQLTypeToString(d_type).c_str());\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\n-\t\tdetected_types.push_back(d_type);\n-\t}\n-\n \t// back to normal\n-\tResetBuffer();\n-\tResetStream();\n-\tResetParseChunk();\n-\tsniffed_column_counts.clear();\n+\tJumpToBeginning(0, false);\n \n \treturn detected_types;\n }\n@@ -719,14 +750,15 @@ add_row : {\n \t\t\tcount++;\n \t\t\tif (count > delimiter_pos && count > quote_pos) {\n \t\t\t\tthrow ParserException(\n-\t\t\t\t\t\"Error on line %s: quote should be followed by end of value, end of row or another quote\",\n-\t\t\t\t\tGetLineNumberStr(linenr, linenr_estimated).c_str());\n+\t\t\t\t    \"Error on line %s: quote should be followed by end of value, end of row or another quote\",\n+\t\t\t\t    GetLineNumberStr(linenr, linenr_estimated).c_str());\n \t\t\t}\n \t\t\tif (delimiter_pos == options.delimiter.size()) {\n \t\t\t\t// quote followed by delimiter, add value\n \t\t\t\toffset = options.quote.size() + options.delimiter.size() - 1;\n \t\t\t\tgoto add_value;\n-\t\t\t} else if (quote_pos == options.quote.size() && (options.escape.size() == 0 || options.escape == options.quote)) {\n+\t\t\t} else if (quote_pos == options.quote.size() &&\n+\t\t\t           (options.escape.size() == 0 || options.escape == options.quote)) {\n \t\t\t\t// quote followed by quote, go back to quoted state and add to escape\n \t\t\t\tescape_positions.push_back(position - start - (options.quote.size() - 1));\n \t\t\t\tgoto in_quotes;\n@@ -734,7 +766,7 @@ add_row : {\n \t\t}\n \t} while (ReadBuffer(start));\n \tthrow ParserException(\"Error on line %s: quote should be followed by end of value, end of row or another quote\",\n-\t\t\t\t\t\t  GetLineNumberStr(linenr, linenr_estimated).c_str());\n+\t                      GetLineNumberStr(linenr, linenr_estimated).c_str());\n handle_escape:\n \tescape_pos = 0;\n \tquote_pos = 0;\n@@ -747,7 +779,7 @@ add_row : {\n \t\t\tcount++;\n \t\t\tif (count > escape_pos && count > quote_pos) {\n \t\t\t\tthrow ParserException(\"Error on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE\",\n-\t\t\t\t\t\t\t\t\t  GetLineNumberStr(linenr, linenr_estimated).c_str());\n+\t\t\t\t                      GetLineNumberStr(linenr, linenr_estimated).c_str());\n \t\t\t}\n \t\t\tif (quote_pos == options.quote.size() || escape_pos == options.escape.size()) {\n \t\t\t\t// found quote or escape: move back to quoted state\n@@ -756,7 +788,7 @@ add_row : {\n \t\t}\n \t} while (ReadBuffer(start));\n \tthrow ParserException(\"Error on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE\",\n-\t\t\t\t\t\t  GetLineNumberStr(linenr, linenr_estimated).c_str());\n+\t                      GetLineNumberStr(linenr, linenr_estimated).c_str());\n carriage_return:\n \t/* state: carriage_return */\n \t// this stage optionally skips a newline (\\n) character, which allows \\r\\n to be interpreted as a single line\n@@ -786,6 +818,8 @@ add_row : {\n \tif (mode == ParserMode::PARSING) {\n \t\tFlush(insert_chunk);\n \t}\n+\n+\tend_of_file_reached = true;\n }\n \n void BufferedCSVReader::ParseSimpleCSV(DataChunk &insert_chunk) {\n@@ -908,7 +942,7 @@ add_row : {\n \t\tgoto add_row;\n \t} else {\n \t\tthrow ParserException(\"Error on line %s: quote should be followed by end of value, end of row or another quote\",\n-\t\t\t\t\t\t\t  GetLineNumberStr(linenr, linenr_estimated).c_str());\n+\t\t                      GetLineNumberStr(linenr, linenr_estimated).c_str());\n \t}\n handle_escape:\n \t/* state: handle_escape */\n@@ -916,11 +950,11 @@ add_row : {\n \tposition++;\n \tif (position >= buffer_size && !ReadBuffer(start)) {\n \t\tthrow ParserException(\"Error on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE\",\n-\t\t\t\t\t\t\t  GetLineNumberStr(linenr, linenr_estimated).c_str());\n+\t\t                      GetLineNumberStr(linenr, linenr_estimated).c_str());\n \t}\n \tif (buffer[position] != options.quote[0] && buffer[position] != options.escape[0]) {\n \t\tthrow ParserException(\"Error on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE\",\n-\t\t\t\t\t\t\t  GetLineNumberStr(linenr, linenr_estimated).c_str());\n+\t\t                      GetLineNumberStr(linenr, linenr_estimated).c_str());\n \t}\n \t// escape was followed by quote or escape, go back to quoted state\n \tgoto in_quotes;\n@@ -956,6 +990,8 @@ add_row : {\n \tif (mode == ParserMode::PARSING) {\n \t\tFlush(insert_chunk);\n \t}\n+\n+\tend_of_file_reached = true;\n }\n \n bool BufferedCSVReader::ReadBuffer(idx_t &start) {\n@@ -1018,7 +1054,7 @@ void BufferedCSVReader::AddValue(char *str_val, idx_t length, idx_t &column, vec\n \t}\n \tif (column >= sql_types.size()) {\n \t\tthrow ParserException(\"Error on line %s: expected %lld values but got %d\",\n-\t\t\t\t\t\t\t  GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(), column + 1);\n+\t\t                      GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(), column + 1);\n \t}\n \n \t// insert the line number into the chunk\n@@ -1060,9 +1096,11 @@ void BufferedCSVReader::AddValue(char *str_val, idx_t length, idx_t &column, vec\n }\n \n bool BufferedCSVReader::AddRow(DataChunk &insert_chunk, idx_t &column) {\n+\tlinenr++;\n+\n \tif (column < sql_types.size() && mode != ParserMode::SNIFFING_DIALECT) {\n \t\tthrow ParserException(\"Error on line %s: expected %lld values but got %d\",\n-\t\t\t\t\t\t\t  GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(), column);\n+\t\t                      GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(), column);\n \t}\n \n \tif (mode == ParserMode::SNIFFING_DIALECT) {\n@@ -1085,7 +1123,6 @@ bool BufferedCSVReader::AddRow(DataChunk &insert_chunk, idx_t &column) {\n \t}\n \n \tcolumn = 0;\n-\tlinenr++;\n \treturn false;\n }\n \n@@ -1106,8 +1143,8 @@ void BufferedCSVReader::Flush(DataChunk &insert_chunk) {\n \t\t\t\t\tauto utf_type = Utf8Proc::Analyze(s.GetData(), s.GetSize());\n \t\t\t\t\tswitch (utf_type) {\n \t\t\t\t\tcase UnicodeType::INVALID:\n-\t\t\t\t\t\tthrow ParserException(\"Error on line %s: file is not valid UTF8\",\n-\t\t\t\t\t\t\t\t\t\t\t  GetLineNumberStr(linenr, linenr_estimated).c_str());\n+\t\t\t\t\t\tthrow ParserException(\"Error between line %d and %d: file is not valid UTF8\",\n+\t\t\t\t\t\t                      linenr - parse_chunk.size(), linenr);\n \t\t\t\t\tcase UnicodeType::ASCII:\n \t\t\t\t\t\tbreak;\n \t\t\t\t\tcase UnicodeType::UNICODE: {\n@@ -1121,22 +1158,36 @@ void BufferedCSVReader::Flush(DataChunk &insert_chunk) {\n \t\t\t}\n \t\t\tinsert_chunk.data[col_idx].Reference(parse_chunk.data[col_idx]);\n \t\t} else if (options.has_date_format && sql_types[col_idx].id == SQLTypeId::DATE) {\n-\t\t\t// use the date format to cast the chunk\n-\t\t\tUnaryExecutor::Execute<string_t, date_t, true>(parse_chunk.data[col_idx], insert_chunk.data[col_idx], parse_chunk.size(), [&](string_t input) {\n-\t\t\t\treturn options.date_format.ParseDate(input);\n-\t\t\t});\n+\t\t\ttry {\n+\t\t\t\t// use the date format to cast the chunk\n+\t\t\t\tUnaryExecutor::Execute<string_t, date_t, true>(\n+\t\t\t\t    parse_chunk.data[col_idx], insert_chunk.data[col_idx], parse_chunk.size(),\n+\t\t\t\t    [&](string_t input) { return options.date_format.ParseDate(input); });\n+\t\t\t} catch (const Exception &e) {\n+\t\t\t\tthrow ParserException(\"Error between line %llu and %llu: %s\", linenr - parse_chunk.size(), linenr,\n+\t\t\t\t                      e.what());\n+\t\t\t}\n \t\t} else if (options.has_timestamp_format && sql_types[col_idx].id == SQLTypeId::TIMESTAMP) {\n-\t\t\t// use the date format to cast the chunk\n-\t\t\tUnaryExecutor::Execute<string_t, timestamp_t, true>(parse_chunk.data[col_idx], insert_chunk.data[col_idx], parse_chunk.size(), [&](string_t input) {\n-\t\t\t\treturn options.timestamp_format.ParseTimestamp(input);\n-\t\t\t});\n+\t\t\ttry {\n+\t\t\t\t// use the date format to cast the chunk\n+\t\t\t\tUnaryExecutor::Execute<string_t, timestamp_t, true>(\n+\t\t\t\t    parse_chunk.data[col_idx], insert_chunk.data[col_idx], parse_chunk.size(),\n+\t\t\t\t    [&](string_t input) { return options.timestamp_format.ParseTimestamp(input); });\n+\t\t\t} catch (const Exception &e) {\n+\t\t\t\tthrow ParserException(\"Error between line %llu and %llu: %s\", linenr - parse_chunk.size(), linenr,\n+\t\t\t\t                      e.what());\n+\t\t\t}\n \t\t} else {\n-\t\t\t// target type is not varchar: perform a cast\n-\t\t\tVectorOperations::Cast(parse_chunk.data[col_idx], insert_chunk.data[col_idx], SQLType::VARCHAR,\n-\t\t\t\t\t\t\t\t   sql_types[col_idx], parse_chunk.size());\n+\t\t\ttry {\n+\t\t\t\t// target type is not varchar: perform a cast\n+\t\t\t\tVectorOperations::Cast(parse_chunk.data[col_idx], insert_chunk.data[col_idx], SQLType::VARCHAR,\n+\t\t\t\t                       sql_types[col_idx], parse_chunk.size());\n+\t\t\t} catch (const Exception &e) {\n+\t\t\t\tthrow ParserException(\"Error between line %llu and %llu: %s\", linenr - parse_chunk.size(), linenr,\n+\t\t\t\t                      e.what());\n+\t\t\t}\n \t\t}\n \t}\n \tparse_chunk.Reset();\n }\n-\n-}\n+} // namespace duckdb\ndiff --git a/src/function/scalar/date/strftime.cpp b/src/function/scalar/date/strftime.cpp\nindex e4d366f43bf7..bda42f371016 100644\n--- a/src/function/scalar/date/strftime.cpp\n+++ b/src/function/scalar/date/strftime.cpp\n@@ -15,6 +15,8 @@\n \n #include \"re2/re2.h\"\n \n+#include <cctype>\n+\n namespace duckdb {\n \n idx_t StrfTimepecifierSize(StrTimeSpecifier specifier) {\ndiff --git a/src/function/table/copy_csv.cpp b/src/function/table/copy_csv.cpp\nindex 20eb83f12f7d..12ec7016d620 100644\n--- a/src/function/table/copy_csv.cpp\n+++ b/src/function/table/copy_csv.cpp\n@@ -13,30 +13,39 @@ using namespace std;\n namespace duckdb {\n \n struct BaseCSVData : public FunctionData {\n-\tBaseCSVData(string file_path) :\n-\t\tfile_path(move(file_path)) {}\n+\tBaseCSVData(string file_path) : file_path(move(file_path)) {\n+\t}\n \n \t//! The file path of the CSV file to read or write\n \tstring file_path;\n+\t//! Whether or not a header information was given by the user\n+\tbool has_header = false;\n \t//! Whether or not to write a header in the file\n \tbool header = false;\n+\t//! Whether or not a delimiter was defined by the user\n+\tbool has_delimiter = false;\n \t//! Delimiter to separate columns within each line\n \tstring delimiter = \",\";\n+\t//! Whether or not a quote sign was defined by the user\n+\tbool has_quote = false;\n \t//! Quote used for columns that contain reserved characters, e.g., delimiter\n \tstring quote = \"\\\"\";\n+\t//! Quote used for columns that contain reserved characters, e.g., delimiter\n+\tbool has_escape = false;\n \t//! Escape character to escape quote chara\u0192cter\n \tstring escape;\n \t//! Specifies the string that represents a null value\n \tstring null_str;\n \t//! Whether or not the options are specified; if not we default to auto detect\n-\tbool is_auto_detect = true;\n+\tbool is_auto_detect = false;\n \n \tvoid Finalize();\n };\n \n struct WriteCSVData : public BaseCSVData {\n-\tWriteCSVData(string file_path, vector<SQLType> sql_types, vector<string> names) :\n-\t\tBaseCSVData(move(file_path)), sql_types(move(sql_types)), names(move(names)) {}\n+\tWriteCSVData(string file_path, vector<SQLType> sql_types, vector<string> names)\n+\t    : BaseCSVData(move(file_path)), sql_types(move(sql_types)), names(move(names)) {\n+\t}\n \n \t//! The SQL types to write\n \tvector<SQLType> sql_types;\n@@ -53,13 +62,18 @@ struct WriteCSVData : public BaseCSVData {\n };\n \n struct ReadCSVData : public BaseCSVData {\n-\tReadCSVData(string file_path, vector<SQLType> sql_types) :\n-\t\tBaseCSVData(move(file_path)), sql_types(move(sql_types)) {}\n+\tReadCSVData(string file_path, vector<SQLType> sql_types)\n+\t    : BaseCSVData(move(file_path)), sql_types(move(sql_types)) {\n+\t}\n \n \t//! The expected SQL types to read\n \tvector<SQLType> sql_types;\n \t//! True, if column with that index must be quoted\n \tvector<bool> force_not_null;\n+\t//! The size of a sample for format/data type detection (csv)\n+\tint sample_size = DEFAULT_SAMPLE_CHUNK_SIZE;\n+\t//! The number of samples for data type detection (csv)\n+\tint num_samples = 10;\n \t//! The DATE_FORMAT to use to read or write dates\n \tStrpTimeFormat date_format;\n \t//! Whether or not there is a date format specified\n@@ -73,7 +87,7 @@ struct ReadCSVData : public BaseCSVData {\n void SubstringDetection(string &str_1, string &str_2, string name_str_1, string name_str_2) {\n \tif (str_1.find(str_2) != string::npos || str_2.find(str_1) != std::string::npos) {\n \t\tthrow BinderException(\"COPY \" + name_str_1 + \" must not appear in the \" + name_str_2 +\n-\t\t                \" specification and vice versa\");\n+\t\t                      \" specification and vice versa\");\n \t}\n }\n \n@@ -102,34 +116,44 @@ static string ParseString(vector<Value> &set) {\n \treturn set[0].str_value;\n }\n \n+static idx_t ParseInteger(vector<Value> &set) {\n+\tif (set.size() != 1) {\n+\t\t// no option specified or multiple options specified\n+\t\tthrow BinderException(\"Expected a single argument as a integer value\");\n+\t}\n+\tif (set[0].type == TypeId::FLOAT || set[0].type == TypeId::DOUBLE) {\n+\t\tthrow BinderException(\"Expected a integer argument!\");\n+\t}\n+\treturn set[0].CastAs(TypeId::INT64).value_.bigint;\n+}\n+\n //===--------------------------------------------------------------------===//\n // Bind\n //===--------------------------------------------------------------------===//\n static bool ParseBaseOption(BaseCSVData &bind_data, string &loption, vector<Value> &set) {\n \tif (StringUtil::StartsWith(loption, \"delim\") || StringUtil::StartsWith(loption, \"sep\")) {\n \t\tbind_data.delimiter = ParseString(set);\n-\t\tbind_data.is_auto_detect = false;\n+\t\tbind_data.has_delimiter = true;\n \t\tif (bind_data.delimiter.length() == 0) {\n-\t\t\tthrow BinderException(\"QUOTE must not be empty\");\n+\t\t\tthrow BinderException(\"DELIM or SEP must not be empty\");\n \t\t}\n \t} else if (loption == \"quote\") {\n \t\tbind_data.quote = ParseString(set);\n-\t\tbind_data.is_auto_detect = false;\n+\t\tbind_data.has_quote = true;\n \t\tif (bind_data.quote.length() == 0) {\n \t\t\tthrow BinderException(\"QUOTE must not be empty\");\n \t\t}\n \t} else if (loption == \"escape\") {\n \t\tbind_data.escape = ParseString(set);\n-\t\tbind_data.is_auto_detect = false;\n+\t\tbind_data.has_escape = true;\n \t\tif (bind_data.escape.length() == 0) {\n \t\t\tthrow BinderException(\"ESCAPE must not be empty\");\n \t\t}\n \t} else if (loption == \"header\") {\n \t\tbind_data.header = ParseBoolean(set);\n-\t\tbind_data.is_auto_detect = false;\n+\t\tbind_data.has_header = true;\n \t} else if (loption == \"null\") {\n \t\tbind_data.null_str = ParseString(set);\n-\t\tbind_data.is_auto_detect = false;\n \t} else if (loption == \"encoding\") {\n \t\tauto encoding = StringUtil::Lower(ParseString(set));\n \t\tif (encoding != \"utf8\" && encoding != \"utf-8\") {\n@@ -175,18 +199,18 @@ static vector<bool> ParseColumnList(vector<Value> &set, vector<string> &names) {\n \t} else {\n \t\t// list of options: parse the list\n \t\tunordered_map<string, bool> option_map;\n-\t\tfor(idx_t i = 0; i < set.size(); i++) {\n+\t\tfor (idx_t i = 0; i < set.size(); i++) {\n \t\t\toption_map[set[i].ToString()] = false;\n \t\t}\n \t\tresult.resize(names.size(), false);\n-\t\tfor(idx_t i = 0; i < names.size(); i++) {\n+\t\tfor (idx_t i = 0; i < names.size(); i++) {\n \t\t\tauto entry = option_map.find(names[i]);\n \t\t\tif (entry != option_map.end()) {\n \t\t\t\tresult[i] = true;\n \t\t\t\tentry->second = true;\n \t\t\t}\n \t\t}\n-\t\tfor(auto entry : option_map) {\n+\t\tfor (auto entry : option_map) {\n \t\t\tif (!entry.second) {\n \t\t\t\tthrow BinderException(\"Column %s not found in table\", entry.first.c_str());\n \t\t\t}\n@@ -196,11 +220,11 @@ static vector<bool> ParseColumnList(vector<Value> &set, vector<string> &names) {\n }\n \n static unique_ptr<FunctionData> write_csv_bind(ClientContext &context, CopyInfo &info, vector<string> &names,\n-                                           vector<SQLType> &sql_types) {\n+                                               vector<SQLType> &sql_types) {\n \tauto bind_data = make_unique<WriteCSVData>(info.file_path, sql_types, names);\n \n \t// check all the options in the copy info\n-\tfor(auto &option : info.options) {\n+\tfor (auto &option : info.options) {\n \t\tauto loption = StringUtil::Lower(option.first);\n \t\tauto &set = option.second;\n \t\tif (ParseBaseOption(*bind_data, loption, set)) {\n@@ -218,23 +242,41 @@ static unique_ptr<FunctionData> write_csv_bind(ClientContext &context, CopyInfo\n \t\tbind_data->force_quote.resize(names.size(), false);\n \t}\n \tbind_data->Finalize();\n-\tbind_data->is_simple = bind_data->delimiter.size() == 1 && bind_data->escape.size() == 1 && bind_data->quote.size() == 1;\n+\tbind_data->is_simple =\n+\t    bind_data->delimiter.size() == 1 && bind_data->escape.size() == 1 && bind_data->quote.size() == 1;\n \treturn move(bind_data);\n }\n \n-static unique_ptr<FunctionData> read_csv_bind(ClientContext &context, CopyInfo &info, vector<string> &expected_names, vector<SQLType> &expected_types) {\n+static unique_ptr<FunctionData> read_csv_bind(ClientContext &context, CopyInfo &info, vector<string> &expected_names,\n+                                              vector<SQLType> &expected_types) {\n \tauto bind_data = make_unique<ReadCSVData>(info.file_path, expected_types);\n \n \t// check all the options in the copy info\n-\tfor(auto &option : info.options) {\n+\tfor (auto &option : info.options) {\n \t\tauto loption = StringUtil::Lower(option.first);\n \t\tauto &set = option.second;\n-\t\tif (ParseBaseOption(*bind_data, loption, set)) {\n+\t\tif (loption == \"auto_detect\") {\n+\t\t\tbind_data->is_auto_detect = ParseBoolean(set);\n+\t\t} else if (ParseBaseOption(*bind_data, loption, set)) {\n \t\t\t// parsed option in base CSV options: continue\n \t\t\tcontinue;\n+\t\t} else if (loption == \"sample_size\") {\n+\t\t\tbind_data->sample_size = ParseInteger(set);\n+\t\t\tif (bind_data->sample_size > STANDARD_VECTOR_SIZE) {\n+\t\t\t\tthrow BinderException(\n+\t\t\t\t    \"Unsupported parameter for SAMPLE_SIZE: cannot be bigger than STANDARD_VECTOR_SIZE %d\",\n+\t\t\t\t    STANDARD_VECTOR_SIZE);\n+\t\t\t} else if (bind_data->sample_size < 1) {\n+\t\t\t\tthrow BinderException(\"Unsupported parameter for SAMPLE_SIZE: cannot be smaller than 1\");\n+\t\t\t}\n+\t\t} else if (loption == \"num_samples\") {\n+\t\t\tbind_data->num_samples = ParseInteger(set);\n+\t\t\tif (bind_data->num_samples < 1) {\n+\t\t\t\tthrow BinderException(\"Unsupported parameter for NUM_SAMPLES: cannot be smaller than 1\");\n+\t\t\t}\n \t\t} else if (loption == \"force_not_null\") {\n \t\t\tbind_data->force_not_null = ParseColumnList(set, expected_names);\n-\t\t}  else if (loption == \"date_format\" || loption == \"dateformat\") {\n+\t\t} else if (loption == \"date_format\" || loption == \"dateformat\") {\n \t\t\tstring format = ParseString(set);\n \t\t\tstring error = StrTimeFormat::ParseFormatSpecifier(format, bind_data->date_format);\n \t\t\tbind_data->date_format.format_specifier = format;\n@@ -263,20 +305,6 @@ static unique_ptr<FunctionData> read_csv_bind(ClientContext &context, CopyInfo &\n \treturn move(bind_data);\n }\n \n-static unique_ptr<FunctionData> read_csv_auto_bind(ClientContext &context, CopyInfo &info, vector<string> &expected_names, vector<SQLType> &expected_types) {\n-\tauto bind_data = make_unique<ReadCSVData>(info.file_path, expected_types);\n-\n-\tfor(auto &option : info.options) {\n-\t\tauto loption = StringUtil::Lower(option.first);\n-\t\t// auto &set = option.second;\n-\t\t// CSV auto accepts no options!\n-\t\tthrow NotImplementedException(\"Unrecognized option for CSV_AUTO: %s\", option.first.c_str());\n-\t}\n-\n-\tbind_data->Finalize();\n-\treturn move(bind_data);\n-}\n-\n //===--------------------------------------------------------------------===//\n // Helper writing functions\n //===--------------------------------------------------------------------===//\n@@ -339,7 +367,8 @@ static bool RequiresQuotes(WriteCSVData &options, const char *str, idx_t len) {\n \t}\n }\n \n-static void WriteQuotedString(Serializer &serializer, WriteCSVData &options, const char *str, idx_t len, bool force_quote) {\n+static void WriteQuotedString(Serializer &serializer, WriteCSVData &options, const char *str, idx_t len,\n+                              bool force_quote) {\n \tif (!force_quote) {\n \t\t// force quote is disabled: check if we need to add quotes anyway\n \t\tforce_quote = RequiresQuotes(options, str, len);\n@@ -368,7 +397,7 @@ static void WriteQuotedString(Serializer &serializer, WriteCSVData &options, con\n \t\tif (!requires_escape) {\n \t\t\t// fast path: no need to escape anything\n \t\t\tserializer.WriteBufferData(options.quote);\n-\t\t\tserializer.WriteData((const_data_ptr_t) str, len);\n+\t\t\tserializer.WriteData((const_data_ptr_t)str, len);\n \t\t\tserializer.WriteBufferData(options.quote);\n \t\t\treturn;\n \t\t}\n@@ -384,7 +413,7 @@ static void WriteQuotedString(Serializer &serializer, WriteCSVData &options, con\n \t\tserializer.WriteBufferData(new_val);\n \t\tserializer.WriteBufferData(options.quote);\n \t} else {\n-\t\tserializer.WriteData((const_data_ptr_t) str, len);\n+\t\tserializer.WriteData((const_data_ptr_t)str, len);\n \t}\n }\n \n@@ -400,12 +429,13 @@ struct LocalReadCSVData : public LocalFunctionData {\n \n struct GlobalWriteCSVData : public GlobalFunctionData {\n \tGlobalWriteCSVData(FileSystem &fs, string file_path) : fs(fs) {\n-\t\thandle = fs.OpenFile(file_path, FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE_NEW, FileLockType::WRITE_LOCK);\n+\t\thandle = fs.OpenFile(file_path, FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE_NEW,\n+\t\t                     FileLockType::WRITE_LOCK);\n \t}\n \n \tvoid WriteData(const_data_ptr_t data, idx_t size) {\n \t\tlock_guard<mutex> flock(lock);\n-\t\tfs.Write(*handle, (void*) data, size);\n+\t\tfs.Write(*handle, (void *)data, size);\n \t}\n \n \tFileSystem &fs;\n@@ -416,7 +446,7 @@ struct GlobalWriteCSVData : public GlobalFunctionData {\n };\n \n static unique_ptr<LocalFunctionData> write_csv_initialize_local(ClientContext &context, FunctionData &bind_data) {\n-\tauto &csv_data = (WriteCSVData &) bind_data;\n+\tauto &csv_data = (WriteCSVData &)bind_data;\n \tauto local_data = make_unique<LocalReadCSVData>();\n \n \t// create the chunk with VARCHAR types\n@@ -428,8 +458,8 @@ static unique_ptr<LocalFunctionData> write_csv_initialize_local(ClientContext &c\n }\n \n static unique_ptr<GlobalFunctionData> write_csv_initialize_global(ClientContext &context, FunctionData &bind_data) {\n-\tauto &csv_data = (WriteCSVData &) bind_data;\n-\tauto global_data =  make_unique<GlobalWriteCSVData>(FileSystem::GetFileSystem(context), csv_data.file_path);\n+\tauto &csv_data = (WriteCSVData &)bind_data;\n+\tauto global_data = make_unique<GlobalWriteCSVData>(FileSystem::GetFileSystem(context), csv_data.file_path);\n \n \tif (csv_data.header) {\n \t\tBufferedSerializer serializer;\n@@ -447,11 +477,11 @@ static unique_ptr<GlobalFunctionData> write_csv_initialize_global(ClientContext\n \treturn move(global_data);\n }\n \n-\n-static void write_csv_sink(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate, LocalFunctionData &lstate, DataChunk &input) {\n-\tauto &csv_data = (WriteCSVData &) bind_data;\n-\tauto &local_data = (LocalReadCSVData &) lstate;\n-\tauto &global_state = (GlobalWriteCSVData &) gstate;\n+static void write_csv_sink(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,\n+                           LocalFunctionData &lstate, DataChunk &input) {\n+\tauto &csv_data = (WriteCSVData &)bind_data;\n+\tauto &local_data = (LocalReadCSVData &)lstate;\n+\tauto &global_state = (GlobalWriteCSVData &)gstate;\n \n \t// write data into the local buffer\n \n@@ -465,7 +495,7 @@ static void write_csv_sink(ClientContext &context, FunctionData &bind_data, Glob\n \t\t} else {\n \t\t\t// non varchar column, perform the cast\n \t\t\tVectorOperations::Cast(input.data[col_idx], cast_chunk.data[col_idx], csv_data.sql_types[col_idx],\n-\t\t\t\t\t\t\t\t\tSQLType::VARCHAR, input.size());\n+\t\t\t                       SQLType::VARCHAR, input.size());\n \t\t}\n \t}\n \n@@ -487,8 +517,11 @@ static void write_csv_sink(ClientContext &context, FunctionData &bind_data, Glob\n \t\t\t// non-null value, fetch the string value from the cast chunk\n \t\t\tauto str_data = FlatVector::GetData<string_t>(cast_chunk.data[col_idx]);\n \t\t\tauto str_value = str_data[row_idx];\n-\t\t\t// FIXME: we could gain some performance here by checking for certain types if they ever require quotes (e.g. integers only require quotes if the delimiter is a number, decimals only require quotes if the delimiter is a number or \".\" character)\n-\t\t\tWriteQuotedString(writer, csv_data, str_value.GetData(), str_value.GetSize(), csv_data.force_quote[col_idx]);\n+\t\t\t// FIXME: we could gain some performance here by checking for certain types if they ever require quotes\n+\t\t\t// (e.g. integers only require quotes if the delimiter is a number, decimals only require quotes if the\n+\t\t\t// delimiter is a number or \".\" character)\n+\t\t\tWriteQuotedString(writer, csv_data, str_value.GetData(), str_value.GetSize(),\n+\t\t\t                  csv_data.force_quote[col_idx]);\n \t\t}\n \t\twriter.WriteBufferData(csv_data.newline);\n \t}\n@@ -503,9 +536,9 @@ static void write_csv_sink(ClientContext &context, FunctionData &bind_data, Glob\n // Combine\n //===--------------------------------------------------------------------===//\n static void write_csv_combine(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,\n-                          LocalFunctionData &lstate) {\n-\tauto &local_data = (LocalReadCSVData &) lstate;\n-\tauto &global_state = (GlobalWriteCSVData &) gstate;\n+                              LocalFunctionData &lstate) {\n+\tauto &local_data = (LocalReadCSVData &)lstate;\n+\tauto &global_state = (GlobalWriteCSVData &)gstate;\n \tauto &writer = local_data.serializer;\n \t// flush the local writer\n \tif (writer.blob.size > 0) {\n@@ -523,20 +556,26 @@ struct GlobalReadCSVData : public GlobalFunctionData {\n \n unique_ptr<GlobalFunctionData> read_csv_initialize(ClientContext &context, FunctionData &fdata) {\n \tauto global_data = make_unique<GlobalReadCSVData>();\n-\tauto &bind_data = (ReadCSVData&) fdata;\n+\tauto &bind_data = (ReadCSVData &)fdata;\n \n \t// set up the CSV reader with the parsed options\n-    BufferedCSVReaderOptions options;\n+\tBufferedCSVReaderOptions options;\n \toptions.file_path = bind_data.file_path;\n \toptions.auto_detect = bind_data.is_auto_detect;\n+\toptions.has_delimiter = bind_data.has_delimiter;\n \toptions.delimiter = bind_data.delimiter;\n+\toptions.has_quote = bind_data.has_quote;\n \toptions.quote = bind_data.quote;\n+\toptions.has_escape = bind_data.has_escape;\n \toptions.escape = bind_data.escape;\n+\toptions.has_header = bind_data.has_header;\n \toptions.header = bind_data.header;\n \toptions.null_str = bind_data.null_str;\n \toptions.skip_rows = 0;\n \toptions.num_cols = bind_data.sql_types.size();\n \toptions.force_not_null = bind_data.force_not_null;\n+\toptions.sample_size = bind_data.sample_size;\n+\toptions.num_samples = bind_data.num_samples;\n \toptions.has_date_format = bind_data.has_date_format;\n \toptions.date_format = move(bind_data.date_format);\n \toptions.has_timestamp_format = bind_data.has_timestamp_format;\n@@ -546,9 +585,10 @@ unique_ptr<GlobalFunctionData> read_csv_initialize(ClientContext &context, Funct\n \treturn move(global_data);\n }\n \n-void read_csv_get_chunk(ExecutionContext &context, GlobalFunctionData &gstate, FunctionData &bind_data, DataChunk &chunk) {\n+void read_csv_get_chunk(ExecutionContext &context, GlobalFunctionData &gstate, FunctionData &bind_data,\n+                        DataChunk &chunk) {\n \t// read a chunk from the CSV reader\n-\tauto &gdata = (GlobalReadCSVData &) gstate;\n+\tauto &gdata = (GlobalReadCSVData &)gstate;\n \tgdata.csv_reader->ParseCSV(chunk);\n }\n \n@@ -564,14 +604,7 @@ void CSVCopyFunction::RegisterFunction(BuiltinFunctions &set) {\n \tinfo.copy_from_initialize = read_csv_initialize;\n \tinfo.copy_from_get_chunk = read_csv_get_chunk;\n \n-\t// CSV_AUTO can only be used in COPY FROM\n-\tCopyFunction auto_info(\"csv_auto\");\n-\tauto_info.copy_from_bind = read_csv_auto_bind;\n-\tauto_info.copy_from_initialize = read_csv_initialize;\n-\tauto_info.copy_from_get_chunk = read_csv_get_chunk;\n-\n \tset.AddFunction(info);\n-\tset.AddFunction(auto_info);\n }\n \n } // namespace duckdb\ndiff --git a/src/function/table/read_csv.cpp b/src/function/table/read_csv.cpp\nindex f5e6723ee56c..8a946d2931d1 100644\n--- a/src/function/table/read_csv.cpp\n+++ b/src/function/table/read_csv.cpp\n@@ -16,7 +16,8 @@ struct ReadCSVFunctionData : public TableFunctionData {\n \tunique_ptr<BufferedCSVReader> csv_reader;\n };\n \n-static unique_ptr<FunctionData> read_csv_bind(ClientContext &context, vector<Value> &inputs, unordered_map<string, Value> &named_parameters,\n+static unique_ptr<FunctionData> read_csv_bind(ClientContext &context, vector<Value> &inputs,\n+                                              unordered_map<string, Value> &named_parameters,\n                                               vector<SQLType> &return_types, vector<string> &names) {\n \n \tif (!context.db.config.enable_copy) {\n@@ -26,27 +27,42 @@ static unique_ptr<FunctionData> read_csv_bind(ClientContext &context, vector<Val\n \n \tBufferedCSVReaderOptions options;\n \toptions.file_path = inputs[0].str_value;\n-\toptions.auto_detect = true;\n+\toptions.auto_detect = false;\n \toptions.header = false;\n \toptions.delimiter = \",\";\n \toptions.quote = \"\\\"\";\n \n-\tfor(auto &kv : named_parameters) {\n-\t\tif (kv.first == \"sep\") {\n-\t\t\toptions.auto_detect = false;\n+\tfor (auto &kv : named_parameters) {\n+\t\tif (kv.first == \"auto_detect\") {\n+\t\t\toptions.auto_detect = kv.second.value_.boolean;\n+\t\t} else if (kv.first == \"sep\" || kv.first == \"delim\") {\n \t\t\toptions.delimiter = kv.second.str_value;\n+\t\t\toptions.has_delimiter = true;\n \t\t} else if (kv.first == \"header\") {\n-\t\t\toptions.auto_detect = false;\n \t\t\toptions.header = kv.second.value_.boolean;\n+\t\t\toptions.has_header = true;\n \t\t} else if (kv.first == \"quote\") {\n-\t\t\toptions.auto_detect = false;\n \t\t\toptions.quote = kv.second.str_value;\n+\t\t\toptions.has_quote = true;\n \t\t} else if (kv.first == \"escape\") {\n-\t\t\toptions.auto_detect = false;\n \t\t\toptions.escape = kv.second.str_value;\n+\t\t\toptions.has_escape = true;\n \t\t} else if (kv.first == \"nullstr\") {\n-\t\t\toptions.auto_detect = false;\n \t\t\toptions.null_str = kv.second.str_value;\n+\t\t} else if (kv.first == \"sample_size\") {\n+\t\t\toptions.sample_size = kv.second.CastAs(TypeId::INT64).value_.bigint;\n+\t\t\tif (options.sample_size > STANDARD_VECTOR_SIZE) {\n+\t\t\t\tthrow BinderException(\n+\t\t\t\t    \"Unsupported parameter for SAMPLE_SIZE: cannot be bigger than STANDARD_VECTOR_SIZE %d\",\n+\t\t\t\t    STANDARD_VECTOR_SIZE);\n+\t\t\t} else if (options.sample_size < 1) {\n+\t\t\t\tthrow BinderException(\"Unsupported parameter for SAMPLE_SIZE: cannot be smaller than 1\");\n+\t\t\t}\n+\t\t} else if (kv.first == \"num_samples\") {\n+\t\t\toptions.num_samples = kv.second.CastAs(TypeId::INT64).value_.bigint;\n+\t\t\tif (options.num_samples < 1) {\n+\t\t\t\tthrow BinderException(\"Unsupported parameter for NUM_SAMPLES: cannot be smaller than 1\");\n+\t\t\t}\n \t\t} else if (kv.first == \"dateformat\") {\n \t\t\toptions.has_date_format = true;\n \t\t\toptions.date_format.format_specifier = kv.second.str_value;\n@@ -62,7 +78,6 @@ static unique_ptr<FunctionData> read_csv_bind(ClientContext &context, vector<Val\n \t\t\t\tthrow InvalidInputException(\"Could not parse TIMESTAMPFORMAT: %s\", error.c_str());\n \t\t\t}\n \t\t} else if (kv.first == \"columns\") {\n-\t\t\toptions.auto_detect = false;\n \t\t\tfor (auto &val : kv.second.struct_value) {\n \t\t\t\tnames.push_back(val.first);\n \t\t\t\tif (val.second.type != TypeId::VARCHAR) {\n@@ -91,24 +106,11 @@ static unique_ptr<FunctionData> read_csv_bind(ClientContext &context, vector<Val\n \treturn move(result);\n }\n \n-static unique_ptr<FunctionData> read_csv_auto_bind(ClientContext &context, vector<Value> &inputs, unordered_map<string, Value> &named_parameters,\n+static unique_ptr<FunctionData> read_csv_auto_bind(ClientContext &context, vector<Value> &inputs,\n+                                                   unordered_map<string, Value> &named_parameters,\n                                                    vector<SQLType> &return_types, vector<string> &names) {\n-\n-\tif (!context.db.config.enable_copy) {\n-\t\tthrow Exception(\"read_csv_auto is disabled by configuration\");\n-\t}\n-\tauto result = make_unique<ReadCSVFunctionData>();\n-\tBufferedCSVReaderOptions options;\n-\toptions.auto_detect = true;\n-\toptions.file_path = inputs[0].str_value;\n-\n-\tresult->csv_reader = make_unique<BufferedCSVReader>(context, move(options));\n-\n-\t// TODO: print detected dialect from result->csv_reader->info\n-\treturn_types.assign(result->csv_reader->sql_types.begin(), result->csv_reader->sql_types.end());\n-\tnames.assign(result->csv_reader->col_names.begin(), result->csv_reader->col_names.end());\n-\n-\treturn move(result);\n+\tnamed_parameters[\"auto_detect\"] = Value::BOOLEAN(true);\n+\treturn read_csv_bind(context, inputs, named_parameters, return_types, names);\n }\n \n static void read_csv_info(ClientContext &context, vector<Value> &input, DataChunk &output, FunctionData *dataptr) {\n@@ -116,23 +118,31 @@ static void read_csv_info(ClientContext &context, vector<Value> &input, DataChun\n \tdata.csv_reader->ParseCSV(output);\n }\n \n+static void add_named_parameters(TableFunction &table_function) {\n+\ttable_function.named_parameters[\"sep\"] = SQLType::VARCHAR;\n+\ttable_function.named_parameters[\"delim\"] = SQLType::VARCHAR;\n+\ttable_function.named_parameters[\"quote\"] = SQLType::VARCHAR;\n+\ttable_function.named_parameters[\"escape\"] = SQLType::VARCHAR;\n+\ttable_function.named_parameters[\"nullstr\"] = SQLType::VARCHAR;\n+\ttable_function.named_parameters[\"columns\"] = SQLType::STRUCT;\n+\ttable_function.named_parameters[\"header\"] = SQLType::BOOLEAN;\n+\ttable_function.named_parameters[\"auto_detect\"] = SQLType::BOOLEAN;\n+\ttable_function.named_parameters[\"sample_size\"] = SQLType::BIGINT;\n+\ttable_function.named_parameters[\"num_samples\"] = SQLType::BIGINT;\n+\ttable_function.named_parameters[\"dateformat\"] = SQLType::VARCHAR;\n+\ttable_function.named_parameters[\"timestampformat\"] = SQLType::VARCHAR;\n+}\n+\n void ReadCSVTableFunction::RegisterFunction(BuiltinFunctions &set) {\n-\tTableFunctionSet read_csv(\"read_csv\");\n-\n-\tTableFunction read_csv_function = TableFunction({SQLType::VARCHAR}, read_csv_bind, read_csv_info, nullptr);\n-\tread_csv_function.named_parameters[\"sep\"] = SQLType::VARCHAR;\n-\tread_csv_function.named_parameters[\"quote\"] = SQLType::VARCHAR;\n-\tread_csv_function.named_parameters[\"escape\"] = SQLType::VARCHAR;\n-\tread_csv_function.named_parameters[\"nullstr\"] = SQLType::VARCHAR;\n-\tread_csv_function.named_parameters[\"columns\"] = SQLType::STRUCT;\n-\tread_csv_function.named_parameters[\"header\"] = SQLType::BOOLEAN;\n-\tread_csv_function.named_parameters[\"dateformat\"] = SQLType::VARCHAR;\n-\tread_csv_function.named_parameters[\"timestampformat\"] = SQLType::VARCHAR;\n-\n-\tread_csv.AddFunction(move(read_csv_function));\n-\n-\tset.AddFunction(read_csv);\n-\tset.AddFunction(TableFunction(\"read_csv_auto\", {SQLType::VARCHAR}, read_csv_auto_bind, read_csv_info, nullptr));\n+\tTableFunction read_csv_function =\n+\t    TableFunction(\"read_csv\", {SQLType::VARCHAR}, read_csv_bind, read_csv_info, nullptr);\n+\tadd_named_parameters(read_csv_function);\n+\tset.AddFunction(read_csv_function);\n+\n+\tTableFunction read_csv_auto_function =\n+\t    TableFunction(\"read_csv_auto\", {SQLType::VARCHAR}, read_csv_auto_bind, read_csv_info, nullptr);\n+\tadd_named_parameters(read_csv_auto_function);\n+\tset.AddFunction(read_csv_auto_function);\n }\n \n void BuiltinFunctions::RegisterReadFunctions() {\ndiff --git a/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp b/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp\nindex 388162d386fd..5eddf2eb2ca3 100644\n--- a/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp\n+++ b/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp\n@@ -14,10 +14,10 @@\n \n #include <sstream>\n \n-#define SAMPLE_CHUNK_SIZE 100\n-#if STANDARD_VECTOR_SIZE < SAMPLE_CHUNK_SIZE\n-#undef SAMPLE_CHUNK_SIZE\n-#define SAMPLE_CHUNK_SIZE STANDARD_VECTOR_SIZE\n+#define DEFAULT_SAMPLE_CHUNK_SIZE 100\n+#if STANDARD_VECTOR_SIZE < DEFAULT_SAMPLE_CHUNK_SIZE\n+#undef DEFAULT_SAMPLE_CHUNK_SIZE\n+#define DEFAULT_SAMPLE_CHUNK_SIZE STANDARD_VECTOR_SIZE\n #endif\n \n namespace duckdb {\n@@ -39,6 +39,9 @@ struct TextSearchShiftArray {\n \tTextSearchShiftArray(string search_term);\n \n \tinline bool Match(uint8_t &position, uint8_t byte_value) {\n+\t\tif (position >= length) {\n+\t\t\treturn false;\n+\t\t}\n \t\tposition = shifts[position * 255 + byte_value];\n \t\treturn position == length;\n \t}\n@@ -47,27 +50,39 @@ struct TextSearchShiftArray {\n \tunique_ptr<uint8_t[]> shifts;\n };\n \n-struct BufferedCSVReaderOptions  {\n+struct BufferedCSVReaderOptions {\n \t//! The file path of the CSV file to read\n \tstring file_path;\n-    //! Whether or not to automatically detect dialect and datatypes\n-    bool auto_detect;\n-    //! Delimiter to separate columns within each line\n-    string delimiter;\n-    //! Quote used for columns that contain reserved characters, e.g., delimiter\n-    string quote;\n-    //! Escape character to escape quote character\n-    string escape;\n-    //! Whether or not the file has a header line\n-    bool header = false;\n-    //! How many leading rows to skip\n-    idx_t skip_rows = 0;\n-    //! Expected number of columns\n-    idx_t num_cols = 0;\n-    //! Specifies the string that represents a null value\n-    string null_str;\n-    //! True, if column with that index must skip null check\n-    vector<bool> force_not_null;\n+\t//! Whether or not to automatically detect dialect and datatypes\n+\tbool auto_detect = true;\n+\t//! Whether or not a delimiter was defined by the user\n+\tbool has_delimiter = false;\n+\t//! Delimiter to separate columns within each line\n+\tstring delimiter;\n+\t//! Whether or not a quote sign was defined by the user\n+\tbool has_quote = false;\n+\t//! Quote used for columns that contain reserved characters, e.g., delimiter\n+\tstring quote;\n+\t//! Whether or not an escape character was defined by the user\n+\tbool has_escape = false;\n+\t//! Escape character to escape quote character\n+\tstring escape;\n+\t//! Whether or not a header information was given by the user\n+\tbool has_header = false;\n+\t//! Whether or not the file has a header line\n+\tbool header = false;\n+\t//! How many leading rows to skip\n+\tidx_t skip_rows = 0;\n+\t//! Expected number of columns\n+\tidx_t num_cols = 0;\n+\t//! Specifies the string that represents a null value\n+\tstring null_str;\n+\t//! True, if column with that index must skip null check\n+\tvector<bool> force_not_null;\n+\t//! Size of sample chunk used for dialect and type detection\n+\tidx_t sample_size = DEFAULT_SAMPLE_CHUNK_SIZE;\n+\t//! Number of sample chunks used for type detection\n+\tidx_t num_samples = 10;\n \t//! The date format to use (if any is specified)\n \tStrpTimeFormat date_format;\n \t//! Whether or not a date format is specified\n@@ -90,12 +105,22 @@ class BufferedCSVReader {\n \tstatic constexpr idx_t INITIAL_BUFFER_SIZE = 16384;\n \t//! Maximum CSV line size: specified because if we reach this amount, we likely have the wrong delimiters\n \tstatic constexpr idx_t MAXIMUM_CSV_LINE_SIZE = 1048576;\n-\tstatic constexpr uint8_t MAX_SAMPLE_CHUNKS = 10;\n \tParserMode mode;\n \n+\t//! Candidates for delimiter auto detection\n+\tvector<string> delim_candidates = {\",\", \"|\", \";\", \"\\t\"};\n+\t//! Candidates for quote rule auto detection\n+\tvector<QuoteRule> quoterule_candidates = {QuoteRule::QUOTES_RFC, QuoteRule::QUOTES_OTHER, QuoteRule::NO_QUOTES};\n+\t//! Candidates for quote sign auto detection (per quote rule)\n+\tvector<vector<string>> quote_candidates_map = {{\"\\\"\"}, {\"\\\"\", \"'\"}, {\"\"}};\n+\t//! Candidates for escape character auto detection (per quote rule)\n+\tvector<vector<string>> escape_candidates_map = {{\"\"}, {\"\\\\\"}, {\"\"}};\n+\n public:\n-\tBufferedCSVReader(ClientContext &context, BufferedCSVReaderOptions options, vector<SQLType> requested_types = vector<SQLType>());\n-\tBufferedCSVReader(BufferedCSVReaderOptions options, vector<SQLType> requested_types, unique_ptr<std::istream> source);\n+\tBufferedCSVReader(ClientContext &context, BufferedCSVReaderOptions options,\n+\t                  vector<SQLType> requested_types = vector<SQLType>());\n+\tBufferedCSVReader(BufferedCSVReaderOptions options, vector<SQLType> requested_types,\n+\t                  unique_ptr<std::istream> source);\n \n \tBufferedCSVReaderOptions options;\n \tvector<SQLType> sql_types;\n@@ -112,9 +137,13 @@ class BufferedCSVReader {\n \tidx_t linenr = 0;\n \tbool linenr_estimated = false;\n \n+\tidx_t SAMPLE_CHUNK_SIZE;\n+\tidx_t MAX_SAMPLE_CHUNKS;\n+\n \tvector<idx_t> sniffed_column_counts;\n \tuint8_t sample_chunk_idx = 0;\n \tbool jumping_samples = false;\n+\tbool end_of_file_reached = false;\n \n \tidx_t bytes_in_chunk = 0;\n \tdouble bytes_per_line_avg = 0;\n@@ -142,10 +171,12 @@ class BufferedCSVReader {\n \tvector<SQLType> SniffCSV(vector<SQLType> requested_types);\n \t//! Try to cast a string value to the specified sql type\n \tbool TryCastValue(Value value, SQLType sql_type);\n+\t//! Try to cast a vector of values to the specified sql type\n+\tbool TryCastVector(Vector &parse_chunk_col, idx_t size, SQLType sql_type);\n \t//! Skips header rows and skip_rows in the input stream\n-\tvoid SkipHeader();\n+\tvoid SkipHeader(idx_t skip_rows, bool skip_header);\n \t//! Jumps back to the beginning of input stream and resets necessary internal states\n-\tvoid JumpToBeginning();\n+\tvoid JumpToBeginning(idx_t skip_rows, bool skip_header);\n \t//! Jumps back to the beginning of input stream and resets necessary internal states\n \tbool JumpToNextSample();\n \t//! Resets the buffer\n@@ -154,6 +185,10 @@ class BufferedCSVReader {\n \tvoid ResetStream();\n \t//! Resets the parse_chunk and related internal states, keep_types keeps the parse_chunk initialized\n \tvoid ResetParseChunk();\n+\t//! Sets size of sample chunk and number of chunks for dialect and type detection\n+\tvoid ConfigureSampling();\n+\t//! Prepare candidate sets for auto detection based on user input\n+\tvoid PrepareCandidateSets();\n \n \t//! Parses a CSV file with a one-byte delimiter, escape and quote character\n \tvoid ParseSimpleCSV(DataChunk &insert_chunk);\ndiff --git a/src/parser/transform/statement/transform_copy.cpp b/src/parser/transform/statement/transform_copy.cpp\nindex 66a2f81365cb..9e1fce5c2ad2 100644\n--- a/src/parser/transform/statement/transform_copy.cpp\n+++ b/src/parser/transform/statement/transform_copy.cpp\n@@ -68,8 +68,7 @@ unique_ptr<CopyStatement> Transformer::TransformCopy(PGNode *node) {\n \t\t\t\t// format specifier: interpret this option\n \t\t\t\tauto *format_val = (PGValue *)(def_elem->arg);\n \t\t\t\tif (!format_val || format_val->type != T_PGString) {\n-\t\t\t\t\tthrow ParserException(\n-\t\t\t\t\t    \"Unsupported parameter type for FORMAT: expected e.g. FORMAT 'csv', 'csv_auto'\");\n+\t\t\t\t\tthrow ParserException(\"Unsupported parameter type for FORMAT: expected e.g. FORMAT 'csv', 'parquet'\");\n \t\t\t\t}\n \t\t\t\tinfo.format = StringUtil::Lower(format_val->val.str);\n \t\t\t\tcontinue;\n",
  "test_patch": "diff --git a/test/sql/copy/csv/auto/test_auto_greek_ncvoter.test b/test/sql/copy/csv/auto/test_auto_greek_ncvoter.test\nindex ca3530665b69..8e9499c73d48 100644\n--- a/test/sql/copy/csv/auto/test_auto_greek_ncvoter.test\n+++ b/test/sql/copy/csv/auto/test_auto_greek_ncvoter.test\n@@ -8,7 +8,7 @@ statement ok\n CREATE TABLE IF NOT EXISTS ncvoters(county_id INTEGER, county_desc STRING, voter_reg_num STRING,status_cd STRING, voter_status_desc STRING, reason_cd STRING, voter_status_reason_desc STRING, absent_ind STRING, name_prefx_cd STRING,last_name STRING, first_name STRING, midl_name STRING, name_sufx_cd STRING, full_name_rep STRING,full_name_mail STRING, house_num STRING, half_code STRING, street_dir STRING, street_name STRING, street_type_cd STRING, street_sufx_cd STRING, unit_designator STRING, unit_num STRING, res_city_desc STRING,state_cd STRING, zip_code STRING, res_street_address STRING, res_city_state_zip STRING, mail_addr1 STRING, mail_addr2 STRING, mail_addr3 STRING, mail_addr4 STRING, mail_city STRING, mail_state STRING, mail_zipcode STRING, mail_city_state_zip STRING, area_cd STRING, phone_num STRING, full_phone_number STRING, drivers_lic STRING, race_code STRING, race_desc STRING, ethnic_code STRING, ethnic_desc STRING, party_cd STRING, party_desc STRING, sex_code STRING, sex STRING, birth_age STRING, birth_place STRING, registr_dt STRING, precinct_abbrv STRING, precinct_desc STRING,municipality_abbrv STRING, municipality_desc STRING, ward_abbrv STRING, ward_desc STRING, cong_dist_abbrv STRING, cong_dist_desc STRING, super_court_abbrv STRING, super_court_desc STRING, judic_dist_abbrv STRING, judic_dist_desc STRING, nc_senate_abbrv STRING, nc_senate_desc STRING, nc_house_abbrv STRING, nc_house_desc STRING,county_commiss_abbrv STRING, county_commiss_desc STRING, township_abbrv STRING, township_desc STRING,school_dist_abbrv STRING, school_dist_desc STRING, fire_dist_abbrv STRING, fire_dist_desc STRING, water_dist_abbrv STRING, water_dist_desc STRING, sewer_dist_abbrv STRING, sewer_dist_desc STRING, sanit_dist_abbrv STRING, sanit_dist_desc STRING, rescue_dist_abbrv STRING, rescue_dist_desc STRING, munic_dist_abbrv STRING, munic_dist_desc STRING, dist_1_abbrv STRING, dist_1_desc STRING, dist_2_abbrv STRING, dist_2_desc STRING, confidential_ind STRING, age STRING, ncid STRING, vtd_abbrv STRING, vtd_desc STRING);\n \n query I\n-COPY ncvoters FROM 'test/sql/copy/csv/data/real/ncvoter.csv' (FORMAT CSV_AUTO);\n+COPY ncvoters FROM 'test/sql/copy/csv/data/real/ncvoter.csv' (FORMAT CSV, AUTO_DETECT TRUE);\n ----\n 10\n \ndiff --git a/test/sql/copy/csv/auto/test_auto_lineitem.test b/test/sql/copy/csv/auto/test_auto_lineitem.test\nindex e4b282258555..933f39222702 100644\n--- a/test/sql/copy/csv/auto/test_auto_lineitem.test\n+++ b/test/sql/copy/csv/auto/test_auto_lineitem.test\n@@ -8,7 +8,7 @@ statement ok\n CREATE TABLE lineitem(l_orderkey INT NOT NULL, l_partkey INT NOT NULL, l_suppkey INT NOT NULL, l_linenumber INT NOT NULL, l_quantity INTEGER NOT NULL, l_extendedprice DECIMAL(15,2) NOT NULL, l_discount DECIMAL(15,2) NOT NULL, l_tax DECIMAL(15,2) NOT NULL, l_returnflag VARCHAR(1) NOT NULL, l_linestatus VARCHAR(1) NOT NULL, l_shipdate DATE NOT NULL, l_commitdate DATE NOT NULL, l_receiptdate DATE NOT NULL, l_shipinstruct VARCHAR(25) NOT NULL, l_shipmode VARCHAR(10) NOT NULL, l_comment VARCHAR(44) NOT NULL);\n \n query I\n-COPY lineitem FROM 'test/sql/copy/csv/data/real/lineitem_sample.csv' (FORMAT CSV_AUTO);\n+COPY lineitem FROM 'test/sql/copy/csv/data/real/lineitem_sample.csv' (FORMAT CSV, AUTO_DETECT TRUE);\n ----\n 10\n \ndiff --git a/test/sql/copy/csv/auto/test_auto_ontime.test b/test/sql/copy/csv/auto/test_auto_ontime.test\nindex db6bc1e1d2e9..2c96ad3c3d7a 100644\n--- a/test/sql/copy/csv/auto/test_auto_ontime.test\n+++ b/test/sql/copy/csv/auto/test_auto_ontime.test\n@@ -8,7 +8,7 @@ statement ok\n CREATE TABLE ontime(year SMALLINT, quarter SMALLINT, month SMALLINT, dayofmonth SMALLINT, dayofweek SMALLINT, flightdate DATE, uniquecarrier CHAR(7), airlineid DECIMAL(8,2), carrier CHAR(2), tailnum VARCHAR(50), flightnum VARCHAR(10), originairportid INTEGER, originairportseqid INTEGER, origincitymarketid INTEGER, origin CHAR(5), origincityname VARCHAR(100), originstate CHAR(2), originstatefips VARCHAR(10), originstatename VARCHAR(100), originwac DECIMAL(8,2), destairportid INTEGER, destairportseqid INTEGER, destcitymarketid INTEGER, dest CHAR(5), destcityname VARCHAR(100), deststate CHAR(2), deststatefips VARCHAR(10), deststatename VARCHAR(100), destwac DECIMAL(8,2), crsdeptime DECIMAL(8,2), deptime DECIMAL(8,2), depdelay DECIMAL(8,2), depdelayminutes DECIMAL(8,2), depdel15 DECIMAL(8,2), departuredelaygroups DECIMAL(8,2), deptimeblk VARCHAR(20), taxiout DECIMAL(8,2), wheelsoff DECIMAL(8,2), wheelson DECIMAL(8,2), taxiin DECIMAL(8,2), crsarrtime DECIMAL(8,2), arrtime DECIMAL(8,2), arrdelay DECIMAL(8,2), arrdelayminutes DECIMAL(8,2), arrdel15 DECIMAL(8,2), arrivaldelaygroups DECIMAL(8,2), arrtimeblk VARCHAR(20), cancelled DECIMAL(8,2), cancellationcode CHAR(1), diverted DECIMAL(8,2), crselapsedtime DECIMAL(8,2), actualelapsedtime DECIMAL(8,2), airtime DECIMAL(8,2), flights DECIMAL(8,2), distance DECIMAL(8,2), distancegroup DECIMAL(8,2), carrierdelay DECIMAL(8,2), weatherdelay DECIMAL(8,2), nasdelay DECIMAL(8,2), securitydelay DECIMAL(8,2), lateaircraftdelay DECIMAL(8,2), firstdeptime VARCHAR(10), totaladdgtime VARCHAR(10), longestaddgtime VARCHAR(10), divairportlandings VARCHAR(10), divreacheddest VARCHAR(10), divactualelapsedtime VARCHAR(10), divarrdelay VARCHAR(10), divdistance VARCHAR(10), div1airport VARCHAR(10), div1aiportid INTEGER, div1airportseqid INTEGER, div1wheelson VARCHAR(10), div1totalgtime VARCHAR(10), div1longestgtime VARCHAR(10), div1wheelsoff VARCHAR(10), div1tailnum VARCHAR(10), div2airport VARCHAR(10), div2airportid INTEGER, div2airportseqid INTEGER, div2wheelson VARCHAR(10), div2totalgtime VARCHAR(10), div2longestgtime VARCHAR(10), div2wheelsoff VARCHAR(10), div2tailnum VARCHAR(10), div3airport VARCHAR(10), div3airportid INTEGER, div3airportseqid INTEGER, div3wheelson VARCHAR(10), div3totalgtime VARCHAR(10), div3longestgtime VARCHAR(10), div3wheelsoff VARCHAR(10), div3tailnum VARCHAR(10), div4airport VARCHAR(10), div4airportid INTEGER, div4airportseqid INTEGER, div4wheelson VARCHAR(10), div4totalgtime VARCHAR(10), div4longestgtime VARCHAR(10), div4wheelsoff VARCHAR(10), div4tailnum VARCHAR(10), div5airport VARCHAR(10), div5airportid INTEGER, div5airportseqid INTEGER, div5wheelson VARCHAR(10), div5totalgtime VARCHAR(10), div5longestgtime VARCHAR(10), div5wheelsoff VARCHAR(10), div5tailnum VARCHAR(10));\n \n query I\n-COPY ontime FROM 'test/sql/copy/csv/data/real/ontime_sample.csv' (FORMAT CSV_AUTO);\n+COPY ontime FROM 'test/sql/copy/csv/data/real/ontime_sample.csv' (HEADER TRUE);\n ----\n 9\n \ndiff --git a/test/sql/copy/csv/auto/test_csv_auto.test b/test/sql/copy/csv/auto/test_csv_auto.test\nindex 45466ae17da4..d02d7275d3ed 100644\n--- a/test/sql/copy/csv/auto/test_csv_auto.test\n+++ b/test/sql/copy/csv/auto/test_csv_auto.test\n@@ -21,7 +21,7 @@ DROP TABLE test;\n # CSV file with RFC-conform dialect quote\n # read_csv is an alias to read_csv_auto when no extra parameters are supplied\n statement ok\n-CREATE TABLE test AS SELECT * FROM read_csv ('test/sql/copy/csv/data/auto/rfc_conform_quote.csv');\n+CREATE TABLE test AS SELECT * FROM read_csv_auto ('test/sql/copy/csv/data/auto/rfc_conform_quote.csv');\n \n query ITT\n SELECT * FROM test ORDER BY column0;\ndiff --git a/test/sql/copy/csv/auto/test_sample_size.test b/test/sql/copy/csv/auto/test_sample_size.test\nnew file mode 100644\nindex 000000000000..cd9bfca256c5\n--- /dev/null\n+++ b/test/sql/copy/csv/auto/test_sample_size.test\n@@ -0,0 +1,68 @@\n+# name: test/sql/copy/csv/auto/test_optional_params.test\n+# description: Test optional parameters for read csv\n+# group: [auto]\n+\n+require vector_size 512\n+\n+# CSV file with very sparse column\n+statement error\n+CREATE TABLE test AS SELECT * FROM read_csv_auto ('test/sql/copy/csv/data/auto/issue_811.csv');\n+\n+# CSV file with very sparse column and sample size 500\n+statement ok\n+CREATE TABLE test AS SELECT * FROM read_csv_auto ('test/sql/copy/csv/data/auto/issue_811.csv', SAMPLE_SIZE = 500);\n+\n+query IRTT\n+SELECT TestInteger, TestDouble, TestDate, TestText FROM test WHERE TestDouble is not NULL ;\n+----\n+5\t1.1\t01.05.2015\tfdf\n+\n+query TTTT\n+SELECT typeof(TestInteger), typeof(TestDouble), typeof(TestDate), typeof(TestText) FROM test LIMIT 1;\n+----\n+INTEGER\tDOUBLE\tVARCHAR\tVARCHAR\n+\n+statement ok\n+drop table test;\n+\n+# CSV file with very sparse column and number of samples 50\n+statement ok\n+CREATE TABLE test AS SELECT * FROM read_csv_auto ('test/sql/copy/csv/data/auto/issue_811.csv', NUM_SAMPLES = 50);\n+\n+query IRTT\n+SELECT TestInteger, TestDouble, TestDate, TestText FROM test WHERE TestDouble is not NULL ;\n+----\n+5\t1.1\t01.05.2015\tfdf\n+\n+query TTTT\n+SELECT typeof(TestInteger), typeof(TestDouble), typeof(TestDate), typeof(TestText) FROM test LIMIT 1;\n+----\n+INTEGER\tDOUBLE\tVARCHAR\tVARCHAR\n+\n+statement ok\n+drop table test;\n+\n+# CSV file with very sparse column with sample size 200 and number of samples 20\n+statement ok\n+CREATE TABLE test AS SELECT * FROM read_csv_auto ('test/sql/copy/csv/data/auto/issue_811.csv', SAMPLE_SIZE = 200, NUM_SAMPLES = 20);\n+\n+query IRTT\n+SELECT TestInteger, TestDouble, TestDate, TestText FROM test WHERE TestDouble is not NULL ;\n+----\n+5\t1.1\t01.05.2015\tfdf\n+\n+query TTTT\n+SELECT typeof(TestInteger), typeof(TestDouble), typeof(TestDate), typeof(TestText) FROM test LIMIT 1;\n+----\n+INTEGER\tDOUBLE\tVARCHAR\tVARCHAR\n+\n+statement ok\n+drop table test;\n+\n+# CSV file with very sparse column using copy into\n+statement ok\n+CREATE TABLE test (TestInteger integer, TestDouble double, TestDate varchar, TestText varchar);\n+\n+# CSV file with very sparse column, automatically aligns column types, small sample size\n+statement ok\n+COPY test FROM 'test/sql/copy/csv/data/auto/issue_811.csv' (SAMPLE_SIZE 2, AUTO_DETECT TRUE);\ndiff --git a/test/sql/copy/csv/data/auto/issue_811.csv b/test/sql/copy/csv/data/auto/issue_811.csv\nnew file mode 100644\nindex 000000000000..6037cabf8e89\n--- /dev/null\n+++ b/test/sql/copy/csv/data/auto/issue_811.csv\n@@ -0,0 +1,2233 @@\n+TestInteger,TestDouble,TestDate,TestText\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,1.1,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\n+1,,01.01.2012,asd\n+2,,01.02.2012,\"sdfsd,sfd\"\n+3,,01.03.2013,sdfa\n+4,,01.04.2014,asdf\n+5,,01.05.2015,fdf\n+6,,01.06.2016,ref\ndiff --git a/test/sql/copy/csv/test_abac.test b/test/sql/copy/csv/test_abac.test\nindex aeb9a54f2fb3..4e7247a0176b 100644\n--- a/test/sql/copy/csv/test_abac.test\n+++ b/test/sql/copy/csv/test_abac.test\n@@ -9,7 +9,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR, b VARCHAR, c VARCHAR);\n \n query I\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/abac.csv' DELIMITER 'ABAC';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/abac.csv' (DELIMITER 'ABAC', AUTO_DETECT FALSE);\n ----\n 1\n \n@@ -23,7 +23,7 @@ statement ok\n DELETE FROM abac_tbl;\n \n query I\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/abac.csv' DELIMITER 'ABAC' QUOTE 'ABABABABABAB';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/abac.csv' (DELIMITER 'ABAC', QUOTE 'ABABABABABAB', AUTO_DETECT FALSE);\n ----\n 1\n \n@@ -47,7 +47,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR, b VARCHAR, c VARCHAR);\n \n query I\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/abac_mix.csv' DELIMITER 'ABAD' QUOTE 'ABAB' ESCAPE 'ABAC';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/abac_mix.csv' (DELIMITER 'ABAD', QUOTE 'ABAB', ESCAPE 'ABAC', AUTO_DETECT FALSE);\n ----\n 1\n \n@@ -65,7 +65,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR);\n \n query I\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/abac_incomplete_quote.csv' QUOTE 'ABABABABAB';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/abac_incomplete_quote.csv' (QUOTE 'ABABABABAB', AUTO_DETECT FALSE);\n ----\n 1\n \n@@ -84,7 +84,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR);\n \n query I\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/abac_newline_in_quote.csv' QUOTE 'ABABABABAB';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/abac_newline_in_quote.csv' (QUOTE 'ABABABABAB', AUTO_DETECT FALSE);\n ----\n 2\n \n@@ -102,7 +102,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR);\n \n statement error\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/simple_unterminated_quote.csv' QUOTE '\"';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/simple_unterminated_quote.csv' (QUOTE '\"', AUTO_DETECT FALSE);\n \n statement ok\n DROP TABLE abac_tbl\n@@ -113,7 +113,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR);\n \n query I\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/file_ends_in_quoted_value.csv' QUOTE '\"';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/file_ends_in_quoted_value.csv' (QUOTE '\"', AUTO_DETECT FALSE);\n ----\n 1\n \n@@ -131,7 +131,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR);\n \n query I\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/file_ends_in_quoted_value.csv' QUOTE '\"' DELIMITER 'AAAB';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/file_ends_in_quoted_value.csv' (QUOTE '\"', DELIMITER 'AAAB', AUTO_DETECT FALSE);\n ----\n 1\n \n@@ -149,7 +149,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR);\n \n statement error\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/unterminated_quote_with_escape.csv' QUOTE '\"' ESCAPE '|';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/unterminated_quote_with_escape.csv' (QUOTE '\"', ESCAPE '|', AUTO_DETECT FALSE);\n \n statement ok\n DROP TABLE abac_tbl\n@@ -159,7 +159,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR);\n \n statement error\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/unterminated_quote_escape.csv' QUOTE '\"' ESCAPE '\"';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/unterminated_quote_escape.csv' (QUOTE '\"', ESCAPE '\"', AUTO_DETECT FALSE);\n \n statement ok\n DROP TABLE abac_tbl\n@@ -169,7 +169,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR);\n \n statement error\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/unterminated_escape.csv' QUOTE '\"' ESCAPE '''';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/unterminated_escape.csv' (QUOTE '\"', ESCAPE '''', AUTO_DETECT FALSE);\n \n statement ok\n DROP TABLE abac_tbl\n@@ -179,7 +179,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR);\n \n statement error\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/complex_unterminated_quote.csv' QUOTE 'ABABAC';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/complex_unterminated_quote.csv' (QUOTE 'ABABAC', AUTO_DETECT FALSE);\n \n statement ok\n DROP TABLE abac_tbl\n@@ -189,10 +189,10 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR);\n \n statement error\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/incomplete_multibyte_delimiter.csv' DELIMITER 'ABAC';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/incomplete_multibyte_delimiter.csv' (DELIMITER 'ABAC', AUTO_DETECT FALSE);\n \n query I\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/incomplete_multibyte_delimiter.csv' DELIMITER 'AB';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/incomplete_multibyte_delimiter.csv' (DELIMITER 'AB', AUTO_DETECT FALSE);\n ----\n 1\n \n@@ -204,7 +204,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR);\n \n statement error\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/unterminated_quote_with_escape_complex.csv' QUOTE 'ABAC' ESCAPE 'ABAB';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/unterminated_quote_with_escape_complex.csv' (QUOTE 'ABAC', ESCAPE 'ABAB', AUTO_DETECT FALSE);\n \n statement ok\n DROP TABLE abac_tbl\n@@ -214,7 +214,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR);\n \n statement error\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/unterminated_quote_escape_complex.csv' QUOTE 'ABAC' ESCAPE 'ABAC';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/unterminated_quote_escape_complex.csv' (QUOTE 'ABAC', ESCAPE 'ABAC', AUTO_DETECT FALSE);\n \n statement ok\n DROP TABLE abac_tbl\n@@ -224,7 +224,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR);\n \n statement error\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/unterminated_escape_complex.csv' QUOTE 'ABAC' ESCAPE 'ABAB';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/unterminated_escape_complex.csv' (QUOTE 'ABAC', ESCAPE 'ABAB', AUTO_DETECT FALSE);\n \n statement ok\n DROP TABLE abac_tbl\n@@ -234,16 +234,16 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR);\n \n statement error\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/abac.csv' QUOTE 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/abac.csv' (QUOTE 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', AUTO_DETECT FALSE);\n \n statement error\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/abac.csv' ESCAPE 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/abac.csv' (ESCAPE 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', AUTO_DETECT FALSE);\n \n statement error\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/abac.csv' DELIMITER 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/abac.csv' (DELIMITER 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', AUTO_DETECT FALSE);\n \n query I\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/abac.csv' QUOTE 'BLABLABLA';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/abac.csv' (QUOTE 'BLABLABLA', AUTO_DETECT FALSE);\n ----\n 1\n \n@@ -255,7 +255,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR, b VARCHAR);\n \n query I\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/carriage_feed_newline.csv' DELIMITER 'BA';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/carriage_feed_newline.csv' (DELIMITER 'BA', AUTO_DETECT FALSE);\n ----\n 2\n \n@@ -273,7 +273,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR, b VARCHAR);\n \n query I\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/windows_newline.csv' DELIMITER 'BA';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/windows_newline.csv' (DELIMITER 'BA', AUTO_DETECT FALSE);\n ----\n 2\n \n@@ -291,7 +291,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR);\n \n statement error\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/unterminated_quote_multi_line.csv' DELIMITER 'BA';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/unterminated_quote_multi_line.csv' (DELIMITER 'BA', AUTO_DETECT FALSE);\n \n statement ok\n DROP TABLE abac_tbl\n@@ -301,7 +301,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR, b VARCHAR);\n \n statement error\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/unquote_without_delimiter.csv' DELIMITER 'BA';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/unquote_without_delimiter.csv' (DELIMITER 'BA', AUTO_DETECT FALSE);\n \n statement ok\n DROP TABLE abac_tbl\n@@ -311,7 +311,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR, b VARCHAR);\n \n statement error\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/escape_non_quote_escape.csv' DELIMITER '|' ESCAPE 'X';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/escape_non_quote_escape.csv' (DELIMITER '|', ESCAPE 'X', AUTO_DETECT FALSE);\n \n statement ok\n DROP TABLE abac_tbl\n@@ -321,7 +321,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR, b VARCHAR);\n \n statement error\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/escape_non_quote_escape_complex.csv' DELIMITER 'BA' ESCAPE 'XX';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/escape_non_quote_escape_complex.csv' (DELIMITER 'BA', ESCAPE 'XX', AUTO_DETECT FALSE);\n \n statement ok\n DROP TABLE abac_tbl\n@@ -331,7 +331,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR);\n \n query I\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/trailing_delimiter_complex.csv' DELIMITER 'BA';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/trailing_delimiter_complex.csv' (DELIMITER 'BA', AUTO_DETECT FALSE);\n ----\n 1\n \n@@ -348,7 +348,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR);\n \n query I\n-COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/trailing_delimiter.csv' DELIMITER '|';\n+COPY abac_tbl FROM 'test/sql/copy/csv/data/abac/trailing_delimiter.csv' (DELIMITER '|', AUTO_DETECT FALSE);\n ----\n 1\n \ndiff --git a/test/sql/copy/csv/test_copy.test b/test/sql/copy/csv/test_copy.test\nindex 0d97c31387ac..d6f47def87a0 100644\n--- a/test/sql/copy/csv/test_copy.test\n+++ b/test/sql/copy/csv/test_copy.test\n@@ -188,7 +188,7 @@ statement ok\n CREATE TABLE unterminated (a VARCHAR);\n \n statement error\n-COPY unterminated FROM 'test/sql/copy/csv/data/test/unterminated.csv' (HEADER 0);\n+COPY unterminated FROM 'test/sql/copy/csv/data/test/unterminated.csv' (HEADER 0, AUTO_DETECT FALSE);\n \n # 1024 rows (vector size)\n # load CSV file into a table\ndiff --git a/test/sql/copy/csv/test_escape_long_value.test b/test/sql/copy/csv/test_escape_long_value.test\nindex 9f072c0556ff..02bca08f5e36 100644\n--- a/test/sql/copy/csv/test_escape_long_value.test\n+++ b/test/sql/copy/csv/test_escape_long_value.test\n@@ -8,7 +8,7 @@ statement ok\n CREATE TABLE long_escaped_value (a INTEGER, b INTEGER, c VARCHAR);\n \n query I\n-COPY long_escaped_value FROM 'test/sql/copy/csv/data/test/long_escaped_value.csv' DELIMITER '\ud83e\udd86';\n+COPY long_escaped_value FROM 'test/sql/copy/csv/data/test/long_escaped_value.csv' (DELIMITER '\ud83e\udd86', AUTO_DETECT FALSE);\n ----\n 1\n \ndiff --git a/test/sql/copy/csv/test_read_csv.test b/test/sql/copy/csv/test_read_csv.test\nindex c60fe3111d37..7908b3d6f517 100644\n--- a/test/sql/copy/csv/test_read_csv.test\n+++ b/test/sql/copy/csv/test_read_csv.test\n@@ -7,7 +7,7 @@ statement ok\n CREATE TABLE abac_tbl (a VARCHAR, b VARCHAR, c VARCHAR);\n \n query I\n-INSERT INTO abac_tbl SELECT * FROM read_csv('test/sql/copy/csv/data/abac/abac.csv', columns=STRUCT_PACK(a := 'VARCHAR', b := 'VARCHAR', c := 'VARCHAR'), sep='ABAC')\n+INSERT INTO abac_tbl SELECT * FROM read_csv('test/sql/copy/csv/data/abac/abac.csv', columns=STRUCT_PACK(a := 'VARCHAR', b := 'VARCHAR', c := 'VARCHAR'), sep='ABAC', auto_detect='false')\n ----\n 1\n \n@@ -35,7 +35,7 @@ SELECT * FROM dates\n \n # dateformat should also work with auto format\n statement ok\n-INSERT INTO dates SELECT * FROM read_csv('test/sql/copy/csv/data/test/dateformat.csv', dateformat='%m/%d/%Y')\n+INSERT INTO dates SELECT * FROM read_csv_auto('test/sql/copy/csv/data/test/dateformat.csv', dateformat='%m/%d/%Y')\n \n query I\n SELECT * FROM dates ORDER BY 1\n@@ -45,7 +45,7 @@ SELECT * FROM dates ORDER BY 1\n \n # we can also do this for timestamps\n statement ok\n-CREATE TABLE timestamps AS SELECT * FROM read_csv('test/sql/copy/csv/data/test/dateformat.csv', timestampformat='%m/%d/%Y')\n+CREATE TABLE timestamps AS SELECT * FROM read_csv_auto('test/sql/copy/csv/data/test/dateformat.csv', timestampformat='%m/%d/%Y')\n \n query I\n SELECT * FROM timestamps\ndiff --git a/test/sql/copy/csv/test_windows_newline.test b/test/sql/copy/csv/test_windows_newline.test\nindex 8821a91fa4fe..52164169787f 100644\n--- a/test/sql/copy/csv/test_windows_newline.test\n+++ b/test/sql/copy/csv/test_windows_newline.test\n@@ -22,7 +22,7 @@ DELETE FROM test;\n \n # now do the same with a multi-byte quote that is not actually used\n query I\n-COPY test FROM 'test/sql/copy/csv/data/test/windows_newline.csv' QUOTE 'BLABLABLA';\n+COPY test FROM 'test/sql/copy/csv/data/test/windows_newline.csv' (QUOTE 'BLABLABLA', AUTO_DETECT FALSE);\n ----\n 20000\n \n",
  "problem_statement": "CASE expressions\n\n",
  "hints_text": "",
  "created_at": "2020-08-13T10:31:52Z"
}