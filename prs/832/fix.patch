diff --git a/src/common/operator/cast_operators.cpp b/src/common/operator/cast_operators.cpp
index 47613c1fa438..e6b2b6586a46 100644
--- a/src/common/operator/cast_operators.cpp
+++ b/src/common/operator/cast_operators.cpp
@@ -163,24 +163,23 @@ template <> float Cast::Operation(double input) {
 template <class T> static T try_cast_string(string_t input) {
 	T result;
 	if (!TryCast::Operation<string_t, T>(input, result)) {
-		throw ConversionException("Could not convert string '%s' to %s", input.GetData(), TypeIdToString(GetTypeId<T>()).c_str());
+		throw ConversionException("Could not convert string '%s' to %s", input.GetData(),
+		                          TypeIdToString(GetTypeId<T>()).c_str());
 	}
 	return result;
 }
 
-
-
 template <class T> static T try_strict_cast_string(string_t input) {
 	T result;
 	if (!TryCast::Operation<string_t, T>(input, result, true)) {
-		throw ConversionException("Could not convert string '%s' to %s", input.GetData(), TypeIdToString(GetTypeId<T>()).c_str());
+		throw ConversionException("Could not convert string '%s' to %s", input.GetData(),
+		                          TypeIdToString(GetTypeId<T>()).c_str());
 	}
 	return result;
 }
 
 struct IntegerCastOperation {
-	template<class T, bool NEGATIVE>
-	static bool HandleDigit(T &result, uint8_t digit) {
+	template <class T, bool NEGATIVE> static bool HandleDigit(T &result, uint8_t digit) {
 		if (NEGATIVE) {
 			if (result < (NumericLimits<T>::Minimum() + digit) / 10) {
 				return false;
@@ -195,8 +194,7 @@ struct IntegerCastOperation {
 		return true;
 	}
 
-	template<class T>
-	static bool HandleExponent(T &result, int64_t exponent) {
+	template <class T> static bool HandleExponent(T &result, int64_t exponent) {
 		double dbl_res = result * pow(10, exponent);
 		if (dbl_res < NumericLimits<T>::Minimum() || dbl_res > NumericLimits<T>::Maximum()) {
 			return false;
@@ -205,17 +203,16 @@ struct IntegerCastOperation {
 		return true;
 	}
 
-	template<class T>
-	static bool Finalize(T &result) {
+	template <class T> static bool Finalize(T &result) {
 		return true;
 	}
 };
 
-template <class T, bool NEGATIVE, bool ALLOW_EXPONENT, class OP=IntegerCastOperation>
+template <class T, bool NEGATIVE, bool ALLOW_EXPONENT, class OP = IntegerCastOperation>
 static bool IntegerCastLoop(const char *buf, idx_t len, T &result, bool strict) {
 	idx_t start_pos = NEGATIVE || *buf == '+' ? 1 : 0;
 	idx_t pos = start_pos;
-	while(pos < len) {
+	while (pos < len) {
 		if (!std::isdigit((unsigned char)buf[pos])) {
 			// not a digit!
 			if (buf[pos] == '.') {
@@ -231,7 +228,7 @@ static bool IntegerCastLoop(const char *buf, idx_t len, T &result, bool strict)
 				// make sure everything after the period is a number
 				pos++;
 				idx_t start_digit = pos;
-				while(pos < len) {
+				while (pos < len) {
 					if (!std::isdigit((unsigned char)buf[pos++])) {
 						return false;
 					}
@@ -242,7 +239,7 @@ static bool IntegerCastLoop(const char *buf, idx_t len, T &result, bool strict)
 			}
 			if (std::isspace((unsigned char)buf[pos])) {
 				// skip any trailing spaces
-				while(++pos < len) {
+				while (++pos < len) {
 					if (!std::isspace((unsigned char)buf[pos])) {
 						return false;
 					}
@@ -279,9 +276,10 @@ static bool IntegerCastLoop(const char *buf, idx_t len, T &result, bool strict)
 	return pos > start_pos;
 }
 
-template <class T, bool ALLOW_EXPONENT = true, class OP=IntegerCastOperation> static bool TryIntegerCast(const char *buf, idx_t len, T &result, bool strict) {
+template <class T, bool ALLOW_EXPONENT = true, class OP = IntegerCastOperation>
+static bool TryIntegerCast(const char *buf, idx_t len, T &result, bool strict) {
 	// skip any spaces at the start
-	while(len > 0 && std::isspace(*buf)) {
+	while (len > 0 && std::isspace(*buf)) {
 		buf++;
 		len--;
 	}
@@ -302,7 +300,7 @@ template <> bool TryCast::Operation(string_t input, bool &result, bool strict) {
 	auto input_data = input.GetData();
 	auto input_size = input.GetSize();
 
-	switch(input_size) {
+	switch (input_size) {
 	case 1: {
 		char c = std::tolower(*input_data);
 		if (c == 't' || (!strict && c == '1')) {
@@ -433,7 +431,7 @@ template <> bool CheckDoubleValidity(double value) {
 
 template <class T> static bool TryDoubleCast(const char *buf, idx_t len, T &result, bool strict) {
 	// skip any spaces at the start
-	while(len > 0 && std::isspace(*buf)) {
+	while (len > 0 && std::isspace(*buf)) {
 		buf++;
 		len--;
 	}
@@ -657,7 +655,8 @@ struct HugeintToStringCast {
 			// we want to avoid doing as many divisions as possible
 			// for that reason we start off doing a division by a large power of ten that uint64_t can hold
 			// (100000000000000000) - this is the third largest
-			// the reason we don't use the largest is because that can result in an overflow inside the division function
+			// the reason we don't use the largest is because that can result in an overflow inside the division
+			// function
 			uint64_t remainder;
 			value = Hugeint::DivModPositive(value, 100000000000000000ULL, remainder);
 
@@ -668,7 +667,7 @@ struct HugeintToStringCast {
 
 			int format_length = startptr - ptr;
 			// pad with zero
-			for(int i = format_length; i < 17; i++) {
+			for (int i = format_length; i < 17; i++) {
 				*--ptr = '0';
 			}
 		}
@@ -917,16 +916,17 @@ template <> string_t CastFromBlob::Operation(string_t input, Vector &vector) {
 }
 
 void CastFromBlob::ToHexString(string_t input, string_t &output) {
-	const char hexa_table[] = {'0','1','2','3','4','5','6','7','8','9','A','B','C','D','E','F'};
+	const char hexa_table[] = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F'};
 	idx_t input_size = input.GetSize();
 	assert(output.GetSize() == (input_size * 2 + 2));
 	auto input_data = input.GetData();
-	auto hexa_data  = output.GetData();
+	auto hexa_data = output.GetData();
 	// hex identifier
-	hexa_data[0] = '\\'; hexa_data[1] = 'x';
+	hexa_data[0] = '\\';
+	hexa_data[1] = 'x';
 	hexa_data += 2;
-	for(idx_t idx = 0; idx < input_size; ++idx) {
-		hexa_data[idx * 2]     = hexa_table[(input_data[idx] >> 4) & 0x0F];
+	for (idx_t idx = 0; idx < input_size; ++idx) {
+		hexa_data[idx * 2] = hexa_table[(input_data[idx] >> 4) & 0x0F];
 		hexa_data[idx * 2 + 1] = hexa_table[input_data[idx] & 0x0F];
 	}
 	output.Finalize();
@@ -935,7 +935,7 @@ void CastFromBlob::ToHexString(string_t input, string_t &output) {
 void CastFromBlob::FromHexToBytes(string_t input, string_t &output) {
 	idx_t in_size = input.GetSize();
 	// amount of hex chars must be even
-	if((in_size % 2) != 0) {
+	if ((in_size % 2) != 0) {
 		throw OutOfRangeException("Hex string must have an even number of bytes.");
 	}
 
@@ -947,21 +947,20 @@ void CastFromBlob::FromHexToBytes(string_t input, string_t &output) {
 	auto out_data = output.GetData();
 	idx_t out_size = output.GetSize();
 	assert(out_size == (in_size / 2));
-	idx_t out_idx=0;
+	idx_t out_idx = 0;
 
 	idx_t num_hex_per_byte = 2;
 	uint8_t hex[2];
 
-	for(idx_t in_idx = 0; in_idx < in_size; in_idx+=2, ++out_idx) {
-		for(idx_t hex_idx = 0; hex_idx < num_hex_per_byte; ++hex_idx) {
+	for (idx_t in_idx = 0; in_idx < in_size; in_idx += 2, ++out_idx) {
+		for (idx_t hex_idx = 0; hex_idx < num_hex_per_byte; ++hex_idx) {
 			uint8_t int_ch = in_data[in_idx + hex_idx];
-			if(int_ch >= (uint8_t)'0' && int_ch <= (uint8_t)'9') {
+			if (int_ch >= (uint8_t)'0' && int_ch <= (uint8_t)'9') {
 				// numeric ascii chars: '0' to '9'
 				hex[hex_idx] = int_ch & 0X0F;
-			}
-			else if((int_ch >= (uint8_t)'A' && int_ch <= (uint8_t)'F') ||
-					(int_ch >= (uint8_t)'a' && int_ch <= (uint8_t)'f')) {
-					// hex chars: ['A':'F'] or ['a':'f']
+			} else if ((int_ch >= (uint8_t)'A' && int_ch <= (uint8_t)'F') ||
+			           (int_ch >= (uint8_t)'a' && int_ch <= (uint8_t)'f')) {
+				// hex chars: ['A':'F'] or ['a':'f']
 				// transforming char into an integer in the range of 10 to 15
 				hex[hex_idx] = ((int_ch & 0X0F) - 1) + 10;
 			} else {
@@ -983,7 +982,7 @@ template <> string_t CastToBlob::Operation(string_t input, Vector &vector) {
 	auto input_data = input.GetData();
 	string_t result;
 	// Check by a hex string
-	if(input_size >= 2 && input_data[0] == '\\' && input_data[1] == 'x') {
+	if (input_size >= 2 && input_data[0] == '\\' && input_data[1] == 'x') {
 		auto output = StringVector::EmptyString(vector, (input_size - 2) / 2);
 		CastFromBlob::FromHexToBytes(input, output);
 		result = output;
@@ -1046,8 +1045,7 @@ struct HugeIntCastData {
 };
 
 struct HugeIntegerCastOperation {
-	template<class T, bool NEGATIVE>
-	static bool HandleDigit(T &result, uint8_t digit) {
+	template <class T, bool NEGATIVE> static bool HandleDigit(T &result, uint8_t digit) {
 		if (NEGATIVE) {
 			if (result.intermediate < (NumericLimits<int64_t>::Minimum() + digit) / 10) {
 				// intermediate is full: need to flush it
@@ -1068,13 +1066,13 @@ struct HugeIntegerCastOperation {
 		return true;
 	}
 
-	template<class T>
-	static bool HandleExponent(T &result, int64_t exponent) {
+	template <class T> static bool HandleExponent(T &result, int64_t exponent) {
 		result.Flush();
 		if (exponent < -38 || exponent > 38) {
 			// out of range for exact exponent: use double and convert
 			double dbl_res = Hugeint::Cast<double>(result.hugeint) * pow(10, exponent);
-			if (dbl_res < Hugeint::Cast<double>(NumericLimits<hugeint_t>::Minimum()) || dbl_res > Hugeint::Cast<double>(NumericLimits<hugeint_t>::Maximum())) {
+			if (dbl_res < Hugeint::Cast<double>(NumericLimits<hugeint_t>::Minimum()) ||
+			    dbl_res > Hugeint::Cast<double>(NumericLimits<hugeint_t>::Maximum())) {
 				return false;
 			}
 			result.hugeint = Hugeint::Convert(dbl_res);
@@ -1090,15 +1088,15 @@ struct HugeIntegerCastOperation {
 		}
 	}
 
-	template<class T>
-	static bool Finalize(T &result) {
+	template <class T> static bool Finalize(T &result) {
 		return result.Flush();
 	}
 };
 
 template <> bool TryCast::Operation(string_t input, hugeint_t &result, bool strict) {
 	HugeIntCastData data;
-	if (!TryIntegerCast<HugeIntCastData, true, HugeIntegerCastOperation>(input.GetData(), input.GetSize(), data, strict)) {
+	if (!TryIntegerCast<HugeIntCastData, true, HugeIntegerCastOperation>(input.GetData(), input.GetSize(), data,
+	                                                                     strict)) {
 		return false;
 	}
 	result = data.hugeint;
@@ -1215,8 +1213,7 @@ template <> bool Cast::Operation(hugeint_t input) {
 	return result;
 }
 
-template<class T>
-static T hugeint_cast_to_numeric(hugeint_t input) {
+template <class T> static T hugeint_cast_to_numeric(hugeint_t input) {
 	T result;
 	if (!TryCast::Operation<hugeint_t, T>(input, result)) {
 		throw OutOfRangeException("Failed to cast from hugeint: value is out of range");
diff --git a/src/common/types.cpp b/src/common/types.cpp
index 882a6cb750dd..66863a8a0ae1 100644
--- a/src/common/types.cpp
+++ b/src/common/types.cpp
@@ -372,24 +372,24 @@ bool SQLType::IsMoreGenericThan(SQLType &other) const {
 	switch (id) {
 	case SQLTypeId::SMALLINT:
 		switch (other.id) {
+		case SQLTypeId::BOOLEAN:
 		case SQLTypeId::TINYINT:
-		case SQLTypeId::SMALLINT:
-		case SQLTypeId::INTEGER:
 			return true;
 		default:
 			return false;
 		}
 	case SQLTypeId::INTEGER:
 		switch (other.id) {
+		case SQLTypeId::BOOLEAN:
 		case SQLTypeId::TINYINT:
 		case SQLTypeId::SMALLINT:
-		case SQLTypeId::INTEGER:
 			return true;
 		default:
 			return false;
 		}
 	case SQLTypeId::BIGINT:
 		switch (other.id) {
+		case SQLTypeId::BOOLEAN:
 		case SQLTypeId::TINYINT:
 		case SQLTypeId::SMALLINT:
 		case SQLTypeId::INTEGER:
@@ -399,6 +399,18 @@ bool SQLType::IsMoreGenericThan(SQLType &other) const {
 		}
 	case SQLTypeId::HUGEINT:
 		switch (other.id) {
+		case SQLTypeId::BOOLEAN:
+		case SQLTypeId::TINYINT:
+		case SQLTypeId::SMALLINT:
+		case SQLTypeId::INTEGER:
+		case SQLTypeId::BIGINT:
+			return true;
+		default:
+			return false;
+		}
+	case SQLTypeId::FLOAT:
+		switch (other.id) {
+		case SQLTypeId::BOOLEAN:
 		case SQLTypeId::TINYINT:
 		case SQLTypeId::SMALLINT:
 		case SQLTypeId::INTEGER:
@@ -407,12 +419,15 @@ bool SQLType::IsMoreGenericThan(SQLType &other) const {
 		default:
 			return false;
 		}
+		return false;
 	case SQLTypeId::DOUBLE:
 		switch (other.id) {
+		case SQLTypeId::BOOLEAN:
 		case SQLTypeId::TINYINT:
 		case SQLTypeId::SMALLINT:
 		case SQLTypeId::INTEGER:
 		case SQLTypeId::BIGINT:
+		case SQLTypeId::FLOAT:
 			return true;
 		default:
 			return false;
diff --git a/src/execution/operator/persistent/buffered_csv_reader.cpp b/src/execution/operator/persistent/buffered_csv_reader.cpp
index 2ebd590d4ffb..a92c07e59df7 100644
--- a/src/execution/operator/persistent/buffered_csv_reader.cpp
+++ b/src/execution/operator/persistent/buffered_csv_reader.cpp
@@ -4,8 +4,8 @@
 #include "duckdb/common/file_system.hpp"
 #include "duckdb/common/gzip_stream.hpp"
 #include "duckdb/common/string_util.hpp"
-#include "duckdb/common/vector_operations/vector_operations.hpp"
 #include "duckdb/common/vector_operations/unary_executor.hpp"
+#include "duckdb/common/vector_operations/vector_operations.hpp"
 #include "duckdb/execution/operator/persistent/physical_copy_from_file.hpp"
 #include "duckdb/function/scalar/strftime.hpp"
 #include "duckdb/main/database.hpp"
@@ -36,7 +36,7 @@ static string GenerateColumnName(const idx_t total_cols, const idx_t col_number,
 
 static string GetLineNumberStr(idx_t linenr, bool linenr_estimated) {
 	string estimated = (linenr_estimated ? string(" (estimated)") : string(""));
-	return std::to_string(linenr) + estimated;
+	return std::to_string(linenr + 1) + estimated;
 }
 
 TextSearchShiftArray::TextSearchShiftArray() {
@@ -70,14 +70,16 @@ TextSearchShiftArray::TextSearchShiftArray(string search_term) : length(search_t
 	}
 }
 
-BufferedCSVReader::BufferedCSVReader(ClientContext &context, BufferedCSVReaderOptions options, vector<SQLType> requested_types)
-	: options(options), buffer_size(0), position(0), start(0) {
+BufferedCSVReader::BufferedCSVReader(ClientContext &context, BufferedCSVReaderOptions options,
+                                     vector<SQLType> requested_types)
+    : options(options), buffer_size(0), position(0), start(0) {
 	source = OpenCSV(context, options);
 	Initialize(requested_types);
 }
 
-BufferedCSVReader::BufferedCSVReader(BufferedCSVReaderOptions options, vector<SQLType> requested_types, unique_ptr<istream> ssource)
-	: options(options), source(move(ssource)), buffer_size(0), position(0), start(0) {
+BufferedCSVReader::BufferedCSVReader(BufferedCSVReaderOptions options, vector<SQLType> requested_types,
+                                     unique_ptr<istream> ssource)
+    : options(options), source(move(ssource)), buffer_size(0), position(0), start(0) {
 	Initialize(requested_types);
 }
 
@@ -90,7 +92,7 @@ void BufferedCSVReader::Initialize(vector<SQLType> requested_types) {
 
 	PrepareComplexParser();
 	InitParseChunk(sql_types.size());
-	SkipHeader();
+	SkipHeader(options.skip_rows, options.header);
 }
 
 void BufferedCSVReader::PrepareComplexParser() {
@@ -99,6 +101,17 @@ void BufferedCSVReader::PrepareComplexParser() {
 	quote_search = TextSearchShiftArray(options.quote);
 }
 
+void BufferedCSVReader::ConfigureSampling() {
+	if (options.sample_size > STANDARD_VECTOR_SIZE) {
+		throw ParserException("Chunk size (%d) cannot be bigger than STANDARD_VECTOR_SIZE (%d)",
+			                    options.sample_size, STANDARD_VECTOR_SIZE);
+	} else if (options.sample_size < 1) {
+		throw ParserException("Chunk size cannot be smaller than 1.");
+	}
+	SAMPLE_CHUNK_SIZE = options.sample_size;
+	MAX_SAMPLE_CHUNKS = options.num_samples;
+}
+
 unique_ptr<istream> BufferedCSVReader::OpenCSV(ClientContext &context, BufferedCSVReaderOptions options) {
 	if (!FileSystem::GetFileSystem(context).FileExists(options.file_path)) {
 		throw IOException("File \"%s\" not found", options.file_path.c_str());
@@ -123,15 +136,15 @@ unique_ptr<istream> BufferedCSVReader::OpenCSV(ClientContext &context, BufferedC
 	return result;
 }
 
-void BufferedCSVReader::SkipHeader() {
-	for (idx_t i = 0; i < options.skip_rows; i++) {
+void BufferedCSVReader::SkipHeader(idx_t skip_rows, bool skip_header) {
+	for (idx_t i = 0; i < skip_rows; i++) {
 		// ignore skip rows
 		string read_line;
 		getline(*source, read_line);
 		linenr++;
 	}
 
-	if (options.header) {
+	if (skip_header) {
 		// ignore the first line as a header line
 		string read_line;
 		getline(*source, read_line);
@@ -182,18 +195,22 @@ void BufferedCSVReader::InitParseChunk(idx_t num_cols) {
 	parse_chunk.Initialize(varchar_types);
 }
 
-void BufferedCSVReader::JumpToBeginning() {
+void BufferedCSVReader::JumpToBeginning(idx_t skip_rows, bool skip_header) {
 	ResetBuffer();
 	ResetStream();
 	ResetParseChunk();
-	SkipHeader();
+	SkipHeader(skip_rows, skip_header);
 }
 
 bool BufferedCSVReader::JumpToNextSample() {
-	if (source->eof() || sample_chunk_idx >= MAX_SAMPLE_CHUNKS) {
+	if (end_of_file_reached || sample_chunk_idx >= MAX_SAMPLE_CHUNKS) {
 		return false;
 	}
 
+	// adjust the value of bytes_in_chunk, based on current state of the buffer
+	idx_t remaining_bytes_in_buffer = buffer_size - start;
+	bytes_in_chunk -= remaining_bytes_in_buffer;
+
 	// update average bytes per line
 	double bytes_per_line = bytes_in_chunk / (double)SAMPLE_CHUNK_SIZE;
 	bytes_per_line_avg = ((bytes_per_line_avg * sample_chunk_idx) + bytes_per_line) / (sample_chunk_idx + 1);
@@ -213,10 +230,6 @@ bool BufferedCSVReader::JumpToNextSample() {
 		return true;
 	}
 
-	// adjust the value of bytes_in_chunk, based on current state of the buffer
-	idx_t remaining_bytes_in_buffer = buffer_size - start;
-	bytes_in_chunk -= remaining_bytes_in_buffer;
-
 	// if none of the previous conditions were met, we can jump
 	idx_t partition_size = (idx_t)round(file_size / (double)MAX_SAMPLE_CHUNKS);
 
@@ -274,33 +287,64 @@ bool BufferedCSVReader::TryCastValue(Value value, SQLType sql_type) {
 	return false;
 }
 
+bool BufferedCSVReader::TryCastVector(Vector &parse_chunk_col, idx_t size, SQLType sql_type) {
+	try {
+		// try vector-cast from string to sql_type
+		Vector dummy_result(GetInternalType(sql_type));
+		if (options.has_date_format && sql_type == SQLTypeId::DATE) {
+			// use the date format to cast the chunk
+			UnaryExecutor::Execute<string_t, date_t, true>(parse_chunk_col, dummy_result, size, [&](string_t input) {
+				return options.date_format.ParseDate(input);
+			});
+		} else if (options.has_timestamp_format && sql_type == SQLTypeId::TIMESTAMP) {
+			// use the date format to cast the chunk
+			UnaryExecutor::Execute<string_t, timestamp_t, true>(
+			    parse_chunk_col, dummy_result, size,
+			    [&](string_t input) { return options.timestamp_format.ParseTimestamp(input); });
+		} else {
+			// target type is not varchar: perform a cast
+			VectorOperations::Cast(parse_chunk_col, dummy_result, SQLType::VARCHAR, sql_type, size, true);
+		}
+	} catch (const Exception &e) {
+		return false;
+	}
+	return true;
+}
+
+void BufferedCSVReader::PrepareCandidateSets() {
+	if (options.has_delimiter) {
+		delim_candidates = {options.delimiter};
+	}
+	if (options.has_quote) {
+		quote_candidates_map = {{options.quote}, {options.quote}, {options.quote}};
+	}
+	if (options.has_escape) {
+		if (options.escape == "") {
+			quoterule_candidates = {QuoteRule::QUOTES_RFC};
+		} else {
+			quoterule_candidates = {QuoteRule::QUOTES_OTHER};
+		}
+		escape_candidates_map[static_cast<uint8_t>(quoterule_candidates[0])] = {options.escape};
+	}
+}
+
 vector<SQLType> BufferedCSVReader::SniffCSV(vector<SQLType> requested_types) {
-	// TODO: sniff for uncommon (UTF-8) delimiter variants in first lines and add them to the list
-	const vector<string> delim_candidates = {",", "|", ";", "\t"};
-	const vector<QuoteRule> quoterule_candidates = {QuoteRule::QUOTES_RFC, QuoteRule::QUOTES_OTHER,
-													QuoteRule::NO_QUOTES};
-	// quote candiates depend on quote rule
-	const vector<vector<string>> quote_candidates_map = {{"\""}, {"\"", "'"}, {""}};
-	// escape candiates also depend on quote rule.
-	// Note: RFC-conform escapes are handled automatically, and without quotes no escape char is required
-	const vector<vector<string>> escape_candidates_map = {{""}, {"\\"}, {""}};
+	ConfigureSampling();
+	PrepareCandidateSets();
 
+	BufferedCSVReaderOptions original_options = options;
 	vector<BufferedCSVReaderOptions> info_candidates;
 	idx_t best_consistent_rows = 0;
 	idx_t best_num_cols = 0;
 
-	// if requested_types were provided, use them already in dialect detection
-	// TODO: currently they only serve to solve the edge case of trailing empty delimiters,
-	// however, they could be used to solve additional ambigious scenarios.
-	sql_types = requested_types;
-	// TODO: add a flag to indicate that no option actually worked and default will be used (RFC-4180)
+	JumpToBeginning(0, false);
 	for (QuoteRule quoterule : quoterule_candidates) {
 		vector<string> quote_candidates = quote_candidates_map[static_cast<uint8_t>(quoterule)];
 		for (const auto &quote : quote_candidates) {
 			for (const auto &delim : delim_candidates) {
 				vector<string> escape_candidates = escape_candidates_map[static_cast<uint8_t>(quoterule)];
 				for (const auto &escape : escape_candidates) {
-					BufferedCSVReaderOptions sniff_info = options;
+					BufferedCSVReaderOptions sniff_info = original_options;
 					sniff_info.delimiter = delim;
 					sniff_info.quote = quote;
 					sniff_info.escape = escape;
@@ -339,7 +383,9 @@ vector<SQLType> BufferedCSVReader::SniffCSV(vector<SQLType> requested_types) {
 					bool more_than_one_column = (num_cols > 1);
 					bool start_good = info_candidates.size() > 0 && (start_row <= info_candidates.front().skip_rows);
 
-					if ((more_values || single_column_before) && rows_consistent) {
+					if (requested_types.size() > 0 && requested_types.size() != num_cols) {
+						continue;
+					} else if ((more_values || single_column_before) && rows_consistent) {
 						sniff_info.skip_rows = start_row;
 						sniff_info.num_cols = num_cols;
 						best_consistent_rows = consistent_rows;
@@ -365,21 +411,25 @@ vector<SQLType> BufferedCSVReader::SniffCSV(vector<SQLType> requested_types) {
 		}
 	}
 
-	// then, file was most likely empty and we can do no more
+	// if not dialect candidate was found, then file was most likely empty and we default to RFC-4180 dialect
 	if (info_candidates.size() < 1) {
 		if (requested_types.size() == 0) {
 			// no types requested and no types/names could be deduced: default to a single varchar column
 			col_names.push_back("col0");
 			requested_types.push_back(SQLType::VARCHAR);
 		}
+
+		// back to normal
+		options = original_options;
+		JumpToBeginning(0, false);
 		return requested_types;
 	}
 
 	// type candidates, ordered by descending specificity (~ from high to low)
-	vector<SQLType> type_candidates = {SQLType::VARCHAR, SQLType::TIMESTAMP, SQLType::DATE,
-									   SQLType::TIME,    SQLType::DOUBLE,    /*SQLType::FLOAT,*/ SQLType::BIGINT,
-									   SQLType::INTEGER, /* SQLType::SMALLINT, */  /*SQLType::TINYINT,*/ SQLType::BOOLEAN,
-									   SQLType::SQLNULL};
+	vector<SQLType> type_candidates = {SQLType::VARCHAR, SQLType::TIMESTAMP,
+	                                   SQLType::DATE,    SQLType::TIME,
+	                                   SQLType::DOUBLE,  /* SQLType::FLOAT,*/ SQLType::BIGINT,
+	                                   SQLType::INTEGER, /*SQLType::SMALLINT, SQLType::TINYINT,*/ SQLType::BOOLEAN};
 
 	// check which info candiate leads to minimum amount of non-varchar columns...
 	BufferedCSVReaderOptions best_options;
@@ -395,7 +445,7 @@ vector<SQLType> BufferedCSVReader::SniffCSV(vector<SQLType> requested_types) {
 		InitParseChunk(sql_types.size());
 
 		// detect types in first chunk
-		JumpToBeginning();
+		JumpToBeginning(options.skip_rows, false);
 		ParseCSV(ParserMode::SNIFFING_DATATYPES);
 		for (idx_t row = 0; row < parse_chunk.size(); row++) {
 			for (idx_t col = 0; col < parse_chunk.column_count(); col++) {
@@ -427,7 +477,7 @@ vector<SQLType> BufferedCSVReader::SniffCSV(vector<SQLType> requested_types) {
 			}
 		}
 
-		// it's good if the dialect creates more non-varchar columns, but only if we sacrifice < 40% of best_num_cols.
+		// it's good if the dialect creates more non-varchar columns, but only if we sacrifice < 30% of best_num_cols.
 		if (varchar_cols < min_varchar_cols && parse_chunk.column_count() > (best_num_cols * 0.7)) {
 			// we have a new best_info candidate
 			best_options = info_candidate;
@@ -438,53 +488,69 @@ vector<SQLType> BufferedCSVReader::SniffCSV(vector<SQLType> requested_types) {
 
 	options = best_options;
 
-	// if data types were provided, exit here if number of columns does not match
-	// TODO: we could think about postponing this to see if the csv happens to contain a superset of requested columns
-	if (requested_types.size() > 0 && requested_types.size() != options.num_cols) {
-		throw ParserException("Error while determining column types: found %lld columns but expected %d", options.num_cols,
-							  requested_types.size());
-	}
-
 	// sql_types and parse_chunk have to be in line with new info
 	sql_types.clear();
 	sql_types.assign(options.num_cols, SQLType::VARCHAR);
 	InitParseChunk(sql_types.size());
 
-	// jump through the rest of the file and continue to refine the sql type guess
-	while (JumpToNextSample()) {
-		// if jump ends up a bad line, we just skip this chunk
-		try {
-			ParseCSV(ParserMode::SNIFFING_DATATYPES);
-		} catch (const ParserException &e) {
-			continue;
+	vector<SQLType> detected_types;
+
+	// if data types were provided, exit here if number of columns does not match
+	if (requested_types.size() > 0) {
+		if (requested_types.size() != options.num_cols) {
+			throw ParserException("Error while determining column types: found %lld columns but expected %d",
+			                      options.num_cols, requested_types.size());
+		} else {
+			detected_types = requested_types;
 		}
-		for (idx_t col = 0; col < parse_chunk.column_count(); col++) {
-			vector<SQLType> &col_type_candidates = best_sql_types_candidates[col];
-			while (col_type_candidates.size() > 1) {
-				try {
+	} else {
+		// jump through the rest of the file and continue to refine the sql type guess
+		while (JumpToNextSample()) {
+			// if jump ends up a bad line, we just skip this chunk
+			try {
+				ParseCSV(ParserMode::SNIFFING_DATATYPES);
+			} catch (const ParserException &e) {
+				continue;
+			}
+			for (idx_t col = 0; col < parse_chunk.column_count(); col++) {
+				vector<SQLType> &col_type_candidates = best_sql_types_candidates[col];
+				while (col_type_candidates.size() > 1) {
 					const auto &sql_type = col_type_candidates.back();
-					// try vector-cast from string to sql_type
-					parse_chunk.data[col];
-					Vector dummy_result(GetInternalType(sql_type));
-					VectorOperations::Cast(parse_chunk.data[col], dummy_result, SQLType::VARCHAR, sql_type,
-										   parse_chunk.size(), true);
-					break;
-				} catch (const Exception &e) {
-					col_type_candidates.pop_back();
+					if (TryCastVector(parse_chunk.data[col], parse_chunk.size(), sql_type)) {
+						break;
+					} else {
+						col_type_candidates.pop_back();
+					}
 				}
 			}
 		}
+
+		// set sql types
+		for (idx_t col = 0; col < best_sql_types_candidates.size(); col++) {
+			SQLType d_type = best_sql_types_candidates[col].back();
+			detected_types.push_back(d_type);
+		}
 	}
 
+	// if all rows are of type string, we will currently make the assumption there is no header.
+	// TODO: Do some kind of string-distance based constistency metic between first row and others
+	/*bool all_types_string = true;
+	for (idx_t col = 0; col < parse_chunk.column_count(); col++) {
+	    const auto &col_type = best_sql_types_candidates[col].back();
+	    all_types_string &= (col_type == SQLType::VARCHAR);
+	}*/
+
 	// information for header detection
 	bool first_row_consistent = true;
-	bool first_row_nulls = true;
+	bool first_row_nulls = false;
 
 	// parse first row again with knowledge from the rest of the file to check
 	// whether first row is consistent with the others or not.
-	JumpToBeginning();
+	JumpToBeginning(options.skip_rows, false);
 	ParseCSV(ParserMode::SNIFFING_DATATYPES);
-	if (parse_chunk.size() > 0) {
+	if (parse_chunk.size() > 1) {
+		first_row_nulls = true;
+
 		for (idx_t col = 0; col < parse_chunk.column_count(); col++) {
 			auto dummy_val = parse_chunk.GetValue(col, 0);
 			// try cast as SQLNULL
@@ -494,24 +560,15 @@ vector<SQLType> BufferedCSVReader::SniffCSV(vector<SQLType> requested_types) {
 				first_row_nulls = false;
 			}
 			// try cast to sql_type of column
-			vector<SQLType> &col_type_candidates = best_sql_types_candidates[col];
-			const auto &sql_type = col_type_candidates.back();
+			const auto &sql_type = detected_types[col];
 			if (!TryCastValue(dummy_val, sql_type)) {
 				first_row_consistent = false;
 			}
 		}
 	}
 
-	// if all rows are of type string, we will currently make the assumption there is no header.
-	// TODO: Do some kind of string-distance based constistency metic between first row and others
-	/*bool all_types_string = true;
-	for (idx_t col = 0; col < parse_chunk.column_count(); col++) {
-		const auto &col_type = best_sql_types_candidates[col].back();
-		all_types_string &= (col_type == SQLType::VARCHAR);
-	}*/
-
 	// update parser info, and read, generate & set col_names based on previous findings
-	if (!first_row_consistent || first_row_nulls) {
+	if (((!first_row_consistent || first_row_nulls) && !options.has_header) || (options.has_header && options.header)) {
 		options.header = true;
 		vector<string> t_col_names;
 		for (idx_t col = 0; col < parse_chunk.column_count(); col++) {
@@ -542,34 +599,8 @@ vector<SQLType> BufferedCSVReader::SniffCSV(vector<SQLType> requested_types) {
 		}
 	}
 
-	// set sql types
-	vector<SQLType> detected_types;
-	for (idx_t col = 0; col < best_sql_types_candidates.size(); col++) {
-		SQLType d_type = best_sql_types_candidates[col].back();
-
-		if (requested_types.size() > 0) {
-			SQLType r_type = requested_types[col];
-
-			// check if the detected types are in line with the provided types
-			if (r_type != d_type) {
-				if (r_type.IsMoreGenericThan(d_type)) {
-					d_type = r_type;
-				} else {
-					throw ParserException(
-						"Error while sniffing data type for column '%s': Requested column type %s, detected type %s",
-						col_names[col].c_str(), SQLTypeToString(r_type).c_str(), SQLTypeToString(d_type).c_str());
-				}
-			}
-		}
-
-		detected_types.push_back(d_type);
-	}
-
 	// back to normal
-	ResetBuffer();
-	ResetStream();
-	ResetParseChunk();
-	sniffed_column_counts.clear();
+	JumpToBeginning(0, false);
 
 	return detected_types;
 }
@@ -719,14 +750,15 @@ add_row : {
 			count++;
 			if (count > delimiter_pos && count > quote_pos) {
 				throw ParserException(
-					"Error on line %s: quote should be followed by end of value, end of row or another quote",
-					GetLineNumberStr(linenr, linenr_estimated).c_str());
+				    "Error on line %s: quote should be followed by end of value, end of row or another quote",
+				    GetLineNumberStr(linenr, linenr_estimated).c_str());
 			}
 			if (delimiter_pos == options.delimiter.size()) {
 				// quote followed by delimiter, add value
 				offset = options.quote.size() + options.delimiter.size() - 1;
 				goto add_value;
-			} else if (quote_pos == options.quote.size() && (options.escape.size() == 0 || options.escape == options.quote)) {
+			} else if (quote_pos == options.quote.size() &&
+			           (options.escape.size() == 0 || options.escape == options.quote)) {
 				// quote followed by quote, go back to quoted state and add to escape
 				escape_positions.push_back(position - start - (options.quote.size() - 1));
 				goto in_quotes;
@@ -734,7 +766,7 @@ add_row : {
 		}
 	} while (ReadBuffer(start));
 	throw ParserException("Error on line %s: quote should be followed by end of value, end of row or another quote",
-						  GetLineNumberStr(linenr, linenr_estimated).c_str());
+	                      GetLineNumberStr(linenr, linenr_estimated).c_str());
 handle_escape:
 	escape_pos = 0;
 	quote_pos = 0;
@@ -747,7 +779,7 @@ add_row : {
 			count++;
 			if (count > escape_pos && count > quote_pos) {
 				throw ParserException("Error on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE",
-									  GetLineNumberStr(linenr, linenr_estimated).c_str());
+				                      GetLineNumberStr(linenr, linenr_estimated).c_str());
 			}
 			if (quote_pos == options.quote.size() || escape_pos == options.escape.size()) {
 				// found quote or escape: move back to quoted state
@@ -756,7 +788,7 @@ add_row : {
 		}
 	} while (ReadBuffer(start));
 	throw ParserException("Error on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE",
-						  GetLineNumberStr(linenr, linenr_estimated).c_str());
+	                      GetLineNumberStr(linenr, linenr_estimated).c_str());
 carriage_return:
 	/* state: carriage_return */
 	// this stage optionally skips a newline (
) character, which allows \r
 to be interpreted as a single line
@@ -786,6 +818,8 @@ add_row : {
 	if (mode == ParserMode::PARSING) {
 		Flush(insert_chunk);
 	}
+
+	end_of_file_reached = true;
 }
 
 void BufferedCSVReader::ParseSimpleCSV(DataChunk &insert_chunk) {
@@ -908,7 +942,7 @@ add_row : {
 		goto add_row;
 	} else {
 		throw ParserException("Error on line %s: quote should be followed by end of value, end of row or another quote",
-							  GetLineNumberStr(linenr, linenr_estimated).c_str());
+		                      GetLineNumberStr(linenr, linenr_estimated).c_str());
 	}
 handle_escape:
 	/* state: handle_escape */
@@ -916,11 +950,11 @@ add_row : {
 	position++;
 	if (position >= buffer_size && !ReadBuffer(start)) {
 		throw ParserException("Error on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE",
-							  GetLineNumberStr(linenr, linenr_estimated).c_str());
+		                      GetLineNumberStr(linenr, linenr_estimated).c_str());
 	}
 	if (buffer[position] != options.quote[0] && buffer[position] != options.escape[0]) {
 		throw ParserException("Error on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE",
-							  GetLineNumberStr(linenr, linenr_estimated).c_str());
+		                      GetLineNumberStr(linenr, linenr_estimated).c_str());
 	}
 	// escape was followed by quote or escape, go back to quoted state
 	goto in_quotes;
@@ -956,6 +990,8 @@ add_row : {
 	if (mode == ParserMode::PARSING) {
 		Flush(insert_chunk);
 	}
+
+	end_of_file_reached = true;
 }
 
 bool BufferedCSVReader::ReadBuffer(idx_t &start) {
@@ -1018,7 +1054,7 @@ void BufferedCSVReader::AddValue(char *str_val, idx_t length, idx_t &column, vec
 	}
 	if (column >= sql_types.size()) {
 		throw ParserException("Error on line %s: expected %lld values but got %d",
-							  GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(), column + 1);
+		                      GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(), column + 1);
 	}
 
 	// insert the line number into the chunk
@@ -1060,9 +1096,11 @@ void BufferedCSVReader::AddValue(char *str_val, idx_t length, idx_t &column, vec
 }
 
 bool BufferedCSVReader::AddRow(DataChunk &insert_chunk, idx_t &column) {
+	linenr++;
+
 	if (column < sql_types.size() && mode != ParserMode::SNIFFING_DIALECT) {
 		throw ParserException("Error on line %s: expected %lld values but got %d",
-							  GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(), column);
+		                      GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(), column);
 	}
 
 	if (mode == ParserMode::SNIFFING_DIALECT) {
@@ -1085,7 +1123,6 @@ bool BufferedCSVReader::AddRow(DataChunk &insert_chunk, idx_t &column) {
 	}
 
 	column = 0;
-	linenr++;
 	return false;
 }
 
@@ -1106,8 +1143,8 @@ void BufferedCSVReader::Flush(DataChunk &insert_chunk) {
 					auto utf_type = Utf8Proc::Analyze(s.GetData(), s.GetSize());
 					switch (utf_type) {
 					case UnicodeType::INVALID:
-						throw ParserException("Error on line %s: file is not valid UTF8",
-											  GetLineNumberStr(linenr, linenr_estimated).c_str());
+						throw ParserException("Error between line %d and %d: file is not valid UTF8",
+						                      linenr - parse_chunk.size(), linenr);
 					case UnicodeType::ASCII:
 						break;
 					case UnicodeType::UNICODE: {
@@ -1121,22 +1158,36 @@ void BufferedCSVReader::Flush(DataChunk &insert_chunk) {
 			}
 			insert_chunk.data[col_idx].Reference(parse_chunk.data[col_idx]);
 		} else if (options.has_date_format && sql_types[col_idx].id == SQLTypeId::DATE) {
-			// use the date format to cast the chunk
-			UnaryExecutor::Execute<string_t, date_t, true>(parse_chunk.data[col_idx], insert_chunk.data[col_idx], parse_chunk.size(), [&](string_t input) {
-				return options.date_format.ParseDate(input);
-			});
+			try {
+				// use the date format to cast the chunk
+				UnaryExecutor::Execute<string_t, date_t, true>(
+				    parse_chunk.data[col_idx], insert_chunk.data[col_idx], parse_chunk.size(),
+				    [&](string_t input) { return options.date_format.ParseDate(input); });
+			} catch (const Exception &e) {
+				throw ParserException("Error between line %llu and %llu: %s", linenr - parse_chunk.size(), linenr,
+				                      e.what());
+			}
 		} else if (options.has_timestamp_format && sql_types[col_idx].id == SQLTypeId::TIMESTAMP) {
-			// use the date format to cast the chunk
-			UnaryExecutor::Execute<string_t, timestamp_t, true>(parse_chunk.data[col_idx], insert_chunk.data[col_idx], parse_chunk.size(), [&](string_t input) {
-				return options.timestamp_format.ParseTimestamp(input);
-			});
+			try {
+				// use the date format to cast the chunk
+				UnaryExecutor::Execute<string_t, timestamp_t, true>(
+				    parse_chunk.data[col_idx], insert_chunk.data[col_idx], parse_chunk.size(),
+				    [&](string_t input) { return options.timestamp_format.ParseTimestamp(input); });
+			} catch (const Exception &e) {
+				throw ParserException("Error between line %llu and %llu: %s", linenr - parse_chunk.size(), linenr,
+				                      e.what());
+			}
 		} else {
-			// target type is not varchar: perform a cast
-			VectorOperations::Cast(parse_chunk.data[col_idx], insert_chunk.data[col_idx], SQLType::VARCHAR,
-								   sql_types[col_idx], parse_chunk.size());
+			try {
+				// target type is not varchar: perform a cast
+				VectorOperations::Cast(parse_chunk.data[col_idx], insert_chunk.data[col_idx], SQLType::VARCHAR,
+				                       sql_types[col_idx], parse_chunk.size());
+			} catch (const Exception &e) {
+				throw ParserException("Error between line %llu and %llu: %s", linenr - parse_chunk.size(), linenr,
+				                      e.what());
+			}
 		}
 	}
 	parse_chunk.Reset();
 }
-
-}
+} // namespace duckdb
diff --git a/src/function/scalar/date/strftime.cpp b/src/function/scalar/date/strftime.cpp
index e4d366f43bf7..bda42f371016 100644
--- a/src/function/scalar/date/strftime.cpp
+++ b/src/function/scalar/date/strftime.cpp
@@ -15,6 +15,8 @@
 
 #include "re2/re2.h"
 
+#include <cctype>
+
 namespace duckdb {
 
 idx_t StrfTimepecifierSize(StrTimeSpecifier specifier) {
diff --git a/src/function/table/copy_csv.cpp b/src/function/table/copy_csv.cpp
index 20eb83f12f7d..12ec7016d620 100644
--- a/src/function/table/copy_csv.cpp
+++ b/src/function/table/copy_csv.cpp
@@ -13,30 +13,39 @@ using namespace std;
 namespace duckdb {
 
 struct BaseCSVData : public FunctionData {
-	BaseCSVData(string file_path) :
-		file_path(move(file_path)) {}
+	BaseCSVData(string file_path) : file_path(move(file_path)) {
+	}
 
 	//! The file path of the CSV file to read or write
 	string file_path;
+	//! Whether or not a header information was given by the user
+	bool has_header = false;
 	//! Whether or not to write a header in the file
 	bool header = false;
+	//! Whether or not a delimiter was defined by the user
+	bool has_delimiter = false;
 	//! Delimiter to separate columns within each line
 	string delimiter = ",";
+	//! Whether or not a quote sign was defined by the user
+	bool has_quote = false;
 	//! Quote used for columns that contain reserved characters, e.g., delimiter
 	string quote = "\"";
+	//! Quote used for columns that contain reserved characters, e.g., delimiter
+	bool has_escape = false;
 	//! Escape character to escape quote chara∆ícter
 	string escape;
 	//! Specifies the string that represents a null value
 	string null_str;
 	//! Whether or not the options are specified; if not we default to auto detect
-	bool is_auto_detect = true;
+	bool is_auto_detect = false;
 
 	void Finalize();
 };
 
 struct WriteCSVData : public BaseCSVData {
-	WriteCSVData(string file_path, vector<SQLType> sql_types, vector<string> names) :
-		BaseCSVData(move(file_path)), sql_types(move(sql_types)), names(move(names)) {}
+	WriteCSVData(string file_path, vector<SQLType> sql_types, vector<string> names)
+	    : BaseCSVData(move(file_path)), sql_types(move(sql_types)), names(move(names)) {
+	}
 
 	//! The SQL types to write
 	vector<SQLType> sql_types;
@@ -53,13 +62,18 @@ struct WriteCSVData : public BaseCSVData {
 };
 
 struct ReadCSVData : public BaseCSVData {
-	ReadCSVData(string file_path, vector<SQLType> sql_types) :
-		BaseCSVData(move(file_path)), sql_types(move(sql_types)) {}
+	ReadCSVData(string file_path, vector<SQLType> sql_types)
+	    : BaseCSVData(move(file_path)), sql_types(move(sql_types)) {
+	}
 
 	//! The expected SQL types to read
 	vector<SQLType> sql_types;
 	//! True, if column with that index must be quoted
 	vector<bool> force_not_null;
+	//! The size of a sample for format/data type detection (csv)
+	int sample_size = DEFAULT_SAMPLE_CHUNK_SIZE;
+	//! The number of samples for data type detection (csv)
+	int num_samples = 10;
 	//! The DATE_FORMAT to use to read or write dates
 	StrpTimeFormat date_format;
 	//! Whether or not there is a date format specified
@@ -73,7 +87,7 @@ struct ReadCSVData : public BaseCSVData {
 void SubstringDetection(string &str_1, string &str_2, string name_str_1, string name_str_2) {
 	if (str_1.find(str_2) != string::npos || str_2.find(str_1) != std::string::npos) {
 		throw BinderException("COPY " + name_str_1 + " must not appear in the " + name_str_2 +
-		                " specification and vice versa");
+		                      " specification and vice versa");
 	}
 }
 
@@ -102,34 +116,44 @@ static string ParseString(vector<Value> &set) {
 	return set[0].str_value;
 }
 
+static idx_t ParseInteger(vector<Value> &set) {
+	if (set.size() != 1) {
+		// no option specified or multiple options specified
+		throw BinderException("Expected a single argument as a integer value");
+	}
+	if (set[0].type == TypeId::FLOAT || set[0].type == TypeId::DOUBLE) {
+		throw BinderException("Expected a integer argument!");
+	}
+	return set[0].CastAs(TypeId::INT64).value_.bigint;
+}
+
 //===--------------------------------------------------------------------===//
 // Bind
 //===--------------------------------------------------------------------===//
 static bool ParseBaseOption(BaseCSVData &bind_data, string &loption, vector<Value> &set) {
 	if (StringUtil::StartsWith(loption, "delim") || StringUtil::StartsWith(loption, "sep")) {
 		bind_data.delimiter = ParseString(set);
-		bind_data.is_auto_detect = false;
+		bind_data.has_delimiter = true;
 		if (bind_data.delimiter.length() == 0) {
-			throw BinderException("QUOTE must not be empty");
+			throw BinderException("DELIM or SEP must not be empty");
 		}
 	} else if (loption == "quote") {
 		bind_data.quote = ParseString(set);
-		bind_data.is_auto_detect = false;
+		bind_data.has_quote = true;
 		if (bind_data.quote.length() == 0) {
 			throw BinderException("QUOTE must not be empty");
 		}
 	} else if (loption == "escape") {
 		bind_data.escape = ParseString(set);
-		bind_data.is_auto_detect = false;
+		bind_data.has_escape = true;
 		if (bind_data.escape.length() == 0) {
 			throw BinderException("ESCAPE must not be empty");
 		}
 	} else if (loption == "header") {
 		bind_data.header = ParseBoolean(set);
-		bind_data.is_auto_detect = false;
+		bind_data.has_header = true;
 	} else if (loption == "null") {
 		bind_data.null_str = ParseString(set);
-		bind_data.is_auto_detect = false;
 	} else if (loption == "encoding") {
 		auto encoding = StringUtil::Lower(ParseString(set));
 		if (encoding != "utf8" && encoding != "utf-8") {
@@ -175,18 +199,18 @@ static vector<bool> ParseColumnList(vector<Value> &set, vector<string> &names) {
 	} else {
 		// list of options: parse the list
 		unordered_map<string, bool> option_map;
-		for(idx_t i = 0; i < set.size(); i++) {
+		for (idx_t i = 0; i < set.size(); i++) {
 			option_map[set[i].ToString()] = false;
 		}
 		result.resize(names.size(), false);
-		for(idx_t i = 0; i < names.size(); i++) {
+		for (idx_t i = 0; i < names.size(); i++) {
 			auto entry = option_map.find(names[i]);
 			if (entry != option_map.end()) {
 				result[i] = true;
 				entry->second = true;
 			}
 		}
-		for(auto entry : option_map) {
+		for (auto entry : option_map) {
 			if (!entry.second) {
 				throw BinderException("Column %s not found in table", entry.first.c_str());
 			}
@@ -196,11 +220,11 @@ static vector<bool> ParseColumnList(vector<Value> &set, vector<string> &names) {
 }
 
 static unique_ptr<FunctionData> write_csv_bind(ClientContext &context, CopyInfo &info, vector<string> &names,
-                                           vector<SQLType> &sql_types) {
+                                               vector<SQLType> &sql_types) {
 	auto bind_data = make_unique<WriteCSVData>(info.file_path, sql_types, names);
 
 	// check all the options in the copy info
-	for(auto &option : info.options) {
+	for (auto &option : info.options) {
 		auto loption = StringUtil::Lower(option.first);
 		auto &set = option.second;
 		if (ParseBaseOption(*bind_data, loption, set)) {
@@ -218,23 +242,41 @@ static unique_ptr<FunctionData> write_csv_bind(ClientContext &context, CopyInfo
 		bind_data->force_quote.resize(names.size(), false);
 	}
 	bind_data->Finalize();
-	bind_data->is_simple = bind_data->delimiter.size() == 1 && bind_data->escape.size() == 1 && bind_data->quote.size() == 1;
+	bind_data->is_simple =
+	    bind_data->delimiter.size() == 1 && bind_data->escape.size() == 1 && bind_data->quote.size() == 1;
 	return move(bind_data);
 }
 
-static unique_ptr<FunctionData> read_csv_bind(ClientContext &context, CopyInfo &info, vector<string> &expected_names, vector<SQLType> &expected_types) {
+static unique_ptr<FunctionData> read_csv_bind(ClientContext &context, CopyInfo &info, vector<string> &expected_names,
+                                              vector<SQLType> &expected_types) {
 	auto bind_data = make_unique<ReadCSVData>(info.file_path, expected_types);
 
 	// check all the options in the copy info
-	for(auto &option : info.options) {
+	for (auto &option : info.options) {
 		auto loption = StringUtil::Lower(option.first);
 		auto &set = option.second;
-		if (ParseBaseOption(*bind_data, loption, set)) {
+		if (loption == "auto_detect") {
+			bind_data->is_auto_detect = ParseBoolean(set);
+		} else if (ParseBaseOption(*bind_data, loption, set)) {
 			// parsed option in base CSV options: continue
 			continue;
+		} else if (loption == "sample_size") {
+			bind_data->sample_size = ParseInteger(set);
+			if (bind_data->sample_size > STANDARD_VECTOR_SIZE) {
+				throw BinderException(
+				    "Unsupported parameter for SAMPLE_SIZE: cannot be bigger than STANDARD_VECTOR_SIZE %d",
+				    STANDARD_VECTOR_SIZE);
+			} else if (bind_data->sample_size < 1) {
+				throw BinderException("Unsupported parameter for SAMPLE_SIZE: cannot be smaller than 1");
+			}
+		} else if (loption == "num_samples") {
+			bind_data->num_samples = ParseInteger(set);
+			if (bind_data->num_samples < 1) {
+				throw BinderException("Unsupported parameter for NUM_SAMPLES: cannot be smaller than 1");
+			}
 		} else if (loption == "force_not_null") {
 			bind_data->force_not_null = ParseColumnList(set, expected_names);
-		}  else if (loption == "date_format" || loption == "dateformat") {
+		} else if (loption == "date_format" || loption == "dateformat") {
 			string format = ParseString(set);
 			string error = StrTimeFormat::ParseFormatSpecifier(format, bind_data->date_format);
 			bind_data->date_format.format_specifier = format;
@@ -263,20 +305,6 @@ static unique_ptr<FunctionData> read_csv_bind(ClientContext &context, CopyInfo &
 	return move(bind_data);
 }
 
-static unique_ptr<FunctionData> read_csv_auto_bind(ClientContext &context, CopyInfo &info, vector<string> &expected_names, vector<SQLType> &expected_types) {
-	auto bind_data = make_unique<ReadCSVData>(info.file_path, expected_types);
-
-	for(auto &option : info.options) {
-		auto loption = StringUtil::Lower(option.first);
-		// auto &set = option.second;
-		// CSV auto accepts no options!
-		throw NotImplementedException("Unrecognized option for CSV_AUTO: %s", option.first.c_str());
-	}
-
-	bind_data->Finalize();
-	return move(bind_data);
-}
-
 //===--------------------------------------------------------------------===//
 // Helper writing functions
 //===--------------------------------------------------------------------===//
@@ -339,7 +367,8 @@ static bool RequiresQuotes(WriteCSVData &options, const char *str, idx_t len) {
 	}
 }
 
-static void WriteQuotedString(Serializer &serializer, WriteCSVData &options, const char *str, idx_t len, bool force_quote) {
+static void WriteQuotedString(Serializer &serializer, WriteCSVData &options, const char *str, idx_t len,
+                              bool force_quote) {
 	if (!force_quote) {
 		// force quote is disabled: check if we need to add quotes anyway
 		force_quote = RequiresQuotes(options, str, len);
@@ -368,7 +397,7 @@ static void WriteQuotedString(Serializer &serializer, WriteCSVData &options, con
 		if (!requires_escape) {
 			// fast path: no need to escape anything
 			serializer.WriteBufferData(options.quote);
-			serializer.WriteData((const_data_ptr_t) str, len);
+			serializer.WriteData((const_data_ptr_t)str, len);
 			serializer.WriteBufferData(options.quote);
 			return;
 		}
@@ -384,7 +413,7 @@ static void WriteQuotedString(Serializer &serializer, WriteCSVData &options, con
 		serializer.WriteBufferData(new_val);
 		serializer.WriteBufferData(options.quote);
 	} else {
-		serializer.WriteData((const_data_ptr_t) str, len);
+		serializer.WriteData((const_data_ptr_t)str, len);
 	}
 }
 
@@ -400,12 +429,13 @@ struct LocalReadCSVData : public LocalFunctionData {
 
 struct GlobalWriteCSVData : public GlobalFunctionData {
 	GlobalWriteCSVData(FileSystem &fs, string file_path) : fs(fs) {
-		handle = fs.OpenFile(file_path, FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE_NEW, FileLockType::WRITE_LOCK);
+		handle = fs.OpenFile(file_path, FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE_NEW,
+		                     FileLockType::WRITE_LOCK);
 	}
 
 	void WriteData(const_data_ptr_t data, idx_t size) {
 		lock_guard<mutex> flock(lock);
-		fs.Write(*handle, (void*) data, size);
+		fs.Write(*handle, (void *)data, size);
 	}
 
 	FileSystem &fs;
@@ -416,7 +446,7 @@ struct GlobalWriteCSVData : public GlobalFunctionData {
 };
 
 static unique_ptr<LocalFunctionData> write_csv_initialize_local(ClientContext &context, FunctionData &bind_data) {
-	auto &csv_data = (WriteCSVData &) bind_data;
+	auto &csv_data = (WriteCSVData &)bind_data;
 	auto local_data = make_unique<LocalReadCSVData>();
 
 	// create the chunk with VARCHAR types
@@ -428,8 +458,8 @@ static unique_ptr<LocalFunctionData> write_csv_initialize_local(ClientContext &c
 }
 
 static unique_ptr<GlobalFunctionData> write_csv_initialize_global(ClientContext &context, FunctionData &bind_data) {
-	auto &csv_data = (WriteCSVData &) bind_data;
-	auto global_data =  make_unique<GlobalWriteCSVData>(FileSystem::GetFileSystem(context), csv_data.file_path);
+	auto &csv_data = (WriteCSVData &)bind_data;
+	auto global_data = make_unique<GlobalWriteCSVData>(FileSystem::GetFileSystem(context), csv_data.file_path);
 
 	if (csv_data.header) {
 		BufferedSerializer serializer;
@@ -447,11 +477,11 @@ static unique_ptr<GlobalFunctionData> write_csv_initialize_global(ClientContext
 	return move(global_data);
 }
 
-
-static void write_csv_sink(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate, LocalFunctionData &lstate, DataChunk &input) {
-	auto &csv_data = (WriteCSVData &) bind_data;
-	auto &local_data = (LocalReadCSVData &) lstate;
-	auto &global_state = (GlobalWriteCSVData &) gstate;
+static void write_csv_sink(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
+                           LocalFunctionData &lstate, DataChunk &input) {
+	auto &csv_data = (WriteCSVData &)bind_data;
+	auto &local_data = (LocalReadCSVData &)lstate;
+	auto &global_state = (GlobalWriteCSVData &)gstate;
 
 	// write data into the local buffer
 
@@ -465,7 +495,7 @@ static void write_csv_sink(ClientContext &context, FunctionData &bind_data, Glob
 		} else {
 			// non varchar column, perform the cast
 			VectorOperations::Cast(input.data[col_idx], cast_chunk.data[col_idx], csv_data.sql_types[col_idx],
-									SQLType::VARCHAR, input.size());
+			                       SQLType::VARCHAR, input.size());
 		}
 	}
 
@@ -487,8 +517,11 @@ static void write_csv_sink(ClientContext &context, FunctionData &bind_data, Glob
 			// non-null value, fetch the string value from the cast chunk
 			auto str_data = FlatVector::GetData<string_t>(cast_chunk.data[col_idx]);
 			auto str_value = str_data[row_idx];
-			// FIXME: we could gain some performance here by checking for certain types if they ever require quotes (e.g. integers only require quotes if the delimiter is a number, decimals only require quotes if the delimiter is a number or "." character)
-			WriteQuotedString(writer, csv_data, str_value.GetData(), str_value.GetSize(), csv_data.force_quote[col_idx]);
+			// FIXME: we could gain some performance here by checking for certain types if they ever require quotes
+			// (e.g. integers only require quotes if the delimiter is a number, decimals only require quotes if the
+			// delimiter is a number or "." character)
+			WriteQuotedString(writer, csv_data, str_value.GetData(), str_value.GetSize(),
+			                  csv_data.force_quote[col_idx]);
 		}
 		writer.WriteBufferData(csv_data.newline);
 	}
@@ -503,9 +536,9 @@ static void write_csv_sink(ClientContext &context, FunctionData &bind_data, Glob
 // Combine
 //===--------------------------------------------------------------------===//
 static void write_csv_combine(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
-                          LocalFunctionData &lstate) {
-	auto &local_data = (LocalReadCSVData &) lstate;
-	auto &global_state = (GlobalWriteCSVData &) gstate;
+                              LocalFunctionData &lstate) {
+	auto &local_data = (LocalReadCSVData &)lstate;
+	auto &global_state = (GlobalWriteCSVData &)gstate;
 	auto &writer = local_data.serializer;
 	// flush the local writer
 	if (writer.blob.size > 0) {
@@ -523,20 +556,26 @@ struct GlobalReadCSVData : public GlobalFunctionData {
 
 unique_ptr<GlobalFunctionData> read_csv_initialize(ClientContext &context, FunctionData &fdata) {
 	auto global_data = make_unique<GlobalReadCSVData>();
-	auto &bind_data = (ReadCSVData&) fdata;
+	auto &bind_data = (ReadCSVData &)fdata;
 
 	// set up the CSV reader with the parsed options
-    BufferedCSVReaderOptions options;
+	BufferedCSVReaderOptions options;
 	options.file_path = bind_data.file_path;
 	options.auto_detect = bind_data.is_auto_detect;
+	options.has_delimiter = bind_data.has_delimiter;
 	options.delimiter = bind_data.delimiter;
+	options.has_quote = bind_data.has_quote;
 	options.quote = bind_data.quote;
+	options.has_escape = bind_data.has_escape;
 	options.escape = bind_data.escape;
+	options.has_header = bind_data.has_header;
 	options.header = bind_data.header;
 	options.null_str = bind_data.null_str;
 	options.skip_rows = 0;
 	options.num_cols = bind_data.sql_types.size();
 	options.force_not_null = bind_data.force_not_null;
+	options.sample_size = bind_data.sample_size;
+	options.num_samples = bind_data.num_samples;
 	options.has_date_format = bind_data.has_date_format;
 	options.date_format = move(bind_data.date_format);
 	options.has_timestamp_format = bind_data.has_timestamp_format;
@@ -546,9 +585,10 @@ unique_ptr<GlobalFunctionData> read_csv_initialize(ClientContext &context, Funct
 	return move(global_data);
 }
 
-void read_csv_get_chunk(ExecutionContext &context, GlobalFunctionData &gstate, FunctionData &bind_data, DataChunk &chunk) {
+void read_csv_get_chunk(ExecutionContext &context, GlobalFunctionData &gstate, FunctionData &bind_data,
+                        DataChunk &chunk) {
 	// read a chunk from the CSV reader
-	auto &gdata = (GlobalReadCSVData &) gstate;
+	auto &gdata = (GlobalReadCSVData &)gstate;
 	gdata.csv_reader->ParseCSV(chunk);
 }
 
@@ -564,14 +604,7 @@ void CSVCopyFunction::RegisterFunction(BuiltinFunctions &set) {
 	info.copy_from_initialize = read_csv_initialize;
 	info.copy_from_get_chunk = read_csv_get_chunk;
 
-	// CSV_AUTO can only be used in COPY FROM
-	CopyFunction auto_info("csv_auto");
-	auto_info.copy_from_bind = read_csv_auto_bind;
-	auto_info.copy_from_initialize = read_csv_initialize;
-	auto_info.copy_from_get_chunk = read_csv_get_chunk;
-
 	set.AddFunction(info);
-	set.AddFunction(auto_info);
 }
 
 } // namespace duckdb
diff --git a/src/function/table/read_csv.cpp b/src/function/table/read_csv.cpp
index f5e6723ee56c..8a946d2931d1 100644
--- a/src/function/table/read_csv.cpp
+++ b/src/function/table/read_csv.cpp
@@ -16,7 +16,8 @@ struct ReadCSVFunctionData : public TableFunctionData {
 	unique_ptr<BufferedCSVReader> csv_reader;
 };
 
-static unique_ptr<FunctionData> read_csv_bind(ClientContext &context, vector<Value> &inputs, unordered_map<string, Value> &named_parameters,
+static unique_ptr<FunctionData> read_csv_bind(ClientContext &context, vector<Value> &inputs,
+                                              unordered_map<string, Value> &named_parameters,
                                               vector<SQLType> &return_types, vector<string> &names) {
 
 	if (!context.db.config.enable_copy) {
@@ -26,27 +27,42 @@ static unique_ptr<FunctionData> read_csv_bind(ClientContext &context, vector<Val
 
 	BufferedCSVReaderOptions options;
 	options.file_path = inputs[0].str_value;
-	options.auto_detect = true;
+	options.auto_detect = false;
 	options.header = false;
 	options.delimiter = ",";
 	options.quote = "\"";
 
-	for(auto &kv : named_parameters) {
-		if (kv.first == "sep") {
-			options.auto_detect = false;
+	for (auto &kv : named_parameters) {
+		if (kv.first == "auto_detect") {
+			options.auto_detect = kv.second.value_.boolean;
+		} else if (kv.first == "sep" || kv.first == "delim") {
 			options.delimiter = kv.second.str_value;
+			options.has_delimiter = true;
 		} else if (kv.first == "header") {
-			options.auto_detect = false;
 			options.header = kv.second.value_.boolean;
+			options.has_header = true;
 		} else if (kv.first == "quote") {
-			options.auto_detect = false;
 			options.quote = kv.second.str_value;
+			options.has_quote = true;
 		} else if (kv.first == "escape") {
-			options.auto_detect = false;
 			options.escape = kv.second.str_value;
+			options.has_escape = true;
 		} else if (kv.first == "nullstr") {
-			options.auto_detect = false;
 			options.null_str = kv.second.str_value;
+		} else if (kv.first == "sample_size") {
+			options.sample_size = kv.second.CastAs(TypeId::INT64).value_.bigint;
+			if (options.sample_size > STANDARD_VECTOR_SIZE) {
+				throw BinderException(
+				    "Unsupported parameter for SAMPLE_SIZE: cannot be bigger than STANDARD_VECTOR_SIZE %d",
+				    STANDARD_VECTOR_SIZE);
+			} else if (options.sample_size < 1) {
+				throw BinderException("Unsupported parameter for SAMPLE_SIZE: cannot be smaller than 1");
+			}
+		} else if (kv.first == "num_samples") {
+			options.num_samples = kv.second.CastAs(TypeId::INT64).value_.bigint;
+			if (options.num_samples < 1) {
+				throw BinderException("Unsupported parameter for NUM_SAMPLES: cannot be smaller than 1");
+			}
 		} else if (kv.first == "dateformat") {
 			options.has_date_format = true;
 			options.date_format.format_specifier = kv.second.str_value;
@@ -62,7 +78,6 @@ static unique_ptr<FunctionData> read_csv_bind(ClientContext &context, vector<Val
 				throw InvalidInputException("Could not parse TIMESTAMPFORMAT: %s", error.c_str());
 			}
 		} else if (kv.first == "columns") {
-			options.auto_detect = false;
 			for (auto &val : kv.second.struct_value) {
 				names.push_back(val.first);
 				if (val.second.type != TypeId::VARCHAR) {
@@ -91,24 +106,11 @@ static unique_ptr<FunctionData> read_csv_bind(ClientContext &context, vector<Val
 	return move(result);
 }
 
-static unique_ptr<FunctionData> read_csv_auto_bind(ClientContext &context, vector<Value> &inputs, unordered_map<string, Value> &named_parameters,
+static unique_ptr<FunctionData> read_csv_auto_bind(ClientContext &context, vector<Value> &inputs,
+                                                   unordered_map<string, Value> &named_parameters,
                                                    vector<SQLType> &return_types, vector<string> &names) {
-
-	if (!context.db.config.enable_copy) {
-		throw Exception("read_csv_auto is disabled by configuration");
-	}
-	auto result = make_unique<ReadCSVFunctionData>();
-	BufferedCSVReaderOptions options;
-	options.auto_detect = true;
-	options.file_path = inputs[0].str_value;
-
-	result->csv_reader = make_unique<BufferedCSVReader>(context, move(options));
-
-	// TODO: print detected dialect from result->csv_reader->info
-	return_types.assign(result->csv_reader->sql_types.begin(), result->csv_reader->sql_types.end());
-	names.assign(result->csv_reader->col_names.begin(), result->csv_reader->col_names.end());
-
-	return move(result);
+	named_parameters["auto_detect"] = Value::BOOLEAN(true);
+	return read_csv_bind(context, inputs, named_parameters, return_types, names);
 }
 
 static void read_csv_info(ClientContext &context, vector<Value> &input, DataChunk &output, FunctionData *dataptr) {
@@ -116,23 +118,31 @@ static void read_csv_info(ClientContext &context, vector<Value> &input, DataChun
 	data.csv_reader->ParseCSV(output);
 }
 
+static void add_named_parameters(TableFunction &table_function) {
+	table_function.named_parameters["sep"] = SQLType::VARCHAR;
+	table_function.named_parameters["delim"] = SQLType::VARCHAR;
+	table_function.named_parameters["quote"] = SQLType::VARCHAR;
+	table_function.named_parameters["escape"] = SQLType::VARCHAR;
+	table_function.named_parameters["nullstr"] = SQLType::VARCHAR;
+	table_function.named_parameters["columns"] = SQLType::STRUCT;
+	table_function.named_parameters["header"] = SQLType::BOOLEAN;
+	table_function.named_parameters["auto_detect"] = SQLType::BOOLEAN;
+	table_function.named_parameters["sample_size"] = SQLType::BIGINT;
+	table_function.named_parameters["num_samples"] = SQLType::BIGINT;
+	table_function.named_parameters["dateformat"] = SQLType::VARCHAR;
+	table_function.named_parameters["timestampformat"] = SQLType::VARCHAR;
+}
+
 void ReadCSVTableFunction::RegisterFunction(BuiltinFunctions &set) {
-	TableFunctionSet read_csv("read_csv");
-
-	TableFunction read_csv_function = TableFunction({SQLType::VARCHAR}, read_csv_bind, read_csv_info, nullptr);
-	read_csv_function.named_parameters["sep"] = SQLType::VARCHAR;
-	read_csv_function.named_parameters["quote"] = SQLType::VARCHAR;
-	read_csv_function.named_parameters["escape"] = SQLType::VARCHAR;
-	read_csv_function.named_parameters["nullstr"] = SQLType::VARCHAR;
-	read_csv_function.named_parameters["columns"] = SQLType::STRUCT;
-	read_csv_function.named_parameters["header"] = SQLType::BOOLEAN;
-	read_csv_function.named_parameters["dateformat"] = SQLType::VARCHAR;
-	read_csv_function.named_parameters["timestampformat"] = SQLType::VARCHAR;
-
-	read_csv.AddFunction(move(read_csv_function));
-
-	set.AddFunction(read_csv);
-	set.AddFunction(TableFunction("read_csv_auto", {SQLType::VARCHAR}, read_csv_auto_bind, read_csv_info, nullptr));
+	TableFunction read_csv_function =
+	    TableFunction("read_csv", {SQLType::VARCHAR}, read_csv_bind, read_csv_info, nullptr);
+	add_named_parameters(read_csv_function);
+	set.AddFunction(read_csv_function);
+
+	TableFunction read_csv_auto_function =
+	    TableFunction("read_csv_auto", {SQLType::VARCHAR}, read_csv_auto_bind, read_csv_info, nullptr);
+	add_named_parameters(read_csv_auto_function);
+	set.AddFunction(read_csv_auto_function);
 }
 
 void BuiltinFunctions::RegisterReadFunctions() {
diff --git a/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp b/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp
index 388162d386fd..5eddf2eb2ca3 100644
--- a/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp
+++ b/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp
@@ -14,10 +14,10 @@
 
 #include <sstream>
 
-#define SAMPLE_CHUNK_SIZE 100
-#if STANDARD_VECTOR_SIZE < SAMPLE_CHUNK_SIZE
-#undef SAMPLE_CHUNK_SIZE
-#define SAMPLE_CHUNK_SIZE STANDARD_VECTOR_SIZE
+#define DEFAULT_SAMPLE_CHUNK_SIZE 100
+#if STANDARD_VECTOR_SIZE < DEFAULT_SAMPLE_CHUNK_SIZE
+#undef DEFAULT_SAMPLE_CHUNK_SIZE
+#define DEFAULT_SAMPLE_CHUNK_SIZE STANDARD_VECTOR_SIZE
 #endif
 
 namespace duckdb {
@@ -39,6 +39,9 @@ struct TextSearchShiftArray {
 	TextSearchShiftArray(string search_term);
 
 	inline bool Match(uint8_t &position, uint8_t byte_value) {
+		if (position >= length) {
+			return false;
+		}
 		position = shifts[position * 255 + byte_value];
 		return position == length;
 	}
@@ -47,27 +50,39 @@ struct TextSearchShiftArray {
 	unique_ptr<uint8_t[]> shifts;
 };
 
-struct BufferedCSVReaderOptions  {
+struct BufferedCSVReaderOptions {
 	//! The file path of the CSV file to read
 	string file_path;
-    //! Whether or not to automatically detect dialect and datatypes
-    bool auto_detect;
-    //! Delimiter to separate columns within each line
-    string delimiter;
-    //! Quote used for columns that contain reserved characters, e.g., delimiter
-    string quote;
-    //! Escape character to escape quote character
-    string escape;
-    //! Whether or not the file has a header line
-    bool header = false;
-    //! How many leading rows to skip
-    idx_t skip_rows = 0;
-    //! Expected number of columns
-    idx_t num_cols = 0;
-    //! Specifies the string that represents a null value
-    string null_str;
-    //! True, if column with that index must skip null check
-    vector<bool> force_not_null;
+	//! Whether or not to automatically detect dialect and datatypes
+	bool auto_detect = true;
+	//! Whether or not a delimiter was defined by the user
+	bool has_delimiter = false;
+	//! Delimiter to separate columns within each line
+	string delimiter;
+	//! Whether or not a quote sign was defined by the user
+	bool has_quote = false;
+	//! Quote used for columns that contain reserved characters, e.g., delimiter
+	string quote;
+	//! Whether or not an escape character was defined by the user
+	bool has_escape = false;
+	//! Escape character to escape quote character
+	string escape;
+	//! Whether or not a header information was given by the user
+	bool has_header = false;
+	//! Whether or not the file has a header line
+	bool header = false;
+	//! How many leading rows to skip
+	idx_t skip_rows = 0;
+	//! Expected number of columns
+	idx_t num_cols = 0;
+	//! Specifies the string that represents a null value
+	string null_str;
+	//! True, if column with that index must skip null check
+	vector<bool> force_not_null;
+	//! Size of sample chunk used for dialect and type detection
+	idx_t sample_size = DEFAULT_SAMPLE_CHUNK_SIZE;
+	//! Number of sample chunks used for type detection
+	idx_t num_samples = 10;
 	//! The date format to use (if any is specified)
 	StrpTimeFormat date_format;
 	//! Whether or not a date format is specified
@@ -90,12 +105,22 @@ class BufferedCSVReader {
 	static constexpr idx_t INITIAL_BUFFER_SIZE = 16384;
 	//! Maximum CSV line size: specified because if we reach this amount, we likely have the wrong delimiters
 	static constexpr idx_t MAXIMUM_CSV_LINE_SIZE = 1048576;
-	static constexpr uint8_t MAX_SAMPLE_CHUNKS = 10;
 	ParserMode mode;
 
+	//! Candidates for delimiter auto detection
+	vector<string> delim_candidates = {",", "|", ";", "\t"};
+	//! Candidates for quote rule auto detection
+	vector<QuoteRule> quoterule_candidates = {QuoteRule::QUOTES_RFC, QuoteRule::QUOTES_OTHER, QuoteRule::NO_QUOTES};
+	//! Candidates for quote sign auto detection (per quote rule)
+	vector<vector<string>> quote_candidates_map = {{"\""}, {"\"", "'"}, {""}};
+	//! Candidates for escape character auto detection (per quote rule)
+	vector<vector<string>> escape_candidates_map = {{""}, {"\\"}, {""}};
+
 public:
-	BufferedCSVReader(ClientContext &context, BufferedCSVReaderOptions options, vector<SQLType> requested_types = vector<SQLType>());
-	BufferedCSVReader(BufferedCSVReaderOptions options, vector<SQLType> requested_types, unique_ptr<std::istream> source);
+	BufferedCSVReader(ClientContext &context, BufferedCSVReaderOptions options,
+	                  vector<SQLType> requested_types = vector<SQLType>());
+	BufferedCSVReader(BufferedCSVReaderOptions options, vector<SQLType> requested_types,
+	                  unique_ptr<std::istream> source);
 
 	BufferedCSVReaderOptions options;
 	vector<SQLType> sql_types;
@@ -112,9 +137,13 @@ class BufferedCSVReader {
 	idx_t linenr = 0;
 	bool linenr_estimated = false;
 
+	idx_t SAMPLE_CHUNK_SIZE;
+	idx_t MAX_SAMPLE_CHUNKS;
+
 	vector<idx_t> sniffed_column_counts;
 	uint8_t sample_chunk_idx = 0;
 	bool jumping_samples = false;
+	bool end_of_file_reached = false;
 
 	idx_t bytes_in_chunk = 0;
 	double bytes_per_line_avg = 0;
@@ -142,10 +171,12 @@ class BufferedCSVReader {
 	vector<SQLType> SniffCSV(vector<SQLType> requested_types);
 	//! Try to cast a string value to the specified sql type
 	bool TryCastValue(Value value, SQLType sql_type);
+	//! Try to cast a vector of values to the specified sql type
+	bool TryCastVector(Vector &parse_chunk_col, idx_t size, SQLType sql_type);
 	//! Skips header rows and skip_rows in the input stream
-	void SkipHeader();
+	void SkipHeader(idx_t skip_rows, bool skip_header);
 	//! Jumps back to the beginning of input stream and resets necessary internal states
-	void JumpToBeginning();
+	void JumpToBeginning(idx_t skip_rows, bool skip_header);
 	//! Jumps back to the beginning of input stream and resets necessary internal states
 	bool JumpToNextSample();
 	//! Resets the buffer
@@ -154,6 +185,10 @@ class BufferedCSVReader {
 	void ResetStream();
 	//! Resets the parse_chunk and related internal states, keep_types keeps the parse_chunk initialized
 	void ResetParseChunk();
+	//! Sets size of sample chunk and number of chunks for dialect and type detection
+	void ConfigureSampling();
+	//! Prepare candidate sets for auto detection based on user input
+	void PrepareCandidateSets();
 
 	//! Parses a CSV file with a one-byte delimiter, escape and quote character
 	void ParseSimpleCSV(DataChunk &insert_chunk);
diff --git a/src/parser/transform/statement/transform_copy.cpp b/src/parser/transform/statement/transform_copy.cpp
index 66a2f81365cb..9e1fce5c2ad2 100644
--- a/src/parser/transform/statement/transform_copy.cpp
+++ b/src/parser/transform/statement/transform_copy.cpp
@@ -68,8 +68,7 @@ unique_ptr<CopyStatement> Transformer::TransformCopy(PGNode *node) {
 				// format specifier: interpret this option
 				auto *format_val = (PGValue *)(def_elem->arg);
 				if (!format_val || format_val->type != T_PGString) {
-					throw ParserException(
-					    "Unsupported parameter type for FORMAT: expected e.g. FORMAT 'csv', 'csv_auto'");
+					throw ParserException("Unsupported parameter type for FORMAT: expected e.g. FORMAT 'csv', 'parquet'");
 				}
 				info.format = StringUtil::Lower(format_val->val.str);
 				continue;
