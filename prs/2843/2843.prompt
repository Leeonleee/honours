You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Not writing parquet statistics when creating parquet files
#### What happens?

When creating a parquet file with DuckDB the metadata for each column chunk is not included.

There are no statistics written in the Parquet file especially when viewed with parquet_metadata().

Please add the ability to write these statistics.

#### To Reproduce

Create a parquet file from a table and inspect the metadata using parquet_metadata(), you will see that stats_min and stats_max are null for all columns.

#### Environment (please complete the following information):
 - OS: MacOSX
 - DuckDB Version: HEAD
 - DuckDB Client: Python

#### Before Submitting

- [x] **Have you tried this on the latest `master` branch?** yes

- [x] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**


</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/docs/why_duckdb.html).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
44: 
45: 
[end of README.md]
[start of extension/parquet/column_reader.cpp]
1: #include "column_reader.hpp"
2: #include "parquet_timestamp.hpp"
3: #include "utf8proc_wrapper.hpp"
4: #include "parquet_reader.hpp"
5: 
6: #include "boolean_column_reader.hpp"
7: #include "callback_column_reader.hpp"
8: #include "decimal_column_reader.hpp"
9: #include "list_column_reader.hpp"
10: #include "string_column_reader.hpp"
11: #include "struct_column_reader.hpp"
12: #include "templated_column_reader.hpp"
13: 
14: #include "snappy.h"
15: #include "miniz_wrapper.hpp"
16: #include "zstd.h"
17: #include <iostream>
18: 
19: #include "duckdb.hpp"
20: #ifndef DUCKDB_AMALGAMATION
21: #include "duckdb/common/types/blob.hpp"
22: #include "duckdb/common/types/chunk_collection.hpp"
23: #endif
24: 
25: namespace duckdb {
26: 
27: using duckdb_parquet::format::CompressionCodec;
28: using duckdb_parquet::format::ConvertedType;
29: using duckdb_parquet::format::Encoding;
30: using duckdb_parquet::format::PageType;
31: using duckdb_parquet::format::Type;
32: 
33: const uint32_t RleBpDecoder::BITPACK_MASKS[] = {
34:     0,       1,       3,        7,        15,       31,        63,        127,       255,        511,       1023,
35:     2047,    4095,    8191,     16383,    32767,    65535,     131071,    262143,    524287,     1048575,   2097151,
36:     4194303, 8388607, 16777215, 33554431, 67108863, 134217727, 268435455, 536870911, 1073741823, 2147483647};
37: 
38: const uint8_t RleBpDecoder::BITPACK_DLEN = 8;
39: 
40: ColumnReader::ColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
41:                            idx_t max_define_p, idx_t max_repeat_p)
42:     : schema(schema_p), file_idx(file_idx_p), max_define(max_define_p), max_repeat(max_repeat_p), reader(reader),
43:       type(move(type_p)), page_rows_available(0) {
44: 
45: 	// dummies for Skip()
46: 	none_filter.none();
47: 	dummy_define.resize(reader.allocator, STANDARD_VECTOR_SIZE);
48: 	dummy_repeat.resize(reader.allocator, STANDARD_VECTOR_SIZE);
49: }
50: 
51: ColumnReader::~ColumnReader() {
52: }
53: 
54: template <class T>
55: unique_ptr<ColumnReader> CreateDecimalReader(ParquetReader &reader, const LogicalType &type_p,
56:                                              const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,
57:                                              idx_t max_repeat) {
58: 	switch (type_p.InternalType()) {
59: 	case PhysicalType::INT16:
60: 		return make_unique<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<T>>>(
61: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
62: 	case PhysicalType::INT32:
63: 		return make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<T>>>(
64: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
65: 	case PhysicalType::INT64:
66: 		return make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<T>>>(
67: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
68: 	default:
69: 		throw NotImplementedException("Unimplemented internal type for CreateDecimalReader");
70: 	}
71: }
72: 
73: unique_ptr<ColumnReader> ColumnReader::CreateReader(ParquetReader &reader, const LogicalType &type_p,
74:                                                     const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,
75:                                                     idx_t max_repeat) {
76: 	switch (type_p.id()) {
77: 	case LogicalTypeId::BOOLEAN:
78: 		return make_unique<BooleanColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
79: 	case LogicalTypeId::UTINYINT:
80: 		return make_unique<TemplatedColumnReader<uint8_t, TemplatedParquetValueConversion<uint32_t>>>(
81: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
82: 	case LogicalTypeId::USMALLINT:
83: 		return make_unique<TemplatedColumnReader<uint16_t, TemplatedParquetValueConversion<uint32_t>>>(
84: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
85: 	case LogicalTypeId::UINTEGER:
86: 		return make_unique<TemplatedColumnReader<uint32_t, TemplatedParquetValueConversion<uint32_t>>>(
87: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
88: 	case LogicalTypeId::UBIGINT:
89: 		return make_unique<TemplatedColumnReader<uint64_t, TemplatedParquetValueConversion<uint64_t>>>(
90: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
91: 	case LogicalTypeId::TINYINT:
92: 		return make_unique<TemplatedColumnReader<int8_t, TemplatedParquetValueConversion<int32_t>>>(
93: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
94: 	case LogicalTypeId::SMALLINT:
95: 		return make_unique<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<int32_t>>>(
96: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
97: 	case LogicalTypeId::INTEGER:
98: 		return make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<int32_t>>>(
99: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
100: 	case LogicalTypeId::BIGINT:
101: 		return make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<int64_t>>>(
102: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
103: 	case LogicalTypeId::FLOAT:
104: 		return make_unique<TemplatedColumnReader<float, TemplatedParquetValueConversion<float>>>(
105: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
106: 	case LogicalTypeId::DOUBLE:
107: 		return make_unique<TemplatedColumnReader<double, TemplatedParquetValueConversion<double>>>(
108: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
109: 	case LogicalTypeId::TIMESTAMP:
110: 		switch (schema_p.type) {
111: 		case Type::INT96:
112: 			return make_unique<CallbackColumnReader<Int96, timestamp_t, ImpalaTimestampToTimestamp>>(
113: 			    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
114: 		case Type::INT64:
115: 			switch (schema_p.converted_type) {
116: 			case ConvertedType::TIMESTAMP_MICROS:
117: 				return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMicrosToTimestamp>>(
118: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
119: 			case ConvertedType::TIMESTAMP_MILLIS:
120: 				return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMsToTimestamp>>(
121: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
122: 			default:
123: 				break;
124: 			}
125: 		default:
126: 			break;
127: 		}
128: 		break;
129: 	case LogicalTypeId::DATE:
130: 		return make_unique<CallbackColumnReader<int32_t, date_t, ParquetIntToDate>>(reader, type_p, schema_p,
131: 		                                                                            file_idx_p, max_define, max_repeat);
132: 	case LogicalTypeId::BLOB:
133: 	case LogicalTypeId::VARCHAR:
134: 		return make_unique<StringColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
135: 	case LogicalTypeId::DECIMAL:
136: 		// we have to figure out what kind of int we need
137: 		switch (schema_p.type) {
138: 		case Type::INT32:
139: 			return CreateDecimalReader<int32_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
140: 		case Type::INT64:
141: 			return CreateDecimalReader<int64_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
142: 		case Type::FIXED_LEN_BYTE_ARRAY:
143: 			switch (type_p.InternalType()) {
144: 			case PhysicalType::INT16:
145: 				return make_unique<DecimalColumnReader<int16_t>>(reader, type_p, schema_p, file_idx_p, max_define,
146: 				                                                 max_repeat);
147: 			case PhysicalType::INT32:
148: 				return make_unique<DecimalColumnReader<int32_t>>(reader, type_p, schema_p, file_idx_p, max_define,
149: 				                                                 max_repeat);
150: 			case PhysicalType::INT64:
151: 				return make_unique<DecimalColumnReader<int64_t>>(reader, type_p, schema_p, file_idx_p, max_define,
152: 				                                                 max_repeat);
153: 			case PhysicalType::INT128:
154: 				return make_unique<DecimalColumnReader<hugeint_t>>(reader, type_p, schema_p, file_idx_p, max_define,
155: 				                                                   max_repeat);
156: 			default:
157: 				throw InternalException("Unrecognized type for Decimal");
158: 			}
159: 		default:
160: 			throw NotImplementedException("Unrecognized Parquet type for Decimal");
161: 		}
162: 		break;
163: 	default:
164: 		break;
165: 	}
166: 	throw NotImplementedException(type_p.ToString());
167: }
168: 
169: void ColumnReader::PrepareRead(parquet_filter_t &filter) {
170: 	dict_decoder.reset();
171: 	defined_decoder.reset();
172: 	block.reset();
173: 
174: 	PageHeader page_hdr;
175: 	page_hdr.read(protocol);
176: 
177: 	//	page_hdr.printTo(std::cout);
178: 	//	std::cout << '\n';
179: 
180: 	PreparePage(page_hdr.compressed_page_size, page_hdr.uncompressed_page_size);
181: 
182: 	switch (page_hdr.type) {
183: 	case PageType::DATA_PAGE_V2:
184: 	case PageType::DATA_PAGE:
185: 		PrepareDataPage(page_hdr);
186: 		break;
187: 	case PageType::DICTIONARY_PAGE:
188: 		Dictionary(move(block), page_hdr.dictionary_page_header.num_values);
189: 		break;
190: 	default:
191: 		break; // ignore INDEX page type and any other custom extensions
192: 	}
193: }
194: 
195: void ColumnReader::PreparePage(idx_t compressed_page_size, idx_t uncompressed_page_size) {
196: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
197: 
198: 	block = make_shared<ResizeableBuffer>(reader.allocator, compressed_page_size + 1);
199: 	trans.read((uint8_t *)block->ptr, compressed_page_size);
200: 
201: 	shared_ptr<ResizeableBuffer> unpacked_block;
202: 	if (chunk->meta_data.codec != CompressionCodec::UNCOMPRESSED) {
203: 		unpacked_block = make_shared<ResizeableBuffer>(reader.allocator, uncompressed_page_size + 1);
204: 	}
205: 
206: 	switch (chunk->meta_data.codec) {
207: 	case CompressionCodec::UNCOMPRESSED:
208: 		break;
209: 	case CompressionCodec::GZIP: {
210: 		MiniZStream s;
211: 
212: 		s.Decompress((const char *)block->ptr, compressed_page_size, (char *)unpacked_block->ptr,
213: 		             uncompressed_page_size);
214: 		block = move(unpacked_block);
215: 
216: 		break;
217: 	}
218: 	case CompressionCodec::SNAPPY: {
219: 		auto res =
220: 		    duckdb_snappy::RawUncompress((const char *)block->ptr, compressed_page_size, (char *)unpacked_block->ptr);
221: 		if (!res) {
222: 			throw std::runtime_error("Decompression failure");
223: 		}
224: 		block = move(unpacked_block);
225: 		break;
226: 	}
227: 	case CompressionCodec::ZSTD: {
228: 		auto res = duckdb_zstd::ZSTD_decompress((char *)unpacked_block->ptr, uncompressed_page_size,
229: 		                                        (const char *)block->ptr, compressed_page_size);
230: 		if (duckdb_zstd::ZSTD_isError(res) || res != (size_t)uncompressed_page_size) {
231: 			throw std::runtime_error("ZSTD Decompression failure");
232: 		}
233: 		block = move(unpacked_block);
234: 		break;
235: 	}
236: 
237: 	default: {
238: 		std::stringstream codec_name;
239: 		codec_name << chunk->meta_data.codec;
240: 		throw std::runtime_error("Unsupported compression codec \"" + codec_name.str() +
241: 		                         "\". Supported options are uncompressed, gzip or snappy");
242: 		break;
243: 	}
244: 	}
245: }
246: 
247: void ColumnReader::PrepareDataPage(PageHeader &page_hdr) {
248: 	if (page_hdr.type == PageType::DATA_PAGE && !page_hdr.__isset.data_page_header) {
249: 		throw std::runtime_error("Missing data page header from data page");
250: 	}
251: 	if (page_hdr.type == PageType::DATA_PAGE_V2 && !page_hdr.__isset.data_page_header_v2) {
252: 		throw std::runtime_error("Missing data page header from data page v2");
253: 	}
254: 
255: 	page_rows_available = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.num_values
256: 	                                                           : page_hdr.data_page_header_v2.num_values;
257: 	auto page_encoding = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.encoding
258: 	                                                          : page_hdr.data_page_header_v2.encoding;
259: 
260: 	if (HasRepeats()) {
261: 		uint32_t rep_length = page_hdr.type == PageType::DATA_PAGE
262: 		                          ? block->read<uint32_t>()
263: 		                          : page_hdr.data_page_header_v2.repetition_levels_byte_length;
264: 		block->available(rep_length);
265: 		repeated_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, rep_length,
266: 		                                             RleBpDecoder::ComputeBitWidth(max_repeat));
267: 		block->inc(rep_length);
268: 	}
269: 
270: 	if (HasDefines()) {
271: 		uint32_t def_length = page_hdr.type == PageType::DATA_PAGE
272: 		                          ? block->read<uint32_t>()
273: 		                          : page_hdr.data_page_header_v2.definition_levels_byte_length;
274: 		block->available(def_length);
275: 		defined_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, def_length,
276: 		                                            RleBpDecoder::ComputeBitWidth(max_define));
277: 		block->inc(def_length);
278: 	}
279: 
280: 	switch (page_encoding) {
281: 	case Encoding::RLE_DICTIONARY:
282: 	case Encoding::PLAIN_DICTIONARY: {
283: 		// TODO there seems to be some confusion whether this is in the bytes for v2
284: 		// where is it otherwise??
285: 		auto dict_width = block->read<uint8_t>();
286: 		// TODO somehow dict_width can be 0 ?
287: 		dict_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, block->len, dict_width);
288: 		block->inc(block->len);
289: 		break;
290: 	}
291: 	case Encoding::PLAIN:
292: 		// nothing to do here, will be read directly below
293: 		break;
294: 
295: 	default:
296: 		throw std::runtime_error("Unsupported page encoding");
297: 	}
298: }
299: 
300: idx_t ColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
301:                          Vector &result) {
302: 	// we need to reset the location because multiple column readers share the same protocol
303: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
304: 	trans.SetLocation(chunk_read_offset);
305: 
306: 	idx_t result_offset = 0;
307: 	auto to_read = num_values;
308: 
309: 	while (to_read > 0) {
310: 		while (page_rows_available == 0) {
311: 			PrepareRead(filter);
312: 		}
313: 
314: 		D_ASSERT(block);
315: 		auto read_now = MinValue<idx_t>(to_read, page_rows_available);
316: 
317: 		D_ASSERT(read_now <= STANDARD_VECTOR_SIZE);
318: 
319: 		if (HasRepeats()) {
320: 			D_ASSERT(repeated_decoder);
321: 			repeated_decoder->GetBatch<uint8_t>((char *)repeat_out + result_offset, read_now);
322: 		}
323: 
324: 		if (HasDefines()) {
325: 			D_ASSERT(defined_decoder);
326: 			defined_decoder->GetBatch<uint8_t>((char *)define_out + result_offset, read_now);
327: 		}
328: 
329: 		if (dict_decoder) {
330: 			// we need the null count because the offsets and plain values have no entries for nulls
331: 			idx_t null_count = 0;
332: 			if (HasDefines()) {
333: 				for (idx_t i = 0; i < read_now; i++) {
334: 					if (define_out[i + result_offset] != max_define) {
335: 						null_count++;
336: 					}
337: 				}
338: 			}
339: 
340: 			offset_buffer.resize(reader.allocator, sizeof(uint32_t) * (read_now - null_count));
341: 			dict_decoder->GetBatch<uint32_t>(offset_buffer.ptr, read_now - null_count);
342: 			DictReference(result);
343: 			Offsets((uint32_t *)offset_buffer.ptr, define_out, read_now, filter, result_offset, result);
344: 		} else {
345: 			PlainReference(block, result);
346: 			Plain(block, define_out, read_now, filter, result_offset, result);
347: 		}
348: 
349: 		result_offset += read_now;
350: 		page_rows_available -= read_now;
351: 		to_read -= read_now;
352: 	}
353: 	group_rows_available -= num_values;
354: 	chunk_read_offset = trans.GetLocation();
355: 
356: 	return num_values;
357: }
358: 
359: void ColumnReader::Skip(idx_t num_values) {
360: 	dummy_define.zero();
361: 	dummy_repeat.zero();
362: 
363: 	// TODO this can be optimized, for example we dont actually have to bitunpack offsets
364: 	Vector dummy_result(type, nullptr);
365: 	auto values_read =
366: 	    Read(num_values, none_filter, (uint8_t *)dummy_define.ptr, (uint8_t *)dummy_repeat.ptr, dummy_result);
367: 	if (values_read != num_values) {
368: 		throw std::runtime_error("Row count mismatch when skipping rows");
369: 	}
370: }
371: 
372: uint32_t StringColumnReader::VerifyString(const char *str_data, uint32_t str_len) {
373: 	if (Type() != LogicalTypeId::VARCHAR) {
374: 		return str_len;
375: 	}
376: 	// verify if a string is actually UTF8, and if there are no null bytes in the middle of the string
377: 	// technically Parquet should guarantee this, but reality is often disappointing
378: 	UnicodeInvalidReason reason;
379: 	size_t pos;
380: 	auto utf_type = Utf8Proc::Analyze(str_data, str_len, &reason, &pos);
381: 	if (utf_type == UnicodeType::INVALID) {
382: 		if (reason == UnicodeInvalidReason::NULL_BYTE) {
383: 			// for null bytes we just truncate the string
384: 			return pos;
385: 		}
386: 		throw InvalidInputException("Invalid string encoding found in Parquet file: value \"" +
387: 		                            Blob::ToString(string_t(str_data, str_len)) + "\" is not valid UTF8!");
388: 	}
389: 	return str_len;
390: }
391: 
392: void StringColumnReader::Dictionary(shared_ptr<ByteBuffer> data, idx_t num_entries) {
393: 	dict = move(data);
394: 	dict_strings = unique_ptr<string_t[]>(new string_t[num_entries]);
395: 	for (idx_t dict_idx = 0; dict_idx < num_entries; dict_idx++) {
396: 		uint32_t str_len = dict->read<uint32_t>();
397: 		dict->available(str_len);
398: 
399: 		auto actual_str_len = VerifyString(dict->ptr, str_len);
400: 		dict_strings[dict_idx] = string_t(dict->ptr, actual_str_len);
401: 		dict->inc(str_len);
402: 	}
403: }
404: 
405: class ParquetStringVectorBuffer : public VectorBuffer {
406: public:
407: 	explicit ParquetStringVectorBuffer(shared_ptr<ByteBuffer> buffer_p)
408: 	    : VectorBuffer(VectorBufferType::OPAQUE_BUFFER), buffer(move(buffer_p)) {
409: 	}
410: 
411: private:
412: 	shared_ptr<ByteBuffer> buffer;
413: };
414: 
415: void StringColumnReader::DictReference(Vector &result) {
416: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(dict));
417: }
418: void StringColumnReader::PlainReference(shared_ptr<ByteBuffer> plain_data, Vector &result) {
419: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(move(plain_data)));
420: }
421: 
422: string_t StringParquetValueConversion::DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
423: 	auto &dict_strings = ((StringColumnReader &)reader).dict_strings;
424: 	return dict_strings[offset];
425: }
426: 
427: string_t StringParquetValueConversion::PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
428: 	auto &scr = ((StringColumnReader &)reader);
429: 	uint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;
430: 	plain_data.available(str_len);
431: 	auto actual_str_len = ((StringColumnReader &)reader).VerifyString(plain_data.ptr, str_len);
432: 	auto ret_str = string_t(plain_data.ptr, actual_str_len);
433: 	plain_data.inc(str_len);
434: 	return ret_str;
435: }
436: 
437: void StringParquetValueConversion::PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
438: 	auto &scr = ((StringColumnReader &)reader);
439: 	uint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;
440: 	plain_data.available(str_len);
441: 	plain_data.inc(str_len);
442: }
443: 
444: //===--------------------------------------------------------------------===//
445: // List Column Reader
446: //===--------------------------------------------------------------------===//
447: idx_t ListColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
448:                              Vector &result_out) {
449: 	idx_t result_offset = 0;
450: 	auto result_ptr = FlatVector::GetData<list_entry_t>(result_out);
451: 	auto &result_mask = FlatVector::Validity(result_out);
452: 
453: 	D_ASSERT(ListVector::GetListSize(result_out) == 0);
454: 	// if an individual list is longer than STANDARD_VECTOR_SIZE we actually have to loop the child read to fill it
455: 	bool finished = false;
456: 	while (!finished) {
457: 		idx_t child_actual_num_values = 0;
458: 
459: 		// check if we have any overflow from a previous read
460: 		if (overflow_child_count == 0) {
461: 			// we don't: read elements from the child reader
462: 			child_defines.zero();
463: 			child_repeats.zero();
464: 			// we don't know in advance how many values to read because of the beautiful repetition/definition setup
465: 			// we just read (up to) a vector from the child column, and see if we have read enough
466: 			// if we have not read enough, we read another vector
467: 			// if we have read enough, we leave any unhandled elements in the overflow vector for a subsequent read
468: 			auto child_req_num_values =
469: 			    MinValue<idx_t>(STANDARD_VECTOR_SIZE, child_column_reader->GroupRowsAvailable());
470: 			read_vector.ResetFromCache(read_cache);
471: 			child_actual_num_values = child_column_reader->Read(child_req_num_values, child_filter, child_defines_ptr,
472: 			                                                    child_repeats_ptr, read_vector);
473: 		} else {
474: 			// we do: use the overflow values
475: 			child_actual_num_values = overflow_child_count;
476: 			overflow_child_count = 0;
477: 		}
478: 
479: 		if (child_actual_num_values == 0) {
480: 			// no more elements available: we are done
481: 			break;
482: 		}
483: 		read_vector.Verify(child_actual_num_values);
484: 		idx_t current_chunk_offset = ListVector::GetListSize(result_out);
485: 
486: 		// hard-won piece of code this, modify at your own risk
487: 		// the intuition is that we have to only collapse values into lists that are repeated *on this level*
488: 		// the rest is pretty much handed up as-is as a single-valued list or NULL
489: 		idx_t child_idx;
490: 		for (child_idx = 0; child_idx < child_actual_num_values; child_idx++) {
491: 			if (child_repeats_ptr[child_idx] == max_repeat) {
492: 				// value repeats on this level, append
493: 				D_ASSERT(result_offset > 0);
494: 				result_ptr[result_offset - 1].length++;
495: 				continue;
496: 			}
497: 
498: 			if (result_offset >= num_values) {
499: 				// we ran out of output space
500: 				finished = true;
501: 				break;
502: 			}
503: 			if (child_defines_ptr[child_idx] >= max_define) {
504: 				// value has been defined down the stack, hence its NOT NULL
505: 				result_ptr[result_offset].offset = child_idx + current_chunk_offset;
506: 				result_ptr[result_offset].length = 1;
507: 			} else if (child_defines_ptr[child_idx] == max_define - 1) {
508: 				// empty list
509: 				result_ptr[result_offset].offset = child_idx + current_chunk_offset;
510: 				result_ptr[result_offset].length = 0;
511: 			} else {
512: 				// value is NULL somewhere up the stack
513: 				result_mask.SetInvalid(result_offset);
514: 				result_ptr[result_offset].offset = 0;
515: 				result_ptr[result_offset].length = 0;
516: 			}
517: 
518: 			repeat_out[result_offset] = child_repeats_ptr[child_idx];
519: 			define_out[result_offset] = child_defines_ptr[child_idx];
520: 
521: 			result_offset++;
522: 		}
523: 		// actually append the required elements to the child list
524: 		ListVector::Append(result_out, read_vector, child_idx);
525: 
526: 		// we have read more values from the child reader than we can fit into the result for this read
527: 		// we have to pass everything from child_idx to child_actual_num_values into the next call
528: 		if (child_idx < child_actual_num_values && result_offset == num_values) {
529: 			read_vector.Slice(read_vector, child_idx);
530: 			overflow_child_count = child_actual_num_values - child_idx;
531: 			read_vector.Verify(overflow_child_count);
532: 
533: 			// move values in the child repeats and defines *backward* by child_idx
534: 			for (idx_t repdef_idx = 0; repdef_idx < overflow_child_count; repdef_idx++) {
535: 				child_defines_ptr[repdef_idx] = child_defines_ptr[child_idx + repdef_idx];
536: 				child_repeats_ptr[repdef_idx] = child_repeats_ptr[child_idx + repdef_idx];
537: 			}
538: 		}
539: 	}
540: 	result_out.Verify(result_offset);
541: 	return result_offset;
542: }
543: 
544: ListColumnReader::ListColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
545:                                    idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p,
546:                                    unique_ptr<ColumnReader> child_column_reader_p)
547:     : ColumnReader(reader, move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p),
548:       child_column_reader(move(child_column_reader_p)), read_cache(ListType::GetChildType(Type())),
549:       read_vector(read_cache), overflow_child_count(0) {
550: 
551: 	child_defines.resize(reader.allocator, STANDARD_VECTOR_SIZE);
552: 	child_repeats.resize(reader.allocator, STANDARD_VECTOR_SIZE);
553: 	child_defines_ptr = (uint8_t *)child_defines.ptr;
554: 	child_repeats_ptr = (uint8_t *)child_repeats.ptr;
555: 
556: 	child_filter.set();
557: }
558: 
559: //===--------------------------------------------------------------------===//
560: // Struct Column Reader
561: //===--------------------------------------------------------------------===//
562: StructColumnReader::StructColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
563:                                        idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p,
564:                                        vector<unique_ptr<ColumnReader>> child_readers_p)
565:     : ColumnReader(reader, move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p),
566:       child_readers(move(child_readers_p)) {
567: 	D_ASSERT(type.InternalType() == PhysicalType::STRUCT);
568: }
569: 
570: ColumnReader *StructColumnReader::GetChildReader(idx_t child_idx) {
571: 	return child_readers[child_idx].get();
572: }
573: 
574: void StructColumnReader::InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) {
575: 	for (auto &child : child_readers) {
576: 		child->InitializeRead(columns, protocol_p);
577: 	}
578: }
579: 
580: idx_t StructColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
581:                                Vector &result) {
582: 	auto &struct_entries = StructVector::GetEntries(result);
583: 	D_ASSERT(StructType::GetChildTypes(Type()).size() == struct_entries.size());
584: 
585: 	idx_t read_count = num_values;
586: 	for (idx_t i = 0; i < struct_entries.size(); i++) {
587: 		auto child_num_values = child_readers[i]->Read(num_values, filter, define_out, repeat_out, *struct_entries[i]);
588: 		if (i == 0) {
589: 			read_count = child_num_values;
590: 		} else if (read_count != child_num_values) {
591: 			throw std::runtime_error("Struct child row count mismatch");
592: 		}
593: 	}
594: 	// set the validity mask for this level
595: 	auto &validity = FlatVector::Validity(result);
596: 	for (idx_t i = 0; i < read_count; i++) {
597: 		if (define_out[i] < max_define) {
598: 			validity.SetInvalid(i);
599: 		}
600: 	}
601: 
602: 	return read_count;
603: }
604: 
605: void StructColumnReader::Skip(idx_t num_values) {
606: 	throw InternalException("Skip not implemented for StructColumnReader");
607: }
608: 
609: idx_t StructColumnReader::GroupRowsAvailable() {
610: 	for (idx_t i = 0; i < child_readers.size(); i++) {
611: 		if (child_readers[i]->Type().id() != LogicalTypeId::LIST) {
612: 			return child_readers[i]->GroupRowsAvailable();
613: 		}
614: 	}
615: 	return child_readers[0]->GroupRowsAvailable();
616: }
617: 
618: } // namespace duckdb
[end of extension/parquet/column_reader.cpp]
[start of extension/parquet/column_writer.cpp]
1: #include "column_writer.hpp"
2: #include "parquet_writer.hpp"
3: #include "parquet_rle_bp_decoder.hpp"
4: 
5: #include "duckdb.hpp"
6: #ifndef DUCKDB_AMALGAMATION
7: #include "duckdb/common/common.hpp"
8: #include "duckdb/common/exception.hpp"
9: #include "duckdb/common/mutex.hpp"
10: #include "duckdb/common/serializer/buffered_file_writer.hpp"
11: #include "duckdb/common/types/chunk_collection.hpp"
12: #include "duckdb/common/types/date.hpp"
13: #include "duckdb/common/types/hugeint.hpp"
14: #include "duckdb/common/types/time.hpp"
15: #include "duckdb/common/types/timestamp.hpp"
16: #include "duckdb/common/serializer/buffered_serializer.hpp"
17: #endif
18: 
19: #include "snappy.h"
20: #include "miniz_wrapper.hpp"
21: #include "zstd.h"
22: 
23: namespace duckdb {
24: 
25: using namespace duckdb_parquet; // NOLINT
26: using namespace duckdb_miniz;   // NOLINT
27: 
28: using duckdb_parquet::format::CompressionCodec;
29: using duckdb_parquet::format::ConvertedType;
30: using duckdb_parquet::format::Encoding;
31: using duckdb_parquet::format::FieldRepetitionType;
32: using duckdb_parquet::format::FileMetaData;
33: using duckdb_parquet::format::PageHeader;
34: using duckdb_parquet::format::PageType;
35: using ParquetRowGroup = duckdb_parquet::format::RowGroup;
36: using duckdb_parquet::format::Type;
37: 
38: #define PARQUET_DEFINE_VALID 65535
39: 
40: //===--------------------------------------------------------------------===//
41: // ColumnWriter
42: //===--------------------------------------------------------------------===//
43: ColumnWriter::ColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define,
44:                            bool can_have_nulls)
45:     : writer(writer), schema_idx(schema_idx), max_repeat(max_repeat), max_define(max_define),
46:       can_have_nulls(can_have_nulls) {
47: }
48: ColumnWriter::~ColumnWriter() {
49: }
50: 
51: ColumnWriterState::~ColumnWriterState() {
52: }
53: 
54: static void VarintEncode(uint32_t val, Serializer &ser) {
55: 	do {
56: 		uint8_t byte = val & 127;
57: 		val >>= 7;
58: 		if (val != 0) {
59: 			byte |= 128;
60: 		}
61: 		ser.Write<uint8_t>(byte);
62: 	} while (val != 0);
63: }
64: 
65: static uint8_t GetVarintSize(uint32_t val) {
66: 	uint8_t res = 0;
67: 	do {
68: 		uint8_t byte = val & 127;
69: 		val >>= 7;
70: 		if (val != 0) {
71: 			byte |= 128;
72: 		}
73: 		res++;
74: 	} while (val != 0);
75: 	return res;
76: }
77: 
78: void ColumnWriter::CompressPage(BufferedSerializer &temp_writer, size_t &compressed_size, data_ptr_t &compressed_data,
79:                                 unique_ptr<data_t[]> &compressed_buf) {
80: 	switch (writer.codec) {
81: 	case CompressionCodec::UNCOMPRESSED:
82: 		compressed_size = temp_writer.blob.size;
83: 		compressed_data = temp_writer.blob.data.get();
84: 		break;
85: 	case CompressionCodec::SNAPPY: {
86: 		compressed_size = duckdb_snappy::MaxCompressedLength(temp_writer.blob.size);
87: 		compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
88: 		duckdb_snappy::RawCompress((const char *)temp_writer.blob.data.get(), temp_writer.blob.size,
89: 		                           (char *)compressed_buf.get(), &compressed_size);
90: 		compressed_data = compressed_buf.get();
91: 		D_ASSERT(compressed_size <= duckdb_snappy::MaxCompressedLength(temp_writer.blob.size));
92: 		break;
93: 	}
94: 	case CompressionCodec::GZIP: {
95: 		MiniZStream s;
96: 		compressed_size = s.MaxCompressedLength(temp_writer.blob.size);
97: 		compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
98: 		s.Compress((const char *)temp_writer.blob.data.get(), temp_writer.blob.size, (char *)compressed_buf.get(),
99: 		           &compressed_size);
100: 		compressed_data = compressed_buf.get();
101: 		break;
102: 	}
103: 	case CompressionCodec::ZSTD: {
104: 		compressed_size = duckdb_zstd::ZSTD_compressBound(temp_writer.blob.size);
105: 		compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
106: 		compressed_size = duckdb_zstd::ZSTD_compress((void *)compressed_buf.get(), compressed_size,
107: 		                                             (const void *)temp_writer.blob.data.get(), temp_writer.blob.size,
108: 		                                             ZSTD_CLEVEL_DEFAULT);
109: 		compressed_data = compressed_buf.get();
110: 		break;
111: 	}
112: 	default:
113: 		throw InternalException("Unsupported codec for Parquet Writer");
114: 	}
115: 
116: 	if (compressed_size > idx_t(NumericLimits<int32_t>::Maximum())) {
117: 		throw InternalException("Parquet writer: %d compressed page size out of range for type integer",
118: 		                        temp_writer.blob.size);
119: 	}
120: }
121: 
122: class ColumnWriterPageState {
123: public:
124: 	virtual ~ColumnWriterPageState() {
125: 	}
126: };
127: 
128: struct PageInformation {
129: 	idx_t offset = 0;
130: 	idx_t row_count = 0;
131: 	idx_t empty_count = 0;
132: 	idx_t estimated_page_size = 0;
133: };
134: 
135: struct PageWriteInformation {
136: 	PageHeader page_header;
137: 	unique_ptr<BufferedSerializer> temp_writer;
138: 	unique_ptr<ColumnWriterPageState> page_state;
139: 	idx_t write_page_idx = 0;
140: 	idx_t write_count = 0;
141: 	idx_t max_write_count = 0;
142: 	size_t compressed_size;
143: 	data_ptr_t compressed_data;
144: 	unique_ptr<data_t[]> compressed_buf;
145: };
146: 
147: class StandardColumnWriterState : public ColumnWriterState {
148: public:
149: 	StandardColumnWriterState(duckdb_parquet::format::RowGroup &row_group, idx_t col_idx)
150: 	    : row_group(row_group), col_idx(col_idx) {
151: 		page_info.emplace_back();
152: 	}
153: 	~StandardColumnWriterState() override = default;
154: 
155: 	duckdb_parquet::format::RowGroup &row_group;
156: 	idx_t col_idx;
157: 	vector<PageInformation> page_info;
158: 	vector<PageWriteInformation> write_info;
159: 	idx_t current_page = 0;
160: };
161: 
162: unique_ptr<ColumnWriterState> ColumnWriter::InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,
163:                                                                  vector<string> schema_path) {
164: 	auto result = make_unique<StandardColumnWriterState>(row_group, row_group.columns.size());
165: 
166: 	duckdb_parquet::format::ColumnChunk column_chunk;
167: 	column_chunk.__isset.meta_data = true;
168: 	column_chunk.meta_data.codec = writer.codec;
169: 	column_chunk.meta_data.path_in_schema = move(schema_path);
170: 	column_chunk.meta_data.path_in_schema.push_back(writer.file_meta_data.schema[schema_idx].name);
171: 	column_chunk.meta_data.num_values = 0;
172: 	column_chunk.meta_data.type = writer.file_meta_data.schema[schema_idx].type;
173: 	row_group.columns.push_back(move(column_chunk));
174: 
175: 	return move(result);
176: }
177: 
178: void ColumnWriter::HandleRepeatLevels(ColumnWriterState &state, ColumnWriterState *parent, idx_t count,
179:                                       idx_t max_repeat) {
180: 	if (!parent) {
181: 		// no repeat levels without a parent node
182: 		return;
183: 	}
184: 	while (state.repetition_levels.size() < parent->repetition_levels.size()) {
185: 		state.repetition_levels.push_back(parent->repetition_levels[state.repetition_levels.size()]);
186: 	}
187: }
188: 
189: void ColumnWriter::HandleDefineLevels(ColumnWriterState &state, ColumnWriterState *parent, ValidityMask &validity,
190:                                       idx_t count, uint16_t define_value, uint16_t null_value) {
191: 	if (parent) {
192: 		// parent node: inherit definition level from the parent
193: 		idx_t vector_index = 0;
194: 		while (state.definition_levels.size() < parent->definition_levels.size()) {
195: 			idx_t current_index = state.definition_levels.size();
196: 			if (parent->definition_levels[current_index] != PARQUET_DEFINE_VALID) {
197: 				state.definition_levels.push_back(parent->definition_levels[current_index]);
198: 			} else if (validity.RowIsValid(vector_index)) {
199: 				state.definition_levels.push_back(define_value);
200: 			} else {
201: 				if (!can_have_nulls) {
202: 					throw IOException("Parquet writer: map key column is not allowed to contain NULL values");
203: 				}
204: 				state.definition_levels.push_back(null_value);
205: 			}
206: 			if (parent->is_empty.empty() || !parent->is_empty[current_index]) {
207: 				vector_index++;
208: 			}
209: 		}
210: 	} else {
211: 		// no parent: set definition levels only from this validity mask
212: 		for (idx_t i = 0; i < count; i++) {
213: 			if (validity.RowIsValid(i)) {
214: 				state.definition_levels.push_back(define_value);
215: 			} else {
216: 				if (!can_have_nulls) {
217: 					throw IOException("Parquet writer: map key column is not allowed to contain NULL values");
218: 				}
219: 				state.definition_levels.push_back(null_value);
220: 			}
221: 		}
222: 	}
223: }
224: 
225: void ColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
226: 	auto &state = (StandardColumnWriterState &)state_p;
227: 	auto &col_chunk = state.row_group.columns[state.col_idx];
228: 
229: 	idx_t start = 0;
230: 	idx_t vcount = parent ? parent->definition_levels.size() - state.definition_levels.size() : count;
231: 	idx_t parent_index = state.definition_levels.size();
232: 	auto &validity = FlatVector::Validity(vector);
233: 	HandleRepeatLevels(state_p, parent, count, max_repeat);
234: 	HandleDefineLevels(state_p, parent, validity, count, max_define, max_define - 1);
235: 
236: 	idx_t vector_index = 0;
237: 	for (idx_t i = start; i < vcount; i++) {
238: 		auto &page_info = state.page_info.back();
239: 		page_info.row_count++;
240: 		col_chunk.meta_data.num_values++;
241: 		if (parent && !parent->is_empty.empty() && parent->is_empty[parent_index + i]) {
242: 			page_info.empty_count++;
243: 			continue;
244: 		}
245: 		if (validity.RowIsValid(vector_index)) {
246: 			page_info.estimated_page_size += GetRowSize(vector, vector_index);
247: 			if (page_info.estimated_page_size >= MAX_UNCOMPRESSED_PAGE_SIZE) {
248: 				PageInformation new_info;
249: 				new_info.offset = page_info.offset + page_info.row_count;
250: 				state.page_info.push_back(new_info);
251: 			}
252: 		}
253: 		vector_index++;
254: 	}
255: }
256: 
257: unique_ptr<ColumnWriterPageState> ColumnWriter::InitializePageState() {
258: 	return nullptr;
259: }
260: 
261: void ColumnWriter::FlushPageState(Serializer &temp_writer, ColumnWriterPageState *state) {
262: }
263: 
264: void ColumnWriter::BeginWrite(ColumnWriterState &state_p) {
265: 	auto &state = (StandardColumnWriterState &)state_p;
266: 
267: 	// set up the page write info
268: 	for (idx_t page_idx = 0; page_idx < state.page_info.size(); page_idx++) {
269: 		auto &page_info = state.page_info[page_idx];
270: 		if (page_info.row_count == 0) {
271: 			D_ASSERT(page_idx + 1 == state.page_info.size());
272: 			state.page_info.erase(state.page_info.begin() + page_idx);
273: 			break;
274: 		}
275: 		PageWriteInformation write_info;
276: 		// set up the header
277: 		auto &hdr = write_info.page_header;
278: 		hdr.compressed_page_size = 0;
279: 		hdr.uncompressed_page_size = 0;
280: 		hdr.type = PageType::DATA_PAGE;
281: 		hdr.__isset.data_page_header = true;
282: 
283: 		hdr.data_page_header.num_values = page_info.row_count;
284: 		hdr.data_page_header.encoding = Encoding::PLAIN;
285: 		hdr.data_page_header.definition_level_encoding = Encoding::RLE;
286: 		hdr.data_page_header.repetition_level_encoding = Encoding::RLE;
287: 
288: 		write_info.temp_writer = make_unique<BufferedSerializer>();
289: 		write_info.write_count = page_info.empty_count;
290: 		write_info.max_write_count = page_info.row_count;
291: 		write_info.page_state = InitializePageState();
292: 
293: 		write_info.compressed_size = 0;
294: 		write_info.compressed_data = nullptr;
295: 
296: 		state.write_info.push_back(move(write_info));
297: 	}
298: 
299: 	// start writing the first page
300: 	NextPage(state_p);
301: }
302: 
303: void ColumnWriter::WriteLevels(Serializer &temp_writer, const vector<uint16_t> &levels, idx_t max_value, idx_t offset,
304:                                idx_t count) {
305: 	if (levels.empty() || count == 0) {
306: 		return;
307: 	}
308: 
309: 	// write the levels
310: 	// we always RLE everything (for now)
311: 	auto bit_width = RleBpDecoder::ComputeBitWidth((max_value));
312: 	auto byte_width = (bit_width + 7) / 8;
313: 
314: 	// figure out how many bytes we are going to need
315: 	idx_t byte_count = 0;
316: 	idx_t run_count = 1;
317: 	idx_t current_run_count = 1;
318: 	for (idx_t i = offset + 1; i <= offset + count; i++) {
319: 		if (i == offset + count || levels[i] != levels[i - 1]) {
320: 			// last value, or value has changed
321: 			// write out the current run
322: 			byte_count += GetVarintSize(current_run_count << 1) + byte_width;
323: 			current_run_count = 1;
324: 			run_count++;
325: 		} else {
326: 			current_run_count++;
327: 		}
328: 	}
329: 	temp_writer.Write<uint32_t>(byte_count);
330: 
331: 	// now actually write the values
332: 	current_run_count = 1;
333: 	for (idx_t i = offset + 1; i <= offset + count; i++) {
334: 		if (i == offset + count || levels[i] != levels[i - 1]) {
335: 			// new run: write out the old run
336: 			// first write the header
337: 			VarintEncode(current_run_count << 1, temp_writer);
338: 			// now write hte value
339: 			switch (byte_width) {
340: 			case 1:
341: 				temp_writer.Write<uint8_t>(levels[i - 1]);
342: 				break;
343: 			case 2:
344: 				temp_writer.Write<uint16_t>(levels[i - 1]);
345: 				break;
346: 			default:
347: 				throw InternalException("unsupported byte width for RLE encoding");
348: 			}
349: 			current_run_count = 1;
350: 		} else {
351: 			current_run_count++;
352: 		}
353: 	}
354: }
355: 
356: void ColumnWriter::NextPage(ColumnWriterState &state_p) {
357: 	auto &state = (StandardColumnWriterState &)state_p;
358: 
359: 	if (state.current_page > 0) {
360: 		// need to flush the current page
361: 		FlushPage(state_p);
362: 	}
363: 	if (state.current_page >= state.write_info.size()) {
364: 		state.current_page = state.write_info.size() + 1;
365: 		return;
366: 	}
367: 	auto &page_info = state.page_info[state.current_page];
368: 	auto &write_info = state.write_info[state.current_page];
369: 	state.current_page++;
370: 
371: 	auto &temp_writer = *write_info.temp_writer;
372: 
373: 	// write the repetition levels
374: 	WriteLevels(temp_writer, state.repetition_levels, max_repeat, page_info.offset, page_info.row_count);
375: 
376: 	// write the definition levels
377: 	WriteLevels(temp_writer, state.definition_levels, max_define, page_info.offset, page_info.row_count);
378: }
379: 
380: void ColumnWriter::FlushPage(ColumnWriterState &state_p) {
381: 	auto &state = (StandardColumnWriterState &)state_p;
382: 	D_ASSERT(state.current_page > 0);
383: 	if (state.current_page > state.write_info.size()) {
384: 		return;
385: 	}
386: 
387: 	// compress the page info
388: 	auto &write_info = state.write_info[state.current_page - 1];
389: 	auto &temp_writer = *write_info.temp_writer;
390: 	auto &hdr = write_info.page_header;
391: 
392: 	FlushPageState(temp_writer, write_info.page_state.get());
393: 
394: 	// now that we have finished writing the data we know the uncompressed size
395: 	if (temp_writer.blob.size > idx_t(NumericLimits<int32_t>::Maximum())) {
396: 		throw InternalException("Parquet writer: %d uncompressed page size out of range for type integer",
397: 		                        temp_writer.blob.size);
398: 	}
399: 	hdr.uncompressed_page_size = temp_writer.blob.size;
400: 
401: 	// compress the data
402: 	CompressPage(temp_writer, write_info.compressed_size, write_info.compressed_data, write_info.compressed_buf);
403: 	hdr.compressed_page_size = write_info.compressed_size;
404: 	D_ASSERT(hdr.uncompressed_page_size > 0);
405: 	D_ASSERT(hdr.compressed_page_size > 0);
406: 
407: 	if (write_info.compressed_buf) {
408: 		// if the data has been compressed, we no longer need the compressed data
409: 		D_ASSERT(write_info.compressed_buf.get() == write_info.compressed_data);
410: 		write_info.temp_writer.reset();
411: 	}
412: }
413: 
414: void ColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count) {
415: 	auto &state = (StandardColumnWriterState &)state_p;
416: 
417: 	idx_t remaining = count;
418: 	idx_t offset = 0;
419: 	while (remaining > 0) {
420: 		auto &write_info = state.write_info[state.current_page - 1];
421: 		if (!write_info.temp_writer) {
422: 			throw InternalException("Writes are not correctly aligned!?");
423: 		}
424: 		auto &temp_writer = *write_info.temp_writer;
425: 		idx_t write_count = MinValue<idx_t>(remaining, write_info.max_write_count - write_info.write_count);
426: 		D_ASSERT(write_count > 0);
427: 
428: 		WriteVector(temp_writer, write_info.page_state.get(), vector, offset, offset + write_count);
429: 
430: 		write_info.write_count += write_count;
431: 		if (write_info.write_count == write_info.max_write_count) {
432: 			NextPage(state_p);
433: 		}
434: 		offset += write_count;
435: 		remaining -= write_count;
436: 	}
437: }
438: 
439: void ColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {
440: 	auto &state = (StandardColumnWriterState &)state_p;
441: 	auto &column_chunk = state.row_group.columns[state.col_idx];
442: 
443: 	// flush the last page (if any remains)
444: 	FlushPage(state);
445: 	// record the start position of the pages for this column
446: 	column_chunk.meta_data.data_page_offset = writer.writer->GetTotalWritten();
447: 	// write the individual pages to disk
448: 	for (auto &write_info : state.write_info) {
449: 		D_ASSERT(write_info.page_header.uncompressed_page_size > 0);
450: 		write_info.page_header.write(writer.protocol.get());
451: 		writer.writer->WriteData(write_info.compressed_data, write_info.compressed_size);
452: 	}
453: 	column_chunk.meta_data.total_compressed_size =
454: 	    writer.writer->GetTotalWritten() - column_chunk.meta_data.data_page_offset;
455: }
456: 
457: //===--------------------------------------------------------------------===//
458: // Standard Column Writer
459: //===--------------------------------------------------------------------===//
460: struct ParquetCastOperator {
461: 	template <class SRC, class TGT>
462: 	static TGT Operation(SRC input) {
463: 		return TGT(input);
464: 	}
465: };
466: 
467: struct ParquetTimestampNSOperator {
468: 	template <class SRC, class TGT>
469: 	static TGT Operation(SRC input) {
470: 		return Timestamp::FromEpochNanoSeconds(input).value;
471: 	}
472: };
473: 
474: struct ParquetTimestampSOperator {
475: 	template <class SRC, class TGT>
476: 	static TGT Operation(SRC input) {
477: 		return Timestamp::FromEpochSeconds(input).value;
478: 	}
479: };
480: 
481: struct ParquetHugeintOperator {
482: 	template <class SRC, class TGT>
483: 	static TGT Operation(SRC input) {
484: 		return Hugeint::Cast<double>(input);
485: 	}
486: };
487: 
488: template <class SRC, class TGT, class OP = ParquetCastOperator>
489: static void TemplatedWritePlain(Vector &col, idx_t chunk_start, idx_t chunk_end, ValidityMask &mask, Serializer &ser) {
490: 	auto *ptr = FlatVector::GetData<SRC>(col);
491: 	for (idx_t r = chunk_start; r < chunk_end; r++) {
492: 		if (mask.RowIsValid(r)) {
493: 			ser.Write<TGT>(OP::template Operation<SRC, TGT>(ptr[r]));
494: 		}
495: 	}
496: }
497: 
498: template <class SRC, class TGT, class OP = ParquetCastOperator>
499: class StandardColumnWriter : public ColumnWriter {
500: public:
501: 	StandardColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define,
502: 	                     bool can_have_nulls)
503: 	    : ColumnWriter(writer, schema_idx, max_repeat, max_define, can_have_nulls) {
504: 	}
505: 	~StandardColumnWriter() override = default;
506: 
507: public:
508: 	void WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,
509: 	                 idx_t chunk_start, idx_t chunk_end) override {
510: 		auto &mask = FlatVector::Validity(input_column);
511: 		TemplatedWritePlain<SRC, TGT, OP>(input_column, chunk_start, chunk_end, mask, temp_writer);
512: 	}
513: 
514: 	idx_t GetRowSize(Vector &vector, idx_t index) override {
515: 		return sizeof(TGT);
516: 	}
517: };
518: 
519: //===--------------------------------------------------------------------===//
520: // Boolean Column Writer
521: //===--------------------------------------------------------------------===//
522: class BooleanWriterPageState : public ColumnWriterPageState {
523: public:
524: 	uint8_t byte = 0;
525: 	uint8_t byte_pos = 0;
526: };
527: 
528: class BooleanColumnWriter : public ColumnWriter {
529: public:
530: 	BooleanColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define,
531: 	                    bool can_have_nulls)
532: 	    : ColumnWriter(writer, schema_idx, max_repeat, max_define, can_have_nulls) {
533: 	}
534: 	~BooleanColumnWriter() override = default;
535: 
536: public:
537: 	void WriteVector(Serializer &temp_writer, ColumnWriterPageState *state_p, Vector &input_column, idx_t chunk_start,
538: 	                 idx_t chunk_end) override {
539: 		auto &state = (BooleanWriterPageState &)*state_p;
540: 		auto &mask = FlatVector::Validity(input_column);
541: 
542: 		auto *ptr = FlatVector::GetData<bool>(input_column);
543: 		for (idx_t r = chunk_start; r < chunk_end; r++) {
544: 			if (mask.RowIsValid(r)) {
545: 				// only encode if non-null
546: 				if (ptr[r]) {
547: 					state.byte |= 1 << state.byte_pos;
548: 				}
549: 				state.byte_pos++;
550: 
551: 				if (state.byte_pos == 8) {
552: 					temp_writer.Write<uint8_t>(state.byte);
553: 					state.byte = 0;
554: 					state.byte_pos = 0;
555: 				}
556: 			}
557: 		}
558: 	}
559: 
560: 	unique_ptr<ColumnWriterPageState> InitializePageState() override {
561: 		return make_unique<BooleanWriterPageState>();
562: 	}
563: 
564: 	void FlushPageState(Serializer &temp_writer, ColumnWriterPageState *state_p) override {
565: 		auto &state = (BooleanWriterPageState &)*state_p;
566: 		if (state.byte_pos > 0) {
567: 			temp_writer.Write<uint8_t>(state.byte);
568: 			state.byte = 0;
569: 			state.byte_pos = 0;
570: 		}
571: 	}
572: 
573: 	idx_t GetRowSize(Vector &vector, idx_t index) override {
574: 		return sizeof(bool);
575: 	}
576: };
577: 
578: //===--------------------------------------------------------------------===//
579: // Decimal Column Writer
580: //===--------------------------------------------------------------------===//
581: class DecimalColumnWriter : public ColumnWriter {
582: public:
583: 	DecimalColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define,
584: 	                    bool can_have_nulls)
585: 	    : ColumnWriter(writer, schema_idx, max_repeat, max_define, can_have_nulls) {
586: 	}
587: 	~DecimalColumnWriter() override = default;
588: 
589: public:
590: 	void WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,
591: 	                 idx_t chunk_start, idx_t chunk_end) override {
592: 		auto &mask = FlatVector::Validity(input_column);
593: 
594: 		// FIXME: fixed length byte array...
595: 		Vector double_vec(LogicalType::DOUBLE, true, false, chunk_end);
596: 		VectorOperations::Cast(input_column, double_vec, chunk_end);
597: 		TemplatedWritePlain<double, double>(double_vec, chunk_start, chunk_end, mask, temp_writer);
598: 	}
599: 
600: 	idx_t GetRowSize(Vector &vector, idx_t index) override {
601: 		return sizeof(double);
602: 	}
603: };
604: 
605: //===--------------------------------------------------------------------===//
606: // String Column Writer
607: //===--------------------------------------------------------------------===//
608: class StringColumnWriter : public ColumnWriter {
609: public:
610: 	StringColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define, bool can_have_nulls)
611: 	    : ColumnWriter(writer, schema_idx, max_repeat, max_define, can_have_nulls) {
612: 	}
613: 	~StringColumnWriter() override = default;
614: 
615: public:
616: 	void WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,
617: 	                 idx_t chunk_start, idx_t chunk_end) override {
618: 		auto &mask = FlatVector::Validity(input_column);
619: 
620: 		auto *ptr = FlatVector::GetData<string_t>(input_column);
621: 		for (idx_t r = chunk_start; r < chunk_end; r++) {
622: 			if (mask.RowIsValid(r)) {
623: 				temp_writer.Write<uint32_t>(ptr[r].GetSize());
624: 				temp_writer.WriteData((const_data_ptr_t)ptr[r].GetDataUnsafe(), ptr[r].GetSize());
625: 			}
626: 		}
627: 	}
628: 
629: 	idx_t GetRowSize(Vector &vector, idx_t index) override {
630: 		auto strings = FlatVector::GetData<string_t>(vector);
631: 		return strings[index].GetSize();
632: 	}
633: };
634: 
635: //===--------------------------------------------------------------------===//
636: // Struct Column Writer
637: //===--------------------------------------------------------------------===//
638: class StructColumnWriter : public ColumnWriter {
639: public:
640: 	StructColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define,
641: 	                   vector<unique_ptr<ColumnWriter>> child_writers_p, bool can_have_nulls)
642: 	    : ColumnWriter(writer, schema_idx, max_repeat, max_define, can_have_nulls),
643: 	      child_writers(move(child_writers_p)) {
644: 	}
645: 	~StructColumnWriter() override = default;
646: 
647: 	vector<unique_ptr<ColumnWriter>> child_writers;
648: 
649: public:
650: 	void WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,
651: 	                 idx_t chunk_start, idx_t chunk_end) override {
652: 		throw InternalException("Cannot write vector of type struct");
653: 	}
654: 
655: 	idx_t GetRowSize(Vector &vector, idx_t index) override {
656: 		throw InternalException("Cannot get row size of struct");
657: 	}
658: 
659: 	unique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,
660: 	                                                   vector<string> schema_path) override;
661: 	void Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;
662: 
663: 	void BeginWrite(ColumnWriterState &state) override;
664: 	void Write(ColumnWriterState &state, Vector &vector, idx_t count) override;
665: 	void FinalizeWrite(ColumnWriterState &state) override;
666: };
667: 
668: class StructColumnWriterState : public ColumnWriterState {
669: public:
670: 	StructColumnWriterState(duckdb_parquet::format::RowGroup &row_group, idx_t col_idx)
671: 	    : row_group(row_group), col_idx(col_idx) {
672: 	}
673: 	~StructColumnWriterState() override = default;
674: 
675: 	duckdb_parquet::format::RowGroup &row_group;
676: 	idx_t col_idx;
677: 	vector<unique_ptr<ColumnWriterState>> child_states;
678: };
679: 
680: unique_ptr<ColumnWriterState> StructColumnWriter::InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,
681:                                                                        vector<string> schema_path) {
682: 	auto result = make_unique<StructColumnWriterState>(row_group, row_group.columns.size());
683: 	schema_path.push_back(writer.file_meta_data.schema[schema_idx].name);
684: 
685: 	result->child_states.reserve(child_writers.size());
686: 	for (auto &child_writer : child_writers) {
687: 		result->child_states.push_back(child_writer->InitializeWriteState(row_group, schema_path));
688: 	}
689: 	return move(result);
690: }
691: 
692: void StructColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
693: 	auto &state = (StructColumnWriterState &)state_p;
694: 
695: 	auto &validity = FlatVector::Validity(vector);
696: 	if (parent) {
697: 		// propagate empty entries from the parent
698: 		while (state.is_empty.size() < parent->is_empty.size()) {
699: 			state.is_empty.push_back(parent->is_empty[state.is_empty.size()]);
700: 		}
701: 	}
702: 	HandleRepeatLevels(state_p, parent, count, max_repeat);
703: 	HandleDefineLevels(state_p, parent, validity, count, PARQUET_DEFINE_VALID, max_define - 1);
704: 	auto &child_vectors = StructVector::GetEntries(vector);
705: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
706: 		child_writers[child_idx]->Prepare(*state.child_states[child_idx], &state_p, *child_vectors[child_idx], count);
707: 	}
708: }
709: 
710: void StructColumnWriter::BeginWrite(ColumnWriterState &state_p) {
711: 	auto &state = (StructColumnWriterState &)state_p;
712: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
713: 		child_writers[child_idx]->BeginWrite(*state.child_states[child_idx]);
714: 	}
715: }
716: 
717: void StructColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count) {
718: 	auto &state = (StructColumnWriterState &)state_p;
719: 	auto &child_vectors = StructVector::GetEntries(vector);
720: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
721: 		child_writers[child_idx]->Write(*state.child_states[child_idx], *child_vectors[child_idx], count);
722: 	}
723: }
724: 
725: void StructColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {
726: 	auto &state = (StructColumnWriterState &)state_p;
727: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
728: 		child_writers[child_idx]->FinalizeWrite(*state.child_states[child_idx]);
729: 	}
730: }
731: 
732: //===--------------------------------------------------------------------===//
733: // List Column Writer
734: //===--------------------------------------------------------------------===//
735: class ListColumnWriter : public ColumnWriter {
736: public:
737: 	ListColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define,
738: 	                 unique_ptr<ColumnWriter> child_writer_p, bool can_have_nulls)
739: 	    : ColumnWriter(writer, schema_idx, max_repeat, max_define, can_have_nulls), child_writer(move(child_writer_p)) {
740: 	}
741: 	~ListColumnWriter() override = default;
742: 
743: 	unique_ptr<ColumnWriter> child_writer;
744: 
745: public:
746: 	void WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,
747: 	                 idx_t chunk_start, idx_t chunk_end) override {
748: 		throw InternalException("Cannot write vector of type list");
749: 	}
750: 
751: 	idx_t GetRowSize(Vector &vector, idx_t index) override {
752: 		throw InternalException("Cannot get row size of list");
753: 	}
754: 
755: 	unique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,
756: 	                                                   vector<string> schema_path) override;
757: 	void Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;
758: 
759: 	void BeginWrite(ColumnWriterState &state) override;
760: 	void Write(ColumnWriterState &state, Vector &vector, idx_t count) override;
761: 	void FinalizeWrite(ColumnWriterState &state) override;
762: };
763: 
764: class ListColumnWriterState : public ColumnWriterState {
765: public:
766: 	ListColumnWriterState(duckdb_parquet::format::RowGroup &row_group, idx_t col_idx)
767: 	    : row_group(row_group), col_idx(col_idx) {
768: 	}
769: 	~ListColumnWriterState() override = default;
770: 
771: 	duckdb_parquet::format::RowGroup &row_group;
772: 	idx_t col_idx;
773: 	unique_ptr<ColumnWriterState> child_state;
774: 	idx_t parent_index = 0;
775: };
776: 
777: unique_ptr<ColumnWriterState> ListColumnWriter::InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,
778:                                                                      vector<string> schema_path) {
779: 	auto result = make_unique<ListColumnWriterState>(row_group, row_group.columns.size());
780: 	schema_path.push_back(writer.file_meta_data.schema[schema_idx].name);
781: 	result->child_state = child_writer->InitializeWriteState(row_group, move(schema_path));
782: 	return move(result);
783: }
784: 
785: void ListColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
786: 	auto &state = (ListColumnWriterState &)state_p;
787: 
788: 	auto list_data = FlatVector::GetData<list_entry_t>(vector);
789: 	auto &validity = FlatVector::Validity(vector);
790: 
791: 	// write definition levels and repeats
792: 	idx_t start = 0;
793: 	idx_t vcount = parent ? parent->definition_levels.size() - state.parent_index : count;
794: 	idx_t vector_index = 0;
795: 	for (idx_t i = start; i < vcount; i++) {
796: 		idx_t parent_index = state.parent_index + i;
797: 		if (parent && !parent->is_empty.empty() && parent->is_empty[parent_index]) {
798: 			state.definition_levels.push_back(parent->definition_levels[parent_index]);
799: 			state.repetition_levels.push_back(parent->repetition_levels[parent_index]);
800: 			state.is_empty.push_back(true);
801: 			continue;
802: 		}
803: 		auto first_repeat_level =
804: 		    parent && !parent->repetition_levels.empty() ? parent->repetition_levels[parent_index] : max_repeat;
805: 		if (parent && parent->definition_levels[parent_index] != PARQUET_DEFINE_VALID) {
806: 			state.definition_levels.push_back(parent->definition_levels[parent_index]);
807: 			state.repetition_levels.push_back(first_repeat_level);
808: 			state.is_empty.push_back(true);
809: 		} else if (validity.RowIsValid(vector_index)) {
810: 			// push the repetition levels
811: 			if (list_data[vector_index].length == 0) {
812: 				state.definition_levels.push_back(max_define);
813: 				state.is_empty.push_back(true);
814: 			} else {
815: 				state.definition_levels.push_back(PARQUET_DEFINE_VALID);
816: 				state.is_empty.push_back(false);
817: 			}
818: 			state.repetition_levels.push_back(first_repeat_level);
819: 			for (idx_t k = 1; k < list_data[vector_index].length; k++) {
820: 				state.repetition_levels.push_back(max_repeat + 1);
821: 				state.definition_levels.push_back(PARQUET_DEFINE_VALID);
822: 				state.is_empty.push_back(false);
823: 			}
824: 		} else {
825: 			if (!can_have_nulls) {
826: 				throw IOException("Parquet writer: map key column is not allowed to contain NULL values");
827: 			}
828: 			state.definition_levels.push_back(max_define - 1);
829: 			state.repetition_levels.push_back(first_repeat_level);
830: 			state.is_empty.push_back(true);
831: 		}
832: 		vector_index++;
833: 	}
834: 	state.parent_index += vcount;
835: 
836: 	auto &list_child = ListVector::GetEntry(vector);
837: 	auto list_count = ListVector::GetListSize(vector);
838: 	child_writer->Prepare(*state.child_state, &state_p, list_child, list_count);
839: }
840: 
841: void ListColumnWriter::BeginWrite(ColumnWriterState &state_p) {
842: 	auto &state = (ListColumnWriterState &)state_p;
843: 	child_writer->BeginWrite(*state.child_state);
844: }
845: 
846: void ListColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count) {
847: 	auto &state = (ListColumnWriterState &)state_p;
848: 
849: 	auto &list_child = ListVector::GetEntry(vector);
850: 	auto list_count = ListVector::GetListSize(vector);
851: 	child_writer->Write(*state.child_state, list_child, list_count);
852: }
853: 
854: void ListColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {
855: 	auto &state = (ListColumnWriterState &)state_p;
856: 	child_writer->FinalizeWrite(*state.child_state);
857: }
858: 
859: //===--------------------------------------------------------------------===//
860: // Create Column Writer
861: //===--------------------------------------------------------------------===//
862: unique_ptr<ColumnWriter> ColumnWriter::CreateWriterRecursive(vector<duckdb_parquet::format::SchemaElement> &schemas,
863:                                                              ParquetWriter &writer, const LogicalType &type,
864:                                                              const string &name, idx_t max_repeat, idx_t max_define,
865:                                                              bool can_have_nulls) {
866: 	auto null_type = can_have_nulls ? FieldRepetitionType::OPTIONAL : FieldRepetitionType::REQUIRED;
867: 	if (!can_have_nulls) {
868: 		max_define--;
869: 	}
870: 	idx_t schema_idx = schemas.size();
871: 	if (type.id() == LogicalTypeId::STRUCT) {
872: 		auto &child_types = StructType::GetChildTypes(type);
873: 		// set up the schema element for this struct
874: 		duckdb_parquet::format::SchemaElement schema_element;
875: 		schema_element.repetition_type = null_type;
876: 		schema_element.num_children = child_types.size();
877: 		schema_element.__isset.num_children = true;
878: 		schema_element.__isset.type = false;
879: 		schema_element.__isset.repetition_type = true;
880: 		schema_element.name = name;
881: 		schemas.push_back(move(schema_element));
882: 		// construct the child types recursively
883: 		vector<unique_ptr<ColumnWriter>> child_writers;
884: 		child_writers.reserve(child_types.size());
885: 		for (auto &child_type : child_types) {
886: 			child_writers.push_back(CreateWriterRecursive(schemas, writer, child_type.second, child_type.first,
887: 			                                              max_repeat, max_define + 1));
888: 		}
889: 		return make_unique<StructColumnWriter>(writer, schema_idx, max_repeat, max_define, move(child_writers),
890: 		                                       can_have_nulls);
891: 	}
892: 	if (type.id() == LogicalTypeId::LIST) {
893: 		auto &child_type = ListType::GetChildType(type);
894: 		// set up the two schema elements for the list
895: 		// for some reason we only set the converted type in the OPTIONAL element
896: 		// first an OPTIONAL element
897: 		duckdb_parquet::format::SchemaElement optional_element;
898: 		optional_element.repetition_type = null_type;
899: 		optional_element.num_children = 1;
900: 		optional_element.converted_type = ConvertedType::LIST;
901: 		optional_element.__isset.num_children = true;
902: 		optional_element.__isset.type = false;
903: 		optional_element.__isset.repetition_type = true;
904: 		optional_element.__isset.converted_type = true;
905: 		optional_element.name = name;
906: 		schemas.push_back(move(optional_element));
907: 
908: 		// then a REPEATED element
909: 		duckdb_parquet::format::SchemaElement repeated_element;
910: 		repeated_element.repetition_type = FieldRepetitionType::REPEATED;
911: 		repeated_element.num_children = 1;
912: 		repeated_element.__isset.num_children = true;
913: 		repeated_element.__isset.type = false;
914: 		repeated_element.__isset.repetition_type = true;
915: 		repeated_element.name = "list";
916: 		schemas.push_back(move(repeated_element));
917: 
918: 		auto child_writer =
919: 		    CreateWriterRecursive(schemas, writer, child_type, "element", max_repeat + 1, max_define + 2);
920: 		return make_unique<ListColumnWriter>(writer, schema_idx, max_repeat, max_define, move(child_writer),
921: 		                                     can_have_nulls);
922: 	}
923: 	if (type.id() == LogicalTypeId::MAP) {
924: 		// map type
925: 		// maps are stored as follows:
926: 		// <map-repetition> group <name> (MAP) {
927: 		// 	repeated group key_value {
928: 		// 		required <key-type> key;
929: 		// 		<value-repetition> <value-type> value;
930: 		// 	}
931: 		// }
932: 		// top map element
933: 		duckdb_parquet::format::SchemaElement top_element;
934: 		top_element.repetition_type = null_type;
935: 		top_element.num_children = 1;
936: 		top_element.converted_type = ConvertedType::MAP;
937: 		top_element.__isset.repetition_type = true;
938: 		top_element.__isset.num_children = true;
939: 		top_element.__isset.converted_type = true;
940: 		top_element.__isset.type = false;
941: 		top_element.name = name;
942: 		schemas.push_back(move(top_element));
943: 
944: 		// key_value element
945: 		duckdb_parquet::format::SchemaElement kv_element;
946: 		kv_element.repetition_type = FieldRepetitionType::REPEATED;
947: 		kv_element.num_children = 2;
948: 		kv_element.__isset.repetition_type = true;
949: 		kv_element.__isset.num_children = true;
950: 		kv_element.__isset.type = false;
951: 		kv_element.name = "key_value";
952: 		schemas.push_back(move(kv_element));
953: 
954: 		// construct the child types recursively
955: 		vector<LogicalType> kv_types {ListType::GetChildType(MapType::KeyType(type)),
956: 		                              ListType::GetChildType(MapType::ValueType(type))};
957: 		vector<string> kv_names {"key", "value"};
958: 		vector<unique_ptr<ColumnWriter>> child_writers;
959: 		child_writers.reserve(2);
960: 		for (idx_t i = 0; i < 2; i++) {
961: 			// key needs to be marked as REQUIRED
962: 			bool is_key = i == 0;
963: 			auto child_writer = CreateWriterRecursive(schemas, writer, kv_types[i], kv_names[i], max_repeat + 1,
964: 			                                          max_define + 2, !is_key);
965: 			auto list_writer = make_unique<ListColumnWriter>(writer, schema_idx, max_repeat, max_define,
966: 			                                                 move(child_writer), can_have_nulls);
967: 			child_writers.push_back(move(list_writer));
968: 		}
969: 		return make_unique<StructColumnWriter>(writer, schema_idx, max_repeat, max_define, move(child_writers),
970: 		                                       can_have_nulls);
971: 	}
972: 	duckdb_parquet::format::SchemaElement schema_element;
973: 	schema_element.type = ParquetWriter::DuckDBTypeToParquetType(type);
974: 	schema_element.repetition_type = null_type;
975: 	schema_element.num_children = 0;
976: 	schema_element.__isset.num_children = true;
977: 	schema_element.__isset.type = true;
978: 	schema_element.__isset.repetition_type = true;
979: 	schema_element.name = name;
980: 	schema_element.__isset.converted_type =
981: 	    ParquetWriter::DuckDBTypeToConvertedType(type, schema_element.converted_type);
982: 	schemas.push_back(move(schema_element));
983: 
984: 	switch (type.id()) {
985: 	case LogicalTypeId::BOOLEAN:
986: 		return make_unique<BooleanColumnWriter>(writer, schema_idx, max_repeat, max_define, can_have_nulls);
987: 	case LogicalTypeId::TINYINT:
988: 		return make_unique<StandardColumnWriter<int8_t, int32_t>>(writer, schema_idx, max_repeat, max_define,
989: 		                                                          can_have_nulls);
990: 	case LogicalTypeId::SMALLINT:
991: 		return make_unique<StandardColumnWriter<int16_t, int32_t>>(writer, schema_idx, max_repeat, max_define,
992: 		                                                           can_have_nulls);
993: 	case LogicalTypeId::INTEGER:
994: 	case LogicalTypeId::DATE:
995: 		return make_unique<StandardColumnWriter<int32_t, int32_t>>(writer, schema_idx, max_repeat, max_define,
996: 		                                                           can_have_nulls);
997: 	case LogicalTypeId::BIGINT:
998: 	case LogicalTypeId::TIMESTAMP:
999: 	case LogicalTypeId::TIMESTAMP_MS:
1000: 		return make_unique<StandardColumnWriter<int64_t, int64_t>>(writer, schema_idx, max_repeat, max_define,
1001: 		                                                           can_have_nulls);
1002: 	case LogicalTypeId::HUGEINT:
1003: 		return make_unique<StandardColumnWriter<hugeint_t, double, ParquetHugeintOperator>>(
1004: 		    writer, schema_idx, max_repeat, max_define, can_have_nulls);
1005: 	case LogicalTypeId::TIMESTAMP_NS:
1006: 		return make_unique<StandardColumnWriter<int64_t, int64_t, ParquetTimestampNSOperator>>(
1007: 		    writer, schema_idx, max_repeat, max_define, can_have_nulls);
1008: 	case LogicalTypeId::TIMESTAMP_SEC:
1009: 		return make_unique<StandardColumnWriter<int64_t, int64_t, ParquetTimestampSOperator>>(
1010: 		    writer, schema_idx, max_repeat, max_define, can_have_nulls);
1011: 	case LogicalTypeId::UTINYINT:
1012: 		return make_unique<StandardColumnWriter<uint8_t, int32_t>>(writer, schema_idx, max_repeat, max_define,
1013: 		                                                           can_have_nulls);
1014: 	case LogicalTypeId::USMALLINT:
1015: 		return make_unique<StandardColumnWriter<uint16_t, int32_t>>(writer, schema_idx, max_repeat, max_define,
1016: 		                                                            can_have_nulls);
1017: 	case LogicalTypeId::UINTEGER:
1018: 		return make_unique<StandardColumnWriter<uint32_t, uint32_t>>(writer, schema_idx, max_repeat, max_define,
1019: 		                                                             can_have_nulls);
1020: 	case LogicalTypeId::UBIGINT:
1021: 		return make_unique<StandardColumnWriter<uint64_t, uint64_t>>(writer, schema_idx, max_repeat, max_define,
1022: 		                                                             can_have_nulls);
1023: 	case LogicalTypeId::FLOAT:
1024: 		return make_unique<StandardColumnWriter<float, float>>(writer, schema_idx, max_repeat, max_define,
1025: 		                                                       can_have_nulls);
1026: 	case LogicalTypeId::DOUBLE:
1027: 		return make_unique<StandardColumnWriter<double, double>>(writer, schema_idx, max_repeat, max_define,
1028: 		                                                         can_have_nulls);
1029: 	case LogicalTypeId::DECIMAL:
1030: 		return make_unique<DecimalColumnWriter>(writer, schema_idx, max_repeat, max_define, can_have_nulls);
1031: 	case LogicalTypeId::BLOB:
1032: 	case LogicalTypeId::VARCHAR:
1033: 		return make_unique<StringColumnWriter>(writer, schema_idx, max_repeat, max_define, can_have_nulls);
1034: 	default:
1035: 		throw InternalException("Unsupported type in Parquet writer");
1036: 	}
1037: }
1038: 
1039: } // namespace duckdb
[end of extension/parquet/column_writer.cpp]
[start of extension/parquet/include/column_reader.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // column_reader.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "parquet_types.h"
12: #include "thrift_tools.hpp"
13: #include "resizable_buffer.hpp"
14: 
15: #include "parquet_rle_bp_decoder.hpp"
16: #include "parquet_statistics.hpp"
17: 
18: #include "duckdb.hpp"
19: #ifndef DUCKDB_AMALGAMATION
20: #include "duckdb/storage/statistics/string_statistics.hpp"
21: #include "duckdb/storage/statistics/numeric_statistics.hpp"
22: #include "duckdb/common/types/vector.hpp"
23: #include "duckdb/common/types/string_type.hpp"
24: #include "duckdb/common/types/chunk_collection.hpp"
25: #include "duckdb/common/operator/cast_operators.hpp"
26: #include "duckdb/common/types/vector_cache.hpp"
27: #endif
28: 
29: namespace duckdb {
30: class ParquetReader;
31: 
32: using duckdb_apache::thrift::protocol::TProtocol;
33: 
34: using duckdb_parquet::format::ColumnChunk;
35: using duckdb_parquet::format::FieldRepetitionType;
36: using duckdb_parquet::format::PageHeader;
37: using duckdb_parquet::format::SchemaElement;
38: using duckdb_parquet::format::Type;
39: 
40: typedef std::bitset<STANDARD_VECTOR_SIZE> parquet_filter_t;
41: 
42: class ColumnReader {
43: public:
44: 	static unique_ptr<ColumnReader> CreateReader(ParquetReader &reader, const LogicalType &type_p,
45: 	                                             const SchemaElement &schema_p, idx_t schema_idx_p, idx_t max_define,
46: 	                                             idx_t max_repeat);
47: 
48: 	ColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
49: 	             idx_t max_define_p, idx_t max_repeat_p);
50: 
51: 	virtual void InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) {
52: 		D_ASSERT(file_idx < columns.size());
53: 		chunk = &columns[file_idx];
54: 		protocol = &protocol_p;
55: 		D_ASSERT(chunk);
56: 		D_ASSERT(chunk->__isset.meta_data);
57: 
58: 		if (chunk->__isset.file_path) {
59: 			throw std::runtime_error("Only inlined data files are supported (no references)");
60: 		}
61: 
62: 		// ugh. sometimes there is an extra offset for the dict. sometimes it's wrong.
63: 		chunk_read_offset = chunk->meta_data.data_page_offset;
64: 		if (chunk->meta_data.__isset.dictionary_page_offset && chunk->meta_data.dictionary_page_offset >= 4) {
65: 			// this assumes the data pages follow the dict pages directly.
66: 			chunk_read_offset = chunk->meta_data.dictionary_page_offset;
67: 		}
68: 		group_rows_available = chunk->meta_data.num_values;
69: 	}
70: 	virtual ~ColumnReader();
71: 
72: 	virtual idx_t Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
73: 	                   Vector &result_out);
74: 
75: 	virtual void Skip(idx_t num_values);
76: 
77: 	const LogicalType &Type() {
78: 		return type;
79: 	}
80: 
81: 	const SchemaElement &Schema() {
82: 		return schema;
83: 	}
84: 
85: 	virtual idx_t GroupRowsAvailable() {
86: 		return group_rows_available;
87: 	}
88: 
89: 	unique_ptr<BaseStatistics> Stats(const std::vector<ColumnChunk> &columns) {
90: 		if (Type().id() == LogicalTypeId::LIST || Type().id() == LogicalTypeId::STRUCT ||
91: 		    Type().id() == LogicalTypeId::MAP) {
92: 			return nullptr;
93: 		}
94: 		return ParquetTransformColumnStatistics(Schema(), Type(), columns[file_idx]);
95: 	}
96: 
97: protected:
98: 	// readers that use the default Read() need to implement those
99: 	virtual void Plain(shared_ptr<ByteBuffer> plain_data, uint8_t *defines, idx_t num_values, parquet_filter_t &filter,
100: 	                   idx_t result_offset, Vector &result) {
101: 		throw NotImplementedException("Plain");
102: 	}
103: 
104: 	virtual void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) {
105: 		throw NotImplementedException("Dictionary");
106: 	}
107: 
108: 	virtual void Offsets(uint32_t *offsets, uint8_t *defines, idx_t num_values, parquet_filter_t &filter,
109: 	                     idx_t result_offset, Vector &result) {
110: 		throw NotImplementedException("Offsets");
111: 	}
112: 
113: 	// these are nops for most types, but not for strings
114: 	virtual void DictReference(Vector &result) {
115: 	}
116: 	virtual void PlainReference(shared_ptr<ByteBuffer>, Vector &result) {
117: 	}
118: 
119: 	bool HasDefines() {
120: 		return max_define > 0;
121: 	}
122: 
123: 	bool HasRepeats() {
124: 		return max_repeat > 0;
125: 	}
126: 
127: protected:
128: 	const SchemaElement &schema;
129: 
130: 	idx_t file_idx;
131: 	idx_t max_define;
132: 	idx_t max_repeat;
133: 
134: 	ParquetReader &reader;
135: 	LogicalType type;
136: 
137: private:
138: 	void PrepareRead(parquet_filter_t &filter);
139: 	void PreparePage(idx_t compressed_page_size, idx_t uncompressed_page_size);
140: 	void PrepareDataPage(PageHeader &page_hdr);
141: 
142: 	const duckdb_parquet::format::ColumnChunk *chunk;
143: 
144: 	duckdb_apache::thrift::protocol::TProtocol *protocol;
145: 	idx_t page_rows_available;
146: 	idx_t group_rows_available;
147: 	idx_t chunk_read_offset;
148: 
149: 	shared_ptr<ResizeableBuffer> block;
150: 
151: 	ResizeableBuffer offset_buffer;
152: 
153: 	unique_ptr<RleBpDecoder> dict_decoder;
154: 	unique_ptr<RleBpDecoder> defined_decoder;
155: 	unique_ptr<RleBpDecoder> repeated_decoder;
156: 
157: 	// dummies for Skip()
158: 	parquet_filter_t none_filter;
159: 	ResizeableBuffer dummy_define;
160: 	ResizeableBuffer dummy_repeat;
161: };
162: 
163: } // namespace duckdb
[end of extension/parquet/include/column_reader.hpp]
[start of extension/parquet/include/column_writer.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // column_writer.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb.hpp"
12: #include "parquet_types.h"
13: 
14: namespace duckdb {
15: class BufferedSerializer;
16: class ParquetWriter;
17: class ColumnWriterPageState;
18: 
19: class ColumnWriterState {
20: public:
21: 	virtual ~ColumnWriterState();
22: 
23: 	vector<uint16_t> definition_levels;
24: 	vector<uint16_t> repetition_levels;
25: 	vector<bool> is_empty;
26: };
27: 
28: class ColumnWriter {
29: 	//! We limit the uncompressed page size to 100MB
30: 	// The max size in Parquet is 2GB, but we choose a more conservative limit
31: 	static constexpr const idx_t MAX_UNCOMPRESSED_PAGE_SIZE = 100000000;
32: 
33: public:
34: 	ColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define, bool can_have_nulls);
35: 	virtual ~ColumnWriter();
36: 
37: 	ParquetWriter &writer;
38: 	idx_t schema_idx;
39: 	idx_t max_repeat;
40: 	idx_t max_define;
41: 	bool can_have_nulls;
42: 
43: public:
44: 	//! Create the column writer for a specific type recursively
45: 	static unique_ptr<ColumnWriter> CreateWriterRecursive(vector<duckdb_parquet::format::SchemaElement> &schemas,
46: 	                                                      ParquetWriter &writer, const LogicalType &type,
47: 	                                                      const string &name, idx_t max_repeat = 0,
48: 	                                                      idx_t max_define = 1, bool can_have_nulls = true);
49: 
50: 	virtual unique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,
51: 	                                                           vector<string> schema_path);
52: 	virtual void Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count);
53: 
54: 	virtual void BeginWrite(ColumnWriterState &state);
55: 	virtual void Write(ColumnWriterState &state, Vector &vector, idx_t count);
56: 	virtual void FinalizeWrite(ColumnWriterState &state);
57: 
58: protected:
59: 	void HandleDefineLevels(ColumnWriterState &state, ColumnWriterState *parent, ValidityMask &validity, idx_t count,
60: 	                        uint16_t define_value, uint16_t null_value);
61: 	void HandleRepeatLevels(ColumnWriterState &state_p, ColumnWriterState *parent, idx_t count, idx_t max_repeat);
62: 
63: 	void WriteLevels(Serializer &temp_writer, const vector<uint16_t> &levels, idx_t max_value, idx_t start_offset,
64: 	                 idx_t count);
65: 
66: 	void NextPage(ColumnWriterState &state_p);
67: 	void FlushPage(ColumnWriterState &state_p);
68: 
69: 	//! Retrieves the row size of a vector at the specified location. Only used for scalar types.
70: 	virtual idx_t GetRowSize(Vector &vector, idx_t index) = 0;
71: 	//! Writes a (subset of a) vector to the specified serializer. Only used for scalar types.
72: 	virtual void WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &vector,
73: 	                         idx_t chunk_start, idx_t chunk_end) = 0;
74: 	//! Initialize the writer for a specific page. Only used for scalar types.
75: 	virtual unique_ptr<ColumnWriterPageState> InitializePageState();
76: 	//! Flushes the writer for a specific page. Only used for scalar types.
77: 	virtual void FlushPageState(Serializer &temp_writer, ColumnWriterPageState *state);
78: 
79: 	void CompressPage(BufferedSerializer &temp_writer, size_t &compressed_size, data_ptr_t &compressed_data,
80: 	                  unique_ptr<data_t[]> &compressed_buf);
81: };
82: 
83: } // namespace duckdb
[end of extension/parquet/include/column_writer.hpp]
[start of extension/parquet/include/decimal_column_reader.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // decimal_column_reader.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "column_reader.hpp"
12: #include "templated_column_reader.hpp"
13: 
14: namespace duckdb {
15: 
16: template <class DUCKDB_PHYSICAL_TYPE>
17: struct DecimalParquetValueConversion {
18: 	static DUCKDB_PHYSICAL_TYPE DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
19: 		auto dict_ptr = (DUCKDB_PHYSICAL_TYPE *)dict.ptr;
20: 		return dict_ptr[offset];
21: 	}
22: 
23: 	static DUCKDB_PHYSICAL_TYPE PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
24: 		DUCKDB_PHYSICAL_TYPE res = 0;
25: 		auto byte_len = (idx_t)reader.Schema().type_length; /* sure, type length needs to be a signed int */
26: 		D_ASSERT(byte_len <= sizeof(DUCKDB_PHYSICAL_TYPE));
27: 		plain_data.available(byte_len);
28: 		auto res_ptr = (uint8_t *)&res;
29: 
30: 		// numbers are stored as two's complement so some muckery is required
31: 		bool positive = (*plain_data.ptr & 0x80) == 0;
32: 
33: 		for (idx_t i = 0; i < byte_len; i++) {
34: 			auto byte = *(plain_data.ptr + (byte_len - i - 1));
35: 			res_ptr[i] = positive ? byte : byte ^ 0xFF;
36: 		}
37: 		plain_data.inc(byte_len);
38: 		if (!positive) {
39: 			res += 1;
40: 			return -res;
41: 		}
42: 		return res;
43: 	}
44: 
45: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
46: 		plain_data.inc(reader.Schema().type_length);
47: 	}
48: };
49: 
50: template <class DUCKDB_PHYSICAL_TYPE>
51: class DecimalColumnReader
52:     : public TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE, DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE>> {
53: 
54: public:
55: 	DecimalColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
56: 	                    idx_t max_define_p, idx_t max_repeat_p)
57: 	    : TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE, DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE>>(
58: 	          reader, move(type_p), schema_p, file_idx_p, max_define_p, max_repeat_p) {};
59: 
60: protected:
61: 	void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) {
62: 		this->dict = make_shared<ResizeableBuffer>(this->reader.allocator, num_entries * sizeof(DUCKDB_PHYSICAL_TYPE));
63: 		auto dict_ptr = (DUCKDB_PHYSICAL_TYPE *)this->dict->ptr;
64: 		for (idx_t i = 0; i < num_entries; i++) {
65: 			dict_ptr[i] = DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE>::PlainRead(*dictionary_data, *this);
66: 		}
67: 	}
68: };
69: 
70: } // namespace duckdb
[end of extension/parquet/include/decimal_column_reader.hpp]
[start of extension/parquet/include/parquet_reader.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // parquet_reader.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb.hpp"
12: #ifndef DUCKDB_AMALGAMATION
13: #include "duckdb/common/common.hpp"
14: #include "duckdb/common/exception.hpp"
15: #include "duckdb/common/string_util.hpp"
16: #include "duckdb/common/types/data_chunk.hpp"
17: #endif
18: #include "column_reader.hpp"
19: #include "parquet_file_metadata_cache.hpp"
20: #include "parquet_rle_bp_decoder.hpp"
21: #include "parquet_types.h"
22: #include "resizable_buffer.hpp"
23: 
24: #include <exception>
25: 
26: namespace duckdb_parquet {
27: namespace format {
28: class FileMetaData;
29: }
30: } // namespace duckdb_parquet
31: 
32: namespace duckdb {
33: class Allocator;
34: class ClientContext;
35: class ChunkCollection;
36: class BaseStatistics;
37: class TableFilterSet;
38: 
39: struct ParquetReaderScanState {
40: 	vector<idx_t> group_idx_list;
41: 	int64_t current_group;
42: 	vector<column_t> column_ids;
43: 	idx_t group_offset;
44: 	unique_ptr<FileHandle> file_handle;
45: 	unique_ptr<ColumnReader> root_reader;
46: 	unique_ptr<duckdb_apache::thrift::protocol::TProtocol> thrift_file_proto;
47: 
48: 	bool finished;
49: 	TableFilterSet *filters;
50: 	SelectionVector sel;
51: 
52: 	ResizeableBuffer define_buf;
53: 	ResizeableBuffer repeat_buf;
54: };
55: 
56: struct ParquetOptions {
57: 	explicit ParquetOptions() {
58: 	}
59: 	explicit ParquetOptions(ClientContext &context);
60: 
61: 	bool binary_as_string = false;
62: };
63: 
64: class ParquetReader {
65: public:
66: 	ParquetReader(Allocator &allocator, unique_ptr<FileHandle> file_handle_p,
67: 	              const vector<LogicalType> &expected_types_p, const string &initial_filename_p = string());
68: 	ParquetReader(Allocator &allocator, unique_ptr<FileHandle> file_handle_p)
69: 	    : ParquetReader(allocator, move(file_handle_p), vector<LogicalType>(), string()) {
70: 	}
71: 
72: 	ParquetReader(ClientContext &context, string file_name, const vector<LogicalType> &expected_types_p,
73: 	              ParquetOptions parquet_options, const string &initial_filename = string());
74: 	ParquetReader(ClientContext &context, string file_name, ParquetOptions parquet_options)
75: 	    : ParquetReader(context, move(file_name), vector<LogicalType>(), parquet_options, string()) {
76: 	}
77: 	~ParquetReader();
78: 
79: 	Allocator &allocator;
80: 	string file_name;
81: 	FileOpener *file_opener;
82: 	vector<LogicalType> return_types;
83: 	vector<string> names;
84: 	shared_ptr<ParquetFileMetadataCache> metadata;
85: 	ParquetOptions parquet_options;
86: 
87: public:
88: 	void InitializeScan(ParquetReaderScanState &state, vector<column_t> column_ids, vector<idx_t> groups_to_read,
89: 	                    TableFilterSet *table_filters);
90: 	void Scan(ParquetReaderScanState &state, DataChunk &output);
91: 
92: 	idx_t NumRows();
93: 	idx_t NumRowGroups();
94: 
95: 	const duckdb_parquet::format::FileMetaData *GetFileMetadata();
96: 
97: 	static unique_ptr<BaseStatistics> ReadStatistics(ParquetReader &reader, LogicalType &type, column_t column_index,
98: 	                                                 const duckdb_parquet::format::FileMetaData *file_meta_data);
99: 
100: private:
101: 	void InitializeSchema(const vector<LogicalType> &expected_types_p, const string &initial_filename_p);
102: 	bool ScanInternal(ParquetReaderScanState &state, DataChunk &output);
103: 	unique_ptr<ColumnReader> CreateReader(const duckdb_parquet::format::FileMetaData *file_meta_data);
104: 
105: 	unique_ptr<ColumnReader> CreateReaderRecursive(const duckdb_parquet::format::FileMetaData *file_meta_data,
106: 	                                               idx_t depth, idx_t max_define, idx_t max_repeat,
107: 	                                               idx_t &next_schema_idx, idx_t &next_file_idx);
108: 	const duckdb_parquet::format::RowGroup &GetGroup(ParquetReaderScanState &state);
109: 	void PrepareRowGroupBuffer(ParquetReaderScanState &state, idx_t out_col_idx);
110: 	LogicalType DeriveLogicalType(const SchemaElement &s_ele);
111: 
112: 	template <typename... Args>
113: 	std::runtime_error FormatException(const string fmt_str, Args... params) {
114: 		return std::runtime_error("Failed to read Parquet file \"" + file_name +
115: 		                          "\": " + StringUtil::Format(fmt_str, params...));
116: 	}
117: 
118: private:
119: 	unique_ptr<FileHandle> file_handle;
120: };
121: 
122: } // namespace duckdb
[end of extension/parquet/include/parquet_reader.hpp]
[start of extension/parquet/include/parquet_statistics.hpp]
1: #pragma once
2: 
3: #include "duckdb.hpp"
4: #ifndef DUCKDB_AMALGAMATION
5: #include "duckdb/storage/statistics/base_statistics.hpp"
6: #endif
7: #include "parquet_types.h"
8: 
9: namespace duckdb {
10: 
11: using duckdb_parquet::format::ColumnChunk;
12: using duckdb_parquet::format::SchemaElement;
13: 
14: struct LogicalType;
15: 
16: unique_ptr<BaseStatistics> ParquetTransformColumnStatistics(const SchemaElement &s_ele, const LogicalType &type,
17:                                                             const ColumnChunk &column_chunk);
18: 
19: } // namespace duckdb
[end of extension/parquet/include/parquet_statistics.hpp]
[start of extension/parquet/include/parquet_timestamp.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // parquet_timestamp.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb.hpp"
12: 
13: namespace duckdb {
14: 
15: struct Int96 {
16: 	uint32_t value[3];
17: };
18: 
19: int64_t ImpalaTimestampToNanoseconds(const Int96 &impala_timestamp);
20: timestamp_t ImpalaTimestampToTimestamp(const Int96 &raw_ts);
21: Int96 TimestampToImpalaTimestamp(timestamp_t &ts);
22: timestamp_t ParquetTimestampMicrosToTimestamp(const int64_t &raw_ts);
23: timestamp_t ParquetTimestampMsToTimestamp(const int64_t &raw_ts);
24: date_t ParquetIntToDate(const int32_t &raw_date);
25: 
26: } // namespace duckdb
[end of extension/parquet/include/parquet_timestamp.hpp]
[start of extension/parquet/include/parquet_writer.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // parquet_writer.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb.hpp"
12: #ifndef DUCKDB_AMALGAMATION
13: #include "duckdb/common/common.hpp"
14: #include "duckdb/common/exception.hpp"
15: #include "duckdb/common/mutex.hpp"
16: #include "duckdb/common/serializer/buffered_file_writer.hpp"
17: #include "duckdb/common/types/chunk_collection.hpp"
18: #endif
19: 
20: #include "parquet_types.h"
21: #include "column_writer.hpp"
22: #include "thrift/protocol/TCompactProtocol.h"
23: 
24: namespace duckdb {
25: class FileSystem;
26: class FileOpener;
27: 
28: class ParquetWriter {
29: 	friend class ColumnWriter;
30: 	friend class ListColumnWriter;
31: 	friend class StructColumnWriter;
32: 
33: public:
34: 	ParquetWriter(FileSystem &fs, string file_name, FileOpener *file_opener, vector<LogicalType> types,
35: 	              vector<string> names, duckdb_parquet::format::CompressionCodec::type codec);
36: 
37: public:
38: 	void Flush(ChunkCollection &buffer);
39: 	void Finalize();
40: 
41: 	static duckdb_parquet::format::Type::type DuckDBTypeToParquetType(const LogicalType &duckdb_type);
42: 	static bool DuckDBTypeToConvertedType(const LogicalType &duckdb_type,
43: 	                                      duckdb_parquet::format::ConvertedType::type &result);
44: 
45: private:
46: 	string file_name;
47: 	vector<LogicalType> sql_types;
48: 	vector<string> column_names;
49: 	duckdb_parquet::format::CompressionCodec::type codec;
50: 
51: 	unique_ptr<BufferedFileWriter> writer;
52: 	shared_ptr<duckdb_apache::thrift::protocol::TProtocol> protocol;
53: 	duckdb_parquet::format::FileMetaData file_meta_data;
54: 	std::mutex lock;
55: 
56: 	vector<unique_ptr<ColumnWriter>> column_writers;
57: };
58: 
59: } // namespace duckdb
[end of extension/parquet/include/parquet_writer.hpp]
[start of extension/parquet/include/string_column_reader.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // string_column_reader.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "column_reader.hpp"
12: 
13: namespace duckdb {
14: 
15: struct StringParquetValueConversion {
16: 	static string_t DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader);
17: 
18: 	static string_t PlainRead(ByteBuffer &plain_data, ColumnReader &reader);
19: 
20: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader);
21: };
22: 
23: class StringColumnReader : public TemplatedColumnReader<string_t, StringParquetValueConversion> {
24: public:
25: 	StringColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t schema_idx_p,
26: 	                   idx_t max_define_p, idx_t max_repeat_p)
27: 	    : TemplatedColumnReader<string_t, StringParquetValueConversion>(reader, move(type_p), schema_p, schema_idx_p,
28: 	                                                                    max_define_p, max_repeat_p) {
29: 		fixed_width_string_length = 0;
30: 		if (schema_p.type == Type::FIXED_LEN_BYTE_ARRAY) {
31: 			D_ASSERT(schema_p.__isset.type_length);
32: 			fixed_width_string_length = schema_p.type_length;
33: 		}
34: 	};
35: 
36: 	unique_ptr<string_t[]> dict_strings;
37: 	idx_t fixed_width_string_length;
38: 
39: public:
40: 	void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) override;
41: 
42: 	uint32_t VerifyString(const char *str_data, uint32_t str_len);
43: 
44: protected:
45: 	void DictReference(Vector &result) override;
46: 	void PlainReference(shared_ptr<ByteBuffer> plain_data, Vector &result) override;
47: };
48: 
49: } // namespace duckdb
[end of extension/parquet/include/string_column_reader.hpp]
[start of extension/parquet/parquet_metadata.cpp]
1: #include "parquet_metadata.hpp"
2: #include <sstream>
3: 
4: #ifndef DUCKDB_AMALGAMATION
5: #include "duckdb/common/types/blob.hpp"
6: #include "duckdb/main/config.hpp"
7: #endif
8: 
9: namespace duckdb {
10: 
11: struct ParquetMetaDataBindData : public FunctionData {
12: 	vector<LogicalType> return_types;
13: 	vector<string> files;
14: };
15: 
16: struct ParquetMetaDataOperatorData : public FunctionOperatorData {
17: 	idx_t file_index;
18: 	ChunkCollection collection;
19: 
20: 	static void BindMetaData(vector<LogicalType> &return_types, vector<string> &names);
21: 	static void BindSchema(vector<LogicalType> &return_types, vector<string> &names);
22: 
23: 	void LoadFileMetaData(ClientContext &context, const vector<LogicalType> &return_types, const string &file_path);
24: 	void LoadSchemaData(ClientContext &context, const vector<LogicalType> &return_types, const string &file_path);
25: };
26: 
27: template <class T>
28: string ConvertParquetElementToString(T &&entry) {
29: 	std::stringstream ss;
30: 	ss << entry;
31: 	return ss.str();
32: }
33: 
34: template <class T>
35: string PrintParquetElementToString(T &&entry) {
36: 	std::stringstream ss;
37: 	entry.printTo(ss);
38: 	return ss.str();
39: }
40: 
41: void ParquetMetaDataOperatorData::BindMetaData(vector<LogicalType> &return_types, vector<string> &names) {
42: 	names.emplace_back("file_name");
43: 	return_types.emplace_back(LogicalType::VARCHAR);
44: 
45: 	names.emplace_back("row_group_id");
46: 	return_types.emplace_back(LogicalType::BIGINT);
47: 
48: 	names.emplace_back("row_group_num_rows");
49: 	return_types.emplace_back(LogicalType::BIGINT);
50: 
51: 	names.emplace_back("row_group_num_columns");
52: 	return_types.emplace_back(LogicalType::BIGINT);
53: 
54: 	names.emplace_back("row_group_bytes");
55: 	return_types.emplace_back(LogicalType::BIGINT);
56: 
57: 	names.emplace_back("column_id");
58: 	return_types.emplace_back(LogicalType::BIGINT);
59: 
60: 	names.emplace_back("file_offset");
61: 	return_types.emplace_back(LogicalType::BIGINT);
62: 
63: 	names.emplace_back("num_values");
64: 	return_types.emplace_back(LogicalType::BIGINT);
65: 
66: 	names.emplace_back("path_in_schema");
67: 	return_types.emplace_back(LogicalType::VARCHAR);
68: 
69: 	names.emplace_back("type");
70: 	return_types.emplace_back(LogicalType::VARCHAR);
71: 
72: 	names.emplace_back("stats_min");
73: 	return_types.emplace_back(LogicalType::VARCHAR);
74: 
75: 	names.emplace_back("stats_max");
76: 	return_types.emplace_back(LogicalType::VARCHAR);
77: 
78: 	names.emplace_back("stats_null_count");
79: 	return_types.emplace_back(LogicalType::BIGINT);
80: 
81: 	names.emplace_back("stats_distinct_count");
82: 	return_types.emplace_back(LogicalType::BIGINT);
83: 
84: 	names.emplace_back("stats_min_value");
85: 	return_types.emplace_back(LogicalType::VARCHAR);
86: 
87: 	names.emplace_back("stats_max_value");
88: 	return_types.emplace_back(LogicalType::VARCHAR);
89: 
90: 	names.emplace_back("compression");
91: 	return_types.emplace_back(LogicalType::VARCHAR);
92: 
93: 	names.emplace_back("encodings");
94: 	return_types.emplace_back(LogicalType::VARCHAR);
95: 
96: 	names.emplace_back("index_page_offset");
97: 	return_types.emplace_back(LogicalType::BIGINT);
98: 
99: 	names.emplace_back("dictionary_page_offset");
100: 	return_types.emplace_back(LogicalType::BIGINT);
101: 
102: 	names.emplace_back("data_page_offset");
103: 	return_types.emplace_back(LogicalType::BIGINT);
104: 
105: 	names.emplace_back("total_compressed_size");
106: 	return_types.emplace_back(LogicalType::BIGINT);
107: 
108: 	names.emplace_back("total_uncompressed_size");
109: 	return_types.emplace_back(LogicalType::BIGINT);
110: }
111: 
112: Value ConvertParquetStats(duckdb_parquet::format::Type::type type, bool stats_is_set, const std::string &stats) {
113: 	if (!stats_is_set) {
114: 		return Value(LogicalType::VARCHAR);
115: 	}
116: 	switch (type) {
117: 	case Type::BOOLEAN:
118: 		if (stats.size() == sizeof(bool)) {
119: 			return Value(Value::BOOLEAN(Load<bool>((data_ptr_t)stats.c_str())).ToString());
120: 		}
121: 		break;
122: 	case Type::INT32:
123: 		if (stats.size() == sizeof(int32_t)) {
124: 			return Value(Value::INTEGER(Load<int32_t>((data_ptr_t)stats.c_str())).ToString());
125: 		}
126: 		break;
127: 	case Type::INT64:
128: 		if (stats.size() == sizeof(int64_t)) {
129: 			return Value(Value::BIGINT(Load<int64_t>((data_ptr_t)stats.c_str())).ToString());
130: 		}
131: 		break;
132: 	case Type::FLOAT:
133: 		if (stats.size() == sizeof(float)) {
134: 			float val = Load<float>((data_ptr_t)stats.c_str());
135: 			if (Value::FloatIsValid(val)) {
136: 				return Value(Value::FLOAT(val).ToString());
137: 			}
138: 		}
139: 		break;
140: 	case Type::DOUBLE:
141: 		if (stats.size() == sizeof(double)) {
142: 			double val = Load<double>((data_ptr_t)stats.c_str());
143: 			if (Value::DoubleIsValid(val)) {
144: 				return Value(Value::DOUBLE(val).ToString());
145: 			}
146: 		}
147: 		break;
148: 	case Type::BYTE_ARRAY:
149: 	case Type::INT96:
150: 	case Type::FIXED_LEN_BYTE_ARRAY:
151: 	default:
152: 		break;
153: 	}
154: 	if (Value::StringIsValid(stats)) {
155: 		return Value(stats);
156: 	} else {
157: 		return Value(Blob::ToString(string_t(stats)));
158: 	}
159: }
160: 
161: void ParquetMetaDataOperatorData::LoadFileMetaData(ClientContext &context, const vector<LogicalType> &return_types,
162:                                                    const string &file_path) {
163: 	collection.Reset();
164: 	ParquetOptions parquet_options(context);
165: 	auto reader = make_unique<ParquetReader>(context, file_path, parquet_options);
166: 	idx_t count = 0;
167: 	DataChunk current_chunk;
168: 	current_chunk.Initialize(return_types);
169: 	auto meta_data = reader->GetFileMetadata();
170: 	for (idx_t row_group_idx = 0; row_group_idx < meta_data->row_groups.size(); row_group_idx++) {
171: 		auto &row_group = meta_data->row_groups[row_group_idx];
172: 
173: 		for (idx_t col_idx = 0; col_idx < row_group.columns.size(); col_idx++) {
174: 			auto &column = row_group.columns[col_idx];
175: 			auto &col_meta = column.meta_data;
176: 			auto &stats = col_meta.statistics;
177: 
178: 			// file_name, LogicalType::VARCHAR
179: 			current_chunk.SetValue(0, count, file_path);
180: 
181: 			// row_group_id, LogicalType::BIGINT
182: 			current_chunk.SetValue(1, count, Value::BIGINT(row_group_idx));
183: 
184: 			// row_group_num_rows, LogicalType::BIGINT
185: 			current_chunk.SetValue(2, count, Value::BIGINT(row_group.num_rows));
186: 
187: 			// row_group_num_columns, LogicalType::BIGINT
188: 			current_chunk.SetValue(3, count, Value::BIGINT(row_group.columns.size()));
189: 
190: 			// row_group_bytes, LogicalType::BIGINT
191: 			current_chunk.SetValue(4, count, Value::BIGINT(row_group.total_byte_size));
192: 
193: 			// column_id, LogicalType::BIGINT
194: 			current_chunk.SetValue(5, count, Value::BIGINT(col_idx));
195: 
196: 			// file_offset, LogicalType::BIGINT
197: 			current_chunk.SetValue(6, count, Value::BIGINT(column.file_offset));
198: 
199: 			// num_values, LogicalType::BIGINT
200: 			current_chunk.SetValue(7, count, Value::BIGINT(col_meta.num_values));
201: 
202: 			// path_in_schema, LogicalType::VARCHAR
203: 			current_chunk.SetValue(8, count, StringUtil::Join(col_meta.path_in_schema, ", "));
204: 
205: 			// type, LogicalType::VARCHAR
206: 			current_chunk.SetValue(9, count, ConvertParquetElementToString(col_meta.type));
207: 
208: 			// stats_min, LogicalType::VARCHAR
209: 			current_chunk.SetValue(10, count, ConvertParquetStats(col_meta.type, stats.__isset.min, stats.min));
210: 
211: 			// stats_max, LogicalType::VARCHAR
212: 			current_chunk.SetValue(11, count, ConvertParquetStats(col_meta.type, stats.__isset.max, stats.max));
213: 
214: 			// stats_null_count, LogicalType::BIGINT
215: 			current_chunk.SetValue(
216: 			    12, count, stats.__isset.null_count ? Value::BIGINT(stats.null_count) : Value(LogicalType::BIGINT));
217: 
218: 			// stats_distinct_count, LogicalType::BIGINT
219: 			current_chunk.SetValue(13, count,
220: 			                       stats.__isset.distinct_count ? Value::BIGINT(stats.distinct_count)
221: 			                                                    : Value(LogicalType::BIGINT));
222: 
223: 			// stats_min_value, LogicalType::VARCHAR
224: 			current_chunk.SetValue(14, count,
225: 			                       ConvertParquetStats(col_meta.type, stats.__isset.min_value, stats.min_value));
226: 
227: 			// stats_max_value, LogicalType::VARCHAR
228: 			current_chunk.SetValue(15, count,
229: 			                       ConvertParquetStats(col_meta.type, stats.__isset.max_value, stats.max_value));
230: 
231: 			// compression, LogicalType::VARCHAR
232: 			current_chunk.SetValue(16, count, ConvertParquetElementToString(col_meta.codec));
233: 
234: 			// encodings, LogicalType::VARCHAR
235: 			vector<string> encoding_string;
236: 			for (auto &encoding : col_meta.encodings) {
237: 				encoding_string.push_back(ConvertParquetElementToString(encoding));
238: 			}
239: 			current_chunk.SetValue(17, count, Value(StringUtil::Join(encoding_string, ", ")));
240: 
241: 			// index_page_offset, LogicalType::BIGINT
242: 			current_chunk.SetValue(18, count, Value::BIGINT(col_meta.index_page_offset));
243: 
244: 			// dictionary_page_offset, LogicalType::BIGINT
245: 			current_chunk.SetValue(19, count, Value::BIGINT(col_meta.dictionary_page_offset));
246: 
247: 			// data_page_offset, LogicalType::BIGINT
248: 			current_chunk.SetValue(20, count, Value::BIGINT(col_meta.data_page_offset));
249: 
250: 			// total_compressed_size, LogicalType::BIGINT
251: 			current_chunk.SetValue(21, count, Value::BIGINT(col_meta.total_compressed_size));
252: 
253: 			// total_uncompressed_size, LogicalType::BIGINT
254: 			current_chunk.SetValue(22, count, Value::BIGINT(col_meta.total_uncompressed_size));
255: 
256: 			count++;
257: 			if (count >= STANDARD_VECTOR_SIZE) {
258: 				current_chunk.SetCardinality(count);
259: 				collection.Append(current_chunk);
260: 
261: 				count = 0;
262: 				current_chunk.Reset();
263: 			}
264: 		}
265: 	}
266: 	current_chunk.SetCardinality(count);
267: 	collection.Append(current_chunk);
268: }
269: 
270: void ParquetMetaDataOperatorData::BindSchema(vector<LogicalType> &return_types, vector<string> &names) {
271: 	names.emplace_back("file_name");
272: 	return_types.emplace_back(LogicalType::VARCHAR);
273: 
274: 	names.emplace_back("name");
275: 	return_types.emplace_back(LogicalType::VARCHAR);
276: 
277: 	names.emplace_back("type");
278: 	return_types.emplace_back(LogicalType::VARCHAR);
279: 
280: 	names.emplace_back("type_length");
281: 	return_types.emplace_back(LogicalType::VARCHAR);
282: 
283: 	names.emplace_back("repetition_type");
284: 	return_types.emplace_back(LogicalType::VARCHAR);
285: 
286: 	names.emplace_back("num_children");
287: 	return_types.emplace_back(LogicalType::BIGINT);
288: 
289: 	names.emplace_back("converted_type");
290: 	return_types.emplace_back(LogicalType::VARCHAR);
291: 
292: 	names.emplace_back("scale");
293: 	return_types.emplace_back(LogicalType::BIGINT);
294: 
295: 	names.emplace_back("precision");
296: 	return_types.emplace_back(LogicalType::BIGINT);
297: 
298: 	names.emplace_back("field_id");
299: 	return_types.emplace_back(LogicalType::BIGINT);
300: 
301: 	names.emplace_back("logical_type");
302: 	return_types.emplace_back(LogicalType::VARCHAR);
303: }
304: 
305: Value ParquetLogicalTypeToString(const duckdb_parquet::format::LogicalType &type) {
306: 
307: 	if (type.__isset.STRING) {
308: 		return Value(PrintParquetElementToString(type.STRING));
309: 	}
310: 	if (type.__isset.MAP) {
311: 		return Value(PrintParquetElementToString(type.MAP));
312: 	}
313: 	if (type.__isset.LIST) {
314: 		return Value(PrintParquetElementToString(type.LIST));
315: 	}
316: 	if (type.__isset.ENUM) {
317: 		return Value(PrintParquetElementToString(type.ENUM));
318: 	}
319: 	if (type.__isset.DECIMAL) {
320: 		return Value(PrintParquetElementToString(type.DECIMAL));
321: 	}
322: 	if (type.__isset.DATE) {
323: 		return Value(PrintParquetElementToString(type.DATE));
324: 	}
325: 	if (type.__isset.TIME) {
326: 		return Value(PrintParquetElementToString(type.TIME));
327: 	}
328: 	if (type.__isset.TIMESTAMP) {
329: 		return Value(PrintParquetElementToString(type.TIMESTAMP));
330: 	}
331: 	if (type.__isset.INTEGER) {
332: 		return Value(PrintParquetElementToString(type.INTEGER));
333: 	}
334: 	if (type.__isset.UNKNOWN) {
335: 		return Value(PrintParquetElementToString(type.UNKNOWN));
336: 	}
337: 	if (type.__isset.JSON) {
338: 		return Value(PrintParquetElementToString(type.JSON));
339: 	}
340: 	if (type.__isset.BSON) {
341: 		return Value(PrintParquetElementToString(type.BSON));
342: 	}
343: 	if (type.__isset.UUID) {
344: 		return Value(PrintParquetElementToString(type.UUID));
345: 	}
346: 	return Value();
347: }
348: 
349: void ParquetMetaDataOperatorData::LoadSchemaData(ClientContext &context, const vector<LogicalType> &return_types,
350:                                                  const string &file_path) {
351: 	collection.Reset();
352: 	ParquetOptions parquet_options(context);
353: 	auto reader = make_unique<ParquetReader>(context, file_path, parquet_options);
354: 	idx_t count = 0;
355: 	DataChunk current_chunk;
356: 	current_chunk.Initialize(return_types);
357: 	auto meta_data = reader->GetFileMetadata();
358: 	for (idx_t col_idx = 0; col_idx < meta_data->schema.size(); col_idx++) {
359: 		auto &column = meta_data->schema[col_idx];
360: 
361: 		// file_name, LogicalType::VARCHAR
362: 		current_chunk.SetValue(0, count, file_path);
363: 
364: 		// name, LogicalType::VARCHAR
365: 		current_chunk.SetValue(1, count, column.name);
366: 
367: 		// type, LogicalType::VARCHAR
368: 		current_chunk.SetValue(2, count, ConvertParquetElementToString(column.type));
369: 
370: 		// type_length, LogicalType::VARCHAR
371: 		current_chunk.SetValue(3, count, Value::INTEGER(column.type_length));
372: 
373: 		// repetition_type, LogicalType::VARCHAR
374: 		current_chunk.SetValue(4, count, ConvertParquetElementToString(column.repetition_type));
375: 
376: 		// num_children, LogicalType::BIGINT
377: 		current_chunk.SetValue(5, count, Value::BIGINT(column.num_children));
378: 
379: 		// converted_type, LogicalType::VARCHAR
380: 		current_chunk.SetValue(6, count, ConvertParquetElementToString(column.converted_type));
381: 
382: 		// scale, LogicalType::BIGINT
383: 		current_chunk.SetValue(7, count, Value::BIGINT(column.scale));
384: 
385: 		// precision, LogicalType::BIGINT
386: 		current_chunk.SetValue(8, count, Value::BIGINT(column.precision));
387: 
388: 		// field_id, LogicalType::BIGINT
389: 		current_chunk.SetValue(9, count, Value::BIGINT(column.field_id));
390: 
391: 		// logical_type, LogicalType::VARCHAR
392: 		current_chunk.SetValue(10, count, ParquetLogicalTypeToString(column.logicalType));
393: 
394: 		count++;
395: 		if (count >= STANDARD_VECTOR_SIZE) {
396: 			current_chunk.SetCardinality(count);
397: 			collection.Append(current_chunk);
398: 
399: 			count = 0;
400: 			current_chunk.Reset();
401: 		}
402: 	}
403: 	current_chunk.SetCardinality(count);
404: 	collection.Append(current_chunk);
405: }
406: 
407: template <bool SCHEMA>
408: unique_ptr<FunctionData> ParquetMetaDataBind(ClientContext &context, vector<Value> &inputs,
409:                                              unordered_map<string, Value> &named_parameters,
410:                                              vector<LogicalType> &input_table_types, vector<string> &input_table_names,
411:                                              vector<LogicalType> &return_types, vector<string> &names) {
412: 	auto &config = DBConfig::GetConfig(context);
413: 	if (!config.enable_external_access) {
414: 		throw PermissionException("Scanning Parquet files is disabled through configuration");
415: 	}
416: 	if (SCHEMA) {
417: 		ParquetMetaDataOperatorData::BindSchema(return_types, names);
418: 	} else {
419: 		ParquetMetaDataOperatorData::BindMetaData(return_types, names);
420: 	}
421: 
422: 	auto file_name = inputs[0].GetValue<string>();
423: 	auto result = make_unique<ParquetMetaDataBindData>();
424: 
425: 	FileSystem &fs = FileSystem::GetFileSystem(context);
426: 	result->return_types = return_types;
427: 	result->files = fs.Glob(file_name);
428: 	if (result->files.empty()) {
429: 		throw IOException("No files found that match the pattern \"%s\"", file_name);
430: 	}
431: 	return move(result);
432: }
433: 
434: template <bool SCHEMA>
435: unique_ptr<FunctionOperatorData> ParquetMetaDataInit(ClientContext &context, const FunctionData *bind_data_p,
436:                                                      const vector<column_t> &column_ids,
437:                                                      TableFilterCollection *filters) {
438: 	auto &bind_data = (ParquetMetaDataBindData &)*bind_data_p;
439: 	D_ASSERT(!bind_data.files.empty());
440: 
441: 	auto result = make_unique<ParquetMetaDataOperatorData>();
442: 	if (SCHEMA) {
443: 		result->LoadSchemaData(context, bind_data.return_types, bind_data.files[0]);
444: 	} else {
445: 		result->LoadFileMetaData(context, bind_data.return_types, bind_data.files[0]);
446: 	}
447: 	result->file_index = 0;
448: 	return move(result);
449: }
450: 
451: template <bool SCHEMA>
452: void ParquetMetaDataImplementation(ClientContext &context, const FunctionData *bind_data_p,
453:                                    FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
454: 	auto &data = (ParquetMetaDataOperatorData &)*operator_state;
455: 	auto &bind_data = (ParquetMetaDataBindData &)*bind_data_p;
456: 	while (true) {
457: 		auto chunk = data.collection.Fetch();
458: 		if (!chunk) {
459: 			if (data.file_index + 1 < bind_data.files.size()) {
460: 				// load the metadata for the next file
461: 				data.file_index++;
462: 				if (SCHEMA) {
463: 					data.LoadSchemaData(context, bind_data.return_types, bind_data.files[data.file_index]);
464: 				} else {
465: 					data.LoadFileMetaData(context, bind_data.return_types, bind_data.files[data.file_index]);
466: 				}
467: 				continue;
468: 			} else {
469: 				// no files remaining: done
470: 				return;
471: 			}
472: 		}
473: 		output.Move(*chunk);
474: 		if (output.size() != 0) {
475: 			return;
476: 		}
477: 	}
478: }
479: 
480: ParquetMetaDataFunction::ParquetMetaDataFunction()
481:     : TableFunction("parquet_metadata", {LogicalType::VARCHAR}, ParquetMetaDataImplementation<false>,
482:                     ParquetMetaDataBind<false>, ParquetMetaDataInit<false>, /* statistics */ nullptr,
483:                     /* cleanup */ nullptr,
484:                     /* dependency */ nullptr, nullptr,
485:                     /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr, nullptr, nullptr, nullptr, nullptr,
486:                     nullptr, false, false, nullptr) {
487: }
488: 
489: ParquetSchemaFunction::ParquetSchemaFunction()
490:     : TableFunction("parquet_schema", {LogicalType::VARCHAR}, ParquetMetaDataImplementation<true>,
491:                     ParquetMetaDataBind<true>, ParquetMetaDataInit<true>, /* statistics */ nullptr,
492:                     /* cleanup */ nullptr,
493:                     /* dependency */ nullptr, nullptr,
494:                     /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr, nullptr, nullptr, nullptr, nullptr,
495:                     nullptr, false, false, nullptr) {
496: }
497: 
498: } // namespace duckdb
[end of extension/parquet/parquet_metadata.cpp]
[start of extension/parquet/parquet_reader.cpp]
1: #include "parquet_reader.hpp"
2: #include "parquet_timestamp.hpp"
3: #include "parquet_statistics.hpp"
4: #include "column_reader.hpp"
5: 
6: #include "boolean_column_reader.hpp"
7: #include "callback_column_reader.hpp"
8: #include "decimal_column_reader.hpp"
9: #include "list_column_reader.hpp"
10: #include "string_column_reader.hpp"
11: #include "struct_column_reader.hpp"
12: #include "templated_column_reader.hpp"
13: 
14: #include "thrift_tools.hpp"
15: 
16: #include "parquet_file_metadata_cache.hpp"
17: 
18: #include "duckdb.hpp"
19: #ifndef DUCKDB_AMALGAMATION
20: #include "duckdb/planner/table_filter.hpp"
21: #include "duckdb/planner/filter/constant_filter.hpp"
22: #include "duckdb/planner/filter/null_filter.hpp"
23: #include "duckdb/planner/filter/conjunction_filter.hpp"
24: #include "duckdb/common/file_system.hpp"
25: #include "duckdb/common/string_util.hpp"
26: #include "duckdb/common/types/date.hpp"
27: #include "duckdb/common/pair.hpp"
28: 
29: #include "duckdb/storage/object_cache.hpp"
30: #endif
31: 
32: #include <sstream>
33: #include <cassert>
34: #include <chrono>
35: #include <cstring>
36: #include <iostream>
37: 
38: namespace duckdb {
39: 
40: using duckdb_parquet::format::ColumnChunk;
41: using duckdb_parquet::format::ConvertedType;
42: using duckdb_parquet::format::FieldRepetitionType;
43: using duckdb_parquet::format::FileMetaData;
44: using ParquetRowGroup = duckdb_parquet::format::RowGroup;
45: using duckdb_parquet::format::SchemaElement;
46: using duckdb_parquet::format::Statistics;
47: using duckdb_parquet::format::Type;
48: 
49: static unique_ptr<duckdb_apache::thrift::protocol::TProtocol> CreateThriftProtocol(Allocator &allocator,
50:                                                                                    FileHandle &file_handle) {
51: 	auto transport = make_shared<ThriftFileTransport>(allocator, file_handle);
52: 	return make_unique<duckdb_apache::thrift::protocol::TCompactProtocolT<ThriftFileTransport>>(move(transport));
53: }
54: 
55: static shared_ptr<ParquetFileMetadataCache> LoadMetadata(Allocator &allocator, FileHandle &file_handle) {
56: 	auto current_time = std::chrono::system_clock::to_time_t(std::chrono::system_clock::now());
57: 
58: 	auto proto = CreateThriftProtocol(allocator, file_handle);
59: 	auto &transport = ((ThriftFileTransport &)*proto->getTransport());
60: 	auto file_size = transport.GetSize();
61: 	if (file_size < 12) {
62: 		throw InvalidInputException("File '%s' too small to be a Parquet file", file_handle.path);
63: 	}
64: 
65: 	ResizeableBuffer buf;
66: 	buf.resize(allocator, 8);
67: 	buf.zero();
68: 
69: 	transport.SetLocation(file_size - 8);
70: 	transport.read((uint8_t *)buf.ptr, 8);
71: 
72: 	if (strncmp(buf.ptr + 4, "PAR1", 4) != 0) {
73: 		throw InvalidInputException("No magic bytes found at end of file '%s'", file_handle.path);
74: 	}
75: 	// read four-byte footer length from just before the end magic bytes
76: 	auto footer_len = *(uint32_t *)buf.ptr;
77: 	if (footer_len <= 0 || file_size < 12 + footer_len) {
78: 		throw InvalidInputException("Footer length error in file '%s'", file_handle.path);
79: 	}
80: 	auto metadata_pos = file_size - (footer_len + 8);
81: 	transport.SetLocation(metadata_pos);
82: 	transport.Prefetch(metadata_pos, footer_len);
83: 
84: 	auto metadata = make_unique<FileMetaData>();
85: 	metadata->read(proto.get());
86: 	return make_shared<ParquetFileMetadataCache>(move(metadata), current_time);
87: }
88: 
89: LogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele) {
90: 	// inner node
91: 	D_ASSERT(s_ele.__isset.type && s_ele.num_children == 0);
92: 	if (s_ele.type == Type::FIXED_LEN_BYTE_ARRAY && !s_ele.__isset.type_length) {
93: 		throw IOException("FIXED_LEN_BYTE_ARRAY requires length to be set");
94: 	}
95: 	if (s_ele.__isset.converted_type) {
96: 		switch (s_ele.converted_type) {
97: 		case ConvertedType::INT_8:
98: 			if (s_ele.type == Type::INT32) {
99: 				return LogicalType::TINYINT;
100: 			} else {
101: 				throw IOException("INT8 converted type can only be set for value of Type::INT32");
102: 			}
103: 		case ConvertedType::INT_16:
104: 			if (s_ele.type == Type::INT32) {
105: 				return LogicalType::SMALLINT;
106: 			} else {
107: 				throw IOException("INT16 converted type can only be set for value of Type::INT32");
108: 			}
109: 		case ConvertedType::INT_32:
110: 			if (s_ele.type == Type::INT32) {
111: 				return LogicalType::INTEGER;
112: 			} else {
113: 				throw IOException("INT32 converted type can only be set for value of Type::INT32");
114: 			}
115: 		case ConvertedType::INT_64:
116: 			if (s_ele.type == Type::INT64) {
117: 				return LogicalType::BIGINT;
118: 			} else {
119: 				throw IOException("INT64 converted type can only be set for value of Type::INT32");
120: 			}
121: 		case ConvertedType::UINT_8:
122: 			if (s_ele.type == Type::INT32) {
123: 				return LogicalType::UTINYINT;
124: 			} else {
125: 				throw IOException("UINT8 converted type can only be set for value of Type::INT32");
126: 			}
127: 		case ConvertedType::UINT_16:
128: 			if (s_ele.type == Type::INT32) {
129: 				return LogicalType::USMALLINT;
130: 			} else {
131: 				throw IOException("UINT16 converted type can only be set for value of Type::INT32");
132: 			}
133: 		case ConvertedType::UINT_32:
134: 			if (s_ele.type == Type::INT32) {
135: 				return LogicalType::UINTEGER;
136: 			} else {
137: 				throw IOException("UINT32 converted type can only be set for value of Type::INT32");
138: 			}
139: 		case ConvertedType::UINT_64:
140: 			if (s_ele.type == Type::INT64) {
141: 				return LogicalType::UBIGINT;
142: 			} else {
143: 				throw IOException("UINT64 converted type can only be set for value of Type::INT64");
144: 			}
145: 		case ConvertedType::DATE:
146: 			if (s_ele.type == Type::INT32) {
147: 				return LogicalType::DATE;
148: 			} else {
149: 				throw IOException("DATE converted type can only be set for value of Type::INT32");
150: 			}
151: 		case ConvertedType::TIMESTAMP_MICROS:
152: 		case ConvertedType::TIMESTAMP_MILLIS:
153: 			if (s_ele.type == Type::INT64) {
154: 				return LogicalType::TIMESTAMP;
155: 			} else {
156: 				throw IOException("TIMESTAMP converted type can only be set for value of Type::INT64");
157: 			}
158: 		case ConvertedType::DECIMAL:
159: 			if (!s_ele.__isset.precision || !s_ele.__isset.scale) {
160: 				throw IOException("DECIMAL requires a length and scale specifier!");
161: 			}
162: 			switch (s_ele.type) {
163: 			case Type::BYTE_ARRAY:
164: 			case Type::FIXED_LEN_BYTE_ARRAY:
165: 			case Type::INT32:
166: 			case Type::INT64:
167: 				return LogicalType::DECIMAL(s_ele.precision, s_ele.scale);
168: 			default:
169: 				throw IOException(
170: 				    "DECIMAL converted type can only be set for value of Type::(FIXED_LEN_)BYTE_ARRAY/INT32/INT64");
171: 			}
172: 		case ConvertedType::UTF8:
173: 			switch (s_ele.type) {
174: 			case Type::BYTE_ARRAY:
175: 			case Type::FIXED_LEN_BYTE_ARRAY:
176: 				return LogicalType::VARCHAR;
177: 			default:
178: 				throw IOException("UTF8 converted type can only be set for Type::(FIXED_LEN_)BYTE_ARRAY");
179: 			}
180: 		case ConvertedType::MAP:
181: 		case ConvertedType::MAP_KEY_VALUE:
182: 		case ConvertedType::LIST:
183: 		case ConvertedType::ENUM:
184: 		case ConvertedType::TIME_MILLIS:
185: 		case ConvertedType::TIME_MICROS:
186: 		case ConvertedType::JSON:
187: 		case ConvertedType::BSON:
188: 		case ConvertedType::INTERVAL:
189: 		default:
190: 			throw IOException("Unsupported converted type");
191: 		}
192: 	} else {
193: 		// no converted type set
194: 		// use default type for each physical type
195: 		switch (s_ele.type) {
196: 		case Type::BOOLEAN:
197: 			return LogicalType::BOOLEAN;
198: 		case Type::INT32:
199: 			return LogicalType::INTEGER;
200: 		case Type::INT64:
201: 			return LogicalType::BIGINT;
202: 		case Type::INT96: // always a timestamp it would seem
203: 			return LogicalType::TIMESTAMP;
204: 		case Type::FLOAT:
205: 			return LogicalType::FLOAT;
206: 		case Type::DOUBLE:
207: 			return LogicalType::DOUBLE;
208: 		case Type::BYTE_ARRAY:
209: 		case Type::FIXED_LEN_BYTE_ARRAY:
210: 			if (parquet_options.binary_as_string) {
211: 				return LogicalType::VARCHAR;
212: 			}
213: 			return LogicalType::BLOB;
214: 		default:
215: 			return LogicalType::INVALID;
216: 		}
217: 	}
218: }
219: 
220: unique_ptr<ColumnReader> ParquetReader::CreateReaderRecursive(const FileMetaData *file_meta_data, idx_t depth,
221:                                                               idx_t max_define, idx_t max_repeat,
222:                                                               idx_t &next_schema_idx, idx_t &next_file_idx) {
223: 	D_ASSERT(file_meta_data);
224: 	D_ASSERT(next_schema_idx < file_meta_data->schema.size());
225: 	auto &s_ele = file_meta_data->schema[next_schema_idx];
226: 	auto this_idx = next_schema_idx;
227: 
228: 	if (s_ele.__isset.repetition_type) {
229: 		if (s_ele.repetition_type != FieldRepetitionType::REQUIRED) {
230: 			max_define++;
231: 		}
232: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
233: 			max_repeat++;
234: 		}
235: 	}
236: 
237: 	if (!s_ele.__isset.type) { // inner node
238: 		if (s_ele.num_children == 0) {
239: 			throw std::runtime_error("Node has no children but should");
240: 		}
241: 		child_list_t<LogicalType> child_types;
242: 		vector<unique_ptr<ColumnReader>> child_readers;
243: 
244: 		idx_t c_idx = 0;
245: 		while (c_idx < (idx_t)s_ele.num_children) {
246: 			next_schema_idx++;
247: 
248: 			auto &child_ele = file_meta_data->schema[next_schema_idx];
249: 
250: 			auto child_reader = CreateReaderRecursive(file_meta_data, depth + 1, max_define, max_repeat,
251: 			                                          next_schema_idx, next_file_idx);
252: 			child_types.push_back(make_pair(child_ele.name, child_reader->Type()));
253: 			child_readers.push_back(move(child_reader));
254: 
255: 			c_idx++;
256: 		}
257: 		D_ASSERT(!child_types.empty());
258: 		unique_ptr<ColumnReader> result;
259: 		LogicalType result_type;
260: 
261: 		bool is_repeated = s_ele.repetition_type == FieldRepetitionType::REPEATED;
262: 		bool is_list = s_ele.__isset.converted_type && s_ele.converted_type == ConvertedType::LIST;
263: 		bool is_map = s_ele.__isset.converted_type && s_ele.converted_type == ConvertedType::MAP;
264: 		bool is_map_kv = s_ele.__isset.converted_type && s_ele.converted_type == ConvertedType::MAP_KEY_VALUE;
265: 		if (!is_map_kv && this_idx > 0) {
266: 			// check if the parent node of this is a map
267: 			auto &p_ele = file_meta_data->schema[this_idx - 1];
268: 			bool parent_is_map = p_ele.__isset.converted_type && p_ele.converted_type == ConvertedType::MAP;
269: 			bool parent_has_children = p_ele.__isset.num_children && p_ele.num_children == 1;
270: 			is_map_kv = parent_is_map && parent_has_children;
271: 		}
272: 
273: 		if (is_map_kv) {
274: 			if (child_types.size() != 2) {
275: 				throw IOException("MAP_KEY_VALUE requires two children");
276: 			}
277: 			if (!is_repeated) {
278: 				throw IOException("MAP_KEY_VALUE needs to be repeated");
279: 			}
280: 			result_type = LogicalType::MAP(move(child_types[0].second), move(child_types[1].second));
281: 			for (auto &child_reader : child_readers) {
282: 				auto child_type = LogicalType::LIST(child_reader->Type());
283: 				child_reader = make_unique<ListColumnReader>(*this, move(child_type), s_ele, this_idx, max_define,
284: 				                                             max_repeat, move(child_reader));
285: 			}
286: 			result = make_unique<StructColumnReader>(*this, result_type, s_ele, this_idx, max_define - 1,
287: 			                                         max_repeat - 1, move(child_readers));
288: 			return result;
289: 		}
290: 		if (child_types.size() > 1 || (!is_list && !is_map && !is_repeated)) {
291: 			result_type = LogicalType::STRUCT(move(child_types));
292: 			result = make_unique<StructColumnReader>(*this, result_type, s_ele, this_idx, max_define, max_repeat,
293: 			                                         move(child_readers));
294: 		} else {
295: 			// if we have a struct with only a single type, pull up
296: 			result_type = child_types[0].second;
297: 			result = move(child_readers[0]);
298: 		}
299: 		if (is_repeated) {
300: 			result_type = LogicalType::LIST(result_type);
301: 			return make_unique<ListColumnReader>(*this, result_type, s_ele, this_idx, max_define, max_repeat,
302: 			                                     move(result));
303: 		}
304: 		return result;
305: 	} else { // leaf node
306: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
307: 			const auto derived_type = DeriveLogicalType(s_ele);
308: 			auto list_type = LogicalType::LIST(derived_type);
309: 
310: 			auto element_reader =
311: 			    ColumnReader::CreateReader(*this, derived_type, s_ele, next_file_idx++, max_define, max_repeat);
312: 
313: 			return make_unique<ListColumnReader>(*this, list_type, s_ele, this_idx, max_define, max_repeat,
314: 			                                     move(element_reader));
315: 		}
316: 
317: 		// TODO check return value of derive type or should we only do this on read()
318: 		return ColumnReader::CreateReader(*this, DeriveLogicalType(s_ele), s_ele, next_file_idx++, max_define,
319: 		                                  max_repeat);
320: 	}
321: }
322: 
323: // TODO we don't need readers for columns we are not going to read ay
324: unique_ptr<ColumnReader> ParquetReader::CreateReader(const duckdb_parquet::format::FileMetaData *file_meta_data) {
325: 	idx_t next_schema_idx = 0;
326: 	idx_t next_file_idx = 0;
327: 
328: 	auto ret = CreateReaderRecursive(file_meta_data, 0, 0, 0, next_schema_idx, next_file_idx);
329: 	D_ASSERT(next_schema_idx == file_meta_data->schema.size() - 1);
330: 	D_ASSERT(file_meta_data->row_groups.empty() || next_file_idx == file_meta_data->row_groups[0].columns.size());
331: 	return ret;
332: }
333: 
334: void ParquetReader::InitializeSchema(const vector<LogicalType> &expected_types_p, const string &initial_filename_p) {
335: 	auto file_meta_data = GetFileMetadata();
336: 
337: 	if (file_meta_data->__isset.encryption_algorithm) {
338: 		throw FormatException("Encrypted Parquet files are not supported");
339: 	}
340: 	// check if we like this schema
341: 	if (file_meta_data->schema.size() < 2) {
342: 		throw FormatException("Need at least one non-root column in the file");
343: 	}
344: 
345: 	bool has_expected_types = !expected_types_p.empty();
346: 	auto root_reader = CreateReader(file_meta_data);
347: 
348: 	auto &root_type = root_reader->Type();
349: 	auto &child_types = StructType::GetChildTypes(root_type);
350: 	D_ASSERT(root_type.id() == LogicalTypeId::STRUCT);
351: 	if (has_expected_types && child_types.size() != expected_types_p.size()) {
352: 		throw FormatException("column count mismatch");
353: 	}
354: 	idx_t col_idx = 0;
355: 	for (auto &type_pair : child_types) {
356: 		if (has_expected_types && expected_types_p[col_idx] != type_pair.second) {
357: 			if (initial_filename_p.empty()) {
358: 				throw FormatException("column \"%d\" in parquet file is of type %s, could not auto cast to "
359: 				                      "expected type %s for this column",
360: 				                      col_idx, type_pair.second, expected_types_p[col_idx].ToString());
361: 			} else {
362: 				throw FormatException("schema mismatch in Parquet glob: column \"%d\" in parquet file is of type "
363: 				                      "%s, but in the original file \"%s\" this column is of type \"%s\"",
364: 				                      col_idx, type_pair.second, initial_filename_p,
365: 				                      expected_types_p[col_idx].ToString());
366: 			}
367: 		} else {
368: 			names.push_back(type_pair.first);
369: 			return_types.push_back(type_pair.second);
370: 		}
371: 		col_idx++;
372: 	}
373: 	D_ASSERT(!names.empty());
374: 	D_ASSERT(!return_types.empty());
375: }
376: 
377: ParquetOptions::ParquetOptions(ClientContext &context) {
378: 	Value binary_as_string_val;
379: 	if (context.TryGetCurrentSetting("binary_as_string", binary_as_string_val)) {
380: 		binary_as_string = binary_as_string_val.GetValue<bool>();
381: 	}
382: }
383: 
384: ParquetReader::ParquetReader(Allocator &allocator_p, unique_ptr<FileHandle> file_handle_p,
385:                              const vector<LogicalType> &expected_types_p, const string &initial_filename_p)
386:     : allocator(allocator_p) {
387: 	file_name = file_handle_p->path;
388: 	file_handle = move(file_handle_p);
389: 	metadata = LoadMetadata(allocator, *file_handle);
390: 	InitializeSchema(expected_types_p, initial_filename_p);
391: }
392: 
393: ParquetReader::ParquetReader(ClientContext &context_p, string file_name_p, const vector<LogicalType> &expected_types_p,
394:                              ParquetOptions parquet_options_p, const string &initial_filename_p)
395:     : allocator(Allocator::Get(context_p)), file_opener(FileSystem::GetFileOpener(context_p)),
396:       parquet_options(parquet_options_p) {
397: 	auto &fs = FileSystem::GetFileSystem(context_p);
398: 	file_name = move(file_name_p);
399: 	file_handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,
400: 	                          FileSystem::DEFAULT_COMPRESSION, file_opener);
401: 	// If object cached is disabled
402: 	// or if this file has cached metadata
403: 	// or if the cached version already expired
404: 	auto last_modify_time = fs.GetLastModifiedTime(*file_handle);
405: 	if (!ObjectCache::ObjectCacheEnabled(context_p)) {
406: 		metadata = LoadMetadata(allocator, *file_handle);
407: 	} else {
408: 		metadata = ObjectCache::GetObjectCache(context_p).Get<ParquetFileMetadataCache>(file_name);
409: 		if (!metadata || (last_modify_time + 10 >= metadata->read_time)) {
410: 			metadata = LoadMetadata(allocator, *file_handle);
411: 			ObjectCache::GetObjectCache(context_p).Put(file_name, metadata);
412: 		}
413: 	}
414: 	InitializeSchema(expected_types_p, initial_filename_p);
415: }
416: 
417: ParquetReader::~ParquetReader() {
418: }
419: 
420: const FileMetaData *ParquetReader::GetFileMetadata() {
421: 	D_ASSERT(metadata);
422: 	D_ASSERT(metadata->metadata);
423: 	return metadata->metadata.get();
424: }
425: 
426: // TODO also somewhat ugly, perhaps this can be moved to the column reader too
427: unique_ptr<BaseStatistics> ParquetReader::ReadStatistics(ParquetReader &reader, LogicalType &type,
428:                                                          column_t file_col_idx, const FileMetaData *file_meta_data) {
429: 	unique_ptr<BaseStatistics> column_stats;
430: 	auto root_reader = reader.CreateReader(file_meta_data);
431: 	auto column_reader = ((StructColumnReader *)root_reader.get())->GetChildReader(file_col_idx);
432: 
433: 	for (auto &row_group : file_meta_data->row_groups) {
434: 		auto chunk_stats = column_reader->Stats(row_group.columns);
435: 		if (!chunk_stats) {
436: 			return nullptr;
437: 		}
438: 		if (!column_stats) {
439: 			column_stats = move(chunk_stats);
440: 		} else {
441: 			column_stats->Merge(*chunk_stats);
442: 		}
443: 	}
444: 	return column_stats;
445: }
446: 
447: const ParquetRowGroup &ParquetReader::GetGroup(ParquetReaderScanState &state) {
448: 	auto file_meta_data = GetFileMetadata();
449: 	D_ASSERT(state.current_group >= 0 && (idx_t)state.current_group < state.group_idx_list.size());
450: 	D_ASSERT(state.group_idx_list[state.current_group] >= 0 &&
451: 	         state.group_idx_list[state.current_group] < file_meta_data->row_groups.size());
452: 	return file_meta_data->row_groups[state.group_idx_list[state.current_group]];
453: }
454: 
455: void ParquetReader::PrepareRowGroupBuffer(ParquetReaderScanState &state, idx_t out_col_idx) {
456: 	auto &group = GetGroup(state);
457: 
458: 	auto column_reader = ((StructColumnReader *)state.root_reader.get())->GetChildReader(state.column_ids[out_col_idx]);
459: 
460: 	// TODO move this to columnreader too
461: 	if (state.filters) {
462: 		auto stats = column_reader->Stats(group.columns);
463: 		// filters contain output chunk index, not file col idx!
464: 		auto filter_entry = state.filters->filters.find(out_col_idx);
465: 		if (stats && filter_entry != state.filters->filters.end()) {
466: 			bool skip_chunk = false;
467: 			auto &filter = *filter_entry->second;
468: 			auto prune_result = filter.CheckStatistics(*stats);
469: 			if (prune_result == FilterPropagateResult::FILTER_ALWAYS_FALSE) {
470: 				skip_chunk = true;
471: 			}
472: 			if (skip_chunk) {
473: 				state.group_offset = group.num_rows;
474: 				return;
475: 				// this effectively will skip this chunk
476: 			}
477: 		}
478: 	}
479: 
480: 	state.root_reader->InitializeRead(group.columns, *state.thrift_file_proto);
481: }
482: 
483: idx_t ParquetReader::NumRows() {
484: 	return GetFileMetadata()->num_rows;
485: }
486: 
487: idx_t ParquetReader::NumRowGroups() {
488: 	return GetFileMetadata()->row_groups.size();
489: }
490: 
491: void ParquetReader::InitializeScan(ParquetReaderScanState &state, vector<column_t> column_ids,
492:                                    vector<idx_t> groups_to_read, TableFilterSet *filters) {
493: 	state.current_group = -1;
494: 	state.finished = false;
495: 	state.column_ids = move(column_ids);
496: 	state.group_offset = 0;
497: 	state.group_idx_list = move(groups_to_read);
498: 	state.filters = filters;
499: 	state.sel.Initialize(STANDARD_VECTOR_SIZE);
500: 	state.file_handle =
501: 	    file_handle->file_system.OpenFile(file_handle->path, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,
502: 	                                      FileSystem::DEFAULT_COMPRESSION, file_opener);
503: 	state.thrift_file_proto = CreateThriftProtocol(allocator, *state.file_handle);
504: 	state.root_reader = CreateReader(GetFileMetadata());
505: 
506: 	state.define_buf.resize(allocator, STANDARD_VECTOR_SIZE);
507: 	state.repeat_buf.resize(allocator, STANDARD_VECTOR_SIZE);
508: }
509: 
510: void FilterIsNull(Vector &v, parquet_filter_t &filter_mask, idx_t count) {
511: 	auto &mask = FlatVector::Validity(v);
512: 	if (mask.AllValid()) {
513: 		filter_mask.reset();
514: 	} else {
515: 		for (idx_t i = 0; i < count; i++) {
516: 			filter_mask[i] = filter_mask[i] && !mask.RowIsValid(i);
517: 		}
518: 	}
519: }
520: 
521: void FilterIsNotNull(Vector &v, parquet_filter_t &filter_mask, idx_t count) {
522: 	auto &mask = FlatVector::Validity(v);
523: 	if (!mask.AllValid()) {
524: 		for (idx_t i = 0; i < count; i++) {
525: 			filter_mask[i] = filter_mask[i] && mask.RowIsValid(i);
526: 		}
527: 	}
528: }
529: 
530: template <class T, class OP>
531: void TemplatedFilterOperation(Vector &v, T constant, parquet_filter_t &filter_mask, idx_t count) {
532: 	D_ASSERT(v.GetVectorType() == VectorType::FLAT_VECTOR); // we just created the damn thing it better be
533: 
534: 	auto v_ptr = FlatVector::GetData<T>(v);
535: 	auto &mask = FlatVector::Validity(v);
536: 
537: 	if (!mask.AllValid()) {
538: 		for (idx_t i = 0; i < count; i++) {
539: 			if (mask.RowIsValid(i)) {
540: 				filter_mask[i] = filter_mask[i] && OP::Operation(v_ptr[i], constant);
541: 			}
542: 		}
543: 	} else {
544: 		for (idx_t i = 0; i < count; i++) {
545: 			filter_mask[i] = filter_mask[i] && OP::Operation(v_ptr[i], constant);
546: 		}
547: 	}
548: }
549: 
550: template <class OP>
551: static void FilterOperationSwitch(Vector &v, Value &constant, parquet_filter_t &filter_mask, idx_t count) {
552: 	if (filter_mask.none() || count == 0) {
553: 		return;
554: 	}
555: 	switch (v.GetType().id()) {
556: 	case LogicalTypeId::BOOLEAN:
557: 		TemplatedFilterOperation<bool, OP>(v, constant.value_.boolean, filter_mask, count);
558: 		break;
559: 	case LogicalTypeId::UTINYINT:
560: 		TemplatedFilterOperation<uint8_t, OP>(v, constant.value_.utinyint, filter_mask, count);
561: 		break;
562: 	case LogicalTypeId::USMALLINT:
563: 		TemplatedFilterOperation<uint16_t, OP>(v, constant.value_.usmallint, filter_mask, count);
564: 		break;
565: 	case LogicalTypeId::UINTEGER:
566: 		TemplatedFilterOperation<uint32_t, OP>(v, constant.value_.uinteger, filter_mask, count);
567: 		break;
568: 	case LogicalTypeId::UBIGINT:
569: 		TemplatedFilterOperation<uint64_t, OP>(v, constant.value_.ubigint, filter_mask, count);
570: 		break;
571: 	case LogicalTypeId::TINYINT:
572: 		TemplatedFilterOperation<int8_t, OP>(v, constant.value_.tinyint, filter_mask, count);
573: 		break;
574: 	case LogicalTypeId::SMALLINT:
575: 		TemplatedFilterOperation<int16_t, OP>(v, constant.value_.smallint, filter_mask, count);
576: 		break;
577: 	case LogicalTypeId::INTEGER:
578: 		TemplatedFilterOperation<int32_t, OP>(v, constant.value_.integer, filter_mask, count);
579: 		break;
580: 	case LogicalTypeId::BIGINT:
581: 		TemplatedFilterOperation<int64_t, OP>(v, constant.value_.bigint, filter_mask, count);
582: 		break;
583: 	case LogicalTypeId::FLOAT:
584: 		TemplatedFilterOperation<float, OP>(v, constant.value_.float_, filter_mask, count);
585: 		break;
586: 	case LogicalTypeId::DOUBLE:
587: 		TemplatedFilterOperation<double, OP>(v, constant.value_.double_, filter_mask, count);
588: 		break;
589: 	case LogicalTypeId::DATE:
590: 		TemplatedFilterOperation<date_t, OP>(v, constant.value_.date, filter_mask, count);
591: 		break;
592: 	case LogicalTypeId::TIMESTAMP:
593: 		TemplatedFilterOperation<timestamp_t, OP>(v, constant.value_.timestamp, filter_mask, count);
594: 		break;
595: 	case LogicalTypeId::BLOB:
596: 	case LogicalTypeId::VARCHAR:
597: 		TemplatedFilterOperation<string_t, OP>(v, string_t(constant.str_value), filter_mask, count);
598: 		break;
599: 	case LogicalTypeId::DECIMAL:
600: 		switch (v.GetType().InternalType()) {
601: 		case PhysicalType::INT16:
602: 			TemplatedFilterOperation<int16_t, OP>(v, constant.value_.smallint, filter_mask, count);
603: 			break;
604: 		case PhysicalType::INT32:
605: 			TemplatedFilterOperation<int32_t, OP>(v, constant.value_.integer, filter_mask, count);
606: 			break;
607: 		case PhysicalType::INT64:
608: 			TemplatedFilterOperation<int64_t, OP>(v, constant.value_.bigint, filter_mask, count);
609: 			break;
610: 		case PhysicalType::INT128:
611: 			TemplatedFilterOperation<hugeint_t, OP>(v, constant.value_.hugeint, filter_mask, count);
612: 			break;
613: 		default:
614: 			throw InternalException("Unsupported internal type for decimal");
615: 		}
616: 		break;
617: 	default:
618: 		throw NotImplementedException("Unsupported type for filter %s", v.ToString());
619: 	}
620: }
621: 
622: static void ApplyFilter(Vector &v, TableFilter &filter, parquet_filter_t &filter_mask, idx_t count) {
623: 	switch (filter.filter_type) {
624: 	case TableFilterType::CONJUNCTION_AND: {
625: 		auto &conjunction = (ConjunctionAndFilter &)filter;
626: 		for (auto &child_filter : conjunction.child_filters) {
627: 			ApplyFilter(v, *child_filter, filter_mask, count);
628: 		}
629: 		break;
630: 	}
631: 	case TableFilterType::CONJUNCTION_OR: {
632: 		auto &conjunction = (ConjunctionOrFilter &)filter;
633: 		for (auto &child_filter : conjunction.child_filters) {
634: 			parquet_filter_t child_mask = filter_mask;
635: 			ApplyFilter(v, *child_filter, child_mask, count);
636: 			filter_mask |= child_mask;
637: 		}
638: 		break;
639: 	}
640: 	case TableFilterType::CONSTANT_COMPARISON: {
641: 		auto &constant_filter = (ConstantFilter &)filter;
642: 		switch (constant_filter.comparison_type) {
643: 		case ExpressionType::COMPARE_EQUAL:
644: 			FilterOperationSwitch<Equals>(v, constant_filter.constant, filter_mask, count);
645: 			break;
646: 		case ExpressionType::COMPARE_LESSTHAN:
647: 			FilterOperationSwitch<LessThan>(v, constant_filter.constant, filter_mask, count);
648: 			break;
649: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
650: 			FilterOperationSwitch<LessThanEquals>(v, constant_filter.constant, filter_mask, count);
651: 			break;
652: 		case ExpressionType::COMPARE_GREATERTHAN:
653: 			FilterOperationSwitch<GreaterThan>(v, constant_filter.constant, filter_mask, count);
654: 			break;
655: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
656: 			FilterOperationSwitch<GreaterThanEquals>(v, constant_filter.constant, filter_mask, count);
657: 			break;
658: 		default:
659: 			D_ASSERT(0);
660: 		}
661: 		break;
662: 	}
663: 	case TableFilterType::IS_NOT_NULL:
664: 		FilterIsNotNull(v, filter_mask, count);
665: 		break;
666: 	case TableFilterType::IS_NULL:
667: 		FilterIsNull(v, filter_mask, count);
668: 		break;
669: 	default:
670: 		D_ASSERT(0);
671: 		break;
672: 	}
673: }
674: 
675: void ParquetReader::Scan(ParquetReaderScanState &state, DataChunk &result) {
676: 	while (ScanInternal(state, result)) {
677: 		if (result.size() > 0) {
678: 			break;
679: 		}
680: 		result.Reset();
681: 	}
682: }
683: 
684: bool ParquetReader::ScanInternal(ParquetReaderScanState &state, DataChunk &result) {
685: 	if (state.finished) {
686: 		return false;
687: 	}
688: 
689: 	// see if we have to switch to the next row group in the parquet file
690: 	if (state.current_group < 0 || (int64_t)state.group_offset >= GetGroup(state).num_rows) {
691: 		state.current_group++;
692: 		state.group_offset = 0;
693: 
694: 		if ((idx_t)state.current_group == state.group_idx_list.size()) {
695: 			state.finished = true;
696: 			return false;
697: 		}
698: 
699: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
700: 			// this is a special case where we are not interested in the actual contents of the file
701: 			if (state.column_ids[out_col_idx] == COLUMN_IDENTIFIER_ROW_ID) {
702: 				continue;
703: 			}
704: 
705: 			PrepareRowGroupBuffer(state, out_col_idx);
706: 		}
707: 		return true;
708: 	}
709: 
710: 	auto this_output_chunk_rows = MinValue<idx_t>(STANDARD_VECTOR_SIZE, GetGroup(state).num_rows - state.group_offset);
711: 	result.SetCardinality(this_output_chunk_rows);
712: 
713: 	if (this_output_chunk_rows == 0) {
714: 		state.finished = true;
715: 		return false; // end of last group, we are done
716: 	}
717: 
718: 	// we evaluate simple table filters directly in this scan so we can skip decoding column data that's never going to
719: 	// be relevant
720: 	parquet_filter_t filter_mask;
721: 	filter_mask.set();
722: 
723: 	state.define_buf.zero();
724: 	state.repeat_buf.zero();
725: 
726: 	auto define_ptr = (uint8_t *)state.define_buf.ptr;
727: 	auto repeat_ptr = (uint8_t *)state.repeat_buf.ptr;
728: 
729: 	auto root_reader = ((StructColumnReader *)state.root_reader.get());
730: 
731: 	if (state.filters) {
732: 		vector<bool> need_to_read(result.ColumnCount(), true);
733: 
734: 		// first load the columns that are used in filters
735: 		for (auto &filter_col : state.filters->filters) {
736: 			auto file_col_idx = state.column_ids[filter_col.first];
737: 
738: 			if (filter_mask.none()) { // if no rows are left we can stop checking filters
739: 				break;
740: 			}
741: 
742: 			root_reader->GetChildReader(file_col_idx)
743: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[filter_col.first]);
744: 
745: 			need_to_read[filter_col.first] = false;
746: 
747: 			ApplyFilter(result.data[filter_col.first], *filter_col.second, filter_mask, this_output_chunk_rows);
748: 		}
749: 
750: 		// we still may have to read some cols
751: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
752: 			if (!need_to_read[out_col_idx]) {
753: 				continue;
754: 			}
755: 			auto file_col_idx = state.column_ids[out_col_idx];
756: 
757: 			if (filter_mask.none()) {
758: 				root_reader->GetChildReader(file_col_idx)->Skip(result.size());
759: 				continue;
760: 			}
761: 			// TODO handle ROWID here, too
762: 			root_reader->GetChildReader(file_col_idx)
763: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);
764: 		}
765: 
766: 		idx_t sel_size = 0;
767: 		for (idx_t i = 0; i < this_output_chunk_rows; i++) {
768: 			if (filter_mask[i]) {
769: 				state.sel.set_index(sel_size++, i);
770: 			}
771: 		}
772: 
773: 		result.Slice(state.sel, sel_size);
774: 		result.Verify();
775: 
776: 	} else { // #nofilter, just fricking load the data
777: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
778: 			auto file_col_idx = state.column_ids[out_col_idx];
779: 
780: 			if (file_col_idx == COLUMN_IDENTIFIER_ROW_ID) {
781: 				Value constant_42 = Value::BIGINT(42);
782: 				result.data[out_col_idx].Reference(constant_42);
783: 				continue;
784: 			}
785: 
786: 			root_reader->GetChildReader(file_col_idx)
787: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);
788: 		}
789: 	}
790: 
791: 	state.group_offset += this_output_chunk_rows;
792: 	return true;
793: }
794: 
795: } // namespace duckdb
[end of extension/parquet/parquet_reader.cpp]
[start of extension/parquet/parquet_statistics.cpp]
1: #include "parquet_statistics.hpp"
2: 
3: #include "duckdb.hpp"
4: #include "parquet_timestamp.hpp"
5: #ifndef DUCKDB_AMALGAMATION
6: #include "duckdb/common/types/value.hpp"
7: #include "duckdb/storage/statistics/numeric_statistics.hpp"
8: #include "duckdb/storage/statistics/string_statistics.hpp"
9: #endif
10: 
11: namespace duckdb {
12: 
13: using duckdb_parquet::format::ConvertedType;
14: using duckdb_parquet::format::Type;
15: 
16: template <Value (*FUNC)(const_data_ptr_t input)>
17: static unique_ptr<BaseStatistics> TemplatedGetNumericStats(const LogicalType &type,
18:                                                            const duckdb_parquet::format::Statistics &parquet_stats) {
19: 	auto stats = make_unique<NumericStatistics>(type);
20: 
21: 	// for reasons unknown to science, Parquet defines *both* `min` and `min_value` as well as `max` and
22: 	// `max_value`. All are optional. such elegance.
23: 	if (parquet_stats.__isset.min) {
24: 		stats->min = FUNC((const_data_ptr_t)parquet_stats.min.data());
25: 	} else if (parquet_stats.__isset.min_value) {
26: 		stats->min = FUNC((const_data_ptr_t)parquet_stats.min_value.data());
27: 	} else {
28: 		stats->min.is_null = true;
29: 	}
30: 	if (parquet_stats.__isset.max) {
31: 		stats->max = FUNC((const_data_ptr_t)parquet_stats.max.data());
32: 	} else if (parquet_stats.__isset.max_value) {
33: 		stats->max = FUNC((const_data_ptr_t)parquet_stats.max_value.data());
34: 	} else {
35: 		stats->max.is_null = true;
36: 	}
37: 	// GCC 4.x insists on a move() here
38: 	return move(stats);
39: }
40: 
41: template <class T>
42: static Value TransformStatisticsPlain(const_data_ptr_t input) {
43: 	return Value::CreateValue<T>(Load<T>(input));
44: }
45: 
46: static Value TransformStatisticsFloat(const_data_ptr_t input) {
47: 	auto val = Load<float>(input);
48: 	if (!Value::FloatIsValid(val)) {
49: 		return Value(LogicalType::FLOAT);
50: 	}
51: 	return Value::CreateValue<float>(val);
52: }
53: 
54: static Value TransformStatisticsDouble(const_data_ptr_t input) {
55: 	auto val = Load<double>(input);
56: 	if (!Value::DoubleIsValid(val)) {
57: 		return Value(LogicalType::DOUBLE);
58: 	}
59: 	return Value::CreateValue<double>(val);
60: }
61: 
62: static Value TransformStatisticsDate(const_data_ptr_t input) {
63: 	return Value::DATE(ParquetIntToDate(Load<int32_t>(input)));
64: }
65: 
66: static Value TransformStatisticsTimestampMs(const_data_ptr_t input) {
67: 	return Value::TIMESTAMP(ParquetTimestampMsToTimestamp(Load<int64_t>(input)));
68: }
69: 
70: static Value TransformStatisticsTimestampMicros(const_data_ptr_t input) {
71: 	return Value::TIMESTAMP(ParquetTimestampMicrosToTimestamp(Load<int64_t>(input)));
72: }
73: 
74: static Value TransformStatisticsTimestampImpala(const_data_ptr_t input) {
75: 	return Value::TIMESTAMP(ImpalaTimestampToTimestamp(Load<Int96>(input)));
76: }
77: 
78: unique_ptr<BaseStatistics> ParquetTransformColumnStatistics(const SchemaElement &s_ele, const LogicalType &type,
79:                                                             const ColumnChunk &column_chunk) {
80: 	if (!column_chunk.__isset.meta_data || !column_chunk.meta_data.__isset.statistics) {
81: 		// no stats present for row group
82: 		return nullptr;
83: 	}
84: 	auto &parquet_stats = column_chunk.meta_data.statistics;
85: 	unique_ptr<BaseStatistics> row_group_stats;
86: 
87: 	switch (type.id()) {
88: 
89: 	case LogicalTypeId::UTINYINT:
90: 		row_group_stats = TemplatedGetNumericStats<TransformStatisticsPlain<uint8_t>>(type, parquet_stats);
91: 		break;
92: 
93: 	case LogicalTypeId::USMALLINT:
94: 		row_group_stats = TemplatedGetNumericStats<TransformStatisticsPlain<uint16_t>>(type, parquet_stats);
95: 		break;
96: 
97: 	case LogicalTypeId::UINTEGER:
98: 		row_group_stats = TemplatedGetNumericStats<TransformStatisticsPlain<uint32_t>>(type, parquet_stats);
99: 		break;
100: 
101: 	case LogicalTypeId::UBIGINT:
102: 		row_group_stats = TemplatedGetNumericStats<TransformStatisticsPlain<uint64_t>>(type, parquet_stats);
103: 		break;
104: 	case LogicalTypeId::INTEGER:
105: 		row_group_stats = TemplatedGetNumericStats<TransformStatisticsPlain<int32_t>>(type, parquet_stats);
106: 		break;
107: 
108: 	case LogicalTypeId::BIGINT:
109: 		row_group_stats = TemplatedGetNumericStats<TransformStatisticsPlain<int64_t>>(type, parquet_stats);
110: 		break;
111: 
112: 	case LogicalTypeId::FLOAT:
113: 		row_group_stats = TemplatedGetNumericStats<TransformStatisticsFloat>(type, parquet_stats);
114: 		break;
115: 
116: 	case LogicalTypeId::DOUBLE:
117: 		row_group_stats = TemplatedGetNumericStats<TransformStatisticsDouble>(type, parquet_stats);
118: 		break;
119: 
120: 	case LogicalTypeId::DATE:
121: 		row_group_stats = TemplatedGetNumericStats<TransformStatisticsDate>(type, parquet_stats);
122: 		break;
123: 
124: 		// here we go, our favorite type
125: 	case LogicalTypeId::TIMESTAMP: {
126: 		switch (s_ele.type) {
127: 		case Type::INT64:
128: 			// arrow timestamp
129: 			switch (s_ele.converted_type) {
130: 			case ConvertedType::TIMESTAMP_MICROS:
131: 				row_group_stats = TemplatedGetNumericStats<TransformStatisticsTimestampMicros>(type, parquet_stats);
132: 				break;
133: 			case ConvertedType::TIMESTAMP_MILLIS:
134: 				row_group_stats = TemplatedGetNumericStats<TransformStatisticsTimestampMs>(type, parquet_stats);
135: 				break;
136: 			default:
137: 				return nullptr;
138: 			}
139: 			break;
140: 		case Type::INT96:
141: 			// impala timestamp
142: 			row_group_stats = TemplatedGetNumericStats<TransformStatisticsTimestampImpala>(type, parquet_stats);
143: 			break;
144: 		default:
145: 			return nullptr;
146: 		}
147: 		break;
148: 	}
149: 	case LogicalTypeId::VARCHAR: {
150: 		auto string_stats = make_unique<StringStatistics>(type);
151: 		if (parquet_stats.__isset.min) {
152: 			string_stats->Update(parquet_stats.min);
153: 		} else if (parquet_stats.__isset.min_value) {
154: 			string_stats->Update(parquet_stats.min_value);
155: 		} else {
156: 			return nullptr;
157: 		}
158: 		if (parquet_stats.__isset.max) {
159: 			string_stats->Update(parquet_stats.max);
160: 		} else if (parquet_stats.__isset.max_value) {
161: 			string_stats->Update(parquet_stats.max_value);
162: 		} else {
163: 			return nullptr;
164: 		}
165: 
166: 		string_stats->has_unicode = true; // we dont know better
167: 		row_group_stats = move(string_stats);
168: 		break;
169: 	}
170: 	default:
171: 		// no stats for you
172: 		break;
173: 	} // end of type switch
174: 
175: 	// null count is generic
176: 	if (row_group_stats) {
177: 		if (column_chunk.meta_data.type == duckdb_parquet::format::Type::FLOAT ||
178: 		    column_chunk.meta_data.type == duckdb_parquet::format::Type::DOUBLE) {
179: 			// floats/doubles can have infinity, which becomes NULL
180: 			row_group_stats->validity_stats = make_unique<ValidityStatistics>(true);
181: 		} else if (parquet_stats.__isset.null_count) {
182: 			row_group_stats->validity_stats = make_unique<ValidityStatistics>(parquet_stats.null_count != 0);
183: 		} else {
184: 			row_group_stats->validity_stats = make_unique<ValidityStatistics>(true);
185: 		}
186: 	} else {
187: 		// if stats are missing from any row group we know squat
188: 		return nullptr;
189: 	}
190: 
191: 	return row_group_stats;
192: }
193: 
194: } // namespace duckdb
[end of extension/parquet/parquet_statistics.cpp]
[start of extension/parquet/parquet_timestamp.cpp]
1: #include "parquet_timestamp.hpp"
2: 
3: #include "duckdb.hpp"
4: #ifndef DUCKDB_AMALGAMATION
5: #include "duckdb/common/types/date.hpp"
6: #include "duckdb/common/types/time.hpp"
7: #include "duckdb/common/types/timestamp.hpp"
8: #endif
9: 
10: namespace duckdb {
11: 
12: // surely they are joking
13: static constexpr int64_t JULIAN_TO_UNIX_EPOCH_DAYS = 2440588LL;
14: static constexpr int64_t MILLISECONDS_PER_DAY = 86400000LL;
15: static constexpr int64_t NANOSECONDS_PER_DAY = MILLISECONDS_PER_DAY * 1000LL * 1000LL;
16: 
17: int64_t ImpalaTimestampToNanoseconds(const Int96 &impala_timestamp) {
18: 	int64_t days_since_epoch = impala_timestamp.value[2] - JULIAN_TO_UNIX_EPOCH_DAYS;
19: 	auto nanoseconds = Load<int64_t>((data_ptr_t)impala_timestamp.value);
20: 	return days_since_epoch * NANOSECONDS_PER_DAY + nanoseconds;
21: }
22: 
23: timestamp_t ImpalaTimestampToTimestamp(const Int96 &raw_ts) {
24: 	auto impala_ns = ImpalaTimestampToNanoseconds(raw_ts);
25: 	return Timestamp::FromEpochMs(impala_ns / 1000000);
26: }
27: 
28: Int96 TimestampToImpalaTimestamp(timestamp_t &ts) {
29: 	int32_t hour, min, sec, msec;
30: 	Time::Convert(Timestamp::GetTime(ts), hour, min, sec, msec);
31: 	uint64_t ms_since_midnight = hour * 60 * 60 * 1000 + min * 60 * 1000 + sec * 1000 + msec;
32: 	auto days_since_epoch = Date::Epoch(Timestamp::GetDate(ts)) / (24 * 60 * 60);
33: 	// first two uint32 in Int96 are nanoseconds since midnights
34: 	// last uint32 is number of days since year 4713 BC ("Julian date")
35: 	Int96 impala_ts;
36: 	Store<uint64_t>(ms_since_midnight * 1000000, (data_ptr_t)impala_ts.value);
37: 	impala_ts.value[2] = days_since_epoch + JULIAN_TO_UNIX_EPOCH_DAYS;
38: 	return impala_ts;
39: }
40: 
41: timestamp_t ParquetTimestampMicrosToTimestamp(const int64_t &raw_ts) {
42: 	return Timestamp::FromEpochMicroSeconds(raw_ts);
43: }
44: timestamp_t ParquetTimestampMsToTimestamp(const int64_t &raw_ts) {
45: 	return Timestamp::FromEpochMs(raw_ts);
46: }
47: 
48: date_t ParquetIntToDate(const int32_t &raw_date) {
49: 	return date_t(raw_date);
50: }
51: 
52: } // namespace duckdb
[end of extension/parquet/parquet_timestamp.cpp]
[start of extension/parquet/parquet_writer.cpp]
1: #include "parquet_writer.hpp"
2: #include "parquet_timestamp.hpp"
3: 
4: #include "duckdb.hpp"
5: #ifndef DUCKDB_AMALGAMATION
6: #include "duckdb/function/table_function.hpp"
7: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
8: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
9: #include "duckdb/main/client_context.hpp"
10: #include "duckdb/main/connection.hpp"
11: #include "duckdb/common/file_system.hpp"
12: #include "duckdb/common/string_util.hpp"
13: #include "duckdb/common/serializer/buffered_file_writer.hpp"
14: #endif
15: 
16: namespace duckdb {
17: 
18: using namespace duckdb_apache::thrift;            // NOLINT
19: using namespace duckdb_apache::thrift::protocol;  // NOLINT
20: using namespace duckdb_apache::thrift::transport; // NOLINT
21: 
22: using duckdb_parquet::format::CompressionCodec;
23: using duckdb_parquet::format::ConvertedType;
24: using duckdb_parquet::format::Encoding;
25: using duckdb_parquet::format::FieldRepetitionType;
26: using duckdb_parquet::format::FileMetaData;
27: using duckdb_parquet::format::PageHeader;
28: using duckdb_parquet::format::PageType;
29: using ParquetRowGroup = duckdb_parquet::format::RowGroup;
30: using duckdb_parquet::format::Type;
31: 
32: class MyTransport : public TTransport {
33: public:
34: 	explicit MyTransport(Serializer &serializer) : serializer(serializer) {
35: 	}
36: 
37: 	bool isOpen() const override {
38: 		return true;
39: 	}
40: 
41: 	void open() override {
42: 	}
43: 
44: 	void close() override {
45: 	}
46: 
47: 	void write_virt(const uint8_t *buf, uint32_t len) override {
48: 		serializer.WriteData((const_data_ptr_t)buf, len);
49: 	}
50: 
51: private:
52: 	Serializer &serializer;
53: };
54: 
55: Type::type ParquetWriter::DuckDBTypeToParquetType(const LogicalType &duckdb_type) {
56: 	switch (duckdb_type.id()) {
57: 	case LogicalTypeId::BOOLEAN:
58: 		return Type::BOOLEAN;
59: 	case LogicalTypeId::TINYINT:
60: 	case LogicalTypeId::SMALLINT:
61: 	case LogicalTypeId::INTEGER:
62: 	case LogicalTypeId::DATE:
63: 		return Type::INT32;
64: 	case LogicalTypeId::BIGINT:
65: 		return Type::INT64;
66: 	case LogicalTypeId::FLOAT:
67: 		return Type::FLOAT;
68: 	case LogicalTypeId::DECIMAL: // for now...
69: 	case LogicalTypeId::DOUBLE:
70: 	case LogicalTypeId::HUGEINT:
71: 		return Type::DOUBLE;
72: 	case LogicalTypeId::VARCHAR:
73: 	case LogicalTypeId::BLOB:
74: 		return Type::BYTE_ARRAY;
75: 	case LogicalTypeId::TIMESTAMP:
76: 	case LogicalTypeId::TIMESTAMP_MS:
77: 	case LogicalTypeId::TIMESTAMP_NS:
78: 	case LogicalTypeId::TIMESTAMP_SEC:
79: 		return Type::INT64;
80: 	case LogicalTypeId::UTINYINT:
81: 	case LogicalTypeId::USMALLINT:
82: 	case LogicalTypeId::UINTEGER:
83: 		return Type::INT32;
84: 	case LogicalTypeId::UBIGINT:
85: 		return Type::INT64;
86: 	default:
87: 		throw NotImplementedException(duckdb_type.ToString());
88: 	}
89: }
90: 
91: bool ParquetWriter::DuckDBTypeToConvertedType(const LogicalType &duckdb_type, ConvertedType::type &result) {
92: 	switch (duckdb_type.id()) {
93: 	case LogicalTypeId::TINYINT:
94: 		result = ConvertedType::INT_8;
95: 		return true;
96: 	case LogicalTypeId::SMALLINT:
97: 		result = ConvertedType::INT_16;
98: 		return true;
99: 	case LogicalTypeId::INTEGER:
100: 		result = ConvertedType::INT_32;
101: 		return true;
102: 	case LogicalTypeId::BIGINT:
103: 		result = ConvertedType::INT_64;
104: 		return true;
105: 	case LogicalTypeId::UTINYINT:
106: 		result = ConvertedType::UINT_8;
107: 		return true;
108: 	case LogicalTypeId::USMALLINT:
109: 		result = ConvertedType::UINT_16;
110: 		return true;
111: 	case LogicalTypeId::UINTEGER:
112: 		result = ConvertedType::UINT_32;
113: 		return true;
114: 	case LogicalTypeId::UBIGINT:
115: 		result = ConvertedType::UINT_64;
116: 		return true;
117: 	case LogicalTypeId::DATE:
118: 		result = ConvertedType::DATE;
119: 		return true;
120: 	case LogicalTypeId::VARCHAR:
121: 		result = ConvertedType::UTF8;
122: 		return true;
123: 	case LogicalTypeId::TIMESTAMP:
124: 	case LogicalTypeId::TIMESTAMP_NS:
125: 	case LogicalTypeId::TIMESTAMP_SEC:
126: 		result = ConvertedType::TIMESTAMP_MICROS;
127: 		return true;
128: 	case LogicalTypeId::TIMESTAMP_MS:
129: 		result = ConvertedType::TIMESTAMP_MILLIS;
130: 		return true;
131: 	default:
132: 		return false;
133: 	}
134: }
135: 
136: ParquetWriter::ParquetWriter(FileSystem &fs, string file_name_p, FileOpener *file_opener_p, vector<LogicalType> types_p,
137:                              vector<string> names_p, CompressionCodec::type codec)
138:     : file_name(move(file_name_p)), sql_types(move(types_p)), column_names(move(names_p)), codec(codec) {
139: 	// initialize the file writer
140: 	writer = make_unique<BufferedFileWriter>(
141: 	    fs, file_name.c_str(), FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE_NEW, file_opener_p);
142: 	// parquet files start with the string "PAR1"
143: 	writer->WriteData((const_data_ptr_t) "PAR1", 4);
144: 	TCompactProtocolFactoryT<MyTransport> tproto_factory;
145: 	protocol = tproto_factory.getProtocol(make_shared<MyTransport>(*writer));
146: 
147: 	file_meta_data.num_rows = 0;
148: 	file_meta_data.version = 1;
149: 
150: 	file_meta_data.__isset.created_by = true;
151: 	file_meta_data.created_by = "DuckDB";
152: 
153: 	file_meta_data.schema.resize(1);
154: 
155: 	// populate root schema object
156: 	file_meta_data.schema[0].name = "duckdb_schema";
157: 	file_meta_data.schema[0].num_children = sql_types.size();
158: 	file_meta_data.schema[0].__isset.num_children = true;
159: 	file_meta_data.schema[0].repetition_type = duckdb_parquet::format::FieldRepetitionType::REQUIRED;
160: 	file_meta_data.schema[0].__isset.repetition_type = true;
161: 
162: 	for (idx_t i = 0; i < sql_types.size(); i++) {
163: 		column_writers.push_back(
164: 		    ColumnWriter::CreateWriterRecursive(file_meta_data.schema, *this, sql_types[i], column_names[i]));
165: 	}
166: }
167: 
168: void ParquetWriter::Flush(ChunkCollection &buffer) {
169: 	if (buffer.Count() == 0) {
170: 		return;
171: 	}
172: 	lock_guard<mutex> glock(lock);
173: 
174: 	// set up a new row group for this chunk collection
175: 	ParquetRowGroup row_group;
176: 	row_group.num_rows = buffer.Count();
177: 	row_group.file_offset = writer->GetTotalWritten();
178: 	row_group.__isset.file_offset = true;
179: 
180: 	// iterate over each of the columns of the chunk collection and write them
181: 	auto &chunks = buffer.Chunks();
182: 	D_ASSERT(buffer.ColumnCount() == column_writers.size());
183: 	for (idx_t col_idx = 0; col_idx < buffer.ColumnCount(); col_idx++) {
184: 		vector<string> schema_path;
185: 		auto write_state = column_writers[col_idx]->InitializeWriteState(row_group, move(schema_path));
186: 		for (idx_t chunk_idx = 0; chunk_idx < chunks.size(); chunk_idx++) {
187: 			column_writers[col_idx]->Prepare(*write_state, nullptr, chunks[chunk_idx]->data[col_idx],
188: 			                                 chunks[chunk_idx]->size());
189: 		}
190: 		column_writers[col_idx]->BeginWrite(*write_state);
191: 		for (idx_t chunk_idx = 0; chunk_idx < chunks.size(); chunk_idx++) {
192: 			column_writers[col_idx]->Write(*write_state, chunks[chunk_idx]->data[col_idx], chunks[chunk_idx]->size());
193: 		}
194: 		column_writers[col_idx]->FinalizeWrite(*write_state);
195: 	}
196: 
197: 	// append the row group to the file meta data
198: 	file_meta_data.row_groups.push_back(row_group);
199: 	file_meta_data.num_rows += buffer.Count();
200: }
201: 
202: void ParquetWriter::Finalize() {
203: 	auto start_offset = writer->GetTotalWritten();
204: 	file_meta_data.write(protocol.get());
205: 
206: 	writer->Write<uint32_t>(writer->GetTotalWritten() - start_offset);
207: 
208: 	// parquet files also end with the string "PAR1"
209: 	writer->WriteData((const_data_ptr_t) "PAR1", 4);
210: 
211: 	// flush to disk
212: 	writer->Sync();
213: 	writer.reset();
214: }
215: 
216: } // namespace duckdb
[end of extension/parquet/parquet_writer.cpp]
[start of src/function/scalar/uuid/gen_random.cpp]
1: #include "duckdb/function/scalar/uuid_functions.hpp"
2: #include "duckdb/main/client_context.hpp"
3: #include "duckdb/planner/expression/bound_function_expression.hpp"
4: #include "duckdb/common/types/uuid.hpp"
5: 
6: namespace duckdb {
7: 
8: struct UUIDRandomBindData : public FunctionData {
9: 	ClientContext &context;
10: 	std::uniform_int_distribution<uint32_t> dist;
11: 
12: 	UUIDRandomBindData(ClientContext &context, std::uniform_int_distribution<uint32_t> dist)
13: 	    : context(context), dist(dist) {
14: 	}
15: 
16: 	unique_ptr<FunctionData> Copy() override {
17: 		return make_unique<UUIDRandomBindData>(context, dist);
18: 	}
19: };
20: 
21: static unique_ptr<FunctionData> UUIDRandomBind(ClientContext &context, ScalarFunction &bound_function,
22:                                                vector<unique_ptr<Expression>> &arguments) {
23: 	std::uniform_int_distribution<uint32_t> dist;
24: 	return make_unique<UUIDRandomBindData>(context, dist);
25: }
26: 
27: static void GenerateUUIDFunction(DataChunk &args, ExpressionState &state, Vector &result) {
28: 	D_ASSERT(args.ColumnCount() == 0);
29: 	auto &func_expr = (BoundFunctionExpression &)state.expr;
30: 	auto &info = (UUIDRandomBindData &)*func_expr.bind_info;
31: 
32: 	result.SetVectorType(VectorType::FLAT_VECTOR);
33: 	auto result_data = FlatVector::GetData<hugeint_t>(result);
34: 	for (idx_t i = 0; i < args.size(); i++) {
35: 		uint8_t bytes[16];
36: 		for (int i = 0; i < 16; i += 4) {
37: 			*reinterpret_cast<uint32_t *>(bytes + i) = info.dist(info.context.random_engine);
38: 		}
39: 		// variant must be 10xxxxxx
40: 		bytes[8] &= 0xBF;
41: 		bytes[8] |= 0x80;
42: 		// version must be 0100xxxx
43: 		bytes[6] &= 0x4F;
44: 		bytes[6] |= 0x40;
45: 
46: 		result_data[i].upper = 0;
47: 		result_data[i].upper |= ((int64_t)bytes[0] << 56);
48: 		result_data[i].upper |= ((int64_t)bytes[1] << 48);
49: 		result_data[i].upper |= ((int64_t)bytes[3] << 40);
50: 		result_data[i].upper |= ((int64_t)bytes[4] << 32);
51: 		result_data[i].upper |= ((int64_t)bytes[5] << 24);
52: 		result_data[i].upper |= ((int64_t)bytes[6] << 16);
53: 		result_data[i].upper |= ((int64_t)bytes[7] << 8);
54: 		result_data[i].upper |= bytes[8];
55: 		result_data[i].lower = 0;
56: 		result_data[i].lower |= ((uint64_t)bytes[8] << 56);
57: 		result_data[i].lower |= ((uint64_t)bytes[9] << 48);
58: 		result_data[i].lower |= ((uint64_t)bytes[10] << 40);
59: 		result_data[i].lower |= ((uint64_t)bytes[11] << 32);
60: 		result_data[i].lower |= ((uint64_t)bytes[12] << 24);
61: 		result_data[i].lower |= ((uint64_t)bytes[13] << 16);
62: 		result_data[i].lower |= ((uint64_t)bytes[14] << 8);
63: 		result_data[i].lower |= bytes[15];
64: 	}
65: }
66: 
67: void UUIDFun::RegisterFunction(BuiltinFunctions &set) {
68: 	// generate a random uuid
69: 	set.AddFunction(
70: 	    ScalarFunction("gen_random_uuid", {}, LogicalType::UUID, GenerateUUIDFunction, true, UUIDRandomBind));
71: }
72: 
73: } // namespace duckdb
[end of src/function/scalar/uuid/gen_random.cpp]
[start of src/main/client_context.cpp]
1: #include "duckdb/main/client_context.hpp"
2: 
3: #include "duckdb/main/client_context_file_opener.hpp"
4: #include "duckdb/main/query_profiler.hpp"
5: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
6: #include "duckdb/catalog/catalog_entry/scalar_function_catalog_entry.hpp"
7: #include "duckdb/catalog/catalog_search_path.hpp"
8: #include "duckdb/common/serializer/buffered_deserializer.hpp"
9: #include "duckdb/common/serializer/buffered_serializer.hpp"
10: #include "duckdb/execution/physical_plan_generator.hpp"
11: #include "duckdb/main/database.hpp"
12: #include "duckdb/main/materialized_query_result.hpp"
13: #include "duckdb/main/query_result.hpp"
14: #include "duckdb/main/stream_query_result.hpp"
15: #include "duckdb/optimizer/optimizer.hpp"
16: #include "duckdb/parser/parser.hpp"
17: #include "duckdb/parser/expression/constant_expression.hpp"
18: #include "duckdb/parser/parsed_data/create_function_info.hpp"
19: #include "duckdb/parser/statement/drop_statement.hpp"
20: #include "duckdb/parser/statement/execute_statement.hpp"
21: #include "duckdb/parser/statement/explain_statement.hpp"
22: #include "duckdb/parser/statement/prepare_statement.hpp"
23: #include "duckdb/parser/statement/select_statement.hpp"
24: #include "duckdb/planner/operator/logical_execute.hpp"
25: #include "duckdb/planner/planner.hpp"
26: #include "duckdb/transaction/transaction_manager.hpp"
27: #include "duckdb/transaction/transaction.hpp"
28: #include "duckdb/storage/data_table.hpp"
29: #include "duckdb/main/appender.hpp"
30: #include "duckdb/main/relation.hpp"
31: #include "duckdb/parser/statement/relation_statement.hpp"
32: #include "duckdb/parallel/task_scheduler.hpp"
33: #include "duckdb/common/serializer/buffered_file_writer.hpp"
34: #include "duckdb/planner/pragma_handler.hpp"
35: #include "duckdb/common/to_string.hpp"
36: #include "duckdb/common/file_system.hpp"
37: #include "duckdb/execution/column_binding_resolver.hpp"
38: 
39: namespace duckdb {
40: 
41: struct ActiveQueryContext {
42: 	//! The query that is currently being executed
43: 	string query;
44: 	//! The currently open result
45: 	BaseQueryResult *open_result = nullptr;
46: 	//! Prepared statement data
47: 	shared_ptr<PreparedStatementData> prepared;
48: 	//! The query executor
49: 	unique_ptr<Executor> executor;
50: 	//! The progress bar
51: 	unique_ptr<ProgressBar> progress_bar;
52: };
53: 
54: ClientContext::ClientContext(shared_ptr<DatabaseInstance> database)
55:     : profiler(make_shared<QueryProfiler>(*this)), query_profiler_history(make_unique<QueryProfilerHistory>()),
56:       db(move(database)), transaction(db->GetTransactionManager(), *this), interrupted(false),
57:       temporary_objects(make_unique<SchemaCatalogEntry>(&db->GetCatalog(), TEMP_SCHEMA, true)),
58:       catalog_search_path(make_unique<CatalogSearchPath>(*this)),
59:       file_opener(make_unique<ClientContextFileOpener>(*this)) {
60: 	std::random_device rd;
61: 	random_engine.seed(rd());
62: }
63: 
64: ClientContext::~ClientContext() {
65: 	if (Exception::UncaughtException()) {
66: 		return;
67: 	}
68: 	// destroy the client context and rollback if there is an active transaction
69: 	// but only if we are not destroying this client context as part of an exception stack unwind
70: 	Destroy();
71: }
72: 
73: unique_ptr<ClientContextLock> ClientContext::LockContext() {
74: 	return make_unique<ClientContextLock>(context_lock);
75: }
76: 
77: void ClientContext::Destroy() {
78: 	auto lock = LockContext();
79: 	if (transaction.HasActiveTransaction()) {
80: 		ActiveTransaction().active_query = MAXIMUM_QUERY_ID;
81: 		if (!transaction.IsAutoCommit()) {
82: 			transaction.Rollback();
83: 		}
84: 	}
85: 	CleanupInternal(*lock);
86: }
87: 
88: unique_ptr<DataChunk> ClientContext::Fetch(ClientContextLock &lock, StreamQueryResult &result) {
89: 	D_ASSERT(IsActiveResult(lock, &result));
90: 	D_ASSERT(active_query->executor);
91: 	return FetchInternal(lock, *active_query->executor, result);
92: }
93: 
94: unique_ptr<DataChunk> ClientContext::FetchInternal(ClientContextLock &lock, Executor &executor,
95:                                                    BaseQueryResult &result) {
96: 	bool invalidate_query = true;
97: 	try {
98: 		// fetch the chunk and return it
99: 		auto chunk = executor.FetchChunk();
100: 		if (!chunk || chunk->size() == 0) {
101: 			CleanupInternal(lock, &result);
102: 		}
103: 		return chunk;
104: 	} catch (StandardException &ex) {
105: 		// standard exceptions do not invalidate the current transaction
106: 		result.error = ex.what();
107: 		invalidate_query = false;
108: 	} catch (std::exception &ex) {
109: 		result.error = ex.what();
110: 	} catch (...) { // LCOV_EXCL_START
111: 		result.error = "Unhandled exception in FetchInternal";
112: 	} // LCOV_EXCL_STOP
113: 	result.success = false;
114: 	CleanupInternal(lock, &result, invalidate_query);
115: 	return nullptr;
116: }
117: 
118: void ClientContext::BeginTransactionInternal(ClientContextLock &lock, bool requires_valid_transaction) {
119: 	// check if we are on AutoCommit. In this case we should start a transaction
120: 	D_ASSERT(!active_query);
121: 	if (requires_valid_transaction && transaction.HasActiveTransaction() &&
122: 	    transaction.ActiveTransaction().IsInvalidated()) {
123: 		throw Exception("Failed: transaction has been invalidated!");
124: 	}
125: 	active_query = make_unique<ActiveQueryContext>();
126: 	if (transaction.IsAutoCommit()) {
127: 		transaction.BeginTransaction();
128: 	}
129: }
130: 
131: void ClientContext::BeginQueryInternal(ClientContextLock &lock, const string &query) {
132: 	BeginTransactionInternal(lock, false);
133: 	LogQueryInternal(lock, query);
134: 	active_query->query = query;
135: 	query_progress = -1;
136: 	ActiveTransaction().active_query = db->GetTransactionManager().GetQueryNumber();
137: }
138: 
139: string ClientContext::EndQueryInternal(ClientContextLock &lock, bool success, bool invalidate_transaction) {
140: 	profiler->EndQuery();
141: 
142: 	D_ASSERT(active_query.get());
143: 	string error;
144: 	try {
145: 		if (transaction.HasActiveTransaction()) {
146: 			// Move the query profiler into the history
147: 			auto &prev_profilers = query_profiler_history->GetPrevProfilers();
148: 			prev_profilers.emplace_back(transaction.ActiveTransaction().active_query, move(profiler));
149: 			// Reinitialize the query profiler
150: 			profiler = make_shared<QueryProfiler>(*this);
151: 			// Propagate settings of the saved query into the new profiler.
152: 			profiler->Propagate(*prev_profilers.back().second);
153: 			if (prev_profilers.size() >= query_profiler_history->GetPrevProfilersSize()) {
154: 				prev_profilers.pop_front();
155: 			}
156: 
157: 			ActiveTransaction().active_query = MAXIMUM_QUERY_ID;
158: 			if (transaction.IsAutoCommit()) {
159: 				if (success) {
160: 					transaction.Commit();
161: 				} else {
162: 					transaction.Rollback();
163: 				}
164: 			} else if (invalidate_transaction) {
165: 				D_ASSERT(!success);
166: 				ActiveTransaction().Invalidate();
167: 			}
168: 		}
169: 	} catch (std::exception &ex) {
170: 		error = ex.what();
171: 	} catch (...) { // LCOV_EXCL_START
172: 		error = "Unhandled exception!";
173: 	} // LCOV_EXCL_STOP
174: 	active_query.reset();
175: 	query_progress = -1;
176: 	return error;
177: }
178: 
179: void ClientContext::CleanupInternal(ClientContextLock &lock, BaseQueryResult *result, bool invalidate_transaction) {
180: 	if (!active_query) {
181: 		// no query currently active
182: 		return;
183: 	}
184: 	if (active_query->executor) {
185: 		active_query->executor->CancelTasks();
186: 	}
187: 	active_query->progress_bar.reset();
188: 
189: 	auto error = EndQueryInternal(lock, result ? result->success : false, invalidate_transaction);
190: 	if (result && result->success) {
191: 		// if an error occurred while committing report it in the result
192: 		result->error = error;
193: 		result->success = error.empty();
194: 	}
195: 	D_ASSERT(!active_query);
196: }
197: 
198: Executor &ClientContext::GetExecutor() {
199: 	D_ASSERT(active_query);
200: 	D_ASSERT(active_query->executor);
201: 	return *active_query->executor;
202: }
203: 
204: const string &ClientContext::GetCurrentQuery() {
205: 	D_ASSERT(active_query);
206: 	return active_query->query;
207: }
208: 
209: unique_ptr<QueryResult> ClientContext::FetchResultInternal(ClientContextLock &lock, PendingQueryResult &pending,
210:                                                            bool allow_stream_result) {
211: 	D_ASSERT(active_query);
212: 	D_ASSERT(active_query->open_result == &pending);
213: 	D_ASSERT(active_query->prepared);
214: 	auto &prepared = *active_query->prepared;
215: 	bool create_stream_result = prepared.allow_stream_result && allow_stream_result;
216: 	if (create_stream_result) {
217: 		active_query->progress_bar.reset();
218: 		query_progress = -1;
219: 
220: 		// successfully compiled SELECT clause and it is the last statement
221: 		// return a StreamQueryResult so the client can call Fetch() on it and stream the result
222: 		auto stream_result =
223: 		    make_unique<StreamQueryResult>(pending.statement_type, shared_from_this(), pending.types, pending.names);
224: 		active_query->open_result = stream_result.get();
225: 		return move(stream_result);
226: 	}
227: 	// create a materialized result by continuously fetching
228: 	auto result = make_unique<MaterializedQueryResult>(pending.statement_type, pending.types, pending.names);
229: 	while (true) {
230: 		auto chunk = FetchInternal(lock, GetExecutor(), *result);
231: 		if (!chunk || chunk->size() == 0) {
232: 			break;
233: 		}
234: #ifdef DEBUG
235: 		for (idx_t i = 0; i < chunk->ColumnCount(); i++) {
236: 			if (pending.types[i].id() == LogicalTypeId::VARCHAR) {
237: 				chunk->data[i].UTFVerify(chunk->size());
238: 			}
239: 		}
240: #endif
241: 		result->collection.Append(*chunk);
242: 	}
243: 	return move(result);
244: }
245: 
246: shared_ptr<PreparedStatementData> ClientContext::CreatePreparedStatement(ClientContextLock &lock, const string &query,
247:                                                                          unique_ptr<SQLStatement> statement) {
248: 	StatementType statement_type = statement->type;
249: 	auto result = make_shared<PreparedStatementData>(statement_type);
250: 
251: 	auto &profiler = QueryProfiler::Get(*this);
252: 	profiler.StartPhase("planner");
253: 	Planner planner(*this);
254: 	planner.CreatePlan(move(statement));
255: 	D_ASSERT(planner.plan);
256: 	profiler.EndPhase();
257: 
258: 	auto plan = move(planner.plan);
259: #ifdef DEBUG
260: 	plan->Verify();
261: #endif
262: 	// extract the result column names from the plan
263: 	result->read_only = planner.read_only;
264: 	result->requires_valid_transaction = planner.requires_valid_transaction;
265: 	result->allow_stream_result = planner.allow_stream_result;
266: 	result->names = planner.names;
267: 	result->types = planner.types;
268: 	result->value_map = move(planner.value_map);
269: 	result->catalog_version = Transaction::GetTransaction(*this).catalog_version;
270: 
271: 	if (config.enable_optimizer) {
272: 		profiler.StartPhase("optimizer");
273: 		Optimizer optimizer(*planner.binder, *this);
274: 		plan = optimizer.Optimize(move(plan));
275: 		D_ASSERT(plan);
276: 		profiler.EndPhase();
277: 
278: #ifdef DEBUG
279: 		plan->Verify();
280: #endif
281: 	}
282: 
283: 	profiler.StartPhase("physical_planner");
284: 	// now convert logical query plan into a physical query plan
285: 	PhysicalPlanGenerator physical_planner(*this);
286: 	auto physical_plan = physical_planner.CreatePlan(move(plan));
287: 	profiler.EndPhase();
288: 
289: #ifdef DEBUG
290: 	D_ASSERT(!physical_plan->ToString().empty());
291: #endif
292: 	result->plan = move(physical_plan);
293: 	return result;
294: }
295: 
296: double ClientContext::GetProgress() {
297: 	return query_progress.load();
298: }
299: 
300: unique_ptr<PendingQueryResult> ClientContext::PendingPreparedStatement(ClientContextLock &lock,
301:                                                                        shared_ptr<PreparedStatementData> statement_p,
302:                                                                        vector<Value> bound_values) {
303: 	D_ASSERT(active_query);
304: 	auto &statement = *statement_p;
305: 	if (ActiveTransaction().IsInvalidated() && statement.requires_valid_transaction) {
306: 		throw Exception("Current transaction is aborted (please ROLLBACK)");
307: 	}
308: 	auto &db_config = DBConfig::GetConfig(*this);
309: 	if (db_config.access_mode == AccessMode::READ_ONLY && !statement.read_only) {
310: 		throw Exception(StringUtil::Format("Cannot execute statement of type \"%s\" in read-only mode!",
311: 		                                   StatementTypeToString(statement.statement_type)));
312: 	}
313: 
314: 	// bind the bound values before execution
315: 	statement.Bind(move(bound_values));
316: 
317: 	active_query->executor = make_unique<Executor>(*this);
318: 	auto &executor = *active_query->executor;
319: 	if (config.enable_progress_bar) {
320: 		active_query->progress_bar = make_unique<ProgressBar>(executor, config.wait_time);
321: 		active_query->progress_bar->Start();
322: 		query_progress = 0;
323: 	}
324: 	executor.Initialize(statement.plan.get());
325: 	auto types = executor.GetTypes();
326: 	D_ASSERT(types == statement.types);
327: 	D_ASSERT(!active_query->open_result);
328: 
329: 	auto pending_result = make_unique<PendingQueryResult>(shared_from_this(), *statement_p, move(types));
330: 	active_query->prepared = move(statement_p);
331: 	active_query->open_result = pending_result.get();
332: 	return pending_result;
333: }
334: 
335: PendingExecutionResult ClientContext::ExecuteTaskInternal(ClientContextLock &lock, PendingQueryResult &result) {
336: 	D_ASSERT(active_query);
337: 	D_ASSERT(active_query->open_result == &result);
338: 	try {
339: 		auto result = active_query->executor->ExecuteTask();
340: 		if (active_query->progress_bar) {
341: 			active_query->progress_bar->Update(result == PendingExecutionResult::RESULT_READY);
342: 			query_progress = active_query->progress_bar->GetCurrentPercentage();
343: 		}
344: 		return result;
345: 	} catch (std::exception &ex) {
346: 		result.error = ex.what();
347: 	} catch (...) { // LCOV_EXCL_START
348: 		result.error = "Unhandled exception in ExecuteTaskInternal";
349: 	} // LCOV_EXCL_STOP
350: 	EndQueryInternal(lock, false, true);
351: 	result.success = false;
352: 	return PendingExecutionResult::EXECUTION_ERROR;
353: }
354: 
355: void ClientContext::InitialCleanup(ClientContextLock &lock) {
356: 	//! Cleanup any open results and reset the interrupted flag
357: 	CleanupInternal(lock);
358: 	interrupted = false;
359: }
360: 
361: vector<unique_ptr<SQLStatement>> ClientContext::ParseStatements(const string &query) {
362: 	auto lock = LockContext();
363: 	return ParseStatementsInternal(*lock, query);
364: }
365: 
366: vector<unique_ptr<SQLStatement>> ClientContext::ParseStatementsInternal(ClientContextLock &lock, const string &query) {
367: 	Parser parser;
368: 	parser.ParseQuery(query);
369: 
370: 	PragmaHandler handler(*this);
371: 	handler.HandlePragmaStatements(lock, parser.statements);
372: 
373: 	return move(parser.statements);
374: }
375: 
376: void ClientContext::HandlePragmaStatements(vector<unique_ptr<SQLStatement>> &statements) {
377: 	auto lock = LockContext();
378: 
379: 	PragmaHandler handler(*this);
380: 	handler.HandlePragmaStatements(*lock, statements);
381: }
382: 
383: unique_ptr<LogicalOperator> ClientContext::ExtractPlan(const string &query) {
384: 	auto lock = LockContext();
385: 
386: 	auto statements = ParseStatementsInternal(*lock, query);
387: 	if (statements.size() != 1) {
388: 		throw Exception("ExtractPlan can only prepare a single statement");
389: 	}
390: 
391: 	unique_ptr<LogicalOperator> plan;
392: 	RunFunctionInTransactionInternal(*lock, [&]() {
393: 		Planner planner(*this);
394: 		planner.CreatePlan(move(statements[0]));
395: 		D_ASSERT(planner.plan);
396: 
397: 		plan = move(planner.plan);
398: 
399: 		if (config.enable_optimizer) {
400: 			Optimizer optimizer(*planner.binder, *this);
401: 			plan = optimizer.Optimize(move(plan));
402: 		}
403: 
404: 		ColumnBindingResolver resolver;
405: 		resolver.VisitOperator(*plan);
406: 
407: 		plan->ResolveOperatorTypes();
408: 	});
409: 	return plan;
410: }
411: 
412: unique_ptr<PreparedStatement> ClientContext::PrepareInternal(ClientContextLock &lock,
413:                                                              unique_ptr<SQLStatement> statement) {
414: 	auto n_param = statement->n_param;
415: 	auto statement_query = statement->query;
416: 	shared_ptr<PreparedStatementData> prepared_data;
417: 	auto unbound_statement = statement->Copy();
418: 	RunFunctionInTransactionInternal(
419: 	    lock, [&]() { prepared_data = CreatePreparedStatement(lock, statement_query, move(statement)); }, false);
420: 	prepared_data->unbound_statement = move(unbound_statement);
421: 	return make_unique<PreparedStatement>(shared_from_this(), move(prepared_data), move(statement_query), n_param);
422: }
423: 
424: unique_ptr<PreparedStatement> ClientContext::Prepare(unique_ptr<SQLStatement> statement) {
425: 	auto lock = LockContext();
426: 	// prepare the query
427: 	try {
428: 		InitialCleanup(*lock);
429: 		return PrepareInternal(*lock, move(statement));
430: 	} catch (std::exception &ex) {
431: 		return make_unique<PreparedStatement>(ex.what());
432: 	}
433: }
434: 
435: unique_ptr<PreparedStatement> ClientContext::Prepare(const string &query) {
436: 	auto lock = LockContext();
437: 	// prepare the query
438: 	try {
439: 		InitialCleanup(*lock);
440: 
441: 		// first parse the query
442: 		auto statements = ParseStatementsInternal(*lock, query);
443: 		if (statements.empty()) {
444: 			throw Exception("No statement to prepare!");
445: 		}
446: 		if (statements.size() > 1) {
447: 			throw Exception("Cannot prepare multiple statements at once!");
448: 		}
449: 		return PrepareInternal(*lock, move(statements[0]));
450: 	} catch (std::exception &ex) {
451: 		return make_unique<PreparedStatement>(ex.what());
452: 	}
453: }
454: 
455: unique_ptr<PendingQueryResult> ClientContext::PendingQueryPreparedInternal(ClientContextLock &lock, const string &query,
456:                                                                            shared_ptr<PreparedStatementData> &prepared,
457:                                                                            vector<Value> &values) {
458: 	try {
459: 		InitialCleanup(lock);
460: 	} catch (std::exception &ex) {
461: 		return make_unique<PendingQueryResult>(ex.what());
462: 	}
463: 	return PendingStatementOrPreparedStatementInternal(lock, query, nullptr, prepared, &values);
464: }
465: 
466: unique_ptr<PendingQueryResult>
467: ClientContext::PendingQuery(const string &query, shared_ptr<PreparedStatementData> &prepared, vector<Value> &values) {
468: 	auto lock = LockContext();
469: 	return PendingQueryPreparedInternal(*lock, query, prepared, values);
470: }
471: 
472: unique_ptr<QueryResult> ClientContext::Execute(const string &query, shared_ptr<PreparedStatementData> &prepared,
473:                                                vector<Value> &values, bool allow_stream_result) {
474: 	auto lock = LockContext();
475: 	auto pending = PendingQueryPreparedInternal(*lock, query, prepared, values);
476: 	if (!pending->success) {
477: 		return make_unique<MaterializedQueryResult>(pending->error);
478: 	}
479: 	return pending->ExecuteInternal(*lock, allow_stream_result);
480: }
481: 
482: unique_ptr<PendingQueryResult> ClientContext::PendingStatementInternal(ClientContextLock &lock, const string &query,
483:                                                                        unique_ptr<SQLStatement> statement) {
484: 	// prepare the query for execution
485: 	auto prepared = CreatePreparedStatement(lock, query, move(statement));
486: 	// by default, no values are bound
487: 	vector<Value> bound_values;
488: 	// execute the prepared statement
489: 	return PendingPreparedStatement(lock, move(prepared), move(bound_values));
490: }
491: 
492: unique_ptr<QueryResult> ClientContext::RunStatementInternal(ClientContextLock &lock, const string &query,
493:                                                             unique_ptr<SQLStatement> statement,
494:                                                             bool allow_stream_result, bool verify) {
495: 	auto pending = PendingQueryInternal(lock, move(statement), verify);
496: 	if (!pending->success) {
497: 		return make_unique<MaterializedQueryResult>(move(pending->error));
498: 	}
499: 	return ExecutePendingQueryInternal(lock, *pending, allow_stream_result);
500: }
501: 
502: bool ClientContext::IsActiveResult(ClientContextLock &lock, BaseQueryResult *result) {
503: 	if (!active_query) {
504: 		return false;
505: 	}
506: 	return active_query->open_result == result;
507: }
508: 
509: static bool IsExplainAnalyze(SQLStatement *statement) {
510: 	if (!statement) {
511: 		return false;
512: 	}
513: 	if (statement->type != StatementType::EXPLAIN_STATEMENT) {
514: 		return false;
515: 	}
516: 	auto &explain = (ExplainStatement &)*statement;
517: 	return explain.explain_type == ExplainType::EXPLAIN_ANALYZE;
518: }
519: 
520: unique_ptr<PendingQueryResult> ClientContext::PendingStatementOrPreparedStatementInternal(
521:     ClientContextLock &lock, const string &query, unique_ptr<SQLStatement> statement,
522:     shared_ptr<PreparedStatementData> &prepared, vector<Value> *values) {
523: 	// check if we are on AutoCommit. In this case we should start a transaction.
524: 	if (statement && config.query_verification_enabled) {
525: 		// query verification is enabled
526: 		// create a copy of the statement, and use the copy
527: 		// this way we verify that the copy correctly copies all properties
528: 		auto copied_statement = statement->Copy();
529: 		if (statement->type == StatementType::SELECT_STATEMENT) {
530: 			// in case this is a select query, we verify the original statement
531: 			string error = VerifyQuery(lock, query, move(statement));
532: 			if (!error.empty()) {
533: 				// error in verifying query
534: 				return make_unique<PendingQueryResult>(error);
535: 			}
536: 		}
537: 		statement = move(copied_statement);
538: 	}
539: 	return PendingStatementOrPreparedStatement(lock, query, move(statement), prepared, values);
540: }
541: 
542: unique_ptr<PendingQueryResult>
543: ClientContext::PendingStatementOrPreparedStatement(ClientContextLock &lock, const string &query,
544:                                                    unique_ptr<SQLStatement> statement,
545:                                                    shared_ptr<PreparedStatementData> &prepared, vector<Value> *values) {
546: 	unique_ptr<PendingQueryResult> result;
547: 
548: 	BeginQueryInternal(lock, query);
549: 	// start the profiler
550: 	auto &profiler = QueryProfiler::Get(*this);
551: 	profiler.StartQuery(query, IsExplainAnalyze(statement ? statement.get() : prepared->unbound_statement.get()));
552: 	bool invalidate_query = true;
553: 	try {
554: 		if (statement) {
555: 			result = PendingStatementInternal(lock, query, move(statement));
556: 		} else {
557: 			auto &catalog = Catalog::GetCatalog(*this);
558: 			if (prepared->unbound_statement && catalog.GetCatalogVersion() != prepared->catalog_version) {
559: 				D_ASSERT(prepared->unbound_statement.get());
560: 				// catalog was modified: rebind the statement before execution
561: 				auto new_prepared = CreatePreparedStatement(lock, query, prepared->unbound_statement->Copy());
562: 				if (prepared->types != new_prepared->types) {
563: 					throw BinderException("Rebinding statement after catalog change resulted in change of types");
564: 				}
565: 				new_prepared->unbound_statement = move(prepared->unbound_statement);
566: 				prepared = move(new_prepared);
567: 			}
568: 			result = PendingPreparedStatement(lock, prepared, *values);
569: 		}
570: 	} catch (StandardException &ex) {
571: 		// standard exceptions do not invalidate the current transaction
572: 		result = make_unique<PendingQueryResult>(ex.what());
573: 		invalidate_query = false;
574: 	} catch (std::exception &ex) {
575: 		// other types of exceptions do invalidate the current transaction
576: 		result = make_unique<PendingQueryResult>(ex.what());
577: 	}
578: 	if (!result->success) {
579: 		// query failed: abort now
580: 		EndQueryInternal(lock, false, invalidate_query);
581: 		return result;
582: 	}
583: 	D_ASSERT(active_query->open_result == result.get());
584: 	return result;
585: }
586: 
587: void ClientContext::LogQueryInternal(ClientContextLock &, const string &query) {
588: 	if (!log_query_writer) {
589: #ifdef DUCKDB_FORCE_QUERY_LOG
590: 		try {
591: 			string log_path(DUCKDB_FORCE_QUERY_LOG);
592: 			log_query_writer = make_unique<BufferedFileWriter>(
593: 			    FileSystem::GetFileSystem(*this), log_path, BufferedFileWriter::DEFAULT_OPEN_FLAGS, file_opener.get());
594: 		} catch (...) {
595: 			return;
596: 		}
597: #else
598: 		return;
599: #endif
600: 	}
601: 	// log query path is set: log the query
602: 	log_query_writer->WriteData((const_data_ptr_t)query.c_str(), query.size());
603: 	log_query_writer->WriteData((const_data_ptr_t) "\n", 1);
604: 	log_query_writer->Flush();
605: 	log_query_writer->Sync();
606: }
607: 
608: unique_ptr<QueryResult> ClientContext::Query(unique_ptr<SQLStatement> statement, bool allow_stream_result) {
609: 	auto pending_query = PendingQuery(move(statement));
610: 	return pending_query->Execute(allow_stream_result);
611: }
612: 
613: unique_ptr<QueryResult> ClientContext::Query(const string &query, bool allow_stream_result) {
614: 	auto lock = LockContext();
615: 
616: 	string error;
617: 	vector<unique_ptr<SQLStatement>> statements;
618: 	if (!ParseStatements(*lock, query, statements, error)) {
619: 		return make_unique<MaterializedQueryResult>(move(error));
620: 	}
621: 	if (statements.empty()) {
622: 		// no statements, return empty successful result
623: 		return make_unique<MaterializedQueryResult>(StatementType::INVALID_STATEMENT);
624: 	}
625: 
626: 	unique_ptr<QueryResult> result;
627: 	QueryResult *last_result = nullptr;
628: 	for (idx_t i = 0; i < statements.size(); i++) {
629: 		auto &statement = statements[i];
630: 		bool is_last_statement = i + 1 == statements.size();
631: 		bool stream_result = allow_stream_result && is_last_statement;
632: 		auto pending_query = PendingQueryInternal(*lock, move(statement));
633: 		unique_ptr<QueryResult> current_result;
634: 		if (!pending_query->success) {
635: 			current_result = make_unique<MaterializedQueryResult>(pending_query->error);
636: 		} else {
637: 			current_result = ExecutePendingQueryInternal(*lock, *pending_query, stream_result);
638: 		}
639: 		// now append the result to the list of results
640: 		if (!last_result) {
641: 			// first result of the query
642: 			result = move(current_result);
643: 			last_result = result.get();
644: 		} else {
645: 			// later results; attach to the result chain
646: 			last_result->next = move(current_result);
647: 			last_result = last_result->next.get();
648: 		}
649: 	}
650: 	return result;
651: }
652: 
653: bool ClientContext::ParseStatements(ClientContextLock &lock, const string &query,
654:                                     vector<unique_ptr<SQLStatement>> &result, string &error) {
655: 	try {
656: 		InitialCleanup(lock);
657: 		// parse the query and transform it into a set of statements
658: 		result = ParseStatementsInternal(lock, query);
659: 		return true;
660: 	} catch (std::exception &ex) {
661: 		error = ex.what();
662: 		return false;
663: 	}
664: }
665: 
666: unique_ptr<PendingQueryResult> ClientContext::PendingQuery(const string &query) {
667: 	auto lock = LockContext();
668: 
669: 	string error;
670: 	vector<unique_ptr<SQLStatement>> statements;
671: 	if (!ParseStatements(*lock, query, statements, error)) {
672: 		return make_unique<PendingQueryResult>(move(error));
673: 	}
674: 	if (statements.size() != 1) {
675: 		return make_unique<PendingQueryResult>("PendingQuery can only take a single statement");
676: 	}
677: 	return PendingQueryInternal(*lock, move(statements[0]));
678: }
679: 
680: unique_ptr<PendingQueryResult> ClientContext::PendingQuery(unique_ptr<SQLStatement> statement) {
681: 	auto lock = LockContext();
682: 	return PendingQueryInternal(*lock, move(statement));
683: }
684: 
685: unique_ptr<PendingQueryResult> ClientContext::PendingQueryInternal(ClientContextLock &lock,
686:                                                                    unique_ptr<SQLStatement> statement, bool verify) {
687: 	auto query = statement->query;
688: 	shared_ptr<PreparedStatementData> prepared;
689: 	if (verify) {
690: 		return PendingStatementOrPreparedStatementInternal(lock, query, move(statement), prepared, nullptr);
691: 	} else {
692: 		return PendingStatementOrPreparedStatement(lock, query, move(statement), prepared, nullptr);
693: 	}
694: }
695: 
696: unique_ptr<QueryResult> ClientContext::ExecutePendingQueryInternal(ClientContextLock &lock, PendingQueryResult &query,
697:                                                                    bool allow_stream_result) {
698: 	return query.ExecuteInternal(lock, allow_stream_result);
699: }
700: 
701: void ClientContext::Interrupt() {
702: 	interrupted = true;
703: }
704: 
705: void ClientContext::EnableProfiling() {
706: 	auto lock = LockContext();
707: 	auto &config = ClientConfig::GetConfig(*this);
708: 	config.enable_profiler = true;
709: }
710: 
711: void ClientContext::DisableProfiling() {
712: 	auto lock = LockContext();
713: 	auto &config = ClientConfig::GetConfig(*this);
714: 	config.enable_profiler = false;
715: }
716: 
717: string ClientContext::VerifyQuery(ClientContextLock &lock, const string &query, unique_ptr<SQLStatement> statement) {
718: 	D_ASSERT(statement->type == StatementType::SELECT_STATEMENT);
719: 	// aggressive query verification
720: 
721: 	// the purpose of this function is to test correctness of otherwise hard to test features:
722: 	// Copy() of statements and expressions
723: 	// Serialize()/Deserialize() of expressions
724: 	// Hash() of expressions
725: 	// Equality() of statements and expressions
726: 	// Correctness of plans both with and without optimizers
727: 	// Correctness of plans both with and without parallelism
728: 
729: 	// copy the statement
730: 	auto select_stmt = (SelectStatement *)statement.get();
731: 	auto copied_stmt = unique_ptr_cast<SQLStatement, SelectStatement>(select_stmt->Copy());
732: 	auto unoptimized_stmt = unique_ptr_cast<SQLStatement, SelectStatement>(select_stmt->Copy());
733: 
734: 	BufferedSerializer serializer;
735: 	select_stmt->Serialize(serializer);
736: 	BufferedDeserializer source(serializer);
737: 	auto deserialized_stmt = SelectStatement::Deserialize(source);
738: 	// all the statements should be equal
739: 	D_ASSERT(copied_stmt->Equals(statement.get()));
740: 	D_ASSERT(deserialized_stmt->Equals(statement.get()));
741: 	D_ASSERT(copied_stmt->Equals(deserialized_stmt.get()));
742: 
743: 	// now perform checking on the expressions
744: #ifdef DEBUG
745: 	auto &orig_expr_list = select_stmt->node->GetSelectList();
746: 	auto &de_expr_list = deserialized_stmt->node->GetSelectList();
747: 	auto &cp_expr_list = copied_stmt->node->GetSelectList();
748: 	D_ASSERT(orig_expr_list.size() == de_expr_list.size() && cp_expr_list.size() == de_expr_list.size());
749: 	for (idx_t i = 0; i < orig_expr_list.size(); i++) {
750: 		// run the ToString, to verify that it doesn't crash
751: 		orig_expr_list[i]->ToString();
752: 		// check that the expressions are equivalent
753: 		D_ASSERT(orig_expr_list[i]->Equals(de_expr_list[i].get()));
754: 		D_ASSERT(orig_expr_list[i]->Equals(cp_expr_list[i].get()));
755: 		D_ASSERT(de_expr_list[i]->Equals(cp_expr_list[i].get()));
756: 		// check that the hashes are equivalent too
757: 		D_ASSERT(orig_expr_list[i]->Hash() == de_expr_list[i]->Hash());
758: 		D_ASSERT(orig_expr_list[i]->Hash() == cp_expr_list[i]->Hash());
759: 
760: 		D_ASSERT(!orig_expr_list[i]->Equals(nullptr));
761: 	}
762: 	// now perform additional checking within the expressions
763: 	for (idx_t outer_idx = 0; outer_idx < orig_expr_list.size(); outer_idx++) {
764: 		auto hash = orig_expr_list[outer_idx]->Hash();
765: 		for (idx_t inner_idx = 0; inner_idx < orig_expr_list.size(); inner_idx++) {
766: 			auto hash2 = orig_expr_list[inner_idx]->Hash();
767: 			if (hash != hash2) {
768: 				// if the hashes are not equivalent, the expressions should not be equivalent
769: 				D_ASSERT(!orig_expr_list[outer_idx]->Equals(orig_expr_list[inner_idx].get()));
770: 			}
771: 		}
772: 	}
773: #endif
774: 
775: 	// disable profiling if it is enabled
776: 	auto &config = ClientConfig::GetConfig(*this);
777: 	bool profiling_is_enabled = config.enable_profiler;
778: 	if (profiling_is_enabled) {
779: 		config.enable_profiler = false;
780: 	}
781: 
782: 	// see below
783: 	auto statement_copy_for_explain = select_stmt->Copy();
784: 
785: 	unique_ptr<MaterializedQueryResult> original_result =
786: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT),
787: 	                                    copied_result =
788: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT),
789: 	                                    deserialized_result =
790: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT),
791: 	                                    unoptimized_result =
792: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT);
793: 
794: 	// execute the original statement
795: 	try {
796: 		auto result = RunStatementInternal(lock, query, move(statement), false, false);
797: 		original_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
798: 	} catch (std::exception &ex) {
799: 		original_result->error = ex.what();
800: 		original_result->success = false;
801: 		interrupted = false;
802: 	}
803: 
804: 	// check explain, only if q does not already contain EXPLAIN
805: 	if (original_result->success) {
806: 		auto explain_q = "EXPLAIN " + query;
807: 		auto explain_stmt = make_unique<ExplainStatement>(move(statement_copy_for_explain));
808: 		try {
809: 			RunStatementInternal(lock, explain_q, move(explain_stmt), false, false);
810: 		} catch (std::exception &ex) { // LCOV_EXCL_START
811: 			return "EXPLAIN failed but query did not (" + string(ex.what()) + ")";
812: 		} // LCOV_EXCL_STOP
813: 	}
814: 
815: 	// now execute the copied statement
816: 	try {
817: 		auto result = RunStatementInternal(lock, query, move(copied_stmt), false, false);
818: 		copied_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
819: 	} catch (std::exception &ex) {
820: 		copied_result->error = ex.what();
821: 		copied_result->success = false;
822: 		interrupted = false;
823: 	}
824: 	// now execute the deserialized statement
825: 	try {
826: 		auto result = RunStatementInternal(lock, query, move(deserialized_stmt), false, false);
827: 		deserialized_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
828: 	} catch (std::exception &ex) {
829: 		deserialized_result->error = ex.what();
830: 		deserialized_result->success = false;
831: 		interrupted = false;
832: 	}
833: 	// now execute the unoptimized statement
834: 	config.enable_optimizer = false;
835: 	try {
836: 		auto result = RunStatementInternal(lock, query, move(unoptimized_stmt), false, false);
837: 		unoptimized_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
838: 	} catch (std::exception &ex) {
839: 		unoptimized_result->error = ex.what();
840: 		unoptimized_result->success = false;
841: 		interrupted = false;
842: 	}
843: 	config.enable_optimizer = true;
844: 
845: 	if (profiling_is_enabled) {
846: 		config.enable_profiler = true;
847: 	}
848: 
849: 	// now compare the results
850: 	// the results of all runs should be identical
851: 	vector<unique_ptr<MaterializedQueryResult>> results;
852: 	results.push_back(move(copied_result));
853: 	results.push_back(move(deserialized_result));
854: 	results.push_back(move(unoptimized_result));
855: 	vector<string> names = {"Copied Result", "Deserialized Result", "Unoptimized Result"};
856: 	for (idx_t i = 0; i < results.size(); i++) {
857: 		if (original_result->success != results[i]->success) { // LCOV_EXCL_START
858: 			string result = names[i] + " differs from original result!\n";
859: 			result += "Original Result:\n" + original_result->ToString();
860: 			result += names[i] + ":\n" + results[i]->ToString();
861: 			return result;
862: 		}                                                                  // LCOV_EXCL_STOP
863: 		if (!original_result->collection.Equals(results[i]->collection)) { // LCOV_EXCL_START
864: 			string result = names[i] + " differs from original result!\n";
865: 			result += "Original Result:\n" + original_result->ToString();
866: 			result += names[i] + ":\n" + results[i]->ToString();
867: 			return result;
868: 		} // LCOV_EXCL_STOP
869: 	}
870: 
871: 	return "";
872: }
873: 
874: bool ClientContext::UpdateFunctionInfoFromEntry(ScalarFunctionCatalogEntry *existing_function,
875:                                                 CreateScalarFunctionInfo *new_info) {
876: 	if (new_info->functions.empty()) {
877: 		throw InternalException("Registering function without scalar function definitions!");
878: 	}
879: 	bool need_rewrite_entry = false;
880: 	idx_t size_new_func = new_info->functions.size();
881: 	for (idx_t exist_idx = 0; exist_idx < existing_function->functions.size(); ++exist_idx) {
882: 		bool can_add = true;
883: 		for (idx_t new_idx = 0; new_idx < size_new_func; ++new_idx) {
884: 			if (new_info->functions[new_idx].Equal(existing_function->functions[exist_idx])) {
885: 				can_add = false;
886: 				break;
887: 			}
888: 		}
889: 		if (can_add) {
890: 			new_info->functions.push_back(existing_function->functions[exist_idx]);
891: 			need_rewrite_entry = true;
892: 		}
893: 	}
894: 	return need_rewrite_entry;
895: }
896: 
897: void ClientContext::RegisterFunction(CreateFunctionInfo *info) {
898: 	RunFunctionInTransaction([&]() {
899: 		auto &catalog = Catalog::GetCatalog(*this);
900: 		auto existing_function = (ScalarFunctionCatalogEntry *)catalog.GetEntry(
901: 		    *this, CatalogType::SCALAR_FUNCTION_ENTRY, info->schema, info->name, true);
902: 		if (existing_function) {
903: 			if (UpdateFunctionInfoFromEntry(existing_function, (CreateScalarFunctionInfo *)info)) {
904: 				// function info was updated from catalog entry, rewrite is needed
905: 				info->on_conflict = OnCreateConflict::REPLACE_ON_CONFLICT;
906: 			}
907: 		}
908: 		// create function
909: 		catalog.CreateFunction(*this, info);
910: 	});
911: }
912: 
913: void ClientContext::RunFunctionInTransactionInternal(ClientContextLock &lock, const std::function<void(void)> &fun,
914:                                                      bool requires_valid_transaction) {
915: 	if (requires_valid_transaction && transaction.HasActiveTransaction() &&
916: 	    transaction.ActiveTransaction().IsInvalidated()) {
917: 		throw Exception("Failed: transaction has been invalidated!");
918: 	}
919: 	// check if we are on AutoCommit. In this case we should start a transaction
920: 	bool require_new_transaction = transaction.IsAutoCommit() && !transaction.HasActiveTransaction();
921: 	if (require_new_transaction) {
922: 		D_ASSERT(!active_query);
923: 		transaction.BeginTransaction();
924: 	}
925: 	try {
926: 		fun();
927: 	} catch (StandardException &ex) {
928: 		if (require_new_transaction) {
929: 			transaction.Rollback();
930: 		}
931: 		throw;
932: 	} catch (std::exception &ex) {
933: 		if (require_new_transaction) {
934: 			transaction.Rollback();
935: 		} else {
936: 			ActiveTransaction().Invalidate();
937: 		}
938: 		throw;
939: 	}
940: 	if (require_new_transaction) {
941: 		transaction.Commit();
942: 	}
943: }
944: 
945: void ClientContext::RunFunctionInTransaction(const std::function<void(void)> &fun, bool requires_valid_transaction) {
946: 	auto lock = LockContext();
947: 	RunFunctionInTransactionInternal(*lock, fun, requires_valid_transaction);
948: }
949: 
950: unique_ptr<TableDescription> ClientContext::TableInfo(const string &schema_name, const string &table_name) {
951: 	unique_ptr<TableDescription> result;
952: 	RunFunctionInTransaction([&]() {
953: 		// obtain the table info
954: 		auto &catalog = Catalog::GetCatalog(*this);
955: 		auto table = catalog.GetEntry<TableCatalogEntry>(*this, schema_name, table_name, true);
956: 		if (!table) {
957: 			return;
958: 		}
959: 		// write the table info to the result
960: 		result = make_unique<TableDescription>();
961: 		result->schema = schema_name;
962: 		result->table = table_name;
963: 		for (auto &column : table->columns) {
964: 			result->columns.emplace_back(column.name, column.type);
965: 		}
966: 	});
967: 	return result;
968: }
969: 
970: void ClientContext::Append(TableDescription &description, ChunkCollection &collection) {
971: 	RunFunctionInTransaction([&]() {
972: 		auto &catalog = Catalog::GetCatalog(*this);
973: 		auto table_entry = catalog.GetEntry<TableCatalogEntry>(*this, description.schema, description.table);
974: 		// verify that the table columns and types match up
975: 		if (description.columns.size() != table_entry->columns.size()) {
976: 			throw Exception("Failed to append: table entry has different number of columns!");
977: 		}
978: 		for (idx_t i = 0; i < description.columns.size(); i++) {
979: 			if (description.columns[i].type != table_entry->columns[i].type) {
980: 				throw Exception("Failed to append: table entry has different number of columns!");
981: 			}
982: 		}
983: 		for (auto &chunk : collection.Chunks()) {
984: 			table_entry->storage->Append(*table_entry, *this, *chunk);
985: 		}
986: 	});
987: }
988: 
989: void ClientContext::TryBindRelation(Relation &relation, vector<ColumnDefinition> &result_columns) {
990: #ifdef DEBUG
991: 	D_ASSERT(!relation.GetAlias().empty());
992: 	D_ASSERT(!relation.ToString().empty());
993: #endif
994: 	RunFunctionInTransaction([&]() {
995: 		// bind the expressions
996: 		auto binder = Binder::CreateBinder(*this);
997: 		auto result = relation.Bind(*binder);
998: 		D_ASSERT(result.names.size() == result.types.size());
999: 		for (idx_t i = 0; i < result.names.size(); i++) {
1000: 			result_columns.emplace_back(result.names[i], result.types[i]);
1001: 		}
1002: 	});
1003: }
1004: 
1005: unordered_set<string> ClientContext::GetTableNames(const string &query) {
1006: 	auto lock = LockContext();
1007: 
1008: 	auto statements = ParseStatementsInternal(*lock, query);
1009: 	if (statements.size() != 1) {
1010: 		throw InvalidInputException("Expected a single statement");
1011: 	}
1012: 
1013: 	unordered_set<string> result;
1014: 	RunFunctionInTransactionInternal(*lock, [&]() {
1015: 		// bind the expressions
1016: 		auto binder = Binder::CreateBinder(*this);
1017: 		binder->SetBindingMode(BindingMode::EXTRACT_NAMES);
1018: 		binder->Bind(*statements[0]);
1019: 		result = binder->GetTableNames();
1020: 	});
1021: 	return result;
1022: }
1023: 
1024: unique_ptr<QueryResult> ClientContext::Execute(const shared_ptr<Relation> &relation) {
1025: 	auto lock = LockContext();
1026: 	InitialCleanup(*lock);
1027: 
1028: 	string query;
1029: 	if (config.query_verification_enabled) {
1030: 		// run the ToString method of any relation we run, mostly to ensure it doesn't crash
1031: 		relation->ToString();
1032: 		relation->GetAlias();
1033: 		if (relation->IsReadOnly()) {
1034: 			// verify read only statements by running a select statement
1035: 			auto select = make_unique<SelectStatement>();
1036: 			select->node = relation->GetQueryNode();
1037: 			RunStatementInternal(*lock, query, move(select), false);
1038: 		}
1039: 	}
1040: 	auto &expected_columns = relation->Columns();
1041: 	auto relation_stmt = make_unique<RelationStatement>(relation);
1042: 
1043: 	unique_ptr<QueryResult> result;
1044: 	result = RunStatementInternal(*lock, query, move(relation_stmt), false);
1045: 	if (!result->success) {
1046: 		return result;
1047: 	}
1048: 	// verify that the result types and result names of the query match the expected result types/names
1049: 	if (result->types.size() == expected_columns.size()) {
1050: 		bool mismatch = false;
1051: 		for (idx_t i = 0; i < result->types.size(); i++) {
1052: 			if (result->types[i] != expected_columns[i].type || result->names[i] != expected_columns[i].name) {
1053: 				mismatch = true;
1054: 				break;
1055: 			}
1056: 		}
1057: 		if (!mismatch) {
1058: 			// all is as expected: return the result
1059: 			return result;
1060: 		}
1061: 	}
1062: 	// result mismatch
1063: 	string err_str = "Result mismatch in query!\nExpected the following columns: [";
1064: 	for (idx_t i = 0; i < expected_columns.size(); i++) {
1065: 		if (i > 0) {
1066: 			err_str += ", ";
1067: 		}
1068: 		err_str += expected_columns[i].name + " " + expected_columns[i].type.ToString();
1069: 	}
1070: 	err_str += "]\nBut result contained the following: ";
1071: 	for (idx_t i = 0; i < result->types.size(); i++) {
1072: 		err_str += i == 0 ? "[" : ", ";
1073: 		err_str += result->names[i] + " " + result->types[i].ToString();
1074: 	}
1075: 	err_str += "]";
1076: 	return make_unique<MaterializedQueryResult>(err_str);
1077: }
1078: 
1079: bool ClientContext::TryGetCurrentSetting(const std::string &key, Value &result) {
1080: 	// first check the built-in settings
1081: 	auto &db_config = DBConfig::GetConfig(*this);
1082: 	auto option = db_config.GetOptionByName(key);
1083: 	if (option) {
1084: 		result = option->get_setting(*this);
1085: 		return true;
1086: 	}
1087: 
1088: 	// then check the session values
1089: 	const auto &session_config_map = config.set_variables;
1090: 	const auto &global_config_map = db_config.set_variables;
1091: 
1092: 	auto session_value = session_config_map.find(key);
1093: 	bool found_session_value = session_value != session_config_map.end();
1094: 	auto global_value = global_config_map.find(key);
1095: 	bool found_global_value = global_value != global_config_map.end();
1096: 	if (!found_session_value && !found_global_value) {
1097: 		return false;
1098: 	}
1099: 
1100: 	result = found_session_value ? session_value->second : global_value->second;
1101: 	return true;
1102: }
1103: 
1104: } // namespace duckdb
[end of src/main/client_context.cpp]
[start of src/optimizer/statistics/expression/propagate_comparison.cpp]
1: #include "duckdb/optimizer/statistics_propagator.hpp"
2: #include "duckdb/planner/expression/bound_comparison_expression.hpp"
3: #include "duckdb/planner/expression/bound_constant_expression.hpp"
4: #include "duckdb/storage/statistics/numeric_statistics.hpp"
5: #include "duckdb/optimizer/expression_rewriter.hpp"
6: 
7: namespace duckdb {
8: 
9: FilterPropagateResult StatisticsPropagator::PropagateComparison(BaseStatistics &left, BaseStatistics &right,
10:                                                                 ExpressionType comparison) {
11: 	// only handle numerics for now
12: 	switch (left.type.InternalType()) {
13: 	case PhysicalType::BOOL:
14: 	case PhysicalType::INT8:
15: 	case PhysicalType::INT16:
16: 	case PhysicalType::INT32:
17: 	case PhysicalType::INT64:
18: 	case PhysicalType::INT128:
19: 	case PhysicalType::FLOAT:
20: 	case PhysicalType::DOUBLE:
21: 		break;
22: 	default:
23: 		return FilterPropagateResult::NO_PRUNING_POSSIBLE;
24: 	}
25: 	auto &lstats = (NumericStatistics &)left;
26: 	auto &rstats = (NumericStatistics &)right;
27: 	if (lstats.min.is_null || lstats.max.is_null || rstats.min.is_null || rstats.max.is_null) {
28: 		// no stats available: nothing to prune
29: 		return FilterPropagateResult::NO_PRUNING_POSSIBLE;
30: 	}
31: 	// the result of the propagation depend on whether or not either side has null values
32: 	// if there are null values present, we cannot say whether or not
33: 	bool has_null = lstats.CanHaveNull() || rstats.CanHaveNull();
34: 	switch (comparison) {
35: 	case ExpressionType::COMPARE_EQUAL:
36: 		// l = r, if l.min > r.max or r.min > l.max equality is not possible
37: 		if (lstats.min > rstats.max || rstats.min > lstats.max) {
38: 			return has_null ? FilterPropagateResult::FILTER_FALSE_OR_NULL : FilterPropagateResult::FILTER_ALWAYS_FALSE;
39: 		} else {
40: 			return FilterPropagateResult::NO_PRUNING_POSSIBLE;
41: 		}
42: 	case ExpressionType::COMPARE_GREATERTHAN:
43: 		// l > r
44: 		if (lstats.min > rstats.max) {
45: 			// if l.min > r.max, it is always true ONLY if neither side contains nulls
46: 			return has_null ? FilterPropagateResult::FILTER_TRUE_OR_NULL : FilterPropagateResult::FILTER_ALWAYS_TRUE;
47: 		}
48: 		// if r.min is bigger or equal to l.max, the filter is always false
49: 		if (rstats.min >= lstats.max) {
50: 			return has_null ? FilterPropagateResult::FILTER_FALSE_OR_NULL : FilterPropagateResult::FILTER_ALWAYS_FALSE;
51: 		}
52: 		return FilterPropagateResult::NO_PRUNING_POSSIBLE;
53: 	case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
54: 		// l >= r
55: 		if (lstats.min >= rstats.max) {
56: 			// if l.min >= r.max, it is always true ONLY if neither side contains nulls
57: 			return has_null ? FilterPropagateResult::FILTER_TRUE_OR_NULL : FilterPropagateResult::FILTER_ALWAYS_TRUE;
58: 		}
59: 		// if r.min > l.max, the filter is always false
60: 		if (rstats.min > lstats.max) {
61: 			return has_null ? FilterPropagateResult::FILTER_FALSE_OR_NULL : FilterPropagateResult::FILTER_ALWAYS_FALSE;
62: 		}
63: 		return FilterPropagateResult::NO_PRUNING_POSSIBLE;
64: 	case ExpressionType::COMPARE_LESSTHAN:
65: 		// l < r
66: 		if (lstats.max < rstats.min) {
67: 			// if l.max < r.min, it is always true ONLY if neither side contains nulls
68: 			return has_null ? FilterPropagateResult::FILTER_TRUE_OR_NULL : FilterPropagateResult::FILTER_ALWAYS_TRUE;
69: 		}
70: 		// if l.min >= rstats.max, the filter is always false
71: 		if (lstats.min >= rstats.max) {
72: 			return has_null ? FilterPropagateResult::FILTER_FALSE_OR_NULL : FilterPropagateResult::FILTER_ALWAYS_FALSE;
73: 		}
74: 		return FilterPropagateResult::NO_PRUNING_POSSIBLE;
75: 	case ExpressionType::COMPARE_LESSTHANOREQUALTO:
76: 		// l <= r
77: 		if (lstats.max <= rstats.min) {
78: 			// if l.max <= r.min, it is always true ONLY if neither side contains nulls
79: 			return has_null ? FilterPropagateResult::FILTER_TRUE_OR_NULL : FilterPropagateResult::FILTER_ALWAYS_TRUE;
80: 		}
81: 		// if l.min > rstats.max, the filter is always false
82: 		if (lstats.min > rstats.max) {
83: 			return has_null ? FilterPropagateResult::FILTER_FALSE_OR_NULL : FilterPropagateResult::FILTER_ALWAYS_FALSE;
84: 		}
85: 		return FilterPropagateResult::NO_PRUNING_POSSIBLE;
86: 	default:
87: 		return FilterPropagateResult::NO_PRUNING_POSSIBLE;
88: 	}
89: }
90: 
91: unique_ptr<BaseStatistics> StatisticsPropagator::PropagateExpression(BoundComparisonExpression &expr,
92:                                                                      unique_ptr<Expression> *expr_ptr) {
93: 	auto left_stats = PropagateExpression(expr.left);
94: 	auto right_stats = PropagateExpression(expr.right);
95: 	if (!left_stats || !right_stats) {
96: 		return nullptr;
97: 	}
98: 	// propagate the statistics of the comparison operator
99: 	auto propagate_result = PropagateComparison(*left_stats, *right_stats, expr.type);
100: 	switch (propagate_result) {
101: 	case FilterPropagateResult::FILTER_ALWAYS_TRUE:
102: 		*expr_ptr = make_unique<BoundConstantExpression>(Value::BOOLEAN(true));
103: 		return PropagateExpression(*expr_ptr);
104: 	case FilterPropagateResult::FILTER_ALWAYS_FALSE:
105: 		*expr_ptr = make_unique<BoundConstantExpression>(Value::BOOLEAN(false));
106: 		return PropagateExpression(*expr_ptr);
107: 	case FilterPropagateResult::FILTER_TRUE_OR_NULL: {
108: 		vector<unique_ptr<Expression>> children;
109: 		children.push_back(move(expr.left));
110: 		children.push_back(move(expr.right));
111: 		*expr_ptr = ExpressionRewriter::ConstantOrNull(move(children), Value::BOOLEAN(true));
112: 		return nullptr;
113: 	}
114: 	case FilterPropagateResult::FILTER_FALSE_OR_NULL: {
115: 		vector<unique_ptr<Expression>> children;
116: 		children.push_back(move(expr.left));
117: 		children.push_back(move(expr.right));
118: 		*expr_ptr = ExpressionRewriter::ConstantOrNull(move(children), Value::BOOLEAN(false));
119: 		return nullptr;
120: 	}
121: 	default:
122: 		// FIXME: we can propagate nulls here, i.e. this expression will have nulls only if left and right has nulls
123: 		return nullptr;
124: 	}
125: }
126: 
127: } // namespace duckdb
[end of src/optimizer/statistics/expression/propagate_comparison.cpp]
[start of src/optimizer/statistics/expression/propagate_constant.cpp]
1: #include "duckdb/optimizer/statistics_propagator.hpp"
2: #include "duckdb/planner/expression/bound_constant_expression.hpp"
3: #include "duckdb/storage/statistics/numeric_statistics.hpp"
4: #include "duckdb/storage/statistics/string_statistics.hpp"
5: #include "duckdb/storage/statistics/struct_statistics.hpp"
6: #include "duckdb/storage/statistics/list_statistics.hpp"
7: 
8: namespace duckdb {
9: 
10: unique_ptr<BaseStatistics> StatisticsPropagator::StatisticsFromValue(const Value &input) {
11: 	switch (input.type().InternalType()) {
12: 	case PhysicalType::BOOL:
13: 	case PhysicalType::INT8:
14: 	case PhysicalType::INT16:
15: 	case PhysicalType::INT32:
16: 	case PhysicalType::INT64:
17: 	case PhysicalType::INT128:
18: 	case PhysicalType::FLOAT:
19: 	case PhysicalType::DOUBLE: {
20: 		auto result = make_unique<NumericStatistics>(input.type(), input, input);
21: 		result->validity_stats = make_unique<ValidityStatistics>(input.is_null, !input.is_null);
22: 		return move(result);
23: 	}
24: 	case PhysicalType::VARCHAR: {
25: 		auto result = make_unique<StringStatistics>(input.type());
26: 		result->validity_stats = make_unique<ValidityStatistics>(input.is_null, !input.is_null);
27: 		if (!input.is_null) {
28: 			string_t str(input.str_value.c_str(), input.str_value.size());
29: 			result->Update(str);
30: 		}
31: 		return move(result);
32: 	}
33: 	case PhysicalType::STRUCT: {
34: 		auto result = make_unique<StructStatistics>(input.type());
35: 		result->validity_stats = make_unique<ValidityStatistics>(input.is_null, !input.is_null);
36: 		if (input.is_null) {
37: 			for (auto &child_stat : result->child_stats) {
38: 				child_stat.reset();
39: 			}
40: 		} else {
41: 			D_ASSERT(result->child_stats.size() == input.struct_value.size());
42: 			for (idx_t i = 0; i < result->child_stats.size(); i++) {
43: 				result->child_stats[i] = StatisticsFromValue(input.struct_value[i]);
44: 			}
45: 		}
46: 		return move(result);
47: 	}
48: 	case PhysicalType::LIST: {
49: 		auto result = make_unique<ListStatistics>(input.type());
50: 		result->validity_stats = make_unique<ValidityStatistics>(input.is_null, !input.is_null);
51: 		if (input.is_null) {
52: 			result->child_stats.reset();
53: 		} else {
54: 			for (auto &child_element : input.list_value) {
55: 				auto child_element_stats = StatisticsFromValue(child_element);
56: 				if (child_element_stats) {
57: 					result->child_stats->Merge(*child_element_stats);
58: 				} else {
59: 					result->child_stats.reset();
60: 				}
61: 			}
62: 		}
63: 		return move(result);
64: 	}
65: 	default:
66: 		return nullptr;
67: 	}
68: }
69: 
70: unique_ptr<BaseStatistics> StatisticsPropagator::PropagateExpression(BoundConstantExpression &constant,
71:                                                                      unique_ptr<Expression> *expr_ptr) {
72: 	return StatisticsFromValue(constant.value);
73: }
74: 
75: } // namespace duckdb
[end of src/optimizer/statistics/expression/propagate_constant.cpp]
[start of src/storage/statistics/string_statistics.cpp]
1: #include "duckdb/storage/statistics/string_statistics.hpp"
2: #include "duckdb/common/serializer.hpp"
3: #include "utf8proc_wrapper.hpp"
4: #include "duckdb/common/string_util.hpp"
5: #include "duckdb/common/types/vector.hpp"
6: 
7: namespace duckdb {
8: 
9: StringStatistics::StringStatistics(LogicalType type_p) : BaseStatistics(move(type_p)) {
10: 	for (idx_t i = 0; i < MAX_STRING_MINMAX_SIZE; i++) {
11: 		min[i] = 0xFF;
12: 		max[i] = 0;
13: 	}
14: 	max_string_length = 0;
15: 	has_unicode = false;
16: 	has_overflow_strings = false;
17: 	validity_stats = make_unique<ValidityStatistics>(false);
18: }
19: 
20: unique_ptr<BaseStatistics> StringStatistics::Copy() {
21: 	auto stats = make_unique<StringStatistics>(type);
22: 	memcpy(stats->min, min, MAX_STRING_MINMAX_SIZE);
23: 	memcpy(stats->max, max, MAX_STRING_MINMAX_SIZE);
24: 	stats->has_unicode = has_unicode;
25: 	stats->max_string_length = max_string_length;
26: 	stats->max_string_length = max_string_length;
27: 	if (validity_stats) {
28: 		stats->validity_stats = validity_stats->Copy();
29: 	}
30: 	return move(stats);
31: }
32: 
33: void StringStatistics::Serialize(Serializer &serializer) {
34: 	BaseStatistics::Serialize(serializer);
35: 	serializer.WriteData(min, MAX_STRING_MINMAX_SIZE);
36: 	serializer.WriteData(max, MAX_STRING_MINMAX_SIZE);
37: 	serializer.Write<bool>(has_unicode);
38: 	serializer.Write<uint32_t>(max_string_length);
39: 	serializer.Write<bool>(has_overflow_strings);
40: }
41: 
42: unique_ptr<BaseStatistics> StringStatistics::Deserialize(Deserializer &source, LogicalType type) {
43: 	auto stats = make_unique<StringStatistics>(move(type));
44: 	source.ReadData(stats->min, MAX_STRING_MINMAX_SIZE);
45: 	source.ReadData(stats->max, MAX_STRING_MINMAX_SIZE);
46: 	stats->has_unicode = source.Read<bool>();
47: 	stats->max_string_length = source.Read<uint32_t>();
48: 	stats->has_overflow_strings = source.Read<bool>();
49: 	return move(stats);
50: }
51: 
52: static int StringValueComparison(const_data_ptr_t data, idx_t len, const_data_ptr_t comparison) {
53: 	D_ASSERT(len <= StringStatistics::MAX_STRING_MINMAX_SIZE);
54: 	for (idx_t i = 0; i < len; i++) {
55: 		if (data[i] < comparison[i]) {
56: 			return -1;
57: 		} else if (data[i] > comparison[i]) {
58: 			return 1;
59: 		}
60: 	}
61: 	return 0;
62: }
63: 
64: static void ConstructValue(const_data_ptr_t data, idx_t size, data_t target[]) {
65: 	idx_t value_size =
66: 	    size > StringStatistics::MAX_STRING_MINMAX_SIZE ? StringStatistics::MAX_STRING_MINMAX_SIZE : size;
67: 	memcpy(target, data, value_size);
68: 	for (idx_t i = value_size; i < StringStatistics::MAX_STRING_MINMAX_SIZE; i++) {
69: 		target[i] = '\0';
70: 	}
71: }
72: 
73: void StringStatistics::Update(const string_t &value) {
74: 	auto data = (const_data_ptr_t)value.GetDataUnsafe();
75: 	auto size = value.GetSize();
76: 
77: 	//! we can only fit 8 bytes, so we might need to trim our string
78: 	// construct the value
79: 	data_t target[MAX_STRING_MINMAX_SIZE];
80: 	ConstructValue(data, size, target);
81: 
82: 	// update the min and max
83: 	if (StringValueComparison(target, MAX_STRING_MINMAX_SIZE, min) < 0) {
84: 		memcpy(min, target, MAX_STRING_MINMAX_SIZE);
85: 	}
86: 	if (StringValueComparison(target, MAX_STRING_MINMAX_SIZE, max) > 0) {
87: 		memcpy(max, target, MAX_STRING_MINMAX_SIZE);
88: 	}
89: 	if (size > max_string_length) {
90: 		max_string_length = size;
91: 	}
92: 	if (type.id() == LogicalTypeId::VARCHAR && !has_unicode) {
93: 		auto unicode = Utf8Proc::Analyze((const char *)data, size);
94: 		if (unicode == UnicodeType::UNICODE) {
95: 			has_unicode = true;
96: 		} else if (unicode == UnicodeType::INVALID) {
97: 			throw InternalException("Invalid unicode detected in segment statistics update!");
98: 		}
99: 	}
100: }
101: 
102: void StringStatistics::Merge(const BaseStatistics &other_p) {
103: 	BaseStatistics::Merge(other_p);
104: 	auto &other = (const StringStatistics &)other_p;
105: 	if (StringValueComparison(other.min, MAX_STRING_MINMAX_SIZE, min) < 0) {
106: 		memcpy(min, other.min, MAX_STRING_MINMAX_SIZE);
107: 	}
108: 	if (StringValueComparison(other.max, MAX_STRING_MINMAX_SIZE, max) > 0) {
109: 		memcpy(max, other.max, MAX_STRING_MINMAX_SIZE);
110: 	}
111: 	has_unicode = has_unicode || other.has_unicode;
112: 	max_string_length = MaxValue<uint32_t>(max_string_length, other.max_string_length);
113: 	has_overflow_strings = has_overflow_strings || other.has_overflow_strings;
114: }
115: 
116: FilterPropagateResult StringStatistics::CheckZonemap(ExpressionType comparison_type, const string &constant) {
117: 	auto data = (const_data_ptr_t)constant.c_str();
118: 	auto size = constant.size();
119: 
120: 	idx_t value_size = size > MAX_STRING_MINMAX_SIZE ? MAX_STRING_MINMAX_SIZE : size;
121: 	int min_comp = StringValueComparison(data, value_size, min);
122: 	int max_comp = StringValueComparison(data, value_size, max);
123: 	switch (comparison_type) {
124: 	case ExpressionType::COMPARE_EQUAL:
125: 		if (min_comp >= 0 && max_comp <= 0) {
126: 			return FilterPropagateResult::NO_PRUNING_POSSIBLE;
127: 		} else {
128: 			return FilterPropagateResult::FILTER_ALWAYS_FALSE;
129: 		}
130: 	case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
131: 	case ExpressionType::COMPARE_GREATERTHAN:
132: 		if (max_comp <= 0) {
133: 			return FilterPropagateResult::NO_PRUNING_POSSIBLE;
134: 		} else {
135: 			return FilterPropagateResult::FILTER_ALWAYS_FALSE;
136: 		}
137: 	case ExpressionType::COMPARE_LESSTHAN:
138: 	case ExpressionType::COMPARE_LESSTHANOREQUALTO:
139: 		if (min_comp >= 0) {
140: 			return FilterPropagateResult::NO_PRUNING_POSSIBLE;
141: 		} else {
142: 			return FilterPropagateResult::FILTER_ALWAYS_FALSE;
143: 		}
144: 	default:
145: 		throw InternalException("Expression type not implemented for string statistics zone map");
146: 	}
147: }
148: 
149: static idx_t GetValidMinMaxSubstring(data_ptr_t data) {
150: 	for (idx_t i = 0; i < StringStatistics::MAX_STRING_MINMAX_SIZE; i++) {
151: 		if (data[i] == '\0') {
152: 			return i;
153: 		}
154: 		if ((data[i] & 0x80) != 0) {
155: 			return i;
156: 		}
157: 	}
158: 	return StringStatistics::MAX_STRING_MINMAX_SIZE;
159: }
160: 
161: string StringStatistics::ToString() {
162: 	idx_t min_len = GetValidMinMaxSubstring(min);
163: 	idx_t max_len = GetValidMinMaxSubstring(max);
164: 	return StringUtil::Format("[Min: %s, Max: %s, Has Unicode: %s, Max String Length: %lld]%s",
165: 	                          string((const char *)min, min_len), string((const char *)max, max_len),
166: 	                          has_unicode ? "true" : "false", max_string_length,
167: 	                          validity_stats ? validity_stats->ToString() : "");
168: }
169: 
170: void StringStatistics::Verify(Vector &vector, const SelectionVector &sel, idx_t count) {
171: 	BaseStatistics::Verify(vector, sel, count);
172: 
173: 	string_t min_string((const char *)min, MAX_STRING_MINMAX_SIZE);
174: 	string_t max_string((const char *)max, MAX_STRING_MINMAX_SIZE);
175: 
176: 	VectorData vdata;
177: 	vector.Orrify(count, vdata);
178: 	auto data = (string_t *)vdata.data;
179: 	for (idx_t i = 0; i < count; i++) {
180: 		auto idx = sel.get_index(i);
181: 		auto index = vdata.sel->get_index(idx);
182: 		if (!vdata.validity.RowIsValid(index)) {
183: 			continue;
184: 		}
185: 		auto value = data[index];
186: 		auto data = value.GetDataUnsafe();
187: 		auto len = value.GetSize();
188: 		// LCOV_EXCL_START
189: 		if (len > max_string_length) {
190: 			throw InternalException(
191: 			    "Statistics mismatch: string value exceeds maximum string length.\nStatistics: %s\nVector: %s",
192: 			    ToString(), vector.ToString(count));
193: 		}
194: 		if (type.id() == LogicalTypeId::VARCHAR && !has_unicode) {
195: 			auto unicode = Utf8Proc::Analyze(data, len);
196: 			if (unicode == UnicodeType::UNICODE) {
197: 				throw InternalException("Statistics mismatch: string value contains unicode, but statistics says it "
198: 				                        "shouldn't.\nStatistics: %s\nVector: %s",
199: 				                        ToString(), vector.ToString(count));
200: 			} else if (unicode == UnicodeType::INVALID) {
201: 				throw InternalException("Invalid unicode detected in vector: %s", vector.ToString(count));
202: 			}
203: 		}
204: 		if (StringValueComparison((const_data_ptr_t)data, MinValue<idx_t>(len, MAX_STRING_MINMAX_SIZE), min) < 0) {
205: 			throw InternalException("Statistics mismatch: value is smaller than min.\nStatistics: %s\nVector: %s",
206: 			                        ToString(), vector.ToString(count));
207: 		}
208: 		if (StringValueComparison((const_data_ptr_t)data, MinValue<idx_t>(len, MAX_STRING_MINMAX_SIZE), max) > 0) {
209: 			throw InternalException("Statistics mismatch: value is bigger than max.\nStatistics: %s\nVector: %s",
210: 			                        ToString(), vector.ToString(count));
211: 		}
212: 		// LCOV_EXCL_STOP
213: 	}
214: }
215: 
216: } // namespace duckdb
[end of src/storage/statistics/string_statistics.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: