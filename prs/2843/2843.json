{
  "repo": "duckdb/duckdb",
  "pull_number": 2843,
  "instance_id": "duckdb__duckdb-2843",
  "issue_numbers": [
    "2664"
  ],
  "base_commit": "6322c0deb9bc1aee3d49f08452d5e03a20395e6b",
  "patch": "diff --git a/extension/parquet/column_reader.cpp b/extension/parquet/column_reader.cpp\nindex da0b6359b13a..fe0281d56b1c 100644\n--- a/extension/parquet/column_reader.cpp\n+++ b/extension/parquet/column_reader.cpp\n@@ -5,7 +5,7 @@\n \n #include \"boolean_column_reader.hpp\"\n #include \"callback_column_reader.hpp\"\n-#include \"decimal_column_reader.hpp\"\n+#include \"parquet_decimal_utils.hpp\"\n #include \"list_column_reader.hpp\"\n #include \"string_column_reader.hpp\"\n #include \"struct_column_reader.hpp\"\n@@ -51,119 +51,63 @@ ColumnReader::ColumnReader(ParquetReader &reader, LogicalType type_p, const Sche\n ColumnReader::~ColumnReader() {\n }\n \n-template <class T>\n-unique_ptr<ColumnReader> CreateDecimalReader(ParquetReader &reader, const LogicalType &type_p,\n-                                             const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,\n-                                             idx_t max_repeat) {\n-\tswitch (type_p.InternalType()) {\n-\tcase PhysicalType::INT16:\n-\t\treturn make_unique<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<T>>>(\n-\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\tcase PhysicalType::INT32:\n-\t\treturn make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<T>>>(\n-\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\tcase PhysicalType::INT64:\n-\t\treturn make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<T>>>(\n-\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\tdefault:\n-\t\tthrow NotImplementedException(\"Unimplemented internal type for CreateDecimalReader\");\n+const LogicalType &ColumnReader::Type() {\n+\treturn type;\n+}\n+\n+const SchemaElement &ColumnReader::Schema() {\n+\treturn schema;\n+}\n+\n+idx_t ColumnReader::GroupRowsAvailable() {\n+\treturn group_rows_available;\n+}\n+\n+unique_ptr<BaseStatistics> ColumnReader::Stats(const std::vector<ColumnChunk> &columns) {\n+\tif (Type().id() == LogicalTypeId::LIST || Type().id() == LogicalTypeId::STRUCT ||\n+\t    Type().id() == LogicalTypeId::MAP) {\n+\t\treturn nullptr;\n \t}\n+\treturn ParquetStatisticsUtils::TransformColumnStatistics(Schema(), Type(), columns[file_idx]);\n }\n \n-unique_ptr<ColumnReader> ColumnReader::CreateReader(ParquetReader &reader, const LogicalType &type_p,\n-                                                    const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,\n-                                                    idx_t max_repeat) {\n-\tswitch (type_p.id()) {\n-\tcase LogicalTypeId::BOOLEAN:\n-\t\treturn make_unique<BooleanColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\tcase LogicalTypeId::UTINYINT:\n-\t\treturn make_unique<TemplatedColumnReader<uint8_t, TemplatedParquetValueConversion<uint32_t>>>(\n-\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\tcase LogicalTypeId::USMALLINT:\n-\t\treturn make_unique<TemplatedColumnReader<uint16_t, TemplatedParquetValueConversion<uint32_t>>>(\n-\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\tcase LogicalTypeId::UINTEGER:\n-\t\treturn make_unique<TemplatedColumnReader<uint32_t, TemplatedParquetValueConversion<uint32_t>>>(\n-\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\tcase LogicalTypeId::UBIGINT:\n-\t\treturn make_unique<TemplatedColumnReader<uint64_t, TemplatedParquetValueConversion<uint64_t>>>(\n-\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\tcase LogicalTypeId::TINYINT:\n-\t\treturn make_unique<TemplatedColumnReader<int8_t, TemplatedParquetValueConversion<int32_t>>>(\n-\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\tcase LogicalTypeId::SMALLINT:\n-\t\treturn make_unique<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<int32_t>>>(\n-\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\tcase LogicalTypeId::INTEGER:\n-\t\treturn make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<int32_t>>>(\n-\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\tcase LogicalTypeId::BIGINT:\n-\t\treturn make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<int64_t>>>(\n-\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\tcase LogicalTypeId::FLOAT:\n-\t\treturn make_unique<TemplatedColumnReader<float, TemplatedParquetValueConversion<float>>>(\n-\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\tcase LogicalTypeId::DOUBLE:\n-\t\treturn make_unique<TemplatedColumnReader<double, TemplatedParquetValueConversion<double>>>(\n-\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\tcase LogicalTypeId::TIMESTAMP:\n-\t\tswitch (schema_p.type) {\n-\t\tcase Type::INT96:\n-\t\t\treturn make_unique<CallbackColumnReader<Int96, timestamp_t, ImpalaTimestampToTimestamp>>(\n-\t\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\t\tcase Type::INT64:\n-\t\t\tswitch (schema_p.converted_type) {\n-\t\t\tcase ConvertedType::TIMESTAMP_MICROS:\n-\t\t\t\treturn make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMicrosToTimestamp>>(\n-\t\t\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\t\t\tcase ConvertedType::TIMESTAMP_MILLIS:\n-\t\t\t\treturn make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMsToTimestamp>>(\n-\t\t\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\t\t\tdefault:\n-\t\t\t\tbreak;\n-\t\t\t}\n-\t\tdefault:\n-\t\t\tbreak;\n-\t\t}\n-\t\tbreak;\n-\tcase LogicalTypeId::DATE:\n-\t\treturn make_unique<CallbackColumnReader<int32_t, date_t, ParquetIntToDate>>(reader, type_p, schema_p,\n-\t\t                                                                            file_idx_p, max_define, max_repeat);\n-\tcase LogicalTypeId::BLOB:\n-\tcase LogicalTypeId::VARCHAR:\n-\t\treturn make_unique<StringColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\tcase LogicalTypeId::DECIMAL:\n-\t\t// we have to figure out what kind of int we need\n-\t\tswitch (schema_p.type) {\n-\t\tcase Type::INT32:\n-\t\t\treturn CreateDecimalReader<int32_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\t\tcase Type::INT64:\n-\t\t\treturn CreateDecimalReader<int64_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n-\t\tcase Type::FIXED_LEN_BYTE_ARRAY:\n-\t\t\tswitch (type_p.InternalType()) {\n-\t\t\tcase PhysicalType::INT16:\n-\t\t\t\treturn make_unique<DecimalColumnReader<int16_t>>(reader, type_p, schema_p, file_idx_p, max_define,\n-\t\t\t\t                                                 max_repeat);\n-\t\t\tcase PhysicalType::INT32:\n-\t\t\t\treturn make_unique<DecimalColumnReader<int32_t>>(reader, type_p, schema_p, file_idx_p, max_define,\n-\t\t\t\t                                                 max_repeat);\n-\t\t\tcase PhysicalType::INT64:\n-\t\t\t\treturn make_unique<DecimalColumnReader<int64_t>>(reader, type_p, schema_p, file_idx_p, max_define,\n-\t\t\t\t                                                 max_repeat);\n-\t\t\tcase PhysicalType::INT128:\n-\t\t\t\treturn make_unique<DecimalColumnReader<hugeint_t>>(reader, type_p, schema_p, file_idx_p, max_define,\n-\t\t\t\t                                                   max_repeat);\n-\t\t\tdefault:\n-\t\t\t\tthrow InternalException(\"Unrecognized type for Decimal\");\n-\t\t\t}\n-\t\tdefault:\n-\t\t\tthrow NotImplementedException(\"Unrecognized Parquet type for Decimal\");\n-\t\t}\n-\t\tbreak;\n-\tdefault:\n-\t\tbreak;\n+void ColumnReader::Plain(shared_ptr<ByteBuffer> plain_data, uint8_t *defines, idx_t num_values, // NOLINT\n+                         parquet_filter_t &filter, idx_t result_offset, Vector &result) {\n+\tthrow NotImplementedException(\"Plain\");\n+}\n+\n+void ColumnReader::Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) { // NOLINT\n+\tthrow NotImplementedException(\"Dictionary\");\n+}\n+\n+void ColumnReader::Offsets(uint32_t *offsets, uint8_t *defines, idx_t num_values, parquet_filter_t &filter,\n+                           idx_t result_offset, Vector &result) {\n+\tthrow NotImplementedException(\"Offsets\");\n+}\n+\n+void ColumnReader::DictReference(Vector &result) {\n+}\n+void ColumnReader::PlainReference(shared_ptr<ByteBuffer>, Vector &result) { // NOLINT\n+}\n+\n+void ColumnReader::InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) {\n+\tD_ASSERT(file_idx < columns.size());\n+\tchunk = &columns[file_idx];\n+\tprotocol = &protocol_p;\n+\tD_ASSERT(chunk);\n+\tD_ASSERT(chunk->__isset.meta_data);\n+\n+\tif (chunk->__isset.file_path) {\n+\t\tthrow std::runtime_error(\"Only inlined data files are supported (no references)\");\n \t}\n-\tthrow NotImplementedException(type_p.ToString());\n+\n+\t// ugh. sometimes there is an extra offset for the dict. sometimes it's wrong.\n+\tchunk_read_offset = chunk->meta_data.data_page_offset;\n+\tif (chunk->meta_data.__isset.dictionary_page_offset && chunk->meta_data.dictionary_page_offset >= 4) {\n+\t\t// this assumes the data pages follow the dict pages directly.\n+\t\tchunk_read_offset = chunk->meta_data.dictionary_page_offset;\n+\t}\n+\tgroup_rows_available = chunk->meta_data.num_values;\n }\n \n void ColumnReader::PrepareRead(parquet_filter_t &filter) {\n@@ -369,6 +313,20 @@ void ColumnReader::Skip(idx_t num_values) {\n \t}\n }\n \n+//===--------------------------------------------------------------------===//\n+// String Column Reader\n+//===--------------------------------------------------------------------===//\n+StringColumnReader::StringColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,\n+                                       idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p)\n+    : TemplatedColumnReader<string_t, StringParquetValueConversion>(reader, move(type_p), schema_p, schema_idx_p,\n+                                                                    max_define_p, max_repeat_p) {\n+\tfixed_width_string_length = 0;\n+\tif (schema_p.type == Type::FIXED_LEN_BYTE_ARRAY) {\n+\t\tD_ASSERT(schema_p.__isset.type_length);\n+\t\tfixed_width_string_length = schema_p.type_length;\n+\t}\n+}\n+\n uint32_t StringColumnReader::VerifyString(const char *str_data, uint32_t str_len) {\n \tif (Type() != LogicalTypeId::VARCHAR) {\n \t\treturn str_len;\n@@ -437,7 +395,6 @@ string_t StringParquetValueConversion::PlainRead(ByteBuffer &plain_data, ColumnR\n void StringParquetValueConversion::PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {\n \tauto &scr = ((StringColumnReader &)reader);\n \tuint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;\n-\tplain_data.available(str_len);\n \tplain_data.inc(str_len);\n }\n \n@@ -615,4 +572,310 @@ idx_t StructColumnReader::GroupRowsAvailable() {\n \treturn child_readers[0]->GroupRowsAvailable();\n }\n \n+//===--------------------------------------------------------------------===//\n+// Decimal Column Reader\n+//===--------------------------------------------------------------------===//\n+template <class DUCKDB_PHYSICAL_TYPE, bool FIXED_LENGTH>\n+struct DecimalParquetValueConversion {\n+\tstatic DUCKDB_PHYSICAL_TYPE DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {\n+\t\tauto dict_ptr = (DUCKDB_PHYSICAL_TYPE *)dict.ptr;\n+\t\treturn dict_ptr[offset];\n+\t}\n+\n+\tstatic DUCKDB_PHYSICAL_TYPE PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {\n+\t\tidx_t byte_len;\n+\t\tif (FIXED_LENGTH) {\n+\t\t\tbyte_len = (idx_t)reader.Schema().type_length; /* sure, type length needs to be a signed int */\n+\t\t} else {\n+\t\t\tbyte_len = plain_data.read<uint32_t>();\n+\t\t}\n+\t\tplain_data.available(byte_len);\n+\t\tauto res =\n+\t\t    ParquetDecimalUtils::ReadDecimalValue<DUCKDB_PHYSICAL_TYPE>((const_data_ptr_t)plain_data.ptr, byte_len);\n+\n+\t\tplain_data.inc(byte_len);\n+\t\treturn res;\n+\t}\n+\n+\tstatic void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {\n+\t\tuint32_t decimal_len = FIXED_LENGTH ? reader.Schema().type_length : plain_data.read<uint32_t>();\n+\t\tplain_data.inc(decimal_len);\n+\t}\n+};\n+\n+template <class DUCKDB_PHYSICAL_TYPE, bool FIXED_LENGTH>\n+class DecimalColumnReader\n+    : public TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE,\n+                                   DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE, FIXED_LENGTH>> {\n+\n+public:\n+\tDecimalColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, // NOLINT\n+\t                    idx_t file_idx_p, idx_t max_define_p, idx_t max_repeat_p)\n+\t    : TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE,\n+\t                            DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE, FIXED_LENGTH>>(\n+\t          reader, move(type_p), schema_p, file_idx_p, max_define_p, max_repeat_p) {};\n+\n+protected:\n+\tvoid Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) { // NOLINT\n+\t\tthis->dict = make_shared<ResizeableBuffer>(this->reader.allocator, num_entries * sizeof(DUCKDB_PHYSICAL_TYPE));\n+\t\tauto dict_ptr = (DUCKDB_PHYSICAL_TYPE *)this->dict->ptr;\n+\t\tfor (idx_t i = 0; i < num_entries; i++) {\n+\t\t\tdict_ptr[i] =\n+\t\t\t    DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE, FIXED_LENGTH>::PlainRead(*dictionary_data, *this);\n+\t\t}\n+\t}\n+};\n+\n+template <bool FIXED_LENGTH>\n+static unique_ptr<ColumnReader> CreateDecimalReaderInternal(ParquetReader &reader, const LogicalType &type_p,\n+                                                            const SchemaElement &schema_p, idx_t file_idx_p,\n+                                                            idx_t max_define, idx_t max_repeat) {\n+\tswitch (type_p.InternalType()) {\n+\tcase PhysicalType::INT16:\n+\t\treturn make_unique<DecimalColumnReader<int16_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p, max_define,\n+\t\t                                                               max_repeat);\n+\tcase PhysicalType::INT32:\n+\t\treturn make_unique<DecimalColumnReader<int32_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p, max_define,\n+\t\t                                                               max_repeat);\n+\tcase PhysicalType::INT64:\n+\t\treturn make_unique<DecimalColumnReader<int64_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p, max_define,\n+\t\t                                                               max_repeat);\n+\tcase PhysicalType::INT128:\n+\t\treturn make_unique<DecimalColumnReader<hugeint_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p,\n+\t\t                                                                 max_define, max_repeat);\n+\tdefault:\n+\t\tthrow InternalException(\"Unrecognized type for Decimal\");\n+\t}\n+}\n+\n+unique_ptr<ColumnReader> ParquetDecimalUtils::CreateReader(ParquetReader &reader, const LogicalType &type_p,\n+                                                           const SchemaElement &schema_p, idx_t file_idx_p,\n+                                                           idx_t max_define, idx_t max_repeat) {\n+\tif (schema_p.__isset.type_length) {\n+\t\treturn CreateDecimalReaderInternal<true>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\t} else {\n+\t\treturn CreateDecimalReaderInternal<false>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\t}\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// UUID Column Reader\n+//===--------------------------------------------------------------------===//\n+struct UUIDValueConversion {\n+\tstatic hugeint_t DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {\n+\t\tauto dict_ptr = (hugeint_t *)dict.ptr;\n+\t\treturn dict_ptr[offset];\n+\t}\n+\n+\tstatic hugeint_t ReadParquetUUID(const_data_ptr_t input) {\n+\t\thugeint_t result;\n+\t\tresult.lower = 0;\n+\t\tuint64_t unsigned_upper = 0;\n+\t\tfor (idx_t i = 0; i < sizeof(uint64_t); i++) {\n+\t\t\tunsigned_upper <<= 8;\n+\t\t\tunsigned_upper += input[i];\n+\t\t}\n+\t\tfor (idx_t i = sizeof(uint64_t); i < sizeof(hugeint_t); i++) {\n+\t\t\tresult.lower <<= 8;\n+\t\t\tresult.lower += input[i];\n+\t\t}\n+\t\tresult.upper = unsigned_upper;\n+\t\tresult.upper ^= (int64_t(1) << 63);\n+\t\treturn result;\n+\t}\n+\n+\tstatic hugeint_t PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {\n+\t\tidx_t byte_len = sizeof(hugeint_t);\n+\t\tplain_data.available(byte_len);\n+\t\tauto res = ReadParquetUUID((const_data_ptr_t)plain_data.ptr);\n+\n+\t\tplain_data.inc(byte_len);\n+\t\treturn res;\n+\t}\n+\n+\tstatic void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {\n+\t\tplain_data.inc(sizeof(hugeint_t));\n+\t}\n+};\n+\n+class UUIDColumnReader : public TemplatedColumnReader<hugeint_t, UUIDValueConversion> {\n+\n+public:\n+\tUUIDColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,\n+\t                 idx_t max_define_p, idx_t max_repeat_p)\n+\t    : TemplatedColumnReader<hugeint_t, UUIDValueConversion>(reader, move(type_p), schema_p, file_idx_p,\n+\t                                                            max_define_p, max_repeat_p) {};\n+\n+protected:\n+\tvoid Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) { // NOLINT\n+\t\tthis->dict = make_shared<ResizeableBuffer>(this->reader.allocator, num_entries * sizeof(hugeint_t));\n+\t\tauto dict_ptr = (hugeint_t *)this->dict->ptr;\n+\t\tfor (idx_t i = 0; i < num_entries; i++) {\n+\t\t\tdict_ptr[i] = UUIDValueConversion::PlainRead(*dictionary_data, *this);\n+\t\t}\n+\t}\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// Interval Column Reader\n+//===--------------------------------------------------------------------===//\n+struct IntervalValueConversion {\n+\tstatic constexpr const idx_t PARQUET_INTERVAL_SIZE = 12;\n+\n+\tstatic interval_t DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {\n+\t\tauto dict_ptr = (interval_t *)dict.ptr;\n+\t\treturn dict_ptr[offset];\n+\t}\n+\n+\tstatic interval_t ReadParquetInterval(const_data_ptr_t input) {\n+\t\tinterval_t result;\n+\t\tresult.months = Load<uint32_t>(input);\n+\t\tresult.days = Load<uint32_t>(input + sizeof(uint32_t));\n+\t\tresult.micros = int64_t(Load<uint32_t>(input + sizeof(uint32_t) * 2)) * 1000;\n+\t\treturn result;\n+\t}\n+\n+\tstatic interval_t PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {\n+\t\tidx_t byte_len = PARQUET_INTERVAL_SIZE;\n+\t\tplain_data.available(byte_len);\n+\t\tauto res = ReadParquetInterval((const_data_ptr_t)plain_data.ptr);\n+\n+\t\tplain_data.inc(byte_len);\n+\t\treturn res;\n+\t}\n+\n+\tstatic void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {\n+\t\tplain_data.inc(PARQUET_INTERVAL_SIZE);\n+\t}\n+};\n+\n+class IntervalColumnReader : public TemplatedColumnReader<interval_t, IntervalValueConversion> {\n+\n+public:\n+\tIntervalColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,\n+\t                     idx_t max_define_p, idx_t max_repeat_p)\n+\t    : TemplatedColumnReader<interval_t, IntervalValueConversion>(reader, move(type_p), schema_p, file_idx_p,\n+\t                                                                 max_define_p, max_repeat_p) {};\n+\n+protected:\n+\tvoid Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) override { // NOLINT\n+\t\tthis->dict = make_shared<ResizeableBuffer>(this->reader.allocator, num_entries * sizeof(interval_t));\n+\t\tauto dict_ptr = (interval_t *)this->dict->ptr;\n+\t\tfor (idx_t i = 0; i < num_entries; i++) {\n+\t\t\tdict_ptr[i] = IntervalValueConversion::PlainRead(*dictionary_data, *this);\n+\t\t}\n+\t}\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// Create Column Reader\n+//===--------------------------------------------------------------------===//\n+template <class T>\n+unique_ptr<ColumnReader> CreateDecimalReader(ParquetReader &reader, const LogicalType &type_p,\n+                                             const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,\n+                                             idx_t max_repeat) {\n+\tswitch (type_p.InternalType()) {\n+\tcase PhysicalType::INT16:\n+\t\treturn make_unique<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<T>>>(\n+\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase PhysicalType::INT32:\n+\t\treturn make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<T>>>(\n+\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase PhysicalType::INT64:\n+\t\treturn make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<T>>>(\n+\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tdefault:\n+\t\tthrow NotImplementedException(\"Unimplemented internal type for CreateDecimalReader\");\n+\t}\n+}\n+\n+unique_ptr<ColumnReader> ColumnReader::CreateReader(ParquetReader &reader, const LogicalType &type_p,\n+                                                    const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,\n+                                                    idx_t max_repeat) {\n+\tswitch (type_p.id()) {\n+\tcase LogicalTypeId::BOOLEAN:\n+\t\treturn make_unique<BooleanColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase LogicalTypeId::UTINYINT:\n+\t\treturn make_unique<TemplatedColumnReader<uint8_t, TemplatedParquetValueConversion<uint32_t>>>(\n+\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase LogicalTypeId::USMALLINT:\n+\t\treturn make_unique<TemplatedColumnReader<uint16_t, TemplatedParquetValueConversion<uint32_t>>>(\n+\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase LogicalTypeId::UINTEGER:\n+\t\treturn make_unique<TemplatedColumnReader<uint32_t, TemplatedParquetValueConversion<uint32_t>>>(\n+\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase LogicalTypeId::UBIGINT:\n+\t\treturn make_unique<TemplatedColumnReader<uint64_t, TemplatedParquetValueConversion<uint64_t>>>(\n+\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase LogicalTypeId::TINYINT:\n+\t\treturn make_unique<TemplatedColumnReader<int8_t, TemplatedParquetValueConversion<int32_t>>>(\n+\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase LogicalTypeId::SMALLINT:\n+\t\treturn make_unique<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<int32_t>>>(\n+\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase LogicalTypeId::INTEGER:\n+\t\treturn make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<int32_t>>>(\n+\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase LogicalTypeId::BIGINT:\n+\t\treturn make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<int64_t>>>(\n+\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase LogicalTypeId::FLOAT:\n+\t\treturn make_unique<TemplatedColumnReader<float, TemplatedParquetValueConversion<float>>>(\n+\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase LogicalTypeId::DOUBLE:\n+\t\treturn make_unique<TemplatedColumnReader<double, TemplatedParquetValueConversion<double>>>(\n+\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase LogicalTypeId::TIMESTAMP:\n+\t\tswitch (schema_p.type) {\n+\t\tcase Type::INT96:\n+\t\t\treturn make_unique<CallbackColumnReader<Int96, timestamp_t, ImpalaTimestampToTimestamp>>(\n+\t\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\t\tcase Type::INT64:\n+\t\t\tswitch (schema_p.converted_type) {\n+\t\t\tcase ConvertedType::TIMESTAMP_MICROS:\n+\t\t\t\treturn make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMicrosToTimestamp>>(\n+\t\t\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\t\t\tcase ConvertedType::TIMESTAMP_MILLIS:\n+\t\t\t\treturn make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMsToTimestamp>>(\n+\t\t\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\t\t\tdefault:\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\tdefault:\n+\t\t\tbreak;\n+\t\t}\n+\t\tbreak;\n+\tcase LogicalTypeId::DATE:\n+\t\treturn make_unique<CallbackColumnReader<int32_t, date_t, ParquetIntToDate>>(reader, type_p, schema_p,\n+\t\t                                                                            file_idx_p, max_define, max_repeat);\n+\tcase LogicalTypeId::TIME:\n+\t\treturn make_unique<CallbackColumnReader<int64_t, dtime_t, ParquetIntToTime>>(\n+\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase LogicalTypeId::BLOB:\n+\tcase LogicalTypeId::VARCHAR:\n+\t\treturn make_unique<StringColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase LogicalTypeId::DECIMAL:\n+\t\t// we have to figure out what kind of int we need\n+\t\tswitch (schema_p.type) {\n+\t\tcase Type::INT32:\n+\t\t\treturn CreateDecimalReader<int32_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\t\tcase Type::INT64:\n+\t\t\treturn CreateDecimalReader<int64_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\t\tcase Type::BYTE_ARRAY:\n+\t\tcase Type::FIXED_LEN_BYTE_ARRAY:\n+\t\t\treturn ParquetDecimalUtils::CreateReader(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\t\tdefault:\n+\t\t\tthrow NotImplementedException(\"Unrecognized Parquet type for Decimal\");\n+\t\t}\n+\t\tbreak;\n+\tcase LogicalTypeId::UUID:\n+\t\treturn make_unique<UUIDColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase LogicalTypeId::INTERVAL:\n+\t\treturn make_unique<IntervalColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tdefault:\n+\t\tbreak;\n+\t}\n+\tthrow NotImplementedException(type_p.ToString());\n+}\n+\n } // namespace duckdb\ndiff --git a/extension/parquet/column_writer.cpp b/extension/parquet/column_writer.cpp\nindex 6d3c5d6883b9..8d0f0ce27f05 100644\n--- a/extension/parquet/column_writer.cpp\n+++ b/extension/parquet/column_writer.cpp\n@@ -1,6 +1,7 @@\n #include \"column_writer.hpp\"\n #include \"parquet_writer.hpp\"\n #include \"parquet_rle_bp_decoder.hpp\"\n+#include \"parquet_rle_bp_encoder.hpp\"\n \n #include \"duckdb.hpp\"\n #ifndef DUCKDB_AMALGAMATION\n@@ -14,6 +15,7 @@\n #include \"duckdb/common/types/time.hpp\"\n #include \"duckdb/common/types/timestamp.hpp\"\n #include \"duckdb/common/serializer/buffered_serializer.hpp\"\n+#include \"duckdb/common/operator/comparison_operators.hpp\"\n #endif\n \n #include \"snappy.h\"\n@@ -37,20 +39,6 @@ using duckdb_parquet::format::Type;\n \n #define PARQUET_DEFINE_VALID 65535\n \n-//===--------------------------------------------------------------------===//\n-// ColumnWriter\n-//===--------------------------------------------------------------------===//\n-ColumnWriter::ColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define,\n-                           bool can_have_nulls)\n-    : writer(writer), schema_idx(schema_idx), max_repeat(max_repeat), max_define(max_define),\n-      can_have_nulls(can_have_nulls) {\n-}\n-ColumnWriter::~ColumnWriter() {\n-}\n-\n-ColumnWriterState::~ColumnWriterState() {\n-}\n-\n static void VarintEncode(uint32_t val, Serializer &ser) {\n \tdo {\n \t\tuint8_t byte = val & 127;\n@@ -75,6 +63,127 @@ static uint8_t GetVarintSize(uint32_t val) {\n \treturn res;\n }\n \n+//===--------------------------------------------------------------------===//\n+// ColumnWriterStatistics\n+//===--------------------------------------------------------------------===//\n+ColumnWriterStatistics::~ColumnWriterStatistics() {\n+}\n+\n+string ColumnWriterStatistics::GetMin() {\n+\treturn string();\n+}\n+\n+string ColumnWriterStatistics::GetMax() {\n+\treturn string();\n+}\n+\n+string ColumnWriterStatistics::GetMinValue() {\n+\treturn string();\n+}\n+\n+string ColumnWriterStatistics::GetMaxValue() {\n+\treturn string();\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// RleBpEncoder\n+//===--------------------------------------------------------------------===//\n+RleBpEncoder::RleBpEncoder(uint32_t bit_width)\n+    : byte_width((bit_width + 7) / 8), byte_count(idx_t(-1)), run_count(idx_t(-1)) {\n+}\n+\n+// we always RLE everything (for now)\n+void RleBpEncoder::BeginPrepare(uint32_t first_value) {\n+\tbyte_count = 0;\n+\trun_count = 1;\n+\tcurrent_run_count = 1;\n+\tlast_value = first_value;\n+}\n+\n+void RleBpEncoder::FinishRun() {\n+\t// last value, or value has changed\n+\t// write out the current run\n+\tbyte_count += GetVarintSize(current_run_count << 1) + byte_width;\n+\tcurrent_run_count = 1;\n+\trun_count++;\n+}\n+\n+void RleBpEncoder::PrepareValue(uint32_t value) {\n+\tif (value != last_value) {\n+\t\tFinishRun();\n+\t\tlast_value = value;\n+\t} else {\n+\t\tcurrent_run_count++;\n+\t}\n+}\n+\n+void RleBpEncoder::FinishPrepare() {\n+\tFinishRun();\n+}\n+\n+idx_t RleBpEncoder::GetByteCount() {\n+\tD_ASSERT(byte_count != idx_t(-1));\n+\treturn byte_count;\n+}\n+\n+void RleBpEncoder::BeginWrite(Serializer &writer, uint32_t first_value) {\n+\t// start the RLE runs\n+\tlast_value = first_value;\n+\tcurrent_run_count = 1;\n+}\n+\n+void RleBpEncoder::WriteRun(Serializer &writer) {\n+\t// write the header of the run\n+\tVarintEncode(current_run_count << 1, writer);\n+\t// now write the value\n+\tswitch (byte_width) {\n+\tcase 1:\n+\t\twriter.Write<uint8_t>(last_value);\n+\t\tbreak;\n+\tcase 2:\n+\t\twriter.Write<uint16_t>(last_value);\n+\t\tbreak;\n+\tcase 3:\n+\t\twriter.Write<uint8_t>(last_value & 0xFF);\n+\t\twriter.Write<uint8_t>((last_value >> 8) & 0xFF);\n+\t\twriter.Write<uint8_t>((last_value >> 16) & 0xFF);\n+\t\tbreak;\n+\tcase 4:\n+\t\twriter.Write<uint32_t>(last_value);\n+\t\tbreak;\n+\tdefault:\n+\t\tthrow InternalException(\"unsupported byte width for RLE encoding\");\n+\t}\n+\tcurrent_run_count = 1;\n+}\n+\n+void RleBpEncoder::WriteValue(Serializer &writer, uint32_t value) {\n+\tif (value != last_value) {\n+\t\tWriteRun(writer);\n+\t\tlast_value = value;\n+\t} else {\n+\t\tcurrent_run_count++;\n+\t}\n+}\n+\n+void RleBpEncoder::FinishWrite(Serializer &writer) {\n+\tWriteRun(writer);\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// ColumnWriter\n+//===--------------------------------------------------------------------===//\n+ColumnWriter::ColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,\n+                           idx_t max_define, bool can_have_nulls)\n+    : writer(writer), schema_idx(schema_idx), schema_path(move(schema_path_p)), max_repeat(max_repeat),\n+      max_define(max_define), can_have_nulls(can_have_nulls), null_count(0) {\n+}\n+ColumnWriter::~ColumnWriter() {\n+}\n+\n+ColumnWriterState::~ColumnWriterState() {\n+}\n+\n void ColumnWriter::CompressPage(BufferedSerializer &temp_writer, size_t &compressed_size, data_ptr_t &compressed_data,\n                                 unique_ptr<data_t[]> &compressed_buf) {\n \tswitch (writer.codec) {\n@@ -156,18 +265,17 @@ class StandardColumnWriterState : public ColumnWriterState {\n \tidx_t col_idx;\n \tvector<PageInformation> page_info;\n \tvector<PageWriteInformation> write_info;\n+\tunique_ptr<ColumnWriterStatistics> stats_state;\n \tidx_t current_page = 0;\n };\n \n-unique_ptr<ColumnWriterState> ColumnWriter::InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,\n-                                                                 vector<string> schema_path) {\n+unique_ptr<ColumnWriterState> ColumnWriter::InitializeWriteState(duckdb_parquet::format::RowGroup &row_group) {\n \tauto result = make_unique<StandardColumnWriterState>(row_group, row_group.columns.size());\n \n \tduckdb_parquet::format::ColumnChunk column_chunk;\n \tcolumn_chunk.__isset.meta_data = true;\n \tcolumn_chunk.meta_data.codec = writer.codec;\n-\tcolumn_chunk.meta_data.path_in_schema = move(schema_path);\n-\tcolumn_chunk.meta_data.path_in_schema.push_back(writer.file_meta_data.schema[schema_idx].name);\n+\tcolumn_chunk.meta_data.path_in_schema = schema_path;\n \tcolumn_chunk.meta_data.num_values = 0;\n \tcolumn_chunk.meta_data.type = writer.file_meta_data.schema[schema_idx].type;\n \trow_group.columns.push_back(move(column_chunk));\n@@ -201,6 +309,7 @@ void ColumnWriter::HandleDefineLevels(ColumnWriterState &state, ColumnWriterStat\n \t\t\t\tif (!can_have_nulls) {\n \t\t\t\t\tthrow IOException(\"Parquet writer: map key column is not allowed to contain NULL values\");\n \t\t\t\t}\n+\t\t\t\tnull_count++;\n \t\t\t\tstate.definition_levels.push_back(null_value);\n \t\t\t}\n \t\t\tif (parent->is_empty.empty() || !parent->is_empty[current_index]) {\n@@ -216,6 +325,7 @@ void ColumnWriter::HandleDefineLevels(ColumnWriterState &state, ColumnWriterStat\n \t\t\t\tif (!can_have_nulls) {\n \t\t\t\t\tthrow IOException(\"Parquet writer: map key column is not allowed to contain NULL values\");\n \t\t\t\t}\n+\t\t\t\tnull_count++;\n \t\t\t\tstate.definition_levels.push_back(null_value);\n \t\t\t}\n \t\t}\n@@ -261,10 +371,15 @@ unique_ptr<ColumnWriterPageState> ColumnWriter::InitializePageState() {\n void ColumnWriter::FlushPageState(Serializer &temp_writer, ColumnWriterPageState *state) {\n }\n \n+duckdb_parquet::format::Encoding::type ColumnWriter::GetEncoding() {\n+\treturn Encoding::PLAIN;\n+}\n+\n void ColumnWriter::BeginWrite(ColumnWriterState &state_p) {\n \tauto &state = (StandardColumnWriterState &)state_p;\n \n \t// set up the page write info\n+\tstate.stats_state = InitializeStatsState();\n \tfor (idx_t page_idx = 0; page_idx < state.page_info.size(); page_idx++) {\n \t\tauto &page_info = state.page_info[page_idx];\n \t\tif (page_info.row_count == 0) {\n@@ -281,7 +396,7 @@ void ColumnWriter::BeginWrite(ColumnWriterState &state_p) {\n \t\thdr.__isset.data_page_header = true;\n \n \t\thdr.data_page_header.num_values = page_info.row_count;\n-\t\thdr.data_page_header.encoding = Encoding::PLAIN;\n+\t\thdr.data_page_header.encoding = GetEncoding();\n \t\thdr.data_page_header.definition_level_encoding = Encoding::RLE;\n \t\thdr.data_page_header.repetition_level_encoding = Encoding::RLE;\n \n@@ -306,51 +421,23 @@ void ColumnWriter::WriteLevels(Serializer &temp_writer, const vector<uint16_t> &\n \t\treturn;\n \t}\n \n-\t// write the levels\n-\t// we always RLE everything (for now)\n+\t// write the levels using the RLE-BP encoding\n \tauto bit_width = RleBpDecoder::ComputeBitWidth((max_value));\n-\tauto byte_width = (bit_width + 7) / 8;\n-\n-\t// figure out how many bytes we are going to need\n-\tidx_t byte_count = 0;\n-\tidx_t run_count = 1;\n-\tidx_t current_run_count = 1;\n-\tfor (idx_t i = offset + 1; i <= offset + count; i++) {\n-\t\tif (i == offset + count || levels[i] != levels[i - 1]) {\n-\t\t\t// last value, or value has changed\n-\t\t\t// write out the current run\n-\t\t\tbyte_count += GetVarintSize(current_run_count << 1) + byte_width;\n-\t\t\tcurrent_run_count = 1;\n-\t\t\trun_count++;\n-\t\t} else {\n-\t\t\tcurrent_run_count++;\n-\t\t}\n+\tRleBpEncoder rle_encoder(bit_width);\n+\n+\trle_encoder.BeginPrepare(levels[offset]);\n+\tfor (idx_t i = offset + 1; i < offset + count; i++) {\n+\t\trle_encoder.PrepareValue(levels[i]);\n \t}\n-\ttemp_writer.Write<uint32_t>(byte_count);\n+\trle_encoder.FinishPrepare();\n \n-\t// now actually write the values\n-\tcurrent_run_count = 1;\n-\tfor (idx_t i = offset + 1; i <= offset + count; i++) {\n-\t\tif (i == offset + count || levels[i] != levels[i - 1]) {\n-\t\t\t// new run: write out the old run\n-\t\t\t// first write the header\n-\t\t\tVarintEncode(current_run_count << 1, temp_writer);\n-\t\t\t// now write hte value\n-\t\t\tswitch (byte_width) {\n-\t\t\tcase 1:\n-\t\t\t\ttemp_writer.Write<uint8_t>(levels[i - 1]);\n-\t\t\t\tbreak;\n-\t\t\tcase 2:\n-\t\t\t\ttemp_writer.Write<uint16_t>(levels[i - 1]);\n-\t\t\t\tbreak;\n-\t\t\tdefault:\n-\t\t\t\tthrow InternalException(\"unsupported byte width for RLE encoding\");\n-\t\t\t}\n-\t\t\tcurrent_run_count = 1;\n-\t\t} else {\n-\t\t\tcurrent_run_count++;\n-\t\t}\n+\t// start off by writing the byte count as a uint32_t\n+\ttemp_writer.Write<uint32_t>(rle_encoder.GetByteCount());\n+\trle_encoder.BeginWrite(temp_writer, levels[offset]);\n+\tfor (idx_t i = offset + 1; i < offset + count; i++) {\n+\t\trle_encoder.WriteValue(temp_writer, levels[i]);\n \t}\n+\trle_encoder.FinishWrite(temp_writer);\n }\n \n void ColumnWriter::NextPage(ColumnWriterState &state_p) {\n@@ -411,6 +498,20 @@ void ColumnWriter::FlushPage(ColumnWriterState &state_p) {\n \t}\n }\n \n+unique_ptr<ColumnWriterStatistics> ColumnWriter::InitializeStatsState() {\n+\treturn make_unique<ColumnWriterStatistics>();\n+}\n+\n+void ColumnWriter::WriteVector(Serializer &temp_writer, ColumnWriterStatistics *stats,\n+                               ColumnWriterPageState *page_state, Vector &input_column, idx_t chunk_start,\n+                               idx_t chunk_end) {\n+\tthrow InternalException(\"WriteVector unsupported for struct/list column writers\");\n+}\n+\n+idx_t ColumnWriter::GetRowSize(Vector &vector, idx_t index) {\n+\tthrow InternalException(\"GetRowSize unsupported for struct/list column writers\");\n+}\n+\n void ColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count) {\n \tauto &state = (StandardColumnWriterState &)state_p;\n \n@@ -425,7 +526,8 @@ void ColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count\n \t\tidx_t write_count = MinValue<idx_t>(remaining, write_info.max_write_count - write_info.write_count);\n \t\tD_ASSERT(write_count > 0);\n \n-\t\tWriteVector(temp_writer, write_info.page_state.get(), vector, offset, offset + write_count);\n+\t\tWriteVector(temp_writer, state.stats_state.get(), write_info.page_state.get(), vector, offset,\n+\t\t            offset + write_count);\n \n \t\twrite_info.write_count += write_count;\n \t\tif (write_info.write_count == write_info.max_write_count) {\n@@ -436,14 +538,54 @@ void ColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count\n \t}\n }\n \n+void ColumnWriter::SetParquetStatistics(StandardColumnWriterState &state,\n+                                        duckdb_parquet::format::ColumnChunk &column_chunk) {\n+\tif (max_repeat == 0) {\n+\t\tcolumn_chunk.meta_data.statistics.null_count = null_count;\n+\t\tcolumn_chunk.meta_data.statistics.__isset.null_count = true;\n+\t\tcolumn_chunk.meta_data.__isset.statistics = true;\n+\t}\n+\t// set min/max/min_value/max_value\n+\t// this code is not going to win any beauty contests, but well\n+\tauto min = state.stats_state->GetMin();\n+\tif (!min.empty()) {\n+\t\tcolumn_chunk.meta_data.statistics.min = move(min);\n+\t\tcolumn_chunk.meta_data.statistics.__isset.min = true;\n+\t\tcolumn_chunk.meta_data.__isset.statistics = true;\n+\t}\n+\tauto max = state.stats_state->GetMax();\n+\tif (!max.empty()) {\n+\t\tcolumn_chunk.meta_data.statistics.max = move(max);\n+\t\tcolumn_chunk.meta_data.statistics.__isset.max = true;\n+\t\tcolumn_chunk.meta_data.__isset.statistics = true;\n+\t}\n+\tauto min_value = state.stats_state->GetMinValue();\n+\tif (!min_value.empty()) {\n+\t\tcolumn_chunk.meta_data.statistics.min_value = move(min_value);\n+\t\tcolumn_chunk.meta_data.statistics.__isset.min_value = true;\n+\t\tcolumn_chunk.meta_data.__isset.statistics = true;\n+\t}\n+\tauto max_value = state.stats_state->GetMaxValue();\n+\tif (!max_value.empty()) {\n+\t\tcolumn_chunk.meta_data.statistics.max_value = move(max_value);\n+\t\tcolumn_chunk.meta_data.statistics.__isset.max_value = true;\n+\t\tcolumn_chunk.meta_data.__isset.statistics = true;\n+\t}\n+}\n+\n void ColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {\n \tauto &state = (StandardColumnWriterState &)state_p;\n \tauto &column_chunk = state.row_group.columns[state.col_idx];\n \n \t// flush the last page (if any remains)\n \tFlushPage(state);\n+\t// flush the dictionary\n+\tFlushDictionary(state, state.stats_state.get());\n+\n \t// record the start position of the pages for this column\n \tcolumn_chunk.meta_data.data_page_offset = writer.writer->GetTotalWritten();\n+\tSetParquetStatistics(state, column_chunk);\n+\n \t// write the individual pages to disk\n \tfor (auto &write_info : state.write_info) {\n \t\tD_ASSERT(write_info.page_header.uncompressed_page_size > 0);\n@@ -454,24 +596,105 @@ void ColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {\n \t    writer.writer->GetTotalWritten() - column_chunk.meta_data.data_page_offset;\n }\n \n+void ColumnWriter::WriteDictionary(ColumnWriterState &state_p, unique_ptr<BufferedSerializer> temp_writer,\n+                                   idx_t row_count) {\n+\tauto &state = (StandardColumnWriterState &)state_p;\n+\tD_ASSERT(temp_writer);\n+\tD_ASSERT(temp_writer->blob.size > 0);\n+\n+\t// write the dictionary page header\n+\tPageWriteInformation write_info;\n+\t// set up the header\n+\tauto &hdr = write_info.page_header;\n+\thdr.uncompressed_page_size = temp_writer->blob.size;\n+\thdr.type = PageType::DICTIONARY_PAGE;\n+\thdr.__isset.dictionary_page_header = true;\n+\n+\thdr.dictionary_page_header.encoding = Encoding::PLAIN;\n+\thdr.dictionary_page_header.is_sorted = false;\n+\thdr.dictionary_page_header.num_values = row_count;\n+\n+\twrite_info.temp_writer = move(temp_writer);\n+\twrite_info.write_count = 0;\n+\twrite_info.max_write_count = 0;\n+\n+\t// compress the contents of the dictionary page\n+\tCompressPage(*write_info.temp_writer, write_info.compressed_size, write_info.compressed_data,\n+\t             write_info.compressed_buf);\n+\thdr.compressed_page_size = write_info.compressed_size;\n+\n+\t// insert the dictionary page as the first page to write for this column\n+\tstate.write_info.insert(state.write_info.begin(), move(write_info));\n+}\n+\n+void ColumnWriter::FlushDictionary(ColumnWriterState &state, ColumnWriterStatistics *stats) {\n+\t// nop: standard pages do not have a dictionary\n+}\n+\n //===--------------------------------------------------------------------===//\n // Standard Column Writer\n //===--------------------------------------------------------------------===//\n-struct ParquetCastOperator {\n+template <class SRC, class T, class OP>\n+class NumericStatisticsState : public ColumnWriterStatistics {\n+public:\n+\tNumericStatisticsState() : min(NumericLimits<T>::Maximum()), max(NumericLimits<T>::Minimum()) {\n+\t}\n+\n+\tT min;\n+\tT max;\n+\n+public:\n+\tbool HasStats() {\n+\t\treturn min <= max;\n+\t}\n+\n+\tstring GetMin() override {\n+\t\treturn NumericLimits<SRC>::IsSigned() ? GetMinValue() : string();\n+\t}\n+\tstring GetMax() override {\n+\t\treturn NumericLimits<SRC>::IsSigned() ? GetMaxValue() : string();\n+\t}\n+\tstring GetMinValue() override {\n+\t\treturn HasStats() ? string((char *)&min, sizeof(T)) : string();\n+\t}\n+\tstring GetMaxValue() override {\n+\t\treturn HasStats() ? string((char *)&max, sizeof(T)) : string();\n+\t}\n+};\n+\n+struct BaseParquetOperator {\n+\ttemplate <class SRC, class TGT>\n+\tstatic unique_ptr<ColumnWriterStatistics> InitializeStats() {\n+\t\treturn make_unique<NumericStatisticsState<SRC, TGT, BaseParquetOperator>>();\n+\t}\n+\n+\ttemplate <class SRC, class TGT>\n+\tstatic void HandleStats(ColumnWriterStatistics *stats, SRC source_value, TGT target_value) {\n+\t\tauto &numeric_stats = (NumericStatisticsState<SRC, TGT, BaseParquetOperator> &)*stats;\n+\t\tif (LessThan::Operation(target_value, numeric_stats.min)) {\n+\t\t\tnumeric_stats.min = target_value;\n+\t\t}\n+\t\tif (GreaterThan::Operation(target_value, numeric_stats.max)) {\n+\t\t\tnumeric_stats.max = target_value;\n+\t\t}\n+\t}\n+};\n+\n+struct ParquetCastOperator : public BaseParquetOperator {\n \ttemplate <class SRC, class TGT>\n \tstatic TGT Operation(SRC input) {\n \t\treturn TGT(input);\n \t}\n };\n \n-struct ParquetTimestampNSOperator {\n+struct ParquetTimestampNSOperator : public BaseParquetOperator {\n \ttemplate <class SRC, class TGT>\n \tstatic TGT Operation(SRC input) {\n \t\treturn Timestamp::FromEpochNanoSeconds(input).value;\n \t}\n };\n \n-struct ParquetTimestampSOperator {\n+struct ParquetTimestampSOperator : public BaseParquetOperator {\n \ttemplate <class SRC, class TGT>\n \tstatic TGT Operation(SRC input) {\n \t\treturn Timestamp::FromEpochSeconds(input).value;\n@@ -483,14 +706,26 @@ struct ParquetHugeintOperator {\n \tstatic TGT Operation(SRC input) {\n \t\treturn Hugeint::Cast<double>(input);\n \t}\n+\n+\ttemplate <class SRC, class TGT>\n+\tstatic unique_ptr<ColumnWriterStatistics> InitializeStats() {\n+\t\treturn make_unique<ColumnWriterStatistics>();\n+\t}\n+\n+\ttemplate <class SRC, class TGT>\n+\tstatic void HandleStats(ColumnWriterStatistics *stats, SRC source_value, TGT target_value) {\n+\t}\n };\n \n template <class SRC, class TGT, class OP = ParquetCastOperator>\n-static void TemplatedWritePlain(Vector &col, idx_t chunk_start, idx_t chunk_end, ValidityMask &mask, Serializer &ser) {\n+static void TemplatedWritePlain(Vector &col, ColumnWriterStatistics *stats, idx_t chunk_start, idx_t chunk_end,\n+                                ValidityMask &mask, Serializer &ser) {\n \tauto *ptr = FlatVector::GetData<SRC>(col);\n \tfor (idx_t r = chunk_start; r < chunk_end; r++) {\n \t\tif (mask.RowIsValid(r)) {\n-\t\t\tser.Write<TGT>(OP::template Operation<SRC, TGT>(ptr[r]));\n+\t\t\tTGT target_value = OP::template Operation<SRC, TGT>(ptr[r]);\n+\t\t\tOP::template HandleStats<SRC, TGT>(stats, ptr[r], target_value);\n+\t\t\tser.Write<TGT>(target_value);\n \t\t}\n \t}\n }\n@@ -498,17 +733,21 @@ static void TemplatedWritePlain(Vector &col, idx_t chunk_start, idx_t chunk_end,\n template <class SRC, class TGT, class OP = ParquetCastOperator>\n class StandardColumnWriter : public ColumnWriter {\n public:\n-\tStandardColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define,\n-\t                     bool can_have_nulls)\n-\t    : ColumnWriter(writer, schema_idx, max_repeat, max_define, can_have_nulls) {\n+\tStandardColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, // NOLINT\n+\t                     idx_t max_repeat, idx_t max_define, bool can_have_nulls)\n+\t    : ColumnWriter(writer, schema_idx, move(schema_path_p), max_repeat, max_define, can_have_nulls) {\n \t}\n \t~StandardColumnWriter() override = default;\n \n public:\n-\tvoid WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,\n-\t                 idx_t chunk_start, idx_t chunk_end) override {\n+\tunique_ptr<ColumnWriterStatistics> InitializeStatsState() override {\n+\t\treturn OP::template InitializeStats<SRC, TGT>();\n+\t}\n+\n+\tvoid WriteVector(Serializer &temp_writer, ColumnWriterStatistics *stats, ColumnWriterPageState *page_state,\n+\t                 Vector &input_column, idx_t chunk_start, idx_t chunk_end) override {\n \t\tauto &mask = FlatVector::Validity(input_column);\n-\t\tTemplatedWritePlain<SRC, TGT, OP>(input_column, chunk_start, chunk_end, mask, temp_writer);\n+\t\tTemplatedWritePlain<SRC, TGT, OP>(input_column, stats, chunk_start, chunk_end, mask, temp_writer);\n \t}\n \n \tidx_t GetRowSize(Vector &vector, idx_t index) override {\n@@ -519,6 +758,33 @@ class StandardColumnWriter : public ColumnWriter {\n //===--------------------------------------------------------------------===//\n // Boolean Column Writer\n //===--------------------------------------------------------------------===//\n+class BooleanStatisticsState : public ColumnWriterStatistics {\n+public:\n+\tBooleanStatisticsState() : min(true), max(false) {\n+\t}\n+\n+\tbool min;\n+\tbool max;\n+\n+public:\n+\tbool HasStats() {\n+\t\treturn !(min && !max);\n+\t}\n+\n+\tstring GetMin() override {\n+\t\treturn GetMinValue();\n+\t}\n+\tstring GetMax() override {\n+\t\treturn GetMaxValue();\n+\t}\n+\tstring GetMinValue() override {\n+\t\treturn HasStats() ? string((char *)&min, sizeof(bool)) : string();\n+\t}\n+\tstring GetMaxValue() override {\n+\t\treturn HasStats() ? string((char *)&max, sizeof(bool)) : string();\n+\t}\n+};\n+\n class BooleanWriterPageState : public ColumnWriterPageState {\n public:\n \tuint8_t byte = 0;\n@@ -527,15 +793,20 @@ class BooleanWriterPageState : public ColumnWriterPageState {\n \n class BooleanColumnWriter : public ColumnWriter {\n public:\n-\tBooleanColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define,\n-\t                    bool can_have_nulls)\n-\t    : ColumnWriter(writer, schema_idx, max_repeat, max_define, can_have_nulls) {\n+\tBooleanColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,\n+\t                    idx_t max_define, bool can_have_nulls)\n+\t    : ColumnWriter(writer, schema_idx, move(schema_path_p), max_repeat, max_define, can_have_nulls) {\n \t}\n \t~BooleanColumnWriter() override = default;\n \n public:\n-\tvoid WriteVector(Serializer &temp_writer, ColumnWriterPageState *state_p, Vector &input_column, idx_t chunk_start,\n-\t                 idx_t chunk_end) override {\n+\tunique_ptr<ColumnWriterStatistics> InitializeStatsState() override {\n+\t\treturn make_unique<BooleanStatisticsState>();\n+\t}\n+\n+\tvoid WriteVector(Serializer &temp_writer, ColumnWriterStatistics *stats_p, ColumnWriterPageState *state_p,\n+\t                 Vector &input_column, idx_t chunk_start, idx_t chunk_end) override {\n+\t\tauto &stats = (BooleanStatisticsState &)*stats_p;\n \t\tauto &state = (BooleanWriterPageState &)*state_p;\n \t\tauto &mask = FlatVector::Validity(input_column);\n \n@@ -544,7 +815,10 @@ class BooleanColumnWriter : public ColumnWriter {\n \t\t\tif (mask.RowIsValid(r)) {\n \t\t\t\t// only encode if non-null\n \t\t\t\tif (ptr[r]) {\n+\t\t\t\t\tstats.max = true;\n \t\t\t\t\tstate.byte |= 1 << state.byte_pos;\n+\t\t\t\t} else {\n+\t\t\t\t\tstats.min = false;\n \t\t\t\t}\n \t\t\t\tstate.byte_pos++;\n \n@@ -578,48 +852,273 @@ class BooleanColumnWriter : public ColumnWriter {\n //===--------------------------------------------------------------------===//\n // Decimal Column Writer\n //===--------------------------------------------------------------------===//\n-class DecimalColumnWriter : public ColumnWriter {\n+static void WriteParquetDecimal(hugeint_t input, data_ptr_t result) {\n+\tbool positive = input >= 0;\n+\t// numbers are stored as two's complement so some muckery is required\n+\tif (!positive) {\n+\t\tinput = NumericLimits<hugeint_t>::Maximum() + input + 1;\n+\t}\n+\tuint64_t high_bytes = uint64_t(input.upper);\n+\tuint64_t low_bytes = input.lower;\n+\n+\tfor (idx_t i = 0; i < sizeof(uint64_t); i++) {\n+\t\tauto shift_count = (sizeof(uint64_t) - i - 1) * 8;\n+\t\tresult[i] = (high_bytes >> shift_count) & 0xFF;\n+\t}\n+\tfor (idx_t i = 0; i < sizeof(uint64_t); i++) {\n+\t\tauto shift_count = (sizeof(uint64_t) - i - 1) * 8;\n+\t\tresult[sizeof(uint64_t) + i] = (low_bytes >> shift_count) & 0xFF;\n+\t}\n+\tif (!positive) {\n+\t\tresult[0] |= 0x80;\n+\t}\n+}\n+\n+class FixedDecimalStatistics : public ColumnWriterStatistics {\n public:\n-\tDecimalColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define,\n-\t                    bool can_have_nulls)\n-\t    : ColumnWriter(writer, schema_idx, max_repeat, max_define, can_have_nulls) {\n+\tFixedDecimalStatistics() : min(NumericLimits<hugeint_t>::Maximum()), max(NumericLimits<hugeint_t>::Minimum()) {\n \t}\n-\t~DecimalColumnWriter() override = default;\n+\n+\thugeint_t min;\n+\thugeint_t max;\n+\n+public:\n+\tstring GetStats(hugeint_t &input) {\n+\t\tdata_t buffer[16];\n+\t\tWriteParquetDecimal(input, buffer);\n+\t\treturn string((char *)buffer, 16);\n+\t}\n+\n+\tbool HasStats() {\n+\t\treturn min <= max;\n+\t}\n+\n+\tvoid Update(hugeint_t &val) {\n+\t\tif (LessThan::Operation(val, min)) {\n+\t\t\tmin = val;\n+\t\t}\n+\t\tif (GreaterThan::Operation(val, max)) {\n+\t\t\tmax = val;\n+\t\t}\n+\t}\n+\n+\tstring GetMin() override {\n+\t\treturn GetMinValue();\n+\t}\n+\tstring GetMax() override {\n+\t\treturn GetMaxValue();\n+\t}\n+\tstring GetMinValue() override {\n+\t\treturn HasStats() ? GetStats(min) : string();\n+\t}\n+\tstring GetMaxValue() override {\n+\t\treturn HasStats() ? GetStats(max) : string();\n+\t}\n+};\n+\n+class FixedDecimalColumnWriter : public ColumnWriter {\n+public:\n+\tFixedDecimalColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,\n+\t                         idx_t max_define, bool can_have_nulls)\n+\t    : ColumnWriter(writer, schema_idx, move(schema_path_p), max_repeat, max_define, can_have_nulls) {\n+\t}\n+\t~FixedDecimalColumnWriter() override = default;\n+\n+public:\n+\tunique_ptr<ColumnWriterStatistics> InitializeStatsState() override {\n+\t\treturn make_unique<FixedDecimalStatistics>();\n+\t}\n+\n+\tvoid WriteVector(Serializer &temp_writer, ColumnWriterStatistics *stats_p, ColumnWriterPageState *page_state,\n+\t                 Vector &input_column, idx_t chunk_start, idx_t chunk_end) override {\n+\t\tauto &mask = FlatVector::Validity(input_column);\n+\t\tauto *ptr = FlatVector::GetData<hugeint_t>(input_column);\n+\t\tauto &stats = (FixedDecimalStatistics &)*stats_p;\n+\n+\t\tdata_t temp_buffer[16];\n+\t\tfor (idx_t r = chunk_start; r < chunk_end; r++) {\n+\t\t\tif (mask.RowIsValid(r)) {\n+\t\t\t\tstats.Update(ptr[r]);\n+\t\t\t\tWriteParquetDecimal(ptr[r], temp_buffer);\n+\t\t\t\ttemp_writer.WriteData(temp_buffer, 16);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tidx_t GetRowSize(Vector &vector, idx_t index) override {\n+\t\treturn sizeof(hugeint_t);\n+\t}\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// UUID Column Writer\n+//===--------------------------------------------------------------------===//\n+class UUIDColumnWriter : public ColumnWriter {\n+\tstatic constexpr const idx_t PARQUET_UUID_SIZE = 16;\n+\n+public:\n+\tUUIDColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,\n+\t                 idx_t max_define, bool can_have_nulls)\n+\t    : ColumnWriter(writer, schema_idx, move(schema_path_p), max_repeat, max_define, can_have_nulls) {\n+\t}\n+\t~UUIDColumnWriter() override = default;\n \n public:\n-\tvoid WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,\n-\t                 idx_t chunk_start, idx_t chunk_end) override {\n+\tstatic void WriteParquetUUID(hugeint_t input, data_ptr_t result) {\n+\t\tuint64_t high_bytes = input.upper ^ (int64_t(1) << 63);\n+\t\tuint64_t low_bytes = input.lower;\n+\n+\t\tfor (idx_t i = 0; i < sizeof(uint64_t); i++) {\n+\t\t\tauto shift_count = (sizeof(uint64_t) - i - 1) * 8;\n+\t\t\tresult[i] = (high_bytes >> shift_count) & 0xFF;\n+\t\t}\n+\t\tfor (idx_t i = 0; i < sizeof(uint64_t); i++) {\n+\t\t\tauto shift_count = (sizeof(uint64_t) - i - 1) * 8;\n+\t\t\tresult[sizeof(uint64_t) + i] = (low_bytes >> shift_count) & 0xFF;\n+\t\t}\n+\t}\n+\n+\tvoid WriteVector(Serializer &temp_writer, ColumnWriterStatistics *stats_p, ColumnWriterPageState *page_state,\n+\t                 Vector &input_column, idx_t chunk_start, idx_t chunk_end) override {\n \t\tauto &mask = FlatVector::Validity(input_column);\n+\t\tauto *ptr = FlatVector::GetData<hugeint_t>(input_column);\n \n-\t\t// FIXME: fixed length byte array...\n-\t\tVector double_vec(LogicalType::DOUBLE, true, false, chunk_end);\n-\t\tVectorOperations::Cast(input_column, double_vec, chunk_end);\n-\t\tTemplatedWritePlain<double, double>(double_vec, chunk_start, chunk_end, mask, temp_writer);\n+\t\tdata_t temp_buffer[PARQUET_UUID_SIZE];\n+\t\tfor (idx_t r = chunk_start; r < chunk_end; r++) {\n+\t\t\tif (mask.RowIsValid(r)) {\n+\t\t\t\tWriteParquetUUID(ptr[r], temp_buffer);\n+\t\t\t\ttemp_writer.WriteData(temp_buffer, PARQUET_UUID_SIZE);\n+\t\t\t}\n+\t\t}\n \t}\n \n \tidx_t GetRowSize(Vector &vector, idx_t index) override {\n-\t\treturn sizeof(double);\n+\t\treturn PARQUET_UUID_SIZE;\n+\t}\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// Interval Column Writer\n+//===--------------------------------------------------------------------===//\n+class IntervalColumnWriter : public ColumnWriter {\n+\tstatic constexpr const idx_t PARQUET_INTERVAL_SIZE = 12;\n+\n+public:\n+\tIntervalColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,\n+\t                     idx_t max_define, bool can_have_nulls)\n+\t    : ColumnWriter(writer, schema_idx, move(schema_path_p), max_repeat, max_define, can_have_nulls) {\n+\t}\n+\t~IntervalColumnWriter() override = default;\n+\n+public:\n+\tstatic void WriteParquetInterval(interval_t input, data_ptr_t result) {\n+\t\tif (input.days < 0 || input.months < 0 || input.micros < 0) {\n+\t\t\tthrow IOException(\"Parquet files do not support negative intervals\");\n+\t\t}\n+\t\tStore<uint32_t>(input.months, result);\n+\t\tStore<uint32_t>(input.days, result + sizeof(uint32_t));\n+\t\tStore<uint32_t>(input.micros / 1000, result + sizeof(uint32_t) * 2);\n+\t}\n+\n+\tvoid WriteVector(Serializer &temp_writer, ColumnWriterStatistics *stats_p, ColumnWriterPageState *page_state,\n+\t                 Vector &input_column, idx_t chunk_start, idx_t chunk_end) override {\n+\t\tauto &mask = FlatVector::Validity(input_column);\n+\t\tauto *ptr = FlatVector::GetData<interval_t>(input_column);\n+\n+\t\tdata_t temp_buffer[PARQUET_INTERVAL_SIZE];\n+\t\tfor (idx_t r = chunk_start; r < chunk_end; r++) {\n+\t\t\tif (mask.RowIsValid(r)) {\n+\t\t\t\tWriteParquetInterval(ptr[r], temp_buffer);\n+\t\t\t\ttemp_writer.WriteData(temp_buffer, PARQUET_INTERVAL_SIZE);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tidx_t GetRowSize(Vector &vector, idx_t index) override {\n+\t\treturn PARQUET_INTERVAL_SIZE;\n \t}\n };\n \n //===--------------------------------------------------------------------===//\n // String Column Writer\n //===--------------------------------------------------------------------===//\n+class StringStatisticsState : public ColumnWriterStatistics {\n+\tstatic constexpr const idx_t MAX_STRING_STATISTICS_SIZE = 10000;\n+\n+public:\n+\tStringStatisticsState() : has_stats(false), values_too_big(false), min(), max() {\n+\t}\n+\n+\tbool has_stats;\n+\tbool values_too_big;\n+\tstring min;\n+\tstring max;\n+\n+public:\n+\tbool HasStats() {\n+\t\treturn has_stats;\n+\t}\n+\n+\tvoid Update(const string_t &val) {\n+\t\tif (values_too_big) {\n+\t\t\treturn;\n+\t\t}\n+\t\tauto str_len = val.GetSize();\n+\t\tif (str_len > MAX_STRING_STATISTICS_SIZE) {\n+\t\t\t// we avoid gathering stats when individual string values are too large\n+\t\t\t// this is because the statistics are copied into the Parquet file meta data in uncompressed format\n+\t\t\t// ideally we avoid placing several mega or giga-byte long strings there\n+\t\t\t// we put a threshold of 10KB, if we see strings that exceed this threshold we avoid gathering stats\n+\t\t\tvalues_too_big = true;\n+\t\t\tmin = string();\n+\t\t\tmax = string();\n+\t\t\treturn;\n+\t\t}\n+\t\tif (!has_stats || LessThan::Operation(val, string_t(min))) {\n+\t\t\tmin = val.GetString();\n+\t\t}\n+\t\tif (!has_stats || GreaterThan::Operation(val, string_t(max))) {\n+\t\t\tmax = val.GetString();\n+\t\t}\n+\t\thas_stats = true;\n+\t}\n+\n+\tstring GetMin() override {\n+\t\treturn GetMinValue();\n+\t}\n+\tstring GetMax() override {\n+\t\treturn GetMaxValue();\n+\t}\n+\tstring GetMinValue() override {\n+\t\treturn HasStats() ? min : string();\n+\t}\n+\tstring GetMaxValue() override {\n+\t\treturn HasStats() ? max : string();\n+\t}\n+};\n+\n class StringColumnWriter : public ColumnWriter {\n public:\n-\tStringColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define, bool can_have_nulls)\n-\t    : ColumnWriter(writer, schema_idx, max_repeat, max_define, can_have_nulls) {\n+\tStringColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,\n+\t                   idx_t max_define, bool can_have_nulls)\n+\t    : ColumnWriter(writer, schema_idx, move(schema_path_p), max_repeat, max_define, can_have_nulls) {\n \t}\n \t~StringColumnWriter() override = default;\n \n public:\n-\tvoid WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,\n-\t                 idx_t chunk_start, idx_t chunk_end) override {\n+\tunique_ptr<ColumnWriterStatistics> InitializeStatsState() override {\n+\t\treturn make_unique<StringStatisticsState>();\n+\t}\n+\n+\tvoid WriteVector(Serializer &temp_writer, ColumnWriterStatistics *stats_p, ColumnWriterPageState *page_state,\n+\t                 Vector &input_column, idx_t chunk_start, idx_t chunk_end) override {\n \t\tauto &mask = FlatVector::Validity(input_column);\n+\t\tauto &stats = (StringStatisticsState &)*stats_p;\n \n \t\tauto *ptr = FlatVector::GetData<string_t>(input_column);\n \t\tfor (idx_t r = chunk_start; r < chunk_end; r++) {\n \t\t\tif (mask.RowIsValid(r)) {\n+\t\t\t\tstats.Update(ptr[r]);\n \t\t\t\ttemp_writer.Write<uint32_t>(ptr[r].GetSize());\n \t\t\t\ttemp_writer.WriteData((const_data_ptr_t)ptr[r].GetDataUnsafe(), ptr[r].GetSize());\n \t\t\t}\n@@ -633,31 +1132,134 @@ class StringColumnWriter : public ColumnWriter {\n };\n \n //===--------------------------------------------------------------------===//\n-// Struct Column Writer\n+// Enum Column Writer\n //===--------------------------------------------------------------------===//\n-class StructColumnWriter : public ColumnWriter {\n+class EnumWriterPageState : public ColumnWriterPageState {\n public:\n-\tStructColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define,\n-\t                   vector<unique_ptr<ColumnWriter>> child_writers_p, bool can_have_nulls)\n-\t    : ColumnWriter(writer, schema_idx, max_repeat, max_define, can_have_nulls),\n-\t      child_writers(move(child_writers_p)) {\n+\texplicit EnumWriterPageState(uint32_t bit_width) : encoder(bit_width), written_value(false) {\n \t}\n-\t~StructColumnWriter() override = default;\n \n-\tvector<unique_ptr<ColumnWriter>> child_writers;\n+\tRleBpEncoder encoder;\n+\tbool written_value;\n+};\n+\n+class EnumColumnWriter : public ColumnWriter {\n+public:\n+\tEnumColumnWriter(ParquetWriter &writer, LogicalType enum_type_p, idx_t schema_idx, vector<string> schema_path_p,\n+\t                 idx_t max_repeat, idx_t max_define, bool can_have_nulls)\n+\t    : ColumnWriter(writer, schema_idx, move(schema_path_p), max_repeat, max_define, can_have_nulls),\n+\t      enum_type(move(enum_type_p)) {\n+\t\tbit_width = RleBpDecoder::ComputeBitWidth(EnumType::GetSize(enum_type));\n+\t}\n+\t~EnumColumnWriter() override = default;\n+\n+\tLogicalType enum_type;\n+\tuint32_t bit_width;\n \n public:\n-\tvoid WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,\n-\t                 idx_t chunk_start, idx_t chunk_end) override {\n-\t\tthrow InternalException(\"Cannot write vector of type struct\");\n+\tunique_ptr<ColumnWriterStatistics> InitializeStatsState() override {\n+\t\treturn make_unique<StringStatisticsState>();\n+\t}\n+\n+\ttemplate <class T>\n+\tvoid WriteEnumInternal(Serializer &temp_writer, Vector &input_column, idx_t chunk_start, idx_t chunk_end,\n+\t                       EnumWriterPageState &page_state) {\n+\t\tauto &mask = FlatVector::Validity(input_column);\n+\t\tauto *ptr = FlatVector::GetData<T>(input_column);\n+\t\tfor (idx_t r = chunk_start; r < chunk_end; r++) {\n+\t\t\tif (mask.RowIsValid(r)) {\n+\t\t\t\tif (!page_state.written_value) {\n+\t\t\t\t\t// first value\n+\t\t\t\t\t// write the bit-width as a one-byte entry\n+\t\t\t\t\ttemp_writer.Write<uint8_t>(bit_width);\n+\t\t\t\t\t// now begin writing the actual value\n+\t\t\t\t\tpage_state.encoder.BeginWrite(temp_writer, ptr[r]);\n+\t\t\t\t\tpage_state.written_value = true;\n+\t\t\t\t} else {\n+\t\t\t\t\tpage_state.encoder.WriteValue(temp_writer, ptr[r]);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tvoid WriteVector(Serializer &temp_writer, ColumnWriterStatistics *stats_p, ColumnWriterPageState *page_state_p,\n+\t                 Vector &input_column, idx_t chunk_start, idx_t chunk_end) override {\n+\t\tauto &page_state = (EnumWriterPageState &)*page_state_p;\n+\t\tswitch (enum_type.InternalType()) {\n+\t\tcase PhysicalType::UINT8:\n+\t\t\tWriteEnumInternal<uint8_t>(temp_writer, input_column, chunk_start, chunk_end, page_state);\n+\t\t\tbreak;\n+\t\tcase PhysicalType::UINT16:\n+\t\t\tWriteEnumInternal<uint16_t>(temp_writer, input_column, chunk_start, chunk_end, page_state);\n+\t\t\tbreak;\n+\t\tcase PhysicalType::UINT32:\n+\t\t\tWriteEnumInternal<uint32_t>(temp_writer, input_column, chunk_start, chunk_end, page_state);\n+\t\t\tbreak;\n+\t\tdefault:\n+\t\t\tthrow InternalException(\"Unsupported internal enum type\");\n+\t\t}\n+\t}\n+\n+\tunique_ptr<ColumnWriterPageState> InitializePageState() override {\n+\t\treturn make_unique<EnumWriterPageState>(bit_width);\n+\t}\n+\n+\tvoid FlushPageState(Serializer &temp_writer, ColumnWriterPageState *state_p) override {\n+\t\tauto &page_state = (EnumWriterPageState &)*state_p;\n+\t\tif (!page_state.written_value) {\n+\t\t\t// all values are null\n+\t\t\t// just write the bit width\n+\t\t\ttemp_writer.Write<uint8_t>(bit_width);\n+\t\t\treturn;\n+\t\t}\n+\t\tpage_state.encoder.FinishWrite(temp_writer);\n+\t}\n+\n+\tduckdb_parquet::format::Encoding::type GetEncoding() override {\n+\t\treturn Encoding::RLE_DICTIONARY;\n+\t}\n+\n+\tvoid FlushDictionary(ColumnWriterState &state, ColumnWriterStatistics *stats_p) override {\n+\t\tauto &stats = (StringStatisticsState &)*stats_p;\n+\t\t// write the enum values to a dictionary page\n+\t\tauto &enum_values = EnumType::GetValuesInsertOrder(enum_type);\n+\t\tauto enum_count = EnumType::GetSize(enum_type);\n+\t\tauto string_values = FlatVector::GetData<string_t>(enum_values);\n+\t\t// first write the contents of the dictionary page to a temporary buffer\n+\t\tauto temp_writer = make_unique<BufferedSerializer>();\n+\t\tfor (idx_t r = 0; r < enum_count; r++) {\n+\t\t\tD_ASSERT(!FlatVector::IsNull(enum_values, r));\n+\t\t\t// update the statistics\n+\t\t\tstats.Update(string_values[r]);\n+\t\t\t// write this string value to the dictionary\n+\t\t\ttemp_writer->Write<uint32_t>(string_values[r].GetSize());\n+\t\t\ttemp_writer->WriteData((const_data_ptr_t)string_values[r].GetDataUnsafe(), string_values[r].GetSize());\n+\t\t}\n+\t\t// flush the dictionary page and add it to the to-be-written pages\n+\t\tWriteDictionary(state, move(temp_writer), enum_count);\n \t}\n \n \tidx_t GetRowSize(Vector &vector, idx_t index) override {\n-\t\tthrow InternalException(\"Cannot get row size of struct\");\n+\t\treturn (bit_width + 7) / 8;\n+\t}\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// Struct Column Writer\n+//===--------------------------------------------------------------------===//\n+class StructColumnWriter : public ColumnWriter {\n+public:\n+\tStructColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,\n+\t                   idx_t max_define, vector<unique_ptr<ColumnWriter>> child_writers_p, bool can_have_nulls)\n+\t    : ColumnWriter(writer, schema_idx, move(schema_path_p), max_repeat, max_define, can_have_nulls),\n+\t      child_writers(move(child_writers_p)) {\n \t}\n+\t~StructColumnWriter() override = default;\n+\n+\tvector<unique_ptr<ColumnWriter>> child_writers;\n \n-\tunique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,\n-\t                                                   vector<string> schema_path) override;\n+public:\n+\tunique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::format::RowGroup &row_group) override;\n \tvoid Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;\n \n \tvoid BeginWrite(ColumnWriterState &state) override;\n@@ -677,14 +1279,12 @@ class StructColumnWriterState : public ColumnWriterState {\n \tvector<unique_ptr<ColumnWriterState>> child_states;\n };\n \n-unique_ptr<ColumnWriterState> StructColumnWriter::InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,\n-                                                                       vector<string> schema_path) {\n+unique_ptr<ColumnWriterState> StructColumnWriter::InitializeWriteState(duckdb_parquet::format::RowGroup &row_group) {\n \tauto result = make_unique<StructColumnWriterState>(row_group, row_group.columns.size());\n-\tschema_path.push_back(writer.file_meta_data.schema[schema_idx].name);\n \n \tresult->child_states.reserve(child_writers.size());\n \tfor (auto &child_writer : child_writers) {\n-\t\tresult->child_states.push_back(child_writer->InitializeWriteState(row_group, schema_path));\n+\t\tresult->child_states.push_back(child_writer->InitializeWriteState(row_group));\n \t}\n \treturn move(result);\n }\n@@ -725,6 +1325,8 @@ void StructColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t\n void StructColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {\n \tauto &state = (StructColumnWriterState &)state_p;\n \tfor (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {\n+\t\t// we add the null count of the struct to the null count of the children\n+\t\tchild_writers[child_idx]->null_count += null_count;\n \t\tchild_writers[child_idx]->FinalizeWrite(*state.child_states[child_idx]);\n \t}\n }\n@@ -734,26 +1336,17 @@ void StructColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {\n //===--------------------------------------------------------------------===//\n class ListColumnWriter : public ColumnWriter {\n public:\n-\tListColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define,\n-\t                 unique_ptr<ColumnWriter> child_writer_p, bool can_have_nulls)\n-\t    : ColumnWriter(writer, schema_idx, max_repeat, max_define, can_have_nulls), child_writer(move(child_writer_p)) {\n+\tListColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,\n+\t                 idx_t max_define, unique_ptr<ColumnWriter> child_writer_p, bool can_have_nulls)\n+\t    : ColumnWriter(writer, schema_idx, move(schema_path_p), max_repeat, max_define, can_have_nulls),\n+\t      child_writer(move(child_writer_p)) {\n \t}\n \t~ListColumnWriter() override = default;\n \n \tunique_ptr<ColumnWriter> child_writer;\n \n public:\n-\tvoid WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,\n-\t                 idx_t chunk_start, idx_t chunk_end) override {\n-\t\tthrow InternalException(\"Cannot write vector of type list\");\n-\t}\n-\n-\tidx_t GetRowSize(Vector &vector, idx_t index) override {\n-\t\tthrow InternalException(\"Cannot get row size of list\");\n-\t}\n-\n-\tunique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,\n-\t                                                   vector<string> schema_path) override;\n+\tunique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::format::RowGroup &row_group) override;\n \tvoid Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;\n \n \tvoid BeginWrite(ColumnWriterState &state) override;\n@@ -774,11 +1367,9 @@ class ListColumnWriterState : public ColumnWriterState {\n \tidx_t parent_index = 0;\n };\n \n-unique_ptr<ColumnWriterState> ListColumnWriter::InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,\n-                                                                     vector<string> schema_path) {\n+unique_ptr<ColumnWriterState> ListColumnWriter::InitializeWriteState(duckdb_parquet::format::RowGroup &row_group) {\n \tauto result = make_unique<ListColumnWriterState>(row_group, row_group.columns.size());\n-\tschema_path.push_back(writer.file_meta_data.schema[schema_idx].name);\n-\tresult->child_state = child_writer->InitializeWriteState(row_group, move(schema_path));\n+\tresult->child_state = child_writer->InitializeWriteState(row_group);\n \treturn move(result);\n }\n \n@@ -861,8 +1452,8 @@ void ListColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {\n //===--------------------------------------------------------------------===//\n unique_ptr<ColumnWriter> ColumnWriter::CreateWriterRecursive(vector<duckdb_parquet::format::SchemaElement> &schemas,\n                                                              ParquetWriter &writer, const LogicalType &type,\n-                                                             const string &name, idx_t max_repeat, idx_t max_define,\n-                                                             bool can_have_nulls) {\n+                                                             const string &name, vector<string> schema_path,\n+                                                             idx_t max_repeat, idx_t max_define, bool can_have_nulls) {\n \tauto null_type = can_have_nulls ? FieldRepetitionType::OPTIONAL : FieldRepetitionType::REQUIRED;\n \tif (!can_have_nulls) {\n \t\tmax_define--;\n@@ -879,15 +1470,17 @@ unique_ptr<ColumnWriter> ColumnWriter::CreateWriterRecursive(vector<duckdb_parqu\n \t\tschema_element.__isset.repetition_type = true;\n \t\tschema_element.name = name;\n \t\tschemas.push_back(move(schema_element));\n+\t\tschema_path.push_back(name);\n+\n \t\t// construct the child types recursively\n \t\tvector<unique_ptr<ColumnWriter>> child_writers;\n \t\tchild_writers.reserve(child_types.size());\n \t\tfor (auto &child_type : child_types) {\n \t\t\tchild_writers.push_back(CreateWriterRecursive(schemas, writer, child_type.second, child_type.first,\n-\t\t\t                                              max_repeat, max_define + 1));\n+\t\t\t                                              schema_path, max_repeat, max_define + 1));\n \t\t}\n-\t\treturn make_unique<StructColumnWriter>(writer, schema_idx, max_repeat, max_define, move(child_writers),\n-\t\t                                       can_have_nulls);\n+\t\treturn make_unique<StructColumnWriter>(writer, schema_idx, move(schema_path), max_repeat, max_define,\n+\t\t                                       move(child_writers), can_have_nulls);\n \t}\n \tif (type.id() == LogicalTypeId::LIST) {\n \t\tauto &child_type = ListType::GetChildType(type);\n@@ -904,6 +1497,7 @@ unique_ptr<ColumnWriter> ColumnWriter::CreateWriterRecursive(vector<duckdb_parqu\n \t\toptional_element.__isset.converted_type = true;\n \t\toptional_element.name = name;\n \t\tschemas.push_back(move(optional_element));\n+\t\tschema_path.push_back(name);\n \n \t\t// then a REPEATED element\n \t\tduckdb_parquet::format::SchemaElement repeated_element;\n@@ -914,11 +1508,12 @@ unique_ptr<ColumnWriter> ColumnWriter::CreateWriterRecursive(vector<duckdb_parqu\n \t\trepeated_element.__isset.repetition_type = true;\n \t\trepeated_element.name = \"list\";\n \t\tschemas.push_back(move(repeated_element));\n+\t\tschema_path.emplace_back(\"list\");\n \n \t\tauto child_writer =\n-\t\t    CreateWriterRecursive(schemas, writer, child_type, \"element\", max_repeat + 1, max_define + 2);\n-\t\treturn make_unique<ListColumnWriter>(writer, schema_idx, max_repeat, max_define, move(child_writer),\n-\t\t                                     can_have_nulls);\n+\t\t    CreateWriterRecursive(schemas, writer, child_type, \"element\", schema_path, max_repeat + 1, max_define + 2);\n+\t\treturn make_unique<ListColumnWriter>(writer, schema_idx, move(schema_path), max_repeat, max_define,\n+\t\t                                     move(child_writer), can_have_nulls);\n \t}\n \tif (type.id() == LogicalTypeId::MAP) {\n \t\t// map type\n@@ -940,6 +1535,7 @@ unique_ptr<ColumnWriter> ColumnWriter::CreateWriterRecursive(vector<duckdb_parqu\n \t\ttop_element.__isset.type = false;\n \t\ttop_element.name = name;\n \t\tschemas.push_back(move(top_element));\n+\t\tschema_path.push_back(name);\n \n \t\t// key_value element\n \t\tduckdb_parquet::format::SchemaElement kv_element;\n@@ -950,6 +1546,7 @@ unique_ptr<ColumnWriter> ColumnWriter::CreateWriterRecursive(vector<duckdb_parqu\n \t\tkv_element.__isset.type = false;\n \t\tkv_element.name = \"key_value\";\n \t\tschemas.push_back(move(kv_element));\n+\t\tschema_path.emplace_back(\"key_value\");\n \n \t\t// construct the child types recursively\n \t\tvector<LogicalType> kv_types {ListType::GetChildType(MapType::KeyType(type)),\n@@ -960,14 +1557,14 @@ unique_ptr<ColumnWriter> ColumnWriter::CreateWriterRecursive(vector<duckdb_parqu\n \t\tfor (idx_t i = 0; i < 2; i++) {\n \t\t\t// key needs to be marked as REQUIRED\n \t\t\tbool is_key = i == 0;\n-\t\t\tauto child_writer = CreateWriterRecursive(schemas, writer, kv_types[i], kv_names[i], max_repeat + 1,\n-\t\t\t                                          max_define + 2, !is_key);\n-\t\t\tauto list_writer = make_unique<ListColumnWriter>(writer, schema_idx, max_repeat, max_define,\n+\t\t\tauto child_writer = CreateWriterRecursive(schemas, writer, kv_types[i], kv_names[i], schema_path,\n+\t\t\t                                          max_repeat + 1, max_define + 2, !is_key);\n+\t\t\tauto list_writer = make_unique<ListColumnWriter>(writer, schema_idx, schema_path, max_repeat, max_define,\n \t\t\t                                                 move(child_writer), can_have_nulls);\n \t\t\tchild_writers.push_back(move(list_writer));\n \t\t}\n-\t\treturn make_unique<StructColumnWriter>(writer, schema_idx, max_repeat, max_define, move(child_writers),\n-\t\t                                       can_have_nulls);\n+\t\treturn make_unique<StructColumnWriter>(writer, schema_idx, schema_path, max_repeat, max_define,\n+\t\t                                       move(child_writers), can_have_nulls);\n \t}\n \tduckdb_parquet::format::SchemaElement schema_element;\n \tschema_element.type = ParquetWriter::DuckDBTypeToParquetType(type);\n@@ -977,62 +1574,90 @@ unique_ptr<ColumnWriter> ColumnWriter::CreateWriterRecursive(vector<duckdb_parqu\n \tschema_element.__isset.type = true;\n \tschema_element.__isset.repetition_type = true;\n \tschema_element.name = name;\n-\tschema_element.__isset.converted_type =\n-\t    ParquetWriter::DuckDBTypeToConvertedType(type, schema_element.converted_type);\n+\tParquetWriter::SetSchemaProperties(type, schema_element);\n \tschemas.push_back(move(schema_element));\n+\tschema_path.push_back(name);\n \n \tswitch (type.id()) {\n \tcase LogicalTypeId::BOOLEAN:\n-\t\treturn make_unique<BooleanColumnWriter>(writer, schema_idx, max_repeat, max_define, can_have_nulls);\n+\t\treturn make_unique<BooleanColumnWriter>(writer, schema_idx, move(schema_path), max_repeat, max_define,\n+\t\t                                        can_have_nulls);\n \tcase LogicalTypeId::TINYINT:\n-\t\treturn make_unique<StandardColumnWriter<int8_t, int32_t>>(writer, schema_idx, max_repeat, max_define,\n-\t\t                                                          can_have_nulls);\n+\t\treturn make_unique<StandardColumnWriter<int8_t, int32_t>>(writer, schema_idx, move(schema_path), max_repeat,\n+\t\t                                                          max_define, can_have_nulls);\n \tcase LogicalTypeId::SMALLINT:\n-\t\treturn make_unique<StandardColumnWriter<int16_t, int32_t>>(writer, schema_idx, max_repeat, max_define,\n-\t\t                                                           can_have_nulls);\n+\t\treturn make_unique<StandardColumnWriter<int16_t, int32_t>>(writer, schema_idx, move(schema_path), max_repeat,\n+\t\t                                                           max_define, can_have_nulls);\n \tcase LogicalTypeId::INTEGER:\n \tcase LogicalTypeId::DATE:\n-\t\treturn make_unique<StandardColumnWriter<int32_t, int32_t>>(writer, schema_idx, max_repeat, max_define,\n-\t\t                                                           can_have_nulls);\n+\tcase LogicalTypeId::DATE_TZ:\n+\t\treturn make_unique<StandardColumnWriter<int32_t, int32_t>>(writer, schema_idx, move(schema_path), max_repeat,\n+\t\t                                                           max_define, can_have_nulls);\n \tcase LogicalTypeId::BIGINT:\n+\tcase LogicalTypeId::TIME:\n+\tcase LogicalTypeId::TIME_TZ:\n \tcase LogicalTypeId::TIMESTAMP:\n+\tcase LogicalTypeId::TIMESTAMP_TZ:\n \tcase LogicalTypeId::TIMESTAMP_MS:\n-\t\treturn make_unique<StandardColumnWriter<int64_t, int64_t>>(writer, schema_idx, max_repeat, max_define,\n-\t\t                                                           can_have_nulls);\n+\t\treturn make_unique<StandardColumnWriter<int64_t, int64_t>>(writer, schema_idx, move(schema_path), max_repeat,\n+\t\t                                                           max_define, can_have_nulls);\n \tcase LogicalTypeId::HUGEINT:\n \t\treturn make_unique<StandardColumnWriter<hugeint_t, double, ParquetHugeintOperator>>(\n-\t\t    writer, schema_idx, max_repeat, max_define, can_have_nulls);\n+\t\t    writer, schema_idx, move(schema_path), max_repeat, max_define, can_have_nulls);\n \tcase LogicalTypeId::TIMESTAMP_NS:\n \t\treturn make_unique<StandardColumnWriter<int64_t, int64_t, ParquetTimestampNSOperator>>(\n-\t\t    writer, schema_idx, max_repeat, max_define, can_have_nulls);\n+\t\t    writer, schema_idx, move(schema_path), max_repeat, max_define, can_have_nulls);\n \tcase LogicalTypeId::TIMESTAMP_SEC:\n \t\treturn make_unique<StandardColumnWriter<int64_t, int64_t, ParquetTimestampSOperator>>(\n-\t\t    writer, schema_idx, max_repeat, max_define, can_have_nulls);\n+\t\t    writer, schema_idx, move(schema_path), max_repeat, max_define, can_have_nulls);\n \tcase LogicalTypeId::UTINYINT:\n-\t\treturn make_unique<StandardColumnWriter<uint8_t, int32_t>>(writer, schema_idx, max_repeat, max_define,\n-\t\t                                                           can_have_nulls);\n+\t\treturn make_unique<StandardColumnWriter<uint8_t, int32_t>>(writer, schema_idx, move(schema_path), max_repeat,\n+\t\t                                                           max_define, can_have_nulls);\n \tcase LogicalTypeId::USMALLINT:\n-\t\treturn make_unique<StandardColumnWriter<uint16_t, int32_t>>(writer, schema_idx, max_repeat, max_define,\n-\t\t                                                            can_have_nulls);\n+\t\treturn make_unique<StandardColumnWriter<uint16_t, int32_t>>(writer, schema_idx, move(schema_path), max_repeat,\n+\t\t                                                            max_define, can_have_nulls);\n \tcase LogicalTypeId::UINTEGER:\n-\t\treturn make_unique<StandardColumnWriter<uint32_t, uint32_t>>(writer, schema_idx, max_repeat, max_define,\n-\t\t                                                             can_have_nulls);\n+\t\treturn make_unique<StandardColumnWriter<uint32_t, uint32_t>>(writer, schema_idx, move(schema_path), max_repeat,\n+\t\t                                                             max_define, can_have_nulls);\n \tcase LogicalTypeId::UBIGINT:\n-\t\treturn make_unique<StandardColumnWriter<uint64_t, uint64_t>>(writer, schema_idx, max_repeat, max_define,\n-\t\t                                                             can_have_nulls);\n+\t\treturn make_unique<StandardColumnWriter<uint64_t, uint64_t>>(writer, schema_idx, move(schema_path), max_repeat,\n+\t\t                                                             max_define, can_have_nulls);\n \tcase LogicalTypeId::FLOAT:\n-\t\treturn make_unique<StandardColumnWriter<float, float>>(writer, schema_idx, max_repeat, max_define,\n-\t\t                                                       can_have_nulls);\n+\t\treturn make_unique<StandardColumnWriter<float, float>>(writer, schema_idx, move(schema_path), max_repeat,\n+\t\t                                                       max_define, can_have_nulls);\n \tcase LogicalTypeId::DOUBLE:\n-\t\treturn make_unique<StandardColumnWriter<double, double>>(writer, schema_idx, max_repeat, max_define,\n-\t\t                                                         can_have_nulls);\n+\t\treturn make_unique<StandardColumnWriter<double, double>>(writer, schema_idx, move(schema_path), max_repeat,\n+\t\t                                                         max_define, can_have_nulls);\n \tcase LogicalTypeId::DECIMAL:\n-\t\treturn make_unique<DecimalColumnWriter>(writer, schema_idx, max_repeat, max_define, can_have_nulls);\n+\t\tswitch (type.InternalType()) {\n+\t\tcase PhysicalType::INT16:\n+\t\t\treturn make_unique<StandardColumnWriter<int16_t, int32_t>>(writer, schema_idx, move(schema_path),\n+\t\t\t                                                           max_repeat, max_define, can_have_nulls);\n+\t\tcase PhysicalType::INT32:\n+\t\t\treturn make_unique<StandardColumnWriter<int32_t, int32_t>>(writer, schema_idx, move(schema_path),\n+\t\t\t                                                           max_repeat, max_define, can_have_nulls);\n+\t\tcase PhysicalType::INT64:\n+\t\t\treturn make_unique<StandardColumnWriter<int64_t, int64_t>>(writer, schema_idx, move(schema_path),\n+\t\t\t                                                           max_repeat, max_define, can_have_nulls);\n+\t\tdefault:\n+\t\t\treturn make_unique<FixedDecimalColumnWriter>(writer, schema_idx, move(schema_path), max_repeat, max_define,\n+\t\t\t                                             can_have_nulls);\n+\t\t}\n \tcase LogicalTypeId::BLOB:\n \tcase LogicalTypeId::VARCHAR:\n-\t\treturn make_unique<StringColumnWriter>(writer, schema_idx, max_repeat, max_define, can_have_nulls);\n+\t\treturn make_unique<StringColumnWriter>(writer, schema_idx, move(schema_path), max_repeat, max_define,\n+\t\t                                       can_have_nulls);\n+\tcase LogicalTypeId::UUID:\n+\t\treturn make_unique<UUIDColumnWriter>(writer, schema_idx, move(schema_path), max_repeat, max_define,\n+\t\t                                     can_have_nulls);\n+\tcase LogicalTypeId::INTERVAL:\n+\t\treturn make_unique<IntervalColumnWriter>(writer, schema_idx, move(schema_path), max_repeat, max_define,\n+\t\t                                         can_have_nulls);\n+\tcase LogicalTypeId::ENUM:\n+\t\treturn make_unique<EnumColumnWriter>(writer, type, schema_idx, move(schema_path), max_repeat, max_define,\n+\t\t                                     can_have_nulls);\n \tdefault:\n-\t\tthrow InternalException(\"Unsupported type in Parquet writer\");\n+\t\tthrow InternalException(\"Unsupported type \\\"%s\\\" in Parquet writer\", type.ToString());\n \t}\n }\n \ndiff --git a/extension/parquet/include/column_reader.hpp b/extension/parquet/include/column_reader.hpp\nindex 78b95c0e1b34..cbea3d859723 100644\n--- a/extension/parquet/include/column_reader.hpp\n+++ b/extension/parquet/include/column_reader.hpp\n@@ -41,80 +41,38 @@ typedef std::bitset<STANDARD_VECTOR_SIZE> parquet_filter_t;\n \n class ColumnReader {\n public:\n-\tstatic unique_ptr<ColumnReader> CreateReader(ParquetReader &reader, const LogicalType &type_p,\n-\t                                             const SchemaElement &schema_p, idx_t schema_idx_p, idx_t max_define,\n-\t                                             idx_t max_repeat);\n-\n \tColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,\n \t             idx_t max_define_p, idx_t max_repeat_p);\n-\n-\tvirtual void InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) {\n-\t\tD_ASSERT(file_idx < columns.size());\n-\t\tchunk = &columns[file_idx];\n-\t\tprotocol = &protocol_p;\n-\t\tD_ASSERT(chunk);\n-\t\tD_ASSERT(chunk->__isset.meta_data);\n-\n-\t\tif (chunk->__isset.file_path) {\n-\t\t\tthrow std::runtime_error(\"Only inlined data files are supported (no references)\");\n-\t\t}\n-\n-\t\t// ugh. sometimes there is an extra offset for the dict. sometimes it's wrong.\n-\t\tchunk_read_offset = chunk->meta_data.data_page_offset;\n-\t\tif (chunk->meta_data.__isset.dictionary_page_offset && chunk->meta_data.dictionary_page_offset >= 4) {\n-\t\t\t// this assumes the data pages follow the dict pages directly.\n-\t\t\tchunk_read_offset = chunk->meta_data.dictionary_page_offset;\n-\t\t}\n-\t\tgroup_rows_available = chunk->meta_data.num_values;\n-\t}\n \tvirtual ~ColumnReader();\n \n+public:\n+\tstatic unique_ptr<ColumnReader> CreateReader(ParquetReader &reader, const LogicalType &type_p,\n+\t                                             const SchemaElement &schema_p, idx_t schema_idx_p, idx_t max_define,\n+\t                                             idx_t max_repeat);\n+\tvirtual void InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p);\n \tvirtual idx_t Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,\n \t                   Vector &result_out);\n \n \tvirtual void Skip(idx_t num_values);\n \n-\tconst LogicalType &Type() {\n-\t\treturn type;\n-\t}\n-\n-\tconst SchemaElement &Schema() {\n-\t\treturn schema;\n-\t}\n+\tconst LogicalType &Type();\n+\tconst SchemaElement &Schema();\n \n-\tvirtual idx_t GroupRowsAvailable() {\n-\t\treturn group_rows_available;\n-\t}\n+\tvirtual idx_t GroupRowsAvailable();\n \n-\tunique_ptr<BaseStatistics> Stats(const std::vector<ColumnChunk> &columns) {\n-\t\tif (Type().id() == LogicalTypeId::LIST || Type().id() == LogicalTypeId::STRUCT ||\n-\t\t    Type().id() == LogicalTypeId::MAP) {\n-\t\t\treturn nullptr;\n-\t\t}\n-\t\treturn ParquetTransformColumnStatistics(Schema(), Type(), columns[file_idx]);\n-\t}\n+\tunique_ptr<BaseStatistics> Stats(const std::vector<ColumnChunk> &columns);\n \n protected:\n \t// readers that use the default Read() need to implement those\n \tvirtual void Plain(shared_ptr<ByteBuffer> plain_data, uint8_t *defines, idx_t num_values, parquet_filter_t &filter,\n-\t                   idx_t result_offset, Vector &result) {\n-\t\tthrow NotImplementedException(\"Plain\");\n-\t}\n-\n-\tvirtual void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) {\n-\t\tthrow NotImplementedException(\"Dictionary\");\n-\t}\n-\n+\t                   idx_t result_offset, Vector &result);\n+\tvirtual void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries);\n \tvirtual void Offsets(uint32_t *offsets, uint8_t *defines, idx_t num_values, parquet_filter_t &filter,\n-\t                     idx_t result_offset, Vector &result) {\n-\t\tthrow NotImplementedException(\"Offsets\");\n-\t}\n+\t                     idx_t result_offset, Vector &result);\n \n \t// these are nops for most types, but not for strings\n-\tvirtual void DictReference(Vector &result) {\n-\t}\n-\tvirtual void PlainReference(shared_ptr<ByteBuffer>, Vector &result) {\n-\t}\n+\tvirtual void DictReference(Vector &result);\n+\tvirtual void PlainReference(shared_ptr<ByteBuffer>, Vector &result);\n \n \tbool HasDefines() {\n \t\treturn max_define > 0;\ndiff --git a/extension/parquet/include/column_writer.hpp b/extension/parquet/include/column_writer.hpp\nindex e840281a2a6c..71787d989c2a 100644\n--- a/extension/parquet/include/column_writer.hpp\n+++ b/extension/parquet/include/column_writer.hpp\n@@ -15,6 +15,7 @@ namespace duckdb {\n class BufferedSerializer;\n class ParquetWriter;\n class ColumnWriterPageState;\n+class StandardColumnWriterState;\n \n class ColumnWriterState {\n public:\n@@ -25,30 +26,44 @@ class ColumnWriterState {\n \tvector<bool> is_empty;\n };\n \n+class ColumnWriterStatistics {\n+public:\n+\tvirtual ~ColumnWriterStatistics();\n+\n+\tvirtual string GetMin();\n+\tvirtual string GetMax();\n+\tvirtual string GetMinValue();\n+\tvirtual string GetMaxValue();\n+};\n+\n class ColumnWriter {\n \t//! We limit the uncompressed page size to 100MB\n \t// The max size in Parquet is 2GB, but we choose a more conservative limit\n \tstatic constexpr const idx_t MAX_UNCOMPRESSED_PAGE_SIZE = 100000000;\n \n public:\n-\tColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define, bool can_have_nulls);\n+\tColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path, idx_t max_repeat,\n+\t             idx_t max_define, bool can_have_nulls);\n \tvirtual ~ColumnWriter();\n \n \tParquetWriter &writer;\n \tidx_t schema_idx;\n+\tvector<string> schema_path;\n \tidx_t max_repeat;\n \tidx_t max_define;\n \tbool can_have_nulls;\n+\t// collected stats\n+\tidx_t null_count;\n \n public:\n \t//! Create the column writer for a specific type recursively\n \tstatic unique_ptr<ColumnWriter> CreateWriterRecursive(vector<duckdb_parquet::format::SchemaElement> &schemas,\n \t                                                      ParquetWriter &writer, const LogicalType &type,\n-\t                                                      const string &name, idx_t max_repeat = 0,\n-\t                                                      idx_t max_define = 1, bool can_have_nulls = true);\n+\t                                                      const string &name, vector<string> schema_path,\n+\t                                                      idx_t max_repeat = 0, idx_t max_define = 1,\n+\t                                                      bool can_have_nulls = true);\n \n-\tvirtual unique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,\n-\t                                                           vector<string> schema_path);\n+\tvirtual unique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::format::RowGroup &row_group);\n \tvirtual void Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count);\n \n \tvirtual void BeginWrite(ColumnWriterState &state);\n@@ -63,14 +78,22 @@ class ColumnWriter {\n \tvoid WriteLevels(Serializer &temp_writer, const vector<uint16_t> &levels, idx_t max_value, idx_t start_offset,\n \t                 idx_t count);\n \n+\tvirtual duckdb_parquet::format::Encoding::type GetEncoding();\n+\n \tvoid NextPage(ColumnWriterState &state_p);\n \tvoid FlushPage(ColumnWriterState &state_p);\n+\tvoid WriteDictionary(ColumnWriterState &state_p, unique_ptr<BufferedSerializer> temp_writer, idx_t row_count);\n+\n+\tvirtual void FlushDictionary(ColumnWriterState &state, ColumnWriterStatistics *stats);\n \n+\t//! Initializes the state used to track statistics during writing. Only used for scalar types.\n+\tvirtual unique_ptr<ColumnWriterStatistics> InitializeStatsState();\n \t//! Retrieves the row size of a vector at the specified location. Only used for scalar types.\n-\tvirtual idx_t GetRowSize(Vector &vector, idx_t index) = 0;\n+\tvirtual idx_t GetRowSize(Vector &vector, idx_t index);\n \t//! Writes a (subset of a) vector to the specified serializer. Only used for scalar types.\n-\tvirtual void WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &vector,\n-\t                         idx_t chunk_start, idx_t chunk_end) = 0;\n+\tvirtual void WriteVector(Serializer &temp_writer, ColumnWriterStatistics *stats, ColumnWriterPageState *page_state,\n+\t                         Vector &vector, idx_t chunk_start, idx_t chunk_end);\n+\n \t//! Initialize the writer for a specific page. Only used for scalar types.\n \tvirtual unique_ptr<ColumnWriterPageState> InitializePageState();\n \t//! Flushes the writer for a specific page. Only used for scalar types.\n@@ -78,6 +101,8 @@ class ColumnWriter {\n \n \tvoid CompressPage(BufferedSerializer &temp_writer, size_t &compressed_size, data_ptr_t &compressed_data,\n \t                  unique_ptr<data_t[]> &compressed_buf);\n+\n+\tvoid SetParquetStatistics(StandardColumnWriterState &state, duckdb_parquet::format::ColumnChunk &column);\n };\n \n } // namespace duckdb\ndiff --git a/extension/parquet/include/decimal_column_reader.hpp b/extension/parquet/include/decimal_column_reader.hpp\ndeleted file mode 100644\nindex e89006d23420..000000000000\n--- a/extension/parquet/include/decimal_column_reader.hpp\n+++ /dev/null\n@@ -1,70 +0,0 @@\n-//===----------------------------------------------------------------------===//\n-//                         DuckDB\n-//\n-// decimal_column_reader.hpp\n-//\n-//\n-//===----------------------------------------------------------------------===//\n-\n-#pragma once\n-\n-#include \"column_reader.hpp\"\n-#include \"templated_column_reader.hpp\"\n-\n-namespace duckdb {\n-\n-template <class DUCKDB_PHYSICAL_TYPE>\n-struct DecimalParquetValueConversion {\n-\tstatic DUCKDB_PHYSICAL_TYPE DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {\n-\t\tauto dict_ptr = (DUCKDB_PHYSICAL_TYPE *)dict.ptr;\n-\t\treturn dict_ptr[offset];\n-\t}\n-\n-\tstatic DUCKDB_PHYSICAL_TYPE PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {\n-\t\tDUCKDB_PHYSICAL_TYPE res = 0;\n-\t\tauto byte_len = (idx_t)reader.Schema().type_length; /* sure, type length needs to be a signed int */\n-\t\tD_ASSERT(byte_len <= sizeof(DUCKDB_PHYSICAL_TYPE));\n-\t\tplain_data.available(byte_len);\n-\t\tauto res_ptr = (uint8_t *)&res;\n-\n-\t\t// numbers are stored as two's complement so some muckery is required\n-\t\tbool positive = (*plain_data.ptr & 0x80) == 0;\n-\n-\t\tfor (idx_t i = 0; i < byte_len; i++) {\n-\t\t\tauto byte = *(plain_data.ptr + (byte_len - i - 1));\n-\t\t\tres_ptr[i] = positive ? byte : byte ^ 0xFF;\n-\t\t}\n-\t\tplain_data.inc(byte_len);\n-\t\tif (!positive) {\n-\t\t\tres += 1;\n-\t\t\treturn -res;\n-\t\t}\n-\t\treturn res;\n-\t}\n-\n-\tstatic void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {\n-\t\tplain_data.inc(reader.Schema().type_length);\n-\t}\n-};\n-\n-template <class DUCKDB_PHYSICAL_TYPE>\n-class DecimalColumnReader\n-    : public TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE, DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE>> {\n-\n-public:\n-\tDecimalColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,\n-\t                    idx_t max_define_p, idx_t max_repeat_p)\n-\t    : TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE, DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE>>(\n-\t          reader, move(type_p), schema_p, file_idx_p, max_define_p, max_repeat_p) {};\n-\n-protected:\n-\tvoid Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) {\n-\t\tthis->dict = make_shared<ResizeableBuffer>(this->reader.allocator, num_entries * sizeof(DUCKDB_PHYSICAL_TYPE));\n-\t\tauto dict_ptr = (DUCKDB_PHYSICAL_TYPE *)this->dict->ptr;\n-\t\tfor (idx_t i = 0; i < num_entries; i++) {\n-\t\t\tdict_ptr[i] = DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE>::PlainRead(*dictionary_data, *this);\n-\t\t}\n-\t}\n-};\n-\n-} // namespace duckdb\n\\ No newline at end of file\ndiff --git a/extension/parquet/include/parquet_decimal_utils.hpp b/extension/parquet/include/parquet_decimal_utils.hpp\nnew file mode 100644\nindex 000000000000..14d0992929e2\n--- /dev/null\n+++ b/extension/parquet/include/parquet_decimal_utils.hpp\n@@ -0,0 +1,43 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// parquet_decimal_utils.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"column_reader.hpp\"\n+#include \"templated_column_reader.hpp\"\n+\n+namespace duckdb {\n+\n+class ParquetDecimalUtils {\n+public:\n+\ttemplate <class PHYSICAL_TYPE>\n+\tstatic PHYSICAL_TYPE ReadDecimalValue(const_data_ptr_t pointer, idx_t size) {\n+\t\tD_ASSERT(size <= sizeof(PHYSICAL_TYPE));\n+\t\tPHYSICAL_TYPE res = 0;\n+\n+\t\tauto res_ptr = (uint8_t *)&res;\n+\t\tbool positive = (*pointer & 0x80) == 0;\n+\n+\t\t// numbers are stored as two's complement so some muckery is required\n+\t\tfor (idx_t i = 0; i < size; i++) {\n+\t\t\tauto byte = *(pointer + (size - i - 1));\n+\t\t\tres_ptr[i] = positive ? byte : byte ^ 0xFF;\n+\t\t}\n+\t\tif (!positive) {\n+\t\t\tres += 1;\n+\t\t\treturn -res;\n+\t\t}\n+\t\treturn res;\n+\t}\n+\n+\tstatic unique_ptr<ColumnReader> CreateReader(ParquetReader &reader, const LogicalType &type_p,\n+\t                                             const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,\n+\t                                             idx_t max_repeat);\n+};\n+\n+} // namespace duckdb\n\\ No newline at end of file\ndiff --git a/extension/parquet/include/parquet_reader.hpp b/extension/parquet/include/parquet_reader.hpp\nindex d08e6df7f0cc..6223aa51d8b1 100644\n--- a/extension/parquet/include/parquet_reader.hpp\n+++ b/extension/parquet/include/parquet_reader.hpp\n@@ -96,6 +96,7 @@ class ParquetReader {\n \n \tstatic unique_ptr<BaseStatistics> ReadStatistics(ParquetReader &reader, LogicalType &type, column_t column_index,\n \t                                                 const duckdb_parquet::format::FileMetaData *file_meta_data);\n+\tstatic LogicalType DeriveLogicalType(const SchemaElement &s_ele, bool binary_as_string);\n \n private:\n \tvoid InitializeSchema(const vector<LogicalType> &expected_types_p, const string &initial_filename_p);\ndiff --git a/extension/parquet/include/parquet_rle_bp_encoder.hpp b/extension/parquet/include/parquet_rle_bp_encoder.hpp\nnew file mode 100644\nindex 000000000000..1fc3b6deae32\n--- /dev/null\n+++ b/extension/parquet/include/parquet_rle_bp_encoder.hpp\n@@ -0,0 +1,49 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// parquet_rle_bp_encoder.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"parquet_types.h\"\n+#include \"thrift_tools.hpp\"\n+#include \"resizable_buffer.hpp\"\n+\n+namespace duckdb {\n+\n+class RleBpEncoder {\n+public:\n+\tRleBpEncoder(uint32_t bit_width);\n+\n+public:\n+\t//! NOTE: Prepare is only required if a byte count is required BEFORE writing\n+\t//! This is the case with e.g. writing repetition/definition levels\n+\t//! If GetByteCount() is not required, prepare can be safely skipped\n+\tvoid BeginPrepare(uint32_t first_value);\n+\tvoid PrepareValue(uint32_t value);\n+\tvoid FinishPrepare();\n+\n+\tvoid BeginWrite(Serializer &writer, uint32_t first_value);\n+\tvoid WriteValue(Serializer &writer, uint32_t value);\n+\tvoid FinishWrite(Serializer &writer);\n+\n+\tidx_t GetByteCount();\n+\n+private:\n+\t//! meta information\n+\tuint32_t byte_width;\n+\t//! RLE run information\n+\tidx_t byte_count;\n+\tidx_t run_count;\n+\tidx_t current_run_count;\n+\tuint32_t last_value;\n+\n+private:\n+\tvoid FinishRun();\n+\tvoid WriteRun(Serializer &writer);\n+};\n+\n+} // namespace duckdb\ndiff --git a/extension/parquet/include/parquet_statistics.hpp b/extension/parquet/include/parquet_statistics.hpp\nindex a27185181b50..23d5cf0dc15a 100644\n--- a/extension/parquet/include/parquet_statistics.hpp\n+++ b/extension/parquet/include/parquet_statistics.hpp\n@@ -13,7 +13,13 @@ using duckdb_parquet::format::SchemaElement;\n \n struct LogicalType;\n \n-unique_ptr<BaseStatistics> ParquetTransformColumnStatistics(const SchemaElement &s_ele, const LogicalType &type,\n-                                                            const ColumnChunk &column_chunk);\n+struct ParquetStatisticsUtils {\n+\n+\tstatic unique_ptr<BaseStatistics> TransformColumnStatistics(const SchemaElement &s_ele, const LogicalType &type,\n+\t                                                            const ColumnChunk &column_chunk);\n+\n+\tstatic Value ConvertValue(const LogicalType &type, const duckdb_parquet::format::SchemaElement &schema_ele,\n+\t                          const std::string &stats);\n+};\n \n } // namespace duckdb\ndiff --git a/extension/parquet/include/parquet_timestamp.hpp b/extension/parquet/include/parquet_timestamp.hpp\nindex 46f35326cdfb..9291399375f1 100644\n--- a/extension/parquet/include/parquet_timestamp.hpp\n+++ b/extension/parquet/include/parquet_timestamp.hpp\n@@ -22,5 +22,6 @@ Int96 TimestampToImpalaTimestamp(timestamp_t &ts);\n timestamp_t ParquetTimestampMicrosToTimestamp(const int64_t &raw_ts);\n timestamp_t ParquetTimestampMsToTimestamp(const int64_t &raw_ts);\n date_t ParquetIntToDate(const int32_t &raw_date);\n+dtime_t ParquetIntToTime(const int64_t &raw_time);\n \n } // namespace duckdb\ndiff --git a/extension/parquet/include/parquet_writer.hpp b/extension/parquet/include/parquet_writer.hpp\nindex 901c85532a33..b2e537595353 100644\n--- a/extension/parquet/include/parquet_writer.hpp\n+++ b/extension/parquet/include/parquet_writer.hpp\n@@ -39,8 +39,7 @@ class ParquetWriter {\n \tvoid Finalize();\n \n \tstatic duckdb_parquet::format::Type::type DuckDBTypeToParquetType(const LogicalType &duckdb_type);\n-\tstatic bool DuckDBTypeToConvertedType(const LogicalType &duckdb_type,\n-\t                                      duckdb_parquet::format::ConvertedType::type &result);\n+\tstatic void SetSchemaProperties(const LogicalType &duckdb_type, duckdb_parquet::format::SchemaElement &schema_ele);\n \n private:\n \tstring file_name;\ndiff --git a/extension/parquet/include/string_column_reader.hpp b/extension/parquet/include/string_column_reader.hpp\nindex 4219e083eb8e..5d753a86e1ec 100644\n--- a/extension/parquet/include/string_column_reader.hpp\n+++ b/extension/parquet/include/string_column_reader.hpp\n@@ -23,15 +23,7 @@ struct StringParquetValueConversion {\n class StringColumnReader : public TemplatedColumnReader<string_t, StringParquetValueConversion> {\n public:\n \tStringColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t schema_idx_p,\n-\t                   idx_t max_define_p, idx_t max_repeat_p)\n-\t    : TemplatedColumnReader<string_t, StringParquetValueConversion>(reader, move(type_p), schema_p, schema_idx_p,\n-\t                                                                    max_define_p, max_repeat_p) {\n-\t\tfixed_width_string_length = 0;\n-\t\tif (schema_p.type == Type::FIXED_LEN_BYTE_ARRAY) {\n-\t\t\tD_ASSERT(schema_p.__isset.type_length);\n-\t\t\tfixed_width_string_length = schema_p.type_length;\n-\t\t}\n-\t};\n+\t                   idx_t max_define_p, idx_t max_repeat_p);\n \n \tunique_ptr<string_t[]> dict_strings;\n \tidx_t fixed_width_string_length;\ndiff --git a/extension/parquet/parquet_metadata.cpp b/extension/parquet/parquet_metadata.cpp\nindex 3c22f44c2b32..cad9551b6f27 100644\n--- a/extension/parquet/parquet_metadata.cpp\n+++ b/extension/parquet/parquet_metadata.cpp\n@@ -1,4 +1,6 @@\n #include \"parquet_metadata.hpp\"\n+#include \"parquet_statistics.hpp\"\n+\n #include <sstream>\n \n #ifndef DUCKDB_AMALGAMATION\n@@ -109,53 +111,12 @@ void ParquetMetaDataOperatorData::BindMetaData(vector<LogicalType> &return_types\n \treturn_types.emplace_back(LogicalType::BIGINT);\n }\n \n-Value ConvertParquetStats(duckdb_parquet::format::Type::type type, bool stats_is_set, const std::string &stats) {\n+Value ConvertParquetStats(const LogicalType &type, const duckdb_parquet::format::SchemaElement &schema_ele,\n+                          bool stats_is_set, const std::string &stats) {\n \tif (!stats_is_set) {\n \t\treturn Value(LogicalType::VARCHAR);\n \t}\n-\tswitch (type) {\n-\tcase Type::BOOLEAN:\n-\t\tif (stats.size() == sizeof(bool)) {\n-\t\t\treturn Value(Value::BOOLEAN(Load<bool>((data_ptr_t)stats.c_str())).ToString());\n-\t\t}\n-\t\tbreak;\n-\tcase Type::INT32:\n-\t\tif (stats.size() == sizeof(int32_t)) {\n-\t\t\treturn Value(Value::INTEGER(Load<int32_t>((data_ptr_t)stats.c_str())).ToString());\n-\t\t}\n-\t\tbreak;\n-\tcase Type::INT64:\n-\t\tif (stats.size() == sizeof(int64_t)) {\n-\t\t\treturn Value(Value::BIGINT(Load<int64_t>((data_ptr_t)stats.c_str())).ToString());\n-\t\t}\n-\t\tbreak;\n-\tcase Type::FLOAT:\n-\t\tif (stats.size() == sizeof(float)) {\n-\t\t\tfloat val = Load<float>((data_ptr_t)stats.c_str());\n-\t\t\tif (Value::FloatIsValid(val)) {\n-\t\t\t\treturn Value(Value::FLOAT(val).ToString());\n-\t\t\t}\n-\t\t}\n-\t\tbreak;\n-\tcase Type::DOUBLE:\n-\t\tif (stats.size() == sizeof(double)) {\n-\t\t\tdouble val = Load<double>((data_ptr_t)stats.c_str());\n-\t\t\tif (Value::DoubleIsValid(val)) {\n-\t\t\t\treturn Value(Value::DOUBLE(val).ToString());\n-\t\t\t}\n-\t\t}\n-\t\tbreak;\n-\tcase Type::BYTE_ARRAY:\n-\tcase Type::INT96:\n-\tcase Type::FIXED_LEN_BYTE_ARRAY:\n-\tdefault:\n-\t\tbreak;\n-\t}\n-\tif (Value::StringIsValid(stats)) {\n-\t\treturn Value(stats);\n-\t} else {\n-\t\treturn Value(Blob::ToString(string_t(stats)));\n-\t}\n+\treturn ParquetStatisticsUtils::ConvertValue(type, schema_ele, stats).CastAs(LogicalType::VARCHAR);\n }\n \n void ParquetMetaDataOperatorData::LoadFileMetaData(ClientContext &context, const vector<LogicalType> &return_types,\n@@ -167,13 +128,29 @@ void ParquetMetaDataOperatorData::LoadFileMetaData(ClientContext &context, const\n \tDataChunk current_chunk;\n \tcurrent_chunk.Initialize(return_types);\n \tauto meta_data = reader->GetFileMetadata();\n+\tvector<LogicalType> column_types;\n+\tvector<idx_t> schema_indexes;\n+\tfor (idx_t schema_idx = 0; schema_idx < meta_data->schema.size(); schema_idx++) {\n+\t\tauto &schema_element = meta_data->schema[schema_idx];\n+\t\tif (schema_element.num_children > 0) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tcolumn_types.push_back(ParquetReader::DeriveLogicalType(schema_element, false));\n+\t\tschema_indexes.push_back(schema_idx);\n+\t}\n+\n \tfor (idx_t row_group_idx = 0; row_group_idx < meta_data->row_groups.size(); row_group_idx++) {\n \t\tauto &row_group = meta_data->row_groups[row_group_idx];\n \n+\t\tif (row_group.columns.size() > column_types.size()) {\n+\t\t\tthrow InternalException(\"Too many column in row group: corrupt file?\");\n+\t\t}\n \t\tfor (idx_t col_idx = 0; col_idx < row_group.columns.size(); col_idx++) {\n \t\t\tauto &column = row_group.columns[col_idx];\n \t\t\tauto &col_meta = column.meta_data;\n \t\t\tauto &stats = col_meta.statistics;\n+\t\t\tauto &schema_element = meta_data->schema[schema_indexes[col_idx]];\n+\t\t\tauto &column_type = column_types[col_idx];\n \n \t\t\t// file_name, LogicalType::VARCHAR\n \t\t\tcurrent_chunk.SetValue(0, count, file_path);\n@@ -206,10 +183,12 @@ void ParquetMetaDataOperatorData::LoadFileMetaData(ClientContext &context, const\n \t\t\tcurrent_chunk.SetValue(9, count, ConvertParquetElementToString(col_meta.type));\n \n \t\t\t// stats_min, LogicalType::VARCHAR\n-\t\t\tcurrent_chunk.SetValue(10, count, ConvertParquetStats(col_meta.type, stats.__isset.min, stats.min));\n+\t\t\tcurrent_chunk.SetValue(10, count,\n+\t\t\t                       ConvertParquetStats(column_type, schema_element, stats.__isset.min, stats.min));\n \n \t\t\t// stats_max, LogicalType::VARCHAR\n-\t\t\tcurrent_chunk.SetValue(11, count, ConvertParquetStats(col_meta.type, stats.__isset.max, stats.max));\n+\t\t\tcurrent_chunk.SetValue(11, count,\n+\t\t\t                       ConvertParquetStats(column_type, schema_element, stats.__isset.max, stats.max));\n \n \t\t\t// stats_null_count, LogicalType::BIGINT\n \t\t\tcurrent_chunk.SetValue(\n@@ -221,12 +200,12 @@ void ParquetMetaDataOperatorData::LoadFileMetaData(ClientContext &context, const\n \t\t\t                                                    : Value(LogicalType::BIGINT));\n \n \t\t\t// stats_min_value, LogicalType::VARCHAR\n-\t\t\tcurrent_chunk.SetValue(14, count,\n-\t\t\t                       ConvertParquetStats(col_meta.type, stats.__isset.min_value, stats.min_value));\n+\t\t\tcurrent_chunk.SetValue(\n+\t\t\t    14, count, ConvertParquetStats(column_type, schema_element, stats.__isset.min_value, stats.min_value));\n \n \t\t\t// stats_max_value, LogicalType::VARCHAR\n-\t\t\tcurrent_chunk.SetValue(15, count,\n-\t\t\t                       ConvertParquetStats(col_meta.type, stats.__isset.max_value, stats.max_value));\n+\t\t\tcurrent_chunk.SetValue(\n+\t\t\t    15, count, ConvertParquetStats(column_type, schema_element, stats.__isset.max_value, stats.max_value));\n \n \t\t\t// compression, LogicalType::VARCHAR\n \t\t\tcurrent_chunk.SetValue(16, count, ConvertParquetElementToString(col_meta.codec));\ndiff --git a/extension/parquet/parquet_reader.cpp b/extension/parquet/parquet_reader.cpp\nindex 6e44da18fb44..b26e22aef664 100644\n--- a/extension/parquet/parquet_reader.cpp\n+++ b/extension/parquet/parquet_reader.cpp\n@@ -5,7 +5,6 @@\n \n #include \"boolean_column_reader.hpp\"\n #include \"callback_column_reader.hpp\"\n-#include \"decimal_column_reader.hpp\"\n #include \"list_column_reader.hpp\"\n #include \"string_column_reader.hpp\"\n #include \"struct_column_reader.hpp\"\n@@ -86,12 +85,15 @@ static shared_ptr<ParquetFileMetadataCache> LoadMetadata(Allocator &allocator, F\n \treturn make_shared<ParquetFileMetadataCache>(move(metadata), current_time);\n }\n \n-LogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele) {\n+LogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele, bool binary_as_string) {\n \t// inner node\n \tD_ASSERT(s_ele.__isset.type && s_ele.num_children == 0);\n \tif (s_ele.type == Type::FIXED_LEN_BYTE_ARRAY && !s_ele.__isset.type_length) {\n \t\tthrow IOException(\"FIXED_LEN_BYTE_ARRAY requires length to be set\");\n \t}\n+\tif (s_ele.type == Type::FIXED_LEN_BYTE_ARRAY && s_ele.__isset.logicalType && s_ele.logicalType.__isset.UUID) {\n+\t\treturn LogicalType::UUID;\n+\t}\n \tif (s_ele.__isset.converted_type) {\n \t\tswitch (s_ele.converted_type) {\n \t\tcase ConvertedType::INT_8:\n@@ -177,15 +179,21 @@ LogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele) {\n \t\t\tdefault:\n \t\t\t\tthrow IOException(\"UTF8 converted type can only be set for Type::(FIXED_LEN_)BYTE_ARRAY\");\n \t\t\t}\n+\t\tcase ConvertedType::TIME_MILLIS:\n+\t\tcase ConvertedType::TIME_MICROS:\n+\t\t\tif (s_ele.type == Type::INT64) {\n+\t\t\t\treturn LogicalType::TIME;\n+\t\t\t} else {\n+\t\t\t\tthrow IOException(\"TIME_MICROS converted type can only be set for value of Type::INT64\");\n+\t\t\t}\n+\t\tcase ConvertedType::INTERVAL:\n+\t\t\treturn LogicalType::INTERVAL;\n \t\tcase ConvertedType::MAP:\n \t\tcase ConvertedType::MAP_KEY_VALUE:\n \t\tcase ConvertedType::LIST:\n \t\tcase ConvertedType::ENUM:\n-\t\tcase ConvertedType::TIME_MILLIS:\n-\t\tcase ConvertedType::TIME_MICROS:\n \t\tcase ConvertedType::JSON:\n \t\tcase ConvertedType::BSON:\n-\t\tcase ConvertedType::INTERVAL:\n \t\tdefault:\n \t\t\tthrow IOException(\"Unsupported converted type\");\n \t\t}\n@@ -207,7 +215,7 @@ LogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele) {\n \t\t\treturn LogicalType::DOUBLE;\n \t\tcase Type::BYTE_ARRAY:\n \t\tcase Type::FIXED_LEN_BYTE_ARRAY:\n-\t\t\tif (parquet_options.binary_as_string) {\n+\t\t\tif (binary_as_string) {\n \t\t\t\treturn LogicalType::VARCHAR;\n \t\t\t}\n \t\t\treturn LogicalType::BLOB;\n@@ -217,6 +225,10 @@ LogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele) {\n \t}\n }\n \n+LogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele) {\n+\treturn DeriveLogicalType(s_ele, parquet_options.binary_as_string);\n+}\n+\n unique_ptr<ColumnReader> ParquetReader::CreateReaderRecursive(const FileMetaData *file_meta_data, idx_t depth,\n                                                               idx_t max_define, idx_t max_repeat,\n                                                               idx_t &next_schema_idx, idx_t &next_file_idx) {\ndiff --git a/extension/parquet/parquet_statistics.cpp b/extension/parquet/parquet_statistics.cpp\nindex 40eef0987aa5..09b031ec1fd3 100644\n--- a/extension/parquet/parquet_statistics.cpp\n+++ b/extension/parquet/parquet_statistics.cpp\n@@ -1,8 +1,10 @@\n #include \"parquet_statistics.hpp\"\n+#include \"parquet_decimal_utils.hpp\"\n+#include \"parquet_timestamp.hpp\"\n \n #include \"duckdb.hpp\"\n-#include \"parquet_timestamp.hpp\"\n #ifndef DUCKDB_AMALGAMATION\n+#include \"duckdb/common/types/blob.hpp\"\n #include \"duckdb/common/types/value.hpp\"\n #include \"duckdb/storage/statistics/numeric_statistics.hpp\"\n #include \"duckdb/storage/statistics/string_statistics.hpp\"\n@@ -13,70 +15,173 @@ namespace duckdb {\n using duckdb_parquet::format::ConvertedType;\n using duckdb_parquet::format::Type;\n \n-template <Value (*FUNC)(const_data_ptr_t input)>\n-static unique_ptr<BaseStatistics> TemplatedGetNumericStats(const LogicalType &type,\n-                                                           const duckdb_parquet::format::Statistics &parquet_stats) {\n+static unique_ptr<BaseStatistics> CreateNumericStats(const LogicalType &type,\n+                                                     const duckdb_parquet::format::SchemaElement &schema_ele,\n+                                                     const duckdb_parquet::format::Statistics &parquet_stats) {\n \tauto stats = make_unique<NumericStatistics>(type);\n \n \t// for reasons unknown to science, Parquet defines *both* `min` and `min_value` as well as `max` and\n \t// `max_value`. All are optional. such elegance.\n \tif (parquet_stats.__isset.min) {\n-\t\tstats->min = FUNC((const_data_ptr_t)parquet_stats.min.data());\n+\t\tstats->min = ParquetStatisticsUtils::ConvertValue(type, schema_ele, parquet_stats.min).CastAs(type);\n \t} else if (parquet_stats.__isset.min_value) {\n-\t\tstats->min = FUNC((const_data_ptr_t)parquet_stats.min_value.data());\n+\t\tstats->min = ParquetStatisticsUtils::ConvertValue(type, schema_ele, parquet_stats.min_value).CastAs(type);\n \t} else {\n \t\tstats->min.is_null = true;\n \t}\n \tif (parquet_stats.__isset.max) {\n-\t\tstats->max = FUNC((const_data_ptr_t)parquet_stats.max.data());\n+\t\tstats->max = ParquetStatisticsUtils::ConvertValue(type, schema_ele, parquet_stats.max).CastAs(type);\n \t} else if (parquet_stats.__isset.max_value) {\n-\t\tstats->max = FUNC((const_data_ptr_t)parquet_stats.max_value.data());\n+\t\tstats->max = ParquetStatisticsUtils::ConvertValue(type, schema_ele, parquet_stats.max_value).CastAs(type);\n \t} else {\n \t\tstats->max.is_null = true;\n \t}\n-\t// GCC 4.x insists on a move() here\n \treturn move(stats);\n }\n \n-template <class T>\n-static Value TransformStatisticsPlain(const_data_ptr_t input) {\n-\treturn Value::CreateValue<T>(Load<T>(input));\n-}\n-\n-static Value TransformStatisticsFloat(const_data_ptr_t input) {\n-\tauto val = Load<float>(input);\n-\tif (!Value::FloatIsValid(val)) {\n-\t\treturn Value(LogicalType::FLOAT);\n+Value ParquetStatisticsUtils::ConvertValue(const LogicalType &type,\n+                                           const duckdb_parquet::format::SchemaElement &schema_ele,\n+                                           const std::string &stats) {\n+\tif (stats.empty()) {\n+\t\treturn Value();\n \t}\n-\treturn Value::CreateValue<float>(val);\n-}\n-\n-static Value TransformStatisticsDouble(const_data_ptr_t input) {\n-\tauto val = Load<double>(input);\n-\tif (!Value::DoubleIsValid(val)) {\n-\t\treturn Value(LogicalType::DOUBLE);\n+\tswitch (type.id()) {\n+\tcase LogicalTypeId::BOOLEAN: {\n+\t\tif (stats.size() != sizeof(bool)) {\n+\t\t\tthrow InternalException(\"Incorrect stats size for type BOOLEAN\");\n+\t\t}\n+\t\treturn Value::BOOLEAN(Load<bool>((data_ptr_t)stats.c_str()));\n+\t}\n+\tcase LogicalTypeId::UTINYINT:\n+\tcase LogicalTypeId::USMALLINT:\n+\tcase LogicalTypeId::UINTEGER:\n+\t\tif (stats.size() != sizeof(uint32_t)) {\n+\t\t\tthrow InternalException(\"Incorrect stats size for type UINTEGER\");\n+\t\t}\n+\t\treturn Value::UINTEGER(Load<uint32_t>((data_ptr_t)stats.c_str()));\n+\tcase LogicalTypeId::UBIGINT:\n+\t\tif (stats.size() != sizeof(uint64_t)) {\n+\t\t\tthrow InternalException(\"Incorrect stats size for type UBIGINT\");\n+\t\t}\n+\t\treturn Value::UBIGINT(Load<uint64_t>((data_ptr_t)stats.c_str()));\n+\tcase LogicalTypeId::TINYINT:\n+\tcase LogicalTypeId::SMALLINT:\n+\tcase LogicalTypeId::INTEGER:\n+\t\tif (stats.size() != sizeof(int32_t)) {\n+\t\t\tthrow InternalException(\"Incorrect stats size for type INTEGER\");\n+\t\t}\n+\t\treturn Value::INTEGER(Load<int32_t>((data_ptr_t)stats.c_str()));\n+\tcase LogicalTypeId::BIGINT:\n+\t\tif (stats.size() != sizeof(int64_t)) {\n+\t\t\tthrow InternalException(\"Incorrect stats size for type BIGINT\");\n+\t\t}\n+\t\treturn Value::BIGINT(Load<int64_t>((data_ptr_t)stats.c_str()));\n+\tcase LogicalTypeId::FLOAT: {\n+\t\tif (stats.size() != sizeof(float)) {\n+\t\t\tthrow InternalException(\"Incorrect stats size for type FLOAT\");\n+\t\t}\n+\t\tauto val = Load<float>((data_ptr_t)stats.c_str());\n+\t\tif (!Value::FloatIsValid(val)) {\n+\t\t\treturn Value();\n+\t\t}\n+\t\treturn Value::FLOAT(val);\n+\t}\n+\tcase LogicalTypeId::DOUBLE: {\n+\t\tif (stats.size() != sizeof(double)) {\n+\t\t\tthrow InternalException(\"Incorrect stats size for type DOUBLE\");\n+\t\t}\n+\t\tauto val = Load<double>((data_ptr_t)stats.c_str());\n+\t\tif (!Value::DoubleIsValid(val)) {\n+\t\t\treturn Value();\n+\t\t}\n+\t\treturn Value::DOUBLE(val);\n+\t}\n+\tcase LogicalTypeId::DECIMAL: {\n+\t\tauto width = DecimalType::GetWidth(type);\n+\t\tauto scale = DecimalType::GetScale(type);\n+\t\tswitch (schema_ele.type) {\n+\t\tcase Type::INT32: {\n+\t\t\tif (stats.size() != sizeof(int32_t)) {\n+\t\t\t\tthrow InternalException(\"Incorrect stats size for type %s\", type.ToString());\n+\t\t\t}\n+\t\t\treturn Value::DECIMAL(Load<int32_t>((data_ptr_t)stats.c_str()), width, scale);\n+\t\t}\n+\t\tcase Type::INT64: {\n+\t\t\tif (stats.size() != sizeof(int64_t)) {\n+\t\t\t\tthrow InternalException(\"Incorrect stats size for type %s\", type.ToString());\n+\t\t\t}\n+\t\t\treturn Value::DECIMAL(Load<int64_t>((data_ptr_t)stats.c_str()), width, scale);\n+\t\t}\n+\t\tcase Type::BYTE_ARRAY:\n+\t\tcase Type::FIXED_LEN_BYTE_ARRAY:\n+\t\t\tswitch (type.InternalType()) {\n+\t\t\tcase PhysicalType::INT16:\n+\t\t\t\treturn Value::DECIMAL(\n+\t\t\t\t    ParquetDecimalUtils::ReadDecimalValue<int16_t>((const_data_ptr_t)stats.c_str(), stats.size()),\n+\t\t\t\t    width, scale);\n+\t\t\tcase PhysicalType::INT32:\n+\t\t\t\treturn Value::DECIMAL(\n+\t\t\t\t    ParquetDecimalUtils::ReadDecimalValue<int32_t>((const_data_ptr_t)stats.c_str(), stats.size()),\n+\t\t\t\t    width, scale);\n+\t\t\tcase PhysicalType::INT64:\n+\t\t\t\treturn Value::DECIMAL(\n+\t\t\t\t    ParquetDecimalUtils::ReadDecimalValue<int64_t>((const_data_ptr_t)stats.c_str(), stats.size()),\n+\t\t\t\t    width, scale);\n+\t\t\tcase PhysicalType::INT128:\n+\t\t\t\treturn Value::DECIMAL(\n+\t\t\t\t    ParquetDecimalUtils::ReadDecimalValue<hugeint_t>((const_data_ptr_t)stats.c_str(), stats.size()),\n+\t\t\t\t    width, scale);\n+\t\t\tdefault:\n+\t\t\t\tthrow InternalException(\"Unsupported internal type for decimal\");\n+\t\t\t}\n+\t\tdefault:\n+\t\t\tthrow InternalException(\"Unsupported internal type for decimal?..\");\n+\t\t}\n+\t}\n+\tcase LogicalType::VARCHAR:\n+\tcase LogicalType::BLOB:\n+\t\tif (Value::StringIsValid(stats)) {\n+\t\t\treturn Value(stats);\n+\t\t} else {\n+\t\t\treturn Value(Blob::ToString(string_t(stats)));\n+\t\t}\n+\tcase LogicalTypeId::DATE:\n+\t\tif (stats.size() != sizeof(int32_t)) {\n+\t\t\tthrow InternalException(\"Incorrect stats size for type DATE\");\n+\t\t}\n+\t\treturn Value::DATE(date_t(Load<int32_t>((data_ptr_t)stats.c_str())));\n+\tcase LogicalTypeId::TIME:\n+\t\tif (stats.size() != sizeof(int64_t)) {\n+\t\t\tthrow InternalException(\"Incorrect stats size for type TIME\");\n+\t\t}\n+\t\treturn Value::TIME(dtime_t(Load<int64_t>((data_ptr_t)stats.c_str())));\n+\tcase LogicalTypeId::TIMESTAMP: {\n+\t\tif (schema_ele.type == Type::INT96) {\n+\t\t\tif (stats.size() != sizeof(Int96)) {\n+\t\t\t\tthrow InternalException(\"Incorrect stats size for type TIMESTAMP\");\n+\t\t\t}\n+\t\t\treturn Value::TIMESTAMP(ImpalaTimestampToTimestamp(Load<Int96>((data_ptr_t)stats.c_str())));\n+\t\t} else {\n+\t\t\tD_ASSERT(schema_ele.type == Type::INT64);\n+\t\t\tif (stats.size() != sizeof(int64_t)) {\n+\t\t\t\tthrow InternalException(\"Incorrect stats size for type TIMESTAMP\");\n+\t\t\t}\n+\t\t\tauto val = Load<int64_t>((data_ptr_t)stats.c_str());\n+\t\t\tif (schema_ele.converted_type == duckdb_parquet::format::ConvertedType::TIMESTAMP_MILLIS) {\n+\t\t\t\treturn Value::TIMESTAMPMS(timestamp_t(val));\n+\t\t\t} else {\n+\t\t\t\treturn Value::TIMESTAMP(timestamp_t(val));\n+\t\t\t}\n+\t\t}\n+\t}\n+\tdefault:\n+\t\tthrow InternalException(\"Unsupported type for stats %s\", type.ToString());\n \t}\n-\treturn Value::CreateValue<double>(val);\n-}\n-\n-static Value TransformStatisticsDate(const_data_ptr_t input) {\n-\treturn Value::DATE(ParquetIntToDate(Load<int32_t>(input)));\n-}\n-\n-static Value TransformStatisticsTimestampMs(const_data_ptr_t input) {\n-\treturn Value::TIMESTAMP(ParquetTimestampMsToTimestamp(Load<int64_t>(input)));\n-}\n-\n-static Value TransformStatisticsTimestampMicros(const_data_ptr_t input) {\n-\treturn Value::TIMESTAMP(ParquetTimestampMicrosToTimestamp(Load<int64_t>(input)));\n-}\n-\n-static Value TransformStatisticsTimestampImpala(const_data_ptr_t input) {\n-\treturn Value::TIMESTAMP(ImpalaTimestampToTimestamp(Load<Int96>(input)));\n }\n \n-unique_ptr<BaseStatistics> ParquetTransformColumnStatistics(const SchemaElement &s_ele, const LogicalType &type,\n-                                                            const ColumnChunk &column_chunk) {\n+unique_ptr<BaseStatistics> ParquetStatisticsUtils::TransformColumnStatistics(const SchemaElement &s_ele,\n+                                                                             const LogicalType &type,\n+                                                                             const ColumnChunk &column_chunk) {\n \tif (!column_chunk.__isset.meta_data || !column_chunk.meta_data.__isset.statistics) {\n \t\t// no stats present for row group\n \t\treturn nullptr;\n@@ -85,67 +190,25 @@ unique_ptr<BaseStatistics> ParquetTransformColumnStatistics(const SchemaElement\n \tunique_ptr<BaseStatistics> row_group_stats;\n \n \tswitch (type.id()) {\n-\n \tcase LogicalTypeId::UTINYINT:\n-\t\trow_group_stats = TemplatedGetNumericStats<TransformStatisticsPlain<uint8_t>>(type, parquet_stats);\n-\t\tbreak;\n-\n \tcase LogicalTypeId::USMALLINT:\n-\t\trow_group_stats = TemplatedGetNumericStats<TransformStatisticsPlain<uint16_t>>(type, parquet_stats);\n-\t\tbreak;\n-\n \tcase LogicalTypeId::UINTEGER:\n-\t\trow_group_stats = TemplatedGetNumericStats<TransformStatisticsPlain<uint32_t>>(type, parquet_stats);\n-\t\tbreak;\n-\n \tcase LogicalTypeId::UBIGINT:\n-\t\trow_group_stats = TemplatedGetNumericStats<TransformStatisticsPlain<uint64_t>>(type, parquet_stats);\n-\t\tbreak;\n+\tcase LogicalTypeId::TINYINT:\n+\tcase LogicalTypeId::SMALLINT:\n \tcase LogicalTypeId::INTEGER:\n-\t\trow_group_stats = TemplatedGetNumericStats<TransformStatisticsPlain<int32_t>>(type, parquet_stats);\n-\t\tbreak;\n-\n \tcase LogicalTypeId::BIGINT:\n-\t\trow_group_stats = TemplatedGetNumericStats<TransformStatisticsPlain<int64_t>>(type, parquet_stats);\n-\t\tbreak;\n-\n \tcase LogicalTypeId::FLOAT:\n-\t\trow_group_stats = TemplatedGetNumericStats<TransformStatisticsFloat>(type, parquet_stats);\n-\t\tbreak;\n-\n \tcase LogicalTypeId::DOUBLE:\n-\t\trow_group_stats = TemplatedGetNumericStats<TransformStatisticsDouble>(type, parquet_stats);\n-\t\tbreak;\n-\n \tcase LogicalTypeId::DATE:\n-\t\trow_group_stats = TemplatedGetNumericStats<TransformStatisticsDate>(type, parquet_stats);\n+\tcase LogicalTypeId::TIME:\n+\tcase LogicalTypeId::TIMESTAMP:\n+\tcase LogicalTypeId::TIMESTAMP_SEC:\n+\tcase LogicalTypeId::TIMESTAMP_MS:\n+\tcase LogicalTypeId::TIMESTAMP_NS:\n+\tcase LogicalTypeId::DECIMAL:\n+\t\trow_group_stats = CreateNumericStats(type, s_ele, parquet_stats);\n \t\tbreak;\n-\n-\t\t// here we go, our favorite type\n-\tcase LogicalTypeId::TIMESTAMP: {\n-\t\tswitch (s_ele.type) {\n-\t\tcase Type::INT64:\n-\t\t\t// arrow timestamp\n-\t\t\tswitch (s_ele.converted_type) {\n-\t\t\tcase ConvertedType::TIMESTAMP_MICROS:\n-\t\t\t\trow_group_stats = TemplatedGetNumericStats<TransformStatisticsTimestampMicros>(type, parquet_stats);\n-\t\t\t\tbreak;\n-\t\t\tcase ConvertedType::TIMESTAMP_MILLIS:\n-\t\t\t\trow_group_stats = TemplatedGetNumericStats<TransformStatisticsTimestampMs>(type, parquet_stats);\n-\t\t\t\tbreak;\n-\t\t\tdefault:\n-\t\t\t\treturn nullptr;\n-\t\t\t}\n-\t\t\tbreak;\n-\t\tcase Type::INT96:\n-\t\t\t// impala timestamp\n-\t\t\trow_group_stats = TemplatedGetNumericStats<TransformStatisticsTimestampImpala>(type, parquet_stats);\n-\t\t\tbreak;\n-\t\tdefault:\n-\t\t\treturn nullptr;\n-\t\t}\n-\t\tbreak;\n-\t}\n \tcase LogicalTypeId::VARCHAR: {\n \t\tauto string_stats = make_unique<StringStatistics>(type);\n \t\tif (parquet_stats.__isset.min) {\n@@ -162,8 +225,8 @@ unique_ptr<BaseStatistics> ParquetTransformColumnStatistics(const SchemaElement\n \t\t} else {\n \t\t\treturn nullptr;\n \t\t}\n-\n \t\tstring_stats->has_unicode = true; // we dont know better\n+\t\tstring_stats->max_string_length = NumericLimits<uint32_t>::Maximum();\n \t\trow_group_stats = move(string_stats);\n \t\tbreak;\n \t}\n@@ -176,7 +239,7 @@ unique_ptr<BaseStatistics> ParquetTransformColumnStatistics(const SchemaElement\n \tif (row_group_stats) {\n \t\tif (column_chunk.meta_data.type == duckdb_parquet::format::Type::FLOAT ||\n \t\t    column_chunk.meta_data.type == duckdb_parquet::format::Type::DOUBLE) {\n-\t\t\t// floats/doubles can have infinity, which becomes NULL\n+\t\t\t// floats/doubles can have infinity, which can become NULL\n \t\t\trow_group_stats->validity_stats = make_unique<ValidityStatistics>(true);\n \t\t} else if (parquet_stats.__isset.null_count) {\n \t\t\trow_group_stats->validity_stats = make_unique<ValidityStatistics>(parquet_stats.null_count != 0);\ndiff --git a/extension/parquet/parquet_timestamp.cpp b/extension/parquet/parquet_timestamp.cpp\nindex 5f0605c039a9..ddb3ee1b489c 100644\n--- a/extension/parquet/parquet_timestamp.cpp\n+++ b/extension/parquet/parquet_timestamp.cpp\n@@ -22,7 +22,7 @@ int64_t ImpalaTimestampToNanoseconds(const Int96 &impala_timestamp) {\n \n timestamp_t ImpalaTimestampToTimestamp(const Int96 &raw_ts) {\n \tauto impala_ns = ImpalaTimestampToNanoseconds(raw_ts);\n-\treturn Timestamp::FromEpochMs(impala_ns / 1000000);\n+\treturn Timestamp::FromEpochNanoSeconds(impala_ns);\n }\n \n Int96 TimestampToImpalaTimestamp(timestamp_t &ts) {\n@@ -49,4 +49,8 @@ date_t ParquetIntToDate(const int32_t &raw_date) {\n \treturn date_t(raw_date);\n }\n \n+dtime_t ParquetIntToTime(const int64_t &raw_time) {\n+\treturn dtime_t(raw_time);\n+}\n+\n } // namespace duckdb\ndiff --git a/extension/parquet/parquet_writer.cpp b/extension/parquet/parquet_writer.cpp\nindex 0653125df3d2..08af4badfad5 100644\n--- a/extension/parquet/parquet_writer.cpp\n+++ b/extension/parquet/parquet_writer.cpp\n@@ -60,19 +60,23 @@ Type::type ParquetWriter::DuckDBTypeToParquetType(const LogicalType &duckdb_type\n \tcase LogicalTypeId::SMALLINT:\n \tcase LogicalTypeId::INTEGER:\n \tcase LogicalTypeId::DATE:\n+\tcase LogicalTypeId::DATE_TZ:\n \t\treturn Type::INT32;\n \tcase LogicalTypeId::BIGINT:\n \t\treturn Type::INT64;\n \tcase LogicalTypeId::FLOAT:\n \t\treturn Type::FLOAT;\n-\tcase LogicalTypeId::DECIMAL: // for now...\n \tcase LogicalTypeId::DOUBLE:\n \tcase LogicalTypeId::HUGEINT:\n \t\treturn Type::DOUBLE;\n+\tcase LogicalTypeId::ENUM:\n \tcase LogicalTypeId::VARCHAR:\n \tcase LogicalTypeId::BLOB:\n \t\treturn Type::BYTE_ARRAY;\n+\tcase LogicalTypeId::TIME:\n+\tcase LogicalTypeId::TIME_TZ:\n \tcase LogicalTypeId::TIMESTAMP:\n+\tcase LogicalTypeId::TIMESTAMP_TZ:\n \tcase LogicalTypeId::TIMESTAMP_MS:\n \tcase LogicalTypeId::TIMESTAMP_NS:\n \tcase LogicalTypeId::TIMESTAMP_SEC:\n@@ -83,53 +87,128 @@ Type::type ParquetWriter::DuckDBTypeToParquetType(const LogicalType &duckdb_type\n \t\treturn Type::INT32;\n \tcase LogicalTypeId::UBIGINT:\n \t\treturn Type::INT64;\n+\tcase LogicalTypeId::INTERVAL:\n+\tcase LogicalTypeId::UUID:\n+\t\treturn Type::FIXED_LEN_BYTE_ARRAY;\n+\tcase LogicalTypeId::DECIMAL:\n+\t\tswitch (duckdb_type.InternalType()) {\n+\t\tcase PhysicalType::INT16:\n+\t\tcase PhysicalType::INT32:\n+\t\t\treturn Type::INT32;\n+\t\tcase PhysicalType::INT64:\n+\t\t\treturn Type::INT64;\n+\t\tcase PhysicalType::INT128:\n+\t\t\treturn Type::FIXED_LEN_BYTE_ARRAY;\n+\t\tdefault:\n+\t\t\tthrow InternalException(\"Unsupported internal decimal type\");\n+\t\t}\n \tdefault:\n-\t\tthrow NotImplementedException(duckdb_type.ToString());\n+\t\tthrow NotImplementedException(\"Unimplemented type for Parquet \\\"%s\\\"\", duckdb_type.ToString());\n \t}\n }\n \n-bool ParquetWriter::DuckDBTypeToConvertedType(const LogicalType &duckdb_type, ConvertedType::type &result) {\n+void ParquetWriter::SetSchemaProperties(const LogicalType &duckdb_type,\n+                                        duckdb_parquet::format::SchemaElement &schema_ele) {\n \tswitch (duckdb_type.id()) {\n \tcase LogicalTypeId::TINYINT:\n-\t\tresult = ConvertedType::INT_8;\n-\t\treturn true;\n+\t\tschema_ele.converted_type = ConvertedType::INT_8;\n+\t\tschema_ele.__isset.converted_type = true;\n+\t\tbreak;\n \tcase LogicalTypeId::SMALLINT:\n-\t\tresult = ConvertedType::INT_16;\n-\t\treturn true;\n+\t\tschema_ele.converted_type = ConvertedType::INT_16;\n+\t\tschema_ele.__isset.converted_type = true;\n+\t\tbreak;\n \tcase LogicalTypeId::INTEGER:\n-\t\tresult = ConvertedType::INT_32;\n-\t\treturn true;\n+\t\tschema_ele.converted_type = ConvertedType::INT_32;\n+\t\tschema_ele.__isset.converted_type = true;\n+\t\tbreak;\n \tcase LogicalTypeId::BIGINT:\n-\t\tresult = ConvertedType::INT_64;\n-\t\treturn true;\n+\t\tschema_ele.converted_type = ConvertedType::INT_64;\n+\t\tschema_ele.__isset.converted_type = true;\n+\t\tbreak;\n \tcase LogicalTypeId::UTINYINT:\n-\t\tresult = ConvertedType::UINT_8;\n-\t\treturn true;\n+\t\tschema_ele.converted_type = ConvertedType::UINT_8;\n+\t\tschema_ele.__isset.converted_type = true;\n+\t\tbreak;\n \tcase LogicalTypeId::USMALLINT:\n-\t\tresult = ConvertedType::UINT_16;\n-\t\treturn true;\n+\t\tschema_ele.converted_type = ConvertedType::UINT_16;\n+\t\tschema_ele.__isset.converted_type = true;\n+\t\tbreak;\n \tcase LogicalTypeId::UINTEGER:\n-\t\tresult = ConvertedType::UINT_32;\n-\t\treturn true;\n+\t\tschema_ele.converted_type = ConvertedType::UINT_32;\n+\t\tschema_ele.__isset.converted_type = true;\n+\t\tbreak;\n \tcase LogicalTypeId::UBIGINT:\n-\t\tresult = ConvertedType::UINT_64;\n-\t\treturn true;\n+\t\tschema_ele.converted_type = ConvertedType::UINT_64;\n+\t\tschema_ele.__isset.converted_type = true;\n+\t\tbreak;\n+\tcase LogicalTypeId::DATE_TZ:\n \tcase LogicalTypeId::DATE:\n-\t\tresult = ConvertedType::DATE;\n-\t\treturn true;\n-\tcase LogicalTypeId::VARCHAR:\n-\t\tresult = ConvertedType::UTF8;\n-\t\treturn true;\n+\t\tschema_ele.converted_type = ConvertedType::DATE;\n+\t\tschema_ele.__isset.converted_type = true;\n+\t\tbreak;\n+\tcase LogicalTypeId::TIME_TZ:\n+\tcase LogicalTypeId::TIME:\n+\t\tschema_ele.converted_type = ConvertedType::TIME_MICROS;\n+\t\tschema_ele.__isset.converted_type = true;\n+\t\tschema_ele.logicalType.__isset.TIME = true;\n+\t\tschema_ele.logicalType.TIME.isAdjustedToUTC = true;\n+\t\tschema_ele.logicalType.TIME.unit.__isset.MICROS = true;\n+\t\tbreak;\n+\tcase LogicalTypeId::TIMESTAMP_TZ:\n \tcase LogicalTypeId::TIMESTAMP:\n \tcase LogicalTypeId::TIMESTAMP_NS:\n \tcase LogicalTypeId::TIMESTAMP_SEC:\n-\t\tresult = ConvertedType::TIMESTAMP_MICROS;\n-\t\treturn true;\n+\t\tschema_ele.converted_type = ConvertedType::TIMESTAMP_MICROS;\n+\t\tschema_ele.__isset.converted_type = true;\n+\t\tschema_ele.__isset.logicalType = true;\n+\t\tschema_ele.logicalType.__isset.TIMESTAMP = true;\n+\t\tschema_ele.logicalType.TIMESTAMP.isAdjustedToUTC = true;\n+\t\tschema_ele.logicalType.TIMESTAMP.unit.__isset.MICROS = true;\n+\t\tbreak;\n \tcase LogicalTypeId::TIMESTAMP_MS:\n-\t\tresult = ConvertedType::TIMESTAMP_MILLIS;\n-\t\treturn true;\n+\t\tschema_ele.converted_type = ConvertedType::TIMESTAMP_MILLIS;\n+\t\tschema_ele.__isset.converted_type = true;\n+\t\tschema_ele.__isset.logicalType = true;\n+\t\tschema_ele.logicalType.__isset.TIMESTAMP = true;\n+\t\tschema_ele.logicalType.TIMESTAMP.isAdjustedToUTC = true;\n+\t\tschema_ele.logicalType.TIMESTAMP.unit.__isset.MILLIS = true;\n+\t\tbreak;\n+\tcase LogicalTypeId::ENUM:\n+\tcase LogicalTypeId::VARCHAR:\n+\t\tschema_ele.converted_type = ConvertedType::UTF8;\n+\t\tschema_ele.__isset.converted_type = true;\n+\t\tbreak;\n+\tcase LogicalTypeId::INTERVAL:\n+\t\tschema_ele.type_length = 12;\n+\t\tschema_ele.converted_type = ConvertedType::INTERVAL;\n+\t\tschema_ele.__isset.type_length = true;\n+\t\tschema_ele.__isset.converted_type = true;\n+\t\tbreak;\n+\tcase LogicalTypeId::UUID:\n+\t\tschema_ele.type_length = 16;\n+\t\tschema_ele.__isset.type_length = true;\n+\t\tschema_ele.__isset.logicalType = true;\n+\t\tschema_ele.logicalType.__isset.UUID = true;\n+\t\tbreak;\n+\tcase LogicalTypeId::DECIMAL:\n+\t\tschema_ele.converted_type = ConvertedType::DECIMAL;\n+\t\tschema_ele.precision = DecimalType::GetWidth(duckdb_type);\n+\t\tschema_ele.scale = DecimalType::GetScale(duckdb_type);\n+\t\tschema_ele.__isset.converted_type = true;\n+\t\tschema_ele.__isset.precision = true;\n+\t\tschema_ele.__isset.scale = true;\n+\t\tif (duckdb_type.InternalType() == PhysicalType::INT128) {\n+\t\t\tschema_ele.type_length = 16;\n+\t\t\tschema_ele.__isset.type_length = true;\n+\t\t}\n+\t\tschema_ele.__isset.logicalType = true;\n+\t\tschema_ele.logicalType.__isset.DECIMAL = true;\n+\t\tschema_ele.logicalType.DECIMAL.precision = schema_ele.precision;\n+\t\tschema_ele.logicalType.DECIMAL.scale = schema_ele.scale;\n+\t\tbreak;\n \tdefault:\n-\t\treturn false;\n+\t\tbreak;\n \t}\n }\n \n@@ -159,9 +238,10 @@ ParquetWriter::ParquetWriter(FileSystem &fs, string file_name_p, FileOpener *fil\n \tfile_meta_data.schema[0].repetition_type = duckdb_parquet::format::FieldRepetitionType::REQUIRED;\n \tfile_meta_data.schema[0].__isset.repetition_type = true;\n \n+\tvector<string> schema_path;\n \tfor (idx_t i = 0; i < sql_types.size(); i++) {\n-\t\tcolumn_writers.push_back(\n-\t\t    ColumnWriter::CreateWriterRecursive(file_meta_data.schema, *this, sql_types[i], column_names[i]));\n+\t\tcolumn_writers.push_back(ColumnWriter::CreateWriterRecursive(file_meta_data.schema, *this, sql_types[i],\n+\t\t                                                             column_names[i], schema_path));\n \t}\n }\n \n@@ -181,8 +261,7 @@ void ParquetWriter::Flush(ChunkCollection &buffer) {\n \tauto &chunks = buffer.Chunks();\n \tD_ASSERT(buffer.ColumnCount() == column_writers.size());\n \tfor (idx_t col_idx = 0; col_idx < buffer.ColumnCount(); col_idx++) {\n-\t\tvector<string> schema_path;\n-\t\tauto write_state = column_writers[col_idx]->InitializeWriteState(row_group, move(schema_path));\n+\t\tauto write_state = column_writers[col_idx]->InitializeWriteState(row_group);\n \t\tfor (idx_t chunk_idx = 0; chunk_idx < chunks.size(); chunk_idx++) {\n \t\t\tcolumn_writers[col_idx]->Prepare(*write_state, nullptr, chunks[chunk_idx]->data[col_idx],\n \t\t\t                                 chunks[chunk_idx]->size());\ndiff --git a/src/function/scalar/uuid/gen_random.cpp b/src/function/scalar/uuid/gen_random.cpp\nindex 474eca1f694a..f802a7f27bef 100644\n--- a/src/function/scalar/uuid/gen_random.cpp\n+++ b/src/function/scalar/uuid/gen_random.cpp\n@@ -65,9 +65,9 @@ static void GenerateUUIDFunction(DataChunk &args, ExpressionState &state, Vector\n }\n \n void UUIDFun::RegisterFunction(BuiltinFunctions &set) {\n+\tScalarFunction uuid_function({}, LogicalType::UUID, GenerateUUIDFunction, true, UUIDRandomBind);\n \t// generate a random uuid\n-\tset.AddFunction(\n-\t    ScalarFunction(\"gen_random_uuid\", {}, LogicalType::UUID, GenerateUUIDFunction, true, UUIDRandomBind));\n+\tset.AddFunction({\"uuid\", \"gen_random_uuid\"}, uuid_function);\n }\n \n } // namespace duckdb\ndiff --git a/src/main/client_context.cpp b/src/main/client_context.cpp\nindex 8016eb28a066..ec05db062f27 100644\n--- a/src/main/client_context.cpp\n+++ b/src/main/client_context.cpp\n@@ -798,8 +798,8 @@ string ClientContext::VerifyQuery(ClientContextLock &lock, const string &query,\n \t} catch (std::exception &ex) {\n \t\toriginal_result->error = ex.what();\n \t\toriginal_result->success = false;\n-\t\tinterrupted = false;\n \t}\n+\tinterrupted = false;\n \n \t// check explain, only if q does not already contain EXPLAIN\n \tif (original_result->success) {\n@@ -819,8 +819,8 @@ string ClientContext::VerifyQuery(ClientContextLock &lock, const string &query,\n \t} catch (std::exception &ex) {\n \t\tcopied_result->error = ex.what();\n \t\tcopied_result->success = false;\n-\t\tinterrupted = false;\n \t}\n+\tinterrupted = false;\n \t// now execute the deserialized statement\n \ttry {\n \t\tauto result = RunStatementInternal(lock, query, move(deserialized_stmt), false, false);\n@@ -828,8 +828,8 @@ string ClientContext::VerifyQuery(ClientContextLock &lock, const string &query,\n \t} catch (std::exception &ex) {\n \t\tdeserialized_result->error = ex.what();\n \t\tdeserialized_result->success = false;\n-\t\tinterrupted = false;\n \t}\n+\tinterrupted = false;\n \t// now execute the unoptimized statement\n \tconfig.enable_optimizer = false;\n \ttry {\n@@ -838,8 +838,8 @@ string ClientContext::VerifyQuery(ClientContextLock &lock, const string &query,\n \t} catch (std::exception &ex) {\n \t\tunoptimized_result->error = ex.what();\n \t\tunoptimized_result->success = false;\n-\t\tinterrupted = false;\n \t}\n+\tinterrupted = false;\n \tconfig.enable_optimizer = true;\n \n \tif (profiling_is_enabled) {\ndiff --git a/src/optimizer/statistics/expression/propagate_comparison.cpp b/src/optimizer/statistics/expression/propagate_comparison.cpp\nindex 97c954419470..f9882120822b 100644\n--- a/src/optimizer/statistics/expression/propagate_comparison.cpp\n+++ b/src/optimizer/statistics/expression/propagate_comparison.cpp\n@@ -11,6 +11,10 @@ FilterPropagateResult StatisticsPropagator::PropagateComparison(BaseStatistics &\n \t// only handle numerics for now\n \tswitch (left.type.InternalType()) {\n \tcase PhysicalType::BOOL:\n+\tcase PhysicalType::UINT8:\n+\tcase PhysicalType::UINT16:\n+\tcase PhysicalType::UINT32:\n+\tcase PhysicalType::UINT64:\n \tcase PhysicalType::INT8:\n \tcase PhysicalType::INT16:\n \tcase PhysicalType::INT32:\ndiff --git a/src/optimizer/statistics/expression/propagate_constant.cpp b/src/optimizer/statistics/expression/propagate_constant.cpp\nindex d5f544efca62..0a4bdb68ebe9 100644\n--- a/src/optimizer/statistics/expression/propagate_constant.cpp\n+++ b/src/optimizer/statistics/expression/propagate_constant.cpp\n@@ -10,6 +10,10 @@ namespace duckdb {\n unique_ptr<BaseStatistics> StatisticsPropagator::StatisticsFromValue(const Value &input) {\n \tswitch (input.type().InternalType()) {\n \tcase PhysicalType::BOOL:\n+\tcase PhysicalType::UINT8:\n+\tcase PhysicalType::UINT16:\n+\tcase PhysicalType::UINT32:\n+\tcase PhysicalType::UINT64:\n \tcase PhysicalType::INT8:\n \tcase PhysicalType::INT16:\n \tcase PhysicalType::INT32:\ndiff --git a/src/storage/statistics/string_statistics.cpp b/src/storage/statistics/string_statistics.cpp\nindex 7f15ef473644..b2eab190a6cd 100644\n--- a/src/storage/statistics/string_statistics.cpp\n+++ b/src/storage/statistics/string_statistics.cpp\n@@ -23,7 +23,6 @@ unique_ptr<BaseStatistics> StringStatistics::Copy() {\n \tmemcpy(stats->max, max, MAX_STRING_MINMAX_SIZE);\n \tstats->has_unicode = has_unicode;\n \tstats->max_string_length = max_string_length;\n-\tstats->max_string_length = max_string_length;\n \tif (validity_stats) {\n \t\tstats->validity_stats = validity_stats->Copy();\n \t}\n",
  "test_patch": "diff --git a/data/parquet-testing/boolean_stats.parquet b/data/parquet-testing/boolean_stats.parquet\nnew file mode 100644\nindex 000000000000..24732e003687\nBinary files /dev/null and b/data/parquet-testing/boolean_stats.parquet differ\ndiff --git a/data/parquet-testing/date_stats.parquet b/data/parquet-testing/date_stats.parquet\nnew file mode 100644\nindex 000000000000..2b9980df5004\nBinary files /dev/null and b/data/parquet-testing/date_stats.parquet differ\ndiff --git a/data/parquet-testing/decimal_stats.parquet b/data/parquet-testing/decimal_stats.parquet\nnew file mode 100644\nindex 000000000000..a61842106769\nBinary files /dev/null and b/data/parquet-testing/decimal_stats.parquet differ\ndiff --git a/data/parquet-testing/signed_stats.parquet b/data/parquet-testing/signed_stats.parquet\nnew file mode 100644\nindex 000000000000..845f9c979eec\nBinary files /dev/null and b/data/parquet-testing/signed_stats.parquet differ\ndiff --git a/data/parquet-testing/unsigned_stats.parquet b/data/parquet-testing/unsigned_stats.parquet\nnew file mode 100644\nindex 000000000000..09d9a2b42eaa\nBinary files /dev/null and b/data/parquet-testing/unsigned_stats.parquet differ\ndiff --git a/data/parquet-testing/varchar_stats.parquet b/data/parquet-testing/varchar_stats.parquet\nnew file mode 100644\nindex 000000000000..0dd319251185\nBinary files /dev/null and b/data/parquet-testing/varchar_stats.parquet differ\ndiff --git a/test/optimizer/statistics/statistics_filter.test b/test/optimizer/statistics/statistics_filter.test\nindex f39425bf98ac..860d92e999c0 100644\n--- a/test/optimizer/statistics/statistics_filter.test\n+++ b/test/optimizer/statistics/statistics_filter.test\n@@ -2,8 +2,10 @@\n # description: Statistics propagation test with filters\n # group: [statistics]\n \n+foreach type utinyint usmallint uinteger ubigint tinyint smallint integer bigint hugeint float double\n+\n statement ok\n-CREATE TABLE integers AS SELECT * FROM (VALUES (1), (2), (3)) tbl(i);\n+CREATE TABLE integers AS SELECT i::${type} i FROM (VALUES (1), (2), (3)) tbl(i);\n \n statement ok\n PRAGMA explain_output = OPTIMIZED_ONLY;\n@@ -250,3 +252,8 @@ query I\n SELECT * FROM (SELECT * FROM integers LIMIT 10) integers(i) WHERE i<=1;\n ----\n 1\n+\n+statement ok\n+DROP TABLE integers;\n+\n+endloop\ndiff --git a/test/parquet/test_parquet_reader.test b/test/parquet/test_parquet_reader.test\nindex ca387b55e57e..c60189b16787 100644\n--- a/test/parquet/test_parquet_reader.test\n+++ b/test/parquet/test_parquet_reader.test\n@@ -347,8 +347,6 @@ SELECT * FROM parquet_scan('data/parquet-testing/struct.parquet') limit 50;\n {'str_field': hello, 'f64_field': NULL}\n {'str_field': NULL, 'f64_field': 1.230000}\n \n-mode skip\n-\n query I\n SELECT * FROM parquet_scan('data/parquet-testing/arrow/byte_array_decimal.parquet') limit 50;\n ----\n@@ -377,8 +375,6 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/byte_array_decimal.parque\n 23.00\t\n 24.00\t\n \n-mode unskip\n-\n query II\n SELECT * FROM parquet_scan('data/parquet-testing/arrow/list_columns.parquet') limit 50;\n ----\n@@ -942,8 +938,6 @@ f2807544-a424-444a-add3-3d5d486b70e2\n d0018041-41e3-4013-ba90-535ba03d46c3\t\n c50e8ade-6051-436f-a26e-acc9c0594be5\t\n \n-mode unskip\n-\n mode skip\n \n query III\ndiff --git a/test/sql/copy/parquet/parquet_stats.test b/test/sql/copy/parquet/parquet_stats.test\nnew file mode 100644\nindex 000000000000..2ec66720dffe\n--- /dev/null\n+++ b/test/sql/copy/parquet/parquet_stats.test\n@@ -0,0 +1,188 @@\n+# name: test/sql/copy/parquet/parquet_stats.test\n+# description: Test stats reading in parquet reader\n+# group: [parquet]\n+\n+require parquet\n+\n+# boolean values\n+query IIII\n+select stats_min, stats_max, stats_min_value, stats_max_value from parquet_metadata('data/parquet-testing/boolean_stats.parquet');\n+----\n+false\ttrue\tfalse\ttrue\n+\n+# signed numbers\n+query IIII\n+select stats_min, stats_max, stats_min_value, stats_max_value from parquet_metadata('data/parquet-testing/signed_stats.parquet');\n+----\n+-128\t127\t-128\t127\n+-32768\t32767\t-32768\t32767\n+-2147483648\t2147483647\t-2147483648\t2147483647\n+-9223372036854775808\t9223372036854775807\t-9223372036854775808\t9223372036854775807\n+\n+query IIII\n+select * from 'data/parquet-testing/signed_stats.parquet';\n+----\n+-128\t-32768\t-2147483648\t-9223372036854775808\n+127\t32767\t2147483647\t9223372036854775807\n+\n+# unsigned numbers\n+query IIII\n+select stats_min, stats_max, stats_min_value, stats_max_value from parquet_metadata('data/parquet-testing/unsigned_stats.parquet');\n+----\n+NULL\tNULL\t0\t255\n+NULL\tNULL\t0\t65535\n+0\t4294967295\t0\t4294967295\n+NULL\tNULL\t0\t18446744073709551615\n+\n+query IIII\n+select * from 'data/parquet-testing/unsigned_stats.parquet';\n+----\n+0\t0\t0\t0\n+255\t65535\t4294967295\t18446744073709551615\n+\n+# dates/times/timestamps\n+query IIII\n+select stats_min, stats_max, stats_min_value, stats_max_value from parquet_metadata('data/parquet-testing/date_stats.parquet');\n+----\n+1900-01-01\t2030-12-31\t1900-01-01\t2030-12-31\n+00:00:00\t23:59:59\t00:00:00\t23:59:59\n+1990-01-01 00:00:00\t2030-12-31 23:59:59\t1990-01-01 00:00:00\t2030-12-31 23:59:59\n+1900-01-01 00:00:00\t2030-12-31 23:59:59\t1900-01-01 00:00:00\t2030-12-31 23:59:59\n+1900-01-01 00:00:00\t2030-12-31 23:59:59\t1900-01-01 00:00:00\t2030-12-31 23:59:59\n+1900-01-01 00:00:00\t2030-12-31 23:59:59\t1900-01-01 00:00:00\t2030-12-31 23:59:59\n+\n+query IIIIII\n+select * from 'data/parquet-testing/date_stats.parquet';\n+----\n+1900-01-01\t00:00:00\t1990-01-01 00:00:00\t1900-01-01 00:00:00\t1900-01-01 00:00:00\t1900-01-01 00:00:00\n+2030-12-31\t23:59:59\t2030-12-31 23:59:59\t2030-12-31 23:59:59\t2030-12-31 23:59:59\t2030-12-31 23:59:59\n+\n+# varchar/blob stats\n+query IIII\n+select stats_min, stats_max, stats_min_value, stats_max_value from parquet_metadata('data/parquet-testing/varchar_stats.parquet');\n+----\n+NULL\tNULL\thello world\tworld hello\n+NULL\tNULL\thello\\x00world\tworld\\x00hello\n+\n+query II\n+select * from 'data/parquet-testing/varchar_stats.parquet';\n+----\n+hello world\thello\\x00world\n+world hello\tworld\\x00hello\n+\n+# decimal stats\n+query IIII\n+select stats_min, stats_max, stats_min_value, stats_max_value from parquet_metadata('data/parquet-testing/decimal_stats.parquet');\n+----\n+-999.9\t999.9\t-999.9\t999.9\n+-999999.999\t999999.999\t-999999.999\t999999.999\n+-9999999999999.99999\t9999999999999.99999\t-9999999999999.99999\t9999999999999.99999\n+-999999999999999999999999999999999.99999\t999999999999999999999999999999999.99999\t-999999999999999999999999999999999.99999\t999999999999999999999999999999999.99999\n+\n+query IIII\n+select * from 'data/parquet-testing/decimal_stats.parquet';\n+----\n+-999.9\t-999999.999\t-9999999999999.99999\t-999999999999999999999999999999999.99999\n+999.9\t999999.999\t9999999999999.99999\t999999999999999999999999999999999.99999\n+\n+# int32 decimal stats\n+query IIII\n+select stats_min, stats_max, stats_min_value, stats_max_value from parquet_metadata('data/parquet-testing/arrow/int32_decimal.parquet');\n+----\n+1.00\t24.00\tNULL\tNULL\n+\n+query I\n+SELECT * FROM 'data/parquet-testing/arrow/int32_decimal.parquet'\n+----\n+1.00\n+2.00\n+3.00\n+4.00\n+5.00\n+6.00\n+7.00\n+8.00\n+9.00\n+10.00\n+11.00\n+12.00\n+13.00\n+14.00\n+15.00\n+16.00\n+17.00\n+18.00\n+19.00\n+20.00\n+21.00\n+22.00\n+23.00\n+24.00\n+\n+# int64 decimal stats\n+query IIII\n+select stats_min, stats_max, stats_min_value, stats_max_value from parquet_metadata('data/parquet-testing/arrow/int64_decimal.parquet');\n+----\n+1.00\t24.00\tNULL\tNULL\n+\n+query I\n+SELECT * FROM 'data/parquet-testing/arrow/int64_decimal.parquet'\n+----\n+1.00\n+2.00\n+3.00\n+4.00\n+5.00\n+6.00\n+7.00\n+8.00\n+9.00\n+10.00\n+11.00\n+12.00\n+13.00\n+14.00\n+15.00\n+16.00\n+17.00\n+18.00\n+19.00\n+20.00\n+21.00\n+22.00\n+23.00\n+24.00\n+\n+# data-types stats\n+query IIII\n+SELECT stats_min, stats_max, stats_min_value, stats_max_value FROM parquet_metadata('data/parquet-testing/data-types.parquet')\n+----\n+-127\t127\t-127\t127\n+-32767\t32767\t-32767\t32767\n+-2147483647\t2147483647\t-2147483647\t2147483647\n+-9223372036854775807\t9223372036854775807\t-9223372036854775807\t9223372036854775807\n+-4.6\t4.6\t-4.6\t4.6\n+-4.7\t4.7\t-4.7\t4.7\n+4.80\t4.80\t4.80\t4.80\n+49\t49\t49\t49\n+50\t50\t50\t50\n+false\ttrue\tfalse\ttrue\n+2019-11-26 20:11:42.501\t2019-11-26 20:11:42.501\t2019-11-26 20:11:42.501\t2019-11-26 20:11:42.501\n+2020-01-10\t2020-01-10\t2020-01-10\t2020-01-10\n+\n+query IIIIIIIIIIII\n+SELECT * FROM 'data/parquet-testing/data-types.parquet'\n+----\n+NULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\n+42\t43\t44\t45\t4.600000\t4.700000\t4.80\t49\t50\tTrue\t2019-11-26 20:11:42.501\t2020-01-10\n+-127\t-32767\t-2147483647\t-9223372036854775807\t-4.600000\t-4.700000\tNULL\tNULL\tNULL\tFalse\tNULL\tNULL\n+127\t32767\t2147483647\t9223372036854775807\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\n+NULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\n+\n+# parquet stats for all parquet files\n+foreach parquet_file data/parquet-testing/manyrowgroups.parquet data/parquet-testing/map.parquet data/parquet-testing/arrow/int32_decimal.parquet data/parquet-testing/arrow/nonnullable.impala.parquet data/parquet-testing/bug687_nulls.parquet data/parquet-testing/bug1554.parquet data/parquet-testing/apkwan.parquet data/parquet-testing/arrow/nested_lists.snappy.parquet data/parquet-testing/arrow/nulls.snappy.parquet data/parquet-testing/nan-float.parquet data/parquet-testing/manyrowgroups2.parquet data/parquet-testing/struct.parquet data/parquet-testing/arrow/list_columns.parquet data/parquet-testing/timestamp-ms.parquet data/parquet-testing/arrow/alltypes_dictionary.parquet data/parquet-testing/arrow/binary.parquet data/parquet-testing/arrow/nation.dict-malformed.parquet data/parquet-testing/lineitem-top10000.gzip.parquet data/parquet-testing/arrow/nested_maps.snappy.parquet data/parquet-testing/arrow/dict-page-offset-zero.parquet data/parquet-testing/silly-names.parquet data/parquet-testing/zstd.parquet data/parquet-testing/bug1618_struct_strings.parquet data/parquet-testing/arrow/single_nan.parquet data/parquet-testing/arrow/int64_decimal.parquet data/parquet-testing/filter_bug1391.parquet data/parquet-testing/arrow/fixed_length_decimal_legacy.parquet data/parquet-testing/timestamp.parquet data/parquet-testing/arrow/fixed_length_decimal.parquet data/parquet-testing/leftdate3_192_loop_1.parquet data/parquet-testing/blob.parquet data/parquet-testing/bug1588.parquet data/parquet-testing/bug1589.parquet data/parquet-testing/arrow/alltypes_plain.parquet data/parquet-testing/arrow/repeated_no_annotation.parquet data/parquet-testing/data-types.parquet data/parquet-testing/unsigned.parquet data/parquet-testing/pandas-date.parquet data/parquet-testing/date.parquet data/parquet-testing/arrow/nullable.impala.parquet data/parquet-testing/fixed.parquet data/parquet-testing/arrow/alltypes_plain.snappy.parquet data/parquet-testing/decimal/int32_decimal.parquet data/parquet-testing/decimal/pandas_decimal.parquet data/parquet-testing/decimal/decimal_dc.parquet data/parquet-testing/decimal/int64_decimal.parquet data/parquet-testing/decimal/fixed_length_decimal_legacy.parquet data/parquet-testing/decimal/fixed_length_decimal.parquet data/parquet-testing/glob2/t1.parquet data/parquet-testing/cache/cache1.parquet data/parquet-testing/cache/cache2.parquet data/parquet-testing/glob/t2.parquet data/parquet-testing/glob/t1.parquet data/parquet-testing/bug2557.parquet\n+\n+statement ok\n+select * from parquet_metadata('${parquet_file}');\n+\n+endloop\ndiff --git a/test/sql/copy/parquet/writer/parquet_test_all_types.test b/test/sql/copy/parquet/writer/parquet_test_all_types.test\nnew file mode 100644\nindex 000000000000..38b740a8ed97\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/parquet_test_all_types.test\n@@ -0,0 +1,36 @@\n+# name: test/sql/copy/parquet/writer/parquet_test_all_types.test\n+# description: Parquet test_all_types function\n+# group: [writer]\n+\n+require parquet\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE all_types AS\n+SELECT * REPLACE (\n+\tcase when extract(month from interval) <> 0 then interval '1 month 1 day 12:13:34.123' else interval end AS interval\n+)\n+FROM test_all_types();\n+\n+statement ok\n+COPY all_types TO '__TEST_DIR__/all_types.parquet' (FORMAT PARQUET);\n+\n+# we have to make some replacements to get result equivalence\n+# hugeint is stored as double -> we have to cast\n+# datetz/timetz/timestamptz lose their tz qualifier -> cast to the non-tz type\n+# intervals are saved with ms precision -> truncate microsecond precision to milisecond\n+query I nosort alltypes\n+SELECT * REPLACE (\n+\thugeint::DOUBLE AS hugeint,\n+\tdate_tz::DATE AS date_tz,\n+\ttime_tz::TIME AS time_tz,\n+\ttimestamp_tz::TIMESTAMP AS timestamp_tz\n+)\n+FROM all_types\n+----\n+\n+query I nosort alltypes\n+SELECT * FROM '__TEST_DIR__/all_types.parquet'\n+----\ndiff --git a/test/sql/copy/parquet/writer/parquet_write_decimals.test b/test/sql/copy/parquet/writer/parquet_write_decimals.test\nnew file mode 100644\nindex 000000000000..b78f3a9918b3\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/parquet_write_decimals.test\n@@ -0,0 +1,87 @@\n+# name: test/sql/copy/parquet/writer/parquet_write_decimals.test\n+# description: Parquet decimal types round trip\n+# group: [writer]\n+\n+require parquet\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE decimals(\n+\tdec4 DECIMAL(4,1),\n+\tdec9 DECIMAL(9,2),\n+\tdec18 DECIMAL(18,3),\n+\tdec38 DECIMAL(38,4)\n+);\n+\n+statement ok\n+INSERT INTO decimals VALUES (\n+\t-999.9,\n+\t-9999999.99,\n+\t-999999999999999.999,\n+\t-999999999999999999999999999999999.9999\n+), (\n+\tNULL, NULL, NULL, NULL\n+), (\n+ \t42, 42, 42, 42\n+), (\n+ \t-42, -42, -42, -42\n+), (\n+  \t0, 0, 0, 0\n+ ), (\n+  \t999.9,\n+  \t9999999.99,\n+  \t999999999999999.999,\n+  \t999999999999999999999999999999999.9999\n+);\n+\n+statement ok\n+COPY decimals TO '__TEST_DIR__/decimals.parquet';\n+\n+query IIII nosort decimal_scan\n+SELECT * FROM decimals;\n+\n+query IIII nosort decimal_scan\n+SELECT * FROM '__TEST_DIR__/decimals.parquet';\n+\n+query IIII\n+SELECT stats_min, stats_max, stats_min_value, stats_max_value FROM parquet_metadata('__TEST_DIR__/decimals.parquet');\n+----\n+-999.9\t999.9\t-999.9\t999.9\n+-9999999.99\t9999999.99\t-9999999.99\t9999999.99\n+-999999999999999.999\t999999999999999.999\t-999999999999999.999\t999999999999999.999\n+-999999999999999999999999999999999.9999\t999999999999999999999999999999999.9999\t-999999999999999999999999999999999.9999\t999999999999999999999999999999999.9999\n+\n+# filter pushdown\n+statement ok\n+DELETE FROM decimals WHERE dec4<-42 OR dec4>42\n+\n+statement ok\n+COPY decimals TO '__TEST_DIR__/decimals.parquet';\n+\n+foreach dec_column dec4 dec9 dec18 dec38\n+\n+query IIII\n+SELECT * FROM '__TEST_DIR__/decimals.parquet' WHERE ${dec_column}=42\n+----\n+42\t42\t42\t42\n+\n+query IIII\n+SELECT * FROM '__TEST_DIR__/decimals.parquet' WHERE ${dec_column}=-43\n+----\n+\n+query IIII\n+SELECT * FROM '__TEST_DIR__/decimals.parquet' WHERE ${dec_column}=43\n+----\n+\n+endloop\n+\n+# check statistics\n+statement ok\n+PRAGMA disable_verification\n+\n+query IIII\n+SELECT stats(dec4), stats(dec9), stats(dec18), stats(dec38) FROM '__TEST_DIR__/decimals.parquet' LIMIT 1\n+----\n+[Min: -42.0, Max: 42.0][Has Null: true]\t[Min: -42.00, Max: 42.00][Has Null: true]\t[Min: -42.000, Max: 42.000][Has Null: true]\t[Min: -42.0000, Max: 42.0000][Has Null: true]\ndiff --git a/test/sql/copy/parquet/writer/parquet_write_enums.test b/test/sql/copy/parquet/writer/parquet_write_enums.test\nnew file mode 100644\nindex 000000000000..8374f92be571\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/parquet_write_enums.test\n@@ -0,0 +1,141 @@\n+# name: test/sql/copy/parquet/writer/parquet_write_enums.test\n+# description: ENUM tests\n+# group: [writer]\n+\n+require parquet\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+# standard enum\n+statement ok\n+CREATE TYPE mood AS ENUM ('joy', 'ok', 'happy');\n+\n+statement ok\n+CREATE TABLE enums(m mood);\n+\n+statement ok\n+INSERT INTO enums VALUES\n+\t('happy'), ('happy'), ('joy'), ('joy'),\n+\t('happy'), ('happy'), ('joy'), ('joy'),\n+    ('happy'), ('happy'), ('joy'), ('joy'),\n+    ('happy'), ('happy'), ('joy'), ('joy'),\n+    ('happy'), ('happy'), ('joy'), ('joy'),\n+    ('happy'), ('happy'), ('joy'), ('joy'),\n+    ('happy'), ('happy'), ('joy'), ('joy'), ('joy')\n+\n+statement ok\n+COPY enums TO '__TEST_DIR__/enums.parquet' (FORMAT PARQUET);\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/enums.parquet'\n+----\n+happy\n+happy\n+joy\n+joy\n+happy\n+happy\n+joy\n+joy\n+happy\n+happy\n+joy\n+joy\n+happy\n+happy\n+joy\n+joy\n+happy\n+happy\n+joy\n+joy\n+happy\n+happy\n+joy\n+joy\n+happy\n+happy\n+joy\n+joy\n+joy\n+\n+# enum with null values\n+statement ok\n+UPDATE enums SET m=NULL WHERE m='joy'\n+\n+statement ok\n+COPY enums TO '__TEST_DIR__/enums.parquet' (FORMAT PARQUET);\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/enums.parquet'\n+----\n+happy\n+happy\n+NULL\n+NULL\n+happy\n+happy\n+NULL\n+NULL\n+happy\n+happy\n+NULL\n+NULL\n+happy\n+happy\n+NULL\n+NULL\n+happy\n+happy\n+NULL\n+NULL\n+happy\n+happy\n+NULL\n+NULL\n+happy\n+happy\n+NULL\n+NULL\n+NULL\n+\n+# all values are null\n+statement ok\n+UPDATE enums SET m=NULL\n+\n+statement ok\n+COPY enums TO '__TEST_DIR__/enums.parquet' (FORMAT PARQUET);\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/enums.parquet'\n+----\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\ndiff --git a/test/sql/copy/parquet/writer/parquet_write_interval.test b/test/sql/copy/parquet/writer/parquet_write_interval.test\nnew file mode 100644\nindex 000000000000..85d7b171ce9e\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/parquet_write_interval.test\n@@ -0,0 +1,34 @@\n+# name: test/sql/copy/parquet/writer/parquet_write_interval.test\n+# description: Parquet interval round trip\n+# group: [writer]\n+\n+require parquet\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE IF NOT EXISTS intervals (i interval);\n+\n+statement ok\n+INSERT INTO intervals VALUES\n+       (interval '1' day),\n+       (interval '00:00:01'),\n+       (NULL),\n+       (interval '0' month),\n+       (interval '1' month)\n+\n+statement ok\n+COPY intervals TO '__TEST_DIR__/intervals.parquet'\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/intervals.parquet' ORDER BY 1\n+----\n+NULL\n+00:00:00\n+00:00:01\n+1 day\n+1 month\n+\n+statement error\n+COPY (SELECT -interval '1 day') TO '__TEST_DIR__/intervals.parquet'\ndiff --git a/test/sql/copy/parquet/writer/parquet_write_tpcds.test_slow b/test/sql/copy/parquet/writer/parquet_write_tpcds.test_slow\nnew file mode 100644\nindex 000000000000..4d1a70496f58\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/parquet_write_tpcds.test_slow\n@@ -0,0 +1,67 @@\n+# name: test/sql/copy/parquet/writer/parquet_write_tpcds.test_slow\n+# description: Parquet TPC-DS tests\n+# group: [writer]\n+\n+require parquet\n+\n+require tpcds\n+\n+# answers are generated from postgres\n+# hence check with NULLS LAST flag\n+statement ok\n+PRAGMA default_null_order='NULLS LAST'\n+\n+statement ok\n+CREATE SCHEMA tpcds;\n+\n+statement ok\n+CALL dsdgen(sf=1, schema='tpcds');\n+\n+foreach tbl call_center catalog_page catalog_returns catalog_sales customer customer_demographics customer_address date_dim household_demographics inventory income_band item promotion reason ship_mode store store_returns store_sales time_dim warehouse web_page web_returns web_sales web_site\n+\n+statement ok\n+COPY tpcds.${tbl} TO '__TEST_DIR__/${tbl}.parquet' (FORMAT 'PARQUET', COMPRESSION 'ZSTD');\n+\n+statement ok\n+CREATE VIEW ${tbl} AS SELECT * FROM parquet_scan('__TEST_DIR__/${tbl}.parquet');\n+\n+endloop\n+\n+# too slow queries:\n+# 64, 85\n+\n+loop i 1 9\n+\n+query I\n+PRAGMA tpcds(${i})\n+----\n+<FILE>:extension/tpcds/dsdgen/answers/sf1/0${i}.csv\n+\n+endloop\n+\n+loop i 10 64\n+\n+query I\n+PRAGMA tpcds(${i})\n+----\n+<FILE>:extension/tpcds/dsdgen/answers/sf1/${i}.csv\n+\n+endloop\n+\n+loop i 65 85\n+\n+query I\n+PRAGMA tpcds(${i})\n+----\n+<FILE>:extension/tpcds/dsdgen/answers/sf1/${i}.csv\n+\n+endloop\n+\n+loop i 86 99\n+\n+query I\n+PRAGMA tpcds(${i})\n+----\n+<FILE>:extension/tpcds/dsdgen/answers/sf1/${i}.csv\n+\n+endloop\ndiff --git a/test/sql/copy/parquet/writer/parquet_write_tpch.test_slow b/test/sql/copy/parquet/writer/parquet_write_tpch.test_slow\nindex 3fdbd92e8a26..02b69dc96d82 100644\n--- a/test/sql/copy/parquet/writer/parquet_write_tpch.test_slow\n+++ b/test/sql/copy/parquet/writer/parquet_write_tpch.test_slow\n@@ -31,18 +31,7 @@ PRAGMA tpch(${i})\n \n endloop\n \n-# skip q15 for now: it is non-deterministic with multi-threading and doubles\n-# this can be re-enabled once we write decimals to parquet\n-loop i 10 15\n-\n-query I\n-PRAGMA tpch(${i})\n-----\n-<FILE>:extension/tpch/dbgen/answers/sf1/q${i}.csv\n-\n-endloop\n-\n-loop i 16 23\n+loop i 10 23\n \n query I\n PRAGMA tpch(${i})\ndiff --git a/test/sql/copy/parquet/writer/parquet_write_uuid.test b/test/sql/copy/parquet/writer/parquet_write_uuid.test\nnew file mode 100644\nindex 000000000000..88efd0a12ea2\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/parquet_write_uuid.test\n@@ -0,0 +1,63 @@\n+# name: test/sql/copy/parquet/writer/parquet_write_uuid.test\n+# description: Parquet UUID round trip\n+# group: [writer]\n+\n+require parquet\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE IF NOT EXISTS uuid (u uuid);\n+\n+statement ok\n+INSERT INTO uuid VALUES\n+       ('A0EEBC99-9C0B-4EF8-BB6D-6BB9BD380A11'),\n+       (NULL),\n+       ('47183823-2574-4bfd-b411-99ed177d3e43'),\n+       ('{10203040506070800102030405060708}'),\n+       ('A0EEBC99-9C0B-4EF8-BB6D-6BB9BD380A11'),\n+       (NULL),\n+       ('00112233-4455-6677-8899-aabbccddeeff'),\n+       ('47183823-2574-4bfd-b411-99ed177d3e43'),\n+       ('{10203040506070800102030405060708}'),\n+       ('00000000-0000-0000-0000-000000000000'),\n+       ('00000000-0000-0000-0000-000000000001'),\n+       ('00000000-0000-0000-8000-000000000001'),\n+       ('80000000-0000-0000-0000-000000000000'),\n+       ('80000000-0000-0000-8000-000000000000'),\n+       ('80000000-0000-0000-8fff-ffffffffffff'),\n+       ('80000000-0000-0000-ffff-ffffffffffff'),\n+       ('8fffffff-ffff-ffff-0000-000000000000'),\n+       ('8fffffff-ffff-ffff-8000-000000000000'),\n+       ('8fffffff-ffff-ffff-8fff-ffffffffffff'),\n+       ('8fffffff-ffff-ffff-ffff-ffffffffffff'),\n+       ('ffffffff-ffff-ffff-ffff-ffffffffffff');\n+\n+statement ok\n+COPY uuid TO '__TEST_DIR__/uuid.parquet'\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/uuid.parquet' ORDER BY 1\n+----\n+NULL\n+NULL\n+00000000-0000-0000-0000-000000000000\n+00000000-0000-0000-0000-000000000001\n+00000000-0000-0000-8000-000000000001\n+00112233-4455-6677-8899-aabbccddeeff\n+10203040-5060-7080-0102-030405060708\n+10203040-5060-7080-0102-030405060708\n+47183823-2574-4bfd-b411-99ed177d3e43\n+47183823-2574-4bfd-b411-99ed177d3e43\n+80000000-0000-0000-0000-000000000000\n+80000000-0000-0000-8000-000000000000\n+80000000-0000-0000-8fff-ffffffffffff\n+80000000-0000-0000-ffff-ffffffffffff\n+8fffffff-ffff-ffff-0000-000000000000\n+8fffffff-ffff-ffff-8000-000000000000\n+8fffffff-ffff-ffff-8fff-ffffffffffff\n+8fffffff-ffff-ffff-ffff-ffffffffffff\n+a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11\n+a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11\n+ffffffff-ffff-ffff-ffff-ffffffffffff\ndiff --git a/test/sql/copy/parquet/writer/write_stats_big_string.test b/test/sql/copy/parquet/writer/write_stats_big_string.test\nnew file mode 100644\nindex 000000000000..9916c24e56aa\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/write_stats_big_string.test\n@@ -0,0 +1,35 @@\n+# name: test/sql/copy/parquet/writer/write_stats_big_string.test\n+# description: We avoid writing min/max stats of large strings\n+# group: [writer]\n+\n+require parquet\n+\n+statement ok\n+PRAGMA enable_verification;\n+\n+statement ok\n+CREATE TABLE varchar(v VARCHAR);\n+\n+statement ok\n+INSERT INTO varchar VALUES (NULL), ('hello'), (NULL), ('world'), (NULL)\n+\n+# we write stats when there are only small strings\n+statement ok\n+COPY varchar TO '__TEST_DIR__/bigvarchar.parquet'\n+\n+query IIII\n+SELECT stats_min_value, stats_max_value, stats_min, stats_max FROM parquet_metadata('__TEST_DIR__/bigvarchar.parquet')\n+----\n+hello\tworld\thello\tworld\n+\n+# we stop writing stats when we encounter a very large string\n+statement ok\n+INSERT INTO varchar SELECT repeat('A', 100000) v\n+\n+statement ok\n+COPY varchar TO '__TEST_DIR__/bigvarchar.parquet'\n+\n+query IIII\n+SELECT stats_min_value, stats_max_value, stats_min, stats_max FROM parquet_metadata('__TEST_DIR__/bigvarchar.parquet')\n+----\n+NULL\tNULL\tNULL\tNULL\n\\ No newline at end of file\ndiff --git a/test/sql/copy/parquet/writer/write_stats_min_max.test b/test/sql/copy/parquet/writer/write_stats_min_max.test\nnew file mode 100644\nindex 000000000000..668ef65e4d5c\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/write_stats_min_max.test\n@@ -0,0 +1,226 @@\n+# name: test/sql/copy/parquet/writer/write_stats_min_max.test\n+# description: Write min/max stats to Parquet files\n+# group: [writer]\n+\n+require parquet\n+\n+statement ok\n+PRAGMA enable_verification;\n+\n+statement ok\n+PRAGMA explain_output = OPTIMIZED_ONLY;\n+\n+statement ok\n+CREATE TABLE boolean_limits AS SELECT (false)::BOOLEAN min, true::BOOLEAN max\n+\n+statement ok\n+CREATE TABLE tinyint_limits AS SELECT (-128)::TINYINT min, 127::TINYINT max\n+\n+statement ok\n+CREATE TABLE smallint_limits AS SELECT (-32768)::SMALLINT min, 32767::SMALLINT max\n+\n+statement ok\n+CREATE TABLE integer_limits AS SELECT (-2147483648)::INTEGER min, 2147483647::INTEGER max\n+\n+statement ok\n+CREATE TABLE bigint_limits AS SELECT (-9223372036854775808)::BIGINT min, 9223372036854775807::BIGINT max\n+\n+statement ok\n+CREATE TABLE float_limits AS SELECT (-0.5)::FLOAT min, 0.5::FLOAT max\n+\n+statement ok\n+CREATE TABLE double_limits AS SELECT (-0.5)::DOUBLE min, 0.5::DOUBLE max\n+\n+statement ok\n+CREATE TABLE varchar_limits AS SELECT 'hello world \ud83d\udc64\ud83c\udfe0\ud83d\udcd5' min, 'look at my ducks \ud83e\udd86\ud83e\udd86\ud83e\udd86' max;\n+\n+statement ok\n+CREATE TABLE blob_limits AS SELECT blob '\\x00hello\\x00world\\x00' min, blob '\\x00look\\x00at\\x00my\\x00nullbytes\\x00' max;\n+\n+statement ok\n+CREATE TABLE date_limits AS SELECT date '1900-01-01' min, date '2030-12-31' max;\n+\n+statement ok\n+CREATE TABLE time_limits AS SELECT time '00:00:00' min, time '23:59:59' max;\n+\n+statement ok\n+CREATE TABLE timestamp_limits AS SELECT timestamp '1900-01-01 00:00:00' min, timestamp '2030-12-31 23:59:59' max;\n+\n+statement ok\n+CREATE TABLE timestamp_s_limits AS SELECT '1900-01-01 00:00:00'::timestamp_s min, '2030-12-31 23:59:59'::timestamp_s max;\n+\n+statement ok\n+CREATE TABLE timestamp_ms_limits AS SELECT '1900-01-01 00:00:00'::timestamp_ms min, '2030-12-31 23:59:59'::timestamp_ms max;\n+\n+statement ok\n+CREATE TABLE timestamp_ns_limits AS SELECT '1900-01-01 00:00:00'::timestamp_ns min, '2030-12-31 23:59:59'::timestamp_ns max;\n+\n+# min/max/min_value/max_value for signed tables\n+foreach type date time timestamp timestamp_s timestamp_ms timestamp_ns varchar blob boolean tinyint smallint integer bigint float double\n+\n+statement ok\n+CREATE TABLE tbl(i ${type});\n+\n+# empty stats (all values are NULL)\n+statement ok\n+INSERT INTO tbl SELECT NULL\n+\n+statement ok\n+COPY tbl TO '__TEST_DIR__/${type}_stats.parquet.parquet' (FORMAT PARQUET);\n+\n+query IIII\n+SELECT stats_min_value::${type}, stats_max_value::${type}, stats_min::${type}, stats_max::${type} FROM parquet_metadata('__TEST_DIR__/${type}_stats.parquet.parquet')\n+----\n+NULL\tNULL\tNULL\tNULL\n+\n+# min/max stats\n+statement ok\n+INSERT INTO tbl SELECT min FROM ${type}_limits\n+\n+statement ok\n+INSERT INTO tbl SELECT max FROM ${type}_limits\n+\n+statement ok\n+COPY tbl TO '__TEST_DIR__/${type}_stats.parquet.parquet' (FORMAT PARQUET);\n+\n+query I\n+SELECT stats_min_value::${type} FROM parquet_metadata('__TEST_DIR__/${type}_stats.parquet.parquet') EXCEPT SELECT min FROM ${type}_limits\n+----\n+\n+query I\n+SELECT stats_max_value::${type} FROM parquet_metadata('__TEST_DIR__/${type}_stats.parquet.parquet') EXCEPT SELECT max FROM ${type}_limits\n+----\n+\n+query I\n+SELECT stats_min::${type} FROM parquet_metadata('__TEST_DIR__/${type}_stats.parquet.parquet') EXCEPT SELECT min FROM ${type}_limits\n+----\n+\n+query I\n+SELECT stats_max::${type} FROM parquet_metadata('__TEST_DIR__/${type}_stats.parquet.parquet') EXCEPT SELECT max FROM ${type}_limits\n+----\n+\n+statement ok\n+DROP TABLE tbl\n+\n+endloop\n+\n+statement ok\n+CREATE TABLE utinyint_limits AS SELECT (0)::UTINYINT min, 255::UTINYINT max\n+\n+statement ok\n+CREATE TABLE usmallint_limits AS SELECT (0)::USMALLINT min, 65535::USMALLINT max\n+\n+statement ok\n+CREATE TABLE uinteger_limits AS SELECT 0::UINTEGER min, 4294967295::UINTEGER max\n+\n+statement ok\n+CREATE TABLE ubigint_limits AS SELECT 0::UBIGINT min, 18446744073709551615::UBIGINT max\n+\n+# unsigned types only define min_value/max_value\n+foreach type utinyint usmallint uinteger ubigint\n+\n+statement ok\n+CREATE TABLE tbl(i ${type});\n+\n+# empty stats (all values are NULL)\n+statement ok\n+INSERT INTO tbl SELECT NULL\n+\n+statement ok\n+COPY tbl TO '__TEST_DIR__/${type}_stats.parquet.parquet' (FORMAT PARQUET);\n+\n+query I\n+SELECT stats_min_value::${type} FROM parquet_metadata('__TEST_DIR__/${type}_stats.parquet.parquet')\n+----\n+NULL\n+\n+query I\n+SELECT stats_max_value::${type} FROM parquet_metadata('__TEST_DIR__/${type}_stats.parquet.parquet')\n+----\n+NULL\n+\n+query I\n+SELECT stats_min::${type} FROM parquet_metadata('__TEST_DIR__/${type}_stats.parquet.parquet')\n+----\n+NULL\n+\n+query I\n+SELECT stats_max::${type} FROM parquet_metadata('__TEST_DIR__/${type}_stats.parquet.parquet')\n+----\n+NULL\n+\n+# min/max stats\n+statement ok\n+INSERT INTO tbl SELECT min FROM ${type}_limits\n+\n+statement ok\n+INSERT INTO tbl SELECT max FROM ${type}_limits\n+\n+statement ok\n+COPY tbl TO '__TEST_DIR__/${type}_stats.parquet.parquet' (FORMAT PARQUET);\n+\n+query I\n+SELECT stats_min_value::${type} FROM parquet_metadata('__TEST_DIR__/${type}_stats.parquet.parquet') EXCEPT SELECT min FROM ${type}_limits\n+----\n+\n+query I\n+SELECT stats_max_value::${type} FROM parquet_metadata('__TEST_DIR__/${type}_stats.parquet.parquet') EXCEPT SELECT max FROM ${type}_limits\n+----\n+\n+query I\n+SELECT stats_min::${type} FROM parquet_metadata('__TEST_DIR__/${type}_stats.parquet.parquet')\n+----\n+NULL\n+\n+query I\n+SELECT stats_max::${type} FROM parquet_metadata('__TEST_DIR__/${type}_stats.parquet.parquet')\n+----\n+NULL\n+\n+statement ok\n+DROP TABLE tbl\n+\n+endloop\n+\n+# no stats for these types\n+statement ok\n+CREATE TABLE hugeint_limits AS SELECT (-170141183460469231731687303715884105727)::HUGEINT min, 170141183460469231731687303715884105727::HUGEINT max\n+\n+foreach type hugeint\n+\n+statement ok\n+CREATE TABLE tbl(i ${type});\n+\n+statement ok\n+INSERT INTO tbl SELECT min FROM ${type}_limits\n+\n+statement ok\n+INSERT INTO tbl SELECT max FROM ${type}_limits\n+\n+statement ok\n+COPY tbl TO '__TEST_DIR__/${type}_stats.parquet.parquet' (FORMAT PARQUET);\n+\n+query I\n+SELECT stats_min_value::${type} FROM parquet_metadata('__TEST_DIR__/${type}_stats.parquet.parquet')\n+----\n+NULL\n+\n+query I\n+SELECT stats_max_value::${type} FROM parquet_metadata('__TEST_DIR__/${type}_stats.parquet.parquet')\n+----\n+NULL\n+\n+query I\n+SELECT stats_min::${type} FROM parquet_metadata('__TEST_DIR__/${type}_stats.parquet.parquet')\n+----\n+NULL\n+\n+query I\n+SELECT stats_max::${type} FROM parquet_metadata('__TEST_DIR__/${type}_stats.parquet.parquet')\n+----\n+NULL\n+\n+statement ok\n+DROP TABLE tbl\n+\n+endloop\n\\ No newline at end of file\ndiff --git a/test/sql/copy/parquet/writer/write_stats_null_count.test b/test/sql/copy/parquet/writer/write_stats_null_count.test\nnew file mode 100644\nindex 000000000000..2c3006fb536c\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/write_stats_null_count.test\n@@ -0,0 +1,82 @@\n+# name: test/sql/copy/parquet/writer/write_stats_null_count.test\n+# description: Write null_count stats to Parquet files\n+# group: [writer]\n+\n+require parquet\n+\n+statement ok\n+PRAGMA enable_verification;\n+\n+statement ok\n+PRAGMA explain_output = OPTIMIZED_ONLY;\n+\n+# null count\n+statement ok\n+COPY (SELECT 42 i) TO '__TEST_DIR__/stats.parquet' (FORMAT PARQUET);\n+\n+query I\n+SELECT stats_null_count FROM parquet_metadata('__TEST_DIR__/stats.parquet')\n+----\n+0\n+\n+# we can filter out the IS NULL clause\n+query II\n+EXPLAIN SELECT COUNT(*) FROM '__TEST_DIR__/stats.parquet' WHERE i IS NULL\n+----\n+logical_opt\t<!REGEX>:.*IS_NULL.*\n+\n+query I\n+SELECT COUNT(*) FROM '__TEST_DIR__/stats.parquet' WHERE i IS NULL\n+----\n+0\n+\n+statement ok\n+COPY (SELECT NULL i) TO '__TEST_DIR__/stats.parquet' (FORMAT PARQUET);\n+\n+query I\n+SELECT stats_null_count FROM parquet_metadata('__TEST_DIR__/stats.parquet')\n+----\n+1\n+\n+# we cannot filter out the IS NULL clause\n+query II\n+EXPLAIN SELECT COUNT(*) FROM '__TEST_DIR__/stats.parquet' WHERE i IS NULL\n+----\n+logical_opt\t<REGEX>:.*IS_NULL.*\n+\n+query I\n+SELECT COUNT(*) FROM '__TEST_DIR__/stats.parquet' WHERE i IS NULL\n+----\n+1\n+\n+# list null count not supported (i.e. we don't write the null count in this case)\n+statement ok\n+COPY (SELECT [42, NULL, 43] i) TO '__TEST_DIR__/stats.parquet' (FORMAT PARQUET);\n+\n+query I\n+SELECT stats_null_count FROM parquet_metadata('__TEST_DIR__/stats.parquet')\n+----\n+NULL\n+\n+statement ok\n+COPY (SELECT {'a': NULL, 'b': 42} i) TO '__TEST_DIR__/stats.parquet' (FORMAT PARQUET);\n+\n+query I\n+SELECT stats_null_count FROM parquet_metadata('__TEST_DIR__/stats.parquet')\n+----\n+1\n+0\n+\n+# struct null count is propagated to the children\n+# i.e. if a struct itself is null, this counts as NULL for the children\n+statement ok\n+CREATE TABLE structs AS SELECT {'a': NULL, 'b': 'hello'} i UNION ALL SELECT NULL UNION ALL SELECT {'a': 84, 'b': 'world'};\n+\n+statement ok\n+COPY structs TO 'stats.parquet' (FORMAT PARQUET);\n+\n+query I\n+SELECT stats_null_count FROM parquet_metadata('stats.parquet')\n+----\n+2\n+1\ndiff --git a/test/sql/function/uuid/test_uuid.test b/test/sql/function/uuid/test_uuid.test\nindex 38775e7fa7ce..f40820aff92b 100644\n--- a/test/sql/function/uuid/test_uuid.test\n+++ b/test/sql/function/uuid/test_uuid.test\n@@ -9,7 +9,7 @@ statement ok\n CREATE TEMPORARY TABLE t1 AS SELECT gen_random_uuid() a FROM range(0, 16);\n \n statement ok\n-CREATE TEMPORARY TABLE t2 AS SELECT gen_random_uuid() b FROM range(0, 16);\n+CREATE TEMPORARY TABLE t2 AS SELECT uuid() b FROM range(0, 16);\n \n statement ok\n CREATE TEMPORARY TABLE t3 AS SELECT gen_random_uuid() c FROM range(0, 16);\n",
  "problem_statement": "Not writing parquet statistics when creating parquet files\n#### What happens?\r\n\r\nWhen creating a parquet file with DuckDB the metadata for each column chunk is not included.\r\n\r\nThere are no statistics written in the Parquet file especially when viewed with parquet_metadata().\r\n\r\nPlease add the ability to write these statistics.\r\n\r\n#### To Reproduce\r\n\r\nCreate a parquet file from a table and inspect the metadata using parquet_metadata(), you will see that stats_min and stats_max are null for all columns.\r\n\r\n#### Environment (please complete the following information):\r\n - OS: MacOSX\r\n - DuckDB Version: HEAD\r\n - DuckDB Client: Python\r\n\r\n#### Before Submitting\r\n\r\n- [x] **Have you tried this on the latest `master` branch?** yes\r\n\r\n- [x] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**\r\n\n",
  "hints_text": "",
  "created_at": "2021-12-28T14:25:00Z"
}