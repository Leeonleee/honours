{
  "repo": "duckdb/duckdb",
  "pull_number": 3767,
  "instance_id": "duckdb__duckdb-3767",
  "issue_numbers": [
    "3748",
    "3748"
  ],
  "base_commit": "463e45ba98754b0e2fdc8bc5b1481dfbc7a7398d",
  "patch": "diff --git a/.github/workflows/Regression.yml b/.github/workflows/Regression.yml\nindex de4d3e2cbf67..45cb8d5cff37 100644\n--- a/.github/workflows/Regression.yml\n+++ b/.github/workflows/Regression.yml\n@@ -77,3 +77,56 @@ jobs:\n       run: |\n         cp -r benchmark duckdb/\n         python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/${{ matrix.benchmark }}.csv --verbose --threads=2\n+\n+ regression-test-python:\n+  name: Regression Test (Python Client)\n+  runs-on: ubuntu-20.04\n+  env:\n+    CC: gcc-10\n+    CXX: g++-10\n+    GEN: ninja\n+\n+  steps:\n+    - uses: actions/checkout@v3\n+      with:\n+        fetch-depth: 0\n+\n+    - uses: actions/setup-python@v2\n+      with:\n+        python-version: '3.7'\n+\n+    - name: Install\n+      run: |\n+        sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build\n+        pip install numpy pytest pandas mypy psutil pyarrow\n+\n+    - name: Build Current Version\n+      run: |\n+        cd tools/pythonpkg\n+        python setup.py install --user\n+        cd ../..\n+\n+    - name: Run New Version\n+      run: |\n+        python scripts/regression_test_python.py --threads=2 --out-file=new.csv\n+\n+    - name: Cleanup New Version\n+      run: |\n+        cd tools/pythonpkg\n+        ./clean.sh\n+        cd ../..\n+\n+    - name: Build Current\n+      run: |\n+        git clone https://github.com/duckdb/duckdb.git --depth=1\n+        cd duckdb/tools/pythonpkg\n+        python setup.py install --user\n+        cd ../../..\n+\n+    - name: Run Current Version\n+      run: |\n+        python scripts/regression_test_python.py --threads=2 --out-file=current.csv\n+\n+    - name: Regression Test\n+      run: |\n+        python scripts/regression_check.py --old=current.csv --new=new.csv\ndiff --git a/benchmark/benchmark_runner.cpp b/benchmark/benchmark_runner.cpp\nindex 15bc48caad0a..4967ce294ba1 100644\n--- a/benchmark/benchmark_runner.cpp\n+++ b/benchmark/benchmark_runner.cpp\n@@ -180,7 +180,7 @@ void BenchmarkRunner::RunBenchmark(Benchmark *benchmark) {\n \n void BenchmarkRunner::RunBenchmarks() {\n \tLogLine(\"Starting benchmark run.\");\n-\tLogLine(\"name\\trun\\tnruns\\ttiming\");\n+\tLogLine(\"name\\trun\\ttiming\");\n \tfor (auto &benchmark : benchmarks) {\n \t\tRunBenchmark(benchmark);\n \t}\ndiff --git a/extension/icu/icu-extension.cpp b/extension/icu/icu-extension.cpp\nindex 72176c4cf094..ac4366d3832f 100644\n--- a/extension/icu/icu-extension.cpp\n+++ b/extension/icu/icu-extension.cpp\n@@ -150,7 +150,7 @@ static void SetICUTimeZone(ClientContext &context, SetScope scope, Value &parame\n \t}\n }\n \n-struct ICUTimeZoneData : public FunctionOperatorData {\n+struct ICUTimeZoneData : public GlobalTableFunctionState {\n \tICUTimeZoneData() : tzs(icu::TimeZone::createEnumeration()) {\n \t\tUErrorCode status = U_ZERO_ERROR;\n \t\tstd::unique_ptr<icu::Calendar> calendar(icu::Calendar::createInstance(status));\n@@ -175,21 +175,12 @@ static unique_ptr<FunctionData> ICUTimeZoneBind(ClientContext &context, TableFun\n \treturn nullptr;\n }\n \n-static unique_ptr<FunctionOperatorData> ICUTimeZoneInit(ClientContext &context, const FunctionData *bind_data,\n-                                                        const vector<column_t> &column_ids,\n-                                                        TableFilterCollection *filters) {\n+static unique_ptr<GlobalTableFunctionState> ICUTimeZoneInit(ClientContext &context, TableFunctionInitInput &input) {\n \treturn make_unique<ICUTimeZoneData>();\n }\n \n-static void ICUTimeZoneCleanup(ClientContext &context, const FunctionData *bind_data,\n-                               FunctionOperatorData *operator_state) {\n-\tauto &data = (ICUTimeZoneData &)*operator_state;\n-\t(void)data.tzs.reset();\n-}\n-\n-static void ICUTimeZoneFunction(ClientContext &context, const FunctionData *bind_data,\n-                                FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &data = (ICUTimeZoneData &)*operator_state;\n+static void ICUTimeZoneFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (ICUTimeZoneData &)*data_p.global_state;\n \tidx_t index = 0;\n \twhile (index < STANDARD_VECTOR_SIZE) {\n \t\tUErrorCode status = U_ZERO_ERROR;\n@@ -234,7 +225,7 @@ static void ICUTimeZoneFunction(ClientContext &context, const FunctionData *bind\n \toutput.SetCardinality(index);\n }\n \n-struct ICUCalendarData : public FunctionOperatorData {\n+struct ICUCalendarData : public GlobalTableFunctionState {\n \tICUCalendarData() {\n \t\t// All calendars are available in all locales\n \t\tUErrorCode status = U_ZERO_ERROR;\n@@ -252,15 +243,12 @@ static unique_ptr<FunctionData> ICUCalendarBind(ClientContext &context, TableFun\n \treturn nullptr;\n }\n \n-static unique_ptr<FunctionOperatorData> ICUCalendarInit(ClientContext &context, const FunctionData *bind_data,\n-                                                        const vector<column_t> &column_ids,\n-                                                        TableFilterCollection *filters) {\n+static unique_ptr<GlobalTableFunctionState> ICUCalendarInit(ClientContext &context, TableFunctionInitInput &input) {\n \treturn make_unique<ICUCalendarData>();\n }\n \n-static void ICUCalendarFunction(ClientContext &context, const FunctionData *bind_data,\n-                                FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &data = (ICUCalendarData &)*operator_state;\n+static void ICUCalendarFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (ICUCalendarData &)*data_p.global_state;\n \tidx_t index = 0;\n \twhile (index < STANDARD_VECTOR_SIZE) {\n \t\tif (!data.calendars) {\n@@ -283,12 +271,6 @@ static void ICUCalendarFunction(ClientContext &context, const FunctionData *bind\n \toutput.SetCardinality(index);\n }\n \n-static void ICUCalendarCleanup(ClientContext &context, const FunctionData *bind_data,\n-                               FunctionOperatorData *operator_state) {\n-\tauto &data = (ICUCalendarData &)*operator_state;\n-\t(void)data.calendars.reset();\n-}\n-\n static void SetICUCalendar(ClientContext &context, SetScope scope, Value &parameter) {\n \tconst auto name = parameter.Value::GetValueUnsafe<string>();\n \tstring locale_key = \"@calendar=\" + name;\n@@ -340,8 +322,7 @@ void ICUExtension::Load(DuckDB &db) {\n \ttz->getID(tz_id).toUTF8String(tz_string);\n \tconfig.set_variables[\"TimeZone\"] = Value(tz_string);\n \n-\tTableFunction tz_names(\"pg_timezone_names\", {}, ICUTimeZoneFunction, ICUTimeZoneBind, ICUTimeZoneInit, nullptr,\n-\t                       ICUTimeZoneCleanup);\n+\tTableFunction tz_names(\"pg_timezone_names\", {}, ICUTimeZoneFunction, ICUTimeZoneBind, ICUTimeZoneInit);\n \tCreateTableFunctionInfo tz_names_info(move(tz_names));\n \tcatalog.CreateTableFunction(*con.context, &tz_names_info);\n \n@@ -358,8 +339,7 @@ void ICUExtension::Load(DuckDB &db) {\n \tstd::unique_ptr<icu::Calendar> cal(icu::Calendar::createInstance(status));\n \tconfig.set_variables[\"Calendar\"] = Value(cal->getType());\n \n-\tTableFunction cal_names(\"icu_calendar_names\", {}, ICUCalendarFunction, ICUCalendarBind, ICUCalendarInit, nullptr,\n-\t                        ICUCalendarCleanup);\n+\tTableFunction cal_names(\"icu_calendar_names\", {}, ICUCalendarFunction, ICUCalendarBind, ICUCalendarInit);\n \tCreateTableFunctionInfo cal_names_info(move(cal_names));\n \tcatalog.CreateTableFunction(*con.context, &cal_names_info);\n \ndiff --git a/extension/parquet/parquet-extension.cpp b/extension/parquet/parquet-extension.cpp\nindex 14a3221570eb..71e2f9491c22 100644\n--- a/extension/parquet/parquet-extension.cpp\n+++ b/extension/parquet/parquet-extension.cpp\n@@ -18,7 +18,6 @@\n #include \"duckdb/function/copy_function.hpp\"\n #include \"duckdb/function/table_function.hpp\"\n #include \"duckdb/common/file_system.hpp\"\n-#include \"duckdb/parallel/parallel_state.hpp\"\n #include \"duckdb/parser/parsed_data/create_copy_function_info.hpp\"\n #include \"duckdb/parser/parsed_data/create_table_function_info.hpp\"\n \n@@ -46,7 +45,7 @@ struct ParquetReadBindData : public TableFunctionData {\n \tvector<LogicalType> types;\n };\n \n-struct ParquetReadOperatorData : public FunctionOperatorData {\n+struct ParquetReadLocalState : public LocalTableFunctionState {\n \tshared_ptr<ParquetReader> reader;\n \tParquetReaderScanState scan_state;\n \tbool is_parallel;\n@@ -56,28 +55,32 @@ struct ParquetReadOperatorData : public FunctionOperatorData {\n \tTableFilterSet *table_filters;\n };\n \n-struct ParquetReadParallelState : public ParallelState {\n+struct ParquetReadGlobalState : public GlobalTableFunctionState {\n \tmutex lock;\n \tshared_ptr<ParquetReader> current_reader;\n \tidx_t batch_index;\n \tidx_t file_index;\n \tidx_t row_group_index;\n+\tidx_t max_threads;\n+\n+\tidx_t MaxThreads() const override {\n+\t\treturn max_threads;\n+\t}\n };\n \n class ParquetScanFunction {\n public:\n \tstatic TableFunctionSet GetFunctionSet() {\n \t\tTableFunctionSet set(\"parquet_scan\");\n-\t\tauto table_function =\n-\t\t    TableFunction({LogicalType::VARCHAR}, ParquetScanImplementation, ParquetScanBind, ParquetScanInit,\n-\t\t                  /* statistics */ ParquetScanStats, /* cleanup */ nullptr,\n-\t\t                  /* dependency */ nullptr, ParquetCardinality,\n-\t\t                  /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr, ParquetScanMaxThreads,\n-\t\t                  ParquetInitParallelState, ParquetScanFuncParallel, ParquetScanParallelInit,\n-\t\t                  ParquetParallelStateNext, true, true, ParquetProgress);\n+\t\tTableFunction table_function({LogicalType::VARCHAR}, ParquetScanImplementation, ParquetScanBind,\n+\t\t                             ParquetScanInitGlobal, ParquetScanInitLocal);\n+\t\ttable_function.statistics = ParquetScanStats;\n+\t\ttable_function.cardinality = ParquetCardinality;\n+\t\ttable_function.table_scan_progress = ParquetProgress;\n \t\ttable_function.named_parameters[\"binary_as_string\"] = LogicalType::BOOLEAN;\n \t\ttable_function.get_batch_index = ParquetScanGetBatchIndex;\n-\t\ttable_function.supports_batch_index = true;\n+\t\ttable_function.projection_pushdown = true;\n+\t\ttable_function.filter_pushdown = true;\n \t\tset.AddFunction(table_function);\n \t\ttable_function.arguments = {LogicalType::LIST(LogicalType::VARCHAR)};\n \t\ttable_function.bind = ParquetScanBindList;\n@@ -171,13 +174,6 @@ class ParquetScanFunction {\n \t\treturn nullptr;\n \t}\n \n-\tstatic void ParquetScanFuncParallel(ClientContext &context, const FunctionData *bind_data,\n-\t                                    FunctionOperatorData *operator_state, DataChunk &output,\n-\t                                    ParallelState *parallel_state_p) {\n-\t\t//! FIXME: Have specialized parallel function from pandas scan here\n-\t\tParquetScanImplementation(context, bind_data, operator_state, output);\n-\t}\n-\n \tstatic unique_ptr<FunctionData> ParquetScanBindInternal(ClientContext &context, vector<string> files,\n \t                                                        vector<LogicalType> &return_types, vector<string> &names,\n \t                                                        ParquetOptions parquet_options) {\n@@ -240,29 +236,8 @@ class ParquetScanFunction {\n \t\treturn ParquetScanBindInternal(context, move(files), return_types, names, parquet_options);\n \t}\n \n-\tstatic unique_ptr<FunctionOperatorData> ParquetScanInit(ClientContext &context, const FunctionData *bind_data_p,\n-\t                                                        const vector<column_t> &column_ids,\n-\t                                                        TableFilterCollection *filters) {\n-\t\tauto &bind_data = (ParquetReadBindData &)*bind_data_p;\n-\t\tbind_data.chunk_count = 0;\n-\t\tbind_data.cur_file = 0;\n-\t\tauto result = make_unique<ParquetReadOperatorData>();\n-\t\tresult->column_ids = column_ids;\n-\n-\t\tresult->is_parallel = false;\n-\t\tresult->file_index = 0;\n-\t\tresult->table_filters = filters->table_filters;\n-\t\t// single-threaded: one thread has to read all groups\n-\t\tvector<idx_t> group_ids;\n-\t\tfor (idx_t i = 0; i < bind_data.initial_reader->NumRowGroups(); i++) {\n-\t\t\tgroup_ids.push_back(i);\n-\t\t}\n-\t\tresult->reader = bind_data.initial_reader;\n-\t\tresult->reader->InitializeScan(result->scan_state, column_ids, move(group_ids), filters->table_filters);\n-\t\treturn move(result);\n-\t}\n-\n-\tstatic double ParquetProgress(ClientContext &context, const FunctionData *bind_data_p) {\n+\tstatic double ParquetProgress(ClientContext &context, const FunctionData *bind_data_p,\n+\t                              const GlobalTableFunctionState *global_state) {\n \t\tauto &bind_data = (ParquetReadBindData &)*bind_data_p;\n \t\tif (bind_data.initial_reader->NumRows() == 0) {\n \t\t\treturn (100.0 * (bind_data.cur_file + 1)) / bind_data.files.size();\n@@ -273,60 +248,57 @@ class ParquetScanFunction {\n \t\treturn percentage;\n \t}\n \n-\tstatic unique_ptr<FunctionOperatorData>\n-\tParquetScanParallelInit(ClientContext &context, const FunctionData *bind_data_p, ParallelState *parallel_state_p,\n-\t                        const vector<column_t> &column_ids, TableFilterCollection *filters) {\n-\t\tauto result = make_unique<ParquetReadOperatorData>();\n-\t\tresult->column_ids = column_ids;\n+\tstatic unique_ptr<LocalTableFunctionState>\n+\tParquetScanInitLocal(ClientContext &context, TableFunctionInitInput &input, GlobalTableFunctionState *gstate_p) {\n+\t\tauto &bind_data = (ParquetReadBindData &)*input.bind_data;\n+\t\tauto &gstate = (ParquetReadGlobalState &)*gstate_p;\n+\n+\t\tauto result = make_unique<ParquetReadLocalState>();\n+\t\tresult->column_ids = input.column_ids;\n \t\tresult->is_parallel = true;\n \t\tresult->batch_index = 0;\n-\t\tresult->table_filters = filters->table_filters;\n-\t\tif (!ParquetParallelStateNext(context, bind_data_p, result.get(), parallel_state_p)) {\n+\t\tresult->table_filters = input.filters;\n+\t\tif (!ParquetParallelStateNext(context, bind_data, *result, gstate)) {\n \t\t\treturn nullptr;\n \t\t}\n \t\treturn move(result);\n \t}\n \n+\tstatic unique_ptr<GlobalTableFunctionState> ParquetScanInitGlobal(ClientContext &context,\n+\t                                                                  TableFunctionInitInput &input) {\n+\t\tauto &bind_data = (ParquetReadBindData &)*input.bind_data;\n+\t\tauto result = make_unique<ParquetReadGlobalState>();\n+\t\tresult->current_reader = bind_data.initial_reader;\n+\t\tresult->row_group_index = 0;\n+\t\tresult->file_index = 0;\n+\t\tresult->batch_index = 0;\n+\t\tresult->max_threads = ParquetScanMaxThreads(context, input.bind_data);\n+\t\treturn move(result);\n+\t}\n+\n \tstatic idx_t ParquetScanGetBatchIndex(ClientContext &context, const FunctionData *bind_data_p,\n-\t                                      FunctionOperatorData *operator_state, ParallelState *parallel_state_p) {\n-\t\tauto &data = (ParquetReadOperatorData &)*operator_state;\n+\t                                      LocalTableFunctionState *local_state,\n+\t                                      GlobalTableFunctionState *global_state) {\n+\t\tauto &data = (ParquetReadLocalState &)*local_state;\n \t\treturn data.batch_index;\n \t}\n \n-\tstatic void ParquetScanImplementation(ClientContext &context, const FunctionData *bind_data_p,\n-\t                                      FunctionOperatorData *operator_state, DataChunk &output) {\n-\t\tif (!operator_state) {\n+\tstatic void ParquetScanImplementation(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\t\tif (!data_p.local_state) {\n \t\t\treturn;\n \t\t}\n-\t\tauto &data = (ParquetReadOperatorData &)*operator_state;\n-\t\tauto &bind_data = (ParquetReadBindData &)*bind_data_p;\n+\t\tauto &data = (ParquetReadLocalState &)*data_p.local_state;\n+\t\tauto &gstate = (ParquetReadGlobalState &)*data_p.global_state;\n+\t\tauto &bind_data = (ParquetReadBindData &)*data_p.bind_data;\n \n \t\tdo {\n \t\t\tdata.reader->Scan(data.scan_state, output);\n \t\t\tbind_data.chunk_count++;\n-\t\t\tif (output.size() == 0 && !data.is_parallel) {\n-\t\t\t\tauto &bind_data = (ParquetReadBindData &)*bind_data_p;\n-\t\t\t\t// check if there is another file\n-\t\t\t\tif (data.file_index + 1 < bind_data.files.size()) {\n-\t\t\t\t\tdata.file_index++;\n-\t\t\t\t\tbind_data.cur_file++;\n-\t\t\t\t\tbind_data.chunk_count = 0;\n-\t\t\t\t\tstring file = bind_data.files[data.file_index];\n-\t\t\t\t\t// move to the next file\n-\t\t\t\t\tdata.reader =\n-\t\t\t\t\t    make_shared<ParquetReader>(context, file, bind_data.names, bind_data.types, data.column_ids,\n-\t\t\t\t\t                               data.reader->parquet_options, bind_data.files[0]);\n-\t\t\t\t\tvector<idx_t> group_ids;\n-\t\t\t\t\tfor (idx_t i = 0; i < data.reader->NumRowGroups(); i++) {\n-\t\t\t\t\t\tgroup_ids.push_back(i);\n-\t\t\t\t\t}\n-\t\t\t\t\tdata.reader->InitializeScan(data.scan_state, data.column_ids, move(group_ids), data.table_filters);\n-\t\t\t\t} else {\n-\t\t\t\t\t// exhausted all the files: done\n-\t\t\t\t\tbreak;\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tbreak;\n+\t\t\tif (output.size() > 0) {\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tif (!ParquetParallelStateNext(context, bind_data, data, gstate)) {\n+\t\t\t\treturn;\n \t\t\t}\n \t\t} while (true);\n \t}\n@@ -341,26 +313,8 @@ class ParquetScanFunction {\n \t\treturn data.initial_reader->NumRowGroups() * data.files.size();\n \t}\n \n-\tstatic unique_ptr<ParallelState> ParquetInitParallelState(ClientContext &context, const FunctionData *bind_data_p,\n-\t                                                          const vector<column_t> &column_ids,\n-\t                                                          TableFilterCollection *filters) {\n-\t\tauto &bind_data = (ParquetReadBindData &)*bind_data_p;\n-\t\tauto result = make_unique<ParquetReadParallelState>();\n-\t\tresult->current_reader = bind_data.initial_reader;\n-\t\tresult->row_group_index = 0;\n-\t\tresult->file_index = 0;\n-\t\tresult->batch_index = 0;\n-\t\treturn move(result);\n-\t}\n-\n-\tstatic bool ParquetParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,\n-\t                                     FunctionOperatorData *state_p, ParallelState *parallel_state_p) {\n-\t\tif (!state_p) {\n-\t\t\treturn false;\n-\t\t}\n-\t\tauto &bind_data = (ParquetReadBindData &)*bind_data_p;\n-\t\tauto &parallel_state = (ParquetReadParallelState &)*parallel_state_p;\n-\t\tauto &scan_data = (ParquetReadOperatorData &)*state_p;\n+\tstatic bool ParquetParallelStateNext(ClientContext &context, const ParquetReadBindData &bind_data,\n+\t                                     ParquetReadLocalState &scan_data, ParquetReadGlobalState &parallel_state) {\n \n \t\tlock_guard<mutex> parallel_lock(parallel_state.lock);\n \t\tif (parallel_state.row_group_index < parallel_state.current_reader->NumRowGroups()) {\ndiff --git a/extension/parquet/parquet_metadata.cpp b/extension/parquet/parquet_metadata.cpp\nindex 728926c7dbba..63da12d659f4 100644\n--- a/extension/parquet/parquet_metadata.cpp\n+++ b/extension/parquet/parquet_metadata.cpp\n@@ -21,7 +21,7 @@ struct ParquetMetaDataBindData : public TableFunctionData {\n \t}\n };\n \n-struct ParquetMetaDataOperatorData : public FunctionOperatorData {\n+struct ParquetMetaDataOperatorData : public GlobalTableFunctionState {\n \tidx_t file_index;\n \tChunkCollection collection;\n \n@@ -415,10 +415,8 @@ unique_ptr<FunctionData> ParquetMetaDataBind(ClientContext &context, TableFuncti\n }\n \n template <bool SCHEMA>\n-unique_ptr<FunctionOperatorData> ParquetMetaDataInit(ClientContext &context, const FunctionData *bind_data_p,\n-                                                     const vector<column_t> &column_ids,\n-                                                     TableFilterCollection *filters) {\n-\tauto &bind_data = (ParquetMetaDataBindData &)*bind_data_p;\n+unique_ptr<GlobalTableFunctionState> ParquetMetaDataInit(ClientContext &context, TableFunctionInitInput &input) {\n+\tauto &bind_data = (ParquetMetaDataBindData &)*input.bind_data;\n \tD_ASSERT(!bind_data.files.empty());\n \n \tauto result = make_unique<ParquetMetaDataOperatorData>();\n@@ -432,10 +430,9 @@ unique_ptr<FunctionOperatorData> ParquetMetaDataInit(ClientContext &context, con\n }\n \n template <bool SCHEMA>\n-void ParquetMetaDataImplementation(ClientContext &context, const FunctionData *bind_data_p,\n-                                   FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &data = (ParquetMetaDataOperatorData &)*operator_state;\n-\tauto &bind_data = (ParquetMetaDataBindData &)*bind_data_p;\n+void ParquetMetaDataImplementation(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (ParquetMetaDataOperatorData &)*data_p.global_state;\n+\tauto &bind_data = (ParquetMetaDataBindData &)*data_p.bind_data;\n \twhile (true) {\n \t\tauto chunk = data.collection.Fetch();\n \t\tif (!chunk) {\n@@ -462,20 +459,12 @@ void ParquetMetaDataImplementation(ClientContext &context, const FunctionData *b\n \n ParquetMetaDataFunction::ParquetMetaDataFunction()\n     : TableFunction(\"parquet_metadata\", {LogicalType::VARCHAR}, ParquetMetaDataImplementation<false>,\n-                    ParquetMetaDataBind<false>, ParquetMetaDataInit<false>, /* statistics */ nullptr,\n-                    /* cleanup */ nullptr,\n-                    /* dependency */ nullptr, nullptr,\n-                    /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr, nullptr, nullptr, nullptr, nullptr,\n-                    nullptr, false, false, nullptr) {\n+                    ParquetMetaDataBind<false>, ParquetMetaDataInit<false>) {\n }\n \n ParquetSchemaFunction::ParquetSchemaFunction()\n     : TableFunction(\"parquet_schema\", {LogicalType::VARCHAR}, ParquetMetaDataImplementation<true>,\n-                    ParquetMetaDataBind<true>, ParquetMetaDataInit<true>, /* statistics */ nullptr,\n-                    /* cleanup */ nullptr,\n-                    /* dependency */ nullptr, nullptr,\n-                    /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr, nullptr, nullptr, nullptr, nullptr,\n-                    nullptr, false, false, nullptr) {\n+                    ParquetMetaDataBind<true>, ParquetMetaDataInit<true>) {\n }\n \n } // namespace duckdb\ndiff --git a/extension/sqlsmith/sqlsmith-extension.cpp b/extension/sqlsmith/sqlsmith-extension.cpp\nindex ccc3786cb8c1..1f3a0db26d33 100644\n--- a/extension/sqlsmith/sqlsmith-extension.cpp\n+++ b/extension/sqlsmith/sqlsmith-extension.cpp\n@@ -54,9 +54,8 @@ static unique_ptr<FunctionData> SQLSmithBind(ClientContext &context, TableFuncti\n \treturn move(result);\n }\n \n-static void SQLSmithFunction(ClientContext &context, const FunctionData *bind_data,\n-                             FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &data = (SQLSmithFunctionData &)*bind_data;\n+static void SQLSmithFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (SQLSmithFunctionData &)*data_p.bind_data;\n \tif (data.finished) {\n \t\treturn;\n \t}\n@@ -101,9 +100,8 @@ static unique_ptr<FunctionData> ReduceSQLBind(ClientContext &context, TableFunct\n \treturn result;\n }\n \n-static void ReduceSQLFunction(ClientContext &context, const FunctionData *bind_data,\n-                              FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &data = (ReduceSQLFunctionData &)*bind_data;\n+static void ReduceSQLFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (ReduceSQLFunctionData &)*data_p.bind_data;\n \tif (data.offset >= data.statements.size()) {\n \t\t// finished returning values\n \t\treturn;\ndiff --git a/extension/substrait/substrait-extension.cpp b/extension/substrait/substrait-extension.cpp\nindex c674399c7b14..7be162760af1 100644\n--- a/extension/substrait/substrait-extension.cpp\n+++ b/extension/substrait/substrait-extension.cpp\n@@ -35,9 +35,8 @@ shared_ptr<Relation> SubstraitPlanToDuckDBRel(Connection &conn, string &serializ\n \treturn transformer_s2d.TransformPlan();\n }\n \n-static void ToSubFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,\n-                          DataChunk &output) {\n-\tauto &data = (ToSubstraitFunctionData &)*bind_data;\n+static void ToSubFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (ToSubstraitFunctionData &)*data_p.bind_data;\n \tif (data.finished) {\n \t\treturn;\n \t}\n@@ -84,9 +83,8 @@ static unique_ptr<FunctionData> FromSubstraitBind(ClientContext &context, TableF\n \treturn move(result);\n }\n \n-static void FromSubFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,\n-                            DataChunk &output) {\n-\tauto &data = (FromSubstraitFunctionData &)*bind_data;\n+static void FromSubFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (FromSubstraitFunctionData &)*data_p.bind_data;\n \tif (!data.res) {\n \t\tdata.res = data.plan->Execute();\n \t}\ndiff --git a/extension/tpcds/tpcds-extension.cpp b/extension/tpcds/tpcds-extension.cpp\nindex 2bd9909b8bed..648f6013192b 100644\n--- a/extension/tpcds/tpcds-extension.cpp\n+++ b/extension/tpcds/tpcds-extension.cpp\n@@ -48,9 +48,8 @@ static unique_ptr<FunctionData> DsdgenBind(ClientContext &context, TableFunction\n \treturn move(result);\n }\n \n-static void DsdgenFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,\n-                           DataChunk &output) {\n-\tauto &data = (DSDGenFunctionData &)*bind_data;\n+static void DsdgenFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (DSDGenFunctionData &)*data_p.bind_data;\n \tif (data.finished) {\n \t\treturn;\n \t}\n@@ -60,14 +59,13 @@ static void DsdgenFunction(ClientContext &context, const FunctionData *bind_data\n \tdata.finished = true;\n }\n \n-struct TPCDSData : public FunctionOperatorData {\n+struct TPCDSData : public GlobalTableFunctionState {\n \tTPCDSData() : offset(0) {\n \t}\n \tidx_t offset;\n };\n \n-unique_ptr<FunctionOperatorData> TPCDSInit(ClientContext &context, const FunctionData *bind_data,\n-                                           const vector<column_t> &column_ids, TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> TPCDSInit(ClientContext &context, TableFunctionInitInput &input) {\n \tauto result = make_unique<TPCDSData>();\n \treturn move(result);\n }\n@@ -83,9 +81,8 @@ static unique_ptr<FunctionData> TPCDSQueryBind(ClientContext &context, TableFunc\n \treturn nullptr;\n }\n \n-static void TPCDSQueryFunction(ClientContext &context, const FunctionData *bind_data,\n-                               FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &data = (TPCDSData &)*operator_state;\n+static void TPCDSQueryFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (TPCDSData &)*data_p.global_state;\n \tidx_t tpcds_queries = tpcds::DSDGenWrapper::QueriesCount();\n \tif (data.offset >= tpcds_queries) {\n \t\t// finished returning values\n@@ -118,9 +115,8 @@ static unique_ptr<FunctionData> TPCDSQueryAnswerBind(ClientContext &context, Tab\n \treturn nullptr;\n }\n \n-static void TPCDSQueryAnswerFunction(ClientContext &context, const FunctionData *bind_data,\n-                                     FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &data = (TPCDSData &)*operator_state;\n+static void TPCDSQueryAnswerFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (TPCDSData &)*data_p.global_state;\n \tidx_t tpcds_queries = tpcds::DSDGenWrapper::QueriesCount();\n \tvector<double> scale_factors {1, 10};\n \tidx_t total_answers = tpcds_queries * scale_factors.size();\ndiff --git a/extension/tpch/tpch-extension.cpp b/extension/tpch/tpch-extension.cpp\nindex 9f678c93dd14..cdc4b862d20e 100644\n--- a/extension/tpch/tpch-extension.cpp\n+++ b/extension/tpch/tpch-extension.cpp\n@@ -46,9 +46,8 @@ static unique_ptr<FunctionData> DbgenBind(ClientContext &context, TableFunctionB\n \treturn move(result);\n }\n \n-static void DbgenFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,\n-                          DataChunk &output) {\n-\tauto &data = (DBGenFunctionData &)*bind_data;\n+static void DbgenFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (DBGenFunctionData &)*data_p.bind_data;\n \tif (data.finished) {\n \t\treturn;\n \t}\n@@ -58,14 +57,13 @@ static void DbgenFunction(ClientContext &context, const FunctionData *bind_data,\n \tdata.finished = true;\n }\n \n-struct TPCHData : public FunctionOperatorData {\n+struct TPCHData : public GlobalTableFunctionState {\n \tTPCHData() : offset(0) {\n \t}\n \tidx_t offset;\n };\n \n-unique_ptr<FunctionOperatorData> TPCHInit(ClientContext &context, const FunctionData *bind_data,\n-                                          const vector<column_t> &column_ids, TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> TPCHInit(ClientContext &context, TableFunctionInitInput &input) {\n \tauto result = make_unique<TPCHData>();\n \treturn move(result);\n }\n@@ -81,9 +79,8 @@ static unique_ptr<FunctionData> TPCHQueryBind(ClientContext &context, TableFunct\n \treturn nullptr;\n }\n \n-static void TPCHQueryFunction(ClientContext &context, const FunctionData *bind_data,\n-                              FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &data = (TPCHData &)*operator_state;\n+static void TPCHQueryFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (TPCHData &)*data_p.global_state;\n \tidx_t tpch_queries = 22;\n \tif (data.offset >= tpch_queries) {\n \t\t// finished returning values\n@@ -116,9 +113,8 @@ static unique_ptr<FunctionData> TPCHQueryAnswerBind(ClientContext &context, Tabl\n \treturn nullptr;\n }\n \n-static void TPCHQueryAnswerFunction(ClientContext &context, const FunctionData *bind_data,\n-                                    FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &data = (TPCHData &)*operator_state;\n+static void TPCHQueryAnswerFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (TPCHData &)*data_p.global_state;\n \tidx_t tpch_queries = 22;\n \tvector<double> scale_factors {0.01, 0.1, 1};\n \tidx_t total_answers = tpch_queries * scale_factors.size();\ndiff --git a/scripts/regression_check.py b/scripts/regression_check.py\nnew file mode 100644\nindex 000000000000..1fa6ae24c47a\n--- /dev/null\n+++ b/scripts/regression_check.py\n@@ -0,0 +1,101 @@\n+import os\n+import sys\n+import duckdb\n+import numpy\n+import subprocess\n+from io import StringIO\n+import csv\n+import statistics\n+\n+old_file = None\n+new_file = None\n+# the threshold at which we consider something a regression (percentage)\n+regression_threshold_percentage = 0.1\n+# minimal seconds diff for something to be a regression (for very fast benchmarks)\n+regression_threshold_seconds = 0.01\n+\n+for arg in sys.argv:\n+    if arg.startswith(\"--old=\"):\n+        old_file = arg.replace(\"--old=\", \"\")\n+    elif arg.startswith(\"--new=\"):\n+        new_file = arg.replace(\"--new=\", \"\")\n+\n+if old_file is None or new_file is None:\n+    print(\"Usage: python scripts/regression_check.py --old=<old_file> --new-<new_file>\")\n+    exit(1)\n+\n+con = duckdb.connect()\n+old_timings_l = con.execute(f\"SELECT name, median(time) FROM read_csv_auto('{old_file}') t(name, nrun, time) GROUP BY ALL ORDER BY ALL\").fetchall()\n+new_timings_l = con.execute(f\"SELECT name, median(time) FROM read_csv_auto('{new_file}') t(name, nrun, time) GROUP BY ALL ORDER BY ALL\").fetchall()\n+\n+old_timings = {}\n+new_timings = {}\n+\n+for entry in old_timings_l:\n+    name  = entry[0]\n+    timing = entry[1]\n+    old_timings[name] = timing\n+\n+for entry in new_timings_l:\n+    name = entry[0]\n+    timing = entry[1]\n+    new_timings[name] = timing\n+\n+slow_keys = []\n+multiply_percentage = 1.0 + regression_threshold_percentage\n+\n+test_keys = list(new_timings.keys())\n+test_keys.sort()\n+\n+for key in test_keys:\n+    new_timing = new_timings[key]\n+    old_timing = old_timings[key]\n+    if (old_timing + regression_threshold_seconds) * multiply_percentage < new_timing:\n+        slow_keys.append(key)\n+\n+return_code = 0\n+if len(slow_keys) > 0:\n+    print('''====================================================\n+==============  REGRESSIONS DETECTED   =============\n+====================================================\n+''')\n+    return_code = 1\n+    for key in slow_keys:\n+        new_timing = new_timings[key]\n+        old_timing = old_timings[key]\n+        print(key)\n+        print(f\"Old timing: {old_timing}\")\n+        print(f\"New timing: {new_timing}\")\n+        print(\"\")\n+\n+    print('''====================================================\n+==================  New Timings   ==================\n+====================================================\n+''')\n+    with open(new_file, 'r') as f:\n+        print(f.read())\n+    print('''====================================================\n+==================  Old Timings   ==================\n+====================================================\n+''')\n+    with open(old_file, 'r') as f:\n+        print(f.read())\n+else:\n+    print('''====================================================\n+============== NO REGRESSIONS DETECTED  =============\n+====================================================\n+''')\n+\n+print('''====================================================\n+=================== ALL TIMINGS  ===================\n+====================================================\n+''')\n+for key in test_keys:\n+    new_timing = new_timings[key]\n+    old_timing = old_timings[key]\n+    print(key)\n+    print(f\"Old timing: {old_timing}\")\n+    print(f\"New timing: {new_timing}\")\n+    print(\"\")\n+\n+exit(return_code)\n\\ No newline at end of file\ndiff --git a/src/common/operator/cast_operators.cpp b/src/common/operator/cast_operators.cpp\nindex 1d4c72ec346e..5122bc947c2b 100644\n--- a/src/common/operator/cast_operators.cpp\n+++ b/src/common/operator/cast_operators.cpp\n@@ -2,6 +2,7 @@\n #include \"duckdb/common/operator/string_cast.hpp\"\n #include \"duckdb/common/operator/numeric_cast.hpp\"\n #include \"duckdb/common/operator/decimal_cast_operators.hpp\"\n+#include \"duckdb/common/operator/multiply.hpp\"\n \n #include \"duckdb/common/exception.hpp\"\n #include \"duckdb/common/limits.hpp\"\n@@ -1216,6 +1217,35 @@ bool TryCastToTimestampSec::Operation(string_t input, timestamp_t &result, bool\n \treturn true;\n }\n \n+template <>\n+bool TryCastToTimestampNS::Operation(date_t input, timestamp_t &result, bool strict) {\n+\tif (!TryCast::Operation<date_t, timestamp_t>(input, result, strict)) {\n+\t\treturn false;\n+\t}\n+\tif (!TryMultiplyOperator::Operation(result.value, Interval::NANOS_PER_MICRO, result.value)) {\n+\t\treturn false;\n+\t}\n+\treturn true;\n+}\n+\n+template <>\n+bool TryCastToTimestampMS::Operation(date_t input, timestamp_t &result, bool strict) {\n+\tif (!TryCast::Operation<date_t, timestamp_t>(input, result, strict)) {\n+\t\treturn false;\n+\t}\n+\tresult.value /= Interval::MICROS_PER_MSEC;\n+\treturn true;\n+}\n+\n+template <>\n+bool TryCastToTimestampSec::Operation(date_t input, timestamp_t &result, bool strict) {\n+\tif (!TryCast::Operation<date_t, timestamp_t>(input, result, strict)) {\n+\t\treturn false;\n+\t}\n+\tresult.value /= Interval::MICROS_PER_MSEC * Interval::MSECS_PER_SEC;\n+\treturn true;\n+}\n+\n //===--------------------------------------------------------------------===//\n // Cast From Blob\n //===--------------------------------------------------------------------===//\ndiff --git a/src/common/vector_operations/vector_cast.cpp b/src/common/vector_operations/vector_cast.cpp\nindex 09a6492e5a26..e9eafd6e99fb 100644\n--- a/src/common/vector_operations/vector_cast.cpp\n+++ b/src/common/vector_operations/vector_cast.cpp\n@@ -339,6 +339,15 @@ static bool DateCastSwitch(Vector &source, Vector &result, idx_t count, string *\n \tcase LogicalTypeId::TIMESTAMP_TZ:\n \t\t// date to timestamp\n \t\treturn VectorTryCastLoop<date_t, timestamp_t, duckdb::TryCast>(source, result, count, error_message);\n+\tcase LogicalTypeId::TIMESTAMP_NS:\n+\t\treturn VectorTryCastLoop<date_t, timestamp_t, duckdb::TryCastToTimestampNS>(source, result, count,\n+\t\t                                                                            error_message);\n+\tcase LogicalTypeId::TIMESTAMP_SEC:\n+\t\treturn VectorTryCastLoop<date_t, timestamp_t, duckdb::TryCastToTimestampSec>(source, result, count,\n+\t\t                                                                             error_message);\n+\tcase LogicalTypeId::TIMESTAMP_MS:\n+\t\treturn VectorTryCastLoop<date_t, timestamp_t, duckdb::TryCastToTimestampMS>(source, result, count,\n+\t\t                                                                            error_message);\n \tdefault:\n \t\treturn TryVectorNullCast(source, result, count, error_message);\n \t}\ndiff --git a/src/execution/operator/projection/physical_tableinout_function.cpp b/src/execution/operator/projection/physical_tableinout_function.cpp\nindex 09fb2f498721..123d67a4b2b8 100644\n--- a/src/execution/operator/projection/physical_tableinout_function.cpp\n+++ b/src/execution/operator/projection/physical_tableinout_function.cpp\n@@ -2,12 +2,20 @@\n \n namespace duckdb {\n \n-class TableInOutFunctionState : public OperatorState {\n+class TableInOutLocalState : public OperatorState {\n public:\n-\tTableInOutFunctionState() {\n+\tTableInOutLocalState() {\n \t}\n \n-\tunique_ptr<FunctionOperatorData> operator_data;\n+\tunique_ptr<LocalTableFunctionState> local_state;\n+};\n+\n+class TableInOutGlobalState : public GlobalOperatorState {\n+public:\n+\tTableInOutGlobalState() {\n+\t}\n+\n+\tunique_ptr<GlobalTableFunctionState> global_state;\n };\n \n PhysicalTableInOutFunction::PhysicalTableInOutFunction(vector<LogicalType> types, TableFunction function_p,\n@@ -18,17 +26,29 @@ PhysicalTableInOutFunction::PhysicalTableInOutFunction(vector<LogicalType> types\n }\n \n unique_ptr<OperatorState> PhysicalTableInOutFunction::GetOperatorState(ClientContext &context) const {\n-\tauto result = make_unique<TableInOutFunctionState>();\n-\tif (function.init) {\n-\t\tresult->operator_data = function.init(context, bind_data.get(), column_ids, nullptr);\n+\tauto result = make_unique<TableInOutLocalState>();\n+\tif (function.init_local) {\n+\t\tTableFunctionInitInput input(bind_data.get(), column_ids, nullptr);\n+\t\tresult->local_state = function.init_local(context, input, nullptr);\n+\t}\n+\treturn move(result);\n+}\n+\n+unique_ptr<GlobalOperatorState> PhysicalTableInOutFunction::GetGlobalOperatorState(ClientContext &context) const {\n+\tauto result = make_unique<TableInOutGlobalState>();\n+\tif (function.init_global) {\n+\t\tTableFunctionInitInput input(bind_data.get(), column_ids, nullptr);\n+\t\tresult->global_state = function.init_global(context, input);\n \t}\n \treturn move(result);\n }\n \n OperatorResultType PhysicalTableInOutFunction::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,\n-                                                       GlobalOperatorState &gstate, OperatorState &state_p) const {\n-\tauto &state = (TableInOutFunctionState &)state_p;\n-\treturn function.in_out_function(context.client, bind_data.get(), state.operator_data.get(), input, chunk);\n+                                                       GlobalOperatorState &gstate_p, OperatorState &state_p) const {\n+\tauto &gstate = (TableInOutGlobalState &)gstate_p;\n+\tauto &state = (TableInOutLocalState &)state_p;\n+\tTableFunctionInput data(bind_data.get(), state.local_state.get(), gstate.global_state.get());\n+\treturn function.in_out_function(context.client, data, input, chunk);\n }\n \n } // namespace duckdb\ndiff --git a/src/execution/operator/scan/physical_table_scan.cpp b/src/execution/operator/scan/physical_table_scan.cpp\nindex b6b9a806488c..e0f406aea574 100644\n--- a/src/execution/operator/scan/physical_table_scan.cpp\n+++ b/src/execution/operator/scan/physical_table_scan.cpp\n@@ -4,7 +4,6 @@\n #include \"duckdb/common/string_util.hpp\"\n #include \"duckdb/planner/expression/bound_conjunction_expression.hpp\"\n #include \"duckdb/transaction/transaction.hpp\"\n-#include \"duckdb/parallel/parallel_state.hpp\"\n \n #include <utility>\n \n@@ -19,103 +18,64 @@ PhysicalTableScan::PhysicalTableScan(vector<LogicalType> types, TableFunction fu\n       table_filters(move(table_filters_p)) {\n }\n \n-class TableScanGlobalState : public GlobalSourceState {\n+class TableScanGlobalSourceState : public GlobalSourceState {\n public:\n-\tTableScanGlobalState(ClientContext &context, const PhysicalTableScan &op) {\n-\t\tif (!op.function.max_threads || !op.function.init_parallel_state) {\n-\t\t\t// table function cannot be parallelized\n-\t\t\treturn;\n-\t\t}\n-\t\t// table function can be parallelized\n-\t\t// check how many threads we can have\n-\t\tmax_threads = op.function.max_threads(context, op.bind_data.get());\n-\t\tif (max_threads <= 1) {\n-\t\t\treturn;\n-\t\t}\n-\t\tif (op.function.init_parallel_state) {\n-\t\t\tTableFilterCollection collection(op.table_filters.get());\n-\t\t\tparallel_state = op.function.init_parallel_state(context, op.bind_data.get(), op.column_ids, &collection);\n+\tTableScanGlobalSourceState(ClientContext &context, const PhysicalTableScan &op) {\n+\t\tif (op.function.init_global) {\n+\t\t\tTableFunctionInitInput input(op.bind_data.get(), op.column_ids, op.table_filters.get());\n+\t\t\tglobal_state = op.function.init_global(context, input);\n+\t\t\tif (global_state) {\n+\t\t\t\tmax_threads = global_state->MaxThreads();\n+\t\t\t}\n+\t\t} else {\n+\t\t\tmax_threads = 1;\n \t\t}\n \t}\n \n \tidx_t max_threads = 0;\n-\tunique_ptr<ParallelState> parallel_state;\n+\tunique_ptr<GlobalTableFunctionState> global_state;\n \n \tidx_t MaxThreads() override {\n \t\treturn max_threads;\n \t}\n };\n \n-class TableScanLocalState : public LocalSourceState {\n+class TableScanLocalSourceState : public LocalSourceState {\n public:\n-\tTableScanLocalState(ExecutionContext &context, TableScanGlobalState &gstate, const PhysicalTableScan &op) {\n-\t\tTableFilterCollection filters(op.table_filters.get());\n-\t\tif (gstate.parallel_state) {\n-\t\t\t// parallel scan init\n-\t\t\toperator_data = op.function.parallel_init(context.client, op.bind_data.get(), gstate.parallel_state.get(),\n-\t\t\t                                          op.column_ids, &filters);\n-\t\t} else if (op.function.init) {\n-\t\t\t// sequential scan init\n-\t\t\toperator_data = op.function.init(context.client, op.bind_data.get(), op.column_ids, &filters);\n+\tTableScanLocalSourceState(ExecutionContext &context, TableScanGlobalSourceState &gstate,\n+\t                          const PhysicalTableScan &op) {\n+\t\tif (op.function.init_local) {\n+\t\t\tTableFunctionInitInput input(op.bind_data.get(), op.column_ids, op.table_filters.get());\n+\t\t\tlocal_state = op.function.init_local(context.client, input, gstate.global_state.get());\n \t\t}\n \t}\n \n-\tunique_ptr<FunctionOperatorData> operator_data;\n+\tunique_ptr<LocalTableFunctionState> local_state;\n };\n \n unique_ptr<LocalSourceState> PhysicalTableScan::GetLocalSourceState(ExecutionContext &context,\n                                                                     GlobalSourceState &gstate) const {\n-\treturn make_unique<TableScanLocalState>(context, (TableScanGlobalState &)gstate, *this);\n+\treturn make_unique<TableScanLocalSourceState>(context, (TableScanGlobalSourceState &)gstate, *this);\n }\n \n unique_ptr<GlobalSourceState> PhysicalTableScan::GetGlobalSourceState(ClientContext &context) const {\n-\treturn make_unique<TableScanGlobalState>(context, *this);\n+\treturn make_unique<TableScanGlobalSourceState>(context, *this);\n }\n \n void PhysicalTableScan::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate_p,\n                                 LocalSourceState &lstate) const {\n \tD_ASSERT(!column_ids.empty());\n-\tauto &gstate = (TableScanGlobalState &)gstate_p;\n-\tauto &state = (TableScanLocalState &)lstate;\n-\n-\tif (!gstate.parallel_state) {\n-\t\t// sequential scan\n-\t\tfunction.function(context.client, bind_data.get(), state.operator_data.get(), chunk);\n-\t\tif (chunk.size() != 0) {\n-\t\t\treturn;\n-\t\t}\n-\t} else {\n-\t\t// parallel scan\n-\t\tdo {\n-\t\t\tif (function.parallel_function) {\n-\t\t\t\tfunction.parallel_function(context.client, bind_data.get(), state.operator_data.get(), chunk,\n-\t\t\t\t                           gstate.parallel_state.get());\n-\t\t\t} else {\n-\t\t\t\tfunction.function(context.client, bind_data.get(), state.operator_data.get(), chunk);\n-\t\t\t}\n+\tauto &gstate = (TableScanGlobalSourceState &)gstate_p;\n+\tauto &state = (TableScanLocalSourceState &)lstate;\n \n-\t\t\tif (chunk.size() == 0) {\n-\t\t\t\tD_ASSERT(function.parallel_state_next);\n-\t\t\t\tif (function.parallel_state_next(context.client, bind_data.get(), state.operator_data.get(),\n-\t\t\t\t                                 gstate.parallel_state.get())) {\n-\t\t\t\t\tcontinue;\n-\t\t\t\t} else {\n-\t\t\t\t\tbreak;\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\treturn;\n-\t\t\t}\n-\t\t} while (true);\n-\t}\n-\tD_ASSERT(chunk.size() == 0);\n-\tif (function.cleanup) {\n-\t\tfunction.cleanup(context.client, bind_data.get(), state.operator_data.get());\n-\t}\n+\tTableFunctionInput data(bind_data.get(), state.local_state.get(), gstate.global_state.get());\n+\tfunction.function(context.client, data, chunk);\n }\n \n double PhysicalTableScan::GetProgress(ClientContext &context, GlobalSourceState &gstate_p) const {\n+\tauto &gstate = (TableScanGlobalSourceState &)gstate_p;\n \tif (function.table_scan_progress) {\n-\t\treturn function.table_scan_progress(context, bind_data.get());\n+\t\treturn function.table_scan_progress(context, bind_data.get(), gstate.global_state.get());\n \t}\n \t// if table_scan_progress is not implemented we don't support this function yet in the progress bar\n \treturn -1;\n@@ -125,10 +85,10 @@ idx_t PhysicalTableScan::GetBatchIndex(ExecutionContext &context, DataChunk &chu\n                                        LocalSourceState &lstate) const {\n \tD_ASSERT(SupportsBatchIndex());\n \tD_ASSERT(function.get_batch_index);\n-\tauto &gstate = (TableScanGlobalState &)gstate_p;\n-\tauto &state = (TableScanLocalState &)lstate;\n-\treturn function.get_batch_index(context.client, bind_data.get(), state.operator_data.get(),\n-\t                                gstate.parallel_state.get());\n+\tauto &gstate = (TableScanGlobalSourceState &)gstate_p;\n+\tauto &state = (TableScanLocalSourceState &)lstate;\n+\treturn function.get_batch_index(context.client, bind_data.get(), state.local_state.get(),\n+\t                                gstate.global_state.get());\n }\n \n string PhysicalTableScan::GetName() const {\ndiff --git a/src/function/table/CMakeLists.txt b/src/function/table/CMakeLists.txt\nindex 53c2403e5c45..65ea201496db 100644\n--- a/src/function/table/CMakeLists.txt\n+++ b/src/function/table/CMakeLists.txt\n@@ -4,6 +4,7 @@ add_library_unity(\n   duckdb_func_table\n   OBJECT\n   arrow.cpp\n+  arrow_conversion.cpp\n   checkpoint.cpp\n   glob.cpp\n   range.cpp\ndiff --git a/src/function/table/arrow.cpp b/src/function/table/arrow.cpp\nindex 8fa3ff9ddcd6..0024ebe2cd51 100644\n--- a/src/function/table/arrow.cpp\n+++ b/src/function/table/arrow.cpp\n@@ -5,18 +5,11 @@\n #include \"duckdb/common/limits.hpp\"\n #include \"duckdb/common/types/date.hpp\"\n #include \"duckdb/common/to_string.hpp\"\n-#include \"duckdb/common/types/hugeint.hpp\"\n #include \"duckdb/function/table/arrow.hpp\"\n #include \"duckdb/function/table_function.hpp\"\n-#include \"duckdb/main/client_context.hpp\"\n-#include \"duckdb/main/connection.hpp\"\n #include \"duckdb/parser/parsed_data/create_table_function_info.hpp\"\n #include \"utf8proc_wrapper.hpp\"\n-#include \"duckdb/common/types/arrow_aux_data.hpp\"\n #include \"duckdb/common/types/vector_buffer.hpp\"\n-#include \"duckdb/common/operator/multiply.hpp\"\n-#include \"duckdb/common/mutex.hpp\"\n-#include <map>\n \n namespace duckdb {\n \n@@ -201,26 +194,14 @@ void RenameArrowColumns(vector<string> &names) {\n \n unique_ptr<FunctionData> ArrowTableFunction::ArrowScanBind(ClientContext &context, TableFunctionBindInput &input,\n                                                            vector<LogicalType> &return_types, vector<string> &names) {\n-\ttypedef unique_ptr<ArrowArrayStreamWrapper> (*stream_factory_produce_t)(\n-\t    uintptr_t stream_factory_ptr,\n-\t    std::pair<std::unordered_map<idx_t, string>, std::vector<string>> & project_columns,\n-\t    TableFilterCollection * filters);\n-\n-\ttypedef void (*stream_factory_get_schema_t)(uintptr_t stream_factory_ptr, ArrowSchemaWrapper & schema);\n-\n \tauto stream_factory_ptr = input.inputs[0].GetPointer();\n \tauto stream_factory_produce = (stream_factory_produce_t)input.inputs[1].GetPointer();\n \tauto stream_factory_get_schema = (stream_factory_get_schema_t)input.inputs[2].GetPointer();\n \tauto rows_per_thread = input.inputs[3].GetValue<uint64_t>();\n \n-\tstd::pair<std::unordered_map<idx_t, string>, std::vector<string>> project_columns;\n-#ifndef DUCKDB_NO_THREADS\n-\n-\tauto res = make_unique<ArrowScanFunctionData>(rows_per_thread, stream_factory_produce, stream_factory_ptr,\n-\t                                              std::this_thread::get_id());\n-#else\n+\tpair<unordered_map<idx_t, string>, vector<string>> project_columns;\n \tauto res = make_unique<ArrowScanFunctionData>(rows_per_thread, stream_factory_produce, stream_factory_ptr);\n-#endif\n+\n \tauto &data = *res;\n \tstream_factory_get_schema(stream_factory_ptr, data.schema_root);\n \tfor (idx_t col_idx = 0; col_idx < (idx_t)data.schema_root.arrow_schema.n_children; col_idx++) {\n@@ -247,8 +228,7 @@ unique_ptr<FunctionData> ArrowTableFunction::ArrowScanBind(ClientContext &contex\n }\n \n unique_ptr<ArrowArrayStreamWrapper> ProduceArrowScan(const ArrowScanFunctionData &function,\n-                                                     const vector<column_t> &column_ids,\n-                                                     TableFilterCollection *filters) {\n+                                                     const vector<column_t> &column_ids, TableFilterSet *filters) {\n \t//! Generate Projection Pushdown Vector\n \tpair<unordered_map<idx_t, string>, vector<string>> project_columns;\n \tD_ASSERT(!column_ids.empty());\n@@ -263,867 +243,6 @@ unique_ptr<ArrowArrayStreamWrapper> ProduceArrowScan(const ArrowScanFunctionData\n \treturn function.scanner_producer(function.stream_factory_ptr, project_columns, filters);\n }\n \n-unique_ptr<FunctionOperatorData> ArrowTableFunction::ArrowScanInit(ClientContext &context,\n-                                                                   const FunctionData *bind_data,\n-                                                                   const vector<column_t> &column_ids,\n-                                                                   TableFilterCollection *filters) {\n-\tauto current_chunk = make_unique<ArrowArrayWrapper>();\n-\tauto result = make_unique<ArrowScanState>(move(current_chunk));\n-\tresult->column_ids = column_ids;\n-\tauto &data = (const ArrowScanFunctionData &)*bind_data;\n-\tresult->stream = ProduceArrowScan(data, column_ids, filters);\n-\treturn move(result);\n-}\n-\n-void ShiftRight(unsigned char *ar, int size, int shift) {\n-\tint carry = 0;\n-\twhile (shift--) {\n-\t\tfor (int i = size - 1; i >= 0; --i) {\n-\t\t\tint next = (ar[i] & 1) ? 0x80 : 0;\n-\t\t\tar[i] = carry | (ar[i] >> 1);\n-\t\t\tcarry = next;\n-\t\t}\n-\t}\n-}\n-\n-void SetValidityMask(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size, int64_t nested_offset,\n-                     bool add_null = false) {\n-\tauto &mask = FlatVector::Validity(vector);\n-\tif (array.null_count != 0 && array.buffers[0]) {\n-\t\tD_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);\n-\t\tauto bit_offset = scan_state.chunk_offset + array.offset;\n-\t\tif (nested_offset != -1) {\n-\t\t\tbit_offset = nested_offset;\n-\t\t}\n-\t\tauto n_bitmask_bytes = (size + 8 - 1) / 8;\n-\t\tmask.EnsureWritable();\n-\t\tif (bit_offset % 8 == 0) {\n-\t\t\t//! just memcpy nullmask\n-\t\t\tmemcpy((void *)mask.GetData(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes);\n-\t\t} else {\n-\t\t\t//! need to re-align nullmask\n-\t\t\tstd::vector<uint8_t> temp_nullmask(n_bitmask_bytes + 1);\n-\t\t\tmemcpy(temp_nullmask.data(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes + 1);\n-\t\t\tShiftRight(temp_nullmask.data(), n_bitmask_bytes + 1,\n-\t\t\t           bit_offset % 8); //! why this has to be a right shift is a mystery to me\n-\t\t\tmemcpy((void *)mask.GetData(), (data_ptr_t)temp_nullmask.data(), n_bitmask_bytes);\n-\t\t}\n-\t}\n-\tif (add_null) {\n-\t\t//! We are setting a validity mask of the data part of dictionary vector\n-\t\t//! For some reason, Nulls are allowed to be indexes, hence we need to set the last element here to be null\n-\t\t//! We might have to resize the mask\n-\t\tmask.Resize(size, size + 1);\n-\t\tmask.SetInvalid(size);\n-\t}\n-}\n-\n-void GetValidityMask(ValidityMask &mask, ArrowArray &array, ArrowScanState &scan_state, idx_t size) {\n-\tif (array.null_count != 0 && array.buffers[0]) {\n-\t\tauto bit_offset = scan_state.chunk_offset + array.offset;\n-\t\tauto n_bitmask_bytes = (size + 8 - 1) / 8;\n-\t\tmask.EnsureWritable();\n-\t\tif (bit_offset % 8 == 0) {\n-\t\t\t//! just memcpy nullmask\n-\t\t\tmemcpy((void *)mask.GetData(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes);\n-\t\t} else {\n-\t\t\t//! need to re-align nullmask\n-\t\t\tstd::vector<uint8_t> temp_nullmask(n_bitmask_bytes + 1);\n-\t\t\tmemcpy(temp_nullmask.data(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes + 1);\n-\t\t\tShiftRight(temp_nullmask.data(), n_bitmask_bytes + 1,\n-\t\t\t           bit_offset % 8); //! why this has to be a right shift is a mystery to me\n-\t\t\tmemcpy((void *)mask.GetData(), (data_ptr_t)temp_nullmask.data(), n_bitmask_bytes);\n-\t\t}\n-\t}\n-}\n-\n-void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,\n-                         std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,\n-                         std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset = -1,\n-                         ValidityMask *parent_mask = nullptr);\n-\n-void ArrowToDuckDBList(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,\n-                       std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,\n-                       std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset, ValidityMask *parent_mask) {\n-\tauto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.first++];\n-\tidx_t list_size = 0;\n-\tSetValidityMask(vector, array, scan_state, size, nested_offset);\n-\tidx_t start_offset = 0;\n-\tidx_t cur_offset = 0;\n-\tif (original_type.first == ArrowVariableSizeType::FIXED_SIZE) {\n-\t\t//! Have to check validity mask before setting this up\n-\t\tidx_t offset = (scan_state.chunk_offset + array.offset) * original_type.second;\n-\t\tif (nested_offset != -1) {\n-\t\t\toffset = original_type.second * nested_offset;\n-\t\t}\n-\t\tstart_offset = offset;\n-\t\tauto list_data = FlatVector::GetData<list_entry_t>(vector);\n-\t\tfor (idx_t i = 0; i < size; i++) {\n-\t\t\tauto &le = list_data[i];\n-\t\t\tle.offset = cur_offset;\n-\t\t\tle.length = original_type.second;\n-\t\t\tcur_offset += original_type.second;\n-\t\t}\n-\t\tlist_size = cur_offset;\n-\t} else if (original_type.first == ArrowVariableSizeType::NORMAL) {\n-\t\tauto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;\n-\t\tif (nested_offset != -1) {\n-\t\t\toffsets = (uint32_t *)array.buffers[1] + nested_offset;\n-\t\t}\n-\t\tstart_offset = offsets[0];\n-\t\tauto list_data = FlatVector::GetData<list_entry_t>(vector);\n-\t\tfor (idx_t i = 0; i < size; i++) {\n-\t\t\tauto &le = list_data[i];\n-\t\t\tle.offset = cur_offset;\n-\t\t\tle.length = offsets[i + 1] - offsets[i];\n-\t\t\tcur_offset += le.length;\n-\t\t}\n-\t\tlist_size = offsets[size];\n-\t} else {\n-\t\tauto offsets = (uint64_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;\n-\t\tif (nested_offset != -1) {\n-\t\t\toffsets = (uint64_t *)array.buffers[1] + nested_offset;\n-\t\t}\n-\t\tstart_offset = offsets[0];\n-\t\tauto list_data = FlatVector::GetData<list_entry_t>(vector);\n-\t\tfor (idx_t i = 0; i < size; i++) {\n-\t\t\tauto &le = list_data[i];\n-\t\t\tle.offset = cur_offset;\n-\t\t\tle.length = offsets[i + 1] - offsets[i];\n-\t\t\tcur_offset += le.length;\n-\t\t}\n-\t\tlist_size = offsets[size];\n-\t}\n-\tlist_size -= start_offset;\n-\tListVector::Reserve(vector, list_size);\n-\tListVector::SetListSize(vector, list_size);\n-\tauto &child_vector = ListVector::GetEntry(vector);\n-\tSetValidityMask(child_vector, *array.children[0], scan_state, list_size, start_offset);\n-\tauto &list_mask = FlatVector::Validity(vector);\n-\tif (parent_mask) {\n-\t\t//! Since this List is owned by a struct we must guarantee their validity map matches on Null\n-\t\tif (!parent_mask->AllValid()) {\n-\t\t\tfor (idx_t i = 0; i < size; i++) {\n-\t\t\t\tif (!parent_mask->RowIsValid(i)) {\n-\t\t\t\t\tlist_mask.SetInvalid(i);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\tif (list_size == 0 && start_offset == 0) {\n-\t\tColumnArrowToDuckDB(child_vector, *array.children[0], scan_state, list_size, arrow_convert_data, col_idx,\n-\t\t                    arrow_convert_idx, -1);\n-\t} else {\n-\t\tColumnArrowToDuckDB(child_vector, *array.children[0], scan_state, list_size, arrow_convert_data, col_idx,\n-\t\t                    arrow_convert_idx, start_offset);\n-\t}\n-}\n-\n-void ArrowToDuckDBBlob(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,\n-                       std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,\n-                       std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset) {\n-\tauto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.first++];\n-\tSetValidityMask(vector, array, scan_state, size, nested_offset);\n-\tif (original_type.first == ArrowVariableSizeType::FIXED_SIZE) {\n-\t\t//! Have to check validity mask before setting this up\n-\t\tidx_t offset = (scan_state.chunk_offset + array.offset) * original_type.second;\n-\t\tif (nested_offset != -1) {\n-\t\t\toffset = original_type.second * nested_offset;\n-\t\t}\n-\t\tauto cdata = (char *)array.buffers[1];\n-\t\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n-\t\t\tif (FlatVector::IsNull(vector, row_idx)) {\n-\t\t\t\tcontinue;\n-\t\t\t}\n-\t\t\tauto bptr = cdata + offset;\n-\t\t\tauto blob_len = original_type.second;\n-\t\t\tFlatVector::GetData<string_t>(vector)[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);\n-\t\t\toffset += blob_len;\n-\t\t}\n-\t} else if (original_type.first == ArrowVariableSizeType::NORMAL) {\n-\t\tauto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;\n-\t\tif (nested_offset != -1) {\n-\t\t\toffsets = (uint32_t *)array.buffers[1] + array.offset + nested_offset;\n-\t\t}\n-\t\tauto cdata = (char *)array.buffers[2];\n-\t\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n-\t\t\tif (FlatVector::IsNull(vector, row_idx)) {\n-\t\t\t\tcontinue;\n-\t\t\t}\n-\t\t\tauto bptr = cdata + offsets[row_idx];\n-\t\t\tauto blob_len = offsets[row_idx + 1] - offsets[row_idx];\n-\t\t\tFlatVector::GetData<string_t>(vector)[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);\n-\t\t}\n-\t} else {\n-\t\t//! Check if last offset is higher than max uint32\n-\t\tif (((uint64_t *)array.buffers[1])[array.length] > NumericLimits<uint32_t>::Maximum()) { // LCOV_EXCL_START\n-\t\t\tthrow std::runtime_error(\"DuckDB does not support Blobs over 4GB\");\n-\t\t} // LCOV_EXCL_STOP\n-\t\tauto offsets = (uint64_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;\n-\t\tif (nested_offset != -1) {\n-\t\t\toffsets = (uint64_t *)array.buffers[1] + array.offset + nested_offset;\n-\t\t}\n-\t\tauto cdata = (char *)array.buffers[2];\n-\t\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n-\t\t\tif (FlatVector::IsNull(vector, row_idx)) {\n-\t\t\t\tcontinue;\n-\t\t\t}\n-\t\t\tauto bptr = cdata + offsets[row_idx];\n-\t\t\tauto blob_len = offsets[row_idx + 1] - offsets[row_idx];\n-\t\t\tFlatVector::GetData<string_t>(vector)[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);\n-\t\t}\n-\t}\n-}\n-\n-void ArrowToDuckDBMapList(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,\n-                          std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,\n-                          std::pair<idx_t, idx_t> &arrow_convert_idx, uint32_t *offsets, ValidityMask *parent_mask) {\n-\tidx_t list_size = offsets[size] - offsets[0];\n-\tListVector::Reserve(vector, list_size);\n-\n-\tauto &child_vector = ListVector::GetEntry(vector);\n-\tauto list_data = FlatVector::GetData<list_entry_t>(vector);\n-\tauto cur_offset = 0;\n-\tfor (idx_t i = 0; i < size; i++) {\n-\t\tauto &le = list_data[i];\n-\t\tle.offset = cur_offset;\n-\t\tle.length = offsets[i + 1] - offsets[i];\n-\t\tcur_offset += le.length;\n-\t}\n-\tListVector::SetListSize(vector, list_size);\n-\tif (list_size == 0 && offsets[0] == 0) {\n-\t\tSetValidityMask(child_vector, array, scan_state, list_size, -1);\n-\t} else {\n-\t\tSetValidityMask(child_vector, array, scan_state, list_size, offsets[0]);\n-\t}\n-\n-\tauto &list_mask = FlatVector::Validity(vector);\n-\tif (parent_mask) {\n-\t\t//! Since this List is owned by a struct we must guarantee their validity map matches on Null\n-\t\tif (!parent_mask->AllValid()) {\n-\t\t\tfor (idx_t i = 0; i < size; i++) {\n-\t\t\t\tif (!parent_mask->RowIsValid(i)) {\n-\t\t\t\t\tlist_mask.SetInvalid(i);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\tif (list_size == 0 && offsets[0] == 0) {\n-\t\tColumnArrowToDuckDB(child_vector, array, scan_state, list_size, arrow_convert_data, col_idx, arrow_convert_idx,\n-\t\t                    -1);\n-\t} else {\n-\t\tColumnArrowToDuckDB(child_vector, array, scan_state, list_size, arrow_convert_data, col_idx, arrow_convert_idx,\n-\t\t                    offsets[0]);\n-\t}\n-}\n-template <class T>\n-static void SetVectorString(Vector &vector, idx_t size, char *cdata, T *offsets) {\n-\tauto strings = FlatVector::GetData<string_t>(vector);\n-\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n-\t\tif (FlatVector::IsNull(vector, row_idx)) {\n-\t\t\tcontinue;\n-\t\t}\n-\t\tauto cptr = cdata + offsets[row_idx];\n-\t\tauto str_len = offsets[row_idx + 1] - offsets[row_idx];\n-\t\tstrings[row_idx] = string_t(cptr, str_len);\n-\t}\n-}\n-\n-void DirectConversion(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, int64_t nested_offset) {\n-\tauto internal_type = GetTypeIdSize(vector.GetType().InternalType());\n-\tauto data_ptr = (data_ptr_t)array.buffers[1] + internal_type * (scan_state.chunk_offset + array.offset);\n-\tif (nested_offset != -1) {\n-\t\tdata_ptr = (data_ptr_t)array.buffers[1] + internal_type * (array.offset + nested_offset);\n-\t}\n-\tFlatVector::SetData(vector, data_ptr);\n-}\n-\n-template <class T>\n-void TimeConversion(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, int64_t nested_offset, idx_t size,\n-                    int64_t conversion) {\n-\tauto tgt_ptr = (dtime_t *)FlatVector::GetData(vector);\n-\tauto &validity_mask = FlatVector::Validity(vector);\n-\tauto src_ptr = (T *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n-\tif (nested_offset != -1) {\n-\t\tsrc_ptr = (T *)array.buffers[1] + nested_offset + array.offset;\n-\t}\n-\tfor (idx_t row = 0; row < size; row++) {\n-\t\tif (!validity_mask.RowIsValid(row)) {\n-\t\t\tcontinue;\n-\t\t}\n-\t\tif (!TryMultiplyOperator::Operation((int64_t)src_ptr[row], conversion, tgt_ptr[row].micros)) {\n-\t\t\tthrow ConversionException(\"Could not convert Time to Microsecond\");\n-\t\t}\n-\t}\n-}\n-\n-void TimestampTZConversion(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, int64_t nested_offset,\n-                           idx_t size, int64_t conversion) {\n-\tauto tgt_ptr = (timestamp_t *)FlatVector::GetData(vector);\n-\tauto &validity_mask = FlatVector::Validity(vector);\n-\tauto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n-\tif (nested_offset != -1) {\n-\t\tsrc_ptr = (int64_t *)array.buffers[1] + nested_offset + array.offset;\n-\t}\n-\tfor (idx_t row = 0; row < size; row++) {\n-\t\tif (!validity_mask.RowIsValid(row)) {\n-\t\t\tcontinue;\n-\t\t}\n-\t\tif (!TryMultiplyOperator::Operation(src_ptr[row], conversion, tgt_ptr[row].value)) {\n-\t\t\tthrow ConversionException(\"Could not convert TimestampTZ to Microsecond\");\n-\t\t}\n-\t}\n-}\n-\n-void IntervalConversionUs(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, int64_t nested_offset,\n-                          idx_t size, int64_t conversion) {\n-\tauto tgt_ptr = (interval_t *)FlatVector::GetData(vector);\n-\tauto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n-\tif (nested_offset != -1) {\n-\t\tsrc_ptr = (int64_t *)array.buffers[1] + nested_offset + array.offset;\n-\t}\n-\tfor (idx_t row = 0; row < size; row++) {\n-\t\ttgt_ptr[row].days = 0;\n-\t\ttgt_ptr[row].months = 0;\n-\t\tif (!TryMultiplyOperator::Operation(src_ptr[row], conversion, tgt_ptr[row].micros)) {\n-\t\t\tthrow ConversionException(\"Could not convert Interval to Microsecond\");\n-\t\t}\n-\t}\n-}\n-\n-void IntervalConversionMonths(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, int64_t nested_offset,\n-                              idx_t size) {\n-\tauto tgt_ptr = (interval_t *)FlatVector::GetData(vector);\n-\tauto src_ptr = (int32_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n-\tif (nested_offset != -1) {\n-\t\tsrc_ptr = (int32_t *)array.buffers[1] + nested_offset + array.offset;\n-\t}\n-\tfor (idx_t row = 0; row < size; row++) {\n-\t\ttgt_ptr[row].days = 0;\n-\t\ttgt_ptr[row].micros = 0;\n-\t\ttgt_ptr[row].months = src_ptr[row];\n-\t}\n-}\n-\n-void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,\n-                         std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,\n-                         std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset, ValidityMask *parent_mask) {\n-\tswitch (vector.GetType().id()) {\n-\tcase LogicalTypeId::SQLNULL:\n-\t\tvector.Reference(Value());\n-\t\tbreak;\n-\tcase LogicalTypeId::BOOLEAN: {\n-\t\t//! Arrow bit-packs boolean values\n-\t\t//! Lets first figure out where we are in the source array\n-\t\tauto src_ptr = (uint8_t *)array.buffers[1] + (scan_state.chunk_offset + array.offset) / 8;\n-\n-\t\tif (nested_offset != -1) {\n-\t\t\tsrc_ptr = (uint8_t *)array.buffers[1] + (nested_offset + array.offset) / 8;\n-\t\t}\n-\t\tauto tgt_ptr = (uint8_t *)FlatVector::GetData(vector);\n-\t\tint src_pos = 0;\n-\t\tidx_t cur_bit = scan_state.chunk_offset % 8;\n-\t\tif (nested_offset != -1) {\n-\t\t\tcur_bit = nested_offset % 8;\n-\t\t}\n-\t\tfor (idx_t row = 0; row < size; row++) {\n-\t\t\tif ((src_ptr[src_pos] & (1 << cur_bit)) == 0) {\n-\t\t\t\ttgt_ptr[row] = 0;\n-\t\t\t} else {\n-\t\t\t\ttgt_ptr[row] = 1;\n-\t\t\t}\n-\t\t\tcur_bit++;\n-\t\t\tif (cur_bit == 8) {\n-\t\t\t\tsrc_pos++;\n-\t\t\t\tcur_bit = 0;\n-\t\t\t}\n-\t\t}\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::TINYINT:\n-\tcase LogicalTypeId::SMALLINT:\n-\tcase LogicalTypeId::INTEGER:\n-\tcase LogicalTypeId::FLOAT:\n-\tcase LogicalTypeId::DOUBLE:\n-\tcase LogicalTypeId::UTINYINT:\n-\tcase LogicalTypeId::USMALLINT:\n-\tcase LogicalTypeId::UINTEGER:\n-\tcase LogicalTypeId::UBIGINT:\n-\tcase LogicalTypeId::BIGINT:\n-\tcase LogicalTypeId::HUGEINT:\n-\tcase LogicalTypeId::TIMESTAMP:\n-\tcase LogicalTypeId::TIMESTAMP_SEC:\n-\tcase LogicalTypeId::TIMESTAMP_MS:\n-\tcase LogicalTypeId::TIMESTAMP_NS: {\n-\t\tDirectConversion(vector, array, scan_state, nested_offset);\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::JSON:\n-\tcase LogicalTypeId::VARCHAR: {\n-\t\tauto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.first++];\n-\t\tauto cdata = (char *)array.buffers[2];\n-\t\tif (original_type.first == ArrowVariableSizeType::SUPER_SIZE) {\n-\t\t\tif (((uint64_t *)array.buffers[1])[array.length] > NumericLimits<uint32_t>::Maximum()) { // LCOV_EXCL_START\n-\t\t\t\tthrow std::runtime_error(\"DuckDB does not support Strings over 4GB\");\n-\t\t\t} // LCOV_EXCL_STOP\n-\t\t\tauto offsets = (uint64_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;\n-\t\t\tif (nested_offset != -1) {\n-\t\t\t\toffsets = (uint64_t *)array.buffers[1] + array.offset + nested_offset;\n-\t\t\t}\n-\t\t\tSetVectorString(vector, size, cdata, offsets);\n-\t\t} else {\n-\t\t\tauto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;\n-\t\t\tif (nested_offset != -1) {\n-\t\t\t\toffsets = (uint32_t *)array.buffers[1] + array.offset + nested_offset;\n-\t\t\t}\n-\t\t\tSetVectorString(vector, size, cdata, offsets);\n-\t\t}\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::DATE: {\n-\t\tauto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];\n-\t\tswitch (precision) {\n-\t\tcase ArrowDateTimeType::DAYS: {\n-\t\t\tDirectConversion(vector, array, scan_state, nested_offset);\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase ArrowDateTimeType::MILLISECONDS: {\n-\t\t\t//! convert date from nanoseconds to days\n-\t\t\tauto src_ptr = (uint64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n-\t\t\tif (nested_offset != -1) {\n-\t\t\t\tsrc_ptr = (uint64_t *)array.buffers[1] + nested_offset + array.offset;\n-\t\t\t}\n-\t\t\tauto tgt_ptr = (date_t *)FlatVector::GetData(vector);\n-\t\t\tfor (idx_t row = 0; row < size; row++) {\n-\t\t\t\ttgt_ptr[row] = date_t(int64_t(src_ptr[row]) / (1000 * 60 * 60 * 24));\n-\t\t\t}\n-\t\t\tbreak;\n-\t\t}\n-\t\tdefault:\n-\t\t\tthrow std::runtime_error(\"Unsupported precision for Date Type \");\n-\t\t}\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::TIME: {\n-\t\tauto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];\n-\t\tswitch (precision) {\n-\t\tcase ArrowDateTimeType::SECONDS: {\n-\t\t\tTimeConversion<int32_t>(vector, array, scan_state, nested_offset, size, 1000000);\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase ArrowDateTimeType::MILLISECONDS: {\n-\t\t\tTimeConversion<int32_t>(vector, array, scan_state, nested_offset, size, 1000);\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase ArrowDateTimeType::MICROSECONDS: {\n-\t\t\tTimeConversion<int64_t>(vector, array, scan_state, nested_offset, size, 1);\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase ArrowDateTimeType::NANOSECONDS: {\n-\t\t\tauto tgt_ptr = (dtime_t *)FlatVector::GetData(vector);\n-\t\t\tauto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n-\t\t\tif (nested_offset != -1) {\n-\t\t\t\tsrc_ptr = (int64_t *)array.buffers[1] + nested_offset + array.offset;\n-\t\t\t}\n-\t\t\tfor (idx_t row = 0; row < size; row++) {\n-\t\t\t\ttgt_ptr[row].micros = src_ptr[row] / 1000;\n-\t\t\t}\n-\t\t\tbreak;\n-\t\t}\n-\t\tdefault:\n-\t\t\tthrow std::runtime_error(\"Unsupported precision for Time Type \");\n-\t\t}\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::TIMESTAMP_TZ: {\n-\t\tauto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];\n-\t\tswitch (precision) {\n-\t\tcase ArrowDateTimeType::SECONDS: {\n-\t\t\tTimestampTZConversion(vector, array, scan_state, nested_offset, size, 1000000);\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase ArrowDateTimeType::MILLISECONDS: {\n-\t\t\tTimestampTZConversion(vector, array, scan_state, nested_offset, size, 1000);\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase ArrowDateTimeType::MICROSECONDS: {\n-\t\t\tDirectConversion(vector, array, scan_state, nested_offset);\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase ArrowDateTimeType::NANOSECONDS: {\n-\t\t\tauto tgt_ptr = (timestamp_t *)FlatVector::GetData(vector);\n-\t\t\tauto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n-\t\t\tif (nested_offset != -1) {\n-\t\t\t\tsrc_ptr = (int64_t *)array.buffers[1] + nested_offset + array.offset;\n-\t\t\t}\n-\t\t\tfor (idx_t row = 0; row < size; row++) {\n-\t\t\t\ttgt_ptr[row].value = src_ptr[row] / 1000;\n-\t\t\t}\n-\t\t\tbreak;\n-\t\t}\n-\t\tdefault:\n-\t\t\tthrow std::runtime_error(\"Unsupported precision for TimestampTZ Type \");\n-\t\t}\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::INTERVAL: {\n-\t\tauto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];\n-\t\tswitch (precision) {\n-\t\tcase ArrowDateTimeType::SECONDS: {\n-\t\t\tIntervalConversionUs(vector, array, scan_state, nested_offset, size, 1000000);\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase ArrowDateTimeType::DAYS:\n-\t\tcase ArrowDateTimeType::MILLISECONDS: {\n-\t\t\tIntervalConversionUs(vector, array, scan_state, nested_offset, size, 1000);\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase ArrowDateTimeType::MICROSECONDS: {\n-\t\t\tIntervalConversionUs(vector, array, scan_state, nested_offset, size, 1);\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase ArrowDateTimeType::NANOSECONDS: {\n-\t\t\tauto tgt_ptr = (interval_t *)FlatVector::GetData(vector);\n-\t\t\tauto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n-\t\t\tif (nested_offset != -1) {\n-\t\t\t\tsrc_ptr = (int64_t *)array.buffers[1] + nested_offset + array.offset;\n-\t\t\t}\n-\t\t\tfor (idx_t row = 0; row < size; row++) {\n-\t\t\t\ttgt_ptr[row].micros = src_ptr[row] / 1000;\n-\t\t\t\ttgt_ptr[row].days = 0;\n-\t\t\t\ttgt_ptr[row].months = 0;\n-\t\t\t}\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase ArrowDateTimeType::MONTHS: {\n-\t\t\tIntervalConversionMonths(vector, array, scan_state, nested_offset, size);\n-\t\t\tbreak;\n-\t\t}\n-\t\tdefault:\n-\t\t\tthrow std::runtime_error(\"Unsupported precision for Interval/Duration Type \");\n-\t\t}\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::DECIMAL: {\n-\t\tauto val_mask = FlatVector::Validity(vector);\n-\t\t//! We have to convert from INT128\n-\t\tauto src_ptr = (hugeint_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n-\t\tif (nested_offset != -1) {\n-\t\t\tsrc_ptr = (hugeint_t *)array.buffers[1] + nested_offset + array.offset;\n-\t\t}\n-\t\tswitch (vector.GetType().InternalType()) {\n-\t\tcase PhysicalType::INT16: {\n-\t\t\tauto tgt_ptr = (int16_t *)FlatVector::GetData(vector);\n-\t\t\tfor (idx_t row = 0; row < size; row++) {\n-\t\t\t\tif (val_mask.RowIsValid(row)) {\n-\t\t\t\t\tauto result = Hugeint::TryCast(src_ptr[row], tgt_ptr[row]);\n-\t\t\t\t\tD_ASSERT(result);\n-\t\t\t\t\t(void)result;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase PhysicalType::INT32: {\n-\t\t\tauto tgt_ptr = (int32_t *)FlatVector::GetData(vector);\n-\t\t\tfor (idx_t row = 0; row < size; row++) {\n-\t\t\t\tif (val_mask.RowIsValid(row)) {\n-\t\t\t\t\tauto result = Hugeint::TryCast(src_ptr[row], tgt_ptr[row]);\n-\t\t\t\t\tD_ASSERT(result);\n-\t\t\t\t\t(void)result;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase PhysicalType::INT64: {\n-\t\t\tauto tgt_ptr = (int64_t *)FlatVector::GetData(vector);\n-\t\t\tfor (idx_t row = 0; row < size; row++) {\n-\t\t\t\tif (val_mask.RowIsValid(row)) {\n-\t\t\t\t\tauto result = Hugeint::TryCast(src_ptr[row], tgt_ptr[row]);\n-\t\t\t\t\tD_ASSERT(result);\n-\t\t\t\t\t(void)result;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase PhysicalType::INT128: {\n-\t\t\tFlatVector::SetData(vector, (data_ptr_t)array.buffers[1] + GetTypeIdSize(vector.GetType().InternalType()) *\n-\t\t\t                                                               (scan_state.chunk_offset + array.offset));\n-\t\t\tbreak;\n-\t\t}\n-\t\tdefault:\n-\t\t\tthrow std::runtime_error(\"Unsupported physical type for Decimal: \" +\n-\t\t\t                         TypeIdToString(vector.GetType().InternalType()));\n-\t\t}\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::BLOB: {\n-\t\tArrowToDuckDBBlob(vector, array, scan_state, size, arrow_convert_data, col_idx, arrow_convert_idx,\n-\t\t                  nested_offset);\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::LIST: {\n-\t\tArrowToDuckDBList(vector, array, scan_state, size, arrow_convert_data, col_idx, arrow_convert_idx,\n-\t\t                  nested_offset, parent_mask);\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::MAP: {\n-\t\t//! Since this is a map we skip first child, because its a struct\n-\t\tauto &struct_arrow = *array.children[0];\n-\t\tauto &child_entries = StructVector::GetEntries(vector);\n-\t\tD_ASSERT(child_entries.size() == 2);\n-\t\tauto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;\n-\t\tif (nested_offset != -1) {\n-\t\t\toffsets = (uint32_t *)array.buffers[1] + nested_offset;\n-\t\t}\n-\t\tauto &struct_validity_mask = FlatVector::Validity(vector);\n-\t\t//! Fill the children\n-\t\tfor (idx_t type_idx = 0; type_idx < (idx_t)struct_arrow.n_children; type_idx++) {\n-\t\t\tArrowToDuckDBMapList(*child_entries[type_idx], *struct_arrow.children[type_idx], scan_state, size,\n-\t\t\t                     arrow_convert_data, col_idx, arrow_convert_idx, offsets, &struct_validity_mask);\n-\t\t}\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::STRUCT: {\n-\t\t//! Fill the children\n-\t\tauto &child_entries = StructVector::GetEntries(vector);\n-\t\tauto &struct_validity_mask = FlatVector::Validity(vector);\n-\t\tfor (idx_t type_idx = 0; type_idx < (idx_t)array.n_children; type_idx++) {\n-\t\t\tSetValidityMask(*child_entries[type_idx], *array.children[type_idx], scan_state, size, nested_offset);\n-\t\t\tif (!struct_validity_mask.AllValid()) {\n-\t\t\t\tauto &child_validity_mark = FlatVector::Validity(*child_entries[type_idx]);\n-\t\t\t\tfor (idx_t i = 0; i < size; i++) {\n-\t\t\t\t\tif (!struct_validity_mask.RowIsValid(i)) {\n-\t\t\t\t\t\tchild_validity_mark.SetInvalid(i);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tColumnArrowToDuckDB(*child_entries[type_idx], *array.children[type_idx], scan_state, size,\n-\t\t\t                    arrow_convert_data, col_idx, arrow_convert_idx, nested_offset, &struct_validity_mask);\n-\t\t}\n-\t\tbreak;\n-\t}\n-\tdefault:\n-\t\tthrow std::runtime_error(\"Unsupported type \" + vector.GetType().ToString());\n-\t}\n-}\n-\n-template <class T>\n-static void SetSelectionVectorLoop(SelectionVector &sel, data_ptr_t indices_p, idx_t size) {\n-\tauto indices = (T *)indices_p;\n-\tfor (idx_t row = 0; row < size; row++) {\n-\t\tsel.set_index(row, indices[row]);\n-\t}\n-}\n-\n-template <class T>\n-static void SetSelectionVectorLoopWithChecks(SelectionVector &sel, data_ptr_t indices_p, idx_t size) {\n-\n-\tauto indices = (T *)indices_p;\n-\tfor (idx_t row = 0; row < size; row++) {\n-\t\tif (indices[row] > NumericLimits<uint32_t>::Maximum()) {\n-\t\t\tthrow std::runtime_error(\"DuckDB only supports indices that fit on an uint32\");\n-\t\t}\n-\t\tsel.set_index(row, indices[row]);\n-\t}\n-}\n-\n-template <class T>\n-static void SetMaskedSelectionVectorLoop(SelectionVector &sel, data_ptr_t indices_p, idx_t size, ValidityMask &mask,\n-                                         idx_t last_element_pos) {\n-\tauto indices = (T *)indices_p;\n-\tfor (idx_t row = 0; row < size; row++) {\n-\t\tif (mask.RowIsValid(row)) {\n-\t\t\tsel.set_index(row, indices[row]);\n-\t\t} else {\n-\t\t\t//! Need to point out to last element\n-\t\t\tsel.set_index(row, last_element_pos);\n-\t\t}\n-\t}\n-}\n-\n-void SetSelectionVector(SelectionVector &sel, data_ptr_t indices_p, LogicalType &logical_type, idx_t size,\n-                        ValidityMask *mask = nullptr, idx_t last_element_pos = 0) {\n-\tsel.Initialize(size);\n-\n-\tif (mask) {\n-\t\tswitch (logical_type.id()) {\n-\t\tcase LogicalTypeId::UTINYINT:\n-\t\t\tSetMaskedSelectionVectorLoop<uint8_t>(sel, indices_p, size, *mask, last_element_pos);\n-\t\t\tbreak;\n-\t\tcase LogicalTypeId::TINYINT:\n-\t\t\tSetMaskedSelectionVectorLoop<int8_t>(sel, indices_p, size, *mask, last_element_pos);\n-\t\t\tbreak;\n-\t\tcase LogicalTypeId::USMALLINT:\n-\t\t\tSetMaskedSelectionVectorLoop<uint16_t>(sel, indices_p, size, *mask, last_element_pos);\n-\t\t\tbreak;\n-\t\tcase LogicalTypeId::SMALLINT:\n-\t\t\tSetMaskedSelectionVectorLoop<int16_t>(sel, indices_p, size, *mask, last_element_pos);\n-\t\t\tbreak;\n-\t\tcase LogicalTypeId::UINTEGER:\n-\t\t\tif (last_element_pos > NumericLimits<uint32_t>::Maximum()) {\n-\t\t\t\t//! Its guaranteed that our indices will point to the last element, so just throw an error\n-\t\t\t\tthrow std::runtime_error(\"DuckDB only supports indices that fit on an uint32\");\n-\t\t\t}\n-\t\t\tSetMaskedSelectionVectorLoop<uint32_t>(sel, indices_p, size, *mask, last_element_pos);\n-\t\t\tbreak;\n-\t\tcase LogicalTypeId::INTEGER:\n-\t\t\tSetMaskedSelectionVectorLoop<int32_t>(sel, indices_p, size, *mask, last_element_pos);\n-\t\t\tbreak;\n-\t\tcase LogicalTypeId::UBIGINT:\n-\t\t\tif (last_element_pos > NumericLimits<uint32_t>::Maximum()) {\n-\t\t\t\t//! Its guaranteed that our indices will point to the last element, so just throw an error\n-\t\t\t\tthrow std::runtime_error(\"DuckDB only supports indices that fit on an uint32\");\n-\t\t\t}\n-\t\t\tSetMaskedSelectionVectorLoop<uint64_t>(sel, indices_p, size, *mask, last_element_pos);\n-\t\t\tbreak;\n-\t\tcase LogicalTypeId::BIGINT:\n-\t\t\tif (last_element_pos > NumericLimits<uint32_t>::Maximum()) {\n-\t\t\t\t//! Its guaranteed that our indices will point to the last element, so just throw an error\n-\t\t\t\tthrow std::runtime_error(\"DuckDB only supports indices that fit on an uint32\");\n-\t\t\t}\n-\t\t\tSetMaskedSelectionVectorLoop<int64_t>(sel, indices_p, size, *mask, last_element_pos);\n-\t\t\tbreak;\n-\n-\t\tdefault:\n-\t\t\tthrow std::runtime_error(\"(Arrow) Unsupported type for selection vectors \" + logical_type.ToString());\n-\t\t}\n-\n-\t} else {\n-\t\tswitch (logical_type.id()) {\n-\t\tcase LogicalTypeId::UTINYINT:\n-\t\t\tSetSelectionVectorLoop<uint8_t>(sel, indices_p, size);\n-\t\t\tbreak;\n-\t\tcase LogicalTypeId::TINYINT:\n-\t\t\tSetSelectionVectorLoop<int8_t>(sel, indices_p, size);\n-\t\t\tbreak;\n-\t\tcase LogicalTypeId::USMALLINT:\n-\t\t\tSetSelectionVectorLoop<uint16_t>(sel, indices_p, size);\n-\t\t\tbreak;\n-\t\tcase LogicalTypeId::SMALLINT:\n-\t\t\tSetSelectionVectorLoop<int16_t>(sel, indices_p, size);\n-\t\t\tbreak;\n-\t\tcase LogicalTypeId::UINTEGER:\n-\t\t\tSetSelectionVectorLoop<uint32_t>(sel, indices_p, size);\n-\t\t\tbreak;\n-\t\tcase LogicalTypeId::INTEGER:\n-\t\t\tSetSelectionVectorLoop<int32_t>(sel, indices_p, size);\n-\t\t\tbreak;\n-\t\tcase LogicalTypeId::UBIGINT:\n-\t\t\tif (last_element_pos > NumericLimits<uint32_t>::Maximum()) {\n-\t\t\t\t//! We need to check if our indexes fit in a uint32_t\n-\t\t\t\tSetSelectionVectorLoopWithChecks<uint64_t>(sel, indices_p, size);\n-\t\t\t} else {\n-\t\t\t\tSetSelectionVectorLoop<uint64_t>(sel, indices_p, size);\n-\t\t\t}\n-\t\t\tbreak;\n-\t\tcase LogicalTypeId::BIGINT:\n-\t\t\tif (last_element_pos > NumericLimits<uint32_t>::Maximum()) {\n-\t\t\t\t//! We need to check if our indexes fit in a uint32_t\n-\t\t\t\tSetSelectionVectorLoopWithChecks<int64_t>(sel, indices_p, size);\n-\t\t\t} else {\n-\t\t\t\tSetSelectionVectorLoop<int64_t>(sel, indices_p, size);\n-\t\t\t}\n-\t\t\tbreak;\n-\t\tdefault:\n-\t\t\tthrow std::runtime_error(\"(Arrow) Unsupported type for selection vectors \" + logical_type.ToString());\n-\t\t}\n-\t}\n-}\n-\n-void ColumnArrowToDuckDBDictionary(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,\n-                                   std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,\n-                                   idx_t col_idx, std::pair<idx_t, idx_t> &arrow_convert_idx) {\n-\tSelectionVector sel;\n-\tauto &dict_vectors = scan_state.arrow_dictionary_vectors;\n-\tif (dict_vectors.find(col_idx) == dict_vectors.end()) {\n-\t\t//! We need to set the dictionary data for this column\n-\t\tauto base_vector = make_unique<Vector>(vector.GetType(), array.dictionary->length);\n-\t\tSetValidityMask(*base_vector, *array.dictionary, scan_state, array.dictionary->length, 0, array.null_count > 0);\n-\t\tColumnArrowToDuckDB(*base_vector, *array.dictionary, scan_state, array.dictionary->length, arrow_convert_data,\n-\t\t                    col_idx, arrow_convert_idx);\n-\t\tdict_vectors[col_idx] = move(base_vector);\n-\t}\n-\tauto dictionary_type = arrow_convert_data[col_idx]->dictionary_type;\n-\t//! Get Pointer to Indices of Dictionary\n-\tauto indices = (data_ptr_t)array.buffers[1] +\n-\t               GetTypeIdSize(dictionary_type.InternalType()) * (scan_state.chunk_offset + array.offset);\n-\tif (array.null_count > 0) {\n-\t\tValidityMask indices_validity;\n-\t\tGetValidityMask(indices_validity, array, scan_state, size);\n-\t\tSetSelectionVector(sel, indices, dictionary_type, size, &indices_validity, array.dictionary->length);\n-\t} else {\n-\t\tSetSelectionVector(sel, indices, dictionary_type, size);\n-\t}\n-\tvector.Slice(*dict_vectors[col_idx], sel, size);\n-}\n-\n-void ArrowTableFunction::ArrowToDuckDB(ArrowScanState &scan_state,\n-                                       std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,\n-                                       DataChunk &output, idx_t start) {\n-\tfor (idx_t idx = 0; idx < output.ColumnCount(); idx++) {\n-\t\tauto col_idx = scan_state.column_ids[idx];\n-\t\tstd::pair<idx_t, idx_t> arrow_convert_idx {0, 0};\n-\t\tauto &array = *scan_state.chunk->arrow_array.children[idx];\n-\t\tif (!array.release) {\n-\t\t\tthrow InvalidInputException(\"arrow_scan: released array passed\");\n-\t\t}\n-\t\tif (array.length != scan_state.chunk->arrow_array.length) {\n-\t\t\tthrow InvalidInputException(\"arrow_scan: array length mismatch\");\n-\t\t}\n-\t\toutput.data[idx].GetBuffer()->SetAuxiliaryData(make_unique<ArrowAuxiliaryData>(scan_state.chunk));\n-\t\tif (array.dictionary) {\n-\t\t\tColumnArrowToDuckDBDictionary(output.data[idx], array, scan_state, output.size(), arrow_convert_data,\n-\t\t\t                              col_idx, arrow_convert_idx);\n-\t\t} else {\n-\t\t\tSetValidityMask(output.data[idx], array, scan_state, output.size(), -1);\n-\t\t\tColumnArrowToDuckDB(output.data[idx], array, scan_state, output.size(), arrow_convert_data, col_idx,\n-\t\t\t                    arrow_convert_idx);\n-\t\t}\n-\t}\n-}\n-\n-void ArrowTableFunction::ArrowScanFunction(ClientContext &context, const FunctionData *bind_data,\n-                                           FunctionOperatorData *operator_state, DataChunk &output) {\n-\n-\tauto &data = (ArrowScanFunctionData &)*bind_data;\n-\tauto &state = (ArrowScanState &)*operator_state;\n-\n-\t//! have we run out of data on the current chunk? move to next one\n-\twhile (state.chunk_offset >= (idx_t)state.chunk->arrow_array.length) {\n-\t\tstate.chunk_offset = 0;\n-\t\tstate.arrow_dictionary_vectors.clear();\n-\t\tstate.chunk = state.stream->GetNextChunk();\n-\t\t//! have we run out of chunks? we are done\n-\t\tif (!state.chunk->arrow_array.release) {\n-\t\t\treturn;\n-\t\t}\n-\t}\n-\n-\tint64_t output_size = MinValue<int64_t>(STANDARD_VECTOR_SIZE, state.chunk->arrow_array.length - state.chunk_offset);\n-\tdata.lines_read += output_size;\n-\toutput.SetCardinality(output_size);\n-\tArrowToDuckDB(state, data.arrow_convert_data, output, data.lines_read - output_size);\n-\toutput.Verify();\n-\tstate.chunk_offset += output.size();\n-}\n-\n-void ArrowTableFunction::ArrowScanFunctionParallel(ClientContext &context, const FunctionData *bind_data,\n-                                                   FunctionOperatorData *operator_state, DataChunk &output,\n-                                                   ParallelState *parallel_state_p) {\n-\tauto &data = (ArrowScanFunctionData &)*bind_data;\n-\tauto &state = (ArrowScanState &)*operator_state;\n-\t//! Out of tuples in this chunk\n-\tif (state.chunk_offset >= (idx_t)state.chunk->arrow_array.length) {\n-\t\treturn;\n-\t}\n-\tint64_t output_size = MinValue<int64_t>(STANDARD_VECTOR_SIZE, state.chunk->arrow_array.length - state.chunk_offset);\n-\tdata.lines_read += output_size;\n-\toutput.SetCardinality(output_size);\n-\tArrowToDuckDB(state, data.arrow_convert_data, output, data.lines_read - output_size);\n-\toutput.Verify();\n-\tstate.chunk_offset += output.size();\n-}\n-\n idx_t ArrowTableFunction::ArrowScanMaxThreads(ClientContext &context, const FunctionData *bind_data_p) {\n \tauto &bind_data = (const ArrowScanFunctionData &)*bind_data_p;\n \tif (bind_data.number_of_rows <= 0 || ClientConfig::GetConfig(context).verify_parallelism) {\n@@ -1132,22 +251,8 @@ idx_t ArrowTableFunction::ArrowScanMaxThreads(ClientContext &context, const Func\n \treturn ((bind_data.number_of_rows + bind_data.rows_per_thread - 1) / bind_data.rows_per_thread) + 1;\n }\n \n-unique_ptr<ParallelState> ArrowTableFunction::ArrowScanInitParallelState(ClientContext &context,\n-                                                                         const FunctionData *bind_data_p,\n-                                                                         const vector<column_t> &column_ids,\n-                                                                         TableFilterCollection *filters) {\n-\tauto &bind_data = (const ArrowScanFunctionData &)*bind_data_p;\n-\tauto result = make_unique<ParallelArrowScanState>();\n-\tresult->stream = ProduceArrowScan(bind_data, column_ids, filters);\n-\treturn move(result);\n-}\n-\n-bool ArrowTableFunction::ArrowScanParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,\n-                                                    FunctionOperatorData *operator_state,\n-                                                    ParallelState *parallel_state_p) {\n-\tauto &state = (ArrowScanState &)*operator_state;\n-\tauto &parallel_state = (ParallelArrowScanState &)*parallel_state_p;\n-\n+bool ArrowScanParallelStateNext(ClientContext &context, const FunctionData *bind_data_p, ArrowScanLocalState &state,\n+                                ArrowScanGlobalState &parallel_state) {\n \tlock_guard<mutex> parallel_lock(parallel_state.main_mutex);\n \tstate.chunk_offset = 0;\n \n@@ -1163,23 +268,58 @@ bool ArrowTableFunction::ArrowScanParallelStateNext(ClientContext &context, cons\n \treturn true;\n }\n \n-unique_ptr<FunctionOperatorData>\n-ArrowTableFunction::ArrowScanParallelInit(ClientContext &context, const FunctionData *bind_data_p, ParallelState *state,\n-                                          const vector<column_t> &column_ids, TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> ArrowTableFunction::ArrowScanInitGlobal(ClientContext &context,\n+                                                                             TableFunctionInitInput &input) {\n+\tauto &bind_data = (const ArrowScanFunctionData &)*input.bind_data;\n+\tauto result = make_unique<ArrowScanGlobalState>();\n+\tresult->stream = ProduceArrowScan(bind_data, input.column_ids, input.filters);\n+\tresult->max_threads = ArrowScanMaxThreads(context, input.bind_data);\n+\treturn move(result);\n+}\n+\n+unique_ptr<LocalTableFunctionState> ArrowTableFunction::ArrowScanInitLocal(ClientContext &context,\n+                                                                           TableFunctionInitInput &input,\n+                                                                           GlobalTableFunctionState *global_state_p) {\n+\tauto &global_state = (ArrowScanGlobalState &)*global_state_p;\n \tauto current_chunk = make_unique<ArrowArrayWrapper>();\n-\tauto result = make_unique<ArrowScanState>(move(current_chunk));\n-\tresult->column_ids = column_ids;\n-\tresult->filters = filters;\n-\tArrowScanParallelStateNext(context, bind_data_p, result.get(), state);\n+\tauto result = make_unique<ArrowScanLocalState>(move(current_chunk));\n+\tresult->column_ids = input.column_ids;\n+\tresult->filters = input.filters;\n+\tif (!ArrowScanParallelStateNext(context, input.bind_data, *result, global_state)) {\n+\t\treturn nullptr;\n+\t}\n \treturn move(result);\n }\n \n+void ArrowTableFunction::ArrowScanFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tif (!data_p.local_state) {\n+\t\treturn;\n+\t}\n+\tauto &data = (ArrowScanFunctionData &)*data_p.bind_data;\n+\tauto &state = (ArrowScanLocalState &)*data_p.local_state;\n+\tauto &global_state = (ArrowScanGlobalState &)*data_p.global_state;\n+\n+\t//! Out of tuples in this chunk\n+\tif (state.chunk_offset >= (idx_t)state.chunk->arrow_array.length) {\n+\t\tif (!ArrowScanParallelStateNext(context, data_p.bind_data, state, global_state)) {\n+\t\t\treturn;\n+\t\t}\n+\t}\n+\tint64_t output_size = MinValue<int64_t>(STANDARD_VECTOR_SIZE, state.chunk->arrow_array.length - state.chunk_offset);\n+\tdata.lines_read += output_size;\n+\toutput.SetCardinality(output_size);\n+\tArrowToDuckDB(state, data.arrow_convert_data, output, data.lines_read - output_size);\n+\toutput.Verify();\n+\tstate.chunk_offset += output.size();\n+}\n+\n unique_ptr<NodeStatistics> ArrowTableFunction::ArrowScanCardinality(ClientContext &context, const FunctionData *data) {\n \tauto &bind_data = (ArrowScanFunctionData &)*data;\n \treturn make_unique<NodeStatistics>(bind_data.number_of_rows, bind_data.number_of_rows);\n }\n \n-double ArrowTableFunction::ArrowProgress(ClientContext &context, const FunctionData *bind_data_p) {\n+double ArrowTableFunction::ArrowProgress(ClientContext &context, const FunctionData *bind_data_p,\n+                                         const GlobalTableFunctionState *global_state) {\n \tauto &bind_data = (const ArrowScanFunctionData &)*bind_data_p;\n \tif (bind_data.number_of_rows == 0) {\n \t\treturn 100;\n@@ -1189,12 +329,13 @@ double ArrowTableFunction::ArrowProgress(ClientContext &context, const FunctionD\n }\n \n void ArrowTableFunction::RegisterFunction(BuiltinFunctions &set) {\n-\tTableFunctionSet arrow(\"arrow_scan\");\n-\tarrow.AddFunction(\n-\t    TableFunction({LogicalType::POINTER, LogicalType::POINTER, LogicalType::POINTER, LogicalType::UBIGINT},\n-\t                  ArrowScanFunction, ArrowScanBind, ArrowScanInit, nullptr, nullptr, nullptr, ArrowScanCardinality,\n-\t                  nullptr, nullptr, ArrowScanMaxThreads, ArrowScanInitParallelState, ArrowScanFunctionParallel,\n-\t                  ArrowScanParallelInit, ArrowScanParallelStateNext, true, true, ArrowProgress));\n+\tTableFunction arrow(\"arrow_scan\",\n+\t                    {LogicalType::POINTER, LogicalType::POINTER, LogicalType::POINTER, LogicalType::UBIGINT},\n+\t                    ArrowScanFunction, ArrowScanBind, ArrowScanInitGlobal, ArrowScanInitLocal);\n+\tarrow.cardinality = ArrowScanCardinality;\n+\tarrow.projection_pushdown = true;\n+\tarrow.filter_pushdown = true;\n+\tarrow.table_scan_progress = ArrowProgress;\n \tset.AddFunction(arrow);\n }\n \ndiff --git a/src/function/table/arrow_conversion.cpp b/src/function/table/arrow_conversion.cpp\nnew file mode 100644\nindex 000000000000..6e089b718f8b\n--- /dev/null\n+++ b/src/function/table/arrow_conversion.cpp\n@@ -0,0 +1,816 @@\n+#include \"duckdb/function/table/arrow.hpp\"\n+#include \"duckdb/common/limits.hpp\"\n+#include \"duckdb/common/operator/multiply.hpp\"\n+#include \"duckdb/common/types/hugeint.hpp\"\n+#include \"duckdb/common/types/arrow_aux_data.hpp\"\n+\n+namespace duckdb {\n+\n+void ShiftRight(unsigned char *ar, int size, int shift) {\n+\tint carry = 0;\n+\twhile (shift--) {\n+\t\tfor (int i = size - 1; i >= 0; --i) {\n+\t\t\tint next = (ar[i] & 1) ? 0x80 : 0;\n+\t\t\tar[i] = carry | (ar[i] >> 1);\n+\t\t\tcarry = next;\n+\t\t}\n+\t}\n+}\n+\n+void SetValidityMask(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n+                     int64_t nested_offset, bool add_null = false) {\n+\tauto &mask = FlatVector::Validity(vector);\n+\tif (array.null_count != 0 && array.buffers[0]) {\n+\t\tD_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);\n+\t\tauto bit_offset = scan_state.chunk_offset + array.offset;\n+\t\tif (nested_offset != -1) {\n+\t\t\tbit_offset = nested_offset;\n+\t\t}\n+\t\tauto n_bitmask_bytes = (size + 8 - 1) / 8;\n+\t\tmask.EnsureWritable();\n+\t\tif (bit_offset % 8 == 0) {\n+\t\t\t//! just memcpy nullmask\n+\t\t\tmemcpy((void *)mask.GetData(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes);\n+\t\t} else {\n+\t\t\t//! need to re-align nullmask\n+\t\t\tstd::vector<uint8_t> temp_nullmask(n_bitmask_bytes + 1);\n+\t\t\tmemcpy(temp_nullmask.data(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes + 1);\n+\t\t\tShiftRight(temp_nullmask.data(), n_bitmask_bytes + 1,\n+\t\t\t           bit_offset % 8); //! why this has to be a right shift is a mystery to me\n+\t\t\tmemcpy((void *)mask.GetData(), (data_ptr_t)temp_nullmask.data(), n_bitmask_bytes);\n+\t\t}\n+\t}\n+\tif (add_null) {\n+\t\t//! We are setting a validity mask of the data part of dictionary vector\n+\t\t//! For some reason, Nulls are allowed to be indexes, hence we need to set the last element here to be null\n+\t\t//! We might have to resize the mask\n+\t\tmask.Resize(size, size + 1);\n+\t\tmask.SetInvalid(size);\n+\t}\n+}\n+\n+void GetValidityMask(ValidityMask &mask, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size) {\n+\tif (array.null_count != 0 && array.buffers[0]) {\n+\t\tauto bit_offset = scan_state.chunk_offset + array.offset;\n+\t\tauto n_bitmask_bytes = (size + 8 - 1) / 8;\n+\t\tmask.EnsureWritable();\n+\t\tif (bit_offset % 8 == 0) {\n+\t\t\t//! just memcpy nullmask\n+\t\t\tmemcpy((void *)mask.GetData(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes);\n+\t\t} else {\n+\t\t\t//! need to re-align nullmask\n+\t\t\tstd::vector<uint8_t> temp_nullmask(n_bitmask_bytes + 1);\n+\t\t\tmemcpy(temp_nullmask.data(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes + 1);\n+\t\t\tShiftRight(temp_nullmask.data(), n_bitmask_bytes + 1,\n+\t\t\t           bit_offset % 8); //! why this has to be a right shift is a mystery to me\n+\t\t\tmemcpy((void *)mask.GetData(), (data_ptr_t)temp_nullmask.data(), n_bitmask_bytes);\n+\t\t}\n+\t}\n+}\n+\n+void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n+                         std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,\n+                         std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset = -1,\n+                         ValidityMask *parent_mask = nullptr);\n+\n+void ArrowToDuckDBList(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n+                       std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,\n+                       std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset, ValidityMask *parent_mask) {\n+\tauto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.first++];\n+\tidx_t list_size = 0;\n+\tSetValidityMask(vector, array, scan_state, size, nested_offset);\n+\tidx_t start_offset = 0;\n+\tidx_t cur_offset = 0;\n+\tif (original_type.first == ArrowVariableSizeType::FIXED_SIZE) {\n+\t\t//! Have to check validity mask before setting this up\n+\t\tidx_t offset = (scan_state.chunk_offset + array.offset) * original_type.second;\n+\t\tif (nested_offset != -1) {\n+\t\t\toffset = original_type.second * nested_offset;\n+\t\t}\n+\t\tstart_offset = offset;\n+\t\tauto list_data = FlatVector::GetData<list_entry_t>(vector);\n+\t\tfor (idx_t i = 0; i < size; i++) {\n+\t\t\tauto &le = list_data[i];\n+\t\t\tle.offset = cur_offset;\n+\t\t\tle.length = original_type.second;\n+\t\t\tcur_offset += original_type.second;\n+\t\t}\n+\t\tlist_size = cur_offset;\n+\t} else if (original_type.first == ArrowVariableSizeType::NORMAL) {\n+\t\tauto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;\n+\t\tif (nested_offset != -1) {\n+\t\t\toffsets = (uint32_t *)array.buffers[1] + nested_offset;\n+\t\t}\n+\t\tstart_offset = offsets[0];\n+\t\tauto list_data = FlatVector::GetData<list_entry_t>(vector);\n+\t\tfor (idx_t i = 0; i < size; i++) {\n+\t\t\tauto &le = list_data[i];\n+\t\t\tle.offset = cur_offset;\n+\t\t\tle.length = offsets[i + 1] - offsets[i];\n+\t\t\tcur_offset += le.length;\n+\t\t}\n+\t\tlist_size = offsets[size];\n+\t} else {\n+\t\tauto offsets = (uint64_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;\n+\t\tif (nested_offset != -1) {\n+\t\t\toffsets = (uint64_t *)array.buffers[1] + nested_offset;\n+\t\t}\n+\t\tstart_offset = offsets[0];\n+\t\tauto list_data = FlatVector::GetData<list_entry_t>(vector);\n+\t\tfor (idx_t i = 0; i < size; i++) {\n+\t\t\tauto &le = list_data[i];\n+\t\t\tle.offset = cur_offset;\n+\t\t\tle.length = offsets[i + 1] - offsets[i];\n+\t\t\tcur_offset += le.length;\n+\t\t}\n+\t\tlist_size = offsets[size];\n+\t}\n+\tlist_size -= start_offset;\n+\tListVector::Reserve(vector, list_size);\n+\tListVector::SetListSize(vector, list_size);\n+\tauto &child_vector = ListVector::GetEntry(vector);\n+\tSetValidityMask(child_vector, *array.children[0], scan_state, list_size, start_offset);\n+\tauto &list_mask = FlatVector::Validity(vector);\n+\tif (parent_mask) {\n+\t\t//! Since this List is owned by a struct we must guarantee their validity map matches on Null\n+\t\tif (!parent_mask->AllValid()) {\n+\t\t\tfor (idx_t i = 0; i < size; i++) {\n+\t\t\t\tif (!parent_mask->RowIsValid(i)) {\n+\t\t\t\t\tlist_mask.SetInvalid(i);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif (list_size == 0 && start_offset == 0) {\n+\t\tColumnArrowToDuckDB(child_vector, *array.children[0], scan_state, list_size, arrow_convert_data, col_idx,\n+\t\t                    arrow_convert_idx, -1);\n+\t} else {\n+\t\tColumnArrowToDuckDB(child_vector, *array.children[0], scan_state, list_size, arrow_convert_data, col_idx,\n+\t\t                    arrow_convert_idx, start_offset);\n+\t}\n+}\n+\n+void ArrowToDuckDBBlob(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n+                       std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,\n+                       std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset) {\n+\tauto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.first++];\n+\tSetValidityMask(vector, array, scan_state, size, nested_offset);\n+\tif (original_type.first == ArrowVariableSizeType::FIXED_SIZE) {\n+\t\t//! Have to check validity mask before setting this up\n+\t\tidx_t offset = (scan_state.chunk_offset + array.offset) * original_type.second;\n+\t\tif (nested_offset != -1) {\n+\t\t\toffset = original_type.second * nested_offset;\n+\t\t}\n+\t\tauto cdata = (char *)array.buffers[1];\n+\t\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n+\t\t\tif (FlatVector::IsNull(vector, row_idx)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tauto bptr = cdata + offset;\n+\t\t\tauto blob_len = original_type.second;\n+\t\t\tFlatVector::GetData<string_t>(vector)[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);\n+\t\t\toffset += blob_len;\n+\t\t}\n+\t} else if (original_type.first == ArrowVariableSizeType::NORMAL) {\n+\t\tauto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;\n+\t\tif (nested_offset != -1) {\n+\t\t\toffsets = (uint32_t *)array.buffers[1] + array.offset + nested_offset;\n+\t\t}\n+\t\tauto cdata = (char *)array.buffers[2];\n+\t\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n+\t\t\tif (FlatVector::IsNull(vector, row_idx)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tauto bptr = cdata + offsets[row_idx];\n+\t\t\tauto blob_len = offsets[row_idx + 1] - offsets[row_idx];\n+\t\t\tFlatVector::GetData<string_t>(vector)[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);\n+\t\t}\n+\t} else {\n+\t\t//! Check if last offset is higher than max uint32\n+\t\tif (((uint64_t *)array.buffers[1])[array.length] > NumericLimits<uint32_t>::Maximum()) { // LCOV_EXCL_START\n+\t\t\tthrow std::runtime_error(\"DuckDB does not support Blobs over 4GB\");\n+\t\t} // LCOV_EXCL_STOP\n+\t\tauto offsets = (uint64_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;\n+\t\tif (nested_offset != -1) {\n+\t\t\toffsets = (uint64_t *)array.buffers[1] + array.offset + nested_offset;\n+\t\t}\n+\t\tauto cdata = (char *)array.buffers[2];\n+\t\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n+\t\t\tif (FlatVector::IsNull(vector, row_idx)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tauto bptr = cdata + offsets[row_idx];\n+\t\t\tauto blob_len = offsets[row_idx + 1] - offsets[row_idx];\n+\t\t\tFlatVector::GetData<string_t>(vector)[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);\n+\t\t}\n+\t}\n+}\n+\n+void ArrowToDuckDBMapList(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n+                          unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,\n+                          pair<idx_t, idx_t> &arrow_convert_idx, uint32_t *offsets, ValidityMask *parent_mask) {\n+\tidx_t list_size = offsets[size] - offsets[0];\n+\tListVector::Reserve(vector, list_size);\n+\n+\tauto &child_vector = ListVector::GetEntry(vector);\n+\tauto list_data = FlatVector::GetData<list_entry_t>(vector);\n+\tauto cur_offset = 0;\n+\tfor (idx_t i = 0; i < size; i++) {\n+\t\tauto &le = list_data[i];\n+\t\tle.offset = cur_offset;\n+\t\tle.length = offsets[i + 1] - offsets[i];\n+\t\tcur_offset += le.length;\n+\t}\n+\tListVector::SetListSize(vector, list_size);\n+\tif (list_size == 0 && offsets[0] == 0) {\n+\t\tSetValidityMask(child_vector, array, scan_state, list_size, -1);\n+\t} else {\n+\t\tSetValidityMask(child_vector, array, scan_state, list_size, offsets[0]);\n+\t}\n+\n+\tauto &list_mask = FlatVector::Validity(vector);\n+\tif (parent_mask) {\n+\t\t//! Since this List is owned by a struct we must guarantee their validity map matches on Null\n+\t\tif (!parent_mask->AllValid()) {\n+\t\t\tfor (idx_t i = 0; i < size; i++) {\n+\t\t\t\tif (!parent_mask->RowIsValid(i)) {\n+\t\t\t\t\tlist_mask.SetInvalid(i);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif (list_size == 0 && offsets[0] == 0) {\n+\t\tColumnArrowToDuckDB(child_vector, array, scan_state, list_size, arrow_convert_data, col_idx, arrow_convert_idx,\n+\t\t                    -1);\n+\t} else {\n+\t\tColumnArrowToDuckDB(child_vector, array, scan_state, list_size, arrow_convert_data, col_idx, arrow_convert_idx,\n+\t\t                    offsets[0]);\n+\t}\n+}\n+template <class T>\n+static void SetVectorString(Vector &vector, idx_t size, char *cdata, T *offsets) {\n+\tauto strings = FlatVector::GetData<string_t>(vector);\n+\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n+\t\tif (FlatVector::IsNull(vector, row_idx)) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tauto cptr = cdata + offsets[row_idx];\n+\t\tauto str_len = offsets[row_idx + 1] - offsets[row_idx];\n+\t\tstrings[row_idx] = string_t(cptr, str_len);\n+\t}\n+}\n+\n+void DirectConversion(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, int64_t nested_offset) {\n+\tauto internal_type = GetTypeIdSize(vector.GetType().InternalType());\n+\tauto data_ptr = (data_ptr_t)array.buffers[1] + internal_type * (scan_state.chunk_offset + array.offset);\n+\tif (nested_offset != -1) {\n+\t\tdata_ptr = (data_ptr_t)array.buffers[1] + internal_type * (array.offset + nested_offset);\n+\t}\n+\tFlatVector::SetData(vector, data_ptr);\n+}\n+\n+template <class T>\n+void TimeConversion(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, int64_t nested_offset,\n+                    idx_t size, int64_t conversion) {\n+\tauto tgt_ptr = (dtime_t *)FlatVector::GetData(vector);\n+\tauto &validity_mask = FlatVector::Validity(vector);\n+\tauto src_ptr = (T *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n+\tif (nested_offset != -1) {\n+\t\tsrc_ptr = (T *)array.buffers[1] + nested_offset + array.offset;\n+\t}\n+\tfor (idx_t row = 0; row < size; row++) {\n+\t\tif (!validity_mask.RowIsValid(row)) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tif (!TryMultiplyOperator::Operation((int64_t)src_ptr[row], conversion, tgt_ptr[row].micros)) {\n+\t\t\tthrow ConversionException(\"Could not convert Time to Microsecond\");\n+\t\t}\n+\t}\n+}\n+\n+void TimestampTZConversion(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, int64_t nested_offset,\n+                           idx_t size, int64_t conversion) {\n+\tauto tgt_ptr = (timestamp_t *)FlatVector::GetData(vector);\n+\tauto &validity_mask = FlatVector::Validity(vector);\n+\tauto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n+\tif (nested_offset != -1) {\n+\t\tsrc_ptr = (int64_t *)array.buffers[1] + nested_offset + array.offset;\n+\t}\n+\tfor (idx_t row = 0; row < size; row++) {\n+\t\tif (!validity_mask.RowIsValid(row)) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tif (!TryMultiplyOperator::Operation(src_ptr[row], conversion, tgt_ptr[row].value)) {\n+\t\t\tthrow ConversionException(\"Could not convert TimestampTZ to Microsecond\");\n+\t\t}\n+\t}\n+}\n+\n+void IntervalConversionUs(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, int64_t nested_offset,\n+                          idx_t size, int64_t conversion) {\n+\tauto tgt_ptr = (interval_t *)FlatVector::GetData(vector);\n+\tauto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n+\tif (nested_offset != -1) {\n+\t\tsrc_ptr = (int64_t *)array.buffers[1] + nested_offset + array.offset;\n+\t}\n+\tfor (idx_t row = 0; row < size; row++) {\n+\t\ttgt_ptr[row].days = 0;\n+\t\ttgt_ptr[row].months = 0;\n+\t\tif (!TryMultiplyOperator::Operation(src_ptr[row], conversion, tgt_ptr[row].micros)) {\n+\t\t\tthrow ConversionException(\"Could not convert Interval to Microsecond\");\n+\t\t}\n+\t}\n+}\n+\n+void IntervalConversionMonths(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, int64_t nested_offset,\n+                              idx_t size) {\n+\tauto tgt_ptr = (interval_t *)FlatVector::GetData(vector);\n+\tauto src_ptr = (int32_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n+\tif (nested_offset != -1) {\n+\t\tsrc_ptr = (int32_t *)array.buffers[1] + nested_offset + array.offset;\n+\t}\n+\tfor (idx_t row = 0; row < size; row++) {\n+\t\ttgt_ptr[row].days = 0;\n+\t\ttgt_ptr[row].micros = 0;\n+\t\ttgt_ptr[row].months = src_ptr[row];\n+\t}\n+}\n+\n+void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n+                         std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,\n+                         std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset, ValidityMask *parent_mask) {\n+\tswitch (vector.GetType().id()) {\n+\tcase LogicalTypeId::SQLNULL:\n+\t\tvector.Reference(Value());\n+\t\tbreak;\n+\tcase LogicalTypeId::BOOLEAN: {\n+\t\t//! Arrow bit-packs boolean values\n+\t\t//! Lets first figure out where we are in the source array\n+\t\tauto src_ptr = (uint8_t *)array.buffers[1] + (scan_state.chunk_offset + array.offset) / 8;\n+\n+\t\tif (nested_offset != -1) {\n+\t\t\tsrc_ptr = (uint8_t *)array.buffers[1] + (nested_offset + array.offset) / 8;\n+\t\t}\n+\t\tauto tgt_ptr = (uint8_t *)FlatVector::GetData(vector);\n+\t\tint src_pos = 0;\n+\t\tidx_t cur_bit = scan_state.chunk_offset % 8;\n+\t\tif (nested_offset != -1) {\n+\t\t\tcur_bit = nested_offset % 8;\n+\t\t}\n+\t\tfor (idx_t row = 0; row < size; row++) {\n+\t\t\tif ((src_ptr[src_pos] & (1 << cur_bit)) == 0) {\n+\t\t\t\ttgt_ptr[row] = 0;\n+\t\t\t} else {\n+\t\t\t\ttgt_ptr[row] = 1;\n+\t\t\t}\n+\t\t\tcur_bit++;\n+\t\t\tif (cur_bit == 8) {\n+\t\t\t\tsrc_pos++;\n+\t\t\t\tcur_bit = 0;\n+\t\t\t}\n+\t\t}\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::TINYINT:\n+\tcase LogicalTypeId::SMALLINT:\n+\tcase LogicalTypeId::INTEGER:\n+\tcase LogicalTypeId::FLOAT:\n+\tcase LogicalTypeId::DOUBLE:\n+\tcase LogicalTypeId::UTINYINT:\n+\tcase LogicalTypeId::USMALLINT:\n+\tcase LogicalTypeId::UINTEGER:\n+\tcase LogicalTypeId::UBIGINT:\n+\tcase LogicalTypeId::BIGINT:\n+\tcase LogicalTypeId::HUGEINT:\n+\tcase LogicalTypeId::TIMESTAMP:\n+\tcase LogicalTypeId::TIMESTAMP_SEC:\n+\tcase LogicalTypeId::TIMESTAMP_MS:\n+\tcase LogicalTypeId::TIMESTAMP_NS: {\n+\t\tDirectConversion(vector, array, scan_state, nested_offset);\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::JSON:\n+\tcase LogicalTypeId::VARCHAR: {\n+\t\tauto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.first++];\n+\t\tauto cdata = (char *)array.buffers[2];\n+\t\tif (original_type.first == ArrowVariableSizeType::SUPER_SIZE) {\n+\t\t\tif (((uint64_t *)array.buffers[1])[array.length] > NumericLimits<uint32_t>::Maximum()) { // LCOV_EXCL_START\n+\t\t\t\tthrow std::runtime_error(\"DuckDB does not support Strings over 4GB\");\n+\t\t\t} // LCOV_EXCL_STOP\n+\t\t\tauto offsets = (uint64_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;\n+\t\t\tif (nested_offset != -1) {\n+\t\t\t\toffsets = (uint64_t *)array.buffers[1] + array.offset + nested_offset;\n+\t\t\t}\n+\t\t\tSetVectorString(vector, size, cdata, offsets);\n+\t\t} else {\n+\t\t\tauto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;\n+\t\t\tif (nested_offset != -1) {\n+\t\t\t\toffsets = (uint32_t *)array.buffers[1] + array.offset + nested_offset;\n+\t\t\t}\n+\t\t\tSetVectorString(vector, size, cdata, offsets);\n+\t\t}\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::DATE: {\n+\t\tauto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];\n+\t\tswitch (precision) {\n+\t\tcase ArrowDateTimeType::DAYS: {\n+\t\t\tDirectConversion(vector, array, scan_state, nested_offset);\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase ArrowDateTimeType::MILLISECONDS: {\n+\t\t\t//! convert date from nanoseconds to days\n+\t\t\tauto src_ptr = (uint64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n+\t\t\tif (nested_offset != -1) {\n+\t\t\t\tsrc_ptr = (uint64_t *)array.buffers[1] + nested_offset + array.offset;\n+\t\t\t}\n+\t\t\tauto tgt_ptr = (date_t *)FlatVector::GetData(vector);\n+\t\t\tfor (idx_t row = 0; row < size; row++) {\n+\t\t\t\ttgt_ptr[row] = date_t(int64_t(src_ptr[row]) / (1000 * 60 * 60 * 24));\n+\t\t\t}\n+\t\t\tbreak;\n+\t\t}\n+\t\tdefault:\n+\t\t\tthrow std::runtime_error(\"Unsupported precision for Date Type \");\n+\t\t}\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::TIME: {\n+\t\tauto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];\n+\t\tswitch (precision) {\n+\t\tcase ArrowDateTimeType::SECONDS: {\n+\t\t\tTimeConversion<int32_t>(vector, array, scan_state, nested_offset, size, 1000000);\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase ArrowDateTimeType::MILLISECONDS: {\n+\t\t\tTimeConversion<int32_t>(vector, array, scan_state, nested_offset, size, 1000);\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase ArrowDateTimeType::MICROSECONDS: {\n+\t\t\tTimeConversion<int64_t>(vector, array, scan_state, nested_offset, size, 1);\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase ArrowDateTimeType::NANOSECONDS: {\n+\t\t\tauto tgt_ptr = (dtime_t *)FlatVector::GetData(vector);\n+\t\t\tauto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n+\t\t\tif (nested_offset != -1) {\n+\t\t\t\tsrc_ptr = (int64_t *)array.buffers[1] + nested_offset + array.offset;\n+\t\t\t}\n+\t\t\tfor (idx_t row = 0; row < size; row++) {\n+\t\t\t\ttgt_ptr[row].micros = src_ptr[row] / 1000;\n+\t\t\t}\n+\t\t\tbreak;\n+\t\t}\n+\t\tdefault:\n+\t\t\tthrow std::runtime_error(\"Unsupported precision for Time Type \");\n+\t\t}\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::TIMESTAMP_TZ: {\n+\t\tauto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];\n+\t\tswitch (precision) {\n+\t\tcase ArrowDateTimeType::SECONDS: {\n+\t\t\tTimestampTZConversion(vector, array, scan_state, nested_offset, size, 1000000);\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase ArrowDateTimeType::MILLISECONDS: {\n+\t\t\tTimestampTZConversion(vector, array, scan_state, nested_offset, size, 1000);\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase ArrowDateTimeType::MICROSECONDS: {\n+\t\t\tDirectConversion(vector, array, scan_state, nested_offset);\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase ArrowDateTimeType::NANOSECONDS: {\n+\t\t\tauto tgt_ptr = (timestamp_t *)FlatVector::GetData(vector);\n+\t\t\tauto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n+\t\t\tif (nested_offset != -1) {\n+\t\t\t\tsrc_ptr = (int64_t *)array.buffers[1] + nested_offset + array.offset;\n+\t\t\t}\n+\t\t\tfor (idx_t row = 0; row < size; row++) {\n+\t\t\t\ttgt_ptr[row].value = src_ptr[row] / 1000;\n+\t\t\t}\n+\t\t\tbreak;\n+\t\t}\n+\t\tdefault:\n+\t\t\tthrow std::runtime_error(\"Unsupported precision for TimestampTZ Type \");\n+\t\t}\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::INTERVAL: {\n+\t\tauto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];\n+\t\tswitch (precision) {\n+\t\tcase ArrowDateTimeType::SECONDS: {\n+\t\t\tIntervalConversionUs(vector, array, scan_state, nested_offset, size, 1000000);\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase ArrowDateTimeType::DAYS:\n+\t\tcase ArrowDateTimeType::MILLISECONDS: {\n+\t\t\tIntervalConversionUs(vector, array, scan_state, nested_offset, size, 1000);\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase ArrowDateTimeType::MICROSECONDS: {\n+\t\t\tIntervalConversionUs(vector, array, scan_state, nested_offset, size, 1);\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase ArrowDateTimeType::NANOSECONDS: {\n+\t\t\tauto tgt_ptr = (interval_t *)FlatVector::GetData(vector);\n+\t\t\tauto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n+\t\t\tif (nested_offset != -1) {\n+\t\t\t\tsrc_ptr = (int64_t *)array.buffers[1] + nested_offset + array.offset;\n+\t\t\t}\n+\t\t\tfor (idx_t row = 0; row < size; row++) {\n+\t\t\t\ttgt_ptr[row].micros = src_ptr[row] / 1000;\n+\t\t\t\ttgt_ptr[row].days = 0;\n+\t\t\t\ttgt_ptr[row].months = 0;\n+\t\t\t}\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase ArrowDateTimeType::MONTHS: {\n+\t\t\tIntervalConversionMonths(vector, array, scan_state, nested_offset, size);\n+\t\t\tbreak;\n+\t\t}\n+\t\tdefault:\n+\t\t\tthrow std::runtime_error(\"Unsupported precision for Interval/Duration Type \");\n+\t\t}\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::DECIMAL: {\n+\t\tauto val_mask = FlatVector::Validity(vector);\n+\t\t//! We have to convert from INT128\n+\t\tauto src_ptr = (hugeint_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n+\t\tif (nested_offset != -1) {\n+\t\t\tsrc_ptr = (hugeint_t *)array.buffers[1] + nested_offset + array.offset;\n+\t\t}\n+\t\tswitch (vector.GetType().InternalType()) {\n+\t\tcase PhysicalType::INT16: {\n+\t\t\tauto tgt_ptr = (int16_t *)FlatVector::GetData(vector);\n+\t\t\tfor (idx_t row = 0; row < size; row++) {\n+\t\t\t\tif (val_mask.RowIsValid(row)) {\n+\t\t\t\t\tauto result = Hugeint::TryCast(src_ptr[row], tgt_ptr[row]);\n+\t\t\t\t\tD_ASSERT(result);\n+\t\t\t\t\t(void)result;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase PhysicalType::INT32: {\n+\t\t\tauto tgt_ptr = (int32_t *)FlatVector::GetData(vector);\n+\t\t\tfor (idx_t row = 0; row < size; row++) {\n+\t\t\t\tif (val_mask.RowIsValid(row)) {\n+\t\t\t\t\tauto result = Hugeint::TryCast(src_ptr[row], tgt_ptr[row]);\n+\t\t\t\t\tD_ASSERT(result);\n+\t\t\t\t\t(void)result;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase PhysicalType::INT64: {\n+\t\t\tauto tgt_ptr = (int64_t *)FlatVector::GetData(vector);\n+\t\t\tfor (idx_t row = 0; row < size; row++) {\n+\t\t\t\tif (val_mask.RowIsValid(row)) {\n+\t\t\t\t\tauto result = Hugeint::TryCast(src_ptr[row], tgt_ptr[row]);\n+\t\t\t\t\tD_ASSERT(result);\n+\t\t\t\t\t(void)result;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase PhysicalType::INT128: {\n+\t\t\tFlatVector::SetData(vector, (data_ptr_t)array.buffers[1] + GetTypeIdSize(vector.GetType().InternalType()) *\n+\t\t\t                                                               (scan_state.chunk_offset + array.offset));\n+\t\t\tbreak;\n+\t\t}\n+\t\tdefault:\n+\t\t\tthrow std::runtime_error(\"Unsupported physical type for Decimal: \" +\n+\t\t\t                         TypeIdToString(vector.GetType().InternalType()));\n+\t\t}\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::BLOB: {\n+\t\tArrowToDuckDBBlob(vector, array, scan_state, size, arrow_convert_data, col_idx, arrow_convert_idx,\n+\t\t                  nested_offset);\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::LIST: {\n+\t\tArrowToDuckDBList(vector, array, scan_state, size, arrow_convert_data, col_idx, arrow_convert_idx,\n+\t\t                  nested_offset, parent_mask);\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::MAP: {\n+\t\t//! Since this is a map we skip first child, because its a struct\n+\t\tauto &struct_arrow = *array.children[0];\n+\t\tauto &child_entries = StructVector::GetEntries(vector);\n+\t\tD_ASSERT(child_entries.size() == 2);\n+\t\tauto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;\n+\t\tif (nested_offset != -1) {\n+\t\t\toffsets = (uint32_t *)array.buffers[1] + nested_offset;\n+\t\t}\n+\t\tauto &struct_validity_mask = FlatVector::Validity(vector);\n+\t\t//! Fill the children\n+\t\tfor (idx_t type_idx = 0; type_idx < (idx_t)struct_arrow.n_children; type_idx++) {\n+\t\t\tArrowToDuckDBMapList(*child_entries[type_idx], *struct_arrow.children[type_idx], scan_state, size,\n+\t\t\t                     arrow_convert_data, col_idx, arrow_convert_idx, offsets, &struct_validity_mask);\n+\t\t}\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::STRUCT: {\n+\t\t//! Fill the children\n+\t\tauto &child_entries = StructVector::GetEntries(vector);\n+\t\tauto &struct_validity_mask = FlatVector::Validity(vector);\n+\t\tfor (idx_t type_idx = 0; type_idx < (idx_t)array.n_children; type_idx++) {\n+\t\t\tSetValidityMask(*child_entries[type_idx], *array.children[type_idx], scan_state, size, nested_offset);\n+\t\t\tif (!struct_validity_mask.AllValid()) {\n+\t\t\t\tauto &child_validity_mark = FlatVector::Validity(*child_entries[type_idx]);\n+\t\t\t\tfor (idx_t i = 0; i < size; i++) {\n+\t\t\t\t\tif (!struct_validity_mask.RowIsValid(i)) {\n+\t\t\t\t\t\tchild_validity_mark.SetInvalid(i);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tColumnArrowToDuckDB(*child_entries[type_idx], *array.children[type_idx], scan_state, size,\n+\t\t\t                    arrow_convert_data, col_idx, arrow_convert_idx, nested_offset, &struct_validity_mask);\n+\t\t}\n+\t\tbreak;\n+\t}\n+\tdefault:\n+\t\tthrow std::runtime_error(\"Unsupported type \" + vector.GetType().ToString());\n+\t}\n+}\n+\n+template <class T>\n+static void SetSelectionVectorLoop(SelectionVector &sel, data_ptr_t indices_p, idx_t size) {\n+\tauto indices = (T *)indices_p;\n+\tfor (idx_t row = 0; row < size; row++) {\n+\t\tsel.set_index(row, indices[row]);\n+\t}\n+}\n+\n+template <class T>\n+static void SetSelectionVectorLoopWithChecks(SelectionVector &sel, data_ptr_t indices_p, idx_t size) {\n+\n+\tauto indices = (T *)indices_p;\n+\tfor (idx_t row = 0; row < size; row++) {\n+\t\tif (indices[row] > NumericLimits<uint32_t>::Maximum()) {\n+\t\t\tthrow std::runtime_error(\"DuckDB only supports indices that fit on an uint32\");\n+\t\t}\n+\t\tsel.set_index(row, indices[row]);\n+\t}\n+}\n+\n+template <class T>\n+static void SetMaskedSelectionVectorLoop(SelectionVector &sel, data_ptr_t indices_p, idx_t size, ValidityMask &mask,\n+                                         idx_t last_element_pos) {\n+\tauto indices = (T *)indices_p;\n+\tfor (idx_t row = 0; row < size; row++) {\n+\t\tif (mask.RowIsValid(row)) {\n+\t\t\tsel.set_index(row, indices[row]);\n+\t\t} else {\n+\t\t\t//! Need to point out to last element\n+\t\t\tsel.set_index(row, last_element_pos);\n+\t\t}\n+\t}\n+}\n+\n+void SetSelectionVector(SelectionVector &sel, data_ptr_t indices_p, LogicalType &logical_type, idx_t size,\n+                        ValidityMask *mask = nullptr, idx_t last_element_pos = 0) {\n+\tsel.Initialize(size);\n+\n+\tif (mask) {\n+\t\tswitch (logical_type.id()) {\n+\t\tcase LogicalTypeId::UTINYINT:\n+\t\t\tSetMaskedSelectionVectorLoop<uint8_t>(sel, indices_p, size, *mask, last_element_pos);\n+\t\t\tbreak;\n+\t\tcase LogicalTypeId::TINYINT:\n+\t\t\tSetMaskedSelectionVectorLoop<int8_t>(sel, indices_p, size, *mask, last_element_pos);\n+\t\t\tbreak;\n+\t\tcase LogicalTypeId::USMALLINT:\n+\t\t\tSetMaskedSelectionVectorLoop<uint16_t>(sel, indices_p, size, *mask, last_element_pos);\n+\t\t\tbreak;\n+\t\tcase LogicalTypeId::SMALLINT:\n+\t\t\tSetMaskedSelectionVectorLoop<int16_t>(sel, indices_p, size, *mask, last_element_pos);\n+\t\t\tbreak;\n+\t\tcase LogicalTypeId::UINTEGER:\n+\t\t\tif (last_element_pos > NumericLimits<uint32_t>::Maximum()) {\n+\t\t\t\t//! Its guaranteed that our indices will point to the last element, so just throw an error\n+\t\t\t\tthrow std::runtime_error(\"DuckDB only supports indices that fit on an uint32\");\n+\t\t\t}\n+\t\t\tSetMaskedSelectionVectorLoop<uint32_t>(sel, indices_p, size, *mask, last_element_pos);\n+\t\t\tbreak;\n+\t\tcase LogicalTypeId::INTEGER:\n+\t\t\tSetMaskedSelectionVectorLoop<int32_t>(sel, indices_p, size, *mask, last_element_pos);\n+\t\t\tbreak;\n+\t\tcase LogicalTypeId::UBIGINT:\n+\t\t\tif (last_element_pos > NumericLimits<uint32_t>::Maximum()) {\n+\t\t\t\t//! Its guaranteed that our indices will point to the last element, so just throw an error\n+\t\t\t\tthrow std::runtime_error(\"DuckDB only supports indices that fit on an uint32\");\n+\t\t\t}\n+\t\t\tSetMaskedSelectionVectorLoop<uint64_t>(sel, indices_p, size, *mask, last_element_pos);\n+\t\t\tbreak;\n+\t\tcase LogicalTypeId::BIGINT:\n+\t\t\tif (last_element_pos > NumericLimits<uint32_t>::Maximum()) {\n+\t\t\t\t//! Its guaranteed that our indices will point to the last element, so just throw an error\n+\t\t\t\tthrow std::runtime_error(\"DuckDB only supports indices that fit on an uint32\");\n+\t\t\t}\n+\t\t\tSetMaskedSelectionVectorLoop<int64_t>(sel, indices_p, size, *mask, last_element_pos);\n+\t\t\tbreak;\n+\n+\t\tdefault:\n+\t\t\tthrow std::runtime_error(\"(Arrow) Unsupported type for selection vectors \" + logical_type.ToString());\n+\t\t}\n+\n+\t} else {\n+\t\tswitch (logical_type.id()) {\n+\t\tcase LogicalTypeId::UTINYINT:\n+\t\t\tSetSelectionVectorLoop<uint8_t>(sel, indices_p, size);\n+\t\t\tbreak;\n+\t\tcase LogicalTypeId::TINYINT:\n+\t\t\tSetSelectionVectorLoop<int8_t>(sel, indices_p, size);\n+\t\t\tbreak;\n+\t\tcase LogicalTypeId::USMALLINT:\n+\t\t\tSetSelectionVectorLoop<uint16_t>(sel, indices_p, size);\n+\t\t\tbreak;\n+\t\tcase LogicalTypeId::SMALLINT:\n+\t\t\tSetSelectionVectorLoop<int16_t>(sel, indices_p, size);\n+\t\t\tbreak;\n+\t\tcase LogicalTypeId::UINTEGER:\n+\t\t\tSetSelectionVectorLoop<uint32_t>(sel, indices_p, size);\n+\t\t\tbreak;\n+\t\tcase LogicalTypeId::INTEGER:\n+\t\t\tSetSelectionVectorLoop<int32_t>(sel, indices_p, size);\n+\t\t\tbreak;\n+\t\tcase LogicalTypeId::UBIGINT:\n+\t\t\tif (last_element_pos > NumericLimits<uint32_t>::Maximum()) {\n+\t\t\t\t//! We need to check if our indexes fit in a uint32_t\n+\t\t\t\tSetSelectionVectorLoopWithChecks<uint64_t>(sel, indices_p, size);\n+\t\t\t} else {\n+\t\t\t\tSetSelectionVectorLoop<uint64_t>(sel, indices_p, size);\n+\t\t\t}\n+\t\t\tbreak;\n+\t\tcase LogicalTypeId::BIGINT:\n+\t\t\tif (last_element_pos > NumericLimits<uint32_t>::Maximum()) {\n+\t\t\t\t//! We need to check if our indexes fit in a uint32_t\n+\t\t\t\tSetSelectionVectorLoopWithChecks<int64_t>(sel, indices_p, size);\n+\t\t\t} else {\n+\t\t\t\tSetSelectionVectorLoop<int64_t>(sel, indices_p, size);\n+\t\t\t}\n+\t\t\tbreak;\n+\t\tdefault:\n+\t\t\tthrow std::runtime_error(\"(Arrow) Unsupported type for selection vectors \" + logical_type.ToString());\n+\t\t}\n+\t}\n+}\n+\n+void ColumnArrowToDuckDBDictionary(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n+                                   std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,\n+                                   idx_t col_idx, std::pair<idx_t, idx_t> &arrow_convert_idx) {\n+\tSelectionVector sel;\n+\tauto &dict_vectors = scan_state.arrow_dictionary_vectors;\n+\tif (dict_vectors.find(col_idx) == dict_vectors.end()) {\n+\t\t//! We need to set the dictionary data for this column\n+\t\tauto base_vector = make_unique<Vector>(vector.GetType(), array.dictionary->length);\n+\t\tSetValidityMask(*base_vector, *array.dictionary, scan_state, array.dictionary->length, 0, array.null_count > 0);\n+\t\tColumnArrowToDuckDB(*base_vector, *array.dictionary, scan_state, array.dictionary->length, arrow_convert_data,\n+\t\t                    col_idx, arrow_convert_idx);\n+\t\tdict_vectors[col_idx] = move(base_vector);\n+\t}\n+\tauto dictionary_type = arrow_convert_data[col_idx]->dictionary_type;\n+\t//! Get Pointer to Indices of Dictionary\n+\tauto indices = (data_ptr_t)array.buffers[1] +\n+\t               GetTypeIdSize(dictionary_type.InternalType()) * (scan_state.chunk_offset + array.offset);\n+\tif (array.null_count > 0) {\n+\t\tValidityMask indices_validity;\n+\t\tGetValidityMask(indices_validity, array, scan_state, size);\n+\t\tSetSelectionVector(sel, indices, dictionary_type, size, &indices_validity, array.dictionary->length);\n+\t} else {\n+\t\tSetSelectionVector(sel, indices, dictionary_type, size);\n+\t}\n+\tvector.Slice(*dict_vectors[col_idx], sel, size);\n+}\n+\n+void ArrowTableFunction::ArrowToDuckDB(ArrowScanLocalState &scan_state,\n+                                       unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,\n+                                       DataChunk &output, idx_t start) {\n+\tfor (idx_t idx = 0; idx < output.ColumnCount(); idx++) {\n+\t\tauto col_idx = scan_state.column_ids[idx];\n+\t\tstd::pair<idx_t, idx_t> arrow_convert_idx {0, 0};\n+\t\tauto &array = *scan_state.chunk->arrow_array.children[idx];\n+\t\tif (!array.release) {\n+\t\t\tthrow InvalidInputException(\"arrow_scan: released array passed\");\n+\t\t}\n+\t\tif (array.length != scan_state.chunk->arrow_array.length) {\n+\t\t\tthrow InvalidInputException(\"arrow_scan: array length mismatch\");\n+\t\t}\n+\t\toutput.data[idx].GetBuffer()->SetAuxiliaryData(make_unique<ArrowAuxiliaryData>(scan_state.chunk));\n+\t\tif (array.dictionary) {\n+\t\t\tColumnArrowToDuckDBDictionary(output.data[idx], array, scan_state, output.size(), arrow_convert_data,\n+\t\t\t                              col_idx, arrow_convert_idx);\n+\t\t} else {\n+\t\t\tSetValidityMask(output.data[idx], array, scan_state, output.size(), -1);\n+\t\t\tColumnArrowToDuckDB(output.data[idx], array, scan_state, output.size(), arrow_convert_data, col_idx,\n+\t\t\t                    arrow_convert_idx);\n+\t\t}\n+\t}\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/function/table/checkpoint.cpp b/src/function/table/checkpoint.cpp\nindex 6b23d0638be6..7c022f2640db 100644\n--- a/src/function/table/checkpoint.cpp\n+++ b/src/function/table/checkpoint.cpp\n@@ -13,8 +13,7 @@ static unique_ptr<FunctionData> CheckpointBind(ClientContext &context, TableFunc\n }\n \n template <bool FORCE>\n-static void TemplatedCheckpointFunction(ClientContext &context, const FunctionData *bind_data_p,\n-                                        FunctionOperatorData *operator_state, DataChunk &output) {\n+static void TemplatedCheckpointFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n \tauto &transaction_manager = TransactionManager::Get(context);\n \ttransaction_manager.Checkpoint(context, FORCE);\n }\ndiff --git a/src/function/table/glob.cpp b/src/function/table/glob.cpp\nindex 36651d06acf4..8f666db76f2e 100644\n--- a/src/function/table/glob.cpp\n+++ b/src/function/table/glob.cpp\n@@ -24,23 +24,20 @@ static unique_ptr<FunctionData> GlobFunctionBind(ClientContext &context, TableFu\n \treturn move(result);\n }\n \n-struct GlobFunctionState : public FunctionOperatorData {\n+struct GlobFunctionState : public GlobalTableFunctionState {\n \tGlobFunctionState() : current_idx(0) {\n \t}\n \n \tidx_t current_idx;\n };\n \n-static unique_ptr<FunctionOperatorData> GlobFunctionInit(ClientContext &context, const FunctionData *bind_data,\n-                                                         const vector<column_t> &column_ids,\n-                                                         TableFilterCollection *filters) {\n+static unique_ptr<GlobalTableFunctionState> GlobFunctionInit(ClientContext &context, TableFunctionInitInput &input) {\n \treturn make_unique<GlobFunctionState>();\n }\n \n-static void GlobFunction(ClientContext &context, const FunctionData *bind_data_p, FunctionOperatorData *state_p,\n-                         DataChunk &output) {\n-\tauto &bind_data = (GlobFunctionBindData &)*bind_data_p;\n-\tauto &state = (GlobFunctionState &)*state_p;\n+static void GlobFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &bind_data = (GlobFunctionBindData &)*data_p.bind_data;\n+\tauto &state = (GlobFunctionState &)*data_p.global_state;\n \n \tidx_t count = 0;\n \tidx_t next_idx = MinValue<idx_t>(state.current_idx + STANDARD_VECTOR_SIZE, bind_data.files.size());\ndiff --git a/src/function/table/pragma_detailed_profiling_output.cpp b/src/function/table/pragma_detailed_profiling_output.cpp\nindex 61ddef8a18ee..58079a97bcb7 100644\n--- a/src/function/table/pragma_detailed_profiling_output.cpp\n+++ b/src/function/table/pragma_detailed_profiling_output.cpp\n@@ -9,7 +9,7 @@\n \n namespace duckdb {\n \n-struct PragmaDetailedProfilingOutputOperatorData : public FunctionOperatorData {\n+struct PragmaDetailedProfilingOutputOperatorData : public GlobalTableFunctionState {\n \texplicit PragmaDetailedProfilingOutputOperatorData() : chunk_index(0), initialized(false) {\n \t}\n \tidx_t chunk_index;\n@@ -56,10 +56,8 @@ static unique_ptr<FunctionData> PragmaDetailedProfilingOutputBind(ClientContext\n \treturn make_unique<PragmaDetailedProfilingOutputData>(return_types);\n }\n \n-unique_ptr<FunctionOperatorData> PragmaDetailedProfilingOutputInit(ClientContext &context,\n-                                                                   const FunctionData *bind_data,\n-                                                                   const vector<column_t> &column_ids,\n-                                                                   TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> PragmaDetailedProfilingOutputInit(ClientContext &context,\n+                                                                       TableFunctionInitInput &input) {\n \treturn make_unique<PragmaDetailedProfilingOutputOperatorData>();\n }\n \n@@ -106,10 +104,10 @@ static void ExtractFunctions(ChunkCollection &collection, ExpressionInfo &info,\n \t}\n }\n \n-static void PragmaDetailedProfilingOutputFunction(ClientContext &context, const FunctionData *bind_data_p,\n-                                                  FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &state = (PragmaDetailedProfilingOutputOperatorData &)*operator_state;\n-\tauto &data = (PragmaDetailedProfilingOutputData &)*bind_data_p;\n+static void PragmaDetailedProfilingOutputFunction(ClientContext &context, TableFunctionInput &data_p,\n+                                                  DataChunk &output) {\n+\tauto &state = (PragmaDetailedProfilingOutputOperatorData &)*data_p.global_state;\n+\tauto &data = (PragmaDetailedProfilingOutputData &)*data_p.bind_data;\n \n \tif (!state.initialized) {\n \t\t// create a ChunkCollection\ndiff --git a/src/function/table/pragma_last_profiling_output.cpp b/src/function/table/pragma_last_profiling_output.cpp\nindex 5078b47320e3..02cab74bb1c4 100644\n--- a/src/function/table/pragma_last_profiling_output.cpp\n+++ b/src/function/table/pragma_last_profiling_output.cpp\n@@ -9,7 +9,7 @@\n \n namespace duckdb {\n \n-struct PragmaLastProfilingOutputOperatorData : public FunctionOperatorData {\n+struct PragmaLastProfilingOutputOperatorData : public GlobalTableFunctionState {\n \tPragmaLastProfilingOutputOperatorData() : chunk_index(0), initialized(false) {\n \t}\n \tidx_t chunk_index;\n@@ -53,16 +53,14 @@ static void SetValue(DataChunk &output, int index, int op_id, string name, doubl\n \toutput.SetValue(4, index, move(description));\n }\n \n-unique_ptr<FunctionOperatorData> PragmaLastProfilingOutputInit(ClientContext &context, const FunctionData *bind_data,\n-                                                               const vector<column_t> &column_ids,\n-                                                               TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> PragmaLastProfilingOutputInit(ClientContext &context,\n+                                                                   TableFunctionInitInput &input) {\n \treturn make_unique<PragmaLastProfilingOutputOperatorData>();\n }\n \n-static void PragmaLastProfilingOutputFunction(ClientContext &context, const FunctionData *bind_data_p,\n-                                              FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &state = (PragmaLastProfilingOutputOperatorData &)*operator_state;\n-\tauto &data = (PragmaLastProfilingOutputData &)*bind_data_p;\n+static void PragmaLastProfilingOutputFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &state = (PragmaLastProfilingOutputOperatorData &)*data_p.global_state;\n+\tauto &data = (PragmaLastProfilingOutputData &)*data_p.bind_data;\n \tif (!state.initialized) {\n \t\t// create a ChunkCollection\n \t\tauto collection = make_unique<ChunkCollection>();\ndiff --git a/src/function/table/range.cpp b/src/function/table/range.cpp\nindex ed279ce28fc9..c477782bad80 100644\n--- a/src/function/table/range.cpp\n+++ b/src/function/table/range.cpp\n@@ -65,23 +65,20 @@ static unique_ptr<FunctionData> RangeFunctionBind(ClientContext &context, TableF\n \treturn move(result);\n }\n \n-struct RangeFunctionState : public FunctionOperatorData {\n+struct RangeFunctionState : public GlobalTableFunctionState {\n \tRangeFunctionState() : current_idx(0) {\n \t}\n \n \tint64_t current_idx;\n };\n \n-static unique_ptr<FunctionOperatorData> RangeFunctionInit(ClientContext &context, const FunctionData *bind_data,\n-                                                          const vector<column_t> &column_ids,\n-                                                          TableFilterCollection *filters) {\n+static unique_ptr<GlobalTableFunctionState> RangeFunctionInit(ClientContext &context, TableFunctionInitInput &input) {\n \treturn make_unique<RangeFunctionState>();\n }\n \n-static void RangeFunction(ClientContext &context, const FunctionData *bind_data_p, FunctionOperatorData *state_p,\n-                          DataChunk &output) {\n-\tauto &bind_data = (RangeFunctionBindData &)*bind_data_p;\n-\tauto &state = (RangeFunctionState &)*state_p;\n+static void RangeFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &bind_data = (RangeFunctionBindData &)*data_p.bind_data;\n+\tauto &state = (RangeFunctionState &)*data_p.global_state;\n \n \tauto increment = bind_data.increment;\n \tauto end = bind_data.end;\n@@ -187,7 +184,7 @@ static unique_ptr<FunctionData> RangeDateTimeBind(ClientContext &context, TableF\n \treturn move(result);\n }\n \n-struct RangeDateTimeState : public FunctionOperatorData {\n+struct RangeDateTimeState : public GlobalTableFunctionState {\n \texplicit RangeDateTimeState(timestamp_t start_p) : current_state(start_p) {\n \t}\n \n@@ -195,17 +192,14 @@ struct RangeDateTimeState : public FunctionOperatorData {\n \tbool finished = false;\n };\n \n-static unique_ptr<FunctionOperatorData> RangeDateTimeInit(ClientContext &context, const FunctionData *bind_data_p,\n-                                                          const vector<column_t> &column_ids,\n-                                                          TableFilterCollection *filters) {\n-\tauto &bind_data = (RangeDateTimeBindData &)*bind_data_p;\n+static unique_ptr<GlobalTableFunctionState> RangeDateTimeInit(ClientContext &context, TableFunctionInitInput &input) {\n+\tauto &bind_data = (RangeDateTimeBindData &)*input.bind_data;\n \treturn make_unique<RangeDateTimeState>(bind_data.start);\n }\n \n-static void RangeDateTimeFunction(ClientContext &context, const FunctionData *bind_data_p,\n-                                  FunctionOperatorData *state_p, DataChunk &output) {\n-\tauto &bind_data = (RangeDateTimeBindData &)*bind_data_p;\n-\tauto &state = (RangeDateTimeState &)*state_p;\n+static void RangeDateTimeFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &bind_data = (RangeDateTimeBindData &)*data_p.bind_data;\n+\tauto &state = (RangeDateTimeState &)*data_p.global_state;\n \tif (state.finished) {\n \t\treturn;\n \t}\n@@ -230,29 +224,29 @@ static void RangeDateTimeFunction(ClientContext &context, const FunctionData *bi\n void RangeTableFunction::RegisterFunction(BuiltinFunctions &set) {\n \tTableFunctionSet range(\"range\");\n \n+\tTableFunction range_function({LogicalType::BIGINT}, RangeFunction, RangeFunctionBind<false>, RangeFunctionInit);\n+\trange_function.cardinality = RangeCardinality;\n+\n \t// single argument range: (end) - implicit start = 0 and increment = 1\n-\trange.AddFunction(TableFunction({LogicalType::BIGINT}, RangeFunction, RangeFunctionBind<false>, RangeFunctionInit,\n-\t                                nullptr, nullptr, nullptr, RangeCardinality));\n+\trange.AddFunction(range_function);\n \t// two arguments range: (start, end) - implicit increment = 1\n-\trange.AddFunction(TableFunction({LogicalType::BIGINT, LogicalType::BIGINT}, RangeFunction, RangeFunctionBind<false>,\n-\t                                RangeFunctionInit, nullptr, nullptr, nullptr, RangeCardinality));\n+\trange_function.arguments = {LogicalType::BIGINT, LogicalType::BIGINT};\n+\trange.AddFunction(range_function);\n \t// three arguments range: (start, end, increment)\n-\trange.AddFunction(TableFunction({LogicalType::BIGINT, LogicalType::BIGINT, LogicalType::BIGINT}, RangeFunction,\n-\t                                RangeFunctionBind<false>, RangeFunctionInit, nullptr, nullptr, nullptr,\n-\t                                RangeCardinality));\n+\trange_function.arguments = {LogicalType::BIGINT, LogicalType::BIGINT, LogicalType::BIGINT};\n+\trange.AddFunction(range_function);\n \trange.AddFunction(TableFunction({LogicalType::TIMESTAMP, LogicalType::TIMESTAMP, LogicalType::INTERVAL},\n \t                                RangeDateTimeFunction, RangeDateTimeBind<false>, RangeDateTimeInit));\n \tset.AddFunction(range);\n \t// generate_series: similar to range, but inclusive instead of exclusive bounds on the RHS\n \tTableFunctionSet generate_series(\"generate_series\");\n-\tgenerate_series.AddFunction(TableFunction({LogicalType::BIGINT}, RangeFunction, RangeFunctionBind<true>,\n-\t                                          RangeFunctionInit, nullptr, nullptr, nullptr, RangeCardinality));\n-\tgenerate_series.AddFunction(TableFunction({LogicalType::BIGINT, LogicalType::BIGINT}, RangeFunction,\n-\t                                          RangeFunctionBind<true>, RangeFunctionInit, nullptr, nullptr, nullptr,\n-\t                                          RangeCardinality));\n-\tgenerate_series.AddFunction(TableFunction({LogicalType::BIGINT, LogicalType::BIGINT, LogicalType::BIGINT},\n-\t                                          RangeFunction, RangeFunctionBind<true>, RangeFunctionInit, nullptr,\n-\t                                          nullptr, nullptr, RangeCardinality));\n+\trange_function.bind = RangeFunctionBind<true>;\n+\trange_function.arguments = {LogicalType::BIGINT};\n+\tgenerate_series.AddFunction(range_function);\n+\trange_function.arguments = {LogicalType::BIGINT, LogicalType::BIGINT};\n+\tgenerate_series.AddFunction(range_function);\n+\trange_function.arguments = {LogicalType::BIGINT, LogicalType::BIGINT, LogicalType::BIGINT};\n+\tgenerate_series.AddFunction(range_function);\n \tgenerate_series.AddFunction(TableFunction({LogicalType::TIMESTAMP, LogicalType::TIMESTAMP, LogicalType::INTERVAL},\n \t                                          RangeDateTimeFunction, RangeDateTimeBind<true>, RangeDateTimeInit));\n \tset.AddFunction(generate_series);\ndiff --git a/src/function/table/read_csv.cpp b/src/function/table/read_csv.cpp\nindex cda33940c66f..4906fff3f4e7 100644\n--- a/src/function/table/read_csv.cpp\n+++ b/src/function/table/read_csv.cpp\n@@ -87,17 +87,19 @@ static unique_ptr<FunctionData> ReadCSVBind(ClientContext &context, TableFunctio\n \treturn move(result);\n }\n \n-struct ReadCSVOperatorData : public FunctionOperatorData {\n+struct ReadCSVOperatorData : public GlobalTableFunctionState {\n \t//! The CSV reader\n \tunique_ptr<BufferedCSVReader> csv_reader;\n \t//! The index of the next file to read (i.e. current file + 1)\n \tidx_t file_index;\n+\t//! Total File Size\n+\tidx_t file_size;\n+\t//! How many bytes were read up to this point\n+\tatomic<idx_t> bytes_read;\n };\n \n-static unique_ptr<FunctionOperatorData> ReadCSVInit(ClientContext &context, const FunctionData *bind_data_p,\n-                                                    const vector<column_t> &column_ids,\n-                                                    TableFilterCollection *filters) {\n-\tauto &bind_data = (ReadCSVData &)*bind_data_p;\n+static unique_ptr<GlobalTableFunctionState> ReadCSVInit(ClientContext &context, TableFunctionInitInput &input) {\n+\tauto &bind_data = (ReadCSVData &)*input.bind_data;\n \tauto result = make_unique<ReadCSVOperatorData>();\n \tif (bind_data.initial_reader) {\n \t\tresult->csv_reader = move(bind_data.initial_reader);\n@@ -105,8 +107,7 @@ static unique_ptr<FunctionOperatorData> ReadCSVInit(ClientContext &context, cons\n \t\tbind_data.options.file_path = bind_data.files[0];\n \t\tresult->csv_reader = make_unique<BufferedCSVReader>(context, bind_data.options, bind_data.sql_types);\n \t}\n-\tbind_data.bytes_read = 0;\n-\tbind_data.file_size = result->csv_reader->GetFileSize();\n+\tresult->file_size = result->csv_reader->GetFileSize();\n \tresult->file_index = 1;\n \treturn move(result);\n }\n@@ -117,13 +118,12 @@ static unique_ptr<FunctionData> ReadCSVAutoBind(ClientContext &context, TableFun\n \treturn ReadCSVBind(context, input, return_types, names);\n }\n \n-static void ReadCSVFunction(ClientContext &context, const FunctionData *bind_data_p,\n-                            FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &bind_data = (ReadCSVData &)*bind_data_p;\n-\tauto &data = (ReadCSVOperatorData &)*operator_state;\n+static void ReadCSVFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &bind_data = (ReadCSVData &)*data_p.bind_data;\n+\tauto &data = (ReadCSVOperatorData &)*data_p.global_state;\n \tdo {\n \t\tdata.csv_reader->ParseCSV(output);\n-\t\tbind_data.bytes_read = data.csv_reader->bytes_in_chunk;\n+\t\tdata.bytes_read = data.csv_reader->bytes_in_chunk;\n \t\tif (output.size() == 0 && data.file_index < bind_data.files.size()) {\n \t\t\t// exhausted this file, but we have more files we can read\n \t\t\t// open the next file and increment the counter\n@@ -164,12 +164,13 @@ static void ReadCSVAddNamedParameters(TableFunction &table_function) {\n \ttable_function.named_parameters[\"maximum_line_size\"] = LogicalType::VARCHAR;\n }\n \n-double CSVReaderProgress(ClientContext &context, const FunctionData *bind_data_p) {\n-\tauto &bind_data = (ReadCSVData &)*bind_data_p;\n-\tif (bind_data.file_size == 0) {\n+double CSVReaderProgress(ClientContext &context, const FunctionData *bind_data_p,\n+                         const GlobalTableFunctionState *global_state) {\n+\tauto &data = (const ReadCSVOperatorData &)*global_state;\n+\tif (data.file_size == 0) {\n \t\treturn 100;\n \t}\n-\tauto percentage = (bind_data.bytes_read * 100.0) / bind_data.file_size;\n+\tauto percentage = (data.bytes_read * 100.0) / data.file_size;\n \treturn percentage;\n }\n \ndiff --git a/src/function/table/repeat.cpp b/src/function/table/repeat.cpp\nindex 0b57aa6fe6a2..3a5e57feaae0 100644\n--- a/src/function/table/repeat.cpp\n+++ b/src/function/table/repeat.cpp\n@@ -11,7 +11,7 @@ struct RepeatFunctionData : public TableFunctionData {\n \tidx_t target_count;\n };\n \n-struct RepeatOperatorData : public FunctionOperatorData {\n+struct RepeatOperatorData : public GlobalTableFunctionState {\n \tRepeatOperatorData() : current_count(0) {\n \t}\n \tidx_t current_count;\n@@ -26,15 +26,13 @@ static unique_ptr<FunctionData> RepeatBind(ClientContext &context, TableFunction\n \treturn make_unique<RepeatFunctionData>(inputs[0], inputs[1].GetValue<int64_t>());\n }\n \n-static unique_ptr<FunctionOperatorData> RepeatInit(ClientContext &context, const FunctionData *bind_data,\n-                                                   const vector<column_t> &column_ids, TableFilterCollection *filters) {\n+static unique_ptr<GlobalTableFunctionState> RepeatInit(ClientContext &context, TableFunctionInitInput &input) {\n \treturn make_unique<RepeatOperatorData>();\n }\n \n-static void RepeatFunction(ClientContext &context, const FunctionData *bind_data_p,\n-                           FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &bind_data = (RepeatFunctionData &)*bind_data_p;\n-\tauto &state = (RepeatOperatorData &)*operator_state;\n+static void RepeatFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &bind_data = (const RepeatFunctionData &)*data_p.bind_data;\n+\tauto &state = (RepeatOperatorData &)*data_p.global_state;\n \n \tidx_t remaining = MinValue<idx_t>(bind_data.target_count - state.current_count, STANDARD_VECTOR_SIZE);\n \toutput.data[0].Reference(bind_data.value);\n@@ -43,13 +41,13 @@ static void RepeatFunction(ClientContext &context, const FunctionData *bind_data\n }\n \n static unique_ptr<NodeStatistics> RepeatCardinality(ClientContext &context, const FunctionData *bind_data_p) {\n-\tauto &bind_data = (RepeatFunctionData &)*bind_data_p;\n+\tauto &bind_data = (const RepeatFunctionData &)*bind_data_p;\n \treturn make_unique<NodeStatistics>(bind_data.target_count, bind_data.target_count);\n }\n \n void RepeatTableFunction::RegisterFunction(BuiltinFunctions &set) {\n-\tTableFunction repeat(\"repeat\", {LogicalType::ANY, LogicalType::BIGINT}, RepeatFunction, RepeatBind, RepeatInit,\n-\t                     nullptr, nullptr, nullptr, RepeatCardinality);\n+\tTableFunction repeat(\"repeat\", {LogicalType::ANY, LogicalType::BIGINT}, RepeatFunction, RepeatBind, RepeatInit);\n+\trepeat.cardinality = RepeatCardinality;\n \tset.AddFunction(repeat);\n }\n \ndiff --git a/src/function/table/summary.cpp b/src/function/table/summary.cpp\nindex e24ba646bde5..cb1db1380526 100644\n--- a/src/function/table/summary.cpp\n+++ b/src/function/table/summary.cpp\n@@ -21,8 +21,8 @@ static unique_ptr<FunctionData> SummaryFunctionBind(ClientContext &context, Tabl\n \treturn make_unique<TableFunctionData>();\n }\n \n-static OperatorResultType SummaryFunction(ClientContext &context, const FunctionData *bind_data_p,\n-                                          FunctionOperatorData *state_p, DataChunk &input, DataChunk &output) {\n+static OperatorResultType SummaryFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &input,\n+                                          DataChunk &output) {\n \toutput.SetCardinality(input.size());\n \n \tfor (idx_t row_idx = 0; row_idx < input.size(); row_idx++) {\ndiff --git a/src/function/table/system/duckdb_columns.cpp b/src/function/table/system/duckdb_columns.cpp\nindex 434279e4bbd3..50a7f74d7fd8 100644\n--- a/src/function/table/system/duckdb_columns.cpp\n+++ b/src/function/table/system/duckdb_columns.cpp\n@@ -12,7 +12,7 @@\n \n namespace duckdb {\n \n-struct DuckDBColumnsData : public FunctionOperatorData {\n+struct DuckDBColumnsData : public GlobalTableFunctionState {\n \tDuckDBColumnsData() : offset(0), column_offset(0) {\n \t}\n \n@@ -71,8 +71,7 @@ static unique_ptr<FunctionData> DuckDBColumnsBind(ClientContext &context, TableF\n \treturn nullptr;\n }\n \n-unique_ptr<FunctionOperatorData> DuckDBColumnsInit(ClientContext &context, const FunctionData *bind_data,\n-                                                   const vector<column_t> &column_ids, TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> DuckDBColumnsInit(ClientContext &context, TableFunctionInitInput &input) {\n \tauto result = make_unique<DuckDBColumnsData>();\n \n \t// scan all the schemas for tables and views and collect them\n@@ -87,8 +86,6 @@ unique_ptr<FunctionOperatorData> DuckDBColumnsInit(ClientContext &context, const\n \treturn move(result);\n }\n \n-namespace { // anonymous namespace for the ColumnHelper classes for working with tables/views\n-\n class ColumnHelper {\n public:\n \tstatic unique_ptr<ColumnHelper> Create(CatalogEntry *entry);\n@@ -278,11 +275,8 @@ void ColumnHelper::WriteColumns(idx_t start_index, idx_t start_col, idx_t end_co\n \t}\n }\n \n-} // anonymous namespace\n-\n-void DuckDBColumnsFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,\n-                           DataChunk &output) {\n-\tauto &data = (DuckDBColumnsData &)*operator_state;\n+void DuckDBColumnsFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (DuckDBColumnsData &)*data_p.global_state;\n \tif (data.offset >= data.entries.size()) {\n \t\t// finished returning values\n \t\treturn;\ndiff --git a/src/function/table/system/duckdb_constraints.cpp b/src/function/table/system/duckdb_constraints.cpp\nindex 731010f94bff..66cfa2f9e530 100644\n--- a/src/function/table/system/duckdb_constraints.cpp\n+++ b/src/function/table/system/duckdb_constraints.cpp\n@@ -17,7 +17,7 @@\n \n namespace duckdb {\n \n-struct DuckDBConstraintsData : public FunctionOperatorData {\n+struct DuckDBConstraintsData : public GlobalTableFunctionState {\n \tDuckDBConstraintsData() : offset(0), constraint_offset(0) {\n \t}\n \n@@ -63,9 +63,7 @@ static unique_ptr<FunctionData> DuckDBConstraintsBind(ClientContext &context, Ta\n \treturn nullptr;\n }\n \n-unique_ptr<FunctionOperatorData> DuckDBConstraintsInit(ClientContext &context, const FunctionData *bind_data,\n-                                                       const vector<column_t> &column_ids,\n-                                                       TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> DuckDBConstraintsInit(ClientContext &context, TableFunctionInitInput &input) {\n \tauto result = make_unique<DuckDBConstraintsData>();\n \n \t// scan all the schemas for tables and collect themand collect them\n@@ -80,9 +78,8 @@ unique_ptr<FunctionOperatorData> DuckDBConstraintsInit(ClientContext &context, c\n \treturn move(result);\n }\n \n-void DuckDBConstraintsFunction(ClientContext &context, const FunctionData *bind_data,\n-                               FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &data = (DuckDBConstraintsData &)*operator_state;\n+void DuckDBConstraintsFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (DuckDBConstraintsData &)*data_p.global_state;\n \tif (data.offset >= data.entries.size()) {\n \t\t// finished returning values\n \t\treturn;\ndiff --git a/src/function/table/system/duckdb_dependencies.cpp b/src/function/table/system/duckdb_dependencies.cpp\nindex 1e6ca71b3b2d..6f03b8678163 100644\n--- a/src/function/table/system/duckdb_dependencies.cpp\n+++ b/src/function/table/system/duckdb_dependencies.cpp\n@@ -13,7 +13,7 @@ struct DependencyInformation {\n \tDependencyType type;\n };\n \n-struct DuckDBDependenciesData : public FunctionOperatorData {\n+struct DuckDBDependenciesData : public GlobalTableFunctionState {\n \tDuckDBDependenciesData() : offset(0) {\n \t}\n \n@@ -47,9 +47,7 @@ static unique_ptr<FunctionData> DuckDBDependenciesBind(ClientContext &context, T\n \treturn nullptr;\n }\n \n-unique_ptr<FunctionOperatorData> DuckDBDependenciesInit(ClientContext &context, const FunctionData *bind_data,\n-                                                        const vector<column_t> &column_ids,\n-                                                        TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> DuckDBDependenciesInit(ClientContext &context, TableFunctionInitInput &input) {\n \tauto result = make_unique<DuckDBDependenciesData>();\n \n \t// scan all the schemas and collect them\n@@ -66,9 +64,8 @@ unique_ptr<FunctionOperatorData> DuckDBDependenciesInit(ClientContext &context,\n \treturn move(result);\n }\n \n-void DuckDBDependenciesFunction(ClientContext &context, const FunctionData *bind_data,\n-                                FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &data = (DuckDBDependenciesData &)*operator_state;\n+void DuckDBDependenciesFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (DuckDBDependenciesData &)*data_p.global_state;\n \tif (data.offset >= data.entries.size()) {\n \t\t// finished returning values\n \t\treturn;\ndiff --git a/src/function/table/system/duckdb_functions.cpp b/src/function/table/system/duckdb_functions.cpp\nindex 0578d155bfe3..dbbbfe6fe8bb 100644\n--- a/src/function/table/system/duckdb_functions.cpp\n+++ b/src/function/table/system/duckdb_functions.cpp\n@@ -17,7 +17,7 @@\n \n namespace duckdb {\n \n-struct DuckDBFunctionsData : public FunctionOperatorData {\n+struct DuckDBFunctionsData : public GlobalTableFunctionState {\n \tDuckDBFunctionsData() : offset(0), offset_in_entry(0) {\n \t}\n \n@@ -71,9 +71,7 @@ static void ExtractFunctionsFromSchema(ClientContext &context, SchemaCatalogEntr\n \t            [&](CatalogEntry *entry) { result.entries.push_back(entry); });\n }\n \n-unique_ptr<FunctionOperatorData> DuckDBFunctionsInit(ClientContext &context, const FunctionData *bind_data,\n-                                                     const vector<column_t> &column_ids,\n-                                                     TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> DuckDBFunctionsInit(ClientContext &context, TableFunctionInitInput &input) {\n \tauto result = make_unique<DuckDBFunctionsData>();\n \n \t// scan all the schemas for tables and collect themand collect them\n@@ -442,9 +440,8 @@ bool ExtractFunctionData(StandardEntry *entry, idx_t function_idx, DataChunk &ou\n \treturn function_idx + 1 == OP::FunctionCount(function);\n }\n \n-void DuckDBFunctionsFunction(ClientContext &context, const FunctionData *bind_data,\n-                             FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &data = (DuckDBFunctionsData &)*operator_state;\n+void DuckDBFunctionsFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (DuckDBFunctionsData &)*data_p.global_state;\n \tif (data.offset >= data.entries.size()) {\n \t\t// finished returning values\n \t\treturn;\ndiff --git a/src/function/table/system/duckdb_indexes.cpp b/src/function/table/system/duckdb_indexes.cpp\nindex d2f7616ae13a..19814b7abad3 100644\n--- a/src/function/table/system/duckdb_indexes.cpp\n+++ b/src/function/table/system/duckdb_indexes.cpp\n@@ -10,7 +10,7 @@\n \n namespace duckdb {\n \n-struct DuckDBIndexesData : public FunctionOperatorData {\n+struct DuckDBIndexesData : public GlobalTableFunctionState {\n \tDuckDBIndexesData() : offset(0) {\n \t}\n \n@@ -53,8 +53,7 @@ static unique_ptr<FunctionData> DuckDBIndexesBind(ClientContext &context, TableF\n \treturn nullptr;\n }\n \n-unique_ptr<FunctionOperatorData> DuckDBIndexesInit(ClientContext &context, const FunctionData *bind_data,\n-                                                   const vector<column_t> &column_ids, TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> DuckDBIndexesInit(ClientContext &context, TableFunctionInitInput &input) {\n \tauto result = make_unique<DuckDBIndexesData>();\n \n \t// scan all the schemas for tables and collect themand collect them\n@@ -69,9 +68,8 @@ unique_ptr<FunctionOperatorData> DuckDBIndexesInit(ClientContext &context, const\n \treturn move(result);\n }\n \n-void DuckDBIndexesFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,\n-                           DataChunk &output) {\n-\tauto &data = (DuckDBIndexesData &)*operator_state;\n+void DuckDBIndexesFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (DuckDBIndexesData &)*data_p.global_state;\n \tif (data.offset >= data.entries.size()) {\n \t\t// finished returning values\n \t\treturn;\ndiff --git a/src/function/table/system/duckdb_keywords.cpp b/src/function/table/system/duckdb_keywords.cpp\nindex 63dedd9eeb16..339ec4faf1a7 100644\n--- a/src/function/table/system/duckdb_keywords.cpp\n+++ b/src/function/table/system/duckdb_keywords.cpp\n@@ -6,7 +6,7 @@\n \n namespace duckdb {\n \n-struct DuckDBKeywordsData : public FunctionOperatorData {\n+struct DuckDBKeywordsData : public GlobalTableFunctionState {\n \tDuckDBKeywordsData() : offset(0) {\n \t}\n \n@@ -25,17 +25,14 @@ static unique_ptr<FunctionData> DuckDBKeywordsBind(ClientContext &context, Table\n \treturn nullptr;\n }\n \n-unique_ptr<FunctionOperatorData> DuckDBKeywordsInit(ClientContext &context, const FunctionData *bind_data,\n-                                                    const vector<column_t> &column_ids,\n-                                                    TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> DuckDBKeywordsInit(ClientContext &context, TableFunctionInitInput &input) {\n \tauto result = make_unique<DuckDBKeywordsData>();\n \tresult->entries = Parser::KeywordList();\n \treturn move(result);\n }\n \n-void DuckDBKeywordsFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,\n-                            DataChunk &output) {\n-\tauto &data = (DuckDBKeywordsData &)*operator_state;\n+void DuckDBKeywordsFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (DuckDBKeywordsData &)*data_p.global_state;\n \tif (data.offset >= data.entries.size()) {\n \t\t// finished returning values\n \t\treturn;\ndiff --git a/src/function/table/system/duckdb_schemas.cpp b/src/function/table/system/duckdb_schemas.cpp\nindex 7d83a19c17eb..645582229dbe 100644\n--- a/src/function/table/system/duckdb_schemas.cpp\n+++ b/src/function/table/system/duckdb_schemas.cpp\n@@ -8,7 +8,7 @@\n \n namespace duckdb {\n \n-struct DuckDBSchemasData : public FunctionOperatorData {\n+struct DuckDBSchemasData : public GlobalTableFunctionState {\n \tDuckDBSchemasData() : offset(0) {\n \t}\n \n@@ -33,8 +33,7 @@ static unique_ptr<FunctionData> DuckDBSchemasBind(ClientContext &context, TableF\n \treturn nullptr;\n }\n \n-unique_ptr<FunctionOperatorData> DuckDBSchemasInit(ClientContext &context, const FunctionData *bind_data,\n-                                                   const vector<column_t> &column_ids, TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> DuckDBSchemasInit(ClientContext &context, TableFunctionInitInput &input) {\n \tauto result = make_unique<DuckDBSchemasData>();\n \n \t// scan all the schemas and collect them\n@@ -46,9 +45,8 @@ unique_ptr<FunctionOperatorData> DuckDBSchemasInit(ClientContext &context, const\n \treturn move(result);\n }\n \n-void DuckDBSchemasFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,\n-                           DataChunk &output) {\n-\tauto &data = (DuckDBSchemasData &)*operator_state;\n+void DuckDBSchemasFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (DuckDBSchemasData &)*data_p.global_state;\n \tif (data.offset >= data.entries.size()) {\n \t\t// finished returning values\n \t\treturn;\ndiff --git a/src/function/table/system/duckdb_sequences.cpp b/src/function/table/system/duckdb_sequences.cpp\nindex ad7236ab2690..a4cd9bfabc24 100644\n--- a/src/function/table/system/duckdb_sequences.cpp\n+++ b/src/function/table/system/duckdb_sequences.cpp\n@@ -9,7 +9,7 @@\n \n namespace duckdb {\n \n-struct DuckDBSequencesData : public FunctionOperatorData {\n+struct DuckDBSequencesData : public GlobalTableFunctionState {\n \tDuckDBSequencesData() : offset(0) {\n \t}\n \n@@ -58,9 +58,7 @@ static unique_ptr<FunctionData> DuckDBSequencesBind(ClientContext &context, Tabl\n \treturn nullptr;\n }\n \n-unique_ptr<FunctionOperatorData> DuckDBSequencesInit(ClientContext &context, const FunctionData *bind_data,\n-                                                     const vector<column_t> &column_ids,\n-                                                     TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> DuckDBSequencesInit(ClientContext &context, TableFunctionInitInput &input) {\n \tauto result = make_unique<DuckDBSequencesData>();\n \n \t// scan all the schemas for tables and collect themand collect them\n@@ -76,9 +74,8 @@ unique_ptr<FunctionOperatorData> DuckDBSequencesInit(ClientContext &context, con\n \treturn move(result);\n }\n \n-void DuckDBSequencesFunction(ClientContext &context, const FunctionData *bind_data,\n-                             FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &data = (DuckDBSequencesData &)*operator_state;\n+void DuckDBSequencesFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (DuckDBSequencesData &)*data_p.global_state;\n \tif (data.offset >= data.entries.size()) {\n \t\t// finished returning values\n \t\treturn;\ndiff --git a/src/function/table/system/duckdb_settings.cpp b/src/function/table/system/duckdb_settings.cpp\nindex 5db66df773c1..c29482b8f24e 100644\n--- a/src/function/table/system/duckdb_settings.cpp\n+++ b/src/function/table/system/duckdb_settings.cpp\n@@ -12,7 +12,7 @@ struct DuckDBSettingValue {\n \tstring input_type;\n };\n \n-struct DuckDBSettingsData : public FunctionOperatorData {\n+struct DuckDBSettingsData : public GlobalTableFunctionState {\n \tDuckDBSettingsData() : offset(0) {\n \t}\n \n@@ -37,9 +37,7 @@ static unique_ptr<FunctionData> DuckDBSettingsBind(ClientContext &context, Table\n \treturn nullptr;\n }\n \n-unique_ptr<FunctionOperatorData> DuckDBSettingsInit(ClientContext &context, const FunctionData *bind_data,\n-                                                    const vector<column_t> &column_ids,\n-                                                    TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> DuckDBSettingsInit(ClientContext &context, TableFunctionInitInput &input) {\n \tauto result = make_unique<DuckDBSettingsData>();\n \n \tauto &config = DBConfig::GetConfig(context);\n@@ -72,9 +70,8 @@ unique_ptr<FunctionOperatorData> DuckDBSettingsInit(ClientContext &context, cons\n \treturn move(result);\n }\n \n-void DuckDBSettingsFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,\n-                            DataChunk &output) {\n-\tauto &data = (DuckDBSettingsData &)*operator_state;\n+void DuckDBSettingsFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (DuckDBSettingsData &)*data_p.global_state;\n \tif (data.offset >= data.settings.size()) {\n \t\t// finished returning values\n \t\treturn;\ndiff --git a/src/function/table/system/duckdb_tables.cpp b/src/function/table/system/duckdb_tables.cpp\nindex aab6db4cc829..17f3ba0ee069 100644\n--- a/src/function/table/system/duckdb_tables.cpp\n+++ b/src/function/table/system/duckdb_tables.cpp\n@@ -12,7 +12,7 @@\n \n namespace duckdb {\n \n-struct DuckDBTablesData : public FunctionOperatorData {\n+struct DuckDBTablesData : public GlobalTableFunctionState {\n \tDuckDBTablesData() : offset(0) {\n \t}\n \n@@ -61,8 +61,7 @@ static unique_ptr<FunctionData> DuckDBTablesBind(ClientContext &context, TableFu\n \treturn nullptr;\n }\n \n-unique_ptr<FunctionOperatorData> DuckDBTablesInit(ClientContext &context, const FunctionData *bind_data,\n-                                                  const vector<column_t> &column_ids, TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> DuckDBTablesInit(ClientContext &context, TableFunctionInitInput &input) {\n \tauto result = make_unique<DuckDBTablesData>();\n \n \t// scan all the schemas for tables and collect themand collect them\n@@ -99,9 +98,8 @@ static idx_t CheckConstraintCount(TableCatalogEntry &table) {\n \treturn check_count;\n }\n \n-void DuckDBTablesFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,\n-                          DataChunk &output) {\n-\tauto &data = (DuckDBTablesData &)*operator_state;\n+void DuckDBTablesFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (DuckDBTablesData &)*data_p.global_state;\n \tif (data.offset >= data.entries.size()) {\n \t\t// finished returning values\n \t\treturn;\ndiff --git a/src/function/table/system/duckdb_types.cpp b/src/function/table/system/duckdb_types.cpp\nindex 0f4fb802de93..acca0b9c937e 100644\n--- a/src/function/table/system/duckdb_types.cpp\n+++ b/src/function/table/system/duckdb_types.cpp\n@@ -9,7 +9,7 @@\n \n namespace duckdb {\n \n-struct DuckDBTypesData : public FunctionOperatorData {\n+struct DuckDBTypesData : public GlobalTableFunctionState {\n \tDuckDBTypesData() : offset(0) {\n \t}\n \n@@ -48,8 +48,7 @@ static unique_ptr<FunctionData> DuckDBTypesBind(ClientContext &context, TableFun\n \treturn nullptr;\n }\n \n-unique_ptr<FunctionOperatorData> DuckDBTypesInit(ClientContext &context, const FunctionData *bind_data,\n-                                                 const vector<column_t> &column_ids, TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> DuckDBTypesInit(ClientContext &context, TableFunctionInitInput &input) {\n \tauto result = make_unique<DuckDBTypesData>();\n \tauto schemas = Catalog::GetCatalog(context).schemas->GetEntries<SchemaCatalogEntry>(context);\n \tfor (auto &schema : schemas) {\n@@ -64,9 +63,8 @@ unique_ptr<FunctionOperatorData> DuckDBTypesInit(ClientContext &context, const F\n \treturn move(result);\n }\n \n-void DuckDBTypesFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,\n-                         DataChunk &output) {\n-\tauto &data = (DuckDBTypesData &)*operator_state;\n+void DuckDBTypesFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (DuckDBTypesData &)*data_p.global_state;\n \tif (data.offset >= data.entries.size()) {\n \t\t// finished returning values\n \t\treturn;\ndiff --git a/src/function/table/system/duckdb_views.cpp b/src/function/table/system/duckdb_views.cpp\nindex b9cd4a6b96fd..badb657f61ed 100644\n--- a/src/function/table/system/duckdb_views.cpp\n+++ b/src/function/table/system/duckdb_views.cpp\n@@ -9,7 +9,7 @@\n \n namespace duckdb {\n \n-struct DuckDBViewsData : public FunctionOperatorData {\n+struct DuckDBViewsData : public GlobalTableFunctionState {\n \tDuckDBViewsData() : offset(0) {\n \t}\n \n@@ -46,8 +46,7 @@ static unique_ptr<FunctionData> DuckDBViewsBind(ClientContext &context, TableFun\n \treturn nullptr;\n }\n \n-unique_ptr<FunctionOperatorData> DuckDBViewsInit(ClientContext &context, const FunctionData *bind_data,\n-                                                 const vector<column_t> &column_ids, TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> DuckDBViewsInit(ClientContext &context, TableFunctionInitInput &input) {\n \tauto result = make_unique<DuckDBViewsData>();\n \n \t// scan all the schemas for tables and collect themand collect them\n@@ -62,9 +61,8 @@ unique_ptr<FunctionOperatorData> DuckDBViewsInit(ClientContext &context, const F\n \treturn move(result);\n }\n \n-void DuckDBViewsFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,\n-                         DataChunk &output) {\n-\tauto &data = (DuckDBViewsData &)*operator_state;\n+void DuckDBViewsFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (DuckDBViewsData &)*data_p.global_state;\n \tif (data.offset >= data.entries.size()) {\n \t\t// finished returning values\n \t\treturn;\ndiff --git a/src/function/table/system/pragma_collations.cpp b/src/function/table/system/pragma_collations.cpp\nindex 60e52e4a8334..c86a7f3ec3f3 100644\n--- a/src/function/table/system/pragma_collations.cpp\n+++ b/src/function/table/system/pragma_collations.cpp\n@@ -7,7 +7,7 @@\n \n namespace duckdb {\n \n-struct PragmaCollateData : public FunctionOperatorData {\n+struct PragmaCollateData : public GlobalTableFunctionState {\n \tPragmaCollateData() : offset(0) {\n \t}\n \n@@ -23,8 +23,7 @@ static unique_ptr<FunctionData> PragmaCollateBind(ClientContext &context, TableF\n \treturn nullptr;\n }\n \n-unique_ptr<FunctionOperatorData> PragmaCollateInit(ClientContext &context, const FunctionData *bind_data,\n-                                                   const vector<column_t> &column_ids, TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> PragmaCollateInit(ClientContext &context, TableFunctionInitInput &input) {\n \tauto result = make_unique<PragmaCollateData>();\n \n \tCatalog::GetCatalog(context).schemas->Scan(context, [&](CatalogEntry *entry) {\n@@ -36,9 +35,8 @@ unique_ptr<FunctionOperatorData> PragmaCollateInit(ClientContext &context, const\n \treturn move(result);\n }\n \n-static void PragmaCollateFunction(ClientContext &context, const FunctionData *bind_data,\n-                                  FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &data = (PragmaCollateData &)*operator_state;\n+static void PragmaCollateFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (PragmaCollateData &)*data_p.global_state;\n \tif (data.offset >= data.entries.size()) {\n \t\t// finished returning values\n \t\treturn;\ndiff --git a/src/function/table/system/pragma_database_list.cpp b/src/function/table/system/pragma_database_list.cpp\nindex bede5f7828a4..94727eea2593 100644\n--- a/src/function/table/system/pragma_database_list.cpp\n+++ b/src/function/table/system/pragma_database_list.cpp\n@@ -4,7 +4,7 @@\n \n namespace duckdb {\n \n-struct PragmaDatabaseListData : public FunctionOperatorData {\n+struct PragmaDatabaseListData : public GlobalTableFunctionState {\n \tPragmaDatabaseListData() : finished(false) {\n \t}\n \n@@ -25,15 +25,12 @@ static unique_ptr<FunctionData> PragmaDatabaseListBind(ClientContext &context, T\n \treturn nullptr;\n }\n \n-unique_ptr<FunctionOperatorData> PragmaDatabaseListInit(ClientContext &context, const FunctionData *bind_data,\n-                                                        const vector<column_t> &column_ids,\n-                                                        TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> PragmaDatabaseListInit(ClientContext &context, TableFunctionInitInput &input) {\n \treturn make_unique<PragmaDatabaseListData>();\n }\n \n-void PragmaDatabaseListFunction(ClientContext &context, const FunctionData *bind_data,\n-                                FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &data = (PragmaDatabaseListData &)*operator_state;\n+void PragmaDatabaseListFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (PragmaDatabaseListData &)*data_p.global_state;\n \tif (data.finished) {\n \t\treturn;\n \t}\ndiff --git a/src/function/table/system/pragma_database_size.cpp b/src/function/table/system/pragma_database_size.cpp\nindex 0f526eab286b..bf0fba2ad21a 100644\n--- a/src/function/table/system/pragma_database_size.cpp\n+++ b/src/function/table/system/pragma_database_size.cpp\n@@ -8,7 +8,7 @@\n \n namespace duckdb {\n \n-struct PragmaDatabaseSizeData : public FunctionOperatorData {\n+struct PragmaDatabaseSizeData : public GlobalTableFunctionState {\n \tPragmaDatabaseSizeData() : finished(false) {\n \t}\n \n@@ -44,15 +44,12 @@ static unique_ptr<FunctionData> PragmaDatabaseSizeBind(ClientContext &context, T\n \treturn nullptr;\n }\n \n-unique_ptr<FunctionOperatorData> PragmaDatabaseSizeInit(ClientContext &context, const FunctionData *bind_data,\n-                                                        const vector<column_t> &column_ids,\n-                                                        TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> PragmaDatabaseSizeInit(ClientContext &context, TableFunctionInitInput &input) {\n \treturn make_unique<PragmaDatabaseSizeData>();\n }\n \n-void PragmaDatabaseSizeFunction(ClientContext &context, const FunctionData *bind_data,\n-                                FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &data = (PragmaDatabaseSizeData &)*operator_state;\n+void PragmaDatabaseSizeFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (PragmaDatabaseSizeData &)*data_p.global_state;\n \tif (data.finished) {\n \t\treturn;\n \t}\ndiff --git a/src/function/table/system/pragma_functions.cpp b/src/function/table/system/pragma_functions.cpp\nindex df00b911985a..69547ed10f1f 100644\n--- a/src/function/table/system/pragma_functions.cpp\n+++ b/src/function/table/system/pragma_functions.cpp\n@@ -9,7 +9,7 @@\n \n namespace duckdb {\n \n-struct PragmaFunctionsData : public FunctionOperatorData {\n+struct PragmaFunctionsData : public GlobalTableFunctionState {\n \tPragmaFunctionsData() : offset(0), offset_in_entry(0) {\n \t}\n \n@@ -41,9 +41,7 @@ static unique_ptr<FunctionData> PragmaFunctionsBind(ClientContext &context, Tabl\n \treturn nullptr;\n }\n \n-unique_ptr<FunctionOperatorData> PragmaFunctionsInit(ClientContext &context, const FunctionData *bind_data,\n-                                                     const vector<column_t> &column_ids,\n-                                                     TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> PragmaFunctionsInit(ClientContext &context, TableFunctionInitInput &input) {\n \tauto result = make_unique<PragmaFunctionsData>();\n \n \tCatalog::GetCatalog(context).schemas->Scan(context, [&](CatalogEntry *entry) {\n@@ -74,9 +72,8 @@ void AddFunction(BaseScalarFunction &f, idx_t &count, DataChunk &output, bool is\n \tcount++;\n }\n \n-static void PragmaFunctionsFunction(ClientContext &context, const FunctionData *bind_data,\n-                                    FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &data = (PragmaFunctionsData &)*operator_state;\n+static void PragmaFunctionsFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (PragmaFunctionsData &)*data_p.global_state;\n \tif (data.offset >= data.entries.size()) {\n \t\t// finished returning values\n \t\treturn;\ndiff --git a/src/function/table/system/pragma_storage_info.cpp b/src/function/table/system/pragma_storage_info.cpp\nindex 83cff5eb08e8..a8cc2dff61ed 100644\n--- a/src/function/table/system/pragma_storage_info.cpp\n+++ b/src/function/table/system/pragma_storage_info.cpp\n@@ -23,7 +23,7 @@ struct PragmaStorageFunctionData : public TableFunctionData {\n \tvector<vector<Value>> storage_info;\n };\n \n-struct PragmaStorageOperatorData : public FunctionOperatorData {\n+struct PragmaStorageOperatorData : public GlobalTableFunctionState {\n \tPragmaStorageOperatorData() : offset(0) {\n \t}\n \n@@ -89,16 +89,13 @@ static unique_ptr<FunctionData> PragmaStorageInfoBind(ClientContext &context, Ta\n \treturn move(result);\n }\n \n-unique_ptr<FunctionOperatorData> PragmaStorageInfoInit(ClientContext &context, const FunctionData *bind_data,\n-                                                       const vector<column_t> &column_ids,\n-                                                       TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> PragmaStorageInfoInit(ClientContext &context, TableFunctionInitInput &input) {\n \treturn make_unique<PragmaStorageOperatorData>();\n }\n \n-static void PragmaStorageInfoFunction(ClientContext &context, const FunctionData *bind_data_p,\n-                                      FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &bind_data = (PragmaStorageFunctionData &)*bind_data_p;\n-\tauto &data = (PragmaStorageOperatorData &)*operator_state;\n+static void PragmaStorageInfoFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &bind_data = (PragmaStorageFunctionData &)*data_p.bind_data;\n+\tauto &data = (PragmaStorageOperatorData &)*data_p.global_state;\n \tidx_t count = 0;\n \twhile (data.offset < bind_data.storage_info.size() && count < STANDARD_VECTOR_SIZE) {\n \t\tauto &entry = bind_data.storage_info[data.offset++];\ndiff --git a/src/function/table/system/pragma_table_info.cpp b/src/function/table/system/pragma_table_info.cpp\nindex 6a8bcedf84e8..42574f70a416 100644\n--- a/src/function/table/system/pragma_table_info.cpp\n+++ b/src/function/table/system/pragma_table_info.cpp\n@@ -21,7 +21,7 @@ struct PragmaTableFunctionData : public TableFunctionData {\n \tCatalogEntry *entry;\n };\n \n-struct PragmaTableOperatorData : public FunctionOperatorData {\n+struct PragmaTableOperatorData : public GlobalTableFunctionState {\n \tPragmaTableOperatorData() : offset(0) {\n \t}\n \tidx_t offset;\n@@ -55,9 +55,7 @@ static unique_ptr<FunctionData> PragmaTableInfoBind(ClientContext &context, Tabl\n \treturn make_unique<PragmaTableFunctionData>(entry);\n }\n \n-unique_ptr<FunctionOperatorData> PragmaTableInfoInit(ClientContext &context, const FunctionData *bind_data,\n-                                                     const vector<column_t> &column_ids,\n-                                                     TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> PragmaTableInfoInit(ClientContext &context, TableFunctionInitInput &input) {\n \treturn make_unique<PragmaTableOperatorData>();\n }\n \n@@ -155,10 +153,9 @@ static void PragmaTableInfoView(PragmaTableOperatorData &data, ViewCatalogEntry\n \tdata.offset = next;\n }\n \n-static void PragmaTableInfoFunction(ClientContext &context, const FunctionData *bind_data_p,\n-                                    FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &bind_data = (PragmaTableFunctionData &)*bind_data_p;\n-\tauto &state = (PragmaTableOperatorData &)*operator_state;\n+static void PragmaTableInfoFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &bind_data = (PragmaTableFunctionData &)*data_p.bind_data;\n+\tauto &state = (PragmaTableOperatorData &)*data_p.global_state;\n \tswitch (bind_data.entry->type) {\n \tcase CatalogType::TABLE_ENTRY:\n \t\tPragmaTableInfoTable(state, (TableCatalogEntry *)bind_data.entry, output);\ndiff --git a/src/function/table/table_scan.cpp b/src/function/table/table_scan.cpp\nindex 34f002718076..1d584cc8d1da 100644\n--- a/src/function/table/table_scan.cpp\n+++ b/src/function/table/table_scan.cpp\n@@ -10,7 +10,6 @@\n #include \"duckdb/planner/expression/bound_between_expression.hpp\"\n #include \"duckdb/planner/expression_iterator.hpp\"\n #include \"duckdb/planner/operator/logical_get.hpp\"\n-#include \"duckdb/parallel/parallel_state.hpp\"\n \n #include \"duckdb/common/mutex.hpp\"\n \n@@ -19,25 +18,45 @@ namespace duckdb {\n //===--------------------------------------------------------------------===//\n // Table Scan\n //===--------------------------------------------------------------------===//\n-bool TableScanParallelStateNext(ClientContext &context, const FunctionData *bind_data,\n-                                FunctionOperatorData *operator_state, ParallelState *parallel_state_p);\n+bool TableScanParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,\n+                                LocalTableFunctionState *local_state, GlobalTableFunctionState *gstate);\n \n-struct TableScanOperatorData : public FunctionOperatorData {\n+struct TableScanLocalState : public LocalTableFunctionState {\n \t//! The current position in the scan\n \tTableScanState scan_state;\n \tvector<column_t> column_ids;\n };\n \n-static unique_ptr<FunctionOperatorData> TableScanInit(ClientContext &context, const FunctionData *bind_data_p,\n-                                                      const vector<column_t> &column_ids,\n-                                                      TableFilterCollection *filters) {\n-\tauto result = make_unique<TableScanOperatorData>();\n-\tauto &transaction = Transaction::GetTransaction(context);\n-\tauto &bind_data = (const TableScanBindData &)*bind_data_p;\n-\tresult->column_ids = column_ids;\n-\tresult->scan_state.table_filters = filters->table_filters;\n-\tbind_data.table->storage->InitializeScan(transaction, result->scan_state, result->column_ids,\n-\t                                         filters->table_filters);\n+struct TableScanGlobalState : public GlobalTableFunctionState {\n+\tTableScanGlobalState(ClientContext &context, const FunctionData *bind_data_p) {\n+\t\tD_ASSERT(bind_data_p);\n+\t\tauto &bind_data = (const TableScanBindData &)*bind_data_p;\n+\t\tmax_threads = bind_data.table->storage->MaxThreads(context);\n+\t}\n+\n+\tParallelTableScanState state;\n+\tmutex lock;\n+\tidx_t max_threads;\n+\n+\tidx_t MaxThreads() const override {\n+\t\treturn max_threads;\n+\t}\n+};\n+\n+static unique_ptr<LocalTableFunctionState> TableScanInitLocal(ClientContext &context, TableFunctionInitInput &input,\n+                                                              GlobalTableFunctionState *gstate) {\n+\tauto result = make_unique<TableScanLocalState>();\n+\tresult->column_ids = input.column_ids;\n+\tresult->scan_state.table_filters = input.filters;\n+\tTableScanParallelStateNext(context, input.bind_data, result.get(), gstate);\n+\treturn move(result);\n+}\n+\n+unique_ptr<GlobalTableFunctionState> TableScanInitGlobal(ClientContext &context, TableFunctionInitInput &input) {\n+\tD_ASSERT(input.bind_data);\n+\tauto &bind_data = (const TableScanBindData &)*input.bind_data;\n+\tauto result = make_unique<TableScanGlobalState>(context, input.bind_data);\n+\tbind_data.table->storage->InitializeParallelScan(context, result->state);\n \treturn move(result);\n }\n \n@@ -52,63 +71,34 @@ static unique_ptr<BaseStatistics> TableScanStatistics(ClientContext &context, co\n \treturn bind_data.table->storage->GetStatistics(context, column_id);\n }\n \n-static unique_ptr<FunctionOperatorData> TableScanParallelInit(ClientContext &context, const FunctionData *bind_data_p,\n-                                                              ParallelState *state, const vector<column_t> &column_ids,\n-                                                              TableFilterCollection *filters) {\n-\tauto result = make_unique<TableScanOperatorData>();\n-\tresult->column_ids = column_ids;\n-\tresult->scan_state.table_filters = filters->table_filters;\n-\tTableScanParallelStateNext(context, bind_data_p, result.get(), state);\n-\treturn move(result);\n-}\n-\n-static void TableScanFunc(ClientContext &context, const FunctionData *bind_data_p, FunctionOperatorData *operator_state,\n-                          DataChunk &output) {\n-\tD_ASSERT(bind_data_p);\n-\tD_ASSERT(operator_state);\n-\tauto &bind_data = (TableScanBindData &)*bind_data_p;\n-\tauto &state = (TableScanOperatorData &)*operator_state;\n+static void TableScanFunc(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &bind_data = (TableScanBindData &)*data_p.bind_data;\n+\tauto &state = (TableScanLocalState &)*data_p.local_state;\n \tauto &transaction = Transaction::GetTransaction(context);\n-\tbind_data.table->storage->Scan(transaction, output, state.scan_state, state.column_ids);\n-\tbind_data.chunk_count++;\n-}\n-\n-struct ParallelTableFunctionScanState : public ParallelState {\n-\tParallelTableScanState state;\n-\tmutex lock;\n-};\n-\n-idx_t TableScanMaxThreads(ClientContext &context, const FunctionData *bind_data_p) {\n-\tD_ASSERT(bind_data_p);\n-\tauto &bind_data = (const TableScanBindData &)*bind_data_p;\n-\treturn bind_data.table->storage->MaxThreads(context);\n-}\n-\n-unique_ptr<ParallelState> TableScanInitParallelState(ClientContext &context, const FunctionData *bind_data_p,\n-                                                     const vector<column_t> &column_ids,\n-                                                     TableFilterCollection *filters) {\n-\tD_ASSERT(bind_data_p);\n-\tauto &bind_data = (const TableScanBindData &)*bind_data_p;\n-\tauto result = make_unique<ParallelTableFunctionScanState>();\n-\tbind_data.table->storage->InitializeParallelScan(context, result->state);\n-\treturn move(result);\n+\tdo {\n+\t\tbind_data.table->storage->Scan(transaction, output, state.scan_state, state.column_ids);\n+\t\tif (output.size() > 0) {\n+\t\t\treturn;\n+\t\t}\n+\t\tif (!TableScanParallelStateNext(context, data_p.bind_data, data_p.local_state, data_p.global_state)) {\n+\t\t\treturn;\n+\t\t}\n+\t} while (true);\n }\n \n bool TableScanParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,\n-                                FunctionOperatorData *operator_state, ParallelState *parallel_state_p) {\n-\tD_ASSERT(bind_data_p);\n-\tD_ASSERT(parallel_state_p);\n-\tD_ASSERT(operator_state);\n+                                LocalTableFunctionState *local_state, GlobalTableFunctionState *global_state) {\n \tauto &bind_data = (const TableScanBindData &)*bind_data_p;\n-\tauto &parallel_state = (ParallelTableFunctionScanState &)*parallel_state_p;\n-\tauto &state = (TableScanOperatorData &)*operator_state;\n+\tauto &parallel_state = (TableScanGlobalState &)*global_state;\n+\tauto &state = (TableScanLocalState &)*local_state;\n \n \tlock_guard<mutex> parallel_lock(parallel_state.lock);\n \treturn bind_data.table->storage->NextParallelScan(context, parallel_state.state, state.scan_state,\n \t                                                  state.column_ids);\n }\n \n-double TableScanProgress(ClientContext &context, const FunctionData *bind_data_p) {\n+double TableScanProgress(ClientContext &context, const FunctionData *bind_data_p,\n+                         const GlobalTableFunctionState *gstate) {\n \tauto &bind_data = (TableScanBindData &)*bind_data_p;\n \tidx_t total_rows = bind_data.table->storage->GetTotalRows();\n \tif (total_rows == 0 || total_rows < STANDARD_VECTOR_SIZE) {\n@@ -125,9 +115,9 @@ double TableScanProgress(ClientContext &context, const FunctionData *bind_data_p\n }\n \n idx_t TableScanGetBatchIndex(ClientContext &context, const FunctionData *bind_data_p,\n-                             FunctionOperatorData *operator_state, ParallelState *parallel_state_p) {\n+                             LocalTableFunctionState *local_state, GlobalTableFunctionState *global_state) {\n \tauto &bind_data = (const TableScanBindData &)*bind_data_p;\n-\tauto &state = (TableScanOperatorData &)*operator_state;\n+\tauto &state = (TableScanLocalState &)*local_state;\n \tif (state.scan_state.row_group_scan_state.row_group) {\n \t\treturn state.scan_state.row_group_scan_state.row_group->start;\n \t}\n@@ -153,8 +143,8 @@ unique_ptr<NodeStatistics> TableScanCardinality(ClientContext &context, const Fu\n //===--------------------------------------------------------------------===//\n // Index Scan\n //===--------------------------------------------------------------------===//\n-struct IndexScanOperatorData : public FunctionOperatorData {\n-\texplicit IndexScanOperatorData(data_ptr_t row_id_data) : row_ids(LogicalType::ROW_TYPE, row_id_data) {\n+struct IndexScanGlobalState : public GlobalTableFunctionState {\n+\texplicit IndexScanGlobalState(data_ptr_t row_id_data) : row_ids(LogicalType::ROW_TYPE, row_id_data) {\n \t}\n \n \tVector row_ids;\n@@ -164,28 +154,24 @@ struct IndexScanOperatorData : public FunctionOperatorData {\n \tbool finished;\n };\n \n-static unique_ptr<FunctionOperatorData> IndexScanInit(ClientContext &context, const FunctionData *bind_data_p,\n-                                                      const vector<column_t> &column_ids,\n-                                                      TableFilterCollection *filters) {\n-\tauto &bind_data = (const TableScanBindData &)*bind_data_p;\n+static unique_ptr<GlobalTableFunctionState> IndexScanInitGlobal(ClientContext &context, TableFunctionInitInput &input) {\n+\tauto &bind_data = (const TableScanBindData &)*input.bind_data;\n \tdata_ptr_t row_id_data = nullptr;\n \tif (!bind_data.result_ids.empty()) {\n \t\trow_id_data = (data_ptr_t)&bind_data.result_ids[0];\n \t}\n-\tauto result = make_unique<IndexScanOperatorData>(row_id_data);\n+\tauto result = make_unique<IndexScanGlobalState>(row_id_data);\n \tauto &transaction = Transaction::GetTransaction(context);\n-\tresult->column_ids = column_ids;\n-\ttransaction.storage.InitializeScan(bind_data.table->storage.get(), result->local_storage_state,\n-\t                                   filters->table_filters);\n+\tresult->column_ids = input.column_ids;\n+\ttransaction.storage.InitializeScan(bind_data.table->storage.get(), result->local_storage_state, input.filters);\n \n \tresult->finished = false;\n \treturn move(result);\n }\n \n-static void IndexScanFunction(ClientContext &context, const FunctionData *bind_data_p,\n-                              FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &bind_data = (const TableScanBindData &)*bind_data_p;\n-\tauto &state = (IndexScanOperatorData &)*operator_state;\n+static void IndexScanFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &bind_data = (const TableScanBindData &)*data_p.bind_data;\n+\tauto &state = (IndexScanGlobalState &)*data_p.global_state;\n \tauto &transaction = Transaction::GetTransaction(context);\n \tif (!state.finished) {\n \t\tbind_data.table->storage->Fetch(transaction, output, state.column_ids, state.row_ids,\n@@ -335,15 +321,12 @@ void TableScanPushdownComplexFilter(ClientContext &context, LogicalGet &get, Fun\n \t\t\tif (index.Scan(transaction, storage, *index_state, STANDARD_VECTOR_SIZE, bind_data.result_ids)) {\n \t\t\t\t// use an index scan!\n \t\t\t\tbind_data.is_index_scan = true;\n-\t\t\t\tget.function.init = IndexScanInit;\n+\t\t\t\tget.function.init_local = nullptr;\n+\t\t\t\tget.function.init_global = IndexScanInitGlobal;\n \t\t\t\tget.function.function = IndexScanFunction;\n-\t\t\t\tget.function.max_threads = nullptr;\n-\t\t\t\tget.function.init_parallel_state = nullptr;\n-\t\t\t\tget.function.parallel_state_next = nullptr;\n \t\t\t\tget.function.table_scan_progress = nullptr;\n \t\t\t\tget.function.get_batch_index = nullptr;\n \t\t\t\tget.function.filter_pushdown = false;\n-\t\t\t\tget.function.supports_batch_index = false;\n \t\t\t} else {\n \t\t\t\tbind_data.result_ids.clear();\n \t\t\t}\n@@ -361,21 +344,17 @@ string TableScanToString(const FunctionData *bind_data_p) {\n \n TableFunction TableScanFunction::GetFunction() {\n \tTableFunction scan_function(\"seq_scan\", {}, TableScanFunc);\n-\tscan_function.init = TableScanInit;\n+\tscan_function.init_local = TableScanInitLocal;\n+\tscan_function.init_global = TableScanInitGlobal;\n \tscan_function.statistics = TableScanStatistics;\n \tscan_function.dependency = TableScanDependency;\n \tscan_function.cardinality = TableScanCardinality;\n \tscan_function.pushdown_complex_filter = TableScanPushdownComplexFilter;\n \tscan_function.to_string = TableScanToString;\n-\tscan_function.max_threads = TableScanMaxThreads;\n-\tscan_function.init_parallel_state = TableScanInitParallelState;\n-\tscan_function.parallel_init = TableScanParallelInit;\n-\tscan_function.parallel_state_next = TableScanParallelStateNext;\n \tscan_function.table_scan_progress = TableScanProgress;\n \tscan_function.get_batch_index = TableScanGetBatchIndex;\n \tscan_function.projection_pushdown = true;\n \tscan_function.filter_pushdown = true;\n-\tscan_function.supports_batch_index = true;\n \treturn scan_function;\n }\n \ndiff --git a/src/function/table/unnest.cpp b/src/function/table/unnest.cpp\nindex ec8ea39fb28f..9f04b9dd2151 100644\n--- a/src/function/table/unnest.cpp\n+++ b/src/function/table/unnest.cpp\n@@ -23,12 +23,16 @@ struct UnnestBindData : public FunctionData {\n \t}\n };\n \n-struct UnnestOperatorData : public FunctionOperatorData {\n+struct UnnestOperatorData : public GlobalTableFunctionState {\n \tUnnestOperatorData() {\n \t}\n \n \tunique_ptr<OperatorState> operator_state;\n \tvector<unique_ptr<Expression>> select_list;\n+\n+\tidx_t MaxThreads() const override {\n+\t\treturn GlobalTableFunctionState::MAX_THREADS;\n+\t}\n };\n \n static unique_ptr<FunctionData> UnnestBind(ClientContext &context, TableFunctionBindInput &input,\n@@ -41,9 +45,8 @@ static unique_ptr<FunctionData> UnnestBind(ClientContext &context, TableFunction\n \treturn make_unique<UnnestBindData>(input.input_table_types[0]);\n }\n \n-static unique_ptr<FunctionOperatorData> UnnestInit(ClientContext &context, const FunctionData *bind_data_p,\n-                                                   const vector<column_t> &column_ids, TableFilterCollection *filters) {\n-\tauto &bind_data = (UnnestBindData &)*bind_data_p;\n+static unique_ptr<GlobalTableFunctionState> UnnestInit(ClientContext &context, TableFunctionInitInput &input) {\n+\tauto &bind_data = (UnnestBindData &)*input.bind_data;\n \tauto result = make_unique<UnnestOperatorData>();\n \tresult->operator_state = PhysicalUnnest::GetState(context);\n \tauto ref = make_unique<BoundReferenceExpression>(bind_data.input_type, 0);\n@@ -53,9 +56,9 @@ static unique_ptr<FunctionOperatorData> UnnestInit(ClientContext &context, const\n \treturn move(result);\n }\n \n-static OperatorResultType UnnestFunction(ClientContext &context, const FunctionData *bind_data_p,\n-                                         FunctionOperatorData *state_p, DataChunk &input, DataChunk &output) {\n-\tauto &state = (UnnestOperatorData &)*state_p;\n+static OperatorResultType UnnestFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &input,\n+                                         DataChunk &output) {\n+\tauto &state = (UnnestOperatorData &)*data_p.global_state;\n \treturn PhysicalUnnest::ExecuteInternal(context, input, output, *state.operator_state, state.select_list, false);\n }\n \ndiff --git a/src/function/table/version/pragma_version.cpp b/src/function/table/version/pragma_version.cpp\nindex 28e5ad19af4d..a3f68515e855 100644\n--- a/src/function/table/version/pragma_version.cpp\n+++ b/src/function/table/version/pragma_version.cpp\n@@ -5,9 +5,10 @@\n \n namespace duckdb {\n \n-struct PragmaVersionData : public FunctionOperatorData {\n+struct PragmaVersionData : public GlobalTableFunctionState {\n \tPragmaVersionData() : finished(false) {\n \t}\n+\n \tbool finished;\n };\n \n@@ -20,15 +21,12 @@ static unique_ptr<FunctionData> PragmaVersionBind(ClientContext &context, TableF\n \treturn nullptr;\n }\n \n-static unique_ptr<FunctionOperatorData> PragmaVersionInit(ClientContext &context, const FunctionData *bind_data,\n-                                                          const vector<column_t> &column_ids,\n-                                                          TableFilterCollection *filters) {\n+static unique_ptr<GlobalTableFunctionState> PragmaVersionInit(ClientContext &context, TableFunctionInitInput &input) {\n \treturn make_unique<PragmaVersionData>();\n }\n \n-static void PragmaVersionFunction(ClientContext &context, const FunctionData *bind_data,\n-                                  FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &data = (PragmaVersionData &)*operator_state;\n+static void PragmaVersionFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (PragmaVersionData &)*data_p.global_state;\n \tif (data.finished) {\n \t\t// finished returning values\n \t\treturn;\n@@ -40,7 +38,10 @@ static void PragmaVersionFunction(ClientContext &context, const FunctionData *bi\n }\n \n void PragmaVersion::RegisterFunction(BuiltinFunctions &set) {\n-\tset.AddFunction(TableFunction(\"pragma_version\", {}, PragmaVersionFunction, PragmaVersionBind, PragmaVersionInit));\n+\tTableFunction pragma_version(\"pragma_version\", {}, PragmaVersionFunction);\n+\tpragma_version.bind = PragmaVersionBind;\n+\tpragma_version.init_global = PragmaVersionInit;\n+\tset.AddFunction(pragma_version);\n }\n \n const char *DuckDB::SourceID() {\ndiff --git a/src/function/table_function.cpp b/src/function/table_function.cpp\nindex dccdf257cd40..1521b703aa4c 100644\n--- a/src/function/table_function.cpp\n+++ b/src/function/table_function.cpp\n@@ -2,49 +2,28 @@\n \n namespace duckdb {\n \n-FunctionOperatorData::~FunctionOperatorData() {\n+GlobalTableFunctionState::~GlobalTableFunctionState() {\n }\n \n-TableFunctionInfo::~TableFunctionInfo() {\n+LocalTableFunctionState::~LocalTableFunctionState() {\n }\n \n-TableFilterCollection::TableFilterCollection(TableFilterSet *table_filters) : table_filters(table_filters) {\n+TableFunctionInfo::~TableFunctionInfo() {\n }\n \n TableFunction::TableFunction(string name, vector<LogicalType> arguments, table_function_t function,\n-                             table_function_bind_t bind, table_function_init_t init, table_statistics_t statistics,\n-                             table_function_cleanup_t cleanup, table_function_dependency_t dependency,\n-                             table_function_cardinality_t cardinality,\n-                             table_function_pushdown_complex_filter_t pushdown_complex_filter,\n-                             table_function_to_string_t to_string, table_function_max_threads_t max_threads,\n-                             table_function_init_parallel_state_t init_parallel_state,\n-                             table_function_parallel_t parallel_function, table_function_init_parallel_t parallel_init,\n-                             table_function_parallel_state_next_t parallel_state_next, bool projection_pushdown,\n-                             bool filter_pushdown, table_function_progress_t query_progress,\n-                             table_in_out_function_t in_out_function)\n-    : SimpleNamedParameterFunction(move(name), move(arguments)), bind(bind), init(init), function(function),\n-      in_out_function(in_out_function), statistics(statistics), cleanup(cleanup), dependency(dependency),\n-      cardinality(cardinality), pushdown_complex_filter(pushdown_complex_filter), to_string(to_string),\n-      max_threads(max_threads), init_parallel_state(init_parallel_state), parallel_function(parallel_function),\n-      parallel_init(parallel_init), parallel_state_next(parallel_state_next), table_scan_progress(query_progress),\n-      projection_pushdown(projection_pushdown), filter_pushdown(filter_pushdown), supports_batch_index(false) {\n+                             table_function_bind_t bind, table_function_init_global_t init_global,\n+                             table_function_init_local_t init_local)\n+    : SimpleNamedParameterFunction(move(name), move(arguments)), bind(bind), init_global(init_global),\n+      init_local(init_local), function(function), in_out_function(nullptr), statistics(nullptr), dependency(nullptr),\n+      cardinality(nullptr), pushdown_complex_filter(nullptr), to_string(nullptr), table_scan_progress(nullptr),\n+      get_batch_index(nullptr), projection_pushdown(false), filter_pushdown(false) {\n }\n \n TableFunction::TableFunction(const vector<LogicalType> &arguments, table_function_t function,\n-                             table_function_bind_t bind, table_function_init_t init, table_statistics_t statistics,\n-                             table_function_cleanup_t cleanup, table_function_dependency_t dependency,\n-                             table_function_cardinality_t cardinality,\n-                             table_function_pushdown_complex_filter_t pushdown_complex_filter,\n-                             table_function_to_string_t to_string, table_function_max_threads_t max_threads,\n-                             table_function_init_parallel_state_t init_parallel_state,\n-                             table_function_parallel_t parallel_function, table_function_init_parallel_t parallel_init,\n-                             table_function_parallel_state_next_t parallel_state_next, bool projection_pushdown,\n-                             bool filter_pushdown, table_function_progress_t query_progress,\n-                             table_in_out_function_t in_out_function)\n-    : TableFunction(string(), arguments, function, bind, init, statistics, cleanup, dependency, cardinality,\n-                    pushdown_complex_filter, to_string, max_threads, init_parallel_state, parallel_function,\n-                    parallel_init, parallel_state_next, projection_pushdown, filter_pushdown, query_progress,\n-                    in_out_function) {\n+                             table_function_bind_t bind, table_function_init_global_t init_global,\n+                             table_function_init_local_t init_local)\n+    : TableFunction(string(), arguments, function, bind, init_global, init_local) {\n }\n TableFunction::TableFunction() : SimpleNamedParameterFunction(\"\", {}) {\n }\ndiff --git a/src/include/duckdb.h b/src/include/duckdb.h\nindex fee68e3d572d..3a18fe56bce6 100644\n--- a/src/include/duckdb.h\n+++ b/src/include/duckdb.h\n@@ -1435,6 +1435,15 @@ Sets the init function of the table function\n */\n DUCKDB_API void duckdb_table_function_set_init(duckdb_table_function table_function, duckdb_table_function_init_t init);\n \n+/*!\n+Sets the thread-local init function of the table function\n+\n+* table_function: The table function\n+* init: The init function\n+*/\n+DUCKDB_API void duckdb_table_function_set_local_init(duckdb_table_function table_function,\n+                                                     duckdb_table_function_init_t init);\n+\n /*!\n Sets the main function of the table function\n \n@@ -1578,6 +1587,14 @@ This function must be used if projection pushdown is enabled to figure out which\n */\n DUCKDB_API idx_t duckdb_init_get_column_index(duckdb_init_info info, idx_t column_index);\n \n+/*!\n+Sets how many threads can process this table function in parallel (default: 1)\n+\n+* info: The info object\n+* max_threads: The maximum amount of threads that can process this table function\n+*/\n+DUCKDB_API void duckdb_init_set_max_threads(duckdb_init_info info, idx_t max_threads);\n+\n /*!\n Report that an error has occurred while calling init.\n \n@@ -1609,13 +1626,21 @@ For tracking state, use the init data instead.\n DUCKDB_API void *duckdb_function_get_bind_data(duckdb_function_info info);\n \n /*!\n-Gets the init data set by `duckdb_bind_set_init_data` during the bind.\n+Gets the init data set by `duckdb_init_set_init_data` during the init.\n \n * info: The info object\n * returns: The init data object\n */\n DUCKDB_API void *duckdb_function_get_init_data(duckdb_function_info info);\n \n+/*!\n+Gets the thread-local init data set by `duckdb_init_set_init_data` during the local_init.\n+\n+* info: The info object\n+* returns: The init data object\n+*/\n+DUCKDB_API void *duckdb_function_get_local_init_data(duckdb_function_info info);\n+\n /*!\n Report that an error has occurred while executing the function.\n \ndiff --git a/src/include/duckdb/common/operator/cast_operators.hpp b/src/include/duckdb/common/operator/cast_operators.hpp\nindex 3ac8f52ac8a7..23f3b2924a39 100644\n--- a/src/include/duckdb/common/operator/cast_operators.hpp\n+++ b/src/include/duckdb/common/operator/cast_operators.hpp\n@@ -529,6 +529,13 @@ DUCKDB_API bool TryCastToTimestampMS::Operation(string_t input, timestamp_t &res\n template <>\n DUCKDB_API bool TryCastToTimestampSec::Operation(string_t input, timestamp_t &result, bool strict);\n \n+template <>\n+DUCKDB_API bool TryCastToTimestampNS::Operation(date_t input, timestamp_t &result, bool strict);\n+template <>\n+DUCKDB_API bool TryCastToTimestampMS::Operation(date_t input, timestamp_t &result, bool strict);\n+template <>\n+DUCKDB_API bool TryCastToTimestampSec::Operation(date_t input, timestamp_t &result, bool strict);\n+\n //===--------------------------------------------------------------------===//\n // Non-Standard Timestamps -> string/standard timestamp\n //===--------------------------------------------------------------------===//\ndiff --git a/src/include/duckdb/common/types/arrow_aux_data.hpp b/src/include/duckdb/common/types/arrow_aux_data.hpp\nindex 76d3d9d177ea..dcc85159cc5f 100644\n--- a/src/include/duckdb/common/types/arrow_aux_data.hpp\n+++ b/src/include/duckdb/common/types/arrow_aux_data.hpp\n@@ -12,12 +12,15 @@\n #include \"duckdb/common/arrow_wrapper.hpp\"\n \n namespace duckdb {\n+\n struct ArrowAuxiliaryData : VectorAuxiliaryData {\n \texplicit ArrowAuxiliaryData(shared_ptr<ArrowArrayWrapper> arrow_array_p)\n \t    : VectorAuxiliaryData(VectorAuxiliaryDataType::ARROW_AUXILIARY), arrow_array(std::move(arrow_array_p)) {\n+\t}\n+\t~ArrowAuxiliaryData() override {\n+\t}\n \n-\t                                                                     };\n-\t~ArrowAuxiliaryData() override {};\n \tshared_ptr<ArrowArrayWrapper> arrow_array;\n };\n+\n } // namespace duckdb\n\\ No newline at end of file\ndiff --git a/src/include/duckdb/execution/operator/projection/physical_tableinout_function.hpp b/src/include/duckdb/execution/operator/projection/physical_tableinout_function.hpp\nindex ca90a77494f6..6a146be740df 100644\n--- a/src/include/duckdb/execution/operator/projection/physical_tableinout_function.hpp\n+++ b/src/include/duckdb/execution/operator/projection/physical_tableinout_function.hpp\n@@ -23,6 +23,7 @@ class PhysicalTableInOutFunction : public PhysicalOperator {\n \n public:\n \tunique_ptr<OperatorState> GetOperatorState(ClientContext &context) const override;\n+\tunique_ptr<GlobalOperatorState> GetGlobalOperatorState(ClientContext &context) const override;\n \tOperatorResultType Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,\n \t                           GlobalOperatorState &gstate, OperatorState &state) const override;\n \ndiff --git a/src/include/duckdb/execution/operator/scan/physical_table_scan.hpp b/src/include/duckdb/execution/operator/scan/physical_table_scan.hpp\nindex f756d03eea3a..a7ca85a550e8 100644\n--- a/src/include/duckdb/execution/operator/scan/physical_table_scan.hpp\n+++ b/src/include/duckdb/execution/operator/scan/physical_table_scan.hpp\n@@ -53,7 +53,7 @@ class PhysicalTableScan : public PhysicalOperator {\n \t}\n \n \tbool SupportsBatchIndex() const override {\n-\t\treturn function.supports_batch_index;\n+\t\treturn function.get_batch_index != nullptr;\n \t}\n \n \tdouble GetProgress(ClientContext &context, GlobalSourceState &gstate) const override;\ndiff --git a/src/include/duckdb/function/table/arrow.hpp b/src/include/duckdb/function/table/arrow.hpp\nindex 7e14847fa890..fcd47b2b032b 100644\n--- a/src/include/duckdb/function/table/arrow.hpp\n+++ b/src/include/duckdb/function/table/arrow.hpp\n@@ -9,13 +9,12 @@\n #pragma once\n \n #include \"duckdb/function/table_function.hpp\"\n-#include \"duckdb/parallel/parallel_state.hpp\"\n #include \"duckdb/common/arrow_wrapper.hpp\"\n #include \"duckdb/common/atomic.hpp\"\n #include \"duckdb/common/mutex.hpp\"\n+#include \"duckdb/common/pair.hpp\"\n #include \"duckdb/common/thread.hpp\"\n-#include <map>\n-#include <condition_variable>\n+#include \"duckdb/common/unordered_map.hpp\"\n \n namespace duckdb {\n //===--------------------------------------------------------------------===//\n@@ -34,78 +33,64 @@ enum class ArrowDateTimeType : uint8_t {\n \tDAYS = 4,\n \tMONTHS = 5\n };\n+\n struct ArrowConvertData {\n \tArrowConvertData(LogicalType type) : dictionary_type(type) {};\n \tArrowConvertData() {};\n \t//! Hold type of dictionary\n \tLogicalType dictionary_type;\n \t//! If its a variable size type (e.g., strings, blobs, lists) holds which type it is\n-\tvector<std::pair<ArrowVariableSizeType, idx_t>> variable_sz_type;\n+\tvector<pair<ArrowVariableSizeType, idx_t>> variable_sz_type;\n \t//! If this is a date/time holds its precision\n \tvector<ArrowDateTimeType> date_time_precision;\n };\n \n-struct ArrowScanFunctionData : public TableFunctionData {\n-#ifndef DUCKDB_NO_THREADS\n-\n-\tArrowScanFunctionData(idx_t rows_per_thread_p,\n-\t                      unique_ptr<ArrowArrayStreamWrapper> (*scanner_producer_p)(\n-\t                          uintptr_t stream_factory_ptr,\n-\t                          std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns,\n-\t                          TableFilterCollection *filters),\n-\t                      uintptr_t stream_factory_ptr_p, std::thread::id thread_id_p)\n-\t    : lines_read(0), rows_per_thread(rows_per_thread_p), stream_factory_ptr(stream_factory_ptr_p),\n-\t      scanner_producer(scanner_producer_p), number_of_rows(0), thread_id(thread_id_p) {\n-\t}\n-#endif\n+typedef unique_ptr<ArrowArrayStreamWrapper> (*stream_factory_produce_t)(\n+    uintptr_t stream_factory_ptr, pair<unordered_map<idx_t, string>, vector<string>> &project_columns,\n+    TableFilterSet *filters);\n+typedef void (*stream_factory_get_schema_t)(uintptr_t stream_factory_ptr, ArrowSchemaWrapper &schema);\n \n-\tArrowScanFunctionData(idx_t rows_per_thread_p,\n-\t                      unique_ptr<ArrowArrayStreamWrapper> (*scanner_producer_p)(\n-\t                          uintptr_t stream_factory_ptr,\n-\t                          std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns,\n-\t                          TableFilterCollection *filters),\n+struct ArrowScanFunctionData : public TableFunctionData {\n+\tArrowScanFunctionData(idx_t rows_per_thread_p, stream_factory_produce_t scanner_producer_p,\n \t                      uintptr_t stream_factory_ptr_p)\n \t    : lines_read(0), rows_per_thread(rows_per_thread_p), stream_factory_ptr(stream_factory_ptr_p),\n \t      scanner_producer(scanner_producer_p), number_of_rows(0) {\n \t}\n \t//! This holds the original list type (col_idx, [ArrowListType,size])\n-\tstd::unordered_map<idx_t, unique_ptr<ArrowConvertData>> arrow_convert_data;\n-\tstd::atomic<idx_t> lines_read;\n+\tunordered_map<idx_t, unique_ptr<ArrowConvertData>> arrow_convert_data;\n+\tatomic<idx_t> lines_read;\n \tArrowSchemaWrapper schema_root;\n \tidx_t rows_per_thread;\n \t//! Pointer to the scanner factory\n \tuintptr_t stream_factory_ptr;\n \t//! Pointer to the scanner factory produce\n-\tunique_ptr<ArrowArrayStreamWrapper> (*scanner_producer)(\n-\t    uintptr_t stream_factory_ptr,\n-\t    std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns,\n-\t    TableFilterCollection *filters);\n+\tstream_factory_produce_t scanner_producer;\n \t//! Number of rows (Used in cardinality and progress bar)\n \tint64_t number_of_rows;\n-#ifndef DUCKDB_NO_THREADS\n-\t// Thread that made first call in the binder\n-\tstd::thread::id thread_id;\n-#endif\n };\n \n-struct ArrowScanState : public FunctionOperatorData {\n-\texplicit ArrowScanState(unique_ptr<ArrowArrayWrapper> current_chunk) : chunk(move(current_chunk)) {\n+struct ArrowScanLocalState : public LocalTableFunctionState {\n+\texplicit ArrowScanLocalState(unique_ptr<ArrowArrayWrapper> current_chunk) : chunk(move(current_chunk)) {\n \t}\n+\n \tunique_ptr<ArrowArrayStreamWrapper> stream;\n \tshared_ptr<ArrowArrayWrapper> chunk;\n \tidx_t chunk_offset = 0;\n \tvector<column_t> column_ids;\n \t//! Store child vectors for Arrow Dictionary Vectors (col-idx,vector)\n \tunordered_map<idx_t, unique_ptr<Vector>> arrow_dictionary_vectors;\n-\tTableFilterCollection *filters = nullptr;\n+\tTableFilterSet *filters = nullptr;\n };\n \n-struct ParallelArrowScanState : public ParallelState {\n-\tParallelArrowScanState() {\n-\t}\n+struct ArrowScanGlobalState : public GlobalTableFunctionState {\n \tunique_ptr<ArrowArrayStreamWrapper> stream;\n-\tstd::mutex main_mutex;\n+\tmutex main_mutex;\n \tbool ready = false;\n+\tidx_t max_threads = 1;\n+\n+\tidx_t MaxThreads() const override {\n+\t\treturn max_threads;\n+\t}\n };\n \n struct ArrowTableFunction {\n@@ -117,45 +102,30 @@ struct ArrowTableFunction {\n \tstatic unique_ptr<FunctionData> ArrowScanBind(ClientContext &context, TableFunctionBindInput &input,\n \t                                              vector<LogicalType> &return_types, vector<string> &names);\n \t//! Actual conversion from Arrow to DuckDB\n-\tstatic void ArrowToDuckDB(ArrowScanState &scan_state,\n+\tstatic void ArrowToDuckDB(ArrowScanLocalState &scan_state,\n \t                          std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,\n \t                          DataChunk &output, idx_t start);\n \n-\t//! -----Single Thread Functions:-----\n-\t//! Initialize Single Thread Scan\n-\tstatic unique_ptr<FunctionOperatorData> ArrowScanInit(ClientContext &context, const FunctionData *bind_data,\n-\t                                                      const vector<column_t> &column_ids,\n-\t                                                      TableFilterCollection *filters);\n-\n-\t//! Scan Function for Single Thread Execution\n-\tstatic void ArrowScanFunction(ClientContext &context, const FunctionData *bind_data,\n-\t                              FunctionOperatorData *operator_state, DataChunk &output);\n-\n-\t//! -----Multi Thread Functions:-----\n-\t//! Initialize Parallel State\n-\tstatic unique_ptr<ParallelState> ArrowScanInitParallelState(ClientContext &context, const FunctionData *bind_data_p,\n-\t                                                            const vector<column_t> &column_ids,\n-\t                                                            TableFilterCollection *filters);\n-\t//! Initialize Parallel Scans\n-\tstatic unique_ptr<FunctionOperatorData> ArrowScanParallelInit(ClientContext &context,\n-\t                                                              const FunctionData *bind_data_p, ParallelState *state,\n-\t                                                              const vector<column_t> &column_ids,\n-\t                                                              TableFilterCollection *filters);\n+\t//! Initialize Global State\n+\tstatic unique_ptr<GlobalTableFunctionState> ArrowScanInitGlobal(ClientContext &context,\n+\t                                                                TableFunctionInitInput &input);\n+\n+\t//! Initialize Local State\n+\tstatic unique_ptr<LocalTableFunctionState> ArrowScanInitLocal(ClientContext &context, TableFunctionInitInput &input,\n+\t                                                              GlobalTableFunctionState *global_state);\n+\n+\t//! Scan Function\n+\tstatic void ArrowScanFunction(ClientContext &context, TableFunctionInput &data, DataChunk &output);\n+\n \t//! Defines Maximum Number of Threads\n-\tstatic idx_t ArrowScanMaxThreads(ClientContext &context, const FunctionData *bind_data_p);\n-\t//! Scan Function for Parallel Execution\n-\tstatic void ArrowScanFunctionParallel(ClientContext &context, const FunctionData *bind_data,\n-\t                                      FunctionOperatorData *operator_state, DataChunk &output,\n-\t                                      ParallelState *parallel_state_p);\n-\t//! Get next chunk for the running thread\n-\tstatic bool ArrowScanParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,\n-\t                                       FunctionOperatorData *operator_state, ParallelState *parallel_state_p);\n+\tstatic idx_t ArrowScanMaxThreads(ClientContext &context, const FunctionData *bind_data);\n \n \t//! -----Utility Functions:-----\n \t//! Gets Arrow Table's Cardinality\n \tstatic unique_ptr<NodeStatistics> ArrowScanCardinality(ClientContext &context, const FunctionData *bind_data);\n \t//! Gets the progress on the table scan, used for Progress Bars\n-\tstatic double ArrowProgress(ClientContext &context, const FunctionData *bind_data_p);\n+\tstatic double ArrowProgress(ClientContext &context, const FunctionData *bind_data,\n+\t                            const GlobalTableFunctionState *global_state);\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/function/table/read_csv.hpp b/src/include/duckdb/function/table/read_csv.hpp\nindex a1eeb1e8b78c..dee33cd90647 100644\n--- a/src/include/duckdb/function/table/read_csv.hpp\n+++ b/src/include/duckdb/function/table/read_csv.hpp\n@@ -45,10 +45,6 @@ struct ReadCSVData : public BaseCSVData {\n \t//! The initial reader (if any): this is used when automatic detection is used during binding.\n \t//! In this case, the CSV reader is already created and might as well be re-used.\n \tunique_ptr<BufferedCSVReader> initial_reader;\n-\t//! Total File Size\n-\tatomic<idx_t> file_size;\n-\t//! How many bytes were read up to this point\n-\tatomic<idx_t> bytes_read;\n };\n \n struct CSVCopyFunction {\ndiff --git a/src/include/duckdb/function/table_function.hpp b/src/include/duckdb/function/table_function.hpp\nindex 0d7816f7467f..51e467be492e 100644\n--- a/src/include/duckdb/function/table_function.hpp\n+++ b/src/include/duckdb/function/table_function.hpp\n@@ -18,21 +18,27 @@ namespace duckdb {\n \n class BaseStatistics;\n class LogicalGet;\n-struct ParallelState;\n class TableFilterSet;\n \n-struct FunctionOperatorData {\n-\tDUCKDB_API virtual ~FunctionOperatorData();\n-};\n-\n struct TableFunctionInfo {\n \tDUCKDB_API virtual ~TableFunctionInfo();\n };\n \n-struct TableFilterCollection {\n-\tDUCKDB_API explicit TableFilterCollection(TableFilterSet *table_filters);\n+struct GlobalTableFunctionState {\n+public:\n+\t// value returned from MaxThreads when as many threads as possible should be used\n+\tconstexpr static const int64_t MAX_THREADS = 999999999;\n+\n+public:\n+\tDUCKDB_API virtual ~GlobalTableFunctionState();\n \n-\tTableFilterSet *table_filters;\n+\tDUCKDB_API virtual idx_t MaxThreads() const {\n+\t\treturn 1;\n+\t}\n+};\n+\n+struct LocalTableFunctionState {\n+\tDUCKDB_API virtual ~LocalTableFunctionState();\n };\n \n struct TableFunctionBindInput {\n@@ -50,40 +56,46 @@ struct TableFunctionBindInput {\n \tTableFunctionInfo *info;\n };\n \n+struct TableFunctionInitInput {\n+\tTableFunctionInitInput(const FunctionData *bind_data_p, const vector<column_t> &column_ids_p,\n+\t                       TableFilterSet *filters_p)\n+\t    : bind_data(bind_data_p), column_ids(column_ids_p), filters(filters_p) {\n+\t}\n+\n+\tconst FunctionData *bind_data;\n+\tconst vector<column_t> &column_ids;\n+\tTableFilterSet *filters;\n+};\n+\n+struct TableFunctionInput {\n+\tTableFunctionInput(const FunctionData *bind_data_p, LocalTableFunctionState *local_state_p,\n+\t                   GlobalTableFunctionState *global_state_p)\n+\t    : bind_data(bind_data_p), local_state(local_state_p), global_state(global_state_p) {\n+\t}\n+\n+\tconst FunctionData *bind_data;\n+\tLocalTableFunctionState *local_state;\n+\tGlobalTableFunctionState *global_state;\n+};\n+\n typedef unique_ptr<FunctionData> (*table_function_bind_t)(ClientContext &context, TableFunctionBindInput &input,\n                                                           vector<LogicalType> &return_types, vector<string> &names);\n-typedef unique_ptr<FunctionOperatorData> (*table_function_init_t)(ClientContext &context, const FunctionData *bind_data,\n-                                                                  const vector<column_t> &column_ids,\n-                                                                  TableFilterCollection *filters);\n+typedef unique_ptr<GlobalTableFunctionState> (*table_function_init_global_t)(ClientContext &context,\n+                                                                             TableFunctionInitInput &input);\n+typedef unique_ptr<LocalTableFunctionState> (*table_function_init_local_t)(ClientContext &context,\n+                                                                           TableFunctionInitInput &input,\n+                                                                           GlobalTableFunctionState *global_state);\n typedef unique_ptr<BaseStatistics> (*table_statistics_t)(ClientContext &context, const FunctionData *bind_data,\n                                                          column_t column_index);\n-typedef void (*table_function_t)(ClientContext &context, const FunctionData *bind_data,\n-                                 FunctionOperatorData *operator_state, DataChunk &output);\n-\n-typedef OperatorResultType (*table_in_out_function_t)(ClientContext &context, const FunctionData *bind_data,\n-                                                      FunctionOperatorData *operator_state, DataChunk &input,\n-                                                      DataChunk &output);\n+typedef void (*table_function_t)(ClientContext &context, TableFunctionInput &data, DataChunk &output);\n \n-typedef void (*table_function_parallel_t)(ClientContext &context, const FunctionData *bind_data,\n-                                          FunctionOperatorData *operator_state, DataChunk &output,\n-                                          ParallelState *parallel_state);\n+typedef OperatorResultType (*table_in_out_function_t)(ClientContext &context, TableFunctionInput &data,\n+                                                      DataChunk &input, DataChunk &output);\n typedef idx_t (*table_function_get_batch_index_t)(ClientContext &context, const FunctionData *bind_data,\n-                                                  FunctionOperatorData *operator_state, ParallelState *parallel_state);\n-typedef void (*table_function_cleanup_t)(ClientContext &context, const FunctionData *bind_data,\n-                                         FunctionOperatorData *operator_state);\n-typedef idx_t (*table_function_max_threads_t)(ClientContext &context, const FunctionData *bind_data);\n-typedef unique_ptr<ParallelState> (*table_function_init_parallel_state_t)(ClientContext &context,\n-                                                                          const FunctionData *bind_data,\n-                                                                          const vector<column_t> &column_ids,\n-                                                                          TableFilterCollection *filters);\n-typedef unique_ptr<FunctionOperatorData> (*table_function_init_parallel_t)(ClientContext &context,\n-                                                                           const FunctionData *bind_data,\n-                                                                           ParallelState *state,\n-                                                                           const vector<column_t> &column_ids,\n-                                                                           TableFilterCollection *filters);\n-typedef bool (*table_function_parallel_state_next_t)(ClientContext &context, const FunctionData *bind_data,\n-                                                     FunctionOperatorData *state, ParallelState *parallel_state);\n-typedef double (*table_function_progress_t)(ClientContext &context, const FunctionData *bind_data);\n+                                                  LocalTableFunctionState *local_state,\n+                                                  GlobalTableFunctionState *global_state);\n+typedef double (*table_function_progress_t)(ClientContext &context, const FunctionData *bind_data,\n+                                            const GlobalTableFunctionState *global_state);\n typedef void (*table_function_dependency_t)(unordered_set<CatalogEntry *> &dependencies, const FunctionData *bind_data);\n typedef unique_ptr<NodeStatistics> (*table_function_cardinality_t)(ClientContext &context,\n                                                                    const FunctionData *bind_data);\n@@ -96,40 +108,26 @@ class TableFunction : public SimpleNamedParameterFunction {\n public:\n \tDUCKDB_API\n \tTableFunction(string name, vector<LogicalType> arguments, table_function_t function,\n-\t              table_function_bind_t bind = nullptr, table_function_init_t init = nullptr,\n-\t              table_statistics_t statistics = nullptr, table_function_cleanup_t cleanup = nullptr,\n-\t              table_function_dependency_t dependency = nullptr, table_function_cardinality_t cardinality = nullptr,\n-\t              table_function_pushdown_complex_filter_t pushdown_complex_filter = nullptr,\n-\t              table_function_to_string_t to_string = nullptr, table_function_max_threads_t max_threads = nullptr,\n-\t              table_function_init_parallel_state_t init_parallel_state = nullptr,\n-\t              table_function_parallel_t parallel_function = nullptr,\n-\t              table_function_init_parallel_t parallel_init = nullptr,\n-\t              table_function_parallel_state_next_t parallel_state_next = nullptr, bool projection_pushdown = false,\n-\t              bool filter_pushdown = false, table_function_progress_t query_progress = nullptr,\n-\t              table_in_out_function_t in_out_function = nullptr);\n+\t              table_function_bind_t bind = nullptr, table_function_init_global_t init_global = nullptr,\n+\t              table_function_init_local_t init_local = nullptr);\n \tDUCKDB_API\n \tTableFunction(const vector<LogicalType> &arguments, table_function_t function, table_function_bind_t bind = nullptr,\n-\t              table_function_init_t init = nullptr, table_statistics_t statistics = nullptr,\n-\t              table_function_cleanup_t cleanup = nullptr, table_function_dependency_t dependency = nullptr,\n-\t              table_function_cardinality_t cardinality = nullptr,\n-\t              table_function_pushdown_complex_filter_t pushdown_complex_filter = nullptr,\n-\t              table_function_to_string_t to_string = nullptr, table_function_max_threads_t max_threads = nullptr,\n-\t              table_function_init_parallel_state_t init_parallel_state = nullptr,\n-\t              table_function_parallel_t parallel_function = nullptr,\n-\t              table_function_init_parallel_t parallel_init = nullptr,\n-\t              table_function_parallel_state_next_t parallel_state_next = nullptr, bool projection_pushdown = false,\n-\t              bool filter_pushdown = false, table_function_progress_t query_progress = nullptr,\n-\t              table_in_out_function_t in_out_function = nullptr);\n+\t              table_function_init_global_t init_global = nullptr, table_function_init_local_t init_local = nullptr);\n \tDUCKDB_API TableFunction();\n \n \t//! Bind function\n \t//! This function is used for determining the return type of a table producing function and returning bind data\n \t//! The returned FunctionData object should be constant and should not be changed during execution.\n \ttable_function_bind_t bind;\n-\t//! (Optional) init function\n-\t//! Initialize the operator state of the function. The operator state is used to keep track of the progress in the\n-\t//! table function.\n-\ttable_function_init_t init;\n+\t//! (Optional) global init function\n+\t//! Initialize the global operator state of the function.\n+\t//! The global operator state is used to keep track of the progress in the table function and is shared between\n+\t//! all threads working on the table function.\n+\ttable_function_init_global_t init_global;\n+\t//! (Optional) local init function\n+\t//! Initialize the local operator state of the function.\n+\t//! The local operator state is used to keep track of the progress in the table function and is thread-local.\n+\ttable_function_init_local_t init_local;\n \t//! The main function\n \ttable_function_t function;\n \t//! The table in-out function (if this is an in-out function)\n@@ -137,9 +135,6 @@ class TableFunction : public SimpleNamedParameterFunction {\n \t//! (Optional) statistics function\n \t//! Returns the statistics of a specified column\n \ttable_statistics_t statistics;\n-\t//! (Optional) cleanup function\n-\t//! The final cleanup function, called after all data is exhausted from the main function\n-\ttable_function_cleanup_t cleanup;\n \t//! (Optional) dependency function\n \t//! Sets up which catalog entries this table function depend on\n \ttable_function_dependency_t dependency;\n@@ -151,17 +146,6 @@ class TableFunction : public SimpleNamedParameterFunction {\n \ttable_function_pushdown_complex_filter_t pushdown_complex_filter;\n \t//! (Optional) function for rendering the operator to a string in profiling output\n \ttable_function_to_string_t to_string;\n-\t//! (Optional) function that returns the maximum amount of threads that can work on this task\n-\ttable_function_max_threads_t max_threads;\n-\t//! (Optional) initialize the parallel scan state, called once in total.\n-\ttable_function_init_parallel_state_t init_parallel_state;\n-\t//! (Optional) Parallel version of the main function\n-\ttable_function_parallel_t parallel_function;\n-\t//! (Optional) initialize the parallel scan given the parallel state. Called once per task. Return nullptr if there\n-\t//! is nothing left to scan.\n-\ttable_function_init_parallel_t parallel_init;\n-\t//! (Optional) return the next chunk to process in the parallel scan, or return nullptr if there is none\n-\ttable_function_parallel_state_next_t parallel_state_next;\n \t//! (Optional) return how much of the table we have scanned up to this point (% of the data)\n \ttable_function_progress_t table_scan_progress;\n \t//! (Optional) returns the current batch index of the current scan operator\n@@ -172,8 +156,6 @@ class TableFunction : public SimpleNamedParameterFunction {\n \t//! Whether or not the table function supports filter pushdown. If not supported a filter will be added\n \t//! that applies the table filter directly.\n \tbool filter_pushdown;\n-\t//! Whether or not the table function supports fetching of a batch index\n-\tbool supports_batch_index;\n \t//! Additional function info, passed to the bind\n \tshared_ptr<TableFunctionInfo> function_info;\n };\ndiff --git a/src/include/duckdb/parallel/parallel_state.hpp b/src/include/duckdb/parallel/parallel_state.hpp\ndeleted file mode 100644\nindex 2220420ca290..000000000000\n--- a/src/include/duckdb/parallel/parallel_state.hpp\n+++ /dev/null\n@@ -1,18 +0,0 @@\n-//===----------------------------------------------------------------------===//\n-//                         DuckDB\n-//\n-// duckdb/parallel/parallel_state.hpp\n-//\n-//\n-//===----------------------------------------------------------------------===//\n-\n-#pragma once\n-\n-namespace duckdb {\n-\n-struct ParallelState {\n-\tvirtual ~ParallelState() {\n-\t}\n-};\n-\n-} // namespace duckdb\ndiff --git a/src/include/duckdb/parallel/pipeline.hpp b/src/include/duckdb/parallel/pipeline.hpp\nindex ecad5b2cb8ac..2df636a1043c 100644\n--- a/src/include/duckdb/parallel/pipeline.hpp\n+++ b/src/include/duckdb/parallel/pipeline.hpp\n@@ -11,7 +11,6 @@\n #include \"duckdb/common/unordered_set.hpp\"\n #include \"duckdb/execution/physical_operator.hpp\"\n #include \"duckdb/function/table_function.hpp\"\n-#include \"duckdb/parallel/parallel_state.hpp\"\n #include \"duckdb/parallel/task_scheduler.hpp\"\n #include \"duckdb/common/atomic.hpp\"\n \ndiff --git a/src/include/duckdb/storage/table/scan_state.hpp b/src/include/duckdb/storage/table/scan_state.hpp\nindex d51fb1012701..743c6171987f 100644\n--- a/src/include/duckdb/storage/table/scan_state.hpp\n+++ b/src/include/duckdb/storage/table/scan_state.hpp\n@@ -39,11 +39,11 @@ typedef unordered_map<block_id_t, unique_ptr<BufferHandle>> buffer_handle_set_t;\n \n struct ColumnScanState {\n \t//! The column segment that is currently being scanned\n-\tColumnSegment *current;\n+\tColumnSegment *current = nullptr;\n \t//! The current row index of the scan\n-\tidx_t row_index;\n+\tidx_t row_index = 0;\n \t//! The internal row index (i.e. the position of the SegmentScanState)\n-\tidx_t internal_index;\n+\tidx_t internal_index = 0;\n \t//! Segment scan state\n \tunique_ptr<SegmentScanState> scan_state;\n \t//! Child states of the vector\n@@ -77,10 +77,10 @@ struct LocalScanState {\n \t\treturn storage.get();\n \t}\n \n-\tidx_t chunk_index;\n-\tidx_t max_index;\n-\tidx_t last_chunk_count;\n-\tTableFilterSet *table_filters;\n+\tidx_t chunk_index = 0;\n+\tidx_t max_index = 0;\n+\tidx_t last_chunk_count = 0;\n+\tTableFilterSet *table_filters = nullptr;\n \n private:\n \tshared_ptr<LocalTableStorage> storage;\n@@ -94,17 +94,13 @@ class RowGroupScanState {\n \t//! The parent scan state\n \tTableScanState &parent;\n \t//! The current row_group we are scanning\n-\tRowGroup *row_group;\n+\tRowGroup *row_group = nullptr;\n \t//! The vector index within the row_group\n-\tidx_t vector_index;\n+\tidx_t vector_index = 0;\n \t//! The maximum row index of this row_group scan\n-\tidx_t max_row;\n+\tidx_t max_row = 0;\n \t//! Child column scans\n \tunique_ptr<ColumnScanState[]> column_scans;\n-\n-public:\n-\t//! Move to the next vector, skipping past the current one\n-\tvoid NextVector();\n };\n \n class TableScanState {\n@@ -114,7 +110,7 @@ class TableScanState {\n \t//! The row_group scan state\n \tRowGroupScanState row_group_scan_state;\n \t//! The total maximum row index\n-\tidx_t max_row;\n+\tidx_t max_row = 0;\n \t//! The column identifiers of the scan\n \tvector<column_t> column_ids;\n \t//! The table filters (if any)\n@@ -123,10 +119,6 @@ class TableScanState {\n \tunique_ptr<AdaptiveFilter> adaptive_filter;\n \t//! Transaction-local scan state\n \tLocalScanState local_state;\n-\n-public:\n-\t//! Move to the next vector\n-\tvoid NextVector();\n };\n \n class CreateIndexScanState : public TableScanState {\ndiff --git a/src/main/capi/table_function-c.cpp b/src/main/capi/table_function-c.cpp\nindex a6dc163f622d..bb27a22e05fa 100644\n--- a/src/main/capi/table_function-c.cpp\n+++ b/src/main/capi/table_function-c.cpp\n@@ -17,6 +17,7 @@ struct CTableFunctionInfo : public TableFunctionInfo {\n \n \tduckdb_table_function_bind_t bind = nullptr;\n \tduckdb_table_function_init_t init = nullptr;\n+\tduckdb_table_function_init_t local_init = nullptr;\n \tduckdb_table_function_t function = nullptr;\n \tvoid *extra_info = nullptr;\n \tduckdb_delete_callback_t delete_callback = nullptr;\n@@ -53,7 +54,7 @@ struct CTableInternalBindInfo {\n \tstring error;\n };\n \n-struct CTableInitData : public FunctionOperatorData {\n+struct CTableInitData {\n \t~CTableInitData() {\n \t\tif (init_data && delete_callback) {\n \t\t\tdelete_callback(init_data);\n@@ -64,29 +65,43 @@ struct CTableInitData : public FunctionOperatorData {\n \n \tvoid *init_data = nullptr;\n \tduckdb_delete_callback_t delete_callback = nullptr;\n+\tidx_t max_threads = 1;\n+};\n+\n+struct CTableGlobalInitData : public GlobalTableFunctionState {\n+\tCTableInitData init_data;\n+\n+\tidx_t MaxThreads() const override {\n+\t\treturn init_data.max_threads;\n+\t}\n+};\n+\n+struct CTableLocalInitData : public LocalTableFunctionState {\n+\tCTableInitData init_data;\n };\n \n struct CTableInternalInitInfo {\n \tCTableInternalInitInfo(CTableBindData &bind_data, CTableInitData &init_data, const vector<column_t> &column_ids,\n-\t                       TableFilterCollection *filters)\n+\t                       TableFilterSet *filters)\n \t    : bind_data(bind_data), init_data(init_data), column_ids(column_ids), filters(filters), success(true) {\n \t}\n \n \tCTableBindData &bind_data;\n \tCTableInitData &init_data;\n \tconst vector<column_t> &column_ids;\n-\tTableFilterCollection *filters;\n+\tTableFilterSet *filters;\n \tbool success;\n \tstring error;\n };\n \n struct CTableInternalFunctionInfo {\n-\tCTableInternalFunctionInfo(CTableBindData &bind_data, CTableInitData &init_data)\n-\t    : bind_data(bind_data), init_data(init_data), success(true) {\n+\tCTableInternalFunctionInfo(CTableBindData &bind_data, CTableInitData &init_data, CTableInitData &local_data)\n+\t    : bind_data(bind_data), init_data(init_data), local_data(local_data), success(true) {\n \t}\n \n \tCTableBindData &bind_data;\n \tCTableInitData &init_data;\n+\tCTableInitData &local_data;\n \tbool success;\n \tstring error;\n };\n@@ -106,13 +121,11 @@ unique_ptr<FunctionData> CTableFunctionBind(ClientContext &context, TableFunctio\n \treturn move(result);\n }\n \n-unique_ptr<FunctionOperatorData> CTableFunctionInit(ClientContext &context, const FunctionData *bind_data_p,\n-                                                    const vector<column_t> &column_ids,\n-                                                    TableFilterCollection *filters) {\n-\tauto &bind_data = (CTableBindData &)*bind_data_p;\n-\tauto result = make_unique<CTableInitData>();\n+unique_ptr<GlobalTableFunctionState> CTableFunctionInit(ClientContext &context, TableFunctionInitInput &data_p) {\n+\tauto &bind_data = (CTableBindData &)*data_p.bind_data;\n+\tauto result = make_unique<CTableGlobalInitData>();\n \n-\tCTableInternalInitInfo init_info(bind_data, *result, column_ids, filters);\n+\tCTableInternalInitInfo init_info(bind_data, result->init_data, data_p.column_ids, data_p.filters);\n \tbind_data.info->init(&init_info);\n \tif (!init_info.success) {\n \t\tthrow Exception(init_info.error);\n@@ -120,11 +133,27 @@ unique_ptr<FunctionOperatorData> CTableFunctionInit(ClientContext &context, cons\n \treturn move(result);\n }\n \n-void CTableFunction(ClientContext &context, const FunctionData *bind_data_p, FunctionOperatorData *operator_state,\n-                    DataChunk &output) {\n-\tauto &bind_data = (CTableBindData &)*bind_data_p;\n-\tauto &init_data = (CTableInitData &)*operator_state;\n-\tCTableInternalFunctionInfo function_info(bind_data, init_data);\n+unique_ptr<LocalTableFunctionState> CTableFunctionLocalInit(ClientContext &context, TableFunctionInitInput &data_p,\n+                                                            GlobalTableFunctionState *gstate) {\n+\tauto &bind_data = (CTableBindData &)*data_p.bind_data;\n+\tauto result = make_unique<CTableLocalInitData>();\n+\tif (!bind_data.info->local_init) {\n+\t\treturn move(result);\n+\t}\n+\n+\tCTableInternalInitInfo init_info(bind_data, result->init_data, data_p.column_ids, data_p.filters);\n+\tbind_data.info->local_init(&init_info);\n+\tif (!init_info.success) {\n+\t\tthrow Exception(init_info.error);\n+\t}\n+\treturn move(result);\n+}\n+\n+void CTableFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &bind_data = (CTableBindData &)*data_p.bind_data;\n+\tauto &global_data = (CTableGlobalInitData &)*data_p.global_state;\n+\tauto &local_data = (CTableLocalInitData &)*data_p.local_state;\n+\tCTableInternalFunctionInfo function_info(bind_data, global_data.init_data, local_data.init_data);\n \tbind_data.info->function(&function_info, &output);\n \tif (!function_info.success) {\n \t\tthrow Exception(function_info.error);\n@@ -138,7 +167,7 @@ void CTableFunction(ClientContext &context, const FunctionData *bind_data_p, Fun\n //===--------------------------------------------------------------------===//\n duckdb_table_function duckdb_create_table_function() {\n \tauto function = new duckdb::TableFunction(\"\", {}, duckdb::CTableFunction, duckdb::CTableFunctionBind,\n-\t                                          duckdb::CTableFunctionInit);\n+\t                                          duckdb::CTableFunctionInit, duckdb::CTableFunctionLocalInit);\n \tfunction->function_info = duckdb::make_shared<duckdb::CTableFunctionInfo>();\n \treturn function;\n }\n@@ -197,6 +226,15 @@ void duckdb_table_function_set_init(duckdb_table_function function, duckdb_table\n \tinfo->init = init;\n }\n \n+void duckdb_table_function_set_local_init(duckdb_table_function function, duckdb_table_function_init_t init) {\n+\tif (!function || !init) {\n+\t\treturn;\n+\t}\n+\tauto tf = (duckdb::TableFunction *)function;\n+\tauto info = (duckdb::CTableFunctionInfo *)tf->function_info.get();\n+\tinfo->local_init = init;\n+}\n+\n void duckdb_table_function_set_function(duckdb_table_function table_function, duckdb_table_function_t function) {\n \tif (!table_function || !function) {\n \t\treturn;\n@@ -344,6 +382,14 @@ idx_t duckdb_init_get_column_index(duckdb_init_info info, idx_t column_index) {\n \treturn function_info->column_ids[column_index];\n }\n \n+void duckdb_init_set_max_threads(duckdb_init_info info, idx_t max_threads) {\n+\tif (!info) {\n+\t\treturn;\n+\t}\n+\tauto function_info = (duckdb::CTableInternalInitInfo *)info;\n+\tfunction_info->init_data.max_threads = max_threads;\n+}\n+\n //===--------------------------------------------------------------------===//\n // Function Interface\n //===--------------------------------------------------------------------===//\n@@ -371,6 +417,14 @@ void *duckdb_function_get_init_data(duckdb_function_info info) {\n \treturn function_info->init_data.init_data;\n }\n \n+void *duckdb_function_get_local_init_data(duckdb_function_info info) {\n+\tif (!info) {\n+\t\treturn nullptr;\n+\t}\n+\tauto function_info = (duckdb::CTableInternalFunctionInfo *)info;\n+\treturn function_info->local_data.init_data;\n+}\n+\n void duckdb_function_set_error(duckdb_function_info info, const char *error) {\n \tif (!info || !error) {\n \t\treturn;\ndiff --git a/src/storage/data_table.cpp b/src/storage/data_table.cpp\nindex 799f5baab7b4..43943f88eea1 100644\n--- a/src/storage/data_table.cpp\n+++ b/src/storage/data_table.cpp\n@@ -305,6 +305,7 @@ void DataTable::InitializeParallelScan(ClientContext &context, ParallelTableScan\n \tstate.transaction_local_data = false;\n \t// figure out the max row we can scan for both the regular and the transaction-local storage\n \tstate.max_row = total_rows;\n+\tstate.vector_index = 0;\n \tstate.local_state.max_index = 0;\n \tauto &transaction = Transaction::GetTransaction(context);\n \ttransaction.storage.InitializeScan(this, state.local_state, nullptr);\n@@ -320,13 +321,19 @@ bool DataTable::NextParallelScan(ClientContext &context, ParallelTableScanState\n \t\t\tmax_row = state.current_row_group->start +\n \t\t\t          MinValue<idx_t>(state.current_row_group->count,\n \t\t\t                          STANDARD_VECTOR_SIZE * state.vector_index + STANDARD_VECTOR_SIZE);\n+\t\t\tD_ASSERT(vector_index * STANDARD_VECTOR_SIZE < state.current_row_group->count);\n \t\t} else {\n \t\t\tvector_index = 0;\n \t\t\tmax_row = state.current_row_group->start + state.current_row_group->count;\n \t\t}\n \t\tmax_row = MinValue<idx_t>(max_row, state.max_row);\n-\t\tbool need_to_scan = InitializeScanInRowGroup(scan_state, column_ids, scan_state.table_filters,\n-\t\t                                             state.current_row_group, vector_index, max_row);\n+\t\tbool need_to_scan;\n+\t\tif (state.current_row_group->count == 0) {\n+\t\t\tneed_to_scan = false;\n+\t\t} else {\n+\t\t\tneed_to_scan = InitializeScanInRowGroup(scan_state, column_ids, scan_state.table_filters,\n+\t\t\t                                        state.current_row_group, vector_index, max_row);\n+\t\t}\n \t\tif (ClientConfig::GetConfig(context).verify_parallelism) {\n \t\t\tstate.vector_index++;\n \t\t\tif (state.vector_index * STANDARD_VECTOR_SIZE >= state.current_row_group->count) {\ndiff --git a/src/storage/local_storage.cpp b/src/storage/local_storage.cpp\nindex 5d738595f98d..ecd6afc09049 100644\n--- a/src/storage/local_storage.cpp\n+++ b/src/storage/local_storage.cpp\n@@ -19,16 +19,18 @@ LocalTableStorage::~LocalTableStorage() {\n }\n \n void LocalTableStorage::InitializeScan(LocalScanState &state, TableFilterSet *table_filters) {\n+\tstate.table_filters = table_filters;\n+\tstate.chunk_index = 0;\n \tif (collection.ChunkCount() == 0) {\n \t\t// nothing to scan\n+\t\tstate.max_index = 0;\n+\t\tstate.last_chunk_count = 0;\n \t\treturn;\n \t}\n \tstate.SetStorage(shared_from_this());\n \n-\tstate.chunk_index = 0;\n \tstate.max_index = collection.ChunkCount() - 1;\n \tstate.last_chunk_count = collection.Chunks().back()->size();\n-\tstate.table_filters = table_filters;\n }\n \n idx_t LocalTableStorage::EstimatedSize() {\ndiff --git a/tools/juliapkg/src/api.jl b/tools/juliapkg/src/api.jl\nindex 883b16989c20..c39fa7caecf6 100644\n--- a/tools/juliapkg/src/api.jl\n+++ b/tools/juliapkg/src/api.jl\n@@ -1789,6 +1789,22 @@ function duckdb_table_function_set_init(table_func, init_func)\n     )\n end\n \n+\"\"\"\n+Sets the thread-local init function of the table function\n+\n+* table_function: The table function\n+* init: The init function\n+\"\"\"\n+function duckdb_table_function_set_local_init(table_func, init_func)\n+    return ccall(\n+        (:duckdb_table_function_set_local_init, libduckdb),\n+        Cvoid,\n+        (duckdb_table_function, Ptr{Cvoid}),\n+        table_func,\n+        init_func\n+    )\n+end\n+\n \n \"\"\"\n Sets the main function of the table function\n@@ -2006,6 +2022,16 @@ function duckdb_init_get_column_index(info, index)\n     return ccall((:duckdb_init_get_column_index, libduckdb), UInt64, (duckdb_init_info, UInt64), info, index - 1) + 1\n end\n \n+\"\"\"\n+Sets how many threads can process this table function in parallel (default: 1)\n+\n+* info: The info object\n+* max_threads: The maximum amount of threads that can process this table function\n+\"\"\"\n+function duckdb_init_set_max_threads(info, max_threads)\n+    return ccall((:duckdb_init_set_max_threads, libduckdb), Cvoid, (duckdb_init_info, UInt64), info, max_threads)\n+end\n+\n \"\"\"\n Report that an error has occurred during init.\n \n@@ -2045,7 +2071,7 @@ function duckdb_function_get_bind_data(info)\n end\n \n \"\"\"\n-Gets the init data set by `duckdb_bind_set_init_data` during the bind.\n+Gets the init data set by `duckdb_init_set_init_data` during the init.\n \n * info: The info object\n * returns: The init data object\n@@ -2054,6 +2080,16 @@ function duckdb_function_get_init_data(info)\n     return ccall((:duckdb_function_get_init_data, libduckdb), Ptr{Cvoid}, (duckdb_function_info,), info)\n end\n \n+\"\"\"\n+Gets the init data set by `duckdb_init_set_init_data` during the local_init.\n+\n+* info: The info object\n+* returns: The init data object\n+\"\"\"\n+function duckdb_function_get_local_init_data(info)\n+    return ccall((:duckdb_function_get_local_init_data, libduckdb), Ptr{Cvoid}, (duckdb_function_info,), info)\n+end\n+\n \"\"\"\n Report that an error has occurred while executing the function.\n \ndiff --git a/tools/juliapkg/src/data_frame_scan.jl b/tools/juliapkg/src/data_frame_scan.jl\nindex 673399d75a32..a8db09f46461 100644\n--- a/tools/juliapkg/src/data_frame_scan.jl\n+++ b/tools/juliapkg/src/data_frame_scan.jl\n@@ -1,6 +1,6 @@\n using DataFrames\n \n-mutable struct DFBindInfo\n+struct DFBindInfo\n     df::DataFrame\n     input_columns::Vector\n     scan_types::Vector{Type}\n@@ -59,7 +59,7 @@ function value_to_duckdb(val::T) where {T}\n end\n \n function df_scan_column(\n-    input_column::Vector{DF_TYPE},\n+    input_column::AbstractVector{DF_TYPE},\n     df_offset::Int64,\n     col_idx::Int64,\n     result_idx::Int64,\n@@ -72,16 +72,17 @@ function df_scan_column(\n     result_array::Vector{DUCK_TYPE} = DuckDB.get_array(vector, DUCK_TYPE)\n     validity::ValidityMask = DuckDB.get_validity(vector)\n     for i::Int64 in 1:scan_count\n-        if input_column[df_offset + i] === missing\n+        val = getindex(input_column, df_offset + i)\n+        if val === missing\n             DuckDB.setinvalid(validity, i)\n         else\n-            result_array[i] = value_to_duckdb(input_column[df_offset + i])\n+            result_array[i] = value_to_duckdb(val)\n         end\n     end\n end\n \n function df_scan_string_column(\n-    input_column::Vector{DF_TYPE},\n+    input_column::AbstractVector{DF_TYPE},\n     df_offset::Int64,\n     col_idx::Int64,\n     result_idx::Int64,\n@@ -93,10 +94,11 @@ function df_scan_string_column(\n     vector::Vec = DuckDB.get_vector(output, result_idx)\n     validity::ValidityMask = DuckDB.get_validity(vector)\n     for i::Int64 in 1:scan_count\n-        if input_column[df_offset + i] === missing\n+        val = getindex(input_column, df_offset + i)\n+        if val === missing\n             DuckDB.setinvalid(validity, i)\n         else\n-            DuckDB.assign_string_element(vector, i, input_column[df_offset + i])\n+            DuckDB.assign_string_element(vector, i, val)\n         end\n     end\n end\n@@ -135,38 +137,74 @@ function df_bind_function(info::DuckDB.BindInfo)\n     return DFBindInfo(df, input_columns, scan_types, result_types, scan_functions)\n end\n \n-mutable struct DFInitInfo\n+mutable struct DFGlobalInfo\n     pos::Int64\n+    global_lock::ReentrantLock\n+\n+    function DFGlobalInfo()\n+        return new(0, ReentrantLock())\n+    end\n+end\n+\n+mutable struct DFLocalInfo\n     columns::Vector{Int64}\n+    current_pos::Int64\n+    end_pos::Int64\n \n-    function DFInitInfo(columns)\n-        return new(0, columns)\n+    function DFLocalInfo(columns)\n+        return new(columns, 0, 0)\n     end\n end\n \n-function df_init_function(info::DuckDB.InitInfo)\n-    return DFInitInfo(DuckDB.get_projected_columns(info))\n+function df_global_init_function(info::DuckDB.InitInfo)\n+    bind_info = DuckDB.get_bind_info(info, DFBindInfo)\n+    # figure out the maximum number of threads to launch from the DF size\n+    row_count::Int64 = size(bind_info.df, 1)\n+    max_threads::Int64 = ceil(row_count / DuckDB.ROW_GROUP_SIZE)\n+    DuckDB.set_max_threads(info, max_threads)\n+    return DFGlobalInfo()\n+end\n+\n+function df_local_init_function(info::DuckDB.InitInfo)\n+    columns = DuckDB.get_projected_columns(info)\n+    return DFLocalInfo(columns)\n end\n \n function df_scan_function(info::DuckDB.FunctionInfo, output::DuckDB.DataChunk)\n     bind_info = DuckDB.get_bind_info(info, DFBindInfo)\n-    init_info = DuckDB.get_init_info(info, DFInitInfo)\n-\n-    row_count = size(bind_info.df, 1)\n+    global_info = DuckDB.get_init_info(info, DFGlobalInfo)\n+    local_info = DuckDB.get_local_info(info, DFLocalInfo)\n+\n+    if local_info.current_pos >= local_info.end_pos\n+        # ran out of data to scan in the local info: fetch new rows from the global state (if any)\n+        # we can in increments of 100 vectors\n+        lock(global_info.global_lock)\n+        row_count::Int64 = size(bind_info.df, 1)\n+        local_info.current_pos = global_info.pos\n+        total_scan_amount::Int64 = DuckDB.ROW_GROUP_SIZE\n+        if local_info.current_pos + total_scan_amount >= row_count\n+            total_scan_amount = row_count - local_info.current_pos\n+        end\n+        local_info.end_pos = local_info.current_pos + total_scan_amount\n+        global_info.pos += total_scan_amount\n+        unlock(global_info.global_lock)\n+    end\n     scan_count::Int64 = DuckDB.VECTOR_SIZE\n-    if init_info.pos + scan_count >= row_count\n-        scan_count = row_count - init_info.pos\n+    current_row::Int64 = local_info.current_pos\n+    if current_row + scan_count >= local_info.end_pos\n+        scan_count = local_info.end_pos - current_row\n     end\n+    local_info.current_pos += scan_count\n \n     result_idx::Int64 = 1\n-    for col_idx in init_info.columns\n+    for col_idx::Int64 in local_info.columns\n         if col_idx == 0\n             result_idx += 1\n             continue\n         end\n         bind_info.scan_functions[col_idx](\n             bind_info.input_columns[col_idx],\n-            init_info.pos,\n+            current_row,\n             col_idx,\n             result_idx,\n             scan_count,\n@@ -176,7 +214,6 @@ function df_scan_function(info::DuckDB.FunctionInfo, output::DuckDB.DataChunk)\n         )\n         result_idx += 1\n     end\n-    init_info.pos += scan_count\n     DuckDB.set_size(output, scan_count)\n     return\n end\n@@ -206,10 +243,11 @@ function _add_data_frame_scan(db::DB)\n         \"julia_df_scan\",\n         [String],\n         df_bind_function,\n-        df_init_function,\n+        df_global_init_function,\n         df_scan_function,\n         db.handle.registered_objects,\n-        true\n+        true,\n+        df_local_init_function\n     )\n     return\n end\ndiff --git a/tools/juliapkg/src/database.jl b/tools/juliapkg/src/database.jl\nindex 562793d10034..108752f1b404 100644\n--- a/tools/juliapkg/src/database.jl\n+++ b/tools/juliapkg/src/database.jl\n@@ -98,6 +98,7 @@ function close_database(db::DB)\n end\n \n const VECTOR_SIZE = duckdb_vector_size()\n+const ROW_GROUP_SIZE = VECTOR_SIZE * 100\n \n DB() = DB(\":memory:\")\n DBInterface.connect(::Type{DB}) = DB()\ndiff --git a/tools/juliapkg/src/table_function.jl b/tools/juliapkg/src/table_function.jl\nindex ff2fca161a6e..8f4696136cee 100644\n--- a/tools/juliapkg/src/table_function.jl\n+++ b/tools/juliapkg/src/table_function.jl\n@@ -42,9 +42,23 @@ function get_extra_data(bind_info::BindInfo)\n     return bind_info.main_function.extra_data\n end\n \n+function _add_global_object(main_function, object)\n+    lock(main_function.global_lock)\n+    push!(main_function.global_objects, object)\n+    unlock(main_function.global_lock)\n+    return\n+end\n+\n+function _remove_global_object(main_function, object)\n+    lock(main_function.global_lock)\n+    delete!(main_function.global_objects, object)\n+    unlock(main_function.global_lock)\n+    return\n+end\n+\n function _table_bind_cleanup(data::Ptr{Cvoid})\n     info::InfoWrapper = unsafe_pointer_to_objref(data)\n-    delete!(info.main_function.global_objects, info)\n+    _remove_global_object(info.main_function, info)\n     return\n end\n \n@@ -54,7 +68,7 @@ function _table_bind_function(info::duckdb_bind_info)\n         binfo = BindInfo(info, main_function)\n         bind_data = InfoWrapper(main_function, main_function.bind_func(binfo))\n         bind_data_pointer = pointer_from_objref(bind_data)\n-        push!(main_function.global_objects, bind_data)\n+        _add_global_object(main_function, bind_data)\n         duckdb_bind_set_bind_data(info, bind_data_pointer, @cfunction(_table_bind_cleanup, Cvoid, (Ptr{Cvoid},)))\n     catch ex\n         duckdb_bind_set_error(info, sprint(showerror, ex))\n@@ -68,7 +82,7 @@ end\n // Table Function Init\n //===--------------------------------------------------------------------===//\n =#\n-mutable struct InitInfo\n+struct InitInfo\n     handle::duckdb_init_info\n     main_function::Any\n \n@@ -78,13 +92,14 @@ mutable struct InitInfo\n     end\n end\n \n-function _table_init_function(info::duckdb_init_info)\n+\n+function _table_init_function_generic(info::duckdb_init_info, init_fun::Function)\n     try\n         main_function = unsafe_pointer_to_objref(duckdb_init_get_extra_info(info))\n         binfo = InitInfo(info, main_function)\n-        init_data = InfoWrapper(main_function, main_function.init_func(binfo))\n+        init_data = InfoWrapper(main_function, init_fun(binfo))\n         init_data_pointer = pointer_from_objref(init_data)\n-        push!(main_function.global_objects, init_data)\n+        _add_global_object(main_function, init_data)\n         duckdb_init_set_init_data(info, init_data_pointer, @cfunction(_table_bind_cleanup, Cvoid, (Ptr{Cvoid},)))\n     catch ex\n         duckdb_init_set_error(info, sprint(showerror, ex))\n@@ -93,6 +108,16 @@ function _table_init_function(info::duckdb_init_info)\n     return\n end\n \n+function _table_init_function(info::duckdb_init_info)\n+    main_function = unsafe_pointer_to_objref(duckdb_init_get_extra_info(info))\n+    return _table_init_function_generic(info, main_function.init_func)\n+end\n+\n+function _table_local_init_function(info::duckdb_init_info)\n+    main_function = unsafe_pointer_to_objref(duckdb_init_get_extra_info(info))\n+    return _table_init_function_generic(info, main_function.init_local_func)\n+end\n+\n function get_bind_info(info::InitInfo, ::Type{T})::T where {T}\n     return unsafe_pointer_to_objref(duckdb_init_get_bind_data(info.handle)).info\n end\n@@ -101,6 +126,10 @@ function get_extra_data(info::InitInfo)\n     return info.main_function.extra_data\n end\n \n+function set_max_threads(info::InitInfo, max_threads)\n+    return duckdb_init_set_max_threads(info.handle, max_threads)\n+end\n+\n function get_projected_columns(info::InitInfo)::Vector{Int64}\n     result::Vector{Int64} = Vector()\n     column_count = duckdb_init_get_column_count(info.handle)\n@@ -110,12 +139,16 @@ function get_projected_columns(info::InitInfo)::Vector{Int64}\n     return result\n end\n \n+function _empty_init_info(info::DuckDB.InitInfo)\n+    return missing\n+end\n+\n #=\n //===--------------------------------------------------------------------===//\n // Main Table Function\n //===--------------------------------------------------------------------===//\n =#\n-mutable struct FunctionInfo\n+struct FunctionInfo\n     handle::duckdb_function_info\n     main_function::Any\n \n@@ -133,6 +166,10 @@ function get_init_info(info::FunctionInfo, ::Type{T})::T where {T}\n     return unsafe_pointer_to_objref(duckdb_function_get_init_data(info.handle)).info\n end\n \n+function get_local_info(info::FunctionInfo, ::Type{T})::T where {T}\n+    return unsafe_pointer_to_objref(duckdb_function_get_local_init_data(info.handle)).info\n+end\n+\n function _table_main_function(info::duckdb_function_info, chunk::duckdb_data_chunk)\n     main_function::TableFunction = unsafe_pointer_to_objref(duckdb_function_get_extra_info(info))\n     binfo::FunctionInfo = FunctionInfo(info, main_function)\n@@ -156,15 +193,18 @@ mutable struct TableFunction\n     handle::duckdb_table_function\n     bind_func::Function\n     init_func::Function\n+    init_local_func::Function\n     main_func::Function\n     extra_data::Any\n     global_objects::Set{Any}\n+    global_lock::ReentrantLock\n \n     function TableFunction(\n         name::AbstractString,\n         parameters::Vector{LogicalType},\n         bind_func::Function,\n         init_func::Function,\n+        init_local_func::Function,\n         main_func::Function,\n         extra_data::Any,\n         projection_pushdown::Bool\n@@ -174,12 +214,13 @@ mutable struct TableFunction\n         for param in parameters\n             duckdb_table_function_add_parameter(handle, param.handle)\n         end\n-        result = new(handle, bind_func, init_func, main_func, extra_data, Set())\n+        result = new(handle, bind_func, init_func, init_local_func, main_func, extra_data, Set(), ReentrantLock())\n         finalizer(_destroy_table_function, result)\n \n         duckdb_table_function_set_extra_info(handle, pointer_from_objref(result))\n         duckdb_table_function_set_bind(handle, @cfunction(_table_bind_function, Cvoid, (duckdb_bind_info,)))\n         duckdb_table_function_set_init(handle, @cfunction(_table_init_function, Cvoid, (duckdb_init_info,)))\n+        duckdb_table_function_set_local_init(handle, @cfunction(_table_local_init_function, Cvoid, (duckdb_init_info,)))\n         duckdb_table_function_set_function(\n             handle,\n             @cfunction(_table_main_function, Cvoid, (duckdb_function_info, duckdb_data_chunk))\n@@ -206,9 +247,22 @@ function create_table_function(\n     init_func::Function,\n     main_func::Function,\n     extra_data::Any = missing,\n-    projection_pushdown::Bool = false\n+    projection_pushdown::Bool = false,\n+    init_local_func::Union{Missing, Function} = missing\n )\n-    fun = TableFunction(name, parameters, bind_func, init_func, main_func, extra_data, projection_pushdown)\n+    if init_local_func === missing\n+        init_local_func = _empty_init_info\n+    end\n+    fun = TableFunction(\n+        name,\n+        parameters,\n+        bind_func,\n+        init_func,\n+        init_local_func,\n+        main_func,\n+        extra_data,\n+        projection_pushdown\n+    )\n     if duckdb_register_table_function(con.handle, fun.handle) != DuckDBSuccess\n         throw(QueryException(string(\"Failed to register table function \\\"\", name, \"\\\"\")))\n     end\n@@ -224,7 +278,8 @@ function create_table_function(\n     init_func::Function,\n     main_func::Function,\n     extra_data::Any = missing,\n-    projection_pushdown::Bool = false\n+    projection_pushdown::Bool = false,\n+    init_local_func::Union{Missing, Function} = missing\n )\n     parameter_types::Vector{LogicalType} = Vector()\n     for parameter_type in parameters\n@@ -238,7 +293,8 @@ function create_table_function(\n         init_func,\n         main_func,\n         extra_data,\n-        projection_pushdown\n+        projection_pushdown,\n+        init_local_func\n     )\n end\n \n@@ -250,7 +306,8 @@ function create_table_function(\n     init_func::Function,\n     main_func::Function,\n     extra_data::Any = missing,\n-    projection_pushdown::Bool = false\n+    projection_pushdown::Bool = false,\n+    init_local_func::Union{Missing, Function} = missing\n )\n     return create_table_function(\n         db.main_connection,\n@@ -260,7 +317,8 @@ function create_table_function(\n         init_func,\n         main_func,\n         extra_data,\n-        projection_pushdown\n+        projection_pushdown,\n+        init_local_func\n     )\n end\n \n@@ -272,7 +330,8 @@ function create_table_function(\n     init_func::Function,\n     main_func::Function,\n     extra_data::Any = missing,\n-    projection_pushdown::Bool = false\n+    projection_pushdown::Bool = false,\n+    init_local_func::Union{Missing, Function} = missing\n )\n     return create_table_function(\n         db.main_connection,\n@@ -282,6 +341,7 @@ function create_table_function(\n         init_func,\n         main_func,\n         extra_data,\n-        projection_pushdown\n+        projection_pushdown,\n+        init_local_func\n     )\n end\ndiff --git a/tools/pythonpkg/clean.py b/tools/pythonpkg/clean.py\nindex e2e27045bb08..d1a215bbe832 100644\n--- a/tools/pythonpkg/clean.py\n+++ b/tools/pythonpkg/clean.py\n@@ -1,11 +1,15 @@\n-import duckdb, shutil, os\n+import os, sys, shutil\n+# avoid importing from the current directory\n+sys.path = sys.path[1:]\n \n-# try:\n-base_dir = duckdb.__file__.rsplit(os.path.sep, 1)[0]\n-for fpath in os.listdir(base_dir):\n-    if '_duckdb_extension' in fpath:\n-        full_path = os.path.join(base_dir, fpath)\n-        if os.path.isfile(full_path):\n-            os.remove(full_path)\n-        else:\n-            shutil.rmtree(os.path.join(base_dir, fpath))\n+try:\n+    import duckdb\n+except:\n+    exit(0)\n+next_dir = duckdb.__file__\n+while 'duckdb' in next_dir:\n+    base_dir = next_dir\n+    next_dir = next_dir.rsplit(os.path.sep, 1)[0]\n+if 'duckdb' not in base_dir:\n+    raise Exception(\"Failed to find DuckDB path to delete\")\n+shutil.rmtree(base_dir)\ndiff --git a/tools/pythonpkg/src/arrow_array_stream.cpp b/tools/pythonpkg/src/arrow_array_stream.cpp\nindex 7490f07942ab..fe605a0523ab 100644\n--- a/tools/pythonpkg/src/arrow_array_stream.cpp\n+++ b/tools/pythonpkg/src/arrow_array_stream.cpp\n@@ -12,9 +12,9 @@ namespace duckdb {\n \n py::object PythonTableArrowArrayStreamFactory::ProduceScanner(\n     py::object &arrow_scanner, py::handle &arrow_obj_handle,\n-    std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns, TableFilterCollection *filters,\n+    std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns, TableFilterSet *filters,\n     ClientConfig &config) {\n-\tbool has_filter = filters && filters->table_filters && !filters->table_filters->filters.empty();\n+\tbool has_filter = filters && !filters->filters.empty();\n \tpy::list projection_list = py::cast(project_columns.second);\n \tif (has_filter) {\n \t\tauto filter = TransformFilter(*filters, project_columns.first, config);\n@@ -33,7 +33,7 @@ py::object PythonTableArrowArrayStreamFactory::ProduceScanner(\n }\n unique_ptr<ArrowArrayStreamWrapper> PythonTableArrowArrayStreamFactory::Produce(\n     uintptr_t factory_ptr, std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns,\n-    TableFilterCollection *filters) {\n+    TableFilterSet *filters) {\n \tpy::gil_scoped_acquire acquire;\n \tPythonTableArrowArrayStreamFactory *factory = (PythonTableArrowArrayStreamFactory *)factory_ptr;\n \tD_ASSERT(factory->arrow_object);\n@@ -110,6 +110,18 @@ py::object GetScalar(Value &constant, const string &timezone_config) {\n \t\tpy::object date_type = py::module_::import(\"pyarrow\").attr(\"timestamp\");\n \t\treturn dataset_scalar(scalar(constant.GetValue<int64_t>(), date_type(\"us\")));\n \t}\n+\tcase LogicalTypeId::TIMESTAMP_MS: {\n+\t\tpy::object date_type = py::module_::import(\"pyarrow\").attr(\"timestamp\");\n+\t\treturn dataset_scalar(scalar(constant.GetValue<int64_t>(), date_type(\"ms\")));\n+\t}\n+\tcase LogicalTypeId::TIMESTAMP_NS: {\n+\t\tpy::object date_type = py::module_::import(\"pyarrow\").attr(\"timestamp\");\n+\t\treturn dataset_scalar(scalar(constant.GetValue<int64_t>(), date_type(\"ns\")));\n+\t}\n+\tcase LogicalTypeId::TIMESTAMP_SEC: {\n+\t\tpy::object date_type = py::module_::import(\"pyarrow\").attr(\"timestamp\");\n+\t\treturn dataset_scalar(scalar(constant.GetValue<int64_t>(), date_type(\"s\")));\n+\t}\n \tcase LogicalTypeId::TIMESTAMP_TZ: {\n \t\tpy::object date_type = py::module_::import(\"pyarrow\").attr(\"timestamp\");\n \t\treturn dataset_scalar(scalar(constant.GetValue<int64_t>(), date_type(\"us\", py::arg(\"tz\") = timezone_config)));\n@@ -223,10 +235,10 @@ py::object TransformFilterRecursive(TableFilter *filter, const string &column_na\n \t}\n }\n \n-py::object PythonTableArrowArrayStreamFactory::TransformFilter(TableFilterCollection &filter_collection,\n+py::object PythonTableArrowArrayStreamFactory::TransformFilter(TableFilterSet &filter_collection,\n                                                                std::unordered_map<idx_t, string> &columns,\n                                                                ClientConfig &config) {\n-\tauto filters_map = &filter_collection.table_filters->filters;\n+\tauto filters_map = &filter_collection.filters;\n \tauto it = filters_map->begin();\n \tD_ASSERT(columns.find(it->first) != columns.end());\n \tstring timezone_config = ClientConfig::ExtractTimezoneFromConfig(config);\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/arrow_array_stream.hpp b/tools/pythonpkg/src/include/duckdb_python/arrow_array_stream.hpp\nindex 7ca9f3997d74..70c44f47bcac 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/arrow_array_stream.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/arrow_array_stream.hpp\n@@ -28,7 +28,7 @@ class PythonTableArrowArrayStreamFactory {\n \t//! Produces an Arrow Scanner, should be only called once when initializing Scan States\n \tstatic unique_ptr<ArrowArrayStreamWrapper>\n \tProduce(uintptr_t factory, std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns,\n-\t        TableFilterCollection *filters = nullptr);\n+\t        TableFilterSet *filters = nullptr);\n \n \t//! Get the schema of the arrow object\n \tstatic void GetSchema(uintptr_t factory_ptr, ArrowSchemaWrapper &schema);\n@@ -39,12 +39,12 @@ class PythonTableArrowArrayStreamFactory {\n \tClientConfig &config;\n \n private:\n-\t//! We transform a TableFilterCollection to an Arrow Expression Object\n-\tstatic py::object TransformFilter(TableFilterCollection &filters, std::unordered_map<idx_t, string> &columns,\n+\t//! We transform a TableFilterSet to an Arrow Expression Object\n+\tstatic py::object TransformFilter(TableFilterSet &filters, std::unordered_map<idx_t, string> &columns,\n \t                                  ClientConfig &config);\n \n \tstatic py::object ProduceScanner(py::object &arrow_scanner, py::handle &arrow_obj_handle,\n \t                                 std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns,\n-\t                                 TableFilterCollection *filters, ClientConfig &config);\n+\t                                 TableFilterSet *filters, ClientConfig &config);\n };\n } // namespace duckdb\n\\ No newline at end of file\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/map.hpp b/tools/pythonpkg/src/include/duckdb_python/map.hpp\nindex 2b40b2e0903c..014de69982c9 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/map.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/map.hpp\n@@ -22,8 +22,8 @@ struct MapFunction : public TableFunction {\n \tstatic unique_ptr<FunctionData> MapFunctionBind(ClientContext &context, TableFunctionBindInput &input,\n \t                                                vector<LogicalType> &return_types, vector<string> &names);\n \n-\tstatic OperatorResultType MapFunctionExec(ClientContext &context, const FunctionData *bind_data_p,\n-\t                                          FunctionOperatorData *state_p, DataChunk &input, DataChunk &output);\n+\tstatic OperatorResultType MapFunctionExec(ClientContext &context, TableFunctionInput &data, DataChunk &input,\n+\t                                          DataChunk &output);\n };\n \n } // namespace duckdb\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pandas_scan.hpp b/tools/pythonpkg/src/include/duckdb_python/pandas_scan.hpp\nindex bc2480b4e9cc..e120d3ccab1a 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pandas_scan.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pandas_scan.hpp\n@@ -25,39 +25,27 @@ struct PandasScanFunction : public TableFunction {\n \tstatic unique_ptr<FunctionData> PandasScanBind(ClientContext &context, TableFunctionBindInput &input,\n \t                                               vector<LogicalType> &return_types, vector<string> &names);\n \n-\tstatic unique_ptr<FunctionOperatorData> PandasScanInit(ClientContext &context, const FunctionData *bind_data_p,\n-\t                                                       const vector<column_t> &column_ids,\n-\t                                                       TableFilterCollection *filters);\n+\tstatic unique_ptr<GlobalTableFunctionState> PandasScanInitGlobal(ClientContext &context,\n+\t                                                                 TableFunctionInitInput &input);\n+\tstatic unique_ptr<LocalTableFunctionState>\n+\tPandasScanInitLocal(ClientContext &context, TableFunctionInitInput &input, GlobalTableFunctionState *gstate);\n \n \tstatic idx_t PandasScanMaxThreads(ClientContext &context, const FunctionData *bind_data_p);\n \n-\tstatic unique_ptr<ParallelState> PandasScanInitParallelState(ClientContext &context,\n-\t                                                             const FunctionData *bind_data_p,\n-\t                                                             const vector<column_t> &column_ids,\n-\t                                                             TableFilterCollection *filters);\n-\n-\tstatic unique_ptr<FunctionOperatorData>\n-\tPandasScanParallelInit(ClientContext &context, const FunctionData *bind_data_p, ParallelState *state,\n-\t                       const vector<column_t> &column_ids, TableFilterCollection *filters);\n-\n \tstatic bool PandasScanParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,\n-\t                                        FunctionOperatorData *operator_state, ParallelState *parallel_state_p);\n+\t                                        LocalTableFunctionState *lstate, GlobalTableFunctionState *gstate);\n \n-\tstatic double PandasProgress(ClientContext &context, const FunctionData *bind_data_p);\n+\tstatic double PandasProgress(ClientContext &context, const FunctionData *bind_data_p,\n+\t                             const GlobalTableFunctionState *gstate);\n \n \t//! The main pandas scan function: note that this can be called in parallel without the GIL\n \t//! hence this needs to be GIL-safe, i.e. no methods that create Python objects are allowed\n-\tstatic void PandasScanFunc(ClientContext &context, const FunctionData *bind_data,\n-\t                           FunctionOperatorData *operator_state, DataChunk &output);\n-\n-\tstatic void PandasScanFuncParallel(ClientContext &context, const FunctionData *bind_data,\n-\t                                   FunctionOperatorData *operator_state, DataChunk &output,\n-\t                                   ParallelState *parallel_state_p);\n+\tstatic void PandasScanFunc(ClientContext &context, TableFunctionInput &data_p, DataChunk &output);\n \n \tstatic unique_ptr<NodeStatistics> PandasScanCardinality(ClientContext &context, const FunctionData *bind_data);\n \n \tstatic idx_t PandasScanGetBatchIndex(ClientContext &context, const FunctionData *bind_data_p,\n-\t                                     FunctionOperatorData *operator_state, ParallelState *parallel_state_p);\n+\t                                     LocalTableFunctionState *local_state, GlobalTableFunctionState *global_state);\n \n \t// Helper function that transform pandas df names to make them work with our binder\n \tstatic py::object PandasReplaceCopiedNames(const py::object &original_df);\ndiff --git a/tools/pythonpkg/src/map.cpp b/tools/pythonpkg/src/map.cpp\nindex 1bf780e6e025..54fa08f29382 100644\n--- a/tools/pythonpkg/src/map.cpp\n+++ b/tools/pythonpkg/src/map.cpp\n@@ -67,15 +67,15 @@ static string TypeVectorToString(vector<LogicalType> &types) {\n \treturn StringUtil::Join(types, types.size(), \", \", [](const LogicalType &argument) { return argument.ToString(); });\n }\n \n-OperatorResultType MapFunction::MapFunctionExec(ClientContext &context, const FunctionData *bind_data_p,\n-                                                FunctionOperatorData *state_p, DataChunk &input, DataChunk &output) {\n+OperatorResultType MapFunction::MapFunctionExec(ClientContext &context, TableFunctionInput &data_p, DataChunk &input,\n+                                                DataChunk &output) {\n \tpy::gil_scoped_acquire acquire;\n \n \tif (input.size() == 0) {\n \t\treturn OperatorResultType::NEED_MORE_INPUT;\n \t}\n \n-\tauto &data = (MapFunctionData &)*bind_data_p;\n+\tauto &data = (MapFunctionData &)*data_p.bind_data;\n \n \tD_ASSERT(input.GetTypes() == data.in_types);\n \tNumpyResultConversion conversion(data.in_types, input.size());\ndiff --git a/tools/pythonpkg/src/pandas_scan.cpp b/tools/pythonpkg/src/pandas_scan.cpp\nindex dbb0d1fe1a09..a6337c925d65 100644\n--- a/tools/pythonpkg/src/pandas_scan.cpp\n+++ b/tools/pythonpkg/src/pandas_scan.cpp\n@@ -1,6 +1,5 @@\n #include \"duckdb_python/pandas_scan.hpp\"\n #include \"duckdb_python/array_wrapper.hpp\"\n-#include \"duckdb/parallel/parallel_state.hpp\"\n #include \"utf8proc_wrapper.hpp\"\n #include \"duckdb/common/types/timestamp.hpp\"\n #include \"duckdb_python/vector_conversion.hpp\"\n@@ -28,8 +27,8 @@ struct PandasScanFunctionData : public TableFunctionData {\n \t}\n };\n \n-struct PandasScanState : public FunctionOperatorData {\n-\tPandasScanState(idx_t start, idx_t end) : start(start), end(end), batch_index(0) {\n+struct PandasScanLocalState : public LocalTableFunctionState {\n+\tPandasScanLocalState(idx_t start, idx_t end) : start(start), end(end), batch_index(0) {\n \t}\n \n \tidx_t start;\n@@ -38,28 +37,33 @@ struct PandasScanState : public FunctionOperatorData {\n \tvector<column_t> column_ids;\n };\n \n-struct ParallelPandasScanState : public ParallelState {\n-\tParallelPandasScanState() : position(0), batch_index(0) {\n+struct PandasScanGlobalState : public GlobalTableFunctionState {\n+\texplicit PandasScanGlobalState(idx_t max_threads) : position(0), batch_index(0), max_threads(max_threads) {\n \t}\n \n \tstd::mutex lock;\n \tidx_t position;\n \tidx_t batch_index;\n+\tidx_t max_threads;\n+\n+\tidx_t MaxThreads() const override {\n+\t\treturn max_threads;\n+\t}\n };\n \n PandasScanFunction::PandasScanFunction()\n-    : TableFunction(\"pandas_scan\", {LogicalType::POINTER}, PandasScanFunc, PandasScanBind, PandasScanInit, nullptr,\n-                    nullptr, nullptr, PandasScanCardinality, nullptr, nullptr, PandasScanMaxThreads,\n-                    PandasScanInitParallelState, PandasScanFuncParallel, PandasScanParallelInit,\n-                    PandasScanParallelStateNext, true, false, PandasProgress) {\n+    : TableFunction(\"pandas_scan\", {LogicalType::POINTER}, PandasScanFunc, PandasScanBind, PandasScanInitGlobal,\n+                    PandasScanInitLocal) {\n \tget_batch_index = PandasScanGetBatchIndex;\n-\tsupports_batch_index = true;\n+\tcardinality = PandasScanCardinality;\n+\ttable_scan_progress = PandasProgress;\n+\tprojection_pushdown = true;\n }\n \n idx_t PandasScanFunction::PandasScanGetBatchIndex(ClientContext &context, const FunctionData *bind_data_p,\n-                                                  FunctionOperatorData *operator_state,\n-                                                  ParallelState *parallel_state_p) {\n-\tauto &data = (PandasScanState &)*operator_state;\n+                                                  LocalTableFunctionState *local_state,\n+                                                  GlobalTableFunctionState *global_state) {\n+\tauto &data = (PandasScanLocalState &)*local_state;\n \treturn data.batch_index;\n }\n \n@@ -78,13 +82,17 @@ unique_ptr<FunctionData> PandasScanFunction::PandasScanBind(ClientContext &conte\n \treturn make_unique<PandasScanFunctionData>(df, row_count, move(pandas_bind_data), return_types);\n }\n \n-unique_ptr<FunctionOperatorData> PandasScanFunction::PandasScanInit(ClientContext &context,\n-                                                                    const FunctionData *bind_data_p,\n-                                                                    const vector<column_t> &column_ids,\n-                                                                    TableFilterCollection *filters) {\n-\tauto &bind_data = (const PandasScanFunctionData &)*bind_data_p;\n-\tauto result = make_unique<PandasScanState>(0, bind_data.row_count);\n-\tresult->column_ids = column_ids;\n+unique_ptr<GlobalTableFunctionState> PandasScanFunction::PandasScanInitGlobal(ClientContext &context,\n+                                                                              TableFunctionInitInput &input) {\n+\treturn make_unique<PandasScanGlobalState>(PandasScanMaxThreads(context, input.bind_data));\n+}\n+\n+unique_ptr<LocalTableFunctionState> PandasScanFunction::PandasScanInitLocal(ClientContext &context,\n+                                                                            TableFunctionInitInput &input,\n+                                                                            GlobalTableFunctionState *gstate) {\n+\tauto result = make_unique<PandasScanLocalState>(0, 0);\n+\tresult->column_ids = input.column_ids;\n+\tPandasScanParallelStateNext(context, input.bind_data, result.get(), gstate);\n \treturn move(result);\n }\n \n@@ -96,32 +104,12 @@ idx_t PandasScanFunction::PandasScanMaxThreads(ClientContext &context, const Fun\n \treturn bind_data.row_count / PANDAS_PARTITION_COUNT + 1;\n }\n \n-unique_ptr<ParallelState> PandasScanFunction::PandasScanInitParallelState(ClientContext &context,\n-                                                                          const FunctionData *bind_data_p,\n-                                                                          const vector<column_t> &column_ids,\n-                                                                          TableFilterCollection *filters) {\n-\treturn make_unique<ParallelPandasScanState>();\n-}\n-\n-unique_ptr<FunctionOperatorData> PandasScanFunction::PandasScanParallelInit(ClientContext &context,\n-                                                                            const FunctionData *bind_data_p,\n-                                                                            ParallelState *state,\n-                                                                            const vector<column_t> &column_ids,\n-                                                                            TableFilterCollection *filters) {\n-\tauto result = make_unique<PandasScanState>(0, 0);\n-\tresult->column_ids = column_ids;\n-\tif (!PandasScanParallelStateNext(context, bind_data_p, result.get(), state)) {\n-\t\treturn nullptr;\n-\t}\n-\treturn move(result);\n-}\n-\n bool PandasScanFunction::PandasScanParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,\n-                                                     FunctionOperatorData *operator_state,\n-                                                     ParallelState *parallel_state_p) {\n+                                                     LocalTableFunctionState *lstate,\n+                                                     GlobalTableFunctionState *gstate) {\n \tauto &bind_data = (const PandasScanFunctionData &)*bind_data_p;\n-\tauto &parallel_state = (ParallelPandasScanState &)*parallel_state_p;\n-\tauto &state = (PandasScanState &)*operator_state;\n+\tauto &parallel_state = (PandasScanGlobalState &)*gstate;\n+\tauto &state = (PandasScanLocalState &)*lstate;\n \n \tlock_guard<mutex> parallel_lock(parallel_state.lock);\n \tif (parallel_state.position >= bind_data.row_count) {\n@@ -137,7 +125,8 @@ bool PandasScanFunction::PandasScanParallelStateNext(ClientContext &context, con\n \treturn true;\n }\n \n-double PandasScanFunction::PandasProgress(ClientContext &context, const FunctionData *bind_data_p) {\n+double PandasScanFunction::PandasProgress(ClientContext &context, const FunctionData *bind_data_p,\n+                                          const GlobalTableFunctionState *gstate) {\n \tauto &bind_data = (const PandasScanFunctionData &)*bind_data_p;\n \tif (bind_data.row_count == 0) {\n \t\treturn 100;\n@@ -146,25 +135,16 @@ double PandasScanFunction::PandasProgress(ClientContext &context, const Function\n \treturn percentage;\n }\n \n-void PandasScanFunction::PandasScanFuncParallel(ClientContext &context, const FunctionData *bind_data,\n-                                                FunctionOperatorData *operator_state, DataChunk &output,\n-                                                ParallelState *parallel_state_p) {\n-\t//! FIXME: Have specialized parallel function from pandas scan here\n-\tPandasScanFunc(context, bind_data, operator_state, output);\n-}\n-\n //! The main pandas scan function: note that this can be called in parallel without the GIL\n //! hence this needs to be GIL-safe, i.e. no methods that create Python objects are allowed\n-void PandasScanFunction::PandasScanFunc(ClientContext &context, const FunctionData *bind_data,\n-                                        FunctionOperatorData *operator_state, DataChunk &output) {\n-\tif (!operator_state) {\n-\t\treturn;\n-\t}\n-\tauto &data = (PandasScanFunctionData &)*bind_data;\n-\tauto &state = (PandasScanState &)*operator_state;\n+void PandasScanFunction::PandasScanFunc(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (PandasScanFunctionData &)*data_p.bind_data;\n+\tauto &state = (PandasScanLocalState &)*data_p.local_state;\n \n \tif (state.start >= state.end) {\n-\t\treturn;\n+\t\tif (!PandasScanParallelStateNext(context, data_p.bind_data, data_p.local_state, data_p.global_state)) {\n+\t\t\treturn;\n+\t\t}\n \t}\n \tidx_t this_count = std::min((idx_t)STANDARD_VECTOR_SIZE, state.end - state.start);\n \toutput.SetCardinality(this_count);\ndiff --git a/tools/pythonpkg/src/pyrelation.cpp b/tools/pythonpkg/src/pyrelation.cpp\nindex 311ede082f22..3aafc7c90f18 100644\n--- a/tools/pythonpkg/src/pyrelation.cpp\n+++ b/tools/pythonpkg/src/pyrelation.cpp\n@@ -514,7 +514,10 @@ unique_ptr<DuckDBPyRelation> DuckDBPyRelation::CreateView(const string &view_nam\n \n unique_ptr<DuckDBPyResult> DuckDBPyRelation::Query(const string &view_name, const string &sql_query) {\n \tauto res = make_unique<DuckDBPyResult>();\n-\tres->result = rel->Query(view_name, sql_query);\n+\t{\n+\t\tpy::gil_scoped_release release;\n+\t\tres->result = rel->Query(view_name, sql_query);\n+\t}\n \tif (!res->result->success) {\n \t\tthrow std::runtime_error(res->result->error);\n \t}\ndiff --git a/tools/rpkg/src/register.cpp b/tools/rpkg/src/register.cpp\nindex 8f0effb1cfca..fb5643f26dd2 100644\n--- a/tools/rpkg/src/register.cpp\n+++ b/tools/rpkg/src/register.cpp\n@@ -47,7 +47,7 @@ class RArrowTabularStreamFactory {\n \n \tstatic unique_ptr<ArrowArrayStreamWrapper>\n \tProduce(uintptr_t factory_p, std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns,\n-\t        TableFilterCollection *filters) {\n+\t        TableFilterSet *filters) {\n \n \t\tRProtector r;\n \t\tauto res = make_unique<ArrowArrayStreamWrapper>();\n@@ -62,7 +62,7 @@ class RArrowTabularStreamFactory {\n \t\t} else {\n \t\t\tauto projection_sexp = r.Protect(StringsToSexp(project_columns.second));\n \t\t\tSEXP filters_sexp = r.Protect(Rf_ScalarLogical(true));\n-\t\t\tif (filters && filters->table_filters && !filters->table_filters->filters.empty()) {\n+\t\t\tif (filters && !filters->filters.empty()) {\n \t\t\t\tauto timezone_config = ClientConfig::ExtractTimezoneFromConfig(factory->config);\n \t\t\t\tfilters_sexp =\n \t\t\t\t    r.Protect(TransformFilter(*filters, project_columns.first, factory->export_fun, timezone_config));\n@@ -162,14 +162,14 @@ class RArrowTabularStreamFactory {\n \t\treturn conjunction_sexp;\n \t}\n \n-\tstatic SEXP TransformFilter(TableFilterCollection &filter_collection, std::unordered_map<idx_t, string> &columns,\n+\tstatic SEXP TransformFilter(TableFilterSet &filter_collection, std::unordered_map<idx_t, string> &columns,\n \t                            SEXP functions, string &timezone_config) {\n \t\tRProtector r;\n \n-\t\tauto fit = filter_collection.table_filters->filters.begin();\n+\t\tauto fit = filter_collection.filters.begin();\n \t\tSEXP res = r.Protect(TransformFilterExpression(*fit->second, columns[fit->first], functions, timezone_config));\n \t\tfit++;\n-\t\tfor (; fit != filter_collection.table_filters->filters.end(); ++fit) {\n+\t\tfor (; fit != filter_collection.filters.end(); ++fit) {\n \t\t\tSEXP rhs =\n \t\t\t    r.Protect(TransformFilterExpression(*fit->second, columns[fit->first], functions, timezone_config));\n \t\t\tres = r.Protect(CreateExpression(functions, \"and_kleene\", res, rhs));\ndiff --git a/tools/rpkg/src/scan.cpp b/tools/rpkg/src/scan.cpp\nindex 8a5b764bab88..da6cdafa213a 100644\n--- a/tools/rpkg/src/scan.cpp\n+++ b/tools/rpkg/src/scan.cpp\n@@ -42,7 +42,7 @@ struct DataFrameScanFunctionData : public TableFunctionData {\n \tvector<RType> rtypes;\n };\n \n-struct DataFrameScanState : public FunctionOperatorData {\n+struct DataFrameScanState : public GlobalTableFunctionState {\n \tDataFrameScanState() : position(0) {\n \t}\n \n@@ -119,16 +119,13 @@ static unique_ptr<FunctionData> dataframe_scan_bind(ClientContext &context, Tabl\n \treturn make_unique<DataFrameScanFunctionData>(df, row_count, rtypes);\n }\n \n-static unique_ptr<FunctionOperatorData> dataframe_scan_init(ClientContext &context, const FunctionData *bind_data,\n-                                                            const vector<column_t> &column_ids,\n-                                                            TableFilterCollection *filters) {\n+static unique_ptr<GlobalTableFunctionState> dataframe_scan_init(ClientContext &context, TableFunctionInitInput &input) {\n \treturn make_unique<DataFrameScanState>();\n }\n \n-static void dataframe_scan_function(ClientContext &context, const FunctionData *bind_data,\n-                                    FunctionOperatorData *operator_state, DataChunk &output) {\n-\tauto &data = (DataFrameScanFunctionData &)*bind_data;\n-\tauto &state = (DataFrameScanState &)*operator_state;\n+static void dataframe_scan_function(ClientContext &context, TableFunctionInput &input, DataChunk &output) {\n+\tauto &data = (DataFrameScanFunctionData &)*input.bind_data;\n+\tauto &state = (DataFrameScanState &)*input.global_state;\n \tif (state.position >= data.row_count) {\n \t\treturn;\n \t}\n@@ -261,4 +258,6 @@ static unique_ptr<NodeStatistics> dataframe_scan_cardinality(ClientContext &cont\n \n DataFrameScanFunction::DataFrameScanFunction()\n     : TableFunction(\"r_dataframe_scan\", {LogicalType::POINTER}, dataframe_scan_function, dataframe_scan_bind,\n-                    dataframe_scan_init, nullptr, nullptr, nullptr, dataframe_scan_cardinality) {};\n+                    dataframe_scan_init) {\n+\tcardinality = dataframe_scan_cardinality;\n+};\n",
  "test_patch": "diff --git a/scripts/regression_test_python.py b/scripts/regression_test_python.py\nnew file mode 100644\nindex 000000000000..6340aa99abb8\n--- /dev/null\n+++ b/scripts/regression_test_python.py\n@@ -0,0 +1,96 @@\n+import os\n+import sys\n+import duckdb\n+import pandas as pd\n+import pyarrow as pa\n+import time\n+\n+threads = None\n+verbose = False\n+out_file = None\n+nruns = 10\n+TPCH_NQUERIES = 22\n+\n+for arg in sys.argv:\n+    if arg == \"--verbose\":\n+        verbose = True\n+    elif arg.startswith(\"--threads=\"):\n+        threads = int(arg.replace(\"--threads=\", \"\"))\n+    elif arg.startswith(\"--nruns=\"):\n+        nruns = int(arg.replace(\"--nruns=\", \"\"))\n+    elif arg.startswith(\"--out-file=\"):\n+        out_file = arg.replace(\"--out-file=\", \"\")\n+\n+# generate data\n+if verbose:\n+    print(\"Generating TPC-H data\")\n+\n+main_con = duckdb.connect()\n+main_con.execute('CALL dbgen(sf=1)')\n+\n+tables = [\n+ \"customer\",\n+ \"lineitem\",\n+ \"nation\",\n+ \"orders\",\n+ \"part\",\n+ \"partsupp\",\n+ \"region\",\n+ \"supplier\",\n+]\n+\n+def open_connection():\n+    con = duckdb.connect()\n+    if threads is not None:\n+        if verbose:\n+            print(f'Limiting threads to {threads}')\n+        con.execute(f\"SET threads={threads}\")\n+    return con\n+\n+def benchmark_query(benchmark_name, con, query):\n+    if verbose:\n+        print(benchmark_name)\n+        print(query)\n+    for nrun in range(nruns):\n+        start = time.time()\n+        df_result = con.execute(query).df()\n+        end = time.time()\n+\n+        bench_result = f\"{benchmark_name}\\t{nrun}\\t{end - start}\"\n+\n+        if out_file is not None:\n+            f.write(bench_result)\n+            f.write('\\n')\n+        else:\n+            print(bench_result)\n+\n+def run_tpch(con, prefix):\n+    for i in range(1, TPCH_NQUERIES + 1):\n+        benchmark_name = \"%stpch_q%02d\" % (prefix, i)\n+        benchmark_query(benchmark_name, con, f'PRAGMA tpch({i})')\n+\n+\n+if out_file is not None:\n+    f = open(out_file, 'w+')\n+\n+# pandas scans\n+data_frames = {}\n+for table in tables:\n+    data_frames[table] = main_con.execute(f\"SELECT * FROM {table}\").df()\n+\n+df_con = open_connection()\n+for table in tables:\n+    df_con.register(table, data_frames[table])\n+\n+run_tpch(df_con, \"pandas_\")\n+\n+# # arrow scans\n+# arrow_tables = {}\n+# for table in tables:\n+#     arrow_tables[table] = pa.Table.from_pandas(data_frames[table])\n+#\n+# arrow_con = open_connection()\n+# for table in tables:\n+#     arrow_con.register(table, arrow_tables[table])\n+#\n+# run_tpch(arrow_con, \"arrow_\")\ndiff --git a/scripts/regression_test_runner.py b/scripts/regression_test_runner.py\nindex bf030979233d..2284b2435df7 100644\n--- a/scripts/regression_test_runner.py\n+++ b/scripts/regression_test_runner.py\n@@ -26,8 +26,8 @@\n         benchmark_file = arg.replace(\"--benchmarks=\", \"\")\n     elif arg == \"--verbose\":\n         verbose = True\n-    elif arg == \"--threads=\":\n-        threads = int(arg.replace(\"--threads=\"))\n+    elif arg.startswith(\"--threads=\"):\n+        threads = int(arg.replace(\"--threads=\", \"\"))\n \n if old_runner is None or new_runner is None or benchmark_file is None:\n     print(\"Expected usage: python3 scripts/regression_test_runner.py --old=/old/benchmark_runner --new=/new/benchmark_runner --benchmarks=/benchmark/list.csv\")\ndiff --git a/src/function/table/system/test_all_types.cpp b/src/function/table/system/test_all_types.cpp\nindex fbe4770d5154..4701f997e9dc 100644\n--- a/src/function/table/system/test_all_types.cpp\n+++ b/src/function/table/system/test_all_types.cpp\n@@ -7,7 +7,7 @@\n \n namespace duckdb {\n \n-struct TestAllTypesData : public FunctionOperatorData {\n+struct TestAllTypesData : public GlobalTableFunctionState {\n \tTestAllTypesData() : offset(0) {\n \t}\n \n@@ -209,8 +209,7 @@ static unique_ptr<FunctionData> TestAllTypesBind(ClientContext &context, TableFu\n \treturn nullptr;\n }\n \n-unique_ptr<FunctionOperatorData> TestAllTypesInit(ClientContext &context, const FunctionData *bind_data,\n-                                                  const vector<column_t> &column_ids, TableFilterCollection *filters) {\n+unique_ptr<GlobalTableFunctionState> TestAllTypesInit(ClientContext &context, TableFunctionInitInput &input) {\n \tauto result = make_unique<TestAllTypesData>();\n \tauto test_types = GetTestTypes();\n \t// 3 rows: min, max and NULL\n@@ -224,9 +223,8 @@ unique_ptr<FunctionOperatorData> TestAllTypesInit(ClientContext &context, const\n \treturn move(result);\n }\n \n-void TestAllTypesFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,\n-                          DataChunk &output) {\n-\tauto &data = (TestAllTypesData &)*operator_state;\n+void TestAllTypesFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (TestAllTypesData &)*data_p.global_state;\n \tif (data.offset >= data.entries.size()) {\n \t\t// finished returning values\n \t\treturn;\ndiff --git a/test/sql/types/timestamp/alternative_timestamp_casts.test b/test/sql/types/timestamp/alternative_timestamp_casts.test\nnew file mode 100644\nindex 000000000000..1dbab0d6c0fa\n--- /dev/null\n+++ b/test/sql/types/timestamp/alternative_timestamp_casts.test\n@@ -0,0 +1,21 @@\n+# name: test/sql/types/timestamp/alternative_timestamp_casts.test\n+# description: Test timestamp casts\n+# group: [timestamp]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+query I\n+SELECT DATE '1992-01-01'::TIMESTAMP_MS\n+----\n+1992-01-01 00:00:00\n+\n+query I\n+SELECT DATE '1992-01-01'::TIMESTAMP_S\n+----\n+1992-01-01 00:00:00\n+\n+query I\n+SELECT DATE '1992-01-01'::TIMESTAMP_NS\n+----\n+1992-01-01 00:00:00\ndiff --git a/tools/juliapkg/test/test_df_scan.jl b/tools/juliapkg/test/test_df_scan.jl\nindex 777358b0164c..d215377d5c9f 100644\n--- a/tools/juliapkg/test/test_df_scan.jl\n+++ b/tools/juliapkg/test/test_df_scan.jl\n@@ -137,3 +137,21 @@ end\n \n     DBInterface.close!(con)\n end\n+\n+@testset \"Test large DataFrame scan\" begin\n+    con = DBInterface.connect(DuckDB.DB)\n+\n+    my_df = DataFrame(DBInterface.execute(con, \"SELECT i%5 AS i FROM range(10000000) tbl(i)\"))\n+\n+    DuckDB.register_data_frame(con, my_df, \"my_df\")\n+    GC.gc()\n+\n+    results = DBInterface.execute(con, \"SELECT SUM(i) AS sum FROM my_df\")\n+    GC.gc()\n+    df = DataFrame(results)\n+    @test names(df) == [\"sum\"]\n+    @test size(df, 1) == 1\n+    @test df.sum == [20000000]\n+\n+    DBInterface.close!(con)\n+end\n",
  "problem_statement": "[Julia] DataFrame table registered with PooledArray fails on query\n#### What happens?\r\nWhen `register_data_frame()` is used to register a table that contains `PooledArrays`, the tables cannot be queried. The queries fails with the error below. Where as if all the columns of table are converted to `String` then they can be successfully queried using DuckDB. \r\n\r\n```\r\nExecute of query \"SELECT * from lookup LIMIT 10;\" failed: MethodError: no method matching df_scan_string_column(::PooledArrays.PooledVector{String15, UInt32, Vector{UInt32}}, ::Int64, ::Int64, ::Int64, ::Int64, ::DuckDB.DataChunk, ::Type{String15}, ::Type{String15})\r\nClosest candidates are:\r\n  df_scan_string_column(!Matched::Vector{DF_TYPE}, ::Int64, ::Int64, ::Int64, ::Int64, ::DuckDB.DataChunk, ::Type{DUCK_TYPE}, ::Type{DF_TYPE}) where {DUCK_TYPE, DF_TYPE} at ~/Desktop/duckdb/tools/juliapkg/src/data_frame_scan.jl:83\r\n\r\nStacktrace:\r\n [1] execute(stmt::DuckDB.Stmt, params::NamedTuple{(), Tuple{}})\r\n   @ DuckDB ~/Desktop/duckdb/tools/juliapkg/src/result.jl:537\r\n [2] execute\r\n   @ ~/Desktop/duckdb/tools/juliapkg/src/result.jl:615 [inlined]\r\n [3] execute\r\n   @ ~/.julia/packages/DBInterface/1Gmxx/src/DBInterface.jl:130 [inlined]\r\n [4] #execute#2\r\n   @ ~/.julia/packages/DBInterface/1Gmxx/src/DBInterface.jl:152 [inlined]\r\n [5] execute(conn::DuckDB.DB, sql::String)\r\n   @ DBInterface ~/.julia/packages/DBInterface/1Gmxx/src/DBInterface.jl:152\r\n [6] top-level scope\r\n   @ In[7]:18\r\n [7] eval\r\n   @ ./boot.jl:368 [inlined]\r\n [8] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\r\n   @ Base ./loading.jl:1277\r\n```\r\n\r\n\r\n#### To Reproduce\r\n\r\n```julia\r\nusing DuckDB\r\nusing DataFrames\r\nusing CSV\r\n\r\n# Read CSV into a DataFrame\r\ncsvfile = \"https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\"\r\nlookup = CSV.File(download(csvfile)) |> DataFrame\r\n\r\n# Create a DuckDB connection\r\ncon=DBInterface.connect(DuckDB.DB, \":memory:\")\r\n\r\n# Register DataFrame in DuckDB as a table\r\nDuckDB.register_data_frame(con, lookup, \"lookup\")\r\n\r\n# -------------- THIS FAILS ---------------- #\r\n# Query the table\r\n# This fails due to the presence of PooledArrays\r\nquery = \"\"\"SELECT * from lookup LIMIT 10;\"\"\"\r\nDBInterface.execute(con, query)\r\n\r\n\r\n# -------------- THIS WORKS ---------------- #\r\n\r\n# Modify the DataFrame (convert PooledArrays to String)\r\nlookup[!, :Borough] = string.(lookup.Borough)   \r\nlookup[!, :service_zone] = string.(lookup.service_zone)\r\nDuckDB.register_data_frame(con, lookup, \"lookup\") # Re-Register\r\n\r\n# Query (This works)\r\nquery = \"\"\"SELECT * from lookup LIMIT 10;\"\"\"\r\nDBInterface.execute(con, query)\r\n\r\n```\r\n\r\n\r\n\r\n\r\n#### Environment (please complete the following information):\r\n - OS: macOS 12.4 (M1)\r\n - DuckDB Version: `v0.3.2#master` \r\n - DuckDB Client: Julia\r\n\r\n#### Before Submitting\r\n\r\n- [x] **Have you tried this on the latest `master` branch?**\r\n\r\n\n[Julia] DataFrame table registered with PooledArray fails on query\n#### What happens?\r\nWhen `register_data_frame()` is used to register a table that contains `PooledArrays`, the tables cannot be queried. The queries fails with the error below. Where as if all the columns of table are converted to `String` then they can be successfully queried using DuckDB. \r\n\r\n```\r\nExecute of query \"SELECT * from lookup LIMIT 10;\" failed: MethodError: no method matching df_scan_string_column(::PooledArrays.PooledVector{String15, UInt32, Vector{UInt32}}, ::Int64, ::Int64, ::Int64, ::Int64, ::DuckDB.DataChunk, ::Type{String15}, ::Type{String15})\r\nClosest candidates are:\r\n  df_scan_string_column(!Matched::Vector{DF_TYPE}, ::Int64, ::Int64, ::Int64, ::Int64, ::DuckDB.DataChunk, ::Type{DUCK_TYPE}, ::Type{DF_TYPE}) where {DUCK_TYPE, DF_TYPE} at ~/Desktop/duckdb/tools/juliapkg/src/data_frame_scan.jl:83\r\n\r\nStacktrace:\r\n [1] execute(stmt::DuckDB.Stmt, params::NamedTuple{(), Tuple{}})\r\n   @ DuckDB ~/Desktop/duckdb/tools/juliapkg/src/result.jl:537\r\n [2] execute\r\n   @ ~/Desktop/duckdb/tools/juliapkg/src/result.jl:615 [inlined]\r\n [3] execute\r\n   @ ~/.julia/packages/DBInterface/1Gmxx/src/DBInterface.jl:130 [inlined]\r\n [4] #execute#2\r\n   @ ~/.julia/packages/DBInterface/1Gmxx/src/DBInterface.jl:152 [inlined]\r\n [5] execute(conn::DuckDB.DB, sql::String)\r\n   @ DBInterface ~/.julia/packages/DBInterface/1Gmxx/src/DBInterface.jl:152\r\n [6] top-level scope\r\n   @ In[7]:18\r\n [7] eval\r\n   @ ./boot.jl:368 [inlined]\r\n [8] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\r\n   @ Base ./loading.jl:1277\r\n```\r\n\r\n\r\n#### To Reproduce\r\n\r\n```julia\r\nusing DuckDB\r\nusing DataFrames\r\nusing CSV\r\n\r\n# Read CSV into a DataFrame\r\ncsvfile = \"https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\"\r\nlookup = CSV.File(download(csvfile)) |> DataFrame\r\n\r\n# Create a DuckDB connection\r\ncon=DBInterface.connect(DuckDB.DB, \":memory:\")\r\n\r\n# Register DataFrame in DuckDB as a table\r\nDuckDB.register_data_frame(con, lookup, \"lookup\")\r\n\r\n# -------------- THIS FAILS ---------------- #\r\n# Query the table\r\n# This fails due to the presence of PooledArrays\r\nquery = \"\"\"SELECT * from lookup LIMIT 10;\"\"\"\r\nDBInterface.execute(con, query)\r\n\r\n\r\n# -------------- THIS WORKS ---------------- #\r\n\r\n# Modify the DataFrame (convert PooledArrays to String)\r\nlookup[!, :Borough] = string.(lookup.Borough)   \r\nlookup[!, :service_zone] = string.(lookup.service_zone)\r\nDuckDB.register_data_frame(con, lookup, \"lookup\") # Re-Register\r\n\r\n# Query (This works)\r\nquery = \"\"\"SELECT * from lookup LIMIT 10;\"\"\"\r\nDBInterface.execute(con, query)\r\n\r\n```\r\n\r\n\r\n\r\n\r\n#### Environment (please complete the following information):\r\n - OS: macOS 12.4 (M1)\r\n - DuckDB Version: `v0.3.2#master` \r\n - DuckDB Client: Julia\r\n\r\n#### Before Submitting\r\n\r\n- [x] **Have you tried this on the latest `master` branch?**\r\n\r\n\n",
  "hints_text": "Thanks for the report! I will have a look at this.\nThanks for the report! I will have a look at this.",
  "created_at": "2022-06-05T15:08:37Z"
}