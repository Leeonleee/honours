{
  "repo": "duckdb/duckdb",
  "pull_number": 11297,
  "instance_id": "duckdb__duckdb-11297",
  "issue_numbers": [
    "9342",
    "9342"
  ],
  "base_commit": "2f58548f84103e18ea9ccfcab28de75714fecddd",
  "patch": "diff --git a/.github/config/out_of_tree_extensions.cmake b/.github/config/out_of_tree_extensions.cmake\nindex f3c0564557ef..11fc11c1c8d1 100644\n--- a/.github/config/out_of_tree_extensions.cmake\n+++ b/.github/config/out_of_tree_extensions.cmake\n@@ -4,6 +4,16 @@\n # to build duckdb with this configuration run:\n #   EXTENSION_CONFIGS=.github/config/out_of_tree_extensions.cmake make\n #\n+#  Note that many of these packages require vcpkg, and a merged manifest must be created to\n+#  compile multiple of them.\n+#\n+#  After setting up vcpkg, build using e.g. the following commands:\n+#  USE_MERGED_VCPKG_MANIFEST=1 BUILD_ALL_EXT=1 make extension_configuration\n+#  USE_MERGED_VCPKG_MANIFEST=1 BUILD_ALL_EXT=1 make debug\n+#\n+#  Make sure the VCPKG_TOOLCHAIN_PATH and VCPKG_TARGET_TRIPLET are set. For example:\n+#  VCPKG_TOOLCHAIN_PATH=~/vcpkg/scripts/buildsystems/vcpkg.cmake\n+#  VCPKG_TARGET_TRIPLET=arm64-osx\n \n ################# ARROW\n if (NOT WIN32)\n@@ -29,6 +39,7 @@ if (NOT MINGW)\n             LOAD_TESTS\n             GIT_URL https://github.com/duckdb/duckdb_azure\n             GIT_TAG 86f39d76157de970d16d6d6537bc90c0ee1c7d35\n+            APPLY_PATCHES\n             )\n endif()\n \n@@ -45,6 +56,7 @@ if (NOT MINGW)\n             ${LOAD_ICEBERG_TESTS}\n             GIT_URL https://github.com/duckdb/duckdb_iceberg\n             GIT_TAG 7aa3d8e4cb7b513d35fdacfa28dc328771bc4047\n+            APPLY_PATCHES\n             )\n endif()\n \n@@ -66,6 +78,7 @@ duckdb_extension_load(spatial\n     GIT_TAG 05c4ba01c500140287bf6946fb6910122e5c2acf\n     INCLUDE_DIR spatial/include\n     TEST_DIR test/sql\n+    APPLY_PATCHES\n     )\n \n ################# SQLITE_SCANNER\ndiff --git a/.github/patches/extensions/azure/open_file.patch b/.github/patches/extensions/azure/open_file.patch\nnew file mode 100644\nindex 000000000000..91a355979cb5\n--- /dev/null\n+++ b/.github/patches/extensions/azure/open_file.patch\n@@ -0,0 +1,439 @@\n+diff --git a/src/azure_blob_filesystem.cpp b/src/azure_blob_filesystem.cpp\n+index 6f4d0dc..93ead81 100644\n+--- a/src/azure_blob_filesystem.cpp\n++++ b/src/azure_blob_filesystem.cpp\n+@@ -66,21 +66,20 @@ AzureBlobContextState::GetBlobContainerClient(const std::string &blobContainerNa\n+ }\n+ \n+ //////// AzureBlobStorageFileHandle ////////\n+-AzureBlobStorageFileHandle::AzureBlobStorageFileHandle(AzureBlobStorageFileSystem &fs, string path, uint8_t flags,\n++AzureBlobStorageFileHandle::AzureBlobStorageFileHandle(AzureBlobStorageFileSystem &fs, string path, FileOpenFlags flags,\n+                                                        const AzureReadOptions &read_options,\n+                                                        Azure::Storage::Blobs::BlobClient blob_client)\n+     : AzureFileHandle(fs, std::move(path), flags, read_options), blob_client(std::move(blob_client)) {\n+ }\n+ \n+ //////// AzureBlobStorageFileSystem ////////\n+-unique_ptr<AzureFileHandle> AzureBlobStorageFileSystem::CreateHandle(const string &path, uint8_t flags,\n+-                                                                     FileLockType lock, FileCompressionType compression,\n+-                                                                     FileOpener *opener) {\n+-\tif (opener == nullptr) {\n++unique_ptr<AzureFileHandle> AzureBlobStorageFileSystem::CreateHandle(const string &path, FileOpenFlags flags,\n++                                                                     optional_ptr<FileOpener> opener) {\n++\tif (!opener) {\n+ \t\tthrow InternalException(\"Cannot do Azure storage CreateHandle without FileOpener\");\n+ \t}\n+ \n+-\tD_ASSERT(compression == FileCompressionType::UNCOMPRESSED);\n++\tD_ASSERT(flags.Compression() == FileCompressionType::UNCOMPRESSED);\n+ \n+ \tauto parsed_url = ParseUrl(path);\n+ \tauto storage_context = GetOrCreateStorageContext(opener, path, parsed_url);\n+@@ -202,7 +201,7 @@ void AzureBlobStorageFileSystem::ReadRange(AzureFileHandle &handle, idx_t file_o\n+ \t}\n+ }\n+ \n+-std::shared_ptr<AzureContextState> AzureBlobStorageFileSystem::CreateStorageContext(FileOpener *opener,\n++std::shared_ptr<AzureContextState> AzureBlobStorageFileSystem::CreateStorageContext(optional_ptr<FileOpener> opener,\n+                                                                                     const string &path,\n+                                                                                     const AzureParsedUrl &parsed_url) {\n+ \tauto azure_read_options = ParseAzureReadOptions(opener);\n+diff --git a/src/azure_dfs_filesystem.cpp b/src/azure_dfs_filesystem.cpp\n+index 34435ae..5ccbed0 100644\n+--- a/src/azure_dfs_filesystem.cpp\n++++ b/src/azure_dfs_filesystem.cpp\n+@@ -83,21 +83,20 @@ AzureDfsContextState::GetDfsFileSystemClient(const std::string &file_system_name\n+ }\n+ \n+ //////// AzureDfsContextState ////////\n+-AzureDfsStorageFileHandle::AzureDfsStorageFileHandle(AzureDfsStorageFileSystem &fs, string path, uint8_t flags,\n++AzureDfsStorageFileHandle::AzureDfsStorageFileHandle(AzureDfsStorageFileSystem &fs, string path, FileOpenFlags flags,\n+                                                      const AzureReadOptions &read_options,\n+                                                      Azure::Storage::Files::DataLake::DataLakeFileClient client)\n+     : AzureFileHandle(fs, std::move(path), flags, read_options), file_client(std::move(client)) {\n+ }\n+ \n+ //////// AzureDfsStorageFileSystem ////////\n+-unique_ptr<AzureFileHandle> AzureDfsStorageFileSystem::CreateHandle(const string &path, uint8_t flags,\n+-                                                                    FileLockType lock, FileCompressionType compression,\n+-                                                                    FileOpener *opener) {\n++unique_ptr<AzureFileHandle> AzureDfsStorageFileSystem::CreateHandle(const string &path, FileOpenFlags flags,\n++                                                                    optional_ptr<FileOpener> opener) {\n+ \tif (opener == nullptr) {\n+ \t\tthrow InternalException(\"Cannot do Azure storage CreateHandle without FileOpener\");\n+ \t}\n+ \n+-\tD_ASSERT(compression == FileCompressionType::UNCOMPRESSED);\n++\tD_ASSERT(flags.Compression() == FileCompressionType::UNCOMPRESSED);\n+ \n+ \tauto parsed_url = ParseUrl(path);\n+ \tauto storage_context = GetOrCreateStorageContext(opener, path, parsed_url);\n+@@ -186,7 +185,7 @@ void AzureDfsStorageFileSystem::ReadRange(AzureFileHandle &handle, idx_t file_of\n+ \t}\n+ }\n+ \n+-std::shared_ptr<AzureContextState> AzureDfsStorageFileSystem::CreateStorageContext(FileOpener *opener,\n++std::shared_ptr<AzureContextState> AzureDfsStorageFileSystem::CreateStorageContext(optional_ptr<FileOpener> opener,\n+                                                                                    const string &path,\n+                                                                                    const AzureParsedUrl &parsed_url) {\n+ \tauto azure_read_options = ParseAzureReadOptions(opener);\n+diff --git a/src/azure_filesystem.cpp b/src/azure_filesystem.cpp\n+index 4c2caed..bbf5275 100644\n+--- a/src/azure_filesystem.cpp\n++++ b/src/azure_filesystem.cpp\n+@@ -18,7 +18,7 @@ void AzureContextState::QueryEnd() {\n+ \tis_valid = false;\n+ }\n+ \n+-AzureFileHandle::AzureFileHandle(AzureStorageFileSystem &fs, string path, uint8_t flags,\n++AzureFileHandle::AzureFileHandle(AzureStorageFileSystem &fs, string path, FileOpenFlags flags,\n+                                  const AzureReadOptions &read_options)\n+     : FileHandle(fs, std::move(path)), flags(flags),\n+       // File info\n+@@ -27,7 +27,7 @@ AzureFileHandle::AzureFileHandle(AzureStorageFileSystem &fs, string path, uint8_\n+       buffer_available(0), buffer_idx(0), file_offset(0), buffer_start(0), buffer_end(0),\n+       // Options\n+       read_options(read_options) {\n+-\tif (flags & FileFlags::FILE_FLAGS_READ) {\n++\tif (flags.OpenForReading()) {\n+ \t\tread_buffer = duckdb::unique_ptr<data_t[]>(new data_t[read_options.buffer_size]);\n+ \t}\n+ }\n+@@ -37,7 +37,7 @@ void AzureFileHandle::PostConstruct() {\n+ }\n+ \n+ void AzureStorageFileSystem::LoadFileInfo(AzureFileHandle &handle) {\n+-\tif (handle.flags & FileFlags::FILE_FLAGS_READ) {\n++\tif (handle.flags.OpenForReading()) {\n+ \t\ttry {\n+ \t\t\tLoadRemoteFileInfo(handle);\n+ \t\t} catch (const Azure::Storage::StorageException &e) {\n+@@ -53,15 +53,15 @@ void AzureStorageFileSystem::LoadFileInfo(AzureFileHandle &handle) {\n+ \t}\n+ }\n+ \n+-unique_ptr<FileHandle> AzureStorageFileSystem::OpenFile(const string &path, uint8_t flags, FileLockType lock,\n+-                                                        FileCompressionType compression, FileOpener *opener) {\n+-\tD_ASSERT(compression == FileCompressionType::UNCOMPRESSED);\n++unique_ptr<FileHandle> AzureStorageFileSystem::OpenFile(const string &path,FileOpenFlags flags,\n++\t\t\t\t\t\t\t\t\t\t\t\t\t\toptional_ptr<FileOpener> opener) {\n++\tD_ASSERT(flags.Compression() == FileCompressionType::UNCOMPRESSED);\n+ \n+-\tif (flags & FileFlags::FILE_FLAGS_WRITE) {\n++\tif (flags.OpenForWriting()) {\n+ \t\tthrow NotImplementedException(\"Writing to Azure containers is currently not supported\");\n+ \t}\n+ \n+-\tauto handle = CreateHandle(path, flags, lock, compression, opener);\n++\tauto handle = CreateHandle(path, flags, opener);\n+ \treturn std::move(handle);\n+ }\n+ \n+@@ -92,7 +92,7 @@ void AzureStorageFileSystem::Read(FileHandle &handle, void *buffer, int64_t nr_b\n+ \tidx_t buffer_offset = 0;\n+ \n+ \t// Don't buffer when DirectIO is set.\n+-\tif (hfh.flags & FileFlags::FILE_FLAGS_DIRECT_IO && to_read > 0) {\n++\tif (hfh.flags.DirectIO() && to_read > 0) {\n+ \t\tReadRange(hfh, location, (char *)buffer, to_read);\n+ \t\thfh.buffer_available = 0;\n+ \t\thfh.buffer_idx = 0;\n+@@ -153,7 +153,7 @@ int64_t AzureStorageFileSystem::Read(FileHandle &handle, void *buffer, int64_t n\n+ \treturn nr_bytes;\n+ }\n+ \n+-std::shared_ptr<AzureContextState> AzureStorageFileSystem::GetOrCreateStorageContext(FileOpener *opener,\n++std::shared_ptr<AzureContextState> AzureStorageFileSystem::GetOrCreateStorageContext(optional_ptr<FileOpener> opener,\n+                                                                                      const string &path,\n+                                                                                      const AzureParsedUrl &parsed_url) {\n+ \tValue value;\n+@@ -164,7 +164,7 @@ std::shared_ptr<AzureContextState> AzureStorageFileSystem::GetOrCreateStorageCon\n+ \n+ \tstd::shared_ptr<AzureContextState> result;\n+ \tif (azure_context_caching) {\n+-\t\tauto *client_context = FileOpener::TryGetClientContext(opener);\n++\t\tauto client_context = FileOpener::TryGetClientContext(opener);\n+ \n+ \t\tauto context_key = GetContextPrefix() + parsed_url.storage_account_name;\n+ \n+@@ -192,7 +192,7 @@ std::shared_ptr<AzureContextState> AzureStorageFileSystem::GetOrCreateStorageCon\n+ \treturn result;\n+ }\n+ \n+-AzureReadOptions AzureStorageFileSystem::ParseAzureReadOptions(FileOpener *opener) {\n++AzureReadOptions AzureStorageFileSystem::ParseAzureReadOptions(optional_ptr<FileOpener> opener) {\n+ \tAzureReadOptions options;\n+ \n+ \tValue concurrency_val;\n+diff --git a/src/azure_storage_account_client.cpp b/src/azure_storage_account_client.cpp\n+index e54ca93..5a22e60 100644\n+--- a/src/azure_storage_account_client.cpp\n++++ b/src/azure_storage_account_client.cpp\n+@@ -35,7 +35,7 @@ namespace duckdb {\n+ const static std::string DEFAULT_BLOB_ENDPOINT = \"blob.core.windows.net\";\n+ const static std::string DEFAULT_DFS_ENDPOINT = \"dfs.core.windows.net\";\n+ \n+-static std::string TryGetCurrentSetting(FileOpener *opener, const std::string &name) {\n++static std::string TryGetCurrentSetting(optional_ptr<FileOpener> opener, const std::string &name) {\n+ \tValue val;\n+ \tif (FileOpener::TryGetCurrentSetting(opener, name, val)) {\n+ \t\treturn val.ToString();\n+@@ -110,7 +110,7 @@ ToTokenCredentialOptions(const Azure::Core::Http::Policies::TransportOptions &tr\n+ \treturn options;\n+ }\n+ \n+-static std::shared_ptr<HTTPState> GetHttpState(FileOpener *opener) {\n++static std::shared_ptr<HTTPState> GetHttpState(optional_ptr<FileOpener> opener) {\n+ \tValue value;\n+ \tbool enable_http_stats = false;\n+ \tif (FileOpener::TryGetCurrentSetting(opener, \"azure_http_stats\", value)) {\n+@@ -270,7 +270,7 @@ static Azure::Core::Http::Policies::TransportOptions GetTransportOptions(const s\n+ \treturn transport_options;\n+ }\n+ \n+-static Azure::Core::Http::Policies::TransportOptions GetTransportOptions(FileOpener *opener,\n++static Azure::Core::Http::Policies::TransportOptions GetTransportOptions(optional_ptr<FileOpener> opener,\n+                                                                          const KeyValueSecret &secret) {\n+ \tauto transport_option_type = TryGetCurrentSetting(opener, \"azure_transport_option_type\");\n+ \n+@@ -302,7 +302,7 @@ static Azure::Core::Http::Policies::TransportOptions GetTransportOptions(FileOpe\n+ }\n+ \n+ static Azure::Storage::Blobs::BlobServiceClient\n+-GetBlobStorageAccountClientFromConfigProvider(FileOpener *opener, const KeyValueSecret &secret,\n++GetBlobStorageAccountClientFromConfigProvider(optional_ptr<FileOpener> opener, const KeyValueSecret &secret,\n+                                               const AzureParsedUrl &azure_parsed_url) {\n+ \tauto transport_options = GetTransportOptions(opener, secret);\n+ \n+@@ -328,7 +328,7 @@ GetBlobStorageAccountClientFromConfigProvider(FileOpener *opener, const KeyValue\n+ }\n+ \n+ static Azure::Storage::Files::DataLake::DataLakeServiceClient\n+-GetDfsStorageAccountClientFromConfigProvider(FileOpener *opener, const KeyValueSecret &secret,\n++GetDfsStorageAccountClientFromConfigProvider(optional_ptr<FileOpener> opener, const KeyValueSecret &secret,\n+                                              const AzureParsedUrl &azure_parsed_url) {\n+ \tauto transport_options = GetTransportOptions(opener, secret);\n+ \n+@@ -355,7 +355,7 @@ GetDfsStorageAccountClientFromConfigProvider(FileOpener *opener, const KeyValueS\n+ }\n+ \n+ static Azure::Storage::Blobs::BlobServiceClient\n+-GetBlobStorageAccountClientFromCredentialChainProvider(FileOpener *opener, const KeyValueSecret &secret,\n++GetBlobStorageAccountClientFromCredentialChainProvider(optional_ptr<FileOpener> opener, const KeyValueSecret &secret,\n+                                                        const AzureParsedUrl &azure_parsed_url) {\n+ \tauto transport_options = GetTransportOptions(opener, secret);\n+ \t// Create credential chain\n+@@ -369,7 +369,7 @@ GetBlobStorageAccountClientFromCredentialChainProvider(FileOpener *opener, const\n+ }\n+ \n+ static Azure::Storage::Files::DataLake::DataLakeServiceClient\n+-GetDfsStorageAccountClientFromCredentialChainProvider(FileOpener *opener, const KeyValueSecret &secret,\n++GetDfsStorageAccountClientFromCredentialChainProvider(optional_ptr<FileOpener> opener, const KeyValueSecret &secret,\n+                                                       const AzureParsedUrl &azure_parsed_url) {\n+ \tauto transport_options = GetTransportOptions(opener, secret);\n+ \t// Create credential chain\n+@@ -383,7 +383,7 @@ GetDfsStorageAccountClientFromCredentialChainProvider(FileOpener *opener, const\n+ }\n+ \n+ static Azure::Storage::Blobs::BlobServiceClient\n+-GetBlobStorageAccountClientFromServicePrincipalProvider(FileOpener *opener, const KeyValueSecret &secret,\n++GetBlobStorageAccountClientFromServicePrincipalProvider(optional_ptr<FileOpener> opener, const KeyValueSecret &secret,\n+                                                         const AzureParsedUrl &azure_parsed_url) {\n+ \tauto transport_options = GetTransportOptions(opener, secret);\n+ \tauto token_credential = CreateClientCredential(secret, transport_options);\n+@@ -396,7 +396,7 @@ GetBlobStorageAccountClientFromServicePrincipalProvider(FileOpener *opener, cons\n+ }\n+ \n+ static Azure::Storage::Files::DataLake::DataLakeServiceClient\n+-GetDfsStorageAccountClientFromServicePrincipalProvider(FileOpener *opener, const KeyValueSecret &secret,\n++GetDfsStorageAccountClientFromServicePrincipalProvider(optional_ptr<FileOpener> opener, const KeyValueSecret &secret,\n+                                                        const AzureParsedUrl &azure_parsed_url) {\n+ \tauto transport_options = GetTransportOptions(opener, secret);\n+ \tauto token_credential = CreateClientCredential(secret, transport_options);\n+@@ -409,7 +409,7 @@ GetDfsStorageAccountClientFromServicePrincipalProvider(FileOpener *opener, const\n+ }\n+ \n+ static Azure::Storage::Blobs::BlobServiceClient\n+-GetBlobStorageAccountClient(FileOpener *opener, const KeyValueSecret &secret, const AzureParsedUrl &azure_parsed_url) {\n++GetBlobStorageAccountClient(optional_ptr<FileOpener> opener, const KeyValueSecret &secret, const AzureParsedUrl &azure_parsed_url) {\n+ \tauto &provider = secret.GetProvider();\n+ \t// default provider\n+ \tif (provider == \"config\") {\n+@@ -424,7 +424,7 @@ GetBlobStorageAccountClient(FileOpener *opener, const KeyValueSecret &secret, co\n+ }\n+ \n+ static Azure::Storage::Files::DataLake::DataLakeServiceClient\n+-GetDfsStorageAccountClient(FileOpener *opener, const KeyValueSecret &secret, const AzureParsedUrl &azure_parsed_url) {\n++GetDfsStorageAccountClient(optional_ptr<FileOpener> opener, const KeyValueSecret &secret, const AzureParsedUrl &azure_parsed_url) {\n+ \tauto &provider = secret.GetProvider();\n+ \t// default provider\n+ \tif (provider == \"config\") {\n+@@ -438,7 +438,7 @@ GetDfsStorageAccountClient(FileOpener *opener, const KeyValueSecret &secret, con\n+ \tthrow InvalidInputException(\"Unsupported provider type %s for azure\", provider);\n+ }\n+ \n+-static Azure::Core::Http::Policies::TransportOptions GetTransportOptions(FileOpener *opener) {\n++static Azure::Core::Http::Policies::TransportOptions GetTransportOptions(optional_ptr<FileOpener> opener) {\n+ \tauto azure_transport_option_type = TryGetCurrentSetting(opener, \"azure_transport_option_type\");\n+ \n+ \t// Load proxy options\n+@@ -449,7 +449,7 @@ static Azure::Core::Http::Policies::TransportOptions GetTransportOptions(FileOpe\n+ \treturn GetTransportOptions(azure_transport_option_type, http_proxy, http_proxy_user_name, http_proxy_password);\n+ }\n+ \n+-static Azure::Storage::Blobs::BlobServiceClient GetBlobStorageAccountClient(FileOpener *opener,\n++static Azure::Storage::Blobs::BlobServiceClient GetBlobStorageAccountClient(optional_ptr<FileOpener> opener,\n+                                                                             const std::string &provided_storage_account,\n+                                                                             const std::string &provided_endpoint) {\n+ \tauto transport_options = GetTransportOptions(opener);\n+@@ -494,7 +494,7 @@ static Azure::Storage::Blobs::BlobServiceClient GetBlobStorageAccountClient(File\n+ \treturn Azure::Storage::Blobs::BlobServiceClient {account_url, blob_options};\n+ }\n+ \n+-const SecretMatch LookupSecret(FileOpener *opener, const std::string &path) {\n++const SecretMatch LookupSecret(optional_ptr<FileOpener> opener, const std::string &path) {\n+ \tauto context = opener->TryGetClientContext();\n+ \n+ \tif (context) {\n+@@ -505,7 +505,7 @@ const SecretMatch LookupSecret(FileOpener *opener, const std::string &path) {\n+ \treturn {};\n+ }\n+ \n+-Azure::Storage::Blobs::BlobServiceClient ConnectToBlobStorageAccount(FileOpener *opener, const std::string &path,\n++Azure::Storage::Blobs::BlobServiceClient ConnectToBlobStorageAccount(optional_ptr<FileOpener> opener, const std::string &path,\n+                                                                      const AzureParsedUrl &azure_parsed_url) {\n+ \n+ \tauto secret_match = LookupSecret(opener, path);\n+@@ -519,7 +519,7 @@ Azure::Storage::Blobs::BlobServiceClient ConnectToBlobStorageAccount(FileOpener\n+ }\n+ \n+ Azure::Storage::Files::DataLake::DataLakeServiceClient\n+-ConnectToDfsStorageAccount(FileOpener *opener, const std::string &path, const AzureParsedUrl &azure_parsed_url) {\n++ConnectToDfsStorageAccount(optional_ptr<FileOpener> opener, const std::string &path, const AzureParsedUrl &azure_parsed_url) {\n+ \tauto secret_match = LookupSecret(opener, path);\n+ \tif (secret_match.HasMatch()) {\n+ \t\tconst auto &base_secret = secret_match.GetSecret();\n+diff --git a/src/include/azure_blob_filesystem.hpp b/src/include/azure_blob_filesystem.hpp\n+index cb76d68..29ddc5b 100644\n+--- a/src/include/azure_blob_filesystem.hpp\n++++ b/src/include/azure_blob_filesystem.hpp\n+@@ -23,7 +23,7 @@ class AzureBlobStorageFileSystem;\n+ \n+ class AzureBlobStorageFileHandle : public AzureFileHandle {\n+ public:\n+-\tAzureBlobStorageFileHandle(AzureBlobStorageFileSystem &fs, string path, uint8_t flags,\n++\tAzureBlobStorageFileHandle(AzureBlobStorageFileSystem &fs, string path, FileOpenFlags flags,\n+ \t                           const AzureReadOptions &read_options, Azure::Storage::Blobs::BlobClient blob_client);\n+ \t~AzureBlobStorageFileHandle() override = default;\n+ \n+@@ -57,10 +57,10 @@ protected:\n+ \tconst string &GetContextPrefix() const override {\n+ \t\treturn PATH_PREFIX;\n+ \t}\n+-\tstd::shared_ptr<AzureContextState> CreateStorageContext(FileOpener *opener, const string &path,\n++\tstd::shared_ptr<AzureContextState> CreateStorageContext(optional_ptr<FileOpener> opener, const string &path,\n+ \t                                                        const AzureParsedUrl &parsed_url) override;\n+-\tduckdb::unique_ptr<AzureFileHandle> CreateHandle(const string &path, uint8_t flags, FileLockType lock,\n+-\t                                                 FileCompressionType compression, FileOpener *opener) override;\n++\tduckdb::unique_ptr<AzureFileHandle> CreateHandle(const string &path, FileOpenFlags flags,\n++\t\t\t\t\t\t\t\t\t\t\t\t\t optional_ptr<FileOpener> opener) override;\n+ \n+ \tvoid ReadRange(AzureFileHandle &handle, idx_t file_offset, char *buffer_out, idx_t buffer_out_len) override;\n+ };\n+diff --git a/src/include/azure_dfs_filesystem.hpp b/src/include/azure_dfs_filesystem.hpp\n+index c4bf8fe..cdcdb23 100644\n+--- a/src/include/azure_dfs_filesystem.hpp\n++++ b/src/include/azure_dfs_filesystem.hpp\n+@@ -25,7 +25,7 @@ class AzureDfsStorageFileSystem;\n+ \n+ class AzureDfsStorageFileHandle : public AzureFileHandle {\n+ public:\n+-\tAzureDfsStorageFileHandle(AzureDfsStorageFileSystem &fs, string path, uint8_t flags,\n++\tAzureDfsStorageFileHandle(AzureDfsStorageFileSystem &fs, string path, FileOpenFlags flags,\n+ \t                          const AzureReadOptions &read_options,\n+ \t                          Azure::Storage::Files::DataLake::DataLakeFileClient client);\n+ \t~AzureDfsStorageFileHandle() override = default;\n+@@ -55,10 +55,10 @@ protected:\n+ \tconst string &GetContextPrefix() const override {\n+ \t\treturn PATH_PREFIX;\n+ \t}\n+-\tstd::shared_ptr<AzureContextState> CreateStorageContext(FileOpener *opener, const string &path,\n++\tstd::shared_ptr<AzureContextState> CreateStorageContext(optional_ptr<FileOpener> opener, const string &path,\n+ \t                                                        const AzureParsedUrl &parsed_url) override;\n+-\tduckdb::unique_ptr<AzureFileHandle> CreateHandle(const string &path, uint8_t flags, FileLockType lock,\n+-\t                                                 FileCompressionType compression, FileOpener *opener) override;\n++\tduckdb::unique_ptr<AzureFileHandle> CreateHandle(const string &path, FileOpenFlags flags,\n++\t\t\t\t\t\t\t\t\t\t\t\t\t optional_ptr<FileOpener> opener) override;\n+ \n+ \tvoid ReadRange(AzureFileHandle &handle, idx_t file_offset, char *buffer_out, idx_t buffer_out_len) override;\n+ };\n+diff --git a/src/include/azure_filesystem.hpp b/src/include/azure_filesystem.hpp\n+index a41d7a2..237301a 100644\n+--- a/src/include/azure_filesystem.hpp\n++++ b/src/include/azure_filesystem.hpp\n+@@ -52,10 +52,10 @@ public:\n+ \t}\n+ \n+ protected:\n+-\tAzureFileHandle(AzureStorageFileSystem &fs, string path, uint8_t flags, const AzureReadOptions &read_options);\n++\tAzureFileHandle(AzureStorageFileSystem &fs, string path, FileOpenFlags flags, const AzureReadOptions &read_options);\n+ \n+ public:\n+-\tconst uint8_t flags;\n++\tconst FileOpenFlags flags;\n+ \n+ \t// File info\n+ \tidx_t length;\n+@@ -76,9 +76,8 @@ public:\n+ class AzureStorageFileSystem : public FileSystem {\n+ public:\n+ \t// FS methods\n+-\tduckdb::unique_ptr<FileHandle> OpenFile(const string &path, uint8_t flags, FileLockType lock = DEFAULT_LOCK,\n+-\t                                        FileCompressionType compression = DEFAULT_COMPRESSION,\n+-\t                                        FileOpener *opener = nullptr) override;\n++\tduckdb::unique_ptr<FileHandle> OpenFile(const string &path, FileOpenFlags flags,\n++\t                                        optional_ptr<FileOpener> opener = nullptr) override;\n+ \n+ \tvoid Read(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location) override;\n+ \tint64_t Read(FileHandle &handle, void *buffer, int64_t nr_bytes) override;\n+@@ -99,18 +98,18 @@ public:\n+ \tvoid LoadFileInfo(AzureFileHandle &handle);\n+ \n+ protected:\n+-\tvirtual duckdb::unique_ptr<AzureFileHandle> CreateHandle(const string &path, uint8_t flags, FileLockType lock,\n+-\t                                                         FileCompressionType compression, FileOpener *opener) = 0;\n++\tvirtual duckdb::unique_ptr<AzureFileHandle> CreateHandle(const string &path, FileOpenFlags flags,\n++\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t optional_ptr<FileOpener> opener) = 0;\n+ \tvirtual void ReadRange(AzureFileHandle &handle, idx_t file_offset, char *buffer_out, idx_t buffer_out_len) = 0;\n+ \n+ \tvirtual const string &GetContextPrefix() const = 0;\n+-\tstd::shared_ptr<AzureContextState> GetOrCreateStorageContext(FileOpener *opener, const string &path,\n++\tstd::shared_ptr<AzureContextState> GetOrCreateStorageContext(optional_ptr<FileOpener> opener, const string &path,\n+ \t                                                             const AzureParsedUrl &parsed_url);\n+-\tvirtual std::shared_ptr<AzureContextState> CreateStorageContext(FileOpener *opener, const string &path,\n++\tvirtual std::shared_ptr<AzureContextState> CreateStorageContext(optional_ptr<FileOpener> opener, const string &path,\n+ \t                                                                const AzureParsedUrl &parsed_url) = 0;\n+ \n+ \tvirtual void LoadRemoteFileInfo(AzureFileHandle &handle) = 0;\n+-\tstatic AzureReadOptions ParseAzureReadOptions(FileOpener *opener);\n++\tstatic AzureReadOptions ParseAzureReadOptions(optional_ptr<FileOpener> opener);\n+ \tstatic time_t ToTimeT(const Azure::DateTime &dt);\n+ };\n+ \n+diff --git a/src/include/azure_storage_account_client.hpp b/src/include/azure_storage_account_client.hpp\n+index 2e22ee0..600fa10 100644\n+--- a/src/include/azure_storage_account_client.hpp\n++++ b/src/include/azure_storage_account_client.hpp\n+@@ -8,10 +8,10 @@\n+ \n+ namespace duckdb {\n+ \n+-Azure::Storage::Blobs::BlobServiceClient ConnectToBlobStorageAccount(FileOpener *opener, const std::string &path,\n++Azure::Storage::Blobs::BlobServiceClient ConnectToBlobStorageAccount(optional_ptr<FileOpener> opener, const std::string &path,\n+                                                                      const AzureParsedUrl &azure_parsed_url);\n+ \n+ Azure::Storage::Files::DataLake::DataLakeServiceClient\n+-ConnectToDfsStorageAccount(FileOpener *opener, const std::string &path, const AzureParsedUrl &azure_parsed_url);\n++ConnectToDfsStorageAccount(optional_ptr<FileOpener> opener, const std::string &path, const AzureParsedUrl &azure_parsed_url);\n+ \n+ } // namespace duckdb\ndiff --git a/.github/patches/extensions/iceberg/open_file.patch b/.github/patches/extensions/iceberg/open_file.patch\nnew file mode 100644\nindex 000000000000..f564792e3957\n--- /dev/null\n+++ b/.github/patches/extensions/iceberg/open_file.patch\n@@ -0,0 +1,66 @@\n+diff --git a/src/common/iceberg.cpp b/src/common/iceberg.cpp\n+index 3c15105..face972 100644\n+--- a/src/common/iceberg.cpp\n++++ b/src/common/iceberg.cpp\n+@@ -127,7 +127,7 @@ unique_ptr<SnapshotParseInfo> IcebergSnapshot::GetParseInfo(const string &path,\n+ \tparse_info->doc = doc;\n+ \tparse_info->document = std::move(metadata_json);\n+ \n+-\treturn std::move(parse_info);\n++\treturn parse_info;\n+ }\n+ \n+ IcebergSnapshot IcebergSnapshot::GetLatestSnapshot(const string &path, FileSystem &fs) {\n+diff --git a/src/common/utils.cpp b/src/common/utils.cpp\n+index c0272bf..d2b01d9 100644\n+--- a/src/common/utils.cpp\n++++ b/src/common/utils.cpp\n+@@ -4,8 +4,7 @@\n+ namespace duckdb {\n+ \n+ string IcebergUtils::FileToString(const string &path, FileSystem &fs) {\n+-\tauto handle =\n+-\t    fs.OpenFile(path, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK, FileSystem::DEFAULT_COMPRESSION);\n++\tauto handle = fs.OpenFile(path, FileFlags::FILE_FLAGS_READ);\n+ \tauto file_size = handle->GetFileSize();\n+ \tstring ret_val(file_size, ' ');\n+ \thandle->Read((char *)ret_val.c_str(), file_size);\n+diff --git a/src/iceberg_extension.cpp b/src/iceberg_extension.cpp\n+index 0e59fdc..895b79d 100644\n+--- a/src/iceberg_extension.cpp\n++++ b/src/iceberg_extension.cpp\n+@@ -16,12 +16,6 @@\n+ namespace duckdb {\n+ \n+ static void LoadInternal(DatabaseInstance &instance) {\n+-\tConnection con(instance);\n+-\tcon.BeginTransaction();\n+-\tauto &context = *con.context;\n+-\n+-\tauto &catalog = Catalog::GetSystemCatalog(*con.context);\n+-\n+ \t// Iceberg Table Functions\n+ \tfor (auto &fun : IcebergFunctions::GetTableFunctions()) {\n+ \t\tExtensionUtil::RegisterFunction(instance, fun);\n+@@ -31,8 +25,6 @@ static void LoadInternal(DatabaseInstance &instance) {\n+ \tfor (auto &fun : IcebergFunctions::GetScalarFunctions()) {\n+ \t\tExtensionUtil::RegisterFunction(instance, fun);\n+ \t}\n+-\n+-\tcon.Commit();\n+ }\n+ \n+ void IcebergExtension::Load(DuckDB &db) {\n+diff --git a/src/iceberg_functions/iceberg_snapshots.cpp b/src/iceberg_functions/iceberg_snapshots.cpp\n+index 6d6ef57..4b67a6d 100644\n+--- a/src/iceberg_functions/iceberg_snapshots.cpp\n++++ b/src/iceberg_functions/iceberg_snapshots.cpp\n+@@ -93,7 +93,7 @@ TableFunctionSet IcebergFunctions::GetIcebergSnapshotsFunction() {\n+ \tTableFunction table_function({LogicalType::VARCHAR}, IcebergSnapshotsFunction, IcebergSnapshotsBind,\n+ \t                             IcebergSnapshotGlobalTableFunctionState::Init);\n+ \tfunction_set.AddFunction(table_function);\n+-\treturn std::move(function_set);\n++\treturn function_set;\n+ }\n+ \n+ } // namespace duckdb\ndiff --git a/.github/patches/extensions/spatial/open_file.patch b/.github/patches/extensions/spatial/open_file.patch\nnew file mode 100644\nindex 000000000000..573b7277d150\n--- /dev/null\n+++ b/.github/patches/extensions/spatial/open_file.patch\n@@ -0,0 +1,53 @@\n+diff --git a/spatial/src/spatial/core/io/osm/st_read_osm.cpp b/spatial/src/spatial/core/io/osm/st_read_osm.cpp\n+index dfe396a..87bf598 100644\n+--- a/spatial/src/spatial/core/io/osm/st_read_osm.cpp\n++++ b/spatial/src/spatial/core/io/osm/st_read_osm.cpp\n+@@ -239,7 +239,7 @@ static unique_ptr<GlobalTableFunctionState> InitGlobal(ClientContext &context, T\n+ \tauto &fs = FileSystem::GetFileSystem(context);\n+ \tauto file_name = bind_data.file_name;\n+ \n+-\tauto handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ, FileLockType::READ_LOCK);\n++\tauto handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ | FileLockType::READ_LOCK);\n+ \tauto file_size = handle->GetFileSize();\n+ \n+ \tauto max_threads = context.db->NumberOfThreads();\n+diff --git a/spatial/src/spatial/gdal/file_handler.cpp b/spatial/src/spatial/gdal/file_handler.cpp\n+index db449df..ebcefe5 100644\n+--- a/spatial/src/spatial/gdal/file_handler.cpp\n++++ b/spatial/src/spatial/gdal/file_handler.cpp\n+@@ -122,7 +122,7 @@ public:\n+ \t\tauto &fs = FileSystem::GetFileSystem(context);\n+ \n+ \t\t// TODO: Double check that this is correct\n+-\t\tuint8_t flags;\n++\t\tFileOpenFlags flags;\n+ \t\tauto len = strlen(access);\n+ \t\tif (access[0] == 'r') {\n+ \t\t\tflags = FileFlags::FILE_FLAGS_READ;\n+@@ -160,14 +160,14 @@ public:\n+ \t\t\t// Check if the file is a directory\n+ \n+ #ifdef _WIN32\n+-\t\t\tif (fs.DirectoryExists(path) && (flags & FileFlags::FILE_FLAGS_READ)) {\n++\t\t\tif (fs.DirectoryExists(path) && flags.OpenForReading()) {\n+ \t\t\t\t// We can't open a directory for reading on windows without special flags\n+ \t\t\t\t// so just open nul instead, gdal will reject it when it tries to read\n+ \t\t\t\tauto file = fs.OpenFile(\"nul\", flags);\n+ \t\t\t\treturn new DuckDBFileHandle(std::move(file));\n+ \t\t\t}\n+ #endif\n+-\t\t\tauto file = fs.OpenFile(file_name, flags, FileSystem::DEFAULT_LOCK, FileCompressionType::AUTO_DETECT);\n++\t\t\tauto file = fs.OpenFile(file_name, flags | FileCompressionType::AUTO_DETECT);\n+ \t\t\treturn new DuckDBFileHandle(std::move(file));\n+ \t\t} catch (std::exception &ex) {\n+ \t\t\t// Failed to open file via DuckDB File System. If this doesnt have a VSI prefix we can return an error here.\n+@@ -209,8 +209,7 @@ public:\n+ \n+ \t\tunique_ptr<FileHandle> file;\n+ \t\ttry {\n+-\t\t\tfile = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,\n+-\t\t\t                   FileCompressionType::AUTO_DETECT);\n++\t\t\tfile = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ | FileCompressionType::AUTO_DETECT);\n+ \t\t} catch (std::exception &ex) {\n+ \t\t\treturn -1;\n+ \t\t}\ndiff --git a/extension/httpfs/httpfs.cpp b/extension/httpfs/httpfs.cpp\nindex abb20d8ff813..feb292b50e65 100644\n--- a/extension/httpfs/httpfs.cpp\n+++ b/extension/httpfs/httpfs.cpp\n@@ -29,7 +29,7 @@ static duckdb::unique_ptr<duckdb_httplib_openssl::Headers> initialize_http_heade\n \treturn headers;\n }\n \n-HTTPParams HTTPParams::ReadFrom(FileOpener *opener) {\n+HTTPParams HTTPParams::ReadFrom(optional_ptr<FileOpener> opener) {\n \tuint64_t timeout = DEFAULT_TIMEOUT;\n \tuint64_t retries = DEFAULT_RETRIES;\n \tuint64_t retry_wait_ms = DEFAULT_RETRY_WAIT_MS;\n@@ -37,7 +37,7 @@ HTTPParams HTTPParams::ReadFrom(FileOpener *opener) {\n \tbool force_download = DEFAULT_FORCE_DOWNLOAD;\n \tbool keep_alive = DEFAULT_KEEP_ALIVE;\n \tbool enable_server_cert_verification = DEFAULT_ENABLE_SERVER_CERT_VERIFICATION;\n-\tstd::string ca_cert_file = \"\";\n+\tstd::string ca_cert_file;\n \n \tValue value;\n \tif (FileOpener::TryGetCurrentSetting(opener, \"http_timeout\", value)) {\n@@ -150,7 +150,7 @@ RunRequestWithRetry(const std::function<duckdb_httplib_openssl::Result(void)> &r\n unique_ptr<ResponseWrapper> HTTPFileSystem::PostRequest(FileHandle &handle, string url, HeaderMap header_map,\n                                                         duckdb::unique_ptr<char[]> &buffer_out, idx_t &buffer_out_len,\n                                                         char *buffer_in, idx_t buffer_in_len, string params) {\n-\tauto &hfs = (HTTPFileHandle &)handle;\n+\tauto &hfs = handle.Cast<HTTPFileHandle>();\n \tstring path, proto_host_port;\n \tParseUrl(url, path, proto_host_port);\n \tauto headers = initialize_http_headers(header_map);\n@@ -212,7 +212,7 @@ unique_ptr<duckdb_httplib_openssl::Client> HTTPFileSystem::GetClient(const HTTPP\n \n unique_ptr<ResponseWrapper> HTTPFileSystem::PutRequest(FileHandle &handle, string url, HeaderMap header_map,\n                                                        char *buffer_in, idx_t buffer_in_len, string params) {\n-\tauto &hfs = (HTTPFileHandle &)handle;\n+\tauto &hfs = handle.Cast<HTTPFileHandle>();\n \tstring path, proto_host_port;\n \tParseUrl(url, path, proto_host_port);\n \tauto headers = initialize_http_headers(header_map);\n@@ -230,7 +230,7 @@ unique_ptr<ResponseWrapper> HTTPFileSystem::PutRequest(FileHandle &handle, strin\n }\n \n unique_ptr<ResponseWrapper> HTTPFileSystem::HeadRequest(FileHandle &handle, string url, HeaderMap header_map) {\n-\tauto &hfs = (HTTPFileHandle &)handle;\n+\tauto &hfs = handle.Cast<HTTPFileHandle>();\n \tstring path, proto_host_port;\n \tParseUrl(url, path, proto_host_port);\n \tauto headers = initialize_http_headers(header_map);\n@@ -306,7 +306,7 @@ unique_ptr<ResponseWrapper> HTTPFileSystem::GetRequest(FileHandle &handle, strin\n \n unique_ptr<ResponseWrapper> HTTPFileSystem::GetRangeRequest(FileHandle &handle, string url, HeaderMap header_map,\n                                                             idx_t file_offset, char *buffer_out, idx_t buffer_out_len) {\n-\tauto &hfs = (HTTPFileHandle &)handle;\n+\tauto &hfs = handle.Cast<HTTPFileHandle>();\n \tstring path, proto_host_port;\n \tParseUrl(url, path, proto_host_port);\n \tauto headers = initialize_http_headers(header_map);\n@@ -371,22 +371,22 @@ unique_ptr<ResponseWrapper> HTTPFileSystem::GetRangeRequest(FileHandle &handle,\n \treturn RunRequestWithRetry(request, url, \"GET Range\", hfs.http_params, on_retry);\n }\n \n-HTTPFileHandle::HTTPFileHandle(FileSystem &fs, string path, uint8_t flags, const HTTPParams &http_params)\n+HTTPFileHandle::HTTPFileHandle(FileSystem &fs, const string &path, FileOpenFlags flags, const HTTPParams &http_params)\n     : FileHandle(fs, path), http_params(http_params), flags(flags), length(0), buffer_available(0), buffer_idx(0),\n       file_offset(0), buffer_start(0), buffer_end(0) {\n }\n \n-unique_ptr<HTTPFileHandle> HTTPFileSystem::CreateHandle(const string &path, uint8_t flags, FileLockType lock,\n-                                                        FileCompressionType compression, FileOpener *opener) {\n-\tD_ASSERT(compression == FileCompressionType::UNCOMPRESSED);\n+unique_ptr<HTTPFileHandle> HTTPFileSystem::CreateHandle(const string &path, FileOpenFlags flags,\n+                                                        optional_ptr<FileOpener> opener) {\n+\tD_ASSERT(flags.Compression() == FileCompressionType::UNCOMPRESSED);\n \treturn duckdb::make_uniq<HTTPFileHandle>(*this, path, flags, HTTPParams::ReadFrom(opener));\n }\n \n-unique_ptr<FileHandle> HTTPFileSystem::OpenFile(const string &path, uint8_t flags, FileLockType lock,\n-                                                FileCompressionType compression, FileOpener *opener) {\n-\tD_ASSERT(compression == FileCompressionType::UNCOMPRESSED);\n+unique_ptr<FileHandle> HTTPFileSystem::OpenFile(const string &path, FileOpenFlags flags,\n+                                                optional_ptr<FileOpener> opener) {\n+\tD_ASSERT(flags.Compression() == FileCompressionType::UNCOMPRESSED);\n \n-\tauto handle = CreateHandle(path, flags, lock, compression, opener);\n+\tauto handle = CreateHandle(path, flags, opener);\n \thandle->Initialize(opener);\n \treturn std::move(handle);\n }\n@@ -410,7 +410,7 @@ void HTTPFileSystem::Read(FileHandle &handle, void *buffer, int64_t nr_bytes, id\n \tidx_t buffer_offset = 0;\n \n \t// Don't buffer when DirectIO is set.\n-\tif (hfh.flags & FileFlags::FILE_FLAGS_DIRECT_IO && to_read > 0) {\n+\tif (hfh.flags.DirectIO() && to_read > 0) {\n \t\tGetRangeRequest(hfh, hfh.path, {}, location, (char *)buffer, to_read);\n \t\thfh.buffer_available = 0;\n \t\thfh.buffer_idx = 0;\n@@ -514,17 +514,17 @@ bool HTTPFileSystem::CanHandleFile(const string &fpath) {\n }\n \n void HTTPFileSystem::Seek(FileHandle &handle, idx_t location) {\n-\tauto &sfh = (HTTPFileHandle &)handle;\n+\tauto &sfh = handle.Cast<HTTPFileHandle>();\n \tsfh.file_offset = location;\n }\n \n idx_t HTTPFileSystem::SeekPosition(FileHandle &handle) {\n-\tauto &sfh = (HTTPFileHandle &)handle;\n+\tauto &sfh = handle.Cast<HTTPFileHandle>();\n \treturn sfh.file_offset;\n }\n \n // Get either the local, global, or no cache depending on settings\n-static optional_ptr<HTTPMetadataCache> TryGetMetadataCache(FileOpener *opener, HTTPFileSystem &httpfs) {\n+static optional_ptr<HTTPMetadataCache> TryGetMetadataCache(optional_ptr<FileOpener> opener, HTTPFileSystem &httpfs) {\n \tauto client_context = FileOpener::TryGetClientContext(opener);\n \tif (!client_context) {\n \t\treturn nullptr;\n@@ -549,9 +549,9 @@ static optional_ptr<HTTPMetadataCache> TryGetMetadataCache(FileOpener *opener, H\n \t}\n }\n \n-void HTTPFileHandle::Initialize(FileOpener *opener) {\n+void HTTPFileHandle::Initialize(optional_ptr<FileOpener> opener) {\n \tInitializeClient();\n-\tauto &hfs = (HTTPFileSystem &)file_system;\n+\tauto &hfs = file_system.Cast<HTTPFileSystem>();\n \tstate = HTTPState::TryGetState(opener);\n \tif (!state) {\n \t\tif (!opener) {\n@@ -565,7 +565,7 @@ void HTTPFileHandle::Initialize(FileOpener *opener) {\n \tauto current_cache = TryGetMetadataCache(opener, hfs);\n \n \tbool should_write_cache = false;\n-\tif (!http_params.force_download && current_cache && !(flags & FileFlags::FILE_FLAGS_WRITE)) {\n+\tif (!http_params.force_download && current_cache && !flags.OpenForWriting()) {\n \n \t\tHTTPMetadataCacheEntry value;\n \t\tbool found = current_cache->Find(path, value);\n@@ -574,7 +574,7 @@ void HTTPFileHandle::Initialize(FileOpener *opener) {\n \t\t\tlast_modified = value.last_modified;\n \t\t\tlength = value.length;\n \n-\t\t\tif (flags & FileFlags::FILE_FLAGS_READ) {\n+\t\t\tif (flags.OpenForReading()) {\n \t\t\t\tread_buffer = duckdb::unique_ptr<data_t[]>(new data_t[READ_BUFFER_LEN]);\n \t\t\t}\n \t\t\treturn;\n@@ -584,7 +584,7 @@ void HTTPFileHandle::Initialize(FileOpener *opener) {\n \t}\n \n \t// If we're writing to a file, we might as well remove it from the cache\n-\tif (current_cache && flags & FileFlags::FILE_FLAGS_WRITE) {\n+\tif (current_cache && flags.OpenForWriting()) {\n \t\tcurrent_cache->Erase(path);\n \t}\n \n@@ -592,8 +592,8 @@ void HTTPFileHandle::Initialize(FileOpener *opener) {\n \tstring range_length;\n \n \tif (res->code != 200) {\n-\t\tif ((flags & FileFlags::FILE_FLAGS_WRITE) && res->code == 404) {\n-\t\t\tif (!(flags & FileFlags::FILE_FLAGS_FILE_CREATE) && !(flags & FileFlags::FILE_FLAGS_FILE_CREATE_NEW)) {\n+\t\tif (flags.OpenForWriting() && res->code == 404) {\n+\t\t\tif (!flags.CreateFileIfNotExists() && !flags.OverwriteExistingFile()) {\n \t\t\t\tthrow IOException(\"Unable to open URL \\\"\" + path +\n \t\t\t\t                  \"\\\" for writing: file does not exist and CREATE flag is not set\");\n \t\t\t}\n@@ -601,7 +601,7 @@ void HTTPFileHandle::Initialize(FileOpener *opener) {\n \t\t\treturn;\n \t\t} else {\n \t\t\t// HEAD request fail, use Range request for another try (read only one byte)\n-\t\t\tif ((flags & FileFlags::FILE_FLAGS_READ) && res->code != 404) {\n+\t\t\tif (flags.OpenForReading() && res->code != 404) {\n \t\t\t\tauto range_res = hfs.GetRangeRequest(*this, path, {}, 0, nullptr, 2);\n \t\t\t\tif (range_res->code != 206) {\n \t\t\t\t\tthrow IOException(\"Unable to connect to URL \\\"%s\\\": %d (%s)\", path, res->code, res->error);\n@@ -627,7 +627,7 @@ void HTTPFileHandle::Initialize(FileOpener *opener) {\n \t}\n \n \t// Initialize the read buffer now that we know the file exists\n-\tif (flags & FileFlags::FILE_FLAGS_READ) {\n+\tif (flags.OpenForReading()) {\n \t\tread_buffer = duckdb::unique_ptr<data_t[]>(new data_t[READ_BUFFER_LEN]);\n \t}\n \ndiff --git a/extension/httpfs/include/httpfs.hpp b/extension/httpfs/include/httpfs.hpp\nindex 2f47bb39d1b5..8909ba5edbd8 100644\n--- a/extension/httpfs/include/httpfs.hpp\n+++ b/extension/httpfs/include/httpfs.hpp\n@@ -47,15 +47,15 @@ struct HTTPParams {\n \tbool enable_server_cert_verification;\n \tstd::string ca_cert_file;\n \n-\tstatic HTTPParams ReadFrom(FileOpener *opener);\n+\tstatic HTTPParams ReadFrom(optional_ptr<FileOpener> opener);\n };\n \n class HTTPFileHandle : public FileHandle {\n public:\n-\tHTTPFileHandle(FileSystem &fs, string path, uint8_t flags, const HTTPParams &params);\n+\tHTTPFileHandle(FileSystem &fs, const string &path, FileOpenFlags flags, const HTTPParams &params);\n \t~HTTPFileHandle() override;\n \t// This two-phase construction allows subclasses more flexible setup.\n-\tvirtual void Initialize(FileOpener *opener);\n+\tvirtual void Initialize(optional_ptr<FileOpener> opener);\n \n \t// We keep an http client stored for connection reuse with keep-alive headers\n \tduckdb::unique_ptr<duckdb_httplib_openssl::Client> http_client;\n@@ -63,7 +63,7 @@ class HTTPFileHandle : public FileHandle {\n \tconst HTTPParams http_params;\n \n \t// File handle info\n-\tuint8_t flags;\n+\tFileOpenFlags flags;\n \tidx_t length;\n \ttime_t last_modified;\n \n@@ -96,9 +96,8 @@ class HTTPFileSystem : public FileSystem {\n \tstatic duckdb::unique_ptr<duckdb_httplib_openssl::Client> GetClient(const HTTPParams &http_params,\n \t                                                                    const char *proto_host_port);\n \tstatic void ParseUrl(string &url, string &path_out, string &proto_host_port_out);\n-\tduckdb::unique_ptr<FileHandle> OpenFile(const string &path, uint8_t flags, FileLockType lock = DEFAULT_LOCK,\n-\t                                        FileCompressionType compression = DEFAULT_COMPRESSION,\n-\t                                        FileOpener *opener = nullptr) final;\n+\tduckdb::unique_ptr<FileHandle> OpenFile(const string &path, FileOpenFlags flags,\n+\t                                        optional_ptr<FileOpener> opener = nullptr) final;\n \n \tvector<string> Glob(const string &path, FileOpener *opener = nullptr) override {\n \t\treturn {path}; // FIXME\n@@ -153,8 +152,8 @@ class HTTPFileSystem : public FileSystem {\n \tduckdb::unique_ptr<HTTPMetadataCache> global_metadata_cache;\n \n protected:\n-\tvirtual duckdb::unique_ptr<HTTPFileHandle> CreateHandle(const string &path, uint8_t flags, FileLockType lock,\n-\t                                                        FileCompressionType compression, FileOpener *opener);\n+\tvirtual duckdb::unique_ptr<HTTPFileHandle> CreateHandle(const string &path, FileOpenFlags flags,\n+\t                                                        optional_ptr<FileOpener> opener);\n };\n \n } // namespace duckdb\ndiff --git a/extension/httpfs/include/s3fs.hpp b/extension/httpfs/include/s3fs.hpp\nindex 218450d7fefa..0209a883f238 100644\n--- a/extension/httpfs/include/s3fs.hpp\n+++ b/extension/httpfs/include/s3fs.hpp\n@@ -31,8 +31,8 @@ struct S3AuthParams {\n \tbool use_ssl = true;\n \tbool s3_url_compatibility_mode = false;\n \n-\tstatic S3AuthParams ReadFrom(FileOpener *opener, FileOpenerInfo &info);\n-\tstatic unique_ptr<S3AuthParams> ReadFromStoredCredentials(FileOpener *opener, string path);\n+\tstatic S3AuthParams ReadFrom(optional_ptr<FileOpener> opener, FileOpenerInfo &info);\n+\tstatic unique_ptr<S3AuthParams> ReadFromStoredCredentials(optional_ptr<FileOpener> opener, string path);\n };\n \n struct AWSEnvironmentCredentialsProvider {\n@@ -63,7 +63,7 @@ struct ParsedS3Url {\n \tconst string query_param;\n \tconst string trimmed_s3_url;\n \n-\tstring GetHTTPUrl(S3AuthParams &auth_params, string http_query_string = \"\");\n+\tstring GetHTTPUrl(S3AuthParams &auth_params, const string &http_query_string = \"\");\n };\n \n struct S3ConfigParams {\n@@ -75,7 +75,7 @@ struct S3ConfigParams {\n \tuint64_t max_parts_per_file;\n \tuint64_t max_upload_threads;\n \n-\tstatic S3ConfigParams ReadFrom(FileOpener *opener);\n+\tstatic S3ConfigParams ReadFrom(optional_ptr<FileOpener> opener);\n };\n \n class S3SecretHelper {\n@@ -117,14 +117,14 @@ class S3FileHandle : public HTTPFileHandle {\n \tfriend class S3FileSystem;\n \n public:\n-\tS3FileHandle(FileSystem &fs, string path_p, uint8_t flags, const HTTPParams &http_params,\n+\tS3FileHandle(FileSystem &fs, string path_p, FileOpenFlags flags, const HTTPParams &http_params,\n \t             const S3AuthParams &auth_params_p, const S3ConfigParams &config_params_p)\n \t    : HTTPFileHandle(fs, std::move(path_p), flags, http_params), auth_params(auth_params_p),\n \t      config_params(config_params_p), uploads_in_progress(0), parts_uploaded(0), upload_finalized(false),\n \t      uploader_has_error(false), upload_exception(nullptr) {\n-\t\tif (flags & FileFlags::FILE_FLAGS_WRITE && flags & FileFlags::FILE_FLAGS_READ) {\n+\t\tif (flags.OpenForReading() && flags.OpenForWriting()) {\n \t\t\tthrow NotImplementedException(\"Cannot open an HTTP file for both reading and writing\");\n-\t\t} else if (flags & FileFlags::FILE_FLAGS_APPEND) {\n+\t\t} else if (flags.OpenForAppending()) {\n \t\t\tthrow NotImplementedException(\"Cannot open an HTTP file for appending\");\n \t\t}\n \t}\n@@ -135,7 +135,7 @@ class S3FileHandle : public HTTPFileHandle {\n \n public:\n \tvoid Close() override;\n-\tvoid Initialize(FileOpener *opener) override;\n+\tvoid Initialize(optional_ptr<FileOpener> opener) override;\n \n \tshared_ptr<S3WriteBuffer> GetBuffer(uint16_t write_buffer_idx);\n \n@@ -235,8 +235,8 @@ class S3FileSystem : public HTTPFileSystem {\n \n protected:\n \tstatic void NotifyUploadsInProgress(S3FileHandle &file_handle);\n-\tduckdb::unique_ptr<HTTPFileHandle> CreateHandle(const string &path, uint8_t flags, FileLockType lock,\n-\t                                                FileCompressionType compression, FileOpener *opener) override;\n+\tduckdb::unique_ptr<HTTPFileHandle> CreateHandle(const string &path, FileOpenFlags flags,\n+\t                                                optional_ptr<FileOpener> opener) override;\n \n \tvoid FlushBuffer(S3FileHandle &handle, shared_ptr<S3WriteBuffer> write_buffer);\n \tstring GetPayloadHash(char *buffer, idx_t buffer_len);\ndiff --git a/extension/httpfs/s3fs.cpp b/extension/httpfs/s3fs.cpp\nindex aac3169c59ca..49c5ca49074f 100644\n--- a/extension/httpfs/s3fs.cpp\n+++ b/extension/httpfs/s3fs.cpp\n@@ -179,7 +179,7 @@ S3AuthParams AWSEnvironmentCredentialsProvider::CreateParams() {\n \treturn params;\n }\n \n-unique_ptr<S3AuthParams> S3AuthParams::ReadFromStoredCredentials(FileOpener *opener, string path) {\n+unique_ptr<S3AuthParams> S3AuthParams::ReadFromStoredCredentials(optional_ptr<FileOpener> opener, string path) {\n \tif (!opener) {\n \t\treturn nullptr;\n \t}\n@@ -208,7 +208,7 @@ unique_ptr<S3AuthParams> S3AuthParams::ReadFromStoredCredentials(FileOpener *ope\n \treturn make_uniq<S3AuthParams>(S3SecretHelper::GetParams(kv_secret));\n }\n \n-S3AuthParams S3AuthParams::ReadFrom(FileOpener *opener, FileOpenerInfo &info) {\n+S3AuthParams S3AuthParams::ReadFrom(optional_ptr<FileOpener> opener, FileOpenerInfo &info) {\n \tS3AuthParams result;\n \tValue value;\n \n@@ -321,7 +321,7 @@ S3FileHandle::~S3FileHandle() {\n \tClose();\n }\n \n-S3ConfigParams S3ConfigParams::ReadFrom(FileOpener *opener) {\n+S3ConfigParams S3ConfigParams::ReadFrom(optional_ptr<FileOpener> opener) {\n \tuint64_t uploader_max_filesize;\n \tuint64_t max_parts_per_file;\n \tuint64_t max_upload_threads;\n@@ -350,7 +350,7 @@ S3ConfigParams S3ConfigParams::ReadFrom(FileOpener *opener) {\n \n void S3FileHandle::Close() {\n \tauto &s3fs = (S3FileSystem &)file_system;\n-\tif ((flags & FileFlags::FILE_FLAGS_WRITE) && !upload_finalized) {\n+\tif (flags.OpenForWriting() && !upload_finalized) {\n \t\ts3fs.FlushAllBuffers(*this);\n \t\tif (parts_uploaded) {\n \t\t\ts3fs.FinalizeMultipartUpload(*this);\n@@ -713,7 +713,7 @@ string S3FileSystem::GetPayloadHash(char *buffer, idx_t buffer_len) {\n \t}\n }\n \n-string ParsedS3Url::GetHTTPUrl(S3AuthParams &auth_params, string http_query_string) {\n+string ParsedS3Url::GetHTTPUrl(S3AuthParams &auth_params, const string &http_query_string) {\n \tstring full_url = http_proto + host + S3FileSystem::UrlEncode(path);\n \n \tif (!http_query_string.empty()) {\n@@ -725,7 +725,7 @@ string ParsedS3Url::GetHTTPUrl(S3AuthParams &auth_params, string http_query_stri\n unique_ptr<ResponseWrapper> S3FileSystem::PostRequest(FileHandle &handle, string url, HeaderMap header_map,\n                                                       duckdb::unique_ptr<char[]> &buffer_out, idx_t &buffer_out_len,\n                                                       char *buffer_in, idx_t buffer_in_len, string http_params) {\n-\tauto auth_params = static_cast<S3FileHandle &>(handle).auth_params;\n+\tauto auth_params = handle.Cast<S3FileHandle>().auth_params;\n \tauto parsed_s3_url = S3UrlParse(url, auth_params);\n \tstring http_url = parsed_s3_url.GetHTTPUrl(auth_params, http_params);\n \tauto payload_hash = GetPayloadHash(buffer_in, buffer_in_len);\n@@ -737,7 +737,7 @@ unique_ptr<ResponseWrapper> S3FileSystem::PostRequest(FileHandle &handle, string\n \n unique_ptr<ResponseWrapper> S3FileSystem::PutRequest(FileHandle &handle, string url, HeaderMap header_map,\n                                                      char *buffer_in, idx_t buffer_in_len, string http_params) {\n-\tauto auth_params = static_cast<S3FileHandle &>(handle).auth_params;\n+\tauto auth_params = handle.Cast<S3FileHandle>().auth_params;\n \tauto parsed_s3_url = S3UrlParse(url, auth_params);\n \tstring http_url = parsed_s3_url.GetHTTPUrl(auth_params, http_params);\n \tauto content_type = \"application/octet-stream\";\n@@ -749,7 +749,7 @@ unique_ptr<ResponseWrapper> S3FileSystem::PutRequest(FileHandle &handle, string\n }\n \n unique_ptr<ResponseWrapper> S3FileSystem::HeadRequest(FileHandle &handle, string s3_url, HeaderMap header_map) {\n-\tauto auth_params = static_cast<S3FileHandle &>(handle).auth_params;\n+\tauto auth_params = handle.Cast<S3FileHandle>().auth_params;\n \tauto parsed_s3_url = S3UrlParse(s3_url, auth_params);\n \tstring http_url = parsed_s3_url.GetHTTPUrl(auth_params);\n \tauto headers =\n@@ -758,7 +758,7 @@ unique_ptr<ResponseWrapper> S3FileSystem::HeadRequest(FileHandle &handle, string\n }\n \n unique_ptr<ResponseWrapper> S3FileSystem::GetRequest(FileHandle &handle, string s3_url, HeaderMap header_map) {\n-\tauto auth_params = static_cast<S3FileHandle &>(handle).auth_params;\n+\tauto auth_params = handle.Cast<S3FileHandle>().auth_params;\n \tauto parsed_s3_url = S3UrlParse(s3_url, auth_params);\n \tstring http_url = parsed_s3_url.GetHTTPUrl(auth_params);\n \tauto headers =\n@@ -768,7 +768,7 @@ unique_ptr<ResponseWrapper> S3FileSystem::GetRequest(FileHandle &handle, string\n \n unique_ptr<ResponseWrapper> S3FileSystem::GetRangeRequest(FileHandle &handle, string s3_url, HeaderMap header_map,\n                                                           idx_t file_offset, char *buffer_out, idx_t buffer_out_len) {\n-\tauto auth_params = static_cast<S3FileHandle &>(handle).auth_params;\n+\tauto auth_params = handle.Cast<S3FileHandle>().auth_params;\n \tauto parsed_s3_url = S3UrlParse(s3_url, auth_params);\n \tstring http_url = parsed_s3_url.GetHTTPUrl(auth_params);\n \tauto headers =\n@@ -776,8 +776,8 @@ unique_ptr<ResponseWrapper> S3FileSystem::GetRangeRequest(FileHandle &handle, st\n \treturn HTTPFileSystem::GetRangeRequest(handle, http_url, headers, file_offset, buffer_out, buffer_out_len);\n }\n \n-unique_ptr<HTTPFileHandle> S3FileSystem::CreateHandle(const string &path, uint8_t flags, FileLockType lock,\n-                                                      FileCompressionType compression, FileOpener *opener) {\n+unique_ptr<HTTPFileHandle> S3FileSystem::CreateHandle(const string &path, FileOpenFlags flags,\n+                                                      optional_ptr<FileOpener> opener) {\n \tFileOpenerInfo info = {path};\n \n \tS3AuthParams auth_params;\n@@ -889,12 +889,12 @@ void S3FileSystem::Verify() {\n \t// TODO add a test that checks the signing for path-style\n }\n \n-void S3FileHandle::Initialize(FileOpener *opener) {\n+void S3FileHandle::Initialize(optional_ptr<FileOpener> opener) {\n \tHTTPFileHandle::Initialize(opener);\n \n-\tauto &s3fs = (S3FileSystem &)file_system;\n+\tauto &s3fs = file_system.Cast<S3FileSystem>();\n \n-\tif (flags & FileFlags::FILE_FLAGS_WRITE) {\n+\tif (flags.OpenForWriting()) {\n \t\tauto aws_minimum_part_size = 5242880; // 5 MiB https://docs.aws.amazon.com/AmazonS3/latest/userguide/qfacts.html\n \t\tauto max_part_count = config_params.max_parts_per_file;\n \t\tauto required_part_size = config_params.max_file_size / max_part_count;\n@@ -916,7 +916,7 @@ bool S3FileSystem::CanHandleFile(const string &fpath) {\n }\n \n void S3FileSystem::FileSync(FileHandle &handle) {\n-\tauto &s3fh = (S3FileHandle &)handle;\n+\tauto &s3fh = handle.Cast<S3FileHandle>();\n \tif (!s3fh.upload_finalized) {\n \t\tFlushAllBuffers(s3fh);\n \t\tFinalizeMultipartUpload(s3fh);\n@@ -924,8 +924,8 @@ void S3FileSystem::FileSync(FileHandle &handle) {\n }\n \n void S3FileSystem::Write(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location) {\n-\tauto &s3fh = (S3FileHandle &)handle;\n-\tif (!(s3fh.flags & FileFlags::FILE_FLAGS_WRITE)) {\n+\tauto &s3fh = handle.Cast<S3FileHandle>();\n+\tif (!s3fh.flags.OpenForWriting()) {\n \t\tthrow InternalException(\"Write called on file not opened in write mode\");\n \t}\n \tint64_t bytes_written = 0;\n@@ -1020,7 +1020,7 @@ vector<string> S3FileSystem::Glob(const string &glob_pattern, FileOpener *opener\n \n \t// Do main listobjectsv2 request\n \tvector<string> s3_keys;\n-\tstring main_continuation_token = \"\";\n+\tstring main_continuation_token;\n \n \t// Main paging loop\n \tdo {\n@@ -1038,7 +1038,7 @@ vector<string> S3FileSystem::Glob(const string &glob_pattern, FileOpener *opener\n \n \t\t\t// TODO we could optimize here by doing a match on the prefix, if it doesn't match we can skip this prefix\n \t\t\t// Paging loop for common prefix requests\n-\t\t\tstring common_prefix_continuation_token = \"\";\n+\t\t\tstring common_prefix_continuation_token;\n \t\t\tdo {\n \t\t\t\tauto prefix_res =\n \t\t\t\t    AWSListObjectV2::Request(prefix_path, http_params, s3_auth_params, common_prefix_continuation_token,\n@@ -1098,7 +1098,7 @@ string AWSListObjectV2::Request(string &path, HTTPParams &http_params, S3AuthPar\n \t// Construct the ListObjectsV2 call\n \tstring req_path = parsed_url.path.substr(0, parsed_url.path.length() - parsed_url.key.length());\n \n-\tstring req_params = \"\";\n+\tstring req_params;\n \tif (!continuation_token.empty()) {\n \t\treq_params += \"continuation-token=\" + S3FileSystem::UrlEncode(continuation_token, true);\n \t\treq_params += \"&\";\ndiff --git a/extension/json/buffered_json_reader.cpp b/extension/json/buffered_json_reader.cpp\nindex 2e4653862f4b..f36db96c6de6 100644\n--- a/extension/json/buffered_json_reader.cpp\n+++ b/extension/json/buffered_json_reader.cpp\n@@ -193,9 +193,8 @@ BufferedJSONReader::BufferedJSONReader(ClientContext &context, BufferedJSONReade\n void BufferedJSONReader::OpenJSONFile() {\n \tlock_guard<mutex> guard(lock);\n \tif (!IsOpen()) {\n-\t\tauto &file_system = FileSystem::GetFileSystem(context);\n-\t\tauto regular_file_handle = file_system.OpenFile(file_name.c_str(), FileFlags::FILE_FLAGS_READ,\n-\t\t                                                FileLockType::NO_LOCK, options.compression);\n+\t\tauto &fs = FileSystem::GetFileSystem(context);\n+\t\tauto regular_file_handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ | options.compression);\n \t\tfile_handle = make_uniq<JSONFileHandle>(std::move(regular_file_handle), BufferAllocator::Get(context));\n \t}\n \tReset();\ndiff --git a/scripts/generate_presigned_url.sh b/scripts/generate_presigned_url.sh\nindex ddda5a6072e5..a60886b67349 100755\n--- a/scripts/generate_presigned_url.sh\n+++ b/scripts/generate_presigned_url.sh\n@@ -1,6 +1,8 @@\n #!/usr/bin/env bash\n #Note: DONT run as root\n \n+set -e\n+\n mkdir -p data/parquet-testing/presigned\n \n generate_large_parquet_query=$(cat <<EOF\ndiff --git a/src/common/enum_util.cpp b/src/common/enum_util.cpp\nindex fab315c289ab..9741c0cdba8b 100644\n--- a/src/common/enum_util.cpp\n+++ b/src/common/enum_util.cpp\n@@ -56,7 +56,7 @@\n #include \"duckdb/common/exception_format_value.hpp\"\n #include \"duckdb/common/extra_type_info.hpp\"\n #include \"duckdb/common/file_buffer.hpp\"\n-#include \"duckdb/common/file_system.hpp\"\n+#include \"duckdb/common/file_open_flags.hpp\"\n #include \"duckdb/common/printer.hpp\"\n #include \"duckdb/common/sort/partition_state.hpp\"\n #include \"duckdb/common/types.hpp\"\ndiff --git a/src/common/file_system.cpp b/src/common/file_system.cpp\nindex d007e8e89862..725e8871cfea 100644\n--- a/src/common/file_system.cpp\n+++ b/src/common/file_system.cpp\n@@ -45,6 +45,40 @@ extern \"C\" WINBASEAPI BOOL WINAPI GetPhysicallyInstalledSystemMemory(PULONGLONG)\n \n namespace duckdb {\n \n+constexpr FileOpenFlags FileFlags::FILE_FLAGS_READ;\n+constexpr FileOpenFlags FileFlags::FILE_FLAGS_WRITE;\n+constexpr FileOpenFlags FileFlags::FILE_FLAGS_DIRECT_IO;\n+constexpr FileOpenFlags FileFlags::FILE_FLAGS_FILE_CREATE;\n+constexpr FileOpenFlags FileFlags::FILE_FLAGS_FILE_CREATE_NEW;\n+constexpr FileOpenFlags FileFlags::FILE_FLAGS_APPEND;\n+constexpr FileOpenFlags FileFlags::FILE_FLAGS_PRIVATE;\n+constexpr FileOpenFlags FileFlags::FILE_FLAGS_NULL_IF_NOT_EXISTS;\n+\n+void FileOpenFlags::Verify() {\n+#ifdef DEBUG\n+\tbool is_read = flags & FileOpenFlags::FILE_FLAGS_READ;\n+\tbool is_write = flags & FileOpenFlags::FILE_FLAGS_WRITE;\n+\tbool is_create =\n+\t    (flags & FileOpenFlags::FILE_FLAGS_FILE_CREATE) || (flags & FileOpenFlags::FILE_FLAGS_FILE_CREATE_NEW);\n+\tbool is_private = (flags & FileOpenFlags::FILE_FLAGS_PRIVATE);\n+\tbool null_if_not_exists = flags & FileOpenFlags::FILE_FLAGS_NULL_IF_NOT_EXISTS;\n+\n+\t// require either READ or WRITE (or both)\n+\tD_ASSERT(is_read || is_write);\n+\t// CREATE/Append flags require writing\n+\tD_ASSERT(is_write || !(flags & FileOpenFlags::FILE_FLAGS_APPEND));\n+\tD_ASSERT(is_write || !(flags & FileOpenFlags::FILE_FLAGS_FILE_CREATE));\n+\tD_ASSERT(is_write || !(flags & FileOpenFlags::FILE_FLAGS_FILE_CREATE_NEW));\n+\t// cannot combine CREATE and CREATE_NEW flags\n+\tD_ASSERT(!(flags & FileOpenFlags::FILE_FLAGS_FILE_CREATE && flags & FileOpenFlags::FILE_FLAGS_FILE_CREATE_NEW));\n+\n+\t// For is_private can only be set along with a create flag\n+\tD_ASSERT(!is_private || is_create);\n+\t// FILE_FLAGS_NULL_IF_NOT_EXISTS cannot be combined with CREATE/CREATE_NEW\n+\tD_ASSERT(!(null_if_not_exists && is_create));\n+#endif\n+}\n+\n FileSystem::~FileSystem() {\n }\n \n@@ -286,8 +320,7 @@ string FileSystem::ExpandPath(const string &path) {\n }\n \n // LCOV_EXCL_START\n-unique_ptr<FileHandle> FileSystem::OpenFile(const string &path, uint8_t flags, FileLockType lock,\n-                                            FileCompressionType compression, FileOpener *opener) {\n+unique_ptr<FileHandle> FileSystem::OpenFile(const string &path, FileOpenFlags flags, optional_ptr<FileOpener> opener) {\n \tthrow NotImplementedException(\"%s: OpenFile is not implemented!\", GetName());\n }\n \ndiff --git a/src/common/http_state.cpp b/src/common/http_state.cpp\nindex 0c6920867e6b..b07c0d4b31f2 100644\n--- a/src/common/http_state.cpp\n+++ b/src/common/http_state.cpp\n@@ -74,7 +74,7 @@ shared_ptr<HTTPState> HTTPState::TryGetState(ClientContext &context, bool create\n \treturn http_state;\n }\n \n-shared_ptr<HTTPState> HTTPState::TryGetState(FileOpener *opener, bool create_on_missing) {\n+shared_ptr<HTTPState> HTTPState::TryGetState(optional_ptr<FileOpener> opener, bool create_on_missing) {\n \tauto client_context = FileOpener::TryGetClientContext(opener);\n \tif (client_context) {\n \t\treturn TryGetState(*client_context, create_on_missing);\ndiff --git a/src/common/local_file_system.cpp b/src/common/local_file_system.cpp\nindex e1c073a06e4d..da504d918a9f 100644\n--- a/src/common/local_file_system.cpp\n+++ b/src/common/local_file_system.cpp\n@@ -44,37 +44,16 @@ extern \"C\" WINBASEAPI BOOL WINAPI GetPhysicallyInstalledSystemMemory(PULONGLONG)\n // See e.g.:\n // https://opensource.apple.com/source/CarbonHeaders/CarbonHeaders-18.1/TargetConditionals.h.auto.html\n #elif defined(__APPLE__)\n-#include <TargetConditionals.h>                             // NOLINT\n-#if not(defined(TARGET_OS_IPHONE) && TARGET_OS_IPHONE == 1) // NOLINT\n-#include <libproc.h>                                        // NOLINT\n-#endif                                                      // NOLINT\n+#include <TargetConditionals.h>\n+#if not(defined(TARGET_OS_IPHONE) && TARGET_OS_IPHONE == 1)\n+#include <libproc.h>\n+#endif\n #elif defined(_WIN32)\n #include <restartmanager.h>\n #endif\n \n namespace duckdb {\n \n-static void AssertValidFileFlags(uint8_t flags) {\n-#ifdef DEBUG\n-\tbool is_read = flags & FileFlags::FILE_FLAGS_READ;\n-\tbool is_write = flags & FileFlags::FILE_FLAGS_WRITE;\n-\tbool is_create = (flags & FileFlags::FILE_FLAGS_FILE_CREATE) || (flags & FileFlags::FILE_FLAGS_FILE_CREATE_NEW);\n-\tbool is_private = (flags & FileFlags::FILE_FLAGS_PRIVATE);\n-\n-\t// require either READ or WRITE (or both)\n-\tD_ASSERT(is_read || is_write);\n-\t// CREATE/Append flags require writing\n-\tD_ASSERT(is_write || !(flags & FileFlags::FILE_FLAGS_APPEND));\n-\tD_ASSERT(is_write || !(flags & FileFlags::FILE_FLAGS_FILE_CREATE));\n-\tD_ASSERT(is_write || !(flags & FileFlags::FILE_FLAGS_FILE_CREATE_NEW));\n-\t// cannot combine CREATE and CREATE_NEW flags\n-\tD_ASSERT(!(flags & FileFlags::FILE_FLAGS_FILE_CREATE && flags & FileFlags::FILE_FLAGS_FILE_CREATE_NEW));\n-\n-\t// For is_private can only be set along with a create flag\n-\tD_ASSERT(!is_private || is_create);\n-#endif\n-}\n-\n #ifndef _WIN32\n bool LocalFileSystem::FileExists(const string &filename) {\n \tif (!filename.empty()) {\n@@ -292,19 +271,19 @@ bool LocalFileSystem::IsPrivateFile(const string &path_p, FileOpener *opener) {\n \treturn true;\n }\n \n-unique_ptr<FileHandle> LocalFileSystem::OpenFile(const string &path_p, uint8_t flags, FileLockType lock_type,\n-                                                 FileCompressionType compression, FileOpener *opener) {\n+unique_ptr<FileHandle> LocalFileSystem::OpenFile(const string &path_p, FileOpenFlags flags,\n+                                                 optional_ptr<FileOpener> opener) {\n \tauto path = FileSystem::ExpandPath(path_p, opener);\n-\tif (compression != FileCompressionType::UNCOMPRESSED) {\n+\tif (flags.Compression() != FileCompressionType::UNCOMPRESSED) {\n \t\tthrow NotImplementedException(\"Unsupported compression type for default file system\");\n \t}\n \n-\tAssertValidFileFlags(flags);\n+\tflags.Verify();\n \n \tint open_flags = 0;\n \tint rc;\n-\tbool open_read = flags & FileFlags::FILE_FLAGS_READ;\n-\tbool open_write = flags & FileFlags::FILE_FLAGS_WRITE;\n+\tbool open_read = flags.OpenForReading();\n+\tbool open_write = flags.OpenForWriting();\n \tif (open_read && open_write) {\n \t\topen_flags = O_RDWR;\n \t} else if (open_read) {\n@@ -316,18 +295,18 @@ unique_ptr<FileHandle> LocalFileSystem::OpenFile(const string &path_p, uint8_t f\n \t}\n \tif (open_write) {\n \t\t// need Read or Write\n-\t\tD_ASSERT(flags & FileFlags::FILE_FLAGS_WRITE);\n+\t\tD_ASSERT(flags.OpenForWriting());\n \t\topen_flags |= O_CLOEXEC;\n-\t\tif (flags & FileFlags::FILE_FLAGS_FILE_CREATE) {\n+\t\tif (flags.CreateFileIfNotExists()) {\n \t\t\topen_flags |= O_CREAT;\n-\t\t} else if (flags & FileFlags::FILE_FLAGS_FILE_CREATE_NEW) {\n+\t\t} else if (flags.OverwriteExistingFile()) {\n \t\t\topen_flags |= O_CREAT | O_TRUNC;\n \t\t}\n-\t\tif (flags & FileFlags::FILE_FLAGS_APPEND) {\n+\t\tif (flags.OpenForAppending()) {\n \t\t\topen_flags |= O_APPEND;\n \t\t}\n \t}\n-\tif (flags & FileFlags::FILE_FLAGS_DIRECT_IO) {\n+\tif (flags.DirectIO()) {\n #if defined(__sun) && defined(__SVR4)\n \t\tthrow InvalidInputException(\"DIRECT_IO not supported on Solaris\");\n #endif\n@@ -341,7 +320,7 @@ unique_ptr<FileHandle> LocalFileSystem::OpenFile(const string &path_p, uint8_t f\n \n \t// Determine permissions\n \tmode_t filesec;\n-\tif (flags & FileFlags::FILE_FLAGS_PRIVATE) {\n+\tif (flags.CreatePrivateFile()) {\n \t\topen_flags |= O_EXCL; // Ensure we error on existing files or the permissions may not set\n \t\tfilesec = 0600;\n \t} else {\n@@ -352,6 +331,9 @@ unique_ptr<FileHandle> LocalFileSystem::OpenFile(const string &path_p, uint8_t f\n \tint fd = open(path.c_str(), open_flags, filesec);\n \n \tif (fd == -1) {\n+\t\tif (flags.ReturnNullIfNotExists() && errno == ENOENT) {\n+\t\t\treturn nullptr;\n+\t\t}\n \t\tthrow IOException(\"Cannot open file \\\"%s\\\": %s\", {{\"errno\", std::to_string(errno)}}, path, strerror(errno));\n \t}\n \t// #if defined(__DARWIN__) || defined(__APPLE__)\n@@ -363,14 +345,14 @@ unique_ptr<FileHandle> LocalFileSystem::OpenFile(const string &path_p, uint8_t f\n \t// \t\t}\n \t// \t}\n \t// #endif\n-\tif (lock_type != FileLockType::NO_LOCK) {\n+\tif (flags.Lock() != FileLockType::NO_LOCK) {\n \t\t// set lock on file\n \t\t// but only if it is not an input/output stream\n \t\tauto file_type = GetFileTypeInternal(fd);\n \t\tif (file_type != FileType::FILE_TYPE_FIFO && file_type != FileType::FILE_TYPE_SOCKET) {\n \t\t\tstruct flock fl;\n \t\t\tmemset(&fl, 0, sizeof fl);\n-\t\t\tfl.l_type = lock_type == FileLockType::READ_LOCK ? F_RDLCK : F_WRLCK;\n+\t\t\tfl.l_type = flags.Lock() == FileLockType::READ_LOCK ? F_RDLCK : F_WRLCK;\n \t\t\tfl.l_whence = SEEK_SET;\n \t\t\tfl.l_start = 0;\n \t\t\tfl.l_len = 0;\n@@ -387,7 +369,7 @@ unique_ptr<FileHandle> LocalFileSystem::OpenFile(const string &path_p, uint8_t f\n \t\t\t\t\tmessage = AdditionalProcessInfo(*this, fl.l_pid);\n \t\t\t\t}\n \n-\t\t\t\tif (lock_type == FileLockType::WRITE_LOCK) {\n+\t\t\t\tif (flags.Lock() == FileLockType::WRITE_LOCK) {\n \t\t\t\t\t// maybe we can get a read lock instead and tell this to the user.\n \t\t\t\t\tfl.l_type = F_RDLCK;\n \t\t\t\t\trc = fcntl(fd, F_SETLK, &fl);\n@@ -776,20 +758,20 @@ bool LocalFileSystem::IsPrivateFile(const string &path_p, FileOpener *opener) {\n \treturn true;\n }\n \n-unique_ptr<FileHandle> LocalFileSystem::OpenFile(const string &path_p, uint8_t flags, FileLockType lock_type,\n-                                                 FileCompressionType compression, FileOpener *opener) {\n+unique_ptr<FileHandle> LocalFileSystem::OpenFile(const string &path_p, FileOpenFlags flags,\n+                                                 optional_ptr<FileOpener> opener) {\n \tauto path = FileSystem::ExpandPath(path_p, opener);\n-\tif (compression != FileCompressionType::UNCOMPRESSED) {\n+\tif (flags.Compression() != FileCompressionType::UNCOMPRESSED) {\n \t\tthrow NotImplementedException(\"Unsupported compression type for default file system\");\n \t}\n-\tAssertValidFileFlags(flags);\n+\tflags.Verify();\n \n \tDWORD desired_access;\n \tDWORD share_mode;\n \tDWORD creation_disposition = OPEN_EXISTING;\n \tDWORD flags_and_attributes = FILE_ATTRIBUTE_NORMAL;\n-\tbool open_read = flags & FileFlags::FILE_FLAGS_READ;\n-\tbool open_write = flags & FileFlags::FILE_FLAGS_WRITE;\n+\tbool open_read = flags.OpenForReading();\n+\tbool open_write = flags.OpenForWriting();\n \tif (open_read && open_write) {\n \t\tdesired_access = GENERIC_READ | GENERIC_WRITE;\n \t\tshare_mode = 0;\n@@ -803,30 +785,33 @@ unique_ptr<FileHandle> LocalFileSystem::OpenFile(const string &path_p, uint8_t f\n \t\tthrow InternalException(\"READ, WRITE or both should be specified when opening a file\");\n \t}\n \tif (open_write) {\n-\t\tif (flags & FileFlags::FILE_FLAGS_FILE_CREATE) {\n+\t\tif (flags.CreateFileIfNotExists()) {\n \t\t\tcreation_disposition = OPEN_ALWAYS;\n-\t\t} else if (flags & FileFlags::FILE_FLAGS_FILE_CREATE_NEW) {\n+\t\t} else if (flags.OverwriteExistingFile()) {\n \t\t\tcreation_disposition = CREATE_ALWAYS;\n \t\t}\n \t}\n-\tif (flags & FileFlags::FILE_FLAGS_DIRECT_IO) {\n+\tif (flags.DirectIO()) {\n \t\tflags_and_attributes |= FILE_FLAG_NO_BUFFERING;\n \t}\n \tauto unicode_path = WindowsUtil::UTF8ToUnicode(path.c_str());\n \tHANDLE hFile = CreateFileW(unicode_path.c_str(), desired_access, share_mode, NULL, creation_disposition,\n \t                           flags_and_attributes, NULL);\n \tif (hFile == INVALID_HANDLE_VALUE) {\n+\t\tif (flags.ReturnNullIfNotExists() && GetLastError() == ERROR_FILE_NOT_FOUND) {\n+\t\t\treturn nullptr;\n+\t\t}\n \t\tauto error = LocalFileSystem::GetLastErrorAsString();\n \n \t\tauto better_error = AdditionalLockInfo(unicode_path);\n \t\tif (!better_error.empty()) {\n \t\t\tthrow IOException(better_error);\n+\t\t} else {\n+\t\t\tthrow IOException(\"Cannot open file \\\"%s\\\": %s\", path.c_str(), error);\n \t\t}\n-\n-\t\tthrow IOException(\"Cannot open file \\\"%s\\\": %s\", path.c_str(), error);\n \t}\n \tauto handle = make_uniq<WindowsFileHandle>(*this, path.c_str(), hFile);\n-\tif (flags & FileFlags::FILE_FLAGS_APPEND) {\n+\tif (flags.OpenForAppending()) {\n \t\tauto file_size = GetFileSize(*handle);\n \t\tSetFilePointer(*handle, file_size);\n \t}\ndiff --git a/src/common/serializer/buffered_file_reader.cpp b/src/common/serializer/buffered_file_reader.cpp\nindex ce76c354032f..762e1ca889a0 100644\n--- a/src/common/serializer/buffered_file_reader.cpp\n+++ b/src/common/serializer/buffered_file_reader.cpp\n@@ -10,7 +10,13 @@ namespace duckdb {\n BufferedFileReader::BufferedFileReader(FileSystem &fs, const char *path, FileLockType lock_type,\n                                        optional_ptr<FileOpener> opener)\n     : fs(fs), data(make_unsafe_uniq_array<data_t>(FILE_BUFFER_SIZE)), offset(0), read_data(0), total_read(0) {\n-\thandle = fs.OpenFile(path, FileFlags::FILE_FLAGS_READ, lock_type, FileSystem::DEFAULT_COMPRESSION, opener.get());\n+\thandle = fs.OpenFile(path, FileFlags::FILE_FLAGS_READ | lock_type, opener.get());\n+\tfile_size = fs.GetFileSize(*handle);\n+}\n+\n+BufferedFileReader::BufferedFileReader(FileSystem &fs, unique_ptr<FileHandle> handle_p)\n+    : fs(fs), data(make_unsafe_uniq_array<data_t>(FILE_BUFFER_SIZE)), offset(0), read_data(0),\n+      handle(std::move(handle_p)), total_read(0) {\n \tfile_size = fs.GetFileSize(*handle);\n }\n \n@@ -51,6 +57,12 @@ void BufferedFileReader::Seek(uint64_t location) {\n \tread_data = offset = 0;\n }\n \n+void BufferedFileReader::Reset() {\n+\thandle->Reset();\n+\ttotal_read = 0;\n+\tread_data = offset = 0;\n+}\n+\n uint64_t BufferedFileReader::CurrentOffset() {\n \treturn total_read + offset;\n }\ndiff --git a/src/common/serializer/buffered_file_writer.cpp b/src/common/serializer/buffered_file_writer.cpp\nindex 6055f4140bd5..109b5d7adfec 100644\n--- a/src/common/serializer/buffered_file_writer.cpp\n+++ b/src/common/serializer/buffered_file_writer.cpp\n@@ -7,11 +7,11 @@\n namespace duckdb {\n \n // Remove this when we switch C++17: https://stackoverflow.com/a/53350948\n-constexpr uint8_t BufferedFileWriter::DEFAULT_OPEN_FLAGS;\n+constexpr FileOpenFlags BufferedFileWriter::DEFAULT_OPEN_FLAGS;\n \n-BufferedFileWriter::BufferedFileWriter(FileSystem &fs, const string &path_p, uint8_t open_flags)\n+BufferedFileWriter::BufferedFileWriter(FileSystem &fs, const string &path_p, FileOpenFlags open_flags)\n     : fs(fs), path(path_p), data(make_unsafe_uniq_array<data_t>(FILE_BUFFER_SIZE)), offset(0), total_written(0) {\n-\thandle = fs.OpenFile(path, open_flags, FileLockType::WRITE_LOCK);\n+\thandle = fs.OpenFile(path, open_flags | FileLockType::WRITE_LOCK);\n }\n \n int64_t BufferedFileWriter::GetFileSize() {\ndiff --git a/src/common/virtual_file_system.cpp b/src/common/virtual_file_system.cpp\nindex 3337623e48c9..2c95db643f78 100644\n--- a/src/common/virtual_file_system.cpp\n+++ b/src/common/virtual_file_system.cpp\n@@ -9,8 +9,9 @@ VirtualFileSystem::VirtualFileSystem() : default_fs(FileSystem::CreateLocal()) {\n \tVirtualFileSystem::RegisterSubSystem(FileCompressionType::GZIP, make_uniq<GZipFileSystem>());\n }\n \n-unique_ptr<FileHandle> VirtualFileSystem::OpenFile(const string &path, uint8_t flags, FileLockType lock,\n-                                                   FileCompressionType compression, FileOpener *opener) {\n+unique_ptr<FileHandle> VirtualFileSystem::OpenFile(const string &path, FileOpenFlags flags,\n+                                                   optional_ptr<FileOpener> opener) {\n+\tauto compression = flags.Compression();\n \tif (compression == FileCompressionType::AUTO_DETECT) {\n \t\t// auto detect compression settings based on file name\n \t\tauto lower_path = StringUtil::Lower(path);\n@@ -26,8 +27,12 @@ unique_ptr<FileHandle> VirtualFileSystem::OpenFile(const string &path, uint8_t f\n \t\t\tcompression = FileCompressionType::UNCOMPRESSED;\n \t\t}\n \t}\n-\t// open the base file handle\n-\tauto file_handle = FindFileSystem(path).OpenFile(path, flags, lock, FileCompressionType::UNCOMPRESSED, opener);\n+\t// open the base file handle in UNCOMPRESSED mode\n+\tflags.SetCompression(FileCompressionType::UNCOMPRESSED);\n+\tauto file_handle = FindFileSystem(path).OpenFile(path, flags, opener);\n+\tif (!file_handle) {\n+\t\treturn nullptr;\n+\t}\n \tif (file_handle->GetType() == FileType::FILE_TYPE_FIFO) {\n \t\tfile_handle = PipeFileSystem::OpenPipe(std::move(file_handle));\n \t} else if (compression != FileCompressionType::UNCOMPRESSED) {\n@@ -36,7 +41,7 @@ unique_ptr<FileHandle> VirtualFileSystem::OpenFile(const string &path, uint8_t f\n \t\t\tthrow NotImplementedException(\n \t\t\t    \"Attempting to open a compressed file, but the compression type is not supported\");\n \t\t}\n-\t\tfile_handle = entry->second->OpenCompressedFile(std::move(file_handle), flags & FileFlags::FILE_FLAGS_WRITE);\n+\t\tfile_handle = entry->second->OpenCompressedFile(std::move(file_handle), flags.OpenForWriting());\n \t}\n \treturn file_handle;\n }\n@@ -103,6 +108,7 @@ bool VirtualFileSystem::FileExists(const string &filename) {\n bool VirtualFileSystem::IsPipe(const string &filename) {\n \treturn FindFileSystem(filename).IsPipe(filename);\n }\n+\n void VirtualFileSystem::RemoveFile(const string &filename) {\n \tFindFileSystem(filename).RemoveFile(filename);\n }\ndiff --git a/src/execution/operator/csv_scanner/buffer_manager/csv_file_handle.cpp b/src/execution/operator/csv_scanner/buffer_manager/csv_file_handle.cpp\nindex cbb1c1cd86e7..5a73815e2763 100644\n--- a/src/execution/operator/csv_scanner/buffer_manager/csv_file_handle.cpp\n+++ b/src/execution/operator/csv_scanner/buffer_manager/csv_file_handle.cpp\n@@ -14,7 +14,7 @@ CSVFileHandle::CSVFileHandle(FileSystem &fs, Allocator &allocator, unique_ptr<Fi\n \n unique_ptr<FileHandle> CSVFileHandle::OpenFileHandle(FileSystem &fs, Allocator &allocator, const string &path,\n                                                      FileCompressionType compression) {\n-\tauto file_handle = fs.OpenFile(path, FileFlags::FILE_FLAGS_READ, FileLockType::NO_LOCK, compression);\n+\tauto file_handle = fs.OpenFile(path, FileFlags::FILE_FLAGS_READ | compression);\n \tif (file_handle->CanSeek()) {\n \t\tfile_handle->Reset();\n \t}\ndiff --git a/src/execution/operator/persistent/physical_export.cpp b/src/execution/operator/persistent/physical_export.cpp\nindex 8209e51801c1..b4e91918e92c 100644\n--- a/src/execution/operator/persistent/physical_export.cpp\n+++ b/src/execution/operator/persistent/physical_export.cpp\n@@ -29,8 +29,8 @@ static void WriteCatalogEntries(stringstream &ss, vector<reference<CatalogEntry>\n \n static void WriteStringStreamToFile(FileSystem &fs, stringstream &ss, const string &path) {\n \tauto ss_string = ss.str();\n-\tauto handle = fs.OpenFile(path, FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE_NEW,\n-\t                          FileLockType::WRITE_LOCK);\n+\tauto handle = fs.OpenFile(path, FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE_NEW |\n+\t                                    FileLockType::WRITE_LOCK);\n \tfs.Write(*handle, (void *)ss_string.c_str(), ss_string.size());\n \thandle.reset();\n }\ndiff --git a/src/function/pragma/pragma_queries.cpp b/src/function/pragma/pragma_queries.cpp\nindex 6b1b180c0f66..a69d735bc826 100644\n--- a/src/function/pragma/pragma_queries.cpp\n+++ b/src/function/pragma/pragma_queries.cpp\n@@ -139,8 +139,7 @@ string PragmaImportDatabase(ClientContext &context, const FunctionParameters &pa\n \tvector<string> files = {\"schema.sql\", \"load.sql\"};\n \tfor (auto &file : files) {\n \t\tauto file_path = fs.JoinPath(parameters.values[0].ToString(), file);\n-\t\tauto handle = fs.OpenFile(file_path, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,\n-\t\t                          FileSystem::DEFAULT_COMPRESSION);\n+\t\tauto handle = fs.OpenFile(file_path, FileFlags::FILE_FLAGS_READ);\n \t\tauto fsize = fs.GetFileSize(*handle);\n \t\tauto buffer = make_unsafe_uniq_array<char>(fsize);\n \t\tfs.Read(*handle, buffer.get(), fsize);\ndiff --git a/src/function/table/copy_csv.cpp b/src/function/table/copy_csv.cpp\nindex e2f9a2403c08..1fa2e46e7694 100644\n--- a/src/function/table/copy_csv.cpp\n+++ b/src/function/table/copy_csv.cpp\n@@ -270,8 +270,8 @@ struct LocalWriteCSVData : public LocalFunctionData {\n struct GlobalWriteCSVData : public GlobalFunctionData {\n \tGlobalWriteCSVData(FileSystem &fs, const string &file_path, FileCompressionType compression)\n \t    : fs(fs), written_anything(false) {\n-\t\thandle = fs.OpenFile(file_path, FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE_NEW,\n-\t\t                     FileLockType::WRITE_LOCK, compression);\n+\t\thandle = fs.OpenFile(file_path, FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE_NEW |\n+\t\t                                    FileLockType::WRITE_LOCK | compression);\n \t}\n \n \t//! Write generic data, e.g., CSV header\ndiff --git a/src/include/duckdb/common/file_open_flags.hpp b/src/include/duckdb/common/file_open_flags.hpp\nnew file mode 100644\nindex 000000000000..17e0c8e9bad9\n--- /dev/null\n+++ b/src/include/duckdb/common/file_open_flags.hpp\n@@ -0,0 +1,127 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/common/file_open_flags.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/common/common.hpp\"\n+#include \"duckdb/common/enums/file_compression_type.hpp\"\n+\n+namespace duckdb {\n+\n+enum class FileLockType : uint8_t { NO_LOCK = 0, READ_LOCK = 1, WRITE_LOCK = 2 };\n+\n+class FileOpenFlags {\n+public:\n+\tstatic constexpr idx_t FILE_FLAGS_READ = idx_t(1 << 0);\n+\tstatic constexpr idx_t FILE_FLAGS_WRITE = idx_t(1 << 1);\n+\tstatic constexpr idx_t FILE_FLAGS_DIRECT_IO = idx_t(1 << 2);\n+\tstatic constexpr idx_t FILE_FLAGS_FILE_CREATE = idx_t(1 << 3);\n+\tstatic constexpr idx_t FILE_FLAGS_FILE_CREATE_NEW = idx_t(1 << 4);\n+\tstatic constexpr idx_t FILE_FLAGS_APPEND = idx_t(1 << 5);\n+\tstatic constexpr idx_t FILE_FLAGS_PRIVATE = idx_t(1 << 6);\n+\tstatic constexpr idx_t FILE_FLAGS_NULL_IF_NOT_EXISTS = idx_t(1 << 7);\n+\n+public:\n+\tFileOpenFlags() = default;\n+\tconstexpr FileOpenFlags(idx_t flags) : flags(flags) { // NOLINT: allow implicit conversion\n+\t}\n+\tconstexpr FileOpenFlags(FileLockType lock) : lock(lock) { // NOLINT: allow implicit conversion\n+\t}\n+\tconstexpr FileOpenFlags(FileCompressionType compression) // NOLINT: allow implicit conversion\n+\t    : compression(compression) {\n+\t}\n+\tconstexpr FileOpenFlags(idx_t flags, FileLockType lock, FileCompressionType compression)\n+\t    : flags(flags), lock(lock), compression(compression) {\n+\t}\n+\n+\tstatic constexpr FileLockType MergeLock(FileLockType a, FileLockType b) {\n+\t\treturn a == FileLockType::NO_LOCK ? b : a;\n+\t}\n+\n+\tstatic constexpr FileCompressionType MergeCompression(FileCompressionType a, FileCompressionType b) {\n+\t\treturn a == FileCompressionType::UNCOMPRESSED ? b : a;\n+\t}\n+\n+\tinline constexpr FileOpenFlags operator|(FileOpenFlags b) const {\n+\t\treturn FileOpenFlags(flags | b.flags, MergeLock(lock, b.lock), MergeCompression(compression, b.compression));\n+\t}\n+\tinline FileOpenFlags &operator|=(FileOpenFlags b) {\n+\t\tflags |= b.flags;\n+\t\tlock = MergeLock(lock, b.lock);\n+\t\tcompression = MergeCompression(compression, b.compression);\n+\t\treturn *this;\n+\t}\n+\n+\tFileLockType Lock() {\n+\t\treturn lock;\n+\t}\n+\n+\tFileCompressionType Compression() {\n+\t\treturn compression;\n+\t}\n+\n+\tvoid SetCompression(FileCompressionType new_compression) {\n+\t\tcompression = new_compression;\n+\t}\n+\n+\tvoid Verify();\n+\n+\tinline bool OpenForReading() const {\n+\t\treturn flags & FILE_FLAGS_READ;\n+\t}\n+\tinline bool OpenForWriting() const {\n+\t\treturn flags & FILE_FLAGS_WRITE;\n+\t}\n+\tinline bool DirectIO() const {\n+\t\treturn flags & FILE_FLAGS_DIRECT_IO;\n+\t}\n+\tinline bool CreateFileIfNotExists() const {\n+\t\treturn flags & FILE_FLAGS_FILE_CREATE;\n+\t}\n+\tinline bool OverwriteExistingFile() const {\n+\t\treturn flags & FILE_FLAGS_FILE_CREATE_NEW;\n+\t}\n+\tinline bool OpenForAppending() const {\n+\t\treturn flags & FILE_FLAGS_APPEND;\n+\t}\n+\tinline bool CreatePrivateFile() const {\n+\t\treturn flags & FILE_FLAGS_PRIVATE;\n+\t}\n+\tinline bool ReturnNullIfNotExists() const {\n+\t\treturn flags & FILE_FLAGS_NULL_IF_NOT_EXISTS;\n+\t}\n+\n+private:\n+\tidx_t flags = 0;\n+\tFileLockType lock = FileLockType::NO_LOCK;\n+\tFileCompressionType compression = FileCompressionType::UNCOMPRESSED;\n+};\n+\n+class FileFlags {\n+public:\n+\t//! Open file with read access\n+\tstatic constexpr FileOpenFlags FILE_FLAGS_READ = FileOpenFlags(FileOpenFlags::FILE_FLAGS_READ);\n+\t//! Open file with write access\n+\tstatic constexpr FileOpenFlags FILE_FLAGS_WRITE = FileOpenFlags(FileOpenFlags::FILE_FLAGS_WRITE);\n+\t//! Use direct IO when reading/writing to the file\n+\tstatic constexpr FileOpenFlags FILE_FLAGS_DIRECT_IO = FileOpenFlags(FileOpenFlags::FILE_FLAGS_DIRECT_IO);\n+\t//! Create file if not exists, can only be used together with WRITE\n+\tstatic constexpr FileOpenFlags FILE_FLAGS_FILE_CREATE = FileOpenFlags(FileOpenFlags::FILE_FLAGS_FILE_CREATE);\n+\t//! Always create a new file. If a file exists, the file is truncated. Cannot be used together with CREATE.\n+\tstatic constexpr FileOpenFlags FILE_FLAGS_FILE_CREATE_NEW =\n+\t    FileOpenFlags(FileOpenFlags::FILE_FLAGS_FILE_CREATE_NEW);\n+\t//! Open file in append mode\n+\tstatic constexpr FileOpenFlags FILE_FLAGS_APPEND = FileOpenFlags(FileOpenFlags::FILE_FLAGS_APPEND);\n+\t//! Open file with restrictive permissions (600 on linux/mac) can only be used when creating, throws if file exists\n+\tstatic constexpr FileOpenFlags FILE_FLAGS_PRIVATE = FileOpenFlags(FileOpenFlags::FILE_FLAGS_PRIVATE);\n+\t//! Return NULL if the file does not exist instead of throwing an error\n+\tstatic constexpr FileOpenFlags FILE_FLAGS_NULL_IF_NOT_EXISTS =\n+\t    FileOpenFlags(FileOpenFlags::FILE_FLAGS_NULL_IF_NOT_EXISTS);\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/common/file_opener.hpp b/src/include/duckdb/common/file_opener.hpp\nindex 502ea926dcf1..d1218e4ccc85 100644\n--- a/src/include/duckdb/common/file_opener.hpp\n+++ b/src/include/duckdb/common/file_opener.hpp\n@@ -30,12 +30,13 @@ class FileOpener {\n \n \tvirtual SettingLookupResult TryGetCurrentSetting(const string &key, Value &result, FileOpenerInfo &info);\n \tvirtual SettingLookupResult TryGetCurrentSetting(const string &key, Value &result) = 0;\n-\tvirtual ClientContext *TryGetClientContext() = 0;\n+\tvirtual optional_ptr<ClientContext> TryGetClientContext() = 0;\n \n-\tDUCKDB_API static ClientContext *TryGetClientContext(FileOpener *opener);\n-\tDUCKDB_API static SettingLookupResult TryGetCurrentSetting(FileOpener *opener, const string &key, Value &result);\n-\tDUCKDB_API static SettingLookupResult TryGetCurrentSetting(FileOpener *opener, const string &key, Value &result,\n-\t                                                           FileOpenerInfo &info);\n+\tDUCKDB_API static optional_ptr<ClientContext> TryGetClientContext(optional_ptr<FileOpener> opener);\n+\tDUCKDB_API static SettingLookupResult TryGetCurrentSetting(optional_ptr<FileOpener> opener, const string &key,\n+\t                                                           Value &result);\n+\tDUCKDB_API static SettingLookupResult TryGetCurrentSetting(optional_ptr<FileOpener> opener, const string &key,\n+\t                                                           Value &result, FileOpenerInfo &info);\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/common/file_system.hpp b/src/include/duckdb/common/file_system.hpp\nindex 408a1acec232..8e7dfcd484fe 100644\n--- a/src/include/duckdb/common/file_system.hpp\n+++ b/src/include/duckdb/common/file_system.hpp\n@@ -16,6 +16,8 @@\n #include \"duckdb/common/vector.hpp\"\n #include \"duckdb/common/enums/file_glob_options.hpp\"\n #include \"duckdb/common/optional_ptr.hpp\"\n+#include \"duckdb/common/error_data.hpp\"\n+#include \"duckdb/common/file_open_flags.hpp\"\n #include <functional>\n \n #undef CreateDirectory\n@@ -86,7 +88,7 @@ struct FileHandle {\n \t}\n \ttemplate <class TARGET>\n \tconst TARGET &Cast() const {\n-\t\tD_ASSERT(dynamic_cast<const TARGET *>(this));\n+\t\tDynamicCastCheck<TARGET>(this);\n \t\treturn reinterpret_cast<const TARGET &>(*this);\n \t}\n \n@@ -95,41 +97,17 @@ struct FileHandle {\n \tstring path;\n };\n \n-enum class FileLockType : uint8_t { NO_LOCK = 0, READ_LOCK = 1, WRITE_LOCK = 2 };\n-\n-class FileFlags {\n-public:\n-\t//! Open file with read access\n-\tstatic constexpr uint8_t FILE_FLAGS_READ = 1 << 0;\n-\t//! Open file with write access\n-\tstatic constexpr uint8_t FILE_FLAGS_WRITE = 1 << 1;\n-\t//! Use direct IO when reading/writing to the file\n-\tstatic constexpr uint8_t FILE_FLAGS_DIRECT_IO = 1 << 2;\n-\t//! Create file if not exists, can only be used together with WRITE\n-\tstatic constexpr uint8_t FILE_FLAGS_FILE_CREATE = 1 << 3;\n-\t//! Always create a new file. If a file exists, the file is truncated. Cannot be used together with CREATE.\n-\tstatic constexpr uint8_t FILE_FLAGS_FILE_CREATE_NEW = 1 << 4;\n-\t//! Open file in append mode\n-\tstatic constexpr uint8_t FILE_FLAGS_APPEND = 1 << 5;\n-\t//! Open file with restrictive permissions (600 on linux/mac) can only be used when creating, throws if file exists\n-\tstatic constexpr uint8_t FILE_FLAGS_PRIVATE = 1 << 6;\n-};\n-\n class FileSystem {\n public:\n \tDUCKDB_API virtual ~FileSystem();\n \n public:\n-\tDUCKDB_API static constexpr FileLockType DEFAULT_LOCK = FileLockType::NO_LOCK;\n-\tDUCKDB_API static constexpr FileCompressionType DEFAULT_COMPRESSION = FileCompressionType::UNCOMPRESSED;\n \tDUCKDB_API static FileSystem &GetFileSystem(ClientContext &context);\n \tDUCKDB_API static FileSystem &GetFileSystem(DatabaseInstance &db);\n \tDUCKDB_API static FileSystem &Get(AttachedDatabase &db);\n \n-\tDUCKDB_API virtual unique_ptr<FileHandle> OpenFile(const string &path, uint8_t flags,\n-\t                                                   FileLockType lock = DEFAULT_LOCK,\n-\t                                                   FileCompressionType compression = DEFAULT_COMPRESSION,\n-\t                                                   FileOpener *opener = nullptr);\n+\tDUCKDB_API virtual unique_ptr<FileHandle> OpenFile(const string &path, FileOpenFlags flags,\n+\t                                                   optional_ptr<FileOpener> opener = nullptr);\n \n \t//! Read exactly nr_bytes from the specified location in the file. Fails if nr_bytes could not be read. This is\n \t//! equivalent to calling SetFilePointer(location) followed by calling Read().\n@@ -150,7 +128,7 @@ class FileSystem {\n \tDUCKDB_API virtual int64_t GetFileSize(FileHandle &handle);\n \t//! Returns the file last modified time of a file handle, returns timespec with zero on all attributes on error\n \tDUCKDB_API virtual time_t GetLastModifiedTime(FileHandle &handle);\n-\t//! Returns the file last modified time of a file handle, returns timespec with zero on all attributes on error\n+\t//! Returns the file type of the attached handle\n \tDUCKDB_API virtual FileType GetFileType(FileHandle &handle);\n \t//! Truncate a file to a maximum size of new_size, new_size should be smaller than or equal to the current size of\n \t//! the file\n@@ -162,6 +140,7 @@ class FileSystem {\n \tDUCKDB_API virtual void CreateDirectory(const string &directory);\n \t//! Recursively remove a directory and all files in it\n \tDUCKDB_API virtual void RemoveDirectory(const string &directory);\n+\n \t//! List files in a directory, invoking the callback method for each one with (filename, is_dir)\n \tDUCKDB_API virtual bool ListFiles(const string &directory,\n \t                                  const std::function<void(const string &, bool)> &callback,\n@@ -255,6 +234,19 @@ class FileSystem {\n \tDUCKDB_API static bool IsRemoteFile(const string &path);\n \n \tDUCKDB_API virtual void SetDisabledFileSystems(const vector<string> &names);\n+\n+public:\n+\ttemplate <class TARGET>\n+\tTARGET &Cast() {\n+\t\tDynamicCastCheck<TARGET>(this);\n+\t\treturn reinterpret_cast<TARGET &>(*this);\n+\t}\n+\n+\ttemplate <class TARGET>\n+\tconst TARGET &Cast() const {\n+\t\tDynamicCastCheck<TARGET>(this);\n+\t\treturn reinterpret_cast<const TARGET &>(*this);\n+\t}\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/common/http_state.hpp b/src/include/duckdb/common/http_state.hpp\nindex 721628692f68..1341b921147c 100644\n--- a/src/include/duckdb/common/http_state.hpp\n+++ b/src/include/duckdb/common/http_state.hpp\n@@ -84,7 +84,7 @@ class HTTPState : public ClientContextState {\n \tshared_ptr<CachedFile> &GetCachedFile(const string &path);\n \t//! Helper functions to get the HTTP state\n \tstatic shared_ptr<HTTPState> TryGetState(ClientContext &context, bool create_on_missing = true);\n-\tstatic shared_ptr<HTTPState> TryGetState(FileOpener *opener, bool create_on_missing = true);\n+\tstatic shared_ptr<HTTPState> TryGetState(optional_ptr<FileOpener> opener, bool create_on_missing = true);\n \n \tbool IsEmpty() {\n \t\treturn head_count == 0 && get_count == 0 && put_count == 0 && post_count == 0 && total_bytes_received == 0 &&\ndiff --git a/src/include/duckdb/common/local_file_system.hpp b/src/include/duckdb/common/local_file_system.hpp\nindex 547ae4aa9bdf..63ef80c4d1f9 100644\n--- a/src/include/duckdb/common/local_file_system.hpp\n+++ b/src/include/duckdb/common/local_file_system.hpp\n@@ -15,9 +15,8 @@ namespace duckdb {\n \n class LocalFileSystem : public FileSystem {\n public:\n-\tunique_ptr<FileHandle> OpenFile(const string &path, uint8_t flags, FileLockType lock = FileLockType::NO_LOCK,\n-\t                                FileCompressionType compression = FileCompressionType::UNCOMPRESSED,\n-\t                                FileOpener *opener = nullptr) override;\n+\tunique_ptr<FileHandle> OpenFile(const string &path, FileOpenFlags flags,\n+\t                                optional_ptr<FileOpener> opener = nullptr) override;\n \n \t//! Read exactly nr_bytes from the specified location in the file. Fails if nr_bytes could not be read. This is\n \t//! equivalent to calling SetFilePointer(location) followed by calling Read().\ndiff --git a/src/include/duckdb/common/opener_file_system.hpp b/src/include/duckdb/common/opener_file_system.hpp\nindex 28b352452ef8..d14af58edb8d 100644\n--- a/src/include/duckdb/common/opener_file_system.hpp\n+++ b/src/include/duckdb/common/opener_file_system.hpp\n@@ -18,13 +18,12 @@ class OpenerFileSystem : public FileSystem {\n \tvirtual FileSystem &GetFileSystem() const = 0;\n \tvirtual optional_ptr<FileOpener> GetOpener() const = 0;\n \n-\tunique_ptr<FileHandle> OpenFile(const string &path, uint8_t flags, FileLockType lock = FileLockType::NO_LOCK,\n-\t                                FileCompressionType compression = FileCompressionType::UNCOMPRESSED,\n-\t                                FileOpener *opener = nullptr) override {\n+\tunique_ptr<FileHandle> OpenFile(const string &path, FileOpenFlags flags,\n+\t                                optional_ptr<FileOpener> opener = nullptr) override {\n \t\tif (opener) {\n \t\t\tthrow InternalException(\"OpenerFileSystem cannot take an opener - the opener is pushed automatically\");\n \t\t}\n-\t\treturn GetFileSystem().OpenFile(path, flags, lock, compression, GetOpener().get());\n+\t\treturn GetFileSystem().OpenFile(path, flags, GetOpener().get());\n \t}\n \n \tvoid Read(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location) override {\ndiff --git a/src/include/duckdb/common/serializer/buffered_file_reader.hpp b/src/include/duckdb/common/serializer/buffered_file_reader.hpp\nindex 34d78e1608be..f39062fe6943 100644\n--- a/src/include/duckdb/common/serializer/buffered_file_reader.hpp\n+++ b/src/include/duckdb/common/serializer/buffered_file_reader.hpp\n@@ -17,6 +17,7 @@ class BufferedFileReader : public ReadStream {\n public:\n \tBufferedFileReader(FileSystem &fs, const char *path, FileLockType lock_type = FileLockType::READ_LOCK,\n \t                   optional_ptr<FileOpener> opener = nullptr);\n+\tBufferedFileReader(FileSystem &fs, unique_ptr<FileHandle> handle);\n \n \tFileSystem &fs;\n \tunsafe_unique_array<data_t> data;\n@@ -33,6 +34,8 @@ class BufferedFileReader : public ReadStream {\n \t\treturn file_size;\n \t}\n \n+\t//! Resets reading - beginning at position 0\n+\tvoid Reset();\n \tvoid Seek(uint64_t location);\n \tuint64_t CurrentOffset();\n \ndiff --git a/src/include/duckdb/common/serializer/buffered_file_writer.hpp b/src/include/duckdb/common/serializer/buffered_file_writer.hpp\nindex 8ad0ffdaa6f5..26f0460c4370 100644\n--- a/src/include/duckdb/common/serializer/buffered_file_writer.hpp\n+++ b/src/include/duckdb/common/serializer/buffered_file_writer.hpp\n@@ -17,11 +17,11 @@ namespace duckdb {\n \n class BufferedFileWriter : public WriteStream {\n public:\n-\tstatic constexpr uint8_t DEFAULT_OPEN_FLAGS = FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE;\n+\tstatic constexpr FileOpenFlags DEFAULT_OPEN_FLAGS = FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE;\n \n \t//! Serializes to a buffer allocated by the serializer, will expand when\n \t//! writing past the initial threshold\n-\tDUCKDB_API BufferedFileWriter(FileSystem &fs, const string &path, uint8_t open_flags = DEFAULT_OPEN_FLAGS);\n+\tDUCKDB_API BufferedFileWriter(FileSystem &fs, const string &path, FileOpenFlags open_flags = DEFAULT_OPEN_FLAGS);\n \n \tFileSystem &fs;\n \tstring path;\ndiff --git a/src/include/duckdb/common/virtual_file_system.hpp b/src/include/duckdb/common/virtual_file_system.hpp\nindex 69990bcbd1b9..9a0ce6d8c4e1 100644\n--- a/src/include/duckdb/common/virtual_file_system.hpp\n+++ b/src/include/duckdb/common/virtual_file_system.hpp\n@@ -19,9 +19,8 @@ class VirtualFileSystem : public FileSystem {\n public:\n \tVirtualFileSystem();\n \n-\tunique_ptr<FileHandle> OpenFile(const string &path, uint8_t flags, FileLockType lock = FileLockType::NO_LOCK,\n-\t                                FileCompressionType compression = FileCompressionType::UNCOMPRESSED,\n-\t                                FileOpener *opener = nullptr) override;\n+\tunique_ptr<FileHandle> OpenFile(const string &path, FileOpenFlags flags,\n+\t                                optional_ptr<FileOpener> opener = nullptr) override;\n \n \tvoid Read(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location) override;\n \tvoid Write(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location) override;\n@@ -52,7 +51,7 @@ class VirtualFileSystem : public FileSystem {\n \tbool FileExists(const string &filename) override;\n \n \tbool IsPipe(const string &filename) override;\n-\tvirtual void RemoveFile(const string &filename) override;\n+\tvoid RemoveFile(const string &filename) override;\n \n \tvirtual vector<string> Glob(const string &path, FileOpener *opener = nullptr) override;\n \ndiff --git a/src/include/duckdb/main/client_context_file_opener.hpp b/src/include/duckdb/main/client_context_file_opener.hpp\nindex 90a4201dc4c3..91780ae7da2e 100644\n--- a/src/include/duckdb/main/client_context_file_opener.hpp\n+++ b/src/include/duckdb/main/client_context_file_opener.hpp\n@@ -24,7 +24,7 @@ class ClientContextFileOpener : public FileOpener {\n \tSettingLookupResult TryGetCurrentSetting(const string &key, Value &result, FileOpenerInfo &info) override;\n \tSettingLookupResult TryGetCurrentSetting(const string &key, Value &result) override;\n \n-\tClientContext *TryGetClientContext() override {\n+\toptional_ptr<ClientContext> TryGetClientContext() override {\n \t\treturn &context;\n \t};\n \ndiff --git a/src/include/duckdb/storage/single_file_block_manager.hpp b/src/include/duckdb/storage/single_file_block_manager.hpp\nindex a1097f1831da..2c23e154527e 100644\n--- a/src/include/duckdb/storage/single_file_block_manager.hpp\n+++ b/src/include/duckdb/storage/single_file_block_manager.hpp\n@@ -36,7 +36,7 @@ class SingleFileBlockManager : public BlockManager {\n public:\n \tSingleFileBlockManager(AttachedDatabase &db, string path, StorageManagerOptions options);\n \n-\tvoid GetFileFlags(uint8_t &flags, FileLockType &lock, bool create_new);\n+\tFileOpenFlags GetFileFlags(bool create_new) const;\n \tvoid CreateNewDatabase();\n \tvoid LoadExistingDatabase();\n \ndiff --git a/src/include/duckdb/storage/write_ahead_log.hpp b/src/include/duckdb/storage/write_ahead_log.hpp\nindex 11a11c25e4ab..6398a20b7184 100644\n--- a/src/include/duckdb/storage/write_ahead_log.hpp\n+++ b/src/include/duckdb/storage/write_ahead_log.hpp\n@@ -52,7 +52,7 @@ class WriteAheadLog {\n \n public:\n \t//! Replay the WAL\n-\tstatic bool Replay(AttachedDatabase &database, string &path);\n+\tstatic bool Replay(AttachedDatabase &database, unique_ptr<FileHandle> handle);\n \n \t//! Returns the current size of the WAL in bytes\n \tint64_t GetWALSize();\ndiff --git a/src/main/client_context_file_opener.cpp b/src/main/client_context_file_opener.cpp\nindex a21e450513df..ee38b6a6cab1 100644\n--- a/src/main/client_context_file_opener.cpp\n+++ b/src/main/client_context_file_opener.cpp\n@@ -14,21 +14,22 @@ SettingLookupResult ClientContextFileOpener::TryGetCurrentSetting(const string &\n \treturn context.TryGetCurrentSetting(key, result);\n }\n \n-ClientContext *FileOpener::TryGetClientContext(FileOpener *opener) {\n+optional_ptr<ClientContext> FileOpener::TryGetClientContext(optional_ptr<FileOpener> opener) {\n \tif (!opener) {\n \t\treturn nullptr;\n \t}\n \treturn opener->TryGetClientContext();\n }\n \n-SettingLookupResult FileOpener::TryGetCurrentSetting(FileOpener *opener, const string &key, Value &result) {\n+SettingLookupResult FileOpener::TryGetCurrentSetting(optional_ptr<FileOpener> opener, const string &key,\n+                                                     Value &result) {\n \tif (!opener) {\n \t\treturn SettingLookupResult();\n \t}\n \treturn opener->TryGetCurrentSetting(key, result);\n }\n \n-SettingLookupResult FileOpener::TryGetCurrentSetting(FileOpener *opener, const string &key, Value &result,\n+SettingLookupResult FileOpener::TryGetCurrentSetting(optional_ptr<FileOpener> opener, const string &key, Value &result,\n                                                      FileOpenerInfo &info) {\n \tif (!opener) {\n \t\treturn SettingLookupResult();\ndiff --git a/src/main/config.cpp b/src/main/config.cpp\nindex 9901d5d89770..f1313d3b553a 100644\n--- a/src/main/config.cpp\n+++ b/src/main/config.cpp\n@@ -287,8 +287,7 @@ idx_t CGroupBandwidthQuota(idx_t physical_cores, FileSystem &fs) {\n \tif (fs.FileExists(CPU_MAX)) {\n \t\t// cgroup v2\n \t\t// https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html\n-\t\thandle =\n-\t\t    fs.OpenFile(CPU_MAX, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK, FileSystem::DEFAULT_COMPRESSION);\n+\t\thandle = fs.OpenFile(CPU_MAX, FileFlags::FILE_FLAGS_READ);\n \t\tread_bytes = fs.Read(*handle, (void *)byte_buffer, 999);\n \t\tbyte_buffer[read_bytes] = '\\0';\n \t\tif (std::sscanf(byte_buffer, \"%\" SCNd64 \" %\" SCNd64 \"\", &quota, &period) != 2) {\n@@ -299,8 +298,7 @@ idx_t CGroupBandwidthQuota(idx_t physical_cores, FileSystem &fs) {\n \t\t// https://www.kernel.org/doc/html/latest/scheduler/sched-bwc.html#management\n \n \t\t// Read the quota, this indicates how many microseconds the CPU can be utilized by this cgroup per period\n-\t\thandle = fs.OpenFile(CFS_QUOTA, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,\n-\t\t                     FileSystem::DEFAULT_COMPRESSION);\n+\t\thandle = fs.OpenFile(CFS_QUOTA, FileFlags::FILE_FLAGS_READ);\n \t\tread_bytes = fs.Read(*handle, (void *)byte_buffer, 999);\n \t\tbyte_buffer[read_bytes] = '\\0';\n \t\tif (std::sscanf(byte_buffer, \"%\" SCNd64 \"\", &quota) != 1) {\n@@ -308,8 +306,7 @@ idx_t CGroupBandwidthQuota(idx_t physical_cores, FileSystem &fs) {\n \t\t}\n \n \t\t// Read the time period, a cgroup can utilize the CPU up to quota microseconds every period\n-\t\thandle = fs.OpenFile(CFS_PERIOD, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,\n-\t\t                     FileSystem::DEFAULT_COMPRESSION);\n+\t\thandle = fs.OpenFile(CFS_PERIOD, FileFlags::FILE_FLAGS_READ);\n \t\tread_bytes = fs.Read(*handle, (void *)byte_buffer, 999);\n \t\tbyte_buffer[read_bytes] = '\\0';\n \t\tif (std::sscanf(byte_buffer, \"%\" SCNd64 \"\", &period) != 1) {\ndiff --git a/src/main/extension/extension_load.cpp b/src/main/extension/extension_load.cpp\nindex a001f2b9979e..a0c5beedda14 100644\n--- a/src/main/extension/extension_load.cpp\n+++ b/src/main/extension/extension_load.cpp\n@@ -226,10 +226,10 @@ bool ExtensionHelper::TryInitialLoad(DBConfig &config, FileSystem &fs, const str\n \t// Trim v's if necessary\n \tstd::string extension_version_trimmed = extension_version;\n \tstd::string engine_version_trimmed = engine_version;\n-\tif (extension_version.length() > 0 && extension_version[0] == 'v') {\n+\tif (!extension_version.empty() && extension_version[0] == 'v') {\n \t\textension_version_trimmed = extension_version.substr(1);\n \t}\n-\tif (engine_version.length() > 0 && engine_version[0] == 'v') {\n+\tif (!engine_version.empty() && engine_version[0] == 'v') {\n \t\tengine_version_trimmed = engine_version.substr(1);\n \t}\n \ndiff --git a/src/storage/magic_bytes.cpp b/src/storage/magic_bytes.cpp\nindex f004d7547cbb..524c84a0e0d8 100644\n--- a/src/storage/magic_bytes.cpp\n+++ b/src/storage/magic_bytes.cpp\n@@ -5,12 +5,15 @@\n namespace duckdb {\n \n DataFileType MagicBytes::CheckMagicBytes(FileSystem *fs_p, const string &path) {\n+\tif (path.empty() || path == IN_MEMORY_PATH) {\n+\t\treturn DataFileType::DUCKDB_FILE;\n+\t}\n \tLocalFileSystem lfs;\n \tFileSystem &fs = fs_p ? *fs_p : lfs;\n-\tif (!fs.FileExists(path)) {\n+\tauto handle = fs.OpenFile(path, FileFlags::FILE_FLAGS_READ | FileFlags::FILE_FLAGS_NULL_IF_NOT_EXISTS);\n+\tif (!handle) {\n \t\treturn DataFileType::FILE_DOES_NOT_EXIST;\n \t}\n-\tauto handle = fs.OpenFile(path, FileFlags::FILE_FLAGS_READ);\n \n \tconstexpr const idx_t MAGIC_BYTES_READ_SIZE = 16;\n \tchar buffer[MAGIC_BYTES_READ_SIZE];\ndiff --git a/src/storage/single_file_block_manager.cpp b/src/storage/single_file_block_manager.cpp\nindex 6bc857d26d7c..ee83ffd9a7a3 100644\n--- a/src/storage/single_file_block_manager.cpp\n+++ b/src/storage/single_file_block_manager.cpp\n@@ -150,31 +150,29 @@ SingleFileBlockManager::SingleFileBlockManager(AttachedDatabase &db, string path\n       iteration_count(0), options(options) {\n }\n \n-void SingleFileBlockManager::GetFileFlags(uint8_t &flags, FileLockType &lock, bool create_new) {\n+FileOpenFlags SingleFileBlockManager::GetFileFlags(bool create_new) const {\n+\tFileOpenFlags result;\n \tif (options.read_only) {\n \t\tD_ASSERT(!create_new);\n-\t\tflags = FileFlags::FILE_FLAGS_READ;\n-\t\tlock = FileLockType::READ_LOCK;\n+\t\tresult = FileFlags::FILE_FLAGS_READ | FileFlags::FILE_FLAGS_NULL_IF_NOT_EXISTS | FileLockType::READ_LOCK;\n \t} else {\n-\t\tflags = FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_READ;\n-\t\tlock = FileLockType::WRITE_LOCK;\n+\t\tresult = FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_READ | FileLockType::WRITE_LOCK;\n \t\tif (create_new) {\n-\t\t\tflags |= FileFlags::FILE_FLAGS_FILE_CREATE;\n+\t\t\tresult |= FileFlags::FILE_FLAGS_FILE_CREATE;\n \t\t}\n \t}\n \tif (options.use_direct_io) {\n-\t\tflags |= FileFlags::FILE_FLAGS_DIRECT_IO;\n+\t\tresult |= FileFlags::FILE_FLAGS_DIRECT_IO;\n \t}\n+\treturn result;\n }\n \n void SingleFileBlockManager::CreateNewDatabase() {\n-\tuint8_t flags;\n-\tFileLockType lock;\n-\tGetFileFlags(flags, lock, true);\n+\tauto flags = GetFileFlags(true);\n \n \t// open the RDBMS handle\n \tauto &fs = FileSystem::Get(db);\n-\thandle = fs.OpenFile(path, flags, lock);\n+\thandle = fs.OpenFile(path, flags);\n \n \t// if we create a new file, we fill the metadata of the file\n \t// first fill in the new header\n@@ -221,13 +219,15 @@ void SingleFileBlockManager::CreateNewDatabase() {\n }\n \n void SingleFileBlockManager::LoadExistingDatabase() {\n-\tuint8_t flags;\n-\tFileLockType lock;\n-\tGetFileFlags(flags, lock, false);\n+\tauto flags = GetFileFlags(false);\n \n \t// open the RDBMS handle\n \tauto &fs = FileSystem::Get(db);\n-\thandle = fs.OpenFile(path, flags, lock);\n+\thandle = fs.OpenFile(path, flags);\n+\tif (!handle) {\n+\t\t// this can only happen in read-only mode - as that is when we set FILE_FLAGS_NULL_IF_NOT_EXISTS\n+\t\tthrow CatalogException(\"Cannot open database \\\"%s\\\" in read-only mode: database does not exist\", path);\n+\t}\n \n \tMainHeader::CheckMagicBytes(*handle);\n \t// otherwise, we check the metadata of the file\ndiff --git a/src/storage/storage_manager.cpp b/src/storage/storage_manager.cpp\nindex 832924afe62f..87007d3acc90 100644\n--- a/src/storage/storage_manager.cpp\n+++ b/src/storage/storage_manager.cpp\n@@ -118,7 +118,6 @@ SingleFileStorageManager::SingleFileStorageManager(AttachedDatabase &db, string\n }\n \n void SingleFileStorageManager::LoadDatabase() {\n-\n \tif (InMemory()) {\n \t\tblock_manager = make_uniq<InMemoryBlockManager>(BufferManager::GetBufferManager(db));\n \t\ttable_io_manager = make_uniq<SingleFileTableIOManager>(*block_manager);\n@@ -139,12 +138,11 @@ void SingleFileStorageManager::LoadDatabase() {\n \toptions.debug_initialize = config.options.debug_initialize;\n \n \t// first check if the database exists\n-\tif (!fs.FileExists(path)) {\n-\t\tif (read_only) {\n-\t\t\tthrow CatalogException(\"Cannot open database \\\"%s\\\" in read-only mode: database does not exist\", path);\n-\t\t}\n+\tif (!read_only && !fs.FileExists(path)) {\n+\t\t// file does not exist and we are in read-write mode\n+\t\t// create a new file\n \n-\t\t// check if the WAL exists\n+\t\t// check if a WAL file already exists\n \t\tauto wal_path = GetWALPath();\n \t\tif (fs.FileExists(wal_path)) {\n \t\t\t// WAL file exists but database file does not\n@@ -157,8 +155,10 @@ void SingleFileStorageManager::LoadDatabase() {\n \t\tsf_block_manager->CreateNewDatabase();\n \t\tblock_manager = std::move(sf_block_manager);\n \t\ttable_io_manager = make_uniq<SingleFileTableIOManager>(*block_manager);\n-\n \t} else {\n+\t\t// either the file exists, or we are in read-only mode\n+\t\t// try to read the existing file on disk\n+\n \t\t// initialize the block manager while loading the current db file\n \t\tauto sf_block_manager = make_uniq<SingleFileBlockManager>(db, path, options);\n \t\tsf_block_manager->LoadExistingDatabase();\n@@ -171,9 +171,10 @@ void SingleFileStorageManager::LoadDatabase() {\n \n \t\t// check if the WAL file exists\n \t\tauto wal_path = GetWALPath();\n-\t\tif (fs.FileExists(wal_path)) {\n+\t\tauto handle = fs.OpenFile(wal_path, FileFlags::FILE_FLAGS_READ | FileFlags::FILE_FLAGS_NULL_IF_NOT_EXISTS);\n+\t\tif (handle) {\n \t\t\t// replay the WAL\n-\t\t\tif (WriteAheadLog::Replay(db, wal_path)) {\n+\t\t\tif (WriteAheadLog::Replay(db, std::move(handle))) {\n \t\t\t\tfs.RemoveFile(wal_path);\n \t\t\t}\n \t\t}\ndiff --git a/src/storage/temporary_file_manager.cpp b/src/storage/temporary_file_manager.cpp\nindex 34ec0ca5dce0..c374829037ec 100644\n--- a/src/storage/temporary_file_manager.cpp\n+++ b/src/storage/temporary_file_manager.cpp\n@@ -135,7 +135,7 @@ void TemporaryFileHandle::CreateFileIfNotExists(TemporaryFileLock &) {\n \t\treturn;\n \t}\n \tauto &fs = FileSystem::GetFileSystem(db);\n-\tuint8_t open_flags = FileFlags::FILE_FLAGS_READ | FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE;\n+\tauto open_flags = FileFlags::FILE_FLAGS_READ | FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE;\n \thandle = fs.OpenFile(path, open_flags);\n }\n \ndiff --git a/src/storage/wal_replay.cpp b/src/storage/wal_replay.cpp\nindex e66f606032da..9699bbac3e9d 100644\n--- a/src/storage/wal_replay.cpp\n+++ b/src/storage/wal_replay.cpp\n@@ -158,10 +158,10 @@ class WriteAheadLogDeserializer {\n //===--------------------------------------------------------------------===//\n // Replay\n //===--------------------------------------------------------------------===//\n-bool WriteAheadLog::Replay(AttachedDatabase &database, string &path) {\n+bool WriteAheadLog::Replay(AttachedDatabase &database, unique_ptr<FileHandle> handle) {\n \tConnection con(database.GetDatabase());\n-\tauto initial_source = make_uniq<BufferedFileReader>(FileSystem::Get(database), path.c_str());\n-\tif (initial_source->Finished()) {\n+\tBufferedFileReader reader(FileSystem::Get(database), std::move(handle));\n+\tif (reader.Finished()) {\n \t\t// WAL is empty\n \t\treturn false;\n \t}\n@@ -174,10 +174,10 @@ bool WriteAheadLog::Replay(AttachedDatabase &database, string &path) {\n \ttry {\n \t\twhile (true) {\n \t\t\t// read the current entry (deserialize only)\n-\t\t\tauto deserializer = WriteAheadLogDeserializer::Open(checkpoint_state, *initial_source, true);\n+\t\t\tauto deserializer = WriteAheadLogDeserializer::Open(checkpoint_state, reader, true);\n \t\t\tif (deserializer.ReplayEntry()) {\n \t\t\t\t// check if the file is exhausted\n-\t\t\t\tif (initial_source->Finished()) {\n+\t\t\t\tif (reader.Finished()) {\n \t\t\t\t\t// we finished reading the file: break\n \t\t\t\t\tbreak;\n \t\t\t\t}\n@@ -196,7 +196,6 @@ bool WriteAheadLog::Replay(AttachedDatabase &database, string &path) {\n \t\tPrinter::Print(\"Unknown Exception in WAL playback during initial read\");\n \t\treturn false;\n \t} // LCOV_EXCL_STOP\n-\tinitial_source.reset();\n \tif (checkpoint_state.checkpoint_id.IsValid()) {\n \t\t// there is a checkpoint flag: check if we need to deserialize the WAL\n \t\tauto &manager = database.GetStorageManager();\n@@ -208,9 +207,11 @@ bool WriteAheadLog::Replay(AttachedDatabase &database, string &path) {\n \t}\n \n \t// we need to recover from the WAL: actually set up the replay state\n-\tBufferedFileReader reader(FileSystem::Get(database), path.c_str());\n \tReplayState state(database, *con.context);\n \n+\t// reset the reader - we are going to read the WAL from the beginning again\n+\treader.Reset();\n+\n \t// replay the WAL\n \t// note that everything is wrapped inside a try/catch block here\n \t// there can be errors in WAL replay because of a corrupt WAL file\ndiff --git a/src/storage/write_ahead_log.cpp b/src/storage/write_ahead_log.cpp\nindex 5dac22c509c1..165ef10e88a8 100644\n--- a/src/storage/write_ahead_log.cpp\n+++ b/src/storage/write_ahead_log.cpp\n@@ -21,10 +21,9 @@ const uint64_t WAL_VERSION_NUMBER = 2;\n \n WriteAheadLog::WriteAheadLog(AttachedDatabase &database, const string &path) : skip_writing(false), database(database) {\n \twal_path = path;\n-\twriter = make_uniq<BufferedFileWriter>(FileSystem::Get(database), path.c_str(),\n-\t                                       NumericCast<uint8_t>(FileFlags::FILE_FLAGS_WRITE |\n-\t                                                            FileFlags::FILE_FLAGS_FILE_CREATE |\n-\t                                                            FileFlags::FILE_FLAGS_APPEND));\n+\twriter = make_uniq<BufferedFileWriter>(FileSystem::Get(database), path,\n+\t                                       FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE |\n+\t                                           FileFlags::FILE_FLAGS_APPEND);\n }\n \n WriteAheadLog::~WriteAheadLog() {\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pyfilesystem.hpp b/tools/pythonpkg/src/include/duckdb_python/pyfilesystem.hpp\nindex c6b96ca475bf..521843cc0602 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pyfilesystem.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pyfilesystem.hpp\n@@ -50,7 +50,7 @@ class PythonFilesystem : public FileSystem {\n private:\n \tconst vector<string> protocols;\n \tconst AbstractFileSystem filesystem;\n-\tstd::string DecodeFlags(uint8_t flags);\n+\tstd::string DecodeFlags(FileOpenFlags flags);\n \tbool Exists(const string &filename, const char *func_name) const;\n \n public:\n@@ -64,8 +64,7 @@ class PythonFilesystem : public FileSystem {\n \t}\n \n public:\n-\tunique_ptr<FileHandle> OpenFile(const string &path, uint8_t flags, FileLockType lock,\n-\t                                FileCompressionType compression, FileOpener *opener) override;\n+\tunique_ptr<FileHandle> OpenFile(const string &path, FileOpenFlags flags, optional_ptr<FileOpener> opener) override;\n \tvoid Seek(duckdb::FileHandle &handle, uint64_t location) override;\n \tFileType GetFileType(FileHandle &handle) override {\n \t\treturn FileType::FILE_TYPE_REGULAR;\ndiff --git a/tools/pythonpkg/src/pyfilesystem.cpp b/tools/pythonpkg/src/pyfilesystem.cpp\nindex 50fafd8aeccf..4d8cda96c660 100644\n--- a/tools/pythonpkg/src/pyfilesystem.cpp\n+++ b/tools/pythonpkg/src/pyfilesystem.cpp\n@@ -15,12 +15,12 @@ PythonFileHandle::~PythonFileHandle() {\n \thandle.release();\n }\n \n-string PythonFilesystem::DecodeFlags(uint8_t flags) {\n+string PythonFilesystem::DecodeFlags(FileOpenFlags flags) {\n \t// see https://stackoverflow.com/a/58925279 for truth table of python file modes\n-\tbool read = flags & FileFlags::FILE_FLAGS_READ;\n-\tbool write = flags & FileFlags::FILE_FLAGS_WRITE;\n-\tbool append = flags & FileFlags::FILE_FLAGS_APPEND;\n-\tbool truncate = flags & FileFlags::FILE_FLAGS_FILE_CREATE_NEW;\n+\tbool read = flags.OpenForReading();\n+\tbool write = flags.OpenForWriting();\n+\tbool append = flags.OpenForAppending();\n+\tbool truncate = flags.OverwriteExistingFile();\n \n \tstring flags_s;\n \tif (read && write && truncate) {\n@@ -44,13 +44,19 @@ string PythonFilesystem::DecodeFlags(uint8_t flags) {\n \treturn flags_s;\n }\n \n-unique_ptr<FileHandle> PythonFilesystem::OpenFile(const string &path, uint8_t flags, FileLockType lock,\n-                                                  FileCompressionType compression, FileOpener *opener) {\n+unique_ptr<FileHandle> PythonFilesystem::OpenFile(const string &path, FileOpenFlags flags,\n+                                                  optional_ptr<FileOpener> opener) {\n \tPythonGILWrapper gil;\n \n-\tif (compression != FileCompressionType::UNCOMPRESSED) {\n+\tif (flags.Compression() != FileCompressionType::UNCOMPRESSED) {\n \t\tthrow IOException(\"Compression not supported\");\n \t}\n+\t// maybe this can be implemented in a better way?\n+\tif (flags.ReturnNullIfNotExists()) {\n+\t\tif (!FileExists(path)) {\n+\t\t\treturn nullptr;\n+\t\t}\n+\t}\n \n \t// TODO: lock support?\n \n",
  "test_patch": "diff --git a/scripts/run_s3_test_server.sh b/scripts/run_s3_test_server.sh\nindex 11e155989267..30d83d64d0db 100755\n--- a/scripts/run_s3_test_server.sh\n+++ b/scripts/run_s3_test_server.sh\n@@ -1,23 +1,32 @@\n #!/usr/bin/env bash\n #Note: DONT run as root\n \n+set -e\n+\n+if [ ! -f data/attach_test/attach.db ]; then\n+    echo \"File data/attach_test/attach.db not found, run ./scripts/generate_presigned_url.sh to generate\"\n+    exit 1\n+fi\n+\n+rm -rf /tmp/minio_test_data\n+rm -rf /tmp/minio_root_data\n mkdir -p /tmp/minio_test_data\n mkdir -p /tmp/minio_root_data\n docker compose -f scripts/minio_s3.yml -p duckdb-minio up -d\n \n # for testing presigned url \n-sleep 3\n+sleep 10\n container_name=$(docker ps -a --format '{{.Names}}' | grep -m 1 \"duckdb-minio\")\n echo $container_name\n \n-export S3_SMALL_CSV_PRESIGNED_URL=$(docker logs $container_name | grep -m 1 'Share:.*phonenumbers\\.csv' | grep -o 'http[s]\\?://[^ ]\\+')\n+export S3_SMALL_CSV_PRESIGNED_URL=$(docker logs $container_name 2>/dev/null | grep -m 1 'Share:.*phonenumbers\\.csv' | grep -o 'http[s]\\?://[^ ]\\+')\n echo $S3_SMALL_CSV_PRESIGNED_URL\n \n-export S3_SMALL_PARQUET_PRESIGNED_URL=$(docker logs $container_name | grep -m 1 'Share:.*t1\\.parquet' | grep -o 'http[s]\\?://[^ ]\\+')\n+export S3_SMALL_PARQUET_PRESIGNED_URL=$(docker logs $container_name 2>/dev/null | grep -m 1 'Share:.*t1\\.parquet' | grep -o 'http[s]\\?://[^ ]\\+')\n echo $S3_SMALL_PARQUET_PRESIGNED_URL\n \n-export S3_LARGE_PARQUET_PRESIGNED_URL=$(docker logs $container_name | grep -m 1 'Share:.*lineitem_large\\.parquet' | grep -o 'http[s]\\?://[^ ]\\+')\n+export S3_LARGE_PARQUET_PRESIGNED_URL=$(docker logs $container_name 2>/dev/null | grep -m 1 'Share:.*lineitem_large\\.parquet' | grep -o 'http[s]\\?://[^ ]\\+')\n echo $S3_LARGE_PARQUET_PRESIGNED_URL\n \n-export S3_ATTACH_DB=$(docker logs $container_name | grep -m 1 'Share:.*attach\\.db' | grep -o 'http[s]\\?://[^ ]\\+')\n+export S3_ATTACH_DB=$(docker logs $container_name 2>/dev/null | grep -m 1 'Share:.*attach\\.db' | grep -o 'http[s]\\?://[^ ]\\+')\n echo $S3_ATTACH_DB\ndiff --git a/test/common/test_file_system.cpp b/test/common/test_file_system.cpp\nindex c473b4f9cecb..81db3f874630 100644\n--- a/test/common/test_file_system.cpp\n+++ b/test/common/test_file_system.cpp\n@@ -76,8 +76,7 @@ TEST_CASE(\"Test file operations\", \"[file_system]\") {\n \t// standard reading/writing test\n \n \t// open file for writing\n-\tREQUIRE_NOTHROW(handle = fs->OpenFile(fname, FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE,\n-\t                                      FileLockType::NO_LOCK));\n+\tREQUIRE_NOTHROW(handle = fs->OpenFile(fname, FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE));\n \t// write 10 integers\n \tREQUIRE_NOTHROW(handle->Write((void *)test_data, sizeof(int64_t) * INTEGER_COUNT, 0));\n \t// close the file\n@@ -87,7 +86,7 @@ TEST_CASE(\"Test file operations\", \"[file_system]\") {\n \t\ttest_data[i] = 0;\n \t}\n \t// now open the file for reading\n-\tREQUIRE_NOTHROW(handle = fs->OpenFile(fname, FileFlags::FILE_FLAGS_READ, FileLockType::NO_LOCK));\n+\tREQUIRE_NOTHROW(handle = fs->OpenFile(fname, FileFlags::FILE_FLAGS_READ));\n \t// read the 10 integers back\n \tREQUIRE_NOTHROW(handle->Read((void *)test_data, sizeof(int64_t) * INTEGER_COUNT, 0));\n \t// check the values of the integers\ndiff --git a/test/sql/copy/s3/README.md b/test/sql/copy/s3/README.md\nindex 2c4f07ad8f83..8f28a871ea60 100644\n--- a/test/sql/copy/s3/README.md\n+++ b/test/sql/copy/s3/README.md\n@@ -20,11 +20,17 @@ Run the `install_s3_test_server` script. This requires root. This makes a few ch\n sudo ./scripts/install_s3_test_server.sh\n ```\n \n+Then, if this has not been done yet, we need to generate some data:\n+\n+```\n+./scripts/generate_presigned_url.sh\n+```\n+\n Then run the test server in the back-ground using Docker. Note that Docker must be opened for this to work. On MacOS you can open the docker gui (`/Applications/Docker`) and leave it open to accomplish this.\n \n \n ```bash\n-./scripts/run_s3_test_server.sh\n+source ./scripts/run_s3_test_server.sh\n ```\n \n Now set up the following environment variables to enable running of the tests.\n@@ -51,3 +57,5 @@ Now you should be able to run the S3 tests using minio, e.g.:\n ```bash\n build/debug/test/unittest test/sql/copy/s3/s3_hive_partition.test\n ```\n+\n+> minio uses port 9000. Clickhouse also uses port 9000. If the tests are not working and you have a running Clickhouse service - try killing it first, e.g. using `killall -9 clickhouse`\n\\ No newline at end of file\ndiff --git a/test/sql/storage/concurrent_attach.test_slow b/test/sql/storage/concurrent_attach.test_slow\nnew file mode 100644\nindex 000000000000..24ae9babb2cb\n--- /dev/null\n+++ b/test/sql/storage/concurrent_attach.test_slow\n@@ -0,0 +1,34 @@\n+# name: test/sql/storage/concurrent_attach.test_slow\n+# description: Test concurrent attaching\n+# group: [storage]\n+\n+concurrentforeach name foo bar\n+\n+statement ok\n+attach '__TEST_DIR__/${name}.duckdb' AS ${name}\n+\n+statement ok\n+create table ${name}.${name}(${name} bigint)\n+\n+loop i 0 1000\n+\n+statement ok\n+insert into ${name}.${name} select sum(i) from range((random()*1000000.0)::INT) r(i)\n+\n+statement ok\n+checkpoint ${name}\n+\n+statement ok\n+detach ${name}\n+\n+statement ok\n+attach '__TEST_DIR__/${name}.duckdb' AS ${name}\n+\n+endloop\n+\n+query I\n+select count(*) FROM ${name}.${name}\n+----\n+1000\n+\n+endloop\ndiff --git a/test/sql/storage/wal_torn_write.cpp b/test/sql/storage/wal_torn_write.cpp\nindex 6026c9560dcd..803e4e77f8e8 100644\n--- a/test/sql/storage/wal_torn_write.cpp\n+++ b/test/sql/storage/wal_torn_write.cpp\n@@ -7,7 +7,7 @@ using namespace duckdb;\n using namespace std;\n \n static idx_t GetWALFileSize(FileSystem &fs, const string &path) {\n-\tauto handle = fs.OpenFile(path, FileFlags::FILE_FLAGS_READ, FileLockType::NO_LOCK);\n+\tauto handle = fs.OpenFile(path, FileFlags::FILE_FLAGS_READ);\n \treturn fs.GetFileSize(*handle);\n }\n \n",
  "problem_statement": "Attach fails with duckdb.IOException: IO Error: Cannot open file \"foo.duckdb.wal\": No such file or directory\n### What happens?\n\nSometimes, when doing attach and detach in parallel, the attach part fails with `duckdb.IOException: IO Error: Cannot open file \"foo.duckdb.wal\": No such file or directory`.\n\n### To Reproduce\n\nRun the following code snippet repeatedly. Since it does parallel checkpoints, it will usually deadlock (https://github.com/duckdb/duckdb/issues/9341), so you'll need to kill it in that case. Sometimes (about 1 in 10 tries), though, the attach would fail with duckdb.IOException: IO Error: Cannot open file \"foo.duckdb.wal\": No such file or directory. Couldn't really reproduce it in another way, sorry...\r\n\r\n```python\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nfrom pathlib import Path\r\nfrom random import random\r\n\r\nimport duckdb\r\n\r\n\r\ndef run_thread(cursor, name):\r\n    cursor.execute(f\"insert into {name}.{name} select sum(i) from range({int(random()*1000000)}) r(i)\")\r\n    cursor.execute(f\"checkpoint {name}\")\r\n\r\n    cursor.execute(f\"detach {name}\")\r\n    cursor.execute(f\"attach '{name}.duckdb' as {name}\")\r\n\r\n\r\nPath(\"foo.duckdb\").unlink(missing_ok=True)\r\nPath(\"bar.duckdb\").unlink(missing_ok=True)\r\nconn = duckdb.connect()\r\ncursor_foo = conn.cursor()\r\ncursor_bar = conn.cursor()\r\ncursor_foo.execute(\"attach 'foo.duckdb' as foo\")\r\ncursor_foo.execute(\"create table foo.foo(foo bigint)\")\r\ncursor_bar.execute(\"attach 'bar.duckdb' as bar\")\r\ncursor_bar.execute(\"create table bar.bar(bar bigint)\")\r\n\r\nfor i in range(1000):\r\n    print(i)\r\n    with ThreadPoolExecutor(max_workers=2) as executor:\r\n        footure = executor.submit(run_thread, cursor_foo, \"foo\")\r\n        barture = executor.submit(run_thread, cursor_bar, \"bar\")\r\n        footure.result()\r\n        barture.result()\r\n```\n\n### OS:\n\nUbuntu x64 in WSL on Windows 11\n\n### DuckDB Version:\n\n0.9.2.dev14+g0ef2a6faa2\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nM\u00edma Hlav\u00e1\u010dek\n\n### Affiliation:\n\nBlindspot.ai\n\n### Have you tried this on the latest `main` branch?\n\nI have tested with a main build\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] Yes, I have\nAttach fails with duckdb.IOException: IO Error: Cannot open file \"foo.duckdb.wal\": No such file or directory\n### What happens?\n\nSometimes, when doing attach and detach in parallel, the attach part fails with `duckdb.IOException: IO Error: Cannot open file \"foo.duckdb.wal\": No such file or directory`.\n\n### To Reproduce\n\nRun the following code snippet repeatedly. Since it does parallel checkpoints, it will usually deadlock (https://github.com/duckdb/duckdb/issues/9341), so you'll need to kill it in that case. Sometimes (about 1 in 10 tries), though, the attach would fail with duckdb.IOException: IO Error: Cannot open file \"foo.duckdb.wal\": No such file or directory. Couldn't really reproduce it in another way, sorry...\r\n\r\n```python\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nfrom pathlib import Path\r\nfrom random import random\r\n\r\nimport duckdb\r\n\r\n\r\ndef run_thread(cursor, name):\r\n    cursor.execute(f\"insert into {name}.{name} select sum(i) from range({int(random()*1000000)}) r(i)\")\r\n    cursor.execute(f\"checkpoint {name}\")\r\n\r\n    cursor.execute(f\"detach {name}\")\r\n    cursor.execute(f\"attach '{name}.duckdb' as {name}\")\r\n\r\n\r\nPath(\"foo.duckdb\").unlink(missing_ok=True)\r\nPath(\"bar.duckdb\").unlink(missing_ok=True)\r\nconn = duckdb.connect()\r\ncursor_foo = conn.cursor()\r\ncursor_bar = conn.cursor()\r\ncursor_foo.execute(\"attach 'foo.duckdb' as foo\")\r\ncursor_foo.execute(\"create table foo.foo(foo bigint)\")\r\ncursor_bar.execute(\"attach 'bar.duckdb' as bar\")\r\ncursor_bar.execute(\"create table bar.bar(bar bigint)\")\r\n\r\nfor i in range(1000):\r\n    print(i)\r\n    with ThreadPoolExecutor(max_workers=2) as executor:\r\n        footure = executor.submit(run_thread, cursor_foo, \"foo\")\r\n        barture = executor.submit(run_thread, cursor_bar, \"bar\")\r\n        footure.result()\r\n        barture.result()\r\n```\n\n### OS:\n\nUbuntu x64 in WSL on Windows 11\n\n### DuckDB Version:\n\n0.9.2.dev14+g0ef2a6faa2\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nM\u00edma Hlav\u00e1\u010dek\n\n### Affiliation:\n\nBlindspot.ai\n\n### Have you tried this on the latest `main` branch?\n\nI have tested with a main build\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] Yes, I have\n",
  "hints_text": "I don't think this is expected to work, checkpoint rewrites/updates the database on disk, attach looks up the database from disk\r\n\r\nThere is no abstraction on top of this, when the checkpoint is being written it is likely unavailable on the filesystem - causing attach to fail\r\n\r\nOr, because you run multiple checkpoints in parallel the WAL has been replayed and promptly deleted by one checkpoint - while another expects it to still exist\nIt's very surprising that this happens, though. There are two threads, and each interacts exclusively with one of the two attached databases. Does that mean that I can expend any transaction to appear in a WAL to any attached database, even if the transaction does not interact in said database in any way?\nHmm that's fair, I hadnt considered that\nThis issue is stale because it has been open 90 days with no activity. Remove stale label or comment or this will be closed in 30 days.\nThis issue was closed because it has been stale for 30 days with no activity.\nI don't think this is expected to work, checkpoint rewrites/updates the database on disk, attach looks up the database from disk\r\n\r\nThere is no abstraction on top of this, when the checkpoint is being written it is likely unavailable on the filesystem - causing attach to fail\r\n\r\nOr, because you run multiple checkpoints in parallel the WAL has been replayed and promptly deleted by one checkpoint - while another expects it to still exist\nIt's very surprising that this happens, though. There are two threads, and each interacts exclusively with one of the two attached databases. Does that mean that I can expend any transaction to appear in a WAL to any attached database, even if the transaction does not interact in said database in any way?\nHmm that's fair, I hadnt considered that\nThis issue is stale because it has been open 90 days with no activity. Remove stale label or comment or this will be closed in 30 days.\nThis issue was closed because it has been stale for 30 days with no activity.",
  "created_at": "2024-03-21T20:09:29Z"
}