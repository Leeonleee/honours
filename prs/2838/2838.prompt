You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Improper casting from parquet map to duckdb map via parquet_scan
It's unclear to me if this has not been implemented or not, so apologies if this is a known issue:

I am writing a map column in my parquet files and trying to read them from duckdb using the python library.
When I query the files using pyarrow, the data looks correct and it sees it as a map:
```
dimensionMap: map<string, string> not null
  child 0, entries: struct<key: string not null, value: string not null> not null
      child 0, key: string not null
      child 1, value: string not null
```

However I cannot seem to actually extract data from the map easily:
When I am unable to use any of the built in map functions, as it seems the types aren't quite working as expected:
if trying to use the bracket notation `map['foo']` I get an error:
```
RuntimeError: Binder Error: No function matches the given name and argument types 'array_extract(LIST<STRUCT<key: VARCHAR, value: VARCHAR>>, VARCHAR)'. You might need to add explicit type casts.
```
and if trying to use `map_extract` directly I get an error :
```
RuntimeError: Binder Error: MAP_EXTRACT can only operate on MAPs
```

It seems some of the types are correct, but the queries don't seem to actually run as expected, the output of parquet_schema(...) for the relevant columns is given below:
![Screen Shot 2021-12-07 at 12 41 53 PM](https://user-images.githubusercontent.com/7540465/145103733-e15caaa9-0b05-48b9-9b17-c3afd1e770ad.png)

#### What happens?
Parquet map types of nested structs cannot be read by duckdb. 

#### To Reproduce
Steps to reproduce the behavior. Bonus points if those are only SQL queries.
I had to create an example file using pyarrow (I was unable to actually insert data using pyarrow since my project writes the data from the jvm).

```
import pyarrow as pa
import pyarrow.parquet as pq
import duckdb 
maptype = pa.map_(pa.string(), pa.string())
t = pa.Table.from_pydict({'my_map':[]}, pa.schema([pa.field('my_map',maptype)])) 
pq.write_table(t, 'test')
cursor = duckdb.connect()
cursor.execute("SELECT my_map['k'] FROM parquet_scan('test')").fetchall()
cursor.execute("SELECT map_extract(my_map,'k') FROM parquet_scan('test')").fetchall()
```

#### Environment (please complete the following information):
 - OS: iOS
 - DuckDB Version: 0.3.1
 - DuckDB Client: python

#### Before Submitting

- [X] **Have you tried this on the latest `master` branch?**
* **Python**: `pip install duckdb --upgrade --pre`
* **R**: `install.packages("https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz", repos = NULL)`
* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.

- [X] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**


</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/docs/why_duckdb.html).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
44: 
45: 
[end of README.md]
[start of extension/parquet/column_reader.cpp]
1: #include "column_reader.hpp"
2: #include "parquet_timestamp.hpp"
3: #include "utf8proc_wrapper.hpp"
4: #include "parquet_reader.hpp"
5: 
6: #include "boolean_column_reader.hpp"
7: #include "callback_column_reader.hpp"
8: #include "decimal_column_reader.hpp"
9: #include "list_column_reader.hpp"
10: #include "string_column_reader.hpp"
11: #include "struct_column_reader.hpp"
12: #include "templated_column_reader.hpp"
13: 
14: #include "snappy.h"
15: #include "miniz_wrapper.hpp"
16: #include "zstd.h"
17: #include <iostream>
18: 
19: #include "duckdb.hpp"
20: #ifndef DUCKDB_AMALGAMATION
21: #include "duckdb/common/types/blob.hpp"
22: #include "duckdb/common/types/chunk_collection.hpp"
23: #endif
24: 
25: namespace duckdb {
26: 
27: using duckdb_parquet::format::CompressionCodec;
28: using duckdb_parquet::format::ConvertedType;
29: using duckdb_parquet::format::Encoding;
30: using duckdb_parquet::format::PageType;
31: using duckdb_parquet::format::Type;
32: 
33: const uint32_t RleBpDecoder::BITPACK_MASKS[] = {
34:     0,       1,       3,        7,        15,       31,        63,        127,       255,        511,       1023,
35:     2047,    4095,    8191,     16383,    32767,    65535,     131071,    262143,    524287,     1048575,   2097151,
36:     4194303, 8388607, 16777215, 33554431, 67108863, 134217727, 268435455, 536870911, 1073741823, 2147483647};
37: 
38: const uint8_t RleBpDecoder::BITPACK_DLEN = 8;
39: 
40: ColumnReader::ColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
41:                            idx_t max_define_p, idx_t max_repeat_p)
42:     : schema(schema_p), file_idx(file_idx_p), max_define(max_define_p), max_repeat(max_repeat_p), reader(reader),
43:       type(move(type_p)), page_rows_available(0) {
44: 
45: 	// dummies for Skip()
46: 	none_filter.none();
47: 	dummy_define.resize(reader.allocator, STANDARD_VECTOR_SIZE);
48: 	dummy_repeat.resize(reader.allocator, STANDARD_VECTOR_SIZE);
49: }
50: 
51: ColumnReader::~ColumnReader() {
52: }
53: 
54: template <class T>
55: unique_ptr<ColumnReader> CreateDecimalReader(ParquetReader &reader, const LogicalType &type_p,
56:                                              const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,
57:                                              idx_t max_repeat) {
58: 	switch (type_p.InternalType()) {
59: 	case PhysicalType::INT16:
60: 		return make_unique<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<T>>>(
61: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
62: 	case PhysicalType::INT32:
63: 		return make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<T>>>(
64: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
65: 	case PhysicalType::INT64:
66: 		return make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<T>>>(
67: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
68: 	default:
69: 		throw NotImplementedException("Unimplemented internal type for CreateDecimalReader");
70: 	}
71: }
72: 
73: unique_ptr<ColumnReader> ColumnReader::CreateReader(ParquetReader &reader, const LogicalType &type_p,
74:                                                     const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,
75:                                                     idx_t max_repeat) {
76: 	switch (type_p.id()) {
77: 	case LogicalTypeId::BOOLEAN:
78: 		return make_unique<BooleanColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
79: 	case LogicalTypeId::UTINYINT:
80: 		return make_unique<TemplatedColumnReader<uint8_t, TemplatedParquetValueConversion<uint32_t>>>(
81: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
82: 	case LogicalTypeId::USMALLINT:
83: 		return make_unique<TemplatedColumnReader<uint16_t, TemplatedParquetValueConversion<uint32_t>>>(
84: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
85: 	case LogicalTypeId::UINTEGER:
86: 		return make_unique<TemplatedColumnReader<uint32_t, TemplatedParquetValueConversion<uint32_t>>>(
87: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
88: 	case LogicalTypeId::UBIGINT:
89: 		return make_unique<TemplatedColumnReader<uint64_t, TemplatedParquetValueConversion<uint64_t>>>(
90: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
91: 	case LogicalTypeId::TINYINT:
92: 		return make_unique<TemplatedColumnReader<int8_t, TemplatedParquetValueConversion<int32_t>>>(
93: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
94: 	case LogicalTypeId::SMALLINT:
95: 		return make_unique<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<int32_t>>>(
96: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
97: 	case LogicalTypeId::INTEGER:
98: 		return make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<int32_t>>>(
99: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
100: 	case LogicalTypeId::BIGINT:
101: 		return make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<int64_t>>>(
102: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
103: 	case LogicalTypeId::FLOAT:
104: 		return make_unique<TemplatedColumnReader<float, TemplatedParquetValueConversion<float>>>(
105: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
106: 	case LogicalTypeId::DOUBLE:
107: 		return make_unique<TemplatedColumnReader<double, TemplatedParquetValueConversion<double>>>(
108: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
109: 	case LogicalTypeId::TIMESTAMP:
110: 		switch (schema_p.type) {
111: 		case Type::INT96:
112: 			return make_unique<CallbackColumnReader<Int96, timestamp_t, ImpalaTimestampToTimestamp>>(
113: 			    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
114: 		case Type::INT64:
115: 			switch (schema_p.converted_type) {
116: 			case ConvertedType::TIMESTAMP_MICROS:
117: 				return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMicrosToTimestamp>>(
118: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
119: 			case ConvertedType::TIMESTAMP_MILLIS:
120: 				return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMsToTimestamp>>(
121: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
122: 			default:
123: 				break;
124: 			}
125: 		default:
126: 			break;
127: 		}
128: 		break;
129: 	case LogicalTypeId::DATE:
130: 		return make_unique<CallbackColumnReader<int32_t, date_t, ParquetIntToDate>>(reader, type_p, schema_p,
131: 		                                                                            file_idx_p, max_define, max_repeat);
132: 	case LogicalTypeId::BLOB:
133: 	case LogicalTypeId::VARCHAR:
134: 		return make_unique<StringColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
135: 	case LogicalTypeId::DECIMAL:
136: 		// we have to figure out what kind of int we need
137: 		switch (schema_p.type) {
138: 		case Type::INT32:
139: 			return CreateDecimalReader<int32_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
140: 		case Type::INT64:
141: 			return CreateDecimalReader<int64_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
142: 		case Type::FIXED_LEN_BYTE_ARRAY:
143: 			switch (type_p.InternalType()) {
144: 			case PhysicalType::INT16:
145: 				return make_unique<DecimalColumnReader<int16_t>>(reader, type_p, schema_p, file_idx_p, max_define,
146: 				                                                 max_repeat);
147: 			case PhysicalType::INT32:
148: 				return make_unique<DecimalColumnReader<int32_t>>(reader, type_p, schema_p, file_idx_p, max_define,
149: 				                                                 max_repeat);
150: 			case PhysicalType::INT64:
151: 				return make_unique<DecimalColumnReader<int64_t>>(reader, type_p, schema_p, file_idx_p, max_define,
152: 				                                                 max_repeat);
153: 			case PhysicalType::INT128:
154: 				return make_unique<DecimalColumnReader<hugeint_t>>(reader, type_p, schema_p, file_idx_p, max_define,
155: 				                                                   max_repeat);
156: 			default:
157: 				throw InternalException("Unrecognized type for Decimal");
158: 			}
159: 		default:
160: 			throw NotImplementedException("Unrecognized type for Decimal");
161: 		}
162: 		break;
163: 	default:
164: 		break;
165: 	}
166: 	throw NotImplementedException(type_p.ToString());
167: }
168: 
169: void ColumnReader::PrepareRead(parquet_filter_t &filter) {
170: 	dict_decoder.reset();
171: 	defined_decoder.reset();
172: 	block.reset();
173: 
174: 	PageHeader page_hdr;
175: 	page_hdr.read(protocol);
176: 
177: 	//	page_hdr.printTo(std::cout);
178: 	//	std::cout << '\n';
179: 
180: 	PreparePage(page_hdr.compressed_page_size, page_hdr.uncompressed_page_size);
181: 
182: 	switch (page_hdr.type) {
183: 	case PageType::DATA_PAGE_V2:
184: 	case PageType::DATA_PAGE:
185: 		PrepareDataPage(page_hdr);
186: 		break;
187: 	case PageType::DICTIONARY_PAGE:
188: 		Dictionary(move(block), page_hdr.dictionary_page_header.num_values);
189: 		break;
190: 	default:
191: 		break; // ignore INDEX page type and any other custom extensions
192: 	}
193: }
194: 
195: void ColumnReader::PreparePage(idx_t compressed_page_size, idx_t uncompressed_page_size) {
196: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
197: 
198: 	block = make_shared<ResizeableBuffer>(reader.allocator, compressed_page_size + 1);
199: 	trans.read((uint8_t *)block->ptr, compressed_page_size);
200: 
201: 	shared_ptr<ResizeableBuffer> unpacked_block;
202: 	if (chunk->meta_data.codec != CompressionCodec::UNCOMPRESSED) {
203: 		unpacked_block = make_shared<ResizeableBuffer>(reader.allocator, uncompressed_page_size + 1);
204: 	}
205: 
206: 	switch (chunk->meta_data.codec) {
207: 	case CompressionCodec::UNCOMPRESSED:
208: 		break;
209: 	case CompressionCodec::GZIP: {
210: 		MiniZStream s;
211: 
212: 		s.Decompress((const char *)block->ptr, compressed_page_size, (char *)unpacked_block->ptr,
213: 		             uncompressed_page_size);
214: 		block = move(unpacked_block);
215: 
216: 		break;
217: 	}
218: 	case CompressionCodec::SNAPPY: {
219: 		auto res =
220: 		    duckdb_snappy::RawUncompress((const char *)block->ptr, compressed_page_size, (char *)unpacked_block->ptr);
221: 		if (!res) {
222: 			throw std::runtime_error("Decompression failure");
223: 		}
224: 		block = move(unpacked_block);
225: 		break;
226: 	}
227: 	case CompressionCodec::ZSTD: {
228: 		auto res = duckdb_zstd::ZSTD_decompress((char *)unpacked_block->ptr, uncompressed_page_size,
229: 		                                        (const char *)block->ptr, compressed_page_size);
230: 		if (duckdb_zstd::ZSTD_isError(res) || res != (size_t)uncompressed_page_size) {
231: 			throw std::runtime_error("ZSTD Decompression failure");
232: 		}
233: 		block = move(unpacked_block);
234: 		break;
235: 	}
236: 
237: 	default: {
238: 		std::stringstream codec_name;
239: 		codec_name << chunk->meta_data.codec;
240: 		throw std::runtime_error("Unsupported compression codec \"" + codec_name.str() +
241: 		                         "\". Supported options are uncompressed, gzip or snappy");
242: 		break;
243: 	}
244: 	}
245: }
246: 
247: void ColumnReader::PrepareDataPage(PageHeader &page_hdr) {
248: 	if (page_hdr.type == PageType::DATA_PAGE && !page_hdr.__isset.data_page_header) {
249: 		throw std::runtime_error("Missing data page header from data page");
250: 	}
251: 	if (page_hdr.type == PageType::DATA_PAGE_V2 && !page_hdr.__isset.data_page_header_v2) {
252: 		throw std::runtime_error("Missing data page header from data page v2");
253: 	}
254: 
255: 	page_rows_available = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.num_values
256: 	                                                           : page_hdr.data_page_header_v2.num_values;
257: 	auto page_encoding = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.encoding
258: 	                                                          : page_hdr.data_page_header_v2.encoding;
259: 
260: 	if (HasRepeats()) {
261: 		uint32_t rep_length = page_hdr.type == PageType::DATA_PAGE
262: 		                          ? block->read<uint32_t>()
263: 		                          : page_hdr.data_page_header_v2.repetition_levels_byte_length;
264: 		block->available(rep_length);
265: 		repeated_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, rep_length,
266: 		                                             RleBpDecoder::ComputeBitWidth(max_repeat));
267: 		block->inc(rep_length);
268: 	}
269: 
270: 	if (HasDefines()) {
271: 		uint32_t def_length = page_hdr.type == PageType::DATA_PAGE
272: 		                          ? block->read<uint32_t>()
273: 		                          : page_hdr.data_page_header_v2.definition_levels_byte_length;
274: 		block->available(def_length);
275: 		defined_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, def_length,
276: 		                                            RleBpDecoder::ComputeBitWidth(max_define));
277: 		block->inc(def_length);
278: 	}
279: 
280: 	switch (page_encoding) {
281: 	case Encoding::RLE_DICTIONARY:
282: 	case Encoding::PLAIN_DICTIONARY: {
283: 		// TODO there seems to be some confusion whether this is in the bytes for v2
284: 		// where is it otherwise??
285: 		auto dict_width = block->read<uint8_t>();
286: 		// TODO somehow dict_width can be 0 ?
287: 		dict_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, block->len, dict_width);
288: 		block->inc(block->len);
289: 		break;
290: 	}
291: 	case Encoding::PLAIN:
292: 		// nothing to do here, will be read directly below
293: 		break;
294: 
295: 	default:
296: 		throw std::runtime_error("Unsupported page encoding");
297: 	}
298: }
299: 
300: idx_t ColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
301:                          Vector &result) {
302: 	// we need to reset the location because multiple column readers share the same protocol
303: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
304: 	trans.SetLocation(chunk_read_offset);
305: 
306: 	idx_t result_offset = 0;
307: 	auto to_read = num_values;
308: 
309: 	while (to_read > 0) {
310: 		while (page_rows_available == 0) {
311: 			PrepareRead(filter);
312: 		}
313: 
314: 		D_ASSERT(block);
315: 		auto read_now = MinValue<idx_t>(to_read, page_rows_available);
316: 
317: 		D_ASSERT(read_now <= STANDARD_VECTOR_SIZE);
318: 
319: 		if (HasRepeats()) {
320: 			D_ASSERT(repeated_decoder);
321: 			repeated_decoder->GetBatch<uint8_t>((char *)repeat_out + result_offset, read_now);
322: 		}
323: 
324: 		if (HasDefines()) {
325: 			D_ASSERT(defined_decoder);
326: 			defined_decoder->GetBatch<uint8_t>((char *)define_out + result_offset, read_now);
327: 		}
328: 
329: 		if (dict_decoder) {
330: 			// we need the null count because the offsets and plain values have no entries for nulls
331: 			idx_t null_count = 0;
332: 			if (HasDefines()) {
333: 				for (idx_t i = 0; i < read_now; i++) {
334: 					if (define_out[i + result_offset] != max_define) {
335: 						null_count++;
336: 					}
337: 				}
338: 			}
339: 
340: 			offset_buffer.resize(reader.allocator, sizeof(uint32_t) * (read_now - null_count));
341: 			dict_decoder->GetBatch<uint32_t>(offset_buffer.ptr, read_now - null_count);
342: 			DictReference(result);
343: 			Offsets((uint32_t *)offset_buffer.ptr, define_out, read_now, filter, result_offset, result);
344: 		} else {
345: 			PlainReference(block, result);
346: 			Plain(block, define_out, read_now, filter, result_offset, result);
347: 		}
348: 
349: 		result_offset += read_now;
350: 		page_rows_available -= read_now;
351: 		to_read -= read_now;
352: 	}
353: 	group_rows_available -= num_values;
354: 	chunk_read_offset = trans.GetLocation();
355: 
356: 	return num_values;
357: }
358: 
359: void ColumnReader::Skip(idx_t num_values) {
360: 	dummy_define.zero();
361: 	dummy_repeat.zero();
362: 
363: 	// TODO this can be optimized, for example we dont actually have to bitunpack offsets
364: 	Vector dummy_result(type, nullptr);
365: 	auto values_read =
366: 	    Read(num_values, none_filter, (uint8_t *)dummy_define.ptr, (uint8_t *)dummy_repeat.ptr, dummy_result);
367: 	if (values_read != num_values) {
368: 		throw std::runtime_error("Row count mismatch when skipping rows");
369: 	}
370: }
371: 
372: uint32_t StringColumnReader::VerifyString(const char *str_data, uint32_t str_len) {
373: 	if (Type() != LogicalTypeId::VARCHAR) {
374: 		return str_len;
375: 	}
376: 	// verify if a string is actually UTF8, and if there are no null bytes in the middle of the string
377: 	// technically Parquet should guarantee this, but reality is often disappointing
378: 	UnicodeInvalidReason reason;
379: 	size_t pos;
380: 	auto utf_type = Utf8Proc::Analyze(str_data, str_len, &reason, &pos);
381: 	if (utf_type == UnicodeType::INVALID) {
382: 		if (reason == UnicodeInvalidReason::NULL_BYTE) {
383: 			// for null bytes we just truncate the string
384: 			return pos;
385: 		}
386: 		throw InvalidInputException("Invalid string encoding found in Parquet file: value \"" +
387: 		                            Blob::ToString(string_t(str_data, str_len)) + "\" is not valid UTF8!");
388: 	}
389: 	return str_len;
390: }
391: 
392: void StringColumnReader::Dictionary(shared_ptr<ByteBuffer> data, idx_t num_entries) {
393: 	dict = move(data);
394: 	dict_strings = unique_ptr<string_t[]>(new string_t[num_entries]);
395: 	for (idx_t dict_idx = 0; dict_idx < num_entries; dict_idx++) {
396: 		uint32_t str_len = dict->read<uint32_t>();
397: 		dict->available(str_len);
398: 
399: 		auto actual_str_len = VerifyString(dict->ptr, str_len);
400: 		dict_strings[dict_idx] = string_t(dict->ptr, actual_str_len);
401: 		dict->inc(str_len);
402: 	}
403: }
404: 
405: class ParquetStringVectorBuffer : public VectorBuffer {
406: public:
407: 	explicit ParquetStringVectorBuffer(shared_ptr<ByteBuffer> buffer_p)
408: 	    : VectorBuffer(VectorBufferType::OPAQUE_BUFFER), buffer(move(buffer_p)) {
409: 	}
410: 
411: private:
412: 	shared_ptr<ByteBuffer> buffer;
413: };
414: 
415: void StringColumnReader::DictReference(Vector &result) {
416: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(dict));
417: }
418: void StringColumnReader::PlainReference(shared_ptr<ByteBuffer> plain_data, Vector &result) {
419: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(move(plain_data)));
420: }
421: 
422: string_t StringParquetValueConversion::DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
423: 	auto &dict_strings = ((StringColumnReader &)reader).dict_strings;
424: 	return dict_strings[offset];
425: }
426: 
427: string_t StringParquetValueConversion::PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
428: 	auto &scr = ((StringColumnReader &)reader);
429: 	uint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;
430: 	plain_data.available(str_len);
431: 	auto actual_str_len = ((StringColumnReader &)reader).VerifyString(plain_data.ptr, str_len);
432: 	auto ret_str = string_t(plain_data.ptr, actual_str_len);
433: 	plain_data.inc(str_len);
434: 	return ret_str;
435: }
436: 
437: void StringParquetValueConversion::PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
438: 	auto &scr = ((StringColumnReader &)reader);
439: 	uint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;
440: 	plain_data.available(str_len);
441: 	plain_data.inc(str_len);
442: }
443: 
444: //===--------------------------------------------------------------------===//
445: // List Column Reader
446: //===--------------------------------------------------------------------===//
447: idx_t ListColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
448:                              Vector &result_out) {
449: 	idx_t result_offset = 0;
450: 	auto result_ptr = FlatVector::GetData<list_entry_t>(result_out);
451: 	auto &result_mask = FlatVector::Validity(result_out);
452: 
453: 	D_ASSERT(ListVector::GetListSize(result_out) == 0);
454: 	// if an individual list is longer than STANDARD_VECTOR_SIZE we actually have to loop the child read to fill it
455: 	bool finished = false;
456: 	while (!finished) {
457: 		idx_t child_actual_num_values = 0;
458: 
459: 		// check if we have any overflow from a previous read
460: 		if (overflow_child_count == 0) {
461: 			// we don't: read elements from the child reader
462: 			child_defines.zero();
463: 			child_repeats.zero();
464: 			// we don't know in advance how many values to read because of the beautiful repetition/definition setup
465: 			// we just read (up to) a vector from the child column, and see if we have read enough
466: 			// if we have not read enough, we read another vector
467: 			// if we have read enough, we leave any unhandled elements in the overflow vector for a subsequent read
468: 			auto child_req_num_values =
469: 			    MinValue<idx_t>(STANDARD_VECTOR_SIZE, child_column_reader->GroupRowsAvailable());
470: 			read_vector.ResetFromCache(read_cache);
471: 			child_actual_num_values = child_column_reader->Read(child_req_num_values, child_filter, child_defines_ptr,
472: 			                                                    child_repeats_ptr, read_vector);
473: 		} else {
474: 			// we do: use the overflow values
475: 			child_actual_num_values = overflow_child_count;
476: 			overflow_child_count = 0;
477: 		}
478: 
479: 		if (child_actual_num_values == 0) {
480: 			// no more elements available: we are done
481: 			break;
482: 		}
483: 		read_vector.Verify(child_actual_num_values);
484: 		idx_t current_chunk_offset = ListVector::GetListSize(result_out);
485: 
486: 		// hard-won piece of code this, modify at your own risk
487: 		// the intuition is that we have to only collapse values into lists that are repeated *on this level*
488: 		// the rest is pretty much handed up as-is as a single-valued list or NULL
489: 		idx_t child_idx;
490: 		for (child_idx = 0; child_idx < child_actual_num_values; child_idx++) {
491: 			if (child_repeats_ptr[child_idx] == max_repeat) {
492: 				// value repeats on this level, append
493: 				D_ASSERT(result_offset > 0);
494: 				result_ptr[result_offset - 1].length++;
495: 				continue;
496: 			}
497: 
498: 			if (result_offset >= num_values) {
499: 				// we ran out of output space
500: 				finished = true;
501: 				break;
502: 			}
503: 			if (child_defines_ptr[child_idx] >= max_define) {
504: 				// value has been defined down the stack, hence its NOT NULL
505: 				result_ptr[result_offset].offset = child_idx + current_chunk_offset;
506: 				result_ptr[result_offset].length = 1;
507: 			} else if (child_defines_ptr[child_idx] == max_define - 1) {
508: 				// empty list
509: 				result_ptr[result_offset].offset = child_idx + current_chunk_offset;
510: 				result_ptr[result_offset].length = 0;
511: 			} else {
512: 				// value is NULL somewhere up the stack
513: 				result_mask.SetInvalid(result_offset);
514: 				result_ptr[result_offset].offset = 0;
515: 				result_ptr[result_offset].length = 0;
516: 			}
517: 
518: 			repeat_out[result_offset] = child_repeats_ptr[child_idx];
519: 			define_out[result_offset] = child_defines_ptr[child_idx];
520: 
521: 			result_offset++;
522: 		}
523: 		// actually append the required elements to the child list
524: 		ListVector::Append(result_out, read_vector, child_idx);
525: 
526: 		// we have read more values from the child reader than we can fit into the result for this read
527: 		// we have to pass everything from child_idx to child_actual_num_values into the next call
528: 		if (child_idx < child_actual_num_values && result_offset == num_values) {
529: 			read_vector.Slice(read_vector, child_idx);
530: 			overflow_child_count = child_actual_num_values - child_idx;
531: 			read_vector.Verify(overflow_child_count);
532: 
533: 			// move values in the child repeats and defines *backward* by child_idx
534: 			for (idx_t repdef_idx = 0; repdef_idx < overflow_child_count; repdef_idx++) {
535: 				child_defines_ptr[repdef_idx] = child_defines_ptr[child_idx + repdef_idx];
536: 				child_repeats_ptr[repdef_idx] = child_repeats_ptr[child_idx + repdef_idx];
537: 			}
538: 		}
539: 	}
540: 	result_out.Verify(result_offset);
541: 	return result_offset;
542: }
543: 
544: ListColumnReader::ListColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
545:                                    idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p,
546:                                    unique_ptr<ColumnReader> child_column_reader_p)
547:     : ColumnReader(reader, move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p),
548:       child_column_reader(move(child_column_reader_p)), read_cache(ListType::GetChildType(Type())),
549:       read_vector(read_cache), overflow_child_count(0) {
550: 
551: 	child_defines.resize(reader.allocator, STANDARD_VECTOR_SIZE);
552: 	child_repeats.resize(reader.allocator, STANDARD_VECTOR_SIZE);
553: 	child_defines_ptr = (uint8_t *)child_defines.ptr;
554: 	child_repeats_ptr = (uint8_t *)child_repeats.ptr;
555: 
556: 	child_filter.set();
557: }
558: 
559: //===--------------------------------------------------------------------===//
560: // Struct Column Reader
561: //===--------------------------------------------------------------------===//
562: StructColumnReader::StructColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
563:                                        idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p,
564:                                        vector<unique_ptr<ColumnReader>> child_readers_p)
565:     : ColumnReader(reader, move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p),
566:       child_readers(move(child_readers_p)) {
567: 	D_ASSERT(type.id() == LogicalTypeId::STRUCT);
568: 	D_ASSERT(!StructType::GetChildTypes(type).empty());
569: }
570: 
571: ColumnReader *StructColumnReader::GetChildReader(idx_t child_idx) {
572: 	return child_readers[child_idx].get();
573: }
574: 
575: void StructColumnReader::InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) {
576: 	for (auto &child : child_readers) {
577: 		child->InitializeRead(columns, protocol_p);
578: 	}
579: }
580: 
581: idx_t StructColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
582:                                Vector &result) {
583: 	auto &struct_entries = StructVector::GetEntries(result);
584: 	D_ASSERT(StructType::GetChildTypes(Type()).size() == struct_entries.size());
585: 
586: 	idx_t read_count = num_values;
587: 	for (idx_t i = 0; i < struct_entries.size(); i++) {
588: 		auto child_num_values = child_readers[i]->Read(num_values, filter, define_out, repeat_out, *struct_entries[i]);
589: 		if (i == 0) {
590: 			read_count = child_num_values;
591: 		} else if (read_count != child_num_values) {
592: 			throw std::runtime_error("Struct child row count mismatch");
593: 		}
594: 	}
595: 	// set the validity mask for this level
596: 	auto &validity = FlatVector::Validity(result);
597: 	for (idx_t i = 0; i < read_count; i++) {
598: 		if (define_out[i] < max_define) {
599: 			validity.SetInvalid(i);
600: 		}
601: 	}
602: 
603: 	return read_count;
604: }
605: 
606: void StructColumnReader::Skip(idx_t num_values) {
607: 	throw InternalException("Skip not implemented for StructColumnReader");
608: }
609: 
610: idx_t StructColumnReader::GroupRowsAvailable() {
611: 	for (idx_t i = 0; i < child_readers.size(); i++) {
612: 		if (child_readers[i]->Type().id() != LogicalTypeId::LIST) {
613: 			return child_readers[i]->GroupRowsAvailable();
614: 		}
615: 	}
616: 	return child_readers[0]->GroupRowsAvailable();
617: }
618: 
619: } // namespace duckdb
[end of extension/parquet/column_reader.cpp]
[start of extension/parquet/column_writer.cpp]
1: #include "column_writer.hpp"
2: #include "parquet_writer.hpp"
3: #include "parquet_rle_bp_decoder.hpp"
4: 
5: #include "duckdb.hpp"
6: #ifndef DUCKDB_AMALGAMATION
7: #include "duckdb/common/common.hpp"
8: #include "duckdb/common/exception.hpp"
9: #include "duckdb/common/mutex.hpp"
10: #include "duckdb/common/serializer/buffered_file_writer.hpp"
11: #include "duckdb/common/types/chunk_collection.hpp"
12: #include "duckdb/common/types/date.hpp"
13: #include "duckdb/common/types/hugeint.hpp"
14: #include "duckdb/common/types/time.hpp"
15: #include "duckdb/common/types/timestamp.hpp"
16: #include "duckdb/common/serializer/buffered_serializer.hpp"
17: #endif
18: 
19: #include "snappy.h"
20: #include "miniz_wrapper.hpp"
21: #include "zstd.h"
22: 
23: namespace duckdb {
24: 
25: using namespace duckdb_parquet; // NOLINT
26: using namespace duckdb_miniz;   // NOLINT
27: 
28: using duckdb_parquet::format::CompressionCodec;
29: using duckdb_parquet::format::ConvertedType;
30: using duckdb_parquet::format::Encoding;
31: using duckdb_parquet::format::FieldRepetitionType;
32: using duckdb_parquet::format::FileMetaData;
33: using duckdb_parquet::format::PageHeader;
34: using duckdb_parquet::format::PageType;
35: using ParquetRowGroup = duckdb_parquet::format::RowGroup;
36: using duckdb_parquet::format::Type;
37: 
38: #define PARQUET_DEFINE_VALID 65535
39: 
40: //===--------------------------------------------------------------------===//
41: // ColumnWriter
42: //===--------------------------------------------------------------------===//
43: ColumnWriter::ColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define)
44:     : writer(writer), schema_idx(schema_idx), max_repeat(max_repeat), max_define(max_define) {
45: }
46: ColumnWriter::~ColumnWriter() {
47: }
48: 
49: ColumnWriterState::~ColumnWriterState() {
50: }
51: 
52: static void VarintEncode(uint32_t val, Serializer &ser) {
53: 	do {
54: 		uint8_t byte = val & 127;
55: 		val >>= 7;
56: 		if (val != 0) {
57: 			byte |= 128;
58: 		}
59: 		ser.Write<uint8_t>(byte);
60: 	} while (val != 0);
61: }
62: 
63: static uint8_t GetVarintSize(uint32_t val) {
64: 	uint8_t res = 0;
65: 	do {
66: 		uint8_t byte = val & 127;
67: 		val >>= 7;
68: 		if (val != 0) {
69: 			byte |= 128;
70: 		}
71: 		res++;
72: 	} while (val != 0);
73: 	return res;
74: }
75: 
76: void ColumnWriter::CompressPage(BufferedSerializer &temp_writer, size_t &compressed_size, data_ptr_t &compressed_data,
77:                                 unique_ptr<data_t[]> &compressed_buf) {
78: 	switch (writer.codec) {
79: 	case CompressionCodec::UNCOMPRESSED:
80: 		compressed_size = temp_writer.blob.size;
81: 		compressed_data = temp_writer.blob.data.get();
82: 		break;
83: 	case CompressionCodec::SNAPPY: {
84: 		compressed_size = duckdb_snappy::MaxCompressedLength(temp_writer.blob.size);
85: 		compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
86: 		duckdb_snappy::RawCompress((const char *)temp_writer.blob.data.get(), temp_writer.blob.size,
87: 		                           (char *)compressed_buf.get(), &compressed_size);
88: 		compressed_data = compressed_buf.get();
89: 		D_ASSERT(compressed_size <= duckdb_snappy::MaxCompressedLength(temp_writer.blob.size));
90: 		break;
91: 	}
92: 	case CompressionCodec::GZIP: {
93: 		MiniZStream s;
94: 		compressed_size = s.MaxCompressedLength(temp_writer.blob.size);
95: 		compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
96: 		s.Compress((const char *)temp_writer.blob.data.get(), temp_writer.blob.size, (char *)compressed_buf.get(),
97: 		           &compressed_size);
98: 		compressed_data = compressed_buf.get();
99: 		break;
100: 	}
101: 	case CompressionCodec::ZSTD: {
102: 		compressed_size = duckdb_zstd::ZSTD_compressBound(temp_writer.blob.size);
103: 		compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
104: 		compressed_size = duckdb_zstd::ZSTD_compress((void *)compressed_buf.get(), compressed_size,
105: 		                                             (const void *)temp_writer.blob.data.get(), temp_writer.blob.size,
106: 		                                             ZSTD_CLEVEL_DEFAULT);
107: 		compressed_data = compressed_buf.get();
108: 		break;
109: 	}
110: 	default:
111: 		throw InternalException("Unsupported codec for Parquet Writer");
112: 	}
113: 
114: 	if (compressed_size > idx_t(NumericLimits<int32_t>::Maximum())) {
115: 		throw InternalException("Parquet writer: %d compressed page size out of range for type integer",
116: 		                        temp_writer.blob.size);
117: 	}
118: }
119: 
120: class ColumnWriterPageState {
121: public:
122: 	virtual ~ColumnWriterPageState() {
123: 	}
124: };
125: 
126: struct PageInformation {
127: 	idx_t offset = 0;
128: 	idx_t row_count = 0;
129: 	idx_t empty_count = 0;
130: 	idx_t estimated_page_size = 0;
131: };
132: 
133: struct PageWriteInformation {
134: 	PageHeader page_header;
135: 	unique_ptr<BufferedSerializer> temp_writer;
136: 	unique_ptr<ColumnWriterPageState> page_state;
137: 	idx_t write_page_idx = 0;
138: 	idx_t write_count = 0;
139: 	idx_t max_write_count = 0;
140: 	size_t compressed_size;
141: 	data_ptr_t compressed_data;
142: 	unique_ptr<data_t[]> compressed_buf;
143: };
144: 
145: class StandardColumnWriterState : public ColumnWriterState {
146: public:
147: 	StandardColumnWriterState(duckdb_parquet::format::RowGroup &row_group, idx_t col_idx)
148: 	    : row_group(row_group), col_idx(col_idx) {
149: 		page_info.emplace_back();
150: 	}
151: 	~StandardColumnWriterState() override = default;
152: 
153: 	duckdb_parquet::format::RowGroup &row_group;
154: 	idx_t col_idx;
155: 	vector<PageInformation> page_info;
156: 	vector<PageWriteInformation> write_info;
157: 	idx_t current_page = 0;
158: };
159: 
160: unique_ptr<ColumnWriterState> ColumnWriter::InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,
161:                                                                  vector<string> schema_path) {
162: 	auto result = make_unique<StandardColumnWriterState>(row_group, row_group.columns.size());
163: 
164: 	duckdb_parquet::format::ColumnChunk column_chunk;
165: 	column_chunk.__isset.meta_data = true;
166: 	column_chunk.meta_data.codec = writer.codec;
167: 	column_chunk.meta_data.path_in_schema = move(schema_path);
168: 	column_chunk.meta_data.path_in_schema.push_back(writer.file_meta_data.schema[schema_idx].name);
169: 	column_chunk.meta_data.num_values = 0;
170: 	column_chunk.meta_data.type = writer.file_meta_data.schema[schema_idx].type;
171: 	row_group.columns.push_back(move(column_chunk));
172: 
173: 	return move(result);
174: }
175: 
176: void ColumnWriter::HandleRepeatLevels(ColumnWriterState &state, ColumnWriterState *parent, idx_t count,
177:                                       idx_t max_repeat) {
178: 	if (!parent) {
179: 		// no repeat levels without a parent node
180: 		return;
181: 	}
182: 	while (state.repetition_levels.size() < parent->repetition_levels.size()) {
183: 		state.repetition_levels.push_back(parent->repetition_levels[state.repetition_levels.size()]);
184: 	}
185: }
186: 
187: void ColumnWriter::HandleDefineLevels(ColumnWriterState &state, ColumnWriterState *parent, ValidityMask &validity,
188:                                       idx_t count, uint16_t define_value, uint16_t null_value) {
189: 	if (parent) {
190: 		// parent node: inherit definition level from the parent
191: 		idx_t vector_index = 0;
192: 		while (state.definition_levels.size() < parent->definition_levels.size()) {
193: 			idx_t current_index = state.definition_levels.size();
194: 			if (parent->definition_levels[current_index] != PARQUET_DEFINE_VALID) {
195: 				state.definition_levels.push_back(parent->definition_levels[current_index]);
196: 			} else if (validity.RowIsValid(vector_index)) {
197: 				state.definition_levels.push_back(define_value);
198: 			} else {
199: 				state.definition_levels.push_back(null_value);
200: 			}
201: 			if (parent->is_empty.empty() || !parent->is_empty[current_index]) {
202: 				vector_index++;
203: 			}
204: 		}
205: 	} else {
206: 		// no parent: set definition levels only from this validity mask
207: 		for (idx_t i = 0; i < count; i++) {
208: 			if (validity.RowIsValid(i)) {
209: 				state.definition_levels.push_back(define_value);
210: 			} else {
211: 				state.definition_levels.push_back(null_value);
212: 			}
213: 		}
214: 	}
215: }
216: 
217: void ColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
218: 	auto &state = (StandardColumnWriterState &)state_p;
219: 	auto &col_chunk = state.row_group.columns[state.col_idx];
220: 
221: 	idx_t start = 0;
222: 	idx_t vcount = parent ? parent->definition_levels.size() - state.definition_levels.size() : count;
223: 	idx_t parent_index = state.definition_levels.size();
224: 	auto &validity = FlatVector::Validity(vector);
225: 	HandleRepeatLevels(state_p, parent, count, max_repeat);
226: 	HandleDefineLevels(state_p, parent, validity, count, max_define, max_define - 1);
227: 
228: 	idx_t vector_index = 0;
229: 	for (idx_t i = start; i < vcount; i++) {
230: 		auto &page_info = state.page_info.back();
231: 		page_info.row_count++;
232: 		col_chunk.meta_data.num_values++;
233: 		if (parent && !parent->is_empty.empty() && parent->is_empty[parent_index + i]) {
234: 			page_info.empty_count++;
235: 			continue;
236: 		}
237: 		if (validity.RowIsValid(vector_index)) {
238: 			page_info.estimated_page_size += GetRowSize(vector, vector_index);
239: 			if (page_info.estimated_page_size >= MAX_UNCOMPRESSED_PAGE_SIZE) {
240: 				PageInformation new_info;
241: 				new_info.offset = page_info.offset + page_info.row_count;
242: 				state.page_info.push_back(new_info);
243: 			}
244: 		}
245: 		vector_index++;
246: 	}
247: }
248: 
249: unique_ptr<ColumnWriterPageState> ColumnWriter::InitializePageState() {
250: 	return nullptr;
251: }
252: 
253: void ColumnWriter::FlushPageState(Serializer &temp_writer, ColumnWriterPageState *state) {
254: }
255: 
256: void ColumnWriter::BeginWrite(ColumnWriterState &state_p) {
257: 	auto &state = (StandardColumnWriterState &)state_p;
258: 
259: 	// set up the page write info
260: 	for (idx_t page_idx = 0; page_idx < state.page_info.size(); page_idx++) {
261: 		auto &page_info = state.page_info[page_idx];
262: 		if (page_info.row_count == 0) {
263: 			D_ASSERT(page_idx + 1 == state.page_info.size());
264: 			state.page_info.erase(state.page_info.begin() + page_idx);
265: 			break;
266: 		}
267: 		PageWriteInformation write_info;
268: 		// set up the header
269: 		auto &hdr = write_info.page_header;
270: 		hdr.compressed_page_size = 0;
271: 		hdr.uncompressed_page_size = 0;
272: 		hdr.type = PageType::DATA_PAGE;
273: 		hdr.__isset.data_page_header = true;
274: 
275: 		hdr.data_page_header.num_values = page_info.row_count;
276: 		hdr.data_page_header.encoding = Encoding::PLAIN;
277: 		hdr.data_page_header.definition_level_encoding = Encoding::RLE;
278: 		hdr.data_page_header.repetition_level_encoding = Encoding::RLE;
279: 
280: 		write_info.temp_writer = make_unique<BufferedSerializer>();
281: 		write_info.write_count = page_info.empty_count;
282: 		write_info.max_write_count = page_info.row_count;
283: 		write_info.page_state = InitializePageState();
284: 
285: 		write_info.compressed_size = 0;
286: 		write_info.compressed_data = nullptr;
287: 
288: 		state.write_info.push_back(move(write_info));
289: 	}
290: 
291: 	// start writing the first page
292: 	NextPage(state_p);
293: }
294: 
295: void ColumnWriter::WriteLevels(Serializer &temp_writer, const vector<uint16_t> &levels, idx_t max_value, idx_t offset,
296:                                idx_t count) {
297: 	if (levels.empty() || count == 0) {
298: 		return;
299: 	}
300: 
301: 	// write the levels
302: 	// we always RLE everything (for now)
303: 	auto bit_width = RleBpDecoder::ComputeBitWidth((max_value));
304: 	auto byte_width = (bit_width + 7) / 8;
305: 
306: 	// figure out how many bytes we are going to need
307: 	idx_t byte_count = 0;
308: 	idx_t run_count = 1;
309: 	idx_t current_run_count = 1;
310: 	for (idx_t i = offset + 1; i <= offset + count; i++) {
311: 		if (i == offset + count || levels[i] != levels[i - 1]) {
312: 			// last value, or value has changed
313: 			// write out the current run
314: 			byte_count += GetVarintSize(current_run_count << 1) + byte_width;
315: 			current_run_count = 1;
316: 			run_count++;
317: 		} else {
318: 			current_run_count++;
319: 		}
320: 	}
321: 	temp_writer.Write<uint32_t>(byte_count);
322: 
323: 	// now actually write the values
324: 	current_run_count = 1;
325: 	for (idx_t i = offset + 1; i <= offset + count; i++) {
326: 		if (i == offset + count || levels[i] != levels[i - 1]) {
327: 			// new run: write out the old run
328: 			// first write the header
329: 			VarintEncode(current_run_count << 1, temp_writer);
330: 			// now write hte value
331: 			switch (byte_width) {
332: 			case 1:
333: 				temp_writer.Write<uint8_t>(levels[i - 1]);
334: 				break;
335: 			case 2:
336: 				temp_writer.Write<uint16_t>(levels[i - 1]);
337: 				break;
338: 			default:
339: 				throw InternalException("unsupported byte width for RLE encoding");
340: 			}
341: 			current_run_count = 1;
342: 		} else {
343: 			current_run_count++;
344: 		}
345: 	}
346: }
347: 
348: void ColumnWriter::NextPage(ColumnWriterState &state_p) {
349: 	auto &state = (StandardColumnWriterState &)state_p;
350: 
351: 	if (state.current_page > 0) {
352: 		// need to flush the current page
353: 		FlushPage(state_p);
354: 	}
355: 	if (state.current_page >= state.write_info.size()) {
356: 		state.current_page = state.write_info.size() + 1;
357: 		return;
358: 	}
359: 	auto &page_info = state.page_info[state.current_page];
360: 	auto &write_info = state.write_info[state.current_page];
361: 	state.current_page++;
362: 
363: 	auto &temp_writer = *write_info.temp_writer;
364: 
365: 	// write the repetition levels
366: 	WriteLevels(temp_writer, state.repetition_levels, max_repeat, page_info.offset, page_info.row_count);
367: 
368: 	// write the definition levels
369: 	WriteLevels(temp_writer, state.definition_levels, max_define, page_info.offset, page_info.row_count);
370: }
371: 
372: void ColumnWriter::FlushPage(ColumnWriterState &state_p) {
373: 	auto &state = (StandardColumnWriterState &)state_p;
374: 	D_ASSERT(state.current_page > 0);
375: 	if (state.current_page > state.write_info.size()) {
376: 		return;
377: 	}
378: 
379: 	// compress the page info
380: 	auto &write_info = state.write_info[state.current_page - 1];
381: 	auto &temp_writer = *write_info.temp_writer;
382: 	auto &hdr = write_info.page_header;
383: 
384: 	FlushPageState(temp_writer, write_info.page_state.get());
385: 
386: 	// now that we have finished writing the data we know the uncompressed size
387: 	if (temp_writer.blob.size > idx_t(NumericLimits<int32_t>::Maximum())) {
388: 		throw InternalException("Parquet writer: %d uncompressed page size out of range for type integer",
389: 		                        temp_writer.blob.size);
390: 	}
391: 	hdr.uncompressed_page_size = temp_writer.blob.size;
392: 
393: 	// compress the data
394: 	CompressPage(temp_writer, write_info.compressed_size, write_info.compressed_data, write_info.compressed_buf);
395: 	hdr.compressed_page_size = write_info.compressed_size;
396: 	D_ASSERT(hdr.uncompressed_page_size > 0);
397: 	D_ASSERT(hdr.compressed_page_size > 0);
398: 
399: 	if (write_info.compressed_buf) {
400: 		// if the data has been compressed, we no longer need the compressed data
401: 		D_ASSERT(write_info.compressed_buf.get() == write_info.compressed_data);
402: 		write_info.temp_writer.reset();
403: 	}
404: }
405: 
406: void ColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count) {
407: 	auto &state = (StandardColumnWriterState &)state_p;
408: 
409: 	idx_t remaining = count;
410: 	idx_t offset = 0;
411: 	while (remaining > 0) {
412: 		auto &write_info = state.write_info[state.current_page - 1];
413: 		if (!write_info.temp_writer) {
414: 			throw InternalException("Writes are not correctly aligned!?");
415: 		}
416: 		auto &temp_writer = *write_info.temp_writer;
417: 		idx_t write_count = MinValue<idx_t>(remaining, write_info.max_write_count - write_info.write_count);
418: 		D_ASSERT(write_count > 0);
419: 
420: 		WriteVector(temp_writer, write_info.page_state.get(), vector, offset, offset + write_count);
421: 
422: 		write_info.write_count += write_count;
423: 		if (write_info.write_count == write_info.max_write_count) {
424: 			NextPage(state_p);
425: 		}
426: 		offset += write_count;
427: 		remaining -= write_count;
428: 	}
429: }
430: 
431: void ColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {
432: 	auto &state = (StandardColumnWriterState &)state_p;
433: 	auto &column_chunk = state.row_group.columns[state.col_idx];
434: 
435: 	// flush the last page (if any remains)
436: 	FlushPage(state);
437: 	// record the start position of the pages for this column
438: 	column_chunk.meta_data.data_page_offset = writer.writer->GetTotalWritten();
439: 	// write the individual pages to disk
440: 	for (auto &write_info : state.write_info) {
441: 		D_ASSERT(write_info.page_header.uncompressed_page_size > 0);
442: 		write_info.page_header.write(writer.protocol.get());
443: 		writer.writer->WriteData(write_info.compressed_data, write_info.compressed_size);
444: 	}
445: 	column_chunk.meta_data.total_compressed_size =
446: 	    writer.writer->GetTotalWritten() - column_chunk.meta_data.data_page_offset;
447: }
448: 
449: //===--------------------------------------------------------------------===//
450: // Standard Column Writer
451: //===--------------------------------------------------------------------===//
452: struct ParquetCastOperator {
453: 	template <class SRC, class TGT>
454: 	static TGT Operation(SRC input) {
455: 		return TGT(input);
456: 	}
457: };
458: 
459: struct ParquetTimestampNSOperator {
460: 	template <class SRC, class TGT>
461: 	static TGT Operation(SRC input) {
462: 		return Timestamp::FromEpochNanoSeconds(input).value;
463: 	}
464: };
465: 
466: struct ParquetTimestampSOperator {
467: 	template <class SRC, class TGT>
468: 	static TGT Operation(SRC input) {
469: 		return Timestamp::FromEpochSeconds(input).value;
470: 	}
471: };
472: 
473: struct ParquetHugeintOperator {
474: 	template <class SRC, class TGT>
475: 	static TGT Operation(SRC input) {
476: 		return Hugeint::Cast<double>(input);
477: 	}
478: };
479: 
480: template <class SRC, class TGT, class OP = ParquetCastOperator>
481: static void TemplatedWritePlain(Vector &col, idx_t chunk_start, idx_t chunk_end, ValidityMask &mask, Serializer &ser) {
482: 	auto *ptr = FlatVector::GetData<SRC>(col);
483: 	for (idx_t r = chunk_start; r < chunk_end; r++) {
484: 		if (mask.RowIsValid(r)) {
485: 			ser.Write<TGT>(OP::template Operation<SRC, TGT>(ptr[r]));
486: 		}
487: 	}
488: }
489: 
490: template <class SRC, class TGT, class OP = ParquetCastOperator>
491: class StandardColumnWriter : public ColumnWriter {
492: public:
493: 	StandardColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define)
494: 	    : ColumnWriter(writer, schema_idx, max_repeat, max_define) {
495: 	}
496: 	~StandardColumnWriter() override = default;
497: 
498: public:
499: 	void WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,
500: 	                 idx_t chunk_start, idx_t chunk_end) override {
501: 		auto &mask = FlatVector::Validity(input_column);
502: 		TemplatedWritePlain<SRC, TGT, OP>(input_column, chunk_start, chunk_end, mask, temp_writer);
503: 	}
504: 
505: 	idx_t GetRowSize(Vector &vector, idx_t index) override {
506: 		return sizeof(TGT);
507: 	}
508: };
509: 
510: //===--------------------------------------------------------------------===//
511: // Boolean Column Writer
512: //===--------------------------------------------------------------------===//
513: class BooleanWriterPageState : public ColumnWriterPageState {
514: public:
515: 	uint8_t byte = 0;
516: 	uint8_t byte_pos = 0;
517: };
518: 
519: class BooleanColumnWriter : public ColumnWriter {
520: public:
521: 	BooleanColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define)
522: 	    : ColumnWriter(writer, schema_idx, max_repeat, max_define) {
523: 	}
524: 	~BooleanColumnWriter() override = default;
525: 
526: public:
527: 	void WriteVector(Serializer &temp_writer, ColumnWriterPageState *state_p, Vector &input_column, idx_t chunk_start,
528: 	                 idx_t chunk_end) override {
529: 		auto &state = (BooleanWriterPageState &)*state_p;
530: 		auto &mask = FlatVector::Validity(input_column);
531: 
532: 		auto *ptr = FlatVector::GetData<bool>(input_column);
533: 		for (idx_t r = chunk_start; r < chunk_end; r++) {
534: 			if (mask.RowIsValid(r)) {
535: 				// only encode if non-null
536: 				if (ptr[r]) {
537: 					state.byte |= 1 << state.byte_pos;
538: 				}
539: 				state.byte_pos++;
540: 
541: 				if (state.byte_pos == 8) {
542: 					temp_writer.Write<uint8_t>(state.byte);
543: 					state.byte = 0;
544: 					state.byte_pos = 0;
545: 				}
546: 			}
547: 		}
548: 	}
549: 
550: 	unique_ptr<ColumnWriterPageState> InitializePageState() override {
551: 		return make_unique<BooleanWriterPageState>();
552: 	}
553: 
554: 	void FlushPageState(Serializer &temp_writer, ColumnWriterPageState *state_p) override {
555: 		auto &state = (BooleanWriterPageState &)*state_p;
556: 		if (state.byte_pos > 0) {
557: 			temp_writer.Write<uint8_t>(state.byte);
558: 			state.byte = 0;
559: 			state.byte_pos = 0;
560: 		}
561: 	}
562: 
563: 	idx_t GetRowSize(Vector &vector, idx_t index) override {
564: 		return sizeof(bool);
565: 	}
566: };
567: 
568: //===--------------------------------------------------------------------===//
569: // Decimal Column Writer
570: //===--------------------------------------------------------------------===//
571: class DecimalColumnWriter : public ColumnWriter {
572: public:
573: 	DecimalColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define)
574: 	    : ColumnWriter(writer, schema_idx, max_repeat, max_define) {
575: 	}
576: 	~DecimalColumnWriter() override = default;
577: 
578: public:
579: 	void WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,
580: 	                 idx_t chunk_start, idx_t chunk_end) override {
581: 		auto &mask = FlatVector::Validity(input_column);
582: 
583: 		// FIXME: fixed length byte array...
584: 		Vector double_vec(LogicalType::DOUBLE, true, false, chunk_end);
585: 		VectorOperations::Cast(input_column, double_vec, chunk_end);
586: 		TemplatedWritePlain<double, double>(double_vec, chunk_start, chunk_end, mask, temp_writer);
587: 	}
588: 
589: 	idx_t GetRowSize(Vector &vector, idx_t index) override {
590: 		return sizeof(double);
591: 	}
592: };
593: 
594: //===--------------------------------------------------------------------===//
595: // String Column Writer
596: //===--------------------------------------------------------------------===//
597: class StringColumnWriter : public ColumnWriter {
598: public:
599: 	StringColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define)
600: 	    : ColumnWriter(writer, schema_idx, max_repeat, max_define) {
601: 	}
602: 	~StringColumnWriter() override = default;
603: 
604: public:
605: 	void WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,
606: 	                 idx_t chunk_start, idx_t chunk_end) override {
607: 		auto &mask = FlatVector::Validity(input_column);
608: 
609: 		auto *ptr = FlatVector::GetData<string_t>(input_column);
610: 		for (idx_t r = chunk_start; r < chunk_end; r++) {
611: 			if (mask.RowIsValid(r)) {
612: 				temp_writer.Write<uint32_t>(ptr[r].GetSize());
613: 				temp_writer.WriteData((const_data_ptr_t)ptr[r].GetDataUnsafe(), ptr[r].GetSize());
614: 			}
615: 		}
616: 	}
617: 
618: 	idx_t GetRowSize(Vector &vector, idx_t index) override {
619: 		auto strings = FlatVector::GetData<string_t>(vector);
620: 		return strings[index].GetSize();
621: 	}
622: };
623: 
624: //===--------------------------------------------------------------------===//
625: // Struct Column Writer
626: //===--------------------------------------------------------------------===//
627: class StructColumnWriter : public ColumnWriter {
628: public:
629: 	StructColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define,
630: 	                   vector<unique_ptr<ColumnWriter>> child_writers_p)
631: 	    : ColumnWriter(writer, schema_idx, max_repeat, max_define), child_writers(move(child_writers_p)) {
632: 	}
633: 	~StructColumnWriter() override = default;
634: 
635: 	vector<unique_ptr<ColumnWriter>> child_writers;
636: 
637: public:
638: 	void WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,
639: 	                 idx_t chunk_start, idx_t chunk_end) override {
640: 		throw InternalException("Cannot write vector of type struct");
641: 	}
642: 
643: 	idx_t GetRowSize(Vector &vector, idx_t index) override {
644: 		throw InternalException("Cannot get row size of struct");
645: 	}
646: 
647: 	unique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,
648: 	                                                   vector<string> schema_path) override;
649: 	void Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;
650: 
651: 	void BeginWrite(ColumnWriterState &state) override;
652: 	void Write(ColumnWriterState &state, Vector &vector, idx_t count) override;
653: 	void FinalizeWrite(ColumnWriterState &state) override;
654: };
655: 
656: class StructColumnWriterState : public ColumnWriterState {
657: public:
658: 	StructColumnWriterState(duckdb_parquet::format::RowGroup &row_group, idx_t col_idx)
659: 	    : row_group(row_group), col_idx(col_idx) {
660: 	}
661: 	~StructColumnWriterState() override = default;
662: 
663: 	duckdb_parquet::format::RowGroup &row_group;
664: 	idx_t col_idx;
665: 	vector<unique_ptr<ColumnWriterState>> child_states;
666: };
667: 
668: unique_ptr<ColumnWriterState> StructColumnWriter::InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,
669:                                                                        vector<string> schema_path) {
670: 	auto result = make_unique<StructColumnWriterState>(row_group, row_group.columns.size());
671: 	schema_path.push_back(writer.file_meta_data.schema[schema_idx].name);
672: 
673: 	result->child_states.reserve(child_writers.size());
674: 	for (auto &child_writer : child_writers) {
675: 		result->child_states.push_back(child_writer->InitializeWriteState(row_group, schema_path));
676: 	}
677: 	return move(result);
678: }
679: 
680: void StructColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
681: 	auto &state = (StructColumnWriterState &)state_p;
682: 
683: 	auto &validity = FlatVector::Validity(vector);
684: 	if (parent) {
685: 		// propagate empty entries from the parent
686: 		while (state.is_empty.size() < parent->is_empty.size()) {
687: 			state.is_empty.push_back(parent->is_empty[state.is_empty.size()]);
688: 		}
689: 	}
690: 	HandleRepeatLevels(state_p, parent, count, max_repeat);
691: 	HandleDefineLevels(state_p, parent, validity, count, PARQUET_DEFINE_VALID, max_define - 1);
692: 	auto &child_vectors = StructVector::GetEntries(vector);
693: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
694: 		child_writers[child_idx]->Prepare(*state.child_states[child_idx], &state_p, *child_vectors[child_idx], count);
695: 	}
696: }
697: 
698: void StructColumnWriter::BeginWrite(ColumnWriterState &state_p) {
699: 	auto &state = (StructColumnWriterState &)state_p;
700: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
701: 		child_writers[child_idx]->BeginWrite(*state.child_states[child_idx]);
702: 	}
703: }
704: 
705: void StructColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count) {
706: 	auto &state = (StructColumnWriterState &)state_p;
707: 	auto &child_vectors = StructVector::GetEntries(vector);
708: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
709: 		child_writers[child_idx]->Write(*state.child_states[child_idx], *child_vectors[child_idx], count);
710: 	}
711: }
712: 
713: void StructColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {
714: 	auto &state = (StructColumnWriterState &)state_p;
715: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
716: 		child_writers[child_idx]->FinalizeWrite(*state.child_states[child_idx]);
717: 	}
718: }
719: 
720: //===--------------------------------------------------------------------===//
721: // List Column Writer
722: //===--------------------------------------------------------------------===//
723: class ListColumnWriter : public ColumnWriter {
724: public:
725: 	ListColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define,
726: 	                 unique_ptr<ColumnWriter> child_writer_p)
727: 	    : ColumnWriter(writer, schema_idx, max_repeat, max_define), child_writer(move(child_writer_p)) {
728: 	}
729: 	~ListColumnWriter() override = default;
730: 
731: 	unique_ptr<ColumnWriter> child_writer;
732: 
733: public:
734: 	void WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,
735: 	                 idx_t chunk_start, idx_t chunk_end) override {
736: 		throw InternalException("Cannot write vector of type list");
737: 	}
738: 
739: 	idx_t GetRowSize(Vector &vector, idx_t index) override {
740: 		throw InternalException("Cannot get row size of list");
741: 	}
742: 
743: 	unique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,
744: 	                                                   vector<string> schema_path) override;
745: 	void Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;
746: 
747: 	void BeginWrite(ColumnWriterState &state) override;
748: 	void Write(ColumnWriterState &state, Vector &vector, idx_t count) override;
749: 	void FinalizeWrite(ColumnWriterState &state) override;
750: };
751: 
752: class ListColumnWriterState : public ColumnWriterState {
753: public:
754: 	ListColumnWriterState(duckdb_parquet::format::RowGroup &row_group, idx_t col_idx)
755: 	    : row_group(row_group), col_idx(col_idx) {
756: 	}
757: 	~ListColumnWriterState() override = default;
758: 
759: 	duckdb_parquet::format::RowGroup &row_group;
760: 	idx_t col_idx;
761: 	unique_ptr<ColumnWriterState> child_state;
762: 	idx_t parent_index = 0;
763: };
764: 
765: unique_ptr<ColumnWriterState> ListColumnWriter::InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,
766:                                                                      vector<string> schema_path) {
767: 	auto result = make_unique<ListColumnWriterState>(row_group, row_group.columns.size());
768: 	schema_path.push_back(writer.file_meta_data.schema[schema_idx].name);
769: 	result->child_state = child_writer->InitializeWriteState(row_group, move(schema_path));
770: 	return move(result);
771: }
772: 
773: void ListColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
774: 	auto &state = (ListColumnWriterState &)state_p;
775: 
776: 	auto list_data = FlatVector::GetData<list_entry_t>(vector);
777: 	auto &validity = FlatVector::Validity(vector);
778: 
779: 	// write definition levels and repeats
780: 	idx_t start = 0;
781: 	idx_t vcount = parent ? parent->definition_levels.size() - state.parent_index : count;
782: 	idx_t vector_index = 0;
783: 	for (idx_t i = start; i < vcount; i++) {
784: 		idx_t parent_index = state.parent_index + i;
785: 		if (parent && !parent->is_empty.empty() && parent->is_empty[parent_index]) {
786: 			state.definition_levels.push_back(parent->definition_levels[parent_index]);
787: 			state.repetition_levels.push_back(parent->repetition_levels[parent_index]);
788: 			state.is_empty.push_back(true);
789: 			continue;
790: 		}
791: 		auto first_repeat_level =
792: 		    parent && !parent->repetition_levels.empty() ? parent->repetition_levels[parent_index] : max_repeat;
793: 		if (parent && parent->definition_levels[parent_index] != PARQUET_DEFINE_VALID) {
794: 			state.definition_levels.push_back(parent->definition_levels[parent_index]);
795: 			state.repetition_levels.push_back(first_repeat_level);
796: 			state.is_empty.push_back(true);
797: 		} else if (validity.RowIsValid(vector_index)) {
798: 			// push the repetition levels
799: 			if (list_data[vector_index].length == 0) {
800: 				state.definition_levels.push_back(max_define);
801: 				state.is_empty.push_back(true);
802: 			} else {
803: 				state.definition_levels.push_back(PARQUET_DEFINE_VALID);
804: 				state.is_empty.push_back(false);
805: 			}
806: 			state.repetition_levels.push_back(first_repeat_level);
807: 			for (idx_t k = 1; k < list_data[vector_index].length; k++) {
808: 				state.repetition_levels.push_back(max_repeat + 1);
809: 				state.definition_levels.push_back(PARQUET_DEFINE_VALID);
810: 				state.is_empty.push_back(false);
811: 			}
812: 		} else {
813: 			state.definition_levels.push_back(max_define - 1);
814: 			state.repetition_levels.push_back(first_repeat_level);
815: 			state.is_empty.push_back(true);
816: 		}
817: 		vector_index++;
818: 	}
819: 	state.parent_index += vcount;
820: 
821: 	auto &list_child = ListVector::GetEntry(vector);
822: 	auto list_count = ListVector::GetListSize(vector);
823: 	child_writer->Prepare(*state.child_state, &state_p, list_child, list_count);
824: }
825: 
826: void ListColumnWriter::BeginWrite(ColumnWriterState &state_p) {
827: 	auto &state = (ListColumnWriterState &)state_p;
828: 	child_writer->BeginWrite(*state.child_state);
829: }
830: 
831: void ListColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count) {
832: 	auto &state = (ListColumnWriterState &)state_p;
833: 
834: 	auto &list_child = ListVector::GetEntry(vector);
835: 	auto list_count = ListVector::GetListSize(vector);
836: 	child_writer->Write(*state.child_state, list_child, list_count);
837: }
838: 
839: void ListColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {
840: 	auto &state = (ListColumnWriterState &)state_p;
841: 	child_writer->FinalizeWrite(*state.child_state);
842: }
843: 
844: //===--------------------------------------------------------------------===//
845: // Create Column Writer
846: //===--------------------------------------------------------------------===//
847: unique_ptr<ColumnWriter> ColumnWriter::CreateWriterRecursive(vector<duckdb_parquet::format::SchemaElement> &schemas,
848:                                                              ParquetWriter &writer, const LogicalType &type,
849:                                                              const string &name, idx_t max_repeat, idx_t max_define) {
850: 	idx_t schema_idx = schemas.size();
851: 	if (type.id() == LogicalTypeId::STRUCT) {
852: 		auto &child_types = StructType::GetChildTypes(type);
853: 		// set up the schema element for this struct
854: 		duckdb_parquet::format::SchemaElement schema_element;
855: 		schema_element.repetition_type = FieldRepetitionType::OPTIONAL;
856: 		schema_element.num_children = child_types.size();
857: 		schema_element.__isset.num_children = true;
858: 		schema_element.__isset.type = false;
859: 		schema_element.__isset.repetition_type = true;
860: 		schema_element.name = name;
861: 		schemas.push_back(move(schema_element));
862: 		// construct the child types recursively
863: 		vector<unique_ptr<ColumnWriter>> child_writers;
864: 		child_writers.reserve(child_types.size());
865: 		for (auto &child_type : child_types) {
866: 			child_writers.push_back(CreateWriterRecursive(schemas, writer, child_type.second, child_type.first,
867: 			                                              max_repeat, max_define + 1));
868: 		}
869: 		return make_unique<StructColumnWriter>(writer, schema_idx, max_repeat, max_define, move(child_writers));
870: 	}
871: 	if (type.id() == LogicalTypeId::LIST) {
872: 		auto &child_type = ListType::GetChildType(type);
873: 		// set up the two schema elements for the list
874: 		// for some reason we only set the converted type in the OPTIONAL element
875: 		// first an OPTIONAL element
876: 		duckdb_parquet::format::SchemaElement optional_element;
877: 		optional_element.repetition_type = FieldRepetitionType::OPTIONAL;
878: 		optional_element.num_children = 1;
879: 		optional_element.converted_type = ConvertedType::LIST;
880: 		optional_element.__isset.num_children = true;
881: 		optional_element.__isset.type = false;
882: 		optional_element.__isset.repetition_type = true;
883: 		optional_element.__isset.converted_type = true;
884: 		optional_element.name = name;
885: 		schemas.push_back(move(optional_element));
886: 
887: 		// then a REPEATED element
888: 		duckdb_parquet::format::SchemaElement repeated_element;
889: 		repeated_element.repetition_type = FieldRepetitionType::REPEATED;
890: 		repeated_element.num_children = 1;
891: 		repeated_element.__isset.num_children = true;
892: 		repeated_element.__isset.type = false;
893: 		repeated_element.__isset.repetition_type = true;
894: 		repeated_element.name = "list";
895: 		schemas.push_back(move(repeated_element));
896: 
897: 		auto child_writer = CreateWriterRecursive(schemas, writer, child_type, "child", max_repeat + 1, max_define + 2);
898: 		return make_unique<ListColumnWriter>(writer, schema_idx, max_repeat, max_define, move(child_writer));
899: 	}
900: 	duckdb_parquet::format::SchemaElement schema_element;
901: 	schema_element.type = ParquetWriter::DuckDBTypeToParquetType(type);
902: 	schema_element.repetition_type = FieldRepetitionType::OPTIONAL;
903: 	schema_element.num_children = 0;
904: 	schema_element.__isset.num_children = true;
905: 	schema_element.__isset.type = true;
906: 	schema_element.__isset.repetition_type = true;
907: 	schema_element.name = name;
908: 	schema_element.__isset.converted_type =
909: 	    ParquetWriter::DuckDBTypeToConvertedType(type, schema_element.converted_type);
910: 	schemas.push_back(move(schema_element));
911: 
912: 	switch (type.id()) {
913: 	case LogicalTypeId::BOOLEAN:
914: 		return make_unique<BooleanColumnWriter>(writer, schema_idx, max_repeat, max_define);
915: 	case LogicalTypeId::TINYINT:
916: 		return make_unique<StandardColumnWriter<int8_t, int32_t>>(writer, schema_idx, max_repeat, max_define);
917: 	case LogicalTypeId::SMALLINT:
918: 		return make_unique<StandardColumnWriter<int16_t, int32_t>>(writer, schema_idx, max_repeat, max_define);
919: 	case LogicalTypeId::INTEGER:
920: 	case LogicalTypeId::DATE:
921: 		return make_unique<StandardColumnWriter<int32_t, int32_t>>(writer, schema_idx, max_repeat, max_define);
922: 	case LogicalTypeId::BIGINT:
923: 	case LogicalTypeId::TIMESTAMP:
924: 	case LogicalTypeId::TIMESTAMP_MS:
925: 		return make_unique<StandardColumnWriter<int64_t, int64_t>>(writer, schema_idx, max_repeat, max_define);
926: 	case LogicalTypeId::HUGEINT:
927: 		return make_unique<StandardColumnWriter<hugeint_t, double, ParquetHugeintOperator>>(writer, schema_idx,
928: 		                                                                                    max_repeat, max_define);
929: 	case LogicalTypeId::TIMESTAMP_NS:
930: 		return make_unique<StandardColumnWriter<int64_t, int64_t, ParquetTimestampNSOperator>>(writer, schema_idx,
931: 		                                                                                       max_repeat, max_define);
932: 	case LogicalTypeId::TIMESTAMP_SEC:
933: 		return make_unique<StandardColumnWriter<int64_t, int64_t, ParquetTimestampSOperator>>(writer, schema_idx,
934: 		                                                                                      max_repeat, max_define);
935: 	case LogicalTypeId::UTINYINT:
936: 		return make_unique<StandardColumnWriter<uint8_t, int32_t>>(writer, schema_idx, max_repeat, max_define);
937: 	case LogicalTypeId::USMALLINT:
938: 		return make_unique<StandardColumnWriter<uint16_t, int32_t>>(writer, schema_idx, max_repeat, max_define);
939: 	case LogicalTypeId::UINTEGER:
940: 		return make_unique<StandardColumnWriter<uint32_t, uint32_t>>(writer, schema_idx, max_repeat, max_define);
941: 	case LogicalTypeId::UBIGINT:
942: 		return make_unique<StandardColumnWriter<uint64_t, uint64_t>>(writer, schema_idx, max_repeat, max_define);
943: 	case LogicalTypeId::FLOAT:
944: 		return make_unique<StandardColumnWriter<float, float>>(writer, schema_idx, max_repeat, max_define);
945: 	case LogicalTypeId::DOUBLE:
946: 		return make_unique<StandardColumnWriter<double, double>>(writer, schema_idx, max_repeat, max_define);
947: 	case LogicalTypeId::DECIMAL:
948: 		return make_unique<DecimalColumnWriter>(writer, schema_idx, max_repeat, max_define);
949: 	case LogicalTypeId::BLOB:
950: 	case LogicalTypeId::VARCHAR:
951: 		return make_unique<StringColumnWriter>(writer, schema_idx, max_repeat, max_define);
952: 	default:
953: 		throw InternalException("Unsupported type in Parquet writer");
954: 	}
955: }
956: 
957: } // namespace duckdb
[end of extension/parquet/column_writer.cpp]
[start of extension/parquet/include/column_writer.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // column_writer.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb.hpp"
12: #include "parquet_types.h"
13: 
14: namespace duckdb {
15: class BufferedSerializer;
16: class ParquetWriter;
17: class ColumnWriterPageState;
18: 
19: class ColumnWriterState {
20: public:
21: 	virtual ~ColumnWriterState();
22: 
23: 	vector<uint16_t> definition_levels;
24: 	vector<uint16_t> repetition_levels;
25: 	vector<bool> is_empty;
26: };
27: 
28: class ColumnWriter {
29: 	//! We limit the uncompressed page size to 100MB
30: 	// The max size in Parquet is 2GB, but we choose a more conservative limit
31: 	static constexpr const idx_t MAX_UNCOMPRESSED_PAGE_SIZE = 100000000;
32: 
33: public:
34: 	ColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define);
35: 	virtual ~ColumnWriter();
36: 
37: 	ParquetWriter &writer;
38: 	idx_t schema_idx;
39: 	idx_t max_repeat;
40: 	idx_t max_define;
41: 
42: public:
43: 	//! Create the column writer for a specific type recursively
44: 	static unique_ptr<ColumnWriter> CreateWriterRecursive(vector<duckdb_parquet::format::SchemaElement> &schemas,
45: 	                                                      ParquetWriter &writer, const LogicalType &type,
46: 	                                                      const string &name, idx_t max_repeat = 0,
47: 	                                                      idx_t max_define = 1);
48: 
49: 	virtual unique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,
50: 	                                                           vector<string> schema_path);
51: 	virtual void Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count);
52: 
53: 	virtual void BeginWrite(ColumnWriterState &state);
54: 	virtual void Write(ColumnWriterState &state, Vector &vector, idx_t count);
55: 	virtual void FinalizeWrite(ColumnWriterState &state);
56: 
57: protected:
58: 	void HandleDefineLevels(ColumnWriterState &state, ColumnWriterState *parent, ValidityMask &validity, idx_t count,
59: 	                        uint16_t define_value, uint16_t null_value);
60: 	void HandleRepeatLevels(ColumnWriterState &state_p, ColumnWriterState *parent, idx_t count, idx_t max_repeat);
61: 
62: 	void WriteLevels(Serializer &temp_writer, const vector<uint16_t> &levels, idx_t max_value, idx_t start_offset,
63: 	                 idx_t count);
64: 
65: 	void NextPage(ColumnWriterState &state_p);
66: 	void FlushPage(ColumnWriterState &state_p);
67: 
68: 	//! Retrieves the row size of a vector at the specified location. Only used for scalar types.
69: 	virtual idx_t GetRowSize(Vector &vector, idx_t index) = 0;
70: 	//! Writes a (subset of a) vector to the specified serializer. Only used for scalar types.
71: 	virtual void WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &vector,
72: 	                         idx_t chunk_start, idx_t chunk_end) = 0;
73: 	//! Initialize the writer for a specific page. Only used for scalar types.
74: 	virtual unique_ptr<ColumnWriterPageState> InitializePageState();
75: 	//! Flushes the writer for a specific page. Only used for scalar types.
76: 	virtual void FlushPageState(Serializer &temp_writer, ColumnWriterPageState *state);
77: 
78: 	void CompressPage(BufferedSerializer &temp_writer, size_t &compressed_size, data_ptr_t &compressed_data,
79: 	                  unique_ptr<data_t[]> &compressed_buf);
80: };
81: 
82: } // namespace duckdb
[end of extension/parquet/include/column_writer.hpp]
[start of extension/parquet/parquet_reader.cpp]
1: #include "parquet_reader.hpp"
2: #include "parquet_timestamp.hpp"
3: #include "parquet_statistics.hpp"
4: #include "column_reader.hpp"
5: 
6: #include "boolean_column_reader.hpp"
7: #include "callback_column_reader.hpp"
8: #include "decimal_column_reader.hpp"
9: #include "list_column_reader.hpp"
10: #include "string_column_reader.hpp"
11: #include "struct_column_reader.hpp"
12: #include "templated_column_reader.hpp"
13: 
14: #include "thrift_tools.hpp"
15: 
16: #include "parquet_file_metadata_cache.hpp"
17: 
18: #include "duckdb.hpp"
19: #ifndef DUCKDB_AMALGAMATION
20: #include "duckdb/planner/table_filter.hpp"
21: #include "duckdb/planner/filter/constant_filter.hpp"
22: #include "duckdb/planner/filter/null_filter.hpp"
23: #include "duckdb/planner/filter/conjunction_filter.hpp"
24: #include "duckdb/common/file_system.hpp"
25: #include "duckdb/common/string_util.hpp"
26: #include "duckdb/common/types/date.hpp"
27: #include "duckdb/common/pair.hpp"
28: 
29: #include "duckdb/storage/object_cache.hpp"
30: #endif
31: 
32: #include <sstream>
33: #include <cassert>
34: #include <chrono>
35: #include <cstring>
36: #include <iostream>
37: 
38: namespace duckdb {
39: 
40: using duckdb_parquet::format::ColumnChunk;
41: using duckdb_parquet::format::ConvertedType;
42: using duckdb_parquet::format::FieldRepetitionType;
43: using duckdb_parquet::format::FileMetaData;
44: using ParquetRowGroup = duckdb_parquet::format::RowGroup;
45: using duckdb_parquet::format::SchemaElement;
46: using duckdb_parquet::format::Statistics;
47: using duckdb_parquet::format::Type;
48: 
49: static unique_ptr<duckdb_apache::thrift::protocol::TProtocol> CreateThriftProtocol(Allocator &allocator,
50:                                                                                    FileHandle &file_handle) {
51: 	auto transport = make_shared<ThriftFileTransport>(allocator, file_handle);
52: 	return make_unique<duckdb_apache::thrift::protocol::TCompactProtocolT<ThriftFileTransport>>(move(transport));
53: }
54: 
55: static shared_ptr<ParquetFileMetadataCache> LoadMetadata(Allocator &allocator, FileHandle &file_handle) {
56: 	auto current_time = std::chrono::system_clock::to_time_t(std::chrono::system_clock::now());
57: 
58: 	auto proto = CreateThriftProtocol(allocator, file_handle);
59: 	auto &transport = ((ThriftFileTransport &)*proto->getTransport());
60: 	auto file_size = transport.GetSize();
61: 	if (file_size < 12) {
62: 		throw InvalidInputException("File '%s' too small to be a Parquet file", file_handle.path);
63: 	}
64: 
65: 	ResizeableBuffer buf;
66: 	buf.resize(allocator, 8);
67: 	buf.zero();
68: 
69: 	transport.SetLocation(file_size - 8);
70: 	transport.read((uint8_t *)buf.ptr, 8);
71: 
72: 	if (strncmp(buf.ptr + 4, "PAR1", 4) != 0) {
73: 		throw InvalidInputException("No magic bytes found at end of file '%s'", file_handle.path);
74: 	}
75: 	// read four-byte footer length from just before the end magic bytes
76: 	auto footer_len = *(uint32_t *)buf.ptr;
77: 	if (footer_len <= 0 || file_size < 12 + footer_len) {
78: 		throw InvalidInputException("Footer length error in file '%s'", file_handle.path);
79: 	}
80: 	auto metadata_pos = file_size - (footer_len + 8);
81: 	transport.SetLocation(metadata_pos);
82: 	transport.Prefetch(metadata_pos, footer_len);
83: 
84: 	auto metadata = make_unique<FileMetaData>();
85: 	metadata->read(proto.get());
86: 	return make_shared<ParquetFileMetadataCache>(move(metadata), current_time);
87: }
88: 
89: LogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele) {
90: 	// inner node
91: 	D_ASSERT(s_ele.__isset.type && s_ele.num_children == 0);
92: 	if (s_ele.type == Type::FIXED_LEN_BYTE_ARRAY && !s_ele.__isset.type_length) {
93: 		throw IOException("FIXED_LEN_BYTE_ARRAY requires length to be set");
94: 	}
95: 	if (s_ele.__isset.converted_type) {
96: 		switch (s_ele.converted_type) {
97: 		case ConvertedType::INT_8:
98: 			if (s_ele.type == Type::INT32) {
99: 				return LogicalType::TINYINT;
100: 			} else {
101: 				throw IOException("INT8 converted type can only be set for value of Type::INT32");
102: 			}
103: 		case ConvertedType::INT_16:
104: 			if (s_ele.type == Type::INT32) {
105: 				return LogicalType::SMALLINT;
106: 			} else {
107: 				throw IOException("INT16 converted type can only be set for value of Type::INT32");
108: 			}
109: 		case ConvertedType::INT_32:
110: 			if (s_ele.type == Type::INT32) {
111: 				return LogicalType::INTEGER;
112: 			} else {
113: 				throw IOException("INT32 converted type can only be set for value of Type::INT32");
114: 			}
115: 		case ConvertedType::INT_64:
116: 			if (s_ele.type == Type::INT64) {
117: 				return LogicalType::BIGINT;
118: 			} else {
119: 				throw IOException("INT64 converted type can only be set for value of Type::INT32");
120: 			}
121: 		case ConvertedType::UINT_8:
122: 			if (s_ele.type == Type::INT32) {
123: 				return LogicalType::UTINYINT;
124: 			} else {
125: 				throw IOException("UINT8 converted type can only be set for value of Type::INT32");
126: 			}
127: 		case ConvertedType::UINT_16:
128: 			if (s_ele.type == Type::INT32) {
129: 				return LogicalType::USMALLINT;
130: 			} else {
131: 				throw IOException("UINT16 converted type can only be set for value of Type::INT32");
132: 			}
133: 		case ConvertedType::UINT_32:
134: 			if (s_ele.type == Type::INT32) {
135: 				return LogicalType::UINTEGER;
136: 			} else {
137: 				throw IOException("UINT32 converted type can only be set for value of Type::INT32");
138: 			}
139: 		case ConvertedType::UINT_64:
140: 			if (s_ele.type == Type::INT64) {
141: 				return LogicalType::UBIGINT;
142: 			} else {
143: 				throw IOException("UINT64 converted type can only be set for value of Type::INT64");
144: 			}
145: 		case ConvertedType::DATE:
146: 			if (s_ele.type == Type::INT32) {
147: 				return LogicalType::DATE;
148: 			} else {
149: 				throw IOException("DATE converted type can only be set for value of Type::INT32");
150: 			}
151: 		case ConvertedType::TIMESTAMP_MICROS:
152: 		case ConvertedType::TIMESTAMP_MILLIS:
153: 			if (s_ele.type == Type::INT64) {
154: 				return LogicalType::TIMESTAMP;
155: 			} else {
156: 				throw IOException("TIMESTAMP converted type can only be set for value of Type::INT64");
157: 			}
158: 		case ConvertedType::DECIMAL:
159: 			if (!s_ele.__isset.precision || !s_ele.__isset.scale) {
160: 				throw IOException("DECIMAL requires a length and scale specifier!");
161: 			}
162: 			switch (s_ele.type) {
163: 			case Type::BYTE_ARRAY:
164: 			case Type::FIXED_LEN_BYTE_ARRAY:
165: 			case Type::INT32:
166: 			case Type::INT64:
167: 				return LogicalType::DECIMAL(s_ele.precision, s_ele.scale);
168: 			default:
169: 				throw IOException(
170: 				    "DECIMAL converted type can only be set for value of Type::(FIXED_LEN_)BYTE_ARRAY/INT32/INT64");
171: 			}
172: 		case ConvertedType::UTF8:
173: 			switch (s_ele.type) {
174: 			case Type::BYTE_ARRAY:
175: 			case Type::FIXED_LEN_BYTE_ARRAY:
176: 				return LogicalType::VARCHAR;
177: 			default:
178: 				throw IOException("UTF8 converted type can only be set for Type::(FIXED_LEN_)BYTE_ARRAY");
179: 			}
180: 		case ConvertedType::MAP:
181: 		case ConvertedType::MAP_KEY_VALUE:
182: 		case ConvertedType::LIST:
183: 		case ConvertedType::ENUM:
184: 		case ConvertedType::TIME_MILLIS:
185: 		case ConvertedType::TIME_MICROS:
186: 		case ConvertedType::JSON:
187: 		case ConvertedType::BSON:
188: 		case ConvertedType::INTERVAL:
189: 		default:
190: 			throw IOException("Unsupported converted type");
191: 		}
192: 	} else {
193: 		// no converted type set
194: 		// use default type for each physical type
195: 		switch (s_ele.type) {
196: 		case Type::BOOLEAN:
197: 			return LogicalType::BOOLEAN;
198: 		case Type::INT32:
199: 			return LogicalType::INTEGER;
200: 		case Type::INT64:
201: 			return LogicalType::BIGINT;
202: 		case Type::INT96: // always a timestamp it would seem
203: 			return LogicalType::TIMESTAMP;
204: 		case Type::FLOAT:
205: 			return LogicalType::FLOAT;
206: 		case Type::DOUBLE:
207: 			return LogicalType::DOUBLE;
208: 		case Type::BYTE_ARRAY:
209: 		case Type::FIXED_LEN_BYTE_ARRAY:
210: 			if (parquet_options.binary_as_string) {
211: 				return LogicalType::VARCHAR;
212: 			}
213: 			return LogicalType::BLOB;
214: 		default:
215: 			return LogicalType::INVALID;
216: 		}
217: 	}
218: }
219: 
220: unique_ptr<ColumnReader> ParquetReader::CreateReaderRecursive(const FileMetaData *file_meta_data, idx_t depth,
221:                                                               idx_t max_define, idx_t max_repeat,
222:                                                               idx_t &next_schema_idx, idx_t &next_file_idx) {
223: 	D_ASSERT(file_meta_data);
224: 	D_ASSERT(next_schema_idx < file_meta_data->schema.size());
225: 	auto &s_ele = file_meta_data->schema[next_schema_idx];
226: 	auto this_idx = next_schema_idx;
227: 
228: 	if (s_ele.__isset.repetition_type) {
229: 		if (s_ele.repetition_type != FieldRepetitionType::REQUIRED) {
230: 			max_define++;
231: 		}
232: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
233: 			max_repeat++;
234: 		}
235: 	}
236: 
237: 	if (!s_ele.__isset.type) { // inner node
238: 		if (s_ele.num_children == 0) {
239: 			throw std::runtime_error("Node has no children but should");
240: 		}
241: 		child_list_t<LogicalType> child_types;
242: 		vector<unique_ptr<ColumnReader>> child_readers;
243: 
244: 		idx_t c_idx = 0;
245: 		while (c_idx < (idx_t)s_ele.num_children) {
246: 			next_schema_idx++;
247: 
248: 			auto &child_ele = file_meta_data->schema[next_schema_idx];
249: 
250: 			auto child_reader = CreateReaderRecursive(file_meta_data, depth + 1, max_define, max_repeat,
251: 			                                          next_schema_idx, next_file_idx);
252: 			child_types.push_back(make_pair(child_ele.name, child_reader->Type()));
253: 			child_readers.push_back(move(child_reader));
254: 
255: 			c_idx++;
256: 		}
257: 		D_ASSERT(!child_types.empty());
258: 		unique_ptr<ColumnReader> result;
259: 		LogicalType result_type;
260: 
261: 		bool is_repeated = s_ele.repetition_type == FieldRepetitionType::REPEATED;
262: 		bool is_list = s_ele.__isset.converted_type && s_ele.converted_type == ConvertedType::LIST;
263: 		bool is_map = s_ele.__isset.converted_type && s_ele.converted_type == ConvertedType::MAP;
264: 		// if we only have a single child no reason to create a struct ay
265: 		if (child_types.size() > 1 || (!is_list && !is_map && !is_repeated)) {
266: 			result_type = LogicalType::STRUCT(move(child_types));
267: 			result = make_unique<StructColumnReader>(*this, result_type, s_ele, this_idx, max_define, max_repeat,
268: 			                                         move(child_readers));
269: 		} else {
270: 			// if we have a struct with only a single type, pull up
271: 			result_type = child_types[0].second;
272: 			result = move(child_readers[0]);
273: 		}
274: 		if (is_repeated) {
275: 			result_type = LogicalType::LIST(result_type);
276: 			return make_unique<ListColumnReader>(*this, result_type, s_ele, this_idx, max_define, max_repeat,
277: 			                                     move(result));
278: 		}
279: 		return result;
280: 	} else { // leaf node
281: 
282: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
283: 			const auto derived_type = DeriveLogicalType(s_ele);
284: 			auto list_type = LogicalType::LIST(derived_type);
285: 
286: 			auto element_reader =
287: 			    ColumnReader::CreateReader(*this, derived_type, s_ele, next_file_idx++, max_define, max_repeat);
288: 
289: 			return make_unique<ListColumnReader>(*this, list_type, s_ele, this_idx, max_define, max_repeat,
290: 			                                     move(element_reader));
291: 		}
292: 
293: 		// TODO check return value of derive type or should we only do this on read()
294: 		return ColumnReader::CreateReader(*this, DeriveLogicalType(s_ele), s_ele, next_file_idx++, max_define,
295: 		                                  max_repeat);
296: 	}
297: }
298: 
299: // TODO we don't need readers for columns we are not going to read ay
300: unique_ptr<ColumnReader> ParquetReader::CreateReader(const duckdb_parquet::format::FileMetaData *file_meta_data) {
301: 	idx_t next_schema_idx = 0;
302: 	idx_t next_file_idx = 0;
303: 
304: 	auto ret = CreateReaderRecursive(file_meta_data, 0, 0, 0, next_schema_idx, next_file_idx);
305: 	D_ASSERT(next_schema_idx == file_meta_data->schema.size() - 1);
306: 	D_ASSERT(file_meta_data->row_groups.empty() || next_file_idx == file_meta_data->row_groups[0].columns.size());
307: 	return ret;
308: }
309: 
310: void ParquetReader::InitializeSchema(const vector<LogicalType> &expected_types_p, const string &initial_filename_p) {
311: 	auto file_meta_data = GetFileMetadata();
312: 
313: 	if (file_meta_data->__isset.encryption_algorithm) {
314: 		throw FormatException("Encrypted Parquet files are not supported");
315: 	}
316: 	// check if we like this schema
317: 	if (file_meta_data->schema.size() < 2) {
318: 		throw FormatException("Need at least one non-root column in the file");
319: 	}
320: 
321: 	bool has_expected_types = !expected_types_p.empty();
322: 	auto root_reader = CreateReader(file_meta_data);
323: 
324: 	auto &root_type = root_reader->Type();
325: 	auto &child_types = StructType::GetChildTypes(root_type);
326: 	D_ASSERT(root_type.id() == LogicalTypeId::STRUCT);
327: 	if (has_expected_types && child_types.size() != expected_types_p.size()) {
328: 		throw FormatException("column count mismatch");
329: 	}
330: 	idx_t col_idx = 0;
331: 	for (auto &type_pair : child_types) {
332: 		if (has_expected_types && expected_types_p[col_idx] != type_pair.second) {
333: 			if (initial_filename_p.empty()) {
334: 				throw FormatException("column \"%d\" in parquet file is of type %s, could not auto cast to "
335: 				                      "expected type %s for this column",
336: 				                      col_idx, type_pair.second, expected_types_p[col_idx].ToString());
337: 			} else {
338: 				throw FormatException("schema mismatch in Parquet glob: column \"%d\" in parquet file is of type "
339: 				                      "%s, but in the original file \"%s\" this column is of type \"%s\"",
340: 				                      col_idx, type_pair.second, initial_filename_p,
341: 				                      expected_types_p[col_idx].ToString());
342: 			}
343: 		} else {
344: 			names.push_back(type_pair.first);
345: 			return_types.push_back(type_pair.second);
346: 		}
347: 		col_idx++;
348: 	}
349: 	D_ASSERT(!names.empty());
350: 	D_ASSERT(!return_types.empty());
351: }
352: 
353: ParquetOptions::ParquetOptions(ClientContext &context) {
354: 	Value binary_as_string_val;
355: 	if (context.TryGetCurrentSetting("binary_as_string", binary_as_string_val)) {
356: 		binary_as_string = binary_as_string_val.GetValue<bool>();
357: 	}
358: }
359: 
360: ParquetReader::ParquetReader(Allocator &allocator_p, unique_ptr<FileHandle> file_handle_p,
361:                              const vector<LogicalType> &expected_types_p, const string &initial_filename_p)
362:     : allocator(allocator_p) {
363: 	file_name = file_handle_p->path;
364: 	file_handle = move(file_handle_p);
365: 	metadata = LoadMetadata(allocator, *file_handle);
366: 	InitializeSchema(expected_types_p, initial_filename_p);
367: }
368: 
369: ParquetReader::ParquetReader(ClientContext &context_p, string file_name_p, const vector<LogicalType> &expected_types_p,
370:                              ParquetOptions parquet_options_p, const string &initial_filename_p)
371:     : allocator(Allocator::Get(context_p)), file_opener(FileSystem::GetFileOpener(context_p)),
372:       parquet_options(parquet_options_p) {
373: 	auto &fs = FileSystem::GetFileSystem(context_p);
374: 	file_name = move(file_name_p);
375: 	file_handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,
376: 	                          FileSystem::DEFAULT_COMPRESSION, file_opener);
377: 	// If object cached is disabled
378: 	// or if this file has cached metadata
379: 	// or if the cached version already expired
380: 	auto last_modify_time = fs.GetLastModifiedTime(*file_handle);
381: 	if (!ObjectCache::ObjectCacheEnabled(context_p)) {
382: 		metadata = LoadMetadata(allocator, *file_handle);
383: 	} else {
384: 		metadata = ObjectCache::GetObjectCache(context_p).Get<ParquetFileMetadataCache>(file_name);
385: 		if (!metadata || (last_modify_time + 10 >= metadata->read_time)) {
386: 			metadata = LoadMetadata(allocator, *file_handle);
387: 			ObjectCache::GetObjectCache(context_p).Put(file_name, metadata);
388: 		}
389: 	}
390: 	InitializeSchema(expected_types_p, initial_filename_p);
391: }
392: 
393: ParquetReader::~ParquetReader() {
394: }
395: 
396: const FileMetaData *ParquetReader::GetFileMetadata() {
397: 	D_ASSERT(metadata);
398: 	D_ASSERT(metadata->metadata);
399: 	return metadata->metadata.get();
400: }
401: 
402: // TODO also somewhat ugly, perhaps this can be moved to the column reader too
403: unique_ptr<BaseStatistics> ParquetReader::ReadStatistics(ParquetReader &reader, LogicalType &type,
404:                                                          column_t file_col_idx, const FileMetaData *file_meta_data) {
405: 	unique_ptr<BaseStatistics> column_stats;
406: 	auto root_reader = reader.CreateReader(file_meta_data);
407: 	auto column_reader = ((StructColumnReader *)root_reader.get())->GetChildReader(file_col_idx);
408: 
409: 	for (auto &row_group : file_meta_data->row_groups) {
410: 		auto chunk_stats = column_reader->Stats(row_group.columns);
411: 		if (!chunk_stats) {
412: 			return nullptr;
413: 		}
414: 		if (!column_stats) {
415: 			column_stats = move(chunk_stats);
416: 		} else {
417: 			column_stats->Merge(*chunk_stats);
418: 		}
419: 	}
420: 	return column_stats;
421: }
422: 
423: const ParquetRowGroup &ParquetReader::GetGroup(ParquetReaderScanState &state) {
424: 	auto file_meta_data = GetFileMetadata();
425: 	D_ASSERT(state.current_group >= 0 && (idx_t)state.current_group < state.group_idx_list.size());
426: 	D_ASSERT(state.group_idx_list[state.current_group] >= 0 &&
427: 	         state.group_idx_list[state.current_group] < file_meta_data->row_groups.size());
428: 	return file_meta_data->row_groups[state.group_idx_list[state.current_group]];
429: }
430: 
431: void ParquetReader::PrepareRowGroupBuffer(ParquetReaderScanState &state, idx_t out_col_idx) {
432: 	auto &group = GetGroup(state);
433: 
434: 	auto column_reader = ((StructColumnReader *)state.root_reader.get())->GetChildReader(state.column_ids[out_col_idx]);
435: 
436: 	// TODO move this to columnreader too
437: 	if (state.filters) {
438: 		auto stats = column_reader->Stats(group.columns);
439: 		// filters contain output chunk index, not file col idx!
440: 		auto filter_entry = state.filters->filters.find(out_col_idx);
441: 		if (stats && filter_entry != state.filters->filters.end()) {
442: 			bool skip_chunk = false;
443: 			auto &filter = *filter_entry->second;
444: 			auto prune_result = filter.CheckStatistics(*stats);
445: 			if (prune_result == FilterPropagateResult::FILTER_ALWAYS_FALSE) {
446: 				skip_chunk = true;
447: 			}
448: 			if (skip_chunk) {
449: 				state.group_offset = group.num_rows;
450: 				return;
451: 				// this effectively will skip this chunk
452: 			}
453: 		}
454: 	}
455: 
456: 	state.root_reader->InitializeRead(group.columns, *state.thrift_file_proto);
457: }
458: 
459: idx_t ParquetReader::NumRows() {
460: 	return GetFileMetadata()->num_rows;
461: }
462: 
463: idx_t ParquetReader::NumRowGroups() {
464: 	return GetFileMetadata()->row_groups.size();
465: }
466: 
467: void ParquetReader::InitializeScan(ParquetReaderScanState &state, vector<column_t> column_ids,
468:                                    vector<idx_t> groups_to_read, TableFilterSet *filters) {
469: 	state.current_group = -1;
470: 	state.finished = false;
471: 	state.column_ids = move(column_ids);
472: 	state.group_offset = 0;
473: 	state.group_idx_list = move(groups_to_read);
474: 	state.filters = filters;
475: 	state.sel.Initialize(STANDARD_VECTOR_SIZE);
476: 	state.file_handle =
477: 	    file_handle->file_system.OpenFile(file_handle->path, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,
478: 	                                      FileSystem::DEFAULT_COMPRESSION, file_opener);
479: 	state.thrift_file_proto = CreateThriftProtocol(allocator, *state.file_handle);
480: 	state.root_reader = CreateReader(GetFileMetadata());
481: 
482: 	state.define_buf.resize(allocator, STANDARD_VECTOR_SIZE);
483: 	state.repeat_buf.resize(allocator, STANDARD_VECTOR_SIZE);
484: }
485: 
486: void FilterIsNull(Vector &v, parquet_filter_t &filter_mask, idx_t count) {
487: 	auto &mask = FlatVector::Validity(v);
488: 	if (mask.AllValid()) {
489: 		filter_mask.reset();
490: 	} else {
491: 		for (idx_t i = 0; i < count; i++) {
492: 			filter_mask[i] = filter_mask[i] && !mask.RowIsValid(i);
493: 		}
494: 	}
495: }
496: 
497: void FilterIsNotNull(Vector &v, parquet_filter_t &filter_mask, idx_t count) {
498: 	auto &mask = FlatVector::Validity(v);
499: 	if (!mask.AllValid()) {
500: 		for (idx_t i = 0; i < count; i++) {
501: 			filter_mask[i] = filter_mask[i] && mask.RowIsValid(i);
502: 		}
503: 	}
504: }
505: 
506: template <class T, class OP>
507: void TemplatedFilterOperation(Vector &v, T constant, parquet_filter_t &filter_mask, idx_t count) {
508: 	D_ASSERT(v.GetVectorType() == VectorType::FLAT_VECTOR); // we just created the damn thing it better be
509: 
510: 	auto v_ptr = FlatVector::GetData<T>(v);
511: 	auto &mask = FlatVector::Validity(v);
512: 
513: 	if (!mask.AllValid()) {
514: 		for (idx_t i = 0; i < count; i++) {
515: 			if (mask.RowIsValid(i)) {
516: 				filter_mask[i] = filter_mask[i] && OP::Operation(v_ptr[i], constant);
517: 			}
518: 		}
519: 	} else {
520: 		for (idx_t i = 0; i < count; i++) {
521: 			filter_mask[i] = filter_mask[i] && OP::Operation(v_ptr[i], constant);
522: 		}
523: 	}
524: }
525: 
526: template <class OP>
527: static void FilterOperationSwitch(Vector &v, Value &constant, parquet_filter_t &filter_mask, idx_t count) {
528: 	if (filter_mask.none() || count == 0) {
529: 		return;
530: 	}
531: 	switch (v.GetType().id()) {
532: 	case LogicalTypeId::BOOLEAN:
533: 		TemplatedFilterOperation<bool, OP>(v, constant.value_.boolean, filter_mask, count);
534: 		break;
535: 	case LogicalTypeId::UTINYINT:
536: 		TemplatedFilterOperation<uint8_t, OP>(v, constant.value_.utinyint, filter_mask, count);
537: 		break;
538: 	case LogicalTypeId::USMALLINT:
539: 		TemplatedFilterOperation<uint16_t, OP>(v, constant.value_.usmallint, filter_mask, count);
540: 		break;
541: 	case LogicalTypeId::UINTEGER:
542: 		TemplatedFilterOperation<uint32_t, OP>(v, constant.value_.uinteger, filter_mask, count);
543: 		break;
544: 	case LogicalTypeId::UBIGINT:
545: 		TemplatedFilterOperation<uint64_t, OP>(v, constant.value_.ubigint, filter_mask, count);
546: 		break;
547: 	case LogicalTypeId::TINYINT:
548: 		TemplatedFilterOperation<int8_t, OP>(v, constant.value_.tinyint, filter_mask, count);
549: 		break;
550: 	case LogicalTypeId::SMALLINT:
551: 		TemplatedFilterOperation<int16_t, OP>(v, constant.value_.smallint, filter_mask, count);
552: 		break;
553: 	case LogicalTypeId::INTEGER:
554: 		TemplatedFilterOperation<int32_t, OP>(v, constant.value_.integer, filter_mask, count);
555: 		break;
556: 	case LogicalTypeId::BIGINT:
557: 		TemplatedFilterOperation<int64_t, OP>(v, constant.value_.bigint, filter_mask, count);
558: 		break;
559: 	case LogicalTypeId::FLOAT:
560: 		TemplatedFilterOperation<float, OP>(v, constant.value_.float_, filter_mask, count);
561: 		break;
562: 	case LogicalTypeId::DOUBLE:
563: 		TemplatedFilterOperation<double, OP>(v, constant.value_.double_, filter_mask, count);
564: 		break;
565: 	case LogicalTypeId::DATE:
566: 		TemplatedFilterOperation<date_t, OP>(v, constant.value_.date, filter_mask, count);
567: 		break;
568: 	case LogicalTypeId::TIMESTAMP:
569: 		TemplatedFilterOperation<timestamp_t, OP>(v, constant.value_.timestamp, filter_mask, count);
570: 		break;
571: 	case LogicalTypeId::BLOB:
572: 	case LogicalTypeId::VARCHAR:
573: 		TemplatedFilterOperation<string_t, OP>(v, string_t(constant.str_value), filter_mask, count);
574: 		break;
575: 	case LogicalTypeId::DECIMAL:
576: 		switch (v.GetType().InternalType()) {
577: 		case PhysicalType::INT16:
578: 			TemplatedFilterOperation<int16_t, OP>(v, constant.value_.smallint, filter_mask, count);
579: 			break;
580: 		case PhysicalType::INT32:
581: 			TemplatedFilterOperation<int32_t, OP>(v, constant.value_.integer, filter_mask, count);
582: 			break;
583: 		case PhysicalType::INT64:
584: 			TemplatedFilterOperation<int64_t, OP>(v, constant.value_.bigint, filter_mask, count);
585: 			break;
586: 		case PhysicalType::INT128:
587: 			TemplatedFilterOperation<hugeint_t, OP>(v, constant.value_.hugeint, filter_mask, count);
588: 			break;
589: 		default:
590: 			throw InternalException("Unsupported internal type for decimal");
591: 		}
592: 		break;
593: 	default:
594: 		throw NotImplementedException("Unsupported type for filter %s", v.ToString());
595: 	}
596: }
597: 
598: static void ApplyFilter(Vector &v, TableFilter &filter, parquet_filter_t &filter_mask, idx_t count) {
599: 	switch (filter.filter_type) {
600: 	case TableFilterType::CONJUNCTION_AND: {
601: 		auto &conjunction = (ConjunctionAndFilter &)filter;
602: 		for (auto &child_filter : conjunction.child_filters) {
603: 			ApplyFilter(v, *child_filter, filter_mask, count);
604: 		}
605: 		break;
606: 	}
607: 	case TableFilterType::CONJUNCTION_OR: {
608: 		auto &conjunction = (ConjunctionOrFilter &)filter;
609: 		for (auto &child_filter : conjunction.child_filters) {
610: 			parquet_filter_t child_mask = filter_mask;
611: 			ApplyFilter(v, *child_filter, child_mask, count);
612: 			filter_mask |= child_mask;
613: 		}
614: 		break;
615: 	}
616: 	case TableFilterType::CONSTANT_COMPARISON: {
617: 		auto &constant_filter = (ConstantFilter &)filter;
618: 		switch (constant_filter.comparison_type) {
619: 		case ExpressionType::COMPARE_EQUAL:
620: 			FilterOperationSwitch<Equals>(v, constant_filter.constant, filter_mask, count);
621: 			break;
622: 		case ExpressionType::COMPARE_LESSTHAN:
623: 			FilterOperationSwitch<LessThan>(v, constant_filter.constant, filter_mask, count);
624: 			break;
625: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
626: 			FilterOperationSwitch<LessThanEquals>(v, constant_filter.constant, filter_mask, count);
627: 			break;
628: 		case ExpressionType::COMPARE_GREATERTHAN:
629: 			FilterOperationSwitch<GreaterThan>(v, constant_filter.constant, filter_mask, count);
630: 			break;
631: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
632: 			FilterOperationSwitch<GreaterThanEquals>(v, constant_filter.constant, filter_mask, count);
633: 			break;
634: 		default:
635: 			D_ASSERT(0);
636: 		}
637: 		break;
638: 	}
639: 	case TableFilterType::IS_NOT_NULL:
640: 		FilterIsNotNull(v, filter_mask, count);
641: 		break;
642: 	case TableFilterType::IS_NULL:
643: 		FilterIsNull(v, filter_mask, count);
644: 		break;
645: 	default:
646: 		D_ASSERT(0);
647: 		break;
648: 	}
649: }
650: 
651: void ParquetReader::Scan(ParquetReaderScanState &state, DataChunk &result) {
652: 	while (ScanInternal(state, result)) {
653: 		if (result.size() > 0) {
654: 			break;
655: 		}
656: 		result.Reset();
657: 	}
658: }
659: 
660: bool ParquetReader::ScanInternal(ParquetReaderScanState &state, DataChunk &result) {
661: 	if (state.finished) {
662: 		return false;
663: 	}
664: 
665: 	// see if we have to switch to the next row group in the parquet file
666: 	if (state.current_group < 0 || (int64_t)state.group_offset >= GetGroup(state).num_rows) {
667: 		state.current_group++;
668: 		state.group_offset = 0;
669: 
670: 		if ((idx_t)state.current_group == state.group_idx_list.size()) {
671: 			state.finished = true;
672: 			return false;
673: 		}
674: 
675: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
676: 			// this is a special case where we are not interested in the actual contents of the file
677: 			if (state.column_ids[out_col_idx] == COLUMN_IDENTIFIER_ROW_ID) {
678: 				continue;
679: 			}
680: 
681: 			PrepareRowGroupBuffer(state, out_col_idx);
682: 		}
683: 		return true;
684: 	}
685: 
686: 	auto this_output_chunk_rows = MinValue<idx_t>(STANDARD_VECTOR_SIZE, GetGroup(state).num_rows - state.group_offset);
687: 	result.SetCardinality(this_output_chunk_rows);
688: 
689: 	if (this_output_chunk_rows == 0) {
690: 		state.finished = true;
691: 		return false; // end of last group, we are done
692: 	}
693: 
694: 	// we evaluate simple table filters directly in this scan so we can skip decoding column data that's never going to
695: 	// be relevant
696: 	parquet_filter_t filter_mask;
697: 	filter_mask.set();
698: 
699: 	state.define_buf.zero();
700: 	state.repeat_buf.zero();
701: 
702: 	auto define_ptr = (uint8_t *)state.define_buf.ptr;
703: 	auto repeat_ptr = (uint8_t *)state.repeat_buf.ptr;
704: 
705: 	auto root_reader = ((StructColumnReader *)state.root_reader.get());
706: 
707: 	if (state.filters) {
708: 		vector<bool> need_to_read(result.ColumnCount(), true);
709: 
710: 		// first load the columns that are used in filters
711: 		for (auto &filter_col : state.filters->filters) {
712: 			auto file_col_idx = state.column_ids[filter_col.first];
713: 
714: 			if (filter_mask.none()) { // if no rows are left we can stop checking filters
715: 				break;
716: 			}
717: 
718: 			root_reader->GetChildReader(file_col_idx)
719: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[filter_col.first]);
720: 
721: 			need_to_read[filter_col.first] = false;
722: 
723: 			ApplyFilter(result.data[filter_col.first], *filter_col.second, filter_mask, this_output_chunk_rows);
724: 		}
725: 
726: 		// we still may have to read some cols
727: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
728: 			if (!need_to_read[out_col_idx]) {
729: 				continue;
730: 			}
731: 			auto file_col_idx = state.column_ids[out_col_idx];
732: 
733: 			if (filter_mask.none()) {
734: 				root_reader->GetChildReader(file_col_idx)->Skip(result.size());
735: 				continue;
736: 			}
737: 			// TODO handle ROWID here, too
738: 			root_reader->GetChildReader(file_col_idx)
739: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);
740: 		}
741: 
742: 		idx_t sel_size = 0;
743: 		for (idx_t i = 0; i < this_output_chunk_rows; i++) {
744: 			if (filter_mask[i]) {
745: 				state.sel.set_index(sel_size++, i);
746: 			}
747: 		}
748: 
749: 		result.Slice(state.sel, sel_size);
750: 		result.Verify();
751: 
752: 	} else { // #nofilter, just fricking load the data
753: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
754: 			auto file_col_idx = state.column_ids[out_col_idx];
755: 
756: 			if (file_col_idx == COLUMN_IDENTIFIER_ROW_ID) {
757: 				Value constant_42 = Value::BIGINT(42);
758: 				result.data[out_col_idx].Reference(constant_42);
759: 				continue;
760: 			}
761: 
762: 			root_reader->GetChildReader(file_col_idx)
763: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);
764: 		}
765: 	}
766: 
767: 	state.group_offset += this_output_chunk_rows;
768: 	return true;
769: }
770: 
771: } // namespace duckdb
[end of extension/parquet/parquet_reader.cpp]
[start of src/common/types.cpp]
1: #include "duckdb/common/types.hpp"
2: 
3: #include "duckdb/common/exception.hpp"
4: #include "duckdb/common/serializer.hpp"
5: #include "duckdb/common/string_util.hpp"
6: #include "duckdb/common/types/decimal.hpp"
7: #include "duckdb/common/types/hash.hpp"
8: #include "duckdb/common/types/string_type.hpp"
9: 
10: #include "duckdb/common/limits.hpp"
11: #include "duckdb/common/types/value.hpp"
12: 
13: #include <cmath>
14: #include "duckdb/common/unordered_map.hpp"
15: #include "duckdb/catalog/catalog_entry/type_catalog_entry.hpp"
16: #include "duckdb/common/types/vector.hpp"
17: #include "duckdb/common/operator/comparison_operators.hpp"
18: 
19: namespace duckdb {
20: 
21: LogicalType::LogicalType() : LogicalType(LogicalTypeId::INVALID) {
22: }
23: 
24: LogicalType::LogicalType(LogicalTypeId id) : id_(id) {
25: 	physical_type_ = GetInternalType();
26: }
27: LogicalType::LogicalType(LogicalTypeId id, shared_ptr<ExtraTypeInfo> type_info_p)
28:     : id_(id), type_info_(move(type_info_p)) {
29: 	physical_type_ = GetInternalType();
30: }
31: 
32: LogicalType::LogicalType(const LogicalType &other)
33:     : id_(other.id_), physical_type_(other.physical_type_), type_info_(other.type_info_) {
34: }
35: 
36: LogicalType::LogicalType(LogicalType &&other) noexcept
37:     : id_(other.id_), physical_type_(other.physical_type_), type_info_(move(other.type_info_)) {
38: }
39: 
40: hash_t LogicalType::Hash() const {
41: 	return duckdb::Hash<uint8_t>((uint8_t)id_);
42: }
43: 
44: PhysicalType LogicalType::GetInternalType() {
45: 	switch (id_) {
46: 	case LogicalTypeId::BOOLEAN:
47: 		return PhysicalType::BOOL;
48: 	case LogicalTypeId::TINYINT:
49: 		return PhysicalType::INT8;
50: 	case LogicalTypeId::UTINYINT:
51: 		return PhysicalType::UINT8;
52: 	case LogicalTypeId::SMALLINT:
53: 		return PhysicalType::INT16;
54: 	case LogicalTypeId::USMALLINT:
55: 		return PhysicalType::UINT16;
56: 	case LogicalTypeId::SQLNULL:
57: 	case LogicalTypeId::DATE:
58: 	case LogicalTypeId::DATE_TZ:
59: 	case LogicalTypeId::INTEGER:
60: 		return PhysicalType::INT32;
61: 	case LogicalTypeId::UINTEGER:
62: 		return PhysicalType::UINT32;
63: 	case LogicalTypeId::BIGINT:
64: 	case LogicalTypeId::TIME:
65: 	case LogicalTypeId::TIMESTAMP:
66: 	case LogicalTypeId::TIMESTAMP_SEC:
67: 	case LogicalTypeId::TIMESTAMP_NS:
68: 	case LogicalTypeId::TIMESTAMP_MS:
69: 	case LogicalTypeId::TIME_TZ:
70: 	case LogicalTypeId::TIMESTAMP_TZ:
71: 		return PhysicalType::INT64;
72: 	case LogicalTypeId::UBIGINT:
73: 		return PhysicalType::UINT64;
74: 	case LogicalTypeId::HUGEINT:
75: 	case LogicalTypeId::UUID:
76: 		return PhysicalType::INT128;
77: 	case LogicalTypeId::FLOAT:
78: 		return PhysicalType::FLOAT;
79: 	case LogicalTypeId::DOUBLE:
80: 		return PhysicalType::DOUBLE;
81: 	case LogicalTypeId::DECIMAL: {
82: 		if (!type_info_) {
83: 			return PhysicalType::INVALID;
84: 		}
85: 		auto width = DecimalType::GetWidth(*this);
86: 		if (width <= Decimal::MAX_WIDTH_INT16) {
87: 			return PhysicalType::INT16;
88: 		} else if (width <= Decimal::MAX_WIDTH_INT32) {
89: 			return PhysicalType::INT32;
90: 		} else if (width <= Decimal::MAX_WIDTH_INT64) {
91: 			return PhysicalType::INT64;
92: 		} else if (width <= Decimal::MAX_WIDTH_INT128) {
93: 			return PhysicalType::INT128;
94: 		} else {
95: 			throw InternalException("Widths bigger than 38 are not supported");
96: 		}
97: 	}
98: 	case LogicalTypeId::VARCHAR:
99: 	case LogicalTypeId::CHAR:
100: 	case LogicalTypeId::BLOB:
101: 		return PhysicalType::VARCHAR;
102: 	case LogicalTypeId::INTERVAL:
103: 		return PhysicalType::INTERVAL;
104: 	case LogicalTypeId::MAP:
105: 	case LogicalTypeId::STRUCT:
106: 		return PhysicalType::STRUCT;
107: 	case LogicalTypeId::LIST:
108: 		return PhysicalType::LIST;
109: 	case LogicalTypeId::HASH:
110: 		static_assert(sizeof(hash_t) == sizeof(uint64_t), "Hash must be uint64_t");
111: 		return PhysicalType::UINT64;
112: 	case LogicalTypeId::POINTER:
113: 		// LCOV_EXCL_START
114: 		if (sizeof(uintptr_t) == sizeof(uint32_t)) {
115: 			return PhysicalType::UINT32;
116: 		} else if (sizeof(uintptr_t) == sizeof(uint64_t)) {
117: 			return PhysicalType::UINT64;
118: 		} else {
119: 			throw InternalException("Unsupported pointer size");
120: 		}
121: 		// LCOV_EXCL_STOP
122: 	case LogicalTypeId::VALIDITY:
123: 		return PhysicalType::BIT;
124: 	case LogicalTypeId::ENUM: {
125: 		D_ASSERT(type_info_);
126: 		auto size = EnumType::GetSize(*this);
127: 		return EnumType::GetPhysicalType(size);
128: 	}
129: 	case LogicalTypeId::TABLE:
130: 	case LogicalTypeId::ANY:
131: 	case LogicalTypeId::INVALID:
132: 	case LogicalTypeId::UNKNOWN:
133: 		return PhysicalType::INVALID;
134: 	case LogicalTypeId::USER:
135: 		return PhysicalType::UNKNOWN;
136: 	default:
137: 		throw InternalException("Invalid LogicalType %s", ToString());
138: 	}
139: }
140: 
141: constexpr const LogicalTypeId LogicalType::INVALID;
142: constexpr const LogicalTypeId LogicalType::SQLNULL;
143: constexpr const LogicalTypeId LogicalType::BOOLEAN;
144: constexpr const LogicalTypeId LogicalType::TINYINT;
145: constexpr const LogicalTypeId LogicalType::UTINYINT;
146: constexpr const LogicalTypeId LogicalType::SMALLINT;
147: constexpr const LogicalTypeId LogicalType::USMALLINT;
148: constexpr const LogicalTypeId LogicalType::INTEGER;
149: constexpr const LogicalTypeId LogicalType::UINTEGER;
150: constexpr const LogicalTypeId LogicalType::BIGINT;
151: constexpr const LogicalTypeId LogicalType::UBIGINT;
152: constexpr const LogicalTypeId LogicalType::HUGEINT;
153: constexpr const LogicalTypeId LogicalType::UUID;
154: constexpr const LogicalTypeId LogicalType::FLOAT;
155: constexpr const LogicalTypeId LogicalType::DOUBLE;
156: constexpr const LogicalTypeId LogicalType::DATE;
157: 
158: constexpr const LogicalTypeId LogicalType::TIMESTAMP;
159: constexpr const LogicalTypeId LogicalType::TIMESTAMP_MS;
160: constexpr const LogicalTypeId LogicalType::TIMESTAMP_NS;
161: constexpr const LogicalTypeId LogicalType::TIMESTAMP_S;
162: 
163: constexpr const LogicalTypeId LogicalType::TIME;
164: 
165: constexpr const LogicalTypeId LogicalType::DATE_TZ;
166: constexpr const LogicalTypeId LogicalType::TIME_TZ;
167: constexpr const LogicalTypeId LogicalType::TIMESTAMP_TZ;
168: 
169: constexpr const LogicalTypeId LogicalType::HASH;
170: constexpr const LogicalTypeId LogicalType::POINTER;
171: 
172: constexpr const LogicalTypeId LogicalType::VARCHAR;
173: 
174: constexpr const LogicalTypeId LogicalType::BLOB;
175: constexpr const LogicalTypeId LogicalType::INTERVAL;
176: constexpr const LogicalTypeId LogicalType::ROW_TYPE;
177: 
178: // TODO these are incomplete and should maybe not exist as such
179: constexpr const LogicalTypeId LogicalType::TABLE;
180: 
181: constexpr const LogicalTypeId LogicalType::ANY;
182: 
183: const vector<LogicalType> LogicalType::Numeric() {
184: 	vector<LogicalType> types = {LogicalType::TINYINT,   LogicalType::SMALLINT,  LogicalType::INTEGER,
185: 	                             LogicalType::BIGINT,    LogicalType::HUGEINT,   LogicalType::FLOAT,
186: 	                             LogicalType::DOUBLE,    LogicalTypeId::DECIMAL, LogicalType::UTINYINT,
187: 	                             LogicalType::USMALLINT, LogicalType::UINTEGER,  LogicalType::UBIGINT};
188: 	return types;
189: }
190: 
191: const vector<LogicalType> LogicalType::Integral() {
192: 	vector<LogicalType> types = {LogicalType::TINYINT,   LogicalType::SMALLINT, LogicalType::INTEGER,
193: 	                             LogicalType::BIGINT,    LogicalType::HUGEINT,  LogicalType::UTINYINT,
194: 	                             LogicalType::USMALLINT, LogicalType::UINTEGER, LogicalType::UBIGINT};
195: 	return types;
196: }
197: 
198: const vector<LogicalType> LogicalType::AllTypes() {
199: 	vector<LogicalType> types = {
200: 	    LogicalType::BOOLEAN,  LogicalType::TINYINT,   LogicalType::SMALLINT,  LogicalType::INTEGER,
201: 	    LogicalType::BIGINT,   LogicalType::DATE,      LogicalType::TIMESTAMP, LogicalType::DOUBLE,
202: 	    LogicalType::FLOAT,    LogicalType::VARCHAR,   LogicalType::BLOB,      LogicalType::INTERVAL,
203: 	    LogicalType::HUGEINT,  LogicalTypeId::DECIMAL, LogicalType::UTINYINT,  LogicalType::USMALLINT,
204: 	    LogicalType::UINTEGER, LogicalType::UBIGINT,   LogicalType::TIME,      LogicalTypeId::LIST,
205: 	    LogicalTypeId::STRUCT, LogicalType::DATE_TZ,   LogicalType::TIME_TZ,   LogicalType::TIMESTAMP_TZ,
206: 	    LogicalTypeId::MAP,    LogicalType::UUID};
207: 	return types;
208: }
209: 
210: const PhysicalType ROW_TYPE = PhysicalType::INT64;
211: 
212: // LCOV_EXCL_START
213: string TypeIdToString(PhysicalType type) {
214: 	switch (type) {
215: 	case PhysicalType::BOOL:
216: 		return "BOOL";
217: 	case PhysicalType::INT8:
218: 		return "INT8";
219: 	case PhysicalType::INT16:
220: 		return "INT16";
221: 	case PhysicalType::INT32:
222: 		return "INT32";
223: 	case PhysicalType::INT64:
224: 		return "INT64";
225: 	case PhysicalType::UINT8:
226: 		return "UINT8";
227: 	case PhysicalType::UINT16:
228: 		return "UINT16";
229: 	case PhysicalType::UINT32:
230: 		return "UINT32";
231: 	case PhysicalType::UINT64:
232: 		return "UINT64";
233: 	case PhysicalType::INT128:
234: 		return "INT128";
235: 	case PhysicalType::FLOAT:
236: 		return "FLOAT";
237: 	case PhysicalType::DOUBLE:
238: 		return "DOUBLE";
239: 	case PhysicalType::VARCHAR:
240: 		return "VARCHAR";
241: 	case PhysicalType::INTERVAL:
242: 		return "INTERVAL";
243: 	case PhysicalType::STRUCT:
244: 		return "STRUCT<?>";
245: 	case PhysicalType::LIST:
246: 		return "LIST<?>";
247: 	case PhysicalType::INVALID:
248: 		return "INVALID";
249: 	case PhysicalType::BIT:
250: 		return "BIT";
251: 	case PhysicalType::NA:
252: 		return "NA";
253: 	case PhysicalType::HALF_FLOAT:
254: 		return "HALF_FLOAT";
255: 	case PhysicalType::STRING:
256: 		return "ARROW_STRING";
257: 	case PhysicalType::BINARY:
258: 		return "BINARY";
259: 	case PhysicalType::FIXED_SIZE_BINARY:
260: 		return "FIXED_SIZE_BINARY";
261: 	case PhysicalType::DATE32:
262: 		return "DATE32";
263: 	case PhysicalType::DATE64:
264: 		return "DATE64";
265: 	case PhysicalType::TIMESTAMP:
266: 		return "TIMESTAMP";
267: 	case PhysicalType::TIME32:
268: 		return "TIME32";
269: 	case PhysicalType::TIME64:
270: 		return "TIME64";
271: 	case PhysicalType::UNION:
272: 		return "UNION";
273: 	case PhysicalType::DICTIONARY:
274: 		return "DICTIONARY";
275: 	case PhysicalType::MAP:
276: 		return "MAP";
277: 	case PhysicalType::EXTENSION:
278: 		return "EXTENSION";
279: 	case PhysicalType::FIXED_SIZE_LIST:
280: 		return "FIXED_SIZE_LIST";
281: 	case PhysicalType::DURATION:
282: 		return "DURATION";
283: 	case PhysicalType::LARGE_STRING:
284: 		return "LARGE_STRING";
285: 	case PhysicalType::LARGE_BINARY:
286: 		return "LARGE_BINARY";
287: 	case PhysicalType::LARGE_LIST:
288: 		return "LARGE_LIST";
289: 	case PhysicalType::UNKNOWN:
290: 		return "UNKNOWN";
291: 	}
292: 	return "INVALID";
293: }
294: // LCOV_EXCL_STOP
295: 
296: idx_t GetTypeIdSize(PhysicalType type) {
297: 	switch (type) {
298: 	case PhysicalType::BIT:
299: 	case PhysicalType::BOOL:
300: 		return sizeof(bool);
301: 	case PhysicalType::INT8:
302: 		return sizeof(int8_t);
303: 	case PhysicalType::INT16:
304: 		return sizeof(int16_t);
305: 	case PhysicalType::INT32:
306: 		return sizeof(int32_t);
307: 	case PhysicalType::INT64:
308: 		return sizeof(int64_t);
309: 	case PhysicalType::UINT8:
310: 		return sizeof(uint8_t);
311: 	case PhysicalType::UINT16:
312: 		return sizeof(uint16_t);
313: 	case PhysicalType::UINT32:
314: 		return sizeof(uint32_t);
315: 	case PhysicalType::UINT64:
316: 		return sizeof(uint64_t);
317: 	case PhysicalType::INT128:
318: 		return sizeof(hugeint_t);
319: 	case PhysicalType::FLOAT:
320: 		return sizeof(float);
321: 	case PhysicalType::DOUBLE:
322: 		return sizeof(double);
323: 	case PhysicalType::VARCHAR:
324: 		return sizeof(string_t);
325: 	case PhysicalType::INTERVAL:
326: 		return sizeof(interval_t);
327: 	case PhysicalType::STRUCT:
328: 	case PhysicalType::UNKNOWN:
329: 		return 0; // no own payload
330: 	case PhysicalType::LIST:
331: 		return sizeof(list_entry_t); // offset + len
332: 	default:
333: 		throw InternalException("Invalid PhysicalType for GetTypeIdSize");
334: 	}
335: }
336: 
337: bool TypeIsConstantSize(PhysicalType type) {
338: 	return (type >= PhysicalType::BOOL && type <= PhysicalType::DOUBLE) ||
339: 	       (type >= PhysicalType::FIXED_SIZE_BINARY && type <= PhysicalType::INTERVAL) ||
340: 	       type == PhysicalType::INTERVAL || type == PhysicalType::INT128;
341: }
342: bool TypeIsIntegral(PhysicalType type) {
343: 	return (type >= PhysicalType::UINT8 && type <= PhysicalType::INT64) || type == PhysicalType::INT128;
344: }
345: bool TypeIsNumeric(PhysicalType type) {
346: 	return (type >= PhysicalType::UINT8 && type <= PhysicalType::DOUBLE) || type == PhysicalType::INT128;
347: }
348: bool TypeIsInteger(PhysicalType type) {
349: 	return (type >= PhysicalType::UINT8 && type <= PhysicalType::INT64) || type == PhysicalType::INT128;
350: }
351: 
352: // LCOV_EXCL_START
353: string LogicalTypeIdToString(LogicalTypeId id) {
354: 	switch (id) {
355: 	case LogicalTypeId::BOOLEAN:
356: 		return "BOOLEAN";
357: 	case LogicalTypeId::TINYINT:
358: 		return "TINYINT";
359: 	case LogicalTypeId::SMALLINT:
360: 		return "SMALLINT";
361: 	case LogicalTypeId::INTEGER:
362: 		return "INTEGER";
363: 	case LogicalTypeId::BIGINT:
364: 		return "BIGINT";
365: 	case LogicalTypeId::HUGEINT:
366: 		return "HUGEINT";
367: 	case LogicalTypeId::UUID:
368: 		return "UUID";
369: 	case LogicalTypeId::UTINYINT:
370: 		return "UTINYINT";
371: 	case LogicalTypeId::USMALLINT:
372: 		return "USMALLINT";
373: 	case LogicalTypeId::UINTEGER:
374: 		return "UINTEGER";
375: 	case LogicalTypeId::UBIGINT:
376: 		return "UBIGINT";
377: 	case LogicalTypeId::DATE:
378: 		return "DATE";
379: 	case LogicalTypeId::TIME:
380: 		return "TIME";
381: 	case LogicalTypeId::TIMESTAMP:
382: 		return "TIMESTAMP";
383: 	case LogicalTypeId::TIMESTAMP_MS:
384: 		return "TIMESTAMP (MS)";
385: 	case LogicalTypeId::TIMESTAMP_NS:
386: 		return "TIMESTAMP (NS)";
387: 	case LogicalTypeId::TIMESTAMP_SEC:
388: 		return "TIMESTAMP (SEC)";
389: 	case LogicalTypeId::TIMESTAMP_TZ:
390: 		return "TIMESTAMP WITH TIME ZONE";
391: 	case LogicalTypeId::TIME_TZ:
392: 		return "TIME WITH TIME ZONE";
393: 	case LogicalTypeId::DATE_TZ:
394: 		return "DATE WITH TIME ZONE";
395: 	case LogicalTypeId::FLOAT:
396: 		return "FLOAT";
397: 	case LogicalTypeId::DOUBLE:
398: 		return "DOUBLE";
399: 	case LogicalTypeId::DECIMAL:
400: 		return "DECIMAL";
401: 	case LogicalTypeId::VARCHAR:
402: 		return "VARCHAR";
403: 	case LogicalTypeId::BLOB:
404: 		return "BLOB";
405: 	case LogicalTypeId::CHAR:
406: 		return "CHAR";
407: 	case LogicalTypeId::INTERVAL:
408: 		return "INTERVAL";
409: 	case LogicalTypeId::SQLNULL:
410: 		return "NULL";
411: 	case LogicalTypeId::ANY:
412: 		return "ANY";
413: 	case LogicalTypeId::VALIDITY:
414: 		return "VALIDITY";
415: 	case LogicalTypeId::STRUCT:
416: 		return "STRUCT<?>";
417: 	case LogicalTypeId::LIST:
418: 		return "LIST<?>";
419: 	case LogicalTypeId::MAP:
420: 		return "MAP<?>";
421: 	case LogicalTypeId::HASH:
422: 		return "HASH";
423: 	case LogicalTypeId::POINTER:
424: 		return "POINTER";
425: 	case LogicalTypeId::TABLE:
426: 		return "TABLE";
427: 	case LogicalTypeId::INVALID:
428: 		return "INVALID";
429: 	case LogicalTypeId::UNKNOWN:
430: 		return "UNKNOWN";
431: 	case LogicalTypeId::ENUM:
432: 		return "ENUM";
433: 	case LogicalTypeId::USER:
434: 		return "USER";
435: 	}
436: 	return "UNDEFINED";
437: }
438: 
439: string LogicalType::ToString() const {
440: 	switch (id_) {
441: 	case LogicalTypeId::STRUCT: {
442: 		if (!type_info_) {
443: 			return "STRUCT";
444: 		}
445: 		auto &child_types = StructType::GetChildTypes(*this);
446: 		string ret = "STRUCT<";
447: 		for (size_t i = 0; i < child_types.size(); i++) {
448: 			ret += child_types[i].first + ": " + child_types[i].second.ToString();
449: 			if (i < child_types.size() - 1) {
450: 				ret += ", ";
451: 			}
452: 		}
453: 		ret += ">";
454: 		return ret;
455: 	}
456: 	case LogicalTypeId::LIST: {
457: 		if (!type_info_) {
458: 			return "LIST";
459: 		}
460: 		return "LIST<" + ListType::GetChildType(*this).ToString() + ">";
461: 	}
462: 	case LogicalTypeId::MAP: {
463: 		if (!type_info_) {
464: 			return "MAP";
465: 		}
466: 		auto &child_types = StructType::GetChildTypes(*this);
467: 		if (child_types.empty()) {
468: 			return "MAP<?>";
469: 		}
470: 		if (child_types.size() != 2) {
471: 			throw InternalException("Map needs exactly two child elements");
472: 		}
473: 		return "MAP<" + ListType::GetChildType(child_types[0].second).ToString() + ", " +
474: 		       ListType::GetChildType(child_types[1].second).ToString() + ">";
475: 	}
476: 	case LogicalTypeId::DECIMAL: {
477: 		if (!type_info_) {
478: 			return "DECIMAL";
479: 		}
480: 		auto width = DecimalType::GetWidth(*this);
481: 		auto scale = DecimalType::GetScale(*this);
482: 		if (width == 0) {
483: 			return "DECIMAL";
484: 		}
485: 		return StringUtil::Format("DECIMAL(%d,%d)", width, scale);
486: 	}
487: 	case LogicalTypeId::ENUM: {
488: 		return EnumType::GetTypeName(*this);
489: 	}
490: 	default:
491: 		return LogicalTypeIdToString(id_);
492: 	}
493: }
494: // LCOV_EXCL_STOP
495: 
496: LogicalTypeId TransformStringToLogicalType(const string &str) {
497: 	auto lower_str = StringUtil::Lower(str);
498: 	// Transform column type
499: 	if (lower_str == "int" || lower_str == "int4" || lower_str == "signed" || lower_str == "integer" ||
500: 	    lower_str == "integral" || lower_str == "int32") {
501: 		return LogicalTypeId::INTEGER;
502: 	} else if (lower_str == "varchar" || lower_str == "bpchar" || lower_str == "text" || lower_str == "string" ||
503: 	           lower_str == "char" || lower_str == "nvarchar") {
504: 		return LogicalTypeId::VARCHAR;
505: 	} else if (lower_str == "bytea" || lower_str == "blob" || lower_str == "varbinary" || lower_str == "binary") {
506: 		return LogicalTypeId::BLOB;
507: 	} else if (lower_str == "int8" || lower_str == "bigint" || lower_str == "int64" || lower_str == "long" ||
508: 	           lower_str == "oid") {
509: 		return LogicalTypeId::BIGINT;
510: 	} else if (lower_str == "int2" || lower_str == "smallint" || lower_str == "short" || lower_str == "int16") {
511: 		return LogicalTypeId::SMALLINT;
512: 	} else if (lower_str == "timestamp" || lower_str == "datetime" || lower_str == "timestamp_us") {
513: 		return LogicalTypeId::TIMESTAMP;
514: 	} else if (lower_str == "timestamp_ms") {
515: 		return LogicalTypeId::TIMESTAMP_MS;
516: 	} else if (lower_str == "timestamp_ns") {
517: 		return LogicalTypeId::TIMESTAMP_NS;
518: 	} else if (lower_str == "timestamp_s") {
519: 		return LogicalTypeId::TIMESTAMP_SEC;
520: 	} else if (lower_str == "bool" || lower_str == "boolean" || lower_str == "logical") {
521: 		return LogicalTypeId::BOOLEAN;
522: 	} else if (lower_str == "decimal" || lower_str == "dec" || lower_str == "numeric") {
523: 		return LogicalTypeId::DECIMAL;
524: 	} else if (lower_str == "real" || lower_str == "float4" || lower_str == "float") {
525: 		return LogicalTypeId::FLOAT;
526: 	} else if (lower_str == "double" || lower_str == "float8") {
527: 		return LogicalTypeId::DOUBLE;
528: 	} else if (lower_str == "tinyint" || lower_str == "int1") {
529: 		return LogicalTypeId::TINYINT;
530: 	} else if (lower_str == "date") {
531: 		return LogicalTypeId::DATE;
532: 	} else if (lower_str == "time") {
533: 		return LogicalTypeId::TIME;
534: 	} else if (lower_str == "interval") {
535: 		return LogicalTypeId::INTERVAL;
536: 	} else if (lower_str == "hugeint" || lower_str == "int128") {
537: 		return LogicalTypeId::HUGEINT;
538: 	} else if (lower_str == "uuid" || lower_str == "guid") {
539: 		return LogicalTypeId::UUID;
540: 	} else if (lower_str == "struct" || lower_str == "row") {
541: 		return LogicalTypeId::STRUCT;
542: 	} else if (lower_str == "map") {
543: 		return LogicalTypeId::MAP;
544: 	} else if (lower_str == "utinyint" || lower_str == "uint8") {
545: 		return LogicalTypeId::UTINYINT;
546: 	} else if (lower_str == "usmallint" || lower_str == "uint16") {
547: 		return LogicalTypeId::USMALLINT;
548: 	} else if (lower_str == "uinteger" || lower_str == "uint32") {
549: 		return LogicalTypeId::UINTEGER;
550: 	} else if (lower_str == "ubigint" || lower_str == "uint64") {
551: 		return LogicalTypeId::UBIGINT;
552: 	} else if (lower_str == "timestamptz") {
553: 		return LogicalTypeId::TIMESTAMP_TZ;
554: 	} else if (lower_str == "timetz") {
555: 		return LogicalTypeId::TIME_TZ;
556: 	} else if (lower_str == "datetz") {
557: 		return LogicalTypeId::DATE_TZ;
558: 	} else {
559: 		// This is a User Type, at this point we don't know if its one of the User Defined Types or an error
560: 		// It is checked in the binder
561: 		return LogicalTypeId::USER;
562: 	}
563: }
564: 
565: bool LogicalType::IsIntegral() const {
566: 	switch (id_) {
567: 	case LogicalTypeId::TINYINT:
568: 	case LogicalTypeId::SMALLINT:
569: 	case LogicalTypeId::INTEGER:
570: 	case LogicalTypeId::BIGINT:
571: 	case LogicalTypeId::UTINYINT:
572: 	case LogicalTypeId::USMALLINT:
573: 	case LogicalTypeId::UINTEGER:
574: 	case LogicalTypeId::UBIGINT:
575: 	case LogicalTypeId::HUGEINT:
576: 		return true;
577: 	default:
578: 		return false;
579: 	}
580: }
581: 
582: bool LogicalType::IsNumeric() const {
583: 	switch (id_) {
584: 	case LogicalTypeId::TINYINT:
585: 	case LogicalTypeId::SMALLINT:
586: 	case LogicalTypeId::INTEGER:
587: 	case LogicalTypeId::BIGINT:
588: 	case LogicalTypeId::HUGEINT:
589: 	case LogicalTypeId::FLOAT:
590: 	case LogicalTypeId::DOUBLE:
591: 	case LogicalTypeId::DECIMAL:
592: 	case LogicalTypeId::UTINYINT:
593: 	case LogicalTypeId::USMALLINT:
594: 	case LogicalTypeId::UINTEGER:
595: 	case LogicalTypeId::UBIGINT:
596: 		return true;
597: 	default:
598: 		return false;
599: 	}
600: }
601: 
602: bool LogicalType::GetDecimalProperties(uint8_t &width, uint8_t &scale) const {
603: 	switch (id_) {
604: 	case LogicalTypeId::SQLNULL:
605: 		width = 0;
606: 		scale = 0;
607: 		break;
608: 	case LogicalTypeId::BOOLEAN:
609: 		width = 1;
610: 		scale = 0;
611: 		break;
612: 	case LogicalTypeId::TINYINT:
613: 		// tinyint: [-127, 127] = DECIMAL(3,0)
614: 		width = 3;
615: 		scale = 0;
616: 		break;
617: 	case LogicalTypeId::SMALLINT:
618: 		// smallint: [-32767, 32767] = DECIMAL(5,0)
619: 		width = 5;
620: 		scale = 0;
621: 		break;
622: 	case LogicalTypeId::INTEGER:
623: 		// integer: [-2147483647, 2147483647] = DECIMAL(10,0)
624: 		width = 10;
625: 		scale = 0;
626: 		break;
627: 	case LogicalTypeId::BIGINT:
628: 		// bigint: [-9223372036854775807, 9223372036854775807] = DECIMAL(19,0)
629: 		width = 19;
630: 		scale = 0;
631: 		break;
632: 	case LogicalTypeId::UTINYINT:
633: 		// UInt8  [0 : 255]
634: 		width = 3;
635: 		scale = 0;
636: 		break;
637: 	case LogicalTypeId::USMALLINT:
638: 		// UInt16  [0 : 65535]
639: 		width = 5;
640: 		scale = 0;
641: 		break;
642: 	case LogicalTypeId::UINTEGER:
643: 		// UInt32  [0 : 4294967295]
644: 		width = 10;
645: 		scale = 0;
646: 		break;
647: 	case LogicalTypeId::UBIGINT:
648: 		// UInt64  [0 : 18446744073709551615]
649: 		width = 20;
650: 		scale = 0;
651: 		break;
652: 	case LogicalTypeId::HUGEINT:
653: 		// hugeint: max size decimal (38, 0)
654: 		// note that a hugeint is not guaranteed to fit in this
655: 		width = 38;
656: 		scale = 0;
657: 		break;
658: 	case LogicalTypeId::DECIMAL:
659: 		width = DecimalType::GetWidth(*this);
660: 		scale = DecimalType::GetScale(*this);
661: 		break;
662: 	default:
663: 		return false;
664: 	}
665: 	return true;
666: }
667: 
668: LogicalType LogicalType::MaxLogicalType(const LogicalType &left, const LogicalType &right) {
669: 	if (left.id() < right.id()) {
670: 		return right;
671: 	} else if (right.id() < left.id()) {
672: 		return left;
673: 	} else {
674: 		// Since both left and right are equal we get the left type as our type_id for checks
675: 		auto type_id = left.id();
676: 		if (type_id == LogicalTypeId::ENUM) {
677: 			// If both types are different ENUMs we do a string comparison.
678: 			return left == right ? left : LogicalType::VARCHAR;
679: 		}
680: 		if (type_id == LogicalTypeId::VARCHAR) {
681: 			// varchar: use type that has collation (if any)
682: 			if (StringType::GetCollation(right).empty()) {
683: 				return left;
684: 			} else {
685: 				return right;
686: 			}
687: 		} else if (type_id == LogicalTypeId::DECIMAL) {
688: 			// use max width/scale of the two types
689: 			auto width = MaxValue<uint8_t>(DecimalType::GetWidth(left), DecimalType::GetWidth(right));
690: 			auto scale = MaxValue<uint8_t>(DecimalType::GetScale(left), DecimalType::GetScale(right));
691: 			return LogicalType::DECIMAL(width, scale);
692: 		} else if (type_id == LogicalTypeId::LIST) {
693: 			// list: perform max recursively on child type
694: 			auto new_child = MaxLogicalType(ListType::GetChildType(left), ListType::GetChildType(right));
695: 			return LogicalType::LIST(move(new_child));
696: 		} else if (type_id == LogicalTypeId::STRUCT) {
697: 			// struct: perform recursively
698: 			auto &left_child_types = StructType::GetChildTypes(left);
699: 			auto &right_child_types = StructType::GetChildTypes(right);
700: 			if (left_child_types.size() != right_child_types.size()) {
701: 				// child types are not of equal size, we can't cast anyway
702: 				// just return the left child
703: 				return left;
704: 			}
705: 			child_list_t<LogicalType> child_types;
706: 			for (idx_t i = 0; i < left_child_types.size(); i++) {
707: 				auto child_type = MaxLogicalType(left_child_types[i].second, right_child_types[i].second);
708: 				child_types.push_back(make_pair(left_child_types[i].first, move(child_type)));
709: 			}
710: 			return LogicalType::STRUCT(move(child_types));
711: 		} else {
712: 			// types are equal but no extra specifier: just return the type
713: 			return left;
714: 		}
715: 	}
716: }
717: 
718: void LogicalType::Verify() const {
719: #ifdef DEBUG
720: 	if (id_ == LogicalTypeId::DECIMAL) {
721: 		D_ASSERT(DecimalType::GetWidth(*this) >= 1 && DecimalType::GetWidth(*this) <= Decimal::MAX_WIDTH_DECIMAL);
722: 		D_ASSERT(DecimalType::GetScale(*this) >= 0 && DecimalType::GetScale(*this) <= DecimalType::GetWidth(*this));
723: 	}
724: #endif
725: }
726: 
727: bool ApproxEqual(float ldecimal, float rdecimal) {
728: 	float epsilon = std::fabs(rdecimal) * 0.01;
729: 	return std::fabs(ldecimal - rdecimal) <= epsilon;
730: }
731: 
732: bool ApproxEqual(double ldecimal, double rdecimal) {
733: 	double epsilon = std::fabs(rdecimal) * 0.01;
734: 	return std::fabs(ldecimal - rdecimal) <= epsilon;
735: }
736: 
737: //===--------------------------------------------------------------------===//
738: // Extra Type Info
739: //===--------------------------------------------------------------------===//
740: enum class ExtraTypeInfoType : uint8_t {
741: 	INVALID_TYPE_INFO = 0,
742: 	DECIMAL_TYPE_INFO = 1,
743: 	STRING_TYPE_INFO = 2,
744: 	LIST_TYPE_INFO = 3,
745: 	STRUCT_TYPE_INFO = 4,
746: 	ENUM_TYPE_INFO = 5,
747: 	USER_TYPE_INFO = 6
748: };
749: 
750: struct ExtraTypeInfo {
751: 	explicit ExtraTypeInfo(ExtraTypeInfoType type) : type(type) {
752: 	}
753: 	virtual ~ExtraTypeInfo() {
754: 	}
755: 
756: 	ExtraTypeInfoType type;
757: 
758: public:
759: 	virtual bool Equals(ExtraTypeInfo *other) = 0;
760: 	//! Serializes a ExtraTypeInfo to a stand-alone binary blob
761: 	virtual void Serialize(Serializer &serializer) = 0;
762: 	//! Serializes a ExtraTypeInfo to a stand-alone binary blob
763: 	static void Serialize(ExtraTypeInfo *info, Serializer &serializer);
764: 	//! Deserializes a blob back into an ExtraTypeInfo
765: 	static shared_ptr<ExtraTypeInfo> Deserialize(Deserializer &source);
766: };
767: 
768: //===--------------------------------------------------------------------===//
769: // Decimal Type
770: //===--------------------------------------------------------------------===//
771: struct DecimalTypeInfo : public ExtraTypeInfo {
772: 	DecimalTypeInfo(uint8_t width_p, uint8_t scale_p)
773: 	    : ExtraTypeInfo(ExtraTypeInfoType::DECIMAL_TYPE_INFO), width(width_p), scale(scale_p) {
774: 	}
775: 
776: 	uint8_t width;
777: 	uint8_t scale;
778: 
779: public:
780: 	bool Equals(ExtraTypeInfo *other_p) override {
781: 		if (!other_p) {
782: 			return false;
783: 		}
784: 		if (type != other_p->type) {
785: 			return false;
786: 		}
787: 		auto &other = (DecimalTypeInfo &)*other_p;
788: 		return width == other.width && scale == other.scale;
789: 	}
790: 
791: 	void Serialize(Serializer &serializer) override {
792: 		serializer.Write<uint8_t>(width);
793: 		serializer.Write<uint8_t>(scale);
794: 	}
795: 
796: 	static shared_ptr<ExtraTypeInfo> Deserialize(Deserializer &source) {
797: 		auto width = source.Read<uint8_t>();
798: 		auto scale = source.Read<uint8_t>();
799: 		return make_shared<DecimalTypeInfo>(width, scale);
800: 	}
801: };
802: 
803: uint8_t DecimalType::GetWidth(const LogicalType &type) {
804: 	D_ASSERT(type.id() == LogicalTypeId::DECIMAL);
805: 	auto info = type.AuxInfo();
806: 	D_ASSERT(info);
807: 	return ((DecimalTypeInfo &)*info).width;
808: }
809: 
810: uint8_t DecimalType::GetScale(const LogicalType &type) {
811: 	D_ASSERT(type.id() == LogicalTypeId::DECIMAL);
812: 	auto info = type.AuxInfo();
813: 	D_ASSERT(info);
814: 	return ((DecimalTypeInfo &)*info).scale;
815: }
816: 
817: LogicalType LogicalType::DECIMAL(int width, int scale) {
818: 	auto type_info = make_shared<DecimalTypeInfo>(width, scale);
819: 	return LogicalType(LogicalTypeId::DECIMAL, move(type_info));
820: }
821: 
822: //===--------------------------------------------------------------------===//
823: // String Type
824: //===--------------------------------------------------------------------===//
825: struct StringTypeInfo : public ExtraTypeInfo {
826: 	explicit StringTypeInfo(string collation_p)
827: 	    : ExtraTypeInfo(ExtraTypeInfoType::STRING_TYPE_INFO), collation(move(collation_p)) {
828: 	}
829: 
830: 	string collation;
831: 
832: public:
833: 	bool Equals(ExtraTypeInfo *other_p) override {
834: 		// collation info has no impact on equality
835: 		return true;
836: 	}
837: 
838: 	void Serialize(Serializer &serializer) override {
839: 		serializer.WriteString(collation);
840: 	}
841: 
842: 	static shared_ptr<ExtraTypeInfo> Deserialize(Deserializer &source) {
843: 		auto collation = source.Read<string>();
844: 		return make_shared<StringTypeInfo>(move(collation));
845: 	}
846: };
847: 
848: string StringType::GetCollation(const LogicalType &type) {
849: 	if (type.id() != LogicalTypeId::VARCHAR) {
850: 		return string();
851: 	}
852: 	auto info = type.AuxInfo();
853: 	if (!info) {
854: 		return string();
855: 	}
856: 	return ((StringTypeInfo &)*info).collation;
857: }
858: 
859: LogicalType LogicalType::VARCHAR_COLLATION(string collation) { // NOLINT
860: 	auto string_info = make_shared<StringTypeInfo>(move(collation));
861: 	return LogicalType(LogicalTypeId::VARCHAR, move(string_info));
862: }
863: 
864: //===--------------------------------------------------------------------===//
865: // List Type
866: //===--------------------------------------------------------------------===//
867: struct ListTypeInfo : public ExtraTypeInfo {
868: 	explicit ListTypeInfo(LogicalType child_type_p)
869: 	    : ExtraTypeInfo(ExtraTypeInfoType::LIST_TYPE_INFO), child_type(move(child_type_p)) {
870: 	}
871: 
872: 	LogicalType child_type;
873: 
874: public:
875: 	bool Equals(ExtraTypeInfo *other_p) override {
876: 		if (!other_p) {
877: 			return false;
878: 		}
879: 		if (type != other_p->type) {
880: 			return false;
881: 		}
882: 		auto &other = (ListTypeInfo &)*other_p;
883: 		return child_type == other.child_type;
884: 	}
885: 
886: 	void Serialize(Serializer &serializer) override {
887: 		child_type.Serialize(serializer);
888: 	}
889: 
890: 	static shared_ptr<ExtraTypeInfo> Deserialize(Deserializer &source) {
891: 		auto child_type = LogicalType::Deserialize(source);
892: 		return make_shared<ListTypeInfo>(move(child_type));
893: 	}
894: };
895: 
896: const LogicalType &ListType::GetChildType(const LogicalType &type) {
897: 	D_ASSERT(type.id() == LogicalTypeId::LIST);
898: 	auto info = type.AuxInfo();
899: 	D_ASSERT(info);
900: 	return ((ListTypeInfo &)*info).child_type;
901: }
902: 
903: LogicalType LogicalType::LIST(LogicalType child) {
904: 	auto info = make_shared<ListTypeInfo>(move(child));
905: 	return LogicalType(LogicalTypeId::LIST, move(info));
906: }
907: 
908: //===--------------------------------------------------------------------===//
909: // Struct Type
910: //===--------------------------------------------------------------------===//
911: struct StructTypeInfo : public ExtraTypeInfo {
912: 	explicit StructTypeInfo(child_list_t<LogicalType> child_types_p)
913: 	    : ExtraTypeInfo(ExtraTypeInfoType::STRUCT_TYPE_INFO), child_types(move(child_types_p)) {
914: 	}
915: 
916: 	child_list_t<LogicalType> child_types;
917: 
918: public:
919: 	bool Equals(ExtraTypeInfo *other_p) override {
920: 		if (!other_p) {
921: 			return false;
922: 		}
923: 		if (type != other_p->type) {
924: 			return false;
925: 		}
926: 		auto &other = (StructTypeInfo &)*other_p;
927: 		return child_types == other.child_types;
928: 	}
929: 
930: 	void Serialize(Serializer &serializer) override {
931: 		serializer.Write<uint32_t>(child_types.size());
932: 		for (idx_t i = 0; i < child_types.size(); i++) {
933: 			serializer.WriteString(child_types[i].first);
934: 			child_types[i].second.Serialize(serializer);
935: 		}
936: 	}
937: 
938: 	static shared_ptr<ExtraTypeInfo> Deserialize(Deserializer &source) {
939: 		child_list_t<LogicalType> child_list;
940: 		auto child_types_size = source.Read<uint32_t>();
941: 		for (uint32_t i = 0; i < child_types_size; i++) {
942: 			auto name = source.Read<string>();
943: 			auto type = LogicalType::Deserialize(source);
944: 			child_list.push_back(make_pair(move(name), move(type)));
945: 		}
946: 		return make_shared<StructTypeInfo>(move(child_list));
947: 	}
948: };
949: 
950: const child_list_t<LogicalType> &StructType::GetChildTypes(const LogicalType &type) {
951: 	D_ASSERT(type.id() == LogicalTypeId::STRUCT || type.id() == LogicalTypeId::MAP);
952: 	auto info = type.AuxInfo();
953: 	D_ASSERT(info);
954: 	return ((StructTypeInfo &)*info).child_types;
955: }
956: 
957: const LogicalType &StructType::GetChildType(const LogicalType &type, idx_t index) {
958: 	auto &child_types = StructType::GetChildTypes(type);
959: 	D_ASSERT(index < child_types.size());
960: 	return child_types[index].second;
961: }
962: 
963: const string &StructType::GetChildName(const LogicalType &type, idx_t index) {
964: 	auto &child_types = StructType::GetChildTypes(type);
965: 	D_ASSERT(index < child_types.size());
966: 	return child_types[index].first;
967: }
968: 
969: idx_t StructType::GetChildCount(const LogicalType &type) {
970: 	return StructType::GetChildTypes(type).size();
971: }
972: 
973: LogicalType LogicalType::STRUCT(child_list_t<LogicalType> children) {
974: 	auto info = make_shared<StructTypeInfo>(move(children));
975: 	return LogicalType(LogicalTypeId::STRUCT, move(info));
976: }
977: 
978: LogicalType LogicalType::MAP(child_list_t<LogicalType> children) {
979: 	auto info = make_shared<StructTypeInfo>(move(children));
980: 	return LogicalType(LogicalTypeId::MAP, move(info));
981: }
982: 
983: LogicalType LogicalType::MAP(LogicalType key, LogicalType value) {
984: 	child_list_t<LogicalType> child_types;
985: 	child_types.push_back({"key", LogicalType::LIST(move(key))});
986: 	child_types.push_back({"value", LogicalType::LIST(move(value))});
987: 	return LogicalType::MAP(move(child_types));
988: }
989: 
990: //===--------------------------------------------------------------------===//
991: // User Type
992: //===--------------------------------------------------------------------===//
993: struct UserTypeInfo : public ExtraTypeInfo {
994: 	explicit UserTypeInfo(string name_p)
995: 	    : ExtraTypeInfo(ExtraTypeInfoType::USER_TYPE_INFO), user_type_name(move(name_p)) {
996: 	}
997: 
998: 	string user_type_name;
999: 
1000: public:
1001: 	bool Equals(ExtraTypeInfo *other_p) override {
1002: 		if (!other_p) {
1003: 			return false;
1004: 		}
1005: 		if (type != other_p->type) {
1006: 			return false;
1007: 		}
1008: 		auto &other = (UserTypeInfo &)*other_p;
1009: 		return other.user_type_name == user_type_name;
1010: 	}
1011: 
1012: 	void Serialize(Serializer &serializer) override {
1013: 		serializer.WriteString(user_type_name);
1014: 	}
1015: 
1016: 	static shared_ptr<ExtraTypeInfo> Deserialize(Deserializer &source) {
1017: 		auto enum_name = source.Read<string>();
1018: 		return make_shared<UserTypeInfo>(move(enum_name));
1019: 	}
1020: };
1021: 
1022: const string &UserType::GetTypeName(const LogicalType &type) {
1023: 	D_ASSERT(type.id() == LogicalTypeId::USER);
1024: 	auto info = type.AuxInfo();
1025: 	D_ASSERT(info);
1026: 	return ((UserTypeInfo &)*info).user_type_name;
1027: }
1028: 
1029: LogicalType LogicalType::USER(const string &user_type_name) {
1030: 	auto info = make_shared<UserTypeInfo>(user_type_name);
1031: 	return LogicalType(LogicalTypeId::USER, move(info));
1032: }
1033: 
1034: //===--------------------------------------------------------------------===//
1035: // Enum Type
1036: //===--------------------------------------------------------------------===//
1037: struct EnumTypeInfo : public ExtraTypeInfo {
1038: 	explicit EnumTypeInfo(string enum_name_p, Vector &values_insert_order_p, idx_t size)
1039: 	    : ExtraTypeInfo(ExtraTypeInfoType::ENUM_TYPE_INFO), enum_name(move(enum_name_p)),
1040: 	      values_insert_order(values_insert_order_p), size(size) {
1041: 	}
1042: 	string enum_name;
1043: 	Vector values_insert_order;
1044: 	idx_t size;
1045: 	TypeCatalogEntry *catalog_entry = nullptr;
1046: 
1047: public:
1048: 	// Equalities are only used in enums with different catalog entries
1049: 	bool Equals(ExtraTypeInfo *other_p) override {
1050: 		if (!other_p) {
1051: 			return false;
1052: 		}
1053: 		if (type != other_p->type) {
1054: 			return false;
1055: 		}
1056: 		auto &other = (EnumTypeInfo &)*other_p;
1057: 
1058: 		// We must check if both enums have the same size
1059: 		if (other.size != size) {
1060: 			return false;
1061: 		}
1062: 		auto other_vector_ptr = FlatVector::GetData<string_t>(other.values_insert_order);
1063: 		auto this_vector_ptr = FlatVector::GetData<string_t>(values_insert_order);
1064: 
1065: 		// Now we must check if all strings are the same
1066: 		for (idx_t i = 0; i < size; i++) {
1067: 			if (!Equals::Operation(other_vector_ptr[i], this_vector_ptr[i])) {
1068: 				return false;
1069: 			}
1070: 		}
1071: 		return true;
1072: 	}
1073: 	void Serialize(Serializer &serializer) override {
1074: 		serializer.Write<uint32_t>(size);
1075: 		serializer.WriteString(enum_name);
1076: 		values_insert_order.Serialize(size, serializer);
1077: 	}
1078: };
1079: 
1080: template <class T>
1081: struct EnumTypeInfoTemplated : public EnumTypeInfo {
1082: 	explicit EnumTypeInfoTemplated(const string &enum_name_p, Vector &values_insert_order_p, idx_t size_p)
1083: 	    : EnumTypeInfo(enum_name_p, values_insert_order_p, size_p) {
1084: 		for (idx_t count = 0; count < size_p; count++) {
1085: 			values[values_insert_order_p.GetValue(count).ToString()] = count;
1086: 		}
1087: 	}
1088: 	static shared_ptr<EnumTypeInfoTemplated> Deserialize(Deserializer &source, uint32_t size) {
1089: 		auto enum_name = source.Read<string>();
1090: 		Vector values_insert_order(LogicalType::VARCHAR, size);
1091: 		values_insert_order.Deserialize(size, source);
1092: 		return make_shared<EnumTypeInfoTemplated>(move(enum_name), values_insert_order, size);
1093: 	}
1094: 	unordered_map<string, T> values;
1095: };
1096: 
1097: const string &EnumType::GetTypeName(const LogicalType &type) {
1098: 	D_ASSERT(type.id() == LogicalTypeId::ENUM);
1099: 	auto info = type.AuxInfo();
1100: 	D_ASSERT(info);
1101: 	return ((EnumTypeInfo &)*info).enum_name;
1102: }
1103: 
1104: LogicalType LogicalType::ENUM(const string &enum_name, Vector &ordered_data, idx_t size) {
1105: 	// Generate EnumTypeInfo
1106: 	shared_ptr<ExtraTypeInfo> info;
1107: 	auto enum_internal_type = EnumType::GetPhysicalType(size);
1108: 	switch (enum_internal_type) {
1109: 	case PhysicalType::UINT8:
1110: 		info = make_shared<EnumTypeInfoTemplated<uint8_t>>(enum_name, ordered_data, size);
1111: 		break;
1112: 	case PhysicalType::UINT16:
1113: 		info = make_shared<EnumTypeInfoTemplated<uint16_t>>(enum_name, ordered_data, size);
1114: 		break;
1115: 	case PhysicalType::UINT32:
1116: 		info = make_shared<EnumTypeInfoTemplated<uint32_t>>(enum_name, ordered_data, size);
1117: 		break;
1118: 	default:
1119: 		throw InternalException("Invalid Physical Type for ENUMs");
1120: 	}
1121: 	// Generate Actual Enum Type
1122: 	return LogicalType(LogicalTypeId::ENUM, info);
1123: }
1124: 
1125: template <class T>
1126: int64_t TemplatedGetPos(unordered_map<string, T> &map, const string &key) {
1127: 	auto it = map.find(key);
1128: 	if (it == map.end()) {
1129: 		return -1;
1130: 	}
1131: 	return it->second;
1132: }
1133: int64_t EnumType::GetPos(const LogicalType &type, const string &key) {
1134: 	auto info = type.AuxInfo();
1135: 	switch (type.InternalType()) {
1136: 	case PhysicalType::UINT8:
1137: 		return TemplatedGetPos(((EnumTypeInfoTemplated<uint8_t> &)*info).values, key);
1138: 	case PhysicalType::UINT16:
1139: 		return TemplatedGetPos(((EnumTypeInfoTemplated<uint16_t> &)*info).values, key);
1140: 	case PhysicalType::UINT32:
1141: 		return TemplatedGetPos(((EnumTypeInfoTemplated<uint32_t> &)*info).values, key);
1142: 	default:
1143: 		throw InternalException("ENUM can only have unsigned integers (except UINT64) as physical types");
1144: 	}
1145: }
1146: 
1147: const string EnumType::GetValue(const Value &val) {
1148: 	auto info = val.type().AuxInfo();
1149: 	auto &values_insert_order = ((EnumTypeInfo &)*info).values_insert_order;
1150: 	switch (val.type().InternalType()) {
1151: 	case PhysicalType::UINT8:
1152: 		return values_insert_order.GetValue(val.value_.utinyint).ToString();
1153: 	case PhysicalType::UINT16:
1154: 		return values_insert_order.GetValue(val.value_.usmallint).ToString();
1155: 	case PhysicalType::UINT32:
1156: 		return values_insert_order.GetValue(val.value_.uinteger).ToString();
1157: 	default:
1158: 		throw InternalException("Invalid Internal Type for ENUMs");
1159: 	}
1160: }
1161: 
1162: Vector &EnumType::GetValuesInsertOrder(const LogicalType &type) {
1163: 	D_ASSERT(type.id() == LogicalTypeId::ENUM);
1164: 	auto info = type.AuxInfo();
1165: 	D_ASSERT(info);
1166: 	return ((EnumTypeInfo &)*info).values_insert_order;
1167: }
1168: 
1169: idx_t EnumType::GetSize(const LogicalType &type) {
1170: 	D_ASSERT(type.id() == LogicalTypeId::ENUM);
1171: 	auto info = type.AuxInfo();
1172: 	D_ASSERT(info);
1173: 	return ((EnumTypeInfo &)*info).size;
1174: }
1175: 
1176: void EnumType::SetCatalog(LogicalType &type, TypeCatalogEntry *catalog_entry) {
1177: 	D_ASSERT(type.id() == LogicalTypeId::ENUM);
1178: 	auto info = type.AuxInfo();
1179: 	D_ASSERT(info);
1180: 	((EnumTypeInfo &)*info).catalog_entry = catalog_entry;
1181: }
1182: TypeCatalogEntry *EnumType::GetCatalog(const LogicalType &type) {
1183: 	D_ASSERT(type.id() == LogicalTypeId::ENUM);
1184: 	auto info = type.AuxInfo();
1185: 	D_ASSERT(info);
1186: 	return ((EnumTypeInfo &)*info).catalog_entry;
1187: }
1188: 
1189: PhysicalType EnumType::GetPhysicalType(idx_t size) {
1190: 	if (size <= NumericLimits<uint8_t>::Maximum()) {
1191: 		return PhysicalType::UINT8;
1192: 	} else if (size <= NumericLimits<uint16_t>::Maximum()) {
1193: 		return PhysicalType::UINT16;
1194: 	} else if (size <= NumericLimits<uint32_t>::Maximum()) {
1195: 		return PhysicalType::UINT32;
1196: 	} else {
1197: 		throw InternalException("Enum size must be lower than " + std::to_string(NumericLimits<uint32_t>::Maximum()));
1198: 	}
1199: }
1200: 
1201: //===--------------------------------------------------------------------===//
1202: // Extra Type Info
1203: //===--------------------------------------------------------------------===//
1204: void ExtraTypeInfo::Serialize(ExtraTypeInfo *info, Serializer &serializer) {
1205: 	if (!info) {
1206: 		serializer.Write<ExtraTypeInfoType>(ExtraTypeInfoType::INVALID_TYPE_INFO);
1207: 	} else {
1208: 		serializer.Write<ExtraTypeInfoType>(info->type);
1209: 		info->Serialize(serializer);
1210: 	}
1211: }
1212: shared_ptr<ExtraTypeInfo> ExtraTypeInfo::Deserialize(Deserializer &source) {
1213: 	auto type = source.Read<ExtraTypeInfoType>();
1214: 	switch (type) {
1215: 	case ExtraTypeInfoType::INVALID_TYPE_INFO:
1216: 		return nullptr;
1217: 	case ExtraTypeInfoType::DECIMAL_TYPE_INFO:
1218: 		return DecimalTypeInfo::Deserialize(source);
1219: 	case ExtraTypeInfoType::STRING_TYPE_INFO:
1220: 		return StringTypeInfo::Deserialize(source);
1221: 	case ExtraTypeInfoType::LIST_TYPE_INFO:
1222: 		return ListTypeInfo::Deserialize(source);
1223: 	case ExtraTypeInfoType::STRUCT_TYPE_INFO:
1224: 		return StructTypeInfo::Deserialize(source);
1225: 	case ExtraTypeInfoType::USER_TYPE_INFO:
1226: 		return UserTypeInfo::Deserialize(source);
1227: 	case ExtraTypeInfoType::ENUM_TYPE_INFO: {
1228: 		auto enum_size = source.Read<uint32_t>();
1229: 		auto enum_internal_type = EnumType::GetPhysicalType(enum_size);
1230: 		switch (enum_internal_type) {
1231: 		case PhysicalType::UINT8:
1232: 			return EnumTypeInfoTemplated<uint8_t>::Deserialize(source, enum_size);
1233: 		case PhysicalType::UINT16:
1234: 			return EnumTypeInfoTemplated<uint16_t>::Deserialize(source, enum_size);
1235: 		case PhysicalType::UINT32:
1236: 			return EnumTypeInfoTemplated<uint32_t>::Deserialize(source, enum_size);
1237: 		default:
1238: 			throw InternalException("Invalid Physical Type for ENUMs");
1239: 		}
1240: 	}
1241: 	default:
1242: 		throw InternalException("Unimplemented type info in ExtraTypeInfo::Deserialize");
1243: 	}
1244: }
1245: 
1246: //===--------------------------------------------------------------------===//
1247: // Logical Type
1248: //===--------------------------------------------------------------------===//
1249: 
1250: // the destructor needs to know about the extra type info
1251: LogicalType::~LogicalType() {
1252: }
1253: 
1254: void LogicalType::Serialize(Serializer &serializer) const {
1255: 	serializer.Write<LogicalTypeId>(id_);
1256: 	ExtraTypeInfo::Serialize(type_info_.get(), serializer);
1257: }
1258: 
1259: LogicalType LogicalType::Deserialize(Deserializer &source) {
1260: 	auto id = source.Read<LogicalTypeId>();
1261: 	auto info = ExtraTypeInfo::Deserialize(source);
1262: 	return LogicalType(id, move(info));
1263: }
1264: 
1265: bool LogicalType::operator==(const LogicalType &rhs) const {
1266: 	if (id_ != rhs.id_) {
1267: 		return false;
1268: 	}
1269: 	if (type_info_.get() == rhs.type_info_.get()) {
1270: 		return true;
1271: 	}
1272: 	if (type_info_) {
1273: 		return type_info_->Equals(rhs.type_info_.get());
1274: 	} else {
1275: 		D_ASSERT(rhs.type_info_);
1276: 		return rhs.type_info_->Equals(type_info_.get());
1277: 	}
1278: }
1279: 
1280: } // namespace duckdb
[end of src/common/types.cpp]
[start of src/include/duckdb/common/types.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/types.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/assert.hpp"
12: #include "duckdb/common/constants.hpp"
13: #include "duckdb/common/single_thread_ptr.hpp"
14: #include "duckdb/common/vector.hpp"
15: 
16: 
17: namespace duckdb {
18: 
19: class Serializer;
20: class Deserializer;
21: class Value;
22: class TypeCatalogEntry;
23: class Vector;
24: //! Type used to represent dates (days since 1970-01-01)
25: struct date_t {
26: 	int32_t days;
27: 
28: 	date_t() = default;
29: 	explicit inline date_t(int32_t days_p) : days(days_p) {}
30: 
31: 	// explicit conversion
32: 	explicit inline operator int32_t() const {return days;}
33: 
34: 	// comparison operators
35: 	inline bool operator==(const date_t &rhs) const {return days == rhs.days;};
36: 	inline bool operator!=(const date_t &rhs) const {return days != rhs.days;};
37: 	inline bool operator<=(const date_t &rhs) const {return days <= rhs.days;};
38: 	inline bool operator<(const date_t &rhs) const {return days < rhs.days;};
39: 	inline bool operator>(const date_t &rhs) const {return days > rhs.days;};
40: 	inline bool operator>=(const date_t &rhs) const {return days >= rhs.days;};
41: 
42: 	// arithmetic operators
43: 	inline date_t operator+(const int32_t &days) const {return date_t(this->days + days);};
44: 	inline date_t operator-(const int32_t &days) const {return date_t(this->days - days);};
45: 
46: 	// in-place operators
47: 	inline date_t &operator+=(const int32_t &days) {this->days += days; return *this;};
48: 	inline date_t &operator-=(const int32_t &days) {this->days -= days; return *this;};
49: };
50: 
51: //! Type used to represent time (microseconds)
52: struct dtime_t {
53:     int64_t micros;
54: 
55: 	dtime_t() = default;
56: 	explicit inline dtime_t(int64_t micros_p) : micros(micros_p) {}
57: 	inline dtime_t& operator=(int64_t micros_p) {micros = micros_p; return *this;}
58: 
59: 	// explicit conversion
60: 	explicit inline operator int64_t() const {return micros;}
61: 	explicit inline operator double() const {return micros;}
62: 
63: 	// comparison operators
64: 	inline bool operator==(const dtime_t &rhs) const {return micros == rhs.micros;};
65: 	inline bool operator!=(const dtime_t &rhs) const {return micros != rhs.micros;};
66: 	inline bool operator<=(const dtime_t &rhs) const {return micros <= rhs.micros;};
67: 	inline bool operator<(const dtime_t &rhs) const {return micros < rhs.micros;};
68: 	inline bool operator>(const dtime_t &rhs) const {return micros > rhs.micros;};
69: 	inline bool operator>=(const dtime_t &rhs) const {return micros >= rhs.micros;};
70: 
71: 	// arithmetic operators
72: 	inline dtime_t operator+(const int64_t &micros) const {return dtime_t(this->micros + micros);};
73: 	inline dtime_t operator+(const double &micros) const {return dtime_t(this->micros + int64_t(micros));};
74: 	inline dtime_t operator-(const int64_t &micros) const {return dtime_t(this->micros - micros);};
75: 	inline dtime_t operator*(const idx_t &copies) const {return dtime_t(this->micros * copies);};
76: 	inline dtime_t operator/(const idx_t &copies) const {return dtime_t(this->micros / copies);};
77: 	inline int64_t operator-(const dtime_t &other) const {return this->micros - other.micros;};
78: 
79: 	// in-place operators
80: 	inline dtime_t &operator+=(const int64_t &micros) {this->micros += micros; return *this;};
81: 	inline dtime_t &operator-=(const int64_t &micros) {this->micros -= micros; return *this;};
82: 	inline dtime_t &operator+=(const dtime_t &other) {this->micros += other.micros; return *this;};
83: };
84: 
85: //! Type used to represent timestamps (seconds,microseconds,milliseconds or nanoseconds since 1970-01-01)
86: struct timestamp_t {
87:     int64_t value;
88: 
89: 	timestamp_t() = default;
90: 	explicit inline timestamp_t(int64_t value_p) : value(value_p) {}
91: 	inline timestamp_t& operator=(int64_t value_p) {value = value_p; return *this;}
92: 
93: 	// explicit conversion
94: 	explicit inline operator int64_t() const {return value;}
95: 
96: 	// comparison operators
97: 	inline bool operator==(const timestamp_t &rhs) const {return value == rhs.value;};
98: 	inline bool operator!=(const timestamp_t &rhs) const {return value != rhs.value;};
99: 	inline bool operator<=(const timestamp_t &rhs) const {return value <= rhs.value;};
100: 	inline bool operator<(const timestamp_t &rhs) const {return value < rhs.value;};
101: 	inline bool operator>(const timestamp_t &rhs) const {return value > rhs.value;};
102: 	inline bool operator>=(const timestamp_t &rhs) const {return value >= rhs.value;};
103: 
104: 	// arithmetic operators
105: 	inline timestamp_t operator+(const double &value) const {return timestamp_t(this->value + int64_t(value));};
106: 	inline int64_t operator-(const timestamp_t &other) const {return this->value - other.value;};
107: 
108: 	// in-place operators
109: 	inline timestamp_t &operator+=(const int64_t &value) {this->value += value; return *this;};
110: 	inline timestamp_t &operator-=(const int64_t &value) {this->value -= value; return *this;};
111: };
112: 
113: struct interval_t {
114: 	int32_t months;
115: 	int32_t days;
116: 	int64_t micros;
117: 
118: 	inline bool operator==(const interval_t &rhs) const {
119: 		return this->days == rhs.days && this->months == rhs.months && this->micros == rhs.micros;
120: 	}
121: };
122: 
123: struct hugeint_t {
124: public:
125: 	uint64_t lower;
126: 	int64_t upper;
127: 
128: public:
129: 	hugeint_t() = default;
130: 	hugeint_t(int64_t value); // NOLINT: Allow implicit conversion from `int64_t`
131: 	hugeint_t(const hugeint_t &rhs) = default;
132: 	hugeint_t(hugeint_t &&rhs) = default;
133: 	hugeint_t &operator=(const hugeint_t &rhs) = default;
134: 	hugeint_t &operator=(hugeint_t &&rhs) = default;
135: 
136: 	string ToString() const;
137: 
138: 	// comparison operators
139: 	bool operator==(const hugeint_t &rhs) const;
140: 	bool operator!=(const hugeint_t &rhs) const;
141: 	bool operator<=(const hugeint_t &rhs) const;
142: 	bool operator<(const hugeint_t &rhs) const;
143: 	bool operator>(const hugeint_t &rhs) const;
144: 	bool operator>=(const hugeint_t &rhs) const;
145: 
146: 	// arithmetic operators
147: 	hugeint_t operator+(const hugeint_t &rhs) const;
148: 	hugeint_t operator-(const hugeint_t &rhs) const;
149: 	hugeint_t operator*(const hugeint_t &rhs) const;
150: 	hugeint_t operator/(const hugeint_t &rhs) const;
151: 	hugeint_t operator%(const hugeint_t &rhs) const;
152: 	hugeint_t operator-() const;
153: 
154: 	// bitwise operators
155: 	hugeint_t operator>>(const hugeint_t &rhs) const;
156: 	hugeint_t operator<<(const hugeint_t &rhs) const;
157: 	hugeint_t operator&(const hugeint_t &rhs) const;
158: 	hugeint_t operator|(const hugeint_t &rhs) const;
159: 	hugeint_t operator^(const hugeint_t &rhs) const;
160: 	hugeint_t operator~() const;
161: 
162: 	// in-place operators
163: 	hugeint_t &operator+=(const hugeint_t &rhs);
164: 	hugeint_t &operator-=(const hugeint_t &rhs);
165: 	hugeint_t &operator*=(const hugeint_t &rhs);
166: 	hugeint_t &operator/=(const hugeint_t &rhs);
167: 	hugeint_t &operator%=(const hugeint_t &rhs);
168: 	hugeint_t &operator>>=(const hugeint_t &rhs);
169: 	hugeint_t &operator<<=(const hugeint_t &rhs);
170: 	hugeint_t &operator&=(const hugeint_t &rhs);
171: 	hugeint_t &operator|=(const hugeint_t &rhs);
172: 	hugeint_t &operator^=(const hugeint_t &rhs);
173: };
174: 
175: struct string_t;
176: 
177: template <class T>
178: using child_list_t = std::vector<std::pair<std::string, T>>;
179: // we should be using single_thread_ptr here but cross-thread access to ChunkCollections currently prohibits this.
180: template <class T>
181: using buffer_ptr = shared_ptr<T>;
182: 
183: template <class T, typename... Args>
184: buffer_ptr<T> make_buffer(Args &&...args) {
185: 	return make_shared<T>(std::forward<Args>(args)...);
186: }
187: 
188: struct list_entry_t {
189: 	list_entry_t() = default;
190: 	list_entry_t(uint64_t offset, uint64_t length) : offset(offset), length(length) {
191: 	}
192: 
193: 	uint64_t offset;
194: 	uint64_t length;
195: };
196: 
197: //===--------------------------------------------------------------------===//
198: // Internal Types
199: //===--------------------------------------------------------------------===//
200: 
201: // taken from arrow's type.h
202: enum class PhysicalType : uint8_t {
203: 	/// A NULL type having no physical storage
204: 	NA = 0,
205: 
206: 	/// Boolean as 8 bit "bool" value
207: 	BOOL = 1,
208: 
209: 	/// Unsigned 8-bit little-endian integer
210: 	UINT8 = 2,
211: 
212: 	/// Signed 8-bit little-endian integer
213: 	INT8 = 3,
214: 
215: 	/// Unsigned 16-bit little-endian integer
216: 	UINT16 = 4,
217: 
218: 	/// Signed 16-bit little-endian integer
219: 	INT16 = 5,
220: 
221: 	/// Unsigned 32-bit little-endian integer
222: 	UINT32 = 6,
223: 
224: 	/// Signed 32-bit little-endian integer
225: 	INT32 = 7,
226: 
227: 	/// Unsigned 64-bit little-endian integer
228: 	UINT64 = 8,
229: 
230: 	/// Signed 64-bit little-endian integer
231: 	INT64 = 9,
232: 
233: 	/// 2-byte floating point value
234: 	HALF_FLOAT = 10,
235: 
236: 	/// 4-byte floating point value
237: 	FLOAT = 11,
238: 
239: 	/// 8-byte floating point value
240: 	DOUBLE = 12,
241: 
242: 	/// UTF8 variable-length string as List<Char>
243: 	STRING = 13,
244: 
245: 	/// Variable-length bytes (no guarantee of UTF8-ness)
246: 	BINARY = 14,
247: 
248: 	/// Fixed-size binary. Each value occupies the same number of bytes
249: 	FIXED_SIZE_BINARY = 15,
250: 
251: 	/// int32_t days since the UNIX epoch
252: 	DATE32 = 16,
253: 
254: 	/// int64_t milliseconds since the UNIX epoch
255: 	DATE64 = 17,
256: 
257: 	/// Exact timestamp encoded with int64 since UNIX epoch
258: 	/// Default unit millisecond
259: 	TIMESTAMP = 18,
260: 
261: 	/// Time as signed 32-bit integer, representing either seconds or
262: 	/// milliseconds since midnight
263: 	TIME32 = 19,
264: 
265: 	/// Time as signed 64-bit integer, representing either microseconds or
266: 	/// nanoseconds since midnight
267: 	TIME64 = 20,
268: 
269: 	/// YEAR_MONTH or DAY_TIME interval in SQL style
270: 	INTERVAL = 21,
271: 
272: 	/// Precision- and scale-based decimal type. Storage type depends on the
273: 	/// parameters.
274: 	// DECIMAL = 22,
275: 
276: 	/// A list of some logical data type
277: 	LIST = 23,
278: 
279: 	/// Struct of logical types
280: 	STRUCT = 24,
281: 
282: 	/// Unions of logical types
283: 	UNION = 25,
284: 
285: 	/// Dictionary-encoded type, also called "categorical" or "factor"
286: 	/// in other programming languages. Holds the dictionary value
287: 	/// type but not the dictionary itself, which is part of the
288: 	/// ArrayData struct
289: 	DICTIONARY = 26,
290: 
291: 	/// Map, a repeated struct logical type
292: 	MAP = 27,
293: 
294: 	/// Custom data type, implemented by user
295: 	EXTENSION = 28,
296: 
297: 	/// Fixed size list of some logical type
298: 	FIXED_SIZE_LIST = 29,
299: 
300: 	/// Measure of elapsed time in either seconds, milliseconds, microseconds
301: 	/// or nanoseconds.
302: 	DURATION = 30,
303: 
304: 	/// Like STRING, but with 64-bit offsets
305: 	LARGE_STRING = 31,
306: 
307: 	/// Like BINARY, but with 64-bit offsets
308: 	LARGE_BINARY = 32,
309: 
310: 	/// Like LIST, but with 64-bit offsets
311: 	LARGE_LIST = 33,
312: 
313: 	/// DuckDB Extensions
314: 	VARCHAR = 200, // our own string representation, different from STRING and LARGE_STRING above
315: 	INT128 = 204, // 128-bit integers
316: 	UNKNOWN = 205, // Unknown physical type of user defined types
317: 	/// Boolean as 1 bit, LSB bit-packed ordering
318: 	BIT = 206,
319: 
320: 	INVALID = 255
321: };
322: 
323: //===--------------------------------------------------------------------===//
324: // SQL Types
325: //===--------------------------------------------------------------------===//
326: enum class LogicalTypeId : uint8_t {
327: 	INVALID = 0,
328: 	SQLNULL = 1, /* NULL type, used for constant NULL */
329: 	UNKNOWN = 2, /* unknown type, used for parameter expressions */
330: 	ANY = 3,     /* ANY type, used for functions that accept any type as parameter */
331: 	USER = 4, /* A User Defined Type (e.g., ENUMs before the binder) */
332: 	BOOLEAN = 10,
333: 	TINYINT = 11,
334: 	SMALLINT = 12,
335: 	INTEGER = 13,
336: 	BIGINT = 14,
337: 	DATE = 15,
338: 	TIME = 16,
339: 	TIMESTAMP_SEC = 17,
340: 	TIMESTAMP_MS = 18,
341: 	TIMESTAMP = 19, //! us
342: 	TIMESTAMP_NS = 20,
343: 	DECIMAL = 21,
344: 	FLOAT = 22,
345: 	DOUBLE = 23,
346: 	CHAR = 24,
347: 	VARCHAR = 25,
348: 	BLOB = 26,
349: 	INTERVAL = 27,
350: 	UTINYINT = 28,
351: 	USMALLINT = 29,
352: 	UINTEGER = 30,
353: 	UBIGINT = 31,
354: 	TIMESTAMP_TZ = 32,
355: 	DATE_TZ = 33,
356: 	TIME_TZ = 34,
357: 
358: 
359: 	HUGEINT = 50,
360: 	POINTER = 51,
361: 	HASH = 52,
362: 	VALIDITY = 53,
363: 	UUID = 54,
364: 
365: 	STRUCT = 100,
366: 	LIST = 101,
367: 	MAP = 102,
368: 	TABLE = 103,
369: 	ENUM = 104
370: };
371: 
372: struct ExtraTypeInfo;
373: 
374: struct LogicalType {
375: 	DUCKDB_API LogicalType();
376: 	DUCKDB_API LogicalType(LogicalTypeId id); // NOLINT: Allow implicit conversion from `LogicalTypeId`
377: 	DUCKDB_API LogicalType(LogicalTypeId id, shared_ptr<ExtraTypeInfo> type_info);
378: 	DUCKDB_API LogicalType(const LogicalType &other);
379: 	DUCKDB_API LogicalType(LogicalType &&other) noexcept;
380: 
381: 	DUCKDB_API ~LogicalType();
382: 
383: 	inline LogicalTypeId id() const {
384: 		return id_;
385: 	}
386: 	inline PhysicalType InternalType() const {
387: 		return physical_type_;
388: 	}
389: 	inline const ExtraTypeInfo *AuxInfo() const {
390: 		return type_info_.get();
391: 	}
392: 
393: 	// copy assignment
394: 	inline LogicalType& operator=(const LogicalType &other) {
395: 		id_ = other.id_;
396: 		physical_type_ = other.physical_type_;
397: 		type_info_ = other.type_info_;
398: 		return *this;
399: 	}
400: 	// move assignment
401: 	inline LogicalType& operator=(LogicalType&& other) {
402: 		id_ = other.id_;
403: 		physical_type_ = other.physical_type_;
404: 		type_info_ = move(other.type_info_);
405: 		return *this;
406: 	}
407: 
408: 	DUCKDB_API bool operator==(const LogicalType &rhs) const;
409: 	inline bool operator!=(const LogicalType &rhs) const {
410: 		return !(*this == rhs);
411: 	}
412: 
413: 	//! Serializes a LogicalType to a stand-alone binary blob
414: 	DUCKDB_API void Serialize(Serializer &serializer) const;
415: 	//! Deserializes a blob back into an LogicalType
416: 	DUCKDB_API static LogicalType Deserialize(Deserializer &source);
417: 
418: 	DUCKDB_API string ToString() const;
419: 	DUCKDB_API bool IsIntegral() const;
420: 	DUCKDB_API bool IsNumeric() const;
421: 	DUCKDB_API hash_t Hash() const;
422: 
423: 	DUCKDB_API static LogicalType MaxLogicalType(const LogicalType &left, const LogicalType &right);
424: 
425: 	//! Gets the decimal properties of a numeric type. Fails if the type is not numeric.
426: 	DUCKDB_API bool GetDecimalProperties(uint8_t &width, uint8_t &scale) const;
427: 
428: 	DUCKDB_API void Verify() const;
429: 
430: private:
431: 	LogicalTypeId id_;
432: 	PhysicalType physical_type_;
433: 	shared_ptr<ExtraTypeInfo> type_info_;
434: 
435: private:
436: 	PhysicalType GetInternalType();
437: 
438: public:
439: 	static constexpr const LogicalTypeId SQLNULL = LogicalTypeId::SQLNULL;
440: 	static constexpr const LogicalTypeId BOOLEAN = LogicalTypeId::BOOLEAN;
441: 	static constexpr const LogicalTypeId TINYINT = LogicalTypeId::TINYINT;
442: 	static constexpr const LogicalTypeId UTINYINT = LogicalTypeId::UTINYINT;
443: 	static constexpr const LogicalTypeId SMALLINT = LogicalTypeId::SMALLINT;
444: 	static constexpr const LogicalTypeId USMALLINT = LogicalTypeId::USMALLINT;
445: 	static constexpr const LogicalTypeId INTEGER = LogicalTypeId::INTEGER;
446: 	static constexpr const LogicalTypeId UINTEGER = LogicalTypeId::UINTEGER;
447: 	static constexpr const LogicalTypeId BIGINT = LogicalTypeId::BIGINT;
448: 	static constexpr const LogicalTypeId UBIGINT = LogicalTypeId::UBIGINT;
449: 	static constexpr const LogicalTypeId FLOAT = LogicalTypeId::FLOAT;
450: 	static constexpr const LogicalTypeId DOUBLE = LogicalTypeId::DOUBLE;
451: 	static constexpr const LogicalTypeId DATE = LogicalTypeId::DATE;
452: 	static constexpr const LogicalTypeId TIMESTAMP = LogicalTypeId::TIMESTAMP;
453: 	static constexpr const LogicalTypeId TIMESTAMP_S = LogicalTypeId::TIMESTAMP_SEC;
454: 	static constexpr const LogicalTypeId TIMESTAMP_MS = LogicalTypeId::TIMESTAMP_MS;
455: 	static constexpr const LogicalTypeId TIMESTAMP_NS = LogicalTypeId::TIMESTAMP_NS;
456: 	static constexpr const LogicalTypeId TIME = LogicalTypeId::TIME;
457: 	static constexpr const LogicalTypeId TIMESTAMP_TZ = LogicalTypeId::TIMESTAMP_TZ;
458: 	static constexpr const LogicalTypeId DATE_TZ = LogicalTypeId::DATE_TZ;
459: 	static constexpr const LogicalTypeId TIME_TZ = LogicalTypeId::TIME_TZ;
460: 	static constexpr const LogicalTypeId VARCHAR = LogicalTypeId::VARCHAR;
461: 	static constexpr const LogicalTypeId ANY = LogicalTypeId::ANY;
462: 	static constexpr const LogicalTypeId BLOB = LogicalTypeId::BLOB;
463: 	static constexpr const LogicalTypeId INTERVAL = LogicalTypeId::INTERVAL;
464: 	static constexpr const LogicalTypeId HUGEINT = LogicalTypeId::HUGEINT;
465: 	static constexpr const LogicalTypeId UUID = LogicalTypeId::UUID;
466: 	static constexpr const LogicalTypeId HASH = LogicalTypeId::HASH;
467: 	static constexpr const LogicalTypeId POINTER = LogicalTypeId::POINTER;
468: 	static constexpr const LogicalTypeId TABLE = LogicalTypeId::TABLE;
469: 	static constexpr const LogicalTypeId INVALID = LogicalTypeId::INVALID;
470: 
471: 	static constexpr const LogicalTypeId ROW_TYPE = LogicalTypeId::BIGINT;
472: 
473: 	// explicitly allowing these functions to be capitalized to be in-line with the remaining functions
474: 	DUCKDB_API static LogicalType DECIMAL(int width, int scale);                 // NOLINT
475: 	DUCKDB_API static LogicalType VARCHAR_COLLATION(string collation);           // NOLINT
476: 	DUCKDB_API static LogicalType LIST( LogicalType child);                       // NOLINT
477: 	DUCKDB_API static LogicalType STRUCT( child_list_t<LogicalType> children);    // NOLINT
478: 	DUCKDB_API static LogicalType MAP( child_list_t<LogicalType> children);       // NOLINT
479: 	DUCKDB_API static LogicalType MAP(LogicalType key, LogicalType value); // NOLINT
480: 	DUCKDB_API static LogicalType ENUM(const string &enum_name, Vector &ordered_data, idx_t size); // NOLINT
481: 	DUCKDB_API static LogicalType USER(const string &user_type_name); // NOLINT
482: 	//! A list of all NUMERIC types (integral and floating point types)
483: 	DUCKDB_API static const vector<LogicalType> Numeric();
484: 	//! A list of all INTEGRAL types
485: 	DUCKDB_API static const vector<LogicalType> Integral();
486: 	//! A list of ALL SQL types
487: 	DUCKDB_API static const vector<LogicalType> AllTypes();
488: };
489: 
490: struct DecimalType {
491: 	DUCKDB_API static uint8_t GetWidth(const LogicalType &type);
492: 	DUCKDB_API static uint8_t GetScale(const LogicalType &type);
493: };
494: 
495: struct StringType {
496: 	DUCKDB_API static string GetCollation(const LogicalType &type);
497: };
498: 
499: struct ListType {
500: 	DUCKDB_API static const LogicalType &GetChildType(const LogicalType &type);
501: };
502: 
503: struct UserType{
504: 	DUCKDB_API static const string &GetTypeName(const LogicalType &type);
505: };
506: 
507: struct EnumType{
508: 	DUCKDB_API static const string &GetTypeName(const LogicalType &type);
509: 	DUCKDB_API static int64_t GetPos(const LogicalType &type, const string& key);
510: 	DUCKDB_API static Vector &GetValuesInsertOrder(const LogicalType &type);
511: 	DUCKDB_API static idx_t GetSize(const LogicalType &type);
512: 	DUCKDB_API static const string GetValue(const Value &val);
513: 	DUCKDB_API static void SetCatalog(LogicalType &type, TypeCatalogEntry* catalog_entry);
514: 	DUCKDB_API static TypeCatalogEntry* GetCatalog(const LogicalType &type);
515: 	DUCKDB_API static PhysicalType GetPhysicalType(idx_t size);
516: };
517: 
518: struct StructType {
519: 	DUCKDB_API static const child_list_t<LogicalType> &GetChildTypes(const LogicalType &type);
520: 	DUCKDB_API static const LogicalType &GetChildType(const LogicalType &type, idx_t index);
521: 	DUCKDB_API static const string &GetChildName(const LogicalType &type, idx_t index);
522: 	DUCKDB_API static idx_t GetChildCount(const LogicalType &type);
523: };
524: 
525: 
526: string LogicalTypeIdToString(LogicalTypeId type);
527: 
528: LogicalTypeId TransformStringToLogicalType(const string &str);
529: 
530: //! Returns the PhysicalType for the given type
531: template <class T>
532: PhysicalType GetTypeId() {
533: 	if (std::is_same<T, bool>()) {
534: 		return PhysicalType::BOOL;
535: 	} else if (std::is_same<T, int8_t>()) {
536: 		return PhysicalType::INT8;
537: 	} else if (std::is_same<T, int16_t>()) {
538: 		return PhysicalType::INT16;
539: 	} else if (std::is_same<T, int32_t>()) {
540: 		return PhysicalType::INT32;
541: 	} else if (std::is_same<T, int64_t>()) {
542: 		return PhysicalType::INT64;
543: 	} else if (std::is_same<T, uint8_t>()) {
544: 		return PhysicalType::UINT8;
545: 	} else if (std::is_same<T, uint16_t>()) {
546: 		return PhysicalType::UINT16;
547: 	} else if (std::is_same<T, uint32_t>()) {
548: 		return PhysicalType::UINT32;
549: 	} else if (std::is_same<T, uint64_t>()) {
550: 		return PhysicalType::UINT64;
551: 	} else if (std::is_same<T, hugeint_t>()) {
552: 		return PhysicalType::INT128;
553: 	} else if (std::is_same<T, date_t>()) {
554: 		return PhysicalType::DATE32;
555: 	} else if (std::is_same<T, dtime_t>()) {
556: 		return PhysicalType::TIME32;
557: 	} else if (std::is_same<T, timestamp_t>()) {
558: 		return PhysicalType::TIMESTAMP;
559: 	} else if (std::is_same<T, float>()) {
560: 		return PhysicalType::FLOAT;
561: 	} else if (std::is_same<T, double>()) {
562: 		return PhysicalType::DOUBLE;
563: 	} else if (std::is_same<T, const char *>() || std::is_same<T, char *>() || std::is_same<T, string_t>()) {
564: 		return PhysicalType::VARCHAR;
565: 	} else if (std::is_same<T, interval_t>()) {
566: 		return PhysicalType::INTERVAL;
567: 	} else {
568: 		return PhysicalType::INVALID;
569: 	}
570: }
571: 
572: template<class T>
573: bool TypeIsNumber() {
574: 	return std::is_integral<T>() || std::is_floating_point<T>() || std::is_same<T, hugeint_t>();
575: }
576: 
577: template <class T>
578: bool IsValidType() {
579: 	return GetTypeId<T>() != PhysicalType::INVALID;
580: }
581: 
582: //! The PhysicalType used by the row identifiers column
583: extern const PhysicalType ROW_TYPE;
584: 
585: DUCKDB_API string TypeIdToString(PhysicalType type);
586: idx_t GetTypeIdSize(PhysicalType type);
587: bool TypeIsConstantSize(PhysicalType type);
588: bool TypeIsIntegral(PhysicalType type);
589: bool TypeIsNumeric(PhysicalType type);
590: bool TypeIsInteger(PhysicalType type);
591: 
592: template <class T>
593: bool IsIntegerType() {
594: 	return TypeIsIntegral(GetTypeId<T>());
595: }
596: 
597: bool ApproxEqual(float l, float r);
598: bool ApproxEqual(double l, double r);
599: 
600: } // namespace duckdb
[end of src/include/duckdb/common/types.hpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: