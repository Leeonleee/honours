{
  "repo": "duckdb/duckdb",
  "pull_number": 6809,
  "instance_id": "duckdb__duckdb-6809",
  "issue_numbers": [
    "6584"
  ],
  "base_commit": "ace90385331296908844d1072c9959b4358b11b4",
  "patch": "diff --git a/tools/pythonpkg/src/arrow_array_stream.cpp b/tools/pythonpkg/src/arrow_array_stream.cpp\nindex 2d07de50b125..b14ca0406c74 100644\n--- a/tools/pythonpkg/src/arrow_array_stream.cpp\n+++ b/tools/pythonpkg/src/arrow_array_stream.cpp\n@@ -16,25 +16,27 @@ namespace duckdb {\n \n void VerifyArrowDatasetLoaded() {\n \tauto &import_cache = *DuckDBPyConnection::ImportCache();\n-\tif (!import_cache.arrow().dataset.IsLoaded()) {\n+\tif (!import_cache.arrow_dataset().IsLoaded()) {\n \t\tthrow InvalidInputException(\"Optional module 'pyarrow.dataset' is required to perform this action\");\n \t}\n }\n \n PyArrowObjectType GetArrowType(const py::handle &obj) {\n \tauto &import_cache = *DuckDBPyConnection::ImportCache();\n-\n-\tauto scanner_class = import_cache.arrow().dataset.Scanner();\n-\tauto table_class = import_cache.arrow().lib.Table();\n-\tauto record_batch_reader_class = import_cache.arrow().lib.RecordBatchReader();\n-\tauto dataset_class = import_cache.arrow().dataset.Dataset();\n-\n-\tif (py::isinstance(obj, scanner_class)) {\n-\t\treturn PyArrowObjectType::Scanner;\n-\t} else if (py::isinstance(obj, table_class)) {\n+\t// First Verify Lib Types\n+\tauto table_class = import_cache.arrow_lib().Table();\n+\tauto record_batch_reader_class = import_cache.arrow_lib().RecordBatchReader();\n+\tif (py::isinstance(obj, table_class)) {\n \t\treturn PyArrowObjectType::Table;\n \t} else if (py::isinstance(obj, record_batch_reader_class)) {\n \t\treturn PyArrowObjectType::RecordBatchReader;\n+\t}\n+\t// Then Verify dataset types\n+\tauto dataset_class = import_cache.arrow_dataset().Dataset();\n+\tauto scanner_class = import_cache.arrow_dataset().Scanner();\n+\n+\tif (py::isinstance(obj, scanner_class)) {\n+\t\treturn PyArrowObjectType::Scanner;\n \t} else if (py::isinstance(obj, dataset_class)) {\n \t\treturn PyArrowObjectType::Dataset;\n \t}\n@@ -70,8 +72,6 @@ unique_ptr<ArrowArrayStreamWrapper> PythonTableArrowArrayStreamFactory::Produce(\n \tpy::handle arrow_obj_handle(factory->arrow_object);\n \tauto arrow_object_type = GetArrowType(arrow_obj_handle);\n \n-\tVerifyArrowDatasetLoaded();\n-\n \tpy::object scanner;\n \tpy::object arrow_batch_scanner = py::module_::import(\"pyarrow.dataset\").attr(\"Scanner\").attr(\"from_batches\");\n \tswitch (arrow_object_type) {\n@@ -113,12 +113,21 @@ unique_ptr<ArrowArrayStreamWrapper> PythonTableArrowArrayStreamFactory::Produce(\n \n void PythonTableArrowArrayStreamFactory::GetSchema(uintptr_t factory_ptr, ArrowSchemaWrapper &schema) {\n \tpy::gil_scoped_acquire acquire;\n-\n-\tVerifyArrowDatasetLoaded();\n \tPythonTableArrowArrayStreamFactory *factory = (PythonTableArrowArrayStreamFactory *)factory_ptr;\n \tD_ASSERT(factory->arrow_object);\n-\tauto scanner_class = py::module::import(\"pyarrow.dataset\").attr(\"Scanner\");\n+\tauto table_class = py::module::import(\"pyarrow\").attr(\"Table\");\n \tpy::handle arrow_obj_handle(factory->arrow_object);\n+\tif (py::isinstance(arrow_obj_handle, table_class)) {\n+\t\tauto obj_schema = arrow_obj_handle.attr(\"schema\");\n+\t\tauto export_to_c = obj_schema.attr(\"_export_to_c\");\n+\t\texport_to_c((uint64_t)&schema);\n+\t\treturn;\n+\t}\n+\n+\tVerifyArrowDatasetLoaded();\n+\n+\tauto scanner_class = py::module::import(\"pyarrow.dataset\").attr(\"Scanner\");\n+\n \tif (py::isinstance(arrow_obj_handle, scanner_class)) {\n \t\tauto obj_schema = arrow_obj_handle.attr(\"projected_schema\");\n \t\tauto export_to_c = obj_schema.attr(\"_export_to_c\");\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/arrow_module.hpp b/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/arrow_module.hpp\nindex 769f1ad485b0..c0565c61d2ca 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/arrow_module.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/arrow_module.hpp\n@@ -14,9 +14,10 @@ namespace duckdb {\n \n struct ArrowLibCacheItem : public PythonImportCacheItem {\n public:\n+\tstatic constexpr const char *Name = \"pyarrow\";\n \t~ArrowLibCacheItem() override {\n \t}\n-\tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n+\tvoid LoadSubtypes(PythonImportCache &cache) override {\n \t\tTable.LoadAttribute(\"Table\", cache, *this);\n \t\tRecordBatchReader.LoadAttribute(\"RecordBatchReader\", cache, *this);\n \t}\n@@ -28,9 +29,10 @@ struct ArrowLibCacheItem : public PythonImportCacheItem {\n \n struct ArrowDatasetCacheItem : public PythonImportCacheItem {\n public:\n+\tstatic constexpr const char *Name = \"pyarrow.dataset\";\n \t~ArrowDatasetCacheItem() override {\n \t}\n-\tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n+\tvoid LoadSubtypes(PythonImportCache &cache) override {\n \t\tDataset.LoadAttribute(\"Dataset\", cache, *this);\n \t\tScanner.LoadAttribute(\"Scanner\", cache, *this);\n \t}\n@@ -40,29 +42,7 @@ struct ArrowDatasetCacheItem : public PythonImportCacheItem {\n \tPythonImportCacheItem Scanner;\n \n protected:\n-\tbool IsRequired() const override final {\n-\t\treturn false;\n-\t}\n-};\n-\n-struct ArrowCacheItem : public PythonImportCacheItem {\n-public:\n-\tstatic constexpr const char *Name = \"pyarrow\";\n-\n-public:\n-\t~ArrowCacheItem() override {\n-\t}\n-\tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n-\t\tlib.LoadAttribute(\"lib\", cache, *this);\n-\t\tdataset.LoadModule(\"pyarrow.dataset\", cache);\n-\t}\n-\n-public:\n-\tArrowLibCacheItem lib;\n-\tArrowDatasetCacheItem dataset;\n-\n-protected:\n-\tbool IsRequired() const override final {\n+\tbool IsRequired() const final {\n \t\treturn false;\n \t}\n };\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/import_cache/python_import_cache.hpp b/tools/pythonpkg/src/include/duckdb_python/import_cache/python_import_cache.hpp\nindex fb85da580485..b0d09afef075 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/import_cache/python_import_cache.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/import_cache/python_import_cache.hpp\n@@ -25,7 +25,7 @@ struct PythonImportCache {\n public:\n \ttemplate <class T>\n \tT &LazyLoadModule(T &module) {\n-\t\tif (!module.LoadAttempted()) {\n+\t\tif (!module.LoadSucceeded()) {\n \t\t\tmodule.LoadModule(T::Name, *this);\n \t\t}\n \t\treturn module;\n@@ -61,8 +61,11 @@ struct PythonImportCache {\n \tPolarsCacheItem &polars() {\n \t\treturn LazyLoadModule(polars_module);\n \t}\n-\tArrowCacheItem &arrow() {\n-\t\treturn LazyLoadModule(arrow_module);\n+\tArrowLibCacheItem &arrow_lib() {\n+\t\treturn LazyLoadModule(arrow_lib_module);\n+\t}\n+\tArrowDatasetCacheItem &arrow_dataset() {\n+\t\treturn LazyLoadModule(arrow_dataset_module);\n \t}\n \tIPythonCacheItem &IPython() {\n \t\treturn LazyLoadModule(IPython_module);\n@@ -82,7 +85,9 @@ struct PythonImportCache {\n \tUUIDCacheItem uuid_module;\n \tPandasCacheItem pandas_module;\n \tPolarsCacheItem polars_module;\n-\tArrowCacheItem arrow_module;\n+\tArrowDatasetCacheItem arrow_dataset_module;\n+\tArrowLibCacheItem arrow_lib_module;\n+\n \tIPythonCacheItem IPython_module;\n \tIpywidgetsCacheItem ipywidgets_module;\n \ndiff --git a/tools/pythonpkg/src/include/duckdb_python/import_cache/python_import_cache_item.hpp b/tools/pythonpkg/src/include/duckdb_python/import_cache/python_import_cache_item.hpp\nindex a16735fdc3e2..c45038803b85 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/import_cache/python_import_cache_item.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/import_cache/python_import_cache_item.hpp\n@@ -19,7 +19,7 @@ struct PythonImportCache;\n \n struct PythonImportCacheItem {\n public:\n-\tPythonImportCacheItem() : load_attempted(false), object(nullptr) {\n+\tPythonImportCacheItem() : load_succeeded(false), object(nullptr) {\n \t}\n \tvirtual ~PythonImportCacheItem() {\n \t}\n@@ -27,7 +27,7 @@ struct PythonImportCacheItem {\n \t}\n \n public:\n-\tbool LoadAttempted() const;\n+\tbool LoadSucceeded() const;\n \tbool IsLoaded() const;\n \tbool IsInstance(py::handle object) const;\n \tpy::handle operator()(void) const;\n@@ -44,7 +44,7 @@ struct PythonImportCacheItem {\n \n private:\n \t//! Whether or not we attempted to load the module\n-\tbool load_attempted;\n+\tbool load_succeeded;\n \t//! The stored item\n \tPyObject *object;\n };\ndiff --git a/tools/pythonpkg/src/pyconnection.cpp b/tools/pythonpkg/src/pyconnection.cpp\nindex 778608ac1966..6b03cde3c9f1 100644\n--- a/tools/pythonpkg/src/pyconnection.cpp\n+++ b/tools/pythonpkg/src/pyconnection.cpp\n@@ -1431,17 +1431,17 @@ bool DuckDBPyConnection::IsPandasDataframe(const py::object &object) {\n \tif (!ModuleIsLoaded<PandasCacheItem>()) {\n \t\treturn false;\n \t}\n-\tauto &import_cache_py = *DuckDBPyConnection::ImportCache();\n-\treturn import_cache_py.pandas().DataFrame.IsInstance(object);\n+\tauto &py_import_cache = *DuckDBPyConnection::ImportCache();\n+\treturn py_import_cache.pandas().DataFrame.IsInstance(object);\n }\n \n bool DuckDBPyConnection::IsPolarsDataframe(const py::object &object) {\n \tif (!ModuleIsLoaded<PolarsCacheItem>()) {\n \t\treturn false;\n \t}\n-\tauto &import_cache_py = *DuckDBPyConnection::ImportCache();\n-\treturn import_cache_py.polars().DataFrame.IsInstance(object) ||\n-\t       import_cache_py.polars().LazyFrame.IsInstance(object);\n+\tauto &py_import_cache = *DuckDBPyConnection::ImportCache();\n+\treturn py_import_cache.polars().DataFrame.IsInstance(object) ||\n+\t       py_import_cache.polars().LazyFrame.IsInstance(object);\n }\n \n bool IsValidNumpyDimensions(const py::handle &object, int &dim) {\n@@ -1495,14 +1495,29 @@ NumpyObjectType DuckDBPyConnection::IsAcceptedNumpyObject(const py::object &obje\n }\n \n bool DuckDBPyConnection::IsAcceptedArrowObject(const py::object &object) {\n-\tif (!ModuleIsLoaded<ArrowCacheItem>()) {\n+\tif (!ModuleIsLoaded<ArrowLibCacheItem>()) {\n \t\treturn false;\n \t}\n-\tauto &import_cache_py = *DuckDBPyConnection::ImportCache();\n-\treturn import_cache_py.arrow().lib.Table.IsInstance(object) ||\n-\t       import_cache_py.arrow().lib.RecordBatchReader.IsInstance(object) ||\n-\t       import_cache_py.arrow().dataset.Dataset.IsInstance(object) ||\n-\t       import_cache_py.arrow().dataset.Scanner.IsInstance(object);\n+\tauto &py_import_cache = *DuckDBPyConnection::ImportCache();\n+\tif (py_import_cache.arrow_lib().Table.IsInstance(object) ||\n+\t    py_import_cache.arrow_lib().RecordBatchReader.IsInstance(object)) {\n+\t\treturn true;\n+\t}\n+\tif (!ModuleIsLoaded<ArrowDatasetCacheItem>()) {\n+\t\treturn false;\n+\t}\n+\treturn py_import_cache.arrow_dataset().Dataset.IsInstance(object) ||\n+\t       py_import_cache.arrow_dataset().Scanner.IsInstance(object);\n+}\n+\n+unique_lock<std::mutex> DuckDBPyConnection::AcquireConnectionLock() {\n+\t// we first release the gil and then acquire the connection lock\n+\tunique_lock<std::mutex> lock(py_connection_lock, std::defer_lock);\n+\t{\n+\t\tpy::gil_scoped_release release;\n+\t\tlock.lock();\n+\t}\n+\treturn lock;\n }\n \n } // namespace duckdb\ndiff --git a/tools/pythonpkg/src/python_import_cache.cpp b/tools/pythonpkg/src/python_import_cache.cpp\nindex e25a127983de..f2ef72040ed5 100644\n--- a/tools/pythonpkg/src/python_import_cache.cpp\n+++ b/tools/pythonpkg/src/python_import_cache.cpp\n@@ -11,8 +11,8 @@ py::handle PythonImportCacheItem::operator()(void) const {\n \treturn object;\n }\n \n-bool PythonImportCacheItem::LoadAttempted() const {\n-\treturn load_attempted;\n+bool PythonImportCacheItem::LoadSucceeded() const {\n+\treturn load_succeeded;\n }\n \n bool PythonImportCacheItem::IsLoaded() const {\n@@ -34,10 +34,10 @@ PyObject *PythonImportCacheItem::AddCache(PythonImportCache &cache, py::object o\n }\n \n void PythonImportCacheItem::LoadModule(const string &name, PythonImportCache &cache) {\n-\tload_attempted = true;\n \ttry {\n \t\tpy::gil_assert();\n \t\tobject = AddCache(cache, std::move(py::module::import(name.c_str())));\n+\t\tload_succeeded = true;\n \t} catch (py::error_already_set &e) {\n \t\tif (IsRequired()) {\n \t\t\tPyErr_PrintEx(1);\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/arrow/test_6584.py b/tools/pythonpkg/tests/fast/arrow/test_6584.py\nnew file mode 100644\nindex 000000000000..93571968e8d9\n--- /dev/null\n+++ b/tools/pythonpkg/tests/fast/arrow/test_6584.py\n@@ -0,0 +1,22 @@\n+from concurrent.futures import ThreadPoolExecutor\n+import duckdb\n+import pytest\n+\n+pyarrow = pytest.importorskip('pyarrow')\n+\n+def f(cur, i, data):    \n+    cur.execute(f\"create table t_{i} as select * from data\")\n+    return cur.execute(f\"select * from t_{i}\").arrow()\n+\n+def test_6584():\n+    pool = ThreadPoolExecutor(max_workers=2)\n+    data = pyarrow.Table.from_pydict({\"a\": [1,2,3]})\n+    c = duckdb.connect()\n+    futures = []\n+    for i in range(2):\n+        fut = pool.submit(f, c.cursor(), i,data)\n+        futures.append(fut)\n+\n+    for fut in futures:\n+        arrow_res = fut.result()\n+        assert data.equals(arrow_res)\n\\ No newline at end of file\ndiff --git a/tools/pythonpkg/tests/fast/test_import_without_pyarrow_dataset.py b/tools/pythonpkg/tests/fast/test_import_without_pyarrow_dataset.py\nindex 06e929e5dd53..dbd55c742472 100644\n--- a/tools/pythonpkg/tests/fast/test_import_without_pyarrow_dataset.py\n+++ b/tools/pythonpkg/tests/fast/test_import_without_pyarrow_dataset.py\n@@ -8,9 +8,9 @@ def test_import(self, monkeypatch: pytest.MonkeyPatch):\n \t\tmonkeypatch.setitem(sys.modules, \"pyarrow.dataset\", None)\n \t\timport duckdb\n \t\t# We should be able to import duckdb even when pyarrow.dataset is missing\n-\n-\t\trel = duckdb.query('select 1')\n-\t\tarrow_table = rel.arrow()\n-\t\twith pytest.raises(ModuleNotFoundError):\n+\t\tcon = duckdb.connect()\n+\t\trel = con.query('select 1')\n+\t\tarrow_record_batch = rel.record_batch()\n+\t\twith pytest.raises(duckdb.InvalidInputException):\n \t\t\t# The replacement scan functionality relies on pyarrow.dataset\n-\t\t\tduckdb.query('select * from arrow_table')\n+\t\t\tcon.query('select * from arrow_record_batch')\n",
  "problem_statement": "Reading from pyarrow is not thread-safe\n### What happens?\n\nReading in multiple pyarrow datasets from different threads causes the error:\r\n```\r\nduckdb.InvalidInputException: Invalid Input Error: Optional module 'pyarrow.dataset' is required to perform this action\r\n```\r\n\r\nManually importing `pyarrow.dataset` within each thread is the current workaround.\n\n### To Reproduce\n\n```python\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nimport duckdb\r\npool = ThreadPoolExecutor(max_workers=2)\r\n\r\nc = duckdb.connect()\r\n\r\ndef f(cur, i):\r\n    import pyarrow\r\n    # import pyarrow.dataset # (uncomment to fix)\r\n    data = pyarrow.Table.from_pydict({\"a\": [1,2,3]})\r\n    cur.execute(f\"create table t_{i} as select * from data\")\r\n\r\nfutures = []\r\nfor i in range(2):\r\n    fut = pool.submit(f, c.cursor(), i)\r\n    futures.append(fut)\r\n\r\nfor fut in futures:\r\n    fut.result()\r\n```\n\n### OS:\n\nMacOS Arm\n\n### DuckDB Version:\n\n0.7.2-dev225 and 0.7.1\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nAlexander Vandenberg-Rodes\n\n### Affiliation:\n\nObsidian Security\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "",
  "created_at": "2023-03-21T15:41:20Z"
}