You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
string_agg only returns final row if there is no Group By clause
#### What happens?
Hi folks! Found an odd one...
If a group by clause is not provided, the string_agg function returns only the last row of data rather than all rows concatenated together. 
This issue is not present in 0.3.0, only in 0.3.1-dev678. I have a workaround (just include a dummy column), but wanted to let you know about this one!

#### To Reproduce
This query does not return correctly (returns 'text3'):
```sql
WITH my_data as (
        SELECT 'text1'::varchar(1000) as my_column union all
        SELECT 'text2'::varchar(1000) as my_column union all
        SELECT 'text3'::varchar(1000) as my_column 
    )
        SELECT string_agg(my_column,', ') as my_string_agg 
        FROM my_data
```
This query returns correctly (returns 'text1, text2, text3'):
```sql
WITH my_data as (
        SELECT 1 as dummy,  'text1'::varchar(1000) as my_column union all
        SELECT 1 as dummy,  'text2'::varchar(1000) as my_column union all
        SELECT 1 as dummy,  'text3'::varchar(1000) as my_column 
    )
        SELECT string_agg(my_column,', ') as my_string_agg 
        FROM my_data
        GROUP BY
            dummy
```
#### Environment (please complete the following information):
 - OS: Windows 10
 - DuckDB Version: 0.3.1-dev678 (issue not present in 0.3.0)
 - DuckDB Client: Node.js

#### Before Submitting

- [X] **Have you tried this on the latest `master` branch?**
* **Python**: `pip install duckdb --upgrade --pre`
* **R**: `install.packages("https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz", repos = NULL)`
* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.

- [X] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**

string_agg only returns final row if there is no Group By clause
#### What happens?
Hi folks! Found an odd one...
If a group by clause is not provided, the string_agg function returns only the last row of data rather than all rows concatenated together. 
This issue is not present in 0.3.0, only in 0.3.1-dev678. I have a workaround (just include a dummy column), but wanted to let you know about this one!

#### To Reproduce
This query does not return correctly (returns 'text3'):
```sql
WITH my_data as (
        SELECT 'text1'::varchar(1000) as my_column union all
        SELECT 'text2'::varchar(1000) as my_column union all
        SELECT 'text3'::varchar(1000) as my_column 
    )
        SELECT string_agg(my_column,', ') as my_string_agg 
        FROM my_data
```
This query returns correctly (returns 'text1, text2, text3'):
```sql
WITH my_data as (
        SELECT 1 as dummy,  'text1'::varchar(1000) as my_column union all
        SELECT 1 as dummy,  'text2'::varchar(1000) as my_column union all
        SELECT 1 as dummy,  'text3'::varchar(1000) as my_column 
    )
        SELECT string_agg(my_column,', ') as my_string_agg 
        FROM my_data
        GROUP BY
            dummy
```
#### Environment (please complete the following information):
 - OS: Windows 10
 - DuckDB Version: 0.3.1-dev678 (issue not present in 0.3.0)
 - DuckDB Client: Node.js

#### Before Submitting

- [X] **Have you tried this on the latest `master` branch?**
* **Python**: `pip install duckdb --upgrade --pre`
* **R**: `install.packages("https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz", repos = NULL)`
* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.

- [X] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**


</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16: </p>
17: 
18: ## DuckDB
19: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/docs/why_duckdb.html).
20: 
21: ## Installation
22: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
23: 
24: ## Data Import
25: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
26: 
27: ```sql
28: SELECT * FROM 'myfile.csv';
29: SELECT * FROM 'myfile.parquet';
30: ```
31: 
32: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
33: 
34: ## SQL Reference
35: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
36: 
37: ## Development
38: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
39: 
40: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
41: 
42: 
[end of README.md]
[start of src/execution/operator/aggregate/physical_simple_aggregate.cpp]
1: #include "duckdb/execution/operator/aggregate/physical_simple_aggregate.hpp"
2: #include "duckdb/parallel/thread_context.hpp"
3: #include "duckdb/common/vector_operations/vector_operations.hpp"
4: #include "duckdb/execution/expression_executor.hpp"
5: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
6: #include "duckdb/catalog/catalog_entry/aggregate_function_catalog_entry.hpp"
7: #include "duckdb/main/client_context.hpp"
8: 
9: namespace duckdb {
10: 
11: PhysicalSimpleAggregate::PhysicalSimpleAggregate(vector<LogicalType> types, vector<unique_ptr<Expression>> expressions,
12:                                                  bool all_combinable, idx_t estimated_cardinality)
13:     : PhysicalOperator(PhysicalOperatorType::SIMPLE_AGGREGATE, move(types), estimated_cardinality),
14:       aggregates(move(expressions)), all_combinable(all_combinable) {
15: }
16: 
17: //===--------------------------------------------------------------------===//
18: // Sink
19: //===--------------------------------------------------------------------===//
20: struct AggregateState {
21: 	explicit AggregateState(const vector<unique_ptr<Expression>> &aggregate_expressions) {
22: 		for (auto &aggregate : aggregate_expressions) {
23: 			D_ASSERT(aggregate->GetExpressionClass() == ExpressionClass::BOUND_AGGREGATE);
24: 			auto &aggr = (BoundAggregateExpression &)*aggregate;
25: 			auto state = unique_ptr<data_t[]>(new data_t[aggr.function.state_size()]);
26: 			aggr.function.initialize(state.get());
27: 			aggregates.push_back(move(state));
28: 			destructors.push_back(aggr.function.destructor);
29: 		}
30: 	}
31: 	~AggregateState() {
32: 		D_ASSERT(destructors.size() == aggregates.size());
33: 		for (idx_t i = 0; i < destructors.size(); i++) {
34: 			if (!destructors[i]) {
35: 				continue;
36: 			}
37: 			Vector state_vector(Value::POINTER((uintptr_t)aggregates[i].get()));
38: 			state_vector.SetVectorType(VectorType::FLAT_VECTOR);
39: 
40: 			destructors[i](state_vector, 1);
41: 		}
42: 	}
43: 
44: 	void Move(AggregateState &other) {
45: 		other.aggregates = move(aggregates);
46: 		other.destructors = move(destructors);
47: 	}
48: 
49: 	//! The aggregate values
50: 	vector<unique_ptr<data_t[]>> aggregates;
51: 	// The destructors
52: 	vector<aggregate_destructor_t> destructors;
53: };
54: 
55: class SimpleAggregateGlobalState : public GlobalSinkState {
56: public:
57: 	explicit SimpleAggregateGlobalState(const vector<unique_ptr<Expression>> &aggregates)
58: 	    : state(aggregates), finished(false) {
59: 	}
60: 
61: 	//! The lock for updating the global aggregate state
62: 	mutex lock;
63: 	//! The global aggregate state
64: 	AggregateState state;
65: 	//! Whether or not the aggregate is finished
66: 	bool finished;
67: };
68: 
69: class SimpleAggregateLocalState : public LocalSinkState {
70: public:
71: 	explicit SimpleAggregateLocalState(const vector<unique_ptr<Expression>> &aggregates) : state(aggregates) {
72: 		vector<LogicalType> payload_types;
73: 		for (auto &aggregate : aggregates) {
74: 			D_ASSERT(aggregate->GetExpressionClass() == ExpressionClass::BOUND_AGGREGATE);
75: 			auto &aggr = (BoundAggregateExpression &)*aggregate;
76: 			// initialize the payload chunk
77: 			if (!aggr.children.empty()) {
78: 				for (auto &child : aggr.children) {
79: 					payload_types.push_back(child->return_type);
80: 					child_executor.AddExpression(*child);
81: 				}
82: 			}
83: 		}
84: 		if (!payload_types.empty()) { // for select count(*) from t; there is no payload at all
85: 			payload_chunk.Initialize(payload_types);
86: 		}
87: 	}
88: 	void Reset() {
89: 		payload_chunk.Reset();
90: 	}
91: 
92: 	//! The local aggregate state
93: 	AggregateState state;
94: 	//! The executor
95: 	ExpressionExecutor child_executor;
96: 	//! The payload chunk
97: 	DataChunk payload_chunk;
98: };
99: 
100: unique_ptr<GlobalSinkState> PhysicalSimpleAggregate::GetGlobalSinkState(ClientContext &context) const {
101: 	return make_unique<SimpleAggregateGlobalState>(aggregates);
102: }
103: 
104: unique_ptr<LocalSinkState> PhysicalSimpleAggregate::GetLocalSinkState(ExecutionContext &context) const {
105: 	return make_unique<SimpleAggregateLocalState>(aggregates);
106: }
107: 
108: SinkResultType PhysicalSimpleAggregate::Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate,
109:                                              DataChunk &input) const {
110: 	auto &sink = (SimpleAggregateLocalState &)lstate;
111: 	// perform the aggregation inside the local state
112: 	idx_t payload_idx = 0, payload_expr_idx = 0;
113: 	sink.Reset();
114: 
115: 	DataChunk &payload_chunk = sink.payload_chunk;
116: 	sink.child_executor.SetChunk(input);
117: 	payload_chunk.SetCardinality(input);
118: 	for (idx_t aggr_idx = 0; aggr_idx < aggregates.size(); aggr_idx++) {
119: 		DataChunk filtered_input;
120: 		auto &aggregate = (BoundAggregateExpression &)*aggregates[aggr_idx];
121: 		idx_t payload_cnt = 0;
122: 		// resolve the filter (if any)
123: 		if (aggregate.filter) {
124: 			ExpressionExecutor filter_execution(aggregate.filter.get());
125: 			SelectionVector true_sel(STANDARD_VECTOR_SIZE);
126: 			auto count = filter_execution.SelectExpression(input, true_sel);
127: 			auto input_types = input.GetTypes();
128: 			filtered_input.Initialize(input_types);
129: 			filtered_input.Slice(input, true_sel, count);
130: 			sink.child_executor.SetChunk(filtered_input);
131: 			payload_chunk.SetCardinality(count);
132: 		}
133: 		// resolve the child expressions of the aggregate (if any)
134: 		if (!aggregate.children.empty()) {
135: 			for (idx_t i = 0; i < aggregate.children.size(); ++i) {
136: 				sink.child_executor.ExecuteExpression(payload_expr_idx, payload_chunk.data[payload_idx + payload_cnt]);
137: 				payload_expr_idx++;
138: 				payload_cnt++;
139: 			}
140: 		}
141: 
142: 		aggregate.function.simple_update(payload_cnt == 0 ? nullptr : &payload_chunk.data[payload_idx],
143: 		                                 aggregate.bind_info.get(), payload_cnt, sink.state.aggregates[aggr_idx].get(),
144: 		                                 payload_chunk.size());
145: 		payload_idx += payload_cnt;
146: 	}
147: 	return SinkResultType::NEED_MORE_INPUT;
148: }
149: 
150: //===--------------------------------------------------------------------===//
151: // Finalize
152: //===--------------------------------------------------------------------===//
153: void PhysicalSimpleAggregate::Combine(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate) const {
154: 	auto &gstate = (SimpleAggregateGlobalState &)state;
155: 	auto &source = (SimpleAggregateLocalState &)lstate;
156: 	D_ASSERT(!gstate.finished);
157: 
158: 	// finalize: combine the local state into the global state
159: 	if (all_combinable) {
160: 		// all aggregates are combinable: we might be doing a parallel aggregate
161: 		// use the combine method to combine the partial aggregates
162: 		lock_guard<mutex> glock(gstate.lock);
163: 		for (idx_t aggr_idx = 0; aggr_idx < aggregates.size(); aggr_idx++) {
164: 			auto &aggregate = (BoundAggregateExpression &)*aggregates[aggr_idx];
165: 			Vector source_state(Value::POINTER((uintptr_t)source.state.aggregates[aggr_idx].get()));
166: 			Vector dest_state(Value::POINTER((uintptr_t)gstate.state.aggregates[aggr_idx].get()));
167: 
168: 			aggregate.function.combine(source_state, dest_state, 1);
169: 		}
170: 	} else {
171: 		// complex aggregates: this is necessarily a non-parallel aggregate
172: 		// simply move over the source state into the global state
173: 		source.state.Move(gstate.state);
174: 	}
175: 
176: 	context.thread.profiler.Flush(this, &source.child_executor, "child_executor", 0);
177: 	context.client.profiler->Flush(context.thread.profiler);
178: }
179: 
180: SinkFinalizeType PhysicalSimpleAggregate::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
181:                                                    GlobalSinkState &gstate_p) const {
182: 	auto &gstate = (SimpleAggregateGlobalState &)gstate_p;
183: 
184: 	D_ASSERT(!gstate.finished);
185: 	gstate.finished = true;
186: 	return SinkFinalizeType::READY;
187: }
188: 
189: //===--------------------------------------------------------------------===//
190: // Source
191: //===--------------------------------------------------------------------===//
192: class SimpleAggregateState : public GlobalSourceState {
193: public:
194: 	SimpleAggregateState() : finished(false) {
195: 	}
196: 
197: 	bool finished;
198: };
199: 
200: unique_ptr<GlobalSourceState> PhysicalSimpleAggregate::GetGlobalSourceState(ClientContext &context) const {
201: 	return make_unique<SimpleAggregateState>();
202: }
203: 
204: void PhysicalSimpleAggregate::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate_p,
205:                                       LocalSourceState &lstate) const {
206: 	auto &gstate = (SimpleAggregateGlobalState &)*sink_state;
207: 	auto &state = (SimpleAggregateState &)gstate_p;
208: 	D_ASSERT(gstate.finished);
209: 	if (state.finished) {
210: 		return;
211: 	}
212: 
213: 	// initialize the result chunk with the aggregate values
214: 	chunk.SetCardinality(1);
215: 	for (idx_t aggr_idx = 0; aggr_idx < aggregates.size(); aggr_idx++) {
216: 		auto &aggregate = (BoundAggregateExpression &)*aggregates[aggr_idx];
217: 
218: 		Vector state_vector(Value::POINTER((uintptr_t)gstate.state.aggregates[aggr_idx].get()));
219: 		aggregate.function.finalize(state_vector, aggregate.bind_info.get(), chunk.data[aggr_idx], 1, 0);
220: 	}
221: 	state.finished = true;
222: }
223: 
224: string PhysicalSimpleAggregate::ParamsToString() const {
225: 	string result;
226: 	for (idx_t i = 0; i < aggregates.size(); i++) {
227: 		auto &aggregate = (BoundAggregateExpression &)*aggregates[i];
228: 		if (i > 0) {
229: 			result += "\n";
230: 		}
231: 		result += aggregates[i]->GetName();
232: 		if (aggregate.filter) {
233: 			result += " Filter: " + aggregate.filter->GetName();
234: 		}
235: 	}
236: 	return result;
237: }
238: 
239: } // namespace duckdb
[end of src/execution/operator/aggregate/physical_simple_aggregate.cpp]
[start of src/execution/physical_plan/plan_aggregate.cpp]
1: #include "duckdb/catalog/catalog_entry/aggregate_function_catalog_entry.hpp"
2: #include "duckdb/common/operator/subtract.hpp"
3: #include "duckdb/execution/operator/aggregate/physical_hash_aggregate.hpp"
4: #include "duckdb/execution/operator/aggregate/physical_perfecthash_aggregate.hpp"
5: #include "duckdb/execution/operator/aggregate/physical_simple_aggregate.hpp"
6: #include "duckdb/execution/operator/projection/physical_projection.hpp"
7: #include "duckdb/execution/physical_plan_generator.hpp"
8: #include "duckdb/main/client_context.hpp"
9: #include "duckdb/parser/expression/comparison_expression.hpp"
10: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
11: #include "duckdb/planner/operator/logical_aggregate.hpp"
12: #include "duckdb/storage/statistics/numeric_statistics.hpp"
13: namespace duckdb {
14: 
15: static uint32_t RequiredBitsForValue(uint32_t n) {
16: 	idx_t required_bits = 0;
17: 	while (n > 0) {
18: 		n >>= 1;
19: 		required_bits++;
20: 	}
21: 	return required_bits;
22: }
23: 
24: static bool CanUsePerfectHashAggregate(ClientContext &context, LogicalAggregate &op, vector<idx_t> &bits_per_group) {
25: 	if (op.grouping_sets.size() > 1 || !op.grouping_functions.empty()) {
26: 		return false;
27: 	}
28: 	idx_t perfect_hash_bits = 0;
29: 	if (op.group_stats.empty()) {
30: 		op.group_stats.resize(op.groups.size());
31: 	}
32: 	for (idx_t group_idx = 0; group_idx < op.groups.size(); group_idx++) {
33: 		auto &group = op.groups[group_idx];
34: 		auto &stats = op.group_stats[group_idx];
35: 
36: 		switch (group->return_type.InternalType()) {
37: 		case PhysicalType::INT8:
38: 		case PhysicalType::INT16:
39: 		case PhysicalType::INT32:
40: 		case PhysicalType::INT64:
41: 			break;
42: 		default:
43: 			// we only support simple integer types for perfect hashing
44: 			return false;
45: 		}
46: 		// check if the group has stats available
47: 		auto &group_type = group->return_type;
48: 		if (!stats) {
49: 			// no stats, but we might still be able to use perfect hashing if the type is small enough
50: 			// for small types we can just set the stats to [type_min, type_max]
51: 			switch (group_type.InternalType()) {
52: 			case PhysicalType::INT8:
53: 				stats = make_unique<NumericStatistics>(group_type, Value::MinimumValue(group_type),
54: 				                                       Value::MaximumValue(group_type));
55: 				break;
56: 			case PhysicalType::INT16:
57: 				stats = make_unique<NumericStatistics>(group_type, Value::MinimumValue(group_type),
58: 				                                       Value::MaximumValue(group_type));
59: 				break;
60: 			default:
61: 				// type is too large and there are no stats: skip perfect hashing
62: 				return false;
63: 			}
64: 			// we had no stats before, so we have no clue if there are null values or not
65: 			stats->validity_stats = make_unique<ValidityStatistics>(true);
66: 		}
67: 		auto &nstats = (NumericStatistics &)*stats;
68: 
69: 		if (nstats.min.is_null || nstats.max.is_null) {
70: 			return false;
71: 		}
72: 		// we have a min and a max value for the stats: use that to figure out how many bits we have
73: 		// we add two here, one for the NULL value, and one to make the computation one-indexed
74: 		// (e.g. if min and max are the same, we still need one entry in total)
75: 		int64_t range;
76: 		switch (group_type.InternalType()) {
77: 		case PhysicalType::INT8:
78: 			range = int64_t(nstats.max.GetValueUnsafe<int8_t>()) - int64_t(nstats.min.GetValueUnsafe<int8_t>());
79: 			break;
80: 		case PhysicalType::INT16:
81: 			range = int64_t(nstats.max.GetValueUnsafe<int16_t>()) - int64_t(nstats.min.GetValueUnsafe<int16_t>());
82: 			break;
83: 		case PhysicalType::INT32:
84: 			range = int64_t(nstats.max.GetValueUnsafe<int32_t>()) - int64_t(nstats.min.GetValueUnsafe<int32_t>());
85: 			break;
86: 		case PhysicalType::INT64:
87: 			if (!TrySubtractOperator::Operation(nstats.max.GetValueUnsafe<int64_t>(),
88: 			                                    nstats.min.GetValueUnsafe<int64_t>(), range)) {
89: 				return false;
90: 			}
91: 			break;
92: 		default:
93: 			throw InternalException("Unsupported type for perfect hash (should be caught before)");
94: 		}
95: 		// bail out on any range bigger than 2^32
96: 		if (range >= NumericLimits<int32_t>::Maximum()) {
97: 			return false;
98: 		}
99: 		range += 2;
100: 		// figure out how many bits we need
101: 		idx_t required_bits = RequiredBitsForValue(range);
102: 		bits_per_group.push_back(required_bits);
103: 		perfect_hash_bits += required_bits;
104: 		// check if we have exceeded the bits for the hash
105: 		if (perfect_hash_bits > context.perfect_ht_threshold) {
106: 			// too many bits for perfect hash
107: 			return false;
108: 		}
109: 	}
110: 	for (auto &expression : op.expressions) {
111: 		auto &aggregate = (BoundAggregateExpression &)*expression;
112: 		if (aggregate.distinct || !aggregate.function.combine) {
113: 			// distinct aggregates are not supported in perfect hash aggregates
114: 			return false;
115: 		}
116: 	}
117: 	return true;
118: }
119: 
120: unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalAggregate &op) {
121: 	unique_ptr<PhysicalOperator> groupby;
122: 	D_ASSERT(op.children.size() == 1);
123: 
124: 	bool all_combinable = true;
125: 	for (auto &expression : op.expressions) {
126: 		auto &aggregate = (BoundAggregateExpression &)*expression;
127: 		if (!aggregate.function.combine) {
128: 			// unsupported aggregate for simple aggregation: use hash aggregation
129: 			all_combinable = false;
130: 			break;
131: 		}
132: 	}
133: 
134: 	auto plan = CreatePlan(*op.children[0]);
135: 
136: 	plan = ExtractAggregateExpressions(move(plan), op.expressions, op.groups);
137: 
138: 	if (op.groups.empty()) {
139: 		// no groups, check if we can use a simple aggregation
140: 		// special case: aggregate entire columns together
141: 		bool use_simple_aggregation = true;
142: 		for (auto &expression : op.expressions) {
143: 			auto &aggregate = (BoundAggregateExpression &)*expression;
144: 			if (!aggregate.function.simple_update || aggregate.distinct) {
145: 				// unsupported aggregate for simple aggregation: use hash aggregation
146: 				use_simple_aggregation = false;
147: 				break;
148: 			}
149: 		}
150: 		if (use_simple_aggregation) {
151: 			groupby = make_unique_base<PhysicalOperator, PhysicalSimpleAggregate>(
152: 			    op.types, move(op.expressions), all_combinable, op.estimated_cardinality);
153: 		} else {
154: 			groupby = make_unique_base<PhysicalOperator, PhysicalHashAggregate>(context, op.types, move(op.expressions),
155: 			                                                                    op.estimated_cardinality);
156: 		}
157: 	} else {
158: 		// groups! create a GROUP BY aggregator
159: 		// use a perfect hash aggregate if possible
160: 		vector<idx_t> required_bits;
161: 		if (CanUsePerfectHashAggregate(context, op, required_bits)) {
162: 			groupby = make_unique_base<PhysicalOperator, PhysicalPerfectHashAggregate>(
163: 			    context, op.types, move(op.expressions), move(op.groups), move(op.group_stats), move(required_bits),
164: 			    op.estimated_cardinality);
165: 		} else {
166: 			groupby = make_unique_base<PhysicalOperator, PhysicalHashAggregate>(
167: 			    context, op.types, move(op.expressions), move(op.groups), move(op.grouping_sets),
168: 			    move(op.grouping_functions), op.estimated_cardinality);
169: 		}
170: 	}
171: 	groupby->children.push_back(move(plan));
172: 	return groupby;
173: }
174: 
175: unique_ptr<PhysicalOperator>
176: PhysicalPlanGenerator::ExtractAggregateExpressions(unique_ptr<PhysicalOperator> child,
177:                                                    vector<unique_ptr<Expression>> &aggregates,
178:                                                    vector<unique_ptr<Expression>> &groups) {
179: 	vector<unique_ptr<Expression>> expressions;
180: 	vector<LogicalType> types;
181: 
182: 	for (auto &group : groups) {
183: 		auto ref = make_unique<BoundReferenceExpression>(group->return_type, expressions.size());
184: 		types.push_back(group->return_type);
185: 		expressions.push_back(move(group));
186: 		group = move(ref);
187: 	}
188: 
189: 	for (auto &aggr : aggregates) {
190: 		auto &bound_aggr = (BoundAggregateExpression &)*aggr;
191: 		for (auto &child : bound_aggr.children) {
192: 			auto ref = make_unique<BoundReferenceExpression>(child->return_type, expressions.size());
193: 			types.push_back(child->return_type);
194: 			expressions.push_back(move(child));
195: 			child = move(ref);
196: 		}
197: 		if (bound_aggr.filter) {
198: 			auto &filter = bound_aggr.filter;
199: 			auto ref = make_unique<BoundReferenceExpression>(filter->return_type, expressions.size());
200: 			types.push_back(filter->return_type);
201: 			expressions.push_back(move(filter));
202: 			bound_aggr.filter = move(ref);
203: 		}
204: 	}
205: 	if (expressions.empty()) {
206: 		return child;
207: 	}
208: 	auto projection = make_unique<PhysicalProjection>(move(types), move(expressions), child->estimated_cardinality);
209: 	projection->children.push_back(move(child));
210: 	return move(projection);
211: }
212: 
213: } // namespace duckdb
[end of src/execution/physical_plan/plan_aggregate.cpp]
[start of src/include/duckdb/execution/operator/aggregate/physical_simple_aggregate.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/aggregate/physical_simple_aggregate.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/execution/physical_operator.hpp"
12: #include "duckdb/planner/expression.hpp"
13: 
14: namespace duckdb {
15: 
16: //! PhysicalSimpleAggregate is an aggregate operator that can only perform aggregates (1) without any groups, and (2)
17: //! without any DISTINCT aggregates
18: class PhysicalSimpleAggregate : public PhysicalOperator {
19: public:
20: 	PhysicalSimpleAggregate(vector<LogicalType> types, vector<unique_ptr<Expression>> expressions, bool all_combinable,
21: 	                        idx_t estimated_cardinality);
22: 
23: 	//! The aggregates that have to be computed
24: 	vector<unique_ptr<Expression>> aggregates;
25: 	//! Whether or not all aggregates are trivially combinable. Aggregates that are trivially combinable can be
26: 	//! parallelized.
27: 	bool all_combinable;
28: 
29: public:
30: 	// Source interface
31: 	unique_ptr<GlobalSourceState> GetGlobalSourceState(ClientContext &context) const override;
32: 	void GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,
33: 	             LocalSourceState &lstate) const override;
34: 
35: public:
36: 	// Sink interface
37: 	SinkResultType Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate,
38: 	                    DataChunk &input) const override;
39: 	void Combine(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate) const override;
40: 	SinkFinalizeType Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
41: 	                          GlobalSinkState &gstate) const override;
42: 
43: 	unique_ptr<LocalSinkState> GetLocalSinkState(ExecutionContext &context) const override;
44: 	unique_ptr<GlobalSinkState> GetGlobalSinkState(ClientContext &context) const override;
45: 
46: 	string ParamsToString() const override;
47: 
48: 	bool IsSink() const override {
49: 		return true;
50: 	}
51: 
52: 	bool ParallelSink() const override {
53: 		// we can only parallelize if all aggregates are combinable
54: 		return all_combinable;
55: 	}
56: };
57: 
58: } // namespace duckdb
[end of src/include/duckdb/execution/operator/aggregate/physical_simple_aggregate.hpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: