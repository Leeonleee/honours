You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Parquet export alters table names
#### What happens?
When exporting duckdb tables to parquet, the resulting parquet files are named based on sanitized versions of the table names: all uppercase are converted to lowercase (ok fine), and all digits are converted to underscores, and a number is prepended.  Luckily this can't create collisions of names, since at least all tables are then prefixed by a number, but I would still prefer the original table names.  
`duckdb` helpfully generates a `load.sql` file mapping the mangled file-names back to their original table-names, which is helpful.  However, because `duckdb` now has the *amazing* ability to read the parquet files directly, I really don't want to run `load.sql` at all, I just want to point duckdb at these nice portable parquet files it just made.  It would be wonderful if duckdb would generate a `parquet.sql` file in this case, or more simply, just not mangle the table names used for the parquet files?

No doubt there are good reasons I'm overlooking for doing this, maybe related to #2075 and backwards compatibility, but it seems unnecessary here?   


#### To Reproduce

https://duckdb.org/docs/sql/statements/export

#### Environment (please complete the following information):
 - OS: Linux
 - DuckDB Version: current release
 - DuckDB Client: R

#### Before Submitting

- [x] **Have you tried this on the latest `master` branch?**
* **Python**: `pip install duckdb --upgrade --pre`
* **R**: `install.packages("https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz", repos = NULL)`
* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.

- [x] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**

When non utf8 data is passed to `duckdb_bind_varchar_length` it will crash the process instead of returning `DuckDBError`
#### What happens?

When you use `duckdb_bind_varchar_length`, and pass non utf8 data (by accident), it crashes the process, instead of returning `DuckDbError`.

I know you should use blobs for arbitrary binary data, but you don't always have control over input in strings. Would it be possible to catch the exception below and return an error?

Output: 
```
libc++abi: terminating with uncaught exception of type duckdb::Exception: String value is not valid UTF8
make: *** [test] Abort trap: 6
```

#### To Reproduce

Call `duckdb_varchar_length` with as input `[0, 40, 41]`, and length `3`.

#### Environment (please complete the following information):
 - OS: macOS
 - DuckDB Version: 0.3.2
 - DuckDB Client: WIP erlang interface.

#### Before Submitting

Tried the latest libduckdb.zip from GH


### FYI

Testcase looks like this:

```erlang
bind_varchar_test() ->
    {ok, Db} = educkdb:open(":memory:"),
    {ok, Conn} = educkdb:connect(Db),

    {ok, [], []} = q(Conn, "create table test(a varchar(10), b varchar(200));"),
    {ok, Insert} = educkdb:prepare(Conn, "insert into test values($1, $2);"),

    ok = educkdb:bind_varchar(Insert, 1, "hello"),
    ok = educkdb:bind_varchar(Insert, 2, "world"),

    {ok, _, [[1]]} = x(Insert),

    ok = educkdb:bind_varchar(Insert, 1, <<"ðŸ˜€"/utf8>>),
    ok = educkdb:bind_varchar(Insert, 2, <<0, "()">>),   % <-- crash here

    {ok, _, [[1]]} = x(Insert),

    {ok, _, [
             [<<"hello">>, <<"world">>],
             [<<"ðŸ˜€"/utf8>>, <<"1234567890">>]
            ]} = q(Conn, "select * from test order by a"),
    ok.
```


</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
44: 
45: 
[end of README.md]
[start of .github/workflows/LinuxRelease.yml]
1: name: LinuxRelease
2: on:
3:   push:
4:     paths-ignore:
5:       - '**.md'
6:   pull_request:
7:     paths-ignore:
8:       - '**.md'
9:       - 'tools/nodejs/**'
10:       - 'tools/pythonpkg/**'
11:       - 'tools/rpkg/**'
12:       - '.github/workflows/NodeJS.yml'
13:       - '.github/workflows/Python.yml'
14:       - '.github/workflows/R.yml'
15: 
16: concurrency:
17:   group: ${{ github.workflow }}-${{ github.ref }}-${{ github.head_ref || '' }}-${{ github.base_ref || '' }}-${{ github.ref != 'refs/heads/master' || github.sha }}
18:   cancel-in-progress: true
19: 
20: defaults:
21:   run:
22:     shell: bash
23: 
24: env:
25:   GH_TOKEN: ${{ secrets.GH_TOKEN }}
26:   TWINE_PASSWORD: ${{ secrets.TWINE_PASSWORD }}
27:   AWS_ACCESS_KEY_ID: AKIAVBLKPL2ZW2T7TYFQ
28:   AWS_SECRET_ACCESS_KEY: ${{ secrets.NODE_PRE_GYP_SECRETACCESSKEY }}
29:   NODE_AUTH_TOKEN: ${{secrets.NODE_AUTH_TOKEN}}
30: 
31: jobs:
32:  linux-release-64:
33:     name: Linux (64 Bit)
34:     runs-on: ubuntu-latest
35:     container: ubuntu:16.04
36:     env:
37:       GEN: ninja
38:       BUILD_BENCHMARK: 1
39:       BUILD_ICU: 1
40:       BUILD_TPCH: 1
41:       BUILD_FTS: 1
42:       BUILD_REST: 1
43:       BUILD_JDBC: 1
44:       BUILD_JSON: 1
45:       BUILD_EXCEL: 1
46:       TREAT_WARNINGS_AS_ERRORS: 1
47:       FORCE_WARN_UNUSED: 1
48: 
49:     steps:
50:     - name: Install
51:       run: |
52:         apt-get update -y -qq
53:         apt-get install -y -qq software-properties-common
54:         add-apt-repository ppa:git-core/ppa
55:         apt-get update -y -qq
56:         apt-get install -y -qq git ninja-build make gcc-multilib g++-multilib libssl-dev wget openjdk-8-jdk zip maven unixodbc-dev
57: 
58:     - name: Install Python 3.7
59:       run: |
60:         wget https://www.python.org/ftp/python/3.7.12/Python-3.7.12.tgz
61:         tar xvf Python-3.7.12.tgz
62:         cd Python-3.7.12
63:         mkdir -p pythonbin
64:         ./configure --with-ensurepip=install
65:         make -j
66:         make install
67:         python3.7 --version
68:         python3.7 -m pip install pip
69:         python3.7 -m pip install requests
70: 
71:     - uses: actions/checkout@v2
72:       with:
73:         fetch-depth: 0
74: 
75:     - name: Version Check
76:       run: |
77:         ldd --version ldd
78:         python3.7 --version
79:         git --version
80: 
81:     - name: Install CMake
82:       run: |
83:         wget https://github.com/Kitware/CMake/releases/download/v3.21.3/cmake-3.21.3-linux-x86_64.sh
84:         chmod +x cmake-3.21.3-linux-x86_64.sh
85:         ./cmake-3.21.3-linux-x86_64.sh --skip-license --prefix=/usr/local
86: 
87:     - name: Build
88:       run: STATIC_LIBCPP=1 BUILD_ODBC=1 make
89: 
90:     - name: Test
91:       run: make allunit
92: 
93:     - name: Tools Tests
94:       run: |
95:         python3.7 tools/shell/shell-test.py build/release/duckdb
96:         python3.7 tools/rest/test_the_rest.py build/release/tools/rest
97:         java -cp build/release/tools/jdbc/duckdb_jdbc.jar org.duckdb.test.TestDuckDBJDBC
98: 
99:     - name: Examples
100:       run: |
101:         (cd examples/embedded-c; make)
102:         (cd examples/embedded-c++; make)
103:         (cd examples/jdbc; make; make maven)
104:         build/release/benchmark/benchmark_runner benchmark/tpch/sf1/q01.benchmark
105:         build/release/duckdb -c "COPY (SELECT 42) TO '/dev/stdout' (FORMAT PARQUET)" | cat
106: 
107:     - name: Deploy
108:       run: |
109:         python3.7 scripts/amalgamation.py
110:         zip -j duckdb_cli-linux-amd64.zip build/release/duckdb
111:         zip -j libduckdb-linux-amd64.zip build/release/src/libduckdb*.so src/amalgamation/duckdb.hpp src/include/duckdb.h
112:         zip -j libduckdb-src.zip src/amalgamation/duckdb.hpp src/amalgamation/duckdb.cpp src/include/duckdb.h
113:         zip -j duckdb_rest-linux-amd64.zip build/release/tools/rest/duckdb_rest_server
114:         zip -j duckdb_odbc-linux-amd64.zip build/release/tools/odbc/libduckdb_odbc.so
115:         python3.7 scripts/asset-upload-gha.py libduckdb-src.zip libduckdb-linux-amd64.zip duckdb_cli-linux-amd64.zip duckdb_rest-linux-amd64.zip duckdb_jdbc-linux-amd64.jar=build/release/tools/jdbc/duckdb_jdbc.jar duckdb_odbc-linux-amd64.zip
116: 
117:     - uses: actions/upload-artifact@v2
118:       with:
119:         name: duckdb-binaries-linux
120:         path: |
121:           libduckdb-linux-amd64.zip
122:           duckdb_cli-linux-amd64.zip
123:           build/release/tools/jdbc/duckdb_jdbc.jar
124: 
125: 
126:  linux-extensions-64:
127:     name: Linux Extensions (64 Bit)
128:     runs-on: ubuntu-latest
129:     container: ubuntu:16.04
130:     needs: linux-release-64
131:     env:
132:       GEN: ninja
133:       BUILD_VISUALIZER: 1
134:       BUILD_ICU: 1
135:       BUILD_TPCH: 1
136:       BUILD_TPCDS: 1
137:       BUILD_FTS: 1
138:       BUILD_HTTPFS: 1
139:       BUILD_JSON: 1
140:       BUILD_EXCEL: 1
141:       BUILD_SUBSTRAIT_EXTENSION: 1
142:       TREAT_WARNINGS_AS_ERRORS: 1
143:       FORCE_WARN_UNUSED: 1
144:       STATIC_OPENSSL: 1
145:       DISABLE_BUILTIN_EXTENSIONS: 1
146:       OPENSSL_ROOT_DIR: /usr/local/ssl
147:       AWS_ACCESS_KEY_ID: ${{secrets.S3_ID}}
148:       AWS_SECRET_ACCESS_KEY: ${{secrets.S3_KEY}}
149:       AWS_DEFAULT_REGION: us-east-1
150: 
151:     steps:
152:     - name: Install
153:       run: |
154:         apt-get update -y -qq
155:         apt-get install -y -qq software-properties-common
156:         add-apt-repository ppa:git-core/ppa
157:         apt-get update -y -qq
158:         apt-get install -y -qq git ninja-build make gcc-multilib g++-multilib libssl-dev wget openjdk-8-jdk zip maven unixodbc-dev libffi-dev
159: 
160:     - name: Install Python 3.7
161:       run: |
162:         wget https://www.python.org/ftp/python/3.7.12/Python-3.7.12.tgz
163:         tar xvf Python-3.7.12.tgz
164:         cd Python-3.7.12
165:         mkdir -p pythonbin
166:         ./configure --with-ensurepip=install
167:         make -j
168:         make install
169:         python3.7 --version
170:         python3.7 -m pip install pip
171:         python3.7 -m pip install requests awscli
172: 
173:     - name: Install OpenSSL 1.1.1
174:       run: |
175:         apt-get install build-essential checkinstall zlib1g-dev -y -qq
176:         wget https://www.openssl.org/source/openssl-1.1.1c.tar.gz
177:         tar -xf openssl-1.1.1c.tar.gz
178:         cd openssl-1.1.1c
179:         ./config --prefix=/usr/local/ssl --openssldir=/usr/local/ssl -static zlib
180:         make
181:         make install
182:         echo "/usr/local/ssl/lib" > /etc/ld.so.conf.d/openssl-1.1.1c.conf
183:         ldconfig -v
184: 
185:     - uses: actions/checkout@v2
186:       with:
187:         fetch-depth: 0
188: 
189:     - name: Version Check
190:       run: |
191:         ldd --version ldd
192:         python3.7 --version
193:         git --version
194: 
195:     - name: Install CMake
196:       run: |
197:         wget https://github.com/Kitware/CMake/releases/download/v3.21.3/cmake-3.21.3-linux-x86_64.sh
198:         chmod +x cmake-3.21.3-linux-x86_64.sh
199:         ./cmake-3.21.3-linux-x86_64.sh --skip-license --prefix=/usr/local
200: 
201:     - name: Build
202:       run: |
203:         make
204:         rm build/release/src/libduckdb*
205: 
206:     - name: Deploy
207:       run: |
208:         if [[ "$GITHUB_REF" =~ ^(refs/heads/master|refs/tags/v.+)$ && "$GITHUB_REPOSITORY" = "duckdb/duckdb" ]] ; then
209:           ./scripts/extension-upload.sh linux_amd64
210:           ./scripts/extension-upload-test.sh
211:         else
212:           ./scripts/extension-upload-test.sh local
213:         fi
214: 
215: 
216: 
217:  linux-release-32:
218:     name: Linux (32 Bit)
219:     runs-on: ubuntu-latest
220:     container: ubuntu:16.04
221:     needs: linux-release-64
222:     env:
223:       GEN: ninja
224: 
225:     steps:
226:     - name: Install
227:       run: |
228:         apt-get update -y -qq
229:         apt-get install -y -qq software-properties-common
230:         add-apt-repository ppa:git-core/ppa
231:         apt-get update -y -qq
232:         apt-get install -y -qq git make ninja-build libc6-dev-i386 gcc-multilib g++-multilib lib32readline6-dev libssl-dev wget openjdk-8-jdk zip
233: 
234:     - name: Install Python 3.7
235:       run: |
236:         wget https://www.python.org/ftp/python/3.7.12/Python-3.7.12.tgz
237:         tar xvf Python-3.7.12.tgz
238:         cd Python-3.7.12
239:         mkdir -p pythonbin
240:         ./configure --with-ensurepip=install
241:         make -j
242:         make install
243:         python3.7 --version
244:         python3.7 -m pip install pip
245:         python3.7 -m pip install requests
246: 
247:     - uses: actions/checkout@v2
248:       with:
249:         fetch-depth: 0
250: 
251:     - name: Version Check
252:       run: |
253:         ldd --version ldd
254:         python3.7 --version
255:         git --version
256: 
257:     - name: Install CMake
258:       run: |
259:         wget https://github.com/Kitware/CMake/releases/download/v3.21.3/cmake-3.21.3-linux-x86_64.sh
260:         chmod +x cmake-3.21.3-linux-x86_64.sh
261:         ./cmake-3.21.3-linux-x86_64.sh --skip-license --prefix=/usr/local
262: 
263:     - name: Build
264:       run: |
265:         mkdir -p build/release
266:         (cd build/release && cmake -DSTATIC_LIBCPP=1 -DJDBC_DRIVER=1 -DBUILD_ICU_EXTENSION=1 -DBUILD_PARQUET_EXTENSION=1 -DBUILD_FTS_EXTENSION=1 -DBUILD_JSON_EXTENSION=1 -DBUILD_EXCEL_EXTENSION=1 -DFORCE_32_BIT=1 -DCMAKE_BUILD_TYPE=Release ../.. && cmake --build .)
267: 
268:     - name: Test
269:       run: build/release/test/unittest "*"
270: 
271:     - name: Deploy
272:       run: |
273:         python3.7 scripts/amalgamation.py
274:         zip -j duckdb_cli-linux-i386.zip build/release/duckdb
275:         zip -j libduckdb-linux-i386.zip build/release/src/libduckdb*.so src/amalgamation/duckdb.hpp src/include/duckdb.h
276:         python3.7 scripts/asset-upload-gha.py libduckdb-linux-i386.zip duckdb_cli-linux-i386.zip duckdb_jdbc-linux-i386.jar=build/release/tools/jdbc/duckdb_jdbc.jar
277: 
278:     - uses: actions/upload-artifact@v2
279:       with:
280:         name: duckdb-binaries-linux
281:         path: |
282:           libduckdb-linux-i386.zip
283:           duckdb_cli-linux-i386.zip
284:           build/release/tools/jdbc/duckdb_jdbc.jar
285: 
286: 
287:  linux-rpi:
288:     name: Linux (Raspberry Pi)
289:     runs-on: ubuntu-20.04
290:     needs: linux-release-64
291:     steps:
292:     - uses: actions/checkout@v2
293:       with:
294:         fetch-depth: 0
295: 
296:     - uses: actions/setup-python@v2
297:       with:
298:         python-version: '3.7'
299: 
300:     - name: Install
301:       run: |
302:         sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
303:         git clone https://github.com/raspberrypi/tools --depth=1 rpi-tools
304: 
305:     - name: Build
306:       run: |
307:         export TOOLCHAIN=`pwd`/rpi-tools
308:         mkdir -p build/release
309:         cd build/release
310:         cmake -G Ninja -DBUILD_TPCH_EXTENSION=1 -DBUILD_TPCDS_EXTENSION=1 -DDUCKDB_RPI_TOOLCHAIN_PREFIX=$TOOLCHAIN -DBUILD_UNITTESTS=0 -DCMAKE_TOOLCHAIN_FILE=../../scripts/raspberry-pi-cmake-toolchain.cmake ../../
311:         cmake --build .
312:         file duckdb
313: 
314:     - name: Deploy
315:       run: |
316:         python scripts/amalgamation.py
317:         zip -j duckdb_cli-linux-rpi.zip build/release/duckdb
318:         zip -j libduckdb-linux-rpi.zip build/release/src/libduckdb*.so src/amalgamation/duckdb.hpp src/include/duckdb.h
319:         python scripts/asset-upload-gha.py libduckdb-linux-rpi.zip duckdb_cli-linux-rpi.zip
320: 
321:     - uses: actions/upload-artifact@v2
322:       with:
323:         name: duckdb-binaries-rpi
324:         path: |
325:           libduckdb-linux-rpi.zip
326:           duckdb_cli-linux-rpi.zip
327: 
328: 
329: 
330: 
331:  old-gcc:
332:     name: GCC 4.8
333:     runs-on: ubuntu-18.04
334:     needs: linux-release-64
335: 
336:     env:
337:       CC: gcc-4.8
338:       CXX: g++-4.8
339: 
340:     steps:
341:     - uses: actions/checkout@v2
342:       with:
343:         fetch-depth: 0
344: 
345:     - uses: actions/setup-python@v2
346:       with:
347:         python-version: '3.7'
348: 
349:     - name: Install
350:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq g++-4.8 binutils
351: 
352:     - name: Build
353:       run: make release
354: 
355:     - name: Test
356:       run: make allunit
357: 
358:  centos:
359:     name: CentOS 7
360:     runs-on: ubuntu-latest
361:     container: centos:7
362:     needs: linux-release-64
363:     steps:
364:     - uses: actions/checkout@v2
365:       with:
366:         fetch-depth: 0
367: 
368:     - name: Install
369:       run: yum install -y gcc gcc-c++ git cmake make
370: 
371:     - name: Build
372:       run: make release
373: 
374:     - name: Test
375:       run: ./build/release/test/unittest
376: 
377:  release-assert:
378:     name: Release Assertions
379:     runs-on: ubuntu-20.04
380:     needs: linux-release-64
381:     env:
382:       CC: gcc-10
383:       CXX: g++-10
384:       GEN: ninja
385:       BUILD_ICU: 1
386:       BUILD_TPCH: 1
387:       BUILD_TPCDS: 1
388:       BUILD_FTS: 1
389:       BUILD_EXCEL: 1
390:       BUILD_VISUALIZER: 1
391:       BUILD_JSON: 1
392:       DISABLE_SANITIZER: 1
393: 
394:     steps:
395:     - uses: actions/checkout@v2
396:       with:
397:         fetch-depth: 0
398: 
399:     - name: Install
400:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
401: 
402:     - name: Build
403:       run: make relassert
404: 
405:     - name: Test
406:       run: |
407:           python3 scripts/run_tests_one_by_one.py build/relassert/test/unittest "*"
408: 
409: 
410:  vector-sizes:
411:     name: Vector Sizes
412:     runs-on: ubuntu-20.04
413:     needs: linux-release-64
414:     env:
415:       CC: gcc-10
416:       CXX: g++-10
417: 
418:     steps:
419:     - uses: actions/checkout@v2
420:       with:
421:         fetch-depth: 0
422: 
423:     - uses: actions/setup-python@v2
424:       with:
425:         python-version: '3.7'
426: 
427:     - name: Test
428:       run: python scripts/test_vector_sizes.py
429: 
430:  linux-wasm-release:
431:     name: WebAssembly Release
432:     runs-on: ubuntu-20.04
433:     needs: linux-release-64
434:     steps:
435:     - uses: actions/checkout@v2
436:       with:
437:         fetch-depth: 0
438: 
439:     - name: Build Amalgamation
440:       run: python scripts/amalgamation.py
441: 
442:     - name: Setup
443:       run: ./scripts/wasm_configure.sh
444: 
445:     - name: Build Library Module
446:       run: ./scripts/wasm_build_lib.sh Release
447: 
448:     - name: Build Test Module
449:       run: ./scripts/wasm_build_test.sh Release
450: 
451:     - name: Test WASM Module
452:       run: node ./test/wasm/hello_wasm_test.js
453: 
454:     - name: Package
455:       run: |
456:         zip -j duckdb-wasm32-nothreads.zip ./.wasm/build/duckdb.wasm
457:         python scripts/asset-upload-gha.py duckdb-wasm32-nothreads.zip
458: 
459:     - uses: actions/upload-artifact@v2
460:       with:
461:         name: duckdb-wasm32-nothreads
462:         path: |
463:           duckdb-wasm32-nothreads.zip
464: 
465:  symbol-leakage:
466:     name: Symbol Leakage
467:     runs-on: ubuntu-20.04
468:     needs: linux-release-64
469: 
470:     steps:
471:     - uses: actions/checkout@v2
472:       with:
473:         fetch-depth: 0
474: 
475:     - uses: actions/setup-python@v2
476:       with:
477:         python-version: '3.7'
478: 
479:     - name: Build
480:       run: make
481: 
482:     - name: Symbol Leakage Test
483:       run: python3.7 scripts/exported_symbols_check.py build/release/src/libduckdb*.so
484: 
485:  linux-httpfs:
486:     name: Linux HTTPFS
487:     runs-on: ubuntu-20.04
488:     needs: linux-release-64
489:     env:
490:       BUILD_VISUALIZER: 1
491:       BUILD_HTTPFS: 1
492:       S3_TEST_SERVER_AVAILABLE: 1
493: 
494:     steps:
495:     - uses: actions/checkout@v2
496:       with:
497:         fetch-depth: 0
498: 
499:     - uses: actions/setup-python@v2
500:       with:
501:         python-version: '3.7'
502: 
503:     - uses: actions/setup-node@v2
504:       with:
505:         node-version: '16'
506: 
507:     - name: Build
508:       run: make
509: 
510:     - name: Start S3/HTTP test server
511:       run: |
512:         sudo ./scripts/install_s3_test_server.sh
513:         mkdir -p /tmp/s3rver
514:         ./scripts/run_s3_test_server.sh &> /tmp/s3rver/access_log.txt &
515: 
516:     - name: Test
517:       run: make allunit
518: 
519:     - name: Check S3/HTTP server log
520:       if: always()
521:       run: cat /tmp/s3rver/access_log.txt
522: 
523:  amalgamation-tests:
524:     name: Amalgamation Tests
525:     runs-on: ubuntu-20.04
526:     env:
527:       CC: clang
528:       CXX: clang++
529: 
530:     steps:
531:     - uses: actions/checkout@v2
532:       with:
533:         fetch-depth: 0
534: 
535:     - uses: actions/setup-python@v2
536:       with:
537:         python-version: '3.7'
538: 
539:     - name: Install LLVM and Clang
540:       uses: KyleMayes/install-llvm-action@v1
541:       with:
542:         version: "10.0"
543: 
544:     - name: Generate Amalgamation
545:       run:  |
546:           python scripts/amalgamation.py --extended
547:           python scripts/parquet_amalgamation.py
548:           clang++ -std=c++17 -Isrc/amalgamation src/amalgamation/parquet-amalgamation.cpp src/amalgamation/duckdb.cpp -emit-llvm -S -O0
[end of .github/workflows/LinuxRelease.yml]
[start of .github/workflows/Main.yml]
1: name: Main
2: on:
3:   push:
4:     paths-ignore:
5:       - '**.md'
6:   pull_request:
7:     paths-ignore:
8:       - '**.md'
9:       - 'tools/nodejs/**'
10:       - 'tools/pythonpkg/**'
11:       - 'tools/rpkg/**'
12:       - '.github/workflows/NodeJS.yml'
13:       - '.github/workflows/Python.yml'
14:       - '.github/workflows/R.yml'
15: 
16: concurrency:
17:   group: ${{ github.workflow }}-${{ github.ref }}-${{ github.head_ref || '' }}-${{ github.base_ref || '' }}-${{ github.ref != 'refs/heads/master' || github.sha }}
18:   cancel-in-progress: true
19: 
20: defaults:
21:   run:
22:     shell: bash
23: 
24: env:
25:   GH_TOKEN: ${{ secrets.GH_TOKEN }}
26:   TWINE_PASSWORD: ${{ secrets.TWINE_PASSWORD }}
27:   AWS_ACCESS_KEY_ID: AKIAVBLKPL2ZW2T7TYFQ
28:   AWS_SECRET_ACCESS_KEY: ${{ secrets.NODE_PRE_GYP_SECRETACCESSKEY }}
29:   NODE_AUTH_TOKEN: ${{secrets.NODE_AUTH_TOKEN}}
30: 
31: jobs:
32:  linux-debug:
33:     name: Linux Debug
34:     runs-on: ubuntu-20.04
35: 
36:     env:
37:       CC: ccache gcc-10
38:       CXX: ccache g++-10
39:       TREAT_WARNINGS_AS_ERRORS: 1
40:       BUILD_VISUALIZER: 1
41:       BUILD_TPCH: 1
42:       BUILD_TPCDS: 1
43:       BUILD_FTS: 1
44:       BUILD_ICU: 1
45:       BUILD_JSON: 1
46:       BUILD_EXCEL: 1
47:       BUILD_PARQUET: 1
48:       BUILD_SUBSTRAIT_EXTENSION: 1
49:       GEN: ninja
50: 
51:     steps:
52:     - uses: actions/checkout@v2
53:       with:
54:         fetch-depth: 0
55: 
56:     - name: Install
57:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build ccache
58: 
59:     - name: Build
60:       run:  make debug
61: 
62:     - name: Test
63:       run: make unittestci
64: 
65: 
66:  force-storage:
67:     name: Force Storage
68:     runs-on: ubuntu-20.04
69:     needs: linux-debug
70:     env:
71:       CC: gcc-10
72:       CXX: g++-10
73:       GEN: ninja
74:       BUILD_ICU: 1
75:       BUILD_PARQUET: 1
76:       BUILD_TPCH: 1
77:       BUILD_TPCDS: 1
78:       BUILD_FTS: 1
79:       BUILD_VISUALIZER: 1
80:       BUILD_JSON: 1
81:       BUILD_EXCEL: 1
82: 
83:     steps:
84:     - uses: actions/checkout@v2
85:       with:
86:         fetch-depth: 0
87: 
88:     - name: Install
89:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
90: 
91:     - name: Build
92:       run: make reldebug
93: 
94:     - name: Test
95:       run: build/reldebug/test/unittest "*" --force-storage
96: 
97: 
98:  linux-arrow:
99:       name: Linux Debug (Arrow Tests)
100:       runs-on: ubuntu-20.04
101:       needs: linux-debug
102:       env:
103:         CC: gcc-10
104:         CXX: g++-10
105:         TREAT_WARNINGS_AS_ERRORS: 1
106:         GEN: ninja
107: 
108:       steps:
109:       - uses: actions/checkout@v2
110:         with:
111:           fetch-depth: 0
112: 
113:       - name: Install
114:         run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
115: 
116:       - name: Build
117:         run: BUILD_ARROW_ABI_TEST=1 make debug
118: 
119:       - name: Test
120:         run: make unittestarrow
121: 
122:  threadsan:
123:     name: Thread Sanitizer
124:     runs-on: ubuntu-20.04
125:     needs: linux-debug
126:     env:
127:       CC: gcc-10
128:       CXX: g++-10
129:       GEN: ninja
130:       BUILD_ICU: 1
131:       BUILD_TPCH: 1
132:       BUILD_TPCDS: 1
133:       BUILD_FTS: 1
134:       BUILD_VISUALIZER: 1
135:       BUILD_JSON: 1
136:       BUILD_EXCEL: 1
137:       TSAN_OPTIONS: suppressions=.sanitizer-thread-suppressions.txt
138: 
139:     steps:
140:     - uses: actions/checkout@v2
141:       with:
142:         fetch-depth: 0
143: 
144:     - name: Install
145:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
146: 
147:     - name: Build
148:       run: THREADSAN=1 make reldebug
149: 
150:     - name: Test
151:       run: |
152:           python3 scripts/run_tests_one_by_one.py build/reldebug/test/unittest
153:           python3 scripts/run_tests_one_by_one.py build/reldebug/test/unittest "[intraquery]"
154:           python3 scripts/run_tests_one_by_one.py build/reldebug/test/unittest "[interquery]"
155:           python3 scripts/run_tests_one_by_one.py build/reldebug/test/unittest "[detailed_profiler]"
156:           python3 scripts/run_tests_one_by_one.py build/reldebug/test/unittest test/sql/tpch/tpch_sf01.test_slow
157: 
158: 
159:  valgrind:
160:     name: Valgrind
161:     runs-on: ubuntu-20.04
162:     needs: linux-debug
163:     env:
164:       CC: gcc-10
165:       CXX: g++-10
166:       DISABLE_SANITIZER: 1
167:       GEN: ninja
168: 
169:     steps:
170:     - uses: actions/checkout@v2
171:       with:
172:         fetch-depth: 0
173: 
174:     - name: Install
175:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build valgrind
176: 
177:     - name: Build
178:       run: make debug
179: 
180:     - name: Test
181:       run: valgrind ./build/debug/test/unittest test/sql/tpch/tpch_sf001.test_slow
182: 
183:  docs:
184:     name: Website Docs
185:     runs-on: ubuntu-20.04
186:     needs: linux-debug
187:     steps:
188:     - uses: actions/checkout@v2
189:       with:
190:         fetch-depth: 0
191: 
192:     - name: Clone Website
193:       run: git clone https://github.com/duckdb/duckdb-web
194: 
195:     - name: Set up Python 3.9
196:       uses: actions/setup-python@v2
197:       with:
198:         python-version: '3.9'
199: 
200:     - name: Package
201:       run: |
202:         cd duckdb-web
203:         python3 scripts/generate_docs.py ..
204: 
205: 
206:  sqllogic:
207:     name: Sqllogic tests
208:     runs-on: ubuntu-20.04
209:     needs: linux-debug
210:     env:
211:       CC: gcc-10
212:       CXX: g++-10
213: 
214:     steps:
215:     - uses: actions/checkout@v2
216:       with:
217:         fetch-depth: 0
218: 
219:     - name: Test
220:       run: make sqlite
221: 
222:  expanded:
223:     name: Expanded
224:     runs-on: ubuntu-20.04
225:     needs: linux-debug
226:     env:
227:       CC: gcc-10
228:       CXX: g++-10
229:       TREAT_WARNINGS_AS_ERRORS: 1
230:       DISABLE_UNITY: 1
231:       GEN: ninja
232: 
233:     steps:
234:     - uses: actions/checkout@v2
235:       with:
236:         fetch-depth: 0
237: 
238:     - name: Install
239:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
240: 
241:     - name: Build
242:       run: make debug
243: 
244:  sqlancer:
245:     name: SQLancer
246:     runs-on: ubuntu-20.04
247:     needs: linux-debug
248:     env:
249:       BUILD_JDBC: 1
250:       FORCE_QUERY_LOG: sqlancer_log.tmp
251:       GEN: ninja
252: 
253:     steps:
254:     - uses: actions/checkout@v2
255:       with:
256:         fetch-depth: 0
257: 
258:     - name: Install
259:       run: |
260:         sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
261:         git clone https://github.com/hannesmuehleisen/sqlancer
262:         cd sqlancer
263:         git checkout persistent
264:         mvn package -q -DskipTests
265: 
266:     - name: Build
267:       run: make reldebug
268: 
269:     - name: Test
270:       run: |
271:         cp build/reldebug/tools/jdbc/duckdb_jdbc.jar sqlancer/target/lib/duckdb_jdbc-*.jar
272:         python3 scripts/run_sqlancer.py
273: 
274: 
275: 
276:  sqlancer_persistent:
277:     name: SQLancer (Persistent)
278:     runs-on: ubuntu-20.04
279:     needs: linux-debug
280:     env:
281:       BUILD_JDBC: 1
282:       FORCE_QUERY_LOG: sqlancer_log.tmp
283:       GEN: ninja
284: 
285:     steps:
286:     - uses: actions/checkout@v2
287:       with:
288:         fetch-depth: 0
289: 
290:     - name: Install
291:       run: |
292:         sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
293:         git clone https://github.com/hannesmuehleisen/sqlancer
294:         cd sqlancer
295:         git checkout persistent
296:         mvn package -q -DskipTests
297: 
298:     - name: Build
299:       run: make reldebug
300: 
301:     - name: Test
302:       run: |
303:         cp build/reldebug/tools/jdbc/duckdb_jdbc.jar sqlancer/target/lib/duckdb_jdbc-*.jar
304:         python3 scripts/run_sqlancer.py --persistent
305: 
306: 
307: 
308:  jdbc:
309:     name: JDBC Compliance
310:     runs-on: ubuntu-18.04
311:     needs: linux-debug
312:     env:
313:       CC: gcc-10
314:       CXX: g++-10
315:       BUILD_JDBC: 1
316:       GEN: ninja
317: 
318:     steps:
319:     - uses: actions/checkout@v2
320:       with:
321:         fetch-depth: 0
322: 
323:     - name: Install
324:       run: |
325:         sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
326:         git clone https://github.com/cwida/jdbccts.git
327: 
328:     - name: Build
329:       run: make release
330: 
331:     - name: Test
332:       run: (cd jdbccts && make DUCKDB_JAR=../build/release/tools/jdbc/duckdb_jdbc.jar test)
333: 
334:  odbc:
335:     name: ODBC
336:     runs-on: ubuntu-20.04
337:     needs: linux-debug
338:     env:
339:       BUILD_ODBC: 1
340:       GEN: ninja
341: 
342:     steps:
343:     - uses: actions/checkout@v2
344:       with:
345:         fetch-depth: 0
346: 
347:     - uses: actions/setup-python@v2
348:       with:
349:         python-version: '3.7'
350: 
351:     - name: Dependencies
352:       run: |
353:         sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build unixodbc-dev python3 python3-pyodbc python3-pip julia
354:         pip3 install pyodbc
355: 
356:     - name: Install nanodbc
357:       run: |
358:         wget https://github.com/nanodbc/nanodbc/archive/refs/tags/v2.13.0.tar.gz -O nanodbc.tgz
359:         (mkdir nanodbc && tar xvf nanodbc.tgz -C nanodbc --strip-components=1 && cd nanodbc && sed -i -e "s/set(test_list/set(test_list odbc/" test/CMakeLists.txt && mkdir build && cd build && cmake -DNANODBC_DISABLE_TESTS=OFF .. && cmake --build .)
360: 
361:     - name: Install psqlodbc
362:       run: |
363:         git clone https://github.com/Mytherin/psqlodbc.git
364:         (cd psqlodbc && git checkout 9863221be0aa6e921c25818509b0402ebda1e561 && make debug)
365: 
366:     - name: Build
367:       run: DISABLE_SANITIZER=1 make debug
368: 
369:     - name: Test nanodbc
370:       run: ./tools/odbc/test/run_nanodbc_tests.sh
371: 
372:     - name: Test psqlodbc
373:       run: ./tools/odbc/test/run_psqlodbc_tests.sh
374: 
375:     - name: Test isql
376:       run: ./tools/odbc/test/run_isql_tests.sh
377: 
378:     - name: Test R ODBC
379:       run: R -f tools/odbc/test/rodbc.R
380: 
381:     - name: Test Python ODBC
382:       run: ./tools/odbc/test/run_pyodbc_tests.sh
383: 
384:     - name: Test Julia ODBC
385:       run: |
386:         export ASAN_OPTIONS=verify_asan_link_order=0
387:         julia tools/odbc/test/julia-test.jl
[end of .github/workflows/Main.yml]
[start of .github/workflows/NodeJS.yml]
1: name: NodeJS
2: on:
3:   push:
4:     paths-ignore:
5:       - '**.md'
6:   pull_request:
7:     paths-ignore:
8:       - '**.md'
9:       - 'tools/pythonpkg/**'
10:       - 'tools/rpkg/**'
11:       - '.github/workflows/Python.yml'
12:       - '.github/workflows/R.yml'
13: 
14: concurrency:
15:   group: ${{ github.workflow }}-${{ github.ref }}-${{ github.head_ref || '' }}-${{ github.base_ref || '' }}-${{ github.ref != 'refs/heads/master' || github.sha }}
16:   cancel-in-progress: true
17: 
18: defaults:
19:   run:
20:     shell: bash
21: 
22: env:
23:   GH_TOKEN: ${{ secrets.GH_TOKEN }}
24:   TWINE_PASSWORD: ${{ secrets.TWINE_PASSWORD }}
25:   AWS_ACCESS_KEY_ID: AKIAVBLKPL2ZW2T7TYFQ
26:   AWS_SECRET_ACCESS_KEY: ${{ secrets.NODE_PRE_GYP_SECRETACCESSKEY }}
27:   NODE_AUTH_TOKEN: ${{secrets.NODE_AUTH_TOKEN}}
28: 
29: jobs:
30:    linux-nodejs:
31:     name: node.js Linux
32:     runs-on: ubuntu-20.04
33: 
34:     steps:
35:     - uses: actions/checkout@v2
36:       with:
37:         fetch-depth: 0
38: 
39:     - name: Install requirements
40:       run: |
41:         sudo apt-get update -y -qq
42:         sudo apt-get install -y git ninja-build make gcc-multilib g++-multilib wget libssl-dev
43: 
44:     - name: Install CMake
45:       run: |
46:         wget https://github.com/Kitware/CMake/releases/download/v3.21.3/cmake-3.21.3-linux-x86_64.sh
47:         chmod +x cmake-3.21.3-linux-x86_64.sh
48:         sudo ./cmake-3.21.3-linux-x86_64.sh --skip-license --prefix=/usr/local
49: 
50:     - name: Build extensions
51:       run: |
52:         GEN=ninja DISABLE_MAIN_DUCKDB_LIBRARY=1 BUILD_TPCH=1 BUILD_HTTPFS=1 STATIC_OPENSSL=1 make
53: 
54:     - name: Setup
55:       run: ./scripts/node_version.sh upload
56: 
57:     - name: Node 10
58:       run: ./scripts/node_build.sh 10
59: 
60:     - name: Node 12
61:       if: github.ref == 'refs/heads/master'
62:       run: ./scripts/node_build.sh 12
63: 
64:     - name: Node 14
65:       if: github.ref == 'refs/heads/master'
66:       run: ./scripts/node_build.sh 14
67: 
68:     - name: Node 15
69:       if: github.ref == 'refs/heads/master'
70:       run: ./scripts/node_build.sh 15
71: 
72:     - name: Node 17
73:       run: ./scripts/node_build.sh 17
74: 
75:    osx-nodejs:
76:       if: github.ref == 'refs/heads/master'
77:       name: node.js OSX
78:       runs-on: macos-latest
79:       needs: linux-nodejs
80:       steps:
81:       - uses: actions/checkout@v2
82:         with:
83:           fetch-depth: 0
84: 
85:       - name: Build extensions
86:         run: |
87:           brew install openssl ninja
88:           export OPENSSL_ROOT_DIR=`brew --prefix openssl`
89:           echo $OPENSSL_ROOT_DIR
90:           GEN=ninja DISABLE_MAIN_DUCKDB_LIBRARY=1 BUILD_TPCH=1 BUILD_HTTPFS=1 STATIC_OPENSSL=1 make
91: 
92:       - name: Setup
93:         run: ./scripts/node_version.sh
94: 
95:       - name: Node 10
96:         run: ./scripts/node_build.sh 10
97: 
98:       - name: Node 12
99:         if: github.ref == 'refs/heads/master'
100:         run: ./scripts/node_build.sh 12
101: 
102:       - name: Node 14
103:         if: github.ref == 'refs/heads/master'
104:         run: ./scripts/node_build.sh 14
105: 
106:       - name: Node 15
107:         if: github.ref == 'refs/heads/master'
108:         run: ./scripts/node_build.sh 15
109: 
110:       - name: Node 17
111:         run: ./scripts/node_build.sh 17
112: 
113:    win-nodejs:
114:      name: node.js Windows
115:      runs-on: windows-latest
116:      needs: linux-nodejs
117: 
118:      strategy:
119:        matrix:
120:          node: [ '10', '12', '14', '15' ]
121:          isMaster:
122:             - ${{ github.ref == 'refs/heads/master' }}
123:          exclude:
124:            - isMaster: false
125:              node: 12
126:            - isMaster: false
127:              node: 14
128: 
129:      steps:
130:        - uses: actions/setup-python@v2
131:          with:
132:            python-version: '3.8'
133: 
134:        - uses: actions/checkout@v2
135:          with:
136:            fetch-depth: 0
137: 
138:        - name: Setup Node
139:          uses: actions/setup-node@v2
140:          with:
141:            node-version: ${{ matrix.node }}
142: 
143:        - name: Versions
144:          run: |
145:            systeminfo
146:            node -v
147:            npm -v
148: 
149:        - name: Windows Build Tools
150:          run: |
151:            choco install python visualstudio2019-workload-vctools -y
152:            npm config set msvs_version 2019
153: 
154:        - name: Node Version
155:          run: ./scripts/node_version.sh
156: 
157:        - name: Node
158:          run: ./scripts/node_build_win.sh
[end of .github/workflows/NodeJS.yml]
[start of .github/workflows/Python.yml]
1: name: Python
2: on:
3:   push:
4:     paths-ignore:
5:       - '**.md'
6:   pull_request:
7:     paths-ignore:
8:       - '**.md'
9:       - 'tools/nodejs/**'
10:       - 'tools/rpkg/**'
11:       - '.github/workflows/NodeJS.yml'
12:       - '.github/workflows/R.yml'
13: 
14: concurrency:
15:   group: ${{ github.workflow }}-${{ github.ref }}-${{ github.head_ref || '' }}-${{ github.base_ref || '' }}-${{ github.ref != 'refs/heads/master' || github.sha }}
16:   cancel-in-progress: true
17: 
18: defaults:
19:   run:
20:     shell: bash
21: 
22: env:
23:   GH_TOKEN: ${{ secrets.GH_TOKEN }}
24:   TWINE_PASSWORD: ${{ secrets.TWINE_PASSWORD }}
25:   AWS_ACCESS_KEY_ID: AKIAVBLKPL2ZW2T7TYFQ
26:   AWS_SECRET_ACCESS_KEY: ${{ secrets.NODE_PRE_GYP_SECRETACCESSKEY }}
27:   NODE_AUTH_TOKEN: ${{secrets.NODE_AUTH_TOKEN}}
28: 
29: jobs:
30: #  This is just a sanity check of Python 3.9 running with Arrow
31:    linux-python3-9:
32:     name: Python 3.9 Linux
33:     runs-on: ubuntu-20.04
34: 
35:     env:
36:       CIBW_BUILD: 'cp39-manylinux_x86_64'
37:       CIBW_BEFORE_BUILD: 'pip install --prefer-binary "pandas>=0.24" "pytest>=4.3"'
38:       CIBW_TEST_REQUIRES: 'pytest'
39:       CIBW_BEFORE_TEST: 'pip install --prefer-binary "pandas>=0.24" pytest-timeout mypy && pip install --prefer-binary "requests>=2.26" && (pip install --prefer-binary "pyarrow==6.0" || true)'
40:       CIBW_TEST_COMMAND: 'python -m pytest {project}/tests'
41:       SETUPTOOLS_SCM_NO_LOCAL: 'yes'
42:       TWINE_USERNAME: 'hfmuehleisen'
43:       PYTEST_TIMEOUT: '600'
44: 
45:     steps:
46:     - uses: actions/checkout@v2
47:       with:
48:         fetch-depth: 0
49: 
50:     - uses: actions/setup-python@v2
51:       with:
52:         python-version: '3.7'
53: 
54:     - name: Install
55:       run: pip install cibuildwheel twine
56: 
57:     - name: Build
58:       run: |
59:         cd tools/pythonpkg
60:         python setup.py sdist
61:         mkdir duckdb_tarball && tar xvf dist/duckdb-*.tar.gz --strip-components=1 -C duckdb_tarball
62:         cibuildwheel --output-dir wheelhouse duckdb_tarball
63: 
64: 
65:    linux-python3:
66:     name: Python 3 Linux
67:     runs-on: ubuntu-20.04
68:     strategy:
69:       matrix:
70:         arch: [auto, aarch64]
71:         python_build: [cp37-*, cp38-*, cp39-*,cp310-*]
72:         isMaster:
73:           - ${{ github.ref == 'refs/heads/master' }}
74:         exclude:
75:           - isMaster: false
76:             python_build: 'cp38-*'
77:           - isMaster: false
78:             python_build: 'cp39-*'
79:     needs: linux-python3-9
80:     env:
81:       CIBW_BUILD: ${{ matrix.python_build}}
82:       CIBW_BEFORE_BUILD: 'pip install --prefer-binary "pandas>=0.24" "pytest>=4.3"'
83:       CIBW_TEST_REQUIRES: 'pytest'
84:       CIBW_BEFORE_TEST: 'pip install --prefer-binary "pandas>=0.24" pytest-timeout mypy && pip install --prefer-binary "requests>=2.26"'
85:       CIBW_TEST_COMMAND: 'python -m pytest {project}/tests/fast'
86:       SETUPTOOLS_SCM_NO_LOCAL: 'yes'
87:       TWINE_USERNAME: 'hfmuehleisen'
88:       PYTEST_TIMEOUT: '600'
89:       DUCKDB_BUILD_UNITY: 1
90: 
91:     steps:
92:     - uses: actions/checkout@v2
93:       with:
94:         fetch-depth: 0
95: 
96:     - uses: actions/setup-python@v2
97:       with:
98:         python-version: '3.7'
99: 
100:     - uses: docker/setup-qemu-action@v1
101:       if: ${{ matrix.arch == 'aarch64' }}
102:       name: Set up QEMU
103: 
104:     - name: Install
105:       run: pip install cibuildwheel twine
106: 
107:     - name: Build
108:       run: |
109:         cd tools/pythonpkg
110:         python setup.py sdist
111:         mkdir duckdb_tarball && tar xvf dist/duckdb-*.tar.gz --strip-components=1 -C duckdb_tarball
112:         cibuildwheel --output-dir wheelhouse duckdb_tarball
113: 
114:     - name: Deploy aarch64
115:       if: ${{ matrix.arch == 'aarch64' }}
116:       run: |
117:         if [[ "$GITHUB_REF" =~ ^(refs/heads/master|refs/tags/v.+)$ && "$GITHUB_REPOSITORY" = "duckdb/duckdb" ]] ; then
118:           twine upload --non-interactive --disable-progress-bar --skip-existing tools/pythonpkg/wheelhouse/*.whl
119:         fi
120: 
121:     - name: Deploy regular
122:       if: ${{ matrix.arch != 'aarch64' }}
123:       run: |
124:         python scripts/asset-upload-gha.py duckdb_python_src.tar.gz=tools/pythonpkg/dist/duckdb-*.tar.gz
125:         if [[ "$GITHUB_REF" =~ ^(refs/heads/master|refs/tags/v.+)$ && "$GITHUB_REPOSITORY" = "duckdb/duckdb" ]] ; then
126:           twine upload --non-interactive --disable-progress-bar --skip-existing tools/pythonpkg/wheelhouse/*.whl tools/pythonpkg/dist/duckdb-*.tar.gz
127:         fi
128: 
129:    osx-python3:
130:       if: github.ref == 'refs/heads/master'
131:       name: Python 3 OSX
132:       runs-on: macos-latest
133:       strategy:
134:        matrix:
135:         python_build: [cp36-*, cp37-*, cp38-*, cp39-*, cp310-*]
136:         isMaster:
137:           - ${{ github.ref == 'refs/heads/master' }}
138:         exclude:
139:           - isMaster: false
140:             python_build: 'cp37-*'
141:           - isMaster: false
142:             python_build: 'cp38-*'
143:           - isMaster: false
144:             python_build: 'cp39-*'
145:       needs: linux-python3-9
146:       env:
147:         CIBW_BUILD: ${{ matrix.python_build}}
148:         CIBW_BEFORE_BUILD: 'pip install --prefer-binary "pandas>=0.24" "pytest>=4.3"'
149:         CIBW_TEST_REQUIRES: 'pytest'
150:         CIBW_BEFORE_TEST: 'pip install --prefer-binary "pandas>=0.24" "requests>=2.26" mypy && (pip install --prefer-binary "pyarrow==6.0" || true)'
151:         CIBW_TEST_COMMAND: 'python -m pytest {project}/tests/fast'
152:         CIBW_ARCHS_MACOS: 'x86_64 universal2 arm64'
153:         SETUPTOOLS_SCM_NO_LOCAL: 'yes'
154:         TWINE_USERNAME: 'hfmuehleisen'
155:         DUCKDB_BUILD_UNITY: 1
156: 
157:       steps:
158:       - uses: actions/checkout@v2
159:         with:
160:           fetch-depth: 0
161: 
162:       - uses: actions/setup-python@v2
163:         with:
164:           python-version: '3.7'
165: 
166:       - name: Install
167:         run: pip install cibuildwheel twine
168: 
169:       - name: Build
170:         run: |
171:           cd tools/pythonpkg
172:           python setup.py sdist
173:           mkdir duckdb_tarball && tar xvf dist/duckdb-*.tar.gz --strip-components=1 -C duckdb_tarball
174:           cibuildwheel --output-dir wheelhouse duckdb_tarball
175: 
176:       - name: Deploy
177:         run: |
178:           if [[ "$GITHUB_REF" =~ ^(refs/heads/master|refs/tags/v.+)$ && "$GITHUB_REPOSITORY" = "duckdb/duckdb" ]] ; then
179:             twine upload --non-interactive --disable-progress-bar --skip-existing tools/pythonpkg/wheelhouse/*.whl
180:           fi
181: 
182:    win-python3:
183:       name: Python 3 Windows
184:       runs-on: windows-latest
185:       strategy:
186:        matrix:
187:         python_build: [cp36-*, cp37-*, cp38-*, cp39-*, cp310-*]
188:         isMaster:
189:           - ${{ github.ref == 'refs/heads/master' }}
190:         exclude:
191:           - isMaster: false
192:             python_build: 'cp37-*'
193:           - isMaster: false
194:             python_build: 'cp38-*'
195:           - isMaster: false
196:             python_build: 'cp39-*'
197:       needs: linux-python3-9
198: 
199:       env:
200:         CIBW_BUILD: ${{ matrix.python_build}}
201:         CIBW_BEFORE_BUILD: 'pip install --prefer-binary "pandas>=0.24" "pytest>=4.3"'
202:         CIBW_TEST_REQUIRES: 'pytest'
203:         CIBW_BEFORE_TEST: 'pip install --prefer-binary "pandas>=0.24" "pytest>=4.3" mypy'
204:         CIBW_TEST_COMMAND: 'python -m pytest {project}/tests/fast'
205:         SETUPTOOLS_SCM_NO_LOCAL: 'yes'
206:         TWINE_USERNAME: 'hfmuehleisen'
207:         DUCKDB_BUILD_UNITY: 1
208: 
209:       steps:
210:       - uses: actions/checkout@v2
211:         with:
212:           fetch-depth: 0
213: 
214:       - uses: actions/setup-python@v2
215:         with:
216:           python-version: '3.7'
217: 
218:       - name: Install
219:         run: pip install cibuildwheel twine
220: 
221:       - name: Build
222:         run: |
223:           cd tools/pythonpkg
224:           python setup.py sdist
225:           mkdir duckdb_tarball && tar xvf dist/duckdb-*.tar.gz --strip-components=1 -C duckdb_tarball
226:           cibuildwheel --output-dir wheelhouse duckdb_tarball
227: 
228:       - name: Deploy
229:         run: |
230:           if [[ "$GITHUB_REF" =~ ^(refs/heads/master|refs/tags/v.+)$ && "$GITHUB_REPOSITORY" = "duckdb/duckdb" ]] ; then
231:             twine upload --non-interactive --disable-progress-bar --skip-existing tools/pythonpkg/wheelhouse/*.whl
232:           fi
233: 
234:    linux-tarball:
235:       name: Python 3 Tarball
236:       runs-on: ubuntu-20.04
237:       needs: linux-python3-9
238: 
239:       steps:
240:       - uses: actions/checkout@v2
241:         with:
242:           fetch-depth: 0
243: 
244:       - uses: actions/setup-python@v2
245:         with:
246:           python-version: '3.7'
247: 
248:       - name: Install
249:         run: pip install numpy pytest pandas mypy
250: 
251:       - name: Build
252:         run: |
253:           python --version
254:           git archive --format zip --output test-tarball.zip HEAD
255:           mkdir duckdb-test-tarball
256:           mv test-tarball.zip duckdb-test-tarball
257:           cd duckdb-test-tarball
258:           unzip test-tarball.zip
259:           cd tools/pythonpkg
260:           export SETUPTOOLS_SCM_PRETEND_VERSION=0.2.2
261:           python setup.py install --user
262:           (cd tests/ && python -m pytest)
[end of .github/workflows/Python.yml]
[start of .github/workflows/R.yml]
1: name: R
2: on:
3:   push:
4:     paths-ignore:
5:       - '**.md'
6:   pull_request:
7:     paths-ignore:
8:       - '**.md'
9:       - 'tools/nodejs/**'
10:       - 'tools/pythonpkg/**'
11:       - '.github/workflows/NodeJS.yml'
12:       - '.github/workflows/Python.yml'
13: 
14: concurrency:
15:   group: ${{ github.workflow }}-${{ github.ref }}-${{ github.head_ref || '' }}-${{ github.base_ref || '' }}-${{ github.ref != 'refs/heads/master' || github.sha }}
16:   cancel-in-progress: true
17: 
18: defaults:
19:   run:
20:     shell: bash
21: 
22: env:
23:   GH_TOKEN: ${{ secrets.GH_TOKEN }}
24:   TWINE_PASSWORD: ${{ secrets.TWINE_PASSWORD }}
25:   AWS_ACCESS_KEY_ID: AKIAVBLKPL2ZW2T7TYFQ
26:   AWS_SECRET_ACCESS_KEY: ${{ secrets.NODE_PRE_GYP_SECRETACCESSKEY }}
27:   NODE_AUTH_TOKEN: ${{secrets.NODE_AUTH_TOKEN}}
28: 
29: jobs:
30:   rstats-linux:
31:     name: R Package Linux
32:     runs-on: ubuntu-20.04
33: 
34:     env:
35:       LIBARROW_BINARY : 'false'
36:       ARROW_RUNTIME_SIMD_LEVEL : 'AVX2'
37:     steps:
38:     - uses: actions/checkout@v2
39:       with:
40:         fetch-depth: 0
41: 
42:     - uses: actions/setup-python@v2
43:       with:
44:         python-version: '3.7'
45: 
46:     - uses: r-lib/actions/setup-r@v1
47:       with:
48:         r-version: 'devel'
49: 
50:     - name: Install
51:       env:
52:         GITHUB_PAT: ${{ github.token }}
53:       run: |
54:         sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build texlive-latex-base texlive-fonts-extra libcurl4-openssl-dev valgrind pandoc
55:         mkdir -p $HOME/.R
56:         R -f tools/rpkg/dependencies.R
57: 
58:     - name: Prepare
59:       run: |
60:         cd tools/rpkg
61:         ./configure
62:         R CMD build .
63: 
64:     - name: Tests
65:       run: |
66:         cd tools/rpkg
67:         R CMD INSTALL -d duckdb_*.tar.gz
68:         (cd tests && R -f testthat.R)
69: 
70:     - name: R CMD check
71:       run: |
72:         cd tools/rpkg
73:         R CMD check --as-cran -o /tmp duckdb_*.tar.gz
74:         if grep WARNING /tmp/duckdb.Rcheck/00check.log ; then exit 1; fi
75: 
76:     - name: Valgrind
77:       run: |
78:         cd tools/rpkg
79:         export NOT_CRAN='false'
80:         R CMD check --use-valgrind -o /tmp duckdb_*.tar.gz
81: 
82:     - name: Coverage
83:       env:
84:         DUCKDB_R_DEBUG: 1
85:       run: |
86:         pkgload::load_all("tools/rpkg")
87:         cov <- covr::codecov("tools/rpkg", relative_path = ".")
88:       shell: Rscript {0}
89: 
90:     - name: Print R log on failure
91:       if: ${{ failure() }}
92:       run: |
93:         ls -R /tmp/duckdb.Rcheck
94:         cat /tmp/duckdb.Rcheck/00check.log
95:         cat /tmp/duckdb.Rcheck/tests/testthat.Rout.fail
96: 
97:     - name: Deploy
98:       run: python scripts/asset-upload-gha.py duckdb_r_src.tar.gz=tools/rpkg/duckdb_*.tar.gz
99: 
100:   rstats-windows:
101:     name: R Package Windows
102:     runs-on: windows-latest
103:     needs: rstats-linux
104: 
105:     steps:
106:     - uses: actions/checkout@v2
107:       with:
108:         fetch-depth: 0
109: 
110:     - uses: actions/setup-python@v2
111:       with:
112:         python-version: '3.7'
113: 
114:     - uses: r-lib/actions/setup-r@v1
115:       with:
116:         r-version: 'devel'
117: 
118:     - name: Install
119:       run: |
120:         R -f tools/rpkg/dependencies.R
121: 
122:     - name: Build
123:       run: |
124:         cd tools/rpkg
125:         ./configure
126:         R CMD build .
127:         R CMD INSTALL duckdb_*.tar.gz
128:         (cd tests && R -f testthat.R)
129:         R CMD check --as-cran --no-manual -o /tmp duckdb_*.tar.gz
130:         if grep WARNING /tmp/duckdb.Rcheck/00check.log ; then exit 1; fi
131: 
132:   rubsan:
133:     name: R UBSAN
134:     runs-on: ubuntu-latest
135:     needs: rstats-linux
136: 
137:     steps:
138:     - uses: actions/checkout@v2
139:       with:
140:         fetch-depth: 0
141: 
142:     - name: Run
143:       run: |
144:         (cd tools/rpkg && ./configure && R CMD build .)
145:         docker run -v `pwd`:/duckdb --cap-add SYS_PTRACE wch1/r-debug:latest bash -c "mkdir -p ~/.R && echo -e \"PKG_CFLAGS=-fno-sanitize-recover=all\\nPKG_CXXFLAGS=-fno-sanitize-recover=all\" > ~/.R/Makevars && export CMAKE_UNITY_BUILD=OFF ARROW_R_DEV=TRUE LIBARROW_BINARY=true && cd /duckdb/tools/rpkg/ && RDsan -f dependencies.R && RDsan CMD INSTALL duckdb_*.tar.gz && cd tests && UBSAN_OPTIONS=print_stacktrace=1 RDsan -f testthat.R"
146: 
147:     - name: Print R log on failure
148:       if: ${{ failure() }}
149:       run: |
150:         cat duckdb.Rcheck/00check.log
151:         cat duckdb.Rcheck/tests/testthat.Rout.fail
[end of .github/workflows/R.yml]
[start of .github/workflows/Regression.yml]
1: name: Regression
2: on:
3:   push:
4:     paths-ignore:
5:       - '**.md'
6:   pull_request:
7:     paths-ignore:
8:       - '**.md'
9:       - 'tools/nodejs/**'
10:       - 'tools/rpkg/**'
11:       - '.github/workflows/NodeJS.yml'
12:       - '.github/workflows/Python.yml'
13:       - '.github/workflows/R.yml'
14: 
15: concurrency:
16:   group: ${{ github.workflow }}-${{ github.ref }}-${{ github.head_ref || '' }}-${{ github.base_ref || '' }}-${{ github.ref != 'refs/heads/master' || github.sha }}
17:   cancel-in-progress: true
18: 
19: defaults:
20:   run:
21:     shell: bash
22: 
23: env:
24:   GH_TOKEN: ${{ secrets.GH_TOKEN }}
25:   TWINE_PASSWORD: ${{ secrets.TWINE_PASSWORD }}
26:   AWS_ACCESS_KEY_ID: AKIAVBLKPL2ZW2T7TYFQ
27:   AWS_SECRET_ACCESS_KEY: ${{ secrets.NODE_PRE_GYP_SECRETACCESSKEY }}
28:   NODE_AUTH_TOKEN: ${{secrets.NODE_AUTH_TOKEN}}
29: 
30: jobs:
31:  regression-test-tpch:
32:   name: Regression Test (TPC-H)
33:   runs-on: ubuntu-20.04
34:   env:
35:     CC: gcc-10
36:     CXX: g++-10
37:     GEN: ninja
38: 
39:   steps:
40:   - uses: actions/checkout@v2
41:     with:
42:       fetch-depth: 0
43: 
44:   - uses: actions/setup-python@v2
45:     with:
46:       python-version: '3.7'
47: 
48:   - name: Install
49:     run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build && pip install requests && pip install numpy
50: 
51:   - name: Build
52:     run: |
53:       git clone https://github.com/duckdb/duckdb.git
54:       cd duckdb/tools/pythonpkg
55:       python setup.py install
56:       cd ../../../tools/pythonpkg
57:       python setup.py --package_name=duckcurrent install
58: 
59:   - name: Regression Test TPC-H SF1
60:     run: python scripts/regression_test.py --benchmark=tpch
61: 
62:  regression-test-tpcds:
63:   name: Regression Test (TPC-DS)
64:   runs-on: ubuntu-20.04
65:   needs: regression-test-tpch
66:   env:
67:     CC: gcc-10
68:     CXX: g++-10
69:     GEN: ninja
70: 
71:   steps:
72:   - uses: actions/checkout@v2
73:     with:
74:       fetch-depth: 0
75: 
76:   - uses: actions/setup-python@v2
77:     with:
78:       python-version: '3.7'
79: 
80:   - name: Install
81:     run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build && pip install requests && pip install numpy
82: 
83:   - name: Build
84:     run: |
85:       git clone https://github.com/duckdb/duckdb.git
86:       cd duckdb/tools/pythonpkg
87:       python setup.py install
88:       cd ../../../tools/pythonpkg
89:       python setup.py --package_name=duckcurrent install
90: 
91:   - name: Regression Test TPC-DS SF1
92:     run: python scripts/regression_test.py --benchmark=tpcds
93: 
94:  regression-test-h20ai:
95:   name: Regression Test (H2OAI)
96:   runs-on: ubuntu-20.04
97:   needs: regression-test-tpch
98:   env:
99:     CC: gcc-10
100:     CXX: g++-10
101:     GEN: ninja
102: 
103:   steps:
104:     - uses: actions/checkout@v2
105:       with:
106:         fetch-depth: 0
107: 
108:     - uses: actions/setup-python@v2
109:       with:
110:         python-version: '3.7'
111: 
112:     - name: Install
113:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build && pip install requests && pip install numpy
114: 
115:     - name: Build
116:       run: |
117:         git clone https://github.com/duckdb/duckdb.git
118:         cd duckdb/tools/pythonpkg
119:         python setup.py install
120:         cd ../../../tools/pythonpkg
121:         python setup.py --package_name=duckcurrent install
122: 
123:     - name: Regression Test H2OAI
124:       run: python scripts/regression_test.py --benchmark=h2oai
[end of .github/workflows/Regression.yml]
[start of .github/workflows/Windows.yml]
1: name: Windows
2: on:
3:   push:
4:     paths-ignore:
5:       - '**.md'
6:   pull_request:
7:     paths-ignore:
8:       - '**.md'
9:       - 'tools/nodejs/**'
10:       - 'tools/pythonpkg/**'
11:       - 'tools/rpkg/**'
12:       - '.github/workflows/NodeJS.yml'
13:       - '.github/workflows/Python.yml'
14:       - '.github/workflows/R.yml'
15: 
16: concurrency:
17:   group: ${{ github.workflow }}-${{ github.ref }}-${{ github.head_ref || '' }}-${{ github.base_ref || '' }}-${{ github.ref != 'refs/heads/master' || github.sha }}
18:   cancel-in-progress: true
19: 
20: defaults:
21:   run:
22:     shell: bash
23: 
24: env:
25:   GH_TOKEN: ${{ secrets.GH_TOKEN }}
26:   TWINE_PASSWORD: ${{ secrets.TWINE_PASSWORD }}
27:   AWS_ACCESS_KEY_ID: AKIAVBLKPL2ZW2T7TYFQ
28:   AWS_SECRET_ACCESS_KEY: ${{ secrets.NODE_PRE_GYP_SECRETACCESSKEY }}
29:   NODE_AUTH_TOKEN: ${{secrets.NODE_AUTH_TOKEN}}
30: 
31: jobs:
32:  win-release-64:
33:     name: Windows (64 Bit)
34:     runs-on: windows-latest
35:     steps:
36:     - uses: actions/checkout@v2
37:       with:
38:         fetch-depth: 0
39: 
40:     - uses: actions/setup-python@v2
41:       with:
42:         python-version: '3.7'
43: 
44:     - name: Build
45:       run: |
46:         python scripts/windows_ci.py
47:         cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_GENERATOR_PLATFORM=x64 -DBUILD_ICU_EXTENSION=1 -DBUILD_PARQUET_EXTENSION=1 -DBUILD_TPCH_EXTENSION=1 -DBUILD_TPCDS_EXTENSION=1 -DBUILD_FTS_EXTENSION=1 -DBUILD_JSON_EXTENSION=1 -DBUILD_EXCEL_EXTENSION=1 -DBUILD_REST=1 -DJDBC_DRIVER=1 -DBUILD_VISUALIZER_EXTENSION=1 -DBUILD_ODBC_DRIVER=1 -DDISABLE_UNITY=1
48:         cmake --build . --config Release
49: 
50:     - name: Test
51:       run: test/Release/unittest.exe
52: 
53:     - name: Tools Test
54:       run: |
55:         python tools/shell/shell-test.py Release/duckdb.exe
56:         java -cp tools/jdbc/duckdb_jdbc.jar org.duckdb.test.TestDuckDBJDBC
57: 
58:     - name: Deploy
59:       run: |
60:         python scripts/amalgamation.py
61:         choco install zip -y --force
62:         zip -j duckdb_cli-windows-amd64.zip Release/duckdb.exe
63:         zip -j libduckdb-windows-amd64.zip src/Release/duckdb.dll src/amalgamation/duckdb.hpp src/include/duckdb.h
64:         zip -j duckdb_odbc-windows-amd64.zip tools/odbc/bin/Release/*
65:         python scripts/asset-upload-gha.py libduckdb-windows-amd64.zip duckdb_cli-windows-amd64.zip duckdb_jdbc-windows-amd64.jar=tools/jdbc/duckdb_jdbc.jar duckdb_odbc-windows-amd64.zip
66: 
67:     - uses: actions/upload-artifact@v2
68:       with:
69:         name: duckdb-binaries-windows
70:         path: |
71:           libduckdb-windows-amd64.zip
72:           duckdb_cli-windows-amd64.zip
73:           tools/jdbc/duckdb_jdbc.jar
74: 
75:     - uses: ilammy/msvc-dev-cmd@v1
76:     - name: Duckdb.dll export symbols with C++ on Windows
77:       run: cl -I src/include examples/embedded-c++-windows/cppintegration.cpp -link src/Release/duckdb.lib
78: 
79:  win-release-32:
80:     name: Windows (32 Bit)
81:     runs-on: windows-latest
82:     needs: win-release-64
83: 
84:     steps:
85:     - uses: actions/checkout@v2
86:       with:
87:         fetch-depth: 0
88: 
89:     - uses: actions/setup-python@v2
90:       with:
91:         python-version: '3.7'
92: 
93:     - name: Build
94:       run: |
95:         cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_GENERATOR_PLATFORM=Win32 -DBUILD_ICU_EXTENSION=1 -DBUILD_PARQUET_EXTENSION=1 -DBUILD_TPCH_EXTENSION=1 -DBUILD_TPCDS_EXTENSION=1 -DBUILD_FTS_EXTENSION=1 -DBUILD_JSON_EXTENSION=1 -DBUILD_EXCEL_EXTENSION=1 -DJDBC_DRIVER=1 -DBUILD_VISUALIZER_EXTENSION=1
96:         cmake --build . --config Release
97: 
98:     - name: Test
99:       run: test/Release/unittest.exe
100: 
101:     - name: Tools Test
102:       run: |
103:         python tools/shell/shell-test.py Release/duckdb.exe
104: 
105:     - name: Deploy
106:       run: |
107:         python scripts/amalgamation.py
108:         choco install zip -y --force
109:         zip -j duckdb_cli-windows-i386.zip Release/duckdb.exe
110:         zip -j libduckdb-windows-i386.zip src/Release/duckdb.dll src/amalgamation/duckdb.hpp src/include/duckdb.h
111:         python scripts/asset-upload-gha.py libduckdb-windows-i386.zip duckdb_cli-windows-i386.zip duckdb_jdbc-windows-i386.jar=tools/jdbc/duckdb_jdbc.jar
112: 
113:     - uses: actions/upload-artifact@v2
114:       with:
115:         name: duckdb-binaries-windows
116:         path: |
117:           libduckdb-windows-i386.zip
118:           duckdb_cli-windows-i386.zip
119:           tools/jdbc/duckdb_jdbc.jar
120: 
121: 
122:  mingw:
123:      name: MingW (64 Bit)
124:      runs-on: windows-latest
125:      needs: win-release-64
126:      defaults:
127:        run:
128:          shell: msys2 {0}
129:      steps:
130:        - uses: actions/checkout@v2
131:        - uses: msys2/setup-msys2@v2
132:          with:
133:            msystem: MINGW64
134:            update: true
135:            install: git mingw-w64-x86_64-toolchain mingw-w64-x86_64-cmake mingw-w64-x86_64-ninja git
136:        # see here: https://gist.github.com/scivision/1de4fd6abea9ba6b2d87dc1e86b5d2ce
137:        - name: Put MSYS2_MinGW64 on PATH
138:          # there is not yet an environment variable for this path from msys2/setup-msys2
139:          run: export PATH=D:/a/_temp/msys/msys64/mingw64/bin:$PATH
140: 
141:        - name: Build
142:          run: |
143:            cmake -G "Ninja" -DCMAKE_BUILD_TYPE=Release -DBUILD_PARQUET_EXTENSION=1
144:            cmake --build . --config Release
145: 
146:        - name: Test
147:          run: |
148:            cp src/libduckdb.dll .
149:            test/unittest.exe
150: 
151:  odbc-win-64:
152:     name: ODBC Windows
153:     runs-on: windows-latest
154:     needs: win-release-64
155:     steps:
156:     - uses: actions/checkout@v2
157:       with:
158:         fetch-depth: 0
159: 
160:     - uses: actions/setup-python@v2
161:       with:
162:         python-version: '3.7'
163: 
164:     - name: Install Git
165:       run: |
166:         choco install git -y --force
167: 
168:     - name: Build
169:       run: |
170:         cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_GENERATOR_PLATFORM=x64 -DBUILD_ICU_EXTENSION=1 -DBUILD_PARQUET_EXTENSION=1 -DBUILD_TPCH_EXTENSION=1 -DBUILD_TPCDS_EXTENSION=1 -DBUILD_FTS_EXTENSION=1 -DBUILD_JSON_EXTENSION=1 -DBUILD_EXCEL_EXTENSION=1 -DBUILD_REST=1 -DJDBC_DRIVER=1 -DBUILD_VISUALIZER_EXTENSION=1 -DBUILD_ODBC_DRIVER=1 -DDISABLE_UNITY=1
171:         cmake --build . --config Release
172: 
173:     - name: Install ODBC Driver
174:       run:  |
175:         tools/odbc/bin/Release/odbc_install.exe //CI //Install
176:         Reg Query "HKLM\SOFTWARE\ODBC\ODBC.INI\ODBC Data Sources"
177:         Reg Query "HKLM\SOFTWARE\ODBC\ODBC.INI\DuckDB"
178:         Reg Query "HKLM\SOFTWARE\ODBC\ODBCINST.INI\DuckDB Driver"
179: 
180:     - name: Enable ODBC Trace HKCU
181:       run: |
182:         REG ADD "HKCU\SOFTWARE\ODBC\ODBC.INI\ODBC" //f
183:         REG ADD "HKCU\SOFTWARE\ODBC\ODBC.INI\ODBC" //v Trace //t REG_SZ //d 1
184:         REG ADD "HKCU\SOFTWARE\ODBC\ODBC.INI\ODBC" //v TraceDll //t REG_SZ //d "C:\Windows\system32\odbctrac.dll"
185:         REG ADD "HKCU\SOFTWARE\ODBC\ODBC.INI\ODBC" //v TraceFile //t REG_SZ //d "D:\a\duckdb\duckdb\ODBC_TRACE.log"
186:         echo "----------------------------------------------------------------"
187:         Reg Query "HKCU\SOFTWARE\ODBC\ODBC.INI\ODBC"
188: 
189:     - name: Install psqlodbc
190:       run: |
191:         git clone https://github.com/Mytherin/psqlodbc.git
192:         (cd psqlodbc && git checkout 9863221be0aa6e921c25818509b0402ebda1e561 && make release)
193: 
194:     - name: Test psqlodbc
195:       run: |
196:         cd psqlodbc
197:         export PSQLODBC_TEST_DSN=DuckDB
198:         build/release/Release/psql_odbc_test.exe -f ../tools/odbc/test/psql_supported_tests
199: 
200:     - name: Print ODBC trace on failure
201:       if: ${{ failure() }}
202:       run: cat ODBC_TRACE.log
203: 
204:  win-extensions-64:
205:    name: Windows Extensions (64-bit)
206:    runs-on: windows-latest
207:    needs: win-release-64
208:    env:
209:      BUILD_VISUALIZER: 1
210:      BUILD_ICU: 1
211:      BUILD_TPCH: 1
212:      BUILD_TPCDS: 1
213:      BUILD_FTS: 1
214:      BUILD_HTTPFS: 1
215:      BUILD_JSON: 1
216:      BUILD_EXCEL: 1
217:      STATIC_OPENSSL: 1
218:      BUILD_SUBSTRAIT_EXTENSION: 1
219:      AWS_ACCESS_KEY_ID: ${{secrets.S3_ID}}
220:      AWS_SECRET_ACCESS_KEY: ${{secrets.S3_KEY}}
221:      AWS_DEFAULT_REGION: us-east-1
222:    steps:
223:      - uses: actions/checkout@v2
224:        with:
225:          fetch-depth: 0
226: 
227:      - uses: actions/setup-python@v2
228:        with:
229:          python-version: '3.7'
230: 
231:      - name: Install OpenSSL
232:        run: |
233:          choco install openssl -y --force
234: 
235:      - name: Build
236:        run: |
237:          make
238:          rm build/release/src/Release/duckdb*
239: 
240:      - name: Uninstall OpenSSL
241:        run: |
242:          choco uninstall openssl -y --force
243: 
244:      - name: Deploy
245:        run: |
246:          if [[ "$GITHUB_REF" =~ ^(refs/heads/master|refs/tags/v.+)$ && "$GITHUB_REPOSITORY" = "duckdb/duckdb" ]] ; then
247:            pip install awscli
248:            ./scripts/extension-upload.sh windows_amd64
249:            ./scripts/extension-upload-test.sh
250:          else
251:            ./scripts/extension-upload-test.sh local
252:          fi
[end of .github/workflows/Windows.yml]
[start of examples/standalone-plan/main.cpp]
1: #include "duckdb.hpp"
2: #ifndef DUCKDB_AMALGAMATION
3: #include "duckdb/planner/logical_operator.hpp"
4: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
5: #include "duckdb/planner/operator/logical_aggregate.hpp"
6: #include "duckdb/planner/operator/logical_get.hpp"
7: #include "duckdb/function/table/table_scan.hpp"
8: #include "duckdb/planner/expression.hpp"
9: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
10: #include "duckdb/planner/expression/bound_cast_expression.hpp"
11: #include "duckdb/planner/expression/bound_conjunction_expression.hpp"
12: #include "duckdb/planner/expression/bound_reference_expression.hpp"
13: #include "duckdb/planner/expression/bound_comparison_expression.hpp"
14: #include "duckdb/planner/expression/bound_constant_expression.hpp"
15: #include "duckdb/planner/expression/bound_function_expression.hpp"
16: #include "duckdb/function/function_set.hpp"
17: #include "duckdb/parser/parsed_data/create_scalar_function_info.hpp"
18: #include "duckdb/parser/parsed_data/create_aggregate_function_info.hpp"
19: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
20: #include "duckdb/parser/tableref/table_function_ref.hpp"
21: #include "duckdb/parser/expression/constant_expression.hpp"
22: #include "duckdb/parser/expression/function_expression.hpp"
23: #include "duckdb/storage/statistics/numeric_statistics.hpp"
24: #endif
25: 
26: using namespace duckdb;
27: 
28: // in this example we build a simple volcano model executor on top of the DuckDB logical plans
29: // for simplicity, the executor only handles integer values and doesn't handle null values
30: 
31: void ExecuteQuery(Connection &con, const string &query);
32: void CreateFunction(Connection &con, string name, vector<LogicalType> arguments, LogicalType return_type);
33: void CreateAggregateFunction(Connection &con, string name, vector<LogicalType> arguments, LogicalType return_type);
34: 
35: //===--------------------------------------------------------------------===//
36: // Example Using DuckDB Catalog/Tables
37: //===--------------------------------------------------------------------===//
38: void RunExampleDuckDBCatalog() {
39: 	// in this example we use the DuckDB CREATE TABLE syntax to create tables to link against
40: 	// this works and the tables are easy to define, but since the tables are empty there are no statistics available
41: 	// we can use our own table functions (see RunExampleTableScan), but this is slightly more involved
42: 
43: 	DBConfig config;
44: 	config.initialize_default_database = false;
45: 
46: 	// disable the statistics propagator optimizer
47: 	// this is required since the statistics propagator will truncate our plan
48: 	// (e.g. it will recognize the table is empty that satisfy the predicate i=3
49: 	//       and then prune the entire plan)
50: 	config.disabled_optimizers.insert(OptimizerType::STATISTICS_PROPAGATION);
51: 	// we don't support filter pushdown yet in our toy example
52: 	config.disabled_optimizers.insert(OptimizerType::FILTER_PUSHDOWN);
53: 
54: 	DuckDB db(nullptr, &config);
55: 	Connection con(db);
56: 
57: 	// we perform an explicit BEGIN TRANSACTION here
58: 	// since "CreateFunction" will directly poke around in the catalog
59: 	// which requires an active transaction
60: 	con.Query("BEGIN TRANSACTION");
61: 
62: 	// register dummy tables (for our binding purposes)
63: 	con.Query("CREATE TABLE mytable(i INTEGER, j INTEGER)");
64: 	con.Query("CREATE TABLE myothertable(k INTEGER)");
65: 	// contents of the tables
66: 	// mytable:
67: 	// i: 1, 2, 3, 4, 5
68: 	// j: 2, 3, 4, 5, 6
69: 	// myothertable
70: 	// k: 1, 10, 20
71: 	// (see MyScanNode)
72: 
73: 	// register functions and aggregates (for our binding purposes)
74: 	CreateFunction(con, "+", {LogicalType::INTEGER, LogicalType::INTEGER}, LogicalType::INTEGER);
75: 	CreateAggregateFunction(con, "count_star", {}, LogicalType::BIGINT);
76: 	CreateAggregateFunction(con, "sum", {LogicalType::INTEGER}, LogicalType::INTEGER);
77: 
78: 	con.Query("COMMIT");
79: 
80: 	// standard projections
81: 	ExecuteQuery(con, "SELECT * FROM mytable");
82: 	ExecuteQuery(con, "SELECT i FROM mytable");
83: 	ExecuteQuery(con, "SELECT j FROM mytable");
84: 	ExecuteQuery(con, "SELECT k FROM myothertable");
85: 	// some simple filter + projection
86: 	ExecuteQuery(con, "SELECT i+1 FROM mytable WHERE i=3 OR i=4");
87: 	// more complex filters
88: 	ExecuteQuery(con, "SELECT i+1 FROM mytable WHERE (i<=2 AND j<=3) OR (i=4 AND j=5)");
89: 	// aggregate
90: 	ExecuteQuery(con, "SELECT COUNT(*), SUM(i) + 1, SUM(j) + 2 FROM mytable WHERE i>2");
91: 	// with a subquery
92: 	ExecuteQuery(con,
93: 	             "SELECT a, b + 1, c + 2 FROM (SELECT COUNT(*), SUM(i), SUM(j) FROM mytable WHERE i > 2) tbl(a, b, c)");
94: }
95: 
96: //===--------------------------------------------------------------------===//
97: // Example Using Custom Scan Function
98: //===--------------------------------------------------------------------===//
99: void CreateMyScanFunction(Connection &con);
100: 
101: unique_ptr<TableFunctionRef> MyReplacementScan(const string &table_name, void *data) {
102: 	auto table_function = make_unique<TableFunctionRef>();
103: 	vector<unique_ptr<ParsedExpression>> children;
104: 	children.push_back(make_unique<ConstantExpression>(Value(table_name)));
105: 	table_function->function = make_unique<FunctionExpression>("my_scan", move(children));
106: 	return table_function;
107: }
108: 
109: void RunExampleTableScan() {
110: 	// in this example we use our own TableFunction instead of the built-in "seq_scan"
111: 	// this allows us to emit our own statistics without needing to insert them into the DuckDB tables
112: 	// it also allows us to define ourselves what we do/do not support
113: 	// (e.g. we can disable projection or filter pushdown in the table function if desired)
114: 	// this means we don't need to disable optimizers anymore
115: 
116: 	DBConfig config;
117: 	config.initialize_default_database = false;
118: 	config.replacement_scans.push_back(ReplacementScan(MyReplacementScan));
119: 
120: 	DuckDB db(nullptr, &config);
121: 	Connection con(db);
122: 
123: 	// we perform an explicit BEGIN TRANSACTION here
124: 	// since "CreateFunction" will directly poke around in the catalog
125: 	// which requires an active transaction
126: 	con.Query("BEGIN TRANSACTION");
127: 
128: 	// register functions and aggregates (for our binding purposes)
129: 	CreateFunction(con, "+", {LogicalType::INTEGER, LogicalType::INTEGER}, LogicalType::INTEGER);
130: 	CreateAggregateFunction(con, "count_star", {}, LogicalType::BIGINT);
131: 	CreateAggregateFunction(con, "sum", {LogicalType::INTEGER}, LogicalType::INTEGER);
132: 
133: 	CreateMyScanFunction(con);
134: 
135: 	con.Query("COMMIT");
136: 
137: 	// standard projections
138: 	ExecuteQuery(con, "SELECT * FROM mytable");
139: 	ExecuteQuery(con, "SELECT i FROM mytable");
140: 	ExecuteQuery(con, "SELECT j FROM mytable");
141: 	ExecuteQuery(con, "SELECT k FROM myothertable");
142: 	// some simple filter + projection
143: 	ExecuteQuery(con, "SELECT i+1 FROM mytable WHERE i=3 OR i=4");
144: 	// more complex filters
145: 	ExecuteQuery(con, "SELECT i+1 FROM mytable WHERE (i<=2 AND j<=3) OR (i=4 AND j=5)");
146: 	// aggregate
147: 	ExecuteQuery(con, "SELECT COUNT(*), SUM(i) + 1, SUM(j) + 2 FROM mytable WHERE i>2");
148: 	// with a subquery
149: 	ExecuteQuery(con,
150: 	             "SELECT a, b + 1, c + 2 FROM (SELECT COUNT(*), SUM(i), SUM(j) FROM mytable WHERE i > 2) tbl(a, b, c)");
151: }
152: 
153: int main() {
154: 	RunExampleDuckDBCatalog();
155: 	RunExampleTableScan();
156: }
157: 
158: //===--------------------------------------------------------------------===//
159: // Create Dummy Scalar/Aggregate Functions in the Catalog
160: //===--------------------------------------------------------------------===//
161: void CreateFunction(Connection &con, string name, vector<LogicalType> arguments, LogicalType return_type) {
162: 	auto &context = *con.context;
163: 	auto &catalog = Catalog::GetCatalog(context);
164: 
165: 	// we can register multiple functions here if we want overloads
166: 	// you may also want to set has_side_effects or varargs in the ScalarFunction (if required)
167: 	ScalarFunctionSet set(name);
168: 	set.AddFunction(ScalarFunction(move(arguments), move(return_type), nullptr));
169: 
170: 	CreateScalarFunctionInfo info(move(set));
171: 	catalog.CreateFunction(context, &info);
172: }
173: 
174: void CreateAggregateFunction(Connection &con, string name, vector<LogicalType> arguments, LogicalType return_type) {
175: 	auto &context = *con.context;
176: 	auto &catalog = Catalog::GetCatalog(context);
177: 
178: 	// we can register multiple functions here if we want overloads
179: 	AggregateFunctionSet set(name);
180: 	set.AddFunction(AggregateFunction(move(arguments), move(return_type), nullptr, nullptr, nullptr, nullptr, nullptr));
181: 
182: 	CreateAggregateFunctionInfo info(move(set));
183: 	catalog.CreateFunction(context, &info);
184: }
185: 
186: //===--------------------------------------------------------------------===//
187: // Custom Table Scan Function
188: //===--------------------------------------------------------------------===//
189: struct MyBindData : public FunctionData {
190: 	MyBindData(string name_p) : table_name(move(name_p)) {
191: 	}
192: 
193: 	string table_name;
194: };
195: 
196: // contents of the tables
197: // mytable:
198: // i: 1, 2, 3, 4, 5
199: // j: 2, 3, 4, 5, 6
200: // myothertable
201: // k: 1, 10, 20
202: // (see MyScanNode)
203: static unique_ptr<FunctionData> MyScanBind(ClientContext &context, vector<Value> &inputs,
204:                                            named_parameter_map_t &named_parameters,
205:                                            vector<LogicalType> &input_table_types, vector<string> &input_table_names,
206:                                            vector<LogicalType> &return_types, vector<string> &names) {
207: 	auto table_name = inputs[0].ToString();
208: 	if (table_name == "mytable") {
209: 		names.emplace_back("i");
210: 		return_types.emplace_back(LogicalType::INTEGER);
211: 
212: 		names.emplace_back("j");
213: 		return_types.emplace_back(LogicalType::INTEGER);
214: 	} else if (table_name == "myothertable") {
215: 		names.emplace_back("k");
216: 		return_types.emplace_back(LogicalType::INTEGER);
217: 	} else {
218: 		throw std::runtime_error("Unknown table " + table_name);
219: 	}
220: 	auto result = make_unique<MyBindData>(table_name);
221: 	return move(result);
222: }
223: 
224: static unique_ptr<BaseStatistics> MyScanStatistics(ClientContext &context, const FunctionData *bind_data_p,
225:                                                    column_t column_id) {
226: 	auto &bind_data = (MyBindData &)*bind_data_p;
227: 	if (bind_data.table_name == "mytable") {
228: 		if (column_id == 0) {
229: 			// i: 1, 2, 3, 4, 5
230: 			return make_unique<NumericStatistics>(LogicalType::INTEGER, Value::INTEGER(1), Value::INTEGER(5));
231: 		} else if (column_id == 1) {
232: 			// j: 2, 3, 4, 5, 6
233: 			return make_unique<NumericStatistics>(LogicalType::INTEGER, Value::INTEGER(2), Value::INTEGER(6));
234: 		}
235: 	} else if (bind_data.table_name == "myothertable") {
236: 		// k: 1, 10, 20
237: 		return make_unique<NumericStatistics>(LogicalType::INTEGER, Value::INTEGER(1), Value::INTEGER(20));
238: 	}
239: 	return nullptr;
240: }
241: 
242: unique_ptr<NodeStatistics> MyScanCardinality(ClientContext &context, const FunctionData *bind_data_p) {
243: 	auto &bind_data = (MyBindData &)*bind_data_p;
244: 	if (bind_data.table_name == "mytable") {
245: 		// 5 tuples
246: 		return make_unique<NodeStatistics>(5, 5);
247: 	} else if (bind_data.table_name == "myothertable") {
248: 		return make_unique<NodeStatistics>(3, 3);
249: 	}
250: 	return nullptr;
251: }
252: 
253: void CreateMyScanFunction(Connection &con) {
254: 	auto &context = *con.context;
255: 	auto &catalog = Catalog::GetCatalog(context);
256: 
257: 	TableFunction my_scan("my_scan", {LogicalType::VARCHAR}, nullptr, MyScanBind, nullptr, MyScanStatistics, nullptr,
258: 	                      nullptr, MyScanCardinality);
259: 	my_scan.projection_pushdown = true;
260: 	my_scan.filter_pushdown = false;
261: 
262: 	CreateTableFunctionInfo info(move(my_scan));
263: 	catalog.CreateTableFunction(context, &info);
264: }
265: 
266: //===--------------------------------------------------------------------===//
267: // Example Execution Engine: Row-based volcano style that only supports int32
268: //===--------------------------------------------------------------------===//
269: class MyNode {
270: public:
271: 	virtual ~MyNode() {
272: 	}
273: 	virtual vector<int> GetNextRow() = 0;
274: 
275: 	unique_ptr<MyNode> child;
276: };
277: 
278: class MyPlanGenerator {
279: public:
280: 	unique_ptr<MyNode> TransformPlan(LogicalOperator &op);
281: };
282: 
283: void ExecuteQuery(Connection &con, const string &query) {
284: 	// create the logical plan
285: 	auto plan = con.ExtractPlan(query);
286: 	plan->Print();
287: 
288: 	// transform the logical plan into our own plan
289: 	MyPlanGenerator generator;
290: 	auto my_plan = generator.TransformPlan(*plan);
291: 
292: 	// execute the plan and print the result
293: 	printf("Executing query: %s\n", query.c_str());
294: 	printf("----------------------\n");
295: 	vector<int> result;
296: 	while (true) {
297: 		result = my_plan->GetNextRow();
298: 		if (result.empty()) {
299: 			break;
300: 		}
301: 		string str;
302: 		for (size_t i = 0; i < result.size(); i++) {
303: 			if (i > 0) {
304: 				str += ", ";
305: 			}
306: 			str += std::to_string(result[i]);
307: 		}
308: 		printf("%s\n", str.c_str());
309: 	}
310: 	printf("----------------------\n");
311: }
312: 
313: //===--------------------------------------------------------------------===//
314: // Table Scan Node
315: //===--------------------------------------------------------------------===//
316: class MyScanNode : public MyNode {
317: public:
318: 	MyScanNode(string name_p, vector<column_t> column_ids_p) : name(move(name_p)), column_ids(move(column_ids_p)) {
319: 		// fill up the data based on which table we are scanning
320: 		if (name == "mytable") {
321: 			// i
322: 			data.push_back({1, 2, 3, 4, 5});
323: 			// j
324: 			data.push_back({2, 3, 4, 5, 6});
325: 		} else if (name == "myothertable") {
326: 			// k
327: 			data.push_back({1, 10, 20});
328: 		} else {
329: 			throw std::runtime_error("Unsupported table!");
330: 		}
331: 	}
332: 
333: 	string name;
334: 	vector<column_t> column_ids;
335: 	vector<vector<int>> data;
336: 	int index = 0;
337: 
338: 	vector<int> GetNextRow() override {
339: 		vector<int> result;
340: 		if (index >= data[0].size()) {
341: 			return result;
342: 		}
343: 		// fill the result based on the projection list (column_ids)
344: 		for (size_t i = 0; i < column_ids.size(); i++) {
345: 			result.push_back(data[column_ids[i]][index]);
346: 		}
347: 		index++;
348: 		return result;
349: 	};
350: };
351: 
352: //===--------------------------------------------------------------------===//
353: // Expression Execution
354: //===--------------------------------------------------------------------===//
355: 
356: // note that we run expression execution directly on top of DuckDB expressions
357: // it is also possible to transform the expressions into our own expressions (MyExpression)
358: class MyExpressionExecutor {
359: public:
360: 	MyExpressionExecutor(vector<int> current_row_p) : current_row(move(current_row_p)) {
361: 	}
362: 
363: 	vector<int> current_row;
364: 
365: 	int Execute(Expression &expression);
366: 
367: protected:
368: 	int Execute(BoundReferenceExpression &expr);
369: 	int Execute(BoundCastExpression &expr);
370: 	int Execute(BoundComparisonExpression &expr);
371: 	int Execute(BoundConjunctionExpression &expr);
372: 	int Execute(BoundConstantExpression &expr);
373: 	int Execute(BoundFunctionExpression &expr);
374: };
375: 
376: //===--------------------------------------------------------------------===//
377: // Filter
378: //===--------------------------------------------------------------------===//
379: class MyFilterNode : public MyNode {
380: public:
381: 	MyFilterNode(unique_ptr<Expression> filter_node) : filter(move(filter_node)) {
382: 	}
383: 
384: 	unique_ptr<Expression> filter;
385: 
386: 	bool ExecuteFilter(Expression &expr, const vector<int> &current_row) {
387: 		MyExpressionExecutor executor(current_row);
388: 		auto val = executor.Execute(expr);
389: 		return val != 0;
390: 	}
391: 
392: 	vector<int> GetNextRow() override {
393: 		D_ASSERT(child);
394: 		while (true) {
395: 			auto next = child->GetNextRow();
396: 			if (next.empty()) {
397: 				return next;
398: 			}
399: 			// check if the filter passes, if it does we return the row
400: 			// if not we return the next row
401: 			if (ExecuteFilter(*filter, next)) {
402: 				return next;
403: 			}
404: 		}
405: 	};
406: };
407: 
408: //===--------------------------------------------------------------------===//
409: // Projection
410: //===--------------------------------------------------------------------===//
411: class MyProjectionNode : public MyNode {
412: public:
413: 	MyProjectionNode(vector<unique_ptr<Expression>> projections_p) : projections(move(projections_p)) {
414: 	}
415: 
416: 	vector<unique_ptr<Expression>> projections;
417: 
418: 	vector<int> GetNextRow() override {
419: 		auto next = child->GetNextRow();
420: 		if (next.empty()) {
421: 			return next;
422: 		}
423: 		MyExpressionExecutor executor(next);
424: 		vector<int> result;
425: 		for (size_t i = 0; i < projections.size(); i++) {
426: 			result.push_back(executor.Execute(*projections[i]));
427: 		}
428: 		return result;
429: 	};
430: };
431: 
432: //===--------------------------------------------------------------------===//
433: // Aggregate
434: //===--------------------------------------------------------------------===//
435: class MyAggregateNode : public MyNode {
436: public:
437: 	MyAggregateNode(vector<unique_ptr<Expression>> aggregates_p) : aggregates(move(aggregates_p)) {
438: 		// initialize aggregate states to 0
439: 		aggregate_states.resize(aggregates.size(), 0);
440: 	}
441: 
442: 	vector<unique_ptr<Expression>> aggregates;
443: 	vector<int> aggregate_states;
444: 
445: 	void ExecuteAggregate(MyExpressionExecutor &executor, int index, BoundAggregateExpression &expr) {
446: 		if (expr.function.name == "sum") {
447: 			int child = executor.Execute(*expr.children[0]);
448: 			aggregate_states[index] += child;
449: 		} else if (expr.function.name == "count_star") {
450: 			aggregate_states[index]++;
451: 		} else {
452: 			throw std::runtime_error("Unsupported aggregate function " + expr.function.name);
453: 		}
454: 	}
455: 
456: 	vector<int> GetNextRow() override {
457: 		if (aggregate_states.empty()) {
458: 			// finished aggregating
459: 			return aggregate_states;
460: 		}
461: 		while (true) {
462: 			auto next = child->GetNextRow();
463: 			if (next.empty()) {
464: 				return move(aggregate_states);
465: 			}
466: 			MyExpressionExecutor executor(next);
467: 			for (size_t i = 0; i < aggregates.size(); i++) {
468: 				ExecuteAggregate(executor, i, (BoundAggregateExpression &)*aggregates[i]);
469: 			}
470: 		}
471: 	};
472: };
473: 
474: //===--------------------------------------------------------------------===//
475: // Plan Transformer - Transform a DuckDB logical plan into a custom plan (MyNode)
476: //===--------------------------------------------------------------------===//
477: unique_ptr<MyNode> MyPlanGenerator::TransformPlan(LogicalOperator &op) {
478: 	switch (op.type) {
479: 	case LogicalOperatorType::LOGICAL_PROJECTION: {
480: 		// projection
481: 		auto child = TransformPlan(*op.children[0]);
482: 		auto node = make_unique<MyProjectionNode>(move(op.expressions));
483: 		node->child = move(child);
484: 		return move(node);
485: 	}
486: 	case LogicalOperatorType::LOGICAL_FILTER: {
487: 		// filter
488: 		auto child = TransformPlan(*op.children[0]);
489: 		auto node = make_unique<MyFilterNode>(move(op.expressions[0]));
490: 		node->child = move(child);
491: 		return move(node);
492: 	}
493: 	case LogicalOperatorType::LOGICAL_AGGREGATE_AND_GROUP_BY: {
494: 		auto &aggr = (LogicalAggregate &)op;
495: 		if (!aggr.groups.empty()) {
496: 			throw std::runtime_error("Grouped aggregate not supported");
497: 		}
498: 		auto child = TransformPlan(*op.children[0]);
499: 		auto node = make_unique<MyAggregateNode>(move(op.expressions));
500: 		node->child = move(child);
501: 		return move(node);
502: 	}
503: 	case LogicalOperatorType::LOGICAL_GET: {
504: 		auto &get = (LogicalGet &)op;
505: 		// table scan or table function
506: 
507: 		// get nodes have two properties: table_filters (filter pushdown) and column_ids (projection pushdown)
508: 		// table_filters are only generated if optimizers are enabled (through the filter pushdown optimizer)
509: 		// column_ids are always generated
510: 		// the column_ids specify which columns should be emitted and in which order
511: 		// e.g. if we have a table "i, j, k" and the column_ids are {0, 2} we should emit ONLY "i, k" and in that order
512: 		if (get.function.name == "seq_scan") {
513: 			// built-in table scan
514: 			auto &table = (TableScanBindData &)*get.bind_data;
515: 			if (!get.table_filters.filters.empty()) {
516: 				// note: filter pushdown will only be triggered if optimizers are enabled
517: 				throw std::runtime_error("Filter pushdown unsupported");
518: 			}
519: 			return make_unique<MyScanNode>(table.table->name, get.column_ids);
520: 		} else if (get.function.name == "my_scan") {
521: 			// our own scan
522: 			auto &my_bind_data = (MyBindData &)*get.bind_data;
523: 			return make_unique<MyScanNode>(my_bind_data.table_name, get.column_ids);
524: 		} else {
525: 			throw std::runtime_error("Unsupported table function");
526: 		}
527: 	}
528: 	default:
529: 		throw std::runtime_error("Unsupported logical operator for transformation");
530: 	}
531: }
532: 
533: //===--------------------------------------------------------------------===//
534: // Expression Execution for various built-in expressions
535: //===--------------------------------------------------------------------===//
536: int MyExpressionExecutor::Execute(BoundReferenceExpression &expr) {
537: 	// column references (e.g. "SELECT a FROM tbl") are turned into BoundReferences
538: 	// these refer to an index within the row they come from
539: 	// because of that it is important to correctly handle the get.column_ids
540: 	return current_row[expr.index];
541: }
542: 
543: int MyExpressionExecutor::Execute(BoundCastExpression &expr) {
544: 	return Execute(*expr.child);
545: }
546: 
547: int MyExpressionExecutor::Execute(BoundConjunctionExpression &expr) {
548: 	int result;
549: 	if (expr.GetExpressionType() == ExpressionType::CONJUNCTION_AND) {
550: 		result = 1;
551: 		for (size_t i = 0; i < expr.children.size(); i++) {
552: 			result = result && Execute(*expr.children[i]);
553: 		}
554: 	} else if (expr.GetExpressionType() == ExpressionType::CONJUNCTION_OR) {
555: 		result = 0;
556: 		for (size_t i = 0; i < expr.children.size(); i++) {
557: 			result = result || Execute(*expr.children[i]);
558: 		}
559: 	} else {
560: 		throw std::runtime_error("Unrecognized conjunction (this shouldn't be possible)");
561: 	}
562: 	return result;
563: }
564: 
565: int MyExpressionExecutor::Execute(BoundConstantExpression &expr) {
566: 	return expr.value.GetValue<int32_t>();
567: }
568: 
569: int MyExpressionExecutor::Execute(BoundComparisonExpression &expr) {
570: 	auto lchild = Execute(*expr.left);
571: 	auto rchild = Execute(*expr.right);
572: 	bool cmp;
573: 	switch (expr.GetExpressionType()) {
574: 	case ExpressionType::COMPARE_EQUAL:
575: 		cmp = lchild == rchild;
576: 		break;
577: 	case ExpressionType::COMPARE_NOTEQUAL:
578: 		cmp = lchild != rchild;
579: 		break;
580: 	case ExpressionType::COMPARE_LESSTHAN:
581: 		cmp = lchild < rchild;
582: 		break;
583: 	case ExpressionType::COMPARE_GREATERTHAN:
584: 		cmp = lchild > rchild;
585: 		break;
586: 	case ExpressionType::COMPARE_LESSTHANOREQUALTO:
587: 		cmp = lchild <= rchild;
588: 		break;
589: 	case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
590: 		cmp = lchild >= rchild;
591: 		break;
592: 	default:
593: 		throw std::runtime_error("Unsupported comparison");
594: 	}
595: 	return cmp ? 1 : 0;
596: }
597: 
598: //===--------------------------------------------------------------------===//
599: // Expression Execution for built-in functions
600: //===--------------------------------------------------------------------===//
601: int MyExpressionExecutor::Execute(BoundFunctionExpression &expr) {
602: 	if (expr.function.name == "+") {
603: 		auto lchild = Execute(*expr.children[0]);
604: 		auto rchild = Execute(*expr.children[1]);
605: 		return lchild + rchild;
606: 	}
607: 	throw std::runtime_error("Unsupported function " + expr.function.name);
608: }
609: 
610: int MyExpressionExecutor::Execute(Expression &expression) {
611: 	switch (expression.GetExpressionClass()) {
612: 	case ExpressionClass::BOUND_REF:
613: 		return Execute((BoundReferenceExpression &)expression);
614: 	case ExpressionClass::BOUND_CAST:
615: 		return Execute((BoundCastExpression &)expression);
616: 	case ExpressionClass::BOUND_COMPARISON:
617: 		return Execute((BoundComparisonExpression &)expression);
618: 	case ExpressionClass::BOUND_CONJUNCTION:
619: 		return Execute((BoundConjunctionExpression &)expression);
620: 	case ExpressionClass::BOUND_CONSTANT:
621: 		return Execute((BoundConstantExpression &)expression);
622: 	case ExpressionClass::BOUND_FUNCTION:
623: 		return Execute((BoundFunctionExpression &)expression);
624: 	default:
625: 		throw std::runtime_error("Unsupported expression for expression executor " + expression.ToString());
626: 	}
627: }
[end of examples/standalone-plan/main.cpp]
[start of extension/icu/icu-extension.cpp]
1: #define DUCKDB_EXTENSION_MAIN
2: 
3: #include "include/icu-extension.hpp"
4: #include "include/icu-collate.hpp"
5: #include "include/icu-dateadd.hpp"
6: #include "include/icu-datepart.hpp"
7: #include "include/icu-datesub.hpp"
8: #include "include/icu-datetrunc.hpp"
9: #include "include/icu-makedate.hpp"
10: 
11: #include "duckdb/main/database.hpp"
12: #include "duckdb/main/connection.hpp"
13: #include "duckdb/main/config.hpp"
14: 
15: #include "duckdb/common/string_util.hpp"
16: #include "duckdb/planner/expression/bound_function_expression.hpp"
17: #include "duckdb/function/scalar_function.hpp"
18: #include "duckdb/common/vector_operations/unary_executor.hpp"
19: #include "duckdb/parser/parsed_data/create_collation_info.hpp"
20: #include "duckdb/parser/parsed_data/create_scalar_function_info.hpp"
21: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
22: #include "duckdb/execution/expression_executor.hpp"
23: #include "duckdb/catalog/catalog.hpp"
24: 
25: #include <cassert>
26: 
27: namespace duckdb {
28: 
29: struct IcuBindData : public FunctionData {
30: 	std::unique_ptr<icu::Collator> collator;
31: 	string language;
32: 	string country;
33: 
34: 	IcuBindData(string language_p, string country_p) : language(move(language_p)), country(move(country_p)) {
35: 		UErrorCode status = U_ZERO_ERROR;
36: 		auto locale = icu::Locale(language.c_str(), country.c_str());
37: 		if (locale.isBogus()) {
38: 			throw InternalException("Locale is bogus!?");
39: 		}
40: 		this->collator = std::unique_ptr<icu::Collator>(icu::Collator::createInstance(locale, status));
41: 		if (U_FAILURE(status)) {
42: 			auto error_name = u_errorName(status);
43: 			throw InternalException("Failed to create ICU collator: %s (language: %s, country: %s)", error_name,
44: 			                        language, country);
45: 		}
46: 	}
47: 
48: 	unique_ptr<FunctionData> Copy() override {
49: 		return make_unique<IcuBindData>(language, country);
50: 	}
51: };
52: 
53: static int32_t ICUGetSortKey(icu::Collator &collator, string_t input, unique_ptr<char[]> &buffer,
54:                              int32_t &buffer_size) {
55: 	int32_t string_size =
56: 	    collator.getSortKey(icu::UnicodeString::fromUTF8(icu::StringPiece(input.GetDataUnsafe(), input.GetSize())),
57: 	                        (uint8_t *)buffer.get(), buffer_size);
58: 	if (string_size > buffer_size) {
59: 		// have to resize the buffer
60: 		buffer_size = string_size;
61: 		buffer = unique_ptr<char[]>(new char[buffer_size]);
62: 
63: 		string_size =
64: 		    collator.getSortKey(icu::UnicodeString::fromUTF8(icu::StringPiece(input.GetDataUnsafe(), input.GetSize())),
65: 		                        (uint8_t *)buffer.get(), buffer_size);
66: 	}
67: 	return string_size;
68: }
69: 
70: static void ICUCollateFunction(DataChunk &args, ExpressionState &state, Vector &result) {
71: 	const char HEX_TABLE[] = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F'};
72: 
73: 	auto &func_expr = (BoundFunctionExpression &)state.expr;
74: 	auto &info = (IcuBindData &)*func_expr.bind_info;
75: 	auto &collator = *info.collator;
76: 
77: 	unique_ptr<char[]> buffer;
78: 	int32_t buffer_size = 0;
79: 	UnaryExecutor::Execute<string_t, string_t>(args.data[0], result, args.size(), [&](string_t input) {
80: 		// create a sort key from the string
81: 		const auto string_size = idx_t(ICUGetSortKey(collator, input, buffer, buffer_size));
82: 		// convert the sort key to hexadecimal
83: 		auto str_result = StringVector::EmptyString(result, (string_size - 1) * 2);
84: 		auto str_data = str_result.GetDataWriteable();
85: 		for (idx_t i = 0; i < string_size - 1; i++) {
86: 			uint8_t byte = uint8_t(buffer[i]);
87: 			D_ASSERT(byte != 0);
88: 			str_data[i * 2] = HEX_TABLE[byte / 16];
89: 			str_data[i * 2 + 1] = HEX_TABLE[byte % 16];
90: 		}
91: 		// printf("%s: %s\n", input.GetString().c_str(), str_result.GetString().c_str());
92: 		return str_result;
93: 	});
94: }
95: 
96: static unique_ptr<FunctionData> ICUCollateBind(ClientContext &context, ScalarFunction &bound_function,
97:                                                vector<unique_ptr<Expression>> &arguments) {
98: 	auto splits = StringUtil::Split(bound_function.name, "_");
99: 	if (splits.size() == 1) {
100: 		return make_unique<IcuBindData>(splits[0], "");
101: 	} else if (splits.size() == 2) {
102: 		return make_unique<IcuBindData>(splits[0], splits[1]);
103: 	} else {
104: 		throw InternalException("Expected one or two splits");
105: 	}
106: }
107: 
108: static unique_ptr<FunctionData> ICUSortKeyBind(ClientContext &context, ScalarFunction &bound_function,
109:                                                vector<unique_ptr<Expression>> &arguments) {
110: 	if (!arguments[1]->IsFoldable()) {
111: 		throw NotImplementedException("ICU_SORT_KEY(VARCHAR, VARCHAR) with non-constant collation is not supported");
112: 	}
113: 	Value val = ExpressionExecutor::EvaluateScalar(*arguments[1]).CastAs(LogicalType::VARCHAR);
114: 	if (val.IsNull()) {
115: 		throw NotImplementedException("ICU_SORT_KEY(VARCHAR, VARCHAR) expected a non-null collation");
116: 	}
117: 	auto splits = StringUtil::Split(StringValue::Get(val), "_");
118: 	if (splits.size() == 1) {
119: 		return make_unique<IcuBindData>(splits[0], "");
120: 	} else if (splits.size() == 2) {
121: 		return make_unique<IcuBindData>(splits[0], splits[1]);
122: 	} else {
123: 		throw InternalException("Expected one or two splits");
124: 	}
125: }
126: 
127: static ScalarFunction GetICUFunction(const string &collation) {
128: 	return ScalarFunction(collation, {LogicalType::VARCHAR}, LogicalType::VARCHAR, ICUCollateFunction, false,
129: 	                      ICUCollateBind);
130: }
131: 
132: static void SetICUTimeZone(ClientContext &context, SetScope scope, Value &parameter) {
133: 	icu::StringPiece utf8(StringValue::Get(parameter));
134: 	const auto uid = icu::UnicodeString::fromUTF8(utf8);
135: 	std::unique_ptr<icu::TimeZone> tz(icu::TimeZone::createTimeZone(uid));
136: 	if (*tz == icu::TimeZone::getUnknown()) {
137: 		throw NotImplementedException("Unknown TimeZone setting");
138: 	}
139: }
140: 
141: struct ICUTimeZoneData : public FunctionOperatorData {
142: 	ICUTimeZoneData() : tzs(icu::TimeZone::createEnumeration()) {
143: 		UErrorCode status = U_ZERO_ERROR;
144: 		std::unique_ptr<icu::Calendar> calendar(icu::Calendar::createInstance(status));
145: 		now = calendar->getNow();
146: 	}
147: 
148: 	std::unique_ptr<icu::StringEnumeration> tzs;
149: 	UDate now;
150: };
151: 
152: static unique_ptr<FunctionData> ICUTimeZoneBind(ClientContext &context, vector<Value> &inputs,
153:                                                 named_parameter_map_t &named_parameters,
154:                                                 vector<LogicalType> &input_table_types,
155:                                                 vector<string> &input_table_names, vector<LogicalType> &return_types,
156:                                                 vector<string> &names) {
157: 	names.emplace_back("name");
158: 	return_types.emplace_back(LogicalType::VARCHAR);
159: 	names.emplace_back("abbrev");
160: 	return_types.emplace_back(LogicalType::VARCHAR);
161: 	names.emplace_back("utc_offset");
162: 	return_types.emplace_back(LogicalType::INTERVAL);
163: 	names.emplace_back("is_dst");
164: 	return_types.emplace_back(LogicalType::BOOLEAN);
165: 
166: 	return nullptr;
167: }
168: 
169: static unique_ptr<FunctionOperatorData> ICUTimeZoneInit(ClientContext &context, const FunctionData *bind_data,
170:                                                         const vector<column_t> &column_ids,
171:                                                         TableFilterCollection *filters) {
172: 	return make_unique<ICUTimeZoneData>();
173: }
174: 
175: static void ICUTimeZoneCleanup(ClientContext &context, const FunctionData *bind_data,
176:                                FunctionOperatorData *operator_state) {
177: 	auto &data = (ICUTimeZoneData &)*operator_state;
178: 	(void)data.tzs.reset();
179: }
180: 
181: static void ICUTimeZoneFunction(ClientContext &context, const FunctionData *bind_data,
182:                                 FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
183: 	auto &data = (ICUTimeZoneData &)*operator_state;
184: 	idx_t index = 0;
185: 	while (index < STANDARD_VECTOR_SIZE) {
186: 		UErrorCode status = U_ZERO_ERROR;
187: 		auto long_id = data.tzs->snext(status);
188: 		if (U_FAILURE(status) || !long_id) {
189: 			break;
190: 		}
191: 
192: 		//	The LONG name is the one we looked up
193: 		std::string utf8;
194: 		long_id->toUTF8String(utf8);
195: 		output.SetValue(0, index, Value(utf8));
196: 
197: 		//	We don't have the zone tree for determining abbreviated names,
198: 		//	so the SHORT name is the first equivalent TZ without a slash.
199: 		icu::UnicodeString short_id = *long_id;
200: 		const auto nIDs = icu::TimeZone::countEquivalentIDs(*long_id);
201: 		for (int32_t idx = 0; idx < nIDs; ++idx) {
202: 			const auto eid = icu::TimeZone::getEquivalentID(*long_id, idx);
203: 			if (eid.indexOf(char16_t('/')) < 0) {
204: 				short_id = eid;
205: 				break;
206: 			}
207: 		}
208: 
209: 		utf8.clear();
210: 		short_id.toUTF8String(utf8);
211: 		output.SetValue(1, index, Value(utf8));
212: 
213: 		std::unique_ptr<icu::TimeZone> tz(icu::TimeZone::createTimeZone(*long_id));
214: 		int32_t raw_offset_ms;
215: 		int32_t dst_offset_ms;
216: 		tz->getOffset(data.now, false, raw_offset_ms, dst_offset_ms, status);
217: 		if (U_FAILURE(status)) {
218: 			break;
219: 		}
220: 
221: 		output.SetValue(2, index, Value::INTERVAL(Interval::FromMicro(raw_offset_ms * Interval::MICROS_PER_MSEC)));
222: 		output.SetValue(3, index, Value(dst_offset_ms != 0));
223: 		++index;
224: 	}
225: 	output.SetCardinality(index);
226: }
227: 
228: struct ICUCalendarData : public FunctionOperatorData {
229: 	ICUCalendarData() {
230: 		// All calendars are available in all locales
231: 		UErrorCode status = U_ZERO_ERROR;
232: 		calendars.reset(icu::Calendar::getKeywordValuesForLocale("calendar", icu::Locale::getDefault(), false, status));
233: 	}
234: 
235: 	std::unique_ptr<icu::StringEnumeration> calendars;
236: };
237: 
238: static unique_ptr<FunctionData> ICUCalendarBind(ClientContext &context, vector<Value> &inputs,
239:                                                 named_parameter_map_t &named_parameters,
240:                                                 vector<LogicalType> &input_table_types,
241:                                                 vector<string> &input_table_names, vector<LogicalType> &return_types,
242:                                                 vector<string> &names) {
243: 	names.emplace_back("name");
244: 	return_types.emplace_back(LogicalType::VARCHAR);
245: 
246: 	return nullptr;
247: }
248: 
249: static unique_ptr<FunctionOperatorData> ICUCalendarInit(ClientContext &context, const FunctionData *bind_data,
250:                                                         const vector<column_t> &column_ids,
251:                                                         TableFilterCollection *filters) {
252: 	return make_unique<ICUCalendarData>();
253: }
254: 
255: static void ICUCalendarFunction(ClientContext &context, const FunctionData *bind_data,
256:                                 FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
257: 	auto &data = (ICUCalendarData &)*operator_state;
258: 	idx_t index = 0;
259: 	while (index < STANDARD_VECTOR_SIZE) {
260: 		if (!data.calendars) {
261: 			break;
262: 		}
263: 
264: 		UErrorCode status = U_ZERO_ERROR;
265: 		auto calendar = data.calendars->snext(status);
266: 		if (U_FAILURE(status) || !calendar) {
267: 			break;
268: 		}
269: 
270: 		//	The calendar name is all we have
271: 		std::string utf8;
272: 		calendar->toUTF8String(utf8);
273: 		output.SetValue(0, index, Value(utf8));
274: 
275: 		++index;
276: 	}
277: 	output.SetCardinality(index);
278: }
279: 
280: static void ICUCalendarCleanup(ClientContext &context, const FunctionData *bind_data,
281:                                FunctionOperatorData *operator_state) {
282: 	auto &data = (ICUCalendarData &)*operator_state;
283: 	(void)data.calendars.reset();
284: }
285: 
286: static void SetICUCalendar(ClientContext &context, SetScope scope, Value &parameter) {
287: 	const auto name = parameter.Value::GetValueUnsafe<string>();
288: 	string locale_key = "@calendar=" + name;
289: 	icu::Locale locale(locale_key.c_str());
290: 
291: 	UErrorCode status = U_ZERO_ERROR;
292: 	std::unique_ptr<icu::Calendar> cal(icu::Calendar::createInstance(locale, status));
293: 	if (U_FAILURE(status) || name != cal->getType()) {
294: 		throw NotImplementedException("Unknown Calendar setting");
295: 	}
296: }
297: 
298: void ICUExtension::Load(DuckDB &db) {
299: 	Connection con(db);
300: 	con.BeginTransaction();
301: 
302: 	auto &catalog = Catalog::GetCatalog(*con.context);
303: 
304: 	// iterate over all the collations
305: 	int32_t count;
306: 	auto locales = icu::Collator::getAvailableLocales(count);
307: 	for (int32_t i = 0; i < count; i++) {
308: 		string collation;
309: 		if (string(locales[i].getCountry()).empty()) {
310: 			// language only
311: 			collation = locales[i].getLanguage();
312: 		} else {
313: 			// language + country
314: 			collation = locales[i].getLanguage() + string("_") + locales[i].getCountry();
315: 		}
316: 		collation = StringUtil::Lower(collation);
317: 
318: 		CreateCollationInfo info(collation, GetICUFunction(collation), false, true);
319: 		info.on_conflict = OnCreateConflict::IGNORE_ON_CONFLICT;
320: 		catalog.CreateCollation(*con.context, &info);
321: 	}
322: 	ScalarFunction sort_key("icu_sort_key", {LogicalType::VARCHAR, LogicalType::VARCHAR}, LogicalType::VARCHAR,
323: 	                        ICUCollateFunction, false, ICUSortKeyBind);
324: 
325: 	CreateScalarFunctionInfo sort_key_info(move(sort_key));
326: 	catalog.CreateFunction(*con.context, &sort_key_info);
327: 
328: 	// Time Zones
329: 	auto &config = DBConfig::GetConfig(*db.instance);
330: 	config.AddExtensionOption("TimeZone", "The current time zone", LogicalType::VARCHAR, SetICUTimeZone);
331: 	std::unique_ptr<icu::TimeZone> tz(icu::TimeZone::createDefault());
332: 	icu::UnicodeString tz_id;
333: 	std::string tz_string;
334: 	tz->getID(tz_id).toUTF8String(tz_string);
335: 	config.set_variables["TimeZone"] = Value(tz_string);
336: 
337: 	TableFunction tz_names("pg_timezone_names", {}, ICUTimeZoneFunction, ICUTimeZoneBind, ICUTimeZoneInit, nullptr,
338: 	                       ICUTimeZoneCleanup);
339: 	CreateTableFunctionInfo tz_names_info(move(tz_names));
340: 	catalog.CreateTableFunction(*con.context, &tz_names_info);
341: 
342: 	RegisterICUDateAddFunctions(*con.context);
343: 	RegisterICUDatePartFunctions(*con.context);
344: 	RegisterICUDateSubFunctions(*con.context);
345: 	RegisterICUDateTruncFunctions(*con.context);
346: 	RegisterICUMakeDateFunctions(*con.context);
347: 
348: 	// Calendars
349: 	config.AddExtensionOption("Calendar", "The current calendar", LogicalType::VARCHAR, SetICUCalendar);
350: 	UErrorCode status = U_ZERO_ERROR;
351: 	std::unique_ptr<icu::Calendar> cal(icu::Calendar::createInstance(status));
352: 	config.set_variables["Calendar"] = Value(cal->getType());
353: 
354: 	TableFunction cal_names("icu_calendar_names", {}, ICUCalendarFunction, ICUCalendarBind, ICUCalendarInit, nullptr,
355: 	                        ICUCalendarCleanup);
356: 	CreateTableFunctionInfo cal_names_info(move(cal_names));
357: 	catalog.CreateTableFunction(*con.context, &cal_names_info);
358: 
359: 	con.Commit();
360: }
361: 
362: std::string ICUExtension::Name() {
363: 	return "icu";
364: }
365: 
366: } // namespace duckdb
367: 
368: extern "C" {
369: 
370: DUCKDB_EXTENSION_API void icu_init(duckdb::DatabaseInstance &db) { // NOLINT
371: 	duckdb::DuckDB db_wrapper(db);
372: 	db_wrapper.LoadExtension<duckdb::ICUExtension>();
373: }
374: 
375: DUCKDB_EXTENSION_API const char *icu_version() { // NOLINT
376: 	return duckdb::DuckDB::LibraryVersion();
377: }
378: }
379: 
380: #ifndef DUCKDB_EXTENSION_MAIN
381: #error DUCKDB_EXTENSION_MAIN not defined
382: #endif
[end of extension/icu/icu-extension.cpp]
[start of extension/parquet/parquet-extension.cpp]
1: #define DUCKDB_EXTENSION_MAIN
2: 
3: #include <string>
4: #include <vector>
5: #include <fstream>
6: #include <iostream>
7: 
8: #include "parquet-extension.hpp"
9: #include "parquet_reader.hpp"
10: #include "parquet_writer.hpp"
11: #include "parquet_metadata.hpp"
12: #include "zstd_file_system.hpp"
13: 
14: #include "duckdb.hpp"
15: #ifndef DUCKDB_AMALGAMATION
16: #include "duckdb/common/file_system.hpp"
17: #include "duckdb/common/types/chunk_collection.hpp"
18: #include "duckdb/function/copy_function.hpp"
19: #include "duckdb/function/table_function.hpp"
20: #include "duckdb/common/file_system.hpp"
21: #include "duckdb/parallel/parallel_state.hpp"
22: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
23: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
24: 
25: #include "duckdb/common/enums/file_compression_type.hpp"
26: #include "duckdb/main/config.hpp"
27: #include "duckdb/parser/expression/constant_expression.hpp"
28: #include "duckdb/parser/expression/function_expression.hpp"
29: #include "duckdb/parser/tableref/table_function_ref.hpp"
30: 
31: #include "duckdb/storage/statistics/base_statistics.hpp"
32: 
33: #include "duckdb/main/client_context.hpp"
34: #include "duckdb/catalog/catalog.hpp"
35: #endif
36: 
37: namespace duckdb {
38: 
39: struct ParquetReadBindData : public FunctionData {
40: 	shared_ptr<ParquetReader> initial_reader;
41: 	vector<string> files;
42: 	vector<column_t> column_ids;
43: 	atomic<idx_t> chunk_count;
44: 	atomic<idx_t> cur_file;
45: 	vector<string> names;
46: 	vector<LogicalType> types;
47: };
48: 
49: struct ParquetReadOperatorData : public FunctionOperatorData {
50: 	shared_ptr<ParquetReader> reader;
51: 	ParquetReaderScanState scan_state;
52: 	bool is_parallel;
53: 	idx_t file_index;
54: 	vector<column_t> column_ids;
55: 	TableFilterSet *table_filters;
56: };
57: 
58: struct ParquetReadParallelState : public ParallelState {
59: 	mutex lock;
60: 	shared_ptr<ParquetReader> current_reader;
61: 	idx_t file_index;
62: 	idx_t row_group_index;
63: };
64: 
65: class ParquetScanFunction {
66: public:
67: 	static TableFunctionSet GetFunctionSet() {
68: 		TableFunctionSet set("parquet_scan");
69: 		auto table_function =
70: 		    TableFunction({LogicalType::VARCHAR}, ParquetScanImplementation, ParquetScanBind, ParquetScanInit,
71: 		                  /* statistics */ ParquetScanStats, /* cleanup */ nullptr,
72: 		                  /* dependency */ nullptr, ParquetCardinality,
73: 		                  /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr, ParquetScanMaxThreads,
74: 		                  ParquetInitParallelState, ParquetScanFuncParallel, ParquetScanParallelInit,
75: 		                  ParquetParallelStateNext, true, true, ParquetProgress);
76: 		table_function.named_parameters["binary_as_string"] = LogicalType::BOOLEAN;
77: 		set.AddFunction(table_function);
78: 		table_function = TableFunction({LogicalType::LIST(LogicalType::VARCHAR)}, ParquetScanImplementation,
79: 		                               ParquetScanBindList, ParquetScanInit, /* statistics */ ParquetScanStats,
80: 		                               /* cleanup */ nullptr,
81: 		                               /* dependency */ nullptr, ParquetCardinality,
82: 		                               /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr,
83: 		                               ParquetScanMaxThreads, ParquetInitParallelState, ParquetScanFuncParallel,
84: 		                               ParquetScanParallelInit, ParquetParallelStateNext, true, true, ParquetProgress);
85: 		table_function.named_parameters["binary_as_string"] = LogicalType::BOOLEAN;
86: 		set.AddFunction(table_function);
87: 		return set;
88: 	}
89: 
90: 	static unique_ptr<FunctionData> ParquetReadBind(ClientContext &context, CopyInfo &info,
91: 	                                                vector<string> &expected_names,
92: 	                                                vector<LogicalType> &expected_types) {
93: 		D_ASSERT(expected_names.size() == expected_types.size());
94: 		for (auto &option : info.options) {
95: 			auto loption = StringUtil::Lower(option.first);
96: 			if (loption == "compression" || loption == "codec") {
97: 				// CODEC option has no effect on parquet read: we determine codec from the file
98: 				continue;
99: 			} else {
100: 				throw NotImplementedException("Unsupported option for COPY FROM parquet: %s", option.first);
101: 			}
102: 		}
103: 		auto result = make_unique<ParquetReadBindData>();
104: 
105: 		FileSystem &fs = FileSystem::GetFileSystem(context);
106: 		result->files = fs.Glob(info.file_path, context);
107: 		if (result->files.empty()) {
108: 			throw IOException("No files found that match the pattern \"%s\"", info.file_path);
109: 		}
110: 		ParquetOptions parquet_options(context);
111: 		result->initial_reader = make_shared<ParquetReader>(context, result->files[0], expected_types, parquet_options);
112: 		result->names = result->initial_reader->names;
113: 		result->types = result->initial_reader->return_types;
114: 		return move(result);
115: 	}
116: 
117: 	static unique_ptr<BaseStatistics> ParquetScanStats(ClientContext &context, const FunctionData *bind_data_p,
118: 	                                                   column_t column_index) {
119: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
120: 
121: 		if (column_index == COLUMN_IDENTIFIER_ROW_ID) {
122: 			return nullptr;
123: 		}
124: 
125: 		// we do not want to parse the Parquet metadata for the sole purpose of getting column statistics
126: 
127: 		// We already parsed the metadata for the first file in a glob because we need some type info.
128: 		auto overall_stats = ParquetReader::ReadStatistics(
129: 		    *bind_data.initial_reader, bind_data.initial_reader->return_types[column_index], column_index,
130: 		    bind_data.initial_reader->metadata->metadata.get());
131: 
132: 		if (!overall_stats) {
133: 			return nullptr;
134: 		}
135: 
136: 		// if there is only one file in the glob (quite common case), we are done
137: 		auto &config = DBConfig::GetConfig(context);
138: 		if (bind_data.files.size() < 2) {
139: 			return overall_stats;
140: 		} else if (config.object_cache_enable) {
141: 			auto &cache = ObjectCache::GetObjectCache(context);
142: 			// for more than one file, we could be lucky and metadata for *every* file is in the object cache (if
143: 			// enabled at all)
144: 			FileSystem &fs = FileSystem::GetFileSystem(context);
145: 			for (idx_t file_idx = 1; file_idx < bind_data.files.size(); file_idx++) {
146: 				auto &file_name = bind_data.files[file_idx];
147: 				auto metadata = cache.Get<ParquetFileMetadataCache>(file_name);
148: 				if (!metadata) {
149: 					// missing metadata entry in cache, no usable stats
150: 					return nullptr;
151: 				}
152: 				auto handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,
153: 				                          FileSystem::DEFAULT_COMPRESSION, FileSystem::GetFileOpener(context));
154: 				// we need to check if the metadata cache entries are current
155: 				if (fs.GetLastModifiedTime(*handle) >= metadata->read_time) {
156: 					// missing or invalid metadata entry in cache, no usable stats overall
157: 					return nullptr;
158: 				}
159: 				// get and merge stats for file
160: 				auto file_stats = ParquetReader::ReadStatistics(*bind_data.initial_reader,
161: 				                                                bind_data.initial_reader->return_types[column_index],
162: 				                                                column_index, metadata->metadata.get());
163: 				if (!file_stats) {
164: 					return nullptr;
165: 				}
166: 				overall_stats->Merge(*file_stats);
167: 			}
168: 			// success!
169: 			return overall_stats;
170: 		}
171: 		// we have more than one file and no object cache so no statistics overall
172: 		return nullptr;
173: 	}
174: 
175: 	static void ParquetScanFuncParallel(ClientContext &context, const FunctionData *bind_data,
176: 	                                    FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output,
177: 	                                    ParallelState *parallel_state_p) {
178: 		//! FIXME: Have specialized parallel function from pandas scan here
179: 		ParquetScanImplementation(context, bind_data, operator_state, input, output);
180: 	}
181: 
182: 	static unique_ptr<FunctionData> ParquetScanBindInternal(ClientContext &context, vector<string> files,
183: 	                                                        vector<LogicalType> &return_types, vector<string> &names,
184: 	                                                        ParquetOptions parquet_options) {
185: 		auto result = make_unique<ParquetReadBindData>();
186: 		result->files = move(files);
187: 
188: 		result->initial_reader = make_shared<ParquetReader>(context, result->files[0], parquet_options);
189: 		return_types = result->types = result->initial_reader->return_types;
190: 		names = result->names = result->initial_reader->names;
191: 		return move(result);
192: 	}
193: 
194: 	static vector<string> ParquetGlob(FileSystem &fs, const string &glob, ClientContext &context) {
195: 		auto files = fs.Glob(glob, FileSystem::GetFileOpener(context));
196: 		if (files.empty()) {
197: 			throw IOException("No files found that match the pattern \"%s\"", glob);
198: 		}
199: 		return files;
200: 	}
201: 
202: 	static unique_ptr<FunctionData> ParquetScanBind(ClientContext &context, vector<Value> &inputs,
203: 	                                                named_parameter_map_t &named_parameters,
204: 	                                                vector<LogicalType> &input_table_types,
205: 	                                                vector<string> &input_table_names,
206: 	                                                vector<LogicalType> &return_types, vector<string> &names) {
207: 		auto &config = DBConfig::GetConfig(context);
208: 		if (!config.enable_external_access) {
209: 			throw PermissionException("Scanning Parquet files is disabled through configuration");
210: 		}
211: 		auto file_name = inputs[0].GetValue<string>();
212: 		ParquetOptions parquet_options(context);
213: 		for (auto &kv : named_parameters) {
214: 			if (kv.first == "binary_as_string") {
215: 				parquet_options.binary_as_string = BooleanValue::Get(kv.second);
216: 			}
217: 		}
218: 		FileSystem &fs = FileSystem::GetFileSystem(context);
219: 		auto files = ParquetGlob(fs, file_name, context);
220: 		return ParquetScanBindInternal(context, move(files), return_types, names, parquet_options);
221: 	}
222: 
223: 	static unique_ptr<FunctionData> ParquetScanBindList(ClientContext &context, vector<Value> &inputs,
224: 	                                                    named_parameter_map_t &named_parameters,
225: 	                                                    vector<LogicalType> &input_table_types,
226: 	                                                    vector<string> &input_table_names,
227: 	                                                    vector<LogicalType> &return_types, vector<string> &names) {
228: 		auto &config = DBConfig::GetConfig(context);
229: 		if (!config.enable_external_access) {
230: 			throw PermissionException("Scanning Parquet files is disabled through configuration");
231: 		}
232: 		FileSystem &fs = FileSystem::GetFileSystem(context);
233: 		vector<string> files;
234: 		for (auto &val : ListValue::GetChildren(inputs[0])) {
235: 			auto glob_files = ParquetGlob(fs, val.ToString(), context);
236: 			files.insert(files.end(), glob_files.begin(), glob_files.end());
237: 		}
238: 		if (files.empty()) {
239: 			throw IOException("Parquet reader needs at least one file to read");
240: 		}
241: 		ParquetOptions parquet_options(context);
242: 		for (auto &kv : named_parameters) {
243: 			if (kv.first == "binary_as_string") {
244: 				parquet_options.binary_as_string = BooleanValue::Get(kv.second);
245: 			}
246: 		}
247: 		return ParquetScanBindInternal(context, move(files), return_types, names, parquet_options);
248: 	}
249: 
250: 	static unique_ptr<FunctionOperatorData> ParquetScanInit(ClientContext &context, const FunctionData *bind_data_p,
251: 	                                                        const vector<column_t> &column_ids,
252: 	                                                        TableFilterCollection *filters) {
253: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
254: 		bind_data.chunk_count = 0;
255: 		bind_data.cur_file = 0;
256: 		auto result = make_unique<ParquetReadOperatorData>();
257: 		result->column_ids = column_ids;
258: 
259: 		result->is_parallel = false;
260: 		result->file_index = 0;
261: 		result->table_filters = filters->table_filters;
262: 		// single-threaded: one thread has to read all groups
263: 		vector<idx_t> group_ids;
264: 		for (idx_t i = 0; i < bind_data.initial_reader->NumRowGroups(); i++) {
265: 			group_ids.push_back(i);
266: 		}
267: 		result->reader = bind_data.initial_reader;
268: 		result->reader->InitializeScan(result->scan_state, column_ids, move(group_ids), filters->table_filters);
269: 		return move(result);
270: 	}
271: 
272: 	static double ParquetProgress(ClientContext &context, const FunctionData *bind_data_p) {
273: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
274: 		if (bind_data.initial_reader->NumRows() == 0) {
275: 			return (100.0 * (bind_data.cur_file + 1)) / bind_data.files.size();
276: 		}
277: 		auto percentage = (bind_data.chunk_count * STANDARD_VECTOR_SIZE * 100.0 / bind_data.initial_reader->NumRows()) /
278: 		                  bind_data.files.size();
279: 		percentage += 100.0 * bind_data.cur_file / bind_data.files.size();
280: 		return percentage;
281: 	}
282: 
283: 	static unique_ptr<FunctionOperatorData>
284: 	ParquetScanParallelInit(ClientContext &context, const FunctionData *bind_data_p, ParallelState *parallel_state_p,
285: 	                        const vector<column_t> &column_ids, TableFilterCollection *filters) {
286: 		auto result = make_unique<ParquetReadOperatorData>();
287: 		result->column_ids = column_ids;
288: 		result->is_parallel = true;
289: 		result->table_filters = filters->table_filters;
290: 		if (!ParquetParallelStateNext(context, bind_data_p, result.get(), parallel_state_p)) {
291: 			return nullptr;
292: 		}
293: 		return move(result);
294: 	}
295: 
296: 	static void ParquetScanImplementation(ClientContext &context, const FunctionData *bind_data_p,
297: 	                                      FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
298: 		if (!operator_state) {
299: 			return;
300: 		}
301: 		auto &data = (ParquetReadOperatorData &)*operator_state;
302: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
303: 
304: 		do {
305: 			data.reader->Scan(data.scan_state, output);
306: 			bind_data.chunk_count++;
307: 			if (output.size() == 0 && !data.is_parallel) {
308: 				auto &bind_data = (ParquetReadBindData &)*bind_data_p;
309: 				// check if there is another file
310: 				if (data.file_index + 1 < bind_data.files.size()) {
311: 					data.file_index++;
312: 					bind_data.cur_file++;
313: 					bind_data.chunk_count = 0;
314: 					string file = bind_data.files[data.file_index];
315: 					// move to the next file
316: 					data.reader =
317: 					    make_shared<ParquetReader>(context, file, bind_data.names, bind_data.types, data.column_ids,
318: 					                               data.reader->parquet_options, bind_data.files[0]);
319: 					vector<idx_t> group_ids;
320: 					for (idx_t i = 0; i < data.reader->NumRowGroups(); i++) {
321: 						group_ids.push_back(i);
322: 					}
323: 					data.reader->InitializeScan(data.scan_state, data.column_ids, move(group_ids), data.table_filters);
324: 				} else {
325: 					// exhausted all the files: done
326: 					break;
327: 				}
328: 			} else {
329: 				break;
330: 			}
331: 		} while (true);
332: 	}
333: 
334: 	static unique_ptr<NodeStatistics> ParquetCardinality(ClientContext &context, const FunctionData *bind_data) {
335: 		auto &data = (ParquetReadBindData &)*bind_data;
336: 		return make_unique<NodeStatistics>(data.initial_reader->NumRows() * data.files.size());
337: 	}
338: 
339: 	static idx_t ParquetScanMaxThreads(ClientContext &context, const FunctionData *bind_data) {
340: 		auto &data = (ParquetReadBindData &)*bind_data;
341: 		return data.initial_reader->NumRowGroups() * data.files.size();
342: 	}
343: 
344: 	static unique_ptr<ParallelState> ParquetInitParallelState(ClientContext &context, const FunctionData *bind_data_p,
345: 	                                                          const vector<column_t> &column_ids,
346: 	                                                          TableFilterCollection *filters) {
347: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
348: 		auto result = make_unique<ParquetReadParallelState>();
349: 		result->current_reader = bind_data.initial_reader;
350: 		result->row_group_index = 0;
351: 		result->file_index = 0;
352: 		return move(result);
353: 	}
354: 
355: 	static bool ParquetParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,
356: 	                                     FunctionOperatorData *state_p, ParallelState *parallel_state_p) {
357: 		if (!state_p) {
358: 			return false;
359: 		}
360: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
361: 		auto &parallel_state = (ParquetReadParallelState &)*parallel_state_p;
362: 		auto &scan_data = (ParquetReadOperatorData &)*state_p;
363: 
364: 		lock_guard<mutex> parallel_lock(parallel_state.lock);
365: 		if (parallel_state.row_group_index < parallel_state.current_reader->NumRowGroups()) {
366: 			// groups remain in the current parquet file: read the next group
367: 			scan_data.reader = parallel_state.current_reader;
368: 			vector<idx_t> group_indexes {parallel_state.row_group_index};
369: 			scan_data.reader->InitializeScan(scan_data.scan_state, scan_data.column_ids, group_indexes,
370: 			                                 scan_data.table_filters);
371: 			parallel_state.row_group_index++;
372: 			return true;
373: 		} else {
374: 			// no groups remain in the current parquet file: check if there are more files to read
375: 			while (parallel_state.file_index + 1 < bind_data.files.size()) {
376: 				// read the next file
377: 				string file = bind_data.files[++parallel_state.file_index];
378: 				parallel_state.current_reader =
379: 				    make_shared<ParquetReader>(context, file, bind_data.names, bind_data.types, scan_data.column_ids,
380: 				                               parallel_state.current_reader->parquet_options, bind_data.files[0]);
381: 				if (parallel_state.current_reader->NumRowGroups() == 0) {
382: 					// empty parquet file, move to next file
383: 					continue;
384: 				}
385: 				// set up the scan state to read the first group
386: 				scan_data.reader = parallel_state.current_reader;
387: 				vector<idx_t> group_indexes {0};
388: 				scan_data.reader->InitializeScan(scan_data.scan_state, scan_data.column_ids, group_indexes,
389: 				                                 scan_data.table_filters);
390: 				parallel_state.row_group_index = 1;
391: 				return true;
392: 			}
393: 		}
394: 		return false;
395: 	}
396: };
397: 
398: struct ParquetWriteBindData : public FunctionData {
399: 	vector<LogicalType> sql_types;
400: 	string file_name;
401: 	vector<string> column_names;
402: 	duckdb_parquet::format::CompressionCodec::type codec = duckdb_parquet::format::CompressionCodec::SNAPPY;
403: 	idx_t row_group_size = 100000;
404: };
405: 
406: struct ParquetWriteGlobalState : public GlobalFunctionData {
407: 	unique_ptr<ParquetWriter> writer;
408: };
409: 
410: struct ParquetWriteLocalState : public LocalFunctionData {
411: 	ParquetWriteLocalState() {
412: 		buffer = make_unique<ChunkCollection>();
413: 	}
414: 
415: 	unique_ptr<ChunkCollection> buffer;
416: };
417: 
418: unique_ptr<FunctionData> ParquetWriteBind(ClientContext &context, CopyInfo &info, vector<string> &names,
419:                                           vector<LogicalType> &sql_types) {
420: 	auto bind_data = make_unique<ParquetWriteBindData>();
421: 	for (auto &option : info.options) {
422: 		auto loption = StringUtil::Lower(option.first);
423: 		if (loption == "row_group_size" || loption == "chunk_size") {
424: 			bind_data->row_group_size = option.second[0].GetValue<uint64_t>();
425: 		} else if (loption == "compression" || loption == "codec") {
426: 			if (!option.second.empty()) {
427: 				auto roption = StringUtil::Lower(option.second[0].ToString());
428: 				if (roption == "uncompressed") {
429: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::UNCOMPRESSED;
430: 					continue;
431: 				} else if (roption == "snappy") {
432: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::SNAPPY;
433: 					continue;
434: 				} else if (roption == "gzip") {
435: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::GZIP;
436: 					continue;
437: 				} else if (roption == "zstd") {
438: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::ZSTD;
439: 					continue;
440: 				}
441: 			}
442: 			throw ParserException("Expected %s argument to be either [uncompressed, snappy, gzip or zstd]", loption);
443: 		} else {
444: 			throw NotImplementedException("Unrecognized option for PARQUET: %s", option.first.c_str());
445: 		}
446: 	}
447: 	bind_data->sql_types = sql_types;
448: 	bind_data->column_names = names;
449: 	bind_data->file_name = info.file_path;
450: 	return move(bind_data);
451: }
452: 
453: unique_ptr<GlobalFunctionData> ParquetWriteInitializeGlobal(ClientContext &context, FunctionData &bind_data,
454:                                                             const string &file_path) {
455: 	auto global_state = make_unique<ParquetWriteGlobalState>();
456: 	auto &parquet_bind = (ParquetWriteBindData &)bind_data;
457: 
458: 	auto &fs = FileSystem::GetFileSystem(context);
459: 	global_state->writer =
460: 	    make_unique<ParquetWriter>(fs, file_path, FileSystem::GetFileOpener(context), parquet_bind.sql_types,
461: 	                               parquet_bind.column_names, parquet_bind.codec);
462: 	return move(global_state);
463: }
464: 
465: void ParquetWriteSink(ClientContext &context, FunctionData &bind_data_p, GlobalFunctionData &gstate,
466:                       LocalFunctionData &lstate, DataChunk &input) {
467: 	auto &bind_data = (ParquetWriteBindData &)bind_data_p;
468: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
469: 	auto &local_state = (ParquetWriteLocalState &)lstate;
470: 
471: 	// append data to the local (buffered) chunk collection
472: 	local_state.buffer->Append(input);
473: 	if (local_state.buffer->Count() > bind_data.row_group_size) {
474: 		// if the chunk collection exceeds a certain size we flush it to the parquet file
475: 		global_state.writer->Flush(*local_state.buffer);
476: 		// and reset the buffer
477: 		local_state.buffer = make_unique<ChunkCollection>();
478: 	}
479: }
480: 
481: void ParquetWriteCombine(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
482:                          LocalFunctionData &lstate) {
483: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
484: 	auto &local_state = (ParquetWriteLocalState &)lstate;
485: 	// flush any data left in the local state to the file
486: 	global_state.writer->Flush(*local_state.buffer);
487: }
488: 
489: void ParquetWriteFinalize(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate) {
490: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
491: 	// finalize: write any additional metadata to the file here
492: 	global_state.writer->Finalize();
493: }
494: 
495: unique_ptr<LocalFunctionData> ParquetWriteInitializeLocal(ClientContext &context, FunctionData &bind_data) {
496: 	return make_unique<ParquetWriteLocalState>();
497: }
498: 
499: unique_ptr<TableFunctionRef> ParquetScanReplacement(const string &table_name, void *data) {
500: 	if (!StringUtil::EndsWith(StringUtil::Lower(table_name), ".parquet")) {
501: 		return nullptr;
502: 	}
503: 	auto table_function = make_unique<TableFunctionRef>();
504: 	vector<unique_ptr<ParsedExpression>> children;
505: 	children.push_back(make_unique<ConstantExpression>(Value(table_name)));
506: 	table_function->function = make_unique<FunctionExpression>("parquet_scan", move(children));
507: 	return table_function;
508: }
509: 
510: void ParquetExtension::Load(DuckDB &db) {
511: 	auto &fs = db.GetFileSystem();
512: 	fs.RegisterSubSystem(FileCompressionType::ZSTD, make_unique<ZStdFileSystem>());
513: 
514: 	auto scan_fun = ParquetScanFunction::GetFunctionSet();
515: 	CreateTableFunctionInfo cinfo(scan_fun);
516: 	cinfo.name = "read_parquet";
517: 	CreateTableFunctionInfo pq_scan = cinfo;
518: 	pq_scan.name = "parquet_scan";
519: 
520: 	ParquetMetaDataFunction meta_fun;
521: 	CreateTableFunctionInfo meta_cinfo(meta_fun);
522: 
523: 	ParquetSchemaFunction schema_fun;
524: 	CreateTableFunctionInfo schema_cinfo(schema_fun);
525: 
526: 	CopyFunction function("parquet");
527: 	function.copy_to_bind = ParquetWriteBind;
528: 	function.copy_to_initialize_global = ParquetWriteInitializeGlobal;
529: 	function.copy_to_initialize_local = ParquetWriteInitializeLocal;
530: 	function.copy_to_sink = ParquetWriteSink;
531: 	function.copy_to_combine = ParquetWriteCombine;
532: 	function.copy_to_finalize = ParquetWriteFinalize;
533: 	function.copy_from_bind = ParquetScanFunction::ParquetReadBind;
534: 	function.copy_from_function = scan_fun.functions[0];
535: 
536: 	function.extension = "parquet";
537: 	CreateCopyFunctionInfo info(function);
538: 
539: 	Connection con(db);
540: 	con.BeginTransaction();
541: 	auto &context = *con.context;
542: 	auto &catalog = Catalog::GetCatalog(context);
543: 	catalog.CreateCopyFunction(context, &info);
544: 	catalog.CreateTableFunction(context, &cinfo);
545: 	catalog.CreateTableFunction(context, &pq_scan);
546: 	catalog.CreateTableFunction(context, &meta_cinfo);
547: 	catalog.CreateTableFunction(context, &schema_cinfo);
548: 	con.Commit();
549: 
550: 	auto &config = DBConfig::GetConfig(*db.instance);
551: 	config.replacement_scans.emplace_back(ParquetScanReplacement);
552: 	config.AddExtensionOption("binary_as_string", "In Parquet files, interpret binary data as a string.",
553: 	                          LogicalType::BOOLEAN);
554: }
555: 
556: std::string ParquetExtension::Name() {
557: 	return "parquet";
558: }
559: 
560: } // namespace duckdb
561: 
562: #ifndef DUCKDB_EXTENSION_MAIN
563: #error DUCKDB_EXTENSION_MAIN not defined
564: #endif
[end of extension/parquet/parquet-extension.cpp]
[start of extension/parquet/parquet_metadata.cpp]
1: #include "parquet_metadata.hpp"
2: #include "parquet_statistics.hpp"
3: 
4: #include <sstream>
5: 
6: #ifndef DUCKDB_AMALGAMATION
7: #include "duckdb/common/types/blob.hpp"
8: #include "duckdb/main/config.hpp"
9: #endif
10: 
11: namespace duckdb {
12: 
13: struct ParquetMetaDataBindData : public FunctionData {
14: 	vector<LogicalType> return_types;
15: 	vector<string> files;
16: };
17: 
18: struct ParquetMetaDataOperatorData : public FunctionOperatorData {
19: 	idx_t file_index;
20: 	ChunkCollection collection;
21: 
22: 	static void BindMetaData(vector<LogicalType> &return_types, vector<string> &names);
23: 	static void BindSchema(vector<LogicalType> &return_types, vector<string> &names);
24: 
25: 	void LoadFileMetaData(ClientContext &context, const vector<LogicalType> &return_types, const string &file_path);
26: 	void LoadSchemaData(ClientContext &context, const vector<LogicalType> &return_types, const string &file_path);
27: };
28: 
29: template <class T>
30: string ConvertParquetElementToString(T &&entry) {
31: 	std::stringstream ss;
32: 	ss << entry;
33: 	return ss.str();
34: }
35: 
36: template <class T>
37: string PrintParquetElementToString(T &&entry) {
38: 	std::stringstream ss;
39: 	entry.printTo(ss);
40: 	return ss.str();
41: }
42: 
43: void ParquetMetaDataOperatorData::BindMetaData(vector<LogicalType> &return_types, vector<string> &names) {
44: 	names.emplace_back("file_name");
45: 	return_types.emplace_back(LogicalType::VARCHAR);
46: 
47: 	names.emplace_back("row_group_id");
48: 	return_types.emplace_back(LogicalType::BIGINT);
49: 
50: 	names.emplace_back("row_group_num_rows");
51: 	return_types.emplace_back(LogicalType::BIGINT);
52: 
53: 	names.emplace_back("row_group_num_columns");
54: 	return_types.emplace_back(LogicalType::BIGINT);
55: 
56: 	names.emplace_back("row_group_bytes");
57: 	return_types.emplace_back(LogicalType::BIGINT);
58: 
59: 	names.emplace_back("column_id");
60: 	return_types.emplace_back(LogicalType::BIGINT);
61: 
62: 	names.emplace_back("file_offset");
63: 	return_types.emplace_back(LogicalType::BIGINT);
64: 
65: 	names.emplace_back("num_values");
66: 	return_types.emplace_back(LogicalType::BIGINT);
67: 
68: 	names.emplace_back("path_in_schema");
69: 	return_types.emplace_back(LogicalType::VARCHAR);
70: 
71: 	names.emplace_back("type");
72: 	return_types.emplace_back(LogicalType::VARCHAR);
73: 
74: 	names.emplace_back("stats_min");
75: 	return_types.emplace_back(LogicalType::VARCHAR);
76: 
77: 	names.emplace_back("stats_max");
78: 	return_types.emplace_back(LogicalType::VARCHAR);
79: 
80: 	names.emplace_back("stats_null_count");
81: 	return_types.emplace_back(LogicalType::BIGINT);
82: 
83: 	names.emplace_back("stats_distinct_count");
84: 	return_types.emplace_back(LogicalType::BIGINT);
85: 
86: 	names.emplace_back("stats_min_value");
87: 	return_types.emplace_back(LogicalType::VARCHAR);
88: 
89: 	names.emplace_back("stats_max_value");
90: 	return_types.emplace_back(LogicalType::VARCHAR);
91: 
92: 	names.emplace_back("compression");
93: 	return_types.emplace_back(LogicalType::VARCHAR);
94: 
95: 	names.emplace_back("encodings");
96: 	return_types.emplace_back(LogicalType::VARCHAR);
97: 
98: 	names.emplace_back("index_page_offset");
99: 	return_types.emplace_back(LogicalType::BIGINT);
100: 
101: 	names.emplace_back("dictionary_page_offset");
102: 	return_types.emplace_back(LogicalType::BIGINT);
103: 
104: 	names.emplace_back("data_page_offset");
105: 	return_types.emplace_back(LogicalType::BIGINT);
106: 
107: 	names.emplace_back("total_compressed_size");
108: 	return_types.emplace_back(LogicalType::BIGINT);
109: 
110: 	names.emplace_back("total_uncompressed_size");
111: 	return_types.emplace_back(LogicalType::BIGINT);
112: }
113: 
114: Value ConvertParquetStats(const LogicalType &type, const duckdb_parquet::format::SchemaElement &schema_ele,
115:                           bool stats_is_set, const std::string &stats) {
116: 	if (!stats_is_set) {
117: 		return Value(LogicalType::VARCHAR);
118: 	}
119: 	return ParquetStatisticsUtils::ConvertValue(type, schema_ele, stats).CastAs(LogicalType::VARCHAR);
120: }
121: 
122: void ParquetMetaDataOperatorData::LoadFileMetaData(ClientContext &context, const vector<LogicalType> &return_types,
123:                                                    const string &file_path) {
124: 	collection.Reset();
125: 	ParquetOptions parquet_options(context);
126: 	auto reader = make_unique<ParquetReader>(context, file_path, parquet_options);
127: 	idx_t count = 0;
128: 	DataChunk current_chunk;
129: 	current_chunk.Initialize(return_types);
130: 	auto meta_data = reader->GetFileMetadata();
131: 	vector<LogicalType> column_types;
132: 	vector<idx_t> schema_indexes;
133: 	for (idx_t schema_idx = 0; schema_idx < meta_data->schema.size(); schema_idx++) {
134: 		auto &schema_element = meta_data->schema[schema_idx];
135: 		if (schema_element.num_children > 0) {
136: 			continue;
137: 		}
138: 		column_types.push_back(ParquetReader::DeriveLogicalType(schema_element, false));
139: 		schema_indexes.push_back(schema_idx);
140: 	}
141: 
142: 	for (idx_t row_group_idx = 0; row_group_idx < meta_data->row_groups.size(); row_group_idx++) {
143: 		auto &row_group = meta_data->row_groups[row_group_idx];
144: 
145: 		if (row_group.columns.size() > column_types.size()) {
146: 			throw InternalException("Too many column in row group: corrupt file?");
147: 		}
148: 		for (idx_t col_idx = 0; col_idx < row_group.columns.size(); col_idx++) {
149: 			auto &column = row_group.columns[col_idx];
150: 			auto &col_meta = column.meta_data;
151: 			auto &stats = col_meta.statistics;
152: 			auto &schema_element = meta_data->schema[schema_indexes[col_idx]];
153: 			auto &column_type = column_types[col_idx];
154: 
155: 			// file_name, LogicalType::VARCHAR
156: 			current_chunk.SetValue(0, count, file_path);
157: 
158: 			// row_group_id, LogicalType::BIGINT
159: 			current_chunk.SetValue(1, count, Value::BIGINT(row_group_idx));
160: 
161: 			// row_group_num_rows, LogicalType::BIGINT
162: 			current_chunk.SetValue(2, count, Value::BIGINT(row_group.num_rows));
163: 
164: 			// row_group_num_columns, LogicalType::BIGINT
165: 			current_chunk.SetValue(3, count, Value::BIGINT(row_group.columns.size()));
166: 
167: 			// row_group_bytes, LogicalType::BIGINT
168: 			current_chunk.SetValue(4, count, Value::BIGINT(row_group.total_byte_size));
169: 
170: 			// column_id, LogicalType::BIGINT
171: 			current_chunk.SetValue(5, count, Value::BIGINT(col_idx));
172: 
173: 			// file_offset, LogicalType::BIGINT
174: 			current_chunk.SetValue(6, count, Value::BIGINT(column.file_offset));
175: 
176: 			// num_values, LogicalType::BIGINT
177: 			current_chunk.SetValue(7, count, Value::BIGINT(col_meta.num_values));
178: 
179: 			// path_in_schema, LogicalType::VARCHAR
180: 			current_chunk.SetValue(8, count, StringUtil::Join(col_meta.path_in_schema, ", "));
181: 
182: 			// type, LogicalType::VARCHAR
183: 			current_chunk.SetValue(9, count, ConvertParquetElementToString(col_meta.type));
184: 
185: 			// stats_min, LogicalType::VARCHAR
186: 			current_chunk.SetValue(10, count,
187: 			                       ConvertParquetStats(column_type, schema_element, stats.__isset.min, stats.min));
188: 
189: 			// stats_max, LogicalType::VARCHAR
190: 			current_chunk.SetValue(11, count,
191: 			                       ConvertParquetStats(column_type, schema_element, stats.__isset.max, stats.max));
192: 
193: 			// stats_null_count, LogicalType::BIGINT
194: 			current_chunk.SetValue(
195: 			    12, count, stats.__isset.null_count ? Value::BIGINT(stats.null_count) : Value(LogicalType::BIGINT));
196: 
197: 			// stats_distinct_count, LogicalType::BIGINT
198: 			current_chunk.SetValue(13, count,
199: 			                       stats.__isset.distinct_count ? Value::BIGINT(stats.distinct_count)
200: 			                                                    : Value(LogicalType::BIGINT));
201: 
202: 			// stats_min_value, LogicalType::VARCHAR
203: 			current_chunk.SetValue(
204: 			    14, count, ConvertParquetStats(column_type, schema_element, stats.__isset.min_value, stats.min_value));
205: 
206: 			// stats_max_value, LogicalType::VARCHAR
207: 			current_chunk.SetValue(
208: 			    15, count, ConvertParquetStats(column_type, schema_element, stats.__isset.max_value, stats.max_value));
209: 
210: 			// compression, LogicalType::VARCHAR
211: 			current_chunk.SetValue(16, count, ConvertParquetElementToString(col_meta.codec));
212: 
213: 			// encodings, LogicalType::VARCHAR
214: 			vector<string> encoding_string;
215: 			for (auto &encoding : col_meta.encodings) {
216: 				encoding_string.push_back(ConvertParquetElementToString(encoding));
217: 			}
218: 			current_chunk.SetValue(17, count, Value(StringUtil::Join(encoding_string, ", ")));
219: 
220: 			// index_page_offset, LogicalType::BIGINT
221: 			current_chunk.SetValue(18, count, Value::BIGINT(col_meta.index_page_offset));
222: 
223: 			// dictionary_page_offset, LogicalType::BIGINT
224: 			current_chunk.SetValue(19, count, Value::BIGINT(col_meta.dictionary_page_offset));
225: 
226: 			// data_page_offset, LogicalType::BIGINT
227: 			current_chunk.SetValue(20, count, Value::BIGINT(col_meta.data_page_offset));
228: 
229: 			// total_compressed_size, LogicalType::BIGINT
230: 			current_chunk.SetValue(21, count, Value::BIGINT(col_meta.total_compressed_size));
231: 
232: 			// total_uncompressed_size, LogicalType::BIGINT
233: 			current_chunk.SetValue(22, count, Value::BIGINT(col_meta.total_uncompressed_size));
234: 
235: 			count++;
236: 			if (count >= STANDARD_VECTOR_SIZE) {
237: 				current_chunk.SetCardinality(count);
238: 				collection.Append(current_chunk);
239: 
240: 				count = 0;
241: 				current_chunk.Reset();
242: 			}
243: 		}
244: 	}
245: 	current_chunk.SetCardinality(count);
246: 	collection.Append(current_chunk);
247: }
248: 
249: void ParquetMetaDataOperatorData::BindSchema(vector<LogicalType> &return_types, vector<string> &names) {
250: 	names.emplace_back("file_name");
251: 	return_types.emplace_back(LogicalType::VARCHAR);
252: 
253: 	names.emplace_back("name");
254: 	return_types.emplace_back(LogicalType::VARCHAR);
255: 
256: 	names.emplace_back("type");
257: 	return_types.emplace_back(LogicalType::VARCHAR);
258: 
259: 	names.emplace_back("type_length");
260: 	return_types.emplace_back(LogicalType::VARCHAR);
261: 
262: 	names.emplace_back("repetition_type");
263: 	return_types.emplace_back(LogicalType::VARCHAR);
264: 
265: 	names.emplace_back("num_children");
266: 	return_types.emplace_back(LogicalType::BIGINT);
267: 
268: 	names.emplace_back("converted_type");
269: 	return_types.emplace_back(LogicalType::VARCHAR);
270: 
271: 	names.emplace_back("scale");
272: 	return_types.emplace_back(LogicalType::BIGINT);
273: 
274: 	names.emplace_back("precision");
275: 	return_types.emplace_back(LogicalType::BIGINT);
276: 
277: 	names.emplace_back("field_id");
278: 	return_types.emplace_back(LogicalType::BIGINT);
279: 
280: 	names.emplace_back("logical_type");
281: 	return_types.emplace_back(LogicalType::VARCHAR);
282: }
283: 
284: Value ParquetLogicalTypeToString(const duckdb_parquet::format::LogicalType &type) {
285: 
286: 	if (type.__isset.STRING) {
287: 		return Value(PrintParquetElementToString(type.STRING));
288: 	}
289: 	if (type.__isset.MAP) {
290: 		return Value(PrintParquetElementToString(type.MAP));
291: 	}
292: 	if (type.__isset.LIST) {
293: 		return Value(PrintParquetElementToString(type.LIST));
294: 	}
295: 	if (type.__isset.ENUM) {
296: 		return Value(PrintParquetElementToString(type.ENUM));
297: 	}
298: 	if (type.__isset.DECIMAL) {
299: 		return Value(PrintParquetElementToString(type.DECIMAL));
300: 	}
301: 	if (type.__isset.DATE) {
302: 		return Value(PrintParquetElementToString(type.DATE));
303: 	}
304: 	if (type.__isset.TIME) {
305: 		return Value(PrintParquetElementToString(type.TIME));
306: 	}
307: 	if (type.__isset.TIMESTAMP) {
308: 		return Value(PrintParquetElementToString(type.TIMESTAMP));
309: 	}
310: 	if (type.__isset.INTEGER) {
311: 		return Value(PrintParquetElementToString(type.INTEGER));
312: 	}
313: 	if (type.__isset.UNKNOWN) {
314: 		return Value(PrintParquetElementToString(type.UNKNOWN));
315: 	}
316: 	if (type.__isset.JSON) {
317: 		return Value(PrintParquetElementToString(type.JSON));
318: 	}
319: 	if (type.__isset.BSON) {
320: 		return Value(PrintParquetElementToString(type.BSON));
321: 	}
322: 	if (type.__isset.UUID) {
323: 		return Value(PrintParquetElementToString(type.UUID));
324: 	}
325: 	return Value();
326: }
327: 
328: void ParquetMetaDataOperatorData::LoadSchemaData(ClientContext &context, const vector<LogicalType> &return_types,
329:                                                  const string &file_path) {
330: 	collection.Reset();
331: 	ParquetOptions parquet_options(context);
332: 	auto reader = make_unique<ParquetReader>(context, file_path, parquet_options);
333: 	idx_t count = 0;
334: 	DataChunk current_chunk;
335: 	current_chunk.Initialize(return_types);
336: 	auto meta_data = reader->GetFileMetadata();
337: 	for (idx_t col_idx = 0; col_idx < meta_data->schema.size(); col_idx++) {
338: 		auto &column = meta_data->schema[col_idx];
339: 
340: 		// file_name, LogicalType::VARCHAR
341: 		current_chunk.SetValue(0, count, file_path);
342: 
343: 		// name, LogicalType::VARCHAR
344: 		current_chunk.SetValue(1, count, column.name);
345: 
346: 		// type, LogicalType::VARCHAR
347: 		current_chunk.SetValue(2, count, ConvertParquetElementToString(column.type));
348: 
349: 		// type_length, LogicalType::VARCHAR
350: 		current_chunk.SetValue(3, count, Value::INTEGER(column.type_length));
351: 
352: 		// repetition_type, LogicalType::VARCHAR
353: 		current_chunk.SetValue(4, count, ConvertParquetElementToString(column.repetition_type));
354: 
355: 		// num_children, LogicalType::BIGINT
356: 		current_chunk.SetValue(5, count, Value::BIGINT(column.num_children));
357: 
358: 		// converted_type, LogicalType::VARCHAR
359: 		current_chunk.SetValue(6, count, ConvertParquetElementToString(column.converted_type));
360: 
361: 		// scale, LogicalType::BIGINT
362: 		current_chunk.SetValue(7, count, Value::BIGINT(column.scale));
363: 
364: 		// precision, LogicalType::BIGINT
365: 		current_chunk.SetValue(8, count, Value::BIGINT(column.precision));
366: 
367: 		// field_id, LogicalType::BIGINT
368: 		current_chunk.SetValue(9, count, Value::BIGINT(column.field_id));
369: 
370: 		// logical_type, LogicalType::VARCHAR
371: 		current_chunk.SetValue(10, count, ParquetLogicalTypeToString(column.logicalType));
372: 
373: 		count++;
374: 		if (count >= STANDARD_VECTOR_SIZE) {
375: 			current_chunk.SetCardinality(count);
376: 			collection.Append(current_chunk);
377: 
378: 			count = 0;
379: 			current_chunk.Reset();
380: 		}
381: 	}
382: 	current_chunk.SetCardinality(count);
383: 	collection.Append(current_chunk);
384: }
385: 
386: template <bool SCHEMA>
387: unique_ptr<FunctionData> ParquetMetaDataBind(ClientContext &context, vector<Value> &inputs,
388:                                              named_parameter_map_t &named_parameters,
389:                                              vector<LogicalType> &input_table_types, vector<string> &input_table_names,
390:                                              vector<LogicalType> &return_types, vector<string> &names) {
391: 	auto &config = DBConfig::GetConfig(context);
392: 	if (!config.enable_external_access) {
393: 		throw PermissionException("Scanning Parquet files is disabled through configuration");
394: 	}
395: 	if (SCHEMA) {
396: 		ParquetMetaDataOperatorData::BindSchema(return_types, names);
397: 	} else {
398: 		ParquetMetaDataOperatorData::BindMetaData(return_types, names);
399: 	}
400: 
401: 	auto file_name = inputs[0].GetValue<string>();
402: 	auto result = make_unique<ParquetMetaDataBindData>();
403: 
404: 	FileSystem &fs = FileSystem::GetFileSystem(context);
405: 	result->return_types = return_types;
406: 	result->files = fs.Glob(file_name, context);
407: 	if (result->files.empty()) {
408: 		throw IOException("No files found that match the pattern \"%s\"", file_name);
409: 	}
410: 	return move(result);
411: }
412: 
413: template <bool SCHEMA>
414: unique_ptr<FunctionOperatorData> ParquetMetaDataInit(ClientContext &context, const FunctionData *bind_data_p,
415:                                                      const vector<column_t> &column_ids,
416:                                                      TableFilterCollection *filters) {
417: 	auto &bind_data = (ParquetMetaDataBindData &)*bind_data_p;
418: 	D_ASSERT(!bind_data.files.empty());
419: 
420: 	auto result = make_unique<ParquetMetaDataOperatorData>();
421: 	if (SCHEMA) {
422: 		result->LoadSchemaData(context, bind_data.return_types, bind_data.files[0]);
423: 	} else {
424: 		result->LoadFileMetaData(context, bind_data.return_types, bind_data.files[0]);
425: 	}
426: 	result->file_index = 0;
427: 	return move(result);
428: }
429: 
430: template <bool SCHEMA>
431: void ParquetMetaDataImplementation(ClientContext &context, const FunctionData *bind_data_p,
432:                                    FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
433: 	auto &data = (ParquetMetaDataOperatorData &)*operator_state;
434: 	auto &bind_data = (ParquetMetaDataBindData &)*bind_data_p;
435: 	while (true) {
436: 		auto chunk = data.collection.Fetch();
437: 		if (!chunk) {
438: 			if (data.file_index + 1 < bind_data.files.size()) {
439: 				// load the metadata for the next file
440: 				data.file_index++;
441: 				if (SCHEMA) {
442: 					data.LoadSchemaData(context, bind_data.return_types, bind_data.files[data.file_index]);
443: 				} else {
444: 					data.LoadFileMetaData(context, bind_data.return_types, bind_data.files[data.file_index]);
445: 				}
446: 				continue;
447: 			} else {
448: 				// no files remaining: done
449: 				return;
450: 			}
451: 		}
452: 		output.Move(*chunk);
453: 		if (output.size() != 0) {
454: 			return;
455: 		}
456: 	}
457: }
458: 
459: ParquetMetaDataFunction::ParquetMetaDataFunction()
460:     : TableFunction("parquet_metadata", {LogicalType::VARCHAR}, ParquetMetaDataImplementation<false>,
461:                     ParquetMetaDataBind<false>, ParquetMetaDataInit<false>, /* statistics */ nullptr,
462:                     /* cleanup */ nullptr,
463:                     /* dependency */ nullptr, nullptr,
464:                     /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr, nullptr, nullptr, nullptr, nullptr,
465:                     nullptr, false, false, nullptr) {
466: }
467: 
468: ParquetSchemaFunction::ParquetSchemaFunction()
469:     : TableFunction("parquet_schema", {LogicalType::VARCHAR}, ParquetMetaDataImplementation<true>,
470:                     ParquetMetaDataBind<true>, ParquetMetaDataInit<true>, /* statistics */ nullptr,
471:                     /* cleanup */ nullptr,
472:                     /* dependency */ nullptr, nullptr,
473:                     /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr, nullptr, nullptr, nullptr, nullptr,
474:                     nullptr, false, false, nullptr) {
475: }
476: 
477: } // namespace duckdb
[end of extension/parquet/parquet_metadata.cpp]
[start of extension/substrait/substrait-extension.cpp]
1: #define DUCKDB_EXTENSION_MAIN
2: 
3: #include "substrait-extension.hpp"
4: #include "to_substrait.hpp"
5: #include "from_substrait.hpp"
6: 
7: #ifndef DUCKDB_AMALGAMATION
8: #include "duckdb/function/table_function.hpp"
9: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
10: #include "duckdb/parser/parsed_data/create_pragma_function_info.hpp"
11: #include "duckdb/main/client_context.hpp"
12: #include "duckdb/main/connection.hpp"
13: #endif
14: 
15: namespace duckdb {
16: 
17: struct ToSubstraitFunctionData : public TableFunctionData {
18: 	ToSubstraitFunctionData() {
19: 	}
20: 	string query;
21: 	bool finished = false;
22: };
23: 
24: static unique_ptr<FunctionData> ToSubstraitBind(ClientContext &context, vector<Value> &inputs,
25:                                                 named_parameter_map_t &named_parameters,
26:                                                 vector<LogicalType> &input_table_types,
27:                                                 vector<string> &input_table_names, vector<LogicalType> &return_types,
28:                                                 vector<string> &names) {
29: 	auto result = make_unique<ToSubstraitFunctionData>();
30: 	result->query = inputs[0].ToString();
31: 	return_types.emplace_back(LogicalType::BLOB);
32: 	names.emplace_back("Plan Blob");
33: 	return move(result);
34: }
35: 
36: shared_ptr<Relation> SubstraitPlanToDuckDBRel(Connection &conn, string &serialized) {
37: 	SubstraitToDuckDB transformer_s2d(conn, serialized);
38: 	return transformer_s2d.TransformPlan();
39: }
40: 
41: static void ToSubFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
42:                           DataChunk *input, DataChunk &output) {
43: 	auto &data = (ToSubstraitFunctionData &)*bind_data;
44: 	if (data.finished) {
45: 		return;
46: 	}
47: 	output.SetCardinality(1);
48: 	auto new_conn = Connection(*context.db);
49: 	auto query_plan = new_conn.context->ExtractPlan(data.query);
50: 	DuckDBToSubstrait transformer_d2s(*query_plan);
51: 	auto serialized = transformer_d2s.SerializeToString();
52: 
53: 	output.SetValue(0, 0, Value::BLOB_RAW(serialized));
54: 	data.finished = true;
55: 	if (context.config.query_verification_enabled) {
56: 		// We round-trip the generated blob and verify if the result is the same
57: 		auto actual_result = new_conn.Query(data.query);
58: 		auto sub_relation = SubstraitPlanToDuckDBRel(new_conn, serialized);
59: 		auto substrait_result = sub_relation->Execute();
60: 		if (!actual_result->Equals(*substrait_result)) {
61: 			query_plan->Print();
62: 			sub_relation->Print();
63: 			throw InternalException("The query result of DuckDB's query plan does not match Substrait");
64: 		}
65: 	}
66: }
67: 
68: struct FromSubstraitFunctionData : public TableFunctionData {
69: 	FromSubstraitFunctionData() {
70: 	}
71: 	shared_ptr<Relation> plan;
72: 	unique_ptr<QueryResult> res;
73: 	unique_ptr<Connection> conn;
74: };
75: 
76: static unique_ptr<FunctionData> FromSubstraitBind(ClientContext &context, vector<Value> &inputs,
77:                                                   named_parameter_map_t &named_parameters,
78:                                                   vector<LogicalType> &input_table_types,
79:                                                   vector<string> &input_table_names, vector<LogicalType> &return_types,
80:                                                   vector<string> &names) {
81: 	auto result = make_unique<FromSubstraitFunctionData>();
82: 	result->conn = make_unique<Connection>(*context.db);
83: 	string serialized = inputs[0].GetValueUnsafe<string>();
84: 	result->plan = SubstraitPlanToDuckDBRel(*result->conn, serialized);
85: 	for (auto &column : result->plan->Columns()) {
86: 		return_types.emplace_back(column.type);
87: 		names.emplace_back(column.name);
88: 	}
89: 	return move(result);
90: }
91: 
92: static void FromSubFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
93:                             DataChunk *input, DataChunk &output) {
94: 	auto &data = (FromSubstraitFunctionData &)*bind_data;
95: 	if (!data.res) {
96: 		data.res = data.plan->Execute();
97: 	}
98: 	auto result_chunk = data.res->Fetch();
99: 	if (!result_chunk) {
100: 		return;
101: 	}
102: 	// Move should work here, no?
103: 	result_chunk->Copy(output);
104: }
105: 
106: void SubstraitExtension::Load(DuckDB &db) {
107: 	Connection con(db);
108: 	con.BeginTransaction();
109: 	auto &catalog = Catalog::GetCatalog(*con.context);
110: 
111: 	// create the get_substrait table function that allows us to get a substrait binary from a valid SQL Query
112: 	TableFunction to_sub_func("get_substrait", {LogicalType::VARCHAR}, ToSubFunction, ToSubstraitBind);
113: 	CreateTableFunctionInfo to_sub_info(to_sub_func);
114: 	catalog.CreateTableFunction(*con.context, &to_sub_info);
115: 
116: 	// create the from_substrait table function that allows us to get a query result from a substrait plan
117: 	TableFunction from_sub_func("from_substrait", {LogicalType::BLOB}, FromSubFunction, FromSubstraitBind);
118: 	CreateTableFunctionInfo from_sub_info(from_sub_func);
119: 	catalog.CreateTableFunction(*con.context, &from_sub_info);
120: 
121: 	con.Commit();
122: }
123: 
124: std::string SubstraitExtension::Name() {
125: 	return "substrait";
126: }
127: 
128: } // namespace duckdb
129: 
130: extern "C" {
131: 
132: DUCKDB_EXTENSION_API void substrait_init(duckdb::DatabaseInstance &db) {
133: 	duckdb::DuckDB db_wrapper(db);
134: 	db_wrapper.LoadExtension<duckdb::SubstraitExtension>();
135: }
136: 
137: DUCKDB_EXTENSION_API const char *substrait_version() {
138: 	return duckdb::DuckDB::LibraryVersion();
139: }
140: }
[end of extension/substrait/substrait-extension.cpp]
[start of extension/tpcds/tpcds-extension.cpp]
1: #define DUCKDB_EXTENSION_MAIN
2: 
3: #include "tpcds-extension.hpp"
4: 
5: #include "dsdgen.hpp"
6: 
7: #ifndef DUCKDB_AMALGAMATION
8: #include "duckdb/function/table_function.hpp"
9: #include "duckdb/parser/parsed_data/create_pragma_function_info.hpp"
10: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
11: #include "duckdb/parser/parsed_data/create_view_info.hpp"
12: #include "duckdb/parser/parser.hpp"
13: #include "duckdb/parser/statement/select_statement.hpp"
14: #endif
15: 
16: namespace duckdb {
17: 
18: struct DSDGenFunctionData : public TableFunctionData {
19: 	DSDGenFunctionData() {
20: 	}
21: 
22: 	bool finished = false;
23: 	double sf = 0;
24: 	string schema = DEFAULT_SCHEMA;
25: 	string suffix;
26: 	bool overwrite = false;
27: 	bool keys = false;
28: };
29: 
30: static unique_ptr<FunctionData> DsdgenBind(ClientContext &context, vector<Value> &inputs,
31:                                            named_parameter_map_t &named_parameters,
32:                                            vector<LogicalType> &input_table_types, vector<string> &input_table_names,
33:                                            vector<LogicalType> &return_types, vector<string> &names) {
34: 	auto result = make_unique<DSDGenFunctionData>();
35: 	for (auto &kv : named_parameters) {
36: 		if (kv.first == "sf") {
37: 			result->sf = kv.second.GetValue<double>();
38: 		} else if (kv.first == "schema") {
39: 			result->schema = StringValue::Get(kv.second);
40: 		} else if (kv.first == "suffix") {
41: 			result->suffix = StringValue::Get(kv.second);
42: 		} else if (kv.first == "overwrite") {
43: 			result->overwrite = kv.second.GetValue<bool>();
44: 		} else if (kv.first == "keys") {
45: 			result->keys = kv.second.GetValue<bool>();
46: 		}
47: 	}
48: 	return_types.emplace_back(LogicalType::BOOLEAN);
49: 	names.emplace_back("Success");
50: 	return move(result);
51: }
52: 
53: static void DsdgenFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
54:                            DataChunk *input, DataChunk &output) {
55: 	auto &data = (DSDGenFunctionData &)*bind_data;
56: 	if (data.finished) {
57: 		return;
58: 	}
59: 	tpcds::DSDGenWrapper::CreateTPCDSSchema(context, data.schema, data.suffix, data.keys, data.overwrite);
60: 	tpcds::DSDGenWrapper::DSDGen(data.sf, context, data.schema, data.suffix);
61: 
62: 	data.finished = true;
63: }
64: 
65: struct TPCDSData : public FunctionOperatorData {
66: 	TPCDSData() : offset(0) {
67: 	}
68: 	idx_t offset;
69: };
70: 
71: unique_ptr<FunctionOperatorData> TPCDSInit(ClientContext &context, const FunctionData *bind_data,
72:                                            const vector<column_t> &column_ids, TableFilterCollection *filters) {
73: 	auto result = make_unique<TPCDSData>();
74: 	return move(result);
75: }
76: 
77: static unique_ptr<FunctionData> TPCDSQueryBind(ClientContext &context, vector<Value> &inputs,
78:                                                named_parameter_map_t &named_parameters,
79:                                                vector<LogicalType> &input_table_types,
80:                                                vector<string> &input_table_names, vector<LogicalType> &return_types,
81:                                                vector<string> &names) {
82: 	names.emplace_back("query_nr");
83: 	return_types.emplace_back(LogicalType::INTEGER);
84: 
85: 	names.emplace_back("query");
86: 	return_types.emplace_back(LogicalType::VARCHAR);
87: 
88: 	return nullptr;
89: }
90: 
91: static void TPCDSQueryFunction(ClientContext &context, const FunctionData *bind_data,
92:                                FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
93: 	auto &data = (TPCDSData &)*operator_state;
94: 	idx_t tpcds_queries = tpcds::DSDGenWrapper::QueriesCount();
95: 	if (data.offset >= tpcds_queries) {
96: 		// finished returning values
97: 		return;
98: 	}
99: 	idx_t chunk_count = 0;
100: 	while (data.offset < tpcds_queries && chunk_count < STANDARD_VECTOR_SIZE) {
101: 		auto query = TPCDSExtension::GetQuery(data.offset + 1);
102: 		// "query_nr", PhysicalType::INT32
103: 		output.SetValue(0, chunk_count, Value::INTEGER((int32_t)data.offset + 1));
104: 		// "query", PhysicalType::VARCHAR
105: 		output.SetValue(1, chunk_count, Value(query));
106: 		data.offset++;
107: 		chunk_count++;
108: 	}
109: 	output.SetCardinality(chunk_count);
110: }
111: 
112: static unique_ptr<FunctionData> TPCDSQueryAnswerBind(ClientContext &context, vector<Value> &inputs,
113:                                                      named_parameter_map_t &named_parameters,
114:                                                      vector<LogicalType> &input_table_types,
115:                                                      vector<string> &input_table_names,
116:                                                      vector<LogicalType> &return_types, vector<string> &names) {
117: 	names.emplace_back("query_nr");
118: 	return_types.emplace_back(LogicalType::INTEGER);
119: 
120: 	names.emplace_back("scale_factor");
121: 	return_types.emplace_back(LogicalType::DOUBLE);
122: 
123: 	names.emplace_back("answer");
124: 	return_types.emplace_back(LogicalType::VARCHAR);
125: 
126: 	return nullptr;
127: }
128: 
129: static void TPCDSQueryAnswerFunction(ClientContext &context, const FunctionData *bind_data,
130:                                      FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
131: 	auto &data = (TPCDSData &)*operator_state;
132: 	idx_t tpcds_queries = tpcds::DSDGenWrapper::QueriesCount();
133: 	vector<double> scale_factors {1, 10};
134: 	idx_t total_answers = tpcds_queries * scale_factors.size();
135: 	if (data.offset >= total_answers) {
136: 		// finished returning values
137: 		return;
138: 	}
139: 	idx_t chunk_count = 0;
140: 	while (data.offset < total_answers && chunk_count < STANDARD_VECTOR_SIZE) {
141: 		idx_t cur_query = data.offset % tpcds_queries;
142: 		idx_t cur_sf = data.offset / tpcds_queries;
143: 		auto answer = TPCDSExtension::GetAnswer(scale_factors[cur_sf], cur_query + 1);
144: 		// "query_nr", PhysicalType::INT32
145: 		output.SetValue(0, chunk_count, Value::INTEGER((int32_t)cur_query + 1));
146: 		// "scale_factor", PhysicalType::DOUBLE
147: 		output.SetValue(1, chunk_count, Value::DOUBLE(scale_factors[cur_sf]));
148: 		// "query", PhysicalType::VARCHAR
149: 		output.SetValue(2, chunk_count, Value(answer));
150: 		data.offset++;
151: 		chunk_count++;
152: 	}
153: 	output.SetCardinality(chunk_count);
154: }
155: 
156: static string PragmaTpcdsQuery(ClientContext &context, const FunctionParameters &parameters) {
157: 	auto index = parameters.values[0].GetValue<int32_t>();
158: 	return tpcds::DSDGenWrapper::GetQuery(index);
159: }
160: 
161: void TPCDSExtension::Load(DuckDB &db) {
162: 	Connection con(db);
163: 	con.BeginTransaction();
164: 
165: 	TableFunction dsdgen_func("dsdgen", {}, DsdgenFunction, DsdgenBind);
166: 	dsdgen_func.named_parameters["sf"] = LogicalType::DOUBLE;
167: 	dsdgen_func.named_parameters["overwrite"] = LogicalType::BOOLEAN;
168: 	dsdgen_func.named_parameters["keys"] = LogicalType::BOOLEAN;
169: 	dsdgen_func.named_parameters["schema"] = LogicalType::VARCHAR;
170: 	dsdgen_func.named_parameters["suffix"] = LogicalType::VARCHAR;
171: 	CreateTableFunctionInfo dsdgen_info(dsdgen_func);
172: 
173: 	// create the dsdgen function
174: 	auto &catalog = Catalog::GetCatalog(*con.context);
175: 	catalog.CreateTableFunction(*con.context, &dsdgen_info);
176: 
177: 	// create the TPCDS pragma that allows us to run the query
178: 	auto tpcds_func = PragmaFunction::PragmaCall("tpcds", PragmaTpcdsQuery, {LogicalType::BIGINT});
179: 	CreatePragmaFunctionInfo info(tpcds_func);
180: 	catalog.CreatePragmaFunction(*con.context, &info);
181: 
182: 	// create the TPCDS_QUERIES function that returns the query
183: 	TableFunction tpcds_query_func("tpcds_queries", {}, TPCDSQueryFunction, TPCDSQueryBind, TPCDSInit);
184: 	CreateTableFunctionInfo tpcds_query_info(tpcds_query_func);
185: 	catalog.CreateTableFunction(*con.context, &tpcds_query_info);
186: 
187: 	// create the TPCDS_ANSWERS that returns the query result
188: 	TableFunction tpcds_query_answer_func("tpcds_answers", {}, TPCDSQueryAnswerFunction, TPCDSQueryAnswerBind,
189: 	                                      TPCDSInit);
190: 	CreateTableFunctionInfo tpcds_query_asnwer_info(tpcds_query_answer_func);
191: 	catalog.CreateTableFunction(*con.context, &tpcds_query_asnwer_info);
192: 
193: 	con.Commit();
194: }
195: 
196: std::string TPCDSExtension::GetQuery(int query) {
197: 	return tpcds::DSDGenWrapper::GetQuery(query);
198: }
199: 
200: std::string TPCDSExtension::GetAnswer(double sf, int query) {
201: 	return tpcds::DSDGenWrapper::GetAnswer(sf, query);
202: }
203: 
204: std::string TPCDSExtension::Name() {
205: 	return "tpcds";
206: }
207: 
208: } // namespace duckdb
209: 
210: extern "C" {
211: DUCKDB_EXTENSION_API void tpcds_init(duckdb::DatabaseInstance &db) {
212: 	duckdb::DuckDB db_wrapper(db);
213: 	db_wrapper.LoadExtension<duckdb::TPCDSExtension>();
214: }
215: 
216: DUCKDB_EXTENSION_API const char *tpcds_version() {
217: 	return duckdb::DuckDB::LibraryVersion();
218: }
219: }
220: 
221: #ifndef DUCKDB_EXTENSION_MAIN
222: #error DUCKDB_EXTENSION_MAIN not defined
223: #endif
[end of extension/tpcds/tpcds-extension.cpp]
[start of extension/tpch/tpch-extension.cpp]
1: #define DUCKDB_EXTENSION_MAIN
2: 
3: #include "tpch-extension.hpp"
4: 
5: #ifndef DUCKDB_AMALGAMATION
6: #include "duckdb/function/table_function.hpp"
7: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
8: #include "duckdb/parser/parsed_data/create_view_info.hpp"
9: #include "duckdb/parser/parser.hpp"
10: #include "duckdb/parser/statement/select_statement.hpp"
11: #include "duckdb/parser/parsed_data/create_pragma_function_info.hpp"
12: #include "duckdb/main/client_context.hpp"
13: #endif
14: 
15: #include "dbgen/dbgen.hpp"
16: 
17: namespace duckdb {
18: 
19: struct DBGenFunctionData : public TableFunctionData {
20: 	DBGenFunctionData() {
21: 	}
22: 
23: 	bool finished = false;
24: 	double sf = 0;
25: 	string schema = DEFAULT_SCHEMA;
26: 	string suffix;
27: 	bool overwrite = false;
28: };
29: 
30: static unique_ptr<FunctionData> DbgenBind(ClientContext &context, vector<Value> &inputs,
31:                                           named_parameter_map_t &named_parameters,
32:                                           vector<LogicalType> &input_table_types, vector<string> &input_table_names,
33:                                           vector<LogicalType> &return_types, vector<string> &names) {
34: 	auto result = make_unique<DBGenFunctionData>();
35: 	for (auto &kv : named_parameters) {
36: 		if (kv.first == "sf") {
37: 			result->sf = DoubleValue::Get(kv.second);
38: 		} else if (kv.first == "schema") {
39: 			result->schema = StringValue::Get(kv.second);
40: 		} else if (kv.first == "suffix") {
41: 			result->suffix = StringValue::Get(kv.second);
42: 		} else if (kv.first == "overwrite") {
43: 			result->overwrite = BooleanValue::Get(kv.second);
44: 		}
45: 	}
46: 	return_types.emplace_back(LogicalType::BOOLEAN);
47: 	names.emplace_back("Success");
48: 	return move(result);
49: }
50: 
51: static void DbgenFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
52:                           DataChunk *input, DataChunk &output) {
53: 	auto &data = (DBGenFunctionData &)*bind_data;
54: 	if (data.finished) {
55: 		return;
56: 	}
57: 	tpch::DBGenWrapper::CreateTPCHSchema(context, data.schema, data.suffix);
58: 	tpch::DBGenWrapper::LoadTPCHData(context, data.sf, data.schema, data.suffix);
59: 
60: 	data.finished = true;
61: }
62: 
63: struct TPCHData : public FunctionOperatorData {
64: 	TPCHData() : offset(0) {
65: 	}
66: 	idx_t offset;
67: };
68: 
69: unique_ptr<FunctionOperatorData> TPCHInit(ClientContext &context, const FunctionData *bind_data,
70:                                           const vector<column_t> &column_ids, TableFilterCollection *filters) {
71: 	auto result = make_unique<TPCHData>();
72: 	return move(result);
73: }
74: 
75: static unique_ptr<FunctionData> TPCHQueryBind(ClientContext &context, vector<Value> &inputs,
76:                                               named_parameter_map_t &named_parameters,
77:                                               vector<LogicalType> &input_table_types, vector<string> &input_table_names,
78:                                               vector<LogicalType> &return_types, vector<string> &names) {
79: 	names.emplace_back("query_nr");
80: 	return_types.emplace_back(LogicalType::INTEGER);
81: 
82: 	names.emplace_back("query");
83: 	return_types.emplace_back(LogicalType::VARCHAR);
84: 
85: 	return nullptr;
86: }
87: 
88: static void TPCHQueryFunction(ClientContext &context, const FunctionData *bind_data,
89:                               FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
90: 	auto &data = (TPCHData &)*operator_state;
91: 	idx_t tpch_queries = 22;
92: 	if (data.offset >= tpch_queries) {
93: 		// finished returning values
94: 		return;
95: 	}
96: 	idx_t chunk_count = 0;
97: 	while (data.offset < tpch_queries && chunk_count < STANDARD_VECTOR_SIZE) {
98: 		auto query = tpch::DBGenWrapper::GetQuery(data.offset + 1);
99: 		// "query_nr", PhysicalType::INT32
100: 		output.SetValue(0, chunk_count, Value::INTEGER((int32_t)data.offset + 1));
101: 		// "query", PhysicalType::VARCHAR
102: 		output.SetValue(1, chunk_count, Value(query));
103: 		data.offset++;
104: 		chunk_count++;
105: 	}
106: 	output.SetCardinality(chunk_count);
107: }
108: 
109: static unique_ptr<FunctionData> TPCHQueryAnswerBind(ClientContext &context, vector<Value> &inputs,
110:                                                     named_parameter_map_t &named_parameters,
111:                                                     vector<LogicalType> &input_table_types,
112:                                                     vector<string> &input_table_names,
113:                                                     vector<LogicalType> &return_types, vector<string> &names) {
114: 	names.emplace_back("query_nr");
115: 	return_types.emplace_back(LogicalType::INTEGER);
116: 
117: 	names.emplace_back("scale_factor");
118: 	return_types.emplace_back(LogicalType::DOUBLE);
119: 
120: 	names.emplace_back("answer");
121: 	return_types.emplace_back(LogicalType::VARCHAR);
122: 
123: 	return nullptr;
124: }
125: 
126: static void TPCHQueryAnswerFunction(ClientContext &context, const FunctionData *bind_data,
127:                                     FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
128: 	auto &data = (TPCHData &)*operator_state;
129: 	idx_t tpch_queries = 22;
130: 	vector<double> scale_factors {0.01, 0.1, 1};
131: 	idx_t total_answers = tpch_queries * scale_factors.size();
132: 	if (data.offset >= total_answers) {
133: 		// finished returning values
134: 		return;
135: 	}
136: 	idx_t chunk_count = 0;
137: 	while (data.offset < total_answers && chunk_count < STANDARD_VECTOR_SIZE) {
138: 		idx_t cur_query = data.offset % tpch_queries;
139: 		idx_t cur_sf = data.offset / tpch_queries;
140: 		auto answer = tpch::DBGenWrapper::GetAnswer(scale_factors[cur_sf], cur_query + 1);
141: 		// "query_nr", PhysicalType::INT32
142: 		output.SetValue(0, chunk_count, Value::INTEGER((int32_t)cur_query + 1));
143: 		// "scale_factor", PhysicalType::INT32
144: 		output.SetValue(1, chunk_count, Value::DOUBLE(scale_factors[cur_sf]));
145: 		// "query", PhysicalType::VARCHAR
146: 		output.SetValue(2, chunk_count, Value(answer));
147: 		data.offset++;
148: 		chunk_count++;
149: 	}
150: 	output.SetCardinality(chunk_count);
151: }
152: 
153: static string PragmaTpchQuery(ClientContext &context, const FunctionParameters &parameters) {
154: 	auto index = parameters.values[0].GetValue<int32_t>();
155: 	return tpch::DBGenWrapper::GetQuery(index);
156: }
157: 
158: void TPCHExtension::Load(DuckDB &db) {
159: 	Connection con(db);
160: 	con.BeginTransaction();
161: 	auto &catalog = Catalog::GetCatalog(*con.context);
162: 
163: 	TableFunction dbgen_func("dbgen", {}, DbgenFunction, DbgenBind);
164: 	dbgen_func.named_parameters["sf"] = LogicalType::DOUBLE;
165: 	dbgen_func.named_parameters["overwrite"] = LogicalType::BOOLEAN;
166: 	dbgen_func.named_parameters["schema"] = LogicalType::VARCHAR;
167: 	dbgen_func.named_parameters["suffix"] = LogicalType::VARCHAR;
168: 	CreateTableFunctionInfo dbgen_info(dbgen_func);
169: 
170: 	// create the dbgen function
171: 	catalog.CreateTableFunction(*con.context, &dbgen_info);
172: 
173: 	// create the TPCH pragma that allows us to run the query
174: 	auto tpch_func = PragmaFunction::PragmaCall("tpch", PragmaTpchQuery, {LogicalType::BIGINT});
175: 	CreatePragmaFunctionInfo info(tpch_func);
176: 	catalog.CreatePragmaFunction(*con.context, &info);
177: 
178: 	// create the TPCH_QUERIES function that returns the query
179: 	TableFunction tpch_query_func("tpch_queries", {}, TPCHQueryFunction, TPCHQueryBind, TPCHInit);
180: 	CreateTableFunctionInfo tpch_query_info(tpch_query_func);
181: 	catalog.CreateTableFunction(*con.context, &tpch_query_info);
182: 
183: 	// create the TPCH_ANSWERS that returns the query result
184: 	TableFunction tpch_query_answer_func("tpch_answers", {}, TPCHQueryAnswerFunction, TPCHQueryAnswerBind, TPCHInit);
185: 	CreateTableFunctionInfo tpch_query_asnwer_info(tpch_query_answer_func);
186: 	catalog.CreateTableFunction(*con.context, &tpch_query_asnwer_info);
187: 
188: 	con.Commit();
189: }
190: 
191: std::string TPCHExtension::GetQuery(int query) {
192: 	return tpch::DBGenWrapper::GetQuery(query);
193: }
194: 
195: std::string TPCHExtension::GetAnswer(double sf, int query) {
196: 	return tpch::DBGenWrapper::GetAnswer(sf, query);
197: }
198: 
199: std::string TPCHExtension::Name() {
200: 	return "tpch";
201: }
202: 
203: } // namespace duckdb
204: 
205: extern "C" {
206: 
207: DUCKDB_EXTENSION_API void tpch_init(duckdb::DatabaseInstance &db) {
208: 	duckdb::DuckDB db_wrapper(db);
209: 	db_wrapper.LoadExtension<duckdb::TPCHExtension>();
210: }
211: 
212: DUCKDB_EXTENSION_API const char *tpch_version() {
213: 	return duckdb::DuckDB::LibraryVersion();
214: }
215: }
216: 
217: #ifndef DUCKDB_EXTENSION_MAIN
218: #error DUCKDB_EXTENSION_MAIN not defined
219: #endif
[end of extension/tpch/tpch-extension.cpp]
[start of src/common/types/vector.cpp]
1: #include "duckdb/common/types/vector.hpp"
2: 
3: #include "duckdb/common/algorithm.hpp"
4: #include "duckdb/common/assert.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/operator/comparison_operators.hpp"
7: #include "duckdb/common/pair.hpp"
8: #include "duckdb/common/printer.hpp"
9: #include "duckdb/common/serializer.hpp"
10: #include "duckdb/common/types/null_value.hpp"
11: #include "duckdb/common/types/sel_cache.hpp"
12: #include "duckdb/common/types/vector_cache.hpp"
13: #include "duckdb/common/vector_operations/vector_operations.hpp"
14: #include "duckdb/storage/buffer/buffer_handle.hpp"
15: 
16: #include <cstring> // strlen() on Solaris
17: 
18: namespace duckdb {
19: 
20: Vector::Vector(LogicalType type_p, bool create_data, bool zero_data, idx_t capacity)
21:     : vector_type(VectorType::FLAT_VECTOR), type(move(type_p)), data(nullptr) {
22: 	if (create_data) {
23: 		Initialize(zero_data, capacity);
24: 	}
25: }
26: 
27: Vector::Vector(LogicalType type_p, idx_t capacity) : Vector(move(type_p), true, false, capacity) {
28: }
29: 
30: Vector::Vector(LogicalType type_p, data_ptr_t dataptr)
31:     : vector_type(VectorType::FLAT_VECTOR), type(move(type_p)), data(dataptr) {
32: 	if (dataptr && type.id() == LogicalTypeId::INVALID) {
33: 		throw InternalException("Cannot create a vector of type INVALID!");
34: 	}
35: }
36: 
37: Vector::Vector(const VectorCache &cache) : type(cache.GetType()) {
38: 	ResetFromCache(cache);
39: }
40: 
41: Vector::Vector(Vector &other) : type(other.type) {
42: 	Reference(other);
43: }
44: 
45: Vector::Vector(Vector &other, const SelectionVector &sel, idx_t count) : type(other.type) {
46: 	Slice(other, sel, count);
47: }
48: 
49: Vector::Vector(Vector &other, idx_t offset) : type(other.type) {
50: 	Slice(other, offset);
51: }
52: 
53: Vector::Vector(const Value &value) : type(value.type()) {
54: 	Reference(value);
55: }
56: 
57: Vector::Vector(Vector &&other) noexcept
58:     : vector_type(other.vector_type), type(move(other.type)), data(other.data), validity(move(other.validity)),
59:       buffer(move(other.buffer)), auxiliary(move(other.auxiliary)) {
60: }
61: 
62: void Vector::Reference(const Value &value) {
63: 	D_ASSERT(GetType().id() == value.type().id());
64: 	this->vector_type = VectorType::CONSTANT_VECTOR;
65: 	buffer = VectorBuffer::CreateConstantVector(value.type());
66: 	auto internal_type = value.type().InternalType();
67: 	if (internal_type == PhysicalType::STRUCT) {
68: 		auto struct_buffer = make_unique<VectorStructBuffer>();
69: 		auto &child_types = StructType::GetChildTypes(value.type());
70: 		auto &child_vectors = struct_buffer->GetChildren();
71: 		auto &value_children = StructValue::GetChildren(value);
72: 		for (idx_t i = 0; i < child_types.size(); i++) {
73: 			auto vector = make_unique<Vector>(value.IsNull() ? Value(child_types[i].second) : value_children[i]);
74: 			child_vectors.push_back(move(vector));
75: 		}
76: 		auxiliary = move(struct_buffer);
77: 		if (value.IsNull()) {
78: 			SetValue(0, value);
79: 		}
80: 	} else if (internal_type == PhysicalType::LIST) {
81: 		auto list_buffer = make_unique<VectorListBuffer>(value.type());
82: 		auxiliary = move(list_buffer);
83: 		data = buffer->GetData();
84: 		SetValue(0, value);
85: 	} else {
86: 		auxiliary.reset();
87: 		data = buffer->GetData();
88: 		SetValue(0, value);
89: 	}
90: }
91: 
92: void Vector::Reference(Vector &other) {
93: 	D_ASSERT(other.GetType() == GetType());
94: 	Reinterpret(other);
95: }
96: 
97: void Vector::ReferenceAndSetType(Vector &other) {
98: 	type = other.GetType();
99: 	Reference(other);
100: }
101: 
102: void Vector::Reinterpret(Vector &other) {
103: 	vector_type = other.vector_type;
104: 	AssignSharedPointer(buffer, other.buffer);
105: 	AssignSharedPointer(auxiliary, other.auxiliary);
106: 	data = other.data;
107: 	validity = other.validity;
108: }
109: 
110: void Vector::ResetFromCache(const VectorCache &cache) {
111: 	cache.ResetFromCache(*this);
112: }
113: 
114: void Vector::Slice(Vector &other, idx_t offset) {
115: 	if (other.GetVectorType() == VectorType::CONSTANT_VECTOR) {
116: 		Reference(other);
117: 		return;
118: 	}
119: 	D_ASSERT(other.GetVectorType() == VectorType::FLAT_VECTOR);
120: 
121: 	auto internal_type = GetType().InternalType();
122: 	if (internal_type == PhysicalType::STRUCT) {
123: 		Vector new_vector(GetType());
124: 		auto &entries = StructVector::GetEntries(new_vector);
125: 		auto &other_entries = StructVector::GetEntries(other);
126: 		D_ASSERT(entries.size() == other_entries.size());
127: 		for (idx_t i = 0; i < entries.size(); i++) {
128: 			entries[i]->Slice(*other_entries[i], offset);
129: 		}
130: 		if (offset > 0) {
131: 			new_vector.validity.Slice(other.validity, offset);
132: 		} else {
133: 			new_vector.validity = other.validity;
134: 		}
135: 		Reference(new_vector);
136: 	} else {
137: 		Reference(other);
138: 		if (offset > 0) {
139: 			data = data + GetTypeIdSize(internal_type) * offset;
140: 			validity.Slice(other.validity, offset);
141: 		}
142: 	}
143: }
144: 
145: void Vector::Slice(Vector &other, const SelectionVector &sel, idx_t count) {
146: 	Reference(other);
147: 	Slice(sel, count);
148: }
149: 
150: void Vector::Slice(const SelectionVector &sel, idx_t count) {
151: 	if (GetVectorType() == VectorType::CONSTANT_VECTOR) {
152: 		// dictionary on a constant is just a constant
153: 		return;
154: 	}
155: 	if (GetVectorType() == VectorType::DICTIONARY_VECTOR) {
156: 		// already a dictionary, slice the current dictionary
157: 		auto &current_sel = DictionaryVector::SelVector(*this);
158: 		auto sliced_dictionary = current_sel.Slice(sel, count);
159: 		buffer = make_buffer<DictionaryBuffer>(move(sliced_dictionary));
160: 		return;
161: 	}
162: 	Vector child_vector(*this);
163: 	auto child_ref = make_buffer<VectorChildBuffer>(move(child_vector));
164: 	auto dict_buffer = make_buffer<DictionaryBuffer>(sel);
165: 	vector_type = VectorType::DICTIONARY_VECTOR;
166: 	buffer = move(dict_buffer);
167: 	auxiliary = move(child_ref);
168: }
169: 
170: void Vector::Slice(const SelectionVector &sel, idx_t count, SelCache &cache) {
171: 	if (GetVectorType() == VectorType::DICTIONARY_VECTOR) {
172: 		// dictionary vector: need to merge dictionaries
173: 		// check if we have a cached entry
174: 		auto &current_sel = DictionaryVector::SelVector(*this);
175: 		auto target_data = current_sel.data();
176: 		auto entry = cache.cache.find(target_data);
177: 		if (entry != cache.cache.end()) {
178: 			// cached entry exists: use that
179: 			this->buffer = make_buffer<DictionaryBuffer>(((DictionaryBuffer &)*entry->second).GetSelVector());
180: 			vector_type = VectorType::DICTIONARY_VECTOR;
181: 		} else {
182: 			Slice(sel, count);
183: 			cache.cache[target_data] = this->buffer;
184: 		}
185: 	} else {
186: 		Slice(sel, count);
187: 	}
188: }
189: 
190: void Vector::Initialize(bool zero_data, idx_t capacity) {
191: 	auxiliary.reset();
192: 	validity.Reset();
193: 	auto &type = GetType();
194: 	auto internal_type = type.InternalType();
195: 	if (internal_type == PhysicalType::STRUCT) {
196: 		auto struct_buffer = make_unique<VectorStructBuffer>(type, capacity);
197: 		auxiliary = move(struct_buffer);
198: 	} else if (internal_type == PhysicalType::LIST) {
199: 		auto list_buffer = make_unique<VectorListBuffer>(type);
200: 		auxiliary = move(list_buffer);
201: 	}
202: 	auto type_size = GetTypeIdSize(internal_type);
203: 	if (type_size > 0) {
204: 		buffer = VectorBuffer::CreateStandardVector(type, capacity);
205: 		data = buffer->GetData();
206: 		if (zero_data) {
207: 			memset(data, 0, capacity * type_size);
208: 		}
209: 	}
210: }
211: 
212: struct DataArrays {
213: 	Vector &vec;
214: 	data_ptr_t data;
215: 	VectorBuffer *buffer;
216: 	idx_t type_size;
217: 	bool is_nested;
218: 	DataArrays(Vector &vec, data_ptr_t data, VectorBuffer *buffer, idx_t type_size, bool is_nested)
219: 	    : vec(vec), data(data), buffer(buffer), type_size(type_size), is_nested(is_nested) {};
220: };
221: 
222: void FindChildren(std::vector<DataArrays> &to_resize, VectorBuffer &auxiliary) {
223: 	if (auxiliary.GetBufferType() == VectorBufferType::LIST_BUFFER) {
224: 		auto &buffer = (VectorListBuffer &)auxiliary;
225: 		auto &child = buffer.GetChild();
226: 		auto data = child.GetData();
227: 		if (!data) {
228: 			//! Nested type
229: 			DataArrays arrays(child, data, child.GetBuffer().get(), GetTypeIdSize(child.GetType().InternalType()),
230: 			                  true);
231: 			to_resize.emplace_back(arrays);
232: 			FindChildren(to_resize, *child.GetAuxiliary());
233: 		} else {
234: 			DataArrays arrays(child, data, child.GetBuffer().get(), GetTypeIdSize(child.GetType().InternalType()),
235: 			                  false);
236: 			to_resize.emplace_back(arrays);
237: 		}
238: 	} else if (auxiliary.GetBufferType() == VectorBufferType::STRUCT_BUFFER) {
239: 		auto &buffer = (VectorStructBuffer &)auxiliary;
240: 		auto &children = buffer.GetChildren();
241: 		for (auto &child : children) {
242: 			auto data = child->GetData();
243: 			if (!data) {
244: 				//! Nested type
245: 				DataArrays arrays(*child, data, child->GetBuffer().get(),
246: 				                  GetTypeIdSize(child->GetType().InternalType()), true);
247: 				to_resize.emplace_back(arrays);
248: 				FindChildren(to_resize, *child->GetAuxiliary());
249: 			} else {
250: 				DataArrays arrays(*child, data, child->GetBuffer().get(),
251: 				                  GetTypeIdSize(child->GetType().InternalType()), false);
252: 				to_resize.emplace_back(arrays);
253: 			}
254: 		}
255: 	}
256: }
257: void Vector::Resize(idx_t cur_size, idx_t new_size) {
258: 	std::vector<DataArrays> to_resize;
259: 	if (!buffer) {
260: 		buffer = make_unique<VectorBuffer>(0);
261: 	}
262: 	if (!data) {
263: 		//! this is a nested structure
264: 		DataArrays arrays(*this, data, buffer.get(), GetTypeIdSize(GetType().InternalType()), true);
265: 		to_resize.emplace_back(arrays);
266: 		FindChildren(to_resize, *auxiliary);
267: 	} else {
268: 		DataArrays arrays(*this, data, buffer.get(), GetTypeIdSize(GetType().InternalType()), false);
269: 		to_resize.emplace_back(arrays);
270: 	}
271: 	for (auto &data_to_resize : to_resize) {
272: 		if (!data_to_resize.is_nested) {
273: 			auto new_data = unique_ptr<data_t[]>(new data_t[new_size * data_to_resize.type_size]);
274: 			memcpy(new_data.get(), data_to_resize.data, cur_size * data_to_resize.type_size * sizeof(data_t));
275: 			data_to_resize.buffer->SetData(move(new_data));
276: 			data_to_resize.vec.data = data_to_resize.buffer->GetData();
277: 		}
278: 		data_to_resize.vec.validity.Resize(cur_size, new_size);
279: 	}
280: }
281: 
282: void Vector::SetValue(idx_t index, const Value &val) {
283: 	if (GetVectorType() == VectorType::DICTIONARY_VECTOR) {
284: 		// dictionary: apply dictionary and forward to child
285: 		auto &sel_vector = DictionaryVector::SelVector(*this);
286: 		auto &child = DictionaryVector::Child(*this);
287: 		return child.SetValue(sel_vector.get_index(index), val);
288: 	}
289: 	if (val.type().InternalType() != GetType().InternalType()) {
290: 		SetValue(index, val.CastAs(GetType()));
291: 		return;
292: 	}
293: 
294: 	validity.EnsureWritable();
295: 	validity.Set(index, !val.IsNull());
296: 	if (val.IsNull() && GetType().InternalType() != PhysicalType::STRUCT) {
297: 		// for structs we still need to set the child-entries to NULL
298: 		// so we do not bail out yet
299: 		return;
300: 	}
301: 
302: 	switch (GetType().InternalType()) {
303: 	case PhysicalType::BOOL:
304: 		((bool *)data)[index] = val.GetValueUnsafe<bool>();
305: 		break;
306: 	case PhysicalType::INT8:
307: 		((int8_t *)data)[index] = val.GetValueUnsafe<int8_t>();
308: 		break;
309: 	case PhysicalType::INT16:
310: 		((int16_t *)data)[index] = val.GetValueUnsafe<int16_t>();
311: 		break;
312: 	case PhysicalType::INT32:
313: 		((int32_t *)data)[index] = val.GetValueUnsafe<int32_t>();
314: 		break;
315: 	case PhysicalType::INT64:
316: 		((int64_t *)data)[index] = val.GetValueUnsafe<int64_t>();
317: 		break;
318: 	case PhysicalType::INT128:
319: 		((hugeint_t *)data)[index] = val.GetValueUnsafe<hugeint_t>();
320: 		break;
321: 	case PhysicalType::UINT8:
322: 		((uint8_t *)data)[index] = val.GetValueUnsafe<uint8_t>();
323: 		break;
324: 	case PhysicalType::UINT16:
325: 		((uint16_t *)data)[index] = val.GetValueUnsafe<uint16_t>();
326: 		break;
327: 	case PhysicalType::UINT32:
328: 		((uint32_t *)data)[index] = val.GetValueUnsafe<uint32_t>();
329: 		break;
330: 	case PhysicalType::UINT64:
331: 		((uint64_t *)data)[index] = val.GetValueUnsafe<uint64_t>();
332: 		break;
333: 	case PhysicalType::FLOAT:
334: 		((float *)data)[index] = val.GetValueUnsafe<float>();
335: 		break;
336: 	case PhysicalType::DOUBLE:
337: 		((double *)data)[index] = val.GetValueUnsafe<double>();
338: 		break;
339: 	case PhysicalType::INTERVAL:
340: 		((interval_t *)data)[index] = val.GetValueUnsafe<interval_t>();
341: 		break;
342: 	case PhysicalType::VARCHAR:
343: 		((string_t *)data)[index] = StringVector::AddStringOrBlob(*this, StringValue::Get(val));
344: 		break;
345: 	case PhysicalType::STRUCT: {
346: 		D_ASSERT(GetVectorType() == VectorType::CONSTANT_VECTOR || GetVectorType() == VectorType::FLAT_VECTOR);
347: 
348: 		auto &children = StructVector::GetEntries(*this);
349: 		auto &val_children = StructValue::GetChildren(val);
350: 		D_ASSERT(val.IsNull() || children.size() == val_children.size());
351: 		for (size_t i = 0; i < children.size(); i++) {
352: 			auto &vec_child = children[i];
353: 			if (!val.IsNull()) {
354: 				auto &struct_child = val_children[i];
355: 				vec_child->SetValue(index, struct_child);
356: 			} else {
357: 				vec_child->SetValue(index, Value());
358: 			}
359: 		}
360: 		break;
361: 	}
362: 	case PhysicalType::LIST: {
363: 		auto offset = ListVector::GetListSize(*this);
364: 		auto &val_children = ListValue::GetChildren(val);
365: 		if (!val_children.empty()) {
366: 			for (idx_t i = 0; i < val_children.size(); i++) {
367: 				ListVector::PushBack(*this, val_children[i]);
368: 			}
369: 		}
370: 		//! now set the pointer
371: 		auto &entry = ((list_entry_t *)data)[index];
372: 		entry.length = val_children.size();
373: 		entry.offset = offset;
374: 		break;
375: 	}
376: 	default:
377: 		throw InternalException("Unimplemented type for Vector::SetValue");
378: 	}
379: }
380: 
381: Value Vector::GetValue(idx_t index) const {
382: 	switch (GetVectorType()) {
383: 	case VectorType::CONSTANT_VECTOR:
384: 		index = 0;
385: 		break;
386: 	case VectorType::FLAT_VECTOR:
387: 		break;
388: 		// dictionary: apply dictionary and forward to child
389: 	case VectorType::DICTIONARY_VECTOR: {
390: 		auto &sel_vector = DictionaryVector::SelVector(*this);
391: 		auto &child = DictionaryVector::Child(*this);
392: 		return child.GetValue(sel_vector.get_index(index));
393: 	}
394: 	case VectorType::SEQUENCE_VECTOR: {
395: 		int64_t start, increment;
396: 		SequenceVector::GetSequence(*this, start, increment);
397: 		return Value::Numeric(GetType(), start + increment * index);
398: 	}
399: 	default:
400: 		throw InternalException("Unimplemented vector type for Vector::GetValue");
401: 	}
402: 
403: 	if (!validity.RowIsValid(index)) {
404: 		return Value(GetType());
405: 	}
406: 	switch (GetType().id()) {
407: 	case LogicalTypeId::BOOLEAN:
408: 		return Value::BOOLEAN(((bool *)data)[index]);
409: 	case LogicalTypeId::TINYINT:
410: 		return Value::TINYINT(((int8_t *)data)[index]);
411: 	case LogicalTypeId::SMALLINT:
412: 		return Value::SMALLINT(((int16_t *)data)[index]);
413: 	case LogicalTypeId::INTEGER:
414: 		return Value::INTEGER(((int32_t *)data)[index]);
415: 	case LogicalTypeId::DATE:
416: 		return Value::DATE(((date_t *)data)[index]);
417: 	case LogicalTypeId::TIME:
418: 		return Value::TIME(((dtime_t *)data)[index]);
419: 	case LogicalTypeId::TIME_TZ:
420: 		return Value::TIMETZ(((dtime_t *)data)[index]);
421: 	case LogicalTypeId::BIGINT:
422: 		return Value::BIGINT(((int64_t *)data)[index]);
423: 	case LogicalTypeId::UTINYINT:
424: 		return Value::UTINYINT(((uint8_t *)data)[index]);
425: 	case LogicalTypeId::USMALLINT:
426: 		return Value::USMALLINT(((uint16_t *)data)[index]);
427: 	case LogicalTypeId::UINTEGER:
428: 		return Value::UINTEGER(((uint32_t *)data)[index]);
429: 	case LogicalTypeId::UBIGINT:
430: 		return Value::UBIGINT(((uint64_t *)data)[index]);
431: 	case LogicalTypeId::TIMESTAMP:
432: 		return Value::TIMESTAMP(((timestamp_t *)data)[index]);
433: 	case LogicalTypeId::TIMESTAMP_NS:
434: 		return Value::TIMESTAMPNS(((timestamp_t *)data)[index]);
435: 	case LogicalTypeId::TIMESTAMP_MS:
436: 		return Value::TIMESTAMPMS(((timestamp_t *)data)[index]);
437: 	case LogicalTypeId::TIMESTAMP_SEC:
438: 		return Value::TIMESTAMPSEC(((timestamp_t *)data)[index]);
439: 	case LogicalTypeId::TIMESTAMP_TZ:
440: 		return Value::TIMESTAMPTZ(((timestamp_t *)data)[index]);
441: 	case LogicalTypeId::HUGEINT:
442: 		return Value::HUGEINT(((hugeint_t *)data)[index]);
443: 	case LogicalTypeId::UUID:
444: 		return Value::UUID(((hugeint_t *)data)[index]);
445: 	case LogicalTypeId::DECIMAL: {
446: 		auto width = DecimalType::GetWidth(GetType());
447: 		auto scale = DecimalType::GetScale(GetType());
448: 		switch (GetType().InternalType()) {
449: 		case PhysicalType::INT16:
450: 			return Value::DECIMAL(((int16_t *)data)[index], width, scale);
451: 		case PhysicalType::INT32:
452: 			return Value::DECIMAL(((int32_t *)data)[index], width, scale);
453: 		case PhysicalType::INT64:
454: 			return Value::DECIMAL(((int64_t *)data)[index], width, scale);
455: 		case PhysicalType::INT128:
456: 			return Value::DECIMAL(((hugeint_t *)data)[index], width, scale);
457: 		default:
458: 			throw InternalException("Widths bigger than 38 are not supported");
459: 		}
460: 	}
461: 	case LogicalTypeId::ENUM: {
462: 		switch (type.InternalType()) {
463: 		case PhysicalType::UINT8:
464: 			return Value::ENUM(((uint8_t *)data)[index], GetType());
465: 		case PhysicalType::UINT16:
466: 			return Value::ENUM(((uint16_t *)data)[index], GetType());
467: 		case PhysicalType::UINT32:
468: 			return Value::ENUM(((uint32_t *)data)[index], GetType());
469: 		default:
470: 			throw InternalException("ENUM can only have unsigned integers (except UINT64) as physical types");
471: 		}
472: 	}
473: 	case LogicalTypeId::HASH:
474: 		return Value::HASH(((hash_t *)data)[index]);
475: 	case LogicalTypeId::POINTER:
476: 		return Value::POINTER(((uintptr_t *)data)[index]);
477: 	case LogicalTypeId::FLOAT:
478: 		return Value::FLOAT(((float *)data)[index]);
479: 	case LogicalTypeId::DOUBLE:
480: 		return Value::DOUBLE(((double *)data)[index]);
481: 	case LogicalTypeId::INTERVAL:
482: 		return Value::INTERVAL(((interval_t *)data)[index]);
483: 	case LogicalTypeId::VARCHAR: {
484: 		auto str = ((string_t *)data)[index];
485: 		return Value(str.GetString());
486: 	}
487: 	case LogicalTypeId::JSON: {
488: 		auto str = ((string_t *)data)[index];
489: 		return Value::JSON(str.GetString());
490: 	}
491: 	case LogicalTypeId::AGGREGATE_STATE:
492: 	case LogicalTypeId::BLOB: {
493: 		auto str = ((string_t *)data)[index];
494: 		return Value::BLOB((const_data_ptr_t)str.GetDataUnsafe(), str.GetSize());
495: 	}
496: 	case LogicalTypeId::MAP: {
497: 		auto &child_entries = StructVector::GetEntries(*this);
498: 		Value key = child_entries[0]->GetValue(index);
499: 		Value value = child_entries[1]->GetValue(index);
500: 		return Value::MAP(move(key), move(value));
501: 	}
502: 	case LogicalTypeId::STRUCT: {
503: 		// we can derive the value schema from the vector schema
504: 		auto &child_entries = StructVector::GetEntries(*this);
505: 		child_list_t<Value> children;
506: 		for (idx_t child_idx = 0; child_idx < child_entries.size(); child_idx++) {
507: 			auto &struct_child = child_entries[child_idx];
508: 			children.push_back(
509: 			    make_pair(StructType::GetChildName(GetType(), child_idx), struct_child->GetValue(index)));
510: 		}
511: 		return Value::STRUCT(move(children));
512: 	}
513: 	case LogicalTypeId::LIST: {
514: 		auto offlen = ((list_entry_t *)data)[index];
515: 		auto &child_vec = ListVector::GetEntry(*this);
516: 		vector<Value> children;
517: 		for (idx_t i = offlen.offset; i < offlen.offset + offlen.length; i++) {
518: 			children.push_back(child_vec.GetValue(i));
519: 		}
520: 		return Value::LIST(ListType::GetChildType(GetType()), move(children));
521: 	}
522: 	default:
523: 		throw InternalException("Unimplemented type for value access");
524: 	}
525: }
526: 
527: // LCOV_EXCL_START
528: string VectorTypeToString(VectorType type) {
529: 	switch (type) {
530: 	case VectorType::FLAT_VECTOR:
531: 		return "FLAT";
532: 	case VectorType::SEQUENCE_VECTOR:
533: 		return "SEQUENCE";
534: 	case VectorType::DICTIONARY_VECTOR:
535: 		return "DICTIONARY";
536: 	case VectorType::CONSTANT_VECTOR:
537: 		return "CONSTANT";
538: 	default:
539: 		return "UNKNOWN";
540: 	}
541: }
542: 
543: string Vector::ToString(idx_t count) const {
544: 	string retval =
545: 	    VectorTypeToString(GetVectorType()) + " " + GetType().ToString() + ": " + to_string(count) + " = [ ";
546: 	switch (GetVectorType()) {
547: 	case VectorType::FLAT_VECTOR:
548: 	case VectorType::DICTIONARY_VECTOR:
549: 		for (idx_t i = 0; i < count; i++) {
550: 			retval += GetValue(i).ToString() + (i == count - 1 ? "" : ", ");
551: 		}
552: 		break;
553: 	case VectorType::CONSTANT_VECTOR:
554: 		retval += GetValue(0).ToString();
555: 		break;
556: 	case VectorType::SEQUENCE_VECTOR: {
557: 		int64_t start, increment;
558: 		SequenceVector::GetSequence(*this, start, increment);
559: 		for (idx_t i = 0; i < count; i++) {
560: 			retval += to_string(start + increment * i) + (i == count - 1 ? "" : ", ");
561: 		}
562: 		break;
563: 	}
564: 	default:
565: 		retval += "UNKNOWN VECTOR TYPE";
566: 		break;
567: 	}
568: 	retval += "]";
569: 	return retval;
570: }
571: 
572: void Vector::Print(idx_t count) {
573: 	Printer::Print(ToString(count));
574: }
575: 
576: string Vector::ToString() const {
577: 	string retval = VectorTypeToString(GetVectorType()) + " " + GetType().ToString() + ": (UNKNOWN COUNT) [ ";
578: 	switch (GetVectorType()) {
579: 	case VectorType::FLAT_VECTOR:
580: 	case VectorType::DICTIONARY_VECTOR:
581: 		break;
582: 	case VectorType::CONSTANT_VECTOR:
583: 		retval += GetValue(0).ToString();
584: 		break;
585: 	case VectorType::SEQUENCE_VECTOR: {
586: 		break;
587: 	}
588: 	default:
589: 		retval += "UNKNOWN VECTOR TYPE";
590: 		break;
591: 	}
592: 	retval += "]";
593: 	return retval;
594: }
595: 
596: void Vector::Print() {
597: 	Printer::Print(ToString());
598: }
599: // LCOV_EXCL_STOP
600: 
601: template <class T>
602: static void TemplatedFlattenConstantVector(data_ptr_t data, data_ptr_t old_data, idx_t count) {
603: 	auto constant = Load<T>(old_data);
604: 	auto output = (T *)data;
605: 	for (idx_t i = 0; i < count; i++) {
606: 		output[i] = constant;
607: 	}
608: }
609: 
610: void Vector::Normalify(idx_t count) {
611: 	switch (GetVectorType()) {
612: 	case VectorType::FLAT_VECTOR:
613: 		// already a flat vector
614: 		break;
615: 	case VectorType::DICTIONARY_VECTOR: {
616: 		// create a new flat vector of this type
617: 		Vector other(GetType());
618: 		// now copy the data of this vector to the other vector, removing the selection vector in the process
619: 		VectorOperations::Copy(*this, other, count, 0, 0);
620: 		// create a reference to the data in the other vector
621: 		this->Reference(other);
622: 		break;
623: 	}
624: 	case VectorType::CONSTANT_VECTOR: {
625: 		bool is_null = ConstantVector::IsNull(*this);
626: 		// allocate a new buffer for the vector
627: 		auto old_buffer = move(buffer);
628: 		auto old_data = data;
629: 		buffer = VectorBuffer::CreateStandardVector(type, MaxValue<idx_t>(STANDARD_VECTOR_SIZE, count));
630: 		data = buffer->GetData();
631: 		vector_type = VectorType::FLAT_VECTOR;
632: 		if (is_null) {
633: 			// constant NULL, set nullmask
634: 			validity.EnsureWritable();
635: 			validity.SetAllInvalid(count);
636: 			return;
637: 		}
638: 		// non-null constant: have to repeat the constant
639: 		switch (GetType().InternalType()) {
640: 		case PhysicalType::BOOL:
641: 			TemplatedFlattenConstantVector<bool>(data, old_data, count);
642: 			break;
643: 		case PhysicalType::INT8:
644: 			TemplatedFlattenConstantVector<int8_t>(data, old_data, count);
645: 			break;
646: 		case PhysicalType::INT16:
647: 			TemplatedFlattenConstantVector<int16_t>(data, old_data, count);
648: 			break;
649: 		case PhysicalType::INT32:
650: 			TemplatedFlattenConstantVector<int32_t>(data, old_data, count);
651: 			break;
652: 		case PhysicalType::INT64:
653: 			TemplatedFlattenConstantVector<int64_t>(data, old_data, count);
654: 			break;
655: 		case PhysicalType::UINT8:
656: 			TemplatedFlattenConstantVector<uint8_t>(data, old_data, count);
657: 			break;
658: 		case PhysicalType::UINT16:
659: 			TemplatedFlattenConstantVector<uint16_t>(data, old_data, count);
660: 			break;
661: 		case PhysicalType::UINT32:
662: 			TemplatedFlattenConstantVector<uint32_t>(data, old_data, count);
663: 			break;
664: 		case PhysicalType::UINT64:
665: 			TemplatedFlattenConstantVector<uint64_t>(data, old_data, count);
666: 			break;
667: 		case PhysicalType::INT128:
668: 			TemplatedFlattenConstantVector<hugeint_t>(data, old_data, count);
669: 			break;
670: 		case PhysicalType::FLOAT:
671: 			TemplatedFlattenConstantVector<float>(data, old_data, count);
672: 			break;
673: 		case PhysicalType::DOUBLE:
674: 			TemplatedFlattenConstantVector<double>(data, old_data, count);
675: 			break;
676: 		case PhysicalType::INTERVAL:
677: 			TemplatedFlattenConstantVector<interval_t>(data, old_data, count);
678: 			break;
679: 		case PhysicalType::VARCHAR:
680: 			TemplatedFlattenConstantVector<string_t>(data, old_data, count);
681: 			break;
682: 		case PhysicalType::LIST: {
683: 			TemplatedFlattenConstantVector<list_entry_t>(data, old_data, count);
684: 			break;
685: 		}
686: 		case PhysicalType::STRUCT: {
687: 			auto normalified_buffer = make_unique<VectorStructBuffer>();
688: 
689: 			auto &new_children = normalified_buffer->GetChildren();
690: 
691: 			auto &child_entries = StructVector::GetEntries(*this);
692: 			for (auto &child : child_entries) {
693: 				D_ASSERT(child->GetVectorType() == VectorType::CONSTANT_VECTOR);
694: 				auto vector = make_unique<Vector>(*child);
695: 				vector->Normalify(count);
696: 				new_children.push_back(move(vector));
697: 			}
698: 			auxiliary = move(normalified_buffer);
699: 		} break;
700: 		default:
701: 			throw InternalException("Unimplemented type for VectorOperations::Normalify");
702: 		}
703: 		break;
704: 	}
705: 	case VectorType::SEQUENCE_VECTOR: {
706: 		int64_t start, increment;
707: 		SequenceVector::GetSequence(*this, start, increment);
708: 
709: 		buffer = VectorBuffer::CreateStandardVector(GetType());
710: 		data = buffer->GetData();
711: 		VectorOperations::GenerateSequence(*this, count, start, increment);
712: 		break;
713: 	}
714: 	default:
715: 		throw InternalException("Unimplemented type for normalify");
716: 	}
717: }
718: 
719: void Vector::Normalify(const SelectionVector &sel, idx_t count) {
720: 	switch (GetVectorType()) {
721: 	case VectorType::FLAT_VECTOR:
722: 		// already a flat vector
723: 		break;
724: 	case VectorType::SEQUENCE_VECTOR: {
725: 		int64_t start, increment;
726: 		SequenceVector::GetSequence(*this, start, increment);
727: 
728: 		buffer = VectorBuffer::CreateStandardVector(GetType());
729: 		data = buffer->GetData();
730: 		VectorOperations::GenerateSequence(*this, count, sel, start, increment);
731: 		break;
732: 	}
733: 	default:
734: 		throw InternalException("Unimplemented type for normalify with selection vector");
735: 	}
736: }
737: 
738: void Vector::Orrify(idx_t count, VectorData &data) {
739: 	switch (GetVectorType()) {
740: 	case VectorType::DICTIONARY_VECTOR: {
741: 		auto &sel = DictionaryVector::SelVector(*this);
742: 		auto &child = DictionaryVector::Child(*this);
743: 		if (child.GetVectorType() == VectorType::FLAT_VECTOR) {
744: 			data.sel = &sel;
745: 			data.data = FlatVector::GetData(child);
746: 			data.validity = FlatVector::Validity(child);
747: 		} else {
748: 			// dictionary with non-flat child: create a new reference to the child and normalify it
749: 			Vector child_vector(child);
750: 			child_vector.Normalify(sel, count);
751: 			auto new_aux = make_buffer<VectorChildBuffer>(move(child_vector));
752: 
753: 			data.sel = &sel;
754: 			data.data = FlatVector::GetData(new_aux->data);
755: 			data.validity = FlatVector::Validity(new_aux->data);
756: 			this->auxiliary = move(new_aux);
757: 		}
758: 		break;
759: 	}
760: 	case VectorType::CONSTANT_VECTOR:
761: 		data.sel = ConstantVector::ZeroSelectionVector(count, data.owned_sel);
762: 		data.data = ConstantVector::GetData(*this);
763: 		data.validity = ConstantVector::Validity(*this);
764: 		break;
765: 	default:
766: 		Normalify(count);
767: 		data.sel = FlatVector::IncrementalSelectionVector(count, data.owned_sel);
768: 		data.data = FlatVector::GetData(*this);
769: 		data.validity = FlatVector::Validity(*this);
770: 		break;
771: 	}
772: }
773: 
774: void Vector::Sequence(int64_t start, int64_t increment) {
775: 	this->vector_type = VectorType::SEQUENCE_VECTOR;
776: 	this->buffer = make_buffer<VectorBuffer>(sizeof(int64_t) * 2);
777: 	auto data = (int64_t *)buffer->GetData();
778: 	data[0] = start;
779: 	data[1] = increment;
780: 	validity.Reset();
781: 	auxiliary.reset();
782: }
783: 
784: void Vector::Serialize(idx_t count, Serializer &serializer) {
785: 	auto &type = GetType();
786: 
787: 	VectorData vdata;
788: 	Orrify(count, vdata);
789: 
790: 	const auto write_validity = (count > 0) && !vdata.validity.AllValid();
791: 	serializer.Write<bool>(write_validity);
792: 	if (write_validity) {
793: 		ValidityMask flat_mask(count);
794: 		for (idx_t i = 0; i < count; ++i) {
795: 			auto row_idx = vdata.sel->get_index(i);
796: 			flat_mask.Set(i, vdata.validity.RowIsValid(row_idx));
797: 		}
798: 		serializer.WriteData((const_data_ptr_t)flat_mask.GetData(), flat_mask.ValidityMaskSize(count));
799: 	}
800: 	if (TypeIsConstantSize(type.InternalType())) {
801: 		// constant size type: simple copy
802: 		idx_t write_size = GetTypeIdSize(type.InternalType()) * count;
803: 		auto ptr = unique_ptr<data_t[]>(new data_t[write_size]);
804: 		VectorOperations::WriteToStorage(*this, count, ptr.get());
805: 		serializer.WriteData(ptr.get(), write_size);
806: 	} else {
807: 		switch (type.InternalType()) {
808: 		case PhysicalType::VARCHAR: {
809: 			auto strings = (string_t *)vdata.data;
810: 			for (idx_t i = 0; i < count; i++) {
811: 				auto idx = vdata.sel->get_index(i);
812: 				auto source = !vdata.validity.RowIsValid(idx) ? NullValue<string_t>() : strings[idx];
813: 				serializer.WriteStringLen((const_data_ptr_t)source.GetDataUnsafe(), source.GetSize());
814: 			}
815: 			break;
816: 		}
817: 		case PhysicalType::STRUCT: {
818: 			Normalify(count);
819: 			auto &entries = StructVector::GetEntries(*this);
820: 			for (auto &entry : entries) {
821: 				entry->Serialize(count, serializer);
822: 			}
823: 			break;
824: 		}
825: 		case PhysicalType::LIST: {
826: 			auto &child = ListVector::GetEntry(*this);
827: 			auto list_size = ListVector::GetListSize(*this);
828: 
829: 			// serialize the list entries in a flat array
830: 			auto data = unique_ptr<list_entry_t[]>(new list_entry_t[count]);
831: 			auto source_array = (list_entry_t *)vdata.data;
832: 			for (idx_t i = 0; i < count; i++) {
833: 				auto idx = vdata.sel->get_index(i);
834: 				auto source = source_array[idx];
835: 				data[i].offset = source.offset;
836: 				data[i].length = source.length;
837: 			}
838: 
839: 			// write the list size
840: 			serializer.Write<idx_t>(list_size);
841: 			serializer.WriteData((data_ptr_t)data.get(), count * sizeof(list_entry_t));
842: 
843: 			child.Serialize(list_size, serializer);
844: 			break;
845: 		}
846: 		default:
847: 			throw InternalException("Unimplemented variable width type for Vector::Serialize!");
848: 		}
849: 	}
850: }
851: 
852: void Vector::Deserialize(idx_t count, Deserializer &source) {
853: 	auto &type = GetType();
854: 
855: 	auto &validity = FlatVector::Validity(*this);
856: 	validity.Reset();
857: 	const auto has_validity = source.Read<bool>();
858: 	if (has_validity) {
859: 		validity.Initialize(count);
860: 		source.ReadData((data_ptr_t)validity.GetData(), validity.ValidityMaskSize(count));
861: 	}
862: 
863: 	if (TypeIsConstantSize(type.InternalType())) {
864: 		// constant size type: read fixed amount of data from
865: 		auto column_size = GetTypeIdSize(type.InternalType()) * count;
866: 		auto ptr = unique_ptr<data_t[]>(new data_t[column_size]);
867: 		source.ReadData(ptr.get(), column_size);
868: 
869: 		VectorOperations::ReadFromStorage(ptr.get(), count, *this);
870: 	} else {
871: 		switch (type.InternalType()) {
872: 		case PhysicalType::VARCHAR: {
873: 			auto strings = FlatVector::GetData<string_t>(*this);
874: 			for (idx_t i = 0; i < count; i++) {
875: 				// read the strings
876: 				auto str = source.Read<string>();
877: 				// now add the string to the StringHeap of the vector
878: 				// and write the pointer into the vector
879: 				if (validity.RowIsValid(i)) {
880: 					strings[i] = StringVector::AddStringOrBlob(*this, str);
881: 				}
882: 			}
883: 			break;
884: 		}
885: 		case PhysicalType::STRUCT: {
886: 			auto &entries = StructVector::GetEntries(*this);
887: 			for (auto &entry : entries) {
888: 				entry->Deserialize(count, source);
889: 			}
890: 			break;
891: 		}
892: 		case PhysicalType::LIST: {
893: 			// read the list size
894: 			auto list_size = source.Read<idx_t>();
895: 			ListVector::Reserve(*this, list_size);
896: 			ListVector::SetListSize(*this, list_size);
897: 
898: 			// read the list entry
899: 			auto list_entries = FlatVector::GetData(*this);
900: 			source.ReadData(list_entries, count * sizeof(list_entry_t));
901: 
902: 			// deserialize the child vector
903: 			auto &child = ListVector::GetEntry(*this);
904: 			child.Deserialize(list_size, source);
905: 
906: 			break;
907: 		}
908: 		default:
909: 			throw InternalException("Unimplemented variable width type for Vector::Deserialize!");
910: 		}
911: 	}
912: }
913: 
914: void Vector::SetVectorType(VectorType vector_type_p) {
915: 	this->vector_type = vector_type_p;
916: 	if (vector_type == VectorType::CONSTANT_VECTOR && GetType().InternalType() == PhysicalType::STRUCT) {
917: 		auto &entries = StructVector::GetEntries(*this);
918: 		for (auto &entry : entries) {
919: 			entry->SetVectorType(vector_type);
920: 		}
921: 	}
922: }
923: 
924: void Vector::UTFVerify(const SelectionVector &sel, idx_t count) {
925: #ifdef DEBUG
926: 	if (count == 0) {
927: 		return;
928: 	}
929: 	if (GetType().InternalType() == PhysicalType::VARCHAR) {
930: 		// we just touch all the strings and let the sanitizer figure out if any
931: 		// of them are deallocated/corrupt
932: 		switch (GetVectorType()) {
933: 		case VectorType::CONSTANT_VECTOR: {
934: 			auto string = ConstantVector::GetData<string_t>(*this);
935: 			if (!ConstantVector::IsNull(*this)) {
936: 				string->Verify();
937: 			}
938: 			break;
939: 		}
940: 		case VectorType::FLAT_VECTOR: {
941: 			auto strings = FlatVector::GetData<string_t>(*this);
942: 			for (idx_t i = 0; i < count; i++) {
943: 				auto oidx = sel.get_index(i);
944: 				if (validity.RowIsValid(oidx)) {
945: 					strings[oidx].Verify();
946: 				}
947: 			}
948: 			break;
949: 		}
950: 		default:
951: 			break;
952: 		}
953: 	}
954: #endif
955: }
956: 
957: void Vector::UTFVerify(idx_t count) {
958: 	SelectionVector owned_sel;
959: 	auto flat_sel = FlatVector::IncrementalSelectionVector(count, owned_sel);
960: 
961: 	UTFVerify(*flat_sel, count);
962: }
963: 
964: void Vector::Verify(const SelectionVector &sel, idx_t count) {
965: #ifdef DEBUG
966: 	if (count == 0) {
967: 		return;
968: 	}
969: 	if (GetVectorType() == VectorType::DICTIONARY_VECTOR) {
970: 		auto &child = DictionaryVector::Child(*this);
971: 		D_ASSERT(child.GetVectorType() != VectorType::DICTIONARY_VECTOR);
972: 		auto &dict_sel = DictionaryVector::SelVector(*this);
973: 		// merge the selection vectors and verify the child
974: 		auto new_buffer = dict_sel.Slice(sel, count);
975: 		SelectionVector new_sel(new_buffer);
976: 		child.Verify(new_sel, count);
977: 		return;
978: 	}
979: 	if (TypeIsConstantSize(GetType().InternalType()) &&
980: 	    (GetVectorType() == VectorType::CONSTANT_VECTOR || GetVectorType() == VectorType::FLAT_VECTOR)) {
981: 		D_ASSERT(!auxiliary);
982: 	}
983: 	if (GetType().InternalType() == PhysicalType::DOUBLE) {
984: 		// verify that there are no INF or NAN values
985: 		switch (GetVectorType()) {
986: 		case VectorType::CONSTANT_VECTOR: {
987: 			auto dbl = ConstantVector::GetData<double>(*this);
988: 			if (!ConstantVector::IsNull(*this)) {
989: 				D_ASSERT(Value::DoubleIsValid(*dbl));
990: 			}
991: 			break;
992: 		}
993: 		case VectorType::FLAT_VECTOR: {
994: 			auto doubles = FlatVector::GetData<double>(*this);
995: 			for (idx_t i = 0; i < count; i++) {
996: 				auto oidx = sel.get_index(i);
997: 				if (validity.RowIsValid(oidx)) {
998: 					D_ASSERT(Value::DoubleIsValid(doubles[oidx]));
999: 				}
1000: 			}
1001: 			break;
1002: 		}
1003: 		default:
1004: 			break;
1005: 		}
1006: 	}
1007: 	if (GetType().id() == LogicalTypeId::VARCHAR || GetType().id() == LogicalTypeId::JSON) {
1008: 		// verify that there are no '\0' bytes in string values
1009: 		switch (GetVectorType()) {
1010: 		case VectorType::FLAT_VECTOR: {
1011: 			auto strings = FlatVector::GetData<string_t>(*this);
1012: 			for (idx_t i = 0; i < count; i++) {
1013: 				auto oidx = sel.get_index(i);
1014: 				if (validity.RowIsValid(oidx)) {
1015: 					strings[oidx].VerifyNull();
1016: 				}
1017: 			}
1018: 			break;
1019: 		}
1020: 		default:
1021: 			break;
1022: 		}
1023: 	}
1024: 
1025: 	if (GetType().InternalType() == PhysicalType::STRUCT) {
1026: 		auto &child_types = StructType::GetChildTypes(GetType());
1027: 		D_ASSERT(!child_types.empty());
1028: 		if (GetVectorType() == VectorType::FLAT_VECTOR || GetVectorType() == VectorType::CONSTANT_VECTOR) {
1029: 			// create a selection vector of the non-null entries of the struct vector
1030: 			auto &children = StructVector::GetEntries(*this);
1031: 			D_ASSERT(child_types.size() == children.size());
1032: 			for (idx_t child_idx = 0; child_idx < children.size(); child_idx++) {
1033: 				if (GetVectorType() == VectorType::CONSTANT_VECTOR) {
1034: 					D_ASSERT(children[child_idx]->GetVectorType() == VectorType::CONSTANT_VECTOR);
1035: 					if (ConstantVector::IsNull(*this)) {
1036: 						D_ASSERT(ConstantVector::IsNull(*children[child_idx]));
1037: 					}
1038: 				} else if (GetVectorType() == VectorType::FLAT_VECTOR &&
1039: 				           children[child_idx]->GetVectorType() == VectorType::FLAT_VECTOR) {
1040: 					// for any NULL entry in the struct, the child should be NULL as well
1041: 					auto &validity = FlatVector::Validity(*this);
1042: 					auto &child_validity = FlatVector::Validity(*children[child_idx]);
1043: 					for (idx_t i = 0; i < count; i++) {
1044: 						auto index = sel.get_index(i);
1045: 						if (!validity.RowIsValid(index)) {
1046: 							D_ASSERT(!child_validity.RowIsValid(index));
1047: 						}
1048: 					}
1049: 				}
1050: 				D_ASSERT(children[child_idx]->GetType() == child_types[child_idx].second);
1051: 				children[child_idx]->Verify(sel, count);
1052: 			}
1053: 		}
1054: 	}
1055: 
1056: 	if (GetType().InternalType() == PhysicalType::LIST) {
1057: 		if (GetVectorType() == VectorType::CONSTANT_VECTOR) {
1058: 			if (!ConstantVector::IsNull(*this)) {
1059: 				auto &child = ListVector::GetEntry(*this);
1060: 				SelectionVector child_sel(ListVector::GetListSize(*this));
1061: 				idx_t child_count = 0;
1062: 				auto le = ConstantVector::GetData<list_entry_t>(*this);
1063: 				D_ASSERT(le->offset + le->length <= ListVector::GetListSize(*this));
1064: 				for (idx_t k = 0; k < le->length; k++) {
1065: 					child_sel.set_index(child_count++, le->offset + k);
1066: 				}
1067: 				child.Verify(child_sel, child_count);
1068: 			}
1069: 		} else if (GetVectorType() == VectorType::FLAT_VECTOR) {
1070: 			auto &child = ListVector::GetEntry(*this);
1071: 			auto child_size = ListVector::GetListSize(*this);
1072: 			auto list_data = FlatVector::GetData<list_entry_t>(*this);
1073: 			idx_t total_size = 0;
1074: 			for (idx_t i = 0; i < count; i++) {
1075: 				auto idx = sel.get_index(i);
1076: 				auto &le = list_data[idx];
1077: 				if (validity.RowIsValid(idx)) {
1078: 					D_ASSERT(le.offset + le.length <= child_size);
1079: 					total_size += le.length;
1080: 				}
1081: 			}
1082: 			SelectionVector child_sel(total_size);
1083: 			idx_t child_count = 0;
1084: 			for (idx_t i = 0; i < count; i++) {
1085: 				auto idx = sel.get_index(i);
1086: 				auto &le = list_data[idx];
1087: 				if (validity.RowIsValid(idx)) {
1088: 					D_ASSERT(le.offset + le.length <= child_size);
1089: 					for (idx_t k = 0; k < le.length; k++) {
1090: 						child_sel.set_index(child_count++, le.offset + k);
1091: 					}
1092: 				}
1093: 			}
1094: 			child.Verify(child_sel, child_count);
1095: 		}
1096: 	}
1097: #endif
1098: }
1099: 
1100: void Vector::Verify(idx_t count) {
1101: 	SelectionVector owned_sel;
1102: 	auto flat_sel = FlatVector::IncrementalSelectionVector(count, owned_sel);
1103: 	Verify(*flat_sel, count);
1104: }
1105: 
1106: void FlatVector::SetNull(Vector &vector, idx_t idx, bool is_null) {
1107: 	D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);
1108: 	vector.validity.Set(idx, !is_null);
1109: 	if (is_null && vector.GetType().InternalType() == PhysicalType::STRUCT) {
1110: 		// set all child entries to null as well
1111: 		auto &entries = StructVector::GetEntries(vector);
1112: 		for (auto &entry : entries) {
1113: 			FlatVector::SetNull(*entry, idx, is_null);
1114: 		}
1115: 	}
1116: }
1117: 
1118: void ConstantVector::SetNull(Vector &vector, bool is_null) {
1119: 	D_ASSERT(vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
1120: 	vector.validity.Set(0, !is_null);
1121: 	if (is_null && vector.GetType().InternalType() == PhysicalType::STRUCT) {
1122: 		// set all child entries to null as well
1123: 		auto &entries = StructVector::GetEntries(vector);
1124: 		for (auto &entry : entries) {
1125: 			entry->SetVectorType(VectorType::CONSTANT_VECTOR);
1126: 			ConstantVector::SetNull(*entry, is_null);
1127: 		}
1128: 	}
1129: }
1130: 
1131: const SelectionVector *FlatVector::IncrementalSelectionVector(idx_t count, SelectionVector &owned_sel) {
1132: 	if (count <= STANDARD_VECTOR_SIZE) {
1133: 		return FlatVector::IncrementalSelectionVector();
1134: 	}
1135: 	owned_sel.Initialize(count);
1136: 	for (idx_t i = 0; i < count; i++) {
1137: 		owned_sel.set_index(i, i);
1138: 	}
1139: 	return &owned_sel;
1140: }
1141: 
1142: const SelectionVector *ConstantVector::ZeroSelectionVector(idx_t count, SelectionVector &owned_sel) {
1143: 	if (count <= STANDARD_VECTOR_SIZE) {
1144: 		return ConstantVector::ZeroSelectionVector();
1145: 	}
1146: 	owned_sel.Initialize(count);
1147: 	for (idx_t i = 0; i < count; i++) {
1148: 		owned_sel.set_index(i, 0);
1149: 	}
1150: 	return &owned_sel;
1151: }
1152: 
1153: void ConstantVector::Reference(Vector &vector, Vector &source, idx_t position, idx_t count) {
1154: 	D_ASSERT(position < count);
1155: 	auto &source_type = source.GetType();
1156: 	switch (source_type.InternalType()) {
1157: 	case PhysicalType::LIST: {
1158: 		// retrieve the list entry from the source vector
1159: 		VectorData vdata;
1160: 		source.Orrify(count, vdata);
1161: 
1162: 		auto list_index = vdata.sel->get_index(position);
1163: 		if (!vdata.validity.RowIsValid(list_index)) {
1164: 			// list is null: create null value
1165: 			Value null_value(source_type);
1166: 			vector.Reference(null_value);
1167: 			break;
1168: 		}
1169: 
1170: 		auto list_data = (list_entry_t *)vdata.data;
1171: 		auto list_entry = list_data[list_index];
1172: 
1173: 		// add the list entry as the first element of "vector"
1174: 		// FIXME: we only need to allocate space for 1 tuple here
1175: 		auto target_data = FlatVector::GetData<list_entry_t>(vector);
1176: 		target_data[0] = list_entry;
1177: 
1178: 		// create a reference to the child list of the source vector
1179: 		auto &child = ListVector::GetEntry(vector);
1180: 		child.Reference(ListVector::GetEntry(source));
1181: 
1182: 		ListVector::SetListSize(vector, ListVector::GetListSize(source));
1183: 		vector.SetVectorType(VectorType::CONSTANT_VECTOR);
1184: 		break;
1185: 	}
1186: 	case PhysicalType::STRUCT: {
1187: 		VectorData vdata;
1188: 		source.Orrify(count, vdata);
1189: 
1190: 		auto struct_index = vdata.sel->get_index(position);
1191: 		if (!vdata.validity.RowIsValid(struct_index)) {
1192: 			// null struct: create null value
1193: 			Value null_value(source_type);
1194: 			vector.Reference(null_value);
1195: 			break;
1196: 		}
1197: 
1198: 		// struct: pass constant reference into child entries
1199: 		auto &source_entries = StructVector::GetEntries(source);
1200: 		auto &target_entries = StructVector::GetEntries(vector);
1201: 		for (idx_t i = 0; i < source_entries.size(); i++) {
1202: 			ConstantVector::Reference(*target_entries[i], *source_entries[i], position, count);
1203: 		}
1204: 		vector.SetVectorType(VectorType::CONSTANT_VECTOR);
1205: 		break;
1206: 	}
1207: 	default:
1208: 		// default behavior: get a value from the vector and reference it
1209: 		// this is not that expensive for scalar types
1210: 		auto value = source.GetValue(position);
1211: 		vector.Reference(value);
1212: 		D_ASSERT(vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
1213: 		break;
1214: 	}
1215: }
1216: 
1217: string_t StringVector::AddString(Vector &vector, const char *data, idx_t len) {
1218: 	return StringVector::AddString(vector, string_t(data, len));
1219: }
1220: 
1221: string_t StringVector::AddStringOrBlob(Vector &vector, const char *data, idx_t len) {
1222: 	return StringVector::AddStringOrBlob(vector, string_t(data, len));
1223: }
1224: 
1225: string_t StringVector::AddString(Vector &vector, const char *data) {
1226: 	return StringVector::AddString(vector, string_t(data, strlen(data)));
1227: }
1228: 
1229: string_t StringVector::AddString(Vector &vector, const string &data) {
1230: 	return StringVector::AddString(vector, string_t(data.c_str(), data.size()));
1231: }
1232: 
1233: string_t StringVector::AddString(Vector &vector, string_t data) {
1234: 	D_ASSERT(vector.GetType().id() == LogicalTypeId::VARCHAR || vector.GetType().id() == LogicalTypeId::JSON);
1235: 	if (data.IsInlined()) {
1236: 		// string will be inlined: no need to store in string heap
1237: 		return data;
1238: 	}
1239: 	if (!vector.auxiliary) {
1240: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
1241: 	}
1242: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::STRING_BUFFER);
1243: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
1244: 	return string_buffer.AddString(data);
1245: }
1246: 
1247: string_t StringVector::AddStringOrBlob(Vector &vector, string_t data) {
1248: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
1249: 	if (data.IsInlined()) {
1250: 		// string will be inlined: no need to store in string heap
1251: 		return data;
1252: 	}
1253: 	if (!vector.auxiliary) {
1254: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
1255: 	}
1256: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::STRING_BUFFER);
1257: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
1258: 	return string_buffer.AddBlob(data);
1259: }
1260: 
1261: string_t StringVector::EmptyString(Vector &vector, idx_t len) {
1262: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
1263: 	if (len < string_t::INLINE_LENGTH) {
1264: 		return string_t(len);
1265: 	}
1266: 	if (!vector.auxiliary) {
1267: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
1268: 	}
1269: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::STRING_BUFFER);
1270: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
1271: 	return string_buffer.EmptyString(len);
1272: }
1273: 
1274: void StringVector::AddHandle(Vector &vector, unique_ptr<BufferHandle> handle) {
1275: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
1276: 	if (!vector.auxiliary) {
1277: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
1278: 	}
1279: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
1280: 	string_buffer.AddHeapReference(make_buffer<ManagedVectorBuffer>(move(handle)));
1281: }
1282: 
1283: void StringVector::AddBuffer(Vector &vector, buffer_ptr<VectorBuffer> buffer) {
1284: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
1285: 	D_ASSERT(buffer.get() != vector.auxiliary.get());
1286: 	if (!vector.auxiliary) {
1287: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
1288: 	}
1289: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
1290: 	string_buffer.AddHeapReference(move(buffer));
1291: }
1292: 
1293: void StringVector::AddHeapReference(Vector &vector, Vector &other) {
1294: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
1295: 	D_ASSERT(other.GetType().InternalType() == PhysicalType::VARCHAR);
1296: 
1297: 	if (other.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
1298: 		StringVector::AddHeapReference(vector, DictionaryVector::Child(other));
1299: 		return;
1300: 	}
1301: 	if (!other.auxiliary) {
1302: 		return;
1303: 	}
1304: 	StringVector::AddBuffer(vector, other.auxiliary);
1305: }
1306: 
1307: vector<unique_ptr<Vector>> &StructVector::GetEntries(Vector &vector) {
1308: 	D_ASSERT(vector.GetType().id() == LogicalTypeId::STRUCT || vector.GetType().id() == LogicalTypeId::MAP);
1309: 	if (vector.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
1310: 		auto &child = DictionaryVector::Child(vector);
1311: 		return StructVector::GetEntries(child);
1312: 	}
1313: 	D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR ||
1314: 	         vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
1315: 	D_ASSERT(vector.auxiliary);
1316: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::STRUCT_BUFFER);
1317: 	return ((VectorStructBuffer *)vector.auxiliary.get())->GetChildren();
1318: }
1319: 
1320: const vector<unique_ptr<Vector>> &StructVector::GetEntries(const Vector &vector) {
1321: 	return GetEntries((Vector &)vector);
1322: }
1323: 
1324: const Vector &ListVector::GetEntry(const Vector &vector) {
1325: 	D_ASSERT(vector.GetType().id() == LogicalTypeId::LIST);
1326: 	if (vector.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
1327: 		auto &child = DictionaryVector::Child(vector);
1328: 		return ListVector::GetEntry(child);
1329: 	}
1330: 	D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR ||
1331: 	         vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
1332: 	D_ASSERT(vector.auxiliary);
1333: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::LIST_BUFFER);
1334: 	return ((VectorListBuffer *)vector.auxiliary.get())->GetChild();
1335: }
1336: 
1337: Vector &ListVector::GetEntry(Vector &vector) {
1338: 	const Vector &cvector = vector;
1339: 	return const_cast<Vector &>(ListVector::GetEntry(cvector));
1340: }
1341: 
1342: void ListVector::Reserve(Vector &vector, idx_t required_capacity) {
1343: 	D_ASSERT(vector.GetType().id() == LogicalTypeId::LIST);
1344: 	D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR ||
1345: 	         vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
1346: 	D_ASSERT(vector.auxiliary);
1347: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::LIST_BUFFER);
1348: 	auto &child_buffer = *((VectorListBuffer *)vector.auxiliary.get());
1349: 	child_buffer.Reserve(required_capacity);
1350: }
1351: 
1352: template <class T>
1353: void TemplatedSearchInMap(Vector &list, T key, vector<idx_t> &offsets, bool is_key_null, idx_t offset, idx_t length) {
1354: 	auto &list_vector = ListVector::GetEntry(list);
1355: 	VectorData vector_data;
1356: 	list_vector.Orrify(ListVector::GetListSize(list), vector_data);
1357: 	auto data = (T *)vector_data.data;
1358: 	auto validity_mask = vector_data.validity;
1359: 
1360: 	if (is_key_null) {
1361: 		for (idx_t i = offset; i < offset + length; i++) {
1362: 			if (!validity_mask.RowIsValid(i)) {
1363: 				offsets.push_back(i);
1364: 			}
1365: 		}
1366: 	} else {
1367: 		for (idx_t i = offset; i < offset + length; i++) {
1368: 			if (!validity_mask.RowIsValid(i)) {
1369: 				continue;
1370: 			}
1371: 			if (key == data[i]) {
1372: 				offsets.push_back(i);
1373: 			}
1374: 		}
1375: 	}
1376: }
1377: 
1378: template <class T>
1379: void TemplatedSearchInMap(Vector &list, const Value &key, vector<idx_t> &offsets, bool is_key_null, idx_t offset,
1380:                           idx_t length) {
1381: 	TemplatedSearchInMap<T>(list, key.template GetValueUnsafe<T>(), offsets, is_key_null, offset, length);
1382: }
1383: 
1384: void SearchStringInMap(Vector &list, const string &key, vector<idx_t> &offsets, bool is_key_null, idx_t offset,
1385:                        idx_t length) {
1386: 	auto &list_vector = ListVector::GetEntry(list);
1387: 	VectorData vector_data;
1388: 	list_vector.Orrify(ListVector::GetListSize(list), vector_data);
1389: 	auto data = (string_t *)vector_data.data;
1390: 	auto validity_mask = vector_data.validity;
1391: 	if (is_key_null) {
1392: 		for (idx_t i = offset; i < offset + length; i++) {
1393: 			if (!validity_mask.RowIsValid(i)) {
1394: 				offsets.push_back(i);
1395: 			}
1396: 		}
1397: 	} else {
1398: 		string_t key_str_t(key);
1399: 		for (idx_t i = offset; i < offset + length; i++) {
1400: 			if (!validity_mask.RowIsValid(i)) {
1401: 				continue;
1402: 			}
1403: 			if (Equals::Operation<string_t>(data[i], key_str_t)) {
1404: 				offsets.push_back(i);
1405: 			}
1406: 		}
1407: 	}
1408: }
1409: 
1410: vector<idx_t> ListVector::Search(Vector &list, const Value &key, idx_t row) {
1411: 	vector<idx_t> offsets;
1412: 
1413: 	auto &list_vector = ListVector::GetEntry(list);
1414: 	auto &entry = ((list_entry_t *)list.GetData())[row];
1415: 
1416: 	switch (list_vector.GetType().InternalType()) {
1417: 	case PhysicalType::BOOL:
1418: 	case PhysicalType::INT8:
1419: 		TemplatedSearchInMap<int8_t>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1420: 		break;
1421: 	case PhysicalType::INT16:
1422: 		TemplatedSearchInMap<int16_t>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1423: 		break;
1424: 	case PhysicalType::INT32:
1425: 		TemplatedSearchInMap<int32_t>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1426: 		break;
1427: 	case PhysicalType::INT64:
1428: 		TemplatedSearchInMap<int64_t>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1429: 		break;
1430: 	case PhysicalType::INT128:
1431: 		TemplatedSearchInMap<hugeint_t>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1432: 		break;
1433: 	case PhysicalType::UINT8:
1434: 		TemplatedSearchInMap<uint8_t>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1435: 		break;
1436: 	case PhysicalType::UINT16:
1437: 		TemplatedSearchInMap<uint16_t>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1438: 		break;
1439: 	case PhysicalType::UINT32:
1440: 		TemplatedSearchInMap<uint32_t>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1441: 		break;
1442: 	case PhysicalType::UINT64:
1443: 		TemplatedSearchInMap<uint64_t>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1444: 		break;
1445: 	case PhysicalType::FLOAT:
1446: 		TemplatedSearchInMap<float>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1447: 		break;
1448: 	case PhysicalType::DOUBLE:
1449: 		TemplatedSearchInMap<double>(list, key, offsets, key.IsNull(), entry.offset, entry.length);
1450: 		break;
1451: 	case PhysicalType::VARCHAR:
1452: 		SearchStringInMap(list, StringValue::Get(key), offsets, key.IsNull(), entry.offset, entry.length);
1453: 		break;
1454: 	default:
1455: 		throw InvalidTypeException(list.GetType().id(), "Invalid type for List Vector Search");
1456: 	}
1457: 	return offsets;
1458: }
1459: 
1460: Value ListVector::GetValuesFromOffsets(Vector &list, vector<idx_t> &offsets) {
1461: 	auto &child_vec = ListVector::GetEntry(list);
1462: 	vector<Value> list_values;
1463: 	list_values.reserve(offsets.size());
1464: 	for (auto &offset : offsets) {
1465: 		list_values.push_back(child_vec.GetValue(offset));
1466: 	}
1467: 	return Value::LIST(ListType::GetChildType(list.GetType()), move(list_values));
1468: }
1469: 
1470: idx_t ListVector::GetListSize(const Vector &vec) {
1471: 	if (vec.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
1472: 		auto &child = DictionaryVector::Child(vec);
1473: 		return ListVector::GetListSize(child);
1474: 	}
1475: 	D_ASSERT(vec.auxiliary);
1476: 	return ((VectorListBuffer &)*vec.auxiliary).size;
1477: }
1478: 
1479: void ListVector::ReferenceEntry(Vector &vector, Vector &other) {
1480: 	D_ASSERT(vector.GetType().id() == LogicalTypeId::LIST);
1481: 	D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR ||
1482: 	         vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
1483: 	D_ASSERT(other.GetType().id() == LogicalTypeId::LIST);
1484: 	D_ASSERT(other.GetVectorType() == VectorType::FLAT_VECTOR || other.GetVectorType() == VectorType::CONSTANT_VECTOR);
1485: 	vector.auxiliary = other.auxiliary;
1486: }
1487: 
1488: void ListVector::SetListSize(Vector &vec, idx_t size) {
1489: 	if (vec.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
1490: 		auto &child = DictionaryVector::Child(vec);
1491: 		ListVector::SetListSize(child, size);
1492: 	}
1493: 	((VectorListBuffer &)*vec.auxiliary).size = size;
1494: }
1495: 
1496: void ListVector::Append(Vector &target, const Vector &source, idx_t source_size, idx_t source_offset) {
1497: 	if (source_size - source_offset == 0) {
1498: 		//! Nothing to add
1499: 		return;
1500: 	}
1501: 	auto &target_buffer = (VectorListBuffer &)*target.auxiliary;
1502: 	target_buffer.Append(source, source_size, source_offset);
1503: }
1504: 
1505: void ListVector::Append(Vector &target, const Vector &source, const SelectionVector &sel, idx_t source_size,
1506:                         idx_t source_offset) {
1507: 	if (source_size - source_offset == 0) {
1508: 		//! Nothing to add
1509: 		return;
1510: 	}
1511: 	auto &target_buffer = (VectorListBuffer &)*target.auxiliary;
1512: 	target_buffer.Append(source, sel, source_size, source_offset);
1513: }
1514: 
1515: void ListVector::PushBack(Vector &target, const Value &insert) {
1516: 	auto &target_buffer = (VectorListBuffer &)*target.auxiliary;
1517: 	target_buffer.PushBack(insert);
1518: }
1519: 
1520: } // namespace duckdb
[end of src/common/types/vector.cpp]
[start of src/execution/operator/join/perfect_hash_join_executor.cpp]
1: #include "duckdb/execution/operator/join/perfect_hash_join_executor.hpp"
2: 
3: #include "duckdb/common/types/row_layout.hpp"
4: #include "duckdb/execution/operator/join/physical_hash_join.hpp"
5: 
6: namespace duckdb {
7: 
8: PerfectHashJoinExecutor::PerfectHashJoinExecutor(const PhysicalHashJoin &join_p, JoinHashTable &ht_p,
9:                                                  PerfectHashJoinStats perfect_join_stats)
10:     : join(join_p), ht(ht_p), perfect_join_statistics(move(perfect_join_stats)) {
11: }
12: 
13: bool PerfectHashJoinExecutor::CanDoPerfectHashJoin() {
14: 	return perfect_join_statistics.is_build_small;
15: }
16: 
17: //===--------------------------------------------------------------------===//
18: // Build
19: //===--------------------------------------------------------------------===//
20: bool PerfectHashJoinExecutor::BuildPerfectHashTable(LogicalType &key_type) {
21: 	// First, allocate memory for each build column
22: 	auto build_size = perfect_join_statistics.build_range + 1;
23: 	for (const auto &type : ht.build_types) {
24: 		perfect_hash_table.emplace_back(type, build_size);
25: 	}
26: 	// and for duplicate_checking
27: 	bitmap_build_idx = unique_ptr<bool[]>(new bool[build_size]);
28: 	memset(bitmap_build_idx.get(), 0, sizeof(bool) * build_size); // set false
29: 
30: 	// Now fill columns with build data
31: 	JoinHTScanState join_ht_state;
32: 	return FullScanHashTable(join_ht_state, key_type);
33: }
34: 
35: bool PerfectHashJoinExecutor::FullScanHashTable(JoinHTScanState &state, LogicalType &key_type) {
36: 	Vector tuples_addresses(LogicalType::POINTER, ht.Count());              // allocate space for all the tuples
37: 	auto key_locations = FlatVector::GetData<data_ptr_t>(tuples_addresses); // get a pointer to vector data
38: 	// TODO: In a parallel finalize: One should exclusively lock and each thread should do one part of the code below.
39: 	// Go through all the blocks and fill the keys addresses
40: 	auto keys_count = ht.FillWithHTOffsets(key_locations, state);
41: 	// Scan the build keys in the hash table
42: 	Vector build_vector(key_type, keys_count);
43: 	RowOperations::FullScanColumn(ht.layout, tuples_addresses, build_vector, keys_count, 0);
44: 	// Now fill the selection vector using the build keys and create a sequential vector
45: 	// todo: add check for fast pass when probe is part of build domain
46: 	SelectionVector sel_build(keys_count + 1);
47: 	SelectionVector sel_tuples(keys_count + 1);
48: 	bool success = FillSelectionVectorSwitchBuild(build_vector, sel_build, sel_tuples, keys_count);
49: 	// early out
50: 	if (!success) {
51: 		return false;
52: 	}
53: 	if (unique_keys == perfect_join_statistics.build_range + 1 && !ht.has_null) {
54: 		perfect_join_statistics.is_build_dense = true;
55: 	}
56: 	keys_count = unique_keys; // do not consider keys out of the range
57: 	// Full scan the remaining build columns and fill the perfect hash table
58: 	for (idx_t i = 0; i < ht.build_types.size(); i++) {
59: 		auto build_size = perfect_join_statistics.build_range + 1;
60: 		auto &vector = perfect_hash_table[i];
61: 		D_ASSERT(vector.GetType() == ht.build_types[i]);
62: 		const auto col_no = ht.condition_types.size() + i;
63: 		const auto col_offset = ht.layout.GetOffsets()[col_no];
64: 		RowOperations::Gather(tuples_addresses, sel_tuples, vector, sel_build, keys_count, col_offset, col_no,
65: 		                      build_size);
66: 	}
67: 	return true;
68: }
69: 
70: bool PerfectHashJoinExecutor::FillSelectionVectorSwitchBuild(Vector &source, SelectionVector &sel_vec,
71:                                                              SelectionVector &seq_sel_vec, idx_t count) {
72: 	switch (source.GetType().InternalType()) {
73: 	case PhysicalType::INT8:
74: 		return TemplatedFillSelectionVectorBuild<int8_t>(source, sel_vec, seq_sel_vec, count);
75: 	case PhysicalType::INT16:
76: 		return TemplatedFillSelectionVectorBuild<int16_t>(source, sel_vec, seq_sel_vec, count);
77: 	case PhysicalType::INT32:
78: 		return TemplatedFillSelectionVectorBuild<int32_t>(source, sel_vec, seq_sel_vec, count);
79: 	case PhysicalType::INT64:
80: 		return TemplatedFillSelectionVectorBuild<int64_t>(source, sel_vec, seq_sel_vec, count);
81: 	case PhysicalType::UINT8:
82: 		return TemplatedFillSelectionVectorBuild<uint8_t>(source, sel_vec, seq_sel_vec, count);
83: 	case PhysicalType::UINT16:
84: 		return TemplatedFillSelectionVectorBuild<uint16_t>(source, sel_vec, seq_sel_vec, count);
85: 	case PhysicalType::UINT32:
86: 		return TemplatedFillSelectionVectorBuild<uint32_t>(source, sel_vec, seq_sel_vec, count);
87: 	case PhysicalType::UINT64:
88: 		return TemplatedFillSelectionVectorBuild<uint64_t>(source, sel_vec, seq_sel_vec, count);
89: 	default:
90: 		throw NotImplementedException("Type not supported for perfect hash join");
91: 	}
92: }
93: 
94: template <typename T>
95: bool PerfectHashJoinExecutor::TemplatedFillSelectionVectorBuild(Vector &source, SelectionVector &sel_vec,
96:                                                                 SelectionVector &seq_sel_vec, idx_t count) {
97: 	if (perfect_join_statistics.build_min.IsNull() || perfect_join_statistics.build_max.IsNull()) {
98: 		return false;
99: 	}
100: 	auto min_value = perfect_join_statistics.build_min.GetValueUnsafe<T>();
101: 	auto max_value = perfect_join_statistics.build_max.GetValueUnsafe<T>();
102: 	VectorData vector_data;
103: 	source.Orrify(count, vector_data);
104: 	auto data = reinterpret_cast<T *>(vector_data.data);
105: 	// generate the selection vector
106: 	for (idx_t i = 0, sel_idx = 0; i < count; ++i) {
107: 		auto data_idx = vector_data.sel->get_index(i);
108: 		auto input_value = data[data_idx];
109: 		// add index to selection vector if value in the range
110: 		if (min_value <= input_value && input_value <= max_value) {
111: 			auto idx = (idx_t)(input_value - min_value); // subtract min value to get the idx position
112: 			sel_vec.set_index(sel_idx, idx);
113: 			if (bitmap_build_idx[idx]) {
114: 				return false;
115: 			} else {
116: 				bitmap_build_idx[idx] = true;
117: 				unique_keys++;
118: 			}
119: 			seq_sel_vec.set_index(sel_idx++, i);
120: 		}
121: 	}
122: 	return true;
123: }
124: 
125: //===--------------------------------------------------------------------===//
126: // Probe
127: //===--------------------------------------------------------------------===//
128: class PerfectHashJoinState : public OperatorState {
129: public:
130: 	DataChunk join_keys;
131: 	ExpressionExecutor probe_executor;
132: 	SelectionVector build_sel_vec;
133: 	SelectionVector probe_sel_vec;
134: 	SelectionVector seq_sel_vec;
135: };
136: 
137: unique_ptr<OperatorState> PerfectHashJoinExecutor::GetOperatorState(ClientContext &context) {
138: 	auto state = make_unique<PerfectHashJoinState>();
139: 	state->join_keys.Initialize(join.condition_types);
140: 	for (auto &cond : join.conditions) {
141: 		state->probe_executor.AddExpression(*cond.left);
142: 	}
143: 	state->build_sel_vec.Initialize(STANDARD_VECTOR_SIZE);
144: 	state->probe_sel_vec.Initialize(STANDARD_VECTOR_SIZE);
145: 	state->seq_sel_vec.Initialize(STANDARD_VECTOR_SIZE);
146: 	return move(state);
147: }
148: 
149: OperatorResultType PerfectHashJoinExecutor::ProbePerfectHashTable(ExecutionContext &context, DataChunk &input,
150:                                                                   DataChunk &result, OperatorState &state_p) {
151: 	auto &state = (PerfectHashJoinState &)state_p;
152: 	// keeps track of how many probe keys have a match
153: 	idx_t probe_sel_count = 0;
154: 
155: 	// fetch the join keys from the chunk
156: 	state.probe_executor.Execute(input, state.join_keys);
157: 	// select the keys that are in the min-max range
158: 	auto &keys_vec = state.join_keys.data[0];
159: 	auto keys_count = state.join_keys.size();
160: 	// todo: add check for fast pass when probe is part of build domain
161: 	FillSelectionVectorSwitchProbe(keys_vec, state.build_sel_vec, state.probe_sel_vec, keys_count, probe_sel_count);
162: 
163: 	// If build is dense and probe is in build's domain, just reference probe
164: 	if (perfect_join_statistics.is_build_dense && keys_count == probe_sel_count) {
165: 		result.Reference(input);
166: 	} else {
167: 		// otherwise, filter it out the values that do not match
168: 		result.Slice(input, state.probe_sel_vec, probe_sel_count, 0);
169: 	}
170: 	// on the build side, we need to fetch the data and build dictionary vectors with the sel_vec
171: 	for (idx_t i = 0; i < ht.build_types.size(); i++) {
172: 		auto &result_vector = result.data[input.ColumnCount() + i];
173: 		D_ASSERT(result_vector.GetType() == ht.build_types[i]);
174: 		auto &build_vec = perfect_hash_table[i];
175: 		result_vector.Reference(build_vec);
176: 		result_vector.Slice(state.build_sel_vec, probe_sel_count);
177: 	}
178: 	return OperatorResultType::NEED_MORE_INPUT;
179: }
180: 
181: void PerfectHashJoinExecutor::FillSelectionVectorSwitchProbe(Vector &source, SelectionVector &build_sel_vec,
182:                                                              SelectionVector &probe_sel_vec, idx_t count,
183:                                                              idx_t &probe_sel_count) {
184: 	switch (source.GetType().InternalType()) {
185: 	case PhysicalType::INT8:
186: 		TemplatedFillSelectionVectorProbe<int8_t>(source, build_sel_vec, probe_sel_vec, count, probe_sel_count);
187: 		break;
188: 	case PhysicalType::INT16:
189: 		TemplatedFillSelectionVectorProbe<int16_t>(source, build_sel_vec, probe_sel_vec, count, probe_sel_count);
190: 		break;
191: 	case PhysicalType::INT32:
192: 		TemplatedFillSelectionVectorProbe<int32_t>(source, build_sel_vec, probe_sel_vec, count, probe_sel_count);
193: 		break;
194: 	case PhysicalType::INT64:
195: 		TemplatedFillSelectionVectorProbe<int64_t>(source, build_sel_vec, probe_sel_vec, count, probe_sel_count);
196: 		break;
197: 	case PhysicalType::UINT8:
198: 		TemplatedFillSelectionVectorProbe<uint8_t>(source, build_sel_vec, probe_sel_vec, count, probe_sel_count);
199: 		break;
200: 	case PhysicalType::UINT16:
201: 		TemplatedFillSelectionVectorProbe<uint16_t>(source, build_sel_vec, probe_sel_vec, count, probe_sel_count);
202: 		break;
203: 	case PhysicalType::UINT32:
204: 		TemplatedFillSelectionVectorProbe<uint32_t>(source, build_sel_vec, probe_sel_vec, count, probe_sel_count);
205: 		break;
206: 	case PhysicalType::UINT64:
207: 		TemplatedFillSelectionVectorProbe<uint64_t>(source, build_sel_vec, probe_sel_vec, count, probe_sel_count);
208: 		break;
209: 	default:
210: 		throw NotImplementedException("Type not supported");
211: 	}
212: }
213: 
214: template <typename T>
215: void PerfectHashJoinExecutor::TemplatedFillSelectionVectorProbe(Vector &source, SelectionVector &build_sel_vec,
216:                                                                 SelectionVector &probe_sel_vec, idx_t count,
217:                                                                 idx_t &probe_sel_count) {
218: 	auto min_value = perfect_join_statistics.build_min.GetValueUnsafe<T>();
219: 	auto max_value = perfect_join_statistics.build_max.GetValueUnsafe<T>();
220: 
221: 	VectorData vector_data;
222: 	source.Orrify(count, vector_data);
223: 	auto data = reinterpret_cast<T *>(vector_data.data);
224: 	auto validity_mask = &vector_data.validity;
225: 	// build selection vector for non-dense build
226: 	if (validity_mask->AllValid()) {
227: 		for (idx_t i = 0, sel_idx = 0; i < count; ++i) {
228: 			// retrieve value from vector
229: 			auto data_idx = vector_data.sel->get_index(i);
230: 			auto input_value = data[data_idx];
231: 			// add index to selection vector if value in the range
232: 			if (min_value <= input_value && input_value <= max_value) {
233: 				auto idx = (idx_t)(input_value - min_value); // subtract min value to get the idx position
234: 				                                             // check for matches in the build
235: 				if (bitmap_build_idx[idx]) {
236: 					build_sel_vec.set_index(sel_idx, idx);
237: 					probe_sel_vec.set_index(sel_idx++, i);
238: 					probe_sel_count++;
239: 				}
240: 			}
241: 		}
242: 	} else {
243: 		for (idx_t i = 0, sel_idx = 0; i < count; ++i) {
244: 			// retrieve value from vector
245: 			auto data_idx = vector_data.sel->get_index(i);
246: 			if (!validity_mask->RowIsValid(data_idx)) {
247: 				continue;
248: 			}
249: 			auto input_value = data[data_idx];
250: 			// add index to selection vector if value in the range
251: 			if (min_value <= input_value && input_value <= max_value) {
252: 				auto idx = (idx_t)(input_value - min_value); // subtract min value to get the idx position
253: 				                                             // check for matches in the build
254: 				if (bitmap_build_idx[idx]) {
255: 					build_sel_vec.set_index(sel_idx, idx);
256: 					probe_sel_vec.set_index(sel_idx++, i);
257: 					probe_sel_count++;
258: 				}
259: 			}
260: 		}
261: 	}
262: }
263: 
264: } // namespace duckdb
[end of src/execution/operator/join/perfect_hash_join_executor.cpp]
[start of src/execution/operator/join/physical_nested_loop_join.cpp]
1: #include "duckdb/execution/operator/join/physical_nested_loop_join.hpp"
2: #include "duckdb/parallel/thread_context.hpp"
3: #include "duckdb/common/operator/comparison_operators.hpp"
4: #include "duckdb/common/vector_operations/vector_operations.hpp"
5: #include "duckdb/execution/expression_executor.hpp"
6: #include "duckdb/execution/nested_loop_join.hpp"
7: #include "duckdb/main/client_context.hpp"
8: 
9: namespace duckdb {
10: 
11: PhysicalNestedLoopJoin::PhysicalNestedLoopJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left,
12:                                                unique_ptr<PhysicalOperator> right, vector<JoinCondition> cond,
13:                                                JoinType join_type, idx_t estimated_cardinality)
14:     : PhysicalComparisonJoin(op, PhysicalOperatorType::NESTED_LOOP_JOIN, move(cond), join_type, estimated_cardinality) {
15: 	children.push_back(move(left));
16: 	children.push_back(move(right));
17: }
18: 
19: static bool HasNullValues(DataChunk &chunk) {
20: 	for (idx_t col_idx = 0; col_idx < chunk.ColumnCount(); col_idx++) {
21: 		VectorData vdata;
22: 		chunk.data[col_idx].Orrify(chunk.size(), vdata);
23: 
24: 		if (vdata.validity.AllValid()) {
25: 			continue;
26: 		}
27: 		for (idx_t i = 0; i < chunk.size(); i++) {
28: 			auto idx = vdata.sel->get_index(i);
29: 			if (!vdata.validity.RowIsValid(idx)) {
30: 				return true;
31: 			}
32: 		}
33: 	}
34: 	return false;
35: }
36: 
37: template <bool MATCH>
38: static void ConstructSemiOrAntiJoinResult(DataChunk &left, DataChunk &result, bool found_match[]) {
39: 	D_ASSERT(left.ColumnCount() == result.ColumnCount());
40: 	// create the selection vector from the matches that were found
41: 	idx_t result_count = 0;
42: 	SelectionVector sel(STANDARD_VECTOR_SIZE);
43: 	for (idx_t i = 0; i < left.size(); i++) {
44: 		if (found_match[i] == MATCH) {
45: 			sel.set_index(result_count++, i);
46: 		}
47: 	}
48: 	// construct the final result
49: 	if (result_count > 0) {
50: 		// we only return the columns on the left side
51: 		// project them using the result selection vector
52: 		// reference the columns of the left side from the result
53: 		result.Slice(left, sel, result_count);
54: 	} else {
55: 		result.SetCardinality(0);
56: 	}
57: }
58: 
59: void PhysicalJoin::ConstructSemiJoinResult(DataChunk &left, DataChunk &result, bool found_match[]) {
60: 	ConstructSemiOrAntiJoinResult<true>(left, result, found_match);
61: }
62: 
63: void PhysicalJoin::ConstructAntiJoinResult(DataChunk &left, DataChunk &result, bool found_match[]) {
64: 	ConstructSemiOrAntiJoinResult<false>(left, result, found_match);
65: }
66: 
67: void PhysicalJoin::ConstructMarkJoinResult(DataChunk &join_keys, DataChunk &left, DataChunk &result, bool found_match[],
68:                                            bool has_null) {
69: 	// for the initial set of columns we just reference the left side
70: 	result.SetCardinality(left);
71: 	for (idx_t i = 0; i < left.ColumnCount(); i++) {
72: 		result.data[i].Reference(left.data[i]);
73: 	}
74: 	auto &mark_vector = result.data.back();
75: 	mark_vector.SetVectorType(VectorType::FLAT_VECTOR);
76: 	// first we set the NULL values from the join keys
77: 	// if there is any NULL in the keys, the result is NULL
78: 	auto bool_result = FlatVector::GetData<bool>(mark_vector);
79: 	auto &mask = FlatVector::Validity(mark_vector);
80: 	for (idx_t col_idx = 0; col_idx < join_keys.ColumnCount(); col_idx++) {
81: 		VectorData jdata;
82: 		join_keys.data[col_idx].Orrify(join_keys.size(), jdata);
83: 		if (!jdata.validity.AllValid()) {
84: 			for (idx_t i = 0; i < join_keys.size(); i++) {
85: 				auto jidx = jdata.sel->get_index(i);
86: 				mask.Set(i, jdata.validity.RowIsValid(jidx));
87: 			}
88: 		}
89: 	}
90: 	// now set the remaining entries to either true or false based on whether a match was found
91: 	if (found_match) {
92: 		for (idx_t i = 0; i < left.size(); i++) {
93: 			bool_result[i] = found_match[i];
94: 		}
95: 	} else {
96: 		memset(bool_result, 0, sizeof(bool) * left.size());
97: 	}
98: 	// if the right side contains NULL values, the result of any FALSE becomes NULL
99: 	if (has_null) {
100: 		for (idx_t i = 0; i < left.size(); i++) {
101: 			if (!bool_result[i]) {
102: 				mask.SetInvalid(i);
103: 			}
104: 		}
105: 	}
106: }
107: 
108: //===--------------------------------------------------------------------===//
109: // Sink
110: //===--------------------------------------------------------------------===//
111: class NestedLoopJoinLocalState : public LocalSinkState {
112: public:
113: 	explicit NestedLoopJoinLocalState(const vector<JoinCondition> &conditions) {
114: 		vector<LogicalType> condition_types;
115: 		for (auto &cond : conditions) {
116: 			rhs_executor.AddExpression(*cond.right);
117: 			condition_types.push_back(cond.right->return_type);
118: 		}
119: 		right_condition.Initialize(condition_types);
120: 	}
121: 
122: 	//! The chunk holding the right condition
123: 	DataChunk right_condition;
124: 	//! The executor of the RHS condition
125: 	ExpressionExecutor rhs_executor;
126: };
127: 
128: class NestedLoopJoinGlobalState : public GlobalSinkState {
129: public:
130: 	NestedLoopJoinGlobalState() : has_null(false) {
131: 	}
132: 
133: 	mutex nj_lock;
134: 	//! Materialized data of the RHS
135: 	ChunkCollection right_data;
136: 	//! Materialized join condition of the RHS
137: 	ChunkCollection right_chunks;
138: 	//! Whether or not the RHS of the nested loop join has NULL values
139: 	bool has_null;
140: 	//! A bool indicating for each tuple in the RHS if they found a match (only used in FULL OUTER JOIN)
141: 	unique_ptr<bool[]> right_found_match;
142: };
143: 
144: SinkResultType PhysicalNestedLoopJoin::Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate,
145:                                             DataChunk &input) const {
146: 	auto &gstate = (NestedLoopJoinGlobalState &)state;
147: 	auto &nlj_state = (NestedLoopJoinLocalState &)lstate;
148: 
149: 	// resolve the join expression of the right side
150: 	nlj_state.right_condition.Reset();
151: 	nlj_state.rhs_executor.Execute(input, nlj_state.right_condition);
152: 
153: 	// if we have not seen any NULL values yet, and we are performing a MARK join, check if there are NULL values in
154: 	// this chunk
155: 	if (join_type == JoinType::MARK && !gstate.has_null) {
156: 		if (HasNullValues(nlj_state.right_condition)) {
157: 			gstate.has_null = true;
158: 		}
159: 	}
160: 
161: 	// append the data and the
162: 	lock_guard<mutex> nj_guard(gstate.nj_lock);
163: 	gstate.right_data.Append(input);
164: 	gstate.right_chunks.Append(nlj_state.right_condition);
165: 	return SinkResultType::NEED_MORE_INPUT;
166: }
167: 
168: void PhysicalNestedLoopJoin::Combine(ExecutionContext &context, GlobalSinkState &gstate, LocalSinkState &lstate) const {
169: 	auto &state = (NestedLoopJoinLocalState &)lstate;
170: 	auto &client_profiler = QueryProfiler::Get(context.client);
171: 
172: 	context.thread.profiler.Flush(this, &state.rhs_executor, "rhs_executor", 1);
173: 	client_profiler.Flush(context.thread.profiler);
174: }
175: 
176: SinkFinalizeType PhysicalNestedLoopJoin::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
177:                                                   GlobalSinkState &gstate_p) const {
178: 	auto &gstate = (NestedLoopJoinGlobalState &)gstate_p;
179: 	if (join_type == JoinType::OUTER || join_type == JoinType::RIGHT) {
180: 		// for FULL/RIGHT OUTER JOIN, initialize found_match to false for every tuple
181: 		gstate.right_found_match = unique_ptr<bool[]>(new bool[gstate.right_data.Count()]);
182: 		memset(gstate.right_found_match.get(), 0, sizeof(bool) * gstate.right_data.Count());
183: 	}
184: 	if (gstate.right_chunks.Count() == 0 && EmptyResultIfRHSIsEmpty()) {
185: 		return SinkFinalizeType::NO_OUTPUT_POSSIBLE;
186: 	}
187: 	return SinkFinalizeType::READY;
188: }
189: 
190: unique_ptr<GlobalSinkState> PhysicalNestedLoopJoin::GetGlobalSinkState(ClientContext &context) const {
191: 	return make_unique<NestedLoopJoinGlobalState>();
192: }
193: 
194: unique_ptr<LocalSinkState> PhysicalNestedLoopJoin::GetLocalSinkState(ExecutionContext &context) const {
195: 	return make_unique<NestedLoopJoinLocalState>(conditions);
196: }
197: 
198: //===--------------------------------------------------------------------===//
199: // Operator
200: //===--------------------------------------------------------------------===//
201: class PhysicalNestedLoopJoinState : public OperatorState {
202: public:
203: 	PhysicalNestedLoopJoinState(const PhysicalNestedLoopJoin &op, const vector<JoinCondition> &conditions)
204: 	    : fetch_next_left(true), fetch_next_right(false), right_chunk(0), left_tuple(0), right_tuple(0) {
205: 		vector<LogicalType> condition_types;
206: 		for (auto &cond : conditions) {
207: 			lhs_executor.AddExpression(*cond.left);
208: 			condition_types.push_back(cond.left->return_type);
209: 		}
210: 		left_condition.Initialize(condition_types);
211: 		if (IsLeftOuterJoin(op.join_type)) {
212: 			left_found_match = unique_ptr<bool[]>(new bool[STANDARD_VECTOR_SIZE]);
213: 			memset(left_found_match.get(), 0, sizeof(bool) * STANDARD_VECTOR_SIZE);
214: 		}
215: 	}
216: 
217: 	bool fetch_next_left;
218: 	bool fetch_next_right;
219: 	idx_t right_chunk;
220: 	DataChunk left_condition;
221: 	//! The executor of the LHS condition
222: 	ExpressionExecutor lhs_executor;
223: 
224: 	idx_t left_tuple;
225: 	idx_t right_tuple;
226: 
227: 	unique_ptr<bool[]> left_found_match;
228: 
229: public:
230: 	void Finalize(PhysicalOperator *op, ExecutionContext &context) override {
231: 		context.thread.profiler.Flush(op, &lhs_executor, "lhs_executor", 0);
232: 	}
233: };
234: 
235: unique_ptr<OperatorState> PhysicalNestedLoopJoin::GetOperatorState(ClientContext &context) const {
236: 	return make_unique<PhysicalNestedLoopJoinState>(*this, conditions);
237: }
238: 
239: OperatorResultType PhysicalNestedLoopJoin::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
240:                                                    OperatorState &state_p) const {
241: 	auto &gstate = (NestedLoopJoinGlobalState &)*sink_state;
242: 
243: 	if (gstate.right_chunks.Count() == 0) {
244: 		// empty RHS
245: 		if (!EmptyResultIfRHSIsEmpty()) {
246: 			ConstructEmptyJoinResult(join_type, gstate.has_null, input, chunk);
247: 			return OperatorResultType::NEED_MORE_INPUT;
248: 		} else {
249: 			return OperatorResultType::FINISHED;
250: 		}
251: 	}
252: 
253: 	switch (join_type) {
254: 	case JoinType::SEMI:
255: 	case JoinType::ANTI:
256: 	case JoinType::MARK:
257: 		// simple joins can have max STANDARD_VECTOR_SIZE matches per chunk
258: 		ResolveSimpleJoin(context, input, chunk, state_p);
259: 		return OperatorResultType::NEED_MORE_INPUT;
260: 	case JoinType::LEFT:
261: 	case JoinType::INNER:
262: 	case JoinType::OUTER:
263: 	case JoinType::RIGHT:
264: 		return ResolveComplexJoin(context, input, chunk, state_p);
265: 	default:
266: 		throw NotImplementedException("Unimplemented type for nested loop join!");
267: 	}
268: }
269: 
270: void PhysicalNestedLoopJoin::ResolveSimpleJoin(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
271:                                                OperatorState &state_p) const {
272: 	auto &state = (PhysicalNestedLoopJoinState &)state_p;
273: 	auto &gstate = (NestedLoopJoinGlobalState &)*sink_state;
274: 
275: 	// resolve the left join condition for the current chunk
276: 	state.lhs_executor.Execute(input, state.left_condition);
277: 
278: 	bool found_match[STANDARD_VECTOR_SIZE] = {false};
279: 	NestedLoopJoinMark::Perform(state.left_condition, gstate.right_chunks, found_match, conditions);
280: 	switch (join_type) {
281: 	case JoinType::MARK:
282: 		// now construct the mark join result from the found matches
283: 		PhysicalJoin::ConstructMarkJoinResult(state.left_condition, input, chunk, found_match, gstate.has_null);
284: 		break;
285: 	case JoinType::SEMI:
286: 		// construct the semi join result from the found matches
287: 		PhysicalJoin::ConstructSemiJoinResult(input, chunk, found_match);
288: 		break;
289: 	case JoinType::ANTI:
290: 		// construct the anti join result from the found matches
291: 		PhysicalJoin::ConstructAntiJoinResult(input, chunk, found_match);
292: 		break;
293: 	default:
294: 		throw NotImplementedException("Unimplemented type for simple nested loop join!");
295: 	}
296: }
297: 
298: void PhysicalJoin::ConstructLeftJoinResult(DataChunk &left, DataChunk &result, bool found_match[]) {
299: 	SelectionVector remaining_sel(STANDARD_VECTOR_SIZE);
300: 	idx_t remaining_count = 0;
301: 	for (idx_t i = 0; i < left.size(); i++) {
302: 		if (!found_match[i]) {
303: 			remaining_sel.set_index(remaining_count++, i);
304: 		}
305: 	}
306: 	if (remaining_count > 0) {
307: 		result.Slice(left, remaining_sel, remaining_count);
308: 		for (idx_t idx = left.ColumnCount(); idx < result.ColumnCount(); idx++) {
309: 			result.data[idx].SetVectorType(VectorType::CONSTANT_VECTOR);
310: 			ConstantVector::SetNull(result.data[idx], true);
311: 		}
312: 	}
313: }
314: 
315: OperatorResultType PhysicalNestedLoopJoin::ResolveComplexJoin(ExecutionContext &context, DataChunk &input,
316:                                                               DataChunk &chunk, OperatorState &state_p) const {
317: 	auto &state = (PhysicalNestedLoopJoinState &)state_p;
318: 	auto &gstate = (NestedLoopJoinGlobalState &)*sink_state;
319: 
320: 	idx_t match_count;
321: 	do {
322: 		if (state.fetch_next_right) {
323: 			// we exhausted the chunk on the right: move to the next chunk on the right
324: 			state.right_chunk++;
325: 			state.left_tuple = 0;
326: 			state.right_tuple = 0;
327: 			state.fetch_next_right = false;
328: 			// check if we exhausted all chunks on the RHS
329: 			if (state.right_chunk >= gstate.right_chunks.ChunkCount()) {
330: 				state.fetch_next_left = true;
331: 				// we exhausted all chunks on the right: move to the next chunk on the left
332: 				if (IsLeftOuterJoin(join_type)) {
333: 					// left join: before we move to the next chunk, see if we need to output any vectors that didn't
334: 					// have a match found
335: 					PhysicalJoin::ConstructLeftJoinResult(input, chunk, state.left_found_match.get());
336: 					memset(state.left_found_match.get(), 0, sizeof(bool) * STANDARD_VECTOR_SIZE);
337: 				}
338: 				return OperatorResultType::NEED_MORE_INPUT;
339: 			}
340: 		}
341: 		if (state.fetch_next_left) {
342: 			// resolve the left join condition for the current chunk
343: 			state.lhs_executor.Execute(input, state.left_condition);
344: 
345: 			state.left_tuple = 0;
346: 			state.right_tuple = 0;
347: 			state.right_chunk = 0;
348: 			state.fetch_next_left = false;
349: 		}
350: 		// now we have a left and a right chunk that we can join together
351: 		// note that we only get here in the case of a LEFT, INNER or FULL join
352: 		auto &left_chunk = input;
353: 		auto &right_chunk = gstate.right_chunks.GetChunk(state.right_chunk);
354: 		auto &right_data = gstate.right_data.GetChunk(state.right_chunk);
355: 
356: 		// sanity check
357: 		left_chunk.Verify();
358: 		right_chunk.Verify();
359: 		right_data.Verify();
360: 
361: 		// now perform the join
362: 		SelectionVector lvector(STANDARD_VECTOR_SIZE), rvector(STANDARD_VECTOR_SIZE);
363: 		match_count = NestedLoopJoinInner::Perform(state.left_tuple, state.right_tuple, state.left_condition,
364: 		                                           right_chunk, lvector, rvector, conditions);
365: 		// we have finished resolving the join conditions
366: 		if (match_count > 0) {
367: 			// we have matching tuples!
368: 			// construct the result
369: 			if (state.left_found_match) {
370: 				for (idx_t i = 0; i < match_count; i++) {
371: 					state.left_found_match[lvector.get_index(i)] = true;
372: 				}
373: 			}
374: 			if (gstate.right_found_match) {
375: 				for (idx_t i = 0; i < match_count; i++) {
376: 					gstate.right_found_match[state.right_chunk * STANDARD_VECTOR_SIZE + rvector.get_index(i)] = true;
377: 				}
378: 			}
379: 			chunk.Slice(input, lvector, match_count);
380: 			chunk.Slice(right_data, rvector, match_count, input.ColumnCount());
381: 		}
382: 
383: 		// check if we exhausted the RHS, if we did we need to move to the next right chunk in the next iteration
384: 		if (state.right_tuple >= right_chunk.size()) {
385: 			state.fetch_next_right = true;
386: 		}
387: 	} while (match_count == 0);
388: 	return OperatorResultType::HAVE_MORE_OUTPUT;
389: }
390: 
391: //===--------------------------------------------------------------------===//
392: // Source
393: //===--------------------------------------------------------------------===//
394: class NestedLoopJoinScanState : public GlobalSourceState {
395: public:
396: 	explicit NestedLoopJoinScanState(const PhysicalNestedLoopJoin &op) : op(op), right_outer_position(0) {
397: 	}
398: 
399: 	mutex lock;
400: 	const PhysicalNestedLoopJoin &op;
401: 	idx_t right_outer_position;
402: 
403: public:
404: 	idx_t MaxThreads() override {
405: 		auto &sink = (NestedLoopJoinGlobalState &)*op.sink_state;
406: 		return sink.right_chunks.Count() / (STANDARD_VECTOR_SIZE * 10);
407: 	}
408: };
409: 
410: unique_ptr<GlobalSourceState> PhysicalNestedLoopJoin::GetGlobalSourceState(ClientContext &context) const {
411: 	return make_unique<NestedLoopJoinScanState>(*this);
412: }
413: 
414: void PhysicalNestedLoopJoin::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,
415:                                      LocalSourceState &lstate) const {
416: 	D_ASSERT(IsRightOuterJoin(join_type));
417: 	// check if we need to scan any unmatched tuples from the RHS for the full/right outer join
418: 	auto &sink = (NestedLoopJoinGlobalState &)*sink_state;
419: 	auto &state = (NestedLoopJoinScanState &)gstate;
420: 
421: 	// if the LHS is exhausted in a FULL/RIGHT OUTER JOIN, we scan the found_match for any chunks we
422: 	// still need to output
423: 	lock_guard<mutex> l(state.lock);
424: 	ConstructFullOuterJoinResult(sink.right_found_match.get(), sink.right_data, chunk, state.right_outer_position);
425: }
426: 
427: } // namespace duckdb
[end of src/execution/operator/join/physical_nested_loop_join.cpp]
[start of src/function/table/arrow.cpp]
1: #include "duckdb/common/arrow.hpp"
2: 
3: #include "duckdb.hpp"
4: #include "duckdb/common/arrow_wrapper.hpp"
5: #include "duckdb/common/limits.hpp"
6: #include "duckdb/common/to_string.hpp"
7: #include "duckdb/common/types/date.hpp"
8: #include "duckdb/common/types/hugeint.hpp"
9: #include "duckdb/common/types/time.hpp"
10: #include "duckdb/common/types/timestamp.hpp"
11: #include "duckdb/function/table/arrow.hpp"
12: #include "duckdb/function/table_function.hpp"
13: #include "duckdb/main/client_context.hpp"
14: #include "duckdb/main/connection.hpp"
15: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
16: #include "utf8proc_wrapper.hpp"
17: #include "duckdb/common/types/arrow_aux_data.hpp"
18: #include "duckdb/common/types/vector_buffer.hpp"
19: #include "duckdb/common/operator/multiply.hpp"
20: #include "duckdb/common/mutex.hpp"
21: #include <map>
22: 
23: namespace duckdb {
24: 
25: LogicalType GetArrowLogicalType(ArrowSchema &schema,
26:                                 std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,
27:                                 idx_t col_idx) {
28: 	auto format = string(schema.format);
29: 	if (arrow_convert_data.find(col_idx) == arrow_convert_data.end()) {
30: 		arrow_convert_data[col_idx] = make_unique<ArrowConvertData>();
31: 	}
32: 	if (format == "n") {
33: 		return LogicalType::SQLNULL;
34: 	} else if (format == "b") {
35: 		return LogicalType::BOOLEAN;
36: 	} else if (format == "c") {
37: 		return LogicalType::TINYINT;
38: 	} else if (format == "s") {
39: 		return LogicalType::SMALLINT;
40: 	} else if (format == "i") {
41: 		return LogicalType::INTEGER;
42: 	} else if (format == "l") {
43: 		return LogicalType::BIGINT;
44: 	} else if (format == "C") {
45: 		return LogicalType::UTINYINT;
46: 	} else if (format == "S") {
47: 		return LogicalType::USMALLINT;
48: 	} else if (format == "I") {
49: 		return LogicalType::UINTEGER;
50: 	} else if (format == "L") {
51: 		return LogicalType::UBIGINT;
52: 	} else if (format == "f") {
53: 		return LogicalType::FLOAT;
54: 	} else if (format == "g") {
55: 		return LogicalType::DOUBLE;
56: 	} else if (format[0] == 'd') { //! this can be either decimal128 or decimal 256 (e.g., d:38,0)
57: 		std::string parameters = format.substr(format.find(':'));
58: 		uint8_t width = std::stoi(parameters.substr(1, parameters.find(',')));
59: 		uint8_t scale = std::stoi(parameters.substr(parameters.find(',') + 1));
60: 		if (width > 38) {
61: 			throw NotImplementedException("Unsupported Internal Arrow Type for Decimal %s", format);
62: 		}
63: 		return LogicalType::DECIMAL(width, scale);
64: 	} else if (format == "u") {
65: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::NORMAL, 0);
66: 		return LogicalType::VARCHAR;
67: 	} else if (format == "U") {
68: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::SUPER_SIZE, 0);
69: 		return LogicalType::VARCHAR;
70: 	} else if (format == "tsn:") {
71: 		return LogicalTypeId::TIMESTAMP_NS;
72: 	} else if (format == "tsu:") {
73: 		return LogicalTypeId::TIMESTAMP;
74: 	} else if (format == "tsm:") {
75: 		return LogicalTypeId::TIMESTAMP_MS;
76: 	} else if (format == "tss:") {
77: 		return LogicalTypeId::TIMESTAMP_SEC;
78: 	} else if (format == "tdD") {
79: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::DAYS);
80: 		return LogicalType::DATE;
81: 	} else if (format == "tdm") {
82: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MILLISECONDS);
83: 		return LogicalType::DATE;
84: 	} else if (format == "tts") {
85: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::SECONDS);
86: 		return LogicalType::TIME;
87: 	} else if (format == "ttm") {
88: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MILLISECONDS);
89: 		return LogicalType::TIME;
90: 	} else if (format == "ttu") {
91: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MICROSECONDS);
92: 		return LogicalType::TIME;
93: 	} else if (format == "ttn") {
94: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::NANOSECONDS);
95: 		return LogicalType::TIME;
96: 	} else if (format == "tDs") {
97: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::SECONDS);
98: 		return LogicalType::INTERVAL;
99: 	} else if (format == "tDm") {
100: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MILLISECONDS);
101: 		return LogicalType::INTERVAL;
102: 	} else if (format == "tDu") {
103: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MICROSECONDS);
104: 		return LogicalType::INTERVAL;
105: 	} else if (format == "tDn") {
106: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::NANOSECONDS);
107: 		return LogicalType::INTERVAL;
108: 	} else if (format == "tiD") {
109: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::DAYS);
110: 		return LogicalType::INTERVAL;
111: 	} else if (format == "tiM") {
112: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MONTHS);
113: 		return LogicalType::INTERVAL;
114: 	} else if (format == "+l") {
115: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::NORMAL, 0);
116: 		auto child_type = GetArrowLogicalType(*schema.children[0], arrow_convert_data, col_idx);
117: 		return LogicalType::LIST(child_type);
118: 	} else if (format == "+L") {
119: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::SUPER_SIZE, 0);
120: 		auto child_type = GetArrowLogicalType(*schema.children[0], arrow_convert_data, col_idx);
121: 		return LogicalType::LIST(child_type);
122: 	} else if (format[0] == '+' && format[1] == 'w') {
123: 		std::string parameters = format.substr(format.find(':') + 1);
124: 		idx_t fixed_size = std::stoi(parameters);
125: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::FIXED_SIZE, fixed_size);
126: 		auto child_type = GetArrowLogicalType(*schema.children[0], arrow_convert_data, col_idx);
127: 		return LogicalType::LIST(move(child_type));
128: 	} else if (format == "+s") {
129: 		child_list_t<LogicalType> child_types;
130: 		for (idx_t type_idx = 0; type_idx < (idx_t)schema.n_children; type_idx++) {
131: 			auto child_type = GetArrowLogicalType(*schema.children[type_idx], arrow_convert_data, col_idx);
132: 			child_types.push_back({schema.children[type_idx]->name, child_type});
133: 		}
134: 		return LogicalType::STRUCT(move(child_types));
135: 
136: 	} else if (format == "+m") {
137: 		child_list_t<LogicalType> child_types;
138: 		//! First type will be struct, so we skip it
139: 		auto &struct_schema = *schema.children[0];
140: 		for (idx_t type_idx = 0; type_idx < (idx_t)struct_schema.n_children; type_idx++) {
141: 			//! The other types must be added on lists
142: 			auto child_type = GetArrowLogicalType(*struct_schema.children[type_idx], arrow_convert_data, col_idx);
143: 
144: 			auto list_type = LogicalType::LIST(child_type);
145: 			child_types.push_back({struct_schema.children[type_idx]->name, list_type});
146: 		}
147: 		return LogicalType::MAP(move(child_types));
148: 	} else if (format == "z") {
149: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::NORMAL, 0);
150: 		return LogicalType::BLOB;
151: 	} else if (format == "Z") {
152: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::SUPER_SIZE, 0);
153: 		return LogicalType::BLOB;
154: 	} else if (format[0] == 'w') {
155: 		std::string parameters = format.substr(format.find(':') + 1);
156: 		idx_t fixed_size = std::stoi(parameters);
157: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::FIXED_SIZE, fixed_size);
158: 		return LogicalType::BLOB;
159: 	} else {
160: 		throw NotImplementedException("Unsupported Internal Arrow Type %s", format);
161: 	}
162: }
163: 
164: unique_ptr<FunctionData> ArrowTableFunction::ArrowScanBind(ClientContext &context, vector<Value> &inputs,
165:                                                            named_parameter_map_t &named_parameters,
166:                                                            vector<LogicalType> &input_table_types,
167:                                                            vector<string> &input_table_names,
168:                                                            vector<LogicalType> &return_types, vector<string> &names) {
169: 	typedef unique_ptr<ArrowArrayStreamWrapper> (*stream_factory_produce_t)(
170: 	    uintptr_t stream_factory_ptr,
171: 	    std::pair<std::unordered_map<idx_t, string>, std::vector<string>> & project_columns,
172: 	    TableFilterCollection * filters);
173: 	auto stream_factory_ptr = inputs[0].GetPointer();
174: 	auto stream_factory_produce = (stream_factory_produce_t)inputs[1].GetPointer();
175: 	auto rows_per_thread = inputs[2].GetValue<uint64_t>();
176: 	std::pair<std::unordered_map<idx_t, string>, std::vector<string>> project_columns;
177: #ifndef DUCKDB_NO_THREADS
178: 
179: 	auto res = make_unique<ArrowScanFunctionData>(rows_per_thread, stream_factory_produce, stream_factory_ptr,
180: 	                                              std::this_thread::get_id());
181: #else
182: 	auto res = make_unique<ArrowScanFunctionData>(rows_per_thread, stream_factory_produce, stream_factory_ptr);
183: #endif
184: 	auto &data = *res;
185: 	auto stream = stream_factory_produce(stream_factory_ptr, project_columns, nullptr);
186: 
187: 	data.number_of_rows = stream->number_of_rows;
188: 	if (!stream) {
189: 		throw InvalidInputException("arrow_scan: NULL pointer passed");
190: 	}
191: 
192: 	stream->GetSchema(data.schema_root);
193: 
194: 	for (idx_t col_idx = 0; col_idx < (idx_t)data.schema_root.arrow_schema.n_children; col_idx++) {
195: 		auto &schema = *data.schema_root.arrow_schema.children[col_idx];
196: 		if (!schema.release) {
197: 			throw InvalidInputException("arrow_scan: released schema passed");
198: 		}
199: 		if (schema.dictionary) {
200: 			res->arrow_convert_data[col_idx] =
201: 			    make_unique<ArrowConvertData>(GetArrowLogicalType(schema, res->arrow_convert_data, col_idx));
202: 			return_types.emplace_back(GetArrowLogicalType(*schema.dictionary, res->arrow_convert_data, col_idx));
203: 		} else {
204: 			return_types.emplace_back(GetArrowLogicalType(schema, res->arrow_convert_data, col_idx));
205: 		}
206: 		auto format = string(schema.format);
207: 		auto name = string(schema.name);
208: 		if (name.empty()) {
209: 			name = string("v") + to_string(col_idx);
210: 		}
211: 		names.push_back(name);
212: 	}
213: 	return move(res);
214: }
215: 
216: unique_ptr<ArrowArrayStreamWrapper> ProduceArrowScan(const ArrowScanFunctionData &function,
217:                                                      const vector<column_t> &column_ids,
218:                                                      TableFilterCollection *filters) {
219: 	//! Generate Projection Pushdown Vector
220: 	pair<unordered_map<idx_t, string>, vector<string>> project_columns;
221: 	D_ASSERT(!column_ids.empty());
222: 	for (idx_t idx = 0; idx < column_ids.size(); idx++) {
223: 		auto col_idx = column_ids[idx];
224: 		if (col_idx != COLUMN_IDENTIFIER_ROW_ID) {
225: 			auto &schema = *function.schema_root.arrow_schema.children[col_idx];
226: 			project_columns.first[idx] = schema.name;
227: 			project_columns.second.emplace_back(schema.name);
228: 		}
229: 	}
230: 	return function.scanner_producer(function.stream_factory_ptr, project_columns, filters);
231: }
232: 
233: unique_ptr<FunctionOperatorData> ArrowTableFunction::ArrowScanInit(ClientContext &context,
234:                                                                    const FunctionData *bind_data,
235:                                                                    const vector<column_t> &column_ids,
236:                                                                    TableFilterCollection *filters) {
237: 	auto current_chunk = make_unique<ArrowArrayWrapper>();
238: 	auto result = make_unique<ArrowScanState>(move(current_chunk));
239: 	result->column_ids = column_ids;
240: 	auto &data = (const ArrowScanFunctionData &)*bind_data;
241: 	result->stream = ProduceArrowScan(data, column_ids, filters);
242: 	return move(result);
243: }
244: 
245: void ShiftRight(unsigned char *ar, int size, int shift) {
246: 	int carry = 0;
247: 	while (shift--) {
248: 		for (int i = size - 1; i >= 0; --i) {
249: 			int next = (ar[i] & 1) ? 0x80 : 0;
250: 			ar[i] = carry | (ar[i] >> 1);
251: 			carry = next;
252: 		}
253: 	}
254: }
255: 
256: void SetValidityMask(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size, int64_t nested_offset,
257:                      bool add_null = false) {
258: 	auto &mask = FlatVector::Validity(vector);
259: 	if (array.null_count != 0 && array.buffers[0]) {
260: 		D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);
261: 		auto bit_offset = scan_state.chunk_offset + array.offset;
262: 		if (nested_offset != -1) {
263: 			bit_offset = nested_offset;
264: 		}
265: 		auto n_bitmask_bytes = (size + 8 - 1) / 8;
266: 		mask.EnsureWritable();
267: 		if (bit_offset % 8 == 0) {
268: 			//! just memcpy nullmask
269: 			memcpy((void *)mask.GetData(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes);
270: 		} else {
271: 			//! need to re-align nullmask
272: 			std::vector<uint8_t> temp_nullmask(n_bitmask_bytes + 1);
273: 			memcpy(temp_nullmask.data(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes + 1);
274: 			ShiftRight(temp_nullmask.data(), n_bitmask_bytes + 1,
275: 			           bit_offset % 8); //! why this has to be a right shift is a mystery to me
276: 			memcpy((void *)mask.GetData(), (data_ptr_t)temp_nullmask.data(), n_bitmask_bytes);
277: 		}
278: 	}
279: 	if (add_null) {
280: 		//! We are setting a validity mask of the data part of dictionary vector
281: 		//! For some reason, Nulls are allowed to be indexes, hence we need to set the last element here to be null
282: 		//! We might have to resize the mask
283: 		mask.Resize(size, size + 1);
284: 		mask.SetInvalid(size);
285: 	}
286: }
287: 
288: void GetValidityMask(ValidityMask &mask, ArrowArray &array, ArrowScanState &scan_state, idx_t size) {
289: 	if (array.null_count != 0 && array.buffers[0]) {
290: 		auto bit_offset = scan_state.chunk_offset + array.offset;
291: 		auto n_bitmask_bytes = (size + 8 - 1) / 8;
292: 		mask.EnsureWritable();
293: 		if (bit_offset % 8 == 0) {
294: 			//! just memcpy nullmask
295: 			memcpy((void *)mask.GetData(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes);
296: 		} else {
297: 			//! need to re-align nullmask
298: 			std::vector<uint8_t> temp_nullmask(n_bitmask_bytes + 1);
299: 			memcpy(temp_nullmask.data(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes + 1);
300: 			ShiftRight(temp_nullmask.data(), n_bitmask_bytes + 1,
301: 			           bit_offset % 8); //! why this has to be a right shift is a mystery to me
302: 			memcpy((void *)mask.GetData(), (data_ptr_t)temp_nullmask.data(), n_bitmask_bytes);
303: 		}
304: 	}
305: }
306: 
307: void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,
308:                          std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,
309:                          std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset = -1,
310:                          ValidityMask *parent_mask = nullptr);
311: 
312: void ArrowToDuckDBList(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,
313:                        std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,
314:                        std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset, ValidityMask *parent_mask) {
315: 	auto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.first++];
316: 	idx_t list_size = 0;
317: 	SetValidityMask(vector, array, scan_state, size, nested_offset);
318: 	idx_t start_offset = 0;
319: 	idx_t cur_offset = 0;
320: 	if (original_type.first == ArrowVariableSizeType::FIXED_SIZE) {
321: 		//! Have to check validity mask before setting this up
322: 		idx_t offset = (scan_state.chunk_offset + array.offset) * original_type.second;
323: 		if (nested_offset != -1) {
324: 			offset = original_type.second * nested_offset;
325: 		}
326: 		start_offset = offset;
327: 		auto list_data = FlatVector::GetData<list_entry_t>(vector);
328: 		for (idx_t i = 0; i < size; i++) {
329: 			auto &le = list_data[i];
330: 			le.offset = cur_offset;
331: 			le.length = original_type.second;
332: 			cur_offset += original_type.second;
333: 		}
334: 		list_size = cur_offset;
335: 	} else if (original_type.first == ArrowVariableSizeType::NORMAL) {
336: 		auto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
337: 		if (nested_offset != -1) {
338: 			offsets = (uint32_t *)array.buffers[1] + nested_offset;
339: 		}
340: 		start_offset = offsets[0];
341: 		auto list_data = FlatVector::GetData<list_entry_t>(vector);
342: 		for (idx_t i = 0; i < size; i++) {
343: 			auto &le = list_data[i];
344: 			le.offset = cur_offset;
345: 			le.length = offsets[i + 1] - offsets[i];
346: 			cur_offset += le.length;
347: 		}
348: 		list_size = offsets[size];
349: 	} else {
350: 		auto offsets = (uint64_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
351: 		if (nested_offset != -1) {
352: 			offsets = (uint64_t *)array.buffers[1] + nested_offset;
353: 		}
354: 		start_offset = offsets[0];
355: 		auto list_data = FlatVector::GetData<list_entry_t>(vector);
356: 		for (idx_t i = 0; i < size; i++) {
357: 			auto &le = list_data[i];
358: 			le.offset = cur_offset;
359: 			le.length = offsets[i + 1] - offsets[i];
360: 			cur_offset += le.length;
361: 		}
362: 		list_size = offsets[size];
363: 	}
364: 	list_size -= start_offset;
365: 	ListVector::Reserve(vector, list_size);
366: 	ListVector::SetListSize(vector, list_size);
367: 	auto &child_vector = ListVector::GetEntry(vector);
368: 	SetValidityMask(child_vector, *array.children[0], scan_state, list_size, start_offset);
369: 	auto &list_mask = FlatVector::Validity(vector);
370: 	if (parent_mask) {
371: 		//! Since this List is owned by a struct we must guarantee their validity map matches on Null
372: 		if (!parent_mask->AllValid()) {
373: 			for (idx_t i = 0; i < size; i++) {
374: 				if (!parent_mask->RowIsValid(i)) {
375: 					list_mask.SetInvalid(i);
376: 				}
377: 			}
378: 		}
379: 	}
380: 	if (list_size == 0 && start_offset == 0) {
381: 		ColumnArrowToDuckDB(child_vector, *array.children[0], scan_state, list_size, arrow_convert_data, col_idx,
382: 		                    arrow_convert_idx, -1);
383: 	} else {
384: 		ColumnArrowToDuckDB(child_vector, *array.children[0], scan_state, list_size, arrow_convert_data, col_idx,
385: 		                    arrow_convert_idx, start_offset);
386: 	}
387: }
388: 
389: void ArrowToDuckDBBlob(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,
390:                        std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,
391:                        std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset) {
392: 	auto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.first++];
393: 	SetValidityMask(vector, array, scan_state, size, nested_offset);
394: 	if (original_type.first == ArrowVariableSizeType::FIXED_SIZE) {
395: 		//! Have to check validity mask before setting this up
396: 		idx_t offset = (scan_state.chunk_offset + array.offset) * original_type.second;
397: 		if (nested_offset != -1) {
398: 			offset = original_type.second * nested_offset;
399: 		}
400: 		auto cdata = (char *)array.buffers[1];
401: 		for (idx_t row_idx = 0; row_idx < size; row_idx++) {
402: 			if (FlatVector::IsNull(vector, row_idx)) {
403: 				continue;
404: 			}
405: 			auto bptr = cdata + offset;
406: 			auto blob_len = original_type.second;
407: 			FlatVector::GetData<string_t>(vector)[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);
408: 			offset += blob_len;
409: 		}
410: 	} else if (original_type.first == ArrowVariableSizeType::NORMAL) {
411: 		auto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
412: 		if (nested_offset != -1) {
413: 			offsets = (uint32_t *)array.buffers[1] + array.offset + nested_offset;
414: 		}
415: 		auto cdata = (char *)array.buffers[2];
416: 		for (idx_t row_idx = 0; row_idx < size; row_idx++) {
417: 			if (FlatVector::IsNull(vector, row_idx)) {
418: 				continue;
419: 			}
420: 			auto bptr = cdata + offsets[row_idx];
421: 			auto blob_len = offsets[row_idx + 1] - offsets[row_idx];
422: 			FlatVector::GetData<string_t>(vector)[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);
423: 		}
424: 	} else {
425: 		//! Check if last offset is higher than max uint32
426: 		if (((uint64_t *)array.buffers[1])[array.length] > NumericLimits<uint32_t>::Maximum()) { // LCOV_EXCL_START
427: 			throw std::runtime_error("DuckDB does not support Blobs over 4GB");
428: 		} // LCOV_EXCL_STOP
429: 		auto offsets = (uint64_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
430: 		if (nested_offset != -1) {
431: 			offsets = (uint64_t *)array.buffers[1] + array.offset + nested_offset;
432: 		}
433: 		auto cdata = (char *)array.buffers[2];
434: 		for (idx_t row_idx = 0; row_idx < size; row_idx++) {
435: 			if (FlatVector::IsNull(vector, row_idx)) {
436: 				continue;
437: 			}
438: 			auto bptr = cdata + offsets[row_idx];
439: 			auto blob_len = offsets[row_idx + 1] - offsets[row_idx];
440: 			FlatVector::GetData<string_t>(vector)[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);
441: 		}
442: 	}
443: }
444: 
445: void ArrowToDuckDBMapList(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,
446:                           std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,
447:                           std::pair<idx_t, idx_t> &arrow_convert_idx, uint32_t *offsets, ValidityMask *parent_mask) {
448: 	idx_t list_size = offsets[size] - offsets[0];
449: 	ListVector::Reserve(vector, list_size);
450: 
451: 	auto &child_vector = ListVector::GetEntry(vector);
452: 	auto list_data = FlatVector::GetData<list_entry_t>(vector);
453: 	auto cur_offset = 0;
454: 	for (idx_t i = 0; i < size; i++) {
455: 		auto &le = list_data[i];
456: 		le.offset = cur_offset;
457: 		le.length = offsets[i + 1] - offsets[i];
458: 		cur_offset += le.length;
459: 	}
460: 	ListVector::SetListSize(vector, list_size);
461: 	if (list_size == 0 && offsets[0] == 0) {
462: 		SetValidityMask(child_vector, array, scan_state, list_size, -1);
463: 	} else {
464: 		SetValidityMask(child_vector, array, scan_state, list_size, offsets[0]);
465: 	}
466: 
467: 	auto &list_mask = FlatVector::Validity(vector);
468: 	if (parent_mask) {
469: 		//! Since this List is owned by a struct we must guarantee their validity map matches on Null
470: 		if (!parent_mask->AllValid()) {
471: 			for (idx_t i = 0; i < size; i++) {
472: 				if (!parent_mask->RowIsValid(i)) {
473: 					list_mask.SetInvalid(i);
474: 				}
475: 			}
476: 		}
477: 	}
478: 	if (list_size == 0 && offsets[0] == 0) {
479: 		ColumnArrowToDuckDB(child_vector, array, scan_state, list_size, arrow_convert_data, col_idx, arrow_convert_idx,
480: 		                    -1);
481: 	} else {
482: 		ColumnArrowToDuckDB(child_vector, array, scan_state, list_size, arrow_convert_data, col_idx, arrow_convert_idx,
483: 		                    offsets[0]);
484: 	}
485: }
486: template <class T>
487: static void SetVectorString(Vector &vector, idx_t size, char *cdata, T *offsets) {
488: 	auto strings = FlatVector::GetData<string_t>(vector);
489: 	for (idx_t row_idx = 0; row_idx < size; row_idx++) {
490: 		if (FlatVector::IsNull(vector, row_idx)) {
491: 			continue;
492: 		}
493: 		auto cptr = cdata + offsets[row_idx];
494: 		auto str_len = offsets[row_idx + 1] - offsets[row_idx];
495: 		strings[row_idx] = string_t(cptr, str_len);
496: 	}
497: }
498: 
499: void DirectConversion(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, int64_t nested_offset) {
500: 	auto internal_type = GetTypeIdSize(vector.GetType().InternalType());
501: 	auto data_ptr = (data_ptr_t)array.buffers[1] + internal_type * (scan_state.chunk_offset + array.offset);
502: 	if (nested_offset != -1) {
503: 		data_ptr = (data_ptr_t)array.buffers[1] + internal_type * (array.offset + nested_offset);
504: 	}
505: 	FlatVector::SetData(vector, data_ptr);
506: }
507: 
508: template <class T>
509: void TimeConversion(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, int64_t nested_offset, idx_t size,
510:                     int64_t conversion) {
511: 	auto tgt_ptr = (dtime_t *)FlatVector::GetData(vector);
512: 	auto &validity_mask = FlatVector::Validity(vector);
513: 	auto src_ptr = (T *)array.buffers[1] + scan_state.chunk_offset + array.offset;
514: 	if (nested_offset != -1) {
515: 		src_ptr = (T *)array.buffers[1] + nested_offset + array.offset;
516: 	}
517: 	for (idx_t row = 0; row < size; row++) {
518: 		if (!validity_mask.RowIsValid(row)) {
519: 			continue;
520: 		}
521: 		if (!TryMultiplyOperator::Operation((int64_t)src_ptr[row], conversion, tgt_ptr[row].micros)) {
522: 			throw ConversionException("Could not convert Interval to Microsecond");
523: 		}
524: 	}
525: }
526: 
527: void IntervalConversionUs(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, int64_t nested_offset,
528:                           idx_t size, int64_t conversion) {
529: 	auto tgt_ptr = (interval_t *)FlatVector::GetData(vector);
530: 	auto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
531: 	if (nested_offset != -1) {
532: 		src_ptr = (int64_t *)array.buffers[1] + nested_offset + array.offset;
533: 	}
534: 	for (idx_t row = 0; row < size; row++) {
535: 		tgt_ptr[row].days = 0;
536: 		tgt_ptr[row].months = 0;
537: 		if (!TryMultiplyOperator::Operation(src_ptr[row], conversion, tgt_ptr[row].micros)) {
538: 			throw ConversionException("Could not convert Interval to Microsecond");
539: 		}
540: 	}
541: }
542: 
543: void IntervalConversionMonths(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, int64_t nested_offset,
544:                               idx_t size) {
545: 	auto tgt_ptr = (interval_t *)FlatVector::GetData(vector);
546: 	auto src_ptr = (int32_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
547: 	if (nested_offset != -1) {
548: 		src_ptr = (int32_t *)array.buffers[1] + nested_offset + array.offset;
549: 	}
550: 	for (idx_t row = 0; row < size; row++) {
551: 		tgt_ptr[row].days = 0;
552: 		tgt_ptr[row].micros = 0;
553: 		tgt_ptr[row].months = src_ptr[row];
554: 	}
555: }
556: 
557: void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,
558:                          std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,
559:                          std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset, ValidityMask *parent_mask) {
560: 	switch (vector.GetType().id()) {
561: 	case LogicalTypeId::SQLNULL:
562: 		vector.Reference(Value());
563: 		break;
564: 	case LogicalTypeId::BOOLEAN: {
565: 		//! Arrow bit-packs boolean values
566: 		//! Lets first figure out where we are in the source array
567: 		auto src_ptr = (uint8_t *)array.buffers[1] + (scan_state.chunk_offset + array.offset) / 8;
568: 
569: 		if (nested_offset != -1) {
570: 			src_ptr = (uint8_t *)array.buffers[1] + (nested_offset + array.offset) / 8;
571: 		}
572: 		auto tgt_ptr = (uint8_t *)FlatVector::GetData(vector);
573: 		int src_pos = 0;
574: 		idx_t cur_bit = scan_state.chunk_offset % 8;
575: 		if (nested_offset != -1) {
576: 			cur_bit = nested_offset % 8;
577: 		}
578: 		for (idx_t row = 0; row < size; row++) {
579: 			if ((src_ptr[src_pos] & (1 << cur_bit)) == 0) {
580: 				tgt_ptr[row] = 0;
581: 			} else {
582: 				tgt_ptr[row] = 1;
583: 			}
584: 			cur_bit++;
585: 			if (cur_bit == 8) {
586: 				src_pos++;
587: 				cur_bit = 0;
588: 			}
589: 		}
590: 		break;
591: 	}
592: 	case LogicalTypeId::TINYINT:
593: 	case LogicalTypeId::SMALLINT:
594: 	case LogicalTypeId::INTEGER:
595: 	case LogicalTypeId::FLOAT:
596: 	case LogicalTypeId::UTINYINT:
597: 	case LogicalTypeId::USMALLINT:
598: 	case LogicalTypeId::UINTEGER:
599: 	case LogicalTypeId::UBIGINT:
600: 	case LogicalTypeId::BIGINT:
601: 	case LogicalTypeId::HUGEINT:
602: 	case LogicalTypeId::TIMESTAMP:
603: 	case LogicalTypeId::TIMESTAMP_SEC:
604: 	case LogicalTypeId::TIMESTAMP_MS:
605: 	case LogicalTypeId::TIMESTAMP_NS: {
606: 		DirectConversion(vector, array, scan_state, nested_offset);
607: 		break;
608: 	}
609: 	case LogicalTypeId::DOUBLE: {
610: 		DirectConversion(vector, array, scan_state, nested_offset);
611: 		//! Need to check if there are NaNs, if yes, must turn that to null
612: 		auto data = (double *)vector.GetData();
613: 		auto &mask = FlatVector::Validity(vector);
614: 		for (idx_t row_idx = 0; row_idx < size; row_idx++) {
615: 			if (!Value::DoubleIsValid(data[row_idx])) {
616: 				mask.SetInvalid(row_idx);
617: 			}
618: 		}
619: 		break;
620: 	}
621: 	case LogicalTypeId::JSON:
622: 	case LogicalTypeId::VARCHAR: {
623: 		auto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.first++];
624: 		auto cdata = (char *)array.buffers[2];
625: 		if (original_type.first == ArrowVariableSizeType::SUPER_SIZE) {
626: 			if (((uint64_t *)array.buffers[1])[array.length] > NumericLimits<uint32_t>::Maximum()) { // LCOV_EXCL_START
627: 				throw std::runtime_error("DuckDB does not support Strings over 4GB");
628: 			} // LCOV_EXCL_STOP
629: 			auto offsets = (uint64_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
630: 			if (nested_offset != -1) {
631: 				offsets = (uint64_t *)array.buffers[1] + array.offset + nested_offset;
632: 			}
633: 			SetVectorString(vector, size, cdata, offsets);
634: 		} else {
635: 			auto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
636: 			if (nested_offset != -1) {
637: 				offsets = (uint32_t *)array.buffers[1] + array.offset + nested_offset;
638: 			}
639: 			SetVectorString(vector, size, cdata, offsets);
640: 		}
641: 		break;
642: 	}
643: 	case LogicalTypeId::DATE: {
644: 		auto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];
645: 		switch (precision) {
646: 		case ArrowDateTimeType::DAYS: {
647: 			DirectConversion(vector, array, scan_state, nested_offset);
648: 			break;
649: 		}
650: 		case ArrowDateTimeType::MILLISECONDS: {
651: 			//! convert date from nanoseconds to days
652: 			auto src_ptr = (uint64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
653: 			if (nested_offset != -1) {
654: 				src_ptr = (uint64_t *)array.buffers[1] + nested_offset + array.offset;
655: 			}
656: 			auto tgt_ptr = (date_t *)FlatVector::GetData(vector);
657: 			for (idx_t row = 0; row < size; row++) {
658: 				tgt_ptr[row] = date_t(int64_t(src_ptr[row]) / (1000 * 60 * 60 * 24));
659: 			}
660: 			break;
661: 		}
662: 		default:
663: 			throw std::runtime_error("Unsupported precision for Date Type ");
664: 		}
665: 		break;
666: 	}
667: 	case LogicalTypeId::TIME: {
668: 		auto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];
669: 		switch (precision) {
670: 		case ArrowDateTimeType::SECONDS: {
671: 			TimeConversion<int32_t>(vector, array, scan_state, nested_offset, size, 1000000);
672: 			break;
673: 		}
674: 		case ArrowDateTimeType::MILLISECONDS: {
675: 			TimeConversion<int32_t>(vector, array, scan_state, nested_offset, size, 1000);
676: 			break;
677: 		}
678: 		case ArrowDateTimeType::MICROSECONDS: {
679: 			TimeConversion<int64_t>(vector, array, scan_state, nested_offset, size, 1);
680: 			break;
681: 		}
682: 		case ArrowDateTimeType::NANOSECONDS: {
683: 			auto tgt_ptr = (dtime_t *)FlatVector::GetData(vector);
684: 			auto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
685: 			if (nested_offset != -1) {
686: 				src_ptr = (int64_t *)array.buffers[1] + nested_offset + array.offset;
687: 			}
688: 			for (idx_t row = 0; row < size; row++) {
689: 				tgt_ptr[row].micros = src_ptr[row] / 1000;
690: 			}
691: 			break;
692: 		}
693: 		default:
694: 			throw std::runtime_error("Unsupported precision for Time Type ");
695: 		}
696: 		break;
697: 	}
698: 	case LogicalTypeId::INTERVAL: {
699: 		auto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];
700: 		switch (precision) {
701: 		case ArrowDateTimeType::SECONDS: {
702: 			IntervalConversionUs(vector, array, scan_state, nested_offset, size, 1000000);
703: 			break;
704: 		}
705: 		case ArrowDateTimeType::DAYS:
706: 		case ArrowDateTimeType::MILLISECONDS: {
707: 			IntervalConversionUs(vector, array, scan_state, nested_offset, size, 1000);
708: 			break;
709: 		}
710: 		case ArrowDateTimeType::MICROSECONDS: {
711: 			IntervalConversionUs(vector, array, scan_state, nested_offset, size, 1);
712: 			break;
713: 		}
714: 		case ArrowDateTimeType::NANOSECONDS: {
715: 			auto tgt_ptr = (interval_t *)FlatVector::GetData(vector);
716: 			auto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
717: 			if (nested_offset != -1) {
718: 				src_ptr = (int64_t *)array.buffers[1] + nested_offset + array.offset;
719: 			}
720: 			for (idx_t row = 0; row < size; row++) {
721: 				tgt_ptr[row].micros = src_ptr[row] / 1000;
722: 				tgt_ptr[row].days = 0;
723: 				tgt_ptr[row].months = 0;
724: 			}
725: 			break;
726: 		}
727: 		case ArrowDateTimeType::MONTHS: {
728: 			IntervalConversionMonths(vector, array, scan_state, nested_offset, size);
729: 			break;
730: 		}
731: 		default:
732: 			throw std::runtime_error("Unsupported precision for Interval/Duration Type ");
733: 		}
734: 		break;
735: 	}
736: 	case LogicalTypeId::DECIMAL: {
737: 		auto val_mask = FlatVector::Validity(vector);
738: 		//! We have to convert from INT128
739: 		auto src_ptr = (hugeint_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
740: 		if (nested_offset != -1) {
741: 			src_ptr = (hugeint_t *)array.buffers[1] + nested_offset + array.offset;
742: 		}
743: 		switch (vector.GetType().InternalType()) {
744: 		case PhysicalType::INT16: {
745: 			auto tgt_ptr = (int16_t *)FlatVector::GetData(vector);
746: 			for (idx_t row = 0; row < size; row++) {
747: 				if (val_mask.RowIsValid(row)) {
748: 					auto result = Hugeint::TryCast(src_ptr[row], tgt_ptr[row]);
749: 					D_ASSERT(result);
750: 					(void)result;
751: 				}
752: 			}
753: 			break;
754: 		}
755: 		case PhysicalType::INT32: {
756: 			auto tgt_ptr = (int32_t *)FlatVector::GetData(vector);
757: 			for (idx_t row = 0; row < size; row++) {
758: 				if (val_mask.RowIsValid(row)) {
759: 					auto result = Hugeint::TryCast(src_ptr[row], tgt_ptr[row]);
760: 					D_ASSERT(result);
761: 					(void)result;
762: 				}
763: 			}
764: 			break;
765: 		}
766: 		case PhysicalType::INT64: {
767: 			auto tgt_ptr = (int64_t *)FlatVector::GetData(vector);
768: 			for (idx_t row = 0; row < size; row++) {
769: 				if (val_mask.RowIsValid(row)) {
770: 					auto result = Hugeint::TryCast(src_ptr[row], tgt_ptr[row]);
771: 					D_ASSERT(result);
772: 					(void)result;
773: 				}
774: 			}
775: 			break;
776: 		}
777: 		case PhysicalType::INT128: {
778: 			FlatVector::SetData(vector, (data_ptr_t)array.buffers[1] + GetTypeIdSize(vector.GetType().InternalType()) *
779: 			                                                               (scan_state.chunk_offset + array.offset));
780: 			break;
781: 		}
782: 		default:
783: 			throw std::runtime_error("Unsupported physical type for Decimal: " +
784: 			                         TypeIdToString(vector.GetType().InternalType()));
785: 		}
786: 		break;
787: 	}
788: 	case LogicalTypeId::BLOB: {
789: 		ArrowToDuckDBBlob(vector, array, scan_state, size, arrow_convert_data, col_idx, arrow_convert_idx,
790: 		                  nested_offset);
791: 		break;
792: 	}
793: 	case LogicalTypeId::LIST: {
794: 		ArrowToDuckDBList(vector, array, scan_state, size, arrow_convert_data, col_idx, arrow_convert_idx,
795: 		                  nested_offset, parent_mask);
796: 		break;
797: 	}
798: 	case LogicalTypeId::MAP: {
799: 		//! Since this is a map we skip first child, because its a struct
800: 		auto &struct_arrow = *array.children[0];
801: 		auto &child_entries = StructVector::GetEntries(vector);
802: 		D_ASSERT(child_entries.size() == 2);
803: 		auto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
804: 		if (nested_offset != -1) {
805: 			offsets = (uint32_t *)array.buffers[1] + nested_offset;
806: 		}
807: 		auto &struct_validity_mask = FlatVector::Validity(vector);
808: 		//! Fill the children
809: 		for (idx_t type_idx = 0; type_idx < (idx_t)struct_arrow.n_children; type_idx++) {
810: 			ArrowToDuckDBMapList(*child_entries[type_idx], *struct_arrow.children[type_idx], scan_state, size,
811: 			                     arrow_convert_data, col_idx, arrow_convert_idx, offsets, &struct_validity_mask);
812: 		}
813: 		break;
814: 	}
815: 	case LogicalTypeId::STRUCT: {
816: 		//! Fill the children
817: 		auto &child_entries = StructVector::GetEntries(vector);
818: 		auto &struct_validity_mask = FlatVector::Validity(vector);
819: 		for (idx_t type_idx = 0; type_idx < (idx_t)array.n_children; type_idx++) {
820: 			SetValidityMask(*child_entries[type_idx], *array.children[type_idx], scan_state, size, nested_offset);
821: 			ColumnArrowToDuckDB(*child_entries[type_idx], *array.children[type_idx], scan_state, size,
822: 			                    arrow_convert_data, col_idx, arrow_convert_idx, nested_offset, &struct_validity_mask);
823: 		}
824: 		break;
825: 	}
826: 	default:
827: 		throw std::runtime_error("Unsupported type " + vector.GetType().ToString());
828: 	}
829: }
830: 
831: template <class T>
832: static void SetSelectionVectorLoop(SelectionVector &sel, data_ptr_t indices_p, idx_t size) {
833: 	auto indices = (T *)indices_p;
834: 	for (idx_t row = 0; row < size; row++) {
835: 		sel.set_index(row, indices[row]);
836: 	}
837: }
838: 
839: template <class T>
840: static void SetSelectionVectorLoopWithChecks(SelectionVector &sel, data_ptr_t indices_p, idx_t size) {
841: 
842: 	auto indices = (T *)indices_p;
843: 	for (idx_t row = 0; row < size; row++) {
844: 		if (indices[row] > NumericLimits<uint32_t>::Maximum()) {
845: 			throw std::runtime_error("DuckDB only supports indices that fit on an uint32");
846: 		}
847: 		sel.set_index(row, indices[row]);
848: 	}
849: }
850: 
851: template <class T>
852: static void SetMaskedSelectionVectorLoop(SelectionVector &sel, data_ptr_t indices_p, idx_t size, ValidityMask &mask,
853:                                          idx_t last_element_pos) {
854: 	auto indices = (T *)indices_p;
855: 	for (idx_t row = 0; row < size; row++) {
856: 		if (mask.RowIsValid(row)) {
857: 			sel.set_index(row, indices[row]);
858: 		} else {
859: 			//! Need to point out to last element
860: 			sel.set_index(row, last_element_pos);
861: 		}
862: 	}
863: }
864: 
865: void SetSelectionVector(SelectionVector &sel, data_ptr_t indices_p, LogicalType &logical_type, idx_t size,
866:                         ValidityMask *mask = nullptr, idx_t last_element_pos = 0) {
867: 	sel.Initialize(size);
868: 
869: 	if (mask) {
870: 		switch (logical_type.id()) {
871: 		case LogicalTypeId::UTINYINT:
872: 			SetMaskedSelectionVectorLoop<uint8_t>(sel, indices_p, size, *mask, last_element_pos);
873: 			break;
874: 		case LogicalTypeId::TINYINT:
875: 			SetMaskedSelectionVectorLoop<int8_t>(sel, indices_p, size, *mask, last_element_pos);
876: 			break;
877: 		case LogicalTypeId::USMALLINT:
878: 			SetMaskedSelectionVectorLoop<uint16_t>(sel, indices_p, size, *mask, last_element_pos);
879: 			break;
880: 		case LogicalTypeId::SMALLINT:
881: 			SetMaskedSelectionVectorLoop<int16_t>(sel, indices_p, size, *mask, last_element_pos);
882: 			break;
883: 		case LogicalTypeId::UINTEGER:
884: 			if (last_element_pos > NumericLimits<uint32_t>::Maximum()) {
885: 				//! Its guaranteed that our indices will point to the last element, so just throw an error
886: 				throw std::runtime_error("DuckDB only supports indices that fit on an uint32");
887: 			}
888: 			SetMaskedSelectionVectorLoop<uint32_t>(sel, indices_p, size, *mask, last_element_pos);
889: 			break;
890: 		case LogicalTypeId::INTEGER:
891: 			SetMaskedSelectionVectorLoop<int32_t>(sel, indices_p, size, *mask, last_element_pos);
892: 			break;
893: 		case LogicalTypeId::UBIGINT:
894: 			if (last_element_pos > NumericLimits<uint32_t>::Maximum()) {
895: 				//! Its guaranteed that our indices will point to the last element, so just throw an error
896: 				throw std::runtime_error("DuckDB only supports indices that fit on an uint32");
897: 			}
898: 			SetMaskedSelectionVectorLoop<uint64_t>(sel, indices_p, size, *mask, last_element_pos);
899: 			break;
900: 		case LogicalTypeId::BIGINT:
901: 			if (last_element_pos > NumericLimits<uint32_t>::Maximum()) {
902: 				//! Its guaranteed that our indices will point to the last element, so just throw an error
903: 				throw std::runtime_error("DuckDB only supports indices that fit on an uint32");
904: 			}
905: 			SetMaskedSelectionVectorLoop<int64_t>(sel, indices_p, size, *mask, last_element_pos);
906: 			break;
907: 
908: 		default:
909: 			throw std::runtime_error("(Arrow) Unsupported type for selection vectors " + logical_type.ToString());
910: 		}
911: 
912: 	} else {
913: 		switch (logical_type.id()) {
914: 		case LogicalTypeId::UTINYINT:
915: 			SetSelectionVectorLoop<uint8_t>(sel, indices_p, size);
916: 			break;
917: 		case LogicalTypeId::TINYINT:
918: 			SetSelectionVectorLoop<int8_t>(sel, indices_p, size);
919: 			break;
920: 		case LogicalTypeId::USMALLINT:
921: 			SetSelectionVectorLoop<uint16_t>(sel, indices_p, size);
922: 			break;
923: 		case LogicalTypeId::SMALLINT:
924: 			SetSelectionVectorLoop<int16_t>(sel, indices_p, size);
925: 			break;
926: 		case LogicalTypeId::UINTEGER:
927: 			SetSelectionVectorLoop<uint32_t>(sel, indices_p, size);
928: 			break;
929: 		case LogicalTypeId::INTEGER:
930: 			SetSelectionVectorLoop<int32_t>(sel, indices_p, size);
931: 			break;
932: 		case LogicalTypeId::UBIGINT:
933: 			if (last_element_pos > NumericLimits<uint32_t>::Maximum()) {
934: 				//! We need to check if our indexes fit in a uint32_t
935: 				SetSelectionVectorLoopWithChecks<uint64_t>(sel, indices_p, size);
936: 			} else {
937: 				SetSelectionVectorLoop<uint64_t>(sel, indices_p, size);
938: 			}
939: 			break;
940: 		case LogicalTypeId::BIGINT:
941: 			if (last_element_pos > NumericLimits<uint32_t>::Maximum()) {
942: 				//! We need to check if our indexes fit in a uint32_t
943: 				SetSelectionVectorLoopWithChecks<int64_t>(sel, indices_p, size);
944: 			} else {
945: 				SetSelectionVectorLoop<int64_t>(sel, indices_p, size);
946: 			}
947: 			break;
948: 		default:
949: 			throw std::runtime_error("(Arrow) Unsupported type for selection vectors " + logical_type.ToString());
950: 		}
951: 	}
952: }
953: 
954: void ColumnArrowToDuckDBDictionary(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,
955:                                    std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,
956:                                    idx_t col_idx, std::pair<idx_t, idx_t> &arrow_convert_idx) {
957: 	SelectionVector sel;
958: 	auto &dict_vectors = scan_state.arrow_dictionary_vectors;
959: 	if (dict_vectors.find(col_idx) == dict_vectors.end()) {
960: 		//! We need to set the dictionary data for this column
961: 		auto base_vector = make_unique<Vector>(vector.GetType(), array.dictionary->length);
962: 		SetValidityMask(*base_vector, *array.dictionary, scan_state, array.dictionary->length, 0, array.null_count > 0);
963: 		ColumnArrowToDuckDB(*base_vector, *array.dictionary, scan_state, array.dictionary->length, arrow_convert_data,
964: 		                    col_idx, arrow_convert_idx);
965: 		dict_vectors[col_idx] = move(base_vector);
966: 	}
967: 	auto dictionary_type = arrow_convert_data[col_idx]->dictionary_type;
968: 	//! Get Pointer to Indices of Dictionary
969: 	auto indices = (data_ptr_t)array.buffers[1] +
970: 	               GetTypeIdSize(dictionary_type.InternalType()) * (scan_state.chunk_offset + array.offset);
971: 	if (array.null_count > 0) {
972: 		ValidityMask indices_validity;
973: 		GetValidityMask(indices_validity, array, scan_state, size);
974: 		SetSelectionVector(sel, indices, dictionary_type, size, &indices_validity, array.dictionary->length);
975: 	} else {
976: 		SetSelectionVector(sel, indices, dictionary_type, size);
977: 	}
978: 	vector.Slice(*dict_vectors[col_idx], sel, size);
979: }
980: 
981: void ArrowTableFunction::ArrowToDuckDB(ArrowScanState &scan_state,
982:                                        std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,
983:                                        DataChunk &output, idx_t start) {
984: 	for (idx_t idx = 0; idx < output.ColumnCount(); idx++) {
985: 		auto col_idx = scan_state.column_ids[idx];
986: 		std::pair<idx_t, idx_t> arrow_convert_idx {0, 0};
987: 		auto &array = *scan_state.chunk->arrow_array.children[idx];
988: 		if (!array.release) {
989: 			throw InvalidInputException("arrow_scan: released array passed");
990: 		}
991: 		if (array.length != scan_state.chunk->arrow_array.length) {
992: 			throw InvalidInputException("arrow_scan: array length mismatch");
993: 		}
994: 		output.data[idx].GetBuffer()->SetAuxiliaryData(make_unique<ArrowAuxiliaryData>(scan_state.chunk),
995: 		                                               VectorAuxiliaryDataType::ARROW_AUXILIARY);
996: 		if (array.dictionary) {
997: 			ColumnArrowToDuckDBDictionary(output.data[idx], array, scan_state, output.size(), arrow_convert_data,
998: 			                              col_idx, arrow_convert_idx);
999: 		} else {
1000: 			SetValidityMask(output.data[idx], array, scan_state, output.size(), -1);
1001: 			ColumnArrowToDuckDB(output.data[idx], array, scan_state, output.size(), arrow_convert_data, col_idx,
1002: 			                    arrow_convert_idx);
1003: 		}
1004: 	}
1005: }
1006: 
1007: void ArrowTableFunction::ArrowScanFunction(ClientContext &context, const FunctionData *bind_data,
1008:                                            FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
1009: 
1010: 	auto &data = (ArrowScanFunctionData &)*bind_data;
1011: 	auto &state = (ArrowScanState &)*operator_state;
1012: 
1013: 	//! have we run out of data on the current chunk? move to next one
1014: 	while (state.chunk_offset >= (idx_t)state.chunk->arrow_array.length) {
1015: 		state.chunk_offset = 0;
1016: 		state.arrow_dictionary_vectors.clear();
1017: 		state.chunk = state.stream->GetNextChunk();
1018: 		//! have we run out of chunks? we are done
1019: 		if (!state.chunk->arrow_array.release) {
1020: 			return;
1021: 		}
1022: 	}
1023: 
1024: 	int64_t output_size = MinValue<int64_t>(STANDARD_VECTOR_SIZE, state.chunk->arrow_array.length - state.chunk_offset);
1025: 	data.lines_read += output_size;
1026: 	output.SetCardinality(output_size);
1027: 	ArrowToDuckDB(state, data.arrow_convert_data, output, data.lines_read - output_size);
1028: 	output.Verify();
1029: 	state.chunk_offset += output.size();
1030: }
1031: 
1032: void ArrowTableFunction::ArrowScanFunctionParallel(ClientContext &context, const FunctionData *bind_data,
1033:                                                    FunctionOperatorData *operator_state, DataChunk *input,
1034:                                                    DataChunk &output, ParallelState *parallel_state_p) {
1035: 	auto &data = (ArrowScanFunctionData &)*bind_data;
1036: 	auto &state = (ArrowScanState &)*operator_state;
1037: 	//! Out of tuples in this chunk
1038: 	if (state.chunk_offset >= (idx_t)state.chunk->arrow_array.length) {
1039: 		return;
1040: 	}
1041: 	int64_t output_size = MinValue<int64_t>(STANDARD_VECTOR_SIZE, state.chunk->arrow_array.length - state.chunk_offset);
1042: 	data.lines_read += output_size;
1043: 	output.SetCardinality(output_size);
1044: 	ArrowToDuckDB(state, data.arrow_convert_data, output, data.lines_read - output_size);
1045: 	output.Verify();
1046: 	state.chunk_offset += output.size();
1047: }
1048: 
1049: idx_t ArrowTableFunction::ArrowScanMaxThreads(ClientContext &context, const FunctionData *bind_data_p) {
1050: 	auto &bind_data = (const ArrowScanFunctionData &)*bind_data_p;
1051: 	if (bind_data.number_of_rows <= 0 || ClientConfig::GetConfig(context).verify_parallelism) {
1052: 		return context.db->NumberOfThreads();
1053: 	}
1054: 	return ((bind_data.number_of_rows + bind_data.rows_per_thread - 1) / bind_data.rows_per_thread) + 1;
1055: }
1056: 
1057: unique_ptr<ParallelState> ArrowTableFunction::ArrowScanInitParallelState(ClientContext &context,
1058:                                                                          const FunctionData *bind_data_p,
1059:                                                                          const vector<column_t> &column_ids,
1060:                                                                          TableFilterCollection *filters) {
1061: 	auto &bind_data = (const ArrowScanFunctionData &)*bind_data_p;
1062: 	auto result = make_unique<ParallelArrowScanState>();
1063: 	result->stream = ProduceArrowScan(bind_data, column_ids, filters);
1064: 	return move(result);
1065: }
1066: 
1067: bool ArrowTableFunction::ArrowScanParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,
1068:                                                     FunctionOperatorData *operator_state,
1069:                                                     ParallelState *parallel_state_p) {
1070: 	auto &state = (ArrowScanState &)*operator_state;
1071: 	auto &parallel_state = (ParallelArrowScanState &)*parallel_state_p;
1072: 
1073: 	lock_guard<mutex> parallel_lock(parallel_state.main_mutex);
1074: 	state.chunk_offset = 0;
1075: 
1076: 	auto current_chunk = parallel_state.stream->GetNextChunk();
1077: 	while (current_chunk->arrow_array.length == 0 && current_chunk->arrow_array.release) {
1078: 		current_chunk = parallel_state.stream->GetNextChunk();
1079: 	}
1080: 	state.chunk = move(current_chunk);
1081: 	//! have we run out of chunks? we are done
1082: 	if (!state.chunk->arrow_array.release) {
1083: 		return false;
1084: 	}
1085: 	return true;
1086: }
1087: 
1088: unique_ptr<FunctionOperatorData>
1089: ArrowTableFunction::ArrowScanParallelInit(ClientContext &context, const FunctionData *bind_data_p, ParallelState *state,
1090:                                           const vector<column_t> &column_ids, TableFilterCollection *filters) {
1091: 	auto current_chunk = make_unique<ArrowArrayWrapper>();
1092: 	auto result = make_unique<ArrowScanState>(move(current_chunk));
1093: 	result->column_ids = column_ids;
1094: 	result->filters = filters;
1095: 	ArrowScanParallelStateNext(context, bind_data_p, result.get(), state);
1096: 	return move(result);
1097: }
1098: 
1099: unique_ptr<NodeStatistics> ArrowTableFunction::ArrowScanCardinality(ClientContext &context, const FunctionData *data) {
1100: 	auto &bind_data = (ArrowScanFunctionData &)*data;
1101: 	return make_unique<NodeStatistics>(bind_data.number_of_rows, bind_data.number_of_rows);
1102: }
1103: 
1104: double ArrowTableFunction::ArrowProgress(ClientContext &context, const FunctionData *bind_data_p) {
1105: 	auto &bind_data = (const ArrowScanFunctionData &)*bind_data_p;
1106: 	if (bind_data.number_of_rows == 0) {
1107: 		return 100;
1108: 	}
1109: 	auto percentage = bind_data.lines_read * 100.0 / bind_data.number_of_rows;
1110: 	return percentage;
1111: }
1112: 
1113: void ArrowTableFunction::RegisterFunction(BuiltinFunctions &set) {
1114: 	TableFunctionSet arrow("arrow_scan");
1115: 	arrow.AddFunction(TableFunction({LogicalType::POINTER, LogicalType::POINTER, LogicalType::UBIGINT},
1116: 	                                ArrowScanFunction, ArrowScanBind, ArrowScanInit, nullptr, nullptr, nullptr,
1117: 	                                ArrowScanCardinality, nullptr, nullptr, ArrowScanMaxThreads,
1118: 	                                ArrowScanInitParallelState, ArrowScanFunctionParallel, ArrowScanParallelInit,
1119: 	                                ArrowScanParallelStateNext, true, true, ArrowProgress));
1120: 	set.AddFunction(arrow);
1121: }
1122: 
1123: void BuiltinFunctions::RegisterArrowFunctions() {
1124: 	ArrowTableFunction::RegisterFunction(*this);
1125: }
1126: } // namespace duckdb
[end of src/function/table/arrow.cpp]
[start of src/function/table/checkpoint.cpp]
1: #include "duckdb/function/table/range.hpp"
2: #include "duckdb/main/client_context.hpp"
3: #include "duckdb/storage/storage_manager.hpp"
4: #include "duckdb/transaction/transaction_manager.hpp"
5: 
6: namespace duckdb {
7: 
8: static unique_ptr<FunctionData> CheckpointBind(ClientContext &context, vector<Value> &inputs,
9:                                                named_parameter_map_t &named_parameters,
10:                                                vector<LogicalType> &input_table_types,
11:                                                vector<string> &input_table_names, vector<LogicalType> &return_types,
12:                                                vector<string> &names) {
13: 	return_types.emplace_back(LogicalType::BOOLEAN);
14: 	names.emplace_back("Success");
15: 	return nullptr;
16: }
17: 
18: template <bool FORCE>
19: static void TemplatedCheckpointFunction(ClientContext &context, const FunctionData *bind_data_p,
20:                                         FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
21: 	auto &transaction_manager = TransactionManager::Get(context);
22: 	transaction_manager.Checkpoint(context, FORCE);
23: }
24: 
25: void CheckpointFunction::RegisterFunction(BuiltinFunctions &set) {
26: 	TableFunction checkpoint("checkpoint", {}, TemplatedCheckpointFunction<false>, CheckpointBind);
27: 	set.AddFunction(checkpoint);
28: 	TableFunction force_checkpoint("force_checkpoint", {}, TemplatedCheckpointFunction<true>, CheckpointBind);
29: 	set.AddFunction(force_checkpoint);
30: }
31: 
32: } // namespace duckdb
[end of src/function/table/checkpoint.cpp]
[start of src/function/table/glob.cpp]
1: #include "duckdb/function/table/range.hpp"
2: #include "duckdb/function/table_function.hpp"
3: #include "duckdb/function/function_set.hpp"
4: #include "duckdb/common/file_system.hpp"
5: #include "duckdb/main/config.hpp"
6: 
7: namespace duckdb {
8: 
9: struct GlobFunctionBindData : public TableFunctionData {
10: 	vector<string> files;
11: };
12: 
13: static unique_ptr<FunctionData> GlobFunctionBind(ClientContext &context, vector<Value> &inputs,
14:                                                  named_parameter_map_t &named_parameters,
15:                                                  vector<LogicalType> &input_table_types,
16:                                                  vector<string> &input_table_names, vector<LogicalType> &return_types,
17:                                                  vector<string> &names) {
18: 	auto &config = DBConfig::GetConfig(context);
19: 	if (!config.enable_external_access) {
20: 		throw PermissionException("Globbing is disabled through configuration");
21: 	}
22: 	auto result = make_unique<GlobFunctionBindData>();
23: 	auto &fs = FileSystem::GetFileSystem(context);
24: 	result->files = fs.Glob(StringValue::Get(inputs[0]), context);
25: 	return_types.emplace_back(LogicalType::VARCHAR);
26: 	names.emplace_back("file");
27: 	return move(result);
28: }
29: 
30: struct GlobFunctionState : public FunctionOperatorData {
31: 	GlobFunctionState() : current_idx(0) {
32: 	}
33: 
34: 	idx_t current_idx;
35: };
36: 
37: static unique_ptr<FunctionOperatorData> GlobFunctionInit(ClientContext &context, const FunctionData *bind_data,
38:                                                          const vector<column_t> &column_ids,
39:                                                          TableFilterCollection *filters) {
40: 	return make_unique<GlobFunctionState>();
41: }
42: 
43: static void GlobFunction(ClientContext &context, const FunctionData *bind_data_p, FunctionOperatorData *state_p,
44:                          DataChunk *input, DataChunk &output) {
45: 	auto &bind_data = (GlobFunctionBindData &)*bind_data_p;
46: 	auto &state = (GlobFunctionState &)*state_p;
47: 
48: 	idx_t count = 0;
49: 	idx_t next_idx = MinValue<idx_t>(state.current_idx + STANDARD_VECTOR_SIZE, bind_data.files.size());
50: 	for (; state.current_idx < next_idx; state.current_idx++) {
51: 		output.data[0].SetValue(count, bind_data.files[state.current_idx]);
52: 		count++;
53: 	}
54: 	output.SetCardinality(count);
55: }
56: 
57: void GlobTableFunction::RegisterFunction(BuiltinFunctions &set) {
58: 	TableFunctionSet glob("glob");
59: 	glob.AddFunction(TableFunction({LogicalType::VARCHAR}, GlobFunction, GlobFunctionBind, GlobFunctionInit));
60: 	set.AddFunction(glob);
61: }
62: 
63: } // namespace duckdb
[end of src/function/table/glob.cpp]
[start of src/function/table/pragma_detailed_profiling_output.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
3: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
4: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
5: #include "duckdb/main/query_profiler.hpp"
6: #include "duckdb/main/client_context.hpp"
7: #include "duckdb/common/limits.hpp"
8: namespace duckdb {
9: 
10: struct PragmaDetailedProfilingOutputOperatorData : public FunctionOperatorData {
11: 	explicit PragmaDetailedProfilingOutputOperatorData() : chunk_index(0), initialized(false) {
12: 	}
13: 	idx_t chunk_index;
14: 	bool initialized;
15: };
16: 
17: struct PragmaDetailedProfilingOutputData : public TableFunctionData {
18: 	explicit PragmaDetailedProfilingOutputData(vector<LogicalType> &types) : types(types) {
19: 	}
20: 	unique_ptr<ChunkCollection> collection;
21: 	vector<LogicalType> types;
22: };
23: 
24: static unique_ptr<FunctionData> PragmaDetailedProfilingOutputBind(ClientContext &context, vector<Value> &inputs,
25:                                                                   named_parameter_map_t &named_parameters,
26:                                                                   vector<LogicalType> &input_table_types,
27:                                                                   vector<string> &input_table_names,
28:                                                                   vector<LogicalType> &return_types,
29:                                                                   vector<string> &names) {
30: 	names.emplace_back("OPERATOR_ID");
31: 	return_types.emplace_back(LogicalType::INTEGER);
32: 
33: 	names.emplace_back("ANNOTATION");
34: 	return_types.emplace_back(LogicalType::VARCHAR);
35: 
36: 	names.emplace_back("ID");
37: 	return_types.emplace_back(LogicalType::INTEGER);
38: 
39: 	names.emplace_back("NAME");
40: 	return_types.emplace_back(LogicalType::VARCHAR);
41: 
42: 	names.emplace_back("TIME");
43: 	return_types.emplace_back(LogicalType::DOUBLE);
44: 
45: 	names.emplace_back("CYCLES_PER_TUPLE");
46: 	return_types.emplace_back(LogicalType::DOUBLE);
47: 
48: 	names.emplace_back("SAMPLE_SIZE");
49: 	return_types.emplace_back(LogicalType::INTEGER);
50: 
51: 	names.emplace_back("INPUT_SIZE");
52: 	return_types.emplace_back(LogicalType::INTEGER);
53: 
54: 	names.emplace_back("EXTRA_INFO");
55: 	return_types.emplace_back(LogicalType::VARCHAR);
56: 
57: 	return make_unique<PragmaDetailedProfilingOutputData>(return_types);
58: }
59: 
60: unique_ptr<FunctionOperatorData> PragmaDetailedProfilingOutputInit(ClientContext &context,
61:                                                                    const FunctionData *bind_data,
62:                                                                    const vector<column_t> &column_ids,
63:                                                                    TableFilterCollection *filters) {
64: 	return make_unique<PragmaDetailedProfilingOutputOperatorData>();
65: }
66: 
67: // Insert a row into the given datachunk
68: static void SetValue(DataChunk &output, int index, int op_id, string annotation, int id, string name, double time,
69:                      int sample_counter, int tuple_counter, string extra_info) {
70: 	output.SetValue(0, index, op_id);
71: 	output.SetValue(1, index, move(annotation));
72: 	output.SetValue(2, index, id);
73: 	output.SetValue(3, index, move(name));
74: #if defined(RDTSC)
75: 	output.SetValue(4, index, Value(nullptr));
76: 	output.SetValue(5, index, time);
77: #else
78: 	output.SetValue(4, index, time);
79: 	output.SetValue(5, index, Value(nullptr));
80: 
81: #endif
82: 	output.SetValue(6, index, sample_counter);
83: 	output.SetValue(7, index, tuple_counter);
84: 	output.SetValue(8, index, move(extra_info));
85: }
86: 
87: static void ExtractFunctions(ChunkCollection &collection, ExpressionInfo &info, DataChunk &chunk, int op_id,
88:                              int &fun_id) {
89: 	if (info.hasfunction) {
90: 		D_ASSERT(info.sample_tuples_count != 0);
91: 		SetValue(chunk, chunk.size(), op_id, "Function", fun_id++, info.function_name,
92: 		         int(info.function_time) / double(info.sample_tuples_count), info.sample_tuples_count,
93: 		         info.tuples_count, "");
94: 
95: 		chunk.SetCardinality(chunk.size() + 1);
96: 		if (chunk.size() == STANDARD_VECTOR_SIZE) {
97: 			collection.Append(chunk);
98: 			chunk.Reset();
99: 		}
100: 	}
101: 	if (info.children.empty()) {
102: 		return;
103: 	}
104: 	// extract the children of this node
105: 	for (auto &child : info.children) {
106: 		ExtractFunctions(collection, *child, chunk, op_id, fun_id);
107: 	}
108: }
109: 
110: static void PragmaDetailedProfilingOutputFunction(ClientContext &context, const FunctionData *bind_data_p,
111:                                                   FunctionOperatorData *operator_state, DataChunk *input,
112:                                                   DataChunk &output) {
113: 	auto &state = (PragmaDetailedProfilingOutputOperatorData &)*operator_state;
114: 	auto &data = (PragmaDetailedProfilingOutputData &)*bind_data_p;
115: 
116: 	if (!state.initialized) {
117: 		// create a ChunkCollection
118: 		auto collection = make_unique<ChunkCollection>();
119: 
120: 		// create a chunk
121: 		DataChunk chunk;
122: 		chunk.Initialize(data.types);
123: 
124: 		// Initialize ids
125: 		int operator_counter = 1;
126: 		int function_counter = 1;
127: 		int expression_counter = 1;
128: 		if (context.query_profiler_history->GetPrevProfilers().empty()) {
129: 			return;
130: 		}
131: 		// For each Operator
132: 		for (auto op : context.query_profiler_history->GetPrevProfilers().back().second->GetTreeMap()) {
133: 			// For each Expression Executor
134: 			for (auto &expr_executor : op.second->info.executors_info) {
135: 				// For each Expression tree
136: 				if (!expr_executor) {
137: 					continue;
138: 				}
139: 				for (auto &expr_timer : expr_executor->roots) {
140: 					D_ASSERT(expr_timer->sample_tuples_count != 0);
141: 					SetValue(chunk, chunk.size(), operator_counter, "ExpressionRoot", expression_counter++,
142: 					         // Sometimes, cycle counter is not accurate, too big or too small. return 0 for
143: 					         // those cases
144: 					         expr_timer->name, int(expr_timer->time) / double(expr_timer->sample_tuples_count),
145: 					         expr_timer->sample_tuples_count, expr_timer->tuples_count, expr_timer->extra_info);
146: 					// Increment cardinality
147: 					chunk.SetCardinality(chunk.size() + 1);
148: 					// Check whether data chunk is full or not
149: 					if (chunk.size() == STANDARD_VECTOR_SIZE) {
150: 						collection->Append(chunk);
151: 						chunk.Reset();
152: 					}
153: 					// Extract all functions inside the tree
154: 					ExtractFunctions(*collection, *expr_timer->root, chunk, operator_counter, function_counter);
155: 				}
156: 			}
157: 			operator_counter++;
158: 		}
159: 		collection->Append(chunk);
160: 		data.collection = move(collection);
161: 		state.initialized = true;
162: 	}
163: 
164: 	if (state.chunk_index >= data.collection->ChunkCount()) {
165: 		output.SetCardinality(0);
166: 		return;
167: 	}
168: 	output.Reference(data.collection->GetChunk(state.chunk_index++));
169: }
170: 
171: void PragmaDetailedProfilingOutput::RegisterFunction(BuiltinFunctions &set) {
172: 	set.AddFunction(TableFunction("pragma_detailed_profiling_output", {}, PragmaDetailedProfilingOutputFunction,
173: 	                              PragmaDetailedProfilingOutputBind, PragmaDetailedProfilingOutputInit));
174: }
175: 
176: } // namespace duckdb
[end of src/function/table/pragma_detailed_profiling_output.cpp]
[start of src/function/table/pragma_last_profiling_output.cpp]
1: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
2: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
3: #include "duckdb/common/limits.hpp"
4: #include "duckdb/function/table/system_functions.hpp"
5: #include "duckdb/main/client_context.hpp"
6: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
7: #include "duckdb/main/query_profiler.hpp"
8: 
9: namespace duckdb {
10: 
11: struct PragmaLastProfilingOutputOperatorData : public FunctionOperatorData {
12: 	PragmaLastProfilingOutputOperatorData() : chunk_index(0), initialized(false) {
13: 	}
14: 	idx_t chunk_index;
15: 	bool initialized;
16: };
17: 
18: struct PragmaLastProfilingOutputData : public TableFunctionData {
19: 	explicit PragmaLastProfilingOutputData(vector<LogicalType> &types) : types(types) {
20: 	}
21: 	unique_ptr<ChunkCollection> collection;
22: 	vector<LogicalType> types;
23: };
24: 
25: static unique_ptr<FunctionData>
26: PragmaLastProfilingOutputBind(ClientContext &context, vector<Value> &inputs, named_parameter_map_t &named_parameters,
27:                               vector<LogicalType> &input_table_types, vector<string> &input_table_names,
28:                               vector<LogicalType> &return_types, vector<string> &names) {
29: 	names.emplace_back("OPERATOR_ID");
30: 	return_types.emplace_back(LogicalType::INTEGER);
31: 
32: 	names.emplace_back("NAME");
33: 	return_types.emplace_back(LogicalType::VARCHAR);
34: 
35: 	names.emplace_back("TIME");
36: 	return_types.emplace_back(LogicalType::DOUBLE);
37: 
38: 	names.emplace_back("CARDINALITY");
39: 	return_types.emplace_back(LogicalType::BIGINT);
40: 
41: 	names.emplace_back("DESCRIPTION");
42: 	return_types.emplace_back(LogicalType::VARCHAR);
43: 
44: 	return make_unique<PragmaLastProfilingOutputData>(return_types);
45: }
46: 
47: static void SetValue(DataChunk &output, int index, int op_id, string name, double time, int64_t car,
48:                      string description) {
49: 	output.SetValue(0, index, op_id);
50: 	output.SetValue(1, index, move(name));
51: 	output.SetValue(2, index, time);
52: 	output.SetValue(3, index, car);
53: 	output.SetValue(4, index, move(description));
54: }
55: 
56: unique_ptr<FunctionOperatorData> PragmaLastProfilingOutputInit(ClientContext &context, const FunctionData *bind_data,
57:                                                                const vector<column_t> &column_ids,
58:                                                                TableFilterCollection *filters) {
59: 	return make_unique<PragmaLastProfilingOutputOperatorData>();
60: }
61: 
62: static void PragmaLastProfilingOutputFunction(ClientContext &context, const FunctionData *bind_data_p,
63:                                               FunctionOperatorData *operator_state, DataChunk *input,
64:                                               DataChunk &output) {
65: 	auto &state = (PragmaLastProfilingOutputOperatorData &)*operator_state;
66: 	auto &data = (PragmaLastProfilingOutputData &)*bind_data_p;
67: 	if (!state.initialized) {
68: 		// create a ChunkCollection
69: 		auto collection = make_unique<ChunkCollection>();
70: 
71: 		DataChunk chunk;
72: 		chunk.Initialize(data.types);
73: 		int operator_counter = 1;
74: 		if (!context.query_profiler_history->GetPrevProfilers().empty()) {
75: 			for (auto op : context.query_profiler_history->GetPrevProfilers().back().second->GetTreeMap()) {
76: 				SetValue(chunk, chunk.size(), operator_counter++, op.second->name, op.second->info.time,
77: 				         op.second->info.elements, " ");
78: 				chunk.SetCardinality(chunk.size() + 1);
79: 				if (chunk.size() == STANDARD_VECTOR_SIZE) {
80: 					collection->Append(chunk);
81: 					chunk.Reset();
82: 				}
83: 			}
84: 		}
85: 		collection->Append(chunk);
86: 		data.collection = move(collection);
87: 		state.initialized = true;
88: 	}
89: 
90: 	if (state.chunk_index >= data.collection->ChunkCount()) {
91: 		output.SetCardinality(0);
92: 		return;
93: 	}
94: 	output.Reference(data.collection->GetChunk(state.chunk_index++));
95: }
96: 
97: void PragmaLastProfilingOutput::RegisterFunction(BuiltinFunctions &set) {
98: 	set.AddFunction(TableFunction("pragma_last_profiling_output", {}, PragmaLastProfilingOutputFunction,
99: 	                              PragmaLastProfilingOutputBind, PragmaLastProfilingOutputInit));
100: }
101: 
102: } // namespace duckdb
[end of src/function/table/pragma_last_profiling_output.cpp]
[start of src/function/table/range.cpp]
1: #include "duckdb/function/table/range.hpp"
2: #include "duckdb/function/table/summary.hpp"
3: #include "duckdb/function/table_function.hpp"
4: #include "duckdb/function/function_set.hpp"
5: #include "duckdb/common/algorithm.hpp"
6: #include "duckdb/common/operator/add.hpp"
7: 
8: namespace duckdb {
9: 
10: //===--------------------------------------------------------------------===//
11: // Range (integers)
12: //===--------------------------------------------------------------------===//
13: struct RangeFunctionBindData : public TableFunctionData {
14: 	hugeint_t start;
15: 	hugeint_t end;
16: 	hugeint_t increment;
17: };
18: 
19: template <bool GENERATE_SERIES>
20: static unique_ptr<FunctionData>
21: RangeFunctionBind(ClientContext &context, vector<Value> &inputs, named_parameter_map_t &named_parameters,
22:                   vector<LogicalType> &input_table_types, vector<string> &input_table_names,
23:                   vector<LogicalType> &return_types, vector<string> &names) {
24: 	auto result = make_unique<RangeFunctionBindData>();
25: 	if (inputs.size() < 2) {
26: 		// single argument: only the end is specified
27: 		result->start = 0;
28: 		result->end = inputs[0].GetValue<int64_t>();
29: 	} else {
30: 		// two arguments: first two arguments are start and end
31: 		result->start = inputs[0].GetValue<int64_t>();
32: 		result->end = inputs[1].GetValue<int64_t>();
33: 	}
34: 	if (inputs.size() < 3) {
35: 		result->increment = 1;
36: 	} else {
37: 		result->increment = inputs[2].GetValue<int64_t>();
38: 	}
39: 	if (result->increment == 0) {
40: 		throw BinderException("interval cannot be 0!");
41: 	}
42: 	if (result->start > result->end && result->increment > 0) {
43: 		throw BinderException("start is bigger than end, but increment is positive: cannot generate infinite series");
44: 	} else if (result->start < result->end && result->increment < 0) {
45: 		throw BinderException("start is smaller than end, but increment is negative: cannot generate infinite series");
46: 	}
47: 	return_types.emplace_back(LogicalType::BIGINT);
48: 	if (GENERATE_SERIES) {
49: 		// generate_series has inclusive bounds on the RHS
50: 		if (result->increment < 0) {
51: 			result->end = result->end - 1;
52: 		} else {
53: 			result->end = result->end + 1;
54: 		}
55: 		names.emplace_back("generate_series");
56: 	} else {
57: 		names.emplace_back("range");
58: 	}
59: 	return move(result);
60: }
61: 
62: struct RangeFunctionState : public FunctionOperatorData {
63: 	RangeFunctionState() : current_idx(0) {
64: 	}
65: 
66: 	int64_t current_idx;
67: };
68: 
69: static unique_ptr<FunctionOperatorData> RangeFunctionInit(ClientContext &context, const FunctionData *bind_data,
70:                                                           const vector<column_t> &column_ids,
71:                                                           TableFilterCollection *filters) {
72: 	return make_unique<RangeFunctionState>();
73: }
74: 
75: static void RangeFunction(ClientContext &context, const FunctionData *bind_data_p, FunctionOperatorData *state_p,
76:                           DataChunk *input, DataChunk &output) {
77: 	auto &bind_data = (RangeFunctionBindData &)*bind_data_p;
78: 	auto &state = (RangeFunctionState &)*state_p;
79: 
80: 	auto increment = bind_data.increment;
81: 	auto end = bind_data.end;
82: 	hugeint_t current_value = bind_data.start + increment * state.current_idx;
83: 	int64_t current_value_i64;
84: 	if (!Hugeint::TryCast<int64_t>(current_value, current_value_i64)) {
85: 		return;
86: 	}
87: 	// set the result vector as a sequence vector
88: 	output.data[0].Sequence(current_value_i64, Hugeint::Cast<int64_t>(increment));
89: 	int64_t offset = increment < 0 ? 1 : -1;
90: 	idx_t remaining = MinValue<idx_t>(Hugeint::Cast<idx_t>((end - current_value + (increment + offset)) / increment),
91: 	                                  STANDARD_VECTOR_SIZE);
92: 	// increment the index pointer by the remaining count
93: 	state.current_idx += remaining;
94: 	output.SetCardinality(remaining);
95: }
96: 
97: unique_ptr<NodeStatistics> RangeCardinality(ClientContext &context, const FunctionData *bind_data_p) {
98: 	auto &bind_data = (RangeFunctionBindData &)*bind_data_p;
99: 	idx_t cardinality = Hugeint::Cast<idx_t>((bind_data.end - bind_data.start) / bind_data.increment);
100: 	return make_unique<NodeStatistics>(cardinality, cardinality);
101: }
102: 
103: //===--------------------------------------------------------------------===//
104: // Range (timestamp)
105: //===--------------------------------------------------------------------===//
106: struct RangeDateTimeBindData : public TableFunctionData {
107: 	timestamp_t start;
108: 	timestamp_t end;
109: 	interval_t increment;
110: 	bool inclusive_bound;
111: 	bool greater_than_check;
112: 
113: 	bool Finished(timestamp_t current_value) {
114: 		if (greater_than_check) {
115: 			if (inclusive_bound) {
116: 				return current_value > end;
117: 			} else {
118: 				return current_value >= end;
119: 			}
120: 		} else {
121: 			if (inclusive_bound) {
122: 				return current_value < end;
123: 			} else {
124: 				return current_value <= end;
125: 			}
126: 		}
127: 	}
128: };
129: 
130: template <bool GENERATE_SERIES>
131: static unique_ptr<FunctionData>
132: RangeDateTimeBind(ClientContext &context, vector<Value> &inputs, named_parameter_map_t &named_parameters,
133:                   vector<LogicalType> &input_table_types, vector<string> &input_table_names,
134:                   vector<LogicalType> &return_types, vector<string> &names) {
135: 	auto result = make_unique<RangeDateTimeBindData>();
136: 	D_ASSERT(inputs.size() == 3);
137: 	result->start = inputs[0].GetValue<timestamp_t>();
138: 	result->end = inputs[1].GetValue<timestamp_t>();
139: 	result->increment = inputs[2].GetValue<interval_t>();
140: 
141: 	if (result->increment.months == 0 && result->increment.days == 0 && result->increment.micros == 0) {
142: 		throw BinderException("interval cannot be 0!");
143: 	}
144: 	// all elements should point in the same direction
145: 	if (result->increment.months > 0 || result->increment.days > 0 || result->increment.micros > 0) {
146: 		if (result->increment.months < 0 || result->increment.days < 0 || result->increment.micros < 0) {
147: 			throw BinderException("RANGE with composite interval that has mixed signs is not supported");
148: 		}
149: 		result->greater_than_check = true;
150: 		if (result->start > result->end) {
151: 			throw BinderException(
152: 			    "start is bigger than end, but increment is positive: cannot generate infinite series");
153: 		}
154: 	} else {
155: 		result->greater_than_check = false;
156: 		if (result->start < result->end) {
157: 			throw BinderException(
158: 			    "start is smaller than end, but increment is negative: cannot generate infinite series");
159: 		}
160: 	}
161: 	return_types.push_back(inputs[0].type());
162: 	if (GENERATE_SERIES) {
163: 		// generate_series has inclusive bounds on the RHS
164: 		result->inclusive_bound = true;
165: 		names.emplace_back("generate_series");
166: 	} else {
167: 		result->inclusive_bound = false;
168: 		names.emplace_back("range");
169: 	}
170: 	return move(result);
171: }
172: 
173: struct RangeDateTimeState : public FunctionOperatorData {
174: 	explicit RangeDateTimeState(timestamp_t start_p) : current_state(start_p) {
175: 	}
176: 
177: 	timestamp_t current_state;
178: 	bool finished = false;
179: };
180: 
181: static unique_ptr<FunctionOperatorData> RangeDateTimeInit(ClientContext &context, const FunctionData *bind_data_p,
182:                                                           const vector<column_t> &column_ids,
183:                                                           TableFilterCollection *filters) {
184: 	auto &bind_data = (RangeDateTimeBindData &)*bind_data_p;
185: 	return make_unique<RangeDateTimeState>(bind_data.start);
186: }
187: 
188: static void RangeDateTimeFunction(ClientContext &context, const FunctionData *bind_data_p,
189:                                   FunctionOperatorData *state_p, DataChunk *input, DataChunk &output) {
190: 	auto &bind_data = (RangeDateTimeBindData &)*bind_data_p;
191: 	auto &state = (RangeDateTimeState &)*state_p;
192: 	if (state.finished) {
193: 		return;
194: 	}
195: 
196: 	idx_t size = 0;
197: 	auto data = FlatVector::GetData<timestamp_t>(output.data[0]);
198: 	while (true) {
199: 		data[size++] = state.current_state;
200: 		state.current_state =
201: 		    AddOperator::Operation<timestamp_t, interval_t, timestamp_t>(state.current_state, bind_data.increment);
202: 		if (bind_data.Finished(state.current_state)) {
203: 			state.finished = true;
204: 			break;
205: 		}
206: 		if (size >= STANDARD_VECTOR_SIZE) {
207: 			break;
208: 		}
209: 	}
210: 	output.SetCardinality(size);
211: }
212: 
213: void RangeTableFunction::RegisterFunction(BuiltinFunctions &set) {
214: 	TableFunctionSet range("range");
215: 
216: 	// single argument range: (end) - implicit start = 0 and increment = 1
217: 	range.AddFunction(TableFunction({LogicalType::BIGINT}, RangeFunction, RangeFunctionBind<false>, RangeFunctionInit,
218: 	                                nullptr, nullptr, nullptr, RangeCardinality));
219: 	// two arguments range: (start, end) - implicit increment = 1
220: 	range.AddFunction(TableFunction({LogicalType::BIGINT, LogicalType::BIGINT}, RangeFunction, RangeFunctionBind<false>,
221: 	                                RangeFunctionInit, nullptr, nullptr, nullptr, RangeCardinality));
222: 	// three arguments range: (start, end, increment)
223: 	range.AddFunction(TableFunction({LogicalType::BIGINT, LogicalType::BIGINT, LogicalType::BIGINT}, RangeFunction,
224: 	                                RangeFunctionBind<false>, RangeFunctionInit, nullptr, nullptr, nullptr,
225: 	                                RangeCardinality));
226: 	range.AddFunction(TableFunction({LogicalType::TIMESTAMP, LogicalType::TIMESTAMP, LogicalType::INTERVAL},
227: 	                                RangeDateTimeFunction, RangeDateTimeBind<false>, RangeDateTimeInit));
228: 	set.AddFunction(range);
229: 	// generate_series: similar to range, but inclusive instead of exclusive bounds on the RHS
230: 	TableFunctionSet generate_series("generate_series");
231: 	generate_series.AddFunction(TableFunction({LogicalType::BIGINT}, RangeFunction, RangeFunctionBind<true>,
232: 	                                          RangeFunctionInit, nullptr, nullptr, nullptr, RangeCardinality));
233: 	generate_series.AddFunction(TableFunction({LogicalType::BIGINT, LogicalType::BIGINT}, RangeFunction,
234: 	                                          RangeFunctionBind<true>, RangeFunctionInit, nullptr, nullptr, nullptr,
235: 	                                          RangeCardinality));
236: 	generate_series.AddFunction(TableFunction({LogicalType::BIGINT, LogicalType::BIGINT, LogicalType::BIGINT},
237: 	                                          RangeFunction, RangeFunctionBind<true>, RangeFunctionInit, nullptr,
238: 	                                          nullptr, nullptr, RangeCardinality));
239: 	generate_series.AddFunction(TableFunction({LogicalType::TIMESTAMP, LogicalType::TIMESTAMP, LogicalType::INTERVAL},
240: 	                                          RangeDateTimeFunction, RangeDateTimeBind<true>, RangeDateTimeInit));
241: 	set.AddFunction(generate_series);
242: }
243: 
244: void BuiltinFunctions::RegisterTableFunctions() {
245: 	CheckpointFunction::RegisterFunction(*this);
246: 	GlobTableFunction::RegisterFunction(*this);
247: 	RangeTableFunction::RegisterFunction(*this);
248: 	RepeatTableFunction::RegisterFunction(*this);
249: 	SummaryTableFunction::RegisterFunction(*this);
250: 	UnnestTableFunction::RegisterFunction(*this);
251: }
252: 
253: } // namespace duckdb
[end of src/function/table/range.cpp]
[start of src/function/table/read_csv.cpp]
1: #include "duckdb/function/table/read_csv.hpp"
2: #include "duckdb/execution/operator/persistent/buffered_csv_reader.hpp"
3: #include "duckdb/function/function_set.hpp"
4: #include "duckdb/main/client_context.hpp"
5: #include "duckdb/main/database.hpp"
6: #include "duckdb/common/string_util.hpp"
7: #include "duckdb/main/config.hpp"
8: #include "duckdb/parser/expression/constant_expression.hpp"
9: #include "duckdb/parser/expression/function_expression.hpp"
10: #include "duckdb/parser/tableref/table_function_ref.hpp"
11: #include "duckdb/common/windows_undefs.hpp"
12: 
13: #include <limits>
14: 
15: namespace duckdb {
16: 
17: static unique_ptr<FunctionData> ReadCSVBind(ClientContext &context, vector<Value> &inputs,
18:                                             named_parameter_map_t &named_parameters,
19:                                             vector<LogicalType> &input_table_types, vector<string> &input_table_names,
20:                                             vector<LogicalType> &return_types, vector<string> &names) {
21: 	auto &config = DBConfig::GetConfig(context);
22: 	if (!config.enable_external_access) {
23: 		throw PermissionException("Scanning CSV files is disabled through configuration");
24: 	}
25: 	auto result = make_unique<ReadCSVData>();
26: 	auto &options = result->options;
27: 
28: 	auto &file_pattern = StringValue::Get(inputs[0]);
29: 
30: 	auto &fs = FileSystem::GetFileSystem(context);
31: 	result->files = fs.Glob(file_pattern, context);
32: 	if (result->files.empty()) {
33: 		throw IOException("No files found that match the pattern \"%s\"", file_pattern);
34: 	}
35: 
36: 	for (auto &kv : named_parameters) {
37: 		auto loption = StringUtil::Lower(kv.first);
38: 		if (loption == "auto_detect") {
39: 			options.auto_detect = BooleanValue::Get(kv.second);
40: 		} else if (loption == "sep" || loption == "delim") {
41: 			options.SetDelimiter(StringValue::Get(kv.second));
42: 		} else if (loption == "header") {
43: 			options.header = BooleanValue::Get(kv.second);
44: 			options.has_header = true;
45: 		} else if (loption == "quote") {
46: 			options.quote = StringValue::Get(kv.second);
47: 			options.has_quote = true;
48: 		} else if (loption == "escape") {
49: 			options.escape = StringValue::Get(kv.second);
50: 			options.has_escape = true;
51: 		} else if (loption == "nullstr") {
52: 			options.null_str = StringValue::Get(kv.second);
53: 		} else if (loption == "sample_size") {
54: 			int64_t sample_size = kv.second.GetValue<int64_t>();
55: 			if (sample_size < 1 && sample_size != -1) {
56: 				throw BinderException("Unsupported parameter for SAMPLE_SIZE: cannot be smaller than 1");
57: 			}
58: 			if (sample_size == -1) {
59: 				options.sample_chunks = std::numeric_limits<uint64_t>::max();
60: 				options.sample_chunk_size = STANDARD_VECTOR_SIZE;
61: 			} else if (sample_size <= STANDARD_VECTOR_SIZE) {
62: 				options.sample_chunk_size = sample_size;
63: 				options.sample_chunks = 1;
64: 			} else {
65: 				options.sample_chunk_size = STANDARD_VECTOR_SIZE;
66: 				options.sample_chunks = sample_size / STANDARD_VECTOR_SIZE;
67: 			}
68: 		} else if (loption == "sample_chunk_size") {
69: 			options.sample_chunk_size = kv.second.GetValue<int64_t>();
70: 			if (options.sample_chunk_size > STANDARD_VECTOR_SIZE) {
71: 				throw BinderException(
72: 				    "Unsupported parameter for SAMPLE_CHUNK_SIZE: cannot be bigger than STANDARD_VECTOR_SIZE %d",
73: 				    STANDARD_VECTOR_SIZE);
74: 			} else if (options.sample_chunk_size < 1) {
75: 				throw BinderException("Unsupported parameter for SAMPLE_CHUNK_SIZE: cannot be smaller than 1");
76: 			}
77: 		} else if (loption == "sample_chunks") {
78: 			options.sample_chunks = kv.second.GetValue<int64_t>();
79: 			if (options.sample_chunks < 1) {
80: 				throw BinderException("Unsupported parameter for SAMPLE_CHUNKS: cannot be smaller than 1");
81: 			}
82: 		} else if (loption == "all_varchar") {
83: 			options.all_varchar = BooleanValue::Get(kv.second);
84: 		} else if (loption == "dateformat") {
85: 			options.has_format[LogicalTypeId::DATE] = true;
86: 			auto &date_format = options.date_format[LogicalTypeId::DATE];
87: 			date_format.format_specifier = StringValue::Get(kv.second);
88: 			string error = StrTimeFormat::ParseFormatSpecifier(date_format.format_specifier, date_format);
89: 			if (!error.empty()) {
90: 				throw InvalidInputException("Could not parse DATEFORMAT: %s", error.c_str());
91: 			}
92: 		} else if (loption == "timestampformat") {
93: 			options.has_format[LogicalTypeId::TIMESTAMP] = true;
94: 			auto &timestamp_format = options.date_format[LogicalTypeId::TIMESTAMP];
95: 			timestamp_format.format_specifier = StringValue::Get(kv.second);
96: 			string error = StrTimeFormat::ParseFormatSpecifier(timestamp_format.format_specifier, timestamp_format);
97: 			if (!error.empty()) {
98: 				throw InvalidInputException("Could not parse TIMESTAMPFORMAT: %s", error.c_str());
99: 			}
100: 		} else if (loption == "normalize_names") {
101: 			options.normalize_names = BooleanValue::Get(kv.second);
102: 		} else if (loption == "columns") {
103: 			auto &child_type = kv.second.type();
104: 			if (child_type.id() != LogicalTypeId::STRUCT) {
105: 				throw BinderException("read_csv columns requires a a struct as input");
106: 			}
107: 			auto &struct_children = StructValue::GetChildren(kv.second);
108: 			D_ASSERT(StructType::GetChildCount(child_type) == struct_children.size());
109: 			for (idx_t i = 0; i < struct_children.size(); i++) {
110: 				auto &name = StructType::GetChildName(child_type, i);
111: 				auto &val = struct_children[i];
112: 				names.push_back(name);
113: 				if (val.type().id() != LogicalTypeId::VARCHAR) {
114: 					throw BinderException("read_csv requires a type specification as string");
115: 				}
116: 				return_types.emplace_back(TransformStringToLogicalTypeId(StringValue::Get(val)));
117: 			}
118: 			if (names.empty()) {
119: 				throw BinderException("read_csv requires at least a single column as input!");
120: 			}
121: 		} else if (loption == "compression") {
122: 			options.compression = FileCompressionTypeFromString(StringValue::Get(kv.second));
123: 		} else if (loption == "filename") {
124: 			result->include_file_name = BooleanValue::Get(kv.second);
125: 		} else if (loption == "skip") {
126: 			options.skip_rows = kv.second.GetValue<int64_t>();
127: 		} else if (loption == "max_line_size" || loption == "maximum_line_size") {
128: 			options.maximum_line_size = kv.second.GetValue<int64_t>();
129: 		} else {
130: 			throw InternalException("Unrecognized parameter %s", kv.first);
131: 		}
132: 	}
133: 	if (!options.auto_detect && return_types.empty()) {
134: 		throw BinderException("read_csv requires columns to be specified. Use read_csv_auto or set read_csv(..., "
135: 		                      "AUTO_DETECT=TRUE) to automatically guess columns.");
136: 	}
137: 	if (options.auto_detect) {
138: 		options.file_path = result->files[0];
139: 		auto initial_reader = make_unique<BufferedCSVReader>(context, options);
140: 
141: 		return_types.assign(initial_reader->sql_types.begin(), initial_reader->sql_types.end());
142: 		if (names.empty()) {
143: 			names.assign(initial_reader->col_names.begin(), initial_reader->col_names.end());
144: 		} else {
145: 			D_ASSERT(return_types.size() == names.size());
146: 		}
147: 		result->initial_reader = move(initial_reader);
148: 	} else {
149: 		result->sql_types = return_types;
150: 		D_ASSERT(return_types.size() == names.size());
151: 	}
152: 	if (result->include_file_name) {
153: 		return_types.emplace_back(LogicalType::VARCHAR);
154: 		names.emplace_back("filename");
155: 	}
156: 	return move(result);
157: }
158: 
159: struct ReadCSVOperatorData : public FunctionOperatorData {
160: 	//! The CSV reader
161: 	unique_ptr<BufferedCSVReader> csv_reader;
162: 	//! The index of the next file to read (i.e. current file + 1)
163: 	idx_t file_index;
164: };
165: 
166: static unique_ptr<FunctionOperatorData> ReadCSVInit(ClientContext &context, const FunctionData *bind_data_p,
167:                                                     const vector<column_t> &column_ids,
168:                                                     TableFilterCollection *filters) {
169: 	auto &bind_data = (ReadCSVData &)*bind_data_p;
170: 	auto result = make_unique<ReadCSVOperatorData>();
171: 	if (bind_data.initial_reader) {
172: 		result->csv_reader = move(bind_data.initial_reader);
173: 	} else {
174: 		bind_data.options.file_path = bind_data.files[0];
175: 		result->csv_reader = make_unique<BufferedCSVReader>(context, bind_data.options, bind_data.sql_types);
176: 	}
177: 	bind_data.bytes_read = 0;
178: 	bind_data.file_size = result->csv_reader->GetFileSize();
179: 	result->file_index = 1;
180: 	return move(result);
181: }
182: 
183: static unique_ptr<FunctionData> ReadCSVAutoBind(ClientContext &context, vector<Value> &inputs,
184:                                                 named_parameter_map_t &named_parameters,
185:                                                 vector<LogicalType> &input_table_types,
186:                                                 vector<string> &input_table_names, vector<LogicalType> &return_types,
187:                                                 vector<string> &names) {
188: 	named_parameters["auto_detect"] = Value::BOOLEAN(true);
189: 	return ReadCSVBind(context, inputs, named_parameters, input_table_types, input_table_names, return_types, names);
190: }
191: 
192: static void ReadCSVFunction(ClientContext &context, const FunctionData *bind_data_p,
193:                             FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
194: 	auto &bind_data = (ReadCSVData &)*bind_data_p;
195: 	auto &data = (ReadCSVOperatorData &)*operator_state;
196: 	do {
197: 		data.csv_reader->ParseCSV(output);
198: 		bind_data.bytes_read = data.csv_reader->bytes_in_chunk;
199: 		if (output.size() == 0 && data.file_index < bind_data.files.size()) {
200: 			// exhausted this file, but we have more files we can read
201: 			// open the next file and increment the counter
202: 			bind_data.options.file_path = bind_data.files[data.file_index];
203: 			data.csv_reader = make_unique<BufferedCSVReader>(context, bind_data.options, data.csv_reader->sql_types);
204: 			data.file_index++;
205: 		} else {
206: 			break;
207: 		}
208: 	} while (true);
209: 	if (bind_data.include_file_name) {
210: 		auto &col = output.data.back();
211: 		col.SetValue(0, Value(data.csv_reader->options.file_path));
212: 		col.SetVectorType(VectorType::CONSTANT_VECTOR);
213: 	}
214: }
215: 
216: static void ReadCSVAddNamedParameters(TableFunction &table_function) {
217: 	table_function.named_parameters["sep"] = LogicalType::VARCHAR;
218: 	table_function.named_parameters["delim"] = LogicalType::VARCHAR;
219: 	table_function.named_parameters["quote"] = LogicalType::VARCHAR;
220: 	table_function.named_parameters["escape"] = LogicalType::VARCHAR;
221: 	table_function.named_parameters["nullstr"] = LogicalType::VARCHAR;
222: 	table_function.named_parameters["columns"] = LogicalType::ANY;
223: 	table_function.named_parameters["header"] = LogicalType::BOOLEAN;
224: 	table_function.named_parameters["auto_detect"] = LogicalType::BOOLEAN;
225: 	table_function.named_parameters["sample_size"] = LogicalType::BIGINT;
226: 	table_function.named_parameters["sample_chunk_size"] = LogicalType::BIGINT;
227: 	table_function.named_parameters["sample_chunks"] = LogicalType::BIGINT;
228: 	table_function.named_parameters["all_varchar"] = LogicalType::BOOLEAN;
229: 	table_function.named_parameters["dateformat"] = LogicalType::VARCHAR;
230: 	table_function.named_parameters["timestampformat"] = LogicalType::VARCHAR;
231: 	table_function.named_parameters["normalize_names"] = LogicalType::BOOLEAN;
232: 	table_function.named_parameters["compression"] = LogicalType::VARCHAR;
233: 	table_function.named_parameters["filename"] = LogicalType::BOOLEAN;
234: 	table_function.named_parameters["skip"] = LogicalType::BIGINT;
235: 	table_function.named_parameters["max_line_size"] = LogicalType::VARCHAR;
236: 	table_function.named_parameters["maximum_line_size"] = LogicalType::VARCHAR;
237: }
238: 
239: double CSVReaderProgress(ClientContext &context, const FunctionData *bind_data_p) {
240: 	auto &bind_data = (ReadCSVData &)*bind_data_p;
241: 	if (bind_data.file_size == 0) {
242: 		return 100;
243: 	}
244: 	auto percentage = (bind_data.bytes_read * 100.0) / bind_data.file_size;
245: 	return percentage;
246: }
247: 
248: TableFunction ReadCSVTableFunction::GetFunction() {
249: 	TableFunction read_csv("read_csv", {LogicalType::VARCHAR}, ReadCSVFunction, ReadCSVBind, ReadCSVInit);
250: 	read_csv.table_scan_progress = CSVReaderProgress;
251: 	ReadCSVAddNamedParameters(read_csv);
252: 	return read_csv;
253: }
254: 
255: void ReadCSVTableFunction::RegisterFunction(BuiltinFunctions &set) {
256: 	set.AddFunction(ReadCSVTableFunction::GetFunction());
257: 
258: 	TableFunction read_csv_auto("read_csv_auto", {LogicalType::VARCHAR}, ReadCSVFunction, ReadCSVAutoBind, ReadCSVInit);
259: 	read_csv_auto.table_scan_progress = CSVReaderProgress;
260: 	ReadCSVAddNamedParameters(read_csv_auto);
261: 	set.AddFunction(read_csv_auto);
262: }
263: 
264: unique_ptr<TableFunctionRef> ReadCSVReplacement(const string &table_name, void *data) {
265: 	auto lower_name = StringUtil::Lower(table_name);
266: 	// remove any compression
267: 	if (StringUtil::EndsWith(lower_name, ".gz")) {
268: 		lower_name = lower_name.substr(0, lower_name.size() - 3);
269: 	} else if (StringUtil::EndsWith(lower_name, ".zst")) {
270: 		lower_name = lower_name.substr(0, lower_name.size() - 4);
271: 	}
272: 	if (!StringUtil::EndsWith(lower_name, ".csv") && !StringUtil::EndsWith(lower_name, ".tsv")) {
273: 		return nullptr;
274: 	}
275: 	auto table_function = make_unique<TableFunctionRef>();
276: 	vector<unique_ptr<ParsedExpression>> children;
277: 	children.push_back(make_unique<ConstantExpression>(Value(table_name)));
278: 	table_function->function = make_unique<FunctionExpression>("read_csv_auto", move(children));
279: 	return table_function;
280: }
281: 
282: void BuiltinFunctions::RegisterReadFunctions() {
283: 	CSVCopyFunction::RegisterFunction(*this);
284: 	ReadCSVTableFunction::RegisterFunction(*this);
285: 
286: 	auto &config = DBConfig::GetConfig(context);
287: 	config.replacement_scans.emplace_back(ReadCSVReplacement);
288: }
289: 
290: } // namespace duckdb
[end of src/function/table/read_csv.cpp]
[start of src/function/table/repeat.cpp]
1: #include "duckdb/function/table/range.hpp"
2: #include "duckdb/common/algorithm.hpp"
3: 
4: namespace duckdb {
5: 
6: struct RepeatFunctionData : public TableFunctionData {
7: 	RepeatFunctionData(Value value, idx_t target_count) : value(move(value)), target_count(target_count) {
8: 	}
9: 
10: 	Value value;
11: 	idx_t target_count;
12: };
13: 
14: struct RepeatOperatorData : public FunctionOperatorData {
15: 	RepeatOperatorData() : current_count(0) {
16: 	}
17: 	idx_t current_count;
18: };
19: 
20: static unique_ptr<FunctionData> RepeatBind(ClientContext &context, vector<Value> &inputs,
21:                                            named_parameter_map_t &named_parameters,
22:                                            vector<LogicalType> &input_table_types, vector<string> &input_table_names,
23:                                            vector<LogicalType> &return_types, vector<string> &names) {
24: 	// the repeat function returns the type of the first argument
25: 	return_types.push_back(inputs[0].type());
26: 	names.push_back(inputs[0].ToString());
27: 	return make_unique<RepeatFunctionData>(inputs[0], inputs[1].GetValue<int64_t>());
28: }
29: 
30: static unique_ptr<FunctionOperatorData> RepeatInit(ClientContext &context, const FunctionData *bind_data,
31:                                                    const vector<column_t> &column_ids, TableFilterCollection *filters) {
32: 	return make_unique<RepeatOperatorData>();
33: }
34: 
35: static void RepeatFunction(ClientContext &context, const FunctionData *bind_data_p,
36:                            FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
37: 	auto &bind_data = (RepeatFunctionData &)*bind_data_p;
38: 	auto &state = (RepeatOperatorData &)*operator_state;
39: 
40: 	idx_t remaining = MinValue<idx_t>(bind_data.target_count - state.current_count, STANDARD_VECTOR_SIZE);
41: 	output.data[0].Reference(bind_data.value);
42: 	output.SetCardinality(remaining);
43: 	state.current_count += remaining;
44: }
45: 
46: static unique_ptr<NodeStatistics> RepeatCardinality(ClientContext &context, const FunctionData *bind_data_p) {
47: 	auto &bind_data = (RepeatFunctionData &)*bind_data_p;
48: 	return make_unique<NodeStatistics>(bind_data.target_count, bind_data.target_count);
49: }
50: 
51: void RepeatTableFunction::RegisterFunction(BuiltinFunctions &set) {
52: 	TableFunction repeat("repeat", {LogicalType::ANY, LogicalType::BIGINT}, RepeatFunction, RepeatBind, RepeatInit,
53: 	                     nullptr, nullptr, nullptr, RepeatCardinality);
54: 	set.AddFunction(repeat);
55: }
56: 
57: } // namespace duckdb
[end of src/function/table/repeat.cpp]
[start of src/function/table/summary.cpp]
1: #include "duckdb/function/table/summary.hpp"
2: #include "duckdb/function/table_function.hpp"
3: #include "duckdb/function/function_set.hpp"
4: #include "duckdb/common/file_system.hpp"
5: 
6: // this function makes not that much sense on its own but is a demo for table-parameter table-producing functions
7: 
8: namespace duckdb {
9: 
10: static unique_ptr<FunctionData> SummaryFunctionBind(ClientContext &context, vector<Value> &inputs,
11:                                                     named_parameter_map_t &named_parameters,
12:                                                     vector<LogicalType> &input_table_types,
13:                                                     vector<string> &input_table_names,
14:                                                     vector<LogicalType> &return_types, vector<string> &names) {
15: 
16: 	return_types.emplace_back(LogicalType::VARCHAR);
17: 	names.emplace_back("summary");
18: 
19: 	for (idx_t i = 0; i < input_table_types.size(); i++) {
20: 		return_types.push_back(input_table_types[i]);
21: 		names.emplace_back(input_table_names[i]);
22: 	}
23: 
24: 	return make_unique<TableFunctionData>();
25: }
26: 
27: static void SummaryFunction(ClientContext &context, const FunctionData *bind_data_p, FunctionOperatorData *state_p,
28:                             DataChunk *input, DataChunk &output) {
29: 	D_ASSERT(input);
30: 	output.SetCardinality(input->size());
31: 
32: 	for (idx_t row_idx = 0; row_idx < input->size(); row_idx++) {
33: 		string summary_val = "[";
34: 
35: 		for (idx_t col_idx = 0; col_idx < input->ColumnCount(); col_idx++) {
36: 			summary_val += input->GetValue(col_idx, row_idx).ToString();
37: 			if (col_idx < input->ColumnCount() - 1) {
38: 				summary_val += ", ";
39: 			}
40: 		}
41: 		summary_val += "]";
42: 		output.SetValue(0, row_idx, Value(summary_val));
43: 	}
44: 	for (idx_t col_idx = 0; col_idx < input->ColumnCount(); col_idx++) {
45: 		output.data[col_idx + 1].Reference(input->data[col_idx]);
46: 	}
47: }
48: 
49: void SummaryTableFunction::RegisterFunction(BuiltinFunctions &set) {
50: 	TableFunctionSet summary("summary");
51: 	summary.AddFunction(TableFunction({LogicalType::TABLE}, SummaryFunction, SummaryFunctionBind));
52: 	set.AddFunction(summary);
53: }
54: 
55: } // namespace duckdb
[end of src/function/table/summary.cpp]
[start of src/function/table/system/duckdb_columns.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb/parser/constraints/not_null_constraint.hpp"
9: 
10: #include <set>
11: 
12: namespace duckdb {
13: 
14: struct DuckDBColumnsData : public FunctionOperatorData {
15: 	DuckDBColumnsData() : offset(0), column_offset(0) {
16: 	}
17: 
18: 	vector<CatalogEntry *> entries;
19: 	idx_t offset;
20: 	idx_t column_offset;
21: };
22: 
23: static unique_ptr<FunctionData> DuckDBColumnsBind(ClientContext &context, vector<Value> &inputs,
24:                                                   named_parameter_map_t &named_parameters,
25:                                                   vector<LogicalType> &input_table_types,
26:                                                   vector<string> &input_table_names, vector<LogicalType> &return_types,
27:                                                   vector<string> &names) {
28: 	names.emplace_back("schema_oid");
29: 	return_types.emplace_back(LogicalType::BIGINT);
30: 
31: 	names.emplace_back("schema_name");
32: 	return_types.emplace_back(LogicalType::VARCHAR);
33: 
34: 	names.emplace_back("table_oid");
35: 	return_types.emplace_back(LogicalType::BIGINT);
36: 
37: 	names.emplace_back("table_name");
38: 	return_types.emplace_back(LogicalType::VARCHAR);
39: 
40: 	names.emplace_back("column_name");
41: 	return_types.emplace_back(LogicalType::VARCHAR);
42: 
43: 	names.emplace_back("column_index");
44: 	return_types.emplace_back(LogicalType::INTEGER);
45: 
46: 	names.emplace_back("internal");
47: 	return_types.emplace_back(LogicalType::BOOLEAN);
48: 
49: 	names.emplace_back("column_default");
50: 	return_types.emplace_back(LogicalType::VARCHAR);
51: 
52: 	names.emplace_back("is_nullable");
53: 	return_types.emplace_back(LogicalType::BOOLEAN);
54: 
55: 	names.emplace_back("data_type");
56: 	return_types.emplace_back(LogicalType::VARCHAR);
57: 
58: 	names.emplace_back("data_type_id");
59: 	return_types.emplace_back(LogicalType::BIGINT);
60: 
61: 	names.emplace_back("character_maximum_length");
62: 	return_types.emplace_back(LogicalType::INTEGER);
63: 
64: 	names.emplace_back("numeric_precision");
65: 	return_types.emplace_back(LogicalType::INTEGER);
66: 
67: 	names.emplace_back("numeric_precision_radix");
68: 	return_types.emplace_back(LogicalType::INTEGER);
69: 
70: 	names.emplace_back("numeric_scale");
71: 	return_types.emplace_back(LogicalType::INTEGER);
72: 
73: 	return nullptr;
74: }
75: 
76: unique_ptr<FunctionOperatorData> DuckDBColumnsInit(ClientContext &context, const FunctionData *bind_data,
77:                                                    const vector<column_t> &column_ids, TableFilterCollection *filters) {
78: 	auto result = make_unique<DuckDBColumnsData>();
79: 
80: 	// scan all the schemas for tables and views and collect them
81: 	auto schemas = Catalog::GetCatalog(context).schemas->GetEntries<SchemaCatalogEntry>(context);
82: 	for (auto &schema : schemas) {
83: 		schema->Scan(context, CatalogType::TABLE_ENTRY, [&](CatalogEntry *entry) { result->entries.push_back(entry); });
84: 	}
85: 
86: 	// check the temp schema as well
87: 	context.temporary_objects->Scan(context, CatalogType::TABLE_ENTRY,
88: 	                                [&](CatalogEntry *entry) { result->entries.push_back(entry); });
89: 	return move(result);
90: }
91: 
92: namespace { // anonymous namespace for the ColumnHelper classes for working with tables/views
93: 
94: class ColumnHelper {
95: public:
96: 	static unique_ptr<ColumnHelper> Create(CatalogEntry *entry);
97: 
98: 	virtual ~ColumnHelper() {
99: 	}
100: 
101: 	virtual StandardEntry *Entry() = 0;
102: 	virtual idx_t NumColumns() = 0;
103: 	virtual const string &ColumnName(idx_t col) = 0;
104: 	virtual const LogicalType &ColumnType(idx_t col) = 0;
105: 	virtual const Value ColumnDefault(idx_t col) = 0;
106: 	virtual bool IsNullable(idx_t col) = 0;
107: 
108: 	void WriteColumns(idx_t index, idx_t start_col, idx_t end_col, DataChunk &output);
109: };
110: 
111: class TableColumnHelper : public ColumnHelper {
112: public:
113: 	explicit TableColumnHelper(TableCatalogEntry *entry) : entry(entry) {
114: 		for (auto &constraint : entry->constraints) {
115: 			if (constraint->type == ConstraintType::NOT_NULL) {
116: 				auto &not_null = *reinterpret_cast<NotNullConstraint *>(constraint.get());
117: 				not_null_cols.insert(not_null.index);
118: 			}
119: 		}
120: 	}
121: 
122: 	StandardEntry *Entry() override {
123: 		return entry;
124: 	}
125: 	idx_t NumColumns() override {
126: 		return entry->columns.size();
127: 	}
128: 	const string &ColumnName(idx_t col) override {
129: 		return entry->columns[col].name;
130: 	}
131: 	const LogicalType &ColumnType(idx_t col) override {
132: 		return entry->columns[col].type;
133: 	}
134: 	const Value ColumnDefault(idx_t col) override {
135: 		if (entry->columns[col].default_value) {
136: 			return Value(entry->columns[col].default_value->ToString());
137: 		}
138: 		return Value();
139: 	}
140: 	bool IsNullable(idx_t col) override {
141: 		return not_null_cols.find(col) == not_null_cols.end();
142: 	}
143: 
144: private:
145: 	TableCatalogEntry *entry;
146: 	std::set<idx_t> not_null_cols;
147: };
148: 
149: class ViewColumnHelper : public ColumnHelper {
150: public:
151: 	explicit ViewColumnHelper(ViewCatalogEntry *entry) : entry(entry) {
152: 	}
153: 
154: 	StandardEntry *Entry() override {
155: 		return entry;
156: 	}
157: 	idx_t NumColumns() override {
158: 		return entry->types.size();
159: 	}
160: 	const string &ColumnName(idx_t col) override {
161: 		return entry->aliases[col];
162: 	}
163: 	const LogicalType &ColumnType(idx_t col) override {
164: 		return entry->types[col];
165: 	}
166: 	const Value ColumnDefault(idx_t col) override {
167: 		return Value();
168: 	}
169: 	bool IsNullable(idx_t col) override {
170: 		return true;
171: 	}
172: 
173: private:
174: 	ViewCatalogEntry *entry;
175: };
176: 
177: unique_ptr<ColumnHelper> ColumnHelper::Create(CatalogEntry *entry) {
178: 	switch (entry->type) {
179: 	case CatalogType::TABLE_ENTRY:
180: 		return make_unique<TableColumnHelper>((TableCatalogEntry *)entry);
181: 	case CatalogType::VIEW_ENTRY:
182: 		return make_unique<ViewColumnHelper>((ViewCatalogEntry *)entry);
183: 	default:
184: 		throw NotImplementedException("Unsupported catalog type for duckdb_columns");
185: 	}
186: }
187: 
188: void ColumnHelper::WriteColumns(idx_t start_index, idx_t start_col, idx_t end_col, DataChunk &output) {
189: 	for (idx_t i = start_col; i < end_col; i++) {
190: 		auto index = start_index + (i - start_col);
191: 		auto &entry = *Entry();
192: 
193: 		// schema_oid, BIGINT
194: 		output.SetValue(0, index, Value::BIGINT(entry.schema->oid));
195: 		// schema_name, VARCHAR
196: 		output.SetValue(1, index, entry.schema->name);
197: 		// table_oid, BIGINT
198: 		output.SetValue(2, index, Value::BIGINT(entry.oid));
199: 		// table_name, VARCHAR
200: 		output.SetValue(3, index, entry.name);
201: 		// column_name, VARCHAR
202: 		output.SetValue(4, index, Value(ColumnName(i)));
203: 		// column_index, INTEGER
204: 		output.SetValue(5, index, Value::INTEGER(i + 1));
205: 		// internal, BOOLEAN
206: 		output.SetValue(6, index, Value::BOOLEAN(entry.internal));
207: 		// column_default, VARCHAR
208: 		output.SetValue(7, index, Value(ColumnDefault(i)));
209: 		// is_nullable, BOOLEAN
210: 		output.SetValue(8, index, Value::BOOLEAN(IsNullable(i)));
211: 		// data_type, VARCHAR
212: 		const LogicalType &type = ColumnType(i);
213: 		output.SetValue(9, index, Value(type.ToString()));
214: 		// data_type_id, BIGINT
215: 		output.SetValue(10, index, Value::BIGINT(int(type.id())));
216: 		if (type == LogicalType::VARCHAR) {
217: 			// FIXME: need check constraints in place to set this correctly
218: 			// character_maximum_length, INTEGER
219: 			output.SetValue(11, index, Value());
220: 		} else {
221: 			// "character_maximum_length", PhysicalType::INTEGER
222: 			output.SetValue(11, index, Value());
223: 		}
224: 
225: 		Value numeric_precision, numeric_scale, numeric_precision_radix;
226: 		switch (type.id()) {
227: 		case LogicalTypeId::DECIMAL:
228: 			numeric_precision = Value::INTEGER(DecimalType::GetWidth(type));
229: 			numeric_scale = Value::INTEGER(DecimalType::GetScale(type));
230: 			numeric_precision_radix = Value::INTEGER(10);
231: 			break;
232: 		case LogicalTypeId::HUGEINT:
233: 			numeric_precision = Value::INTEGER(128);
234: 			numeric_scale = Value::INTEGER(0);
235: 			numeric_precision_radix = Value::INTEGER(2);
236: 			break;
237: 		case LogicalTypeId::BIGINT:
238: 			numeric_precision = Value::INTEGER(64);
239: 			numeric_scale = Value::INTEGER(0);
240: 			numeric_precision_radix = Value::INTEGER(2);
241: 			break;
242: 		case LogicalTypeId::INTEGER:
243: 			numeric_precision = Value::INTEGER(32);
244: 			numeric_scale = Value::INTEGER(0);
245: 			numeric_precision_radix = Value::INTEGER(2);
246: 			break;
247: 		case LogicalTypeId::SMALLINT:
248: 			numeric_precision = Value::INTEGER(16);
249: 			numeric_scale = Value::INTEGER(0);
250: 			numeric_precision_radix = Value::INTEGER(2);
251: 			break;
252: 		case LogicalTypeId::TINYINT:
253: 			numeric_precision = Value::INTEGER(8);
254: 			numeric_scale = Value::INTEGER(0);
255: 			numeric_precision_radix = Value::INTEGER(2);
256: 			break;
257: 		case LogicalTypeId::FLOAT:
258: 			numeric_precision = Value::INTEGER(24);
259: 			numeric_scale = Value::INTEGER(0);
260: 			numeric_precision_radix = Value::INTEGER(2);
261: 			break;
262: 		case LogicalTypeId::DOUBLE:
263: 			numeric_precision = Value::INTEGER(53);
264: 			numeric_scale = Value::INTEGER(0);
265: 			numeric_precision_radix = Value::INTEGER(2);
266: 			break;
267: 		default:
268: 			numeric_precision = Value();
269: 			numeric_scale = Value();
270: 			numeric_precision_radix = Value();
271: 			break;
272: 		}
273: 
274: 		// numeric_precision, INTEGER
275: 		output.SetValue(12, index, numeric_precision);
276: 		// numeric_precision_radix, INTEGER
277: 		output.SetValue(13, index, numeric_precision_radix);
278: 		// numeric_scale, INTEGER
279: 		output.SetValue(14, index, numeric_scale);
280: 	}
281: }
282: 
283: } // anonymous namespace
284: 
285: void DuckDBColumnsFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
286:                            DataChunk *input, DataChunk &output) {
287: 	auto &data = (DuckDBColumnsData &)*operator_state;
288: 	if (data.offset >= data.entries.size()) {
289: 		// finished returning values
290: 		return;
291: 	}
292: 
293: 	// We need to track the offset of the relation we're writing as well as the last column
294: 	// we wrote from that relation (if any); it's possible that we can fill up the output
295: 	// with a partial list of columns from a relation and will need to pick up processing the
296: 	// next chunk at the same spot.
297: 	idx_t next = data.offset;
298: 	idx_t column_offset = data.column_offset;
299: 	idx_t index = 0;
300: 	while (next < data.entries.size() && index < STANDARD_VECTOR_SIZE) {
301: 		auto column_helper = ColumnHelper::Create(data.entries[next]);
302: 		idx_t columns = column_helper->NumColumns();
303: 
304: 		// Check to see if we are going to exceed the maximum index for a DataChunk
305: 		if (index + (columns - column_offset) > STANDARD_VECTOR_SIZE) {
306: 			idx_t column_limit = column_offset + (STANDARD_VECTOR_SIZE - index);
307: 			output.SetCardinality(STANDARD_VECTOR_SIZE);
308: 			column_helper->WriteColumns(index, column_offset, column_limit, output);
309: 
310: 			// Make the current column limit the column offset when we process the next chunk
311: 			column_offset = column_limit;
312: 			break;
313: 		} else {
314: 			// Otherwise, write all of the columns from the current relation and
315: 			// then move on to the next one.
316: 			output.SetCardinality(index + (columns - column_offset));
317: 			column_helper->WriteColumns(index, column_offset, columns, output);
318: 			index += columns - column_offset;
319: 			next++;
320: 			column_offset = 0;
321: 		}
322: 	}
323: 	data.offset = next;
324: 	data.column_offset = column_offset;
325: }
326: 
327: void DuckDBColumnsFun::RegisterFunction(BuiltinFunctions &set) {
328: 	set.AddFunction(TableFunction("duckdb_columns", {}, DuckDBColumnsFunction, DuckDBColumnsBind, DuckDBColumnsInit));
329: }
330: 
331: } // namespace duckdb
[end of src/function/table/system/duckdb_columns.cpp]
[start of src/function/table/system/duckdb_constraints.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb/parser/constraint.hpp"
9: #include "duckdb/parser/constraints/check_constraint.hpp"
10: #include "duckdb/parser/constraints/unique_constraint.hpp"
11: #include "duckdb/planner/constraints/bound_unique_constraint.hpp"
12: #include "duckdb/planner/constraints/bound_check_constraint.hpp"
13: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
14: #include "duckdb/planner/constraints/bound_foreign_key_constraint.hpp"
15: #include "duckdb/storage/data_table.hpp"
16: 
17: namespace duckdb {
18: 
19: struct DuckDBConstraintsData : public FunctionOperatorData {
20: 	DuckDBConstraintsData() : offset(0), constraint_offset(0) {
21: 	}
22: 
23: 	vector<CatalogEntry *> entries;
24: 	idx_t offset;
25: 	idx_t constraint_offset;
26: };
27: 
28: static unique_ptr<FunctionData> DuckDBConstraintsBind(ClientContext &context, vector<Value> &inputs,
29:                                                       named_parameter_map_t &named_parameters,
30:                                                       vector<LogicalType> &input_table_types,
31:                                                       vector<string> &input_table_names,
32:                                                       vector<LogicalType> &return_types, vector<string> &names) {
33: 	names.emplace_back("schema_name");
34: 	return_types.emplace_back(LogicalType::VARCHAR);
35: 
36: 	names.emplace_back("schema_oid");
37: 	return_types.emplace_back(LogicalType::BIGINT);
38: 
39: 	names.emplace_back("table_name");
40: 	return_types.emplace_back(LogicalType::VARCHAR);
41: 
42: 	names.emplace_back("table_oid");
43: 	return_types.emplace_back(LogicalType::BIGINT);
44: 
45: 	names.emplace_back("constraint_index");
46: 	return_types.emplace_back(LogicalType::BIGINT);
47: 
48: 	// CHECK, PRIMARY KEY or UNIQUE
49: 	names.emplace_back("constraint_type");
50: 	return_types.emplace_back(LogicalType::VARCHAR);
51: 
52: 	names.emplace_back("constraint_text");
53: 	return_types.emplace_back(LogicalType::VARCHAR);
54: 
55: 	names.emplace_back("expression");
56: 	return_types.emplace_back(LogicalType::VARCHAR);
57: 
58: 	names.emplace_back("constraint_column_indexes");
59: 	;
60: 	return_types.push_back(LogicalType::LIST(LogicalType::BIGINT));
61: 
62: 	names.emplace_back("constraint_column_names");
63: 	return_types.push_back(LogicalType::LIST(LogicalType::VARCHAR));
64: 
65: 	return nullptr;
66: }
67: 
68: unique_ptr<FunctionOperatorData> DuckDBConstraintsInit(ClientContext &context, const FunctionData *bind_data,
69:                                                        const vector<column_t> &column_ids,
70:                                                        TableFilterCollection *filters) {
71: 	auto result = make_unique<DuckDBConstraintsData>();
72: 
73: 	// scan all the schemas for tables and collect themand collect them
74: 	auto schemas = Catalog::GetCatalog(context).schemas->GetEntries<SchemaCatalogEntry>(context);
75: 	for (auto &schema : schemas) {
76: 		schema->Scan(context, CatalogType::TABLE_ENTRY, [&](CatalogEntry *entry) { result->entries.push_back(entry); });
77: 	};
78: 
79: 	// check the temp schema as well
80: 	context.temporary_objects->Scan(context, CatalogType::TABLE_ENTRY,
81: 	                                [&](CatalogEntry *entry) { result->entries.push_back(entry); });
82: 	return move(result);
83: }
84: 
85: void DuckDBConstraintsFunction(ClientContext &context, const FunctionData *bind_data,
86:                                FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
87: 	auto &data = (DuckDBConstraintsData &)*operator_state;
88: 	if (data.offset >= data.entries.size()) {
89: 		// finished returning values
90: 		return;
91: 	}
92: 	// start returning values
93: 	// either fill up the chunk or return all the remaining columns
94: 	idx_t count = 0;
95: 	while (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {
96: 		auto &entry = data.entries[data.offset];
97: 
98: 		if (entry->type != CatalogType::TABLE_ENTRY) {
99: 			data.offset++;
100: 			continue;
101: 		}
102: 
103: 		auto &table = (TableCatalogEntry &)*entry;
104: 		for (; data.constraint_offset < table.constraints.size() && count < STANDARD_VECTOR_SIZE;
105: 		     data.constraint_offset++) {
106: 			auto &constraint = table.constraints[data.constraint_offset];
107: 			// return values:
108: 			// schema_name, LogicalType::VARCHAR
109: 			output.SetValue(0, count, Value(table.schema->name));
110: 			// schema_oid, LogicalType::BIGINT
111: 			output.SetValue(1, count, Value::BIGINT(table.schema->oid));
112: 			// table_name, LogicalType::VARCHAR
113: 			output.SetValue(2, count, Value(table.name));
114: 			// table_oid, LogicalType::BIGINT
115: 			output.SetValue(3, count, Value::BIGINT(table.oid));
116: 
117: 			// constraint_index, BIGINT
118: 			output.SetValue(4, count, Value::BIGINT(data.constraint_offset));
119: 
120: 			// constraint_type, VARCHAR
121: 			string constraint_type;
122: 			switch (constraint->type) {
123: 			case ConstraintType::CHECK:
124: 				constraint_type = "CHECK";
125: 				break;
126: 			case ConstraintType::UNIQUE: {
127: 				auto &unique = (UniqueConstraint &)*constraint;
128: 				constraint_type = unique.is_primary_key ? "PRIMARY KEY" : "UNIQUE";
129: 				break;
130: 			}
131: 			case ConstraintType::NOT_NULL:
132: 				constraint_type = "NOT NULL";
133: 				break;
134: 			case ConstraintType::FOREIGN_KEY:
135: 				constraint_type = "FOREIGN KEY";
136: 				break;
137: 			default:
138: 				throw NotImplementedException("Unimplemented constraint for duckdb_constraints");
139: 			}
140: 			output.SetValue(5, count, Value(constraint_type));
141: 
142: 			// constraint_text, VARCHAR
143: 			output.SetValue(6, count, Value(constraint->ToString()));
144: 
145: 			// expression, VARCHAR
146: 			Value expression_text;
147: 			if (constraint->type == ConstraintType::CHECK) {
148: 				auto &check = (CheckConstraint &)*constraint;
149: 				expression_text = Value(check.expression->ToString());
150: 			}
151: 			output.SetValue(7, count, expression_text);
152: 
153: 			auto &bound_constraint = (BoundConstraint &)*table.bound_constraints[data.constraint_offset];
154: 			vector<column_t> column_index_list;
155: 			switch (bound_constraint.type) {
156: 			case ConstraintType::CHECK: {
157: 				auto &bound_check = (BoundCheckConstraint &)bound_constraint;
158: 				for (auto &col_idx : bound_check.bound_columns) {
159: 					column_index_list.push_back(col_idx);
160: 				}
161: 				break;
162: 			}
163: 			case ConstraintType::UNIQUE: {
164: 				auto &bound_unique = (BoundUniqueConstraint &)bound_constraint;
165: 				for (auto &col_idx : bound_unique.keys) {
166: 					column_index_list.push_back(column_t(col_idx));
167: 				}
168: 				break;
169: 			}
170: 			case ConstraintType::NOT_NULL: {
171: 				auto &bound_not_null = (BoundNotNullConstraint &)bound_constraint;
172: 				column_index_list.push_back(bound_not_null.index);
173: 				break;
174: 			}
175: 			case ConstraintType::FOREIGN_KEY: {
176: 				auto &bound_foreign_key = (BoundForeignKeyConstraint &)bound_constraint;
177: 				for (auto &col_idx : bound_foreign_key.info.fk_keys) {
178: 					column_index_list.push_back(column_t(col_idx));
179: 				}
180: 				break;
181: 			}
182: 			default:
183: 				throw NotImplementedException("Unimplemented constraint for duckdb_constraints");
184: 			}
185: 
186: 			vector<Value> index_list;
187: 			vector<Value> column_name_list;
188: 			for (auto column_index : column_index_list) {
189: 				index_list.push_back(Value::BIGINT(column_index));
190: 				column_name_list.emplace_back(table.columns[column_index].name);
191: 			}
192: 
193: 			// constraint_column_indexes, LIST
194: 			output.SetValue(8, count, Value::LIST(move(index_list)));
195: 
196: 			// constraint_column_names, LIST
197: 			output.SetValue(9, count, Value::LIST(move(column_name_list)));
198: 
199: 			count++;
200: 		}
201: 		if (data.constraint_offset >= table.constraints.size()) {
202: 			data.constraint_offset = 0;
203: 			data.offset++;
204: 		}
205: 	}
206: 	output.SetCardinality(count);
207: }
208: 
209: void DuckDBConstraintsFun::RegisterFunction(BuiltinFunctions &set) {
210: 	set.AddFunction(TableFunction("duckdb_constraints", {}, DuckDBConstraintsFunction, DuckDBConstraintsBind,
211: 	                              DuckDBConstraintsInit));
212: }
213: 
214: } // namespace duckdb
[end of src/function/table/system/duckdb_constraints.cpp]
[start of src/function/table/system/duckdb_dependencies.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/dependency_manager.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/main/client_context.hpp"
7: 
8: namespace duckdb {
9: 
10: struct DependencyInformation {
11: 	CatalogEntry *object;
12: 	CatalogEntry *dependent;
13: 	DependencyType type;
14: };
15: 
16: struct DuckDBDependenciesData : public FunctionOperatorData {
17: 	DuckDBDependenciesData() : offset(0) {
18: 	}
19: 
20: 	vector<DependencyInformation> entries;
21: 	idx_t offset;
22: };
23: 
24: static unique_ptr<FunctionData> DuckDBDependenciesBind(ClientContext &context, vector<Value> &inputs,
25:                                                        named_parameter_map_t &named_parameters,
26:                                                        vector<LogicalType> &input_table_types,
27:                                                        vector<string> &input_table_names,
28:                                                        vector<LogicalType> &return_types, vector<string> &names) {
29: 	names.emplace_back("classid");
30: 	return_types.emplace_back(LogicalType::BIGINT);
31: 
32: 	names.emplace_back("objid");
33: 	return_types.emplace_back(LogicalType::BIGINT);
34: 
35: 	names.emplace_back("objsubid");
36: 	return_types.emplace_back(LogicalType::INTEGER);
37: 
38: 	names.emplace_back("refclassid");
39: 	return_types.emplace_back(LogicalType::BIGINT);
40: 
41: 	names.emplace_back("refobjid");
42: 	return_types.emplace_back(LogicalType::BIGINT);
43: 
44: 	names.emplace_back("refobjsubid");
45: 	return_types.emplace_back(LogicalType::INTEGER);
46: 
47: 	names.emplace_back("deptype");
48: 	return_types.emplace_back(LogicalType::VARCHAR);
49: 
50: 	return nullptr;
51: }
52: 
53: unique_ptr<FunctionOperatorData> DuckDBDependenciesInit(ClientContext &context, const FunctionData *bind_data,
54:                                                         const vector<column_t> &column_ids,
55:                                                         TableFilterCollection *filters) {
56: 	auto result = make_unique<DuckDBDependenciesData>();
57: 
58: 	// scan all the schemas and collect them
59: 	auto &catalog = Catalog::GetCatalog(context);
60: 	auto &dependency_manager = catalog.GetDependencyManager();
61: 	dependency_manager.Scan([&](CatalogEntry *obj, CatalogEntry *dependent, DependencyType type) {
62: 		DependencyInformation info;
63: 		info.object = obj;
64: 		info.dependent = dependent;
65: 		info.type = type;
66: 		result->entries.push_back(info);
67: 	});
68: 
69: 	return move(result);
70: }
71: 
72: void DuckDBDependenciesFunction(ClientContext &context, const FunctionData *bind_data,
73:                                 FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
74: 	auto &data = (DuckDBDependenciesData &)*operator_state;
75: 	if (data.offset >= data.entries.size()) {
76: 		// finished returning values
77: 		return;
78: 	}
79: 	// start returning values
80: 	// either fill up the chunk or return all the remaining columns
81: 	idx_t count = 0;
82: 	while (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {
83: 		auto &entry = data.entries[data.offset];
84: 
85: 		// return values:
86: 		// classid, LogicalType::BIGINT
87: 		output.SetValue(0, count, Value::BIGINT(0));
88: 		// objid, LogicalType::BIGINT
89: 		output.SetValue(1, count, Value::BIGINT(entry.object->oid));
90: 		// objsubid, LogicalType::INTEGER
91: 		output.SetValue(2, count, Value::INTEGER(0));
92: 		// refclassid, LogicalType::BIGINT
93: 		output.SetValue(3, count, Value::BIGINT(0));
94: 		// refobjid, LogicalType::BIGINT
95: 		output.SetValue(4, count, Value::BIGINT(entry.dependent->oid));
96: 		// refobjsubid, LogicalType::INTEGER
97: 		output.SetValue(5, count, Value::INTEGER(0));
98: 		// deptype, LogicalType::VARCHAR
99: 		string dependency_type_str;
100: 		switch (entry.type) {
101: 		case DependencyType::DEPENDENCY_REGULAR:
102: 			dependency_type_str = "n";
103: 			break;
104: 		case DependencyType::DEPENDENCY_AUTOMATIC:
105: 			dependency_type_str = "a";
106: 			break;
107: 		default:
108: 			throw NotImplementedException("Unimplemented dependency type");
109: 		}
110: 		output.SetValue(6, count, Value(dependency_type_str));
111: 
112: 		data.offset++;
113: 		count++;
114: 	}
115: 	output.SetCardinality(count);
116: }
117: 
118: void DuckDBDependenciesFun::RegisterFunction(BuiltinFunctions &set) {
119: 	set.AddFunction(TableFunction("duckdb_dependencies", {}, DuckDBDependenciesFunction, DuckDBDependenciesBind,
120: 	                              DuckDBDependenciesInit));
121: }
122: 
123: } // namespace duckdb
[end of src/function/table/system/duckdb_dependencies.cpp]
[start of src/function/table/system/duckdb_functions.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/scalar_function_catalog_entry.hpp"
6: #include "duckdb/catalog/catalog_entry/aggregate_function_catalog_entry.hpp"
7: #include "duckdb/catalog/catalog_entry/macro_catalog_entry.hpp"
8: #include "duckdb/catalog/catalog_entry/table_function_catalog_entry.hpp"
9: #include "duckdb/catalog/catalog_entry/pragma_function_catalog_entry.hpp"
10: #include "duckdb/parser/expression/columnref_expression.hpp"
11: #include "duckdb/common/algorithm.hpp"
12: 
13: namespace duckdb {
14: 
15: struct DuckDBFunctionsData : public FunctionOperatorData {
16: 	DuckDBFunctionsData() : offset(0), offset_in_entry(0) {
17: 	}
18: 
19: 	vector<CatalogEntry *> entries;
20: 	idx_t offset;
21: 	idx_t offset_in_entry;
22: };
23: 
24: static unique_ptr<FunctionData> DuckDBFunctionsBind(ClientContext &context, vector<Value> &inputs,
25:                                                     named_parameter_map_t &named_parameters,
26:                                                     vector<LogicalType> &input_table_types,
27:                                                     vector<string> &input_table_names,
28:                                                     vector<LogicalType> &return_types, vector<string> &names) {
29: 	names.emplace_back("schema_name");
30: 	return_types.emplace_back(LogicalType::VARCHAR);
31: 
32: 	names.emplace_back("function_name");
33: 	return_types.emplace_back(LogicalType::VARCHAR);
34: 
35: 	names.emplace_back("function_type");
36: 	return_types.emplace_back(LogicalType::VARCHAR);
37: 
38: 	names.emplace_back("description");
39: 	return_types.emplace_back(LogicalType::VARCHAR);
40: 
41: 	names.emplace_back("return_type");
42: 	return_types.emplace_back(LogicalType::VARCHAR);
43: 
44: 	names.emplace_back("parameters");
45: 	return_types.push_back(LogicalType::LIST(LogicalType::VARCHAR));
46: 
47: 	names.emplace_back("parameter_types");
48: 	return_types.push_back(LogicalType::LIST(LogicalType::VARCHAR));
49: 
50: 	names.emplace_back("varargs");
51: 	return_types.emplace_back(LogicalType::VARCHAR);
52: 
53: 	names.emplace_back("macro_definition");
54: 	return_types.emplace_back(LogicalType::VARCHAR);
55: 
56: 	return nullptr;
57: }
58: 
59: static void ExtractFunctionsFromSchema(ClientContext &context, SchemaCatalogEntry &schema,
60:                                        DuckDBFunctionsData &result) {
61: 	schema.Scan(context, CatalogType::SCALAR_FUNCTION_ENTRY,
62: 	            [&](CatalogEntry *entry) { result.entries.push_back(entry); });
63: 	schema.Scan(context, CatalogType::TABLE_FUNCTION_ENTRY,
64: 	            [&](CatalogEntry *entry) { result.entries.push_back(entry); });
65: 	schema.Scan(context, CatalogType::PRAGMA_FUNCTION_ENTRY,
66: 	            [&](CatalogEntry *entry) { result.entries.push_back(entry); });
67: }
68: 
69: unique_ptr<FunctionOperatorData> DuckDBFunctionsInit(ClientContext &context, const FunctionData *bind_data,
70:                                                      const vector<column_t> &column_ids,
71:                                                      TableFilterCollection *filters) {
72: 	auto result = make_unique<DuckDBFunctionsData>();
73: 
74: 	// scan all the schemas for tables and collect themand collect them
75: 	auto schemas = Catalog::GetCatalog(context).schemas->GetEntries<SchemaCatalogEntry>(context);
76: 	for (auto &schema : schemas) {
77: 		ExtractFunctionsFromSchema(context, *schema, *result);
78: 	};
79: 	ExtractFunctionsFromSchema(context, *context.temporary_objects, *result);
80: 
81: 	std::sort(result->entries.begin(), result->entries.end(),
82: 	          [&](CatalogEntry *a, CatalogEntry *b) { return (int)a->type < (int)b->type; });
83: 	return move(result);
84: }
85: 
86: struct ScalarFunctionExtractor {
87: 	static idx_t FunctionCount(ScalarFunctionCatalogEntry &entry) {
88: 		return entry.functions.size();
89: 	}
90: 
91: 	static Value GetFunctionType() {
92: 		return Value("scalar");
93: 	}
94: 
95: 	static Value GetFunctionDescription(ScalarFunctionCatalogEntry &entry, idx_t offset) {
96: 		return Value();
97: 	}
98: 
99: 	static Value GetReturnType(ScalarFunctionCatalogEntry &entry, idx_t offset) {
100: 		return Value(entry.functions[offset].return_type.ToString());
101: 	}
102: 
103: 	static Value GetParameters(ScalarFunctionCatalogEntry &entry, idx_t offset) {
104: 		vector<Value> results;
105: 		for (idx_t i = 0; i < entry.functions[offset].arguments.size(); i++) {
106: 			results.emplace_back("col" + to_string(i));
107: 		}
108: 		return Value::LIST(LogicalType::VARCHAR, move(results));
109: 	}
110: 
111: 	static Value GetParameterTypes(ScalarFunctionCatalogEntry &entry, idx_t offset) {
112: 		vector<Value> results;
113: 		for (idx_t i = 0; i < entry.functions[offset].arguments.size(); i++) {
114: 			results.emplace_back(entry.functions[offset].arguments[i].ToString());
115: 		}
116: 		return Value::LIST(LogicalType::VARCHAR, move(results));
117: 	}
118: 
119: 	static Value GetVarArgs(ScalarFunctionCatalogEntry &entry, idx_t offset) {
120: 		return entry.functions[offset].varargs.id() == LogicalTypeId::INVALID
121: 		           ? Value()
122: 		           : Value(entry.functions[offset].varargs.ToString());
123: 	}
124: 
125: 	static Value GetMacroDefinition(ScalarFunctionCatalogEntry &entry, idx_t offset) {
126: 		return Value();
127: 	}
128: };
129: 
130: struct AggregateFunctionExtractor {
131: 	static idx_t FunctionCount(AggregateFunctionCatalogEntry &entry) {
132: 		return entry.functions.size();
133: 	}
134: 
135: 	static Value GetFunctionType() {
136: 		return Value("aggregate");
137: 	}
138: 
139: 	static Value GetFunctionDescription(AggregateFunctionCatalogEntry &entry, idx_t offset) {
140: 		return Value();
141: 	}
142: 
143: 	static Value GetReturnType(AggregateFunctionCatalogEntry &entry, idx_t offset) {
144: 		return Value(entry.functions[offset].return_type.ToString());
145: 	}
146: 
147: 	static Value GetParameters(AggregateFunctionCatalogEntry &entry, idx_t offset) {
148: 		vector<Value> results;
149: 		for (idx_t i = 0; i < entry.functions[offset].arguments.size(); i++) {
150: 			results.emplace_back("col" + to_string(i));
151: 		}
152: 		return Value::LIST(LogicalType::VARCHAR, move(results));
153: 	}
154: 
155: 	static Value GetParameterTypes(AggregateFunctionCatalogEntry &entry, idx_t offset) {
156: 		vector<Value> results;
157: 		for (idx_t i = 0; i < entry.functions[offset].arguments.size(); i++) {
158: 			results.emplace_back(entry.functions[offset].arguments[i].ToString());
159: 		}
160: 		return Value::LIST(LogicalType::VARCHAR, move(results));
161: 	}
162: 
163: 	static Value GetVarArgs(AggregateFunctionCatalogEntry &entry, idx_t offset) {
164: 		return entry.functions[offset].varargs.id() == LogicalTypeId::INVALID
165: 		           ? Value()
166: 		           : Value(entry.functions[offset].varargs.ToString());
167: 	}
168: 
169: 	static Value GetMacroDefinition(AggregateFunctionCatalogEntry &entry, idx_t offset) {
170: 		return Value();
171: 	}
172: };
173: 
174: struct MacroExtractor {
175: 	static idx_t FunctionCount(MacroCatalogEntry &entry) {
176: 		return 1;
177: 	}
178: 
179: 	static Value GetFunctionType() {
180: 		return Value("macro");
181: 	}
182: 
183: 	static Value GetFunctionDescription(MacroCatalogEntry &entry, idx_t offset) {
184: 		return Value();
185: 	}
186: 
187: 	static Value GetReturnType(MacroCatalogEntry &entry, idx_t offset) {
188: 		return Value();
189: 	}
190: 
191: 	static Value GetParameters(MacroCatalogEntry &entry, idx_t offset) {
192: 		vector<Value> results;
193: 		for (auto &param : entry.function->parameters) {
194: 			D_ASSERT(param->type == ExpressionType::COLUMN_REF);
195: 			auto &colref = (ColumnRefExpression &)*param;
196: 			results.emplace_back(colref.GetColumnName());
197: 		}
198: 		for (auto &param_entry : entry.function->default_parameters) {
199: 			results.emplace_back(param_entry.first);
200: 		}
201: 		return Value::LIST(LogicalType::VARCHAR, move(results));
202: 	}
203: 
204: 	static Value GetParameterTypes(MacroCatalogEntry &entry, idx_t offset) {
205: 		vector<Value> results;
206: 		for (idx_t i = 0; i < entry.function->parameters.size(); i++) {
207: 			results.emplace_back(LogicalType::VARCHAR);
208: 		}
209: 		for (idx_t i = 0; i < entry.function->default_parameters.size(); i++) {
210: 			results.emplace_back(LogicalType::VARCHAR);
211: 		}
212: 		return Value::LIST(LogicalType::VARCHAR, move(results));
213: 	}
214: 
215: 	static Value GetVarArgs(MacroCatalogEntry &entry, idx_t offset) {
216: 		return Value();
217: 	}
218: 
219: 	static Value GetMacroDefinition(MacroCatalogEntry &entry, idx_t offset) {
220: 		return entry.function->expression->ToString();
221: 	}
222: };
223: 
224: struct TableFunctionExtractor {
225: 	static idx_t FunctionCount(TableFunctionCatalogEntry &entry) {
226: 		return entry.functions.size();
227: 	}
228: 
229: 	static Value GetFunctionType() {
230: 		return Value("table");
231: 	}
232: 
233: 	static Value GetFunctionDescription(TableFunctionCatalogEntry &entry, idx_t offset) {
234: 		return Value();
235: 	}
236: 
237: 	static Value GetReturnType(TableFunctionCatalogEntry &entry, idx_t offset) {
238: 		return Value();
239: 	}
240: 
241: 	static Value GetParameters(TableFunctionCatalogEntry &entry, idx_t offset) {
242: 		vector<Value> results;
243: 		for (idx_t i = 0; i < entry.functions[offset].arguments.size(); i++) {
244: 			results.emplace_back("col" + to_string(i));
245: 		}
246: 		for (auto &param : entry.functions[offset].named_parameters) {
247: 			results.emplace_back(param.first);
248: 		}
249: 		return Value::LIST(LogicalType::VARCHAR, move(results));
250: 	}
251: 
252: 	static Value GetParameterTypes(TableFunctionCatalogEntry &entry, idx_t offset) {
253: 		vector<Value> results;
254: 		for (idx_t i = 0; i < entry.functions[offset].arguments.size(); i++) {
255: 			results.emplace_back(entry.functions[offset].arguments[i].ToString());
256: 		}
257: 		for (auto &param : entry.functions[offset].named_parameters) {
258: 			results.emplace_back(param.second.ToString());
259: 		}
260: 		return Value::LIST(LogicalType::VARCHAR, move(results));
261: 	}
262: 
263: 	static Value GetVarArgs(TableFunctionCatalogEntry &entry, idx_t offset) {
264: 		return entry.functions[offset].varargs.id() == LogicalTypeId::INVALID
265: 		           ? Value()
266: 		           : Value(entry.functions[offset].varargs.ToString());
267: 	}
268: 
269: 	static Value GetMacroDefinition(TableFunctionCatalogEntry &entry, idx_t offset) {
270: 		return Value();
271: 	}
272: };
273: 
274: struct PragmaFunctionExtractor {
275: 	static idx_t FunctionCount(PragmaFunctionCatalogEntry &entry) {
276: 		return entry.functions.size();
277: 	}
278: 
279: 	static Value GetFunctionType() {
280: 		return Value("pragma");
281: 	}
282: 
283: 	static Value GetFunctionDescription(PragmaFunctionCatalogEntry &entry, idx_t offset) {
284: 		return Value();
285: 	}
286: 
287: 	static Value GetReturnType(PragmaFunctionCatalogEntry &entry, idx_t offset) {
288: 		return Value();
289: 	}
290: 
291: 	static Value GetParameters(PragmaFunctionCatalogEntry &entry, idx_t offset) {
292: 		vector<Value> results;
293: 		for (idx_t i = 0; i < entry.functions[offset].arguments.size(); i++) {
294: 			results.emplace_back("col" + to_string(i));
295: 		}
296: 		for (auto &param : entry.functions[offset].named_parameters) {
297: 			results.emplace_back(param.first);
298: 		}
299: 		return Value::LIST(LogicalType::VARCHAR, move(results));
300: 	}
301: 
302: 	static Value GetParameterTypes(PragmaFunctionCatalogEntry &entry, idx_t offset) {
303: 		vector<Value> results;
304: 		for (idx_t i = 0; i < entry.functions[offset].arguments.size(); i++) {
305: 			results.emplace_back(entry.functions[offset].arguments[i].ToString());
306: 		}
307: 		for (auto &param : entry.functions[offset].named_parameters) {
308: 			results.emplace_back(param.second.ToString());
309: 		}
310: 		return Value::LIST(LogicalType::VARCHAR, move(results));
311: 	}
312: 
313: 	static Value GetVarArgs(PragmaFunctionCatalogEntry &entry, idx_t offset) {
314: 		return entry.functions[offset].varargs.id() == LogicalTypeId::INVALID
315: 		           ? Value()
316: 		           : Value(entry.functions[offset].varargs.ToString());
317: 	}
318: 
319: 	static Value GetMacroDefinition(PragmaFunctionCatalogEntry &entry, idx_t offset) {
320: 		return Value();
321: 	}
322: };
323: 
324: template <class T, class OP>
325: bool ExtractFunctionData(StandardEntry *entry, idx_t function_idx, DataChunk &output, idx_t output_offset) {
326: 	auto &function = (T &)*entry;
327: 	// schema_name, LogicalType::VARCHAR
328: 	output.SetValue(0, output_offset, Value(entry->schema->name));
329: 
330: 	// function_name, LogicalType::VARCHAR
331: 	output.SetValue(1, output_offset, Value(entry->name));
332: 
333: 	// function_type, LogicalType::VARCHAR
334: 	output.SetValue(2, output_offset, Value(OP::GetFunctionType()));
335: 
336: 	// function_description, LogicalType::VARCHAR
337: 	output.SetValue(3, output_offset, OP::GetFunctionDescription(function, function_idx));
338: 
339: 	// return_type, LogicalType::VARCHAR
340: 	output.SetValue(4, output_offset, OP::GetReturnType(function, function_idx));
341: 
342: 	// parameters, LogicalType::LIST(LogicalType::VARCHAR)
343: 	output.SetValue(5, output_offset, OP::GetParameters(function, function_idx));
344: 
345: 	// parameter_types, LogicalType::LIST(LogicalType::VARCHAR)
346: 	output.SetValue(6, output_offset, OP::GetParameterTypes(function, function_idx));
347: 
348: 	// varargs, LogicalType::VARCHAR
349: 	output.SetValue(7, output_offset, OP::GetVarArgs(function, function_idx));
350: 
351: 	// macro_definition, LogicalType::VARCHAR
352: 	output.SetValue(8, output_offset, OP::GetMacroDefinition(function, function_idx));
353: 
354: 	return function_idx + 1 == OP::FunctionCount(function);
355: }
356: 
357: void DuckDBFunctionsFunction(ClientContext &context, const FunctionData *bind_data,
358:                              FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
359: 	auto &data = (DuckDBFunctionsData &)*operator_state;
360: 	if (data.offset >= data.entries.size()) {
361: 		// finished returning values
362: 		return;
363: 	}
364: 	// start returning values
365: 	// either fill up the chunk or return all the remaining columns
366: 	idx_t count = 0;
367: 	while (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {
368: 		auto &entry = data.entries[data.offset];
369: 		auto standard_entry = (StandardEntry *)entry;
370: 		bool finished = false;
371: 
372: 		switch (entry->type) {
373: 		case CatalogType::SCALAR_FUNCTION_ENTRY:
374: 			finished = ExtractFunctionData<ScalarFunctionCatalogEntry, ScalarFunctionExtractor>(
375: 			    standard_entry, data.offset_in_entry, output, count);
376: 			break;
377: 		case CatalogType::AGGREGATE_FUNCTION_ENTRY:
378: 			finished = ExtractFunctionData<AggregateFunctionCatalogEntry, AggregateFunctionExtractor>(
379: 			    standard_entry, data.offset_in_entry, output, count);
380: 			break;
381: 		case CatalogType::MACRO_ENTRY:
382: 			finished = ExtractFunctionData<MacroCatalogEntry, MacroExtractor>(standard_entry, data.offset_in_entry,
383: 			                                                                  output, count);
384: 			break;
385: 		case CatalogType::TABLE_FUNCTION_ENTRY:
386: 			finished = ExtractFunctionData<TableFunctionCatalogEntry, TableFunctionExtractor>(
387: 			    standard_entry, data.offset_in_entry, output, count);
388: 			break;
389: 		case CatalogType::PRAGMA_FUNCTION_ENTRY:
390: 			finished = ExtractFunctionData<PragmaFunctionCatalogEntry, PragmaFunctionExtractor>(
391: 			    standard_entry, data.offset_in_entry, output, count);
392: 			break;
393: 		default:
394: 			throw InternalException("FIXME: unrecognized function type in duckdb_functions");
395: 		}
396: 		if (finished) {
397: 			// finished with this function, move to the next function
398: 			data.offset++;
399: 			data.offset_in_entry = 0;
400: 		} else {
401: 			// more functions remain
402: 			data.offset_in_entry++;
403: 		}
404: 		count++;
405: 	}
406: 	output.SetCardinality(count);
407: }
408: 
409: void DuckDBFunctionsFun::RegisterFunction(BuiltinFunctions &set) {
410: 	set.AddFunction(
411: 	    TableFunction("duckdb_functions", {}, DuckDBFunctionsFunction, DuckDBFunctionsBind, DuckDBFunctionsInit));
412: }
413: 
414: } // namespace duckdb
[end of src/function/table/system/duckdb_functions.cpp]
[start of src/function/table/system/duckdb_indexes.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/index_catalog_entry.hpp"
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb/storage/data_table.hpp"
9: #include "duckdb/storage/index.hpp"
10: 
11: namespace duckdb {
12: 
13: struct DuckDBIndexesData : public FunctionOperatorData {
14: 	DuckDBIndexesData() : offset(0) {
15: 	}
16: 
17: 	vector<CatalogEntry *> entries;
18: 	idx_t offset;
19: };
20: 
21: static unique_ptr<FunctionData> DuckDBIndexesBind(ClientContext &context, vector<Value> &inputs,
22:                                                   named_parameter_map_t &named_parameters,
23:                                                   vector<LogicalType> &input_table_types,
24:                                                   vector<string> &input_table_names, vector<LogicalType> &return_types,
25:                                                   vector<string> &names) {
26: 	names.emplace_back("schema_name");
27: 	return_types.emplace_back(LogicalType::VARCHAR);
28: 
29: 	names.emplace_back("schema_oid");
30: 	return_types.emplace_back(LogicalType::BIGINT);
31: 
32: 	names.emplace_back("index_name");
33: 	return_types.emplace_back(LogicalType::VARCHAR);
34: 
35: 	names.emplace_back("index_oid");
36: 	return_types.emplace_back(LogicalType::BIGINT);
37: 
38: 	names.emplace_back("table_name");
39: 	return_types.emplace_back(LogicalType::VARCHAR);
40: 
41: 	names.emplace_back("table_oid");
42: 	return_types.emplace_back(LogicalType::BIGINT);
43: 
44: 	names.emplace_back("is_unique");
45: 	return_types.emplace_back(LogicalType::BOOLEAN);
46: 
47: 	names.emplace_back("is_primary");
48: 	return_types.emplace_back(LogicalType::BOOLEAN);
49: 
50: 	names.emplace_back("expressions");
51: 	return_types.emplace_back(LogicalType::VARCHAR);
52: 
53: 	names.emplace_back("sql");
54: 	return_types.emplace_back(LogicalType::VARCHAR);
55: 
56: 	return nullptr;
57: }
58: 
59: unique_ptr<FunctionOperatorData> DuckDBIndexesInit(ClientContext &context, const FunctionData *bind_data,
60:                                                    const vector<column_t> &column_ids, TableFilterCollection *filters) {
61: 	auto result = make_unique<DuckDBIndexesData>();
62: 
63: 	// scan all the schemas for tables and collect themand collect them
64: 	auto schemas = Catalog::GetCatalog(context).schemas->GetEntries<SchemaCatalogEntry>(context);
65: 	for (auto &schema : schemas) {
66: 		schema->Scan(context, CatalogType::INDEX_ENTRY, [&](CatalogEntry *entry) { result->entries.push_back(entry); });
67: 	};
68: 
69: 	// check the temp schema as well
70: 	context.temporary_objects->Scan(context, CatalogType::INDEX_ENTRY,
71: 	                                [&](CatalogEntry *entry) { result->entries.push_back(entry); });
72: 	return move(result);
73: }
74: 
75: void DuckDBIndexesFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
76:                            DataChunk *input, DataChunk &output) {
77: 	auto &data = (DuckDBIndexesData &)*operator_state;
78: 	if (data.offset >= data.entries.size()) {
79: 		// finished returning values
80: 		return;
81: 	}
82: 	// start returning values
83: 	// either fill up the chunk or return all the remaining columns
84: 	idx_t count = 0;
85: 	while (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {
86: 		auto &entry = data.entries[data.offset++];
87: 
88: 		auto &index = (IndexCatalogEntry &)*entry;
89: 		// return values:
90: 
91: 		// schema_name, VARCHAR
92: 		output.SetValue(0, count, Value(index.schema->name));
93: 		// schema_oid, BIGINT
94: 		output.SetValue(1, count, Value::BIGINT(index.schema->oid));
95: 		// index_name, VARCHAR
96: 		output.SetValue(2, count, Value(index.name));
97: 		// index_oid, BIGINT
98: 		output.SetValue(3, count, Value::BIGINT(index.oid));
99: 		// table_name, VARCHAR
100: 		output.SetValue(4, count, Value(index.info->table));
101: 		// table_oid, BIGINT
102: 		// find the table in the catalog
103: 		auto &catalog = Catalog::GetCatalog(context);
104: 		auto table_entry = catalog.GetEntry(context, CatalogType::TABLE_ENTRY, index.info->schema, index.info->table);
105: 		output.SetValue(5, count, Value::BIGINT(table_entry->oid));
106: 		// is_unique, BOOLEAN
107: 		output.SetValue(6, count, Value::BOOLEAN(index.index->IsUnique()));
108: 		// is_primary, BOOLEAN
109: 		output.SetValue(7, count, Value::BOOLEAN(index.index->IsPrimary()));
110: 		// expressions, VARCHAR
111: 		output.SetValue(8, count, Value());
112: 		// sql, VARCHAR
113: 		output.SetValue(9, count, Value(index.ToSQL()));
114: 
115: 		count++;
116: 	}
117: 	output.SetCardinality(count);
118: }
119: 
120: void DuckDBIndexesFun::RegisterFunction(BuiltinFunctions &set) {
121: 	set.AddFunction(TableFunction("duckdb_indexes", {}, DuckDBIndexesFunction, DuckDBIndexesBind, DuckDBIndexesInit));
122: }
123: 
124: } // namespace duckdb
[end of src/function/table/system/duckdb_indexes.cpp]
[start of src/function/table/system/duckdb_keywords.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/common/exception.hpp"
4: #include "duckdb/main/client_context.hpp"
5: #include "duckdb/parser/parser.hpp"
6: 
7: namespace duckdb {
8: 
9: struct DuckDBKeywordsData : public FunctionOperatorData {
10: 	DuckDBKeywordsData() : offset(0) {
11: 	}
12: 
13: 	vector<ParserKeyword> entries;
14: 	idx_t offset;
15: };
16: 
17: static unique_ptr<FunctionData> DuckDBKeywordsBind(ClientContext &context, vector<Value> &inputs,
18:                                                    named_parameter_map_t &named_parameters,
19:                                                    vector<LogicalType> &input_table_types,
20:                                                    vector<string> &input_table_names, vector<LogicalType> &return_types,
21:                                                    vector<string> &names) {
22: 	names.emplace_back("keyword_name");
23: 	return_types.emplace_back(LogicalType::VARCHAR);
24: 
25: 	names.emplace_back("keyword_category");
26: 	return_types.emplace_back(LogicalType::VARCHAR);
27: 
28: 	return nullptr;
29: }
30: 
31: unique_ptr<FunctionOperatorData> DuckDBKeywordsInit(ClientContext &context, const FunctionData *bind_data,
32:                                                     const vector<column_t> &column_ids,
33:                                                     TableFilterCollection *filters) {
34: 	auto result = make_unique<DuckDBKeywordsData>();
35: 	result->entries = Parser::KeywordList();
36: 	return move(result);
37: }
38: 
39: void DuckDBKeywordsFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
40:                             DataChunk *input, DataChunk &output) {
41: 	auto &data = (DuckDBKeywordsData &)*operator_state;
42: 	if (data.offset >= data.entries.size()) {
43: 		// finished returning values
44: 		return;
45: 	}
46: 	// start returning values
47: 	// either fill up the chunk or return all the remaining columns
48: 	idx_t count = 0;
49: 	while (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {
50: 		auto &entry = data.entries[data.offset++];
51: 
52: 		// keyword_name, VARCHAR
53: 		output.SetValue(0, count, Value(entry.name));
54: 		// keyword_category, VARCHAR
55: 		string category_name;
56: 		switch (entry.category) {
57: 		case KeywordCategory::KEYWORD_RESERVED:
58: 			category_name = "reserved";
59: 			break;
60: 		case KeywordCategory::KEYWORD_UNRESERVED:
61: 			category_name = "unreserved";
62: 			break;
63: 		case KeywordCategory::KEYWORD_TYPE_FUNC:
64: 			category_name = "type_function";
65: 			break;
66: 		case KeywordCategory::KEYWORD_COL_NAME:
67: 			category_name = "column_name";
68: 			break;
69: 		default:
70: 			throw InternalException("Unrecognized keyword category");
71: 		}
72: 		output.SetValue(1, count, Value(move(category_name)));
73: 
74: 		count++;
75: 	}
76: 	output.SetCardinality(count);
77: }
78: 
79: void DuckDBKeywordsFun::RegisterFunction(BuiltinFunctions &set) {
80: 	set.AddFunction(
81: 	    TableFunction("duckdb_keywords", {}, DuckDBKeywordsFunction, DuckDBKeywordsBind, DuckDBKeywordsInit));
82: }
83: 
84: } // namespace duckdb
[end of src/function/table/system/duckdb_keywords.cpp]
[start of src/function/table/system/duckdb_schemas.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/main/client_context.hpp"
7: 
8: namespace duckdb {
9: 
10: struct DuckDBSchemasData : public FunctionOperatorData {
11: 	DuckDBSchemasData() : offset(0) {
12: 	}
13: 
14: 	vector<SchemaCatalogEntry *> entries;
15: 	idx_t offset;
16: };
17: 
18: static unique_ptr<FunctionData> DuckDBSchemasBind(ClientContext &context, vector<Value> &inputs,
19:                                                   named_parameter_map_t &named_parameters,
20:                                                   vector<LogicalType> &input_table_types,
21:                                                   vector<string> &input_table_names, vector<LogicalType> &return_types,
22:                                                   vector<string> &names) {
23: 	names.emplace_back("oid");
24: 	return_types.emplace_back(LogicalType::BIGINT);
25: 
26: 	names.emplace_back("schema_name");
27: 	return_types.emplace_back(LogicalType::VARCHAR);
28: 
29: 	names.emplace_back("internal");
30: 	return_types.emplace_back(LogicalType::BOOLEAN);
31: 
32: 	names.emplace_back("sql");
33: 	return_types.emplace_back(LogicalType::VARCHAR);
34: 
35: 	return nullptr;
36: }
37: 
38: unique_ptr<FunctionOperatorData> DuckDBSchemasInit(ClientContext &context, const FunctionData *bind_data,
39:                                                    const vector<column_t> &column_ids, TableFilterCollection *filters) {
40: 	auto result = make_unique<DuckDBSchemasData>();
41: 
42: 	// scan all the schemas and collect them
43: 	Catalog::GetCatalog(context).ScanSchemas(
44: 	    context, [&](CatalogEntry *entry) { result->entries.push_back((SchemaCatalogEntry *)entry); });
45: 	// get the temp schema as well
46: 	result->entries.push_back(context.temporary_objects.get());
47: 
48: 	return move(result);
49: }
50: 
51: void DuckDBSchemasFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
52:                            DataChunk *input, DataChunk &output) {
53: 	auto &data = (DuckDBSchemasData &)*operator_state;
54: 	if (data.offset >= data.entries.size()) {
55: 		// finished returning values
56: 		return;
57: 	}
58: 	// start returning values
59: 	// either fill up the chunk or return all the remaining columns
60: 	idx_t count = 0;
61: 	while (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {
62: 		auto &entry = data.entries[data.offset];
63: 
64: 		// return values:
65: 		// "oid", PhysicalType::BIGINT
66: 		output.SetValue(0, count, Value::BIGINT(entry->oid));
67: 		// "schema_name", PhysicalType::VARCHAR
68: 		output.SetValue(1, count, Value(entry->name));
69: 		// "internal", PhysicalType::BOOLEAN
70: 		output.SetValue(2, count, Value::BOOLEAN(entry->internal));
71: 		// "sql", PhysicalType::VARCHAR
72: 		output.SetValue(3, count, Value());
73: 
74: 		data.offset++;
75: 		count++;
76: 	}
77: 	output.SetCardinality(count);
78: }
79: 
80: void DuckDBSchemasFun::RegisterFunction(BuiltinFunctions &set) {
81: 	set.AddFunction(TableFunction("duckdb_schemas", {}, DuckDBSchemasFunction, DuckDBSchemasBind, DuckDBSchemasInit));
82: }
83: 
84: } // namespace duckdb
[end of src/function/table/system/duckdb_schemas.cpp]
[start of src/function/table/system/duckdb_sequences.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/sequence_catalog_entry.hpp"
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/main/client_context.hpp"
8: 
9: namespace duckdb {
10: 
11: struct DuckDBSequencesData : public FunctionOperatorData {
12: 	DuckDBSequencesData() : offset(0) {
13: 	}
14: 
15: 	vector<CatalogEntry *> entries;
16: 	idx_t offset;
17: };
18: 
19: static unique_ptr<FunctionData> DuckDBSequencesBind(ClientContext &context, vector<Value> &inputs,
20:                                                     named_parameter_map_t &named_parameters,
21:                                                     vector<LogicalType> &input_table_types,
22:                                                     vector<string> &input_table_names,
23:                                                     vector<LogicalType> &return_types, vector<string> &names) {
24: 	names.emplace_back("schema_name");
25: 	return_types.emplace_back(LogicalType::VARCHAR);
26: 
27: 	names.emplace_back("schema_oid");
28: 	return_types.emplace_back(LogicalType::BIGINT);
29: 
30: 	names.emplace_back("sequence_name");
31: 	return_types.emplace_back(LogicalType::VARCHAR);
32: 
33: 	names.emplace_back("sequence_oid");
34: 	return_types.emplace_back(LogicalType::BIGINT);
35: 
36: 	names.emplace_back("temporary");
37: 	return_types.emplace_back(LogicalType::BOOLEAN);
38: 
39: 	names.emplace_back("start_value");
40: 	return_types.emplace_back(LogicalType::BIGINT);
41: 
42: 	names.emplace_back("min_value");
43: 	return_types.emplace_back(LogicalType::BIGINT);
44: 
45: 	names.emplace_back("max_value");
46: 	return_types.emplace_back(LogicalType::BIGINT);
47: 
48: 	names.emplace_back("increment_by");
49: 	return_types.emplace_back(LogicalType::BIGINT);
50: 
51: 	names.emplace_back("cycle");
52: 	return_types.emplace_back(LogicalType::BOOLEAN);
53: 
54: 	names.emplace_back("last_value");
55: 	return_types.emplace_back(LogicalType::BIGINT);
56: 
57: 	names.emplace_back("sql");
58: 	return_types.emplace_back(LogicalType::VARCHAR);
59: 
60: 	return nullptr;
61: }
62: 
63: unique_ptr<FunctionOperatorData> DuckDBSequencesInit(ClientContext &context, const FunctionData *bind_data,
64:                                                      const vector<column_t> &column_ids,
65:                                                      TableFilterCollection *filters) {
66: 	auto result = make_unique<DuckDBSequencesData>();
67: 
68: 	// scan all the schemas for tables and collect themand collect them
69: 	auto schemas = Catalog::GetCatalog(context).schemas->GetEntries<SchemaCatalogEntry>(context);
70: 	for (auto &schema : schemas) {
71: 		schema->Scan(context, CatalogType::SEQUENCE_ENTRY,
72: 		             [&](CatalogEntry *entry) { result->entries.push_back(entry); });
73: 	};
74: 
75: 	// check the temp schema as well
76: 	context.temporary_objects->Scan(context, CatalogType::SEQUENCE_ENTRY,
77: 	                                [&](CatalogEntry *entry) { result->entries.push_back(entry); });
78: 	return move(result);
79: }
80: 
81: void DuckDBSequencesFunction(ClientContext &context, const FunctionData *bind_data,
82:                              FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
83: 	auto &data = (DuckDBSequencesData &)*operator_state;
84: 	if (data.offset >= data.entries.size()) {
85: 		// finished returning values
86: 		return;
87: 	}
88: 	// start returning values
89: 	// either fill up the chunk or return all the remaining columns
90: 	idx_t count = 0;
91: 	while (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {
92: 		auto &entry = data.entries[data.offset++];
93: 
94: 		auto &seq = (SequenceCatalogEntry &)*entry;
95: 		// return values:
96: 		// schema_name, VARCHAR
97: 		output.SetValue(0, count, Value(seq.schema->name));
98: 		// schema_oid, BIGINT
99: 		output.SetValue(1, count, Value::BIGINT(seq.schema->oid));
100: 		// sequence_name, VARCHAR
101: 		output.SetValue(2, count, Value(seq.name));
102: 		// sequence_oid, BIGINT
103: 		output.SetValue(3, count, Value::BIGINT(seq.oid));
104: 		// temporary, BOOLEAN
105: 		output.SetValue(4, count, Value::BOOLEAN(seq.temporary));
106: 		// start_value, BIGINT
107: 		output.SetValue(5, count, Value::BIGINT(seq.start_value));
108: 		// min_value, BIGINT
109: 		output.SetValue(6, count, Value::BIGINT(seq.min_value));
110: 		// max_value, BIGINT
111: 		output.SetValue(7, count, Value::BIGINT(seq.max_value));
112: 		// increment_by, BIGINT
113: 		output.SetValue(8, count, Value::BIGINT(seq.increment));
114: 		// cycle, BOOLEAN
115: 		output.SetValue(9, count, Value::BOOLEAN(seq.cycle));
116: 		// last_value, BIGINT
117: 		output.SetValue(10, count, seq.usage_count == 0 ? Value() : Value::BOOLEAN(seq.last_value));
118: 		// sql, LogicalType::VARCHAR
119: 		output.SetValue(11, count, Value(seq.ToSQL()));
120: 
121: 		count++;
122: 	}
123: 	output.SetCardinality(count);
124: }
125: 
126: void DuckDBSequencesFun::RegisterFunction(BuiltinFunctions &set) {
127: 	set.AddFunction(
128: 	    TableFunction("duckdb_sequences", {}, DuckDBSequencesFunction, DuckDBSequencesBind, DuckDBSequencesInit));
129: }
130: 
131: } // namespace duckdb
[end of src/function/table/system/duckdb_sequences.cpp]
[start of src/function/table/system/duckdb_settings.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: #include "duckdb/common/types/chunk_collection.hpp"
3: #include "duckdb/main/config.hpp"
4: #include "duckdb/main/client_context.hpp"
5: 
6: namespace duckdb {
7: 
8: struct DuckDBSettingValue {
9: 	string name;
10: 	string value;
11: 	string description;
12: 	string input_type;
13: };
14: 
15: struct DuckDBSettingsData : public FunctionOperatorData {
16: 	DuckDBSettingsData() : offset(0) {
17: 	}
18: 
19: 	vector<DuckDBSettingValue> settings;
20: 	idx_t offset;
21: };
22: 
23: static unique_ptr<FunctionData> DuckDBSettingsBind(ClientContext &context, vector<Value> &inputs,
24:                                                    named_parameter_map_t &named_parameters,
25:                                                    vector<LogicalType> &input_table_types,
26:                                                    vector<string> &input_table_names, vector<LogicalType> &return_types,
27:                                                    vector<string> &names) {
28: 	names.emplace_back("name");
29: 	return_types.emplace_back(LogicalType::VARCHAR);
30: 
31: 	names.emplace_back("value");
32: 	return_types.emplace_back(LogicalType::VARCHAR);
33: 
34: 	names.emplace_back("description");
35: 	return_types.emplace_back(LogicalType::VARCHAR);
36: 
37: 	names.emplace_back("input_type");
38: 	return_types.emplace_back(LogicalType::VARCHAR);
39: 
40: 	return nullptr;
41: }
42: 
43: unique_ptr<FunctionOperatorData> DuckDBSettingsInit(ClientContext &context, const FunctionData *bind_data,
44:                                                     const vector<column_t> &column_ids,
45:                                                     TableFilterCollection *filters) {
46: 	auto result = make_unique<DuckDBSettingsData>();
47: 
48: 	auto &config = DBConfig::GetConfig(context);
49: 	auto options_count = DBConfig::GetOptionCount();
50: 	for (idx_t i = 0; i < options_count; i++) {
51: 		auto option = DBConfig::GetOptionByIndex(i);
52: 		D_ASSERT(option);
53: 		DuckDBSettingValue value;
54: 		value.name = option->name;
55: 		value.value = option->get_setting(context).ToString();
56: 		value.description = option->description;
57: 		value.input_type = LogicalTypeIdToString(option->parameter_type);
58: 
59: 		result->settings.push_back(move(value));
60: 	}
61: 	for (auto &ext_param : config.extension_parameters) {
62: 		Value setting_val;
63: 		string setting_str_val;
64: 		if (context.TryGetCurrentSetting(ext_param.first, setting_val)) {
65: 			setting_str_val = setting_val.ToString();
66: 		}
67: 		DuckDBSettingValue value;
68: 		value.name = ext_param.first;
69: 		value.value = move(setting_str_val);
70: 		value.description = ext_param.second.description;
71: 		value.input_type = ext_param.second.type.ToString();
72: 
73: 		result->settings.push_back(move(value));
74: 	}
75: 	return move(result);
76: }
77: 
78: void DuckDBSettingsFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
79:                             DataChunk *input, DataChunk &output) {
80: 	auto &data = (DuckDBSettingsData &)*operator_state;
81: 	if (data.offset >= data.settings.size()) {
82: 		// finished returning values
83: 		return;
84: 	}
85: 	// start returning values
86: 	// either fill up the chunk or return all the remaining columns
87: 	idx_t count = 0;
88: 	while (data.offset < data.settings.size() && count < STANDARD_VECTOR_SIZE) {
89: 		auto &entry = data.settings[data.offset++];
90: 
91: 		// return values:
92: 		// name, LogicalType::VARCHAR
93: 		output.SetValue(0, count, Value(entry.name));
94: 		// value, LogicalType::VARCHAR
95: 		output.SetValue(1, count, Value(entry.value));
96: 		// description, LogicalType::VARCHAR
97: 		output.SetValue(2, count, Value(entry.description));
98: 		// input_type, LogicalType::VARCHAR
99: 		output.SetValue(3, count, Value(entry.input_type));
100: 		count++;
101: 	}
102: 	output.SetCardinality(count);
103: }
104: 
105: void DuckDBSettingsFun::RegisterFunction(BuiltinFunctions &set) {
106: 	set.AddFunction(
107: 	    TableFunction("duckdb_settings", {}, DuckDBSettingsFunction, DuckDBSettingsBind, DuckDBSettingsInit));
108: }
109: 
110: } // namespace duckdb
[end of src/function/table/system/duckdb_settings.cpp]
[start of src/function/table/system/duckdb_tables.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb/parser/constraint.hpp"
9: #include "duckdb/parser/constraints/unique_constraint.hpp"
10: #include "duckdb/storage/data_table.hpp"
11: 
12: namespace duckdb {
13: 
14: struct DuckDBTablesData : public FunctionOperatorData {
15: 	DuckDBTablesData() : offset(0) {
16: 	}
17: 
18: 	vector<CatalogEntry *> entries;
19: 	idx_t offset;
20: };
21: 
22: static unique_ptr<FunctionData> DuckDBTablesBind(ClientContext &context, vector<Value> &inputs,
23:                                                  named_parameter_map_t &named_parameters,
24:                                                  vector<LogicalType> &input_table_types,
25:                                                  vector<string> &input_table_names, vector<LogicalType> &return_types,
26:                                                  vector<string> &names) {
27: 	names.emplace_back("schema_name");
28: 	return_types.emplace_back(LogicalType::VARCHAR);
29: 
30: 	names.emplace_back("schema_oid");
31: 	return_types.emplace_back(LogicalType::BIGINT);
32: 
33: 	names.emplace_back("table_name");
34: 	return_types.emplace_back(LogicalType::VARCHAR);
35: 
36: 	names.emplace_back("table_oid");
37: 	return_types.emplace_back(LogicalType::BIGINT);
38: 
39: 	names.emplace_back("internal");
40: 	return_types.emplace_back(LogicalType::BOOLEAN);
41: 
42: 	names.emplace_back("temporary");
43: 	return_types.emplace_back(LogicalType::BOOLEAN);
44: 
45: 	names.emplace_back("has_primary_key");
46: 	return_types.emplace_back(LogicalType::BOOLEAN);
47: 
48: 	names.emplace_back("estimated_size");
49: 	return_types.emplace_back(LogicalType::BIGINT);
50: 
51: 	names.emplace_back("column_count");
52: 	return_types.emplace_back(LogicalType::BIGINT);
53: 
54: 	names.emplace_back("index_count");
55: 	return_types.emplace_back(LogicalType::BIGINT);
56: 
57: 	names.emplace_back("check_constraint_count");
58: 	return_types.emplace_back(LogicalType::BIGINT);
59: 
60: 	names.emplace_back("sql");
61: 	return_types.emplace_back(LogicalType::VARCHAR);
62: 
63: 	return nullptr;
64: }
65: 
66: unique_ptr<FunctionOperatorData> DuckDBTablesInit(ClientContext &context, const FunctionData *bind_data,
67:                                                   const vector<column_t> &column_ids, TableFilterCollection *filters) {
68: 	auto result = make_unique<DuckDBTablesData>();
69: 
70: 	// scan all the schemas for tables and collect themand collect them
71: 	auto schemas = Catalog::GetCatalog(context).schemas->GetEntries<SchemaCatalogEntry>(context);
72: 	for (auto &schema : schemas) {
73: 		schema->Scan(context, CatalogType::TABLE_ENTRY, [&](CatalogEntry *entry) { result->entries.push_back(entry); });
74: 	};
75: 
76: 	// check the temp schema as well
77: 	context.temporary_objects->Scan(context, CatalogType::TABLE_ENTRY,
78: 	                                [&](CatalogEntry *entry) { result->entries.push_back(entry); });
79: 	return move(result);
80: }
81: 
82: static bool TableHasPrimaryKey(TableCatalogEntry &table) {
83: 	for (auto &constraint : table.constraints) {
84: 		if (constraint->type == ConstraintType::UNIQUE) {
85: 			auto &unique = (UniqueConstraint &)*constraint;
86: 			if (unique.is_primary_key) {
87: 				return true;
88: 			}
89: 		}
90: 	}
91: 	return false;
92: }
93: 
94: static idx_t CheckConstraintCount(TableCatalogEntry &table) {
95: 	idx_t check_count = 0;
96: 	for (auto &constraint : table.constraints) {
97: 		if (constraint->type == ConstraintType::CHECK) {
98: 			check_count++;
99: 		}
100: 	}
101: 	return check_count;
102: }
103: 
104: void DuckDBTablesFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
105:                           DataChunk *input, DataChunk &output) {
106: 	auto &data = (DuckDBTablesData &)*operator_state;
107: 	if (data.offset >= data.entries.size()) {
108: 		// finished returning values
109: 		return;
110: 	}
111: 	// start returning values
112: 	// either fill up the chunk or return all the remaining columns
113: 	idx_t count = 0;
114: 	while (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {
115: 		auto &entry = data.entries[data.offset++];
116: 
117: 		if (entry->type != CatalogType::TABLE_ENTRY) {
118: 			continue;
119: 		}
120: 		auto &table = (TableCatalogEntry &)*entry;
121: 		// return values:
122: 		// schema_name, LogicalType::VARCHAR
123: 		output.SetValue(0, count, Value(table.schema->name));
124: 		// schema_oid, LogicalType::BIGINT
125: 		output.SetValue(1, count, Value::BIGINT(table.schema->oid));
126: 		// table_name, LogicalType::VARCHAR
127: 		output.SetValue(2, count, Value(table.name));
128: 		// table_oid, LogicalType::BIGINT
129: 		output.SetValue(3, count, Value::BIGINT(table.oid));
130: 		// internal, LogicalType::BOOLEAN
131: 		output.SetValue(4, count, Value::BOOLEAN(table.internal));
132: 		// temporary, LogicalType::BOOLEAN
133: 		output.SetValue(5, count, Value::BOOLEAN(table.temporary));
134: 		// has_primary_key, LogicalType::BOOLEAN
135: 		output.SetValue(6, count, Value::BOOLEAN(TableHasPrimaryKey(table)));
136: 		// estimated_size, LogicalType::BIGINT
137: 		output.SetValue(7, count, Value::BIGINT(table.storage->info->cardinality.load()));
138: 		// column_count, LogicalType::BIGINT
139: 		output.SetValue(8, count, Value::BIGINT(table.columns.size()));
140: 		// index_count, LogicalType::BIGINT
141: 		output.SetValue(9, count, Value::BIGINT(table.storage->info->indexes.Count()));
142: 		// check_constraint_count, LogicalType::BIGINT
143: 		output.SetValue(10, count, Value::BIGINT(CheckConstraintCount(table)));
144: 		// sql, LogicalType::VARCHAR
145: 		output.SetValue(11, count, Value(table.ToSQL()));
146: 
147: 		count++;
148: 	}
149: 	output.SetCardinality(count);
150: }
151: 
152: void DuckDBTablesFun::RegisterFunction(BuiltinFunctions &set) {
153: 	set.AddFunction(TableFunction("duckdb_tables", {}, DuckDBTablesFunction, DuckDBTablesBind, DuckDBTablesInit));
154: }
155: 
156: } // namespace duckdb
[end of src/function/table/system/duckdb_tables.cpp]
[start of src/function/table/system/duckdb_types.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/sequence_catalog_entry.hpp"
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/main/client_context.hpp"
8: 
9: namespace duckdb {
10: 
11: struct DuckDBTypesData : public FunctionOperatorData {
12: 	DuckDBTypesData() : offset(0) {
13: 	}
14: 
15: 	vector<LogicalType> types;
16: 	idx_t offset;
17: };
18: 
19: static unique_ptr<FunctionData> DuckDBTypesBind(ClientContext &context, vector<Value> &inputs,
20:                                                 named_parameter_map_t &named_parameters,
21:                                                 vector<LogicalType> &input_table_types,
22:                                                 vector<string> &input_table_names, vector<LogicalType> &return_types,
23:                                                 vector<string> &names) {
24: 	names.emplace_back("schema_name");
25: 	return_types.emplace_back(LogicalType::VARCHAR);
26: 
27: 	names.emplace_back("schema_oid");
28: 	return_types.emplace_back(LogicalType::BIGINT);
29: 
30: 	names.emplace_back("type_oid");
31: 	return_types.emplace_back(LogicalType::BIGINT);
32: 
33: 	names.emplace_back("type_name");
34: 	return_types.emplace_back(LogicalType::VARCHAR);
35: 
36: 	names.emplace_back("type_size");
37: 	return_types.emplace_back(LogicalType::BIGINT);
38: 
39: 	// NUMERIC, STRING, DATETIME, BOOLEAN, COMPOSITE, USER
40: 	names.emplace_back("type_category");
41: 	return_types.emplace_back(LogicalType::VARCHAR);
42: 
43: 	names.emplace_back("internal");
44: 	return_types.emplace_back(LogicalType::BOOLEAN);
45: 
46: 	return nullptr;
47: }
48: 
49: unique_ptr<FunctionOperatorData> DuckDBTypesInit(ClientContext &context, const FunctionData *bind_data,
50:                                                  const vector<column_t> &column_ids, TableFilterCollection *filters) {
51: 	auto result = make_unique<DuckDBTypesData>();
52: 	result->types = LogicalType::AllTypes();
53: 	// FIXME: add user-defined types here (when we have them)
54: 	return move(result);
55: }
56: 
57: void DuckDBTypesFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
58:                          DataChunk *input, DataChunk &output) {
59: 	auto &data = (DuckDBTypesData &)*operator_state;
60: 	if (data.offset >= data.types.size()) {
61: 		// finished returning values
62: 		return;
63: 	}
64: 	// start returning values
65: 	// either fill up the chunk or return all the remaining columns
66: 	idx_t count = 0;
67: 	while (data.offset < data.types.size() && count < STANDARD_VECTOR_SIZE) {
68: 		auto &type = data.types[data.offset++];
69: 
70: 		// return values:
71: 		// schema_name, VARCHAR
72: 		output.SetValue(0, count, Value());
73: 		// schema_oid, BIGINT
74: 		output.SetValue(1, count, Value());
75: 		// type_oid, BIGINT
76: 		output.SetValue(2, count, Value::BIGINT(int(type.id())));
77: 		// type_name, VARCHAR
78: 		output.SetValue(3, count, Value(type.ToString()));
79: 		// type_size, BIGINT
80: 		auto internal_type = type.InternalType();
81: 		output.SetValue(4, count,
82: 		                internal_type == PhysicalType::INVALID ? Value() : Value::BIGINT(GetTypeIdSize(internal_type)));
83: 		// type_category, VARCHAR
84: 		string category;
85: 		switch (type.id()) {
86: 		case LogicalTypeId::TINYINT:
87: 		case LogicalTypeId::SMALLINT:
88: 		case LogicalTypeId::INTEGER:
89: 		case LogicalTypeId::BIGINT:
90: 		case LogicalTypeId::DECIMAL:
91: 		case LogicalTypeId::FLOAT:
92: 		case LogicalTypeId::DOUBLE:
93: 		case LogicalTypeId::UTINYINT:
94: 		case LogicalTypeId::USMALLINT:
95: 		case LogicalTypeId::UINTEGER:
96: 		case LogicalTypeId::UBIGINT:
97: 		case LogicalTypeId::HUGEINT:
98: 			category = "NUMERIC";
99: 			break;
100: 		case LogicalTypeId::DATE:
101: 		case LogicalTypeId::TIME:
102: 		case LogicalTypeId::TIMESTAMP_SEC:
103: 		case LogicalTypeId::TIMESTAMP_MS:
104: 		case LogicalTypeId::TIMESTAMP:
105: 		case LogicalTypeId::TIMESTAMP_NS:
106: 		case LogicalTypeId::INTERVAL:
107: 		case LogicalTypeId::TIME_TZ:
108: 		case LogicalTypeId::TIMESTAMP_TZ:
109: 			category = "DATETIME";
110: 			break;
111: 		case LogicalTypeId::CHAR:
112: 		case LogicalTypeId::VARCHAR:
113: 			category = "STRING";
114: 			break;
115: 		case LogicalTypeId::BOOLEAN:
116: 			category = "BOOLEAN";
117: 			break;
118: 		case LogicalTypeId::STRUCT:
119: 		case LogicalTypeId::LIST:
120: 		case LogicalTypeId::MAP:
121: 			category = "COMPOSITE";
122: 			break;
123: 		default:
124: 			break;
125: 		}
126: 		output.SetValue(5, count, category.empty() ? Value() : Value(category));
127: 		// internal, BOOLEAN
128: 		output.SetValue(6, count, Value::BOOLEAN(true));
129: 
130: 		count++;
131: 	}
132: 	output.SetCardinality(count);
133: }
134: 
135: void DuckDBTypesFun::RegisterFunction(BuiltinFunctions &set) {
136: 	set.AddFunction(TableFunction("duckdb_types", {}, DuckDBTypesFunction, DuckDBTypesBind, DuckDBTypesInit));
137: }
138: 
139: } // namespace duckdb
[end of src/function/table/system/duckdb_types.cpp]
[start of src/function/table/system/duckdb_views.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/main/client_context.hpp"
8: 
9: namespace duckdb {
10: 
11: struct DuckDBViewsData : public FunctionOperatorData {
12: 	DuckDBViewsData() : offset(0) {
13: 	}
14: 
15: 	vector<CatalogEntry *> entries;
16: 	idx_t offset;
17: };
18: 
19: static unique_ptr<FunctionData> DuckDBViewsBind(ClientContext &context, vector<Value> &inputs,
20:                                                 named_parameter_map_t &named_parameters,
21:                                                 vector<LogicalType> &input_table_types,
22:                                                 vector<string> &input_table_names, vector<LogicalType> &return_types,
23:                                                 vector<string> &names) {
24: 	names.emplace_back("schema_name");
25: 	return_types.emplace_back(LogicalType::VARCHAR);
26: 
27: 	names.emplace_back("schema_oid");
28: 	return_types.emplace_back(LogicalType::BIGINT);
29: 
30: 	names.emplace_back("view_name");
31: 	return_types.emplace_back(LogicalType::VARCHAR);
32: 
33: 	names.emplace_back("view_oid");
34: 	return_types.emplace_back(LogicalType::BIGINT);
35: 
36: 	names.emplace_back("internal");
37: 	return_types.emplace_back(LogicalType::BOOLEAN);
38: 
39: 	names.emplace_back("temporary");
40: 	return_types.emplace_back(LogicalType::BOOLEAN);
41: 
42: 	names.emplace_back("column_count");
43: 	return_types.emplace_back(LogicalType::BIGINT);
44: 
45: 	names.emplace_back("sql");
46: 	return_types.emplace_back(LogicalType::VARCHAR);
47: 
48: 	return nullptr;
49: }
50: 
51: unique_ptr<FunctionOperatorData> DuckDBViewsInit(ClientContext &context, const FunctionData *bind_data,
52:                                                  const vector<column_t> &column_ids, TableFilterCollection *filters) {
53: 	auto result = make_unique<DuckDBViewsData>();
54: 
55: 	// scan all the schemas for tables and collect themand collect them
56: 	auto schemas = Catalog::GetCatalog(context).schemas->GetEntries<SchemaCatalogEntry>(context);
57: 	for (auto &schema : schemas) {
58: 		schema->Scan(context, CatalogType::VIEW_ENTRY, [&](CatalogEntry *entry) { result->entries.push_back(entry); });
59: 	};
60: 
61: 	// check the temp schema as well
62: 	context.temporary_objects->Scan(context, CatalogType::VIEW_ENTRY,
63: 	                                [&](CatalogEntry *entry) { result->entries.push_back(entry); });
64: 	return move(result);
65: }
66: 
67: void DuckDBViewsFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
68:                          DataChunk *input, DataChunk &output) {
69: 	auto &data = (DuckDBViewsData &)*operator_state;
70: 	if (data.offset >= data.entries.size()) {
71: 		// finished returning values
72: 		return;
73: 	}
74: 	// start returning values
75: 	// either fill up the chunk or return all the remaining columns
76: 	idx_t count = 0;
77: 	while (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {
78: 		auto &entry = data.entries[data.offset++];
79: 
80: 		if (entry->type != CatalogType::VIEW_ENTRY) {
81: 			continue;
82: 		}
83: 		auto &view = (ViewCatalogEntry &)*entry;
84: 
85: 		// return values:
86: 		// schema_name, LogicalType::VARCHAR
87: 		output.SetValue(0, count, Value(view.schema->name));
88: 		// schema_oid, LogicalType::BIGINT
89: 		output.SetValue(1, count, Value::BIGINT(view.schema->oid));
90: 		// view_name, LogicalType::VARCHAR
91: 		output.SetValue(2, count, Value(view.name));
92: 		// view_oid, LogicalType::BIGINT
93: 		output.SetValue(3, count, Value::BIGINT(view.oid));
94: 		// internal, LogicalType::BOOLEAN
95: 		output.SetValue(4, count, Value::BOOLEAN(view.internal));
96: 		// temporary, LogicalType::BOOLEAN
97: 		output.SetValue(5, count, Value::BOOLEAN(view.temporary));
98: 		// column_count, LogicalType::BIGINT
99: 		output.SetValue(6, count, Value::BIGINT(view.types.size()));
100: 		// sql, LogicalType::VARCHAR
101: 		output.SetValue(7, count, Value(view.ToSQL()));
102: 
103: 		count++;
104: 	}
105: 	output.SetCardinality(count);
106: }
107: 
108: void DuckDBViewsFun::RegisterFunction(BuiltinFunctions &set) {
109: 	set.AddFunction(TableFunction("duckdb_views", {}, DuckDBViewsFunction, DuckDBViewsBind, DuckDBViewsInit));
110: }
111: 
112: } // namespace duckdb
[end of src/function/table/system/duckdb_views.cpp]
[start of src/function/table/system/pragma_collations.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/collate_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
6: #include "duckdb/common/exception.hpp"
7: 
8: namespace duckdb {
9: 
10: struct PragmaCollateData : public FunctionOperatorData {
11: 	PragmaCollateData() : offset(0) {
12: 	}
13: 
14: 	vector<string> entries;
15: 	idx_t offset;
16: };
17: 
18: static unique_ptr<FunctionData> PragmaCollateBind(ClientContext &context, vector<Value> &inputs,
19:                                                   named_parameter_map_t &named_parameters,
20:                                                   vector<LogicalType> &input_table_types,
21:                                                   vector<string> &input_table_names, vector<LogicalType> &return_types,
22:                                                   vector<string> &names) {
23: 	names.emplace_back("collname");
24: 	return_types.emplace_back(LogicalType::VARCHAR);
25: 
26: 	return nullptr;
27: }
28: 
29: unique_ptr<FunctionOperatorData> PragmaCollateInit(ClientContext &context, const FunctionData *bind_data,
30:                                                    const vector<column_t> &column_ids, TableFilterCollection *filters) {
31: 	auto result = make_unique<PragmaCollateData>();
32: 
33: 	Catalog::GetCatalog(context).schemas->Scan(context, [&](CatalogEntry *entry) {
34: 		auto schema = (SchemaCatalogEntry *)entry;
35: 		schema->Scan(context, CatalogType::COLLATION_ENTRY,
36: 		             [&](CatalogEntry *entry) { result->entries.push_back(entry->name); });
37: 	});
38: 
39: 	return move(result);
40: }
41: 
42: static void PragmaCollateFunction(ClientContext &context, const FunctionData *bind_data,
43:                                   FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
44: 	auto &data = (PragmaCollateData &)*operator_state;
45: 	if (data.offset >= data.entries.size()) {
46: 		// finished returning values
47: 		return;
48: 	}
49: 	idx_t next = MinValue<idx_t>(data.offset + STANDARD_VECTOR_SIZE, data.entries.size());
50: 	output.SetCardinality(next - data.offset);
51: 	for (idx_t i = data.offset; i < next; i++) {
52: 		auto index = i - data.offset;
53: 		output.SetValue(0, index, Value(data.entries[i]));
54: 	}
55: 
56: 	data.offset = next;
57: }
58: 
59: void PragmaCollations::RegisterFunction(BuiltinFunctions &set) {
60: 	set.AddFunction(
61: 	    TableFunction("pragma_collations", {}, PragmaCollateFunction, PragmaCollateBind, PragmaCollateInit));
62: }
63: 
64: } // namespace duckdb
[end of src/function/table/system/pragma_collations.cpp]
[start of src/function/table/system/pragma_database_list.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/storage/storage_manager.hpp"
4: 
5: namespace duckdb {
6: 
7: struct PragmaDatabaseListData : public FunctionOperatorData {
8: 	PragmaDatabaseListData() : finished(false) {
9: 	}
10: 
11: 	bool finished;
12: };
13: 
14: static unique_ptr<FunctionData> PragmaDatabaseListBind(ClientContext &context, vector<Value> &inputs,
15:                                                        named_parameter_map_t &named_parameters,
16:                                                        vector<LogicalType> &input_table_types,
17:                                                        vector<string> &input_table_names,
18:                                                        vector<LogicalType> &return_types, vector<string> &names) {
19: 	names.emplace_back("seq");
20: 	return_types.emplace_back(LogicalType::INTEGER);
21: 
22: 	names.emplace_back("name");
23: 	return_types.emplace_back(LogicalType::VARCHAR);
24: 
25: 	names.emplace_back("file");
26: 	return_types.emplace_back(LogicalType::VARCHAR);
27: 
28: 	return nullptr;
29: }
30: 
31: unique_ptr<FunctionOperatorData> PragmaDatabaseListInit(ClientContext &context, const FunctionData *bind_data,
32:                                                         const vector<column_t> &column_ids,
33:                                                         TableFilterCollection *filters) {
34: 	return make_unique<PragmaDatabaseListData>();
35: }
36: 
37: void PragmaDatabaseListFunction(ClientContext &context, const FunctionData *bind_data,
38:                                 FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
39: 	auto &data = (PragmaDatabaseListData &)*operator_state;
40: 	if (data.finished) {
41: 		return;
42: 	}
43: 
44: 	output.SetCardinality(1);
45: 	output.data[0].SetValue(0, Value::INTEGER(0));
46: 	output.data[1].SetValue(0, Value("main"));
47: 	output.data[2].SetValue(0, Value(StorageManager::GetStorageManager(context).GetDBPath()));
48: 
49: 	data.finished = true;
50: }
51: 
52: void PragmaDatabaseList::RegisterFunction(BuiltinFunctions &set) {
53: 	set.AddFunction(TableFunction("pragma_database_list", {}, PragmaDatabaseListFunction, PragmaDatabaseListBind,
54: 	                              PragmaDatabaseListInit));
55: }
56: 
57: } // namespace duckdb
[end of src/function/table/system/pragma_database_list.cpp]
[start of src/function/table/system/pragma_database_size.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/storage/storage_manager.hpp"
4: #include "duckdb/storage/block_manager.hpp"
5: #include "duckdb/storage/storage_info.hpp"
6: #include "duckdb/common/to_string.hpp"
7: #include "duckdb/common/string_util.hpp"
8: 
9: namespace duckdb {
10: 
11: struct PragmaDatabaseSizeData : public FunctionOperatorData {
12: 	PragmaDatabaseSizeData() : finished(false) {
13: 	}
14: 
15: 	bool finished;
16: };
17: 
18: static unique_ptr<FunctionData> PragmaDatabaseSizeBind(ClientContext &context, vector<Value> &inputs,
19:                                                        named_parameter_map_t &named_parameters,
20:                                                        vector<LogicalType> &input_table_types,
21:                                                        vector<string> &input_table_names,
22:                                                        vector<LogicalType> &return_types, vector<string> &names) {
23: 	names.emplace_back("database_size");
24: 	return_types.emplace_back(LogicalType::VARCHAR);
25: 
26: 	names.emplace_back("block_size");
27: 	return_types.emplace_back(LogicalType::BIGINT);
28: 
29: 	names.emplace_back("total_blocks");
30: 	return_types.emplace_back(LogicalType::BIGINT);
31: 
32: 	names.emplace_back("used_blocks");
33: 	return_types.emplace_back(LogicalType::BIGINT);
34: 
35: 	names.emplace_back("free_blocks");
36: 	return_types.emplace_back(LogicalType::BIGINT);
37: 
38: 	names.emplace_back("wal_size");
39: 	return_types.emplace_back(LogicalType::VARCHAR);
40: 
41: 	names.emplace_back("memory_usage");
42: 	return_types.emplace_back(LogicalType::VARCHAR);
43: 
44: 	names.emplace_back("memory_limit");
45: 	return_types.emplace_back(LogicalType::VARCHAR);
46: 
47: 	return nullptr;
48: }
49: 
50: unique_ptr<FunctionOperatorData> PragmaDatabaseSizeInit(ClientContext &context, const FunctionData *bind_data,
51:                                                         const vector<column_t> &column_ids,
52:                                                         TableFilterCollection *filters) {
53: 	return make_unique<PragmaDatabaseSizeData>();
54: }
55: 
56: void PragmaDatabaseSizeFunction(ClientContext &context, const FunctionData *bind_data,
57:                                 FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
58: 	auto &data = (PragmaDatabaseSizeData &)*operator_state;
59: 	if (data.finished) {
60: 		return;
61: 	}
62: 	auto &storage = StorageManager::GetStorageManager(context);
63: 	auto &block_manager = BlockManager::GetBlockManager(context);
64: 	auto &buffer_manager = BufferManager::GetBufferManager(context);
65: 
66: 	output.SetCardinality(1);
67: 	if (!storage.InMemory()) {
68: 		auto total_blocks = block_manager.TotalBlocks();
69: 		auto block_size = Storage::BLOCK_ALLOC_SIZE;
70: 		auto free_blocks = block_manager.FreeBlocks();
71: 		auto used_blocks = total_blocks - free_blocks;
72: 		auto bytes = (total_blocks * block_size);
73: 		auto wal = storage.GetWriteAheadLog();
74: 		auto wal_size = wal ? wal->GetWALSize() : 0;
75: 		output.data[0].SetValue(0, Value(StringUtil::BytesToHumanReadableString(bytes)));
76: 		output.data[1].SetValue(0, Value::BIGINT(block_size));
77: 		output.data[2].SetValue(0, Value::BIGINT(total_blocks));
78: 		output.data[3].SetValue(0, Value::BIGINT(used_blocks));
79: 		output.data[4].SetValue(0, Value::BIGINT(free_blocks));
80: 		output.data[5].SetValue(0, Value(StringUtil::BytesToHumanReadableString(wal_size)));
81: 	} else {
82: 		output.data[0].SetValue(0, Value());
83: 		output.data[1].SetValue(0, Value());
84: 		output.data[2].SetValue(0, Value());
85: 		output.data[3].SetValue(0, Value());
86: 		output.data[4].SetValue(0, Value());
87: 		output.data[5].SetValue(0, Value());
88: 	}
89: 	output.data[6].SetValue(0, Value(StringUtil::BytesToHumanReadableString(buffer_manager.GetUsedMemory())));
90: 	auto max_memory = buffer_manager.GetMaxMemory();
91: 	output.data[7].SetValue(0, max_memory == (idx_t)-1 ? Value("Unlimited")
92: 	                                                   : Value(StringUtil::BytesToHumanReadableString(max_memory)));
93: 
94: 	data.finished = true;
95: }
96: 
97: void PragmaDatabaseSize::RegisterFunction(BuiltinFunctions &set) {
98: 	set.AddFunction(TableFunction("pragma_database_size", {}, PragmaDatabaseSizeFunction, PragmaDatabaseSizeBind,
99: 	                              PragmaDatabaseSizeInit));
100: }
101: 
102: } // namespace duckdb
[end of src/function/table/system/pragma_database_size.cpp]
[start of src/function/table/system/pragma_functions.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/aggregate_function_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/scalar_function_catalog_entry.hpp"
6: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
7: #include "duckdb/common/types/chunk_collection.hpp"
8: #include "duckdb/common/exception.hpp"
9: 
10: namespace duckdb {
11: 
12: struct PragmaFunctionsData : public FunctionOperatorData {
13: 	PragmaFunctionsData() : offset(0), offset_in_entry(0) {
14: 	}
15: 
16: 	vector<CatalogEntry *> entries;
17: 	idx_t offset;
18: 	idx_t offset_in_entry;
19: };
20: 
21: static unique_ptr<FunctionData> PragmaFunctionsBind(ClientContext &context, vector<Value> &inputs,
22:                                                     named_parameter_map_t &named_parameters,
23:                                                     vector<LogicalType> &input_table_types,
24:                                                     vector<string> &input_table_names,
25:                                                     vector<LogicalType> &return_types, vector<string> &names) {
26: 	names.emplace_back("name");
27: 	return_types.emplace_back(LogicalType::VARCHAR);
28: 
29: 	names.emplace_back("type");
30: 	return_types.emplace_back(LogicalType::VARCHAR);
31: 
32: 	names.emplace_back("parameters");
33: 	return_types.push_back(LogicalType::LIST(LogicalType::VARCHAR));
34: 
35: 	names.emplace_back("varargs");
36: 	return_types.emplace_back(LogicalType::VARCHAR);
37: 
38: 	names.emplace_back("return_type");
39: 	return_types.emplace_back(LogicalType::VARCHAR);
40: 
41: 	names.emplace_back("side_effects");
42: 	return_types.emplace_back(LogicalType::BOOLEAN);
43: 
44: 	return nullptr;
45: }
46: 
47: unique_ptr<FunctionOperatorData> PragmaFunctionsInit(ClientContext &context, const FunctionData *bind_data,
48:                                                      const vector<column_t> &column_ids,
49:                                                      TableFilterCollection *filters) {
50: 	auto result = make_unique<PragmaFunctionsData>();
51: 
52: 	Catalog::GetCatalog(context).schemas->Scan(context, [&](CatalogEntry *entry) {
53: 		auto schema = (SchemaCatalogEntry *)entry;
54: 		schema->Scan(context, CatalogType::SCALAR_FUNCTION_ENTRY,
55: 		             [&](CatalogEntry *entry) { result->entries.push_back(entry); });
56: 	});
57: 
58: 	return move(result);
59: }
60: 
61: void AddFunction(BaseScalarFunction &f, idx_t &count, DataChunk &output, bool is_aggregate) {
62: 	output.SetValue(0, count, Value(f.name));
63: 	output.SetValue(1, count, Value(is_aggregate ? "AGGREGATE" : "SCALAR"));
64: 	auto result_data = FlatVector::GetData<list_entry_t>(output.data[2]);
65: 	result_data[count].offset = ListVector::GetListSize(output.data[2]);
66: 	result_data[count].length = f.arguments.size();
67: 	string parameters;
68: 	for (idx_t i = 0; i < f.arguments.size(); i++) {
69: 		auto val = Value(f.arguments[i].ToString());
70: 		ListVector::PushBack(output.data[2], val);
71: 	}
72: 
73: 	output.SetValue(3, count, f.varargs.id() != LogicalTypeId::INVALID ? Value(f.varargs.ToString()) : Value());
74: 	output.SetValue(4, count, f.return_type.ToString());
75: 	output.SetValue(5, count, Value::BOOLEAN(f.has_side_effects));
76: 
77: 	count++;
78: }
79: 
80: static void PragmaFunctionsFunction(ClientContext &context, const FunctionData *bind_data,
81:                                     FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
82: 	auto &data = (PragmaFunctionsData &)*operator_state;
83: 	if (data.offset >= data.entries.size()) {
84: 		// finished returning values
85: 		return;
86: 	}
87: 	idx_t count = 0;
88: 	while (count < STANDARD_VECTOR_SIZE && data.offset < data.entries.size()) {
89: 		auto &entry = data.entries[data.offset];
90: 		switch (entry->type) {
91: 		case CatalogType::SCALAR_FUNCTION_ENTRY: {
92: 			auto &func = (ScalarFunctionCatalogEntry &)*entry;
93: 			if (data.offset_in_entry >= func.functions.size()) {
94: 				data.offset++;
95: 				data.offset_in_entry = 0;
96: 				break;
97: 			}
98: 			AddFunction(func.functions[data.offset_in_entry++], count, output, false);
99: 			break;
100: 		}
101: 		case CatalogType::AGGREGATE_FUNCTION_ENTRY: {
102: 			auto &aggr = (AggregateFunctionCatalogEntry &)*entry;
103: 			if (data.offset_in_entry >= aggr.functions.size()) {
104: 				data.offset++;
105: 				data.offset_in_entry = 0;
106: 				break;
107: 			}
108: 			AddFunction(aggr.functions[data.offset_in_entry++], count, output, true);
109: 			break;
110: 		}
111: 		default:
112: 			data.offset++;
113: 			break;
114: 		}
115: 	}
116: 	output.SetCardinality(count);
117: }
118: 
119: void PragmaFunctionPragma::RegisterFunction(BuiltinFunctions &set) {
120: 	set.AddFunction(
121: 	    TableFunction("pragma_functions", {}, PragmaFunctionsFunction, PragmaFunctionsBind, PragmaFunctionsInit));
122: }
123: 
124: } // namespace duckdb
[end of src/function/table/system/pragma_functions.cpp]
[start of src/function/table/system/pragma_storage_info.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
6: #include "duckdb/parser/qualified_name.hpp"
7: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
8: #include "duckdb/planner/constraints/bound_unique_constraint.hpp"
9: 
10: #include "duckdb/common/exception.hpp"
11: #include "duckdb/common/limits.hpp"
12: #include "duckdb/storage/data_table.hpp"
13: 
14: #include <algorithm>
15: 
16: namespace duckdb {
17: 
18: struct PragmaStorageFunctionData : public TableFunctionData {
19: 	explicit PragmaStorageFunctionData(TableCatalogEntry *table_entry) : table_entry(table_entry) {
20: 	}
21: 
22: 	TableCatalogEntry *table_entry;
23: 	vector<vector<Value>> storage_info;
24: };
25: 
26: struct PragmaStorageOperatorData : public FunctionOperatorData {
27: 	PragmaStorageOperatorData() : offset(0) {
28: 	}
29: 
30: 	idx_t offset;
31: };
32: 
33: static unique_ptr<FunctionData> PragmaStorageInfoBind(ClientContext &context, vector<Value> &inputs,
34:                                                       named_parameter_map_t &named_parameters,
35:                                                       vector<LogicalType> &input_table_types,
36:                                                       vector<string> &input_table_names,
37:                                                       vector<LogicalType> &return_types, vector<string> &names) {
38: 	names.emplace_back("row_group_id");
39: 	return_types.emplace_back(LogicalType::BIGINT);
40: 
41: 	names.emplace_back("column_name");
42: 	return_types.emplace_back(LogicalType::VARCHAR);
43: 
44: 	names.emplace_back("column_id");
45: 	return_types.emplace_back(LogicalType::BIGINT);
46: 
47: 	names.emplace_back("column_path");
48: 	return_types.emplace_back(LogicalType::VARCHAR);
49: 
50: 	names.emplace_back("segment_id");
51: 	return_types.emplace_back(LogicalType::BIGINT);
52: 
53: 	names.emplace_back("segment_type");
54: 	return_types.emplace_back(LogicalType::VARCHAR);
55: 
56: 	names.emplace_back("start");
57: 	return_types.emplace_back(LogicalType::BIGINT);
58: 
59: 	names.emplace_back("count");
60: 	return_types.emplace_back(LogicalType::BIGINT);
61: 
62: 	names.emplace_back("compression");
63: 	return_types.emplace_back(LogicalType::VARCHAR);
64: 
65: 	names.emplace_back("stats");
66: 	return_types.emplace_back(LogicalType::VARCHAR);
67: 
68: 	names.emplace_back("has_updates");
69: 	return_types.emplace_back(LogicalType::BOOLEAN);
70: 
71: 	names.emplace_back("persistent");
72: 	return_types.emplace_back(LogicalType::BOOLEAN);
73: 
74: 	names.emplace_back("block_id");
75: 	return_types.emplace_back(LogicalType::BIGINT);
76: 
77: 	names.emplace_back("block_offset");
78: 	return_types.emplace_back(LogicalType::BIGINT);
79: 
80: 	auto qname = QualifiedName::Parse(inputs[0].GetValue<string>());
81: 
82: 	// look up the table name in the catalog
83: 	auto &catalog = Catalog::GetCatalog(context);
84: 	auto entry = catalog.GetEntry(context, CatalogType::TABLE_ENTRY, qname.schema, qname.name);
85: 	if (entry->type != CatalogType::TABLE_ENTRY) {
86: 		throw Exception("storage_info requires a table as parameter");
87: 	}
88: 	auto table_entry = (TableCatalogEntry *)entry;
89: 
90: 	auto result = make_unique<PragmaStorageFunctionData>(table_entry);
91: 	result->storage_info = table_entry->storage->GetStorageInfo();
92: 	return move(result);
93: }
94: 
95: unique_ptr<FunctionOperatorData> PragmaStorageInfoInit(ClientContext &context, const FunctionData *bind_data,
96:                                                        const vector<column_t> &column_ids,
97:                                                        TableFilterCollection *filters) {
98: 	return make_unique<PragmaStorageOperatorData>();
99: }
100: 
101: static void PragmaStorageInfoFunction(ClientContext &context, const FunctionData *bind_data_p,
102:                                       FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
103: 	auto &bind_data = (PragmaStorageFunctionData &)*bind_data_p;
104: 	auto &data = (PragmaStorageOperatorData &)*operator_state;
105: 	idx_t count = 0;
106: 	while (data.offset < bind_data.storage_info.size() && count < STANDARD_VECTOR_SIZE) {
107: 		auto &entry = bind_data.storage_info[data.offset++];
108: 		D_ASSERT(entry.size() + 1 == output.ColumnCount());
109: 		idx_t result_idx = 0;
110: 		for (idx_t col_idx = 0; col_idx < entry.size(); col_idx++, result_idx++) {
111: 			if (col_idx == 1) {
112: 				// write the column name
113: 				auto column_index = entry[col_idx].GetValue<int64_t>();
114: 				output.SetValue(result_idx, count, Value(bind_data.table_entry->columns[column_index].name));
115: 				result_idx++;
116: 			}
117: 			output.SetValue(result_idx, count, entry[col_idx]);
118: 		}
119: 
120: 		count++;
121: 	}
122: 	output.SetCardinality(count);
123: }
124: 
125: void PragmaStorageInfo::RegisterFunction(BuiltinFunctions &set) {
126: 	set.AddFunction(TableFunction("pragma_storage_info", {LogicalType::VARCHAR}, PragmaStorageInfoFunction,
127: 	                              PragmaStorageInfoBind, PragmaStorageInfoInit));
128: }
129: 
130: } // namespace duckdb
[end of src/function/table/system/pragma_storage_info.cpp]
[start of src/function/table/system/pragma_table_info.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
6: #include "duckdb/parser/qualified_name.hpp"
7: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
8: #include "duckdb/planner/constraints/bound_unique_constraint.hpp"
9: 
10: #include "duckdb/common/exception.hpp"
11: #include "duckdb/common/limits.hpp"
12: 
13: #include <algorithm>
14: 
15: namespace duckdb {
16: 
17: struct PragmaTableFunctionData : public TableFunctionData {
18: 	explicit PragmaTableFunctionData(CatalogEntry *entry_p) : entry(entry_p) {
19: 	}
20: 
21: 	CatalogEntry *entry;
22: };
23: 
24: struct PragmaTableOperatorData : public FunctionOperatorData {
25: 	PragmaTableOperatorData() : offset(0) {
26: 	}
27: 	idx_t offset;
28: };
29: 
30: static unique_ptr<FunctionData> PragmaTableInfoBind(ClientContext &context, vector<Value> &inputs,
31:                                                     named_parameter_map_t &named_parameters,
32:                                                     vector<LogicalType> &input_table_types,
33:                                                     vector<string> &input_table_names,
34:                                                     vector<LogicalType> &return_types, vector<string> &names) {
35: 	names.emplace_back("cid");
36: 	return_types.emplace_back(LogicalType::INTEGER);
37: 
38: 	names.emplace_back("name");
39: 	return_types.emplace_back(LogicalType::VARCHAR);
40: 
41: 	names.emplace_back("type");
42: 	return_types.emplace_back(LogicalType::VARCHAR);
43: 
44: 	names.emplace_back("notnull");
45: 	return_types.emplace_back(LogicalType::BOOLEAN);
46: 
47: 	names.emplace_back("dflt_value");
48: 	return_types.emplace_back(LogicalType::VARCHAR);
49: 
50: 	names.emplace_back("pk");
51: 	return_types.emplace_back(LogicalType::BOOLEAN);
52: 
53: 	auto qname = QualifiedName::Parse(inputs[0].GetValue<string>());
54: 
55: 	// look up the table name in the catalog
56: 	auto &catalog = Catalog::GetCatalog(context);
57: 	auto entry = catalog.GetEntry(context, CatalogType::TABLE_ENTRY, qname.schema, qname.name);
58: 	return make_unique<PragmaTableFunctionData>(entry);
59: }
60: 
61: unique_ptr<FunctionOperatorData> PragmaTableInfoInit(ClientContext &context, const FunctionData *bind_data,
62:                                                      const vector<column_t> &column_ids,
63:                                                      TableFilterCollection *filters) {
64: 	return make_unique<PragmaTableOperatorData>();
65: }
66: 
67: static void CheckConstraints(TableCatalogEntry *table, idx_t oid, bool &out_not_null, bool &out_pk) {
68: 	out_not_null = false;
69: 	out_pk = false;
70: 	// check all constraints
71: 	// FIXME: this is pretty inefficient, it probably doesn't matter
72: 	for (auto &constraint : table->bound_constraints) {
73: 		switch (constraint->type) {
74: 		case ConstraintType::NOT_NULL: {
75: 			auto &not_null = (BoundNotNullConstraint &)*constraint;
76: 			if (not_null.index == oid) {
77: 				out_not_null = true;
78: 			}
79: 			break;
80: 		}
81: 		case ConstraintType::UNIQUE: {
82: 			auto &unique = (BoundUniqueConstraint &)*constraint;
83: 			if (unique.is_primary_key && unique.key_set.find(oid) != unique.key_set.end()) {
84: 				out_pk = true;
85: 			}
86: 			break;
87: 		}
88: 		default:
89: 			break;
90: 		}
91: 	}
92: }
93: 
94: static void PragmaTableInfoTable(PragmaTableOperatorData &data, TableCatalogEntry *table, DataChunk &output) {
95: 	if (data.offset >= table->columns.size()) {
96: 		// finished returning values
97: 		return;
98: 	}
99: 	// start returning values
100: 	// either fill up the chunk or return all the remaining columns
101: 	idx_t next = MinValue<idx_t>(data.offset + STANDARD_VECTOR_SIZE, table->columns.size());
102: 	output.SetCardinality(next - data.offset);
103: 
104: 	for (idx_t i = data.offset; i < next; i++) {
105: 		bool not_null, pk;
106: 		auto index = i - data.offset;
107: 		auto &column = table->columns[i];
108: 		D_ASSERT(column.oid < (idx_t)NumericLimits<int32_t>::Maximum());
109: 		CheckConstraints(table, column.oid, not_null, pk);
110: 
111: 		// return values:
112: 		// "cid", PhysicalType::INT32
113: 		output.SetValue(0, index, Value::INTEGER((int32_t)column.oid));
114: 		// "name", PhysicalType::VARCHAR
115: 		output.SetValue(1, index, Value(column.name));
116: 		// "type", PhysicalType::VARCHAR
117: 		output.SetValue(2, index, Value(column.type.ToString()));
118: 		// "notnull", PhysicalType::BOOL
119: 		output.SetValue(3, index, Value::BOOLEAN(not_null));
120: 		// "dflt_value", PhysicalType::VARCHAR
121: 		Value def_value = column.default_value ? Value(column.default_value->ToString()) : Value();
122: 		output.SetValue(4, index, def_value);
123: 		// "pk", PhysicalType::BOOL
124: 		output.SetValue(5, index, Value::BOOLEAN(pk));
125: 	}
126: 	data.offset = next;
127: }
128: 
129: static void PragmaTableInfoView(PragmaTableOperatorData &data, ViewCatalogEntry *view, DataChunk &output) {
130: 	if (data.offset >= view->types.size()) {
131: 		// finished returning values
132: 		return;
133: 	}
134: 	// start returning values
135: 	// either fill up the chunk or return all the remaining columns
136: 	idx_t next = MinValue<idx_t>(data.offset + STANDARD_VECTOR_SIZE, view->types.size());
137: 	output.SetCardinality(next - data.offset);
138: 
139: 	for (idx_t i = data.offset; i < next; i++) {
140: 		auto index = i - data.offset;
141: 		auto type = view->types[index];
142: 		auto &name = view->aliases[index];
143: 		// return values:
144: 		// "cid", PhysicalType::INT32
145: 
146: 		output.SetValue(0, index, Value::INTEGER((int32_t)index));
147: 		// "name", PhysicalType::VARCHAR
148: 		output.SetValue(1, index, Value(name));
149: 		// "type", PhysicalType::VARCHAR
150: 		output.SetValue(2, index, Value(type.ToString()));
151: 		// "notnull", PhysicalType::BOOL
152: 		output.SetValue(3, index, Value::BOOLEAN(false));
153: 		// "dflt_value", PhysicalType::VARCHAR
154: 		output.SetValue(4, index, Value());
155: 		// "pk", PhysicalType::BOOL
156: 		output.SetValue(5, index, Value::BOOLEAN(false));
157: 	}
158: 	data.offset = next;
159: }
160: 
161: static void PragmaTableInfoFunction(ClientContext &context, const FunctionData *bind_data_p,
162:                                     FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
163: 	auto &bind_data = (PragmaTableFunctionData &)*bind_data_p;
164: 	auto &state = (PragmaTableOperatorData &)*operator_state;
165: 	switch (bind_data.entry->type) {
166: 	case CatalogType::TABLE_ENTRY:
167: 		PragmaTableInfoTable(state, (TableCatalogEntry *)bind_data.entry, output);
168: 		break;
169: 	case CatalogType::VIEW_ENTRY:
170: 		PragmaTableInfoView(state, (ViewCatalogEntry *)bind_data.entry, output);
171: 		break;
172: 	default:
173: 		throw NotImplementedException("Unimplemented catalog type for pragma_table_info");
174: 	}
175: }
176: 
177: void PragmaTableInfo::RegisterFunction(BuiltinFunctions &set) {
178: 	set.AddFunction(TableFunction("pragma_table_info", {LogicalType::VARCHAR}, PragmaTableInfoFunction,
179: 	                              PragmaTableInfoBind, PragmaTableInfoInit));
180: }
181: 
182: } // namespace duckdb
[end of src/function/table/system/pragma_table_info.cpp]
[start of src/function/table/unnest.cpp]
1: #include "duckdb/function/table/range.hpp"
2: #include "duckdb/common/algorithm.hpp"
3: 
4: namespace duckdb {
5: 
6: struct UnnestFunctionData : public TableFunctionData {
7: 	explicit UnnestFunctionData(Value value) : value(move(value)) {
8: 	}
9: 
10: 	Value value;
11: };
12: 
13: struct UnnestOperatorData : public FunctionOperatorData {
14: 	UnnestOperatorData() : current_count(0) {
15: 	}
16: 
17: 	idx_t current_count;
18: };
19: 
20: static unique_ptr<FunctionData> UnnestBind(ClientContext &context, vector<Value> &inputs,
21:                                            named_parameter_map_t &named_parameters,
22:                                            vector<LogicalType> &input_table_types, vector<string> &input_table_names,
23:                                            vector<LogicalType> &return_types, vector<string> &names) {
24: 	return_types.push_back(ListType::GetChildType(inputs[0].type()));
25: 	names.push_back(inputs[0].ToString());
26: 	return make_unique<UnnestFunctionData>(inputs[0]);
27: }
28: 
29: static unique_ptr<FunctionOperatorData> UnnestInit(ClientContext &context, const FunctionData *bind_data,
30:                                                    const vector<column_t> &column_ids, TableFilterCollection *filters) {
31: 	return make_unique<UnnestOperatorData>();
32: }
33: 
34: static void UnnestFunction(ClientContext &context, const FunctionData *bind_data_p,
35:                            FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
36: 	auto &bind_data = (UnnestFunctionData &)*bind_data_p;
37: 	auto &state = (UnnestOperatorData &)*operator_state;
38: 
39: 	auto &list_value = ListValue::GetChildren(bind_data.value);
40: 	idx_t count = 0;
41: 	for (; state.current_count < list_value.size() && count < STANDARD_VECTOR_SIZE; state.current_count++) {
42: 		output.data[0].SetValue(count, list_value[state.current_count]);
43: 		count++;
44: 	}
45: 	output.SetCardinality(count);
46: }
47: 
48: void UnnestTableFunction::RegisterFunction(BuiltinFunctions &set) {
49: 	TableFunction unnest_function("unnest", {LogicalTypeId::LIST}, UnnestFunction, UnnestBind, UnnestInit);
50: 	set.AddFunction(unnest_function);
51: }
52: 
53: } // namespace duckdb
[end of src/function/table/unnest.cpp]
[start of src/function/table/version/pragma_version.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: #include "duckdb/main/database.hpp"
3: 
4: namespace duckdb {
5: 
6: struct PragmaVersionData : public FunctionOperatorData {
7: 	PragmaVersionData() : finished(false) {
8: 	}
9: 	bool finished;
10: };
11: 
12: static unique_ptr<FunctionData> PragmaVersionBind(ClientContext &context, vector<Value> &inputs,
13:                                                   named_parameter_map_t &named_parameters,
14:                                                   vector<LogicalType> &input_table_types,
15:                                                   vector<string> &input_table_names, vector<LogicalType> &return_types,
16:                                                   vector<string> &names) {
17: 	names.emplace_back("library_version");
18: 	return_types.emplace_back(LogicalType::VARCHAR);
19: 	names.emplace_back("source_id");
20: 	return_types.emplace_back(LogicalType::VARCHAR);
21: 	return nullptr;
22: }
23: 
24: static unique_ptr<FunctionOperatorData> PragmaVersionInit(ClientContext &context, const FunctionData *bind_data,
25:                                                           const vector<column_t> &column_ids,
26:                                                           TableFilterCollection *filters) {
27: 	return make_unique<PragmaVersionData>();
28: }
29: 
30: static void PragmaVersionFunction(ClientContext &context, const FunctionData *bind_data,
31:                                   FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
32: 	auto &data = (PragmaVersionData &)*operator_state;
33: 	if (data.finished) {
34: 		// finished returning values
35: 		return;
36: 	}
37: 	output.SetCardinality(1);
38: 	output.SetValue(0, 0, DuckDB::LibraryVersion());
39: 	output.SetValue(1, 0, DuckDB::SourceID());
40: 	data.finished = true;
41: }
42: 
43: void PragmaVersion::RegisterFunction(BuiltinFunctions &set) {
44: 	set.AddFunction(TableFunction("pragma_version", {}, PragmaVersionFunction, PragmaVersionBind, PragmaVersionInit));
45: }
46: 
47: const char *DuckDB::SourceID() {
48: 	return DUCKDB_SOURCE_ID;
49: }
50: 
51: const char *DuckDB::LibraryVersion() {
52: 	return DUCKDB_VERSION;
53: }
54: 
55: string DuckDB::Platform() {
56: 	string os = "linux";
57: 	string arch = "amd64";
58: #ifdef _WIN32
59: 	os = "windows";
60: #elif defined(__APPLE__)
61: 	os = "osx";
62: #endif
63: #if defined(__aarch64__) || defined(__ARM_ARCH_ISA_A64)
64: 	arch = "arm64";
65: #endif
66: 	return os + "_" + arch;
67: }
68: 
69: } // namespace duckdb
[end of src/function/table/version/pragma_version.cpp]
[start of src/function/table_function.cpp]
1: #include "duckdb/function/table_function.hpp"
2: 
3: namespace duckdb {
4: 
5: FunctionOperatorData::~FunctionOperatorData() {
6: }
7: 
8: TableFilterCollection::TableFilterCollection(TableFilterSet *table_filters) : table_filters(table_filters) {
9: }
10: 
11: TableFunction::TableFunction(string name, vector<LogicalType> arguments, table_function_t function,
12:                              table_function_bind_t bind, table_function_init_t init, table_statistics_t statistics,
13:                              table_function_cleanup_t cleanup, table_function_dependency_t dependency,
14:                              table_function_cardinality_t cardinality,
15:                              table_function_pushdown_complex_filter_t pushdown_complex_filter,
16:                              table_function_to_string_t to_string, table_function_max_threads_t max_threads,
17:                              table_function_init_parallel_state_t init_parallel_state,
18:                              table_function_parallel_t parallel_function, table_function_init_parallel_t parallel_init,
19:                              table_function_parallel_state_next_t parallel_state_next, bool projection_pushdown,
20:                              bool filter_pushdown, table_function_progress_t query_progress)
21:     : SimpleNamedParameterFunction(move(name), move(arguments)), bind(bind), init(init), function(function),
22:       statistics(statistics), cleanup(cleanup), dependency(dependency), cardinality(cardinality),
23:       pushdown_complex_filter(pushdown_complex_filter), to_string(to_string), max_threads(max_threads),
24:       init_parallel_state(init_parallel_state), parallel_function(parallel_function), parallel_init(parallel_init),
25:       parallel_state_next(parallel_state_next), table_scan_progress(query_progress),
26:       projection_pushdown(projection_pushdown), filter_pushdown(filter_pushdown) {
27: }
28: 
29: TableFunction::TableFunction(const vector<LogicalType> &arguments, table_function_t function,
30:                              table_function_bind_t bind, table_function_init_t init, table_statistics_t statistics,
31:                              table_function_cleanup_t cleanup, table_function_dependency_t dependency,
32:                              table_function_cardinality_t cardinality,
33:                              table_function_pushdown_complex_filter_t pushdown_complex_filter,
34:                              table_function_to_string_t to_string, table_function_max_threads_t max_threads,
35:                              table_function_init_parallel_state_t init_parallel_state,
36:                              table_function_parallel_t parallel_function, table_function_init_parallel_t parallel_init,
37:                              table_function_parallel_state_next_t parallel_state_next, bool projection_pushdown,
38:                              bool filter_pushdown, table_function_progress_t query_progress)
39:     : TableFunction(string(), arguments, function, bind, init, statistics, cleanup, dependency, cardinality,
40:                     pushdown_complex_filter, to_string, max_threads, init_parallel_state, parallel_function,
41:                     parallel_init, parallel_state_next, projection_pushdown, filter_pushdown, query_progress) {
42: }
43: TableFunction::TableFunction() : SimpleNamedParameterFunction("", {}) {
44: }
45: 
46: } // namespace duckdb
[end of src/function/table_function.cpp]
[start of src/include/duckdb.h]
1: //===----------------------------------------------------------------------===//
2: //
3: //                         DuckDB
4: //
5: // duckdb.h
6: //
7: //
8: //===----------------------------------------------------------------------===//
9: 
10: #pragma once
11: 
12: // duplicate of duckdb/main/winapi.hpp
13: #ifndef DUCKDB_API
14: #ifdef _WIN32
15: #if defined(DUCKDB_BUILD_LIBRARY) && !defined(DUCKDB_BUILD_LOADABLE_EXTENSION)
16: #define DUCKDB_API __declspec(dllexport)
17: #else
18: #define DUCKDB_API __declspec(dllimport)
19: #endif
20: #else
21: #define DUCKDB_API
22: #endif
23: #endif
24: 
25: // duplicate of duckdb/common/constants.hpp
26: #ifndef DUCKDB_API_0_3_1
27: #define DUCKDB_API_0_3_1 1
28: #endif
29: #ifndef DUCKDB_API_0_3_2
30: #define DUCKDB_API_0_3_2 2
31: #endif
32: #ifndef DUCKDB_API_LATEST
33: #define DUCKDB_API_LATEST DUCKDB_API_0_3_2
34: #endif
35: 
36: #ifndef DUCKDB_API_VERSION
37: #define DUCKDB_API_VERSION DUCKDB_API_LATEST
38: #endif
39: 
40: #include <stdbool.h>
41: #include <stdint.h>
42: #include <stdlib.h>
43: 
44: #ifdef __cplusplus
45: extern "C" {
46: #endif
47: 
48: //===--------------------------------------------------------------------===//
49: // Type Information
50: //===--------------------------------------------------------------------===//
51: typedef uint64_t idx_t;
52: 
53: typedef enum DUCKDB_TYPE {
54: 	DUCKDB_TYPE_INVALID = 0,
55: 	// bool
56: 	DUCKDB_TYPE_BOOLEAN,
57: 	// int8_t
58: 	DUCKDB_TYPE_TINYINT,
59: 	// int16_t
60: 	DUCKDB_TYPE_SMALLINT,
61: 	// int32_t
62: 	DUCKDB_TYPE_INTEGER,
63: 	// int64_t
64: 	DUCKDB_TYPE_BIGINT,
65: 	// uint8_t
66: 	DUCKDB_TYPE_UTINYINT,
67: 	// uint16_t
68: 	DUCKDB_TYPE_USMALLINT,
69: 	// uint32_t
70: 	DUCKDB_TYPE_UINTEGER,
71: 	// uint64_t
72: 	DUCKDB_TYPE_UBIGINT,
73: 	// float
74: 	DUCKDB_TYPE_FLOAT,
75: 	// double
76: 	DUCKDB_TYPE_DOUBLE,
77: 	// duckdb_timestamp
78: 	DUCKDB_TYPE_TIMESTAMP,
79: 	// duckdb_date
80: 	DUCKDB_TYPE_DATE,
81: 	// duckdb_time
82: 	DUCKDB_TYPE_TIME,
83: 	// duckdb_interval
84: 	DUCKDB_TYPE_INTERVAL,
85: 	// duckdb_hugeint
86: 	DUCKDB_TYPE_HUGEINT,
87: 	// const char*
88: 	DUCKDB_TYPE_VARCHAR,
89: 	// duckdb_blob
90: 	DUCKDB_TYPE_BLOB,
91: 	// decimal
92: 	DUCKDB_TYPE_DECIMAL
93: } duckdb_type;
94: 
95: //! Days are stored as days since 1970-01-01
96: //! Use the duckdb_from_date/duckdb_to_date function to extract individual information
97: typedef struct {
98: 	int32_t days;
99: } duckdb_date;
100: 
101: typedef struct {
102: 	int32_t year;
103: 	int8_t month;
104: 	int8_t day;
105: } duckdb_date_struct;
106: 
107: //! Time is stored as microseconds since 00:00:00
108: //! Use the duckdb_from_time/duckdb_to_time function to extract individual information
109: typedef struct {
110: 	int64_t micros;
111: } duckdb_time;
112: 
113: typedef struct {
114: 	int8_t hour;
115: 	int8_t min;
116: 	int8_t sec;
117: 	int32_t micros;
118: } duckdb_time_struct;
119: 
120: //! Timestamps are stored as microseconds since 1970-01-01
121: //! Use the duckdb_from_timestamp/duckdb_to_timestamp function to extract individual information
122: typedef struct {
123: 	int64_t micros;
124: } duckdb_timestamp;
125: 
126: typedef struct {
127: 	duckdb_date_struct date;
128: 	duckdb_time_struct time;
129: } duckdb_timestamp_struct;
130: 
131: typedef struct {
132: 	int32_t months;
133: 	int32_t days;
134: 	int64_t micros;
135: } duckdb_interval;
136: 
137: //! Hugeints are composed in a (lower, upper) component
138: //! The value of the hugeint is upper * 2^64 + lower
139: //! For easy usage, the functions duckdb_hugeint_to_double/duckdb_double_to_hugeint are recommended
140: typedef struct {
141: 	uint64_t lower;
142: 	int64_t upper;
143: } duckdb_hugeint;
144: 
145: typedef struct {
146: 	uint8_t width;
147: 	uint8_t scale;
148: 
149: 	duckdb_hugeint value;
150: } duckdb_decimal;
151: 
152: typedef struct {
153: 	void *data;
154: 	idx_t size;
155: } duckdb_blob;
156: 
157: typedef struct {
158: #if DUCKDB_API_VERSION < DUCKDB_API_0_3_2
159: 	void *data;
160: 	bool *nullmask;
161: 	duckdb_type type;
162: 	char *name;
163: #else
164: 	// deprecated, use duckdb_column_data
165: 	void *__deprecated_data;
166: 	// deprecated, use duckdb_nullmask_data
167: 	bool *__deprecated_nullmask;
168: 	// deprecated, use duckdb_column_type
169: 	duckdb_type __deprecated_type;
170: 	// deprecated, use duckdb_column_name
171: 	char *__deprecated_name;
172: #endif
173: 	void *internal_data;
174: } duckdb_column;
175: 
176: typedef struct {
177: #if DUCKDB_API_VERSION < DUCKDB_API_0_3_2
178: 	idx_t column_count;
179: 	idx_t row_count;
180: 	idx_t rows_changed;
181: 	duckdb_column *columns;
182: 	char *error_message;
183: #else
184: 	// deprecated, use duckdb_column_count
185: 	idx_t __deprecated_column_count;
186: 	// deprecated, use duckdb_row_count
187: 	idx_t __deprecated_row_count;
188: 	// deprecated, use duckdb_rows_changed
189: 	idx_t __deprecated_rows_changed;
190: 	// deprecated, use duckdb_column_ family of functions
191: 	duckdb_column *__deprecated_columns;
192: 	// deprecated, use duckdb_result_error
193: 	char *__deprecated_error_message;
194: #endif
195: 	void *internal_data;
196: } duckdb_result;
197: 
198: typedef void *duckdb_database;
199: typedef void *duckdb_connection;
200: typedef void *duckdb_prepared_statement;
201: typedef void *duckdb_appender;
202: typedef void *duckdb_arrow;
203: typedef void *duckdb_config;
204: typedef void *duckdb_arrow_schema;
205: typedef void *duckdb_arrow_array;
206: 
207: typedef enum { DuckDBSuccess = 0, DuckDBError = 1 } duckdb_state;
208: 
209: //===--------------------------------------------------------------------===//
210: // Open/Connect
211: //===--------------------------------------------------------------------===//
212: 
213: /*!
214: Creates a new database or opens an existing database file stored at the the given path.
215: If no path is given a new in-memory database is created instead.
216: 
217: * path: Path to the database file on disk, or `nullptr` or `:memory:` to open an in-memory database.
218: * out_database: The result database object.
219: * returns: `DuckDBSuccess` on success or `DuckDBError` on failure.
220: */
221: DUCKDB_API duckdb_state duckdb_open(const char *path, duckdb_database *out_database);
222: 
223: /*!
224: Extended version of duckdb_open. Creates a new database or opens an existing database file stored at the the given path.
225: 
226: * path: Path to the database file on disk, or `nullptr` or `:memory:` to open an in-memory database.
227: * out_database: The result database object.
228: * config: (Optional) configuration used to start up the database system.
229: * out_error: If set and the function returns DuckDBError, this will contain the reason why the start-up failed.
230: Note that the error must be freed using `duckdb_free`.
231: * returns: `DuckDBSuccess` on success or `DuckDBError` on failure.
232: */
233: DUCKDB_API duckdb_state duckdb_open_ext(const char *path, duckdb_database *out_database, duckdb_config config,
234:                                         char **out_error);
235: 
236: /*!
237: Closes the specified database and de-allocates all memory allocated for that database.
238: This should be called after you are done with any database allocated through `duckdb_open`.
239: Note that failing to call `duckdb_close` (in case of e.g. a program crash) will not cause data corruption.
240: Still it is recommended to always correctly close a database object after you are done with it.
241: 
242: * database: The database object to shut down.
243: */
244: DUCKDB_API void duckdb_close(duckdb_database *database);
245: 
246: /*!
247: Opens a connection to a database. Connections are required to query the database, and store transactional state
248: associated with the connection.
249: 
250: * database: The database file to connect to.
251: * out_connection: The result connection object.
252: * returns: `DuckDBSuccess` on success or `DuckDBError` on failure.
253: */
254: DUCKDB_API duckdb_state duckdb_connect(duckdb_database database, duckdb_connection *out_connection);
255: 
256: /*!
257: Closes the specified connection and de-allocates all memory allocated for that connection.
258: 
259: * connection: The connection to close.
260: */
261: DUCKDB_API void duckdb_disconnect(duckdb_connection *connection);
262: 
263: //===--------------------------------------------------------------------===//
264: // Configuration
265: //===--------------------------------------------------------------------===//
266: /*!
267: Initializes an empty configuration object that can be used to provide start-up options for the DuckDB instance
268: through `duckdb_open_ext`.
269: 
270: This will always succeed unless there is a malloc failure.
271: 
272: * out_config: The result configuration object.
273: * returns: `DuckDBSuccess` on success or `DuckDBError` on failure.
274: */
275: DUCKDB_API duckdb_state duckdb_create_config(duckdb_config *out_config);
276: 
277: /*!
278: This returns the total amount of configuration options available for usage with `duckdb_get_config_flag`.
279: 
280: This should not be called in a loop as it internally loops over all the options.
281: 
282: * returns: The amount of config options available.
283: */
284: DUCKDB_API size_t duckdb_config_count();
285: 
286: /*!
287: Obtains a human-readable name and description of a specific configuration option. This can be used to e.g.
288: display configuration options. This will succeed unless `index` is out of range (i.e. `>= duckdb_config_count`).
289: 
290: The result name or description MUST NOT be freed.
291: 
292: * index: The index of the configuration option (between 0 and `duckdb_config_count`)
293: * out_name: A name of the configuration flag.
294: * out_description: A description of the configuration flag.
295: * returns: `DuckDBSuccess` on success or `DuckDBError` on failure.
296: */
297: DUCKDB_API duckdb_state duckdb_get_config_flag(size_t index, const char **out_name, const char **out_description);
298: 
299: /*!
300: Sets the specified option for the specified configuration. The configuration option is indicated by name.
301: To obtain a list of config options, see `duckdb_get_config_flag`.
302: 
303: In the source code, configuration options are defined in `config.cpp`.
304: 
305: This can fail if either the name is invalid, or if the value provided for the option is invalid.
306: 
307: * duckdb_config: The configuration object to set the option on.
308: * name: The name of the configuration flag to set.
309: * option: The value to set the configuration flag to.
310: * returns: `DuckDBSuccess` on success or `DuckDBError` on failure.
311: */
312: DUCKDB_API duckdb_state duckdb_set_config(duckdb_config config, const char *name, const char *option);
313: 
314: /*!
315: Destroys the specified configuration option and de-allocates all memory allocated for the object.
316: 
317: * config: The configuration object to destroy.
318: */
319: DUCKDB_API void duckdb_destroy_config(duckdb_config *config);
320: 
321: //===--------------------------------------------------------------------===//
322: // Query Execution
323: //===--------------------------------------------------------------------===//
324: /*!
325: Executes a SQL query within a connection and stores the full (materialized) result in the out_result pointer.
326: If the query fails to execute, DuckDBError is returned and the error message can be retrieved by calling
327: `duckdb_result_error`.
328: 
329: Note that after running `duckdb_query`, `duckdb_destroy_result` must be called on the result object even if the
330: query fails, otherwise the error stored within the result will not be freed correctly.
331: 
332: * connection: The connection to perform the query in.
333: * query: The SQL query to run.
334: * out_result: The query result.
335: * returns: `DuckDBSuccess` on success or `DuckDBError` on failure.
336: */
337: DUCKDB_API duckdb_state duckdb_query(duckdb_connection connection, const char *query, duckdb_result *out_result);
338: 
339: /*!
340: Closes the result and de-allocates all memory allocated for that connection.
341: 
342: * result: The result to destroy.
343: */
344: DUCKDB_API void duckdb_destroy_result(duckdb_result *result);
345: 
346: /*!
347: Returns the column name of the specified column. The result should not need be freed; the column names will
348: automatically be destroyed when the result is destroyed.
349: 
350: Returns `NULL` if the column is out of range.
351: 
352: * result: The result object to fetch the column name from.
353: * col: The column index.
354: * returns: The column name of the specified column.
355: */
356: DUCKDB_API const char *duckdb_column_name(duckdb_result *result, idx_t col);
357: 
358: /*!
359: Returns the column type of the specified column.
360: 
361: Returns `DUCKDB_TYPE_INVALID` if the column is out of range.
362: 
363: * result: The result object to fetch the column type from.
364: * col: The column index.
365: * returns: The column type of the specified column.
366: */
367: DUCKDB_API duckdb_type duckdb_column_type(duckdb_result *result, idx_t col);
368: 
369: /*!
370: Returns the number of columns present in a the result object.
371: 
372: * result: The result object.
373: * returns: The number of columns present in the result object.
374: */
375: DUCKDB_API idx_t duckdb_column_count(duckdb_result *result);
376: 
377: /*!
378: Returns the number of rows present in a the result object.
379: 
380: * result: The result object.
381: * returns: The number of rows present in the result object.
382: */
383: DUCKDB_API idx_t duckdb_row_count(duckdb_result *result);
384: 
385: /*!
386: Returns the number of rows changed by the query stored in the result. This is relevant only for INSERT/UPDATE/DELETE
387: queries. For other queries the rows_changed will be 0.
388: 
389: * result: The result object.
390: * returns: The number of rows changed.
391: */
392: DUCKDB_API idx_t duckdb_rows_changed(duckdb_result *result);
393: 
394: /*!
395: Returns the data of a specific column of a result in columnar format. This is the fastest way of accessing data in a
396: query result, as no conversion or type checking must be performed (outside of the original switch). If performance
397: is a concern, it is recommended to use this API over the `duckdb_value` functions.
398: 
399: The function returns a dense array which contains the result data. The exact type stored in the array depends on the
400: corresponding duckdb_type (as provided by `duckdb_column_type`). For the exact type by which the data should be
401: accessed, see the comments in [the types section](types) or the `DUCKDB_TYPE` enum.
402: 
403: For example, for a column of type `DUCKDB_TYPE_INTEGER`, rows can be accessed in the following manner:
404: ```c
405: int32_t *data = (int32_t *) duckdb_column_data(&result, 0);
406: printf("Data for row %d: %d\n", row, data[row]);
407: ```
408: 
409: * result: The result object to fetch the column data from.
410: * col: The column index.
411: * returns: The column data of the specified column.
412: */
413: DUCKDB_API void *duckdb_column_data(duckdb_result *result, idx_t col);
414: 
415: /*!
416: Returns the nullmask of a specific column of a result in columnar format. The nullmask indicates for every row
417: whether or not the corresponding row is `NULL`. If a row is `NULL`, the values present in the array provided
418: by `duckdb_column_data` are undefined.
419: 
420: ```c
421: int32_t *data = (int32_t *) duckdb_column_data(&result, 0);
422: bool *nullmask = duckdb_nullmask_data(&result, 0);
423: if (nullmask[row]) {
424:     printf("Data for row %d: NULL\n", row);
425: } else {
426:     printf("Data for row %d: %d\n", row, data[row]);
427: }
428: ```
429: 
430: * result: The result object to fetch the nullmask from.
431: * col: The column index.
432: * returns: The nullmask of the specified column.
433: */
434: DUCKDB_API bool *duckdb_nullmask_data(duckdb_result *result, idx_t col);
435: 
436: /*!
437: Returns the error message contained within the result. The error is only set if `duckdb_query` returns `DuckDBError`.
438: 
439: The result of this function must not be freed. It will be cleaned up when `duckdb_destroy_result` is called.
440: 
441: * result: The result object to fetch the nullmask from.
442: * returns: The error of the result.
443: */
444: DUCKDB_API char *duckdb_result_error(duckdb_result *result);
445: 
446: //===--------------------------------------------------------------------===//
447: // Result Functions
448: //===--------------------------------------------------------------------===//
449: 
450: // Safe fetch functions
451: // These functions will perform conversions if necessary.
452: // On failure (e.g. if conversion cannot be performed or if the value is NULL) a default value is returned.
453: // Note that these functions are slow since they perform bounds checking and conversion
454: // For fast access of values prefer using duckdb_column_data and duckdb_nullmask_data
455: 
456: /*!
457:  * returns: The boolean value at the specified location, or false if the value cannot be converted.
458:  */
459: DUCKDB_API bool duckdb_value_boolean(duckdb_result *result, idx_t col, idx_t row);
460: 
461: /*!
462:  * returns: The int8_t value at the specified location, or 0 if the value cannot be converted.
463:  */
464: DUCKDB_API int8_t duckdb_value_int8(duckdb_result *result, idx_t col, idx_t row);
465: 
466: /*!
467:  * returns: The int16_t value at the specified location, or 0 if the value cannot be converted.
468:  */
469: DUCKDB_API int16_t duckdb_value_int16(duckdb_result *result, idx_t col, idx_t row);
470: 
471: /*!
472:  * returns: The int32_t value at the specified location, or 0 if the value cannot be converted.
473:  */
474: DUCKDB_API int32_t duckdb_value_int32(duckdb_result *result, idx_t col, idx_t row);
475: 
476: /*!
477:  * returns: The int64_t value at the specified location, or 0 if the value cannot be converted.
478:  */
479: DUCKDB_API int64_t duckdb_value_int64(duckdb_result *result, idx_t col, idx_t row);
480: 
481: /*!
482:  * returns: The duckdb_hugeint value at the specified location, or 0 if the value cannot be converted.
483:  */
484: DUCKDB_API duckdb_hugeint duckdb_value_hugeint(duckdb_result *result, idx_t col, idx_t row);
485: 
486: /*!
487:  * returns: The duckdb_decimal value at the specified location, or 0 if the value cannot be converted.
488:  */
489: DUCKDB_API duckdb_decimal duckdb_value_decimal(duckdb_result *result, idx_t col, idx_t row);
490: 
491: /*!
492:  * returns: The uint8_t value at the specified location, or 0 if the value cannot be converted.
493:  */
494: DUCKDB_API uint8_t duckdb_value_uint8(duckdb_result *result, idx_t col, idx_t row);
495: 
496: /*!
497:  * returns: The uint16_t value at the specified location, or 0 if the value cannot be converted.
498:  */
499: DUCKDB_API uint16_t duckdb_value_uint16(duckdb_result *result, idx_t col, idx_t row);
500: 
501: /*!
502:  * returns: The uint32_t value at the specified location, or 0 if the value cannot be converted.
503:  */
504: DUCKDB_API uint32_t duckdb_value_uint32(duckdb_result *result, idx_t col, idx_t row);
505: 
506: /*!
507:  * returns: The uint64_t value at the specified location, or 0 if the value cannot be converted.
508:  */
509: DUCKDB_API uint64_t duckdb_value_uint64(duckdb_result *result, idx_t col, idx_t row);
510: 
511: /*!
512:  * returns: The float value at the specified location, or 0 if the value cannot be converted.
513:  */
514: DUCKDB_API float duckdb_value_float(duckdb_result *result, idx_t col, idx_t row);
515: 
516: /*!
517:  * returns: The double value at the specified location, or 0 if the value cannot be converted.
518:  */
519: DUCKDB_API double duckdb_value_double(duckdb_result *result, idx_t col, idx_t row);
520: 
521: /*!
522:  * returns: The duckdb_date value at the specified location, or 0 if the value cannot be converted.
523:  */
524: DUCKDB_API duckdb_date duckdb_value_date(duckdb_result *result, idx_t col, idx_t row);
525: 
526: /*!
527:  * returns: The duckdb_time value at the specified location, or 0 if the value cannot be converted.
528:  */
529: DUCKDB_API duckdb_time duckdb_value_time(duckdb_result *result, idx_t col, idx_t row);
530: 
531: /*!
532:  * returns: The duckdb_timestamp value at the specified location, or 0 if the value cannot be converted.
533:  */
534: DUCKDB_API duckdb_timestamp duckdb_value_timestamp(duckdb_result *result, idx_t col, idx_t row);
535: 
536: /*!
537:  * returns: The duckdb_interval value at the specified location, or 0 if the value cannot be converted.
538:  */
539: DUCKDB_API duckdb_interval duckdb_value_interval(duckdb_result *result, idx_t col, idx_t row);
540: 
541: /*!
542: * returns: The char* value at the specified location, or nullptr if the value cannot be converted.
543: The result must be freed with `duckdb_free`.
544: */
545: DUCKDB_API char *duckdb_value_varchar(duckdb_result *result, idx_t col, idx_t row);
546: 
547: /*!
548: * returns: The char* value at the specified location. ONLY works on VARCHAR columns and does not auto-cast.
549: If the column is NOT a VARCHAR column this function will return NULL.
550: 
551: The result must NOT be freed.
552: */
553: DUCKDB_API char *duckdb_value_varchar_internal(duckdb_result *result, idx_t col, idx_t row);
554: 
555: /*!
556: * returns: The duckdb_blob value at the specified location. Returns a blob with blob.data set to nullptr if the
557: value cannot be converted. The resulting "blob.data" must be freed with `duckdb_free.`
558: */
559: DUCKDB_API duckdb_blob duckdb_value_blob(duckdb_result *result, idx_t col, idx_t row);
560: 
561: /*!
562:  * returns: Returns true if the value at the specified index is NULL, and false otherwise.
563:  */
564: DUCKDB_API bool duckdb_value_is_null(duckdb_result *result, idx_t col, idx_t row);
565: 
566: //===--------------------------------------------------------------------===//
567: // Helpers
568: //===--------------------------------------------------------------------===//
569: /*!
570: Allocate `size` bytes of memory using the duckdb internal malloc function. Any memory allocated in this manner
571: should be freed using `duckdb_free`.
572: 
573: * size: The number of bytes to allocate.
574: * returns: A pointer to the allocated memory region.
575: */
576: DUCKDB_API void *duckdb_malloc(size_t size);
577: 
578: /*!
579: Free a value returned from `duckdb_malloc`, `duckdb_value_varchar` or `duckdb_value_blob`.
580: 
581: * ptr: The memory region to de-allocate.
582: */
583: DUCKDB_API void duckdb_free(void *ptr);
584: 
585: //===--------------------------------------------------------------------===//
586: // Date/Time/Timestamp Helpers
587: //===--------------------------------------------------------------------===//
588: /*!
589: Decompose a `duckdb_date` object into year, month and date (stored as `duckdb_date_struct`).
590: 
591: * date: The date object, as obtained from a `DUCKDB_TYPE_DATE` column.
592: * returns: The `duckdb_date_struct` with the decomposed elements.
593: */
594: DUCKDB_API duckdb_date_struct duckdb_from_date(duckdb_date date);
595: 
596: /*!
597: Re-compose a `duckdb_date` from year, month and date (`duckdb_date_struct`).
598: 
599: * date: The year, month and date stored in a `duckdb_date_struct`.
600: * returns: The `duckdb_date` element.
601: */
602: DUCKDB_API duckdb_date duckdb_to_date(duckdb_date_struct date);
603: 
604: /*!
605: Decompose a `duckdb_time` object into hour, minute, second and microsecond (stored as `duckdb_time_struct`).
606: 
607: * time: The time object, as obtained from a `DUCKDB_TYPE_TIME` column.
608: * returns: The `duckdb_time_struct` with the decomposed elements.
609: */
610: DUCKDB_API duckdb_time_struct duckdb_from_time(duckdb_time time);
611: 
612: /*!
613: Re-compose a `duckdb_time` from hour, minute, second and microsecond (`duckdb_time_struct`).
614: 
615: * time: The hour, minute, second and microsecond in a `duckdb_time_struct`.
616: * returns: The `duckdb_time` element.
617: */
618: DUCKDB_API duckdb_time duckdb_to_time(duckdb_time_struct time);
619: 
620: /*!
621: Decompose a `duckdb_timestamp` object into a `duckdb_timestamp_struct`.
622: 
623: * ts: The ts object, as obtained from a `DUCKDB_TYPE_TIMESTAMP` column.
624: * returns: The `duckdb_timestamp_struct` with the decomposed elements.
625: */
626: DUCKDB_API duckdb_timestamp_struct duckdb_from_timestamp(duckdb_timestamp ts);
627: 
628: /*!
629: Re-compose a `duckdb_timestamp` from a duckdb_timestamp_struct.
630: 
631: * ts: The de-composed elements in a `duckdb_timestamp_struct`.
632: * returns: The `duckdb_timestamp` element.
633: */
634: DUCKDB_API duckdb_timestamp duckdb_to_timestamp(duckdb_timestamp_struct ts);
635: 
636: //===--------------------------------------------------------------------===//
637: // Hugeint Helpers
638: //===--------------------------------------------------------------------===//
639: /*!
640: Converts a duckdb_hugeint object (as obtained from a `DUCKDB_TYPE_HUGEINT` column) into a double.
641: 
642: * val: The hugeint value.
643: * returns: The converted `double` element.
644: */
645: DUCKDB_API double duckdb_hugeint_to_double(duckdb_hugeint val);
646: 
647: /*!
648: Converts a double value to a duckdb_hugeint object.
649: 
650: If the conversion fails because the double value is too big the result will be 0.
651: 
652: * val: The double value.
653: * returns: The converted `duckdb_hugeint` element.
654: */
655: DUCKDB_API duckdb_hugeint duckdb_double_to_hugeint(double val);
656: 
657: //===--------------------------------------------------------------------===//
658: // Decimal Helpers
659: //===--------------------------------------------------------------------===//
660: /*!
661: Converts a duckdb_decimal object (as obtained from a `DUCKDB_TYPE_DECIMAL` column) into a double.
662: 
663: * val: The decimal value.
664: * returns: The converted `double` element.
665: */
666: DUCKDB_API double duckdb_decimal_to_double(duckdb_decimal val);
667: 
668: //===--------------------------------------------------------------------===//
669: // Prepared Statements
670: //===--------------------------------------------------------------------===//
671: // A prepared statement is a parameterized query that allows you to bind parameters to it.
672: // * This is useful to easily supply parameters to functions and avoid SQL injection attacks.
673: // * This is useful to speed up queries that you will execute several times with different parameters.
674: // Because the query will only be parsed, bound, optimized and planned once during the prepare stage,
675: // rather than once per execution.
676: // For example:
677: //   SELECT * FROM tbl WHERE id=?
678: // Or a query with multiple parameters:
679: //   SELECT * FROM tbl WHERE id=$1 OR name=$2
680: 
681: /*!
682: Create a prepared statement object from a query.
683: 
684: Note that after calling `duckdb_prepare`, the prepared statement should always be destroyed using
685: `duckdb_destroy_prepare`, even if the prepare fails.
686: 
687: If the prepare fails, `duckdb_prepare_error` can be called to obtain the reason why the prepare failed.
688: 
689: * connection: The connection object
690: * query: The SQL query to prepare
691: * out_prepared_statement: The resulting prepared statement object
692: * returns: `DuckDBSuccess` on success or `DuckDBError` on failure.
693: */
694: DUCKDB_API duckdb_state duckdb_prepare(duckdb_connection connection, const char *query,
695:                                        duckdb_prepared_statement *out_prepared_statement);
696: 
697: /*!
698: Closes the prepared statement and de-allocates all memory allocated for that connection.
699: 
700: * prepared_statement: The prepared statement to destroy.
701: */
702: DUCKDB_API void duckdb_destroy_prepare(duckdb_prepared_statement *prepared_statement);
703: 
704: /*!
705: Returns the error message associated with the given prepared statement.
706: If the prepared statement has no error message, this returns `nullptr` instead.
707: 
708: The error message should not be freed. It will be de-allocated when `duckdb_destroy_prepare` is called.
709: 
710: * prepared_statement: The prepared statement to obtain the error from.
711: * returns: The error message, or `nullptr` if there is none.
712: */
713: DUCKDB_API const char *duckdb_prepare_error(duckdb_prepared_statement prepared_statement);
714: 
715: /*!
716: Returns the number of parameters that can be provided to the given prepared statement.
717: 
718: Returns 0 if the query was not successfully prepared.
719: 
720: * prepared_statement: The prepared statement to obtain the number of parameters for.
721: */
722: DUCKDB_API idx_t duckdb_nparams(duckdb_prepared_statement prepared_statement);
723: 
724: /*!
725: Returns the parameter type for the parameter at the given index.
726: 
727: Returns `DUCKDB_TYPE_INVALID` if the parameter index is out of range or the statement was not successfully prepared.
728: 
729: * prepared_statement: The prepared statement.
730: * param_idx: The parameter index.
731: * returns: The parameter type
732: */
733: DUCKDB_API duckdb_type duckdb_param_type(duckdb_prepared_statement prepared_statement, idx_t param_idx);
734: 
735: /*!
736: Binds a bool value to the prepared statement at the specified index.
737: */
738: DUCKDB_API duckdb_state duckdb_bind_boolean(duckdb_prepared_statement prepared_statement, idx_t param_idx, bool val);
739: 
740: /*!
741: Binds an int8_t value to the prepared statement at the specified index.
742: */
743: DUCKDB_API duckdb_state duckdb_bind_int8(duckdb_prepared_statement prepared_statement, idx_t param_idx, int8_t val);
744: 
745: /*!
746: Binds an int16_t value to the prepared statement at the specified index.
747: */
748: DUCKDB_API duckdb_state duckdb_bind_int16(duckdb_prepared_statement prepared_statement, idx_t param_idx, int16_t val);
749: 
750: /*!
751: Binds an int32_t value to the prepared statement at the specified index.
752: */
753: DUCKDB_API duckdb_state duckdb_bind_int32(duckdb_prepared_statement prepared_statement, idx_t param_idx, int32_t val);
754: 
755: /*!
756: Binds an int64_t value to the prepared statement at the specified index.
757: */
758: DUCKDB_API duckdb_state duckdb_bind_int64(duckdb_prepared_statement prepared_statement, idx_t param_idx, int64_t val);
759: 
760: /*!
761: Binds an duckdb_hugeint value to the prepared statement at the specified index.
762: */
763: DUCKDB_API duckdb_state duckdb_bind_hugeint(duckdb_prepared_statement prepared_statement, idx_t param_idx,
764:                                             duckdb_hugeint val);
765: 
766: /*!
767: Binds an uint8_t value to the prepared statement at the specified index.
768: */
769: DUCKDB_API duckdb_state duckdb_bind_uint8(duckdb_prepared_statement prepared_statement, idx_t param_idx, uint8_t val);
770: 
771: /*!
772: Binds an uint16_t value to the prepared statement at the specified index.
773: */
774: DUCKDB_API duckdb_state duckdb_bind_uint16(duckdb_prepared_statement prepared_statement, idx_t param_idx, uint16_t val);
775: 
776: /*!
777: Binds an uint32_t value to the prepared statement at the specified index.
778: */
779: DUCKDB_API duckdb_state duckdb_bind_uint32(duckdb_prepared_statement prepared_statement, idx_t param_idx, uint32_t val);
780: 
781: /*!
782: Binds an uint64_t value to the prepared statement at the specified index.
783: */
784: DUCKDB_API duckdb_state duckdb_bind_uint64(duckdb_prepared_statement prepared_statement, idx_t param_idx, uint64_t val);
785: 
786: /*!
787: Binds an float value to the prepared statement at the specified index.
788: */
789: DUCKDB_API duckdb_state duckdb_bind_float(duckdb_prepared_statement prepared_statement, idx_t param_idx, float val);
790: 
791: /*!
792: Binds an double value to the prepared statement at the specified index.
793: */
794: DUCKDB_API duckdb_state duckdb_bind_double(duckdb_prepared_statement prepared_statement, idx_t param_idx, double val);
795: 
796: /*!
797: Binds a duckdb_date value to the prepared statement at the specified index.
798: */
799: DUCKDB_API duckdb_state duckdb_bind_date(duckdb_prepared_statement prepared_statement, idx_t param_idx,
800:                                          duckdb_date val);
801: 
802: /*!
803: Binds a duckdb_time value to the prepared statement at the specified index.
804: */
805: DUCKDB_API duckdb_state duckdb_bind_time(duckdb_prepared_statement prepared_statement, idx_t param_idx,
806:                                          duckdb_time val);
807: 
808: /*!
809: Binds a duckdb_timestamp value to the prepared statement at the specified index.
810: */
811: DUCKDB_API duckdb_state duckdb_bind_timestamp(duckdb_prepared_statement prepared_statement, idx_t param_idx,
812:                                               duckdb_timestamp val);
813: 
814: /*!
815: Binds a duckdb_interval value to the prepared statement at the specified index.
816: */
817: DUCKDB_API duckdb_state duckdb_bind_interval(duckdb_prepared_statement prepared_statement, idx_t param_idx,
818:                                              duckdb_interval val);
819: 
820: /*!
821: Binds a null-terminated varchar value to the prepared statement at the specified index.
822: */
823: DUCKDB_API duckdb_state duckdb_bind_varchar(duckdb_prepared_statement prepared_statement, idx_t param_idx,
824:                                             const char *val);
825: 
826: /*!
827: Binds a varchar value to the prepared statement at the specified index.
828: */
829: DUCKDB_API duckdb_state duckdb_bind_varchar_length(duckdb_prepared_statement prepared_statement, idx_t param_idx,
830:                                                    const char *val, idx_t length);
831: 
832: /*!
833: Binds a blob value to the prepared statement at the specified index.
834: */
835: DUCKDB_API duckdb_state duckdb_bind_blob(duckdb_prepared_statement prepared_statement, idx_t param_idx,
836:                                          const void *data, idx_t length);
837: 
838: /*!
839: Binds a NULL value to the prepared statement at the specified index.
840: */
841: DUCKDB_API duckdb_state duckdb_bind_null(duckdb_prepared_statement prepared_statement, idx_t param_idx);
842: 
843: /*!
844: Executes the prepared statement with the given bound parameters, and returns a materialized query result.
845: 
846: This method can be called multiple times for each prepared statement, and the parameters can be modified
847: between calls to this function.
848: 
849: * prepared_statement: The prepared statement to execute.
850: * out_result: The query result.
851: * returns: `DuckDBSuccess` on success or `DuckDBError` on failure.
852: */
853: DUCKDB_API duckdb_state duckdb_execute_prepared(duckdb_prepared_statement prepared_statement,
854:                                                 duckdb_result *out_result);
855: 
856: /*!
857: Executes the prepared statement with the given bound parameters, and returns an arrow query result.
858: 
859: * prepared_statement: The prepared statement to execute.
860: * out_result: The query result.
861: * returns: `DuckDBSuccess` on success or `DuckDBError` on failure.
862: */
863: DUCKDB_API duckdb_state duckdb_execute_prepared_arrow(duckdb_prepared_statement prepared_statement,
864:                                                       duckdb_arrow *out_result);
865: 
866: //===--------------------------------------------------------------------===//
867: // Appender
868: //===--------------------------------------------------------------------===//
869: 
870: // Appenders are the most efficient way of loading data into DuckDB from within the C interface, and are recommended for
871: // fast data loading. The appender is much faster than using prepared statements or individual `INSERT INTO` statements.
872: 
873: // Appends are made in row-wise format. For every column, a `duckdb_append_[type]` call should be made, after which
874: // the row should be finished by calling `duckdb_appender_end_row`. After all rows have been appended,
875: // `duckdb_appender_destroy` should be used to finalize the appender and clean up the resulting memory.
876: 
877: // Note that `duckdb_appender_destroy` should always be called on the resulting appender, even if the function returns
878: // `DuckDBError`.
879: 
880: /*!
881: Creates an appender object.
882: 
883: * connection: The connection context to create the appender in.
884: * schema: The schema of the table to append to, or `nullptr` for the default schema.
885: * table: The table name to append to.
886: * out_appender: The resulting appender object.
887: * returns: `DuckDBSuccess` on success or `DuckDBError` on failure.
888: */
889: DUCKDB_API duckdb_state duckdb_appender_create(duckdb_connection connection, const char *schema, const char *table,
890:                                                duckdb_appender *out_appender);
891: 
892: /*!
893: Returns the error message associated with the given appender.
894: If the appender has no error message, this returns `nullptr` instead.
895: 
896: The error message should not be freed. It will be de-allocated when `duckdb_appender_destroy` is called.
897: 
898: * appender: The appender to get the error from.
899: * returns: The error message, or `nullptr` if there is none.
900: */
901: DUCKDB_API const char *duckdb_appender_error(duckdb_appender appender);
902: 
903: /*!
904: Flush the appender to the table, forcing the cache of the appender to be cleared and the data to be appended to the
905: base table.
906: 
907: This should generally not be used unless you know what you are doing. Instead, call `duckdb_appender_destroy` when you
908: are done with the appender.
909: 
910: * appender: The appender to flush.
911: * returns: `DuckDBSuccess` on success or `DuckDBError` on failure.
912: */
913: DUCKDB_API duckdb_state duckdb_appender_flush(duckdb_appender appender);
914: 
915: /*!
916: Close the appender, flushing all intermediate state in the appender to the table and closing it for further appends.
917: 
918: This is generally not necessary. Call `duckdb_appender_destroy` instead.
919: 
920: * appender: The appender to flush and close.
921: * returns: `DuckDBSuccess` on success or `DuckDBError` on failure.
922: */
923: DUCKDB_API duckdb_state duckdb_appender_close(duckdb_appender appender);
924: 
925: /*!
926: Close the appender and destroy it. Flushing all intermediate state in the appender to the table, and de-allocating
927: all memory associated with the appender.
928: 
929: * appender: The appender to flush, close and destroy.
930: * returns: `DuckDBSuccess` on success or `DuckDBError` on failure.
931: */
932: DUCKDB_API duckdb_state duckdb_appender_destroy(duckdb_appender *appender);
933: 
934: /*!
935: A nop function, provided for backwards compatibility reasons. Does nothing. Only `duckdb_appender_end_row` is required.
936: */
937: DUCKDB_API duckdb_state duckdb_appender_begin_row(duckdb_appender appender);
938: 
939: /*!
940: Finish the current row of appends. After end_row is called, the next row can be appended.
941: 
942: * appender: The appender.
943: * returns: `DuckDBSuccess` on success or `DuckDBError` on failure.
944: */
945: DUCKDB_API duckdb_state duckdb_appender_end_row(duckdb_appender appender);
946: 
947: /*!
948: Append a bool value to the appender.
949: */
950: DUCKDB_API duckdb_state duckdb_append_bool(duckdb_appender appender, bool value);
951: 
952: /*!
953: Append an int8_t value to the appender.
954: */
955: DUCKDB_API duckdb_state duckdb_append_int8(duckdb_appender appender, int8_t value);
956: /*!
957: Append an int16_t value to the appender.
958: */
959: DUCKDB_API duckdb_state duckdb_append_int16(duckdb_appender appender, int16_t value);
960: /*!
961: Append an int32_t value to the appender.
962: */
963: DUCKDB_API duckdb_state duckdb_append_int32(duckdb_appender appender, int32_t value);
964: /*!
965: Append an int64_t value to the appender.
966: */
967: DUCKDB_API duckdb_state duckdb_append_int64(duckdb_appender appender, int64_t value);
968: /*!
969: Append a duckdb_hugeint value to the appender.
970: */
971: DUCKDB_API duckdb_state duckdb_append_hugeint(duckdb_appender appender, duckdb_hugeint value);
972: 
973: /*!
974: Append a uint8_t value to the appender.
975: */
976: DUCKDB_API duckdb_state duckdb_append_uint8(duckdb_appender appender, uint8_t value);
977: /*!
978: Append a uint16_t value to the appender.
979: */
980: DUCKDB_API duckdb_state duckdb_append_uint16(duckdb_appender appender, uint16_t value);
981: /*!
982: Append a uint32_t value to the appender.
983: */
984: DUCKDB_API duckdb_state duckdb_append_uint32(duckdb_appender appender, uint32_t value);
985: /*!
986: Append a uint64_t value to the appender.
987: */
988: DUCKDB_API duckdb_state duckdb_append_uint64(duckdb_appender appender, uint64_t value);
989: 
990: /*!
991: Append a float value to the appender.
992: */
993: DUCKDB_API duckdb_state duckdb_append_float(duckdb_appender appender, float value);
994: /*!
995: Append a double value to the appender.
996: */
997: DUCKDB_API duckdb_state duckdb_append_double(duckdb_appender appender, double value);
998: 
999: /*!
1000: Append a duckdb_date value to the appender.
1001: */
1002: DUCKDB_API duckdb_state duckdb_append_date(duckdb_appender appender, duckdb_date value);
1003: /*!
1004: Append a duckdb_time value to the appender.
1005: */
1006: DUCKDB_API duckdb_state duckdb_append_time(duckdb_appender appender, duckdb_time value);
1007: /*!
1008: Append a duckdb_timestamp value to the appender.
1009: */
1010: DUCKDB_API duckdb_state duckdb_append_timestamp(duckdb_appender appender, duckdb_timestamp value);
1011: /*!
1012: Append a duckdb_interval value to the appender.
1013: */
1014: DUCKDB_API duckdb_state duckdb_append_interval(duckdb_appender appender, duckdb_interval value);
1015: 
1016: /*!
1017: Append a varchar value to the appender.
1018: */
1019: DUCKDB_API duckdb_state duckdb_append_varchar(duckdb_appender appender, const char *val);
1020: /*!
1021: Append a varchar value to the appender.
1022: */
1023: DUCKDB_API duckdb_state duckdb_append_varchar_length(duckdb_appender appender, const char *val, idx_t length);
1024: /*!
1025: Append a blob value to the appender.
1026: */
1027: DUCKDB_API duckdb_state duckdb_append_blob(duckdb_appender appender, const void *data, idx_t length);
1028: /*!
1029: Append a NULL value to the appender (of any type).
1030: */
1031: DUCKDB_API duckdb_state duckdb_append_null(duckdb_appender appender);
1032: 
1033: //===--------------------------------------------------------------------===//
1034: // Arrow Interface
1035: //===--------------------------------------------------------------------===//
1036: /*!
1037: Executes a SQL query within a connection and stores the full (materialized) result in an arrow structure.
1038: If the query fails to execute, DuckDBError is returned and the error message can be retrieved by calling
1039: `duckdb_query_arrow_error`.
1040: 
1041: Note that after running `duckdb_query_arrow`, `duckdb_destroy_arrow` must be called on the result object even if the
1042: query fails, otherwise the error stored within the result will not be freed correctly.
1043: 
1044: * connection: The connection to perform the query in.
1045: * query: The SQL query to run.
1046: * out_result: The query result.
1047: * returns: `DuckDBSuccess` on success or `DuckDBError` on failure.
1048: */
1049: DUCKDB_API duckdb_state duckdb_query_arrow(duckdb_connection connection, const char *query, duckdb_arrow *out_result);
1050: 
1051: /*!
1052: Fetch the internal arrow schema from the arrow result.
1053: 
1054: * result: The result to fetch the schema from.
1055: * out_schema: The output schema.
1056: * returns: `DuckDBSuccess` on success or `DuckDBError` on failure.
1057: */
1058: DUCKDB_API duckdb_state duckdb_query_arrow_schema(duckdb_arrow result, duckdb_arrow_schema *out_schema);
1059: 
1060: /*!
1061: Fetch an internal arrow array from the arrow result.
1062: 
1063: This function can be called multiple time to get next chunks, which will free the previous out_array.
1064: So consume the out_array before calling this function again.
1065: 
1066: * result: The result to fetch the array from.
1067: * out_array: The output array.
1068: * returns: `DuckDBSuccess` on success or `DuckDBError` on failure.
1069: */
1070: DUCKDB_API duckdb_state duckdb_query_arrow_array(duckdb_arrow result, duckdb_arrow_array *out_array);
1071: 
1072: /*!
1073: Returns the number of columns present in a the arrow result object.
1074: 
1075: * result: The result object.
1076: * returns: The number of columns present in the result object.
1077: */
1078: DUCKDB_API idx_t duckdb_arrow_column_count(duckdb_arrow result);
1079: 
1080: /*!
1081: Returns the number of rows present in a the arrow result object.
1082: 
1083: * result: The result object.
1084: * returns: The number of rows present in the result object.
1085: */
1086: DUCKDB_API idx_t duckdb_arrow_row_count(duckdb_arrow result);
1087: 
1088: /*!
1089: Returns the number of rows changed by the query stored in the arrow result. This is relevant only for
1090: INSERT/UPDATE/DELETE queries. For other queries the rows_changed will be 0.
1091: 
1092: * result: The result object.
1093: * returns: The number of rows changed.
1094: */
1095: DUCKDB_API idx_t duckdb_arrow_rows_changed(duckdb_arrow result);
1096: 
1097: /*!
1098: Returns the error message contained within the result. The error is only set if `duckdb_query_arrow` returns
1099: `DuckDBError`.
1100: 
1101: The error message should not be freed. It will be de-allocated when `duckdb_destroy_arrow` is called.
1102: 
1103: * result: The result object to fetch the nullmask from.
1104: * returns: The error of the result.
1105: */
1106: DUCKDB_API const char *duckdb_query_arrow_error(duckdb_arrow result);
1107: 
1108: /*!
1109: Closes the result and de-allocates all memory allocated for the arrow result.
1110: 
1111: * result: The result to destroy.
1112: */
1113: DUCKDB_API void duckdb_destroy_arrow(duckdb_arrow *result);
1114: 
1115: #ifdef __cplusplus
1116: }
1117: #endif
[end of src/include/duckdb.h]
[start of src/include/duckdb/function/replacement_scan.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/function/replacement_scan.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: 
13: namespace duckdb {
14: 
15: class TableFunctionRef;
16: 
17: typedef unique_ptr<TableFunctionRef> (*replacement_scan_t)(const string &table_name, void *data);
18: 
19: //! Replacement table scans are automatically attempted when a table name cannot be found in the schema
20: //! This allows you to do e.g. SELECT * FROM 'filename.csv', and automatically convert this into a CSV scan
21: struct ReplacementScan {
22: 	explicit ReplacementScan(replacement_scan_t function, void *data = nullptr) : function(function), data(data) {
23: 	}
24: 
25: 	replacement_scan_t function;
26: 	void *data;
27: };
28: 
29: } // namespace duckdb
[end of src/include/duckdb/function/replacement_scan.hpp]
[start of src/include/duckdb/function/table/arrow.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/function/table/arrow.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/function/table_function.hpp"
12: #include "duckdb/parallel/parallel_state.hpp"
13: #include "duckdb/common/arrow_wrapper.hpp"
14: #include "duckdb/common/atomic.hpp"
15: #include "duckdb/common/mutex.hpp"
16: #include "duckdb/common/thread.hpp"
17: #include <map>
18: #include <condition_variable>
19: 
20: namespace duckdb {
21: //===--------------------------------------------------------------------===//
22: // Arrow Variable Size Types
23: //===--------------------------------------------------------------------===//
24: enum class ArrowVariableSizeType : uint8_t { FIXED_SIZE = 0, NORMAL = 1, SUPER_SIZE = 2 };
25: 
26: //===--------------------------------------------------------------------===//
27: // Arrow Time/Date Types
28: //===--------------------------------------------------------------------===//
29: enum class ArrowDateTimeType : uint8_t {
30: 	MILLISECONDS = 0,
31: 	MICROSECONDS = 1,
32: 	NANOSECONDS = 2,
33: 	SECONDS = 3,
34: 	DAYS = 4,
35: 	MONTHS = 5
36: };
37: struct ArrowConvertData {
38: 	ArrowConvertData(LogicalType type) : dictionary_type(type) {};
39: 	ArrowConvertData() {};
40: 	//! Hold type of dictionary
41: 	LogicalType dictionary_type;
42: 	//! If its a variable size type (e.g., strings, blobs, lists) holds which type it is
43: 	vector<std::pair<ArrowVariableSizeType, idx_t>> variable_sz_type;
44: 	//! If this is a date/time holds its precision
45: 	vector<ArrowDateTimeType> date_time_precision;
46: };
47: 
48: struct ArrowScanFunctionData : public TableFunctionData {
49: #ifndef DUCKDB_NO_THREADS
50: 
51: 	ArrowScanFunctionData(idx_t rows_per_thread_p,
52: 	                      unique_ptr<ArrowArrayStreamWrapper> (*scanner_producer_p)(
53: 	                          uintptr_t stream_factory_ptr,
54: 	                          std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns,
55: 	                          TableFilterCollection *filters),
56: 	                      uintptr_t stream_factory_ptr_p, std::thread::id thread_id_p)
57: 	    : lines_read(0), rows_per_thread(rows_per_thread_p), stream_factory_ptr(stream_factory_ptr_p),
58: 	      scanner_producer(scanner_producer_p), number_of_rows(0), thread_id(thread_id_p) {
59: 	}
60: #endif
61: 
62: 	ArrowScanFunctionData(idx_t rows_per_thread_p,
63: 	                      unique_ptr<ArrowArrayStreamWrapper> (*scanner_producer_p)(
64: 	                          uintptr_t stream_factory_ptr,
65: 	                          std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns,
66: 	                          TableFilterCollection *filters),
67: 	                      uintptr_t stream_factory_ptr_p)
68: 	    : lines_read(0), rows_per_thread(rows_per_thread_p), stream_factory_ptr(stream_factory_ptr_p),
69: 	      scanner_producer(scanner_producer_p), number_of_rows(0) {
70: 	}
71: 	//! This holds the original list type (col_idx, [ArrowListType,size])
72: 	std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> arrow_convert_data;
73: 	std::atomic<idx_t> lines_read;
74: 	ArrowSchemaWrapper schema_root;
75: 	idx_t rows_per_thread;
76: 	//! Pointer to the scanner factory
77: 	uintptr_t stream_factory_ptr;
78: 	//! Pointer to the scanner factory produce
79: 	unique_ptr<ArrowArrayStreamWrapper> (*scanner_producer)(
80: 	    uintptr_t stream_factory_ptr,
81: 	    std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns,
82: 	    TableFilterCollection *filters);
83: 	//! Number of rows (Used in cardinality and progress bar)
84: 	int64_t number_of_rows;
85: #ifndef DUCKDB_NO_THREADS
86: 	// Thread that made first call in the binder
87: 	std::thread::id thread_id;
88: #endif
89: };
90: 
91: struct ArrowScanState : public FunctionOperatorData {
92: 	explicit ArrowScanState(unique_ptr<ArrowArrayWrapper> current_chunk) : chunk(move(current_chunk)) {
93: 	}
94: 	unique_ptr<ArrowArrayStreamWrapper> stream;
95: 	shared_ptr<ArrowArrayWrapper> chunk;
96: 	idx_t chunk_offset = 0;
97: 	vector<column_t> column_ids;
98: 	//! Store child vectors for Arrow Dictionary Vectors (col-idx,vector)
99: 	unordered_map<idx_t, unique_ptr<Vector>> arrow_dictionary_vectors;
100: 	TableFilterCollection *filters = nullptr;
101: };
102: 
103: struct ParallelArrowScanState : public ParallelState {
104: 	ParallelArrowScanState() {
105: 	}
106: 	unique_ptr<ArrowArrayStreamWrapper> stream;
107: 	std::mutex main_mutex;
108: 	bool ready = false;
109: };
110: 
111: struct ArrowTableFunction {
112: public:
113: 	static void RegisterFunction(BuiltinFunctions &set);
114: 
115: private:
116: 	//! Binds an arrow table
117: 	static unique_ptr<FunctionData> ArrowScanBind(ClientContext &context, vector<Value> &inputs,
118: 	                                              named_parameter_map_t &named_parameters,
119: 	                                              vector<LogicalType> &input_table_types,
120: 	                                              vector<string> &input_table_names, vector<LogicalType> &return_types,
121: 	                                              vector<string> &names);
122: 	//! Actual conversion from Arrow to DuckDB
123: 	static void ArrowToDuckDB(ArrowScanState &scan_state,
124: 	                          std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,
125: 	                          DataChunk &output, idx_t start);
126: 
127: 	//! -----Single Thread Functions:-----
128: 	//! Initialize Single Thread Scan
129: 	static unique_ptr<FunctionOperatorData> ArrowScanInit(ClientContext &context, const FunctionData *bind_data,
130: 	                                                      const vector<column_t> &column_ids,
131: 	                                                      TableFilterCollection *filters);
132: 
133: 	//! Scan Function for Single Thread Execution
134: 	static void ArrowScanFunction(ClientContext &context, const FunctionData *bind_data,
135: 	                              FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output);
136: 
137: 	//! -----Multi Thread Functions:-----
138: 	//! Initialize Parallel State
139: 	static unique_ptr<ParallelState> ArrowScanInitParallelState(ClientContext &context, const FunctionData *bind_data_p,
140: 	                                                            const vector<column_t> &column_ids,
141: 	                                                            TableFilterCollection *filters);
142: 	//! Initialize Parallel Scans
143: 	static unique_ptr<FunctionOperatorData> ArrowScanParallelInit(ClientContext &context,
144: 	                                                              const FunctionData *bind_data_p, ParallelState *state,
145: 	                                                              const vector<column_t> &column_ids,
146: 	                                                              TableFilterCollection *filters);
147: 	//! Defines Maximum Number of Threads
148: 	static idx_t ArrowScanMaxThreads(ClientContext &context, const FunctionData *bind_data_p);
149: 	//! Scan Function for Parallel Execution
150: 	static void ArrowScanFunctionParallel(ClientContext &context, const FunctionData *bind_data,
151: 	                                      FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output,
152: 	                                      ParallelState *parallel_state_p);
153: 	//! Get next chunk for the running thread
154: 	static bool ArrowScanParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,
155: 	                                       FunctionOperatorData *operator_state, ParallelState *parallel_state_p);
156: 
157: 	//! -----Utility Functions:-----
158: 	//! Gets Arrow Table's Cardinality
159: 	static unique_ptr<NodeStatistics> ArrowScanCardinality(ClientContext &context, const FunctionData *bind_data);
160: 	//! Gets the progress on the table scan, used for Progress Bars
161: 	static double ArrowProgress(ClientContext &context, const FunctionData *bind_data_p);
162: };
163: 
164: } // namespace duckdb
[end of src/include/duckdb/function/table/arrow.hpp]
[start of src/include/duckdb/function/table_function.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/function/table_function.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: #include "duckdb/function/function.hpp"
11: #include "duckdb/storage/statistics/node_statistics.hpp"
12: 
13: #include <functional>
14: 
15: namespace duckdb {
16: class BaseStatistics;
17: class LogicalGet;
18: struct ParallelState;
19: class TableFilterSet;
20: 
21: struct FunctionOperatorData {
22: 	DUCKDB_API virtual ~FunctionOperatorData();
23: };
24: 
25: struct TableFilterCollection {
26: 	DUCKDB_API explicit TableFilterCollection(TableFilterSet *table_filters);
27: 
28: 	TableFilterSet *table_filters;
29: };
30: 
31: typedef unique_ptr<FunctionData> (*table_function_bind_t)(ClientContext &context, vector<Value> &inputs,
32:                                                           named_parameter_map_t &named_parameters,
33:                                                           vector<LogicalType> &input_table_types,
34:                                                           vector<string> &input_table_names,
35:                                                           vector<LogicalType> &return_types, vector<string> &names);
36: typedef unique_ptr<FunctionOperatorData> (*table_function_init_t)(ClientContext &context, const FunctionData *bind_data,
37:                                                                   const vector<column_t> &column_ids,
38:                                                                   TableFilterCollection *filters);
39: typedef unique_ptr<BaseStatistics> (*table_statistics_t)(ClientContext &context, const FunctionData *bind_data,
40:                                                          column_t column_index);
41: typedef void (*table_function_t)(ClientContext &context, const FunctionData *bind_data,
42:                                  FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output);
43: 
44: typedef void (*table_function_parallel_t)(ClientContext &context, const FunctionData *bind_data,
45:                                           FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output,
46:                                           ParallelState *parallel_state);
47: 
48: typedef void (*table_function_cleanup_t)(ClientContext &context, const FunctionData *bind_data,
49:                                          FunctionOperatorData *operator_state);
50: typedef idx_t (*table_function_max_threads_t)(ClientContext &context, const FunctionData *bind_data);
51: typedef unique_ptr<ParallelState> (*table_function_init_parallel_state_t)(ClientContext &context,
52:                                                                           const FunctionData *bind_data,
53:                                                                           const vector<column_t> &column_ids,
54:                                                                           TableFilterCollection *filters);
55: typedef unique_ptr<FunctionOperatorData> (*table_function_init_parallel_t)(ClientContext &context,
56:                                                                            const FunctionData *bind_data,
57:                                                                            ParallelState *state,
58:                                                                            const vector<column_t> &column_ids,
59:                                                                            TableFilterCollection *filters);
60: typedef bool (*table_function_parallel_state_next_t)(ClientContext &context, const FunctionData *bind_data,
61:                                                      FunctionOperatorData *state, ParallelState *parallel_state);
62: typedef double (*table_function_progress_t)(ClientContext &context, const FunctionData *bind_data);
63: typedef void (*table_function_dependency_t)(unordered_set<CatalogEntry *> &dependencies, const FunctionData *bind_data);
64: typedef unique_ptr<NodeStatistics> (*table_function_cardinality_t)(ClientContext &context,
65:                                                                    const FunctionData *bind_data);
66: typedef void (*table_function_pushdown_complex_filter_t)(ClientContext &context, LogicalGet &get,
67:                                                          FunctionData *bind_data,
68:                                                          vector<unique_ptr<Expression>> &filters);
69: typedef string (*table_function_to_string_t)(const FunctionData *bind_data);
70: 
71: class TableFunction : public SimpleNamedParameterFunction {
72: public:
73: 	DUCKDB_API
74: 	TableFunction(string name, vector<LogicalType> arguments, table_function_t function,
75: 	              table_function_bind_t bind = nullptr, table_function_init_t init = nullptr,
76: 	              table_statistics_t statistics = nullptr, table_function_cleanup_t cleanup = nullptr,
77: 	              table_function_dependency_t dependency = nullptr, table_function_cardinality_t cardinality = nullptr,
78: 	              table_function_pushdown_complex_filter_t pushdown_complex_filter = nullptr,
79: 	              table_function_to_string_t to_string = nullptr, table_function_max_threads_t max_threads = nullptr,
80: 	              table_function_init_parallel_state_t init_parallel_state = nullptr,
81: 	              table_function_parallel_t parallel_function = nullptr,
82: 	              table_function_init_parallel_t parallel_init = nullptr,
83: 	              table_function_parallel_state_next_t parallel_state_next = nullptr, bool projection_pushdown = false,
84: 	              bool filter_pushdown = false, table_function_progress_t query_progress = nullptr);
85: 	DUCKDB_API
86: 	TableFunction(const vector<LogicalType> &arguments, table_function_t function, table_function_bind_t bind = nullptr,
87: 	              table_function_init_t init = nullptr, table_statistics_t statistics = nullptr,
88: 	              table_function_cleanup_t cleanup = nullptr, table_function_dependency_t dependency = nullptr,
89: 	              table_function_cardinality_t cardinality = nullptr,
90: 	              table_function_pushdown_complex_filter_t pushdown_complex_filter = nullptr,
91: 	              table_function_to_string_t to_string = nullptr, table_function_max_threads_t max_threads = nullptr,
92: 	              table_function_init_parallel_state_t init_parallel_state = nullptr,
93: 	              table_function_parallel_t parallel_function = nullptr,
94: 	              table_function_init_parallel_t parallel_init = nullptr,
95: 	              table_function_parallel_state_next_t parallel_state_next = nullptr, bool projection_pushdown = false,
96: 	              bool filter_pushdown = false, table_function_progress_t query_progress = nullptr);
97: 	DUCKDB_API TableFunction();
98: 
99: 	//! Bind function
100: 	//! This function is used for determining the return type of a table producing function and returning bind data
101: 	//! The returned FunctionData object should be constant and should not be changed during execution.
102: 	table_function_bind_t bind;
103: 	//! (Optional) init function
104: 	//! Initialize the operator state of the function. The operator state is used to keep track of the progress in the
105: 	//! table function.
106: 	table_function_init_t init;
107: 	//! The main function
108: 	table_function_t function;
109: 	//! (Optional) statistics function
110: 	//! Returns the statistics of a specified column
111: 	table_statistics_t statistics;
112: 	//! (Optional) cleanup function
113: 	//! The final cleanup function, called after all data is exhausted from the main function
114: 	table_function_cleanup_t cleanup;
115: 	//! (Optional) dependency function
116: 	//! Sets up which catalog entries this table function depend on
117: 	table_function_dependency_t dependency;
118: 	//! (Optional) cardinality function
119: 	//! Returns the expected cardinality of this scan
120: 	table_function_cardinality_t cardinality;
121: 	//! (Optional) pushdown a set of arbitrary filter expressions, rather than only simple comparisons with a constant
122: 	//! Any functions remaining in the expression list will be pushed as a regular filter after the scan
123: 	table_function_pushdown_complex_filter_t pushdown_complex_filter;
124: 	//! (Optional) function for rendering the operator to a string in profiling output
125: 	table_function_to_string_t to_string;
126: 	//! (Optional) function that returns the maximum amount of threads that can work on this task
127: 	table_function_max_threads_t max_threads;
128: 	//! (Optional) initialize the parallel scan state, called once in total.
129: 	table_function_init_parallel_state_t init_parallel_state;
130: 	//! (Optional) Parallel version of the main function
131: 	table_function_parallel_t parallel_function;
132: 	//! (Optional) initialize the parallel scan given the parallel state. Called once per task. Return nullptr if there
133: 	//! is nothing left to scan.
134: 	table_function_init_parallel_t parallel_init;
135: 	//! (Optional) return the next chunk to process in the parallel scan, or return nullptr if there is none
136: 	table_function_parallel_state_next_t parallel_state_next;
137: 	//! (Optional) return how much of the table we have scanned up to this point (% of the data)
138: 	table_function_progress_t table_scan_progress;
139: 	//! Whether or not the table function supports projection pushdown. If not supported a projection will be added
140: 	//! that filters out unused columns.
141: 	bool projection_pushdown;
142: 	//! Whether or not the table function supports filter pushdown. If not supported a filter will be added
143: 	//! that applies the table filter directly.
144: 	bool filter_pushdown;
145: };
146: 
147: } // namespace duckdb
[end of src/include/duckdb/function/table_function.hpp]
[start of src/include/duckdb/main/appender.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/main/appender.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/data_chunk.hpp"
12: #include "duckdb/common/winapi.hpp"
13: #include "duckdb/main/table_description.hpp"
14: #include "duckdb/common/types/chunk_collection.hpp"
15: 
16: namespace duckdb {
17: 
18: class ClientContext;
19: class DuckDB;
20: class TableCatalogEntry;
21: class Connection;
22: 
23: //! The Appender class can be used to append elements to a table.
24: class BaseAppender {
25: protected:
26: 	//! The amount of chunks that will be gathered in the chunk collection before flushing
27: 	static constexpr const idx_t FLUSH_COUNT = 100;
28: 
29: 	//! The append types
30: 	vector<LogicalType> types;
31: 	//! The buffered data for the append
32: 	ChunkCollection collection;
33: 	//! Internal chunk used for appends
34: 	unique_ptr<DataChunk> chunk;
35: 	//! The current column to append to
36: 	idx_t column = 0;
37: 
38: public:
39: 	DUCKDB_API BaseAppender();
40: 	DUCKDB_API BaseAppender(vector<LogicalType> types);
41: 	DUCKDB_API virtual ~BaseAppender();
42: 
43: 	//! Begins a new row append, after calling this the other AppendX() functions
44: 	//! should be called the correct amount of times. After that,
45: 	//! EndRow() should be called.
46: 	DUCKDB_API void BeginRow();
47: 	//! Finishes appending the current row.
48: 	DUCKDB_API void EndRow();
49: 
50: 	// Append functions
51: 	template <class T>
52: 	void Append(T value) {
53: 		throw Exception("Undefined type for Appender::Append!");
54: 	}
55: 
56: 	DUCKDB_API void Append(const char *value, uint32_t length);
57: 
58: 	// prepared statements
59: 	template <typename... Args>
60: 	void AppendRow(Args... args) {
61: 		BeginRow();
62: 		AppendRowRecursive(args...);
63: 	}
64: 
65: 	//! Commit the changes made by the appender.
66: 	DUCKDB_API void Flush();
67: 	//! Flush the changes made by the appender and close it. The appender cannot be used after this point
68: 	DUCKDB_API void Close();
69: 
70: 	DUCKDB_API vector<LogicalType> &GetTypes() {
71: 		return types;
72: 	}
73: 	DUCKDB_API idx_t CurrentColumn() {
74: 		return column;
75: 	}
76: 
77: protected:
78: 	void Destructor();
79: 	virtual void FlushInternal(ChunkCollection &collection) = 0;
80: 	void InitializeChunk();
81: 	void FlushChunk();
82: 
83: 	template <class T>
84: 	void AppendValueInternal(T value);
85: 	template <class SRC, class DST>
86: 	void AppendValueInternal(Vector &vector, SRC input);
87: 
88: 	void AppendRowRecursive() {
89: 		EndRow();
90: 	}
91: 
92: 	template <typename T, typename... Args>
93: 	void AppendRowRecursive(T value, Args... args) {
94: 		Append<T>(value);
95: 		AppendRowRecursive(args...);
96: 	}
97: 
98: 	void AppendValue(const Value &value);
99: };
100: 
101: class Appender : public BaseAppender {
102: 	//! A reference to a database connection that created this appender
103: 	shared_ptr<ClientContext> context;
104: 	//! The table description (including column names)
105: 	unique_ptr<TableDescription> description;
106: 
107: public:
108: 	DUCKDB_API Appender(Connection &con, const string &schema_name, const string &table_name);
109: 	DUCKDB_API Appender(Connection &con, const string &table_name);
110: 	DUCKDB_API ~Appender() override;
111: 
112: protected:
113: 	void FlushInternal(ChunkCollection &collection) override;
114: };
115: 
116: class InternalAppender : public BaseAppender {
117: 	//! The client context
118: 	ClientContext &context;
119: 	//! The internal table entry to append to
120: 	TableCatalogEntry &table;
121: 
122: public:
123: 	DUCKDB_API InternalAppender(ClientContext &context, TableCatalogEntry &table);
124: 	DUCKDB_API ~InternalAppender() override;
125: 
126: protected:
127: 	void FlushInternal(ChunkCollection &collection) override;
128: };
129: 
130: template <>
131: DUCKDB_API void BaseAppender::Append(bool value);
132: template <>
133: DUCKDB_API void BaseAppender::Append(int8_t value);
134: template <>
135: DUCKDB_API void BaseAppender::Append(int16_t value);
136: template <>
137: DUCKDB_API void BaseAppender::Append(int32_t value);
138: template <>
139: DUCKDB_API void BaseAppender::Append(int64_t value);
140: template <>
141: DUCKDB_API void BaseAppender::Append(hugeint_t value);
142: template <>
143: DUCKDB_API void BaseAppender::Append(uint8_t value);
144: template <>
145: DUCKDB_API void BaseAppender::Append(uint16_t value);
146: template <>
147: DUCKDB_API void BaseAppender::Append(uint32_t value);
148: template <>
149: DUCKDB_API void BaseAppender::Append(uint64_t value);
150: template <>
151: DUCKDB_API void BaseAppender::Append(float value);
152: template <>
153: DUCKDB_API void BaseAppender::Append(double value);
154: template <>
155: DUCKDB_API void BaseAppender::Append(date_t value);
156: template <>
157: DUCKDB_API void BaseAppender::Append(dtime_t value);
158: template <>
159: DUCKDB_API void BaseAppender::Append(timestamp_t value);
160: template <>
161: DUCKDB_API void BaseAppender::Append(interval_t value);
162: template <>
163: DUCKDB_API void BaseAppender::Append(const char *value);
164: template <>
165: DUCKDB_API void BaseAppender::Append(string_t value);
166: template <>
167: DUCKDB_API void BaseAppender::Append(Value value);
168: template <>
169: DUCKDB_API void BaseAppender::Append(std::nullptr_t value);
170: 
171: } // namespace duckdb
[end of src/include/duckdb/main/appender.hpp]
[start of src/include/duckdb/main/capi_internal.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/main/capi_internal.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb.h"
12: #include "duckdb.hpp"
13: #include "duckdb/common/types.hpp"
14: #include "duckdb/common/types/data_chunk.hpp"
15: #include "duckdb/main/appender.hpp"
16: #include <cstring>
17: #include <cassert>
18: 
19: #ifdef _WIN32
20: #ifndef strdup
21: #define strdup _strdup
22: #endif
23: #endif
24: 
25: namespace duckdb {
26: 
27: struct DatabaseData {
28: 	unique_ptr<DuckDB> database;
29: };
30: 
31: struct PreparedStatementWrapper {
32: 	unique_ptr<PreparedStatement> statement;
33: 	vector<Value> values;
34: };
35: 
36: struct ArrowResultWrapper {
37: 	unique_ptr<MaterializedQueryResult> result;
38: 	unique_ptr<DataChunk> current_chunk;
39: };
40: 
41: struct AppenderWrapper {
42: 	unique_ptr<Appender> appender;
43: 	string error;
44: };
45: 
46: duckdb_type ConvertCPPTypeToC(const LogicalType &type);
47: idx_t GetCTypeSize(duckdb_type type);
48: duckdb_state duckdb_translate_result(MaterializedQueryResult *result, duckdb_result *out);
49: 
50: struct DuckDBColumnData {
51: 	LogicalType type;
52: };
53: 
54: } // namespace duckdb
[end of src/include/duckdb/main/capi_internal.hpp]
[start of src/main/appender.cpp]
1: #include "duckdb/main/appender.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/exception.hpp"
5: #include "duckdb/main/connection.hpp"
6: #include "duckdb/main/client_context.hpp"
7: #include "duckdb/main/database.hpp"
8: #include "duckdb/storage/data_table.hpp"
9: #include "duckdb/common/string_util.hpp"
10: #include "duckdb/common/operator/cast_operators.hpp"
11: #include "duckdb/common/operator/string_cast.hpp"
12: #include "duckdb/storage/data_table.hpp"
13: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
14: 
15: namespace duckdb {
16: 
17: BaseAppender::BaseAppender() : column(0) {
18: }
19: 
20: BaseAppender::BaseAppender(vector<LogicalType> types_p) : types(move(types_p)), column(0) {
21: 	InitializeChunk();
22: }
23: 
24: BaseAppender::~BaseAppender() {
25: }
26: 
27: void BaseAppender::Destructor() {
28: 	if (Exception::UncaughtException()) {
29: 		return;
30: 	}
31: 	// flush any remaining chunks, but only if we are not cleaning up the appender as part of an exception stack unwind
32: 	// wrapped in a try/catch because Close() can throw if the table was dropped in the meantime
33: 	try {
34: 		Close();
35: 	} catch (...) {
36: 	}
37: }
38: 
39: InternalAppender::InternalAppender(ClientContext &context_p, TableCatalogEntry &table_p)
40:     : BaseAppender(table_p.GetTypes()), context(context_p), table(table_p) {
41: }
42: 
43: InternalAppender::~InternalAppender() {
44: 	Destructor();
45: }
46: 
47: Appender::Appender(Connection &con, const string &schema_name, const string &table_name)
48:     : BaseAppender(), context(con.context) {
49: 	description = con.TableInfo(schema_name, table_name);
50: 	if (!description) {
51: 		// table could not be found
52: 		throw CatalogException(StringUtil::Format("Table \"%s.%s\" could not be found", schema_name, table_name));
53: 	}
54: 	for (auto &column : description->columns) {
55: 		types.push_back(column.type);
56: 	}
57: 	InitializeChunk();
58: }
59: 
60: Appender::Appender(Connection &con, const string &table_name) : Appender(con, DEFAULT_SCHEMA, table_name) {
61: }
62: 
63: Appender::~Appender() {
64: 	Destructor();
65: }
66: 
67: void BaseAppender::InitializeChunk() {
68: 	chunk = make_unique<DataChunk>();
69: 	chunk->Initialize(types);
70: }
71: 
72: void BaseAppender::BeginRow() {
73: }
74: 
75: void BaseAppender::EndRow() {
76: 	// check that all rows have been appended to
77: 	if (column != chunk->ColumnCount()) {
78: 		throw InvalidInputException("Call to EndRow before all rows have been appended to!");
79: 	}
80: 	column = 0;
81: 	chunk->SetCardinality(chunk->size() + 1);
82: 	if (chunk->size() >= STANDARD_VECTOR_SIZE) {
83: 		FlushChunk();
84: 	}
85: }
86: 
87: template <class SRC, class DST>
88: void BaseAppender::AppendValueInternal(Vector &col, SRC input) {
89: 	FlatVector::GetData<DST>(col)[chunk->size()] = Cast::Operation<SRC, DST>(input);
90: }
91: 
92: template <class T>
93: void BaseAppender::AppendValueInternal(T input) {
94: 	if (column >= types.size()) {
95: 		throw InvalidInputException("Too many appends for chunk!");
96: 	}
97: 	auto &col = chunk->data[column];
98: 	switch (col.GetType().InternalType()) {
99: 	case PhysicalType::BOOL:
100: 		AppendValueInternal<T, bool>(col, input);
101: 		break;
102: 	case PhysicalType::UINT8:
103: 		AppendValueInternal<T, uint8_t>(col, input);
104: 		break;
105: 	case PhysicalType::INT8:
106: 		AppendValueInternal<T, int8_t>(col, input);
107: 		break;
108: 	case PhysicalType::UINT16:
109: 		AppendValueInternal<T, uint16_t>(col, input);
110: 		break;
111: 	case PhysicalType::INT16:
112: 		AppendValueInternal<T, int16_t>(col, input);
113: 		break;
114: 	case PhysicalType::UINT32:
115: 		AppendValueInternal<T, uint32_t>(col, input);
116: 		break;
117: 	case PhysicalType::INT32:
118: 		AppendValueInternal<T, int32_t>(col, input);
119: 		break;
120: 	case PhysicalType::UINT64:
121: 		AppendValueInternal<T, uint64_t>(col, input);
122: 		break;
123: 	case PhysicalType::INT64:
124: 		AppendValueInternal<T, int64_t>(col, input);
125: 		break;
126: 	case PhysicalType::INT128:
127: 		AppendValueInternal<T, hugeint_t>(col, input);
128: 		break;
129: 	case PhysicalType::FLOAT:
130: 		AppendValueInternal<T, float>(col, input);
131: 		break;
132: 	case PhysicalType::DOUBLE:
133: 		AppendValueInternal<T, double>(col, input);
134: 		break;
135: 	case PhysicalType::VARCHAR:
136: 		FlatVector::GetData<string_t>(col)[chunk->size()] = StringCast::Operation<T>(input, col);
137: 		break;
138: 	default:
139: 		AppendValue(Value::CreateValue<T>(input));
140: 		return;
141: 	}
142: 	column++;
143: }
144: 
145: template <>
146: void BaseAppender::Append(bool value) {
147: 	AppendValueInternal<bool>(value);
148: }
149: 
150: template <>
151: void BaseAppender::Append(int8_t value) {
152: 	AppendValueInternal<int8_t>(value);
153: }
154: 
155: template <>
156: void BaseAppender::Append(int16_t value) {
157: 	AppendValueInternal<int16_t>(value);
158: }
159: 
160: template <>
161: void BaseAppender::Append(int32_t value) {
162: 	AppendValueInternal<int32_t>(value);
163: }
164: 
165: template <>
166: void BaseAppender::Append(int64_t value) {
167: 	AppendValueInternal<int64_t>(value);
168: }
169: 
170: template <>
171: void BaseAppender::Append(hugeint_t value) {
172: 	AppendValueInternal<hugeint_t>(value);
173: }
174: 
175: template <>
176: void BaseAppender::Append(uint8_t value) {
177: 	AppendValueInternal<uint8_t>(value);
178: }
179: 
180: template <>
181: void BaseAppender::Append(uint16_t value) {
182: 	AppendValueInternal<uint16_t>(value);
183: }
184: 
185: template <>
186: void BaseAppender::Append(uint32_t value) {
187: 	AppendValueInternal<uint32_t>(value);
188: }
189: 
190: template <>
191: void BaseAppender::Append(uint64_t value) {
192: 	AppendValueInternal<uint64_t>(value);
193: }
194: 
195: template <>
196: void BaseAppender::Append(const char *value) {
197: 	AppendValueInternal<string_t>(string_t(value));
198: }
199: 
200: void BaseAppender::Append(const char *value, uint32_t length) {
201: 	AppendValueInternal<string_t>(string_t(value, length));
202: }
203: 
204: template <>
205: void BaseAppender::Append(string_t value) {
206: 	AppendValueInternal<string_t>(value);
207: }
208: 
209: template <>
210: void BaseAppender::Append(float value) {
211: 	if (!Value::FloatIsValid(value)) {
212: 		throw InvalidInputException("Float value is out of range!");
213: 	}
214: 	AppendValueInternal<float>(value);
215: }
216: 
217: template <>
218: void BaseAppender::Append(double value) {
219: 	if (!Value::DoubleIsValid(value)) {
220: 		throw InvalidInputException("Double value is out of range!");
221: 	}
222: 	AppendValueInternal<double>(value);
223: }
224: 
225: template <>
226: void BaseAppender::Append(date_t value) {
227: 	AppendValueInternal<int32_t>(value.days);
228: }
229: 
230: template <>
231: void BaseAppender::Append(dtime_t value) {
232: 	AppendValueInternal<int64_t>(value.micros);
233: }
234: 
235: template <>
236: void BaseAppender::Append(timestamp_t value) {
237: 	AppendValueInternal<int64_t>(value.value);
238: }
239: 
240: template <>
241: void BaseAppender::Append(interval_t value) {
242: 	AppendValueInternal<interval_t>(value);
243: }
244: 
245: template <>
246: void BaseAppender::Append(Value value) { // NOLINT: template shtuff
247: 	if (column >= chunk->ColumnCount()) {
248: 		throw InvalidInputException("Too many appends for chunk!");
249: 	}
250: 	AppendValue(value);
251: }
252: 
253: template <>
254: void BaseAppender::Append(std::nullptr_t value) {
255: 	if (column >= chunk->ColumnCount()) {
256: 		throw InvalidInputException("Too many appends for chunk!");
257: 	}
258: 	auto &col = chunk->data[column++];
259: 	FlatVector::SetNull(col, chunk->size(), true);
260: }
261: 
262: void BaseAppender::AppendValue(const Value &value) {
263: 	chunk->SetValue(column, chunk->size(), value);
264: 	column++;
265: }
266: 
267: void BaseAppender::FlushChunk() {
268: 	if (chunk->size() == 0) {
269: 		return;
270: 	}
271: 	collection.Append(move(chunk));
272: 	InitializeChunk();
273: 	if (collection.ChunkCount() >= FLUSH_COUNT) {
274: 		Flush();
275: 	}
276: }
277: 
278: void BaseAppender::Flush() {
279: 	// check that all vectors have the same length before appending
280: 	if (column != 0) {
281: 		throw InvalidInputException("Failed to Flush appender: incomplete append to row!");
282: 	}
283: 
284: 	FlushChunk();
285: 	if (collection.Count() == 0) {
286: 		return;
287: 	}
288: 	FlushInternal(collection);
289: 
290: 	collection.Reset();
291: 	column = 0;
292: }
293: 
294: void Appender::FlushInternal(ChunkCollection &collection) {
295: 	context->Append(*description, collection);
296: }
297: 
298: void InternalAppender::FlushInternal(ChunkCollection &collection) {
299: 	for (auto &chunk : collection.Chunks()) {
300: 		table.storage->Append(table, context, *chunk);
301: 	}
302: }
303: 
304: void BaseAppender::Close() {
305: 	if (column == 0 || column == types.size()) {
306: 		Flush();
307: 	}
308: }
309: 
310: } // namespace duckdb
[end of src/main/appender.cpp]
[start of src/main/capi/CMakeLists.txt]
1: add_library_unity(
2:   duckdb_main_capi
3:   OBJECT
4:   appender-c.cpp
5:   arrow-c.cpp
6:   config-c.cpp
7:   datetime-c.cpp
8:   duckdb-c.cpp
9:   helper-c.cpp
10:   hugeint-c.cpp
11:   prepared-c.cpp
12:   result-c.cpp
13:   value-c.cpp)
14: 
15: set(ALL_OBJECT_FILES
16:     ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:duckdb_main_capi>
17:     PARENT_SCOPE)
[end of src/main/capi/CMakeLists.txt]
[start of src/main/capi/appender-c.cpp]
1: #include "duckdb/main/capi_internal.hpp"
2: 
3: using duckdb::Appender;
4: using duckdb::AppenderWrapper;
5: using duckdb::Connection;
6: using duckdb::date_t;
7: using duckdb::dtime_t;
8: using duckdb::hugeint_t;
9: using duckdb::interval_t;
10: using duckdb::string_t;
11: using duckdb::timestamp_t;
12: 
13: duckdb_state duckdb_appender_create(duckdb_connection connection, const char *schema, const char *table,
14:                                     duckdb_appender *out_appender) {
15: 	Connection *conn = (Connection *)connection;
16: 
17: 	if (!connection || !table || !out_appender) {
18: 		return DuckDBError;
19: 	}
20: 	if (schema == nullptr) {
21: 		schema = DEFAULT_SCHEMA;
22: 	}
23: 	auto wrapper = new AppenderWrapper();
24: 	*out_appender = (duckdb_appender)wrapper;
25: 	try {
26: 		wrapper->appender = duckdb::make_unique<Appender>(*conn, schema, table);
27: 	} catch (std::exception &ex) {
28: 		wrapper->error = ex.what();
29: 		return DuckDBError;
30: 	} catch (...) { // LCOV_EXCL_START
31: 		wrapper->error = "Unknown create appender error";
32: 		return DuckDBError;
33: 	} // LCOV_EXCL_STOP
34: 	return DuckDBSuccess;
35: }
36: 
37: duckdb_state duckdb_appender_destroy(duckdb_appender *appender) {
38: 	if (!appender || !*appender) {
39: 		return DuckDBError;
40: 	}
41: 	duckdb_appender_close(*appender);
42: 	auto wrapper = (AppenderWrapper *)*appender;
43: 	if (wrapper) {
44: 		delete wrapper;
45: 	}
46: 	*appender = nullptr;
47: 	return DuckDBSuccess;
48: }
49: 
50: template <class FUN>
51: duckdb_state duckdb_appender_run_function(duckdb_appender appender, FUN &&function) {
52: 	if (!appender) {
53: 		return DuckDBError;
54: 	}
55: 	auto wrapper = (AppenderWrapper *)appender;
56: 	if (!wrapper->appender) {
57: 		return DuckDBError;
58: 	}
59: 	try {
60: 		function(*wrapper->appender);
61: 	} catch (std::exception &ex) {
62: 		wrapper->error = ex.what();
63: 		return DuckDBError;
64: 	} catch (...) { // LCOV_EXCL_START
65: 		wrapper->error = "Unknown error";
66: 		return DuckDBError;
67: 	} // LCOV_EXCL_STOP
68: 	return DuckDBSuccess;
69: }
70: 
71: const char *duckdb_appender_error(duckdb_appender appender) {
72: 	if (!appender) {
73: 		return nullptr;
74: 	}
75: 	auto wrapper = (AppenderWrapper *)appender;
76: 	if (wrapper->error.empty()) {
77: 		return nullptr;
78: 	}
79: 	return wrapper->error.c_str();
80: }
81: 
82: duckdb_state duckdb_appender_begin_row(duckdb_appender appender) {
83: 	return DuckDBSuccess;
84: }
85: 
86: duckdb_state duckdb_appender_end_row(duckdb_appender appender) {
87: 	return duckdb_appender_run_function(appender, [&](Appender &appender) { appender.EndRow(); });
88: }
89: 
90: template <class T>
91: duckdb_state duckdb_append_internal(duckdb_appender appender, T value) {
92: 	if (!appender) {
93: 		return DuckDBError;
94: 	}
95: 	auto *appender_instance = (AppenderWrapper *)appender;
96: 	try {
97: 		appender_instance->appender->Append<T>(value);
98: 	} catch (...) {
99: 		return DuckDBError;
100: 	}
101: 	return DuckDBSuccess;
102: }
103: 
104: duckdb_state duckdb_append_bool(duckdb_appender appender, bool value) {
105: 	return duckdb_append_internal<bool>(appender, value);
106: }
107: 
108: duckdb_state duckdb_append_int8(duckdb_appender appender, int8_t value) {
109: 	return duckdb_append_internal<int8_t>(appender, value);
110: }
111: 
112: duckdb_state duckdb_append_int16(duckdb_appender appender, int16_t value) {
113: 	return duckdb_append_internal<int16_t>(appender, value);
114: }
115: 
116: duckdb_state duckdb_append_int32(duckdb_appender appender, int32_t value) {
117: 	return duckdb_append_internal<int32_t>(appender, value);
118: }
119: 
120: duckdb_state duckdb_append_int64(duckdb_appender appender, int64_t value) {
121: 	return duckdb_append_internal<int64_t>(appender, value);
122: }
123: 
124: duckdb_state duckdb_append_hugeint(duckdb_appender appender, duckdb_hugeint value) {
125: 	hugeint_t internal;
126: 	internal.lower = value.lower;
127: 	internal.upper = value.upper;
128: 	return duckdb_append_internal<hugeint_t>(appender, internal);
129: }
130: 
131: duckdb_state duckdb_append_uint8(duckdb_appender appender, uint8_t value) {
132: 	return duckdb_append_internal<uint8_t>(appender, value);
133: }
134: 
135: duckdb_state duckdb_append_uint16(duckdb_appender appender, uint16_t value) {
136: 	return duckdb_append_internal<uint16_t>(appender, value);
137: }
138: 
139: duckdb_state duckdb_append_uint32(duckdb_appender appender, uint32_t value) {
140: 	return duckdb_append_internal<uint32_t>(appender, value);
141: }
142: 
143: duckdb_state duckdb_append_uint64(duckdb_appender appender, uint64_t value) {
144: 	return duckdb_append_internal<uint64_t>(appender, value);
145: }
146: 
147: duckdb_state duckdb_append_float(duckdb_appender appender, float value) {
148: 	return duckdb_append_internal<float>(appender, value);
149: }
150: 
151: duckdb_state duckdb_append_double(duckdb_appender appender, double value) {
152: 	return duckdb_append_internal<double>(appender, value);
153: }
154: 
155: duckdb_state duckdb_append_date(duckdb_appender appender, duckdb_date value) {
156: 	return duckdb_append_internal<date_t>(appender, date_t(value.days));
157: }
158: 
159: duckdb_state duckdb_append_time(duckdb_appender appender, duckdb_time value) {
160: 	return duckdb_append_internal<dtime_t>(appender, dtime_t(value.micros));
161: }
162: 
163: duckdb_state duckdb_append_timestamp(duckdb_appender appender, duckdb_timestamp value) {
164: 	return duckdb_append_internal<timestamp_t>(appender, timestamp_t(value.micros));
165: }
166: 
167: duckdb_state duckdb_append_interval(duckdb_appender appender, duckdb_interval value) {
168: 	interval_t interval;
169: 	interval.months = value.months;
170: 	interval.days = value.days;
171: 	interval.micros = value.micros;
172: 	return duckdb_append_internal<interval_t>(appender, interval);
173: }
174: 
175: duckdb_state duckdb_append_null(duckdb_appender appender) {
176: 	return duckdb_append_internal<std::nullptr_t>(appender, nullptr);
177: }
178: 
179: duckdb_state duckdb_append_varchar(duckdb_appender appender, const char *val) {
180: 	return duckdb_append_internal<const char *>(appender, val);
181: }
182: 
183: duckdb_state duckdb_append_varchar_length(duckdb_appender appender, const char *val, idx_t length) {
184: 	return duckdb_append_internal<string_t>(appender, string_t(val, length));
185: }
186: duckdb_state duckdb_append_blob(duckdb_appender appender, const void *data, idx_t length) {
187: 	return duckdb_append_internal<string_t>(appender, string_t((const char *)data, length));
188: }
189: 
190: duckdb_state duckdb_appender_flush(duckdb_appender appender) {
191: 	return duckdb_appender_run_function(appender, [&](Appender &appender) { appender.Flush(); });
192: }
193: 
194: duckdb_state duckdb_appender_close(duckdb_appender appender) {
195: 	return duckdb_appender_run_function(appender, [&](Appender &appender) { appender.Close(); });
196: }
[end of src/main/capi/appender-c.cpp]
[start of src/main/capi/duckdb-c.cpp]
1: #include "duckdb/main/capi_internal.hpp"
2: 
3: using duckdb::Connection;
4: using duckdb::DatabaseData;
5: using duckdb::DBConfig;
6: using duckdb::DuckDB;
7: 
8: duckdb_state duckdb_open_ext(const char *path, duckdb_database *out, duckdb_config config, char **error) {
9: 	auto wrapper = new DatabaseData();
10: 	try {
11: 		auto db_config = (DBConfig *)config;
12: 		wrapper->database = duckdb::make_unique<DuckDB>(path, db_config);
13: 	} catch (std::exception &ex) {
14: 		if (error) {
15: 			*error = strdup(ex.what());
16: 		}
17: 		delete wrapper;
18: 		return DuckDBError;
19: 	} catch (...) { // LCOV_EXCL_START
20: 		if (error) {
21: 			*error = strdup("Unknown error");
22: 		}
23: 		delete wrapper;
24: 		return DuckDBError;
25: 	} // LCOV_EXCL_STOP
26: 	*out = (duckdb_database)wrapper;
27: 	return DuckDBSuccess;
28: }
29: 
30: duckdb_state duckdb_open(const char *path, duckdb_database *out) {
31: 	return duckdb_open_ext(path, out, nullptr, nullptr);
32: }
33: 
34: void duckdb_close(duckdb_database *database) {
35: 	if (database && *database) {
36: 		auto wrapper = (DatabaseData *)*database;
37: 		delete wrapper;
38: 		*database = nullptr;
39: 	}
40: }
41: 
42: duckdb_state duckdb_connect(duckdb_database database, duckdb_connection *out) {
43: 	if (!database || !out) {
44: 		return DuckDBError;
45: 	}
46: 	auto wrapper = (DatabaseData *)database;
47: 	Connection *connection;
48: 	try {
49: 		connection = new Connection(*wrapper->database);
50: 	} catch (...) { // LCOV_EXCL_START
51: 		return DuckDBError;
52: 	} // LCOV_EXCL_STOP
53: 	*out = (duckdb_connection)connection;
54: 	return DuckDBSuccess;
55: }
56: 
57: void duckdb_disconnect(duckdb_connection *connection) {
58: 	if (connection && *connection) {
59: 		Connection *conn = (Connection *)*connection;
60: 		delete conn;
61: 		*connection = nullptr;
62: 	}
63: }
64: 
65: duckdb_state duckdb_query(duckdb_connection connection, const char *query, duckdb_result *out) {
66: 	Connection *conn = (Connection *)connection;
67: 	auto result = conn->Query(query);
68: 	return duckdb_translate_result(result.get(), out);
69: }
[end of src/main/capi/duckdb-c.cpp]
[start of src/main/capi/helper-c.cpp]
1: #include "duckdb/main/capi_internal.hpp"
2: 
3: namespace duckdb {
4: 
5: duckdb_type ConvertCPPTypeToC(const LogicalType &sql_type) {
6: 	switch (sql_type.id()) {
7: 	case LogicalTypeId::BOOLEAN:
8: 		return DUCKDB_TYPE_BOOLEAN;
9: 	case LogicalTypeId::TINYINT:
10: 		return DUCKDB_TYPE_TINYINT;
11: 	case LogicalTypeId::SMALLINT:
12: 		return DUCKDB_TYPE_SMALLINT;
13: 	case LogicalTypeId::INTEGER:
14: 		return DUCKDB_TYPE_INTEGER;
15: 	case LogicalTypeId::BIGINT:
16: 		return DUCKDB_TYPE_BIGINT;
17: 	case LogicalTypeId::UTINYINT:
18: 		return DUCKDB_TYPE_UTINYINT;
19: 	case LogicalTypeId::USMALLINT:
20: 		return DUCKDB_TYPE_USMALLINT;
21: 	case LogicalTypeId::UINTEGER:
22: 		return DUCKDB_TYPE_UINTEGER;
23: 	case LogicalTypeId::UBIGINT:
24: 		return DUCKDB_TYPE_UBIGINT;
25: 	case LogicalTypeId::HUGEINT:
26: 		return DUCKDB_TYPE_HUGEINT;
27: 	case LogicalTypeId::FLOAT:
28: 		return DUCKDB_TYPE_FLOAT;
29: 	case LogicalTypeId::DOUBLE:
30: 		return DUCKDB_TYPE_DOUBLE;
31: 	case LogicalTypeId::TIMESTAMP:
32: 	case LogicalTypeId::TIMESTAMP_SEC:
33: 	case LogicalTypeId::TIMESTAMP_MS:
34: 	case LogicalTypeId::TIMESTAMP_NS:
35: 	case LogicalTypeId::TIMESTAMP_TZ:
36: 		return DUCKDB_TYPE_TIMESTAMP;
37: 	case LogicalTypeId::DATE:
38: 		return DUCKDB_TYPE_DATE;
39: 	case LogicalTypeId::TIME:
40: 	case LogicalTypeId::TIME_TZ:
41: 		return DUCKDB_TYPE_TIME;
42: 	case LogicalTypeId::VARCHAR:
43: 		return DUCKDB_TYPE_VARCHAR;
44: 	case LogicalTypeId::BLOB:
45: 		return DUCKDB_TYPE_BLOB;
46: 	case LogicalTypeId::INTERVAL:
47: 		return DUCKDB_TYPE_INTERVAL;
48: 	case LogicalTypeId::DECIMAL:
49: 		return DUCKDB_TYPE_DECIMAL;
50: 	default: // LCOV_EXCL_START
51: 		D_ASSERT(0);
52: 		return DUCKDB_TYPE_INVALID;
53: 	} // LCOV_EXCL_STOP
54: }
55: idx_t GetCTypeSize(duckdb_type type) {
56: 	switch (type) {
57: 	case DUCKDB_TYPE_BOOLEAN:
58: 		return sizeof(bool);
59: 	case DUCKDB_TYPE_TINYINT:
60: 		return sizeof(int8_t);
61: 	case DUCKDB_TYPE_SMALLINT:
62: 		return sizeof(int16_t);
63: 	case DUCKDB_TYPE_INTEGER:
64: 		return sizeof(int32_t);
65: 	case DUCKDB_TYPE_BIGINT:
66: 		return sizeof(int64_t);
67: 	case DUCKDB_TYPE_UTINYINT:
68: 		return sizeof(uint8_t);
69: 	case DUCKDB_TYPE_USMALLINT:
70: 		return sizeof(uint16_t);
71: 	case DUCKDB_TYPE_UINTEGER:
72: 		return sizeof(uint32_t);
73: 	case DUCKDB_TYPE_UBIGINT:
74: 		return sizeof(uint64_t);
75: 	case DUCKDB_TYPE_HUGEINT:
76: 		return sizeof(duckdb_hugeint);
77: 	case DUCKDB_TYPE_FLOAT:
78: 		return sizeof(float);
79: 	case DUCKDB_TYPE_DOUBLE:
80: 		return sizeof(double);
81: 	case DUCKDB_TYPE_DATE:
82: 		return sizeof(duckdb_date);
83: 	case DUCKDB_TYPE_TIME:
84: 		return sizeof(duckdb_time);
85: 	case DUCKDB_TYPE_TIMESTAMP:
86: 		return sizeof(duckdb_timestamp);
87: 	case DUCKDB_TYPE_VARCHAR:
88: 		return sizeof(const char *);
89: 	case DUCKDB_TYPE_BLOB:
90: 		return sizeof(duckdb_blob);
91: 	case DUCKDB_TYPE_INTERVAL:
92: 		return sizeof(duckdb_interval);
93: 	case DUCKDB_TYPE_DECIMAL:
94: 		return sizeof(duckdb_hugeint);
95: 	default: // LCOV_EXCL_START
96: 		// unsupported type
97: 		D_ASSERT(0);
98: 		return sizeof(const char *);
99: 	} // LCOV_EXCL_STOP
100: }
101: 
102: } // namespace duckdb
103: 
104: void *duckdb_malloc(size_t size) {
105: 	return malloc(size);
106: }
107: 
108: void duckdb_free(void *ptr) {
109: 	free(ptr);
110: }
[end of src/main/capi/helper-c.cpp]
[start of src/main/capi/prepared-c.cpp]
1: #include "duckdb/main/capi_internal.hpp"
2: #include "duckdb/common/assert.hpp"
3: #include "duckdb/main/query_result.hpp"
4: #include "duckdb/main/prepared_statement_data.hpp"
5: 
6: using duckdb::Connection;
7: using duckdb::date_t;
8: using duckdb::dtime_t;
9: using duckdb::hugeint_t;
10: using duckdb::MaterializedQueryResult;
11: using duckdb::PreparedStatementWrapper;
12: using duckdb::QueryResultType;
13: using duckdb::timestamp_t;
14: using duckdb::Value;
15: 
16: duckdb_state duckdb_prepare(duckdb_connection connection, const char *query,
17:                             duckdb_prepared_statement *out_prepared_statement) {
18: 	if (!connection || !query || !out_prepared_statement) {
19: 		return DuckDBError;
20: 	}
21: 	auto wrapper = new PreparedStatementWrapper();
22: 	Connection *conn = (Connection *)connection;
23: 	wrapper->statement = conn->Prepare(query);
24: 	*out_prepared_statement = (duckdb_prepared_statement)wrapper;
25: 	return wrapper->statement->success ? DuckDBSuccess : DuckDBError;
26: }
27: 
28: const char *duckdb_prepare_error(duckdb_prepared_statement prepared_statement) {
29: 	auto wrapper = (PreparedStatementWrapper *)prepared_statement;
30: 	if (!wrapper || !wrapper->statement || wrapper->statement->success) {
31: 		return nullptr;
32: 	}
33: 	return wrapper->statement->error.c_str();
34: }
35: 
36: idx_t duckdb_nparams(duckdb_prepared_statement prepared_statement) {
37: 	auto wrapper = (PreparedStatementWrapper *)prepared_statement;
38: 	if (!wrapper || !wrapper->statement || !wrapper->statement->success) {
39: 		return 0;
40: 	}
41: 	return wrapper->statement->n_param;
42: }
43: 
44: duckdb_type duckdb_param_type(duckdb_prepared_statement prepared_statement, idx_t param_idx) {
45: 	auto wrapper = (PreparedStatementWrapper *)prepared_statement;
46: 	if (!wrapper || !wrapper->statement || !wrapper->statement->success) {
47: 		return DUCKDB_TYPE_INVALID;
48: 	}
49: 	auto entry = wrapper->statement->data->value_map.find(param_idx);
50: 	if (entry == wrapper->statement->data->value_map.end()) {
51: 		return DUCKDB_TYPE_INVALID;
52: 	}
53: 	return ConvertCPPTypeToC(entry->second[0]->type());
54: }
55: 
56: static duckdb_state duckdb_bind_value(duckdb_prepared_statement prepared_statement, idx_t param_idx, Value val) {
57: 	auto wrapper = (PreparedStatementWrapper *)prepared_statement;
58: 	if (!wrapper || !wrapper->statement || !wrapper->statement->success) {
59: 		return DuckDBError;
60: 	}
61: 	if (param_idx > wrapper->statement->n_param) {
62: 		return DuckDBError;
63: 	}
64: 	if (param_idx > wrapper->values.size()) {
65: 		wrapper->values.resize(param_idx);
66: 	}
67: 	wrapper->values[param_idx - 1] = val;
68: 	return DuckDBSuccess;
69: }
70: 
71: duckdb_state duckdb_bind_boolean(duckdb_prepared_statement prepared_statement, idx_t param_idx, bool val) {
72: 	return duckdb_bind_value(prepared_statement, param_idx, Value::BOOLEAN(val));
73: }
74: 
75: duckdb_state duckdb_bind_int8(duckdb_prepared_statement prepared_statement, idx_t param_idx, int8_t val) {
76: 	return duckdb_bind_value(prepared_statement, param_idx, Value::TINYINT(val));
77: }
78: 
79: duckdb_state duckdb_bind_int16(duckdb_prepared_statement prepared_statement, idx_t param_idx, int16_t val) {
80: 	return duckdb_bind_value(prepared_statement, param_idx, Value::SMALLINT(val));
81: }
82: 
83: duckdb_state duckdb_bind_int32(duckdb_prepared_statement prepared_statement, idx_t param_idx, int32_t val) {
84: 	return duckdb_bind_value(prepared_statement, param_idx, Value::INTEGER(val));
85: }
86: 
87: duckdb_state duckdb_bind_int64(duckdb_prepared_statement prepared_statement, idx_t param_idx, int64_t val) {
88: 	return duckdb_bind_value(prepared_statement, param_idx, Value::BIGINT(val));
89: }
90: 
91: duckdb_state duckdb_bind_hugeint(duckdb_prepared_statement prepared_statement, idx_t param_idx, duckdb_hugeint val) {
92: 	hugeint_t internal;
93: 	internal.lower = val.lower;
94: 	internal.upper = val.upper;
95: 	return duckdb_bind_value(prepared_statement, param_idx, Value::HUGEINT(internal));
96: }
97: 
98: duckdb_state duckdb_bind_uint8(duckdb_prepared_statement prepared_statement, idx_t param_idx, uint8_t val) {
99: 	return duckdb_bind_value(prepared_statement, param_idx, Value::UTINYINT(val));
100: }
101: 
102: duckdb_state duckdb_bind_uint16(duckdb_prepared_statement prepared_statement, idx_t param_idx, uint16_t val) {
103: 	return duckdb_bind_value(prepared_statement, param_idx, Value::USMALLINT(val));
104: }
105: 
106: duckdb_state duckdb_bind_uint32(duckdb_prepared_statement prepared_statement, idx_t param_idx, uint32_t val) {
107: 	return duckdb_bind_value(prepared_statement, param_idx, Value::UINTEGER(val));
108: }
109: 
110: duckdb_state duckdb_bind_uint64(duckdb_prepared_statement prepared_statement, idx_t param_idx, uint64_t val) {
111: 	return duckdb_bind_value(prepared_statement, param_idx, Value::UBIGINT(val));
112: }
113: 
114: duckdb_state duckdb_bind_float(duckdb_prepared_statement prepared_statement, idx_t param_idx, float val) {
115: 	if (!Value::FloatIsValid(val)) {
116: 		return DuckDBError;
117: 	}
118: 	return duckdb_bind_value(prepared_statement, param_idx, Value::FLOAT(val));
119: }
120: 
121: duckdb_state duckdb_bind_double(duckdb_prepared_statement prepared_statement, idx_t param_idx, double val) {
122: 	if (!Value::DoubleIsValid(val)) {
123: 		return DuckDBError;
124: 	}
125: 	return duckdb_bind_value(prepared_statement, param_idx, Value::DOUBLE(val));
126: }
127: 
128: duckdb_state duckdb_bind_date(duckdb_prepared_statement prepared_statement, idx_t param_idx, duckdb_date val) {
129: 	return duckdb_bind_value(prepared_statement, param_idx, Value::DATE(date_t(val.days)));
130: }
131: 
132: duckdb_state duckdb_bind_time(duckdb_prepared_statement prepared_statement, idx_t param_idx, duckdb_time val) {
133: 	return duckdb_bind_value(prepared_statement, param_idx, Value::TIME(dtime_t(val.micros)));
134: }
135: 
136: duckdb_state duckdb_bind_timestamp(duckdb_prepared_statement prepared_statement, idx_t param_idx,
137:                                    duckdb_timestamp val) {
138: 	return duckdb_bind_value(prepared_statement, param_idx, Value::TIMESTAMP(timestamp_t(val.micros)));
139: }
140: 
141: duckdb_state duckdb_bind_interval(duckdb_prepared_statement prepared_statement, idx_t param_idx, duckdb_interval val) {
142: 	return duckdb_bind_value(prepared_statement, param_idx, Value::INTERVAL(val.months, val.days, val.micros));
143: }
144: 
145: duckdb_state duckdb_bind_varchar(duckdb_prepared_statement prepared_statement, idx_t param_idx, const char *val) {
146: 	return duckdb_bind_value(prepared_statement, param_idx, Value(val));
147: }
148: 
149: duckdb_state duckdb_bind_varchar_length(duckdb_prepared_statement prepared_statement, idx_t param_idx, const char *val,
150:                                         idx_t length) {
151: 	return duckdb_bind_value(prepared_statement, param_idx, Value(std::string(val, length)));
152: }
153: 
154: duckdb_state duckdb_bind_blob(duckdb_prepared_statement prepared_statement, idx_t param_idx, const void *data,
155:                               idx_t length) {
156: 	return duckdb_bind_value(prepared_statement, param_idx, Value::BLOB((duckdb::const_data_ptr_t)data, length));
157: }
158: 
159: duckdb_state duckdb_bind_null(duckdb_prepared_statement prepared_statement, idx_t param_idx) {
160: 	return duckdb_bind_value(prepared_statement, param_idx, Value());
161: }
162: 
163: duckdb_state duckdb_execute_prepared(duckdb_prepared_statement prepared_statement, duckdb_result *out_result) {
164: 	auto wrapper = (PreparedStatementWrapper *)prepared_statement;
165: 	if (!wrapper || !wrapper->statement || !wrapper->statement->success) {
166: 		return DuckDBError;
167: 	}
168: 	auto result = wrapper->statement->Execute(wrapper->values, false);
169: 	D_ASSERT(result->type == QueryResultType::MATERIALIZED_RESULT);
170: 	auto mat_res = (MaterializedQueryResult *)result.get();
171: 	return duckdb_translate_result(mat_res, out_result);
172: }
173: 
174: void duckdb_destroy_prepare(duckdb_prepared_statement *prepared_statement) {
175: 	if (!prepared_statement) {
176: 		return;
177: 	}
178: 	auto wrapper = (PreparedStatementWrapper *)*prepared_statement;
179: 	if (wrapper) {
180: 		delete wrapper;
181: 	}
182: 	*prepared_statement = nullptr;
183: }
[end of src/main/capi/prepared-c.cpp]
[start of src/main/capi/result-c.cpp]
1: #include "duckdb/main/capi_internal.hpp"
2: #include "duckdb/common/types/timestamp.hpp"
3: 
4: namespace duckdb {
5: 
6: template <class T>
7: void WriteData(duckdb_result *out, ChunkCollection &source, idx_t col) {
8: 	idx_t row = 0;
9: 	auto target = (T *)out->__deprecated_columns[col].__deprecated_data;
10: 	for (auto &chunk : source.Chunks()) {
11: 		auto source = FlatVector::GetData<T>(chunk->data[col]);
12: 		auto &mask = FlatVector::Validity(chunk->data[col]);
13: 
14: 		for (idx_t k = 0; k < chunk->size(); k++, row++) {
15: 			if (!mask.RowIsValid(k)) {
16: 				continue;
17: 			}
18: 			target[row] = source[k];
19: 		}
20: 	}
21: }
22: 
23: duckdb_state duckdb_translate_result(MaterializedQueryResult *result, duckdb_result *out) {
24: 	D_ASSERT(result);
25: 	if (!out) {
26: 		// no result to write to, only return the status
27: 		return result->success ? DuckDBSuccess : DuckDBError;
28: 	}
29: 	memset(out, 0, sizeof(duckdb_result));
30: 	if (!result->success) {
31: 		// write the error message
32: 		out->__deprecated_error_message = strdup(result->error.c_str());
33: 		return DuckDBError;
34: 	}
35: 	// copy the data
36: 	// first write the meta data
37: 	out->__deprecated_column_count = result->types.size();
38: 	out->__deprecated_row_count = result->collection.Count();
39: 	out->__deprecated_rows_changed = 0;
40: 	if (out->__deprecated_row_count > 0 && StatementTypeReturnChanges(result->statement_type)) {
41: 		// update total changes
42: 		auto row_changes = result->GetValue(0, 0);
43: 		if (!row_changes.IsNull() && row_changes.TryCastAs(LogicalType::BIGINT)) {
44: 			out->__deprecated_rows_changed = row_changes.GetValue<int64_t>();
45: 		}
46: 	}
47: 	out->__deprecated_columns = (duckdb_column *)duckdb_malloc(sizeof(duckdb_column) * out->__deprecated_column_count);
48: 	if (!out->__deprecated_columns) { // LCOV_EXCL_START
49: 		// malloc failure
50: 		return DuckDBError;
51: 	} // LCOV_EXCL_STOP
52: 
53: 	// zero initialize the columns (so we can cleanly delete it in case a malloc fails)
54: 	memset(out->__deprecated_columns, 0, sizeof(duckdb_column) * out->__deprecated_column_count);
55: 	for (idx_t i = 0; i < out->__deprecated_column_count; i++) {
56: 		auto column_data = new DuckDBColumnData();
57: 		column_data->type = result->types[i];
58: 		out->__deprecated_columns[i].internal_data = column_data;
59: 		out->__deprecated_columns[i].__deprecated_type = ConvertCPPTypeToC(result->types[i]);
60: 		out->__deprecated_columns[i].__deprecated_name = strdup(result->names[i].c_str());
61: 		out->__deprecated_columns[i].__deprecated_nullmask =
62: 		    (bool *)duckdb_malloc(sizeof(bool) * out->__deprecated_row_count);
63: 		out->__deprecated_columns[i].__deprecated_data =
64: 		    duckdb_malloc(GetCTypeSize(out->__deprecated_columns[i].__deprecated_type) * out->__deprecated_row_count);
65: 		if (!out->__deprecated_columns[i].__deprecated_nullmask || !out->__deprecated_columns[i].__deprecated_name ||
66: 		    !out->__deprecated_columns[i].__deprecated_data) { // LCOV_EXCL_START
67: 			// malloc failure
68: 			return DuckDBError;
69: 		} // LCOV_EXCL_STOP
70: 	}
71: 	// now write the data
72: 	for (idx_t col = 0; col < out->__deprecated_column_count; col++) {
73: 		// first set the nullmask
74: 		idx_t row = 0;
75: 		for (auto &chunk : result->collection.Chunks()) {
76: 			for (idx_t k = 0; k < chunk->size(); k++) {
77: 				out->__deprecated_columns[col].__deprecated_nullmask[row++] = FlatVector::IsNull(chunk->data[col], k);
78: 			}
79: 		}
80: 		// then write the data
81: 		switch (result->types[col].id()) {
82: 		case LogicalTypeId::BOOLEAN:
83: 			WriteData<bool>(out, result->collection, col);
84: 			break;
85: 		case LogicalTypeId::TINYINT:
86: 			WriteData<int8_t>(out, result->collection, col);
87: 			break;
88: 		case LogicalTypeId::SMALLINT:
89: 			WriteData<int16_t>(out, result->collection, col);
90: 			break;
91: 		case LogicalTypeId::INTEGER:
92: 			WriteData<int32_t>(out, result->collection, col);
93: 			break;
94: 		case LogicalTypeId::BIGINT:
95: 			WriteData<int64_t>(out, result->collection, col);
96: 			break;
97: 		case LogicalTypeId::UTINYINT:
98: 			WriteData<uint8_t>(out, result->collection, col);
99: 			break;
100: 		case LogicalTypeId::USMALLINT:
101: 			WriteData<uint16_t>(out, result->collection, col);
102: 			break;
103: 		case LogicalTypeId::UINTEGER:
104: 			WriteData<uint32_t>(out, result->collection, col);
105: 			break;
106: 		case LogicalTypeId::UBIGINT:
107: 			WriteData<uint64_t>(out, result->collection, col);
108: 			break;
109: 		case LogicalTypeId::FLOAT:
110: 			WriteData<float>(out, result->collection, col);
111: 			break;
112: 		case LogicalTypeId::DOUBLE:
113: 			WriteData<double>(out, result->collection, col);
114: 			break;
115: 		case LogicalTypeId::DATE:
116: 			WriteData<date_t>(out, result->collection, col);
117: 			break;
118: 		case LogicalTypeId::TIME:
119: 		case LogicalTypeId::TIME_TZ:
120: 			WriteData<dtime_t>(out, result->collection, col);
121: 			break;
122: 		case LogicalTypeId::TIMESTAMP:
123: 		case LogicalTypeId::TIMESTAMP_TZ:
124: 			WriteData<timestamp_t>(out, result->collection, col);
125: 			break;
126: 		case LogicalTypeId::VARCHAR: {
127: 			idx_t row = 0;
128: 			auto target = (const char **)out->__deprecated_columns[col].__deprecated_data;
129: 			for (auto &chunk : result->collection.Chunks()) {
130: 				auto source = FlatVector::GetData<string_t>(chunk->data[col]);
131: 				for (idx_t k = 0; k < chunk->size(); k++) {
132: 					if (!FlatVector::IsNull(chunk->data[col], k)) {
133: 						target[row] = (char *)duckdb_malloc(source[k].GetSize() + 1);
134: 						assert(target[row]);
135: 						memcpy((void *)target[row], source[k].GetDataUnsafe(), source[k].GetSize());
136: 						auto write_arr = (char *)target[row];
137: 						write_arr[source[k].GetSize()] = '\0';
138: 					} else {
139: 						target[row] = nullptr;
140: 					}
141: 					row++;
142: 				}
143: 			}
144: 			break;
145: 		}
146: 		case LogicalTypeId::BLOB: {
147: 			idx_t row = 0;
148: 			auto target = (duckdb_blob *)out->__deprecated_columns[col].__deprecated_data;
149: 			for (auto &chunk : result->collection.Chunks()) {
150: 				auto source = FlatVector::GetData<string_t>(chunk->data[col]);
151: 				for (idx_t k = 0; k < chunk->size(); k++) {
152: 					if (!FlatVector::IsNull(chunk->data[col], k)) {
153: 						target[row].data = (char *)duckdb_malloc(source[k].GetSize());
154: 						target[row].size = source[k].GetSize();
155: 						assert(target[row].data);
156: 						memcpy((void *)target[row].data, source[k].GetDataUnsafe(), source[k].GetSize());
157: 					} else {
158: 						target[row].data = nullptr;
159: 						target[row].size = 0;
160: 					}
161: 					row++;
162: 				}
163: 			}
164: 			break;
165: 		}
166: 		case LogicalTypeId::TIMESTAMP_NS:
167: 		case LogicalTypeId::TIMESTAMP_MS:
168: 		case LogicalTypeId::TIMESTAMP_SEC: {
169: 			idx_t row = 0;
170: 			auto target = (timestamp_t *)out->__deprecated_columns[col].__deprecated_data;
171: 			for (auto &chunk : result->collection.Chunks()) {
172: 				auto source = FlatVector::GetData<timestamp_t>(chunk->data[col]);
173: 
174: 				for (idx_t k = 0; k < chunk->size(); k++) {
175: 					if (!FlatVector::IsNull(chunk->data[col], k)) {
176: 						if (result->types[col].id() == LogicalTypeId::TIMESTAMP_NS) {
177: 							target[row] = Timestamp::FromEpochNanoSeconds(source[k].value);
178: 						} else if (result->types[col].id() == LogicalTypeId::TIMESTAMP_MS) {
179: 							target[row] = Timestamp::FromEpochMs(source[k].value);
180: 						} else {
181: 							D_ASSERT(result->types[col].id() == LogicalTypeId::TIMESTAMP_SEC);
182: 							target[row] = Timestamp::FromEpochSeconds(source[k].value);
183: 						}
184: 					}
185: 					row++;
186: 				}
187: 			}
188: 			break;
189: 		}
190: 		case LogicalTypeId::HUGEINT: {
191: 			idx_t row = 0;
192: 			auto target = (duckdb_hugeint *)out->__deprecated_columns[col].__deprecated_data;
193: 			for (auto &chunk : result->collection.Chunks()) {
194: 				auto source = FlatVector::GetData<hugeint_t>(chunk->data[col]);
195: 				for (idx_t k = 0; k < chunk->size(); k++) {
196: 					if (!FlatVector::IsNull(chunk->data[col], k)) {
197: 						target[row].lower = source[k].lower;
198: 						target[row].upper = source[k].upper;
199: 					}
200: 					row++;
201: 				}
202: 			}
203: 			break;
204: 		}
205: 		case LogicalTypeId::INTERVAL: {
206: 			idx_t row = 0;
207: 			auto target = (duckdb_interval *)out->__deprecated_columns[col].__deprecated_data;
208: 			for (auto &chunk : result->collection.Chunks()) {
209: 				auto source = FlatVector::GetData<interval_t>(chunk->data[col]);
210: 				for (idx_t k = 0; k < chunk->size(); k++) {
211: 					if (!FlatVector::IsNull(chunk->data[col], k)) {
212: 						target[row].days = source[k].days;
213: 						target[row].months = source[k].months;
214: 						target[row].micros = source[k].micros;
215: 					}
216: 					row++;
217: 				}
218: 			}
219: 			break;
220: 		}
221: 		case LogicalTypeId::DECIMAL: {
222: 			// get data
223: 			idx_t row = 0;
224: 			auto target = (hugeint_t *)out->__deprecated_columns[col].__deprecated_data;
225: 			switch (result->types[col].InternalType()) {
226: 			case PhysicalType::INT16: {
227: 				for (auto &chunk : result->collection.Chunks()) {
228: 					auto source = FlatVector::GetData<int16_t>(chunk->data[col]);
229: 					for (idx_t k = 0; k < chunk->size(); k++) {
230: 						if (!FlatVector::IsNull(chunk->data[col], k)) {
231: 							target[row].lower = source[k];
232: 							target[row].upper = 0;
233: 						}
234: 						row++;
235: 					}
236: 				}
237: 				break;
238: 			}
239: 			case PhysicalType::INT32: {
240: 				for (auto &chunk : result->collection.Chunks()) {
241: 					auto source = FlatVector::GetData<int32_t>(chunk->data[col]);
242: 					for (idx_t k = 0; k < chunk->size(); k++) {
243: 						if (!FlatVector::IsNull(chunk->data[col], k)) {
244: 							target[row].lower = source[k];
245: 							target[row].upper = 0;
246: 						}
247: 						row++;
248: 					}
249: 				}
250: 				break;
251: 			}
252: 			case PhysicalType::INT64: {
253: 				for (auto &chunk : result->collection.Chunks()) {
254: 					auto source = FlatVector::GetData<int64_t>(chunk->data[col]);
255: 					for (idx_t k = 0; k < chunk->size(); k++) {
256: 						if (!FlatVector::IsNull(chunk->data[col], k)) {
257: 							target[row].lower = source[k];
258: 							target[row].upper = 0;
259: 						}
260: 						row++;
261: 					}
262: 				}
263: 				break;
264: 			}
265: 			case PhysicalType::INT128: {
266: 				for (auto &chunk : result->collection.Chunks()) {
267: 					auto source = FlatVector::GetData<hugeint_t>(chunk->data[col]);
268: 					for (idx_t k = 0; k < chunk->size(); k++) {
269: 						if (!FlatVector::IsNull(chunk->data[col], k)) {
270: 							target[row].lower = source[k].lower;
271: 							target[row].upper = source[k].upper;
272: 						}
273: 						row++;
274: 					}
275: 				}
276: 				break;
277: 			}
278: 			default:
279: 				throw std::runtime_error("Unsupported physical type for Decimal" +
280: 				                         TypeIdToString(result->types[col].InternalType()));
281: 			}
282: 			break;
283: 		}
284: 		default: // LCOV_EXCL_START
285: 			std::string err_msg = "Unsupported type for C API: " + result->types[col].ToString();
286: 			out->__deprecated_error_message = strdup(err_msg.c_str());
287: 			return DuckDBError;
288: 		} // LCOV_EXCL_STOP
289: 	}
290: 	return DuckDBSuccess;
291: }
292: 
293: } // namespace duckdb
294: 
295: static void DuckdbDestroyColumn(duckdb_column column, idx_t count) {
296: 	if (column.__deprecated_data) {
297: 		if (column.__deprecated_type == DUCKDB_TYPE_VARCHAR) {
298: 			// varchar, delete individual strings
299: 			auto data = (char **)column.__deprecated_data;
300: 			for (idx_t i = 0; i < count; i++) {
301: 				if (data[i]) {
302: 					duckdb_free(data[i]);
303: 				}
304: 			}
305: 		} else if (column.__deprecated_type == DUCKDB_TYPE_BLOB) {
306: 			// blob, delete individual blobs
307: 			auto data = (duckdb_blob *)column.__deprecated_data;
308: 			for (idx_t i = 0; i < count; i++) {
309: 				if (data[i].data) {
310: 					duckdb_free((void *)data[i].data);
311: 				}
312: 			}
313: 		}
314: 		duckdb_free(column.__deprecated_data);
315: 	}
316: 	if (column.__deprecated_nullmask) {
317: 		duckdb_free(column.__deprecated_nullmask);
318: 	}
319: 	if (column.__deprecated_name) {
320: 		duckdb_free(column.__deprecated_name);
321: 	}
322: 	if (column.internal_data) {
323: 		auto column_data = (duckdb::DuckDBColumnData *)column.internal_data;
324: 		delete column_data;
325: 	}
326: }
327: 
328: void duckdb_destroy_result(duckdb_result *result) {
329: 	if (result->__deprecated_error_message) {
330: 		duckdb_free(result->__deprecated_error_message);
331: 	}
332: 	if (result->__deprecated_columns) {
333: 		for (idx_t i = 0; i < result->__deprecated_column_count; i++) {
334: 			DuckdbDestroyColumn(result->__deprecated_columns[i], result->__deprecated_row_count);
335: 		}
336: 		duckdb_free(result->__deprecated_columns);
337: 	}
338: 	memset(result, 0, sizeof(duckdb_result));
339: }
340: 
341: const char *duckdb_column_name(duckdb_result *result, idx_t col) {
342: 	if (!result || col >= result->__deprecated_column_count) {
343: 		return nullptr;
344: 	}
345: 	return result->__deprecated_columns[col].__deprecated_name;
346: }
347: 
348: duckdb_type duckdb_column_type(duckdb_result *result, idx_t col) {
349: 	if (!result || col >= result->__deprecated_column_count) {
350: 		return DUCKDB_TYPE_INVALID;
351: 	}
352: 	return result->__deprecated_columns[col].__deprecated_type;
353: }
354: 
355: idx_t duckdb_column_count(duckdb_result *result) {
356: 	if (!result) {
357: 		return 0;
358: 	}
359: 	return result->__deprecated_column_count;
360: }
361: 
362: idx_t duckdb_row_count(duckdb_result *result) {
363: 	if (!result) {
364: 		return 0;
365: 	}
366: 	return result->__deprecated_row_count;
367: }
368: 
369: idx_t duckdb_rows_changed(duckdb_result *result) {
370: 	if (!result) {
371: 		return 0;
372: 	}
373: 	return result->__deprecated_rows_changed;
374: }
375: 
376: void *duckdb_column_data(duckdb_result *result, idx_t col) {
377: 	if (!result || col >= result->__deprecated_column_count) {
378: 		return nullptr;
379: 	}
380: 	return result->__deprecated_columns[col].__deprecated_data;
381: }
382: 
383: bool *duckdb_nullmask_data(duckdb_result *result, idx_t col) {
384: 	if (!result || col >= result->__deprecated_column_count) {
385: 		return nullptr;
386: 	}
387: 	return result->__deprecated_columns[col].__deprecated_nullmask;
388: }
389: 
390: char *duckdb_result_error(duckdb_result *result) {
391: 	if (!result) {
392: 		return nullptr;
393: 	}
394: 	return result->__deprecated_error_message;
395: }
[end of src/main/capi/result-c.cpp]
[start of src/main/capi/value-c.cpp]
1: #include "duckdb/main/capi_internal.hpp"
2: #include "duckdb/common/types/date.hpp"
3: #include "duckdb/common/types/time.hpp"
4: #include "duckdb/common/types/timestamp.hpp"
5: #include "duckdb/common/operator/cast_operators.hpp"
6: #include "duckdb/common/operator/string_cast.hpp"
7: 
8: using duckdb::const_data_ptr_t;
9: using duckdb::Date;
10: using duckdb::date_t;
11: using duckdb::dtime_t;
12: using duckdb::hugeint_t;
13: using duckdb::interval_t;
14: using duckdb::LogicalType;
15: using duckdb::string;
16: using duckdb::string_t;
17: using duckdb::Time;
18: using duckdb::Timestamp;
19: using duckdb::timestamp_t;
20: using duckdb::Value;
21: using duckdb::Vector;
22: 
23: namespace duckdb {
24: 
25: //===--------------------------------------------------------------------===//
26: // Unsafe Fetch (for internal use only)
27: //===--------------------------------------------------------------------===//
28: template <class T>
29: T UnsafeFetch(duckdb_result *result, idx_t col, idx_t row) {
30: 	D_ASSERT(row < result->__deprecated_row_count);
31: 	return ((T *)result->__deprecated_columns[col].__deprecated_data)[row];
32: }
33: 
34: //===--------------------------------------------------------------------===//
35: // Fetch Default Value
36: //===--------------------------------------------------------------------===//
37: struct FetchDefaultValue {
38: 	template <class T>
39: 	static T Operation() {
40: 		return 0;
41: 	}
42: };
43: 
44: template <>
45: date_t FetchDefaultValue::Operation() {
46: 	date_t result;
47: 	result.days = 0;
48: 	return result;
49: }
50: 
51: template <>
52: dtime_t FetchDefaultValue::Operation() {
53: 	dtime_t result;
54: 	result.micros = 0;
55: 	return result;
56: }
57: 
58: template <>
59: timestamp_t FetchDefaultValue::Operation() {
60: 	timestamp_t result;
61: 	result.value = 0;
62: 	return result;
63: }
64: 
65: template <>
66: interval_t FetchDefaultValue::Operation() {
67: 	interval_t result;
68: 	result.months = 0;
69: 	result.days = 0;
70: 	result.micros = 0;
71: 	return result;
72: }
73: 
74: template <>
75: char *FetchDefaultValue::Operation() {
76: 	return nullptr;
77: }
78: 
79: template <>
80: duckdb_blob FetchDefaultValue::Operation() {
81: 	duckdb_blob result;
82: 	result.data = nullptr;
83: 	result.size = 0;
84: 	return result;
85: }
86: 
87: //===--------------------------------------------------------------------===//
88: // String Casts
89: //===--------------------------------------------------------------------===//
90: template <class OP>
91: struct FromCStringCastWrapper {
92: 	template <class SOURCE_TYPE, class RESULT_TYPE>
93: 	static bool Operation(SOURCE_TYPE input_str, RESULT_TYPE &result) {
94: 		string_t input(input_str);
95: 		return OP::template Operation<string_t, RESULT_TYPE>(input, result);
96: 	}
97: };
98: 
99: template <class OP>
100: struct ToCStringCastWrapper {
101: 	template <class SOURCE_TYPE, class RESULT_TYPE>
102: 	static bool Operation(SOURCE_TYPE input, RESULT_TYPE &result) {
103: 		Vector result_vector(LogicalType::VARCHAR, nullptr);
104: 		auto result_string = OP::template Operation<SOURCE_TYPE>(input, result_vector);
105: 		auto result_size = result_string.GetSize();
106: 		auto result_data = result_string.GetDataUnsafe();
107: 
108: 		result = (char *)duckdb_malloc(result_size + 1);
109: 		memcpy(result, result_data, result_size);
110: 		result[result_size] = '\0';
111: 		return true;
112: 	}
113: };
114: 
115: //===--------------------------------------------------------------------===//
116: // Blob Casts
117: //===--------------------------------------------------------------------===//
118: struct FromCBlobCastWrapper {
119: 	template <class SOURCE_TYPE, class RESULT_TYPE>
120: 	static bool Operation(SOURCE_TYPE input_str, RESULT_TYPE &result) {
121: 		return false;
122: 	}
123: };
124: 
125: template <>
126: bool FromCBlobCastWrapper::Operation(duckdb_blob input, char *&result) {
127: 	string_t input_str((const char *)input.data, input.size);
128: 	return ToCStringCastWrapper<duckdb::CastFromBlob>::template Operation<string_t, char *>(input_str, result);
129: }
130: 
131: } // namespace duckdb
132: 
133: using duckdb::FetchDefaultValue;
134: using duckdb::FromCBlobCastWrapper;
135: using duckdb::FromCStringCastWrapper;
136: using duckdb::ToCStringCastWrapper;
137: using duckdb::UnsafeFetch;
138: 
139: //===--------------------------------------------------------------------===//
140: // Templated Casts
141: //===--------------------------------------------------------------------===//
142: template <class SOURCE_TYPE, class RESULT_TYPE, class OP>
143: RESULT_TYPE TryCastCInternal(duckdb_result *result, idx_t col, idx_t row) {
144: 	RESULT_TYPE result_value;
145: 	try {
146: 		if (!OP::template Operation<SOURCE_TYPE, RESULT_TYPE>(UnsafeFetch<SOURCE_TYPE>(result, col, row),
147: 		                                                      result_value)) {
148: 			return FetchDefaultValue::Operation<RESULT_TYPE>();
149: 		}
150: 	} catch (...) {
151: 		return FetchDefaultValue::Operation<RESULT_TYPE>();
152: 	}
153: 	return result_value;
154: }
155: 
156: static bool CanFetchValue(duckdb_result *result, idx_t col, idx_t row) {
157: 	if (!result || col >= result->__deprecated_column_count || row >= result->__deprecated_row_count) {
158: 		return false;
159: 	}
160: 	if (result->__deprecated_columns[col].__deprecated_nullmask[row]) {
161: 		return false;
162: 	}
163: 	return true;
164: }
165: 
166: template <class RESULT_TYPE, class OP = duckdb::TryCast>
167: static RESULT_TYPE GetInternalCValue(duckdb_result *result, idx_t col, idx_t row) {
168: 	if (!CanFetchValue(result, col, row)) {
169: 		return FetchDefaultValue::Operation<RESULT_TYPE>();
170: 	}
171: 	switch (result->__deprecated_columns[col].__deprecated_type) {
172: 	case DUCKDB_TYPE_BOOLEAN:
173: 		return TryCastCInternal<bool, RESULT_TYPE, OP>(result, col, row);
174: 	case DUCKDB_TYPE_TINYINT:
175: 		return TryCastCInternal<int8_t, RESULT_TYPE, OP>(result, col, row);
176: 	case DUCKDB_TYPE_SMALLINT:
177: 		return TryCastCInternal<int16_t, RESULT_TYPE, OP>(result, col, row);
178: 	case DUCKDB_TYPE_INTEGER:
179: 		return TryCastCInternal<int32_t, RESULT_TYPE, OP>(result, col, row);
180: 	case DUCKDB_TYPE_BIGINT:
181: 		return TryCastCInternal<int64_t, RESULT_TYPE, OP>(result, col, row);
182: 	case DUCKDB_TYPE_UTINYINT:
183: 		return TryCastCInternal<uint8_t, RESULT_TYPE, OP>(result, col, row);
184: 	case DUCKDB_TYPE_USMALLINT:
185: 		return TryCastCInternal<uint16_t, RESULT_TYPE, OP>(result, col, row);
186: 	case DUCKDB_TYPE_UINTEGER:
187: 		return TryCastCInternal<uint32_t, RESULT_TYPE, OP>(result, col, row);
188: 	case DUCKDB_TYPE_UBIGINT:
189: 		return TryCastCInternal<uint64_t, RESULT_TYPE, OP>(result, col, row);
190: 	case DUCKDB_TYPE_FLOAT:
191: 		return TryCastCInternal<float, RESULT_TYPE, OP>(result, col, row);
192: 	case DUCKDB_TYPE_DOUBLE:
193: 		return TryCastCInternal<double, RESULT_TYPE, OP>(result, col, row);
194: 	case DUCKDB_TYPE_DATE:
195: 		return TryCastCInternal<date_t, RESULT_TYPE, OP>(result, col, row);
196: 	case DUCKDB_TYPE_TIME:
197: 		return TryCastCInternal<dtime_t, RESULT_TYPE, OP>(result, col, row);
198: 	case DUCKDB_TYPE_TIMESTAMP:
199: 		return TryCastCInternal<timestamp_t, RESULT_TYPE, OP>(result, col, row);
200: 	case DUCKDB_TYPE_HUGEINT:
201: 		return TryCastCInternal<hugeint_t, RESULT_TYPE, OP>(result, col, row);
202: 	case DUCKDB_TYPE_DECIMAL:
203: 		return TryCastCInternal<hugeint_t, RESULT_TYPE, OP>(result, col, row);
204: 	case DUCKDB_TYPE_INTERVAL:
205: 		return TryCastCInternal<interval_t, RESULT_TYPE, OP>(result, col, row);
206: 	case DUCKDB_TYPE_VARCHAR:
207: 		return TryCastCInternal<char *, RESULT_TYPE, FromCStringCastWrapper<OP>>(result, col, row);
208: 	case DUCKDB_TYPE_BLOB:
209: 		return TryCastCInternal<duckdb_blob, RESULT_TYPE, FromCBlobCastWrapper>(result, col, row);
210: 	default: // LCOV_EXCL_START
211: 		// invalid type for C to C++ conversion
212: 		D_ASSERT(0);
213: 		return FetchDefaultValue::Operation<RESULT_TYPE>();
214: 	} // LCOV_EXCL_STOP
215: }
216: 
217: //===--------------------------------------------------------------------===//
218: // duckdb_value_ functions
219: //===--------------------------------------------------------------------===//
220: bool duckdb_value_boolean(duckdb_result *result, idx_t col, idx_t row) {
221: 	return GetInternalCValue<bool>(result, col, row);
222: }
223: 
224: int8_t duckdb_value_int8(duckdb_result *result, idx_t col, idx_t row) {
225: 	return GetInternalCValue<int8_t>(result, col, row);
226: }
227: 
228: int16_t duckdb_value_int16(duckdb_result *result, idx_t col, idx_t row) {
229: 	return GetInternalCValue<int16_t>(result, col, row);
230: }
231: 
232: int32_t duckdb_value_int32(duckdb_result *result, idx_t col, idx_t row) {
233: 	return GetInternalCValue<int32_t>(result, col, row);
234: }
235: 
236: int64_t duckdb_value_int64(duckdb_result *result, idx_t col, idx_t row) {
237: 	return GetInternalCValue<int64_t>(result, col, row);
238: }
239: 
240: duckdb_decimal duckdb_value_decimal(duckdb_result *result, idx_t col, idx_t row) {
241: 	duckdb_decimal result_value;
242: 
243: 	auto column_data = (duckdb::DuckDBColumnData *)result->__deprecated_columns[col].internal_data;
244: 	column_data->type.GetDecimalProperties(result_value.width, result_value.scale);
245: 
246: 	auto internal_value = GetInternalCValue<hugeint_t>(result, col, row);
247: 	result_value.value.lower = internal_value.lower;
248: 	result_value.value.upper = internal_value.upper;
249: 	return result_value;
250: }
251: 
252: duckdb_hugeint duckdb_value_hugeint(duckdb_result *result, idx_t col, idx_t row) {
253: 	duckdb_hugeint result_value;
254: 	auto internal_value = GetInternalCValue<hugeint_t>(result, col, row);
255: 	result_value.lower = internal_value.lower;
256: 	result_value.upper = internal_value.upper;
257: 	return result_value;
258: }
259: 
260: uint8_t duckdb_value_uint8(duckdb_result *result, idx_t col, idx_t row) {
261: 	return GetInternalCValue<uint8_t>(result, col, row);
262: }
263: 
264: uint16_t duckdb_value_uint16(duckdb_result *result, idx_t col, idx_t row) {
265: 	return GetInternalCValue<uint16_t>(result, col, row);
266: }
267: 
268: uint32_t duckdb_value_uint32(duckdb_result *result, idx_t col, idx_t row) {
269: 	return GetInternalCValue<uint32_t>(result, col, row);
270: }
271: 
272: uint64_t duckdb_value_uint64(duckdb_result *result, idx_t col, idx_t row) {
273: 	return GetInternalCValue<uint64_t>(result, col, row);
274: }
275: 
276: float duckdb_value_float(duckdb_result *result, idx_t col, idx_t row) {
277: 	return GetInternalCValue<float>(result, col, row);
278: }
279: 
280: double duckdb_value_double(duckdb_result *result, idx_t col, idx_t row) {
281: 	return GetInternalCValue<double>(result, col, row);
282: }
283: 
284: duckdb_date duckdb_value_date(duckdb_result *result, idx_t col, idx_t row) {
285: 	duckdb_date result_value;
286: 	result_value.days = GetInternalCValue<date_t>(result, col, row).days;
287: 	return result_value;
288: }
289: 
290: duckdb_time duckdb_value_time(duckdb_result *result, idx_t col, idx_t row) {
291: 	duckdb_time result_value;
292: 	result_value.micros = GetInternalCValue<dtime_t>(result, col, row).micros;
293: 	return result_value;
294: }
295: 
296: duckdb_timestamp duckdb_value_timestamp(duckdb_result *result, idx_t col, idx_t row) {
297: 	duckdb_timestamp result_value;
298: 	result_value.micros = GetInternalCValue<timestamp_t>(result, col, row).value;
299: 	return result_value;
300: }
301: 
302: duckdb_interval duckdb_value_interval(duckdb_result *result, idx_t col, idx_t row) {
303: 	duckdb_interval result_value;
304: 	auto ival = GetInternalCValue<interval_t>(result, col, row);
305: 	result_value.months = ival.months;
306: 	result_value.days = ival.days;
307: 	result_value.micros = ival.micros;
308: 	return result_value;
309: }
310: 
311: char *duckdb_value_varchar(duckdb_result *result, idx_t col, idx_t row) {
312: 	return GetInternalCValue<char *, ToCStringCastWrapper<duckdb::StringCast>>(result, col, row);
313: }
314: 
315: char *duckdb_value_varchar_internal(duckdb_result *result, idx_t col, idx_t row) {
316: 	if (!CanFetchValue(result, col, row)) {
317: 		return nullptr;
318: 	}
319: 	if (duckdb_column_type(result, col) != DUCKDB_TYPE_VARCHAR) {
320: 		return nullptr;
321: 	}
322: 	return UnsafeFetch<char *>(result, col, row);
323: }
324: 
325: duckdb_blob duckdb_value_blob(duckdb_result *result, idx_t col, idx_t row) {
326: 	if (CanFetchValue(result, col, row) && result->__deprecated_columns[col].__deprecated_type == DUCKDB_TYPE_BLOB) {
327: 		auto internal_result = UnsafeFetch<duckdb_blob>(result, col, row);
328: 
329: 		duckdb_blob result_blob;
330: 		result_blob.data = malloc(internal_result.size);
331: 		result_blob.size = internal_result.size;
332: 		memcpy(result_blob.data, internal_result.data, internal_result.size);
333: 		return result_blob;
334: 	}
335: 	return FetchDefaultValue::Operation<duckdb_blob>();
336: }
337: 
338: bool duckdb_value_is_null(duckdb_result *result, idx_t col, idx_t row) {
339: 	if (!result || col >= result->__deprecated_column_count || row >= result->__deprecated_row_count) {
340: 		return false;
341: 	}
342: 	return result->__deprecated_columns[col].__deprecated_nullmask[row];
343: }
[end of src/main/capi/value-c.cpp]
[start of src/planner/binder/statement/bind_export.cpp]
1: #include "duckdb/catalog/catalog.hpp"
2: #include "duckdb/parser/statement/export_statement.hpp"
3: #include "duckdb/planner/binder.hpp"
4: #include "duckdb/planner/operator/logical_export.hpp"
5: #include "duckdb/catalog/catalog_entry/copy_function_catalog_entry.hpp"
6: #include "duckdb/parser/statement/copy_statement.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb/main/database.hpp"
9: #include "duckdb/common/file_system.hpp"
10: #include "duckdb/planner/operator/logical_set_operation.hpp"
11: #include "duckdb/parser/parsed_data/exported_table_data.hpp"
12: #include "duckdb/parser/constraints/foreign_key_constraint.hpp"
13: 
14: #include "duckdb/common/string_util.hpp"
15: #include <algorithm>
16: 
17: namespace duckdb {
18: 
19: //! Sanitizes a string to have only low case chars and underscores
20: string SanitizeExportIdentifier(const string &str) {
21: 	// Copy the original string to result
22: 	string result(str);
23: 
24: 	for (idx_t i = 0; i < str.length(); ++i) {
25: 		auto c = str[i];
26: 		if (c >= 'a' && c <= 'z') {
27: 			// If it is lower case just continue
28: 			continue;
29: 		}
30: 
31: 		if (c >= 'A' && c <= 'Z') {
32: 			// To lowercase
33: 			result[i] = tolower(c);
34: 		} else {
35: 			// Substitute to underscore
36: 			result[i] = '_';
37: 		}
38: 	}
39: 
40: 	return result;
41: }
42: 
43: bool IsExistMainKeyTable(string &table_name, vector<TableCatalogEntry *> &unordered) {
44: 	for (idx_t i = 0; i < unordered.size(); i++) {
45: 		if (unordered[i]->name == table_name) {
46: 			return true;
47: 		}
48: 	}
49: 	return false;
50: }
51: 
52: void ScanForeignKeyTable(vector<TableCatalogEntry *> &ordered, vector<TableCatalogEntry *> &unordered,
53:                          bool move_only_pk_table) {
54: 	for (auto i = unordered.begin(); i != unordered.end();) {
55: 		auto table_entry = *i;
56: 		bool move_to_ordered = true;
57: 		for (idx_t j = 0; j < table_entry->constraints.size(); j++) {
58: 			auto &cond = table_entry->constraints[j];
59: 			if (cond->type == ConstraintType::FOREIGN_KEY) {
60: 				auto &fk = (ForeignKeyConstraint &)*cond;
61: 				if ((move_only_pk_table && fk.info.type == ForeignKeyType::FK_TYPE_FOREIGN_KEY_TABLE) ||
62: 				    (!move_only_pk_table && fk.info.type == ForeignKeyType::FK_TYPE_FOREIGN_KEY_TABLE &&
63: 				     IsExistMainKeyTable(fk.info.table, unordered))) {
64: 					move_to_ordered = false;
65: 					break;
66: 				}
67: 			}
68: 		}
69: 		if (move_to_ordered) {
70: 			ordered.push_back(table_entry);
71: 			i = unordered.erase(i);
72: 		} else {
73: 			i++;
74: 		}
75: 	}
76: }
77: 
78: void ReorderTableEntries(vector<TableCatalogEntry *> &tables) {
79: 	vector<TableCatalogEntry *> ordered;
80: 	vector<TableCatalogEntry *> unordered = tables;
81: 	ScanForeignKeyTable(ordered, unordered, true);
82: 	while (!unordered.empty()) {
83: 		ScanForeignKeyTable(ordered, unordered, false);
84: 	}
85: 	tables = ordered;
86: }
87: 
88: BoundStatement Binder::Bind(ExportStatement &stmt) {
89: 	// COPY TO a file
90: 	auto &config = DBConfig::GetConfig(context);
91: 	if (!config.enable_external_access) {
92: 		throw PermissionException("COPY TO is disabled through configuration");
93: 	}
94: 	BoundStatement result;
95: 	result.types = {LogicalType::BOOLEAN};
96: 	result.names = {"Success"};
97: 
98: 	// lookup the format in the catalog
99: 	auto &catalog = Catalog::GetCatalog(context);
100: 	auto copy_function = catalog.GetEntry<CopyFunctionCatalogEntry>(context, DEFAULT_SCHEMA, stmt.info->format);
101: 	if (!copy_function->function.copy_to_bind) {
102: 		throw NotImplementedException("COPY TO is not supported for FORMAT \"%s\"", stmt.info->format);
103: 	}
104: 
105: 	// gather a list of all the tables
106: 	vector<TableCatalogEntry *> tables;
107: 	auto schemas = catalog.schemas->GetEntries<SchemaCatalogEntry>(context);
108: 	for (auto &schema : schemas) {
109: 		schema->Scan(context, CatalogType::TABLE_ENTRY, [&](CatalogEntry *entry) {
110: 			if (entry->type == CatalogType::TABLE_ENTRY) {
111: 				tables.push_back((TableCatalogEntry *)entry);
112: 			}
113: 		});
114: 	}
115: 
116: 	// reorder tables because of foreign key constraint
117: 	ReorderTableEntries(tables);
118: 
119: 	// now generate the COPY statements for each of the tables
120: 	auto &fs = FileSystem::GetFileSystem(context);
121: 	unique_ptr<LogicalOperator> child_operator;
122: 
123: 	BoundExportData exported_tables;
124: 
125: 	idx_t id = 0; // Id for table
126: 	for (auto &table : tables) {
127: 		auto info = make_unique<CopyInfo>();
128: 		// we copy the options supplied to the EXPORT
129: 		info->format = stmt.info->format;
130: 		info->options = stmt.info->options;
131: 		// set up the file name for the COPY TO
132: 
133: 		auto exported_data = ExportedTableData();
134: 		if (table->schema->name == DEFAULT_SCHEMA) {
135: 			info->file_path =
136: 			    fs.JoinPath(stmt.info->file_path,
137: 			                StringUtil::Format("%s_%s.%s", to_string(id), SanitizeExportIdentifier(table->name),
138: 			                                   copy_function->function.extension));
139: 		} else {
140: 			info->file_path = fs.JoinPath(
141: 			    stmt.info->file_path,
142: 			    StringUtil::Format("%s_%s_%s.%s", SanitizeExportIdentifier(table->schema->name), to_string(id),
143: 			                       SanitizeExportIdentifier(table->name), copy_function->function.extension));
144: 		}
145: 		info->is_from = false;
146: 		info->schema = table->schema->name;
147: 		info->table = table->name;
148: 
149: 		exported_data.table_name = info->table;
150: 		exported_data.schema_name = info->schema;
151: 		exported_data.file_path = info->file_path;
152: 
153: 		ExportedTableInfo table_info;
154: 		table_info.entry = table;
155: 		table_info.table_data = exported_data;
156: 		exported_tables.data.push_back(table_info);
157: 		id++;
158: 
159: 		// generate the copy statement and bind it
160: 		CopyStatement copy_stmt;
161: 		copy_stmt.info = move(info);
162: 
163: 		auto copy_binder = Binder::CreateBinder(context);
164: 		auto bound_statement = copy_binder->Bind(copy_stmt);
165: 		if (child_operator) {
166: 			// use UNION ALL to combine the individual copy statements into a single node
167: 			auto copy_union =
168: 			    make_unique<LogicalSetOperation>(GenerateTableIndex(), 1, move(child_operator),
169: 			                                     move(bound_statement.plan), LogicalOperatorType::LOGICAL_UNION);
170: 			child_operator = move(copy_union);
171: 		} else {
172: 			child_operator = move(bound_statement.plan);
173: 		}
174: 	}
175: 
176: 	// try to create the directory, if it doesn't exist yet
177: 	// a bit hacky to do it here, but we need to create the directory BEFORE the copy statements run
178: 	if (!fs.DirectoryExists(stmt.info->file_path)) {
179: 		fs.CreateDirectory(stmt.info->file_path);
180: 	}
181: 
182: 	// create the export node
183: 	auto export_node = make_unique<LogicalExport>(copy_function->function, move(stmt.info), exported_tables);
184: 
185: 	if (child_operator) {
186: 		export_node->children.push_back(move(child_operator));
187: 	}
188: 
189: 	result.plan = move(export_node);
190: 	this->allow_stream_result = false;
191: 	return result;
192: }
193: 
194: } // namespace duckdb
[end of src/planner/binder/statement/bind_export.cpp]
[start of src/planner/binder/tableref/bind_basetableref.cpp]
1: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
2: #include "duckdb/parser/tableref/basetableref.hpp"
3: #include "duckdb/parser/tableref/subqueryref.hpp"
4: #include "duckdb/parser/query_node/select_node.hpp"
5: #include "duckdb/planner/binder.hpp"
6: #include "duckdb/planner/tableref/bound_basetableref.hpp"
7: #include "duckdb/planner/tableref/bound_subqueryref.hpp"
8: #include "duckdb/planner/tableref/bound_cteref.hpp"
9: #include "duckdb/planner/operator/logical_get.hpp"
10: #include "duckdb/parser/statement/select_statement.hpp"
11: #include "duckdb/function/table/table_scan.hpp"
12: #include "duckdb/common/string_util.hpp"
13: #include "duckdb/parser/tableref/table_function_ref.hpp"
14: #include "duckdb/main/config.hpp"
15: #include "duckdb/planner/tableref/bound_dummytableref.hpp"
16: 
17: namespace duckdb {
18: 
19: unique_ptr<BoundTableRef> Binder::Bind(BaseTableRef &ref) {
20: 	QueryErrorContext error_context(root_statement, ref.query_location);
21: 	// CTEs and views are also referred to using BaseTableRefs, hence need to distinguish here
22: 	// check if the table name refers to a CTE
23: 	auto cte = FindCTE(ref.table_name, ref.table_name == alias);
24: 	if (cte) {
25: 		// Check if there is a CTE binding in the BindContext
26: 		auto ctebinding = bind_context.GetCTEBinding(ref.table_name);
27: 		if (!ctebinding) {
28: 			if (CTEIsAlreadyBound(cte)) {
29: 				throw BinderException("Circular reference to CTE \"%s\", use WITH RECURSIVE to use recursive CTEs",
30: 				                      ref.table_name);
31: 			}
32: 			// Move CTE to subquery and bind recursively
33: 			SubqueryRef subquery(unique_ptr_cast<SQLStatement, SelectStatement>(cte->query->Copy()));
34: 			subquery.alias = ref.alias.empty() ? ref.table_name : ref.alias;
35: 			subquery.column_name_alias = cte->aliases;
36: 			for (idx_t i = 0; i < ref.column_name_alias.size(); i++) {
37: 				if (i < subquery.column_name_alias.size()) {
38: 					subquery.column_name_alias[i] = ref.column_name_alias[i];
39: 				} else {
40: 					subquery.column_name_alias.push_back(ref.column_name_alias[i]);
41: 				}
42: 			}
43: 			return Bind(subquery, cte);
44: 		} else {
45: 			// There is a CTE binding in the BindContext.
46: 			// This can only be the case if there is a recursive CTE present.
47: 			auto index = GenerateTableIndex();
48: 			auto result = make_unique<BoundCTERef>(index, ctebinding->index);
49: 			auto b = ctebinding;
50: 			auto alias = ref.alias.empty() ? ref.table_name : ref.alias;
51: 			auto names = BindContext::AliasColumnNames(alias, b->names, ref.column_name_alias);
52: 
53: 			bind_context.AddGenericBinding(index, alias, names, b->types);
54: 			// Update references to CTE
55: 			auto cteref = bind_context.cte_references[ref.table_name];
56: 			(*cteref)++;
57: 
58: 			result->types = b->types;
59: 			result->bound_columns = move(names);
60: 			return move(result);
61: 		}
62: 	}
63: 	// not a CTE
64: 	// extract a table or view from the catalog
65: 	auto table_or_view =
66: 	    Catalog::GetCatalog(context).GetEntry(context, CatalogType::TABLE_ENTRY, ref.schema_name, ref.table_name,
67: 	                                          ref.schema_name.empty() ? true : false, error_context);
68: 	if (!table_or_view) {
69: 		// table could not be found: try to bind a replacement scan
70: 		auto &config = DBConfig::GetConfig(context);
71: 		for (auto &scan : config.replacement_scans) {
72: 			auto replacement_function = scan.function(ref.table_name, scan.data);
73: 			if (replacement_function) {
74: 				replacement_function->alias = ref.alias.empty() ? ref.table_name : ref.alias;
75: 				replacement_function->column_name_alias = ref.column_name_alias;
76: 				return Bind(*replacement_function);
77: 			}
78: 		}
79: 		// we still didn't find the table
80: 		if (GetBindingMode() == BindingMode::EXTRACT_NAMES) {
81: 			// if we are in EXTRACT_NAMES, we create a dummy table ref
82: 			AddTableName(ref.table_name);
83: 
84: 			// add a bind context entry
85: 			auto table_index = GenerateTableIndex();
86: 			auto alias = ref.alias.empty() ? ref.table_name : ref.alias;
87: 			vector<LogicalType> types {LogicalType::INTEGER};
88: 			vector<string> names {"__dummy_col" + to_string(table_index)};
89: 			bind_context.AddGenericBinding(table_index, alias, names, types);
90: 			return make_unique_base<BoundTableRef, BoundEmptyTableRef>(table_index);
91: 		}
92: 		// could not find an alternative: bind again to get the error
93: 		table_or_view = Catalog::GetCatalog(context).GetEntry(context, CatalogType::TABLE_ENTRY, ref.schema_name,
94: 		                                                      ref.table_name, false, error_context);
95: 	}
96: 	switch (table_or_view->type) {
97: 	case CatalogType::TABLE_ENTRY: {
98: 		// base table: create the BoundBaseTableRef node
99: 		auto table_index = GenerateTableIndex();
100: 		auto table = (TableCatalogEntry *)table_or_view;
101: 
102: 		auto scan_function = TableScanFunction::GetFunction();
103: 		auto bind_data = make_unique<TableScanBindData>(table);
104: 		auto alias = ref.alias.empty() ? ref.table_name : ref.alias;
105: 		vector<LogicalType> table_types;
106: 		vector<string> table_names;
107: 		for (auto &col : table->columns) {
108: 			table_types.push_back(col.type);
109: 			table_names.push_back(col.name);
110: 		}
111: 		table_names = BindContext::AliasColumnNames(alias, table_names, ref.column_name_alias);
112: 
113: 		auto logical_get =
114: 		    make_unique<LogicalGet>(table_index, scan_function, move(bind_data), table_types, table_names);
115: 		bind_context.AddBaseTable(table_index, alias, table_names, table_types, *logical_get);
116: 		return make_unique_base<BoundTableRef, BoundBaseTableRef>(table, move(logical_get));
117: 	}
118: 	case CatalogType::VIEW_ENTRY: {
119: 		// the node is a view: get the query that the view represents
120: 		auto view_catalog_entry = (ViewCatalogEntry *)table_or_view;
121: 		// We need to use a new binder for the view that doesn't reference any CTEs
122: 		// defined for this binder so there are no collisions between the CTEs defined
123: 		// for the view and for the current query
124: 		bool inherit_ctes = false;
125: 		auto view_binder = Binder::CreateBinder(context, this, inherit_ctes);
126: 		view_binder->can_contain_nulls = true;
127: 		SubqueryRef subquery(unique_ptr_cast<SQLStatement, SelectStatement>(view_catalog_entry->query->Copy()));
128: 		subquery.alias = ref.alias.empty() ? ref.table_name : ref.alias;
129: 		subquery.column_name_alias =
130: 		    BindContext::AliasColumnNames(subquery.alias, view_catalog_entry->aliases, ref.column_name_alias);
131: 		// bind the child subquery
132: 		view_binder->AddBoundView(view_catalog_entry);
133: 		auto bound_child = view_binder->Bind(subquery);
134: 
135: 		D_ASSERT(bound_child->type == TableReferenceType::SUBQUERY);
136: 		// verify that the types and names match up with the expected types and names
137: 		auto &bound_subquery = (BoundSubqueryRef &)*bound_child;
138: 		if (bound_subquery.subquery->types != view_catalog_entry->types) {
139: 			throw BinderException("Contents of view were altered: types don't match!");
140: 		}
141: 		bind_context.AddSubquery(bound_subquery.subquery->GetRootIndex(), subquery.alias, subquery,
142: 		                         *bound_subquery.subquery);
143: 		return bound_child;
144: 	}
145: 	default:
146: 		throw InternalException("Catalog entry type");
147: 	}
148: }
149: } // namespace duckdb
[end of src/planner/binder/tableref/bind_basetableref.cpp]
[start of src/planner/binder/tableref/bind_table_function.cpp]
1: #include "duckdb/catalog/catalog.hpp"
2: #include "duckdb/parser/expression/function_expression.hpp"
3: #include "duckdb/parser/tableref/table_function_ref.hpp"
4: #include "duckdb/planner/binder.hpp"
5: #include "duckdb/parser/expression/columnref_expression.hpp"
6: #include "duckdb/parser/expression/comparison_expression.hpp"
7: #include "duckdb/planner/expression_binder/constant_binder.hpp"
8: #include "duckdb/planner/expression_binder/select_binder.hpp"
9: #include "duckdb/planner/operator/logical_get.hpp"
10: #include "duckdb/planner/tableref/bound_table_function.hpp"
11: #include "duckdb/planner/tableref/bound_subqueryref.hpp"
12: #include "duckdb/planner/query_node/bound_select_node.hpp"
13: #include "duckdb/execution/expression_executor.hpp"
14: #include "duckdb/common/algorithm.hpp"
15: #include "duckdb/parser/expression/subquery_expression.hpp"
16: 
17: namespace duckdb {
18: 
19: bool Binder::BindFunctionParameters(vector<unique_ptr<ParsedExpression>> &expressions, vector<LogicalType> &arguments,
20:                                     vector<Value> &parameters, named_parameter_map_t &named_parameters,
21:                                     unique_ptr<BoundSubqueryRef> &subquery, string &error) {
22: 	bool seen_subquery = false;
23: 	for (auto &child : expressions) {
24: 		string parameter_name;
25: 
26: 		// hack to make named parameters work
27: 		if (child->type == ExpressionType::COMPARE_EQUAL) {
28: 			// comparison, check if the LHS is a columnref
29: 			auto &comp = (ComparisonExpression &)*child;
30: 			if (comp.left->type == ExpressionType::COLUMN_REF) {
31: 				auto &colref = (ColumnRefExpression &)*comp.left;
32: 				if (!colref.IsQualified()) {
33: 					parameter_name = colref.GetColumnName();
34: 					child = move(comp.right);
35: 				}
36: 			}
37: 		}
38: 		if (child->type == ExpressionType::SUBQUERY) {
39: 			if (seen_subquery) {
40: 				error = "Table function can have at most one subquery parameter ";
41: 				return false;
42: 			}
43: 			auto binder = Binder::CreateBinder(this->context, this, true);
44: 			auto &se = (SubqueryExpression &)*child;
45: 			auto node = binder->BindNode(*se.subquery->node);
46: 			subquery = make_unique<BoundSubqueryRef>(move(binder), move(node));
47: 			seen_subquery = true;
48: 			arguments.emplace_back(LogicalTypeId::TABLE);
49: 			continue;
50: 		}
51: 		ConstantBinder binder(*this, context, "TABLE FUNCTION parameter");
52: 		LogicalType sql_type;
53: 		auto expr = binder.Bind(child, &sql_type);
54: 		if (!expr->IsFoldable()) {
55: 			error = "Table function requires a constant parameter";
56: 			return false;
57: 		}
58: 		auto constant = ExpressionExecutor::EvaluateScalar(*expr);
59: 		if (parameter_name.empty()) {
60: 			// unnamed parameter
61: 			if (!named_parameters.empty()) {
62: 				error = "Unnamed parameters cannot come after named parameters";
63: 				return false;
64: 			}
65: 			arguments.emplace_back(sql_type);
66: 			parameters.emplace_back(move(constant));
67: 		} else {
68: 			named_parameters[parameter_name] = move(constant);
69: 		}
70: 	}
71: 	return true;
72: }
73: 
74: unique_ptr<BoundTableRef> Binder::Bind(TableFunctionRef &ref) {
75: 	QueryErrorContext error_context(root_statement, ref.query_location);
76: 	auto bind_index = GenerateTableIndex();
77: 
78: 	D_ASSERT(ref.function->type == ExpressionType::FUNCTION);
79: 	auto fexpr = (FunctionExpression *)ref.function.get();
80: 
81: 	// evaluate the input parameters to the function
82: 	vector<LogicalType> arguments;
83: 	vector<Value> parameters;
84: 	named_parameter_map_t named_parameters;
85: 	unique_ptr<BoundSubqueryRef> subquery;
86: 	string error;
87: 	if (!BindFunctionParameters(fexpr->children, arguments, parameters, named_parameters, subquery, error)) {
88: 		throw BinderException(FormatError(ref, error));
89: 	}
90: 
91: 	// fetch the function from the catalog
92: 	auto &catalog = Catalog::GetCatalog(context);
93: 	auto function =
94: 	    catalog.GetEntry<TableFunctionCatalogEntry>(context, fexpr->schema, fexpr->function_name, false, error_context);
95: 
96: 	// select the function based on the input parameters
97: 	idx_t best_function_idx = Function::BindFunction(function->name, function->functions, arguments, error);
98: 	if (best_function_idx == DConstants::INVALID_INDEX) {
99: 		throw BinderException(FormatError(ref, error));
100: 	}
101: 	auto &table_function = function->functions[best_function_idx];
102: 
103: 	// now check the named parameters
104: 	BindNamedParameters(table_function.named_parameters, named_parameters, error_context, table_function.name);
105: 
106: 	// cast the parameters to the type of the function
107: 	for (idx_t i = 0; i < arguments.size(); i++) {
108: 		if (table_function.arguments[i] != LogicalType::ANY && table_function.arguments[i] != LogicalType::TABLE &&
109: 		    table_function.arguments[i] != LogicalType::POINTER &&
110: 		    table_function.arguments[i].id() != LogicalTypeId::LIST) {
111: 			parameters[i] = parameters[i].CastAs(table_function.arguments[i]);
112: 		}
113: 	}
114: 
115: 	vector<LogicalType> input_table_types;
116: 	vector<string> input_table_names;
117: 
118: 	if (subquery) {
119: 		input_table_types = subquery->subquery->types;
120: 		input_table_names = subquery->subquery->names;
121: 	}
122: 
123: 	// perform the binding
124: 	unique_ptr<FunctionData> bind_data;
125: 	vector<LogicalType> return_types;
126: 	vector<string> return_names;
127: 	if (table_function.bind) {
128: 		bind_data = table_function.bind(context, parameters, named_parameters, input_table_types, input_table_names,
129: 		                                return_types, return_names);
130: 	}
131: 	D_ASSERT(return_types.size() == return_names.size());
132: 	D_ASSERT(return_types.size() > 0);
133: 	// overwrite the names with any supplied aliases
134: 	for (idx_t i = 0; i < ref.column_name_alias.size() && i < return_names.size(); i++) {
135: 		return_names[i] = ref.column_name_alias[i];
136: 	}
137: 	for (idx_t i = 0; i < return_names.size(); i++) {
138: 		if (return_names[i].empty()) {
139: 			return_names[i] = "C" + to_string(i);
140: 		}
141: 	}
142: 	auto get = make_unique<LogicalGet>(bind_index, table_function, move(bind_data), return_types, return_names);
143: 	// now add the table function to the bind context so its columns can be bound
144: 	bind_context.AddTableFunction(bind_index, ref.alias.empty() ? fexpr->function_name : ref.alias, return_names,
145: 	                              return_types, *get);
146: 	if (subquery) {
147: 		get->children.push_back(Binder::CreatePlan(*subquery));
148: 	}
149: 
150: 	return make_unique_base<BoundTableRef, BoundTableFunction>(move(get));
151: }
152: 
153: } // namespace duckdb
[end of src/planner/binder/tableref/bind_table_function.cpp]
[start of tools/pythonpkg/src/include/duckdb_python/map.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb_python/pandas_scan.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb.hpp"
12: #include "duckdb_python/pybind_wrapper.hpp"
13: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
14: 
15: namespace duckdb {
16: 
17: struct MapFunction : public TableFunction {
18: 
19: public:
20: 	MapFunction();
21: 
22: 	static unique_ptr<FunctionData> MapFunctionBind(ClientContext &context, vector<Value> &inputs,
23: 	                                                named_parameter_map_t &named_parameters,
24: 	                                                vector<LogicalType> &input_table_types,
25: 	                                                vector<string> &input_table_names,
26: 	                                                vector<LogicalType> &return_types, vector<string> &names);
27: 
28: 	static void MapFunctionExec(ClientContext &context, const FunctionData *bind_data,
29: 	                            FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output);
30: };
31: 
32: } // namespace duckdb
[end of tools/pythonpkg/src/include/duckdb_python/map.hpp]
[start of tools/pythonpkg/src/include/duckdb_python/pandas_scan.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb_python/pandas_scan.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb.hpp"
12: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
13: 
14: namespace duckdb {
15: 
16: struct PandasScanFunction : public TableFunction {
17: public:
18: 	static constexpr idx_t PANDAS_PARTITION_COUNT = 50 * STANDARD_VECTOR_SIZE;
19: 
20: public:
21: 	PandasScanFunction();
22: 
23: 	static unique_ptr<FunctionData> PandasScanBind(ClientContext &context, vector<Value> &inputs,
24: 	                                               named_parameter_map_t &named_parameters,
25: 	                                               vector<LogicalType> &input_table_types,
26: 	                                               vector<string> &input_table_names, vector<LogicalType> &return_types,
27: 	                                               vector<string> &names);
28: 
29: 	static unique_ptr<FunctionOperatorData> PandasScanInit(ClientContext &context, const FunctionData *bind_data_p,
30: 	                                                       const vector<column_t> &column_ids,
31: 	                                                       TableFilterCollection *filters);
32: 
33: 	static idx_t PandasScanMaxThreads(ClientContext &context, const FunctionData *bind_data_p);
34: 
35: 	static unique_ptr<ParallelState> PandasScanInitParallelState(ClientContext &context,
36: 	                                                             const FunctionData *bind_data_p,
37: 	                                                             const vector<column_t> &column_ids,
38: 	                                                             TableFilterCollection *filters);
39: 
40: 	static unique_ptr<FunctionOperatorData>
41: 	PandasScanParallelInit(ClientContext &context, const FunctionData *bind_data_p, ParallelState *state,
42: 	                       const vector<column_t> &column_ids, TableFilterCollection *filters);
43: 
44: 	static bool PandasScanParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,
45: 	                                        FunctionOperatorData *operator_state, ParallelState *parallel_state_p);
46: 
47: 	static double PandasProgress(ClientContext &context, const FunctionData *bind_data_p);
48: 
49: 	//! The main pandas scan function: note that this can be called in parallel without the GIL
50: 	//! hence this needs to be GIL-safe, i.e. no methods that create Python objects are allowed
51: 	static void PandasScanFunc(ClientContext &context, const FunctionData *bind_data,
52: 	                           FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output);
53: 
54: 	static void PandasScanFuncParallel(ClientContext &context, const FunctionData *bind_data,
55: 	                                   FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output,
56: 	                                   ParallelState *parallel_state_p);
57: 
58: 	static unique_ptr<NodeStatistics> PandasScanCardinality(ClientContext &context, const FunctionData *bind_data);
59: };
60: 
61: } // namespace duckdb
[end of tools/pythonpkg/src/include/duckdb_python/pandas_scan.hpp]
[start of tools/pythonpkg/src/map.cpp]
1: #include "duckdb_python/map.hpp"
2: #include "duckdb_python/vector_conversion.hpp"
3: #include "duckdb_python/array_wrapper.hpp"
4: #include "duckdb/common/string_util.hpp"
5: 
6: namespace duckdb {
7: 
8: MapFunction::MapFunction()
9:     : TableFunction("python_map_function", {LogicalType::TABLE, LogicalType::POINTER}, MapFunctionExec,
10:                     MapFunctionBind) {
11: }
12: 
13: struct MapFunctionData : public TableFunctionData {
14: 	MapFunctionData() : function(nullptr) {
15: 	}
16: 	PyObject *function;
17: 	vector<LogicalType> in_types, out_types;
18: 	vector<string> in_names, out_names;
19: };
20: 
21: static py::handle FunctionCall(NumpyResultConversion &conversion, vector<string> &names, PyObject *function) {
22: 	py::dict in_numpy_dict;
23: 	for (idx_t col_idx = 0; col_idx < names.size(); col_idx++) {
24: 		in_numpy_dict[names[col_idx].c_str()] = conversion.ToArray(col_idx);
25: 	}
26: 	auto in_df = py::module::import("pandas").attr("DataFrame").attr("from_dict")(in_numpy_dict);
27: 	D_ASSERT(in_df.ptr());
28: 
29: 	D_ASSERT(function);
30: 	auto *df_obj = PyObject_CallObject(function, PyTuple_Pack(1, in_df.ptr()));
31: 	if (!df_obj) {
32: 		PyErr_PrintEx(1);
33: 		throw InvalidInputException("Python error. See above for a stack trace.");
34: 	}
35: 
36: 	py::handle df(df_obj);
37: 	if (df.is_none()) { // no return, probably modified in place
38: 		throw InvalidInputException("No return value from Python function");
39: 	}
40: 
41: 	return df;
42: }
43: 
44: // we call the passed function with a zero-row data frame to infer the output columns and their names.
45: // they better not change in the actual execution ^^
46: unique_ptr<FunctionData> MapFunction::MapFunctionBind(ClientContext &context, vector<Value> &inputs,
47:                                                       named_parameter_map_t &named_parameters,
48:                                                       vector<LogicalType> &input_table_types,
49:                                                       vector<string> &input_table_names,
50:                                                       vector<LogicalType> &return_types, vector<string> &names) {
51: 	py::gil_scoped_acquire acquire;
52: 
53: 	auto data_uptr = make_unique<MapFunctionData>();
54: 	auto &data = *data_uptr;
55: 	data.function = (PyObject *)inputs[0].GetPointer();
56: 	data.in_names = input_table_names;
57: 	data.in_types = input_table_types;
58: 
59: 	NumpyResultConversion conversion(data.in_types, 0);
60: 	auto df = FunctionCall(conversion, data.in_names, data.function);
61: 	vector<PandasColumnBindData> pandas_bind_data; // unused
62: 	VectorConversion::BindPandas(df, pandas_bind_data, return_types, names);
63: 
64: 	data.out_names = names;
65: 	data.out_types = return_types;
66: 	return move(data_uptr);
67: }
68: 
69: static string TypeVectorToString(vector<LogicalType> &types) {
70: 	return StringUtil::Join(types, types.size(), ", ", [](const LogicalType &argument) { return argument.ToString(); });
71: }
72: 
73: void MapFunction::MapFunctionExec(ClientContext &context, const FunctionData *bind_data,
74:                                   FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
75: 
76: 	py::gil_scoped_acquire acquire;
77: 
78: 	if (input->size() == 0) {
79: 		return;
80: 	}
81: 
82: 	auto &data = (MapFunctionData &)*bind_data;
83: 
84: 	D_ASSERT(input->GetTypes() == data.in_types);
85: 	NumpyResultConversion conversion(data.in_types, input->size());
86: 	conversion.Append(*input);
87: 
88: 	auto df = FunctionCall(conversion, data.in_names, data.function);
89: 
90: 	vector<PandasColumnBindData> pandas_bind_data;
91: 	vector<LogicalType> pandas_return_types;
92: 	vector<string> pandas_names;
93: 
94: 	VectorConversion::BindPandas(df, pandas_bind_data, pandas_return_types, pandas_names);
95: 	if (pandas_return_types.size() != output.ColumnCount()) {
96: 		throw InvalidInputException("Expected %llu columns from UDF, got %llu", output.ColumnCount(),
97: 		                            pandas_return_types.size());
98: 	}
99: 	D_ASSERT(output.GetTypes() == data.out_types);
100: 	if (pandas_return_types != output.GetTypes()) {
101: 		throw InvalidInputException("UDF column type mismatch, expected [%s], got [%s]",
102: 		                            TypeVectorToString(data.out_types), TypeVectorToString(pandas_return_types));
103: 	}
104: 	if (pandas_names != data.out_names) {
105: 		throw InvalidInputException("UDF column name mismatch, expected [%s], got [%s]",
106: 		                            StringUtil::Join(data.out_names, ", "), StringUtil::Join(pandas_names, ", "));
107: 	}
108: 
109: 	auto df_columns = py::list(df.attr("columns"));
110: 	auto get_fun = df.attr("__getitem__");
111: 
112: 	idx_t row_count = py::len(get_fun(df_columns[0]));
113: 	if (row_count > STANDARD_VECTOR_SIZE) {
114: 		throw InvalidInputException("UDF returned more than %llu rows, which is not allowed.", STANDARD_VECTOR_SIZE);
115: 	}
116: 
117: 	for (idx_t col_idx = 0; col_idx < output.ColumnCount(); col_idx++) {
118: 		VectorConversion::NumpyToDuckDB(pandas_bind_data[col_idx], pandas_bind_data[col_idx].numpy_col, row_count, 0,
119: 		                                output.data[col_idx]);
120: 	}
121: 	output.SetCardinality(row_count);
122: }
123: 
124: } // namespace duckdb
[end of tools/pythonpkg/src/map.cpp]
[start of tools/pythonpkg/src/pandas_scan.cpp]
1: #include "duckdb_python/pandas_scan.hpp"
2: #include "duckdb_python/array_wrapper.hpp"
3: #include "duckdb/parallel/parallel_state.hpp"
4: #include "utf8proc_wrapper.hpp"
5: #include "duckdb/common/types/timestamp.hpp"
6: #include "duckdb_python/vector_conversion.hpp"
7: #include "duckdb/main/client_context.hpp"
8: 
9: #include "duckdb/common/atomic.hpp"
10: 
11: namespace duckdb {
12: 
13: struct PandasScanFunctionData : public TableFunctionData {
14: 	PandasScanFunctionData(py::handle df, idx_t row_count, vector<PandasColumnBindData> pandas_bind_data,
15: 	                       vector<LogicalType> sql_types)
16: 	    : df(df), row_count(row_count), lines_read(0), pandas_bind_data(move(pandas_bind_data)),
17: 	      sql_types(move(sql_types)) {
18: 	}
19: 	py::handle df;
20: 	idx_t row_count;
21: 	atomic<idx_t> lines_read;
22: 	vector<PandasColumnBindData> pandas_bind_data;
23: 	vector<LogicalType> sql_types;
24: 
25: 	~PandasScanFunctionData() override {
26: 		py::gil_scoped_acquire acquire;
27: 		pandas_bind_data.clear();
28: 	}
29: };
30: 
31: struct PandasScanState : public FunctionOperatorData {
32: 	PandasScanState(idx_t start, idx_t end) : start(start), end(end) {
33: 	}
34: 
35: 	idx_t start;
36: 	idx_t end;
37: 	vector<column_t> column_ids;
38: };
39: 
40: struct ParallelPandasScanState : public ParallelState {
41: 	ParallelPandasScanState() : position(0) {
42: 	}
43: 
44: 	std::mutex lock;
45: 	idx_t position;
46: };
47: 
48: PandasScanFunction::PandasScanFunction()
49:     : TableFunction("pandas_scan", {LogicalType::POINTER}, PandasScanFunc, PandasScanBind, PandasScanInit, nullptr,
50:                     nullptr, nullptr, PandasScanCardinality, nullptr, nullptr, PandasScanMaxThreads,
51:                     PandasScanInitParallelState, PandasScanFuncParallel, PandasScanParallelInit,
52:                     PandasScanParallelStateNext, true, false, PandasProgress) {
53: }
54: 
55: unique_ptr<FunctionData> PandasScanFunction::PandasScanBind(ClientContext &context, vector<Value> &inputs,
56:                                                             named_parameter_map_t &named_parameters,
57:                                                             vector<LogicalType> &input_table_types,
58:                                                             vector<string> &input_table_names,
59:                                                             vector<LogicalType> &return_types, vector<string> &names) {
60: 	py::gil_scoped_acquire acquire;
61: 	py::handle df((PyObject *)(inputs[0].GetPointer()));
62: 
63: 	vector<PandasColumnBindData> pandas_bind_data;
64: 	VectorConversion::BindPandas(df, pandas_bind_data, return_types, names);
65: 
66: 	auto df_columns = py::list(df.attr("columns"));
67: 	auto get_fun = df.attr("__getitem__");
68: 
69: 	idx_t row_count = py::len(get_fun(df_columns[0]));
70: 	return make_unique<PandasScanFunctionData>(df, row_count, move(pandas_bind_data), return_types);
71: }
72: 
73: unique_ptr<FunctionOperatorData> PandasScanFunction::PandasScanInit(ClientContext &context,
74:                                                                     const FunctionData *bind_data_p,
75:                                                                     const vector<column_t> &column_ids,
76:                                                                     TableFilterCollection *filters) {
77: 	auto &bind_data = (const PandasScanFunctionData &)*bind_data_p;
78: 	auto result = make_unique<PandasScanState>(0, bind_data.row_count);
79: 	result->column_ids = column_ids;
80: 	return move(result);
81: }
82: 
83: idx_t PandasScanFunction::PandasScanMaxThreads(ClientContext &context, const FunctionData *bind_data_p) {
84: 	if (ClientConfig::GetConfig(context).verify_parallelism) {
85: 		return context.db->NumberOfThreads();
86: 	}
87: 	auto &bind_data = (const PandasScanFunctionData &)*bind_data_p;
88: 	return bind_data.row_count / PANDAS_PARTITION_COUNT + 1;
89: }
90: 
91: unique_ptr<ParallelState> PandasScanFunction::PandasScanInitParallelState(ClientContext &context,
92:                                                                           const FunctionData *bind_data_p,
93:                                                                           const vector<column_t> &column_ids,
94:                                                                           TableFilterCollection *filters) {
95: 	return make_unique<ParallelPandasScanState>();
96: }
97: 
98: unique_ptr<FunctionOperatorData> PandasScanFunction::PandasScanParallelInit(ClientContext &context,
99:                                                                             const FunctionData *bind_data_p,
100:                                                                             ParallelState *state,
101:                                                                             const vector<column_t> &column_ids,
102:                                                                             TableFilterCollection *filters) {
103: 	auto result = make_unique<PandasScanState>(0, 0);
104: 	result->column_ids = column_ids;
105: 	if (!PandasScanParallelStateNext(context, bind_data_p, result.get(), state)) {
106: 		return nullptr;
107: 	}
108: 	return move(result);
109: }
110: 
111: bool PandasScanFunction::PandasScanParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,
112:                                                      FunctionOperatorData *operator_state,
113:                                                      ParallelState *parallel_state_p) {
114: 	auto &bind_data = (const PandasScanFunctionData &)*bind_data_p;
115: 	auto &parallel_state = (ParallelPandasScanState &)*parallel_state_p;
116: 	auto &state = (PandasScanState &)*operator_state;
117: 
118: 	lock_guard<mutex> parallel_lock(parallel_state.lock);
119: 	if (parallel_state.position >= bind_data.row_count) {
120: 		return false;
121: 	}
122: 	state.start = parallel_state.position;
123: 	parallel_state.position += PANDAS_PARTITION_COUNT;
124: 	if (parallel_state.position > bind_data.row_count) {
125: 		parallel_state.position = bind_data.row_count;
126: 	}
127: 	state.end = parallel_state.position;
128: 	return true;
129: }
130: 
131: double PandasScanFunction::PandasProgress(ClientContext &context, const FunctionData *bind_data_p) {
132: 	auto &bind_data = (const PandasScanFunctionData &)*bind_data_p;
133: 	if (bind_data.row_count == 0) {
134: 		return 100;
135: 	}
136: 	auto percentage = (bind_data.lines_read * 100.0) / bind_data.row_count;
137: 	return percentage;
138: }
139: 
140: void PandasScanFunction::PandasScanFuncParallel(ClientContext &context, const FunctionData *bind_data,
141:                                                 FunctionOperatorData *operator_state, DataChunk *input,
142:                                                 DataChunk &output, ParallelState *parallel_state_p) {
143: 	//! FIXME: Have specialized parallel function from pandas scan here
144: 	PandasScanFunc(context, bind_data, operator_state, input, output);
145: }
146: 
147: //! The main pandas scan function: note that this can be called in parallel without the GIL
148: //! hence this needs to be GIL-safe, i.e. no methods that create Python objects are allowed
149: void PandasScanFunction::PandasScanFunc(ClientContext &context, const FunctionData *bind_data,
150:                                         FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
151: 	if (!operator_state) {
152: 		return;
153: 	}
154: 	auto &data = (PandasScanFunctionData &)*bind_data;
155: 	auto &state = (PandasScanState &)*operator_state;
156: 
157: 	if (state.start >= state.end) {
158: 		return;
159: 	}
160: 	idx_t this_count = std::min((idx_t)STANDARD_VECTOR_SIZE, state.end - state.start);
161: 	output.SetCardinality(this_count);
162: 	for (idx_t idx = 0; idx < state.column_ids.size(); idx++) {
163: 		auto col_idx = state.column_ids[idx];
164: 		if (col_idx == COLUMN_IDENTIFIER_ROW_ID) {
165: 			output.data[idx].Sequence(state.start, this_count);
166: 		} else {
167: 			VectorConversion::NumpyToDuckDB(data.pandas_bind_data[col_idx], data.pandas_bind_data[col_idx].numpy_col,
168: 			                                this_count, state.start, output.data[idx]);
169: 		}
170: 	}
171: 	state.start += this_count;
172: 	data.lines_read += this_count;
173: }
174: 
175: unique_ptr<NodeStatistics> PandasScanFunction::PandasScanCardinality(ClientContext &context,
176:                                                                      const FunctionData *bind_data) {
177: 	auto &data = (PandasScanFunctionData &)*bind_data;
178: 	return make_unique<NodeStatistics>(data.row_count, data.row_count);
179: }
180: 
181: } // namespace duckdb
[end of tools/pythonpkg/src/pandas_scan.cpp]
[start of tools/pythonpkg/src/pyconnection.cpp]
1: #include "duckdb_python/pyconnection.hpp"
2: #include "duckdb_python/pyresult.hpp"
3: #include "duckdb_python/pyrelation.hpp"
4: #include "duckdb_python/pandas_scan.hpp"
5: #include "duckdb_python/map.hpp"
6: 
7: #include "duckdb/common/arrow.hpp"
8: #include "duckdb_python/arrow_array_stream.hpp"
9: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
10: #include "duckdb/main/client_context.hpp"
11: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
12: #include "duckdb/common/types/vector.hpp"
13: #include "duckdb/common/types.hpp"
14: #include "duckdb/common/printer.hpp"
15: #include "duckdb/main/config.hpp"
16: #include "duckdb/parser/expression/constant_expression.hpp"
17: #include "duckdb/parser/expression/function_expression.hpp"
18: #include "duckdb/parser/tableref/table_function_ref.hpp"
19: #include "duckdb/parser/parser.hpp"
20: 
21: #include "datetime.h" // from Python
22: 
23: #include <random>
24: 
25: namespace duckdb {
26: 
27: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::default_connection = nullptr;
28: 
29: void DuckDBPyConnection::Initialize(py::handle &m) {
30: 	py::class_<DuckDBPyConnection, shared_ptr<DuckDBPyConnection>>(m, "DuckDBPyConnection", py::module_local())
31: 	    .def("cursor", &DuckDBPyConnection::Cursor, "Create a duplicate of the current connection")
32: 	    .def("duplicate", &DuckDBPyConnection::Cursor, "Create a duplicate of the current connection")
33: 	    .def("execute", &DuckDBPyConnection::Execute,
34: 	         "Execute the given SQL query, optionally using prepared statements with parameters set", py::arg("query"),
35: 	         py::arg("parameters") = py::list(), py::arg("multiple_parameter_sets") = false)
36: 	    .def("executemany", &DuckDBPyConnection::ExecuteMany,
37: 	         "Execute the given prepared statement multiple times using the list of parameter sets in parameters",
38: 	         py::arg("query"), py::arg("parameters") = py::list())
39: 	    .def("close", &DuckDBPyConnection::Close, "Close the connection")
40: 	    .def("fetchone", &DuckDBPyConnection::FetchOne, "Fetch a single row from a result following execute")
41: 	    .def("fetchall", &DuckDBPyConnection::FetchAll, "Fetch all rows from a result following execute")
42: 	    .def("fetchnumpy", &DuckDBPyConnection::FetchNumpy, "Fetch a result as list of NumPy arrays following execute")
43: 	    .def("fetchdf", &DuckDBPyConnection::FetchDF, "Fetch a result as Data.Frame following execute()")
44: 	    .def("fetch_df", &DuckDBPyConnection::FetchDF, "Fetch a result as Data.Frame following execute()")
45: 	    .def("fetch_df_chunk", &DuckDBPyConnection::FetchDFChunk,
46: 	         "Fetch a chunk of the result as Data.Frame following execute()", py::arg("vectors_per_chunk") = 1)
47: 	    .def("df", &DuckDBPyConnection::FetchDF, "Fetch a result as Data.Frame following execute()")
48: 	    .def("fetch_arrow_table", &DuckDBPyConnection::FetchArrow, "Fetch a result as Arrow table following execute()",
49: 	         py::arg("chunk_size") = 1000000)
50: 	    .def("fetch_record_batch", &DuckDBPyConnection::FetchRecordBatchReader,
51: 	         "Fetch an Arrow RecordBatchReader following execute()", py::arg("chunk_size") = 1000000)
52: 	    .def("arrow", &DuckDBPyConnection::FetchArrow, "Fetch a result as Arrow table following execute()",
53: 	         py::arg("chunk_size") = 1000000)
54: 	    .def("begin", &DuckDBPyConnection::Begin, "Start a new transaction")
55: 	    .def("commit", &DuckDBPyConnection::Commit, "Commit changes performed within a transaction")
56: 	    .def("rollback", &DuckDBPyConnection::Rollback, "Roll back changes performed within a transaction")
57: 	    .def("append", &DuckDBPyConnection::Append, "Append the passed Data.Frame to the named table",
58: 	         py::arg("table_name"), py::arg("df"))
59: 	    .def("register", &DuckDBPyConnection::RegisterPythonObject,
60: 	         "Register the passed Python Object value for querying with a view", py::arg("view_name"),
61: 	         py::arg("python_object"), py::arg("rows_per_thread") = 1000000)
62: 	    .def("unregister", &DuckDBPyConnection::UnregisterPythonObject, "Unregister the view name",
63: 	         py::arg("view_name"))
64: 	    .def("table", &DuckDBPyConnection::Table, "Create a relation object for the name'd table",
65: 	         py::arg("table_name"))
66: 	    .def("view", &DuckDBPyConnection::View, "Create a relation object for the name'd view", py::arg("view_name"))
67: 	    .def("values", &DuckDBPyConnection::Values, "Create a relation object from the passed values",
68: 	         py::arg("values"))
69: 	    .def("table_function", &DuckDBPyConnection::TableFunction,
70: 	         "Create a relation object from the name'd table function with given parameters", py::arg("name"),
71: 	         py::arg("parameters") = py::list())
72: 	    .def("from_query", &DuckDBPyConnection::FromQuery, "Create a relation object from the given SQL query",
73: 	         py::arg("query"), py::arg("alias") = "query_relation")
74: 	    .def("query", &DuckDBPyConnection::RunQuery,
75: 	         "Run a SQL query. If it is a SELECT statement, create a relation object from the given SQL query, "
76: 	         "otherwise run the query as-is.",
77: 	         py::arg("query"), py::arg("alias") = "query_relation")
78: 	    .def("from_df", &DuckDBPyConnection::FromDF, "Create a relation object from the Data.Frame in df",
79: 	         py::arg("df") = py::none())
80: 	    .def("from_arrow_table", &DuckDBPyConnection::FromArrowTable, "Create a relation object from an Arrow table",
81: 	         py::arg("table"), py::arg("rows_per_thread") = 1000000)
82: 	    .def("df", &DuckDBPyConnection::FromDF, "Create a relation object from the Data.Frame in df (alias of from_df)",
83: 	         py::arg("df"))
84: 	    .def("from_csv_auto", &DuckDBPyConnection::FromCsvAuto,
85: 	         "Create a relation object from the CSV file in file_name", py::arg("file_name"))
86: 	    .def("from_parquet", &DuckDBPyConnection::FromParquet,
87: 	         "Create a relation object from the Parquet file in file_name", py::arg("file_name"),
88: 	         py::arg("binary_as_string") = false)
89: 	    .def_property_readonly("description", &DuckDBPyConnection::GetDescription,
90: 	                           "Get result set attributes, mainly column names");
91: 
92: 	PyDateTime_IMPORT;
93: }
94: 
95: DuckDBPyConnection *DuckDBPyConnection::ExecuteMany(const string &query, py::object params) {
96: 	Execute(query, std::move(params), true);
97: 	return this;
98: }
99: 
100: DuckDBPyConnection *DuckDBPyConnection::Execute(const string &query, py::object params, bool many) {
101: 	if (!connection) {
102: 		throw std::runtime_error("connection closed");
103: 	}
104: 	if (std::this_thread::get_id() != thread_id && check_same_thread) {
105: 		throw std::runtime_error("DuckDB objects created in a thread can only be used in that same thread. The object "
106: 		                         "was created in thread id " +
107: 		                         to_string(std::hash<std::thread::id> {}(thread_id)) + " and this is thread id " +
108: 		                         to_string(std::hash<std::thread::id> {}(std::this_thread::get_id())));
109: 	}
110: 	result = nullptr;
111: 	unique_ptr<PreparedStatement> prep;
112: 	{
113: 		py::gil_scoped_release release;
114: 		auto statements = connection->ExtractStatements(query);
115: 		if (statements.empty()) {
116: 			// no statements to execute
117: 			return this;
118: 		}
119: 		// if there are multiple statements, we directly execute the statements besides the last one
120: 		// we only return the result of the last statement to the user, unless one of the previous statements fails
121: 		for (idx_t i = 0; i + 1 < statements.size(); i++) {
122: 			auto res = connection->Query(move(statements[i]));
123: 			if (!res->success) {
124: 				throw std::runtime_error(res->error);
125: 			}
126: 		}
127: 
128: 		prep = connection->Prepare(move(statements.back()));
129: 		if (!prep->success) {
130: 			throw std::runtime_error(prep->error);
131: 		}
132: 	}
133: 
134: 	// this is a list of a list of parameters in executemany
135: 	py::list params_set;
136: 	if (!many) {
137: 		params_set = py::list(1);
138: 		params_set[0] = params;
139: 	} else {
140: 		params_set = params;
141: 	}
142: 
143: 	for (pybind11::handle single_query_params : params_set) {
144: 		if (prep->n_param != py::len(single_query_params)) {
145: 			throw std::runtime_error("Prepared statement needs " + to_string(prep->n_param) + " parameters, " +
146: 			                         to_string(py::len(single_query_params)) + " given");
147: 		}
148: 		auto args = DuckDBPyConnection::TransformPythonParamList(single_query_params);
149: 		auto res = make_unique<DuckDBPyResult>();
150: 		{
151: 			py::gil_scoped_release release;
152: 			res->result = prep->Execute(args);
153: 			if (!res->result->success) {
154: 				throw std::runtime_error(res->result->error);
155: 			}
156: 		}
157: 
158: 		if (!many) {
159: 			result = move(res);
160: 		}
161: 	}
162: 	return this;
163: }
164: 
165: DuckDBPyConnection *DuckDBPyConnection::Append(const string &name, py::object value) {
166: 	RegisterPythonObject("__append_df", std::move(value));
167: 	return Execute("INSERT INTO \"" + name + "\" SELECT * FROM __append_df");
168: }
169: 
170: DuckDBPyConnection *DuckDBPyConnection::RegisterPythonObject(const string &name, py::object python_object,
171:                                                              const idx_t rows_per_tuple) {
172: 	if (!connection) {
173: 		throw std::runtime_error("connection closed");
174: 	}
175: 	auto py_object_type = string(py::str(python_object.get_type().attr("__name__")));
176: 
177: 	if (py_object_type == "DataFrame") {
178: 		{
179: 			py::gil_scoped_release release;
180: 			connection->TableFunction("pandas_scan", {Value::POINTER((uintptr_t)python_object.ptr())})
181: 			    ->CreateView(name, true, true);
182: 		}
183: 
184: 		// keep a reference
185: 		auto object = make_unique<RegisteredObject>(python_object);
186: 		registered_objects[name] = move(object);
187: 	} else if (py_object_type == "Table" || py_object_type == "FileSystemDataset" ||
188: 	           py_object_type == "InMemoryDataset") {
189: 		auto stream_factory = make_unique<PythonTableArrowArrayStreamFactory>(python_object.ptr());
190: 
191: 		auto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;
192: 		{
193: 			py::gil_scoped_release release;
194: 			connection
195: 			    ->TableFunction("arrow_scan",
196: 			                    {Value::POINTER((uintptr_t)stream_factory.get()),
197: 			                     Value::POINTER((uintptr_t)stream_factory_produce), Value::UBIGINT(rows_per_tuple)})
198: 			    ->CreateView(name, true, true);
199: 		}
200: 		auto object = make_unique<RegisteredArrow>(move(stream_factory), move(python_object));
201: 		registered_objects[name] = move(object);
202: 	} else {
203: 		throw std::runtime_error("Python Object " + py_object_type + " not suitable to be registered as a view");
204: 	}
205: 	return this;
206: }
207: 
208: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromQuery(const string &query, const string &alias) {
209: 	if (!connection) {
210: 		throw std::runtime_error("connection closed");
211: 	}
212: 	const char *duckdb_query_error = R"(duckdb.from_query cannot be used to run arbitrary SQL queries.
213: It can only be used to run individual SELECT statements, and converts the result of that SELECT
214: statement into a Relation object.
215: Use duckdb.query to run arbitrary SQL queries.)";
216: 	return make_unique<DuckDBPyRelation>(connection->RelationFromQuery(query, alias, duckdb_query_error));
217: }
218: 
219: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::RunQuery(const string &query, const string &alias) {
220: 	if (!connection) {
221: 		throw std::runtime_error("connection closed");
222: 	}
223: 	Parser parser(connection->context->GetParserOptions());
224: 	parser.ParseQuery(query);
225: 	if (parser.statements.size() == 1 && parser.statements[0]->type == StatementType::SELECT_STATEMENT) {
226: 		return make_unique<DuckDBPyRelation>(connection->RelationFromQuery(
227: 		    unique_ptr_cast<SQLStatement, SelectStatement>(move(parser.statements[0])), alias));
228: 	}
229: 	Execute(query);
230: 	if (result) {
231: 		FetchAll();
232: 	}
233: 	return nullptr;
234: }
235: 
236: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::Table(const string &tname) {
237: 	if (!connection) {
238: 		throw std::runtime_error("connection closed");
239: 	}
240: 	return make_unique<DuckDBPyRelation>(connection->Table(tname));
241: }
242: 
243: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::Values(py::object params) {
244: 	if (!connection) {
245: 		throw std::runtime_error("connection closed");
246: 	}
247: 	vector<vector<Value>> values {DuckDBPyConnection::TransformPythonParamList(std::move(params))};
248: 	return make_unique<DuckDBPyRelation>(connection->Values(values));
249: }
250: 
251: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::View(const string &vname) {
252: 	if (!connection) {
253: 		throw std::runtime_error("connection closed");
254: 	}
255: 	return make_unique<DuckDBPyRelation>(connection->View(vname));
256: }
257: 
258: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::TableFunction(const string &fname, py::object params) {
259: 	if (!connection) {
260: 		throw std::runtime_error("connection closed");
261: 	}
262: 
263: 	return make_unique<DuckDBPyRelation>(
264: 	    connection->TableFunction(fname, DuckDBPyConnection::TransformPythonParamList(std::move(params))));
265: }
266: 
267: static std::string GenerateRandomName() {
268: 	std::random_device rd;
269: 	std::mt19937 gen(rd());
270: 	std::uniform_int_distribution<> dis(0, 15);
271: 
272: 	std::stringstream ss;
273: 	int i;
274: 	ss << std::hex;
275: 	for (i = 0; i < 16; i++) {
276: 		ss << dis(gen);
277: 	}
278: 	return ss.str();
279: }
280: 
281: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromDF(py::object value) {
282: 	if (!connection) {
283: 		throw std::runtime_error("connection closed");
284: 	}
285: 	string name = "df_" + GenerateRandomName();
286: 	registered_objects[name] = make_unique<RegisteredObject>(value);
287: 	vector<Value> params;
288: 	params.emplace_back(Value::POINTER((uintptr_t)value.ptr()));
289: 	return make_unique<DuckDBPyRelation>(connection->TableFunction("pandas_scan", params)->Alias(name));
290: }
291: 
292: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromCsvAuto(const string &filename) {
293: 	if (!connection) {
294: 		throw std::runtime_error("connection closed");
295: 	}
296: 	vector<Value> params;
297: 	params.emplace_back(filename);
298: 	return make_unique<DuckDBPyRelation>(connection->TableFunction("read_csv_auto", params)->Alias(filename));
299: }
300: 
301: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquet(const string &filename, bool binary_as_string) {
302: 	if (!connection) {
303: 		throw std::runtime_error("connection closed");
304: 	}
305: 	vector<Value> params;
306: 	params.emplace_back(filename);
307: 	named_parameter_map_t named_parameters({{"binary_as_string", Value::BOOLEAN(binary_as_string)}});
308: 	return make_unique<DuckDBPyRelation>(
309: 	    connection->TableFunction("parquet_scan", params, named_parameters)->Alias(filename));
310: }
311: 
312: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromArrowTable(py::object &table, const idx_t rows_per_tuple) {
313: 	if (!connection) {
314: 		throw std::runtime_error("connection closed");
315: 	}
316: 	py::gil_scoped_acquire acquire;
317: 	string name = "arrow_table_" + GenerateRandomName();
318: 
319: 	auto stream_factory = make_unique<PythonTableArrowArrayStreamFactory>(table.ptr());
320: 
321: 	auto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;
322: 	auto rel = make_unique<DuckDBPyRelation>(
323: 	    connection
324: 	        ->TableFunction("arrow_scan",
325: 	                        {Value::POINTER((uintptr_t)stream_factory.get()),
326: 	                         Value::POINTER((uintptr_t)stream_factory_produce), Value::UBIGINT(rows_per_tuple)})
327: 	        ->Alias(name));
328: 	registered_objects[name] = make_unique<RegisteredArrow>(move(stream_factory), table);
329: 	return rel;
330: }
331: 
332: DuckDBPyConnection *DuckDBPyConnection::UnregisterPythonObject(const string &name) {
333: 	registered_objects.erase(name);
334: 	py::gil_scoped_release release;
335: 	if (connection) {
336: 		connection->Query("DROP VIEW \"" + name + "\"");
337: 	}
338: 	return this;
339: }
340: 
341: DuckDBPyConnection *DuckDBPyConnection::Begin() {
342: 	Execute("BEGIN TRANSACTION");
343: 	return this;
344: }
345: 
346: DuckDBPyConnection *DuckDBPyConnection::Commit() {
347: 	if (connection->context->transaction.IsAutoCommit()) {
348: 		return this;
349: 	}
350: 	Execute("COMMIT");
351: 	return this;
352: }
353: 
354: DuckDBPyConnection *DuckDBPyConnection::Rollback() {
355: 	Execute("ROLLBACK");
356: 	return this;
357: }
358: 
359: py::object DuckDBPyConnection::GetDescription() {
360: 	if (!result) {
361: 		return py::none();
362: 	}
363: 	return result->Description();
364: }
365: 
366: void DuckDBPyConnection::Close() {
367: 	result = nullptr;
368: 	connection = nullptr;
369: 	database = nullptr;
370: 	for (auto &cur : cursors) {
371: 		cur->Close();
372: 	}
373: 	cursors.clear();
374: }
375: 
376: // cursor() is stupid
377: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Cursor() {
378: 	auto res = make_shared<DuckDBPyConnection>(thread_id);
379: 	res->database = database;
380: 	res->connection = connection;
381: 	cursors.push_back(res);
382: 	return res;
383: }
384: 
385: // these should be functions on the result but well
386: py::object DuckDBPyConnection::FetchOne() {
387: 	if (!result) {
388: 		throw std::runtime_error("no open result set");
389: 	}
390: 	return result->Fetchone();
391: }
392: 
393: py::list DuckDBPyConnection::FetchAll() {
394: 	if (!result) {
395: 		throw std::runtime_error("no open result set");
396: 	}
397: 	return result->Fetchall();
398: }
399: 
400: py::dict DuckDBPyConnection::FetchNumpy() {
401: 	if (!result) {
402: 		throw std::runtime_error("no open result set");
403: 	}
404: 	return result->FetchNumpyInternal();
405: }
406: py::object DuckDBPyConnection::FetchDF() {
407: 	if (!result) {
408: 		throw std::runtime_error("no open result set");
409: 	}
410: 	return result->FetchDF();
411: }
412: 
413: py::object DuckDBPyConnection::FetchDFChunk(const idx_t vectors_per_chunk) const {
414: 	if (!result) {
415: 		throw std::runtime_error("no open result set");
416: 	}
417: 	return result->FetchDFChunk(vectors_per_chunk);
418: }
419: 
420: py::object DuckDBPyConnection::FetchArrow(idx_t chunk_size) {
421: 	if (!result) {
422: 		throw std::runtime_error("no open result set");
423: 	}
424: 	return result->FetchArrowTable(chunk_size);
425: }
426: 
427: py::object DuckDBPyConnection::FetchRecordBatchReader(const idx_t chunk_size) const {
428: 	if (!result) {
429: 		throw std::runtime_error("no open result set");
430: 	}
431: 	return result->FetchRecordBatchReader(chunk_size);
432: }
433: static unique_ptr<TableFunctionRef>
434: TryReplacement(py::dict &dict, py::str &table_name,
435:                unordered_map<string, unique_ptr<RegisteredObject>> &registered_objects) {
436: 	if (!dict.contains(table_name)) {
437: 		// not present in the globals
438: 		return nullptr;
439: 	}
440: 	auto entry = dict[table_name];
441: 	auto py_object_type = string(py::str(entry.get_type().attr("__name__")));
442: 	auto table_function = make_unique<TableFunctionRef>();
443: 	vector<unique_ptr<ParsedExpression>> children;
444: 	if (py_object_type == "DataFrame") {
445: 		string name = "df_" + GenerateRandomName();
446: 		children.push_back(make_unique<ConstantExpression>(Value::POINTER((uintptr_t)entry.ptr())));
447: 		table_function->function = make_unique<FunctionExpression>("pandas_scan", move(children));
448: 		// keep a reference
449: 		auto object = make_unique<RegisteredObject>(entry);
450: 		registered_objects[name] = move(object);
451: 	} else if (py_object_type == "Table" || py_object_type == "FileSystemDataset" ||
452: 	           py_object_type == "InMemoryDataset") {
453: 		string name = "arrow_" + GenerateRandomName();
454: 		auto stream_factory = make_unique<PythonTableArrowArrayStreamFactory>(entry.ptr());
455: 		auto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;
456: 		children.push_back(make_unique<ConstantExpression>(Value::POINTER((uintptr_t)stream_factory.get())));
457: 		children.push_back(make_unique<ConstantExpression>(Value::POINTER((uintptr_t)stream_factory_produce)));
458: 		children.push_back(make_unique<ConstantExpression>(Value::UBIGINT(1000000)));
459: 		table_function->function = make_unique<FunctionExpression>("arrow_scan", move(children));
460: 		registered_objects[name] = make_unique<RegisteredArrow>(move(stream_factory), entry);
461: 	} else {
462: 		throw std::runtime_error("Python Object " + py_object_type + " not suitable for replacement scans");
463: 	}
464: 	return table_function;
465: }
466: 
467: static unique_ptr<TableFunctionRef> ScanReplacement(const string &table_name, void *data) {
468: 	py::gil_scoped_acquire acquire;
469: 	auto registered_objects = (unordered_map<string, unique_ptr<RegisteredObject>> *)data;
470: 	auto py_table_name = py::str(table_name);
471: 	// Here we do an exhaustive search on the frame lineage
472: 	auto current_frame = py::module::import("inspect").attr("currentframe")();
473: 	while (hasattr(current_frame, "f_locals")) {
474: 		auto local_dict = py::reinterpret_borrow<py::dict>(current_frame.attr("f_locals"));
475: 		// search local dictionary
476: 		if (local_dict) {
477: 			auto result = TryReplacement(local_dict, py_table_name, *registered_objects);
478: 			if (result) {
479: 				return result;
480: 			}
481: 		}
482: 		// search global dictionary
483: 		auto global_dict = py::reinterpret_borrow<py::dict>(current_frame.attr("f_globals"));
484: 		if (global_dict) {
485: 			auto result = TryReplacement(global_dict, py_table_name, *registered_objects);
486: 			if (result) {
487: 				return result;
488: 			}
489: 		}
490: 		current_frame = current_frame.attr("f_back");
491: 	}
492: 	// Not found :(
493: 	return nullptr;
494: }
495: 
496: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Connect(const string &database, bool read_only,
497:                                                            const py::dict &config_dict, bool check_same_thread) {
498: 	auto res = make_shared<DuckDBPyConnection>();
499: 	DBConfig config;
500: 	if (read_only) {
501: 		config.access_mode = AccessMode::READ_ONLY;
502: 	}
503: 	for (auto &kv : config_dict) {
504: 		string key = py::str(kv.first);
505: 		string val = py::str(kv.second);
506: 		auto config_property = DBConfig::GetOptionByName(key);
507: 		if (!config_property) {
508: 			throw InvalidInputException("Unrecognized configuration property \"%s\"", key);
509: 		}
510: 		config.SetOption(*config_property, Value(val));
511: 	}
512: 	if (config.enable_external_access) {
513: 		config.replacement_scans.emplace_back(ScanReplacement, (void *)&res->registered_objects);
514: 	}
515: 
516: 	res->database = make_unique<DuckDB>(database, &config);
517: 	res->connection = make_unique<Connection>(*res->database);
518: 	res->check_same_thread = check_same_thread;
519: 	PandasScanFunction scan_fun;
520: 	CreateTableFunctionInfo scan_info(scan_fun);
521: 
522: 	MapFunction map_fun;
523: 	CreateTableFunctionInfo map_info(map_fun);
524: 
525: 	auto &context = *res->connection->context;
526: 	auto &catalog = Catalog::GetCatalog(context);
527: 	context.transaction.BeginTransaction();
528: 	catalog.CreateTableFunction(context, &scan_info);
529: 	catalog.CreateTableFunction(context, &map_info);
530: 
531: 	context.transaction.Commit();
532: 
533: 	return res;
534: }
535: 
536: Value TransformPythonValue(py::handle ele) {
537: 	auto datetime_mod = py::module::import("datetime");
538: 	auto datetime_date = datetime_mod.attr("date");
539: 	auto datetime_datetime = datetime_mod.attr("datetime");
540: 	auto datetime_time = datetime_mod.attr("time");
541: 	auto decimal_mod = py::module::import("decimal");
542: 	auto decimal_decimal = decimal_mod.attr("Decimal");
543: 
544: 	if (ele.is_none()) {
545: 		return Value();
546: 	} else if (py::isinstance<py::bool_>(ele)) {
547: 		return Value::BOOLEAN(ele.cast<bool>());
548: 	} else if (py::isinstance<py::int_>(ele)) {
549: 		return Value::BIGINT(ele.cast<int64_t>());
550: 	} else if (py::isinstance<py::float_>(ele)) {
551: 		return Value::DOUBLE(ele.cast<double>());
552: 	} else if (py::isinstance(ele, decimal_decimal)) {
553: 		return py::str(ele).cast<string>();
554: 	} else if (py::isinstance(ele, datetime_datetime)) {
555: 		auto ptr = ele.ptr();
556: 		auto year = PyDateTime_GET_YEAR(ptr);
557: 		auto month = PyDateTime_GET_MONTH(ptr);
558: 		auto day = PyDateTime_GET_DAY(ptr);
559: 		auto hour = PyDateTime_DATE_GET_HOUR(ptr);
560: 		auto minute = PyDateTime_DATE_GET_MINUTE(ptr);
561: 		auto second = PyDateTime_DATE_GET_SECOND(ptr);
562: 		auto micros = PyDateTime_DATE_GET_MICROSECOND(ptr);
563: 		return Value::TIMESTAMP(year, month, day, hour, minute, second, micros);
564: 	} else if (py::isinstance(ele, datetime_time)) {
565: 		auto ptr = ele.ptr();
566: 		auto hour = PyDateTime_TIME_GET_HOUR(ptr);
567: 		auto minute = PyDateTime_TIME_GET_MINUTE(ptr);
568: 		auto second = PyDateTime_TIME_GET_SECOND(ptr);
569: 		auto micros = PyDateTime_TIME_GET_MICROSECOND(ptr);
570: 		return Value::TIME(hour, minute, second, micros);
571: 	} else if (py::isinstance(ele, datetime_date)) {
572: 		auto ptr = ele.ptr();
573: 		auto year = PyDateTime_GET_YEAR(ptr);
574: 		auto month = PyDateTime_GET_MONTH(ptr);
575: 		auto day = PyDateTime_GET_DAY(ptr);
576: 		return Value::DATE(year, month, day);
577: 	} else if (py::isinstance<py::str>(ele)) {
578: 		return ele.cast<string>();
579: 	} else if (py::isinstance<py::memoryview>(ele)) {
580: 		py::memoryview py_view = ele.cast<py::memoryview>();
581: 		PyObject *py_view_ptr = py_view.ptr();
582: 		Py_buffer *py_buf = PyMemoryView_GET_BUFFER(py_view_ptr);
583: 		return Value::BLOB(const_data_ptr_t(py_buf->buf), idx_t(py_buf->len));
584: 	} else if (py::isinstance<py::bytes>(ele)) {
585: 		const string &ele_string = ele.cast<string>();
586: 		return Value::BLOB(const_data_ptr_t(ele_string.data()), ele_string.size());
587: 	} else if (py::isinstance<py::list>(ele)) {
588: 		auto size = py::len(ele);
589: 
590: 		if (size == 0) {
591: 			return Value::EMPTYLIST(LogicalType::SQLNULL);
592: 		}
593: 
594: 		vector<Value> values;
595: 		values.reserve(size);
596: 
597: 		for (auto py_val : ele) {
598: 			values.emplace_back(TransformPythonValue(py_val));
599: 		}
600: 
601: 		return Value::LIST(values);
602: 	} else {
603: 		throw std::runtime_error("unknown param type " + py::str(ele.get_type()).cast<string>());
604: 	}
605: }
606: 
607: vector<Value> DuckDBPyConnection::TransformPythonParamList(py::handle params) {
608: 	vector<Value> args;
609: 	args.reserve(py::len(params));
610: 
611: 	for (auto param : params) {
612: 		args.emplace_back(TransformPythonValue(param));
613: 	}
614: 	return args;
615: }
616: 
617: DuckDBPyConnection *DuckDBPyConnection::DefaultConnection() {
618: 	if (!default_connection) {
619: 		py::dict config_dict;
620: 		default_connection = DuckDBPyConnection::Connect(":memory:", false, config_dict, true);
621: 	}
622: 	return default_connection.get();
623: }
624: 
625: void DuckDBPyConnection::Cleanup() {
626: 	default_connection.reset();
627: }
628: 
629: } // namespace duckdb
[end of tools/pythonpkg/src/pyconnection.cpp]
[start of tools/rpkg/src/database.cpp]
1: #include "rapi.hpp"
2: 
3: #include "duckdb/main/client_context.hpp"
4: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
5: 
6: using namespace duckdb;
7: 
8: void duckdb::DBDeleter(DBWrapper *db) {
9: 	cpp11::warning("Database is garbage-collected, use dbDisconnect(con, shutdown=TRUE) or "
10: 	               "duckdb::duckdb_shutdown(drv) to avoid this.");
11: 	delete db;
12: }
13: 
14: [[cpp11::register]] duckdb::db_eptr_t rapi_startup(std::string dbdir, bool readonly, cpp11::list configsexp) {
15: 
16: 	const char *dbdirchar;
17: 
18: 	if (dbdir.length() == 0 || dbdir.compare(":memory:") == 0) {
19: 		dbdirchar = NULL;
20: 	} else {
21: 		dbdirchar = dbdir.c_str();
22: 	}
23: 
24: 	DBConfig config;
25: 	if (readonly) {
26: 		config.access_mode = AccessMode::READ_ONLY;
27: 	}
28: 
29: 	auto confignames = configsexp.names();
30: 
31: 	for (auto it = confignames.begin(); it != confignames.end(); ++it) {
32: 		std::string key = *it;
33: 		std::string val = cpp11::as_cpp<std::string>(configsexp[key]);
34: 		auto config_property = DBConfig::GetOptionByName(key);
35: 		if (!config_property) {
36: 			cpp11::stop("rapi_startup: Unrecognized configuration property '%s'", key.c_str());
37: 		}
38: 		try {
39: 			config.SetOption(*config_property, Value(val));
40: 		} catch (std::exception &e) {
41: 			cpp11::stop("rapi_startup: Failed to set configuration option: %s", e.what());
42: 		}
43: 	}
44: 
45: 	DBWrapper *wrapper;
46: 
47: 	try {
48: 		wrapper = new DBWrapper();
49: 		config.replacement_scans.emplace_back(ArrowScanReplacement, wrapper);
50: 		wrapper->db = make_unique<DuckDB>(dbdirchar, &config);
51: 	} catch (std::exception &e) {
52: 		cpp11::stop("rapi_startup: Failed to open database: %s", e.what());
53: 	}
54: 	D_ASSERT(wrapper->db);
55: 
56: 	DataFrameScanFunction scan_fun;
57: 	CreateTableFunctionInfo info(scan_fun);
58: 	Connection conn(*wrapper->db);
59: 	auto &context = *conn.context;
60: 	auto &catalog = Catalog::GetCatalog(context);
61: 	context.transaction.BeginTransaction();
62: 	catalog.CreateTableFunction(context, &info);
63: 	context.transaction.Commit();
64: 
65: 	return db_eptr_t(wrapper);
66: }
67: 
68: [[cpp11::register]] void rapi_shutdown(duckdb::db_eptr_t dbsexp) {
69: 	auto db_wrapper = dbsexp.release();
70: 	if (db_wrapper) {
71: 		delete db_wrapper;
72: 	}
73: }
[end of tools/rpkg/src/database.cpp]
[start of tools/rpkg/src/include/rapi.hpp]
1: #pragma once
2: 
3: #include "cpp11.hpp"
4: 
5: #include <Rdefines.h>
6: #include <R_ext/Altrep.h>
7: 
8: #include "duckdb.hpp"
9: #include "duckdb/function/table_function.hpp"
10: #include "duckdb/common/unordered_map.hpp"
11: #include "duckdb/parser/tableref/table_function_ref.hpp"
12: #include "duckdb/common/mutex.hpp"
13: 
14: namespace duckdb {
15: 
16: typedef unordered_map<std::string, SEXP> arrow_scans_t;
17: 
18: struct DBWrapper {
19: 	unique_ptr<DuckDB> db;
20: 	arrow_scans_t arrow_scans;
21: 	mutex lock;
22: };
23: 
24: void DBDeleter(DBWrapper *);
25: typedef cpp11::external_pointer<DBWrapper, DBDeleter> db_eptr_t;
26: 
27: struct ConnWrapper {
28: 	unique_ptr<Connection> conn;
29: 	db_eptr_t db_eptr;
30: };
31: 
32: void ConnDeleter(ConnWrapper *);
33: typedef cpp11::external_pointer<ConnWrapper, ConnDeleter> conn_eptr_t;
34: 
35: struct RStatement {
36: 	unique_ptr<PreparedStatement> stmt;
37: 	vector<Value> parameters;
38: };
39: 
40: typedef cpp11::external_pointer<RStatement> stmt_eptr_t;
41: 
42: struct RQueryResult {
43: 	unique_ptr<QueryResult> result;
44: };
45: 
46: typedef cpp11::external_pointer<RQueryResult> rqry_eptr_t;
47: 
48: // internal
49: unique_ptr<TableFunctionRef> ArrowScanReplacement(const std::string &table_name, void *data);
50: 
51: SEXP StringsToSexp(vector<std::string> s);
52: 
53: SEXP ToUtf8(SEXP string_sexp);
54: 
55: struct RProtector {
56: 	RProtector() : protect_count(0) {
57: 	}
58: 	~RProtector() {
59: 		if (protect_count > 0) {
60: 			UNPROTECT(protect_count);
61: 		}
62: 	}
63: 
64: 	SEXP Protect(SEXP sexp) {
65: 		protect_count++;
66: 		return PROTECT(sexp);
67: 	}
68: 
69: private:
70: 	int protect_count;
71: };
72: 
73: struct DataFrameScanFunction : public TableFunction {
74: 	DataFrameScanFunction();
75: };
76: 
77: struct RStrings {
78: 	SEXP secs; // Rf_mkChar
79: 	SEXP mins;
80: 	SEXP hours;
81: 	SEXP days;
82: 	SEXP weeks;
83: 	SEXP POSIXct;
84: 	SEXP POSIXt;
85: 	SEXP UTC_str; // Rf_mkString
86: 	SEXP Date_str;
87: 	SEXP factor_str;
88: 	SEXP difftime_str;
89: 	SEXP secs_str;
90: 	SEXP arrow_str; // StringsToSexp
91: 	SEXP POSIXct_POSIXt_str;
92: 	SEXP enc2utf8_sym; // Rf_install
93: 	SEXP tzone_sym;
94: 	SEXP units_sym;
95: 	SEXP getNamespace_sym;
96: 	SEXP Table__from_record_batches_sym;
97: 	SEXP ImportSchema_sym;
98: 	SEXP ImportRecordBatch_sym;
99: 	SEXP ImportRecordBatchReader_sym;
100: 
101: 	static const RStrings &get() {
102: 		// On demand
103: 		static RStrings strings;
104: 		return strings;
105: 	}
106: 
107: private:
108: 	RStrings();
109: };
110: 
111: } // namespace duckdb
112: 
113: // moved out of duckdb namespace for the time being (r-lib/cpp11#262)
114: 
115: duckdb::db_eptr_t rapi_startup(std::string, bool, cpp11::list);
116: 
117: void rapi_shutdown(duckdb::db_eptr_t);
118: 
119: duckdb::conn_eptr_t rapi_connect(duckdb::db_eptr_t);
120: 
121: void rapi_disconnect(duckdb::conn_eptr_t);
122: 
123: cpp11::list rapi_prepare(duckdb::conn_eptr_t, std::string);
124: 
125: cpp11::list rapi_bind(duckdb::stmt_eptr_t, SEXP paramsexp, bool);
126: 
127: SEXP rapi_execute(duckdb::stmt_eptr_t, bool);
128: 
129: void rapi_release(duckdb::stmt_eptr_t);
130: 
131: void rapi_register_df(duckdb::conn_eptr_t, std::string, cpp11::data_frame);
132: 
133: void rapi_unregister_df(duckdb::conn_eptr_t, std::string);
134: 
135: void rapi_register_arrow(duckdb::conn_eptr_t, SEXP namesexp, SEXP export_funsexp, SEXP valuesexp);
136: 
137: void rapi_unregister_arrow(duckdb::conn_eptr_t, SEXP namesexp);
138: 
139: SEXP rapi_execute_arrow(duckdb::rqry_eptr_t, int);
140: 
141: SEXP rapi_record_batch(duckdb::rqry_eptr_t, int);
142: 
143: cpp11::r_string rapi_ptr_to_str(SEXP extptr);
[end of tools/rpkg/src/include/rapi.hpp]
[start of tools/rpkg/src/register.cpp]
1: #include "rapi.hpp"
2: #include "typesr.hpp"
3: 
4: #include "duckdb/common/arrow_wrapper.hpp"
5: #include "duckdb/planner/table_filter.hpp"
6: #include "duckdb/planner/filter/constant_filter.hpp"
7: #include "duckdb/planner/filter/conjunction_filter.hpp"
8: #include "duckdb/parser/expression/constant_expression.hpp"
9: #include "duckdb/parser/expression/function_expression.hpp"
10: 
11: using namespace duckdb;
12: 
13: [[cpp11::register]] void rapi_register_df(duckdb::conn_eptr_t conn, std::string name, cpp11::data_frame value) {
14: 	if (!conn || !conn->conn) {
15: 		cpp11::stop("rapi_register_df: Invalid connection");
16: 	}
17: 	if (name.empty()) {
18: 		cpp11::stop("rapi_register_df: Name cannot be empty");
19: 	}
20: 	if (value.ncol() < 1) {
21: 		cpp11::stop("rapi_register_df: Data frame with at least one column required");
22: 	}
23: 	try {
24: 		conn->conn->TableFunction("r_dataframe_scan", {Value::POINTER((uintptr_t)value.data())})
25: 		    ->CreateView(name, true, true);
26: 		static_cast<cpp11::sexp>(conn).attr("_registered_df_" + name) = value;
27: 	} catch (std::exception &e) {
28: 		cpp11::stop("rapi_register_df: Failed to register data frame: %s", e.what());
29: 	}
30: }
31: 
32: [[cpp11::register]] void rapi_unregister_df(duckdb::conn_eptr_t conn, std::string name) {
33: 	if (!conn || !conn->conn) {
34: 		cpp11::stop("rapi_unregister_df: Invalid connection");
35: 	}
36: 	static_cast<cpp11::sexp>(conn).attr("_registered_df_" + name) = R_NilValue;
37: 	auto res = conn->conn->Query("DROP VIEW IF EXISTS \"" + name + "\"");
38: 	if (!res->success) {
39: 		cpp11::stop(res->error.c_str());
40: 	}
41: }
42: 
43: class RArrowTabularStreamFactory {
44: public:
45: 	RArrowTabularStreamFactory(SEXP export_fun_p, SEXP arrow_scannable_p)
46: 	    : arrow_scannable(arrow_scannable_p), export_fun(export_fun_p) {};
47: 
48: 	static unique_ptr<ArrowArrayStreamWrapper>
49: 	Produce(uintptr_t factory_p, std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns,
50: 	        TableFilterCollection *filters) {
51: 
52: 		RProtector r;
53: 		auto res = make_unique<ArrowArrayStreamWrapper>();
54: 		auto factory = (RArrowTabularStreamFactory *)factory_p;
55: 		auto stream_ptr_sexp =
56: 		    r.Protect(Rf_ScalarReal(static_cast<double>(reinterpret_cast<uintptr_t>(&res->arrow_array_stream))));
57: 
58: 		cpp11::function export_fun = VECTOR_ELT(factory->export_fun, 0);
59: 
60: 		if (project_columns.second.empty()) {
61: 			export_fun(factory->arrow_scannable, stream_ptr_sexp);
62: 		} else {
63: 			auto projection_sexp = r.Protect(StringsToSexp(project_columns.second));
64: 			SEXP filters_sexp = r.Protect(Rf_ScalarLogical(true));
65: 			if (filters && filters->table_filters && !filters->table_filters->filters.empty()) {
66: 
67: 				filters_sexp = r.Protect(TransformFilter(*filters, project_columns.first, factory->export_fun));
68: 			}
69: 			export_fun(factory->arrow_scannable, stream_ptr_sexp, projection_sexp, filters_sexp);
70: 		}
71: 		return res;
72: 	}
73: 
74: 	SEXP arrow_scannable;
75: 	SEXP export_fun;
76: 
77: private:
78: 	static SEXP TransformFilterExpression(TableFilter &filter, const string &column_name, SEXP functions) {
79: 		RProtector r;
80: 		auto column_name_sexp = r.Protect(Rf_mkString(column_name.c_str()));
81: 		auto column_name_expr = r.Protect(CreateFieldRef(functions, column_name_sexp));
82: 
83: 		switch (filter.filter_type) {
84: 		case TableFilterType::CONSTANT_COMPARISON: {
85: 			auto constant_filter = (ConstantFilter &)filter;
86: 			auto constant_sexp = r.Protect(RApiTypes::ValueToSexp(constant_filter.constant));
87: 			auto constant_expr = r.Protect(CreateScalar(functions, constant_sexp));
88: 			switch (constant_filter.comparison_type) {
89: 			case ExpressionType::COMPARE_EQUAL: {
90: 				return CreateExpression(functions, "equal", column_name_expr, constant_expr);
91: 			}
92: 			case ExpressionType::COMPARE_GREATERTHAN: {
93: 				return CreateExpression(functions, "greater", column_name_expr, constant_expr);
94: 			}
95: 			case ExpressionType::COMPARE_GREATERTHANOREQUALTO: {
96: 				return CreateExpression(functions, "greater_equal", column_name_expr, constant_expr);
97: 			}
98: 			case ExpressionType::COMPARE_LESSTHAN: {
99: 				return CreateExpression(functions, "less", column_name_expr, constant_expr);
100: 			}
101: 			case ExpressionType::COMPARE_LESSTHANOREQUALTO: {
102: 				return CreateExpression(functions, "less_equal", column_name_expr, constant_expr);
103: 			}
104: 			case ExpressionType::COMPARE_NOTEQUAL: {
105: 				return CreateExpression(functions, "not_equal", column_name_expr, constant_expr);
106: 			}
107: 			default:
108: 				throw InternalException("%s can't be transformed to Arrow Scan Pushdown Filter",
109: 				                        filter.ToString(column_name));
110: 			}
111: 		}
112: 		case TableFilterType::IS_NULL: {
113: 			return CreateExpression(functions, "is_null", column_name_expr);
114: 		}
115: 		case TableFilterType::IS_NOT_NULL: {
116: 			auto is_null_expr = r.Protect(CreateExpression(functions, "is_null", column_name_expr));
117: 			return CreateExpression(functions, "invert", is_null_expr);
118: 		}
119: 		case TableFilterType::CONJUNCTION_AND: {
120: 			auto &and_filter = (ConjunctionAndFilter &)filter;
121: 			return TransformChildFilters(functions, column_name, "and_kleene", and_filter.child_filters);
122: 		}
123: 		case TableFilterType::CONJUNCTION_OR: {
124: 			auto &and_filter = (ConjunctionAndFilter &)filter;
125: 			return TransformChildFilters(functions, column_name, "or_kleene", and_filter.child_filters);
126: 		}
127: 
128: 		default:
129: 			throw NotImplementedException("Arrow table filter pushdown %s not supported yet",
130: 			                              filter.ToString(column_name));
131: 		}
132: 	}
133: 
134: 	static SEXP TransformChildFilters(SEXP functions, const string &column_name, const string op,
135: 	                                  vector<unique_ptr<TableFilter>> &filters) {
136: 		auto fit = filters.begin();
137: 		RProtector r;
138: 		auto conjunction_sexp = r.Protect(TransformFilterExpression(**fit, column_name, functions));
139: 		fit++;
140: 		for (; fit != filters.end(); ++fit) {
141: 			SEXP rhs = r.Protect(TransformFilterExpression(**fit, column_name, functions));
142: 			conjunction_sexp = r.Protect(CreateExpression(functions, op, conjunction_sexp, rhs));
143: 		}
144: 		return conjunction_sexp;
145: 	}
146: 
147: 	static SEXP TransformFilter(TableFilterCollection &filter_collection, std::unordered_map<idx_t, string> &columns,
148: 	                            SEXP functions) {
149: 		RProtector r;
150: 
151: 		auto fit = filter_collection.table_filters->filters.begin();
152: 		SEXP res = r.Protect(TransformFilterExpression(*fit->second, columns[fit->first], functions));
153: 		fit++;
154: 		for (; fit != filter_collection.table_filters->filters.end(); ++fit) {
155: 			SEXP rhs = r.Protect(TransformFilterExpression(*fit->second, columns[fit->first], functions));
156: 			res = r.Protect(CreateExpression(functions, "and_kleene", res, rhs));
157: 		}
158: 		return res;
159: 	}
160: 
161: 	static SEXP CallArrowFactory(SEXP functions, idx_t idx, SEXP op1, SEXP op2 = R_NilValue, SEXP op3 = R_NilValue) {
162: 		cpp11::function create_fun = VECTOR_ELT(functions, idx);
163: 		if (Rf_isNull(op2)) {
164: 			return create_fun(op1);
165: 		} else if (Rf_isNull(op3)) {
166: 			return create_fun(op1, op2);
167: 		} else {
168: 			return create_fun(op1, op2, op3);
169: 		}
170: 	}
171: 
172: 	static SEXP CreateExpression(SEXP functions, const string name, SEXP op1, SEXP op2 = R_NilValue) {
173: 		RProtector r;
174: 		auto name_sexp = r.Protect(Rf_mkString(name.c_str()));
175: 		return CallArrowFactory(functions, 1, name_sexp, op1, op2);
176: 	}
177: 
178: 	static SEXP CreateFieldRef(SEXP functions, SEXP op) {
179: 		return CallArrowFactory(functions, 2, op);
180: 	}
181: 
182: 	static SEXP CreateScalar(SEXP functions, SEXP op) {
183: 		return CallArrowFactory(functions, 3, op);
184: 	}
185: };
186: 
187: unique_ptr<TableFunctionRef> duckdb::ArrowScanReplacement(const string &table_name, void *data) {
188: 	auto db_wrapper = (DBWrapper *)data;
189: 	lock_guard<mutex> arrow_scans_lock(db_wrapper->lock);
190: 	for (auto &e : db_wrapper->arrow_scans) {
191: 		if (e.first == table_name) {
192: 			auto table_function = make_unique<TableFunctionRef>();
193: 			vector<unique_ptr<ParsedExpression>> children;
194: 			children.push_back(make_unique<ConstantExpression>(Value::POINTER((uintptr_t)R_ExternalPtrAddr(e.second))));
195: 			children.push_back(
196: 			    make_unique<ConstantExpression>(Value::POINTER((uintptr_t)RArrowTabularStreamFactory::Produce)));
197: 			children.push_back(make_unique<ConstantExpression>(Value::UBIGINT(100000)));
198: 			table_function->function = make_unique<FunctionExpression>("arrow_scan", move(children));
199: 			return table_function;
200: 		}
201: 	}
202: 	return nullptr;
203: }
204: 
205: [[cpp11::register]] void rapi_register_arrow(duckdb::conn_eptr_t conn, std::string name, cpp11::list export_funs,
206:                                              cpp11::sexp valuesexp) {
207: 	if (!conn || !conn->conn) {
208: 		cpp11::stop("rapi_register_arrow: Invalid connection");
209: 	}
210: 	if (name.empty()) {
211: 		cpp11::stop("rapi_register_arrow: Name cannot be empty");
212: 	}
213: 
214: 	auto stream_factory = new RArrowTabularStreamFactory(export_funs, valuesexp);
215: 	// make r external ptr object to keep factory around until arrow table is unregistered
216: 	cpp11::external_pointer<RArrowTabularStreamFactory> factorysexp(stream_factory);
217: 
218: 	{
219: 		// TODO check if this name already exists?
220: 		lock_guard<mutex> arrow_scans_lock(conn->db_eptr->lock);
221: 		auto &arrow_scans = conn->db_eptr->arrow_scans;
222: 		arrow_scans[name] = factorysexp;
223: 	}
224: 	cpp11::writable::list state_list = {export_funs, valuesexp, factorysexp};
225: 	static_cast<cpp11::sexp>(conn->db_eptr).attr("_registered_arrow_" + name) = state_list;
226: }
227: 
228: [[cpp11::register]] void rapi_unregister_arrow(duckdb::conn_eptr_t conn, std::string name) {
229: 	if (!conn || !conn->conn) {
230: 		cpp11::stop("rapi_unregister_arrow: Invalid connection");
231: 	}
232: 
233: 	{
234: 		lock_guard<mutex> arrow_scans_lock(conn->db_eptr->lock);
235: 		auto &arrow_scans = conn->db_eptr->arrow_scans;
236: 		arrow_scans.erase(name);
237: 	}
238: 	static_cast<cpp11::sexp>(conn->db_eptr).attr("_registered_arrow_" + name) = R_NilValue;
239: }
[end of tools/rpkg/src/register.cpp]
[start of tools/rpkg/src/scan.cpp]
1: #include "rapi.hpp"
2: #include "typesr.hpp"
3: #include "altrepstring.hpp"
4: 
5: #include "duckdb/main/client_context.hpp"
6: 
7: using namespace duckdb;
8: 
9: template <class SRC, class DST, class RTYPE>
10: static void AppendColumnSegment(SRC *source_data, Vector &result, idx_t count) {
11: 	auto result_data = FlatVector::GetData<DST>(result);
12: 	auto &result_mask = FlatVector::Validity(result);
13: 	for (idx_t i = 0; i < count; i++) {
14: 		auto val = source_data[i];
15: 		if (RTYPE::IsNull(val)) {
16: 			result_mask.SetInvalid(i);
17: 		} else {
18: 			result_data[i] = RTYPE::Convert(val);
19: 		}
20: 	}
21: }
22: 
23: static void AppendStringSegment(SEXP coldata, Vector &result, idx_t row_idx, idx_t count) {
24: 	auto result_data = FlatVector::GetData<string_t>(result);
25: 	auto &result_mask = FlatVector::Validity(result);
26: 	for (idx_t i = 0; i < count; i++) {
27: 		SEXP val = STRING_ELT(coldata, row_idx + i);
28: 		if (val == NA_STRING) {
29: 			result_mask.SetInvalid(i);
30: 		} else {
31: 			result_data[i] = string_t((char *)CHAR(val));
32: 		}
33: 	}
34: }
35: 
36: struct DataFrameScanFunctionData : public TableFunctionData {
37: 	DataFrameScanFunctionData(SEXP df, idx_t row_count, vector<RType> rtypes)
38: 	    : df(df), row_count(row_count), rtypes(rtypes) {
39: 	}
40: 	SEXP df;
41: 	idx_t row_count;
42: 	vector<RType> rtypes;
43: };
44: 
45: struct DataFrameScanState : public FunctionOperatorData {
46: 	DataFrameScanState() : position(0) {
47: 	}
48: 
49: 	idx_t position;
50: };
51: 
52: static unique_ptr<FunctionData> dataframe_scan_bind(ClientContext &context, vector<Value> &inputs,
53:                                                     named_parameter_map_t &named_parameters,
54:                                                     vector<LogicalType> &input_table_types,
55:                                                     vector<string> &input_table_names,
56:                                                     vector<LogicalType> &return_types, vector<string> &names) {
57: 	RProtector r;
58: 	SEXP df((SEXP)inputs[0].GetPointer());
59: 
60: 	auto df_names = r.Protect(GET_NAMES(df));
61: 	vector<RType> rtypes;
62: 
63: 	for (idx_t col_idx = 0; col_idx < (idx_t)Rf_length(df); col_idx++) {
64: 		auto column_name = string(CHAR(STRING_ELT(df_names, col_idx)));
65: 		names.push_back(column_name);
66: 		SEXP coldata = VECTOR_ELT(df, col_idx);
67: 		rtypes.push_back(RApiTypes::DetectRType(coldata));
68: 		LogicalType duckdb_col_type;
69: 		switch (rtypes[col_idx]) {
70: 		case RType::LOGICAL:
71: 			duckdb_col_type = LogicalType::BOOLEAN;
72: 			break;
73: 		case RType::INTEGER:
74: 			duckdb_col_type = LogicalType::INTEGER;
75: 			break;
76: 		case RType::NUMERIC:
77: 			duckdb_col_type = LogicalType::DOUBLE;
78: 			break;
79: 		case RType::FACTOR: {
80: 			// TODO What about factors that use numeric?
81: 
82: 			auto levels = r.Protect(GET_LEVELS(coldata));
83: 			idx_t size = LENGTH(levels);
84: 			Vector duckdb_levels(LogicalType::VARCHAR, size);
85: 			for (idx_t level_idx = 0; level_idx < size; level_idx++) {
86: 				duckdb_levels.SetValue(level_idx, string(CHAR(STRING_ELT(levels, level_idx))));
87: 			}
88: 			duckdb_col_type = LogicalType::ENUM(column_name, duckdb_levels, size);
89: 			break;
90: 		}
91: 		case RType::STRING:
92: 			duckdb_col_type = LogicalType::VARCHAR;
93: 			break;
94: 		case RType::TIMESTAMP:
95: 			duckdb_col_type = LogicalType::TIMESTAMP;
96: 			break;
97: 		case RType::TIME_SECONDS:
98: 		case RType::TIME_MINUTES:
99: 		case RType::TIME_HOURS:
100: 		case RType::TIME_DAYS:
101: 		case RType::TIME_WEEKS:
102: 		case RType::TIME_SECONDS_INTEGER:
103: 		case RType::TIME_MINUTES_INTEGER:
104: 		case RType::TIME_HOURS_INTEGER:
105: 		case RType::TIME_DAYS_INTEGER:
106: 		case RType::TIME_WEEKS_INTEGER:
107: 			duckdb_col_type = LogicalType::TIME;
108: 			break;
109: 		case RType::DATE:
110: 		case RType::DATE_INTEGER:
111: 			duckdb_col_type = LogicalType::DATE;
112: 			break;
113: 		default:
114: 			cpp11::stop("rapi_execute: Unsupported column type for scan");
115: 		}
116: 		return_types.push_back(duckdb_col_type);
117: 	}
118: 
119: 	auto row_count = Rf_length(VECTOR_ELT(df, 0));
120: 	return make_unique<DataFrameScanFunctionData>(df, row_count, rtypes);
121: }
122: 
123: static unique_ptr<FunctionOperatorData> dataframe_scan_init(ClientContext &context, const FunctionData *bind_data,
124:                                                             const vector<column_t> &column_ids,
125:                                                             TableFilterCollection *filters) {
126: 	return make_unique<DataFrameScanState>();
127: }
128: 
129: static void dataframe_scan_function(ClientContext &context, const FunctionData *bind_data,
130:                                     FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
131: 	auto &data = (DataFrameScanFunctionData &)*bind_data;
132: 	auto &state = (DataFrameScanState &)*operator_state;
133: 	if (state.position >= data.row_count) {
134: 		return;
135: 	}
136: 	idx_t this_count = std::min((idx_t)STANDARD_VECTOR_SIZE, data.row_count - state.position);
137: 
138: 	output.SetCardinality(this_count);
139: 
140: 	for (idx_t col_idx = 0; col_idx < output.ColumnCount(); col_idx++) {
141: 		auto &v = output.data[col_idx];
142: 		SEXP coldata = VECTOR_ELT(data.df, col_idx);
143: 
144: 		switch (data.rtypes[col_idx]) {
145: 		case RType::LOGICAL: {
146: 			auto data_ptr = INTEGER_POINTER(coldata) + state.position;
147: 			AppendColumnSegment<int, bool, RBooleanType>(data_ptr, v, this_count);
148: 			break;
149: 		}
150: 		case RType::INTEGER: {
151: 			auto data_ptr = INTEGER_POINTER(coldata) + state.position;
152: 			AppendColumnSegment<int, int, RIntegerType>(data_ptr, v, this_count);
153: 			break;
154: 		}
155: 		case RType::NUMERIC: {
156: 			auto data_ptr = NUMERIC_POINTER(coldata) + state.position;
157: 			AppendColumnSegment<double, double, RDoubleType>(data_ptr, v, this_count);
158: 			break;
159: 		}
160: 		case RType::STRING: {
161: 			AppendStringSegment(coldata, v, state.position, this_count);
162: 			break;
163: 		}
164: 		case RType::FACTOR: {
165: 			auto data_ptr = INTEGER_POINTER(coldata) + state.position;
166: 			switch (v.GetType().InternalType()) {
167: 			case PhysicalType::UINT8:
168: 				AppendColumnSegment<int, uint8_t, RFactorType>(data_ptr, v, this_count);
169: 				break;
170: 
171: 			case PhysicalType::UINT16:
172: 				AppendColumnSegment<int, uint16_t, RFactorType>(data_ptr, v, this_count);
173: 				break;
174: 
175: 			case PhysicalType::UINT32:
176: 				AppendColumnSegment<int, uint32_t, RFactorType>(data_ptr, v, this_count);
177: 				break;
178: 
179: 			default:
180: 				cpp11::stop("rapi_execute: Unknown enum type for scan: %s",
181: 				            TypeIdToString(v.GetType().InternalType()).c_str());
182: 			}
183: 			break;
184: 		}
185: 		case RType::TIMESTAMP: {
186: 			auto data_ptr = NUMERIC_POINTER(coldata) + state.position;
187: 			AppendColumnSegment<double, timestamp_t, RTimestampType>(data_ptr, v, this_count);
188: 			break;
189: 		}
190: 		case RType::TIME_SECONDS: {
191: 			auto data_ptr = NUMERIC_POINTER(coldata) + state.position;
192: 			AppendColumnSegment<double, dtime_t, RTimeSecondsType>(data_ptr, v, this_count);
193: 			break;
194: 		}
195: 		case RType::TIME_MINUTES: {
196: 			auto data_ptr = NUMERIC_POINTER(coldata) + state.position;
197: 			AppendColumnSegment<double, dtime_t, RTimeMinutesType>(data_ptr, v, this_count);
198: 			break;
199: 		}
200: 		case RType::TIME_HOURS: {
201: 			auto data_ptr = NUMERIC_POINTER(coldata) + state.position;
202: 			AppendColumnSegment<double, dtime_t, RTimeHoursType>(data_ptr, v, this_count);
203: 			break;
204: 		}
205: 		case RType::TIME_DAYS: {
206: 			auto data_ptr = NUMERIC_POINTER(coldata) + state.position;
207: 			AppendColumnSegment<double, dtime_t, RTimeDaysType>(data_ptr, v, this_count);
208: 			break;
209: 		}
210: 		case RType::TIME_WEEKS: {
211: 			auto data_ptr = NUMERIC_POINTER(coldata) + state.position;
212: 			AppendColumnSegment<double, dtime_t, RTimeWeeksType>(data_ptr, v, this_count);
213: 			break;
214: 		}
215: 		case RType::TIME_SECONDS_INTEGER: {
216: 			auto data_ptr = INTEGER_POINTER(coldata) + state.position;
217: 			AppendColumnSegment<int, dtime_t, RTimeSecondsType>(data_ptr, v, this_count);
218: 			break;
219: 		}
220: 		case RType::TIME_MINUTES_INTEGER: {
221: 			auto data_ptr = INTEGER_POINTER(coldata) + state.position;
222: 			AppendColumnSegment<int, dtime_t, RTimeMinutesType>(data_ptr, v, this_count);
223: 			break;
224: 		}
225: 		case RType::TIME_HOURS_INTEGER: {
226: 			auto data_ptr = INTEGER_POINTER(coldata) + state.position;
227: 			AppendColumnSegment<int, dtime_t, RTimeHoursType>(data_ptr, v, this_count);
228: 			break;
229: 		}
230: 		case RType::TIME_DAYS_INTEGER: {
231: 			auto data_ptr = INTEGER_POINTER(coldata) + state.position;
232: 			AppendColumnSegment<int, dtime_t, RTimeDaysType>(data_ptr, v, this_count);
233: 			break;
234: 		}
235: 		case RType::TIME_WEEKS_INTEGER: {
236: 			auto data_ptr = INTEGER_POINTER(coldata) + state.position;
237: 			AppendColumnSegment<int, dtime_t, RTimeWeeksType>(data_ptr, v, this_count);
238: 			break;
239: 		}
240: 		case RType::DATE: {
241: 			auto data_ptr = NUMERIC_POINTER(coldata) + state.position;
242: 			AppendColumnSegment<double, date_t, RDateType>(data_ptr, v, this_count);
243: 			break;
244: 		}
245: 		case RType::DATE_INTEGER: {
246: 			auto data_ptr = INTEGER_POINTER(coldata) + state.position;
247: 			AppendColumnSegment<int, date_t, RDateType>(data_ptr, v, this_count);
248: 			break;
249: 		}
250: 		default:
251: 			throw;
252: 		}
253: 	}
254: 
255: 	state.position += this_count;
256: }
257: 
258: static unique_ptr<NodeStatistics> dataframe_scan_cardinality(ClientContext &context, const FunctionData *bind_data) {
259: 	auto &data = (DataFrameScanFunctionData &)*bind_data;
260: 	return make_unique<NodeStatistics>(data.row_count, data.row_count);
261: }
262: 
263: DataFrameScanFunction::DataFrameScanFunction()
264:     : TableFunction("r_dataframe_scan", {LogicalType::POINTER}, dataframe_scan_function, dataframe_scan_bind,
265:                     dataframe_scan_init, nullptr, nullptr, nullptr, dataframe_scan_cardinality) {};
[end of tools/rpkg/src/scan.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: