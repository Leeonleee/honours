You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Support EXPLAIN ANALYZE
Postgres supports `EXPLAIN ANALYZE` which runs the query and shows query times, etc for the query. We should support the same, as this is essentially the same as our profiling output (`PRAGMA enable_profiling`) but actually returned as a query result which makes it easier to integrate into clients, etc.

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16: </p>
17: 
18: ## DuckDB
19: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/docs/why_duckdb.html).
20: 
21: ## Installation
22: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
23: 
24: ## Data Import
25: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
26: 
27: ```sql
28: SELECT * FROM 'myfile.csv';
29: SELECT * FROM 'myfile.parquet';
30: ```
31: 
32: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
33: 
34: ## SQL Reference
35: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
36: 
37: ## Development
38: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
39: 
40: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
41: 
42: 
[end of README.md]
[start of src/common/enums/physical_operator_type.cpp]
1: #include "duckdb/common/enums/physical_operator_type.hpp"
2: 
3: namespace duckdb {
4: 
5: // LCOV_EXCL_START
6: string PhysicalOperatorToString(PhysicalOperatorType type) {
7: 	switch (type) {
8: 	case PhysicalOperatorType::TABLE_SCAN:
9: 		return "TABLE_SCAN";
10: 	case PhysicalOperatorType::DUMMY_SCAN:
11: 		return "DUMMY_SCAN";
12: 	case PhysicalOperatorType::CHUNK_SCAN:
13: 		return "CHUNK_SCAN";
14: 	case PhysicalOperatorType::DELIM_SCAN:
15: 		return "DELIM_SCAN";
16: 	case PhysicalOperatorType::ORDER_BY:
17: 		return "ORDER_BY";
18: 	case PhysicalOperatorType::LIMIT:
19: 		return "LIMIT";
20: 	case PhysicalOperatorType::RESERVOIR_SAMPLE:
21: 		return "RESERVOIR_SAMPLE";
22: 	case PhysicalOperatorType::STREAMING_SAMPLE:
23: 		return "STREAMING_SAMPLE";
24: 	case PhysicalOperatorType::TOP_N:
25: 		return "TOP_N";
26: 	case PhysicalOperatorType::WINDOW:
27: 		return "WINDOW";
28: 	case PhysicalOperatorType::UNNEST:
29: 		return "UNNEST";
30: 	case PhysicalOperatorType::SIMPLE_AGGREGATE:
31: 		return "SIMPLE_AGGREGATE";
32: 	case PhysicalOperatorType::HASH_GROUP_BY:
33: 		return "HASH_GROUP_BY";
34: 	case PhysicalOperatorType::PERFECT_HASH_GROUP_BY:
35: 		return "PERFECT_HASH_GROUP_BY";
36: 	case PhysicalOperatorType::FILTER:
37: 		return "FILTER";
38: 	case PhysicalOperatorType::PROJECTION:
39: 		return "PROJECTION";
40: 	case PhysicalOperatorType::COPY_TO_FILE:
41: 		return "COPY_TO_FILE";
42: 	case PhysicalOperatorType::DELIM_JOIN:
43: 		return "DELIM_JOIN";
44: 	case PhysicalOperatorType::BLOCKWISE_NL_JOIN:
45: 		return "BLOCKWISE_NL_JOIN";
46: 	case PhysicalOperatorType::NESTED_LOOP_JOIN:
47: 		return "NESTED_LOOP_JOIN";
48: 	case PhysicalOperatorType::HASH_JOIN:
49: 		return "HASH_JOIN";
50: 	case PhysicalOperatorType::INDEX_JOIN:
51: 		return "INDEX_JOIN";
52: 	case PhysicalOperatorType::PIECEWISE_MERGE_JOIN:
53: 		return "PIECEWISE_MERGE_JOIN";
54: 	case PhysicalOperatorType::CROSS_PRODUCT:
55: 		return "CROSS_PRODUCT";
56: 	case PhysicalOperatorType::UNION:
57: 		return "UNION";
58: 	case PhysicalOperatorType::INSERT:
59: 		return "INSERT";
60: 	case PhysicalOperatorType::DELETE_OPERATOR:
61: 		return "DELETE";
62: 	case PhysicalOperatorType::UPDATE:
63: 		return "UPDATE";
64: 	case PhysicalOperatorType::EMPTY_RESULT:
65: 		return "EMPTY_RESULT";
66: 	case PhysicalOperatorType::CREATE_TABLE:
67: 		return "CREATE_TABLE";
68: 	case PhysicalOperatorType::CREATE_TABLE_AS:
69: 		return "CREATE_TABLE_AS";
70: 	case PhysicalOperatorType::CREATE_INDEX:
71: 		return "CREATE_INDEX";
72: 	case PhysicalOperatorType::EXPLAIN:
73: 		return "EXPLAIN";
74: 	case PhysicalOperatorType::EXECUTE:
75: 		return "EXECUTE";
76: 	case PhysicalOperatorType::VACUUM:
77: 		return "VACUUM";
78: 	case PhysicalOperatorType::RECURSIVE_CTE:
79: 		return "REC_CTE";
80: 	case PhysicalOperatorType::RECURSIVE_CTE_SCAN:
81: 		return "REC_CTE_SCAN";
82: 	case PhysicalOperatorType::EXPRESSION_SCAN:
83: 		return "EXPRESSION_SCAN";
84: 	case PhysicalOperatorType::ALTER:
85: 		return "ALTER";
86: 	case PhysicalOperatorType::CREATE_SEQUENCE:
87: 		return "CREATE_SEQUENCE";
88: 	case PhysicalOperatorType::CREATE_VIEW:
89: 		return "CREATE_VIEW";
90: 	case PhysicalOperatorType::CREATE_SCHEMA:
91: 		return "CREATE_SCHEMA";
92: 	case PhysicalOperatorType::CREATE_MACRO:
93: 		return "CREATE_MACRO";
94: 	case PhysicalOperatorType::DROP:
95: 		return "DROP";
96: 	case PhysicalOperatorType::PRAGMA:
97: 		return "PRAGMA";
98: 	case PhysicalOperatorType::TRANSACTION:
99: 		return "TRANSACTION";
100: 	case PhysicalOperatorType::PREPARE:
101: 		return "PREPARE";
102: 	case PhysicalOperatorType::EXPORT:
103: 		return "EXPORT";
104: 	case PhysicalOperatorType::SET:
105: 		return "SET";
106: 	case PhysicalOperatorType::LOAD:
107: 		return "LOAD";
108: 	case PhysicalOperatorType::INOUT_FUNCTION:
109: 		return "INOUT_FUNCTION";
110: 	case PhysicalOperatorType::CREATE_TYPE:
111: 		return "CREATE_TYPE";
112: 	case PhysicalOperatorType::INVALID:
113: 		break;
114: 	}
115: 	return "INVALID";
116: }
117: // LCOV_EXCL_STOP
118: 
119: } // namespace duckdb
[end of src/common/enums/physical_operator_type.cpp]
[start of src/execution/operator/helper/CMakeLists.txt]
1: add_library_unity(
2:   duckdb_operator_helper
3:   OBJECT
4:   physical_execute.cpp
5:   physical_limit.cpp
6:   physical_load.cpp
7:   physical_pragma.cpp
8:   physical_prepare.cpp
9:   physical_reservoir_sample.cpp
10:   physical_set.cpp
11:   physical_streaming_sample.cpp
12:   physical_transaction.cpp
13:   physical_vacuum.cpp)
14: set(ALL_OBJECT_FILES
15:     ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:duckdb_operator_helper>
16:     PARENT_SCOPE)
[end of src/execution/operator/helper/CMakeLists.txt]
[start of src/execution/physical_plan/plan_explain.cpp]
1: #include "duckdb/execution/operator/scan/physical_chunk_scan.hpp"
2: #include "duckdb/execution/physical_plan_generator.hpp"
3: #include "duckdb/planner/operator/logical_explain.hpp"
4: #include "duckdb/main/client_context.hpp"
5: 
6: #include "duckdb/common/tree_renderer.hpp"
7: 
8: namespace duckdb {
9: 
10: unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalExplain &op) {
11: 	D_ASSERT(op.children.size() == 1);
12: 	auto logical_plan_opt = op.children[0]->ToString();
13: 	auto plan = CreatePlan(*op.children[0]);
14: 
15: 	op.physical_plan = plan->ToString();
16: 
17: 	// the output of the explain
18: 	vector<string> keys, values;
19: 	switch (context.explain_output_type) {
20: 	case ExplainOutputType::OPTIMIZED_ONLY:
21: 		keys = {"logical_opt"};
22: 		values = {logical_plan_opt};
23: 		break;
24: 	case ExplainOutputType::PHYSICAL_ONLY:
25: 		keys = {"physical_plan"};
26: 		values = {op.physical_plan};
27: 		break;
28: 	default:
29: 		keys = {"logical_plan", "logical_opt", "physical_plan"};
30: 		values = {op.logical_plan_unopt, logical_plan_opt, op.physical_plan};
31: 	}
32: 
33: 	// create a ChunkCollection from the output
34: 	auto collection = make_unique<ChunkCollection>();
35: 
36: 	DataChunk chunk;
37: 	chunk.Initialize(op.types);
38: 	for (idx_t i = 0; i < keys.size(); i++) {
39: 		chunk.SetValue(0, chunk.size(), Value(keys[i]));
40: 		chunk.SetValue(1, chunk.size(), Value(values[i]));
41: 		chunk.SetCardinality(chunk.size() + 1);
42: 		if (chunk.size() == STANDARD_VECTOR_SIZE) {
43: 			collection->Append(chunk);
44: 			chunk.Reset();
45: 		}
46: 	}
47: 	collection->Append(chunk);
48: 
49: 	// create a chunk scan to output the result
50: 	auto chunk_scan =
51: 	    make_unique<PhysicalChunkScan>(op.types, PhysicalOperatorType::CHUNK_SCAN, op.estimated_cardinality);
52: 	chunk_scan->owned_collection = move(collection);
53: 	chunk_scan->collection = chunk_scan->owned_collection.get();
54: 	return move(chunk_scan);
55: }
56: 
57: } // namespace duckdb
[end of src/execution/physical_plan/plan_explain.cpp]
[start of src/include/duckdb/common/enums/physical_operator_type.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/enums/physical_operator_type.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/constants.hpp"
12: 
13: namespace duckdb {
14: 
15: //===--------------------------------------------------------------------===//
16: // Physical Operator Types
17: //===--------------------------------------------------------------------===//
18: enum class PhysicalOperatorType : uint8_t {
19: 	INVALID,
20: 	ORDER_BY,
21: 	LIMIT,
22: 	TOP_N,
23: 	WINDOW,
24: 	UNNEST,
25: 	SIMPLE_AGGREGATE,
26: 	HASH_GROUP_BY,
27: 	PERFECT_HASH_GROUP_BY,
28: 	FILTER,
29: 	PROJECTION,
30: 	COPY_TO_FILE,
31: 	RESERVOIR_SAMPLE,
32: 	STREAMING_SAMPLE,
33: 	// -----------------------------
34: 	// Scans
35: 	// -----------------------------
36: 	TABLE_SCAN,
37: 	DUMMY_SCAN,
38: 	CHUNK_SCAN,
39: 	RECURSIVE_CTE_SCAN,
40: 	DELIM_SCAN,
41: 	EXPRESSION_SCAN,
42: 	// -----------------------------
43: 	// Joins
44: 	// -----------------------------
45: 	BLOCKWISE_NL_JOIN,
46: 	NESTED_LOOP_JOIN,
47: 	HASH_JOIN,
48: 	CROSS_PRODUCT,
49: 	PIECEWISE_MERGE_JOIN,
50: 	DELIM_JOIN,
51: 	INDEX_JOIN,
52: 	// -----------------------------
53: 	// SetOps
54: 	// -----------------------------
55: 	UNION,
56: 	RECURSIVE_CTE,
57: 
58: 	// -----------------------------
59: 	// Updates
60: 	// -----------------------------
61: 	INSERT,
62: 	DELETE_OPERATOR,
63: 	UPDATE,
64: 
65: 	// -----------------------------
66: 	// Schema
67: 	// -----------------------------
68: 	CREATE_TABLE,
69: 	CREATE_TABLE_AS,
70: 	CREATE_INDEX,
71: 	ALTER,
72: 	CREATE_SEQUENCE,
73: 	CREATE_VIEW,
74: 	CREATE_SCHEMA,
75: 	CREATE_MACRO,
76: 	DROP,
77: 	PRAGMA,
78: 	TRANSACTION,
79: 	CREATE_TYPE,
80: 
81: 	// -----------------------------
82: 	// Helpers
83: 	// -----------------------------
84: 	EXPLAIN,
85: 	EMPTY_RESULT,
86: 	EXECUTE,
87: 	PREPARE,
88: 	VACUUM,
89: 	EXPORT,
90: 	SET,
91: 	LOAD,
92: 	INOUT_FUNCTION
93: };
94: 
95: string PhysicalOperatorToString(PhysicalOperatorType type);
96: 
97: } // namespace duckdb
[end of src/include/duckdb/common/enums/physical_operator_type.hpp]
[start of src/include/duckdb/execution/operator/aggregate/physical_simple_aggregate.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/aggregate/physical_simple_aggregate.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/execution/physical_operator.hpp"
12: #include "duckdb/planner/expression.hpp"
13: 
14: namespace duckdb {
15: 
16: //! PhysicalSimpleAggregate is an aggregate operator that can only perform aggregates (1) without any groups, (2)
17: //! without any DISTINCT aggregates, and (3) when all aggregates are combineable
18: class PhysicalSimpleAggregate : public PhysicalOperator {
19: public:
20: 	PhysicalSimpleAggregate(vector<LogicalType> types, vector<unique_ptr<Expression>> expressions,
21: 	                        idx_t estimated_cardinality);
22: 
23: 	//! The aggregates that have to be computed
24: 	vector<unique_ptr<Expression>> aggregates;
25: 
26: public:
27: 	// Source interface
28: 	unique_ptr<GlobalSourceState> GetGlobalSourceState(ClientContext &context) const override;
29: 	void GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,
30: 	             LocalSourceState &lstate) const override;
31: 
32: public:
33: 	// Sink interface
34: 	SinkResultType Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate,
35: 	                    DataChunk &input) const override;
36: 	void Combine(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate) const override;
37: 	SinkFinalizeType Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
38: 	                          GlobalSinkState &gstate) const override;
39: 
40: 	unique_ptr<LocalSinkState> GetLocalSinkState(ExecutionContext &context) const override;
41: 	unique_ptr<GlobalSinkState> GetGlobalSinkState(ClientContext &context) const override;
42: 
43: 	string ParamsToString() const override;
44: 
45: 	bool IsSink() const override {
46: 		return true;
47: 	}
48: 
49: 	bool ParallelSink() const override {
50: 		// we can only parallelize if all aggregates are combinable
51: 		return true;
52: 	}
53: };
54: 
55: } // namespace duckdb
[end of src/include/duckdb/execution/operator/aggregate/physical_simple_aggregate.hpp]
[start of src/include/duckdb/main/query_profiler.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/main/query_profiler.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/enums/profiler_format.hpp"
13: #include "duckdb/common/profiler.hpp"
14: #include "duckdb/common/string_util.hpp"
15: #include "duckdb/common/types/data_chunk.hpp"
16: #include "duckdb/common/unordered_map.hpp"
17: #include "duckdb/common/winapi.hpp"
18: #include "duckdb/execution/physical_operator.hpp"
19: #include "duckdb/execution/expression_executor_state.hpp"
20: #include <stack>
21: #include "duckdb/common/pair.hpp"
22: #include "duckdb/common/deque.hpp"
23: 
24: namespace duckdb {
25: class ExpressionExecutor;
26: class PhysicalOperator;
27: class SQLStatement;
28: 
29: //! The ExpressionInfo keeps information related to an expression
30: struct ExpressionInfo {
31: 	explicit ExpressionInfo() : hasfunction(false) {
32: 	}
33: 	// A vector of children
34: 	vector<unique_ptr<ExpressionInfo>> children;
35: 	// Extract ExpressionInformation from a given expression state
36: 	void ExtractExpressionsRecursive(unique_ptr<ExpressionState> &state);
37: 	//! Whether or not expression has function
38: 	bool hasfunction;
39: 	//! The function Name
40: 	string function_name;
41: 	//! The function time
42: 	uint64_t function_time;
43: 	//! Count the number of ALL tuples
44: 	uint64_t tuples_count = 0;
45: 	//! Count the number of tuples sampled
46: 	uint64_t sample_tuples_count = 0;
47: };
48: 
49: //! The ExpressionRootInfo keeps information related to the root of an expression tree
50: struct ExpressionRootInfo {
51: 	ExpressionRootInfo(ExpressionExecutorState &executor, string name);
52: 	//! Count the number of time the executor called
53: 	uint64_t total_count = 0;
54: 	//! Count the number of time the executor called since last sampling
55: 	uint64_t current_count = 0;
56: 	//! Count the number of samples
57: 	uint64_t sample_count = 0;
58: 	//! Count the number of tuples in all samples
59: 	uint64_t sample_tuples_count = 0;
60: 	//! Count the number of tuples processed by this executor
61: 	uint64_t tuples_count = 0;
62: 	//! A vector which contain the pointer to root of each expression tree
63: 	unique_ptr<ExpressionInfo> root;
64: 	//! Name
65: 	string name;
66: 	//! Elapsed time
67: 	double time;
68: 	//! Extra Info
69: 	string extra_info;
70: };
71: 
72: struct ExpressionExecutorInfo {
73: 	explicit ExpressionExecutorInfo() {};
74: 	explicit ExpressionExecutorInfo(ExpressionExecutor &executor, const string &name, int id);
75: 	//! A vector which contain the pointer to all ExpressionRootInfo
76: 	vector<unique_ptr<ExpressionRootInfo>> roots;
77: 	//! Id, it will be used as index for executors_info vector
78: 	int id;
79: };
80: 
81: struct OperatorInformation {
82: 	double time = 0;
83: 	idx_t elements = 0;
84: 	string name;
85: 	explicit OperatorInformation(double time_ = 0, idx_t elements_ = 0) : time(time_), elements(elements_) {
86: 	}
87: 	//! A vector of Expression Executor Info
88: 	vector<unique_ptr<ExpressionExecutorInfo>> executors_info;
89: };
90: 
91: //! The OperatorProfiler measures timings of individual operators
92: class OperatorProfiler {
93: 	friend class QueryProfiler;
94: 
95: public:
96: 	DUCKDB_API explicit OperatorProfiler(bool enabled);
97: 
98: 	DUCKDB_API void StartOperator(const PhysicalOperator *phys_op);
99: 	DUCKDB_API void EndOperator(DataChunk *chunk);
100: 	DUCKDB_API void Flush(const PhysicalOperator *phys_op, ExpressionExecutor *expression_executor, const string &name,
101: 	                      int id);
102: 
103: 	~OperatorProfiler() {
104: 	}
105: 
106: private:
107: 	void AddTiming(const PhysicalOperator *op, double time, idx_t elements);
108: 
109: 	//! Whether or not the profiler is enabled
110: 	bool enabled;
111: 	//! The timer used to time the execution time of the individual Physical Operators
112: 	Profiler<system_clock> op;
113: 	//! The stack of Physical Operators that are currently active
114: 	const PhysicalOperator *active_operator;
115: 	//! A mapping of physical operators to recorded timings
116: 	unordered_map<const PhysicalOperator *, OperatorInformation> timings;
117: };
118: 
119: //! The QueryProfiler can be used to measure timings of queries
120: class QueryProfiler {
121: public:
122: 	DUCKDB_API QueryProfiler()
123: 	    : automatic_print_format(ProfilerPrintFormat::NONE), enabled(false), detailed_enabled(false), running(false),
124: 	      query_requires_profiling(false) {
125: 	}
126: 
127: public:
128: 	struct TreeNode {
129: 		PhysicalOperatorType type;
130: 		string name;
131: 		string extra_info;
132: 		OperatorInformation info;
133: 		vector<unique_ptr<TreeNode>> children;
134: 		idx_t depth = 0;
135: 	};
136: 
137: 	// Propagate save_location, enabled, detailed_enabled and automatic_print_format.
138: 	void Propagate(QueryProfiler &qp);
139: 
140: 	using TreeMap = unordered_map<const PhysicalOperator *, TreeNode *>;
141: 
142: private:
143: 	unique_ptr<TreeNode> CreateTree(PhysicalOperator *root, idx_t depth = 0);
144: 	void Render(const TreeNode &node, std::ostream &str) const;
145: 	//! The lock used for flushing information from a thread into the global query profiler
146: 	mutex flush_lock;
147: 
148: public:
149: 	DUCKDB_API void Enable() {
150: 		enabled = true;
151: 		detailed_enabled = false;
152: 	}
153: 
154: 	DUCKDB_API void DetailedEnable() {
155: 		detailed_enabled = true;
156: 	}
157: 
158: 	DUCKDB_API void Disable() {
159: 		enabled = false;
160: 	}
161: 
162: 	DUCKDB_API bool IsEnabled() {
163: 		return enabled;
164: 	}
165: 
166: 	bool IsDetailedEnabled() const {
167: 		return detailed_enabled;
168: 	}
169: 
170: 	DUCKDB_API void StartQuery(string query);
171: 	DUCKDB_API void EndQuery();
172: 
173: 	//! Adds the timings gathered by an OperatorProfiler to this query profiler
174: 	DUCKDB_API void Flush(OperatorProfiler &profiler);
175: 
176: 	DUCKDB_API void StartPhase(string phase);
177: 	DUCKDB_API void EndPhase();
178: 
179: 	DUCKDB_API void Initialize(PhysicalOperator *root);
180: 
181: 	DUCKDB_API string ToString(bool print_optimizer_output = false) const;
182: 	DUCKDB_API void ToStream(std::ostream &str, bool print_optimizer_output = false) const;
183: 	DUCKDB_API void Print();
184: 
185: 	DUCKDB_API string ToJSON() const;
186: 	DUCKDB_API void WriteToFile(const char *path, string &info) const;
187: 
188: 	//! The format to automatically print query profiling information in (default: disabled)
189: 	ProfilerPrintFormat automatic_print_format;
190: 	//! The file to save query profiling information to, instead of printing it to the console (empty = print to
191: 	//! console)
192: 	string save_location;
193: 
194: 	idx_t OperatorSize() {
195: 		return tree_map.size();
196: 	}
197: 
198: 	void Finalize(TreeNode &node);
199: 
200: private:
201: 	//! Whether or not query profiling is enabled
202: 	bool enabled;
203: 	//! Whether or not detailed query profiling is enabled
204: 	bool detailed_enabled;
205: 	//! Whether or not the query profiler is running
206: 	bool running;
207: 
208: 	bool query_requires_profiling;
209: 
210: 	//! The root of the query tree
211: 	unique_ptr<TreeNode> root;
212: 	//! The query string
213: 	string query;
214: 	//! The timer used to time the execution time of the entire query
215: 	Profiler<system_clock> main_query;
216: 	//! A map of a Physical Operator pointer to a tree node
217: 	TreeMap tree_map;
218: 
219: public:
220: 	const TreeMap &GetTreeMap() const {
221: 		return tree_map;
222: 	}
223: 
224: private:
225: 	//! The timer used to time the individual phases of the planning process
226: 	Profiler<system_clock> phase_profiler;
227: 	//! A mapping of the phase names to the timings
228: 	using PhaseTimingStorage = unordered_map<string, double>;
229: 	PhaseTimingStorage phase_timings;
230: 	using PhaseTimingItem = PhaseTimingStorage::value_type;
231: 	//! The stack of currently active phases
232: 	vector<string> phase_stack;
233: 
234: private:
235: 	vector<PhaseTimingItem> GetOrderedPhaseTimings() const;
236: 
237: 	//! Check whether or not an operator type requires query profiling. If none of the ops in a query require profiling
238: 	//! no profiling information is output.
239: 	bool OperatorRequiresProfiling(PhysicalOperatorType op_type);
240: };
241: 
242: //! The QueryProfilerHistory can be used to access the profiler of previous queries
243: class QueryProfilerHistory {
244: private:
245: 	//! Previous Query profilers
246: 	deque<pair<transaction_t, unique_ptr<QueryProfiler>>> prev_profilers;
247: 	//! Previous Query profilers size
248: 	uint64_t prev_profilers_size = 20;
249: 
250: public:
251: 	deque<pair<transaction_t, unique_ptr<QueryProfiler>>> &GetPrevProfilers() {
252: 		return prev_profilers;
253: 	}
254: 	QueryProfilerHistory() {
255: 	}
256: 
257: 	void SetPrevProfilersSize(uint64_t prevProfilersSize) {
258: 		prev_profilers_size = prevProfilersSize;
259: 	}
260: 	uint64_t GetPrevProfilersSize() const {
261: 		return prev_profilers_size;
262: 	}
263: 
264: public:
265: 	void SetProfilerHistorySize(uint64_t size) {
266: 		this->prev_profilers_size = size;
267: 	}
268: };
269: } // namespace duckdb
[end of src/include/duckdb/main/query_profiler.hpp]
[start of src/include/duckdb/parser/statement/explain_statement.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/parser/statement/explain_statement.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/parser/parsed_expression.hpp"
12: #include "duckdb/parser/sql_statement.hpp"
13: 
14: namespace duckdb {
15: 
16: class ExplainStatement : public SQLStatement {
17: public:
18: 	explicit ExplainStatement(unique_ptr<SQLStatement> stmt);
19: 
20: 	unique_ptr<SQLStatement> stmt;
21: 
22: public:
23: 	unique_ptr<SQLStatement> Copy() const override;
24: };
25: 
26: } // namespace duckdb
[end of src/include/duckdb/parser/statement/explain_statement.hpp]
[start of src/include/duckdb/planner/operator/logical_explain.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/operator/logical_explain.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/planner/logical_operator.hpp"
12: 
13: namespace duckdb {
14: 
15: class LogicalExplain : public LogicalOperator {
16: public:
17: 	explicit LogicalExplain(unique_ptr<LogicalOperator> plan) : LogicalOperator(LogicalOperatorType::LOGICAL_EXPLAIN) {
18: 		children.push_back(move(plan));
19: 	}
20: 
21: 	string physical_plan;
22: 	string logical_plan_unopt;
23: 	string logical_plan_opt;
24: 
25: protected:
26: 	void ResolveTypes() override {
27: 		types = {LogicalType::VARCHAR, LogicalType::VARCHAR};
28: 	}
29: 	vector<ColumnBinding> GetColumnBindings() override {
30: 		return {ColumnBinding(0, 0), ColumnBinding(0, 1)};
31: 	}
32: };
33: } // namespace duckdb
[end of src/include/duckdb/planner/operator/logical_explain.hpp]
[start of src/main/client_context.cpp]
1: #include "duckdb/main/client_context.hpp"
2: 
3: #include "duckdb/main/client_context_file_opener.hpp"
4: #include "duckdb/main/query_profiler.hpp"
5: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
6: #include "duckdb/catalog/catalog_entry/scalar_function_catalog_entry.hpp"
7: #include "duckdb/catalog/catalog_search_path.hpp"
8: #include "duckdb/common/serializer/buffered_deserializer.hpp"
9: #include "duckdb/common/serializer/buffered_serializer.hpp"
10: #include "duckdb/execution/physical_plan_generator.hpp"
11: #include "duckdb/main/database.hpp"
12: #include "duckdb/main/materialized_query_result.hpp"
13: #include "duckdb/main/query_result.hpp"
14: #include "duckdb/main/stream_query_result.hpp"
15: #include "duckdb/optimizer/optimizer.hpp"
16: #include "duckdb/parser/parser.hpp"
17: #include "duckdb/parser/expression/constant_expression.hpp"
18: #include "duckdb/parser/parsed_data/create_function_info.hpp"
19: #include "duckdb/parser/statement/drop_statement.hpp"
20: #include "duckdb/parser/statement/execute_statement.hpp"
21: #include "duckdb/parser/statement/explain_statement.hpp"
22: #include "duckdb/parser/statement/prepare_statement.hpp"
23: #include "duckdb/parser/statement/select_statement.hpp"
24: #include "duckdb/planner/operator/logical_execute.hpp"
25: #include "duckdb/planner/planner.hpp"
26: #include "duckdb/transaction/transaction_manager.hpp"
27: #include "duckdb/transaction/transaction.hpp"
28: #include "duckdb/storage/data_table.hpp"
29: #include "duckdb/main/appender.hpp"
30: #include "duckdb/main/relation.hpp"
31: #include "duckdb/parser/statement/relation_statement.hpp"
32: #include "duckdb/parallel/task_scheduler.hpp"
33: #include "duckdb/common/serializer/buffered_file_writer.hpp"
34: #include "duckdb/planner/pragma_handler.hpp"
35: #include "duckdb/common/to_string.hpp"
36: #include "duckdb/common/file_system.hpp"
37: #include "duckdb/execution/column_binding_resolver.hpp"
38: 
39: namespace duckdb {
40: 
41: class ClientContextLock {
42: public:
43: 	explicit ClientContextLock(mutex &context_lock) : client_guard(context_lock) {
44: 	}
45: 
46: 	~ClientContextLock() {
47: 	}
48: 
49: private:
50: 	lock_guard<mutex> client_guard;
51: };
52: 
53: ClientContext::ClientContext(shared_ptr<DatabaseInstance> database)
54:     : profiler(make_unique<QueryProfiler>()), query_profiler_history(make_unique<QueryProfilerHistory>()),
55:       db(move(database)), transaction(db->GetTransactionManager(), *this), interrupted(false), executor(*this),
56:       temporary_objects(make_unique<SchemaCatalogEntry>(&db->GetCatalog(), TEMP_SCHEMA, true)),
57:       catalog_search_path(make_unique<CatalogSearchPath>(*this)),
58:       file_opener(make_unique<ClientContextFileOpener>(*this)), open_result(nullptr) {
59: 	std::random_device rd;
60: 	random_engine.seed(rd());
61: 
62: 	progress_bar = make_unique<ProgressBar>(&executor, wait_time);
63: }
64: 
65: ClientContext::~ClientContext() {
66: 	if (std::uncaught_exception()) {
67: 		return;
68: 	}
69: 	// destroy the client context and rollback if there is an active transaction
70: 	// but only if we are not destroying this client context as part of an exception stack unwind
71: 	Destroy();
72: }
73: 
74: unique_ptr<ClientContextLock> ClientContext::LockContext() {
75: 	return make_unique<ClientContextLock>(context_lock);
76: }
77: 
78: void ClientContext::Destroy() {
79: 	auto lock = LockContext();
80: 	if (transaction.HasActiveTransaction()) {
81: 		ActiveTransaction().active_query = MAXIMUM_QUERY_ID;
82: 		if (!transaction.IsAutoCommit()) {
83: 			transaction.Rollback();
84: 		}
85: 	}
86: 	CleanupInternal(*lock);
87: }
88: 
89: void ClientContext::Cleanup() {
90: 	auto lock = LockContext();
91: 	CleanupInternal(*lock);
92: }
93: 
94: unique_ptr<DataChunk> ClientContext::Fetch() {
95: 	auto lock = LockContext();
96: 	if (!open_result) {
97: 		throw InternalException("Fetch was called, but there is no open result (or the result was previously closed)");
98: 	}
99: 	try {
100: 		// fetch the chunk and return it
101: 		auto chunk = FetchInternal(*lock);
102: 		return chunk;
103: 	} catch (std::exception &ex) {
104: 		open_result->error = ex.what();
105: 	} catch (...) { // LCOV_EXCL_START
106: 		open_result->error = "Unhandled exception in Fetch";
107: 	} // LCOV_EXCL_STOP
108: 	open_result->success = false;
109: 	CleanupInternal(*lock);
110: 	return nullptr;
111: }
112: 
113: string ClientContext::FinalizeQuery(ClientContextLock &lock, bool success) {
114: 	profiler->EndQuery();
115: 	executor.Reset();
116: 
117: 	string error;
118: 	if (transaction.HasActiveTransaction()) {
119: 		ActiveTransaction().active_query = MAXIMUM_QUERY_ID;
120: 		// Move the query profiler into the history
121: 		auto &prev_profilers = query_profiler_history->GetPrevProfilers();
122: 		prev_profilers.emplace_back(transaction.ActiveTransaction().active_query, move(profiler));
123: 		// Reinitialize the query profiler
124: 		profiler = make_unique<QueryProfiler>();
125: 		// Propagate settings of the saved query into the new profiler.
126: 		profiler->Propagate(*prev_profilers.back().second);
127: 		if (prev_profilers.size() >= query_profiler_history->GetPrevProfilersSize()) {
128: 			prev_profilers.pop_front();
129: 		}
130: 		try {
131: 			if (transaction.IsAutoCommit()) {
132: 				if (success) {
133: 					// query was successful: commit
134: 					transaction.Commit();
135: 				} else {
136: 					// query was unsuccessful: rollback
137: 					transaction.Rollback();
138: 				}
139: 			}
140: 		} catch (std::exception &ex) {
141: 			error = ex.what();
142: 		} catch (...) { // LCOV_EXCL_START
143: 			error = "Unhandled exception!";
144: 		} // LCOV_EXCL_STOP
145: 	}
146: 	return error;
147: }
148: 
149: void ClientContext::CleanupInternal(ClientContextLock &lock) {
150: 	if (!open_result) {
151: 		// no result currently open
152: 		return;
153: 	}
154: 
155: 	auto error = FinalizeQuery(lock, open_result->success);
156: 	if (open_result->success) {
157: 		// if an error occurred while committing report it in the result
158: 		open_result->error = error;
159: 		open_result->success = error.empty();
160: 	}
161: 
162: 	open_result->is_open = false;
163: 	open_result = nullptr;
164: 
165: 	this->query = string();
166: }
167: 
168: unique_ptr<DataChunk> ClientContext::FetchInternal(ClientContextLock &) {
169: 	return executor.FetchChunk();
170: }
171: 
172: shared_ptr<PreparedStatementData> ClientContext::CreatePreparedStatement(ClientContextLock &lock, const string &query,
173:                                                                          unique_ptr<SQLStatement> statement) {
174: 	StatementType statement_type = statement->type;
175: 	auto result = make_shared<PreparedStatementData>(statement_type);
176: 
177: 	profiler->StartPhase("planner");
178: 	Planner planner(*this);
179: 	planner.CreatePlan(move(statement));
180: 	D_ASSERT(planner.plan);
181: 	profiler->EndPhase();
182: 
183: 	auto plan = move(planner.plan);
184: #ifdef DEBUG
185: 	plan->Verify();
186: #endif
187: 	// extract the result column names from the plan
188: 	result->read_only = planner.read_only;
189: 	result->requires_valid_transaction = planner.requires_valid_transaction;
190: 	result->allow_stream_result = planner.allow_stream_result;
191: 	result->names = planner.names;
192: 	result->types = planner.types;
193: 	result->value_map = move(planner.value_map);
194: 	result->catalog_version = Transaction::GetTransaction(*this).catalog_version;
195: 
196: 	if (enable_optimizer) {
197: 		profiler->StartPhase("optimizer");
198: 		Optimizer optimizer(*planner.binder, *this);
199: 		plan = optimizer.Optimize(move(plan));
200: 		D_ASSERT(plan);
201: 		profiler->EndPhase();
202: 
203: #ifdef DEBUG
204: 		plan->Verify();
205: #endif
206: 	}
207: 
208: 	profiler->StartPhase("physical_planner");
209: 	// now convert logical query plan into a physical query plan
210: 	PhysicalPlanGenerator physical_planner(*this);
211: 	auto physical_plan = physical_planner.CreatePlan(move(plan));
212: 	profiler->EndPhase();
213: 
214: #ifdef DEBUG
215: 	D_ASSERT(!physical_plan->ToString().empty());
216: #endif
217: 	result->plan = move(physical_plan);
218: 	return result;
219: }
220: 
221: int ClientContext::GetProgress() {
222: 	D_ASSERT(progress_bar);
223: 	return progress_bar->GetCurrentPercentage();
224: }
225: 
226: unique_ptr<QueryResult> ClientContext::ExecutePreparedStatement(ClientContextLock &lock, const string &query,
227:                                                                 shared_ptr<PreparedStatementData> statement_p,
228:                                                                 vector<Value> bound_values, bool allow_stream_result) {
229: 	auto &statement = *statement_p;
230: 	if (ActiveTransaction().IsInvalidated() && statement.requires_valid_transaction) {
231: 		throw Exception("Current transaction is aborted (please ROLLBACK)");
232: 	}
233: 	auto &config = DBConfig::GetConfig(*this);
234: 	if (config.access_mode == AccessMode::READ_ONLY && !statement.read_only) {
235: 		throw Exception(StringUtil::Format("Cannot execute statement of type \"%s\" in read-only mode!",
236: 		                                   StatementTypeToString(statement.statement_type)));
237: 	}
238: 
239: 	// bind the bound values before execution
240: 	statement.Bind(move(bound_values));
241: 
242: 	bool create_stream_result = statement.allow_stream_result && allow_stream_result;
243: 	if (enable_progress_bar) {
244: 		progress_bar->Initialize(wait_time);
245: 		progress_bar->Start();
246: 	}
247: 	// store the physical plan in the context for calls to Fetch()
248: 	executor.Initialize(statement.plan.get());
249: 
250: 	auto types = executor.GetTypes();
251: 
252: 	D_ASSERT(types == statement.types);
253: 
254: 	if (create_stream_result) {
255: 		if (enable_progress_bar) {
256: 			progress_bar->Stop();
257: 		}
258: 		// successfully compiled SELECT clause and it is the last statement
259: 		// return a StreamQueryResult so the client can call Fetch() on it and stream the result
260: 		return make_unique<StreamQueryResult>(statement.statement_type, shared_from_this(), statement.types,
261: 		                                      statement.names, move(statement_p));
262: 	}
263: 	// create a materialized result by continuously fetching
264: 	auto result = make_unique<MaterializedQueryResult>(statement.statement_type, statement.types, statement.names);
265: 	while (true) {
266: 		auto chunk = FetchInternal(lock);
267: 		if (chunk->size() == 0) {
268: 			break;
269: 		}
270: #ifdef DEBUG
271: 		for (idx_t i = 0; i < chunk->ColumnCount(); i++) {
272: 			if (statement.types[i].id() == LogicalTypeId::VARCHAR) {
273: 				chunk->data[i].UTFVerify(chunk->size());
274: 			}
275: 		}
276: #endif
277: 		result->collection.Append(*chunk);
278: 	}
279: 	if (enable_progress_bar) {
280: 		progress_bar->Stop();
281: 	}
282: 	return move(result);
283: }
284: 
285: void ClientContext::InitialCleanup(ClientContextLock &lock) {
286: 	//! Cleanup any open results and reset the interrupted flag
287: 	CleanupInternal(lock);
288: 	interrupted = false;
289: }
290: 
291: vector<unique_ptr<SQLStatement>> ClientContext::ParseStatements(const string &query) {
292: 	auto lock = LockContext();
293: 	return ParseStatementsInternal(*lock, query);
294: }
295: 
296: vector<unique_ptr<SQLStatement>> ClientContext::ParseStatementsInternal(ClientContextLock &lock, const string &query) {
297: 	Parser parser;
298: 	parser.ParseQuery(query);
299: 
300: 	PragmaHandler handler(*this);
301: 	handler.HandlePragmaStatements(lock, parser.statements);
302: 
303: 	return move(parser.statements);
304: }
305: 
306: void ClientContext::HandlePragmaStatements(vector<unique_ptr<SQLStatement>> &statements) {
307: 	auto lock = LockContext();
308: 
309: 	PragmaHandler handler(*this);
310: 	handler.HandlePragmaStatements(*lock, statements);
311: }
312: 
313: unique_ptr<LogicalOperator> ClientContext::ExtractPlan(const string &query) {
314: 	auto lock = LockContext();
315: 
316: 	auto statements = ParseStatementsInternal(*lock, query);
317: 	if (statements.size() != 1) {
318: 		throw Exception("ExtractPlan can only prepare a single statement");
319: 	}
320: 
321: 	unique_ptr<LogicalOperator> plan;
322: 	RunFunctionInTransactionInternal(*lock, [&]() {
323: 		Planner planner(*this);
324: 		planner.CreatePlan(move(statements[0]));
325: 		D_ASSERT(planner.plan);
326: 
327: 		plan = move(planner.plan);
328: 
329: 		if (enable_optimizer) {
330: 			Optimizer optimizer(*planner.binder, *this);
331: 			plan = optimizer.Optimize(move(plan));
332: 		}
333: 
334: 		ColumnBindingResolver resolver;
335: 		resolver.VisitOperator(*plan);
336: 
337: 		plan->ResolveOperatorTypes();
338: 	});
339: 	return plan;
340: }
341: 
342: unique_ptr<PreparedStatement> ClientContext::PrepareInternal(ClientContextLock &lock,
343:                                                              unique_ptr<SQLStatement> statement) {
344: 	auto n_param = statement->n_param;
345: 	auto statement_query = statement->query;
346: 	shared_ptr<PreparedStatementData> prepared_data;
347: 	auto unbound_statement = statement->Copy();
348: 	RunFunctionInTransactionInternal(
349: 	    lock, [&]() { prepared_data = CreatePreparedStatement(lock, statement_query, move(statement)); }, false);
350: 	prepared_data->unbound_statement = move(unbound_statement);
351: 	return make_unique<PreparedStatement>(shared_from_this(), move(prepared_data), move(statement_query), n_param);
352: }
353: 
354: unique_ptr<PreparedStatement> ClientContext::Prepare(unique_ptr<SQLStatement> statement) {
355: 	auto lock = LockContext();
356: 	// prepare the query
357: 	try {
358: 		InitialCleanup(*lock);
359: 		return PrepareInternal(*lock, move(statement));
360: 	} catch (std::exception &ex) {
361: 		return make_unique<PreparedStatement>(ex.what());
362: 	}
363: }
364: 
365: unique_ptr<PreparedStatement> ClientContext::Prepare(const string &query) {
366: 	auto lock = LockContext();
367: 	// prepare the query
368: 	try {
369: 		InitialCleanup(*lock);
370: 
371: 		// first parse the query
372: 		auto statements = ParseStatementsInternal(*lock, query);
373: 		if (statements.empty()) {
374: 			throw Exception("No statement to prepare!");
375: 		}
376: 		if (statements.size() > 1) {
377: 			throw Exception("Cannot prepare multiple statements at once!");
378: 		}
379: 		return PrepareInternal(*lock, move(statements[0]));
380: 	} catch (std::exception &ex) {
381: 		return make_unique<PreparedStatement>(ex.what());
382: 	}
383: }
384: 
385: unique_ptr<QueryResult> ClientContext::Execute(const string &query, shared_ptr<PreparedStatementData> &prepared,
386:                                                vector<Value> &values, bool allow_stream_result) {
387: 	auto lock = LockContext();
388: 	try {
389: 		InitialCleanup(*lock);
390: 	} catch (std::exception &ex) {
391: 		return make_unique<MaterializedQueryResult>(ex.what());
392: 	}
393: 	LogQueryInternal(*lock, query);
394: 	return RunStatementOrPreparedStatement(*lock, query, nullptr, prepared, &values, allow_stream_result);
395: }
396: 
397: unique_ptr<QueryResult> ClientContext::RunStatementInternal(ClientContextLock &lock, const string &query,
398:                                                             unique_ptr<SQLStatement> statement,
399:                                                             bool allow_stream_result) {
400: 	// prepare the query for execution
401: 	auto prepared = CreatePreparedStatement(lock, query, move(statement));
402: 	// by default, no values are bound
403: 	vector<Value> bound_values;
404: 	// execute the prepared statement
405: 	return ExecutePreparedStatement(lock, query, move(prepared), move(bound_values), allow_stream_result);
406: }
407: 
408: unique_ptr<QueryResult> ClientContext::RunStatementOrPreparedStatement(ClientContextLock &lock, const string &query,
409:                                                                        unique_ptr<SQLStatement> statement,
410:                                                                        shared_ptr<PreparedStatementData> &prepared,
411:                                                                        vector<Value> *values,
412:                                                                        bool allow_stream_result) {
413: 	this->query = query;
414: 
415: 	unique_ptr<QueryResult> result;
416: 	// check if we are on AutoCommit. In this case we should start a transaction.
417: 	if (transaction.IsAutoCommit()) {
418: 		transaction.BeginTransaction();
419: 	}
420: 	ActiveTransaction().active_query = db->GetTransactionManager().GetQueryNumber();
421: 	if (statement && query_verification_enabled) {
422: 		// query verification is enabled
423: 		// create a copy of the statement, and use the copy
424: 		// this way we verify that the copy correctly copies all properties
425: 		auto copied_statement = statement->Copy();
426: 		if (statement->type == StatementType::SELECT_STATEMENT) {
427: 			// in case this is a select query, we verify the original statement
428: 			string error = VerifyQuery(lock, query, move(statement));
429: 			if (!error.empty()) {
430: 				// query failed: abort now
431: 				FinalizeQuery(lock, false);
432: 				// error in verifying query
433: 				return make_unique<MaterializedQueryResult>(error);
434: 			}
435: 		}
436: 		statement = move(copied_statement);
437: 	}
438: 	// start the profiler
439: 	profiler->StartQuery(query);
440: 	try {
441: 		if (statement) {
442: 			result = RunStatementInternal(lock, query, move(statement), allow_stream_result);
443: 		} else {
444: 			auto &catalog = Catalog::GetCatalog(*this);
445: 			if (prepared->unbound_statement && catalog.GetCatalogVersion() != prepared->catalog_version) {
446: 				D_ASSERT(prepared->unbound_statement.get());
447: 				// catalog was modified: rebind the statement before execution
448: 				auto new_prepared = CreatePreparedStatement(lock, query, prepared->unbound_statement->Copy());
449: 				if (prepared->types != new_prepared->types) {
450: 					throw BinderException("Rebinding statement after catalog change resulted in change of types");
451: 				}
452: 				new_prepared->unbound_statement = move(prepared->unbound_statement);
453: 				prepared = move(new_prepared);
454: 			}
455: 			result = ExecutePreparedStatement(lock, query, prepared, *values, allow_stream_result);
456: 		}
457: 	} catch (StandardException &ex) {
458: 		// standard exceptions do not invalidate the current transaction
459: 		result = make_unique<MaterializedQueryResult>(ex.what());
460: 	} catch (std::exception &ex) {
461: 		// other types of exceptions do invalidate the current transaction
462: 		if (transaction.HasActiveTransaction()) {
463: 			ActiveTransaction().Invalidate();
464: 		}
465: 		result = make_unique<MaterializedQueryResult>(ex.what());
466: 	}
467: 	if (!result->success) {
468: 		// initial failures should always be reported as MaterializedResult
469: 		D_ASSERT(result->type != QueryResultType::STREAM_RESULT);
470: 		// query failed: abort now
471: 		FinalizeQuery(lock, false);
472: 		return result;
473: 	}
474: 	// query succeeded, append to list of results
475: 	if (result->type == QueryResultType::STREAM_RESULT) {
476: 		// store as currently open result if it is a stream result
477: 		this->open_result = (StreamQueryResult *)result.get();
478: 	} else {
479: 		// finalize the query if it is not a stream result
480: 		string error = FinalizeQuery(lock, true);
481: 		if (!error.empty()) {
482: 			// failure in committing transaction
483: 			return make_unique<MaterializedQueryResult>(error);
484: 		}
485: 	}
486: 	return result;
487: }
488: 
489: unique_ptr<QueryResult> ClientContext::RunStatement(ClientContextLock &lock, const string &query,
490:                                                     unique_ptr<SQLStatement> statement, bool allow_stream_result) {
491: 	shared_ptr<PreparedStatementData> prepared;
492: 	return RunStatementOrPreparedStatement(lock, query, move(statement), prepared, nullptr, allow_stream_result);
493: }
494: 
495: unique_ptr<QueryResult> ClientContext::RunStatements(ClientContextLock &lock, const string &query,
496:                                                      vector<unique_ptr<SQLStatement>> &statements,
497:                                                      bool allow_stream_result) {
498: 	// now we have a list of statements
499: 	// iterate over them and execute them one by one
500: 	unique_ptr<QueryResult> result;
501: 	QueryResult *last_result = nullptr;
502: 	for (idx_t i = 0; i < statements.size(); i++) {
503: 		auto &statement = statements[i];
504: 		bool is_last_statement = i + 1 == statements.size();
505: 		auto current_result = RunStatement(lock, query, move(statement), allow_stream_result && is_last_statement);
506: 		// now append the result to the list of results
507: 		if (!last_result) {
508: 			// first result of the query
509: 			result = move(current_result);
510: 			last_result = result.get();
511: 		} else {
512: 			// later results; attach to the result chain
513: 			last_result->next = move(current_result);
514: 			last_result = last_result->next.get();
515: 		}
516: 	}
517: 	return result;
518: }
519: 
520: void ClientContext::LogQueryInternal(ClientContextLock &, const string &query) {
521: 	if (!log_query_writer) {
522: #ifdef DUCKDB_FORCE_QUERY_LOG
523: 		try {
524: 			string log_path(DUCKDB_FORCE_QUERY_LOG);
525: 			log_query_writer = make_unique<BufferedFileWriter>(
526: 			    FileSystem::GetFileSystem(*this), log_path, BufferedFileWriter::DEFAULT_OPEN_FLAGS, file_opener.get());
527: 		} catch (...) {
528: 			return;
529: 		}
530: #else
531: 		return;
532: #endif
533: 	}
534: 	// log query path is set: log the query
535: 	log_query_writer->WriteData((const_data_ptr_t)query.c_str(), query.size());
536: 	log_query_writer->WriteData((const_data_ptr_t) "\n", 1);
537: 	log_query_writer->Flush();
538: 	log_query_writer->Sync();
539: }
540: 
541: unique_ptr<QueryResult> ClientContext::Query(unique_ptr<SQLStatement> statement, bool allow_stream_result) {
542: 	auto lock = LockContext();
543: 	LogQueryInternal(*lock, statement->query.substr(statement->stmt_location, statement->stmt_length));
544: 
545: 	vector<unique_ptr<SQLStatement>> statements;
546: 	statements.push_back(move(statement));
547: 
548: 	return RunStatements(*lock, query, statements, allow_stream_result);
549: }
550: 
551: unique_ptr<QueryResult> ClientContext::Query(const string &query, bool allow_stream_result) {
552: 	auto lock = LockContext();
553: 	LogQueryInternal(*lock, query);
554: 
555: 	vector<unique_ptr<SQLStatement>> statements;
556: 	try {
557: 		InitialCleanup(*lock);
558: 		// parse the query and transform it into a set of statements
559: 		statements = ParseStatementsInternal(*lock, query);
560: 	} catch (std::exception &ex) {
561: 		return make_unique<MaterializedQueryResult>(ex.what());
562: 	}
563: 
564: 	if (statements.empty()) {
565: 		// no statements, return empty successful result
566: 		return make_unique<MaterializedQueryResult>(StatementType::INVALID_STATEMENT);
567: 	}
568: 
569: 	return RunStatements(*lock, query, statements, allow_stream_result);
570: }
571: 
572: void ClientContext::Interrupt() {
573: 	interrupted = true;
574: }
575: 
576: void ClientContext::EnableProfiling() {
577: 	auto lock = LockContext();
578: 	profiler->Enable();
579: }
580: 
581: void ClientContext::DisableProfiling() {
582: 	auto lock = LockContext();
583: 	profiler->Disable();
584: }
585: 
586: string ClientContext::VerifyQuery(ClientContextLock &lock, const string &query, unique_ptr<SQLStatement> statement) {
587: 	D_ASSERT(statement->type == StatementType::SELECT_STATEMENT);
588: 	// aggressive query verification
589: 
590: 	// the purpose of this function is to test correctness of otherwise hard to test features:
591: 	// Copy() of statements and expressions
592: 	// Serialize()/Deserialize() of expressions
593: 	// Hash() of expressions
594: 	// Equality() of statements and expressions
595: 	// Correctness of plans both with and without optimizers
596: 	// Correctness of plans both with and without parallelism
597: 
598: 	// copy the statement
599: 	auto select_stmt = (SelectStatement *)statement.get();
600: 	auto copied_stmt = unique_ptr_cast<SQLStatement, SelectStatement>(select_stmt->Copy());
601: 	auto unoptimized_stmt = unique_ptr_cast<SQLStatement, SelectStatement>(select_stmt->Copy());
602: 
603: 	BufferedSerializer serializer;
604: 	select_stmt->Serialize(serializer);
605: 	BufferedDeserializer source(serializer);
606: 	auto deserialized_stmt = SelectStatement::Deserialize(source);
607: 	// all the statements should be equal
608: 	D_ASSERT(copied_stmt->Equals(statement.get()));
609: 	D_ASSERT(deserialized_stmt->Equals(statement.get()));
610: 	D_ASSERT(copied_stmt->Equals(deserialized_stmt.get()));
611: 
612: 	// now perform checking on the expressions
613: #ifdef DEBUG
614: 	auto &orig_expr_list = select_stmt->node->GetSelectList();
615: 	auto &de_expr_list = deserialized_stmt->node->GetSelectList();
616: 	auto &cp_expr_list = copied_stmt->node->GetSelectList();
617: 	D_ASSERT(orig_expr_list.size() == de_expr_list.size() && cp_expr_list.size() == de_expr_list.size());
618: 	for (idx_t i = 0; i < orig_expr_list.size(); i++) {
619: 		// run the ToString, to verify that it doesn't crash
620: 		orig_expr_list[i]->ToString();
621: 		// check that the expressions are equivalent
622: 		D_ASSERT(orig_expr_list[i]->Equals(de_expr_list[i].get()));
623: 		D_ASSERT(orig_expr_list[i]->Equals(cp_expr_list[i].get()));
624: 		D_ASSERT(de_expr_list[i]->Equals(cp_expr_list[i].get()));
625: 		// check that the hashes are equivalent too
626: 		D_ASSERT(orig_expr_list[i]->Hash() == de_expr_list[i]->Hash());
627: 		D_ASSERT(orig_expr_list[i]->Hash() == cp_expr_list[i]->Hash());
628: 
629: 		D_ASSERT(!orig_expr_list[i]->Equals(nullptr));
630: 	}
631: 	// now perform additional checking within the expressions
632: 	for (idx_t outer_idx = 0; outer_idx < orig_expr_list.size(); outer_idx++) {
633: 		auto hash = orig_expr_list[outer_idx]->Hash();
634: 		for (idx_t inner_idx = 0; inner_idx < orig_expr_list.size(); inner_idx++) {
635: 			auto hash2 = orig_expr_list[inner_idx]->Hash();
636: 			if (hash != hash2) {
637: 				// if the hashes are not equivalent, the expressions should not be equivalent
638: 				D_ASSERT(!orig_expr_list[outer_idx]->Equals(orig_expr_list[inner_idx].get()));
639: 			}
640: 		}
641: 	}
642: #endif
643: 
644: 	// disable profiling if it is enabled
645: 	bool profiling_is_enabled = profiler->IsEnabled();
646: 	if (profiling_is_enabled) {
647: 		profiler->Disable();
648: 	}
649: 
650: 	// see below
651: 	auto statement_copy_for_explain = select_stmt->Copy();
652: 
653: 	unique_ptr<MaterializedQueryResult> original_result =
654: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT),
655: 	                                    copied_result =
656: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT),
657: 	                                    deserialized_result =
658: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT),
659: 	                                    unoptimized_result =
660: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT);
661: 
662: 	// execute the original statement
663: 	try {
664: 		auto result = RunStatementInternal(lock, query, move(statement), false);
665: 		original_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
666: 	} catch (std::exception &ex) {
667: 		original_result->error = ex.what();
668: 		original_result->success = false;
669: 		interrupted = false;
670: 	}
671: 
672: 	// check explain, only if q does not already contain EXPLAIN
673: 	if (original_result->success) {
674: 		auto explain_q = "EXPLAIN " + query;
675: 		auto explain_stmt = make_unique<ExplainStatement>(move(statement_copy_for_explain));
676: 		try {
677: 			RunStatementInternal(lock, explain_q, move(explain_stmt), false);
678: 		} catch (std::exception &ex) { // LCOV_EXCL_START
679: 			return "EXPLAIN failed but query did not (" + string(ex.what()) + ")";
680: 		} // LCOV_EXCL_STOP
681: 	}
682: 
683: 	// now execute the copied statement
684: 	try {
685: 		auto result = RunStatementInternal(lock, query, move(copied_stmt), false);
686: 		copied_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
687: 	} catch (std::exception &ex) {
688: 		copied_result->error = ex.what();
689: 		copied_result->success = false;
690: 		interrupted = false;
691: 	}
692: 	// now execute the deserialized statement
693: 	try {
694: 		auto result = RunStatementInternal(lock, query, move(deserialized_stmt), false);
695: 		deserialized_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
696: 	} catch (std::exception &ex) {
697: 		deserialized_result->error = ex.what();
698: 		deserialized_result->success = false;
699: 		interrupted = false;
700: 	}
701: 	// now execute the unoptimized statement
702: 	enable_optimizer = false;
703: 	try {
704: 		auto result = RunStatementInternal(lock, query, move(unoptimized_stmt), false);
705: 		unoptimized_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
706: 	} catch (std::exception &ex) {
707: 		unoptimized_result->error = ex.what();
708: 		unoptimized_result->success = false;
709: 		interrupted = false;
710: 	}
711: 	enable_optimizer = true;
712: 
713: 	if (profiling_is_enabled) {
714: 		profiler->Enable();
715: 	}
716: 
717: 	// now compare the results
718: 	// the results of all runs should be identical
719: 	vector<unique_ptr<MaterializedQueryResult>> results;
720: 	results.push_back(move(copied_result));
721: 	results.push_back(move(deserialized_result));
722: 	results.push_back(move(unoptimized_result));
723: 	vector<string> names = {"Copied Result", "Deserialized Result", "Unoptimized Result"};
724: 	for (idx_t i = 0; i < results.size(); i++) {
725: 		if (original_result->success != results[i]->success) { // LCOV_EXCL_START
726: 			string result = names[i] + " differs from original result!\n";
727: 			result += "Original Result:\n" + original_result->ToString();
728: 			result += names[i] + ":\n" + results[i]->ToString();
729: 			return result;
730: 		}                                                                  // LCOV_EXCL_STOP
731: 		if (!original_result->collection.Equals(results[i]->collection)) { // LCOV_EXCL_START
732: 			string result = names[i] + " differs from original result!\n";
733: 			result += "Original Result:\n" + original_result->ToString();
734: 			result += names[i] + ":\n" + results[i]->ToString();
735: 			return result;
736: 		} // LCOV_EXCL_STOP
737: 	}
738: 
739: 	return "";
740: }
741: 
742: bool ClientContext::UpdateFunctionInfoFromEntry(ScalarFunctionCatalogEntry *existing_function,
743:                                                 CreateScalarFunctionInfo *new_info) {
744: 	if (new_info->functions.empty()) {
745: 		throw InternalException("Registering function without scalar function definitions!");
746: 	}
747: 	bool need_rewrite_entry = false;
748: 	idx_t size_new_func = new_info->functions.size();
749: 	for (idx_t exist_idx = 0; exist_idx < existing_function->functions.size(); ++exist_idx) {
750: 		bool can_add = true;
751: 		for (idx_t new_idx = 0; new_idx < size_new_func; ++new_idx) {
752: 			if (new_info->functions[new_idx].Equal(existing_function->functions[exist_idx])) {
753: 				can_add = false;
754: 				break;
755: 			}
756: 		}
757: 		if (can_add) {
758: 			new_info->functions.push_back(existing_function->functions[exist_idx]);
759: 			need_rewrite_entry = true;
760: 		}
761: 	}
762: 	return need_rewrite_entry;
763: }
764: 
765: void ClientContext::RegisterFunction(CreateFunctionInfo *info) {
766: 	RunFunctionInTransaction([&]() {
767: 		auto &catalog = Catalog::GetCatalog(*this);
768: 		ScalarFunctionCatalogEntry *existing_function = (ScalarFunctionCatalogEntry *)catalog.GetEntry(
769: 		    *this, CatalogType::SCALAR_FUNCTION_ENTRY, info->schema, info->name, true);
770: 		if (existing_function) {
771: 			if (UpdateFunctionInfoFromEntry(existing_function, (CreateScalarFunctionInfo *)info)) {
772: 				// function info was updated from catalog entry, rewrite is needed
773: 				info->on_conflict = OnCreateConflict::REPLACE_ON_CONFLICT;
774: 			}
775: 		}
776: 		// create function
777: 		catalog.CreateFunction(*this, info);
778: 	});
779: }
780: 
781: void ClientContext::RunFunctionInTransactionInternal(ClientContextLock &lock, const std::function<void(void)> &fun,
782:                                                      bool requires_valid_transaction) {
783: 	if (requires_valid_transaction && transaction.HasActiveTransaction() &&
784: 	    transaction.ActiveTransaction().IsInvalidated()) {
785: 		throw Exception("Failed: transaction has been invalidated!");
786: 	}
787: 	// check if we are on AutoCommit. In this case we should start a transaction
788: 	bool require_new_transaction = transaction.IsAutoCommit() && !transaction.HasActiveTransaction();
789: 	if (require_new_transaction) {
790: 		transaction.BeginTransaction();
791: 	}
792: 	try {
793: 		fun();
794: 	} catch (StandardException &ex) {
795: 		if (require_new_transaction) {
796: 			transaction.Rollback();
797: 		}
798: 		throw;
799: 	} catch (std::exception &ex) {
800: 		if (require_new_transaction) {
801: 			transaction.Rollback();
802: 		} else {
803: 			ActiveTransaction().Invalidate();
804: 		}
805: 		throw;
806: 	}
807: 	if (require_new_transaction) {
808: 		transaction.Commit();
809: 	}
810: }
811: 
812: void ClientContext::RunFunctionInTransaction(const std::function<void(void)> &fun, bool requires_valid_transaction) {
813: 	auto lock = LockContext();
814: 	RunFunctionInTransactionInternal(*lock, fun, requires_valid_transaction);
815: }
816: 
817: unique_ptr<TableDescription> ClientContext::TableInfo(const string &schema_name, const string &table_name) {
818: 	unique_ptr<TableDescription> result;
819: 	RunFunctionInTransaction([&]() {
820: 		// obtain the table info
821: 		auto &catalog = Catalog::GetCatalog(*this);
822: 		auto table = catalog.GetEntry<TableCatalogEntry>(*this, schema_name, table_name, true);
823: 		if (!table) {
824: 			return;
825: 		}
826: 		// write the table info to the result
827: 		result = make_unique<TableDescription>();
828: 		result->schema = schema_name;
829: 		result->table = table_name;
830: 		for (auto &column : table->columns) {
831: 			result->columns.emplace_back(column.name, column.type);
832: 		}
833: 	});
834: 	return result;
835: }
836: 
837: void ClientContext::Append(TableDescription &description, ChunkCollection &collection) {
838: 	RunFunctionInTransaction([&]() {
839: 		auto &catalog = Catalog::GetCatalog(*this);
840: 		auto table_entry = catalog.GetEntry<TableCatalogEntry>(*this, description.schema, description.table);
841: 		// verify that the table columns and types match up
842: 		if (description.columns.size() != table_entry->columns.size()) {
843: 			throw Exception("Failed to append: table entry has different number of columns!");
844: 		}
845: 		for (idx_t i = 0; i < description.columns.size(); i++) {
846: 			if (description.columns[i].type != table_entry->columns[i].type) {
847: 				throw Exception("Failed to append: table entry has different number of columns!");
848: 			}
849: 		}
850: 		for (auto &chunk : collection.Chunks()) {
851: 			table_entry->storage->Append(*table_entry, *this, *chunk);
852: 		}
853: 	});
854: }
855: 
856: void ClientContext::TryBindRelation(Relation &relation, vector<ColumnDefinition> &result_columns) {
857: #ifdef DEBUG
858: 	D_ASSERT(!relation.GetAlias().empty());
859: 	D_ASSERT(!relation.ToString().empty());
860: #endif
861: 	RunFunctionInTransaction([&]() {
862: 		// bind the expressions
863: 		auto binder = Binder::CreateBinder(*this);
864: 		auto result = relation.Bind(*binder);
865: 		D_ASSERT(result.names.size() == result.types.size());
866: 		for (idx_t i = 0; i < result.names.size(); i++) {
867: 			result_columns.emplace_back(result.names[i], result.types[i]);
868: 		}
869: 	});
870: }
871: 
872: unique_ptr<QueryResult> ClientContext::Execute(const shared_ptr<Relation> &relation) {
873: 	auto lock = LockContext();
874: 	InitialCleanup(*lock);
875: 
876: 	string query;
877: 	if (query_verification_enabled) {
878: 		// run the ToString method of any relation we run, mostly to ensure it doesn't crash
879: 		relation->ToString();
880: 		relation->GetAlias();
881: 		if (relation->IsReadOnly()) {
882: 			// verify read only statements by running a select statement
883: 			auto select = make_unique<SelectStatement>();
884: 			select->node = relation->GetQueryNode();
885: 			RunStatement(*lock, query, move(select), false);
886: 		}
887: 	}
888: 	auto &expected_columns = relation->Columns();
889: 	auto relation_stmt = make_unique<RelationStatement>(relation);
890: 	auto result = RunStatement(*lock, query, move(relation_stmt), false);
891: 	if (!result->success) {
892: 		return result;
893: 	}
894: 	// verify that the result types and result names of the query match the expected result types/names
895: 	if (result->types.size() == expected_columns.size()) {
896: 		bool mismatch = false;
897: 		for (idx_t i = 0; i < result->types.size(); i++) {
898: 			if (result->types[i] != expected_columns[i].type || result->names[i] != expected_columns[i].name) {
899: 				mismatch = true;
900: 				break;
901: 			}
902: 		}
903: 		if (!mismatch) {
904: 			// all is as expected: return the result
905: 			return result;
906: 		}
907: 	}
908: 	// result mismatch
909: 	string err_str = "Result mismatch in query!\nExpected the following columns: [";
910: 	for (idx_t i = 0; i < expected_columns.size(); i++) {
911: 		if (i > 0) {
912: 			err_str += ", ";
913: 		}
914: 		err_str += expected_columns[i].name + " " + expected_columns[i].type.ToString();
915: 	}
916: 	err_str += "]\nBut result contained the following: ";
917: 	for (idx_t i = 0; i < result->types.size(); i++) {
918: 		err_str += i == 0 ? "[" : ", ";
919: 		err_str += result->names[i] + " " + result->types[i].ToString();
920: 	}
921: 	err_str += "]";
922: 	return make_unique<MaterializedQueryResult>(err_str);
923: }
924: 
925: bool ClientContext::TryGetCurrentSetting(const std::string &key, Value &result) {
926: 	const auto &session_config_map = set_variables;
927: 	const auto &global_config_map = db->config.set_variables;
928: 
929: 	auto session_value = session_config_map.find(key);
930: 	bool found_session_value = session_value != session_config_map.end();
931: 	auto global_value = global_config_map.find(key);
932: 	bool found_global_value = global_value != global_config_map.end();
933: 	if (!found_session_value && !found_global_value) {
934: 		return false;
935: 	}
936: 
937: 	result = found_session_value ? session_value->second : global_value->second;
938: 	return true;
939: }
940: 
941: } // namespace duckdb
[end of src/main/client_context.cpp]
[start of src/main/query_profiler.cpp]
1: #include "duckdb/main/query_profiler.hpp"
2: #include "duckdb/common/to_string.hpp"
3: #include "duckdb/common/fstream.hpp"
4: #include "duckdb/common/printer.hpp"
5: #include "duckdb/common/string_util.hpp"
6: #include "duckdb/execution/physical_operator.hpp"
7: #include "duckdb/execution/operator/join/physical_delim_join.hpp"
8: #include "duckdb/execution/operator/helper/physical_execute.hpp"
9: #include "duckdb/common/tree_renderer.hpp"
10: #include "duckdb/parser/sql_statement.hpp"
11: #include "duckdb/common/limits.hpp"
12: #include "duckdb/execution/expression_executor.hpp"
13: #include "duckdb/planner/expression/bound_function_expression.hpp"
14: #include <utility>
15: #include <algorithm>
16: 
17: namespace duckdb {
18: 
19: void QueryProfiler::StartQuery(string query) {
20: 	if (!enabled) {
21: 		return;
22: 	}
23: 	this->running = true;
24: 	this->query = move(query);
25: 	tree_map.clear();
26: 	root = nullptr;
27: 	phase_timings.clear();
28: 	phase_stack.clear();
29: 
30: 	main_query.Start();
31: }
32: 
33: bool QueryProfiler::OperatorRequiresProfiling(PhysicalOperatorType op_type) {
34: 	switch (op_type) {
35: 	case PhysicalOperatorType::ORDER_BY:
36: 	case PhysicalOperatorType::RESERVOIR_SAMPLE:
37: 	case PhysicalOperatorType::STREAMING_SAMPLE:
38: 	case PhysicalOperatorType::LIMIT:
39: 	case PhysicalOperatorType::TOP_N:
40: 	case PhysicalOperatorType::WINDOW:
41: 	case PhysicalOperatorType::UNNEST:
42: 	case PhysicalOperatorType::SIMPLE_AGGREGATE:
43: 	case PhysicalOperatorType::HASH_GROUP_BY:
44: 	case PhysicalOperatorType::FILTER:
45: 	case PhysicalOperatorType::PROJECTION:
46: 	case PhysicalOperatorType::COPY_TO_FILE:
47: 	case PhysicalOperatorType::TABLE_SCAN:
48: 	case PhysicalOperatorType::CHUNK_SCAN:
49: 	case PhysicalOperatorType::DELIM_SCAN:
50: 	case PhysicalOperatorType::EXPRESSION_SCAN:
51: 	case PhysicalOperatorType::BLOCKWISE_NL_JOIN:
52: 	case PhysicalOperatorType::NESTED_LOOP_JOIN:
53: 	case PhysicalOperatorType::HASH_JOIN:
54: 	case PhysicalOperatorType::CROSS_PRODUCT:
55: 	case PhysicalOperatorType::PIECEWISE_MERGE_JOIN:
56: 	case PhysicalOperatorType::DELIM_JOIN:
57: 	case PhysicalOperatorType::UNION:
58: 	case PhysicalOperatorType::RECURSIVE_CTE:
59: 	case PhysicalOperatorType::EMPTY_RESULT:
60: 		return true;
61: 	default:
62: 		return false;
63: 	}
64: }
65: 
66: void QueryProfiler::Finalize(TreeNode &node) {
67: 	for (auto &child : node.children) {
68: 		Finalize(*child);
69: 		if (node.type == PhysicalOperatorType::UNION) {
70: 			node.info.elements += child->info.elements;
71: 		}
72: 	}
73: }
74: 
75: void QueryProfiler::EndQuery() {
76: 	if (!enabled || !running) {
77: 		return;
78: 	}
79: 
80: 	main_query.End();
81: 	if (root) {
82: 		Finalize(*root);
83: 	}
84: 	this->running = false;
85: 	// print or output the query profiling after termination, if this is enabled
86: 	if (automatic_print_format != ProfilerPrintFormat::NONE) {
87: 		// check if this query should be output based on the operator types
88: 		string query_info;
89: 		if (automatic_print_format == ProfilerPrintFormat::JSON) {
90: 			query_info = ToJSON();
91: 		} else if (automatic_print_format == ProfilerPrintFormat::QUERY_TREE) {
92: 			query_info = ToString();
93: 		} else if (automatic_print_format == ProfilerPrintFormat::QUERY_TREE_OPTIMIZER) {
94: 			query_info = ToString(true);
95: 		}
96: 
97: 		if (save_location.empty()) {
98: 			Printer::Print(query_info);
99: 			Printer::Print("\n");
100: 		} else {
101: 			WriteToFile(save_location.c_str(), query_info);
102: 		}
103: 	}
104: }
105: 
106: void QueryProfiler::StartPhase(string new_phase) {
107: 	if (!enabled || !running) {
108: 		return;
109: 	}
110: 
111: 	if (!phase_stack.empty()) {
112: 		// there are active phases
113: 		phase_profiler.End();
114: 		// add the timing to all phases prior to this one
115: 		string prefix = "";
116: 		for (auto &phase : phase_stack) {
117: 			phase_timings[phase] += phase_profiler.Elapsed();
118: 			prefix += phase + " > ";
119: 		}
120: 		// when there are previous phases, we prefix the current phase with those phases
121: 		new_phase = prefix + new_phase;
122: 	}
123: 
124: 	// start a new phase
125: 	phase_stack.push_back(new_phase);
126: 	// restart the timer
127: 	phase_profiler.Start();
128: }
129: 
130: void QueryProfiler::EndPhase() {
131: 	if (!enabled || !running) {
132: 		return;
133: 	}
134: 	D_ASSERT(phase_stack.size() > 0);
135: 
136: 	// end the timer
137: 	phase_profiler.End();
138: 	// add the timing to all currently active phases
139: 	for (auto &phase : phase_stack) {
140: 		phase_timings[phase] += phase_profiler.Elapsed();
141: 	}
142: 	// now remove the last added phase
143: 	phase_stack.pop_back();
144: 
145: 	if (!phase_stack.empty()) {
146: 		phase_profiler.Start();
147: 	}
148: }
149: 
150: void QueryProfiler::Initialize(PhysicalOperator *root_op) {
151: 	if (!enabled || !running) {
152: 		return;
153: 	}
154: 	this->query_requires_profiling = false;
155: 	this->root = CreateTree(root_op);
156: 	if (!query_requires_profiling) {
157: 		// query does not require profiling: disable profiling for this query
158: 		this->running = false;
159: 		tree_map.clear();
160: 		root = nullptr;
161: 		phase_timings.clear();
162: 		phase_stack.clear();
163: 	}
164: }
165: 
166: OperatorProfiler::OperatorProfiler(bool enabled_p) : enabled(enabled_p), active_operator(nullptr) {
167: }
168: 
169: void OperatorProfiler::StartOperator(const PhysicalOperator *phys_op) {
170: 	if (!enabled) {
171: 		return;
172: 	}
173: 
174: 	if (active_operator) {
175: 		throw InternalException("OperatorProfiler: Attempting to call StartOperator while another operator is active");
176: 	}
177: 
178: 	active_operator = phys_op;
179: 
180: 	// start timing for current element
181: 	op.Start();
182: }
183: 
184: void OperatorProfiler::EndOperator(DataChunk *chunk) {
185: 	if (!enabled) {
186: 		return;
187: 	}
188: 
189: 	if (!active_operator) {
190: 		throw InternalException("OperatorProfiler: Attempting to call EndOperator while another operator is active");
191: 	}
192: 
193: 	// finish timing for the current element
194: 	op.End();
195: 
196: 	AddTiming(active_operator, op.Elapsed(), chunk ? chunk->size() : 0);
197: 	active_operator = nullptr;
198: }
199: 
200: void OperatorProfiler::AddTiming(const PhysicalOperator *op, double time, idx_t elements) {
201: 	if (!enabled) {
202: 		return;
203: 	}
204: 	if (!Value::DoubleIsValid(time)) {
205: 		return;
206: 	}
207: 	auto entry = timings.find(op);
208: 	if (entry == timings.end()) {
209: 		// add new entry
210: 		timings[op] = OperatorInformation(time, elements);
211: 	} else {
212: 		// add to existing entry
213: 		entry->second.time += time;
214: 		entry->second.elements += elements;
215: 	}
216: }
217: void OperatorProfiler::Flush(const PhysicalOperator *phys_op, ExpressionExecutor *expression_executor,
218:                              const string &name, int id) {
219: 	auto entry = timings.find(phys_op);
220: 	if (entry == timings.end()) {
221: 		return;
222: 	}
223: 	auto &operator_timing = timings.find(phys_op)->second;
224: 	if (int(operator_timing.executors_info.size()) <= id) {
225: 		operator_timing.executors_info.resize(id + 1);
226: 	}
227: 	operator_timing.executors_info[id] = make_unique<ExpressionExecutorInfo>(*expression_executor, name, id);
228: 	operator_timing.name = phys_op->GetName();
229: }
230: 
231: void QueryProfiler::Flush(OperatorProfiler &profiler) {
232: 	if (!enabled || !running) {
233: 		return;
234: 	}
235: 	lock_guard<mutex> guard(flush_lock);
236: 	for (auto &node : profiler.timings) {
237: 		auto entry = tree_map.find(node.first);
238: 		D_ASSERT(entry != tree_map.end());
239: 
240: 		entry->second->info.time += node.second.time;
241: 		entry->second->info.elements += node.second.elements;
242: 		if (!detailed_enabled) {
243: 			continue;
244: 		}
245: 		for (auto &info : node.second.executors_info) {
246: 			if (!info) {
247: 				continue;
248: 			}
249: 			auto info_id = info->id;
250: 			if (int(entry->second->info.executors_info.size()) <= info_id) {
251: 				entry->second->info.executors_info.resize(info_id + 1);
252: 			}
253: 			entry->second->info.executors_info[info_id] = move(info);
254: 		}
255: 	}
256: 	profiler.timings.clear();
257: }
258: 
259: static string DrawPadded(const string &str, idx_t width) {
260: 	if (str.size() > width) {
261: 		return str.substr(0, width);
262: 	} else {
263: 		width -= str.size();
264: 		int half_spaces = width / 2;
265: 		int extra_left_space = width % 2 != 0 ? 1 : 0;
266: 		return string(half_spaces + extra_left_space, ' ') + str + string(half_spaces, ' ');
267: 	}
268: }
269: 
270: static string RenderTitleCase(string str) {
271: 	str = StringUtil::Lower(str);
272: 	str[0] = toupper(str[0]);
273: 	for (idx_t i = 0; i < str.size(); i++) {
274: 		if (str[i] == '_') {
275: 			str[i] = ' ';
276: 			if (i + 1 < str.size()) {
277: 				str[i + 1] = toupper(str[i + 1]);
278: 			}
279: 		}
280: 	}
281: 	return str;
282: }
283: 
284: static string RenderTiming(double timing) {
285: 	string timing_s;
286: 	if (timing >= 1) {
287: 		timing_s = StringUtil::Format("%.2f", timing);
288: 	} else if (timing >= 0.1) {
289: 		timing_s = StringUtil::Format("%.3f", timing);
290: 	} else {
291: 		timing_s = StringUtil::Format("%.4f", timing);
292: 	}
293: 	return timing_s + "s";
294: }
295: 
296: string QueryProfiler::ToString(bool print_optimizer_output) const {
297: 	std::stringstream str;
298: 	ToStream(str, print_optimizer_output);
299: 	return str.str();
300: }
301: 
302: void QueryProfiler::ToStream(std::ostream &ss, bool print_optimizer_output) const {
303: 	if (!enabled) {
304: 		ss << "Query profiling is disabled. Call "
305: 		      "Connection::EnableProfiling() to enable profiling!";
306: 		return;
307: 	}
308: 	ss << "\n";
309: 	ss << "\n";
310: 	ss << "    Query Profiling Information    \n";
311: 	ss << "\n";
312: 	ss << "\n";
313: 	ss << StringUtil::Replace(query, "\n", " ") + "\n";
314: 	if (query.empty()) {
315: 		return;
316: 	}
317: 
318: 	constexpr idx_t TOTAL_BOX_WIDTH = 39;
319: 	ss << "\n";
320: 	ss << "\n";
321: 	string total_time = "Total Time: " + RenderTiming(main_query.Elapsed());
322: 	ss << "" + DrawPadded(total_time, TOTAL_BOX_WIDTH - 4) + "\n";
323: 	ss << "\n";
324: 	ss << "\n";
325: 	// print phase timings
326: 	if (print_optimizer_output) {
327: 		bool has_previous_phase = false;
328: 		for (const auto &entry : GetOrderedPhaseTimings()) {
329: 			if (!StringUtil::Contains(entry.first, " > ")) {
330: 				// primary phase!
331: 				if (has_previous_phase) {
332: 					ss << "\n";
333: 					ss << "\n";
334: 				}
335: 				ss << "\n";
336: 				ss << "" +
337: 				          DrawPadded(RenderTitleCase(entry.first) + ": " + RenderTiming(entry.second),
338: 				                     TOTAL_BOX_WIDTH - 2) +
339: 				          "\n";
340: 				ss << "\n";
341: 				has_previous_phase = true;
342: 			} else {
343: 				string entry_name = StringUtil::Split(entry.first, " > ")[1];
344: 				ss << "" +
345: 				          DrawPadded(RenderTitleCase(entry_name) + ": " + RenderTiming(entry.second),
346: 				                     TOTAL_BOX_WIDTH - 4) +
347: 				          "\n";
348: 			}
349: 		}
350: 		if (has_previous_phase) {
351: 			ss << "\n";
352: 			ss << "\n";
353: 		}
354: 	}
355: 	// render the main operator tree
356: 	if (root) {
357: 		Render(*root, ss);
358: 	}
359: }
360: 
361: static string JSONSanitize(const string &text) {
362: 	string result;
363: 	result.reserve(text.size());
364: 	for (idx_t i = 0; i < text.size(); i++) {
365: 		switch (text[i]) {
366: 		case '\b':
367: 			result += "\\b";
368: 			break;
369: 		case '\f':
370: 			result += "\\f";
371: 			break;
372: 		case '\n':
373: 			result += "\\n";
374: 			break;
375: 		case '\r':
376: 			result += "\\r";
377: 			break;
378: 		case '\t':
379: 			result += "\\t";
380: 			break;
381: 		case '"':
382: 			result += "\\\"";
383: 			break;
384: 		case '\\':
385: 			result += "\\\\";
386: 			break;
387: 		default:
388: 			result += text[i];
389: 			break;
390: 		}
391: 	}
392: 	return result;
393: }
394: 
395: // Print a row
396: static void PrintRow(std::ostream &ss, const string &annotation, int id, const string &name, double time,
397:                      int sample_counter, int tuple_counter, const string &extra_info, int depth) {
398: 	ss << string(depth * 3, ' ') << " {\n";
399: 	ss << string(depth * 3, ' ') << "   \"annotation\": \"" + JSONSanitize(annotation) + "\",\n";
400: 	ss << string(depth * 3, ' ') << "   \"id\": " + to_string(id) + ",\n";
401: 	ss << string(depth * 3, ' ') << "   \"name\": \"" + JSONSanitize(name) + "\",\n";
402: #if defined(RDTSC)
403: 	ss << string(depth * 3, ' ') << "   \"timing\": \"NULL\" ,\n";
404: 	ss << string(depth * 3, ' ') << "   \"cycles_per_tuple\": " + StringUtil::Format("%.4f", time) + ",\n";
405: #else
406: 	ss << string(depth * 3, ' ') << "   \"timing\":" + to_string(time) + ",\n";
407: 	ss << string(depth * 3, ' ') << "   \"cycles_per_tuple\": \"NULL\" ,\n";
408: #endif
409: 	ss << string(depth * 3, ' ') << "   \"sample_size\": " << to_string(sample_counter) + ",\n";
410: 	ss << string(depth * 3, ' ') << "   \"input_size\": " << to_string(tuple_counter) + ",\n";
411: 	ss << string(depth * 3, ' ') << "   \"extra_info\": \"" << JSONSanitize(extra_info) + "\"\n";
412: 	ss << string(depth * 3, ' ') << " },\n";
413: }
414: 
415: static void ExtractFunctions(std::ostream &ss, ExpressionInfo &info, int &fun_id, int depth) {
416: 	if (info.hasfunction) {
417: 		D_ASSERT(info.sample_tuples_count != 0);
418: 		PrintRow(ss, "Function", fun_id++, info.function_name,
419: 		         int(info.function_time) / double(info.sample_tuples_count), info.sample_tuples_count,
420: 		         info.tuples_count, "", depth);
421: 	}
422: 	if (info.children.empty()) {
423: 		return;
424: 	}
425: 	// extract the children of this node
426: 	for (auto &child : info.children) {
427: 		ExtractFunctions(ss, *child, fun_id, depth);
428: 	}
429: }
430: 
431: static void ToJSONRecursive(QueryProfiler::TreeNode &node, std::ostream &ss, int depth = 1) {
432: 	ss << string(depth * 3, ' ') << " {\n";
433: 	ss << string(depth * 3, ' ') << "   \"name\": \"" + JSONSanitize(node.name) + "\",\n";
434: 	ss << string(depth * 3, ' ') << "   \"timing\":" + to_string(node.info.time) + ",\n";
435: 	ss << string(depth * 3, ' ') << "   \"cardinality\":" + to_string(node.info.elements) + ",\n";
436: 	ss << string(depth * 3, ' ') << "   \"extra_info\": \"" + JSONSanitize(node.extra_info) + "\",\n";
437: 	ss << string(depth * 3, ' ') << "   \"timings\": [";
438: 	int32_t function_counter = 1;
439: 	int32_t expression_counter = 1;
440: 	ss << "\n ";
441: 	for (auto &expr_executor : node.info.executors_info) {
442: 		// For each Expression tree
443: 		if (!expr_executor) {
444: 			continue;
445: 		}
446: 		for (auto &expr_timer : expr_executor->roots) {
447: 			D_ASSERT(expr_timer->sample_tuples_count != 0);
448: 			PrintRow(ss, "ExpressionRoot", expression_counter++, expr_timer->name,
449: 			         int(expr_timer->time) / double(expr_timer->sample_tuples_count), expr_timer->sample_tuples_count,
450: 			         expr_timer->tuples_count, expr_timer->extra_info, depth + 1);
451: 			// Extract all functions inside the tree
452: 			ExtractFunctions(ss, *expr_timer->root, function_counter, depth + 1);
453: 		}
454: 	}
455: 	ss.seekp(-2, ss.cur);
456: 	ss << "\n";
457: 	ss << string(depth * 3, ' ') << "   ],\n";
458: 	ss << string(depth * 3, ' ') << "   \"children\": [\n";
459: 	if (node.children.empty()) {
460: 		ss << string(depth * 3, ' ') << "   ]\n";
461: 	} else {
462: 		for (idx_t i = 0; i < node.children.size(); i++) {
463: 			if (i > 0) {
464: 				ss << ",\n";
465: 			}
466: 			ToJSONRecursive(*node.children[i], ss, depth + 1);
467: 		}
468: 		ss << string(depth * 3, ' ') << "   ]\n";
469: 	}
470: 	ss << string(depth * 3, ' ') << " }\n";
471: }
472: 
473: string QueryProfiler::ToJSON() const {
474: 	if (!enabled) {
475: 		return "{ \"result\": \"disabled\" }\n";
476: 	}
477: 	if (query.empty()) {
478: 		return "{ \"result\": \"empty\" }\n";
479: 	}
480: 	if (!root) {
481: 		return "{ \"result\": \"error\" }\n";
482: 	}
483: 	std::stringstream ss;
484: 	ss << "{\n";
485: 	ss << "   \"name\":  \"Query\", \n";
486: 	ss << "   \"result\": " + to_string(main_query.Elapsed()) + ",\n";
487: 	ss << "   \"timing\": " + to_string(main_query.Elapsed()) + ",\n";
488: 	ss << "   \"cardinality\": " + to_string(root->info.elements) + ",\n";
489: 	// JSON cannot have literal control characters in string literals
490: 	string extra_info = JSONSanitize(query);
491: 	ss << "   \"extra-info\": \"" + extra_info + "\", \n";
492: 	// print the phase timings
493: 	ss << "   \"timings\": [\n";
494: 	const auto &ordered_phase_timings = GetOrderedPhaseTimings();
495: 	for (idx_t i = 0; i < ordered_phase_timings.size(); i++) {
496: 		if (i > 0) {
497: 			ss << ",\n";
498: 		}
499: 		ss << "   {\n";
500: 		ss << "   \"annotation\": \"" + ordered_phase_timings[i].first + "\", \n";
501: 		ss << "   \"timing\": " + to_string(ordered_phase_timings[i].second) + "\n";
502: 		ss << "   }";
503: 	}
504: 	ss << "\n";
505: 	ss << "   ],\n";
506: 	// recursively print the physical operator tree
507: 	ss << "   \"children\": [\n";
508: 	ToJSONRecursive(*root, ss);
509: 	ss << "   ]\n";
510: 	ss << "}";
511: 	return ss.str();
512: }
513: 
514: void QueryProfiler::WriteToFile(const char *path, string &info) const {
515: 	ofstream out(path);
516: 	out << info;
517: 	out.close();
518: 	// throw an IO exception if it fails to write the file
519: 	if (out.fail()) {
520: 		throw IOException(strerror(errno));
521: 	}
522: }
523: 
524: unique_ptr<QueryProfiler::TreeNode> QueryProfiler::CreateTree(PhysicalOperator *root, idx_t depth) {
525: 	if (OperatorRequiresProfiling(root->type)) {
526: 		this->query_requires_profiling = true;
527: 	}
528: 	auto node = make_unique<QueryProfiler::TreeNode>();
529: 	node->type = root->type;
530: 	node->name = root->GetName();
531: 	node->extra_info = root->ParamsToString();
532: 	node->depth = depth;
533: 	tree_map[root] = node.get();
534: 	for (auto &child : root->children) {
535: 		auto child_node = CreateTree(child.get(), depth + 1);
536: 		node->children.push_back(move(child_node));
537: 	}
538: 	switch (root->type) {
539: 	case PhysicalOperatorType::DELIM_JOIN: {
540: 		auto &delim_join = (PhysicalDelimJoin &)*root;
541: 		auto child_node = CreateTree((PhysicalOperator *)delim_join.join.get(), depth + 1);
542: 		node->children.push_back(move(child_node));
543: 		child_node = CreateTree((PhysicalOperator *)delim_join.distinct.get(), depth + 1);
544: 		node->children.push_back(move(child_node));
545: 		break;
546: 	}
547: 	case PhysicalOperatorType::EXECUTE: {
548: 		auto &execute = (PhysicalExecute &)*root;
549: 		auto child_node = CreateTree((PhysicalOperator *)execute.plan, depth + 1);
550: 		node->children.push_back(move(child_node));
551: 		break;
552: 	}
553: 	default:
554: 		break;
555: 	}
556: 	return node;
557: }
558: 
559: void QueryProfiler::Render(const QueryProfiler::TreeNode &node, std::ostream &ss) const {
560: 	TreeRenderer renderer;
561: 	if (IsDetailedEnabled()) {
562: 		renderer.EnableDetailed();
563: 	} else {
564: 		renderer.EnableStandard();
565: 	}
566: 	renderer.Render(node, ss);
567: }
568: 
569: void QueryProfiler::Print() {
570: 	Printer::Print(ToString());
571: }
572: 
573: vector<QueryProfiler::PhaseTimingItem> QueryProfiler::GetOrderedPhaseTimings() const {
574: 	vector<PhaseTimingItem> result;
575: 	// first sort the phases alphabetically
576: 	vector<string> phases;
577: 	for (auto &entry : phase_timings) {
578: 		phases.push_back(entry.first);
579: 	}
580: 	std::sort(phases.begin(), phases.end());
581: 	for (const auto &phase : phases) {
582: 		auto entry = phase_timings.find(phase);
583: 		D_ASSERT(entry != phase_timings.end());
584: 		result.emplace_back(entry->first, entry->second);
585: 	}
586: 	return result;
587: }
588: void QueryProfiler::Propagate(QueryProfiler &qp) {
589: 	this->automatic_print_format = qp.automatic_print_format;
590: 	this->save_location = qp.save_location;
591: 	this->enabled = qp.enabled;
592: 	this->detailed_enabled = qp.detailed_enabled;
593: }
594: 
595: void ExpressionInfo::ExtractExpressionsRecursive(unique_ptr<ExpressionState> &state) {
596: 	if (state->child_states.empty()) {
597: 		return;
598: 	}
599: 	// extract the children of this node
600: 	for (auto &child : state->child_states) {
601: 		auto expr_info = make_unique<ExpressionInfo>();
602: 		if (child->expr.expression_class == ExpressionClass::BOUND_FUNCTION) {
603: 			expr_info->hasfunction = true;
604: 			expr_info->function_name = ((BoundFunctionExpression &)child->expr).function.ToString();
605: 			expr_info->function_time = child->profiler.time;
606: 			expr_info->sample_tuples_count = child->profiler.sample_tuples_count;
607: 			expr_info->tuples_count = child->profiler.tuples_count;
608: 		}
609: 		expr_info->ExtractExpressionsRecursive(child);
610: 		children.push_back(move(expr_info));
611: 	}
612: 	return;
613: }
614: 
615: ExpressionExecutorInfo::ExpressionExecutorInfo(ExpressionExecutor &executor, const string &name, int id) : id(id) {
616: 	// Extract Expression Root Information from ExpressionExecutorStats
617: 	for (auto &state : executor.GetStates()) {
618: 		roots.push_back(make_unique<ExpressionRootInfo>(*state, name));
619: 	}
620: }
621: 
622: ExpressionRootInfo::ExpressionRootInfo(ExpressionExecutorState &state, string name)
623:     : current_count(state.profiler.current_count), sample_count(state.profiler.sample_count),
624:       sample_tuples_count(state.profiler.sample_tuples_count), tuples_count(state.profiler.tuples_count),
625:       name(state.name), time(state.profiler.time) {
626: 	// Use the name of expression-tree as extra-info
627: 	extra_info = move(name);
628: 	auto expression_info_p = make_unique<ExpressionInfo>();
629: 	// Maybe root has a function
630: 	if (state.root_state->expr.expression_class == ExpressionClass::BOUND_FUNCTION) {
631: 		expression_info_p->hasfunction = true;
632: 		expression_info_p->function_name = ((BoundFunctionExpression &)state.root_state->expr).function.name;
633: 		expression_info_p->function_time = state.root_state->profiler.time;
634: 		expression_info_p->sample_tuples_count = state.root_state->profiler.sample_tuples_count;
635: 		expression_info_p->tuples_count = state.root_state->profiler.tuples_count;
636: 	}
637: 	expression_info_p->ExtractExpressionsRecursive(state.root_state);
638: 	root = move(expression_info_p);
639: }
640: } // namespace duckdb
[end of src/main/query_profiler.cpp]
[start of src/parallel/executor.cpp]
1: #include "duckdb/execution/executor.hpp"
2: 
3: #include "duckdb/execution/operator/helper/physical_execute.hpp"
4: #include "duckdb/execution/operator/join/physical_delim_join.hpp"
5: #include "duckdb/execution/operator/scan/physical_chunk_scan.hpp"
6: #include "duckdb/execution/operator/set/physical_recursive_cte.hpp"
7: #include "duckdb/execution/physical_operator.hpp"
8: #include "duckdb/main/client_context.hpp"
9: #include "duckdb/execution/execution_context.hpp"
10: #include "duckdb/parallel/thread_context.hpp"
11: #include "duckdb/parallel/task_scheduler.hpp"
12: #include "duckdb/parallel/pipeline_executor.hpp"
13: 
14: #include "duckdb/parallel/pipeline_event.hpp"
15: #include "duckdb/parallel/pipeline_finish_event.hpp"
16: #include "duckdb/parallel/pipeline_complete_event.hpp"
17: 
18: #include <algorithm>
19: 
20: namespace duckdb {
21: 
22: Executor::Executor(ClientContext &context) : context(context) {
23: }
24: 
25: Executor::~Executor() {
26: }
27: 
28: Executor &Executor::Get(ClientContext &context) {
29: 	return context.executor;
30: }
31: 
32: void Executor::AddEvent(shared_ptr<Event> event) {
33: 	lock_guard<mutex> elock(executor_lock);
34: 	events.push_back(move(event));
35: }
36: 
37: struct PipelineEventStack {
38: 	Event *pipeline_event;
39: 	Event *pipeline_finish_event;
40: 	Event *pipeline_complete_event;
41: };
42: 
43: Pipeline *Executor::ScheduleUnionPipeline(const shared_ptr<Pipeline> &pipeline, const Pipeline *parent,
44:                                           event_map_t &event_map, vector<shared_ptr<Event>> &events) {
45: 	pipeline->Ready();
46: 
47: 	D_ASSERT(pipeline);
48: 	auto pipeline_event = make_shared<PipelineEvent>(pipeline);
49: 
50: 	auto parent_stack_entry = event_map.find(parent);
51: 	D_ASSERT(parent_stack_entry != event_map.end());
52: 
53: 	auto &parent_stack = parent_stack_entry->second;
54: 
55: 	PipelineEventStack stack;
56: 	stack.pipeline_event = pipeline_event.get();
57: 	stack.pipeline_finish_event = parent_stack.pipeline_finish_event;
58: 	stack.pipeline_complete_event = parent_stack.pipeline_complete_event;
59: 
60: 	stack.pipeline_event->AddDependency(*parent_stack.pipeline_event);
61: 	parent_stack.pipeline_finish_event->AddDependency(*pipeline_event);
62: 
63: 	events.push_back(move(pipeline_event));
64: 	event_map.insert(make_pair(pipeline.get(), stack));
65: 
66: 	auto parent_pipeline = pipeline.get();
67: 
68: 	auto union_entry = union_pipelines.find(pipeline.get());
69: 	if (union_entry != union_pipelines.end()) {
70: 		for (auto &entry : union_entry->second) {
71: 			parent_pipeline = ScheduleUnionPipeline(entry, parent_pipeline, event_map, events);
72: 		}
73: 	}
74: 
75: 	return parent_pipeline;
76: }
77: 
78: void Executor::ScheduleChildPipeline(Pipeline *parent, const shared_ptr<Pipeline> &pipeline, event_map_t &event_map,
79:                                      vector<shared_ptr<Event>> &events) {
80: 	pipeline->Ready();
81: 
82: 	auto child_ptr = pipeline.get();
83: 	auto dependencies = child_dependencies.find(child_ptr);
84: 	D_ASSERT(union_pipelines.find(child_ptr) == union_pipelines.end());
85: 	D_ASSERT(dependencies != child_dependencies.end());
86: 	// create the pipeline event and the event stack
87: 	auto pipeline_event = make_shared<PipelineEvent>(pipeline);
88: 
89: 	auto parent_entry = event_map.find(parent);
90: 	PipelineEventStack stack;
91: 	stack.pipeline_event = pipeline_event.get();
92: 	stack.pipeline_finish_event = parent_entry->second.pipeline_finish_event;
93: 	stack.pipeline_complete_event = parent_entry->second.pipeline_complete_event;
94: 
95: 	// set up the dependencies for this child pipeline
96: 	unordered_set<Event *> finish_events;
97: 	for (auto &dep : dependencies->second) {
98: 		auto dep_entry = event_map.find(dep);
99: 		D_ASSERT(dep_entry != event_map.end());
100: 		D_ASSERT(dep_entry->second.pipeline_event);
101: 		D_ASSERT(dep_entry->second.pipeline_finish_event);
102: 
103: 		auto finish_event = dep_entry->second.pipeline_finish_event;
104: 		stack.pipeline_event->AddDependency(*dep_entry->second.pipeline_event);
105: 		if (finish_events.find(finish_event) == finish_events.end()) {
106: 			finish_event->AddDependency(*stack.pipeline_event);
107: 			finish_events.insert(finish_event);
108: 		}
109: 	}
110: 
111: 	events.push_back(move(pipeline_event));
112: 	event_map.insert(make_pair(child_ptr, stack));
113: }
114: 
115: void Executor::SchedulePipeline(const shared_ptr<Pipeline> &pipeline, event_map_t &event_map,
116:                                 vector<shared_ptr<Event>> &events, bool complete_pipeline) {
117: 	D_ASSERT(pipeline);
118: 
119: 	pipeline->Ready();
120: 
121: 	auto pipeline_event = make_shared<PipelineEvent>(pipeline);
122: 	auto pipeline_finish_event = make_shared<PipelineFinishEvent>(pipeline);
123: 	auto pipeline_complete_event = make_shared<PipelineCompleteEvent>(pipeline->executor, complete_pipeline);
124: 
125: 	PipelineEventStack stack;
126: 	stack.pipeline_event = pipeline_event.get();
127: 	stack.pipeline_finish_event = pipeline_finish_event.get();
128: 	stack.pipeline_complete_event = pipeline_complete_event.get();
129: 
130: 	pipeline_finish_event->AddDependency(*pipeline_event);
131: 	pipeline_complete_event->AddDependency(*pipeline_finish_event);
132: 
133: 	events.push_back(move(pipeline_event));
134: 	events.push_back(move(pipeline_finish_event));
135: 	events.push_back(move(pipeline_complete_event));
136: 
137: 	event_map.insert(make_pair(pipeline.get(), stack));
138: 
139: 	auto union_entry = union_pipelines.find(pipeline.get());
140: 	if (union_entry != union_pipelines.end()) {
141: 		auto parent_pipeline = pipeline.get();
142: 		for (auto &entry : union_entry->second) {
143: 			parent_pipeline = ScheduleUnionPipeline(entry, parent_pipeline, event_map, events);
144: 		}
145: 	}
146: }
147: 
148: void Executor::ScheduleEventsInternal(const vector<shared_ptr<Pipeline>> &pipelines,
149:                                       unordered_map<Pipeline *, vector<shared_ptr<Pipeline>>> &child_pipelines,
150:                                       vector<shared_ptr<Event>> &events, bool main_schedule) {
151: 	D_ASSERT(events.empty());
152: 	// create all the required pipeline events
153: 	event_map_t event_map;
154: 	for (auto &pipeline : pipelines) {
155: 		SchedulePipeline(pipeline, event_map, events, main_schedule);
156: 	}
157: 	// schedule child pipelines
158: 	for (auto &entry : child_pipelines) {
159: 		// iterate in reverse order
160: 		// since child entries are added from top to bottom
161: 		// dependencies are in reverse order (bottom to top)
162: 		for (idx_t i = entry.second.size(); i > 0; i--) {
163: 			auto &child_entry = entry.second[i - 1];
164: 			ScheduleChildPipeline(entry.first, child_entry, event_map, events);
165: 		}
166: 	}
167: 	// set up the dependencies between pipeline events
168: 	for (auto &entry : event_map) {
169: 		auto pipeline = entry.first;
170: 		for (auto &dependency : pipeline->dependencies) {
171: 			auto dep = dependency.lock();
172: 			D_ASSERT(dep);
173: 			auto event_map_entry = event_map.find(dep.get());
174: 			D_ASSERT(event_map_entry != event_map.end());
175: 			auto &dep_entry = event_map_entry->second;
176: 			D_ASSERT(dep_entry.pipeline_complete_event);
177: 			entry.second.pipeline_event->AddDependency(*dep_entry.pipeline_complete_event);
178: 		}
179: 	}
180: 	// schedule the pipelines that do not have dependencies
181: 	for (auto &event : events) {
182: 		if (!event->HasDependencies()) {
183: 			event->Schedule();
184: 		}
185: 	}
186: }
187: 
188: void Executor::ScheduleEvents() {
189: 	ScheduleEventsInternal(pipelines, child_pipelines, events);
190: }
191: 
192: void Executor::ReschedulePipelines(const vector<shared_ptr<Pipeline>> &pipelines, vector<shared_ptr<Event>> &events) {
193: 	unordered_map<Pipeline *, vector<shared_ptr<Pipeline>>> child_pipelines;
194: 	ScheduleEventsInternal(pipelines, child_pipelines, events, false);
195: }
196: 
197: void Executor::ExtractPipelines(shared_ptr<Pipeline> &pipeline, vector<shared_ptr<Pipeline>> &result) {
198: 	pipeline->Ready();
199: 
200: 	auto pipeline_ptr = pipeline.get();
201: 	result.push_back(move(pipeline));
202: 	auto union_entry = union_pipelines.find(pipeline_ptr);
203: 	if (union_entry != union_pipelines.end()) {
204: 		auto &union_pipeline_list = union_entry->second;
205: 		for (auto &pipeline : union_pipeline_list) {
206: 			ExtractPipelines(pipeline, result);
207: 		}
208: 		union_pipelines.erase(pipeline_ptr);
209: 	}
210: 	auto child_entry = child_pipelines.find(pipeline_ptr);
211: 	if (child_entry != child_pipelines.end()) {
212: 		for (auto &entry : child_entry->second) {
213: 			ExtractPipelines(entry, result);
214: 		}
215: 		child_pipelines.erase(pipeline_ptr);
216: 	}
217: }
218: 
219: bool Executor::NextExecutor() {
220: 	if (root_pipeline_idx >= root_pipelines.size()) {
221: 		return false;
222: 	}
223: 	root_executor = make_unique<PipelineExecutor>(context, *root_pipelines[root_pipeline_idx]);
224: 	root_pipeline_idx++;
225: 	return true;
226: }
227: 
228: void Executor::VerifyPipeline(Pipeline &pipeline) {
229: 	D_ASSERT(!pipeline.ToString().empty());
230: 	auto operators = pipeline.GetOperators();
231: 	for (auto &other_pipeline : pipelines) {
232: 		auto other_operators = other_pipeline->GetOperators();
233: 		for (idx_t op_idx = 0; op_idx < operators.size(); op_idx++) {
234: 			for (idx_t other_idx = 0; other_idx < other_operators.size(); other_idx++) {
235: 				auto &left = *operators[op_idx];
236: 				auto &right = *other_operators[other_idx];
237: 				if (left.Equals(right)) {
238: 					D_ASSERT(right.Equals(left));
239: 				} else {
240: 					D_ASSERT(!right.Equals(left));
241: 				}
242: 			}
243: 		}
244: 	}
245: }
246: 
247: void Executor::VerifyPipelines() {
248: #ifdef DEBUG
249: 	for (auto &pipeline : pipelines) {
250: 		VerifyPipeline(*pipeline);
251: 	}
252: 	for (auto &pipeline : root_pipelines) {
253: 		VerifyPipeline(*pipeline);
254: 	}
255: #endif
256: }
257: 
258: void Executor::WorkOnTasks() {
259: 	auto &scheduler = TaskScheduler::GetScheduler(context);
260: 
261: 	unique_ptr<Task> task;
262: 	while (scheduler.GetTaskFromProducer(*producer, task)) {
263: 		task->Execute();
264: 		task.reset();
265: 	}
266: }
267: 
268: void Executor::Initialize(PhysicalOperator *plan) {
269: 	Reset();
270: 
271: 	auto &scheduler = TaskScheduler::GetScheduler(context);
272: 	{
273: 		lock_guard<mutex> elock(executor_lock);
274: 		physical_plan = plan;
275: 
276: 		context.profiler->Initialize(physical_plan);
277: 		this->producer = scheduler.CreateProducer();
278: 
279: 		auto root_pipeline = make_shared<Pipeline>(*this);
280: 		root_pipeline->sink = nullptr;
281: 		BuildPipelines(physical_plan, root_pipeline.get());
282: 
283: 		this->total_pipelines = pipelines.size();
284: 
285: 		root_pipeline_idx = 0;
286: 		ExtractPipelines(root_pipeline, root_pipelines);
287: 
288: 		VerifyPipelines();
289: 
290: 		ScheduleEvents();
291: 	}
292: 
293: 	// now execute tasks from this producer until all pipelines are completed
294: 	while (completed_pipelines < total_pipelines) {
295: 		WorkOnTasks();
296: 		if (!HasError()) {
297: 			// no exceptions: continue
298: 			continue;
299: 		}
300: 
301: 		// an exception has occurred executing one of the pipelines
302: 		// we need to wait until all threads are finished
303: 		// we do this by creating weak pointers to all pipelines
304: 		// then clearing our references to the pipelines
305: 		// and waiting until all pipelines have been destroyed
306: 		vector<weak_ptr<Pipeline>> weak_references;
307: 		{
308: 			lock_guard<mutex> elock(executor_lock);
309: 			weak_references.reserve(pipelines.size());
310: 			for (auto &pipeline : pipelines) {
311: 				weak_references.push_back(weak_ptr<Pipeline>(pipeline));
312: 			}
313: 			for (auto &kv : union_pipelines) {
314: 				for (auto &pipeline : kv.second) {
315: 					weak_references.push_back(weak_ptr<Pipeline>(pipeline));
316: 				}
317: 			}
318: 			for (auto &kv : child_pipelines) {
319: 				for (auto &pipeline : kv.second) {
320: 					weak_references.push_back(weak_ptr<Pipeline>(pipeline));
321: 				}
322: 			}
323: 			pipelines.clear();
324: 			union_pipelines.clear();
325: 			child_pipelines.clear();
326: 			events.clear();
327: 		}
328: 		for (auto &weak_ref : weak_references) {
329: 			while (true) {
330: 				auto weak = weak_ref.lock();
331: 				if (!weak) {
332: 					break;
333: 				}
334: 			}
335: 		}
336: 		ThrowException();
337: 	}
338: 
339: 	lock_guard<mutex> elock(executor_lock);
340: 	pipelines.clear();
341: 	NextExecutor();
342: 	if (!exceptions.empty()) { // LCOV_EXCL_START
343: 		// an exception has occurred executing one of the pipelines
344: 		ThrowExceptionInternal();
345: 	} // LCOV_EXCL_STOP
346: }
347: 
348: void Executor::Reset() {
349: 	lock_guard<mutex> elock(executor_lock);
350: 	delim_join_dependencies.clear();
351: 	recursive_cte = nullptr;
352: 	physical_plan = nullptr;
353: 	root_executor.reset();
354: 	root_pipelines.clear();
355: 	root_pipeline_idx = 0;
356: 	completed_pipelines = 0;
357: 	total_pipelines = 0;
358: 	exceptions.clear();
359: 	pipelines.clear();
360: 	events.clear();
361: 	union_pipelines.clear();
362: 	child_pipelines.clear();
363: 	child_dependencies.clear();
364: }
365: 
366: void Executor::AddChildPipeline(Pipeline *current) {
367: 	D_ASSERT(!current->operators.empty());
368: 	// found another operator that is a source
369: 	// schedule a child pipeline
370: 	auto child_pipeline = make_shared<Pipeline>(*this);
371: 	auto child_pipeline_ptr = child_pipeline.get();
372: 	child_pipeline->sink = current->sink;
373: 	child_pipeline->operators = current->operators;
374: 	child_pipeline->source = current->operators.back();
375: 	D_ASSERT(child_pipeline->source->IsSource());
376: 	child_pipeline->operators.pop_back();
377: 
378: 	vector<Pipeline *> dependencies;
379: 	dependencies.push_back(current);
380: 	auto child_entry = child_pipelines.find(current);
381: 	if (child_entry != child_pipelines.end()) {
382: 		for (auto &current_child : child_entry->second) {
383: 			D_ASSERT(child_dependencies.find(current_child.get()) != child_dependencies.end());
384: 			child_dependencies[current_child.get()].push_back(child_pipeline_ptr);
385: 		}
386: 	}
387: 	D_ASSERT(child_dependencies.find(child_pipeline_ptr) == child_dependencies.end());
388: 	child_dependencies.insert(make_pair(child_pipeline_ptr, move(dependencies)));
389: 	child_pipelines[current].push_back(move(child_pipeline));
390: }
391: 
392: void Executor::BuildPipelines(PhysicalOperator *op, Pipeline *current) {
393: 	D_ASSERT(current);
394: 	if (op->IsSink()) {
395: 		// operator is a sink, build a pipeline
396: 		op->sink_state.reset();
397: 
398: 		PhysicalOperator *pipeline_child = nullptr;
399: 		switch (op->type) {
400: 		case PhysicalOperatorType::CREATE_TABLE_AS:
401: 		case PhysicalOperatorType::INSERT:
402: 		case PhysicalOperatorType::DELETE_OPERATOR:
403: 		case PhysicalOperatorType::UPDATE:
404: 		case PhysicalOperatorType::HASH_GROUP_BY:
405: 		case PhysicalOperatorType::SIMPLE_AGGREGATE:
406: 		case PhysicalOperatorType::PERFECT_HASH_GROUP_BY:
407: 		case PhysicalOperatorType::WINDOW:
408: 		case PhysicalOperatorType::ORDER_BY:
409: 		case PhysicalOperatorType::RESERVOIR_SAMPLE:
410: 		case PhysicalOperatorType::TOP_N:
411: 		case PhysicalOperatorType::COPY_TO_FILE:
412: 		case PhysicalOperatorType::LIMIT:
413: 		case PhysicalOperatorType::EXPRESSION_SCAN:
414: 			D_ASSERT(op->children.size() == 1);
415: 			// single operator:
416: 			// the operator becomes the data source of the current pipeline
417: 			current->source = op;
418: 			// we create a new pipeline starting from the child
419: 			pipeline_child = op->children[0].get();
420: 			break;
421: 		case PhysicalOperatorType::EXPORT:
422: 			// EXPORT has an optional child
423: 			// we only need to schedule child pipelines if there is a child
424: 			current->source = op;
425: 			if (op->children.empty()) {
426: 				return;
427: 			}
428: 			D_ASSERT(op->children.size() == 1);
429: 			pipeline_child = op->children[0].get();
430: 			break;
431: 		case PhysicalOperatorType::NESTED_LOOP_JOIN:
432: 		case PhysicalOperatorType::BLOCKWISE_NL_JOIN:
433: 		case PhysicalOperatorType::HASH_JOIN:
434: 		case PhysicalOperatorType::PIECEWISE_MERGE_JOIN:
435: 		case PhysicalOperatorType::CROSS_PRODUCT:
436: 			// regular join, create a pipeline with RHS source that sinks into this pipeline
437: 			pipeline_child = op->children[1].get();
438: 			// on the LHS (probe child), the operator becomes a regular operator
439: 			current->operators.push_back(op);
440: 			if (op->IsSource()) {
441: 				// FULL or RIGHT outer join
442: 				// schedule a scan of the node as a child pipeline
443: 				// this scan has to be performed AFTER all the probing has happened
444: 				if (recursive_cte) {
445: 					throw NotImplementedException("FULL and RIGHT outer joins are not supported in recursive CTEs yet");
446: 				}
447: 				AddChildPipeline(current);
448: 			}
449: 			BuildPipelines(op->children[0].get(), current);
450: 			break;
451: 		case PhysicalOperatorType::DELIM_JOIN: {
452: 			// duplicate eliminated join
453: 			// for delim joins, recurse into the actual join
454: 			pipeline_child = op->children[0].get();
455: 			break;
456: 		}
457: 		case PhysicalOperatorType::RECURSIVE_CTE: {
458: 			auto &cte_node = (PhysicalRecursiveCTE &)*op;
459: 
460: 			// recursive CTE
461: 			current->source = op;
462: 			// the LHS of the recursive CTE is our initial state
463: 			// we build this pipeline as normal
464: 			pipeline_child = op->children[0].get();
465: 			// for the RHS, we gather all pipelines that depend on the recursive cte
466: 			// these pipelines need to be rerun
467: 			if (recursive_cte) {
468: 				throw InternalException("Recursive CTE detected WITHIN a recursive CTE node");
469: 			}
470: 			recursive_cte = op;
471: 
472: 			auto recursive_pipeline = make_shared<Pipeline>(*this);
473: 			recursive_pipeline->sink = op;
474: 			op->sink_state.reset();
475: 			BuildPipelines(op->children[1].get(), recursive_pipeline.get());
476: 
477: 			cte_node.pipelines.push_back(move(recursive_pipeline));
478: 
479: 			recursive_cte = nullptr;
480: 			break;
481: 		}
482: 		default:
483: 			throw InternalException("Unimplemented sink type!");
484: 		}
485: 		// the current is dependent on this pipeline to complete
486: 		auto pipeline = make_shared<Pipeline>(*this);
487: 		pipeline->sink = op;
488: 		current->AddDependency(pipeline);
489: 		D_ASSERT(pipeline_child);
490: 		// recurse into the pipeline child
491: 		BuildPipelines(pipeline_child, pipeline.get());
492: 		if (op->type == PhysicalOperatorType::DELIM_JOIN) {
493: 			// for delim joins, recurse into the actual join
494: 			// any pipelines in there depend on the main pipeline
495: 			auto &delim_join = (PhysicalDelimJoin &)*op;
496: 			// any scan of the duplicate eliminated data on the RHS depends on this pipeline
497: 			// we add an entry to the mapping of (PhysicalOperator*) -> (Pipeline*)
498: 			for (auto &delim_scan : delim_join.delim_scans) {
499: 				delim_join_dependencies[delim_scan] = pipeline.get();
500: 			}
501: 			BuildPipelines(delim_join.join.get(), current);
502: 		}
503: 		if (!recursive_cte) {
504: 			// regular pipeline: schedule it
505: 			pipelines.push_back(move(pipeline));
506: 		} else {
507: 			// CTE pipeline! add it to the CTE pipelines
508: 			D_ASSERT(recursive_cte);
509: 			auto &cte = (PhysicalRecursiveCTE &)*recursive_cte;
510: 			cte.pipelines.push_back(move(pipeline));
511: 		}
512: 	} else {
513: 		// operator is not a sink! recurse in children
514: 		// first check if there is any additional action we need to do depending on the type
515: 		switch (op->type) {
516: 		case PhysicalOperatorType::DELIM_SCAN: {
517: 			D_ASSERT(op->children.empty());
518: 			auto entry = delim_join_dependencies.find(op);
519: 			D_ASSERT(entry != delim_join_dependencies.end());
520: 			// this chunk scan introduces a dependency to the current pipeline
521: 			// namely a dependency on the duplicate elimination pipeline to finish
522: 			auto delim_dependency = entry->second->shared_from_this();
523: 			D_ASSERT(delim_dependency->sink->type == PhysicalOperatorType::DELIM_JOIN);
524: 			auto &delim_join = (PhysicalDelimJoin &)*delim_dependency->sink;
525: 			current->AddDependency(delim_dependency);
526: 			current->source = (PhysicalOperator *)delim_join.distinct.get();
527: 			return;
528: 		}
529: 		case PhysicalOperatorType::EXECUTE: {
530: 			// EXECUTE statement: build pipeline on child
531: 			auto &execute = (PhysicalExecute &)*op;
532: 			BuildPipelines(execute.plan, current);
533: 			return;
534: 		}
535: 		case PhysicalOperatorType::RECURSIVE_CTE_SCAN: {
536: 			if (!recursive_cte) {
537: 				throw InternalException("Recursive CTE scan found without recursive CTE node");
538: 			}
539: 			break;
540: 		}
541: 		case PhysicalOperatorType::INDEX_JOIN: {
542: 			// index join: we only continue into the LHS
543: 			// the right side is probed by the index join
544: 			// so we don't need to do anything in the pipeline with this child
545: 			current->operators.push_back(op);
546: 			BuildPipelines(op->children[0].get(), current);
547: 			return;
548: 		}
549: 		case PhysicalOperatorType::UNION: {
550: 			if (recursive_cte) {
551: 				throw NotImplementedException("UNIONS are not supported in recursive CTEs yet");
552: 			}
553: 			auto union_pipeline = make_shared<Pipeline>(*this);
554: 			auto pipeline_ptr = union_pipeline.get();
555: 			// set up dependencies for any child pipelines to this union pipeline
556: 			auto child_entry = child_pipelines.find(current);
557: 			if (child_entry != child_pipelines.end()) {
558: 				for (auto &current_child : child_entry->second) {
559: 					D_ASSERT(child_dependencies.find(current_child.get()) != child_dependencies.end());
560: 					child_dependencies[current_child.get()].push_back(pipeline_ptr);
561: 				}
562: 			}
563: 			// for the current pipeline, continue building on the LHS
564: 			union_pipeline->operators = current->operators;
565: 			BuildPipelines(op->children[0].get(), current);
566: 			// insert the union pipeline as a union pipeline of the current node
567: 			union_pipelines[current].push_back(move(union_pipeline));
568: 
569: 			// for the union pipeline, build on the RHS
570: 			pipeline_ptr->sink = current->sink;
571: 			BuildPipelines(op->children[1].get(), pipeline_ptr);
572: 			return;
573: 		}
574: 		default:
575: 			break;
576: 		}
577: 		if (op->children.empty()) {
578: 			// source
579: 			current->source = op;
580: 		} else {
581: 			if (op->children.size() != 1) {
582: 				throw InternalException("Operator not supported yet");
583: 			}
584: 			current->operators.push_back(op);
585: 			BuildPipelines(op->children[0].get(), current);
586: 		}
587: 	}
588: }
589: 
590: vector<LogicalType> Executor::GetTypes() {
591: 	D_ASSERT(physical_plan);
592: 	return physical_plan->GetTypes();
593: }
594: 
595: void Executor::PushError(ExceptionType type, const string &exception) {
596: 	lock_guard<mutex> elock(executor_lock);
597: 	// interrupt execution of any other pipelines that belong to this executor
598: 	context.interrupted = true;
599: 	// push the exception onto the stack
600: 	exceptions.emplace_back(type, exception);
601: }
602: 
603: bool Executor::HasError() {
604: 	lock_guard<mutex> elock(executor_lock);
605: 	return !exceptions.empty();
606: }
607: 
608: void Executor::ThrowException() {
609: 	lock_guard<mutex> elock(executor_lock);
610: 	ThrowExceptionInternal();
611: }
612: 
613: void Executor::ThrowExceptionInternal() { // LCOV_EXCL_START
614: 	D_ASSERT(!exceptions.empty());
615: 	auto &entry = exceptions[0];
616: 	switch (entry.first) {
617: 	case ExceptionType::TRANSACTION:
618: 		throw TransactionException(entry.second);
619: 	case ExceptionType::CATALOG:
620: 		throw CatalogException(entry.second);
621: 	case ExceptionType::PARSER:
622: 		throw ParserException(entry.second);
623: 	case ExceptionType::BINDER:
624: 		throw BinderException(entry.second);
625: 	case ExceptionType::INTERRUPT:
626: 		throw InterruptException();
627: 	case ExceptionType::FATAL:
628: 		throw FatalException(entry.second);
629: 	case ExceptionType::INTERNAL:
630: 		throw InternalException(entry.second);
631: 	case ExceptionType::IO:
632: 		throw IOException(entry.second);
633: 	case ExceptionType::CONSTRAINT:
634: 		throw ConstraintException(entry.second);
635: 	case ExceptionType::CONVERSION:
636: 		throw ConversionException(entry.second);
637: 	default:
638: 		throw Exception(entry.second);
639: 	}
640: } // LCOV_EXCL_STOP
641: 
642: void Executor::Flush(ThreadContext &tcontext) {
643: 	lock_guard<mutex> elock(executor_lock);
644: 	context.profiler->Flush(tcontext.profiler);
645: }
646: 
647: bool Executor::GetPipelinesProgress(int &current_progress) { // LCOV_EXCL_START
648: 	lock_guard<mutex> elock(executor_lock);
649: 
650: 	if (!pipelines.empty()) {
651: 		return pipelines.back()->GetProgress(current_progress);
652: 	} else {
653: 		current_progress = -1;
654: 		return true;
655: 	}
656: } // LCOV_EXCL_STOP
657: 
658: unique_ptr<DataChunk> Executor::FetchChunk() {
659: 	D_ASSERT(physical_plan);
660: 
661: 	auto chunk = make_unique<DataChunk>();
662: 	root_executor->InitializeChunk(*chunk);
663: 	while (true) {
664: 		root_executor->ExecutePull(*chunk);
665: 		if (chunk->size() == 0) {
666: 			root_executor->PullFinalize();
667: 			if (NextExecutor()) {
668: 				continue;
669: 			}
670: 			break;
671: 		} else {
672: 			break;
673: 		}
674: 	}
675: 	return chunk;
676: }
677: 
678: } // namespace duckdb
[end of src/parallel/executor.cpp]
[start of src/parser/statement/explain_statement.cpp]
1: #include "duckdb/parser/statement/explain_statement.hpp"
2: 
3: namespace duckdb {
4: 
5: ExplainStatement::ExplainStatement(unique_ptr<SQLStatement> stmt)
6:     : SQLStatement(StatementType::EXPLAIN_STATEMENT), stmt(move(stmt)) {
7: }
8: 
9: unique_ptr<SQLStatement> ExplainStatement::Copy() const {
10: 	auto result = make_unique<ExplainStatement>(stmt->Copy());
11: 	return move(result);
12: }
13: 
14: } // namespace duckdb
[end of src/parser/statement/explain_statement.cpp]
[start of src/parser/transform/statement/transform_explain.cpp]
1: #include "duckdb/parser/statement/explain_statement.hpp"
2: #include "duckdb/parser/transformer.hpp"
3: 
4: namespace duckdb {
5: 
6: unique_ptr<ExplainStatement> Transformer::TransformExplain(duckdb_libpgquery::PGNode *node) {
7: 	auto stmt = reinterpret_cast<duckdb_libpgquery::PGExplainStmt *>(node);
8: 	D_ASSERT(stmt);
9: 	return make_unique<ExplainStatement>(TransformStatement(stmt->query));
10: }
11: 
12: } // namespace duckdb
[end of src/parser/transform/statement/transform_explain.cpp]
[start of src/planner/binder/statement/bind_explain.cpp]
1: #include "duckdb/planner/binder.hpp"
2: #include "duckdb/parser/statement/explain_statement.hpp"
3: #include "duckdb/planner/operator/logical_explain.hpp"
4: 
5: namespace duckdb {
6: 
7: BoundStatement Binder::Bind(ExplainStatement &stmt) {
8: 	BoundStatement result;
9: 
10: 	// bind the underlying statement
11: 	auto plan = Bind(*stmt.stmt);
12: 	// get the unoptimized logical plan, and create the explain statement
13: 	auto logical_plan_unopt = plan.plan->ToString();
14: 	auto explain = make_unique<LogicalExplain>(move(plan.plan));
15: 	explain->logical_plan_unopt = logical_plan_unopt;
16: 
17: 	result.plan = move(explain);
18: 	result.names = {"explain_key", "explain_value"};
19: 	result.types = {LogicalType::VARCHAR, LogicalType::VARCHAR};
20: 	return result;
21: }
22: 
23: } // namespace duckdb
[end of src/planner/binder/statement/bind_explain.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: