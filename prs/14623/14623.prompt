You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
import + pivot + export creates un-importable dump 
### What happens?

If you create a table from a pivot command, export the db, import it again elsewhere, do another pivot, then export again, the exported database will not be importable because of a bug in the schema.sql wherein there are multiple lines that start with `CREATE TYPE __pivot_enum_0_0 AS ENUM`

### To Reproduce

```py
# demonstrate bug
import numpy as np
import pandas as pd
import duckdb
from pathlib import Path

TEMP_PATH0 = "/tmp/duckdemo0"
TEMP_PATH1 = "/tmp/duckdemo1"
N = 1000
df = pd.DataFrame({"a":np.random.choice(["u", "v", "w"], N, True),
                   "b":np.random.choice(["x", "y", "z"], N, True),
                   "c":np.random.randn(N),})
conn0 = duckdb.connect(":memory:0")

ddf = conn0.from_df(df)
ddf.create("input_data")

xx = conn0.query("pivot input_data on a using max(c) group by b;")
xx.create("xx")
conn0.execute(f"EXPORT DATABASE '{TEMP_PATH0}'  (FORMAT PARQUET);")
conn1 = duckdb.connect(":memory:1")
conn1.execute(f"IMPORT DATABASE '{TEMP_PATH0}';")
yy = conn1.query("pivot input_data on b using max(c) group by a;")
yy.create("yy")
conn1.execute(f"EXPORT DATABASE '{TEMP_PATH1}'  (FORMAT PARQUET);")
with (Path(TEMP_PATH1) / "schema.sql").open("r") as f:
    print(f.read())

# CREATE TYPE __pivot_enum_0_0 AS ENUM ( 'x', 'y', 'z' );
# CREATE TYPE __pivot_enum_0_0 AS ENUM ( 'u', 'v', 'w' );

# CREATE TABLE input_data(a VARCHAR, b VARCHAR, c DOUBLE);
# CREATE TABLE xx(b VARCHAR, u DOUBLE, v DOUBLE, w DOUBLE);
# CREATE TABLE yy(a VARCHAR, x DOUBLE, y DOUBLE, z DOUBLE);

# Renames the pivot type. Breaks importing even though it's never used.

conn2 = duckdb.connect(":memory:2")
conn2.execute(f"IMPORT DATABASE '{TEMP_PATH1}';")
# Fails with message:
# CatalogException: Catalog Error: Type with name "__pivot_enum_0_0" already exists!
```

### OS:

Ubuntu 22.04.5 LTS x86_64

### DuckDB Version:

1.1.1

### DuckDB Client:

python

### Hardware:

_No response_

### Full Name:

Michael Hankin

### Affiliation:

Mothball Labs

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have not tested with any build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://duckdb.org/docs/installation/) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/execution/operator/persistent/physical_export.cpp]
1: #include "duckdb/execution/operator/persistent/physical_export.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
6: #include "duckdb/common/file_system.hpp"
7: #include "duckdb/common/string_util.hpp"
8: #include "duckdb/parallel/meta_pipeline.hpp"
9: #include "duckdb/parallel/pipeline.hpp"
10: #include "duckdb/parser/keyword_helper.hpp"
11: #include "duckdb/transaction/transaction.hpp"
12: 
13: #include <algorithm>
14: #include <sstream>
15: 
16: namespace duckdb {
17: 
18: using std::stringstream;
19: 
20: void ReorderTableEntries(catalog_entry_vector_t &tables);
21: 
22: PhysicalExport::PhysicalExport(vector<LogicalType> types, CopyFunction function, unique_ptr<CopyInfo> info,
23:                                idx_t estimated_cardinality, unique_ptr<BoundExportData> exported_tables)
24:     : PhysicalOperator(PhysicalOperatorType::EXPORT, std::move(types), estimated_cardinality),
25:       function(std::move(function)), info(std::move(info)), exported_tables(std::move(exported_tables)) {
26: }
27: 
28: static void WriteCatalogEntries(stringstream &ss, catalog_entry_vector_t &entries) {
29: 	for (auto &entry : entries) {
30: 		if (entry.get().internal) {
31: 			continue;
32: 		}
33: 		auto create_info = entry.get().GetInfo();
34: 		try {
35: 			// Strip the catalog from the info
36: 			create_info->catalog.clear();
37: 			auto to_string = create_info->ToString();
38: 			ss << to_string;
39: 		} catch (const NotImplementedException &) {
40: 			ss << entry.get().ToSQL();
41: 		}
42: 		ss << '\n';
43: 	}
44: 	ss << '\n';
45: }
46: 
47: static void WriteStringStreamToFile(FileSystem &fs, stringstream &ss, const string &path) {
48: 	auto ss_string = ss.str();
49: 	auto handle = fs.OpenFile(path, FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE_NEW |
50: 	                                    FileLockType::WRITE_LOCK);
51: 	fs.Write(*handle, (void *)ss_string.c_str(), NumericCast<int64_t>(ss_string.size()));
52: 	handle.reset();
53: }
54: 
55: static void WriteCopyStatement(FileSystem &fs, stringstream &ss, CopyInfo &info, ExportedTableData &exported_table,
56:                                CopyFunction const &function) {
57: 	ss << "COPY ";
58: 
59: 	//! NOTE: The catalog is explicitly not set here
60: 	if (exported_table.schema_name != DEFAULT_SCHEMA && !exported_table.schema_name.empty()) {
61: 		ss << KeywordHelper::WriteOptionallyQuoted(exported_table.schema_name) << ".";
62: 	}
63: 
64: 	auto file_path = StringUtil::Replace(exported_table.file_path, "\\", "/");
65: 	ss << StringUtil::Format("%s FROM %s (", SQLIdentifier(exported_table.table_name), SQLString(file_path));
66: 	// write the copy options
67: 	ss << "FORMAT '" << info.format << "'";
68: 	if (info.format == "csv") {
69: 		// insert default csv options, if not specified
70: 		if (info.options.find("header") == info.options.end()) {
71: 			info.options["header"].push_back(Value::INTEGER(1));
72: 		}
73: 		if (info.options.find("delimiter") == info.options.end() && info.options.find("sep") == info.options.end() &&
74: 		    info.options.find("delim") == info.options.end()) {
75: 			info.options["delimiter"].push_back(Value(","));
76: 		}
77: 		if (info.options.find("quote") == info.options.end()) {
78: 			info.options["quote"].push_back(Value("\""));
79: 		}
80: 		info.options.erase("force_not_null");
81: 		for (auto &not_null_column : exported_table.not_null_columns) {
82: 			info.options["force_not_null"].push_back(not_null_column);
83: 		}
84: 	}
85: 	for (auto &copy_option : info.options) {
86: 		if (copy_option.first == "force_quote") {
87: 			continue;
88: 		}
89: 		if (copy_option.second.empty()) {
90: 			// empty options are interpreted as TRUE
91: 			copy_option.second.push_back(true);
92: 		}
93: 		ss << ", " << copy_option.first << " ";
94: 		if (copy_option.second.size() == 1) {
95: 			ss << copy_option.second[0].ToSQLString();
96: 		} else {
97: 			// For Lists
98: 			ss << "(";
99: 			for (idx_t i = 0; i < copy_option.second.size(); i++) {
100: 				ss << copy_option.second[i].ToSQLString();
101: 				if (i != copy_option.second.size() - 1) {
102: 					ss << ", ";
103: 				}
104: 			}
105: 			ss << ")";
106: 		}
107: 	}
108: 	ss << ");" << '\n';
109: }
110: 
111: //===--------------------------------------------------------------------===//
112: // Source
113: //===--------------------------------------------------------------------===//
114: class ExportSourceState : public GlobalSourceState {
115: public:
116: 	ExportSourceState() : finished(false) {
117: 	}
118: 
119: 	bool finished;
120: };
121: 
122: unique_ptr<GlobalSourceState> PhysicalExport::GetGlobalSourceState(ClientContext &context) const {
123: 	return make_uniq<ExportSourceState>();
124: }
125: 
126: void PhysicalExport::ExtractEntries(ClientContext &context, vector<reference<SchemaCatalogEntry>> &schema_list,
127:                                     ExportEntries &result) {
128: 	for (auto &schema_p : schema_list) {
129: 		auto &schema = schema_p.get();
130: 		if (!schema.internal) {
131: 			result.schemas.push_back(schema);
132: 		}
133: 		schema.Scan(context, CatalogType::TABLE_ENTRY, [&](CatalogEntry &entry) {
134: 			if (entry.internal) {
135: 				return;
136: 			}
137: 			if (entry.type != CatalogType::TABLE_ENTRY) {
138: 				result.views.push_back(entry);
139: 			}
140: 			if (entry.type == CatalogType::TABLE_ENTRY) {
141: 				result.tables.push_back(entry);
142: 			}
143: 		});
144: 		schema.Scan(context, CatalogType::SEQUENCE_ENTRY, [&](CatalogEntry &entry) {
145: 			if (entry.internal) {
146: 				return;
147: 			}
148: 			result.sequences.push_back(entry);
149: 		});
150: 		schema.Scan(context, CatalogType::TYPE_ENTRY, [&](CatalogEntry &entry) {
151: 			if (entry.internal) {
152: 				return;
153: 			}
154: 			result.custom_types.push_back(entry);
155: 		});
156: 		schema.Scan(context, CatalogType::INDEX_ENTRY, [&](CatalogEntry &entry) {
157: 			if (entry.internal) {
158: 				return;
159: 			}
160: 			result.indexes.push_back(entry);
161: 		});
162: 		schema.Scan(context, CatalogType::MACRO_ENTRY, [&](CatalogEntry &entry) {
163: 			if (!entry.internal && entry.type == CatalogType::MACRO_ENTRY) {
164: 				result.macros.push_back(entry);
165: 			}
166: 		});
167: 		schema.Scan(context, CatalogType::TABLE_MACRO_ENTRY, [&](CatalogEntry &entry) {
168: 			if (!entry.internal && entry.type == CatalogType::TABLE_MACRO_ENTRY) {
169: 				result.macros.push_back(entry);
170: 			}
171: 		});
172: 	}
173: }
174: 
175: static void AddEntries(catalog_entry_vector_t &all_entries, catalog_entry_vector_t &to_add) {
176: 	for (auto &entry : to_add) {
177: 		all_entries.push_back(entry);
178: 	}
179: 	to_add.clear();
180: }
181: 
182: catalog_entry_vector_t PhysicalExport::GetNaiveExportOrder(ClientContext &context, Catalog &catalog) {
183: 	// gather all catalog types to export
184: 	ExportEntries entries;
185: 	auto schema_list = catalog.GetSchemas(context);
186: 	PhysicalExport::ExtractEntries(context, schema_list, entries);
187: 
188: 	ReorderTableEntries(entries.tables);
189: 
190: 	// order macro's by timestamp so nested macro's are imported nicely
191: 	sort(entries.macros.begin(), entries.macros.end(),
192: 	     [](const reference<CatalogEntry> &lhs, const reference<CatalogEntry> &rhs) {
193: 		     return lhs.get().oid < rhs.get().oid;
194: 	     });
195: 
196: 	catalog_entry_vector_t catalog_entries;
197: 	idx_t size = 0;
198: 	size += entries.schemas.size();
199: 	size += entries.custom_types.size();
200: 	size += entries.sequences.size();
201: 	size += entries.tables.size();
202: 	size += entries.views.size();
203: 	size += entries.indexes.size();
204: 	size += entries.macros.size();
205: 	catalog_entries.reserve(size);
206: 	AddEntries(catalog_entries, entries.schemas);
207: 	AddEntries(catalog_entries, entries.sequences);
208: 	AddEntries(catalog_entries, entries.custom_types);
209: 	AddEntries(catalog_entries, entries.tables);
210: 	AddEntries(catalog_entries, entries.macros);
211: 	AddEntries(catalog_entries, entries.views);
212: 	AddEntries(catalog_entries, entries.indexes);
213: 	return catalog_entries;
214: }
215: 
216: SourceResultType PhysicalExport::GetData(ExecutionContext &context, DataChunk &chunk,
217:                                          OperatorSourceInput &input) const {
218: 	auto &state = input.global_state.Cast<ExportSourceState>();
219: 	if (state.finished) {
220: 		return SourceResultType::FINISHED;
221: 	}
222: 
223: 	auto &ccontext = context.client;
224: 	auto &fs = FileSystem::GetFileSystem(ccontext);
225: 
226: 	// gather all catalog types to export
227: 	ExportEntries entries;
228: 
229: 	auto schema_list = Catalog::GetSchemas(ccontext, info->catalog);
230: 	ExtractEntries(context.client, schema_list, entries);
231: 
232: 	// consider the order of tables because of foreign key constraint
233: 	entries.tables.clear();
234: 	for (idx_t i = 0; i < exported_tables->data.size(); i++) {
235: 		entries.tables.push_back(exported_tables->data[i].entry);
236: 	}
237: 
238: 	// order macro's by timestamp so nested macro's are imported nicely
239: 	sort(entries.macros.begin(), entries.macros.end(),
240: 	     [](const reference<CatalogEntry> &lhs, const reference<CatalogEntry> &rhs) {
241: 		     return lhs.get().oid < rhs.get().oid;
242: 	     });
243: 
244: 	// write the schema.sql file
245: 	// export order is SCHEMA -> SEQUENCE -> TABLE -> VIEW -> INDEX
246: 
247: 	stringstream ss;
248: 	WriteCatalogEntries(ss, entries.schemas);
249: 	WriteCatalogEntries(ss, entries.custom_types);
250: 	WriteCatalogEntries(ss, entries.sequences);
251: 	WriteCatalogEntries(ss, entries.tables);
252: 	WriteCatalogEntries(ss, entries.views);
253: 	WriteCatalogEntries(ss, entries.indexes);
254: 	WriteCatalogEntries(ss, entries.macros);
255: 
256: 	WriteStringStreamToFile(fs, ss, fs.JoinPath(info->file_path, "schema.sql"));
257: 
258: 	// write the load.sql file
259: 	// for every table, we write COPY INTO statement with the specified options
260: 	stringstream load_ss;
261: 	for (idx_t i = 0; i < exported_tables->data.size(); i++) {
262: 		auto exported_table_info = exported_tables->data[i].table_data;
263: 		WriteCopyStatement(fs, load_ss, *info, exported_table_info, function);
264: 	}
265: 	WriteStringStreamToFile(fs, load_ss, fs.JoinPath(info->file_path, "load.sql"));
266: 	state.finished = true;
267: 
268: 	return SourceResultType::FINISHED;
269: }
270: 
271: //===--------------------------------------------------------------------===//
272: // Sink
273: //===--------------------------------------------------------------------===//
274: SinkResultType PhysicalExport::Sink(ExecutionContext &context, DataChunk &chunk, OperatorSinkInput &input) const {
275: 	// nop
276: 	return SinkResultType::NEED_MORE_INPUT;
277: }
278: 
279: //===--------------------------------------------------------------------===//
280: // Pipeline Construction
281: //===--------------------------------------------------------------------===//
282: void PhysicalExport::BuildPipelines(Pipeline &current, MetaPipeline &meta_pipeline) {
283: 	// EXPORT has an optional child
284: 	// we only need to schedule child pipelines if there is a child
285: 	auto &state = meta_pipeline.GetState();
286: 	state.SetPipelineSource(current, *this);
287: 	if (children.empty()) {
288: 		return;
289: 	}
290: 	PhysicalOperator::BuildPipelines(current, meta_pipeline);
291: }
292: 
293: vector<const_reference<PhysicalOperator>> PhysicalExport::GetSources() const {
294: 	return {*this};
295: }
296: 
297: } // namespace duckdb
[end of src/execution/operator/persistent/physical_export.cpp]
[start of src/parser/transform/statement/transform_pivot_stmt.cpp]
1: #include "duckdb/parser/transformer.hpp"
2: #include "duckdb/parser/tableref/pivotref.hpp"
3: #include "duckdb/parser/parsed_data/create_type_info.hpp"
4: #include "duckdb/parser/statement/create_statement.hpp"
5: #include "duckdb/parser/statement/select_statement.hpp"
6: #include "duckdb/parser/expression/columnref_expression.hpp"
7: #include "duckdb/parser/expression/star_expression.hpp"
8: #include "duckdb/parser/query_node/select_node.hpp"
9: #include "duckdb/parser/statement/multi_statement.hpp"
10: #include "duckdb/parser/statement/drop_statement.hpp"
11: #include "duckdb/parser/parsed_data/drop_info.hpp"
12: #include "duckdb/parser/expression/cast_expression.hpp"
13: #include "duckdb/parser/expression/constant_expression.hpp"
14: #include "duckdb/parser/expression/operator_expression.hpp"
15: #include "duckdb/parser/expression/function_expression.hpp"
16: #include "duckdb/parser/result_modifier.hpp"
17: #include "duckdb/parser/tableref/subqueryref.hpp"
18: 
19: namespace duckdb {
20: 
21: void Transformer::AddPivotEntry(string enum_name, unique_ptr<SelectNode> base, unique_ptr<ParsedExpression> column,
22:                                 unique_ptr<QueryNode> subquery, bool has_parameters) {
23: 	if (parent) {
24: 		parent->AddPivotEntry(std::move(enum_name), std::move(base), std::move(column), std::move(subquery),
25: 		                      has_parameters);
26: 		return;
27: 	}
28: 	auto result = make_uniq<CreatePivotEntry>();
29: 	result->enum_name = std::move(enum_name);
30: 	result->base = std::move(base);
31: 	result->column = std::move(column);
32: 	result->subquery = std::move(subquery);
33: 	result->has_parameters = has_parameters;
34: 
35: 	pivot_entries.push_back(std::move(result));
36: }
37: 
38: bool Transformer::HasPivotEntries() {
39: 	return !GetPivotEntries().empty();
40: }
41: 
42: idx_t Transformer::PivotEntryCount() {
43: 	return GetPivotEntries().size();
44: }
45: 
46: vector<unique_ptr<Transformer::CreatePivotEntry>> &Transformer::GetPivotEntries() {
47: 	if (parent) {
48: 		return parent->GetPivotEntries();
49: 	}
50: 	return pivot_entries;
51: }
52: 
53: void Transformer::PivotEntryCheck(const string &type) {
54: 	auto &entries = GetPivotEntries();
55: 	if (!entries.empty()) {
56: 		throw ParserException(
57: 		    "PIVOT statements with pivot elements extracted from the data cannot be used in %ss.\nIn order to use "
58: 		    "PIVOT in a %s the PIVOT values must be manually specified, e.g.:\nPIVOT ... ON %s IN (val1, val2, ...)",
59: 		    type, type, entries[0]->column->ToString());
60: 	}
61: }
62: unique_ptr<SQLStatement> Transformer::GenerateCreateEnumStmt(unique_ptr<CreatePivotEntry> entry) {
63: 	auto result = make_uniq<CreateStatement>();
64: 	auto info = make_uniq<CreateTypeInfo>();
65: 
66: 	info->temporary = true;
67: 	info->internal = false;
68: 	info->catalog = INVALID_CATALOG;
69: 	info->schema = INVALID_SCHEMA;
70: 	info->name = std::move(entry->enum_name);
71: 	info->on_conflict = OnCreateConflict::REPLACE_ON_CONFLICT;
72: 
73: 	// generate the query that will result in the enum creation
74: 	unique_ptr<QueryNode> subselect;
75: 	if (!entry->subquery) {
76: 		auto select_node = std::move(entry->base);
77: 		auto columnref = entry->column->Copy();
78: 		auto cast = make_uniq<CastExpression>(LogicalType::VARCHAR, std::move(columnref));
79: 		select_node->select_list.push_back(std::move(cast));
80: 
81: 		auto is_not_null =
82: 		    make_uniq<OperatorExpression>(ExpressionType::OPERATOR_IS_NOT_NULL, std::move(entry->column));
83: 		select_node->where_clause = std::move(is_not_null);
84: 
85: 		// order by the column
86: 		select_node->modifiers.push_back(make_uniq<DistinctModifier>());
87: 		auto modifier = make_uniq<OrderModifier>();
88: 		modifier->orders.emplace_back(OrderType::ASCENDING, OrderByNullType::ORDER_DEFAULT,
89: 		                              make_uniq<ConstantExpression>(Value::INTEGER(1)));
90: 		select_node->modifiers.push_back(std::move(modifier));
91: 		subselect = std::move(select_node);
92: 	} else {
93: 		subselect = std::move(entry->subquery);
94: 	}
95: 
96: 	auto select = make_uniq<SelectStatement>();
97: 	select->node = TransformMaterializedCTE(std::move(subselect));
98: 	info->query = std::move(select);
99: 	info->type = LogicalType::INVALID;
100: 
101: 	result->info = std::move(info);
102: 	return std::move(result);
103: }
104: 
105: // unique_ptr<SQLStatement> GenerateDropEnumStmt(string enum_name) {
106: //	auto result = make_uniq<DropStatement>();
107: //	result->info->if_exists = true;
108: //	result->info->schema = INVALID_SCHEMA;
109: //	result->info->catalog = INVALID_CATALOG;
110: //	result->info->name = std::move(enum_name);
111: //	result->info->type = CatalogType::TYPE_ENTRY;
112: //	return std::move(result);
113: //}
114: 
115: unique_ptr<SQLStatement> Transformer::CreatePivotStatement(unique_ptr<SQLStatement> statement) {
116: 	auto result = make_uniq<MultiStatement>();
117: 	for (auto &pivot : pivot_entries) {
118: 		if (pivot->has_parameters) {
119: 			throw ParserException(
120: 			    "PIVOT statements with pivot elements extracted from the data cannot have parameters in their source.\n"
121: 			    "In order to use parameters the PIVOT values must be manually specified, e.g.:\n"
122: 			    "PIVOT ... ON %s IN (val1, val2, ...)",
123: 			    pivot->column->ToString());
124: 		}
125: 		result->statements.push_back(GenerateCreateEnumStmt(std::move(pivot)));
126: 	}
127: 	result->statements.push_back(std::move(statement));
128: 	// FIXME: drop the types again!?
129: 	//	for(auto &pivot : pivot_entries) {
130: 	//		result->statements.push_back(GenerateDropEnumStmt(std::move(pivot->enum_name)));
131: 	//	}
132: 	return std::move(result);
133: }
134: 
135: unique_ptr<QueryNode> Transformer::TransformPivotStatement(duckdb_libpgquery::PGSelectStmt &select) {
136: 	auto pivot = select.pivot;
137: 	auto current_param_count = ParamCount();
138: 	auto source = TransformTableRefNode(*pivot->source);
139: 	auto next_param_count = ParamCount();
140: 	bool has_parameters = next_param_count > current_param_count;
141: 
142: 	auto select_node = make_uniq<SelectNode>();
143: 	// handle the CTEs
144: 	if (select.withClause) {
145: 		TransformCTE(*PGPointerCast<duckdb_libpgquery::PGWithClause>(select.withClause), select_node->cte_map);
146: 	}
147: 	if (!pivot->columns) {
148: 		// no pivot columns - not actually a pivot
149: 		select_node->from_table = std::move(source);
150: 		if (pivot->groups) {
151: 			auto groups = TransformStringList(pivot->groups);
152: 			GroupingSet set;
153: 			for (idx_t gr = 0; gr < groups.size(); gr++) {
154: 				auto &group = groups[gr];
155: 				auto colref = make_uniq<ColumnRefExpression>(group);
156: 				select_node->select_list.push_back(colref->Copy());
157: 				select_node->groups.group_expressions.push_back(std::move(colref));
158: 				set.insert(gr);
159: 			}
160: 			select_node->groups.grouping_sets.push_back(std::move(set));
161: 		}
162: 		if (pivot->aggrs) {
163: 			TransformExpressionList(*pivot->aggrs, select_node->select_list);
164: 		}
165: 		return std::move(select_node);
166: 	}
167: 
168: 	// generate CREATE TYPE statements for each of the columns that do not have an IN list
169: 	bool is_pivot = !pivot->unpivots;
170: 	auto columns = TransformPivotList(*pivot->columns, is_pivot);
171: 	auto pivot_idx = PivotEntryCount();
172: 	for (idx_t c = 0; c < columns.size(); c++) {
173: 		auto &col = columns[c];
174: 		if (!col.pivot_enum.empty() || !col.entries.empty()) {
175: 			continue;
176: 		}
177: 		if (col.pivot_expressions.size() != 1) {
178: 			throw InternalException("PIVOT statement with multiple names in pivot entry!?");
179: 		}
180: 		auto enum_name = "__pivot_enum_" + std::to_string(pivot_idx) + "_" + std::to_string(c);
181: 
182: 		auto new_select = make_uniq<SelectNode>();
183: 		ExtractCTEsRecursive(new_select->cte_map);
184: 		new_select->from_table = source->Copy();
185: 		AddPivotEntry(enum_name, std::move(new_select), col.pivot_expressions[0]->Copy(), std::move(col.subquery),
186: 		              has_parameters);
187: 		col.pivot_enum = enum_name;
188: 	}
189: 
190: 	// generate the actual query, including the pivot
191: 	select_node->select_list.push_back(make_uniq<StarExpression>());
192: 
193: 	auto pivot_ref = make_uniq<PivotRef>();
194: 	pivot_ref->source = std::move(source);
195: 	if (pivot->unpivots) {
196: 		pivot_ref->unpivot_names = TransformStringList(pivot->unpivots);
197: 	} else {
198: 		if (pivot->aggrs) {
199: 			TransformExpressionList(*pivot->aggrs, pivot_ref->aggregates);
200: 		} else {
201: 			// pivot but no aggregates specified - push a count star
202: 			vector<unique_ptr<ParsedExpression>> children;
203: 			auto function = make_uniq<FunctionExpression>("count_star", std::move(children));
204: 			pivot_ref->aggregates.push_back(std::move(function));
205: 		}
206: 	}
207: 	if (pivot->groups) {
208: 		pivot_ref->groups = TransformStringList(pivot->groups);
209: 	}
210: 	pivot_ref->pivots = std::move(columns);
211: 	SetQueryLocation(*pivot_ref, pivot->location);
212: 	select_node->from_table = std::move(pivot_ref);
213: 	// transform order by/limit modifiers
214: 	TransformModifiers(select, *select_node);
215: 
216: 	return std::move(select_node);
217: }
218: 
219: } // namespace duckdb
[end of src/parser/transform/statement/transform_pivot_stmt.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: