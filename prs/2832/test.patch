diff --git a/test/parquet/test_parquet_reader.test b/test/parquet/test_parquet_reader.test
index 020d156cf836..8b5b1014afc6 100644
--- a/test/parquet/test_parquet_reader.test
+++ b/test/parquet/test_parquet_reader.test
@@ -61,7 +61,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/manyrowgroups.parquet') limit 5
 89	
 90	
 
-mode skip 
+mode skip
 
 query I
 SELECT * FROM parquet_scan('data/parquet-testing/map.parquet') limit 50;
@@ -117,7 +117,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/map.parquet') limit 50;
 {key: [Content-Encoding, X-Frame-Options, Connection, Via, X-Xss-Protection, Content-Type, Date, X-Cache, Vary, Server, X-Cache-Lookup, X-Content-Type-Options, Content-Length], value: [gzip, SAMEORIGIN, keep-alive, 1.1 ip-10-1-1-216.ec2.internal (squid/4.10-20200322-r358ad2fdf), 1; mode=block, text/html;charset=utf-8, Sat, 30 Jan 2021 16:20:50 GMT, MISS from ip-10-1-1-216.ec2.internal, Accept-Encoding, nginx/1.10.3, HIT from ip-10-1-1-216.ec2.internal:3128, nosniff, 892]}	
 {key: [Content-Encoding, X-Frame-Options, Connection, Via, X-Xss-Protection, Content-Type, Date, X-Cache, Vary, Server, X-Cache-Lookup, X-Content-Type-Options, Content-Length], value: [gzip, SAMEORIGIN, keep-alive, 1.1 ip-10-1-1-216.ec2.internal (squid/4.10-20200322-r358ad2fdf), 1; mode=block, text/html;charset=utf-8, Sat, 30 Jan 2021 16:20:53 GMT, MISS from ip-10-1-1-216.ec2.internal, Accept-Encoding, nginx/1.10.3, HIT from ip-10-1-1-216.ec2.internal:3128, nosniff, 891]}	
 
-mode unskip 
+mode unskip
 
 query I
 SELECT * FROM parquet_scan('data/parquet-testing/arrow/int32_decimal.parquet') limit 50;
@@ -147,14 +147,14 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/int32_decimal.parquet') l
 23.00	
 24.00	
 
-mode skip 
+mode skip
 
 query IIIIII
 SELECT * FROM parquet_scan('data/parquet-testing/arrow/nonnullable.impala.parquet') limit 50;
 ----
 8	[-1]	[[-1, -2], []]	{key: [k1], value: [-1]}	[{key: [], value: []}, {key: [k1], value: [1]}, {key: [], value: []}, {key: [], value: []}]	{a: -1, B: [-1], c: {D: [[{e: -1, f: nonnullable}]]}, G: {key: [], value: []}}	
 
-mode unskip 
+mode unskip
 
 query IIIIIIIIII
 SELECT * FROM parquet_scan('data/parquet-testing/bug687_nulls.parquet') limit 50;
@@ -264,7 +264,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/bug1554.parquet') limit 50;
 1584883:7ANjserj/Xp/vz4XFpL1wOC68SXgZ4LvfE5ggEiTrl1yjOtH4TWIezdNsg4TNakKE5TsYM06P4Qd9HQenS9ksA==	True	NULL	200	
 1584883:sV6uqASHK17GJVEXh2mxbbRIk08qivvqS561cy09Zn+SCUMHZL7J/BLRsx0/kYi1Uzkh52SsocpbQzuYeRT+lQ==	False	NULL	200	
 
-mode skip 
+mode skip
 
 query IIIII
 SELECT * FROM parquet_scan('data/parquet-testing/apkwan.parquet') limit 50;
@@ -320,7 +320,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/apkwan.parquet') limit 50;
 53e99785b7602d9701f447a2	[https://link.springer.com/10.1007/s10039-006-1167-2]	[{name: U. Culemann, id: NULL, org: Universit채tsklinikum des Saarlandes Klinik f체r Unfall-, Hand- und Wiederherstellungschirurgie 66421 Homburg/Saar Deutschland 66421 Homburg/Saar Deutschland}, {name: U. Culemann, id: 53f47248dabfaeee22a7fb77, org: Klinik f체r Unfall|Universit채tsklinikum des Saarlandes}]	en	Beckenringverletzungen	
 53e997d1b7602d9701fc2268	[http://dx.doi.org/10.1098/rstb.1982.0151, http://www.ncbi.nlm.nih.gov/pubmed/6130546?report=xml&format=text]	[{name: K. J. Ullrich, id: 5406d5addabfae44f0860eb9, org: NULL}, {name: H. Murer, id: 53f437cedabfaedf4358c982, org: NULL}]	en	Sulphate and Phosphate Transport in the Renal Proximal Tubule	
 
-mode unskip 
+mode unskip
 
 query II
 SELECT * FROM parquet_scan('data/parquet-testing/arrow/nested_lists.snappy.parquet') limit 50;
@@ -329,21 +329,17 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/nested_lists.snappy.parqu
 [[[a, b], [c, d]], [NULL, [e]]]	1
 [[[a, b], [c, d], [e]], [NULL, [f]]]	1
 
-mode skip 
-
 query I
 SELECT * FROM parquet_scan('data/parquet-testing/arrow/nulls.snappy.parquet') limit 50;
 ----
-{b_c_int: NULL}	
-{b_c_int: NULL}	
-{b_c_int: NULL}	
-{b_c_int: NULL}	
-{b_c_int: NULL}	
-{b_c_int: NULL}	
-{b_c_int: NULL}	
-{b_c_int: NULL}	
-
-mode unskip 
+{'b_c_int': NULL}
+{'b_c_int': NULL}
+{'b_c_int': NULL}
+{'b_c_int': NULL}
+{'b_c_int': NULL}
+{'b_c_int': NULL}
+{'b_c_int': NULL}
+{'b_c_int': NULL}
 
 query III
 SELECT * FROM parquet_scan('data/parquet-testing/nan-float.parquet') limit 50;
@@ -412,7 +408,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/struct.parquet') limit 50;
 {'str_field': hello, 'f64_field': NULL}
 {'str_field': NULL, 'f64_field': 1.230000}
 
-mode skip 
+mode skip
 
 query I
 SELECT * FROM parquet_scan('data/parquet-testing/arrow/byte_array_decimal.parquet') limit 50;
@@ -442,7 +438,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/byte_array_decimal.parque
 23.00	
 24.00	
 
-mode unskip 
+mode unskip
 
 query II
 SELECT * FROM parquet_scan('data/parquet-testing/arrow/list_columns.parquet') limit 50;
@@ -561,7 +557,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/lineitem-top10000.gzip.parquet'
 38	175839	874	1	44	84252.52	0.04	0.02	N	O	1996-09-29	1996-11-17	1996-09-30	COLLECT COD	MAIL	s. blithely unusual theodolites am	
 39	2320	9821	1	44	53782.08	0.09	0.06	N	O	1996-11-14	1996-12-15	1996-12-12	COLLECT COD	RAIL	eodolites. careful	
 
-mode skip 
+mode skip
 
 query III
 SELECT * FROM parquet_scan('data/parquet-testing/arrow/nested_maps.snappy.parquet') limit 50;
@@ -573,7 +569,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/nested_maps.snappy.parque
 {key: [e], value: [{key: [1], value: [True]}]}	1	1.0	
 {key: [f], value: [{key: [3, 4, 5], value: [True, False, True]}]}	1	1.0	
 
-mode unskip 
+mode unskip
 
 query I
 SELECT * FROM parquet_scan('data/parquet-testing/arrow/dict-page-offset-zero.parquet') limit 50;
@@ -955,7 +951,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/bug1589.parquet') limit 50;
 200	NULL	
 300	NULL	
 
-mode skip 
+mode skip
 
 query I
 SELECT * FROM parquet_scan('data/parquet-testing/arrow/hadoop_lz4_compressed_larger.parquet') limit 50;
@@ -1011,9 +1007,9 @@ f2807544-a424-444a-add3-3d5d486b70e2
 d0018041-41e3-4013-ba90-535ba03d46c3	
 c50e8ade-6051-436f-a26e-acc9c0594be5	
 
-mode unskip 
+mode unskip
 
-mode skip 
+mode skip
 
 query III
 SELECT * FROM parquet_scan('data/parquet-testing/arrow/non_hadoop_lz4_compressed.parquet') limit 50;
@@ -1023,7 +1019,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/non_hadoop_lz4_compressed
 1593604801	abc	42.125	
 1593604801	def	7.7	
 
-mode unskip 
+mode unskip
 
 query IIIIIIIIIII
 SELECT * FROM parquet_scan('data/parquet-testing/arrow/alltypes_plain.parquet') limit 50;
@@ -1051,19 +1047,15 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/repeated_no_annotation.pa
 
 mode unskip 
 
-mode skip 
-
 query IIIIIIIIIIII
 SELECT * FROM parquet_scan('data/parquet-testing/data-types.parquet') limit 50;
 ----
 NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	
-42	43	44	45	4.599999904632568	4.7	4.80	49	50	True	2019-11-26 20:11:42.501000	2020-01-10	
+42	43	44	45	4.599999904632568	4.7	4.80	49	50	True	2019-11-26 20:11:42.501	2020-01-10
 -127	-32767	-2147483647	-9223372036854775807	-4.599999904632568	-4.7	NULL	NULL	NULL	False	NULL	NULL	
 127	32767	2147483647	9223372036854775807	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	
 NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	
 
-mode unskip 
-
 query IIII
 SELECT * FROM parquet_scan('data/parquet-testing/unsigned.parquet') limit 50;
 ----
@@ -1120,7 +1112,7 @@ NULL
 1996-01-01	
 1997-01-01	
 
-mode skip 
+mode skip
 
 query IIIIII
 SELECT * FROM parquet_scan('data/parquet-testing/arrow/nullable.impala.parquet') limit 50;
@@ -1133,9 +1125,9 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/nullable.impala.parquet')
 6	NULL	NULL	NULL	NULL	NULL	
 7	NULL	[NULL, [5, 6]]	{key: [k1, k3], value: [NULL, NULL]}	NULL	{A: 7, : [2, 3, NULL], C: {d: [[], [NULL], NULL]}, g: NULL}	
 
-mode unskip 
+mode unskip
 
-mode skip 
+mode skip
 
 query III
 SELECT * FROM parquet_scan('data/parquet-testing/arrow/hadoop_lz4_compressed.parquet') limit 50;
@@ -1145,7 +1137,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/hadoop_lz4_compressed.par
 1593604801	abc	42.125	
 1593604801	def	7.7	
 
-mode unskip 
+mode unskip
 
 query I
 SELECT * FROM parquet_scan('data/parquet-testing/fixed.parquet') limit 50;
@@ -1396,14 +1388,14 @@ SELECT * FROM parquet_scan('data/parquet-testing/glob/t1.parquet') limit 50;
 query III
 SELECT * FROM parquet_scan('data/parquet-testing/bug2557.parquet') limit 10
 ----
-[adipiscing, elit]	[267]	NULL
+[adipiscing, elit]	[267]	[]
 [adipiscing, elit]	[58, 146]	[3105.735731, 7332.144961, 2693.459659, 2058.830347]
 [dolor, sit, amet, consectetur, adipiscing, elit]	[26, 701]	[2252.315041]
 [Lorem, ipsum, dolor, sit, amet, consectetur, adipiscing, elit]	[763]	[4318.131164, 703.332322]
-[consectetur, adipiscing, elit]	NULL	[4921.065813]
+[consectetur, adipiscing, elit]	[]	[4921.065813]
 [ipsum, dolor, sit, amet, consectetur, adipiscing, elit]	[503]	[3143.311724]
 [amet, consectetur, adipiscing, elit]	[981]	[1556.844782, 5388.780546]
-[consectetur, adipiscing, elit]	[822, 843, 702, 469]	NULL
+[consectetur, adipiscing, elit]	[822, 843, 702, 469]	[]
 [dolor, sit, amet, consectetur, adipiscing, elit]	[698, 385]	[3591.160509]
 [Lorem, ipsum, dolor, sit, amet, consectetur, adipiscing, elit]	[597, 719]	[1218.803740]
 
diff --git a/test/sql/copy/parquet/parquet_1589.test b/test/sql/copy/parquet/parquet_1589.test
index 800b91722e80..f6d6b487b1eb 100644
--- a/test/sql/copy/parquet/parquet_1589.test
+++ b/test/sql/copy/parquet/parquet_1589.test
@@ -4,8 +4,8 @@
 
 require parquet
 
-# statement ok
-# pragma enable_verification
+statement ok
+pragma enable_verification
 
 query I
 SELECT backlink_count FROM parquet_scan('data/parquet-testing/bug1589.parquet') LIMIT 1
diff --git a/test/sql/copy/parquet/parquet_2267.test b/test/sql/copy/parquet/parquet_2267.test
index 766e237a423c..d5d09c5de92f 100644
--- a/test/sql/copy/parquet/parquet_2267.test
+++ b/test/sql/copy/parquet/parquet_2267.test
@@ -7,7 +7,7 @@ require parquet
 query I
 SELECT * FROM parquet_scan('data/parquet-testing/bug2267.parquet')
 ----
-[{'disabledPlans': [bea4c11e-220a-4e6d-8eb8-8ea15d019f90], 'skuId': c7df2760-2c81-4ef7-b578-5b5392b571df}, {'disabledPlans': [8a256a2b-b617-496d-b51b-e76466e88db0, 41781fb2-bc02-4b7c-bd55-b576c07bb09d, eec0eb4f-6444-4f95-aba0-50c24d67f998], 'skuId': 84a661c4-e949-4bd2-a560-ed7766fcaf2b}, {'disabledPlans': NULL, 'skuId': b05e124f-c7cc-45a0-a6aa-8cf78c946968}, {'disabledPlans': NULL, 'skuId': f30db892-07e9-47e9-837c-80727f46fd3d}]
+[{'disabledPlans': [bea4c11e-220a-4e6d-8eb8-8ea15d019f90], 'skuId': c7df2760-2c81-4ef7-b578-5b5392b571df}, {'disabledPlans': [8a256a2b-b617-496d-b51b-e76466e88db0, 41781fb2-bc02-4b7c-bd55-b576c07bb09d, eec0eb4f-6444-4f95-aba0-50c24d67f998], 'skuId': 84a661c4-e949-4bd2-a560-ed7766fcaf2b}, {'disabledPlans': [], 'skuId': b05e124f-c7cc-45a0-a6aa-8cf78c946968}, {'disabledPlans': [], 'skuId': f30db892-07e9-47e9-837c-80727f46fd3d}]
 
 query I
 SELECT assignedLicenses[0] FROM parquet_scan('data/parquet-testing/bug2267.parquet')
diff --git a/test/sql/copy/parquet/parquet_filter_bug1391.test b/test/sql/copy/parquet/parquet_filter_bug1391.test
index 0462328d3f8d..841171b64494 100644
--- a/test/sql/copy/parquet/parquet_filter_bug1391.test
+++ b/test/sql/copy/parquet/parquet_filter_bug1391.test
@@ -3,32 +3,31 @@
 # group: [parquet]
 
 require parquet
-require vector_size 512
 
 statement ok
 PRAGMA enable_verification
 
 statement ok
 CREATE VIEW tbl AS SELECT * FROM PARQUET_SCAN('data/parquet-testing/filter_bug1391.parquet');
-#
-#query I
-#SELECT ORGUNITID FROM tbl LIMIT 10
-#----
-#98
-#13
-#175
-#200
-#262
-#206
-#204
-#131
-#181
-#269
-#
-#query I
-#SELECT COUNT(*) FROM tbl;
-#----
-#9789
+
+query I
+SELECT ORGUNITID FROM tbl LIMIT 10
+----
+98
+13
+175
+200
+262
+206
+204
+131
+181
+269
+
+query I
+SELECT COUNT(*) FROM tbl;
+----
+9789
 
 query I
 SELECT COUNT(*) FROM tbl
diff --git a/test/sql/copy/parquet/parquet_write_codecs.test b/test/sql/copy/parquet/parquet_write_codecs.test
index 294e4de474cc..bd8c169db110 100644
--- a/test/sql/copy/parquet/parquet_write_codecs.test
+++ b/test/sql/copy/parquet/parquet_write_codecs.test
@@ -4,8 +4,6 @@
 
 require parquet
 
-require vector_size 64
-
 # codec uncompressed
 statement ok
 COPY (SELECT 42, 'hello') TO '__TEST_DIR__/uncompressed.parquet' (FORMAT 'parquet', CODEC 'UNCOMPRESSED');
diff --git a/test/sql/copy/parquet/test_aws_files.test b/test/sql/copy/parquet/test_aws_files.test
index 7b6a0c1b30ff..5dce1a9dcd10 100644
--- a/test/sql/copy/parquet/test_aws_files.test
+++ b/test/sql/copy/parquet/test_aws_files.test
@@ -3,7 +3,6 @@
 # group: [parquet]
 
 require parquet
-require vector_size 512
 
 statement ok
 PRAGMA enable_verification
diff --git a/test/sql/copy/parquet/test_parquet_filter_pushdown.test b/test/sql/copy/parquet/test_parquet_filter_pushdown.test
index 2ef24b434b19..e47bcf0b809b 100644
--- a/test/sql/copy/parquet/test_parquet_filter_pushdown.test
+++ b/test/sql/copy/parquet/test_parquet_filter_pushdown.test
@@ -3,7 +3,6 @@
 # group: [parquet]
 
 require parquet
-require vector_size 512
 
 statement ok
 pragma enable_verification
diff --git a/test/sql/copy/parquet/test_parquet_nested.test b/test/sql/copy/parquet/test_parquet_nested.test
index ee5f1d25d302..dfda69aea05a 100644
--- a/test/sql/copy/parquet/test_parquet_nested.test
+++ b/test/sql/copy/parquet/test_parquet_nested.test
@@ -96,13 +96,13 @@ query II
 SELECT id, url FROM parquet_scan('data/parquet-testing/apkwan.parquet') limit 10
 ----
 53e997b9b7602d9701f9f044	[https://link.springer.com/10.1007/s00108-004-1229-0]
-53e997b2b7602d9701f8fea5	NULL
+53e997b2b7602d9701f8fea5	[]
 53e997aeb7602d9701f8856e	[http://www.ncbi.nlm.nih.gov/pubmed/4669724?report=xml&format=text, http://www.ncbi.nlm.nih.gov/pubmed/5123793?report=xml&format=text, http://www.ncbi.nlm.nih.gov/pubmed/5315218?report=xml&format=text]
-53e997bab7602d9701fa1e34	NULL
-53e997abb7602d9701f846c0	NULL
-53e9978db7602d9701f4d7e8	NULL
+53e997bab7602d9701fa1e34	[]
+53e997abb7602d9701f846c0	[]
+53e9978db7602d9701f4d7e8	[]
 53e9984bb7602d970207c61d	[http://subs.emis.de/LNI/Proceedings/Proceedings26/article639.html]
-53e99796b7602d9701f5cd36	NULL
+53e99796b7602d9701f5cd36	[]
 53e99809b7602d970201f551	[http://dx.doi.org/10.1016/S0140-6736(00)82170-4, http://www.ncbi.nlm.nih.gov/pubmed/20914302?report=xml&format=text]
 53e997a6b7602d9701f7ffb0	[http://www.ncbi.nlm.nih.gov/pubmed/4051185?report=xml&format=text]
 
diff --git a/test/sql/copy/parquet/test_parquet_stats.test b/test/sql/copy/parquet/test_parquet_stats.test
index a09e313b98a9..dcc7cff2a1d0 100644
--- a/test/sql/copy/parquet/test_parquet_stats.test
+++ b/test/sql/copy/parquet/test_parquet_stats.test
@@ -3,7 +3,6 @@
 # group: [parquet]
 
 require parquet
-require vector_size 512
 
 statement ok
 PRAGMA explain_output = PHYSICAL_ONLY
diff --git a/test/sql/copy/parquet/writer/list_of_bools.test b/test/sql/copy/parquet/writer/list_of_bools.test
new file mode 100644
index 000000000000..23fa9b10adc6
--- /dev/null
+++ b/test/sql/copy/parquet/writer/list_of_bools.test
@@ -0,0 +1,91 @@
+# name: test/sql/copy/parquet/writer/list_of_bools.test
+# description: Parquet write list of bools
+# group: [writer]
+
+require parquet
+
+# big list of bools
+statement ok
+CREATE TABLE list_of_bools AS
+     SELECT LIST(i%2==0) l FROM range(1373) tbl(i)
+     UNION ALL
+     SELECT [true, false, NULL, false, true]
+     UNION ALL
+     SELECT []
+     UNION ALL
+     SELECT NULL
+     UNION ALL
+     SELECT LIST(i%3==0) l FROM range(9937) tbl(i)
+     UNION ALL
+     SELECT [true, false, NULL, false, true]
+
+query III
+SELECT COUNT(*), COUNT(b), SUM(CASE WHEN b THEN 1 ELSE 0 END)
+FROM (SELECT unnest(l) b FROM list_of_bools)
+----
+11320	11318	4004
+
+statement ok
+COPY list_of_bools TO '__TEST_DIR__/list_of_bools.parquet' (FORMAT PARQUET)
+
+query III
+SELECT COUNT(*), COUNT(b), SUM(CASE WHEN b THEN 1 ELSE 0 END)
+FROM (SELECT unnest(l) b FROM '__TEST_DIR__/list_of_bools.parquet')
+----
+11320	11318	4004
+
+# many lists of integers
+statement ok
+CREATE TABLE many_ints AS
+	SELECT [1, 0, 1] AS l FROM range(1373)
+	UNION ALL
+	SELECT []
+	UNION ALL
+	SELECT NULL
+	UNION ALL
+	SELECT [1, 0, NULL, 0, 1]
+	UNION ALL
+    SELECT [1, 0, NULL, 1] l FROM range(9937) tbl(i)
+
+query III
+SELECT COUNT(*), COUNT(b), SUM(b)
+FROM (SELECT unnest(l) b FROM many_ints)
+----
+43872	33934	22622
+
+statement ok
+COPY many_ints TO 'many_ints.parquet' (FORMAT PARQUET)
+
+query III
+SELECT COUNT(*), COUNT(b), SUM(b)
+FROM (SELECT unnest(l) b FROM 'many_ints.parquet')
+----
+43872	33934	22622
+
+# many lists of bools
+statement ok
+CREATE TABLE many_bools AS
+	SELECT [true, false, true] AS l FROM range(1373)
+	UNION ALL
+	SELECT []
+	UNION ALL
+	SELECT NULL
+	UNION ALL
+	SELECT [true, false, NULL, false, true]
+	UNION ALL
+    SELECT [true, false, NULL, true] l FROM range(9937) tbl(i)
+
+query III
+SELECT COUNT(*), COUNT(b), SUM(CASE WHEN b THEN 1 ELSE 0 END)
+FROM (SELECT unnest(l) b FROM many_bools)
+----
+43872	33934	22622
+
+statement ok
+COPY many_bools TO '__TEST_DIR__/many_bools.parquet' (FORMAT PARQUET)
+
+query III
+SELECT COUNT(*), COUNT(b), SUM(CASE WHEN b THEN 1 ELSE 0 END)
+FROM (SELECT unnest(l) b FROM '__TEST_DIR__/many_bools.parquet')
+----
+43872	33934	22622
diff --git a/test/sql/copy/parquet/writer/parquet_large_blobs.test_coverage b/test/sql/copy/parquet/writer/parquet_large_blobs.test_coverage
new file mode 100644
index 000000000000..d9a76df1433d
--- /dev/null
+++ b/test/sql/copy/parquet/writer/parquet_large_blobs.test_coverage
@@ -0,0 +1,20 @@
+# name: test/sql/copy/parquet/writer/parquet_large_blobs.test_coverage
+# description: Test writing of large blobs into parquet files
+# group: [writer]
+
+require parquet
+
+statement ok
+CREATE TABLE large_strings AS SELECT repeat('duckduck', 10000+i) i FROM range(4000) tbl(i);
+
+query III nosort minmaxstrlen
+SELECT MIN(strlen(i)), MAX(strlen(i)), AVG(strlen(i)) FROM large_strings;
+
+statement ok
+COPY large_strings TO '__TEST_DIR__/largestrings.parquet' (FORMAT PARQUET);
+
+statement ok
+SELECT * FROM parquet_metadata('__TEST_DIR__/largestrings.parquet');
+
+query III nosort minmaxstrlen
+SELECT MIN(strlen(i)), MAX(strlen(i)), AVG(strlen(i)) FROM large_strings;
diff --git a/test/sql/copy/parquet/writer/parquet_write_booleans.test b/test/sql/copy/parquet/writer/parquet_write_booleans.test
new file mode 100644
index 000000000000..0362a442d163
--- /dev/null
+++ b/test/sql/copy/parquet/writer/parquet_write_booleans.test
@@ -0,0 +1,36 @@
+# name: test/sql/copy/parquet/writer/parquet_write_booleans.test
+# description: Parquet bools round trip
+# group: [writer]
+
+require parquet
+
+require vector_size 512
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE bools(b BOOL)
+
+statement ok
+INSERT INTO bools SELECT CASE WHEN i%2=0 THEN NULL ELSE i%7=0 OR i%3=0 END b FROM range(10000) tbl(i);
+
+query IIIIII
+SELECT COUNT(*), COUNT(b), BOOL_AND(b), BOOL_OR(b), SUM(CASE WHEN b THEN 1 ELSE 0 END) true_count, SUM(CASE WHEN b THEN 0 ELSE 1 END) false_count
+FROM bools
+----
+10000	5000	False	True	2143	7857
+
+statement ok
+COPY bools TO '__TEST_DIR__/bools.parquet' (FORMAT 'parquet');
+
+query IIIIII
+SELECT COUNT(*), COUNT(b), BOOL_AND(b), BOOL_OR(b), SUM(CASE WHEN b THEN 1 ELSE 0 END) true_count, SUM(CASE WHEN b THEN 0 ELSE 1 END) false_count
+FROM '__TEST_DIR__/bools.parquet'
+----
+10000	5000	False	True	2143	7857
+
+query I
+SELECT typeof(b) FROM '__TEST_DIR__/bools.parquet' LIMIT 1
+----
+BOOLEAN
diff --git a/test/sql/copy/parquet/writer/parquet_write_date.test b/test/sql/copy/parquet/writer/parquet_write_date.test
index eb7f60ca2f7c..6fb962f2f258 100644
--- a/test/sql/copy/parquet/writer/parquet_write_date.test
+++ b/test/sql/copy/parquet/writer/parquet_write_date.test
@@ -4,8 +4,6 @@
 
 require parquet
 
-require vector_size 64
-
 statement ok
 PRAGMA enable_verification
 
diff --git a/test/sql/copy/parquet/writer/parquet_write_hugeint.test b/test/sql/copy/parquet/writer/parquet_write_hugeint.test
index 9e61b1f7fc14..3000230ff5db 100644
--- a/test/sql/copy/parquet/writer/parquet_write_hugeint.test
+++ b/test/sql/copy/parquet/writer/parquet_write_hugeint.test
@@ -4,8 +4,6 @@
 
 require parquet
 
-require vector_size 64
-
 statement ok
 PRAGMA enable_verification
 
diff --git a/test/sql/copy/parquet/writer/parquet_write_signed.test b/test/sql/copy/parquet/writer/parquet_write_signed.test
index af5c14286962..a2f8a1734049 100644
--- a/test/sql/copy/parquet/writer/parquet_write_signed.test
+++ b/test/sql/copy/parquet/writer/parquet_write_signed.test
@@ -4,8 +4,6 @@
 
 require parquet
 
-require vector_size 64
-
 statement ok
 PRAGMA enable_verification
 
diff --git a/test/sql/copy/parquet/writer/parquet_write_timestamp.test b/test/sql/copy/parquet/writer/parquet_write_timestamp.test
index 415d5338db94..eeb3c73a16e8 100644
--- a/test/sql/copy/parquet/writer/parquet_write_timestamp.test
+++ b/test/sql/copy/parquet/writer/parquet_write_timestamp.test
@@ -4,8 +4,6 @@
 
 require parquet
 
-require vector_size 64
-
 statement ok
 PRAGMA enable_verification
 
diff --git a/test/sql/copy/parquet/writer/parquet_write_tpch.test_slow b/test/sql/copy/parquet/writer/parquet_write_tpch.test_slow
new file mode 100644
index 000000000000..3fdbd92e8a26
--- /dev/null
+++ b/test/sql/copy/parquet/writer/parquet_write_tpch.test_slow
@@ -0,0 +1,52 @@
+# name: test/sql/copy/parquet/writer/parquet_write_tpch.test_slow
+# description: Parquet TPC-H tests
+# group: [writer]
+
+require parquet
+
+require tpch
+
+statement ok
+CREATE SCHEMA tpch;
+
+statement ok
+CALL dbgen(sf=1, schema='tpch');
+
+foreach tbl lineitem nation orders supplier part partsupp region customer
+
+statement ok
+COPY tpch.${tbl} TO '__TEST_DIR__/${tbl}.parquet' (FORMAT 'PARQUET', COMPRESSION 'ZSTD');
+
+statement ok
+CREATE VIEW ${tbl} AS SELECT * FROM parquet_scan('__TEST_DIR__/${tbl}.parquet');
+
+endloop
+
+loop i 1 9
+
+query I
+PRAGMA tpch(${i})
+----
+<FILE>:extension/tpch/dbgen/answers/sf1/q0${i}.csv
+
+endloop
+
+# skip q15 for now: it is non-deterministic with multi-threading and doubles
+# this can be re-enabled once we write decimals to parquet
+loop i 10 15
+
+query I
+PRAGMA tpch(${i})
+----
+<FILE>:extension/tpch/dbgen/answers/sf1/q${i}.csv
+
+endloop
+
+loop i 16 23
+
+query I
+PRAGMA tpch(${i})
+----
+<FILE>:extension/tpch/dbgen/answers/sf1/q${i}.csv
+
+endloop
diff --git a/test/sql/copy/parquet/writer/parquet_write_tpch_nested.test_slow b/test/sql/copy/parquet/writer/parquet_write_tpch_nested.test_slow
new file mode 100644
index 000000000000..ee5b8f2c98cd
--- /dev/null
+++ b/test/sql/copy/parquet/writer/parquet_write_tpch_nested.test_slow
@@ -0,0 +1,84 @@
+# name: test/sql/copy/parquet/writer/parquet_write_tpch_nested.test_slow
+# description: Parquet TPC-H tests
+# group: [writer]
+
+require parquet
+
+require tpch
+
+statement ok
+CREATE SCHEMA tpch;
+
+statement ok
+CALL dbgen(sf=0.1, schema='tpch');
+
+
+# transform lineitem into a list of structs
+statement ok
+CREATE VIEW lineitem_array_view AS SELECT LIST({'l_orderkey': l_orderkey,
+	'l_partkey': l_partkey,
+	'l_suppkey': l_suppkey,
+	'l_linenumber': l_linenumber,
+	'l_quantity': l_quantity,
+	'l_extendedprice': l_extendedprice,
+	'l_discount': l_discount,
+	'l_tax': l_tax,
+	'l_returnflag': l_returnflag,
+	'l_linestatus': l_linestatus,
+	'l_shipdate': l_shipdate,
+	'l_commitdate': l_commitdate,
+	'l_receiptdate': l_receiptdate,
+	'l_shipinstruct': l_shipinstruct,
+	'l_shipmode': l_shipmode,
+	'l_comment': l_comment}) lineitem_array FROM tpch.lineitem
+
+statement ok
+COPY lineitem_array_view TO '__TEST_DIR__/lineitem.parquet' (FORMAT 'PARQUET', COMPRESSION 'ZSTD');
+
+statement ok
+CREATE VIEW lineitem AS SELECT
+	s.l_orderkey AS l_orderkey,
+	s.l_partkey AS l_partkey,
+	s.l_suppkey AS l_suppkey,
+	s.l_linenumber AS l_linenumber,
+	s.l_quantity AS l_quantity,
+	s.l_extendedprice AS l_extendedprice,
+	s.l_discount AS l_discount,
+	s.l_tax AS l_tax,
+	s.l_returnflag AS l_returnflag,
+	s.l_linestatus AS l_linestatus,
+	s.l_shipdate AS l_shipdate,
+	s.l_commitdate AS l_commitdate,
+	s.l_receiptdate AS l_receiptdate,
+	s.l_shipinstruct AS l_shipinstruct,
+	s.l_shipmode AS l_shipmode,
+	s.l_comment AS l_comment
+	FROM (SELECT UNNEST(lineitem_array) s FROM parquet_scan('__TEST_DIR__/lineitem.parquet'));
+
+foreach tbl nation orders supplier part partsupp region customer
+
+statement ok
+COPY tpch.${tbl} TO '__TEST_DIR__/${tbl}.parquet' (FORMAT 'PARQUET', COMPRESSION 'ZSTD');
+
+statement ok
+CREATE VIEW ${tbl} AS SELECT * FROM parquet_scan('__TEST_DIR__/${tbl}.parquet');
+
+endloop
+
+loop i 1 9
+
+query I
+PRAGMA tpch(${i})
+----
+<FILE>:extension/tpch/dbgen/answers/sf0.1/q0${i}.csv
+
+endloop
+
+loop i 10 23
+
+query I
+PRAGMA tpch(${i})
+----
+<FILE>:extension/tpch/dbgen/answers/sf0.1/q${i}.csv
+
+endloop
diff --git a/test/sql/copy/parquet/writer/parquet_write_unsigned.test b/test/sql/copy/parquet/writer/parquet_write_unsigned.test
index c97dd5c22586..f8f4137bd6e8 100644
--- a/test/sql/copy/parquet/writer/parquet_write_unsigned.test
+++ b/test/sql/copy/parquet/writer/parquet_write_unsigned.test
@@ -4,8 +4,6 @@
 
 require parquet
 
-require vector_size 64
-
 statement ok
 PRAGMA enable_verification
 
diff --git a/test/sql/copy/parquet/writer/parquet_zstd_sequence.test_slow b/test/sql/copy/parquet/writer/parquet_zstd_sequence.test_slow
new file mode 100644
index 000000000000..29120344c07e
--- /dev/null
+++ b/test/sql/copy/parquet/writer/parquet_zstd_sequence.test_slow
@@ -0,0 +1,40 @@
+# name: test/sql/copy/parquet/writer/parquet_zstd_sequence.test_slow
+# description: Test writing of large blobs into parquet files
+# group: [writer]
+
+require parquet
+
+require 64bit
+
+statement ok
+COPY (SELECT * FROM read_csv_auto('data/csv/sequences.csv.gz', delim=',', header=True) LIMIT 25000) TO '__TEST_DIR__/duckseq.parquet' (FORMAT 'PARQUET', CODEC 'ZSTD', ROW_GROUP_SIZE 25000);
+
+query IIIIII
+select count(*), min(strain), max(strain), min(strlen(sequence)), max(strlen(sequence)), avg(strlen(sequence))
+from '__TEST_DIR__/duckseq.parquet';
+----
+25000	AUS/NT01/2020	canine/HKG/20-03695/2020	17340	30018	29855.647080
+
+statement ok
+COPY
+(
+	SELECT lstrain::VARCHAR[] lstrain, lsequence::VARCHAR[] lsequence FROM (VALUES ([], []), (NULL, NULL), ([], [])) tbl(lstrain, lsequence)
+	UNION ALL
+	SELECT * FROM (
+		SELECT LIST(strain) AS lstrain, LIST(sequence) AS lsequence FROM '__TEST_DIR__/duckseq.parquet' LIMIT 10000
+	)
+	UNION ALL
+	SELECT * FROM (VALUES ([], []), (NULL, NULL), ([], []))
+)
+TO '__TEST_DIR__/duckseq2.parquet' (FORMAT 'PARQUET', CODEC 'ZSTD');
+
+query I
+SELECT COUNT(*) FROM '__TEST_DIR__/duckseq2.parquet'
+----
+7
+
+query IIIIII nosort querylabel
+select count(*), min(strain), max(strain), min(strlen(sequence)), max(strlen(sequence)), avg(strlen(sequence))
+from (SELECT UNNEST(lstrain) AS strain, UNNEST(lsequence) AS sequence FROM '__TEST_DIR__/duckseq2.parquet');
+----
+100000	ARG/Cordoba-1006-155/2020	tiger/NY/040420/2020	17340	30643	29821.264410
diff --git a/test/sql/copy/parquet/writer/test_parquet_write.test b/test/sql/copy/parquet/writer/test_parquet_write.test
index b7d715fdcd80..4f491c5bf600 100644
--- a/test/sql/copy/parquet/writer/test_parquet_write.test
+++ b/test/sql/copy/parquet/writer/test_parquet_write.test
@@ -4,9 +4,6 @@
 
 require parquet
 
-# single scalar value
-require vector_size 64
-
 statement ok
 COPY (SELECT 42) TO '__TEST_DIR__/scalar.parquet' (FORMAT 'parquet');
 
diff --git a/test/sql/copy/parquet/writer/test_parquet_write_complex.test b/test/sql/copy/parquet/writer/test_parquet_write_complex.test
index d650ab3a1946..90ac7229fac5 100644
--- a/test/sql/copy/parquet/writer/test_parquet_write_complex.test
+++ b/test/sql/copy/parquet/writer/test_parquet_write_complex.test
@@ -4,8 +4,7 @@
 
 require parquet
 
-# writer requires vector_size >= 64
-require vector_size 64
+require vector_size 512
 
 # alltypes_dictionary: scan as parquet
 query I nosort alltypes_dictionary
diff --git a/test/sql/copy/parquet/writer/write_complex_nested.test b/test/sql/copy/parquet/writer/write_complex_nested.test
new file mode 100644
index 000000000000..04f495841acc
--- /dev/null
+++ b/test/sql/copy/parquet/writer/write_complex_nested.test
@@ -0,0 +1,137 @@
+# name: test/sql/copy/parquet/writer/write_complex_nested.test
+# description: Parquet write complex structures
+# group: [writer]
+
+require parquet
+
+# struct of lists
+statement ok
+CREATE TABLE struct_of_lists AS SELECT * FROM (VALUES
+	({'a': [1, 2, 3], 'b': ['hello', 'world']}),
+	({'a': [4, NULL, 5], 'b': ['duckduck', 'goose']}),
+	({'a': NULL, 'b': ['longlonglonglonglonglong', NULL, NULL]}),
+	(NULL),
+    ({'a': [], 'b': []}),
+    ({'a': [1, 2, 3], 'b': NULL})
+) tbl(i);
+
+statement ok
+COPY struct_of_lists TO '__TEST_DIR__/complex_list.parquet' (FORMAT 'parquet');
+
+query I
+SELECT i FROM parquet_scan('__TEST_DIR__/complex_list.parquet');
+----
+{'a': [1, 2, 3], 'b': [hello, world]}
+{'a': [4, NULL, 5], 'b': [duckduck, goose]}
+{'a': NULL, 'b': [longlonglonglonglonglong, NULL, NULL]}
+NULL
+{'a': [], 'b': []}
+{'a': [1, 2, 3], 'b': NULL}
+
+# list of structs
+statement ok
+CREATE TABLE list_of_structs AS SELECT * FROM (VALUES
+	([{'a': 1, 'b': 100}, NULL, {'a': 2, 'b': 101}]),
+	(NULL),
+	([]),
+	([{'a': NULL, 'b': 102}, {'a': 3, 'b': NULL}, NULL])
+) tbl(i);
+
+statement ok
+COPY list_of_structs TO '__TEST_DIR__/complex_list.parquet' (FORMAT 'parquet');
+
+query I
+SELECT i FROM parquet_scan('__TEST_DIR__/complex_list.parquet');
+----
+[{'a': 1, 'b': 100}, NULL, {'a': 2, 'b': 101}]
+NULL
+[]
+[{'a': NULL, 'b': 102}, {'a': 3, 'b': NULL}, NULL]
+
+# list of structs of structs
+statement ok
+CREATE TABLE list_of_struct_of_structs AS SELECT * FROM (VALUES
+	([{'a': {'x': 33}, 'b': {'y': 42, 'z': 99}}, NULL, {'a': {'x': NULL}, 'b': {'y': 43, 'z': 100}}]),
+	(NULL),
+	([]),
+	([{'a': NULL, 'b': {'y': NULL, 'z': 101}}, {'a': {'x': 34}, 'b': {'y': 43, 'z': NULL}}]),
+	([{'a': NULL, 'b': NULL}])
+) tbl(i);
+
+statement ok
+COPY list_of_struct_of_structs TO '__TEST_DIR__/complex_list.parquet' (FORMAT 'parquet');
+
+query I
+SELECT i FROM parquet_scan('__TEST_DIR__/complex_list.parquet');
+----
+[{'a': {'x': 33}, 'b': {'y': 42, 'z': 99}}, NULL, {'a': {'x': NULL}, 'b': {'y': 43, 'z': 100}}]
+NULL
+[]
+[{'a': NULL, 'b': {'y': NULL, 'z': 101}}, {'a': {'x': 34}, 'b': {'y': 43, 'z': NULL}}]
+[{'a': NULL, 'b': NULL}]
+
+# list of lists
+# no empty lists or nulls
+statement ok
+CREATE TABLE list_of_lists_simple AS SELECT * FROM (VALUES
+	([[1, 2, 3], [4, 5]]),
+	([[6, 7]]),
+	([[8, 9, 10], [11, 12]])
+) tbl(i);
+
+statement ok
+COPY list_of_lists_simple TO '__TEST_DIR__/complex_list.parquet' (FORMAT 'parquet');
+
+query I
+SELECT i FROM parquet_scan('__TEST_DIR__/complex_list.parquet');
+----
+[[1, 2, 3], [4, 5]]
+[[6, 7]]
+[[8, 9, 10], [11, 12]]
+
+# list of lists with nulls and empty lists
+statement ok
+CREATE TABLE list_of_lists AS SELECT * FROM (VALUES
+	([[1, 2, 3], [4, 5], [], [6, 7]]),
+	([[8, NULL, 10], NULL, []]),
+	([]),
+	(NULL),
+	([[11, 12, 13, 14], [], NULL, [], [], [15], [NULL, NULL, NULL]])
+) tbl(i);
+
+statement ok
+COPY list_of_lists TO '__TEST_DIR__/complex_list.parquet' (FORMAT 'parquet');
+
+query I
+SELECT i FROM parquet_scan('__TEST_DIR__/complex_list.parquet');
+----
+[[1, 2, 3], [4, 5], [], [6, 7]]
+[[8, NULL, 10], NULL, []]
+[]
+NULL
+[[11, 12, 13, 14], [], NULL, [], [], [15], [NULL, NULL, NULL]]
+
+# list of lists of lists of lists
+statement ok
+CREATE TABLE list_of_lists_of_lists_of_lists AS
+   SELECT [LIST(i)] i FROM list_of_lists
+   UNION ALL
+   SELECT NULL
+   UNION ALL
+   SELECT [NULL]
+   UNION ALL
+   SELECT [[], NULL, [], []]
+   UNION ALL
+   SELECT [[[NULL, NULL, [NULL]], NULL, [[], [7, 8, 9], [NULL], NULL, []]], [], [NULL]]
+
+statement ok
+COPY list_of_lists_of_lists_of_lists TO '__TEST_DIR__/complex_list.parquet' (FORMAT 'parquet');
+
+query I
+SELECT i FROM parquet_scan('__TEST_DIR__/complex_list.parquet');
+----
+[[[[1, 2, 3], [4, 5], [], [6, 7]], [[8, NULL, 10], NULL, []], [], NULL, [[11, 12, 13, 14], [], NULL, [], [], [15], [NULL, NULL, NULL]]]]
+NULL
+[NULL]
+[[], NULL, [], []]
+[[[NULL, NULL, [NULL]], NULL, [[], [7, 8, 9], [NULL], NULL, []]], [], [NULL]]
diff --git a/test/sql/copy/parquet/writer/write_list.test b/test/sql/copy/parquet/writer/write_list.test
new file mode 100644
index 000000000000..b102a4dd2c74
--- /dev/null
+++ b/test/sql/copy/parquet/writer/write_list.test
@@ -0,0 +1,92 @@
+# name: test/sql/copy/parquet/writer/write_list.test
+# description: Parquet write list
+# group: [writer]
+
+require parquet
+
+# standard list
+statement ok
+CREATE TABLE list AS SELECT * FROM (VALUES
+	([1, 2, 3]),
+	([4, 5]),
+	([6, 7]),
+    ([8, 9, 10, 11])
+) tbl(i);
+
+statement ok
+COPY list TO '__TEST_DIR__/test_list.parquet' (FORMAT 'parquet');
+
+query I
+SELECT i FROM parquet_scan('__TEST_DIR__/test_list.parquet');
+----
+[1, 2, 3]
+[4, 5]
+[6, 7]
+[8, 9, 10, 11]
+
+# empty and NULL lists
+statement ok
+CREATE TABLE null_empty_list AS SELECT * FROM (VALUES
+	([1, 2, 3]),
+	([4, 5]),
+	([6, 7]),
+	([NULL]),
+	([]),
+	([]),
+	([]),
+	([]),
+    ([8, NULL, 10, 11]),
+    (NULL)
+) tbl(i);
+
+statement ok
+COPY null_empty_list TO '__TEST_DIR__/test_list.parquet' (FORMAT 'parquet');
+
+query I
+SELECT * FROM parquet_scan('__TEST_DIR__/test_list.parquet');
+----
+[1, 2, 3]
+[4, 5]
+[6, 7]
+[NULL]
+[]
+[]
+[]
+[]
+[8, NULL, 10, 11]
+NULL
+
+# empty list
+statement ok
+COPY (SELECT []::INT[]) TO '__TEST_DIR__/test_empty_list.parquet' (FORMAT 'parquet');
+
+query I
+SELECT * FROM '__TEST_DIR__/test_empty_list.parquet'
+----
+[]
+
+# null list
+statement ok
+COPY (SELECT NULL::INT[]) TO '__TEST_DIR__/test_null_list.parquet' (FORMAT 'parquet');
+
+query I
+SELECT * FROM '__TEST_DIR__/test_null_list.parquet'
+----
+NULL
+
+# big list (> vector size)
+statement ok
+CREATE TABLE big_list AS SELECT LIST(CASE WHEN i%2=0 THEN NULL ELSE i END) l FROM range(20000) tbl(i);
+
+query I
+SELECT SUM(i) FROM (SELECT UNNEST(l) FROM big_list) t(i)
+----
+100000000
+
+statement ok
+COPY big_list TO '__TEST_DIR__/big_list.parquet' (FORMAT 'parquet');
+
+query I
+SELECT SUM(i) FROM (SELECT UNNEST(l) FROM '__TEST_DIR__/big_list.parquet') t(i)
+----
+100000000
diff --git a/test/sql/copy/parquet/writer/write_struct.test b/test/sql/copy/parquet/writer/write_struct.test
new file mode 100644
index 000000000000..29fb3923e167
--- /dev/null
+++ b/test/sql/copy/parquet/writer/write_struct.test
@@ -0,0 +1,139 @@
+# name: test/sql/copy/parquet/writer/write_struct.test
+# description: Parquet write struct
+# group: [writer]
+
+require parquet
+
+# standard struct
+statement ok
+CREATE TABLE struct AS SELECT * FROM (VALUES
+	({'a': 42, 'b': 84}),
+	({'a': 33, 'b': 32}),
+	({'a': 42, 'b': 27})
+) tbl(i);
+
+statement ok
+COPY struct TO '__TEST_DIR__/test_struct.parquet' (FORMAT 'parquet');
+
+query I
+SELECT * FROM parquet_scan('__TEST_DIR__/test_struct.parquet');
+----
+{'a': 42, 'b': 84}
+{'a': 33, 'b': 32}
+{'a': 42, 'b': 27}
+
+# struct with nulls
+statement ok
+CREATE TABLE struct_nulls AS SELECT * FROM (VALUES
+	({'a': 42, 'b': 84}),
+	({'a': NULL, 'b': 32}),
+	(NULL),
+	({'a': 42, 'b': NULL})
+) tbl(i);
+
+statement ok
+COPY struct_nulls TO '__TEST_DIR__/test_struct_nulls.parquet' (FORMAT 'parquet');
+
+query I
+SELECT * FROM parquet_scan('__TEST_DIR__/test_struct_nulls.parquet');
+----
+{'a': 42, 'b': 84}
+{'a': NULL, 'b': 32}
+NULL
+{'a': 42, 'b': NULL}
+
+# nested structs
+statement ok
+CREATE TABLE struct_nested AS SELECT * FROM (VALUES
+	({'a': {'x': 3, 'x1': 22}, 'b': {'y': 27, 'y1': 44}}),
+	({'a': {'x': 9, 'x1': 26}, 'b': {'y': 1, 'y1': 999}}),
+	({'a': {'x': 17, 'x1': 23}, 'b': {'y': 3, 'y1': 9999}})
+) tbl(i);
+
+statement ok
+COPY struct_nested TO '__TEST_DIR__/struct_nested.parquet' (FORMAT 'parquet');
+
+query I
+SELECT * FROM parquet_scan('__TEST_DIR__/struct_nested.parquet');
+----
+{'a': {'x': 3, 'x1': 22}, 'b': {'y': 27, 'y1': 44}}
+{'a': {'x': 9, 'x1': 26}, 'b': {'y': 1, 'y1': 999}}
+{'a': {'x': 17, 'x1': 23}, 'b': {'y': 3, 'y1': 9999}}
+
+# nested structs
+statement ok
+CREATE TABLE struct_nested_null AS SELECT * FROM (VALUES
+	({'a': {'x': 3, 'x1': 22}, 'b': {'y': NULL, 'y1': 44}}),
+	({'a': {'x': NULL, 'x1': 26}, 'b': {'y': 1, 'y1': NULL}}),
+	({'a': {'x': 17, 'x1': NULL}, 'b': {'y': 3, 'y1': 9999}}),
+	(NULL),
+	({'a': NULL, 'b': NULL})
+) tbl(i);
+
+statement ok
+COPY struct_nested_null TO '__TEST_DIR__/struct_nested_null.parquet' (FORMAT 'parquet');
+
+query I
+SELECT * FROM parquet_scan('__TEST_DIR__/struct_nested_null.parquet');
+----
+{'a': {'x': 3, 'x1': 22}, 'b': {'y': NULL, 'y1': 44}}
+{'a': {'x': NULL, 'x1': 26}, 'b': {'y': 1, 'y1': NULL}}
+{'a': {'x': 17, 'x1': NULL}, 'b': {'y': 3, 'y1': 9999}}
+NULL
+{'a': NULL, 'b': NULL}
+
+# single struct
+statement ok
+CREATE TABLE single_struct AS SELECT * FROM (VALUES
+	({'a': 42}),
+	({'a': 33}),
+	({'a': 42})
+) tbl(i);
+
+statement ok
+COPY single_struct TO '__TEST_DIR__/single_struct.parquet' (FORMAT 'parquet');
+
+query I
+SELECT * FROM parquet_scan('__TEST_DIR__/single_struct.parquet');
+----
+{'a': 42}
+{'a': 33}
+{'a': 42}
+
+# single struct nulls
+statement ok
+CREATE TABLE single_struct_null AS SELECT * FROM (VALUES
+	({'a': 42}),
+	({'a': NULL}),
+	(NULL)
+) tbl(i);
+
+statement ok
+COPY single_struct_null TO '__TEST_DIR__/single_struct_null.parquet' (FORMAT 'parquet');
+
+query I
+SELECT * FROM parquet_scan('__TEST_DIR__/single_struct_null.parquet');
+----
+{'a': 42}
+{'a': NULL}
+NULL
+
+# nested single struct
+statement ok
+CREATE TABLE nested_single_struct AS SELECT * FROM (VALUES
+	({'a': {'b': 42}}),
+	({'a': {'b': NULL}}),
+	({'a': NULL}),
+	(NULL)
+) tbl(i);
+
+statement ok
+COPY nested_single_struct TO '__TEST_DIR__/nested_single_struct.parquet' (FORMAT 'parquet');
+
+query I
+SELECT * FROM parquet_scan('__TEST_DIR__/nested_single_struct.parquet');
+----
+{'a': {'b': 42}}
+{'a': {'b': NULL}}
+{'a': NULL}
+NULL
diff --git a/test/sql/copy/parquet/writer/writer_round_trip.test_slow b/test/sql/copy/parquet/writer/writer_round_trip.test_slow
new file mode 100644
index 000000000000..40afd4e86097
--- /dev/null
+++ b/test/sql/copy/parquet/writer/writer_round_trip.test_slow
@@ -0,0 +1,35 @@
+# name: test/sql/copy/parquet/writer/writer_round_trip.test_slow
+# description: Parquet read and re-write various files
+# group: [writer]
+
+require parquet
+
+foreach parquet_file data/parquet-testing/manyrowgroups.parquet data/parquet-testing/map.parquet data/parquet-testing/arrow/int32_decimal.parquet data/parquet-testing/arrow/nonnullable.impala.parquet data/parquet-testing/bug687_nulls.parquet data/parquet-testing/bug1554.parquet data/parquet-testing/apkwan.parquet data/parquet-testing/arrow/nested_lists.snappy.parquet data/parquet-testing/arrow/nulls.snappy.parquet data/parquet-testing/nan-float.parquet data/parquet-testing/manyrowgroups2.parquet data/parquet-testing/struct.parquet data/parquet-testing/arrow/list_columns.parquet data/parquet-testing/timestamp-ms.parquet data/parquet-testing/arrow/alltypes_dictionary.parquet data/parquet-testing/arrow/binary.parquet data/parquet-testing/arrow/nation.dict-malformed.parquet data/parquet-testing/lineitem-top10000.gzip.parquet data/parquet-testing/arrow/nested_maps.snappy.parquet data/parquet-testing/arrow/dict-page-offset-zero.parquet data/parquet-testing/silly-names.parquet data/parquet-testing/zstd.parquet data/parquet-testing/bug1618_struct_strings.parquet data/parquet-testing/arrow/single_nan.parquet data/parquet-testing/arrow/int64_decimal.parquet data/parquet-testing/filter_bug1391.parquet data/parquet-testing/arrow/fixed_length_decimal_legacy.parquet data/parquet-testing/timestamp.parquet data/parquet-testing/arrow/fixed_length_decimal.parquet data/parquet-testing/leftdate3_192_loop_1.parquet data/parquet-testing/blob.parquet data/parquet-testing/bug1588.parquet data/parquet-testing/bug1589.parquet data/parquet-testing/arrow/alltypes_plain.parquet data/parquet-testing/arrow/repeated_no_annotation.parquet data/parquet-testing/data-types.parquet data/parquet-testing/unsigned.parquet data/parquet-testing/pandas-date.parquet data/parquet-testing/date.parquet data/parquet-testing/arrow/nullable.impala.parquet data/parquet-testing/fixed.parquet data/parquet-testing/arrow/alltypes_plain.snappy.parquet data/parquet-testing/decimal/int32_decimal.parquet data/parquet-testing/decimal/pandas_decimal.parquet data/parquet-testing/decimal/decimal_dc.parquet data/parquet-testing/decimal/int64_decimal.parquet data/parquet-testing/decimal/fixed_length_decimal_legacy.parquet data/parquet-testing/decimal/fixed_length_decimal.parquet data/parquet-testing/glob2/t1.parquet data/parquet-testing/cache/cache1.parquet data/parquet-testing/cache/cache2.parquet data/parquet-testing/glob/t2.parquet data/parquet-testing/glob/t1.parquet data/parquet-testing/bug2557.parquet
+
+statement ok
+CREATE TABLE parquet_read AS SELECT * FROM parquet_scan('${parquet_file}');
+
+statement ok
+COPY parquet_read TO '__TEST_DIR__/test_round_trip.parquet'
+
+statement ok
+CREATE TABLE parquet_write AS SELECT * FROM parquet_scan('__TEST_DIR__/test_round_trip.parquet');
+
+# verify that the count is the same
+query I
+SELECT COUNT(*) FROM parquet_read EXCEPT SELECT COUNT(*) FROM parquet_write
+----
+
+# verify that the data is the same
+query I
+SELECT COUNT(*) FROM (SELECT * FROM parquet_read EXCEPT SELECT * FROM parquet_write)
+----
+0
+
+statement ok
+DROP TABLE parquet_read
+
+statement ok
+DROP TABLE parquet_write
+
+endloop
\ No newline at end of file
diff --git a/test/sqlite/result_helper.cpp b/test/sqlite/result_helper.cpp
index 4c90ab26cb8f..c24e8fa9d271 100644
--- a/test/sqlite/result_helper.cpp
+++ b/test/sqlite/result_helper.cpp
@@ -600,36 +600,29 @@ bool TestResultHelper::CompareValues(string lvalue_str, string rvalue_str, idx_t
 	auto sql_type = result.types[current_column];
 	if (sql_type.IsNumeric()) {
 		bool converted_lvalue = false;
-		try {
-			if (lvalue_str == "NULL") {
-				lvalue = Value(sql_type);
-			} else {
-				lvalue = Value(lvalue_str);
-				if (!lvalue.TryCastAs(sql_type)) {
-					return false;
-				}
-			}
+		bool converted_rvalue = false;
+		if (lvalue_str == "NULL") {
+			lvalue = Value(sql_type);
 			converted_lvalue = true;
-			if (rvalue_str == "NULL") {
-				rvalue = Value(sql_type);
-			} else {
-				rvalue = Value(rvalue_str);
-				if (!rvalue.TryCastAs(sql_type)) {
-					return false;
-				}
+		} else {
+			lvalue = Value(lvalue_str);
+			if (lvalue.TryCastAs(sql_type)) {
+				converted_lvalue = true;
+			}
+		}
+		if (rvalue_str == "NULL") {
+			rvalue = Value(sql_type);
+			converted_rvalue = true;
+		} else {
+			rvalue = Value(rvalue_str);
+			if (rvalue.TryCastAs(sql_type)) {
+				converted_rvalue = true;
 			}
+		}
+		if (converted_lvalue && converted_rvalue) {
 			error = !Value::ValuesAreEqual(lvalue, rvalue);
-		} catch (std::exception &ex) {
-			PrintErrorHeader("Test error!");
-			PrintLineSep();
-			PrintSQL(sql_query);
-			PrintLineSep();
-			std::cerr << termcolor::red << termcolor::bold << "Cannot convert value "
-			          << (converted_lvalue ? rvalue_str : lvalue_str) << " to type " << sql_type.ToString()
-			          << termcolor::reset << std::endl;
-			std::cerr << termcolor::red << termcolor::bold << ex.what() << termcolor::reset << std::endl;
-			PrintLineSep();
-			return false;
+		} else {
+			error = true;
 		}
 	} else if (sql_type == LogicalType::BOOLEAN) {
 		auto low_r_val = StringUtil::Lower(rvalue_str);
diff --git a/test/sqlite/sqllogic_test_runner.cpp b/test/sqlite/sqllogic_test_runner.cpp
index 68bb7eead6c2..22598f988356 100644
--- a/test/sqlite/sqllogic_test_runner.cpp
+++ b/test/sqlite/sqllogic_test_runner.cpp
@@ -386,6 +386,10 @@ void SQLLogicTestRunner::ExecuteFile(string script) {
 #if LDBL_MANT_DIG < 54
 				return;
 #endif
+			} else if (param == "64bit") {
+				if (sizeof(void *) != 8) {
+					return;
+				}
 			} else if (param == "noforcestorage") {
 				if (TestForceStorage()) {
 					return;
