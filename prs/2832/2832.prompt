You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Parquet list columns read incorrectly
#### What happens?
When scanning parquet files with list columns, results with incorrect offsets can be returned.

#### To Reproduce

```python
import numpy as np
import pyarrow as pa
import duckdb
import pyarrow.parquet as pq
R = 100_000
fake_data = pa.array([np.arange(i, i + 100).astype('float64') for i in range(R)])
tb = pa.table({'word0': pa.array([str(i) for i in range(0, R)]), 'year_counts': fake_data})
pq.write_table(tb, "test.parquet")

con = duckdb.connect(":memory:")

con.query("SELECT year_counts FROM parquet_scan('test.parquet') WHERE word0='90000'")

```

This should return a list-column of length 100 starting at 90,000; instead it starts with 912.

```python
pq.read_table("test.parquet", columns = ['year_counts'], filters = [("word0", "=", "90000")])['year_counts']
```

returns the correct result.

#### Environment (please complete the following information):
 - OS: OS X
 - DuckDB Version: 0.3.1-dev550
 - DuckDB Client: Python / WASM

#### Before Submitting

- [x] **Have you tried this on the latest `master` branch?**
* **Python**: `pip install duckdb --upgrade --pre`
* **R**: `install.packages("https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz", repos = NULL)`
* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.

- [X] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**

COPY TO  parquet with ZSTD produces broken column with 100k rows (~3GB uncompressed, 45MB compressed)
#### What happens?
COPY TO  parquet with ZSTD produces broken column with 100k rows (~3GB uncompressed, 30MB compressed)

```python
duckdb.query("""
    SELECT sequence FROM 'duckseq.parquet'
    USING SAMPLE 10;
    """).fetchall()

RuntimeError                              Traceback (most recent call last)
~/code/duckdb-test/test.py in <module>
----> 47 duckdb.query("""
      48     SELECT sequence FROM 'duckseq.parquet'
      49     USING SAMPLE 10;
      50     """).fetchall()

RuntimeError: ZSTD Decompression failure
```

#### To Reproduce
```python
import duckdb
con = duckdb.connect("::memory::")
con.execute("""
    COPY (SELECT * FROM read_csv_auto('sequences.csv.gz', delim=',', header=True) )
    TO 'duckseq.parquet' 
    (FORMAT 'PARQUET', CODEC 'ZSTD');
    """)
```
File: https://transfer.sh/0ugKVN/sequences.csv.gz

#### Environment (please complete the following information):
 - OS: macOS 12.1 Intel
 - DuckDB Version: 0.3.1
 - DuckDB Client: Python

#### Possible root cause

I think I know what happens. When I observe htop, I see that the memory requirements balloon, going to 30% and more. Then suddenly memory goes to 0 while python command still runs. Only a few seconds later does Python complete, _without throwing an error_.

It looks like the error handling is broken. The process gets killed as OOM but Python never tells the end user about it nor cleans up the resulting file.



</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/docs/why_duckdb.html).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
44: 
45: 
[end of README.md]
[start of extension/parquet/CMakeLists.txt]
1: cmake_minimum_required(VERSION 2.8.12)
2: 
3: project(ParquetExtension)
4: 
5: include_directories(
6:   include ../../third_party/parquet ../../third_party/snappy
7:   ../../third_party/miniz ../../third_party/thrift
8:   ../../third_party/zstd/include)
9: 
10: set(PARQUET_EXTENSION_FILES
11:     parquet-extension.cpp
12:     parquet_metadata.cpp
13:     parquet_reader.cpp
14:     parquet_timestamp.cpp
15:     parquet_writer.cpp
16:     parquet_statistics.cpp
17:     zstd_file_system.cpp
18:     column_reader.cpp)
19: 
20: if(NOT CLANG_TIDY)
21:   set(PARQUET_EXTENSION_FILES
22:       ${PARQUET_EXTENSION_FILES}
23:       ../../third_party/parquet/parquet_constants.cpp
24:       ../../third_party/parquet/parquet_types.cpp
25:       ../../third_party/thrift/thrift/protocol/TProtocol.cpp
26:       ../../third_party/thrift/thrift/transport/TTransportException.cpp
27:       ../../third_party/thrift/thrift/transport/TBufferTransports.cpp
28:       ../../third_party/snappy/snappy.cc
29:       ../../third_party/snappy/snappy-sinksource.cc
30:       ../../third_party/zstd/decompress/zstd_ddict.cpp
31:       ../../third_party/zstd/decompress/huf_decompress.cpp
32:       ../../third_party/zstd/decompress/zstd_decompress.cpp
33:       ../../third_party/zstd/decompress/zstd_decompress_block.cpp
34:       ../../third_party/zstd/common/entropy_common.cpp
35:       ../../third_party/zstd/common/fse_decompress.cpp
36:       ../../third_party/zstd/common/zstd_common.cpp
37:       ../../third_party/zstd/common/error_private.cpp
38:       ../../third_party/zstd/common/xxhash.cpp
39:       ../../third_party/zstd/compress/fse_compress.cpp
40:       ../../third_party/zstd/compress/hist.cpp
41:       ../../third_party/zstd/compress/huf_compress.cpp
42:       ../../third_party/zstd/compress/zstd_compress.cpp
43:       ../../third_party/zstd/compress/zstd_compress_literals.cpp
44:       ../../third_party/zstd/compress/zstd_compress_sequences.cpp
45:       ../../third_party/zstd/compress/zstd_compress_superblock.cpp
46:       ../../third_party/zstd/compress/zstd_double_fast.cpp
47:       ../../third_party/zstd/compress/zstd_fast.cpp
48:       ../../third_party/zstd/compress/zstd_lazy.cpp
49:       ../../third_party/zstd/compress/zstd_ldm.cpp
50:       ../../third_party/zstd/compress/zstd_opt.cpp)
51: endif()
52: 
53: add_library(parquet_extension STATIC ${PARQUET_EXTENSION_FILES})
54: 
55: install(
56:   TARGETS parquet_extension
57:   EXPORT "${DUCKDB_EXPORT_SET}"
58:   LIBRARY DESTINATION "${INSTALL_LIB_DIR}"
59:   ARCHIVE DESTINATION "${INSTALL_LIB_DIR}")
60: 
61: if(NOT CLANG_TIDY)
62:   add_executable(parquetcli parquetcli.cpp)
63:   target_link_libraries(parquetcli parquet_extension)
64:   target_link_libraries(parquetcli duckdb_static)
65: endif()
[end of extension/parquet/CMakeLists.txt]
[start of extension/parquet/column_reader.cpp]
1: #include "column_reader.hpp"
2: #include "parquet_timestamp.hpp"
3: #include "utf8proc_wrapper.hpp"
4: #include "parquet_reader.hpp"
5: 
6: #include "boolean_column_reader.hpp"
7: #include "callback_column_reader.hpp"
8: #include "decimal_column_reader.hpp"
9: #include "list_column_reader.hpp"
10: #include "string_column_reader.hpp"
11: #include "struct_column_reader.hpp"
12: #include "templated_column_reader.hpp"
13: 
14: #include "snappy.h"
15: #include "miniz_wrapper.hpp"
16: #include "zstd.h"
17: #include <iostream>
18: 
19: #include "duckdb.hpp"
20: #ifndef DUCKDB_AMALGAMATION
21: #include "duckdb/common/types/blob.hpp"
22: #include "duckdb/common/types/chunk_collection.hpp"
23: #endif
24: 
25: namespace duckdb {
26: 
27: using duckdb_parquet::format::CompressionCodec;
28: using duckdb_parquet::format::ConvertedType;
29: using duckdb_parquet::format::Encoding;
30: using duckdb_parquet::format::PageType;
31: using duckdb_parquet::format::Type;
32: 
33: const uint32_t RleBpDecoder::BITPACK_MASKS[] = {
34:     0,       1,       3,        7,        15,       31,        63,        127,       255,        511,       1023,
35:     2047,    4095,    8191,     16383,    32767,    65535,     131071,    262143,    524287,     1048575,   2097151,
36:     4194303, 8388607, 16777215, 33554431, 67108863, 134217727, 268435455, 536870911, 1073741823, 2147483647};
37: 
38: const uint8_t RleBpDecoder::BITPACK_DLEN = 8;
39: 
40: ColumnReader::ColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
41:                            idx_t max_define_p, idx_t max_repeat_p)
42:     : schema(schema_p), file_idx(file_idx_p), max_define(max_define_p), max_repeat(max_repeat_p), reader(reader),
43:       type(move(type_p)), page_rows_available(0) {
44: 
45: 	// dummies for Skip()
46: 	none_filter.none();
47: 	dummy_define.resize(reader.allocator, STANDARD_VECTOR_SIZE);
48: 	dummy_repeat.resize(reader.allocator, STANDARD_VECTOR_SIZE);
49: }
50: 
51: ColumnReader::~ColumnReader() {
52: }
53: 
54: template <class T>
55: unique_ptr<ColumnReader> CreateDecimalReader(ParquetReader &reader, const LogicalType &type_p,
56:                                              const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,
57:                                              idx_t max_repeat) {
58: 	switch (type_p.InternalType()) {
59: 	case PhysicalType::INT16:
60: 		return make_unique<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<T>>>(
61: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
62: 	case PhysicalType::INT32:
63: 		return make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<T>>>(
64: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
65: 	case PhysicalType::INT64:
66: 		return make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<T>>>(
67: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
68: 	default:
69: 		throw NotImplementedException("Unimplemented internal type for CreateDecimalReader");
70: 	}
71: }
72: 
73: unique_ptr<ColumnReader> ColumnReader::CreateReader(ParquetReader &reader, const LogicalType &type_p,
74:                                                     const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,
75:                                                     idx_t max_repeat) {
76: 	switch (type_p.id()) {
77: 	case LogicalTypeId::BOOLEAN:
78: 		return make_unique<BooleanColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
79: 	case LogicalTypeId::UTINYINT:
80: 		return make_unique<TemplatedColumnReader<uint8_t, TemplatedParquetValueConversion<uint32_t>>>(
81: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
82: 	case LogicalTypeId::USMALLINT:
83: 		return make_unique<TemplatedColumnReader<uint16_t, TemplatedParquetValueConversion<uint32_t>>>(
84: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
85: 	case LogicalTypeId::UINTEGER:
86: 		return make_unique<TemplatedColumnReader<uint32_t, TemplatedParquetValueConversion<uint32_t>>>(
87: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
88: 	case LogicalTypeId::UBIGINT:
89: 		return make_unique<TemplatedColumnReader<uint64_t, TemplatedParquetValueConversion<uint64_t>>>(
90: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
91: 	case LogicalTypeId::TINYINT:
92: 		return make_unique<TemplatedColumnReader<int8_t, TemplatedParquetValueConversion<int32_t>>>(
93: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
94: 	case LogicalTypeId::SMALLINT:
95: 		return make_unique<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<int32_t>>>(
96: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
97: 	case LogicalTypeId::INTEGER:
98: 		return make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<int32_t>>>(
99: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
100: 	case LogicalTypeId::BIGINT:
101: 		return make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<int64_t>>>(
102: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
103: 	case LogicalTypeId::FLOAT:
104: 		return make_unique<TemplatedColumnReader<float, TemplatedParquetValueConversion<float>>>(
105: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
106: 	case LogicalTypeId::DOUBLE:
107: 		return make_unique<TemplatedColumnReader<double, TemplatedParquetValueConversion<double>>>(
108: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
109: 	case LogicalTypeId::TIMESTAMP:
110: 		switch (schema_p.type) {
111: 		case Type::INT96:
112: 			return make_unique<CallbackColumnReader<Int96, timestamp_t, ImpalaTimestampToTimestamp>>(
113: 			    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
114: 		case Type::INT64:
115: 			switch (schema_p.converted_type) {
116: 			case ConvertedType::TIMESTAMP_MICROS:
117: 				return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMicrosToTimestamp>>(
118: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
119: 			case ConvertedType::TIMESTAMP_MILLIS:
120: 				return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMsToTimestamp>>(
121: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
122: 			default:
123: 				break;
124: 			}
125: 		default:
126: 			break;
127: 		}
128: 		break;
129: 	case LogicalTypeId::DATE:
130: 		return make_unique<CallbackColumnReader<int32_t, date_t, ParquetIntToDate>>(reader, type_p, schema_p,
131: 		                                                                            file_idx_p, max_define, max_repeat);
132: 	case LogicalTypeId::BLOB:
133: 	case LogicalTypeId::VARCHAR:
134: 		return make_unique<StringColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
135: 	case LogicalTypeId::DECIMAL:
136: 		// we have to figure out what kind of int we need
137: 		switch (schema_p.type) {
138: 		case Type::INT32:
139: 			return CreateDecimalReader<int32_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
140: 		case Type::INT64:
141: 			return CreateDecimalReader<int64_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
142: 		case Type::FIXED_LEN_BYTE_ARRAY:
143: 			switch (type_p.InternalType()) {
144: 			case PhysicalType::INT16:
145: 				return make_unique<DecimalColumnReader<int16_t>>(reader, type_p, schema_p, file_idx_p, max_define,
146: 				                                                 max_repeat);
147: 			case PhysicalType::INT32:
148: 				return make_unique<DecimalColumnReader<int32_t>>(reader, type_p, schema_p, file_idx_p, max_define,
149: 				                                                 max_repeat);
150: 			case PhysicalType::INT64:
151: 				return make_unique<DecimalColumnReader<int64_t>>(reader, type_p, schema_p, file_idx_p, max_define,
152: 				                                                 max_repeat);
153: 			case PhysicalType::INT128:
154: 				return make_unique<DecimalColumnReader<hugeint_t>>(reader, type_p, schema_p, file_idx_p, max_define,
155: 				                                                   max_repeat);
156: 			default:
157: 				throw InternalException("Unrecognized type for Decimal");
158: 			}
159: 		default:
160: 			throw NotImplementedException("Unrecognized type for Decimal");
161: 		}
162: 		break;
163: 	default:
164: 		break;
165: 	}
166: 	throw NotImplementedException(type_p.ToString());
167: }
168: 
169: void ColumnReader::PrepareRead(parquet_filter_t &filter) {
170: 	dict_decoder.reset();
171: 	defined_decoder.reset();
172: 	block.reset();
173: 
174: 	PageHeader page_hdr;
175: 	page_hdr.read(protocol);
176: 
177: 	//	page_hdr.printTo(std::cout);
178: 	//	std::cout << '\n';
179: 
180: 	PreparePage(page_hdr.compressed_page_size, page_hdr.uncompressed_page_size);
181: 
182: 	switch (page_hdr.type) {
183: 	case PageType::DATA_PAGE_V2:
184: 	case PageType::DATA_PAGE:
185: 		PrepareDataPage(page_hdr);
186: 		break;
187: 	case PageType::DICTIONARY_PAGE:
188: 		Dictionary(move(block), page_hdr.dictionary_page_header.num_values);
189: 		break;
190: 	default:
191: 		break; // ignore INDEX page type and any other custom extensions
192: 	}
193: }
194: 
195: void ColumnReader::PreparePage(idx_t compressed_page_size, idx_t uncompressed_page_size) {
196: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
197: 
198: 	block = make_shared<ResizeableBuffer>(reader.allocator, compressed_page_size + 1);
199: 	trans.read((uint8_t *)block->ptr, compressed_page_size);
200: 
201: 	shared_ptr<ResizeableBuffer> unpacked_block;
202: 	if (chunk->meta_data.codec != CompressionCodec::UNCOMPRESSED) {
203: 		unpacked_block = make_shared<ResizeableBuffer>(reader.allocator, uncompressed_page_size + 1);
204: 	}
205: 
206: 	switch (chunk->meta_data.codec) {
207: 	case CompressionCodec::UNCOMPRESSED:
208: 		break;
209: 	case CompressionCodec::GZIP: {
210: 		MiniZStream s;
211: 
212: 		s.Decompress((const char *)block->ptr, compressed_page_size, (char *)unpacked_block->ptr,
213: 		             uncompressed_page_size);
214: 		block = move(unpacked_block);
215: 
216: 		break;
217: 	}
218: 	case CompressionCodec::SNAPPY: {
219: 		auto res =
220: 		    duckdb_snappy::RawUncompress((const char *)block->ptr, compressed_page_size, (char *)unpacked_block->ptr);
221: 		if (!res) {
222: 			throw std::runtime_error("Decompression failure");
223: 		}
224: 		block = move(unpacked_block);
225: 		break;
226: 	}
227: 	case CompressionCodec::ZSTD: {
228: 		auto res = duckdb_zstd::ZSTD_decompress((char *)unpacked_block->ptr, uncompressed_page_size,
229: 		                                        (const char *)block->ptr, compressed_page_size);
230: 		if (duckdb_zstd::ZSTD_isError(res) || res != (size_t)uncompressed_page_size) {
231: 			throw std::runtime_error("ZSTD Decompression failure");
232: 		}
233: 		block = move(unpacked_block);
234: 		break;
235: 	}
236: 
237: 	default: {
238: 		std::stringstream codec_name;
239: 		codec_name << chunk->meta_data.codec;
240: 		throw std::runtime_error("Unsupported compression codec \"" + codec_name.str() +
241: 		                         "\". Supported options are uncompressed, gzip or snappy");
242: 		break;
243: 	}
244: 	}
245: }
246: 
247: static uint8_t ComputeBitWidth(idx_t val) {
248: 	if (val == 0) {
249: 		return 0;
250: 	}
251: 	uint8_t ret = 1;
252: 	while (((idx_t)(1 << ret) - 1) < val) {
253: 		ret++;
254: 	}
255: 	return ret;
256: }
257: 
258: void ColumnReader::PrepareDataPage(PageHeader &page_hdr) {
259: 	if (page_hdr.type == PageType::DATA_PAGE && !page_hdr.__isset.data_page_header) {
260: 		throw std::runtime_error("Missing data page header from data page");
261: 	}
262: 	if (page_hdr.type == PageType::DATA_PAGE_V2 && !page_hdr.__isset.data_page_header_v2) {
263: 		throw std::runtime_error("Missing data page header from data page v2");
264: 	}
265: 
266: 	page_rows_available = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.num_values
267: 	                                                           : page_hdr.data_page_header_v2.num_values;
268: 	auto page_encoding = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.encoding
269: 	                                                          : page_hdr.data_page_header_v2.encoding;
270: 
271: 	if (HasRepeats()) {
272: 		uint32_t rep_length = page_hdr.type == PageType::DATA_PAGE
273: 		                          ? block->read<uint32_t>()
274: 		                          : page_hdr.data_page_header_v2.repetition_levels_byte_length;
275: 		block->available(rep_length);
276: 		repeated_decoder =
277: 		    make_unique<RleBpDecoder>((const uint8_t *)block->ptr, rep_length, ComputeBitWidth(max_repeat));
278: 		block->inc(rep_length);
279: 	}
280: 
281: 	if (HasDefines()) {
282: 		uint32_t def_length = page_hdr.type == PageType::DATA_PAGE
283: 		                          ? block->read<uint32_t>()
284: 		                          : page_hdr.data_page_header_v2.definition_levels_byte_length;
285: 		block->available(def_length);
286: 		defined_decoder =
287: 		    make_unique<RleBpDecoder>((const uint8_t *)block->ptr, def_length, ComputeBitWidth(max_define));
288: 		block->inc(def_length);
289: 	}
290: 
291: 	switch (page_encoding) {
292: 	case Encoding::RLE_DICTIONARY:
293: 	case Encoding::PLAIN_DICTIONARY: {
294: 		// TODO there seems to be some confusion whether this is in the bytes for v2
295: 		// where is it otherwise??
296: 		auto dict_width = block->read<uint8_t>();
297: 		// TODO somehow dict_width can be 0 ?
298: 		dict_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, block->len, dict_width);
299: 		block->inc(block->len);
300: 		break;
301: 	}
302: 	case Encoding::PLAIN:
303: 		// nothing to do here, will be read directly below
304: 		break;
305: 
306: 	default:
307: 		throw std::runtime_error("Unsupported page encoding");
308: 	}
309: }
310: 
311: idx_t ColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
312:                          Vector &result) {
313: 	// we need to reset the location because multiple column readers share the same protocol
314: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
315: 	trans.SetLocation(chunk_read_offset);
316: 
317: 	idx_t result_offset = 0;
318: 	auto to_read = num_values;
319: 
320: 	while (to_read > 0) {
321: 		while (page_rows_available == 0) {
322: 			PrepareRead(filter);
323: 		}
324: 
325: 		D_ASSERT(block);
326: 		auto read_now = MinValue<idx_t>(to_read, page_rows_available);
327: 
328: 		D_ASSERT(read_now <= STANDARD_VECTOR_SIZE);
329: 
330: 		if (HasRepeats()) {
331: 			D_ASSERT(repeated_decoder);
332: 			repeated_decoder->GetBatch<uint8_t>((char *)repeat_out + result_offset, read_now);
333: 		}
334: 
335: 		if (HasDefines()) {
336: 			D_ASSERT(defined_decoder);
337: 			defined_decoder->GetBatch<uint8_t>((char *)define_out + result_offset, read_now);
338: 		}
339: 
340: 		if (dict_decoder) {
341: 			// we need the null count because the offsets and plain values have no entries for nulls
342: 			idx_t null_count = 0;
343: 			if (HasDefines()) {
344: 				for (idx_t i = 0; i < read_now; i++) {
345: 					if (define_out[i + result_offset] != max_define) {
346: 						null_count++;
347: 					}
348: 				}
349: 			}
350: 
351: 			offset_buffer.resize(reader.allocator, sizeof(uint32_t) * (read_now - null_count));
352: 			dict_decoder->GetBatch<uint32_t>(offset_buffer.ptr, read_now - null_count);
353: 			DictReference(result);
354: 			Offsets((uint32_t *)offset_buffer.ptr, define_out, read_now, filter, result_offset, result);
355: 		} else {
356: 			PlainReference(block, result);
357: 			Plain(block, define_out, read_now, filter, result_offset, result);
358: 		}
359: 
360: 		result_offset += read_now;
361: 		page_rows_available -= read_now;
362: 		to_read -= read_now;
363: 	}
364: 	group_rows_available -= num_values;
365: 	chunk_read_offset = trans.GetLocation();
366: 
367: 	return num_values;
368: }
369: 
370: void ColumnReader::Skip(idx_t num_values) {
371: 	dummy_define.zero();
372: 	dummy_repeat.zero();
373: 
374: 	// TODO this can be optimized, for example we dont actually have to bitunpack offsets
375: 	Vector dummy_result(type, nullptr);
376: 	auto values_read =
377: 	    Read(num_values, none_filter, (uint8_t *)dummy_define.ptr, (uint8_t *)dummy_repeat.ptr, dummy_result);
378: 	if (values_read != num_values) {
379: 		throw std::runtime_error("Row count mismatch when skipping rows");
380: 	}
381: }
382: 
383: uint32_t StringColumnReader::VerifyString(const char *str_data, uint32_t str_len) {
384: 	if (Type() != LogicalTypeId::VARCHAR) {
385: 		return str_len;
386: 	}
387: 	// verify if a string is actually UTF8, and if there are no null bytes in the middle of the string
388: 	// technically Parquet should guarantee this, but reality is often disappointing
389: 	UnicodeInvalidReason reason;
390: 	size_t pos;
391: 	auto utf_type = Utf8Proc::Analyze(str_data, str_len, &reason, &pos);
392: 	if (utf_type == UnicodeType::INVALID) {
393: 		if (reason == UnicodeInvalidReason::NULL_BYTE) {
394: 			// for null bytes we just truncate the string
395: 			return pos;
396: 		}
397: 		throw InvalidInputException("Invalid string encoding found in Parquet file: value \"" +
398: 		                            Blob::ToString(string_t(str_data, str_len)) + "\" is not valid UTF8!");
399: 	}
400: 	return str_len;
401: }
402: 
403: void StringColumnReader::Dictionary(shared_ptr<ByteBuffer> data, idx_t num_entries) {
404: 	dict = move(data);
405: 	dict_strings = unique_ptr<string_t[]>(new string_t[num_entries]);
406: 	for (idx_t dict_idx = 0; dict_idx < num_entries; dict_idx++) {
407: 		uint32_t str_len = dict->read<uint32_t>();
408: 		dict->available(str_len);
409: 
410: 		auto actual_str_len = VerifyString(dict->ptr, str_len);
411: 		dict_strings[dict_idx] = string_t(dict->ptr, actual_str_len);
412: 		dict->inc(str_len);
413: 	}
414: }
415: 
416: class ParquetStringVectorBuffer : public VectorBuffer {
417: public:
418: 	explicit ParquetStringVectorBuffer(shared_ptr<ByteBuffer> buffer_p)
419: 	    : VectorBuffer(VectorBufferType::OPAQUE_BUFFER), buffer(move(buffer_p)) {
420: 	}
421: 
422: private:
423: 	shared_ptr<ByteBuffer> buffer;
424: };
425: 
426: void StringColumnReader::DictReference(Vector &result) {
427: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(dict));
428: }
429: void StringColumnReader::PlainReference(shared_ptr<ByteBuffer> plain_data, Vector &result) {
430: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(move(plain_data)));
431: }
432: 
433: string_t StringParquetValueConversion::DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
434: 	auto &dict_strings = ((StringColumnReader &)reader).dict_strings;
435: 	return dict_strings[offset];
436: }
437: 
438: string_t StringParquetValueConversion::PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
439: 	auto &scr = ((StringColumnReader &)reader);
440: 	uint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;
441: 	plain_data.available(str_len);
442: 	auto actual_str_len = ((StringColumnReader &)reader).VerifyString(plain_data.ptr, str_len);
443: 	auto ret_str = string_t(plain_data.ptr, actual_str_len);
444: 	plain_data.inc(str_len);
445: 	return ret_str;
446: }
447: 
448: void StringParquetValueConversion::PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
449: 	auto &scr = ((StringColumnReader &)reader);
450: 	uint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;
451: 	plain_data.available(str_len);
452: 	plain_data.inc(str_len);
453: }
454: 
455: idx_t ListColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
456:                              Vector &result_out) {
457: 	idx_t result_offset = 0;
458: 	auto result_ptr = FlatVector::GetData<list_entry_t>(result_out);
459: 	auto &result_mask = FlatVector::Validity(result_out);
460: 
461: 	D_ASSERT(ListVector::GetListSize(result_out) == 0);
462: 	// if an individual list is longer than STANDARD_VECTOR_SIZE we actually have to loop the child read to fill it
463: 	bool finished = false;
464: 	while (!finished) {
465: 		idx_t child_actual_num_values = 0;
466: 
467: 		// check if we have any overflow from a previous read
468: 		if (overflow_child_count == 0) {
469: 			// we don't: read elements from the child reader
470: 			child_defines.zero();
471: 			child_repeats.zero();
472: 			// we don't know in advance how many values to read because of the beautiful repetition/definition setup
473: 			// we just read (up to) a vector from the child column, and see if we have read enough
474: 			// if we have not read enough, we read another vector
475: 			// if we have read enough, we leave any unhandled elements in the overflow vector for a subsequent read
476: 			auto child_req_num_values =
477: 			    MinValue<idx_t>(STANDARD_VECTOR_SIZE, child_column_reader->GroupRowsAvailable());
478: 			read_vector.ResetFromCache(read_cache);
479: 			child_actual_num_values = child_column_reader->Read(child_req_num_values, child_filter, child_defines_ptr,
480: 			                                                    child_repeats_ptr, read_vector);
481: 		} else {
482: 			// we do: use the overflow values
483: 			child_actual_num_values = overflow_child_count;
484: 			overflow_child_count = 0;
485: 		}
486: 
487: 		if (child_actual_num_values == 0) {
488: 			// no more elements available: we are done
489: 			break;
490: 		}
491: 		read_vector.Verify(child_actual_num_values);
492: 		idx_t current_chunk_offset = ListVector::GetListSize(result_out);
493: 
494: 		// hard-won piece of code this, modify at your own risk
495: 		// the intuition is that we have to only collapse values into lists that are repeated *on this level*
496: 		// the rest is pretty much handed up as-is as a single-valued list or NULL
497: 		idx_t child_idx;
498: 		for (child_idx = 0; child_idx < child_actual_num_values; child_idx++) {
499: 			if (child_repeats_ptr[child_idx] == max_repeat) {
500: 				// value repeats on this level, append
501: 				D_ASSERT(result_offset > 0);
502: 				result_ptr[result_offset - 1].length++;
503: 				continue;
504: 			}
505: 
506: 			if (result_offset >= num_values) {
507: 				// we ran out of output space
508: 				finished = true;
509: 				break;
510: 			}
511: 			if (child_defines_ptr[child_idx] >= max_define) {
512: 				// value has been defined down the stack, hence its NOT NULL
513: 				result_ptr[result_offset].offset = child_idx + current_chunk_offset;
514: 				result_ptr[result_offset].length = 1;
515: 			} else {
516: 				// value is NULL somewhere up the stack
517: 				result_mask.SetInvalid(result_offset);
518: 				result_ptr[result_offset].offset = 0;
519: 				result_ptr[result_offset].length = 0;
520: 			}
521: 
522: 			repeat_out[result_offset] = child_repeats_ptr[child_idx];
523: 			define_out[result_offset] = child_defines_ptr[child_idx];
524: 
525: 			result_offset++;
526: 		}
527: 		// actually append the required elements to the child list
528: 		ListVector::Append(result_out, read_vector, child_idx);
529: 
530: 		// we have read more values from the child reader than we can fit into the result for this read
531: 		// we have to pass everything from child_idx to child_actual_num_values into the next call
532: 		if (child_idx < child_actual_num_values && result_offset == num_values) {
533: 			read_vector.Slice(read_vector, child_idx);
534: 			overflow_child_count = child_actual_num_values - child_idx;
535: 			read_vector.Verify(overflow_child_count);
536: 
537: 			// move values in the child repeats and defines *backward* by child_idx
538: 			for (idx_t repdef_idx = 0; repdef_idx < overflow_child_count; repdef_idx++) {
539: 				child_defines_ptr[repdef_idx] = child_defines_ptr[child_idx + repdef_idx];
540: 				child_repeats_ptr[repdef_idx] = child_repeats_ptr[child_idx + repdef_idx];
541: 			}
542: 		}
543: 	}
544: 	result_out.Verify(result_offset);
545: 	return result_offset;
546: }
547: 
548: ListColumnReader::ListColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
549:                                    idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p,
550:                                    unique_ptr<ColumnReader> child_column_reader_p)
551:     : ColumnReader(reader, move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p),
552:       child_column_reader(move(child_column_reader_p)), read_cache(ListType::GetChildType(Type())),
553:       read_vector(read_cache), overflow_child_count(0) {
554: 
555: 	child_defines.resize(reader.allocator, STANDARD_VECTOR_SIZE);
556: 	child_repeats.resize(reader.allocator, STANDARD_VECTOR_SIZE);
557: 	child_defines_ptr = (uint8_t *)child_defines.ptr;
558: 	child_repeats_ptr = (uint8_t *)child_repeats.ptr;
559: 
560: 	child_filter.set();
561: }
562: 
563: } // namespace duckdb
[end of extension/parquet/column_reader.cpp]
[start of extension/parquet/include/parquet_rle_bp_decoder.hpp]
1: #pragma once
2: namespace duckdb {
3: class RleBpDecoder {
4: public:
5: 	/// Create a decoder object. buffer/buffer_len is the decoded data.
6: 	/// bit_width is the width of each value (before encoding).
7: 	RleBpDecoder(const uint8_t *buffer, uint32_t buffer_len, uint32_t bit_width)
8: 	    : buffer_((char *)buffer, buffer_len), bit_width_(bit_width), current_value_(0), repeat_count_(0),
9: 	      literal_count_(0) {
10: 		if (bit_width >= 64) {
11: 			throw std::runtime_error("Decode bit width too large");
12: 		}
13: 		byte_encoded_len = ((bit_width_ + 7) / 8);
14: 		max_val = (1 << bit_width_) - 1;
15: 	}
16: 
17: 	template <typename T>
18: 	void GetBatch(char *values_target_ptr, uint32_t batch_size) {
19: 		auto values = (T *)values_target_ptr;
20: 		uint32_t values_read = 0;
21: 
22: 		while (values_read < batch_size) {
23: 			if (repeat_count_ > 0) {
24: 				int repeat_batch = MinValue(batch_size - values_read, static_cast<uint32_t>(repeat_count_));
25: 				std::fill(values + values_read, values + values_read + repeat_batch, static_cast<T>(current_value_));
26: 				repeat_count_ -= repeat_batch;
27: 				values_read += repeat_batch;
28: 			} else if (literal_count_ > 0) {
29: 				uint32_t literal_batch = MinValue(batch_size - values_read, static_cast<uint32_t>(literal_count_));
30: 				uint32_t actual_read = BitUnpack<T>(values + values_read, literal_batch);
31: 				if (literal_batch != actual_read) {
32: 					throw std::runtime_error("Did not find enough values");
33: 				}
34: 				literal_count_ -= literal_batch;
35: 				values_read += literal_batch;
36: 			} else {
37: 				if (!NextCounts<T>()) {
38: 					if (values_read != batch_size) {
39: 						throw std::runtime_error("RLE decode did not find enough values");
40: 					}
41: 					return;
42: 				}
43: 			}
44: 		}
45: 		if (values_read != batch_size) {
46: 			throw std::runtime_error("RLE decode did not find enough values");
47: 		}
48: 	}
49: 
50: private:
51: 	ByteBuffer buffer_;
52: 
53: 	/// Number of bits needed to encode the value. Must be between 0 and 64.
54: 	int bit_width_;
55: 	uint64_t current_value_;
56: 	uint32_t repeat_count_;
57: 	uint32_t literal_count_;
58: 	uint8_t byte_encoded_len;
59: 	uint32_t max_val;
60: 
61: 	int8_t bitpack_pos = 0;
62: 
63: 	// this is slow but whatever, calls are rare
64: 	uint32_t VarintDecode() {
65: 		uint32_t result = 0;
66: 		uint8_t shift = 0;
67: 		uint8_t len = 0;
68: 		while (true) {
69: 			auto byte = buffer_.read<uint8_t>();
70: 			len++;
71: 			result |= (byte & 127) << shift;
72: 			if ((byte & 128) == 0)
73: 				break;
74: 			shift += 7;
75: 			if (shift > 32) {
76: 				throw std::runtime_error("Varint-decoding found too large number");
77: 			}
78: 		}
79: 		return result;
80: 	}
81: 
82: 	/// Fills literal_count_ and repeat_count_ with next values. Returns false if there
83: 	/// are no more.
84: 	template <typename T>
85: 	bool NextCounts() {
86: 		// Read the next run's indicator int, it could be a literal or repeated run.
87: 		// The int is encoded as a vlq-encoded value.
88: 		if (bitpack_pos != 0) {
89: 			buffer_.inc(1);
90: 			bitpack_pos = 0;
91: 		}
92: 		auto indicator_value = VarintDecode();
93: 
94: 		// lsb indicates if it is a literal run or repeated run
95: 		bool is_literal = indicator_value & 1;
96: 		if (is_literal) {
97: 			literal_count_ = (indicator_value >> 1) * 8;
98: 		} else {
99: 			repeat_count_ = indicator_value >> 1;
100: 			// (ARROW-4018) this is not big-endian compatible, lol
101: 			current_value_ = 0;
102: 			for (auto i = 0; i < byte_encoded_len; i++) {
103: 				current_value_ |= (buffer_.read<uint8_t>() << (i * 8));
104: 			}
105: 			// sanity check
106: 			if (repeat_count_ > 0 && current_value_ > max_val) {
107: 				throw std::runtime_error("Payload value bigger than allowed. Corrupted file?");
108: 			}
109: 		}
110: 		// TODO complain if we run out of buffer
111: 		return true;
112: 	}
113: 
114: 	// somewhat optimized implementation that avoids non-alignment
115: 
116: 	static const uint32_t BITPACK_MASKS[];
117: 	static const uint8_t BITPACK_DLEN;
118: 
119: 	template <typename T>
120: 	uint32_t BitUnpack(T *dest, uint32_t count) {
121: 		auto mask = BITPACK_MASKS[bit_width_];
122: 
123: 		for (uint32_t i = 0; i < count; i++) {
124: 			T val = (buffer_.get<uint8_t>() >> bitpack_pos) & mask;
125: 			bitpack_pos += bit_width_;
126: 			while (bitpack_pos > BITPACK_DLEN) {
127: 				buffer_.inc(1);
128: 				val |= (buffer_.get<uint8_t>() << (BITPACK_DLEN - (bitpack_pos - bit_width_))) & mask;
129: 				bitpack_pos -= BITPACK_DLEN;
130: 			}
131: 			dest[i] = val;
132: 		}
133: 		return count;
134: 	}
135: };
136: } // namespace duckdb
[end of extension/parquet/include/parquet_rle_bp_decoder.hpp]
[start of extension/parquet/include/parquet_writer.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // parquet_writer.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb.hpp"
12: #ifndef DUCKDB_AMALGAMATION
13: #include "duckdb/common/common.hpp"
14: #include "duckdb/common/exception.hpp"
15: #include "duckdb/common/mutex.hpp"
16: #include "duckdb/common/serializer/buffered_file_writer.hpp"
17: #include "duckdb/common/types/chunk_collection.hpp"
18: #endif
19: 
20: #include "parquet_types.h"
21: #include "thrift/protocol/TCompactProtocol.h"
22: 
23: namespace duckdb {
24: class FileSystem;
25: class FileOpener;
26: 
27: class ParquetWriter {
28: public:
29: 	ParquetWriter(FileSystem &fs, string file_name, FileOpener *file_opener, vector<LogicalType> types,
30: 	              vector<string> names, duckdb_parquet::format::CompressionCodec::type codec);
31: 
32: public:
33: 	void Flush(ChunkCollection &buffer);
34: 	void Finalize();
35: 
36: private:
37: 	string file_name;
38: 	vector<LogicalType> sql_types;
39: 	vector<string> column_names;
40: 	duckdb_parquet::format::CompressionCodec::type codec;
41: 
42: 	unique_ptr<BufferedFileWriter> writer;
43: 	shared_ptr<duckdb_apache::thrift::protocol::TProtocol> protocol;
44: 	duckdb_parquet::format::FileMetaData file_meta_data;
45: 	std::mutex lock;
46: };
47: 
48: } // namespace duckdb
[end of extension/parquet/include/parquet_writer.hpp]
[start of extension/parquet/include/struct_column_reader.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // struct_column_reader.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "column_reader.hpp"
12: #include "templated_column_reader.hpp"
13: 
14: namespace duckdb {
15: 
16: class StructColumnReader : public ColumnReader {
17: public:
18: 	StructColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t schema_idx_p,
19: 	                   idx_t max_define_p, idx_t max_repeat_p, vector<unique_ptr<ColumnReader>> child_readers_p)
20: 	    : ColumnReader(reader, move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p),
21: 	      child_readers(move(child_readers_p)) {
22: 		D_ASSERT(type.id() == LogicalTypeId::STRUCT);
23: 		D_ASSERT(!StructType::GetChildTypes(type).empty());
24: 	};
25: 
26: 	ColumnReader *GetChildReader(idx_t child_idx) {
27: 		return child_readers[child_idx].get();
28: 	}
29: 
30: 	void InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) override {
31: 		for (auto &child : child_readers) {
32: 			child->InitializeRead(columns, protocol_p);
33: 		}
34: 	}
35: 
36: 	idx_t Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
37: 	           Vector &result) override {
38: 		auto &struct_entries = StructVector::GetEntries(result);
39: 		D_ASSERT(StructType::GetChildTypes(Type()).size() == struct_entries.size());
40: 
41: 		idx_t read_count = num_values;
42: 		for (idx_t i = 0; i < struct_entries.size(); i++) {
43: 			auto child_num_values =
44: 			    child_readers[i]->Read(num_values, filter, define_out, repeat_out, *struct_entries[i]);
45: 			if (i == 0) {
46: 				read_count = child_num_values;
47: 			} else if (read_count != child_num_values) {
48: 				throw std::runtime_error("Struct child row count mismatch");
49: 			}
50: 		}
51: 
52: 		return read_count;
53: 	}
54: 
55: 	virtual void Skip(idx_t num_values) override {
56: 		D_ASSERT(0);
57: 	}
58: 
59: 	idx_t GroupRowsAvailable() override {
60: 		for (idx_t i = 0; i < child_readers.size(); i++) {
61: 			if (child_readers[i]->Type().id() != LogicalTypeId::LIST) {
62: 				return child_readers[i]->GroupRowsAvailable();
63: 			}
64: 		}
65: 		return child_readers[0]->GroupRowsAvailable();
66: 	}
67: 
68: 	vector<unique_ptr<ColumnReader>> child_readers;
69: };
70: 
71: } // namespace duckdb
[end of extension/parquet/include/struct_column_reader.hpp]
[start of extension/parquet/parquet-extension.cpp]
1: #define DUCKDB_EXTENSION_MAIN
2: 
3: #include <string>
4: #include <vector>
5: #include <fstream>
6: #include <iostream>
7: 
8: #include "parquet-extension.hpp"
9: #include "parquet_reader.hpp"
10: #include "parquet_writer.hpp"
11: #include "parquet_metadata.hpp"
12: #include "zstd_file_system.hpp"
13: 
14: #include "duckdb.hpp"
15: #ifndef DUCKDB_AMALGAMATION
16: #include "duckdb/common/file_system.hpp"
17: #include "duckdb/common/types/chunk_collection.hpp"
18: #include "duckdb/function/copy_function.hpp"
19: #include "duckdb/function/table_function.hpp"
20: #include "duckdb/common/file_system.hpp"
21: #include "duckdb/parallel/parallel_state.hpp"
22: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
23: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
24: 
25: #include "duckdb/common/enums/file_compression_type.hpp"
26: #include "duckdb/main/config.hpp"
27: #include "duckdb/parser/expression/constant_expression.hpp"
28: #include "duckdb/parser/expression/function_expression.hpp"
29: #include "duckdb/parser/tableref/table_function_ref.hpp"
30: 
31: #include "duckdb/storage/statistics/base_statistics.hpp"
32: 
33: #include "duckdb/main/client_context.hpp"
34: #include "duckdb/catalog/catalog.hpp"
35: #endif
36: 
37: namespace duckdb {
38: 
39: struct ParquetReadBindData : public FunctionData {
40: 	shared_ptr<ParquetReader> initial_reader;
41: 	vector<string> files;
42: 	vector<column_t> column_ids;
43: 	atomic<idx_t> chunk_count;
44: 	atomic<idx_t> cur_file;
45: };
46: 
47: struct ParquetReadOperatorData : public FunctionOperatorData {
48: 	shared_ptr<ParquetReader> reader;
49: 	ParquetReaderScanState scan_state;
50: 	bool is_parallel;
51: 	idx_t file_index;
52: 	vector<column_t> column_ids;
53: 	TableFilterSet *table_filters;
54: };
55: 
56: struct ParquetReadParallelState : public ParallelState {
57: 	mutex lock;
58: 	shared_ptr<ParquetReader> current_reader;
59: 	idx_t file_index;
60: 	idx_t row_group_index;
61: };
62: 
63: class ParquetScanFunction {
64: public:
65: 	static TableFunctionSet GetFunctionSet() {
66: 		TableFunctionSet set("parquet_scan");
67: 		auto table_function =
68: 		    TableFunction({LogicalType::VARCHAR}, ParquetScanImplementation, ParquetScanBind, ParquetScanInit,
69: 		                  /* statistics */ ParquetScanStats, /* cleanup */ nullptr,
70: 		                  /* dependency */ nullptr, ParquetCardinality,
71: 		                  /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr, ParquetScanMaxThreads,
72: 		                  ParquetInitParallelState, ParquetScanFuncParallel, ParquetScanParallelInit,
73: 		                  ParquetParallelStateNext, true, true, ParquetProgress);
74: 		table_function.named_parameters["binary_as_string"] = LogicalType::BOOLEAN;
75: 		set.AddFunction(table_function);
76: 		table_function = TableFunction({LogicalType::LIST(LogicalType::VARCHAR)}, ParquetScanImplementation,
77: 		                               ParquetScanBindList, ParquetScanInit, /* statistics */ ParquetScanStats,
78: 		                               /* cleanup */ nullptr,
79: 		                               /* dependency */ nullptr, ParquetCardinality,
80: 		                               /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr,
81: 		                               ParquetScanMaxThreads, ParquetInitParallelState, ParquetScanFuncParallel,
82: 		                               ParquetScanParallelInit, ParquetParallelStateNext, true, true, ParquetProgress);
83: 		table_function.named_parameters["binary_as_string"] = LogicalType::BOOLEAN;
84: 		set.AddFunction(table_function);
85: 		return set;
86: 	}
87: 
88: 	static unique_ptr<FunctionData> ParquetReadBind(ClientContext &context, CopyInfo &info,
89: 	                                                vector<string> &expected_names,
90: 	                                                vector<LogicalType> &expected_types) {
91: 		for (auto &option : info.options) {
92: 			auto loption = StringUtil::Lower(option.first);
93: 			if (loption == "compression" || loption == "codec") {
94: 				// CODEC option has no effect on parquet read: we determine codec from the file
95: 				continue;
96: 			} else {
97: 				throw NotImplementedException("Unsupported option for COPY FROM parquet: %s", option.first);
98: 			}
99: 		}
100: 		auto result = make_unique<ParquetReadBindData>();
101: 
102: 		FileSystem &fs = FileSystem::GetFileSystem(context);
103: 		result->files = fs.Glob(info.file_path);
104: 		if (result->files.empty()) {
105: 			throw IOException("No files found that match the pattern \"%s\"", info.file_path);
106: 		}
107: 		ParquetOptions parquet_options(context);
108: 		result->initial_reader = make_shared<ParquetReader>(context, result->files[0], expected_types, parquet_options);
109: 		return move(result);
110: 	}
111: 
112: 	static unique_ptr<BaseStatistics> ParquetScanStats(ClientContext &context, const FunctionData *bind_data_p,
113: 	                                                   column_t column_index) {
114: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
115: 
116: 		if (column_index == COLUMN_IDENTIFIER_ROW_ID) {
117: 			return nullptr;
118: 		}
119: 
120: 		// we do not want to parse the Parquet metadata for the sole purpose of getting column statistics
121: 
122: 		// We already parsed the metadata for the first file in a glob because we need some type info.
123: 		auto overall_stats = ParquetReader::ReadStatistics(
124: 		    *bind_data.initial_reader, bind_data.initial_reader->return_types[column_index], column_index,
125: 		    bind_data.initial_reader->metadata->metadata.get());
126: 
127: 		if (!overall_stats) {
128: 			return nullptr;
129: 		}
130: 
131: 		// if there is only one file in the glob (quite common case), we are done
132: 		auto &config = DBConfig::GetConfig(context);
133: 		if (bind_data.files.size() < 2) {
134: 			return overall_stats;
135: 		} else if (config.object_cache_enable) {
136: 			auto &cache = ObjectCache::GetObjectCache(context);
137: 			// for more than one file, we could be lucky and metadata for *every* file is in the object cache (if
138: 			// enabled at all)
139: 			FileSystem &fs = FileSystem::GetFileSystem(context);
140: 			for (idx_t file_idx = 1; file_idx < bind_data.files.size(); file_idx++) {
141: 				auto &file_name = bind_data.files[file_idx];
142: 				auto metadata = cache.Get<ParquetFileMetadataCache>(file_name);
143: 				if (!metadata) {
144: 					// missing metadata entry in cache, no usable stats
145: 					return nullptr;
146: 				}
147: 				auto handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,
148: 				                          FileSystem::DEFAULT_COMPRESSION, FileSystem::GetFileOpener(context));
149: 				// we need to check if the metadata cache entries are current
150: 				if (fs.GetLastModifiedTime(*handle) >= metadata->read_time) {
151: 					// missing or invalid metadata entry in cache, no usable stats overall
152: 					return nullptr;
153: 				}
154: 				// get and merge stats for file
155: 				auto file_stats = ParquetReader::ReadStatistics(*bind_data.initial_reader,
156: 				                                                bind_data.initial_reader->return_types[column_index],
157: 				                                                column_index, metadata->metadata.get());
158: 				if (!file_stats) {
159: 					return nullptr;
160: 				}
161: 				overall_stats->Merge(*file_stats);
162: 			}
163: 			// success!
164: 			return overall_stats;
165: 		}
166: 		// we have more than one file and no object cache so no statistics overall
167: 		return nullptr;
168: 	}
169: 
170: 	static void ParquetScanFuncParallel(ClientContext &context, const FunctionData *bind_data,
171: 	                                    FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output,
172: 	                                    ParallelState *parallel_state_p) {
173: 		//! FIXME: Have specialized parallel function from pandas scan here
174: 		ParquetScanImplementation(context, bind_data, operator_state, input, output);
175: 	}
176: 
177: 	static unique_ptr<FunctionData> ParquetScanBindInternal(ClientContext &context, vector<string> files,
178: 	                                                        vector<LogicalType> &return_types, vector<string> &names,
179: 	                                                        ParquetOptions parquet_options) {
180: 		auto result = make_unique<ParquetReadBindData>();
181: 		result->files = move(files);
182: 
183: 		result->initial_reader = make_shared<ParquetReader>(context, result->files[0], parquet_options);
184: 		return_types = result->initial_reader->return_types;
185: 
186: 		names = result->initial_reader->names;
187: 		return move(result);
188: 	}
189: 
190: 	static vector<string> ParquetGlob(FileSystem &fs, const string &glob) {
191: 		auto files = fs.Glob(glob);
192: 		if (files.empty()) {
193: 			throw IOException("No files found that match the pattern \"%s\"", glob);
194: 		}
195: 		return files;
196: 	}
197: 
198: 	static unique_ptr<FunctionData> ParquetScanBind(ClientContext &context, vector<Value> &inputs,
199: 	                                                unordered_map<string, Value> &named_parameters,
200: 	                                                vector<LogicalType> &input_table_types,
201: 	                                                vector<string> &input_table_names,
202: 	                                                vector<LogicalType> &return_types, vector<string> &names) {
203: 		auto &config = DBConfig::GetConfig(context);
204: 		if (!config.enable_external_access) {
205: 			throw PermissionException("Scanning Parquet files is disabled through configuration");
206: 		}
207: 		auto file_name = inputs[0].GetValue<string>();
208: 		ParquetOptions parquet_options(context);
209: 		for (auto &kv : named_parameters) {
210: 			if (kv.first == "binary_as_string") {
211: 				parquet_options.binary_as_string = kv.second.value_.boolean;
212: 			}
213: 		}
214: 		FileSystem &fs = FileSystem::GetFileSystem(context);
215: 		auto files = ParquetGlob(fs, file_name);
216: 		return ParquetScanBindInternal(context, move(files), return_types, names, parquet_options);
217: 	}
218: 
219: 	static unique_ptr<FunctionData> ParquetScanBindList(ClientContext &context, vector<Value> &inputs,
220: 	                                                    unordered_map<string, Value> &named_parameters,
221: 	                                                    vector<LogicalType> &input_table_types,
222: 	                                                    vector<string> &input_table_names,
223: 	                                                    vector<LogicalType> &return_types, vector<string> &names) {
224: 		auto &config = DBConfig::GetConfig(context);
225: 		if (!config.enable_external_access) {
226: 			throw PermissionException("Scanning Parquet files is disabled through configuration");
227: 		}
228: 		FileSystem &fs = FileSystem::GetFileSystem(context);
229: 		vector<string> files;
230: 		for (auto &val : inputs[0].list_value) {
231: 			auto glob_files = ParquetGlob(fs, val.ToString());
232: 			files.insert(files.end(), glob_files.begin(), glob_files.end());
233: 		}
234: 		if (files.empty()) {
235: 			throw IOException("Parquet reader needs at least one file to read");
236: 		}
237: 		ParquetOptions parquet_options(context);
238: 		for (auto &kv : named_parameters) {
239: 			if (kv.first == "binary_as_string") {
240: 				parquet_options.binary_as_string = kv.second.value_.boolean;
241: 			}
242: 		}
243: 		return ParquetScanBindInternal(context, move(files), return_types, names, parquet_options);
244: 	}
245: 
246: 	static unique_ptr<FunctionOperatorData> ParquetScanInit(ClientContext &context, const FunctionData *bind_data_p,
247: 	                                                        const vector<column_t> &column_ids,
248: 	                                                        TableFilterCollection *filters) {
249: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
250: 		bind_data.chunk_count = 0;
251: 		bind_data.cur_file = 0;
252: 		auto result = make_unique<ParquetReadOperatorData>();
253: 		result->column_ids = column_ids;
254: 
255: 		result->is_parallel = false;
256: 		result->file_index = 0;
257: 		result->table_filters = filters->table_filters;
258: 		// single-threaded: one thread has to read all groups
259: 		vector<idx_t> group_ids;
260: 		for (idx_t i = 0; i < bind_data.initial_reader->NumRowGroups(); i++) {
261: 			group_ids.push_back(i);
262: 		}
263: 		result->reader = bind_data.initial_reader;
264: 		result->reader->InitializeScan(result->scan_state, column_ids, move(group_ids), filters->table_filters);
265: 		return move(result);
266: 	}
267: 
268: 	static double ParquetProgress(ClientContext &context, const FunctionData *bind_data_p) {
269: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
270: 		if (bind_data.initial_reader->NumRows() == 0) {
271: 			return (100.0 * (bind_data.cur_file + 1)) / bind_data.files.size();
272: 		}
273: 		auto percentage = (bind_data.chunk_count * STANDARD_VECTOR_SIZE * 100.0 / bind_data.initial_reader->NumRows()) /
274: 		                  bind_data.files.size();
275: 		percentage += 100.0 * bind_data.cur_file / bind_data.files.size();
276: 		return percentage;
277: 	}
278: 
279: 	static unique_ptr<FunctionOperatorData>
280: 	ParquetScanParallelInit(ClientContext &context, const FunctionData *bind_data_p, ParallelState *parallel_state_p,
281: 	                        const vector<column_t> &column_ids, TableFilterCollection *filters) {
282: 		auto result = make_unique<ParquetReadOperatorData>();
283: 		result->column_ids = column_ids;
284: 		result->is_parallel = true;
285: 		result->table_filters = filters->table_filters;
286: 		if (!ParquetParallelStateNext(context, bind_data_p, result.get(), parallel_state_p)) {
287: 			return nullptr;
288: 		}
289: 		return move(result);
290: 	}
291: 
292: 	static void ParquetScanImplementation(ClientContext &context, const FunctionData *bind_data_p,
293: 	                                      FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
294: 		if (!operator_state) {
295: 			return;
296: 		}
297: 		auto &data = (ParquetReadOperatorData &)*operator_state;
298: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
299: 
300: 		do {
301: 			data.reader->Scan(data.scan_state, output);
302: 			bind_data.chunk_count++;
303: 			if (output.size() == 0 && !data.is_parallel) {
304: 				auto &bind_data = (ParquetReadBindData &)*bind_data_p;
305: 				// check if there is another file
306: 				if (data.file_index + 1 < bind_data.files.size()) {
307: 					data.file_index++;
308: 					bind_data.cur_file++;
309: 					bind_data.chunk_count = 0;
310: 					string file = bind_data.files[data.file_index];
311: 					// move to the next file
312: 					data.reader = make_shared<ParquetReader>(context, file, data.reader->return_types,
313: 					                                         data.reader->parquet_options, bind_data.files[0]);
314: 					vector<idx_t> group_ids;
315: 					for (idx_t i = 0; i < data.reader->NumRowGroups(); i++) {
316: 						group_ids.push_back(i);
317: 					}
318: 					data.reader->InitializeScan(data.scan_state, data.column_ids, move(group_ids), data.table_filters);
319: 				} else {
320: 					// exhausted all the files: done
321: 					break;
322: 				}
323: 			} else {
324: 				break;
325: 			}
326: 		} while (true);
327: 	}
328: 
329: 	static unique_ptr<NodeStatistics> ParquetCardinality(ClientContext &context, const FunctionData *bind_data) {
330: 		auto &data = (ParquetReadBindData &)*bind_data;
331: 		return make_unique<NodeStatistics>(data.initial_reader->NumRows() * data.files.size());
332: 	}
333: 
334: 	static idx_t ParquetScanMaxThreads(ClientContext &context, const FunctionData *bind_data) {
335: 		auto &data = (ParquetReadBindData &)*bind_data;
336: 		return data.initial_reader->NumRowGroups() * data.files.size();
337: 	}
338: 
339: 	static unique_ptr<ParallelState> ParquetInitParallelState(ClientContext &context, const FunctionData *bind_data_p,
340: 	                                                          const vector<column_t> &column_ids,
341: 	                                                          TableFilterCollection *filters) {
342: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
343: 		auto result = make_unique<ParquetReadParallelState>();
344: 		result->current_reader = bind_data.initial_reader;
345: 		result->row_group_index = 0;
346: 		result->file_index = 0;
347: 		return move(result);
348: 	}
349: 
350: 	static bool ParquetParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,
351: 	                                     FunctionOperatorData *state_p, ParallelState *parallel_state_p) {
352: 		if (!state_p) {
353: 			return false;
354: 		}
355: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
356: 		auto &parallel_state = (ParquetReadParallelState &)*parallel_state_p;
357: 		auto &scan_data = (ParquetReadOperatorData &)*state_p;
358: 
359: 		lock_guard<mutex> parallel_lock(parallel_state.lock);
360: 		if (parallel_state.row_group_index < parallel_state.current_reader->NumRowGroups()) {
361: 			// groups remain in the current parquet file: read the next group
362: 			scan_data.reader = parallel_state.current_reader;
363: 			vector<idx_t> group_indexes {parallel_state.row_group_index};
364: 			scan_data.reader->InitializeScan(scan_data.scan_state, scan_data.column_ids, group_indexes,
365: 			                                 scan_data.table_filters);
366: 			parallel_state.row_group_index++;
367: 			return true;
368: 		} else {
369: 			// no groups remain in the current parquet file: check if there are more files to read
370: 			while (parallel_state.file_index + 1 < bind_data.files.size()) {
371: 				// read the next file
372: 				string file = bind_data.files[++parallel_state.file_index];
373: 				parallel_state.current_reader =
374: 				    make_shared<ParquetReader>(context, file, parallel_state.current_reader->return_types,
375: 				                               parallel_state.current_reader->parquet_options);
376: 				if (parallel_state.current_reader->NumRowGroups() == 0) {
377: 					// empty parquet file, move to next file
378: 					continue;
379: 				}
380: 				// set up the scan state to read the first group
381: 				scan_data.reader = parallel_state.current_reader;
382: 				vector<idx_t> group_indexes {0};
383: 				scan_data.reader->InitializeScan(scan_data.scan_state, scan_data.column_ids, group_indexes,
384: 				                                 scan_data.table_filters);
385: 				parallel_state.row_group_index = 1;
386: 				return true;
387: 			}
388: 		}
389: 		return false;
390: 	}
391: };
392: 
393: struct ParquetWriteBindData : public FunctionData {
394: 	vector<LogicalType> sql_types;
395: 	string file_name;
396: 	vector<string> column_names;
397: 	duckdb_parquet::format::CompressionCodec::type codec = duckdb_parquet::format::CompressionCodec::SNAPPY;
398: };
399: 
400: struct ParquetWriteGlobalState : public GlobalFunctionData {
401: 	unique_ptr<ParquetWriter> writer;
402: };
403: 
404: struct ParquetWriteLocalState : public LocalFunctionData {
405: 	ParquetWriteLocalState() {
406: 		buffer = make_unique<ChunkCollection>();
407: 	}
408: 
409: 	unique_ptr<ChunkCollection> buffer;
410: };
411: 
412: unique_ptr<FunctionData> ParquetWriteBind(ClientContext &context, CopyInfo &info, vector<string> &names,
413:                                           vector<LogicalType> &sql_types) {
414: 	auto bind_data = make_unique<ParquetWriteBindData>();
415: 	for (auto &option : info.options) {
416: 		auto loption = StringUtil::Lower(option.first);
417: 		if (loption == "compression" || loption == "codec") {
418: 			if (!option.second.empty()) {
419: 				auto roption = StringUtil::Lower(option.second[0].ToString());
420: 				if (roption == "uncompressed") {
421: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::UNCOMPRESSED;
422: 					continue;
423: 				} else if (roption == "snappy") {
424: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::SNAPPY;
425: 					continue;
426: 				} else if (roption == "gzip") {
427: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::GZIP;
428: 					continue;
429: 				} else if (roption == "zstd") {
430: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::ZSTD;
431: 					continue;
432: 				}
433: 			}
434: 			throw ParserException("Expected %s argument to be either [uncompressed, snappy, gzip or zstd]", loption);
435: 		} else {
436: 			throw NotImplementedException("Unrecognized option for PARQUET: %s", option.first.c_str());
437: 		}
438: 	}
439: 	bind_data->sql_types = sql_types;
440: 	bind_data->column_names = names;
441: 	bind_data->file_name = info.file_path;
442: 	return move(bind_data);
443: }
444: 
445: unique_ptr<GlobalFunctionData> ParquetWriteInitializeGlobal(ClientContext &context, FunctionData &bind_data) {
446: 	auto global_state = make_unique<ParquetWriteGlobalState>();
447: 	auto &parquet_bind = (ParquetWriteBindData &)bind_data;
448: 
449: 	auto &fs = FileSystem::GetFileSystem(context);
450: 	global_state->writer =
451: 	    make_unique<ParquetWriter>(fs, parquet_bind.file_name, FileSystem::GetFileOpener(context),
452: 	                               parquet_bind.sql_types, parquet_bind.column_names, parquet_bind.codec);
453: 	return move(global_state);
454: }
455: 
456: void ParquetWriteSink(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
457:                       LocalFunctionData &lstate, DataChunk &input) {
458: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
459: 	auto &local_state = (ParquetWriteLocalState &)lstate;
460: 
461: 	// append data to the local (buffered) chunk collection
462: 	local_state.buffer->Append(input);
463: 	if (local_state.buffer->Count() > 100000) {
464: 		// if the chunk collection exceeds a certain size we flush it to the parquet file
465: 		global_state.writer->Flush(*local_state.buffer);
466: 		// and reset the buffer
467: 		local_state.buffer = make_unique<ChunkCollection>();
468: 	}
469: }
470: 
471: void ParquetWriteCombine(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
472:                          LocalFunctionData &lstate) {
473: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
474: 	auto &local_state = (ParquetWriteLocalState &)lstate;
475: 	// flush any data left in the local state to the file
476: 	global_state.writer->Flush(*local_state.buffer);
477: }
478: 
479: void ParquetWriteFinalize(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate) {
480: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
481: 	// finalize: write any additional metadata to the file here
482: 	global_state.writer->Finalize();
483: }
484: 
485: unique_ptr<LocalFunctionData> ParquetWriteInitializeLocal(ClientContext &context, FunctionData &bind_data) {
486: 	return make_unique<ParquetWriteLocalState>();
487: }
488: 
489: unique_ptr<TableFunctionRef> ParquetScanReplacement(const string &table_name, void *data) {
490: 	if (!StringUtil::EndsWith(StringUtil::Lower(table_name), ".parquet")) {
491: 		return nullptr;
492: 	}
493: 	auto table_function = make_unique<TableFunctionRef>();
494: 	vector<unique_ptr<ParsedExpression>> children;
495: 	children.push_back(make_unique<ConstantExpression>(Value(table_name)));
496: 	table_function->function = make_unique<FunctionExpression>("parquet_scan", move(children));
497: 	return table_function;
498: }
499: 
500: void ParquetExtension::Load(DuckDB &db) {
501: 	auto &fs = db.GetFileSystem();
502: 	fs.RegisterSubSystem(FileCompressionType::ZSTD, make_unique<ZStdFileSystem>());
503: 
504: 	auto scan_fun = ParquetScanFunction::GetFunctionSet();
505: 	CreateTableFunctionInfo cinfo(scan_fun);
506: 	cinfo.name = "read_parquet";
507: 	CreateTableFunctionInfo pq_scan = cinfo;
508: 	pq_scan.name = "parquet_scan";
509: 
510: 	ParquetMetaDataFunction meta_fun;
511: 	CreateTableFunctionInfo meta_cinfo(meta_fun);
512: 
513: 	ParquetSchemaFunction schema_fun;
514: 	CreateTableFunctionInfo schema_cinfo(schema_fun);
515: 
516: 	CopyFunction function("parquet");
517: 	function.copy_to_bind = ParquetWriteBind;
518: 	function.copy_to_initialize_global = ParquetWriteInitializeGlobal;
519: 	function.copy_to_initialize_local = ParquetWriteInitializeLocal;
520: 	function.copy_to_sink = ParquetWriteSink;
521: 	function.copy_to_combine = ParquetWriteCombine;
522: 	function.copy_to_finalize = ParquetWriteFinalize;
523: 	function.copy_from_bind = ParquetScanFunction::ParquetReadBind;
524: 	function.copy_from_function = scan_fun.functions[0];
525: 
526: 	function.extension = "parquet";
527: 	CreateCopyFunctionInfo info(function);
528: 
529: 	Connection con(db);
530: 	con.BeginTransaction();
531: 	auto &context = *con.context;
532: 	auto &catalog = Catalog::GetCatalog(context);
533: 	catalog.CreateCopyFunction(context, &info);
534: 	catalog.CreateTableFunction(context, &cinfo);
535: 	catalog.CreateTableFunction(context, &pq_scan);
536: 	catalog.CreateTableFunction(context, &meta_cinfo);
537: 	catalog.CreateTableFunction(context, &schema_cinfo);
538: 	con.Commit();
539: 
540: 	auto &config = DBConfig::GetConfig(*db.instance);
541: 	config.replacement_scans.emplace_back(ParquetScanReplacement);
542: 	config.AddExtensionOption("binary_as_string", "In Parquet files, interpret binary data as a string.",
543: 	                          LogicalType::BOOLEAN);
544: }
545: 
546: std::string ParquetExtension::Name() {
547: 	return "parquet";
548: }
549: 
550: } // namespace duckdb
[end of extension/parquet/parquet-extension.cpp]
[start of extension/parquet/parquet_config.py]
1: import os
2: # list all include directories
3: include_directories = [os.path.sep.join(x.split('/')) for x in ['extension/parquet/include', 'third_party/parquet', 'third_party/snappy', 'third_party/thrift', 'third_party/zstd/include']]
4: # source files
5: source_files = [os.path.sep.join(x.split('/')) for x in ['extension/parquet/parquet-extension.cpp', 'third_party/parquet/parquet_constants.cpp',  'third_party/parquet/parquet_types.cpp',  'third_party/thrift/thrift/protocol/TProtocol.cpp',  'third_party/thrift/thrift/transport/TTransportException.cpp',  'third_party/thrift/thrift/transport/TBufferTransports.cpp',  'third_party/snappy/snappy.cc',  'third_party/snappy/snappy-sinksource.cc']]
6: # zstd
7: source_files += [os.path.sep.join(x.split('/')) for x in ['third_party/zstd/decompress/zstd_ddict.cpp', 'third_party/zstd/decompress/huf_decompress.cpp', 'third_party/zstd/decompress/zstd_decompress.cpp', 'third_party/zstd/decompress/zstd_decompress_block.cpp', 'third_party/zstd/common/entropy_common.cpp', 'third_party/zstd/common/fse_decompress.cpp', 'third_party/zstd/common/zstd_common.cpp', 'third_party/zstd/common/error_private.cpp', 'third_party/zstd/common/xxhash.cpp']]
8: source_files += [os.path.sep.join(x.split('/')) for x in ['third_party/zstd/compress/fse_compress.cpp', 'third_party/zstd/compress/hist.cpp', 'third_party/zstd/compress/huf_compress.cpp', 'third_party/zstd/compress/zstd_compress.cpp', 'third_party/zstd/compress/zstd_compress_literals.cpp', 'third_party/zstd/compress/zstd_compress_sequences.cpp', 'third_party/zstd/compress/zstd_compress_superblock.cpp', 'third_party/zstd/compress/zstd_double_fast.cpp', 'third_party/zstd/compress/zstd_fast.cpp', 'third_party/zstd/compress/zstd_lazy.cpp', 'third_party/zstd/compress/zstd_ldm.cpp', 'third_party/zstd/compress/zstd_opt.cpp']]
9: source_files += [os.path.sep.join(x.split('/')) for x in ['extension/parquet/parquet_reader.cpp', 'extension/parquet/parquet_timestamp.cpp', 'extension/parquet/parquet_writer.cpp', 'extension/parquet/column_reader.cpp', 'extension/parquet/parquet_statistics.cpp', 'extension/parquet/parquet_metadata.cpp', 'extension/parquet/zstd_file_system.cpp']]
[end of extension/parquet/parquet_config.py]
[start of extension/parquet/parquet_reader.cpp]
1: #include "parquet_reader.hpp"
2: #include "parquet_timestamp.hpp"
3: #include "parquet_statistics.hpp"
4: #include "column_reader.hpp"
5: 
6: #include "boolean_column_reader.hpp"
7: #include "callback_column_reader.hpp"
8: #include "decimal_column_reader.hpp"
9: #include "list_column_reader.hpp"
10: #include "string_column_reader.hpp"
11: #include "struct_column_reader.hpp"
12: #include "templated_column_reader.hpp"
13: 
14: #include "thrift_tools.hpp"
15: 
16: #include "parquet_file_metadata_cache.hpp"
17: 
18: #include "duckdb.hpp"
19: #ifndef DUCKDB_AMALGAMATION
20: #include "duckdb/planner/table_filter.hpp"
21: #include "duckdb/planner/filter/constant_filter.hpp"
22: #include "duckdb/planner/filter/null_filter.hpp"
23: #include "duckdb/planner/filter/conjunction_filter.hpp"
24: #include "duckdb/common/file_system.hpp"
25: #include "duckdb/common/string_util.hpp"
26: #include "duckdb/common/types/date.hpp"
27: #include "duckdb/common/pair.hpp"
28: 
29: #include "duckdb/storage/object_cache.hpp"
30: #endif
31: 
32: #include <sstream>
33: #include <cassert>
34: #include <chrono>
35: #include <cstring>
36: #include <iostream>
37: 
38: namespace duckdb {
39: 
40: using duckdb_parquet::format::ColumnChunk;
41: using duckdb_parquet::format::ConvertedType;
42: using duckdb_parquet::format::FieldRepetitionType;
43: using duckdb_parquet::format::FileMetaData;
44: using ParquetRowGroup = duckdb_parquet::format::RowGroup;
45: using duckdb_parquet::format::SchemaElement;
46: using duckdb_parquet::format::Statistics;
47: using duckdb_parquet::format::Type;
48: 
49: static unique_ptr<duckdb_apache::thrift::protocol::TProtocol> CreateThriftProtocol(Allocator &allocator,
50:                                                                                    FileHandle &file_handle) {
51: 	auto transport = make_shared<ThriftFileTransport>(allocator, file_handle);
52: 	return make_unique<duckdb_apache::thrift::protocol::TCompactProtocolT<ThriftFileTransport>>(move(transport));
53: }
54: 
55: static shared_ptr<ParquetFileMetadataCache> LoadMetadata(Allocator &allocator, FileHandle &file_handle) {
56: 	auto current_time = std::chrono::system_clock::to_time_t(std::chrono::system_clock::now());
57: 
58: 	auto proto = CreateThriftProtocol(allocator, file_handle);
59: 	auto &transport = ((ThriftFileTransport &)*proto->getTransport());
60: 	auto file_size = transport.GetSize();
61: 	if (file_size < 12) {
62: 		throw InvalidInputException("File '%s' too small to be a Parquet file", file_handle.path);
63: 	}
64: 
65: 	ResizeableBuffer buf;
66: 	buf.resize(allocator, 8);
67: 	buf.zero();
68: 
69: 	transport.SetLocation(file_size - 8);
70: 	transport.read((uint8_t *)buf.ptr, 8);
71: 
72: 	if (strncmp(buf.ptr + 4, "PAR1", 4) != 0) {
73: 		throw InvalidInputException("No magic bytes found at end of file '%s'", file_handle.path);
74: 	}
75: 	// read four-byte footer length from just before the end magic bytes
76: 	auto footer_len = *(uint32_t *)buf.ptr;
77: 	if (footer_len <= 0 || file_size < 12 + footer_len) {
78: 		throw InvalidInputException("Footer length error in file '%s'", file_handle.path);
79: 	}
80: 	auto metadata_pos = file_size - (footer_len + 8);
81: 	transport.SetLocation(metadata_pos);
82: 	transport.Prefetch(metadata_pos, footer_len);
83: 
84: 	auto metadata = make_unique<FileMetaData>();
85: 	metadata->read(proto.get());
86: 	return make_shared<ParquetFileMetadataCache>(move(metadata), current_time);
87: }
88: 
89: LogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele) {
90: 	// inner node
91: 	D_ASSERT(s_ele.__isset.type && s_ele.num_children == 0);
92: 	if (s_ele.type == Type::FIXED_LEN_BYTE_ARRAY && !s_ele.__isset.type_length) {
93: 		throw IOException("FIXED_LEN_BYTE_ARRAY requires length to be set");
94: 	}
95: 	if (s_ele.__isset.converted_type) {
96: 		switch (s_ele.converted_type) {
97: 		case ConvertedType::INT_8:
98: 			if (s_ele.type == Type::INT32) {
99: 				return LogicalType::TINYINT;
100: 			} else {
101: 				throw IOException("INT8 converted type can only be set for value of Type::INT32");
102: 			}
103: 		case ConvertedType::INT_16:
104: 			if (s_ele.type == Type::INT32) {
105: 				return LogicalType::SMALLINT;
106: 			} else {
107: 				throw IOException("INT16 converted type can only be set for value of Type::INT32");
108: 			}
109: 		case ConvertedType::INT_32:
110: 			if (s_ele.type == Type::INT32) {
111: 				return LogicalType::INTEGER;
112: 			} else {
113: 				throw IOException("INT32 converted type can only be set for value of Type::INT32");
114: 			}
115: 		case ConvertedType::INT_64:
116: 			if (s_ele.type == Type::INT64) {
117: 				return LogicalType::BIGINT;
118: 			} else {
119: 				throw IOException("INT64 converted type can only be set for value of Type::INT32");
120: 			}
121: 		case ConvertedType::UINT_8:
122: 			if (s_ele.type == Type::INT32) {
123: 				return LogicalType::UTINYINT;
124: 			} else {
125: 				throw IOException("UINT8 converted type can only be set for value of Type::INT32");
126: 			}
127: 		case ConvertedType::UINT_16:
128: 			if (s_ele.type == Type::INT32) {
129: 				return LogicalType::USMALLINT;
130: 			} else {
131: 				throw IOException("UINT16 converted type can only be set for value of Type::INT32");
132: 			}
133: 		case ConvertedType::UINT_32:
134: 			if (s_ele.type == Type::INT32) {
135: 				return LogicalType::UINTEGER;
136: 			} else {
137: 				throw IOException("UINT32 converted type can only be set for value of Type::INT32");
138: 			}
139: 		case ConvertedType::UINT_64:
140: 			if (s_ele.type == Type::INT64) {
141: 				return LogicalType::UBIGINT;
142: 			} else {
143: 				throw IOException("UINT64 converted type can only be set for value of Type::INT64");
144: 			}
145: 		case ConvertedType::DATE:
146: 			if (s_ele.type == Type::INT32) {
147: 				return LogicalType::DATE;
148: 			} else {
149: 				throw IOException("DATE converted type can only be set for value of Type::INT32");
150: 			}
151: 		case ConvertedType::TIMESTAMP_MICROS:
152: 		case ConvertedType::TIMESTAMP_MILLIS:
153: 			if (s_ele.type == Type::INT64) {
154: 				return LogicalType::TIMESTAMP;
155: 			} else {
156: 				throw IOException("TIMESTAMP converted type can only be set for value of Type::INT64");
157: 			}
158: 		case ConvertedType::DECIMAL:
159: 			if (!s_ele.__isset.precision || !s_ele.__isset.scale) {
160: 				throw IOException("DECIMAL requires a length and scale specifier!");
161: 			}
162: 			switch (s_ele.type) {
163: 			case Type::BYTE_ARRAY:
164: 			case Type::FIXED_LEN_BYTE_ARRAY:
165: 			case Type::INT32:
166: 			case Type::INT64:
167: 				return LogicalType::DECIMAL(s_ele.precision, s_ele.scale);
168: 			default:
169: 				throw IOException(
170: 				    "DECIMAL converted type can only be set for value of Type::(FIXED_LEN_)BYTE_ARRAY/INT32/INT64");
171: 			}
172: 		case ConvertedType::UTF8:
173: 			switch (s_ele.type) {
174: 			case Type::BYTE_ARRAY:
175: 			case Type::FIXED_LEN_BYTE_ARRAY:
176: 				return LogicalType::VARCHAR;
177: 			default:
178: 				throw IOException("UTF8 converted type can only be set for Type::(FIXED_LEN_)BYTE_ARRAY");
179: 			}
180: 		case ConvertedType::MAP:
181: 		case ConvertedType::MAP_KEY_VALUE:
182: 		case ConvertedType::LIST:
183: 		case ConvertedType::ENUM:
184: 		case ConvertedType::TIME_MILLIS:
185: 		case ConvertedType::TIME_MICROS:
186: 		case ConvertedType::JSON:
187: 		case ConvertedType::BSON:
188: 		case ConvertedType::INTERVAL:
189: 		default:
190: 			throw IOException("Unsupported converted type");
191: 		}
192: 	} else {
193: 		// no converted type set
194: 		// use default type for each physical type
195: 		switch (s_ele.type) {
196: 		case Type::BOOLEAN:
197: 			return LogicalType::BOOLEAN;
198: 		case Type::INT32:
199: 			return LogicalType::INTEGER;
200: 		case Type::INT64:
201: 			return LogicalType::BIGINT;
202: 		case Type::INT96: // always a timestamp it would seem
203: 			return LogicalType::TIMESTAMP;
204: 		case Type::FLOAT:
205: 			return LogicalType::FLOAT;
206: 		case Type::DOUBLE:
207: 			return LogicalType::DOUBLE;
208: 		case Type::BYTE_ARRAY:
209: 		case Type::FIXED_LEN_BYTE_ARRAY:
210: 			if (parquet_options.binary_as_string) {
211: 				return LogicalType::VARCHAR;
212: 			}
213: 			return LogicalType::BLOB;
214: 		default:
215: 			return LogicalType::INVALID;
216: 		}
217: 	}
218: }
219: 
220: unique_ptr<ColumnReader> ParquetReader::CreateReaderRecursive(const FileMetaData *file_meta_data, idx_t depth,
221:                                                               idx_t max_define, idx_t max_repeat,
222:                                                               idx_t &next_schema_idx, idx_t &next_file_idx) {
223: 	D_ASSERT(file_meta_data);
224: 	D_ASSERT(next_schema_idx < file_meta_data->schema.size());
225: 	auto &s_ele = file_meta_data->schema[next_schema_idx];
226: 	auto this_idx = next_schema_idx;
227: 
228: 	if (s_ele.__isset.repetition_type) {
229: 		if (s_ele.repetition_type != FieldRepetitionType::REQUIRED) {
230: 			max_define++;
231: 		}
232: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
233: 			max_repeat++;
234: 		}
235: 	}
236: 
237: 	if (!s_ele.__isset.type) { // inner node
238: 		if (s_ele.num_children == 0) {
239: 			throw std::runtime_error("Node has no children but should");
240: 		}
241: 		child_list_t<LogicalType> child_types;
242: 		vector<unique_ptr<ColumnReader>> child_readers;
243: 
244: 		idx_t c_idx = 0;
245: 		while (c_idx < (idx_t)s_ele.num_children) {
246: 			next_schema_idx++;
247: 
248: 			auto &child_ele = file_meta_data->schema[next_schema_idx];
249: 
250: 			auto child_reader = CreateReaderRecursive(file_meta_data, depth + 1, max_define, max_repeat,
251: 			                                          next_schema_idx, next_file_idx);
252: 			child_types.push_back(make_pair(child_ele.name, child_reader->Type()));
253: 			child_readers.push_back(move(child_reader));
254: 
255: 			c_idx++;
256: 		}
257: 		D_ASSERT(!child_types.empty());
258: 		unique_ptr<ColumnReader> result;
259: 		LogicalType result_type;
260: 		// if we only have a single child no reason to create a struct ay
261: 		if (child_types.size() > 1 || depth == 0) {
262: 			result_type = LogicalType::STRUCT(move(child_types));
263: 			result = make_unique<StructColumnReader>(*this, result_type, s_ele, this_idx, max_define, max_repeat,
264: 			                                         move(child_readers));
265: 		} else {
266: 			// if we have a struct with only a single type, pull up
267: 			result_type = child_types[0].second;
268: 			result = move(child_readers[0]);
269: 		}
270: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
271: 			result_type = LogicalType::LIST(result_type);
272: 			return make_unique<ListColumnReader>(*this, result_type, s_ele, this_idx, max_define, max_repeat,
273: 			                                     move(result));
274: 		}
275: 		return result;
276: 	} else { // leaf node
277: 
278: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
279: 			const auto derived_type = DeriveLogicalType(s_ele);
280: 			auto list_type = LogicalType::LIST(derived_type);
281: 
282: 			auto element_reader =
283: 			    ColumnReader::CreateReader(*this, derived_type, s_ele, next_file_idx++, max_define, max_repeat);
284: 
285: 			return make_unique<ListColumnReader>(*this, list_type, s_ele, this_idx, max_define, max_repeat,
286: 			                                     move(element_reader));
287: 		}
288: 
289: 		// TODO check return value of derive type or should we only do this on read()
290: 		return ColumnReader::CreateReader(*this, DeriveLogicalType(s_ele), s_ele, next_file_idx++, max_define,
291: 		                                  max_repeat);
292: 	}
293: }
294: 
295: // TODO we don't need readers for columns we are not going to read ay
296: unique_ptr<ColumnReader> ParquetReader::CreateReader(const duckdb_parquet::format::FileMetaData *file_meta_data) {
297: 	idx_t next_schema_idx = 0;
298: 	idx_t next_file_idx = 0;
299: 
300: 	auto ret = CreateReaderRecursive(file_meta_data, 0, 0, 0, next_schema_idx, next_file_idx);
301: 	D_ASSERT(next_schema_idx == file_meta_data->schema.size() - 1);
302: 	D_ASSERT(file_meta_data->row_groups.empty() || next_file_idx == file_meta_data->row_groups[0].columns.size());
303: 	return ret;
304: }
305: 
306: void ParquetReader::InitializeSchema(const vector<LogicalType> &expected_types_p, const string &initial_filename_p) {
307: 	auto file_meta_data = GetFileMetadata();
308: 
309: 	if (file_meta_data->__isset.encryption_algorithm) {
310: 		throw FormatException("Encrypted Parquet files are not supported");
311: 	}
312: 	// check if we like this schema
313: 	if (file_meta_data->schema.size() < 2) {
314: 		throw FormatException("Need at least one non-root column in the file");
315: 	}
316: 
317: 	bool has_expected_types = !expected_types_p.empty();
318: 	auto root_reader = CreateReader(file_meta_data);
319: 
320: 	auto &root_type = root_reader->Type();
321: 	auto &child_types = StructType::GetChildTypes(root_type);
322: 	D_ASSERT(root_type.id() == LogicalTypeId::STRUCT);
323: 	if (has_expected_types && child_types.size() != expected_types_p.size()) {
324: 		throw FormatException("column count mismatch");
325: 	}
326: 	idx_t col_idx = 0;
327: 	for (auto &type_pair : child_types) {
328: 		if (has_expected_types && expected_types_p[col_idx] != type_pair.second) {
329: 			if (initial_filename_p.empty()) {
330: 				throw FormatException("column \"%d\" in parquet file is of type %s, could not auto cast to "
331: 				                      "expected type %s for this column",
332: 				                      col_idx, type_pair.second, expected_types_p[col_idx].ToString());
333: 			} else {
334: 				throw FormatException("schema mismatch in Parquet glob: column \"%d\" in parquet file is of type "
335: 				                      "%s, but in the original file \"%s\" this column is of type \"%s\"",
336: 				                      col_idx, type_pair.second, initial_filename_p,
337: 				                      expected_types_p[col_idx].ToString());
338: 			}
339: 		} else {
340: 			names.push_back(type_pair.first);
341: 			return_types.push_back(type_pair.second);
342: 		}
343: 		col_idx++;
344: 	}
345: 	D_ASSERT(!names.empty());
346: 	D_ASSERT(!return_types.empty());
347: }
348: 
349: ParquetOptions::ParquetOptions(ClientContext &context) {
350: 	Value binary_as_string_val;
351: 	if (context.TryGetCurrentSetting("binary_as_string", binary_as_string_val)) {
352: 		binary_as_string = binary_as_string_val.GetValue<bool>();
353: 	}
354: }
355: 
356: ParquetReader::ParquetReader(Allocator &allocator_p, unique_ptr<FileHandle> file_handle_p,
357:                              const vector<LogicalType> &expected_types_p, const string &initial_filename_p)
358:     : allocator(allocator_p) {
359: 	file_name = file_handle_p->path;
360: 	file_handle = move(file_handle_p);
361: 	metadata = LoadMetadata(allocator, *file_handle);
362: 	InitializeSchema(expected_types_p, initial_filename_p);
363: }
364: 
365: ParquetReader::ParquetReader(ClientContext &context_p, string file_name_p, const vector<LogicalType> &expected_types_p,
366:                              ParquetOptions parquet_options_p, const string &initial_filename_p)
367:     : allocator(Allocator::Get(context_p)), file_opener(FileSystem::GetFileOpener(context_p)),
368:       parquet_options(parquet_options_p) {
369: 	auto &fs = FileSystem::GetFileSystem(context_p);
370: 	file_name = move(file_name_p);
371: 	file_handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,
372: 	                          FileSystem::DEFAULT_COMPRESSION, file_opener);
373: 	// If object cached is disabled
374: 	// or if this file has cached metadata
375: 	// or if the cached version already expired
376: 	auto last_modify_time = fs.GetLastModifiedTime(*file_handle);
377: 	if (!ObjectCache::ObjectCacheEnabled(context_p)) {
378: 		metadata = LoadMetadata(allocator, *file_handle);
379: 	} else {
380: 		metadata = ObjectCache::GetObjectCache(context_p).Get<ParquetFileMetadataCache>(file_name);
381: 		if (!metadata || (last_modify_time + 10 >= metadata->read_time)) {
382: 			metadata = LoadMetadata(allocator, *file_handle);
383: 			ObjectCache::GetObjectCache(context_p).Put(file_name, metadata);
384: 		}
385: 	}
386: 	InitializeSchema(expected_types_p, initial_filename_p);
387: }
388: 
389: ParquetReader::~ParquetReader() {
390: }
391: 
392: const FileMetaData *ParquetReader::GetFileMetadata() {
393: 	D_ASSERT(metadata);
394: 	D_ASSERT(metadata->metadata);
395: 	return metadata->metadata.get();
396: }
397: 
398: // TODO also somewhat ugly, perhaps this can be moved to the column reader too
399: unique_ptr<BaseStatistics> ParquetReader::ReadStatistics(ParquetReader &reader, LogicalType &type,
400:                                                          column_t file_col_idx, const FileMetaData *file_meta_data) {
401: 	unique_ptr<BaseStatistics> column_stats;
402: 	auto root_reader = reader.CreateReader(file_meta_data);
403: 	auto column_reader = ((StructColumnReader *)root_reader.get())->GetChildReader(file_col_idx);
404: 
405: 	for (auto &row_group : file_meta_data->row_groups) {
406: 		auto chunk_stats = column_reader->Stats(row_group.columns);
407: 		if (!chunk_stats) {
408: 			return nullptr;
409: 		}
410: 		if (!column_stats) {
411: 			column_stats = move(chunk_stats);
412: 		} else {
413: 			column_stats->Merge(*chunk_stats);
414: 		}
415: 	}
416: 	return column_stats;
417: }
418: 
419: const ParquetRowGroup &ParquetReader::GetGroup(ParquetReaderScanState &state) {
420: 	auto file_meta_data = GetFileMetadata();
421: 	D_ASSERT(state.current_group >= 0 && (idx_t)state.current_group < state.group_idx_list.size());
422: 	D_ASSERT(state.group_idx_list[state.current_group] >= 0 &&
423: 	         state.group_idx_list[state.current_group] < file_meta_data->row_groups.size());
424: 	return file_meta_data->row_groups[state.group_idx_list[state.current_group]];
425: }
426: 
427: void ParquetReader::PrepareRowGroupBuffer(ParquetReaderScanState &state, idx_t out_col_idx) {
428: 	auto &group = GetGroup(state);
429: 
430: 	auto column_reader = ((StructColumnReader *)state.root_reader.get())->GetChildReader(state.column_ids[out_col_idx]);
431: 
432: 	// TODO move this to columnreader too
433: 	if (state.filters) {
434: 		auto stats = column_reader->Stats(group.columns);
435: 		// filters contain output chunk index, not file col idx!
436: 		auto filter_entry = state.filters->filters.find(out_col_idx);
437: 		if (stats && filter_entry != state.filters->filters.end()) {
438: 			bool skip_chunk = false;
439: 			auto &filter = *filter_entry->second;
440: 			auto prune_result = filter.CheckStatistics(*stats);
441: 			if (prune_result == FilterPropagateResult::FILTER_ALWAYS_FALSE) {
442: 				skip_chunk = true;
443: 			}
444: 			if (skip_chunk) {
445: 				state.group_offset = group.num_rows;
446: 				return;
447: 				// this effectively will skip this chunk
448: 			}
449: 		}
450: 	}
451: 
452: 	state.root_reader->InitializeRead(group.columns, *state.thrift_file_proto);
453: }
454: 
455: idx_t ParquetReader::NumRows() {
456: 	return GetFileMetadata()->num_rows;
457: }
458: 
459: idx_t ParquetReader::NumRowGroups() {
460: 	return GetFileMetadata()->row_groups.size();
461: }
462: 
463: void ParquetReader::InitializeScan(ParquetReaderScanState &state, vector<column_t> column_ids,
464:                                    vector<idx_t> groups_to_read, TableFilterSet *filters) {
465: 	state.current_group = -1;
466: 	state.finished = false;
467: 	state.column_ids = move(column_ids);
468: 	state.group_offset = 0;
469: 	state.group_idx_list = move(groups_to_read);
470: 	state.filters = filters;
471: 	state.sel.Initialize(STANDARD_VECTOR_SIZE);
472: 	state.file_handle =
473: 	    file_handle->file_system.OpenFile(file_handle->path, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,
474: 	                                      FileSystem::DEFAULT_COMPRESSION, file_opener);
475: 	state.thrift_file_proto = CreateThriftProtocol(allocator, *state.file_handle);
476: 	state.root_reader = CreateReader(GetFileMetadata());
477: 
478: 	state.define_buf.resize(allocator, STANDARD_VECTOR_SIZE);
479: 	state.repeat_buf.resize(allocator, STANDARD_VECTOR_SIZE);
480: }
481: 
482: void FilterIsNull(Vector &v, parquet_filter_t &filter_mask, idx_t count) {
483: 	auto &mask = FlatVector::Validity(v);
484: 	if (mask.AllValid()) {
485: 		filter_mask.reset();
486: 	} else {
487: 		for (idx_t i = 0; i < count; i++) {
488: 			filter_mask[i] = filter_mask[i] && !mask.RowIsValid(i);
489: 		}
490: 	}
491: }
492: 
493: void FilterIsNotNull(Vector &v, parquet_filter_t &filter_mask, idx_t count) {
494: 	auto &mask = FlatVector::Validity(v);
495: 	if (!mask.AllValid()) {
496: 		for (idx_t i = 0; i < count; i++) {
497: 			filter_mask[i] = filter_mask[i] && mask.RowIsValid(i);
498: 		}
499: 	}
500: }
501: 
502: template <class T, class OP>
503: void TemplatedFilterOperation(Vector &v, T constant, parquet_filter_t &filter_mask, idx_t count) {
504: 	D_ASSERT(v.GetVectorType() == VectorType::FLAT_VECTOR); // we just created the damn thing it better be
505: 
506: 	auto v_ptr = FlatVector::GetData<T>(v);
507: 	auto &mask = FlatVector::Validity(v);
508: 
509: 	if (!mask.AllValid()) {
510: 		for (idx_t i = 0; i < count; i++) {
511: 			if (mask.RowIsValid(i)) {
512: 				filter_mask[i] = filter_mask[i] && OP::Operation(v_ptr[i], constant);
513: 			}
514: 		}
515: 	} else {
516: 		for (idx_t i = 0; i < count; i++) {
517: 			filter_mask[i] = filter_mask[i] && OP::Operation(v_ptr[i], constant);
518: 		}
519: 	}
520: }
521: 
522: template <class OP>
523: static void FilterOperationSwitch(Vector &v, Value &constant, parquet_filter_t &filter_mask, idx_t count) {
524: 	if (filter_mask.none() || count == 0) {
525: 		return;
526: 	}
527: 	switch (v.GetType().id()) {
528: 	case LogicalTypeId::BOOLEAN:
529: 		TemplatedFilterOperation<bool, OP>(v, constant.value_.boolean, filter_mask, count);
530: 		break;
531: 	case LogicalTypeId::UTINYINT:
532: 		TemplatedFilterOperation<uint8_t, OP>(v, constant.value_.utinyint, filter_mask, count);
533: 		break;
534: 	case LogicalTypeId::USMALLINT:
535: 		TemplatedFilterOperation<uint16_t, OP>(v, constant.value_.usmallint, filter_mask, count);
536: 		break;
537: 	case LogicalTypeId::UINTEGER:
538: 		TemplatedFilterOperation<uint32_t, OP>(v, constant.value_.uinteger, filter_mask, count);
539: 		break;
540: 	case LogicalTypeId::UBIGINT:
541: 		TemplatedFilterOperation<uint64_t, OP>(v, constant.value_.ubigint, filter_mask, count);
542: 		break;
543: 	case LogicalTypeId::TINYINT:
544: 		TemplatedFilterOperation<int8_t, OP>(v, constant.value_.tinyint, filter_mask, count);
545: 		break;
546: 	case LogicalTypeId::SMALLINT:
547: 		TemplatedFilterOperation<int16_t, OP>(v, constant.value_.smallint, filter_mask, count);
548: 		break;
549: 	case LogicalTypeId::INTEGER:
550: 		TemplatedFilterOperation<int32_t, OP>(v, constant.value_.integer, filter_mask, count);
551: 		break;
552: 	case LogicalTypeId::BIGINT:
553: 		TemplatedFilterOperation<int64_t, OP>(v, constant.value_.bigint, filter_mask, count);
554: 		break;
555: 	case LogicalTypeId::FLOAT:
556: 		TemplatedFilterOperation<float, OP>(v, constant.value_.float_, filter_mask, count);
557: 		break;
558: 	case LogicalTypeId::DOUBLE:
559: 		TemplatedFilterOperation<double, OP>(v, constant.value_.double_, filter_mask, count);
560: 		break;
561: 	case LogicalTypeId::DATE:
562: 		TemplatedFilterOperation<date_t, OP>(v, constant.value_.date, filter_mask, count);
563: 		break;
564: 	case LogicalTypeId::TIMESTAMP:
565: 		TemplatedFilterOperation<timestamp_t, OP>(v, constant.value_.timestamp, filter_mask, count);
566: 		break;
567: 	case LogicalTypeId::BLOB:
568: 	case LogicalTypeId::VARCHAR:
569: 		TemplatedFilterOperation<string_t, OP>(v, string_t(constant.str_value), filter_mask, count);
570: 		break;
571: 	case LogicalTypeId::DECIMAL:
572: 		switch (v.GetType().InternalType()) {
573: 		case PhysicalType::INT16:
574: 			TemplatedFilterOperation<int16_t, OP>(v, constant.value_.smallint, filter_mask, count);
575: 			break;
576: 		case PhysicalType::INT32:
577: 			TemplatedFilterOperation<int32_t, OP>(v, constant.value_.integer, filter_mask, count);
578: 			break;
579: 		case PhysicalType::INT64:
580: 			TemplatedFilterOperation<int64_t, OP>(v, constant.value_.bigint, filter_mask, count);
581: 			break;
582: 		case PhysicalType::INT128:
583: 			TemplatedFilterOperation<hugeint_t, OP>(v, constant.value_.hugeint, filter_mask, count);
584: 			break;
585: 		default:
586: 			throw InternalException("Unsupported internal type for decimal");
587: 		}
588: 		break;
589: 	default:
590: 		throw NotImplementedException("Unsupported type for filter %s", v.ToString());
591: 	}
592: }
593: 
594: static void ApplyFilter(Vector &v, TableFilter &filter, parquet_filter_t &filter_mask, idx_t count) {
595: 	switch (filter.filter_type) {
596: 	case TableFilterType::CONJUNCTION_AND: {
597: 		auto &conjunction = (ConjunctionAndFilter &)filter;
598: 		for (auto &child_filter : conjunction.child_filters) {
599: 			ApplyFilter(v, *child_filter, filter_mask, count);
600: 		}
601: 		break;
602: 	}
603: 	case TableFilterType::CONJUNCTION_OR: {
604: 		auto &conjunction = (ConjunctionOrFilter &)filter;
605: 		for (auto &child_filter : conjunction.child_filters) {
606: 			parquet_filter_t child_mask = filter_mask;
607: 			ApplyFilter(v, *child_filter, child_mask, count);
608: 			filter_mask |= child_mask;
609: 		}
610: 		break;
611: 	}
612: 	case TableFilterType::CONSTANT_COMPARISON: {
613: 		auto &constant_filter = (ConstantFilter &)filter;
614: 		switch (constant_filter.comparison_type) {
615: 		case ExpressionType::COMPARE_EQUAL:
616: 			FilterOperationSwitch<Equals>(v, constant_filter.constant, filter_mask, count);
617: 			break;
618: 		case ExpressionType::COMPARE_LESSTHAN:
619: 			FilterOperationSwitch<LessThan>(v, constant_filter.constant, filter_mask, count);
620: 			break;
621: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
622: 			FilterOperationSwitch<LessThanEquals>(v, constant_filter.constant, filter_mask, count);
623: 			break;
624: 		case ExpressionType::COMPARE_GREATERTHAN:
625: 			FilterOperationSwitch<GreaterThan>(v, constant_filter.constant, filter_mask, count);
626: 			break;
627: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
628: 			FilterOperationSwitch<GreaterThanEquals>(v, constant_filter.constant, filter_mask, count);
629: 			break;
630: 		default:
631: 			D_ASSERT(0);
632: 		}
633: 		break;
634: 	}
635: 	case TableFilterType::IS_NOT_NULL:
636: 		FilterIsNotNull(v, filter_mask, count);
637: 		break;
638: 	case TableFilterType::IS_NULL:
639: 		FilterIsNull(v, filter_mask, count);
640: 		break;
641: 	default:
642: 		D_ASSERT(0);
643: 		break;
644: 	}
645: }
646: 
647: void ParquetReader::Scan(ParquetReaderScanState &state, DataChunk &result) {
648: 	while (ScanInternal(state, result)) {
649: 		if (result.size() > 0) {
650: 			break;
651: 		}
652: 		result.Reset();
653: 	}
654: }
655: 
656: bool ParquetReader::ScanInternal(ParquetReaderScanState &state, DataChunk &result) {
657: 	if (state.finished) {
658: 		return false;
659: 	}
660: 
661: 	// see if we have to switch to the next row group in the parquet file
662: 	if (state.current_group < 0 || (int64_t)state.group_offset >= GetGroup(state).num_rows) {
663: 		state.current_group++;
664: 		state.group_offset = 0;
665: 
666: 		if ((idx_t)state.current_group == state.group_idx_list.size()) {
667: 			state.finished = true;
668: 			return false;
669: 		}
670: 
671: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
672: 			// this is a special case where we are not interested in the actual contents of the file
673: 			if (state.column_ids[out_col_idx] == COLUMN_IDENTIFIER_ROW_ID) {
674: 				continue;
675: 			}
676: 
677: 			PrepareRowGroupBuffer(state, out_col_idx);
678: 		}
679: 		return true;
680: 	}
681: 
682: 	auto this_output_chunk_rows = MinValue<idx_t>(STANDARD_VECTOR_SIZE, GetGroup(state).num_rows - state.group_offset);
683: 	result.SetCardinality(this_output_chunk_rows);
684: 
685: 	if (this_output_chunk_rows == 0) {
686: 		state.finished = true;
687: 		return false; // end of last group, we are done
688: 	}
689: 
690: 	// we evaluate simple table filters directly in this scan so we can skip decoding column data that's never going to
691: 	// be relevant
692: 	parquet_filter_t filter_mask;
693: 	filter_mask.set();
694: 
695: 	state.define_buf.zero();
696: 	state.repeat_buf.zero();
697: 
698: 	auto define_ptr = (uint8_t *)state.define_buf.ptr;
699: 	auto repeat_ptr = (uint8_t *)state.repeat_buf.ptr;
700: 
701: 	auto root_reader = ((StructColumnReader *)state.root_reader.get());
702: 
703: 	if (state.filters) {
704: 		vector<bool> need_to_read(result.ColumnCount(), true);
705: 
706: 		// first load the columns that are used in filters
707: 		for (auto &filter_col : state.filters->filters) {
708: 			auto file_col_idx = state.column_ids[filter_col.first];
709: 
710: 			if (filter_mask.none()) { // if no rows are left we can stop checking filters
711: 				break;
712: 			}
713: 
714: 			root_reader->GetChildReader(file_col_idx)
715: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[filter_col.first]);
716: 
717: 			need_to_read[filter_col.first] = false;
718: 
719: 			ApplyFilter(result.data[filter_col.first], *filter_col.second, filter_mask, this_output_chunk_rows);
720: 		}
721: 
722: 		// we still may have to read some cols
723: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
724: 			if (!need_to_read[out_col_idx]) {
725: 				continue;
726: 			}
727: 			auto file_col_idx = state.column_ids[out_col_idx];
728: 
729: 			if (filter_mask.none()) {
730: 				root_reader->GetChildReader(file_col_idx)->Skip(result.size());
731: 				continue;
732: 			}
733: 			// TODO handle ROWID here, too
734: 			root_reader->GetChildReader(file_col_idx)
735: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);
736: 		}
737: 
738: 		idx_t sel_size = 0;
739: 		for (idx_t i = 0; i < this_output_chunk_rows; i++) {
740: 			if (filter_mask[i]) {
741: 				state.sel.set_index(sel_size++, i);
742: 			}
743: 		}
744: 
745: 		result.Slice(state.sel, sel_size);
746: 		result.Verify();
747: 
748: 	} else { // #nofilter, just fricking load the data
749: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
750: 			auto file_col_idx = state.column_ids[out_col_idx];
751: 
752: 			if (file_col_idx == COLUMN_IDENTIFIER_ROW_ID) {
753: 				Value constant_42 = Value::BIGINT(42);
754: 				result.data[out_col_idx].Reference(constant_42);
755: 				continue;
756: 			}
757: 
758: 			root_reader->GetChildReader(file_col_idx)
759: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);
760: 		}
761: 	}
762: 
763: 	state.group_offset += this_output_chunk_rows;
764: 	return true;
765: }
766: 
767: } // namespace duckdb
[end of extension/parquet/parquet_reader.cpp]
[start of extension/parquet/parquet_writer.cpp]
1: #include "parquet_writer.hpp"
2: #include "parquet_timestamp.hpp"
3: 
4: #include "duckdb.hpp"
5: #ifndef DUCKDB_AMALGAMATION
6: #include "duckdb/function/table_function.hpp"
7: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
8: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
9: #include "duckdb/main/client_context.hpp"
10: #include "duckdb/main/connection.hpp"
11: #include "duckdb/common/file_system.hpp"
12: #include "duckdb/common/string_util.hpp"
13: #include "duckdb/common/types/date.hpp"
14: #include "duckdb/common/types/hugeint.hpp"
15: #include "duckdb/common/types/time.hpp"
16: #include "duckdb/common/types/timestamp.hpp"
17: #include "duckdb/common/serializer/buffered_file_writer.hpp"
18: #include "duckdb/common/serializer/buffered_serializer.hpp"
19: #endif
20: 
21: #include "snappy.h"
22: #include "miniz_wrapper.hpp"
23: #include "zstd.h"
24: 
25: namespace duckdb {
26: 
27: using namespace duckdb_parquet;                   // NOLINT
28: using namespace duckdb_apache::thrift;            // NOLINT
29: using namespace duckdb_apache::thrift::protocol;  // NOLINT
30: using namespace duckdb_apache::thrift::transport; // NOLINT
31: using namespace duckdb_miniz;                     // NOLINT
32: 
33: using duckdb_parquet::format::CompressionCodec;
34: using duckdb_parquet::format::ConvertedType;
35: using duckdb_parquet::format::Encoding;
36: using duckdb_parquet::format::FieldRepetitionType;
37: using duckdb_parquet::format::FileMetaData;
38: using duckdb_parquet::format::PageHeader;
39: using duckdb_parquet::format::PageType;
40: using ParquetRowGroup = duckdb_parquet::format::RowGroup;
41: using duckdb_parquet::format::Type;
42: 
43: class MyTransport : public TTransport {
44: public:
45: 	explicit MyTransport(Serializer &serializer) : serializer(serializer) {
46: 	}
47: 
48: 	bool isOpen() const override {
49: 		return true;
50: 	}
51: 
52: 	void open() override {
53: 	}
54: 
55: 	void close() override {
56: 	}
57: 
58: 	void write_virt(const uint8_t *buf, uint32_t len) override {
59: 		serializer.WriteData((const_data_ptr_t)buf, len);
60: 	}
61: 
62: private:
63: 	Serializer &serializer;
64: };
65: 
66: static Type::type DuckDBTypeToParquetType(const LogicalType &duckdb_type) {
67: 	switch (duckdb_type.id()) {
68: 	case LogicalTypeId::BOOLEAN:
69: 		return Type::BOOLEAN;
70: 	case LogicalTypeId::TINYINT:
71: 	case LogicalTypeId::SMALLINT:
72: 	case LogicalTypeId::INTEGER:
73: 	case LogicalTypeId::DATE:
74: 		return Type::INT32;
75: 	case LogicalTypeId::BIGINT:
76: 		return Type::INT64;
77: 	case LogicalTypeId::FLOAT:
78: 		return Type::FLOAT;
79: 	case LogicalTypeId::DECIMAL: // for now...
80: 	case LogicalTypeId::DOUBLE:
81: 	case LogicalTypeId::HUGEINT:
82: 		return Type::DOUBLE;
83: 	case LogicalTypeId::VARCHAR:
84: 	case LogicalTypeId::BLOB:
85: 		return Type::BYTE_ARRAY;
86: 	case LogicalTypeId::TIMESTAMP:
87: 	case LogicalTypeId::TIMESTAMP_MS:
88: 	case LogicalTypeId::TIMESTAMP_NS:
89: 	case LogicalTypeId::TIMESTAMP_SEC:
90: 		return Type::INT64;
91: 	case LogicalTypeId::UTINYINT:
92: 	case LogicalTypeId::USMALLINT:
93: 	case LogicalTypeId::UINTEGER:
94: 		return Type::INT32;
95: 	case LogicalTypeId::UBIGINT:
96: 		return Type::INT64;
97: 	default:
98: 		throw NotImplementedException(duckdb_type.ToString());
99: 	}
100: }
101: 
102: static bool DuckDBTypeToConvertedType(const LogicalType &duckdb_type, ConvertedType::type &result) {
103: 	switch (duckdb_type.id()) {
104: 	case LogicalTypeId::TINYINT:
105: 		result = ConvertedType::INT_8;
106: 		return true;
107: 	case LogicalTypeId::SMALLINT:
108: 		result = ConvertedType::INT_16;
109: 		return true;
110: 	case LogicalTypeId::INTEGER:
111: 		result = ConvertedType::INT_32;
112: 		return true;
113: 	case LogicalTypeId::BIGINT:
114: 		result = ConvertedType::INT_64;
115: 		return true;
116: 	case LogicalTypeId::UTINYINT:
117: 		result = ConvertedType::UINT_8;
118: 		return true;
119: 	case LogicalTypeId::USMALLINT:
120: 		result = ConvertedType::UINT_16;
121: 		return true;
122: 	case LogicalTypeId::UINTEGER:
123: 		result = ConvertedType::UINT_32;
124: 		return true;
125: 	case LogicalTypeId::UBIGINT:
126: 		result = ConvertedType::UINT_64;
127: 		return true;
128: 	case LogicalTypeId::DATE:
129: 		result = ConvertedType::DATE;
130: 		return true;
131: 	case LogicalTypeId::VARCHAR:
132: 		result = ConvertedType::UTF8;
133: 		return true;
134: 	case LogicalTypeId::TIMESTAMP:
135: 	case LogicalTypeId::TIMESTAMP_NS:
136: 	case LogicalTypeId::TIMESTAMP_SEC:
137: 		result = ConvertedType::TIMESTAMP_MICROS;
138: 		return true;
139: 	case LogicalTypeId::TIMESTAMP_MS:
140: 		result = ConvertedType::TIMESTAMP_MILLIS;
141: 		return true;
142: 	default:
143: 		return false;
144: 	}
145: }
146: 
147: static void VarintEncode(uint32_t val, Serializer &ser) {
148: 	do {
149: 		uint8_t byte = val & 127;
150: 		val >>= 7;
151: 		if (val != 0) {
152: 			byte |= 128;
153: 		}
154: 		ser.Write<uint8_t>(byte);
155: 	} while (val != 0);
156: }
157: 
158: static uint8_t GetVarintSize(uint32_t val) {
159: 	uint8_t res = 0;
160: 	do {
161: 		uint8_t byte = val & 127;
162: 		val >>= 7;
163: 		if (val != 0) {
164: 			byte |= 128;
165: 		}
166: 		res++;
167: 	} while (val != 0);
168: 	return res;
169: }
170: 
171: struct ParquetCastOperator {
172: 	template <class SRC, class TGT>
173: 	static TGT Operation(SRC input) {
174: 		return TGT(input);
175: 	}
176: };
177: 
178: struct ParquetTimestampNSOperator {
179: 	template <class SRC, class TGT>
180: 	static TGT Operation(SRC input) {
181: 		return Timestamp::FromEpochNanoSeconds(input).value;
182: 	}
183: };
184: 
185: struct ParquetTimestampSOperator {
186: 	template <class SRC, class TGT>
187: 	static TGT Operation(SRC input) {
188: 		return Timestamp::FromEpochSeconds(input).value;
189: 	}
190: };
191: 
192: struct ParquetHugeintOperator {
193: 	template <class SRC, class TGT>
194: 	static TGT Operation(SRC input) {
195: 		return Hugeint::Cast<double>(input);
196: 	}
197: };
198: 
199: template <class SRC, class TGT, class OP = ParquetCastOperator>
200: static void TemplatedWritePlain(Vector &col, idx_t length, ValidityMask &mask, Serializer &ser) {
201: 	auto *ptr = FlatVector::GetData<SRC>(col);
202: 	for (idx_t r = 0; r < length; r++) {
203: 		if (mask.RowIsValid(r)) {
204: 			ser.Write<TGT>(OP::template Operation<SRC, TGT>(ptr[r]));
205: 		}
206: 	}
207: }
208: 
209: ParquetWriter::ParquetWriter(FileSystem &fs, string file_name_p, FileOpener *file_opener_p, vector<LogicalType> types_p,
210:                              vector<string> names_p, CompressionCodec::type codec)
211:     : file_name(move(file_name_p)), sql_types(move(types_p)), column_names(move(names_p)), codec(codec) {
212: #if STANDARD_VECTOR_SIZE < 64
213: 	throw NotImplementedException("Parquet writer is not supported for vector sizes < 64");
214: #endif
215: 
216: 	// initialize the file writer
217: 	writer = make_unique<BufferedFileWriter>(
218: 	    fs, file_name.c_str(), FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE_NEW, file_opener_p);
219: 	// parquet files start with the string "PAR1"
220: 	writer->WriteData((const_data_ptr_t) "PAR1", 4);
221: 	TCompactProtocolFactoryT<MyTransport> tproto_factory;
222: 	protocol = tproto_factory.getProtocol(make_shared<MyTransport>(*writer));
223: 
224: 	file_meta_data.num_rows = 0;
225: 	file_meta_data.version = 1;
226: 
227: 	file_meta_data.__isset.created_by = true;
228: 	file_meta_data.created_by = "DuckDB";
229: 
230: 	file_meta_data.schema.resize(sql_types.size() + 1);
231: 
232: 	// populate root schema object
233: 	file_meta_data.schema[0].name = "duckdb_schema";
234: 	file_meta_data.schema[0].num_children = sql_types.size();
235: 	file_meta_data.schema[0].__isset.num_children = true;
236: 
237: 	for (idx_t i = 0; i < sql_types.size(); i++) {
238: 		auto &schema_element = file_meta_data.schema[i + 1];
239: 
240: 		schema_element.type = DuckDBTypeToParquetType(sql_types[i]);
241: 		schema_element.repetition_type = FieldRepetitionType::OPTIONAL;
242: 		schema_element.num_children = 0;
243: 		schema_element.__isset.num_children = true;
244: 		schema_element.__isset.type = true;
245: 		schema_element.__isset.repetition_type = true;
246: 		schema_element.name = column_names[i];
247: 		schema_element.__isset.converted_type = DuckDBTypeToConvertedType(sql_types[i], schema_element.converted_type);
248: 	}
249: }
250: 
251: void ParquetWriter::Flush(ChunkCollection &buffer) {
252: 	if (buffer.Count() == 0) {
253: 		return;
254: 	}
255: 	lock_guard<mutex> glock(lock);
256: 
257: 	// set up a new row group for this chunk collection
258: 	ParquetRowGroup row_group;
259: 	row_group.num_rows = 0;
260: 	row_group.file_offset = writer->GetTotalWritten();
261: 	row_group.__isset.file_offset = true;
262: 	row_group.columns.resize(buffer.ColumnCount());
263: 
264: 	// iterate over each of the columns of the chunk collection and write them
265: 	for (idx_t i = 0; i < buffer.ColumnCount(); i++) {
266: 		// we start off by writing everything into a temporary buffer
267: 		// this is necessary to (1) know the total written size, and (2) to compress it afterwards
268: 		BufferedSerializer temp_writer;
269: 
270: 		// set up some metadata
271: 		PageHeader hdr;
272: 		hdr.compressed_page_size = 0;
273: 		hdr.uncompressed_page_size = 0;
274: 		hdr.type = PageType::DATA_PAGE;
275: 		hdr.__isset.data_page_header = true;
276: 
277: 		hdr.data_page_header.num_values = buffer.Count();
278: 		hdr.data_page_header.encoding = Encoding::PLAIN;
279: 		hdr.data_page_header.definition_level_encoding = Encoding::RLE;
280: 		hdr.data_page_header.repetition_level_encoding = Encoding::BIT_PACKED;
281: 		// hdr.data_page_header.statistics.__isset.max
282: 		// hdr.data_page_header.statistics.max
283: 
284: 		// record the current offset of the writer into the file
285: 		// this is the starting position of the current page
286: 		auto start_offset = writer->GetTotalWritten();
287: 
288: 		// write the definition levels (i.e. the inverse of the nullmask)
289: 		// we always bit pack everything
290: 
291: 		// first figure out how many bytes we need (1 byte per 8 rows, rounded up)
292: 		auto define_byte_count = (buffer.Count() + 7) / 8;
293: 		// we need to set up the count as a varint, plus an added marker for the RLE scheme
294: 		// for this marker we shift the count left 1 and set low bit to 1 to indicate bit packed literals
295: 		uint32_t define_header = (define_byte_count << 1) | 1;
296: 		uint32_t define_size = GetVarintSize(define_header) + define_byte_count;
297: 
298: 		// we write the actual definitions into the temp_writer for now
299: 		temp_writer.Write<uint32_t>(define_size);
300: 		VarintEncode(define_header, temp_writer);
301: 
302: 		for (auto &chunk : buffer.Chunks()) {
303: 			auto &validity = FlatVector::Validity(chunk->data[i]);
304: 			auto validity_data = validity.GetData();
305: 			auto chunk_define_byte_count = (chunk->size() + 7) / 8;
306: 			if (!validity_data) {
307: 				ValidityMask nop_mask(chunk->size());
308: 				temp_writer.WriteData((const_data_ptr_t)nop_mask.GetData(), chunk_define_byte_count);
309: 			} else {
310: 				// write the bits of the nullmask
311: 				temp_writer.WriteData((const_data_ptr_t)validity_data, chunk_define_byte_count);
312: 			}
313: 		}
314: 
315: 		// now write the actual payload: we write this as PLAIN values (for now? possibly for ever?)
316: 		for (auto &chunk : buffer.Chunks()) {
317: 			auto &input = *chunk;
318: 			auto &input_column = input.data[i];
319: 			auto &mask = FlatVector::Validity(input_column);
320: 
321: 			// write actual payload data
322: 			switch (sql_types[i].id()) {
323: 			case LogicalTypeId::BOOLEAN: {
324: 				auto *ptr = FlatVector::GetData<bool>(input_column);
325: 				uint8_t byte = 0;
326: 				uint8_t byte_pos = 0;
327: 				for (idx_t r = 0; r < input.size(); r++) {
328: 					if (mask.RowIsValid(r)) { // only encode if non-null
329: 						byte |= (ptr[r] & 1) << byte_pos;
330: 						byte_pos++;
331: 
332: 						if (byte_pos == 8) {
333: 							temp_writer.Write<uint8_t>(byte);
334: 							byte = 0;
335: 							byte_pos = 0;
336: 						}
337: 					}
338: 				}
339: 				// flush last byte if req
340: 				if (byte_pos > 0) {
341: 					temp_writer.Write<uint8_t>(byte);
342: 				}
343: 				break;
344: 			}
345: 			case LogicalTypeId::TINYINT:
346: 				TemplatedWritePlain<int8_t, int32_t>(input_column, input.size(), mask, temp_writer);
347: 				break;
348: 			case LogicalTypeId::SMALLINT:
349: 				TemplatedWritePlain<int16_t, int32_t>(input_column, input.size(), mask, temp_writer);
350: 				break;
351: 			case LogicalTypeId::INTEGER:
352: 			case LogicalTypeId::DATE:
353: 				TemplatedWritePlain<int32_t, int32_t>(input_column, input.size(), mask, temp_writer);
354: 				break;
355: 			case LogicalTypeId::BIGINT:
356: 			case LogicalTypeId::TIMESTAMP:
357: 			case LogicalTypeId::TIMESTAMP_MS:
358: 				TemplatedWritePlain<int64_t, int64_t>(input_column, input.size(), mask, temp_writer);
359: 				break;
360: 			case LogicalTypeId::HUGEINT:
361: 				TemplatedWritePlain<hugeint_t, double, ParquetHugeintOperator>(input_column, input.size(), mask,
362: 				                                                               temp_writer);
363: 				break;
364: 			case LogicalTypeId::TIMESTAMP_NS:
365: 				TemplatedWritePlain<int64_t, int64_t, ParquetTimestampNSOperator>(input_column, input.size(), mask,
366: 				                                                                  temp_writer);
367: 				break;
368: 			case LogicalTypeId::TIMESTAMP_SEC:
369: 				TemplatedWritePlain<int64_t, int64_t, ParquetTimestampSOperator>(input_column, input.size(), mask,
370: 				                                                                 temp_writer);
371: 				break;
372: 			case LogicalTypeId::UTINYINT:
373: 				TemplatedWritePlain<uint8_t, int32_t>(input_column, input.size(), mask, temp_writer);
374: 				break;
375: 			case LogicalTypeId::USMALLINT:
376: 				TemplatedWritePlain<uint16_t, int32_t>(input_column, input.size(), mask, temp_writer);
377: 				break;
378: 			case LogicalTypeId::UINTEGER:
379: 				TemplatedWritePlain<uint32_t, uint32_t>(input_column, input.size(), mask, temp_writer);
380: 				break;
381: 			case LogicalTypeId::UBIGINT:
382: 				TemplatedWritePlain<uint64_t, uint64_t>(input_column, input.size(), mask, temp_writer);
383: 				break;
384: 			case LogicalTypeId::FLOAT:
385: 				TemplatedWritePlain<float, float>(input_column, input.size(), mask, temp_writer);
386: 				break;
387: 			case LogicalTypeId::DECIMAL: {
388: 				// FIXME: fixed length byte array...
389: 				Vector double_vec(LogicalType::DOUBLE);
390: 				VectorOperations::Cast(input_column, double_vec, input.size());
391: 				TemplatedWritePlain<double, double>(double_vec, input.size(), mask, temp_writer);
392: 				break;
393: 			}
394: 			case LogicalTypeId::DOUBLE:
395: 				TemplatedWritePlain<double, double>(input_column, input.size(), mask, temp_writer);
396: 				break;
397: 			case LogicalTypeId::BLOB:
398: 			case LogicalTypeId::VARCHAR: {
399: 				auto *ptr = FlatVector::GetData<string_t>(input_column);
400: 				for (idx_t r = 0; r < input.size(); r++) {
401: 					if (mask.RowIsValid(r)) {
402: 						temp_writer.Write<uint32_t>(ptr[r].GetSize());
403: 						temp_writer.WriteData((const_data_ptr_t)ptr[r].GetDataUnsafe(), ptr[r].GetSize());
404: 					}
405: 				}
406: 				break;
407: 			}
408: 			default:
409: 				throw NotImplementedException((sql_types[i].ToString()));
410: 			}
411: 		}
412: 
413: 		// now that we have finished writing the data we know the uncompressed size
414: 		hdr.uncompressed_page_size = temp_writer.blob.size;
415: 
416: 		// compress the data based
417: 		size_t compressed_size;
418: 		data_ptr_t compressed_data;
419: 		unique_ptr<data_t[]> compressed_buf;
420: 		switch (codec) {
421: 		case CompressionCodec::UNCOMPRESSED:
422: 			compressed_size = temp_writer.blob.size;
423: 			compressed_data = temp_writer.blob.data.get();
424: 			break;
425: 		case CompressionCodec::SNAPPY: {
426: 			compressed_size = duckdb_snappy::MaxCompressedLength(temp_writer.blob.size);
427: 			compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
428: 			duckdb_snappy::RawCompress((const char *)temp_writer.blob.data.get(), temp_writer.blob.size,
429: 			                           (char *)compressed_buf.get(), &compressed_size);
430: 			compressed_data = compressed_buf.get();
431: 			break;
432: 		}
433: 		case CompressionCodec::GZIP: {
434: 			MiniZStream s;
435: 			compressed_size = s.MaxCompressedLength(temp_writer.blob.size);
436: 			compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
437: 			s.Compress((const char *)temp_writer.blob.data.get(), temp_writer.blob.size, (char *)compressed_buf.get(),
438: 			           &compressed_size);
439: 			compressed_data = compressed_buf.get();
440: 			break;
441: 		}
442: 		case CompressionCodec::ZSTD: {
443: 			compressed_size = duckdb_zstd::ZSTD_compressBound(temp_writer.blob.size);
444: 			compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
445: 			compressed_size = duckdb_zstd::ZSTD_compress((void *)compressed_buf.get(), compressed_size,
446: 			                                             (const void *)temp_writer.blob.data.get(),
447: 			                                             temp_writer.blob.size, ZSTD_CLEVEL_DEFAULT);
448: 			compressed_data = compressed_buf.get();
449: 			break;
450: 		}
451: 		default:
452: 			throw InternalException("Unsupported codec for Parquet Writer");
453: 		}
454: 
455: 		hdr.compressed_page_size = compressed_size;
456: 		// now finally write the data to the actual file
457: 		hdr.write(protocol.get());
458: 		writer->WriteData(compressed_data, compressed_size);
459: 
460: 		auto &column_chunk = row_group.columns[i];
461: 		column_chunk.__isset.meta_data = true;
462: 		column_chunk.meta_data.data_page_offset = start_offset;
463: 		column_chunk.meta_data.total_compressed_size = writer->GetTotalWritten() - start_offset;
464: 		column_chunk.meta_data.codec = codec;
465: 		column_chunk.meta_data.path_in_schema.push_back(file_meta_data.schema[i + 1].name);
466: 		column_chunk.meta_data.num_values = buffer.Count();
467: 		column_chunk.meta_data.type = file_meta_data.schema[i + 1].type;
468: 	}
469: 	row_group.num_rows += buffer.Count();
470: 
471: 	// append the row group to the file meta data
472: 	file_meta_data.row_groups.push_back(row_group);
473: 	file_meta_data.num_rows += buffer.Count();
474: }
475: 
476: void ParquetWriter::Finalize() {
477: 	auto start_offset = writer->GetTotalWritten();
478: 	file_meta_data.write(protocol.get());
479: 
480: 	writer->Write<uint32_t>(writer->GetTotalWritten() - start_offset);
481: 
482: 	// parquet files also end with the string "PAR1"
483: 	writer->WriteData((const_data_ptr_t) "PAR1", 4);
484: 
485: 	// flush to disk
486: 	writer->Sync();
487: 	writer.reset();
488: }
489: 
490: } // namespace duckdb
[end of extension/parquet/parquet_writer.cpp]
[start of src/common/types/validity_mask.cpp]
1: #include "duckdb/common/types/validity_mask.hpp"
2: 
3: namespace duckdb {
4: 
5: ValidityData::ValidityData(idx_t count) : TemplatedValidityData(count) {
6: }
7: ValidityData::ValidityData(const ValidityMask &original, idx_t count)
8:     : TemplatedValidityData(original.GetData(), count) {
9: }
10: 
11: void ValidityMask::Combine(const ValidityMask &other, idx_t count) {
12: 	if (other.AllValid()) {
13: 		// X & 1 = X
14: 		return;
15: 	}
16: 	if (AllValid()) {
17: 		// 1 & Y = Y
18: 		Initialize(other);
19: 		return;
20: 	}
21: 	if (validity_mask == other.validity_mask) {
22: 		// X & X == X
23: 		return;
24: 	}
25: 	// have to merge
26: 	// create a new validity mask that contains the combined mask
27: 	auto owned_data = move(validity_data);
28: 	auto data = GetData();
29: 	auto other_data = other.GetData();
30: 
31: 	Initialize(count);
32: 	auto result_data = GetData();
33: 
34: 	auto entry_count = ValidityData::EntryCount(count);
35: 	for (idx_t entry_idx = 0; entry_idx < entry_count; entry_idx++) {
36: 		result_data[entry_idx] = data[entry_idx] & other_data[entry_idx];
37: 	}
38: }
39: 
40: // LCOV_EXCL_START
41: string ValidityMask::ToString(idx_t count) const {
42: 	string result = "Validity Mask (" + to_string(count) + ") [";
43: 	for (idx_t i = 0; i < count; i++) {
44: 		result += RowIsValid(i) ? "." : "X";
45: 	}
46: 	result += "]";
47: 	return result;
48: }
49: // LCOV_EXCL_STOP
50: 
51: void ValidityMask::Resize(idx_t old_size, idx_t new_size) {
52: 	if (validity_mask) {
53: 		auto new_size_count = EntryCount(new_size);
54: 		auto old_size_count = EntryCount(old_size);
55: 		auto new_owned_data = unique_ptr<validity_t[]>(new validity_t[new_size_count]);
56: 		for (idx_t entry_idx = 0; entry_idx < old_size_count; entry_idx++) {
57: 			new_owned_data[entry_idx] = validity_mask[entry_idx];
58: 		}
59: 		for (idx_t entry_idx = old_size_count; entry_idx < new_size_count; entry_idx++) {
60: 			new_owned_data[entry_idx] = ValidityData::MAX_ENTRY;
61: 		}
62: 		validity_data->owned_data = move(new_owned_data);
63: 		validity_mask = validity_data->owned_data.get();
64: 	} else {
65: 		Initialize(new_size);
66: 	}
67: }
68: 
69: void ValidityMask::Slice(const ValidityMask &other, idx_t offset) {
70: 	if (other.AllValid()) {
71: 		validity_mask = nullptr;
72: 		validity_data.reset();
73: 		return;
74: 	}
75: 	if (offset == 0) {
76: 		Initialize(other);
77: 		return;
78: 	}
79: 	Initialize(STANDARD_VECTOR_SIZE);
80: 
81: // FIXME THIS NEEDS FIXING!
82: #if 1
83: 	for (idx_t i = offset; i < STANDARD_VECTOR_SIZE; i++) {
84: 		Set(i - offset, other.RowIsValid(i));
85: 	}
86: #else
87: 	// first shift the "whole" units
88: 	idx_t entire_units = offset / BITS_PER_VALUE;
89: 	idx_t sub_units = offset - entire_units * BITS_PER_VALUE;
90: 	if (entire_units > 0) {
91: 		idx_t validity_idx;
92: 		for (validity_idx = 0; validity_idx + entire_units < STANDARD_ENTRY_COUNT; validity_idx++) {
93: 			validity_mask[validity_idx] = other.validity_mask[validity_idx + entire_units];
94: 		}
95: 	}
96: 	// now we shift the remaining sub units
97: 	// this gets a bit more complicated because we have to shift over the borders of the entries
98: 	// e.g. suppose we have 2 entries of length 4 and we left-shift by two
99: 	// 0101|1010
100: 	// a regular left-shift of both gets us:
101: 	// 0100|1000
102: 	// we then OR the overflow (right-shifted by BITS_PER_VALUE - offset) together to get the correct result
103: 	// 0100|1000 ->
104: 	// 0110|1000
105: 	if (sub_units > 0) {
106: 		idx_t validity_idx;
107: 		for (validity_idx = 0; validity_idx + 1 < STANDARD_ENTRY_COUNT; validity_idx++) {
108: 			validity_mask[validity_idx] = (other.validity_mask[validity_idx] >> sub_units) |
109: 			                              (other.validity_mask[validity_idx + 1] << (BITS_PER_VALUE - sub_units));
110: 		}
111: 		validity_mask[validity_idx] >>= sub_units;
112: 	}
113: #ifdef DEBUG
114: 	for (idx_t i = offset; i < STANDARD_VECTOR_SIZE; i++) {
115: 		D_ASSERT(RowIsValid(i - offset) == other.RowIsValid(i));
116: 	}
117: #endif
118: #endif
119: }
120: 
121: } // namespace duckdb
[end of src/common/types/validity_mask.cpp]
[start of third_party/fastpforlib/bitpackinghelpers.h]
1: /**
2: * This code is released under the
3: * Apache License Version 2.0 http://www.apache.org/licenses/.
4: *
5: * (c) Daniel Lemire, http://lemire.me/en/
6: */
7: #pragma once
8: #include "bitpacking.h"
9: 
10: #include <stdexcept>
11: 
12: namespace duckdb_fastpforlib {
13: 
14: namespace internal {
15: 
16: // Note that this only packs 8 values
17: inline void fastunpack_quarter(const uint8_t *__restrict in, uint8_t *__restrict out, const uint32_t bit) {
18: 	// Could have used function pointers instead of switch.
19: 	// Switch calls do offer the compiler more opportunities for optimization in
20: 	// theory. In this case, it makes no difference with a good compiler.
21: 	switch (bit) {
22: 	case 0:
23: 		internal::__fastunpack0(in, out);
24: 		break;
25: 	case 1:
26: 		internal::__fastunpack1(in, out);
27: 		break;
28: 	case 2:
29: 		internal::__fastunpack2(in, out);
30: 		break;
31: 	case 3:
32: 		internal::__fastunpack3(in, out);
33: 		break;
34: 	case 4:
35: 		internal::__fastunpack4(in, out);
36: 		break;
37: 	case 5:
38: 		internal::__fastunpack5(in, out);
39: 		break;
40: 	case 6:
41: 		internal::__fastunpack6(in, out);
42: 		break;
43: 	case 7:
44: 		internal::__fastunpack7(in, out);
45: 		break;
46: 	case 8:
47: 		internal::__fastunpack8(in, out);
48: 		break;
49: 	default:
50: 		throw std::logic_error("Invalid bit width for bitpacking");
51: 	}
52: }
53: 
54: // Note that this only packs 8 values
55: inline void fastpack_quarter(const uint8_t *__restrict in, uint8_t *__restrict out, const uint32_t bit) {
56: 	// Could have used function pointers instead of switch.
57: 	// Switch calls do offer the compiler more opportunities for optimization in
58: 	// theory. In this case, it makes no difference with a good compiler.
59: 	switch (bit) {
60: 	case 0:
61: 		internal::__fastpack0(in, out);
62: 		break;
63: 	case 1:
64: 		internal::__fastpack1(in, out);
65: 		break;
66: 	case 2:
67: 		internal::__fastpack2(in, out);
68: 		break;
69: 	case 3:
70: 		internal::__fastpack3(in, out);
71: 		break;
72: 	case 4:
73: 		internal::__fastpack4(in, out);
74: 		break;
75: 	case 5:
76: 		internal::__fastpack5(in, out);
77: 		break;
78: 	case 6:
79: 		internal::__fastpack6(in, out);
80: 		break;
81: 	case 7:
82: 		internal::__fastpack7(in, out);
83: 		break;
84: 	case 8:
85: 		internal::__fastpack8(in, out);
86: 		break;
87: 	default:
88: 		throw std::logic_error("Invalid bit width for bitpacking");
89: 	}
90: }
91: 
92: // Note that this only packs 16 values
93: inline void fastunpack_half(const uint16_t *__restrict in, uint16_t *__restrict out, const uint32_t bit) {
94: 	// Could have used function pointers instead of switch.
95: 	// Switch calls do offer the compiler more opportunities for optimization in
96: 	// theory. In this case, it makes no difference with a good compiler.
97: 	switch (bit) {
98: 	case 0:
99: 		internal::__fastunpack0(in, out);
100: 		break;
101: 	case 1:
102: 		internal::__fastunpack1(in, out);
103: 		break;
104: 	case 2:
105: 		internal::__fastunpack2(in, out);
106: 		break;
107: 	case 3:
108: 		internal::__fastunpack3(in, out);
109: 		break;
110: 	case 4:
111: 		internal::__fastunpack4(in, out);
112: 		break;
113: 	case 5:
114: 		internal::__fastunpack5(in, out);
115: 		break;
116: 	case 6:
117: 		internal::__fastunpack6(in, out);
118: 		break;
119: 	case 7:
120: 		internal::__fastunpack7(in, out);
121: 		break;
122: 	case 8:
123: 		internal::__fastunpack8(in, out);
124: 		break;
125: 	case 9:
126: 		internal::__fastunpack9(in, out);
127: 		break;
128: 	case 10:
129: 		internal::__fastunpack10(in, out);
130: 		break;
131: 	case 11:
132: 		internal::__fastunpack11(in, out);
133: 		break;
134: 	case 12:
135: 		internal::__fastunpack12(in, out);
136: 		break;
137: 	case 13:
138: 		internal::__fastunpack13(in, out);
139: 		break;
140: 	case 14:
141: 		internal::__fastunpack14(in, out);
142: 		break;
143: 	case 15:
144: 		internal::__fastunpack15(in, out);
145: 		break;
146: 	case 16:
147: 		internal::__fastunpack16(in, out);
148: 		break;
149: 	default:
150: 		throw std::logic_error("Invalid bit width for bitpacking");
151: 	}
152: }
153: 
154: // Note that this only packs 16 values
155: inline void fastpack_half(const uint16_t *__restrict in, uint16_t *__restrict out, const uint32_t bit) {
156: 	// Could have used function pointers instead of switch.
157: 	// Switch calls do offer the compiler more opportunities for optimization in
158: 	// theory. In this case, it makes no difference with a good compiler.
159: 	switch (bit) {
160: 	case 0:
161: 		internal::__fastpack0(in, out);
162: 		break;
163: 	case 1:
164: 		internal::__fastpack1(in, out);
165: 		break;
166: 	case 2:
167: 		internal::__fastpack2(in, out);
168: 		break;
169: 	case 3:
170: 		internal::__fastpack3(in, out);
171: 		break;
172: 	case 4:
173: 		internal::__fastpack4(in, out);
174: 		break;
175: 	case 5:
176: 		internal::__fastpack5(in, out);
177: 		break;
178: 	case 6:
179: 		internal::__fastpack6(in, out);
180: 		break;
181: 	case 7:
182: 		internal::__fastpack7(in, out);
183: 		break;
184: 	case 8:
185: 		internal::__fastpack8(in, out);
186: 		break;
187: 	case 9:
188: 		internal::__fastpack9(in, out);
189: 		break;
190: 	case 10:
191: 		internal::__fastpack10(in, out);
192: 		break;
193: 	case 11:
194: 		internal::__fastpack11(in, out);
195: 		break;
196: 	case 12:
197: 		internal::__fastpack12(in, out);
198: 		break;
199: 	case 13:
200: 		internal::__fastpack13(in, out);
201: 		break;
202: 	case 14:
203: 		internal::__fastpack14(in, out);
204: 		break;
205: 	case 15:
206: 		internal::__fastpack15(in, out);
207: 		break;
208: 	case 16:
209: 		internal::__fastpack16(in, out);
210: 		break;
211: 	default:
212: 		throw std::logic_error("Invalid bit width for bitpacking");
213: 	}
214: }
215: }
216: 
217: inline void fastunpack(const uint8_t *__restrict in, uint8_t *__restrict out, const uint32_t bit) {
218: 	for (uint8_t i = 0; i < 4; i++) {
219: 		internal::fastunpack_quarter(in + (i*bit), out+(i*8), bit);
220: 	}
221: }
222: 
223: inline void fastunpack(const uint16_t *__restrict in, uint16_t *__restrict out, const uint32_t bit) {
224: 	internal::fastunpack_half(in, out, bit);
225: 	internal::fastunpack_half(in + bit, out+16, bit);
226: }
227: 
228: inline void fastunpack(const uint32_t *__restrict in,
229:                        uint32_t *__restrict out, const uint32_t bit) {
230:   // Could have used function pointers instead of switch.
231:   // Switch calls do offer the compiler more opportunities for optimization in
232:   // theory. In this case, it makes no difference with a good compiler.
233:   switch (bit) {
234:   case 0:
235:     internal::__fastunpack0(in, out);
236:     break;
237:   case 1:
238:     internal::__fastunpack1(in, out);
239:     break;
240:   case 2:
241:     internal::__fastunpack2(in, out);
242:     break;
243:   case 3:
244:     internal::__fastunpack3(in, out);
245:     break;
246:   case 4:
247:     internal::__fastunpack4(in, out);
248:     break;
249:   case 5:
250:     internal::__fastunpack5(in, out);
251:     break;
252:   case 6:
253:     internal::__fastunpack6(in, out);
254:     break;
255:   case 7:
256:     internal::__fastunpack7(in, out);
257:     break;
258:   case 8:
259:     internal::__fastunpack8(in, out);
260:     break;
261:   case 9:
262:     internal::__fastunpack9(in, out);
263:     break;
264:   case 10:
265:     internal::__fastunpack10(in, out);
266:     break;
267:   case 11:
268:     internal::__fastunpack11(in, out);
269:     break;
270:   case 12:
271:     internal::__fastunpack12(in, out);
272:     break;
273:   case 13:
274:     internal::__fastunpack13(in, out);
275:     break;
276:   case 14:
277:     internal::__fastunpack14(in, out);
278:     break;
279:   case 15:
280:     internal::__fastunpack15(in, out);
281:     break;
282:   case 16:
283:     internal::__fastunpack16(in, out);
284:     break;
285:   case 17:
286:     internal::__fastunpack17(in, out);
287:     break;
288:   case 18:
289:     internal::__fastunpack18(in, out);
290:     break;
291:   case 19:
292:     internal::__fastunpack19(in, out);
293:     break;
294:   case 20:
295:     internal::__fastunpack20(in, out);
296:     break;
297:   case 21:
298:     internal::__fastunpack21(in, out);
299:     break;
300:   case 22:
301:     internal::__fastunpack22(in, out);
302:     break;
303:   case 23:
304:     internal::__fastunpack23(in, out);
305:     break;
306:   case 24:
307:     internal::__fastunpack24(in, out);
308:     break;
309:   case 25:
310:     internal::__fastunpack25(in, out);
311:     break;
312:   case 26:
313:     internal::__fastunpack26(in, out);
314:     break;
315:   case 27:
316:     internal::__fastunpack27(in, out);
317:     break;
318:   case 28:
319:     internal::__fastunpack28(in, out);
320:     break;
321:   case 29:
322:     internal::__fastunpack29(in, out);
323:     break;
324:   case 30:
325:     internal::__fastunpack30(in, out);
326:     break;
327:   case 31:
328:     internal::__fastunpack31(in, out);
329:     break;
330:   case 32:
331:     internal::__fastunpack32(in, out);
332:     break;
333:   default:
334:     throw std::logic_error("Invalid bit width for bitpacking");
335:   }
336: }
337: 
338: inline void fastunpack(const uint32_t *__restrict in,
339:                        uint64_t *__restrict out, const uint32_t bit) {
340:   // Could have used function pointers instead of switch.
341:   // Switch calls do offer the compiler more opportunities for optimization in
342:   // theory. In this case, it makes no difference with a good compiler.
343:   switch (bit) {
344:   case 0:
345:     internal::__fastunpack0(in, out);
346:     break;
347:   case 1:
348:     internal::__fastunpack1(in, out);
349:     break;
350:   case 2:
351:     internal::__fastunpack2(in, out);
352:     break;
353:   case 3:
354:     internal::__fastunpack3(in, out);
355:     break;
356:   case 4:
357:     internal::__fastunpack4(in, out);
358:     break;
359:   case 5:
360:     internal::__fastunpack5(in, out);
361:     break;
362:   case 6:
363:     internal::__fastunpack6(in, out);
364:     break;
365:   case 7:
366:     internal::__fastunpack7(in, out);
367:     break;
368:   case 8:
369:     internal::__fastunpack8(in, out);
370:     break;
371:   case 9:
372:     internal::__fastunpack9(in, out);
373:     break;
374:   case 10:
375:     internal::__fastunpack10(in, out);
376:     break;
377:   case 11:
378:     internal::__fastunpack11(in, out);
379:     break;
380:   case 12:
381:     internal::__fastunpack12(in, out);
382:     break;
383:   case 13:
384:     internal::__fastunpack13(in, out);
385:     break;
386:   case 14:
387:     internal::__fastunpack14(in, out);
388:     break;
389:   case 15:
390:     internal::__fastunpack15(in, out);
391:     break;
392:   case 16:
393:     internal::__fastunpack16(in, out);
394:     break;
395:   case 17:
396:     internal::__fastunpack17(in, out);
397:     break;
398:   case 18:
399:     internal::__fastunpack18(in, out);
400:     break;
401:   case 19:
402:     internal::__fastunpack19(in, out);
403:     break;
404:   case 20:
405:     internal::__fastunpack20(in, out);
406:     break;
407:   case 21:
408:     internal::__fastunpack21(in, out);
409:     break;
410:   case 22:
411:     internal::__fastunpack22(in, out);
412:     break;
413:   case 23:
414:     internal::__fastunpack23(in, out);
415:     break;
416:   case 24:
417:     internal::__fastunpack24(in, out);
418:     break;
419:   case 25:
420:     internal::__fastunpack25(in, out);
421:     break;
422:   case 26:
423:     internal::__fastunpack26(in, out);
424:     break;
425:   case 27:
426:     internal::__fastunpack27(in, out);
427:     break;
428:   case 28:
429:     internal::__fastunpack28(in, out);
430:     break;
431:   case 29:
432:     internal::__fastunpack29(in, out);
433:     break;
434:   case 30:
435:     internal::__fastunpack30(in, out);
436:     break;
437:   case 31:
438:     internal::__fastunpack31(in, out);
439:     break;
440:   case 32:
441:     internal::__fastunpack32(in, out);
442:     break;
443:   case 33:
444:     internal::__fastunpack33(in, out);
445:     break;
446:   case 34:
447:     internal::__fastunpack34(in, out);
448:     break;
449:   case 35:
450:     internal::__fastunpack35(in, out);
451:     break;
452:   case 36:
453:     internal::__fastunpack36(in, out);
454:     break;
455:   case 37:
456:     internal::__fastunpack37(in, out);
457:     break;
458:   case 38:
459:     internal::__fastunpack38(in, out);
460:     break;
461:   case 39:
462:     internal::__fastunpack39(in, out);
463:     break;
464:   case 40:
465:     internal::__fastunpack40(in, out);
466:     break;
467:   case 41:
468:     internal::__fastunpack41(in, out);
469:     break;
470:   case 42:
471:     internal::__fastunpack42(in, out);
472:     break;
473:   case 43:
474:     internal::__fastunpack43(in, out);
475:     break;
476:   case 44:
477:     internal::__fastunpack44(in, out);
478:     break;
479:   case 45:
480:     internal::__fastunpack45(in, out);
481:     break;
482:   case 46:
483:     internal::__fastunpack46(in, out);
484:     break;
485:   case 47:
486:     internal::__fastunpack47(in, out);
487:     break;
488:   case 48:
489:     internal::__fastunpack48(in, out);
490:     break;
491:   case 49:
492:     internal::__fastunpack49(in, out);
493:     break;
494:   case 50:
495:     internal::__fastunpack50(in, out);
496:     break;
497:   case 51:
498:     internal::__fastunpack51(in, out);
499:     break;
500:   case 52:
501:     internal::__fastunpack52(in, out);
502:     break;
503:   case 53:
504:     internal::__fastunpack53(in, out);
505:     break;
506:   case 54:
507:     internal::__fastunpack54(in, out);
508:     break;
509:   case 55:
510:     internal::__fastunpack55(in, out);
511:     break;
512:   case 56:
513:     internal::__fastunpack56(in, out);
514:     break;
515:   case 57:
516:     internal::__fastunpack57(in, out);
517:     break;
518:   case 58:
519:     internal::__fastunpack58(in, out);
520:     break;
521:   case 59:
522:     internal::__fastunpack59(in, out);
523:     break;
524:   case 60:
525:     internal::__fastunpack60(in, out);
526:     break;
527:   case 61:
528:     internal::__fastunpack61(in, out);
529:     break;
530:   case 62:
531:     internal::__fastunpack62(in, out);
532:     break;
533:   case 63:
534:     internal::__fastunpack63(in, out);
535:     break;
536:   case 64:
537:     internal::__fastunpack64(in, out);
538:     break;
539:   default:
540: 	throw std::logic_error("Invalid bit width for bitpacking");
541:   }
542: }
543: 
544: inline void fastpack(const uint8_t *__restrict in, uint8_t *__restrict out, const uint32_t bit) {
545: 
546: 	for (uint8_t i = 0; i < 4; i++) {
547: 		internal::fastpack_quarter(in+(i*8), out + (i*bit), bit);
548: 	}
549: }
550: 
551: inline void fastpack(const uint16_t *__restrict in, uint16_t *__restrict out, const uint32_t bit) {
552: 	internal::fastpack_half(in, out, bit);
553: 	internal::fastpack_half(in+16, out + bit, bit);
554: }
555: 
556: inline void fastpack(const uint32_t *__restrict in,
557:                      uint32_t *__restrict out, const uint32_t bit) {
558:   // Could have used function pointers instead of switch.
559:   // Switch calls do offer the compiler more opportunities for optimization in
560:   // theory. In this case, it makes no difference with a good compiler.
561:   switch (bit) {
562:   case 0:
563:     internal::__fastpack0(in, out);
564:     break;
565:   case 1:
566:     internal::__fastpack1(in, out);
567:     break;
568:   case 2:
569:     internal::__fastpack2(in, out);
570:     break;
571:   case 3:
572:     internal::__fastpack3(in, out);
573:     break;
574:   case 4:
575:     internal::__fastpack4(in, out);
576:     break;
577:   case 5:
578:     internal::__fastpack5(in, out);
579:     break;
580:   case 6:
581:     internal::__fastpack6(in, out);
582:     break;
583:   case 7:
584:     internal::__fastpack7(in, out);
585:     break;
586:   case 8:
587:     internal::__fastpack8(in, out);
588:     break;
589:   case 9:
590:     internal::__fastpack9(in, out);
591:     break;
592:   case 10:
593:     internal::__fastpack10(in, out);
594:     break;
595:   case 11:
596:     internal::__fastpack11(in, out);
597:     break;
598:   case 12:
599:     internal::__fastpack12(in, out);
600:     break;
601:   case 13:
602:     internal::__fastpack13(in, out);
603:     break;
604:   case 14:
605:     internal::__fastpack14(in, out);
606:     break;
607:   case 15:
608:     internal::__fastpack15(in, out);
609:     break;
610:   case 16:
611:     internal::__fastpack16(in, out);
612:     break;
613:   case 17:
614:     internal::__fastpack17(in, out);
615:     break;
616:   case 18:
617:     internal::__fastpack18(in, out);
618:     break;
619:   case 19:
620:     internal::__fastpack19(in, out);
621:     break;
622:   case 20:
623:     internal::__fastpack20(in, out);
624:     break;
625:   case 21:
626:     internal::__fastpack21(in, out);
627:     break;
628:   case 22:
629:     internal::__fastpack22(in, out);
630:     break;
631:   case 23:
632:     internal::__fastpack23(in, out);
633:     break;
634:   case 24:
635:     internal::__fastpack24(in, out);
636:     break;
637:   case 25:
638:     internal::__fastpack25(in, out);
639:     break;
640:   case 26:
641:     internal::__fastpack26(in, out);
642:     break;
643:   case 27:
644:     internal::__fastpack27(in, out);
645:     break;
646:   case 28:
647:     internal::__fastpack28(in, out);
648:     break;
649:   case 29:
650:     internal::__fastpack29(in, out);
651:     break;
652:   case 30:
653:     internal::__fastpack30(in, out);
654:     break;
655:   case 31:
656:     internal::__fastpack31(in, out);
657:     break;
658:   case 32:
659:     internal::__fastpack32(in, out);
660:     break;
661:   default:
662: 	throw std::logic_error("Invalid bit width for bitpacking");
663:   }
664: }
665: 
666: inline void fastpack(const uint64_t *__restrict in,
667:                      uint32_t *__restrict out, const uint32_t bit) {
668:   switch (bit) {
669:   case 0:
670:     internal::__fastpack0(in, out);
671:     break;
672:   case 1:
673:     internal::__fastpack1(in, out);
674:     break;
675:   case 2:
676:     internal::__fastpack2(in, out);
677:     break;
678:   case 3:
679:     internal::__fastpack3(in, out);
680:     break;
681:   case 4:
682:     internal::__fastpack4(in, out);
683:     break;
684:   case 5:
685:     internal::__fastpack5(in, out);
686:     break;
687:   case 6:
688:     internal::__fastpack6(in, out);
689:     break;
690:   case 7:
691:     internal::__fastpack7(in, out);
692:     break;
693:   case 8:
694:     internal::__fastpack8(in, out);
695:     break;
696:   case 9:
697:     internal::__fastpack9(in, out);
698:     break;
699:   case 10:
700:     internal::__fastpack10(in, out);
701:     break;
702:   case 11:
703:     internal::__fastpack11(in, out);
704:     break;
705:   case 12:
706:     internal::__fastpack12(in, out);
707:     break;
708:   case 13:
709:     internal::__fastpack13(in, out);
710:     break;
711:   case 14:
712:     internal::__fastpack14(in, out);
713:     break;
714:   case 15:
715:     internal::__fastpack15(in, out);
716:     break;
717:   case 16:
718:     internal::__fastpack16(in, out);
719:     break;
720:   case 17:
721:     internal::__fastpack17(in, out);
722:     break;
723:   case 18:
724:     internal::__fastpack18(in, out);
725:     break;
726:   case 19:
727:     internal::__fastpack19(in, out);
728:     break;
729:   case 20:
730:     internal::__fastpack20(in, out);
731:     break;
732:   case 21:
733:     internal::__fastpack21(in, out);
734:     break;
735:   case 22:
736:     internal::__fastpack22(in, out);
737:     break;
738:   case 23:
739:     internal::__fastpack23(in, out);
740:     break;
741:   case 24:
742:     internal::__fastpack24(in, out);
743:     break;
744:   case 25:
745:     internal::__fastpack25(in, out);
746:     break;
747:   case 26:
748:     internal::__fastpack26(in, out);
749:     break;
750:   case 27:
751:     internal::__fastpack27(in, out);
752:     break;
753:   case 28:
754:     internal::__fastpack28(in, out);
755:     break;
756:   case 29:
757:     internal::__fastpack29(in, out);
758:     break;
759:   case 30:
760:     internal::__fastpack30(in, out);
761:     break;
762:   case 31:
763:     internal::__fastpack31(in, out);
764:     break;
765:   case 32:
766:     internal::__fastpack32(in, out);
767:     break;
768:   case 33:
769:     internal::__fastpack33(in, out);
770:     break;
771:   case 34:
772:     internal::__fastpack34(in, out);
773:     break;
774:   case 35:
775:     internal::__fastpack35(in, out);
776:     break;
777:   case 36:
778:     internal::__fastpack36(in, out);
779:     break;
780:   case 37:
781:     internal::__fastpack37(in, out);
782:     break;
783:   case 38:
784:     internal::__fastpack38(in, out);
785:     break;
786:   case 39:
787:     internal::__fastpack39(in, out);
788:     break;
789:   case 40:
790:     internal::__fastpack40(in, out);
791:     break;
792:   case 41:
793:     internal::__fastpack41(in, out);
794:     break;
795:   case 42:
796:     internal::__fastpack42(in, out);
797:     break;
798:   case 43:
799:     internal::__fastpack43(in, out);
800:     break;
801:   case 44:
802:     internal::__fastpack44(in, out);
803:     break;
804:   case 45:
805:     internal::__fastpack45(in, out);
806:     break;
807:   case 46:
808:     internal::__fastpack46(in, out);
809:     break;
810:   case 47:
811:     internal::__fastpack47(in, out);
812:     break;
813:   case 48:
814:     internal::__fastpack48(in, out);
815:     break;
816:   case 49:
817:     internal::__fastpack49(in, out);
818:     break;
819:   case 50:
820:     internal::__fastpack50(in, out);
821:     break;
822:   case 51:
823:     internal::__fastpack51(in, out);
824:     break;
825:   case 52:
826:     internal::__fastpack52(in, out);
827:     break;
828:   case 53:
829:     internal::__fastpack53(in, out);
830:     break;
831:   case 54:
832:     internal::__fastpack54(in, out);
833:     break;
834:   case 55:
835:     internal::__fastpack55(in, out);
836:     break;
837:   case 56:
838:     internal::__fastpack56(in, out);
839:     break;
840:   case 57:
841:     internal::__fastpack57(in, out);
842:     break;
843:   case 58:
844:     internal::__fastpack58(in, out);
845:     break;
846:   case 59:
847:     internal::__fastpack59(in, out);
848:     break;
849:   case 60:
850:     internal::__fastpack60(in, out);
851:     break;
852:   case 61:
853:     internal::__fastpack61(in, out);
854:     break;
855:   case 62:
856:     internal::__fastpack62(in, out);
857:     break;
858:   case 63:
859:     internal::__fastpack63(in, out);
860:     break;
861:   case 64:
862:     internal::__fastpack64(in, out);
863:     break;
864:   default:
865: 	throw std::logic_error("Invalid bit width for bitpacking");
866:   }
867: }
868: } // namespace fastpfor_lib
[end of third_party/fastpforlib/bitpackinghelpers.h]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: