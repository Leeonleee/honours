{
  "repo": "duckdb/duckdb",
  "pull_number": 2832,
  "instance_id": "duckdb__duckdb-2832",
  "issue_numbers": [
    "2557",
    "2815"
  ],
  "base_commit": "2bb22136150176a56f6c45ad21c40d45e52b810a",
  "patch": "diff --git a/data/csv/sequences.csv.gz b/data/csv/sequences.csv.gz\nnew file mode 100644\nindex 000000000000..c5696e0e47c7\nBinary files /dev/null and b/data/csv/sequences.csv.gz differ\ndiff --git a/extension/parquet/CMakeLists.txt b/extension/parquet/CMakeLists.txt\nindex da2f57fee15a..44f14944f586 100644\n--- a/extension/parquet/CMakeLists.txt\n+++ b/extension/parquet/CMakeLists.txt\n@@ -8,6 +8,7 @@ include_directories(\n   ../../third_party/zstd/include)\n \n set(PARQUET_EXTENSION_FILES\n+    column_writer.cpp\n     parquet-extension.cpp\n     parquet_metadata.cpp\n     parquet_reader.cpp\ndiff --git a/extension/parquet/column_reader.cpp b/extension/parquet/column_reader.cpp\nindex 076d22115223..667932f957c1 100644\n--- a/extension/parquet/column_reader.cpp\n+++ b/extension/parquet/column_reader.cpp\n@@ -244,17 +244,6 @@ void ColumnReader::PreparePage(idx_t compressed_page_size, idx_t uncompressed_pa\n \t}\n }\n \n-static uint8_t ComputeBitWidth(idx_t val) {\n-\tif (val == 0) {\n-\t\treturn 0;\n-\t}\n-\tuint8_t ret = 1;\n-\twhile (((idx_t)(1 << ret) - 1) < val) {\n-\t\tret++;\n-\t}\n-\treturn ret;\n-}\n-\n void ColumnReader::PrepareDataPage(PageHeader &page_hdr) {\n \tif (page_hdr.type == PageType::DATA_PAGE && !page_hdr.__isset.data_page_header) {\n \t\tthrow std::runtime_error(\"Missing data page header from data page\");\n@@ -273,8 +262,8 @@ void ColumnReader::PrepareDataPage(PageHeader &page_hdr) {\n \t\t                          ? block->read<uint32_t>()\n \t\t                          : page_hdr.data_page_header_v2.repetition_levels_byte_length;\n \t\tblock->available(rep_length);\n-\t\trepeated_decoder =\n-\t\t    make_unique<RleBpDecoder>((const uint8_t *)block->ptr, rep_length, ComputeBitWidth(max_repeat));\n+\t\trepeated_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, rep_length,\n+\t\t                                             RleBpDecoder::ComputeBitWidth(max_repeat));\n \t\tblock->inc(rep_length);\n \t}\n \n@@ -283,8 +272,8 @@ void ColumnReader::PrepareDataPage(PageHeader &page_hdr) {\n \t\t                          ? block->read<uint32_t>()\n \t\t                          : page_hdr.data_page_header_v2.definition_levels_byte_length;\n \t\tblock->available(def_length);\n-\t\tdefined_decoder =\n-\t\t    make_unique<RleBpDecoder>((const uint8_t *)block->ptr, def_length, ComputeBitWidth(max_define));\n+\t\tdefined_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, def_length,\n+\t\t                                            RleBpDecoder::ComputeBitWidth(max_define));\n \t\tblock->inc(def_length);\n \t}\n \n@@ -452,6 +441,9 @@ void StringParquetValueConversion::PlainSkip(ByteBuffer &plain_data, ColumnReade\n \tplain_data.inc(str_len);\n }\n \n+//===--------------------------------------------------------------------===//\n+// List Column Reader\n+//===--------------------------------------------------------------------===//\n idx_t ListColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,\n                              Vector &result_out) {\n \tidx_t result_offset = 0;\n@@ -512,6 +504,10 @@ idx_t ListColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint\n \t\t\t\t// value has been defined down the stack, hence its NOT NULL\n \t\t\t\tresult_ptr[result_offset].offset = child_idx + current_chunk_offset;\n \t\t\t\tresult_ptr[result_offset].length = 1;\n+\t\t\t} else if (child_defines_ptr[child_idx] == max_define - 1) {\n+\t\t\t\t// empty list\n+\t\t\t\tresult_ptr[result_offset].offset = child_idx + current_chunk_offset;\n+\t\t\t\tresult_ptr[result_offset].length = 0;\n \t\t\t} else {\n \t\t\t\t// value is NULL somewhere up the stack\n \t\t\t\tresult_mask.SetInvalid(result_offset);\n@@ -560,4 +556,64 @@ ListColumnReader::ListColumnReader(ParquetReader &reader, LogicalType type_p, co\n \tchild_filter.set();\n }\n \n+//===--------------------------------------------------------------------===//\n+// Struct Column Reader\n+//===--------------------------------------------------------------------===//\n+StructColumnReader::StructColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,\n+                                       idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p,\n+                                       vector<unique_ptr<ColumnReader>> child_readers_p)\n+    : ColumnReader(reader, move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p),\n+      child_readers(move(child_readers_p)) {\n+\tD_ASSERT(type.id() == LogicalTypeId::STRUCT);\n+\tD_ASSERT(!StructType::GetChildTypes(type).empty());\n+}\n+\n+ColumnReader *StructColumnReader::GetChildReader(idx_t child_idx) {\n+\treturn child_readers[child_idx].get();\n+}\n+\n+void StructColumnReader::InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) {\n+\tfor (auto &child : child_readers) {\n+\t\tchild->InitializeRead(columns, protocol_p);\n+\t}\n+}\n+\n+idx_t StructColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,\n+                               Vector &result) {\n+\tauto &struct_entries = StructVector::GetEntries(result);\n+\tD_ASSERT(StructType::GetChildTypes(Type()).size() == struct_entries.size());\n+\n+\tidx_t read_count = num_values;\n+\tfor (idx_t i = 0; i < struct_entries.size(); i++) {\n+\t\tauto child_num_values = child_readers[i]->Read(num_values, filter, define_out, repeat_out, *struct_entries[i]);\n+\t\tif (i == 0) {\n+\t\t\tread_count = child_num_values;\n+\t\t} else if (read_count != child_num_values) {\n+\t\t\tthrow std::runtime_error(\"Struct child row count mismatch\");\n+\t\t}\n+\t}\n+\t// set the validity mask for this level\n+\tauto &validity = FlatVector::Validity(result);\n+\tfor (idx_t i = 0; i < read_count; i++) {\n+\t\tif (define_out[i] < max_define) {\n+\t\t\tvalidity.SetInvalid(i);\n+\t\t}\n+\t}\n+\n+\treturn read_count;\n+}\n+\n+void StructColumnReader::Skip(idx_t num_values) {\n+\tthrow InternalException(\"Skip not implemented for StructColumnReader\");\n+}\n+\n+idx_t StructColumnReader::GroupRowsAvailable() {\n+\tfor (idx_t i = 0; i < child_readers.size(); i++) {\n+\t\tif (child_readers[i]->Type().id() != LogicalTypeId::LIST) {\n+\t\t\treturn child_readers[i]->GroupRowsAvailable();\n+\t\t}\n+\t}\n+\treturn child_readers[0]->GroupRowsAvailable();\n+}\n+\n } // namespace duckdb\ndiff --git a/extension/parquet/column_writer.cpp b/extension/parquet/column_writer.cpp\nnew file mode 100644\nindex 000000000000..5de7cfcfca26\n--- /dev/null\n+++ b/extension/parquet/column_writer.cpp\n@@ -0,0 +1,957 @@\n+#include \"column_writer.hpp\"\n+#include \"parquet_writer.hpp\"\n+#include \"parquet_rle_bp_decoder.hpp\"\n+\n+#include \"duckdb.hpp\"\n+#ifndef DUCKDB_AMALGAMATION\n+#include \"duckdb/common/common.hpp\"\n+#include \"duckdb/common/exception.hpp\"\n+#include \"duckdb/common/mutex.hpp\"\n+#include \"duckdb/common/serializer/buffered_file_writer.hpp\"\n+#include \"duckdb/common/types/chunk_collection.hpp\"\n+#include \"duckdb/common/types/date.hpp\"\n+#include \"duckdb/common/types/hugeint.hpp\"\n+#include \"duckdb/common/types/time.hpp\"\n+#include \"duckdb/common/types/timestamp.hpp\"\n+#include \"duckdb/common/serializer/buffered_serializer.hpp\"\n+#endif\n+\n+#include \"snappy.h\"\n+#include \"miniz_wrapper.hpp\"\n+#include \"zstd.h\"\n+\n+namespace duckdb {\n+\n+using namespace duckdb_parquet; // NOLINT\n+using namespace duckdb_miniz;   // NOLINT\n+\n+using duckdb_parquet::format::CompressionCodec;\n+using duckdb_parquet::format::ConvertedType;\n+using duckdb_parquet::format::Encoding;\n+using duckdb_parquet::format::FieldRepetitionType;\n+using duckdb_parquet::format::FileMetaData;\n+using duckdb_parquet::format::PageHeader;\n+using duckdb_parquet::format::PageType;\n+using ParquetRowGroup = duckdb_parquet::format::RowGroup;\n+using duckdb_parquet::format::Type;\n+\n+#define PARQUET_DEFINE_VALID 65535\n+\n+//===--------------------------------------------------------------------===//\n+// ColumnWriter\n+//===--------------------------------------------------------------------===//\n+ColumnWriter::ColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define)\n+    : writer(writer), schema_idx(schema_idx), max_repeat(max_repeat), max_define(max_define) {\n+}\n+ColumnWriter::~ColumnWriter() {\n+}\n+\n+ColumnWriterState::~ColumnWriterState() {\n+}\n+\n+static void VarintEncode(uint32_t val, Serializer &ser) {\n+\tdo {\n+\t\tuint8_t byte = val & 127;\n+\t\tval >>= 7;\n+\t\tif (val != 0) {\n+\t\t\tbyte |= 128;\n+\t\t}\n+\t\tser.Write<uint8_t>(byte);\n+\t} while (val != 0);\n+}\n+\n+static uint8_t GetVarintSize(uint32_t val) {\n+\tuint8_t res = 0;\n+\tdo {\n+\t\tuint8_t byte = val & 127;\n+\t\tval >>= 7;\n+\t\tif (val != 0) {\n+\t\t\tbyte |= 128;\n+\t\t}\n+\t\tres++;\n+\t} while (val != 0);\n+\treturn res;\n+}\n+\n+void ColumnWriter::CompressPage(BufferedSerializer &temp_writer, size_t &compressed_size, data_ptr_t &compressed_data,\n+                                unique_ptr<data_t[]> &compressed_buf) {\n+\tswitch (writer.codec) {\n+\tcase CompressionCodec::UNCOMPRESSED:\n+\t\tcompressed_size = temp_writer.blob.size;\n+\t\tcompressed_data = temp_writer.blob.data.get();\n+\t\tbreak;\n+\tcase CompressionCodec::SNAPPY: {\n+\t\tcompressed_size = duckdb_snappy::MaxCompressedLength(temp_writer.blob.size);\n+\t\tcompressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);\n+\t\tduckdb_snappy::RawCompress((const char *)temp_writer.blob.data.get(), temp_writer.blob.size,\n+\t\t                           (char *)compressed_buf.get(), &compressed_size);\n+\t\tcompressed_data = compressed_buf.get();\n+\t\tD_ASSERT(compressed_size <= duckdb_snappy::MaxCompressedLength(temp_writer.blob.size));\n+\t\tbreak;\n+\t}\n+\tcase CompressionCodec::GZIP: {\n+\t\tMiniZStream s;\n+\t\tcompressed_size = s.MaxCompressedLength(temp_writer.blob.size);\n+\t\tcompressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);\n+\t\ts.Compress((const char *)temp_writer.blob.data.get(), temp_writer.blob.size, (char *)compressed_buf.get(),\n+\t\t           &compressed_size);\n+\t\tcompressed_data = compressed_buf.get();\n+\t\tbreak;\n+\t}\n+\tcase CompressionCodec::ZSTD: {\n+\t\tcompressed_size = duckdb_zstd::ZSTD_compressBound(temp_writer.blob.size);\n+\t\tcompressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);\n+\t\tcompressed_size = duckdb_zstd::ZSTD_compress((void *)compressed_buf.get(), compressed_size,\n+\t\t                                             (const void *)temp_writer.blob.data.get(), temp_writer.blob.size,\n+\t\t                                             ZSTD_CLEVEL_DEFAULT);\n+\t\tcompressed_data = compressed_buf.get();\n+\t\tbreak;\n+\t}\n+\tdefault:\n+\t\tthrow InternalException(\"Unsupported codec for Parquet Writer\");\n+\t}\n+\n+\tif (compressed_size > idx_t(NumericLimits<int32_t>::Maximum())) {\n+\t\tthrow InternalException(\"Parquet writer: %d compressed page size out of range for type integer\",\n+\t\t                        temp_writer.blob.size);\n+\t}\n+}\n+\n+class ColumnWriterPageState {\n+public:\n+\tvirtual ~ColumnWriterPageState() {\n+\t}\n+};\n+\n+struct PageInformation {\n+\tidx_t offset = 0;\n+\tidx_t row_count = 0;\n+\tidx_t empty_count = 0;\n+\tidx_t estimated_page_size = 0;\n+};\n+\n+struct PageWriteInformation {\n+\tPageHeader page_header;\n+\tunique_ptr<BufferedSerializer> temp_writer;\n+\tunique_ptr<ColumnWriterPageState> page_state;\n+\tidx_t write_page_idx = 0;\n+\tidx_t write_count = 0;\n+\tidx_t max_write_count = 0;\n+\tsize_t compressed_size;\n+\tdata_ptr_t compressed_data;\n+\tunique_ptr<data_t[]> compressed_buf;\n+};\n+\n+class StandardColumnWriterState : public ColumnWriterState {\n+public:\n+\tStandardColumnWriterState(duckdb_parquet::format::RowGroup &row_group, idx_t col_idx)\n+\t    : row_group(row_group), col_idx(col_idx) {\n+\t\tpage_info.emplace_back();\n+\t}\n+\t~StandardColumnWriterState() override = default;\n+\n+\tduckdb_parquet::format::RowGroup &row_group;\n+\tidx_t col_idx;\n+\tvector<PageInformation> page_info;\n+\tvector<PageWriteInformation> write_info;\n+\tidx_t current_page = 0;\n+};\n+\n+unique_ptr<ColumnWriterState> ColumnWriter::InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,\n+                                                                 vector<string> schema_path) {\n+\tauto result = make_unique<StandardColumnWriterState>(row_group, row_group.columns.size());\n+\n+\tduckdb_parquet::format::ColumnChunk column_chunk;\n+\tcolumn_chunk.__isset.meta_data = true;\n+\tcolumn_chunk.meta_data.codec = writer.codec;\n+\tcolumn_chunk.meta_data.path_in_schema = move(schema_path);\n+\tcolumn_chunk.meta_data.path_in_schema.push_back(writer.file_meta_data.schema[schema_idx].name);\n+\tcolumn_chunk.meta_data.num_values = 0;\n+\tcolumn_chunk.meta_data.type = writer.file_meta_data.schema[schema_idx].type;\n+\trow_group.columns.push_back(move(column_chunk));\n+\n+\treturn move(result);\n+}\n+\n+void ColumnWriter::HandleRepeatLevels(ColumnWriterState &state, ColumnWriterState *parent, idx_t count,\n+                                      idx_t max_repeat) {\n+\tif (!parent) {\n+\t\t// no repeat levels without a parent node\n+\t\treturn;\n+\t}\n+\twhile (state.repetition_levels.size() < parent->repetition_levels.size()) {\n+\t\tstate.repetition_levels.push_back(parent->repetition_levels[state.repetition_levels.size()]);\n+\t}\n+}\n+\n+void ColumnWriter::HandleDefineLevels(ColumnWriterState &state, ColumnWriterState *parent, ValidityMask &validity,\n+                                      idx_t count, uint16_t define_value, uint16_t null_value) {\n+\tif (parent) {\n+\t\t// parent node: inherit definition level from the parent\n+\t\tidx_t vector_index = 0;\n+\t\twhile (state.definition_levels.size() < parent->definition_levels.size()) {\n+\t\t\tidx_t current_index = state.definition_levels.size();\n+\t\t\tif (parent->definition_levels[current_index] != PARQUET_DEFINE_VALID) {\n+\t\t\t\tstate.definition_levels.push_back(parent->definition_levels[current_index]);\n+\t\t\t} else if (validity.RowIsValid(vector_index)) {\n+\t\t\t\tstate.definition_levels.push_back(define_value);\n+\t\t\t} else {\n+\t\t\t\tstate.definition_levels.push_back(null_value);\n+\t\t\t}\n+\t\t\tif (parent->is_empty.empty() || !parent->is_empty[current_index]) {\n+\t\t\t\tvector_index++;\n+\t\t\t}\n+\t\t}\n+\t} else {\n+\t\t// no parent: set definition levels only from this validity mask\n+\t\tfor (idx_t i = 0; i < count; i++) {\n+\t\t\tif (validity.RowIsValid(i)) {\n+\t\t\t\tstate.definition_levels.push_back(define_value);\n+\t\t\t} else {\n+\t\t\t\tstate.definition_levels.push_back(null_value);\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+void ColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {\n+\tauto &state = (StandardColumnWriterState &)state_p;\n+\tauto &col_chunk = state.row_group.columns[state.col_idx];\n+\n+\tidx_t start = 0;\n+\tidx_t vcount = parent ? parent->definition_levels.size() - state.definition_levels.size() : count;\n+\tidx_t parent_index = state.definition_levels.size();\n+\tauto &validity = FlatVector::Validity(vector);\n+\tHandleRepeatLevels(state_p, parent, count, max_repeat);\n+\tHandleDefineLevels(state_p, parent, validity, count, max_define, max_define - 1);\n+\n+\tidx_t vector_index = 0;\n+\tfor (idx_t i = start; i < vcount; i++) {\n+\t\tauto &page_info = state.page_info.back();\n+\t\tpage_info.row_count++;\n+\t\tcol_chunk.meta_data.num_values++;\n+\t\tif (parent && !parent->is_empty.empty() && parent->is_empty[parent_index + i]) {\n+\t\t\tpage_info.empty_count++;\n+\t\t\tcontinue;\n+\t\t}\n+\t\tif (validity.RowIsValid(vector_index)) {\n+\t\t\tpage_info.estimated_page_size += GetRowSize(vector, vector_index);\n+\t\t\tif (page_info.estimated_page_size >= MAX_UNCOMPRESSED_PAGE_SIZE) {\n+\t\t\t\tPageInformation new_info;\n+\t\t\t\tnew_info.offset = page_info.offset + page_info.row_count;\n+\t\t\t\tstate.page_info.push_back(new_info);\n+\t\t\t}\n+\t\t}\n+\t\tvector_index++;\n+\t}\n+}\n+\n+unique_ptr<ColumnWriterPageState> ColumnWriter::InitializePageState() {\n+\treturn nullptr;\n+}\n+\n+void ColumnWriter::FlushPageState(Serializer &temp_writer, ColumnWriterPageState *state) {\n+}\n+\n+void ColumnWriter::BeginWrite(ColumnWriterState &state_p) {\n+\tauto &state = (StandardColumnWriterState &)state_p;\n+\n+\t// set up the page write info\n+\tfor (idx_t page_idx = 0; page_idx < state.page_info.size(); page_idx++) {\n+\t\tauto &page_info = state.page_info[page_idx];\n+\t\tif (page_info.row_count == 0) {\n+\t\t\tD_ASSERT(page_idx + 1 == state.page_info.size());\n+\t\t\tstate.page_info.erase(state.page_info.begin() + page_idx);\n+\t\t\tbreak;\n+\t\t}\n+\t\tPageWriteInformation write_info;\n+\t\t// set up the header\n+\t\tauto &hdr = write_info.page_header;\n+\t\thdr.compressed_page_size = 0;\n+\t\thdr.uncompressed_page_size = 0;\n+\t\thdr.type = PageType::DATA_PAGE;\n+\t\thdr.__isset.data_page_header = true;\n+\n+\t\thdr.data_page_header.num_values = page_info.row_count;\n+\t\thdr.data_page_header.encoding = Encoding::PLAIN;\n+\t\thdr.data_page_header.definition_level_encoding = Encoding::RLE;\n+\t\thdr.data_page_header.repetition_level_encoding = Encoding::RLE;\n+\n+\t\twrite_info.temp_writer = make_unique<BufferedSerializer>();\n+\t\twrite_info.write_count = page_info.empty_count;\n+\t\twrite_info.max_write_count = page_info.row_count;\n+\t\twrite_info.page_state = InitializePageState();\n+\n+\t\twrite_info.compressed_size = 0;\n+\t\twrite_info.compressed_data = nullptr;\n+\n+\t\tstate.write_info.push_back(move(write_info));\n+\t}\n+\n+\t// start writing the first page\n+\tNextPage(state_p);\n+}\n+\n+void ColumnWriter::WriteLevels(Serializer &temp_writer, const vector<uint16_t> &levels, idx_t max_value, idx_t offset,\n+                               idx_t count) {\n+\tif (levels.empty() || count == 0) {\n+\t\treturn;\n+\t}\n+\n+\t// write the levels\n+\t// we always RLE everything (for now)\n+\tauto bit_width = RleBpDecoder::ComputeBitWidth((max_value));\n+\tauto byte_width = (bit_width + 7) / 8;\n+\n+\t// figure out how many bytes we are going to need\n+\tidx_t byte_count = 0;\n+\tidx_t run_count = 1;\n+\tidx_t current_run_count = 1;\n+\tfor (idx_t i = offset + 1; i <= offset + count; i++) {\n+\t\tif (i == offset + count || levels[i] != levels[i - 1]) {\n+\t\t\t// last value, or value has changed\n+\t\t\t// write out the current run\n+\t\t\tbyte_count += GetVarintSize(current_run_count << 1) + byte_width;\n+\t\t\tcurrent_run_count = 1;\n+\t\t\trun_count++;\n+\t\t} else {\n+\t\t\tcurrent_run_count++;\n+\t\t}\n+\t}\n+\ttemp_writer.Write<uint32_t>(byte_count);\n+\n+\t// now actually write the values\n+\tcurrent_run_count = 1;\n+\tfor (idx_t i = offset + 1; i <= offset + count; i++) {\n+\t\tif (i == offset + count || levels[i] != levels[i - 1]) {\n+\t\t\t// new run: write out the old run\n+\t\t\t// first write the header\n+\t\t\tVarintEncode(current_run_count << 1, temp_writer);\n+\t\t\t// now write hte value\n+\t\t\tswitch (byte_width) {\n+\t\t\tcase 1:\n+\t\t\t\ttemp_writer.Write<uint8_t>(levels[i - 1]);\n+\t\t\t\tbreak;\n+\t\t\tcase 2:\n+\t\t\t\ttemp_writer.Write<uint16_t>(levels[i - 1]);\n+\t\t\t\tbreak;\n+\t\t\tdefault:\n+\t\t\t\tthrow InternalException(\"unsupported byte width for RLE encoding\");\n+\t\t\t}\n+\t\t\tcurrent_run_count = 1;\n+\t\t} else {\n+\t\t\tcurrent_run_count++;\n+\t\t}\n+\t}\n+}\n+\n+void ColumnWriter::NextPage(ColumnWriterState &state_p) {\n+\tauto &state = (StandardColumnWriterState &)state_p;\n+\n+\tif (state.current_page > 0) {\n+\t\t// need to flush the current page\n+\t\tFlushPage(state_p);\n+\t}\n+\tif (state.current_page >= state.write_info.size()) {\n+\t\tstate.current_page = state.write_info.size() + 1;\n+\t\treturn;\n+\t}\n+\tauto &page_info = state.page_info[state.current_page];\n+\tauto &write_info = state.write_info[state.current_page];\n+\tstate.current_page++;\n+\n+\tauto &temp_writer = *write_info.temp_writer;\n+\n+\t// write the repetition levels\n+\tWriteLevels(temp_writer, state.repetition_levels, max_repeat, page_info.offset, page_info.row_count);\n+\n+\t// write the definition levels\n+\tWriteLevels(temp_writer, state.definition_levels, max_define, page_info.offset, page_info.row_count);\n+}\n+\n+void ColumnWriter::FlushPage(ColumnWriterState &state_p) {\n+\tauto &state = (StandardColumnWriterState &)state_p;\n+\tD_ASSERT(state.current_page > 0);\n+\tif (state.current_page > state.write_info.size()) {\n+\t\treturn;\n+\t}\n+\n+\t// compress the page info\n+\tauto &write_info = state.write_info[state.current_page - 1];\n+\tauto &temp_writer = *write_info.temp_writer;\n+\tauto &hdr = write_info.page_header;\n+\n+\tFlushPageState(temp_writer, write_info.page_state.get());\n+\n+\t// now that we have finished writing the data we know the uncompressed size\n+\tif (temp_writer.blob.size > idx_t(NumericLimits<int32_t>::Maximum())) {\n+\t\tthrow InternalException(\"Parquet writer: %d uncompressed page size out of range for type integer\",\n+\t\t                        temp_writer.blob.size);\n+\t}\n+\thdr.uncompressed_page_size = temp_writer.blob.size;\n+\n+\t// compress the data\n+\tCompressPage(temp_writer, write_info.compressed_size, write_info.compressed_data, write_info.compressed_buf);\n+\thdr.compressed_page_size = write_info.compressed_size;\n+\tD_ASSERT(hdr.uncompressed_page_size > 0);\n+\tD_ASSERT(hdr.compressed_page_size > 0);\n+\n+\tif (write_info.compressed_buf) {\n+\t\t// if the data has been compressed, we no longer need the compressed data\n+\t\tD_ASSERT(write_info.compressed_buf.get() == write_info.compressed_data);\n+\t\twrite_info.temp_writer.reset();\n+\t}\n+}\n+\n+void ColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count) {\n+\tauto &state = (StandardColumnWriterState &)state_p;\n+\n+\tidx_t remaining = count;\n+\tidx_t offset = 0;\n+\twhile (remaining > 0) {\n+\t\tauto &write_info = state.write_info[state.current_page - 1];\n+\t\tif (!write_info.temp_writer) {\n+\t\t\tthrow InternalException(\"Writes are not correctly aligned!?\");\n+\t\t}\n+\t\tauto &temp_writer = *write_info.temp_writer;\n+\t\tidx_t write_count = MinValue<idx_t>(remaining, write_info.max_write_count - write_info.write_count);\n+\t\tD_ASSERT(write_count > 0);\n+\n+\t\tWriteVector(temp_writer, write_info.page_state.get(), vector, offset, offset + write_count);\n+\n+\t\twrite_info.write_count += write_count;\n+\t\tif (write_info.write_count == write_info.max_write_count) {\n+\t\t\tNextPage(state_p);\n+\t\t}\n+\t\toffset += write_count;\n+\t\tremaining -= write_count;\n+\t}\n+}\n+\n+void ColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {\n+\tauto &state = (StandardColumnWriterState &)state_p;\n+\tauto &column_chunk = state.row_group.columns[state.col_idx];\n+\n+\t// flush the last page (if any remains)\n+\tFlushPage(state);\n+\t// record the start position of the pages for this column\n+\tcolumn_chunk.meta_data.data_page_offset = writer.writer->GetTotalWritten();\n+\t// write the individual pages to disk\n+\tfor (auto &write_info : state.write_info) {\n+\t\tD_ASSERT(write_info.page_header.uncompressed_page_size > 0);\n+\t\twrite_info.page_header.write(writer.protocol.get());\n+\t\twriter.writer->WriteData(write_info.compressed_data, write_info.compressed_size);\n+\t}\n+\tcolumn_chunk.meta_data.total_compressed_size =\n+\t    writer.writer->GetTotalWritten() - column_chunk.meta_data.data_page_offset;\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// Standard Column Writer\n+//===--------------------------------------------------------------------===//\n+struct ParquetCastOperator {\n+\ttemplate <class SRC, class TGT>\n+\tstatic TGT Operation(SRC input) {\n+\t\treturn TGT(input);\n+\t}\n+};\n+\n+struct ParquetTimestampNSOperator {\n+\ttemplate <class SRC, class TGT>\n+\tstatic TGT Operation(SRC input) {\n+\t\treturn Timestamp::FromEpochNanoSeconds(input).value;\n+\t}\n+};\n+\n+struct ParquetTimestampSOperator {\n+\ttemplate <class SRC, class TGT>\n+\tstatic TGT Operation(SRC input) {\n+\t\treturn Timestamp::FromEpochSeconds(input).value;\n+\t}\n+};\n+\n+struct ParquetHugeintOperator {\n+\ttemplate <class SRC, class TGT>\n+\tstatic TGT Operation(SRC input) {\n+\t\treturn Hugeint::Cast<double>(input);\n+\t}\n+};\n+\n+template <class SRC, class TGT, class OP = ParquetCastOperator>\n+static void TemplatedWritePlain(Vector &col, idx_t chunk_start, idx_t chunk_end, ValidityMask &mask, Serializer &ser) {\n+\tauto *ptr = FlatVector::GetData<SRC>(col);\n+\tfor (idx_t r = chunk_start; r < chunk_end; r++) {\n+\t\tif (mask.RowIsValid(r)) {\n+\t\t\tser.Write<TGT>(OP::template Operation<SRC, TGT>(ptr[r]));\n+\t\t}\n+\t}\n+}\n+\n+template <class SRC, class TGT, class OP = ParquetCastOperator>\n+class StandardColumnWriter : public ColumnWriter {\n+public:\n+\tStandardColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define)\n+\t    : ColumnWriter(writer, schema_idx, max_repeat, max_define) {\n+\t}\n+\t~StandardColumnWriter() override = default;\n+\n+public:\n+\tvoid WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,\n+\t                 idx_t chunk_start, idx_t chunk_end) override {\n+\t\tauto &mask = FlatVector::Validity(input_column);\n+\t\tTemplatedWritePlain<SRC, TGT, OP>(input_column, chunk_start, chunk_end, mask, temp_writer);\n+\t}\n+\n+\tidx_t GetRowSize(Vector &vector, idx_t index) override {\n+\t\treturn sizeof(TGT);\n+\t}\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// Boolean Column Writer\n+//===--------------------------------------------------------------------===//\n+class BooleanWriterPageState : public ColumnWriterPageState {\n+public:\n+\tuint8_t byte = 0;\n+\tuint8_t byte_pos = 0;\n+};\n+\n+class BooleanColumnWriter : public ColumnWriter {\n+public:\n+\tBooleanColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define)\n+\t    : ColumnWriter(writer, schema_idx, max_repeat, max_define) {\n+\t}\n+\t~BooleanColumnWriter() override = default;\n+\n+public:\n+\tvoid WriteVector(Serializer &temp_writer, ColumnWriterPageState *state_p, Vector &input_column, idx_t chunk_start,\n+\t                 idx_t chunk_end) override {\n+\t\tauto &state = (BooleanWriterPageState &)*state_p;\n+\t\tauto &mask = FlatVector::Validity(input_column);\n+\n+\t\tauto *ptr = FlatVector::GetData<bool>(input_column);\n+\t\tfor (idx_t r = chunk_start; r < chunk_end; r++) {\n+\t\t\tif (mask.RowIsValid(r)) {\n+\t\t\t\t// only encode if non-null\n+\t\t\t\tif (ptr[r]) {\n+\t\t\t\t\tstate.byte |= 1 << state.byte_pos;\n+\t\t\t\t}\n+\t\t\t\tstate.byte_pos++;\n+\n+\t\t\t\tif (state.byte_pos == 8) {\n+\t\t\t\t\ttemp_writer.Write<uint8_t>(state.byte);\n+\t\t\t\t\tstate.byte = 0;\n+\t\t\t\t\tstate.byte_pos = 0;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tunique_ptr<ColumnWriterPageState> InitializePageState() override {\n+\t\treturn make_unique<BooleanWriterPageState>();\n+\t}\n+\n+\tvoid FlushPageState(Serializer &temp_writer, ColumnWriterPageState *state_p) override {\n+\t\tauto &state = (BooleanWriterPageState &)*state_p;\n+\t\tif (state.byte_pos > 0) {\n+\t\t\ttemp_writer.Write<uint8_t>(state.byte);\n+\t\t\tstate.byte = 0;\n+\t\t\tstate.byte_pos = 0;\n+\t\t}\n+\t}\n+\n+\tidx_t GetRowSize(Vector &vector, idx_t index) override {\n+\t\treturn sizeof(bool);\n+\t}\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// Decimal Column Writer\n+//===--------------------------------------------------------------------===//\n+class DecimalColumnWriter : public ColumnWriter {\n+public:\n+\tDecimalColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define)\n+\t    : ColumnWriter(writer, schema_idx, max_repeat, max_define) {\n+\t}\n+\t~DecimalColumnWriter() override = default;\n+\n+public:\n+\tvoid WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,\n+\t                 idx_t chunk_start, idx_t chunk_end) override {\n+\t\tauto &mask = FlatVector::Validity(input_column);\n+\n+\t\t// FIXME: fixed length byte array...\n+\t\tVector double_vec(LogicalType::DOUBLE, true, false, chunk_end);\n+\t\tVectorOperations::Cast(input_column, double_vec, chunk_end);\n+\t\tTemplatedWritePlain<double, double>(double_vec, chunk_start, chunk_end, mask, temp_writer);\n+\t}\n+\n+\tidx_t GetRowSize(Vector &vector, idx_t index) override {\n+\t\treturn sizeof(double);\n+\t}\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// String Column Writer\n+//===--------------------------------------------------------------------===//\n+class StringColumnWriter : public ColumnWriter {\n+public:\n+\tStringColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define)\n+\t    : ColumnWriter(writer, schema_idx, max_repeat, max_define) {\n+\t}\n+\t~StringColumnWriter() override = default;\n+\n+public:\n+\tvoid WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,\n+\t                 idx_t chunk_start, idx_t chunk_end) override {\n+\t\tauto &mask = FlatVector::Validity(input_column);\n+\n+\t\tauto *ptr = FlatVector::GetData<string_t>(input_column);\n+\t\tfor (idx_t r = chunk_start; r < chunk_end; r++) {\n+\t\t\tif (mask.RowIsValid(r)) {\n+\t\t\t\ttemp_writer.Write<uint32_t>(ptr[r].GetSize());\n+\t\t\t\ttemp_writer.WriteData((const_data_ptr_t)ptr[r].GetDataUnsafe(), ptr[r].GetSize());\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tidx_t GetRowSize(Vector &vector, idx_t index) override {\n+\t\tauto strings = FlatVector::GetData<string_t>(vector);\n+\t\treturn strings[index].GetSize();\n+\t}\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// Struct Column Writer\n+//===--------------------------------------------------------------------===//\n+class StructColumnWriter : public ColumnWriter {\n+public:\n+\tStructColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define,\n+\t                   vector<unique_ptr<ColumnWriter>> child_writers_p)\n+\t    : ColumnWriter(writer, schema_idx, max_repeat, max_define), child_writers(move(child_writers_p)) {\n+\t}\n+\t~StructColumnWriter() override = default;\n+\n+\tvector<unique_ptr<ColumnWriter>> child_writers;\n+\n+public:\n+\tvoid WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,\n+\t                 idx_t chunk_start, idx_t chunk_end) override {\n+\t\tthrow InternalException(\"Cannot write vector of type struct\");\n+\t}\n+\n+\tidx_t GetRowSize(Vector &vector, idx_t index) override {\n+\t\tthrow InternalException(\"Cannot get row size of struct\");\n+\t}\n+\n+\tunique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,\n+\t                                                   vector<string> schema_path) override;\n+\tvoid Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;\n+\n+\tvoid BeginWrite(ColumnWriterState &state) override;\n+\tvoid Write(ColumnWriterState &state, Vector &vector, idx_t count) override;\n+\tvoid FinalizeWrite(ColumnWriterState &state) override;\n+};\n+\n+class StructColumnWriterState : public ColumnWriterState {\n+public:\n+\tStructColumnWriterState(duckdb_parquet::format::RowGroup &row_group, idx_t col_idx)\n+\t    : row_group(row_group), col_idx(col_idx) {\n+\t}\n+\t~StructColumnWriterState() override = default;\n+\n+\tduckdb_parquet::format::RowGroup &row_group;\n+\tidx_t col_idx;\n+\tvector<unique_ptr<ColumnWriterState>> child_states;\n+};\n+\n+unique_ptr<ColumnWriterState> StructColumnWriter::InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,\n+                                                                       vector<string> schema_path) {\n+\tauto result = make_unique<StructColumnWriterState>(row_group, row_group.columns.size());\n+\tschema_path.push_back(writer.file_meta_data.schema[schema_idx].name);\n+\n+\tresult->child_states.reserve(child_writers.size());\n+\tfor (auto &child_writer : child_writers) {\n+\t\tresult->child_states.push_back(child_writer->InitializeWriteState(row_group, schema_path));\n+\t}\n+\treturn move(result);\n+}\n+\n+void StructColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {\n+\tauto &state = (StructColumnWriterState &)state_p;\n+\n+\tauto &validity = FlatVector::Validity(vector);\n+\tif (parent) {\n+\t\t// propagate empty entries from the parent\n+\t\twhile (state.is_empty.size() < parent->is_empty.size()) {\n+\t\t\tstate.is_empty.push_back(parent->is_empty[state.is_empty.size()]);\n+\t\t}\n+\t}\n+\tHandleRepeatLevels(state_p, parent, count, max_repeat);\n+\tHandleDefineLevels(state_p, parent, validity, count, PARQUET_DEFINE_VALID, max_define - 1);\n+\tauto &child_vectors = StructVector::GetEntries(vector);\n+\tfor (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {\n+\t\tchild_writers[child_idx]->Prepare(*state.child_states[child_idx], &state_p, *child_vectors[child_idx], count);\n+\t}\n+}\n+\n+void StructColumnWriter::BeginWrite(ColumnWriterState &state_p) {\n+\tauto &state = (StructColumnWriterState &)state_p;\n+\tfor (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {\n+\t\tchild_writers[child_idx]->BeginWrite(*state.child_states[child_idx]);\n+\t}\n+}\n+\n+void StructColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count) {\n+\tauto &state = (StructColumnWriterState &)state_p;\n+\tauto &child_vectors = StructVector::GetEntries(vector);\n+\tfor (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {\n+\t\tchild_writers[child_idx]->Write(*state.child_states[child_idx], *child_vectors[child_idx], count);\n+\t}\n+}\n+\n+void StructColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {\n+\tauto &state = (StructColumnWriterState &)state_p;\n+\tfor (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {\n+\t\tchild_writers[child_idx]->FinalizeWrite(*state.child_states[child_idx]);\n+\t}\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// List Column Writer\n+//===--------------------------------------------------------------------===//\n+class ListColumnWriter : public ColumnWriter {\n+public:\n+\tListColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define,\n+\t                 unique_ptr<ColumnWriter> child_writer_p)\n+\t    : ColumnWriter(writer, schema_idx, max_repeat, max_define), child_writer(move(child_writer_p)) {\n+\t}\n+\t~ListColumnWriter() override = default;\n+\n+\tunique_ptr<ColumnWriter> child_writer;\n+\n+public:\n+\tvoid WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &input_column,\n+\t                 idx_t chunk_start, idx_t chunk_end) override {\n+\t\tthrow InternalException(\"Cannot write vector of type list\");\n+\t}\n+\n+\tidx_t GetRowSize(Vector &vector, idx_t index) override {\n+\t\tthrow InternalException(\"Cannot get row size of list\");\n+\t}\n+\n+\tunique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,\n+\t                                                   vector<string> schema_path) override;\n+\tvoid Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;\n+\n+\tvoid BeginWrite(ColumnWriterState &state) override;\n+\tvoid Write(ColumnWriterState &state, Vector &vector, idx_t count) override;\n+\tvoid FinalizeWrite(ColumnWriterState &state) override;\n+};\n+\n+class ListColumnWriterState : public ColumnWriterState {\n+public:\n+\tListColumnWriterState(duckdb_parquet::format::RowGroup &row_group, idx_t col_idx)\n+\t    : row_group(row_group), col_idx(col_idx) {\n+\t}\n+\t~ListColumnWriterState() override = default;\n+\n+\tduckdb_parquet::format::RowGroup &row_group;\n+\tidx_t col_idx;\n+\tunique_ptr<ColumnWriterState> child_state;\n+\tidx_t parent_index = 0;\n+};\n+\n+unique_ptr<ColumnWriterState> ListColumnWriter::InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,\n+                                                                     vector<string> schema_path) {\n+\tauto result = make_unique<ListColumnWriterState>(row_group, row_group.columns.size());\n+\tschema_path.push_back(writer.file_meta_data.schema[schema_idx].name);\n+\tresult->child_state = child_writer->InitializeWriteState(row_group, move(schema_path));\n+\treturn move(result);\n+}\n+\n+void ListColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {\n+\tauto &state = (ListColumnWriterState &)state_p;\n+\n+\tauto list_data = FlatVector::GetData<list_entry_t>(vector);\n+\tauto &validity = FlatVector::Validity(vector);\n+\n+\t// write definition levels and repeats\n+\tidx_t start = 0;\n+\tidx_t vcount = parent ? parent->definition_levels.size() - state.parent_index : count;\n+\tidx_t vector_index = 0;\n+\tfor (idx_t i = start; i < vcount; i++) {\n+\t\tidx_t parent_index = state.parent_index + i;\n+\t\tif (parent && !parent->is_empty.empty() && parent->is_empty[parent_index]) {\n+\t\t\tstate.definition_levels.push_back(parent->definition_levels[parent_index]);\n+\t\t\tstate.repetition_levels.push_back(parent->repetition_levels[parent_index]);\n+\t\t\tstate.is_empty.push_back(true);\n+\t\t\tcontinue;\n+\t\t}\n+\t\tauto first_repeat_level =\n+\t\t    parent && !parent->repetition_levels.empty() ? parent->repetition_levels[parent_index] : max_repeat;\n+\t\tif (parent && parent->definition_levels[parent_index] != PARQUET_DEFINE_VALID) {\n+\t\t\tstate.definition_levels.push_back(parent->definition_levels[parent_index]);\n+\t\t\tstate.repetition_levels.push_back(first_repeat_level);\n+\t\t\tstate.is_empty.push_back(true);\n+\t\t} else if (validity.RowIsValid(vector_index)) {\n+\t\t\t// push the repetition levels\n+\t\t\tif (list_data[vector_index].length == 0) {\n+\t\t\t\tstate.definition_levels.push_back(max_define);\n+\t\t\t\tstate.is_empty.push_back(true);\n+\t\t\t} else {\n+\t\t\t\tstate.definition_levels.push_back(PARQUET_DEFINE_VALID);\n+\t\t\t\tstate.is_empty.push_back(false);\n+\t\t\t}\n+\t\t\tstate.repetition_levels.push_back(first_repeat_level);\n+\t\t\tfor (idx_t k = 1; k < list_data[vector_index].length; k++) {\n+\t\t\t\tstate.repetition_levels.push_back(max_repeat + 1);\n+\t\t\t\tstate.definition_levels.push_back(PARQUET_DEFINE_VALID);\n+\t\t\t\tstate.is_empty.push_back(false);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tstate.definition_levels.push_back(max_define - 1);\n+\t\t\tstate.repetition_levels.push_back(first_repeat_level);\n+\t\t\tstate.is_empty.push_back(true);\n+\t\t}\n+\t\tvector_index++;\n+\t}\n+\tstate.parent_index += vcount;\n+\n+\tauto &list_child = ListVector::GetEntry(vector);\n+\tauto list_count = ListVector::GetListSize(vector);\n+\tchild_writer->Prepare(*state.child_state, &state_p, list_child, list_count);\n+}\n+\n+void ListColumnWriter::BeginWrite(ColumnWriterState &state_p) {\n+\tauto &state = (ListColumnWriterState &)state_p;\n+\tchild_writer->BeginWrite(*state.child_state);\n+}\n+\n+void ListColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count) {\n+\tauto &state = (ListColumnWriterState &)state_p;\n+\n+\tauto &list_child = ListVector::GetEntry(vector);\n+\tauto list_count = ListVector::GetListSize(vector);\n+\tchild_writer->Write(*state.child_state, list_child, list_count);\n+}\n+\n+void ListColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {\n+\tauto &state = (ListColumnWriterState &)state_p;\n+\tchild_writer->FinalizeWrite(*state.child_state);\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// Create Column Writer\n+//===--------------------------------------------------------------------===//\n+unique_ptr<ColumnWriter> ColumnWriter::CreateWriterRecursive(vector<duckdb_parquet::format::SchemaElement> &schemas,\n+                                                             ParquetWriter &writer, const LogicalType &type,\n+                                                             const string &name, idx_t max_repeat, idx_t max_define) {\n+\tidx_t schema_idx = schemas.size();\n+\tif (type.id() == LogicalTypeId::STRUCT) {\n+\t\tauto &child_types = StructType::GetChildTypes(type);\n+\t\t// set up the schema element for this struct\n+\t\tduckdb_parquet::format::SchemaElement schema_element;\n+\t\tschema_element.repetition_type = FieldRepetitionType::OPTIONAL;\n+\t\tschema_element.num_children = child_types.size();\n+\t\tschema_element.__isset.num_children = true;\n+\t\tschema_element.__isset.type = false;\n+\t\tschema_element.__isset.repetition_type = true;\n+\t\tschema_element.name = name;\n+\t\tschemas.push_back(move(schema_element));\n+\t\t// construct the child types recursively\n+\t\tvector<unique_ptr<ColumnWriter>> child_writers;\n+\t\tchild_writers.reserve(child_types.size());\n+\t\tfor (auto &child_type : child_types) {\n+\t\t\tchild_writers.push_back(CreateWriterRecursive(schemas, writer, child_type.second, child_type.first,\n+\t\t\t                                              max_repeat, max_define + 1));\n+\t\t}\n+\t\treturn make_unique<StructColumnWriter>(writer, schema_idx, max_repeat, max_define, move(child_writers));\n+\t}\n+\tif (type.id() == LogicalTypeId::LIST) {\n+\t\tauto &child_type = ListType::GetChildType(type);\n+\t\t// set up the two schema elements for the list\n+\t\t// for some reason we only set the converted type in the OPTIONAL element\n+\t\t// first an OPTIONAL element\n+\t\tduckdb_parquet::format::SchemaElement optional_element;\n+\t\toptional_element.repetition_type = FieldRepetitionType::OPTIONAL;\n+\t\toptional_element.num_children = 1;\n+\t\toptional_element.converted_type = ConvertedType::LIST;\n+\t\toptional_element.__isset.num_children = true;\n+\t\toptional_element.__isset.type = false;\n+\t\toptional_element.__isset.repetition_type = true;\n+\t\toptional_element.__isset.converted_type = true;\n+\t\toptional_element.name = name;\n+\t\tschemas.push_back(move(optional_element));\n+\n+\t\t// then a REPEATED element\n+\t\tduckdb_parquet::format::SchemaElement repeated_element;\n+\t\trepeated_element.repetition_type = FieldRepetitionType::REPEATED;\n+\t\trepeated_element.num_children = 1;\n+\t\trepeated_element.__isset.num_children = true;\n+\t\trepeated_element.__isset.type = false;\n+\t\trepeated_element.__isset.repetition_type = true;\n+\t\trepeated_element.name = \"list\";\n+\t\tschemas.push_back(move(repeated_element));\n+\n+\t\tauto child_writer = CreateWriterRecursive(schemas, writer, child_type, \"child\", max_repeat + 1, max_define + 2);\n+\t\treturn make_unique<ListColumnWriter>(writer, schema_idx, max_repeat, max_define, move(child_writer));\n+\t}\n+\tduckdb_parquet::format::SchemaElement schema_element;\n+\tschema_element.type = ParquetWriter::DuckDBTypeToParquetType(type);\n+\tschema_element.repetition_type = FieldRepetitionType::OPTIONAL;\n+\tschema_element.num_children = 0;\n+\tschema_element.__isset.num_children = true;\n+\tschema_element.__isset.type = true;\n+\tschema_element.__isset.repetition_type = true;\n+\tschema_element.name = name;\n+\tschema_element.__isset.converted_type =\n+\t    ParquetWriter::DuckDBTypeToConvertedType(type, schema_element.converted_type);\n+\tschemas.push_back(move(schema_element));\n+\n+\tswitch (type.id()) {\n+\tcase LogicalTypeId::BOOLEAN:\n+\t\treturn make_unique<BooleanColumnWriter>(writer, schema_idx, max_repeat, max_define);\n+\tcase LogicalTypeId::TINYINT:\n+\t\treturn make_unique<StandardColumnWriter<int8_t, int32_t>>(writer, schema_idx, max_repeat, max_define);\n+\tcase LogicalTypeId::SMALLINT:\n+\t\treturn make_unique<StandardColumnWriter<int16_t, int32_t>>(writer, schema_idx, max_repeat, max_define);\n+\tcase LogicalTypeId::INTEGER:\n+\tcase LogicalTypeId::DATE:\n+\t\treturn make_unique<StandardColumnWriter<int32_t, int32_t>>(writer, schema_idx, max_repeat, max_define);\n+\tcase LogicalTypeId::BIGINT:\n+\tcase LogicalTypeId::TIMESTAMP:\n+\tcase LogicalTypeId::TIMESTAMP_MS:\n+\t\treturn make_unique<StandardColumnWriter<int64_t, int64_t>>(writer, schema_idx, max_repeat, max_define);\n+\tcase LogicalTypeId::HUGEINT:\n+\t\treturn make_unique<StandardColumnWriter<hugeint_t, double, ParquetHugeintOperator>>(writer, schema_idx,\n+\t\t                                                                                    max_repeat, max_define);\n+\tcase LogicalTypeId::TIMESTAMP_NS:\n+\t\treturn make_unique<StandardColumnWriter<int64_t, int64_t, ParquetTimestampNSOperator>>(writer, schema_idx,\n+\t\t                                                                                       max_repeat, max_define);\n+\tcase LogicalTypeId::TIMESTAMP_SEC:\n+\t\treturn make_unique<StandardColumnWriter<int64_t, int64_t, ParquetTimestampSOperator>>(writer, schema_idx,\n+\t\t                                                                                      max_repeat, max_define);\n+\tcase LogicalTypeId::UTINYINT:\n+\t\treturn make_unique<StandardColumnWriter<uint8_t, int32_t>>(writer, schema_idx, max_repeat, max_define);\n+\tcase LogicalTypeId::USMALLINT:\n+\t\treturn make_unique<StandardColumnWriter<uint16_t, int32_t>>(writer, schema_idx, max_repeat, max_define);\n+\tcase LogicalTypeId::UINTEGER:\n+\t\treturn make_unique<StandardColumnWriter<uint32_t, uint32_t>>(writer, schema_idx, max_repeat, max_define);\n+\tcase LogicalTypeId::UBIGINT:\n+\t\treturn make_unique<StandardColumnWriter<uint64_t, uint64_t>>(writer, schema_idx, max_repeat, max_define);\n+\tcase LogicalTypeId::FLOAT:\n+\t\treturn make_unique<StandardColumnWriter<float, float>>(writer, schema_idx, max_repeat, max_define);\n+\tcase LogicalTypeId::DOUBLE:\n+\t\treturn make_unique<StandardColumnWriter<double, double>>(writer, schema_idx, max_repeat, max_define);\n+\tcase LogicalTypeId::DECIMAL:\n+\t\treturn make_unique<DecimalColumnWriter>(writer, schema_idx, max_repeat, max_define);\n+\tcase LogicalTypeId::BLOB:\n+\tcase LogicalTypeId::VARCHAR:\n+\t\treturn make_unique<StringColumnWriter>(writer, schema_idx, max_repeat, max_define);\n+\tdefault:\n+\t\tthrow InternalException(\"Unsupported type in Parquet writer\");\n+\t}\n+}\n+\n+} // namespace duckdb\ndiff --git a/extension/parquet/include/column_writer.hpp b/extension/parquet/include/column_writer.hpp\nnew file mode 100644\nindex 000000000000..a412ec4140c0\n--- /dev/null\n+++ b/extension/parquet/include/column_writer.hpp\n@@ -0,0 +1,82 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// column_writer.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb.hpp\"\n+#include \"parquet_types.h\"\n+\n+namespace duckdb {\n+class BufferedSerializer;\n+class ParquetWriter;\n+class ColumnWriterPageState;\n+\n+class ColumnWriterState {\n+public:\n+\tvirtual ~ColumnWriterState();\n+\n+\tvector<uint16_t> definition_levels;\n+\tvector<uint16_t> repetition_levels;\n+\tvector<bool> is_empty;\n+};\n+\n+class ColumnWriter {\n+\t//! We limit the uncompressed page size to 100MB\n+\t// The max size in Parquet is 2GB, but we choose a more conservative limit\n+\tstatic constexpr const idx_t MAX_UNCOMPRESSED_PAGE_SIZE = 100000000;\n+\n+public:\n+\tColumnWriter(ParquetWriter &writer, idx_t schema_idx, idx_t max_repeat, idx_t max_define);\n+\tvirtual ~ColumnWriter();\n+\n+\tParquetWriter &writer;\n+\tidx_t schema_idx;\n+\tidx_t max_repeat;\n+\tidx_t max_define;\n+\n+public:\n+\t//! Create the column writer for a specific type recursively\n+\tstatic unique_ptr<ColumnWriter> CreateWriterRecursive(vector<duckdb_parquet::format::SchemaElement> &schemas,\n+\t                                                      ParquetWriter &writer, const LogicalType &type,\n+\t                                                      const string &name, idx_t max_repeat = 0,\n+\t                                                      idx_t max_define = 1);\n+\n+\tvirtual unique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::format::RowGroup &row_group,\n+\t                                                           vector<string> schema_path);\n+\tvirtual void Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count);\n+\n+\tvirtual void BeginWrite(ColumnWriterState &state);\n+\tvirtual void Write(ColumnWriterState &state, Vector &vector, idx_t count);\n+\tvirtual void FinalizeWrite(ColumnWriterState &state);\n+\n+protected:\n+\tvoid HandleDefineLevels(ColumnWriterState &state, ColumnWriterState *parent, ValidityMask &validity, idx_t count,\n+\t                        uint16_t define_value, uint16_t null_value);\n+\tvoid HandleRepeatLevels(ColumnWriterState &state_p, ColumnWriterState *parent, idx_t count, idx_t max_repeat);\n+\n+\tvoid WriteLevels(Serializer &temp_writer, const vector<uint16_t> &levels, idx_t max_value, idx_t start_offset,\n+\t                 idx_t count);\n+\n+\tvoid NextPage(ColumnWriterState &state_p);\n+\tvoid FlushPage(ColumnWriterState &state_p);\n+\n+\t//! Retrieves the row size of a vector at the specified location. Only used for scalar types.\n+\tvirtual idx_t GetRowSize(Vector &vector, idx_t index) = 0;\n+\t//! Writes a (subset of a) vector to the specified serializer. Only used for scalar types.\n+\tvirtual void WriteVector(Serializer &temp_writer, ColumnWriterPageState *page_state, Vector &vector,\n+\t                         idx_t chunk_start, idx_t chunk_end) = 0;\n+\t//! Initialize the writer for a specific page. Only used for scalar types.\n+\tvirtual unique_ptr<ColumnWriterPageState> InitializePageState();\n+\t//! Flushes the writer for a specific page. Only used for scalar types.\n+\tvirtual void FlushPageState(Serializer &temp_writer, ColumnWriterPageState *state);\n+\n+\tvoid CompressPage(BufferedSerializer &temp_writer, size_t &compressed_size, data_ptr_t &compressed_data,\n+\t                  unique_ptr<data_t[]> &compressed_buf);\n+};\n+\n+} // namespace duckdb\ndiff --git a/extension/parquet/include/parquet_rle_bp_decoder.hpp b/extension/parquet/include/parquet_rle_bp_decoder.hpp\nindex 5bca21658db6..3760ec10e316 100644\n--- a/extension/parquet/include/parquet_rle_bp_decoder.hpp\n+++ b/extension/parquet/include/parquet_rle_bp_decoder.hpp\n@@ -1,5 +1,19 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// parquet_rle_bp_decoder.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n #pragma once\n+\n+#include \"parquet_types.h\"\n+#include \"thrift_tools.hpp\"\n+#include \"resizable_buffer.hpp\"\n+\n namespace duckdb {\n+\n class RleBpDecoder {\n public:\n \t/// Create a decoder object. buffer/buffer_len is the decoded data.\n@@ -47,6 +61,17 @@ class RleBpDecoder {\n \t\t}\n \t}\n \n+\tstatic uint8_t ComputeBitWidth(idx_t val) {\n+\t\tif (val == 0) {\n+\t\t\treturn 0;\n+\t\t}\n+\t\tuint8_t ret = 1;\n+\t\twhile (((idx_t)(1 << ret) - 1) < val) {\n+\t\t\tret++;\n+\t\t}\n+\t\treturn ret;\n+\t}\n+\n private:\n \tByteBuffer buffer_;\n \ndiff --git a/extension/parquet/include/parquet_writer.hpp b/extension/parquet/include/parquet_writer.hpp\nindex fc0e632cf9eb..901c85532a33 100644\n--- a/extension/parquet/include/parquet_writer.hpp\n+++ b/extension/parquet/include/parquet_writer.hpp\n@@ -18,6 +18,7 @@\n #endif\n \n #include \"parquet_types.h\"\n+#include \"column_writer.hpp\"\n #include \"thrift/protocol/TCompactProtocol.h\"\n \n namespace duckdb {\n@@ -25,6 +26,10 @@ class FileSystem;\n class FileOpener;\n \n class ParquetWriter {\n+\tfriend class ColumnWriter;\n+\tfriend class ListColumnWriter;\n+\tfriend class StructColumnWriter;\n+\n public:\n \tParquetWriter(FileSystem &fs, string file_name, FileOpener *file_opener, vector<LogicalType> types,\n \t              vector<string> names, duckdb_parquet::format::CompressionCodec::type codec);\n@@ -33,6 +38,10 @@ class ParquetWriter {\n \tvoid Flush(ChunkCollection &buffer);\n \tvoid Finalize();\n \n+\tstatic duckdb_parquet::format::Type::type DuckDBTypeToParquetType(const LogicalType &duckdb_type);\n+\tstatic bool DuckDBTypeToConvertedType(const LogicalType &duckdb_type,\n+\t                                      duckdb_parquet::format::ConvertedType::type &result);\n+\n private:\n \tstring file_name;\n \tvector<LogicalType> sql_types;\n@@ -43,6 +52,8 @@ class ParquetWriter {\n \tshared_ptr<duckdb_apache::thrift::protocol::TProtocol> protocol;\n \tduckdb_parquet::format::FileMetaData file_meta_data;\n \tstd::mutex lock;\n+\n+\tvector<unique_ptr<ColumnWriter>> column_writers;\n };\n \n } // namespace duckdb\ndiff --git a/extension/parquet/include/struct_column_reader.hpp b/extension/parquet/include/struct_column_reader.hpp\nindex a6ad89d92a01..3dc75d4cc9c7 100644\n--- a/extension/parquet/include/struct_column_reader.hpp\n+++ b/extension/parquet/include/struct_column_reader.hpp\n@@ -16,56 +16,20 @@ namespace duckdb {\n class StructColumnReader : public ColumnReader {\n public:\n \tStructColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t schema_idx_p,\n-\t                   idx_t max_define_p, idx_t max_repeat_p, vector<unique_ptr<ColumnReader>> child_readers_p)\n-\t    : ColumnReader(reader, move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p),\n-\t      child_readers(move(child_readers_p)) {\n-\t\tD_ASSERT(type.id() == LogicalTypeId::STRUCT);\n-\t\tD_ASSERT(!StructType::GetChildTypes(type).empty());\n-\t};\n+\t                   idx_t max_define_p, idx_t max_repeat_p, vector<unique_ptr<ColumnReader>> child_readers_p);\n \n-\tColumnReader *GetChildReader(idx_t child_idx) {\n-\t\treturn child_readers[child_idx].get();\n-\t}\n-\n-\tvoid InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) override {\n-\t\tfor (auto &child : child_readers) {\n-\t\t\tchild->InitializeRead(columns, protocol_p);\n-\t\t}\n-\t}\n-\n-\tidx_t Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,\n-\t           Vector &result) override {\n-\t\tauto &struct_entries = StructVector::GetEntries(result);\n-\t\tD_ASSERT(StructType::GetChildTypes(Type()).size() == struct_entries.size());\n-\n-\t\tidx_t read_count = num_values;\n-\t\tfor (idx_t i = 0; i < struct_entries.size(); i++) {\n-\t\t\tauto child_num_values =\n-\t\t\t    child_readers[i]->Read(num_values, filter, define_out, repeat_out, *struct_entries[i]);\n-\t\t\tif (i == 0) {\n-\t\t\t\tread_count = child_num_values;\n-\t\t\t} else if (read_count != child_num_values) {\n-\t\t\t\tthrow std::runtime_error(\"Struct child row count mismatch\");\n-\t\t\t}\n-\t\t}\n+\tvector<unique_ptr<ColumnReader>> child_readers;\n \n-\t\treturn read_count;\n-\t}\n+public:\n+\tColumnReader *GetChildReader(idx_t child_idx);\n \n-\tvirtual void Skip(idx_t num_values) override {\n-\t\tD_ASSERT(0);\n-\t}\n+\tvoid InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) override;\n \n-\tidx_t GroupRowsAvailable() override {\n-\t\tfor (idx_t i = 0; i < child_readers.size(); i++) {\n-\t\t\tif (child_readers[i]->Type().id() != LogicalTypeId::LIST) {\n-\t\t\t\treturn child_readers[i]->GroupRowsAvailable();\n-\t\t\t}\n-\t\t}\n-\t\treturn child_readers[0]->GroupRowsAvailable();\n-\t}\n+\tidx_t Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,\n+\t           Vector &result) override;\n \n-\tvector<unique_ptr<ColumnReader>> child_readers;\n+\tvoid Skip(idx_t num_values) override;\n+\tidx_t GroupRowsAvailable() override;\n };\n \n } // namespace duckdb\ndiff --git a/extension/parquet/parquet-extension.cpp b/extension/parquet/parquet-extension.cpp\nindex 186c665b879d..faa5dda94375 100644\n--- a/extension/parquet/parquet-extension.cpp\n+++ b/extension/parquet/parquet-extension.cpp\n@@ -391,6 +391,7 @@ struct ParquetWriteBindData : public FunctionData {\n \tstring file_name;\n \tvector<string> column_names;\n \tduckdb_parquet::format::CompressionCodec::type codec = duckdb_parquet::format::CompressionCodec::SNAPPY;\n+\tidx_t row_group_size = 100000;\n };\n \n struct ParquetWriteGlobalState : public GlobalFunctionData {\n@@ -410,7 +411,9 @@ unique_ptr<FunctionData> ParquetWriteBind(ClientContext &context, CopyInfo &info\n \tauto bind_data = make_unique<ParquetWriteBindData>();\n \tfor (auto &option : info.options) {\n \t\tauto loption = StringUtil::Lower(option.first);\n-\t\tif (loption == \"compression\" || loption == \"codec\") {\n+\t\tif (loption == \"row_group_size\" || loption == \"chunk_size\") {\n+\t\t\tbind_data->row_group_size = option.second[0].GetValue<uint64_t>();\n+\t\t} else if (loption == \"compression\" || loption == \"codec\") {\n \t\t\tif (!option.second.empty()) {\n \t\t\t\tauto roption = StringUtil::Lower(option.second[0].ToString());\n \t\t\t\tif (roption == \"uncompressed\") {\n@@ -449,14 +452,15 @@ unique_ptr<GlobalFunctionData> ParquetWriteInitializeGlobal(ClientContext &conte\n \treturn move(global_state);\n }\n \n-void ParquetWriteSink(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,\n+void ParquetWriteSink(ClientContext &context, FunctionData &bind_data_p, GlobalFunctionData &gstate,\n                       LocalFunctionData &lstate, DataChunk &input) {\n+\tauto &bind_data = (ParquetWriteBindData &)bind_data_p;\n \tauto &global_state = (ParquetWriteGlobalState &)gstate;\n \tauto &local_state = (ParquetWriteLocalState &)lstate;\n \n \t// append data to the local (buffered) chunk collection\n \tlocal_state.buffer->Append(input);\n-\tif (local_state.buffer->Count() > 100000) {\n+\tif (local_state.buffer->Count() > bind_data.row_group_size) {\n \t\t// if the chunk collection exceeds a certain size we flush it to the parquet file\n \t\tglobal_state.writer->Flush(*local_state.buffer);\n \t\t// and reset the buffer\ndiff --git a/extension/parquet/parquet_config.py b/extension/parquet/parquet_config.py\nindex 4066f7aed5a2..e8f614d90a32 100644\n--- a/extension/parquet/parquet_config.py\n+++ b/extension/parquet/parquet_config.py\n@@ -2,7 +2,7 @@\n # list all include directories\n include_directories = [os.path.sep.join(x.split('/')) for x in ['extension/parquet/include', 'third_party/parquet', 'third_party/snappy', 'third_party/thrift', 'third_party/zstd/include']]\n # source files\n-source_files = [os.path.sep.join(x.split('/')) for x in ['extension/parquet/parquet-extension.cpp', 'third_party/parquet/parquet_constants.cpp',  'third_party/parquet/parquet_types.cpp',  'third_party/thrift/thrift/protocol/TProtocol.cpp',  'third_party/thrift/thrift/transport/TTransportException.cpp',  'third_party/thrift/thrift/transport/TBufferTransports.cpp',  'third_party/snappy/snappy.cc',  'third_party/snappy/snappy-sinksource.cc']]\n+source_files = [os.path.sep.join(x.split('/')) for x in ['extension/parquet/parquet-extension.cpp', 'extension/parquet/column_writer.cpp', 'third_party/parquet/parquet_constants.cpp',  'third_party/parquet/parquet_types.cpp',  'third_party/thrift/thrift/protocol/TProtocol.cpp',  'third_party/thrift/thrift/transport/TTransportException.cpp',  'third_party/thrift/thrift/transport/TBufferTransports.cpp',  'third_party/snappy/snappy.cc',  'third_party/snappy/snappy-sinksource.cc']]\n # zstd\n source_files += [os.path.sep.join(x.split('/')) for x in ['third_party/zstd/decompress/zstd_ddict.cpp', 'third_party/zstd/decompress/huf_decompress.cpp', 'third_party/zstd/decompress/zstd_decompress.cpp', 'third_party/zstd/decompress/zstd_decompress_block.cpp', 'third_party/zstd/common/entropy_common.cpp', 'third_party/zstd/common/fse_decompress.cpp', 'third_party/zstd/common/zstd_common.cpp', 'third_party/zstd/common/error_private.cpp', 'third_party/zstd/common/xxhash.cpp']]\n source_files += [os.path.sep.join(x.split('/')) for x in ['third_party/zstd/compress/fse_compress.cpp', 'third_party/zstd/compress/hist.cpp', 'third_party/zstd/compress/huf_compress.cpp', 'third_party/zstd/compress/zstd_compress.cpp', 'third_party/zstd/compress/zstd_compress_literals.cpp', 'third_party/zstd/compress/zstd_compress_sequences.cpp', 'third_party/zstd/compress/zstd_compress_superblock.cpp', 'third_party/zstd/compress/zstd_double_fast.cpp', 'third_party/zstd/compress/zstd_fast.cpp', 'third_party/zstd/compress/zstd_lazy.cpp', 'third_party/zstd/compress/zstd_ldm.cpp', 'third_party/zstd/compress/zstd_opt.cpp']]\ndiff --git a/extension/parquet/parquet_reader.cpp b/extension/parquet/parquet_reader.cpp\nindex 1cd41f618444..114447bceaf9 100644\n--- a/extension/parquet/parquet_reader.cpp\n+++ b/extension/parquet/parquet_reader.cpp\n@@ -257,8 +257,12 @@ unique_ptr<ColumnReader> ParquetReader::CreateReaderRecursive(const FileMetaData\n \t\tD_ASSERT(!child_types.empty());\n \t\tunique_ptr<ColumnReader> result;\n \t\tLogicalType result_type;\n+\n+\t\tbool is_repeated = s_ele.repetition_type == FieldRepetitionType::REPEATED;\n+\t\tbool is_list = s_ele.__isset.converted_type && s_ele.converted_type == ConvertedType::LIST;\n+\t\tbool is_map = s_ele.__isset.converted_type && s_ele.converted_type == ConvertedType::MAP;\n \t\t// if we only have a single child no reason to create a struct ay\n-\t\tif (child_types.size() > 1 || depth == 0) {\n+\t\tif (child_types.size() > 1 || (!is_list && !is_map && !is_repeated)) {\n \t\t\tresult_type = LogicalType::STRUCT(move(child_types));\n \t\t\tresult = make_unique<StructColumnReader>(*this, result_type, s_ele, this_idx, max_define, max_repeat,\n \t\t\t                                         move(child_readers));\n@@ -267,7 +271,7 @@ unique_ptr<ColumnReader> ParquetReader::CreateReaderRecursive(const FileMetaData\n \t\t\tresult_type = child_types[0].second;\n \t\t\tresult = move(child_readers[0]);\n \t\t}\n-\t\tif (s_ele.repetition_type == FieldRepetitionType::REPEATED) {\n+\t\tif (is_repeated) {\n \t\t\tresult_type = LogicalType::LIST(result_type);\n \t\t\treturn make_unique<ListColumnReader>(*this, result_type, s_ele, this_idx, max_define, max_repeat,\n \t\t\t                                     move(result));\ndiff --git a/extension/parquet/parquet_writer.cpp b/extension/parquet/parquet_writer.cpp\nindex 18098b2f2de5..0653125df3d2 100644\n--- a/extension/parquet/parquet_writer.cpp\n+++ b/extension/parquet/parquet_writer.cpp\n@@ -10,25 +10,14 @@\n #include \"duckdb/main/connection.hpp\"\n #include \"duckdb/common/file_system.hpp\"\n #include \"duckdb/common/string_util.hpp\"\n-#include \"duckdb/common/types/date.hpp\"\n-#include \"duckdb/common/types/hugeint.hpp\"\n-#include \"duckdb/common/types/time.hpp\"\n-#include \"duckdb/common/types/timestamp.hpp\"\n #include \"duckdb/common/serializer/buffered_file_writer.hpp\"\n-#include \"duckdb/common/serializer/buffered_serializer.hpp\"\n #endif\n \n-#include \"snappy.h\"\n-#include \"miniz_wrapper.hpp\"\n-#include \"zstd.h\"\n-\n namespace duckdb {\n \n-using namespace duckdb_parquet;                   // NOLINT\n using namespace duckdb_apache::thrift;            // NOLINT\n using namespace duckdb_apache::thrift::protocol;  // NOLINT\n using namespace duckdb_apache::thrift::transport; // NOLINT\n-using namespace duckdb_miniz;                     // NOLINT\n \n using duckdb_parquet::format::CompressionCodec;\n using duckdb_parquet::format::ConvertedType;\n@@ -63,7 +52,7 @@ class MyTransport : public TTransport {\n \tSerializer &serializer;\n };\n \n-static Type::type DuckDBTypeToParquetType(const LogicalType &duckdb_type) {\n+Type::type ParquetWriter::DuckDBTypeToParquetType(const LogicalType &duckdb_type) {\n \tswitch (duckdb_type.id()) {\n \tcase LogicalTypeId::BOOLEAN:\n \t\treturn Type::BOOLEAN;\n@@ -99,7 +88,7 @@ static Type::type DuckDBTypeToParquetType(const LogicalType &duckdb_type) {\n \t}\n }\n \n-static bool DuckDBTypeToConvertedType(const LogicalType &duckdb_type, ConvertedType::type &result) {\n+bool ParquetWriter::DuckDBTypeToConvertedType(const LogicalType &duckdb_type, ConvertedType::type &result) {\n \tswitch (duckdb_type.id()) {\n \tcase LogicalTypeId::TINYINT:\n \t\tresult = ConvertedType::INT_8;\n@@ -144,75 +133,9 @@ static bool DuckDBTypeToConvertedType(const LogicalType &duckdb_type, ConvertedT\n \t}\n }\n \n-static void VarintEncode(uint32_t val, Serializer &ser) {\n-\tdo {\n-\t\tuint8_t byte = val & 127;\n-\t\tval >>= 7;\n-\t\tif (val != 0) {\n-\t\t\tbyte |= 128;\n-\t\t}\n-\t\tser.Write<uint8_t>(byte);\n-\t} while (val != 0);\n-}\n-\n-static uint8_t GetVarintSize(uint32_t val) {\n-\tuint8_t res = 0;\n-\tdo {\n-\t\tuint8_t byte = val & 127;\n-\t\tval >>= 7;\n-\t\tif (val != 0) {\n-\t\t\tbyte |= 128;\n-\t\t}\n-\t\tres++;\n-\t} while (val != 0);\n-\treturn res;\n-}\n-\n-struct ParquetCastOperator {\n-\ttemplate <class SRC, class TGT>\n-\tstatic TGT Operation(SRC input) {\n-\t\treturn TGT(input);\n-\t}\n-};\n-\n-struct ParquetTimestampNSOperator {\n-\ttemplate <class SRC, class TGT>\n-\tstatic TGT Operation(SRC input) {\n-\t\treturn Timestamp::FromEpochNanoSeconds(input).value;\n-\t}\n-};\n-\n-struct ParquetTimestampSOperator {\n-\ttemplate <class SRC, class TGT>\n-\tstatic TGT Operation(SRC input) {\n-\t\treturn Timestamp::FromEpochSeconds(input).value;\n-\t}\n-};\n-\n-struct ParquetHugeintOperator {\n-\ttemplate <class SRC, class TGT>\n-\tstatic TGT Operation(SRC input) {\n-\t\treturn Hugeint::Cast<double>(input);\n-\t}\n-};\n-\n-template <class SRC, class TGT, class OP = ParquetCastOperator>\n-static void TemplatedWritePlain(Vector &col, idx_t length, ValidityMask &mask, Serializer &ser) {\n-\tauto *ptr = FlatVector::GetData<SRC>(col);\n-\tfor (idx_t r = 0; r < length; r++) {\n-\t\tif (mask.RowIsValid(r)) {\n-\t\t\tser.Write<TGT>(OP::template Operation<SRC, TGT>(ptr[r]));\n-\t\t}\n-\t}\n-}\n-\n ParquetWriter::ParquetWriter(FileSystem &fs, string file_name_p, FileOpener *file_opener_p, vector<LogicalType> types_p,\n                              vector<string> names_p, CompressionCodec::type codec)\n     : file_name(move(file_name_p)), sql_types(move(types_p)), column_names(move(names_p)), codec(codec) {\n-#if STANDARD_VECTOR_SIZE < 64\n-\tthrow NotImplementedException(\"Parquet writer is not supported for vector sizes < 64\");\n-#endif\n-\n \t// initialize the file writer\n \twriter = make_unique<BufferedFileWriter>(\n \t    fs, file_name.c_str(), FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE_NEW, file_opener_p);\n@@ -227,24 +150,18 @@ ParquetWriter::ParquetWriter(FileSystem &fs, string file_name_p, FileOpener *fil\n \tfile_meta_data.__isset.created_by = true;\n \tfile_meta_data.created_by = \"DuckDB\";\n \n-\tfile_meta_data.schema.resize(sql_types.size() + 1);\n+\tfile_meta_data.schema.resize(1);\n \n \t// populate root schema object\n \tfile_meta_data.schema[0].name = \"duckdb_schema\";\n \tfile_meta_data.schema[0].num_children = sql_types.size();\n \tfile_meta_data.schema[0].__isset.num_children = true;\n+\tfile_meta_data.schema[0].repetition_type = duckdb_parquet::format::FieldRepetitionType::REQUIRED;\n+\tfile_meta_data.schema[0].__isset.repetition_type = true;\n \n \tfor (idx_t i = 0; i < sql_types.size(); i++) {\n-\t\tauto &schema_element = file_meta_data.schema[i + 1];\n-\n-\t\tschema_element.type = DuckDBTypeToParquetType(sql_types[i]);\n-\t\tschema_element.repetition_type = FieldRepetitionType::OPTIONAL;\n-\t\tschema_element.num_children = 0;\n-\t\tschema_element.__isset.num_children = true;\n-\t\tschema_element.__isset.type = true;\n-\t\tschema_element.__isset.repetition_type = true;\n-\t\tschema_element.name = column_names[i];\n-\t\tschema_element.__isset.converted_type = DuckDBTypeToConvertedType(sql_types[i], schema_element.converted_type);\n+\t\tcolumn_writers.push_back(\n+\t\t    ColumnWriter::CreateWriterRecursive(file_meta_data.schema, *this, sql_types[i], column_names[i]));\n \t}\n }\n \n@@ -256,217 +173,26 @@ void ParquetWriter::Flush(ChunkCollection &buffer) {\n \n \t// set up a new row group for this chunk collection\n \tParquetRowGroup row_group;\n-\trow_group.num_rows = 0;\n+\trow_group.num_rows = buffer.Count();\n \trow_group.file_offset = writer->GetTotalWritten();\n \trow_group.__isset.file_offset = true;\n-\trow_group.columns.resize(buffer.ColumnCount());\n \n \t// iterate over each of the columns of the chunk collection and write them\n-\tfor (idx_t i = 0; i < buffer.ColumnCount(); i++) {\n-\t\t// we start off by writing everything into a temporary buffer\n-\t\t// this is necessary to (1) know the total written size, and (2) to compress it afterwards\n-\t\tBufferedSerializer temp_writer;\n-\n-\t\t// set up some metadata\n-\t\tPageHeader hdr;\n-\t\thdr.compressed_page_size = 0;\n-\t\thdr.uncompressed_page_size = 0;\n-\t\thdr.type = PageType::DATA_PAGE;\n-\t\thdr.__isset.data_page_header = true;\n-\n-\t\thdr.data_page_header.num_values = buffer.Count();\n-\t\thdr.data_page_header.encoding = Encoding::PLAIN;\n-\t\thdr.data_page_header.definition_level_encoding = Encoding::RLE;\n-\t\thdr.data_page_header.repetition_level_encoding = Encoding::BIT_PACKED;\n-\t\t// hdr.data_page_header.statistics.__isset.max\n-\t\t// hdr.data_page_header.statistics.max\n-\n-\t\t// record the current offset of the writer into the file\n-\t\t// this is the starting position of the current page\n-\t\tauto start_offset = writer->GetTotalWritten();\n-\n-\t\t// write the definition levels (i.e. the inverse of the nullmask)\n-\t\t// we always bit pack everything\n-\n-\t\t// first figure out how many bytes we need (1 byte per 8 rows, rounded up)\n-\t\tauto define_byte_count = (buffer.Count() + 7) / 8;\n-\t\t// we need to set up the count as a varint, plus an added marker for the RLE scheme\n-\t\t// for this marker we shift the count left 1 and set low bit to 1 to indicate bit packed literals\n-\t\tuint32_t define_header = (define_byte_count << 1) | 1;\n-\t\tuint32_t define_size = GetVarintSize(define_header) + define_byte_count;\n-\n-\t\t// we write the actual definitions into the temp_writer for now\n-\t\ttemp_writer.Write<uint32_t>(define_size);\n-\t\tVarintEncode(define_header, temp_writer);\n-\n-\t\tfor (auto &chunk : buffer.Chunks()) {\n-\t\t\tauto &validity = FlatVector::Validity(chunk->data[i]);\n-\t\t\tauto validity_data = validity.GetData();\n-\t\t\tauto chunk_define_byte_count = (chunk->size() + 7) / 8;\n-\t\t\tif (!validity_data) {\n-\t\t\t\tValidityMask nop_mask(chunk->size());\n-\t\t\t\ttemp_writer.WriteData((const_data_ptr_t)nop_mask.GetData(), chunk_define_byte_count);\n-\t\t\t} else {\n-\t\t\t\t// write the bits of the nullmask\n-\t\t\t\ttemp_writer.WriteData((const_data_ptr_t)validity_data, chunk_define_byte_count);\n-\t\t\t}\n+\tauto &chunks = buffer.Chunks();\n+\tD_ASSERT(buffer.ColumnCount() == column_writers.size());\n+\tfor (idx_t col_idx = 0; col_idx < buffer.ColumnCount(); col_idx++) {\n+\t\tvector<string> schema_path;\n+\t\tauto write_state = column_writers[col_idx]->InitializeWriteState(row_group, move(schema_path));\n+\t\tfor (idx_t chunk_idx = 0; chunk_idx < chunks.size(); chunk_idx++) {\n+\t\t\tcolumn_writers[col_idx]->Prepare(*write_state, nullptr, chunks[chunk_idx]->data[col_idx],\n+\t\t\t                                 chunks[chunk_idx]->size());\n \t\t}\n-\n-\t\t// now write the actual payload: we write this as PLAIN values (for now? possibly for ever?)\n-\t\tfor (auto &chunk : buffer.Chunks()) {\n-\t\t\tauto &input = *chunk;\n-\t\t\tauto &input_column = input.data[i];\n-\t\t\tauto &mask = FlatVector::Validity(input_column);\n-\n-\t\t\t// write actual payload data\n-\t\t\tswitch (sql_types[i].id()) {\n-\t\t\tcase LogicalTypeId::BOOLEAN: {\n-\t\t\t\tauto *ptr = FlatVector::GetData<bool>(input_column);\n-\t\t\t\tuint8_t byte = 0;\n-\t\t\t\tuint8_t byte_pos = 0;\n-\t\t\t\tfor (idx_t r = 0; r < input.size(); r++) {\n-\t\t\t\t\tif (mask.RowIsValid(r)) { // only encode if non-null\n-\t\t\t\t\t\tbyte |= (ptr[r] & 1) << byte_pos;\n-\t\t\t\t\t\tbyte_pos++;\n-\n-\t\t\t\t\t\tif (byte_pos == 8) {\n-\t\t\t\t\t\t\ttemp_writer.Write<uint8_t>(byte);\n-\t\t\t\t\t\t\tbyte = 0;\n-\t\t\t\t\t\t\tbyte_pos = 0;\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\t// flush last byte if req\n-\t\t\t\tif (byte_pos > 0) {\n-\t\t\t\t\ttemp_writer.Write<uint8_t>(byte);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\t}\n-\t\t\tcase LogicalTypeId::TINYINT:\n-\t\t\t\tTemplatedWritePlain<int8_t, int32_t>(input_column, input.size(), mask, temp_writer);\n-\t\t\t\tbreak;\n-\t\t\tcase LogicalTypeId::SMALLINT:\n-\t\t\t\tTemplatedWritePlain<int16_t, int32_t>(input_column, input.size(), mask, temp_writer);\n-\t\t\t\tbreak;\n-\t\t\tcase LogicalTypeId::INTEGER:\n-\t\t\tcase LogicalTypeId::DATE:\n-\t\t\t\tTemplatedWritePlain<int32_t, int32_t>(input_column, input.size(), mask, temp_writer);\n-\t\t\t\tbreak;\n-\t\t\tcase LogicalTypeId::BIGINT:\n-\t\t\tcase LogicalTypeId::TIMESTAMP:\n-\t\t\tcase LogicalTypeId::TIMESTAMP_MS:\n-\t\t\t\tTemplatedWritePlain<int64_t, int64_t>(input_column, input.size(), mask, temp_writer);\n-\t\t\t\tbreak;\n-\t\t\tcase LogicalTypeId::HUGEINT:\n-\t\t\t\tTemplatedWritePlain<hugeint_t, double, ParquetHugeintOperator>(input_column, input.size(), mask,\n-\t\t\t\t                                                               temp_writer);\n-\t\t\t\tbreak;\n-\t\t\tcase LogicalTypeId::TIMESTAMP_NS:\n-\t\t\t\tTemplatedWritePlain<int64_t, int64_t, ParquetTimestampNSOperator>(input_column, input.size(), mask,\n-\t\t\t\t                                                                  temp_writer);\n-\t\t\t\tbreak;\n-\t\t\tcase LogicalTypeId::TIMESTAMP_SEC:\n-\t\t\t\tTemplatedWritePlain<int64_t, int64_t, ParquetTimestampSOperator>(input_column, input.size(), mask,\n-\t\t\t\t                                                                 temp_writer);\n-\t\t\t\tbreak;\n-\t\t\tcase LogicalTypeId::UTINYINT:\n-\t\t\t\tTemplatedWritePlain<uint8_t, int32_t>(input_column, input.size(), mask, temp_writer);\n-\t\t\t\tbreak;\n-\t\t\tcase LogicalTypeId::USMALLINT:\n-\t\t\t\tTemplatedWritePlain<uint16_t, int32_t>(input_column, input.size(), mask, temp_writer);\n-\t\t\t\tbreak;\n-\t\t\tcase LogicalTypeId::UINTEGER:\n-\t\t\t\tTemplatedWritePlain<uint32_t, uint32_t>(input_column, input.size(), mask, temp_writer);\n-\t\t\t\tbreak;\n-\t\t\tcase LogicalTypeId::UBIGINT:\n-\t\t\t\tTemplatedWritePlain<uint64_t, uint64_t>(input_column, input.size(), mask, temp_writer);\n-\t\t\t\tbreak;\n-\t\t\tcase LogicalTypeId::FLOAT:\n-\t\t\t\tTemplatedWritePlain<float, float>(input_column, input.size(), mask, temp_writer);\n-\t\t\t\tbreak;\n-\t\t\tcase LogicalTypeId::DECIMAL: {\n-\t\t\t\t// FIXME: fixed length byte array...\n-\t\t\t\tVector double_vec(LogicalType::DOUBLE);\n-\t\t\t\tVectorOperations::Cast(input_column, double_vec, input.size());\n-\t\t\t\tTemplatedWritePlain<double, double>(double_vec, input.size(), mask, temp_writer);\n-\t\t\t\tbreak;\n-\t\t\t}\n-\t\t\tcase LogicalTypeId::DOUBLE:\n-\t\t\t\tTemplatedWritePlain<double, double>(input_column, input.size(), mask, temp_writer);\n-\t\t\t\tbreak;\n-\t\t\tcase LogicalTypeId::BLOB:\n-\t\t\tcase LogicalTypeId::VARCHAR: {\n-\t\t\t\tauto *ptr = FlatVector::GetData<string_t>(input_column);\n-\t\t\t\tfor (idx_t r = 0; r < input.size(); r++) {\n-\t\t\t\t\tif (mask.RowIsValid(r)) {\n-\t\t\t\t\t\ttemp_writer.Write<uint32_t>(ptr[r].GetSize());\n-\t\t\t\t\t\ttemp_writer.WriteData((const_data_ptr_t)ptr[r].GetDataUnsafe(), ptr[r].GetSize());\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\t}\n-\t\t\tdefault:\n-\t\t\t\tthrow NotImplementedException((sql_types[i].ToString()));\n-\t\t\t}\n-\t\t}\n-\n-\t\t// now that we have finished writing the data we know the uncompressed size\n-\t\thdr.uncompressed_page_size = temp_writer.blob.size;\n-\n-\t\t// compress the data based\n-\t\tsize_t compressed_size;\n-\t\tdata_ptr_t compressed_data;\n-\t\tunique_ptr<data_t[]> compressed_buf;\n-\t\tswitch (codec) {\n-\t\tcase CompressionCodec::UNCOMPRESSED:\n-\t\t\tcompressed_size = temp_writer.blob.size;\n-\t\t\tcompressed_data = temp_writer.blob.data.get();\n-\t\t\tbreak;\n-\t\tcase CompressionCodec::SNAPPY: {\n-\t\t\tcompressed_size = duckdb_snappy::MaxCompressedLength(temp_writer.blob.size);\n-\t\t\tcompressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);\n-\t\t\tduckdb_snappy::RawCompress((const char *)temp_writer.blob.data.get(), temp_writer.blob.size,\n-\t\t\t                           (char *)compressed_buf.get(), &compressed_size);\n-\t\t\tcompressed_data = compressed_buf.get();\n-\t\t\tbreak;\n+\t\tcolumn_writers[col_idx]->BeginWrite(*write_state);\n+\t\tfor (idx_t chunk_idx = 0; chunk_idx < chunks.size(); chunk_idx++) {\n+\t\t\tcolumn_writers[col_idx]->Write(*write_state, chunks[chunk_idx]->data[col_idx], chunks[chunk_idx]->size());\n \t\t}\n-\t\tcase CompressionCodec::GZIP: {\n-\t\t\tMiniZStream s;\n-\t\t\tcompressed_size = s.MaxCompressedLength(temp_writer.blob.size);\n-\t\t\tcompressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);\n-\t\t\ts.Compress((const char *)temp_writer.blob.data.get(), temp_writer.blob.size, (char *)compressed_buf.get(),\n-\t\t\t           &compressed_size);\n-\t\t\tcompressed_data = compressed_buf.get();\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase CompressionCodec::ZSTD: {\n-\t\t\tcompressed_size = duckdb_zstd::ZSTD_compressBound(temp_writer.blob.size);\n-\t\t\tcompressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);\n-\t\t\tcompressed_size = duckdb_zstd::ZSTD_compress((void *)compressed_buf.get(), compressed_size,\n-\t\t\t                                             (const void *)temp_writer.blob.data.get(),\n-\t\t\t                                             temp_writer.blob.size, ZSTD_CLEVEL_DEFAULT);\n-\t\t\tcompressed_data = compressed_buf.get();\n-\t\t\tbreak;\n-\t\t}\n-\t\tdefault:\n-\t\t\tthrow InternalException(\"Unsupported codec for Parquet Writer\");\n-\t\t}\n-\n-\t\thdr.compressed_page_size = compressed_size;\n-\t\t// now finally write the data to the actual file\n-\t\thdr.write(protocol.get());\n-\t\twriter->WriteData(compressed_data, compressed_size);\n-\n-\t\tauto &column_chunk = row_group.columns[i];\n-\t\tcolumn_chunk.__isset.meta_data = true;\n-\t\tcolumn_chunk.meta_data.data_page_offset = start_offset;\n-\t\tcolumn_chunk.meta_data.total_compressed_size = writer->GetTotalWritten() - start_offset;\n-\t\tcolumn_chunk.meta_data.codec = codec;\n-\t\tcolumn_chunk.meta_data.path_in_schema.push_back(file_meta_data.schema[i + 1].name);\n-\t\tcolumn_chunk.meta_data.num_values = buffer.Count();\n-\t\tcolumn_chunk.meta_data.type = file_meta_data.schema[i + 1].type;\n+\t\tcolumn_writers[col_idx]->FinalizeWrite(*write_state);\n \t}\n-\trow_group.num_rows += buffer.Count();\n \n \t// append the row group to the file meta data\n \tfile_meta_data.row_groups.push_back(row_group);\ndiff --git a/src/common/types/validity_mask.cpp b/src/common/types/validity_mask.cpp\nindex ecd5dccd80bd..63c3e6b7645b 100644\n--- a/src/common/types/validity_mask.cpp\n+++ b/src/common/types/validity_mask.cpp\n@@ -76,13 +76,14 @@ void ValidityMask::Slice(const ValidityMask &other, idx_t offset) {\n \t\tInitialize(other);\n \t\treturn;\n \t}\n-\tInitialize(STANDARD_VECTOR_SIZE);\n+\tValidityMask new_mask(STANDARD_VECTOR_SIZE);\n \n // FIXME THIS NEEDS FIXING!\n #if 1\n \tfor (idx_t i = offset; i < STANDARD_VECTOR_SIZE; i++) {\n-\t\tSet(i - offset, other.RowIsValid(i));\n+\t\tnew_mask.Set(i - offset, other.RowIsValid(i));\n \t}\n+\tInitialize(new_mask);\n #else\n \t// first shift the \"whole\" units\n \tidx_t entire_units = offset / BITS_PER_VALUE;\n@@ -90,7 +91,7 @@ void ValidityMask::Slice(const ValidityMask &other, idx_t offset) {\n \tif (entire_units > 0) {\n \t\tidx_t validity_idx;\n \t\tfor (validity_idx = 0; validity_idx + entire_units < STANDARD_ENTRY_COUNT; validity_idx++) {\n-\t\t\tvalidity_mask[validity_idx] = other.validity_mask[validity_idx + entire_units];\n+\t\t\tnew_mask.validity_mask[validity_idx] = other.validity_mask[validity_idx + entire_units];\n \t\t}\n \t}\n \t// now we shift the remaining sub units\n@@ -105,15 +106,17 @@ void ValidityMask::Slice(const ValidityMask &other, idx_t offset) {\n \tif (sub_units > 0) {\n \t\tidx_t validity_idx;\n \t\tfor (validity_idx = 0; validity_idx + 1 < STANDARD_ENTRY_COUNT; validity_idx++) {\n-\t\t\tvalidity_mask[validity_idx] = (other.validity_mask[validity_idx] >> sub_units) |\n-\t\t\t                              (other.validity_mask[validity_idx + 1] << (BITS_PER_VALUE - sub_units));\n+\t\t\tnew_mask.validity_mask[validity_idx] =\n+\t\t\t    (other.validity_mask[validity_idx] >> sub_units) |\n+\t\t\t    (other.validity_mask[validity_idx + 1] << (BITS_PER_VALUE - sub_units));\n \t\t}\n-\t\tvalidity_mask[validity_idx] >>= sub_units;\n+\t\tnew_mask.validity_mask[validity_idx] >>= sub_units;\n \t}\n #ifdef DEBUG\n \tfor (idx_t i = offset; i < STANDARD_VECTOR_SIZE; i++) {\n-\t\tD_ASSERT(RowIsValid(i - offset) == other.RowIsValid(i));\n+\t\tD_ASSERT(new_mask.RowIsValid(i - offset) == other.RowIsValid(i));\n \t}\n+\tInitialize(new_mask);\n #endif\n #endif\n }\ndiff --git a/third_party/fastpforlib/bitpackinghelpers.h b/third_party/fastpforlib/bitpackinghelpers.h\nindex ccfde32c2456..3d3a092568cf 100644\n--- a/third_party/fastpforlib/bitpackinghelpers.h\n+++ b/third_party/fastpforlib/bitpackinghelpers.h\n@@ -7,6 +7,7 @@\n #pragma once\n #include \"bitpacking.h\"\n \n+\n #include <stdexcept>\n \n namespace duckdb_fastpforlib {\n",
  "test_patch": "diff --git a/test/parquet/test_parquet_reader.test b/test/parquet/test_parquet_reader.test\nindex 020d156cf836..8b5b1014afc6 100644\n--- a/test/parquet/test_parquet_reader.test\n+++ b/test/parquet/test_parquet_reader.test\n@@ -61,7 +61,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/manyrowgroups.parquet') limit 5\n 89\t\n 90\t\n \n-mode skip \n+mode skip\n \n query I\n SELECT * FROM parquet_scan('data/parquet-testing/map.parquet') limit 50;\n@@ -117,7 +117,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/map.parquet') limit 50;\n {key: [Content-Encoding, X-Frame-Options, Connection, Via, X-Xss-Protection, Content-Type, Date, X-Cache, Vary, Server, X-Cache-Lookup, X-Content-Type-Options, Content-Length], value: [gzip, SAMEORIGIN, keep-alive, 1.1 ip-10-1-1-216.ec2.internal (squid/4.10-20200322-r358ad2fdf), 1; mode=block, text/html;charset=utf-8, Sat, 30 Jan 2021 16:20:50 GMT, MISS from ip-10-1-1-216.ec2.internal, Accept-Encoding, nginx/1.10.3, HIT from ip-10-1-1-216.ec2.internal:3128, nosniff, 892]}\t\n {key: [Content-Encoding, X-Frame-Options, Connection, Via, X-Xss-Protection, Content-Type, Date, X-Cache, Vary, Server, X-Cache-Lookup, X-Content-Type-Options, Content-Length], value: [gzip, SAMEORIGIN, keep-alive, 1.1 ip-10-1-1-216.ec2.internal (squid/4.10-20200322-r358ad2fdf), 1; mode=block, text/html;charset=utf-8, Sat, 30 Jan 2021 16:20:53 GMT, MISS from ip-10-1-1-216.ec2.internal, Accept-Encoding, nginx/1.10.3, HIT from ip-10-1-1-216.ec2.internal:3128, nosniff, 891]}\t\n \n-mode unskip \n+mode unskip\n \n query I\n SELECT * FROM parquet_scan('data/parquet-testing/arrow/int32_decimal.parquet') limit 50;\n@@ -147,14 +147,14 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/int32_decimal.parquet') l\n 23.00\t\n 24.00\t\n \n-mode skip \n+mode skip\n \n query IIIIII\n SELECT * FROM parquet_scan('data/parquet-testing/arrow/nonnullable.impala.parquet') limit 50;\n ----\n 8\t[-1]\t[[-1, -2], []]\t{key: [k1], value: [-1]}\t[{key: [], value: []}, {key: [k1], value: [1]}, {key: [], value: []}, {key: [], value: []}]\t{a: -1, B: [-1], c: {D: [[{e: -1, f: nonnullable}]]}, G: {key: [], value: []}}\t\n \n-mode unskip \n+mode unskip\n \n query IIIIIIIIII\n SELECT * FROM parquet_scan('data/parquet-testing/bug687_nulls.parquet') limit 50;\n@@ -264,7 +264,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/bug1554.parquet') limit 50;\n 1584883:7ANjserj/Xp/vz4XFpL1wOC68SXgZ4LvfE5ggEiTrl1yjOtH4TWIezdNsg4TNakKE5TsYM06P4Qd9HQenS9ksA==\tTrue\tNULL\t200\t\n 1584883:sV6uqASHK17GJVEXh2mxbbRIk08qivvqS561cy09Zn+SCUMHZL7J/BLRsx0/kYi1Uzkh52SsocpbQzuYeRT+lQ==\tFalse\tNULL\t200\t\n \n-mode skip \n+mode skip\n \n query IIIII\n SELECT * FROM parquet_scan('data/parquet-testing/apkwan.parquet') limit 50;\n@@ -320,7 +320,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/apkwan.parquet') limit 50;\n 53e99785b7602d9701f447a2\t[https://link.springer.com/10.1007/s10039-006-1167-2]\t[{name: U. Culemann, id: NULL, org: Universit\u00e4tsklinikum des Saarlandes Klinik f\u00fcr Unfall-, Hand- und Wiederherstellungschirurgie 66421 Homburg/Saar Deutschland 66421 Homburg/Saar Deutschland}, {name: U. Culemann, id: 53f47248dabfaeee22a7fb77, org: Klinik f\u00fcr Unfall|Universit\u00e4tsklinikum des Saarlandes}]\ten\tBeckenringverletzungen\t\n 53e997d1b7602d9701fc2268\t[http://dx.doi.org/10.1098/rstb.1982.0151, http://www.ncbi.nlm.nih.gov/pubmed/6130546?report=xml&format=text]\t[{name: K. J. Ullrich, id: 5406d5addabfae44f0860eb9, org: NULL}, {name: H. Murer, id: 53f437cedabfaedf4358c982, org: NULL}]\ten\tSulphate and Phosphate Transport in the Renal Proximal Tubule\t\n \n-mode unskip \n+mode unskip\n \n query II\n SELECT * FROM parquet_scan('data/parquet-testing/arrow/nested_lists.snappy.parquet') limit 50;\n@@ -329,21 +329,17 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/nested_lists.snappy.parqu\n [[[a, b], [c, d]], [NULL, [e]]]\t1\n [[[a, b], [c, d], [e]], [NULL, [f]]]\t1\n \n-mode skip \n-\n query I\n SELECT * FROM parquet_scan('data/parquet-testing/arrow/nulls.snappy.parquet') limit 50;\n ----\n-{b_c_int: NULL}\t\n-{b_c_int: NULL}\t\n-{b_c_int: NULL}\t\n-{b_c_int: NULL}\t\n-{b_c_int: NULL}\t\n-{b_c_int: NULL}\t\n-{b_c_int: NULL}\t\n-{b_c_int: NULL}\t\n-\n-mode unskip \n+{'b_c_int': NULL}\n+{'b_c_int': NULL}\n+{'b_c_int': NULL}\n+{'b_c_int': NULL}\n+{'b_c_int': NULL}\n+{'b_c_int': NULL}\n+{'b_c_int': NULL}\n+{'b_c_int': NULL}\n \n query III\n SELECT * FROM parquet_scan('data/parquet-testing/nan-float.parquet') limit 50;\n@@ -412,7 +408,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/struct.parquet') limit 50;\n {'str_field': hello, 'f64_field': NULL}\n {'str_field': NULL, 'f64_field': 1.230000}\n \n-mode skip \n+mode skip\n \n query I\n SELECT * FROM parquet_scan('data/parquet-testing/arrow/byte_array_decimal.parquet') limit 50;\n@@ -442,7 +438,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/byte_array_decimal.parque\n 23.00\t\n 24.00\t\n \n-mode unskip \n+mode unskip\n \n query II\n SELECT * FROM parquet_scan('data/parquet-testing/arrow/list_columns.parquet') limit 50;\n@@ -561,7 +557,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/lineitem-top10000.gzip.parquet'\n 38\t175839\t874\t1\t44\t84252.52\t0.04\t0.02\tN\tO\t1996-09-29\t1996-11-17\t1996-09-30\tCOLLECT COD\tMAIL\ts. blithely unusual theodolites am\t\n 39\t2320\t9821\t1\t44\t53782.08\t0.09\t0.06\tN\tO\t1996-11-14\t1996-12-15\t1996-12-12\tCOLLECT COD\tRAIL\teodolites. careful\t\n \n-mode skip \n+mode skip\n \n query III\n SELECT * FROM parquet_scan('data/parquet-testing/arrow/nested_maps.snappy.parquet') limit 50;\n@@ -573,7 +569,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/nested_maps.snappy.parque\n {key: [e], value: [{key: [1], value: [True]}]}\t1\t1.0\t\n {key: [f], value: [{key: [3, 4, 5], value: [True, False, True]}]}\t1\t1.0\t\n \n-mode unskip \n+mode unskip\n \n query I\n SELECT * FROM parquet_scan('data/parquet-testing/arrow/dict-page-offset-zero.parquet') limit 50;\n@@ -955,7 +951,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/bug1589.parquet') limit 50;\n 200\tNULL\t\n 300\tNULL\t\n \n-mode skip \n+mode skip\n \n query I\n SELECT * FROM parquet_scan('data/parquet-testing/arrow/hadoop_lz4_compressed_larger.parquet') limit 50;\n@@ -1011,9 +1007,9 @@ f2807544-a424-444a-add3-3d5d486b70e2\n d0018041-41e3-4013-ba90-535ba03d46c3\t\n c50e8ade-6051-436f-a26e-acc9c0594be5\t\n \n-mode unskip \n+mode unskip\n \n-mode skip \n+mode skip\n \n query III\n SELECT * FROM parquet_scan('data/parquet-testing/arrow/non_hadoop_lz4_compressed.parquet') limit 50;\n@@ -1023,7 +1019,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/non_hadoop_lz4_compressed\n 1593604801\tabc\t42.125\t\n 1593604801\tdef\t7.7\t\n \n-mode unskip \n+mode unskip\n \n query IIIIIIIIIII\n SELECT * FROM parquet_scan('data/parquet-testing/arrow/alltypes_plain.parquet') limit 50;\n@@ -1051,19 +1047,15 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/repeated_no_annotation.pa\n \n mode unskip \n \n-mode skip \n-\n query IIIIIIIIIIII\n SELECT * FROM parquet_scan('data/parquet-testing/data-types.parquet') limit 50;\n ----\n NULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\t\n-42\t43\t44\t45\t4.599999904632568\t4.7\t4.80\t49\t50\tTrue\t2019-11-26 20:11:42.501000\t2020-01-10\t\n+42\t43\t44\t45\t4.599999904632568\t4.7\t4.80\t49\t50\tTrue\t2019-11-26 20:11:42.501\t2020-01-10\n -127\t-32767\t-2147483647\t-9223372036854775807\t-4.599999904632568\t-4.7\tNULL\tNULL\tNULL\tFalse\tNULL\tNULL\t\n 127\t32767\t2147483647\t9223372036854775807\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\t\n NULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\t\n \n-mode unskip \n-\n query IIII\n SELECT * FROM parquet_scan('data/parquet-testing/unsigned.parquet') limit 50;\n ----\n@@ -1120,7 +1112,7 @@ NULL\n 1996-01-01\t\n 1997-01-01\t\n \n-mode skip \n+mode skip\n \n query IIIIII\n SELECT * FROM parquet_scan('data/parquet-testing/arrow/nullable.impala.parquet') limit 50;\n@@ -1133,9 +1125,9 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/nullable.impala.parquet')\n 6\tNULL\tNULL\tNULL\tNULL\tNULL\t\n 7\tNULL\t[NULL, [5, 6]]\t{key: [k1, k3], value: [NULL, NULL]}\tNULL\t{A: 7, : [2, 3, NULL], C: {d: [[], [NULL], NULL]}, g: NULL}\t\n \n-mode unskip \n+mode unskip\n \n-mode skip \n+mode skip\n \n query III\n SELECT * FROM parquet_scan('data/parquet-testing/arrow/hadoop_lz4_compressed.parquet') limit 50;\n@@ -1145,7 +1137,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/hadoop_lz4_compressed.par\n 1593604801\tabc\t42.125\t\n 1593604801\tdef\t7.7\t\n \n-mode unskip \n+mode unskip\n \n query I\n SELECT * FROM parquet_scan('data/parquet-testing/fixed.parquet') limit 50;\n@@ -1396,14 +1388,14 @@ SELECT * FROM parquet_scan('data/parquet-testing/glob/t1.parquet') limit 50;\n query III\n SELECT * FROM parquet_scan('data/parquet-testing/bug2557.parquet') limit 10\n ----\n-[adipiscing, elit]\t[267]\tNULL\n+[adipiscing, elit]\t[267]\t[]\n [adipiscing, elit]\t[58, 146]\t[3105.735731, 7332.144961, 2693.459659, 2058.830347]\n [dolor, sit, amet, consectetur, adipiscing, elit]\t[26, 701]\t[2252.315041]\n [Lorem, ipsum, dolor, sit, amet, consectetur, adipiscing, elit]\t[763]\t[4318.131164, 703.332322]\n-[consectetur, adipiscing, elit]\tNULL\t[4921.065813]\n+[consectetur, adipiscing, elit]\t[]\t[4921.065813]\n [ipsum, dolor, sit, amet, consectetur, adipiscing, elit]\t[503]\t[3143.311724]\n [amet, consectetur, adipiscing, elit]\t[981]\t[1556.844782, 5388.780546]\n-[consectetur, adipiscing, elit]\t[822, 843, 702, 469]\tNULL\n+[consectetur, adipiscing, elit]\t[822, 843, 702, 469]\t[]\n [dolor, sit, amet, consectetur, adipiscing, elit]\t[698, 385]\t[3591.160509]\n [Lorem, ipsum, dolor, sit, amet, consectetur, adipiscing, elit]\t[597, 719]\t[1218.803740]\n \ndiff --git a/test/sql/copy/parquet/parquet_1589.test b/test/sql/copy/parquet/parquet_1589.test\nindex 800b91722e80..f6d6b487b1eb 100644\n--- a/test/sql/copy/parquet/parquet_1589.test\n+++ b/test/sql/copy/parquet/parquet_1589.test\n@@ -4,8 +4,8 @@\n \n require parquet\n \n-# statement ok\n-# pragma enable_verification\n+statement ok\n+pragma enable_verification\n \n query I\n SELECT backlink_count FROM parquet_scan('data/parquet-testing/bug1589.parquet') LIMIT 1\ndiff --git a/test/sql/copy/parquet/parquet_2267.test b/test/sql/copy/parquet/parquet_2267.test\nindex 766e237a423c..d5d09c5de92f 100644\n--- a/test/sql/copy/parquet/parquet_2267.test\n+++ b/test/sql/copy/parquet/parquet_2267.test\n@@ -7,7 +7,7 @@ require parquet\n query I\n SELECT * FROM parquet_scan('data/parquet-testing/bug2267.parquet')\n ----\n-[{'disabledPlans': [bea4c11e-220a-4e6d-8eb8-8ea15d019f90], 'skuId': c7df2760-2c81-4ef7-b578-5b5392b571df}, {'disabledPlans': [8a256a2b-b617-496d-b51b-e76466e88db0, 41781fb2-bc02-4b7c-bd55-b576c07bb09d, eec0eb4f-6444-4f95-aba0-50c24d67f998], 'skuId': 84a661c4-e949-4bd2-a560-ed7766fcaf2b}, {'disabledPlans': NULL, 'skuId': b05e124f-c7cc-45a0-a6aa-8cf78c946968}, {'disabledPlans': NULL, 'skuId': f30db892-07e9-47e9-837c-80727f46fd3d}]\n+[{'disabledPlans': [bea4c11e-220a-4e6d-8eb8-8ea15d019f90], 'skuId': c7df2760-2c81-4ef7-b578-5b5392b571df}, {'disabledPlans': [8a256a2b-b617-496d-b51b-e76466e88db0, 41781fb2-bc02-4b7c-bd55-b576c07bb09d, eec0eb4f-6444-4f95-aba0-50c24d67f998], 'skuId': 84a661c4-e949-4bd2-a560-ed7766fcaf2b}, {'disabledPlans': [], 'skuId': b05e124f-c7cc-45a0-a6aa-8cf78c946968}, {'disabledPlans': [], 'skuId': f30db892-07e9-47e9-837c-80727f46fd3d}]\n \n query I\n SELECT assignedLicenses[0] FROM parquet_scan('data/parquet-testing/bug2267.parquet')\ndiff --git a/test/sql/copy/parquet/parquet_filter_bug1391.test b/test/sql/copy/parquet/parquet_filter_bug1391.test\nindex 0462328d3f8d..841171b64494 100644\n--- a/test/sql/copy/parquet/parquet_filter_bug1391.test\n+++ b/test/sql/copy/parquet/parquet_filter_bug1391.test\n@@ -3,32 +3,31 @@\n # group: [parquet]\n \n require parquet\n-require vector_size 512\n \n statement ok\n PRAGMA enable_verification\n \n statement ok\n CREATE VIEW tbl AS SELECT * FROM PARQUET_SCAN('data/parquet-testing/filter_bug1391.parquet');\n-#\n-#query I\n-#SELECT ORGUNITID FROM tbl LIMIT 10\n-#----\n-#98\n-#13\n-#175\n-#200\n-#262\n-#206\n-#204\n-#131\n-#181\n-#269\n-#\n-#query I\n-#SELECT COUNT(*) FROM tbl;\n-#----\n-#9789\n+\n+query I\n+SELECT ORGUNITID FROM tbl LIMIT 10\n+----\n+98\n+13\n+175\n+200\n+262\n+206\n+204\n+131\n+181\n+269\n+\n+query I\n+SELECT COUNT(*) FROM tbl;\n+----\n+9789\n \n query I\n SELECT COUNT(*) FROM tbl\ndiff --git a/test/sql/copy/parquet/parquet_write_codecs.test b/test/sql/copy/parquet/parquet_write_codecs.test\nindex 294e4de474cc..bd8c169db110 100644\n--- a/test/sql/copy/parquet/parquet_write_codecs.test\n+++ b/test/sql/copy/parquet/parquet_write_codecs.test\n@@ -4,8 +4,6 @@\n \n require parquet\n \n-require vector_size 64\n-\n # codec uncompressed\n statement ok\n COPY (SELECT 42, 'hello') TO '__TEST_DIR__/uncompressed.parquet' (FORMAT 'parquet', CODEC 'UNCOMPRESSED');\ndiff --git a/test/sql/copy/parquet/test_aws_files.test b/test/sql/copy/parquet/test_aws_files.test\nindex 7b6a0c1b30ff..5dce1a9dcd10 100644\n--- a/test/sql/copy/parquet/test_aws_files.test\n+++ b/test/sql/copy/parquet/test_aws_files.test\n@@ -3,7 +3,6 @@\n # group: [parquet]\n \n require parquet\n-require vector_size 512\n \n statement ok\n PRAGMA enable_verification\ndiff --git a/test/sql/copy/parquet/test_parquet_filter_pushdown.test b/test/sql/copy/parquet/test_parquet_filter_pushdown.test\nindex 2ef24b434b19..e47bcf0b809b 100644\n--- a/test/sql/copy/parquet/test_parquet_filter_pushdown.test\n+++ b/test/sql/copy/parquet/test_parquet_filter_pushdown.test\n@@ -3,7 +3,6 @@\n # group: [parquet]\n \n require parquet\n-require vector_size 512\n \n statement ok\n pragma enable_verification\ndiff --git a/test/sql/copy/parquet/test_parquet_nested.test b/test/sql/copy/parquet/test_parquet_nested.test\nindex ee5f1d25d302..dfda69aea05a 100644\n--- a/test/sql/copy/parquet/test_parquet_nested.test\n+++ b/test/sql/copy/parquet/test_parquet_nested.test\n@@ -96,13 +96,13 @@ query II\n SELECT id, url FROM parquet_scan('data/parquet-testing/apkwan.parquet') limit 10\n ----\n 53e997b9b7602d9701f9f044\t[https://link.springer.com/10.1007/s00108-004-1229-0]\n-53e997b2b7602d9701f8fea5\tNULL\n+53e997b2b7602d9701f8fea5\t[]\n 53e997aeb7602d9701f8856e\t[http://www.ncbi.nlm.nih.gov/pubmed/4669724?report=xml&format=text, http://www.ncbi.nlm.nih.gov/pubmed/5123793?report=xml&format=text, http://www.ncbi.nlm.nih.gov/pubmed/5315218?report=xml&format=text]\n-53e997bab7602d9701fa1e34\tNULL\n-53e997abb7602d9701f846c0\tNULL\n-53e9978db7602d9701f4d7e8\tNULL\n+53e997bab7602d9701fa1e34\t[]\n+53e997abb7602d9701f846c0\t[]\n+53e9978db7602d9701f4d7e8\t[]\n 53e9984bb7602d970207c61d\t[http://subs.emis.de/LNI/Proceedings/Proceedings26/article639.html]\n-53e99796b7602d9701f5cd36\tNULL\n+53e99796b7602d9701f5cd36\t[]\n 53e99809b7602d970201f551\t[http://dx.doi.org/10.1016/S0140-6736(00)82170-4, http://www.ncbi.nlm.nih.gov/pubmed/20914302?report=xml&format=text]\n 53e997a6b7602d9701f7ffb0\t[http://www.ncbi.nlm.nih.gov/pubmed/4051185?report=xml&format=text]\n \ndiff --git a/test/sql/copy/parquet/test_parquet_stats.test b/test/sql/copy/parquet/test_parquet_stats.test\nindex a09e313b98a9..dcc7cff2a1d0 100644\n--- a/test/sql/copy/parquet/test_parquet_stats.test\n+++ b/test/sql/copy/parquet/test_parquet_stats.test\n@@ -3,7 +3,6 @@\n # group: [parquet]\n \n require parquet\n-require vector_size 512\n \n statement ok\n PRAGMA explain_output = PHYSICAL_ONLY\ndiff --git a/test/sql/copy/parquet/writer/list_of_bools.test b/test/sql/copy/parquet/writer/list_of_bools.test\nnew file mode 100644\nindex 000000000000..23fa9b10adc6\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/list_of_bools.test\n@@ -0,0 +1,91 @@\n+# name: test/sql/copy/parquet/writer/list_of_bools.test\n+# description: Parquet write list of bools\n+# group: [writer]\n+\n+require parquet\n+\n+# big list of bools\n+statement ok\n+CREATE TABLE list_of_bools AS\n+     SELECT LIST(i%2==0) l FROM range(1373) tbl(i)\n+     UNION ALL\n+     SELECT [true, false, NULL, false, true]\n+     UNION ALL\n+     SELECT []\n+     UNION ALL\n+     SELECT NULL\n+     UNION ALL\n+     SELECT LIST(i%3==0) l FROM range(9937) tbl(i)\n+     UNION ALL\n+     SELECT [true, false, NULL, false, true]\n+\n+query III\n+SELECT COUNT(*), COUNT(b), SUM(CASE WHEN b THEN 1 ELSE 0 END)\n+FROM (SELECT unnest(l) b FROM list_of_bools)\n+----\n+11320\t11318\t4004\n+\n+statement ok\n+COPY list_of_bools TO '__TEST_DIR__/list_of_bools.parquet' (FORMAT PARQUET)\n+\n+query III\n+SELECT COUNT(*), COUNT(b), SUM(CASE WHEN b THEN 1 ELSE 0 END)\n+FROM (SELECT unnest(l) b FROM '__TEST_DIR__/list_of_bools.parquet')\n+----\n+11320\t11318\t4004\n+\n+# many lists of integers\n+statement ok\n+CREATE TABLE many_ints AS\n+\tSELECT [1, 0, 1] AS l FROM range(1373)\n+\tUNION ALL\n+\tSELECT []\n+\tUNION ALL\n+\tSELECT NULL\n+\tUNION ALL\n+\tSELECT [1, 0, NULL, 0, 1]\n+\tUNION ALL\n+    SELECT [1, 0, NULL, 1] l FROM range(9937) tbl(i)\n+\n+query III\n+SELECT COUNT(*), COUNT(b), SUM(b)\n+FROM (SELECT unnest(l) b FROM many_ints)\n+----\n+43872\t33934\t22622\n+\n+statement ok\n+COPY many_ints TO 'many_ints.parquet' (FORMAT PARQUET)\n+\n+query III\n+SELECT COUNT(*), COUNT(b), SUM(b)\n+FROM (SELECT unnest(l) b FROM 'many_ints.parquet')\n+----\n+43872\t33934\t22622\n+\n+# many lists of bools\n+statement ok\n+CREATE TABLE many_bools AS\n+\tSELECT [true, false, true] AS l FROM range(1373)\n+\tUNION ALL\n+\tSELECT []\n+\tUNION ALL\n+\tSELECT NULL\n+\tUNION ALL\n+\tSELECT [true, false, NULL, false, true]\n+\tUNION ALL\n+    SELECT [true, false, NULL, true] l FROM range(9937) tbl(i)\n+\n+query III\n+SELECT COUNT(*), COUNT(b), SUM(CASE WHEN b THEN 1 ELSE 0 END)\n+FROM (SELECT unnest(l) b FROM many_bools)\n+----\n+43872\t33934\t22622\n+\n+statement ok\n+COPY many_bools TO '__TEST_DIR__/many_bools.parquet' (FORMAT PARQUET)\n+\n+query III\n+SELECT COUNT(*), COUNT(b), SUM(CASE WHEN b THEN 1 ELSE 0 END)\n+FROM (SELECT unnest(l) b FROM '__TEST_DIR__/many_bools.parquet')\n+----\n+43872\t33934\t22622\ndiff --git a/test/sql/copy/parquet/writer/parquet_large_blobs.test_coverage b/test/sql/copy/parquet/writer/parquet_large_blobs.test_coverage\nnew file mode 100644\nindex 000000000000..d9a76df1433d\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/parquet_large_blobs.test_coverage\n@@ -0,0 +1,20 @@\n+# name: test/sql/copy/parquet/writer/parquet_large_blobs.test_coverage\n+# description: Test writing of large blobs into parquet files\n+# group: [writer]\n+\n+require parquet\n+\n+statement ok\n+CREATE TABLE large_strings AS SELECT repeat('duckduck', 10000+i) i FROM range(4000) tbl(i);\n+\n+query III nosort minmaxstrlen\n+SELECT MIN(strlen(i)), MAX(strlen(i)), AVG(strlen(i)) FROM large_strings;\n+\n+statement ok\n+COPY large_strings TO '__TEST_DIR__/largestrings.parquet' (FORMAT PARQUET);\n+\n+statement ok\n+SELECT * FROM parquet_metadata('__TEST_DIR__/largestrings.parquet');\n+\n+query III nosort minmaxstrlen\n+SELECT MIN(strlen(i)), MAX(strlen(i)), AVG(strlen(i)) FROM large_strings;\ndiff --git a/test/sql/copy/parquet/writer/parquet_write_booleans.test b/test/sql/copy/parquet/writer/parquet_write_booleans.test\nnew file mode 100644\nindex 000000000000..0362a442d163\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/parquet_write_booleans.test\n@@ -0,0 +1,36 @@\n+# name: test/sql/copy/parquet/writer/parquet_write_booleans.test\n+# description: Parquet bools round trip\n+# group: [writer]\n+\n+require parquet\n+\n+require vector_size 512\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE bools(b BOOL)\n+\n+statement ok\n+INSERT INTO bools SELECT CASE WHEN i%2=0 THEN NULL ELSE i%7=0 OR i%3=0 END b FROM range(10000) tbl(i);\n+\n+query IIIIII\n+SELECT COUNT(*), COUNT(b), BOOL_AND(b), BOOL_OR(b), SUM(CASE WHEN b THEN 1 ELSE 0 END) true_count, SUM(CASE WHEN b THEN 0 ELSE 1 END) false_count\n+FROM bools\n+----\n+10000\t5000\tFalse\tTrue\t2143\t7857\n+\n+statement ok\n+COPY bools TO '__TEST_DIR__/bools.parquet' (FORMAT 'parquet');\n+\n+query IIIIII\n+SELECT COUNT(*), COUNT(b), BOOL_AND(b), BOOL_OR(b), SUM(CASE WHEN b THEN 1 ELSE 0 END) true_count, SUM(CASE WHEN b THEN 0 ELSE 1 END) false_count\n+FROM '__TEST_DIR__/bools.parquet'\n+----\n+10000\t5000\tFalse\tTrue\t2143\t7857\n+\n+query I\n+SELECT typeof(b) FROM '__TEST_DIR__/bools.parquet' LIMIT 1\n+----\n+BOOLEAN\ndiff --git a/test/sql/copy/parquet/writer/parquet_write_date.test b/test/sql/copy/parquet/writer/parquet_write_date.test\nindex eb7f60ca2f7c..6fb962f2f258 100644\n--- a/test/sql/copy/parquet/writer/parquet_write_date.test\n+++ b/test/sql/copy/parquet/writer/parquet_write_date.test\n@@ -4,8 +4,6 @@\n \n require parquet\n \n-require vector_size 64\n-\n statement ok\n PRAGMA enable_verification\n \ndiff --git a/test/sql/copy/parquet/writer/parquet_write_hugeint.test b/test/sql/copy/parquet/writer/parquet_write_hugeint.test\nindex 9e61b1f7fc14..3000230ff5db 100644\n--- a/test/sql/copy/parquet/writer/parquet_write_hugeint.test\n+++ b/test/sql/copy/parquet/writer/parquet_write_hugeint.test\n@@ -4,8 +4,6 @@\n \n require parquet\n \n-require vector_size 64\n-\n statement ok\n PRAGMA enable_verification\n \ndiff --git a/test/sql/copy/parquet/writer/parquet_write_signed.test b/test/sql/copy/parquet/writer/parquet_write_signed.test\nindex af5c14286962..a2f8a1734049 100644\n--- a/test/sql/copy/parquet/writer/parquet_write_signed.test\n+++ b/test/sql/copy/parquet/writer/parquet_write_signed.test\n@@ -4,8 +4,6 @@\n \n require parquet\n \n-require vector_size 64\n-\n statement ok\n PRAGMA enable_verification\n \ndiff --git a/test/sql/copy/parquet/writer/parquet_write_timestamp.test b/test/sql/copy/parquet/writer/parquet_write_timestamp.test\nindex 415d5338db94..eeb3c73a16e8 100644\n--- a/test/sql/copy/parquet/writer/parquet_write_timestamp.test\n+++ b/test/sql/copy/parquet/writer/parquet_write_timestamp.test\n@@ -4,8 +4,6 @@\n \n require parquet\n \n-require vector_size 64\n-\n statement ok\n PRAGMA enable_verification\n \ndiff --git a/test/sql/copy/parquet/writer/parquet_write_tpch.test_slow b/test/sql/copy/parquet/writer/parquet_write_tpch.test_slow\nnew file mode 100644\nindex 000000000000..3fdbd92e8a26\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/parquet_write_tpch.test_slow\n@@ -0,0 +1,52 @@\n+# name: test/sql/copy/parquet/writer/parquet_write_tpch.test_slow\n+# description: Parquet TPC-H tests\n+# group: [writer]\n+\n+require parquet\n+\n+require tpch\n+\n+statement ok\n+CREATE SCHEMA tpch;\n+\n+statement ok\n+CALL dbgen(sf=1, schema='tpch');\n+\n+foreach tbl lineitem nation orders supplier part partsupp region customer\n+\n+statement ok\n+COPY tpch.${tbl} TO '__TEST_DIR__/${tbl}.parquet' (FORMAT 'PARQUET', COMPRESSION 'ZSTD');\n+\n+statement ok\n+CREATE VIEW ${tbl} AS SELECT * FROM parquet_scan('__TEST_DIR__/${tbl}.parquet');\n+\n+endloop\n+\n+loop i 1 9\n+\n+query I\n+PRAGMA tpch(${i})\n+----\n+<FILE>:extension/tpch/dbgen/answers/sf1/q0${i}.csv\n+\n+endloop\n+\n+# skip q15 for now: it is non-deterministic with multi-threading and doubles\n+# this can be re-enabled once we write decimals to parquet\n+loop i 10 15\n+\n+query I\n+PRAGMA tpch(${i})\n+----\n+<FILE>:extension/tpch/dbgen/answers/sf1/q${i}.csv\n+\n+endloop\n+\n+loop i 16 23\n+\n+query I\n+PRAGMA tpch(${i})\n+----\n+<FILE>:extension/tpch/dbgen/answers/sf1/q${i}.csv\n+\n+endloop\ndiff --git a/test/sql/copy/parquet/writer/parquet_write_tpch_nested.test_slow b/test/sql/copy/parquet/writer/parquet_write_tpch_nested.test_slow\nnew file mode 100644\nindex 000000000000..ee5b8f2c98cd\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/parquet_write_tpch_nested.test_slow\n@@ -0,0 +1,84 @@\n+# name: test/sql/copy/parquet/writer/parquet_write_tpch_nested.test_slow\n+# description: Parquet TPC-H tests\n+# group: [writer]\n+\n+require parquet\n+\n+require tpch\n+\n+statement ok\n+CREATE SCHEMA tpch;\n+\n+statement ok\n+CALL dbgen(sf=0.1, schema='tpch');\n+\n+\n+# transform lineitem into a list of structs\n+statement ok\n+CREATE VIEW lineitem_array_view AS SELECT LIST({'l_orderkey': l_orderkey,\n+\t'l_partkey': l_partkey,\n+\t'l_suppkey': l_suppkey,\n+\t'l_linenumber': l_linenumber,\n+\t'l_quantity': l_quantity,\n+\t'l_extendedprice': l_extendedprice,\n+\t'l_discount': l_discount,\n+\t'l_tax': l_tax,\n+\t'l_returnflag': l_returnflag,\n+\t'l_linestatus': l_linestatus,\n+\t'l_shipdate': l_shipdate,\n+\t'l_commitdate': l_commitdate,\n+\t'l_receiptdate': l_receiptdate,\n+\t'l_shipinstruct': l_shipinstruct,\n+\t'l_shipmode': l_shipmode,\n+\t'l_comment': l_comment}) lineitem_array FROM tpch.lineitem\n+\n+statement ok\n+COPY lineitem_array_view TO '__TEST_DIR__/lineitem.parquet' (FORMAT 'PARQUET', COMPRESSION 'ZSTD');\n+\n+statement ok\n+CREATE VIEW lineitem AS SELECT\n+\ts.l_orderkey AS l_orderkey,\n+\ts.l_partkey AS l_partkey,\n+\ts.l_suppkey AS l_suppkey,\n+\ts.l_linenumber AS l_linenumber,\n+\ts.l_quantity AS l_quantity,\n+\ts.l_extendedprice AS l_extendedprice,\n+\ts.l_discount AS l_discount,\n+\ts.l_tax AS l_tax,\n+\ts.l_returnflag AS l_returnflag,\n+\ts.l_linestatus AS l_linestatus,\n+\ts.l_shipdate AS l_shipdate,\n+\ts.l_commitdate AS l_commitdate,\n+\ts.l_receiptdate AS l_receiptdate,\n+\ts.l_shipinstruct AS l_shipinstruct,\n+\ts.l_shipmode AS l_shipmode,\n+\ts.l_comment AS l_comment\n+\tFROM (SELECT UNNEST(lineitem_array) s FROM parquet_scan('__TEST_DIR__/lineitem.parquet'));\n+\n+foreach tbl nation orders supplier part partsupp region customer\n+\n+statement ok\n+COPY tpch.${tbl} TO '__TEST_DIR__/${tbl}.parquet' (FORMAT 'PARQUET', COMPRESSION 'ZSTD');\n+\n+statement ok\n+CREATE VIEW ${tbl} AS SELECT * FROM parquet_scan('__TEST_DIR__/${tbl}.parquet');\n+\n+endloop\n+\n+loop i 1 9\n+\n+query I\n+PRAGMA tpch(${i})\n+----\n+<FILE>:extension/tpch/dbgen/answers/sf0.1/q0${i}.csv\n+\n+endloop\n+\n+loop i 10 23\n+\n+query I\n+PRAGMA tpch(${i})\n+----\n+<FILE>:extension/tpch/dbgen/answers/sf0.1/q${i}.csv\n+\n+endloop\ndiff --git a/test/sql/copy/parquet/writer/parquet_write_unsigned.test b/test/sql/copy/parquet/writer/parquet_write_unsigned.test\nindex c97dd5c22586..f8f4137bd6e8 100644\n--- a/test/sql/copy/parquet/writer/parquet_write_unsigned.test\n+++ b/test/sql/copy/parquet/writer/parquet_write_unsigned.test\n@@ -4,8 +4,6 @@\n \n require parquet\n \n-require vector_size 64\n-\n statement ok\n PRAGMA enable_verification\n \ndiff --git a/test/sql/copy/parquet/writer/parquet_zstd_sequence.test_slow b/test/sql/copy/parquet/writer/parquet_zstd_sequence.test_slow\nnew file mode 100644\nindex 000000000000..29120344c07e\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/parquet_zstd_sequence.test_slow\n@@ -0,0 +1,40 @@\n+# name: test/sql/copy/parquet/writer/parquet_zstd_sequence.test_slow\n+# description: Test writing of large blobs into parquet files\n+# group: [writer]\n+\n+require parquet\n+\n+require 64bit\n+\n+statement ok\n+COPY (SELECT * FROM read_csv_auto('data/csv/sequences.csv.gz', delim=',', header=True) LIMIT 25000) TO '__TEST_DIR__/duckseq.parquet' (FORMAT 'PARQUET', CODEC 'ZSTD', ROW_GROUP_SIZE 25000);\n+\n+query IIIIII\n+select count(*), min(strain), max(strain), min(strlen(sequence)), max(strlen(sequence)), avg(strlen(sequence))\n+from '__TEST_DIR__/duckseq.parquet';\n+----\n+25000\tAUS/NT01/2020\tcanine/HKG/20-03695/2020\t17340\t30018\t29855.647080\n+\n+statement ok\n+COPY\n+(\n+\tSELECT lstrain::VARCHAR[] lstrain, lsequence::VARCHAR[] lsequence FROM (VALUES ([], []), (NULL, NULL), ([], [])) tbl(lstrain, lsequence)\n+\tUNION ALL\n+\tSELECT * FROM (\n+\t\tSELECT LIST(strain) AS lstrain, LIST(sequence) AS lsequence FROM '__TEST_DIR__/duckseq.parquet' LIMIT 10000\n+\t)\n+\tUNION ALL\n+\tSELECT * FROM (VALUES ([], []), (NULL, NULL), ([], []))\n+)\n+TO '__TEST_DIR__/duckseq2.parquet' (FORMAT 'PARQUET', CODEC 'ZSTD');\n+\n+query I\n+SELECT COUNT(*) FROM '__TEST_DIR__/duckseq2.parquet'\n+----\n+7\n+\n+query IIIIII nosort querylabel\n+select count(*), min(strain), max(strain), min(strlen(sequence)), max(strlen(sequence)), avg(strlen(sequence))\n+from (SELECT UNNEST(lstrain) AS strain, UNNEST(lsequence) AS sequence FROM '__TEST_DIR__/duckseq2.parquet');\n+----\n+100000\tARG/Cordoba-1006-155/2020\ttiger/NY/040420/2020\t17340\t30643\t29821.264410\ndiff --git a/test/sql/copy/parquet/writer/test_parquet_write.test b/test/sql/copy/parquet/writer/test_parquet_write.test\nindex b7d715fdcd80..4f491c5bf600 100644\n--- a/test/sql/copy/parquet/writer/test_parquet_write.test\n+++ b/test/sql/copy/parquet/writer/test_parquet_write.test\n@@ -4,9 +4,6 @@\n \n require parquet\n \n-# single scalar value\n-require vector_size 64\n-\n statement ok\n COPY (SELECT 42) TO '__TEST_DIR__/scalar.parquet' (FORMAT 'parquet');\n \ndiff --git a/test/sql/copy/parquet/writer/test_parquet_write_complex.test b/test/sql/copy/parquet/writer/test_parquet_write_complex.test\nindex d650ab3a1946..90ac7229fac5 100644\n--- a/test/sql/copy/parquet/writer/test_parquet_write_complex.test\n+++ b/test/sql/copy/parquet/writer/test_parquet_write_complex.test\n@@ -4,8 +4,7 @@\n \n require parquet\n \n-# writer requires vector_size >= 64\n-require vector_size 64\n+require vector_size 512\n \n # alltypes_dictionary: scan as parquet\n query I nosort alltypes_dictionary\ndiff --git a/test/sql/copy/parquet/writer/write_complex_nested.test b/test/sql/copy/parquet/writer/write_complex_nested.test\nnew file mode 100644\nindex 000000000000..04f495841acc\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/write_complex_nested.test\n@@ -0,0 +1,137 @@\n+# name: test/sql/copy/parquet/writer/write_complex_nested.test\n+# description: Parquet write complex structures\n+# group: [writer]\n+\n+require parquet\n+\n+# struct of lists\n+statement ok\n+CREATE TABLE struct_of_lists AS SELECT * FROM (VALUES\n+\t({'a': [1, 2, 3], 'b': ['hello', 'world']}),\n+\t({'a': [4, NULL, 5], 'b': ['duckduck', 'goose']}),\n+\t({'a': NULL, 'b': ['longlonglonglonglonglong', NULL, NULL]}),\n+\t(NULL),\n+    ({'a': [], 'b': []}),\n+    ({'a': [1, 2, 3], 'b': NULL})\n+) tbl(i);\n+\n+statement ok\n+COPY struct_of_lists TO '__TEST_DIR__/complex_list.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT i FROM parquet_scan('__TEST_DIR__/complex_list.parquet');\n+----\n+{'a': [1, 2, 3], 'b': [hello, world]}\n+{'a': [4, NULL, 5], 'b': [duckduck, goose]}\n+{'a': NULL, 'b': [longlonglonglonglonglong, NULL, NULL]}\n+NULL\n+{'a': [], 'b': []}\n+{'a': [1, 2, 3], 'b': NULL}\n+\n+# list of structs\n+statement ok\n+CREATE TABLE list_of_structs AS SELECT * FROM (VALUES\n+\t([{'a': 1, 'b': 100}, NULL, {'a': 2, 'b': 101}]),\n+\t(NULL),\n+\t([]),\n+\t([{'a': NULL, 'b': 102}, {'a': 3, 'b': NULL}, NULL])\n+) tbl(i);\n+\n+statement ok\n+COPY list_of_structs TO '__TEST_DIR__/complex_list.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT i FROM parquet_scan('__TEST_DIR__/complex_list.parquet');\n+----\n+[{'a': 1, 'b': 100}, NULL, {'a': 2, 'b': 101}]\n+NULL\n+[]\n+[{'a': NULL, 'b': 102}, {'a': 3, 'b': NULL}, NULL]\n+\n+# list of structs of structs\n+statement ok\n+CREATE TABLE list_of_struct_of_structs AS SELECT * FROM (VALUES\n+\t([{'a': {'x': 33}, 'b': {'y': 42, 'z': 99}}, NULL, {'a': {'x': NULL}, 'b': {'y': 43, 'z': 100}}]),\n+\t(NULL),\n+\t([]),\n+\t([{'a': NULL, 'b': {'y': NULL, 'z': 101}}, {'a': {'x': 34}, 'b': {'y': 43, 'z': NULL}}]),\n+\t([{'a': NULL, 'b': NULL}])\n+) tbl(i);\n+\n+statement ok\n+COPY list_of_struct_of_structs TO '__TEST_DIR__/complex_list.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT i FROM parquet_scan('__TEST_DIR__/complex_list.parquet');\n+----\n+[{'a': {'x': 33}, 'b': {'y': 42, 'z': 99}}, NULL, {'a': {'x': NULL}, 'b': {'y': 43, 'z': 100}}]\n+NULL\n+[]\n+[{'a': NULL, 'b': {'y': NULL, 'z': 101}}, {'a': {'x': 34}, 'b': {'y': 43, 'z': NULL}}]\n+[{'a': NULL, 'b': NULL}]\n+\n+# list of lists\n+# no empty lists or nulls\n+statement ok\n+CREATE TABLE list_of_lists_simple AS SELECT * FROM (VALUES\n+\t([[1, 2, 3], [4, 5]]),\n+\t([[6, 7]]),\n+\t([[8, 9, 10], [11, 12]])\n+) tbl(i);\n+\n+statement ok\n+COPY list_of_lists_simple TO '__TEST_DIR__/complex_list.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT i FROM parquet_scan('__TEST_DIR__/complex_list.parquet');\n+----\n+[[1, 2, 3], [4, 5]]\n+[[6, 7]]\n+[[8, 9, 10], [11, 12]]\n+\n+# list of lists with nulls and empty lists\n+statement ok\n+CREATE TABLE list_of_lists AS SELECT * FROM (VALUES\n+\t([[1, 2, 3], [4, 5], [], [6, 7]]),\n+\t([[8, NULL, 10], NULL, []]),\n+\t([]),\n+\t(NULL),\n+\t([[11, 12, 13, 14], [], NULL, [], [], [15], [NULL, NULL, NULL]])\n+) tbl(i);\n+\n+statement ok\n+COPY list_of_lists TO '__TEST_DIR__/complex_list.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT i FROM parquet_scan('__TEST_DIR__/complex_list.parquet');\n+----\n+[[1, 2, 3], [4, 5], [], [6, 7]]\n+[[8, NULL, 10], NULL, []]\n+[]\n+NULL\n+[[11, 12, 13, 14], [], NULL, [], [], [15], [NULL, NULL, NULL]]\n+\n+# list of lists of lists of lists\n+statement ok\n+CREATE TABLE list_of_lists_of_lists_of_lists AS\n+   SELECT [LIST(i)] i FROM list_of_lists\n+   UNION ALL\n+   SELECT NULL\n+   UNION ALL\n+   SELECT [NULL]\n+   UNION ALL\n+   SELECT [[], NULL, [], []]\n+   UNION ALL\n+   SELECT [[[NULL, NULL, [NULL]], NULL, [[], [7, 8, 9], [NULL], NULL, []]], [], [NULL]]\n+\n+statement ok\n+COPY list_of_lists_of_lists_of_lists TO '__TEST_DIR__/complex_list.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT i FROM parquet_scan('__TEST_DIR__/complex_list.parquet');\n+----\n+[[[[1, 2, 3], [4, 5], [], [6, 7]], [[8, NULL, 10], NULL, []], [], NULL, [[11, 12, 13, 14], [], NULL, [], [], [15], [NULL, NULL, NULL]]]]\n+NULL\n+[NULL]\n+[[], NULL, [], []]\n+[[[NULL, NULL, [NULL]], NULL, [[], [7, 8, 9], [NULL], NULL, []]], [], [NULL]]\ndiff --git a/test/sql/copy/parquet/writer/write_list.test b/test/sql/copy/parquet/writer/write_list.test\nnew file mode 100644\nindex 000000000000..b102a4dd2c74\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/write_list.test\n@@ -0,0 +1,92 @@\n+# name: test/sql/copy/parquet/writer/write_list.test\n+# description: Parquet write list\n+# group: [writer]\n+\n+require parquet\n+\n+# standard list\n+statement ok\n+CREATE TABLE list AS SELECT * FROM (VALUES\n+\t([1, 2, 3]),\n+\t([4, 5]),\n+\t([6, 7]),\n+    ([8, 9, 10, 11])\n+) tbl(i);\n+\n+statement ok\n+COPY list TO '__TEST_DIR__/test_list.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT i FROM parquet_scan('__TEST_DIR__/test_list.parquet');\n+----\n+[1, 2, 3]\n+[4, 5]\n+[6, 7]\n+[8, 9, 10, 11]\n+\n+# empty and NULL lists\n+statement ok\n+CREATE TABLE null_empty_list AS SELECT * FROM (VALUES\n+\t([1, 2, 3]),\n+\t([4, 5]),\n+\t([6, 7]),\n+\t([NULL]),\n+\t([]),\n+\t([]),\n+\t([]),\n+\t([]),\n+    ([8, NULL, 10, 11]),\n+    (NULL)\n+) tbl(i);\n+\n+statement ok\n+COPY null_empty_list TO '__TEST_DIR__/test_list.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT * FROM parquet_scan('__TEST_DIR__/test_list.parquet');\n+----\n+[1, 2, 3]\n+[4, 5]\n+[6, 7]\n+[NULL]\n+[]\n+[]\n+[]\n+[]\n+[8, NULL, 10, 11]\n+NULL\n+\n+# empty list\n+statement ok\n+COPY (SELECT []::INT[]) TO '__TEST_DIR__/test_empty_list.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/test_empty_list.parquet'\n+----\n+[]\n+\n+# null list\n+statement ok\n+COPY (SELECT NULL::INT[]) TO '__TEST_DIR__/test_null_list.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/test_null_list.parquet'\n+----\n+NULL\n+\n+# big list (> vector size)\n+statement ok\n+CREATE TABLE big_list AS SELECT LIST(CASE WHEN i%2=0 THEN NULL ELSE i END) l FROM range(20000) tbl(i);\n+\n+query I\n+SELECT SUM(i) FROM (SELECT UNNEST(l) FROM big_list) t(i)\n+----\n+100000000\n+\n+statement ok\n+COPY big_list TO '__TEST_DIR__/big_list.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT SUM(i) FROM (SELECT UNNEST(l) FROM '__TEST_DIR__/big_list.parquet') t(i)\n+----\n+100000000\ndiff --git a/test/sql/copy/parquet/writer/write_struct.test b/test/sql/copy/parquet/writer/write_struct.test\nnew file mode 100644\nindex 000000000000..29fb3923e167\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/write_struct.test\n@@ -0,0 +1,139 @@\n+# name: test/sql/copy/parquet/writer/write_struct.test\n+# description: Parquet write struct\n+# group: [writer]\n+\n+require parquet\n+\n+# standard struct\n+statement ok\n+CREATE TABLE struct AS SELECT * FROM (VALUES\n+\t({'a': 42, 'b': 84}),\n+\t({'a': 33, 'b': 32}),\n+\t({'a': 42, 'b': 27})\n+) tbl(i);\n+\n+statement ok\n+COPY struct TO '__TEST_DIR__/test_struct.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT * FROM parquet_scan('__TEST_DIR__/test_struct.parquet');\n+----\n+{'a': 42, 'b': 84}\n+{'a': 33, 'b': 32}\n+{'a': 42, 'b': 27}\n+\n+# struct with nulls\n+statement ok\n+CREATE TABLE struct_nulls AS SELECT * FROM (VALUES\n+\t({'a': 42, 'b': 84}),\n+\t({'a': NULL, 'b': 32}),\n+\t(NULL),\n+\t({'a': 42, 'b': NULL})\n+) tbl(i);\n+\n+statement ok\n+COPY struct_nulls TO '__TEST_DIR__/test_struct_nulls.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT * FROM parquet_scan('__TEST_DIR__/test_struct_nulls.parquet');\n+----\n+{'a': 42, 'b': 84}\n+{'a': NULL, 'b': 32}\n+NULL\n+{'a': 42, 'b': NULL}\n+\n+# nested structs\n+statement ok\n+CREATE TABLE struct_nested AS SELECT * FROM (VALUES\n+\t({'a': {'x': 3, 'x1': 22}, 'b': {'y': 27, 'y1': 44}}),\n+\t({'a': {'x': 9, 'x1': 26}, 'b': {'y': 1, 'y1': 999}}),\n+\t({'a': {'x': 17, 'x1': 23}, 'b': {'y': 3, 'y1': 9999}})\n+) tbl(i);\n+\n+statement ok\n+COPY struct_nested TO '__TEST_DIR__/struct_nested.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT * FROM parquet_scan('__TEST_DIR__/struct_nested.parquet');\n+----\n+{'a': {'x': 3, 'x1': 22}, 'b': {'y': 27, 'y1': 44}}\n+{'a': {'x': 9, 'x1': 26}, 'b': {'y': 1, 'y1': 999}}\n+{'a': {'x': 17, 'x1': 23}, 'b': {'y': 3, 'y1': 9999}}\n+\n+# nested structs\n+statement ok\n+CREATE TABLE struct_nested_null AS SELECT * FROM (VALUES\n+\t({'a': {'x': 3, 'x1': 22}, 'b': {'y': NULL, 'y1': 44}}),\n+\t({'a': {'x': NULL, 'x1': 26}, 'b': {'y': 1, 'y1': NULL}}),\n+\t({'a': {'x': 17, 'x1': NULL}, 'b': {'y': 3, 'y1': 9999}}),\n+\t(NULL),\n+\t({'a': NULL, 'b': NULL})\n+) tbl(i);\n+\n+statement ok\n+COPY struct_nested_null TO '__TEST_DIR__/struct_nested_null.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT * FROM parquet_scan('__TEST_DIR__/struct_nested_null.parquet');\n+----\n+{'a': {'x': 3, 'x1': 22}, 'b': {'y': NULL, 'y1': 44}}\n+{'a': {'x': NULL, 'x1': 26}, 'b': {'y': 1, 'y1': NULL}}\n+{'a': {'x': 17, 'x1': NULL}, 'b': {'y': 3, 'y1': 9999}}\n+NULL\n+{'a': NULL, 'b': NULL}\n+\n+# single struct\n+statement ok\n+CREATE TABLE single_struct AS SELECT * FROM (VALUES\n+\t({'a': 42}),\n+\t({'a': 33}),\n+\t({'a': 42})\n+) tbl(i);\n+\n+statement ok\n+COPY single_struct TO '__TEST_DIR__/single_struct.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT * FROM parquet_scan('__TEST_DIR__/single_struct.parquet');\n+----\n+{'a': 42}\n+{'a': 33}\n+{'a': 42}\n+\n+# single struct nulls\n+statement ok\n+CREATE TABLE single_struct_null AS SELECT * FROM (VALUES\n+\t({'a': 42}),\n+\t({'a': NULL}),\n+\t(NULL)\n+) tbl(i);\n+\n+statement ok\n+COPY single_struct_null TO '__TEST_DIR__/single_struct_null.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT * FROM parquet_scan('__TEST_DIR__/single_struct_null.parquet');\n+----\n+{'a': 42}\n+{'a': NULL}\n+NULL\n+\n+# nested single struct\n+statement ok\n+CREATE TABLE nested_single_struct AS SELECT * FROM (VALUES\n+\t({'a': {'b': 42}}),\n+\t({'a': {'b': NULL}}),\n+\t({'a': NULL}),\n+\t(NULL)\n+) tbl(i);\n+\n+statement ok\n+COPY nested_single_struct TO '__TEST_DIR__/nested_single_struct.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT * FROM parquet_scan('__TEST_DIR__/nested_single_struct.parquet');\n+----\n+{'a': {'b': 42}}\n+{'a': {'b': NULL}}\n+{'a': NULL}\n+NULL\ndiff --git a/test/sql/copy/parquet/writer/writer_round_trip.test_slow b/test/sql/copy/parquet/writer/writer_round_trip.test_slow\nnew file mode 100644\nindex 000000000000..40afd4e86097\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/writer_round_trip.test_slow\n@@ -0,0 +1,35 @@\n+# name: test/sql/copy/parquet/writer/writer_round_trip.test_slow\n+# description: Parquet read and re-write various files\n+# group: [writer]\n+\n+require parquet\n+\n+foreach parquet_file data/parquet-testing/manyrowgroups.parquet data/parquet-testing/map.parquet data/parquet-testing/arrow/int32_decimal.parquet data/parquet-testing/arrow/nonnullable.impala.parquet data/parquet-testing/bug687_nulls.parquet data/parquet-testing/bug1554.parquet data/parquet-testing/apkwan.parquet data/parquet-testing/arrow/nested_lists.snappy.parquet data/parquet-testing/arrow/nulls.snappy.parquet data/parquet-testing/nan-float.parquet data/parquet-testing/manyrowgroups2.parquet data/parquet-testing/struct.parquet data/parquet-testing/arrow/list_columns.parquet data/parquet-testing/timestamp-ms.parquet data/parquet-testing/arrow/alltypes_dictionary.parquet data/parquet-testing/arrow/binary.parquet data/parquet-testing/arrow/nation.dict-malformed.parquet data/parquet-testing/lineitem-top10000.gzip.parquet data/parquet-testing/arrow/nested_maps.snappy.parquet data/parquet-testing/arrow/dict-page-offset-zero.parquet data/parquet-testing/silly-names.parquet data/parquet-testing/zstd.parquet data/parquet-testing/bug1618_struct_strings.parquet data/parquet-testing/arrow/single_nan.parquet data/parquet-testing/arrow/int64_decimal.parquet data/parquet-testing/filter_bug1391.parquet data/parquet-testing/arrow/fixed_length_decimal_legacy.parquet data/parquet-testing/timestamp.parquet data/parquet-testing/arrow/fixed_length_decimal.parquet data/parquet-testing/leftdate3_192_loop_1.parquet data/parquet-testing/blob.parquet data/parquet-testing/bug1588.parquet data/parquet-testing/bug1589.parquet data/parquet-testing/arrow/alltypes_plain.parquet data/parquet-testing/arrow/repeated_no_annotation.parquet data/parquet-testing/data-types.parquet data/parquet-testing/unsigned.parquet data/parquet-testing/pandas-date.parquet data/parquet-testing/date.parquet data/parquet-testing/arrow/nullable.impala.parquet data/parquet-testing/fixed.parquet data/parquet-testing/arrow/alltypes_plain.snappy.parquet data/parquet-testing/decimal/int32_decimal.parquet data/parquet-testing/decimal/pandas_decimal.parquet data/parquet-testing/decimal/decimal_dc.parquet data/parquet-testing/decimal/int64_decimal.parquet data/parquet-testing/decimal/fixed_length_decimal_legacy.parquet data/parquet-testing/decimal/fixed_length_decimal.parquet data/parquet-testing/glob2/t1.parquet data/parquet-testing/cache/cache1.parquet data/parquet-testing/cache/cache2.parquet data/parquet-testing/glob/t2.parquet data/parquet-testing/glob/t1.parquet data/parquet-testing/bug2557.parquet\n+\n+statement ok\n+CREATE TABLE parquet_read AS SELECT * FROM parquet_scan('${parquet_file}');\n+\n+statement ok\n+COPY parquet_read TO '__TEST_DIR__/test_round_trip.parquet'\n+\n+statement ok\n+CREATE TABLE parquet_write AS SELECT * FROM parquet_scan('__TEST_DIR__/test_round_trip.parquet');\n+\n+# verify that the count is the same\n+query I\n+SELECT COUNT(*) FROM parquet_read EXCEPT SELECT COUNT(*) FROM parquet_write\n+----\n+\n+# verify that the data is the same\n+query I\n+SELECT COUNT(*) FROM (SELECT * FROM parquet_read EXCEPT SELECT * FROM parquet_write)\n+----\n+0\n+\n+statement ok\n+DROP TABLE parquet_read\n+\n+statement ok\n+DROP TABLE parquet_write\n+\n+endloop\n\\ No newline at end of file\ndiff --git a/test/sqlite/result_helper.cpp b/test/sqlite/result_helper.cpp\nindex 4c90ab26cb8f..c24e8fa9d271 100644\n--- a/test/sqlite/result_helper.cpp\n+++ b/test/sqlite/result_helper.cpp\n@@ -600,36 +600,29 @@ bool TestResultHelper::CompareValues(string lvalue_str, string rvalue_str, idx_t\n \tauto sql_type = result.types[current_column];\n \tif (sql_type.IsNumeric()) {\n \t\tbool converted_lvalue = false;\n-\t\ttry {\n-\t\t\tif (lvalue_str == \"NULL\") {\n-\t\t\t\tlvalue = Value(sql_type);\n-\t\t\t} else {\n-\t\t\t\tlvalue = Value(lvalue_str);\n-\t\t\t\tif (!lvalue.TryCastAs(sql_type)) {\n-\t\t\t\t\treturn false;\n-\t\t\t\t}\n-\t\t\t}\n+\t\tbool converted_rvalue = false;\n+\t\tif (lvalue_str == \"NULL\") {\n+\t\t\tlvalue = Value(sql_type);\n \t\t\tconverted_lvalue = true;\n-\t\t\tif (rvalue_str == \"NULL\") {\n-\t\t\t\trvalue = Value(sql_type);\n-\t\t\t} else {\n-\t\t\t\trvalue = Value(rvalue_str);\n-\t\t\t\tif (!rvalue.TryCastAs(sql_type)) {\n-\t\t\t\t\treturn false;\n-\t\t\t\t}\n+\t\t} else {\n+\t\t\tlvalue = Value(lvalue_str);\n+\t\t\tif (lvalue.TryCastAs(sql_type)) {\n+\t\t\t\tconverted_lvalue = true;\n+\t\t\t}\n+\t\t}\n+\t\tif (rvalue_str == \"NULL\") {\n+\t\t\trvalue = Value(sql_type);\n+\t\t\tconverted_rvalue = true;\n+\t\t} else {\n+\t\t\trvalue = Value(rvalue_str);\n+\t\t\tif (rvalue.TryCastAs(sql_type)) {\n+\t\t\t\tconverted_rvalue = true;\n \t\t\t}\n+\t\t}\n+\t\tif (converted_lvalue && converted_rvalue) {\n \t\t\terror = !Value::ValuesAreEqual(lvalue, rvalue);\n-\t\t} catch (std::exception &ex) {\n-\t\t\tPrintErrorHeader(\"Test error!\");\n-\t\t\tPrintLineSep();\n-\t\t\tPrintSQL(sql_query);\n-\t\t\tPrintLineSep();\n-\t\t\tstd::cerr << termcolor::red << termcolor::bold << \"Cannot convert value \"\n-\t\t\t          << (converted_lvalue ? rvalue_str : lvalue_str) << \" to type \" << sql_type.ToString()\n-\t\t\t          << termcolor::reset << std::endl;\n-\t\t\tstd::cerr << termcolor::red << termcolor::bold << ex.what() << termcolor::reset << std::endl;\n-\t\t\tPrintLineSep();\n-\t\t\treturn false;\n+\t\t} else {\n+\t\t\terror = true;\n \t\t}\n \t} else if (sql_type == LogicalType::BOOLEAN) {\n \t\tauto low_r_val = StringUtil::Lower(rvalue_str);\ndiff --git a/test/sqlite/sqllogic_test_runner.cpp b/test/sqlite/sqllogic_test_runner.cpp\nindex 68bb7eead6c2..22598f988356 100644\n--- a/test/sqlite/sqllogic_test_runner.cpp\n+++ b/test/sqlite/sqllogic_test_runner.cpp\n@@ -386,6 +386,10 @@ void SQLLogicTestRunner::ExecuteFile(string script) {\n #if LDBL_MANT_DIG < 54\n \t\t\t\treturn;\n #endif\n+\t\t\t} else if (param == \"64bit\") {\n+\t\t\t\tif (sizeof(void *) != 8) {\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n \t\t\t} else if (param == \"noforcestorage\") {\n \t\t\t\tif (TestForceStorage()) {\n \t\t\t\t\treturn;\n",
  "problem_statement": "Parquet list columns read incorrectly\n#### What happens?\r\nWhen scanning parquet files with list columns, results with incorrect offsets can be returned.\r\n\r\n#### To Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nimport pyarrow as pa\r\nimport duckdb\r\nimport pyarrow.parquet as pq\r\nR = 100_000\r\nfake_data = pa.array([np.arange(i, i + 100).astype('float64') for i in range(R)])\r\ntb = pa.table({'word0': pa.array([str(i) for i in range(0, R)]), 'year_counts': fake_data})\r\npq.write_table(tb, \"test.parquet\")\r\n\r\ncon = duckdb.connect(\":memory:\")\r\n\r\ncon.query(\"SELECT year_counts FROM parquet_scan('test.parquet') WHERE word0='90000'\")\r\n\r\n```\r\n\r\nThis should return a list-column of length 100 starting at 90,000; instead it starts with 912.\r\n\r\n```python\r\npq.read_table(\"test.parquet\", columns = ['year_counts'], filters = [(\"word0\", \"=\", \"90000\")])['year_counts']\r\n```\r\n\r\nreturns the correct result.\r\n\r\n#### Environment (please complete the following information):\r\n - OS: OS X\r\n - DuckDB Version: 0.3.1-dev550\r\n - DuckDB Client: Python / WASM\r\n\r\n#### Before Submitting\r\n\r\n- [x] **Have you tried this on the latest `master` branch?**\r\n* **Python**: `pip install duckdb --upgrade --pre`\r\n* **R**: `install.packages(\"https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz\", repos = NULL)`\r\n* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.\r\n\r\n- [X] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**\r\n\nCOPY TO  parquet with ZSTD produces broken column with 100k rows (~3GB uncompressed, 45MB compressed)\n#### What happens?\r\nCOPY TO  parquet with ZSTD produces broken column with 100k rows (~3GB uncompressed, 30MB compressed)\r\n\r\n```python\r\nduckdb.query(\"\"\"\r\n    SELECT sequence FROM 'duckseq.parquet'\r\n    USING SAMPLE 10;\r\n    \"\"\").fetchall()\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n~/code/duckdb-test/test.py in <module>\r\n----> 47 duckdb.query(\"\"\"\r\n      48     SELECT sequence FROM 'duckseq.parquet'\r\n      49     USING SAMPLE 10;\r\n      50     \"\"\").fetchall()\r\n\r\nRuntimeError: ZSTD Decompression failure\r\n```\r\n\r\n#### To Reproduce\r\n```python\r\nimport duckdb\r\ncon = duckdb.connect(\"::memory::\")\r\ncon.execute(\"\"\"\r\n    COPY (SELECT * FROM read_csv_auto('sequences.csv.gz', delim=',', header=True) )\r\n    TO 'duckseq.parquet' \r\n    (FORMAT 'PARQUET', CODEC 'ZSTD');\r\n    \"\"\")\r\n```\r\nFile: https://transfer.sh/0ugKVN/sequences.csv.gz\r\n\r\n#### Environment (please complete the following information):\r\n - OS: macOS 12.1 Intel\r\n - DuckDB Version: 0.3.1\r\n - DuckDB Client: Python\r\n\r\n#### Possible root cause\r\n\r\nI think I know what happens. When I observe htop, I see that the memory requirements balloon, going to 30% and more. Then suddenly memory goes to 0 while python command still runs. Only a few seconds later does Python complete, _without throwing an error_.\r\n\r\nIt looks like the error handling is broken. The process gets killed as OOM but Python never tells the end user about it nor cleans up the resulting file.\r\n\r\n\n",
  "hints_text": "Thanks for reporting, we will have a look\nHello everyone, this bug impacted one of the prototypes we were exploring using DuckDB + Node for, so I took a stab at fixing the problem myself. I will add some regression tests to ensure that this return in some nasty form, but in the meantime if there is anything else that needs doing to help this get merged, please let me know :)\n",
  "created_at": "2021-12-22T16:26:20Z"
}