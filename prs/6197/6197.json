{
  "repo": "duckdb/duckdb",
  "pull_number": 6197,
  "instance_id": "duckdb__duckdb-6197",
  "issue_numbers": [
    "5983",
    "5983",
    "5983"
  ],
  "base_commit": "33d17b8cfefbb1694224781779c5a3087c5177f3",
  "patch": "diff --git a/src/common/types/value.cpp b/src/common/types/value.cpp\nindex a407149c5dde..b97865238bb1 100644\n--- a/src/common/types/value.cpp\n+++ b/src/common/types/value.cpp\n@@ -1584,6 +1584,10 @@ bool Value::DefaultTryCastAs(const LogicalType &target_type, bool strict) {\n \treturn TryCastAs(set, get_input, target_type, strict);\n }\n \n+void Value::Reinterpret(LogicalType new_type) {\n+\tthis->type_ = std::move(new_type);\n+}\n+\n void Value::Serialize(Serializer &main_serializer) const {\n \tFieldWriter writer(main_serializer);\n \twriter.WriteSerializable(type_);\ndiff --git a/src/common/types/vector.cpp b/src/common/types/vector.cpp\nindex b3bcb10f6505..db35dfd0f24a 100644\n--- a/src/common/types/vector.cpp\n+++ b/src/common/types/vector.cpp\n@@ -588,7 +588,7 @@ Value Vector::GetValue(const Vector &v_p, idx_t index_p) {\n \tauto value = GetValueInternal(v_p, index_p);\n \t// set the alias of the type to the correct value, if there is a type alias\n \tif (v_p.GetType().HasAlias()) {\n-\t\tvalue.type().CopyAuxInfo(v_p.GetType());\n+\t\tvalue.GetTypeMutable().CopyAuxInfo(v_p.GetType());\n \t}\n \tif (v_p.GetType().id() != LogicalTypeId::AGGREGATE_STATE && value.type().id() != LogicalTypeId::AGGREGATE_STATE) {\n \ndiff --git a/src/include/duckdb/common/types/value.hpp b/src/include/duckdb/common/types/value.hpp\nindex 5dfe43c63257..7083f2d2ced9 100644\n--- a/src/include/duckdb/common/types/value.hpp\n+++ b/src/include/duckdb/common/types/value.hpp\n@@ -63,7 +63,7 @@ class Value {\n \t// move assignment\n \tDUCKDB_API Value &operator=(Value &&other) noexcept;\n \n-\tinline LogicalType &type() {\n+\tinline LogicalType &GetTypeMutable() {\n \t\treturn type_;\n \t}\n \tinline const LogicalType &type() const {\n@@ -221,6 +221,8 @@ class Value {\n \tDUCKDB_API bool TryCastAs(ClientContext &context, const LogicalType &target_type, bool strict = false);\n \tDUCKDB_API bool DefaultTryCastAs(const LogicalType &target_type, bool strict = false);\n \n+\tDUCKDB_API void Reinterpret(LogicalType new_type);\n+\n \t//! Serializes a Value to a stand-alone binary blob\n \tDUCKDB_API void Serialize(Serializer &serializer) const;\n \t//! Deserializes a Value from a blob\ndiff --git a/src/storage/statistics/numeric_statistics.cpp b/src/storage/statistics/numeric_statistics.cpp\nindex d28a01bc55dd..ad9434bbc52a 100644\n--- a/src/storage/statistics/numeric_statistics.cpp\n+++ b/src/storage/statistics/numeric_statistics.cpp\n@@ -124,15 +124,113 @@ bool NumericStatistics::IsConstant() const {\n \treturn max <= min;\n }\n \n+void SerializeNumericStatsValue(const Value &val, FieldWriter &writer) {\n+\twriter.WriteField<bool>(val.IsNull());\n+\tif (val.IsNull()) {\n+\t\treturn;\n+\t}\n+\tswitch (val.type().InternalType()) {\n+\tcase PhysicalType::BOOL:\n+\t\twriter.WriteField<bool>(BooleanValue::Get(val));\n+\t\tbreak;\n+\tcase PhysicalType::INT8:\n+\t\twriter.WriteField<int8_t>(TinyIntValue::Get(val));\n+\t\tbreak;\n+\tcase PhysicalType::INT16:\n+\t\twriter.WriteField<int16_t>(SmallIntValue::Get(val));\n+\t\tbreak;\n+\tcase PhysicalType::INT32:\n+\t\twriter.WriteField<int32_t>(IntegerValue::Get(val));\n+\t\tbreak;\n+\tcase PhysicalType::INT64:\n+\t\twriter.WriteField<int64_t>(BigIntValue::Get(val));\n+\t\tbreak;\n+\tcase PhysicalType::UINT8:\n+\t\twriter.WriteField<int8_t>(UTinyIntValue::Get(val));\n+\t\tbreak;\n+\tcase PhysicalType::UINT16:\n+\t\twriter.WriteField<int16_t>(USmallIntValue::Get(val));\n+\t\tbreak;\n+\tcase PhysicalType::UINT32:\n+\t\twriter.WriteField<int32_t>(UIntegerValue::Get(val));\n+\t\tbreak;\n+\tcase PhysicalType::UINT64:\n+\t\twriter.WriteField<int64_t>(UBigIntValue::Get(val));\n+\t\tbreak;\n+\tcase PhysicalType::INT128:\n+\t\twriter.WriteField<hugeint_t>(HugeIntValue::Get(val));\n+\t\tbreak;\n+\tcase PhysicalType::FLOAT:\n+\t\twriter.WriteField<float>(FloatValue::Get(val));\n+\t\tbreak;\n+\tcase PhysicalType::DOUBLE:\n+\t\twriter.WriteField<double>(DoubleValue::Get(val));\n+\t\tbreak;\n+\tdefault:\n+\t\tthrow InternalException(\"Unsupported type for serializing numeric statistics\");\n+\t}\n+}\n+\n void NumericStatistics::Serialize(FieldWriter &writer) const {\n-\twriter.WriteSerializable(min);\n-\twriter.WriteSerializable(max);\n+\tSerializeNumericStatsValue(min, writer);\n+\tSerializeNumericStatsValue(max, writer);\n+}\n+\n+Value DeserializeNumericStatsValue(const LogicalType &type, FieldReader &reader) {\n+\tauto is_null = reader.ReadRequired<bool>();\n+\tif (is_null) {\n+\t\treturn Value(type);\n+\t}\n+\tValue result;\n+\tswitch (type.InternalType()) {\n+\tcase PhysicalType::BOOL:\n+\t\tresult = Value::BOOLEAN(reader.ReadRequired<bool>());\n+\t\tbreak;\n+\tcase PhysicalType::INT8:\n+\t\tresult = Value::TINYINT(reader.ReadRequired<int8_t>());\n+\t\tbreak;\n+\tcase PhysicalType::INT16:\n+\t\tresult = Value::SMALLINT(reader.ReadRequired<int16_t>());\n+\t\tbreak;\n+\tcase PhysicalType::INT32:\n+\t\tresult = Value::INTEGER(reader.ReadRequired<int32_t>());\n+\t\tbreak;\n+\tcase PhysicalType::INT64:\n+\t\tresult = Value::BIGINT(reader.ReadRequired<int64_t>());\n+\t\tbreak;\n+\tcase PhysicalType::UINT8:\n+\t\tresult = Value::UTINYINT(reader.ReadRequired<uint8_t>());\n+\t\tbreak;\n+\tcase PhysicalType::UINT16:\n+\t\tresult = Value::USMALLINT(reader.ReadRequired<uint16_t>());\n+\t\tbreak;\n+\tcase PhysicalType::UINT32:\n+\t\tresult = Value::UINTEGER(reader.ReadRequired<uint32_t>());\n+\t\tbreak;\n+\tcase PhysicalType::UINT64:\n+\t\tresult = Value::UBIGINT(reader.ReadRequired<uint64_t>());\n+\t\tbreak;\n+\tcase PhysicalType::INT128:\n+\t\tresult = Value::HUGEINT(reader.ReadRequired<hugeint_t>());\n+\t\tbreak;\n+\tcase PhysicalType::FLOAT:\n+\t\tresult = Value::FLOAT(reader.ReadRequired<float>());\n+\t\tbreak;\n+\tcase PhysicalType::DOUBLE:\n+\t\tresult = Value::DOUBLE(reader.ReadRequired<double>());\n+\t\tbreak;\n+\tdefault:\n+\t\tthrow InternalException(\"Unsupported type for deserializing numeric statistics\");\n+\t}\n+\tresult.Reinterpret(type);\n+\treturn result;\n }\n \n unique_ptr<BaseStatistics> NumericStatistics::Deserialize(FieldReader &reader, LogicalType type) {\n-\tauto min = reader.ReadRequiredSerializable<Value, Value>();\n-\tauto max = reader.ReadRequiredSerializable<Value, Value>();\n-\treturn make_unique_base<BaseStatistics, NumericStatistics>(std::move(type), min, max, StatisticsType::LOCAL_STATS);\n+\tauto min = DeserializeNumericStatsValue(type, reader);\n+\tauto max = DeserializeNumericStatsValue(type, reader);\n+\treturn make_unique_base<BaseStatistics, NumericStatistics>(std::move(type), std::move(min), std::move(max),\n+\t                                                           StatisticsType::LOCAL_STATS);\n }\n \n string NumericStatistics::ToString() const {\ndiff --git a/src/storage/storage_info.cpp b/src/storage/storage_info.cpp\nindex 8456cf4e32d3..c9da9142bbe8 100644\n--- a/src/storage/storage_info.cpp\n+++ b/src/storage/storage_info.cpp\n@@ -2,7 +2,7 @@\n \n namespace duckdb {\n \n-const uint64_t VERSION_NUMBER = 42;\n+const uint64_t VERSION_NUMBER = 43;\n \n struct StorageVersionInfo {\n \tconst char *version_name;\n",
  "test_patch": "diff --git a/test/sql/storage_version/storage_version.db b/test/sql/storage_version/storage_version.db\nindex 9b06b0453a0d..1994ddb6b10c 100644\nBinary files a/test/sql/storage_version/storage_version.db and b/test/sql/storage_version/storage_version.db differ\n",
  "problem_statement": "Enums functionality fails when memory limit is low\n### What happens?\r\n\r\nConverting a table with varchar columns to a table with enum columns increases the size of a duckdb database by more than double. The operation does not complete either. This happens when the size of the database is close to the memory limit of the duckdb persistent session.\r\n### To Reproduce\r\n\r\n```bash\r\n./build/debug/duckdb mem_limit_enum.duckdb\r\n```\r\n\r\n```SQL\r\nset memory_limit='2GB';\r\nload tpch;\r\ncall dbgen(sf=2);\r\nSELECT count(distinct(l_comment)) from lineitem;\r\nCREATE TYPE l_comment_enum as ENUM(select l_comment from lineitem); # statement 1\r\nCREATE TABLE lineitem2 (comment l_comment_enum); # statement 2\r\n```\r\nAfter statement 2, the creation of the table increases the size of the database by over 50%.\r\nYou can keep creating tables to show this\r\n```\r\nINSERT INTO lineitem2 (select l_comment from lineitem);\r\n```\r\nObserve as the size of mem_limit_enum.duckdb increases to over 5GB and the insert statement does not complete.\r\n\r\n\r\n\r\n### OS:\r\n\r\nMacOs Monterey\r\n\r\n### DuckDB Version:\r\n\r\nv0.6.2-dev1218\r\n\r\n### DuckDB Client:\r\n\r\nCLI\r\n\r\n### Full Name:\r\n\r\nTom Ebergen\r\n\r\n### Affiliation:\r\n\r\nDuckDB labs\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\n- [X] I agree\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] I agree\nEnums functionality fails when memory limit is low\n### What happens?\r\n\r\nConverting a table with varchar columns to a table with enum columns increases the size of a duckdb database by more than double. The operation does not complete either. This happens when the size of the database is close to the memory limit of the duckdb persistent session.\r\n### To Reproduce\r\n\r\n```bash\r\n./build/debug/duckdb mem_limit_enum.duckdb\r\n```\r\n\r\n```SQL\r\nset memory_limit='2GB';\r\nload tpch;\r\ncall dbgen(sf=2);\r\nSELECT count(distinct(l_comment)) from lineitem;\r\nCREATE TYPE l_comment_enum as ENUM(select l_comment from lineitem); # statement 1\r\nCREATE TABLE lineitem2 (comment l_comment_enum); # statement 2\r\n```\r\nAfter statement 2, the creation of the table increases the size of the database by over 50%.\r\nYou can keep creating tables to show this\r\n```\r\nINSERT INTO lineitem2 (select l_comment from lineitem);\r\n```\r\nObserve as the size of mem_limit_enum.duckdb increases to over 5GB and the insert statement does not complete.\r\n\r\n\r\n\r\n### OS:\r\n\r\nMacOs Monterey\r\n\r\n### DuckDB Version:\r\n\r\nv0.6.2-dev1218\r\n\r\n### DuckDB Client:\r\n\r\nCLI\r\n\r\n### Full Name:\r\n\r\nTom Ebergen\r\n\r\n### Affiliation:\r\n\r\nDuckDB labs\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\n- [X] I agree\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] I agree\nEnums functionality fails when memory limit is low\n### What happens?\r\n\r\nConverting a table with varchar columns to a table with enum columns increases the size of a duckdb database by more than double. The operation does not complete either. This happens when the size of the database is close to the memory limit of the duckdb persistent session.\r\n### To Reproduce\r\n\r\n```bash\r\n./build/debug/duckdb mem_limit_enum.duckdb\r\n```\r\n\r\n```SQL\r\nset memory_limit='2GB';\r\nload tpch;\r\ncall dbgen(sf=2);\r\nSELECT count(distinct(l_comment)) from lineitem;\r\nCREATE TYPE l_comment_enum as ENUM(select l_comment from lineitem); # statement 1\r\nCREATE TABLE lineitem2 (comment l_comment_enum); # statement 2\r\n```\r\nAfter statement 2, the creation of the table increases the size of the database by over 50%.\r\nYou can keep creating tables to show this\r\n```\r\nINSERT INTO lineitem2 (select l_comment from lineitem);\r\n```\r\nObserve as the size of mem_limit_enum.duckdb increases to over 5GB and the insert statement does not complete.\r\n\r\n\r\n\r\n### OS:\r\n\r\nMacOs Monterey\r\n\r\n### DuckDB Version:\r\n\r\nv0.6.2-dev1218\r\n\r\n### DuckDB Client:\r\n\r\nCLI\r\n\r\n### Full Name:\r\n\r\nTom Ebergen\r\n\r\n### Affiliation:\r\n\r\nDuckDB labs\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\n- [X] I agree\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] I agree\n",
  "hints_text": "I suspect this has to do with the size of the enum and it being serialized in separate places. The `l_comment` enum must be at least 1GB by itself. I think exceeding the memory limit is not that much of a problem - memory managing enums would create a lot of complications. Perhaps we should hard-limit enums to some reasonable value like 10MB of string data?\nI agree with Mytherin that managing the memory of an ENUM can be challenging. \r\n\r\nImplementing a hard limit may prevent users from creating an ENUM, but perhaps we should consider making it a configurable option.\r\n\r\nOne potential solution for making ENUM memory management easier is to use an ART in place of an unordered map. However, this might have nasty consequences for the OG memory footprint. (:\nSo the specific case where this is causing issues for me is a enum type with 1000000 (1 mil) values in a table with 1,000,000,000 (1 billion) rows.\r\n\r\nEach enum has a length of 3 - 8 characters. I'll see if I can reproduce an example using a tpch dataset\nI wonder if we might be serializing types in too many places in general - which wouldn't be a problem for regular types but becomes a problem with larger enums. \n@pdet Here are steps to reproduce where only 750,001 enums are created for a table that will have 11mil rows eventually. I also did not need to change my memory limit (16GB by default).\r\n\r\n```\r\ncall dbgen(sf=2);\r\nselect distinct(l_orderkey/4) from lineitem; -- shows the amount of values we will have in our enum\r\ncreate type orderkey_enum as enum (Select (l_orderkey/4)::VARCHAR from lineitem);\r\ncreate table t2 (c1 orderkey_enum);\r\ninsert into t2 (select (l_orderkey/4)::VARCHAR from lineitem); -- this blows up the size of the database\r\n```\nI had a long go at this today. Unfortunately, I did not succeed in reproducing the issue on my machine.\r\n\r\nI did ensure that nothing was being copied out of the enum type (by making all members private and explicitly deleting the copy constructor/assignment). I've also removed the many scans over the data in constructing the enum type from queries *, which is most likely unrelated to this issue*.\r\n\r\n@Tmonster, maybe you can do a quick build and run of https://github.com/pdet/duckdb/tree/enum_test ? If that does not help, we can sit together tomorrow and have a go at this.\n@pdet the issue isn't as bad now, but the size of the database still increases from 492M to 3.5GB as a result of running \r\n\r\n`insert into t2 (select (l_orderkey/4)::VARCHAR from lineitem); `\r\n\r\nIt seems to me there is still an issue. \nAh, sorry, I understood that the problem was a memory blow-up, I guess I mixed information from the meetings and this issue (:. Indeed, Mytherin analyzed this correctly. \r\n\r\nThe issue with the disk blowup is that the type is reserialized on the catalog and at every column that defined the enum, we should probably serialize just the one in the catalog and properly consume it back from the catalog on a load. Will work on that tomorrow.\nWould the type reserialization result in a 3GB expansion on disk when one enum column is populated? I would expect the size of the DB to increase according to the number of rows inserted, and 750001 rows in a one column table doesn't sound like 3GB, especially since tpch at sf=2 is only 500MB.\r\n\r\n\nFigured out the issue - the `NumericStatistics` class serializes a `Value` as the `min/max`, which includes re-serializing the type. This causes the enum to be re-serialized for every row group.\r\n\r\nThere is still the issue of duplicating the enum for every column that uses it but that does not scale with the size of the data so that is less of an issue.\nI suspect this has to do with the size of the enum and it being serialized in separate places. The `l_comment` enum must be at least 1GB by itself. I think exceeding the memory limit is not that much of a problem - memory managing enums would create a lot of complications. Perhaps we should hard-limit enums to some reasonable value like 10MB of string data?\nI agree with Mytherin that managing the memory of an ENUM can be challenging. \r\n\r\nImplementing a hard limit may prevent users from creating an ENUM, but perhaps we should consider making it a configurable option.\r\n\r\nOne potential solution for making ENUM memory management easier is to use an ART in place of an unordered map. However, this might have nasty consequences for the OG memory footprint. (:\nSo the specific case where this is causing issues for me is a enum type with 1000000 (1 mil) values in a table with 1,000,000,000 (1 billion) rows.\r\n\r\nEach enum has a length of 3 - 8 characters. I'll see if I can reproduce an example using a tpch dataset\nI wonder if we might be serializing types in too many places in general - which wouldn't be a problem for regular types but becomes a problem with larger enums. \n@pdet Here are steps to reproduce where only 750,001 enums are created for a table that will have 11mil rows eventually. I also did not need to change my memory limit (16GB by default).\r\n\r\n```\r\ncall dbgen(sf=2);\r\nselect distinct(l_orderkey/4) from lineitem; -- shows the amount of values we will have in our enum\r\ncreate type orderkey_enum as enum (Select (l_orderkey/4)::VARCHAR from lineitem);\r\ncreate table t2 (c1 orderkey_enum);\r\ninsert into t2 (select (l_orderkey/4)::VARCHAR from lineitem); -- this blows up the size of the database\r\n```\nI had a long go at this today. Unfortunately, I did not succeed in reproducing the issue on my machine.\r\n\r\nI did ensure that nothing was being copied out of the enum type (by making all members private and explicitly deleting the copy constructor/assignment). I've also removed the many scans over the data in constructing the enum type from queries *, which is most likely unrelated to this issue*.\r\n\r\n@Tmonster, maybe you can do a quick build and run of https://github.com/pdet/duckdb/tree/enum_test ? If that does not help, we can sit together tomorrow and have a go at this.\n@pdet the issue isn't as bad now, but the size of the database still increases from 492M to 3.5GB as a result of running \r\n\r\n`insert into t2 (select (l_orderkey/4)::VARCHAR from lineitem); `\r\n\r\nIt seems to me there is still an issue. \nAh, sorry, I understood that the problem was a memory blow-up, I guess I mixed information from the meetings and this issue (:. Indeed, Mytherin analyzed this correctly. \r\n\r\nThe issue with the disk blowup is that the type is reserialized on the catalog and at every column that defined the enum, we should probably serialize just the one in the catalog and properly consume it back from the catalog on a load. Will work on that tomorrow.\nWould the type reserialization result in a 3GB expansion on disk when one enum column is populated? I would expect the size of the DB to increase according to the number of rows inserted, and 750001 rows in a one column table doesn't sound like 3GB, especially since tpch at sf=2 is only 500MB.\r\n\r\n\nFigured out the issue - the `NumericStatistics` class serializes a `Value` as the `min/max`, which includes re-serializing the type. This causes the enum to be re-serialized for every row group.\r\n\r\nThere is still the issue of duplicating the enum for every column that uses it but that does not scale with the size of the data so that is less of an issue.\nI suspect this has to do with the size of the enum and it being serialized in separate places. The `l_comment` enum must be at least 1GB by itself. I think exceeding the memory limit is not that much of a problem - memory managing enums would create a lot of complications. Perhaps we should hard-limit enums to some reasonable value like 10MB of string data?\nI agree with Mytherin that managing the memory of an ENUM can be challenging. \r\n\r\nImplementing a hard limit may prevent users from creating an ENUM, but perhaps we should consider making it a configurable option.\r\n\r\nOne potential solution for making ENUM memory management easier is to use an ART in place of an unordered map. However, this might have nasty consequences for the OG memory footprint. (:\nSo the specific case where this is causing issues for me is a enum type with 1000000 (1 mil) values in a table with 1,000,000,000 (1 billion) rows.\r\n\r\nEach enum has a length of 3 - 8 characters. I'll see if I can reproduce an example using a tpch dataset\nI wonder if we might be serializing types in too many places in general - which wouldn't be a problem for regular types but becomes a problem with larger enums. \n@pdet Here are steps to reproduce where only 750,001 enums are created for a table that will have 11mil rows eventually. I also did not need to change my memory limit (16GB by default).\r\n\r\n```\r\ncall dbgen(sf=2);\r\nselect distinct(l_orderkey/4) from lineitem; -- shows the amount of values we will have in our enum\r\ncreate type orderkey_enum as enum (Select (l_orderkey/4)::VARCHAR from lineitem);\r\ncreate table t2 (c1 orderkey_enum);\r\ninsert into t2 (select (l_orderkey/4)::VARCHAR from lineitem); -- this blows up the size of the database\r\n```\nI had a long go at this today. Unfortunately, I did not succeed in reproducing the issue on my machine.\r\n\r\nI did ensure that nothing was being copied out of the enum type (by making all members private and explicitly deleting the copy constructor/assignment). I've also removed the many scans over the data in constructing the enum type from queries *, which is most likely unrelated to this issue*.\r\n\r\n@Tmonster, maybe you can do a quick build and run of https://github.com/pdet/duckdb/tree/enum_test ? If that does not help, we can sit together tomorrow and have a go at this.\n@pdet the issue isn't as bad now, but the size of the database still increases from 492M to 3.5GB as a result of running \r\n\r\n`insert into t2 (select (l_orderkey/4)::VARCHAR from lineitem); `\r\n\r\nIt seems to me there is still an issue. \nAh, sorry, I understood that the problem was a memory blow-up, I guess I mixed information from the meetings and this issue (:. Indeed, Mytherin analyzed this correctly. \r\n\r\nThe issue with the disk blowup is that the type is reserialized on the catalog and at every column that defined the enum, we should probably serialize just the one in the catalog and properly consume it back from the catalog on a load. Will work on that tomorrow.\nWould the type reserialization result in a 3GB expansion on disk when one enum column is populated? I would expect the size of the DB to increase according to the number of rows inserted, and 750001 rows in a one column table doesn't sound like 3GB, especially since tpch at sf=2 is only 500MB.\r\n\r\n\nFigured out the issue - the `NumericStatistics` class serializes a `Value` as the `min/max`, which includes re-serializing the type. This causes the enum to be re-serialized for every row group.\r\n\r\nThere is still the issue of duplicating the enum for every column that uses it but that does not scale with the size of the data so that is less of an issue.",
  "created_at": "2023-02-10T13:55:07Z"
}