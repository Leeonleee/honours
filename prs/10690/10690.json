{
  "repo": "duckdb/duckdb",
  "pull_number": 10690,
  "instance_id": "duckdb__duckdb-10690",
  "issue_numbers": [
    "9349"
  ],
  "base_commit": "d745e29fd1aeecdbdb48d9539067938903e56436",
  "patch": "diff --git a/src/main/relation/read_csv_relation.cpp b/src/main/relation/read_csv_relation.cpp\nindex a885ae0f6899..1500720e0069 100644\n--- a/src/main/relation/read_csv_relation.cpp\n+++ b/src/main/relation/read_csv_relation.cpp\n@@ -38,7 +38,9 @@ ReadCSVRelation::ReadCSVRelation(const std::shared_ptr<ClientContext> &context,\n \tInitializeAlias(input);\n \n \tauto file_list = CreateValueFromFileList(input);\n-\tauto files = MultiFileReader::GetFileList(*context, file_list, \"CSV\");\n+\n+\tvector<string> files;\n+\tcontext->RunFunctionInTransaction([&]() { files = MultiFileReader::GetFileList(*context, file_list, \"CSV\"); });\n \tD_ASSERT(!files.empty());\n \n \tauto &file_name = files[0];\n@@ -52,14 +54,17 @@ ReadCSVRelation::ReadCSVRelation(const std::shared_ptr<ClientContext> &context,\n \n \t// Run the auto-detect, populating the options with the detected settings\n \n-\tauto buffer_manager = make_shared<CSVBufferManager>(*context, csv_options, files[0], 0);\n-\tCSVSniffer sniffer(csv_options, buffer_manager, CSVStateMachineCache::Get(*context));\n-\tauto sniffer_result = sniffer.SniffCSV();\n-\tauto &types = sniffer_result.return_types;\n-\tauto &names = sniffer_result.names;\n-\tfor (idx_t i = 0; i < types.size(); i++) {\n-\t\tcolumns.emplace_back(names[i], types[i]);\n-\t}\n+\tshared_ptr<CSVBufferManager> buffer_manager;\n+\tcontext->RunFunctionInTransaction([&]() {\n+\t\tbuffer_manager = make_shared<CSVBufferManager>(*context, csv_options, files[0], 0);\n+\t\tCSVSniffer sniffer(csv_options, buffer_manager, CSVStateMachineCache::Get(*context));\n+\t\tauto sniffer_result = sniffer.SniffCSV();\n+\t\tauto &types = sniffer_result.return_types;\n+\t\tauto &names = sniffer_result.names;\n+\t\tfor (idx_t i = 0; i < types.size(); i++) {\n+\t\t\tcolumns.emplace_back(names[i], types[i]);\n+\t\t}\n+\t});\n \n \t// After sniffing we can consider these set, so they are exported as named parameters\n \t// FIXME: This is horribly hacky, should be refactored at some point\ndiff --git a/tools/pythonpkg/README.md b/tools/pythonpkg/README.md\nindex f9a507ac2263..09f65bfeb9f6 100644\n--- a/tools/pythonpkg/README.md\n+++ b/tools/pythonpkg/README.md\n@@ -107,6 +107,28 @@ pytest tests/stubs\n \n All the above should be done in a virtualenv.\n \n+## Frequently encountered issue with extensions:\n+\n+If you are faced with an error on `import duckdb`:\n+```\n+Traceback (most recent call last):\n+  File \"<stdin>\", line 1, in <module>\n+  File \"/usr/bin/python3/site-packages/duckdb/__init__.py\", line 4, in <module>\n+    import duckdb.functional as functional\n+  File \"/usr/bin/python3/site-packages/duckdb/functional/__init__.py\", line 1, in <module>\n+    from duckdb.duckdb.functional import (\n+ImportError: dlopen(/usr/bin/python3/site-packages/duckdb/duckdb.cpython-311-darwin.so, 0x0002): symbol not found in flat namespace '_MD5_Final'\n+```\n+\n+When building DuckDB it outputs which extensions are linked into DuckDB, the python package does not deal with linked in extensions very well.\n+The output looks something like this:\n+`-- Extensions linked into DuckDB: [json, fts, tpcds, tpch, parquet, icu, httpfs]`\n+\n+`httpfs` should not be in that list, among others.\n+Refer to `extension/extension_config_local.cmake` or the other `*.cmake` files and make sure you add DONT_LINK to the problematic extension.\n+`tools/pythonpkg/duckdb_extension_config.cmake` contains the default list of extensions built for the python package\n+Anything that is linked that is not listed there should be considered problematic.\n+\n ## Clang-tidy and CMakeLists\n \n The pythonpkg does not use the CMakeLists for compilation, for that it uses pip and `package_build.py` mostly.\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/extensions/test_httpfs.py b/tools/pythonpkg/tests/extensions/test_httpfs.py\nindex 2ca4bb8847dd..a00d6b5a22dd 100644\n--- a/tools/pythonpkg/tests/extensions/test_httpfs.py\n+++ b/tools/pythonpkg/tests/extensions/test_httpfs.py\n@@ -3,6 +3,7 @@\n from pytest import raises, mark\n import pytest\n from conftest import NumpyPandas, ArrowPandas\n+import datetime\n \n # We only run this test if this env var is set\n pytestmark = mark.skipif(\n@@ -13,9 +14,21 @@\n class TestHTTPFS(object):\n     def test_read_json_httpfs(self, require):\n         connection = require('httpfs')\n-        # FIXME: add test back\n-        # res = connection.read_json('https://jsonplaceholder.typicode.com/todos')\n-        # assert len(res.types) == 4\n+        try:\n+            res = connection.read_json('https://jsonplaceholder.typicode.com/todos')\n+            assert len(res.types) == 4\n+        except duckdb.Error as e:\n+            if '403' in e:\n+                pytest.skip(reason=\"Test is flaky, sometimes returns 403\")\n+            else:\n+                pytest.fail(str(e))\n+\n+    def test_s3fs(self, require):\n+        connection = require('httpfs')\n+\n+        rel = connection.read_csv(f\"s3://duckdb-blobs/data/Star_Trek-Season_1.csv\", header=True)\n+        res = rel.fetchone()\n+        assert res == (1, 0, datetime.date(1965, 2, 28), 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 6, 0, 0, 0, 0)\n \n     @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n     def test_httpfs(self, require, pandas):\n",
  "problem_statement": "[Python] Spark API requires loading httpfs extension manually\n### What happens?\r\n\r\nWhen using the Spark API with an S3 URI, a RuntimeError exception is raised unless the httpfs extension is manually loaded.\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/private/tmp/emr-serverless-samples/examples/pyspark/repro.py\", line 13, in <module>\r\n    df = spark.read.csv(f\"s3://noaa-gsod-pds/2023/01001099999.csv\", header=True)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/private/tmp/emr-serverless-samples/.venv/lib/python3.11/site-packages/duckdb/experimental/spark/sql/readwriter.py\", line 164, in csv\r\n    rel = self.session.conn.read_csv(\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: INTERNAL Error: State was not defined in this HTTP File Handle\r\n```\r\n\r\n### To Reproduce\r\n\r\nYou must have AWS credentials configured already. For my test, I exported them as environment credentials using `$(aws configure export-credentials --format env)`\r\n\r\n```python\r\nfrom duckdb.experimental.spark.sql import SparkSession\r\nfrom duckdb.experimental.spark.sql import functions as F\r\n\r\nspark = SparkSession.builder.appName(\"Weather\").getOrCreate()\r\n\r\n\r\ndef load_ext():\r\n    spark.conn.execute(\r\n        f\"\"\"\r\n        INSTALL httpfs;\r\n        LOAD httpfs;\r\n    \"\"\"\r\n    )\r\n\r\n\r\n# Without httpfs installed, the spark.read fails with the \"State was not defined\" error.\r\n# With httpfs installed, the query succeeds.\r\ntry:\r\n    df = spark.read.csv(f\"s3://noaa-gsod-pds/2023/01001099999.csv\", header=True)\r\n    print(\"Load 1\", df.count())\r\nexcept Exception as e:\r\n    print(\"Load 1 failed: \", e)\r\n    load_ext()\r\n    df = spark.read.csv(f\"s3://noaa-gsod-pds/2023/01001099999.csv\", header=True)\r\n    print(\"Load 2\", df.count())\r\n```\r\n\r\n### OS:\r\n\r\nmacOS Ventura 13.5.2 osx-arm64\r\n\r\n### DuckDB Version:\r\n\r\nv0.9.1\r\n\r\n### DuckDB Client:\r\n\r\nPython\r\n\r\n### Full Name:\r\n\r\nDamon Cortesi\r\n\r\n### Affiliation:\r\n\r\nAWS\r\n\r\n### Have you tried this on the latest `main` branch?\r\n\r\nI have tested with a main build\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] Yes, I have\n",
  "hints_text": "Thanks for raising this, this is some problem at the intersection between autoloading of the httpfs extension and some Python specific logic that we are aware of.\r\n\r\nWorkaround is for now explicitly loading the httpfs extension. \nThis issue is stale because it has been open 90 days with no activity. Remove stale label or comment or this will be closed in 30 days.\nThis issue was closed because it has been stale for 30 days with no activity.",
  "created_at": "2024-02-15T12:34:28Z"
}