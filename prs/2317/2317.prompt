You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Set default schema that is used at runtime
e.g.
```sql
CREATE SCHEMA s1;
SET default_schema TO s1;
```

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16: </p>
17: 
18: ## DuckDB
19: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/docs/why_duckdb.html).
20: 
21: ## Installation
22: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
23: 
24: ## Data Import
25: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
26: 
27: ```sql
28: SELECT * FROM 'myfile.csv';
29: SELECT * FROM 'myfile.parquet';
30: ```
31: 
32: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
33: 
34: ## SQL Reference
35: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
36: 
37: ## Development
38: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
39: 
40: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
41: 
42: 
[end of README.md]
[start of src/catalog/CMakeLists.txt]
1: add_subdirectory(catalog_entry)
2: add_subdirectory(default)
3: 
4: add_library_unity(duckdb_catalog OBJECT catalog_entry.cpp catalog.cpp
5:                   catalog_set.cpp dependency_manager.cpp)
6: set(ALL_OBJECT_FILES
7:     ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:duckdb_catalog>
8:     PARENT_SCOPE)
[end of src/catalog/CMakeLists.txt]
[start of src/catalog/catalog.cpp]
1: #include "duckdb/catalog/catalog.hpp"
2: #include "duckdb/catalog/catalog_set.hpp"
3: 
4: #include "duckdb/catalog/catalog_entry/list.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/main/client_context.hpp"
7: #include "duckdb/parser/expression/function_expression.hpp"
8: #include "duckdb/parser/parsed_data/alter_table_info.hpp"
9: #include "duckdb/parser/parsed_data/create_index_info.hpp"
10: #include "duckdb/parser/parsed_data/create_aggregate_function_info.hpp"
11: #include "duckdb/parser/parsed_data/create_collation_info.hpp"
12: #include "duckdb/parser/parsed_data/create_scalar_function_info.hpp"
13: #include "duckdb/parser/parsed_data/create_schema_info.hpp"
14: #include "duckdb/parser/parsed_data/create_sequence_info.hpp"
15: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
16: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
17: #include "duckdb/parser/parsed_data/create_pragma_function_info.hpp"
18: #include "duckdb/parser/parsed_data/create_view_info.hpp"
19: #include "duckdb/parser/parsed_data/drop_info.hpp"
20: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
21: #include "duckdb/main/database.hpp"
22: #include "duckdb/catalog/dependency_manager.hpp"
23: 
24: #include "duckdb/catalog/default/default_schemas.hpp"
25: 
26: namespace duckdb {
27: 
28: Catalog::Catalog(DatabaseInstance &db)
29:     : db(db), schemas(make_unique<CatalogSet>(*this, make_unique<DefaultSchemaGenerator>(*this))),
30:       dependency_manager(make_unique<DependencyManager>(*this)) {
31: 	catalog_version = 0;
32: }
33: Catalog::~Catalog() {
34: }
35: 
36: Catalog &Catalog::GetCatalog(ClientContext &context) {
37: 	return context.db->GetCatalog();
38: }
39: 
40: CatalogEntry *Catalog::CreateTable(ClientContext &context, BoundCreateTableInfo *info) {
41: 	auto schema = GetSchema(context, info->base->schema);
42: 	return CreateTable(context, schema, info);
43: }
44: 
45: CatalogEntry *Catalog::CreateTable(ClientContext &context, SchemaCatalogEntry *schema, BoundCreateTableInfo *info) {
46: 	return schema->CreateTable(context, info);
47: }
48: 
49: CatalogEntry *Catalog::CreateView(ClientContext &context, CreateViewInfo *info) {
50: 	auto schema = GetSchema(context, info->schema);
51: 	return CreateView(context, schema, info);
52: }
53: 
54: CatalogEntry *Catalog::CreateView(ClientContext &context, SchemaCatalogEntry *schema, CreateViewInfo *info) {
55: 	return schema->CreateView(context, info);
56: }
57: 
58: CatalogEntry *Catalog::CreateSequence(ClientContext &context, CreateSequenceInfo *info) {
59: 	auto schema = GetSchema(context, info->schema);
60: 	return CreateSequence(context, schema, info);
61: }
62: 
63: CatalogEntry *Catalog::CreateSequence(ClientContext &context, SchemaCatalogEntry *schema, CreateSequenceInfo *info) {
64: 	return schema->CreateSequence(context, info);
65: }
66: 
67: CatalogEntry *Catalog::CreateTableFunction(ClientContext &context, CreateTableFunctionInfo *info) {
68: 	auto schema = GetSchema(context, info->schema);
69: 	return CreateTableFunction(context, schema, info);
70: }
71: 
72: CatalogEntry *Catalog::CreateTableFunction(ClientContext &context, SchemaCatalogEntry *schema,
73:                                            CreateTableFunctionInfo *info) {
74: 	return schema->CreateTableFunction(context, info);
75: }
76: 
77: CatalogEntry *Catalog::CreateCopyFunction(ClientContext &context, CreateCopyFunctionInfo *info) {
78: 	auto schema = GetSchema(context, info->schema);
79: 	return CreateCopyFunction(context, schema, info);
80: }
81: 
82: CatalogEntry *Catalog::CreateCopyFunction(ClientContext &context, SchemaCatalogEntry *schema,
83:                                           CreateCopyFunctionInfo *info) {
84: 	return schema->CreateCopyFunction(context, info);
85: }
86: 
87: CatalogEntry *Catalog::CreatePragmaFunction(ClientContext &context, CreatePragmaFunctionInfo *info) {
88: 	auto schema = GetSchema(context, info->schema);
89: 	return CreatePragmaFunction(context, schema, info);
90: }
91: 
92: CatalogEntry *Catalog::CreatePragmaFunction(ClientContext &context, SchemaCatalogEntry *schema,
93:                                             CreatePragmaFunctionInfo *info) {
94: 	return schema->CreatePragmaFunction(context, info);
95: }
96: 
97: CatalogEntry *Catalog::CreateFunction(ClientContext &context, CreateFunctionInfo *info) {
98: 	auto schema = GetSchema(context, info->schema);
99: 	return CreateFunction(context, schema, info);
100: }
101: 
102: CatalogEntry *Catalog::CreateFunction(ClientContext &context, SchemaCatalogEntry *schema, CreateFunctionInfo *info) {
103: 	return schema->CreateFunction(context, info);
104: }
105: 
106: CatalogEntry *Catalog::CreateCollation(ClientContext &context, CreateCollationInfo *info) {
107: 	auto schema = GetSchema(context, info->schema);
108: 	return CreateCollation(context, schema, info);
109: }
110: 
111: CatalogEntry *Catalog::CreateCollation(ClientContext &context, SchemaCatalogEntry *schema, CreateCollationInfo *info) {
112: 	return schema->CreateCollation(context, info);
113: }
114: 
115: CatalogEntry *Catalog::CreateSchema(ClientContext &context, CreateSchemaInfo *info) {
116: 	D_ASSERT(!info->schema.empty());
117: 	if (info->schema == TEMP_SCHEMA) {
118: 		throw CatalogException("Cannot create built-in schema \"%s\"", info->schema);
119: 	}
120: 
121: 	unordered_set<CatalogEntry *> dependencies;
122: 	auto entry = make_unique<SchemaCatalogEntry>(this, info->schema, info->internal);
123: 	auto result = entry.get();
124: 	if (!schemas->CreateEntry(context, info->schema, move(entry), dependencies)) {
125: 		if (info->on_conflict == OnCreateConflict::ERROR_ON_CONFLICT) {
126: 			throw CatalogException("Schema with name %s already exists!", info->schema);
127: 		} else {
128: 			D_ASSERT(info->on_conflict == OnCreateConflict::IGNORE_ON_CONFLICT);
129: 		}
130: 		return nullptr;
131: 	}
132: 	return result;
133: }
134: 
135: void Catalog::DropSchema(ClientContext &context, DropInfo *info) {
136: 	D_ASSERT(!info->name.empty());
137: 	ModifyCatalog();
138: 	if (!schemas->DropEntry(context, info->name, info->cascade)) {
139: 		if (!info->if_exists) {
140: 			throw CatalogException("Schema with name \"%s\" does not exist!", info->name);
141: 		}
142: 	}
143: }
144: 
145: void Catalog::DropEntry(ClientContext &context, DropInfo *info) {
146: 	ModifyCatalog();
147: 	if (info->type == CatalogType::SCHEMA_ENTRY) {
148: 		// DROP SCHEMA
149: 		DropSchema(context, info);
150: 	} else {
151: 		if (info->schema.empty()) {
152: 			info->schema = DEFAULT_SCHEMA;
153: 		}
154: 		auto schema = GetSchema(context, info->schema);
155: 		schema->DropEntry(context, info);
156: 	}
157: }
158: 
159: SchemaCatalogEntry *Catalog::GetSchema(ClientContext &context, const string &schema_name,
160:                                        QueryErrorContext error_context) {
161: 	D_ASSERT(!schema_name.empty());
162: 	if (schema_name == TEMP_SCHEMA) {
163: 		return context.temporary_objects.get();
164: 	}
165: 	auto entry = schemas->GetEntry(context, schema_name);
166: 	if (!entry) {
167: 		throw CatalogException(error_context.FormatError("Schema with name %s does not exist!", schema_name));
168: 	}
169: 	return (SchemaCatalogEntry *)entry;
170: }
171: 
172: void Catalog::ScanSchemas(ClientContext &context, std::function<void(CatalogEntry *)> callback) {
173: 	// create all default schemas first
174: 	schemas->Scan(context, [&](CatalogEntry *entry) { callback(entry); });
175: }
176: 
177: CatalogEntry *Catalog::GetEntry(ClientContext &context, CatalogType type, string schema_name, const string &name,
178:                                 bool if_exists, QueryErrorContext error_context) {
179: 	if (schema_name.empty()) {
180: 		// no schema provided: check the catalog search path in order
181: 		if (context.catalog_search_path.empty()) {
182: 			throw InternalException("Empty catalog search path");
183: 		}
184: 		schema_name = DEFAULT_SCHEMA;
185: 		for (idx_t i = 0; i < context.catalog_search_path.size(); i++) {
186: 			auto entry = GetEntry(context, type, context.catalog_search_path[i], name, true);
187: 			if (entry) {
188: 				return entry;
189: 			}
190: 		}
191: 	}
192: 	auto schema = GetSchema(context, schema_name, error_context);
193: 	return schema->GetEntry(context, type, name, if_exists, error_context);
194: }
195: 
196: template <>
197: TableCatalogEntry *Catalog::GetEntry(ClientContext &context, string schema_name, const string &name, bool if_exists,
198:                                      QueryErrorContext error_context) {
199: 	auto entry = GetEntry(context, CatalogType::TABLE_ENTRY, move(schema_name), name, if_exists);
200: 	if (!entry) {
201: 		return nullptr;
202: 	}
203: 	if (entry->type != CatalogType::TABLE_ENTRY) {
204: 		throw CatalogException(error_context.FormatError("%s is not a table", name));
205: 	}
206: 	return (TableCatalogEntry *)entry;
207: }
208: 
209: template <>
210: SequenceCatalogEntry *Catalog::GetEntry(ClientContext &context, string schema_name, const string &name, bool if_exists,
211:                                         QueryErrorContext error_context) {
212: 	return (SequenceCatalogEntry *)GetEntry(context, CatalogType::SEQUENCE_ENTRY, move(schema_name), name, if_exists,
213: 	                                        error_context);
214: }
215: 
216: template <>
217: TableFunctionCatalogEntry *Catalog::GetEntry(ClientContext &context, string schema_name, const string &name,
218:                                              bool if_exists, QueryErrorContext error_context) {
219: 	return (TableFunctionCatalogEntry *)GetEntry(context, CatalogType::TABLE_FUNCTION_ENTRY, move(schema_name), name,
220: 	                                             if_exists, error_context);
221: }
222: 
223: template <>
224: CopyFunctionCatalogEntry *Catalog::GetEntry(ClientContext &context, string schema_name, const string &name,
225:                                             bool if_exists, QueryErrorContext error_context) {
226: 	return (CopyFunctionCatalogEntry *)GetEntry(context, CatalogType::COPY_FUNCTION_ENTRY, move(schema_name), name,
227: 	                                            if_exists, error_context);
228: }
229: 
230: template <>
231: PragmaFunctionCatalogEntry *Catalog::GetEntry(ClientContext &context, string schema_name, const string &name,
232:                                               bool if_exists, QueryErrorContext error_context) {
233: 	return (PragmaFunctionCatalogEntry *)GetEntry(context, CatalogType::PRAGMA_FUNCTION_ENTRY, move(schema_name), name,
234: 	                                              if_exists, error_context);
235: }
236: 
237: template <>
238: AggregateFunctionCatalogEntry *Catalog::GetEntry(ClientContext &context, string schema_name, const string &name,
239:                                                  bool if_exists, QueryErrorContext error_context) {
240: 	auto entry =
241: 	    GetEntry(context, CatalogType::AGGREGATE_FUNCTION_ENTRY, move(schema_name), name, if_exists, error_context);
242: 	if (entry->type != CatalogType::AGGREGATE_FUNCTION_ENTRY) {
243: 		throw CatalogException(error_context.FormatError("%s is not an aggregate function", name));
244: 	}
245: 	return (AggregateFunctionCatalogEntry *)entry;
246: }
247: 
248: template <>
249: CollateCatalogEntry *Catalog::GetEntry(ClientContext &context, string schema_name, const string &name, bool if_exists,
250:                                        QueryErrorContext error_context) {
251: 	return (CollateCatalogEntry *)GetEntry(context, CatalogType::COLLATION_ENTRY, move(schema_name), name, if_exists,
252: 	                                       error_context);
253: }
254: 
255: void Catalog::Alter(ClientContext &context, AlterInfo *info) {
256: 	ModifyCatalog();
257: 	if (info->schema.empty()) {
258: 		auto catalog_type = info->GetCatalogType();
259: 		// invalid schema: search the catalog search path
260: 		info->schema = DEFAULT_SCHEMA;
261: 		for (idx_t i = 0; i < context.catalog_search_path.size(); i++) {
262: 			auto entry = GetEntry(context, catalog_type, context.catalog_search_path[i], info->name, true);
263: 			if (entry) {
264: 				// entry exists in this schema: alter there
265: 				info->schema = context.catalog_search_path[i];
266: 				break;
267: 			}
268: 		}
269: 	}
270: 	auto schema = GetSchema(context, info->schema);
271: 	return schema->Alter(context, info);
272: }
273: 
274: idx_t Catalog::GetCatalogVersion() {
275: 	return catalog_version;
276: }
277: 
278: idx_t Catalog::ModifyCatalog() {
279: 	return catalog_version++;
280: }
281: 
282: } // namespace duckdb
[end of src/catalog/catalog.cpp]
[start of src/catalog/catalog_entry/schema_catalog_entry.cpp]
1: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/aggregate_function_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/collate_catalog_entry.hpp"
6: #include "duckdb/catalog/catalog_entry/copy_function_catalog_entry.hpp"
7: #include "duckdb/catalog/catalog_entry/index_catalog_entry.hpp"
8: #include "duckdb/catalog/catalog_entry/macro_catalog_entry.hpp"
9: #include "duckdb/catalog/catalog_entry/pragma_function_catalog_entry.hpp"
10: #include "duckdb/catalog/catalog_entry/scalar_function_catalog_entry.hpp"
11: #include "duckdb/catalog/catalog_entry/sequence_catalog_entry.hpp"
12: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
13: #include "duckdb/catalog/catalog_entry/table_function_catalog_entry.hpp"
14: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
15: #include "duckdb/catalog/default/default_functions.hpp"
16: #include "duckdb/catalog/default/default_views.hpp"
17: #include "duckdb/common/exception.hpp"
18: #include "duckdb/parser/parsed_data/alter_table_info.hpp"
19: #include "duckdb/parser/parsed_data/create_collation_info.hpp"
20: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
21: #include "duckdb/parser/parsed_data/create_index_info.hpp"
22: #include "duckdb/parser/parsed_data/create_pragma_function_info.hpp"
23: #include "duckdb/parser/parsed_data/create_scalar_function_info.hpp"
24: #include "duckdb/parser/parsed_data/create_schema_info.hpp"
25: #include "duckdb/parser/parsed_data/create_sequence_info.hpp"
26: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
27: #include "duckdb/parser/parsed_data/create_view_info.hpp"
28: #include "duckdb/parser/parsed_data/drop_info.hpp"
29: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
30: #include "duckdb/transaction/transaction.hpp"
31: #include "duckdb/storage/data_table.hpp"
32: #include <algorithm>
33: #include <sstream>
34: 
35: namespace duckdb {
36: 
37: SchemaCatalogEntry::SchemaCatalogEntry(Catalog *catalog, string name_p, bool internal)
38:     : CatalogEntry(CatalogType::SCHEMA_ENTRY, catalog, move(name_p)),
39:       tables(*catalog, make_unique<DefaultViewGenerator>(*catalog, this)), indexes(*catalog), table_functions(*catalog),
40:       copy_functions(*catalog), pragma_functions(*catalog),
41:       functions(*catalog, make_unique<DefaultFunctionGenerator>(*catalog, this)), sequences(*catalog),
42:       collations(*catalog) {
43: 	this->internal = internal;
44: }
45: 
46: CatalogEntry *SchemaCatalogEntry::AddEntry(ClientContext &context, unique_ptr<StandardEntry> entry,
47:                                            OnCreateConflict on_conflict, unordered_set<CatalogEntry *> dependencies) {
48: 	auto entry_name = entry->name;
49: 	auto entry_type = entry->type;
50: 	auto result = entry.get();
51: 
52: 	// first find the set for this entry
53: 	auto &set = GetCatalogSet(entry_type);
54: 
55: 	if (name != TEMP_SCHEMA) {
56: 		dependencies.insert(this);
57: 	} else {
58: 		entry->temporary = true;
59: 	}
60: 	if (on_conflict == OnCreateConflict::REPLACE_ON_CONFLICT) {
61: 		// CREATE OR REPLACE: first try to drop the entry
62: 		auto old_entry = set.GetEntry(context, entry_name);
63: 		if (old_entry) {
64: 			if (old_entry->type != entry_type) {
65: 				throw CatalogException("Existing object %s is of type %s, trying to replace with type %s", entry_name,
66: 				                       CatalogTypeToString(old_entry->type), CatalogTypeToString(entry_type));
67: 			}
68: 			(void)set.DropEntry(context, entry_name, false);
69: 		}
70: 	}
71: 	// now try to add the entry
72: 	if (!set.CreateEntry(context, entry_name, move(entry), dependencies)) {
73: 		// entry already exists!
74: 		if (on_conflict == OnCreateConflict::ERROR_ON_CONFLICT) {
75: 			throw CatalogException("%s with name \"%s\" already exists!", CatalogTypeToString(entry_type), entry_name);
76: 		} else {
77: 			return nullptr;
78: 		}
79: 	}
80: 	return result;
81: }
82: 
83: CatalogEntry *SchemaCatalogEntry::AddEntry(ClientContext &context, unique_ptr<StandardEntry> entry,
84:                                            OnCreateConflict on_conflict) {
85: 	unordered_set<CatalogEntry *> dependencies;
86: 	return AddEntry(context, move(entry), on_conflict, dependencies);
87: }
88: 
89: CatalogEntry *SchemaCatalogEntry::CreateSequence(ClientContext &context, CreateSequenceInfo *info) {
90: 	auto sequence = make_unique<SequenceCatalogEntry>(catalog, this, info);
91: 	return AddEntry(context, move(sequence), info->on_conflict);
92: }
93: 
94: CatalogEntry *SchemaCatalogEntry::CreateTable(ClientContext &context, BoundCreateTableInfo *info) {
95: 	auto table = make_unique<TableCatalogEntry>(catalog, this, info);
96: 	table->storage->info->cardinality = table->storage->GetTotalRows();
97: 	return AddEntry(context, move(table), info->Base().on_conflict, info->dependencies);
98: }
99: 
100: CatalogEntry *SchemaCatalogEntry::CreateView(ClientContext &context, CreateViewInfo *info) {
101: 	auto view = make_unique<ViewCatalogEntry>(catalog, this, info);
102: 	return AddEntry(context, move(view), info->on_conflict);
103: }
104: 
105: CatalogEntry *SchemaCatalogEntry::CreateIndex(ClientContext &context, CreateIndexInfo *info, TableCatalogEntry *table) {
106: 	unordered_set<CatalogEntry *> dependencies;
107: 	dependencies.insert(table);
108: 	auto index = make_unique<IndexCatalogEntry>(catalog, this, info);
109: 	return AddEntry(context, move(index), info->on_conflict, dependencies);
110: }
111: 
112: CatalogEntry *SchemaCatalogEntry::CreateCollation(ClientContext &context, CreateCollationInfo *info) {
113: 	auto collation = make_unique<CollateCatalogEntry>(catalog, this, info);
114: 	return AddEntry(context, move(collation), info->on_conflict);
115: }
116: 
117: CatalogEntry *SchemaCatalogEntry::CreateTableFunction(ClientContext &context, CreateTableFunctionInfo *info) {
118: 	auto table_function = make_unique<TableFunctionCatalogEntry>(catalog, this, info);
119: 	return AddEntry(context, move(table_function), info->on_conflict);
120: }
121: 
122: CatalogEntry *SchemaCatalogEntry::CreateCopyFunction(ClientContext &context, CreateCopyFunctionInfo *info) {
123: 	auto copy_function = make_unique<CopyFunctionCatalogEntry>(catalog, this, info);
124: 	return AddEntry(context, move(copy_function), info->on_conflict);
125: }
126: 
127: CatalogEntry *SchemaCatalogEntry::CreatePragmaFunction(ClientContext &context, CreatePragmaFunctionInfo *info) {
128: 	auto pragma_function = make_unique<PragmaFunctionCatalogEntry>(catalog, this, info);
129: 	return AddEntry(context, move(pragma_function), info->on_conflict);
130: }
131: 
132: CatalogEntry *SchemaCatalogEntry::CreateFunction(ClientContext &context, CreateFunctionInfo *info) {
133: 	unique_ptr<StandardEntry> function;
134: 	switch (info->type) {
135: 	case CatalogType::SCALAR_FUNCTION_ENTRY:
136: 		function = make_unique_base<StandardEntry, ScalarFunctionCatalogEntry>(catalog, this,
137: 		                                                                       (CreateScalarFunctionInfo *)info);
138: 		break;
139: 	case CatalogType::MACRO_ENTRY:
140: 		// create a macro function
141: 		function = make_unique_base<StandardEntry, MacroCatalogEntry>(catalog, this, (CreateMacroInfo *)info);
142: 		break;
143: 	case CatalogType::AGGREGATE_FUNCTION_ENTRY:
144: 		D_ASSERT(info->type == CatalogType::AGGREGATE_FUNCTION_ENTRY);
145: 		// create an aggregate function
146: 		function = make_unique_base<StandardEntry, AggregateFunctionCatalogEntry>(catalog, this,
147: 		                                                                          (CreateAggregateFunctionInfo *)info);
148: 		break;
149: 	default:
150: 		throw InternalException("Unknown function type \"%s\"", CatalogTypeToString(info->type));
151: 	}
152: 	return AddEntry(context, move(function), info->on_conflict);
153: }
154: 
155: void SchemaCatalogEntry::DropEntry(ClientContext &context, DropInfo *info) {
156: 	auto &set = GetCatalogSet(info->type);
157: 
158: 	// first find the entry
159: 	auto existing_entry = set.GetEntry(context, info->name);
160: 	if (!existing_entry) {
161: 		if (!info->if_exists) {
162: 			throw CatalogException("%s with name \"%s\" does not exist!", CatalogTypeToString(info->type), info->name);
163: 		}
164: 		return;
165: 	}
166: 	if (existing_entry->type != info->type) {
167: 		throw CatalogException("Existing object %s is of type %s, trying to replace with type %s", info->name,
168: 		                       CatalogTypeToString(existing_entry->type), CatalogTypeToString(info->type));
169: 	}
170: 	if (!set.DropEntry(context, info->name, info->cascade)) {
171: 		throw InternalException("Could not drop element because of an internal error");
172: 	}
173: }
174: 
175: void SchemaCatalogEntry::Alter(ClientContext &context, AlterInfo *info) {
176: 	CatalogType type = info->GetCatalogType();
177: 	string name = info->name;
178: 	auto &set = GetCatalogSet(type);
179: 	if (!set.AlterEntry(context, name, info)) {
180: 		throw CatalogException("Entry with name \"%s\" does not exist!", name);
181: 	}
182: }
183: 
184: CatalogEntry *SchemaCatalogEntry::GetEntry(ClientContext &context, CatalogType type, const string &entry_name,
185:                                            bool if_exists, QueryErrorContext error_context) {
186: 	auto &set = GetCatalogSet(type);
187: 
188: 	auto entry = set.GetEntry(context, entry_name);
189: 	if (!entry) {
190: 		if (!if_exists) {
191: 			auto entry = set.SimilarEntry(context, entry_name);
192: 			string did_you_mean;
193: 			if (!entry.empty()) {
194: 				did_you_mean = "\nDid you mean \"" + entry + "\"?";
195: 			}
196: 			throw CatalogException(error_context.FormatError("%s with name %s does not exist!%s",
197: 			                                                 CatalogTypeToString(type), entry_name, did_you_mean));
198: 		}
199: 		return nullptr;
200: 	}
201: 	return entry;
202: }
203: 
204: void SchemaCatalogEntry::Scan(ClientContext &context, CatalogType type,
205:                               const std::function<void(CatalogEntry *)> &callback) {
206: 	auto &set = GetCatalogSet(type);
207: 	set.Scan(context, callback);
208: }
209: 
210: void SchemaCatalogEntry::Scan(CatalogType type, const std::function<void(CatalogEntry *)> &callback) {
211: 	auto &set = GetCatalogSet(type);
212: 	set.Scan(callback);
213: }
214: 
215: void SchemaCatalogEntry::Serialize(Serializer &serializer) {
216: 	serializer.WriteString(name);
217: }
218: 
219: unique_ptr<CreateSchemaInfo> SchemaCatalogEntry::Deserialize(Deserializer &source) {
220: 	auto info = make_unique<CreateSchemaInfo>();
221: 	info->schema = source.Read<string>();
222: 	return info;
223: }
224: 
225: string SchemaCatalogEntry::ToSQL() {
226: 	std::stringstream ss;
227: 	ss << "CREATE SCHEMA " << name << ";";
228: 	return ss.str();
229: }
230: 
231: CatalogSet &SchemaCatalogEntry::GetCatalogSet(CatalogType type) {
232: 	switch (type) {
233: 	case CatalogType::VIEW_ENTRY:
234: 	case CatalogType::TABLE_ENTRY:
235: 		return tables;
236: 	case CatalogType::INDEX_ENTRY:
237: 		return indexes;
238: 	case CatalogType::TABLE_FUNCTION_ENTRY:
239: 		return table_functions;
240: 	case CatalogType::COPY_FUNCTION_ENTRY:
241: 		return copy_functions;
242: 	case CatalogType::PRAGMA_FUNCTION_ENTRY:
243: 		return pragma_functions;
244: 	case CatalogType::AGGREGATE_FUNCTION_ENTRY:
245: 	case CatalogType::SCALAR_FUNCTION_ENTRY:
246: 	case CatalogType::MACRO_ENTRY:
247: 		return functions;
248: 	case CatalogType::SEQUENCE_ENTRY:
249: 		return sequences;
250: 	case CatalogType::COLLATION_ENTRY:
251: 		return collations;
252: 	default:
253: 		throw InternalException("Unsupported catalog type in schema");
254: 	}
255: }
256: 
257: } // namespace duckdb
[end of src/catalog/catalog_entry/schema_catalog_entry.cpp]
[start of src/catalog/catalog_set.cpp]
1: #include "duckdb/catalog/catalog_set.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/common/exception.hpp"
5: #include "duckdb/transaction/transaction_manager.hpp"
6: #include "duckdb/transaction/transaction.hpp"
7: #include "duckdb/common/serializer/buffered_serializer.hpp"
8: #include "duckdb/parser/parsed_data/alter_table_info.hpp"
9: #include "duckdb/catalog/dependency_manager.hpp"
10: #include "duckdb/common/string_util.hpp"
11: 
12: namespace duckdb {
13: 
14: CatalogSet::CatalogSet(Catalog &catalog, unique_ptr<DefaultGenerator> defaults)
15:     : catalog(catalog), defaults(move(defaults)) {
16: }
17: 
18: bool CatalogSet::CreateEntry(ClientContext &context, const string &name, unique_ptr<CatalogEntry> value,
19:                              unordered_set<CatalogEntry *> &dependencies) {
20: 	auto &transaction = Transaction::GetTransaction(context);
21: 	// lock the catalog for writing
22: 	lock_guard<mutex> write_lock(catalog.write_lock);
23: 	// lock this catalog set to disallow reading
24: 	lock_guard<mutex> read_lock(catalog_lock);
25: 
26: 	// first check if the entry exists in the unordered set
27: 	idx_t entry_index;
28: 	auto mapping_value = GetMapping(context, name);
29: 	if (mapping_value == nullptr || mapping_value->deleted) {
30: 		// if it does not: entry has never been created
31: 
32: 		// first create a dummy deleted entry for this entry
33: 		// so transactions started before the commit of this transaction don't
34: 		// see it yet
35: 		entry_index = current_entry++;
36: 		auto dummy_node = make_unique<CatalogEntry>(CatalogType::INVALID, value->catalog, name);
37: 		dummy_node->timestamp = 0;
38: 		dummy_node->deleted = true;
39: 		dummy_node->set = this;
40: 
41: 		entries[entry_index] = move(dummy_node);
42: 		PutMapping(context, name, entry_index);
43: 	} else {
44: 		entry_index = mapping_value->index;
45: 		auto &current = *entries[entry_index];
46: 		// if it does, we have to check version numbers
47: 		if (HasConflict(context, current.timestamp)) {
48: 			// current version has been written to by a currently active
49: 			// transaction
50: 			throw TransactionException("Catalog write-write conflict on create with \"%s\"", current.name);
51: 		}
52: 		// there is a current version that has been committed
53: 		// if it has not been deleted there is a conflict
54: 		if (!current.deleted) {
55: 			return false;
56: 		}
57: 	}
58: 	// create a new entry and replace the currently stored one
59: 	// set the timestamp to the timestamp of the current transaction
60: 	// and point it at the dummy node
61: 	value->timestamp = transaction.transaction_id;
62: 	value->set = this;
63: 
64: 	// now add the dependency set of this object to the dependency manager
65: 	catalog.dependency_manager->AddObject(context, value.get(), dependencies);
66: 
67: 	value->child = move(entries[entry_index]);
68: 	value->child->parent = value.get();
69: 	// push the old entry in the undo buffer for this transaction
70: 	transaction.PushCatalogEntry(value->child.get());
71: 	entries[entry_index] = move(value);
72: 	return true;
73: }
74: 
75: bool CatalogSet::GetEntryInternal(ClientContext &context, idx_t entry_index, CatalogEntry *&catalog_entry) {
76: 	catalog_entry = entries[entry_index].get();
77: 	// if it does: we have to retrieve the entry and to check version numbers
78: 	if (HasConflict(context, catalog_entry->timestamp)) {
79: 		// current version has been written to by a currently active
80: 		// transaction
81: 		throw TransactionException("Catalog write-write conflict on alter with \"%s\"", catalog_entry->name);
82: 	}
83: 	// there is a current version that has been committed by this transaction
84: 	if (catalog_entry->deleted) {
85: 		// if the entry was already deleted, it now does not exist anymore
86: 		// so we return that we could not find it
87: 		return false;
88: 	}
89: 	return true;
90: }
91: 
92: bool CatalogSet::GetEntryInternal(ClientContext &context, const string &name, idx_t &entry_index,
93:                                   CatalogEntry *&catalog_entry) {
94: 	auto mapping_value = GetMapping(context, name);
95: 	if (mapping_value == nullptr || mapping_value->deleted) {
96: 		// the entry does not exist, check if we can create a default entry
97: 		return false;
98: 	}
99: 	entry_index = mapping_value->index;
100: 	return GetEntryInternal(context, entry_index, catalog_entry);
101: }
102: 
103: bool CatalogSet::AlterEntry(ClientContext &context, const string &name, AlterInfo *alter_info) {
104: 	auto &transaction = Transaction::GetTransaction(context);
105: 	// lock the catalog for writing
106: 	lock_guard<mutex> write_lock(catalog.write_lock);
107: 
108: 	// first check if the entry exists in the unordered set
109: 	idx_t entry_index;
110: 	CatalogEntry *entry;
111: 	if (!GetEntryInternal(context, name, entry_index, entry)) {
112: 		return false;
113: 	}
114: 	if (entry->internal) {
115: 		throw CatalogException("Cannot alter entry \"%s\" because it is an internal system entry", entry->name);
116: 	}
117: 
118: 	// lock this catalog set to disallow reading
119: 	lock_guard<mutex> read_lock(catalog_lock);
120: 
121: 	// create a new entry and replace the currently stored one
122: 	// set the timestamp to the timestamp of the current transaction
123: 	// and point it to the updated table node
124: 	string original_name = entry->name;
125: 	auto value = entry->AlterEntry(context, alter_info);
126: 	if (!value) {
127: 		// alter failed, but did not result in an error
128: 		return true;
129: 	}
130: 	if (value->name != original_name) {
131: 		auto mapping_value = GetMapping(context, value->name);
132: 		if (mapping_value && !mapping_value->deleted) {
133: 			auto entry = GetEntryForTransaction(context, entries[mapping_value->index].get());
134: 			if (!entry->deleted) {
135: 				string rename_err_msg =
136: 				    "Could not rename \"%s\" to \"%s\": another entry with this name already exists!";
137: 				throw CatalogException(rename_err_msg, original_name, value->name);
138: 			}
139: 		}
140: 		PutMapping(context, value->name, entry_index);
141: 		DeleteMapping(context, original_name);
142: 	}
143: 	//! Check the dependency manager to verify that there are no conflicting dependencies with this alter
144: 	catalog.dependency_manager->AlterObject(context, entry, value.get());
145: 
146: 	value->timestamp = transaction.transaction_id;
147: 	value->child = move(entries[entry_index]);
148: 	value->child->parent = value.get();
149: 	value->set = this;
150: 
151: 	// serialize the AlterInfo into a temporary buffer
152: 	BufferedSerializer serializer;
153: 	alter_info->Serialize(serializer);
154: 	BinaryData serialized_alter = serializer.GetData();
155: 
156: 	// push the old entry in the undo buffer for this transaction
157: 	transaction.PushCatalogEntry(value->child.get(), serialized_alter.data.get(), serialized_alter.size);
158: 	entries[entry_index] = move(value);
159: 
160: 	return true;
161: }
162: 
163: void CatalogSet::DropEntryInternal(ClientContext &context, idx_t entry_index, CatalogEntry &entry, bool cascade,
164:                                    set_lock_map_t &lock_set) {
165: 	auto &transaction = Transaction::GetTransaction(context);
166: 	// check any dependencies of this object
167: 	entry.catalog->dependency_manager->DropObject(context, &entry, cascade, lock_set);
168: 
169: 	// add this catalog to the lock set, if it is not there yet
170: 	if (lock_set.find(this) == lock_set.end()) {
171: 		lock_set.insert(make_pair(this, unique_lock<mutex>(catalog_lock)));
172: 	}
173: 
174: 	// create a new entry and replace the currently stored one
175: 	// set the timestamp to the timestamp of the current transaction
176: 	// and point it at the dummy node
177: 	auto value = make_unique<CatalogEntry>(CatalogType::DELETED_ENTRY, entry.catalog, entry.name);
178: 	value->timestamp = transaction.transaction_id;
179: 	value->child = move(entries[entry_index]);
180: 	value->child->parent = value.get();
181: 	value->set = this;
182: 	value->deleted = true;
183: 
184: 	// push the old entry in the undo buffer for this transaction
185: 	transaction.PushCatalogEntry(value->child.get());
186: 
187: 	entries[entry_index] = move(value);
188: }
189: 
190: bool CatalogSet::DropEntry(ClientContext &context, const string &name, bool cascade) {
191: 	// lock the catalog for writing
192: 	lock_guard<mutex> write_lock(catalog.write_lock);
193: 	// we can only delete an entry that exists
194: 	idx_t entry_index;
195: 	CatalogEntry *entry;
196: 	if (!GetEntryInternal(context, name, entry_index, entry)) {
197: 		return false;
198: 	}
199: 	if (entry->internal) {
200: 		throw CatalogException("Cannot drop entry \"%s\" because it is an internal system entry", entry->name);
201: 	}
202: 
203: 	// create the lock set for this delete operation
204: 	set_lock_map_t lock_set;
205: 	DropEntryInternal(context, entry_index, *entry, cascade, lock_set);
206: 	return true;
207: }
208: 
209: void CatalogSet::CleanupEntry(CatalogEntry *catalog_entry) {
210: 	// destroy the backed up entry: it is no longer required
211: 	D_ASSERT(catalog_entry->parent);
212: 	if (catalog_entry->parent->type != CatalogType::UPDATED_ENTRY) {
213: 		lock_guard<mutex> lock(catalog_lock);
214: 		if (!catalog_entry->deleted) {
215: 			// delete the entry from the dependency manager, if it is not deleted yet
216: 			catalog_entry->catalog->dependency_manager->EraseObject(catalog_entry);
217: 		}
218: 		catalog_entry->parent->child = move(catalog_entry->child);
219: 	}
220: }
221: 
222: bool CatalogSet::HasConflict(ClientContext &context, transaction_t timestamp) {
223: 	auto &transaction = Transaction::GetTransaction(context);
224: 	return (timestamp >= TRANSACTION_ID_START && timestamp != transaction.transaction_id) ||
225: 	       (timestamp < TRANSACTION_ID_START && timestamp > transaction.start_time);
226: }
227: 
228: MappingValue *CatalogSet::GetMapping(ClientContext &context, const string &name, bool get_latest) {
229: 	MappingValue *mapping_value;
230: 	auto entry = mapping.find(name);
231: 	if (entry != mapping.end()) {
232: 		mapping_value = entry->second.get();
233: 	} else {
234: 		return nullptr;
235: 	}
236: 	if (get_latest) {
237: 		return mapping_value;
238: 	}
239: 	while (mapping_value->child) {
240: 		if (UseTimestamp(context, mapping_value->timestamp)) {
241: 			break;
242: 		}
243: 		mapping_value = mapping_value->child.get();
244: 		D_ASSERT(mapping_value);
245: 	}
246: 	return mapping_value;
247: }
248: 
249: void CatalogSet::PutMapping(ClientContext &context, const string &name, idx_t entry_index) {
250: 	auto entry = mapping.find(name);
251: 	auto new_value = make_unique<MappingValue>(entry_index);
252: 	new_value->timestamp = Transaction::GetTransaction(context).transaction_id;
253: 	if (entry != mapping.end()) {
254: 		if (HasConflict(context, entry->second->timestamp)) {
255: 			throw TransactionException("Catalog write-write conflict on name \"%s\"", name);
256: 		}
257: 		new_value->child = move(entry->second);
258: 		new_value->child->parent = new_value.get();
259: 	}
260: 	mapping[name] = move(new_value);
261: }
262: 
263: void CatalogSet::DeleteMapping(ClientContext &context, const string &name) {
264: 	auto entry = mapping.find(name);
265: 	D_ASSERT(entry != mapping.end());
266: 	auto delete_marker = make_unique<MappingValue>(entry->second->index);
267: 	delete_marker->deleted = true;
268: 	delete_marker->timestamp = Transaction::GetTransaction(context).transaction_id;
269: 	delete_marker->child = move(entry->second);
270: 	delete_marker->child->parent = delete_marker.get();
271: 	mapping[name] = move(delete_marker);
272: }
273: 
274: bool CatalogSet::UseTimestamp(ClientContext &context, transaction_t timestamp) {
275: 	auto &transaction = Transaction::GetTransaction(context);
276: 	if (timestamp == transaction.transaction_id) {
277: 		// we created this version
278: 		return true;
279: 	}
280: 	if (timestamp < transaction.start_time) {
281: 		// this version was commited before we started the transaction
282: 		return true;
283: 	}
284: 	return false;
285: }
286: 
287: CatalogEntry *CatalogSet::GetEntryForTransaction(ClientContext &context, CatalogEntry *current) {
288: 	while (current->child) {
289: 		if (UseTimestamp(context, current->timestamp)) {
290: 			break;
291: 		}
292: 		current = current->child.get();
293: 		D_ASSERT(current);
294: 	}
295: 	return current;
296: }
297: 
298: CatalogEntry *CatalogSet::GetCommittedEntry(CatalogEntry *current) {
299: 	while (current->child) {
300: 		if (current->timestamp < TRANSACTION_ID_START) {
301: 			// this entry is committed: use it
302: 			break;
303: 		}
304: 		current = current->child.get();
305: 		D_ASSERT(current);
306: 	}
307: 	return current;
308: }
309: 
310: string CatalogSet::SimilarEntry(ClientContext &context, const string &name) {
311: 	lock_guard<mutex> lock(catalog_lock);
312: 
313: 	string result;
314: 	idx_t current_score = (idx_t)-1;
315: 	for (auto &kv : mapping) {
316: 		auto mapping_value = GetMapping(context, kv.first);
317: 		if (mapping_value && !mapping_value->deleted) {
318: 			auto ldist = StringUtil::LevenshteinDistance(kv.first, name);
319: 			if (ldist < current_score) {
320: 				current_score = ldist;
321: 				result = kv.first;
322: 			}
323: 		}
324: 	}
325: 	return result;
326: }
327: 
328: CatalogEntry *CatalogSet::CreateEntryInternal(ClientContext &context, unique_ptr<CatalogEntry> entry) {
329: 	if (mapping.find(entry->name) != mapping.end()) {
330: 		return nullptr;
331: 	}
332: 	auto &name = entry->name;
333: 	auto entry_index = current_entry++;
334: 	auto catalog_entry = entry.get();
335: 
336: 	entry->timestamp = 0;
337: 
338: 	PutMapping(context, name, entry_index);
339: 	mapping[name]->timestamp = 0;
340: 	entries[entry_index] = move(entry);
341: 	return catalog_entry;
342: }
343: 
344: CatalogEntry *CatalogSet::GetEntry(ClientContext &context, const string &name) {
345: 	unique_lock<mutex> lock(catalog_lock);
346: 	auto mapping_value = GetMapping(context, name);
347: 	if (mapping_value != nullptr && !mapping_value->deleted) {
348: 		// we found an entry for this name
349: 		// check the version numbers
350: 
351: 		auto catalog_entry = entries[mapping_value->index].get();
352: 		CatalogEntry *current = GetEntryForTransaction(context, catalog_entry);
353: 		if (current->deleted || (current->name != name && !UseTimestamp(context, mapping_value->timestamp))) {
354: 			return nullptr;
355: 		}
356: 		return current;
357: 	}
358: 	// no entry found with this name, check for defaults
359: 	if (!defaults || defaults->created_all_entries) {
360: 		// no defaults either: return null
361: 		return nullptr;
362: 	}
363: 	// this catalog set has a default map defined
364: 	// check if there is a default entry that we can create with this name
365: 	lock.unlock();
366: 	auto entry = defaults->CreateDefaultEntry(context, name);
367: 
368: 	lock.lock();
369: 	if (!entry) {
370: 		// no default entry
371: 		return nullptr;
372: 	}
373: 	// there is a default entry! create it
374: 	auto result = CreateEntryInternal(context, move(entry));
375: 	if (result) {
376: 		return result;
377: 	}
378: 	// we found a default entry, but failed
379: 	// this means somebody else created the entry first
380: 	// just retry?
381: 	lock.unlock();
382: 	return GetEntry(context, name);
383: }
384: 
385: void CatalogSet::UpdateTimestamp(CatalogEntry *entry, transaction_t timestamp) {
386: 	entry->timestamp = timestamp;
387: 	mapping[entry->name]->timestamp = timestamp;
388: }
389: 
390: void CatalogSet::Undo(CatalogEntry *entry) {
391: 	lock_guard<mutex> write_lock(catalog.write_lock);
392: 
393: 	lock_guard<mutex> lock(catalog_lock);
394: 
395: 	// entry has to be restored
396: 	// and entry->parent has to be removed ("rolled back")
397: 
398: 	// i.e. we have to place (entry) as (entry->parent) again
399: 	auto &to_be_removed_node = entry->parent;
400: 	if (!to_be_removed_node->deleted) {
401: 		// delete the entry from the dependency manager as well
402: 		catalog.dependency_manager->EraseObject(to_be_removed_node);
403: 	}
404: 	if (entry->name != to_be_removed_node->name) {
405: 		// rename: clean up the new name when the rename is rolled back
406: 		auto removed_entry = mapping.find(to_be_removed_node->name);
407: 		if (removed_entry->second->child) {
408: 			removed_entry->second->child->parent = nullptr;
409: 			mapping[to_be_removed_node->name] = move(removed_entry->second->child);
410: 		} else {
411: 			mapping.erase(removed_entry);
412: 		}
413: 	}
414: 	if (to_be_removed_node->parent) {
415: 		// if the to be removed node has a parent, set the child pointer to the
416: 		// to be restored node
417: 		to_be_removed_node->parent->child = move(to_be_removed_node->child);
418: 		entry->parent = to_be_removed_node->parent;
419: 	} else {
420: 		// otherwise we need to update the base entry tables
421: 		auto &name = entry->name;
422: 		to_be_removed_node->child->SetAsRoot();
423: 		entries[mapping[name]->index] = move(to_be_removed_node->child);
424: 		entry->parent = nullptr;
425: 	}
426: 
427: 	// restore the name if it was deleted
428: 	auto restored_entry = mapping.find(entry->name);
429: 	if (restored_entry->second->deleted || entry->type == CatalogType::INVALID) {
430: 		if (restored_entry->second->child) {
431: 			restored_entry->second->child->parent = nullptr;
432: 			mapping[entry->name] = move(restored_entry->second->child);
433: 		} else {
434: 			mapping.erase(restored_entry);
435: 		}
436: 	}
437: 	// we mark the catalog as being modified, since this action can lead to e.g. tables being dropped
438: 	entry->catalog->ModifyCatalog();
439: }
440: 
441: void CatalogSet::Scan(ClientContext &context, const std::function<void(CatalogEntry *)> &callback) {
442: 	// lock the catalog set
443: 	unique_lock<mutex> lock(catalog_lock);
444: 	if (defaults && !defaults->created_all_entries) {
445: 		// this catalog set has a default set defined:
446: 		auto default_entries = defaults->GetDefaultEntries();
447: 		for (auto &default_entry : default_entries) {
448: 			auto map_entry = mapping.find(default_entry);
449: 			if (map_entry == mapping.end()) {
450: 				// we unlock during the CreateEntry, since it might reference other catalog sets...
451: 				// specifically for views this can happen since the view will be bound
452: 				lock.unlock();
453: 				auto entry = defaults->CreateDefaultEntry(context, default_entry);
454: 
455: 				lock.lock();
456: 				CreateEntryInternal(context, move(entry));
457: 			}
458: 		}
459: 		defaults->created_all_entries = true;
460: 	}
461: 	for (auto &kv : entries) {
462: 		auto entry = kv.second.get();
463: 		entry = GetEntryForTransaction(context, entry);
464: 		if (!entry->deleted) {
465: 			callback(entry);
466: 		}
467: 	}
468: }
469: 
470: void CatalogSet::Scan(const std::function<void(CatalogEntry *)> &callback) {
471: 	// lock the catalog set
472: 	lock_guard<mutex> lock(catalog_lock);
473: 	for (auto &kv : entries) {
474: 		auto entry = kv.second.get();
475: 		entry = GetCommittedEntry(entry);
476: 		if (!entry->deleted) {
477: 			callback(entry);
478: 		}
479: 	}
480: }
481: 
482: } // namespace duckdb
[end of src/catalog/catalog_set.cpp]
[start of src/common/string_util.cpp]
1: #include "duckdb/common/string_util.hpp"
2: #include "duckdb/common/pair.hpp"
3: #include "duckdb/common/to_string.hpp"
4: #include "duckdb/common/string_util.hpp"
5: 
6: #include <algorithm>
7: #include <cctype>
8: #include <iomanip>
9: #include <memory>
10: #include <sstream>
11: #include <stdarg.h>
12: #include <string.h>
13: 
14: namespace duckdb {
15: 
16: bool StringUtil::Contains(const string &haystack, const string &needle) {
17: 	return (haystack.find(needle) != string::npos);
18: }
19: 
20: void StringUtil::LTrim(string &str) {
21: 	auto it = str.begin();
22: 	while (CharacterIsSpace(*it)) {
23: 		it++;
24: 	}
25: 	str.erase(str.begin(), it);
26: }
27: 
28: // Remove trailing ' ', '\f', '\n', '\r', '\t', '\v'
29: void StringUtil::RTrim(string &str) {
30: 	str.erase(find_if(str.rbegin(), str.rend(), [](int ch) { return ch > 0 && !CharacterIsSpace(ch); }).base(),
31: 	          str.end());
32: }
33: 
34: void StringUtil::Trim(string &str) {
35: 	StringUtil::LTrim(str);
36: 	StringUtil::RTrim(str);
37: }
38: 
39: bool StringUtil::StartsWith(string str, string prefix) {
40: 	if (prefix.size() > str.size()) {
41: 		return false;
42: 	}
43: 	return equal(prefix.begin(), prefix.end(), str.begin());
44: }
45: 
46: bool StringUtil::EndsWith(const string &str, const string &suffix) {
47: 	if (suffix.size() > str.size()) {
48: 		return false;
49: 	}
50: 	return equal(suffix.rbegin(), suffix.rend(), str.rbegin());
51: }
52: 
53: string StringUtil::Repeat(const string &str, idx_t n) {
54: 	std::ostringstream os;
55: 	for (idx_t i = 0; i < n; i++) {
56: 		os << str;
57: 	}
58: 	return (os.str());
59: }
60: 
61: vector<string> StringUtil::Split(const string &str, char delimiter) {
62: 	std::stringstream ss(str);
63: 	vector<string> lines;
64: 	string temp;
65: 	while (getline(ss, temp, delimiter)) {
66: 		lines.push_back(temp);
67: 	}
68: 	return (lines);
69: }
70: 
71: string StringUtil::Join(const vector<string> &input, const string &separator) {
72: 	return StringUtil::Join(input, input.size(), separator, [](const string &s) { return s; });
73: }
74: 
75: string StringUtil::BytesToHumanReadableString(idx_t bytes) {
76: 	string db_size;
77: 	auto kilobytes = bytes / 1000;
78: 	auto megabytes = kilobytes / 1000;
79: 	kilobytes -= megabytes * 1000;
80: 	auto gigabytes = megabytes / 1000;
81: 	megabytes -= gigabytes * 1000;
82: 	auto terabytes = gigabytes / 1000;
83: 	gigabytes -= terabytes * 1000;
84: 	if (terabytes > 0) {
85: 		return to_string(terabytes) + "." + to_string(gigabytes / 100) + "TB";
86: 	} else if (gigabytes > 0) {
87: 		return to_string(gigabytes) + "." + to_string(megabytes / 100) + "GB";
88: 	} else if (megabytes > 0) {
89: 		return to_string(megabytes) + "." + to_string(kilobytes / 100) + "MB";
90: 	} else if (kilobytes > 0) {
91: 		return to_string(kilobytes) + "KB";
92: 	} else {
93: 		return to_string(bytes) + " bytes";
94: 	}
95: }
96: 
97: string StringUtil::Upper(const string &str) {
98: 	string copy(str);
99: 	transform(copy.begin(), copy.end(), copy.begin(), [](unsigned char c) { return std::toupper(c); });
100: 	return (copy);
101: }
102: 
103: string StringUtil::Lower(const string &str) {
104: 	string copy(str);
105: 	transform(copy.begin(), copy.end(), copy.begin(), [](unsigned char c) { return std::tolower(c); });
106: 	return (copy);
107: }
108: 
109: vector<string> StringUtil::Split(const string &input, const string &split) {
110: 	vector<string> splits;
111: 
112: 	idx_t last = 0;
113: 	idx_t input_len = input.size();
114: 	idx_t split_len = split.size();
115: 	while (last <= input_len) {
116: 		idx_t next = input.find(split, last);
117: 		if (next == string::npos) {
118: 			next = input_len;
119: 		}
120: 
121: 		// Push the substring [last, next) on to splits
122: 		string substr = input.substr(last, next - last);
123: 		if (substr.empty() == false) {
124: 			splits.push_back(substr);
125: 		}
126: 		last = next + split_len;
127: 	}
128: 	return splits;
129: }
130: 
131: string StringUtil::Replace(string source, const string &from, const string &to) {
132: 	idx_t start_pos = 0;
133: 	while ((start_pos = source.find(from, start_pos)) != string::npos) {
134: 		source.replace(start_pos, from.length(), to);
135: 		start_pos += to.length(); // In case 'to' contains 'from', like
136: 		                          // replacing 'x' with 'yx'
137: 	}
138: 	return source;
139: }
140: 
141: vector<string> StringUtil::TopNStrings(vector<pair<string, idx_t>> scores, idx_t n, idx_t threshold) {
142: 	if (scores.empty()) {
143: 		return vector<string>();
144: 	}
145: 	sort(scores.begin(), scores.end(),
146: 	     [](const pair<string, idx_t> &a, const pair<string, idx_t> &b) -> bool { return a.second < b.second; });
147: 	vector<string> result;
148: 	result.push_back(scores[0].first);
149: 	for (idx_t i = 1; i < MinValue<idx_t>(scores.size(), n); i++) {
150: 		if (scores[i].second > threshold) {
151: 			break;
152: 		}
153: 		result.push_back(scores[i].first);
154: 	}
155: 	return result;
156: }
157: 
158: struct LevenshteinArray {
159: 	LevenshteinArray(idx_t len1, idx_t len2) : len1(len1) {
160: 		dist = unique_ptr<idx_t[]>(new idx_t[len1 * len2]);
161: 	}
162: 
163: 	idx_t &Score(idx_t i, idx_t j) {
164: 		return dist[GetIndex(i, j)];
165: 	}
166: 
167: private:
168: 	idx_t len1;
169: 	unique_ptr<idx_t[]> dist;
170: 
171: 	idx_t GetIndex(idx_t i, idx_t j) {
172: 		return j * len1 + i;
173: 	}
174: };
175: 
176: // adapted from https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#C++
177: idx_t StringUtil::LevenshteinDistance(const string &s1, const string &s2) {
178: 	idx_t len1 = s1.size();
179: 	idx_t len2 = s2.size();
180: 	if (len1 == 0) {
181: 		return len2;
182: 	}
183: 	if (len2 == 0) {
184: 		return len1;
185: 	}
186: 	LevenshteinArray array(len1 + 1, len2 + 1);
187: 	array.Score(0, 0) = 0;
188: 	for (idx_t i = 0; i <= len1; i++) {
189: 		array.Score(i, 0) = i;
190: 	}
191: 	for (idx_t j = 0; j <= len2; j++) {
192: 		array.Score(0, j) = j;
193: 	}
194: 	for (idx_t i = 1; i <= len1; i++) {
195: 		for (idx_t j = 1; j <= len2; j++) {
196: 			// d[i][j] = std::min({ d[i - 1][j] + 1,
197: 			//                      d[i][j - 1] + 1,
198: 			//                      d[i - 1][j - 1] + (s1[i - 1] == s2[j - 1] ? 0 : 1) });
199: 			int equal = s1[i - 1] == s2[j - 1] ? 0 : 1;
200: 			idx_t adjacent_score1 = array.Score(i - 1, j) + 1;
201: 			idx_t adjacent_score2 = array.Score(i, j - 1) + 1;
202: 			idx_t adjacent_score3 = array.Score(i - 1, j - 1) + equal;
203: 
204: 			idx_t t = MinValue<idx_t>(adjacent_score1, adjacent_score2);
205: 			array.Score(i, j) = MinValue<idx_t>(t, adjacent_score3);
206: 		}
207: 	}
208: 	return array.Score(len1, len2);
209: }
210: 
211: vector<string> StringUtil::TopNLevenshtein(const vector<string> &strings, const string &target, idx_t n,
212:                                            idx_t threshold) {
213: 	vector<pair<string, idx_t>> scores;
214: 	scores.reserve(strings.size());
215: 	for (auto &str : strings) {
216: 		scores.emplace_back(str, LevenshteinDistance(str, target));
217: 	}
218: 	return TopNStrings(scores, n, threshold);
219: }
220: 
221: string StringUtil::CandidatesMessage(const vector<string> &candidates, const string &candidate) {
222: 	string result_str;
223: 	if (!candidates.empty()) {
224: 		result_str = "\n" + candidate + ": ";
225: 		for (idx_t i = 0; i < candidates.size(); i++) {
226: 			if (i > 0) {
227: 				result_str += ", ";
228: 			}
229: 			result_str += "\"" + candidates[i] + "\"";
230: 		}
231: 	}
232: 	return result_str;
233: }
234: 
235: } // namespace duckdb
[end of src/common/string_util.cpp]
[start of src/execution/operator/helper/physical_set.cpp]
1: #include "duckdb/execution/operator/helper/physical_set.hpp"
2: #include "duckdb/main/database.hpp"
3: #include "duckdb/main/client_context.hpp"
4: 
5: namespace duckdb {
6: 
7: void PhysicalSet::GetChunkInternal(ExecutionContext &context, DataChunk &chunk, PhysicalOperatorState *state) const {
8: 	D_ASSERT(scope == SetScope::GLOBAL || scope == SetScope::SESSION);
9: 
10: 	if (scope == SetScope::GLOBAL) {
11: 		context.client.db->config.set_variables[name] = value;
12: 	} else {
13: 		context.client.set_variables[name] = value;
14: 	}
15: 
16: 	state->finished = true;
17: }
18: 
19: } // namespace duckdb
[end of src/execution/operator/helper/physical_set.cpp]
[start of src/function/scalar/generic/current_setting.cpp]
1: #include "duckdb/function/scalar/generic_functions.hpp"
2: #include "duckdb/main/database.hpp"
3: #include "duckdb/main/client_context.hpp"
4: #include "duckdb/planner/expression/bound_function_expression.hpp"
5: #include "duckdb/execution/expression_executor.hpp"
6: 
7: namespace duckdb {
8: 
9: struct CurrentSettingBindData : public FunctionData {
10: 	explicit CurrentSettingBindData(Value value_p) : value(move(value_p)) {
11: 	}
12: 
13: 	Value value;
14: 
15: public:
16: 	unique_ptr<FunctionData> Copy() override {
17: 		return make_unique<CurrentSettingBindData>(value);
18: 	}
19: };
20: 
21: static void CurrentSettingFunction(DataChunk &args, ExpressionState &state, Vector &result) {
22: 	auto &func_expr = (BoundFunctionExpression &)state.expr;
23: 	auto &info = (CurrentSettingBindData &)*func_expr.bind_info;
24: 	result.Reference(info.value);
25: }
26: 
27: unique_ptr<FunctionData> CurrentSettingBind(ClientContext &context, ScalarFunction &bound_function,
28:                                             vector<unique_ptr<Expression>> &arguments) {
29: 
30: 	auto &key_child = arguments[0];
31: 
32: 	if (key_child->return_type.id() != LogicalTypeId::VARCHAR ||
33: 	    key_child->return_type.id() != LogicalTypeId::VARCHAR || !key_child->IsFoldable()) {
34: 		throw ParserException("Key name for struct_extract needs to be a constant string");
35: 	}
36: 	Value key_val = ExpressionExecutor::EvaluateScalar(*key_child.get());
37: 	D_ASSERT(key_val.type().id() == LogicalTypeId::VARCHAR);
38: 	if (key_val.is_null || key_val.str_value.length() < 1) {
39: 		throw ParserException("Key name for struct_extract needs to be neither NULL nor empty");
40: 	}
41: 
42: 	const auto &key = key_val.str_value;
43: 	Value val;
44: 	if (!context.TryGetCurrentSetting(key, val)) {
45: 		throw InvalidInputException("Variable '%s' was not SET in this context", key);
46: 	}
47: 
48: 	bound_function.return_type = val.type();
49: 	return make_unique<CurrentSettingBindData>(val);
50: }
51: 
52: void CurrentSettingFun::RegisterFunction(BuiltinFunctions &set) {
53: 	set.AddFunction(ScalarFunction("current_setting", {LogicalType::VARCHAR}, LogicalType::ANY, CurrentSettingFunction,
54: 	                               false, CurrentSettingBind));
55: }
56: 
57: } // namespace duckdb
[end of src/function/scalar/generic/current_setting.cpp]
[start of src/include/duckdb/catalog/catalog.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/catalog/catalog.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/catalog/catalog_entry.hpp"
12: #include "duckdb/common/mutex.hpp"
13: #include "duckdb/parser/query_error_context.hpp"
14: 
15: #include <functional>
16: #include "duckdb/common/atomic.hpp"
17: 
18: namespace duckdb {
19: struct CreateSchemaInfo;
20: struct DropInfo;
21: struct BoundCreateTableInfo;
22: struct AlterTableInfo;
23: struct CreateTableFunctionInfo;
24: struct CreateCopyFunctionInfo;
25: struct CreatePragmaFunctionInfo;
26: struct CreateFunctionInfo;
27: struct CreateViewInfo;
28: struct CreateSequenceInfo;
29: struct CreateCollationInfo;
30: 
31: class ClientContext;
32: class Transaction;
33: 
34: class AggregateFunctionCatalogEntry;
35: class CollateCatalogEntry;
36: class SchemaCatalogEntry;
37: class TableCatalogEntry;
38: class ViewCatalogEntry;
39: class SequenceCatalogEntry;
40: class TableFunctionCatalogEntry;
41: class CopyFunctionCatalogEntry;
42: class PragmaFunctionCatalogEntry;
43: class CatalogSet;
44: class DatabaseInstance;
45: class DependencyManager;
46: 
47: //! The Catalog object represents the catalog of the database.
48: class Catalog {
49: public:
50: 	explicit Catalog(DatabaseInstance &db);
51: 	~Catalog();
52: 
53: 	//! Reference to the database
54: 	DatabaseInstance &db;
55: 	//! The catalog set holding the schemas
56: 	unique_ptr<CatalogSet> schemas;
57: 	//! The DependencyManager manages dependencies between different catalog objects
58: 	unique_ptr<DependencyManager> dependency_manager;
59: 	//! Write lock for the catalog
60: 	mutex write_lock;
61: 
62: public:
63: 	//! Get the ClientContext from the Catalog
64: 	static Catalog &GetCatalog(ClientContext &context);
65: 	static Catalog &GetCatalog(DatabaseInstance &db);
66: 
67: 	DependencyManager &GetDependencyManager() {
68: 		return *dependency_manager;
69: 	}
70: 
71: 	//! Returns the current version of the catalog (incremented whenever anything changes, not stored between restarts)
72: 	idx_t GetCatalogVersion();
73: 	//! Trigger a modification in the catalog, increasing the catalog version and returning the previous version
74: 	idx_t ModifyCatalog();
75: 
76: 	//! Creates a schema in the catalog.
77: 	CatalogEntry *CreateSchema(ClientContext &context, CreateSchemaInfo *info);
78: 	//! Creates a table in the catalog.
79: 	CatalogEntry *CreateTable(ClientContext &context, BoundCreateTableInfo *info);
80: 	//! Create a table function in the catalog
81: 	CatalogEntry *CreateTableFunction(ClientContext &context, CreateTableFunctionInfo *info);
82: 	//! Create a copy function in the catalog
83: 	CatalogEntry *CreateCopyFunction(ClientContext &context, CreateCopyFunctionInfo *info);
84: 	//! Create a pragma function in the catalog
85: 	CatalogEntry *CreatePragmaFunction(ClientContext &context, CreatePragmaFunctionInfo *info);
86: 	//! Create a scalar or aggregate function in the catalog
87: 	CatalogEntry *CreateFunction(ClientContext &context, CreateFunctionInfo *info);
88: 	//! Creates a table in the catalog.
89: 	CatalogEntry *CreateView(ClientContext &context, CreateViewInfo *info);
90: 	//! Creates a table in the catalog.
91: 	CatalogEntry *CreateSequence(ClientContext &context, CreateSequenceInfo *info);
92: 	//! Creates a collation in the catalog
93: 	CatalogEntry *CreateCollation(ClientContext &context, CreateCollationInfo *info);
94: 
95: 	//! Creates a table in the catalog.
96: 	CatalogEntry *CreateTable(ClientContext &context, SchemaCatalogEntry *schema, BoundCreateTableInfo *info);
97: 	//! Create a table function in the catalog
98: 	CatalogEntry *CreateTableFunction(ClientContext &context, SchemaCatalogEntry *schema,
99: 	                                  CreateTableFunctionInfo *info);
100: 	//! Create a copy function in the catalog
101: 	CatalogEntry *CreateCopyFunction(ClientContext &context, SchemaCatalogEntry *schema, CreateCopyFunctionInfo *info);
102: 	//! Create a pragma function in the catalog
103: 	CatalogEntry *CreatePragmaFunction(ClientContext &context, SchemaCatalogEntry *schema,
104: 	                                   CreatePragmaFunctionInfo *info);
105: 	//! Create a scalar or aggregate function in the catalog
106: 	CatalogEntry *CreateFunction(ClientContext &context, SchemaCatalogEntry *schema, CreateFunctionInfo *info);
107: 	//! Creates a table in the catalog.
108: 	CatalogEntry *CreateView(ClientContext &context, SchemaCatalogEntry *schema, CreateViewInfo *info);
109: 	//! Creates a table in the catalog.
110: 	CatalogEntry *CreateSequence(ClientContext &context, SchemaCatalogEntry *schema, CreateSequenceInfo *info);
111: 	//! Creates a collation in the catalog
112: 	CatalogEntry *CreateCollation(ClientContext &context, SchemaCatalogEntry *schema, CreateCollationInfo *info);
113: 
114: 	//! Drops an entry from the catalog
115: 	void DropEntry(ClientContext &context, DropInfo *info);
116: 
117: 	//! Returns the schema object with the specified name, or throws an exception if it does not exist
118: 	SchemaCatalogEntry *GetSchema(ClientContext &context, const string &name = DEFAULT_SCHEMA,
119: 	                              QueryErrorContext error_context = QueryErrorContext());
120: 	//! Scans all the schemas in the system one-by-one, invoking the callback for each entry
121: 	void ScanSchemas(ClientContext &context, std::function<void(CatalogEntry *)> callback);
122: 	//! Gets the "schema.name" entry of the specified type, if if_exists=true returns nullptr if entry does not exist,
123: 	//! otherwise an exception is thrown
124: 	CatalogEntry *GetEntry(ClientContext &context, CatalogType type, string schema, const string &name,
125: 	                       bool if_exists = false, QueryErrorContext error_context = QueryErrorContext());
126: 
127: 	template <class T>
128: 	T *GetEntry(ClientContext &context, string schema_name, const string &name, bool if_exists = false,
129: 	            QueryErrorContext error_context = QueryErrorContext());
130: 
131: 	//! Alter an existing entry in the catalog.
132: 	void Alter(ClientContext &context, AlterInfo *info);
133: 
134: private:
135: 	//! The catalog version, incremented whenever anything changes in the catalog
136: 	atomic<idx_t> catalog_version;
137: 
138: private:
139: 	void DropSchema(ClientContext &context, DropInfo *info);
140: };
141: 
142: template <>
143: TableCatalogEntry *Catalog::GetEntry(ClientContext &context, string schema_name, const string &name, bool if_exists,
144:                                      QueryErrorContext error_context);
145: template <>
146: SequenceCatalogEntry *Catalog::GetEntry(ClientContext &context, string schema_name, const string &name, bool if_exists,
147:                                         QueryErrorContext error_context);
148: template <>
149: TableFunctionCatalogEntry *Catalog::GetEntry(ClientContext &context, string schema_name, const string &name,
150:                                              bool if_exists, QueryErrorContext error_context);
151: template <>
152: CopyFunctionCatalogEntry *Catalog::GetEntry(ClientContext &context, string schema_name, const string &name,
153:                                             bool if_exists, QueryErrorContext error_context);
154: template <>
155: PragmaFunctionCatalogEntry *Catalog::GetEntry(ClientContext &context, string schema_name, const string &name,
156:                                               bool if_exists, QueryErrorContext error_context);
157: template <>
158: AggregateFunctionCatalogEntry *Catalog::GetEntry(ClientContext &context, string schema_name, const string &name,
159:                                                  bool if_exists, QueryErrorContext error_context);
160: template <>
161: CollateCatalogEntry *Catalog::GetEntry(ClientContext &context, string schema_name, const string &name, bool if_exists,
162:                                        QueryErrorContext error_context);
163: 
164: } // namespace duckdb
[end of src/include/duckdb/catalog/catalog.hpp]
[start of src/include/duckdb/catalog/catalog_entry/schema_catalog_entry.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/catalog/catalog_entry/schema_catalog_entry.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/catalog/catalog_entry.hpp"
12: #include "duckdb/catalog/catalog_set.hpp"
13: #include "duckdb/parser/query_error_context.hpp"
14: 
15: namespace duckdb {
16: class ClientContext;
17: 
18: class StandardEntry;
19: class TableCatalogEntry;
20: class TableFunctionCatalogEntry;
21: class SequenceCatalogEntry;
22: class Serializer;
23: class Deserializer;
24: 
25: enum class OnCreateConflict : uint8_t;
26: 
27: struct AlterTableInfo;
28: struct CreateIndexInfo;
29: struct CreateFunctionInfo;
30: struct CreateCollationInfo;
31: struct CreateViewInfo;
32: struct BoundCreateTableInfo;
33: struct CreatePragmaFunctionInfo;
34: struct CreateSequenceInfo;
35: struct CreateSchemaInfo;
36: struct CreateTableFunctionInfo;
37: struct CreateCopyFunctionInfo;
38: 
39: struct DropInfo;
40: 
41: //! A schema in the catalog
42: class SchemaCatalogEntry : public CatalogEntry {
43: 	friend class Catalog;
44: 
45: public:
46: 	SchemaCatalogEntry(Catalog *catalog, string name, bool is_internal);
47: 
48: private:
49: 	//! The catalog set holding the tables
50: 	CatalogSet tables;
51: 	//! The catalog set holding the indexes
52: 	CatalogSet indexes;
53: 	//! The catalog set holding the table functions
54: 	CatalogSet table_functions;
55: 	//! The catalog set holding the copy functions
56: 	CatalogSet copy_functions;
57: 	//! The catalog set holding the pragma functions
58: 	CatalogSet pragma_functions;
59: 	//! The catalog set holding the scalar and aggregate functions
60: 	CatalogSet functions;
61: 	//! The catalog set holding the sequences
62: 	CatalogSet sequences;
63: 	//! The catalog set holding the collations
64: 	CatalogSet collations;
65: 
66: public:
67: 	//! Gets a catalog entry from the given catalog set matching the given name
68: 	CatalogEntry *GetEntry(ClientContext &context, CatalogType type, const string &name, bool if_exists,
69: 	                       QueryErrorContext error_context = QueryErrorContext());
70: 
71: 	//! Scan the specified catalog set, invoking the callback method for every entry
72: 	void Scan(ClientContext &context, CatalogType type, const std::function<void(CatalogEntry *)> &callback);
73: 	//! Scan the specified catalog set, invoking the callback method for every committed entry
74: 	void Scan(CatalogType type, const std::function<void(CatalogEntry *)> &callback);
75: 
76: 	//! Serialize the meta information of the SchemaCatalogEntry a serializer
77: 	virtual void Serialize(Serializer &serializer);
78: 	//! Deserializes to a CreateSchemaInfo
79: 	static unique_ptr<CreateSchemaInfo> Deserialize(Deserializer &source);
80: 
81: 	string ToSQL() override;
82: 
83: 	//! Creates an index with the given name in the schema
84: 	CatalogEntry *CreateIndex(ClientContext &context, CreateIndexInfo *info, TableCatalogEntry *table);
85: 
86: private:
87: 	//! Create a scalar or aggregate function within the given schema
88: 	CatalogEntry *CreateFunction(ClientContext &context, CreateFunctionInfo *info);
89: 	//! Creates a table with the given name in the schema
90: 	CatalogEntry *CreateTable(ClientContext &context, BoundCreateTableInfo *info);
91: 	//! Creates a view with the given name in the schema
92: 	CatalogEntry *CreateView(ClientContext &context, CreateViewInfo *info);
93: 	//! Creates a sequence with the given name in the schema
94: 	CatalogEntry *CreateSequence(ClientContext &context, CreateSequenceInfo *info);
95: 	//! Create a table function within the given schema
96: 	CatalogEntry *CreateTableFunction(ClientContext &context, CreateTableFunctionInfo *info);
97: 	//! Create a copy function within the given schema
98: 	CatalogEntry *CreateCopyFunction(ClientContext &context, CreateCopyFunctionInfo *info);
99: 	//! Create a pragma function within the given schema
100: 	CatalogEntry *CreatePragmaFunction(ClientContext &context, CreatePragmaFunctionInfo *info);
101: 	//! Create a collation within the given schema
102: 	CatalogEntry *CreateCollation(ClientContext &context, CreateCollationInfo *info);
103: 
104: 	//! Drops an entry from the schema
105: 	void DropEntry(ClientContext &context, DropInfo *info);
106: 
107: 	//! Alters a catalog entry
108: 	void Alter(ClientContext &context, AlterInfo *info);
109: 
110: 	//! Add a catalog entry to this schema
111: 	CatalogEntry *AddEntry(ClientContext &context, unique_ptr<StandardEntry> entry, OnCreateConflict on_conflict);
112: 	//! Add a catalog entry to this schema
113: 	CatalogEntry *AddEntry(ClientContext &context, unique_ptr<StandardEntry> entry, OnCreateConflict on_conflict,
114: 	                       unordered_set<CatalogEntry *> dependencies);
115: 
116: 	//! Get the catalog set for the specified type
117: 	CatalogSet &GetCatalogSet(CatalogType type);
118: };
119: } // namespace duckdb
[end of src/include/duckdb/catalog/catalog_entry/schema_catalog_entry.hpp]
[start of src/include/duckdb/catalog/catalog_set.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/catalog/catalog_set.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/catalog/catalog_entry.hpp"
12: #include "duckdb/catalog/default/default_generator.hpp"
13: #include "duckdb/common/common.hpp"
14: #include "duckdb/common/case_insensitive_map.hpp"
15: #include "duckdb/common/unordered_set.hpp"
16: #include "duckdb/common/mutex.hpp"
17: 
18: #include <functional>
19: #include <memory>
20: 
21: namespace duckdb {
22: struct AlterInfo;
23: 
24: class ClientContext;
25: 
26: typedef unordered_map<CatalogSet *, unique_lock<mutex>> set_lock_map_t;
27: 
28: struct MappingValue {
29: 	explicit MappingValue(idx_t index_) : index(index_), timestamp(0), deleted(false), parent(nullptr) {
30: 	}
31: 
32: 	idx_t index;
33: 	transaction_t timestamp;
34: 	bool deleted;
35: 	unique_ptr<MappingValue> child;
36: 	MappingValue *parent;
37: };
38: 
39: //! The Catalog Set stores (key, value) map of a set of CatalogEntries
40: class CatalogSet {
41: 	friend class DependencyManager;
42: 
43: public:
44: 	explicit CatalogSet(Catalog &catalog, unique_ptr<DefaultGenerator> defaults = nullptr);
45: 
46: 	//! Create an entry in the catalog set. Returns whether or not it was
47: 	//! successful.
48: 	bool CreateEntry(ClientContext &context, const string &name, unique_ptr<CatalogEntry> value,
49: 	                 unordered_set<CatalogEntry *> &dependencies);
50: 
51: 	bool AlterEntry(ClientContext &context, const string &name, AlterInfo *alter_info);
52: 
53: 	bool DropEntry(ClientContext &context, const string &name, bool cascade);
54: 
55: 	void CleanupEntry(CatalogEntry *catalog_entry);
56: 
57: 	//! Returns the entry with the specified name
58: 	CatalogEntry *GetEntry(ClientContext &context, const string &name);
59: 
60: 	//! Gets the entry that is most similar to the given name (i.e. smallest levenshtein distance), or empty string if
61: 	//! none is found
62: 	string SimilarEntry(ClientContext &context, const string &name);
63: 
64: 	//! Rollback <entry> to be the currently valid entry for a certain catalog
65: 	//! entry
66: 	void Undo(CatalogEntry *entry);
67: 
68: 	//! Scan the catalog set, invoking the callback method for every committed entry
69: 	void Scan(const std::function<void(CatalogEntry *)> &callback);
70: 	//! Scan the catalog set, invoking the callback method for every entry
71: 	void Scan(ClientContext &context, const std::function<void(CatalogEntry *)> &callback);
72: 
73: 	template <class T>
74: 	vector<T *> GetEntries(ClientContext &context) {
75: 		vector<T *> result;
76: 		Scan(context, [&](CatalogEntry *entry) { result.push_back((T *)entry); });
77: 		return result;
78: 	}
79: 
80: 	static bool HasConflict(ClientContext &context, transaction_t timestamp);
81: 	static bool UseTimestamp(ClientContext &context, transaction_t timestamp);
82: 
83: 	CatalogEntry *GetEntryFromIndex(idx_t index);
84: 	void UpdateTimestamp(CatalogEntry *entry, transaction_t timestamp);
85: 
86: private:
87: 	//! Given a root entry, gets the entry valid for this transaction
88: 	CatalogEntry *GetEntryForTransaction(ClientContext &context, CatalogEntry *current);
89: 	CatalogEntry *GetCommittedEntry(CatalogEntry *current);
90: 	bool GetEntryInternal(ClientContext &context, const string &name, idx_t &entry_index, CatalogEntry *&entry);
91: 	bool GetEntryInternal(ClientContext &context, idx_t entry_index, CatalogEntry *&entry);
92: 	//! Drops an entry from the catalog set; must hold the catalog_lock to safely call this
93: 	void DropEntryInternal(ClientContext &context, idx_t entry_index, CatalogEntry &entry, bool cascade,
94: 	                       set_lock_map_t &lock_set);
95: 	CatalogEntry *CreateEntryInternal(ClientContext &context, unique_ptr<CatalogEntry> entry);
96: 	MappingValue *GetMapping(ClientContext &context, const string &name, bool get_latest = false);
97: 	void PutMapping(ClientContext &context, const string &name, idx_t entry_index);
98: 	void DeleteMapping(ClientContext &context, const string &name);
99: 
100: private:
101: 	Catalog &catalog;
102: 	//! The catalog lock is used to make changes to the data
103: 	mutex catalog_lock;
104: 	//! Mapping of string to catalog entry
105: 	case_insensitive_map_t<unique_ptr<MappingValue>> mapping;
106: 	//! The set of catalog entries
107: 	unordered_map<idx_t, unique_ptr<CatalogEntry>> entries;
108: 	//! The current catalog entry index
109: 	idx_t current_entry = 0;
110: 	//! The generator used to generate default internal entries
111: 	unique_ptr<DefaultGenerator> defaults;
112: };
113: 
114: } // namespace duckdb
[end of src/include/duckdb/catalog/catalog_set.hpp]
[start of src/include/duckdb/common/string_util.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/string_util.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/constants.hpp"
12: #include "duckdb/common/exception.hpp"
13: #include "duckdb/common/vector.hpp"
14: 
15: namespace duckdb {
16: /**
17:  * String Utility Functions
18:  * Note that these are not the most efficient implementations (i.e., they copy
19:  * memory) and therefore they should only be used for debug messages and other
20:  * such things.
21:  */
22: class StringUtil {
23: public:
24: 	static bool CharacterIsSpace(char c) {
25: 		return c == ' ' || c == '\t' || c == '\n' || c == '\v' || c == '\f' || c == '\r';
26: 	}
27: 	static bool CharacterIsNewline(char c) {
28: 		return c == '\n' || c == '\r';
29: 	}
30: 	static bool CharacterIsDigit(char c) {
31: 		return c >= '0' && c <= '9';
32: 	}
33: 	static char CharacterToLower(char c) {
34: 		if (c >= 'A' && c <= 'Z') {
35: 			return c - ('A' - 'a');
36: 		}
37: 		return c;
38: 	}
39: 
40: 	//! Returns true if the needle string exists in the haystack
41: 	static bool Contains(const string &haystack, const string &needle);
42: 
43: 	//! Returns true if the target string starts with the given prefix
44: 	static bool StartsWith(string str, string prefix);
45: 
46: 	//! Returns true if the target string <b>ends</b> with the given suffix.
47: 	static bool EndsWith(const string &str, const string &suffix);
48: 
49: 	//! Repeat a string multiple times
50: 	static string Repeat(const string &str, const idx_t n);
51: 
52: 	//! Split the input string based on newline char
53: 	static vector<string> Split(const string &str, char delimiter);
54: 
55: 	//! Join multiple strings into one string. Components are concatenated by the given separator
56: 	static string Join(const vector<string> &input, const string &separator);
57: 
58: 	//! Join multiple items of container with given size, transformed to string
59: 	//! using function, into one string using the given separator
60: 	template <typename C, typename S, typename Func>
61: 	static string Join(const C &input, S count, const string &separator, Func f) {
62: 		// The result
63: 		std::string result;
64: 
65: 		// If the input isn't empty, append the first element. We do this so we
66: 		// don't need to introduce an if into the loop.
67: 		if (count > 0) {
68: 			result += f(input[0]);
69: 		}
70: 
71: 		// Append the remaining input components, after the first
72: 		for (size_t i = 1; i < count; i++) {
73: 			result += separator + f(input[i]);
74: 		}
75: 
76: 		return result;
77: 	}
78: 
79: 	//! Return a string that formats the give number of bytes
80: 	static string BytesToHumanReadableString(idx_t bytes);
81: 
82: 	//! Convert a string to uppercase
83: 	static string Upper(const string &str);
84: 
85: 	//! Convert a string to lowercase
86: 	static string Lower(const string &str);
87: 
88: 	//! Format a string using printf semantics
89: 	template <typename... Args>
90: 	static string Format(const string fmt_str, Args... params) {
91: 		return Exception::ConstructMessage(fmt_str, params...);
92: 	}
93: 
94: 	//! Split the input string into a vector of strings based on the split string
95: 	static vector<string> Split(const string &input, const string &split);
96: 
97: 	//! Remove the whitespace char in the left end of the string
98: 	static void LTrim(string &str);
99: 	//! Remove the whitespace char in the right end of the string
100: 	static void RTrim(string &str);
101: 	//! Remove the whitespace char in the left and right end of the string
102: 	static void Trim(string &str);
103: 
104: 	static string Replace(string source, const string &from, const string &to);
105: 
106: 	//! Get the levenshtein distance from two strings
107: 	static idx_t LevenshteinDistance(const string &s1, const string &s2);
108: 
109: 	//! Get the top-n strings (sorted by the given score distance) from a set of scores.
110: 	//! At least one entry is returned (if there is one).
111: 	//! Strings are only returned if they have a score less than the threshold.
112: 	static vector<string> TopNStrings(vector<std::pair<string, idx_t>> scores, idx_t n = 5, idx_t threshold = 5);
113: 	//! Computes the levenshtein distance of each string in strings, and compares it to target, then returns TopNStrings
114: 	//! with the given params.
115: 	static vector<string> TopNLevenshtein(const vector<string> &strings, const string &target, idx_t n = 5,
116: 	                                      idx_t threshold = 5);
117: 	static string CandidatesMessage(const vector<string> &candidates, const string &candidate = "Candidate bindings");
118: };
119: } // namespace duckdb
[end of src/include/duckdb/common/string_util.hpp]
[start of src/include/duckdb/execution/operator/helper/physical_set.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/helper/physical_set.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/enums/set_scope.h"
12: #include "duckdb/execution/physical_operator.hpp"
13: #include "duckdb/parser/parsed_data/vacuum_info.hpp"
14: 
15: namespace duckdb {
16: 
17: //! PhysicalSet represents a SET operation (e.g. SET a = 42)
18: class PhysicalSet : public PhysicalOperator {
19: public:
20: 	PhysicalSet(std::string name_p, Value value_p, SetScope scope_p, idx_t estimated_cardinality)
21: 	    : PhysicalOperator(PhysicalOperatorType::SET, {LogicalType::BOOLEAN}, estimated_cardinality), name(name_p),
22: 	      value(value_p), scope(scope_p) {
23: 	}
24: 
25: public:
26: 	void GetChunkInternal(ExecutionContext &context, DataChunk &chunk, PhysicalOperatorState *state) const override;
27: 
28: public:
29: 	std::string name;
30: 	Value value;
31: 	SetScope scope;
32: };
33: 
34: } // namespace duckdb
[end of src/include/duckdb/execution/operator/helper/physical_set.hpp]
[start of src/include/duckdb/main/client_context.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/main/client_context.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
12: #include "duckdb/catalog/catalog_set.hpp"
13: #include "duckdb/common/deque.hpp"
14: #include "duckdb/common/enums/output_type.hpp"
15: #include "duckdb/common/pair.hpp"
16: #include "duckdb/common/progress_bar.hpp"
17: #include "duckdb/common/unordered_set.hpp"
18: #include "duckdb/common/winapi.hpp"
19: #include "duckdb/execution/executor.hpp"
20: #include "duckdb/main/prepared_statement.hpp"
21: #include "duckdb/main/stream_query_result.hpp"
22: #include "duckdb/main/table_description.hpp"
23: #include "duckdb/transaction/transaction_context.hpp"
24: #include <random>
25: #include "duckdb/common/atomic.hpp"
26: 
27: namespace duckdb {
28: class Appender;
29: class Catalog;
30: class ChunkCollection;
31: class DatabaseInstance;
32: class FileOpener;
33: class LogicalOperator;
34: class PreparedStatementData;
35: class Relation;
36: class BufferedFileWriter;
37: class QueryProfiler;
38: class QueryProfilerHistory;
39: class ClientContextLock;
40: struct CreateScalarFunctionInfo;
41: class ScalarFunctionCatalogEntry;
42: 
43: //! The ClientContext holds information relevant to the current client session
44: //! during execution
45: class ClientContext : public std::enable_shared_from_this<ClientContext> {
46: 	friend class TransactionManager;
47: 
48: public:
49: 	DUCKDB_API explicit ClientContext(shared_ptr<DatabaseInstance> db);
50: 	DUCKDB_API ~ClientContext();
51: 	//! Query profiler
52: 	unique_ptr<QueryProfiler> profiler;
53: 	//! QueryProfiler History
54: 	unique_ptr<QueryProfilerHistory> query_profiler_history;
55: 	//! The database that this client is connected to
56: 	shared_ptr<DatabaseInstance> db;
57: 	//! Data for the currently running transaction
58: 	TransactionContext transaction;
59: 	//! Whether or not the query is interrupted
60: 	atomic<bool> interrupted;
61: 	//! The current query being executed by the client context
62: 	string query;
63: 
64: 	//! The query executor
65: 	Executor executor;
66: 
67: 	//! The Progress Bar
68: 	unique_ptr<ProgressBar> progress_bar;
69: 	//! If the progress bar is enabled or not.
70: 	bool enable_progress_bar = false;
71: 	//! If the print of the progress bar is enabled
72: 	bool print_progress_bar = true;
73: 	//! The wait time before showing the progress bar
74: 	int wait_time = 2000;
75: 
76: 	unique_ptr<SchemaCatalogEntry> temporary_objects;
77: 	unordered_map<string, shared_ptr<PreparedStatementData>> prepared_statements;
78: 
79: 	unordered_map<string, Value> set_variables;
80: 
81: 	// Whether or not aggressive query verification is enabled
82: 	bool query_verification_enabled = false;
83: 	//! Enable the running of optimizers
84: 	bool enable_optimizer = true;
85: 	//! Force parallelism of small tables, used for testing
86: 	bool verify_parallelism = false;
87: 	//! Force index join independent of table cardinality, used for testing
88: 	bool force_index_join = false;
89: 	//! Force out-of-core computation for operators that support it, used for testing
90: 	bool force_external = false;
91: 	//! Maximum bits allowed for using a perfect hash table (i.e. the perfect HT can hold up to 2^perfect_ht_threshold
92: 	//! elements)
93: 	idx_t perfect_ht_threshold = 12;
94: 	//! The writer used to log queries (if logging is enabled)
95: 	unique_ptr<BufferedFileWriter> log_query_writer;
96: 	//! The explain output type used when none is specified (default: PHYSICAL_ONLY)
97: 	ExplainOutputType explain_output_type = ExplainOutputType::PHYSICAL_ONLY;
98: 	//! The random generator used by random(). Its seed value can be set by setseed().
99: 	std::mt19937 random_engine;
100: 
101: 	//! The schema search path, in order by which entries are searched if no schema entry is provided
102: 	vector<string> catalog_search_path = {TEMP_SCHEMA, DEFAULT_SCHEMA, "pg_catalog"};
103: 
104: 	unique_ptr<FileOpener> file_opener;
105: 
106: public:
107: 	DUCKDB_API Transaction &ActiveTransaction() {
108: 		return transaction.ActiveTransaction();
109: 	}
110: 
111: 	//! Interrupt execution of a query
112: 	DUCKDB_API void Interrupt();
113: 	//! Enable query profiling
114: 	DUCKDB_API void EnableProfiling();
115: 	//! Disable query profiling
116: 	DUCKDB_API void DisableProfiling();
117: 
118: 	//! Issue a query, returning a QueryResult. The QueryResult can be either a StreamQueryResult or a
119: 	//! MaterializedQueryResult. The StreamQueryResult will only be returned in the case of a successful SELECT
120: 	//! statement.
121: 	DUCKDB_API unique_ptr<QueryResult> Query(const string &query, bool allow_stream_result);
122: 	DUCKDB_API unique_ptr<QueryResult> Query(unique_ptr<SQLStatement> statement, bool allow_stream_result);
123: 	//! Fetch a query from the current result set (if any)
124: 	DUCKDB_API unique_ptr<DataChunk> Fetch();
125: 	//! Cleanup the result set (if any).
126: 	DUCKDB_API void Cleanup();
127: 	//! Destroy the client context
128: 	DUCKDB_API void Destroy();
129: 
130: 	//! Get the table info of a specific table, or nullptr if it cannot be found
131: 	DUCKDB_API unique_ptr<TableDescription> TableInfo(const string &schema_name, const string &table_name);
132: 	//! Appends a DataChunk to the specified table. Returns whether or not the append was successful.
133: 	DUCKDB_API void Append(TableDescription &description, ChunkCollection &collection);
134: 	//! Try to bind a relation in the current client context; either throws an exception or fills the result_columns
135: 	//! list with the set of returned columns
136: 	DUCKDB_API void TryBindRelation(Relation &relation, vector<ColumnDefinition> &result_columns);
137: 
138: 	//! Execute a relation
139: 	DUCKDB_API unique_ptr<QueryResult> Execute(const shared_ptr<Relation> &relation);
140: 
141: 	//! Prepare a query
142: 	DUCKDB_API unique_ptr<PreparedStatement> Prepare(const string &query);
143: 	//! Directly prepare a SQL statement
144: 	DUCKDB_API unique_ptr<PreparedStatement> Prepare(unique_ptr<SQLStatement> statement);
145: 
146: 	//! Execute a prepared statement with the given name and set of parameters
147: 	//! It is possible that the prepared statement will be re-bound. This will generally happen if the catalog is
148: 	//! modified in between the prepared statement being bound and the prepared statement being run.
149: 	DUCKDB_API unique_ptr<QueryResult> Execute(const string &query, shared_ptr<PreparedStatementData> &prepared,
150: 	                                           vector<Value> &values, bool allow_stream_result = true);
151: 
152: 	//! Gets current percentage of the query's progress, returns 0 in case the progress bar is disabled.
153: 	int GetProgress();
154: 
155: 	//! Register function in the temporary schema
156: 	DUCKDB_API void RegisterFunction(CreateFunctionInfo *info);
157: 
158: 	//! Parse statements from a query
159: 	DUCKDB_API vector<unique_ptr<SQLStatement>> ParseStatements(const string &query);
160: 	//! Extract the logical plan of a query
161: 	DUCKDB_API unique_ptr<LogicalOperator> ExtractPlan(const string &query);
162: 	void HandlePragmaStatements(vector<unique_ptr<SQLStatement>> &statements);
163: 
164: 	//! Runs a function with a valid transaction context, potentially starting a transaction if the context is in auto
165: 	//! commit mode.
166: 	DUCKDB_API void RunFunctionInTransaction(const std::function<void(void)> &fun,
167: 	                                         bool requires_valid_transaction = true);
168: 	//! Same as RunFunctionInTransaction, but does not obtain a lock on the client context or check for validation
169: 	DUCKDB_API void RunFunctionInTransactionInternal(ClientContextLock &lock, const std::function<void(void)> &fun,
170: 	                                                 bool requires_valid_transaction = true);
171: 
172: 	//! Equivalent to CURRENT_SETTING(key) SQL function.
173: 	DUCKDB_API bool TryGetCurrentSetting(const std::string &key, Value &result);
174: 
175: private:
176: 	//! Parse statements from a query
177: 	vector<unique_ptr<SQLStatement>> ParseStatementsInternal(ClientContextLock &lock, const string &query);
178: 	//! Perform aggressive query verification of a SELECT statement. Only called when query_verification_enabled is
179: 	//! true.
180: 	string VerifyQuery(ClientContextLock &lock, const string &query, unique_ptr<SQLStatement> statement);
181: 
182: 	void InitialCleanup(ClientContextLock &lock);
183: 	//! Internal clean up, does not lock. Caller must hold the context_lock.
184: 	void CleanupInternal(ClientContextLock &lock);
185: 	string FinalizeQuery(ClientContextLock &lock, bool success);
186: 	//! Internal fetch, does not lock. Caller must hold the context_lock.
187: 	unique_ptr<DataChunk> FetchInternal(ClientContextLock &lock);
188: 	//! Internally execute a set of SQL statement. Caller must hold the context_lock.
189: 	unique_ptr<QueryResult> RunStatements(ClientContextLock &lock, const string &query,
190: 	                                      vector<unique_ptr<SQLStatement>> &statements, bool allow_stream_result);
191: 	//! Internally prepare and execute a prepared SQL statement. Caller must hold the context_lock.
192: 	unique_ptr<QueryResult> RunStatement(ClientContextLock &lock, const string &query,
193: 	                                     unique_ptr<SQLStatement> statement, bool allow_stream_result);
194: 	unique_ptr<QueryResult> RunStatementOrPreparedStatement(ClientContextLock &lock, const string &query,
195: 	                                                        unique_ptr<SQLStatement> statement,
196: 	                                                        shared_ptr<PreparedStatementData> &prepared,
197: 	                                                        vector<Value> *values, bool allow_stream_result);
198: 
199: 	//! Internally prepare a SQL statement. Caller must hold the context_lock.
200: 	shared_ptr<PreparedStatementData> CreatePreparedStatement(ClientContextLock &lock, const string &query,
201: 	                                                          unique_ptr<SQLStatement> statement);
202: 	//! Internally execute a prepared SQL statement. Caller must hold the context_lock.
203: 	unique_ptr<QueryResult> ExecutePreparedStatement(ClientContextLock &lock, const string &query,
204: 	                                                 shared_ptr<PreparedStatementData> statement,
205: 	                                                 vector<Value> bound_values, bool allow_stream_result);
206: 	//! Call CreatePreparedStatement() and ExecutePreparedStatement() without any bound values
207: 	unique_ptr<QueryResult> RunStatementInternal(ClientContextLock &lock, const string &query,
208: 	                                             unique_ptr<SQLStatement> statement, bool allow_stream_result);
209: 	unique_ptr<PreparedStatement> PrepareInternal(ClientContextLock &lock, unique_ptr<SQLStatement> statement);
210: 	void LogQueryInternal(ClientContextLock &lock, const string &query);
211: 
212: 	unique_ptr<ClientContextLock> LockContext();
213: 
214: 	bool UpdateFunctionInfoFromEntry(ScalarFunctionCatalogEntry *existing_function, CreateScalarFunctionInfo *new_info);
215: 
216: private:
217: 	//! The currently opened StreamQueryResult (if any)
218: 	StreamQueryResult *open_result = nullptr;
219: 	//! Lock on using the ClientContext in parallel
220: 	mutex context_lock;
221: };
222: 
223: } // namespace duckdb
[end of src/include/duckdb/main/client_context.hpp]
[start of src/include/duckdb/main/config.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/main/config.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/allocator.hpp"
12: #include "duckdb/common/common.hpp"
13: #include "duckdb/common/enums/order_type.hpp"
14: #include "duckdb/common/file_system.hpp"
15: #include "duckdb/common/winapi.hpp"
16: #include "duckdb/common/types/value.hpp"
17: #include "duckdb/common/vector.hpp"
18: #include "duckdb/function/replacement_scan.hpp"
19: #include "duckdb/common/set.hpp"
20: #include "duckdb/common/enums/compression_type.hpp"
21: #include "duckdb/common/enums/optimizer_type.hpp"
22: 
23: namespace duckdb {
24: class ClientContext;
25: class TableFunctionRef;
26: class CompressionFunction;
27: 
28: struct CompressionFunctionSet;
29: 
30: enum class AccessMode : uint8_t { UNDEFINED = 0, AUTOMATIC = 1, READ_ONLY = 2, READ_WRITE = 3 };
31: 
32: enum class CheckpointAbort : uint8_t {
33: 	NO_ABORT = 0,
34: 	DEBUG_ABORT_BEFORE_TRUNCATE = 1,
35: 	DEBUG_ABORT_BEFORE_HEADER = 2,
36: 	DEBUG_ABORT_AFTER_FREE_LIST_WRITE = 3
37: };
38: 
39: enum class ConfigurationOptionType : uint32_t {
40: 	INVALID = 0,
41: 	ACCESS_MODE,
42: 	DEFAULT_ORDER_TYPE,
43: 	DEFAULT_NULL_ORDER,
44: 	ENABLE_EXTERNAL_ACCESS,
45: 	ENABLE_OBJECT_CACHE,
46: 	MAXIMUM_MEMORY,
47: 	THREADS
48: };
49: 
50: struct ConfigurationOption {
51: 	ConfigurationOptionType type;
52: 	const char *name;
53: 	const char *description;
54: 	LogicalTypeId parameter_type;
55: };
56: 
57: // this is optional and only used in tests at the moment
58: struct DBConfig {
59: 	friend class DatabaseInstance;
60: 	friend class StorageManager;
61: 
62: public:
63: 	DUCKDB_API DBConfig();
64: 	DUCKDB_API ~DBConfig();
65: 
66: 	//! Access mode of the database (AUTOMATIC, READ_ONLY or READ_WRITE)
67: 	AccessMode access_mode = AccessMode::AUTOMATIC;
68: 	//! The allocator used by the system
69: 	Allocator allocator;
70: 	// Checkpoint when WAL reaches this size (default: 16MB)
71: 	idx_t checkpoint_wal_size = 1 << 24;
72: 	//! Whether or not to use Direct IO, bypassing operating system buffers
73: 	bool use_direct_io = false;
74: 	//! The FileSystem to use, can be overwritten to allow for injecting custom file systems for testing purposes (e.g.
75: 	//! RamFS or something similar)
76: 	unique_ptr<FileSystem> file_system;
77: 	//! The maximum memory used by the database system (in bytes). Default: 80% of System available memory
78: 	idx_t maximum_memory = (idx_t)-1;
79: 	//! The maximum amount of CPU threads used by the database system. Default: all available.
80: 	idx_t maximum_threads = (idx_t)-1;
81: 	//! Whether or not to create and use a temporary directory to store intermediates that do not fit in memory
82: 	bool use_temporary_directory = true;
83: 	//! Directory to store temporary structures that do not fit in memory
84: 	string temporary_directory;
85: 	//! The collation type of the database
86: 	string collation = string();
87: 	//! The order type used when none is specified (default: ASC)
88: 	OrderType default_order_type = OrderType::ASCENDING;
89: 	//! Null ordering used when none is specified (default: NULLS FIRST)
90: 	OrderByNullType default_null_order = OrderByNullType::NULLS_FIRST;
91: 	//! enable COPY and related commands
92: 	bool enable_external_access = true;
93: 	//! Whether or not object cache is used
94: 	bool object_cache_enable = false;
95: 	//! Database configuration variables as controlled by SET
96: 	unordered_map<std::string, Value> set_variables;
97: 	//! Force checkpoint when CHECKPOINT is called or on shutdown, even if no changes have been made
98: 	bool force_checkpoint = false;
99: 	//! Run a checkpoint on successful shutdown and delete the WAL, to leave only a single database file behind
100: 	bool checkpoint_on_shutdown = true;
101: 	//! Debug flag that decides when a checkpoing should be aborted. Only used for testing purposes.
102: 	CheckpointAbort checkpoint_abort = CheckpointAbort::NO_ABORT;
103: 	//! Replacement table scans are automatically attempted when a table name cannot be found in the schema
104: 	vector<ReplacementScan> replacement_scans;
105: 	//! Initialize the database with the standard set of DuckDB functions
106: 	//! You should probably not touch this unless you know what you are doing
107: 	bool initialize_default_database = true;
108: 	//! The set of disabled optimizers (default empty)
109: 	set<OptimizerType> disabled_optimizers;
110: 	//! Force a specific compression method to be used when checkpointing (if available)
111: 	CompressionType force_compression = CompressionType::COMPRESSION_INVALID;
112: 	//! Debug flag that adds additional (unnecessary) free_list blocks to the storage
113: 	bool debug_many_free_list_blocks = false;
114: 
115: public:
116: 	DUCKDB_API static DBConfig &GetConfig(ClientContext &context);
117: 	DUCKDB_API static DBConfig &GetConfig(DatabaseInstance &db);
118: 	DUCKDB_API static vector<ConfigurationOption> GetOptions();
119: 	DUCKDB_API static idx_t GetOptionCount();
120: 
121: 	//! Fetch an option by index. Returns a pointer to the option, or nullptr if out of range
122: 	DUCKDB_API static ConfigurationOption *GetOptionByIndex(idx_t index);
123: 	//! Fetch an option by name. Returns a pointer to the option, or nullptr if none exists.
124: 	DUCKDB_API static ConfigurationOption *GetOptionByName(const string &name);
125: 
126: 	DUCKDB_API void SetOption(const ConfigurationOption &option, const Value &value);
127: 
128: 	DUCKDB_API static idx_t ParseMemoryLimit(const string &arg);
129: 
130: 	//! Return the list of possible compression functions for the specific physical type
131: 	DUCKDB_API vector<CompressionFunction *> GetCompressionFunctions(PhysicalType data_type);
132: 	//! Return the compression function for the specified compression type/physical type combo
133: 	DUCKDB_API CompressionFunction *GetCompressionFunction(CompressionType type, PhysicalType data_type);
134: 
135: private:
136: 	unique_ptr<CompressionFunctionSet> compression_functions;
137: };
138: 
139: } // namespace duckdb
[end of src/include/duckdb/main/config.hpp]
[start of src/include/duckdb/parser/parsed_data/create_function_info.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/parser/parsed_data/create_function_info.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/parser/parsed_data/create_info.hpp"
12: #include "duckdb/function/function.hpp"
13: 
14: namespace duckdb {
15: 
16: struct CreateFunctionInfo : public CreateInfo {
17: 	explicit CreateFunctionInfo(CatalogType type) : CreateInfo(type) {
18: 		D_ASSERT(type == CatalogType::SCALAR_FUNCTION_ENTRY || type == CatalogType::AGGREGATE_FUNCTION_ENTRY ||
19: 		         type == CatalogType::TABLE_FUNCTION_ENTRY || type == CatalogType::PRAGMA_FUNCTION_ENTRY ||
20: 		         type == CatalogType::MACRO_ENTRY);
21: 	}
22: 
23: 	//! Function name
24: 	string name;
25: };
26: 
27: } // namespace duckdb
[end of src/include/duckdb/parser/parsed_data/create_function_info.hpp]
[start of src/include/duckdb/parser/parsed_data/create_macro_info.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/parser/parsed_data/create_macro_info.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/parser/parsed_data/create_function_info.hpp"
12: #include "duckdb/function/macro_function.hpp"
13: 
14: namespace duckdb {
15: 
16: struct CreateMacroInfo : public CreateFunctionInfo {
17: 	CreateMacroInfo() : CreateFunctionInfo(CatalogType::MACRO_ENTRY) {
18: 	}
19: 
20: 	unique_ptr<MacroFunction> function;
21: 
22: public:
23: 	unique_ptr<CreateInfo> Copy() const override {
24: 		auto result = make_unique<CreateMacroInfo>();
25: 		result->function = function->Copy();
26: 		result->name = name;
27: 		CopyProperties(*result);
28: 		return move(result);
29: 	}
30: };
31: 
32: } // namespace duckdb
[end of src/include/duckdb/parser/parsed_data/create_macro_info.hpp]
[start of src/include/duckdb/parser/parsed_data/create_sequence_info.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/parser/parsed_data/create_sequence_info.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/parser/parsed_data/create_info.hpp"
12: #include "duckdb/common/limits.hpp"
13: 
14: namespace duckdb {
15: 
16: struct CreateSequenceInfo : public CreateInfo {
17: 	CreateSequenceInfo()
18: 	    : CreateInfo(CatalogType::SEQUENCE_ENTRY), name(string()), usage_count(0), increment(1), min_value(1),
19: 	      max_value(NumericLimits<int64_t>::Maximum()), start_value(1), cycle(false) {
20: 	}
21: 
22: 	//! Sequence name to create
23: 	string name;
24: 	//! Usage count of the sequence
25: 	uint64_t usage_count;
26: 	//! The increment value
27: 	int64_t increment;
28: 	//! The minimum value of the sequence
29: 	int64_t min_value;
30: 	//! The maximum value of the sequence
31: 	int64_t max_value;
32: 	//! The start value of the sequence
33: 	int64_t start_value;
34: 	//! Whether or not the sequence cycles
35: 	bool cycle;
36: 
37: public:
38: 	unique_ptr<CreateInfo> Copy() const override {
39: 		auto result = make_unique<CreateSequenceInfo>();
40: 		CopyProperties(*result);
41: 		result->name = name;
42: 		result->usage_count = usage_count;
43: 		result->increment = increment;
44: 		result->min_value = min_value;
45: 		result->max_value = max_value;
46: 		result->start_value = start_value;
47: 		result->cycle = cycle;
48: 		return move(result);
49: 	}
50: };
51: 
52: } // namespace duckdb
[end of src/include/duckdb/parser/parsed_data/create_sequence_info.hpp]
[start of src/include/duckdb/parser/parsed_data/create_view_info.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/parser/parsed_data/create_view_info.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/parser/parsed_data/create_info.hpp"
12: #include "duckdb/parser/statement/select_statement.hpp"
13: 
14: namespace duckdb {
15: 
16: struct CreateViewInfo : public CreateInfo {
17: 	CreateViewInfo() : CreateInfo(CatalogType::VIEW_ENTRY) {
18: 	}
19: 	CreateViewInfo(string schema, string view_name)
20: 	    : CreateInfo(CatalogType::VIEW_ENTRY, schema), view_name(view_name) {
21: 	}
22: 
23: 	//! Table name to insert to
24: 	string view_name;
25: 	//! Aliases of the view
26: 	vector<string> aliases;
27: 	//! Return types
28: 	vector<LogicalType> types;
29: 	//! The SelectStatement of the view
30: 	unique_ptr<SelectStatement> query;
31: 
32: public:
33: 	unique_ptr<CreateInfo> Copy() const override {
34: 		auto result = make_unique<CreateViewInfo>(schema, view_name);
35: 		CopyProperties(*result);
36: 		result->aliases = aliases;
37: 		result->types = types;
38: 		result->query = unique_ptr_cast<SQLStatement, SelectStatement>(query->Copy());
39: 		return move(result);
40: 	}
41: };
42: 
43: } // namespace duckdb
[end of src/include/duckdb/parser/parsed_data/create_view_info.hpp]
[start of src/main/client_context.cpp]
1: #include "duckdb/main/client_context.hpp"
2: 
3: #include "duckdb/main/client_context_file_opener.hpp"
4: #include "duckdb/main/query_profiler.hpp"
5: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
6: #include "duckdb/catalog/catalog_entry/scalar_function_catalog_entry.hpp"
7: #include "duckdb/common/serializer/buffered_deserializer.hpp"
8: #include "duckdb/common/serializer/buffered_serializer.hpp"
9: #include "duckdb/execution/physical_plan_generator.hpp"
10: #include "duckdb/main/database.hpp"
11: #include "duckdb/main/materialized_query_result.hpp"
12: #include "duckdb/main/query_result.hpp"
13: #include "duckdb/main/stream_query_result.hpp"
14: #include "duckdb/optimizer/optimizer.hpp"
15: #include "duckdb/parser/parser.hpp"
16: #include "duckdb/parser/expression/constant_expression.hpp"
17: #include "duckdb/parser/parsed_data/create_function_info.hpp"
18: #include "duckdb/parser/statement/drop_statement.hpp"
19: #include "duckdb/parser/statement/execute_statement.hpp"
20: #include "duckdb/parser/statement/explain_statement.hpp"
21: #include "duckdb/parser/statement/prepare_statement.hpp"
22: #include "duckdb/parser/statement/select_statement.hpp"
23: #include "duckdb/planner/operator/logical_execute.hpp"
24: #include "duckdb/planner/planner.hpp"
25: #include "duckdb/transaction/transaction_manager.hpp"
26: #include "duckdb/transaction/transaction.hpp"
27: #include "duckdb/storage/data_table.hpp"
28: #include "duckdb/main/appender.hpp"
29: #include "duckdb/main/relation.hpp"
30: #include "duckdb/parser/statement/relation_statement.hpp"
31: #include "duckdb/parallel/task_scheduler.hpp"
32: #include "duckdb/common/serializer/buffered_file_writer.hpp"
33: #include "duckdb/planner/pragma_handler.hpp"
34: #include "duckdb/common/to_string.hpp"
35: #include "duckdb/common/file_system.hpp"
36: #include "duckdb/execution/column_binding_resolver.hpp"
37: 
38: namespace duckdb {
39: 
40: class ClientContextLock {
41: public:
42: 	explicit ClientContextLock(mutex &context_lock) : client_guard(context_lock) {
43: 	}
44: 
45: 	~ClientContextLock() {
46: 	}
47: 
48: private:
49: 	lock_guard<mutex> client_guard;
50: };
51: 
52: ClientContext::ClientContext(shared_ptr<DatabaseInstance> database)
53:     : profiler(make_unique<QueryProfiler>()), query_profiler_history(make_unique<QueryProfilerHistory>()),
54:       db(move(database)), transaction(db->GetTransactionManager(), *this), interrupted(false), executor(*this),
55:       temporary_objects(make_unique<SchemaCatalogEntry>(&db->GetCatalog(), TEMP_SCHEMA, true)),
56:       file_opener(make_unique<ClientContextFileOpener>(*this)), open_result(nullptr) {
57: 	std::random_device rd;
58: 	random_engine.seed(rd());
59: 
60: 	progress_bar = make_unique<ProgressBar>(&executor, wait_time);
61: }
62: 
63: ClientContext::~ClientContext() {
64: 	if (std::uncaught_exception()) {
65: 		return;
66: 	}
67: 	// destroy the client context and rollback if there is an active transaction
68: 	// but only if we are not destroying this client context as part of an exception stack unwind
69: 	Destroy();
70: }
71: 
72: unique_ptr<ClientContextLock> ClientContext::LockContext() {
73: 	return make_unique<ClientContextLock>(context_lock);
74: }
75: 
76: void ClientContext::Destroy() {
77: 	auto lock = LockContext();
78: 	if (transaction.HasActiveTransaction()) {
79: 		ActiveTransaction().active_query = MAXIMUM_QUERY_ID;
80: 		if (!transaction.IsAutoCommit()) {
81: 			transaction.Rollback();
82: 		}
83: 	}
84: 	CleanupInternal(*lock);
85: }
86: 
87: void ClientContext::Cleanup() {
88: 	auto lock = LockContext();
89: 	CleanupInternal(*lock);
90: }
91: 
92: unique_ptr<DataChunk> ClientContext::Fetch() {
93: 	auto lock = LockContext();
94: 	if (!open_result) {
95: 		throw InternalException("Fetch was called, but there is no open result (or the result was previously closed)");
96: 	}
97: 	try {
98: 		// fetch the chunk and return it
99: 		auto chunk = FetchInternal(*lock);
100: 		return chunk;
101: 	} catch (std::exception &ex) {
102: 		open_result->error = ex.what();
103: 	} catch (...) { // LCOV_EXCL_START
104: 		open_result->error = "Unhandled exception in Fetch";
105: 	} // LCOV_EXCL_STOP
106: 	open_result->success = false;
107: 	CleanupInternal(*lock);
108: 	return nullptr;
109: }
110: 
111: string ClientContext::FinalizeQuery(ClientContextLock &lock, bool success) {
112: 	profiler->EndQuery();
113: 	executor.Reset();
114: 
115: 	string error;
116: 	if (transaction.HasActiveTransaction()) {
117: 		ActiveTransaction().active_query = MAXIMUM_QUERY_ID;
118: 		// Move the query profiler into the history
119: 		auto &prev_profilers = query_profiler_history->GetPrevProfilers();
120: 		prev_profilers.emplace_back(transaction.ActiveTransaction().active_query, move(profiler));
121: 		// Reinitialize the query profiler
122: 		profiler = make_unique<QueryProfiler>();
123: 		// Propagate settings of the saved query into the new profiler.
124: 		profiler->Propagate(*prev_profilers.back().second);
125: 		if (prev_profilers.size() >= query_profiler_history->GetPrevProfilersSize()) {
126: 			prev_profilers.pop_front();
127: 		}
128: 		try {
129: 			if (transaction.IsAutoCommit()) {
130: 				if (success) {
131: 					// query was successful: commit
132: 					transaction.Commit();
133: 				} else {
134: 					// query was unsuccessful: rollback
135: 					transaction.Rollback();
136: 				}
137: 			}
138: 		} catch (std::exception &ex) {
139: 			error = ex.what();
140: 		} catch (...) { // LCOV_EXCL_START
141: 			error = "Unhandled exception!";
142: 		} // LCOV_EXCL_STOP
143: 	}
144: 	return error;
145: }
146: 
147: void ClientContext::CleanupInternal(ClientContextLock &lock) {
148: 	if (!open_result) {
149: 		// no result currently open
150: 		return;
151: 	}
152: 
153: 	auto error = FinalizeQuery(lock, open_result->success);
154: 	if (open_result->success) {
155: 		// if an error occurred while committing report it in the result
156: 		open_result->error = error;
157: 		open_result->success = error.empty();
158: 	}
159: 
160: 	open_result->is_open = false;
161: 	open_result = nullptr;
162: 
163: 	this->query = string();
164: }
165: 
166: unique_ptr<DataChunk> ClientContext::FetchInternal(ClientContextLock &) {
167: 	return executor.FetchChunk();
168: }
169: 
170: shared_ptr<PreparedStatementData> ClientContext::CreatePreparedStatement(ClientContextLock &lock, const string &query,
171:                                                                          unique_ptr<SQLStatement> statement) {
172: 	StatementType statement_type = statement->type;
173: 	auto result = make_shared<PreparedStatementData>(statement_type);
174: 
175: 	profiler->StartPhase("planner");
176: 	Planner planner(*this);
177: 	planner.CreatePlan(move(statement));
178: 	D_ASSERT(planner.plan);
179: 	profiler->EndPhase();
180: 
181: 	auto plan = move(planner.plan);
182: #ifdef DEBUG
183: 	plan->Verify();
184: #endif
185: 	// extract the result column names from the plan
186: 	result->read_only = planner.read_only;
187: 	result->requires_valid_transaction = planner.requires_valid_transaction;
188: 	result->allow_stream_result = planner.allow_stream_result;
189: 	result->names = planner.names;
190: 	result->types = planner.types;
191: 	result->value_map = move(planner.value_map);
192: 	result->catalog_version = Transaction::GetTransaction(*this).catalog_version;
193: 
194: 	if (enable_optimizer) {
195: 		profiler->StartPhase("optimizer");
196: 		Optimizer optimizer(*planner.binder, *this);
197: 		plan = optimizer.Optimize(move(plan));
198: 		D_ASSERT(plan);
199: 		profiler->EndPhase();
200: 
201: #ifdef DEBUG
202: 		plan->Verify();
203: #endif
204: 	}
205: 
206: 	profiler->StartPhase("physical_planner");
207: 	// now convert logical query plan into a physical query plan
208: 	PhysicalPlanGenerator physical_planner(*this);
209: 	auto physical_plan = physical_planner.CreatePlan(move(plan));
210: 	profiler->EndPhase();
211: 
212: #ifdef DEBUG
213: 	D_ASSERT(!physical_plan->ToString().empty());
214: #endif
215: 	result->plan = move(physical_plan);
216: 	return result;
217: }
218: 
219: int ClientContext::GetProgress() {
220: 	D_ASSERT(progress_bar);
221: 	return progress_bar->GetCurrentPercentage();
222: }
223: 
224: unique_ptr<QueryResult> ClientContext::ExecutePreparedStatement(ClientContextLock &lock, const string &query,
225:                                                                 shared_ptr<PreparedStatementData> statement_p,
226:                                                                 vector<Value> bound_values, bool allow_stream_result) {
227: 	auto &statement = *statement_p;
228: 	if (ActiveTransaction().IsInvalidated() && statement.requires_valid_transaction) {
229: 		throw Exception("Current transaction is aborted (please ROLLBACK)");
230: 	}
231: 	auto &config = DBConfig::GetConfig(*this);
232: 	if (config.access_mode == AccessMode::READ_ONLY && !statement.read_only) {
233: 		throw Exception(StringUtil::Format("Cannot execute statement of type \"%s\" in read-only mode!",
234: 		                                   StatementTypeToString(statement.statement_type)));
235: 	}
236: 
237: 	// bind the bound values before execution
238: 	statement.Bind(move(bound_values));
239: 
240: 	bool create_stream_result = statement.allow_stream_result && allow_stream_result;
241: 	if (enable_progress_bar) {
242: 		progress_bar->Initialize(wait_time);
243: 		progress_bar->Start();
244: 	}
245: 	// store the physical plan in the context for calls to Fetch()
246: 	executor.Initialize(statement.plan.get());
247: 
248: 	auto types = executor.GetTypes();
249: 
250: 	D_ASSERT(types == statement.types);
251: 
252: 	if (create_stream_result) {
253: 		if (enable_progress_bar) {
254: 			progress_bar->Stop();
255: 		}
256: 		// successfully compiled SELECT clause and it is the last statement
257: 		// return a StreamQueryResult so the client can call Fetch() on it and stream the result
258: 		return make_unique<StreamQueryResult>(statement.statement_type, shared_from_this(), statement.types,
259: 		                                      statement.names, move(statement_p));
260: 	}
261: 	// create a materialized result by continuously fetching
262: 	auto result = make_unique<MaterializedQueryResult>(statement.statement_type, statement.types, statement.names);
263: 	while (true) {
264: 		auto chunk = FetchInternal(lock);
265: 		if (chunk->size() == 0) {
266: 			break;
267: 		}
268: #ifdef DEBUG
269: 		for (idx_t i = 0; i < chunk->ColumnCount(); i++) {
270: 			if (statement.types[i].id() == LogicalTypeId::VARCHAR) {
271: 				chunk->data[i].UTFVerify(chunk->size());
272: 			}
273: 		}
274: #endif
275: 		result->collection.Append(*chunk);
276: 	}
277: 	if (enable_progress_bar) {
278: 		progress_bar->Stop();
279: 	}
280: 	return move(result);
281: }
282: 
283: void ClientContext::InitialCleanup(ClientContextLock &lock) {
284: 	//! Cleanup any open results and reset the interrupted flag
285: 	CleanupInternal(lock);
286: 	interrupted = false;
287: }
288: 
289: vector<unique_ptr<SQLStatement>> ClientContext::ParseStatements(const string &query) {
290: 	auto lock = LockContext();
291: 	return ParseStatementsInternal(*lock, query);
292: }
293: 
294: vector<unique_ptr<SQLStatement>> ClientContext::ParseStatementsInternal(ClientContextLock &lock, const string &query) {
295: 	Parser parser;
296: 	parser.ParseQuery(query);
297: 
298: 	PragmaHandler handler(*this);
299: 	handler.HandlePragmaStatements(lock, parser.statements);
300: 
301: 	return move(parser.statements);
302: }
303: 
304: void ClientContext::HandlePragmaStatements(vector<unique_ptr<SQLStatement>> &statements) {
305: 	auto lock = LockContext();
306: 
307: 	PragmaHandler handler(*this);
308: 	handler.HandlePragmaStatements(*lock, statements);
309: }
310: 
311: unique_ptr<LogicalOperator> ClientContext::ExtractPlan(const string &query) {
312: 	auto lock = LockContext();
313: 
314: 	auto statements = ParseStatementsInternal(*lock, query);
315: 	if (statements.size() != 1) {
316: 		throw Exception("ExtractPlan can only prepare a single statement");
317: 	}
318: 
319: 	unique_ptr<LogicalOperator> plan;
320: 	RunFunctionInTransactionInternal(*lock, [&]() {
321: 		Planner planner(*this);
322: 		planner.CreatePlan(move(statements[0]));
323: 		D_ASSERT(planner.plan);
324: 
325: 		plan = move(planner.plan);
326: 
327: 		if (enable_optimizer) {
328: 			Optimizer optimizer(*planner.binder, *this);
329: 			plan = optimizer.Optimize(move(plan));
330: 		}
331: 
332: 		ColumnBindingResolver resolver;
333: 		resolver.VisitOperator(*plan);
334: 
335: 		plan->ResolveOperatorTypes();
336: 	});
337: 	return plan;
338: }
339: 
340: unique_ptr<PreparedStatement> ClientContext::PrepareInternal(ClientContextLock &lock,
341:                                                              unique_ptr<SQLStatement> statement) {
342: 	auto n_param = statement->n_param;
343: 	auto statement_query = statement->query;
344: 	shared_ptr<PreparedStatementData> prepared_data;
345: 	auto unbound_statement = statement->Copy();
346: 	RunFunctionInTransactionInternal(
347: 	    lock, [&]() { prepared_data = CreatePreparedStatement(lock, statement_query, move(statement)); }, false);
348: 	prepared_data->unbound_statement = move(unbound_statement);
349: 	return make_unique<PreparedStatement>(shared_from_this(), move(prepared_data), move(statement_query), n_param);
350: }
351: 
352: unique_ptr<PreparedStatement> ClientContext::Prepare(unique_ptr<SQLStatement> statement) {
353: 	auto lock = LockContext();
354: 	// prepare the query
355: 	try {
356: 		InitialCleanup(*lock);
357: 		return PrepareInternal(*lock, move(statement));
358: 	} catch (std::exception &ex) {
359: 		return make_unique<PreparedStatement>(ex.what());
360: 	}
361: }
362: 
363: unique_ptr<PreparedStatement> ClientContext::Prepare(const string &query) {
364: 	auto lock = LockContext();
365: 	// prepare the query
366: 	try {
367: 		InitialCleanup(*lock);
368: 
369: 		// first parse the query
370: 		auto statements = ParseStatementsInternal(*lock, query);
371: 		if (statements.empty()) {
372: 			throw Exception("No statement to prepare!");
373: 		}
374: 		if (statements.size() > 1) {
375: 			throw Exception("Cannot prepare multiple statements at once!");
376: 		}
377: 		return PrepareInternal(*lock, move(statements[0]));
378: 	} catch (std::exception &ex) {
379: 		return make_unique<PreparedStatement>(ex.what());
380: 	}
381: }
382: 
383: unique_ptr<QueryResult> ClientContext::Execute(const string &query, shared_ptr<PreparedStatementData> &prepared,
384:                                                vector<Value> &values, bool allow_stream_result) {
385: 	auto lock = LockContext();
386: 	try {
387: 		InitialCleanup(*lock);
388: 	} catch (std::exception &ex) {
389: 		return make_unique<MaterializedQueryResult>(ex.what());
390: 	}
391: 	LogQueryInternal(*lock, query);
392: 	return RunStatementOrPreparedStatement(*lock, query, nullptr, prepared, &values, allow_stream_result);
393: }
394: 
395: unique_ptr<QueryResult> ClientContext::RunStatementInternal(ClientContextLock &lock, const string &query,
396:                                                             unique_ptr<SQLStatement> statement,
397:                                                             bool allow_stream_result) {
398: 	// prepare the query for execution
399: 	auto prepared = CreatePreparedStatement(lock, query, move(statement));
400: 	// by default, no values are bound
401: 	vector<Value> bound_values;
402: 	// execute the prepared statement
403: 	return ExecutePreparedStatement(lock, query, move(prepared), move(bound_values), allow_stream_result);
404: }
405: 
406: unique_ptr<QueryResult> ClientContext::RunStatementOrPreparedStatement(ClientContextLock &lock, const string &query,
407:                                                                        unique_ptr<SQLStatement> statement,
408:                                                                        shared_ptr<PreparedStatementData> &prepared,
409:                                                                        vector<Value> *values,
410:                                                                        bool allow_stream_result) {
411: 	this->query = query;
412: 
413: 	unique_ptr<QueryResult> result;
414: 	// check if we are on AutoCommit. In this case we should start a transaction.
415: 	if (transaction.IsAutoCommit()) {
416: 		transaction.BeginTransaction();
417: 	}
418: 	ActiveTransaction().active_query = db->GetTransactionManager().GetQueryNumber();
419: 	if (statement && query_verification_enabled) {
420: 		// query verification is enabled
421: 		// create a copy of the statement, and use the copy
422: 		// this way we verify that the copy correctly copies all properties
423: 		auto copied_statement = statement->Copy();
424: 		if (statement->type == StatementType::SELECT_STATEMENT) {
425: 			// in case this is a select query, we verify the original statement
426: 			string error = VerifyQuery(lock, query, move(statement));
427: 			if (!error.empty()) {
428: 				// query failed: abort now
429: 				FinalizeQuery(lock, false);
430: 				// error in verifying query
431: 				return make_unique<MaterializedQueryResult>(error);
432: 			}
433: 		}
434: 		statement = move(copied_statement);
435: 	}
436: 	// start the profiler
437: 	profiler->StartQuery(query);
438: 	try {
439: 		if (statement) {
440: 			result = RunStatementInternal(lock, query, move(statement), allow_stream_result);
441: 		} else {
442: 			auto &catalog = Catalog::GetCatalog(*this);
443: 			if (prepared->unbound_statement && catalog.GetCatalogVersion() != prepared->catalog_version) {
444: 				D_ASSERT(prepared->unbound_statement.get());
445: 				// catalog was modified: rebind the statement before execution
446: 				auto new_prepared = CreatePreparedStatement(lock, query, prepared->unbound_statement->Copy());
447: 				if (prepared->types != new_prepared->types) {
448: 					throw BinderException("Rebinding statement after catalog change resulted in change of types");
449: 				}
450: 				new_prepared->unbound_statement = move(prepared->unbound_statement);
451: 				prepared = move(new_prepared);
452: 			}
453: 			result = ExecutePreparedStatement(lock, query, prepared, *values, allow_stream_result);
454: 		}
455: 	} catch (StandardException &ex) {
456: 		// standard exceptions do not invalidate the current transaction
457: 		result = make_unique<MaterializedQueryResult>(ex.what());
458: 	} catch (std::exception &ex) {
459: 		// other types of exceptions do invalidate the current transaction
460: 		if (transaction.HasActiveTransaction()) {
461: 			ActiveTransaction().Invalidate();
462: 		}
463: 		result = make_unique<MaterializedQueryResult>(ex.what());
464: 	}
465: 	if (!result->success) {
466: 		// initial failures should always be reported as MaterializedResult
467: 		D_ASSERT(result->type != QueryResultType::STREAM_RESULT);
468: 		// query failed: abort now
469: 		FinalizeQuery(lock, false);
470: 		return result;
471: 	}
472: 	// query succeeded, append to list of results
473: 	if (result->type == QueryResultType::STREAM_RESULT) {
474: 		// store as currently open result if it is a stream result
475: 		this->open_result = (StreamQueryResult *)result.get();
476: 	} else {
477: 		// finalize the query if it is not a stream result
478: 		string error = FinalizeQuery(lock, true);
479: 		if (!error.empty()) {
480: 			// failure in committing transaction
481: 			return make_unique<MaterializedQueryResult>(error);
482: 		}
483: 	}
484: 	return result;
485: }
486: 
487: unique_ptr<QueryResult> ClientContext::RunStatement(ClientContextLock &lock, const string &query,
488:                                                     unique_ptr<SQLStatement> statement, bool allow_stream_result) {
489: 	shared_ptr<PreparedStatementData> prepared;
490: 	return RunStatementOrPreparedStatement(lock, query, move(statement), prepared, nullptr, allow_stream_result);
491: }
492: 
493: unique_ptr<QueryResult> ClientContext::RunStatements(ClientContextLock &lock, const string &query,
494:                                                      vector<unique_ptr<SQLStatement>> &statements,
495:                                                      bool allow_stream_result) {
496: 	// now we have a list of statements
497: 	// iterate over them and execute them one by one
498: 	unique_ptr<QueryResult> result;
499: 	QueryResult *last_result = nullptr;
500: 	for (idx_t i = 0; i < statements.size(); i++) {
501: 		auto &statement = statements[i];
502: 		bool is_last_statement = i + 1 == statements.size();
503: 		auto current_result = RunStatement(lock, query, move(statement), allow_stream_result && is_last_statement);
504: 		// now append the result to the list of results
505: 		if (!last_result) {
506: 			// first result of the query
507: 			result = move(current_result);
508: 			last_result = result.get();
509: 		} else {
510: 			// later results; attach to the result chain
511: 			last_result->next = move(current_result);
512: 			last_result = last_result->next.get();
513: 		}
514: 	}
515: 	return result;
516: }
517: 
518: void ClientContext::LogQueryInternal(ClientContextLock &, const string &query) {
519: 	if (!log_query_writer) {
520: #ifdef DUCKDB_FORCE_QUERY_LOG
521: 		try {
522: 			string log_path(DUCKDB_FORCE_QUERY_LOG);
523: 			log_query_writer = make_unique<BufferedFileWriter>(
524: 			    FileSystem::GetFileSystem(*this), log_path, BufferedFileWriter::DEFAULT_OPEN_FLAGS, file_opener.get());
525: 		} catch (...) {
526: 			return;
527: 		}
528: #else
529: 		return;
530: #endif
531: 	}
532: 	// log query path is set: log the query
533: 	log_query_writer->WriteData((const_data_ptr_t)query.c_str(), query.size());
534: 	log_query_writer->WriteData((const_data_ptr_t) "\n", 1);
535: 	log_query_writer->Flush();
536: 	log_query_writer->Sync();
537: }
538: 
539: unique_ptr<QueryResult> ClientContext::Query(unique_ptr<SQLStatement> statement, bool allow_stream_result) {
540: 	auto lock = LockContext();
541: 	LogQueryInternal(*lock, statement->query.substr(statement->stmt_location, statement->stmt_length));
542: 
543: 	vector<unique_ptr<SQLStatement>> statements;
544: 	statements.push_back(move(statement));
545: 
546: 	return RunStatements(*lock, query, statements, allow_stream_result);
547: }
548: 
549: unique_ptr<QueryResult> ClientContext::Query(const string &query, bool allow_stream_result) {
550: 	auto lock = LockContext();
551: 	LogQueryInternal(*lock, query);
552: 
553: 	vector<unique_ptr<SQLStatement>> statements;
554: 	try {
555: 		InitialCleanup(*lock);
556: 		// parse the query and transform it into a set of statements
557: 		statements = ParseStatementsInternal(*lock, query);
558: 	} catch (std::exception &ex) {
559: 		return make_unique<MaterializedQueryResult>(ex.what());
560: 	}
561: 
562: 	if (statements.empty()) {
563: 		// no statements, return empty successful result
564: 		return make_unique<MaterializedQueryResult>(StatementType::INVALID_STATEMENT);
565: 	}
566: 
567: 	return RunStatements(*lock, query, statements, allow_stream_result);
568: }
569: 
570: void ClientContext::Interrupt() {
571: 	interrupted = true;
572: }
573: 
574: void ClientContext::EnableProfiling() {
575: 	auto lock = LockContext();
576: 	profiler->Enable();
577: }
578: 
579: void ClientContext::DisableProfiling() {
580: 	auto lock = LockContext();
581: 	profiler->Disable();
582: }
583: 
584: string ClientContext::VerifyQuery(ClientContextLock &lock, const string &query, unique_ptr<SQLStatement> statement) {
585: 	D_ASSERT(statement->type == StatementType::SELECT_STATEMENT);
586: 	// aggressive query verification
587: 
588: 	// the purpose of this function is to test correctness of otherwise hard to test features:
589: 	// Copy() of statements and expressions
590: 	// Serialize()/Deserialize() of expressions
591: 	// Hash() of expressions
592: 	// Equality() of statements and expressions
593: 	// Correctness of plans both with and without optimizers
594: 	// Correctness of plans both with and without parallelism
595: 
596: 	// copy the statement
597: 	auto select_stmt = (SelectStatement *)statement.get();
598: 	auto copied_stmt = unique_ptr_cast<SQLStatement, SelectStatement>(select_stmt->Copy());
599: 	auto unoptimized_stmt = unique_ptr_cast<SQLStatement, SelectStatement>(select_stmt->Copy());
600: 
601: 	BufferedSerializer serializer;
602: 	select_stmt->Serialize(serializer);
603: 	BufferedDeserializer source(serializer);
604: 	auto deserialized_stmt = SelectStatement::Deserialize(source);
605: 	// all the statements should be equal
606: 	D_ASSERT(copied_stmt->Equals(statement.get()));
607: 	D_ASSERT(deserialized_stmt->Equals(statement.get()));
608: 	D_ASSERT(copied_stmt->Equals(deserialized_stmt.get()));
609: 
610: 	// now perform checking on the expressions
611: #ifdef DEBUG
612: 	auto &orig_expr_list = select_stmt->node->GetSelectList();
613: 	auto &de_expr_list = deserialized_stmt->node->GetSelectList();
614: 	auto &cp_expr_list = copied_stmt->node->GetSelectList();
615: 	D_ASSERT(orig_expr_list.size() == de_expr_list.size() && cp_expr_list.size() == de_expr_list.size());
616: 	for (idx_t i = 0; i < orig_expr_list.size(); i++) {
617: 		// run the ToString, to verify that it doesn't crash
618: 		orig_expr_list[i]->ToString();
619: 		// check that the expressions are equivalent
620: 		D_ASSERT(orig_expr_list[i]->Equals(de_expr_list[i].get()));
621: 		D_ASSERT(orig_expr_list[i]->Equals(cp_expr_list[i].get()));
622: 		D_ASSERT(de_expr_list[i]->Equals(cp_expr_list[i].get()));
623: 		// check that the hashes are equivalent too
624: 		D_ASSERT(orig_expr_list[i]->Hash() == de_expr_list[i]->Hash());
625: 		D_ASSERT(orig_expr_list[i]->Hash() == cp_expr_list[i]->Hash());
626: 
627: 		D_ASSERT(!orig_expr_list[i]->Equals(nullptr));
628: 	}
629: 	// now perform additional checking within the expressions
630: 	for (idx_t outer_idx = 0; outer_idx < orig_expr_list.size(); outer_idx++) {
631: 		auto hash = orig_expr_list[outer_idx]->Hash();
632: 		for (idx_t inner_idx = 0; inner_idx < orig_expr_list.size(); inner_idx++) {
633: 			auto hash2 = orig_expr_list[inner_idx]->Hash();
634: 			if (hash != hash2) {
635: 				// if the hashes are not equivalent, the expressions should not be equivalent
636: 				D_ASSERT(!orig_expr_list[outer_idx]->Equals(orig_expr_list[inner_idx].get()));
637: 			}
638: 		}
639: 	}
640: #endif
641: 
642: 	// disable profiling if it is enabled
643: 	bool profiling_is_enabled = profiler->IsEnabled();
644: 	if (profiling_is_enabled) {
645: 		profiler->Disable();
646: 	}
647: 
648: 	// see below
649: 	auto statement_copy_for_explain = select_stmt->Copy();
650: 
651: 	unique_ptr<MaterializedQueryResult> original_result =
652: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT),
653: 	                                    copied_result =
654: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT),
655: 	                                    deserialized_result =
656: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT),
657: 	                                    unoptimized_result =
658: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT);
659: 
660: 	// execute the original statement
661: 	try {
662: 		auto result = RunStatementInternal(lock, query, move(statement), false);
663: 		original_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
664: 	} catch (std::exception &ex) {
665: 		original_result->error = ex.what();
666: 		original_result->success = false;
667: 		interrupted = false;
668: 	}
669: 
670: 	// check explain, only if q does not already contain EXPLAIN
671: 	if (original_result->success) {
672: 		auto explain_q = "EXPLAIN " + query;
673: 		auto explain_stmt = make_unique<ExplainStatement>(move(statement_copy_for_explain));
674: 		try {
675: 			RunStatementInternal(lock, explain_q, move(explain_stmt), false);
676: 		} catch (std::exception &ex) { // LCOV_EXCL_START
677: 			return "EXPLAIN failed but query did not (" + string(ex.what()) + ")";
678: 		} // LCOV_EXCL_STOP
679: 	}
680: 
681: 	// now execute the copied statement
682: 	try {
683: 		auto result = RunStatementInternal(lock, query, move(copied_stmt), false);
684: 		copied_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
685: 	} catch (std::exception &ex) {
686: 		copied_result->error = ex.what();
687: 		copied_result->success = false;
688: 		interrupted = false;
689: 	}
690: 	// now execute the deserialized statement
691: 	try {
692: 		auto result = RunStatementInternal(lock, query, move(deserialized_stmt), false);
693: 		deserialized_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
694: 	} catch (std::exception &ex) {
695: 		deserialized_result->error = ex.what();
696: 		deserialized_result->success = false;
697: 		interrupted = false;
698: 	}
699: 	// now execute the unoptimized statement
700: 	enable_optimizer = false;
701: 	try {
702: 		auto result = RunStatementInternal(lock, query, move(unoptimized_stmt), false);
703: 		unoptimized_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
704: 	} catch (std::exception &ex) {
705: 		unoptimized_result->error = ex.what();
706: 		unoptimized_result->success = false;
707: 		interrupted = false;
708: 	}
709: 	enable_optimizer = true;
710: 
711: 	if (profiling_is_enabled) {
712: 		profiler->Enable();
713: 	}
714: 
715: 	// now compare the results
716: 	// the results of all runs should be identical
717: 	vector<unique_ptr<MaterializedQueryResult>> results;
718: 	results.push_back(move(copied_result));
719: 	results.push_back(move(deserialized_result));
720: 	results.push_back(move(unoptimized_result));
721: 	vector<string> names = {"Copied Result", "Deserialized Result", "Unoptimized Result"};
722: 	for (idx_t i = 0; i < results.size(); i++) {
723: 		if (original_result->success != results[i]->success) { // LCOV_EXCL_START
724: 			string result = names[i] + " differs from original result!\n";
725: 			result += "Original Result:\n" + original_result->ToString();
726: 			result += names[i] + ":\n" + results[i]->ToString();
727: 			return result;
728: 		}                                                                  // LCOV_EXCL_STOP
729: 		if (!original_result->collection.Equals(results[i]->collection)) { // LCOV_EXCL_START
730: 			string result = names[i] + " differs from original result!\n";
731: 			result += "Original Result:\n" + original_result->ToString();
732: 			result += names[i] + ":\n" + results[i]->ToString();
733: 			return result;
734: 		} // LCOV_EXCL_STOP
735: 	}
736: 
737: 	return "";
738: }
739: 
740: bool ClientContext::UpdateFunctionInfoFromEntry(ScalarFunctionCatalogEntry *existing_function,
741:                                                 CreateScalarFunctionInfo *new_info) {
742: 	if (new_info->functions.empty()) {
743: 		throw InternalException("Registering function without scalar function definitions!");
744: 	}
745: 	bool need_rewrite_entry = false;
746: 	idx_t size_new_func = new_info->functions.size();
747: 	for (idx_t exist_idx = 0; exist_idx < existing_function->functions.size(); ++exist_idx) {
748: 		bool can_add = true;
749: 		for (idx_t new_idx = 0; new_idx < size_new_func; ++new_idx) {
750: 			if (new_info->functions[new_idx].Equal(existing_function->functions[exist_idx])) {
751: 				can_add = false;
752: 				break;
753: 			}
754: 		}
755: 		if (can_add) {
756: 			new_info->functions.push_back(existing_function->functions[exist_idx]);
757: 			need_rewrite_entry = true;
758: 		}
759: 	}
760: 	return need_rewrite_entry;
761: }
762: 
763: void ClientContext::RegisterFunction(CreateFunctionInfo *info) {
764: 	RunFunctionInTransaction([&]() {
765: 		auto &catalog = Catalog::GetCatalog(*this);
766: 		ScalarFunctionCatalogEntry *existing_function = (ScalarFunctionCatalogEntry *)catalog.GetEntry(
767: 		    *this, CatalogType::SCALAR_FUNCTION_ENTRY, info->schema, info->name, true);
768: 		if (existing_function) {
769: 			if (UpdateFunctionInfoFromEntry(existing_function, (CreateScalarFunctionInfo *)info)) {
770: 				// function info was updated from catalog entry, rewrite is needed
771: 				info->on_conflict = OnCreateConflict::REPLACE_ON_CONFLICT;
772: 			}
773: 		}
774: 		// create function
775: 		catalog.CreateFunction(*this, info);
776: 	});
777: }
778: 
779: void ClientContext::RunFunctionInTransactionInternal(ClientContextLock &lock, const std::function<void(void)> &fun,
780:                                                      bool requires_valid_transaction) {
781: 	if (requires_valid_transaction && transaction.HasActiveTransaction() &&
782: 	    transaction.ActiveTransaction().IsInvalidated()) {
783: 		throw Exception("Failed: transaction has been invalidated!");
784: 	}
785: 	// check if we are on AutoCommit. In this case we should start a transaction
786: 	bool require_new_transaction = transaction.IsAutoCommit() && !transaction.HasActiveTransaction();
787: 	if (require_new_transaction) {
788: 		transaction.BeginTransaction();
789: 	}
790: 	try {
791: 		fun();
792: 	} catch (StandardException &ex) {
793: 		if (require_new_transaction) {
794: 			transaction.Rollback();
795: 		}
796: 		throw;
797: 	} catch (std::exception &ex) {
798: 		if (require_new_transaction) {
799: 			transaction.Rollback();
800: 		} else {
801: 			ActiveTransaction().Invalidate();
802: 		}
803: 		throw;
804: 	}
805: 	if (require_new_transaction) {
806: 		transaction.Commit();
807: 	}
808: }
809: 
810: void ClientContext::RunFunctionInTransaction(const std::function<void(void)> &fun, bool requires_valid_transaction) {
811: 	auto lock = LockContext();
812: 	RunFunctionInTransactionInternal(*lock, fun, requires_valid_transaction);
813: }
814: 
815: unique_ptr<TableDescription> ClientContext::TableInfo(const string &schema_name, const string &table_name) {
816: 	unique_ptr<TableDescription> result;
817: 	RunFunctionInTransaction([&]() {
818: 		// obtain the table info
819: 		auto &catalog = Catalog::GetCatalog(*this);
820: 		auto table = catalog.GetEntry<TableCatalogEntry>(*this, schema_name, table_name, true);
821: 		if (!table) {
822: 			return;
823: 		}
824: 		// write the table info to the result
825: 		result = make_unique<TableDescription>();
826: 		result->schema = schema_name;
827: 		result->table = table_name;
828: 		for (auto &column : table->columns) {
829: 			result->columns.emplace_back(column.name, column.type);
830: 		}
831: 	});
832: 	return result;
833: }
834: 
835: void ClientContext::Append(TableDescription &description, ChunkCollection &collection) {
836: 	RunFunctionInTransaction([&]() {
837: 		auto &catalog = Catalog::GetCatalog(*this);
838: 		auto table_entry = catalog.GetEntry<TableCatalogEntry>(*this, description.schema, description.table);
839: 		// verify that the table columns and types match up
840: 		if (description.columns.size() != table_entry->columns.size()) {
841: 			throw Exception("Failed to append: table entry has different number of columns!");
842: 		}
843: 		for (idx_t i = 0; i < description.columns.size(); i++) {
844: 			if (description.columns[i].type != table_entry->columns[i].type) {
845: 				throw Exception("Failed to append: table entry has different number of columns!");
846: 			}
847: 		}
848: 		for (auto &chunk : collection.Chunks()) {
849: 			table_entry->storage->Append(*table_entry, *this, *chunk);
850: 		}
851: 	});
852: }
853: 
854: void ClientContext::TryBindRelation(Relation &relation, vector<ColumnDefinition> &result_columns) {
855: #ifdef DEBUG
856: 	D_ASSERT(!relation.GetAlias().empty());
857: 	D_ASSERT(!relation.ToString().empty());
858: #endif
859: 	RunFunctionInTransaction([&]() {
860: 		// bind the expressions
861: 		auto binder = Binder::CreateBinder(*this);
862: 		auto result = relation.Bind(*binder);
863: 		D_ASSERT(result.names.size() == result.types.size());
864: 		for (idx_t i = 0; i < result.names.size(); i++) {
865: 			result_columns.emplace_back(result.names[i], result.types[i]);
866: 		}
867: 	});
868: }
869: 
870: unique_ptr<QueryResult> ClientContext::Execute(const shared_ptr<Relation> &relation) {
871: 	auto lock = LockContext();
872: 	InitialCleanup(*lock);
873: 
874: 	string query;
875: 	if (query_verification_enabled) {
876: 		// run the ToString method of any relation we run, mostly to ensure it doesn't crash
877: 		relation->ToString();
878: 		relation->GetAlias();
879: 		if (relation->IsReadOnly()) {
880: 			// verify read only statements by running a select statement
881: 			auto select = make_unique<SelectStatement>();
882: 			select->node = relation->GetQueryNode();
883: 			RunStatement(*lock, query, move(select), false);
884: 		}
885: 	}
886: 	auto &expected_columns = relation->Columns();
887: 	auto relation_stmt = make_unique<RelationStatement>(relation);
888: 	auto result = RunStatement(*lock, query, move(relation_stmt), false);
889: 	if (!result->success) {
890: 		return result;
891: 	}
892: 	// verify that the result types and result names of the query match the expected result types/names
893: 	if (result->types.size() == expected_columns.size()) {
894: 		bool mismatch = false;
895: 		for (idx_t i = 0; i < result->types.size(); i++) {
896: 			if (result->types[i] != expected_columns[i].type || result->names[i] != expected_columns[i].name) {
897: 				mismatch = true;
898: 				break;
899: 			}
900: 		}
901: 		if (!mismatch) {
902: 			// all is as expected: return the result
903: 			return result;
904: 		}
905: 	}
906: 	// result mismatch
907: 	string err_str = "Result mismatch in query!\nExpected the following columns: [";
908: 	for (idx_t i = 0; i < expected_columns.size(); i++) {
909: 		if (i > 0) {
910: 			err_str += ", ";
911: 		}
912: 		err_str += expected_columns[i].name + " " + expected_columns[i].type.ToString();
913: 	}
914: 	err_str += "]\nBut result contained the following: ";
915: 	for (idx_t i = 0; i < result->types.size(); i++) {
916: 		err_str += i == 0 ? "[" : ", ";
917: 		err_str += result->names[i] + " " + result->types[i].ToString();
918: 	}
919: 	err_str += "]";
920: 	return make_unique<MaterializedQueryResult>(err_str);
921: }
922: 
923: bool ClientContext::TryGetCurrentSetting(const std::string &key, Value &result) {
924: 	const auto &session_config_map = set_variables;
925: 	const auto &global_config_map = db->config.set_variables;
926: 
927: 	auto session_value = session_config_map.find(key);
928: 	bool found_session_value = session_value != session_config_map.end();
929: 	auto global_value = global_config_map.find(key);
930: 	bool found_global_value = global_value != global_config_map.end();
931: 	if (!found_session_value && !found_global_value) {
932: 		return false;
933: 	}
934: 
935: 	result = found_session_value ? session_value->second : global_value->second;
936: 	return true;
937: }
938: 
939: } // namespace duckdb
[end of src/main/client_context.cpp]
[start of src/parser/transform/statement/transform_set.cpp]
1: #include "duckdb/parser/statement/set_statement.hpp"
2: #include "duckdb/parser/transformer.hpp"
3: #include "duckdb/parser/expression/constant_expression.hpp"
4: 
5: namespace duckdb {
6: 
7: namespace {
8: 
9: SetScope ToSetScope(duckdb_libpgquery::VariableSetScope pg_scope) {
10: 	switch (pg_scope) {
11: 	case duckdb_libpgquery::VariableSetScope::VAR_SET_SCOPE_LOCAL:
12: 		return SetScope::LOCAL;
13: 	case duckdb_libpgquery::VariableSetScope::VAR_SET_SCOPE_SESSION:
14: 		return SetScope::SESSION;
15: 	case duckdb_libpgquery::VariableSetScope::VAR_SET_SCOPE_GLOBAL:
16: 		return SetScope::GLOBAL;
17: 	case duckdb_libpgquery::VariableSetScope::VAR_SET_SCOPE_DEFAULT:
18: 		return SetScope::SESSION;
19: 	default:
20: 		throw InternalException("Unexpected pg_scope: %d", pg_scope);
21: 	}
22: }
23: 
24: } // namespace
25: 
26: unique_ptr<SetStatement> Transformer::TransformSet(duckdb_libpgquery::PGNode *node) {
27: 	D_ASSERT(node->type == duckdb_libpgquery::T_PGVariableSetStmt);
28: 	auto stmt = reinterpret_cast<duckdb_libpgquery::PGVariableSetStmt *>(node);
29: 
30: 	if (stmt->kind != duckdb_libpgquery::VariableSetKind::VAR_SET_VALUE) {
31: 		throw ParserException("Can only SET a variable to a value");
32: 	}
33: 
34: 	if (stmt->scope == duckdb_libpgquery::VariableSetScope::VAR_SET_SCOPE_LOCAL) {
35: 		throw NotImplementedException("SET LOCAL is not implemented.");
36: 	}
37: 
38: 	auto name = std::string(stmt->name);
39: 	D_ASSERT(!name.empty()); // parser protect us!
40: 	if (stmt->args->length != 1) {
41: 		throw ParserException("SET needs a single scalar value parameter");
42: 	}
43: 	D_ASSERT(stmt->args->head && stmt->args->head->data.ptr_value);
44: 	D_ASSERT(((duckdb_libpgquery::PGNode *)stmt->args->head->data.ptr_value)->type == duckdb_libpgquery::T_PGAConst);
45: 
46: 	auto value = TransformValue(((duckdb_libpgquery::PGAConst *)stmt->args->head->data.ptr_value)->val, 0)->value;
47: 
48: 	return make_unique<SetStatement>(name, value, ToSetScope(stmt->scope));
49: }
50: 
51: } // namespace duckdb
[end of src/parser/transform/statement/transform_set.cpp]
[start of src/planner/binder/statement/bind_create.cpp]
1: #include "duckdb/catalog/catalog.hpp"
2: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
3: #include "duckdb/main/client_context.hpp"
4: #include "duckdb/parser/expression/subquery_expression.hpp"
5: #include "duckdb/parser/expression/constant_expression.hpp"
6: #include "duckdb/parser/parsed_data/create_index_info.hpp"
7: #include "duckdb/parser/parsed_data/create_macro_info.hpp"
8: #include "duckdb/parser/parsed_data/create_view_info.hpp"
9: #include "duckdb/parser/parsed_expression_iterator.hpp"
10: #include "duckdb/parser/statement/create_statement.hpp"
11: #include "duckdb/planner/binder.hpp"
12: #include "duckdb/planner/bound_query_node.hpp"
13: #include "duckdb/planner/query_node/bound_select_node.hpp"
14: #include "duckdb/planner/expression_binder/aggregate_binder.hpp"
15: #include "duckdb/planner/expression_binder/index_binder.hpp"
16: #include "duckdb/planner/expression_binder/select_binder.hpp"
17: #include "duckdb/planner/operator/logical_create.hpp"
18: #include "duckdb/planner/operator/logical_create_index.hpp"
19: #include "duckdb/planner/operator/logical_create_table.hpp"
20: #include "duckdb/planner/operator/logical_get.hpp"
21: #include "duckdb/planner/parsed_data/bound_create_function_info.hpp"
22: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
23: #include "duckdb/planner/tableref/bound_basetableref.hpp"
24: 
25: namespace duckdb {
26: 
27: SchemaCatalogEntry *Binder::BindSchema(CreateInfo &info) {
28: 	if (info.schema.empty()) {
29: 		info.schema = info.temporary ? TEMP_SCHEMA : DEFAULT_SCHEMA;
30: 	}
31: 
32: 	if (!info.temporary) {
33: 		// non-temporary create: not read only
34: 		if (info.schema == TEMP_SCHEMA) {
35: 			throw ParserException("Only TEMPORARY table names can use the \"temp\" schema");
36: 		}
37: 		this->read_only = false;
38: 	} else {
39: 		if (info.schema != TEMP_SCHEMA) {
40: 			throw ParserException("TEMPORARY table names can *only* use the \"%s\" schema", TEMP_SCHEMA);
41: 		}
42: 	}
43: 	// fetch the schema in which we want to create the object
44: 	auto schema_obj = Catalog::GetCatalog(context).GetSchema(context, info.schema);
45: 	D_ASSERT(schema_obj->type == CatalogType::SCHEMA_ENTRY);
46: 	info.schema = schema_obj->name;
47: 	return schema_obj;
48: }
49: 
50: void Binder::BindCreateViewInfo(CreateViewInfo &base) {
51: 	// bind the view as if it were a query so we can catch errors
52: 	// note that we bind the original, and replace the original with a copy
53: 	// this is because the original has
54: 	this->can_contain_nulls = true;
55: 
56: 	auto copy = base.query->Copy();
57: 	auto query_node = Bind(*base.query);
58: 	base.query = unique_ptr_cast<SQLStatement, SelectStatement>(move(copy));
59: 	if (base.aliases.size() > query_node.names.size()) {
60: 		throw BinderException("More VIEW aliases than columns in query result");
61: 	}
62: 	// fill up the aliases with the remaining names of the bound query
63: 	for (idx_t i = base.aliases.size(); i < query_node.names.size(); i++) {
64: 		base.aliases.push_back(query_node.names[i]);
65: 	}
66: 	base.types = query_node.types;
67: }
68: 
69: SchemaCatalogEntry *Binder::BindCreateFunctionInfo(CreateInfo &info) {
70: 	auto &base = (CreateMacroInfo &)info;
71: 
72: 	if (base.function->expression->HasParameter()) {
73: 		throw BinderException("Parameter expressions within macro's are not supported!");
74: 	}
75: 
76: 	// create macro binding in order to bind the function
77: 	vector<LogicalType> dummy_types;
78: 	vector<string> dummy_names;
79: 	// positional parameters
80: 	for (idx_t i = 0; i < base.function->parameters.size(); i++) {
81: 		auto param = (ColumnRefExpression &)*base.function->parameters[i];
82: 		if (!param.table_name.empty()) {
83: 			throw BinderException("Invalid parameter name '%s'", param.ToString());
84: 		}
85: 		dummy_types.push_back(LogicalType::SQLNULL);
86: 		dummy_names.push_back(param.column_name);
87: 	}
88: 	// default parameters
89: 	for (auto it = base.function->default_parameters.begin(); it != base.function->default_parameters.end(); it++) {
90: 		auto &val = (ConstantExpression &)*it->second;
91: 		dummy_types.push_back(val.value.type());
92: 		dummy_names.push_back(it->first);
93: 	}
94: 	auto this_macro_binding = make_unique<MacroBinding>(dummy_types, dummy_names, base.name);
95: 	macro_binding = this_macro_binding.get();
96: 
97: 	// create a copy of the expression because we do not want to alter the original
98: 	auto expression = base.function->expression->Copy();
99: 
100: 	// bind it to verify the function was defined correctly
101: 	string error;
102: 	auto sel_node = make_unique<BoundSelectNode>();
103: 	auto group_info = make_unique<BoundGroupInformation>();
104: 	SelectBinder binder(*this, context, *sel_node, *group_info);
105: 	error = binder.Bind(&expression, 0, false);
106: 
107: 	if (!error.empty()) {
108: 		throw BinderException(error);
109: 	}
110: 
111: 	return BindSchema(info);
112: }
113: 
114: BoundStatement Binder::Bind(CreateStatement &stmt) {
115: 	BoundStatement result;
116: 	result.names = {"Count"};
117: 	result.types = {LogicalType::BIGINT};
118: 
119: 	auto catalog_type = stmt.info->type;
120: 	switch (catalog_type) {
121: 	case CatalogType::SCHEMA_ENTRY:
122: 		result.plan = make_unique<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_SCHEMA, move(stmt.info));
123: 		break;
124: 	case CatalogType::VIEW_ENTRY: {
125: 		auto &base = (CreateViewInfo &)*stmt.info;
126: 		// bind the schema
127: 		auto schema = BindSchema(*stmt.info);
128: 		BindCreateViewInfo(base);
129: 		result.plan = make_unique<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_VIEW, move(stmt.info), schema);
130: 		break;
131: 	}
132: 	case CatalogType::SEQUENCE_ENTRY: {
133: 		auto schema = BindSchema(*stmt.info);
134: 		result.plan = make_unique<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_SEQUENCE, move(stmt.info), schema);
135: 		break;
136: 	}
137: 	case CatalogType::MACRO_ENTRY: {
138: 		auto schema = BindCreateFunctionInfo(*stmt.info);
139: 		result.plan = make_unique<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_MACRO, move(stmt.info), schema);
140: 		break;
141: 	}
142: 	case CatalogType::INDEX_ENTRY: {
143: 		auto &base = (CreateIndexInfo &)*stmt.info;
144: 
145: 		// visit the table reference
146: 		auto bound_table = Bind(*base.table);
147: 		if (bound_table->type != TableReferenceType::BASE_TABLE) {
148: 			throw BinderException("Can only delete from base table!");
149: 		}
150: 		auto &table_binding = (BoundBaseTableRef &)*bound_table;
151: 		auto table = table_binding.table;
152: 		// bind the index expressions
153: 		vector<unique_ptr<Expression>> expressions;
154: 		IndexBinder binder(*this, context);
155: 		for (auto &expr : base.expressions) {
156: 			expressions.push_back(binder.Bind(expr));
157: 		}
158: 
159: 		auto plan = CreatePlan(*bound_table);
160: 		if (plan->type != LogicalOperatorType::LOGICAL_GET) {
161: 			throw BinderException("Cannot create index on a view!");
162: 		}
163: 		auto &get = (LogicalGet &)*plan;
164: 		for (auto &column_id : get.column_ids) {
165: 			if (column_id == COLUMN_IDENTIFIER_ROW_ID) {
166: 				throw BinderException("Cannot create an index on the rowid!");
167: 			}
168: 		}
169: 		// this gives us a logical table scan
170: 		// we take the required columns from here
171: 		// create the logical operator
172: 		result.plan = make_unique<LogicalCreateIndex>(*table, get.column_ids, move(expressions),
173: 		                                              unique_ptr_cast<CreateInfo, CreateIndexInfo>(move(stmt.info)));
174: 		break;
175: 	}
176: 	case CatalogType::TABLE_ENTRY: {
177: 		auto bound_info = BindCreateTableInfo(move(stmt.info));
178: 		auto root = move(bound_info->query);
179: 
180: 		// create the logical operator
181: 		auto &schema = bound_info->schema;
182: 		auto create_table = make_unique<LogicalCreateTable>(schema, move(bound_info));
183: 		if (root) {
184: 			create_table->children.push_back(move(root));
185: 		}
186: 		result.plan = move(create_table);
187: 		break;
188: 	}
189: 	default:
190: 		throw Exception("Unrecognized type!");
191: 	}
192: 	this->allow_stream_result = false;
193: 	return result;
194: }
195: 
196: } // namespace duckdb
[end of src/planner/binder/statement/bind_create.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: