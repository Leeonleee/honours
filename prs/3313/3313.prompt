You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
DELETE triggers assertion with STANDARD_VECTOR_SIZE = 2
#### What happens?

I created a table and then inserted some lists into that table. This followed by a `DELETE` on that table triggers an assertion. This happens when compiling DuckDB with a `STANDARD_VECTOR_SIZE` of 2. I am assuming that this is a bug because we test DuckDB for different vector sizes.

#### To Reproduce
Steps to reproduce the behavior.

Change the `STANDARD_VECTOR_SIZE` in `vector_size.hpp` to 2. Then compile and run the following queries.

```
CREATE TABLE aggr (k int[]);
INSERT INTO aggr VALUES ([0, 1, 1, 1, 4, 0, 3, 3, 2, 2, 4, 4, 2, 4, 0, 0, 0, 1, 2, 3, 4, 2, 3, 3, 1]);
INSERT INTO aggr VALUES ([]), ([NULL]), (NULL), ([0, 1, 1, 1, 4, NULL, 0, 3, 3, 2, NULL, 2, 4, 4, 2, 4, 0, 0, 0, 1, NULL, 2, 3, 4, 2, 3, 3, 1]);

DELETE FROM aggr;
```

With the following assertion:
```
Error: INTERNAL Error: INTERNAL Error: Assertion triggered in file "/Users/tania/DuckDB/duckdb-master/duckdb/src/include/duckdb/common/types/validity_mask.hpp" on line 227: row_idx <= STANDARD_VECTOR_SIZE
```

#### Environment (please complete the following information):
 - OS: iOS
 - DuckDB Version: 22
 - DuckDB Client: CLI

#### Before Submitting

- [x] **Have you tried this on the latest `master` branch?**
* **Python**: `pip install duckdb --upgrade --pre`
* **R**: `install.packages("https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz", repos = NULL)`
* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.

- [x] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**


</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
44: 
45: 
[end of README.md]
[start of src/catalog/catalog_set.cpp]
1: #include "duckdb/catalog/catalog_set.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/common/exception.hpp"
5: #include "duckdb/transaction/transaction_manager.hpp"
6: #include "duckdb/transaction/transaction.hpp"
7: #include "duckdb/common/serializer/buffered_serializer.hpp"
8: #include "duckdb/parser/parsed_data/alter_table_info.hpp"
9: #include "duckdb/catalog/dependency_manager.hpp"
10: #include "duckdb/common/string_util.hpp"
11: #include "duckdb/parser/column_definition.hpp"
12: #include "duckdb/parser/expression/constant_expression.hpp"
13: 
14: namespace duckdb {
15: 
16: //! Class responsible to keep track of state when removing entries from the catalog.
17: //! When deleting, many types of errors can be thrown, since we want to avoid try/catch blocks
18: //! this class makes sure that whatever elements were modified are returned to a correct state
19: //! when exceptions are thrown.
20: //! The idea here is to use RAII (Resource acquisition is initialization) to mimic a try/catch/finally block.
21: //! If any exception is raised when this object exists, then its destructor will be called
22: //! and the entry will return to its previous state during deconstruction.
23: class EntryDropper {
24: public:
25: 	//! Both constructor and destructor are privates because they should only be called by DropEntryDependencies
26: 	explicit EntryDropper(CatalogSet &catalog_set, idx_t entry_index)
27: 	    : catalog_set(catalog_set), entry_index(entry_index) {
28: 		old_deleted = catalog_set.entries[entry_index].get()->deleted;
29: 	}
30: 
31: 	~EntryDropper() {
32: 		catalog_set.entries[entry_index].get()->deleted = old_deleted;
33: 	}
34: 
35: private:
36: 	//! The current catalog_set
37: 	CatalogSet &catalog_set;
38: 	//! Keeps track of the state of the entry before starting the delete
39: 	bool old_deleted;
40: 	//! Index of entry to be deleted
41: 	idx_t entry_index;
42: };
43: 
44: CatalogSet::CatalogSet(Catalog &catalog, unique_ptr<DefaultGenerator> defaults)
45:     : catalog(catalog), defaults(move(defaults)) {
46: }
47: 
48: bool CatalogSet::CreateEntry(ClientContext &context, const string &name, unique_ptr<CatalogEntry> value,
49:                              unordered_set<CatalogEntry *> &dependencies) {
50: 	auto &transaction = Transaction::GetTransaction(context);
51: 	// lock the catalog for writing
52: 	lock_guard<mutex> write_lock(catalog.write_lock);
53: 	// lock this catalog set to disallow reading
54: 	lock_guard<mutex> read_lock(catalog_lock);
55: 
56: 	// first check if the entry exists in the unordered set
57: 	idx_t entry_index;
58: 	auto mapping_value = GetMapping(context, name);
59: 	if (mapping_value == nullptr || mapping_value->deleted) {
60: 		// if it does not: entry has never been created
61: 
62: 		// first create a dummy deleted entry for this entry
63: 		// so transactions started before the commit of this transaction don't
64: 		// see it yet
65: 		entry_index = current_entry++;
66: 		auto dummy_node = make_unique<CatalogEntry>(CatalogType::INVALID, value->catalog, name);
67: 		dummy_node->timestamp = 0;
68: 		dummy_node->deleted = true;
69: 		dummy_node->set = this;
70: 
71: 		entries[entry_index] = move(dummy_node);
72: 		PutMapping(context, name, entry_index);
73: 	} else {
74: 		entry_index = mapping_value->index;
75: 		auto &current = *entries[entry_index];
76: 		// if it does, we have to check version numbers
77: 		if (HasConflict(context, current.timestamp)) {
78: 			// current version has been written to by a currently active
79: 			// transaction
80: 			throw TransactionException("Catalog write-write conflict on create with \"%s\"", current.name);
81: 		}
82: 		// there is a current version that has been committed
83: 		// if it has not been deleted there is a conflict
84: 		if (!current.deleted) {
85: 			return false;
86: 		}
87: 	}
88: 	// create a new entry and replace the currently stored one
89: 	// set the timestamp to the timestamp of the current transaction
90: 	// and point it at the dummy node
91: 	value->timestamp = transaction.transaction_id;
92: 	value->set = this;
93: 
94: 	// now add the dependency set of this object to the dependency manager
95: 	catalog.dependency_manager->AddObject(context, value.get(), dependencies);
96: 
97: 	value->child = move(entries[entry_index]);
98: 	value->child->parent = value.get();
99: 	// push the old entry in the undo buffer for this transaction
100: 	transaction.PushCatalogEntry(value->child.get());
101: 	entries[entry_index] = move(value);
102: 	return true;
103: }
104: 
105: bool CatalogSet::GetEntryInternal(ClientContext &context, idx_t entry_index, CatalogEntry *&catalog_entry) {
106: 	catalog_entry = entries[entry_index].get();
107: 	// if it does: we have to retrieve the entry and to check version numbers
108: 	if (HasConflict(context, catalog_entry->timestamp)) {
109: 		// current version has been written to by a currently active
110: 		// transaction
111: 		throw TransactionException("Catalog write-write conflict on alter with \"%s\"", catalog_entry->name);
112: 	}
113: 	// there is a current version that has been committed by this transaction
114: 	if (catalog_entry->deleted) {
115: 		// if the entry was already deleted, it now does not exist anymore
116: 		// so we return that we could not find it
117: 		return false;
118: 	}
119: 	return true;
120: }
121: 
122: bool CatalogSet::GetEntryInternal(ClientContext &context, const string &name, idx_t &entry_index,
123:                                   CatalogEntry *&catalog_entry) {
124: 	auto mapping_value = GetMapping(context, name);
125: 	if (mapping_value == nullptr || mapping_value->deleted) {
126: 		// the entry does not exist, check if we can create a default entry
127: 		return false;
128: 	}
129: 	entry_index = mapping_value->index;
130: 	return GetEntryInternal(context, entry_index, catalog_entry);
131: }
132: 
133: bool CatalogSet::AlterOwnership(ClientContext &context, ChangeOwnershipInfo *info) {
134: 	idx_t entry_index;
135: 	CatalogEntry *entry;
136: 	if (!GetEntryInternal(context, info->name, entry_index, entry)) {
137: 		return false;
138: 	}
139: 
140: 	auto owner_entry = catalog.GetEntry(context, info->owner_schema, info->owner_name);
141: 	if (!owner_entry) {
142: 		return false;
143: 	}
144: 
145: 	catalog.dependency_manager->AddOwnership(context, owner_entry, entry);
146: 
147: 	return true;
148: }
149: 
150: bool CatalogSet::AlterEntry(ClientContext &context, const string &name, AlterInfo *alter_info) {
151: 	auto &transaction = Transaction::GetTransaction(context);
152: 	// lock the catalog for writing
153: 	lock_guard<mutex> write_lock(catalog.write_lock);
154: 
155: 	// first check if the entry exists in the unordered set
156: 	idx_t entry_index;
157: 	CatalogEntry *entry;
158: 	if (!GetEntryInternal(context, name, entry_index, entry)) {
159: 		return false;
160: 	}
161: 	if (entry->internal) {
162: 		throw CatalogException("Cannot alter entry \"%s\" because it is an internal system entry", entry->name);
163: 	}
164: 
165: 	// lock this catalog set to disallow reading
166: 	lock_guard<mutex> read_lock(catalog_lock);
167: 
168: 	// create a new entry and replace the currently stored one
169: 	// set the timestamp to the timestamp of the current transaction
170: 	// and point it to the updated table node
171: 	string original_name = entry->name;
172: 	auto value = entry->AlterEntry(context, alter_info);
173: 	if (!value) {
174: 		// alter failed, but did not result in an error
175: 		return true;
176: 	}
177: 
178: 	if (value->name != original_name) {
179: 		auto mapping_value = GetMapping(context, value->name);
180: 		if (mapping_value && !mapping_value->deleted) {
181: 			auto entry = GetEntryForTransaction(context, entries[mapping_value->index].get());
182: 			if (!entry->deleted) {
183: 				string rename_err_msg =
184: 				    "Could not rename \"%s\" to \"%s\": another entry with this name already exists!";
185: 				throw CatalogException(rename_err_msg, original_name, value->name);
186: 			}
187: 		}
188: 		PutMapping(context, value->name, entry_index);
189: 		DeleteMapping(context, original_name);
190: 	}
191: 	//! Check the dependency manager to verify that there are no conflicting dependencies with this alter
192: 	catalog.dependency_manager->AlterObject(context, entry, value.get());
193: 
194: 	value->timestamp = transaction.transaction_id;
195: 	value->child = move(entries[entry_index]);
196: 	value->child->parent = value.get();
197: 	value->set = this;
198: 
199: 	// serialize the AlterInfo into a temporary buffer
200: 	BufferedSerializer serializer;
201: 	alter_info->Serialize(serializer);
202: 	BinaryData serialized_alter = serializer.GetData();
203: 
204: 	// push the old entry in the undo buffer for this transaction
205: 	transaction.PushCatalogEntry(value->child.get(), serialized_alter.data.get(), serialized_alter.size);
206: 	entries[entry_index] = move(value);
207: 
208: 	return true;
209: }
210: 
211: void CatalogSet::DropEntryDependencies(ClientContext &context, idx_t entry_index, CatalogEntry &entry, bool cascade) {
212: 
213: 	// Stores the deleted value of the entry before starting the process
214: 	EntryDropper dropper(*this, entry_index);
215: 
216: 	// To correctly delete the object and its dependencies, it temporarily is set to deleted.
217: 	entries[entry_index].get()->deleted = true;
218: 
219: 	// check any dependencies of this object
220: 	entry.catalog->dependency_manager->DropObject(context, &entry, cascade);
221: 
222: 	// dropper destructor is called here
223: 	// the destructor makes sure to return the value to the previous state
224: 	// dropper.~EntryDropper()
225: }
226: 
227: void CatalogSet::DropEntryInternal(ClientContext &context, idx_t entry_index, CatalogEntry &entry, bool cascade) {
228: 	auto &transaction = Transaction::GetTransaction(context);
229: 
230: 	DropEntryDependencies(context, entry_index, entry, cascade);
231: 
232: 	// create a new entry and replace the currently stored one
233: 	// set the timestamp to the timestamp of the current transaction
234: 	// and point it at the dummy node
235: 	auto value = make_unique<CatalogEntry>(CatalogType::DELETED_ENTRY, entry.catalog, entry.name);
236: 	value->timestamp = transaction.transaction_id;
237: 	value->child = move(entries[entry_index]);
238: 	value->child->parent = value.get();
239: 	value->set = this;
240: 	value->deleted = true;
241: 
242: 	// push the old entry in the undo buffer for this transaction
243: 	transaction.PushCatalogEntry(value->child.get());
244: 
245: 	entries[entry_index] = move(value);
246: }
247: 
248: bool CatalogSet::DropEntry(ClientContext &context, const string &name, bool cascade) {
249: 	// lock the catalog for writing
250: 	lock_guard<mutex> write_lock(catalog.write_lock);
251: 	// we can only delete an entry that exists
252: 	idx_t entry_index;
253: 	CatalogEntry *entry;
254: 	if (!GetEntryInternal(context, name, entry_index, entry)) {
255: 		return false;
256: 	}
257: 	if (entry->internal) {
258: 		throw CatalogException("Cannot drop entry \"%s\" because it is an internal system entry", entry->name);
259: 	}
260: 
261: 	DropEntryInternal(context, entry_index, *entry, cascade);
262: 	return true;
263: }
264: 
265: void CatalogSet::CleanupEntry(CatalogEntry *catalog_entry) {
266: 	// destroy the backed up entry: it is no longer required
267: 	D_ASSERT(catalog_entry->parent);
268: 	if (catalog_entry->parent->type != CatalogType::UPDATED_ENTRY) {
269: 		lock_guard<mutex> lock(catalog_lock);
270: 		if (!catalog_entry->deleted) {
271: 			// delete the entry from the dependency manager, if it is not deleted yet
272: 			catalog_entry->catalog->dependency_manager->EraseObject(catalog_entry);
273: 		}
274: 		catalog_entry->parent->child = move(catalog_entry->child);
275: 	}
276: }
277: 
278: bool CatalogSet::HasConflict(ClientContext &context, transaction_t timestamp) {
279: 	auto &transaction = Transaction::GetTransaction(context);
280: 	return (timestamp >= TRANSACTION_ID_START && timestamp != transaction.transaction_id) ||
281: 	       (timestamp < TRANSACTION_ID_START && timestamp > transaction.start_time);
282: }
283: 
284: MappingValue *CatalogSet::GetMapping(ClientContext &context, const string &name, bool get_latest) {
285: 	MappingValue *mapping_value;
286: 	auto entry = mapping.find(name);
287: 	if (entry != mapping.end()) {
288: 		mapping_value = entry->second.get();
289: 	} else {
290: 		return nullptr;
291: 	}
292: 	if (get_latest) {
293: 		return mapping_value;
294: 	}
295: 	while (mapping_value->child) {
296: 		if (UseTimestamp(context, mapping_value->timestamp)) {
297: 			break;
298: 		}
299: 		mapping_value = mapping_value->child.get();
300: 		D_ASSERT(mapping_value);
301: 	}
302: 	return mapping_value;
303: }
304: 
305: void CatalogSet::PutMapping(ClientContext &context, const string &name, idx_t entry_index) {
306: 	auto entry = mapping.find(name);
307: 	auto new_value = make_unique<MappingValue>(entry_index);
308: 	new_value->timestamp = Transaction::GetTransaction(context).transaction_id;
309: 	if (entry != mapping.end()) {
310: 		if (HasConflict(context, entry->second->timestamp)) {
311: 			throw TransactionException("Catalog write-write conflict on name \"%s\"", name);
312: 		}
313: 		new_value->child = move(entry->second);
314: 		new_value->child->parent = new_value.get();
315: 	}
316: 	mapping[name] = move(new_value);
317: }
318: 
319: void CatalogSet::DeleteMapping(ClientContext &context, const string &name) {
320: 	auto entry = mapping.find(name);
321: 	D_ASSERT(entry != mapping.end());
322: 	auto delete_marker = make_unique<MappingValue>(entry->second->index);
323: 	delete_marker->deleted = true;
324: 	delete_marker->timestamp = Transaction::GetTransaction(context).transaction_id;
325: 	delete_marker->child = move(entry->second);
326: 	delete_marker->child->parent = delete_marker.get();
327: 	mapping[name] = move(delete_marker);
328: }
329: 
330: bool CatalogSet::UseTimestamp(ClientContext &context, transaction_t timestamp) {
331: 	auto &transaction = Transaction::GetTransaction(context);
332: 	if (timestamp == transaction.transaction_id) {
333: 		// we created this version
334: 		return true;
335: 	}
336: 	if (timestamp < transaction.start_time) {
337: 		// this version was commited before we started the transaction
338: 		return true;
339: 	}
340: 	return false;
341: }
342: 
343: CatalogEntry *CatalogSet::GetEntryForTransaction(ClientContext &context, CatalogEntry *current) {
344: 	while (current->child) {
345: 		if (UseTimestamp(context, current->timestamp)) {
346: 			break;
347: 		}
348: 		current = current->child.get();
349: 		D_ASSERT(current);
350: 	}
351: 	return current;
352: }
353: 
354: CatalogEntry *CatalogSet::GetCommittedEntry(CatalogEntry *current) {
355: 	while (current->child) {
356: 		if (current->timestamp < TRANSACTION_ID_START) {
357: 			// this entry is committed: use it
358: 			break;
359: 		}
360: 		current = current->child.get();
361: 		D_ASSERT(current);
362: 	}
363: 	return current;
364: }
365: 
366: pair<string, idx_t> CatalogSet::SimilarEntry(ClientContext &context, const string &name) {
367: 	lock_guard<mutex> lock(catalog_lock);
368: 
369: 	string result;
370: 	idx_t current_score = (idx_t)-1;
371: 	for (auto &kv : mapping) {
372: 		auto mapping_value = GetMapping(context, kv.first);
373: 		if (mapping_value && !mapping_value->deleted) {
374: 			auto ldist = StringUtil::LevenshteinDistance(kv.first, name);
375: 			if (ldist < current_score) {
376: 				current_score = ldist;
377: 				result = kv.first;
378: 			}
379: 		}
380: 	}
381: 	return {result, current_score};
382: }
383: 
384: CatalogEntry *CatalogSet::CreateEntryInternal(ClientContext &context, unique_ptr<CatalogEntry> entry) {
385: 	if (mapping.find(entry->name) != mapping.end()) {
386: 		return nullptr;
387: 	}
388: 	auto &name = entry->name;
389: 	auto entry_index = current_entry++;
390: 	auto catalog_entry = entry.get();
391: 
392: 	entry->timestamp = 0;
393: 
394: 	PutMapping(context, name, entry_index);
395: 	mapping[name]->timestamp = 0;
396: 	entries[entry_index] = move(entry);
397: 	return catalog_entry;
398: }
399: 
400: CatalogEntry *CatalogSet::GetEntry(ClientContext &context, const string &name) {
401: 	unique_lock<mutex> lock(catalog_lock);
402: 	auto mapping_value = GetMapping(context, name);
403: 	if (mapping_value != nullptr && !mapping_value->deleted) {
404: 		// we found an entry for this name
405: 		// check the version numbers
406: 
407: 		auto catalog_entry = entries[mapping_value->index].get();
408: 		CatalogEntry *current = GetEntryForTransaction(context, catalog_entry);
409: 		if (current->deleted || (current->name != name && !UseTimestamp(context, mapping_value->timestamp))) {
410: 			return nullptr;
411: 		}
412: 		return current;
413: 	}
414: 	// no entry found with this name, check for defaults
415: 	if (!defaults || defaults->created_all_entries) {
416: 		// no defaults either: return null
417: 		return nullptr;
418: 	}
419: 	// this catalog set has a default map defined
420: 	// check if there is a default entry that we can create with this name
421: 	lock.unlock();
422: 	auto entry = defaults->CreateDefaultEntry(context, name);
423: 
424: 	lock.lock();
425: 	if (!entry) {
426: 		// no default entry
427: 		return nullptr;
428: 	}
429: 	// there is a default entry! create it
430: 	auto result = CreateEntryInternal(context, move(entry));
431: 	if (result) {
432: 		return result;
433: 	}
434: 	// we found a default entry, but failed
435: 	// this means somebody else created the entry first
436: 	// just retry?
437: 	lock.unlock();
438: 	return GetEntry(context, name);
439: }
440: 
441: void CatalogSet::UpdateTimestamp(CatalogEntry *entry, transaction_t timestamp) {
442: 	entry->timestamp = timestamp;
443: 	mapping[entry->name]->timestamp = timestamp;
444: }
445: 
446: void CatalogSet::AdjustEnumDependency(CatalogEntry *entry, ColumnDefinition &column, bool remove) {
447: 	CatalogEntry *enum_type_catalog = (CatalogEntry *)EnumType::GetCatalog(column.type);
448: 	if (enum_type_catalog) {
449: 		if (remove) {
450: 			catalog.dependency_manager->dependents_map[enum_type_catalog].erase(entry->parent);
451: 			catalog.dependency_manager->dependencies_map[entry->parent].erase(enum_type_catalog);
452: 		} else {
453: 			catalog.dependency_manager->dependents_map[enum_type_catalog].insert(entry);
454: 			catalog.dependency_manager->dependencies_map[entry].insert(enum_type_catalog);
455: 		}
456: 	}
457: }
458: 
459: void CatalogSet::AdjustDependency(CatalogEntry *entry, TableCatalogEntry *table, ColumnDefinition &column,
460:                                   bool remove) {
461: 	bool found = false;
462: 	if (column.type.id() == LogicalTypeId::ENUM) {
463: 		for (auto &old_column : table->columns) {
464: 			if (old_column.name == column.name && old_column.type.id() != LogicalTypeId::ENUM) {
465: 				AdjustEnumDependency(entry, column, remove);
466: 				found = true;
467: 			}
468: 		}
469: 		if (!found) {
470: 			AdjustEnumDependency(entry, column, remove);
471: 		}
472: 	}
473: }
474: 
475: void CatalogSet::AdjustTableDependencies(CatalogEntry *entry) {
476: 	if (entry->type == CatalogType::TABLE_ENTRY && entry->parent->type == CatalogType::TABLE_ENTRY) {
477: 		// If it's a table entry we have to check for possibly removing or adding user type dependencies
478: 		auto old_table = (TableCatalogEntry *)entry->parent;
479: 		auto new_table = (TableCatalogEntry *)entry;
480: 
481: 		for (auto &new_column : new_table->columns) {
482: 			AdjustDependency(entry, old_table, new_column, false);
483: 		}
484: 		for (auto &old_column : old_table->columns) {
485: 			AdjustDependency(entry, new_table, old_column, true);
486: 		}
487: 	}
488: }
489: 
490: void CatalogSet::Undo(CatalogEntry *entry) {
491: 	lock_guard<mutex> write_lock(catalog.write_lock);
492: 
493: 	lock_guard<mutex> lock(catalog_lock);
494: 
495: 	// entry has to be restored
496: 	// and entry->parent has to be removed ("rolled back")
497: 
498: 	// i.e. we have to place (entry) as (entry->parent) again
499: 	auto &to_be_removed_node = entry->parent;
500: 
501: 	AdjustTableDependencies(entry);
502: 
503: 	if (!to_be_removed_node->deleted) {
504: 		// delete the entry from the dependency manager as well
505: 		catalog.dependency_manager->EraseObject(to_be_removed_node);
506: 	}
507: 	if (entry->name != to_be_removed_node->name) {
508: 		// rename: clean up the new name when the rename is rolled back
509: 		auto removed_entry = mapping.find(to_be_removed_node->name);
510: 		if (removed_entry->second->child) {
511: 			removed_entry->second->child->parent = nullptr;
512: 			mapping[to_be_removed_node->name] = move(removed_entry->second->child);
513: 		} else {
514: 			mapping.erase(removed_entry);
515: 		}
516: 	}
517: 	if (to_be_removed_node->parent) {
518: 		// if the to be removed node has a parent, set the child pointer to the
519: 		// to be restored node
520: 		to_be_removed_node->parent->child = move(to_be_removed_node->child);
521: 		entry->parent = to_be_removed_node->parent;
522: 	} else {
523: 		// otherwise we need to update the base entry tables
524: 		auto &name = entry->name;
525: 		to_be_removed_node->child->SetAsRoot();
526: 		entries[mapping[name]->index] = move(to_be_removed_node->child);
527: 		entry->parent = nullptr;
528: 	}
529: 
530: 	// restore the name if it was deleted
531: 	auto restored_entry = mapping.find(entry->name);
532: 	if (restored_entry->second->deleted || entry->type == CatalogType::INVALID) {
533: 		if (restored_entry->second->child) {
534: 			restored_entry->second->child->parent = nullptr;
535: 			mapping[entry->name] = move(restored_entry->second->child);
536: 		} else {
537: 			mapping.erase(restored_entry);
538: 		}
539: 	}
540: 	// we mark the catalog as being modified, since this action can lead to e.g. tables being dropped
541: 	entry->catalog->ModifyCatalog();
542: }
543: 
544: void CatalogSet::Scan(ClientContext &context, const std::function<void(CatalogEntry *)> &callback) {
545: 	// lock the catalog set
546: 	unique_lock<mutex> lock(catalog_lock);
547: 	if (defaults && !defaults->created_all_entries) {
548: 		// this catalog set has a default set defined:
549: 		auto default_entries = defaults->GetDefaultEntries();
550: 		for (auto &default_entry : default_entries) {
551: 			auto map_entry = mapping.find(default_entry);
552: 			if (map_entry == mapping.end()) {
553: 				// we unlock during the CreateEntry, since it might reference other catalog sets...
554: 				// specifically for views this can happen since the view will be bound
555: 				lock.unlock();
556: 				auto entry = defaults->CreateDefaultEntry(context, default_entry);
557: 
558: 				lock.lock();
559: 				CreateEntryInternal(context, move(entry));
560: 			}
561: 		}
562: 		defaults->created_all_entries = true;
563: 	}
564: 	for (auto &kv : entries) {
565: 		auto entry = kv.second.get();
566: 		entry = GetEntryForTransaction(context, entry);
567: 		if (!entry->deleted) {
568: 			callback(entry);
569: 		}
570: 	}
571: }
572: 
573: void CatalogSet::Scan(const std::function<void(CatalogEntry *)> &callback) {
574: 	// lock the catalog set
575: 	lock_guard<mutex> lock(catalog_lock);
576: 	for (auto &kv : entries) {
577: 		auto entry = kv.second.get();
578: 		entry = GetCommittedEntry(entry);
579: 		if (!entry->deleted) {
580: 			callback(entry);
581: 		}
582: 	}
583: }
584: } // namespace duckdb
[end of src/catalog/catalog_set.cpp]
[start of src/execution/operator/aggregate/physical_streaming_window.cpp]
1: #include "duckdb/execution/operator/aggregate/physical_streaming_window.hpp"
2: 
3: #include "duckdb/execution/expression_executor.hpp"
4: #include "duckdb/parallel/thread_context.hpp"
5: #include "duckdb/planner/expression/bound_reference_expression.hpp"
6: #include "duckdb/planner/expression/bound_window_expression.hpp"
7: 
8: namespace duckdb {
9: 
10: PhysicalStreamingWindow::PhysicalStreamingWindow(vector<LogicalType> types, vector<unique_ptr<Expression>> select_list,
11:                                                  idx_t estimated_cardinality, PhysicalOperatorType type)
12:     : PhysicalOperator(type, move(types), estimated_cardinality), select_list(move(select_list)) {
13: }
14: 
15: class StreamingWindowState : public OperatorState {
16: public:
17: 	StreamingWindowState() : initialized(false), row_number(1) {
18: 	}
19: 
20: 	void Initialize(DataChunk &input, const vector<unique_ptr<Expression>> &expressions) {
21: 		for (idx_t expr_idx = 0; expr_idx < expressions.size(); expr_idx++) {
22: 			auto &expr = *expressions[expr_idx];
23: 			switch (expr.GetExpressionType()) {
24: 			case ExpressionType::WINDOW_FIRST_VALUE: {
25: 				auto &wexpr = (BoundWindowExpression &)expr;
26: 				auto &ref = (BoundReferenceExpression &)*wexpr.children[0];
27: 				const_vectors.push_back(make_unique<Vector>(input.data[ref.index].GetValue(0)));
28: 				break;
29: 			}
30: 			case ExpressionType::WINDOW_PERCENT_RANK: {
31: 				const_vectors.push_back(make_unique<Vector>(Value((double)0)));
32: 				break;
33: 			}
34: 			case ExpressionType::WINDOW_RANK:
35: 			case ExpressionType::WINDOW_RANK_DENSE: {
36: 				const_vectors.push_back(make_unique<Vector>(Value((int64_t)1)));
37: 				break;
38: 			}
39: 			default:
40: 				const_vectors.push_back(nullptr);
41: 			}
42: 		}
43: 		initialized = true;
44: 	}
45: 
46: public:
47: 	bool initialized;
48: 	int64_t row_number;
49: 	vector<unique_ptr<Vector>> const_vectors;
50: };
51: 
52: unique_ptr<OperatorState> PhysicalStreamingWindow::GetOperatorState(ClientContext &context) const {
53: 	return make_unique<StreamingWindowState>();
54: }
55: 
56: OperatorResultType PhysicalStreamingWindow::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
57:                                                     OperatorState &state_p) const {
58: 	auto &state = (StreamingWindowState &)state_p;
59: 	if (!state.initialized) {
60: 		state.Initialize(input, select_list);
61: 	}
62: 	// Put payload columns in place
63: 	for (idx_t col_idx = 0; col_idx < input.data.size(); col_idx++) {
64: 		chunk.data[col_idx].Reference(input.data[col_idx]);
65: 	}
66: 	// Compute window function
67: 	const idx_t count = input.size();
68: 	for (idx_t expr_idx = 0; expr_idx < select_list.size(); expr_idx++) {
69: 		idx_t col_idx = input.data.size() + expr_idx;
70: 		auto &expr = *select_list[expr_idx];
71: 		switch (expr.GetExpressionType()) {
72: 		case ExpressionType::WINDOW_FIRST_VALUE:
73: 		case ExpressionType::WINDOW_PERCENT_RANK:
74: 		case ExpressionType::WINDOW_RANK:
75: 		case ExpressionType::WINDOW_RANK_DENSE: {
76: 			// Reference constant vector
77: 			chunk.data[col_idx].Reference(*state.const_vectors[expr_idx]);
78: 			break;
79: 		}
80: 		case ExpressionType::WINDOW_ROW_NUMBER: {
81: 			// Set row numbers
82: 			auto rdata = FlatVector::GetData<int64_t>(chunk.data[col_idx]);
83: 			for (idx_t i = 0; i < count; i++) {
84: 				rdata[i] = state.row_number + i;
85: 			}
86: 			break;
87: 		}
88: 		default:
89: 			throw NotImplementedException("%s for StreamingWindow", ExpressionTypeToString(expr.GetExpressionType()));
90: 		}
91: 	}
92: 	state.row_number += count;
93: 	chunk.SetCardinality(count);
94: 	return OperatorResultType::NEED_MORE_INPUT;
95: }
96: 
97: string PhysicalStreamingWindow::ParamsToString() const {
98: 	string result;
99: 	for (idx_t i = 0; i < select_list.size(); i++) {
100: 		if (i > 0) {
101: 			result += "\n";
102: 		}
103: 		result += select_list[i]->GetName();
104: 	}
105: 	return result;
106: }
107: 
108: } // namespace duckdb
[end of src/execution/operator/aggregate/physical_streaming_window.cpp]
[start of src/execution/operator/filter/physical_filter.cpp]
1: #include "duckdb/execution/operator/filter/physical_filter.hpp"
2: #include "duckdb/execution/expression_executor.hpp"
3: #include "duckdb/planner/expression/bound_conjunction_expression.hpp"
4: #include "duckdb/parallel/thread_context.hpp"
5: namespace duckdb {
6: 
7: PhysicalFilter::PhysicalFilter(vector<LogicalType> types, vector<unique_ptr<Expression>> select_list,
8:                                idx_t estimated_cardinality)
9:     : PhysicalOperator(PhysicalOperatorType::FILTER, move(types), estimated_cardinality) {
10: 	D_ASSERT(select_list.size() > 0);
11: 	if (select_list.size() > 1) {
12: 		// create a big AND out of the expressions
13: 		auto conjunction = make_unique<BoundConjunctionExpression>(ExpressionType::CONJUNCTION_AND);
14: 		for (auto &expr : select_list) {
15: 			conjunction->children.push_back(move(expr));
16: 		}
17: 		expression = move(conjunction);
18: 	} else {
19: 		expression = move(select_list[0]);
20: 	}
21: }
22: 
23: class FilterState : public OperatorState {
24: public:
25: 	explicit FilterState(Expression &expr) : executor(expr), sel(STANDARD_VECTOR_SIZE) {
26: 	}
27: 
28: 	ExpressionExecutor executor;
29: 	SelectionVector sel;
30: 
31: public:
32: 	void Finalize(PhysicalOperator *op, ExecutionContext &context) override {
33: 		context.thread.profiler.Flush(op, &executor, "filter", 0);
34: 	}
35: };
36: 
37: unique_ptr<OperatorState> PhysicalFilter::GetOperatorState(ClientContext &context) const {
38: 	return make_unique<FilterState>(*expression);
39: }
40: 
41: OperatorResultType PhysicalFilter::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
42:                                            OperatorState &state_p) const {
43: 	auto &state = (FilterState &)state_p;
44: 	idx_t result_count = state.executor.SelectExpression(input, state.sel);
45: 	if (result_count == input.size()) {
46: 		// nothing was filtered: skip adding any selection vectors
47: 		chunk.Reference(input);
48: 	} else {
49: 		chunk.Slice(input, state.sel, result_count);
50: 	}
51: 	return OperatorResultType::NEED_MORE_INPUT;
52: }
53: 
54: string PhysicalFilter::ParamsToString() const {
55: 	return expression->GetName();
56: }
57: 
58: } // namespace duckdb
[end of src/execution/operator/filter/physical_filter.cpp]
[start of src/execution/operator/helper/physical_streaming_sample.cpp]
1: #include "duckdb/execution/operator/helper/physical_streaming_sample.hpp"
2: #include "duckdb/common/random_engine.hpp"
3: #include "duckdb/common/to_string.hpp"
4: 
5: namespace duckdb {
6: 
7: PhysicalStreamingSample::PhysicalStreamingSample(vector<LogicalType> types, SampleMethod method, double percentage,
8:                                                  int64_t seed, idx_t estimated_cardinality)
9:     : PhysicalOperator(PhysicalOperatorType::STREAMING_SAMPLE, move(types), estimated_cardinality), method(method),
10:       percentage(percentage / 100), seed(seed) {
11: }
12: 
13: //===--------------------------------------------------------------------===//
14: // Operator
15: //===--------------------------------------------------------------------===//
16: class StreamingSampleOperatorState : public OperatorState {
17: public:
18: 	explicit StreamingSampleOperatorState(int64_t seed) : random(seed) {
19: 	}
20: 
21: 	RandomEngine random;
22: };
23: 
24: void PhysicalStreamingSample::SystemSample(DataChunk &input, DataChunk &result, OperatorState &state_p) const {
25: 	// system sampling: we throw one dice per chunk
26: 	auto &state = (StreamingSampleOperatorState &)state_p;
27: 	double rand = state.random.NextRandom();
28: 	if (rand <= percentage) {
29: 		// rand is smaller than sample_size: output chunk
30: 		result.Reference(input);
31: 	}
32: }
33: 
34: void PhysicalStreamingSample::BernoulliSample(DataChunk &input, DataChunk &result, OperatorState &state_p) const {
35: 	// bernoulli sampling: we throw one dice per tuple
36: 	// then slice the result chunk
37: 	auto &state = (StreamingSampleOperatorState &)state_p;
38: 	idx_t result_count = 0;
39: 	SelectionVector sel(STANDARD_VECTOR_SIZE);
40: 	for (idx_t i = 0; i < input.size(); i++) {
41: 		double rand = state.random.NextRandom();
42: 		if (rand <= percentage) {
43: 			sel.set_index(result_count++, i);
44: 		}
45: 	}
46: 	if (result_count > 0) {
47: 		result.Slice(input, sel, result_count);
48: 	}
49: }
50: 
51: unique_ptr<OperatorState> PhysicalStreamingSample::GetOperatorState(ClientContext &context) const {
52: 	return make_unique<StreamingSampleOperatorState>(seed);
53: }
54: 
55: OperatorResultType PhysicalStreamingSample::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
56:                                                     OperatorState &state) const {
57: 	switch (method) {
58: 	case SampleMethod::BERNOULLI_SAMPLE:
59: 		BernoulliSample(input, chunk, state);
60: 		break;
61: 	case SampleMethod::SYSTEM_SAMPLE:
62: 		SystemSample(input, chunk, state);
63: 		break;
64: 	default:
65: 		throw InternalException("Unsupported sample method for streaming sample");
66: 	}
67: 	return OperatorResultType::NEED_MORE_INPUT;
68: }
69: 
70: string PhysicalStreamingSample::ParamsToString() const {
71: 	return SampleMethodToString(method) + ": " + to_string(100 * percentage) + "%";
72: }
73: 
74: } // namespace duckdb
[end of src/execution/operator/helper/physical_streaming_sample.cpp]
[start of src/execution/operator/join/physical_blockwise_nl_join.cpp]
1: #include "duckdb/execution/operator/join/physical_blockwise_nl_join.hpp"
2: 
3: #include "duckdb/common/vector_operations/vector_operations.hpp"
4: #include "duckdb/execution/expression_executor.hpp"
5: #include "duckdb/execution/operator/join/physical_comparison_join.hpp"
6: 
7: namespace duckdb {
8: 
9: PhysicalBlockwiseNLJoin::PhysicalBlockwiseNLJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left,
10:                                                  unique_ptr<PhysicalOperator> right, unique_ptr<Expression> condition,
11:                                                  JoinType join_type, idx_t estimated_cardinality)
12:     : PhysicalJoin(op, PhysicalOperatorType::BLOCKWISE_NL_JOIN, join_type, estimated_cardinality),
13:       condition(move(condition)) {
14: 	children.push_back(move(left));
15: 	children.push_back(move(right));
16: 	// MARK and SINGLE joins not handled
17: 	D_ASSERT(join_type != JoinType::MARK);
18: 	D_ASSERT(join_type != JoinType::SINGLE);
19: }
20: 
21: //===--------------------------------------------------------------------===//
22: // Sink
23: //===--------------------------------------------------------------------===//
24: class BlockwiseNLJoinLocalState : public LocalSinkState {
25: public:
26: 	BlockwiseNLJoinLocalState() {
27: 	}
28: };
29: 
30: class BlockwiseNLJoinGlobalState : public GlobalSinkState {
31: public:
32: 	mutex lock;
33: 	ChunkCollection right_chunks;
34: 	//! Whether or not a tuple on the RHS has found a match, only used for FULL OUTER joins
35: 	unique_ptr<bool[]> rhs_found_match;
36: };
37: 
38: unique_ptr<GlobalSinkState> PhysicalBlockwiseNLJoin::GetGlobalSinkState(ClientContext &context) const {
39: 	return make_unique<BlockwiseNLJoinGlobalState>();
40: }
41: 
42: unique_ptr<LocalSinkState> PhysicalBlockwiseNLJoin::GetLocalSinkState(ExecutionContext &context) const {
43: 	return make_unique<BlockwiseNLJoinLocalState>();
44: }
45: 
46: SinkResultType PhysicalBlockwiseNLJoin::Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate,
47:                                              DataChunk &input) const {
48: 	auto &gstate = (BlockwiseNLJoinGlobalState &)state;
49: 	lock_guard<mutex> nl_lock(gstate.lock);
50: 	gstate.right_chunks.Append(input);
51: 	return SinkResultType::NEED_MORE_INPUT;
52: }
53: 
54: //===--------------------------------------------------------------------===//
55: // Finalize
56: //===--------------------------------------------------------------------===//
57: SinkFinalizeType PhysicalBlockwiseNLJoin::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
58:                                                    GlobalSinkState &gstate_p) const {
59: 	auto &gstate = (BlockwiseNLJoinGlobalState &)gstate_p;
60: 	if (IsRightOuterJoin(join_type)) {
61: 		gstate.rhs_found_match = unique_ptr<bool[]>(new bool[gstate.right_chunks.Count()]);
62: 		memset(gstate.rhs_found_match.get(), 0, sizeof(bool) * gstate.right_chunks.Count());
63: 	}
64: 	if (gstate.right_chunks.Count() == 0 && EmptyResultIfRHSIsEmpty()) {
65: 		return SinkFinalizeType::NO_OUTPUT_POSSIBLE;
66: 	}
67: 	return SinkFinalizeType::READY;
68: }
69: 
70: //===--------------------------------------------------------------------===//
71: // Operator
72: //===--------------------------------------------------------------------===//
73: class BlockwiseNLJoinState : public OperatorState {
74: public:
75: 	explicit BlockwiseNLJoinState(const PhysicalBlockwiseNLJoin &op)
76: 	    : left_position(0), right_position(0), executor(*op.condition) {
77: 		if (IsLeftOuterJoin(op.join_type)) {
78: 			left_found_match = unique_ptr<bool[]>(new bool[STANDARD_VECTOR_SIZE]);
79: 			memset(left_found_match.get(), 0, sizeof(bool) * STANDARD_VECTOR_SIZE);
80: 		}
81: 	}
82: 
83: 	//! Whether or not a tuple on the LHS has found a match, only used for LEFT OUTER and FULL OUTER joins
84: 	unique_ptr<bool[]> left_found_match;
85: 	idx_t left_position;
86: 	idx_t right_position;
87: 	ExpressionExecutor executor;
88: };
89: 
90: unique_ptr<OperatorState> PhysicalBlockwiseNLJoin::GetOperatorState(ClientContext &context) const {
91: 	return make_unique<BlockwiseNLJoinState>(*this);
92: }
93: 
94: OperatorResultType PhysicalBlockwiseNLJoin::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
95:                                                     OperatorState &state_p) const {
96: 	D_ASSERT(input.size() > 0);
97: 	auto &state = (BlockwiseNLJoinState &)state_p;
98: 	auto &gstate = (BlockwiseNLJoinGlobalState &)*sink_state;
99: 
100: 	if (gstate.right_chunks.Count() == 0) {
101: 		// empty RHS
102: 		if (!EmptyResultIfRHSIsEmpty()) {
103: 			PhysicalComparisonJoin::ConstructEmptyJoinResult(join_type, false, input, chunk);
104: 			return OperatorResultType::NEED_MORE_INPUT;
105: 		} else {
106: 			return OperatorResultType::FINISHED;
107: 		}
108: 	}
109: 
110: 	// now perform the actual join
111: 	// we construct a combined DataChunk by referencing the LHS and the RHS
112: 	// every step that we do not have output results we shift the vectors of the RHS one up or down
113: 	// this creates a new "alignment" between the tuples, exhausting all possible O(n^2) combinations
114: 	// while allowing us to use vectorized execution for every step
115: 	idx_t result_count = 0;
116: 	do {
117: 		if (state.left_position >= input.size()) {
118: 			// exhausted LHS, have to pull new LHS chunk
119: 			if (state.left_found_match) {
120: 				// left join: before we move to the next chunk, see if we need to output any vectors that didn't
121: 				// have a match found
122: 				PhysicalJoin::ConstructLeftJoinResult(input, chunk, state.left_found_match.get());
123: 				memset(state.left_found_match.get(), 0, sizeof(bool) * STANDARD_VECTOR_SIZE);
124: 			}
125: 			state.left_position = 0;
126: 			state.right_position = 0;
127: 			return OperatorResultType::NEED_MORE_INPUT;
128: 		}
129: 		auto &lchunk = input;
130: 		auto &rchunk = gstate.right_chunks.GetChunk(state.right_position);
131: 
132: 		// fill in the current element of the LHS into the chunk
133: 		D_ASSERT(chunk.ColumnCount() == lchunk.ColumnCount() + rchunk.ColumnCount());
134: 		for (idx_t i = 0; i < lchunk.ColumnCount(); i++) {
135: 			ConstantVector::Reference(chunk.data[i], lchunk.data[i], state.left_position, lchunk.size());
136: 		}
137: 		// for the RHS we just reference the entire vector
138: 		for (idx_t i = 0; i < rchunk.ColumnCount(); i++) {
139: 			chunk.data[lchunk.ColumnCount() + i].Reference(rchunk.data[i]);
140: 		}
141: 		chunk.SetCardinality(rchunk.size());
142: 
143: 		// now perform the computation
144: 		SelectionVector match_sel(STANDARD_VECTOR_SIZE);
145: 		result_count = state.executor.SelectExpression(chunk, match_sel);
146: 		if (result_count > 0) {
147: 			// found a match!
148: 			// set the match flags in the LHS
149: 			if (state.left_found_match) {
150: 				state.left_found_match[state.left_position] = true;
151: 			}
152: 			// set the match flags in the RHS
153: 			if (gstate.rhs_found_match) {
154: 				for (idx_t i = 0; i < result_count; i++) {
155: 					auto idx = match_sel.get_index(i);
156: 					gstate.rhs_found_match[state.right_position * STANDARD_VECTOR_SIZE + idx] = true;
157: 				}
158: 			}
159: 			chunk.Slice(match_sel, result_count);
160: 		} else {
161: 			// no result: reset the chunk
162: 			chunk.Reset();
163: 		}
164: 		// move to the next tuple on the LHS
165: 		state.left_position++;
166: 		if (state.left_position >= input.size()) {
167: 			// exhausted the current chunk, move to the next RHS chunk
168: 			state.right_position++;
169: 			if (state.right_position < gstate.right_chunks.ChunkCount()) {
170: 				// we still have chunks left! start over on the LHS
171: 				state.left_position = 0;
172: 			}
173: 		}
174: 	} while (result_count == 0);
175: 	return OperatorResultType::HAVE_MORE_OUTPUT;
176: }
177: 
178: string PhysicalBlockwiseNLJoin::ParamsToString() const {
179: 	string extra_info = JoinTypeToString(join_type) + "\n";
180: 	extra_info += condition->GetName();
181: 	return extra_info;
182: }
183: 
184: //===--------------------------------------------------------------------===//
185: // Source
186: //===--------------------------------------------------------------------===//
187: class BlockwiseNLJoinScanState : public GlobalSourceState {
188: public:
189: 	explicit BlockwiseNLJoinScanState(const PhysicalBlockwiseNLJoin &op) : op(op), right_outer_position(0) {
190: 	}
191: 
192: 	mutex lock;
193: 	const PhysicalBlockwiseNLJoin &op;
194: 	//! The position in the RHS in the final scan of the FULL OUTER JOIN
195: 	idx_t right_outer_position;
196: 
197: public:
198: 	idx_t MaxThreads() override {
199: 		auto &sink = (BlockwiseNLJoinGlobalState &)*op.sink_state;
200: 		return sink.right_chunks.Count() / (STANDARD_VECTOR_SIZE * 10);
201: 	}
202: };
203: 
204: unique_ptr<GlobalSourceState> PhysicalBlockwiseNLJoin::GetGlobalSourceState(ClientContext &context) const {
205: 	return make_unique<BlockwiseNLJoinScanState>(*this);
206: }
207: 
208: void PhysicalBlockwiseNLJoin::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,
209:                                       LocalSourceState &lstate) const {
210: 	D_ASSERT(IsRightOuterJoin(join_type));
211: 	// check if we need to scan any unmatched tuples from the RHS for the full/right outer join
212: 	auto &sink = (BlockwiseNLJoinGlobalState &)*sink_state;
213: 	auto &state = (BlockwiseNLJoinScanState &)gstate;
214: 
215: 	// if the LHS is exhausted in a FULL/RIGHT OUTER JOIN, we scan the found_match for any chunks we
216: 	// still need to output
217: 	lock_guard<mutex> l(state.lock);
218: 	PhysicalComparisonJoin::ConstructFullOuterJoinResult(sink.rhs_found_match.get(), sink.right_chunks, chunk,
219: 	                                                     state.right_outer_position);
220: }
221: 
222: } // namespace duckdb
[end of src/execution/operator/join/physical_blockwise_nl_join.cpp]
[start of src/execution/operator/join/physical_cross_product.cpp]
1: #include "duckdb/execution/operator/join/physical_cross_product.hpp"
2: 
3: #include "duckdb/common/vector_operations/vector_operations.hpp"
4: 
5: namespace duckdb {
6: 
7: PhysicalCrossProduct::PhysicalCrossProduct(vector<LogicalType> types, unique_ptr<PhysicalOperator> left,
8:                                            unique_ptr<PhysicalOperator> right, idx_t estimated_cardinality)
9:     : PhysicalOperator(PhysicalOperatorType::CROSS_PRODUCT, move(types), estimated_cardinality) {
10: 	children.push_back(move(left));
11: 	children.push_back(move(right));
12: }
13: 
14: //===--------------------------------------------------------------------===//
15: // Sink
16: //===--------------------------------------------------------------------===//
17: class CrossProductGlobalState : public GlobalSinkState {
18: public:
19: 	CrossProductGlobalState() {
20: 	}
21: 
22: 	ChunkCollection rhs_materialized;
23: 	mutex rhs_lock;
24: };
25: 
26: unique_ptr<GlobalSinkState> PhysicalCrossProduct::GetGlobalSinkState(ClientContext &context) const {
27: 	return make_unique<CrossProductGlobalState>();
28: }
29: 
30: SinkResultType PhysicalCrossProduct::Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate_p,
31:                                           DataChunk &input) const {
32: 	auto &sink = (CrossProductGlobalState &)state;
33: 	lock_guard<mutex> client_guard(sink.rhs_lock);
34: 	sink.rhs_materialized.Append(input);
35: 	return SinkResultType::NEED_MORE_INPUT;
36: }
37: 
38: //===--------------------------------------------------------------------===//
39: // Operator
40: //===--------------------------------------------------------------------===//
41: class CrossProductOperatorState : public OperatorState {
42: public:
43: 	CrossProductOperatorState() : right_position(0) {
44: 	}
45: 
46: 	idx_t right_position;
47: };
48: 
49: unique_ptr<OperatorState> PhysicalCrossProduct::GetOperatorState(ClientContext &context) const {
50: 	return make_unique<CrossProductOperatorState>();
51: }
52: 
53: OperatorResultType PhysicalCrossProduct::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
54:                                                  OperatorState &state_p) const {
55: 	auto &state = (CrossProductOperatorState &)state_p;
56: 	auto &sink = (CrossProductGlobalState &)*sink_state;
57: 	auto &right_collection = sink.rhs_materialized;
58: 
59: 	if (sink.rhs_materialized.Count() == 0) {
60: 		// no RHS: empty result
61: 		return OperatorResultType::FINISHED;
62: 	}
63: 	if (state.right_position >= right_collection.Count()) {
64: 		// ran out of entries on the RHS
65: 		// reset the RHS and move to the next chunk on the LHS
66: 		state.right_position = 0;
67: 		return OperatorResultType::NEED_MORE_INPUT;
68: 	}
69: 
70: 	auto &left_chunk = input;
71: 	// now match the current vector of the left relation with the current row
72: 	// from the right relation
73: 	chunk.SetCardinality(left_chunk.size());
74: 	// create a reference to the vectors of the left column
75: 	for (idx_t i = 0; i < left_chunk.ColumnCount(); i++) {
76: 		chunk.data[i].Reference(left_chunk.data[i]);
77: 	}
78: 	// duplicate the values on the right side
79: 	auto &right_chunk = right_collection.GetChunkForRow(state.right_position);
80: 	auto row_in_chunk = state.right_position % STANDARD_VECTOR_SIZE;
81: 	for (idx_t i = 0; i < right_collection.ColumnCount(); i++) {
82: 		ConstantVector::Reference(chunk.data[left_chunk.ColumnCount() + i], right_chunk.data[i], row_in_chunk,
83: 		                          right_chunk.size());
84: 	}
85: 
86: 	// for the next iteration, move to the next position on the right side
87: 	state.right_position++;
88: 	return OperatorResultType::HAVE_MORE_OUTPUT;
89: }
90: 
91: } // namespace duckdb
[end of src/execution/operator/join/physical_cross_product.cpp]
[start of src/execution/operator/join/physical_hash_join.cpp]
1: #include "duckdb/execution/operator/join/physical_hash_join.hpp"
2: #include "duckdb/common/vector_operations/vector_operations.hpp"
3: #include "duckdb/execution/expression_executor.hpp"
4: #include "duckdb/function/aggregate/distributive_functions.hpp"
5: #include "duckdb/main/client_context.hpp"
6: #include "duckdb/main/query_profiler.hpp"
7: #include "duckdb/parallel/thread_context.hpp"
8: #include "duckdb/storage/buffer_manager.hpp"
9: #include "duckdb/storage/storage_manager.hpp"
10: 
11: namespace duckdb {
12: 
13: PhysicalHashJoin::PhysicalHashJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left,
14:                                    unique_ptr<PhysicalOperator> right, vector<JoinCondition> cond, JoinType join_type,
15:                                    const vector<idx_t> &left_projection_map,
16:                                    const vector<idx_t> &right_projection_map_p, vector<LogicalType> delim_types,
17:                                    idx_t estimated_cardinality, PerfectHashJoinStats perfect_join_stats)
18:     : PhysicalComparisonJoin(op, PhysicalOperatorType::HASH_JOIN, move(cond), join_type, estimated_cardinality),
19:       right_projection_map(right_projection_map_p), delim_types(move(delim_types)),
20:       perfect_join_statistics(move(perfect_join_stats)) {
21: 
22: 	children.push_back(move(left));
23: 	children.push_back(move(right));
24: 
25: 	D_ASSERT(left_projection_map.empty());
26: 	for (auto &condition : conditions) {
27: 		condition_types.push_back(condition.left->return_type);
28: 	}
29: 
30: 	// for ANTI, SEMI and MARK join, we only need to store the keys, so for these the build types are empty
31: 	if (join_type != JoinType::ANTI && join_type != JoinType::SEMI && join_type != JoinType::MARK) {
32: 		build_types = LogicalOperator::MapTypes(children[1]->GetTypes(), right_projection_map);
33: 	}
34: }
35: 
36: PhysicalHashJoin::PhysicalHashJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left,
37:                                    unique_ptr<PhysicalOperator> right, vector<JoinCondition> cond, JoinType join_type,
38:                                    idx_t estimated_cardinality, PerfectHashJoinStats perfect_join_state)
39:     : PhysicalHashJoin(op, move(left), move(right), move(cond), join_type, {}, {}, {}, estimated_cardinality,
40:                        std::move(perfect_join_state)) {
41: }
42: 
43: //===--------------------------------------------------------------------===//
44: // Sink
45: //===--------------------------------------------------------------------===//
46: class HashJoinLocalState : public LocalSinkState {
47: public:
48: 	DataChunk build_chunk;
49: 	DataChunk join_keys;
50: 	ExpressionExecutor build_executor;
51: };
52: 
53: class HashJoinGlobalState : public GlobalSinkState {
54: public:
55: 	HashJoinGlobalState() {
56: 	}
57: 
58: 	//! The HT used by the join
59: 	unique_ptr<JoinHashTable> hash_table;
60: 	//! The perfect hash join executor (if any)
61: 	unique_ptr<PerfectHashJoinExecutor> perfect_join_executor;
62: 	//! Whether or not the hash table has been finalized
63: 	bool finalized = false;
64: };
65: 
66: unique_ptr<GlobalSinkState> PhysicalHashJoin::GetGlobalSinkState(ClientContext &context) const {
67: 	auto state = make_unique<HashJoinGlobalState>();
68: 	state->hash_table =
69: 	    make_unique<JoinHashTable>(BufferManager::GetBufferManager(context), conditions, build_types, join_type);
70: 	if (!delim_types.empty() && join_type == JoinType::MARK) {
71: 		// correlated MARK join
72: 		if (delim_types.size() + 1 == conditions.size()) {
73: 			// the correlated MARK join has one more condition than the amount of correlated columns
74: 			// this is the case in a correlated ANY() expression
75: 			// in this case we need to keep track of additional entries, namely:
76: 			// - (1) the total amount of elements per group
77: 			// - (2) the amount of non-null elements per group
78: 			// we need these to correctly deal with the cases of either:
79: 			// - (1) the group being empty [in which case the result is always false, even if the comparison is NULL]
80: 			// - (2) the group containing a NULL value [in which case FALSE becomes NULL]
81: 			auto &info = state->hash_table->correlated_mark_join_info;
82: 
83: 			vector<LogicalType> payload_types;
84: 			vector<BoundAggregateExpression *> correlated_aggregates;
85: 			unique_ptr<BoundAggregateExpression> aggr;
86: 
87: 			// jury-rigging the GroupedAggregateHashTable
88: 			// we need a count_star and a count to get counts with and without NULLs
89: 			aggr = AggregateFunction::BindAggregateFunction(context, CountStarFun::GetFunction(), {}, nullptr, false);
90: 			correlated_aggregates.push_back(&*aggr);
91: 			payload_types.push_back(aggr->return_type);
92: 			info.correlated_aggregates.push_back(move(aggr));
93: 
94: 			auto count_fun = CountFun::GetFunction();
95: 			vector<unique_ptr<Expression>> children;
96: 			// this is a dummy but we need it to make the hash table understand whats going on
97: 			children.push_back(make_unique_base<Expression, BoundReferenceExpression>(count_fun.return_type, 0));
98: 			aggr = AggregateFunction::BindAggregateFunction(context, count_fun, move(children), nullptr, false);
99: 			correlated_aggregates.push_back(&*aggr);
100: 			payload_types.push_back(aggr->return_type);
101: 			info.correlated_aggregates.push_back(move(aggr));
102: 
103: 			info.correlated_counts = make_unique<GroupedAggregateHashTable>(
104: 			    BufferManager::GetBufferManager(context), delim_types, payload_types, correlated_aggregates);
105: 			info.correlated_types = delim_types;
106: 			info.group_chunk.Initialize(delim_types);
107: 			info.result_chunk.Initialize(payload_types);
108: 		}
109: 	}
110: 	// for perfect hash join
111: 	state->perfect_join_executor =
112: 	    make_unique<PerfectHashJoinExecutor>(*this, *state->hash_table, perfect_join_statistics);
113: 	return move(state);
114: }
115: 
116: unique_ptr<LocalSinkState> PhysicalHashJoin::GetLocalSinkState(ExecutionContext &context) const {
117: 	auto state = make_unique<HashJoinLocalState>();
118: 	if (!right_projection_map.empty()) {
119: 		state->build_chunk.Initialize(build_types);
120: 	}
121: 	for (auto &cond : conditions) {
122: 		state->build_executor.AddExpression(*cond.right);
123: 	}
124: 	state->join_keys.Initialize(condition_types);
125: 	return move(state);
126: }
127: 
128: SinkResultType PhysicalHashJoin::Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate_p,
129:                                       DataChunk &input) const {
130: 	auto &sink = (HashJoinGlobalState &)state;
131: 	auto &lstate = (HashJoinLocalState &)lstate_p;
132: 	// resolve the join keys for the right chunk
133: 	lstate.join_keys.Reset();
134: 	lstate.build_executor.Execute(input, lstate.join_keys);
135: 	// TODO: add statement to check for possible per
136: 	// build the HT
137: 	if (!right_projection_map.empty()) {
138: 		// there is a projection map: fill the build chunk with the projected columns
139: 		lstate.build_chunk.Reset();
140: 		lstate.build_chunk.SetCardinality(input);
141: 		for (idx_t i = 0; i < right_projection_map.size(); i++) {
142: 			lstate.build_chunk.data[i].Reference(input.data[right_projection_map[i]]);
143: 		}
144: 		sink.hash_table->Build(lstate.join_keys, lstate.build_chunk);
145: 	} else if (!build_types.empty()) {
146: 		// there is not a projected map: place the entire right chunk in the HT
147: 		sink.hash_table->Build(lstate.join_keys, input);
148: 	} else {
149: 		// there are only keys: place an empty chunk in the payload
150: 		lstate.build_chunk.SetCardinality(input.size());
151: 		sink.hash_table->Build(lstate.join_keys, lstate.build_chunk);
152: 	}
153: 	return SinkResultType::NEED_MORE_INPUT;
154: }
155: 
156: void PhysicalHashJoin::Combine(ExecutionContext &context, GlobalSinkState &gstate, LocalSinkState &lstate) const {
157: 	auto &state = (HashJoinLocalState &)lstate;
158: 	auto &client_profiler = QueryProfiler::Get(context.client);
159: 	context.thread.profiler.Flush(this, &state.build_executor, "build_executor", 1);
160: 	client_profiler.Flush(context.thread.profiler);
161: }
162: 
163: //===--------------------------------------------------------------------===//
164: // Finalize
165: //===--------------------------------------------------------------------===//
166: SinkFinalizeType PhysicalHashJoin::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
167:                                             GlobalSinkState &gstate) const {
168: 	auto &sink = (HashJoinGlobalState &)gstate;
169: 	// check for possible perfect hash table
170: 	auto use_perfect_hash = sink.perfect_join_executor->CanDoPerfectHashJoin();
171: 	if (use_perfect_hash) {
172: 		D_ASSERT(sink.hash_table->equality_types.size() == 1);
173: 		auto key_type = sink.hash_table->equality_types[0];
174: 		use_perfect_hash = sink.perfect_join_executor->BuildPerfectHashTable(key_type);
175: 	}
176: 	// In case of a large build side or duplicates, use regular hash join
177: 	if (!use_perfect_hash) {
178: 		sink.perfect_join_executor.reset();
179: 		sink.hash_table->Finalize();
180: 	}
181: 	sink.finalized = true;
182: 	if (sink.hash_table->Count() == 0 && EmptyResultIfRHSIsEmpty()) {
183: 		return SinkFinalizeType::NO_OUTPUT_POSSIBLE;
184: 	}
185: 	return SinkFinalizeType::READY;
186: }
187: 
188: //===--------------------------------------------------------------------===//
189: // Operator
190: //===--------------------------------------------------------------------===//
191: class PhysicalHashJoinState : public OperatorState {
192: public:
193: 	DataChunk join_keys;
194: 	ExpressionExecutor probe_executor;
195: 	unique_ptr<JoinHashTable::ScanStructure> scan_structure;
196: 	unique_ptr<OperatorState> perfect_hash_join_state;
197: 
198: public:
199: 	void Finalize(PhysicalOperator *op, ExecutionContext &context) override {
200: 		context.thread.profiler.Flush(op, &probe_executor, "probe_executor", 0);
201: 	}
202: };
203: 
204: unique_ptr<OperatorState> PhysicalHashJoin::GetOperatorState(ClientContext &context) const {
205: 	auto state = make_unique<PhysicalHashJoinState>();
206: 	auto &sink = (HashJoinGlobalState &)*sink_state;
207: 	if (sink.perfect_join_executor) {
208: 		state->perfect_hash_join_state = sink.perfect_join_executor->GetOperatorState(context);
209: 	} else {
210: 		state->join_keys.Initialize(condition_types);
211: 		for (auto &cond : conditions) {
212: 			state->probe_executor.AddExpression(*cond.left);
213: 		}
214: 	}
215: 	return move(state);
216: }
217: 
218: OperatorResultType PhysicalHashJoin::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
219:                                              OperatorState &state_p) const {
220: 	auto &state = (PhysicalHashJoinState &)state_p;
221: 	auto &sink = (HashJoinGlobalState &)*sink_state;
222: 	D_ASSERT(sink.finalized);
223: 
224: 	if (sink.hash_table->Count() == 0 && EmptyResultIfRHSIsEmpty()) {
225: 		return OperatorResultType::FINISHED;
226: 	}
227: 	if (sink.perfect_join_executor) {
228: 		return sink.perfect_join_executor->ProbePerfectHashTable(context, input, chunk, *state.perfect_hash_join_state);
229: 	}
230: 
231: 	if (state.scan_structure) {
232: 		// still have elements remaining from the previous probe (i.e. we got
233: 		// >1024 elements in the previous probe)
234: 		state.scan_structure->Next(state.join_keys, input, chunk);
235: 		if (chunk.size() > 0) {
236: 			return OperatorResultType::HAVE_MORE_OUTPUT;
237: 		}
238: 		state.scan_structure = nullptr;
239: 		return OperatorResultType::NEED_MORE_INPUT;
240: 	}
241: 
242: 	// probe the HT
243: 	if (sink.hash_table->Count() == 0) {
244: 		ConstructEmptyJoinResult(sink.hash_table->join_type, sink.hash_table->has_null, input, chunk);
245: 		return OperatorResultType::NEED_MORE_INPUT;
246: 	}
247: 	// resolve the join keys for the left chunk
248: 	state.join_keys.Reset();
249: 	state.probe_executor.Execute(input, state.join_keys);
250: 
251: 	// perform the actual probe
252: 	state.scan_structure = sink.hash_table->Probe(state.join_keys);
253: 	state.scan_structure->Next(state.join_keys, input, chunk);
254: 	return OperatorResultType::HAVE_MORE_OUTPUT;
255: }
256: 
257: //===--------------------------------------------------------------------===//
258: // Source
259: //===--------------------------------------------------------------------===//
260: class HashJoinScanState : public GlobalSourceState {
261: public:
262: 	explicit HashJoinScanState(const PhysicalHashJoin &op) : op(op) {
263: 	}
264: 
265: 	const PhysicalHashJoin &op;
266: 	//! Only used for FULL OUTER JOIN: scan state of the final scan to find unmatched tuples in the build-side
267: 	JoinHTScanState ht_scan_state;
268: 
269: 	idx_t MaxThreads() override {
270: 		auto &sink = (HashJoinGlobalState &)*op.sink_state;
271: 		return sink.hash_table->Count() / (STANDARD_VECTOR_SIZE * 10);
272: 	}
273: };
274: 
275: unique_ptr<GlobalSourceState> PhysicalHashJoin::GetGlobalSourceState(ClientContext &context) const {
276: 	return make_unique<HashJoinScanState>(*this);
277: }
278: 
279: void PhysicalHashJoin::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,
280:                                LocalSourceState &lstate) const {
281: 	D_ASSERT(IsRightOuterJoin(join_type));
282: 	// check if we need to scan any unmatched tuples from the RHS for the full/right outer join
283: 	auto &sink = (HashJoinGlobalState &)*sink_state;
284: 	auto &state = (HashJoinScanState &)gstate;
285: 	sink.hash_table->ScanFullOuter(chunk, state.ht_scan_state);
286: }
287: 
288: } // namespace duckdb
[end of src/execution/operator/join/physical_hash_join.cpp]
[start of src/execution/operator/join/physical_iejoin.cpp]
1: #include "duckdb/execution/operator/join/physical_iejoin.hpp"
2: 
3: #include "duckdb/common/fast_mem.hpp"
4: #include "duckdb/common/operator/comparison_operators.hpp"
5: #include "duckdb/common/row_operations/row_operations.hpp"
6: #include "duckdb/common/sort/comparators.hpp"
7: #include "duckdb/common/sort/sort.hpp"
8: #include "duckdb/common/vector_operations/vector_operations.hpp"
9: #include "duckdb/execution/expression_executor.hpp"
10: #include "duckdb/main/client_context.hpp"
11: #include "duckdb/parallel/event.hpp"
12: #include "duckdb/parallel/thread_context.hpp"
13: 
14: #include <thread>
15: 
16: namespace duckdb {
17: 
18: PhysicalIEJoin::PhysicalIEJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left,
19:                                unique_ptr<PhysicalOperator> right, vector<JoinCondition> cond, JoinType join_type,
20:                                idx_t estimated_cardinality)
21:     : PhysicalComparisonJoin(op, PhysicalOperatorType::IE_JOIN, move(cond), join_type, estimated_cardinality) {
22: 	// Reorder the conditions so that ranges are at the front.
23: 	// TODO: use stats to improve the choice?
24: 	// TODO: Prefer fixed length types?
25: 	auto conditions_p = std::move(conditions);
26: 	conditions.resize(conditions_p.size());
27: 	idx_t range_position = 0;
28: 	idx_t other_position = conditions_p.size();
29: 	for (idx_t i = 0; i < conditions_p.size(); ++i) {
30: 		switch (conditions_p[i].comparison) {
31: 		case ExpressionType::COMPARE_LESSTHAN:
32: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
33: 		case ExpressionType::COMPARE_GREATERTHAN:
34: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
35: 			conditions[range_position++] = std::move(conditions_p[i]);
36: 			break;
37: 		case ExpressionType::COMPARE_NOTEQUAL:
38: 		case ExpressionType::COMPARE_DISTINCT_FROM:
39: 			// Allowed in multi-predicate joins, but can't be first/sort.
40: 			conditions[--other_position] = std::move(conditions_p[i]);
41: 			break;
42: 		default:
43: 			// COMPARE EQUAL not supported with iejoin join
44: 			throw NotImplementedException("Unimplemented join type for IEJoin");
45: 		}
46: 	}
47: 
48: 	// IEJoin requires at least two comparisons.
49: 	D_ASSERT(range_position > 1);
50: 
51: 	// 1. let L1 (resp. L2) be the array of column X (resp. Y)
52: 	D_ASSERT(conditions.size() >= 2);
53: 	lhs_orders.resize(2);
54: 	rhs_orders.resize(2);
55: 	for (idx_t i = 0; i < 2; ++i) {
56: 		auto &cond = conditions[i];
57: 		D_ASSERT(cond.left->return_type == cond.right->return_type);
58: 		join_key_types.push_back(cond.left->return_type);
59: 
60: 		// Convert the conditions to sort orders
61: 		auto left = cond.left->Copy();
62: 		auto right = cond.right->Copy();
63: 		auto sense = OrderType::INVALID;
64: 
65: 		// 2. if (op1  {>, }) sort L1 in descending order
66: 		// 3. else if (op1  {<, }) sort L1 in ascending order
67: 		// 4. if (op2  {>, }) sort L2 in ascending order
68: 		// 5. else if (op2  {<, }) sort L2 in descending order
69: 		switch (cond.comparison) {
70: 		case ExpressionType::COMPARE_GREATERTHAN:
71: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
72: 			sense = i ? OrderType::ASCENDING : OrderType::DESCENDING;
73: 			break;
74: 		default:
75: 			sense = i ? OrderType::DESCENDING : OrderType::ASCENDING;
76: 			break;
77: 		}
78: 		lhs_orders[i].emplace_back(BoundOrderByNode(sense, OrderByNullType::NULLS_LAST, move(left)));
79: 		rhs_orders[i].emplace_back(BoundOrderByNode(sense, OrderByNullType::NULLS_LAST, move(right)));
80: 	}
81: 
82: 	for (idx_t i = 2; i < conditions.size(); ++i) {
83: 		auto &cond = conditions[i];
84: 		D_ASSERT(cond.left->return_type == cond.right->return_type);
85: 		join_key_types.push_back(cond.left->return_type);
86: 	}
87: 
88: 	children.push_back(move(left));
89: 	children.push_back(move(right));
90: }
91: 
92: //===--------------------------------------------------------------------===//
93: // Sink
94: //===--------------------------------------------------------------------===//
95: class IEJoinLocalState : public LocalSinkState {
96: public:
97: 	explicit IEJoinLocalState(const vector<JoinCondition> &conditions, const idx_t child) : has_null(0), count(0) {
98: 		// Initialize order clause expression executor and key DataChunk
99: 		vector<LogicalType> types;
100: 		for (const auto &cond : conditions) {
101: 			comparisons.emplace_back(cond.comparison);
102: 
103: 			const auto &expr = child ? cond.right : cond.left;
104: 			executor.AddExpression(*expr);
105: 
106: 			types.push_back(expr->return_type);
107: 		}
108: 		keys.Initialize(types);
109: 	}
110: 
111: 	//! The local sort state
112: 	LocalSortState local_sort_state;
113: 	//! Local copy of the sorting expression executor
114: 	ExpressionExecutor executor;
115: 	//! Holds a vector of incoming sorting columns
116: 	DataChunk keys;
117: 	//! The comparison list (for null merging)
118: 	vector<ExpressionType> comparisons;
119: 	//! The number of NULL values
120: 	idx_t has_null;
121: 	//! The total number of rows
122: 	idx_t count;
123: 
124: 	idx_t MergeKeyNulls();
125: 
126: 	void Sink(DataChunk &input, GlobalSortState &global_sort_state) {
127: 		// Initialize local state (if necessary)
128: 		if (!local_sort_state.initialized) {
129: 			local_sort_state.Initialize(global_sort_state, global_sort_state.buffer_manager);
130: 		}
131: 
132: 		// Obtain sorting columns
133: 		keys.Reset();
134: 		executor.Execute(input, keys);
135: 
136: 		// Count the NULLs so we can exclude them later
137: 		has_null += MergeKeyNulls();
138: 		count += keys.size();
139: 
140: 		// Sink the data into the local sort state
141: 		D_ASSERT(keys.ColumnCount() > 1);
142: 		//	Only sort the primary key
143: 		DataChunk join_head;
144: 		join_head.data.emplace_back(Vector(keys.data[0]));
145: 		join_head.SetCardinality(keys.size());
146: 
147: 		local_sort_state.SinkChunk(join_head, input);
148: 	}
149: 
150: 	void Sort(GlobalSortState &gss) {
151: 		local_sort_state.Sort(gss, true);
152: 	}
153: 	void Reset() {
154: 		has_null = 0;
155: 		count = 0;
156: 	}
157: };
158: 
159: idx_t IEJoinLocalState::MergeKeyNulls() {
160: 	// Merge the validity masks of the comparison keys into the primary
161: 	// Return the number of NULLs in the resulting chunk
162: 	D_ASSERT(keys.ColumnCount() > 0);
163: 	const auto count = keys.size();
164: 
165: 	size_t all_constant = 0;
166: 	for (auto &v : keys.data) {
167: 		all_constant += int(v.GetVectorType() == VectorType::CONSTANT_VECTOR);
168: 	}
169: 
170: 	auto &primary = keys.data[0];
171: 	if (all_constant == keys.data.size()) {
172: 		//	Either all NULL or no NULLs
173: 		for (auto &v : keys.data) {
174: 			if (ConstantVector::IsNull(v)) {
175: 				ConstantVector::SetNull(primary, true);
176: 				return count;
177: 			}
178: 		}
179: 		return 0;
180: 	} else if (keys.ColumnCount() > 1) {
181: 		//	Normalify the primary, as it will need to merge arbitrary validity masks
182: 		primary.Normalify(count);
183: 		auto &pvalidity = FlatVector::Validity(primary);
184: 		D_ASSERT(keys.ColumnCount() == comparisons.size());
185: 		for (size_t c = 1; c < keys.data.size(); ++c) {
186: 			// Skip comparisons that accept NULLs
187: 			if (comparisons[c] == ExpressionType::COMPARE_DISTINCT_FROM) {
188: 				continue;
189: 			}
190: 			//	Orrify the rest, as the sort code will do this anyway.
191: 			auto &v = keys.data[c];
192: 			VectorData vdata;
193: 			v.Orrify(count, vdata);
194: 			auto &vvalidity = vdata.validity;
195: 			if (vvalidity.AllValid()) {
196: 				continue;
197: 			}
198: 			pvalidity.EnsureWritable();
199: 			auto pmask = pvalidity.GetData();
200: 			if (v.GetVectorType() == VectorType::FLAT_VECTOR) {
201: 				//	Merge entire entries
202: 				const auto entry_count = pvalidity.EntryCount(count);
203: 				for (idx_t entry_idx = 0; entry_idx < entry_count; ++entry_idx) {
204: 					pmask[entry_idx] &= vvalidity.GetValidityEntry(entry_idx);
205: 				}
206: 			}
207: 		}
208: 		return count - pvalidity.CountValid(count);
209: 	} else {
210: 		return count - VectorOperations::CountNotNull(primary, count);
211: 	}
212: }
213: 
214: class IEJoinSortedTable {
215: public:
216: 	IEJoinSortedTable(ClientContext &context, const vector<BoundOrderByNode> &orders, RowLayout &payload_layout)
217: 	    : global_sort_state(BufferManager::GetBufferManager(context), orders, payload_layout), has_null(0), count(0),
218: 	      memory_per_thread(0) {
219: 		D_ASSERT(orders.size() == 1);
220: 
221: 		// Set external (can be force with the PRAGMA)
222: 		auto &config = ClientConfig::GetConfig(context);
223: 		global_sort_state.external = config.force_external;
224: 		// Memory usage per thread should scale with max mem / num threads
225: 		// We take 1/4th of this, to be conservative
226: 		idx_t max_memory = global_sort_state.buffer_manager.GetMaxMemory();
227: 		idx_t num_threads = TaskScheduler::GetScheduler(context).NumberOfThreads();
228: 		memory_per_thread = (max_memory / num_threads) / 4;
229: 	}
230: 
231: 	inline idx_t Count() const {
232: 		return count;
233: 	}
234: 
235: 	inline idx_t BlockCount() const {
236: 		if (global_sort_state.sorted_blocks.empty()) {
237: 			return 0;
238: 		}
239: 		D_ASSERT(global_sort_state.sorted_blocks.size() == 1);
240: 		return global_sort_state.sorted_blocks[0]->radix_sorting_data.size();
241: 	}
242: 
243: 	inline idx_t BlockSize(idx_t i) const {
244: 		return global_sort_state.sorted_blocks[0]->radix_sorting_data[i].count;
245: 	}
246: 
247: 	inline void Combine(IEJoinLocalState &lstate) {
248: 		global_sort_state.AddLocalState(lstate.local_sort_state);
249: 		has_null += lstate.has_null;
250: 		count += lstate.count;
251: 	}
252: 
253: 	inline void IntializeMatches() {
254: 		found_match = unique_ptr<bool[]>(new bool[Count()]);
255: 		memset(found_match.get(), 0, sizeof(bool) * Count());
256: 	}
257: 
258: 	void Print() {
259: 		PayloadScanner scanner(global_sort_state, false);
260: 		DataChunk chunk;
261: 		chunk.Initialize(scanner.GetPayloadTypes());
262: 		for (;;) {
263: 			scanner.Scan(chunk);
264: 			const auto count = chunk.size();
265: 			if (!count) {
266: 				break;
267: 			}
268: 			chunk.Print();
269: 		}
270: 	}
271: 
272: 	GlobalSortState global_sort_state;
273: 	//! Whether or not the RHS has NULL values
274: 	atomic<idx_t> has_null;
275: 	//! The total number of rows in the RHS
276: 	atomic<idx_t> count;
277: 	//! A bool indicating for each tuple in the RHS if they found a match (only used in FULL OUTER JOIN)
278: 	unique_ptr<bool[]> found_match;
279: 	//! Memory usage per thread
280: 	idx_t memory_per_thread;
281: };
282: 
283: class IEJoinGlobalState : public GlobalSinkState {
284: public:
285: 	IEJoinGlobalState(ClientContext &context, const PhysicalIEJoin &op) : child(0) {
286: 		tables.resize(2);
287: 		RowLayout lhs_layout;
288: 		lhs_layout.Initialize(op.children[0]->types);
289: 		vector<BoundOrderByNode> lhs_order;
290: 		lhs_order.emplace_back(op.lhs_orders[0][0].Copy());
291: 		tables[0] = make_unique<IEJoinSortedTable>(context, lhs_order, lhs_layout);
292: 
293: 		RowLayout rhs_layout;
294: 		rhs_layout.Initialize(op.children[1]->types);
295: 		vector<BoundOrderByNode> rhs_order;
296: 		rhs_order.emplace_back(op.rhs_orders[0][0].Copy());
297: 		tables[1] = make_unique<IEJoinSortedTable>(context, rhs_order, rhs_layout);
298: 	}
299: 
300: 	IEJoinGlobalState(IEJoinGlobalState &prev)
301: 	    : GlobalSinkState(prev), tables(move(prev.tables)), child(prev.child + 1) {
302: 	}
303: 
304: 	void Sink(DataChunk &input, IEJoinLocalState &lstate) {
305: 		auto &table = *tables[child];
306: 		auto &global_sort_state = table.global_sort_state;
307: 		auto &local_sort_state = lstate.local_sort_state;
308: 
309: 		// Sink the data into the local sort state
310: 		lstate.Sink(input, global_sort_state);
311: 
312: 		// When sorting data reaches a certain size, we sort it
313: 		if (local_sort_state.SizeInBytes() >= table.memory_per_thread) {
314: 			local_sort_state.Sort(global_sort_state, true);
315: 		}
316: 	}
317: 
318: 	vector<unique_ptr<IEJoinSortedTable>> tables;
319: 	size_t child;
320: };
321: 
322: unique_ptr<GlobalSinkState> PhysicalIEJoin::GetGlobalSinkState(ClientContext &context) const {
323: 	D_ASSERT(!sink_state);
324: 	return make_unique<IEJoinGlobalState>(context, *this);
325: }
326: 
327: unique_ptr<LocalSinkState> PhysicalIEJoin::GetLocalSinkState(ExecutionContext &context) const {
328: 	idx_t sink_child = 0;
329: 	if (sink_state) {
330: 		const auto &ie_sink = (IEJoinGlobalState &)*sink_state;
331: 		sink_child = ie_sink.child;
332: 	}
333: 	return make_unique<IEJoinLocalState>(conditions, sink_child);
334: }
335: 
336: SinkResultType PhysicalIEJoin::Sink(ExecutionContext &context, GlobalSinkState &gstate_p, LocalSinkState &lstate_p,
337:                                     DataChunk &input) const {
338: 	auto &gstate = (IEJoinGlobalState &)gstate_p;
339: 	auto &lstate = (IEJoinLocalState &)lstate_p;
340: 
341: 	gstate.Sink(input, lstate);
342: 
343: 	return SinkResultType::NEED_MORE_INPUT;
344: }
345: 
346: void PhysicalIEJoin::Combine(ExecutionContext &context, GlobalSinkState &gstate_p, LocalSinkState &lstate_p) const {
347: 	auto &gstate = (IEJoinGlobalState &)gstate_p;
348: 	auto &lstate = (IEJoinLocalState &)lstate_p;
349: 	gstate.tables[gstate.child]->Combine(lstate);
350: 	auto &client_profiler = QueryProfiler::Get(context.client);
351: 
352: 	context.thread.profiler.Flush(this, &lstate.executor, gstate.child ? "rhs_executor" : "lhs_executor", 1);
353: 	client_profiler.Flush(context.thread.profiler);
354: }
355: 
356: //===--------------------------------------------------------------------===//
357: // Finalize
358: //===--------------------------------------------------------------------===//
359: class IEJoinFinalizeTask : public ExecutorTask {
360: public:
361: 	IEJoinFinalizeTask(shared_ptr<Event> event_p, ClientContext &context, IEJoinSortedTable &table)
362: 	    : ExecutorTask(context), event(move(event_p)), context(context), table(table) {
363: 	}
364: 
365: 	TaskExecutionResult ExecuteTask(TaskExecutionMode mode) override {
366: 		// Initialize iejoin sorted and iterate until done
367: 		auto &global_sort_state = table.global_sort_state;
368: 		MergeSorter merge_sorter(global_sort_state, BufferManager::GetBufferManager(context));
369: 		merge_sorter.PerformInMergeRound();
370: 		event->FinishTask();
371: 
372: 		return TaskExecutionResult::TASK_FINISHED;
373: 	}
374: 
375: private:
376: 	shared_ptr<Event> event;
377: 	ClientContext &context;
378: 	IEJoinSortedTable &table;
379: };
380: 
381: class IEJoinFinalizeEvent : public Event {
382: public:
383: 	IEJoinFinalizeEvent(IEJoinSortedTable &table_p, Pipeline &pipeline_p)
384: 	    : Event(pipeline_p.executor), table(table_p), pipeline(pipeline_p) {
385: 	}
386: 
387: 	IEJoinSortedTable &table;
388: 	Pipeline &pipeline;
389: 
390: public:
391: 	void Schedule() override {
392: 		auto &context = pipeline.GetClientContext();
393: 
394: 		// Schedule tasks equal to the number of threads, which will each iejoin multiple partitions
395: 		auto &ts = TaskScheduler::GetScheduler(context);
396: 		idx_t num_threads = ts.NumberOfThreads();
397: 
398: 		vector<unique_ptr<Task>> iejoin_tasks;
399: 		for (idx_t tnum = 0; tnum < num_threads; tnum++) {
400: 			iejoin_tasks.push_back(make_unique<IEJoinFinalizeTask>(shared_from_this(), context, table));
401: 		}
402: 		SetTasks(move(iejoin_tasks));
403: 	}
404: 
405: 	void FinishEvent() override {
406: 		auto &global_sort_state = table.global_sort_state;
407: 
408: 		global_sort_state.CompleteMergeRound(true);
409: 		if (global_sort_state.sorted_blocks.size() > 1) {
410: 			// Multiple blocks remaining: Schedule the next round
411: 			PhysicalIEJoin::ScheduleMergeTasks(pipeline, *this, table);
412: 		}
413: 	}
414: };
415: 
416: void PhysicalIEJoin::ScheduleMergeTasks(Pipeline &pipeline, Event &event, IEJoinSortedTable &table) {
417: 	// Initialize global sort state for a round of merging
418: 	table.global_sort_state.InitializeMergeRound();
419: 	auto new_event = make_shared<IEJoinFinalizeEvent>(table, pipeline);
420: 	event.InsertEvent(move(new_event));
421: }
422: 
423: SinkFinalizeType PhysicalIEJoin::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
424:                                           GlobalSinkState &gstate_p) const {
425: 	auto &gstate = (IEJoinGlobalState &)gstate_p;
426: 	auto &table = *gstate.tables[gstate.child];
427: 	auto &global_sort_state = table.global_sort_state;
428: 
429: 	if ((gstate.child == 1 && IsRightOuterJoin(join_type)) || (gstate.child == 0 && IsLeftOuterJoin(join_type))) {
430: 		// for FULL/LEFT/RIGHT OUTER JOIN, initialize found_match to false for every tuple
431: 		table.IntializeMatches();
432: 	}
433: 	if (gstate.child == 1 && global_sort_state.sorted_blocks.empty() && EmptyResultIfRHSIsEmpty()) {
434: 		// Empty input!
435: 		return SinkFinalizeType::NO_OUTPUT_POSSIBLE;
436: 	}
437: 
438: 	// Prepare for child sort phase
439: 	global_sort_state.PrepareMergePhase();
440: 
441: 	// Start the iejoin phase or finish if a iejoin is not necessary
442: 	if (global_sort_state.sorted_blocks.size() > 1) {
443: 		PhysicalIEJoin::ScheduleMergeTasks(pipeline, event, table);
444: 	}
445: 
446: 	++gstate.child;
447: 
448: 	return SinkFinalizeType::READY;
449: }
450: 
451: //===--------------------------------------------------------------------===//
452: // Operator
453: //===--------------------------------------------------------------------===//
454: struct SBIterator {
455: 	static int ComparisonValue(ExpressionType comparison) {
456: 		switch (comparison) {
457: 		case ExpressionType::COMPARE_LESSTHAN:
458: 		case ExpressionType::COMPARE_GREATERTHAN:
459: 			return -1;
460: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
461: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
462: 			return 0;
463: 		default:
464: 			throw InternalException("Unimplemented comparison type for IEJoin!");
465: 		}
466: 	}
467: 
468: 	explicit SBIterator(GlobalSortState &gss, ExpressionType comparison, idx_t entry_idx_p = 0)
469: 	    : sort_layout(gss.sort_layout), block_count(gss.sorted_blocks[0]->radix_sorting_data.size()),
470: 	      block_capacity(gss.block_capacity), cmp_size(sort_layout.comparison_size), entry_size(sort_layout.entry_size),
471: 	      all_constant(sort_layout.all_constant), external(gss.external), cmp(ComparisonValue(comparison)),
472: 	      scan(gss.buffer_manager, gss), block_ptr(nullptr), entry_ptr(nullptr) {
473: 
474: 		scan.sb = gss.sorted_blocks[0].get();
475: 		scan.block_idx = block_count;
476: 		SetIndex(entry_idx_p);
477: 	}
478: 
479: 	inline idx_t GetIndex() const {
480: 		return entry_idx;
481: 	}
482: 
483: 	inline void SetIndex(idx_t entry_idx_p) {
484: 		const auto new_block_idx = entry_idx_p / block_capacity;
485: 		if (new_block_idx != scan.block_idx) {
486: 			scan.SetIndices(new_block_idx, 0);
487: 			if (new_block_idx < block_count) {
488: 				scan.PinRadix(scan.block_idx);
489: 				block_ptr = scan.RadixPtr();
490: 				if (!all_constant) {
491: 					scan.PinData(*scan.sb->blob_sorting_data);
492: 				}
493: 			}
494: 		}
495: 
496: 		scan.entry_idx = entry_idx_p % block_capacity;
497: 		entry_ptr = block_ptr + scan.entry_idx * entry_size;
498: 		entry_idx = entry_idx_p;
499: 	}
500: 
501: 	inline SBIterator &operator++() {
502: 		if (++scan.entry_idx < block_capacity) {
503: 			entry_ptr += entry_size;
504: 			++entry_idx;
505: 		} else {
506: 			SetIndex(entry_idx + 1);
507: 		}
508: 
509: 		return *this;
510: 	}
511: 
512: 	inline SBIterator &operator--() {
513: 		if (scan.entry_idx) {
514: 			--scan.entry_idx;
515: 			--entry_idx;
516: 			entry_ptr -= entry_size;
517: 		} else {
518: 			SetIndex(entry_idx - 1);
519: 		}
520: 
521: 		return *this;
522: 	}
523: 
524: 	inline bool Compare(const SBIterator &other) const {
525: 		int comp_res;
526: 		if (all_constant) {
527: 			comp_res = FastMemcmp(entry_ptr, other.entry_ptr, cmp_size);
528: 		} else {
529: 			comp_res = Comparators::CompareTuple(scan, other.scan, entry_ptr, other.entry_ptr, sort_layout, external);
530: 		}
531: 
532: 		return comp_res <= cmp;
533: 	}
534: 
535: 	// Fixed comparison parameters
536: 	const SortLayout &sort_layout;
537: 	const idx_t block_count;
538: 	const idx_t block_capacity;
539: 	const size_t cmp_size;
540: 	const size_t entry_size;
541: 	const bool all_constant;
542: 	const bool external;
543: 	const int cmp;
544: 
545: 	// Iteration state
546: 	SBScanState scan;
547: 	idx_t entry_idx;
548: 	data_ptr_t block_ptr;
549: 	data_ptr_t entry_ptr;
550: };
551: 
552: struct IEJoinUnion {
553: 	using SortedTable = IEJoinSortedTable;
554: 
555: 	static idx_t AppendKey(SortedTable &table, ExpressionExecutor &executor, SortedTable &marked, int64_t increment,
556: 	                       int64_t base, const idx_t block_idx);
557: 
558: 	static void Sort(SortedTable &table) {
559: 		auto &global_sort_state = table.global_sort_state;
560: 		global_sort_state.PrepareMergePhase();
561: 		while (global_sort_state.sorted_blocks.size() > 1) {
562: 			global_sort_state.InitializeMergeRound();
563: 			MergeSorter merge_sorter(global_sort_state, global_sort_state.buffer_manager);
564: 			merge_sorter.PerformInMergeRound();
565: 			global_sort_state.CompleteMergeRound(true);
566: 		}
567: 	}
568: 
569: 	template <typename T>
570: 	static vector<T> ExtractColumn(SortedTable &table, idx_t col_idx) {
571: 		vector<T> result;
572: 		result.reserve(table.count);
573: 
574: 		auto &gstate = table.global_sort_state;
575: 		auto &blocks = *gstate.sorted_blocks[0]->payload_data;
576: 		PayloadScanner scanner(blocks, gstate, false);
577: 
578: 		DataChunk payload;
579: 		payload.Initialize(gstate.payload_layout.GetTypes());
580: 		for (;;) {
581: 			scanner.Scan(payload);
582: 			const auto count = payload.size();
583: 			if (!count) {
584: 				break;
585: 			}
586: 
587: 			const auto data_ptr = FlatVector::GetData<T>(payload.data[col_idx]);
588: 			result.insert(result.end(), data_ptr, data_ptr + count);
589: 		}
590: 
591: 		return result;
592: 	}
593: 
594: 	IEJoinUnion(ClientContext &context, const PhysicalIEJoin &op, SortedTable &t1, const idx_t b1, SortedTable &t2,
595: 	            const idx_t b2);
596: 
597: 	bool NextRow();
598: 
599: 	//! Inverted loop
600: 	idx_t JoinComplexBlocks(SelectionVector &lsel, SelectionVector &rsel);
601: 
602: 	//! L1
603: 	unique_ptr<SortedTable> l1;
604: 	//! L2
605: 	unique_ptr<SortedTable> l2;
606: 
607: 	//! Li
608: 	vector<int64_t> li;
609: 	//! P
610: 	vector<idx_t> p;
611: 
612: 	//! B
613: 	vector<validity_t> bit_array;
614: 	ValidityMask bit_mask;
615: 
616: 	//! Bloom Filter
617: 	static constexpr idx_t BLOOM_CHUNK_BITS = 1024;
618: 	idx_t bloom_count;
619: 	vector<validity_t> bloom_array;
620: 	ValidityMask bloom_filter;
621: 
622: 	//! Iteration state
623: 	idx_t n;
624: 	idx_t i;
625: 	unique_ptr<SBIterator> op1;
626: 	unique_ptr<SBIterator> off1;
627: 	unique_ptr<SBIterator> op2;
628: 	unique_ptr<SBIterator> off2;
629: 	int64_t lrid;
630: };
631: 
632: idx_t IEJoinUnion::AppendKey(SortedTable &table, ExpressionExecutor &executor, SortedTable &marked, int64_t increment,
633:                              int64_t base, const idx_t block_idx) {
634: 	LocalSortState local_sort_state;
635: 	local_sort_state.Initialize(marked.global_sort_state, marked.global_sort_state.buffer_manager);
636: 
637: 	// Reading
638: 	const auto valid = table.count - table.has_null;
639: 	auto &gstate = table.global_sort_state;
640: 	PayloadScanner scanner(gstate, block_idx);
641: 	auto table_idx = block_idx * gstate.block_capacity;
642: 
643: 	DataChunk scanned;
644: 	scanned.Initialize(scanner.GetPayloadTypes());
645: 
646: 	// Writing
647: 	auto types = local_sort_state.sort_layout->logical_types;
648: 	const idx_t payload_idx = types.size();
649: 
650: 	const auto &payload_types = local_sort_state.payload_layout->GetTypes();
651: 	types.insert(types.end(), payload_types.begin(), payload_types.end());
652: 	const idx_t rid_idx = types.size() - 1;
653: 
654: 	DataChunk keys;
655: 	DataChunk payload;
656: 	keys.Initialize(types);
657: 
658: 	idx_t inserted = 0;
659: 	for (auto rid = base; table_idx < valid;) {
660: 		scanner.Scan(scanned);
661: 
662: 		// NULLs are at the end, so stop when we reach them
663: 		auto scan_count = scanned.size();
664: 		if (table_idx + scan_count > valid) {
665: 			scan_count = valid - table_idx;
666: 			scanned.SetCardinality(scan_count);
667: 		}
668: 		if (scan_count == 0) {
669: 			break;
670: 		}
671: 		table_idx += scan_count;
672: 
673: 		// Compute the input columns from the payload
674: 		keys.Reset();
675: 		keys.Split(payload, rid_idx);
676: 		executor.Execute(scanned, keys);
677: 
678: 		// Mark the rid column
679: 		payload.data[0].Sequence(rid, increment);
680: 		payload.SetCardinality(scan_count);
681: 		keys.Fuse(payload);
682: 		rid += increment * scan_count;
683: 
684: 		// Sort on the sort columns (which will no longer be needed)
685: 		keys.Split(payload, payload_idx);
686: 		local_sort_state.SinkChunk(keys, payload);
687: 		inserted += scan_count;
688: 		keys.Fuse(payload);
689: 
690: 		// Flush when we have enough data
691: 		if (local_sort_state.SizeInBytes() >= marked.memory_per_thread) {
692: 			local_sort_state.Sort(marked.global_sort_state, true);
693: 		}
694: 	}
695: 	marked.global_sort_state.AddLocalState(local_sort_state);
696: 	marked.count += inserted;
697: 
698: 	return inserted;
699: }
700: 
701: IEJoinUnion::IEJoinUnion(ClientContext &context, const PhysicalIEJoin &op, SortedTable &t1, const idx_t b1,
702:                          SortedTable &t2, const idx_t b2)
703:     : n(0), i(0) {
704: 	// input : query Q with 2 join predicates t1.X op1 t2.X' and t1.Y op2 t2.Y', tables T, T' of sizes m and n resp.
705: 	// output: a list of tuple pairs (ti , tj)
706: 	// Note that T/T' are already sorted on X/X' and contain the payload data
707: 	// We only join the two block numbers and use the sizes of the blocks as the counts
708: 
709: 	// 0. Filter out tables with no overlap
710: 	if (!t1.BlockSize(b1) || !t2.BlockSize(b2)) {
711: 		return;
712: 	}
713: 
714: 	const auto &cmp1 = op.conditions[0].comparison;
715: 	SBIterator bounds1(t1.global_sort_state, cmp1);
716: 	SBIterator bounds2(t2.global_sort_state, cmp1);
717: 
718: 	// t1.X[0] op1 t2.X'[-1]
719: 	bounds1.SetIndex(bounds1.block_capacity * b1);
720: 	bounds2.SetIndex(bounds2.block_capacity * b2 + t2.BlockSize(b2) - 1);
721: 	if (!bounds1.Compare(bounds2)) {
722: 		return;
723: 	}
724: 
725: 	// 1. let L1 (resp. L2) be the array of column X (resp. Y )
726: 	const auto &order1 = op.lhs_orders[0][0];
727: 	const auto &order2 = op.lhs_orders[1][0];
728: 
729: 	// 2. if (op1  {>, }) sort L1 in descending order
730: 	// 3. else if (op1  {<, }) sort L1 in ascending order
731: 
732: 	// For the union algorithm, we make a unified table with the keys and the rids as the payload:
733: 	//		X/X', Y/Y', R/R'/Li
734: 	// The first position is the sort key.
735: 	vector<LogicalType> types;
736: 	types.emplace_back(order2.expression->return_type);
737: 	types.emplace_back(LogicalType::BIGINT);
738: 	RowLayout payload_layout;
739: 	payload_layout.Initialize(types);
740: 
741: 	// Sort on the first expression
742: 	auto ref = make_unique<BoundReferenceExpression>(order1.expression->return_type, 0);
743: 	vector<BoundOrderByNode> orders;
744: 	orders.emplace_back(BoundOrderByNode(order1.type, order1.null_order, move(ref)));
745: 
746: 	l1 = make_unique<SortedTable>(context, orders, payload_layout);
747: 
748: 	// LHS has positive rids
749: 	ExpressionExecutor l_executor;
750: 	l_executor.AddExpression(*order1.expression);
751: 	l_executor.AddExpression(*order2.expression);
752: 	AppendKey(t1, l_executor, *l1, 1, 1, b1);
753: 
754: 	// RHS has negative rids
755: 	ExpressionExecutor r_executor;
756: 	r_executor.AddExpression(*op.rhs_orders[0][0].expression);
757: 	r_executor.AddExpression(*op.rhs_orders[1][0].expression);
758: 	AppendKey(t2, r_executor, *l1, -1, -1, b2);
759: 
760: 	Sort(*l1);
761: 
762: 	op1 = make_unique<SBIterator>(l1->global_sort_state, cmp1);
763: 	off1 = make_unique<SBIterator>(l1->global_sort_state, cmp1);
764: 
765: 	// We don't actually need the L1 column, just its sort key, which is in the sort blocks
766: 	li = ExtractColumn<int64_t>(*l1, types.size() - 1);
767: 
768: 	// 4. if (op2  {>, }) sort L2 in ascending order
769: 	// 5. else if (op2  {<, }) sort L2 in descending order
770: 
771: 	// We sort on Y/Y' to obtain the sort keys and the permutation array.
772: 	// For this we just need a two-column table of Y, P
773: 	types.clear();
774: 	types.emplace_back(LogicalType::BIGINT);
775: 	payload_layout.Initialize(types);
776: 
777: 	// Sort on the first expression
778: 	orders.clear();
779: 	ref = make_unique<BoundReferenceExpression>(order2.expression->return_type, 0);
780: 	orders.emplace_back(BoundOrderByNode(order2.type, order2.null_order, move(ref)));
781: 
782: 	ExpressionExecutor executor;
783: 	executor.AddExpression(*orders[0].expression);
784: 
785: 	l2 = make_unique<SortedTable>(context, orders, payload_layout);
786: 	for (idx_t base = 0, block_idx = 0; block_idx < l1->BlockCount(); ++block_idx) {
787: 		base += AppendKey(*l1, executor, *l2, 1, base, block_idx);
788: 	}
789: 
790: 	Sort(*l2);
791: 
792: 	// We don't actually need the L2 column, just its sort key, which is in the sort blocks
793: 
794: 	// 6. compute the permutation array P of L2 w.r.t. L1
795: 	p = ExtractColumn<idx_t>(*l2, types.size() - 1);
796: 
797: 	// 7. initialize bit-array B (|B| = n), and set all bits to 0
798: 	n = l2->count.load();
799: 	bit_array.resize(ValidityMask::EntryCount(n), 0);
800: 	bit_mask.Initialize(bit_array.data());
801: 
802: 	// Bloom filter
803: 	bloom_count = (n + (BLOOM_CHUNK_BITS - 1)) / BLOOM_CHUNK_BITS;
804: 	bloom_array.resize(ValidityMask::EntryCount(bloom_count), 0);
805: 	bloom_filter.Initialize(bloom_array.data());
806: 
807: 	// 11. for(i1 to n) do
808: 	const auto &cmp2 = op.conditions[1].comparison;
809: 	op2 = make_unique<SBIterator>(l2->global_sort_state, cmp2);
810: 	off2 = make_unique<SBIterator>(l2->global_sort_state, cmp2);
811: 	i = 0;
812: 	(void)NextRow();
813: }
814: 
815: bool IEJoinUnion::NextRow() {
816: 	for (; i < n; ++i) {
817: 		// 12. pos  P[i]
818: 		auto pos = p[i];
819: 		lrid = li[pos];
820: 		if (lrid < 0) {
821: 			continue;
822: 		}
823: 
824: 		// 16. B[pos]  1
825: 		op2->SetIndex(i);
826: 		for (; off2->GetIndex() < n; ++(*off2)) {
827: 			if (!off2->Compare(*op2)) {
828: 				break;
829: 			}
830: 			const auto p2 = p[off2->GetIndex()];
831: 			bit_mask.SetValid(p2);
832: 			bloom_filter.SetValid(p2 / BLOOM_CHUNK_BITS);
833: 		}
834: 
835: 		// 9.  if (op1  {,} and op2  {,}) eqOff = 0
836: 		// 10. else eqOff = 1
837: 		// No, because there could be more than one equal value.
838: 		// Scan the neighborhood instead
839: 		op1->SetIndex(pos);
840: 		off1->SetIndex(pos);
841: 		for (; off1->GetIndex() > 0 && op1->Compare(*off1); --(*off1)) {
842: 			continue;
843: 		}
844: 		for (; off1->GetIndex() < n && !op1->Compare(*off1); ++(*off1)) {
845: 			continue;
846: 		}
847: 
848: 		return true;
849: 	}
850: 	return false;
851: }
852: 
853: static idx_t NextValid(const ValidityMask &bits, idx_t j, const idx_t n) {
854: 	if (j >= n) {
855: 		return n;
856: 	}
857: 
858: 	// We can do a first approximation by checking entries one at a time
859: 	// which gives 64:1.
860: 	idx_t entry_idx, idx_in_entry;
861: 	bits.GetEntryIndex(j, entry_idx, idx_in_entry);
862: 	auto entry = bits.GetValidityEntry(entry_idx++);
863: 
864: 	// Trim the bits before the start position
865: 	entry &= (ValidityMask::ValidityBuffer::MAX_ENTRY << idx_in_entry);
866: 
867: 	// Check the non-ragged entries
868: 	for (const auto entry_count = bits.EntryCount(n); entry_idx < entry_count; ++entry_idx) {
869: 		if (entry) {
870: 			for (; idx_in_entry < bits.BITS_PER_VALUE; ++idx_in_entry, ++j) {
871: 				if (bits.RowIsValid(entry, idx_in_entry)) {
872: 					return j;
873: 				}
874: 			}
875: 		} else {
876: 			j += bits.BITS_PER_VALUE - idx_in_entry;
877: 		}
878: 
879: 		entry = bits.GetValidityEntry(entry_idx);
880: 		idx_in_entry = 0;
881: 	}
882: 
883: 	// Check the final entry
884: 	for (; j < n; ++idx_in_entry, ++j) {
885: 		if (bits.RowIsValid(entry, idx_in_entry)) {
886: 			return j;
887: 		}
888: 	}
889: 
890: 	return j;
891: }
892: 
893: idx_t IEJoinUnion::JoinComplexBlocks(SelectionVector &lsel, SelectionVector &rsel) {
894: 	// 8. initialize join result as an empty list for tuple pairs
895: 	idx_t result_count = 0;
896: 
897: 	// 11. for(i1 to n) do
898: 	while (i < n) {
899: 		// 13. for (j  pos+eqOff to n) do
900: 		for (;;) {
901: 			// 14. if B[j] = 1 then
902: 			auto j = off1->GetIndex();
903: 
904: 			//	Use the Bloom filter to find candidate blocks
905: 			while (j < n) {
906: 				auto bloom_begin = NextValid(bloom_filter, j / BLOOM_CHUNK_BITS, bloom_count) * BLOOM_CHUNK_BITS;
907: 				auto bloom_end = MinValue<idx_t>(n, bloom_begin + BLOOM_CHUNK_BITS);
908: 
909: 				j = MaxValue<idx_t>(j, bloom_begin);
910: 				j = NextValid(bit_mask, j, bloom_end);
911: 				if (j < bloom_end) {
912: 					break;
913: 				}
914: 			}
915: 
916: 			if (j >= n) {
917: 				break;
918: 			}
919: 			off1->SetIndex(j + 1);
920: 
921: 			// Filter out tuples with the same sign (they come from the same table)
922: 			const auto rrid = li[j];
923: 
924: 			// 15. add tuples w.r.t. (L1[j], L1[i]) to join result
925: 			if (lrid > 0 && rrid < 0) {
926: 				lsel.set_index(result_count, sel_t(+lrid - 1));
927: 				rsel.set_index(result_count, sel_t(-rrid - 1));
928: 				++result_count;
929: 				if (result_count == STANDARD_VECTOR_SIZE) {
930: 					// out of space!
931: 					return result_count;
932: 				}
933: 			}
934: 		}
935: 		++i;
936: 
937: 		if (!NextRow()) {
938: 			break;
939: 		}
940: 	}
941: 
942: 	return result_count;
943: }
944: 
945: class IEJoinState : public OperatorState {
946: public:
947: 	explicit IEJoinState(const PhysicalIEJoin &op) : local_left(op.conditions, 0) {};
948: 
949: 	IEJoinLocalState local_left;
950: };
951: 
952: static void SliceSortedPayload(DataChunk &payload, GlobalSortState &state, const idx_t block_idx,
953:                                const SelectionVector &result, const idx_t result_count, const idx_t left_cols = 0) {
954: 	// There should only be one sorted block if they have been sorted
955: 	D_ASSERT(state.sorted_blocks.size() == 1);
956: 	SBScanState read_state(state.buffer_manager, state);
957: 	read_state.sb = state.sorted_blocks[0].get();
958: 	auto &sorted_data = *read_state.sb->payload_data;
959: 
960: 	read_state.SetIndices(block_idx, 0);
961: 	read_state.PinData(sorted_data);
962: 	const auto data_ptr = read_state.DataPtr(sorted_data);
963: 
964: 	// Set up a batch of pointers to scan data from
965: 	Vector addresses(LogicalType::POINTER, result_count);
966: 	auto data_pointers = FlatVector::GetData<data_ptr_t>(addresses);
967: 
968: 	// Set up the data pointers for the values that are actually referenced
969: 	const idx_t &row_width = sorted_data.layout.GetRowWidth();
970: 
971: 	auto prev_idx = result.get_index(0);
972: 	SelectionVector gsel(result_count);
973: 	idx_t addr_count = 0;
974: 	gsel.set_index(0, addr_count);
975: 	data_pointers[addr_count] = data_ptr + prev_idx * row_width;
976: 	for (idx_t i = 1; i < result_count; ++i) {
977: 		const auto row_idx = result.get_index(i);
978: 		if (row_idx != prev_idx) {
979: 			data_pointers[++addr_count] = data_ptr + row_idx * row_width;
980: 			prev_idx = row_idx;
981: 		}
982: 		gsel.set_index(i, addr_count);
983: 	}
984: 	++addr_count;
985: 
986: 	// Unswizzle the offsets back to pointers (if needed)
987: 	if (!sorted_data.layout.AllConstant() && state.external) {
988: 		RowOperations::UnswizzlePointers(sorted_data.layout, data_ptr, read_state.payload_heap_handle->Ptr(),
989: 		                                 addr_count);
990: 	}
991: 
992: 	// Deserialize the payload data
993: 	auto sel = FlatVector::IncrementalSelectionVector();
994: 	for (idx_t col_idx = 0; col_idx < sorted_data.layout.ColumnCount(); col_idx++) {
995: 		const auto col_offset = sorted_data.layout.GetOffsets()[col_idx];
996: 		auto &col = payload.data[left_cols + col_idx];
997: 		RowOperations::Gather(addresses, *sel, col, *sel, addr_count, col_offset, col_idx);
998: 		col.Slice(gsel, result_count);
999: 	}
1000: }
1001: 
1002: class IEJoinLocalSourceState : public LocalSourceState {
1003: public:
1004: 	explicit IEJoinLocalSourceState(const PhysicalIEJoin &op)
1005: 	    : op(op), true_sel(STANDARD_VECTOR_SIZE), left_matches(nullptr), right_matches(nullptr) {
1006: 
1007: 		if (op.conditions.size() < 3) {
1008: 			return;
1009: 		}
1010: 
1011: 		vector<LogicalType> left_types;
1012: 		vector<LogicalType> right_types;
1013: 		for (idx_t i = 2; i < op.conditions.size(); ++i) {
1014: 			const auto &cond = op.conditions[i];
1015: 
1016: 			left_types.push_back(cond.left->return_type);
1017: 			left_executor.AddExpression(*cond.left);
1018: 
1019: 			right_types.push_back(cond.left->return_type);
1020: 			right_executor.AddExpression(*cond.right);
1021: 		}
1022: 
1023: 		left_keys.Initialize(left_types);
1024: 		right_keys.Initialize(right_types);
1025: 	}
1026: 
1027: 	idx_t SelectJoinTail(const ExpressionType &condition, Vector &left, Vector &right, const SelectionVector *sel,
1028: 	                     idx_t count);
1029: 
1030: 	idx_t SelectOuterRows(bool *matches) {
1031: 		idx_t count = 0;
1032: 		for (; outer_idx < outer_count; ++outer_idx) {
1033: 			if (!matches[outer_idx]) {
1034: 				true_sel.set_index(count++, outer_idx);
1035: 				if (count >= STANDARD_VECTOR_SIZE) {
1036: 					break;
1037: 				}
1038: 			}
1039: 		}
1040: 
1041: 		return count;
1042: 	}
1043: 
1044: 	const PhysicalIEJoin &op;
1045: 
1046: 	// Joining
1047: 	unique_ptr<IEJoinUnion> joiner;
1048: 
1049: 	idx_t left_base;
1050: 	idx_t left_block_index;
1051: 
1052: 	idx_t right_base;
1053: 	idx_t right_block_index;
1054: 
1055: 	// Trailing predicates
1056: 	SelectionVector true_sel;
1057: 
1058: 	ExpressionExecutor left_executor;
1059: 	DataChunk left_keys;
1060: 
1061: 	ExpressionExecutor right_executor;
1062: 	DataChunk right_keys;
1063: 
1064: 	// Outer joins
1065: 	idx_t outer_idx;
1066: 	idx_t outer_count;
1067: 	bool *left_matches;
1068: 	bool *right_matches;
1069: };
1070: 
1071: idx_t IEJoinLocalSourceState::SelectJoinTail(const ExpressionType &condition, Vector &left, Vector &right,
1072:                                              const SelectionVector *sel, idx_t count) {
1073: 	switch (condition) {
1074: 	case ExpressionType::COMPARE_NOTEQUAL:
1075: 		return VectorOperations::NotEquals(left, right, sel, count, &true_sel, nullptr);
1076: 	case ExpressionType::COMPARE_LESSTHAN:
1077: 		return VectorOperations::LessThan(left, right, sel, count, &true_sel, nullptr);
1078: 	case ExpressionType::COMPARE_GREATERTHAN:
1079: 		return VectorOperations::GreaterThan(left, right, sel, count, &true_sel, nullptr);
1080: 	case ExpressionType::COMPARE_LESSTHANOREQUALTO:
1081: 		return VectorOperations::LessThanEquals(left, right, sel, count, &true_sel, nullptr);
1082: 	case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
1083: 		return VectorOperations::GreaterThanEquals(left, right, sel, count, &true_sel, nullptr);
1084: 	case ExpressionType::COMPARE_DISTINCT_FROM:
1085: 		return VectorOperations::DistinctFrom(left, right, sel, count, &true_sel, nullptr);
1086: 	case ExpressionType::COMPARE_NOT_DISTINCT_FROM:
1087: 	case ExpressionType::COMPARE_EQUAL:
1088: 	default:
1089: 		throw InternalException("Unsupported comparison type for PhysicalIEJoin");
1090: 	}
1091: 
1092: 	return count;
1093: }
1094: 
1095: void PhysicalIEJoin::ResolveComplexJoin(ExecutionContext &context, DataChunk &chunk, LocalSourceState &state_p) const {
1096: 	auto &state = (IEJoinLocalSourceState &)state_p;
1097: 	auto &ie_sink = (IEJoinGlobalState &)*sink_state;
1098: 	auto &left_table = *ie_sink.tables[0];
1099: 	auto &right_table = *ie_sink.tables[1];
1100: 
1101: 	const auto left_cols = children[0]->GetTypes().size();
1102: 	do {
1103: 		SelectionVector lsel(STANDARD_VECTOR_SIZE);
1104: 		SelectionVector rsel(STANDARD_VECTOR_SIZE);
1105: 		auto result_count = state.joiner->JoinComplexBlocks(lsel, rsel);
1106: 		if (result_count == 0) {
1107: 			// exhausted this pair
1108: 			return;
1109: 		}
1110: 
1111: 		// found matches: extract them
1112: 		chunk.Reset();
1113: 		SliceSortedPayload(chunk, left_table.global_sort_state, state.left_block_index, lsel, result_count, 0);
1114: 		SliceSortedPayload(chunk, right_table.global_sort_state, state.right_block_index, rsel, result_count,
1115: 		                   left_cols);
1116: 		chunk.SetCardinality(result_count);
1117: 
1118: 		auto sel = FlatVector::IncrementalSelectionVector();
1119: 		if (conditions.size() > 2) {
1120: 			// If there are more expressions to compute,
1121: 			// split the result chunk into the left and right halves
1122: 			// so we can compute the values for comparison.
1123: 			const auto tail_cols = conditions.size() - 2;
1124: 
1125: 			DataChunk right_chunk;
1126: 			chunk.Split(right_chunk, left_cols);
1127: 			state.left_executor.SetChunk(chunk);
1128: 			state.right_executor.SetChunk(right_chunk);
1129: 
1130: 			auto tail_count = result_count;
1131: 			for (size_t cmp_idx = 0; cmp_idx < tail_cols; ++cmp_idx) {
1132: 				auto &left = state.left_keys.data[cmp_idx];
1133: 				state.left_executor.ExecuteExpression(cmp_idx, left);
1134: 
1135: 				auto &right = state.right_keys.data[cmp_idx];
1136: 				state.right_executor.ExecuteExpression(cmp_idx, right);
1137: 
1138: 				if (tail_count < result_count) {
1139: 					left.Slice(*sel, tail_count);
1140: 					right.Slice(*sel, tail_count);
1141: 				}
1142: 				tail_count = state.SelectJoinTail(conditions[cmp_idx + 2].comparison, left, right, sel, tail_count);
1143: 				sel = &state.true_sel;
1144: 			}
1145: 			chunk.Fuse(right_chunk);
1146: 
1147: 			if (tail_count < result_count) {
1148: 				result_count = tail_count;
1149: 				chunk.Slice(*sel, result_count);
1150: 			}
1151: 		}
1152: 
1153: 		// found matches: mark the found matches if required
1154: 		if (left_table.found_match) {
1155: 			for (idx_t i = 0; i < result_count; i++) {
1156: 				left_table.found_match[state.left_base + lsel[sel->get_index(i)]] = true;
1157: 			}
1158: 		}
1159: 		if (right_table.found_match) {
1160: 			for (idx_t i = 0; i < result_count; i++) {
1161: 				right_table.found_match[state.right_base + rsel[sel->get_index(i)]] = true;
1162: 			}
1163: 		}
1164: 		chunk.Verify();
1165: 	} while (chunk.size() == 0);
1166: }
1167: 
1168: OperatorResultType PhysicalIEJoin::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
1169:                                            OperatorState &state) const {
1170: 	return OperatorResultType::FINISHED;
1171: }
1172: 
1173: //===--------------------------------------------------------------------===//
1174: // Source
1175: //===--------------------------------------------------------------------===//
1176: class IEJoinGlobalSourceState : public GlobalSourceState {
1177: public:
1178: 	explicit IEJoinGlobalSourceState(const PhysicalIEJoin &op)
1179: 	    : op(op), initialized(false), next_pair(0), completed(0), left_outers(0), next_left(0), right_outers(0),
1180: 	      next_right(0) {
1181: 	}
1182: 
1183: 	void Initialize(IEJoinGlobalState &sink_state) {
1184: 		lock_guard<mutex> initializing(lock);
1185: 		if (initialized) {
1186: 			return;
1187: 		}
1188: 
1189: 		// Compute the starting row for reach block
1190: 		// (In theory these are all the same size, but you never know...)
1191: 		auto &left_table = *sink_state.tables[0];
1192: 		const auto left_blocks = left_table.BlockCount();
1193: 		idx_t left_base = 0;
1194: 
1195: 		for (size_t lhs = 0; lhs < left_blocks; ++lhs) {
1196: 			left_bases.emplace_back(left_base);
1197: 			left_base += left_table.BlockSize(lhs);
1198: 		}
1199: 
1200: 		auto &right_table = *sink_state.tables[1];
1201: 		const auto right_blocks = right_table.BlockCount();
1202: 		idx_t right_base = 0;
1203: 		for (size_t rhs = 0; rhs < right_blocks; ++rhs) {
1204: 			right_bases.emplace_back(right_base);
1205: 			right_base += right_table.BlockSize(rhs);
1206: 		}
1207: 
1208: 		// Outer join block counts
1209: 		if (left_table.found_match) {
1210: 			left_outers = left_blocks;
1211: 		}
1212: 
1213: 		if (right_table.found_match) {
1214: 			right_outers = right_blocks;
1215: 		}
1216: 
1217: 		// Ready for action
1218: 		initialized = true;
1219: 	}
1220: 
1221: public:
1222: 	idx_t MaxThreads() override {
1223: 		// We can't leverage any more threads than block pairs.
1224: 		const auto &sink_state = ((IEJoinGlobalState &)*op.sink_state);
1225: 		return sink_state.tables[0]->BlockCount() * sink_state.tables[1]->BlockCount();
1226: 	}
1227: 
1228: 	void GetNextPair(ClientContext &client, IEJoinGlobalState &gstate, IEJoinLocalSourceState &lstate) {
1229: 		auto &left_table = *gstate.tables[0];
1230: 		auto &right_table = *gstate.tables[1];
1231: 
1232: 		const auto left_blocks = left_table.BlockCount();
1233: 		const auto right_blocks = right_table.BlockCount();
1234: 		const auto pair_count = left_blocks * right_blocks;
1235: 
1236: 		// Regular block
1237: 		const auto i = next_pair++;
1238: 		if (i < pair_count) {
1239: 			const auto b1 = i / right_blocks;
1240: 			const auto b2 = i % right_blocks;
1241: 
1242: 			lstate.left_block_index = b1;
1243: 			lstate.left_base = left_bases[b1];
1244: 
1245: 			lstate.right_block_index = b2;
1246: 			lstate.right_base = right_bases[b2];
1247: 
1248: 			lstate.joiner = make_unique<IEJoinUnion>(client, op, left_table, b1, right_table, b2);
1249: 			return;
1250: 		} else {
1251: 			--next_pair;
1252: 		}
1253: 
1254: 		// Outer joins
1255: 		if (!left_outers && !right_outers) {
1256: 			return;
1257: 		}
1258: 
1259: 		// Spin wait for regular blocks to finish(!)
1260: 		while (completed < pair_count) {
1261: 			std::this_thread::yield();
1262: 		}
1263: 
1264: 		// Left outer blocks
1265: 		const auto l = next_left++;
1266: 		if (l < left_outers) {
1267: 			lstate.left_block_index = l;
1268: 			lstate.left_base = left_bases[l];
1269: 
1270: 			lstate.left_matches = left_table.found_match.get() + lstate.left_base;
1271: 			lstate.outer_idx = 0;
1272: 			lstate.outer_count = left_table.BlockSize(l);
1273: 			return;
1274: 		} else {
1275: 			lstate.left_matches = nullptr;
1276: 			--next_left;
1277: 		}
1278: 
1279: 		// Right outer block
1280: 		const auto r = next_right++;
1281: 		if (r < right_outers) {
1282: 			lstate.right_block_index = r;
1283: 			lstate.right_base = right_bases[r];
1284: 
1285: 			lstate.right_matches = right_table.found_match.get() + lstate.right_base;
1286: 			lstate.outer_idx = 0;
1287: 			lstate.outer_count = right_table.BlockSize(r);
1288: 			return;
1289: 		} else {
1290: 			lstate.right_matches = nullptr;
1291: 			--next_right;
1292: 		}
1293: 	}
1294: 
1295: 	void PairCompleted(ClientContext &client, IEJoinGlobalState &gstate, IEJoinLocalSourceState &lstate) {
1296: 		lstate.joiner.reset();
1297: 		++completed;
1298: 		GetNextPair(client, gstate, lstate);
1299: 	}
1300: 
1301: 	const PhysicalIEJoin &op;
1302: 
1303: 	mutex lock;
1304: 	bool initialized;
1305: 
1306: 	// Join queue state
1307: 	std::atomic<size_t> next_pair;
1308: 	std::atomic<size_t> completed;
1309: 
1310: 	// Block base row number
1311: 	vector<idx_t> left_bases;
1312: 	vector<idx_t> right_bases;
1313: 
1314: 	// Outer joins
1315: 	idx_t left_outers;
1316: 	std::atomic<idx_t> next_left;
1317: 
1318: 	idx_t right_outers;
1319: 	std::atomic<idx_t> next_right;
1320: };
1321: 
1322: unique_ptr<GlobalSourceState> PhysicalIEJoin::GetGlobalSourceState(ClientContext &context) const {
1323: 	return make_unique<IEJoinGlobalSourceState>(*this);
1324: }
1325: 
1326: unique_ptr<LocalSourceState> PhysicalIEJoin::GetLocalSourceState(ExecutionContext &context,
1327:                                                                  GlobalSourceState &gstate) const {
1328: 	return make_unique<IEJoinLocalSourceState>(*this);
1329: }
1330: 
1331: void PhysicalIEJoin::GetData(ExecutionContext &context, DataChunk &result, GlobalSourceState &gstate,
1332:                              LocalSourceState &lstate) const {
1333: 	auto &ie_sink = (IEJoinGlobalState &)*sink_state;
1334: 	auto &ie_gstate = (IEJoinGlobalSourceState &)gstate;
1335: 	auto &ie_lstate = (IEJoinLocalSourceState &)lstate;
1336: 
1337: 	ie_gstate.Initialize(ie_sink);
1338: 
1339: 	if (!ie_lstate.joiner) {
1340: 		ie_gstate.GetNextPair(context.client, ie_sink, ie_lstate);
1341: 	}
1342: 
1343: 	// Process INNER results
1344: 	while (ie_lstate.joiner) {
1345: 		ResolveComplexJoin(context, result, ie_lstate);
1346: 
1347: 		if (result.size()) {
1348: 			return;
1349: 		}
1350: 
1351: 		ie_gstate.PairCompleted(context.client, ie_sink, ie_lstate);
1352: 	}
1353: 
1354: 	// Process LEFT OUTER results
1355: 	const auto left_cols = children[0]->GetTypes().size();
1356: 	while (ie_lstate.left_matches) {
1357: 		const idx_t count = ie_lstate.SelectOuterRows(ie_lstate.left_matches);
1358: 		if (!count) {
1359: 			ie_gstate.GetNextPair(context.client, ie_sink, ie_lstate);
1360: 			continue;
1361: 		}
1362: 
1363: 		SliceSortedPayload(result, ie_sink.tables[0]->global_sort_state, ie_lstate.left_base, ie_lstate.true_sel,
1364: 		                   count);
1365: 
1366: 		// Fill in NULLs to the right
1367: 		for (auto col_idx = left_cols; col_idx < result.ColumnCount(); ++col_idx) {
1368: 			result.data[col_idx].SetVectorType(VectorType::CONSTANT_VECTOR);
1369: 			ConstantVector::SetNull(result.data[col_idx], true);
1370: 		}
1371: 
1372: 		result.SetCardinality(count);
1373: 		result.Verify();
1374: 
1375: 		return;
1376: 	}
1377: 
1378: 	// Process RIGHT OUTER results
1379: 	while (ie_lstate.right_matches) {
1380: 		const idx_t count = ie_lstate.SelectOuterRows(ie_lstate.right_matches);
1381: 		if (!count) {
1382: 			ie_gstate.GetNextPair(context.client, ie_sink, ie_lstate);
1383: 		}
1384: 
1385: 		SliceSortedPayload(result, ie_sink.tables[1]->global_sort_state, ie_lstate.right_base, ie_lstate.true_sel,
1386: 		                   count, left_cols);
1387: 
1388: 		// Fill in NULLs to the left
1389: 		for (idx_t col_idx = 0; col_idx < left_cols; ++col_idx) {
1390: 			result.data[col_idx].SetVectorType(VectorType::CONSTANT_VECTOR);
1391: 			ConstantVector::SetNull(result.data[col_idx], true);
1392: 		}
1393: 
1394: 		result.SetCardinality(count);
1395: 		result.Verify();
1396: 
1397: 		return;
1398: 	}
1399: }
1400: 
1401: } // namespace duckdb
[end of src/execution/operator/join/physical_iejoin.cpp]
[start of src/execution/operator/join/physical_index_join.cpp]
1: #include "duckdb/execution/operator/join/physical_index_join.hpp"
2: #include "duckdb/parallel/thread_context.hpp"
3: #include "duckdb/common/vector_operations/vector_operations.hpp"
4: #include "duckdb/execution/expression_executor.hpp"
5: #include "duckdb/execution/index/art/art.hpp"
6: #include "duckdb/execution/operator/scan/physical_table_scan.hpp"
7: #include "duckdb/function/table/table_scan.hpp"
8: #include "duckdb/storage/buffer_manager.hpp"
9: #include "duckdb/storage/storage_manager.hpp"
10: #include "duckdb/transaction/transaction.hpp"
11: #include "duckdb/storage/table/append_state.hpp"
12: 
13: namespace duckdb {
14: 
15: class IndexJoinOperatorState : public OperatorState {
16: public:
17: 	explicit IndexJoinOperatorState(const PhysicalIndexJoin &op) {
18: 		rhs_rows.resize(STANDARD_VECTOR_SIZE);
19: 		result_sizes.resize(STANDARD_VECTOR_SIZE);
20: 
21: 		join_keys.Initialize(op.condition_types);
22: 		for (auto &cond : op.conditions) {
23: 			probe_executor.AddExpression(*cond.left);
24: 		}
25: 		if (!op.fetch_types.empty()) {
26: 			rhs_chunk.Initialize(op.fetch_types);
27: 		}
28: 		rhs_sel.Initialize(STANDARD_VECTOR_SIZE);
29: 	}
30: 
31: 	bool first_fetch = true;
32: 	idx_t lhs_idx = 0;
33: 	idx_t rhs_idx = 0;
34: 	idx_t result_size = 0;
35: 	vector<idx_t> result_sizes;
36: 	DataChunk join_keys;
37: 	DataChunk rhs_chunk;
38: 	SelectionVector rhs_sel;
39: 	//! Vector of rows that mush be fetched for every LHS key
40: 	vector<vector<row_t>> rhs_rows;
41: 	ExpressionExecutor probe_executor;
42: 
43: public:
44: 	void Finalize(PhysicalOperator *op, ExecutionContext &context) override {
45: 		context.thread.profiler.Flush(op, &probe_executor, "probe_executor", 0);
46: 	}
47: };
48: 
49: PhysicalIndexJoin::PhysicalIndexJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left,
50:                                      unique_ptr<PhysicalOperator> right, vector<JoinCondition> cond, JoinType join_type,
51:                                      const vector<idx_t> &left_projection_map_p, vector<idx_t> right_projection_map_p,
52:                                      vector<column_t> column_ids_p, Index *index_p, bool lhs_first,
53:                                      idx_t estimated_cardinality)
54:     : PhysicalOperator(PhysicalOperatorType::INDEX_JOIN, move(op.types), estimated_cardinality),
55:       left_projection_map(left_projection_map_p), right_projection_map(move(right_projection_map_p)), index(index_p),
56:       conditions(move(cond)), join_type(join_type), lhs_first(lhs_first) {
57: 	column_ids = move(column_ids_p);
58: 	children.push_back(move(left));
59: 	children.push_back(move(right));
60: 	for (auto &condition : conditions) {
61: 		condition_types.push_back(condition.left->return_type);
62: 	}
63: 	//! Only add to fetch_ids columns that are not indexed
64: 	for (auto &index_id : index->column_ids) {
65: 		index_ids.insert(index_id);
66: 	}
67: 	for (idx_t column_id = 0; column_id < column_ids.size(); column_id++) {
68: 		auto it = index_ids.find(column_ids[column_id]);
69: 		if (it == index_ids.end()) {
70: 			fetch_ids.push_back(column_ids[column_id]);
71: 			fetch_types.push_back(children[1]->types[column_id]);
72: 		}
73: 	}
74: 	if (right_projection_map.empty()) {
75: 		for (column_t i = 0; i < column_ids.size(); i++) {
76: 			right_projection_map.push_back(i);
77: 		}
78: 	}
79: 	if (left_projection_map.empty()) {
80: 		for (column_t i = 0; i < children[0]->types.size(); i++) {
81: 			left_projection_map.push_back(i);
82: 		}
83: 	}
84: }
85: 
86: unique_ptr<OperatorState> PhysicalIndexJoin::GetOperatorState(ClientContext &context) const {
87: 	return make_unique<IndexJoinOperatorState>(*this);
88: }
89: 
90: void PhysicalIndexJoin::Output(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
91:                                OperatorState &state_p) const {
92: 	auto &transaction = Transaction::GetTransaction(context.client);
93: 	auto &phy_tbl_scan = (PhysicalTableScan &)*children[1];
94: 	auto &bind_tbl = (TableScanBindData &)*phy_tbl_scan.bind_data;
95: 	auto &state = (IndexJoinOperatorState &)state_p;
96: 
97: 	auto tbl = bind_tbl.table->storage.get();
98: 	idx_t output_sel_idx = 0;
99: 	vector<row_t> fetch_rows;
100: 
101: 	while (output_sel_idx < STANDARD_VECTOR_SIZE && state.lhs_idx < input.size()) {
102: 		if (state.rhs_idx < state.result_sizes[state.lhs_idx]) {
103: 			state.rhs_sel.set_index(output_sel_idx++, state.lhs_idx);
104: 			if (!fetch_types.empty()) {
105: 				//! We need to collect the rows we want to fetch
106: 				fetch_rows.push_back(state.rhs_rows[state.lhs_idx][state.rhs_idx]);
107: 			}
108: 			state.rhs_idx++;
109: 		} else {
110: 			//! We are done with the matches from this LHS Key
111: 			state.rhs_idx = 0;
112: 			state.lhs_idx++;
113: 		}
114: 	}
115: 	//! Now we fetch the RHS data
116: 	if (!fetch_types.empty()) {
117: 		if (fetch_rows.empty()) {
118: 			return;
119: 		}
120: 		state.rhs_chunk.Reset();
121: 		ColumnFetchState fetch_state;
122: 		Vector row_ids(LogicalType::ROW_TYPE, (data_ptr_t)&fetch_rows[0]);
123: 		tbl->Fetch(transaction, state.rhs_chunk, fetch_ids, row_ids, output_sel_idx, fetch_state);
124: 	}
125: 
126: 	//! Now we actually produce our result chunk
127: 	idx_t left_offset = lhs_first ? 0 : right_projection_map.size();
128: 	idx_t right_offset = lhs_first ? left_projection_map.size() : 0;
129: 	idx_t rhs_column_idx = 0;
130: 	for (idx_t i = 0; i < right_projection_map.size(); i++) {
131: 		auto it = index_ids.find(column_ids[right_projection_map[i]]);
132: 		if (it == index_ids.end()) {
133: 			chunk.data[right_offset + i].Reference(state.rhs_chunk.data[rhs_column_idx++]);
134: 		} else {
135: 			chunk.data[right_offset + i].Slice(state.join_keys.data[0], state.rhs_sel, output_sel_idx);
136: 		}
137: 	}
138: 	for (idx_t i = 0; i < left_projection_map.size(); i++) {
139: 		chunk.data[left_offset + i].Slice(input.data[left_projection_map[i]], state.rhs_sel, output_sel_idx);
140: 	}
141: 
142: 	state.result_size = output_sel_idx;
143: 	chunk.SetCardinality(state.result_size);
144: }
145: 
146: void PhysicalIndexJoin::GetRHSMatches(ExecutionContext &context, DataChunk &input, OperatorState &state_p) const {
147: 	auto &state = (IndexJoinOperatorState &)state_p;
148: 	auto &art = (ART &)*index;
149: 	auto &transaction = Transaction::GetTransaction(context.client);
150: 	for (idx_t i = 0; i < input.size(); i++) {
151: 		auto equal_value = state.join_keys.GetValue(0, i);
152: 		auto index_state = art.InitializeScanSinglePredicate(transaction, equal_value, ExpressionType::COMPARE_EQUAL);
153: 		state.rhs_rows[i].clear();
154: 		if (!equal_value.IsNull()) {
155: 			if (fetch_types.empty()) {
156: 				IndexLock lock;
157: 				index->InitializeLock(lock);
158: 				art.SearchEqualJoinNoFetch(equal_value, state.result_sizes[i]);
159: 			} else {
160: 				IndexLock lock;
161: 				index->InitializeLock(lock);
162: 				art.SearchEqual((ARTIndexScanState *)index_state.get(), (idx_t)-1, state.rhs_rows[i]);
163: 				state.result_sizes[i] = state.rhs_rows[i].size();
164: 			}
165: 		} else {
166: 			//! This is null so no matches
167: 			state.result_sizes[i] = 0;
168: 		}
169: 	}
170: 	for (idx_t i = input.size(); i < STANDARD_VECTOR_SIZE; i++) {
171: 		//! No LHS chunk value so result size is empty
172: 		state.result_sizes[i] = 0;
173: 	}
174: }
175: 
176: OperatorResultType PhysicalIndexJoin::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
177:                                               OperatorState &state_p) const {
178: 	auto &state = (IndexJoinOperatorState &)state_p;
179: 
180: 	state.result_size = 0;
181: 	if (state.first_fetch) {
182: 		state.probe_executor.Execute(input, state.join_keys);
183: 
184: 		//! Fill Matches for the current LHS chunk
185: 		GetRHSMatches(context, input, state_p);
186: 		state.first_fetch = false;
187: 	}
188: 	//! Check if we need to get a new LHS chunk
189: 	if (state.lhs_idx >= input.size()) {
190: 		state.lhs_idx = 0;
191: 		state.rhs_idx = 0;
192: 		state.first_fetch = true;
193: 		return OperatorResultType::NEED_MORE_INPUT;
194: 	}
195: 	//! Output vectors
196: 	if (state.lhs_idx < input.size()) {
197: 		Output(context, input, chunk, state_p);
198: 	}
199: 	return OperatorResultType::HAVE_MORE_OUTPUT;
200: }
201: 
202: } // namespace duckdb
[end of src/execution/operator/join/physical_index_join.cpp]
[start of src/execution/operator/join/physical_nested_loop_join.cpp]
1: #include "duckdb/execution/operator/join/physical_nested_loop_join.hpp"
2: #include "duckdb/parallel/thread_context.hpp"
3: #include "duckdb/common/operator/comparison_operators.hpp"
4: #include "duckdb/common/vector_operations/vector_operations.hpp"
5: #include "duckdb/execution/expression_executor.hpp"
6: #include "duckdb/execution/nested_loop_join.hpp"
7: #include "duckdb/main/client_context.hpp"
8: 
9: namespace duckdb {
10: 
11: PhysicalNestedLoopJoin::PhysicalNestedLoopJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left,
12:                                                unique_ptr<PhysicalOperator> right, vector<JoinCondition> cond,
13:                                                JoinType join_type, idx_t estimated_cardinality)
14:     : PhysicalComparisonJoin(op, PhysicalOperatorType::NESTED_LOOP_JOIN, move(cond), join_type, estimated_cardinality) {
15: 	children.push_back(move(left));
16: 	children.push_back(move(right));
17: }
18: 
19: static bool HasNullValues(DataChunk &chunk) {
20: 	for (idx_t col_idx = 0; col_idx < chunk.ColumnCount(); col_idx++) {
21: 		VectorData vdata;
22: 		chunk.data[col_idx].Orrify(chunk.size(), vdata);
23: 
24: 		if (vdata.validity.AllValid()) {
25: 			continue;
26: 		}
27: 		for (idx_t i = 0; i < chunk.size(); i++) {
28: 			auto idx = vdata.sel->get_index(i);
29: 			if (!vdata.validity.RowIsValid(idx)) {
30: 				return true;
31: 			}
32: 		}
33: 	}
34: 	return false;
35: }
36: 
37: template <bool MATCH>
38: static void ConstructSemiOrAntiJoinResult(DataChunk &left, DataChunk &result, bool found_match[]) {
39: 	D_ASSERT(left.ColumnCount() == result.ColumnCount());
40: 	// create the selection vector from the matches that were found
41: 	idx_t result_count = 0;
42: 	SelectionVector sel(STANDARD_VECTOR_SIZE);
43: 	for (idx_t i = 0; i < left.size(); i++) {
44: 		if (found_match[i] == MATCH) {
45: 			sel.set_index(result_count++, i);
46: 		}
47: 	}
48: 	// construct the final result
49: 	if (result_count > 0) {
50: 		// we only return the columns on the left side
51: 		// project them using the result selection vector
52: 		// reference the columns of the left side from the result
53: 		result.Slice(left, sel, result_count);
54: 	} else {
55: 		result.SetCardinality(0);
56: 	}
57: }
58: 
59: void PhysicalJoin::ConstructSemiJoinResult(DataChunk &left, DataChunk &result, bool found_match[]) {
60: 	ConstructSemiOrAntiJoinResult<true>(left, result, found_match);
61: }
62: 
63: void PhysicalJoin::ConstructAntiJoinResult(DataChunk &left, DataChunk &result, bool found_match[]) {
64: 	ConstructSemiOrAntiJoinResult<false>(left, result, found_match);
65: }
66: 
67: void PhysicalJoin::ConstructMarkJoinResult(DataChunk &join_keys, DataChunk &left, DataChunk &result, bool found_match[],
68:                                            bool has_null) {
69: 	// for the initial set of columns we just reference the left side
70: 	result.SetCardinality(left);
71: 	for (idx_t i = 0; i < left.ColumnCount(); i++) {
72: 		result.data[i].Reference(left.data[i]);
73: 	}
74: 	auto &mark_vector = result.data.back();
75: 	mark_vector.SetVectorType(VectorType::FLAT_VECTOR);
76: 	// first we set the NULL values from the join keys
77: 	// if there is any NULL in the keys, the result is NULL
78: 	auto bool_result = FlatVector::GetData<bool>(mark_vector);
79: 	auto &mask = FlatVector::Validity(mark_vector);
80: 	for (idx_t col_idx = 0; col_idx < join_keys.ColumnCount(); col_idx++) {
81: 		VectorData jdata;
82: 		join_keys.data[col_idx].Orrify(join_keys.size(), jdata);
83: 		if (!jdata.validity.AllValid()) {
84: 			for (idx_t i = 0; i < join_keys.size(); i++) {
85: 				auto jidx = jdata.sel->get_index(i);
86: 				mask.Set(i, jdata.validity.RowIsValid(jidx));
87: 			}
88: 		}
89: 	}
90: 	// now set the remaining entries to either true or false based on whether a match was found
91: 	if (found_match) {
92: 		for (idx_t i = 0; i < left.size(); i++) {
93: 			bool_result[i] = found_match[i];
94: 		}
95: 	} else {
96: 		memset(bool_result, 0, sizeof(bool) * left.size());
97: 	}
98: 	// if the right side contains NULL values, the result of any FALSE becomes NULL
99: 	if (has_null) {
100: 		for (idx_t i = 0; i < left.size(); i++) {
101: 			if (!bool_result[i]) {
102: 				mask.SetInvalid(i);
103: 			}
104: 		}
105: 	}
106: }
107: 
108: //===--------------------------------------------------------------------===//
109: // Sink
110: //===--------------------------------------------------------------------===//
111: class NestedLoopJoinLocalState : public LocalSinkState {
112: public:
113: 	explicit NestedLoopJoinLocalState(const vector<JoinCondition> &conditions) {
114: 		vector<LogicalType> condition_types;
115: 		for (auto &cond : conditions) {
116: 			rhs_executor.AddExpression(*cond.right);
117: 			condition_types.push_back(cond.right->return_type);
118: 		}
119: 		right_condition.Initialize(condition_types);
120: 	}
121: 
122: 	//! The chunk holding the right condition
123: 	DataChunk right_condition;
124: 	//! The executor of the RHS condition
125: 	ExpressionExecutor rhs_executor;
126: };
127: 
128: class NestedLoopJoinGlobalState : public GlobalSinkState {
129: public:
130: 	NestedLoopJoinGlobalState() : has_null(false) {
131: 	}
132: 
133: 	mutex nj_lock;
134: 	//! Materialized data of the RHS
135: 	ChunkCollection right_data;
136: 	//! Materialized join condition of the RHS
137: 	ChunkCollection right_chunks;
138: 	//! Whether or not the RHS of the nested loop join has NULL values
139: 	bool has_null;
140: 	//! A bool indicating for each tuple in the RHS if they found a match (only used in FULL OUTER JOIN)
141: 	unique_ptr<bool[]> right_found_match;
142: };
143: 
144: SinkResultType PhysicalNestedLoopJoin::Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate,
145:                                             DataChunk &input) const {
146: 	auto &gstate = (NestedLoopJoinGlobalState &)state;
147: 	auto &nlj_state = (NestedLoopJoinLocalState &)lstate;
148: 
149: 	// resolve the join expression of the right side
150: 	nlj_state.right_condition.Reset();
151: 	nlj_state.rhs_executor.Execute(input, nlj_state.right_condition);
152: 
153: 	// if we have not seen any NULL values yet, and we are performing a MARK join, check if there are NULL values in
154: 	// this chunk
155: 	if (join_type == JoinType::MARK && !gstate.has_null) {
156: 		if (HasNullValues(nlj_state.right_condition)) {
157: 			gstate.has_null = true;
158: 		}
159: 	}
160: 
161: 	// append the data and the
162: 	lock_guard<mutex> nj_guard(gstate.nj_lock);
163: 	gstate.right_data.Append(input);
164: 	gstate.right_chunks.Append(nlj_state.right_condition);
165: 	return SinkResultType::NEED_MORE_INPUT;
166: }
167: 
168: void PhysicalNestedLoopJoin::Combine(ExecutionContext &context, GlobalSinkState &gstate, LocalSinkState &lstate) const {
169: 	auto &state = (NestedLoopJoinLocalState &)lstate;
170: 	auto &client_profiler = QueryProfiler::Get(context.client);
171: 
172: 	context.thread.profiler.Flush(this, &state.rhs_executor, "rhs_executor", 1);
173: 	client_profiler.Flush(context.thread.profiler);
174: }
175: 
176: SinkFinalizeType PhysicalNestedLoopJoin::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
177:                                                   GlobalSinkState &gstate_p) const {
178: 	auto &gstate = (NestedLoopJoinGlobalState &)gstate_p;
179: 	if (join_type == JoinType::OUTER || join_type == JoinType::RIGHT) {
180: 		// for FULL/RIGHT OUTER JOIN, initialize found_match to false for every tuple
181: 		gstate.right_found_match = unique_ptr<bool[]>(new bool[gstate.right_data.Count()]);
182: 		memset(gstate.right_found_match.get(), 0, sizeof(bool) * gstate.right_data.Count());
183: 	}
184: 	if (gstate.right_chunks.Count() == 0 && EmptyResultIfRHSIsEmpty()) {
185: 		return SinkFinalizeType::NO_OUTPUT_POSSIBLE;
186: 	}
187: 	return SinkFinalizeType::READY;
188: }
189: 
190: unique_ptr<GlobalSinkState> PhysicalNestedLoopJoin::GetGlobalSinkState(ClientContext &context) const {
191: 	return make_unique<NestedLoopJoinGlobalState>();
192: }
193: 
194: unique_ptr<LocalSinkState> PhysicalNestedLoopJoin::GetLocalSinkState(ExecutionContext &context) const {
195: 	return make_unique<NestedLoopJoinLocalState>(conditions);
196: }
197: 
198: //===--------------------------------------------------------------------===//
199: // Operator
200: //===--------------------------------------------------------------------===//
201: class PhysicalNestedLoopJoinState : public OperatorState {
202: public:
203: 	PhysicalNestedLoopJoinState(const PhysicalNestedLoopJoin &op, const vector<JoinCondition> &conditions)
204: 	    : fetch_next_left(true), fetch_next_right(false), right_chunk(0), left_tuple(0), right_tuple(0) {
205: 		vector<LogicalType> condition_types;
206: 		for (auto &cond : conditions) {
207: 			lhs_executor.AddExpression(*cond.left);
208: 			condition_types.push_back(cond.left->return_type);
209: 		}
210: 		left_condition.Initialize(condition_types);
211: 		if (IsLeftOuterJoin(op.join_type)) {
212: 			left_found_match = unique_ptr<bool[]>(new bool[STANDARD_VECTOR_SIZE]);
213: 			memset(left_found_match.get(), 0, sizeof(bool) * STANDARD_VECTOR_SIZE);
214: 		}
215: 	}
216: 
217: 	bool fetch_next_left;
218: 	bool fetch_next_right;
219: 	idx_t right_chunk;
220: 	DataChunk left_condition;
221: 	//! The executor of the LHS condition
222: 	ExpressionExecutor lhs_executor;
223: 
224: 	idx_t left_tuple;
225: 	idx_t right_tuple;
226: 
227: 	unique_ptr<bool[]> left_found_match;
228: 
229: public:
230: 	void Finalize(PhysicalOperator *op, ExecutionContext &context) override {
231: 		context.thread.profiler.Flush(op, &lhs_executor, "lhs_executor", 0);
232: 	}
233: };
234: 
235: unique_ptr<OperatorState> PhysicalNestedLoopJoin::GetOperatorState(ClientContext &context) const {
236: 	return make_unique<PhysicalNestedLoopJoinState>(*this, conditions);
237: }
238: 
239: OperatorResultType PhysicalNestedLoopJoin::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
240:                                                    OperatorState &state_p) const {
241: 	auto &gstate = (NestedLoopJoinGlobalState &)*sink_state;
242: 
243: 	if (gstate.right_chunks.Count() == 0) {
244: 		// empty RHS
245: 		if (!EmptyResultIfRHSIsEmpty()) {
246: 			ConstructEmptyJoinResult(join_type, gstate.has_null, input, chunk);
247: 			return OperatorResultType::NEED_MORE_INPUT;
248: 		} else {
249: 			return OperatorResultType::FINISHED;
250: 		}
251: 	}
252: 
253: 	switch (join_type) {
254: 	case JoinType::SEMI:
255: 	case JoinType::ANTI:
256: 	case JoinType::MARK:
257: 		// simple joins can have max STANDARD_VECTOR_SIZE matches per chunk
258: 		ResolveSimpleJoin(context, input, chunk, state_p);
259: 		return OperatorResultType::NEED_MORE_INPUT;
260: 	case JoinType::LEFT:
261: 	case JoinType::INNER:
262: 	case JoinType::OUTER:
263: 	case JoinType::RIGHT:
264: 		return ResolveComplexJoin(context, input, chunk, state_p);
265: 	default:
266: 		throw NotImplementedException("Unimplemented type for nested loop join!");
267: 	}
268: }
269: 
270: void PhysicalNestedLoopJoin::ResolveSimpleJoin(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
271:                                                OperatorState &state_p) const {
272: 	auto &state = (PhysicalNestedLoopJoinState &)state_p;
273: 	auto &gstate = (NestedLoopJoinGlobalState &)*sink_state;
274: 
275: 	// resolve the left join condition for the current chunk
276: 	state.lhs_executor.Execute(input, state.left_condition);
277: 
278: 	bool found_match[STANDARD_VECTOR_SIZE] = {false};
279: 	NestedLoopJoinMark::Perform(state.left_condition, gstate.right_chunks, found_match, conditions);
280: 	switch (join_type) {
281: 	case JoinType::MARK:
282: 		// now construct the mark join result from the found matches
283: 		PhysicalJoin::ConstructMarkJoinResult(state.left_condition, input, chunk, found_match, gstate.has_null);
284: 		break;
285: 	case JoinType::SEMI:
286: 		// construct the semi join result from the found matches
287: 		PhysicalJoin::ConstructSemiJoinResult(input, chunk, found_match);
288: 		break;
289: 	case JoinType::ANTI:
290: 		// construct the anti join result from the found matches
291: 		PhysicalJoin::ConstructAntiJoinResult(input, chunk, found_match);
292: 		break;
293: 	default:
294: 		throw NotImplementedException("Unimplemented type for simple nested loop join!");
295: 	}
296: }
297: 
298: void PhysicalJoin::ConstructLeftJoinResult(DataChunk &left, DataChunk &result, bool found_match[]) {
299: 	SelectionVector remaining_sel(STANDARD_VECTOR_SIZE);
300: 	idx_t remaining_count = 0;
301: 	for (idx_t i = 0; i < left.size(); i++) {
302: 		if (!found_match[i]) {
303: 			remaining_sel.set_index(remaining_count++, i);
304: 		}
305: 	}
306: 	if (remaining_count > 0) {
307: 		result.Slice(left, remaining_sel, remaining_count);
308: 		for (idx_t idx = left.ColumnCount(); idx < result.ColumnCount(); idx++) {
309: 			result.data[idx].SetVectorType(VectorType::CONSTANT_VECTOR);
310: 			ConstantVector::SetNull(result.data[idx], true);
311: 		}
312: 	}
313: }
314: 
315: OperatorResultType PhysicalNestedLoopJoin::ResolveComplexJoin(ExecutionContext &context, DataChunk &input,
316:                                                               DataChunk &chunk, OperatorState &state_p) const {
317: 	auto &state = (PhysicalNestedLoopJoinState &)state_p;
318: 	auto &gstate = (NestedLoopJoinGlobalState &)*sink_state;
319: 
320: 	idx_t match_count;
321: 	do {
322: 		if (state.fetch_next_right) {
323: 			// we exhausted the chunk on the right: move to the next chunk on the right
324: 			state.right_chunk++;
325: 			state.left_tuple = 0;
326: 			state.right_tuple = 0;
327: 			state.fetch_next_right = false;
328: 			// check if we exhausted all chunks on the RHS
329: 			if (state.right_chunk >= gstate.right_chunks.ChunkCount()) {
330: 				state.fetch_next_left = true;
331: 				// we exhausted all chunks on the right: move to the next chunk on the left
332: 				if (IsLeftOuterJoin(join_type)) {
333: 					// left join: before we move to the next chunk, see if we need to output any vectors that didn't
334: 					// have a match found
335: 					PhysicalJoin::ConstructLeftJoinResult(input, chunk, state.left_found_match.get());
336: 					memset(state.left_found_match.get(), 0, sizeof(bool) * STANDARD_VECTOR_SIZE);
337: 				}
338: 				return OperatorResultType::NEED_MORE_INPUT;
339: 			}
340: 		}
341: 		if (state.fetch_next_left) {
342: 			// resolve the left join condition for the current chunk
343: 			state.left_condition.Reset();
344: 			state.lhs_executor.Execute(input, state.left_condition);
345: 
346: 			state.left_tuple = 0;
347: 			state.right_tuple = 0;
348: 			state.right_chunk = 0;
349: 			state.fetch_next_left = false;
350: 		}
351: 		// now we have a left and a right chunk that we can join together
352: 		// note that we only get here in the case of a LEFT, INNER or FULL join
353: 		auto &left_chunk = input;
354: 		auto &right_chunk = gstate.right_chunks.GetChunk(state.right_chunk);
355: 		auto &right_data = gstate.right_data.GetChunk(state.right_chunk);
356: 
357: 		// sanity check
358: 		left_chunk.Verify();
359: 		right_chunk.Verify();
360: 		right_data.Verify();
361: 
362: 		// now perform the join
363: 		SelectionVector lvector(STANDARD_VECTOR_SIZE), rvector(STANDARD_VECTOR_SIZE);
364: 		match_count = NestedLoopJoinInner::Perform(state.left_tuple, state.right_tuple, state.left_condition,
365: 		                                           right_chunk, lvector, rvector, conditions);
366: 		// we have finished resolving the join conditions
367: 		if (match_count > 0) {
368: 			// we have matching tuples!
369: 			// construct the result
370: 			if (state.left_found_match) {
371: 				for (idx_t i = 0; i < match_count; i++) {
372: 					state.left_found_match[lvector.get_index(i)] = true;
373: 				}
374: 			}
375: 			if (gstate.right_found_match) {
376: 				for (idx_t i = 0; i < match_count; i++) {
377: 					gstate.right_found_match[state.right_chunk * STANDARD_VECTOR_SIZE + rvector.get_index(i)] = true;
378: 				}
379: 			}
380: 			chunk.Slice(input, lvector, match_count);
381: 			chunk.Slice(right_data, rvector, match_count, input.ColumnCount());
382: 		}
383: 
384: 		// check if we exhausted the RHS, if we did we need to move to the next right chunk in the next iteration
385: 		if (state.right_tuple >= right_chunk.size()) {
386: 			state.fetch_next_right = true;
387: 		}
388: 	} while (match_count == 0);
389: 	return OperatorResultType::HAVE_MORE_OUTPUT;
390: }
391: 
392: //===--------------------------------------------------------------------===//
393: // Source
394: //===--------------------------------------------------------------------===//
395: class NestedLoopJoinScanState : public GlobalSourceState {
396: public:
397: 	explicit NestedLoopJoinScanState(const PhysicalNestedLoopJoin &op) : op(op), right_outer_position(0) {
398: 	}
399: 
400: 	mutex lock;
401: 	const PhysicalNestedLoopJoin &op;
402: 	idx_t right_outer_position;
403: 
404: public:
405: 	idx_t MaxThreads() override {
406: 		auto &sink = (NestedLoopJoinGlobalState &)*op.sink_state;
407: 		return sink.right_chunks.Count() / (STANDARD_VECTOR_SIZE * 10);
408: 	}
409: };
410: 
411: unique_ptr<GlobalSourceState> PhysicalNestedLoopJoin::GetGlobalSourceState(ClientContext &context) const {
412: 	return make_unique<NestedLoopJoinScanState>(*this);
413: }
414: 
415: void PhysicalNestedLoopJoin::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,
416:                                      LocalSourceState &lstate) const {
417: 	D_ASSERT(IsRightOuterJoin(join_type));
418: 	// check if we need to scan any unmatched tuples from the RHS for the full/right outer join
419: 	auto &sink = (NestedLoopJoinGlobalState &)*sink_state;
420: 	auto &state = (NestedLoopJoinScanState &)gstate;
421: 
422: 	// if the LHS is exhausted in a FULL/RIGHT OUTER JOIN, we scan the found_match for any chunks we
423: 	// still need to output
424: 	lock_guard<mutex> l(state.lock);
425: 	ConstructFullOuterJoinResult(sink.right_found_match.get(), sink.right_data, chunk, state.right_outer_position);
426: }
427: 
428: } // namespace duckdb
[end of src/execution/operator/join/physical_nested_loop_join.cpp]
[start of src/execution/operator/join/physical_piecewise_merge_join.cpp]
1: #include "duckdb/execution/operator/join/physical_piecewise_merge_join.hpp"
2: 
3: #include "duckdb/common/fast_mem.hpp"
4: #include "duckdb/common/operator/comparison_operators.hpp"
5: #include "duckdb/common/row_operations/row_operations.hpp"
6: #include "duckdb/common/sort/comparators.hpp"
7: #include "duckdb/common/sort/sort.hpp"
8: #include "duckdb/common/vector_operations/vector_operations.hpp"
9: #include "duckdb/execution/expression_executor.hpp"
10: #include "duckdb/main/client_context.hpp"
11: #include "duckdb/parallel/event.hpp"
12: #include "duckdb/parallel/thread_context.hpp"
13: 
14: namespace duckdb {
15: 
16: PhysicalPiecewiseMergeJoin::PhysicalPiecewiseMergeJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left,
17:                                                        unique_ptr<PhysicalOperator> right, vector<JoinCondition> cond,
18:                                                        JoinType join_type, idx_t estimated_cardinality)
19:     : PhysicalComparisonJoin(op, PhysicalOperatorType::PIECEWISE_MERGE_JOIN, move(cond), join_type,
20:                              estimated_cardinality) {
21: 	// Reorder the conditions so that ranges are at the front.
22: 	// TODO: use stats to improve the choice?
23: 	if (conditions.size() > 1) {
24: 		auto conditions_p = std::move(conditions);
25: 		conditions.resize(conditions_p.size());
26: 		idx_t range_position = 0;
27: 		idx_t other_position = conditions_p.size();
28: 		for (idx_t i = 0; i < conditions_p.size(); ++i) {
29: 			switch (conditions_p[i].comparison) {
30: 			case ExpressionType::COMPARE_LESSTHAN:
31: 			case ExpressionType::COMPARE_LESSTHANOREQUALTO:
32: 			case ExpressionType::COMPARE_GREATERTHAN:
33: 			case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
34: 				conditions[range_position++] = std::move(conditions_p[i]);
35: 				break;
36: 			default:
37: 				conditions[--other_position] = std::move(conditions_p[i]);
38: 				break;
39: 			}
40: 		}
41: 	}
42: 
43: 	for (auto &cond : conditions) {
44: 		D_ASSERT(cond.left->return_type == cond.right->return_type);
45: 		join_key_types.push_back(cond.left->return_type);
46: 
47: 		// Convert the conditions to sort orders
48: 		auto left = cond.left->Copy();
49: 		auto right = cond.right->Copy();
50: 		switch (cond.comparison) {
51: 		case ExpressionType::COMPARE_LESSTHAN:
52: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
53: 			lhs_orders.emplace_back(BoundOrderByNode(OrderType::ASCENDING, OrderByNullType::NULLS_LAST, move(left)));
54: 			rhs_orders.emplace_back(BoundOrderByNode(OrderType::ASCENDING, OrderByNullType::NULLS_LAST, move(right)));
55: 			break;
56: 		case ExpressionType::COMPARE_GREATERTHAN:
57: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
58: 			lhs_orders.emplace_back(BoundOrderByNode(OrderType::DESCENDING, OrderByNullType::NULLS_LAST, move(left)));
59: 			rhs_orders.emplace_back(BoundOrderByNode(OrderType::DESCENDING, OrderByNullType::NULLS_LAST, move(right)));
60: 			break;
61: 		case ExpressionType::COMPARE_NOTEQUAL:
62: 		case ExpressionType::COMPARE_DISTINCT_FROM:
63: 			// Allowed in multi-predicate joins, but can't be first/sort.
64: 			D_ASSERT(!lhs_orders.empty());
65: 			lhs_orders.emplace_back(BoundOrderByNode(OrderType::INVALID, OrderByNullType::NULLS_LAST, move(left)));
66: 			rhs_orders.emplace_back(BoundOrderByNode(OrderType::INVALID, OrderByNullType::NULLS_LAST, move(right)));
67: 			break;
68: 
69: 		default:
70: 			// COMPARE EQUAL not supported with merge join
71: 			throw NotImplementedException("Unimplemented join type for merge join");
72: 		}
73: 	}
74: 	children.push_back(move(left));
75: 	children.push_back(move(right));
76: }
77: 
78: //===--------------------------------------------------------------------===//
79: // Sink
80: //===--------------------------------------------------------------------===//
81: class MergeJoinGlobalState : public GlobalSinkState {
82: public:
83: 	MergeJoinGlobalState(BufferManager &buffer_manager, const vector<BoundOrderByNode> &orders, RowLayout &rhs_layout)
84: 	    : rhs_global_sort_state(buffer_manager, orders, rhs_layout), rhs_has_null(0), rhs_count(0),
85: 	      memory_per_thread(0) {
86: 		D_ASSERT(orders.size() == 1);
87: 	}
88: 
89: 	inline idx_t Count() const {
90: 		return rhs_count;
91: 	}
92: 
93: 	//! The lock for updating the global state
94: 	mutex lock;
95: 	//! Global sort state
96: 	GlobalSortState rhs_global_sort_state;
97: 	//! Whether or not the RHS has NULL values
98: 	idx_t rhs_has_null;
99: 	//! The total number of rows in the RHS
100: 	idx_t rhs_count;
101: 	//! A bool indicating for each tuple in the RHS if they found a match (only used in FULL OUTER JOIN)
102: 	unique_ptr<bool[]> rhs_found_match;
103: 	//! Memory usage per thread
104: 	idx_t memory_per_thread;
105: };
106: 
107: unique_ptr<GlobalSinkState> PhysicalPiecewiseMergeJoin::GetGlobalSinkState(ClientContext &context) const {
108: 	// Get the payload layout from the rhs types and tail predicates
109: 	RowLayout rhs_layout;
110: 	rhs_layout.Initialize(children[1]->types);
111: 	vector<BoundOrderByNode> rhs_order;
112: 	rhs_order.emplace_back(rhs_orders[0].Copy());
113: 	auto state = make_unique<MergeJoinGlobalState>(BufferManager::GetBufferManager(context), rhs_order, rhs_layout);
114: 	// Set external (can be force with the PRAGMA)
115: 	auto &config = ClientConfig::GetConfig(context);
116: 	state->rhs_global_sort_state.external = config.force_external;
117: 	// Memory usage per thread should scale with max mem / num threads
118: 	// We take 1/4th of this, to be conservative
119: 	idx_t max_memory = BufferManager::GetBufferManager(context).GetMaxMemory();
120: 	idx_t num_threads = TaskScheduler::GetScheduler(context).NumberOfThreads();
121: 	state->memory_per_thread = (max_memory / num_threads) / 4;
122: 	return move(state);
123: }
124: 
125: class MergeJoinLocalState : public LocalSinkState {
126: public:
127: 	explicit MergeJoinLocalState() : rhs_has_null(0), rhs_count(0) {
128: 	}
129: 
130: 	//! The local sort state
131: 	LocalSortState rhs_local_sort_state;
132: 	//! Local copy of the sorting expression executor
133: 	ExpressionExecutor rhs_executor;
134: 	//! Holds a vector of incoming sorting columns
135: 	DataChunk rhs_keys;
136: 	//! Whether or not the RHS has NULL values
137: 	idx_t rhs_has_null;
138: 	//! The total number of rows in the RHS
139: 	idx_t rhs_count;
140: };
141: 
142: unique_ptr<LocalSinkState> PhysicalPiecewiseMergeJoin::GetLocalSinkState(ExecutionContext &context) const {
143: 	auto result = make_unique<MergeJoinLocalState>();
144: 	// Initialize order clause expression executor and DataChunk
145: 	vector<LogicalType> types;
146: 	for (auto &order : rhs_orders) {
147: 		types.push_back(order.expression->return_type);
148: 		result->rhs_executor.AddExpression(*order.expression);
149: 	}
150: 	result->rhs_keys.Initialize(types);
151: 	return move(result);
152: }
153: 
154: static idx_t PiecewiseMergeNulls(DataChunk &keys, const vector<JoinCondition> &conditions) {
155: 	// Merge the validity masks of the comparison keys into the primary
156: 	// Return the number of NULLs in the resulting chunk
157: 	D_ASSERT(keys.ColumnCount() > 0);
158: 	const auto count = keys.size();
159: 
160: 	size_t all_constant = 0;
161: 	for (auto &v : keys.data) {
162: 		if (v.GetVectorType() == VectorType::CONSTANT_VECTOR) {
163: 			++all_constant;
164: 		}
165: 	}
166: 
167: 	auto &primary = keys.data[0];
168: 	if (all_constant == keys.data.size()) {
169: 		//	Either all NULL or no NULLs
170: 		for (auto &v : keys.data) {
171: 			if (ConstantVector::IsNull(v)) {
172: 				ConstantVector::SetNull(primary, true);
173: 				return count;
174: 			}
175: 		}
176: 		return 0;
177: 	} else if (keys.ColumnCount() > 1) {
178: 		//	Normalify the primary, as it will need to merge arbitrary validity masks
179: 		primary.Normalify(count);
180: 		auto &pvalidity = FlatVector::Validity(primary);
181: 		for (size_t c = 1; c < keys.data.size(); ++c) {
182: 			// Skip comparisons that accept NULLs
183: 			if (conditions[c].comparison == ExpressionType::COMPARE_DISTINCT_FROM) {
184: 				continue;
185: 			}
186: 			//	Orrify the rest, as the sort code will do this anyway.
187: 			auto &v = keys.data[c];
188: 			VectorData vdata;
189: 			v.Orrify(count, vdata);
190: 			auto &vvalidity = vdata.validity;
191: 			if (vvalidity.AllValid()) {
192: 				continue;
193: 			}
194: 			pvalidity.EnsureWritable();
195: 			switch (v.GetVectorType()) {
196: 			case VectorType::FLAT_VECTOR: {
197: 				// Merge entire entries
198: 				auto pmask = pvalidity.GetData();
199: 				const auto entry_count = pvalidity.EntryCount(count);
200: 				for (idx_t entry_idx = 0; entry_idx < entry_count; ++entry_idx) {
201: 					pmask[entry_idx] &= vvalidity.GetValidityEntry(entry_idx);
202: 				}
203: 				break;
204: 			}
205: 			case VectorType::CONSTANT_VECTOR:
206: 				// All or nothing
207: 				if (ConstantVector::IsNull(v)) {
208: 					pvalidity.SetAllInvalid(count);
209: 					return count;
210: 				}
211: 				break;
212: 			default:
213: 				// One by one
214: 				for (idx_t i = 0; i < count; ++i) {
215: 					const auto idx = vdata.sel->get_index(i);
216: 					if (!vvalidity.RowIsValidUnsafe(idx)) {
217: 						pvalidity.SetInvalidUnsafe(i);
218: 					}
219: 				}
220: 				break;
221: 			}
222: 		}
223: 		return count - pvalidity.CountValid(count);
224: 	} else {
225: 		return count - VectorOperations::CountNotNull(primary, count);
226: 	}
227: }
228: 
229: static inline void SinkPiecewiseMergeChunk(LocalSortState &sort_state, DataChunk &join_keys, DataChunk &input) {
230: 	if (join_keys.ColumnCount() > 1) {
231: 		//	Only sort the first key
232: 		DataChunk join_head;
233: 		join_head.data.emplace_back(Vector(join_keys.data[0]));
234: 		join_head.SetCardinality(join_keys.size());
235: 
236: 		sort_state.SinkChunk(join_head, input);
237: 	} else {
238: 		sort_state.SinkChunk(join_keys, input);
239: 	}
240: }
241: 
242: SinkResultType PhysicalPiecewiseMergeJoin::Sink(ExecutionContext &context, GlobalSinkState &gstate_p,
243:                                                 LocalSinkState &lstate_p, DataChunk &input) const {
244: 	auto &gstate = (MergeJoinGlobalState &)gstate_p;
245: 	auto &lstate = (MergeJoinLocalState &)lstate_p;
246: 
247: 	auto &global_sort_state = gstate.rhs_global_sort_state;
248: 	auto &local_sort_state = lstate.rhs_local_sort_state;
249: 
250: 	// Initialize local state (if necessary)
251: 	if (!local_sort_state.initialized) {
252: 		local_sort_state.Initialize(global_sort_state, BufferManager::GetBufferManager(context.client));
253: 	}
254: 
255: 	// Obtain sorting columns
256: 	auto &join_keys = lstate.rhs_keys;
257: 	join_keys.Reset();
258: 	lstate.rhs_executor.Execute(input, join_keys);
259: 
260: 	// Count the NULLs so we can exclude them later
261: 	lstate.rhs_has_null += PiecewiseMergeNulls(join_keys, conditions);
262: 	lstate.rhs_count += join_keys.size();
263: 
264: 	// Sink the data into the local sort state
265: 	SinkPiecewiseMergeChunk(local_sort_state, join_keys, input);
266: 
267: 	// When sorting data reaches a certain size, we sort it
268: 	if (local_sort_state.SizeInBytes() >= gstate.memory_per_thread) {
269: 		local_sort_state.Sort(global_sort_state, true);
270: 	}
271: 	return SinkResultType::NEED_MORE_INPUT;
272: }
273: 
274: void PhysicalPiecewiseMergeJoin::Combine(ExecutionContext &context, GlobalSinkState &gstate_p,
275:                                          LocalSinkState &lstate_p) const {
276: 	auto &gstate = (MergeJoinGlobalState &)gstate_p;
277: 	auto &lstate = (MergeJoinLocalState &)lstate_p;
278: 	gstate.rhs_global_sort_state.AddLocalState(lstate.rhs_local_sort_state);
279: 	lock_guard<mutex> locked(gstate.lock);
280: 	gstate.rhs_has_null += lstate.rhs_has_null;
281: 	gstate.rhs_count += lstate.rhs_count;
282: 	auto &client_profiler = QueryProfiler::Get(context.client);
283: 
284: 	context.thread.profiler.Flush(this, &lstate.rhs_executor, "rhs_executor", 1);
285: 	client_profiler.Flush(context.thread.profiler);
286: }
287: 
288: //===--------------------------------------------------------------------===//
289: // Finalize
290: //===--------------------------------------------------------------------===//
291: class MergeJoinFinalizeTask : public ExecutorTask {
292: public:
293: 	MergeJoinFinalizeTask(shared_ptr<Event> event_p, ClientContext &context, MergeJoinGlobalState &state)
294: 	    : ExecutorTask(context), event(move(event_p)), context(context), state(state) {
295: 	}
296: 
297: 	TaskExecutionResult ExecuteTask(TaskExecutionMode mode) override {
298: 		// Initialize merge sorted and iterate until done
299: 		auto &global_sort_state = state.rhs_global_sort_state;
300: 		MergeSorter merge_sorter(global_sort_state, BufferManager::GetBufferManager(context));
301: 		merge_sorter.PerformInMergeRound();
302: 		event->FinishTask();
303: 
304: 		return TaskExecutionResult::TASK_FINISHED;
305: 	}
306: 
307: private:
308: 	shared_ptr<Event> event;
309: 	ClientContext &context;
310: 	MergeJoinGlobalState &state;
311: };
312: 
313: class MergeJoinFinalizeEvent : public Event {
314: public:
315: 	MergeJoinFinalizeEvent(MergeJoinGlobalState &gstate_p, Pipeline &pipeline_p)
316: 	    : Event(pipeline_p.executor), gstate(gstate_p), pipeline(pipeline_p) {
317: 	}
318: 
319: 	MergeJoinGlobalState &gstate;
320: 	Pipeline &pipeline;
321: 
322: public:
323: 	void Schedule() override {
324: 		auto &context = pipeline.GetClientContext();
325: 
326: 		// Schedule tasks equal to the number of threads, which will each merge multiple partitions
327: 		auto &ts = TaskScheduler::GetScheduler(context);
328: 		idx_t num_threads = ts.NumberOfThreads();
329: 
330: 		vector<unique_ptr<Task>> merge_tasks;
331: 		for (idx_t tnum = 0; tnum < num_threads; tnum++) {
332: 			merge_tasks.push_back(make_unique<MergeJoinFinalizeTask>(shared_from_this(), context, gstate));
333: 		}
334: 		SetTasks(move(merge_tasks));
335: 	}
336: 
337: 	void FinishEvent() override {
338: 		auto &global_sort_state = gstate.rhs_global_sort_state;
339: 
340: 		global_sort_state.CompleteMergeRound(true);
341: 		if (global_sort_state.sorted_blocks.size() > 1) {
342: 			// Multiple blocks remaining: Schedule the next round
343: 			PhysicalPiecewiseMergeJoin::ScheduleMergeTasks(pipeline, *this, gstate);
344: 		}
345: 	}
346: };
347: 
348: void PhysicalPiecewiseMergeJoin::ScheduleMergeTasks(Pipeline &pipeline, Event &event, MergeJoinGlobalState &gstate) {
349: 	// Initialize global sort state for a round of merging
350: 	gstate.rhs_global_sort_state.InitializeMergeRound();
351: 	auto new_event = make_shared<MergeJoinFinalizeEvent>(gstate, pipeline);
352: 	event.InsertEvent(move(new_event));
353: }
354: 
355: SinkFinalizeType PhysicalPiecewiseMergeJoin::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
356:                                                       GlobalSinkState &gstate_p) const {
357: 	auto &gstate = (MergeJoinGlobalState &)gstate_p;
358: 	auto &global_sort_state = gstate.rhs_global_sort_state;
359: 
360: 	if (IsRightOuterJoin(join_type)) {
361: 		// for FULL/RIGHT OUTER JOIN, initialize found_match to false for every tuple
362: 		gstate.rhs_found_match = unique_ptr<bool[]>(new bool[gstate.Count()]);
363: 		memset(gstate.rhs_found_match.get(), 0, sizeof(bool) * gstate.Count());
364: 	}
365: 	if (global_sort_state.sorted_blocks.empty() && EmptyResultIfRHSIsEmpty()) {
366: 		// Empty input!
367: 		return SinkFinalizeType::NO_OUTPUT_POSSIBLE;
368: 	}
369: 
370: 	// Prepare for merge sort phase
371: 	global_sort_state.PrepareMergePhase();
372: 
373: 	// Start the merge phase or finish if a merge is not necessary
374: 	if (global_sort_state.sorted_blocks.size() > 1) {
375: 		PhysicalPiecewiseMergeJoin::ScheduleMergeTasks(pipeline, event, gstate);
376: 	}
377: 	return SinkFinalizeType::READY;
378: }
379: 
380: //===--------------------------------------------------------------------===//
381: // Operator
382: //===--------------------------------------------------------------------===//
383: class PiecewiseMergeJoinState : public OperatorState {
384: public:
385: 	explicit PiecewiseMergeJoinState(const PhysicalPiecewiseMergeJoin &op, BufferManager &buffer_manager,
386: 	                                 bool force_external)
387: 	    : op(op), buffer_manager(buffer_manager), force_external(force_external), left_position(0), first_fetch(true),
388: 	      finished(true), right_position(0), right_chunk_index(0) {
389: 		vector<LogicalType> condition_types;
390: 		for (auto &order : op.lhs_orders) {
391: 			lhs_executor.AddExpression(*order.expression);
392: 			condition_types.push_back(order.expression->return_type);
393: 		}
394: 		lhs_keys.Initialize(condition_types);
395: 		if (IsLeftOuterJoin(op.join_type)) {
396: 			lhs_found_match = unique_ptr<bool[]>(new bool[STANDARD_VECTOR_SIZE]);
397: 			memset(lhs_found_match.get(), 0, sizeof(bool) * STANDARD_VECTOR_SIZE);
398: 		}
399: 		lhs_layout.Initialize(op.children[0]->types);
400: 		lhs_payload.Initialize(op.children[0]->types);
401: 
402: 		lhs_order.emplace_back(op.lhs_orders[0].Copy());
403: 
404: 		// Set up shared data for multiple predicates
405: 		sel.Initialize(STANDARD_VECTOR_SIZE);
406: 		condition_types.clear();
407: 		for (auto &order : op.rhs_orders) {
408: 			rhs_executor.AddExpression(*order.expression);
409: 			condition_types.push_back(order.expression->return_type);
410: 		}
411: 		rhs_keys.Initialize(condition_types);
412: 	}
413: 
414: 	const PhysicalPiecewiseMergeJoin &op;
415: 	BufferManager &buffer_manager;
416: 	bool force_external;
417: 
418: 	// Block sorting
419: 	DataChunk lhs_keys;
420: 	DataChunk lhs_payload;
421: 	ExpressionExecutor lhs_executor;
422: 	unique_ptr<bool[]> lhs_found_match;
423: 	vector<BoundOrderByNode> lhs_order;
424: 	RowLayout lhs_layout;
425: 	unique_ptr<LocalSortState> lhs_local_state;
426: 	unique_ptr<GlobalSortState> lhs_global_state;
427: 	idx_t lhs_count;
428: 	idx_t lhs_has_null;
429: 
430: 	// Simple scans
431: 	idx_t left_position;
432: 
433: 	// Complex scans
434: 	bool first_fetch;
435: 	bool finished;
436: 	idx_t right_position;
437: 	idx_t right_chunk_index;
438: 	idx_t right_base;
439: 
440: 	// Secondary predicate shared data
441: 	SelectionVector sel;
442: 	DataChunk rhs_keys;
443: 	DataChunk rhs_input;
444: 	ExpressionExecutor rhs_executor;
445: 
446: public:
447: 	void ResolveJoinKeys(DataChunk &input) {
448: 		// resolve the join keys for the input
449: 		lhs_keys.Reset();
450: 		lhs_executor.Execute(input, lhs_keys);
451: 
452: 		// Count the NULLs so we can exclude them later
453: 		lhs_count = lhs_keys.size();
454: 		lhs_has_null = PiecewiseMergeNulls(lhs_keys, op.conditions);
455: 
456: 		// sort by join key
457: 		lhs_global_state = make_unique<GlobalSortState>(buffer_manager, lhs_order, lhs_layout);
458: 		lhs_local_state = make_unique<LocalSortState>();
459: 		lhs_local_state->Initialize(*lhs_global_state, buffer_manager);
460: 		SinkPiecewiseMergeChunk(*lhs_local_state, lhs_keys, input);
461: 
462: 		// Set external (can be force with the PRAGMA)
463: 		lhs_global_state->external = force_external;
464: 		lhs_global_state->AddLocalState(*lhs_local_state);
465: 		lhs_global_state->PrepareMergePhase();
466: 		while (lhs_global_state->sorted_blocks.size() > 1) {
467: 			MergeSorter merge_sorter(*lhs_global_state, buffer_manager);
468: 			merge_sorter.PerformInMergeRound();
469: 			lhs_global_state->CompleteMergeRound();
470: 		}
471: 
472: 		// Scan the sorted payload
473: 		D_ASSERT(lhs_global_state->sorted_blocks.size() == 1);
474: 
475: 		PayloadScanner scanner(*lhs_global_state->sorted_blocks[0]->payload_data, *lhs_global_state);
476: 		lhs_payload.Reset();
477: 		scanner.Scan(lhs_payload);
478: 
479: 		// Recompute the sorted keys from the sorted input
480: 		lhs_keys.Reset();
481: 		lhs_executor.Execute(lhs_payload, lhs_keys);
482: 	}
483: 
484: 	void Finalize(PhysicalOperator *op, ExecutionContext &context) override {
485: 		context.thread.profiler.Flush(op, &lhs_executor, "lhs_executor", 0);
486: 	}
487: };
488: 
489: unique_ptr<OperatorState> PhysicalPiecewiseMergeJoin::GetOperatorState(ClientContext &context) const {
490: 	auto &buffer_manager = BufferManager::GetBufferManager(context);
491: 	auto &config = ClientConfig::GetConfig(context);
492: 	return make_unique<PiecewiseMergeJoinState>(*this, buffer_manager, config.force_external);
493: }
494: 
495: static inline idx_t SortedBlockNotNull(const idx_t base, const idx_t count, const idx_t not_null) {
496: 	return MinValue(base + count, MaxValue(base, not_null)) - base;
497: }
498: 
499: static int MergeJoinComparisonValue(ExpressionType comparison) {
500: 	switch (comparison) {
501: 	case ExpressionType::COMPARE_LESSTHAN:
502: 	case ExpressionType::COMPARE_GREATERTHAN:
503: 		return -1;
504: 	case ExpressionType::COMPARE_LESSTHANOREQUALTO:
505: 	case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
506: 		return 0;
507: 	default:
508: 		throw InternalException("Unimplemented comparison type for merge join!");
509: 	}
510: }
511: 
512: struct BlockMergeInfo {
513: 	GlobalSortState &state;
514: 	//! The block being scanned
515: 	const idx_t block_idx;
516: 	//! The start position being read from the block
517: 	const idx_t base_idx;
518: 	//! The number of not-NULL values in the block (they are at the end)
519: 	const idx_t not_null;
520: 	//! The current offset in the block
521: 	idx_t &entry_idx;
522: 	SelectionVector result;
523: 
524: 	BlockMergeInfo(GlobalSortState &state, idx_t block_idx, idx_t base_idx, idx_t &entry_idx, idx_t not_null)
525: 	    : state(state), block_idx(block_idx), base_idx(base_idx), not_null(not_null), entry_idx(entry_idx),
526: 	      result(STANDARD_VECTOR_SIZE) {
527: 	}
528: };
529: 
530: static idx_t SliceSortedPayload(DataChunk &payload, BlockMergeInfo &info, const idx_t result_count,
531:                                 const idx_t left_cols = 0) {
532: 	// There should only be one sorted block if they have been sorted
533: 	D_ASSERT(info.state.sorted_blocks.size() == 1);
534: 	SBScanState read_state(info.state.buffer_manager, info.state);
535: 	read_state.sb = info.state.sorted_blocks[0].get();
536: 	auto &sorted_data = *read_state.sb->payload_data;
537: 
538: 	// We have to create pointers for the entire block
539: 	// because unswizzle works on ranges not selections.
540: 	const auto first_idx = info.result.get_index(0);
541: 	read_state.SetIndices(info.block_idx, info.base_idx + first_idx);
542: 	read_state.PinData(sorted_data);
543: 	const auto data_ptr = read_state.DataPtr(sorted_data);
544: 
545: 	// Set up a batch of pointers to scan data from
546: 	Vector addresses(LogicalType::POINTER, result_count);
547: 	auto data_pointers = FlatVector::GetData<data_ptr_t>(addresses);
548: 
549: 	// Set up the data pointers for the values that are actually referenced
550: 	// and normalise the selection vector to zero
551: 	data_ptr_t row_ptr = data_ptr;
552: 	const idx_t &row_width = sorted_data.layout.GetRowWidth();
553: 
554: 	auto prev_idx = first_idx;
555: 	info.result.set_index(0, 0);
556: 	idx_t addr_count = 0;
557: 	data_pointers[addr_count++] = row_ptr;
558: 	for (idx_t i = 1; i < result_count; ++i) {
559: 		const auto row_idx = info.result.get_index(i);
560: 		info.result.set_index(i, row_idx - first_idx);
561: 		if (row_idx == prev_idx) {
562: 			continue;
563: 		}
564: 		row_ptr += (row_idx - prev_idx) * row_width;
565: 		data_pointers[addr_count++] = row_ptr;
566: 		prev_idx = row_idx;
567: 	}
568: 	// Unswizzle the offsets back to pointers (if needed)
569: 	if (!sorted_data.layout.AllConstant() && info.state.external) {
570: 		const auto next = prev_idx + 1;
571: 		RowOperations::UnswizzlePointers(sorted_data.layout, data_ptr, read_state.payload_heap_handle->Ptr(), next);
572: 	}
573: 
574: 	// Deserialize the payload data
575: 	auto sel = FlatVector::IncrementalSelectionVector();
576: 	for (idx_t col_idx = 0; col_idx < sorted_data.layout.ColumnCount(); col_idx++) {
577: 		const auto col_offset = sorted_data.layout.GetOffsets()[col_idx];
578: 		auto &col = payload.data[left_cols + col_idx];
579: 		RowOperations::Gather(addresses, *sel, col, *sel, addr_count, col_offset, col_idx);
580: 		col.Slice(info.result, result_count);
581: 	}
582: 
583: 	return first_idx;
584: }
585: 
586: static void MergeJoinPinSortingBlock(SBScanState &scan, const idx_t block_idx) {
587: 	scan.SetIndices(block_idx, 0);
588: 	scan.PinRadix(block_idx);
589: 
590: 	auto &sd = *scan.sb->blob_sorting_data;
591: 	if (block_idx < sd.data_blocks.size()) {
592: 		scan.PinData(sd);
593: 	}
594: }
595: 
596: static data_ptr_t MergeJoinRadixPtr(SBScanState &scan, const idx_t entry_idx) {
597: 	scan.entry_idx = entry_idx;
598: 	return scan.RadixPtr();
599: }
600: 
601: static idx_t MergeJoinSimpleBlocks(PiecewiseMergeJoinState &lstate, MergeJoinGlobalState &rstate, bool *found_match,
602:                                    const ExpressionType comparison) {
603: 	const auto cmp = MergeJoinComparisonValue(comparison);
604: 
605: 	// The sort parameters should all be the same
606: 	auto &lsort = *lstate.lhs_global_state;
607: 	auto &rsort = rstate.rhs_global_sort_state;
608: 	D_ASSERT(lsort.sort_layout.all_constant == rsort.sort_layout.all_constant);
609: 	const auto all_constant = lsort.sort_layout.all_constant;
610: 	D_ASSERT(lsort.external == rsort.external);
611: 	const auto external = lsort.external;
612: 
613: 	// There should only be one sorted block if they have been sorted
614: 	D_ASSERT(lsort.sorted_blocks.size() == 1);
615: 	SBScanState lread(lsort.buffer_manager, lsort);
616: 	lread.sb = lsort.sorted_blocks[0].get();
617: 
618: 	const idx_t l_block_idx = 0;
619: 	idx_t l_entry_idx = 0;
620: 	const auto lhs_not_null = lstate.lhs_count - lstate.lhs_has_null;
621: 	MergeJoinPinSortingBlock(lread, l_block_idx);
622: 	auto l_ptr = MergeJoinRadixPtr(lread, l_entry_idx);
623: 
624: 	D_ASSERT(rsort.sorted_blocks.size() == 1);
625: 	SBScanState rread(rsort.buffer_manager, rsort);
626: 	rread.sb = rsort.sorted_blocks[0].get();
627: 
628: 	const auto cmp_size = lsort.sort_layout.comparison_size;
629: 	const auto entry_size = lsort.sort_layout.entry_size;
630: 
631: 	idx_t right_base = 0;
632: 	for (idx_t r_block_idx = 0; r_block_idx < rread.sb->radix_sorting_data.size(); r_block_idx++) {
633: 		// we only care about the BIGGEST value in each of the RHS data blocks
634: 		// because we want to figure out if the LHS values are less than [or equal] to ANY value
635: 		// get the biggest value from the RHS chunk
636: 		MergeJoinPinSortingBlock(rread, r_block_idx);
637: 
638: 		auto &rblock = rread.sb->radix_sorting_data[r_block_idx];
639: 		const auto r_not_null = SortedBlockNotNull(right_base, rblock.count, rstate.rhs_count - rstate.rhs_has_null);
640: 		if (r_not_null == 0) {
641: 			break;
642: 		}
643: 		const auto r_entry_idx = r_not_null - 1;
644: 		right_base += rblock.count;
645: 
646: 		auto r_ptr = MergeJoinRadixPtr(rread, r_entry_idx);
647: 
648: 		// now we start from the current lpos value and check if we found a new value that is [<= OR <] the max RHS
649: 		// value
650: 		while (true) {
651: 			int comp_res;
652: 			if (all_constant) {
653: 				comp_res = FastMemcmp(l_ptr, r_ptr, cmp_size);
654: 			} else {
655: 				lread.entry_idx = l_entry_idx;
656: 				rread.entry_idx = r_entry_idx;
657: 				comp_res = Comparators::CompareTuple(lread, rread, l_ptr, r_ptr, lsort.sort_layout, external);
658: 			}
659: 
660: 			if (comp_res <= cmp) {
661: 				// found a match for lpos, set it in the found_match vector
662: 				found_match[l_entry_idx] = true;
663: 				l_entry_idx++;
664: 				l_ptr += entry_size;
665: 				if (l_entry_idx >= lhs_not_null) {
666: 					// early out: we exhausted the entire LHS and they all match
667: 					return 0;
668: 				}
669: 			} else {
670: 				// we found no match: any subsequent value from the LHS we scan now will be bigger and thus also not
671: 				// match move to the next RHS chunk
672: 				break;
673: 			}
674: 		}
675: 	}
676: 	return 0;
677: }
678: 
679: void PhysicalPiecewiseMergeJoin::ResolveSimpleJoin(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
680:                                                    OperatorState &state_p) const {
681: 	auto &state = (PiecewiseMergeJoinState &)state_p;
682: 	auto &gstate = (MergeJoinGlobalState &)*sink_state;
683: 
684: 	state.ResolveJoinKeys(input);
685: 
686: 	// perform the actual join
687: 	bool found_match[STANDARD_VECTOR_SIZE];
688: 	memset(found_match, 0, sizeof(found_match));
689: 	MergeJoinSimpleBlocks(state, gstate, found_match, conditions[0].comparison);
690: 
691: 	// use the sorted payload
692: 	const auto lhs_not_null = state.lhs_count - state.lhs_has_null;
693: 	auto &payload = state.lhs_payload;
694: 
695: 	// now construct the result based on the join result
696: 	switch (join_type) {
697: 	case JoinType::MARK: {
698: 		// The only part of the join keys that is actually used is the validity mask.
699: 		// Since the payload is sorted, we can just set the tail end of the validity masks to invalid.
700: 		for (auto &key : state.lhs_keys.data) {
701: 			key.Normalify(state.lhs_keys.size());
702: 			auto &mask = FlatVector::Validity(key);
703: 			if (mask.AllValid()) {
704: 				continue;
705: 			}
706: 			mask.SetAllValid(lhs_not_null);
707: 			for (idx_t i = lhs_not_null; i < state.lhs_count; ++i) {
708: 				mask.SetInvalid(i);
709: 			}
710: 		}
711: 		// So we make a set of keys that have the validity mask set for the
712: 		PhysicalJoin::ConstructMarkJoinResult(state.lhs_keys, payload, chunk, found_match, gstate.rhs_has_null);
713: 		break;
714: 	}
715: 	case JoinType::SEMI:
716: 		PhysicalJoin::ConstructSemiJoinResult(payload, chunk, found_match);
717: 		break;
718: 	case JoinType::ANTI:
719: 		PhysicalJoin::ConstructAntiJoinResult(payload, chunk, found_match);
720: 		break;
721: 	default:
722: 		throw NotImplementedException("Unimplemented join type for merge join");
723: 	}
724: }
725: 
726: static idx_t MergeJoinComplexBlocks(BlockMergeInfo &l, BlockMergeInfo &r, const ExpressionType comparison) {
727: 	const auto cmp = MergeJoinComparisonValue(comparison);
728: 
729: 	// The sort parameters should all be the same
730: 	D_ASSERT(l.state.sort_layout.all_constant == r.state.sort_layout.all_constant);
731: 	const auto all_constant = r.state.sort_layout.all_constant;
732: 	D_ASSERT(l.state.external == r.state.external);
733: 	const auto external = l.state.external;
734: 
735: 	// There should only be one sorted block if they have been sorted
736: 	D_ASSERT(l.state.sorted_blocks.size() == 1);
737: 	SBScanState lread(l.state.buffer_manager, l.state);
738: 	lread.sb = l.state.sorted_blocks[0].get();
739: 	D_ASSERT(lread.sb->radix_sorting_data.size() == 1);
740: 	MergeJoinPinSortingBlock(lread, l.block_idx);
741: 	auto l_start = MergeJoinRadixPtr(lread, 0);
742: 	auto l_ptr = MergeJoinRadixPtr(lread, l.entry_idx);
743: 
744: 	D_ASSERT(r.state.sorted_blocks.size() == 1);
745: 	SBScanState rread(r.state.buffer_manager, r.state);
746: 	rread.sb = r.state.sorted_blocks[0].get();
747: 
748: 	if (r.entry_idx >= r.not_null) {
749: 		return 0;
750: 	}
751: 
752: 	MergeJoinPinSortingBlock(rread, r.block_idx);
753: 	auto r_ptr = MergeJoinRadixPtr(rread, r.entry_idx);
754: 
755: 	const auto cmp_size = l.state.sort_layout.comparison_size;
756: 	const auto entry_size = l.state.sort_layout.entry_size;
757: 
758: 	idx_t result_count = 0;
759: 	while (true) {
760: 		if (l.entry_idx < l.not_null) {
761: 			int comp_res;
762: 			if (all_constant) {
763: 				comp_res = FastMemcmp(l_ptr, r_ptr, cmp_size);
764: 			} else {
765: 				lread.entry_idx = l.entry_idx;
766: 				rread.entry_idx = r.entry_idx;
767: 				comp_res = Comparators::CompareTuple(lread, rread, l_ptr, r_ptr, l.state.sort_layout, external);
768: 			}
769: 
770: 			if (comp_res <= cmp) {
771: 				// left side smaller: found match
772: 				l.result.set_index(result_count, sel_t(l.entry_idx - l.base_idx));
773: 				r.result.set_index(result_count, sel_t(r.entry_idx - r.base_idx));
774: 				result_count++;
775: 				// move left side forward
776: 				l.entry_idx++;
777: 				l_ptr += entry_size;
778: 				if (result_count == STANDARD_VECTOR_SIZE) {
779: 					// out of space!
780: 					break;
781: 				}
782: 				continue;
783: 			}
784: 		}
785: 		// right side smaller or equal, or left side exhausted: move
786: 		// right pointer forward reset left side to start
787: 		r.entry_idx++;
788: 		if (r.entry_idx >= r.not_null) {
789: 			break;
790: 		}
791: 		r_ptr += entry_size;
792: 
793: 		l_ptr = l_start;
794: 		l.entry_idx = 0;
795: 	}
796: 
797: 	return result_count;
798: }
799: 
800: static idx_t SelectJoinTail(const ExpressionType &condition, Vector &left, Vector &right, const SelectionVector *sel,
801:                             idx_t count, SelectionVector *true_sel) {
802: 	switch (condition) {
803: 	case ExpressionType::COMPARE_NOTEQUAL:
804: 		return VectorOperations::NotEquals(left, right, sel, count, true_sel, nullptr);
805: 	case ExpressionType::COMPARE_LESSTHAN:
806: 		return VectorOperations::LessThan(left, right, sel, count, true_sel, nullptr);
807: 	case ExpressionType::COMPARE_GREATERTHAN:
808: 		return VectorOperations::GreaterThan(left, right, sel, count, true_sel, nullptr);
809: 	case ExpressionType::COMPARE_LESSTHANOREQUALTO:
810: 		return VectorOperations::LessThanEquals(left, right, sel, count, true_sel, nullptr);
811: 	case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
812: 		return VectorOperations::GreaterThanEquals(left, right, sel, count, true_sel, nullptr);
813: 	case ExpressionType::COMPARE_DISTINCT_FROM:
814: 		return VectorOperations::DistinctFrom(left, right, sel, count, true_sel, nullptr);
815: 	case ExpressionType::COMPARE_NOT_DISTINCT_FROM:
816: 	case ExpressionType::COMPARE_EQUAL:
817: 	default:
818: 		throw InternalException("Unsupported comparison type for PhysicalPiecewiseMergeJoin");
819: 	}
820: 
821: 	return count;
822: }
823: 
824: OperatorResultType PhysicalPiecewiseMergeJoin::ResolveComplexJoin(ExecutionContext &context, DataChunk &input,
825:                                                                   DataChunk &chunk, OperatorState &state_p) const {
826: 	auto &state = (PiecewiseMergeJoinState &)state_p;
827: 	auto &gstate = (MergeJoinGlobalState &)*sink_state;
828: 	auto &rsorted = *gstate.rhs_global_sort_state.sorted_blocks[0];
829: 	const auto left_cols = input.ColumnCount();
830: 	const auto tail_cols = conditions.size() - 1;
831: 	do {
832: 		if (state.first_fetch) {
833: 			state.ResolveJoinKeys(input);
834: 
835: 			state.right_chunk_index = 0;
836: 			state.right_base = 0;
837: 			state.left_position = 0;
838: 			state.right_position = 0;
839: 			state.first_fetch = false;
840: 			state.finished = false;
841: 		}
842: 		if (state.finished) {
843: 			if (IsLeftOuterJoin(join_type)) {
844: 				// left join: before we move to the next chunk, see if we need to output any vectors that didn't
845: 				// have a match found
846: 				PhysicalJoin::ConstructLeftJoinResult(state.lhs_payload, chunk, state.lhs_found_match.get());
847: 				memset(state.lhs_found_match.get(), 0, sizeof(bool) * STANDARD_VECTOR_SIZE);
848: 			}
849: 			state.first_fetch = true;
850: 			state.finished = false;
851: 			return OperatorResultType::NEED_MORE_INPUT;
852: 		}
853: 
854: 		const auto lhs_not_null = state.lhs_count - state.lhs_has_null;
855: 		BlockMergeInfo left_info(*state.lhs_global_state, 0, 0, state.left_position, lhs_not_null);
856: 
857: 		const auto &rblock = rsorted.radix_sorting_data[state.right_chunk_index];
858: 		const auto rhs_not_null =
859: 		    SortedBlockNotNull(state.right_base, rblock.count, gstate.rhs_count - gstate.rhs_has_null);
860: 		BlockMergeInfo right_info(gstate.rhs_global_sort_state, state.right_chunk_index, state.right_position,
861: 		                          state.right_position, rhs_not_null);
862: 
863: 		idx_t result_count = MergeJoinComplexBlocks(left_info, right_info, conditions[0].comparison);
864: 		if (result_count == 0) {
865: 			// exhausted this chunk on the right side
866: 			// move to the next right chunk
867: 			state.left_position = 0;
868: 			state.right_position = 0;
869: 			state.right_base += rsorted.radix_sorting_data[state.right_chunk_index].count;
870: 			state.right_chunk_index++;
871: 			if (state.right_chunk_index >= rsorted.radix_sorting_data.size()) {
872: 				state.finished = true;
873: 			}
874: 		} else {
875: 			// found matches: extract them
876: 			chunk.Reset();
877: 			for (idx_t c = 0; c < state.lhs_payload.ColumnCount(); ++c) {
878: 				chunk.data[c].Slice(state.lhs_payload.data[c], left_info.result, result_count);
879: 			}
880: 			const auto first_idx = SliceSortedPayload(chunk, right_info, result_count, left_cols);
881: 			chunk.SetCardinality(result_count);
882: 
883: 			auto sel = FlatVector::IncrementalSelectionVector();
884: 			if (tail_cols) {
885: 				// If there are more expressions to compute,
886: 				// split the result chunk into the left and right halves
887: 				// so we can compute the values for comparison.
888: 				chunk.Split(state.rhs_input, left_cols);
889: 				state.rhs_executor.SetChunk(state.rhs_input);
890: 				state.rhs_keys.Reset();
891: 
892: 				auto tail_count = result_count;
893: 				for (size_t cmp_idx = 1; cmp_idx < conditions.size(); ++cmp_idx) {
894: 					Vector left(state.lhs_keys.data[cmp_idx]);
895: 					left.Slice(left_info.result, result_count);
896: 
897: 					auto &right = state.rhs_keys.data[cmp_idx];
898: 					state.rhs_executor.ExecuteExpression(cmp_idx, right);
899: 
900: 					if (tail_count < result_count) {
901: 						left.Slice(*sel, tail_count);
902: 						right.Slice(*sel, tail_count);
903: 					}
904: 					tail_count =
905: 					    SelectJoinTail(conditions[cmp_idx].comparison, left, right, sel, tail_count, &state.sel);
906: 					sel = &state.sel;
907: 				}
908: 				chunk.Fuse(state.rhs_input);
909: 
910: 				if (tail_count < result_count) {
911: 					result_count = tail_count;
912: 					chunk.Slice(*sel, result_count);
913: 				}
914: 			}
915: 
916: 			// found matches: mark the found matches if required
917: 			if (state.lhs_found_match) {
918: 				for (idx_t i = 0; i < result_count; i++) {
919: 					state.lhs_found_match[left_info.result[sel->get_index(i)]] = true;
920: 				}
921: 			}
922: 			if (gstate.rhs_found_match) {
923: 				//	Absolute position of the block + start position inside that block
924: 				const idx_t base_index = right_info.base_idx + first_idx;
925: 				for (idx_t i = 0; i < result_count; i++) {
926: 					gstate.rhs_found_match[base_index + right_info.result[sel->get_index(i)]] = true;
927: 				}
928: 			}
929: 			chunk.SetCardinality(result_count);
930: 			chunk.Verify();
931: 		}
932: 	} while (chunk.size() == 0);
933: 	return OperatorResultType::HAVE_MORE_OUTPUT;
934: }
935: 
936: OperatorResultType PhysicalPiecewiseMergeJoin::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
937:                                                        OperatorState &state) const {
938: 	auto &gstate = (MergeJoinGlobalState &)*sink_state;
939: 
940: 	if (gstate.Count() == 0) {
941: 		// empty RHS
942: 		if (!EmptyResultIfRHSIsEmpty()) {
943: 			ConstructEmptyJoinResult(join_type, gstate.rhs_has_null, input, chunk);
944: 			return OperatorResultType::NEED_MORE_INPUT;
945: 		} else {
946: 			return OperatorResultType::FINISHED;
947: 		}
948: 	}
949: 
950: 	switch (join_type) {
951: 	case JoinType::SEMI:
952: 	case JoinType::ANTI:
953: 	case JoinType::MARK:
954: 		// simple joins can have max STANDARD_VECTOR_SIZE matches per chunk
955: 		ResolveSimpleJoin(context, input, chunk, state);
956: 		return OperatorResultType::NEED_MORE_INPUT;
957: 	case JoinType::LEFT:
958: 	case JoinType::INNER:
959: 	case JoinType::RIGHT:
960: 	case JoinType::OUTER:
961: 		return ResolveComplexJoin(context, input, chunk, state);
962: 	default:
963: 		throw NotImplementedException("Unimplemented type for piecewise merge loop join!");
964: 	}
965: }
966: 
967: //===--------------------------------------------------------------------===//
968: // Source
969: //===--------------------------------------------------------------------===//
970: class PiecewiseJoinScanState : public GlobalSourceState {
971: public:
972: 	explicit PiecewiseJoinScanState(const PhysicalPiecewiseMergeJoin &op) : op(op), right_outer_position(0) {
973: 	}
974: 
975: 	mutex lock;
976: 	const PhysicalPiecewiseMergeJoin &op;
977: 	unique_ptr<PayloadScanner> scanner;
978: 	idx_t right_outer_position;
979: 
980: public:
981: 	idx_t MaxThreads() override {
982: 		auto &sink = (MergeJoinGlobalState &)*op.sink_state;
983: 		return sink.Count() / (STANDARD_VECTOR_SIZE * idx_t(10));
984: 	}
985: };
986: 
987: unique_ptr<GlobalSourceState> PhysicalPiecewiseMergeJoin::GetGlobalSourceState(ClientContext &context) const {
988: 	return make_unique<PiecewiseJoinScanState>(*this);
989: }
990: 
991: void PhysicalPiecewiseMergeJoin::GetData(ExecutionContext &context, DataChunk &result, GlobalSourceState &gstate,
992:                                          LocalSourceState &lstate) const {
993: 	D_ASSERT(IsRightOuterJoin(join_type));
994: 	// check if we need to scan any unmatched tuples from the RHS for the full/right outer join
995: 	auto &sink = (MergeJoinGlobalState &)*sink_state;
996: 	auto &state = (PiecewiseJoinScanState &)gstate;
997: 
998: 	lock_guard<mutex> l(state.lock);
999: 	if (!state.scanner) {
1000: 		// Initialize scanner (if not yet initialized)
1001: 		auto &sort_state = sink.rhs_global_sort_state;
1002: 		if (sort_state.sorted_blocks.empty()) {
1003: 			return;
1004: 		}
1005: 		state.scanner = make_unique<PayloadScanner>(*sort_state.sorted_blocks[0]->payload_data, sort_state);
1006: 	}
1007: 
1008: 	// if the LHS is exhausted in a FULL/RIGHT OUTER JOIN, we scan the found_match for any chunks we
1009: 	// still need to output
1010: 	const auto found_match = sink.rhs_found_match.get();
1011: 
1012: 	// ConstructFullOuterJoinResult(sink.rhs_found_match.get(), sink.right_chunks, chunk, state.right_outer_position);
1013: 	DataChunk rhs_chunk;
1014: 	rhs_chunk.Initialize(sink.rhs_global_sort_state.payload_layout.GetTypes());
1015: 	SelectionVector rsel(STANDARD_VECTOR_SIZE);
1016: 	for (;;) {
1017: 		// Read the next sorted chunk
1018: 		state.scanner->Scan(rhs_chunk);
1019: 
1020: 		const auto count = rhs_chunk.size();
1021: 		if (count == 0) {
1022: 			return;
1023: 		}
1024: 
1025: 		idx_t result_count = 0;
1026: 		// figure out which tuples didn't find a match in the RHS
1027: 		for (idx_t i = 0; i < count; i++) {
1028: 			if (!found_match[state.right_outer_position + i]) {
1029: 				rsel.set_index(result_count++, i);
1030: 			}
1031: 		}
1032: 		state.right_outer_position += count;
1033: 
1034: 		if (result_count > 0) {
1035: 			// if there were any tuples that didn't find a match, output them
1036: 			const idx_t left_column_count = children[0]->types.size();
1037: 			for (idx_t col_idx = 0; col_idx < left_column_count; ++col_idx) {
1038: 				result.data[col_idx].SetVectorType(VectorType::CONSTANT_VECTOR);
1039: 				ConstantVector::SetNull(result.data[col_idx], true);
1040: 			}
1041: 			const idx_t right_column_count = children[1]->types.size();
1042: 			;
1043: 			for (idx_t col_idx = 0; col_idx < right_column_count; ++col_idx) {
1044: 				result.data[left_column_count + col_idx].Slice(rhs_chunk.data[col_idx], rsel, result_count);
1045: 			}
1046: 			result.SetCardinality(result_count);
1047: 			return;
1048: 		}
1049: 	}
1050: }
1051: 
1052: } // namespace duckdb
[end of src/execution/operator/join/physical_piecewise_merge_join.cpp]
[start of src/execution/operator/projection/physical_projection.cpp]
1: #include "duckdb/execution/operator/projection/physical_projection.hpp"
2: #include "duckdb/parallel/thread_context.hpp"
3: #include "duckdb/execution/expression_executor.hpp"
4: 
5: namespace duckdb {
6: 
7: class ProjectionState : public OperatorState {
8: public:
9: 	explicit ProjectionState(const vector<unique_ptr<Expression>> &expressions) : executor(expressions) {
10: 	}
11: 
12: 	ExpressionExecutor executor;
13: 
14: public:
15: 	void Finalize(PhysicalOperator *op, ExecutionContext &context) override {
16: 		context.thread.profiler.Flush(op, &executor, "projection", 0);
17: 	}
18: };
19: 
20: PhysicalProjection::PhysicalProjection(vector<LogicalType> types, vector<unique_ptr<Expression>> select_list,
21:                                        idx_t estimated_cardinality)
22:     : PhysicalOperator(PhysicalOperatorType::PROJECTION, move(types), estimated_cardinality),
23:       select_list(move(select_list)) {
24: }
25: 
26: OperatorResultType PhysicalProjection::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
27:                                                OperatorState &state_p) const {
28: 	auto &state = (ProjectionState &)state_p;
29: 	state.executor.Execute(input, chunk);
30: 	return OperatorResultType::NEED_MORE_INPUT;
31: }
32: 
33: unique_ptr<OperatorState> PhysicalProjection::GetOperatorState(ClientContext &context) const {
34: 	return make_unique<ProjectionState>(select_list);
35: }
36: 
37: string PhysicalProjection::ParamsToString() const {
38: 	string extra_info;
39: 	for (auto &expr : select_list) {
40: 		extra_info += expr->GetName() + "\n";
41: 	}
42: 	return extra_info;
43: }
44: 
45: } // namespace duckdb
[end of src/execution/operator/projection/physical_projection.cpp]
[start of src/execution/operator/projection/physical_tableinout_function.cpp]
1: #include "duckdb/execution/operator/projection/physical_tableinout_function.hpp"
2: 
3: namespace duckdb {
4: 
5: class TableInOutFunctionState : public OperatorState {
6: public:
7: 	TableInOutFunctionState() : initialized(false) {
8: 	}
9: 
10: 	unique_ptr<FunctionOperatorData> operator_data;
11: 	bool initialized = false;
12: };
13: 
14: PhysicalTableInOutFunction::PhysicalTableInOutFunction(vector<LogicalType> types, TableFunction function_p,
15:                                                        unique_ptr<FunctionData> bind_data_p,
16:                                                        vector<column_t> column_ids_p, idx_t estimated_cardinality)
17:     : PhysicalOperator(PhysicalOperatorType::INOUT_FUNCTION, move(types), estimated_cardinality),
18:       function(move(function_p)), bind_data(move(bind_data_p)), column_ids(move(column_ids_p)) {
19: }
20: 
21: unique_ptr<OperatorState> PhysicalTableInOutFunction::GetOperatorState(ClientContext &context) const {
22: 	return make_unique<TableInOutFunctionState>();
23: }
24: 
25: OperatorResultType PhysicalTableInOutFunction::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
26:                                                        OperatorState &state_p) const {
27: 	auto &state = (TableInOutFunctionState &)state_p;
28: 
29: 	if (!state.initialized) {
30: 		if (function.init) {
31: 			state.operator_data = function.init(context.client, bind_data.get(), column_ids, nullptr);
32: 		}
33: 		state.initialized = true;
34: 	}
35: 
36: 	function.function(context.client, bind_data.get(), state.operator_data.get(), &input, chunk);
37: 	return OperatorResultType::NEED_MORE_INPUT;
38: }
39: 
40: } // namespace duckdb
[end of src/execution/operator/projection/physical_tableinout_function.cpp]
[start of src/execution/operator/projection/physical_unnest.cpp]
1: #include "duckdb/execution/operator/projection/physical_unnest.hpp"
2: 
3: #include "duckdb/common/vector_operations/vector_operations.hpp"
4: #include "duckdb/common/algorithm.hpp"
5: #include "duckdb/execution/expression_executor.hpp"
6: #include "duckdb/planner/expression/bound_reference_expression.hpp"
7: #include "duckdb/planner/expression/bound_unnest_expression.hpp"
8: 
9: namespace duckdb {
10: 
11: //! The operator state of the window
12: class UnnestOperatorState : public OperatorState {
13: public:
14: 	UnnestOperatorState() : parent_position(0), list_position(0), list_length(-1), first_fetch(true) {
15: 	}
16: 
17: 	idx_t parent_position;
18: 	idx_t list_position;
19: 	int64_t list_length;
20: 	bool first_fetch;
21: 
22: 	DataChunk list_data;
23: 	vector<VectorData> list_vector_data;
24: 	vector<VectorData> list_child_data;
25: };
26: 
27: // this implements a sorted window functions variant
28: PhysicalUnnest::PhysicalUnnest(vector<LogicalType> types, vector<unique_ptr<Expression>> select_list,
29:                                idx_t estimated_cardinality, PhysicalOperatorType type)
30:     : PhysicalOperator(type, move(types), estimated_cardinality), select_list(std::move(select_list)) {
31: 	D_ASSERT(!this->select_list.empty());
32: }
33: 
34: static void UnnestNull(idx_t start, idx_t end, Vector &result) {
35: 	if (result.GetType().InternalType() == PhysicalType::STRUCT) {
36: 		auto &children = StructVector::GetEntries(result);
37: 		for (auto &child : children) {
38: 			UnnestNull(start, end, *child);
39: 		}
40: 	}
41: 	auto &validity = FlatVector::Validity(result);
42: 	for (idx_t i = start; i < end; i++) {
43: 		validity.SetInvalid(i);
44: 	}
45: 	if (result.GetType().InternalType() == PhysicalType::STRUCT) {
46: 		auto &struct_children = StructVector::GetEntries(result);
47: 		for (auto &child : struct_children) {
48: 			UnnestNull(start, end, *child);
49: 		}
50: 	}
51: }
52: 
53: template <class T>
54: static void TemplatedUnnest(VectorData &vdata, idx_t start, idx_t end, Vector &result) {
55: 	auto source_data = (T *)vdata.data;
56: 	auto &source_mask = vdata.validity;
57: 	auto result_data = FlatVector::GetData<T>(result);
58: 	auto &result_mask = FlatVector::Validity(result);
59: 
60: 	for (idx_t i = start; i < end; i++) {
61: 		auto source_idx = vdata.sel->get_index(i);
62: 		auto target_idx = i - start;
63: 		if (source_mask.RowIsValid(source_idx)) {
64: 			result_data[target_idx] = source_data[source_idx];
65: 			result_mask.SetValid(target_idx);
66: 		} else {
67: 			result_mask.SetInvalid(target_idx);
68: 		}
69: 	}
70: }
71: 
72: static void UnnestValidity(VectorData &vdata, idx_t start, idx_t end, Vector &result) {
73: 	auto &source_mask = vdata.validity;
74: 	auto &result_mask = FlatVector::Validity(result);
75: 
76: 	for (idx_t i = start; i < end; i++) {
77: 		auto source_idx = vdata.sel->get_index(i);
78: 		auto target_idx = i - start;
79: 		result_mask.Set(target_idx, source_mask.RowIsValid(source_idx));
80: 	}
81: }
82: 
83: static void UnnestVector(VectorData &vdata, Vector &source, idx_t list_size, idx_t start, idx_t end, Vector &result) {
84: 	switch (result.GetType().InternalType()) {
85: 	case PhysicalType::BOOL:
86: 	case PhysicalType::INT8:
87: 		TemplatedUnnest<int8_t>(vdata, start, end, result);
88: 		break;
89: 	case PhysicalType::INT16:
90: 		TemplatedUnnest<int16_t>(vdata, start, end, result);
91: 		break;
92: 	case PhysicalType::INT32:
93: 		TemplatedUnnest<int32_t>(vdata, start, end, result);
94: 		break;
95: 	case PhysicalType::INT64:
96: 		TemplatedUnnest<int64_t>(vdata, start, end, result);
97: 		break;
98: 	case PhysicalType::INT128:
99: 		TemplatedUnnest<hugeint_t>(vdata, start, end, result);
100: 		break;
101: 	case PhysicalType::UINT8:
102: 		TemplatedUnnest<uint8_t>(vdata, start, end, result);
103: 		break;
104: 	case PhysicalType::UINT16:
105: 		TemplatedUnnest<uint16_t>(vdata, start, end, result);
106: 		break;
107: 	case PhysicalType::UINT32:
108: 		TemplatedUnnest<uint32_t>(vdata, start, end, result);
109: 		break;
110: 	case PhysicalType::UINT64:
111: 		TemplatedUnnest<uint64_t>(vdata, start, end, result);
112: 		break;
113: 	case PhysicalType::FLOAT:
114: 		TemplatedUnnest<float>(vdata, start, end, result);
115: 		break;
116: 	case PhysicalType::DOUBLE:
117: 		TemplatedUnnest<double>(vdata, start, end, result);
118: 		break;
119: 	case PhysicalType::INTERVAL:
120: 		TemplatedUnnest<interval_t>(vdata, start, end, result);
121: 		break;
122: 	case PhysicalType::VARCHAR:
123: 		TemplatedUnnest<string_t>(vdata, start, end, result);
124: 		break;
125: 	case PhysicalType::LIST: {
126: 		auto &target = ListVector::GetEntry(result);
127: 		target.Reference(ListVector::GetEntry(source));
128: 		ListVector::SetListSize(result, ListVector::GetListSize(source));
129: 		TemplatedUnnest<list_entry_t>(vdata, start, end, result);
130: 		break;
131: 	}
132: 	case PhysicalType::STRUCT: {
133: 		auto &source_entries = StructVector::GetEntries(source);
134: 		auto &target_entries = StructVector::GetEntries(result);
135: 		UnnestValidity(vdata, start, end, result);
136: 		for (idx_t i = 0; i < source_entries.size(); i++) {
137: 			VectorData sdata;
138: 			source_entries[i]->Orrify(list_size, sdata);
139: 			UnnestVector(sdata, *source_entries[i], list_size, start, end, *target_entries[i]);
140: 		}
141: 		break;
142: 	}
143: 	default:
144: 		throw InternalException("Unimplemented type for UNNEST");
145: 	}
146: }
147: 
148: unique_ptr<OperatorState> PhysicalUnnest::GetOperatorState(ClientContext &context) const {
149: 	return make_unique<UnnestOperatorState>();
150: }
151: 
152: OperatorResultType PhysicalUnnest::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
153:                                            OperatorState &state_p) const {
154: 	auto &state = (UnnestOperatorState &)state_p;
155: 	do {
156: 		if (state.first_fetch) {
157: 			// get the list data to unnest
158: 			ExpressionExecutor executor;
159: 			vector<LogicalType> list_data_types;
160: 			for (auto &exp : select_list) {
161: 				D_ASSERT(exp->type == ExpressionType::BOUND_UNNEST);
162: 				auto bue = (BoundUnnestExpression *)exp.get();
163: 				list_data_types.push_back(bue->child->return_type);
164: 				executor.AddExpression(*bue->child.get());
165: 			}
166: 			state.list_data.Destroy();
167: 			state.list_data.Initialize(list_data_types);
168: 			executor.Execute(input, state.list_data);
169: 
170: 			// paranoia aplenty
171: 			state.list_data.Verify();
172: 			D_ASSERT(input.size() == state.list_data.size());
173: 			D_ASSERT(state.list_data.ColumnCount() == select_list.size());
174: 
175: 			// initialize VectorData object so the nullmask can accessed
176: 			state.list_vector_data.resize(state.list_data.ColumnCount());
177: 			state.list_child_data.resize(state.list_data.ColumnCount());
178: 			for (idx_t col_idx = 0; col_idx < state.list_data.ColumnCount(); col_idx++) {
179: 				auto &list_vector = state.list_data.data[col_idx];
180: 				list_vector.Orrify(state.list_data.size(), state.list_vector_data[col_idx]);
181: 
182: 				if (list_vector.GetType() == LogicalType::SQLNULL) {
183: 					// UNNEST(NULL)
184: 					auto &child_vector = list_vector;
185: 					child_vector.Orrify(0, state.list_child_data[col_idx]);
186: 				} else {
187: 					auto list_size = ListVector::GetListSize(list_vector);
188: 					auto &child_vector = ListVector::GetEntry(list_vector);
189: 					child_vector.Orrify(list_size, state.list_child_data[col_idx]);
190: 				}
191: 			}
192: 			state.first_fetch = false;
193: 		}
194: 		if (state.parent_position >= input.size()) {
195: 			// finished with this input chunk
196: 			state.parent_position = 0;
197: 			state.list_position = 0;
198: 			state.list_length = -1;
199: 			state.first_fetch = true;
200: 			return OperatorResultType::NEED_MORE_INPUT;
201: 		}
202: 
203: 		// need to figure out how many times we need to repeat for current row
204: 		if (state.list_length < 0) {
205: 			for (idx_t col_idx = 0; col_idx < state.list_data.ColumnCount(); col_idx++) {
206: 				auto &vdata = state.list_vector_data[col_idx];
207: 				auto current_idx = vdata.sel->get_index(state.parent_position);
208: 
209: 				int64_t list_length;
210: 				// deal with NULL values
211: 				if (!vdata.validity.RowIsValid(current_idx)) {
212: 					list_length = 0;
213: 				} else {
214: 					auto list_data = (list_entry_t *)vdata.data;
215: 					auto list_entry = list_data[current_idx];
216: 					list_length = (int64_t)list_entry.length;
217: 				}
218: 
219: 				if (list_length > state.list_length) {
220: 					state.list_length = list_length;
221: 				}
222: 			}
223: 		}
224: 
225: 		D_ASSERT(state.list_length >= 0);
226: 
227: 		auto this_chunk_len = MinValue<idx_t>(STANDARD_VECTOR_SIZE, state.list_length - state.list_position);
228: 
229: 		// first cols are from child, last n cols from unnest
230: 		chunk.SetCardinality(this_chunk_len);
231: 
232: 		for (idx_t col_idx = 0; col_idx < input.ColumnCount(); col_idx++) {
233: 			ConstantVector::Reference(chunk.data[col_idx], input.data[col_idx], state.parent_position, input.size());
234: 		}
235: 
236: 		for (idx_t col_idx = 0; col_idx < state.list_data.ColumnCount(); col_idx++) {
237: 			auto &result_vector = chunk.data[col_idx + input.ColumnCount()];
238: 
239: 			if (state.list_data.data[col_idx].GetType() == LogicalType::SQLNULL) {
240: 				// UNNEST(NULL)
241: 				chunk.SetCardinality(0);
242: 			} else {
243: 				auto &vdata = state.list_vector_data[col_idx];
244: 				auto &child_data = state.list_child_data[col_idx];
245: 				auto current_idx = vdata.sel->get_index(state.parent_position);
246: 
247: 				auto list_data = (list_entry_t *)vdata.data;
248: 				auto list_entry = list_data[current_idx];
249: 
250: 				idx_t list_count;
251: 				if (state.list_position >= list_entry.length) {
252: 					list_count = 0;
253: 				} else {
254: 					list_count = MinValue<idx_t>(this_chunk_len, list_entry.length - state.list_position);
255: 				}
256: 
257: 				if (list_entry.length > state.list_position) {
258: 					if (!vdata.validity.RowIsValid(current_idx)) {
259: 						UnnestNull(0, list_count, result_vector);
260: 					} else {
261: 						auto &list_vector = state.list_data.data[col_idx];
262: 						auto &child_vector = ListVector::GetEntry(list_vector);
263: 						auto list_size = ListVector::GetListSize(list_vector);
264: 
265: 						auto base_offset = list_entry.offset + state.list_position;
266: 						UnnestVector(child_data, child_vector, list_size, base_offset, base_offset + list_count,
267: 						             result_vector);
268: 					}
269: 				}
270: 
271: 				UnnestNull(list_count, this_chunk_len, result_vector);
272: 			}
273: 		}
274: 
275: 		state.list_position += this_chunk_len;
276: 		if ((int64_t)state.list_position == state.list_length) {
277: 			state.parent_position++;
278: 			state.list_length = -1;
279: 			state.list_position = 0;
280: 		}
281: 
282: 		chunk.Verify();
283: 	} while (chunk.size() == 0);
284: 	return OperatorResultType::HAVE_MORE_OUTPUT;
285: }
286: 
287: } // namespace duckdb
[end of src/execution/operator/projection/physical_unnest.cpp]
[start of src/execution/operator/scan/physical_expression_scan.cpp]
1: #include "duckdb/execution/operator/scan/physical_expression_scan.hpp"
2: #include "duckdb/parallel/thread_context.hpp"
3: #include "duckdb/execution/expression_executor.hpp"
4: 
5: namespace duckdb {
6: 
7: class ExpressionScanState : public OperatorState {
8: public:
9: 	explicit ExpressionScanState(const PhysicalExpressionScan &op) : expression_index(0) {
10: 		temp_chunk.Initialize(op.GetTypes());
11: 	}
12: 
13: 	//! The current position in the scan
14: 	idx_t expression_index;
15: 	//! Temporary chunk for evaluating expressions
16: 	DataChunk temp_chunk;
17: };
18: 
19: unique_ptr<OperatorState> PhysicalExpressionScan::GetOperatorState(ClientContext &context) const {
20: 	return make_unique<ExpressionScanState>(*this);
21: }
22: 
23: OperatorResultType PhysicalExpressionScan::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
24:                                                    OperatorState &state_p) const {
25: 	auto &state = (ExpressionScanState &)state_p;
26: 
27: 	for (; chunk.size() + input.size() <= STANDARD_VECTOR_SIZE && state.expression_index < expressions.size();
28: 	     state.expression_index++) {
29: 		state.temp_chunk.Reset();
30: 		EvaluateExpression(state.expression_index, &input, state.temp_chunk);
31: 		chunk.Append(state.temp_chunk);
32: 	}
33: 	if (state.expression_index < expressions.size()) {
34: 		return OperatorResultType::HAVE_MORE_OUTPUT;
35: 	} else {
36: 		state.expression_index = 0;
37: 		return OperatorResultType::NEED_MORE_INPUT;
38: 	}
39: }
40: 
41: void PhysicalExpressionScan::EvaluateExpression(idx_t expression_idx, DataChunk *child_chunk, DataChunk &result) const {
42: 	ExpressionExecutor executor(expressions[expression_idx]);
43: 	if (child_chunk) {
44: 		child_chunk->Verify();
45: 		executor.Execute(*child_chunk, result);
46: 	} else {
47: 		executor.Execute(result);
48: 	}
49: }
50: 
51: bool PhysicalExpressionScan::IsFoldable() const {
52: 	for (auto &expr_list : expressions) {
53: 		for (auto &expr : expr_list) {
54: 			if (!expr->IsFoldable()) {
55: 				return false;
56: 			}
57: 		}
58: 	}
59: 	return true;
60: }
61: 
62: } // namespace duckdb
[end of src/execution/operator/scan/physical_expression_scan.cpp]
[start of src/execution/operator/set/physical_recursive_cte.cpp]
1: #include "duckdb/execution/operator/set/physical_recursive_cte.hpp"
2: 
3: #include "duckdb/common/vector_operations/vector_operations.hpp"
4: 
5: #include "duckdb/common/types/chunk_collection.hpp"
6: #include "duckdb/execution/aggregate_hashtable.hpp"
7: #include "duckdb/parallel/pipeline.hpp"
8: #include "duckdb/storage/buffer_manager.hpp"
9: #include "duckdb/parallel/task_scheduler.hpp"
10: #include "duckdb/execution/executor.hpp"
11: #include "duckdb/parallel/event.hpp"
12: 
13: namespace duckdb {
14: 
15: PhysicalRecursiveCTE::PhysicalRecursiveCTE(vector<LogicalType> types, bool union_all, unique_ptr<PhysicalOperator> top,
16:                                            unique_ptr<PhysicalOperator> bottom, idx_t estimated_cardinality)
17:     : PhysicalOperator(PhysicalOperatorType::RECURSIVE_CTE, move(types), estimated_cardinality), union_all(union_all) {
18: 	children.push_back(move(top));
19: 	children.push_back(move(bottom));
20: }
21: 
22: PhysicalRecursiveCTE::~PhysicalRecursiveCTE() {
23: }
24: 
25: //===--------------------------------------------------------------------===//
26: // Sink
27: //===--------------------------------------------------------------------===//
28: class RecursiveCTEState : public GlobalSinkState {
29: public:
30: 	explicit RecursiveCTEState(ClientContext &context, const PhysicalRecursiveCTE &op)
31: 	    : new_groups(STANDARD_VECTOR_SIZE) {
32: 		ht = make_unique<GroupedAggregateHashTable>(BufferManager::GetBufferManager(context), op.types,
33: 		                                            vector<LogicalType>(), vector<BoundAggregateExpression *>());
34: 	}
35: 
36: 	unique_ptr<GroupedAggregateHashTable> ht;
37: 
38: 	bool intermediate_empty = true;
39: 	ChunkCollection intermediate_table;
40: 	idx_t chunk_idx = 0;
41: 	SelectionVector new_groups;
42: };
43: 
44: unique_ptr<GlobalSinkState> PhysicalRecursiveCTE::GetGlobalSinkState(ClientContext &context) const {
45: 	return make_unique<RecursiveCTEState>(context, *this);
46: }
47: 
48: idx_t PhysicalRecursiveCTE::ProbeHT(DataChunk &chunk, RecursiveCTEState &state) const {
49: 	Vector dummy_addresses(LogicalType::POINTER);
50: 
51: 	// Use the HT to eliminate duplicate rows
52: 	idx_t new_group_count = state.ht->FindOrCreateGroups(chunk, dummy_addresses, state.new_groups);
53: 
54: 	// we only return entries we have not seen before (i.e. new groups)
55: 	chunk.Slice(state.new_groups, new_group_count);
56: 
57: 	return new_group_count;
58: }
59: 
60: SinkResultType PhysicalRecursiveCTE::Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate,
61:                                           DataChunk &input) const {
62: 	auto &gstate = (RecursiveCTEState &)state;
63: 	if (!union_all) {
64: 		idx_t match_count = ProbeHT(input, gstate);
65: 		if (match_count > 0) {
66: 			gstate.intermediate_table.Append(input);
67: 		}
68: 	} else {
69: 		gstate.intermediate_table.Append(input);
70: 	}
71: 	return SinkResultType::NEED_MORE_INPUT;
72: }
73: 
74: //===--------------------------------------------------------------------===//
75: // Source
76: //===--------------------------------------------------------------------===//
77: void PhysicalRecursiveCTE::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate_p,
78:                                    LocalSourceState &lstate) const {
79: 	auto &gstate = (RecursiveCTEState &)*sink_state;
80: 	while (chunk.size() == 0) {
81: 		if (gstate.chunk_idx < gstate.intermediate_table.ChunkCount()) {
82: 			// scan any chunks we have collected so far
83: 			chunk.Reference(gstate.intermediate_table.GetChunk(gstate.chunk_idx));
84: 			gstate.chunk_idx++;
85: 			break;
86: 		} else {
87: 			// we have run out of chunks
88: 			// now we need to recurse
89: 			// we set up the working table as the data we gathered in this iteration of the recursion
90: 			working_table->Reset();
91: 			working_table->Merge(gstate.intermediate_table);
92: 			// and we clear the intermediate table
93: 			gstate.intermediate_table.Reset();
94: 			gstate.chunk_idx = 0;
95: 			// now we need to re-execute all of the pipelines that depend on the recursion
96: 			ExecuteRecursivePipelines(context);
97: 
98: 			// check if we obtained any results
99: 			// if not, we are done
100: 			if (gstate.intermediate_table.Count() == 0) {
101: 				break;
102: 			}
103: 		}
104: 	}
105: }
106: 
107: void PhysicalRecursiveCTE::ExecuteRecursivePipelines(ExecutionContext &context) const {
108: 	if (pipelines.empty()) {
109: 		throw InternalException("Missing pipelines for recursive CTE");
110: 	}
111: 
112: 	for (auto &pipeline : pipelines) {
113: 		auto sink = pipeline->GetSink();
114: 		if (sink != this) {
115: 			// reset the sink state for any intermediate sinks
116: 			sink->sink_state = sink->GetGlobalSinkState(context.client);
117: 		}
118: 		pipeline->Reset();
119: 	}
120: 	auto &executor = pipelines[0]->executor;
121: 
122: 	vector<shared_ptr<Event>> events;
123: 	executor.ReschedulePipelines(pipelines, events);
124: 
125: 	while (true) {
126: 		executor.WorkOnTasks();
127: 		if (executor.HasError()) {
128: 			executor.ThrowException();
129: 		}
130: 		bool finished = true;
131: 		for (auto &event : events) {
132: 			if (!event->IsFinished()) {
133: 				finished = false;
134: 				break;
135: 			}
136: 		}
137: 		if (finished) {
138: 			// all pipelines finished: done!
139: 			break;
140: 		}
141: 	}
142: }
143: 
144: } // namespace duckdb
[end of src/execution/operator/set/physical_recursive_cte.cpp]
[start of src/execution/physical_operator.cpp]
1: #include "duckdb/execution/physical_operator.hpp"
2: 
3: #include "duckdb/common/printer.hpp"
4: #include "duckdb/common/string_util.hpp"
5: #include "duckdb/execution/execution_context.hpp"
6: #include "duckdb/main/client_context.hpp"
7: #include "duckdb/parallel/thread_context.hpp"
8: #include "duckdb/common/tree_renderer.hpp"
9: 
10: namespace duckdb {
11: 
12: string PhysicalOperator::GetName() const {
13: 	return PhysicalOperatorToString(type);
14: }
15: 
16: string PhysicalOperator::ToString() const {
17: 	TreeRenderer renderer;
18: 	return renderer.ToString(*this);
19: }
20: 
21: // LCOV_EXCL_START
22: void PhysicalOperator::Print() const {
23: 	Printer::Print(ToString());
24: }
25: // LCOV_EXCL_STOP
26: 
27: //===--------------------------------------------------------------------===//
28: // Operator
29: //===--------------------------------------------------------------------===//
30: // LCOV_EXCL_START
31: unique_ptr<OperatorState> PhysicalOperator::GetOperatorState(ClientContext &context) const {
32: 	return make_unique<OperatorState>();
33: }
34: 
35: OperatorResultType PhysicalOperator::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
36:                                              OperatorState &state) const {
37: 	throw InternalException("Calling Execute on a node that is not an operator!");
38: }
39: // LCOV_EXCL_STOP
40: 
41: //===--------------------------------------------------------------------===//
42: // Source
43: //===--------------------------------------------------------------------===//
44: unique_ptr<LocalSourceState> PhysicalOperator::GetLocalSourceState(ExecutionContext &context,
45:                                                                    GlobalSourceState &gstate) const {
46: 	return make_unique<LocalSourceState>();
47: }
48: 
49: unique_ptr<GlobalSourceState> PhysicalOperator::GetGlobalSourceState(ClientContext &context) const {
50: 	return make_unique<GlobalSourceState>();
51: }
52: 
53: // LCOV_EXCL_START
54: void PhysicalOperator::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,
55:                                LocalSourceState &lstate) const {
56: 	throw InternalException("Calling GetData on a node that is not a source!");
57: }
58: // LCOV_EXCL_STOP
59: 
60: //===--------------------------------------------------------------------===//
61: // Sink
62: //===--------------------------------------------------------------------===//
63: // LCOV_EXCL_START
64: SinkResultType PhysicalOperator::Sink(ExecutionContext &context, GlobalSinkState &gstate, LocalSinkState &lstate,
65:                                       DataChunk &input) const {
66: 	throw InternalException("Calling Sink on a node that is not a sink!");
67: }
68: // LCOV_EXCL_STOP
69: 
70: void PhysicalOperator::Combine(ExecutionContext &context, GlobalSinkState &gstate, LocalSinkState &lstate) const {
71: }
72: 
73: SinkFinalizeType PhysicalOperator::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
74:                                             GlobalSinkState &gstate) const {
75: 	return SinkFinalizeType::READY;
76: }
77: 
78: unique_ptr<LocalSinkState> PhysicalOperator::GetLocalSinkState(ExecutionContext &context) const {
79: 	return make_unique<LocalSinkState>();
80: }
81: 
82: unique_ptr<GlobalSinkState> PhysicalOperator::GetGlobalSinkState(ClientContext &context) const {
83: 	return make_unique<GlobalSinkState>();
84: }
85: 
86: } // namespace duckdb
[end of src/execution/physical_operator.cpp]
[start of src/function/scalar/list/list_aggregates.cpp]
1: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
2: #include "duckdb/planner/expression/bound_function_expression.hpp"
3: #include "duckdb/function/scalar/nested_functions.hpp"
4: #include "duckdb/planner/expression_binder.hpp"
5: #include "duckdb/catalog/catalog.hpp"
6: #include "duckdb/catalog/catalog_entry/aggregate_function_catalog_entry.hpp"
7: #include "duckdb/execution/expression_executor.hpp"
8: 
9: namespace duckdb {
10: 
11: // FIXME: use a local state for each thread to increase performance?
12: // FIXME: benchmark the use of simple_update against using update (if applicable)
13: 
14: struct ListAggregatesBindData : public FunctionData {
15: 	ListAggregatesBindData(const LogicalType &stype_p, unique_ptr<Expression> aggr_expr_p);
16: 	~ListAggregatesBindData() override;
17: 
18: 	LogicalType stype;
19: 	unique_ptr<Expression> aggr_expr;
20: 
21: 	unique_ptr<FunctionData> Copy() override;
22: };
23: 
24: ListAggregatesBindData::ListAggregatesBindData(const LogicalType &stype_p, unique_ptr<Expression> aggr_expr_p)
25:     : stype(stype_p), aggr_expr(move(aggr_expr_p)) {
26: }
27: 
28: unique_ptr<FunctionData> ListAggregatesBindData::Copy() {
29: 	return make_unique<ListAggregatesBindData>(stype, aggr_expr->Copy());
30: }
31: 
32: ListAggregatesBindData::~ListAggregatesBindData() {
33: }
34: 
35: struct StateVector {
36: 	StateVector(idx_t count_p, unique_ptr<Expression> aggr_expr_p)
37: 	    : count(count_p), aggr_expr(move(aggr_expr_p)), state_vector(Vector(LogicalType::POINTER, count_p)) {
38: 	}
39: 
40: 	~StateVector() {
41: 		// destroy objects within the aggregate states
42: 		auto &aggr = (BoundAggregateExpression &)*aggr_expr;
43: 		if (aggr.function.destructor) {
44: 			aggr.function.destructor(state_vector, count);
45: 		}
46: 	}
47: 
48: 	idx_t count;
49: 	unique_ptr<Expression> aggr_expr;
50: 	Vector state_vector;
51: };
52: 
53: static void ListAggregateFunction(DataChunk &args, ExpressionState &state, Vector &result) {
54: 
55: 	D_ASSERT(args.ColumnCount() == 2);
56: 	auto count = args.size();
57: 	Vector &lists = args.data[0];
58: 
59: 	// set the result vector
60: 	result.SetVectorType(VectorType::FLAT_VECTOR);
61: 	auto &result_validity = FlatVector::Validity(result);
62: 
63: 	if (lists.GetType().id() == LogicalTypeId::SQLNULL) {
64: 		result_validity.SetInvalid(0);
65: 		return;
66: 	}
67: 
68: 	// get the aggregate function
69: 	auto &func_expr = (BoundFunctionExpression &)state.expr;
70: 	auto &info = (ListAggregatesBindData &)*func_expr.bind_info;
71: 	auto &aggr = (BoundAggregateExpression &)*info.aggr_expr;
72: 
73: 	D_ASSERT(aggr.function.update);
74: 
75: 	auto lists_size = ListVector::GetListSize(lists);
76: 	auto &child_vector = ListVector::GetEntry(lists);
77: 
78: 	VectorData child_data;
79: 	child_vector.Orrify(lists_size, child_data);
80: 
81: 	VectorData lists_data;
82: 	lists.Orrify(count, lists_data);
83: 	auto list_entries = (list_entry_t *)lists_data.data;
84: 
85: 	// state_buffer holds the state for each list of this chunk
86: 	idx_t size = aggr.function.state_size();
87: 	auto state_buffer = unique_ptr<data_t[]>(new data_t[size * count]);
88: 
89: 	// state vector for initialize and finalize
90: 	StateVector state_vector(count, info.aggr_expr->Copy());
91: 	auto states = FlatVector::GetData<data_ptr_t>(state_vector.state_vector);
92: 
93: 	// state vector of STANDARD_VECTOR_SIZE holds the pointers to the states
94: 	Vector state_vector_update = Vector(LogicalType::POINTER);
95: 	auto states_update = FlatVector::GetData<data_ptr_t>(state_vector_update);
96: 
97: 	// selection vector pointing to the data
98: 	SelectionVector sel_vector(STANDARD_VECTOR_SIZE);
99: 	idx_t states_idx = 0;
100: 
101: 	for (idx_t i = 0; i < count; i++) {
102: 
103: 		// initialize the state for this list
104: 		auto state_ptr = state_buffer.get() + size * i;
105: 		states[i] = state_ptr;
106: 		aggr.function.initialize(states[i]);
107: 
108: 		auto lists_index = lists_data.sel->get_index(i);
109: 		const auto &list_entry = list_entries[lists_index];
110: 
111: 		// nothing to do for this list
112: 		if (!lists_data.validity.RowIsValid(lists_index)) {
113: 			result_validity.SetInvalid(i);
114: 			continue;
115: 		}
116: 
117: 		// skip empty list
118: 		if (list_entry.length == 0) {
119: 			continue;
120: 		}
121: 
122: 		auto source_idx = child_data.sel->get_index(list_entry.offset);
123: 		idx_t child_idx = 0;
124: 
125: 		while (child_idx < list_entry.length) {
126: 
127: 			// states vector is full, update
128: 			if (states_idx == STANDARD_VECTOR_SIZE) {
129: 
130: 				// update the aggregate state(s)
131: 				Vector slice = Vector(child_vector, sel_vector, states_idx);
132: 				aggr.function.update(&slice, aggr.bind_info.get(), 1, state_vector_update, states_idx);
133: 
134: 				// reset values
135: 				states_idx = 0;
136: 			}
137: 
138: 			sel_vector.set_index(states_idx, source_idx + child_idx);
139: 			states_update[states_idx] = state_ptr;
140: 			states_idx++;
141: 			child_idx++;
142: 		}
143: 	}
144: 
145: 	// update the remaining elements of the last list(s)
146: 	if (states_idx != 0) {
147: 		Vector slice = Vector(child_vector, sel_vector, states_idx);
148: 		aggr.function.update(&slice, aggr.bind_info.get(), 1, state_vector_update, states_idx);
149: 	}
150: 
151: 	// finalize all the aggregate states
152: 	aggr.function.finalize(state_vector.state_vector, aggr.bind_info.get(), result, count, 0);
153: }
154: 
155: static unique_ptr<FunctionData> ListAggregateBind(ClientContext &context, ScalarFunction &bound_function,
156:                                                   vector<unique_ptr<Expression>> &arguments) {
157: 
158: 	// the list column and the name of the aggregate function
159: 	D_ASSERT(bound_function.arguments.size() == 2);
160: 	D_ASSERT(arguments.size() == 2);
161: 
162: 	if (arguments[0]->return_type.id() == LogicalTypeId::SQLNULL) {
163: 		bound_function.arguments[0] = LogicalType::SQLNULL;
164: 		bound_function.return_type = LogicalType::SQLNULL;
165: 		return make_unique<VariableReturnBindData>(bound_function.return_type);
166: 	}
167: 
168: 	D_ASSERT(LogicalTypeId::LIST == arguments[0]->return_type.id());
169: 	auto list_child_type = ListType::GetChildType(arguments[0]->return_type);
170: 	bound_function.return_type = list_child_type;
171: 
172: 	if (!arguments[1]->IsFoldable()) {
173: 		throw InvalidInputException("Aggregate function name must be a constant");
174: 	}
175: 
176: 	// get the function name
177: 	Value function_value = ExpressionExecutor::EvaluateScalar(*arguments[1]);
178: 	auto function_name = StringValue::Get(function_value);
179: 
180: 	vector<LogicalType> types;
181: 	types.push_back(list_child_type);
182: 
183: 	// create the child expression and its type
184: 	vector<unique_ptr<Expression>> children;
185: 	auto expr = make_unique<BoundConstantExpression>(Value(LogicalType::SQLNULL));
186: 	expr->return_type = list_child_type;
187: 	children.push_back(move(expr));
188: 
189: 	// look up the aggregate function in the catalog
190: 	QueryErrorContext error_context(nullptr, 0);
191: 	auto func = (AggregateFunctionCatalogEntry *)Catalog::GetCatalog(context).GetEntry<AggregateFunctionCatalogEntry>(
192: 	    context, DEFAULT_SCHEMA, function_name, false, error_context);
193: 	D_ASSERT(func->type == CatalogType::AGGREGATE_FUNCTION_ENTRY);
194: 
195: 	// find a matching aggregate function
196: 	string error;
197: 	auto best_function_idx = Function::BindFunction(func->name, func->functions, types, error);
198: 	if (best_function_idx == DConstants::INVALID_INDEX) {
199: 		throw BinderException("No matching aggregate function");
200: 	}
201: 
202: 	// found a matching function, bind it as an aggregate
203: 	auto &best_function = func->functions[best_function_idx];
204: 	auto bound_aggr_function = AggregateFunction::BindAggregateFunction(context, best_function, move(children));
205: 
206: 	bound_function.arguments[0] =
207: 	    LogicalType::LIST(bound_aggr_function->function.arguments[0]); // for proper casting of the vectors
208: 	bound_function.return_type = bound_aggr_function->function.return_type;
209: 	return make_unique<ListAggregatesBindData>(bound_function.return_type, move(bound_aggr_function));
210: }
211: 
212: ScalarFunction ListAggregateFun::GetFunction() {
213: 	return ScalarFunction({LogicalType::LIST(LogicalType::ANY), LogicalType::VARCHAR}, LogicalType::ANY,
214: 	                      ListAggregateFunction, false, ListAggregateBind, nullptr, nullptr, nullptr);
215: }
216: 
217: void ListAggregateFun::RegisterFunction(BuiltinFunctions &set) {
218: 	set.AddFunction({"list_aggregate", "array_aggregate", "list_aggr", "array_aggr"}, GetFunction());
219: }
220: 
221: } // namespace duckdb
[end of src/function/scalar/list/list_aggregates.cpp]
[start of src/include/duckdb/execution/operator/aggregate/physical_streaming_window.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/aggregate/physical_window.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/execution/physical_operator.hpp"
12: #include "duckdb/planner/expression.hpp"
13: 
14: namespace duckdb {
15: 
16: //! PhysicalStreamingWindow implements streaming window functions (i.e. with an empty OVER clause)
17: class PhysicalStreamingWindow : public PhysicalOperator {
18: public:
19: 	PhysicalStreamingWindow(vector<LogicalType> types, vector<unique_ptr<Expression>> select_list,
20: 	                        idx_t estimated_cardinality,
21: 	                        PhysicalOperatorType type = PhysicalOperatorType::STREAMING_WINDOW);
22: 
23: 	//! The projection list of the WINDOW statement
24: 	vector<unique_ptr<Expression>> select_list;
25: 
26: public:
27: 	unique_ptr<OperatorState> GetOperatorState(ClientContext &context) const override;
28: 
29: 	OperatorResultType Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
30: 	                           OperatorState &state) const override;
31: 
32: 	string ParamsToString() const override;
33: };
34: 
35: } // namespace duckdb
[end of src/include/duckdb/execution/operator/aggregate/physical_streaming_window.hpp]
[start of src/include/duckdb/execution/operator/filter/physical_filter.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/filter/physical_filter.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/execution/physical_operator.hpp"
12: #include "duckdb/planner/expression.hpp"
13: 
14: namespace duckdb {
15: 
16: //! PhysicalFilter represents a filter operator. It removes non-matching tuples
17: //! from the result. Note that it does not physically change the data, it only
18: //! adds a selection vector to the chunk.
19: class PhysicalFilter : public PhysicalOperator {
20: public:
21: 	PhysicalFilter(vector<LogicalType> types, vector<unique_ptr<Expression>> select_list, idx_t estimated_cardinality);
22: 
23: 	//! The filter expression
24: 	unique_ptr<Expression> expression;
25: 
26: public:
27: 	unique_ptr<OperatorState> GetOperatorState(ClientContext &context) const override;
28: 
29: 	OperatorResultType Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
30: 	                           OperatorState &state) const override;
31: 
32: 	bool ParallelOperator() const override {
33: 		return true;
34: 	}
35: 	bool RequiresCache() const override {
36: 		return true;
37: 	}
38: 
39: 	string ParamsToString() const override;
40: };
41: } // namespace duckdb
[end of src/include/duckdb/execution/operator/filter/physical_filter.hpp]
[start of src/include/duckdb/execution/operator/helper/physical_streaming_sample.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/helper/physical_streaming_sample.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/execution/physical_operator.hpp"
12: #include "duckdb/parser/parsed_data/sample_options.hpp"
13: 
14: namespace duckdb {
15: 
16: //! PhysicalStreamingSample represents a streaming sample using either system or bernoulli sampling
17: class PhysicalStreamingSample : public PhysicalOperator {
18: public:
19: 	PhysicalStreamingSample(vector<LogicalType> types, SampleMethod method, double percentage, int64_t seed,
20: 	                        idx_t estimated_cardinality);
21: 
22: 	SampleMethod method;
23: 	double percentage;
24: 	int64_t seed;
25: 
26: public:
27: 	// Operator interface
28: 	unique_ptr<OperatorState> GetOperatorState(ClientContext &context) const override;
29: 	OperatorResultType Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
30: 	                           OperatorState &state) const override;
31: 
32: 	bool ParallelOperator() const override {
33: 		return true;
34: 	}
35: 
36: 	string ParamsToString() const override;
37: 
38: private:
39: 	void SystemSample(DataChunk &input, DataChunk &result, OperatorState &state) const;
40: 	void BernoulliSample(DataChunk &input, DataChunk &result, OperatorState &state) const;
41: };
42: 
43: } // namespace duckdb
[end of src/include/duckdb/execution/operator/helper/physical_streaming_sample.hpp]
[start of src/include/duckdb/execution/operator/join/physical_blockwise_nl_join.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/join/physical_blockwise_nl_join.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/chunk_collection.hpp"
12: #include "duckdb/execution/operator/join/physical_join.hpp"
13: 
14: namespace duckdb {
15: 
16: //! PhysicalBlockwiseNLJoin represents a nested loop join between two tables on arbitrary expressions. This is different
17: //! from the PhysicalNestedLoopJoin in that it does not require expressions to be comparisons between the LHS and the
18: //! RHS.
19: class PhysicalBlockwiseNLJoin : public PhysicalJoin {
20: public:
21: 	PhysicalBlockwiseNLJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left, unique_ptr<PhysicalOperator> right,
22: 	                        unique_ptr<Expression> condition, JoinType join_type, idx_t estimated_cardinality);
23: 
24: 	unique_ptr<Expression> condition;
25: 
26: public:
27: 	// Operator Interface
28: 	unique_ptr<OperatorState> GetOperatorState(ClientContext &context) const override;
29: 	OperatorResultType Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
30: 	                           OperatorState &state) const override;
31: 
32: 	bool ParallelOperator() const override {
33: 		return true;
34: 	}
35: 
36: 	bool RequiresCache() const override {
37: 		return true;
38: 	}
39: 
40: public:
41: 	// Source interface
42: 	unique_ptr<GlobalSourceState> GetGlobalSourceState(ClientContext &context) const override;
43: 	void GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,
44: 	             LocalSourceState &lstate) const override;
45: 
46: 	bool IsSource() const override {
47: 		return IsRightOuterJoin(join_type);
48: 	}
49: 	bool ParallelSource() const override {
50: 		return true;
51: 	}
52: 
53: public:
54: 	// Sink interface
55: 	unique_ptr<GlobalSinkState> GetGlobalSinkState(ClientContext &context) const override;
56: 
57: 	unique_ptr<LocalSinkState> GetLocalSinkState(ExecutionContext &context) const override;
58: 	SinkResultType Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate,
59: 	                    DataChunk &input) const override;
60: 	SinkFinalizeType Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
61: 	                          GlobalSinkState &gstate) const override;
62: 
63: 	bool IsSink() const override {
64: 		return true;
65: 	}
66: 	bool ParallelSink() const override {
67: 		return true;
68: 	}
69: 
70: public:
71: 	string ParamsToString() const override;
72: };
73: 
74: } // namespace duckdb
[end of src/include/duckdb/execution/operator/join/physical_blockwise_nl_join.hpp]
[start of src/include/duckdb/execution/operator/join/physical_cross_product.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/join/physical_cross_product.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/chunk_collection.hpp"
12: #include "duckdb/execution/physical_operator.hpp"
13: 
14: namespace duckdb {
15: //! PhysicalCrossProduct represents a cross product between two tables
16: class PhysicalCrossProduct : public PhysicalOperator {
17: public:
18: 	PhysicalCrossProduct(vector<LogicalType> types, unique_ptr<PhysicalOperator> left,
19: 	                     unique_ptr<PhysicalOperator> right, idx_t estimated_cardinality);
20: 
21: public:
22: 	// Operator Interface
23: 	unique_ptr<OperatorState> GetOperatorState(ClientContext &context) const override;
24: 	OperatorResultType Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
25: 	                           OperatorState &state) const override;
26: 
27: 	bool ParallelOperator() const override {
28: 		return true;
29: 	}
30: 
31: 	bool RequiresCache() const override {
32: 		return true;
33: 	}
34: 
35: public:
36: 	// Sink Interface
37: 	unique_ptr<GlobalSinkState> GetGlobalSinkState(ClientContext &context) const override;
38: 	SinkResultType Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate,
39: 	                    DataChunk &input) const override;
40: 
41: 	bool IsSink() const override {
42: 		return true;
43: 	}
44: 	bool ParallelSink() const override {
45: 		return true;
46: 	}
47: };
48: 
49: } // namespace duckdb
[end of src/include/duckdb/execution/operator/join/physical_cross_product.hpp]
[start of src/include/duckdb/execution/operator/join/physical_hash_join.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/join/physical_hash_join.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/chunk_collection.hpp"
12: #include "duckdb/common/value_operations/value_operations.hpp"
13: #include "duckdb/execution/join_hashtable.hpp"
14: #include "duckdb/execution/operator/join/perfect_hash_join_executor.hpp"
15: #include "duckdb/execution/operator/join/physical_comparison_join.hpp"
16: #include "duckdb/execution/physical_operator.hpp"
17: #include "duckdb/planner/operator/logical_join.hpp"
18: 
19: namespace duckdb {
20: 
21: //! PhysicalHashJoin represents a hash loop join between two tables
22: class PhysicalHashJoin : public PhysicalComparisonJoin {
23: public:
24: 	PhysicalHashJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left, unique_ptr<PhysicalOperator> right,
25: 	                 vector<JoinCondition> cond, JoinType join_type, const vector<idx_t> &left_projection_map,
26: 	                 const vector<idx_t> &right_projection_map, vector<LogicalType> delim_types,
27: 	                 idx_t estimated_cardinality, PerfectHashJoinStats perfect_join_stats);
28: 	PhysicalHashJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left, unique_ptr<PhysicalOperator> right,
29: 	                 vector<JoinCondition> cond, JoinType join_type, idx_t estimated_cardinality,
30: 	                 PerfectHashJoinStats join_state);
31: 
32: 	vector<idx_t> right_projection_map;
33: 	//! The types of the keys
34: 	vector<LogicalType> condition_types;
35: 	//! The types of all conditions
36: 	vector<LogicalType> build_types;
37: 	//! Duplicate eliminated types; only used for delim_joins (i.e. correlated subqueries)
38: 	vector<LogicalType> delim_types;
39: 	// used in perfect hash join
40: 	PerfectHashJoinStats perfect_join_statistics;
41: 
42: public:
43: 	// Operator Interface
44: 	unique_ptr<OperatorState> GetOperatorState(ClientContext &context) const override;
45: 	OperatorResultType Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
46: 	                           OperatorState &state) const override;
47: 
48: 	bool ParallelOperator() const override {
49: 		return true;
50: 	}
51: 
52: 	bool RequiresCache() const override {
53: 		return true;
54: 	}
55: 
56: public:
57: 	// Source interface
58: 	unique_ptr<GlobalSourceState> GetGlobalSourceState(ClientContext &context) const override;
59: 	void GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,
60: 	             LocalSourceState &lstate) const override;
61: 
62: 	bool IsSource() const override {
63: 		return IsRightOuterJoin(join_type);
64: 	}
65: 	bool ParallelSource() const override {
66: 		return true;
67: 	}
68: 
69: public:
70: 	// Sink Interface
71: 	unique_ptr<GlobalSinkState> GetGlobalSinkState(ClientContext &context) const override;
72: 
73: 	unique_ptr<LocalSinkState> GetLocalSinkState(ExecutionContext &context) const override;
74: 	SinkResultType Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate,
75: 	                    DataChunk &input) const override;
76: 	void Combine(ExecutionContext &context, GlobalSinkState &gstate, LocalSinkState &lstate) const override;
77: 	SinkFinalizeType Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
78: 	                          GlobalSinkState &gstate) const override;
79: 
80: 	bool IsSink() const override {
81: 		return true;
82: 	}
83: 	bool ParallelSink() const override {
84: 		return true;
85: 	}
86: };
87: 
88: } // namespace duckdb
[end of src/include/duckdb/execution/operator/join/physical_hash_join.hpp]
[start of src/include/duckdb/execution/operator/join/physical_iejoin.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/join/physical_piecewise_merge_join.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/execution/operator/join/physical_comparison_join.hpp"
12: #include "duckdb/planner/bound_result_modifier.hpp"
13: 
14: namespace duckdb {
15: 
16: class IEJoinSortedTable;
17: 
18: //! PhysicalIEJoin represents a two inequality range join between
19: //! two tables
20: class PhysicalIEJoin : public PhysicalComparisonJoin {
21: public:
22: 	PhysicalIEJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left, unique_ptr<PhysicalOperator> right,
23: 	               vector<JoinCondition> cond, JoinType join_type, idx_t estimated_cardinality);
24: 
25: 	vector<LogicalType> join_key_types;
26: 	vector<vector<BoundOrderByNode>> lhs_orders;
27: 	vector<vector<BoundOrderByNode>> rhs_orders;
28: 
29: public:
30: 	// Operator Interface
31: 	OperatorResultType Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
32: 	                           OperatorState &state) const override;
33: 
34: public:
35: 	// Source interface
36: 	unique_ptr<LocalSourceState> GetLocalSourceState(ExecutionContext &context,
37: 	                                                 GlobalSourceState &gstate) const override;
38: 	unique_ptr<GlobalSourceState> GetGlobalSourceState(ClientContext &context) const override;
39: 	void GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,
40: 	             LocalSourceState &lstate) const override;
41: 
42: 	bool IsSource() const override {
43: 		return true;
44: 	}
45: 	bool ParallelSource() const override {
46: 		return true;
47: 	}
48: 
49: public:
50: 	// Sink Interface
51: 	unique_ptr<GlobalSinkState> GetGlobalSinkState(ClientContext &context) const override;
52: 	unique_ptr<LocalSinkState> GetLocalSinkState(ExecutionContext &context) const override;
53: 	SinkResultType Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate,
54: 	                    DataChunk &input) const override;
55: 	void Combine(ExecutionContext &context, GlobalSinkState &gstate, LocalSinkState &lstate) const override;
56: 	SinkFinalizeType Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
57: 	                          GlobalSinkState &gstate) const override;
58: 
59: 	//! Schedules tasks to merge sort the current child's data during a Finalize phase
60: 	static void ScheduleMergeTasks(Pipeline &pipeline, Event &event, IEJoinSortedTable &table);
61: 
62: 	bool IsSink() const override {
63: 		return true;
64: 	}
65: 	bool ParallelSink() const override {
66: 		return true;
67: 	}
68: 
69: private:
70: 	// resolve joins that can potentially output N*M elements (INNER, LEFT, FULL)
71: 	void ResolveComplexJoin(ExecutionContext &context, DataChunk &result, LocalSourceState &state) const;
72: };
73: 
74: } // namespace duckdb
[end of src/include/duckdb/execution/operator/join/physical_iejoin.hpp]
[start of src/include/duckdb/execution/operator/join/physical_index_join.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/join/physical_index_join.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/chunk_collection.hpp"
12: #include "duckdb/execution/operator/join/physical_comparison_join.hpp"
13: #include "duckdb/execution/physical_operator.hpp"
14: #include "duckdb/planner/operator/logical_join.hpp"
15: #include "duckdb/storage/index.hpp"
16: 
17: namespace duckdb {
18: 
19: //! PhysicalIndexJoin represents an index join between two tables
20: class PhysicalIndexJoin : public PhysicalOperator {
21: public:
22: 	PhysicalIndexJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left, unique_ptr<PhysicalOperator> right,
23: 	                  vector<JoinCondition> cond, JoinType join_type, const vector<idx_t> &left_projection_map,
24: 	                  vector<idx_t> right_projection_map, vector<column_t> column_ids, Index *index, bool lhs_first,
25: 	                  idx_t estimated_cardinality);
26: 
27: 	//! Columns from RHS used in the query
28: 	vector<column_t> column_ids;
29: 	//! Columns to be fetched
30: 	vector<column_t> fetch_ids;
31: 	//! Types of fetch columns
32: 	vector<LogicalType> fetch_types;
33: 	//! Columns indexed by index
34: 	unordered_set<column_t> index_ids;
35: 	//! Projected ids from LHS
36: 	vector<column_t> left_projection_map;
37: 	//! Projected ids from RHS
38: 	vector<column_t> right_projection_map;
39: 	//! The types of the keys
40: 	vector<LogicalType> condition_types;
41: 	//! The types of all conditions
42: 	vector<LogicalType> build_types;
43: 	//! Index used for join
44: 	Index *index;
45: 
46: 	vector<JoinCondition> conditions;
47: 
48: 	JoinType join_type;
49: 	//! In case we swap rhs with lhs we need to output columns related to rhs first.
50: 	bool lhs_first = true;
51: 
52: public:
53: 	unique_ptr<OperatorState> GetOperatorState(ClientContext &context) const override;
54: 	OperatorResultType Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
55: 	                           OperatorState &state) const override;
56: 
57: 	bool ParallelOperator() const override {
58: 		return true;
59: 	}
60: 
61: 	bool RequiresCache() const override {
62: 		return true;
63: 	}
64: 
65: private:
66: 	void GetRHSMatches(ExecutionContext &context, DataChunk &input, OperatorState &state_p) const;
67: 	//! Fills result chunk
68: 	void Output(ExecutionContext &context, DataChunk &input, DataChunk &chunk, OperatorState &state_p) const;
69: };
70: 
71: } // namespace duckdb
[end of src/include/duckdb/execution/operator/join/physical_index_join.hpp]
[start of src/include/duckdb/execution/operator/join/physical_nested_loop_join.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/join/physical_nested_loop_join.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/chunk_collection.hpp"
12: #include "duckdb/execution/operator/join/physical_comparison_join.hpp"
13: 
14: namespace duckdb {
15: idx_t nested_loop_join(ExpressionType op, Vector &left, Vector &right, idx_t &lpos, idx_t &rpos, sel_t lvector[],
16:                        sel_t rvector[]);
17: idx_t nested_loop_comparison(ExpressionType op, Vector &left, Vector &right, sel_t lvector[], sel_t rvector[],
18:                              idx_t count);
19: 
20: //! PhysicalNestedLoopJoin represents a nested loop join between two tables
21: class PhysicalNestedLoopJoin : public PhysicalComparisonJoin {
22: public:
23: 	PhysicalNestedLoopJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left, unique_ptr<PhysicalOperator> right,
24: 	                       vector<JoinCondition> cond, JoinType join_type, idx_t estimated_cardinality);
25: 
26: public:
27: 	// Operator Interface
28: 	unique_ptr<OperatorState> GetOperatorState(ClientContext &context) const override;
29: 	OperatorResultType Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
30: 	                           OperatorState &state) const override;
31: 
32: 	bool ParallelOperator() const override {
33: 		return true;
34: 	}
35: 
36: 	bool RequiresCache() const override {
37: 		return true;
38: 	}
39: 
40: public:
41: 	// Source interface
42: 	unique_ptr<GlobalSourceState> GetGlobalSourceState(ClientContext &context) const override;
43: 	void GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,
44: 	             LocalSourceState &lstate) const override;
45: 
46: 	bool IsSource() const override {
47: 		return IsRightOuterJoin(join_type);
48: 	}
49: 	bool ParallelSource() const override {
50: 		return true;
51: 	}
52: 
53: public:
54: 	// Sink Interface
55: 	unique_ptr<GlobalSinkState> GetGlobalSinkState(ClientContext &context) const override;
56: 	unique_ptr<LocalSinkState> GetLocalSinkState(ExecutionContext &context) const override;
57: 	SinkResultType Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate,
58: 	                    DataChunk &input) const override;
59: 	void Combine(ExecutionContext &context, GlobalSinkState &gstate, LocalSinkState &lstate) const override;
60: 	SinkFinalizeType Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
61: 	                          GlobalSinkState &gstate) const override;
62: 
63: 	bool IsSink() const override {
64: 		return true;
65: 	}
66: 	bool ParallelSink() const override {
67: 		return true;
68: 	}
69: 
70: private:
71: 	// resolve joins that output max N elements (SEMI, ANTI, MARK)
72: 	void ResolveSimpleJoin(ExecutionContext &context, DataChunk &input, DataChunk &chunk, OperatorState &state) const;
73: 	// resolve joins that can potentially output N*M elements (INNER, LEFT, FULL)
74: 	OperatorResultType ResolveComplexJoin(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
75: 	                                      OperatorState &state) const;
76: };
77: 
78: } // namespace duckdb
[end of src/include/duckdb/execution/operator/join/physical_nested_loop_join.hpp]
[start of src/include/duckdb/execution/operator/join/physical_piecewise_merge_join.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/join/physical_piecewise_merge_join.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/execution/operator/join/physical_comparison_join.hpp"
12: #include "duckdb/planner/bound_result_modifier.hpp"
13: 
14: namespace duckdb {
15: 
16: class MergeJoinGlobalState;
17: 
18: //! PhysicalPiecewiseMergeJoin represents a piecewise merge loop join between
19: //! two tables
20: class PhysicalPiecewiseMergeJoin : public PhysicalComparisonJoin {
21: public:
22: 	PhysicalPiecewiseMergeJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left,
23: 	                           unique_ptr<PhysicalOperator> right, vector<JoinCondition> cond, JoinType join_type,
24: 	                           idx_t estimated_cardinality);
25: 
26: 	vector<LogicalType> join_key_types;
27: 	vector<BoundOrderByNode> lhs_orders;
28: 	vector<BoundOrderByNode> rhs_orders;
29: 
30: public:
31: 	// Operator Interface
32: 	unique_ptr<OperatorState> GetOperatorState(ClientContext &context) const override;
33: 	OperatorResultType Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
34: 	                           OperatorState &state) const override;
35: 
36: 	bool ParallelOperator() const override {
37: 		return true;
38: 	}
39: 
40: 	bool RequiresCache() const override {
41: 		return true;
42: 	}
43: 
44: public:
45: 	// Source interface
46: 	unique_ptr<GlobalSourceState> GetGlobalSourceState(ClientContext &context) const override;
47: 	void GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,
48: 	             LocalSourceState &lstate) const override;
49: 
50: 	bool IsSource() const override {
51: 		return IsRightOuterJoin(join_type);
52: 	}
53: 	bool ParallelSource() const override {
54: 		return true;
55: 	}
56: 
57: public:
58: 	// Sink Interface
59: 	unique_ptr<GlobalSinkState> GetGlobalSinkState(ClientContext &context) const override;
60: 	unique_ptr<LocalSinkState> GetLocalSinkState(ExecutionContext &context) const override;
61: 	SinkResultType Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate,
62: 	                    DataChunk &input) const override;
63: 	void Combine(ExecutionContext &context, GlobalSinkState &gstate, LocalSinkState &lstate) const override;
64: 	SinkFinalizeType Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
65: 	                          GlobalSinkState &gstate) const override;
66: 
67: 	//! Schedules tasks to merge sort the RHS data during the Finalize phase
68: 	static void ScheduleMergeTasks(Pipeline &pipeline, Event &event, MergeJoinGlobalState &state);
69: 
70: 	bool IsSink() const override {
71: 		return true;
72: 	}
73: 	bool ParallelSink() const override {
74: 		return true;
75: 	}
76: 
77: private:
78: 	// resolve joins that output max N elements (SEMI, ANTI, MARK)
79: 	void ResolveSimpleJoin(ExecutionContext &context, DataChunk &input, DataChunk &chunk, OperatorState &state) const;
80: 	// resolve joins that can potentially output N*M elements (INNER, LEFT, FULL)
81: 	OperatorResultType ResolveComplexJoin(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
82: 	                                      OperatorState &state) const;
83: };
84: 
85: } // namespace duckdb
[end of src/include/duckdb/execution/operator/join/physical_piecewise_merge_join.hpp]
[start of src/include/duckdb/execution/operator/projection/physical_projection.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/projection/physical_projection.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/execution/physical_operator.hpp"
12: #include "duckdb/planner/expression.hpp"
13: 
14: namespace duckdb {
15: 
16: class PhysicalProjection : public PhysicalOperator {
17: public:
18: 	PhysicalProjection(vector<LogicalType> types, vector<unique_ptr<Expression>> select_list,
19: 	                   idx_t estimated_cardinality);
20: 
21: 	vector<unique_ptr<Expression>> select_list;
22: 
23: public:
24: 	unique_ptr<OperatorState> GetOperatorState(ClientContext &context) const override;
25: 	OperatorResultType Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
26: 	                           OperatorState &state) const override;
27: 
28: 	bool ParallelOperator() const override {
29: 		return true;
30: 	}
31: 
32: 	string ParamsToString() const override;
33: };
34: 
35: } // namespace duckdb
[end of src/include/duckdb/execution/operator/projection/physical_projection.hpp]
[start of src/include/duckdb/execution/operator/projection/physical_tableinout_function.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/projection/physical_unnest.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/execution/physical_operator.hpp"
12: #include "duckdb/function/function.hpp"
13: #include "duckdb/function/table_function.hpp"
14: 
15: namespace duckdb {
16: 
17: //! PhysicalWindow implements window functions
18: class PhysicalTableInOutFunction : public PhysicalOperator {
19: public:
20: 	PhysicalTableInOutFunction(vector<LogicalType> types, TableFunction function_p,
21: 	                           unique_ptr<FunctionData> bind_data_p, vector<column_t> column_ids_p,
22: 	                           idx_t estimated_cardinality);
23: 
24: public:
25: 	unique_ptr<OperatorState> GetOperatorState(ClientContext &context) const override;
26: 	OperatorResultType Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
27: 	                           OperatorState &state) const override;
28: 
29: 	bool ParallelOperator() const override {
30: 		return true;
31: 	}
32: 
33: private:
34: 	//! The table function
35: 	TableFunction function;
36: 	//! Bind data of the function
37: 	unique_ptr<FunctionData> bind_data;
38: 
39: 	vector<column_t> column_ids;
40: };
41: 
42: } // namespace duckdb
[end of src/include/duckdb/execution/operator/projection/physical_tableinout_function.hpp]
[start of src/include/duckdb/execution/operator/projection/physical_unnest.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/projection/physical_unnest.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/chunk_collection.hpp"
12: #include "duckdb/execution/physical_operator.hpp"
13: #include "duckdb/planner/expression.hpp"
14: 
15: namespace duckdb {
16: 
17: //! PhysicalWindow implements window functions
18: class PhysicalUnnest : public PhysicalOperator {
19: public:
20: 	PhysicalUnnest(vector<LogicalType> types, vector<unique_ptr<Expression>> select_list, idx_t estimated_cardinality,
21: 	               PhysicalOperatorType type = PhysicalOperatorType::UNNEST);
22: 
23: 	//! The projection list of the SELECT statement (that contains aggregates)
24: 	vector<unique_ptr<Expression>> select_list;
25: 
26: public:
27: 	unique_ptr<OperatorState> GetOperatorState(ClientContext &context) const override;
28: 	OperatorResultType Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
29: 	                           OperatorState &state) const override;
30: 
31: 	bool ParallelOperator() const override {
32: 		return true;
33: 	}
34: };
35: 
36: } // namespace duckdb
[end of src/include/duckdb/execution/operator/projection/physical_unnest.hpp]
[start of src/include/duckdb/execution/operator/scan/physical_expression_scan.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/scan/physical_expression_scan.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/chunk_collection.hpp"
12: #include "duckdb/execution/physical_operator.hpp"
13: #include "duckdb/planner/expression.hpp"
14: 
15: namespace duckdb {
16: 
17: //! The PhysicalExpressionScan scans a set of expressions
18: class PhysicalExpressionScan : public PhysicalOperator {
19: public:
20: 	PhysicalExpressionScan(vector<LogicalType> types, vector<vector<unique_ptr<Expression>>> expressions,
21: 	                       idx_t estimated_cardinality)
22: 	    : PhysicalOperator(PhysicalOperatorType::EXPRESSION_SCAN, move(types), estimated_cardinality),
23: 	      expressions(move(expressions)) {
24: 	}
25: 
26: 	//! The set of expressions to scan
27: 	vector<vector<unique_ptr<Expression>>> expressions;
28: 
29: public:
30: 	unique_ptr<OperatorState> GetOperatorState(ClientContext &context) const override;
31: 	OperatorResultType Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
32: 	                           OperatorState &state) const override;
33: 
34: 	bool ParallelOperator() const override {
35: 		return true;
36: 	}
37: 
38: public:
39: 	bool IsFoldable() const;
40: 	void EvaluateExpression(idx_t expression_idx, DataChunk *child_chunk, DataChunk &result) const;
41: };
42: 
43: } // namespace duckdb
[end of src/include/duckdb/execution/operator/scan/physical_expression_scan.hpp]
[start of src/include/duckdb/execution/physical_operator.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/physical_operator.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/catalog/catalog.hpp"
12: #include "duckdb/common/common.hpp"
13: #include "duckdb/common/enums/physical_operator_type.hpp"
14: #include "duckdb/common/types/data_chunk.hpp"
15: #include "duckdb/execution/execution_context.hpp"
16: #include "duckdb/common/enums/operator_result_type.hpp"
17: 
18: namespace duckdb {
19: class Event;
20: class PhysicalOperator;
21: class Pipeline;
22: 
23: // LCOV_EXCL_START
24: class OperatorState {
25: public:
26: 	virtual ~OperatorState() {
27: 	}
28: 
29: 	virtual void Finalize(PhysicalOperator *op, ExecutionContext &context) {
30: 	}
31: };
32: 
33: class GlobalSinkState {
34: public:
35: 	GlobalSinkState() : state(SinkFinalizeType::READY) {
36: 	}
37: 	virtual ~GlobalSinkState() {
38: 	}
39: 
40: 	SinkFinalizeType state;
41: };
42: 
43: class LocalSinkState {
44: public:
45: 	virtual ~LocalSinkState() {
46: 	}
47: };
48: 
49: class GlobalSourceState {
50: public:
51: 	virtual ~GlobalSourceState() {
52: 	}
53: 
54: 	virtual idx_t MaxThreads() {
55: 		return 1;
56: 	}
57: };
58: 
59: class LocalSourceState {
60: public:
61: 	virtual ~LocalSourceState() {
62: 	}
63: };
64: // LCOV_EXCL_STOP
65: 
66: //! PhysicalOperator is the base class of the physical operators present in the
67: //! execution plan
68: class PhysicalOperator {
69: public:
70: 	PhysicalOperator(PhysicalOperatorType type, vector<LogicalType> types, idx_t estimated_cardinality)
71: 	    : type(type), types(std::move(types)), estimated_cardinality(estimated_cardinality) {
72: 	}
73: 	virtual ~PhysicalOperator() {
74: 	}
75: 
76: 	//! The physical operator type
77: 	PhysicalOperatorType type;
78: 	//! The set of children of the operator
79: 	vector<unique_ptr<PhysicalOperator>> children;
80: 	//! The types returned by this physical operator
81: 	vector<LogicalType> types;
82: 	//! The estimated cardinality of this physical operator
83: 	idx_t estimated_cardinality;
84: 	//! The global sink state of this operator
85: 	unique_ptr<GlobalSinkState> sink_state;
86: 
87: public:
88: 	virtual string GetName() const;
89: 	virtual string ParamsToString() const {
90: 		return "";
91: 	}
92: 	virtual string ToString() const;
93: 	void Print() const;
94: 
95: 	//! Return a vector of the types that will be returned by this operator
96: 	const vector<LogicalType> &GetTypes() const {
97: 		return types;
98: 	}
99: 
100: 	virtual bool Equals(const PhysicalOperator &other) const {
101: 		return false;
102: 	}
103: 
104: public:
105: 	// Operator interface
106: 	virtual unique_ptr<OperatorState> GetOperatorState(ClientContext &context) const;
107: 	virtual OperatorResultType Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
108: 	                                   OperatorState &state) const;
109: 
110: 	virtual bool ParallelOperator() const {
111: 		return false;
112: 	}
113: 
114: 	virtual bool RequiresCache() const {
115: 		return false;
116: 	}
117: 
118: public:
119: 	// Source interface
120: 	virtual unique_ptr<LocalSourceState> GetLocalSourceState(ExecutionContext &context,
121: 	                                                         GlobalSourceState &gstate) const;
122: 	virtual unique_ptr<GlobalSourceState> GetGlobalSourceState(ClientContext &context) const;
123: 	virtual void GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,
124: 	                     LocalSourceState &lstate) const;
125: 
126: 	virtual bool IsSource() const {
127: 		return false;
128: 	}
129: 
130: 	virtual bool ParallelSource() const {
131: 		return false;
132: 	}
133: 
134: public:
135: 	// Sink interface
136: 
137: 	//! The sink method is called constantly with new input, as long as new input is available. Note that this method
138: 	//! CAN be called in parallel, proper locking is needed when accessing data inside the GlobalSinkState.
139: 	virtual SinkResultType Sink(ExecutionContext &context, GlobalSinkState &gstate, LocalSinkState &lstate,
140: 	                            DataChunk &input) const;
141: 	// The combine is called when a single thread has completed execution of its part of the pipeline, it is the final
142: 	// time that a specific LocalSinkState is accessible. This method can be called in parallel while other Sink() or
143: 	// Combine() calls are active on the same GlobalSinkState.
144: 	virtual void Combine(ExecutionContext &context, GlobalSinkState &gstate, LocalSinkState &lstate) const;
145: 	//! The finalize is called when ALL threads are finished execution. It is called only once per pipeline, and is
146: 	//! entirely single threaded.
147: 	//! If Finalize returns SinkResultType::FINISHED, the sink is marked as finished
148: 	virtual SinkFinalizeType Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
149: 	                                  GlobalSinkState &gstate) const;
150: 
151: 	virtual unique_ptr<LocalSinkState> GetLocalSinkState(ExecutionContext &context) const;
152: 	virtual unique_ptr<GlobalSinkState> GetGlobalSinkState(ClientContext &context) const;
153: 
154: 	virtual bool IsSink() const {
155: 		return false;
156: 	}
157: 
158: 	virtual bool ParallelSink() const {
159: 		return false;
160: 	}
161: 
162: 	virtual bool SinkOrderMatters() const {
163: 		return false;
164: 	}
165: };
166: 
167: } // namespace duckdb
[end of src/include/duckdb/execution/physical_operator.hpp]
[start of src/include/duckdb/parallel/pipeline.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/parallel/pipeline.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/unordered_set.hpp"
12: #include "duckdb/execution/physical_operator.hpp"
13: #include "duckdb/function/table_function.hpp"
14: #include "duckdb/parallel/parallel_state.hpp"
15: #include "duckdb/parallel/task_scheduler.hpp"
16: #include "duckdb/common/atomic.hpp"
17: 
18: namespace duckdb {
19: class Executor;
20: class Event;
21: 
22: //! The Pipeline class represents an execution pipeline
23: class Pipeline : public std::enable_shared_from_this<Pipeline> {
24: 	friend class Executor;
25: 	friend class PipelineExecutor;
26: 	friend class PipelineEvent;
27: 	friend class PipelineFinishEvent;
28: 
29: public:
30: 	Pipeline(Executor &execution_context);
31: 
32: 	Executor &executor;
33: 
34: public:
35: 	ClientContext &GetClientContext();
36: 
37: 	void AddDependency(shared_ptr<Pipeline> &pipeline);
38: 
39: 	void Ready();
40: 	void Reset();
41: 	void ResetSource();
42: 	void Schedule(shared_ptr<Event> &event);
43: 
44: 	//! Finalize this pipeline
45: 	void Finalize(Event &event);
46: 
47: 	string ToString() const;
48: 	void Print() const;
49: 
50: 	//! Returns query progress
51: 	bool GetProgress(double &current_percentage);
52: 
53: 	//! Returns a list of all operators (including source and sink) involved in this pipeline
54: 	vector<PhysicalOperator *> GetOperators() const;
55: 
56: 	PhysicalOperator *GetSink() {
57: 		return sink;
58: 	}
59: 
60: private:
61: 	//! Whether or not the pipeline has been readied
62: 	bool ready;
63: 	//! The source of this pipeline
64: 	PhysicalOperator *source;
65: 	//! The chain of intermediate operators
66: 	vector<PhysicalOperator *> operators;
67: 	//! The sink (i.e. destination) for data; this is e.g. a hash table to-be-built
68: 	PhysicalOperator *sink;
69: 
70: 	//! The global source state
71: 	unique_ptr<GlobalSourceState> source_state;
72: 
73: 	//! The parent pipelines (i.e. pipelines that are dependent on this pipeline to finish)
74: 	vector<weak_ptr<Pipeline>> parents;
75: 	//! The dependencies of this pipeline
76: 	vector<weak_ptr<Pipeline>> dependencies;
77: 
78: private:
79: 	bool GetProgressInternal(ClientContext &context, PhysicalOperator *op, double &current_percentage);
80: 	void ScheduleSequentialTask(shared_ptr<Event> &event);
81: 	bool LaunchScanTasks(shared_ptr<Event> &event, idx_t max_threads);
82: 
83: 	bool ScheduleParallel(shared_ptr<Event> &event);
84: };
85: 
86: } // namespace duckdb
[end of src/include/duckdb/parallel/pipeline.hpp]
[start of src/include/duckdb/storage/table/scan_state.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/table/scan_state.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/storage/buffer/buffer_handle.hpp"
13: #include "duckdb/storage/storage_lock.hpp"
14: 
15: #include "duckdb/execution/adaptive_filter.hpp"
16: 
17: namespace duckdb {
18: class ColumnSegment;
19: class LocalTableStorage;
20: class Index;
21: class RowGroup;
22: class UpdateSegment;
23: class TableScanState;
24: class ColumnSegment;
25: class ValiditySegment;
26: class TableFilterSet;
27: 
28: struct SegmentScanState {
29: 	virtual ~SegmentScanState() {
30: 	}
31: };
32: 
33: struct IndexScanState {
34: 	virtual ~IndexScanState() {
35: 	}
36: };
37: 
38: typedef unordered_map<block_id_t, unique_ptr<BufferHandle>> buffer_handle_set_t;
39: 
40: struct ColumnScanState {
41: 	//! The column segment that is currently being scanned
42: 	ColumnSegment *current;
43: 	//! The current row index of the scan
44: 	idx_t row_index;
45: 	//! The internal row index (i.e. the position of the SegmentScanState)
46: 	idx_t internal_index;
47: 	//! Segment scan state
48: 	unique_ptr<SegmentScanState> scan_state;
49: 	//! Child states of the vector
50: 	vector<ColumnScanState> child_states;
51: 	//! Whether or not InitializeState has been called for this segment
52: 	bool initialized = false;
53: 	//! If this segment has already been checked for skipping purposes
54: 	bool segment_checked = false;
55: 
56: public:
57: 	//! Move the scan state forward by "count" rows (including all child states)
58: 	void Next(idx_t count);
59: 	//! Move ONLY this state forward by "count" rows (i.e. not the child states)
60: 	void NextInternal(idx_t count);
61: 	//! Move the scan state forward by STANDARD_VECTOR_SIZE rows
62: 	void NextVector();
63: };
64: 
65: struct ColumnFetchState {
66: 	//! The set of pinned block handles for this set of fetches
67: 	buffer_handle_set_t handles;
68: 	//! Any child states of the fetch
69: 	vector<unique_ptr<ColumnFetchState>> child_states;
70: };
71: 
72: struct LocalScanState {
73: 	~LocalScanState();
74: 
75: 	void SetStorage(LocalTableStorage *storage);
76: 	LocalTableStorage *GetStorage() {
77: 		return storage;
78: 	}
79: 
80: 	idx_t chunk_index;
81: 	idx_t max_index;
82: 	idx_t last_chunk_count;
83: 	TableFilterSet *table_filters;
84: 
85: private:
86: 	LocalTableStorage *storage = nullptr;
87: };
88: 
89: class RowGroupScanState {
90: public:
91: 	RowGroupScanState(TableScanState &parent_p) : parent(parent_p), vector_index(0), max_row(0) {
92: 	}
93: 
94: 	//! The parent scan state
95: 	TableScanState &parent;
96: 	//! The current row_group we are scanning
97: 	RowGroup *row_group;
98: 	//! The vector index within the row_group
99: 	idx_t vector_index;
100: 	//! The maximum row index of this row_group scan
101: 	idx_t max_row;
102: 	//! Child column scans
103: 	unique_ptr<ColumnScanState[]> column_scans;
104: 
105: public:
106: 	//! Move to the next vector, skipping past the current one
107: 	void NextVector();
108: };
109: 
110: class TableScanState {
111: public:
112: 	TableScanState() : row_group_scan_state(*this), max_row(0) {};
113: 
114: 	//! The row_group scan state
115: 	RowGroupScanState row_group_scan_state;
116: 	//! The total maximum row index
117: 	idx_t max_row;
118: 	//! The column identifiers of the scan
119: 	vector<column_t> column_ids;
120: 	//! The table filters (if any)
121: 	TableFilterSet *table_filters = nullptr;
122: 	//! Adaptive filter info (if any)
123: 	unique_ptr<AdaptiveFilter> adaptive_filter;
124: 	//! Transaction-local scan state
125: 	LocalScanState local_state;
126: 
127: public:
128: 	//! Move to the next vector
129: 	void NextVector();
130: };
131: 
132: class CreateIndexScanState : public TableScanState {
133: public:
134: 	vector<unique_ptr<StorageLockKey>> locks;
135: 	unique_lock<mutex> append_lock;
136: 	unique_lock<mutex> delete_lock;
137: };
138: 
139: } // namespace duckdb
[end of src/include/duckdb/storage/table/scan_state.hpp]
[start of src/include/duckdb/transaction/local_storage.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/transaction/local_storage.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/chunk_collection.hpp"
12: #include "duckdb/storage/table/scan_state.hpp"
13: 
14: namespace duckdb {
15: class DataTable;
16: class WriteAheadLog;
17: struct TableAppendState;
18: 
19: class LocalTableStorage {
20: public:
21: 	explicit LocalTableStorage(DataTable &table);
22: 	~LocalTableStorage();
23: 
24: 	DataTable &table;
25: 	//! The main chunk collection holding the data
26: 	ChunkCollection collection;
27: 	//! The set of unique indexes
28: 	vector<unique_ptr<Index>> indexes;
29: 	//! The set of deleted entries
30: 	unordered_map<idx_t, unique_ptr<bool[]>> deleted_entries;
31: 	//! The number of deleted rows
32: 	idx_t deleted_rows;
33: 	//! The number of active scans
34: 	atomic<idx_t> active_scans;
35: 
36: public:
37: 	void InitializeScan(LocalScanState &state, TableFilterSet *table_filters = nullptr);
38: 	idx_t EstimatedSize();
39: 
40: 	void Clear();
41: };
42: 
43: //! The LocalStorage class holds appends that have not been committed yet
44: class LocalStorage {
45: public:
46: 	struct CommitState {
47: 		unordered_map<DataTable *, unique_ptr<TableAppendState>> append_states;
48: 	};
49: 
50: public:
51: 	explicit LocalStorage(Transaction &transaction) : transaction(transaction) {
52: 	}
53: 
54: 	//! Initialize a scan of the local storage
55: 	void InitializeScan(DataTable *table, LocalScanState &state, TableFilterSet *table_filters);
56: 	//! Scan
57: 	void Scan(LocalScanState &state, const vector<column_t> &column_ids, DataChunk &result);
58: 
59: 	//! Append a chunk to the local storage
60: 	void Append(DataTable *table, DataChunk &chunk);
61: 	//! Delete a set of rows from the local storage
62: 	idx_t Delete(DataTable *table, Vector &row_ids, idx_t count);
63: 	//! Update a set of rows in the local storage
64: 	void Update(DataTable *table, Vector &row_ids, const vector<column_t> &column_ids, DataChunk &data);
65: 
66: 	//! Commits the local storage, writing it to the WAL and completing the commit
67: 	void Commit(LocalStorage::CommitState &commit_state, Transaction &transaction, WriteAheadLog *log,
68: 	            transaction_t commit_id);
69: 
70: 	bool ChangesMade() noexcept {
71: 		return table_storage.size() > 0;
72: 	}
73: 	idx_t EstimatedSize();
74: 
75: 	bool Find(DataTable *table) {
76: 		return table_storage.find(table) != table_storage.end();
77: 	}
78: 
79: 	idx_t AddedRows(DataTable *table) {
80: 		auto entry = table_storage.find(table);
81: 		if (entry == table_storage.end()) {
82: 			return 0;
83: 		}
84: 		return entry->second->collection.Count() - entry->second->deleted_rows;
85: 	}
86: 
87: 	void AddColumn(DataTable *old_dt, DataTable *new_dt, ColumnDefinition &new_column, Expression *default_value);
88: 	void ChangeType(DataTable *old_dt, DataTable *new_dt, idx_t changed_idx, const LogicalType &target_type,
89: 	                const vector<column_t> &bound_columns, Expression &cast_expr);
90: 
91: 	void FetchChunk(DataTable *table, Vector &row_ids, idx_t count, DataChunk &chunk);
92: 	vector<unique_ptr<Index>> &GetIndexes(DataTable *table);
93: 
94: private:
95: 	LocalTableStorage *GetStorage(DataTable *table);
96: 
97: 	template <class T>
98: 	bool ScanTableStorage(DataTable &table, LocalTableStorage &storage, T &&fun);
99: 
100: private:
101: 	Transaction &transaction;
102: 	unordered_map<DataTable *, unique_ptr<LocalTableStorage>> table_storage;
103: 
104: 	void Flush(DataTable &table, LocalTableStorage &storage);
105: };
106: 
107: } // namespace duckdb
[end of src/include/duckdb/transaction/local_storage.hpp]
[start of src/parallel/executor.cpp]
1: #include "duckdb/execution/executor.hpp"
2: 
3: #include "duckdb/execution/operator/helper/physical_execute.hpp"
4: #include "duckdb/execution/operator/join/physical_delim_join.hpp"
5: #include "duckdb/execution/operator/join/physical_iejoin.hpp"
6: #include "duckdb/execution/operator/scan/physical_chunk_scan.hpp"
7: #include "duckdb/execution/operator/set/physical_recursive_cte.hpp"
8: #include "duckdb/execution/physical_operator.hpp"
9: #include "duckdb/main/client_context.hpp"
10: #include "duckdb/execution/execution_context.hpp"
11: #include "duckdb/parallel/thread_context.hpp"
12: #include "duckdb/parallel/task_scheduler.hpp"
13: #include "duckdb/parallel/pipeline_executor.hpp"
14: 
15: #include "duckdb/parallel/pipeline_event.hpp"
16: #include "duckdb/parallel/pipeline_finish_event.hpp"
17: #include "duckdb/parallel/pipeline_complete_event.hpp"
18: 
19: #include <algorithm>
20: 
21: namespace duckdb {
22: 
23: Executor::Executor(ClientContext &context) : context(context) {
24: }
25: 
26: Executor::~Executor() {
27: }
28: 
29: Executor &Executor::Get(ClientContext &context) {
30: 	return context.GetExecutor();
31: }
32: 
33: void Executor::AddEvent(shared_ptr<Event> event) {
34: 	lock_guard<mutex> elock(executor_lock);
35: 	events.push_back(move(event));
36: }
37: 
38: struct PipelineEventStack {
39: 	Event *pipeline_event;
40: 	Event *pipeline_finish_event;
41: 	Event *pipeline_complete_event;
42: };
43: 
44: Pipeline *Executor::ScheduleUnionPipeline(const shared_ptr<Pipeline> &pipeline, const Pipeline *parent,
45:                                           event_map_t &event_map, vector<shared_ptr<Event>> &events) {
46: 	pipeline->Ready();
47: 
48: 	D_ASSERT(pipeline);
49: 	auto pipeline_event = make_shared<PipelineEvent>(pipeline);
50: 
51: 	auto parent_stack_entry = event_map.find(parent);
52: 	D_ASSERT(parent_stack_entry != event_map.end());
53: 
54: 	auto &parent_stack = parent_stack_entry->second;
55: 
56: 	PipelineEventStack stack;
57: 	stack.pipeline_event = pipeline_event.get();
58: 	stack.pipeline_finish_event = parent_stack.pipeline_finish_event;
59: 	stack.pipeline_complete_event = parent_stack.pipeline_complete_event;
60: 
61: 	stack.pipeline_event->AddDependency(*parent_stack.pipeline_event);
62: 	parent_stack.pipeline_finish_event->AddDependency(*pipeline_event);
63: 
64: 	events.push_back(move(pipeline_event));
65: 	event_map.insert(make_pair(pipeline.get(), stack));
66: 
67: 	auto parent_pipeline = pipeline.get();
68: 
69: 	auto union_entry = union_pipelines.find(pipeline.get());
70: 	if (union_entry != union_pipelines.end()) {
71: 		for (auto &entry : union_entry->second) {
72: 			parent_pipeline = ScheduleUnionPipeline(entry, parent_pipeline, event_map, events);
73: 		}
74: 	}
75: 
76: 	return parent_pipeline;
77: }
78: 
79: void Executor::ScheduleChildPipeline(Pipeline *parent, const shared_ptr<Pipeline> &pipeline, event_map_t &event_map,
80:                                      vector<shared_ptr<Event>> &events) {
81: 	pipeline->Ready();
82: 
83: 	auto child_ptr = pipeline.get();
84: 	auto dependencies = child_dependencies.find(child_ptr);
85: 	D_ASSERT(union_pipelines.find(child_ptr) == union_pipelines.end());
86: 	D_ASSERT(dependencies != child_dependencies.end());
87: 	// create the pipeline event and the event stack
88: 	auto pipeline_event = make_shared<PipelineEvent>(pipeline);
89: 
90: 	auto parent_entry = event_map.find(parent);
91: 	PipelineEventStack stack;
92: 	stack.pipeline_event = pipeline_event.get();
93: 	stack.pipeline_finish_event = parent_entry->second.pipeline_finish_event;
94: 	stack.pipeline_complete_event = parent_entry->second.pipeline_complete_event;
95: 
96: 	// set up the dependencies for this child pipeline
97: 	unordered_set<Event *> finish_events;
98: 	for (auto &dep : dependencies->second) {
99: 		auto dep_entry = event_map.find(dep);
100: 		D_ASSERT(dep_entry != event_map.end());
101: 		D_ASSERT(dep_entry->second.pipeline_event);
102: 		D_ASSERT(dep_entry->second.pipeline_finish_event);
103: 
104: 		auto finish_event = dep_entry->second.pipeline_finish_event;
105: 		stack.pipeline_event->AddDependency(*dep_entry->second.pipeline_event);
106: 		if (finish_events.find(finish_event) == finish_events.end()) {
107: 			finish_event->AddDependency(*stack.pipeline_event);
108: 			finish_events.insert(finish_event);
109: 		}
110: 	}
111: 
112: 	events.push_back(move(pipeline_event));
113: 	event_map.insert(make_pair(child_ptr, stack));
114: }
115: 
116: void Executor::SchedulePipeline(const shared_ptr<Pipeline> &pipeline, event_map_t &event_map,
117:                                 vector<shared_ptr<Event>> &events, bool complete_pipeline) {
118: 	D_ASSERT(pipeline);
119: 
120: 	pipeline->Ready();
121: 
122: 	auto pipeline_event = make_shared<PipelineEvent>(pipeline);
123: 	auto pipeline_finish_event = make_shared<PipelineFinishEvent>(pipeline);
124: 	auto pipeline_complete_event = make_shared<PipelineCompleteEvent>(pipeline->executor, complete_pipeline);
125: 
126: 	PipelineEventStack stack;
127: 	stack.pipeline_event = pipeline_event.get();
128: 	stack.pipeline_finish_event = pipeline_finish_event.get();
129: 	stack.pipeline_complete_event = pipeline_complete_event.get();
130: 
131: 	pipeline_finish_event->AddDependency(*pipeline_event);
132: 	pipeline_complete_event->AddDependency(*pipeline_finish_event);
133: 
134: 	events.push_back(move(pipeline_event));
135: 	events.push_back(move(pipeline_finish_event));
136: 	events.push_back(move(pipeline_complete_event));
137: 
138: 	event_map.insert(make_pair(pipeline.get(), stack));
139: 
140: 	auto union_entry = union_pipelines.find(pipeline.get());
141: 	if (union_entry != union_pipelines.end()) {
142: 		auto parent_pipeline = pipeline.get();
143: 		for (auto &entry : union_entry->second) {
144: 			parent_pipeline = ScheduleUnionPipeline(entry, parent_pipeline, event_map, events);
145: 		}
146: 	}
147: }
148: 
149: void Executor::ScheduleEventsInternal(const vector<shared_ptr<Pipeline>> &pipelines,
150:                                       unordered_map<Pipeline *, vector<shared_ptr<Pipeline>>> &child_pipelines,
151:                                       vector<shared_ptr<Event>> &events, bool main_schedule) {
152: 	D_ASSERT(events.empty());
153: 	// create all the required pipeline events
154: 	event_map_t event_map;
155: 	for (auto &pipeline : pipelines) {
156: 		SchedulePipeline(pipeline, event_map, events, main_schedule);
157: 	}
158: 	// schedule child pipelines
159: 	for (auto &entry : child_pipelines) {
160: 		// iterate in reverse order
161: 		// since child entries are added from top to bottom
162: 		// dependencies are in reverse order (bottom to top)
163: 		for (idx_t i = entry.second.size(); i > 0; i--) {
164: 			auto &child_entry = entry.second[i - 1];
165: 			ScheduleChildPipeline(entry.first, child_entry, event_map, events);
166: 		}
167: 	}
168: 	// set up the dependencies between pipeline events
169: 	for (auto &entry : event_map) {
170: 		auto pipeline = entry.first;
171: 		for (auto &dependency : pipeline->dependencies) {
172: 			auto dep = dependency.lock();
173: 			D_ASSERT(dep);
174: 			auto event_map_entry = event_map.find(dep.get());
175: 			D_ASSERT(event_map_entry != event_map.end());
176: 			auto &dep_entry = event_map_entry->second;
177: 			D_ASSERT(dep_entry.pipeline_complete_event);
178: 			entry.second.pipeline_event->AddDependency(*dep_entry.pipeline_complete_event);
179: 		}
180: 	}
181: 	// schedule the pipelines that do not have dependencies
182: 	for (auto &event : events) {
183: 		if (!event->HasDependencies()) {
184: 			event->Schedule();
185: 		}
186: 	}
187: }
188: 
189: void Executor::ScheduleEvents() {
190: 	ScheduleEventsInternal(pipelines, child_pipelines, events);
191: }
192: 
193: void Executor::ReschedulePipelines(const vector<shared_ptr<Pipeline>> &pipelines, vector<shared_ptr<Event>> &events) {
194: 	unordered_map<Pipeline *, vector<shared_ptr<Pipeline>>> child_pipelines;
195: 	ScheduleEventsInternal(pipelines, child_pipelines, events, false);
196: }
197: 
198: void Executor::ExtractPipelines(shared_ptr<Pipeline> &pipeline, vector<shared_ptr<Pipeline>> &result) {
199: 	pipeline->Ready();
200: 
201: 	auto pipeline_ptr = pipeline.get();
202: 	result.push_back(move(pipeline));
203: 	auto union_entry = union_pipelines.find(pipeline_ptr);
204: 	if (union_entry != union_pipelines.end()) {
205: 		auto &union_pipeline_list = union_entry->second;
206: 		for (auto &pipeline : union_pipeline_list) {
207: 			ExtractPipelines(pipeline, result);
208: 		}
209: 		union_pipelines.erase(pipeline_ptr);
210: 	}
211: 	auto child_entry = child_pipelines.find(pipeline_ptr);
212: 	if (child_entry != child_pipelines.end()) {
213: 		for (auto &entry : child_entry->second) {
214: 			ExtractPipelines(entry, result);
215: 		}
216: 		child_pipelines.erase(pipeline_ptr);
217: 	}
218: }
219: 
220: bool Executor::NextExecutor() {
221: 	if (root_pipeline_idx >= root_pipelines.size()) {
222: 		return false;
223: 	}
224: 	root_executor = make_unique<PipelineExecutor>(context, *root_pipelines[root_pipeline_idx]);
225: 	root_pipeline_idx++;
226: 	return true;
227: }
228: 
229: void Executor::VerifyPipeline(Pipeline &pipeline) {
230: 	D_ASSERT(!pipeline.ToString().empty());
231: 	auto operators = pipeline.GetOperators();
232: 	for (auto &other_pipeline : pipelines) {
233: 		auto other_operators = other_pipeline->GetOperators();
234: 		for (idx_t op_idx = 0; op_idx < operators.size(); op_idx++) {
235: 			for (idx_t other_idx = 0; other_idx < other_operators.size(); other_idx++) {
236: 				auto &left = *operators[op_idx];
237: 				auto &right = *other_operators[other_idx];
238: 				if (left.Equals(right)) {
239: 					D_ASSERT(right.Equals(left));
240: 				} else {
241: 					D_ASSERT(!right.Equals(left));
242: 				}
243: 			}
244: 		}
245: 	}
246: }
247: 
248: void Executor::VerifyPipelines() {
249: #ifdef DEBUG
250: 	for (auto &pipeline : pipelines) {
251: 		VerifyPipeline(*pipeline);
252: 	}
253: 	for (auto &pipeline : root_pipelines) {
254: 		VerifyPipeline(*pipeline);
255: 	}
256: #endif
257: }
258: 
259: void Executor::Initialize(PhysicalOperator *plan) {
260: 	Reset();
261: 
262: 	auto &scheduler = TaskScheduler::GetScheduler(context);
263: 	{
264: 		lock_guard<mutex> elock(executor_lock);
265: 		physical_plan = plan;
266: 
267: 		this->profiler = context.profiler;
268: 		profiler->Initialize(physical_plan);
269: 		this->producer = scheduler.CreateProducer();
270: 
271: 		auto root_pipeline = make_shared<Pipeline>(*this);
272: 		root_pipeline->sink = nullptr;
273: 		BuildPipelines(physical_plan, root_pipeline.get());
274: 
275: 		this->total_pipelines = pipelines.size();
276: 
277: 		root_pipeline_idx = 0;
278: 		ExtractPipelines(root_pipeline, root_pipelines);
279: 
280: 		VerifyPipelines();
281: 
282: 		ScheduleEvents();
283: 	}
284: }
285: 
286: void Executor::CancelTasks() {
287: 	task.reset();
288: 	// we do this by creating weak pointers to all pipelines
289: 	// then clearing our references to the pipelines
290: 	// and waiting until all pipelines have been destroyed
291: 	vector<weak_ptr<Pipeline>> weak_references;
292: 	{
293: 		lock_guard<mutex> elock(executor_lock);
294: 		if (pipelines.empty()) {
295: 			return;
296: 		}
297: 		weak_references.reserve(pipelines.size());
298: 		for (auto &pipeline : pipelines) {
299: 			weak_references.push_back(weak_ptr<Pipeline>(pipeline));
300: 		}
301: 		for (auto &kv : union_pipelines) {
302: 			for (auto &pipeline : kv.second) {
303: 				weak_references.push_back(weak_ptr<Pipeline>(pipeline));
304: 			}
305: 		}
306: 		for (auto &kv : child_pipelines) {
307: 			for (auto &pipeline : kv.second) {
308: 				weak_references.push_back(weak_ptr<Pipeline>(pipeline));
309: 			}
310: 		}
311: 		pipelines.clear();
312: 		union_pipelines.clear();
313: 		child_pipelines.clear();
314: 		events.clear();
315: 	}
316: 	WorkOnTasks();
317: 	for (auto &weak_ref : weak_references) {
318: 		while (true) {
319: 			auto weak = weak_ref.lock();
320: 			if (!weak) {
321: 				break;
322: 			}
323: 		}
324: 	}
325: }
326: 
327: void Executor::WorkOnTasks() {
328: 	auto &scheduler = TaskScheduler::GetScheduler(context);
329: 
330: 	unique_ptr<Task> task;
331: 	while (scheduler.GetTaskFromProducer(*producer, task)) {
332: 		task->Execute(TaskExecutionMode::PROCESS_ALL);
333: 		task.reset();
334: 	}
335: }
336: 
337: PendingExecutionResult Executor::ExecuteTask() {
338: 	if (execution_result != PendingExecutionResult::RESULT_NOT_READY) {
339: 		return execution_result;
340: 	}
341: 	// check if there are any incomplete pipelines
342: 	auto &scheduler = TaskScheduler::GetScheduler(context);
343: 	while (completed_pipelines < total_pipelines) {
344: 		// there are! if we don't already have a task, fetch one
345: 		if (!task) {
346: 			scheduler.GetTaskFromProducer(*producer, task);
347: 		}
348: 		if (task) {
349: 			// if we have a task, partially process it
350: 			auto result = task->Execute(TaskExecutionMode::PROCESS_PARTIAL);
351: 			if (result != TaskExecutionResult::TASK_NOT_FINISHED) {
352: 				// if the task is finished, clean it up
353: 				task.reset();
354: 			}
355: 		}
356: 		if (!HasError()) {
357: 			// we (partially) processed a task and no exceptions were thrown
358: 			// give back control to the caller
359: 			return PendingExecutionResult::RESULT_NOT_READY;
360: 		}
361: 		execution_result = PendingExecutionResult::EXECUTION_ERROR;
362: 
363: 		// an exception has occurred executing one of the pipelines
364: 		// we need to cancel all tasks associated with this executor
365: 		CancelTasks();
366: 		ThrowException();
367: 	}
368: 	D_ASSERT(!task);
369: 
370: 	lock_guard<mutex> elock(executor_lock);
371: 	pipelines.clear();
372: 	NextExecutor();
373: 	if (!exceptions.empty()) { // LCOV_EXCL_START
374: 		// an exception has occurred executing one of the pipelines
375: 		execution_result = PendingExecutionResult::EXECUTION_ERROR;
376: 		ThrowExceptionInternal();
377: 	} // LCOV_EXCL_STOP
378: 	execution_result = PendingExecutionResult::RESULT_READY;
379: 	return execution_result;
380: }
381: 
382: void Executor::Reset() {
383: 	lock_guard<mutex> elock(executor_lock);
384: 	delim_join_dependencies.clear();
385: 	recursive_cte = nullptr;
386: 	physical_plan = nullptr;
387: 	root_executor.reset();
388: 	root_pipelines.clear();
389: 	root_pipeline_idx = 0;
390: 	completed_pipelines = 0;
391: 	total_pipelines = 0;
392: 	exceptions.clear();
393: 	pipelines.clear();
394: 	events.clear();
395: 	union_pipelines.clear();
396: 	child_pipelines.clear();
397: 	child_dependencies.clear();
398: 	execution_result = PendingExecutionResult::RESULT_NOT_READY;
399: }
400: 
401: void Executor::AddChildPipeline(Pipeline *current) {
402: 	D_ASSERT(!current->operators.empty());
403: 	// found another operator that is a source
404: 	// schedule a child pipeline
405: 	auto child_pipeline = make_shared<Pipeline>(*this);
406: 	auto child_pipeline_ptr = child_pipeline.get();
407: 	child_pipeline->sink = current->sink;
408: 	child_pipeline->operators = current->operators;
409: 	child_pipeline->source = current->operators.back();
410: 	D_ASSERT(child_pipeline->source->IsSource());
411: 	child_pipeline->operators.pop_back();
412: 
413: 	vector<Pipeline *> dependencies;
414: 	dependencies.push_back(current);
415: 	auto child_entry = child_pipelines.find(current);
416: 	if (child_entry != child_pipelines.end()) {
417: 		for (auto &current_child : child_entry->second) {
418: 			D_ASSERT(child_dependencies.find(current_child.get()) != child_dependencies.end());
419: 			child_dependencies[current_child.get()].push_back(child_pipeline_ptr);
420: 		}
421: 	}
422: 	D_ASSERT(child_dependencies.find(child_pipeline_ptr) == child_dependencies.end());
423: 	child_dependencies.insert(make_pair(child_pipeline_ptr, move(dependencies)));
424: 	child_pipelines[current].push_back(move(child_pipeline));
425: }
426: 
427: void Executor::BuildPipelines(PhysicalOperator *op, Pipeline *current) {
428: 	D_ASSERT(current);
429: 	if (op->IsSink()) {
430: 		// operator is a sink, build a pipeline
431: 		op->sink_state.reset();
432: 
433: 		PhysicalOperator *pipeline_child = nullptr;
434: 		switch (op->type) {
435: 		case PhysicalOperatorType::CREATE_TABLE_AS:
436: 		case PhysicalOperatorType::INSERT:
437: 		case PhysicalOperatorType::DELETE_OPERATOR:
438: 		case PhysicalOperatorType::UPDATE:
439: 		case PhysicalOperatorType::HASH_GROUP_BY:
440: 		case PhysicalOperatorType::SIMPLE_AGGREGATE:
441: 		case PhysicalOperatorType::PERFECT_HASH_GROUP_BY:
442: 		case PhysicalOperatorType::WINDOW:
443: 		case PhysicalOperatorType::ORDER_BY:
444: 		case PhysicalOperatorType::RESERVOIR_SAMPLE:
445: 		case PhysicalOperatorType::TOP_N:
446: 		case PhysicalOperatorType::COPY_TO_FILE:
447: 		case PhysicalOperatorType::LIMIT:
448: 		case PhysicalOperatorType::LIMIT_PERCENT:
449: 		case PhysicalOperatorType::EXPLAIN_ANALYZE:
450: 			D_ASSERT(op->children.size() == 1);
451: 			// single operator:
452: 			// the operator becomes the data source of the current pipeline
453: 			current->source = op;
454: 			// we create a new pipeline starting from the child
455: 			pipeline_child = op->children[0].get();
456: 			break;
457: 		case PhysicalOperatorType::EXPORT:
458: 			// EXPORT has an optional child
459: 			// we only need to schedule child pipelines if there is a child
460: 			current->source = op;
461: 			if (op->children.empty()) {
462: 				return;
463: 			}
464: 			D_ASSERT(op->children.size() == 1);
465: 			pipeline_child = op->children[0].get();
466: 			break;
467: 		case PhysicalOperatorType::NESTED_LOOP_JOIN:
468: 		case PhysicalOperatorType::BLOCKWISE_NL_JOIN:
469: 		case PhysicalOperatorType::HASH_JOIN:
470: 		case PhysicalOperatorType::PIECEWISE_MERGE_JOIN:
471: 		case PhysicalOperatorType::CROSS_PRODUCT:
472: 			// regular join, create a pipeline with RHS source that sinks into this pipeline
473: 			pipeline_child = op->children[1].get();
474: 			// on the LHS (probe child), the operator becomes a regular operator
475: 			current->operators.push_back(op);
476: 			if (op->IsSource()) {
477: 				// FULL or RIGHT outer join
478: 				// schedule a scan of the node as a child pipeline
479: 				// this scan has to be performed AFTER all the probing has happened
480: 				if (recursive_cte) {
481: 					throw NotImplementedException("FULL and RIGHT outer joins are not supported in recursive CTEs yet");
482: 				}
483: 				AddChildPipeline(current);
484: 			}
485: 			BuildPipelines(op->children[0].get(), current);
486: 			break;
487: 		case PhysicalOperatorType::IE_JOIN: {
488: 			D_ASSERT(op->children.size() == 2);
489: 			if (recursive_cte) {
490: 				throw NotImplementedException("IEJoins are not supported in recursive CTEs yet");
491: 			}
492: 
493: 			// Build the LHS
494: 			auto lhs_pipeline = make_shared<Pipeline>(*this);
495: 			lhs_pipeline->sink = op;
496: 			D_ASSERT(op->children[0].get());
497: 			BuildPipelines(op->children[0].get(), lhs_pipeline.get());
498: 
499: 			// Build the RHS
500: 			auto rhs_pipeline = make_shared<Pipeline>(*this);
501: 			rhs_pipeline->sink = op;
502: 			D_ASSERT(op->children[1].get());
503: 			BuildPipelines(op->children[1].get(), rhs_pipeline.get());
504: 
505: 			// RHS => LHS => current
506: 			current->AddDependency(rhs_pipeline);
507: 			rhs_pipeline->AddDependency(lhs_pipeline);
508: 
509: 			pipelines.emplace_back(move(lhs_pipeline));
510: 			pipelines.emplace_back(move(rhs_pipeline));
511: 
512: 			// Now build both and scan
513: 			current->source = op;
514: 			return;
515: 		}
516: 		case PhysicalOperatorType::DELIM_JOIN: {
517: 			// duplicate eliminated join
518: 			// for delim joins, recurse into the actual join
519: 			pipeline_child = op->children[0].get();
520: 			break;
521: 		}
522: 		case PhysicalOperatorType::RECURSIVE_CTE: {
523: 			auto &cte_node = (PhysicalRecursiveCTE &)*op;
524: 
525: 			// recursive CTE
526: 			current->source = op;
527: 			// the LHS of the recursive CTE is our initial state
528: 			// we build this pipeline as normal
529: 			pipeline_child = op->children[0].get();
530: 			// for the RHS, we gather all pipelines that depend on the recursive cte
531: 			// these pipelines need to be rerun
532: 			if (recursive_cte) {
533: 				throw InternalException("Recursive CTE detected WITHIN a recursive CTE node");
534: 			}
535: 			recursive_cte = op;
536: 
537: 			auto recursive_pipeline = make_shared<Pipeline>(*this);
538: 			recursive_pipeline->sink = op;
539: 			op->sink_state.reset();
540: 			BuildPipelines(op->children[1].get(), recursive_pipeline.get());
541: 
542: 			cte_node.pipelines.push_back(move(recursive_pipeline));
543: 
544: 			recursive_cte = nullptr;
545: 			break;
546: 		}
547: 		default:
548: 			throw InternalException("Unimplemented sink type!");
549: 		}
550: 		// the current is dependent on this pipeline to complete
551: 		auto pipeline = make_shared<Pipeline>(*this);
552: 		pipeline->sink = op;
553: 		current->AddDependency(pipeline);
554: 		D_ASSERT(pipeline_child);
555: 		// recurse into the pipeline child
556: 		BuildPipelines(pipeline_child, pipeline.get());
557: 		if (op->type == PhysicalOperatorType::DELIM_JOIN) {
558: 			// for delim joins, recurse into the actual join
559: 			// any pipelines in there depend on the main pipeline
560: 			auto &delim_join = (PhysicalDelimJoin &)*op;
561: 			// any scan of the duplicate eliminated data on the RHS depends on this pipeline
562: 			// we add an entry to the mapping of (PhysicalOperator*) -> (Pipeline*)
563: 			for (auto &delim_scan : delim_join.delim_scans) {
564: 				delim_join_dependencies[delim_scan] = pipeline.get();
565: 			}
566: 			BuildPipelines(delim_join.join.get(), current);
567: 		}
568: 		if (!recursive_cte) {
569: 			// regular pipeline: schedule it
570: 			pipelines.push_back(move(pipeline));
571: 		} else {
572: 			// CTE pipeline! add it to the CTE pipelines
573: 			D_ASSERT(recursive_cte);
574: 			auto &cte = (PhysicalRecursiveCTE &)*recursive_cte;
575: 			cte.pipelines.push_back(move(pipeline));
576: 		}
577: 	} else {
578: 		// operator is not a sink! recurse in children
579: 		// first check if there is any additional action we need to do depending on the type
580: 		switch (op->type) {
581: 		case PhysicalOperatorType::DELIM_SCAN: {
582: 			D_ASSERT(op->children.empty());
583: 			auto entry = delim_join_dependencies.find(op);
584: 			D_ASSERT(entry != delim_join_dependencies.end());
585: 			// this chunk scan introduces a dependency to the current pipeline
586: 			// namely a dependency on the duplicate elimination pipeline to finish
587: 			auto delim_dependency = entry->second->shared_from_this();
588: 			D_ASSERT(delim_dependency->sink->type == PhysicalOperatorType::DELIM_JOIN);
589: 			auto &delim_join = (PhysicalDelimJoin &)*delim_dependency->sink;
590: 			current->AddDependency(delim_dependency);
591: 			current->source = (PhysicalOperator *)delim_join.distinct.get();
592: 			return;
593: 		}
594: 		case PhysicalOperatorType::EXECUTE: {
595: 			// EXECUTE statement: build pipeline on child
596: 			auto &execute = (PhysicalExecute &)*op;
597: 			BuildPipelines(execute.plan, current);
598: 			return;
599: 		}
600: 		case PhysicalOperatorType::RECURSIVE_CTE_SCAN: {
601: 			if (!recursive_cte) {
602: 				throw InternalException("Recursive CTE scan found without recursive CTE node");
603: 			}
604: 			break;
605: 		}
606: 		case PhysicalOperatorType::INDEX_JOIN: {
607: 			// index join: we only continue into the LHS
608: 			// the right side is probed by the index join
609: 			// so we don't need to do anything in the pipeline with this child
610: 			current->operators.push_back(op);
611: 			BuildPipelines(op->children[0].get(), current);
612: 			return;
613: 		}
614: 		case PhysicalOperatorType::UNION: {
615: 			if (recursive_cte) {
616: 				throw NotImplementedException("UNIONS are not supported in recursive CTEs yet");
617: 			}
618: 			auto union_pipeline = make_shared<Pipeline>(*this);
619: 			auto pipeline_ptr = union_pipeline.get();
620: 			// set up dependencies for any child pipelines to this union pipeline
621: 			auto child_entry = child_pipelines.find(current);
622: 			if (child_entry != child_pipelines.end()) {
623: 				for (auto &current_child : child_entry->second) {
624: 					D_ASSERT(child_dependencies.find(current_child.get()) != child_dependencies.end());
625: 					child_dependencies[current_child.get()].push_back(pipeline_ptr);
626: 				}
627: 			}
628: 			// for the current pipeline, continue building on the LHS
629: 			union_pipeline->operators = current->operators;
630: 			BuildPipelines(op->children[0].get(), current);
631: 			// insert the union pipeline as a union pipeline of the current node
632: 			union_pipelines[current].push_back(move(union_pipeline));
633: 
634: 			// for the union pipeline, build on the RHS
635: 			pipeline_ptr->sink = current->sink;
636: 			BuildPipelines(op->children[1].get(), pipeline_ptr);
637: 			return;
638: 		}
639: 		default:
640: 			break;
641: 		}
642: 		if (op->children.empty()) {
643: 			// source
644: 			current->source = op;
645: 		} else {
646: 			if (op->children.size() != 1) {
647: 				throw InternalException("Operator not supported yet");
648: 			}
649: 			current->operators.push_back(op);
650: 			BuildPipelines(op->children[0].get(), current);
651: 		}
652: 	}
653: }
654: 
655: vector<LogicalType> Executor::GetTypes() {
656: 	D_ASSERT(physical_plan);
657: 	return physical_plan->GetTypes();
658: }
659: 
660: void Executor::PushError(ExceptionType type, const string &exception) {
661: 	lock_guard<mutex> elock(executor_lock);
662: 	// interrupt execution of any other pipelines that belong to this executor
663: 	context.interrupted = true;
664: 	// push the exception onto the stack
665: 	exceptions.emplace_back(type, exception);
666: }
667: 
668: bool Executor::HasError() {
669: 	lock_guard<mutex> elock(executor_lock);
670: 	return !exceptions.empty();
671: }
672: 
673: void Executor::ThrowException() {
674: 	lock_guard<mutex> elock(executor_lock);
675: 	ThrowExceptionInternal();
676: }
677: 
678: void Executor::ThrowExceptionInternal() { // LCOV_EXCL_START
679: 	D_ASSERT(!exceptions.empty());
680: 	auto &entry = exceptions[0];
681: 	switch (entry.first) {
682: 	case ExceptionType::TRANSACTION:
683: 		throw TransactionException(entry.second);
684: 	case ExceptionType::CATALOG:
685: 		throw CatalogException(entry.second);
686: 	case ExceptionType::PARSER:
687: 		throw ParserException(entry.second);
688: 	case ExceptionType::BINDER:
689: 		throw BinderException(entry.second);
690: 	case ExceptionType::INTERRUPT:
691: 		throw InterruptException();
692: 	case ExceptionType::FATAL:
693: 		throw FatalException(entry.second);
694: 	case ExceptionType::INTERNAL:
695: 		throw InternalException(entry.second);
696: 	case ExceptionType::IO:
697: 		throw IOException(entry.second);
698: 	case ExceptionType::CONSTRAINT:
699: 		throw ConstraintException(entry.second);
700: 	case ExceptionType::CONVERSION:
701: 		throw ConversionException(entry.second);
702: 	default:
703: 		throw Exception(entry.second);
704: 	}
705: } // LCOV_EXCL_STOP
706: 
707: void Executor::Flush(ThreadContext &tcontext) {
708: 	profiler->Flush(tcontext.profiler);
709: }
710: 
711: bool Executor::GetPipelinesProgress(double &current_progress) { // LCOV_EXCL_START
712: 	lock_guard<mutex> elock(executor_lock);
713: 
714: 	if (!pipelines.empty()) {
715: 		return pipelines.back()->GetProgress(current_progress);
716: 	} else {
717: 		current_progress = -1;
718: 		return true;
719: 	}
720: } // LCOV_EXCL_STOP
721: 
722: unique_ptr<DataChunk> Executor::FetchChunk() {
723: 	D_ASSERT(physical_plan);
724: 
725: 	auto chunk = make_unique<DataChunk>();
726: 	root_executor->InitializeChunk(*chunk);
727: 	while (true) {
728: 		root_executor->ExecutePull(*chunk);
729: 		if (chunk->size() == 0) {
730: 			root_executor->PullFinalize();
731: 			if (NextExecutor()) {
732: 				continue;
733: 			}
734: 			break;
735: 		} else {
736: 			break;
737: 		}
738: 	}
739: 	return chunk;
740: }
741: 
742: } // namespace duckdb
[end of src/parallel/executor.cpp]
[start of src/parallel/pipeline.cpp]
1: #include "duckdb/parallel/pipeline.hpp"
2: 
3: #include "duckdb/common/printer.hpp"
4: #include "duckdb/execution/executor.hpp"
5: #include "duckdb/main/client_context.hpp"
6: #include "duckdb/parallel/thread_context.hpp"
7: #include "duckdb/parallel/task_scheduler.hpp"
8: #include "duckdb/main/database.hpp"
9: 
10: #include "duckdb/execution/operator/aggregate/physical_simple_aggregate.hpp"
11: #include "duckdb/execution/operator/aggregate/physical_window.hpp"
12: #include "duckdb/execution/operator/scan/physical_table_scan.hpp"
13: #include "duckdb/execution/operator/order/physical_order.hpp"
14: #include "duckdb/execution/operator/aggregate/physical_hash_aggregate.hpp"
15: #include "duckdb/execution/operator/join/physical_hash_join.hpp"
16: #include "duckdb/parallel/pipeline_executor.hpp"
17: #include "duckdb/parallel/pipeline_event.hpp"
18: 
19: #include "duckdb/common/algorithm.hpp"
20: #include "duckdb/common/tree_renderer.hpp"
21: 
22: namespace duckdb {
23: 
24: class PipelineTask : public ExecutorTask {
25: 	static constexpr const idx_t PARTIAL_CHUNK_COUNT = 50;
26: 
27: public:
28: 	explicit PipelineTask(Pipeline &pipeline_p, shared_ptr<Event> event_p)
29: 	    : ExecutorTask(pipeline_p.executor), pipeline(pipeline_p), event(move(event_p)) {
30: 	}
31: 
32: 	Pipeline &pipeline;
33: 	shared_ptr<Event> event;
34: 	unique_ptr<PipelineExecutor> pipeline_executor;
35: 
36: public:
37: 	TaskExecutionResult ExecuteTask(TaskExecutionMode mode) override {
38: 		if (!pipeline_executor) {
39: 			pipeline_executor = make_unique<PipelineExecutor>(pipeline.GetClientContext(), pipeline);
40: 		}
41: 		if (mode == TaskExecutionMode::PROCESS_PARTIAL) {
42: 			bool finished = pipeline_executor->Execute(PARTIAL_CHUNK_COUNT);
43: 			if (!finished) {
44: 				return TaskExecutionResult::TASK_NOT_FINISHED;
45: 			}
46: 		} else {
47: 			pipeline_executor->Execute();
48: 		}
49: 		event->FinishTask();
50: 		pipeline_executor.reset();
51: 		return TaskExecutionResult::TASK_FINISHED;
52: 	}
53: };
54: 
55: Pipeline::Pipeline(Executor &executor_p) : executor(executor_p), ready(false), source(nullptr), sink(nullptr) {
56: }
57: 
58: ClientContext &Pipeline::GetClientContext() {
59: 	return executor.context;
60: }
61: 
62: // LCOV_EXCL_START
63: bool Pipeline::GetProgressInternal(ClientContext &context, PhysicalOperator *op, double &current_percentage) {
64: 	current_percentage = -1;
65: 	switch (op->type) {
66: 	case PhysicalOperatorType::TABLE_SCAN: {
67: 		auto &get = (PhysicalTableScan &)*op;
68: 		if (get.function.table_scan_progress) {
69: 			current_percentage = get.function.table_scan_progress(context, get.bind_data.get());
70: 			return true;
71: 		}
72: 		// If the table_scan_progress is not implemented it means we don't support this function yet in the progress
73: 		// bar
74: 		return false;
75: 	}
76: 		// If it is not a table scan we go down on all children until we reach the leaf operators
77: 	default: {
78: 		vector<idx_t> progress;
79: 		vector<idx_t> cardinality;
80: 		double total_cardinality = 0;
81: 		current_percentage = 0;
82: 		for (auto &op_child : op->children) {
83: 			double child_percentage = 0;
84: 			if (!GetProgressInternal(context, op_child.get(), child_percentage)) {
85: 				return false;
86: 			}
87: 			progress.push_back(child_percentage);
88: 			cardinality.push_back(op_child->estimated_cardinality);
89: 			total_cardinality += op_child->estimated_cardinality;
90: 		}
91: 		for (size_t i = 0; i < progress.size(); i++) {
92: 			current_percentage += progress[i] * cardinality[i] / total_cardinality;
93: 		}
94: 		return true;
95: 	}
96: 	}
97: }
98: // LCOV_EXCL_STOP
99: 
100: bool Pipeline::GetProgress(double &current_percentage) {
101: 	auto &client = executor.context;
102: 	return GetProgressInternal(client, source, current_percentage);
103: }
104: 
105: void Pipeline::ScheduleSequentialTask(shared_ptr<Event> &event) {
106: 	vector<unique_ptr<Task>> tasks;
107: 	tasks.push_back(make_unique<PipelineTask>(*this, event));
108: 	event->SetTasks(move(tasks));
109: }
110: 
111: bool Pipeline::ScheduleParallel(shared_ptr<Event> &event) {
112: 	if (!sink->ParallelSink()) {
113: 		return false;
114: 	}
115: 	if (!source->ParallelSource()) {
116: 		return false;
117: 	}
118: 	for (auto &op : operators) {
119: 		if (!op->ParallelOperator()) {
120: 			return false;
121: 		}
122: 	}
123: 	idx_t max_threads = source_state->MaxThreads();
124: 	return LaunchScanTasks(event, max_threads);
125: }
126: 
127: void Pipeline::Schedule(shared_ptr<Event> &event) {
128: 	D_ASSERT(ready);
129: 	D_ASSERT(sink);
130: 	if (!ScheduleParallel(event)) {
131: 		// could not parallelize this pipeline: push a sequential task instead
132: 		ScheduleSequentialTask(event);
133: 	}
134: }
135: 
136: bool Pipeline::LaunchScanTasks(shared_ptr<Event> &event, idx_t max_threads) {
137: 	// split the scan up into parts and schedule the parts
138: 	auto &scheduler = TaskScheduler::GetScheduler(executor.context);
139: 	idx_t active_threads = scheduler.NumberOfThreads();
140: 	if (max_threads > active_threads) {
141: 		max_threads = active_threads;
142: 	}
143: 	if (max_threads <= 1) {
144: 		// too small to parallelize
145: 		return false;
146: 	}
147: 
148: 	// launch a task for every thread
149: 	vector<unique_ptr<Task>> tasks;
150: 	for (idx_t i = 0; i < max_threads; i++) {
151: 		tasks.push_back(make_unique<PipelineTask>(*this, event));
152: 	}
153: 	event->SetTasks(move(tasks));
154: 	return true;
155: }
156: 
157: void Pipeline::Reset() {
158: 	if (sink && !sink->sink_state) {
159: 		sink->sink_state = sink->GetGlobalSinkState(GetClientContext());
160: 	}
161: 	ResetSource();
162: }
163: 
164: void Pipeline::ResetSource() {
165: 	source_state = source->GetGlobalSourceState(GetClientContext());
166: }
167: 
168: void Pipeline::Ready() {
169: 	if (ready) {
170: 		return;
171: 	}
172: 	ready = true;
173: 	std::reverse(operators.begin(), operators.end());
174: 	Reset();
175: }
176: 
177: void Pipeline::Finalize(Event &event) {
178: 	D_ASSERT(ready);
179: 	try {
180: 		auto sink_state = sink->Finalize(*this, event, executor.context, *sink->sink_state);
181: 		sink->sink_state->state = sink_state;
182: 	} catch (Exception &ex) { // LCOV_EXCL_START
183: 		executor.PushError(ex.type, ex.what());
184: 	} catch (std::exception &ex) {
185: 		executor.PushError(ExceptionType::UNKNOWN_TYPE, ex.what());
186: 	} catch (...) {
187: 		executor.PushError(ExceptionType::UNKNOWN_TYPE, "Unknown exception in Finalize!");
188: 	} // LCOV_EXCL_STOP
189: }
190: 
191: void Pipeline::AddDependency(shared_ptr<Pipeline> &pipeline) {
192: 	D_ASSERT(pipeline);
193: 	dependencies.push_back(weak_ptr<Pipeline>(pipeline));
194: 	pipeline->parents.push_back(weak_ptr<Pipeline>(shared_from_this()));
195: }
196: 
197: string Pipeline::ToString() const {
198: 	TreeRenderer renderer;
199: 	return renderer.ToString(*this);
200: }
201: 
202: void Pipeline::Print() const {
203: 	Printer::Print(ToString());
204: }
205: 
206: vector<PhysicalOperator *> Pipeline::GetOperators() const {
207: 	vector<PhysicalOperator *> result;
208: 	D_ASSERT(source);
209: 	result.push_back(source);
210: 	result.insert(result.end(), operators.begin(), operators.end());
211: 	if (sink) {
212: 		result.push_back(sink);
213: 	}
214: 	return result;
215: }
216: 
217: } // namespace duckdb
[end of src/parallel/pipeline.cpp]
[start of src/parallel/pipeline_executor.cpp]
1: #include "duckdb/parallel/pipeline_executor.hpp"
2: #include "duckdb/main/client_context.hpp"
3: 
4: namespace duckdb {
5: 
6: PipelineExecutor::PipelineExecutor(ClientContext &context_p, Pipeline &pipeline_p)
7:     : pipeline(pipeline_p), thread(context_p), context(context_p, thread) {
8: 	D_ASSERT(pipeline.source_state);
9: 	local_source_state = pipeline.source->GetLocalSourceState(context, *pipeline.source_state);
10: 	if (pipeline.sink) {
11: 		local_sink_state = pipeline.sink->GetLocalSinkState(context);
12: 	}
13: 	intermediate_chunks.reserve(pipeline.operators.size());
14: 	intermediate_states.reserve(pipeline.operators.size());
15: 	cached_chunks.resize(pipeline.operators.size());
16: 	for (idx_t i = 0; i < pipeline.operators.size(); i++) {
17: 		auto prev_operator = i == 0 ? pipeline.source : pipeline.operators[i - 1];
18: 		auto current_operator = pipeline.operators[i];
19: 		auto chunk = make_unique<DataChunk>();
20: 		chunk->Initialize(prev_operator->GetTypes());
21: 		intermediate_chunks.push_back(move(chunk));
22: 		intermediate_states.push_back(current_operator->GetOperatorState(context.client));
23: 		if (pipeline.sink && !pipeline.sink->SinkOrderMatters() && current_operator->RequiresCache()) {
24: 			auto &cache_types = current_operator->GetTypes();
25: 			bool can_cache = true;
26: 			for (auto &type : cache_types) {
27: 				if (!CanCacheType(type)) {
28: 					can_cache = false;
29: 					break;
30: 				}
31: 			}
32: 			if (!can_cache) {
33: 				continue;
34: 			}
35: 			cached_chunks[i] = make_unique<DataChunk>();
36: 			cached_chunks[i]->Initialize(current_operator->GetTypes());
37: 		}
38: 		if (current_operator->IsSink() && current_operator->sink_state->state == SinkFinalizeType::NO_OUTPUT_POSSIBLE) {
39: 			// one of the operators has already figured out no output is possible
40: 			// we can skip executing the pipeline
41: 			finished_processing = true;
42: 		}
43: 	}
44: 	InitializeChunk(final_chunk);
45: }
46: 
47: bool PipelineExecutor::Execute(idx_t max_chunks) {
48: 	D_ASSERT(pipeline.sink);
49: 	bool exhausted_source = false;
50: 	auto &source_chunk = pipeline.operators.empty() ? final_chunk : *intermediate_chunks[0];
51: 	for (idx_t i = 0; i < max_chunks; i++) {
52: 		if (finished_processing) {
53: 			break;
54: 		}
55: 		source_chunk.Reset();
56: 		FetchFromSource(source_chunk);
57: 		if (source_chunk.size() == 0) {
58: 			exhausted_source = true;
59: 			break;
60: 		}
61: 		auto result = ExecutePushInternal(source_chunk);
62: 		if (result == OperatorResultType::FINISHED) {
63: 			finished_processing = true;
64: 			break;
65: 		}
66: 	}
67: 	if (!exhausted_source && !finished_processing) {
68: 		return false;
69: 	}
70: 	PushFinalize();
71: 	return true;
72: }
73: 
74: void PipelineExecutor::Execute() {
75: 	Execute(NumericLimits<idx_t>::Maximum());
76: }
77: 
78: OperatorResultType PipelineExecutor::ExecutePush(DataChunk &input) { // LCOV_EXCL_START
79: 	return ExecutePushInternal(input);
80: } // LCOV_EXCL_STOP
81: 
82: OperatorResultType PipelineExecutor::ExecutePushInternal(DataChunk &input, idx_t initial_idx) {
83: 	D_ASSERT(pipeline.sink);
84: 	if (input.size() == 0) { // LCOV_EXCL_START
85: 		return OperatorResultType::NEED_MORE_INPUT;
86: 	} // LCOV_EXCL_STOP
87: 	while (true) {
88: 		OperatorResultType result;
89: 		if (!pipeline.operators.empty()) {
90: 			final_chunk.Reset();
91: 			result = Execute(input, final_chunk, initial_idx);
92: 			if (result == OperatorResultType::FINISHED) {
93: 				return OperatorResultType::FINISHED;
94: 			}
95: 		} else {
96: 			result = OperatorResultType::NEED_MORE_INPUT;
97: 		}
98: 		auto &sink_chunk = pipeline.operators.empty() ? input : final_chunk;
99: 		if (sink_chunk.size() > 0) {
100: 			StartOperator(pipeline.sink);
101: 			D_ASSERT(pipeline.sink);
102: 			D_ASSERT(pipeline.sink->sink_state);
103: 			auto sink_result = pipeline.sink->Sink(context, *pipeline.sink->sink_state, *local_sink_state, sink_chunk);
104: 			EndOperator(pipeline.sink, nullptr);
105: 			if (sink_result == SinkResultType::FINISHED) {
106: 				return OperatorResultType::FINISHED;
107: 			}
108: 		}
109: 		if (result == OperatorResultType::NEED_MORE_INPUT) {
110: 			return OperatorResultType::NEED_MORE_INPUT;
111: 		}
112: 	}
113: 	return OperatorResultType::FINISHED;
114: }
115: 
116: void PipelineExecutor::PushFinalize() {
117: 	if (finalized) {
118: 		throw InternalException("Calling PushFinalize on a pipeline that has been finalized already");
119: 	}
120: 	finalized = true;
121: 	// flush all caches
122: 	if (!finished_processing) {
123: 		D_ASSERT(in_process_operators.empty());
124: 		for (idx_t i = 0; i < cached_chunks.size(); i++) {
125: 			if (cached_chunks[i] && cached_chunks[i]->size() > 0) {
126: 				ExecutePushInternal(*cached_chunks[i], i + 1);
127: 				cached_chunks[i].reset();
128: 			}
129: 		}
130: 	}
131: 	D_ASSERT(local_sink_state);
132: 	// run the combine for the sink
133: 	pipeline.sink->Combine(context, *pipeline.sink->sink_state, *local_sink_state);
134: 
135: 	// flush all query profiler info
136: 	for (idx_t i = 0; i < intermediate_states.size(); i++) {
137: 		intermediate_states[i]->Finalize(pipeline.operators[i], context);
138: 	}
139: 	pipeline.executor.Flush(thread);
140: 	local_sink_state.reset();
141: }
142: 
143: bool PipelineExecutor::CanCacheType(const LogicalType &type) {
144: 	switch (type.id()) {
145: 	case LogicalTypeId::LIST:
146: 	case LogicalTypeId::MAP:
147: 		return false;
148: 	case LogicalTypeId::STRUCT: {
149: 		auto &entries = StructType::GetChildTypes(type);
150: 		for (auto &entry : entries) {
151: 			if (!CanCacheType(entry.second)) {
152: 				return false;
153: 			}
154: 		}
155: 		return true;
156: 	}
157: 	default:
158: 		return true;
159: 	}
160: }
161: 
162: void PipelineExecutor::CacheChunk(DataChunk &current_chunk, idx_t operator_idx) {
163: #if STANDARD_VECTOR_SIZE >= 128
164: 	if (cached_chunks[operator_idx]) {
165: 		if (current_chunk.size() < CACHE_THRESHOLD) {
166: 			// we have filtered out a significant amount of tuples
167: 			// add this chunk to the cache and continue
168: 			auto &chunk_cache = *cached_chunks[operator_idx];
169: 			chunk_cache.Append(current_chunk);
170: 			if (chunk_cache.size() >= (STANDARD_VECTOR_SIZE - CACHE_THRESHOLD)) {
171: 				// chunk cache full: return it
172: 				current_chunk.Move(chunk_cache);
173: 				chunk_cache.Initialize(pipeline.operators[operator_idx]->GetTypes());
174: 			} else {
175: 				// chunk cache not full: probe again
176: 				current_chunk.Reset();
177: 			}
178: 		}
179: 	}
180: #endif
181: }
182: 
183: void PipelineExecutor::ExecutePull(DataChunk &result) {
184: 	if (finished_processing) {
185: 		return;
186: 	}
187: 	auto &executor = pipeline.executor;
188: 	try {
189: 		D_ASSERT(!pipeline.sink);
190: 		auto &source_chunk = pipeline.operators.empty() ? result : *intermediate_chunks[0];
191: 		while (result.size() == 0) {
192: 			if (in_process_operators.empty()) {
193: 				source_chunk.Reset();
194: 				FetchFromSource(source_chunk);
195: 				if (source_chunk.size() == 0) {
196: 					break;
197: 				}
198: 			}
199: 			if (!pipeline.operators.empty()) {
200: 				Execute(source_chunk, result);
201: 			}
202: 		}
203: 	} catch (std::exception &ex) { // LCOV_EXCL_START
204: 		if (executor.HasError()) {
205: 			executor.ThrowException();
206: 		}
207: 		throw;
208: 	} catch (...) {
209: 		if (executor.HasError()) {
210: 			executor.ThrowException();
211: 		}
212: 		throw;
213: 	} // LCOV_EXCL_STOP
214: }
215: 
216: void PipelineExecutor::PullFinalize() {
217: 	if (finalized) {
218: 		throw InternalException("Calling PullFinalize on a pipeline that has been finalized already");
219: 	}
220: 	finalized = true;
221: 	pipeline.executor.Flush(thread);
222: }
223: 
224: void PipelineExecutor::GoToSource(idx_t &current_idx, idx_t initial_idx) {
225: 	// we go back to the first operator (the source)
226: 	current_idx = initial_idx;
227: 	if (!in_process_operators.empty()) {
228: 		// ... UNLESS there is an in process operator
229: 		// if there is an in-process operator, we start executing at the latest one
230: 		// for example, if we have a join operator that has tuples left, we first need to emit those tuples
231: 		current_idx = in_process_operators.top();
232: 		in_process_operators.pop();
233: 	}
234: 	D_ASSERT(current_idx >= initial_idx);
235: }
236: 
237: OperatorResultType PipelineExecutor::Execute(DataChunk &input, DataChunk &result, idx_t initial_idx) {
238: 	if (input.size() == 0) { // LCOV_EXCL_START
239: 		return OperatorResultType::NEED_MORE_INPUT;
240: 	} // LCOV_EXCL_STOP
241: 	D_ASSERT(!pipeline.operators.empty());
242: 
243: 	idx_t current_idx;
244: 	GoToSource(current_idx, initial_idx);
245: 	if (current_idx == initial_idx) {
246: 		current_idx++;
247: 	}
248: 	if (current_idx > pipeline.operators.size()) {
249: 		result.Reference(input);
250: 		return OperatorResultType::NEED_MORE_INPUT;
251: 	}
252: 	while (true) {
253: 		if (context.client.interrupted) {
254: 			throw InterruptException();
255: 		}
256: 		// now figure out where to put the chunk
257: 		// if current_idx is the last possible index (>= operators.size()) we write to the result
258: 		// otherwise we write to an intermediate chunk
259: 		auto current_intermediate = current_idx;
260: 		auto &current_chunk =
261: 		    current_intermediate >= intermediate_chunks.size() ? result : *intermediate_chunks[current_intermediate];
262: 		current_chunk.Reset();
263: 		if (current_idx == initial_idx) {
264: 			// we went back to the source: we need more input
265: 			return OperatorResultType::NEED_MORE_INPUT;
266: 		} else {
267: 			auto &prev_chunk =
268: 			    current_intermediate == initial_idx + 1 ? input : *intermediate_chunks[current_intermediate - 1];
269: 			auto operator_idx = current_idx - 1;
270: 			auto current_operator = pipeline.operators[operator_idx];
271: 
272: 			// if current_idx > source_idx, we pass the previous' operators output through the Execute of the current
273: 			// operator
274: 			StartOperator(current_operator);
275: 			auto result = current_operator->Execute(context, prev_chunk, current_chunk,
276: 			                                        *intermediate_states[current_intermediate - 1]);
277: 			EndOperator(current_operator, &current_chunk);
278: 			if (result == OperatorResultType::HAVE_MORE_OUTPUT) {
279: 				// more data remains in this operator
280: 				// push in-process marker
281: 				in_process_operators.push(current_idx);
282: 			} else if (result == OperatorResultType::FINISHED) {
283: 				D_ASSERT(current_chunk.size() == 0);
284: 				return OperatorResultType::FINISHED;
285: 			}
286: 			current_chunk.Verify();
287: 			CacheChunk(current_chunk, operator_idx);
288: 		}
289: 
290: 		if (current_chunk.size() == 0) {
291: 			// no output from this operator!
292: 			if (current_idx == initial_idx) {
293: 				// if we got no output from the scan, we are done
294: 				break;
295: 			} else {
296: 				// if we got no output from an intermediate op
297: 				// we go back and try to pull data from the source again
298: 				GoToSource(current_idx, initial_idx);
299: 				continue;
300: 			}
301: 		} else {
302: 			// we got output! continue to the next operator
303: 			current_idx++;
304: 			if (current_idx > pipeline.operators.size()) {
305: 				// if we got output and are at the last operator, we are finished executing for this output chunk
306: 				// return the data and push it into the chunk
307: 				break;
308: 			}
309: 		}
310: 	}
311: 	return in_process_operators.empty() ? OperatorResultType::NEED_MORE_INPUT : OperatorResultType::HAVE_MORE_OUTPUT;
312: }
313: 
314: void PipelineExecutor::FetchFromSource(DataChunk &result) {
315: 	StartOperator(pipeline.source);
316: 	pipeline.source->GetData(context, result, *pipeline.source_state, *local_source_state);
317: 	EndOperator(pipeline.source, &result);
318: }
319: 
320: void PipelineExecutor::InitializeChunk(DataChunk &chunk) {
321: 	PhysicalOperator *last_op = pipeline.operators.empty() ? pipeline.source : pipeline.operators.back();
322: 	chunk.Initialize(last_op->GetTypes());
323: }
324: 
325: void PipelineExecutor::StartOperator(PhysicalOperator *op) {
326: 	if (context.client.interrupted) {
327: 		throw InterruptException();
328: 	}
329: 	context.thread.profiler.StartOperator(op);
330: }
331: 
332: void PipelineExecutor::EndOperator(PhysicalOperator *op, DataChunk *chunk) {
333: 	context.thread.profiler.EndOperator(chunk);
334: 
335: 	if (chunk) {
336: 		chunk->Verify();
337: 	}
338: }
339: 
340: } // namespace duckdb
[end of src/parallel/pipeline_executor.cpp]
[start of src/storage/compression/validity_uncompressed.cpp]
1: #include "duckdb/storage/segment/uncompressed.hpp"
2: #include "duckdb/storage/buffer_manager.hpp"
3: #include "duckdb/common/types/vector.hpp"
4: #include "duckdb/storage/table/append_state.hpp"
5: #include "duckdb/storage/statistics/validity_statistics.hpp"
6: #include "duckdb/common/types/null_value.hpp"
7: #include "duckdb/storage/table/column_segment.hpp"
8: #include "duckdb/function/compression_function.hpp"
9: #include "duckdb/main/config.hpp"
10: 
11: namespace duckdb {
12: 
13: //===--------------------------------------------------------------------===//
14: // Mask constants
15: //===--------------------------------------------------------------------===//
16: // LOWER_MASKS contains masks with all the lower bits set until a specific value
17: // LOWER_MASKS[0] has the 0 lowest bits set, i.e.:
18: // 0b0000000000000000000000000000000000000000000000000000000000000000,
19: // LOWER_MASKS[10] has the 10 lowest bits set, i.e.:
20: // 0b0000000000000000000000000000000000000000000000000000000111111111,
21: // etc...
22: // 0b0000000000000000000000000000000000000001111111111111111111111111,
23: // ...
24: // 0b0000000000000000000001111111111111111111111111111111111111111111,
25: // until LOWER_MASKS[64], which has all bits set:
26: // 0b1111111111111111111111111111111111111111111111111111111111111111
27: // generated with this python snippet:
28: // for i in range(65):
29: //   print(hex(int((64 - i) * '0' + i * '1', 2)) + ",")
30: const validity_t ValidityUncompressed::LOWER_MASKS[] = {0x0,
31:                                                         0x1,
32:                                                         0x3,
33:                                                         0x7,
34:                                                         0xf,
35:                                                         0x1f,
36:                                                         0x3f,
37:                                                         0x7f,
38:                                                         0xff,
39:                                                         0x1ff,
40:                                                         0x3ff,
41:                                                         0x7ff,
42:                                                         0xfff,
43:                                                         0x1fff,
44:                                                         0x3fff,
45:                                                         0x7fff,
46:                                                         0xffff,
47:                                                         0x1ffff,
48:                                                         0x3ffff,
49:                                                         0x7ffff,
50:                                                         0xfffff,
51:                                                         0x1fffff,
52:                                                         0x3fffff,
53:                                                         0x7fffff,
54:                                                         0xffffff,
55:                                                         0x1ffffff,
56:                                                         0x3ffffff,
57:                                                         0x7ffffff,
58:                                                         0xfffffff,
59:                                                         0x1fffffff,
60:                                                         0x3fffffff,
61:                                                         0x7fffffff,
62:                                                         0xffffffff,
63:                                                         0x1ffffffff,
64:                                                         0x3ffffffff,
65:                                                         0x7ffffffff,
66:                                                         0xfffffffff,
67:                                                         0x1fffffffff,
68:                                                         0x3fffffffff,
69:                                                         0x7fffffffff,
70:                                                         0xffffffffff,
71:                                                         0x1ffffffffff,
72:                                                         0x3ffffffffff,
73:                                                         0x7ffffffffff,
74:                                                         0xfffffffffff,
75:                                                         0x1fffffffffff,
76:                                                         0x3fffffffffff,
77:                                                         0x7fffffffffff,
78:                                                         0xffffffffffff,
79:                                                         0x1ffffffffffff,
80:                                                         0x3ffffffffffff,
81:                                                         0x7ffffffffffff,
82:                                                         0xfffffffffffff,
83:                                                         0x1fffffffffffff,
84:                                                         0x3fffffffffffff,
85:                                                         0x7fffffffffffff,
86:                                                         0xffffffffffffff,
87:                                                         0x1ffffffffffffff,
88:                                                         0x3ffffffffffffff,
89:                                                         0x7ffffffffffffff,
90:                                                         0xfffffffffffffff,
91:                                                         0x1fffffffffffffff,
92:                                                         0x3fffffffffffffff,
93:                                                         0x7fffffffffffffff,
94:                                                         0xffffffffffffffff};
95: 
96: // UPPER_MASKS contains masks with all the highest bits set until a specific value
97: // UPPER_MASKS[0] has the 0 highest bits set, i.e.:
98: // 0b0000000000000000000000000000000000000000000000000000000000000000,
99: // UPPER_MASKS[10] has the 10 highest bits set, i.e.:
100: // 0b1111111111110000000000000000000000000000000000000000000000000000,
101: // etc...
102: // 0b1111111111111111111111110000000000000000000000000000000000000000,
103: // ...
104: // 0b1111111111111111111111111111111111111110000000000000000000000000,
105: // until UPPER_MASKS[64], which has all bits set:
106: // 0b1111111111111111111111111111111111111111111111111111111111111111
107: // generated with this python snippet:
108: // for i in range(65):
109: //   print(hex(int(i * '1' + (64 - i) * '0', 2)) + ",")
110: const validity_t ValidityUncompressed::UPPER_MASKS[] = {0x0,
111:                                                         0x8000000000000000,
112:                                                         0xc000000000000000,
113:                                                         0xe000000000000000,
114:                                                         0xf000000000000000,
115:                                                         0xf800000000000000,
116:                                                         0xfc00000000000000,
117:                                                         0xfe00000000000000,
118:                                                         0xff00000000000000,
119:                                                         0xff80000000000000,
120:                                                         0xffc0000000000000,
121:                                                         0xffe0000000000000,
122:                                                         0xfff0000000000000,
123:                                                         0xfff8000000000000,
124:                                                         0xfffc000000000000,
125:                                                         0xfffe000000000000,
126:                                                         0xffff000000000000,
127:                                                         0xffff800000000000,
128:                                                         0xffffc00000000000,
129:                                                         0xffffe00000000000,
130:                                                         0xfffff00000000000,
131:                                                         0xfffff80000000000,
132:                                                         0xfffffc0000000000,
133:                                                         0xfffffe0000000000,
134:                                                         0xffffff0000000000,
135:                                                         0xffffff8000000000,
136:                                                         0xffffffc000000000,
137:                                                         0xffffffe000000000,
138:                                                         0xfffffff000000000,
139:                                                         0xfffffff800000000,
140:                                                         0xfffffffc00000000,
141:                                                         0xfffffffe00000000,
142:                                                         0xffffffff00000000,
143:                                                         0xffffffff80000000,
144:                                                         0xffffffffc0000000,
145:                                                         0xffffffffe0000000,
146:                                                         0xfffffffff0000000,
147:                                                         0xfffffffff8000000,
148:                                                         0xfffffffffc000000,
149:                                                         0xfffffffffe000000,
150:                                                         0xffffffffff000000,
151:                                                         0xffffffffff800000,
152:                                                         0xffffffffffc00000,
153:                                                         0xffffffffffe00000,
154:                                                         0xfffffffffff00000,
155:                                                         0xfffffffffff80000,
156:                                                         0xfffffffffffc0000,
157:                                                         0xfffffffffffe0000,
158:                                                         0xffffffffffff0000,
159:                                                         0xffffffffffff8000,
160:                                                         0xffffffffffffc000,
161:                                                         0xffffffffffffe000,
162:                                                         0xfffffffffffff000,
163:                                                         0xfffffffffffff800,
164:                                                         0xfffffffffffffc00,
165:                                                         0xfffffffffffffe00,
166:                                                         0xffffffffffffff00,
167:                                                         0xffffffffffffff80,
168:                                                         0xffffffffffffffc0,
169:                                                         0xffffffffffffffe0,
170:                                                         0xfffffffffffffff0,
171:                                                         0xfffffffffffffff8,
172:                                                         0xfffffffffffffffc,
173:                                                         0xfffffffffffffffe,
174:                                                         0xffffffffffffffff};
175: 
176: //===--------------------------------------------------------------------===//
177: // Analyze
178: //===--------------------------------------------------------------------===//
179: struct ValidityAnalyzeState : public AnalyzeState {
180: 	ValidityAnalyzeState() : count(0) {
181: 	}
182: 
183: 	idx_t count;
184: };
185: 
186: unique_ptr<AnalyzeState> ValidityInitAnalyze(ColumnData &col_data, PhysicalType type) {
187: 	return make_unique<ValidityAnalyzeState>();
188: }
189: 
190: bool ValidityAnalyze(AnalyzeState &state_p, Vector &input, idx_t count) {
191: 	auto &state = (ValidityAnalyzeState &)state_p;
192: 	state.count += count;
193: 	return true;
194: }
195: 
196: idx_t ValidityFinalAnalyze(AnalyzeState &state_p) {
197: 	auto &state = (ValidityAnalyzeState &)state_p;
198: 	return (state.count + 7) / 8;
199: }
200: 
201: //===--------------------------------------------------------------------===//
202: // Scan
203: //===--------------------------------------------------------------------===//
204: struct ValidityScanState : public SegmentScanState {
205: 	unique_ptr<BufferHandle> handle;
206: };
207: 
208: unique_ptr<SegmentScanState> ValidityInitScan(ColumnSegment &segment) {
209: 	auto result = make_unique<ValidityScanState>();
210: 	auto &buffer_manager = BufferManager::GetBufferManager(segment.db);
211: 	result->handle = buffer_manager.Pin(segment.block);
212: 	return move(result);
213: }
214: 
215: //===--------------------------------------------------------------------===//
216: // Scan base data
217: //===--------------------------------------------------------------------===//
218: void ValidityScanPartial(ColumnSegment &segment, ColumnScanState &state, idx_t scan_count, Vector &result,
219:                          idx_t result_offset) {
220: 	auto start = segment.GetRelativeIndex(state.row_index);
221: 
222: 	static_assert(sizeof(validity_t) == sizeof(uint64_t), "validity_t should be 64-bit");
223: 	auto &scan_state = (ValidityScanState &)*state.scan_state;
224: 
225: 	auto &result_mask = FlatVector::Validity(result);
226: 	auto buffer_ptr = scan_state.handle->node->buffer + segment.GetBlockOffset();
227: 	auto input_data = (validity_t *)buffer_ptr;
228: 
229: #ifdef DEBUG
230: 	// this method relies on all the bits we are going to write to being set to valid
231: 	for (idx_t i = 0; i < scan_count; i++) {
232: 		D_ASSERT(result_mask.RowIsValid(result_offset + i));
233: 	}
234: #endif
235: #if STANDARD_VECTOR_SIZE < 128
236: 	// fallback for tiny vector sizes
237: 	// the bitwise ops we use below don't work if the vector size is too small
238: 	ValidityMask source_mask(input_data);
239: 	for (idx_t i = 0; i < scan_count; i++) {
240: 		result_mask.Set(result_offset + i, source_mask.RowIsValid(start + i));
241: 	}
242: #else
243: 	// the code below does what the fallback code above states, but using bitwise ops:
244: 	auto result_data = (validity_t *)result_mask.GetData();
245: 
246: 	// set up the initial positions
247: 	// we need to find the validity_entry to modify, together with the bit-index WITHIN the validity entry
248: 	idx_t result_entry = result_offset / ValidityMask::BITS_PER_VALUE;
249: 	idx_t result_idx = result_offset - result_entry * ValidityMask::BITS_PER_VALUE;
250: 
251: 	// same for the input: find the validity_entry we are pulling from, together with the bit-index WITHIN that entry
252: 	idx_t input_entry = start / ValidityMask::BITS_PER_VALUE;
253: 	idx_t input_idx = start - input_entry * ValidityMask::BITS_PER_VALUE;
254: 
255: 	// now start the bit games
256: 	idx_t pos = 0;
257: 	while (pos < scan_count) {
258: 		// these are the current validity entries we are dealing with
259: 		idx_t current_result_idx = result_entry;
260: 		idx_t offset;
261: 		validity_t input_mask = input_data[input_entry];
262: 
263: 		// construct the mask to AND together with the result
264: 		if (result_idx < input_idx) {
265: 			// we have to shift the input RIGHT if the result_idx is smaller than the input_idx
266: 			auto shift_amount = input_idx - result_idx;
267: 			D_ASSERT(shift_amount > 0 && shift_amount <= ValidityMask::BITS_PER_VALUE);
268: 
269: 			input_mask = input_mask >> shift_amount;
270: 
271: 			// now the upper "shift_amount" bits are set to 0
272: 			// we need them to be set to 1
273: 			// otherwise the subsequent bitwise & will modify values outside of the range of values we want to alter
274: 			input_mask |= ValidityUncompressed::UPPER_MASKS[shift_amount];
275: 
276: 			// after this, we move to the next input_entry
277: 			offset = ValidityMask::BITS_PER_VALUE - input_idx;
278: 			input_entry++;
279: 			input_idx = 0;
280: 			result_idx += offset;
281: 		} else if (result_idx > input_idx) {
282: 			// we have to shift the input LEFT if the result_idx is bigger than the input_idx
283: 			auto shift_amount = result_idx - input_idx;
284: 			D_ASSERT(shift_amount > 0 && shift_amount <= ValidityMask::BITS_PER_VALUE);
285: 
286: 			// to avoid overflows, we set the upper "shift_amount" values to 0 first
287: 			input_mask = (input_mask & ~ValidityUncompressed::UPPER_MASKS[shift_amount]) << shift_amount;
288: 
289: 			// now the lower "shift_amount" bits are set to 0
290: 			// we need them to be set to 1
291: 			// otherwise the subsequent bitwise & will modify values outside of the range of values we want to alter
292: 			input_mask |= ValidityUncompressed::LOWER_MASKS[shift_amount];
293: 
294: 			// after this, we move to the next result_entry
295: 			offset = ValidityMask::BITS_PER_VALUE - result_idx;
296: 			result_entry++;
297: 			result_idx = 0;
298: 			input_idx += offset;
299: 		} else {
300: 			// if the input_idx is equal to result_idx they are already aligned
301: 			// we just move to the next entry for both after this
302: 			offset = ValidityMask::BITS_PER_VALUE - result_idx;
303: 			input_entry++;
304: 			result_entry++;
305: 			result_idx = input_idx = 0;
306: 		}
307: 		// now we need to check if we should include the ENTIRE mask
308: 		// OR if we need to mask from the right side
309: 		pos += offset;
310: 		if (pos > scan_count) {
311: 			// we need to set any bits that are past the scan_count on the right-side to 1
312: 			// this is required so we don't influence any bits that are not part of the scan
313: 			input_mask |= ValidityUncompressed::UPPER_MASKS[pos - scan_count];
314: 		}
315: 		// now finally we can merge the input mask with the result mask
316: 		if (input_mask != ValidityMask::ValidityBuffer::MAX_ENTRY) {
317: 			if (!result_data) {
318: 				result_mask.Initialize(MaxValue<idx_t>(STANDARD_VECTOR_SIZE, result_offset + scan_count));
319: 				result_data = (validity_t *)result_mask.GetData();
320: 			}
321: 			result_data[current_result_idx] &= input_mask;
322: 		}
323: 	}
324: #endif
325: 
326: #ifdef DEBUG
327: 	// verify that we actually accomplished the bitwise ops equivalent that we wanted to do
328: 	ValidityMask input_mask(input_data);
329: 	for (idx_t i = 0; i < scan_count; i++) {
330: 		D_ASSERT(result_mask.RowIsValid(result_offset + i) == input_mask.RowIsValid(start + i));
331: 	}
332: #endif
333: }
334: 
335: void ValidityScan(ColumnSegment &segment, ColumnScanState &state, idx_t scan_count, Vector &result) {
336: 	result.Normalify(scan_count);
337: 
338: 	auto start = segment.GetRelativeIndex(state.row_index);
339: 	if (start % ValidityMask::BITS_PER_VALUE == 0) {
340: 		auto &scan_state = (ValidityScanState &)*state.scan_state;
341: 
342: 		// aligned scan: no need to do anything fancy
343: 		// note: this is only an optimization which avoids having to do messy bitshifting in the common case
344: 		// it is not required for correctness
345: 		auto &result_mask = FlatVector::Validity(result);
346: 		auto buffer_ptr = scan_state.handle->node->buffer + segment.GetBlockOffset();
347: 		auto input_data = (validity_t *)buffer_ptr;
348: 		auto result_data = (validity_t *)result_mask.GetData();
349: 		idx_t start_offset = start / ValidityMask::BITS_PER_VALUE;
350: 		idx_t entry_scan_count = (scan_count + ValidityMask::BITS_PER_VALUE - 1) / ValidityMask::BITS_PER_VALUE;
351: 		for (idx_t i = 0; i < entry_scan_count; i++) {
352: 			auto input_entry = input_data[start_offset + i];
353: 			if (!result_data && input_entry == ValidityMask::ValidityBuffer::MAX_ENTRY) {
354: 				continue;
355: 			}
356: 			if (!result_data) {
357: 				result_mask.Initialize(MaxValue<idx_t>(STANDARD_VECTOR_SIZE, scan_count));
358: 				result_data = (validity_t *)result_mask.GetData();
359: 			}
360: 			result_data[i] = input_entry;
361: 		}
362: 	} else {
363: 		// unaligned scan: fall back to scan_partial which does bitshift tricks
364: 		ValidityScanPartial(segment, state, scan_count, result, 0);
365: 	}
366: }
367: 
368: //===--------------------------------------------------------------------===//
369: // Fetch
370: //===--------------------------------------------------------------------===//
371: void ValidityFetchRow(ColumnSegment &segment, ColumnFetchState &state, row_t row_id, Vector &result, idx_t result_idx) {
372: 	D_ASSERT(row_id >= 0 && row_id < row_t(segment.count));
373: 	auto &buffer_manager = BufferManager::GetBufferManager(segment.db);
374: 	auto handle = buffer_manager.Pin(segment.block);
375: 	auto dataptr = handle->node->buffer + segment.GetBlockOffset();
376: 	ValidityMask mask((validity_t *)dataptr);
377: 	auto &result_mask = FlatVector::Validity(result);
378: 	if (!mask.RowIsValidUnsafe(row_id)) {
379: 		result_mask.SetInvalid(result_idx);
380: 	}
381: }
382: 
383: //===--------------------------------------------------------------------===//
384: // Append
385: //===--------------------------------------------------------------------===//
386: unique_ptr<CompressedSegmentState> ValidityInitSegment(ColumnSegment &segment, block_id_t block_id) {
387: 	auto &buffer_manager = BufferManager::GetBufferManager(segment.db);
388: 	if (block_id == INVALID_BLOCK) {
389: 		auto handle = buffer_manager.Pin(segment.block);
390: 		memset(handle->node->buffer, 0xFF, Storage::BLOCK_SIZE);
391: 	}
392: 	return nullptr;
393: }
394: 
395: idx_t ValidityAppend(ColumnSegment &segment, SegmentStatistics &stats, VectorData &data, idx_t offset, idx_t vcount) {
396: 	D_ASSERT(segment.GetBlockOffset() == 0);
397: 	auto &validity_stats = (ValidityStatistics &)*stats.statistics;
398: 
399: 	auto max_tuples = Storage::BLOCK_SIZE / ValidityMask::STANDARD_MASK_SIZE * STANDARD_VECTOR_SIZE;
400: 	idx_t append_count = MinValue<idx_t>(vcount, max_tuples - segment.count);
401: 	if (data.validity.AllValid()) {
402: 		// no null values: skip append
403: 		segment.count += append_count;
404: 		validity_stats.has_no_null = true;
405: 		return append_count;
406: 	}
407: 	auto &buffer_manager = BufferManager::GetBufferManager(segment.db);
408: 	auto handle = buffer_manager.Pin(segment.block);
409: 
410: 	ValidityMask mask((validity_t *)handle->node->buffer);
411: 	for (idx_t i = 0; i < append_count; i++) {
412: 		auto idx = data.sel->get_index(offset + i);
413: 		if (!data.validity.RowIsValidUnsafe(idx)) {
414: 			mask.SetInvalidUnsafe(segment.count + i);
415: 			validity_stats.has_null = true;
416: 		} else {
417: 			validity_stats.has_no_null = true;
418: 		}
419: 	}
420: 	segment.count += append_count;
421: 	return append_count;
422: }
423: 
424: idx_t ValidityFinalizeAppend(ColumnSegment &segment, SegmentStatistics &stats) {
425: 	return ((segment.count + STANDARD_VECTOR_SIZE - 1) / STANDARD_VECTOR_SIZE) * ValidityMask::STANDARD_MASK_SIZE;
426: }
427: 
428: void ValidityRevertAppend(ColumnSegment &segment, idx_t start_row) {
429: 	idx_t start_bit = start_row - segment.start;
430: 
431: 	auto &buffer_manager = BufferManager::GetBufferManager(segment.db);
432: 	auto handle = buffer_manager.Pin(segment.block);
433: 	idx_t revert_start;
434: 	if (start_bit % 8 != 0) {
435: 		// handle sub-bit stuff (yay)
436: 		idx_t byte_pos = start_bit / 8;
437: 		idx_t bit_start = byte_pos * 8;
438: 		idx_t bit_end = (byte_pos + 1) * 8;
439: 		ValidityMask mask((validity_t *)handle->node->buffer + byte_pos);
440: 		for (idx_t i = start_bit; i < bit_end; i++) {
441: 			mask.SetValid(i - bit_start);
442: 		}
443: 		revert_start = bit_end / 8;
444: 	} else {
445: 		revert_start = start_bit / 8;
446: 	}
447: 	// for the rest, we just memset
448: 	memset(handle->node->buffer + revert_start, 0xFF, Storage::BLOCK_SIZE - revert_start);
449: }
450: 
451: //===--------------------------------------------------------------------===//
452: // Get Function
453: //===--------------------------------------------------------------------===//
454: CompressionFunction ValidityUncompressed::GetFunction(PhysicalType data_type) {
455: 	D_ASSERT(data_type == PhysicalType::BIT);
456: 	return CompressionFunction(CompressionType::COMPRESSION_UNCOMPRESSED, data_type, ValidityInitAnalyze,
457: 	                           ValidityAnalyze, ValidityFinalAnalyze, UncompressedFunctions::InitCompression,
458: 	                           UncompressedFunctions::Compress, UncompressedFunctions::FinalizeCompress,
459: 	                           ValidityInitScan, ValidityScan, ValidityScanPartial, ValidityFetchRow,
460: 	                           UncompressedFunctions::EmptySkip, ValidityInitSegment, ValidityAppend,
461: 	                           ValidityFinalizeAppend, ValidityRevertAppend);
462: }
463: 
464: } // namespace duckdb
[end of src/storage/compression/validity_uncompressed.cpp]
[start of src/storage/local_storage.cpp]
1: #include "duckdb/transaction/local_storage.hpp"
2: #include "duckdb/execution/index/art/art.hpp"
3: #include "duckdb/storage/table/append_state.hpp"
4: #include "duckdb/storage/write_ahead_log.hpp"
5: #include "duckdb/common/vector_operations/vector_operations.hpp"
6: #include "duckdb/storage/table/row_group.hpp"
7: #include "duckdb/transaction/transaction.hpp"
8: #include "duckdb/planner/table_filter.hpp"
9: 
10: #include "duckdb/storage/table/column_segment.hpp"
11: 
12: namespace duckdb {
13: 
14: LocalTableStorage::LocalTableStorage(DataTable &table) : table(table), active_scans(0) {
15: 	Clear();
16: }
17: 
18: LocalTableStorage::~LocalTableStorage() {
19: }
20: 
21: void LocalTableStorage::InitializeScan(LocalScanState &state, TableFilterSet *table_filters) {
22: 	if (collection.ChunkCount() == 0) {
23: 		// nothing to scan
24: 		return;
25: 	}
26: 	state.SetStorage(this);
27: 
28: 	state.chunk_index = 0;
29: 	state.max_index = collection.ChunkCount() - 1;
30: 	state.last_chunk_count = collection.Chunks().back()->size();
31: 	state.table_filters = table_filters;
32: }
33: 
34: idx_t LocalTableStorage::EstimatedSize() {
35: 	idx_t appended_rows = collection.Count() - deleted_rows;
36: 	if (appended_rows == 0) {
37: 		return 0;
38: 	}
39: 	idx_t row_size = 0;
40: 	for (auto &type : collection.Types()) {
41: 		row_size += GetTypeIdSize(type.InternalType());
42: 	}
43: 	return appended_rows * row_size;
44: }
45: 
46: LocalScanState::~LocalScanState() {
47: 	SetStorage(nullptr);
48: }
49: 
50: void LocalScanState::SetStorage(LocalTableStorage *new_storage) {
51: 	if (storage != nullptr) {
52: 		D_ASSERT(storage->active_scans > 0);
53: 		storage->active_scans--;
54: 	}
55: 	storage = new_storage;
56: 	if (storage) {
57: 		storage->active_scans++;
58: 	}
59: }
60: 
61: void LocalTableStorage::Clear() {
62: 	collection.Reset();
63: 	deleted_entries.clear();
64: 	indexes.clear();
65: 	deleted_rows = 0;
66: 	table.info->indexes.Scan([&](Index &index) {
67: 		D_ASSERT(index.type == IndexType::ART);
68: 		auto &art = (ART &)index;
69: 		if (art.constraint_type != IndexConstraintType::NONE) {
70: 			// unique index: create a local ART index that maintains the same unique constraint
71: 			vector<unique_ptr<Expression>> unbound_expressions;
72: 			for (auto &expr : art.unbound_expressions) {
73: 				unbound_expressions.push_back(expr->Copy());
74: 			}
75: 			indexes.push_back(make_unique<ART>(art.column_ids, move(unbound_expressions), art.constraint_type));
76: 		}
77: 		return false;
78: 	});
79: }
80: 
81: void LocalStorage::InitializeScan(DataTable *table, LocalScanState &state, TableFilterSet *table_filters) {
82: 	auto entry = table_storage.find(table);
83: 	if (entry == table_storage.end()) {
84: 		// no local storage for table: set scan to nullptr
85: 		state.SetStorage(nullptr);
86: 		return;
87: 	}
88: 	auto storage = entry->second.get();
89: 	storage->InitializeScan(state, table_filters);
90: }
91: 
92: void LocalStorage::Scan(LocalScanState &state, const vector<column_t> &column_ids, DataChunk &result) {
93: 	auto storage = state.GetStorage();
94: 	if (!storage || state.chunk_index > state.max_index) {
95: 		// nothing left to scan
96: 		result.Reset();
97: 		return;
98: 	}
99: 	auto &chunk = storage->collection.GetChunk(state.chunk_index);
100: 	idx_t chunk_count = state.chunk_index == state.max_index ? state.last_chunk_count : chunk.size();
101: 	idx_t count = chunk_count;
102: 
103: 	// first create a selection vector from the deleted entries (if any)
104: 	SelectionVector valid_sel(STANDARD_VECTOR_SIZE);
105: 	auto entry = storage->deleted_entries.find(state.chunk_index);
106: 	if (entry != storage->deleted_entries.end()) {
107: 		// deleted entries! create a selection vector
108: 		auto deleted = entry->second.get();
109: 		idx_t new_count = 0;
110: 		for (idx_t i = 0; i < count; i++) {
111: 			if (!deleted[i]) {
112: 				valid_sel.set_index(new_count++, i);
113: 			}
114: 		}
115: 		if (new_count == 0 && count > 0) {
116: 			// all entries in this chunk were deleted: continue to next chunk
117: 			state.chunk_index++;
118: 			Scan(state, column_ids, result);
119: 			return;
120: 		}
121: 		count = new_count;
122: 	}
123: 
124: 	SelectionVector sel;
125: 	if (count != chunk_count) {
126: 		sel.Initialize(valid_sel);
127: 	} else {
128: 		sel.Initialize(nullptr);
129: 	}
130: 	// now scan the vectors of the chunk
131: 	for (idx_t i = 0; i < column_ids.size(); i++) {
132: 		auto id = column_ids[i];
133: 		if (id == COLUMN_IDENTIFIER_ROW_ID) {
134: 			// row identifier: return a sequence of rowids starting from MAX_ROW_ID plus the row offset in the chunk
135: 			result.data[i].Sequence(MAX_ROW_ID + state.chunk_index * STANDARD_VECTOR_SIZE, 1);
136: 		} else {
137: 			result.data[i].Reference(chunk.data[id]);
138: 		}
139: 		idx_t approved_tuple_count = count;
140: 		if (state.table_filters) {
141: 			auto column_filters = state.table_filters->filters.find(i);
142: 			if (column_filters != state.table_filters->filters.end()) {
143: 				//! We have filters to apply here
144: 				auto &mask = FlatVector::Validity(result.data[i]);
145: 				ColumnSegment::FilterSelection(sel, result.data[i], *column_filters->second, approved_tuple_count,
146: 				                               mask);
147: 				count = approved_tuple_count;
148: 			}
149: 		}
150: 	}
151: 	if (count == 0) {
152: 		// all entries in this chunk were filtered:: Continue on next chunk
153: 		state.chunk_index++;
154: 		Scan(state, column_ids, result);
155: 		return;
156: 	}
157: 	if (count == chunk_count) {
158: 		result.SetCardinality(count);
159: 	} else {
160: 		result.Slice(sel, count);
161: 	}
162: 	state.chunk_index++;
163: }
164: 
165: void LocalStorage::Append(DataTable *table, DataChunk &chunk) {
166: 	auto entry = table_storage.find(table);
167: 	LocalTableStorage *storage;
168: 	if (entry == table_storage.end()) {
169: 		auto new_storage = make_unique<LocalTableStorage>(*table);
170: 		storage = new_storage.get();
171: 		table_storage.insert(make_pair(table, move(new_storage)));
172: 	} else {
173: 		storage = entry->second.get();
174: 	}
175: 	// append to unique indices (if any)
176: 	if (!storage->indexes.empty()) {
177: 		idx_t base_id = MAX_ROW_ID + storage->collection.Count();
178: 
179: 		// first generate the vector of row identifiers
180: 		Vector row_ids(LogicalType::ROW_TYPE);
181: 		VectorOperations::GenerateSequence(row_ids, chunk.size(), base_id, 1);
182: 
183: 		// now append the entries to the indices
184: 		for (auto &index : storage->indexes) {
185: 			if (!index->Append(chunk, row_ids)) {
186: 				throw ConstraintException("PRIMARY KEY or UNIQUE constraint violated: duplicated key");
187: 			}
188: 		}
189: 	}
190: 	//! Append to the chunk
191: 	storage->collection.Append(chunk);
192: 	if (storage->active_scans == 0 && storage->collection.Count() >= RowGroup::ROW_GROUP_SIZE * 2) {
193: 		// flush to base storage
194: 		Flush(*table, *storage);
195: 	}
196: }
197: 
198: LocalTableStorage *LocalStorage::GetStorage(DataTable *table) {
199: 	auto entry = table_storage.find(table);
200: 	D_ASSERT(entry != table_storage.end());
201: 	return entry->second.get();
202: }
203: 
204: idx_t LocalStorage::EstimatedSize() {
205: 	idx_t estimated_size = 0;
206: 	for (auto &storage : table_storage) {
207: 		estimated_size += storage.second->EstimatedSize();
208: 	}
209: 	return estimated_size;
210: }
211: 
212: static idx_t GetChunk(Vector &row_ids) {
213: 	auto ids = FlatVector::GetData<row_t>(row_ids);
214: 	auto first_id = ids[0] - MAX_ROW_ID;
215: 
216: 	return first_id / STANDARD_VECTOR_SIZE;
217: }
218: 
219: idx_t LocalStorage::Delete(DataTable *table, Vector &row_ids, idx_t count) {
220: 	auto storage = GetStorage(table);
221: 	// figure out the chunk from which these row ids came
222: 	idx_t chunk_idx = GetChunk(row_ids);
223: 	D_ASSERT(chunk_idx < storage->collection.ChunkCount());
224: 
225: 	// delete from unique indices (if any)
226: 	if (!storage->indexes.empty()) {
227: 		// Index::Delete assumes that ALL rows are being deleted, so
228: 		// Slice out the rows that are being deleted from the storage Chunk
229: 		auto &chunk = storage->collection.GetChunk(chunk_idx);
230: 
231: 		VectorData row_ids_data;
232: 		row_ids.Orrify(count, row_ids_data);
233: 		auto row_identifiers = (const row_t *)row_ids_data.data;
234: 		SelectionVector sel(count);
235: 		for (idx_t i = 0; i < count; ++i) {
236: 			const auto idx = row_ids_data.sel->get_index(i);
237: 			sel.set_index(i, row_identifiers[idx] - MAX_ROW_ID);
238: 		}
239: 
240: 		DataChunk deleted;
241: 		deleted.InitializeEmpty(chunk.GetTypes());
242: 		deleted.Slice(chunk, sel, count);
243: 		for (auto &index : storage->indexes) {
244: 			index->Delete(deleted, row_ids);
245: 		}
246: 	}
247: 
248: 	// get a pointer to the deleted entries for this chunk
249: 	bool *deleted;
250: 	auto entry = storage->deleted_entries.find(chunk_idx);
251: 	if (entry == storage->deleted_entries.end()) {
252: 		// nothing deleted yet, add the deleted entries
253: 		auto del_entries = unique_ptr<bool[]>(new bool[STANDARD_VECTOR_SIZE]);
254: 		memset(del_entries.get(), 0, sizeof(bool) * STANDARD_VECTOR_SIZE);
255: 		deleted = del_entries.get();
256: 		storage->deleted_entries.insert(make_pair(chunk_idx, move(del_entries)));
257: 	} else {
258: 		deleted = entry->second.get();
259: 	}
260: 
261: 	// now actually mark the entries as deleted in the deleted vector
262: 	idx_t base_index = MAX_ROW_ID + chunk_idx * STANDARD_VECTOR_SIZE;
263: 
264: 	idx_t deleted_count = 0;
265: 	auto ids = FlatVector::GetData<row_t>(row_ids);
266: 	for (idx_t i = 0; i < count; i++) {
267: 		auto id = ids[i] - base_index;
268: 		if (!deleted[id]) {
269: 			deleted_count++;
270: 		}
271: 		deleted[id] = true;
272: 	}
273: 	storage->deleted_rows += deleted_count;
274: 	return deleted_count;
275: }
276: 
277: template <class T>
278: static void TemplatedUpdateLoop(Vector &data_vector, Vector &update_vector, Vector &row_ids, idx_t count,
279:                                 idx_t base_index) {
280: 	VectorData udata;
281: 	update_vector.Orrify(count, udata);
282: 
283: 	auto target = FlatVector::GetData<T>(data_vector);
284: 	auto &mask = FlatVector::Validity(data_vector);
285: 	auto ids = FlatVector::GetData<row_t>(row_ids);
286: 	auto updates = (T *)udata.data;
287: 
288: 	for (idx_t i = 0; i < count; i++) {
289: 		auto uidx = udata.sel->get_index(i);
290: 
291: 		auto id = ids[i] - base_index;
292: 		target[id] = updates[uidx];
293: 		mask.Set(id, udata.validity.RowIsValid(uidx));
294: 	}
295: }
296: 
297: static void UpdateChunk(Vector &data, Vector &updates, Vector &row_ids, idx_t count, idx_t base_index) {
298: 	D_ASSERT(data.GetType() == updates.GetType());
299: 	D_ASSERT(row_ids.GetType() == LogicalType::ROW_TYPE);
300: 
301: 	switch (data.GetType().InternalType()) {
302: 	case PhysicalType::INT8:
303: 		TemplatedUpdateLoop<int8_t>(data, updates, row_ids, count, base_index);
304: 		break;
305: 	case PhysicalType::INT16:
306: 		TemplatedUpdateLoop<int16_t>(data, updates, row_ids, count, base_index);
307: 		break;
308: 	case PhysicalType::INT32:
309: 		TemplatedUpdateLoop<int32_t>(data, updates, row_ids, count, base_index);
310: 		break;
311: 	case PhysicalType::INT64:
312: 		TemplatedUpdateLoop<int64_t>(data, updates, row_ids, count, base_index);
313: 		break;
314: 	case PhysicalType::FLOAT:
315: 		TemplatedUpdateLoop<float>(data, updates, row_ids, count, base_index);
316: 		break;
317: 	case PhysicalType::DOUBLE:
318: 		TemplatedUpdateLoop<double>(data, updates, row_ids, count, base_index);
319: 		break;
320: 	case PhysicalType::VARCHAR:
321: 		TemplatedUpdateLoop<string_t>(data, updates, row_ids, count, base_index);
322: 		break;
323: 	default:
324: 		throw Exception("Unsupported type for in-place update: " + TypeIdToString(data.GetType().InternalType()));
325: 	}
326: }
327: 
328: void LocalStorage::Update(DataTable *table, Vector &row_ids, const vector<column_t> &column_ids, DataChunk &data) {
329: 	auto storage = GetStorage(table);
330: 	// figure out the chunk from which these row ids came
331: 	idx_t chunk_idx = GetChunk(row_ids);
332: 	D_ASSERT(chunk_idx < storage->collection.ChunkCount());
333: 
334: 	idx_t base_index = MAX_ROW_ID + chunk_idx * STANDARD_VECTOR_SIZE;
335: 
336: 	// now perform the actual update
337: 	auto &chunk = storage->collection.GetChunk(chunk_idx);
338: 	for (idx_t i = 0; i < column_ids.size(); i++) {
339: 		auto col_idx = column_ids[i];
340: 		UpdateChunk(chunk.data[col_idx], data.data[i], row_ids, data.size(), base_index);
341: 	}
342: }
343: 
344: template <class T>
345: bool LocalStorage::ScanTableStorage(DataTable &table, LocalTableStorage &storage, T &&fun) {
346: 	vector<column_t> column_ids;
347: 	for (idx_t i = 0; i < table.column_definitions.size(); i++) {
348: 		column_ids.push_back(i);
349: 	}
350: 
351: 	DataChunk chunk;
352: 	chunk.Initialize(table.GetTypes());
353: 
354: 	// initialize the scan
355: 	LocalScanState state;
356: 	storage.InitializeScan(state);
357: 
358: 	while (true) {
359: 		Scan(state, column_ids, chunk);
360: 		if (chunk.size() == 0) {
361: 			return true;
362: 		}
363: 		if (!fun(chunk)) {
364: 			return false;
365: 		}
366: 	}
367: }
368: 
369: void LocalStorage::Flush(DataTable &table, LocalTableStorage &storage) {
370: 	if (storage.collection.Count() <= storage.deleted_rows) {
371: 		return;
372: 	}
373: 	idx_t append_count = storage.collection.Count() - storage.deleted_rows;
374: 	TableAppendState append_state;
375: 	table.InitializeAppend(transaction, append_state, append_count);
376: 
377: 	bool constraint_violated = false;
378: 	ScanTableStorage(table, storage, [&](DataChunk &chunk) -> bool {
379: 		// append this chunk to the indexes of the table
380: 		if (!table.AppendToIndexes(append_state, chunk, append_state.current_row)) {
381: 			constraint_violated = true;
382: 			return false;
383: 		}
384: 		// append to base table
385: 		table.Append(transaction, chunk, append_state);
386: 		return true;
387: 	});
388: 	if (constraint_violated) {
389: 		// need to revert the append
390: 		row_t current_row = append_state.row_start;
391: 		// remove the data from the indexes, if there are any indexes
392: 		ScanTableStorage(table, storage, [&](DataChunk &chunk) -> bool {
393: 			// append this chunk to the indexes of the table
394: 			table.RemoveFromIndexes(append_state, chunk, current_row);
395: 
396: 			current_row += chunk.size();
397: 			if (current_row >= append_state.current_row) {
398: 				// finished deleting all rows from the index: abort now
399: 				return false;
400: 			}
401: 			return true;
402: 		});
403: 		table.RevertAppendInternal(append_state.row_start, append_count);
404: 		storage.Clear();
405: 		throw ConstraintException("PRIMARY KEY or UNIQUE constraint violated: duplicated key");
406: 	}
407: 	storage.Clear();
408: 	transaction.PushAppend(&table, append_state.row_start, append_count);
409: }
410: 
411: void LocalStorage::Commit(LocalStorage::CommitState &commit_state, Transaction &transaction, WriteAheadLog *log,
412:                           transaction_t commit_id) {
413: 	// commit local storage, iterate over all entries in the table storage map
414: 	for (auto &entry : table_storage) {
415: 		auto table = entry.first;
416: 		auto storage = entry.second.get();
417: 		Flush(*table, *storage);
418: 	}
419: 	// finished commit: clear local storage
420: 	table_storage.clear();
421: }
422: 
423: void LocalStorage::AddColumn(DataTable *old_dt, DataTable *new_dt, ColumnDefinition &new_column,
424:                              Expression *default_value) {
425: 	// check if there are any pending appends for the old version of the table
426: 	auto entry = table_storage.find(old_dt);
427: 	if (entry == table_storage.end()) {
428: 		return;
429: 	}
430: 	// take over the storage from the old entry
431: 	auto new_storage = move(entry->second);
432: 
433: 	// now add the new column filled with the default value to all chunks
434: 	auto new_column_type = new_column.type;
435: 	ExpressionExecutor executor;
436: 	DataChunk dummy_chunk;
437: 	if (default_value) {
438: 		executor.AddExpression(*default_value);
439: 	}
440: 
441: 	new_storage->collection.Types().push_back(new_column_type);
442: 	for (idx_t chunk_idx = 0; chunk_idx < new_storage->collection.ChunkCount(); chunk_idx++) {
443: 		auto &chunk = new_storage->collection.GetChunk(chunk_idx);
444: 		Vector result(new_column_type);
445: 		if (default_value) {
446: 			dummy_chunk.SetCardinality(chunk.size());
447: 			executor.ExecuteExpression(dummy_chunk, result);
448: 		} else {
449: 			FlatVector::Validity(result).SetAllInvalid(chunk.size());
450: 		}
451: 		result.Normalify(chunk.size());
452: 		chunk.data.push_back(move(result));
453: 	}
454: 
455: 	table_storage.erase(entry);
456: 	table_storage[new_dt] = move(new_storage);
457: }
458: 
459: void LocalStorage::ChangeType(DataTable *old_dt, DataTable *new_dt, idx_t changed_idx, const LogicalType &target_type,
460:                               const vector<column_t> &bound_columns, Expression &cast_expr) {
461: 	// check if there are any pending appends for the old version of the table
462: 	auto entry = table_storage.find(old_dt);
463: 	if (entry == table_storage.end()) {
464: 		return;
465: 	}
466: 	throw NotImplementedException("FIXME: ALTER TYPE with transaction local data not currently supported");
467: }
468: 
469: void LocalStorage::FetchChunk(DataTable *table, Vector &row_ids, idx_t count, DataChunk &dst_chunk) {
470: 	auto storage = GetStorage(table);
471: 	idx_t chunk_idx = GetChunk(row_ids);
472: 	auto &chunk = storage->collection.GetChunk(chunk_idx);
473: 
474: 	VectorData row_ids_data;
475: 	row_ids.Orrify(count, row_ids_data);
476: 	auto row_identifiers = (const row_t *)row_ids_data.data;
477: 	SelectionVector sel(count);
478: 	for (idx_t i = 0; i < count; ++i) {
479: 		const auto idx = row_ids_data.sel->get_index(i);
480: 		sel.set_index(i, row_identifiers[idx] - MAX_ROW_ID);
481: 	}
482: 
483: 	dst_chunk.InitializeEmpty(chunk.GetTypes());
484: 	dst_chunk.Slice(chunk, sel, count);
485: }
486: 
487: vector<unique_ptr<Index>> &LocalStorage::GetIndexes(DataTable *table) {
488: 	auto storage = GetStorage(table);
489: 
490: 	return storage->indexes;
491: }
492: 
493: } // namespace duckdb
[end of src/storage/local_storage.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: