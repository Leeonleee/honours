You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
INTERNAL Error: Skip not implemented for StructColumnReader when filtering for uncommon events in a large dataset
#### What happens?
When using an equals opertator for sparse elements in a sizable dataset that contains a map I notice an error:
```
RuntimeError: INTERNAL Error: Skip not implemented for StructColumnReader
```
Also worth noting, when the query is changed to instead use an `IN` clause (as opposed to equals) I no longer see the error. 

#### To Reproduce
This snippet doesn't seem to error if the data set is smaller ie, if you use a range of 1,000 instead of 10,000.

```
import pyarrow as pa
import pyarrow.parquet as pq
import duckdb 
maptype = pa.map_(pa.string(), pa.string())

elems = []
filter_vals = []
for i in range(10000):
    key_name = str(chr(i % 26 + 65))
    filter_vals.append(str(i))
    if i % 10 == 0:
        value = None
    else:
        value = 'foo'
    elems.append([(key_name,value)])

t = pa.Table.from_pydict({'my_map':elems, 'filter': filter_vals}, pa.schema([pa.field('my_map',maptype), pa.field('filter', pa.string())])) 
pq.write_table(t, 'test')
cursor = duckdb.connect()
cursor.execute("SELECT my_map['A'], * FROM parquet_scan('test') where filter == '0'").fetchall()
```

#### Environment (please complete the following information):
 - OS: iOS (and Ubuntu)
 - DuckDB Version: .3.2 (and latest master off of .3.3)
 - DuckDB Client: python

#### Before Submitting

- [X] **Have you tried this on the latest `master` branch?**
* **Python**: `pip install duckdb --upgrade --pre`
* **R**: `install.packages("https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz", repos = NULL)`
* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.

- [X] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**

INTERNAL Error: Skip not implemented for StructColumnReader when filtering for uncommon events in a large dataset
#### What happens?
When using an equals opertator for sparse elements in a sizable dataset that contains a map I notice an error:
```
RuntimeError: INTERNAL Error: Skip not implemented for StructColumnReader
```
Also worth noting, when the query is changed to instead use an `IN` clause (as opposed to equals) I no longer see the error. 

#### To Reproduce
This snippet doesn't seem to error if the data set is smaller ie, if you use a range of 1,000 instead of 10,000.

```
import pyarrow as pa
import pyarrow.parquet as pq
import duckdb 
maptype = pa.map_(pa.string(), pa.string())

elems = []
filter_vals = []
for i in range(10000):
    key_name = str(chr(i % 26 + 65))
    filter_vals.append(str(i))
    if i % 10 == 0:
        value = None
    else:
        value = 'foo'
    elems.append([(key_name,value)])

t = pa.Table.from_pydict({'my_map':elems, 'filter': filter_vals}, pa.schema([pa.field('my_map',maptype), pa.field('filter', pa.string())])) 
pq.write_table(t, 'test')
cursor = duckdb.connect()
cursor.execute("SELECT my_map['A'], * FROM parquet_scan('test') where filter == '0'").fetchall()
```

#### Environment (please complete the following information):
 - OS: iOS (and Ubuntu)
 - DuckDB Version: .3.2 (and latest master off of .3.3)
 - DuckDB Client: python

#### Before Submitting

- [X] **Have you tried this on the latest `master` branch?**
* **Python**: `pip install duckdb --upgrade --pre`
* **R**: `install.packages("https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz", repos = NULL)`
* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.

- [X] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**


</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
44: 
45: 
[end of README.md]
[start of extension/parquet/column_reader.cpp]
1: #include "column_reader.hpp"
2: #include "parquet_timestamp.hpp"
3: #include "utf8proc_wrapper.hpp"
4: #include "parquet_reader.hpp"
5: 
6: #include "boolean_column_reader.hpp"
7: #include "callback_column_reader.hpp"
8: #include "parquet_decimal_utils.hpp"
9: #include "list_column_reader.hpp"
10: #include "string_column_reader.hpp"
11: #include "struct_column_reader.hpp"
12: #include "templated_column_reader.hpp"
13: 
14: #include "snappy.h"
15: #include "miniz_wrapper.hpp"
16: #include "zstd.h"
17: #include <iostream>
18: 
19: #include "duckdb.hpp"
20: #ifndef DUCKDB_AMALGAMATION
21: #include "duckdb/common/types/blob.hpp"
22: #include "duckdb/common/types/chunk_collection.hpp"
23: #endif
24: 
25: namespace duckdb {
26: 
27: using duckdb_parquet::format::CompressionCodec;
28: using duckdb_parquet::format::ConvertedType;
29: using duckdb_parquet::format::Encoding;
30: using duckdb_parquet::format::PageType;
31: using duckdb_parquet::format::Type;
32: 
33: const uint32_t ParquetDecodeUtils::BITPACK_MASKS[] = {
34:     0,       1,       3,        7,        15,       31,        63,        127,       255,        511,       1023,
35:     2047,    4095,    8191,     16383,    32767,    65535,     131071,    262143,    524287,     1048575,   2097151,
36:     4194303, 8388607, 16777215, 33554431, 67108863, 134217727, 268435455, 536870911, 1073741823, 2147483647};
37: 
38: const uint8_t ParquetDecodeUtils::BITPACK_DLEN = 8;
39: 
40: ColumnReader::ColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
41:                            idx_t max_define_p, idx_t max_repeat_p)
42:     : schema(schema_p), file_idx(file_idx_p), max_define(max_define_p), max_repeat(max_repeat_p), reader(reader),
43:       type(move(type_p)), page_rows_available(0) {
44: 
45: 	// dummies for Skip()
46: 	none_filter.none();
47: 	dummy_define.resize(reader.allocator, STANDARD_VECTOR_SIZE);
48: 	dummy_repeat.resize(reader.allocator, STANDARD_VECTOR_SIZE);
49: }
50: 
51: ColumnReader::~ColumnReader() {
52: }
53: 
54: const LogicalType &ColumnReader::Type() {
55: 	return type;
56: }
57: 
58: const SchemaElement &ColumnReader::Schema() {
59: 	return schema;
60: }
61: 
62: idx_t ColumnReader::GroupRowsAvailable() {
63: 	return group_rows_available;
64: }
65: 
66: unique_ptr<BaseStatistics> ColumnReader::Stats(const std::vector<ColumnChunk> &columns) {
67: 	if (Type().id() == LogicalTypeId::LIST || Type().id() == LogicalTypeId::STRUCT ||
68: 	    Type().id() == LogicalTypeId::MAP) {
69: 		return nullptr;
70: 	}
71: 	return ParquetStatisticsUtils::TransformColumnStatistics(Schema(), Type(), columns[file_idx]);
72: }
73: 
74: void ColumnReader::Plain(shared_ptr<ByteBuffer> plain_data, uint8_t *defines, idx_t num_values, // NOLINT
75:                          parquet_filter_t &filter, idx_t result_offset, Vector &result) {
76: 	throw NotImplementedException("Plain");
77: }
78: 
79: void ColumnReader::Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) { // NOLINT
80: 	throw NotImplementedException("Dictionary");
81: }
82: 
83: void ColumnReader::Offsets(uint32_t *offsets, uint8_t *defines, idx_t num_values, parquet_filter_t &filter,
84:                            idx_t result_offset, Vector &result) {
85: 	throw NotImplementedException("Offsets");
86: }
87: 
88: void ColumnReader::DictReference(Vector &result) {
89: }
90: void ColumnReader::PlainReference(shared_ptr<ByteBuffer>, Vector &result) { // NOLINT
91: }
92: 
93: void ColumnReader::InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) {
94: 	D_ASSERT(file_idx < columns.size());
95: 	chunk = &columns[file_idx];
96: 	protocol = &protocol_p;
97: 	D_ASSERT(chunk);
98: 	D_ASSERT(chunk->__isset.meta_data);
99: 
100: 	if (chunk->__isset.file_path) {
101: 		throw std::runtime_error("Only inlined data files are supported (no references)");
102: 	}
103: 
104: 	// ugh. sometimes there is an extra offset for the dict. sometimes it's wrong.
105: 	chunk_read_offset = chunk->meta_data.data_page_offset;
106: 	if (chunk->meta_data.__isset.dictionary_page_offset && chunk->meta_data.dictionary_page_offset >= 4) {
107: 		// this assumes the data pages follow the dict pages directly.
108: 		chunk_read_offset = chunk->meta_data.dictionary_page_offset;
109: 	}
110: 	group_rows_available = chunk->meta_data.num_values;
111: }
112: 
113: void ColumnReader::PrepareRead(parquet_filter_t &filter) {
114: 	dict_decoder.reset();
115: 	defined_decoder.reset();
116: 	block.reset();
117: 
118: 	PageHeader page_hdr;
119: 	page_hdr.read(protocol);
120: 
121: 	//		page_hdr.printTo(std::cout);
122: 	//		std::cout << '\n';
123: 
124: 	switch (page_hdr.type) {
125: 	case PageType::DATA_PAGE_V2:
126: 		PreparePageV2(page_hdr);
127: 		PrepareDataPage(page_hdr);
128: 		break;
129: 	case PageType::DATA_PAGE:
130: 		PreparePage(page_hdr.compressed_page_size, page_hdr.uncompressed_page_size);
131: 		PrepareDataPage(page_hdr);
132: 		break;
133: 	case PageType::DICTIONARY_PAGE:
134: 		PreparePage(page_hdr.compressed_page_size, page_hdr.uncompressed_page_size);
135: 		Dictionary(move(block), page_hdr.dictionary_page_header.num_values);
136: 		break;
137: 	default:
138: 		break; // ignore INDEX page type and any other custom extensions
139: 	}
140: }
141: 
142: void ColumnReader::PreparePageV2(PageHeader &page_hdr) {
143: 	// FIXME this is copied from the other prepare, merge the decomp part
144: 
145: 	D_ASSERT(page_hdr.type == PageType::DATA_PAGE_V2);
146: 
147: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
148: 
149: 	block = make_shared<ResizeableBuffer>(reader.allocator, page_hdr.uncompressed_page_size + 1);
150: 	// copy repeats & defines as-is because FOR SOME REASON they are uncompressed
151: 	auto uncompressed_bytes = page_hdr.data_page_header_v2.repetition_levels_byte_length +
152: 	                          page_hdr.data_page_header_v2.definition_levels_byte_length;
153: 	auto possibly_compressed_bytes = page_hdr.compressed_page_size - uncompressed_bytes;
154: 	trans.read((uint8_t *)block->ptr, uncompressed_bytes);
155: 
156: 	switch (chunk->meta_data.codec) {
157: 	case CompressionCodec::UNCOMPRESSED:
158: 		trans.read(((uint8_t *)block->ptr) + uncompressed_bytes, possibly_compressed_bytes);
159: 		break;
160: 
161: 	case CompressionCodec::SNAPPY: {
162: 		// TODO move allocation outta here
163: 		ResizeableBuffer compressed_bytes_buffer(reader.allocator, possibly_compressed_bytes);
164: 		trans.read((uint8_t *)compressed_bytes_buffer.ptr, possibly_compressed_bytes);
165: 
166: 		auto res = duckdb_snappy::RawUncompress((const char *)compressed_bytes_buffer.ptr, possibly_compressed_bytes,
167: 		                                        ((char *)block->ptr) + uncompressed_bytes);
168: 		if (!res) {
169: 			throw std::runtime_error("Decompression failure");
170: 		}
171: 		break;
172: 	}
173: 
174: 	default: {
175: 		std::stringstream codec_name;
176: 		codec_name << chunk->meta_data.codec;
177: 		throw std::runtime_error("Unsupported compression codec \"" + codec_name.str() +
178: 		                         "\". Supported options are uncompressed, gzip or snappy");
179: 		break;
180: 	}
181: 	}
182: }
183: 
184: void ColumnReader::PreparePage(idx_t compressed_page_size, idx_t uncompressed_page_size) {
185: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
186: 
187: 	block = make_shared<ResizeableBuffer>(reader.allocator, compressed_page_size + 1);
188: 	trans.read((uint8_t *)block->ptr, compressed_page_size);
189: 
190: 	// TODO this allocation should probably be avoided
191: 	shared_ptr<ResizeableBuffer> unpacked_block;
192: 	if (chunk->meta_data.codec != CompressionCodec::UNCOMPRESSED) {
193: 		unpacked_block = make_shared<ResizeableBuffer>(reader.allocator, uncompressed_page_size + 1);
194: 	}
195: 
196: 	switch (chunk->meta_data.codec) {
197: 	case CompressionCodec::UNCOMPRESSED:
198: 		break;
199: 	case CompressionCodec::GZIP: {
200: 		MiniZStream s;
201: 
202: 		s.Decompress((const char *)block->ptr, compressed_page_size, (char *)unpacked_block->ptr,
203: 		             uncompressed_page_size);
204: 		block = move(unpacked_block);
205: 
206: 		break;
207: 	}
208: 	case CompressionCodec::SNAPPY: {
209: 		auto res =
210: 		    duckdb_snappy::RawUncompress((const char *)block->ptr, compressed_page_size, (char *)unpacked_block->ptr);
211: 		if (!res) {
212: 			throw std::runtime_error("Decompression failure");
213: 		}
214: 		block = move(unpacked_block);
215: 		break;
216: 	}
217: 	case CompressionCodec::ZSTD: {
218: 		auto res = duckdb_zstd::ZSTD_decompress((char *)unpacked_block->ptr, uncompressed_page_size,
219: 		                                        (const char *)block->ptr, compressed_page_size);
220: 		if (duckdb_zstd::ZSTD_isError(res) || res != (size_t)uncompressed_page_size) {
221: 			throw std::runtime_error("ZSTD Decompression failure");
222: 		}
223: 		block = move(unpacked_block);
224: 		break;
225: 	}
226: 
227: 	default: {
228: 		std::stringstream codec_name;
229: 		codec_name << chunk->meta_data.codec;
230: 		throw std::runtime_error("Unsupported compression codec \"" + codec_name.str() +
231: 		                         "\". Supported options are uncompressed, gzip or snappy");
232: 		break;
233: 	}
234: 	}
235: }
236: 
237: void ColumnReader::PrepareDataPage(PageHeader &page_hdr) {
238: 	if (page_hdr.type == PageType::DATA_PAGE && !page_hdr.__isset.data_page_header) {
239: 		throw std::runtime_error("Missing data page header from data page");
240: 	}
241: 	if (page_hdr.type == PageType::DATA_PAGE_V2 && !page_hdr.__isset.data_page_header_v2) {
242: 		throw std::runtime_error("Missing data page header from data page v2");
243: 	}
244: 
245: 	page_rows_available = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.num_values
246: 	                                                           : page_hdr.data_page_header_v2.num_values;
247: 	auto page_encoding = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.encoding
248: 	                                                          : page_hdr.data_page_header_v2.encoding;
249: 
250: 	if (HasRepeats()) {
251: 		uint32_t rep_length = page_hdr.type == PageType::DATA_PAGE
252: 		                          ? block->read<uint32_t>()
253: 		                          : page_hdr.data_page_header_v2.repetition_levels_byte_length;
254: 		block->available(rep_length);
255: 		repeated_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, rep_length,
256: 		                                             RleBpDecoder::ComputeBitWidth(max_repeat));
257: 		block->inc(rep_length);
258: 	}
259: 
260: 	if (HasDefines()) {
261: 		uint32_t def_length = page_hdr.type == PageType::DATA_PAGE
262: 		                          ? block->read<uint32_t>()
263: 		                          : page_hdr.data_page_header_v2.definition_levels_byte_length;
264: 		block->available(def_length);
265: 		defined_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, def_length,
266: 		                                            RleBpDecoder::ComputeBitWidth(max_define));
267: 		block->inc(def_length);
268: 	}
269: 
270: 	switch (page_encoding) {
271: 	case Encoding::RLE_DICTIONARY:
272: 	case Encoding::PLAIN_DICTIONARY: {
273: 		// where is it otherwise??
274: 		auto dict_width = block->read<uint8_t>();
275: 		// TODO somehow dict_width can be 0 ?
276: 		dict_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, block->len, dict_width);
277: 		block->inc(block->len);
278: 		break;
279: 	}
280: 	case Encoding::DELTA_BINARY_PACKED: {
281: 		dbp_decoder = make_unique<DbpDecoder>((const uint8_t *)block->ptr, block->len);
282: 		block->inc(block->len);
283: 		break;
284: 	}
285: 		/*
286: 	case Encoding::DELTA_BYTE_ARRAY: {
287: 		dbp_decoder = make_unique<DbpDecoder>((const uint8_t *)block->ptr, block->len);
288: 		auto prefix_buffer = make_shared<ResizeableBuffer>();
289: 		prefix_buffer->resize(reader.allocator, sizeof(uint32_t) * page_hdr.data_page_header_v2.num_rows);
290: 
291: 		auto suffix_buffer = make_shared<ResizeableBuffer>();
292: 		suffix_buffer->resize(reader.allocator, sizeof(uint32_t) * page_hdr.data_page_header_v2.num_rows);
293: 
294: 		dbp_decoder->GetBatch<uint32_t>(prefix_buffer->ptr, page_hdr.data_page_header_v2.num_rows);
295: 		auto buffer_after_prefixes = dbp_decoder->BufferPtr();
296: 
297: 		dbp_decoder = make_unique<DbpDecoder>((const uint8_t *)buffer_after_prefixes.ptr, buffer_after_prefixes.len);
298: 		dbp_decoder->GetBatch<uint32_t>(suffix_buffer->ptr, page_hdr.data_page_header_v2.num_rows);
299: 
300: 		auto string_buffer = dbp_decoder->BufferPtr();
301: 
302: 		for (idx_t i = 0 ; i < page_hdr.data_page_header_v2.num_rows; i++) {
303: 		    auto suffix_length = (uint32_t*) suffix_buffer->ptr;
304: 		    string str( suffix_length[i] + 1, '\0');
305: 		    string_buffer.copy_to((char*) str.data(), suffix_length[i]);
306: 		    printf("%s\n", str.c_str());
307: 		}
308: 		throw std::runtime_error("eek");
309: 
310: 
311: 		// This is also known as incremental encoding or front compression: for each element in a sequence of strings,
312: 		// store the prefix length of the previous entry plus the suffix. This is stored as a sequence of delta-encoded
313: 		// prefix lengths (DELTA_BINARY_PACKED), followed by the suffixes encoded as delta length byte arrays
314: 		// (DELTA_LENGTH_BYTE_ARRAY). DELTA_LENGTH_BYTE_ARRAY: The encoded data would be DeltaEncoding(5, 5, 6, 6)
315: 		// "HelloWorldFoobarABCDEF"
316: 
317: 		// TODO actually do something here
318: 		break;
319: 	}
320: 		 */
321: 	case Encoding::PLAIN:
322: 		// nothing to do here, will be read directly below
323: 		break;
324: 
325: 	default:
326: 		throw std::runtime_error("Unsupported page encoding");
327: 	}
328: }
329: 
330: idx_t ColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
331:                          Vector &result) {
332: 	// we need to reset the location because multiple column readers share the same protocol
333: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
334: 	trans.SetLocation(chunk_read_offset);
335: 
336: 	idx_t result_offset = 0;
337: 	auto to_read = num_values;
338: 
339: 	while (to_read > 0) {
340: 		while (page_rows_available == 0) {
341: 			PrepareRead(filter);
342: 		}
343: 
344: 		D_ASSERT(block);
345: 		auto read_now = MinValue<idx_t>(to_read, page_rows_available);
346: 
347: 		D_ASSERT(read_now <= STANDARD_VECTOR_SIZE);
348: 
349: 		if (HasRepeats()) {
350: 			D_ASSERT(repeated_decoder);
351: 			repeated_decoder->GetBatch<uint8_t>((char *)repeat_out + result_offset, read_now);
352: 		}
353: 
354: 		if (HasDefines()) {
355: 			D_ASSERT(defined_decoder);
356: 			defined_decoder->GetBatch<uint8_t>((char *)define_out + result_offset, read_now);
357: 		}
358: 
359: 		idx_t null_count = 0;
360: 
361: 		if ((dict_decoder || dbp_decoder) && HasDefines()) {
362: 			// we need the null count because the dictionary offsets have no entries for nulls
363: 			for (idx_t i = 0; i < read_now; i++) {
364: 				if (define_out[i + result_offset] != max_define) {
365: 					null_count++;
366: 				}
367: 			}
368: 		}
369: 
370: 		if (dict_decoder) {
371: 			offset_buffer.resize(reader.allocator, sizeof(uint32_t) * (read_now - null_count));
372: 			dict_decoder->GetBatch<uint32_t>(offset_buffer.ptr, read_now - null_count);
373: 			DictReference(result);
374: 			Offsets((uint32_t *)offset_buffer.ptr, define_out, read_now, filter, result_offset, result);
375: 		} else if (dbp_decoder) {
376: 			// TODO keep this in the state
377: 			auto read_buf = make_shared<ResizeableBuffer>();
378: 
379: 			switch (type.id()) {
380: 			case LogicalTypeId::INTEGER:
381: 				read_buf->resize(reader.allocator, sizeof(int32_t) * (read_now - null_count));
382: 				dbp_decoder->GetBatch<int32_t>(read_buf->ptr, read_now - null_count);
383: 
384: 				break;
385: 			case LogicalTypeId::BIGINT:
386: 				read_buf->resize(reader.allocator, sizeof(int64_t) * (read_now - null_count));
387: 				dbp_decoder->GetBatch<int64_t>(read_buf->ptr, read_now - null_count);
388: 				break;
389: 
390: 			default:
391: 				throw std::runtime_error("DELTA_BINARY_PACKED should only be INT32 or INT64");
392: 			}
393: 			// Plain() will put NULLs in the right place
394: 			Plain(read_buf, define_out, read_now, filter, result_offset, result);
395: 		} else {
396: 			PlainReference(block, result);
397: 			Plain(block, define_out, read_now, filter, result_offset, result);
398: 		}
399: 
400: 		result_offset += read_now;
401: 		page_rows_available -= read_now;
402: 		to_read -= read_now;
403: 	}
404: 	group_rows_available -= num_values;
405: 	chunk_read_offset = trans.GetLocation();
406: 
407: 	return num_values;
408: }
409: 
410: void ColumnReader::Skip(idx_t num_values) {
411: 	dummy_define.zero();
412: 	dummy_repeat.zero();
413: 
414: 	// TODO this can be optimized, for example we dont actually have to bitunpack offsets
415: 	Vector dummy_result(type, nullptr);
416: 	auto values_read =
417: 	    Read(num_values, none_filter, (uint8_t *)dummy_define.ptr, (uint8_t *)dummy_repeat.ptr, dummy_result);
418: 	if (values_read != num_values) {
419: 		throw std::runtime_error("Row count mismatch when skipping rows");
420: 	}
421: }
422: 
423: //===--------------------------------------------------------------------===//
424: // String Column Reader
425: //===--------------------------------------------------------------------===//
426: StringColumnReader::StringColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
427:                                        idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p)
428:     : TemplatedColumnReader<string_t, StringParquetValueConversion>(reader, move(type_p), schema_p, schema_idx_p,
429:                                                                     max_define_p, max_repeat_p) {
430: 	fixed_width_string_length = 0;
431: 	if (schema_p.type == Type::FIXED_LEN_BYTE_ARRAY) {
432: 		D_ASSERT(schema_p.__isset.type_length);
433: 		fixed_width_string_length = schema_p.type_length;
434: 	}
435: }
436: 
437: uint32_t StringColumnReader::VerifyString(const char *str_data, uint32_t str_len) {
438: 	if (Type() != LogicalTypeId::VARCHAR) {
439: 		return str_len;
440: 	}
441: 	// verify if a string is actually UTF8, and if there are no null bytes in the middle of the string
442: 	// technically Parquet should guarantee this, but reality is often disappointing
443: 	UnicodeInvalidReason reason;
444: 	size_t pos;
445: 	auto utf_type = Utf8Proc::Analyze(str_data, str_len, &reason, &pos);
446: 	if (utf_type == UnicodeType::INVALID) {
447: 		if (reason == UnicodeInvalidReason::NULL_BYTE) {
448: 			// for null bytes we just truncate the string
449: 			return pos;
450: 		}
451: 		throw InvalidInputException("Invalid string encoding found in Parquet file: value \"" +
452: 		                            Blob::ToString(string_t(str_data, str_len)) + "\" is not valid UTF8!");
453: 	}
454: 	return str_len;
455: }
456: 
457: void StringColumnReader::Dictionary(shared_ptr<ByteBuffer> data, idx_t num_entries) {
458: 	dict = move(data);
459: 	dict_strings = unique_ptr<string_t[]>(new string_t[num_entries]);
460: 	for (idx_t dict_idx = 0; dict_idx < num_entries; dict_idx++) {
461: 		uint32_t str_len = dict->read<uint32_t>();
462: 		dict->available(str_len);
463: 
464: 		auto actual_str_len = VerifyString(dict->ptr, str_len);
465: 		dict_strings[dict_idx] = string_t(dict->ptr, actual_str_len);
466: 		dict->inc(str_len);
467: 	}
468: }
469: 
470: class ParquetStringVectorBuffer : public VectorBuffer {
471: public:
472: 	explicit ParquetStringVectorBuffer(shared_ptr<ByteBuffer> buffer_p)
473: 	    : VectorBuffer(VectorBufferType::OPAQUE_BUFFER), buffer(move(buffer_p)) {
474: 	}
475: 
476: private:
477: 	shared_ptr<ByteBuffer> buffer;
478: };
479: 
480: void StringColumnReader::DictReference(Vector &result) {
481: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(dict));
482: }
483: void StringColumnReader::PlainReference(shared_ptr<ByteBuffer> plain_data, Vector &result) {
484: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(move(plain_data)));
485: }
486: 
487: string_t StringParquetValueConversion::DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
488: 	auto &dict_strings = ((StringColumnReader &)reader).dict_strings;
489: 	return dict_strings[offset];
490: }
491: 
492: string_t StringParquetValueConversion::PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
493: 	auto &scr = ((StringColumnReader &)reader);
494: 	uint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;
495: 	plain_data.available(str_len);
496: 	auto actual_str_len = ((StringColumnReader &)reader).VerifyString(plain_data.ptr, str_len);
497: 	auto ret_str = string_t(plain_data.ptr, actual_str_len);
498: 	plain_data.inc(str_len);
499: 	return ret_str;
500: }
501: 
502: void StringParquetValueConversion::PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
503: 	auto &scr = ((StringColumnReader &)reader);
504: 	uint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;
505: 	plain_data.inc(str_len);
506: }
507: 
508: //===--------------------------------------------------------------------===//
509: // List Column Reader
510: //===--------------------------------------------------------------------===//
511: idx_t ListColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
512:                              Vector &result_out) {
513: 	idx_t result_offset = 0;
514: 	auto result_ptr = FlatVector::GetData<list_entry_t>(result_out);
515: 	auto &result_mask = FlatVector::Validity(result_out);
516: 
517: 	D_ASSERT(ListVector::GetListSize(result_out) == 0);
518: 	// if an individual list is longer than STANDARD_VECTOR_SIZE we actually have to loop the child read to fill it
519: 	bool finished = false;
520: 	while (!finished) {
521: 		idx_t child_actual_num_values = 0;
522: 
523: 		// check if we have any overflow from a previous read
524: 		if (overflow_child_count == 0) {
525: 			// we don't: read elements from the child reader
526: 			child_defines.zero();
527: 			child_repeats.zero();
528: 			// we don't know in advance how many values to read because of the beautiful repetition/definition setup
529: 			// we just read (up to) a vector from the child column, and see if we have read enough
530: 			// if we have not read enough, we read another vector
531: 			// if we have read enough, we leave any unhandled elements in the overflow vector for a subsequent read
532: 			auto child_req_num_values =
533: 			    MinValue<idx_t>(STANDARD_VECTOR_SIZE, child_column_reader->GroupRowsAvailable());
534: 			read_vector.ResetFromCache(read_cache);
535: 			child_actual_num_values = child_column_reader->Read(child_req_num_values, child_filter, child_defines_ptr,
536: 			                                                    child_repeats_ptr, read_vector);
537: 		} else {
538: 			// we do: use the overflow values
539: 			child_actual_num_values = overflow_child_count;
540: 			overflow_child_count = 0;
541: 		}
542: 
543: 		if (child_actual_num_values == 0) {
544: 			// no more elements available: we are done
545: 			break;
546: 		}
547: 		read_vector.Verify(child_actual_num_values);
548: 		idx_t current_chunk_offset = ListVector::GetListSize(result_out);
549: 
550: 		// hard-won piece of code this, modify at your own risk
551: 		// the intuition is that we have to only collapse values into lists that are repeated *on this level*
552: 		// the rest is pretty much handed up as-is as a single-valued list or NULL
553: 		idx_t child_idx;
554: 		for (child_idx = 0; child_idx < child_actual_num_values; child_idx++) {
555: 			if (child_repeats_ptr[child_idx] == max_repeat) {
556: 				// value repeats on this level, append
557: 				D_ASSERT(result_offset > 0);
558: 				result_ptr[result_offset - 1].length++;
559: 				continue;
560: 			}
561: 
562: 			if (result_offset >= num_values) {
563: 				// we ran out of output space
564: 				finished = true;
565: 				break;
566: 			}
567: 			if (child_defines_ptr[child_idx] >= max_define) {
568: 				// value has been defined down the stack, hence its NOT NULL
569: 				result_ptr[result_offset].offset = child_idx + current_chunk_offset;
570: 				result_ptr[result_offset].length = 1;
571: 			} else if (child_defines_ptr[child_idx] == max_define - 1) {
572: 				// empty list
573: 				result_ptr[result_offset].offset = child_idx + current_chunk_offset;
574: 				result_ptr[result_offset].length = 0;
575: 			} else {
576: 				// value is NULL somewhere up the stack
577: 				result_mask.SetInvalid(result_offset);
578: 				result_ptr[result_offset].offset = 0;
579: 				result_ptr[result_offset].length = 0;
580: 			}
581: 
582: 			repeat_out[result_offset] = child_repeats_ptr[child_idx];
583: 			define_out[result_offset] = child_defines_ptr[child_idx];
584: 
585: 			result_offset++;
586: 		}
587: 		// actually append the required elements to the child list
588: 		ListVector::Append(result_out, read_vector, child_idx);
589: 
590: 		// we have read more values from the child reader than we can fit into the result for this read
591: 		// we have to pass everything from child_idx to child_actual_num_values into the next call
592: 		if (child_idx < child_actual_num_values && result_offset == num_values) {
593: 			read_vector.Slice(read_vector, child_idx);
594: 			overflow_child_count = child_actual_num_values - child_idx;
595: 			read_vector.Verify(overflow_child_count);
596: 
597: 			// move values in the child repeats and defines *backward* by child_idx
598: 			for (idx_t repdef_idx = 0; repdef_idx < overflow_child_count; repdef_idx++) {
599: 				child_defines_ptr[repdef_idx] = child_defines_ptr[child_idx + repdef_idx];
600: 				child_repeats_ptr[repdef_idx] = child_repeats_ptr[child_idx + repdef_idx];
601: 			}
602: 		}
603: 	}
604: 	result_out.Verify(result_offset);
605: 	return result_offset;
606: }
607: 
608: ListColumnReader::ListColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
609:                                    idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p,
610:                                    unique_ptr<ColumnReader> child_column_reader_p)
611:     : ColumnReader(reader, move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p),
612:       child_column_reader(move(child_column_reader_p)), read_cache(ListType::GetChildType(Type())),
613:       read_vector(read_cache), overflow_child_count(0) {
614: 
615: 	child_defines.resize(reader.allocator, STANDARD_VECTOR_SIZE);
616: 	child_repeats.resize(reader.allocator, STANDARD_VECTOR_SIZE);
617: 	child_defines_ptr = (uint8_t *)child_defines.ptr;
618: 	child_repeats_ptr = (uint8_t *)child_repeats.ptr;
619: 
620: 	child_filter.set();
621: }
622: 
623: //===--------------------------------------------------------------------===//
624: // Struct Column Reader
625: //===--------------------------------------------------------------------===//
626: StructColumnReader::StructColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
627:                                        idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p,
628:                                        vector<unique_ptr<ColumnReader>> child_readers_p)
629:     : ColumnReader(reader, move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p),
630:       child_readers(move(child_readers_p)) {
631: 	D_ASSERT(type.InternalType() == PhysicalType::STRUCT);
632: }
633: 
634: ColumnReader *StructColumnReader::GetChildReader(idx_t child_idx) {
635: 	return child_readers[child_idx].get();
636: }
637: 
638: void StructColumnReader::InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) {
639: 	for (auto &child : child_readers) {
640: 		child->InitializeRead(columns, protocol_p);
641: 	}
642: }
643: 
644: idx_t StructColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
645:                                Vector &result) {
646: 	auto &struct_entries = StructVector::GetEntries(result);
647: 	D_ASSERT(StructType::GetChildTypes(Type()).size() == struct_entries.size());
648: 
649: 	idx_t read_count = num_values;
650: 	for (idx_t i = 0; i < struct_entries.size(); i++) {
651: 		auto child_num_values = child_readers[i]->Read(num_values, filter, define_out, repeat_out, *struct_entries[i]);
652: 		if (i == 0) {
653: 			read_count = child_num_values;
654: 		} else if (read_count != child_num_values) {
655: 			throw std::runtime_error("Struct child row count mismatch");
656: 		}
657: 	}
658: 	// set the validity mask for this level
659: 	auto &validity = FlatVector::Validity(result);
660: 	for (idx_t i = 0; i < read_count; i++) {
661: 		if (define_out[i] < max_define) {
662: 			validity.SetInvalid(i);
663: 		}
664: 	}
665: 
666: 	return read_count;
667: }
668: 
669: void StructColumnReader::Skip(idx_t num_values) {
670: 	throw InternalException("Skip not implemented for StructColumnReader");
671: }
672: 
673: idx_t StructColumnReader::GroupRowsAvailable() {
674: 	for (idx_t i = 0; i < child_readers.size(); i++) {
675: 		if (child_readers[i]->Type().id() != LogicalTypeId::LIST) {
676: 			return child_readers[i]->GroupRowsAvailable();
677: 		}
678: 	}
679: 	return child_readers[0]->GroupRowsAvailable();
680: }
681: 
682: //===--------------------------------------------------------------------===//
683: // Decimal Column Reader
684: //===--------------------------------------------------------------------===//
685: template <class DUCKDB_PHYSICAL_TYPE, bool FIXED_LENGTH>
686: struct DecimalParquetValueConversion {
687: 	static DUCKDB_PHYSICAL_TYPE DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
688: 		auto dict_ptr = (DUCKDB_PHYSICAL_TYPE *)dict.ptr;
689: 		return dict_ptr[offset];
690: 	}
691: 
692: 	static DUCKDB_PHYSICAL_TYPE PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
693: 		idx_t byte_len;
694: 		if (FIXED_LENGTH) {
695: 			byte_len = (idx_t)reader.Schema().type_length; /* sure, type length needs to be a signed int */
696: 		} else {
697: 			byte_len = plain_data.read<uint32_t>();
698: 		}
699: 		plain_data.available(byte_len);
700: 		auto res =
701: 		    ParquetDecimalUtils::ReadDecimalValue<DUCKDB_PHYSICAL_TYPE>((const_data_ptr_t)plain_data.ptr, byte_len);
702: 
703: 		plain_data.inc(byte_len);
704: 		return res;
705: 	}
706: 
707: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
708: 		uint32_t decimal_len = FIXED_LENGTH ? reader.Schema().type_length : plain_data.read<uint32_t>();
709: 		plain_data.inc(decimal_len);
710: 	}
711: };
712: 
713: template <class DUCKDB_PHYSICAL_TYPE, bool FIXED_LENGTH>
714: class DecimalColumnReader
715:     : public TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE,
716:                                    DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE, FIXED_LENGTH>> {
717: 
718: public:
719: 	DecimalColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, // NOLINT
720: 	                    idx_t file_idx_p, idx_t max_define_p, idx_t max_repeat_p)
721: 	    : TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE,
722: 	                            DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE, FIXED_LENGTH>>(
723: 	          reader, move(type_p), schema_p, file_idx_p, max_define_p, max_repeat_p) {};
724: 
725: protected:
726: 	void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) { // NOLINT
727: 		this->dict = make_shared<ResizeableBuffer>(this->reader.allocator, num_entries * sizeof(DUCKDB_PHYSICAL_TYPE));
728: 		auto dict_ptr = (DUCKDB_PHYSICAL_TYPE *)this->dict->ptr;
729: 		for (idx_t i = 0; i < num_entries; i++) {
730: 			dict_ptr[i] =
731: 			    DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE, FIXED_LENGTH>::PlainRead(*dictionary_data, *this);
732: 		}
733: 	}
734: };
735: 
736: template <bool FIXED_LENGTH>
737: static unique_ptr<ColumnReader> CreateDecimalReaderInternal(ParquetReader &reader, const LogicalType &type_p,
738:                                                             const SchemaElement &schema_p, idx_t file_idx_p,
739:                                                             idx_t max_define, idx_t max_repeat) {
740: 	switch (type_p.InternalType()) {
741: 	case PhysicalType::INT16:
742: 		return make_unique<DecimalColumnReader<int16_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p, max_define,
743: 		                                                               max_repeat);
744: 	case PhysicalType::INT32:
745: 		return make_unique<DecimalColumnReader<int32_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p, max_define,
746: 		                                                               max_repeat);
747: 	case PhysicalType::INT64:
748: 		return make_unique<DecimalColumnReader<int64_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p, max_define,
749: 		                                                               max_repeat);
750: 	case PhysicalType::INT128:
751: 		return make_unique<DecimalColumnReader<hugeint_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p,
752: 		                                                                 max_define, max_repeat);
753: 	default:
754: 		throw InternalException("Unrecognized type for Decimal");
755: 	}
756: }
757: 
758: unique_ptr<ColumnReader> ParquetDecimalUtils::CreateReader(ParquetReader &reader, const LogicalType &type_p,
759:                                                            const SchemaElement &schema_p, idx_t file_idx_p,
760:                                                            idx_t max_define, idx_t max_repeat) {
761: 	if (schema_p.__isset.type_length) {
762: 		return CreateDecimalReaderInternal<true>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
763: 	} else {
764: 		return CreateDecimalReaderInternal<false>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
765: 	}
766: }
767: 
768: //===--------------------------------------------------------------------===//
769: // UUID Column Reader
770: //===--------------------------------------------------------------------===//
771: struct UUIDValueConversion {
772: 	static hugeint_t DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
773: 		auto dict_ptr = (hugeint_t *)dict.ptr;
774: 		return dict_ptr[offset];
775: 	}
776: 
777: 	static hugeint_t ReadParquetUUID(const_data_ptr_t input) {
778: 		hugeint_t result;
779: 		result.lower = 0;
780: 		uint64_t unsigned_upper = 0;
781: 		for (idx_t i = 0; i < sizeof(uint64_t); i++) {
782: 			unsigned_upper <<= 8;
783: 			unsigned_upper += input[i];
784: 		}
785: 		for (idx_t i = sizeof(uint64_t); i < sizeof(hugeint_t); i++) {
786: 			result.lower <<= 8;
787: 			result.lower += input[i];
788: 		}
789: 		result.upper = unsigned_upper;
790: 		result.upper ^= (int64_t(1) << 63);
791: 		return result;
792: 	}
793: 
794: 	static hugeint_t PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
795: 		idx_t byte_len = sizeof(hugeint_t);
796: 		plain_data.available(byte_len);
797: 		auto res = ReadParquetUUID((const_data_ptr_t)plain_data.ptr);
798: 
799: 		plain_data.inc(byte_len);
800: 		return res;
801: 	}
802: 
803: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
804: 		plain_data.inc(sizeof(hugeint_t));
805: 	}
806: };
807: 
808: class UUIDColumnReader : public TemplatedColumnReader<hugeint_t, UUIDValueConversion> {
809: 
810: public:
811: 	UUIDColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
812: 	                 idx_t max_define_p, idx_t max_repeat_p)
813: 	    : TemplatedColumnReader<hugeint_t, UUIDValueConversion>(reader, move(type_p), schema_p, file_idx_p,
814: 	                                                            max_define_p, max_repeat_p) {};
815: 
816: protected:
817: 	void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) { // NOLINT
818: 		this->dict = make_shared<ResizeableBuffer>(this->reader.allocator, num_entries * sizeof(hugeint_t));
819: 		auto dict_ptr = (hugeint_t *)this->dict->ptr;
820: 		for (idx_t i = 0; i < num_entries; i++) {
821: 			dict_ptr[i] = UUIDValueConversion::PlainRead(*dictionary_data, *this);
822: 		}
823: 	}
824: };
825: 
826: //===--------------------------------------------------------------------===//
827: // Interval Column Reader
828: //===--------------------------------------------------------------------===//
829: struct IntervalValueConversion {
830: 	static constexpr const idx_t PARQUET_INTERVAL_SIZE = 12;
831: 
832: 	static interval_t DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
833: 		auto dict_ptr = (interval_t *)dict.ptr;
834: 		return dict_ptr[offset];
835: 	}
836: 
837: 	static interval_t ReadParquetInterval(const_data_ptr_t input) {
838: 		interval_t result;
839: 		result.months = Load<uint32_t>(input);
840: 		result.days = Load<uint32_t>(input + sizeof(uint32_t));
841: 		result.micros = int64_t(Load<uint32_t>(input + sizeof(uint32_t) * 2)) * 1000;
842: 		return result;
843: 	}
844: 
845: 	static interval_t PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
846: 		idx_t byte_len = PARQUET_INTERVAL_SIZE;
847: 		plain_data.available(byte_len);
848: 		auto res = ReadParquetInterval((const_data_ptr_t)plain_data.ptr);
849: 
850: 		plain_data.inc(byte_len);
851: 		return res;
852: 	}
853: 
854: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
855: 		plain_data.inc(PARQUET_INTERVAL_SIZE);
856: 	}
857: };
858: 
859: class IntervalColumnReader : public TemplatedColumnReader<interval_t, IntervalValueConversion> {
860: 
861: public:
862: 	IntervalColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
863: 	                     idx_t max_define_p, idx_t max_repeat_p)
864: 	    : TemplatedColumnReader<interval_t, IntervalValueConversion>(reader, move(type_p), schema_p, file_idx_p,
865: 	                                                                 max_define_p, max_repeat_p) {};
866: 
867: protected:
868: 	void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) override { // NOLINT
869: 		this->dict = make_shared<ResizeableBuffer>(this->reader.allocator, num_entries * sizeof(interval_t));
870: 		auto dict_ptr = (interval_t *)this->dict->ptr;
871: 		for (idx_t i = 0; i < num_entries; i++) {
872: 			dict_ptr[i] = IntervalValueConversion::PlainRead(*dictionary_data, *this);
873: 		}
874: 	}
875: };
876: 
877: //===--------------------------------------------------------------------===//
878: // Create Column Reader
879: //===--------------------------------------------------------------------===//
880: template <class T>
881: unique_ptr<ColumnReader> CreateDecimalReader(ParquetReader &reader, const LogicalType &type_p,
882:                                              const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,
883:                                              idx_t max_repeat) {
884: 	switch (type_p.InternalType()) {
885: 	case PhysicalType::INT16:
886: 		return make_unique<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<T>>>(
887: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
888: 	case PhysicalType::INT32:
889: 		return make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<T>>>(
890: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
891: 	case PhysicalType::INT64:
892: 		return make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<T>>>(
893: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
894: 	default:
895: 		throw NotImplementedException("Unimplemented internal type for CreateDecimalReader");
896: 	}
897: }
898: 
899: unique_ptr<ColumnReader> ColumnReader::CreateReader(ParquetReader &reader, const LogicalType &type_p,
900:                                                     const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,
901:                                                     idx_t max_repeat) {
902: 	switch (type_p.id()) {
903: 	case LogicalTypeId::BOOLEAN:
904: 		return make_unique<BooleanColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
905: 	case LogicalTypeId::UTINYINT:
906: 		return make_unique<TemplatedColumnReader<uint8_t, TemplatedParquetValueConversion<uint32_t>>>(
907: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
908: 	case LogicalTypeId::USMALLINT:
909: 		return make_unique<TemplatedColumnReader<uint16_t, TemplatedParquetValueConversion<uint32_t>>>(
910: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
911: 	case LogicalTypeId::UINTEGER:
912: 		return make_unique<TemplatedColumnReader<uint32_t, TemplatedParquetValueConversion<uint32_t>>>(
913: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
914: 	case LogicalTypeId::UBIGINT:
915: 		return make_unique<TemplatedColumnReader<uint64_t, TemplatedParquetValueConversion<uint64_t>>>(
916: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
917: 	case LogicalTypeId::TINYINT:
918: 		return make_unique<TemplatedColumnReader<int8_t, TemplatedParquetValueConversion<int32_t>>>(
919: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
920: 	case LogicalTypeId::SMALLINT:
921: 		return make_unique<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<int32_t>>>(
922: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
923: 	case LogicalTypeId::INTEGER:
924: 		return make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<int32_t>>>(
925: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
926: 	case LogicalTypeId::BIGINT:
927: 		return make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<int64_t>>>(
928: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
929: 	case LogicalTypeId::FLOAT:
930: 		return make_unique<TemplatedColumnReader<float, TemplatedParquetValueConversion<float>>>(
931: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
932: 	case LogicalTypeId::DOUBLE:
933: 		return make_unique<TemplatedColumnReader<double, TemplatedParquetValueConversion<double>>>(
934: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
935: 	case LogicalTypeId::TIMESTAMP:
936: 		switch (schema_p.type) {
937: 		case Type::INT96:
938: 			return make_unique<CallbackColumnReader<Int96, timestamp_t, ImpalaTimestampToTimestamp>>(
939: 			    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
940: 		case Type::INT64:
941: 			switch (schema_p.converted_type) {
942: 			case ConvertedType::TIMESTAMP_MICROS:
943: 				return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMicrosToTimestamp>>(
944: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
945: 			case ConvertedType::TIMESTAMP_MILLIS:
946: 				return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMsToTimestamp>>(
947: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
948: 			default:
949: 				break;
950: 			}
951: 		default:
952: 			break;
953: 		}
954: 		break;
955: 	case LogicalTypeId::DATE:
956: 		return make_unique<CallbackColumnReader<int32_t, date_t, ParquetIntToDate>>(reader, type_p, schema_p,
957: 		                                                                            file_idx_p, max_define, max_repeat);
958: 	case LogicalTypeId::TIME:
959: 		return make_unique<CallbackColumnReader<int64_t, dtime_t, ParquetIntToTime>>(
960: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
961: 	case LogicalTypeId::BLOB:
962: 	case LogicalTypeId::VARCHAR:
963: 		return make_unique<StringColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
964: 	case LogicalTypeId::DECIMAL:
965: 		// we have to figure out what kind of int we need
966: 		switch (schema_p.type) {
967: 		case Type::INT32:
968: 			return CreateDecimalReader<int32_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
969: 		case Type::INT64:
970: 			return CreateDecimalReader<int64_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
971: 		case Type::BYTE_ARRAY:
972: 		case Type::FIXED_LEN_BYTE_ARRAY:
973: 			return ParquetDecimalUtils::CreateReader(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
974: 		default:
975: 			throw NotImplementedException("Unrecognized Parquet type for Decimal");
976: 		}
977: 		break;
978: 	case LogicalTypeId::UUID:
979: 		return make_unique<UUIDColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
980: 	case LogicalTypeId::INTERVAL:
981: 		return make_unique<IntervalColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
982: 	default:
983: 		break;
984: 	}
985: 	throw NotImplementedException(type_p.ToString());
986: }
987: 
988: } // namespace duckdb
[end of extension/parquet/column_reader.cpp]
[start of extension/parquet/include/list_column_reader.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // list_column_reader.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "column_reader.hpp"
12: #include "templated_column_reader.hpp"
13: 
14: namespace duckdb {
15: 
16: class ListColumnReader : public ColumnReader {
17: public:
18: 	ListColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t schema_idx_p,
19: 	                 idx_t max_define_p, idx_t max_repeat_p, unique_ptr<ColumnReader> child_column_reader_p);
20: 
21: 	idx_t Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
22: 	           Vector &result_out) override;
23: 
24: 	virtual void Skip(idx_t num_values) override {
25: 		D_ASSERT(0);
26: 	}
27: 
28: 	void InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) override {
29: 		child_column_reader->InitializeRead(columns, protocol_p);
30: 	}
31: 
32: 	idx_t GroupRowsAvailable() override {
33: 		return child_column_reader->GroupRowsAvailable() + overflow_child_count;
34: 	}
35: 
36: private:
37: 	unique_ptr<ColumnReader> child_column_reader;
38: 	ResizeableBuffer child_defines;
39: 	ResizeableBuffer child_repeats;
40: 	uint8_t *child_defines_ptr;
41: 	uint8_t *child_repeats_ptr;
42: 
43: 	VectorCache read_cache;
44: 	Vector read_vector;
45: 
46: 	parquet_filter_t child_filter;
47: 
48: 	idx_t overflow_child_count;
49: };
50: 
51: } // namespace duckdb
[end of extension/parquet/include/list_column_reader.hpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: