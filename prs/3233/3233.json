{
  "repo": "duckdb/duckdb",
  "pull_number": 3233,
  "instance_id": "duckdb__duckdb-3233",
  "issue_numbers": [
    "3205",
    "3205"
  ],
  "base_commit": "a004ed237f40bf9a6d916731207b958abb912db6",
  "patch": "diff --git a/extension/parquet/column_reader.cpp b/extension/parquet/column_reader.cpp\nindex fe51325b83df..f3f39379f218 100644\n--- a/extension/parquet/column_reader.cpp\n+++ b/extension/parquet/column_reader.cpp\n@@ -620,6 +620,15 @@ ListColumnReader::ListColumnReader(ParquetReader &reader, LogicalType type_p, co\n \tchild_filter.set();\n }\n \n+// ListColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,\n+//                             Vector &result_out)\n+void ListColumnReader::Skip(idx_t num_values) {\n+\tparquet_filter_t filter;\n+\tauto define_out = unique_ptr<uint8_t[]>(new uint8_t[num_values]);\n+\tauto repeat_out = unique_ptr<uint8_t[]>(new uint8_t[num_values]);\n+\tVector result_out(Type());\n+\tRead(num_values, filter, define_out.get(), repeat_out.get(), result_out);\n+}\n //===--------------------------------------------------------------------===//\n // Struct Column Reader\n //===--------------------------------------------------------------------===//\n@@ -667,7 +676,9 @@ idx_t StructColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, ui\n }\n \n void StructColumnReader::Skip(idx_t num_values) {\n-\tthrow InternalException(\"Skip not implemented for StructColumnReader\");\n+\tfor (auto &child_reader : child_readers) {\n+\t\tchild_reader->Skip(num_values);\n+\t}\n }\n \n idx_t StructColumnReader::GroupRowsAvailable() {\ndiff --git a/extension/parquet/include/list_column_reader.hpp b/extension/parquet/include/list_column_reader.hpp\nindex b4ccd4842def..e734079ce2dc 100644\n--- a/extension/parquet/include/list_column_reader.hpp\n+++ b/extension/parquet/include/list_column_reader.hpp\n@@ -21,9 +21,7 @@ class ListColumnReader : public ColumnReader {\n \tidx_t Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,\n \t           Vector &result_out) override;\n \n-\tvirtual void Skip(idx_t num_values) override {\n-\t\tD_ASSERT(0);\n-\t}\n+\tvoid Skip(idx_t num_values) override;\n \n \tvoid InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) override {\n \t\tchild_column_reader->InitializeRead(columns, protocol_p);\n",
  "test_patch": "diff --git a/data/parquet-testing/struct_skip_test.parquet b/data/parquet-testing/struct_skip_test.parquet\nnew file mode 100644\nindex 000000000000..88988e74f439\nBinary files /dev/null and b/data/parquet-testing/struct_skip_test.parquet differ\ndiff --git a/test/sql/copy/parquet/struct_column_reader_skip.test b/test/sql/copy/parquet/struct_column_reader_skip.test\nnew file mode 100644\nindex 000000000000..f94e4c53d00c\n--- /dev/null\n+++ b/test/sql/copy/parquet/struct_column_reader_skip.test\n@@ -0,0 +1,13 @@\n+# name: test/sql/copy/parquet/struct_column_reader_skip.test\n+# description: Issue #3205: skip in struct column reader\n+# group: [parquet]\n+\n+require parquet\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+query III\n+SELECT my_map['A'], * FROM parquet_scan('data/parquet-testing/struct_skip_test.parquet') where filter == '0'\n+----\n+[NULL]\t{A=NULL}\t0\n",
  "problem_statement": "INTERNAL Error: Skip not implemented for StructColumnReader when filtering for uncommon events in a large dataset\n#### What happens?\r\nWhen using an equals opertator for sparse elements in a sizable dataset that contains a map I notice an error:\r\n```\r\nRuntimeError: INTERNAL Error: Skip not implemented for StructColumnReader\r\n```\r\nAlso worth noting, when the query is changed to instead use an `IN` clause (as opposed to equals) I no longer see the error. \r\n\r\n#### To Reproduce\r\nThis snippet doesn't seem to error if the data set is smaller ie, if you use a range of 1,000 instead of 10,000.\r\n\r\n```\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport duckdb \r\nmaptype = pa.map_(pa.string(), pa.string())\r\n\r\nelems = []\r\nfilter_vals = []\r\nfor i in range(10000):\r\n    key_name = str(chr(i % 26 + 65))\r\n    filter_vals.append(str(i))\r\n    if i % 10 == 0:\r\n        value = None\r\n    else:\r\n        value = 'foo'\r\n    elems.append([(key_name,value)])\r\n\r\nt = pa.Table.from_pydict({'my_map':elems, 'filter': filter_vals}, pa.schema([pa.field('my_map',maptype), pa.field('filter', pa.string())])) \r\npq.write_table(t, 'test')\r\ncursor = duckdb.connect()\r\ncursor.execute(\"SELECT my_map['A'], * FROM parquet_scan('test') where filter == '0'\").fetchall()\r\n```\r\n\r\n#### Environment (please complete the following information):\r\n - OS: iOS (and Ubuntu)\r\n - DuckDB Version: .3.2 (and latest master off of .3.3)\r\n - DuckDB Client: python\r\n\r\n#### Before Submitting\r\n\r\n- [X] **Have you tried this on the latest `master` branch?**\r\n* **Python**: `pip install duckdb --upgrade --pre`\r\n* **R**: `install.packages(\"https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz\", repos = NULL)`\r\n* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.\r\n\r\n- [X] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**\r\n\nINTERNAL Error: Skip not implemented for StructColumnReader when filtering for uncommon events in a large dataset\n#### What happens?\r\nWhen using an equals opertator for sparse elements in a sizable dataset that contains a map I notice an error:\r\n```\r\nRuntimeError: INTERNAL Error: Skip not implemented for StructColumnReader\r\n```\r\nAlso worth noting, when the query is changed to instead use an `IN` clause (as opposed to equals) I no longer see the error. \r\n\r\n#### To Reproduce\r\nThis snippet doesn't seem to error if the data set is smaller ie, if you use a range of 1,000 instead of 10,000.\r\n\r\n```\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport duckdb \r\nmaptype = pa.map_(pa.string(), pa.string())\r\n\r\nelems = []\r\nfilter_vals = []\r\nfor i in range(10000):\r\n    key_name = str(chr(i % 26 + 65))\r\n    filter_vals.append(str(i))\r\n    if i % 10 == 0:\r\n        value = None\r\n    else:\r\n        value = 'foo'\r\n    elems.append([(key_name,value)])\r\n\r\nt = pa.Table.from_pydict({'my_map':elems, 'filter': filter_vals}, pa.schema([pa.field('my_map',maptype), pa.field('filter', pa.string())])) \r\npq.write_table(t, 'test')\r\ncursor = duckdb.connect()\r\ncursor.execute(\"SELECT my_map['A'], * FROM parquet_scan('test') where filter == '0'\").fetchall()\r\n```\r\n\r\n#### Environment (please complete the following information):\r\n - OS: iOS (and Ubuntu)\r\n - DuckDB Version: .3.2 (and latest master off of .3.3)\r\n - DuckDB Client: python\r\n\r\n#### Before Submitting\r\n\r\n- [X] **Have you tried this on the latest `master` branch?**\r\n* **Python**: `pip install duckdb --upgrade --pre`\r\n* **R**: `install.packages(\"https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz\", repos = NULL)`\r\n* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.\r\n\r\n- [X] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**\r\n\n",
  "hints_text": "This should probably be implemented, yes :) Thanks for providing a test case!\nThis should probably be implemented, yes :) Thanks for providing a test case!",
  "created_at": "2022-03-15T09:47:01Z"
}