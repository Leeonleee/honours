You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
create_function -> Invalid Input Error: This function can not be called with an active transaction!, commit or abort the existing one first
### What happens?

  ```
conn = duckdb.connect(database=':memory:')
conn.execute(f"SELECT installed,loaded FROM duckdb_extensions() ;").fetchone()   
conn.create_function("line_3point_min_angle",CheckUdf.line_3point_min_angle )
```
before create_function,fetchone will be error 
`    conn.create_function(attr_name, attr_value,
duckdb.duckdb.InvalidInputException: Invalid Input Error: This function can not be called with an active transaction!, commit or abort the existing one first`

but fetchall()  not error
```
 conn = duckdb.connect(database=':memory:')
conn.execute(f"SELECT installed,loaded FROM duckdb_extensions() ;").fetchall()
conn.create_function("line_3point_min_angle",CheckUdf.line_3point_min_angle )

```



### To Reproduce

```
    conn = duckdb.connect(database=':memory:')
    conn.execute(f"SELECT installed,loaded FROM duckdb_extensions() ;").fetchone()
    conn.create_function(CheckUdf.line_3point_min_angle, "line_3point_min_angle")
```
fetchone() then create_function will be error
```
conn.create_function(attr_name, attr_value,
duckdb.duckdb.InvalidInputException: Invalid Input Error: This function can not be called with an active transaction!, commit or abort the existing one first

```
but fetchall() not error
```
    conn = duckdb.connect(database=':memory:')
    conn.execute(f"SELECT installed,loaded FROM duckdb_extensions() ;").fetchall()
    conn.create_function(CheckUdf.line_3point_min_angle, "line_3point_min_angle")
```

### OS:

MACOS

### DuckDB Version:

1.0.0

### DuckDB Client:

python

### Full Name:

s

### Affiliation:

l

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Not applicable - the reproduction does not require a data set

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://www.duckdb.org/docs/installation) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/include/duckdb/main/client_context.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/main/client_context.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
12: #include "duckdb/catalog/catalog_set.hpp"
13: #include "duckdb/common/atomic.hpp"
14: #include "duckdb/common/deque.hpp"
15: #include "duckdb/common/enums/pending_execution_result.hpp"
16: #include "duckdb/common/enums/prepared_statement_mode.hpp"
17: #include "duckdb/common/error_data.hpp"
18: #include "duckdb/common/pair.hpp"
19: #include "duckdb/common/unordered_set.hpp"
20: #include "duckdb/common/winapi.hpp"
21: #include "duckdb/main/client_config.hpp"
22: #include "duckdb/main/client_context_state.hpp"
23: #include "duckdb/main/client_properties.hpp"
24: #include "duckdb/main/external_dependencies.hpp"
25: #include "duckdb/main/pending_query_result.hpp"
26: #include "duckdb/main/prepared_statement.hpp"
27: #include "duckdb/main/settings.hpp"
28: #include "duckdb/main/stream_query_result.hpp"
29: #include "duckdb/main/table_description.hpp"
30: #include "duckdb/planner/expression/bound_parameter_data.hpp"
31: #include "duckdb/transaction/transaction_context.hpp"
32: 
33: namespace duckdb {
34: class Appender;
35: class Catalog;
36: class CatalogSearchPath;
37: class ColumnDataCollection;
38: class DatabaseInstance;
39: class FileOpener;
40: class LogicalOperator;
41: class PreparedStatementData;
42: class Relation;
43: class BufferedFileWriter;
44: class QueryProfiler;
45: class ClientContextLock;
46: struct CreateScalarFunctionInfo;
47: class ScalarFunctionCatalogEntry;
48: struct ActiveQueryContext;
49: struct ParserOptions;
50: class SimpleBufferedData;
51: class BufferedData;
52: struct ClientData;
53: class ClientContextState;
54: class RegisteredStateManager;
55: 
56: struct PendingQueryParameters {
57: 	//! Prepared statement parameters (if any)
58: 	optional_ptr<case_insensitive_map_t<BoundParameterData>> parameters;
59: 	//! Whether or not a stream result should be allowed
60: 	bool allow_stream_result = false;
61: };
62: 
63: //! The ClientContext holds information relevant to the current client session
64: //! during execution
65: class ClientContext : public enable_shared_from_this<ClientContext> {
66: 	friend class PendingQueryResult;  // LockContext
67: 	friend class BufferedData;        // ExecuteTaskInternal
68: 	friend class SimpleBufferedData;  // ExecuteTaskInternal
69: 	friend class BatchedBufferedData; // ExecuteTaskInternal
70: 	friend class StreamQueryResult;   // LockContext
71: 	friend class ConnectionManager;
72: 
73: public:
74: 	DUCKDB_API explicit ClientContext(shared_ptr<DatabaseInstance> db);
75: 	DUCKDB_API ~ClientContext();
76: 
77: 	//! The database that this client is connected to
78: 	shared_ptr<DatabaseInstance> db;
79: 	//! Whether or not the query is interrupted
80: 	atomic<bool> interrupted;
81: 	//! Set of optional states (e.g. Caches) that can be held by the ClientContext
82: 	unique_ptr<RegisteredStateManager> registered_state;
83: 	//! The client configuration
84: 	ClientConfig config;
85: 	//! The set of client-specific data
86: 	unique_ptr<ClientData> client_data;
87: 	//! Data for the currently running transaction
88: 	TransactionContext transaction;
89: 
90: public:
91: 	MetaTransaction &ActiveTransaction() {
92: 		return transaction.ActiveTransaction();
93: 	}
94: 
95: 	//! Interrupt execution of a query
96: 	DUCKDB_API void Interrupt();
97: 
98: 	//! Enable query profiling
99: 	DUCKDB_API void EnableProfiling();
100: 	//! Disable query profiling
101: 	DUCKDB_API void DisableProfiling();
102: 
103: 	//! Issue a query, returning a QueryResult. The QueryResult can be either a StreamQueryResult or a
104: 	//! MaterializedQueryResult. The StreamQueryResult will only be returned in the case of a successful SELECT
105: 	//! statement.
106: 	DUCKDB_API unique_ptr<QueryResult> Query(const string &query, bool allow_stream_result);
107: 	DUCKDB_API unique_ptr<QueryResult> Query(unique_ptr<SQLStatement> statement, bool allow_stream_result);
108: 
109: 	//! Issues a query to the database and returns a Pending Query Result. Note that "query" may only contain
110: 	//! a single statement.
111: 	DUCKDB_API unique_ptr<PendingQueryResult> PendingQuery(const string &query, bool allow_stream_result);
112: 	//! Issues a query to the database and returns a Pending Query Result
113: 	DUCKDB_API unique_ptr<PendingQueryResult> PendingQuery(unique_ptr<SQLStatement> statement,
114: 	                                                       bool allow_stream_result);
115: 
116: 	//! Destroy the client context
117: 	DUCKDB_API void Destroy();
118: 
119: 	//! Get the table info of a specific table, or nullptr if it cannot be found
120: 	DUCKDB_API unique_ptr<TableDescription> TableInfo(const string &schema_name, const string &table_name);
121: 	//! Appends a DataChunk to the specified table. Returns whether or not the append was successful.
122: 	DUCKDB_API void Append(TableDescription &description, ColumnDataCollection &collection);
123: 	//! Try to bind a relation in the current client context; either throws an exception or fills the result_columns
124: 	//! list with the set of returned columns
125: 	DUCKDB_API void TryBindRelation(Relation &relation, vector<ColumnDefinition> &result_columns);
126: 
127: 	//! Execute a relation
128: 	DUCKDB_API unique_ptr<PendingQueryResult> PendingQuery(const shared_ptr<Relation> &relation,
129: 	                                                       bool allow_stream_result);
130: 	DUCKDB_API unique_ptr<QueryResult> Execute(const shared_ptr<Relation> &relation);
131: 
132: 	//! Prepare a query
133: 	DUCKDB_API unique_ptr<PreparedStatement> Prepare(const string &query);
134: 	//! Directly prepare a SQL statement
135: 	DUCKDB_API unique_ptr<PreparedStatement> Prepare(unique_ptr<SQLStatement> statement);
136: 
137: 	//! Create a pending query result from a prepared statement with the given name and set of parameters
138: 	//! It is possible that the prepared statement will be re-bound. This will generally happen if the catalog is
139: 	//! modified in between the prepared statement being bound and the prepared statement being run.
140: 	DUCKDB_API unique_ptr<PendingQueryResult> PendingQuery(const string &query,
141: 	                                                       shared_ptr<PreparedStatementData> &prepared,
142: 	                                                       const PendingQueryParameters &parameters);
143: 
144: 	//! Execute a prepared statement with the given name and set of parameters
145: 	//! It is possible that the prepared statement will be re-bound. This will generally happen if the catalog is
146: 	//! modified in between the prepared statement being bound and the prepared statement being run.
147: 	DUCKDB_API unique_ptr<QueryResult> Execute(const string &query, shared_ptr<PreparedStatementData> &prepared,
148: 	                                           case_insensitive_map_t<BoundParameterData> &values,
149: 	                                           bool allow_stream_result = true);
150: 	DUCKDB_API unique_ptr<QueryResult> Execute(const string &query, shared_ptr<PreparedStatementData> &prepared,
151: 	                                           const PendingQueryParameters &parameters);
152: 
153: 	//! Gets current percentage of the query's progress, returns 0 in case the progress bar is disabled.
154: 	DUCKDB_API QueryProgress GetQueryProgress();
155: 
156: 	//! Register function in the temporary schema
157: 	DUCKDB_API void RegisterFunction(CreateFunctionInfo &info);
158: 
159: 	//! Parse statements from a query
160: 	DUCKDB_API vector<unique_ptr<SQLStatement>> ParseStatements(const string &query);
161: 
162: 	//! Extract the logical plan of a query
163: 	DUCKDB_API unique_ptr<LogicalOperator> ExtractPlan(const string &query);
164: 	DUCKDB_API void HandlePragmaStatements(vector<unique_ptr<SQLStatement>> &statements);
165: 
166: 	//! Runs a function with a valid transaction context, potentially starting a transaction if the context is in auto
167: 	//! commit mode.
168: 	DUCKDB_API void RunFunctionInTransaction(const std::function<void(void)> &fun,
169: 	                                         bool requires_valid_transaction = true);
170: 	//! Same as RunFunctionInTransaction, but does not obtain a lock on the client context or check for validation
171: 	DUCKDB_API void RunFunctionInTransactionInternal(ClientContextLock &lock, const std::function<void(void)> &fun,
172: 	                                                 bool requires_valid_transaction = true);
173: 
174: 	//! Equivalent to CURRENT_SETTING(key) SQL function.
175: 	DUCKDB_API SettingLookupResult TryGetCurrentSetting(const std::string &key, Value &result) const;
176: 
177: 	//! Returns the parser options for this client context
178: 	DUCKDB_API ParserOptions GetParserOptions() const;
179: 
180: 	//! Whether or not the given result object (streaming query result or pending query result) is active
181: 	DUCKDB_API bool IsActiveResult(ClientContextLock &lock, BaseQueryResult &result);
182: 
183: 	//! Returns the current executor
184: 	Executor &GetExecutor();
185: 
186: 	//! Returns the current query string (if any)
187: 	const string &GetCurrentQuery();
188: 
189: 	//! Fetch a list of table names that are required for a given query
190: 	DUCKDB_API unordered_set<string> GetTableNames(const string &query);
191: 
192: 	DUCKDB_API ClientProperties GetClientProperties() const;
193: 
194: 	//! Returns true if execution of the current query is finished
195: 	DUCKDB_API bool ExecutionIsFinished();
196: 
197: 	//! Process an error for display to the user
198: 	DUCKDB_API void ProcessError(ErrorData &error, const string &query) const;
199: 
200: private:
201: 	//! Parse statements and resolve pragmas from a query
202: 	bool ParseStatements(ClientContextLock &lock, const string &query, vector<unique_ptr<SQLStatement>> &result,
203: 	                     ErrorData &error);
204: 	//! Issues a query to the database and returns a Pending Query Result
205: 	unique_ptr<PendingQueryResult> PendingQueryInternal(ClientContextLock &lock, unique_ptr<SQLStatement> statement,
206: 	                                                    const PendingQueryParameters &parameters, bool verify = true);
207: 	unique_ptr<QueryResult> ExecutePendingQueryInternal(ClientContextLock &lock, PendingQueryResult &query);
208: 
209: 	//! Parse statements from a query
210: 	vector<unique_ptr<SQLStatement>> ParseStatementsInternal(ClientContextLock &lock, const string &query);
211: 	//! Perform aggressive query verification of a SELECT statement. Only called when query_verification_enabled is
212: 	//! true.
213: 	ErrorData VerifyQuery(ClientContextLock &lock, const string &query, unique_ptr<SQLStatement> statement);
214: 
215: 	void InitialCleanup(ClientContextLock &lock);
216: 	//! Internal clean up, does not lock. Caller must hold the context_lock.
217: 	void CleanupInternal(ClientContextLock &lock, BaseQueryResult *result = nullptr,
218: 	                     bool invalidate_transaction = false);
219: 	unique_ptr<PendingQueryResult> PendingStatementOrPreparedStatement(ClientContextLock &lock, const string &query,
220: 	                                                                   unique_ptr<SQLStatement> statement,
221: 	                                                                   shared_ptr<PreparedStatementData> &prepared,
222: 	                                                                   const PendingQueryParameters &parameters);
223: 	unique_ptr<PendingQueryResult> PendingPreparedStatement(ClientContextLock &lock, const string &query,
224: 	                                                        shared_ptr<PreparedStatementData> statement_p,
225: 	                                                        const PendingQueryParameters &parameters);
226: 	unique_ptr<PendingQueryResult> PendingPreparedStatementInternal(ClientContextLock &lock,
227: 	                                                                shared_ptr<PreparedStatementData> statement_p,
228: 	                                                                const PendingQueryParameters &parameters);
229: 	void CheckIfPreparedStatementIsExecutable(PreparedStatementData &statement);
230: 
231: 	//! Internally prepare a SQL statement. Caller must hold the context_lock.
232: 	shared_ptr<PreparedStatementData>
233: 	CreatePreparedStatement(ClientContextLock &lock, const string &query, unique_ptr<SQLStatement> statement,
234: 	                        optional_ptr<case_insensitive_map_t<BoundParameterData>> values = nullptr,
235: 	                        PreparedStatementMode mode = PreparedStatementMode::PREPARE_ONLY);
236: 	unique_ptr<PendingQueryResult> PendingStatementInternal(ClientContextLock &lock, const string &query,
237: 	                                                        unique_ptr<SQLStatement> statement,
238: 	                                                        const PendingQueryParameters &parameters);
239: 	unique_ptr<QueryResult> RunStatementInternal(ClientContextLock &lock, const string &query,
240: 	                                             unique_ptr<SQLStatement> statement, bool allow_stream_result,
241: 	                                             bool verify = true);
242: 	unique_ptr<PreparedStatement> PrepareInternal(ClientContextLock &lock, unique_ptr<SQLStatement> statement);
243: 	void LogQueryInternal(ClientContextLock &lock, const string &query);
244: 
245: 	unique_ptr<QueryResult> FetchResultInternal(ClientContextLock &lock, PendingQueryResult &pending);
246: 
247: 	unique_ptr<ClientContextLock> LockContext();
248: 
249: 	void BeginQueryInternal(ClientContextLock &lock, const string &query);
250: 	ErrorData EndQueryInternal(ClientContextLock &lock, bool success, bool invalidate_transaction,
251: 	                           optional_ptr<ErrorData> previous_error);
252: 
253: 	//! Wait until a task is available to execute
254: 	void WaitForTask(ClientContextLock &lock, BaseQueryResult &result);
255: 	PendingExecutionResult ExecuteTaskInternal(ClientContextLock &lock, BaseQueryResult &result, bool dry_run = false);
256: 
257: 	unique_ptr<PendingQueryResult> PendingStatementOrPreparedStatementInternal(
258: 	    ClientContextLock &lock, const string &query, unique_ptr<SQLStatement> statement,
259: 	    shared_ptr<PreparedStatementData> &prepared, const PendingQueryParameters &parameters);
260: 
261: 	unique_ptr<PendingQueryResult> PendingQueryPreparedInternal(ClientContextLock &lock, const string &query,
262: 	                                                            shared_ptr<PreparedStatementData> &prepared,
263: 	                                                            const PendingQueryParameters &parameters);
264: 
265: 	unique_ptr<PendingQueryResult> PendingQueryInternal(ClientContextLock &, const shared_ptr<Relation> &relation,
266: 	                                                    bool allow_stream_result);
267: 
268: 	void RebindPreparedStatement(ClientContextLock &lock, const string &query,
269: 	                             shared_ptr<PreparedStatementData> &prepared, const PendingQueryParameters &parameters);
270: 
271: 	template <class T>
272: 	unique_ptr<T> ErrorResult(ErrorData error, const string &query = string());
273: 
274: 	shared_ptr<PreparedStatementData>
275: 	CreatePreparedStatementInternal(ClientContextLock &lock, const string &query, unique_ptr<SQLStatement> statement,
276: 	                                optional_ptr<case_insensitive_map_t<BoundParameterData>> values);
277: 
278: private:
279: 	//! Lock on using the ClientContext in parallel
280: 	mutex context_lock;
281: 	//! The currently active query context
282: 	unique_ptr<ActiveQueryContext> active_query;
283: 	//! The current query progress
284: 	QueryProgress query_progress;
285: };
286: 
287: class ClientContextLock {
288: public:
289: 	explicit ClientContextLock(mutex &context_lock) : client_guard(context_lock) {
290: 	}
291: 
292: 	~ClientContextLock() {
293: 	}
294: 
295: private:
296: 	lock_guard<mutex> client_guard;
297: };
298: 
299: } // namespace duckdb
[end of src/include/duckdb/main/client_context.hpp]
[start of src/main/client_context.cpp]
1: #include "duckdb/main/client_context.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/scalar_function_catalog_entry.hpp"
4: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_search_path.hpp"
6: #include "duckdb/common/error_data.hpp"
7: #include "duckdb/common/exception/transaction_exception.hpp"
8: #include "duckdb/common/progress_bar/progress_bar.hpp"
9: #include "duckdb/common/serializer/buffered_file_writer.hpp"
10: #include "duckdb/common/types/column/column_data_collection.hpp"
11: #include "duckdb/execution/column_binding_resolver.hpp"
12: #include "duckdb/execution/operator/helper/physical_result_collector.hpp"
13: #include "duckdb/execution/physical_plan_generator.hpp"
14: #include "duckdb/main/appender.hpp"
15: #include "duckdb/main/attached_database.hpp"
16: #include "duckdb/main/client_context_file_opener.hpp"
17: #include "duckdb/main/client_context_state.hpp"
18: #include "duckdb/main/client_data.hpp"
19: #include "duckdb/main/database.hpp"
20: #include "duckdb/main/database_manager.hpp"
21: #include "duckdb/main/error_manager.hpp"
22: #include "duckdb/main/materialized_query_result.hpp"
23: #include "duckdb/main/query_profiler.hpp"
24: #include "duckdb/main/query_result.hpp"
25: #include "duckdb/main/relation.hpp"
26: #include "duckdb/main/stream_query_result.hpp"
27: #include "duckdb/optimizer/optimizer.hpp"
28: #include "duckdb/parser/expression/constant_expression.hpp"
29: #include "duckdb/parser/expression/parameter_expression.hpp"
30: #include "duckdb/parser/parsed_data/create_function_info.hpp"
31: #include "duckdb/parser/parser.hpp"
32: #include "duckdb/parser/query_node/select_node.hpp"
33: #include "duckdb/parser/statement/drop_statement.hpp"
34: #include "duckdb/parser/statement/execute_statement.hpp"
35: #include "duckdb/parser/statement/explain_statement.hpp"
36: #include "duckdb/parser/statement/prepare_statement.hpp"
37: #include "duckdb/parser/statement/relation_statement.hpp"
38: #include "duckdb/parser/statement/select_statement.hpp"
39: #include "duckdb/planner/operator/logical_execute.hpp"
40: #include "duckdb/planner/planner.hpp"
41: #include "duckdb/planner/pragma_handler.hpp"
42: #include "duckdb/storage/data_table.hpp"
43: #include "duckdb/transaction/meta_transaction.hpp"
44: #include "duckdb/transaction/transaction_manager.hpp"
45: 
46: namespace duckdb {
47: 
48: struct ActiveQueryContext {
49: public:
50: 	//! The query that is currently being executed
51: 	string query;
52: 	//! Prepared statement data
53: 	shared_ptr<PreparedStatementData> prepared;
54: 	//! The query executor
55: 	unique_ptr<Executor> executor;
56: 	//! The progress bar
57: 	unique_ptr<ProgressBar> progress_bar;
58: 
59: public:
60: 	void SetOpenResult(BaseQueryResult &result) {
61: 		open_result = &result;
62: 	}
63: 	bool IsOpenResult(BaseQueryResult &result) {
64: 		return open_result == &result;
65: 	}
66: 	bool HasOpenResult() const {
67: 		return open_result != nullptr;
68: 	}
69: 
70: private:
71: 	//! The currently open result
72: 	BaseQueryResult *open_result = nullptr;
73: };
74: 
75: #ifdef DEBUG
76: struct DebugClientContextState : public ClientContextState {
77: 	~DebugClientContextState() override {
78: 		D_ASSERT(!active_transaction);
79: 		D_ASSERT(!active_query);
80: 	}
81: 
82: 	bool active_transaction = false;
83: 	bool active_query = false;
84: 
85: 	void QueryBegin(ClientContext &context) override {
86: 		if (active_query) {
87: 			throw InternalException("DebugClientContextState::QueryBegin called when a query is already active");
88: 		}
89: 		active_query = true;
90: 	}
91: 	void QueryEnd(ClientContext &context) override {
92: 		if (!active_query) {
93: 			throw InternalException("DebugClientContextState::QueryEnd called when no query is active");
94: 		}
95: 		active_query = false;
96: 	}
97: 	void TransactionBegin(MetaTransaction &transaction, ClientContext &context) override {
98: 		if (active_transaction) {
99: 			throw InternalException(
100: 			    "DebugClientContextState::TransactionBegin called when a transaction is already active");
101: 		}
102: 		active_transaction = true;
103: 	}
104: 	void TransactionCommit(MetaTransaction &transaction, ClientContext &context) override {
105: 		if (!active_transaction) {
106: 			throw InternalException("DebugClientContextState::TransactionCommit called when no transaction is active");
107: 		}
108: 		active_transaction = false;
109: 	}
110: 	void TransactionRollback(MetaTransaction &transaction, ClientContext &context) override {
111: 		if (!active_transaction) {
112: 			throw InternalException(
113: 			    "DebugClientContextState::TransactionRollback called when no transaction is active");
114: 		}
115: 		active_transaction = false;
116: 	}
117: #ifdef DUCKDB_DEBUG_REBIND
118: 	RebindQueryInfo OnPlanningError(ClientContext &context, SQLStatement &statement, ErrorData &error) override {
119: 		return RebindQueryInfo::ATTEMPT_TO_REBIND;
120: 	}
121: 	RebindQueryInfo OnFinalizePrepare(ClientContext &context, PreparedStatementData &prepared,
122: 	                                  PreparedStatementMode mode) override {
123: 		if (mode == PreparedStatementMode::PREPARE_AND_EXECUTE) {
124: 			return RebindQueryInfo::ATTEMPT_TO_REBIND;
125: 		}
126: 		return RebindQueryInfo::DO_NOT_REBIND;
127: 	}
128: 	RebindQueryInfo OnExecutePrepared(ClientContext &context, PreparedStatementCallbackInfo &info,
129: 	                                  RebindQueryInfo current_rebind) override {
130: 		return RebindQueryInfo::ATTEMPT_TO_REBIND;
131: 	}
132: #endif
133: };
134: #endif
135: 
136: ClientContext::ClientContext(shared_ptr<DatabaseInstance> database)
137:     : db(std::move(database)), interrupted(false), client_data(make_uniq<ClientData>(*this)), transaction(*this) {
138: 	registered_state = make_uniq<RegisteredStateManager>();
139: #ifdef DEBUG
140: 	registered_state->GetOrCreate<DebugClientContextState>("debug_client_context_state");
141: #endif
142: }
143: 
144: ClientContext::~ClientContext() {
145: 	if (Exception::UncaughtException()) {
146: 		return;
147: 	}
148: 	// destroy the client context and rollback if there is an active transaction
149: 	// but only if we are not destroying this client context as part of an exception stack unwind
150: 	Destroy();
151: }
152: 
153: unique_ptr<ClientContextLock> ClientContext::LockContext() {
154: 	return make_uniq<ClientContextLock>(context_lock);
155: }
156: 
157: void ClientContext::Destroy() {
158: 	auto lock = LockContext();
159: 	if (transaction.HasActiveTransaction()) {
160: 		transaction.ResetActiveQuery();
161: 		if (!transaction.IsAutoCommit()) {
162: 			transaction.Rollback(nullptr);
163: 		}
164: 	}
165: 	CleanupInternal(*lock);
166: }
167: 
168: void ClientContext::ProcessError(ErrorData &error, const string &query) const {
169: 	if (config.errors_as_json) {
170: 		error.ConvertErrorToJSON();
171: 	} else if (!query.empty()) {
172: 		error.AddErrorLocation(query);
173: 	}
174: }
175: 
176: template <class T>
177: unique_ptr<T> ClientContext::ErrorResult(ErrorData error, const string &query) {
178: 	ProcessError(error, query);
179: 	return make_uniq<T>(std::move(error));
180: }
181: 
182: void ClientContext::BeginQueryInternal(ClientContextLock &lock, const string &query) {
183: 	// check if we are on AutoCommit. In this case we should start a transaction
184: 	D_ASSERT(!active_query);
185: 	auto &db_inst = DatabaseInstance::GetDatabase(*this);
186: 	if (ValidChecker::IsInvalidated(db_inst)) {
187: 		throw ErrorManager::InvalidatedDatabase(*this, ValidChecker::InvalidatedMessage(db_inst));
188: 	}
189: 	active_query = make_uniq<ActiveQueryContext>();
190: 	if (transaction.IsAutoCommit()) {
191: 		transaction.BeginTransaction();
192: 	}
193: 	transaction.SetActiveQuery(db->GetDatabaseManager().GetNewQueryNumber());
194: 	LogQueryInternal(lock, query);
195: 	active_query->query = query;
196: 
197: 	query_progress.Initialize();
198: 	// Notify any registered state of query begin
199: 	for (auto &state : registered_state->States()) {
200: 		state->QueryBegin(*this);
201: 	}
202: }
203: 
204: ErrorData ClientContext::EndQueryInternal(ClientContextLock &lock, bool success, bool invalidate_transaction,
205:                                           optional_ptr<ErrorData> previous_error) {
206: 	client_data->profiler->EndQuery();
207: 
208: 	if (active_query->executor) {
209: 		active_query->executor->CancelTasks();
210: 	}
211: 	active_query->progress_bar.reset();
212: 
213: 	D_ASSERT(active_query.get());
214: 	active_query.reset();
215: 	query_progress.Initialize();
216: 	ErrorData error;
217: 	try {
218: 		if (transaction.HasActiveTransaction()) {
219: 			transaction.ResetActiveQuery();
220: 			if (transaction.IsAutoCommit()) {
221: 				if (success) {
222: 					transaction.Commit();
223: 				} else {
224: 					transaction.Rollback(previous_error);
225: 				}
226: 			} else if (invalidate_transaction) {
227: 				D_ASSERT(!success);
228: 				ValidChecker::Invalidate(ActiveTransaction(), "Failed to commit");
229: 			}
230: 		}
231: 	} catch (std::exception &ex) {
232: 		error = ErrorData(ex);
233: 		if (Exception::InvalidatesDatabase(error.Type())) {
234: 			auto &db_inst = DatabaseInstance::GetDatabase(*this);
235: 			ValidChecker::Invalidate(db_inst, error.RawMessage());
236: 		}
237: 	} catch (...) { // LCOV_EXCL_START
238: 		error = ErrorData("Unhandled exception!");
239: 	} // LCOV_EXCL_STOP
240: 
241: 	// Notify any registered state of query end
242: 	for (auto const &s : registered_state->States()) {
243: 		if (error.HasError()) {
244: 			s->QueryEnd(*this, &error);
245: 		} else {
246: 			s->QueryEnd(*this, previous_error);
247: 		}
248: 	}
249: 
250: 	return error;
251: }
252: 
253: void ClientContext::CleanupInternal(ClientContextLock &lock, BaseQueryResult *result, bool invalidate_transaction) {
254: 	if (!active_query) {
255: 		// no query currently active
256: 		return;
257: 	}
258: 	if (active_query->executor) {
259: 		active_query->executor->CancelTasks();
260: 	}
261: 	active_query->progress_bar.reset();
262: 
263: 	// Relaunch the threads if a SET THREADS command was issued
264: 	auto &scheduler = TaskScheduler::GetScheduler(*this);
265: 	scheduler.RelaunchThreads();
266: 
267: 	optional_ptr<ErrorData> passed_error = nullptr;
268: 	if (result && result->HasError()) {
269: 		passed_error = result->GetErrorObject();
270: 	}
271: 	auto error = EndQueryInternal(lock, result ? !result->HasError() : false, invalidate_transaction, passed_error);
272: 	if (result && !result->HasError()) {
273: 		// if an error occurred while committing report it in the result
274: 		result->SetError(error);
275: 	}
276: 	D_ASSERT(!active_query);
277: }
278: 
279: Executor &ClientContext::GetExecutor() {
280: 	D_ASSERT(active_query);
281: 	D_ASSERT(active_query->executor);
282: 	return *active_query->executor;
283: }
284: 
285: const string &ClientContext::GetCurrentQuery() {
286: 	D_ASSERT(active_query);
287: 	return active_query->query;
288: }
289: 
290: unique_ptr<QueryResult> ClientContext::FetchResultInternal(ClientContextLock &lock, PendingQueryResult &pending) {
291: 	D_ASSERT(active_query);
292: 	D_ASSERT(active_query->IsOpenResult(pending));
293: 	D_ASSERT(active_query->prepared);
294: 	auto &executor = GetExecutor();
295: 	auto &prepared = *active_query->prepared;
296: 	bool create_stream_result = prepared.properties.allow_stream_result && pending.allow_stream_result;
297: 	unique_ptr<QueryResult> result;
298: 	D_ASSERT(executor.HasResultCollector());
299: 	// we have a result collector - fetch the result directly from the result collector
300: 	result = executor.GetResult();
301: 	if (!create_stream_result) {
302: 		CleanupInternal(lock, result.get(), false);
303: 	} else {
304: 		active_query->SetOpenResult(*result);
305: 	}
306: 	return result;
307: }
308: 
309: static bool IsExplainAnalyze(SQLStatement *statement) {
310: 	if (!statement) {
311: 		return false;
312: 	}
313: 	if (statement->type != StatementType::EXPLAIN_STATEMENT) {
314: 		return false;
315: 	}
316: 	auto &explain = statement->Cast<ExplainStatement>();
317: 	return explain.explain_type == ExplainType::EXPLAIN_ANALYZE;
318: }
319: 
320: shared_ptr<PreparedStatementData>
321: ClientContext::CreatePreparedStatementInternal(ClientContextLock &lock, const string &query,
322:                                                unique_ptr<SQLStatement> statement,
323:                                                optional_ptr<case_insensitive_map_t<BoundParameterData>> values) {
324: 	StatementType statement_type = statement->type;
325: 	auto result = make_shared_ptr<PreparedStatementData>(statement_type);
326: 
327: 	auto &profiler = QueryProfiler::Get(*this);
328: 	profiler.StartQuery(query, IsExplainAnalyze(statement.get()), true);
329: 	profiler.StartPhase("planner");
330: 	Planner planner(*this);
331: 	if (values) {
332: 		auto &parameter_values = *values;
333: 		for (auto &value : parameter_values) {
334: 			planner.parameter_data.emplace(value.first, BoundParameterData(value.second));
335: 		}
336: 	}
337: 
338: 	planner.CreatePlan(std::move(statement));
339: 	D_ASSERT(planner.plan || !planner.properties.bound_all_parameters);
340: 	profiler.EndPhase();
341: 
342: 	auto plan = std::move(planner.plan);
343: 	// extract the result column names from the plan
344: 	result->properties = planner.properties;
345: 	result->names = planner.names;
346: 	result->types = planner.types;
347: 	result->value_map = std::move(planner.value_map);
348: 	if (!planner.properties.bound_all_parameters) {
349: 		return result;
350: 	}
351: #ifdef DEBUG
352: 	plan->Verify(*this);
353: #endif
354: 	if (config.enable_optimizer && plan->RequireOptimizer()) {
355: 		profiler.StartPhase("optimizer");
356: 		Optimizer optimizer(*planner.binder, *this);
357: 		plan = optimizer.Optimize(std::move(plan));
358: 		D_ASSERT(plan);
359: 		profiler.EndPhase();
360: 
361: #ifdef DEBUG
362: 		plan->Verify(*this);
363: #endif
364: 	}
365: 
366: 	profiler.StartPhase("physical_planner");
367: 	// now convert logical query plan into a physical query plan
368: 	PhysicalPlanGenerator physical_planner(*this);
369: 	auto physical_plan = physical_planner.CreatePlan(std::move(plan));
370: 	profiler.EndPhase();
371: 
372: #ifdef DEBUG
373: 	D_ASSERT(!physical_plan->ToString().empty());
374: #endif
375: 	result->plan = std::move(physical_plan);
376: 	return result;
377: }
378: 
379: shared_ptr<PreparedStatementData>
380: ClientContext::CreatePreparedStatement(ClientContextLock &lock, const string &query, unique_ptr<SQLStatement> statement,
381:                                        optional_ptr<case_insensitive_map_t<BoundParameterData>> values,
382:                                        PreparedStatementMode mode) {
383: 	// check if any client context state could request a rebind
384: 	bool can_request_rebind = false;
385: 	for (auto &state : registered_state->States()) {
386: 		if (state->CanRequestRebind()) {
387: 			can_request_rebind = true;
388: 		}
389: 	}
390: 	if (can_request_rebind) {
391: 		bool rebind = false;
392: 		// if any registered state can request a rebind we do the binding on a copy first
393: 		shared_ptr<PreparedStatementData> result;
394: 		try {
395: 			result = CreatePreparedStatementInternal(lock, query, statement->Copy(), values);
396: 		} catch (std::exception &ex) {
397: 			ErrorData error(ex);
398: 			// check if any registered client context state wants to try a rebind
399: 			for (auto &state : registered_state->States()) {
400: 				auto info = state->OnPlanningError(*this, *statement, error);
401: 				if (info == RebindQueryInfo::ATTEMPT_TO_REBIND) {
402: 					rebind = true;
403: 				}
404: 			}
405: 			if (!rebind) {
406: 				throw;
407: 			}
408: 		}
409: 		if (result) {
410: 			D_ASSERT(!rebind);
411: 			for (auto &state : registered_state->States()) {
412: 				auto info = state->OnFinalizePrepare(*this, *result, mode);
413: 				if (info == RebindQueryInfo::ATTEMPT_TO_REBIND) {
414: 					rebind = true;
415: 				}
416: 			}
417: 		}
418: 		if (!rebind) {
419: 			return result;
420: 		}
421: 		// an extension wants to do a rebind - do it once
422: 	}
423: 
424: 	return CreatePreparedStatementInternal(lock, query, std::move(statement), values);
425: }
426: 
427: QueryProgress ClientContext::GetQueryProgress() {
428: 	return query_progress;
429: }
430: 
431: void BindPreparedStatementParameters(PreparedStatementData &statement, const PendingQueryParameters &parameters) {
432: 	case_insensitive_map_t<BoundParameterData> owned_values;
433: 	if (parameters.parameters) {
434: 		auto &params = *parameters.parameters;
435: 		for (auto &val : params) {
436: 			owned_values.emplace(val);
437: 		}
438: 	}
439: 	statement.Bind(std::move(owned_values));
440: }
441: 
442: void ClientContext::RebindPreparedStatement(ClientContextLock &lock, const string &query,
443:                                             shared_ptr<PreparedStatementData> &prepared,
444:                                             const PendingQueryParameters &parameters) {
445: 	if (!prepared->unbound_statement) {
446: 		throw InternalException("ClientContext::RebindPreparedStatement called but PreparedStatementData did not have "
447: 		                        "an unbound statement so rebinding cannot be done");
448: 	}
449: 	// catalog was modified: rebind the statement before execution
450: 	auto new_prepared =
451: 	    CreatePreparedStatement(lock, query, prepared->unbound_statement->Copy(), parameters.parameters);
452: 	D_ASSERT(new_prepared->properties.bound_all_parameters);
453: 	prepared = std::move(new_prepared);
454: 	prepared->properties.bound_all_parameters = false;
455: }
456: 
457: void ClientContext::CheckIfPreparedStatementIsExecutable(PreparedStatementData &statement) {
458: 	if (ValidChecker::IsInvalidated(ActiveTransaction()) && statement.properties.requires_valid_transaction) {
459: 		throw ErrorManager::InvalidatedTransaction(*this);
460: 	}
461: 	auto &meta_transaction = MetaTransaction::Get(*this);
462: 	auto &manager = DatabaseManager::Get(*this);
463: 	for (auto &it : statement.properties.modified_databases) {
464: 		auto &modified_database = it.first;
465: 		auto entry = manager.GetDatabase(*this, modified_database);
466: 		if (!entry) {
467: 			throw InternalException("Database \"%s\" not found", modified_database);
468: 		}
469: 		if (entry->IsReadOnly()) {
470: 			throw InvalidInputException(StringUtil::Format(
471: 			    "Cannot execute statement of type \"%s\" on database \"%s\" which is attached in read-only mode!",
472: 			    StatementTypeToString(statement.statement_type), modified_database));
473: 		}
474: 		meta_transaction.ModifyDatabase(*entry);
475: 	}
476: }
477: 
478: unique_ptr<PendingQueryResult>
479: ClientContext::PendingPreparedStatementInternal(ClientContextLock &lock, shared_ptr<PreparedStatementData> statement_p,
480:                                                 const PendingQueryParameters &parameters) {
481: 	D_ASSERT(active_query);
482: 	auto &statement = *statement_p;
483: 
484: 	BindPreparedStatementParameters(statement, parameters);
485: 
486: 	active_query->executor = make_uniq<Executor>(*this);
487: 	auto &executor = *active_query->executor;
488: 	if (config.enable_progress_bar) {
489: 		progress_bar_display_create_func_t display_create_func = nullptr;
490: 		if (config.print_progress_bar) {
491: 			// If a custom display is set, use that, otherwise just use the default
492: 			display_create_func =
493: 			    config.display_create_func ? config.display_create_func : ProgressBar::DefaultProgressBarDisplay;
494: 		}
495: 		active_query->progress_bar =
496: 		    make_uniq<ProgressBar>(executor, NumericCast<idx_t>(config.wait_time), display_create_func);
497: 		active_query->progress_bar->Start();
498: 		query_progress.Restart();
499: 	}
500: 	auto stream_result = parameters.allow_stream_result && statement.properties.allow_stream_result;
501: 
502: 	get_result_collector_t get_method = PhysicalResultCollector::GetResultCollector;
503: 	auto &client_config = ClientConfig::GetConfig(*this);
504: 	if (!stream_result && client_config.result_collector) {
505: 		get_method = client_config.result_collector;
506: 	}
507: 	statement.is_streaming = stream_result;
508: 	auto collector = get_method(*this, statement);
509: 	D_ASSERT(collector->type == PhysicalOperatorType::RESULT_COLLECTOR);
510: 	executor.Initialize(std::move(collector));
511: 
512: 	auto types = executor.GetTypes();
513: 	D_ASSERT(types == statement.types);
514: 	D_ASSERT(!active_query->HasOpenResult());
515: 
516: 	auto pending_result =
517: 	    make_uniq<PendingQueryResult>(shared_from_this(), *statement_p, std::move(types), stream_result);
518: 	active_query->prepared = std::move(statement_p);
519: 	active_query->SetOpenResult(*pending_result);
520: 	return pending_result;
521: }
522: 
523: unique_ptr<PendingQueryResult> ClientContext::PendingPreparedStatement(ClientContextLock &lock, const string &query,
524:                                                                        shared_ptr<PreparedStatementData> prepared,
525:                                                                        const PendingQueryParameters &parameters) {
526: 	CheckIfPreparedStatementIsExecutable(*prepared);
527: 
528: 	RebindQueryInfo rebind = RebindQueryInfo::DO_NOT_REBIND;
529: 	if (prepared->RequireRebind(*this, parameters.parameters)) {
530: 		rebind = RebindQueryInfo::ATTEMPT_TO_REBIND;
531: 	}
532: 
533: 	for (auto &state : registered_state->States()) {
534: 		PreparedStatementCallbackInfo info(*prepared, parameters);
535: 		auto new_rebind = state->OnExecutePrepared(*this, info, rebind);
536: 		if (new_rebind == RebindQueryInfo::ATTEMPT_TO_REBIND) {
537: 			rebind = RebindQueryInfo::ATTEMPT_TO_REBIND;
538: 		}
539: 	}
540: 	if (rebind == RebindQueryInfo::ATTEMPT_TO_REBIND) {
541: 		RebindPreparedStatement(lock, query, prepared, parameters);
542: 	}
543: 	return PendingPreparedStatementInternal(lock, prepared, parameters);
544: }
545: 
546: void ClientContext::WaitForTask(ClientContextLock &lock, BaseQueryResult &result) {
547: 	active_query->executor->WaitForTask();
548: }
549: 
550: PendingExecutionResult ClientContext::ExecuteTaskInternal(ClientContextLock &lock, BaseQueryResult &result,
551:                                                           bool dry_run) {
552: 	D_ASSERT(active_query);
553: 	D_ASSERT(active_query->IsOpenResult(result));
554: 	bool invalidate_transaction = true;
555: 	try {
556: 		auto query_result = active_query->executor->ExecuteTask(dry_run);
557: 		if (active_query->progress_bar) {
558: 			auto is_finished = PendingQueryResult::IsResultReady(query_result);
559: 			active_query->progress_bar->Update(is_finished);
560: 			query_progress = active_query->progress_bar->GetDetailedQueryProgress();
561: 		}
562: 		return query_result;
563: 	} catch (std::exception &ex) {
564: 		auto error = ErrorData(ex);
565: 		if (error.Type() == ExceptionType::INTERRUPT) {
566: 			auto &executor = *active_query->executor;
567: 			if (!executor.HasError()) {
568: 				// Interrupted by the user
569: 				result.SetError(ex);
570: 				invalidate_transaction = true;
571: 			} else {
572: 				// Interrupted by an exception caused in a worker thread
573: 				error = executor.GetError();
574: 				invalidate_transaction = Exception::InvalidatesTransaction(error.Type());
575: 				result.SetError(error);
576: 			}
577: 		} else if (!Exception::InvalidatesTransaction(error.Type())) {
578: 			invalidate_transaction = false;
579: 		} else if (Exception::InvalidatesDatabase(error.Type())) {
580: 			// fatal exceptions invalidate the entire database
581: 			auto &db_instance = DatabaseInstance::GetDatabase(*this);
582: 			ValidChecker::Invalidate(db_instance, error.RawMessage());
583: 		}
584: 		ProcessError(error, active_query->query);
585: 		result.SetError(std::move(error));
586: 	} catch (...) { // LCOV_EXCL_START
587: 		result.SetError(ErrorData("Unhandled exception in ExecuteTaskInternal"));
588: 	} // LCOV_EXCL_STOP
589: 	EndQueryInternal(lock, false, invalidate_transaction, result.GetErrorObject());
590: 	return PendingExecutionResult::EXECUTION_ERROR;
591: }
592: 
593: void ClientContext::InitialCleanup(ClientContextLock &lock) {
594: 	//! Cleanup any open results and reset the interrupted flag
595: 	CleanupInternal(lock);
596: 	interrupted = false;
597: }
598: 
599: vector<unique_ptr<SQLStatement>> ClientContext::ParseStatements(const string &query) {
600: 	auto lock = LockContext();
601: 	return ParseStatementsInternal(*lock, query);
602: }
603: 
604: vector<unique_ptr<SQLStatement>> ClientContext::ParseStatementsInternal(ClientContextLock &lock, const string &query) {
605: 	Parser parser(GetParserOptions());
606: 	parser.ParseQuery(query);
607: 
608: 	PragmaHandler handler(*this);
609: 	handler.HandlePragmaStatements(lock, parser.statements);
610: 
611: 	return std::move(parser.statements);
612: }
613: 
614: void ClientContext::HandlePragmaStatements(vector<unique_ptr<SQLStatement>> &statements) {
615: 	auto lock = LockContext();
616: 
617: 	PragmaHandler handler(*this);
618: 	handler.HandlePragmaStatements(*lock, statements);
619: }
620: 
621: unique_ptr<LogicalOperator> ClientContext::ExtractPlan(const string &query) {
622: 	auto lock = LockContext();
623: 
624: 	auto statements = ParseStatementsInternal(*lock, query);
625: 	if (statements.size() != 1) {
626: 		throw InvalidInputException("ExtractPlan can only prepare a single statement");
627: 	}
628: 
629: 	unique_ptr<LogicalOperator> plan;
630: 	RunFunctionInTransactionInternal(*lock, [&]() {
631: 		Planner planner(*this);
632: 		planner.CreatePlan(std::move(statements[0]));
633: 		D_ASSERT(planner.plan);
634: 
635: 		plan = std::move(planner.plan);
636: 
637: 		if (config.enable_optimizer) {
638: 			Optimizer optimizer(*planner.binder, *this);
639: 			plan = optimizer.Optimize(std::move(plan));
640: 		}
641: 
642: 		ColumnBindingResolver resolver;
643: 		resolver.Verify(*plan);
644: 		resolver.VisitOperator(*plan);
645: 
646: 		plan->ResolveOperatorTypes();
647: 	});
648: 	return plan;
649: }
650: 
651: unique_ptr<PreparedStatement> ClientContext::PrepareInternal(ClientContextLock &lock,
652:                                                              unique_ptr<SQLStatement> statement) {
653: 	auto named_param_map = statement->named_param_map;
654: 	auto statement_query = statement->query;
655: 	shared_ptr<PreparedStatementData> prepared_data;
656: 	auto unbound_statement = statement->Copy();
657: 	RunFunctionInTransactionInternal(
658: 	    lock, [&]() { prepared_data = CreatePreparedStatement(lock, statement_query, std::move(statement)); }, false);
659: 	prepared_data->unbound_statement = std::move(unbound_statement);
660: 	return make_uniq<PreparedStatement>(shared_from_this(), std::move(prepared_data), std::move(statement_query),
661: 	                                    std::move(named_param_map));
662: }
663: 
664: unique_ptr<PreparedStatement> ClientContext::Prepare(unique_ptr<SQLStatement> statement) {
665: 	auto lock = LockContext();
666: 	// prepare the query
667: 	auto query = statement->query;
668: 	try {
669: 		InitialCleanup(*lock);
670: 		return PrepareInternal(*lock, std::move(statement));
671: 	} catch (std::exception &ex) {
672: 		return ErrorResult<PreparedStatement>(ErrorData(ex), query);
673: 	}
674: }
675: 
676: unique_ptr<PreparedStatement> ClientContext::Prepare(const string &query) {
677: 	auto lock = LockContext();
678: 	// prepare the query
679: 	try {
680: 		InitialCleanup(*lock);
681: 
682: 		// first parse the query
683: 		auto statements = ParseStatementsInternal(*lock, query);
684: 		if (statements.empty()) {
685: 			throw InvalidInputException("No statement to prepare!");
686: 		}
687: 		if (statements.size() > 1) {
688: 			throw InvalidInputException("Cannot prepare multiple statements at once!");
689: 		}
690: 		return PrepareInternal(*lock, std::move(statements[0]));
691: 	} catch (std::exception &ex) {
692: 		return ErrorResult<PreparedStatement>(ErrorData(ex), query);
693: 	}
694: }
695: 
696: unique_ptr<PendingQueryResult> ClientContext::PendingQueryPreparedInternal(ClientContextLock &lock, const string &query,
697:                                                                            shared_ptr<PreparedStatementData> &prepared,
698:                                                                            const PendingQueryParameters &parameters) {
699: 	try {
700: 		InitialCleanup(lock);
701: 	} catch (std::exception &ex) {
702: 		return ErrorResult<PendingQueryResult>(ErrorData(ex), query);
703: 	}
704: 	return PendingStatementOrPreparedStatementInternal(lock, query, nullptr, prepared, parameters);
705: }
706: 
707: unique_ptr<PendingQueryResult> ClientContext::PendingQuery(const string &query,
708:                                                            shared_ptr<PreparedStatementData> &prepared,
709:                                                            const PendingQueryParameters &parameters) {
710: 	auto lock = LockContext();
711: 	return PendingQueryPreparedInternal(*lock, query, prepared, parameters);
712: }
713: 
714: unique_ptr<QueryResult> ClientContext::Execute(const string &query, shared_ptr<PreparedStatementData> &prepared,
715:                                                const PendingQueryParameters &parameters) {
716: 	auto lock = LockContext();
717: 	auto pending = PendingQueryPreparedInternal(*lock, query, prepared, parameters);
718: 	if (pending->HasError()) {
719: 		return ErrorResult<MaterializedQueryResult>(pending->GetErrorObject());
720: 	}
721: 	return pending->ExecuteInternal(*lock);
722: }
723: 
724: unique_ptr<QueryResult> ClientContext::Execute(const string &query, shared_ptr<PreparedStatementData> &prepared,
725:                                                case_insensitive_map_t<BoundParameterData> &values,
726:                                                bool allow_stream_result) {
727: 	PendingQueryParameters parameters;
728: 	parameters.parameters = &values;
729: 	parameters.allow_stream_result = allow_stream_result;
730: 	return Execute(query, prepared, parameters);
731: }
732: 
733: unique_ptr<PendingQueryResult> ClientContext::PendingStatementInternal(ClientContextLock &lock, const string &query,
734:                                                                        unique_ptr<SQLStatement> statement,
735:                                                                        const PendingQueryParameters &parameters) {
736: 	// prepare the query for execution
737: 	auto prepared = CreatePreparedStatement(lock, query, std::move(statement), parameters.parameters,
738: 	                                        PreparedStatementMode::PREPARE_AND_EXECUTE);
739: 	idx_t parameter_count = !parameters.parameters ? 0 : parameters.parameters->size();
740: 	if (prepared->properties.parameter_count > 0 && parameter_count == 0) {
741: 		string error_message = StringUtil::Format("Expected %lld parameters, but none were supplied",
742: 		                                          prepared->properties.parameter_count);
743: 		return ErrorResult<PendingQueryResult>(InvalidInputException(error_message), query);
744: 	}
745: 	if (!prepared->properties.bound_all_parameters) {
746: 		return ErrorResult<PendingQueryResult>(InvalidInputException("Not all parameters were bound"), query);
747: 	}
748: 	// execute the prepared statement
749: 	CheckIfPreparedStatementIsExecutable(*prepared);
750: 	return PendingPreparedStatementInternal(lock, std::move(prepared), parameters);
751: }
752: 
753: unique_ptr<QueryResult> ClientContext::RunStatementInternal(ClientContextLock &lock, const string &query,
754:                                                             unique_ptr<SQLStatement> statement,
755:                                                             bool allow_stream_result, bool verify) {
756: 	PendingQueryParameters parameters;
757: 	parameters.allow_stream_result = allow_stream_result;
758: 	auto pending = PendingQueryInternal(lock, std::move(statement), parameters, verify);
759: 	if (pending->HasError()) {
760: 		return ErrorResult<MaterializedQueryResult>(pending->GetErrorObject());
761: 	}
762: 	return ExecutePendingQueryInternal(lock, *pending);
763: }
764: 
765: bool ClientContext::IsActiveResult(ClientContextLock &lock, BaseQueryResult &result) {
766: 	if (!active_query) {
767: 		return false;
768: 	}
769: 	return active_query->IsOpenResult(result);
770: }
771: 
772: unique_ptr<PendingQueryResult> ClientContext::PendingStatementOrPreparedStatementInternal(
773:     ClientContextLock &lock, const string &query, unique_ptr<SQLStatement> statement,
774:     shared_ptr<PreparedStatementData> &prepared, const PendingQueryParameters &parameters) {
775: #ifdef DUCKDB_ALTERNATIVE_VERIFY
776: 	if (statement && statement->type != StatementType::LOGICAL_PLAN_STATEMENT) {
777: 		statement = statement->Copy();
778: 	}
779: #endif
780: 	// check if we are on AutoCommit. In this case we should start a transaction.
781: 	if (statement && config.AnyVerification()) {
782: 		// query verification is enabled
783: 		// create a copy of the statement, and use the copy
784: 		// this way we verify that the copy correctly copies all properties
785: 		auto copied_statement = statement->Copy();
786: 		switch (statement->type) {
787: 		case StatementType::SELECT_STATEMENT: {
788: 			// in case this is a select query, we verify the original statement
789: 			ErrorData error;
790: 			try {
791: 				error = VerifyQuery(lock, query, std::move(statement));
792: 			} catch (std::exception &ex) {
793: 				error = ErrorData(ex);
794: 			}
795: 			if (error.HasError()) {
796: 				// error in verifying query
797: 				return ErrorResult<PendingQueryResult>(std::move(error), query);
798: 			}
799: 			statement = std::move(copied_statement);
800: 			break;
801: 		}
802: 		default: {
803: #ifndef DUCKDB_ALTERNATIVE_VERIFY
804: 			bool reparse_statement = true;
805: #else
806: 			bool reparse_statement = false;
807: #endif
808: 			statement = std::move(copied_statement);
809: 			if (statement->type == StatementType::RELATION_STATEMENT) {
810: 				reparse_statement = false;
811: 			}
812: 			if (reparse_statement) {
813: 				try {
814: 					Parser parser(GetParserOptions());
815: 					ErrorData error;
816: 					parser.ParseQuery(statement->ToString());
817: 					statement = std::move(parser.statements[0]);
818: 				} catch (const NotImplementedException &) {
819: 					// ToString was not implemented, just use the copied statement
820: 				}
821: 			}
822: 			break;
823: 		}
824: 		}
825: 	}
826: 	return PendingStatementOrPreparedStatement(lock, query, std::move(statement), prepared, parameters);
827: }
828: 
829: unique_ptr<PendingQueryResult> ClientContext::PendingStatementOrPreparedStatement(
830:     ClientContextLock &lock, const string &query, unique_ptr<SQLStatement> statement,
831:     shared_ptr<PreparedStatementData> &prepared, const PendingQueryParameters &parameters) {
832: 	unique_ptr<PendingQueryResult> pending;
833: 
834: 	try {
835: 		BeginQueryInternal(lock, query);
836: 	} catch (std::exception &ex) {
837: 		ErrorData error(ex);
838: 		if (Exception::InvalidatesDatabase(error.Type())) {
839: 			// fatal exceptions invalidate the entire database
840: 			auto &db_instance = DatabaseInstance::GetDatabase(*this);
841: 			ValidChecker::Invalidate(db_instance, error.RawMessage());
842: 		}
843: 		return ErrorResult<PendingQueryResult>(std::move(error), query);
844: 	}
845: 	// start the profiler
846: 	auto &profiler = QueryProfiler::Get(*this);
847: 	profiler.StartQuery(query, IsExplainAnalyze(statement ? statement.get() : prepared->unbound_statement.get()));
848: 
849: 	bool invalidate_query = true;
850: 	try {
851: 		if (statement) {
852: 			pending = PendingStatementInternal(lock, query, std::move(statement), parameters);
853: 		} else {
854: 			pending = PendingPreparedStatement(lock, query, prepared, parameters);
855: 		}
856: 	} catch (std::exception &ex) {
857: 		ErrorData error(ex);
858: 		if (!Exception::InvalidatesTransaction(error.Type())) {
859: 			// standard exceptions do not invalidate the current transaction
860: 			invalidate_query = false;
861: 		} else if (Exception::InvalidatesDatabase(error.Type())) {
862: 			// fatal exceptions invalidate the entire database
863: 			if (!config.query_verification_enabled) {
864: 				auto &db_instance = DatabaseInstance::GetDatabase(*this);
865: 				ValidChecker::Invalidate(db_instance, error.RawMessage());
866: 			}
867: 		}
868: 		// other types of exceptions do invalidate the current transaction
869: 		pending = ErrorResult<PendingQueryResult>(std::move(error), query);
870: 	}
871: 	if (pending->HasError()) {
872: 		// query failed: abort now
873: 		EndQueryInternal(lock, false, invalidate_query, pending->GetErrorObject());
874: 		return pending;
875: 	}
876: 	D_ASSERT(active_query->IsOpenResult(*pending));
877: 	return pending;
878: }
879: 
880: void ClientContext::LogQueryInternal(ClientContextLock &, const string &query) {
881: 	if (!client_data->log_query_writer) {
882: #ifdef DUCKDB_FORCE_QUERY_LOG
883: 		try {
884: 			string log_path(DUCKDB_FORCE_QUERY_LOG);
885: 			client_data->log_query_writer =
886: 			    make_uniq<BufferedFileWriter>(FileSystem::GetFileSystem(*this), log_path,
887: 			                                  BufferedFileWriter::DEFAULT_OPEN_FLAGS, client_data->file_opener.get());
888: 		} catch (...) {
889: 			return;
890: 		}
891: #else
892: 		return;
893: #endif
894: 	}
895: 	// log query path is set: log the query
896: 	client_data->log_query_writer->WriteData(const_data_ptr_cast(query.c_str()), query.size());
897: 	client_data->log_query_writer->WriteData(const_data_ptr_cast("\n"), 1);
898: 	client_data->log_query_writer->Flush();
899: 	client_data->log_query_writer->Sync();
900: }
901: 
902: unique_ptr<QueryResult> ClientContext::Query(unique_ptr<SQLStatement> statement, bool allow_stream_result) {
903: 	auto pending_query = PendingQuery(std::move(statement), allow_stream_result);
904: 	if (pending_query->HasError()) {
905: 		return ErrorResult<MaterializedQueryResult>(pending_query->GetErrorObject());
906: 	}
907: 	return pending_query->Execute();
908: }
909: 
910: unique_ptr<QueryResult> ClientContext::Query(const string &query, bool allow_stream_result) {
911: 	auto lock = LockContext();
912: 
913: 	ErrorData error;
914: 	vector<unique_ptr<SQLStatement>> statements;
915: 	if (!ParseStatements(*lock, query, statements, error)) {
916: 		return ErrorResult<MaterializedQueryResult>(std::move(error), query);
917: 	}
918: 	if (statements.empty()) {
919: 		// no statements, return empty successful result
920: 		StatementProperties properties;
921: 		vector<string> names;
922: 		auto collection = make_uniq<ColumnDataCollection>(Allocator::DefaultAllocator());
923: 		return make_uniq<MaterializedQueryResult>(StatementType::INVALID_STATEMENT, properties, std::move(names),
924: 		                                          std::move(collection), GetClientProperties());
925: 	}
926: 
927: 	unique_ptr<QueryResult> result;
928: 	QueryResult *last_result = nullptr;
929: 	bool last_had_result = false;
930: 	for (idx_t i = 0; i < statements.size(); i++) {
931: 		auto &statement = statements[i];
932: 		bool is_last_statement = i + 1 == statements.size();
933: 		PendingQueryParameters parameters;
934: 		parameters.allow_stream_result = allow_stream_result && is_last_statement;
935: 		auto pending_query = PendingQueryInternal(*lock, std::move(statement), parameters);
936: 		auto has_result = pending_query->properties.return_type == StatementReturnType::QUERY_RESULT;
937: 		unique_ptr<QueryResult> current_result;
938: 		if (pending_query->HasError()) {
939: 			current_result = ErrorResult<MaterializedQueryResult>(pending_query->GetErrorObject());
940: 		} else {
941: 			current_result = ExecutePendingQueryInternal(*lock, *pending_query);
942: 		}
943: 		// now append the result to the list of results
944: 		if (!last_result || !last_had_result) {
945: 			// first result of the query
946: 			result = std::move(current_result);
947: 			last_result = result.get();
948: 			last_had_result = has_result;
949: 		} else {
950: 			// later results; attach to the result chain
951: 			// but only if there is a result
952: 			if (!has_result) {
953: 				continue;
954: 			}
955: 			last_result->next = std::move(current_result);
956: 			last_result = last_result->next.get();
957: 		}
958: 		D_ASSERT(last_result);
959: 		if (last_result->HasError()) {
960: 			// Reset the interrupted flag, this was set by the task that found the error
961: 			// Next statements should not be bothered by that interruption
962: 			interrupted = false;
963: 		}
964: 	}
965: 	return result;
966: }
967: 
968: bool ClientContext::ParseStatements(ClientContextLock &lock, const string &query,
969:                                     vector<unique_ptr<SQLStatement>> &result, ErrorData &error) {
970: 	try {
971: 		InitialCleanup(lock);
972: 		// parse the query and transform it into a set of statements
973: 		result = ParseStatementsInternal(lock, query);
974: 		return true;
975: 	} catch (std::exception &ex) {
976: 		error = ErrorData(ex);
977: 		return false;
978: 	}
979: }
980: 
981: unique_ptr<PendingQueryResult> ClientContext::PendingQuery(const string &query, bool allow_stream_result) {
982: 	auto lock = LockContext();
983: 
984: 	ErrorData error;
985: 	vector<unique_ptr<SQLStatement>> statements;
986: 	if (!ParseStatements(*lock, query, statements, error)) {
987: 		return ErrorResult<PendingQueryResult>(std::move(error), query);
988: 	}
989: 	if (statements.size() != 1) {
990: 		return ErrorResult<PendingQueryResult>(ErrorData("PendingQuery can only take a single statement"), query);
991: 	}
992: 	PendingQueryParameters parameters;
993: 	parameters.allow_stream_result = allow_stream_result;
994: 	return PendingQueryInternal(*lock, std::move(statements[0]), parameters);
995: }
996: 
997: unique_ptr<PendingQueryResult> ClientContext::PendingQuery(unique_ptr<SQLStatement> statement,
998:                                                            bool allow_stream_result) {
999: 	auto lock = LockContext();
1000: 
1001: 	try {
1002: 		InitialCleanup(*lock);
1003: 	} catch (std::exception &ex) {
1004: 		return ErrorResult<PendingQueryResult>(ErrorData(ex));
1005: 	}
1006: 
1007: 	PendingQueryParameters parameters;
1008: 	parameters.allow_stream_result = allow_stream_result;
1009: 	return PendingQueryInternal(*lock, std::move(statement), parameters);
1010: }
1011: 
1012: unique_ptr<PendingQueryResult> ClientContext::PendingQueryInternal(ClientContextLock &lock,
1013:                                                                    unique_ptr<SQLStatement> statement,
1014:                                                                    const PendingQueryParameters &parameters,
1015:                                                                    bool verify) {
1016: 	auto query = statement->query;
1017: 	shared_ptr<PreparedStatementData> prepared;
1018: 	if (verify) {
1019: 		return PendingStatementOrPreparedStatementInternal(lock, query, std::move(statement), prepared, parameters);
1020: 	} else {
1021: 		return PendingStatementOrPreparedStatement(lock, query, std::move(statement), prepared, parameters);
1022: 	}
1023: }
1024: 
1025: unique_ptr<QueryResult> ClientContext::ExecutePendingQueryInternal(ClientContextLock &lock, PendingQueryResult &query) {
1026: 	return query.ExecuteInternal(lock);
1027: }
1028: 
1029: void ClientContext::Interrupt() {
1030: 	interrupted = true;
1031: }
1032: 
1033: void ClientContext::EnableProfiling() {
1034: 	auto lock = LockContext();
1035: 	auto &client_config = ClientConfig::GetConfig(*this);
1036: 	client_config.enable_profiler = true;
1037: 	client_config.emit_profiler_output = true;
1038: }
1039: 
1040: void ClientContext::DisableProfiling() {
1041: 	auto lock = LockContext();
1042: 	auto &client_config = ClientConfig::GetConfig(*this);
1043: 	client_config.enable_profiler = false;
1044: }
1045: 
1046: void ClientContext::RegisterFunction(CreateFunctionInfo &info) {
1047: 	RunFunctionInTransaction([&]() {
1048: 		auto existing_function = Catalog::GetEntry<ScalarFunctionCatalogEntry>(*this, INVALID_CATALOG, info.schema,
1049: 		                                                                       info.name, OnEntryNotFound::RETURN_NULL);
1050: 		if (existing_function) {
1051: 			auto &new_info = info.Cast<CreateScalarFunctionInfo>();
1052: 			if (new_info.functions.MergeFunctionSet(existing_function->functions)) {
1053: 				// function info was updated from catalog entry, rewrite is needed
1054: 				info.on_conflict = OnCreateConflict::REPLACE_ON_CONFLICT;
1055: 			}
1056: 		}
1057: 		// create function
1058: 		auto &catalog = Catalog::GetSystemCatalog(*this);
1059: 		catalog.CreateFunction(*this, info);
1060: 	});
1061: }
1062: 
1063: void ClientContext::RunFunctionInTransactionInternal(ClientContextLock &lock, const std::function<void(void)> &fun,
1064:                                                      bool requires_valid_transaction) {
1065: 	if (requires_valid_transaction && transaction.HasActiveTransaction() &&
1066: 	    ValidChecker::IsInvalidated(ActiveTransaction())) {
1067: 		throw TransactionException(ErrorManager::FormatException(*this, ErrorType::INVALIDATED_TRANSACTION));
1068: 	}
1069: 	// check if we are on AutoCommit. In this case we should start a transaction
1070: 	bool require_new_transaction = transaction.IsAutoCommit() && !transaction.HasActiveTransaction();
1071: 	if (require_new_transaction) {
1072: 		D_ASSERT(!active_query);
1073: 		transaction.BeginTransaction();
1074: 	}
1075: 	try {
1076: 		fun();
1077: 	} catch (std::exception &ex) {
1078: 		ErrorData error(ex);
1079: 		bool invalidates_transaction = true;
1080: 		if (!Exception::InvalidatesTransaction(error.Type())) {
1081: 			// standard exceptions don't invalidate the transaction
1082: 			invalidates_transaction = false;
1083: 		} else if (Exception::InvalidatesDatabase(error.Type())) {
1084: 			auto &db_instance = DatabaseInstance::GetDatabase(*this);
1085: 			ValidChecker::Invalidate(db_instance, error.RawMessage());
1086: 		}
1087: 		if (require_new_transaction) {
1088: 			transaction.Rollback(error);
1089: 		} else if (invalidates_transaction) {
1090: 			ValidChecker::Invalidate(ActiveTransaction(), error.RawMessage());
1091: 		}
1092: 		throw;
1093: 	}
1094: 	if (require_new_transaction) {
1095: 		transaction.Commit();
1096: 	}
1097: }
1098: 
1099: void ClientContext::RunFunctionInTransaction(const std::function<void(void)> &fun, bool requires_valid_transaction) {
1100: 	auto lock = LockContext();
1101: 	RunFunctionInTransactionInternal(*lock, fun, requires_valid_transaction);
1102: }
1103: 
1104: unique_ptr<TableDescription> ClientContext::TableInfo(const string &schema_name, const string &table_name) {
1105: 	unique_ptr<TableDescription> result;
1106: 	RunFunctionInTransaction([&]() {
1107: 		// obtain the table info
1108: 		auto table = Catalog::GetEntry<TableCatalogEntry>(*this, INVALID_CATALOG, schema_name, table_name,
1109: 		                                                  OnEntryNotFound::RETURN_NULL);
1110: 		if (!table) {
1111: 			return;
1112: 		}
1113: 		// write the table info to the result
1114: 		result = make_uniq<TableDescription>();
1115: 		result->schema = schema_name;
1116: 		result->table = table_name;
1117: 		for (auto &column : table->GetColumns().Logical()) {
1118: 			result->columns.emplace_back(column.Copy());
1119: 		}
1120: 	});
1121: 	return result;
1122: }
1123: 
1124: void ClientContext::Append(TableDescription &description, ColumnDataCollection &collection) {
1125: 	RunFunctionInTransaction([&]() {
1126: 		auto &table_entry =
1127: 		    Catalog::GetEntry<TableCatalogEntry>(*this, INVALID_CATALOG, description.schema, description.table);
1128: 		// verify that the table columns and types match up
1129: 		if (description.columns.size() != table_entry.GetColumns().PhysicalColumnCount()) {
1130: 			throw InvalidInputException("Failed to append: table entry has different number of columns!");
1131: 		}
1132: 		for (idx_t i = 0; i < description.columns.size(); i++) {
1133: 			if (description.columns[i].Type() != table_entry.GetColumns().GetColumn(PhysicalIndex(i)).Type()) {
1134: 				throw InvalidInputException("Failed to append: table entry has different number of columns!");
1135: 			}
1136: 		}
1137: 		auto binder = Binder::CreateBinder(*this);
1138: 		auto bound_constraints = binder->BindConstraints(table_entry);
1139: 		MetaTransaction::Get(*this).ModifyDatabase(table_entry.ParentCatalog().GetAttached());
1140: 		table_entry.GetStorage().LocalAppend(table_entry, *this, collection, bound_constraints);
1141: 	});
1142: }
1143: 
1144: void ClientContext::TryBindRelation(Relation &relation, vector<ColumnDefinition> &result_columns) {
1145: #ifdef DEBUG
1146: 	D_ASSERT(!relation.GetAlias().empty());
1147: 	D_ASSERT(!relation.ToString().empty());
1148: #endif
1149: 	RunFunctionInTransaction([&]() {
1150: 		// bind the expressions
1151: 		auto binder = Binder::CreateBinder(*this);
1152: 		auto result = relation.Bind(*binder);
1153: 		D_ASSERT(result.names.size() == result.types.size());
1154: 
1155: 		result_columns.reserve(result_columns.size() + result.names.size());
1156: 		for (idx_t i = 0; i < result.names.size(); i++) {
1157: 			result_columns.emplace_back(result.names[i], result.types[i]);
1158: 		}
1159: 	});
1160: }
1161: 
1162: unordered_set<string> ClientContext::GetTableNames(const string &query) {
1163: 	auto lock = LockContext();
1164: 
1165: 	auto statements = ParseStatementsInternal(*lock, query);
1166: 	if (statements.size() != 1) {
1167: 		throw InvalidInputException("Expected a single statement");
1168: 	}
1169: 
1170: 	unordered_set<string> result;
1171: 	RunFunctionInTransactionInternal(*lock, [&]() {
1172: 		// bind the expressions
1173: 		auto binder = Binder::CreateBinder(*this);
1174: 		binder->SetBindingMode(BindingMode::EXTRACT_NAMES);
1175: 		binder->Bind(*statements[0]);
1176: 		result = binder->GetTableNames();
1177: 	});
1178: 	return result;
1179: }
1180: 
1181: unique_ptr<PendingQueryResult> ClientContext::PendingQueryInternal(ClientContextLock &lock,
1182:                                                                    const shared_ptr<Relation> &relation,
1183:                                                                    bool allow_stream_result) {
1184: 	InitialCleanup(lock);
1185: 
1186: 	string query;
1187: 	if (config.query_verification_enabled) {
1188: 		// run the ToString method of any relation we run, mostly to ensure it doesn't crash
1189: 		relation->ToString();
1190: 		relation->GetAlias();
1191: 		if (relation->IsReadOnly()) {
1192: 			// verify read only statements by running a select statement
1193: 			auto select = make_uniq<SelectStatement>();
1194: 			select->node = relation->GetQueryNode();
1195: 			RunStatementInternal(lock, query, std::move(select), false);
1196: 		}
1197: 	}
1198: 
1199: 	auto relation_stmt = make_uniq<RelationStatement>(relation);
1200: 	PendingQueryParameters parameters;
1201: 	parameters.allow_stream_result = allow_stream_result;
1202: 	return PendingQueryInternal(lock, std::move(relation_stmt), parameters);
1203: }
1204: 
1205: unique_ptr<PendingQueryResult> ClientContext::PendingQuery(const shared_ptr<Relation> &relation,
1206:                                                            bool allow_stream_result) {
1207: 	auto lock = LockContext();
1208: 	return PendingQueryInternal(*lock, relation, allow_stream_result);
1209: }
1210: 
1211: unique_ptr<QueryResult> ClientContext::Execute(const shared_ptr<Relation> &relation) {
1212: 	auto lock = LockContext();
1213: 	auto &expected_columns = relation->Columns();
1214: 	auto pending = PendingQueryInternal(*lock, relation, false);
1215: 	if (!pending->success) {
1216: 		return ErrorResult<MaterializedQueryResult>(pending->GetErrorObject());
1217: 	}
1218: 
1219: 	unique_ptr<QueryResult> result;
1220: 	result = ExecutePendingQueryInternal(*lock, *pending);
1221: 	if (result->HasError()) {
1222: 		return result;
1223: 	}
1224: 	// verify that the result types and result names of the query match the expected result types/names
1225: 	if (result->types.size() == expected_columns.size()) {
1226: 		bool mismatch = false;
1227: 		for (idx_t i = 0; i < result->types.size(); i++) {
1228: 			if (result->types[i] != expected_columns[i].Type() || result->names[i] != expected_columns[i].Name()) {
1229: 				mismatch = true;
1230: 				break;
1231: 			}
1232: 		}
1233: 		if (!mismatch) {
1234: 			// all is as expected: return the result
1235: 			return result;
1236: 		}
1237: 	}
1238: 	// result mismatch
1239: 	string err_str = "Result mismatch in query!\nExpected the following columns: [";
1240: 	for (idx_t i = 0; i < expected_columns.size(); i++) {
1241: 		if (i > 0) {
1242: 			err_str += ", ";
1243: 		}
1244: 		err_str += expected_columns[i].Name() + " " + expected_columns[i].Type().ToString();
1245: 	}
1246: 	err_str += "]\nBut result contained the following: ";
1247: 	for (idx_t i = 0; i < result->types.size(); i++) {
1248: 		err_str += i == 0 ? "[" : ", ";
1249: 		err_str += result->names[i] + " " + result->types[i].ToString();
1250: 	}
1251: 	err_str += "]";
1252: 	return ErrorResult<MaterializedQueryResult>(ErrorData(err_str));
1253: }
1254: 
1255: SettingLookupResult ClientContext::TryGetCurrentSetting(const std::string &key, Value &result) const {
1256: 	// first check the built-in settings
1257: 	auto &db_config = DBConfig::GetConfig(*this);
1258: 	auto option = db_config.GetOptionByName(key);
1259: 	if (option) {
1260: 		result = option->get_setting(*this);
1261: 		return SettingLookupResult(SettingScope::LOCAL);
1262: 	}
1263: 
1264: 	// check the client session values
1265: 	const auto &session_config_map = config.set_variables;
1266: 
1267: 	auto session_value = session_config_map.find(key);
1268: 	bool found_session_value = session_value != session_config_map.end();
1269: 	if (found_session_value) {
1270: 		result = session_value->second;
1271: 		return SettingLookupResult(SettingScope::LOCAL);
1272: 	}
1273: 	// finally check the global session values
1274: 	return db->TryGetCurrentSetting(key, result);
1275: }
1276: 
1277: ParserOptions ClientContext::GetParserOptions() const {
1278: 	auto &client_config = ClientConfig::GetConfig(*this);
1279: 	ParserOptions options;
1280: 	options.preserve_identifier_case = client_config.preserve_identifier_case;
1281: 	options.integer_division = client_config.integer_division;
1282: 	options.max_expression_depth = client_config.max_expression_depth;
1283: 	options.extensions = &DBConfig::GetConfig(*this).parser_extensions;
1284: 	return options;
1285: }
1286: 
1287: ClientProperties ClientContext::GetClientProperties() const {
1288: 	string timezone = "UTC";
1289: 	Value result;
1290: 
1291: 	if (TryGetCurrentSetting("TimeZone", result)) {
1292: 		timezone = result.ToString();
1293: 	}
1294: 	return {timezone, db->config.options.arrow_offset_size, db->config.options.arrow_use_list_view,
1295: 	        db->config.options.produce_arrow_string_views};
1296: }
1297: 
1298: bool ClientContext::ExecutionIsFinished() {
1299: 	if (!active_query || !active_query->executor) {
1300: 		return false;
1301: 	}
1302: 	return active_query->executor->ExecutionIsFinished();
1303: }
1304: 
1305: } // namespace duckdb
[end of src/main/client_context.cpp]
[start of tools/pythonpkg/src/pyconnection.cpp]
1: #include "duckdb_python/pyconnection/pyconnection.hpp"
2: 
3: #include "duckdb/catalog/default/default_types.hpp"
4: #include "duckdb/common/arrow/arrow.hpp"
5: #include "duckdb/common/enums/file_compression_type.hpp"
6: #include "duckdb/common/printer.hpp"
7: #include "duckdb/common/types.hpp"
8: #include "duckdb/common/types/vector.hpp"
9: #include "duckdb/function/table/read_csv.hpp"
10: #include "duckdb/main/client_config.hpp"
11: #include "duckdb/main/client_context.hpp"
12: #include "duckdb/main/config.hpp"
13: #include "duckdb/main/db_instance_cache.hpp"
14: #include "duckdb/main/extension_helper.hpp"
15: #include "duckdb/main/prepared_statement.hpp"
16: #include "duckdb/main/relation/read_csv_relation.hpp"
17: #include "duckdb/main/relation/read_json_relation.hpp"
18: #include "duckdb/main/relation/value_relation.hpp"
19: #include "duckdb/main/relation/view_relation.hpp"
20: #include "duckdb/parser/expression/constant_expression.hpp"
21: #include "duckdb/parser/expression/function_expression.hpp"
22: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
23: #include "duckdb/parser/parser.hpp"
24: #include "duckdb/parser/statement/select_statement.hpp"
25: #include "duckdb/parser/tableref/subqueryref.hpp"
26: #include "duckdb/parser/tableref/table_function_ref.hpp"
27: #include "duckdb_python/arrow/arrow_array_stream.hpp"
28: #include "duckdb_python/map.hpp"
29: #include "duckdb_python/pandas/pandas_scan.hpp"
30: #include "duckdb_python/pyrelation.hpp"
31: #include "duckdb_python/pystatement.hpp"
32: #include "duckdb_python/pyresult.hpp"
33: #include "duckdb_python/python_conversion.hpp"
34: #include "duckdb_python/numpy/numpy_type.hpp"
35: #include "duckdb/main/prepared_statement.hpp"
36: #include "duckdb_python/jupyter_progress_bar_display.hpp"
37: #include "duckdb_python/pyfilesystem.hpp"
38: #include "duckdb/main/client_config.hpp"
39: #include "duckdb/function/table/read_csv.hpp"
40: #include "duckdb/common/enums/file_compression_type.hpp"
41: #include "duckdb/catalog/default/default_types.hpp"
42: #include "duckdb/main/relation/value_relation.hpp"
43: #include "duckdb_python/filesystem_object.hpp"
44: #include "duckdb/parser/parsed_data/create_scalar_function_info.hpp"
45: #include "duckdb/function/scalar_function.hpp"
46: #include "duckdb_python/pandas/pandas_scan.hpp"
47: #include "duckdb_python/python_objects.hpp"
48: #include "duckdb/function/function.hpp"
49: #include "duckdb_python/pybind11/conversions/exception_handling_enum.hpp"
50: #include "duckdb/parser/parsed_data/drop_info.hpp"
51: #include "duckdb/catalog/catalog_entry/scalar_function_catalog_entry.hpp"
52: #include "duckdb/main/pending_query_result.hpp"
53: #include "duckdb/parser/keyword_helper.hpp"
54: #include "duckdb_python/python_replacement_scan.hpp"
55: #include "duckdb/common/shared_ptr.hpp"
56: #include "duckdb/main/materialized_query_result.hpp"
57: #include "duckdb/main/stream_query_result.hpp"
58: #include "duckdb/main/relation/materialized_relation.hpp"
59: #include "duckdb/main/relation/query_relation.hpp"
60: #include "duckdb/main/extension_util.hpp"
61: 
62: #include <random>
63: 
64: #include "duckdb/common/printer.hpp"
65: 
66: namespace duckdb {
67: 
68: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::default_connection = nullptr;       // NOLINT: allow global
69: DBInstanceCache instance_cache;                                                        // NOLINT: allow global
70: shared_ptr<PythonImportCache> DuckDBPyConnection::import_cache = nullptr;              // NOLINT: allow global
71: PythonEnvironmentType DuckDBPyConnection::environment = PythonEnvironmentType::NORMAL; // NOLINT: allow global
72: 
73: DuckDBPyConnection::~DuckDBPyConnection() {
74: 	try {
75: 		py::gil_scoped_release gil;
76: 		// Release any structures that do not need to hold the GIL here
77: 		con.SetDatabase(nullptr);
78: 		con.SetConnection(nullptr);
79: 	} catch (...) { // NOLINT
80: 	}
81: }
82: 
83: void DuckDBPyConnection::DetectEnvironment() {
84: 	// If __main__ does not have a __file__ attribute, we are in interactive mode
85: 	auto main_module = py::module_::import("__main__");
86: 	if (py::hasattr(main_module, "__file__")) {
87: 		return;
88: 	}
89: 	DuckDBPyConnection::environment = PythonEnvironmentType::INTERACTIVE;
90: 	if (!ModuleIsLoaded<IpythonCacheItem>()) {
91: 		return;
92: 	}
93: 
94: 	// Check to see if we are in a Jupyter Notebook
95: 	auto &import_cache_py = *DuckDBPyConnection::ImportCache();
96: 	auto get_ipython = import_cache_py.IPython.get_ipython();
97: 	if (get_ipython.ptr() == nullptr) {
98: 		// Could either not load the IPython module, or it has no 'get_ipython' attribute
99: 		return;
100: 	}
101: 	auto ipython = get_ipython();
102: 	if (!py::hasattr(ipython, "config")) {
103: 		return;
104: 	}
105: 	py::dict ipython_config = ipython.attr("config");
106: 	if (ipython_config.contains("IPKernelApp")) {
107: 		DuckDBPyConnection::environment = PythonEnvironmentType::JUPYTER;
108: 	}
109: 	return;
110: }
111: 
112: bool DuckDBPyConnection::DetectAndGetEnvironment() {
113: 	DuckDBPyConnection::DetectEnvironment();
114: 	return DuckDBPyConnection::IsInteractive();
115: }
116: 
117: bool DuckDBPyConnection::IsJupyter() {
118: 	return DuckDBPyConnection::environment == PythonEnvironmentType::JUPYTER;
119: }
120: 
121: // NOTE: this function is generated by tools/pythonpkg/scripts/generate_connection_methods.py.
122: // Do not edit this function manually, your changes will be overwritten!
123: 
124: static void InitializeConnectionMethods(py::class_<DuckDBPyConnection, shared_ptr<DuckDBPyConnection>> &m) {
125: 	m.def("cursor", &DuckDBPyConnection::Cursor, "Create a duplicate of the current connection");
126: 	m.def("register_filesystem", &DuckDBPyConnection::RegisterFilesystem, "Register a fsspec compliant filesystem",
127: 	      py::arg("filesystem"));
128: 	m.def("unregister_filesystem", &DuckDBPyConnection::UnregisterFilesystem, "Unregister a filesystem",
129: 	      py::arg("name"));
130: 	m.def("list_filesystems", &DuckDBPyConnection::ListFilesystems,
131: 	      "List registered filesystems, including builtin ones");
132: 	m.def("filesystem_is_registered", &DuckDBPyConnection::FileSystemIsRegistered,
133: 	      "Check if a filesystem with the provided name is currently registered", py::arg("name"));
134: 	m.def("create_function", &DuckDBPyConnection::RegisterScalarUDF,
135: 	      "Create a DuckDB function out of the passing in Python function so it can be used in queries",
136: 	      py::arg("name"), py::arg("function"), py::arg("parameters") = py::none(), py::arg("return_type") = py::none(),
137: 	      py::kw_only(), py::arg("type") = PythonUDFType::NATIVE,
138: 	      py::arg("null_handling") = FunctionNullHandling::DEFAULT_NULL_HANDLING,
139: 	      py::arg("exception_handling") = PythonExceptionHandling::FORWARD_ERROR, py::arg("side_effects") = false);
140: 	m.def("remove_function", &DuckDBPyConnection::UnregisterUDF, "Remove a previously created function",
141: 	      py::arg("name"));
142: 	m.def("sqltype", &DuckDBPyConnection::Type, "Create a type object by parsing the 'type_str' string",
143: 	      py::arg("type_str"));
144: 	m.def("dtype", &DuckDBPyConnection::Type, "Create a type object by parsing the 'type_str' string",
145: 	      py::arg("type_str"));
146: 	m.def("type", &DuckDBPyConnection::Type, "Create a type object by parsing the 'type_str' string",
147: 	      py::arg("type_str"));
148: 	m.def("array_type", &DuckDBPyConnection::ArrayType, "Create an array type object of 'type'",
149: 	      py::arg("type").none(false), py::arg("size"));
150: 	m.def("list_type", &DuckDBPyConnection::ListType, "Create a list type object of 'type'",
151: 	      py::arg("type").none(false));
152: 	m.def("union_type", &DuckDBPyConnection::UnionType, "Create a union type object from 'members'",
153: 	      py::arg("members").none(false));
154: 	m.def("string_type", &DuckDBPyConnection::StringType, "Create a string type with an optional collation",
155: 	      py::arg("collation") = "");
156: 	m.def("enum_type", &DuckDBPyConnection::EnumType,
157: 	      "Create an enum type of underlying 'type', consisting of the list of 'values'", py::arg("name"),
158: 	      py::arg("type"), py::arg("values"));
159: 	m.def("decimal_type", &DuckDBPyConnection::DecimalType, "Create a decimal type with 'width' and 'scale'",
160: 	      py::arg("width"), py::arg("scale"));
161: 	m.def("struct_type", &DuckDBPyConnection::StructType, "Create a struct type object from 'fields'",
162: 	      py::arg("fields"));
163: 	m.def("row_type", &DuckDBPyConnection::StructType, "Create a struct type object from 'fields'", py::arg("fields"));
164: 	m.def("map_type", &DuckDBPyConnection::MapType, "Create a map type object from 'key_type' and 'value_type'",
165: 	      py::arg("key").none(false), py::arg("value").none(false));
166: 	m.def("duplicate", &DuckDBPyConnection::Cursor, "Create a duplicate of the current connection");
167: 	m.def("execute", &DuckDBPyConnection::Execute,
168: 	      "Execute the given SQL query, optionally using prepared statements with parameters set", py::arg("query"),
169: 	      py::arg("parameters") = py::none());
170: 	m.def("executemany", &DuckDBPyConnection::ExecuteMany,
171: 	      "Execute the given prepared statement multiple times using the list of parameter sets in parameters",
172: 	      py::arg("query"), py::arg("parameters") = py::none());
173: 	m.def("close", &DuckDBPyConnection::Close, "Close the connection");
174: 	m.def("interrupt", &DuckDBPyConnection::Interrupt, "Interrupt pending operations");
175: 	m.def("fetchone", &DuckDBPyConnection::FetchOne, "Fetch a single row from a result following execute");
176: 	m.def("fetchmany", &DuckDBPyConnection::FetchMany, "Fetch the next set of rows from a result following execute",
177: 	      py::arg("size") = 1);
178: 	m.def("fetchall", &DuckDBPyConnection::FetchAll, "Fetch all rows from a result following execute");
179: 	m.def("fetchnumpy", &DuckDBPyConnection::FetchNumpy, "Fetch a result as list of NumPy arrays following execute");
180: 	m.def("fetchdf", &DuckDBPyConnection::FetchDF, "Fetch a result as DataFrame following execute()", py::kw_only(),
181: 	      py::arg("date_as_object") = false);
182: 	m.def("fetch_df", &DuckDBPyConnection::FetchDF, "Fetch a result as DataFrame following execute()", py::kw_only(),
183: 	      py::arg("date_as_object") = false);
184: 	m.def("df", &DuckDBPyConnection::FetchDF, "Fetch a result as DataFrame following execute()", py::kw_only(),
185: 	      py::arg("date_as_object") = false);
186: 	m.def("fetch_df_chunk", &DuckDBPyConnection::FetchDFChunk,
187: 	      "Fetch a chunk of the result as DataFrame following execute()", py::arg("vectors_per_chunk") = 1,
188: 	      py::kw_only(), py::arg("date_as_object") = false);
189: 	m.def("pl", &DuckDBPyConnection::FetchPolars, "Fetch a result as Polars DataFrame following execute()",
190: 	      py::arg("rows_per_batch") = 1000000);
191: 	m.def("fetch_arrow_table", &DuckDBPyConnection::FetchArrow, "Fetch a result as Arrow table following execute()",
192: 	      py::arg("rows_per_batch") = 1000000);
193: 	m.def("arrow", &DuckDBPyConnection::FetchArrow, "Fetch a result as Arrow table following execute()",
194: 	      py::arg("rows_per_batch") = 1000000);
195: 	m.def("fetch_record_batch", &DuckDBPyConnection::FetchRecordBatchReader,
196: 	      "Fetch an Arrow RecordBatchReader following execute()", py::arg("rows_per_batch") = 1000000);
197: 	m.def("torch", &DuckDBPyConnection::FetchPyTorch, "Fetch a result as dict of PyTorch Tensors following execute()");
198: 	m.def("tf", &DuckDBPyConnection::FetchTF, "Fetch a result as dict of TensorFlow Tensors following execute()");
199: 	m.def("begin", &DuckDBPyConnection::Begin, "Start a new transaction");
200: 	m.def("commit", &DuckDBPyConnection::Commit, "Commit changes performed within a transaction");
201: 	m.def("rollback", &DuckDBPyConnection::Rollback, "Roll back changes performed within a transaction");
202: 	m.def("checkpoint", &DuckDBPyConnection::Checkpoint,
203: 	      "Synchronizes data in the write-ahead log (WAL) to the database data file (no-op for in-memory connections)");
204: 	m.def("append", &DuckDBPyConnection::Append, "Append the passed DataFrame to the named table",
205: 	      py::arg("table_name"), py::arg("df"), py::kw_only(), py::arg("by_name") = false);
206: 	m.def("register", &DuckDBPyConnection::RegisterPythonObject,
207: 	      "Register the passed Python Object value for querying with a view", py::arg("view_name"),
208: 	      py::arg("python_object"));
209: 	m.def("unregister", &DuckDBPyConnection::UnregisterPythonObject, "Unregister the view name", py::arg("view_name"));
210: 	m.def("table", &DuckDBPyConnection::Table, "Create a relation object for the named table", py::arg("table_name"));
211: 	m.def("view", &DuckDBPyConnection::View, "Create a relation object for the named view", py::arg("view_name"));
212: 	m.def("values", &DuckDBPyConnection::Values, "Create a relation object from the passed values", py::arg("values"));
213: 	m.def("table_function", &DuckDBPyConnection::TableFunction,
214: 	      "Create a relation object from the named table function with given parameters", py::arg("name"),
215: 	      py::arg("parameters") = py::none());
216: 	m.def("read_json", &DuckDBPyConnection::ReadJSON, "Create a relation object from the JSON file in 'name'",
217: 	      py::arg("path_or_buffer"), py::kw_only(), py::arg("columns") = py::none(),
218: 	      py::arg("sample_size") = py::none(), py::arg("maximum_depth") = py::none(), py::arg("records") = py::none(),
219: 	      py::arg("format") = py::none(), py::arg("date_format") = py::none(), py::arg("timestamp_format") = py::none(),
220: 	      py::arg("compression") = py::none(), py::arg("maximum_object_size") = py::none(),
221: 	      py::arg("ignore_errors") = py::none(), py::arg("convert_strings_to_integers") = py::none(),
222: 	      py::arg("field_appearance_threshold") = py::none(), py::arg("map_inference_threshold") = py::none(),
223: 	      py::arg("maximum_sample_files") = py::none(), py::arg("filename") = py::none(),
224: 	      py::arg("hive_partitioning") = py::none(), py::arg("union_by_name") = py::none(),
225: 	      py::arg("hive_types") = py::none(), py::arg("hive_types_autocast") = py::none());
226: 	m.def("extract_statements", &DuckDBPyConnection::ExtractStatements,
227: 	      "Parse the query string and extract the Statement object(s) produced", py::arg("query"));
228: 	m.def("sql", &DuckDBPyConnection::RunQuery,
229: 	      "Run a SQL query. If it is a SELECT statement, create a relation object from the given SQL query, otherwise "
230: 	      "run the query as-is.",
231: 	      py::arg("query"), py::kw_only(), py::arg("alias") = "", py::arg("params") = py::none());
232: 	m.def("query", &DuckDBPyConnection::RunQuery,
233: 	      "Run a SQL query. If it is a SELECT statement, create a relation object from the given SQL query, otherwise "
234: 	      "run the query as-is.",
235: 	      py::arg("query"), py::kw_only(), py::arg("alias") = "", py::arg("params") = py::none());
236: 	m.def("from_query", &DuckDBPyConnection::RunQuery,
237: 	      "Run a SQL query. If it is a SELECT statement, create a relation object from the given SQL query, otherwise "
238: 	      "run the query as-is.",
239: 	      py::arg("query"), py::kw_only(), py::arg("alias") = "", py::arg("params") = py::none());
240: 	m.def("read_csv", &DuckDBPyConnection::ReadCSV, "Create a relation object from the CSV file in 'name'",
241: 	      py::arg("path_or_buffer"), py::kw_only());
242: 	m.def("from_csv_auto", &DuckDBPyConnection::ReadCSV, "Create a relation object from the CSV file in 'name'",
243: 	      py::arg("path_or_buffer"), py::kw_only());
244: 	m.def("from_df", &DuckDBPyConnection::FromDF, "Create a relation object from the DataFrame in df", py::arg("df"));
245: 	m.def("from_arrow", &DuckDBPyConnection::FromArrow, "Create a relation object from an Arrow object",
246: 	      py::arg("arrow_object"));
247: 	m.def("from_parquet", &DuckDBPyConnection::FromParquet,
248: 	      "Create a relation object from the Parquet files in file_glob", py::arg("file_glob"),
249: 	      py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
250: 	      py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
251: 	      py::arg("compression") = py::none());
252: 	m.def("read_parquet", &DuckDBPyConnection::FromParquet,
253: 	      "Create a relation object from the Parquet files in file_glob", py::arg("file_glob"),
254: 	      py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
255: 	      py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
256: 	      py::arg("compression") = py::none());
257: 	m.def("from_parquet", &DuckDBPyConnection::FromParquets,
258: 	      "Create a relation object from the Parquet files in file_globs", py::arg("file_globs"),
259: 	      py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
260: 	      py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
261: 	      py::arg("compression") = py::none());
262: 	m.def("read_parquet", &DuckDBPyConnection::FromParquets,
263: 	      "Create a relation object from the Parquet files in file_globs", py::arg("file_globs"),
264: 	      py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
265: 	      py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
266: 	      py::arg("compression") = py::none());
267: 	m.def("from_substrait", &DuckDBPyConnection::FromSubstrait, "Create a query object from protobuf plan",
268: 	      py::arg("proto"));
269: 	m.def("get_substrait", &DuckDBPyConnection::GetSubstrait, "Serialize a query to protobuf", py::arg("query"),
270: 	      py::kw_only(), py::arg("enable_optimizer") = true);
271: 	m.def("get_substrait_json", &DuckDBPyConnection::GetSubstraitJSON,
272: 	      "Serialize a query to protobuf on the JSON format", py::arg("query"), py::kw_only(),
273: 	      py::arg("enable_optimizer") = true);
274: 	m.def("from_substrait_json", &DuckDBPyConnection::FromSubstraitJSON,
275: 	      "Create a query object from a JSON protobuf plan", py::arg("json"));
276: 	m.def("get_table_names", &DuckDBPyConnection::GetTableNames, "Extract the required table names from a query",
277: 	      py::arg("query"));
278: 	m.def("install_extension", &DuckDBPyConnection::InstallExtension, "Install an extension by name",
279: 	      py::arg("extension"), py::kw_only(), py::arg("force_install") = false);
280: 	m.def("load_extension", &DuckDBPyConnection::LoadExtension, "Load an installed extension", py::arg("extension"));
281: } // END_OF_CONNECTION_METHODS
282: 
283: void DuckDBPyConnection::UnregisterFilesystem(const py::str &name) {
284: 	auto &database = con.GetDatabase();
285: 	auto &fs = database.GetFileSystem();
286: 
287: 	fs.UnregisterSubSystem(name);
288: }
289: 
290: void DuckDBPyConnection::RegisterFilesystem(AbstractFileSystem filesystem) {
291: 	PythonGILWrapper gil_wrapper;
292: 
293: 	auto &database = con.GetDatabase();
294: 	if (!py::isinstance<AbstractFileSystem>(filesystem)) {
295: 		throw InvalidInputException("Bad filesystem instance");
296: 	}
297: 
298: 	auto &fs = database.GetFileSystem();
299: 
300: 	auto protocol = filesystem.attr("protocol");
301: 	if (protocol.is_none() || py::str("abstract").equal(protocol)) {
302: 		throw InvalidInputException("Must provide concrete fsspec implementation");
303: 	}
304: 
305: 	vector<string> protocols;
306: 	if (py::isinstance<py::str>(protocol)) {
307: 		protocols.push_back(py::str(protocol));
308: 	} else {
309: 		for (const auto &sub_protocol : protocol) {
310: 			protocols.push_back(py::str(sub_protocol));
311: 		}
312: 	}
313: 
314: 	fs.RegisterSubSystem(make_uniq<PythonFilesystem>(std::move(protocols), std::move(filesystem)));
315: }
316: 
317: py::list DuckDBPyConnection::ListFilesystems() {
318: 	auto &database = con.GetDatabase();
319: 	auto subsystems = database.GetFileSystem().ListSubSystems();
320: 	py::list names;
321: 	for (auto &name : subsystems) {
322: 		names.append(py::str(name));
323: 	}
324: 	return names;
325: }
326: 
327: py::list DuckDBPyConnection::ExtractStatements(const string &query) {
328: 	py::list result;
329: 	auto &connection = con.GetConnection();
330: 	auto statements = connection.ExtractStatements(query);
331: 	for (auto &statement : statements) {
332: 		result.append(make_uniq<DuckDBPyStatement>(std::move(statement)));
333: 	}
334: 	return result;
335: }
336: 
337: bool DuckDBPyConnection::FileSystemIsRegistered(const string &name) {
338: 	auto &database = con.GetDatabase();
339: 	auto subsystems = database.GetFileSystem().ListSubSystems();
340: 	return std::find(subsystems.begin(), subsystems.end(), name) != subsystems.end();
341: }
342: 
343: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::UnregisterUDF(const string &name) {
344: 	auto entry = registered_functions.find(name);
345: 	if (entry == registered_functions.end()) {
346: 		// Not registered or already unregistered
347: 		throw InvalidInputException("No function by the name of '%s' was found in the list of registered functions",
348: 		                            name);
349: 	}
350: 
351: 	auto &connection = con.GetConnection();
352: 	auto &context = *connection.context;
353: 
354: 	context.RunFunctionInTransaction([&]() {
355: 		// create function
356: 		auto &catalog = Catalog::GetCatalog(context, SYSTEM_CATALOG);
357: 		DropInfo info;
358: 		info.type = CatalogType::SCALAR_FUNCTION_ENTRY;
359: 		info.name = name;
360: 		info.allow_drop_internal = true;
361: 		info.cascade = false;
362: 		info.if_not_found = OnEntryNotFound::THROW_EXCEPTION;
363: 		catalog.DropEntry(context, info);
364: 	});
365: 	registered_functions.erase(entry);
366: 
367: 	return shared_from_this();
368: }
369: 
370: shared_ptr<DuckDBPyConnection>
371: DuckDBPyConnection::RegisterScalarUDF(const string &name, const py::function &udf, const py::object &parameters_p,
372:                                       const shared_ptr<DuckDBPyType> &return_type_p, PythonUDFType type,
373:                                       FunctionNullHandling null_handling, PythonExceptionHandling exception_handling,
374:                                       bool side_effects) {
375: 	auto &connection = con.GetConnection();
376: 	auto &context = *connection.context;
377: 
378: 	if (context.transaction.HasActiveTransaction()) {
379: 		throw InvalidInputException(
380: 		    "This function can not be called with an active transaction!, commit or abort the existing one first");
381: 	}
382: 	if (registered_functions.find(name) != registered_functions.end()) {
383: 		throw NotImplementedException("A function by the name of '%s' is already created, creating multiple "
384: 		                              "functions with the same name is not supported yet, please remove it first",
385: 		                              name);
386: 	}
387: 	auto scalar_function = CreateScalarUDF(name, udf, parameters_p, return_type_p, type == PythonUDFType::ARROW,
388: 	                                       null_handling, exception_handling, side_effects);
389: 	CreateScalarFunctionInfo info(scalar_function);
390: 
391: 	context.RegisterFunction(info);
392: 
393: 	auto dependency = make_uniq<ExternalDependency>();
394: 	dependency->AddDependency("function", PythonDependencyItem::Create(udf));
395: 	registered_functions[name] = std::move(dependency);
396: 
397: 	return shared_from_this();
398: }
399: 
400: void DuckDBPyConnection::Initialize(py::handle &m) {
401: 	auto connection_module =
402: 	    py::class_<DuckDBPyConnection, shared_ptr<DuckDBPyConnection>>(m, "DuckDBPyConnection", py::module_local());
403: 
404: 	connection_module.def("__enter__", &DuckDBPyConnection::Enter)
405: 	    .def("__exit__", &DuckDBPyConnection::Exit, py::arg("exc_type"), py::arg("exc"), py::arg("traceback"));
406: 	connection_module.def("__del__", &DuckDBPyConnection::Close);
407: 
408: 	InitializeConnectionMethods(connection_module);
409: 	connection_module.def_property_readonly("description", &DuckDBPyConnection::GetDescription,
410: 	                                        "Get result set attributes, mainly column names");
411: 	connection_module.def_property_readonly("rowcount", &DuckDBPyConnection::GetRowcount, "Get result set row count");
412: 	PyDateTime_IMPORT; // NOLINT
413: 	DuckDBPyConnection::ImportCache();
414: }
415: 
416: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::ExecuteMany(const py::object &query, py::object params_p) {
417: 	con.SetResult(nullptr);
418: 	if (params_p.is_none()) {
419: 		params_p = py::list();
420: 	}
421: 
422: 	auto statements = GetStatements(query);
423: 	if (statements.empty()) {
424: 		// TODO: should we throw?
425: 		return nullptr;
426: 	}
427: 
428: 	auto last_statement = std::move(statements.back());
429: 	statements.pop_back();
430: 	// First immediately execute any preceding statements (if any)
431: 	// FIXME: DBAPI says to not accept an 'executemany' call with multiple statements
432: 	ExecuteImmediately(std::move(statements));
433: 
434: 	auto prep = PrepareQuery(std::move(last_statement));
435: 
436: 	if (!py::is_list_like(params_p)) {
437: 		throw InvalidInputException("executemany requires a list of parameter sets to be provided");
438: 	}
439: 	auto outer_list = py::list(params_p);
440: 	if (outer_list.empty()) {
441: 		throw InvalidInputException("executemany requires a non-empty list of parameter sets to be provided");
442: 	}
443: 
444: 	unique_ptr<QueryResult> query_result;
445: 	// Execute once for every set of parameters that are provided
446: 	for (auto &parameters : outer_list) {
447: 		auto params = py::reinterpret_borrow<py::object>(parameters);
448: 		query_result = ExecuteInternal(*prep, std::move(params));
449: 	}
450: 	// Set the internal 'result' object
451: 	if (query_result) {
452: 		auto py_result = make_uniq<DuckDBPyResult>(std::move(query_result));
453: 		con.SetResult(make_uniq<DuckDBPyRelation>(std::move(py_result)));
454: 	}
455: 
456: 	return shared_from_this();
457: }
458: 
459: unique_ptr<QueryResult> DuckDBPyConnection::CompletePendingQuery(PendingQueryResult &pending_query) {
460: 	PendingExecutionResult execution_result;
461: 	while (!PendingQueryResult::IsResultReady(execution_result = pending_query.ExecuteTask())) {
462: 		{
463: 			py::gil_scoped_acquire gil;
464: 			if (PyErr_CheckSignals() != 0) {
465: 				throw std::runtime_error("Query interrupted");
466: 			}
467: 		}
468: 		if (execution_result == PendingExecutionResult::BLOCKED) {
469: 			pending_query.WaitForTask();
470: 		}
471: 	}
472: 	if (execution_result == PendingExecutionResult::EXECUTION_ERROR) {
473: 		pending_query.ThrowError();
474: 	}
475: 	return pending_query.Execute();
476: }
477: 
478: py::list TransformNamedParameters(const case_insensitive_map_t<idx_t> &named_param_map, const py::dict &params) {
479: 	py::list new_params(params.size());
480: 
481: 	for (auto &item : params) {
482: 		const std::string &item_name = item.first.cast<std::string>();
483: 		auto entry = named_param_map.find(item_name);
484: 		if (entry == named_param_map.end()) {
485: 			throw InvalidInputException(
486: 			    "Named parameters could not be transformed, because query string is missing named parameter '%s'",
487: 			    item_name);
488: 		}
489: 		auto param_idx = entry->second;
490: 		// Add the value of the named parameter to the list
491: 		new_params[param_idx - 1] = item.second;
492: 	}
493: 
494: 	if (named_param_map.size() != params.size()) {
495: 		// One or more named parameters were expected, but not found
496: 		vector<string> missing_params;
497: 		missing_params.reserve(named_param_map.size());
498: 		for (auto &entry : named_param_map) {
499: 			auto &name = entry.first;
500: 			if (!params.contains(name)) {
501: 				missing_params.push_back(name);
502: 			}
503: 		}
504: 		auto message = StringUtil::Join(missing_params, ", ");
505: 		throw InvalidInputException("Not all named parameters have been located, missing: %s", message);
506: 	}
507: 
508: 	return new_params;
509: }
510: 
511: case_insensitive_map_t<BoundParameterData> TransformPreparedParameters(PreparedStatement &prep,
512:                                                                        const py::object &params) {
513: 	case_insensitive_map_t<BoundParameterData> named_values;
514: 	if (py::is_list_like(params)) {
515: 		if (prep.named_param_map.size() != py::len(params)) {
516: 			if (py::len(params) == 0) {
517: 				throw InvalidInputException("Expected %d parameters, but none were supplied",
518: 				                            prep.named_param_map.size());
519: 			}
520: 			throw InvalidInputException("Prepared statement needs %d parameters, %d given", prep.named_param_map.size(),
521: 			                            py::len(params));
522: 		}
523: 		auto unnamed_values = DuckDBPyConnection::TransformPythonParamList(params);
524: 		for (idx_t i = 0; i < unnamed_values.size(); i++) {
525: 			auto &value = unnamed_values[i];
526: 			auto identifier = std::to_string(i + 1);
527: 			named_values[identifier] = BoundParameterData(std::move(value));
528: 		}
529: 	} else if (py::is_dict_like(params)) {
530: 		auto dict = py::cast<py::dict>(params);
531: 		named_values = DuckDBPyConnection::TransformPythonParamDict(dict);
532: 	} else {
533: 		throw InvalidInputException("Prepared parameters can only be passed as a list or a dictionary");
534: 	}
535: 	return named_values;
536: }
537: 
538: unique_ptr<PreparedStatement> DuckDBPyConnection::PrepareQuery(unique_ptr<SQLStatement> statement) {
539: 	auto &connection = con.GetConnection();
540: 	unique_ptr<PreparedStatement> prep;
541: 	{
542: 		py::gil_scoped_release release;
543: 		unique_lock<mutex> lock(py_connection_lock);
544: 
545: 		prep = connection.Prepare(std::move(statement));
546: 		if (prep->HasError()) {
547: 			prep->error.Throw();
548: 		}
549: 	}
550: 	return prep;
551: }
552: 
553: unique_ptr<QueryResult> DuckDBPyConnection::ExecuteInternal(PreparedStatement &prep, py::object params) {
554: 	if (params.is_none()) {
555: 		params = py::list();
556: 	}
557: 
558: 	// Execute the prepared statement with the prepared parameters
559: 	auto named_values = TransformPreparedParameters(prep, params);
560: 	unique_ptr<QueryResult> res;
561: 	{
562: 		py::gil_scoped_release release;
563: 		unique_lock<std::mutex> lock(py_connection_lock);
564: 
565: 		auto pending_query = prep.PendingQuery(named_values);
566: 		if (pending_query->HasError()) {
567: 			pending_query->ThrowError();
568: 		}
569: 		res = CompletePendingQuery(*pending_query);
570: 
571: 		if (res->HasError()) {
572: 			res->ThrowError();
573: 		}
574: 	}
575: 	return res;
576: }
577: 
578: vector<unique_ptr<SQLStatement>> DuckDBPyConnection::GetStatements(const py::object &query) {
579: 	vector<unique_ptr<SQLStatement>> result;
580: 	auto &connection = con.GetConnection();
581: 
582: 	shared_ptr<DuckDBPyStatement> statement_obj;
583: 	if (py::try_cast(query, statement_obj)) {
584: 		result.push_back(statement_obj->GetStatement());
585: 		return result;
586: 	}
587: 	if (py::isinstance<py::str>(query)) {
588: 		auto sql_query = std::string(py::str(query));
589: 		return connection.ExtractStatements(sql_query);
590: 	}
591: 	throw InvalidInputException("Please provide either a DuckDBPyStatement or a string representing the query");
592: }
593: 
594: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::ExecuteFromString(const string &query) {
595: 	return Execute(py::str(query));
596: }
597: 
598: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Execute(const py::object &query, py::object params) {
599: 	con.SetResult(nullptr);
600: 
601: 	auto statements = GetStatements(query);
602: 	if (statements.empty()) {
603: 		// TODO: should we throw?
604: 		return nullptr;
605: 	}
606: 
607: 	auto last_statement = std::move(statements.back());
608: 	statements.pop_back();
609: 	// First immediately execute any preceding statements (if any)
610: 	// FIXME: SQLites implementation says to not accept an 'execute' call with multiple statements
611: 	ExecuteImmediately(std::move(statements));
612: 
613: 	auto prep = PrepareQuery(std::move(last_statement));
614: 	auto res = ExecuteInternal(*prep, std::move(params));
615: 
616: 	// Set the internal 'result' object
617: 	if (res) {
618: 		auto py_result = make_uniq<DuckDBPyResult>(std::move(res));
619: 		con.SetResult(make_uniq<DuckDBPyRelation>(std::move(py_result)));
620: 	}
621: 	return shared_from_this();
622: }
623: 
624: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Append(const string &name, const PandasDataFrame &value,
625:                                                           bool by_name) {
626: 	RegisterPythonObject("__append_df", value);
627: 	string columns = "";
628: 	if (by_name) {
629: 		auto df_columns = value.attr("columns");
630: 		vector<string> column_names;
631: 		for (auto &column : df_columns) {
632: 			column_names.push_back(std::string(py::str(column)));
633: 		}
634: 		columns += "(";
635: 		for (idx_t i = 0; i < column_names.size(); i++) {
636: 			auto &column = column_names[i];
637: 			if (i != 0) {
638: 				columns += ", ";
639: 			}
640: 			columns += StringUtil::Format("%s", SQLIdentifier(column));
641: 		}
642: 		columns += ")";
643: 	}
644: 
645: 	auto sql_query = StringUtil::Format("INSERT INTO %s %s SELECT * FROM __append_df", SQLIdentifier(name), columns);
646: 	return Execute(py::str(sql_query));
647: }
648: 
649: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::RegisterPythonObject(const string &name,
650:                                                                         const py::object &python_object) {
651: 	auto &connection = con.GetConnection();
652: 	auto &client = *connection.context;
653: 	auto object = PythonReplacementScan::ReplacementObject(python_object, name, client);
654: 	auto view_rel = make_shared_ptr<ViewRelation>(connection.context, std::move(object), name);
655: 	bool replace = registered_objects.count(name);
656: 	view_rel->CreateView(name, replace, true);
657: 	registered_objects.insert(name);
658: 	return shared_from_this();
659: }
660: 
661: static void ParseMultiFileReaderOptions(named_parameter_map_t &options, const Optional<py::object> &filename,
662:                                         const Optional<py::object> &hive_partitioning,
663:                                         const Optional<py::object> &union_by_name,
664:                                         const Optional<py::object> &hive_types,
665:                                         const Optional<py::object> &hive_types_autocast) {
666: 	if (!py::none().is(filename)) {
667: 		auto val = TransformPythonValue(filename);
668: 		options["filename"] = val;
669: 	}
670: 
671: 	if (!py::none().is(hive_types)) {
672: 		auto val = TransformPythonValue(hive_types);
673: 		options["hive_types"] = val;
674: 	}
675: 
676: 	if (!py::none().is(hive_partitioning)) {
677: 		if (!py::isinstance<py::bool_>(hive_partitioning)) {
678: 			string actual_type = py::str(hive_partitioning.get_type());
679: 			throw BinderException("read_json only accepts 'hive_partitioning' as a boolean, not '%s'", actual_type);
680: 		}
681: 		auto val = TransformPythonValue(hive_partitioning, LogicalTypeId::BOOLEAN);
682: 		options["hive_partitioning"] = val;
683: 	}
684: 
685: 	if (!py::none().is(union_by_name)) {
686: 		if (!py::isinstance<py::bool_>(union_by_name)) {
687: 			string actual_type = py::str(union_by_name.get_type());
688: 			throw BinderException("read_json only accepts 'union_by_name' as a boolean, not '%s'", actual_type);
689: 		}
690: 		auto val = TransformPythonValue(union_by_name, LogicalTypeId::BOOLEAN);
691: 		options["union_by_name"] = val;
692: 	}
693: 
694: 	if (!py::none().is(hive_types_autocast)) {
695: 		if (!py::isinstance<py::bool_>(hive_types_autocast)) {
696: 			string actual_type = py::str(hive_types_autocast.get_type());
697: 			throw BinderException("read_json only accepts 'hive_types_autocast' as a boolean, not '%s'", actual_type);
698: 		}
699: 		auto val = TransformPythonValue(hive_types_autocast, LogicalTypeId::BOOLEAN);
700: 		options["hive_types_autocast"] = val;
701: 	}
702: }
703: 
704: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadJSON(
705:     const py::object &name_p, const Optional<py::object> &columns, const Optional<py::object> &sample_size,
706:     const Optional<py::object> &maximum_depth, const Optional<py::str> &records, const Optional<py::str> &format,
707:     const Optional<py::object> &date_format, const Optional<py::object> &timestamp_format,
708:     const Optional<py::object> &compression, const Optional<py::object> &maximum_object_size,
709:     const Optional<py::object> &ignore_errors, const Optional<py::object> &convert_strings_to_integers,
710:     const Optional<py::object> &field_appearance_threshold, const Optional<py::object> &map_inference_threshold,
711:     const Optional<py::object> &maximum_sample_files, const Optional<py::object> &filename,
712:     const Optional<py::object> &hive_partitioning, const Optional<py::object> &union_by_name,
713:     const Optional<py::object> &hive_types, const Optional<py::object> &hive_types_autocast) {
714: 
715: 	named_parameter_map_t options;
716: 
717: 	auto &connection = con.GetConnection();
718: 	auto path_like = GetPathLike(name_p);
719: 	auto &name = path_like.files;
720: 	auto file_like_object_wrapper = std::move(path_like.dependency);
721: 
722: 	ParseMultiFileReaderOptions(options, filename, hive_partitioning, union_by_name, hive_types, hive_types_autocast);
723: 
724: 	if (!py::none().is(columns)) {
725: 		if (!py::is_dict_like(columns)) {
726: 			throw BinderException("read_json only accepts 'columns' as a dict[str, str]");
727: 		}
728: 		py::dict columns_dict = columns;
729: 		child_list_t<Value> struct_fields;
730: 
731: 		for (auto &kv : columns_dict) {
732: 			auto &column_name = kv.first;
733: 			auto &type = kv.second;
734: 			if (!py::isinstance<py::str>(column_name)) {
735: 				string actual_type = py::str(column_name.get_type());
736: 				throw BinderException("The provided column name must be a str, not of type '%s'", actual_type);
737: 			}
738: 			if (!py::isinstance<py::str>(type)) {
739: 				string actual_type = py::str(column_name.get_type());
740: 				throw BinderException("The provided column type must be a str, not of type '%s'", actual_type);
741: 			}
742: 			struct_fields.emplace_back(py::str(column_name), Value(py::str(type)));
743: 		}
744: 		auto dtype_struct = Value::STRUCT(std::move(struct_fields));
745: 		options["columns"] = std::move(dtype_struct);
746: 	}
747: 
748: 	if (!py::none().is(records)) {
749: 		if (!py::isinstance<py::str>(records)) {
750: 			string actual_type = py::str(records.get_type());
751: 			throw BinderException("read_json only accepts 'records' as a string, not '%s'", actual_type);
752: 		}
753: 		auto records_s = py::reinterpret_borrow<py::str>(records);
754: 		auto records_option = std::string(py::str(records_s));
755: 		options["records"] = Value(records_option);
756: 	}
757: 
758: 	if (!py::none().is(format)) {
759: 		if (!py::isinstance<py::str>(format)) {
760: 			string actual_type = py::str(format.get_type());
761: 			throw BinderException("read_json only accepts 'format' as a string, not '%s'", actual_type);
762: 		}
763: 		auto format_s = py::reinterpret_borrow<py::str>(format);
764: 		auto format_option = std::string(py::str(format_s));
765: 		options["format"] = Value(format_option);
766: 	}
767: 
768: 	if (!py::none().is(date_format)) {
769: 		if (!py::isinstance<py::str>(date_format)) {
770: 			string actual_type = py::str(date_format.get_type());
771: 			throw BinderException("read_json only accepts 'date_format' as a string, not '%s'", actual_type);
772: 		}
773: 		auto date_format_s = py::reinterpret_borrow<py::str>(date_format);
774: 		auto date_format_option = std::string(py::str(date_format_s));
775: 		options["date_format"] = Value(date_format_option);
776: 	}
777: 
778: 	if (!py::none().is(timestamp_format)) {
779: 		if (!py::isinstance<py::str>(timestamp_format)) {
780: 			string actual_type = py::str(timestamp_format.get_type());
781: 			throw BinderException("read_json only accepts 'timestamp_format' as a string, not '%s'", actual_type);
782: 		}
783: 		auto timestamp_format_s = py::reinterpret_borrow<py::str>(timestamp_format);
784: 		auto timestamp_format_option = std::string(py::str(timestamp_format_s));
785: 		options["timestamp_format"] = Value(timestamp_format_option);
786: 	}
787: 
788: 	if (!py::none().is(compression)) {
789: 		if (!py::isinstance<py::str>(compression)) {
790: 			string actual_type = py::str(compression.get_type());
791: 			throw BinderException("read_json only accepts 'compression' as a string, not '%s'", actual_type);
792: 		}
793: 		auto compression_s = py::reinterpret_borrow<py::str>(compression);
794: 		auto compression_option = std::string(py::str(compression_s));
795: 		options["compression"] = Value(compression_option);
796: 	}
797: 
798: 	if (!py::none().is(sample_size)) {
799: 		if (!py::isinstance<py::int_>(sample_size)) {
800: 			string actual_type = py::str(sample_size.get_type());
801: 			throw BinderException("read_json only accepts 'sample_size' as an integer, not '%s'", actual_type);
802: 		}
803: 		options["sample_size"] = Value::INTEGER(py::int_(sample_size));
804: 	}
805: 
806: 	if (!py::none().is(maximum_depth)) {
807: 		if (!py::isinstance<py::int_>(maximum_depth)) {
808: 			string actual_type = py::str(maximum_depth.get_type());
809: 			throw BinderException("read_json only accepts 'maximum_depth' as an integer, not '%s'", actual_type);
810: 		}
811: 		options["maximum_depth"] = Value::INTEGER(py::int_(maximum_depth));
812: 	}
813: 
814: 	if (!py::none().is(maximum_object_size)) {
815: 		if (!py::isinstance<py::int_>(maximum_object_size)) {
816: 			string actual_type = py::str(maximum_object_size.get_type());
817: 			throw BinderException("read_json only accepts 'maximum_object_size' as an unsigned integer, not '%s'",
818: 			                      actual_type);
819: 		}
820: 		auto val = TransformPythonValue(maximum_object_size, LogicalTypeId::UINTEGER);
821: 		options["maximum_object_size"] = val;
822: 	}
823: 
824: 	if (!py::none().is(ignore_errors)) {
825: 		if (!py::isinstance<py::bool_>(ignore_errors)) {
826: 			string actual_type = py::str(ignore_errors.get_type());
827: 			throw BinderException("read_json only accepts 'ignore_errors' as a boolean, not '%s'", actual_type);
828: 		}
829: 		auto val = TransformPythonValue(ignore_errors, LogicalTypeId::BOOLEAN);
830: 		options["ignore_errors"] = val;
831: 	}
832: 
833: 	if (!py::none().is(convert_strings_to_integers)) {
834: 		if (!py::isinstance<py::bool_>(convert_strings_to_integers)) {
835: 			string actual_type = py::str(convert_strings_to_integers.get_type());
836: 			throw BinderException("read_json only accepts 'convert_strings_to_integers' as a boolean, not '%s'",
837: 			                      actual_type);
838: 		}
839: 		auto val = TransformPythonValue(convert_strings_to_integers, LogicalTypeId::BOOLEAN);
840: 		options["convert_strings_to_integers"] = val;
841: 	}
842: 
843: 	if (!py::none().is(field_appearance_threshold)) {
844: 		if (!py::isinstance<py::float_>(field_appearance_threshold)) {
845: 			string actual_type = py::str(field_appearance_threshold.get_type());
846: 			throw BinderException("read_json only accepts 'field_appearance_threshold' as a float, not '%s'",
847: 			                      actual_type);
848: 		}
849: 		auto val = TransformPythonValue(field_appearance_threshold, LogicalTypeId::DOUBLE);
850: 		options["field_appearance_threshold"] = val;
851: 	}
852: 
853: 	if (!py::none().is(map_inference_threshold)) {
854: 		if (!py::isinstance<py::int_>(map_inference_threshold)) {
855: 			string actual_type = py::str(map_inference_threshold.get_type());
856: 			throw BinderException("read_json only accepts 'map_inference_threshold' as an integer, not '%s'",
857: 			                      actual_type);
858: 		}
859: 		auto val = TransformPythonValue(map_inference_threshold, LogicalTypeId::BIGINT);
860: 		options["map_inference_threshold"] = val;
861: 	}
862: 
863: 	if (!py::none().is(maximum_sample_files)) {
864: 		if (!py::isinstance<py::int_>(maximum_sample_files)) {
865: 			string actual_type = py::str(maximum_sample_files.get_type());
866: 			throw BinderException("read_json only accepts 'maximum_sample_files' as an integer, not '%s'", actual_type);
867: 		}
868: 		auto val = TransformPythonValue(maximum_sample_files, LogicalTypeId::BIGINT);
869: 		options["maximum_sample_files"] = val;
870: 	}
871: 
872: 	bool auto_detect = false;
873: 	if (!options.count("columns")) {
874: 		options["auto_detect"] = Value::BOOLEAN(true);
875: 		auto_detect = true;
876: 	}
877: 
878: 	auto read_json_relation =
879: 	    make_shared_ptr<ReadJSONRelation>(connection.context, name, std::move(options), auto_detect);
880: 	if (read_json_relation == nullptr) {
881: 		throw BinderException("read_json can only be used when the JSON extension is (statically) loaded");
882: 	}
883: 	if (file_like_object_wrapper) {
884: 		read_json_relation->AddExternalDependency(std::move(file_like_object_wrapper));
885: 	}
886: 	return make_uniq<DuckDBPyRelation>(std::move(read_json_relation));
887: }
888: 
889: PathLike DuckDBPyConnection::GetPathLike(const py::object &object) {
890: 	return PathLike::Create(object, *this);
891: }
892: 
893: static py::object GetValueOrNone(py::kwargs &kwargs, const string &key) {
894: 	if (kwargs.contains(key)) {
895: 		return kwargs[key.c_str()];
896: 	}
897: 	return py::none();
898: }
899: 
900: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadCSV(const py::object &name_p, py::kwargs &kwargs) {
901: 	auto header = GetValueOrNone(kwargs, "header");
902: 	auto compression = GetValueOrNone(kwargs, "compression");
903: 	auto sep = GetValueOrNone(kwargs, "sep");
904: 	auto delimiter = GetValueOrNone(kwargs, "delimiter");
905: 	auto dtype = GetValueOrNone(kwargs, "dtype");
906: 	auto na_values = GetValueOrNone(kwargs, "na_values");
907: 	auto skiprows = GetValueOrNone(kwargs, "skiprows");
908: 	auto quotechar = GetValueOrNone(kwargs, "quotechar");
909: 	auto escapechar = GetValueOrNone(kwargs, "escapechar");
910: 	auto encoding = GetValueOrNone(kwargs, "encoding");
911: 	auto parallel = GetValueOrNone(kwargs, "parallel");
912: 	auto date_format = GetValueOrNone(kwargs, "date_format");
913: 	auto timestamp_format = GetValueOrNone(kwargs, "timestamp_format");
914: 	auto sample_size = GetValueOrNone(kwargs, "sample_size");
915: 	auto all_varchar = GetValueOrNone(kwargs, "all_varchar");
916: 	auto normalize_names = GetValueOrNone(kwargs, "normalize_names");
917: 	auto null_padding = GetValueOrNone(kwargs, "null_padding");
918: 	auto names_p = GetValueOrNone(kwargs, "names");
919: 	auto lineterminator = GetValueOrNone(kwargs, "lineterminator");
920: 	auto columns = GetValueOrNone(kwargs, "columns");
921: 	auto auto_type_candidates = GetValueOrNone(kwargs, "auto_type_candidates");
922: 	auto max_line_size = GetValueOrNone(kwargs, "max_line_size");
923: 	auto ignore_errors = GetValueOrNone(kwargs, "ignore_errors");
924: 	auto store_rejects = GetValueOrNone(kwargs, "store_rejects");
925: 	auto rejects_table = GetValueOrNone(kwargs, "rejects_table");
926: 	auto rejects_scan = GetValueOrNone(kwargs, "rejects_scan");
927: 	auto rejects_limit = GetValueOrNone(kwargs, "rejects_limit");
928: 	auto force_not_null = GetValueOrNone(kwargs, "force_not_null");
929: 	auto buffer_size = GetValueOrNone(kwargs, "buffer_size");
930: 	auto decimal = GetValueOrNone(kwargs, "decimal");
931: 	auto allow_quoted_nulls = GetValueOrNone(kwargs, "allow_quoted_nulls");
932: 	auto filename = GetValueOrNone(kwargs, "filename");
933: 	auto hive_partitioning = GetValueOrNone(kwargs, "hive_partitioning");
934: 	auto union_by_name = GetValueOrNone(kwargs, "union_by_name");
935: 	auto hive_types = GetValueOrNone(kwargs, "hive_types");
936: 	auto hive_types_autocast = GetValueOrNone(kwargs, "hive_types_autocast");
937: 
938: 	auto &connection = con.GetConnection();
939: 	CSVReaderOptions options;
940: 	auto path_like = GetPathLike(name_p);
941: 	auto &name = path_like.files;
942: 	auto file_like_object_wrapper = std::move(path_like.dependency);
943: 	named_parameter_map_t bind_parameters;
944: 
945: 	ParseMultiFileReaderOptions(bind_parameters, filename, hive_partitioning, union_by_name, hive_types,
946: 	                            hive_types_autocast);
947: 
948: 	// First check if the header is explicitly set
949: 	// when false this affects the returned types, so it needs to be known at initialization of the relation
950: 	if (!py::none().is(header)) {
951: 
952: 		bool header_as_int = py::isinstance<py::int_>(header);
953: 		bool header_as_bool = py::isinstance<py::bool_>(header);
954: 
955: 		bool header_value;
956: 		if (header_as_bool) {
957: 			header_value = py::bool_(header);
958: 		} else if (header_as_int) {
959: 			if ((int)py::int_(header) != 0) {
960: 				throw InvalidInputException("read_csv only accepts 0 if 'header' is given as an integer");
961: 			}
962: 			header_value = true;
963: 		} else {
964: 			throw InvalidInputException("read_csv only accepts 'header' as an integer, or a boolean");
965: 		}
966: 		bind_parameters["header"] = Value::BOOLEAN(header_value);
967: 	}
968: 
969: 	if (!py::none().is(compression)) {
970: 		if (!py::isinstance<py::str>(compression)) {
971: 			throw InvalidInputException("read_csv only accepts 'compression' as a string");
972: 		}
973: 		bind_parameters["compression"] = Value(py::str(compression));
974: 	}
975: 
976: 	if (!py::none().is(dtype)) {
977: 		if (py::is_dict_like(dtype)) {
978: 			child_list_t<Value> struct_fields;
979: 			py::dict dtype_dict = dtype;
980: 			for (auto &kv : dtype_dict) {
981: 				shared_ptr<DuckDBPyType> sql_type;
982: 				if (!py::try_cast(kv.second, sql_type)) {
983: 					throw py::value_error("The types provided to 'dtype' have to be DuckDBPyType");
984: 				}
985: 				struct_fields.emplace_back(py::str(kv.first), Value(sql_type->ToString()));
986: 			}
987: 			auto dtype_struct = Value::STRUCT(std::move(struct_fields));
988: 			bind_parameters["dtypes"] = std::move(dtype_struct);
989: 		} else if (py::is_list_like(dtype)) {
990: 			vector<Value> list_values;
991: 			py::list dtype_list = dtype;
992: 			for (auto &child : dtype_list) {
993: 				shared_ptr<DuckDBPyType> sql_type;
994: 				if (!py::try_cast(child, sql_type)) {
995: 					throw py::value_error("The types provided to 'dtype' have to be DuckDBPyType");
996: 				}
997: 				list_values.push_back(sql_type->ToString());
998: 			}
999: 			bind_parameters["dtypes"] = Value::LIST(LogicalType::VARCHAR, std::move(list_values));
1000: 		} else {
1001: 			throw InvalidInputException("read_csv only accepts 'dtype' as a dictionary or a list of strings");
1002: 		}
1003: 	}
1004: 
1005: 	bool has_sep = !py::none().is(sep);
1006: 	bool has_delimiter = !py::none().is(delimiter);
1007: 	if (has_sep && has_delimiter) {
1008: 		throw InvalidInputException("read_csv takes either 'delimiter' or 'sep', not both");
1009: 	}
1010: 	if (has_sep) {
1011: 		bind_parameters["delim"] = Value(py::str(sep));
1012: 	} else if (has_delimiter) {
1013: 		bind_parameters["delim"] = Value(py::str(delimiter));
1014: 	}
1015: 
1016: 	if (!py::none().is(names_p)) {
1017: 		if (!py::is_list_like(names_p)) {
1018: 			throw InvalidInputException("read_csv only accepts 'names' as a list of strings");
1019: 		}
1020: 		vector<Value> names;
1021: 		py::list names_list = names_p;
1022: 		for (auto &elem : names_list) {
1023: 			if (!py::isinstance<py::str>(elem)) {
1024: 				throw InvalidInputException("read_csv 'names' list has to consist of only strings");
1025: 			}
1026: 			names.push_back(Value(std::string(py::str(elem))));
1027: 		}
1028: 		bind_parameters["names"] = Value::LIST(LogicalType::VARCHAR, std::move(names));
1029: 	}
1030: 
1031: 	if (!py::none().is(na_values)) {
1032: 		vector<Value> null_values;
1033: 		if (!py::isinstance<py::str>(na_values) && !py::is_list_like(na_values)) {
1034: 			throw InvalidInputException("read_csv only accepts 'na_values' as a string or a list of strings");
1035: 		} else if (py::isinstance<py::str>(na_values)) {
1036: 			null_values.push_back(Value(py::str(na_values)));
1037: 		} else {
1038: 			py::list null_list = na_values;
1039: 			for (auto &elem : null_list) {
1040: 				if (!py::isinstance<py::str>(elem)) {
1041: 					throw InvalidInputException("read_csv 'na_values' list has to consist of only strings");
1042: 				}
1043: 				null_values.push_back(Value(std::string(py::str(elem))));
1044: 			}
1045: 		}
1046: 		bind_parameters["nullstr"] = Value::LIST(LogicalType::VARCHAR, std::move(null_values));
1047: 	}
1048: 
1049: 	if (!py::none().is(skiprows)) {
1050: 		if (!py::isinstance<py::int_>(skiprows)) {
1051: 			throw InvalidInputException("read_csv only accepts 'skiprows' as an integer");
1052: 		}
1053: 		bind_parameters["skip"] = Value::INTEGER(py::int_(skiprows));
1054: 	}
1055: 
1056: 	if (!py::none().is(parallel)) {
1057: 		if (!py::isinstance<py::bool_>(parallel)) {
1058: 			throw InvalidInputException("read_csv only accepts 'parallel' as a boolean");
1059: 		}
1060: 		bind_parameters["parallel"] = Value::BOOLEAN(py::bool_(parallel));
1061: 	}
1062: 
1063: 	if (!py::none().is(quotechar)) {
1064: 		if (!py::isinstance<py::str>(quotechar)) {
1065: 			throw InvalidInputException("read_csv only accepts 'quotechar' as a string");
1066: 		}
1067: 		bind_parameters["quote"] = Value(py::str(quotechar));
1068: 	}
1069: 
1070: 	if (!py::none().is(escapechar)) {
1071: 		if (!py::isinstance<py::str>(escapechar)) {
1072: 			throw InvalidInputException("read_csv only accepts 'escapechar' as a string");
1073: 		}
1074: 		bind_parameters["escape"] = Value(py::str(escapechar));
1075: 	}
1076: 
1077: 	if (!py::none().is(encoding)) {
1078: 		if (!py::isinstance<py::str>(encoding)) {
1079: 			throw InvalidInputException("read_csv only accepts 'encoding' as a string");
1080: 		}
1081: 		string encoding_str = StringUtil::Lower(py::str(encoding));
1082: 		if (encoding_str != "utf8" && encoding_str != "utf-8") {
1083: 			throw BinderException("Copy is only supported for UTF-8 encoded files, ENCODING 'UTF-8'");
1084: 		}
1085: 	}
1086: 
1087: 	if (!py::none().is(date_format)) {
1088: 		if (!py::isinstance<py::str>(date_format)) {
1089: 			throw InvalidInputException("read_csv only accepts 'date_format' as a string");
1090: 		}
1091: 		bind_parameters["dateformat"] = Value(py::str(date_format));
1092: 	}
1093: 
1094: 	if (!py::none().is(timestamp_format)) {
1095: 		if (!py::isinstance<py::str>(timestamp_format)) {
1096: 			throw InvalidInputException("read_csv only accepts 'timestamp_format' as a string");
1097: 		}
1098: 		bind_parameters["timestampformat"] = Value(py::str(timestamp_format));
1099: 	}
1100: 
1101: 	if (!py::none().is(sample_size)) {
1102: 		if (!py::isinstance<py::int_>(sample_size)) {
1103: 			throw InvalidInputException("read_csv only accepts 'sample_size' as an integer");
1104: 		}
1105: 		bind_parameters["sample_size"] = Value::INTEGER(py::int_(sample_size));
1106: 	}
1107: 
1108: 	if (!py::none().is(all_varchar)) {
1109: 		if (!py::isinstance<py::bool_>(all_varchar)) {
1110: 			throw InvalidInputException("read_csv only accepts 'all_varchar' as a boolean");
1111: 		}
1112: 		bind_parameters["all_varchar"] = Value::BOOLEAN(py::bool_(all_varchar));
1113: 	}
1114: 
1115: 	if (!py::none().is(normalize_names)) {
1116: 		if (!py::isinstance<py::bool_>(normalize_names)) {
1117: 			throw InvalidInputException("read_csv only accepts 'normalize_names' as a boolean");
1118: 		}
1119: 		bind_parameters["normalize_names"] = Value::BOOLEAN(py::bool_(normalize_names));
1120: 	}
1121: 
1122: 	if (!py::none().is(null_padding)) {
1123: 		if (!py::isinstance<py::bool_>(null_padding)) {
1124: 			throw InvalidInputException("read_csv only accepts 'null_padding' as a boolean");
1125: 		}
1126: 		bind_parameters["null_padding"] = Value::BOOLEAN(py::bool_(null_padding));
1127: 	}
1128: 
1129: 	if (!py::none().is(lineterminator)) {
1130: 		PythonCSVLineTerminator::Type new_line_type;
1131: 		if (!py::try_cast<PythonCSVLineTerminator::Type>(lineterminator, new_line_type)) {
1132: 			string actual_type = py::str(lineterminator.get_type());
1133: 			throw BinderException("read_csv only accepts 'lineterminator' as a string or CSVLineTerminator, not '%s'",
1134: 			                      actual_type);
1135: 		}
1136: 		bind_parameters["new_line"] = Value(PythonCSVLineTerminator::ToString(new_line_type));
1137: 	}
1138: 
1139: 	if (!py::none().is(max_line_size)) {
1140: 		if (!py::isinstance<py::str>(max_line_size) && !py::isinstance<py::int_>(max_line_size)) {
1141: 			string actual_type = py::str(max_line_size.get_type());
1142: 			throw BinderException("read_csv only accepts 'max_line_size' as a string or an integer, not '%s'",
1143: 			                      actual_type);
1144: 		}
1145: 		auto val = TransformPythonValue(max_line_size, LogicalTypeId::VARCHAR);
1146: 		bind_parameters["max_line_size"] = val;
1147: 	}
1148: 
1149: 	if (!py::none().is(auto_type_candidates)) {
1150: 		if (!py::isinstance<py::list>(auto_type_candidates)) {
1151: 			string actual_type = py::str(auto_type_candidates.get_type());
1152: 			throw BinderException("read_csv only accepts 'auto_type_candidates' as a list[str], not '%s'", actual_type);
1153: 		}
1154: 		auto val = TransformPythonValue(auto_type_candidates, LogicalType::LIST(LogicalTypeId::VARCHAR));
1155: 		bind_parameters["auto_type_candidates"] = val;
1156: 	}
1157: 
1158: 	if (!py::none().is(ignore_errors)) {
1159: 		if (!py::isinstance<py::bool_>(ignore_errors)) {
1160: 			string actual_type = py::str(ignore_errors.get_type());
1161: 			throw BinderException("read_csv only accepts 'ignore_errors' as a bool, not '%s'", actual_type);
1162: 		}
1163: 		auto val = TransformPythonValue(ignore_errors, LogicalTypeId::BOOLEAN);
1164: 		bind_parameters["ignore_errors"] = val;
1165: 	}
1166: 
1167: 	if (!py::none().is(store_rejects)) {
1168: 		if (!py::isinstance<py::bool_>(store_rejects)) {
1169: 			string actual_type = py::str(store_rejects.get_type());
1170: 			throw BinderException("read_csv only accepts 'store_rejects' as a bool, not '%s'", actual_type);
1171: 		}
1172: 		auto val = TransformPythonValue(store_rejects, LogicalTypeId::BOOLEAN);
1173: 		bind_parameters["store_rejects"] = val;
1174: 	}
1175: 
1176: 	if (!py::none().is(rejects_table)) {
1177: 		if (!py::isinstance<py::str>(rejects_table)) {
1178: 			string actual_type = py::str(rejects_table.get_type());
1179: 			throw BinderException("read_csv only accepts 'rejects_table' as a string, not '%s'", actual_type);
1180: 		}
1181: 		auto val = TransformPythonValue(rejects_table, LogicalTypeId::VARCHAR);
1182: 		bind_parameters["rejects_table"] = val;
1183: 	}
1184: 
1185: 	if (!py::none().is(rejects_scan)) {
1186: 		if (!py::isinstance<py::str>(rejects_scan)) {
1187: 			string actual_type = py::str(rejects_scan.get_type());
1188: 			throw BinderException("read_csv only accepts 'rejects_scan' as a string, not '%s'", actual_type);
1189: 		}
1190: 		auto val = TransformPythonValue(rejects_scan, LogicalTypeId::VARCHAR);
1191: 		bind_parameters["rejects_scan"] = val;
1192: 	}
1193: 
1194: 	if (!py::none().is(rejects_limit)) {
1195: 		if (!py::isinstance<py::int_>(rejects_limit)) {
1196: 			string actual_type = py::str(rejects_limit.get_type());
1197: 			throw BinderException("read_csv only accepts 'rejects_limit' as an int, not '%s'", actual_type);
1198: 		}
1199: 		auto val = TransformPythonValue(rejects_limit, LogicalTypeId::BIGINT);
1200: 		bind_parameters["rejects_limit"] = val;
1201: 	}
1202: 
1203: 	if (!py::none().is(force_not_null)) {
1204: 		if (!py::isinstance<py::list>(force_not_null)) {
1205: 			string actual_type = py::str(force_not_null.get_type());
1206: 			throw BinderException("read_csv only accepts 'force_not_null' as a list[str], not '%s'", actual_type);
1207: 		}
1208: 		auto val = TransformPythonValue(force_not_null, LogicalType::LIST(LogicalTypeId::VARCHAR));
1209: 		bind_parameters["force_not_null"] = val;
1210: 	}
1211: 
1212: 	if (!py::none().is(buffer_size)) {
1213: 		if (!py::isinstance<py::int_>(buffer_size)) {
1214: 			string actual_type = py::str(buffer_size.get_type());
1215: 			throw BinderException("read_csv only accepts 'buffer_size' as a list[str], not '%s'", actual_type);
1216: 		}
1217: 		auto val = TransformPythonValue(buffer_size, LogicalTypeId::UBIGINT);
1218: 		bind_parameters["buffer_size"] = val;
1219: 	}
1220: 
1221: 	if (!py::none().is(decimal)) {
1222: 		if (!py::isinstance<py::str>(decimal)) {
1223: 			string actual_type = py::str(decimal.get_type());
1224: 			throw BinderException("read_csv only accepts 'decimal' as a string, not '%s'", actual_type);
1225: 		}
1226: 		auto val = TransformPythonValue(decimal, LogicalTypeId::VARCHAR);
1227: 		bind_parameters["decimal_separator"] = val;
1228: 	}
1229: 
1230: 	if (!py::none().is(allow_quoted_nulls)) {
1231: 		if (!py::isinstance<py::bool_>(allow_quoted_nulls)) {
1232: 			string actual_type = py::str(allow_quoted_nulls.get_type());
1233: 			throw BinderException("read_csv only accepts 'allow_quoted_nulls' as a bool, not '%s'", actual_type);
1234: 		}
1235: 		auto val = TransformPythonValue(allow_quoted_nulls, LogicalTypeId::BOOLEAN);
1236: 		bind_parameters["allow_quoted_nulls"] = val;
1237: 	}
1238: 
1239: 	if (!py::none().is(columns)) {
1240: 		if (!py::is_dict_like(columns)) {
1241: 			throw BinderException("read_csv only accepts 'columns' as a dict[str, str]");
1242: 		}
1243: 		py::dict columns_dict = columns;
1244: 		child_list_t<Value> struct_fields;
1245: 
1246: 		for (auto &kv : columns_dict) {
1247: 			auto &column_name = kv.first;
1248: 			auto &type = kv.second;
1249: 			if (!py::isinstance<py::str>(column_name)) {
1250: 				string actual_type = py::str(column_name.get_type());
1251: 				throw BinderException("The provided column name must be a str, not of type '%s'", actual_type);
1252: 			}
1253: 			if (!py::isinstance<py::str>(type)) {
1254: 				string actual_type = py::str(column_name.get_type());
1255: 				throw BinderException("The provided column type must be a str, not of type '%s'", actual_type);
1256: 			}
1257: 			struct_fields.emplace_back(py::str(column_name), Value(py::str(type)));
1258: 		}
1259: 		auto dtype_struct = Value::STRUCT(std::move(struct_fields));
1260: 		bind_parameters["columns"] = std::move(dtype_struct);
1261: 	}
1262: 
1263: 	// Create the ReadCSV Relation using the 'options'
1264: 
1265: 	auto read_csv_p = connection.ReadCSV(name, std::move(bind_parameters));
1266: 	auto &read_csv = read_csv_p->Cast<ReadCSVRelation>();
1267: 	if (file_like_object_wrapper) {
1268: 		read_csv.AddExternalDependency(std::move(file_like_object_wrapper));
1269: 	}
1270: 
1271: 	return make_uniq<DuckDBPyRelation>(read_csv_p->Alias(read_csv.alias));
1272: }
1273: 
1274: void DuckDBPyConnection::ExecuteImmediately(vector<unique_ptr<SQLStatement>> statements) {
1275: 	auto &connection = con.GetConnection();
1276: 	if (statements.empty()) {
1277: 		return;
1278: 	}
1279: 	for (auto &stmt : statements) {
1280: 		if (!stmt->named_param_map.empty()) {
1281: 			throw NotImplementedException(
1282: 			    "Prepared parameters are only supported for the last statement, please split your query up into "
1283: 			    "separate 'execute' calls if you want to use prepared parameters");
1284: 		}
1285: 		auto pending_query = connection.PendingQuery(std::move(stmt), false);
1286: 		if (pending_query->HasError()) {
1287: 			pending_query->ThrowError();
1288: 		}
1289: 		auto res = CompletePendingQuery(*pending_query);
1290: 
1291: 		if (res->HasError()) {
1292: 			res->ThrowError();
1293: 		}
1294: 	}
1295: }
1296: 
1297: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::RunQuery(const py::object &query, string alias, py::object params) {
1298: 	auto &connection = con.GetConnection();
1299: 	if (alias.empty()) {
1300: 		alias = "unnamed_relation_" + StringUtil::GenerateRandomName(16);
1301: 	}
1302: 
1303: 	auto statements = GetStatements(query);
1304: 	if (statements.empty()) {
1305: 		// TODO: should we throw?
1306: 		return nullptr;
1307: 	}
1308: 
1309: 	auto last_statement = std::move(statements.back());
1310: 	statements.pop_back();
1311: 	// First immediately execute any preceding statements (if any)
1312: 	ExecuteImmediately(std::move(statements));
1313: 
1314: 	// Attempt to create a Relation for lazy execution if possible
1315: 	shared_ptr<Relation> relation;
1316: 	if (py::none().is(params)) {
1317: 		// FIXME: currently we can't create relations with prepared parameters
1318: 		auto statement_type = last_statement->type;
1319: 		switch (statement_type) {
1320: 		case StatementType::SELECT_STATEMENT: {
1321: 			auto select_statement = unique_ptr_cast<SQLStatement, SelectStatement>(std::move(last_statement));
1322: 			relation = connection.RelationFromQuery(std::move(select_statement), alias);
1323: 			break;
1324: 		}
1325: 		default:
1326: 			break;
1327: 		}
1328: 	}
1329: 
1330: 	if (!relation) {
1331: 		// Could not create a relation, resort to direct execution
1332: 		auto prep = PrepareQuery(std::move(last_statement));
1333: 		auto res = ExecuteInternal(*prep, std::move(params));
1334: 		if (!res) {
1335: 			return nullptr;
1336: 		}
1337: 		if (res->properties.return_type != StatementReturnType::QUERY_RESULT) {
1338: 			return nullptr;
1339: 		}
1340: 		if (res->type == QueryResultType::STREAM_RESULT) {
1341: 			auto &stream_result = res->Cast<StreamQueryResult>();
1342: 			res = stream_result.Materialize();
1343: 		}
1344: 		auto &materialized_result = res->Cast<MaterializedQueryResult>();
1345: 		relation = make_shared_ptr<MaterializedRelation>(connection.context, materialized_result.TakeCollection(),
1346: 		                                                 res->names, alias);
1347: 	}
1348: 	return make_uniq<DuckDBPyRelation>(std::move(relation));
1349: }
1350: 
1351: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::Table(const string &tname) {
1352: 	auto &connection = con.GetConnection();
1353: 	auto qualified_name = QualifiedName::Parse(tname);
1354: 	if (qualified_name.schema.empty()) {
1355: 		qualified_name.schema = DEFAULT_SCHEMA;
1356: 	}
1357: 	try {
1358: 		return make_uniq<DuckDBPyRelation>(connection.Table(qualified_name.schema, qualified_name.name));
1359: 	} catch (const CatalogException &) {
1360: 		// CatalogException will be of the type '... is not a table'
1361: 		// Not a table in the database, make a query relation that can perform replacement scans
1362: 		auto sql_query = StringUtil::Format("from %s", KeywordHelper::WriteOptionallyQuoted(tname));
1363: 		return RunQuery(py::str(sql_query), tname);
1364: 	}
1365: }
1366: 
1367: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::Values(py::object params) {
1368: 	auto &connection = con.GetConnection();
1369: 	if (params.is_none()) {
1370: 		params = py::list();
1371: 	}
1372: 	if (!py::hasattr(params, "__len__")) {
1373: 		throw InvalidInputException("Type of object passed to parameter 'values' must be iterable");
1374: 	}
1375: 	vector<vector<Value>> values {DuckDBPyConnection::TransformPythonParamList(params)};
1376: 	return make_uniq<DuckDBPyRelation>(connection.Values(values));
1377: }
1378: 
1379: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::View(const string &vname) {
1380: 	auto &connection = con.GetConnection();
1381: 	return make_uniq<DuckDBPyRelation>(connection.View(vname));
1382: }
1383: 
1384: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::TableFunction(const string &fname, py::object params) {
1385: 	auto &connection = con.GetConnection();
1386: 	if (params.is_none()) {
1387: 		params = py::list();
1388: 	}
1389: 	if (!py::is_list_like(params)) {
1390: 		throw InvalidInputException("'params' has to be a list of parameters");
1391: 	}
1392: 
1393: 	return make_uniq<DuckDBPyRelation>(
1394: 	    connection.TableFunction(fname, DuckDBPyConnection::TransformPythonParamList(params)));
1395: }
1396: 
1397: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromDF(const PandasDataFrame &value) {
1398: 	auto &connection = con.GetConnection();
1399: 	string name = "df_" + StringUtil::GenerateRandomName();
1400: 	if (PandasDataFrame::IsPyArrowBacked(value)) {
1401: 		auto table = PandasDataFrame::ToArrowTable(value);
1402: 		return DuckDBPyConnection::FromArrow(table);
1403: 	}
1404: 	auto tableref = PythonReplacementScan::ReplacementObject(value, name, *connection.context);
1405: 	D_ASSERT(tableref);
1406: 	auto rel = make_shared_ptr<ViewRelation>(connection.context, std::move(tableref), name);
1407: 	return make_uniq<DuckDBPyRelation>(std::move(rel));
1408: }
1409: 
1410: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquet(const string &file_glob, bool binary_as_string,
1411:                                                              bool file_row_number, bool filename,
1412:                                                              bool hive_partitioning, bool union_by_name,
1413:                                                              const py::object &compression) {
1414: 	auto &connection = con.GetConnection();
1415: 	string name = "parquet_" + StringUtil::GenerateRandomName();
1416: 	vector<Value> params;
1417: 	params.emplace_back(file_glob);
1418: 	named_parameter_map_t named_parameters({{"binary_as_string", Value::BOOLEAN(binary_as_string)},
1419: 	                                        {"file_row_number", Value::BOOLEAN(file_row_number)},
1420: 	                                        {"filename", Value::BOOLEAN(filename)},
1421: 	                                        {"hive_partitioning", Value::BOOLEAN(hive_partitioning)},
1422: 	                                        {"union_by_name", Value::BOOLEAN(union_by_name)}});
1423: 
1424: 	if (!py::none().is(compression)) {
1425: 		if (!py::isinstance<py::str>(compression)) {
1426: 			throw InvalidInputException("from_parquet only accepts 'compression' as a string");
1427: 		}
1428: 		named_parameters["compression"] = Value(py::str(compression));
1429: 	}
1430: 	return make_uniq<DuckDBPyRelation>(connection.TableFunction("parquet_scan", params, named_parameters)->Alias(name));
1431: }
1432: 
1433: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquets(const vector<string> &file_globs, bool binary_as_string,
1434:                                                               bool file_row_number, bool filename,
1435:                                                               bool hive_partitioning, bool union_by_name,
1436:                                                               const py::object &compression) {
1437: 	auto &connection = con.GetConnection();
1438: 	string name = "parquet_" + StringUtil::GenerateRandomName();
1439: 	vector<Value> params;
1440: 	auto file_globs_as_value = vector<Value>();
1441: 	for (const auto &file : file_globs) {
1442: 		file_globs_as_value.emplace_back(file);
1443: 	}
1444: 	params.emplace_back(Value::LIST(file_globs_as_value));
1445: 	named_parameter_map_t named_parameters({{"binary_as_string", Value::BOOLEAN(binary_as_string)},
1446: 	                                        {"file_row_number", Value::BOOLEAN(file_row_number)},
1447: 	                                        {"filename", Value::BOOLEAN(filename)},
1448: 	                                        {"hive_partitioning", Value::BOOLEAN(hive_partitioning)},
1449: 	                                        {"union_by_name", Value::BOOLEAN(union_by_name)}});
1450: 
1451: 	if (!py::none().is(compression)) {
1452: 		if (!py::isinstance<py::str>(compression)) {
1453: 			throw InvalidInputException("from_parquet only accepts 'compression' as a string");
1454: 		}
1455: 		named_parameters["compression"] = Value(py::str(compression));
1456: 	}
1457: 
1458: 	return make_uniq<DuckDBPyRelation>(connection.TableFunction("parquet_scan", params, named_parameters)->Alias(name));
1459: }
1460: 
1461: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromArrow(py::object &arrow_object) {
1462: 	auto &connection = con.GetConnection();
1463: 	string name = "arrow_object_" + StringUtil::GenerateRandomName();
1464: 	if (!IsAcceptedArrowObject(arrow_object)) {
1465: 		auto py_object_type = string(py::str(arrow_object.get_type().attr("__name__")));
1466: 		throw InvalidInputException("Python Object Type %s is not an accepted Arrow Object.", py_object_type);
1467: 	}
1468: 	auto tableref = PythonReplacementScan::ReplacementObject(arrow_object, name, *connection.context);
1469: 	D_ASSERT(tableref);
1470: 	auto rel = make_shared_ptr<ViewRelation>(connection.context, std::move(tableref), name);
1471: 	return make_uniq<DuckDBPyRelation>(std::move(rel));
1472: }
1473: 
1474: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromSubstrait(py::bytes &proto) {
1475: 	auto &connection = con.GetConnection();
1476: 	string name = "substrait_" + StringUtil::GenerateRandomName();
1477: 	vector<Value> params;
1478: 	params.emplace_back(Value::BLOB_RAW(proto));
1479: 	return make_uniq<DuckDBPyRelation>(connection.TableFunction("from_substrait", params)->Alias(name));
1480: }
1481: 
1482: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::GetSubstrait(const string &query, bool enable_optimizer) {
1483: 	auto &connection = con.GetConnection();
1484: 	vector<Value> params;
1485: 	params.emplace_back(query);
1486: 	named_parameter_map_t named_parameters({{"enable_optimizer", Value::BOOLEAN(enable_optimizer)}});
1487: 	return make_uniq<DuckDBPyRelation>(
1488: 	    connection.TableFunction("get_substrait", params, named_parameters)->Alias(query));
1489: }
1490: 
1491: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::GetSubstraitJSON(const string &query, bool enable_optimizer) {
1492: 	auto &connection = con.GetConnection();
1493: 	vector<Value> params;
1494: 	params.emplace_back(query);
1495: 	named_parameter_map_t named_parameters({{"enable_optimizer", Value::BOOLEAN(enable_optimizer)}});
1496: 	return make_uniq<DuckDBPyRelation>(
1497: 	    connection.TableFunction("get_substrait_json", params, named_parameters)->Alias(query));
1498: }
1499: 
1500: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromSubstraitJSON(const string &json) {
1501: 	auto &connection = con.GetConnection();
1502: 	string name = "from_substrait_" + StringUtil::GenerateRandomName();
1503: 	vector<Value> params;
1504: 	params.emplace_back(json);
1505: 	return make_uniq<DuckDBPyRelation>(connection.TableFunction("from_substrait_json", params)->Alias(name));
1506: }
1507: 
1508: unordered_set<string> DuckDBPyConnection::GetTableNames(const string &query) {
1509: 	auto &connection = con.GetConnection();
1510: 	return connection.GetTableNames(query);
1511: }
1512: 
1513: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::UnregisterPythonObject(const string &name) {
1514: 	auto &connection = con.GetConnection();
1515: 	if (!registered_objects.count(name)) {
1516: 		return shared_from_this();
1517: 	}
1518: 	py::gil_scoped_release release;
1519: 	// FIXME: DROP TEMPORARY VIEW? doesn't exist?
1520: 	connection.Query("DROP VIEW \"" + name + "\"");
1521: 	registered_objects.erase(name);
1522: 	return shared_from_this();
1523: }
1524: 
1525: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Begin() {
1526: 	ExecuteFromString("BEGIN TRANSACTION");
1527: 	return shared_from_this();
1528: }
1529: 
1530: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Commit() {
1531: 	auto &connection = con.GetConnection();
1532: 	if (connection.context->transaction.IsAutoCommit()) {
1533: 		return shared_from_this();
1534: 	}
1535: 	ExecuteFromString("COMMIT");
1536: 	return shared_from_this();
1537: }
1538: 
1539: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Rollback() {
1540: 	ExecuteFromString("ROLLBACK");
1541: 	return shared_from_this();
1542: }
1543: 
1544: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Checkpoint() {
1545: 	ExecuteFromString("CHECKPOINT");
1546: 	return shared_from_this();
1547: }
1548: 
1549: Optional<py::list> DuckDBPyConnection::GetDescription() {
1550: 	if (!con.HasResult()) {
1551: 		return py::none();
1552: 	}
1553: 	auto &result = con.GetResult();
1554: 	return result.Description();
1555: }
1556: 
1557: int DuckDBPyConnection::GetRowcount() {
1558: 	return -1;
1559: }
1560: 
1561: void DuckDBPyConnection::Close() {
1562: 	con.SetResult(nullptr);
1563: 	con.SetConnection(nullptr);
1564: 	con.SetDatabase(nullptr);
1565: 	// https://peps.python.org/pep-0249/#Connection.close
1566: 	cursors.ClearCursors();
1567: 	registered_functions.clear();
1568: }
1569: 
1570: void DuckDBPyConnection::Interrupt() {
1571: 	auto &connection = con.GetConnection();
1572: 	connection.Interrupt();
1573: }
1574: 
1575: void DuckDBPyConnection::InstallExtension(const string &extension, bool force_install) {
1576: 	auto &connection = con.GetConnection();
1577: 	ExtensionHelper::InstallExtension(*connection.context, extension, force_install);
1578: }
1579: 
1580: void DuckDBPyConnection::LoadExtension(const string &extension) {
1581: 	auto &connection = con.GetConnection();
1582: 	ExtensionHelper::LoadExternalExtension(*connection.context, extension);
1583: }
1584: 
1585: void DuckDBPyConnection::Cursors::AddCursor(shared_ptr<DuckDBPyConnection> conn) {
1586: 	lock_guard<mutex> l(lock);
1587: 
1588: 	// Clean up previously created cursors
1589: 	vector<weak_ptr<DuckDBPyConnection>> compacted_cursors;
1590: 	bool needs_compaction = false;
1591: 	for (auto &cur_p : cursors) {
1592: 		auto cur = cur_p.lock();
1593: 		if (!cur) {
1594: 			needs_compaction = true;
1595: 			continue;
1596: 		}
1597: 		compacted_cursors.push_back(cur_p);
1598: 	}
1599: 	if (needs_compaction) {
1600: 		cursors = std::move(compacted_cursors);
1601: 	}
1602: 
1603: 	cursors.push_back(conn);
1604: }
1605: 
1606: void DuckDBPyConnection::Cursors::ClearCursors() {
1607: 	lock_guard<mutex> l(lock);
1608: 
1609: 	for (auto &cur : cursors) {
1610: 		auto cursor = cur.lock();
1611: 		if (!cursor) {
1612: 			// The cursor has already been closed
1613: 			continue;
1614: 		}
1615: 		cursor->Close();
1616: 	}
1617: 
1618: 	cursors.clear();
1619: }
1620: 
1621: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Cursor() {
1622: 	auto res = make_shared_ptr<DuckDBPyConnection>();
1623: 	res->con.SetDatabase(con);
1624: 	res->con.SetConnection(make_uniq<Connection>(res->con.GetDatabase()));
1625: 	cursors.AddCursor(res);
1626: 	return res;
1627: }
1628: 
1629: // these should be functions on the result but well
1630: Optional<py::tuple> DuckDBPyConnection::FetchOne() {
1631: 	if (!con.HasResult()) {
1632: 		throw InvalidInputException("No open result set");
1633: 	}
1634: 	auto &result = con.GetResult();
1635: 	return result.FetchOne();
1636: }
1637: 
1638: py::list DuckDBPyConnection::FetchMany(idx_t size) {
1639: 	if (!con.HasResult()) {
1640: 		throw InvalidInputException("No open result set");
1641: 	}
1642: 	auto &result = con.GetResult();
1643: 	return result.FetchMany(size);
1644: }
1645: 
1646: py::list DuckDBPyConnection::FetchAll() {
1647: 	if (!con.HasResult()) {
1648: 		throw InvalidInputException("No open result set");
1649: 	}
1650: 	auto &result = con.GetResult();
1651: 	return result.FetchAll();
1652: }
1653: 
1654: py::dict DuckDBPyConnection::FetchNumpy() {
1655: 	if (!con.HasResult()) {
1656: 		throw InvalidInputException("No open result set");
1657: 	}
1658: 	auto &result = con.GetResult();
1659: 	return result.FetchNumpyInternal();
1660: }
1661: 
1662: PandasDataFrame DuckDBPyConnection::FetchDF(bool date_as_object) {
1663: 	if (!con.HasResult()) {
1664: 		throw InvalidInputException("No open result set");
1665: 	}
1666: 	auto &result = con.GetResult();
1667: 	return result.FetchDF(date_as_object);
1668: }
1669: 
1670: PandasDataFrame DuckDBPyConnection::FetchDFChunk(const idx_t vectors_per_chunk, bool date_as_object) {
1671: 	if (!con.HasResult()) {
1672: 		throw InvalidInputException("No open result set");
1673: 	}
1674: 	auto &result = con.GetResult();
1675: 	return result.FetchDFChunk(vectors_per_chunk, date_as_object);
1676: }
1677: 
1678: duckdb::pyarrow::Table DuckDBPyConnection::FetchArrow(idx_t rows_per_batch) {
1679: 	if (!con.HasResult()) {
1680: 		throw InvalidInputException("No open result set");
1681: 	}
1682: 	auto &result = con.GetResult();
1683: 	return result.ToArrowTable(rows_per_batch);
1684: }
1685: 
1686: py::dict DuckDBPyConnection::FetchPyTorch() {
1687: 	if (!con.HasResult()) {
1688: 		throw InvalidInputException("No open result set");
1689: 	}
1690: 	auto &result = con.GetResult();
1691: 	return result.FetchPyTorch();
1692: }
1693: 
1694: py::dict DuckDBPyConnection::FetchTF() {
1695: 	if (!con.HasResult()) {
1696: 		throw InvalidInputException("No open result set");
1697: 	}
1698: 	auto &result = con.GetResult();
1699: 	return result.FetchTF();
1700: }
1701: 
1702: PolarsDataFrame DuckDBPyConnection::FetchPolars(idx_t rows_per_batch) {
1703: 	auto arrow = FetchArrow(rows_per_batch);
1704: 	return py::cast<PolarsDataFrame>(py::module::import("polars").attr("DataFrame")(arrow));
1705: }
1706: 
1707: duckdb::pyarrow::RecordBatchReader DuckDBPyConnection::FetchRecordBatchReader(const idx_t rows_per_batch) {
1708: 	if (!con.HasResult()) {
1709: 		throw InvalidInputException("No open result set");
1710: 	}
1711: 	auto &result = con.GetResult();
1712: 	return result.FetchRecordBatchReader(rows_per_batch);
1713: }
1714: 
1715: case_insensitive_map_t<Value> TransformPyConfigDict(const py::dict &py_config_dict) {
1716: 	case_insensitive_map_t<Value> config_dict;
1717: 	for (auto &kv : py_config_dict) {
1718: 		auto key = py::str(kv.first);
1719: 		auto val = py::str(kv.second);
1720: 		config_dict[key] = Value(val);
1721: 	}
1722: 	return config_dict;
1723: }
1724: 
1725: static bool HasJupyterProgressBarDependencies() {
1726: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
1727: 	if (!import_cache.ipywidgets()) {
1728: 		// ipywidgets not installed, needed to support the progress bar
1729: 		return false;
1730: 	}
1731: 	return true;
1732: }
1733: 
1734: static void SetDefaultConfigArguments(ClientContext &context) {
1735: 	if (!DuckDBPyConnection::IsInteractive()) {
1736: 		// Don't need to set any special default arguments
1737: 		return;
1738: 	}
1739: 
1740: 	auto &config = ClientConfig::GetConfig(context);
1741: 	config.enable_progress_bar = true;
1742: 
1743: 	if (!DuckDBPyConnection::IsJupyter()) {
1744: 		return;
1745: 	}
1746: 	if (!HasJupyterProgressBarDependencies()) {
1747: 		// Disable progress bar altogether
1748: 		config.system_progress_bar_disable_reason =
1749: 		    "required package 'ipywidgets' is missing, which is needed to render progress bars in Jupyter";
1750: 		config.enable_progress_bar = false;
1751: 		return;
1752: 	}
1753: 
1754: 	// Set the function used to create the display for the progress bar
1755: 	context.config.display_create_func = JupyterProgressBarDisplay::Create;
1756: }
1757: 
1758: void InstantiateNewInstance(DuckDB &db) {
1759: 	auto &db_instance = *db.instance;
1760: 	PandasScanFunction scan_fun;
1761: 	MapFunction map_fun;
1762: 	ExtensionUtil::RegisterFunction(db_instance, scan_fun);
1763: 	ExtensionUtil::RegisterFunction(db_instance, map_fun);
1764: }
1765: 
1766: static shared_ptr<DuckDBPyConnection> FetchOrCreateInstance(const string &database_path, DBConfig &config) {
1767: 	auto res = make_shared_ptr<DuckDBPyConnection>();
1768: 	bool cache_instance = database_path != ":memory:" && !database_path.empty();
1769: 	config.replacement_scans.emplace_back(PythonReplacementScan::Replace);
1770: 	{
1771: 		py::gil_scoped_release release;
1772: 		unique_lock<mutex> lock(res->py_connection_lock);
1773: 		auto database =
1774: 		    instance_cache.GetOrCreateInstance(database_path, config, cache_instance, InstantiateNewInstance);
1775: 		res->con.SetDatabase(std::move(database));
1776: 		res->con.SetConnection(make_uniq<Connection>(res->con.GetDatabase()));
1777: 	}
1778: 	return res;
1779: }
1780: 
1781: bool IsDefaultConnectionString(const string &database, bool read_only, case_insensitive_map_t<Value> &config) {
1782: 	bool is_default = StringUtil::CIEquals(database, ":default:");
1783: 	if (!is_default) {
1784: 		return false;
1785: 	}
1786: 	// Only allow fetching the default connection when no options are passed
1787: 	if (read_only == true || !config.empty()) {
1788: 		throw InvalidInputException("Default connection fetching is only allowed without additional options");
1789: 	}
1790: 	return true;
1791: }
1792: 
1793: static string GetPathString(const py::object &path) {
1794: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
1795: 	const bool is_path = py::isinstance(path, import_cache.pathlib.Path());
1796: 	if (is_path || py::isinstance<py::str>(path)) {
1797: 		return std::string(py::str(path));
1798: 	}
1799: 	string actual_type = py::str(path.get_type());
1800: 	throw InvalidInputException("Please provide either a str or a pathlib.Path, not %s", actual_type);
1801: }
1802: 
1803: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Connect(const py::object &database_p, bool read_only,
1804:                                                            const py::dict &config_options) {
1805: 	auto config_dict = TransformPyConfigDict(config_options);
1806: 	auto database = GetPathString(database_p);
1807: 	if (IsDefaultConnectionString(database, read_only, config_dict)) {
1808: 		return DuckDBPyConnection::DefaultConnection();
1809: 	}
1810: 
1811: 	DBConfig config(read_only);
1812: 	config.AddExtensionOption("pandas_analyze_sample",
1813: 	                          "The maximum number of rows to sample when analyzing a pandas object column.",
1814: 	                          LogicalType::UBIGINT, Value::UBIGINT(1000));
1815: 	config.AddExtensionOption("python_enable_replacements",
1816: 	                          "Whether variables visible to the current stack should be used for replacement scans.",
1817: 	                          LogicalType::BOOLEAN, Value::BOOLEAN(true));
1818: 	if (!DuckDBPyConnection::IsJupyter()) {
1819: 		config_dict["duckdb_api"] = Value("python");
1820: 	} else {
1821: 		config_dict["duckdb_api"] = Value("python jupyter");
1822: 	}
1823: 	config.SetOptionsByName(config_dict);
1824: 
1825: 	auto res = FetchOrCreateInstance(database, config);
1826: 	auto &client_context = *res->con.GetConnection().context;
1827: 	SetDefaultConfigArguments(client_context);
1828: 	return res;
1829: }
1830: 
1831: vector<Value> DuckDBPyConnection::TransformPythonParamList(const py::handle &params) {
1832: 	vector<Value> args;
1833: 	args.reserve(py::len(params));
1834: 
1835: 	for (auto param : params) {
1836: 		args.emplace_back(TransformPythonValue(param, LogicalType::UNKNOWN, false));
1837: 	}
1838: 	return args;
1839: }
1840: 
1841: case_insensitive_map_t<BoundParameterData> DuckDBPyConnection::TransformPythonParamDict(const py::dict &params) {
1842: 	case_insensitive_map_t<BoundParameterData> args;
1843: 
1844: 	for (auto pair : params) {
1845: 		auto &key = pair.first;
1846: 		auto &value = pair.second;
1847: 		args[std::string(py::str(key))] = BoundParameterData(TransformPythonValue(value, LogicalType::UNKNOWN, false));
1848: 	}
1849: 	return args;
1850: }
1851: 
1852: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::DefaultConnection() {
1853: 	if (!default_connection) {
1854: 		py::dict config_dict;
1855: 		default_connection = DuckDBPyConnection::Connect(py::str(":memory:"), false, config_dict);
1856: 	}
1857: 	return default_connection;
1858: }
1859: 
1860: PythonImportCache *DuckDBPyConnection::ImportCache() {
1861: 	if (!import_cache) {
1862: 		import_cache = make_shared_ptr<PythonImportCache>();
1863: 	}
1864: 	return import_cache.get();
1865: }
1866: 
1867: ModifiedMemoryFileSystem &DuckDBPyConnection::GetObjectFileSystem() {
1868: 	if (!internal_object_filesystem) {
1869: 		D_ASSERT(!FileSystemIsRegistered("DUCKDB_INTERNAL_OBJECTSTORE"));
1870: 		auto &import_cache_py = *ImportCache();
1871: 		auto modified_memory_fs = import_cache_py.duckdb.filesystem.ModifiedMemoryFileSystem();
1872: 		if (modified_memory_fs.ptr() == nullptr) {
1873: 			throw InvalidInputException(
1874: 			    "This operation could not be completed because required module 'fsspec' is not installed");
1875: 		}
1876: 		internal_object_filesystem = make_shared_ptr<ModifiedMemoryFileSystem>(modified_memory_fs());
1877: 		auto &abstract_fs = reinterpret_cast<AbstractFileSystem &>(*internal_object_filesystem);
1878: 		RegisterFilesystem(abstract_fs);
1879: 	}
1880: 	return *internal_object_filesystem;
1881: }
1882: 
1883: bool DuckDBPyConnection::IsInteractive() {
1884: 	return DuckDBPyConnection::environment != PythonEnvironmentType::NORMAL;
1885: }
1886: 
1887: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Enter() {
1888: 	return shared_from_this();
1889: }
1890: 
1891: void DuckDBPyConnection::Exit(DuckDBPyConnection &self, const py::object &exc_type, const py::object &exc,
1892:                               const py::object &traceback) {
1893: 	self.Close();
1894: 	if (exc_type.ptr() != Py_None) {
1895: 		// Propagate the exception if any occurred
1896: 		PyErr_SetObject(exc_type.ptr(), exc.ptr());
1897: 		throw py::error_already_set();
1898: 	}
1899: }
1900: 
1901: void DuckDBPyConnection::Cleanup() {
1902: 	default_connection.reset();
1903: 	import_cache.reset();
1904: }
1905: 
1906: bool DuckDBPyConnection::IsPandasDataframe(const py::object &object) {
1907: 	if (!ModuleIsLoaded<PandasCacheItem>()) {
1908: 		return false;
1909: 	}
1910: 	auto &import_cache_py = *DuckDBPyConnection::ImportCache();
1911: 	return py::isinstance(object, import_cache_py.pandas.DataFrame());
1912: }
1913: 
1914: bool DuckDBPyConnection::IsPolarsDataframe(const py::object &object) {
1915: 	if (!ModuleIsLoaded<PolarsCacheItem>()) {
1916: 		return false;
1917: 	}
1918: 	auto &import_cache_py = *DuckDBPyConnection::ImportCache();
1919: 	return py::isinstance(object, import_cache_py.polars.DataFrame()) ||
1920: 	       py::isinstance(object, import_cache_py.polars.LazyFrame());
1921: }
1922: 
1923: bool IsValidNumpyDimensions(const py::handle &object, int &dim) {
1924: 	// check the dimensions of numpy arrays
1925: 	// should only be called by IsAcceptedNumpyObject
1926: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
1927: 	if (!py::isinstance(object, import_cache.numpy.ndarray())) {
1928: 		return false;
1929: 	}
1930: 	auto shape = (py::cast<py::array>(object)).attr("shape");
1931: 	if (py::len(shape) != 1) {
1932: 		return false;
1933: 	}
1934: 	int cur_dim = (shape.attr("__getitem__")(0)).cast<int>();
1935: 	dim = dim == -1 ? cur_dim : dim;
1936: 	return dim == cur_dim;
1937: }
1938: NumpyObjectType DuckDBPyConnection::IsAcceptedNumpyObject(const py::object &object) {
1939: 	if (!ModuleIsLoaded<NumpyCacheItem>()) {
1940: 		return NumpyObjectType::INVALID;
1941: 	}
1942: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
1943: 	if (py::isinstance(object, import_cache.numpy.ndarray())) {
1944: 		auto len = py::len((py::cast<py::array>(object)).attr("shape"));
1945: 		switch (len) {
1946: 		case 1:
1947: 			return NumpyObjectType::NDARRAY1D;
1948: 		case 2:
1949: 			return NumpyObjectType::NDARRAY2D;
1950: 		default:
1951: 			return NumpyObjectType::INVALID;
1952: 		}
1953: 	} else if (py::is_dict_like(object)) {
1954: 		int dim = -1;
1955: 		for (auto item : py::cast<py::dict>(object)) {
1956: 			if (!IsValidNumpyDimensions(item.second, dim)) {
1957: 				return NumpyObjectType::INVALID;
1958: 			}
1959: 		}
1960: 		return NumpyObjectType::DICT;
1961: 	} else if (py::is_list_like(object)) {
1962: 		int dim = -1;
1963: 		for (auto item : py::cast<py::list>(object)) {
1964: 			if (!IsValidNumpyDimensions(item, dim)) {
1965: 				return NumpyObjectType::INVALID;
1966: 			}
1967: 		}
1968: 		return NumpyObjectType::LIST;
1969: 	}
1970: 	return NumpyObjectType::INVALID;
1971: }
1972: 
1973: PyArrowObjectType DuckDBPyConnection::GetArrowType(const py::handle &obj) {
1974: 	D_ASSERT(py::gil_check());
1975: 
1976: 	if (py::isinstance<py::capsule>(obj)) {
1977: 		auto capsule = py::reinterpret_borrow<py::capsule>(obj);
1978: 		if (string(capsule.name()) != "arrow_array_stream") {
1979: 			throw InvalidInputException("Expected a 'arrow_array_stream' PyCapsule, got: %s", string(capsule.name()));
1980: 		}
1981: 		auto stream = capsule.get_pointer<struct ArrowArrayStream>();
1982: 		if (!stream->release) {
1983: 			throw InvalidInputException("The ArrowArrayStream was already released");
1984: 		}
1985: 		return PyArrowObjectType::PyCapsule;
1986: 	}
1987: 
1988: 	if (!ModuleIsLoaded<PyarrowCacheItem>()) {
1989: 		return PyArrowObjectType::Invalid;
1990: 	}
1991: 
1992: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
1993: 	// First Verify Lib Types
1994: 	auto table_class = import_cache.pyarrow.Table();
1995: 	auto record_batch_reader_class = import_cache.pyarrow.RecordBatchReader();
1996: 	if (py::isinstance(obj, table_class)) {
1997: 		return PyArrowObjectType::Table;
1998: 	} else if (py::isinstance(obj, record_batch_reader_class)) {
1999: 		return PyArrowObjectType::RecordBatchReader;
2000: 	}
2001: 
2002: 	if (!ModuleIsLoaded<PyarrowDatasetCacheItem>()) {
2003: 		return PyArrowObjectType::Invalid;
2004: 	}
2005: 
2006: 	// Then Verify dataset types
2007: 	auto dataset_class = import_cache.pyarrow.dataset.Dataset();
2008: 	auto scanner_class = import_cache.pyarrow.dataset.Scanner();
2009: 
2010: 	if (py::isinstance(obj, scanner_class)) {
2011: 		return PyArrowObjectType::Scanner;
2012: 	} else if (py::isinstance(obj, dataset_class)) {
2013: 		return PyArrowObjectType::Dataset;
2014: 	}
2015: 	return PyArrowObjectType::Invalid;
2016: }
2017: 
2018: bool DuckDBPyConnection::IsAcceptedArrowObject(const py::object &object) {
2019: 	return DuckDBPyConnection::GetArrowType(object) != PyArrowObjectType::Invalid;
2020: }
2021: 
2022: unique_lock<std::mutex> DuckDBPyConnection::AcquireConnectionLock() {
2023: 	// we first release the gil and then acquire the connection lock
2024: 	unique_lock<std::mutex> lock(py_connection_lock, std::defer_lock);
2025: 	{
2026: 		py::gil_scoped_release release;
2027: 		lock.lock();
2028: 	}
2029: 	return lock;
2030: }
2031: 
2032: } // namespace duckdb
[end of tools/pythonpkg/src/pyconnection.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: