{
  "repo": "duckdb/duckdb",
  "pull_number": 8943,
  "instance_id": "duckdb__duckdb-8943",
  "issue_numbers": [
    "7887",
    "7887",
    "8420"
  ],
  "base_commit": "ba71015ee711d95fe13886f372824143cae7493c",
  "patch": "diff --git a/scripts/generate_serialization.py b/scripts/generate_serialization.py\nindex a4cea4fb34fd..b72261b6f1cf 100644\n--- a/scripts/generate_serialization.py\n+++ b/scripts/generate_serialization.py\n@@ -515,7 +515,15 @@ def generate_class_code(class_entry):\n             class_deserialize += get_deserialize_assignment(\n                 entry.deserialize_property, entry.type, class_entry.pointer_type\n             )\n+        if entry.name in class_entry.set_parameter_names:\n+            class_deserialize += set_deserialize_parameter.replace('${PROPERTY_TYPE}', entry.type).replace(\n+                '${PROPERTY_NAME}', entry.name\n+            )\n \n+    for entry in class_entry.set_parameters:\n+        class_deserialize += unset_deserialize_parameter.replace('${PROPERTY_TYPE}', entry.type).replace(\n+            '${PROPERTY_NAME}', entry.name\n+        )\n     class_deserialize += generate_return(class_entry)\n     deserialize_return = get_return_value(class_entry.pointer_type, class_entry.return_type)\n \ndiff --git a/src/execution/operator/persistent/physical_batch_insert.cpp b/src/execution/operator/persistent/physical_batch_insert.cpp\nindex 8a57ece39891..d807e0e9f02f 100644\n--- a/src/execution/operator/persistent/physical_batch_insert.cpp\n+++ b/src/execution/operator/persistent/physical_batch_insert.cpp\n@@ -6,6 +6,7 @@\n #include \"duckdb/storage/table_io_manager.hpp\"\n #include \"duckdb/transaction/local_storage.hpp\"\n #include \"duckdb/catalog/catalog_entry/duck_table_entry.hpp\"\n+#include \"duckdb/transaction/duck_transaction.hpp\"\n #include \"duckdb/storage/table/append_state.hpp\"\n #include \"duckdb/storage/table/scan_state.hpp\"\n \n@@ -119,6 +120,7 @@ class BatchInsertGlobalState : public GlobalSinkState {\n \tidx_t insert_count;\n \tvector<RowGroupBatchEntry> collections;\n \tidx_t next_start = 0;\n+\tbool optimistically_written = false;\n \n \tvoid FindMergeCollections(idx_t min_batch_index, optional_idx &merged_batch_index,\n \t                          vector<unique_ptr<RowGroupCollection>> &result) {\n@@ -176,10 +178,12 @@ class BatchInsertGlobalState : public GlobalSinkState {\n \tunique_ptr<RowGroupCollection> MergeCollections(ClientContext &context,\n \t                                                vector<unique_ptr<RowGroupCollection>> merge_collections,\n \t                                                OptimisticDataWriter &writer) {\n+\t\tD_ASSERT(!merge_collections.empty());\n \t\tCollectionMerger merger(context);\n \t\tfor (auto &collection : merge_collections) {\n \t\t\tmerger.AddCollection(std::move(collection));\n \t\t}\n+\t\toptimistically_written = true;\n \t\treturn merger.Flush(writer);\n \t}\n \n@@ -373,48 +377,65 @@ SinkFinalizeType PhysicalBatchInsert::Finalize(Pipeline &pipeline, Event &event,\n                                                OperatorSinkFinalizeInput &input) const {\n \tauto &gstate = input.global_state.Cast<BatchInsertGlobalState>();\n \n-\t// in the finalize, do a final pass over all of the collections we created and try to merge smaller collections\n-\t// together\n-\tvector<unique_ptr<CollectionMerger>> mergers;\n-\tunique_ptr<CollectionMerger> current_merger;\n-\n-\tauto &storage = gstate.table.GetStorage();\n-\tfor (auto &entry : gstate.collections) {\n-\t\tif (entry.type == RowGroupBatchType::NOT_FLUSHED) {\n-\t\t\t// this collection has not been flushed: add it to the merge set\n-\t\t\tif (!current_merger) {\n-\t\t\t\tcurrent_merger = make_uniq<CollectionMerger>(context);\n-\t\t\t}\n-\t\t\tcurrent_merger->AddCollection(std::move(entry.collection));\n-\t\t} else {\n-\t\t\t// this collection has been flushed: it does not need to be merged\n-\t\t\t// create a separate collection merger only for this entry\n-\t\t\tif (current_merger) {\n-\t\t\t\t// we have small collections remaining: flush them\n-\t\t\t\tmergers.push_back(std::move(current_merger));\n-\t\t\t\tcurrent_merger.reset();\n+\tif (gstate.optimistically_written || gstate.insert_count >= LocalStorage::MERGE_THRESHOLD) {\n+\t\t// we have written data to disk optimistically or are inserting a large amount of data\n+\t\t// perform a final pass over all of the row groups and merge them together\n+\t\tvector<unique_ptr<CollectionMerger>> mergers;\n+\t\tunique_ptr<CollectionMerger> current_merger;\n+\n+\t\tauto &storage = gstate.table.GetStorage();\n+\t\tfor (auto &entry : gstate.collections) {\n+\t\t\tif (entry.type == RowGroupBatchType::NOT_FLUSHED) {\n+\t\t\t\t// this collection has not been flushed: add it to the merge set\n+\t\t\t\tif (!current_merger) {\n+\t\t\t\t\tcurrent_merger = make_uniq<CollectionMerger>(context);\n+\t\t\t\t}\n+\t\t\t\tcurrent_merger->AddCollection(std::move(entry.collection));\n+\t\t\t} else {\n+\t\t\t\t// this collection has been flushed: it does not need to be merged\n+\t\t\t\t// create a separate collection merger only for this entry\n+\t\t\t\tif (current_merger) {\n+\t\t\t\t\t// we have small collections remaining: flush them\n+\t\t\t\t\tmergers.push_back(std::move(current_merger));\n+\t\t\t\t\tcurrent_merger.reset();\n+\t\t\t\t}\n+\t\t\t\tauto larger_merger = make_uniq<CollectionMerger>(context);\n+\t\t\t\tlarger_merger->AddCollection(std::move(entry.collection));\n+\t\t\t\tmergers.push_back(std::move(larger_merger));\n \t\t\t}\n-\t\t\tauto larger_merger = make_uniq<CollectionMerger>(context);\n-\t\t\tlarger_merger->AddCollection(std::move(entry.collection));\n-\t\t\tmergers.push_back(std::move(larger_merger));\n \t\t}\n-\t}\n-\tif (current_merger) {\n-\t\tmergers.push_back(std::move(current_merger));\n-\t}\n+\t\tif (current_merger) {\n+\t\t\tmergers.push_back(std::move(current_merger));\n+\t\t}\n \n-\t// now that we have created all of the mergers, perform the actual merging\n-\tvector<unique_ptr<RowGroupCollection>> final_collections;\n-\tfinal_collections.reserve(mergers.size());\n-\tauto &writer = storage.CreateOptimisticWriter(context);\n-\tfor (auto &merger : mergers) {\n-\t\tfinal_collections.push_back(merger->Flush(writer));\n-\t}\n-\tstorage.FinalizeOptimisticWriter(context, writer);\n+\t\t// now that we have created all of the mergers, perform the actual merging\n+\t\tvector<unique_ptr<RowGroupCollection>> final_collections;\n+\t\tfinal_collections.reserve(mergers.size());\n+\t\tauto &writer = storage.CreateOptimisticWriter(context);\n+\t\tfor (auto &merger : mergers) {\n+\t\t\tfinal_collections.push_back(merger->Flush(writer));\n+\t\t}\n+\t\tstorage.FinalizeOptimisticWriter(context, writer);\n \n-\t// finally, merge the row groups into the local storage\n-\tfor (auto &collection : final_collections) {\n-\t\tstorage.LocalMerge(context, *collection);\n+\t\t// finally, merge the row groups into the local storage\n+\t\tfor (auto &collection : final_collections) {\n+\t\t\tstorage.LocalMerge(context, *collection);\n+\t\t}\n+\t} else {\n+\t\t// we are writing a small amount of data to disk\n+\t\t// append directly to transaction local storage\n+\t\tauto &table = gstate.table;\n+\t\tauto &storage = table.GetStorage();\n+\t\tLocalAppendState append_state;\n+\t\tstorage.InitializeLocalAppend(append_state, context);\n+\t\tauto &transaction = DuckTransaction::Get(context, table.catalog);\n+\t\tfor (auto &entry : gstate.collections) {\n+\t\t\tentry.collection->Scan(transaction, [&](DataChunk &insert_chunk) {\n+\t\t\t\tstorage.LocalAppend(append_state, table, context, insert_chunk);\n+\t\t\t\treturn true;\n+\t\t\t});\n+\t\t}\n+\t\tstorage.FinalizeLocalAppend(append_state);\n \t}\n \treturn SinkFinalizeType::READY;\n }\ndiff --git a/src/function/table/system/pragma_storage_info.cpp b/src/function/table/system/pragma_storage_info.cpp\nindex d584a7990618..90c60d15c040 100644\n--- a/src/function/table/system/pragma_storage_info.cpp\n+++ b/src/function/table/system/pragma_storage_info.cpp\n@@ -76,6 +76,9 @@ static unique_ptr<FunctionData> PragmaStorageInfoBind(ClientContext &context, Ta\n \tnames.emplace_back(\"block_offset\");\n \treturn_types.emplace_back(LogicalType::BIGINT);\n \n+\tnames.emplace_back(\"segment_info\");\n+\treturn_types.emplace_back(LogicalType::VARCHAR);\n+\n \tauto qname = QualifiedName::Parse(input.inputs[0].GetValue<string>());\n \n \t// look up the table name in the catalog\n@@ -133,6 +136,8 @@ static void PragmaStorageInfoFunction(ClientContext &context, TableFunctionInput\n \t\t\toutput.SetValue(col_idx++, count, Value());\n \t\t\toutput.SetValue(col_idx++, count, Value());\n \t\t}\n+\t\t// segment_info\n+\t\toutput.SetValue(col_idx++, count, Value(entry.segment_info));\n \t\tcount++;\n \t}\n \toutput.SetCardinality(count);\ndiff --git a/src/include/duckdb/common/serializer/deserialization_data.hpp b/src/include/duckdb/common/serializer/deserialization_data.hpp\nindex 9e4a6c1a9d0f..c936644ec9c4 100644\n--- a/src/include/duckdb/common/serializer/deserialization_data.hpp\n+++ b/src/include/duckdb/common/serializer/deserialization_data.hpp\n@@ -15,10 +15,12 @@\n namespace duckdb {\n class ClientContext;\n class Catalog;\n+class DatabaseInstance;\n enum class ExpressionType : uint8_t;\n \n struct DeserializationData {\n \tstack<reference<ClientContext>> contexts;\n+\tstack<reference<DatabaseInstance>> databases;\n \tstack<idx_t> enums;\n \tstack<reference<bound_parameter_map_t>> parameter_data;\n \tstack<reference<LogicalType>> types;\n@@ -74,6 +76,23 @@ inline void DeserializationData::Unset<LogicalOperatorType>() {\n \tenums.pop();\n }\n \n+template <>\n+inline void DeserializationData::Set(CompressionType type) {\n+\tenums.push(idx_t(type));\n+}\n+\n+template <>\n+inline CompressionType DeserializationData::Get() {\n+\tAssertNotEmpty(enums);\n+\treturn CompressionType(enums.top());\n+}\n+\n+template <>\n+inline void DeserializationData::Unset<CompressionType>() {\n+\tAssertNotEmpty(enums);\n+\tenums.pop();\n+}\n+\n template <>\n inline void DeserializationData::Set(CatalogType type) {\n \tenums.push(idx_t(type));\n@@ -108,6 +127,23 @@ inline void DeserializationData::Unset<ClientContext>() {\n \tcontexts.pop();\n }\n \n+template <>\n+inline void DeserializationData::Set(DatabaseInstance &db) {\n+\tdatabases.push(db);\n+}\n+\n+template <>\n+inline DatabaseInstance &DeserializationData::Get() {\n+\tAssertNotEmpty(databases);\n+\treturn databases.top();\n+}\n+\n+template <>\n+inline void DeserializationData::Unset<DatabaseInstance>() {\n+\tAssertNotEmpty(databases);\n+\tdatabases.pop();\n+}\n+\n template <>\n inline void DeserializationData::Set(bound_parameter_map_t &context) {\n \tparameter_data.push(context);\ndiff --git a/src/include/duckdb/function/compression_function.hpp b/src/include/duckdb/function/compression_function.hpp\nindex a691a8aa82cd..24bb45e01e5a 100644\n--- a/src/include/duckdb/function/compression_function.hpp\n+++ b/src/include/duckdb/function/compression_function.hpp\n@@ -14,6 +14,7 @@\n #include \"duckdb/common/map.hpp\"\n #include \"duckdb/storage/storage_info.hpp\"\n #include \"duckdb/common/mutex.hpp\"\n+#include \"duckdb/storage/data_pointer.hpp\"\n \n namespace duckdb {\n class DatabaseInstance;\n@@ -21,6 +22,7 @@ class ColumnData;\n class ColumnDataCheckpointer;\n class ColumnSegment;\n class SegmentStatistics;\n+struct ColumnSegmentState;\n \n struct ColumnFetchState;\n struct ColumnScanState;\n@@ -62,6 +64,11 @@ struct CompressedSegmentState {\n \tvirtual ~CompressedSegmentState() {\n \t}\n \n+\t//! Display info for PRAGMA storage_info\n+\tvirtual string GetSegmentInfo() const { // LCOV_EXCL_START\n+\t\treturn \"\";\n+\t} // LCOV_EXCL_STOP\n+\n \ttemplate <class TARGET>\n \tTARGET &Cast() {\n \t\tD_ASSERT(dynamic_cast<TARGET *>(this));\n@@ -75,7 +82,7 @@ struct CompressedSegmentState {\n };\n \n struct CompressionAppendState {\n-\tCompressionAppendState(BufferHandle handle_p) : handle(std::move(handle_p)) {\n+\texplicit CompressionAppendState(BufferHandle handle_p) : handle(std::move(handle_p)) {\n \t}\n \tvirtual ~CompressionAppendState() {\n \t}\n@@ -139,13 +146,24 @@ typedef void (*compression_skip_t)(ColumnSegment &segment, ColumnScanState &stat\n //===--------------------------------------------------------------------===//\n // Append (optional)\n //===--------------------------------------------------------------------===//\n-typedef unique_ptr<CompressedSegmentState> (*compression_init_segment_t)(ColumnSegment &segment, block_id_t block_id);\n+typedef unique_ptr<CompressedSegmentState> (*compression_init_segment_t)(\n+    ColumnSegment &segment, block_id_t block_id, optional_ptr<ColumnSegmentState> segment_state);\n typedef unique_ptr<CompressionAppendState> (*compression_init_append_t)(ColumnSegment &segment);\n typedef idx_t (*compression_append_t)(CompressionAppendState &append_state, ColumnSegment &segment,\n                                       SegmentStatistics &stats, UnifiedVectorFormat &data, idx_t offset, idx_t count);\n typedef idx_t (*compression_finalize_append_t)(ColumnSegment &segment, SegmentStatistics &stats);\n typedef void (*compression_revert_append_t)(ColumnSegment &segment, idx_t start_row);\n \n+//===--------------------------------------------------------------------===//\n+// Serialization (optional)\n+//===--------------------------------------------------------------------===//\n+//! Function prototype for serializing the segment state\n+typedef unique_ptr<ColumnSegmentState> (*compression_serialize_state_t)(ColumnSegment &segment);\n+//! Function prototype for deserializing the segment state\n+typedef unique_ptr<ColumnSegmentState> (*compression_deserialize_state_t)(Deserializer &deserializer);\n+//! Function prototype for cleaning up the segment state when the column data is dropped\n+typedef void (*compression_cleanup_state_t)(ColumnSegment &segment);\n+\n class CompressionFunction {\n public:\n \tCompressionFunction(CompressionType type, PhysicalType data_type, compression_init_analyze_t init_analyze,\n@@ -157,12 +175,16 @@ class CompressionFunction {\n \t                    compression_init_segment_t init_segment = nullptr,\n \t                    compression_init_append_t init_append = nullptr, compression_append_t append = nullptr,\n \t                    compression_finalize_append_t finalize_append = nullptr,\n-\t                    compression_revert_append_t revert_append = nullptr)\n+\t                    compression_revert_append_t revert_append = nullptr,\n+\t                    compression_serialize_state_t serialize_state = nullptr,\n+\t                    compression_deserialize_state_t deserialize_state = nullptr,\n+\t                    compression_cleanup_state_t cleanup_state = nullptr)\n \t    : type(type), data_type(data_type), init_analyze(init_analyze), analyze(analyze), final_analyze(final_analyze),\n \t      init_compression(init_compression), compress(compress), compress_finalize(compress_finalize),\n \t      init_scan(init_scan), scan_vector(scan_vector), scan_partial(scan_partial), fetch_row(fetch_row), skip(skip),\n \t      init_segment(init_segment), init_append(init_append), append(append), finalize_append(finalize_append),\n-\t      revert_append(revert_append) {\n+\t      revert_append(revert_append), serialize_state(serialize_state), deserialize_state(deserialize_state),\n+\t      cleanup_state(cleanup_state) {\n \t}\n \n \t//! Compression type\n@@ -218,6 +240,16 @@ class CompressionFunction {\n \tcompression_finalize_append_t finalize_append;\n \t//! Revert append (optional)\n \tcompression_revert_append_t revert_append;\n+\n+\t// State serialize functions\n+\t//! This is only necessary if the segment state has information that must be written to disk in the metadata\n+\n+\t//! Serialize the segment state to the metadata (optional)\n+\tcompression_serialize_state_t serialize_state;\n+\t//! Deserialize the segment state to the metadata (optional)\n+\tcompression_deserialize_state_t deserialize_state;\n+\t//! Cleanup the segment state (optional)\n+\tcompression_cleanup_state_t cleanup_state;\n };\n \n //! The set of compression functions\ndiff --git a/src/include/duckdb/storage/checkpoint/string_checkpoint_state.hpp b/src/include/duckdb/storage/checkpoint/string_checkpoint_state.hpp\nindex d66104de6070..f9de616325d3 100644\n--- a/src/include/duckdb/storage/checkpoint/string_checkpoint_state.hpp\n+++ b/src/include/duckdb/storage/checkpoint/string_checkpoint_state.hpp\n@@ -14,13 +14,16 @@\n #include \"duckdb/function/compression_function.hpp\"\n \n namespace duckdb {\n+struct UncompressedStringSegmentState;\n \n class OverflowStringWriter {\n public:\n \tvirtual ~OverflowStringWriter() {\n \t}\n \n-\tvirtual void WriteString(string_t string, block_id_t &result_block, int32_t &result_offset) = 0;\n+\tvirtual void WriteString(UncompressedStringSegmentState &state, string_t string, block_id_t &result_block,\n+\t                         int32_t &result_offset) = 0;\n+\tvirtual void Flush() = 0;\n };\n \n struct StringBlock {\n@@ -43,15 +46,35 @@ struct string_location_t {\n };\n \n struct UncompressedStringSegmentState : public CompressedSegmentState {\n-\t~UncompressedStringSegmentState();\n+\t~UncompressedStringSegmentState() override;\n \n \t//! The string block holding strings that do not fit in the main block\n \t//! FIXME: this should be replaced by a heap that also allows freeing of unused strings\n \tunique_ptr<StringBlock> head;\n+\t//! Map of block id to string block\n+\tunordered_map<block_id_t, reference<StringBlock>> overflow_blocks;\n \t//! Overflow string writer (if any), if not set overflow strings will be written to memory blocks\n \tunique_ptr<OverflowStringWriter> overflow_writer;\n-\t//! Map of block id to string block\n-\tunordered_map<block_id_t, StringBlock *> overflow_blocks;\n+\t//! The set of overflow blocks written to disk (if any)\n+\tvector<block_id_t> on_disk_blocks;\n+\n+public:\n+\tshared_ptr<BlockHandle> GetHandle(BlockManager &manager, block_id_t block_id);\n+\n+\tvoid RegisterBlock(BlockManager &manager, block_id_t block_id);\n+\n+\tstring GetSegmentInfo() const override {\n+\t\tif (on_disk_blocks.empty()) {\n+\t\t\treturn \"\";\n+\t\t}\n+\t\tstring result = StringUtil::Join(on_disk_blocks, on_disk_blocks.size(), \", \",\n+\t\t                                 [&](block_id_t block) { return to_string(block); });\n+\t\treturn \"Overflow String Block Ids: \" + result;\n+\t}\n+\n+private:\n+\tmutex block_lock;\n+\tunordered_map<block_id_t, shared_ptr<BlockHandle>> handles;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/storage/checkpoint/write_overflow_strings_to_disk.hpp b/src/include/duckdb/storage/checkpoint/write_overflow_strings_to_disk.hpp\nindex af6f20e1cb05..c1949a5a07d6 100644\n--- a/src/include/duckdb/storage/checkpoint/write_overflow_strings_to_disk.hpp\n+++ b/src/include/duckdb/storage/checkpoint/write_overflow_strings_to_disk.hpp\n@@ -30,10 +30,12 @@ class WriteOverflowStringsToDisk : public OverflowStringWriter {\n \tstatic constexpr idx_t STRING_SPACE = Storage::BLOCK_SIZE - sizeof(block_id_t);\n \n public:\n-\tvoid WriteString(string_t string, block_id_t &result_block, int32_t &result_offset) override;\n+\tvoid WriteString(UncompressedStringSegmentState &state, string_t string, block_id_t &result_block,\n+\t                 int32_t &result_offset) override;\n+\tvoid Flush() override;\n \n private:\n-\tvoid AllocateNewBlock(block_id_t new_block_id);\n+\tvoid AllocateNewBlock(UncompressedStringSegmentState &state, block_id_t new_block_id);\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/storage/data_pointer.hpp b/src/include/duckdb/storage/data_pointer.hpp\nindex 314f27f015cd..c9f0bf38cf0c 100644\n--- a/src/include/duckdb/storage/data_pointer.hpp\n+++ b/src/include/duckdb/storage/data_pointer.hpp\n@@ -20,8 +20,27 @@ namespace duckdb {\n class Serializer;\n class Deserializer;\n \n+struct ColumnSegmentState {\n+\tvirtual ~ColumnSegmentState() {\n+\t}\n+\n+\tvirtual void Serialize(Serializer &serializer) const = 0;\n+\tstatic unique_ptr<ColumnSegmentState> Deserialize(Deserializer &deserializer);\n+\n+\ttemplate <class TARGET>\n+\tTARGET &Cast() {\n+\t\tD_ASSERT(dynamic_cast<TARGET *>(this));\n+\t\treturn reinterpret_cast<TARGET &>(*this);\n+\t}\n+\ttemplate <class TARGET>\n+\tconst TARGET &Cast() const {\n+\t\tD_ASSERT(dynamic_cast<const TARGET *>(this));\n+\t\treturn reinterpret_cast<const TARGET &>(*this);\n+\t}\n+};\n+\n struct DataPointer {\n-\tDataPointer(BaseStatistics stats) : statistics(std::move(stats)) {\n+\texplicit DataPointer(BaseStatistics stats) : statistics(std::move(stats)) {\n \t}\n \n \tuint64_t row_start;\n@@ -30,6 +49,8 @@ struct DataPointer {\n \tCompressionType compression_type;\n \t//! Type-specific statistics of the segment\n \tBaseStatistics statistics;\n+\t//! Serialized segment state\n+\tunique_ptr<ColumnSegmentState> segment_state;\n \n \tvoid Serialize(Serializer &serializer) const;\n \tstatic DataPointer Deserialize(Deserializer &source);\ndiff --git a/src/include/duckdb/storage/serialization/storage.json b/src/include/duckdb/storage/serialization/storage.json\nindex 8b46df8ddeab..0b2e4cbdbeac 100644\n--- a/src/include/duckdb/storage/serialization/storage.json\n+++ b/src/include/duckdb/storage/serialization/storage.json\n@@ -35,8 +35,7 @@\n     ],\n     \"pointer_type\": \"none\",\n     \"constructor\": [\"block_id\", \"offset\"]\n-  },\n-  {\n+  },  {\n     \"class\": \"DataPointer\",\n     \"includes\": [\n       \"duckdb/storage/data_pointer.hpp\"\n@@ -66,8 +65,14 @@\n         \"id\": 104,\n         \"name\": \"statistics\",\n         \"type\": \"BaseStatistics\"\n+      },\n+      {\n+        \"id\": 105,\n+        \"name\": \"segment_state\",\n+        \"type\": \"ColumnSegmentState*\"\n       }\n     ],\n+    \"set_parameters\": [\"compression_type\"],\n     \"pointer_type\": \"none\",\n     \"constructor\": [\"statistics\"]\n   },\ndiff --git a/src/include/duckdb/storage/string_uncompressed.hpp b/src/include/duckdb/storage/string_uncompressed.hpp\nindex fc0ecd9b386b..f5eab9b4fe38 100644\n--- a/src/include/duckdb/storage/string_uncompressed.hpp\n+++ b/src/include/duckdb/storage/string_uncompressed.hpp\n@@ -56,7 +56,8 @@ struct UncompressedStringStorage {\n \tstatic void StringScan(ColumnSegment &segment, ColumnScanState &state, idx_t scan_count, Vector &result);\n \tstatic void StringFetchRow(ColumnSegment &segment, ColumnFetchState &state, row_t row_id, Vector &result,\n \t                           idx_t result_idx);\n-\tstatic unique_ptr<CompressedSegmentState> StringInitSegment(ColumnSegment &segment, block_id_t block_id);\n+\tstatic unique_ptr<CompressedSegmentState> StringInitSegment(ColumnSegment &segment, block_id_t block_id,\n+\t                                                            optional_ptr<ColumnSegmentState> segment_state);\n \n \tstatic unique_ptr<CompressionAppendState> StringInitAppend(ColumnSegment &segment) {\n \t\tauto &buffer_manager = BufferManager::GetBufferManager(segment.db);\n@@ -194,5 +195,9 @@ struct UncompressedStringStorage {\n \t                                    data_ptr_t baseptr, int32_t dict_offset, uint32_t string_length);\n \tstatic string_t FetchString(ColumnSegment &segment, StringDictionaryContainer dict, Vector &result,\n \t                            data_ptr_t baseptr, string_location_t location, uint32_t string_length);\n+\n+\tstatic unique_ptr<ColumnSegmentState> SerializeState(ColumnSegment &segment);\n+\tstatic unique_ptr<ColumnSegmentState> DeserializeState(Deserializer &deserializer);\n+\tstatic void CleanupState(ColumnSegment &segment);\n };\n } // namespace duckdb\ndiff --git a/src/include/duckdb/storage/table/column_segment.hpp b/src/include/duckdb/storage/table/column_segment.hpp\nindex cb98a9eb361b..89a9e2b5e2d1 100644\n--- a/src/include/duckdb/storage/table/column_segment.hpp\n+++ b/src/include/duckdb/storage/table/column_segment.hpp\n@@ -57,7 +57,8 @@ class ColumnSegment : public SegmentBase<ColumnSegment> {\n \tstatic unique_ptr<ColumnSegment> CreatePersistentSegment(DatabaseInstance &db, BlockManager &block_manager,\n \t                                                         block_id_t id, idx_t offset, const LogicalType &type_p,\n \t                                                         idx_t start, idx_t count, CompressionType compression_type,\n-\t                                                         BaseStatistics statistics);\n+\t                                                         BaseStatistics statistics,\n+\t                                                         unique_ptr<ColumnSegmentState> segment_state);\n \tstatic unique_ptr<ColumnSegment> CreateTransientSegment(DatabaseInstance &db, const LogicalType &type, idx_t start,\n \t                                                        idx_t segment_size = Storage::BLOCK_SIZE);\n \tstatic unique_ptr<ColumnSegment> CreateSegment(ColumnSegment &other, idx_t start);\n@@ -118,14 +119,17 @@ class ColumnSegment : public SegmentBase<ColumnSegment> {\n \t\treturn row_index - this->start;\n \t}\n \n-\tCompressedSegmentState *GetSegmentState() {\n+\toptional_ptr<CompressedSegmentState> GetSegmentState() {\n \t\treturn segment_state.get();\n \t}\n \n+\tvoid CommitDropSegment();\n+\n public:\n \tColumnSegment(DatabaseInstance &db, shared_ptr<BlockHandle> block, LogicalType type, ColumnSegmentType segment_type,\n \t              idx_t start, idx_t count, CompressionFunction &function, BaseStatistics statistics,\n-\t              block_id_t block_id, idx_t offset, idx_t segment_size);\n+\t              block_id_t block_id, idx_t offset, idx_t segment_size,\n+\t              unique_ptr<ColumnSegmentState> segment_state = nullptr);\n \tColumnSegment(ColumnSegment &other, idx_t start);\n \n private:\ndiff --git a/src/include/duckdb/storage/table_storage_info.hpp b/src/include/duckdb/storage/table_storage_info.hpp\nindex 41680b192769..37282d36d42c 100644\n--- a/src/include/duckdb/storage/table_storage_info.hpp\n+++ b/src/include/duckdb/storage/table_storage_info.hpp\n@@ -28,6 +28,7 @@ struct ColumnSegmentInfo {\n \tbool persistent;\n \tblock_id_t block_id;\n \tidx_t block_offset;\n+\tstring segment_info;\n };\n \n struct IndexInfo {\ndiff --git a/src/storage/CMakeLists.txt b/src/storage/CMakeLists.txt\nindex 96c2c45c3651..9491feb50dae 100644\n--- a/src/storage/CMakeLists.txt\n+++ b/src/storage/CMakeLists.txt\n@@ -13,6 +13,7 @@ add_library_unity(\n   buffer_manager.cpp\n   checkpoint_manager.cpp\n   block.cpp\n+  data_pointer.cpp\n   data_table.cpp\n   index.cpp\n   local_storage.cpp\ndiff --git a/src/storage/checkpoint/row_group_writer.cpp b/src/storage/checkpoint/row_group_writer.cpp\nindex 9242602d06ff..18e4d1dd576e 100644\n--- a/src/storage/checkpoint/row_group_writer.cpp\n+++ b/src/storage/checkpoint/row_group_writer.cpp\n@@ -20,10 +20,7 @@ PartialBlockAllocation RowGroupWriter::GetBlockAllocation(uint32_t segment_size)\n void SingleFileRowGroupWriter::WriteColumnDataPointers(ColumnCheckpointState &column_checkpoint_state,\n                                                        Serializer &serializer) {\n \tconst auto &data_pointers = column_checkpoint_state.data_pointers;\n-\tserializer.WriteList(100, \"data_pointers\", data_pointers.size(), [&](Serializer::List &list, idx_t i) {\n-\t\tauto &data_pointer = data_pointers[i];\n-\t\tlist.WriteElement(data_pointer);\n-\t});\n+\tserializer.WriteProperty(100, \"data_pointers\", data_pointers);\n }\n \n MetadataWriter &SingleFileRowGroupWriter::GetPayloadWriter() {\ndiff --git a/src/storage/checkpoint/write_overflow_strings_to_disk.cpp b/src/storage/checkpoint/write_overflow_strings_to_disk.cpp\nindex 7e931475dcf9..855fa34d9917 100644\n--- a/src/storage/checkpoint/write_overflow_strings_to_disk.cpp\n+++ b/src/storage/checkpoint/write_overflow_strings_to_disk.cpp\n@@ -10,19 +10,42 @@ WriteOverflowStringsToDisk::WriteOverflowStringsToDisk(BlockManager &block_manag\n }\n \n WriteOverflowStringsToDisk::~WriteOverflowStringsToDisk() {\n-\tif (offset > 0) {\n-\t\tblock_manager.Write(handle.GetFileBuffer(), block_id);\n+\t// verify that the overflow writer has been flushed\n+\tD_ASSERT(Exception::UncaughtException() || offset == 0);\n+}\n+\n+shared_ptr<BlockHandle> UncompressedStringSegmentState::GetHandle(BlockManager &manager, block_id_t block_id) {\n+\tlock_guard<mutex> lock(block_lock);\n+\tauto entry = handles.find(block_id);\n+\tif (entry != handles.end()) {\n+\t\treturn entry->second;\n \t}\n+\tauto result = manager.RegisterBlock(block_id);\n+\thandles.insert(make_pair(block_id, result));\n+\treturn result;\n }\n \n-void WriteOverflowStringsToDisk::WriteString(string_t string, block_id_t &result_block, int32_t &result_offset) {\n+void UncompressedStringSegmentState::RegisterBlock(BlockManager &manager, block_id_t block_id) {\n+\tlock_guard<mutex> lock(block_lock);\n+\tauto entry = handles.find(block_id);\n+\tif (entry != handles.end()) {\n+\t\tthrow InternalException(\"UncompressedStringSegmentState::RegisterBlock - block id %llu already exists\",\n+\t\t                        block_id);\n+\t}\n+\tauto result = manager.RegisterBlock(block_id);\n+\thandles.insert(make_pair(block_id, std::move(result)));\n+\ton_disk_blocks.push_back(block_id);\n+}\n+\n+void WriteOverflowStringsToDisk::WriteString(UncompressedStringSegmentState &state, string_t string,\n+                                             block_id_t &result_block, int32_t &result_offset) {\n \tauto &buffer_manager = block_manager.buffer_manager;\n \tif (!handle.IsValid()) {\n \t\thandle = buffer_manager.Allocate(Storage::BLOCK_SIZE);\n \t}\n \t// first write the length of the string\n \tif (block_id == INVALID_BLOCK || offset + 2 * sizeof(uint32_t) >= STRING_SPACE) {\n-\t\tAllocateNewBlock(block_manager.GetFreeBlockId());\n+\t\tAllocateNewBlock(state, block_manager.GetFreeBlockId());\n \t}\n \tresult_block = block_id;\n \tresult_offset = offset;\n@@ -55,23 +78,37 @@ void WriteOverflowStringsToDisk::WriteString(string_t string, block_id_t &result\n \t\t\tstrptr += to_write;\n \t\t}\n \t\tif (remaining > 0) {\n+\t\t\tD_ASSERT(offset == WriteOverflowStringsToDisk::STRING_SPACE);\n \t\t\t// there is still remaining stuff to write\n-\t\t\t// first get the new block id and write it to the end of the previous block\n-\t\t\tauto new_block_id = block_manager.GetFreeBlockId();\n-\t\t\tStore<block_id_t>(new_block_id, data_ptr + offset);\n \t\t\t// now write the current block to disk and allocate a new block\n-\t\t\tAllocateNewBlock(new_block_id);\n+\t\t\tAllocateNewBlock(state, block_manager.GetFreeBlockId());\n \t\t}\n \t}\n }\n \n-void WriteOverflowStringsToDisk::AllocateNewBlock(block_id_t new_block_id) {\n+void WriteOverflowStringsToDisk::Flush() {\n+\tif (block_id != INVALID_BLOCK && offset > 0) {\n+\t\t// zero-initialize the empty part of the overflow string buffer (if any)\n+\t\tif (offset < STRING_SPACE) {\n+\t\t\tmemset(handle.Ptr() + offset, 0, STRING_SPACE - offset);\n+\t\t}\n+\t\t// write to disk\n+\t\tblock_manager.Write(handle.GetFileBuffer(), block_id);\n+\t}\n+\tblock_id = INVALID_BLOCK;\n+\toffset = 0;\n+}\n+\n+void WriteOverflowStringsToDisk::AllocateNewBlock(UncompressedStringSegmentState &state, block_id_t new_block_id) {\n \tif (block_id != INVALID_BLOCK) {\n \t\t// there is an old block, write it first\n-\t\tblock_manager.Write(handle.GetFileBuffer(), block_id);\n+\t\t// write the new block id at the end of the previous block\n+\t\tStore<block_id_t>(new_block_id, handle.Ptr() + WriteOverflowStringsToDisk::STRING_SPACE);\n+\t\tFlush();\n \t}\n \toffset = 0;\n \tblock_id = new_block_id;\n+\tstate.RegisterBlock(block_manager, new_block_id);\n }\n \n } // namespace duckdb\ndiff --git a/src/storage/compression/fixed_size_uncompressed.cpp b/src/storage/compression/fixed_size_uncompressed.cpp\nindex 8b12c251f635..ea77f9269992 100644\n--- a/src/storage/compression/fixed_size_uncompressed.cpp\n+++ b/src/storage/compression/fixed_size_uncompressed.cpp\n@@ -65,7 +65,7 @@ void UncompressedCompressState::CreateEmptySegment(idx_t row_start) {\n \tauto compressed_segment = ColumnSegment::CreateTransientSegment(db, type, row_start);\n \tif (type.InternalType() == PhysicalType::VARCHAR) {\n \t\tauto &state = compressed_segment->GetSegmentState()->Cast<UncompressedStringSegmentState>();\n-\t\tstate.overflow_writer = make_uniq<WriteOverflowStringsToDisk>(checkpointer.GetColumnData().GetBlockManager());\n+\t\tstate.overflow_writer = make_uniq<WriteOverflowStringsToDisk>(checkpointer.GetRowGroup().GetBlockManager());\n \t}\n \tcurrent_segment = std::move(compressed_segment);\n \tcurrent_segment->InitializeAppend(append_state);\n@@ -73,6 +73,11 @@ void UncompressedCompressState::CreateEmptySegment(idx_t row_start) {\n \n void UncompressedCompressState::FlushSegment(idx_t segment_size) {\n \tauto &state = checkpointer.GetCheckpointState();\n+\tif (current_segment->type.InternalType() == PhysicalType::VARCHAR) {\n+\t\tauto &segment_state = current_segment->GetSegmentState()->Cast<UncompressedStringSegmentState>();\n+\t\tsegment_state.overflow_writer->Flush();\n+\t\tsegment_state.overflow_writer.reset();\n+\t}\n \tstate.FlushSegment(std::move(current_segment), segment_size);\n }\n \ndiff --git a/src/storage/compression/string_uncompressed.cpp b/src/storage/compression/string_uncompressed.cpp\nindex 3963447587a3..36cb32309756 100644\n--- a/src/storage/compression/string_uncompressed.cpp\n+++ b/src/storage/compression/string_uncompressed.cpp\n@@ -3,6 +3,9 @@\n #include \"duckdb/common/pair.hpp\"\n #include \"duckdb/storage/checkpoint/write_overflow_strings_to_disk.hpp\"\n #include \"miniz_wrapper.hpp\"\n+#include \"duckdb/common/serializer/serializer.hpp\"\n+#include \"duckdb/common/serializer/deserializer.hpp\"\n+#include \"duckdb/storage/table/column_data.hpp\"\n \n namespace duckdb {\n \n@@ -141,9 +144,22 @@ void UncompressedStringStorage::StringFetchRow(ColumnSegment &segment, ColumnFet\n //===--------------------------------------------------------------------===//\n // Append\n //===--------------------------------------------------------------------===//\n+struct SerializedStringSegmentState : public ColumnSegmentState {\n+\tSerializedStringSegmentState() {\n+\t}\n+\texplicit SerializedStringSegmentState(vector<block_id_t> blocks_p) : blocks(std::move(blocks_p)) {\n+\t}\n+\n+\tvector<block_id_t> blocks;\n+\n+\tvoid Serialize(Serializer &serializer) const override {\n+\t\tserializer.WriteProperty(1, \"overflow_blocks\", blocks);\n+\t}\n+};\n \n-unique_ptr<CompressedSegmentState> UncompressedStringStorage::StringInitSegment(ColumnSegment &segment,\n-                                                                                block_id_t block_id) {\n+unique_ptr<CompressedSegmentState>\n+UncompressedStringStorage::StringInitSegment(ColumnSegment &segment, block_id_t block_id,\n+                                             optional_ptr<ColumnSegmentState> segment_state) {\n \tauto &buffer_manager = BufferManager::GetBufferManager(segment.db);\n \tif (block_id == INVALID_BLOCK) {\n \t\tauto handle = buffer_manager.Pin(segment.block);\n@@ -152,7 +168,12 @@ unique_ptr<CompressedSegmentState> UncompressedStringStorage::StringInitSegment(\n \t\tdictionary.end = segment.SegmentSize();\n \t\tSetDictionary(segment, handle, dictionary);\n \t}\n-\treturn make_uniq<UncompressedStringSegmentState>();\n+\tauto result = make_uniq<UncompressedStringSegmentState>();\n+\tif (segment_state) {\n+\t\tauto &serialized_state = segment_state->Cast<SerializedStringSegmentState>();\n+\t\tresult->on_disk_blocks = std::move(serialized_state.blocks);\n+\t}\n+\treturn std::move(result);\n }\n \n idx_t UncompressedStringStorage::FinalizeAppend(ColumnSegment &segment, SegmentStatistics &stats) {\n@@ -179,6 +200,32 @@ idx_t UncompressedStringStorage::FinalizeAppend(ColumnSegment &segment, SegmentS\n \treturn total_size;\n }\n \n+//===--------------------------------------------------------------------===//\n+// Serialization & Cleanup\n+//===--------------------------------------------------------------------===//\n+unique_ptr<ColumnSegmentState> UncompressedStringStorage::SerializeState(ColumnSegment &segment) {\n+\tauto &state = segment.GetSegmentState()->Cast<UncompressedStringSegmentState>();\n+\tif (state.on_disk_blocks.empty()) {\n+\t\t// no on-disk blocks - nothing to write\n+\t\treturn nullptr;\n+\t}\n+\treturn make_uniq<SerializedStringSegmentState>(state.on_disk_blocks);\n+}\n+\n+unique_ptr<ColumnSegmentState> UncompressedStringStorage::DeserializeState(Deserializer &deserializer) {\n+\tauto result = make_uniq<SerializedStringSegmentState>();\n+\tdeserializer.ReadProperty(1, \"overflow_blocks\", result->blocks);\n+\treturn std::move(result);\n+}\n+\n+void UncompressedStringStorage::CleanupState(ColumnSegment &segment) {\n+\tauto &state = segment.GetSegmentState()->Cast<UncompressedStringSegmentState>();\n+\tauto &block_manager = segment.GetBlockManager();\n+\tfor (auto &block_id : state.on_disk_blocks) {\n+\t\tblock_manager.MarkBlockAsModified(block_id);\n+\t}\n+}\n+\n //===--------------------------------------------------------------------===//\n // Get Function\n //===--------------------------------------------------------------------===//\n@@ -192,7 +239,9 @@ CompressionFunction StringUncompressed::GetFunction(PhysicalType data_type) {\n \t                           UncompressedStringStorage::StringScanPartial, UncompressedStringStorage::StringFetchRow,\n \t                           UncompressedFunctions::EmptySkip, UncompressedStringStorage::StringInitSegment,\n \t                           UncompressedStringStorage::StringInitAppend, UncompressedStringStorage::StringAppend,\n-\t                           UncompressedStringStorage::FinalizeAppend);\n+\t                           UncompressedStringStorage::FinalizeAppend, nullptr,\n+\t                           UncompressedStringStorage::SerializeState, UncompressedStringStorage::DeserializeState,\n+\t                           UncompressedStringStorage::CleanupState);\n }\n \n //===--------------------------------------------------------------------===//\n@@ -226,7 +275,7 @@ void UncompressedStringStorage::WriteString(ColumnSegment &segment, string_t str\n \tauto &state = segment.GetSegmentState()->Cast<UncompressedStringSegmentState>();\n \tif (state.overflow_writer) {\n \t\t// overflow writer is set: write string there\n-\t\tstate.overflow_writer->WriteString(string, result_block, result_offset);\n+\t\tstate.overflow_writer->WriteString(state, string, result_block, result_offset);\n \t} else {\n \t\t// default overflow behavior: use in-memory buffer to store the overflow string\n \t\tWriteStringMemory(segment, string, result_block, result_offset);\n@@ -251,7 +300,7 @@ void UncompressedStringStorage::WriteStringMemory(ColumnSegment &segment, string\n \t\tnew_block->size = alloc_size;\n \t\t// allocate an in-memory buffer for it\n \t\thandle = buffer_manager.Allocate(alloc_size, false, &block);\n-\t\tstate.overflow_blocks[block->BlockId()] = new_block.get();\n+\t\tstate.overflow_blocks.insert(make_pair(block->BlockId(), reference<StringBlock>(*new_block)));\n \t\tnew_block->block = std::move(block);\n \t\tnew_block->next = std::move(state.head);\n \t\tstate.head = std::move(new_block);\n@@ -282,7 +331,7 @@ string_t UncompressedStringStorage::ReadOverflowString(ColumnSegment &segment, V\n \tif (block < MAXIMUM_BLOCK) {\n \t\t// read the overflow string from disk\n \t\t// pin the initial handle and read the length\n-\t\tauto block_handle = block_manager.RegisterBlock(block);\n+\t\tauto block_handle = state.GetHandle(block_manager, block);\n \t\tauto handle = buffer_manager.Pin(block_handle);\n \n \t\t// read header\n@@ -295,7 +344,7 @@ string_t UncompressedStringStorage::ReadOverflowString(ColumnSegment &segment, V\n \t\tunsafe_unique_array<data_t> decompression_buffer;\n \n \t\t// If string is in single block we decompress straight from it, else we copy first\n-\t\tif (remaining <= Storage::BLOCK_SIZE - sizeof(block_id_t) - offset) {\n+\t\tif (remaining <= WriteOverflowStringsToDisk::STRING_SPACE - offset) {\n \t\t\tdecompression_ptr = handle.Ptr() + offset;\n \t\t} else {\n \t\t\tdecompression_buffer = make_unsafe_uniq_array<data_t>(compressed_size);\n@@ -303,7 +352,7 @@ string_t UncompressedStringStorage::ReadOverflowString(ColumnSegment &segment, V\n \n \t\t\t// now append the string to the single buffer\n \t\t\twhile (remaining > 0) {\n-\t\t\t\tidx_t to_write = MinValue<idx_t>(remaining, Storage::BLOCK_SIZE - sizeof(block_id_t) - offset);\n+\t\t\t\tidx_t to_write = MinValue<idx_t>(remaining, WriteOverflowStringsToDisk::STRING_SPACE - offset);\n \t\t\t\tmemcpy(target_ptr, handle.Ptr() + offset, to_write);\n \n \t\t\t\tremaining -= to_write;\n@@ -311,8 +360,9 @@ string_t UncompressedStringStorage::ReadOverflowString(ColumnSegment &segment, V\n \t\t\t\ttarget_ptr += to_write;\n \t\t\t\tif (remaining > 0) {\n \t\t\t\t\t// read the next block\n-\t\t\t\t\tblock_id_t next_block = Load<block_id_t>(handle.Ptr() + offset);\n-\t\t\t\t\tblock_handle = block_manager.RegisterBlock(next_block);\n+\t\t\t\t\tD_ASSERT(offset == WriteOverflowStringsToDisk::STRING_SPACE);\n+\t\t\t\t\tblock_id_t next_block = Load<block_id_t>(handle.Ptr() + WriteOverflowStringsToDisk::STRING_SPACE);\n+\t\t\t\t\tblock_handle = state.GetHandle(block_manager, next_block);\n \t\t\t\t\thandle = buffer_manager.Pin(block_handle);\n \t\t\t\t\toffset = 0;\n \t\t\t\t}\n@@ -336,7 +386,7 @@ string_t UncompressedStringStorage::ReadOverflowString(ColumnSegment &segment, V\n \t\t// first pin the handle, if it is not pinned yet\n \t\tauto entry = state.overflow_blocks.find(block);\n \t\tD_ASSERT(entry != state.overflow_blocks.end());\n-\t\tauto handle = buffer_manager.Pin(entry->second->block);\n+\t\tauto handle = buffer_manager.Pin(entry->second.get().block);\n \t\tauto final_buffer = handle.Ptr();\n \t\tStringVector::AddHandle(result, std::move(handle));\n \t\treturn ReadStringWithLength(final_buffer, offset);\ndiff --git a/src/storage/compression/validity_uncompressed.cpp b/src/storage/compression/validity_uncompressed.cpp\nindex 4046418e6f6b..047b56839b22 100644\n--- a/src/storage/compression/validity_uncompressed.cpp\n+++ b/src/storage/compression/validity_uncompressed.cpp\n@@ -398,7 +398,8 @@ static unique_ptr<CompressionAppendState> ValidityInitAppend(ColumnSegment &segm\n \treturn make_uniq<CompressionAppendState>(std::move(handle));\n }\n \n-unique_ptr<CompressedSegmentState> ValidityInitSegment(ColumnSegment &segment, block_id_t block_id) {\n+unique_ptr<CompressedSegmentState> ValidityInitSegment(ColumnSegment &segment, block_id_t block_id,\n+                                                       optional_ptr<ColumnSegmentState> segment_state) {\n \tauto &buffer_manager = BufferManager::GetBufferManager(segment.db);\n \tif (block_id == INVALID_BLOCK) {\n \t\tauto handle = buffer_manager.Pin(segment.block);\ndiff --git a/src/storage/data_pointer.cpp b/src/storage/data_pointer.cpp\nnew file mode 100644\nindex 000000000000..29718e72bbad\n--- /dev/null\n+++ b/src/storage/data_pointer.cpp\n@@ -0,0 +1,20 @@\n+#include \"duckdb/storage/data_pointer.hpp\"\n+#include \"duckdb/common/serializer/serializer.hpp\"\n+#include \"duckdb/common/serializer/deserializer.hpp\"\n+#include \"duckdb/main/config.hpp\"\n+#include \"duckdb/function/compression_function.hpp\"\n+\n+namespace duckdb {\n+\n+unique_ptr<ColumnSegmentState> ColumnSegmentState::Deserialize(Deserializer &deserializer) {\n+\tauto compression_type = deserializer.Get<CompressionType>();\n+\tauto &db = deserializer.Get<DatabaseInstance &>();\n+\tauto &type = deserializer.Get<LogicalType &>();\n+\tauto compression_function = DBConfig::GetConfig(db).GetCompressionFunction(compression_type, type.InternalType());\n+\tif (!compression_function || !compression_function->deserialize_state) {\n+\t\tthrow SerializationException(\"Deserializing a ColumnSegmentState but could not find deserialize method\");\n+\t}\n+\treturn compression_function->deserialize_state(deserializer);\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/storage/local_storage.cpp b/src/storage/local_storage.cpp\nindex c34bca263d80..251403c796d2 100644\n--- a/src/storage/local_storage.cpp\n+++ b/src/storage/local_storage.cpp\n@@ -74,17 +74,13 @@ LocalTableStorage::~LocalTableStorage() {\n \n void LocalTableStorage::InitializeScan(CollectionScanState &state, optional_ptr<TableFilterSet> table_filters) {\n \tif (row_groups->GetTotalRows() == 0) {\n-\t\t// nothing to scan\n-\t\treturn;\n+\t\tthrow InternalException(\"No rows in LocalTableStorage row group for scan\");\n \t}\n \trow_groups->InitializeScan(state, state.GetColumnIds(), table_filters.get());\n }\n \n idx_t LocalTableStorage::EstimatedSize() {\n \tidx_t appended_rows = row_groups->GetTotalRows() - deleted_rows;\n-\tif (appended_rows == 0) {\n-\t\treturn 0;\n-\t}\n \tidx_t row_size = 0;\n \tauto &types = row_groups->GetTypes();\n \tfor (auto &type : types) {\n@@ -169,10 +165,10 @@ void LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, TableAppen\n \t\t\t} catch (Exception &ex) {\n \t\t\t\terror = PreservedError(ex);\n \t\t\t\treturn false;\n-\t\t\t} catch (std::exception &ex) {\n+\t\t\t} catch (std::exception &ex) { // LCOV_EXCL_START\n \t\t\t\terror = PreservedError(ex);\n \t\t\t\treturn false;\n-\t\t\t}\n+\t\t\t} // LCOV_EXCL_STOP\n \n \t\t\tcurrent_row += chunk.size();\n \t\t\tif (current_row >= append_state.current_row) {\ndiff --git a/src/storage/serialization/serialize_storage.cpp b/src/storage/serialization/serialize_storage.cpp\nindex 369b96544931..27e6fad827f5 100644\n--- a/src/storage/serialization/serialize_storage.cpp\n+++ b/src/storage/serialization/serialize_storage.cpp\n@@ -29,6 +29,7 @@ void DataPointer::Serialize(Serializer &serializer) const {\n \tserializer.WriteProperty(102, \"block_pointer\", block_pointer);\n \tserializer.WriteProperty(103, \"compression_type\", compression_type);\n \tserializer.WriteProperty(104, \"statistics\", statistics);\n+\tserializer.WriteProperty(105, \"segment_state\", segment_state);\n }\n \n DataPointer DataPointer::Deserialize(Deserializer &deserializer) {\n@@ -42,6 +43,9 @@ DataPointer DataPointer::Deserialize(Deserializer &deserializer) {\n \tresult.tuple_count = tuple_count;\n \tresult.block_pointer = block_pointer;\n \tresult.compression_type = compression_type;\n+\tdeserializer.Set<CompressionType>(compression_type);\n+\tdeserializer.ReadProperty(105, \"segment_state\", result.segment_state);\n+\tdeserializer.Unset<CompressionType>();\n \treturn result;\n }\n \ndiff --git a/src/storage/single_file_block_manager.cpp b/src/storage/single_file_block_manager.cpp\nindex e8d0d5f135e0..b82572759430 100644\n--- a/src/storage/single_file_block_manager.cpp\n+++ b/src/storage/single_file_block_manager.cpp\n@@ -391,18 +391,23 @@ void SingleFileBlockManager::Truncate() {\n vector<MetadataHandle> SingleFileBlockManager::GetFreeListBlocks() {\n \tvector<MetadataHandle> free_list_blocks;\n \n-\tauto free_list_size = sizeof(uint64_t) + sizeof(block_id_t) * (free_list.size() + modified_blocks.size());\n-\tauto multi_use_blocks_size = sizeof(uint64_t) + (sizeof(block_id_t) + sizeof(uint32_t)) * multi_use_blocks.size();\n-\tauto metadata_blocks = sizeof(uint64_t) + (sizeof(idx_t) * 2) * GetMetadataManager().BlockCount();\n-\tauto total_size = free_list_size + multi_use_blocks_size + metadata_blocks;\n-\n-\t// reserve the blocks that we are going to write\n+\t// reserve all blocks that we are going to write the free list to\n \t// since these blocks are no longer free we cannot just include them in the free list!\n \tauto block_size = MetadataManager::METADATA_BLOCK_SIZE - sizeof(idx_t);\n-\twhile (total_size > 0) {\n+\tidx_t allocated_size = 0;\n+\twhile (true) {\n+\t\tauto free_list_size = sizeof(uint64_t) + sizeof(block_id_t) * (free_list.size() + modified_blocks.size());\n+\t\tauto multi_use_blocks_size =\n+\t\t    sizeof(uint64_t) + (sizeof(block_id_t) + sizeof(uint32_t)) * multi_use_blocks.size();\n+\t\tauto metadata_blocks =\n+\t\t    sizeof(uint64_t) + (sizeof(block_id_t) + sizeof(idx_t)) * GetMetadataManager().BlockCount();\n+\t\tauto total_size = free_list_size + multi_use_blocks_size + metadata_blocks;\n+\t\tif (total_size < allocated_size) {\n+\t\t\tbreak;\n+\t\t}\n \t\tauto free_list_handle = GetMetadataManager().AllocateHandle();\n \t\tfree_list_blocks.push_back(std::move(free_list_handle));\n-\t\ttotal_size -= MinValue<idx_t>(total_size, block_size);\n+\t\tallocated_size += block_size;\n \t}\n \n \treturn free_list_blocks;\ndiff --git a/src/storage/storage_info.cpp b/src/storage/storage_info.cpp\nindex d931608b449c..2d417efc339b 100644\n--- a/src/storage/storage_info.cpp\n+++ b/src/storage/storage_info.cpp\n@@ -2,7 +2,7 @@\n \n namespace duckdb {\n \n-const uint64_t VERSION_NUMBER = 60;\n+const uint64_t VERSION_NUMBER = 61;\n \n struct StorageVersionInfo {\n \tconst char *version_name;\ndiff --git a/src/storage/table/column_checkpoint_state.cpp b/src/storage/table/column_checkpoint_state.cpp\nindex 194ee11e51c0..88d803b77d08 100644\n--- a/src/storage/table/column_checkpoint_state.cpp\n+++ b/src/storage/table/column_checkpoint_state.cpp\n@@ -181,6 +181,9 @@ void ColumnCheckpointState::FlushSegment(unique_ptr<ColumnSegment> segment, idx_\n \t}\n \tdata_pointer.tuple_count = tuple_count;\n \tdata_pointer.compression_type = segment->function.get().type;\n+\tif (segment->function.get().serialize_state) {\n+\t\tdata_pointer.segment_state = segment->function.get().serialize_state(*segment);\n+\t}\n \n \t// append the segment to the new segment tree\n \tnew_tree.AppendSegment(std::move(segment));\ndiff --git a/src/storage/table/column_data.cpp b/src/storage/table/column_data.cpp\nindex 773d0b1da73e..d6a1bd938035 100644\n--- a/src/storage/table/column_data.cpp\n+++ b/src/storage/table/column_data.cpp\n@@ -402,12 +402,7 @@ void ColumnData::AppendTransientSegment(SegmentLock &l, idx_t start_row) {\n void ColumnData::CommitDropColumn() {\n \tfor (auto &segment_p : data.Segments()) {\n \t\tauto &segment = segment_p;\n-\t\tif (segment.segment_type == ColumnSegmentType::PERSISTENT) {\n-\t\t\tauto block_id = segment.GetBlockId();\n-\t\t\tif (block_id != INVALID_BLOCK) {\n-\t\t\t\tblock_manager.MarkBlockAsModified(block_id);\n-\t\t\t}\n-\t\t}\n+\t\tsegment.CommitDropSegment();\n \t}\n }\n \n@@ -453,12 +448,18 @@ unique_ptr<ColumnCheckpointState> ColumnData::Checkpoint(RowGroup &row_group,\n \n void ColumnData::DeserializeColumn(Deserializer &deserializer) {\n \t// load the data pointers for the column\n-\tthis->count = 0;\n+\tdeserializer.Set<DatabaseInstance &>(info.db.GetDatabase());\n \tdeserializer.Set<LogicalType &>(type);\n \n-\tdeserializer.ReadList(100, \"data_pointers\", [&](Deserializer::List &list, idx_t i) {\n-\t\tauto data_pointer = list.ReadElement<DataPointer>();\n+\tvector<DataPointer> data_pointers;\n+\tdeserializer.ReadProperty(100, \"data_pointers\", data_pointers);\n+\n+\tdeserializer.Unset<DatabaseInstance>();\n+\tdeserializer.Unset<LogicalType>();\n \n+\t// construct the segments based on the data pointers\n+\tthis->count = 0;\n+\tfor (auto &data_pointer : data_pointers) {\n \t\t// Update the count and statistics\n \t\tthis->count += data_pointer.tuple_count;\n \t\tif (stats) {\n@@ -469,12 +470,10 @@ void ColumnData::DeserializeColumn(Deserializer &deserializer) {\n \t\tauto segment = ColumnSegment::CreatePersistentSegment(\n \t\t    GetDatabase(), block_manager, data_pointer.block_pointer.block_id, data_pointer.block_pointer.offset, type,\n \t\t    data_pointer.row_start, data_pointer.tuple_count, data_pointer.compression_type,\n-\t\t    std::move(data_pointer.statistics));\n+\t\t    std::move(data_pointer.statistics), std::move(data_pointer.segment_state));\n \n \t\tdata.AppendSegment(std::move(segment));\n-\t});\n-\n-\tdeserializer.Unset<LogicalType>();\n+\t}\n }\n \n shared_ptr<ColumnData> ColumnData::Deserialize(BlockManager &block_manager, DataTableInfo &info, idx_t column_index,\n@@ -530,10 +529,14 @@ void ColumnData::GetColumnSegmentInfo(idx_t row_group_index, vector<idx_t> col_p\n \t\t} else {\n \t\t\tcolumn_info.persistent = false;\n \t\t}\n+\t\tauto segment_state = segment->GetSegmentState();\n+\t\tif (segment_state) {\n+\t\t\tcolumn_info.segment_info = segment_state->GetSegmentInfo();\n+\t\t}\n \t\tresult.emplace_back(column_info);\n \n \t\tsegment_idx++;\n-\t\tsegment = (ColumnSegment *)data.GetNextSegment(segment);\n+\t\tsegment = data.GetNextSegment(segment);\n \t}\n }\n \ndiff --git a/src/storage/table/column_data_checkpointer.cpp b/src/storage/table/column_data_checkpointer.cpp\nindex bc2ab5335ddd..271c67eb6e96 100644\n--- a/src/storage/table/column_data_checkpointer.cpp\n+++ b/src/storage/table/column_data_checkpointer.cpp\n@@ -168,16 +168,9 @@ void ColumnDataCheckpointer::WriteToDisk() {\n \t// first we check the current segments\n \t// if there are any persistent segments, we will mark their old block ids as modified\n \t// since the segments will be rewritten their old on disk data is no longer required\n-\tauto &block_manager = col_data.GetBlockManager();\n \tfor (idx_t segment_idx = 0; segment_idx < nodes.size(); segment_idx++) {\n \t\tauto segment = nodes[segment_idx].node.get();\n-\t\tif (segment->segment_type == ColumnSegmentType::PERSISTENT) {\n-\t\t\t// persistent segment has updates: mark it as modified and rewrite the block with the merged updates\n-\t\t\tauto block_id = segment->GetBlockId();\n-\t\t\tif (block_id != INVALID_BLOCK) {\n-\t\t\t\tblock_manager.MarkBlockAsModified(block_id);\n-\t\t\t}\n-\t\t}\n+\t\tsegment->CommitDropSegment();\n \t}\n \n \t// now we need to write our segment\n@@ -231,6 +224,9 @@ void ColumnDataCheckpointer::WritePersistentSegments() {\n \t\tpointer.row_start = segment->start;\n \t\tpointer.tuple_count = segment->count;\n \t\tpointer.compression_type = segment->function.get().type;\n+\t\tif (segment->function.get().serialize_state) {\n+\t\t\tpointer.segment_state = segment->function.get().serialize_state(*segment);\n+\t\t}\n \n \t\t// merge the persistent stats into the global column stats\n \t\tstate.global_stats->Merge(segment->stats.statistics);\ndiff --git a/src/storage/table/column_segment.cpp b/src/storage/table/column_segment.cpp\nindex a0d560534cd6..b97c12291f66 100644\n--- a/src/storage/table/column_segment.cpp\n+++ b/src/storage/table/column_segment.cpp\n@@ -9,6 +9,7 @@\n #include \"duckdb/planner/filter/constant_filter.hpp\"\n #include \"duckdb/main/config.hpp\"\n #include \"duckdb/storage/table/scan_state.hpp\"\n+#include \"duckdb/storage/data_pointer.hpp\"\n \n #include <cstring>\n \n@@ -18,7 +19,8 @@ unique_ptr<ColumnSegment> ColumnSegment::CreatePersistentSegment(DatabaseInstanc\n                                                                  block_id_t block_id, idx_t offset,\n                                                                  const LogicalType &type, idx_t start, idx_t count,\n                                                                  CompressionType compression_type,\n-                                                                 BaseStatistics statistics) {\n+                                                                 BaseStatistics statistics,\n+                                                                 unique_ptr<ColumnSegmentState> segment_state) {\n \tauto &config = DBConfig::GetConfig(db);\n \toptional_ptr<CompressionFunction> function;\n \tshared_ptr<BlockHandle> block;\n@@ -31,7 +33,7 @@ unique_ptr<ColumnSegment> ColumnSegment::CreatePersistentSegment(DatabaseInstanc\n \t}\n \tauto segment_size = Storage::BLOCK_SIZE;\n \treturn make_uniq<ColumnSegment>(db, std::move(block), type, ColumnSegmentType::PERSISTENT, start, count, *function,\n-\t                                std::move(statistics), block_id, offset, segment_size);\n+\t                                std::move(statistics), block_id, offset, segment_size, std::move(segment_state));\n }\n \n unique_ptr<ColumnSegment> ColumnSegment::CreateTransientSegment(DatabaseInstance &db, const LogicalType &type,\n@@ -56,13 +58,14 @@ unique_ptr<ColumnSegment> ColumnSegment::CreateSegment(ColumnSegment &other, idx\n \n ColumnSegment::ColumnSegment(DatabaseInstance &db, shared_ptr<BlockHandle> block, LogicalType type_p,\n                              ColumnSegmentType segment_type, idx_t start, idx_t count, CompressionFunction &function_p,\n-                             BaseStatistics statistics, block_id_t block_id_p, idx_t offset_p, idx_t segment_size_p)\n+                             BaseStatistics statistics, block_id_t block_id_p, idx_t offset_p, idx_t segment_size_p,\n+                             unique_ptr<ColumnSegmentState> segment_state)\n     : SegmentBase<ColumnSegment>(start, count), db(db), type(std::move(type_p)),\n       type_size(GetTypeIdSize(type.InternalType())), segment_type(segment_type), function(function_p),\n       stats(std::move(statistics)), block(std::move(block)), block_id(block_id_p), offset(offset_p),\n       segment_size(segment_size_p) {\n \tif (function.get().init_segment) {\n-\t\tsegment_state = function.get().init_segment(*this, block_id);\n+\t\tthis->segment_state = function.get().init_segment(*this, block_id, segment_state.get());\n \t}\n }\n \n@@ -190,11 +193,6 @@ void ColumnSegment::ConvertToPersistent(optional_ptr<BlockManager> block_manager\n \t\t// instead of copying the data we alter some metadata so the buffer points to an on-disk block\n \t\tblock = block_manager->ConvertToPersistent(block_id, std::move(block));\n \t}\n-\n-\tsegment_state.reset();\n-\tif (function.get().init_segment) {\n-\t\tsegment_state = function.get().init_segment(*this, block_id);\n-\t}\n }\n \n void ColumnSegment::MarkAsPersistent(shared_ptr<BlockHandle> block_p, uint32_t offset_p) {\n@@ -204,10 +202,21 @@ void ColumnSegment::MarkAsPersistent(shared_ptr<BlockHandle> block_p, uint32_t o\n \tblock_id = block_p->BlockId();\n \toffset = offset_p;\n \tblock = std::move(block_p);\n+}\n \n-\tsegment_state.reset();\n-\tif (function.get().init_segment) {\n-\t\tsegment_state = function.get().init_segment(*this, block_id);\n+//===--------------------------------------------------------------------===//\n+// Drop Segment\n+//===--------------------------------------------------------------------===//\n+void ColumnSegment::CommitDropSegment() {\n+\tif (segment_type != ColumnSegmentType::PERSISTENT) {\n+\t\t// not persistent\n+\t\treturn;\n+\t}\n+\tif (block_id != INVALID_BLOCK) {\n+\t\tGetBlockManager().MarkBlockAsModified(block_id);\n+\t}\n+\tif (function.get().cleanup_state) {\n+\t\tfunction.get().cleanup_state(*this);\n \t}\n }\n \ndiff --git a/tools/shell/linenoise.cpp b/tools/shell/linenoise.cpp\nindex f0c7512cc934..2b07a0c88f26 100644\n--- a/tools/shell/linenoise.cpp\n+++ b/tools/shell/linenoise.cpp\n@@ -130,7 +130,7 @@\n #endif\n \n #define LINENOISE_DEFAULT_HISTORY_MAX_LEN 100\n-#define LINENOISE_MAX_LINE                4096\n+#define LINENOISE_MAX_LINE                20480\n static const char *unsupported_term[] = {\"dumb\", \"cons25\", \"emacs\", NULL};\n static linenoiseCompletionCallback *completionCallback = NULL;\n static linenoiseHintsCallback *hintsCallback = NULL;\n",
  "test_patch": "diff --git a/test/sql/storage/compression/patas/patas_min_max.test b/test/sql/storage/compression/patas/patas_min_max.test\nindex b5cdc77e995e..2ce521c6905c 100644\n--- a/test/sql/storage/compression/patas/patas_min_max.test\n+++ b/test/sql/storage/compression/patas/patas_min_max.test\n@@ -24,7 +24,7 @@ INSERT INTO all_types SELECT ${type} FROM all_types;\n statement ok\n checkpoint\n \n-query IIIIIIIIIIIIII\n+query IIIIIIIIIIIIIII\n SELECT * FROM pragma_storage_info('all_types') WHERE segment_type == '${type}' AND compression != 'Patas';\n ----\n \ndiff --git a/test/sql/storage/optimistic_write/optimistic_write_large_strings.test b/test/sql/storage/optimistic_write/optimistic_write_large_strings.test\nnew file mode 100644\nindex 000000000000..9b5aa6eb76f1\n--- /dev/null\n+++ b/test/sql/storage/optimistic_write/optimistic_write_large_strings.test\n@@ -0,0 +1,43 @@\n+# name: test/sql/storage/optimistic_write/optimistic_write_large_strings.test\n+# description: Issue #7887 - Test writing large strings using batch insertion\n+# group: [optimistic_write]\n+\n+require parquet\n+\n+# load the DB from disk\n+load __TEST_DIR__/optimistic_write_large_strings.db\n+\n+statement ok\n+CREATE TABLE test(val VARCHAR);\n+\n+statement ok\n+INSERT INTO test VALUES (NULL);\n+\n+statement ok\n+COPY (SELECT repeat('X', len) FROM (VALUES (903), (4932)) t(len)) TO '__TEST_DIR__/large_strings.parquet';\n+\n+statement ok\n+INSERT INTO test FROM read_parquet([\n+\t'__TEST_DIR__/large_strings.parquet',\n+\t'__TEST_DIR__/large_strings.parquet']\n+);\n+\n+query I\n+SELECT strlen(val) FROM test\n+----\n+NULL\n+903\n+4932\n+903\n+4932\n+\n+restart\n+\n+query I\n+SELECT strlen(val) FROM test\n+----\n+NULL\n+903\n+4932\n+903\n+4932\ndiff --git a/test/sql/storage/optimistic_write/optimistic_write_large_strings_deletes.test_slow b/test/sql/storage/optimistic_write/optimistic_write_large_strings_deletes.test_slow\nnew file mode 100644\nindex 000000000000..de1db54a084c\n--- /dev/null\n+++ b/test/sql/storage/optimistic_write/optimistic_write_large_strings_deletes.test_slow\n@@ -0,0 +1,54 @@\n+# name: test/sql/storage/optimistic_write/optimistic_write_large_strings_deletes.test_slow\n+# description: Test writing large strings with deletes\n+# group: [optimistic_write]\n+\n+require parquet\n+\n+# load the DB from disk\n+load __TEST_DIR__/optimistic_write_large_strings_deletes.db\n+\n+statement ok\n+CREATE TABLE test(val VARCHAR);\n+\n+statement ok\n+INSERT INTO test VALUES (NULL);\n+\n+statement ok\n+COPY (SELECT 'hello' AS str FROM range(250000)) TO '__TEST_DIR__/small_strings.parquet';\n+\n+statement ok\n+COPY (SELECT repeat('X', len) AS str FROM (VALUES (903), (4932)) t(len)) TO '__TEST_DIR__/large_strings.parquet';\n+\n+statement ok\n+BEGIN\n+\n+statement ok\n+INSERT INTO test FROM read_parquet([\n+\t'__TEST_DIR__/small_strings.parquet',\n+\t'__TEST_DIR__/large_strings.parquet']\n+);\n+\n+query I\n+SELECT MAX(strlen(val)) FROM test\n+----\n+4932\n+\n+query I\n+DELETE FROM test WHERE val='hello'\n+----\n+250000\n+\n+query I\n+SELECT MAX(strlen(val)) FROM test\n+----\n+4932\n+\n+statement ok\n+COMMIT\n+\n+query I\n+SELECT strlen(val) FROM test\n+----\n+NULL\n+903\n+4932\ndiff --git a/test/sql/storage/overflow_strings/load_overflow_strings_slowly.test b/test/sql/storage/overflow_strings/load_overflow_strings_slowly.test\nnew file mode 100644\nindex 000000000000..24550f4b9f69\n--- /dev/null\n+++ b/test/sql/storage/overflow_strings/load_overflow_strings_slowly.test\n@@ -0,0 +1,39 @@\n+# name: test/sql/storage/overflow_strings/load_overflow_strings_slowly.test\n+# description: Test loading overflow strings in small batches\n+# group: [overflow_strings]\n+\n+load __TEST_DIR__/load_overflow_strings.db\n+\n+# FIXME - there is an issue with metadata not being freed correctly causing this test to fail in --force-reload\n+require skip_reload\n+\n+loop x 0 10\n+\n+statement ok\n+CREATE TABLE strings(s VARCHAR);\n+\n+# load large strings into the table iteratively\n+loop it 0 10\n+\n+statement ok\n+INSERT INTO strings SELECT repeat('X', case when i%17=0 then 5000 else i%7 end) AS s FROM generate_series(0,2500) tbl(i);\n+\n+statement ok\n+CHECKPOINT;\n+\n+endloop\n+\n+query I\n+SELECT COUNT(*) FROM pragma_storage_info('strings') WHERE contains(segment_info, 'Overflow String');\n+----\n+1\n+\n+query I\n+select total_blocks < 10 from pragma_database_size();\n+----\n+true\n+\n+statement ok\n+DROP TABLE strings;\n+\n+endloop\ndiff --git a/test/sql/storage/reclaim_space/reclaim_space_drop_column_overflow_strings.test_slow b/test/sql/storage/reclaim_space/reclaim_space_drop_column_overflow_strings.test_slow\nnew file mode 100644\nindex 000000000000..4ff953dc6bfc\n--- /dev/null\n+++ b/test/sql/storage/reclaim_space/reclaim_space_drop_column_overflow_strings.test_slow\n@@ -0,0 +1,53 @@\n+# name: test/sql/storage/reclaim_space/reclaim_space_drop_column_overflow_strings.test_slow\n+# description: Test that we reclaim space when dropping columns containing overflow strings\n+# group: [reclaim_space]\n+\n+load __TEST_DIR__/reclaim_space_drop_column_overflow_strings.db\n+\n+statement ok\n+PRAGMA force_checkpoint;\n+\n+statement ok\n+CREATE TABLE strings AS SELECT i, repeat('X', case when i%17=0 then 5000 else i%7 end) AS s FROM generate_series(0,150000) tbl(i);\n+\n+statement ok\n+CHECKPOINT;\n+\n+statement ok\n+CHECKPOINT;\n+\n+query IIIIII\n+SELECT AVG(STRLEN(s)), MIN(STRLEN(S)), MAX(STRLEN(S)), SUM(STRLEN(S)), MIN(S[1]), MAX(S[1]) FROM strings\n+----\n+296.955\t0\t5000\t44543527\t(empty)\tX\n+\n+loop i 0 10\n+\n+statement ok\n+ALTER TABLE strings DROP COLUMN s;\n+\n+statement ok\n+ALTER TABLE strings ADD COLUMN s VARCHAR;\n+\n+statement ok\n+UPDATE strings SET s=repeat('X', case when i%17=0 then 5000 else i%7 end);\n+\n+query IIIIII\n+SELECT AVG(STRLEN(s)), MIN(STRLEN(S)), MAX(STRLEN(S)), SUM(STRLEN(S)), MIN(S[1]), MAX(S[1]) FROM strings\n+----\n+296.955\t0\t5000\t44543527\t(empty)\tX\n+\n+statement ok\n+CHECKPOINT;\n+\n+query I nosort expected_blocks\n+select total_blocks from pragma_database_size();\n+\n+restart\n+\n+query IIIIII\n+SELECT AVG(STRLEN(s)), MIN(STRLEN(S)), MAX(STRLEN(S)), SUM(STRLEN(S)), MIN(S[1]), MAX(S[1]) FROM strings\n+----\n+296.955\t0\t5000\t44543527\t(empty)\tX\n+\n+endloop\ndiff --git a/test/sql/storage/reclaim_space/reclaim_space_drop_overflow_strings.test_slow b/test/sql/storage/reclaim_space/reclaim_space_drop_overflow_strings.test_slow\nnew file mode 100644\nindex 000000000000..cac51d433902\n--- /dev/null\n+++ b/test/sql/storage/reclaim_space/reclaim_space_drop_overflow_strings.test_slow\n@@ -0,0 +1,50 @@\n+# name: test/sql/storage/reclaim_space/reclaim_space_drop_overflow_strings.test_slow\n+# description: Test that we reclaim space when dropping tables containing overflow strings\n+# group: [reclaim_space]\n+\n+load __TEST_DIR__/reclaim_space_overflow_strings.db\n+\n+statement ok\n+PRAGMA force_checkpoint;\n+\n+statement ok\n+CREATE TABLE strings AS SELECT repeat('X', case when i%17=0 then 5000 else i%7 end) AS s FROM generate_series(0,150000) tbl(i);\n+\n+statement ok\n+CHECKPOINT;\n+\n+statement ok\n+CHECKPOINT;\n+\n+query IIIIII\n+SELECT AVG(STRLEN(s)), MIN(STRLEN(S)), MAX(STRLEN(S)), SUM(STRLEN(S)), MIN(S[1]), MAX(S[1]) FROM strings\n+----\n+296.955\t0\t5000\t44543527\t(empty)\tX\n+\n+loop i 0 10\n+\n+statement ok\n+DROP TABLE strings;\n+\n+statement ok\n+CREATE TABLE strings AS SELECT repeat('X', case when i%17=0 then 5000 else i%7 end) AS s FROM generate_series(0,150000) tbl(i);\n+\n+query IIIIII\n+SELECT AVG(STRLEN(s)), MIN(STRLEN(S)), MAX(STRLEN(S)), SUM(STRLEN(S)), MIN(S[1]), MAX(S[1]) FROM strings\n+----\n+296.955\t0\t5000\t44543527\t(empty)\tX\n+\n+statement ok\n+CHECKPOINT;\n+\n+query I nosort expected_blocks\n+select total_blocks from pragma_database_size();\n+\n+restart\n+\n+query IIIIII\n+SELECT AVG(STRLEN(s)), MIN(STRLEN(S)), MAX(STRLEN(S)), SUM(STRLEN(S)), MIN(S[1]), MAX(S[1]) FROM strings\n+----\n+296.955\t0\t5000\t44543527\t(empty)\tX\n+\n+endloop\ndiff --git a/test/sql/storage/reclaim_space/test_reclaim_space_drop_column.test_slow b/test/sql/storage/reclaim_space/test_reclaim_space_drop_column.test_slow\nindex 2a00ce5eb3dc..0e1410b0588b 100644\n--- a/test/sql/storage/reclaim_space/test_reclaim_space_drop_column.test_slow\n+++ b/test/sql/storage/reclaim_space/test_reclaim_space_drop_column.test_slow\n@@ -2,7 +2,7 @@\n # description: Test that we reclaim space when dropping and adding columns\n # group: [reclaim_space]\n \n-load __TEST_DIR__/test_reclaim_space.db\n+load __TEST_DIR__/test_reclaim_space_drop_column.db\n \n statement ok\n PRAGMA force_checkpoint;\ndiff --git a/test/sql/storage_version/storage_version.db b/test/sql/storage_version/storage_version.db\nindex 87056483c68f..1c7aa1720bb0 100644\nBinary files a/test/sql/storage_version/storage_version.db and b/test/sql/storage_version/storage_version.db differ\n",
  "problem_statement": "Insert into table fails with `INTERNAL Error: Cannot perform IO in in-memory database - CreateBlock!`\n### What happens?\r\n\r\n1. I opened duckdb cli and opened it on a persistent database. \r\n2. I created a table with a parquet file. \r\n`create or replace table test as select * from read_parquet('2023_06_07_1686176284392_0.parquet');`\r\n3. I added more data into the table from some more parquet files using \r\n`insert into  test select * from read_parquet('*.parquet');`\r\n4. The insert command fails with error :\r\n`Error: TransactionContext Error: Failed to commit: INTERNAL Error: Cannot perform IO in in-memory database - CreateBlock!`\r\n\r\nIngesting all files in one table with `create or replace table test as select * from read_parquet('*.parquet');` succeeds as well ingesting all the files individually.\r\n\r\n### To Reproduce\r\n\r\nRunning following commands should reproduce the issue:\r\n\r\n```sql\r\n.open file.db\r\nCREATE OR REPLACE TABLE test AS SELECT * FROM read_parquet('2023_06_07_1686176284392_0.parquet');\r\nINSERT INTO test SELECT * FROM read_parquet('*.parquet');\r\n```\r\n```console\r\nError: TransactionContext Error: Failed to commit: INTERNAL Error: Cannot perform IO in in-memory database - CreateBlock!\r\n```\r\nAttaching parquet files as well : \r\n[data.zip](https://github.com/duckdb/duckdb/files/11704459/data.zip)\r\n\r\n\r\n### OS:\r\n\r\nmacOS 13.4\r\n\r\n### DuckDB Version:\r\n\r\nv0.8 and latest as well\r\n\r\n### DuckDB Client:\r\n\r\nCLI\r\n\r\n### Full Name:\r\n\r\nAnshul Khandelwal\r\n\r\n### Affiliation:\r\n\r\nRill data\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\n- [X] I agree\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] I agree\nInsert into table fails with `INTERNAL Error: Cannot perform IO in in-memory database - CreateBlock!`\n### What happens?\r\n\r\n1. I opened duckdb cli and opened it on a persistent database. \r\n2. I created a table with a parquet file. \r\n`create or replace table test as select * from read_parquet('2023_06_07_1686176284392_0.parquet');`\r\n3. I added more data into the table from some more parquet files using \r\n`insert into  test select * from read_parquet('*.parquet');`\r\n4. The insert command fails with error :\r\n`Error: TransactionContext Error: Failed to commit: INTERNAL Error: Cannot perform IO in in-memory database - CreateBlock!`\r\n\r\nIngesting all files in one table with `create or replace table test as select * from read_parquet('*.parquet');` succeeds as well ingesting all the files individually.\r\n\r\n### To Reproduce\r\n\r\nRunning following commands should reproduce the issue:\r\n\r\n```sql\r\n.open file.db\r\nCREATE OR REPLACE TABLE test AS SELECT * FROM read_parquet('2023_06_07_1686176284392_0.parquet');\r\nINSERT INTO test SELECT * FROM read_parquet('*.parquet');\r\n```\r\n```console\r\nError: TransactionContext Error: Failed to commit: INTERNAL Error: Cannot perform IO in in-memory database - CreateBlock!\r\n```\r\nAttaching parquet files as well : \r\n[data.zip](https://github.com/duckdb/duckdb/files/11704459/data.zip)\r\n\r\n\r\n### OS:\r\n\r\nmacOS 13.4\r\n\r\n### DuckDB Version:\r\n\r\nv0.8 and latest as well\r\n\r\n### DuckDB Client:\r\n\r\nCLI\r\n\r\n### Full Name:\r\n\r\nAnshul Khandelwal\r\n\r\n### Affiliation:\r\n\r\nRill data\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\n- [X] I agree\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] I agree\nTransient issue on 0.8.1: `duckdb.TransactionException: TransactionContext Error: Failed to commit: Input is invalid/unsupported GZIP stream`\n### What happens?\n\nI run a pipeline to ingest multiple `json.gz` files daily into a DuckDB database, using a code like \r\n```\r\ninsert into my_table by name select \r\n            * \r\n        from read_json_auto('/folder/my_file.json.gz', sample_size=10000)\r\n```\r\n\r\nThis used to work without a problem with DuckDB 0.7 but since I upgrade to 0.8.1 recently, I get transient errors of \r\n```\r\nduckdb.TransactionException: TransactionContext Error: Failed to commit: Input is invalid/unsupported GZIP stream\r\n```\r\n\r\nRe-running my pipeline a second time usually fixes the problem though\n\n### To Reproduce\n\nI can't reproduce the issue as it comes and goes.\n\n### OS:\n\nx86_64\n\n### DuckDB Version:\n\n0.8.1\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nBenoit Perigaud\n\n### Affiliation:\n\nNA\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "\n\nI can actually reproduce the issue in 0.8.1 every time with the following code.\r\nIt usually dies at the iteration 2 or 3 in my loop\r\n\r\n```\r\nimport duckdb\r\n\r\nconn = duckdb.connect(\"mydb.db\")\r\nconn.sql(\"create or replace table test_table as select * from read_json_auto('myfile.json.gz', sample_size=10000)\")\r\nfor i in range(10):\r\n    print(i)\r\n    conn.sql(\"insert into test_table select * from read_json_auto('myfile.json.gz', sample_size=10000)\")\r\n```\r\n\r\nI just tried with `0.8.2.dev2320` now and ran it a few times without a problem. The issue now is that the DB format is different between `0.8.1` and `0.8.2.dev` so I'll need to update the CLI as well and export/import my DB to the new format.\nAnd finally, I also tried in `0.8.1` to un-gzip the file before loading it but I still get the same error despite the file being a json one and not a gzip.\r\n```\r\nduckdb.Error: Invalid Error: Input is invalid/unsupported GZIP stream\r\n```\nI have also tried closing the connection after each step and reopening for the next iteration of the loop and in that case there is no error in `0.8.1`\nHello,\r\n\r\nAt Rill Data, we have also seen this issue intermittently since v0.8, but have not been able to replicate it. Here are some additional findings that can hopefully help you triage the issue:\r\n\r\n- The problem usually arises after a connection has been open and used for a while (rarely on a first run).\r\n- We have seen the issue when loading files that have nothing to do with GZIP, such as plain JSON files and Parquet files that don't contain gzip'ed data\r\n  - I haven't been able to identify the exact query that triggers it, but most queries on the call path where we get this error take the form `INSERT INTO x BY NAME (SELECT * FROM read_xxx(...))`.\r\n- We have also seen the issue when loading data using the DuckDB appender C API\r\n  - In that instance, we got this internal error on the next DuckDB query: `INTERNAL Error: Could not find node in column segment tree!\\nAttempting to find row number \\\"0\\\" in 0 nodes\\n`\r\n- We do not use DuckDB transactions.\r\n- We have seen the error on DuckDB 0.8.1 on Intel Linux and ARM macOS.\r\n\r\nOverall, the error does not feel related to GZIP input. I noticed that DuckDB also uses GZIP in its internal storage format for overflow strings. Could some corruption be happening there?\nIf it's intermittent is it maybe possible to share a reproduction script that when ran enough times eventually shows the issue? Because that would already help a lot tracking this down.\nI keep getting it occasionally in CI/CD that runs every 3 hours, see the export to MotherDuck step: https://github.com/lostmygithubaccount/ibis-analytics/actions/runs/5922595390/job/16056805728\n@b-per can you share the file that demonstrates the issue please?\nI use 0.7.1 duckdb and there also exists this problem, I use java call duckdb , program run a day and this accident was happen\uff0ccopy duckdb file and run sql is accessable  \r\n```java\r\n### Error querying database.  Cause: java.sql.SQLException: Invalid Error: Input is invalid/unsupported GZIP stream\\\\\\\\n\r\n### The error may exist in xxxxxMapper.java (best guess)\\\\\\\\n\r\n### The error may involve xxxxxMapper.selectWithTimeInsensitive-Inline\\\\\\\\n\r\n### The error occurred while setting parameters\\\\\\\\n\r\n### SQL: select * from XXXX where namespace ilike ? and name ilike ? and params ilike ? order by update_time desc limit 1\\\\\\\\n\r\n### Cause: java.sql.SQLException: Invalid Error: Input is invalid/unsupported GZIP stream\\\\\\\\n; uncategorized SQLException; SQL state [null]; error code [0];\r\n Invalid Error: Input is invalid/unsupported GZIP stream; nested exception is java.sql.SQLException: Invalid Error: Input is invalid/unsupported GZIP stream\r\n```\n@Stongtong do you see the issue in 0.8.1? And can you provide the file?\n@Mause I not upgrade the version to 0.8.1 yet ,there is 400+ tables and 12+GB size in the db data file, share it difficultly\nI just ran into this after bumping from `0.7.x` to `0.8.1` as well.\r\n\r\nIn my case\r\n\r\n```\r\nselect * from read_json_auto('$FILE.log.gz');\r\n```\r\n\r\nreturns\r\n\r\n```Error: IO Error: Input is not a GZIP Stream```\r\n\r\nBut after `gunzip $FILE.log.gz && gzip $FILE.log` the file is able to be properly read using `read_json_auto`.\r\n\r\n(Happy to share $FILE privately - this was working prior to bump to `0.8.1`)\r\n\n We have also seen this at MotherDuck in production on 0.8.1. Have attempted to reproduce with no luck\r\n * repeatedly loading from parquet\r\n * repeatedly loading from a dataframe\r\n * same but with large strings to trigger Overflow blocks\r\n * in a fuzz test where SELECTS, UPDATES, FORCE CHECKPOINT and loading CTAS activity done over 8 threads, resetting connection at times\r\n\r\nStill trying to gather more information. However we ran into an issue yesterday where within 15 min of GZIP errors, we got the column segment tree issue from #3789 that signifies corrupted database. I'm not certain which is cause and which is effect. I suspect that WAL replay during checkpoints may be playing a role here when it happens after an unexpected query error. The exception happens because a block that is supposed to have the 4 byte magic string to signify GZIP is missing them.\r\n\r\nAgree with @begelundmuller that this doesn't seem related to GZIP input, but rather internal GZIP compression atm.\r\n\nCC @Mytherin \nIn case it is helpful, another user encountered this issue when inserting into MotherDuck in this GitHub action (in the \"export to MotherDuck\" section): \r\nhttps://github.com/lostmygithubaccount/ibis-analytics/actions/runs/6039101657/job/16386985693 \r\n",
  "created_at": "2023-09-15T13:53:28Z"
}