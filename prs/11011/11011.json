{
  "repo": "duckdb/duckdb",
  "pull_number": 11011,
  "instance_id": "duckdb__duckdb-11011",
  "issue_numbers": [
    "10982",
    "10982"
  ],
  "base_commit": "18e3582f5b5130a3c1c4635866ae2224b548771c",
  "patch": "diff --git a/src/include/duckdb/storage/data_table.hpp b/src/include/duckdb/storage/data_table.hpp\nindex 4c2e0d6b580c..c6ae98a9f24e 100644\n--- a/src/include/duckdb/storage/data_table.hpp\n+++ b/src/include/duckdb/storage/data_table.hpp\n@@ -128,9 +128,11 @@ class DataTable {\n \t//! Fetches an append lock\n \tvoid AppendLock(TableAppendState &state);\n \t//! Begin appending structs to this table, obtaining necessary locks, etc\n-\tvoid InitializeAppend(DuckTransaction &transaction, TableAppendState &state, idx_t append_count);\n+\tvoid InitializeAppend(DuckTransaction &transaction, TableAppendState &state);\n \t//! Append a chunk to the table using the AppendState obtained from InitializeAppend\n \tvoid Append(DataChunk &chunk, TableAppendState &state);\n+\t//! Finalize an append\n+\tvoid FinalizeAppend(DuckTransaction &transaction, TableAppendState &state);\n \t//! Commit the append\n \tvoid CommitAppend(transaction_t commit_id, idx_t row_start, idx_t count);\n \t//! Write a segment of the table to the WAL\ndiff --git a/src/include/duckdb/storage/table/append_state.hpp b/src/include/duckdb/storage/table/append_state.hpp\nindex 382f86da5d00..5c5ebd0c68fa 100644\n--- a/src/include/duckdb/storage/table/append_state.hpp\n+++ b/src/include/duckdb/storage/table/append_state.hpp\n@@ -67,8 +67,6 @@ struct TableAppendState {\n \tRowGroup *start_row_group;\n \t//! The transaction data\n \tTransactionData transaction;\n-\t//! The remaining append count, only if the append count is known beforehand\n-\tidx_t remaining;\n };\n \n struct LocalAppendState {\ndiff --git a/src/include/duckdb/storage/table/row_group_collection.hpp b/src/include/duckdb/storage/table/row_group_collection.hpp\nindex 9401cb9aee96..85d99ba19501 100644\n--- a/src/include/duckdb/storage/table/row_group_collection.hpp\n+++ b/src/include/duckdb/storage/table/row_group_collection.hpp\n@@ -69,8 +69,9 @@ class RowGroupCollection {\n \n \t//! Initialize an append of a variable number of rows. FinalizeAppend must be called after appending is done.\n \tvoid InitializeAppend(TableAppendState &state);\n-\t//! Initialize an append with a known number of rows. FinalizeAppend should not be called after appending is done.\n-\tvoid InitializeAppend(TransactionData transaction, TableAppendState &state, idx_t append_count);\n+\t//! Initialize an append with a variable number of rows. FinalizeAppend should not be called after appending is\n+\t//! done.\n+\tvoid InitializeAppend(TransactionData transaction, TableAppendState &state);\n \t//! Appends to the row group collection. Returns true if a new row group has been created to append to\n \tbool Append(DataChunk &chunk, TableAppendState &state);\n \t//! FinalizeAppend flushes an append with a variable number of rows.\ndiff --git a/src/storage/data_table.cpp b/src/storage/data_table.cpp\nindex 0e64ab831f03..435766de0186 100644\n--- a/src/storage/data_table.cpp\n+++ b/src/storage/data_table.cpp\n@@ -747,12 +747,12 @@ void DataTable::AppendLock(TableAppendState &state) {\n \tstate.current_row = state.row_start;\n }\n \n-void DataTable::InitializeAppend(DuckTransaction &transaction, TableAppendState &state, idx_t append_count) {\n+void DataTable::InitializeAppend(DuckTransaction &transaction, TableAppendState &state) {\n \t// obtain the append lock for this table\n \tif (!state.append_lock) {\n \t\tthrow InternalException(\"DataTable::AppendLock should be called before DataTable::InitializeAppend\");\n \t}\n-\trow_groups->InitializeAppend(transaction, state, append_count);\n+\trow_groups->InitializeAppend(transaction, state);\n }\n \n void DataTable::Append(DataChunk &chunk, TableAppendState &state) {\n@@ -760,6 +760,10 @@ void DataTable::Append(DataChunk &chunk, TableAppendState &state) {\n \trow_groups->Append(chunk, state);\n }\n \n+void DataTable::FinalizeAppend(DuckTransaction &transaction, TableAppendState &state) {\n+\trow_groups->FinalizeAppend(transaction, state);\n+}\n+\n void DataTable::ScanTableSegment(idx_t row_start, idx_t count, const std::function<void(DataChunk &chunk)> &function) {\n \tif (count == 0) {\n \t\treturn;\ndiff --git a/src/storage/local_storage.cpp b/src/storage/local_storage.cpp\nindex 2aa7ab7363c0..2c7fb0fe1d79 100644\n--- a/src/storage/local_storage.cpp\n+++ b/src/storage/local_storage.cpp\n@@ -151,7 +151,7 @@ void LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, TableAppen\n                                         idx_t append_count, bool append_to_table) {\n \tauto &table = table_ref.get();\n \tif (append_to_table) {\n-\t\ttable.InitializeAppend(transaction, append_state, append_count);\n+\t\ttable.InitializeAppend(transaction, append_state);\n \t}\n \tErrorData error;\n \tif (append_to_table) {\n@@ -202,6 +202,9 @@ void LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, TableAppen\n \t\t});\n \t\terror.Throw();\n \t}\n+\tif (append_to_table) {\n+\t\ttable.FinalizeAppend(transaction, append_state);\n+\t}\n }\n \n OptimisticDataWriter &LocalTableStorage::CreateOptimisticWriter() {\n@@ -352,7 +355,7 @@ bool LocalStorage::NextParallelScan(ClientContext &context, DataTable &table, Pa\n \n void LocalStorage::InitializeAppend(LocalAppendState &state, DataTable &table) {\n \tstate.storage = &table_manager.GetOrCreateStorage(table);\n-\tstate.storage->row_groups->InitializeAppend(TransactionData(transaction), state.append_state, 0);\n+\tstate.storage->row_groups->InitializeAppend(TransactionData(transaction), state.append_state);\n }\n \n void LocalStorage::Append(LocalAppendState &state, DataChunk &chunk) {\ndiff --git a/src/storage/table/row_group.cpp b/src/storage/table/row_group.cpp\nindex f70ebbc8f394..67e17737211a 100644\n--- a/src/storage/table/row_group.cpp\n+++ b/src/storage/table/row_group.cpp\n@@ -184,12 +184,17 @@ bool RowGroup::InitializeScanWithOffset(CollectionScanState &state, idx_t vector\n \tstate.vector_index = vector_offset;\n \tstate.max_row_group_row =\n \t    this->start > state.max_row ? 0 : MinValue<idx_t>(this->count, state.max_row - this->start);\n+\tauto row_number = start + vector_offset * STANDARD_VECTOR_SIZE;\n+\tif (state.max_row_group_row == 0) {\n+\t\t// exceeded row groups to scan\n+\t\treturn false;\n+\t}\n \tD_ASSERT(state.column_scans);\n \tfor (idx_t i = 0; i < column_ids.size(); i++) {\n \t\tconst auto &column = column_ids[i];\n \t\tif (column != COLUMN_IDENTIFIER_ROW_ID) {\n \t\t\tauto &column_data = GetColumn(column);\n-\t\t\tcolumn_data.InitializeScanWithOffset(state.column_scans[i], start + vector_offset * STANDARD_VECTOR_SIZE);\n+\t\t\tcolumn_data.InitializeScanWithOffset(state.column_scans[i], row_number);\n \t\t\tstate.column_scans[i].scan_options = &state.GetOptions();\n \t\t} else {\n \t\t\tstate.column_scans[i].current = nullptr;\ndiff --git a/src/storage/table/row_group_collection.cpp b/src/storage/table/row_group_collection.cpp\nindex 91c843ed72c4..8c8a2a3fd785 100644\n--- a/src/storage/table/row_group_collection.cpp\n+++ b/src/storage/table/row_group_collection.cpp\n@@ -288,11 +288,10 @@ void RowGroupCollection::Fetch(TransactionData transaction, DataChunk &result, c\n // Append\n //===--------------------------------------------------------------------===//\n TableAppendState::TableAppendState()\n-    : row_group_append_state(*this), total_append_count(0), start_row_group(nullptr), transaction(0, 0), remaining(0) {\n+    : row_group_append_state(*this), total_append_count(0), start_row_group(nullptr), transaction(0, 0) {\n }\n \n TableAppendState::~TableAppendState() {\n-\tD_ASSERT(Exception::UncaughtException() || remaining == 0);\n }\n \n bool RowGroupCollection::IsEmpty() const {\n@@ -304,7 +303,7 @@ bool RowGroupCollection::IsEmpty(SegmentLock &l) const {\n \treturn row_groups->IsEmpty(l);\n }\n \n-void RowGroupCollection::InitializeAppend(TransactionData transaction, TableAppendState &state, idx_t append_count) {\n+void RowGroupCollection::InitializeAppend(TransactionData transaction, TableAppendState &state) {\n \tstate.row_start = total_rows;\n \tstate.current_row = state.row_start;\n \tstate.total_append_count = 0;\n@@ -318,17 +317,12 @@ void RowGroupCollection::InitializeAppend(TransactionData transaction, TableAppe\n \tstate.start_row_group = row_groups->GetLastSegment(l);\n \tD_ASSERT(this->row_start + total_rows == state.start_row_group->start + state.start_row_group->count);\n \tstate.start_row_group->InitializeAppend(state.row_group_append_state);\n-\tstate.remaining = append_count;\n \tstate.transaction = transaction;\n-\tif (state.remaining > 0) {\n-\t\tstate.start_row_group->AppendVersionInfo(transaction, state.remaining);\n-\t\ttotal_rows += state.remaining;\n-\t}\n }\n \n void RowGroupCollection::InitializeAppend(TableAppendState &state) {\n \tTransactionData tdata(0, 0);\n-\tInitializeAppend(tdata, state, 0);\n+\tInitializeAppend(tdata, state);\n }\n \n bool RowGroupCollection::Append(DataChunk &chunk, TableAppendState &state) {\n@@ -336,9 +330,9 @@ bool RowGroupCollection::Append(DataChunk &chunk, TableAppendState &state) {\n \tchunk.Verify();\n \n \tbool new_row_group = false;\n-\tidx_t append_count = chunk.size();\n+\tidx_t total_append_count = chunk.size();\n \tidx_t remaining = chunk.size();\n-\tstate.total_append_count += append_count;\n+\tstate.total_append_count += total_append_count;\n \twhile (true) {\n \t\tauto current_row_group = state.row_group_append_state.row_group;\n \t\t// check how much we can fit into the current row_group\n@@ -355,9 +349,6 @@ bool RowGroupCollection::Append(DataChunk &chunk, TableAppendState &state) {\n \t\t\t}\n \t\t}\n \t\tremaining -= append_count;\n-\t\tif (state.remaining > 0) {\n-\t\t\tstate.remaining -= append_count;\n-\t\t}\n \t\tif (remaining > 0) {\n \t\t\t// we expect max 1 iteration of this loop (i.e. a single chunk should never overflow more than one\n \t\t\t// row_group)\n@@ -375,15 +366,12 @@ bool RowGroupCollection::Append(DataChunk &chunk, TableAppendState &state) {\n \t\t\t// set up the append state for this row_group\n \t\t\tauto last_row_group = row_groups->GetLastSegment(l);\n \t\t\tlast_row_group->InitializeAppend(state.row_group_append_state);\n-\t\t\tif (state.remaining > 0) {\n-\t\t\t\tlast_row_group->AppendVersionInfo(state.transaction, state.remaining);\n-\t\t\t}\n \t\t\tcontinue;\n \t\t} else {\n \t\t\tbreak;\n \t\t}\n \t}\n-\tstate.current_row += append_count;\n+\tstate.current_row += row_t(total_append_count);\n \tauto stats_lock = stats.GetLock();\n \tfor (idx_t col_idx = 0; col_idx < types.size(); col_idx++) {\n \t\tstats.GetStats(col_idx).UpdateDistinctStatistics(chunk.data[col_idx], chunk.size());\n",
  "test_patch": "diff --git a/test/sql/storage/concurrent_table_insertion.test_slow b/test/sql/storage/concurrent_table_insertion.test_slow\nnew file mode 100644\nindex 000000000000..96710b56019b\n--- /dev/null\n+++ b/test/sql/storage/concurrent_table_insertion.test_slow\n@@ -0,0 +1,29 @@\n+# name: test/sql/storage/concurrent_table_insertion.test_slow\n+# description: Test concurrent table insertions\n+# group: [storage]\n+\n+# load the DB from disk\n+load __TEST_DIR__/concurrent_table_insertions.db\n+\n+statement ok\n+CREATE OR REPLACE TABLE d (x INT, y INT, z INT);\n+\n+concurrentloop c 0 100\n+\n+loop i 0 10\n+\n+statement ok\n+CREATE OR REPLACE TEMP TABLE _tt AS SELECT 1 AS x, (SELECT COUNT(*) FROM d WHERE z != ${i}) AS y, ${i} AS z;\n+\n+statement ok\n+INSERT INTO d SELECT * FROM _tt\n+\n+statement ok\n+CREATE OR REPLACE TEMP TABLE _tt AS SELECT * FROM d LIMIT 20000\n+\n+statement ok\n+INSERT INTO d SELECT * FROM _tt\n+\n+endloop\n+\n+endloop\n",
  "problem_statement": "Could not find node in column segment tree!\n### What happens?\n\nWhen we mix python threading + cursors + temp table creation we see occasional hanging jobs and crashes.\r\nTrying to reproduce the mixture of events gives us `Could not find node in column segment tree!`\n\n### To Reproduce\n\nOn a MacBook PRO the following code will crash every 3~8 runs with\r\n\r\n```\r\nc.execute(f\"CREATE OR REPLACE TEMP TABLE _tt AS SELECT * FROM d LIMIT 20000\")\r\nduckdb.duckdb.InvalidInputException: Invalid Input Error: Attempting to execute an unsuccessful or closed pending query result\r\nError: FATAL Error: Failed: database has been invalidated because of a previous fatal error. The database must be restarted prior to being used again.\r\nOriginal error: \"Failed: database has been invalidated because of a previous fatal error. The database must be restarted prior to being used again.\r\nOriginal error: \"Could not find node in column segment tree!\r\nAttempting to find row number \"1228800\" in 1 nodes\r\nNode 0: Start 1228800, Count 0\"\"\r\n```\r\n\r\n```\r\nimport duckdb\r\nimport os\r\nimport shutil\r\nimport concurrent.futures\r\n\r\ndef task(i : int, con: duckdb.DuckDBPyConnection) -> None:\r\n    for _ in range(2):\r\n        with con.cursor() as c:\r\n            c.execute(f\"CREATE TEMP TABLE _tt AS SELECT 1 AS x, (SELECT COUNT(*) FROM d WHERE z != {i}) AS y, {i} AS z;\")\r\n            c.execute(f\"INSERT INTO d SELECT * FROM _tt\")\r\n            c.execute(f\"CREATE OR REPLACE TEMP TABLE _tt AS SELECT * FROM d LIMIT 20000\")\r\n            c.execute(f\"INSERT INTO d SELECT * FROM _tt\")\r\n    print(f\"Done {i:3}\")\r\n    \r\ndef threaded_ddl() -> None:\r\n    print('-------------------------')\r\n    con : duckdb.DuckDBPyConnection = duckdb.connect('/tmp/duckdb_thread_test/duck.db')\r\n    \r\n    print(con.execute('SELECT version()').fetchall()[0][0])    \r\n    con.execute(f\"CREATE OR REPLACE TABLE d (x INT, y INT, z INT);\")\r\n    \r\n    executor = concurrent.futures.ThreadPoolExecutor(max_workers=20)\r\n\r\n    results = []\r\n    for i in range(100):\r\n        results.append(executor.submit(task, i, con.cursor()))\r\n        \r\n    for r in results:\r\n        r.result()\r\n        \r\n    print('-------------------------')\r\n\r\ndef pressure_threads() -> None:\r\n    print('=========================')\r\n    if os.path.exists('/tmp/duckdb_thread_test'):\r\n        shutil.rmtree('/tmp/duckdb_thread_test')\r\n    os.mkdir('/tmp/duckdb_thread_test')\r\n    threaded_ddl()\r\n    print('=========================')\r\n    \r\ndef main() -> None:\r\n    pressure_threads()\r\n\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\n\n### OS:\n\nmacOS 14.3.1\n\n### DuckDB Version:\n\n0.10.0\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nRob Jackson\n\n### Affiliation:\n\nExaforce\n\n### Have you tried this on the latest [nightly build](https://duckdb.org/docs/installation/?version=main)?\n\nI have tested with a nightly build\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] Yes, I have\nCould not find node in column segment tree!\n### What happens?\n\nWhen we mix python threading + cursors + temp table creation we see occasional hanging jobs and crashes.\r\nTrying to reproduce the mixture of events gives us `Could not find node in column segment tree!`\n\n### To Reproduce\n\nOn a MacBook PRO the following code will crash every 3~8 runs with\r\n\r\n```\r\nc.execute(f\"CREATE OR REPLACE TEMP TABLE _tt AS SELECT * FROM d LIMIT 20000\")\r\nduckdb.duckdb.InvalidInputException: Invalid Input Error: Attempting to execute an unsuccessful or closed pending query result\r\nError: FATAL Error: Failed: database has been invalidated because of a previous fatal error. The database must be restarted prior to being used again.\r\nOriginal error: \"Failed: database has been invalidated because of a previous fatal error. The database must be restarted prior to being used again.\r\nOriginal error: \"Could not find node in column segment tree!\r\nAttempting to find row number \"1228800\" in 1 nodes\r\nNode 0: Start 1228800, Count 0\"\"\r\n```\r\n\r\n```\r\nimport duckdb\r\nimport os\r\nimport shutil\r\nimport concurrent.futures\r\n\r\ndef task(i : int, con: duckdb.DuckDBPyConnection) -> None:\r\n    for _ in range(2):\r\n        with con.cursor() as c:\r\n            c.execute(f\"CREATE TEMP TABLE _tt AS SELECT 1 AS x, (SELECT COUNT(*) FROM d WHERE z != {i}) AS y, {i} AS z;\")\r\n            c.execute(f\"INSERT INTO d SELECT * FROM _tt\")\r\n            c.execute(f\"CREATE OR REPLACE TEMP TABLE _tt AS SELECT * FROM d LIMIT 20000\")\r\n            c.execute(f\"INSERT INTO d SELECT * FROM _tt\")\r\n    print(f\"Done {i:3}\")\r\n    \r\ndef threaded_ddl() -> None:\r\n    print('-------------------------')\r\n    con : duckdb.DuckDBPyConnection = duckdb.connect('/tmp/duckdb_thread_test/duck.db')\r\n    \r\n    print(con.execute('SELECT version()').fetchall()[0][0])    \r\n    con.execute(f\"CREATE OR REPLACE TABLE d (x INT, y INT, z INT);\")\r\n    \r\n    executor = concurrent.futures.ThreadPoolExecutor(max_workers=20)\r\n\r\n    results = []\r\n    for i in range(100):\r\n        results.append(executor.submit(task, i, con.cursor()))\r\n        \r\n    for r in results:\r\n        r.result()\r\n        \r\n    print('-------------------------')\r\n\r\ndef pressure_threads() -> None:\r\n    print('=========================')\r\n    if os.path.exists('/tmp/duckdb_thread_test'):\r\n        shutil.rmtree('/tmp/duckdb_thread_test')\r\n    os.mkdir('/tmp/duckdb_thread_test')\r\n    threaded_ddl()\r\n    print('=========================')\r\n    \r\ndef main() -> None:\r\n    pressure_threads()\r\n\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\n\n### OS:\n\nmacOS 14.3.1\n\n### DuckDB Version:\n\n0.10.0\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nRob Jackson\n\n### Affiliation:\n\nExaforce\n\n### Have you tried this on the latest [nightly build](https://duckdb.org/docs/installation/?version=main)?\n\nI have tested with a nightly build\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] Yes, I have\n",
  "hints_text": "\n",
  "created_at": "2024-03-06T14:32:02Z"
}