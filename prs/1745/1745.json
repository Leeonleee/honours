{
  "repo": "duckdb/duckdb",
  "pull_number": 1745,
  "instance_id": "duckdb__duckdb-1745",
  "issue_numbers": [
    "1737"
  ],
  "base_commit": "30fda3efeb7ca07390a84e787c3110ede289153f",
  "patch": "diff --git a/src/storage/table/validity_segment.cpp b/src/storage/table/validity_segment.cpp\nindex 86a06eb84874..0246ac2b38de 100644\n--- a/src/storage/table/validity_segment.cpp\n+++ b/src/storage/table/validity_segment.cpp\n@@ -60,7 +60,7 @@ idx_t ValiditySegment::Append(SegmentStatistics &stats, VectorData &data, idx_t\n \tauto &validity_stats = (ValidityStatistics &)*stats.statistics;\n \tValidityMask mask((validity_t *)handle->node->buffer);\n \tfor (idx_t i = 0; i < append_count; i++) {\n-\t\tauto idx = data.sel->get_index(i);\n+\t\tauto idx = data.sel->get_index(offset + i);\n \t\tif (!data.validity.RowIsValidUnsafe(idx)) {\n \t\t\tmask.SetInvalidUnsafe(tuple_count + i);\n \t\t\tvalidity_stats.has_null = true;\n",
  "test_patch": "diff --git a/test/db-benchmark/groupby.test_slow b/test/db-benchmark/groupby.test_slow\nnew file mode 100644\nindex 000000000000..e0cf3810f25f\n--- /dev/null\n+++ b/test/db-benchmark/groupby.test_slow\n@@ -0,0 +1,137 @@\n+# name: test/db-benchmark/groupby.test_slow\n+# description: Group By benchmark (0.5GB - small dataset) from h2oai db-benchmark (https://github.com/h2oai/db-benchmark)\n+# group: [db-benchmark]\n+\n+require httpfs\n+\n+statement ok\n+pragma threads=16\n+\n+# 5% nulls\n+statement ok\n+CREATE TABLE x AS SELECT * FROM read_csv_auto('https://github.com/cwida/duckdb-data/releases/download/v1.0/G1_1e7_1e2_5_0.csv.gz');\n+\n+# statement ok\n+# CREATE TABLE x AS SELECT * FROM read_csv_auto('G1_1e7_1e2_5_0.csv.gz');\n+\n+# q1\n+statement ok\n+CREATE TABLE ans AS SELECT id1, sum(v1) AS v1 FROM x GROUP BY id1\n+\n+query II\n+SELECT COUNT(*), sum(v1)::varchar AS v1 FROM ans\n+----\n+96\t28498857\n+\n+statement ok\n+DROP TABLE ans\n+\n+# q2\n+statement ok\n+CREATE TABLE ans AS SELECT id1, id2, sum(v1) AS v1 FROM x GROUP BY id1, id2;\n+\n+query II\n+SELECT count(*), sum(v1) AS v1 FROM ans;\n+----\n+9216\t28498857\n+\n+statement ok\n+DROP TABLE ans;\n+\n+# q3\n+statement ok\n+CREATE TABLE ans AS SELECT id3, sum(v1) AS v1, avg(v3) AS v3 FROM x GROUP BY id3;\n+\n+query III\n+SELECT COUNT(*), sum(v1) AS v1, sum(v3) AS v3 FROM ans;\n+----\n+95001\t28498857\t4749467.631946747\n+\n+statement ok\n+DROP TABLE ans;\n+\n+# q4\n+statement ok\n+CREATE TABLE ans AS SELECT id4, avg(v1) AS v1, avg(v2) AS v2, avg(v3) AS v3 FROM x GROUP BY id4;\n+\n+query IIII\n+SELECT COUNT(*), sum(v1) AS v1, sum(v2) AS v2, sum(v3) AS v3 FROM ans\n+----\n+96\t287.9894309270616821\t767.8529216923457105\t4799.873270453372\n+\n+statement ok\n+DROP TABLE ans;\n+\n+# q5\n+statement ok\n+CREATE TABLE ans AS SELECT id6, sum(v1) AS v1, sum(v2) AS v2, sum(v3) AS v3 FROM x GROUP BY id6;\n+\n+query IIII\n+SELECT COUNT(*), sum(v1) AS v1, sum(v2) AS v2, sum(v3) AS v3 FROM ans\n+----\n+95001\t28498857\t75988394\t474969574.04777884\n+\n+statement ok\n+DROP TABLE ans;\n+\n+# q6\n+statement ok\n+CREATE TABLE ans AS SELECT id4, id5, quantile_cont(v3, 0.5) AS median_v3, stddev(v3) AS sd_v3 FROM x GROUP BY id4, id5;\n+\n+# WARNING: this result might be incorrect\n+# could not verify using Postgres because of lack of median\n+query III\n+SELECT COUNT(*), sum(median_v3) AS median_v3, sum(sd_v3) AS sd_v3 FROM ans\n+----\n+9216\t460771.216444\t266006.904622\n+\n+statement ok\n+DROP TABLE ans;\n+\n+# q7\n+statement ok\n+CREATE TABLE ans AS SELECT id3, max(v1)-min(v2) AS range_v1_v2 FROM x GROUP BY id3;\n+\n+query II\n+SELECT count(*), sum(range_v1_v2) AS range_v1_v2 FROM ans;\n+----\n+95001\t379850\n+\n+statement ok\n+DROP TABLE ans;\n+\n+# q8\n+statement ok\n+CREATE TABLE ans AS SELECT id6, v3 AS largest2_v3 FROM (SELECT id6, v3, row_number() OVER (PARTITION BY id6 ORDER BY v3 DESC) AS order_v3 FROM x WHERE v3 IS NOT NULL) sub_query WHERE order_v3 <= 2\n+\n+query II\n+SELECT count(*), sum(largest2_v3) AS largest2_v3 FROM ans\n+----\n+190002\t18700554.779631943\n+\n+statement ok\n+DROP TABLE ans;\n+\n+# q9\n+statement ok\n+CREATE TABLE ans AS SELECT id2, id4, pow(corr(v1, v2), 2) AS r2 FROM x GROUP BY id2, id4;\n+\n+query II\n+SELECT count(*), sum(r2) AS r2 FROM ans\n+----\n+9216\t9.940515516534346\n+\n+statement ok\n+DROP TABLE ans;\n+\n+# q10\n+statement ok\n+CREATE TABLE ans AS SELECT id1, id2, id3, id4, id5, id6, sum(v3) AS v3, count(*) AS count FROM x GROUP BY id1, id2, id3, id4, id5, id6;\n+\n+query II\n+SELECT sum(v3) AS v3, sum(count) AS count FROM ans;\n+----\n+474969574\t10000000\n+\n+statement ok\n+DROP TABLE ans\ndiff --git a/test/db-benchmark/join.test_slow b/test/db-benchmark/join.test_slow\nnew file mode 100644\nindex 000000000000..f5aed79f124e\n--- /dev/null\n+++ b/test/db-benchmark/join.test_slow\n@@ -0,0 +1,100 @@\n+# name: test/db-benchmark/join.test_slow\n+# description: Join benchmark (0.5GB - small dataset) from h2oai db-benchmark (https://github.com/h2oai/db-benchmark)\n+# group: [db-benchmark]\n+\n+require httpfs\n+\n+statement ok\n+pragma threads=16\n+\n+statement ok\n+CREATE TABLE x AS SELECT * FROM read_csv_auto('https://github.com/cwida/duckdb-data/releases/download/v1.0/J1_1e7_NA_0_0.csv.gz');\n+\n+statement ok\n+CREATE TABLE small AS SELECT * FROM read_csv_auto('https://github.com/cwida/duckdb-data/releases/download/v1.0/J1_1e7_1e1_0_0.csv.gz');\n+\n+statement ok\n+CREATE TABLE medium AS SELECT * FROM read_csv_auto('https://github.com/cwida/duckdb-data/releases/download/v1.0/J1_1e7_1e4_0_0.csv.gz');\n+\n+statement ok\n+CREATE TABLE big AS SELECT * FROM read_csv_auto('https://github.com/cwida/duckdb-data/releases/download/v1.0/J1_1e7_1e7_0_0.csv.gz');\n+\n+query I\n+SELECT COUNT(*) FROM x;\n+----\n+10000000\n+\n+query I\n+SELECT COUNT(*) FROM small;\n+----\n+10\n+\n+query I\n+SELECT COUNT(*) FROM medium;\n+----\n+10000\n+\n+query I\n+SELECT COUNT(*) FROM big;\n+----\n+10000000\n+\n+# q1\n+statement ok\n+CREATE TABLE ans AS SELECT x.*, small.id4 AS small_id4, v2 FROM x JOIN small USING (id1);\n+\n+query III\n+SELECT COUNT(*), SUM(v1) AS v1, SUM(v2) AS v2 FROM ans;\n+----\n+8998860\t450015153.57734203\t347720187.39596415\n+\n+statement ok\n+DROP TABLE ans;\n+\n+# q2\n+statement ok\n+CREATE TABLE ans AS SELECT x.*, medium.id1 AS medium_id1, medium.id4 AS medium_id4, medium.id5 AS medium_id5, v2 FROM x JOIN medium USING (id2);\n+\n+query III\n+SELECT COUNT(*), SUM(v1) AS v1, SUM(v2) AS v2 FROM ans;\n+----\n+8998412\t449954076.0263213\t449999844.93746006\n+\n+statement ok\n+DROP TABLE ans;\n+\n+# q3\n+statement ok\n+CREATE TABLE ans AS SELECT x.*, medium.id1 AS medium_id1, medium.id4 AS medium_id4, medium.id5 AS medium_id5, v2 FROM x LEFT JOIN medium USING (id2);\n+\n+query III\n+SELECT COUNT(*), SUM(v1) AS v1, SUM(v2) AS v2 FROM ans;\n+----\n+10000000\t500043740.7523774\t449999844.93746\n+\n+statement ok\n+DROP TABLE ans;\n+\n+# q4\n+statement ok\n+CREATE TABLE ans AS SELECT x.*, medium.id1 AS medium_id1, medium.id2 AS medium_id2, medium.id4 AS medium_id4, v2 FROM x JOIN medium USING (id5);\n+\n+query III\n+SELECT COUNT(*), SUM(v1) AS v1, SUM(v2) AS v2 FROM ans;\n+----\n+8998412\t449954076.02631813\t449999844.93746257\n+\n+statement ok\n+DROP TABLE ans;\n+\n+# q5\n+statement ok\n+CREATE TABLE ans AS SELECT x.*, big.id1 AS big_id1, big.id2 AS big_id2, big.id4 AS big_id4, big.id5 AS big_id5, big.id6 AS big_id6, v2 FROM x JOIN big USING (id3);\n+\n+query III\n+SELECT COUNT(*), SUM(v1) AS v1, SUM(v2) AS v2 FROM ans;\n+----\n+9000000\t450032091.8405316\t449860428.6155452\n+\n+statement ok\n+DROP TABLE ans;\n",
  "problem_statement": "Breaking change in sum handling NAs?\n**What does happen?**\r\nGetting different values in 0.2.5 and 0.2.6\r\n\r\n**What should happen?**\r\nShould be the same\r\n\r\n**To Reproduce**\r\nFollowing instructions on https://github.com/h2oai/db-benchmark\r\nGenerate mentioned data.\r\nRun mentioned queries,\r\nRun checksum queries.\r\n\r\nPreview of results:\r\n```r\r\nlibrary(data.table)\r\nt = fread(\"https://h2oai.github.io/db-benchmark/time.csv\")\r\nt[solution==\"duckdb\"\r\n  ][.(c(\"groupby\",\"join\"), c(\"G1_1e7_1e2_5_0\",\"J1_1e7_NA_5_0\"), c(\"sum v3 count by id1:id6\",\"small inner on int\")), on=c(\"task\",\"data\",\"question\")\r\n    ][, .(task, data, question, version, run, chk)]\r\n#      task           data                question version run                 chk\r\n#1: groupby G1_1e7_1e2_5_0 sum v3 count by id1:id6   0.2.5   1  474969574;10000000\r\n#2: groupby G1_1e7_1e2_5_0 sum v3 count by id1:id6   0.2.5   2  474969574;10000000\r\n#3: groupby G1_1e7_1e2_5_0 sum v3 count by id1:id6   0.2.6   1  474832257;10000000\r\n#4:    join  J1_1e7_NA_5_0      small inner on int   0.2.5   1 427503549;436095569\r\n#5:    join  J1_1e7_NA_5_0      small inner on int   0.2.5   2 427503549;436095569\r\n#6:    join  J1_1e7_NA_5_0      small inner on int   0.2.6   1 427370928;436095569\r\n```\r\n\r\n**Environment (please complete the following information):**\r\nUbuntu 16.04\r\n\r\n**Before submitting**\r\n- haven't yet tried to reproduce that interactively\r\n- haven't tried on master because 0.2.6 is AFAIK near the to master.\n",
  "hints_text": "comparing 0.2.6 vs data.table\r\n```\r\n|task    |data           |question                    |data.table                        |duckdb                         |\r\n|:-------|:--------------|:---------------------------|:---------------------------------|:------------------------------|\r\n|groupby |G1_1e7_1e2_5_0 |sum v3 count by id1:id6     |474969574;10000000                |474832257;10000000             \r\n|join    |J1_1e7_NA_5_0  |small inner on int          |427503549;436095569               |427370928;436095569            |\r\n```\r\nwe can conclude that 0.2.5 was giving correct results while 0.2.6 is giving incorrect.\nThanks for the report! Going to investigate this now.",
  "created_at": "2021-05-11T13:02:51Z"
}