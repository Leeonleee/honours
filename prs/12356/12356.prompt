You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
BM25 matching scores seems to be invalid
### What happens?

Result table ordering based on BM25 scores seems to be incorrect. Scores can be even negative if search query contains terms that are very common among searched documents.

### To Reproduce

```
INSTALL fts;
LOAD fts;

CREATE OR REPLACE TABLE documents (
    id VARCHAR,
    content VARCHAR
);

INSERT INTO documents VALUES 
    ('doc1', 'DuckDB database lorem'), 
    ('doc2', 'DuckDB database ipsum'), 
    ('doc3', 'DuckDB database ipsum');

PRAGMA create_fts_index('documents', 'id', 'content');

SELECT 
    id,
    fts_main_documents.match_bm25(id, 'DuckDB') score_DuckDB,
    fts_main_documents.match_bm25(id, 'DuckDB database') score_DuckDB_Database,
    fts_main_documents.match_bm25(id, 'DuckDB database lorem') score_DuckDB_Database_lorem,
    fts_main_documents.match_bm25(id, 'lorem') score_lorem,
    fts_main_documents.match_bm25(id, 'DuckDB database ipsum') score_DuckDB_Database_ipsum,
FROM 
    documents;
```
Gives following result table:
```
┌─────────┬─────────────────────┬───────────────────────┬─────────────────────────────┬─────────────────────┬─────────────────────────────┐
│   id    │    score_DuckDB     │ score_DuckDB_Database │ score_DuckDB_Database_lorem │     score_lorem     │ score_DuckDB_Database_ipsum │
│ varchar │       double        │        double         │           double            │       double        │           double            │
├─────────┼─────────────────────┼───────────────────────┼─────────────────────────────┼─────────────────────┼─────────────────────────────┤
│ doc1    │ -0.8450980400142569 │   -1.6901960800285138 │         -1.4683473304121575 │ 0.22184874961635637 │         -1.6901960800285138 │
│ doc2    │ -0.8450980400142569 │   -1.6901960800285138 │         -1.6901960800285138 │                     │         -1.9120448296448702 │
│ doc3    │ -0.8450980400142569 │   -1.6901960800285138 │         -1.6901960800285138 │                     │         -1.9120448296448702 │
└─────────┴─────────────────────┴───────────────────────┴─────────────────────────────┴─────────────────────┴─────────────────────────────┘

```
Higher score means better match. You can notice that scores are counter intuitive. E.g. 
1. (Edit, Improved example): Query 'DuckDB database ipsum' (score_DuckDB_Database_ipsum) matches perfectly with documents doc2 and doc3, but clearly worse doc1 gets better score
2. (older, more unclear example) search query 'DuckDB database lorem' is equal to doc1 content, but it gets lower score (score_DuckDB_Database_lorem: -0.8450980400142569) than only partially matching query 'lorem' (score_lorem: 0.22184874961635637).

I've identified a potential issue in the formula used to calculate the BM25 scores, specifically in the IDF component. It appears that there is a critical omission in the logarithmic term of the IDF calculation. According to the [BM25 formula referenced on Wikipedia](https://en.wikipedia.org/wiki/Okapi_BM25), the logarithmic function should include an increment by 1 to prevent values below 1 inside the logarithm.

The issue occurs in[ line 194 of the code](https://github.com/duckdb/duckdb/blob/d4a8e042dfbe152fc9e22b351373c5f8b681a152/extension/fts/fts_indexing.cpp#L152C9-L215C6), where the IDF part of the formula is calculated as follows:
```sql
log(((SELECT num_docs FROM %fts_schema%.stats) - df + 0.5) / (df + 0.5))
```
To align with the standard BM25 formula and ensure the log function operates within a valid range, the formula should be modified to:
```sql
log(((SELECT num_docs FROM %fts_schema%.stats) - df + 0.5) / (df + 0.5) + 1)
```
Without this adjustment, the log input can fall within the range [0,1], where the logarithm would yield negative values, potentially leading to incorrect scoring results.

I was working with PR, but I'm not sure which implementation of BM25 is wanted. There are at least two ways to fix IDF part:
1. Use "Wikipedia" version of IDF as described above or
2. Extend current version with special cases to avoid negative IDF. See [page 4 from Arxiv paper](https://arxiv.org/pdf/1602.03606). Paper gives formula: 
![image](https://github.com/duckdb/duckdb/assets/24547559/72ed008c-ddcf-46ad-8e2c-8fd4aa194289)

I haven't checked very carefully if these are valid fixes to issue, but testing with fix 1 seems to give improved ordering:
```
INSTALL fts;
LOAD fts;

CREATE OR REPLACE TABLE documents (
    id VARCHAR,
    content VARCHAR
);

INSERT INTO documents VALUES 
    ('doc1', 'DuckDB database lorem'), 
    ('doc2', 'DuckDB database ipsum'), 
    ('doc3', 'DuckDB database ipsum');

PRAGMA create_fts_index('documents', 'id', 'content');

--PATCH: assuming that adding +1 to log(((SELECT num_docs FROM fts_main_documents.stats) - df + 0.5) / (df + 0.5) + 1) is the fix
--                                                                                                                ---
CREATE OR REPLACE MACRO fts_main_documents.match_bm25(docname, query_string, fields := NULL, k := 1.2, b := 0.75, conjunctive := 0) AS (
            WITH tokens AS (
                SELECT DISTINCT stem(unnest(fts_main_documents.tokenize(query_string)), 'english') AS t
            ),
            fieldids AS (
                SELECT fieldid
                FROM fts_main_documents.fields
                WHERE CASE WHEN fields IS NULL THEN 1 ELSE field IN (SELECT * FROM (SELECT UNNEST(string_split(fields, ','))) AS fsq) END
            ),
            qtermids AS (
                SELECT termid
                FROM fts_main_documents.dict AS dict,
                     tokens
                WHERE dict.term = tokens.t
            ),
            qterms AS (
                SELECT termid,
                       docid
                FROM fts_main_documents.terms AS terms
                WHERE CASE WHEN fields IS NULL THEN 1 ELSE fieldid IN (SELECT * FROM fieldids) END
                  AND termid IN (SELECT qtermids.termid FROM qtermids)
            ),
			term_tf AS (
				SELECT termid,
				   	   docid,
                       COUNT(*) AS tf
				FROM qterms
				GROUP BY docid,
						 termid
			),
			cdocs AS (
				SELECT docid
				FROM qterms
				GROUP BY docid
				HAVING CASE WHEN conjunctive THEN COUNT(DISTINCT termid) = (SELECT COUNT(*) FROM tokens) ELSE 1 END
			),
            subscores AS (
                SELECT docs.docid,
                       len,
                       term_tf.termid,
                       tf,
                       df,
                       (log(((SELECT num_docs FROM fts_main_documents.stats) - df + 0.5) / (df + 0.5) + 1)* ((tf * (k + 1)/(tf + k * (1 - b + b * (len / (SELECT avgdl FROM fts_main_documents.stats))))))) AS subscore
                FROM term_tf,
					 cdocs,
					 fts_main_documents.docs AS docs,
					 fts_main_documents.dict AS dict
				WHERE term_tf.docid = cdocs.docid
				  AND term_tf.docid = docs.docid
                  AND term_tf.termid = dict.termid
            ),
			scores AS (
				SELECT docid,
					   sum(subscore) AS score
				FROM subscores
				GROUP BY docid
			)
            SELECT score
            FROM scores,
				 fts_main_documents.docs AS docs
            WHERE scores.docid = docs.docid
              AND docs.name = docname
        );

SELECT 
    id,
    fts_main_documents.match_bm25(id, 'DuckDB') score_DuckDB,
    fts_main_documents.match_bm25(id, 'DuckDB database') score_DuckDB_Database,
    fts_main_documents.match_bm25(id, 'DuckDB database lorem') score_DuckDB_Database_lorem,
    fts_main_documents.match_bm25(id, 'lorem') score_lorem,
    fts_main_documents.match_bm25(id, 'DuckDB database ipsum') score_DuckDB_Database_ipsum,
FROM 
    documents;
```
Gives following result table:
```
┌─────────┬─────────────────────┬───────────────────────┬─────────────────────────────┬─────────────────────┬─────────────────────────────┐
│   id    │    score_DuckDB     │ score_DuckDB_Database │ score_DuckDB_Database_lorem │     score_lorem     │ score_DuckDB_Database_ipsum │
│ varchar │       double        │        double         │           double            │       double        │           double            │
├─────────┼─────────────────────┼───────────────────────┼─────────────────────────────┼─────────────────────┼─────────────────────────────┤
│ doc1    │ 0.05799194697768673 │   0.11598389395537347 │          0.5419526262276546 │ 0.42596873227228116 │         0.11598389395537347 │
│ doc2    │ 0.05799194697768673 │   0.11598389395537347 │         0.11598389395537347 │                     │          0.3201038766112983 │
│ doc3    │ 0.05799194697768673 │   0.11598389395537347 │         0.11598389395537347 │                     │          0.3201038766112983 │
└─────────┴─────────────────────┴───────────────────────┴─────────────────────────────┴─────────────────────┴─────────────────────────────┘
```
Analysing result table after patch:
1. (Edit, Improved example): Query 'DuckDB database ipsum' (score_DuckDB_Database_ipsum) matches perfectly with documents doc2 and doc3 and scores are better for these documents than doc1
2. (older, more unclear example) Now search query 'DuckDB database lorem' is equal to doc1 content, and gets higher score (score_DuckDB_Database_lorem:0.5419526262276546) than only partially matching query 'lorem' (score_lorem: 0.42596873227228116). 
3. Also all scores are positive.

Please let me know if further details are needed or if I can assist in any other way.

### OS:

Ubuntu

### DuckDB Version:

0.10.3

### DuckDB Client:

CLI

### Full Name:

Jaakko Routamaa

### Affiliation:

-

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a nightly build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [ ] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
18: 
19: ## Installation
20: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
21: 
22: ## Data Import
23: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
24: 
25: ```sql
26: SELECT * FROM 'myfile.csv';
27: SELECT * FROM 'myfile.parquet';
28: ```
29: 
30: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
31: 
32: ## SQL Reference
33: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
34: 
35: ## Development
36: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
37: 
38: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
39: 
40: ## Support
41: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of extension/fts/fts_indexing.cpp]
1: #include "fts_indexing.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/catalog/catalog_search_path.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/string_util.hpp"
7: #include "duckdb/main/client_data.hpp"
8: #include "duckdb/main/connection.hpp"
9: #include "duckdb/parser/qualified_name.hpp"
10: 
11: namespace duckdb {
12: 
13: static QualifiedName GetQualifiedName(ClientContext &context, const string &qname_str) {
14: 	auto qname = QualifiedName::Parse(qname_str);
15: 	if (qname.schema == INVALID_SCHEMA) {
16: 		qname.schema = ClientData::Get(context).catalog_search_path->GetDefaultSchema(qname.catalog);
17: 	}
18: 	return qname;
19: }
20: 
21: static string GetFTSSchema(QualifiedName &qname) {
22: 	auto result = qname.catalog == INVALID_CATALOG ? "" : StringUtil::Format("%s.", qname.catalog);
23: 	result += StringUtil::Format("fts_%s_%s", qname.schema, qname.name);
24: 	return result;
25: }
26: 
27: string FTSIndexing::DropFTSIndexQuery(ClientContext &context, const FunctionParameters &parameters) {
28: 	auto qname = GetQualifiedName(context, StringValue::Get(parameters.values[0]));
29: 	string fts_schema = GetFTSSchema(qname);
30: 
31: 	if (!Catalog::GetSchema(context, qname.catalog, fts_schema, OnEntryNotFound::RETURN_NULL)) {
32: 		throw CatalogException(
33: 		    "a FTS index does not exist on table '%s.%s'. Create one with 'PRAGMA create_fts_index()'.", qname.schema,
34: 		    qname.name);
35: 	}
36: 
37: 	return StringUtil::Format("DROP SCHEMA %s CASCADE;", fts_schema);
38: }
39: 
40: static string IndexingScript(ClientContext &context, QualifiedName &qname, const string &input_id,
41:                              const vector<string> &input_values, const string &stemmer, const string &stopwords,
42:                              const string &ignore, bool strip_accents, bool lower) {
43: 	// clang-format off
44:     string result = R"(
45:         DROP SCHEMA IF EXISTS %fts_schema% CASCADE;
46:         CREATE SCHEMA %fts_schema%;
47:         CREATE TABLE %fts_schema%.stopwords (sw VARCHAR);
48:     )";
49: 	// clang-format on
50: 
51: 	if (stopwords == "none") {
52: 		// do nothing
53: 	} else if (stopwords == "english") {
54: 		// default list of english stopwords from "The SMART system"
55: 		// clang-format off
56:         result += R"(
57:             INSERT INTO %fts_schema%.stopwords VALUES ('a'), ('a''s'), ('able'), ('about'), ('above'), ('according'), ('accordingly'), ('across'), ('actually'), ('after'), ('afterwards'), ('again'), ('against'), ('ain''t'), ('all'), ('allow'), ('allows'), ('almost'), ('alone'), ('along'), ('already'), ('also'), ('although'), ('always'), ('am'), ('among'), ('amongst'), ('an'), ('and'), ('another'), ('any'), ('anybody'), ('anyhow'), ('anyone'), ('anything'), ('anyway'), ('anyways'), ('anywhere'), ('apart'), ('appear'), ('appreciate'), ('appropriate'), ('are'), ('aren''t'), ('around'), ('as'), ('aside'), ('ask'), ('asking'), ('associated'), ('at'), ('available'), ('away'), ('awfully'), ('b'), ('be'), ('became'), ('because'), ('become'), ('becomes'), ('becoming'), ('been'), ('before'), ('beforehand'), ('behind'), ('being'), ('believe'), ('below'), ('beside'), ('besides'), ('best'), ('better'), ('between'), ('beyond'), ('both'), ('brief'), ('but'), ('by'), ('c'), ('c''mon'), ('c''s'), ('came'), ('can'), ('can''t'), ('cannot'), ('cant'), ('cause'), ('causes'), ('certain'), ('certainly'), ('changes'), ('clearly'), ('co'), ('com'), ('come'), ('comes'), ('concerning'), ('consequently'), ('consider'), ('considering'), ('contain'), ('containing'), ('contains'), ('corresponding'), ('could'), ('couldn''t'), ('course'), ('currently'), ('d'), ('definitely'), ('described'), ('despite'), ('did'), ('didn''t'), ('different'), ('do'), ('does'), ('doesn''t'), ('doing'), ('don''t'), ('done'), ('down'), ('downwards'), ('during'), ('e'), ('each'), ('edu'), ('eg'), ('eight'), ('either'), ('else'), ('elsewhere'), ('enough'), ('entirely'), ('especially'), ('et'), ('etc'), ('even'), ('ever'), ('every'), ('everybody'), ('everyone'), ('everything'), ('everywhere'), ('ex'), ('exactly'), ('example'), ('except'), ('f'), ('far'), ('few'), ('fifth'), ('first'), ('five'), ('followed'), ('following'), ('follows'), ('for'), ('former'), ('formerly'), ('forth'), ('four'), ('from'), ('further'), ('furthermore'), ('g'), ('get'), ('gets'), ('getting'), ('given'), ('gives'), ('go'), ('goes'), ('going'), ('gone'), ('got'), ('gotten'), ('greetings'), ('h'), ('had'), ('hadn''t'), ('happens'), ('hardly'), ('has'), ('hasn''t'), ('have'), ('haven''t'), ('having'), ('he'), ('he''s'), ('hello'), ('help'), ('hence'), ('her'), ('here'), ('here''s'), ('hereafter'), ('hereby'), ('herein'), ('hereupon'), ('hers'), ('herself'), ('hi'), ('him'), ('himself'), ('his'), ('hither'), ('hopefully'), ('how'), ('howbeit'), ('however'), ('i'), ('i''d'), ('i''ll'), ('i''m'), ('i''ve'), ('ie'), ('if'), ('ignored'), ('immediate'), ('in'), ('inasmuch'), ('inc'), ('indeed'), ('indicate'), ('indicated'), ('indicates'), ('inner'), ('insofar'), ('instead'), ('into'), ('inward'), ('is'), ('isn''t'), ('it'), ('it''d'), ('it''ll'), ('it''s'), ('its'), ('itself'), ('j'), ('just'), ('k'), ('keep'), ('keeps'), ('kept'), ('know'), ('knows'), ('known'), ('l'), ('last'), ('lately'), ('later'), ('latter'), ('latterly'), ('least'), ('less'), ('lest'), ('let'), ('let''s'), ('like'), ('liked'), ('likely'), ('little'), ('look'), ('looking'), ('looks'), ('ltd'), ('m'), ('mainly'), ('many'), ('may'), ('maybe'), ('me'), ('mean'), ('meanwhile'), ('merely'), ('might'), ('more'), ('moreover'), ('most'), ('mostly'), ('much'), ('must'), ('my'), ('myself'), ('n'), ('name'), ('namely'), ('nd'), ('near'), ('nearly'), ('necessary'), ('need'), ('needs'), ('neither'), ('never'), ('nevertheless'), ('new'), ('next'), ('nine'), ('no'), ('nobody'), ('non'), ('none'), ('noone'), ('nor'), ('normally'), ('not'), ('nothing'), ('novel'), ('now'), ('nowhere'), ('o'), ('obviously'), ('of'), ('off'), ('often'), ('oh'), ('ok'), ('okay'), ('old'), ('on'), ('once'), ('one'), ('ones'), ('only'), ('onto'), ('or'), ('other'), ('others'), ('otherwise'), ('ought'), ('our'), ('ours'), ('ourselves'), ('out'), ('outside'), ('over'), ('overall'), ('own');
58:             INSERT INTO %fts_schema%.stopwords VALUES ('p'), ('particular'), ('particularly'), ('per'), ('perhaps'), ('placed'), ('please'), ('plus'), ('possible'), ('presumably'), ('probably'), ('provides'), ('q'), ('que'), ('quite'), ('qv'), ('r'), ('rather'), ('rd'), ('re'), ('really'), ('reasonably'), ('regarding'), ('regardless'), ('regards'), ('relatively'), ('respectively'), ('right'), ('s'), ('said'), ('same'), ('saw'), ('say'), ('saying'), ('says'), ('second'), ('secondly'), ('see'), ('seeing'), ('seem'), ('seemed'), ('seeming'), ('seems'), ('seen'), ('self'), ('selves'), ('sensible'), ('sent'), ('serious'), ('seriously'), ('seven'), ('several'), ('shall'), ('she'), ('should'), ('shouldn''t'), ('since'), ('six'), ('so'), ('some'), ('somebody'), ('somehow'), ('someone'), ('something'), ('sometime'), ('sometimes'), ('somewhat'), ('somewhere'), ('soon'), ('sorry'), ('specified'), ('specify'), ('specifying'), ('still'), ('sub'), ('such'), ('sup'), ('sure'), ('t'), ('t''s'), ('take'), ('taken'), ('tell'), ('tends'), ('th'), ('than'), ('thank'), ('thanks'), ('thanx'), ('that'), ('that''s'), ('thats'), ('the'), ('their'), ('theirs'), ('them'), ('themselves'), ('then'), ('thence'), ('there'), ('there''s'), ('thereafter'), ('thereby'), ('therefore'), ('therein'), ('theres'), ('thereupon'), ('these'), ('they'), ('they''d'), ('they''ll'), ('they''re'), ('they''ve'), ('think'), ('third'), ('this'), ('thorough'), ('thoroughly'), ('those'), ('though'), ('three'), ('through'), ('throughout'), ('thru'), ('thus'), ('to'), ('together'), ('too'), ('took'), ('toward'), ('towards'), ('tried'), ('tries'), ('truly'), ('try'), ('trying'), ('twice'), ('two'), ('u'), ('un'), ('under'), ('unfortunately'), ('unless'), ('unlikely'), ('until'), ('unto'), ('up'), ('upon'), ('us'), ('use'), ('used'), ('useful'), ('uses'), ('using'), ('usually'), ('uucp'), ('v'), ('value'), ('various'), ('very'), ('via'), ('viz'), ('vs'), ('w'), ('want'), ('wants'), ('was'), ('wasn''t'), ('way'), ('we'), ('we''d'), ('we''ll'), ('we''re'), ('we''ve'), ('welcome'), ('well'), ('went'), ('were'), ('weren''t'), ('what'), ('what''s'), ('whatever'), ('when'), ('whence'), ('whenever'), ('where'), ('where''s'), ('whereafter'), ('whereas'), ('whereby'), ('wherein'), ('whereupon'), ('wherever'), ('whether'), ('which'), ('while'), ('whither'), ('who'), ('who''s'), ('whoever'), ('whole'), ('whom'), ('whose'), ('why'), ('will'), ('willing'), ('wish'), ('with'), ('within'), ('without'), ('won''t'), ('wonder'), ('would'), ('would'), ('wouldn''t'), ('x'), ('y'), ('yes'), ('yet'), ('you'), ('you''d'), ('you''ll'), ('you''re'), ('you''ve'), ('your'), ('yours'), ('yourself'), ('yourselves'), ('z'), ('zero');
59:         )";
60: 		// clang-format on
61: 	} else {
62: 		// custom stopwords
63: 		result += "INSERT INTO %fts_schema%.stopwords SELECT * FROM " + stopwords + ";";
64: 	}
65: 
66: 	// create tokenize macro based on parameters
67: 	string tokenize = "s::VARCHAR";
68: 	vector<string> before;
69: 	vector<string> after;
70: 	if (strip_accents) {
71: 		tokenize = "strip_accents(" + tokenize + ")";
72: 	}
73: 	if (lower) {
74: 		tokenize = "lower(" + tokenize + ")";
75: 	}
76: 	tokenize = "regexp_replace(" + tokenize + ", '" + ignore + "', " + "' ', 'g')";
77: 	tokenize = "string_split_regex(" + tokenize + ", '\\s+')";
78: 	result += "CREATE MACRO %fts_schema%.tokenize(s) AS " + tokenize + ";";
79: 
80: 	// parameterized definition of indexing and retrieval model
81: 	// clang-format off
82: 	result += R"(
83:         CREATE TABLE %fts_schema%.docs AS (
84:             SELECT rowid AS docid,
85:                    "%input_id%" AS name
86:             FROM %input_table%
87:         );
88: 
89: 	    CREATE TABLE %fts_schema%.fields (fieldid BIGINT, field VARCHAR);
90: 	    INSERT INTO %fts_schema%.fields VALUES %field_values%;
91: 
92:         CREATE TABLE %fts_schema%.terms AS
93:         WITH tokenized AS (
94:             %union_fields_query%
95:         ),
96: 	    stemmed_stopped AS (
97:             SELECT stem(t.w, '%stemmer%') AS term,
98: 	               t.docid AS docid,
99:                    t.fieldid AS fieldid
100: 	        FROM tokenized AS t
101: 	        WHERE t.w NOT NULL
102:               AND len(t.w) > 0
103: 	          AND t.w NOT IN (SELECT sw FROM %fts_schema%.stopwords)
104:         )
105: 	    SELECT ss.term,
106: 	           ss.docid,
107: 	           ss.fieldid
108:         FROM stemmed_stopped AS ss;
109: 
110:         ALTER TABLE %fts_schema%.docs ADD len BIGINT;
111:         UPDATE %fts_schema%.docs d
112:         SET len = (
113:             SELECT count(term)
114:             FROM %fts_schema%.terms AS t
115:             WHERE t.docid = d.docid
116:         );
117: 
118:         CREATE TABLE %fts_schema%.dict AS
119:         WITH distinct_terms AS (
120:             SELECT DISTINCT term
121:             FROM %fts_schema%.terms
122:             ORDER BY docid, term
123:         )
124:         SELECT row_number() OVER () - 1 AS termid,
125:                dt.term
126:         FROM distinct_terms AS dt;
127: 
128:         ALTER TABLE %fts_schema%.terms ADD termid BIGINT;
129:         UPDATE %fts_schema%.terms t
130:         SET termid = (
131:             SELECT termid
132:             FROM %fts_schema%.dict d
133:             WHERE t.term = d.term
134:         );
135:         ALTER TABLE %fts_schema%.terms DROP term;
136: 
137:         ALTER TABLE %fts_schema%.dict ADD df BIGINT;
138:         UPDATE %fts_schema%.dict d
139:         SET df = (
140:             SELECT count(distinct docid)
141:             FROM %fts_schema%.terms t
142:             WHERE d.termid = t.termid
143:             GROUP BY termid
144:         );
145: 
146:         CREATE TABLE %fts_schema%.stats AS (
147:             SELECT COUNT(docs.docid) AS num_docs,
148:                    SUM(docs.len) / COUNT(docs.len) AS avgdl
149:             FROM %fts_schema%.docs AS docs
150:         );
151: 
152:         CREATE MACRO %fts_schema%.match_bm25(docname, query_string, fields := NULL, k := 1.2, b := 0.75, conjunctive := 0) AS (
153:             WITH tokens AS (
154:                 SELECT DISTINCT stem(unnest(%fts_schema%.tokenize(query_string)), '%stemmer%') AS t
155:             ),
156:             fieldids AS (
157:                 SELECT fieldid
158:                 FROM %fts_schema%.fields
159:                 WHERE CASE WHEN fields IS NULL THEN 1 ELSE field IN (SELECT * FROM (SELECT UNNEST(string_split(fields, ','))) AS fsq) END
160:             ),
161:             qtermids AS (
162:                 SELECT termid
163:                 FROM %fts_schema%.dict AS dict,
164:                      tokens
165:                 WHERE dict.term = tokens.t
166:             ),
167:             qterms AS (
168:                 SELECT termid,
169:                        docid
170:                 FROM %fts_schema%.terms AS terms
171:                 WHERE CASE WHEN fields IS NULL THEN 1 ELSE fieldid IN (SELECT * FROM fieldids) END
172:                   AND termid IN (SELECT qtermids.termid FROM qtermids)
173:             ),
174: 			term_tf AS (
175: 				SELECT termid,
176: 				   	   docid,
177:                        COUNT(*) AS tf
178: 				FROM qterms
179: 				GROUP BY docid,
180: 						 termid
181: 			),
182: 			cdocs AS (
183: 				SELECT docid
184: 				FROM qterms
185: 				GROUP BY docid
186: 				HAVING CASE WHEN conjunctive THEN COUNT(DISTINCT termid) = (SELECT COUNT(*) FROM tokens) ELSE 1 END
187: 			),
188:             subscores AS (
189:                 SELECT docs.docid,
190:                        len,
191:                        term_tf.termid,
192:                        tf,
193:                        df,
194:                        (log(((SELECT num_docs FROM %fts_schema%.stats) - df + 0.5) / (df + 0.5))* ((tf * (k + 1)/(tf + k * (1 - b + b * (len / (SELECT avgdl FROM %fts_schema%.stats))))))) AS subscore
195:                 FROM term_tf,
196: 					 cdocs,
197: 					 %fts_schema%.docs AS docs,
198: 					 %fts_schema%.dict AS dict
199: 				WHERE term_tf.docid = cdocs.docid
200: 				  AND term_tf.docid = docs.docid
201:                   AND term_tf.termid = dict.termid
202:             ),
203: 			scores AS (
204: 				SELECT docid,
205: 					   sum(subscore) AS score
206: 				FROM subscores
207: 				GROUP BY docid
208: 			)
209:             SELECT score
210:             FROM scores,
211: 				 %fts_schema%.docs AS docs
212:             WHERE scores.docid = docs.docid
213:               AND docs.name = docname
214:         );
215:     )";
216: 
217:     // we may have more than 1 input field, therefore we union over the fields, retaining information which field it came from
218: 	string tokenize_field_query = R"(
219:         SELECT unnest(%fts_schema%.tokenize(fts_ii."%input_value%")) AS w,
220: 	           rowid AS docid,
221: 	           (SELECT fieldid FROM %fts_schema%.fields WHERE field = '%input_value%') AS fieldid
222:         FROM %input_table% AS fts_ii
223:     )";
224: 	// clang-format on
225: 	vector<string> field_values;
226: 	vector<string> tokenize_fields;
227: 	for (idx_t i = 0; i < input_values.size(); i++) {
228: 		field_values.push_back(StringUtil::Format("(%i, '%s')", i, input_values[i]));
229: 		tokenize_fields.push_back(StringUtil::Replace(tokenize_field_query, "%input_value%", input_values[i]));
230: 	}
231: 	result = StringUtil::Replace(result, "%field_values%", StringUtil::Join(field_values, ", "));
232: 	result = StringUtil::Replace(result, "%union_fields_query%", StringUtil::Join(tokenize_fields, " UNION ALL "));
233: 
234: 	string fts_schema = GetFTSSchema(qname);
235: 	string input_table = qname.catalog == INVALID_CATALOG ? "" : StringUtil::Format("%s.", qname.catalog);
236: 	input_table += StringUtil::Format("%s.%s", qname.schema, qname.name);
237: 
238: 	// fill in variables (inefficiently, but keeps SQL script readable)
239: 	result = StringUtil::Replace(result, "%fts_schema%", fts_schema);
240: 	result = StringUtil::Replace(result, "%input_table%", input_table);
241: 	result = StringUtil::Replace(result, "%input_id%", input_id);
242: 	result = StringUtil::Replace(result, "%stemmer%", stemmer);
243: 
244: 	return result;
245: }
246: 
247: static void CheckIfTableExists(ClientContext &context, QualifiedName &qname) {
248: 	Catalog::GetEntry<TableCatalogEntry>(context, qname.catalog, qname.schema, qname.name);
249: }
250: 
251: string FTSIndexing::CreateFTSIndexQuery(ClientContext &context, const FunctionParameters &parameters) {
252: 	auto qname = GetQualifiedName(context, StringValue::Get(parameters.values[0]));
253: 	CheckIfTableExists(context, qname);
254: 
255: 	// get named parameters
256: 	string stemmer = "porter";
257: 	auto stemmer_entry = parameters.named_parameters.find("stemmer");
258: 	if (stemmer_entry != parameters.named_parameters.end()) {
259: 		stemmer = StringValue::Get(stemmer_entry->second);
260: 	}
261: 
262: 	string stopwords = "english";
263: 	auto stopword_entry = parameters.named_parameters.find("stopwords");
264: 	if (stopword_entry != parameters.named_parameters.end()) {
265: 		stopwords = StringValue::Get(stopword_entry->second);
266: 		if (stopwords != "english" && stopwords != "none") {
267: 			auto stopwords_qname = GetQualifiedName(context, stopwords);
268: 			CheckIfTableExists(context, stopwords_qname);
269: 		}
270: 	}
271: 
272: 	string ignore = "[0-9!@#$%^&*()_+={}\\[\\]:;<>,.?~\\\\/\\|''\"`-]+";
273: 	auto ignore_entry = parameters.named_parameters.find("ignore");
274: 	if (ignore_entry != parameters.named_parameters.end()) {
275: 		ignore = StringValue::Get(ignore_entry->second);
276: 	}
277: 
278: 	bool strip_accents = true;
279: 	auto strip_accents_entry = parameters.named_parameters.find("strip_accents");
280: 	if (strip_accents_entry != parameters.named_parameters.end()) {
281: 		strip_accents = BooleanValue::Get(strip_accents_entry->second);
282: 	}
283: 
284: 	bool lower = true;
285: 	auto lower_entry = parameters.named_parameters.find("lower");
286: 	if (lower_entry != parameters.named_parameters.end()) {
287: 		lower = BooleanValue::Get(lower_entry->second);
288: 	}
289: 
290: 	bool overwrite = false;
291: 	auto overwrite_entry = parameters.named_parameters.find("overwrite");
292: 	if (overwrite_entry != parameters.named_parameters.end()) {
293: 		overwrite = BooleanValue::Get(overwrite_entry->second);
294: 	}
295: 
296: 	// throw error if an index already exists on this table
297: 	const string fts_schema = GetFTSSchema(qname);
298: 	if (Catalog::GetSchema(context, qname.catalog, fts_schema, OnEntryNotFound::RETURN_NULL) && !overwrite) {
299: 		throw CatalogException("a FTS index already exists on table '%s.%s'. Supply 'overwrite=1' to overwrite, or "
300: 		                       "drop the existing index with 'PRAGMA drop_fts_index()' before creating a new one.",
301: 		                       qname.schema, qname.name);
302: 	}
303: 
304: 	// positional parameters
305: 	auto doc_id = StringValue::Get(parameters.values[1]);
306: 	// check all specified columns
307: 	auto &table = Catalog::GetEntry<TableCatalogEntry>(context, qname.catalog, qname.schema, qname.name);
308: 	vector<string> doc_values;
309: 	for (idx_t i = 2; i < parameters.values.size(); i++) {
310: 		string col_name = StringValue::Get(parameters.values[i]);
311: 		if (col_name == "*") {
312: 			// star found - get all columns
313: 			doc_values.clear();
314: 			for (auto &cd : table.GetColumns().Logical()) {
315: 				if (cd.Type() == LogicalType::VARCHAR) {
316: 					doc_values.push_back(cd.Name());
317: 				}
318: 			}
319: 			break;
320: 		}
321: 		if (!table.ColumnExists(col_name)) {
322: 			// we check this here because else we we end up with an error halfway the indexing script
323: 			throw CatalogException("Table '%s.%s' does not have a column named '%s'!", qname.schema, qname.name,
324: 			                       col_name);
325: 		}
326: 		doc_values.push_back(col_name);
327: 	}
328: 	if (doc_values.empty()) {
329: 		throw InvalidInputException("at least one column must be supplied for indexing!");
330: 	}
331: 
332: 	return IndexingScript(context, qname, doc_id, doc_values, stemmer, stopwords, ignore, strip_accents, lower);
333: }
334: 
335: } // namespace duckdb
[end of extension/fts/fts_indexing.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: