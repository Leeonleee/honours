You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
An error was encountered using both `distinct` and `struct`
### What happens?

![image](https://github.com/user-attachments/assets/4cc06266-e2e5-4088-88a5-013e210788a4)


### To Reproduce

```sql
select 
    category,
    array_agg(distinct name) filter(where id != 5)            as a_list,
    array_agg(name)          filter(where id != 5)            as b_list,
    array_agg({'id': id, 'name': name, 'catetory': category}) as c_list
from (
    select 1 as id, '大熊猫' as name, '熊' as category union all
    select 2 as id, '大熊猫' as name, '猫' as category union all
    select 3 as id, '小熊猫' as name, '猫' as category
) t
group by category;
```

```console
INTERNAL Error: Attempted to dereference shared_ptr that is NULL!
This error signals an assertion failure within DuckDB. This usually occurs due to unexpected conditions or errors in the program's logic.
For more information, see https://duckdb.org/docs/dev/internal_errors
```

### OS:

ubuntu/x86_64

### DuckDB Version:

v1.1.1 af39bd0dcf

### DuckDB Client:

cli

### Hardware:

_No response_

### Full Name:

icefery

### Affiliation:

icefery@163.com

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have not tested with any build

### Did you include all relevant data sets for reproducing the issue?

No - Other reason (please specify in the issue body)

### Did you include all code required to reproduce the issue?

- [ ] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [ ] Yes, I have
An error was encountered using both `distinct` and `struct`
### What happens?

![image](https://github.com/user-attachments/assets/4cc06266-e2e5-4088-88a5-013e210788a4)


### To Reproduce

```sql
select 
    category,
    array_agg(distinct name) filter(where id != 5)            as a_list,
    array_agg(name)          filter(where id != 5)            as b_list,
    array_agg({'id': id, 'name': name, 'catetory': category}) as c_list
from (
    select 1 as id, '大熊猫' as name, '熊' as category union all
    select 2 as id, '大熊猫' as name, '猫' as category union all
    select 3 as id, '小熊猫' as name, '猫' as category
) t
group by category;
```

```console
INTERNAL Error: Attempted to dereference shared_ptr that is NULL!
This error signals an assertion failure within DuckDB. This usually occurs due to unexpected conditions or errors in the program's logic.
For more information, see https://duckdb.org/docs/dev/internal_errors
```

### OS:

ubuntu/x86_64

### DuckDB Version:

v1.1.1 af39bd0dcf

### DuckDB Client:

cli

### Hardware:

_No response_

### Full Name:

icefery

### Affiliation:

icefery@163.com

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have not tested with any build

### Did you include all relevant data sets for reproducing the issue?

No - Other reason (please specify in the issue body)

### Did you include all code required to reproduce the issue?

- [ ] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [ ] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://duckdb.org/docs/installation/) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/execution/operator/aggregate/physical_hash_aggregate.cpp]
1: #include "duckdb/execution/operator/aggregate/physical_hash_aggregate.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/aggregate_function_catalog_entry.hpp"
4: #include "duckdb/common/atomic.hpp"
5: #include "duckdb/common/optional_idx.hpp"
6: #include "duckdb/common/vector_operations/vector_operations.hpp"
7: #include "duckdb/execution/aggregate_hashtable.hpp"
8: #include "duckdb/execution/operator/aggregate/distinct_aggregate_data.hpp"
9: #include "duckdb/main/client_context.hpp"
10: #include "duckdb/parallel/base_pipeline_event.hpp"
11: #include "duckdb/parallel/interrupt.hpp"
12: #include "duckdb/parallel/pipeline.hpp"
13: #include "duckdb/parallel/task_scheduler.hpp"
14: #include "duckdb/parallel/thread_context.hpp"
15: #include "duckdb/parallel/executor_task.hpp"
16: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
17: #include "duckdb/planner/expression/bound_constant_expression.hpp"
18: #include "duckdb/planner/expression/bound_reference_expression.hpp"
19: 
20: namespace duckdb {
21: 
22: HashAggregateGroupingData::HashAggregateGroupingData(GroupingSet &grouping_set_p,
23:                                                      const GroupedAggregateData &grouped_aggregate_data,
24:                                                      unique_ptr<DistinctAggregateCollectionInfo> &info)
25:     : table_data(grouping_set_p, grouped_aggregate_data) {
26: 	if (info) {
27: 		distinct_data = make_uniq<DistinctAggregateData>(*info, grouping_set_p, &grouped_aggregate_data.groups);
28: 	}
29: }
30: 
31: bool HashAggregateGroupingData::HasDistinct() const {
32: 	return distinct_data != nullptr;
33: }
34: 
35: HashAggregateGroupingGlobalState::HashAggregateGroupingGlobalState(const HashAggregateGroupingData &data,
36:                                                                    ClientContext &context) {
37: 	table_state = data.table_data.GetGlobalSinkState(context);
38: 	if (data.HasDistinct()) {
39: 		distinct_state = make_uniq<DistinctAggregateState>(*data.distinct_data, context);
40: 	}
41: }
42: 
43: HashAggregateGroupingLocalState::HashAggregateGroupingLocalState(const PhysicalHashAggregate &op,
44:                                                                  const HashAggregateGroupingData &data,
45:                                                                  ExecutionContext &context) {
46: 	table_state = data.table_data.GetLocalSinkState(context);
47: 	if (!data.HasDistinct()) {
48: 		return;
49: 	}
50: 	auto &distinct_data = *data.distinct_data;
51: 
52: 	auto &distinct_indices = op.distinct_collection_info->Indices();
53: 	D_ASSERT(!distinct_indices.empty());
54: 
55: 	distinct_states.resize(op.distinct_collection_info->aggregates.size());
56: 	auto &table_map = op.distinct_collection_info->table_map;
57: 
58: 	for (auto &idx : distinct_indices) {
59: 		idx_t table_idx = table_map[idx];
60: 		auto &radix_table = distinct_data.radix_tables[table_idx];
61: 		if (radix_table == nullptr) {
62: 			// This aggregate has identical input as another aggregate, so no table is created for it
63: 			continue;
64: 		}
65: 		// Initialize the states of the radix tables used for the distinct aggregates
66: 		distinct_states[table_idx] = radix_table->GetLocalSinkState(context);
67: 	}
68: }
69: 
70: static vector<LogicalType> CreateGroupChunkTypes(vector<unique_ptr<Expression>> &groups) {
71: 	set<idx_t> group_indices;
72: 
73: 	if (groups.empty()) {
74: 		return {};
75: 	}
76: 
77: 	for (auto &group : groups) {
78: 		D_ASSERT(group->type == ExpressionType::BOUND_REF);
79: 		auto &bound_ref = group->Cast<BoundReferenceExpression>();
80: 		group_indices.insert(bound_ref.index);
81: 	}
82: 	idx_t highest_index = *group_indices.rbegin();
83: 	vector<LogicalType> types(highest_index + 1, LogicalType::SQLNULL);
84: 	for (auto &group : groups) {
85: 		auto &bound_ref = group->Cast<BoundReferenceExpression>();
86: 		types[bound_ref.index] = bound_ref.return_type;
87: 	}
88: 	return types;
89: }
90: 
91: bool PhysicalHashAggregate::CanSkipRegularSink() const {
92: 	if (!filter_indexes.empty()) {
93: 		// If we have filters, we can't skip the regular sink, because we might lose groups otherwise.
94: 		return false;
95: 	}
96: 	if (grouped_aggregate_data.aggregates.empty()) {
97: 		// When there are no aggregates, we have to add to the main ht right away
98: 		return false;
99: 	}
100: 	if (!non_distinct_filter.empty()) {
101: 		return false;
102: 	}
103: 	return true;
104: }
105: 
106: PhysicalHashAggregate::PhysicalHashAggregate(ClientContext &context, vector<LogicalType> types,
107:                                              vector<unique_ptr<Expression>> expressions, idx_t estimated_cardinality)
108:     : PhysicalHashAggregate(context, std::move(types), std::move(expressions), {}, estimated_cardinality) {
109: }
110: 
111: PhysicalHashAggregate::PhysicalHashAggregate(ClientContext &context, vector<LogicalType> types,
112:                                              vector<unique_ptr<Expression>> expressions,
113:                                              vector<unique_ptr<Expression>> groups_p, idx_t estimated_cardinality)
114:     : PhysicalHashAggregate(context, std::move(types), std::move(expressions), std::move(groups_p), {}, {},
115:                             estimated_cardinality) {
116: }
117: 
118: PhysicalHashAggregate::PhysicalHashAggregate(ClientContext &context, vector<LogicalType> types,
119:                                              vector<unique_ptr<Expression>> expressions,
120:                                              vector<unique_ptr<Expression>> groups_p,
121:                                              vector<GroupingSet> grouping_sets_p,
122:                                              vector<unsafe_vector<idx_t>> grouping_functions_p,
123:                                              idx_t estimated_cardinality)
124:     : PhysicalOperator(PhysicalOperatorType::HASH_GROUP_BY, std::move(types), estimated_cardinality),
125:       grouping_sets(std::move(grouping_sets_p)) {
126: 	// get a list of all aggregates to be computed
127: 	const idx_t group_count = groups_p.size();
128: 	if (grouping_sets.empty()) {
129: 		GroupingSet set;
130: 		for (idx_t i = 0; i < group_count; i++) {
131: 			set.insert(i);
132: 		}
133: 		grouping_sets.push_back(std::move(set));
134: 	}
135: 	input_group_types = CreateGroupChunkTypes(groups_p);
136: 
137: 	grouped_aggregate_data.InitializeGroupby(std::move(groups_p), std::move(expressions),
138: 	                                         std::move(grouping_functions_p));
139: 
140: 	auto &aggregates = grouped_aggregate_data.aggregates;
141: 	// filter_indexes must be pre-built, not lazily instantiated in parallel...
142: 	// Because everything that lives in this class should be read-only at execution time
143: 	idx_t aggregate_input_idx = 0;
144: 	for (idx_t i = 0; i < aggregates.size(); i++) {
145: 		auto &aggregate = aggregates[i];
146: 		auto &aggr = aggregate->Cast<BoundAggregateExpression>();
147: 		aggregate_input_idx += aggr.children.size();
148: 		if (aggr.aggr_type == AggregateType::DISTINCT) {
149: 			distinct_filter.push_back(i);
150: 		} else if (aggr.aggr_type == AggregateType::NON_DISTINCT) {
151: 			non_distinct_filter.push_back(i);
152: 		} else { // LCOV_EXCL_START
153: 			throw NotImplementedException("AggregateType not implemented in PhysicalHashAggregate");
154: 		} // LCOV_EXCL_STOP
155: 	}
156: 
157: 	for (idx_t i = 0; i < aggregates.size(); i++) {
158: 		auto &aggregate = aggregates[i];
159: 		auto &aggr = aggregate->Cast<BoundAggregateExpression>();
160: 		if (aggr.filter) {
161: 			auto &bound_ref_expr = aggr.filter->Cast<BoundReferenceExpression>();
162: 			if (!filter_indexes.count(aggr.filter.get())) {
163: 				// Replace the bound reference expression's index with the corresponding index of the payload chunk
164: 				filter_indexes[aggr.filter.get()] = bound_ref_expr.index;
165: 				bound_ref_expr.index = aggregate_input_idx;
166: 			}
167: 			aggregate_input_idx++;
168: 		}
169: 	}
170: 
171: 	distinct_collection_info = DistinctAggregateCollectionInfo::Create(grouped_aggregate_data.aggregates);
172: 
173: 	for (idx_t i = 0; i < grouping_sets.size(); i++) {
174: 		groupings.emplace_back(grouping_sets[i], grouped_aggregate_data, distinct_collection_info);
175: 	}
176: }
177: 
178: //===--------------------------------------------------------------------===//
179: // Sink
180: //===--------------------------------------------------------------------===//
181: class HashAggregateGlobalSinkState : public GlobalSinkState {
182: public:
183: 	HashAggregateGlobalSinkState(const PhysicalHashAggregate &op, ClientContext &context) {
184: 		grouping_states.reserve(op.groupings.size());
185: 		for (idx_t i = 0; i < op.groupings.size(); i++) {
186: 			auto &grouping = op.groupings[i];
187: 			grouping_states.emplace_back(grouping, context);
188: 		}
189: 		vector<LogicalType> filter_types;
190: 		for (auto &aggr : op.grouped_aggregate_data.aggregates) {
191: 			auto &aggregate = aggr->Cast<BoundAggregateExpression>();
192: 			for (auto &child : aggregate.children) {
193: 				payload_types.push_back(child->return_type);
194: 			}
195: 			if (aggregate.filter) {
196: 				filter_types.push_back(aggregate.filter->return_type);
197: 			}
198: 		}
199: 		payload_types.reserve(payload_types.size() + filter_types.size());
200: 		payload_types.insert(payload_types.end(), filter_types.begin(), filter_types.end());
201: 	}
202: 
203: 	vector<HashAggregateGroupingGlobalState> grouping_states;
204: 	vector<LogicalType> payload_types;
205: 	//! Whether or not the aggregate is finished
206: 	bool finished = false;
207: };
208: 
209: class HashAggregateLocalSinkState : public LocalSinkState {
210: public:
211: 	HashAggregateLocalSinkState(const PhysicalHashAggregate &op, ExecutionContext &context) {
212: 
213: 		auto &payload_types = op.grouped_aggregate_data.payload_types;
214: 		if (!payload_types.empty()) {
215: 			aggregate_input_chunk.InitializeEmpty(payload_types);
216: 		}
217: 
218: 		grouping_states.reserve(op.groupings.size());
219: 		for (auto &grouping : op.groupings) {
220: 			grouping_states.emplace_back(op, grouping, context);
221: 		}
222: 		// The filter set is only needed here for the distinct aggregates
223: 		// the filtering of data for the regular aggregates is done within the hashtable
224: 		vector<AggregateObject> aggregate_objects;
225: 		for (auto &aggregate : op.grouped_aggregate_data.aggregates) {
226: 			auto &aggr = aggregate->Cast<BoundAggregateExpression>();
227: 			aggregate_objects.emplace_back(&aggr);
228: 		}
229: 
230: 		filter_set.Initialize(context.client, aggregate_objects, payload_types);
231: 	}
232: 
233: 	DataChunk aggregate_input_chunk;
234: 	vector<HashAggregateGroupingLocalState> grouping_states;
235: 	AggregateFilterDataSet filter_set;
236: };
237: 
238: void PhysicalHashAggregate::SetMultiScan(GlobalSinkState &state) {
239: 	auto &gstate = state.Cast<HashAggregateGlobalSinkState>();
240: 	for (auto &grouping_state : gstate.grouping_states) {
241: 		RadixPartitionedHashTable::SetMultiScan(*grouping_state.table_state);
242: 		if (!grouping_state.distinct_state) {
243: 			continue;
244: 		}
245: 	}
246: }
247: 
248: //===--------------------------------------------------------------------===//
249: // Sink
250: //===--------------------------------------------------------------------===//
251: unique_ptr<GlobalSinkState> PhysicalHashAggregate::GetGlobalSinkState(ClientContext &context) const {
252: 	return make_uniq<HashAggregateGlobalSinkState>(*this, context);
253: }
254: 
255: unique_ptr<LocalSinkState> PhysicalHashAggregate::GetLocalSinkState(ExecutionContext &context) const {
256: 	return make_uniq<HashAggregateLocalSinkState>(*this, context);
257: }
258: 
259: void PhysicalHashAggregate::SinkDistinctGrouping(ExecutionContext &context, DataChunk &chunk, OperatorSinkInput &input,
260:                                                  idx_t grouping_idx) const {
261: 	auto &sink = input.local_state.Cast<HashAggregateLocalSinkState>();
262: 	auto &global_sink = input.global_state.Cast<HashAggregateGlobalSinkState>();
263: 
264: 	auto &grouping_gstate = global_sink.grouping_states[grouping_idx];
265: 	auto &grouping_lstate = sink.grouping_states[grouping_idx];
266: 	auto &distinct_info = *distinct_collection_info;
267: 
268: 	auto &distinct_state = grouping_gstate.distinct_state;
269: 	auto &distinct_data = groupings[grouping_idx].distinct_data;
270: 
271: 	DataChunk empty_chunk;
272: 
273: 	// Create an empty filter for Sink, since we don't need to update any aggregate states here
274: 	unsafe_vector<idx_t> empty_filter;
275: 
276: 	for (idx_t &idx : distinct_info.indices) {
277: 		auto &aggregate = grouped_aggregate_data.aggregates[idx]->Cast<BoundAggregateExpression>();
278: 
279: 		D_ASSERT(distinct_info.table_map.count(idx));
280: 		idx_t table_idx = distinct_info.table_map[idx];
281: 		if (!distinct_data->radix_tables[table_idx]) {
282: 			continue;
283: 		}
284: 		D_ASSERT(distinct_data->radix_tables[table_idx]);
285: 		auto &radix_table = *distinct_data->radix_tables[table_idx];
286: 		auto &radix_global_sink = *distinct_state->radix_states[table_idx];
287: 		auto &radix_local_sink = *grouping_lstate.distinct_states[table_idx];
288: 
289: 		InterruptState interrupt_state;
290: 		OperatorSinkInput sink_input {radix_global_sink, radix_local_sink, interrupt_state};
291: 
292: 		if (aggregate.filter) {
293: 			DataChunk filter_chunk;
294: 			auto &filtered_data = sink.filter_set.GetFilterData(idx);
295: 			filter_chunk.InitializeEmpty(filtered_data.filtered_payload.GetTypes());
296: 
297: 			// Add the filter Vector (BOOL)
298: 			auto it = filter_indexes.find(aggregate.filter.get());
299: 			D_ASSERT(it != filter_indexes.end());
300: 			D_ASSERT(it->second < chunk.data.size());
301: 			auto &filter_bound_ref = aggregate.filter->Cast<BoundReferenceExpression>();
302: 			filter_chunk.data[filter_bound_ref.index].Reference(chunk.data[it->second]);
303: 			filter_chunk.SetCardinality(chunk.size());
304: 
305: 			// We cant use the AggregateFilterData::ApplyFilter method, because the chunk we need to
306: 			// apply the filter to also has the groups, and the filtered_data.filtered_payload does not have those.
307: 			SelectionVector sel_vec(STANDARD_VECTOR_SIZE);
308: 			idx_t count = filtered_data.filter_executor.SelectExpression(filter_chunk, sel_vec);
309: 
310: 			if (count == 0) {
311: 				continue;
312: 			}
313: 
314: 			// Because the 'input' chunk needs to be re-used after this, we need to create
315: 			// a duplicate of it, that we can apply the filter to
316: 			DataChunk filtered_input;
317: 			filtered_input.InitializeEmpty(chunk.GetTypes());
318: 
319: 			for (idx_t group_idx = 0; group_idx < grouped_aggregate_data.groups.size(); group_idx++) {
320: 				auto &group = grouped_aggregate_data.groups[group_idx];
321: 				auto &bound_ref = group->Cast<BoundReferenceExpression>();
322: 				filtered_input.data[bound_ref.index].Reference(chunk.data[bound_ref.index]);
323: 			}
324: 			for (idx_t child_idx = 0; child_idx < aggregate.children.size(); child_idx++) {
325: 				auto &child = aggregate.children[child_idx];
326: 				auto &bound_ref = child->Cast<BoundReferenceExpression>();
327: 
328: 				filtered_input.data[bound_ref.index].Reference(chunk.data[bound_ref.index]);
329: 			}
330: 			filtered_input.Slice(sel_vec, count);
331: 			filtered_input.SetCardinality(count);
332: 
333: 			radix_table.Sink(context, filtered_input, sink_input, empty_chunk, empty_filter);
334: 		} else {
335: 			radix_table.Sink(context, chunk, sink_input, empty_chunk, empty_filter);
336: 		}
337: 	}
338: }
339: 
340: void PhysicalHashAggregate::SinkDistinct(ExecutionContext &context, DataChunk &chunk, OperatorSinkInput &input) const {
341: 	for (idx_t i = 0; i < groupings.size(); i++) {
342: 		SinkDistinctGrouping(context, chunk, input, i);
343: 	}
344: }
345: 
346: SinkResultType PhysicalHashAggregate::Sink(ExecutionContext &context, DataChunk &chunk,
347:                                            OperatorSinkInput &input) const {
348: 	auto &local_state = input.local_state.Cast<HashAggregateLocalSinkState>();
349: 	auto &global_state = input.global_state.Cast<HashAggregateGlobalSinkState>();
350: 
351: 	if (distinct_collection_info) {
352: 		SinkDistinct(context, chunk, input);
353: 	}
354: 
355: 	if (CanSkipRegularSink()) {
356: 		return SinkResultType::NEED_MORE_INPUT;
357: 	}
358: 
359: 	DataChunk &aggregate_input_chunk = local_state.aggregate_input_chunk;
360: 	auto &aggregates = grouped_aggregate_data.aggregates;
361: 	idx_t aggregate_input_idx = 0;
362: 
363: 	// Populate the aggregate child vectors
364: 	for (auto &aggregate : aggregates) {
365: 		auto &aggr = aggregate->Cast<BoundAggregateExpression>();
366: 		for (auto &child_expr : aggr.children) {
367: 			D_ASSERT(child_expr->type == ExpressionType::BOUND_REF);
368: 			auto &bound_ref_expr = child_expr->Cast<BoundReferenceExpression>();
369: 			D_ASSERT(bound_ref_expr.index < chunk.data.size());
370: 			aggregate_input_chunk.data[aggregate_input_idx++].Reference(chunk.data[bound_ref_expr.index]);
371: 		}
372: 	}
373: 	// Populate the filter vectors
374: 	for (auto &aggregate : aggregates) {
375: 		auto &aggr = aggregate->Cast<BoundAggregateExpression>();
376: 		if (aggr.filter) {
377: 			auto it = filter_indexes.find(aggr.filter.get());
378: 			D_ASSERT(it != filter_indexes.end());
379: 			D_ASSERT(it->second < chunk.data.size());
380: 			aggregate_input_chunk.data[aggregate_input_idx++].Reference(chunk.data[it->second]);
381: 		}
382: 	}
383: 
384: 	aggregate_input_chunk.SetCardinality(chunk.size());
385: 	aggregate_input_chunk.Verify();
386: 
387: 	// For every grouping set there is one radix_table
388: 	for (idx_t i = 0; i < groupings.size(); i++) {
389: 		auto &grouping_global_state = global_state.grouping_states[i];
390: 		auto &grouping_local_state = local_state.grouping_states[i];
391: 		InterruptState interrupt_state;
392: 		OperatorSinkInput sink_input {*grouping_global_state.table_state, *grouping_local_state.table_state,
393: 		                              interrupt_state};
394: 
395: 		auto &grouping = groupings[i];
396: 		auto &table = grouping.table_data;
397: 		table.Sink(context, chunk, sink_input, aggregate_input_chunk, non_distinct_filter);
398: 	}
399: 
400: 	return SinkResultType::NEED_MORE_INPUT;
401: }
402: 
403: //===--------------------------------------------------------------------===//
404: // Combine
405: //===--------------------------------------------------------------------===//
406: void PhysicalHashAggregate::CombineDistinct(ExecutionContext &context, OperatorSinkCombineInput &input) const {
407: 
408: 	auto &global_sink = input.global_state.Cast<HashAggregateGlobalSinkState>();
409: 	auto &sink = input.local_state.Cast<HashAggregateLocalSinkState>();
410: 
411: 	if (!distinct_collection_info) {
412: 		return;
413: 	}
414: 	for (idx_t i = 0; i < groupings.size(); i++) {
415: 		auto &grouping_gstate = global_sink.grouping_states[i];
416: 		auto &grouping_lstate = sink.grouping_states[i];
417: 
418: 		auto &distinct_data = groupings[i].distinct_data;
419: 		auto &distinct_state = grouping_gstate.distinct_state;
420: 
421: 		const auto table_count = distinct_data->radix_tables.size();
422: 		for (idx_t table_idx = 0; table_idx < table_count; table_idx++) {
423: 			if (!distinct_data->radix_tables[table_idx]) {
424: 				continue;
425: 			}
426: 			auto &radix_table = *distinct_data->radix_tables[table_idx];
427: 			auto &radix_global_sink = *distinct_state->radix_states[table_idx];
428: 			auto &radix_local_sink = *grouping_lstate.distinct_states[table_idx];
429: 
430: 			radix_table.Combine(context, radix_global_sink, radix_local_sink);
431: 		}
432: 	}
433: }
434: 
435: SinkCombineResultType PhysicalHashAggregate::Combine(ExecutionContext &context, OperatorSinkCombineInput &input) const {
436: 	auto &gstate = input.global_state.Cast<HashAggregateGlobalSinkState>();
437: 	auto &llstate = input.local_state.Cast<HashAggregateLocalSinkState>();
438: 
439: 	OperatorSinkCombineInput combine_distinct_input {gstate, llstate, input.interrupt_state};
440: 	CombineDistinct(context, combine_distinct_input);
441: 
442: 	if (CanSkipRegularSink()) {
443: 		return SinkCombineResultType::FINISHED;
444: 	}
445: 	for (idx_t i = 0; i < groupings.size(); i++) {
446: 		auto &grouping_gstate = gstate.grouping_states[i];
447: 		auto &grouping_lstate = llstate.grouping_states[i];
448: 
449: 		auto &grouping = groupings[i];
450: 		auto &table = grouping.table_data;
451: 		table.Combine(context, *grouping_gstate.table_state, *grouping_lstate.table_state);
452: 	}
453: 
454: 	return SinkCombineResultType::FINISHED;
455: }
456: 
457: //===--------------------------------------------------------------------===//
458: // Finalize
459: //===--------------------------------------------------------------------===//
460: class HashAggregateFinalizeEvent : public BasePipelineEvent {
461: public:
462: 	//! "Regular" Finalize Event that is scheduled after combining the thread-local distinct HTs
463: 	HashAggregateFinalizeEvent(ClientContext &context, Pipeline *pipeline_p, const PhysicalHashAggregate &op_p,
464: 	                           HashAggregateGlobalSinkState &gstate_p)
465: 	    : BasePipelineEvent(*pipeline_p), context(context), op(op_p), gstate(gstate_p) {
466: 	}
467: 
468: public:
469: 	void Schedule() override;
470: 
471: private:
472: 	ClientContext &context;
473: 
474: 	const PhysicalHashAggregate &op;
475: 	HashAggregateGlobalSinkState &gstate;
476: };
477: 
478: class HashAggregateFinalizeTask : public ExecutorTask {
479: public:
480: 	HashAggregateFinalizeTask(ClientContext &context, Pipeline &pipeline, shared_ptr<Event> event_p,
481: 	                          const PhysicalHashAggregate &op, HashAggregateGlobalSinkState &state_p)
482: 	    : ExecutorTask(pipeline.executor, std::move(event_p)), context(context), pipeline(pipeline), op(op),
483: 	      gstate(state_p) {
484: 	}
485: 
486: public:
487: 	TaskExecutionResult ExecuteTask(TaskExecutionMode mode) override;
488: 
489: private:
490: 	ClientContext &context;
491: 	Pipeline &pipeline;
492: 
493: 	const PhysicalHashAggregate &op;
494: 	HashAggregateGlobalSinkState &gstate;
495: };
496: 
497: void HashAggregateFinalizeEvent::Schedule() {
498: 	vector<shared_ptr<Task>> tasks;
499: 	tasks.push_back(make_uniq<HashAggregateFinalizeTask>(context, *pipeline, shared_from_this(), op, gstate));
500: 	D_ASSERT(!tasks.empty());
501: 	SetTasks(std::move(tasks));
502: }
503: 
504: TaskExecutionResult HashAggregateFinalizeTask::ExecuteTask(TaskExecutionMode mode) {
505: 	op.FinalizeInternal(pipeline, *event, context, gstate, false);
506: 	D_ASSERT(!gstate.finished);
507: 	gstate.finished = true;
508: 	event->FinishTask();
509: 	return TaskExecutionResult::TASK_FINISHED;
510: }
511: 
512: class HashAggregateDistinctFinalizeEvent : public BasePipelineEvent {
513: public:
514: 	//! Distinct Finalize Event that is scheduled if we have distinct aggregates
515: 	HashAggregateDistinctFinalizeEvent(ClientContext &context, Pipeline &pipeline_p, const PhysicalHashAggregate &op_p,
516: 	                                   HashAggregateGlobalSinkState &gstate_p)
517: 	    : BasePipelineEvent(pipeline_p), context(context), op(op_p), gstate(gstate_p) {
518: 	}
519: 
520: public:
521: 	void Schedule() override;
522: 	void FinishEvent() override;
523: 
524: private:
525: 	idx_t CreateGlobalSources();
526: 
527: private:
528: 	ClientContext &context;
529: 
530: 	const PhysicalHashAggregate &op;
531: 	HashAggregateGlobalSinkState &gstate;
532: 
533: public:
534: 	//! The GlobalSourceStates for all the radix tables of the distinct aggregates
535: 	vector<vector<unique_ptr<GlobalSourceState>>> global_source_states;
536: };
537: 
538: class HashAggregateDistinctFinalizeTask : public ExecutorTask {
539: public:
540: 	HashAggregateDistinctFinalizeTask(Pipeline &pipeline, shared_ptr<Event> event_p, const PhysicalHashAggregate &op,
541: 	                                  HashAggregateGlobalSinkState &state_p)
542: 	    : ExecutorTask(pipeline.executor, std::move(event_p)), pipeline(pipeline), op(op), gstate(state_p) {
543: 	}
544: 
545: public:
546: 	TaskExecutionResult ExecuteTask(TaskExecutionMode mode) override;
547: 
548: private:
549: 	TaskExecutionResult AggregateDistinctGrouping(const idx_t grouping_idx);
550: 
551: private:
552: 	Pipeline &pipeline;
553: 
554: 	const PhysicalHashAggregate &op;
555: 	HashAggregateGlobalSinkState &gstate;
556: 
557: 	unique_ptr<LocalSinkState> local_sink_state;
558: 	idx_t grouping_idx = 0;
559: 	unique_ptr<LocalSourceState> radix_table_lstate;
560: 	bool blocked = false;
561: 	idx_t aggregation_idx = 0;
562: 	idx_t payload_idx = 0;
563: 	idx_t next_payload_idx = 0;
564: };
565: 
566: void HashAggregateDistinctFinalizeEvent::Schedule() {
567: 	auto n_tasks = CreateGlobalSources();
568: 	n_tasks = MinValue<idx_t>(n_tasks, NumericCast<idx_t>(TaskScheduler::GetScheduler(context).NumberOfThreads()));
569: 	vector<shared_ptr<Task>> tasks;
570: 	for (idx_t i = 0; i < n_tasks; i++) {
571: 		tasks.push_back(make_uniq<HashAggregateDistinctFinalizeTask>(*pipeline, shared_from_this(), op, gstate));
572: 	}
573: 	SetTasks(std::move(tasks));
574: }
575: 
576: idx_t HashAggregateDistinctFinalizeEvent::CreateGlobalSources() {
577: 	auto &aggregates = op.grouped_aggregate_data.aggregates;
578: 	global_source_states.reserve(op.groupings.size());
579: 
580: 	idx_t n_tasks = 0;
581: 	for (idx_t grouping_idx = 0; grouping_idx < op.groupings.size(); grouping_idx++) {
582: 		auto &grouping = op.groupings[grouping_idx];
583: 		auto &distinct_state = *gstate.grouping_states[grouping_idx].distinct_state;
584: 		auto &distinct_data = *grouping.distinct_data;
585: 
586: 		vector<unique_ptr<GlobalSourceState>> aggregate_sources;
587: 		aggregate_sources.reserve(aggregates.size());
588: 		for (idx_t agg_idx = 0; agg_idx < aggregates.size(); agg_idx++) {
589: 			auto &aggregate = aggregates[agg_idx];
590: 			auto &aggr = aggregate->Cast<BoundAggregateExpression>();
591: 
592: 			if (!aggr.IsDistinct()) {
593: 				aggregate_sources.push_back(nullptr);
594: 				continue;
595: 			}
596: 			D_ASSERT(distinct_data.info.table_map.count(agg_idx));
597: 
598: 			auto table_idx = distinct_data.info.table_map.at(agg_idx);
599: 			auto &radix_table_p = distinct_data.radix_tables[table_idx];
600: 			n_tasks += radix_table_p->MaxThreads(*distinct_state.radix_states[table_idx]);
601: 			aggregate_sources.push_back(radix_table_p->GetGlobalSourceState(context));
602: 		}
603: 		global_source_states.push_back(std::move(aggregate_sources));
604: 	}
605: 
606: 	return MaxValue<idx_t>(n_tasks, 1);
607: }
608: 
609: void HashAggregateDistinctFinalizeEvent::FinishEvent() {
610: 	// Now that everything is added to the main ht, we can actually finalize
611: 	auto new_event = make_shared_ptr<HashAggregateFinalizeEvent>(context, pipeline.get(), op, gstate);
612: 	this->InsertEvent(std::move(new_event));
613: }
614: 
615: TaskExecutionResult HashAggregateDistinctFinalizeTask::ExecuteTask(TaskExecutionMode mode) {
616: 	for (; grouping_idx < op.groupings.size(); grouping_idx++) {
617: 		auto res = AggregateDistinctGrouping(grouping_idx);
618: 		if (res == TaskExecutionResult::TASK_BLOCKED) {
619: 			return res;
620: 		}
621: 		D_ASSERT(res == TaskExecutionResult::TASK_FINISHED);
622: 		aggregation_idx = 0;
623: 		payload_idx = 0;
624: 		next_payload_idx = 0;
625: 		local_sink_state = nullptr;
626: 	}
627: 	event->FinishTask();
628: 	return TaskExecutionResult::TASK_FINISHED;
629: }
630: 
631: TaskExecutionResult HashAggregateDistinctFinalizeTask::AggregateDistinctGrouping(const idx_t grouping_idx) {
632: 	D_ASSERT(op.distinct_collection_info);
633: 	auto &info = *op.distinct_collection_info;
634: 
635: 	auto &grouping_data = op.groupings[grouping_idx];
636: 	auto &grouping_state = gstate.grouping_states[grouping_idx];
637: 	D_ASSERT(grouping_state.distinct_state);
638: 	auto &distinct_state = *grouping_state.distinct_state;
639: 	auto &distinct_data = *grouping_data.distinct_data;
640: 
641: 	auto &aggregates = info.aggregates;
642: 
643: 	// Thread-local contexts
644: 	ThreadContext thread_context(executor.context);
645: 	ExecutionContext execution_context(executor.context, thread_context, &pipeline);
646: 
647: 	// Sink state to sink into global HTs
648: 	InterruptState interrupt_state(shared_from_this());
649: 	auto &global_sink_state = *grouping_state.table_state;
650: 	if (!local_sink_state) {
651: 		local_sink_state = grouping_data.table_data.GetLocalSinkState(execution_context);
652: 	}
653: 	OperatorSinkInput sink_input {global_sink_state, *local_sink_state, interrupt_state};
654: 
655: 	// Create a chunk that mimics the 'input' chunk in Sink, for storing the group vectors
656: 	DataChunk group_chunk;
657: 	if (!op.input_group_types.empty()) {
658: 		group_chunk.Initialize(executor.context, op.input_group_types);
659: 	}
660: 
661: 	const idx_t group_by_size = op.grouped_aggregate_data.groups.size();
662: 
663: 	DataChunk aggregate_input_chunk;
664: 	if (!gstate.payload_types.empty()) {
665: 		aggregate_input_chunk.Initialize(executor.context, gstate.payload_types);
666: 	}
667: 
668: 	const auto &finalize_event = event->Cast<HashAggregateDistinctFinalizeEvent>();
669: 
670: 	auto &agg_idx = aggregation_idx;
671: 	for (; agg_idx < op.grouped_aggregate_data.aggregates.size(); agg_idx++) {
672: 		auto &aggregate = aggregates[agg_idx]->Cast<BoundAggregateExpression>();
673: 
674: 		if (!blocked) {
675: 			// Forward the payload idx
676: 			payload_idx = next_payload_idx;
677: 			next_payload_idx = payload_idx + aggregate.children.size();
678: 		}
679: 
680: 		// If aggregate is not distinct, skip it
681: 		if (!distinct_data.IsDistinct(agg_idx)) {
682: 			continue;
683: 		}
684: 
685: 		D_ASSERT(distinct_data.info.table_map.count(agg_idx));
686: 		const auto &table_idx = distinct_data.info.table_map.at(agg_idx);
687: 		auto &radix_table = distinct_data.radix_tables[table_idx];
688: 
689: 		auto &sink = *distinct_state.radix_states[table_idx];
690: 		if (!blocked) {
691: 			radix_table_lstate = radix_table->GetLocalSourceState(execution_context);
692: 		}
693: 		auto &local_source = *radix_table_lstate;
694: 		OperatorSourceInput source_input {*finalize_event.global_source_states[grouping_idx][agg_idx], local_source,
695: 		                                  interrupt_state};
696: 
697: 		// Create a duplicate of the output_chunk, because of multi-threading we cant alter the original
698: 		DataChunk output_chunk;
699: 		output_chunk.Initialize(executor.context, distinct_state.distinct_output_chunks[table_idx]->GetTypes());
700: 
701: 		// Fetch all the data from the aggregate ht, and Sink it into the main ht
702: 		while (true) {
703: 			output_chunk.Reset();
704: 			group_chunk.Reset();
705: 			aggregate_input_chunk.Reset();
706: 
707: 			auto res = radix_table->GetData(execution_context, output_chunk, sink, source_input);
708: 			if (res == SourceResultType::FINISHED) {
709: 				D_ASSERT(output_chunk.size() == 0);
710: 				break;
711: 			} else if (res == SourceResultType::BLOCKED) {
712: 				blocked = true;
713: 				return TaskExecutionResult::TASK_BLOCKED;
714: 			}
715: 
716: 			auto &grouped_aggregate_data = *distinct_data.grouped_aggregate_data[table_idx];
717: 			for (idx_t group_idx = 0; group_idx < group_by_size; group_idx++) {
718: 				auto &group = grouped_aggregate_data.groups[group_idx];
719: 				auto &bound_ref_expr = group->Cast<BoundReferenceExpression>();
720: 				group_chunk.data[bound_ref_expr.index].Reference(output_chunk.data[group_idx]);
721: 			}
722: 			group_chunk.SetCardinality(output_chunk);
723: 
724: 			for (idx_t child_idx = 0; child_idx < grouped_aggregate_data.groups.size() - group_by_size; child_idx++) {
725: 				aggregate_input_chunk.data[payload_idx + child_idx].Reference(
726: 				    output_chunk.data[group_by_size + child_idx]);
727: 			}
728: 			aggregate_input_chunk.SetCardinality(output_chunk);
729: 
730: 			// Sink it into the main ht
731: 			grouping_data.table_data.Sink(execution_context, group_chunk, sink_input, aggregate_input_chunk, {agg_idx});
732: 		}
733: 		blocked = false;
734: 	}
735: 	grouping_data.table_data.Combine(execution_context, global_sink_state, *local_sink_state);
736: 	return TaskExecutionResult::TASK_FINISHED;
737: }
738: 
739: SinkFinalizeType PhysicalHashAggregate::FinalizeDistinct(Pipeline &pipeline, Event &event, ClientContext &context,
740:                                                          GlobalSinkState &gstate_p) const {
741: 	auto &gstate = gstate_p.Cast<HashAggregateGlobalSinkState>();
742: 	D_ASSERT(distinct_collection_info);
743: 
744: 	for (idx_t i = 0; i < groupings.size(); i++) {
745: 		auto &grouping = groupings[i];
746: 		auto &distinct_data = *grouping.distinct_data;
747: 		auto &distinct_state = *gstate.grouping_states[i].distinct_state;
748: 
749: 		for (idx_t table_idx = 0; table_idx < distinct_data.radix_tables.size(); table_idx++) {
750: 			if (!distinct_data.radix_tables[table_idx]) {
751: 				continue;
752: 			}
753: 			auto &radix_table = distinct_data.radix_tables[table_idx];
754: 			auto &radix_state = *distinct_state.radix_states[table_idx];
755: 			radix_table->Finalize(context, radix_state);
756: 		}
757: 	}
758: 	auto new_event = make_shared_ptr<HashAggregateDistinctFinalizeEvent>(context, pipeline, *this, gstate);
759: 	event.InsertEvent(std::move(new_event));
760: 	return SinkFinalizeType::READY;
761: }
762: 
763: SinkFinalizeType PhysicalHashAggregate::FinalizeInternal(Pipeline &pipeline, Event &event, ClientContext &context,
764:                                                          GlobalSinkState &gstate_p, bool check_distinct) const {
765: 	auto &gstate = gstate_p.Cast<HashAggregateGlobalSinkState>();
766: 
767: 	if (check_distinct && distinct_collection_info) {
768: 		// There are distinct aggregates
769: 		// If these are partitioned those need to be combined first
770: 		// Then we Finalize again, skipping this step
771: 		return FinalizeDistinct(pipeline, event, context, gstate_p);
772: 	}
773: 
774: 	for (idx_t i = 0; i < groupings.size(); i++) {
775: 		auto &grouping = groupings[i];
776: 		auto &grouping_gstate = gstate.grouping_states[i];
777: 		grouping.table_data.Finalize(context, *grouping_gstate.table_state);
778: 	}
779: 	return SinkFinalizeType::READY;
780: }
781: 
782: SinkFinalizeType PhysicalHashAggregate::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
783:                                                  OperatorSinkFinalizeInput &input) const {
784: 	return FinalizeInternal(pipeline, event, context, input.global_state, true);
785: }
786: 
787: //===--------------------------------------------------------------------===//
788: // Source
789: //===--------------------------------------------------------------------===//
790: class HashAggregateGlobalSourceState : public GlobalSourceState {
791: public:
792: 	HashAggregateGlobalSourceState(ClientContext &context, const PhysicalHashAggregate &op) : op(op), state_index(0) {
793: 		for (auto &grouping : op.groupings) {
794: 			auto &rt = grouping.table_data;
795: 			radix_states.push_back(rt.GetGlobalSourceState(context));
796: 		}
797: 	}
798: 
799: 	const PhysicalHashAggregate &op;
800: 	atomic<idx_t> state_index;
801: 
802: 	vector<unique_ptr<GlobalSourceState>> radix_states;
803: 
804: public:
805: 	idx_t MaxThreads() override {
806: 		// If there are no tables, we only need one thread.
807: 		if (op.groupings.empty()) {
808: 			return 1;
809: 		}
810: 
811: 		auto &ht_state = op.sink_state->Cast<HashAggregateGlobalSinkState>();
812: 		idx_t threads = 0;
813: 		for (size_t sidx = 0; sidx < op.groupings.size(); ++sidx) {
814: 			auto &grouping = op.groupings[sidx];
815: 			auto &grouping_gstate = ht_state.grouping_states[sidx];
816: 			threads += grouping.table_data.MaxThreads(*grouping_gstate.table_state);
817: 		}
818: 		return MaxValue<idx_t>(1, threads);
819: 	}
820: };
821: 
822: unique_ptr<GlobalSourceState> PhysicalHashAggregate::GetGlobalSourceState(ClientContext &context) const {
823: 	return make_uniq<HashAggregateGlobalSourceState>(context, *this);
824: }
825: 
826: class HashAggregateLocalSourceState : public LocalSourceState {
827: public:
828: 	explicit HashAggregateLocalSourceState(ExecutionContext &context, const PhysicalHashAggregate &op) {
829: 		for (auto &grouping : op.groupings) {
830: 			auto &rt = grouping.table_data;
831: 			radix_states.push_back(rt.GetLocalSourceState(context));
832: 		}
833: 	}
834: 
835: 	optional_idx radix_idx;
836: 	vector<unique_ptr<LocalSourceState>> radix_states;
837: };
838: 
839: unique_ptr<LocalSourceState> PhysicalHashAggregate::GetLocalSourceState(ExecutionContext &context,
840:                                                                         GlobalSourceState &gstate) const {
841: 	return make_uniq<HashAggregateLocalSourceState>(context, *this);
842: }
843: 
844: SourceResultType PhysicalHashAggregate::GetData(ExecutionContext &context, DataChunk &chunk,
845:                                                 OperatorSourceInput &input) const {
846: 	auto &sink_gstate = sink_state->Cast<HashAggregateGlobalSinkState>();
847: 	auto &gstate = input.global_state.Cast<HashAggregateGlobalSourceState>();
848: 	auto &lstate = input.local_state.Cast<HashAggregateLocalSourceState>();
849: 	while (true) {
850: 		if (!lstate.radix_idx.IsValid()) {
851: 			lstate.radix_idx = gstate.state_index.load();
852: 		}
853: 		const auto radix_idx = lstate.radix_idx.GetIndex();
854: 		if (radix_idx >= groupings.size()) {
855: 			break;
856: 		}
857: 
858: 		auto &grouping = groupings[radix_idx];
859: 		auto &radix_table = grouping.table_data;
860: 		auto &grouping_gstate = sink_gstate.grouping_states[radix_idx];
861: 
862: 		OperatorSourceInput source_input {*gstate.radix_states[radix_idx], *lstate.radix_states[radix_idx],
863: 		                                  input.interrupt_state};
864: 		auto res = radix_table.GetData(context, chunk, *grouping_gstate.table_state, source_input);
865: 		if (res == SourceResultType::BLOCKED) {
866: 			return res;
867: 		}
868: 		if (chunk.size() != 0) {
869: 			return SourceResultType::HAVE_MORE_OUTPUT;
870: 		}
871: 
872: 		// move to the next table
873: 		auto guard = gstate.Lock();
874: 		lstate.radix_idx = lstate.radix_idx.GetIndex() + 1;
875: 		if (lstate.radix_idx.GetIndex() > gstate.state_index) {
876: 			// we have not yet worked on the table
877: 			// move the global index forwards
878: 			gstate.state_index = lstate.radix_idx.GetIndex();
879: 		}
880: 		lstate.radix_idx = gstate.state_index.load();
881: 	}
882: 
883: 	return chunk.size() == 0 ? SourceResultType::FINISHED : SourceResultType::HAVE_MORE_OUTPUT;
884: }
885: 
886: double PhysicalHashAggregate::GetProgress(ClientContext &context, GlobalSourceState &gstate_p) const {
887: 	auto &sink_gstate = sink_state->Cast<HashAggregateGlobalSinkState>();
888: 	auto &gstate = gstate_p.Cast<HashAggregateGlobalSourceState>();
889: 	double total_progress = 0;
890: 	for (idx_t radix_idx = 0; radix_idx < groupings.size(); radix_idx++) {
891: 		total_progress += groupings[radix_idx].table_data.GetProgress(
892: 		    context, *sink_gstate.grouping_states[radix_idx].table_state, *gstate.radix_states[radix_idx]);
893: 	}
894: 	return total_progress / double(groupings.size());
895: }
896: 
897: InsertionOrderPreservingMap<string> PhysicalHashAggregate::ParamsToString() const {
898: 	InsertionOrderPreservingMap<string> result;
899: 	auto &groups = grouped_aggregate_data.groups;
900: 	auto &aggregates = grouped_aggregate_data.aggregates;
901: 	string groups_info;
902: 	for (idx_t i = 0; i < groups.size(); i++) {
903: 		if (i > 0) {
904: 			groups_info += "\n";
905: 		}
906: 		groups_info += groups[i]->GetName();
907: 	}
908: 	result["Groups"] = groups_info;
909: 
910: 	string aggregate_info;
911: 	for (idx_t i = 0; i < aggregates.size(); i++) {
912: 		auto &aggregate = aggregates[i]->Cast<BoundAggregateExpression>();
913: 		if (i > 0) {
914: 			aggregate_info += "\n";
915: 		}
916: 		aggregate_info += aggregates[i]->GetName();
917: 		if (aggregate.filter) {
918: 			aggregate_info += " Filter: " + aggregate.filter->GetName();
919: 		}
920: 	}
921: 	result["Aggregates"] = aggregate_info;
922: 	SetEstimatedCardinality(result, estimated_cardinality);
923: 	return result;
924: }
925: 
926: } // namespace duckdb
[end of src/execution/operator/aggregate/physical_hash_aggregate.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: