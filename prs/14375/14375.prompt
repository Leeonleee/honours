You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Out of memory on basic hash aggregations with large values/aggregates
### What happens?

We see persistent out of memory issues with certain hash aggregations and hash joins for slightly bigger data sets, which also reproduce on surprisingly simple queries.

The problem occurs if there are many groups and the values/aggregates, rather than the keys, are of non-trivial size (strings, geometries, etc.). The spilling to disk seems no longer effective in avoiding memory_limit.

Possibly by design, but it's a very common scenario. 

### To Reproduce

Example with 100M MD5 values, distinct on, Parquet:
```sql
copy (select s a, md5(s::text) b from generate_series(1,100_000_000) as g(s)) to '/tmp/test.parquet';
set memory_limit to '4GB';
select distinct on (a) b from '/tmp/test.parquet' limit 10;
```
```consoke
Out of Memory Error: could not allocate block of size 256.0 KiB (3.7 GiB/3.7 GiB used)
```

Example with 1M long string values, group by, DuckDB table:
```sql
create table tbl as select s a, repeat('#', 10_000) b from generate_series(1,1_000_000) as g(s);
set memory_limit to '4GB';
select a, max(b) from tbl group by a limit 10;
```
```console
Out of Memory Error: could not allocate block of size 256.0 KiB (3.7 GiB/3.7 GiB used)
```

Large key small value, e.g. `select distinct on (b) a from '/tmp/test.parquet' limit 10;` works fine.

Using a persistent database with max_temp_directory_size 400GB.

Also noted in https://news.ycombinator.com/item?id=40645100

### OS:

Ubuntu 22.04 x86_64

### DuckDB Version:

1.1.1

### DuckDB Client:

CLI

### Hardware:

_No response_

### Full Name:

Marco Slot

### Affiliation:

Crunchy Data

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - I cannot share the data sets because they are confidential

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://duckdb.org/docs/installation/) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of extension/json/json_scan.cpp]
1: #include "json_scan.hpp"
2: 
3: #include "duckdb/common/enum_util.hpp"
4: #include "duckdb/common/multi_file_reader.hpp"
5: #include "duckdb/common/serializer/deserializer.hpp"
6: #include "duckdb/common/serializer/serializer.hpp"
7: #include "duckdb/main/extension_helper.hpp"
8: #include "duckdb/parallel/task_scheduler.hpp"
9: #include "duckdb/storage/buffer_manager.hpp"
10: 
11: namespace duckdb {
12: 
13: JSONScanData::JSONScanData() {
14: }
15: 
16: JSONScanData::JSONScanData(ClientContext &context, vector<string> files_p, string date_format_p,
17:                            string timestamp_format_p)
18:     : files(std::move(files_p)), date_format(std::move(date_format_p)),
19:       timestamp_format(std::move(timestamp_format_p)) {
20: 	InitializeReaders(context);
21: 	InitializeFormats();
22: }
23: 
24: void JSONScanData::Bind(ClientContext &context, TableFunctionBindInput &input) {
25: 	auto &info = input.info->Cast<JSONScanInfo>();
26: 	type = info.type;
27: 	options.format = info.format;
28: 	options.record_type = info.record_type;
29: 	auto_detect = info.auto_detect;
30: 
31: 	for (auto &kv : input.named_parameters) {
32: 		if (kv.second.IsNull()) {
33: 			throw BinderException("Cannot use NULL as function argument");
34: 		}
35: 		if (MultiFileReader().ParseOption(kv.first, kv.second, options.file_options, context)) {
36: 			continue;
37: 		}
38: 		auto loption = StringUtil::Lower(kv.first);
39: 		if (loption == "ignore_errors") {
40: 			ignore_errors = BooleanValue::Get(kv.second);
41: 		} else if (loption == "maximum_object_size") {
42: 			maximum_object_size = MaxValue<idx_t>(UIntegerValue::Get(kv.second), maximum_object_size);
43: 		} else if (loption == "format") {
44: 			auto arg = StringUtil::Lower(StringValue::Get(kv.second));
45: 			static const auto FORMAT_OPTIONS =
46: 			    case_insensitive_map_t<JSONFormat> {{"auto", JSONFormat::AUTO_DETECT},
47: 			                                        {"unstructured", JSONFormat::UNSTRUCTURED},
48: 			                                        {"newline_delimited", JSONFormat::NEWLINE_DELIMITED},
49: 			                                        {"nd", JSONFormat::NEWLINE_DELIMITED},
50: 			                                        {"array", JSONFormat::ARRAY}};
51: 			auto lookup = FORMAT_OPTIONS.find(arg);
52: 			if (lookup == FORMAT_OPTIONS.end()) {
53: 				vector<string> valid_options;
54: 				for (auto &pair : FORMAT_OPTIONS) {
55: 					valid_options.push_back(StringUtil::Format("'%s'", pair.first));
56: 				}
57: 				throw BinderException("format must be one of [%s], not '%s'", StringUtil::Join(valid_options, ", "),
58: 				                      arg);
59: 			}
60: 			options.format = lookup->second;
61: 		} else if (loption == "compression") {
62: 			SetCompression(StringUtil::Lower(StringValue::Get(kv.second)));
63: 		}
64: 	}
65: 
66: 	auto multi_file_reader = MultiFileReader::Create(input.table_function);
67: 	auto file_list = multi_file_reader->CreateFileList(context, input.inputs[0]);
68: 	options.file_options.AutoDetectHivePartitioning(*file_list, context);
69: 
70: 	// TODO: store the MultiFilelist instead
71: 	files = file_list->GetAllFiles();
72: 
73: 	InitializeReaders(context);
74: }
75: 
76: void JSONScanData::InitializeReaders(ClientContext &context) {
77: 	union_readers.resize(files.empty() ? 0 : files.size() - 1);
78: 	for (idx_t file_idx = 0; file_idx < files.size(); file_idx++) {
79: 		if (file_idx == 0) {
80: 			initial_reader = make_uniq<BufferedJSONReader>(context, options, files[0]);
81: 		} else {
82: 			union_readers[file_idx - 1] = make_uniq<BufferedJSONReader>(context, options, files[file_idx]);
83: 		}
84: 	}
85: }
86: 
87: void JSONScanData::InitializeFormats() {
88: 	InitializeFormats(auto_detect);
89: }
90: 
91: void JSONScanData::InitializeFormats(bool auto_detect_p) {
92: 	// Initialize date_format_map if anything was specified
93: 	if (!date_format.empty()) {
94: 		date_format_map.AddFormat(LogicalTypeId::DATE, date_format);
95: 	}
96: 	if (!timestamp_format.empty()) {
97: 		date_format_map.AddFormat(LogicalTypeId::TIMESTAMP, timestamp_format);
98: 	}
99: 
100: 	if (auto_detect_p) {
101: 		static const type_id_map_t<vector<const char *>> FORMAT_TEMPLATES = {
102: 		    {LogicalTypeId::DATE, {"%m-%d-%Y", "%m-%d-%y", "%d-%m-%Y", "%d-%m-%y", "%Y-%m-%d", "%y-%m-%d"}},
103: 		    {LogicalTypeId::TIMESTAMP,
104: 		     {"%Y-%m-%d %H:%M:%S.%f", "%m-%d-%Y %I:%M:%S %p", "%m-%d-%y %I:%M:%S %p", "%d-%m-%Y %H:%M:%S",
105: 		      "%d-%m-%y %H:%M:%S", "%Y-%m-%d %H:%M:%S", "%y-%m-%d %H:%M:%S", "%Y-%m-%dT%H:%M:%SZ"}},
106: 		};
107: 
108: 		// Populate possible date/timestamp formats, assume this is consistent across columns
109: 		for (auto &kv : FORMAT_TEMPLATES) {
110: 			const auto &logical_type = kv.first;
111: 			if (date_format_map.HasFormats(logical_type)) {
112: 				continue; // Already populated
113: 			}
114: 			const auto &format_strings = kv.second;
115: 			for (auto &format_string : format_strings) {
116: 				date_format_map.AddFormat(logical_type, format_string);
117: 			}
118: 		}
119: 	}
120: }
121: 
122: void JSONScanData::SetCompression(const string &compression) {
123: 	options.compression = EnumUtil::FromString<FileCompressionType>(StringUtil::Upper(compression));
124: }
125: 
126: string JSONScanData::GetDateFormat() const {
127: 	if (!date_format.empty()) {
128: 		return date_format;
129: 	} else if (date_format_map.HasFormats(LogicalTypeId::DATE)) {
130: 		return date_format_map.GetFormat(LogicalTypeId::DATE).format_specifier;
131: 	} else {
132: 		return string();
133: 	}
134: }
135: 
136: string JSONScanData::GetTimestampFormat() const {
137: 	if (!timestamp_format.empty()) {
138: 		return timestamp_format;
139: 	} else if (date_format_map.HasFormats(LogicalTypeId::TIMESTAMP)) {
140: 		return date_format_map.GetFormat(LogicalTypeId::TIMESTAMP).format_specifier;
141: 	} else {
142: 		return string();
143: 	}
144: }
145: 
146: JSONScanGlobalState::JSONScanGlobalState(ClientContext &context, const JSONScanData &bind_data_p)
147:     : bind_data(bind_data_p), transform_options(bind_data.transform_options),
148:       allocator(BufferManager::GetBufferManager(context).GetBufferAllocator()),
149:       buffer_capacity(bind_data.maximum_object_size * 2), file_index(0), batch_index(0),
150:       system_threads(TaskScheduler::GetScheduler(context).NumberOfThreads()),
151:       enable_parallel_scans(bind_data.files.size() < system_threads) {
152: }
153: 
154: JSONScanLocalState::JSONScanLocalState(ClientContext &context, JSONScanGlobalState &gstate)
155:     : scan_count(0), batch_index(DConstants::INVALID_INDEX), total_read_size(0), total_tuple_count(0),
156:       bind_data(gstate.bind_data), allocator(BufferAllocator::Get(context)), is_last(false),
157:       fs(FileSystem::GetFileSystem(context)), buffer_size(0), buffer_offset(0), prev_buffer_remainder(0) {
158: }
159: 
160: JSONGlobalTableFunctionState::JSONGlobalTableFunctionState(ClientContext &context, TableFunctionInitInput &input)
161:     : state(context, input.bind_data->Cast<JSONScanData>()) {
162: }
163: 
164: unique_ptr<GlobalTableFunctionState> JSONGlobalTableFunctionState::Init(ClientContext &context,
165:                                                                         TableFunctionInitInput &input) {
166: 	auto &bind_data = input.bind_data->Cast<JSONScanData>();
167: 	auto result = make_uniq<JSONGlobalTableFunctionState>(context, input);
168: 	auto &gstate = result->state;
169: 
170: 	// Perform projection pushdown
171: 	for (idx_t col_idx = 0; col_idx < input.column_ids.size(); col_idx++) {
172: 		const auto &col_id = input.column_ids[col_idx];
173: 
174: 		// Skip any multi-file reader / row id stuff
175: 		if (col_id == bind_data.reader_bind.filename_idx || IsRowIdColumnId(col_id)) {
176: 			continue;
177: 		}
178: 		bool skip = false;
179: 		for (const auto &hive_partitioning_index : bind_data.reader_bind.hive_partitioning_indexes) {
180: 			if (col_id == hive_partitioning_index.index) {
181: 				skip = true;
182: 				break;
183: 			}
184: 		}
185: 		if (skip) {
186: 			continue;
187: 		}
188: 
189: 		gstate.column_indices.push_back(col_idx);
190: 		gstate.names.push_back(bind_data.names[col_id]);
191: 	}
192: 
193: 	if (gstate.names.size() < bind_data.names.size() || bind_data.options.file_options.union_by_name) {
194: 		// If we are auto-detecting, but don't need all columns present in the file,
195: 		// then we don't need to throw an error if we encounter an unseen column
196: 		gstate.transform_options.error_unknown_key = false;
197: 	}
198: 
199: 	// Place readers where they belong
200: 	if (bind_data.initial_reader) {
201: 		bind_data.initial_reader->Reset();
202: 		gstate.json_readers.emplace_back(bind_data.initial_reader.get());
203: 	}
204: 	for (const auto &reader : bind_data.union_readers) {
205: 		reader->Reset();
206: 		gstate.json_readers.emplace_back(reader.get());
207: 	}
208: 
209: 	vector<LogicalType> dummy_types(input.column_ids.size(), LogicalType::ANY);
210: 	for (auto &reader : gstate.json_readers) {
211: 		MultiFileReader().FinalizeBind(reader->GetOptions().file_options, gstate.bind_data.reader_bind,
212: 		                               reader->GetFileName(), gstate.names, dummy_types, bind_data.names,
213: 		                               input.column_ids, reader->reader_data, context, nullptr);
214: 	}
215: 
216: 	return std::move(result);
217: }
218: 
219: idx_t JSONGlobalTableFunctionState::MaxThreads() const {
220: 	auto &bind_data = state.bind_data;
221: 
222: 	if (!state.json_readers.empty() && state.json_readers[0]->HasFileHandle()) {
223: 		// We opened and auto-detected a file, so we can get a better estimate
224: 		auto &reader = *state.json_readers[0];
225: 		if (bind_data.options.format == JSONFormat::NEWLINE_DELIMITED ||
226: 		    reader.GetFormat() == JSONFormat::NEWLINE_DELIMITED) {
227: 			return MaxValue<idx_t>(state.json_readers[0]->GetFileHandle().FileSize() / bind_data.maximum_object_size,
228: 			                       1);
229: 		}
230: 	}
231: 
232: 	if (bind_data.options.format == JSONFormat::NEWLINE_DELIMITED) {
233: 		// We haven't opened any files, so this is our best bet
234: 		return state.system_threads;
235: 	}
236: 
237: 	// One reader per file
238: 	return bind_data.files.size();
239: }
240: 
241: JSONLocalTableFunctionState::JSONLocalTableFunctionState(ClientContext &context, JSONScanGlobalState &gstate)
242:     : state(context, gstate) {
243: }
244: 
245: unique_ptr<LocalTableFunctionState> JSONLocalTableFunctionState::Init(ExecutionContext &context,
246:                                                                       TableFunctionInitInput &,
247:                                                                       GlobalTableFunctionState *global_state) {
248: 	auto &gstate = global_state->Cast<JSONGlobalTableFunctionState>();
249: 	auto result = make_uniq<JSONLocalTableFunctionState>(context.client, gstate.state);
250: 
251: 	// Copy the transform options / date format map because we need to do thread-local stuff
252: 	result->state.date_format_map = gstate.state.bind_data.date_format_map;
253: 	result->state.transform_options = gstate.state.transform_options;
254: 	result->state.transform_options.date_format_map = &result->state.date_format_map;
255: 
256: 	return std::move(result);
257: }
258: 
259: idx_t JSONLocalTableFunctionState::GetBatchIndex() const {
260: 	return state.batch_index;
261: }
262: 
263: static inline void SkipWhitespace(const char *buffer_ptr, idx_t &buffer_offset, const idx_t &buffer_size) {
264: 	for (; buffer_offset != buffer_size; buffer_offset++) {
265: 		if (!StringUtil::CharacterIsSpace(buffer_ptr[buffer_offset])) {
266: 			break;
267: 		}
268: 	}
269: }
270: 
271: idx_t JSONScanLocalState::ReadNext(JSONScanGlobalState &gstate) {
272: 	allocator.Reset();
273: 	scan_count = 0;
274: 
275: 	// We have to wrap this in a loop otherwise we stop scanning too early when there's an empty JSON file
276: 	while (scan_count == 0) {
277: 		if (buffer_offset == buffer_size) {
278: 			if (!ReadNextBuffer(gstate)) {
279: 				break;
280: 			}
281: 			if (current_buffer_handle->buffer_index != 0 &&
282: 			    current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {
283: 				if (ReconstructFirstObject(gstate)) {
284: 					scan_count++;
285: 				}
286: 			}
287: 		}
288: 
289: 		ParseNextChunk(gstate);
290: 	}
291: 
292: 	return scan_count;
293: }
294: 
295: static inline const char *NextNewline(const char *ptr, const idx_t size) {
296: 	return const_char_ptr_cast(memchr(ptr, '\n', size));
297: }
298: 
299: static inline const char *PreviousNewline(const char *ptr, const idx_t size) {
300: 	const auto end = ptr - size;
301: 	for (ptr--; ptr != end; ptr--) {
302: 		if (*ptr == '\n') {
303: 			break;
304: 		}
305: 	}
306: 	return ptr;
307: }
308: 
309: static inline const char *NextJSONDefault(const char *ptr, const char *const end) {
310: 	idx_t parents = 0;
311: 	while (ptr != end) {
312: 		switch (*ptr++) {
313: 		case '{':
314: 		case '[':
315: 			parents++;
316: 			continue;
317: 		case '}':
318: 		case ']':
319: 			parents--;
320: 			break;
321: 		case '"':
322: 			while (ptr != end) {
323: 				auto string_char = *ptr++;
324: 				if (string_char == '"') {
325: 					break;
326: 				} else if (string_char == '\\') {
327: 					if (ptr != end) {
328: 						ptr++; // Skip the escaped char
329: 					}
330: 				}
331: 			}
332: 			break;
333: 		default:
334: 			continue;
335: 		}
336: 
337: 		if (parents == 0) {
338: 			break;
339: 		}
340: 	}
341: 
342: 	return ptr;
343: }
344: 
345: static inline const char *NextJSON(const char *ptr, const idx_t size) {
346: 	D_ASSERT(!StringUtil::CharacterIsSpace(*ptr)); // Should be handled before
347: 
348: 	const char *const end = ptr + size;
349: 	switch (*ptr) {
350: 	case '{':
351: 	case '[':
352: 	case '"':
353: 		ptr = NextJSONDefault(ptr, end);
354: 		break;
355: 	default:
356: 		// Special case: JSON array containing JSON without clear "parents", i.e., not obj/arr/str
357: 		while (ptr != end) {
358: 			switch (*ptr++) {
359: 			case ',':
360: 			case ']':
361: 				ptr--;
362: 				break;
363: 			default:
364: 				continue;
365: 			}
366: 			break;
367: 		}
368: 	}
369: 
370: 	return ptr == end ? nullptr : ptr;
371: }
372: 
373: static inline void TrimWhitespace(JSONString &line) {
374: 	while (line.size != 0 && StringUtil::CharacterIsSpace(line[0])) {
375: 		line.pointer++;
376: 		line.size--;
377: 	}
378: 	while (line.size != 0 && StringUtil::CharacterIsSpace(line[line.size - 1])) {
379: 		line.size--;
380: 	}
381: }
382: 
383: void JSONScanLocalState::ParseJSON(char *const json_start, const idx_t json_size, const idx_t remaining) {
384: 	yyjson_doc *doc;
385: 	yyjson_read_err err;
386: 	if (bind_data.type == JSONScanType::READ_JSON_OBJECTS) { // If we return strings, we cannot parse INSITU
387: 		doc = JSONCommon::ReadDocumentUnsafe(json_start, json_size, JSONCommon::READ_STOP_FLAG, allocator.GetYYAlc(),
388: 		                                     &err);
389: 	} else {
390: 		doc = JSONCommon::ReadDocumentUnsafe(json_start, remaining, JSONCommon::READ_INSITU_FLAG, allocator.GetYYAlc(),
391: 		                                     &err);
392: 	}
393: 	if (!bind_data.ignore_errors && err.code != YYJSON_READ_SUCCESS) {
394: 		current_reader->ThrowParseError(current_buffer_handle->buffer_index, lines_or_objects_in_buffer, err);
395: 	}
396: 
397: 	// We parse with YYJSON_STOP_WHEN_DONE, so we need to check this by hand
398: 	const auto read_size = yyjson_doc_get_read_size(doc);
399: 	if (read_size > json_size) {
400: 		// Can't go past the boundary, even with ignore_errors
401: 		err.code = YYJSON_READ_ERROR_UNEXPECTED_END;
402: 		err.msg = "unexpected end of data";
403: 		err.pos = json_size;
404: 		current_reader->ThrowParseError(current_buffer_handle->buffer_index, lines_or_objects_in_buffer, err,
405: 		                                "Try auto-detecting the JSON format");
406: 	} else if (!bind_data.ignore_errors && read_size < json_size) {
407: 		idx_t off = read_size;
408: 		idx_t rem = json_size;
409: 		SkipWhitespace(json_start, off, rem);
410: 		if (off != rem) { // Between end of document and boundary should be whitespace only
411: 			err.code = YYJSON_READ_ERROR_UNEXPECTED_CONTENT;
412: 			err.msg = "unexpected content after document";
413: 			err.pos = read_size;
414: 			current_reader->ThrowParseError(current_buffer_handle->buffer_index, lines_or_objects_in_buffer, err,
415: 			                                "Try auto-detecting the JSON format");
416: 		}
417: 	}
418: 
419: 	lines_or_objects_in_buffer++;
420: 	if (!doc) {
421: 		values[scan_count] = nullptr;
422: 		return;
423: 	}
424: 
425: 	// Set the JSONLine and trim
426: 	units[scan_count] = JSONString(json_start, json_size);
427: 	TrimWhitespace(units[scan_count]);
428: 	values[scan_count] = doc->root;
429: }
430: 
431: void JSONScanLocalState::ThrowObjectSizeError(const idx_t object_size) {
432: 	throw InvalidInputException(
433: 	    "\"maximum_object_size\" of %llu bytes exceeded while reading file \"%s\" (>%llu bytes)."
434: 	    "\n Try increasing \"maximum_object_size\".",
435: 	    bind_data.maximum_object_size, current_reader->GetFileName(), object_size);
436: }
437: 
438: void JSONScanLocalState::TryIncrementFileIndex(JSONScanGlobalState &gstate) const {
439: 	if (gstate.file_index < gstate.json_readers.size() &&
440: 	    RefersToSameObject(*current_reader, *gstate.json_readers[gstate.file_index])) {
441: 		gstate.file_index++;
442: 	}
443: }
444: 
445: bool JSONScanLocalState::IsParallel(JSONScanGlobalState &gstate) const {
446: 	if (bind_data.files.size() >= gstate.system_threads) {
447: 		return false; // More files than threads, just parallelize over the files
448: 	}
449: 
450: 	// NDJSON can be read in parallel
451: 	return current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED;
452: }
453: 
454: static pair<JSONFormat, JSONRecordType> DetectFormatAndRecordType(char *const buffer_ptr, const idx_t buffer_size,
455:                                                                   yyjson_alc *alc) {
456: 	// First we do the easy check whether it's NEWLINE_DELIMITED
457: 	auto line_end = NextNewline(buffer_ptr, buffer_size);
458: 	if (line_end != nullptr) {
459: 		idx_t line_size = line_end - buffer_ptr;
460: 		SkipWhitespace(buffer_ptr, line_size, buffer_size);
461: 
462: 		yyjson_read_err error;
463: 		auto doc = JSONCommon::ReadDocumentUnsafe(buffer_ptr, line_size, JSONCommon::READ_FLAG, alc, &error);
464: 		if (error.code == YYJSON_READ_SUCCESS) { // We successfully read the line
465: 			if (yyjson_is_arr(doc->root) && line_size == buffer_size) {
466: 				// It's just one array, let's actually assume ARRAY, not NEWLINE_DELIMITED
467: 				if (yyjson_arr_size(doc->root) == 0 || yyjson_is_obj(yyjson_arr_get(doc->root, 0))) {
468: 					// Either an empty array (assume records), or an array of objects
469: 					return make_pair(JSONFormat::ARRAY, JSONRecordType::RECORDS);
470: 				} else {
471: 					return make_pair(JSONFormat::ARRAY, JSONRecordType::VALUES);
472: 				}
473: 			} else if (yyjson_is_obj(doc->root)) {
474: 				return make_pair(JSONFormat::NEWLINE_DELIMITED, JSONRecordType::RECORDS);
475: 			} else {
476: 				return make_pair(JSONFormat::NEWLINE_DELIMITED, JSONRecordType::VALUES);
477: 			}
478: 		}
479: 	}
480: 
481: 	// Skip whitespace
482: 	idx_t buffer_offset = 0;
483: 	SkipWhitespace(buffer_ptr, buffer_offset, buffer_size);
484: 	auto remaining = buffer_size - buffer_offset;
485: 
486: 	// We know it's not NEWLINE_DELIMITED at this point, if there's a '{', we know it's not ARRAY either
487: 	// Also if it's fully whitespace we just return something because we don't know
488: 	if (remaining == 0 || buffer_ptr[buffer_offset] == '{') {
489: 		return make_pair(JSONFormat::UNSTRUCTURED, JSONRecordType::RECORDS);
490: 	}
491: 
492: 	// We know it's not top-level records, if it's not '[', it's not ARRAY either
493: 	if (buffer_ptr[buffer_offset] != '[') {
494: 		return make_pair(JSONFormat::UNSTRUCTURED, JSONRecordType::VALUES);
495: 	}
496: 
497: 	// It's definitely an ARRAY, but now we have to figure out if there's more than one top-level array
498: 	yyjson_read_err error;
499: 	auto doc =
500: 	    JSONCommon::ReadDocumentUnsafe(buffer_ptr + buffer_offset, remaining, JSONCommon::READ_STOP_FLAG, alc, &error);
501: 	if (error.code == YYJSON_READ_SUCCESS) {
502: 		D_ASSERT(yyjson_is_arr(doc->root));
503: 
504: 		// We successfully read something!
505: 		buffer_offset += yyjson_doc_get_read_size(doc);
506: 		SkipWhitespace(buffer_ptr, buffer_offset, buffer_size);
507: 		remaining = buffer_size - buffer_offset;
508: 
509: 		if (remaining != 0) { // There's more
510: 			return make_pair(JSONFormat::UNSTRUCTURED, JSONRecordType::VALUES);
511: 		}
512: 
513: 		// Just one array, check what's in there
514: 		if (yyjson_arr_size(doc->root) == 0 || yyjson_is_obj(yyjson_arr_get(doc->root, 0))) {
515: 			// Either an empty array (assume records), or an array of objects
516: 			return make_pair(JSONFormat::ARRAY, JSONRecordType::RECORDS);
517: 		} else {
518: 			return make_pair(JSONFormat::ARRAY, JSONRecordType::VALUES);
519: 		}
520: 	}
521: 
522: 	// We weren't able to parse an array, could be broken or an array larger than our buffer size, let's skip over '['
523: 	SkipWhitespace(buffer_ptr, ++buffer_offset, --remaining);
524: 	remaining = buffer_size - buffer_offset;
525: 
526: 	// If it's '{' we know there's RECORDS in the ARRAY, else it's VALUES
527: 	if (remaining == 0 || buffer_ptr[buffer_offset] == '{') {
528: 		return make_pair(JSONFormat::ARRAY, JSONRecordType::RECORDS);
529: 	}
530: 
531: 	// It's not RECORDS, so it must be VALUES
532: 	return make_pair(JSONFormat::ARRAY, JSONRecordType::VALUES);
533: }
534: 
535: bool JSONScanLocalState::ReadNextBuffer(JSONScanGlobalState &gstate) {
536: 	// First we make sure we have a buffer to read into
537: 	AllocatedData buffer;
538: 
539: 	// Try to re-use a buffer that was used before
540: 	if (current_reader && current_buffer_handle) {
541: 		current_reader->SetBufferLineOrObjectCount(*current_buffer_handle, lines_or_objects_in_buffer);
542: 		if (--current_buffer_handle->readers == 0) {
543: 			buffer = current_reader->RemoveBuffer(*current_buffer_handle);
544: 		}
545: 	}
546: 
547: 	// Copy last bit of previous buffer
548: 	if (current_reader && current_reader->GetFormat() != JSONFormat::NEWLINE_DELIMITED && !is_last) {
549: 		if (!buffer.IsSet()) {
550: 			buffer = AllocateBuffer(gstate);
551: 		}
552: 		memcpy(buffer_ptr, GetReconstructBuffer(gstate), prev_buffer_remainder);
553: 	}
554: 
555: 	optional_idx buffer_index;
556: 	while (true) {
557: 		// Continue with the current reader
558: 		if (current_reader) {
559: 			// Try to read (if we were not the last read in the previous iteration)
560: 			bool file_done = false;
561: 			bool read_success = ReadNextBufferInternal(gstate, buffer, buffer_index, file_done);
562: 			if (!is_last && read_success) {
563: 				// We read something
564: 				if (buffer_index.GetIndex() == 0 && current_reader->GetFormat() == JSONFormat::ARRAY) {
565: 					SkipOverArrayStart();
566: 				}
567: 			}
568: 
569: 			if (file_done) {
570: 				lock_guard<mutex> guard(gstate.lock);
571: 				TryIncrementFileIndex(gstate);
572: 				lock_guard<mutex> reader_guard(current_reader->lock);
573: 				current_reader->GetFileHandle().Close();
574: 			}
575: 
576: 			if (read_success) {
577: 				break;
578: 			}
579: 
580: 			// We were the last reader last time, or we didn't read anything this time
581: 			current_reader = nullptr;
582: 			current_buffer_handle = nullptr;
583: 			is_last = false;
584: 		}
585: 		D_ASSERT(!current_buffer_handle);
586: 
587: 		// If we got here, we don't have a reader (anymore). Try to get one
588: 		unique_lock<mutex> guard(gstate.lock);
589: 		if (gstate.file_index == gstate.json_readers.size()) {
590: 			return false; // No more files left
591: 		}
592: 
593: 		// Assign the next reader to this thread
594: 		current_reader = gstate.json_readers[gstate.file_index].get();
595: 
596: 		batch_index = gstate.batch_index++;
597: 		if (!gstate.enable_parallel_scans) {
598: 			// Non-parallel scans, increment file index and unlock
599: 			gstate.file_index++;
600: 			guard.unlock();
601: 		}
602: 
603: 		// Open the file if it is not yet open
604: 		if (!current_reader->IsOpen()) {
605: 			current_reader->OpenJSONFile();
606: 		}
607: 
608: 		// Auto-detect if we haven't yet done this during the bind
609: 		if (gstate.bind_data.options.record_type == JSONRecordType::AUTO_DETECT ||
610: 		    current_reader->GetFormat() == JSONFormat::AUTO_DETECT) {
611: 			bool file_done = false;
612: 			ReadAndAutoDetect(gstate, buffer, buffer_index, file_done);
613: 			if (file_done) {
614: 				TryIncrementFileIndex(gstate);
615: 				lock_guard<mutex> reader_guard(current_reader->lock);
616: 				current_reader->GetFileHandle().Close();
617: 			}
618: 		}
619: 
620: 		if (gstate.enable_parallel_scans) {
621: 			if (!IsParallel(gstate)) {
622: 				// We still have the lock here if parallel scans are enabled
623: 				TryIncrementFileIndex(gstate);
624: 			}
625: 		}
626: 
627: 		if (!buffer_index.IsValid() || buffer_size == 0) {
628: 			// If we didn't get a buffer index (because not auto-detecting), or the file was empty, just re-enter loop
629: 			continue;
630: 		}
631: 
632: 		break;
633: 	}
634: 	D_ASSERT(buffer_index.IsValid());
635: 
636: 	idx_t readers = 1;
637: 	if (current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {
638: 		readers = is_last ? 1 : 2;
639: 	}
640: 
641: 	// Create an entry and insert it into the map
642: 	auto json_buffer_handle =
643: 	    make_uniq<JSONBufferHandle>(buffer_index.GetIndex(), readers, std::move(buffer), buffer_size);
644: 	current_buffer_handle = json_buffer_handle.get();
645: 	current_reader->InsertBuffer(buffer_index.GetIndex(), std::move(json_buffer_handle));
646: 
647: 	prev_buffer_remainder = 0;
648: 	lines_or_objects_in_buffer = 0;
649: 
650: 	// YYJSON needs this
651: 	memset(buffer_ptr + buffer_size, 0, YYJSON_PADDING_SIZE);
652: 
653: 	return true;
654: }
655: 
656: void JSONScanLocalState::ReadAndAutoDetect(JSONScanGlobalState &gstate, AllocatedData &buffer,
657:                                            optional_idx &buffer_index, bool &file_done) {
658: 	// We have to detect the JSON format - hold the gstate lock while we do this
659: 	if (!ReadNextBufferInternal(gstate, buffer, buffer_index, file_done)) {
660: 		return;
661: 	}
662: 	if (buffer_size == 0) {
663: 		return;
664: 	}
665: 
666: 	auto format_and_record_type = DetectFormatAndRecordType(buffer_ptr, buffer_size, allocator.GetYYAlc());
667: 	if (current_reader->GetFormat() == JSONFormat::AUTO_DETECT) {
668: 		current_reader->SetFormat(format_and_record_type.first);
669: 	}
670: 	if (current_reader->GetRecordType() == JSONRecordType::AUTO_DETECT) {
671: 		current_reader->SetRecordType(format_and_record_type.second);
672: 	}
673: 	if (current_reader->GetFormat() == JSONFormat::ARRAY) {
674: 		SkipOverArrayStart();
675: 	}
676: 
677: 	if (!bind_data.ignore_errors && bind_data.options.record_type == JSONRecordType::RECORDS &&
678: 	    current_reader->GetRecordType() != JSONRecordType::RECORDS) {
679: 		throw InvalidInputException("Expected file \"%s\" to contain records, detected non-record JSON instead.",
680: 		                            current_reader->GetFileName());
681: 	}
682: }
683: 
684: bool JSONScanLocalState::ReadNextBufferInternal(JSONScanGlobalState &gstate, AllocatedData &buffer,
685:                                                 optional_idx &buffer_index, bool &file_done) {
686: 	if (current_reader->GetFileHandle().CanSeek()) {
687: 		if (!ReadNextBufferSeek(gstate, buffer, buffer_index, file_done)) {
688: 			return false;
689: 		}
690: 	} else {
691: 		if (!ReadNextBufferNoSeek(gstate, buffer, buffer_index, file_done)) {
692: 			return false;
693: 		}
694: 	}
695: 
696: 	buffer_offset = 0;
697: 
698: 	return true;
699: }
700: 
701: bool JSONScanLocalState::ReadNextBufferSeek(JSONScanGlobalState &gstate, AllocatedData &buffer,
702:                                             optional_idx &buffer_index, bool &file_done) {
703: 	auto &file_handle = current_reader->GetFileHandle();
704: 
705: 	idx_t request_size = gstate.buffer_capacity - prev_buffer_remainder - YYJSON_PADDING_SIZE;
706: 	idx_t read_position;
707: 	idx_t read_size;
708: 
709: 	{
710: 		lock_guard<mutex> reader_guard(current_reader->lock);
711: 		if (file_handle.LastReadRequested()) {
712: 			return false;
713: 		}
714: 		if (!buffer.IsSet()) {
715: 			buffer = AllocateBuffer(gstate);
716: 		}
717: 		if (!file_handle.GetPositionAndSize(read_position, read_size, request_size)) {
718: 			return false; // We weren't able to read
719: 		}
720: 		buffer_index = current_reader->GetBufferIndex();
721: 		is_last = read_size == 0;
722: 
723: 		if (current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {
724: 			batch_index = gstate.batch_index++;
725: 		}
726: 	}
727: 	buffer_size = prev_buffer_remainder + read_size;
728: 
729: 	if (read_size != 0) {
730: 		auto &raw_handle = file_handle.GetHandle();
731: 		// For non-on-disk files, we create a handle per thread: this is faster for e.g. S3Filesystem where throttling
732: 		// per tcp connection can occur meaning that using multiple connections is faster.
733: 		if (!raw_handle.OnDiskFile() && raw_handle.CanSeek()) {
734: 			if (!thread_local_filehandle || thread_local_filehandle->GetPath() != raw_handle.GetPath()) {
735: 				thread_local_filehandle =
736: 				    fs.OpenFile(raw_handle.GetPath(), FileFlags::FILE_FLAGS_READ | FileFlags::FILE_FLAGS_DIRECT_IO);
737: 			}
738: 		} else if (thread_local_filehandle) {
739: 			thread_local_filehandle = nullptr;
740: 		}
741: 	}
742: 
743: 	// Now read the file lock-free!
744: 	file_handle.ReadAtPosition(buffer_ptr + prev_buffer_remainder, read_size, read_position, file_done,
745: 	                           gstate.bind_data.type == JSONScanType::SAMPLE, thread_local_filehandle);
746: 
747: 	return true;
748: }
749: 
750: bool JSONScanLocalState::ReadNextBufferNoSeek(JSONScanGlobalState &gstate, AllocatedData &buffer,
751:                                               optional_idx &buffer_index, bool &file_done) {
752: 	idx_t request_size = gstate.buffer_capacity - prev_buffer_remainder - YYJSON_PADDING_SIZE;
753: 	idx_t read_size;
754: 
755: 	{
756: 		lock_guard<mutex> reader_guard(current_reader->lock);
757: 		if (!current_reader->HasFileHandle() || !current_reader->IsOpen()) {
758: 			return false; // Couldn't read anything
759: 		}
760: 		auto &file_handle = current_reader->GetFileHandle();
761: 		if (file_handle.LastReadRequested()) {
762: 			return false;
763: 		}
764: 		if (!buffer.IsSet()) {
765: 			buffer = AllocateBuffer(gstate);
766: 		}
767: 		if (!file_handle.Read(buffer_ptr + prev_buffer_remainder, read_size, request_size, file_done,
768: 		                      gstate.bind_data.type == JSONScanType::SAMPLE)) {
769: 			return false; // Couldn't read anything
770: 		}
771: 		buffer_index = current_reader->GetBufferIndex();
772: 		is_last = read_size == 0;
773: 
774: 		if (current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {
775: 			batch_index = gstate.batch_index++;
776: 		}
777: 	}
778: 	buffer_size = prev_buffer_remainder + read_size;
779: 
780: 	return true;
781: }
782: 
783: AllocatedData JSONScanLocalState::AllocateBuffer(JSONScanGlobalState &gstate) {
784: 	auto buffer = gstate.allocator.Allocate(gstate.buffer_capacity);
785: 	buffer_ptr = char_ptr_cast(buffer.get());
786: 	return buffer;
787: }
788: 
789: data_ptr_t JSONScanLocalState::GetReconstructBuffer(JSONScanGlobalState &gstate) {
790: 	if (!reconstruct_buffer.IsSet()) {
791: 		reconstruct_buffer = gstate.allocator.Allocate(gstate.buffer_capacity);
792: 	}
793: 	return reconstruct_buffer.get();
794: }
795: 
796: void JSONScanLocalState::SkipOverArrayStart() {
797: 	// First read of this buffer, check if it's actually an array and skip over the bytes
798: 	SkipWhitespace(buffer_ptr, buffer_offset, buffer_size);
799: 	if (buffer_offset == buffer_size) {
800: 		return; // Empty file
801: 	}
802: 	if (buffer_ptr[buffer_offset] != '[') {
803: 		throw InvalidInputException(
804: 		    "Expected top-level JSON array with format='array', but first character is '%c' in file \"%s\"."
805: 		    "\n Try setting format='auto' or format='newline_delimited'.",
806: 		    buffer_ptr[buffer_offset], current_reader->GetFileName());
807: 	}
808: 	SkipWhitespace(buffer_ptr, ++buffer_offset, buffer_size);
809: 	if (buffer_offset >= buffer_size) {
810: 		throw InvalidInputException("Missing closing brace ']' in JSON array with format='array' in file \"%s\"",
811: 		                            current_reader->GetFileName());
812: 	}
813: 	if (buffer_ptr[buffer_offset] == ']') {
814: 		// Empty array
815: 		SkipWhitespace(buffer_ptr, ++buffer_offset, buffer_size);
816: 		if (buffer_offset != buffer_size) {
817: 			throw InvalidInputException(
818: 			    "Empty array with trailing data when parsing JSON array with format='array' in file \"%s\"",
819: 			    current_reader->GetFileName());
820: 		}
821: 		return;
822: 	}
823: }
824: 
825: bool JSONScanLocalState::ReconstructFirstObject(JSONScanGlobalState &gstate) {
826: 	D_ASSERT(current_buffer_handle->buffer_index != 0);
827: 	D_ASSERT(current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED);
828: 
829: 	// Spinlock until the previous batch index has also read its buffer
830: 	optional_ptr<JSONBufferHandle> previous_buffer_handle;
831: 	while (!previous_buffer_handle) {
832: 		previous_buffer_handle = current_reader->GetBuffer(current_buffer_handle->buffer_index - 1);
833: 	}
834: 
835: 	// First we find the newline in the previous block
836: 	auto prev_buffer_ptr = char_ptr_cast(previous_buffer_handle->buffer.get()) + previous_buffer_handle->buffer_size;
837: 	auto part1_ptr = PreviousNewline(prev_buffer_ptr, previous_buffer_handle->buffer_size);
838: 	auto part1_size = prev_buffer_ptr - part1_ptr;
839: 
840: 	// Now copy the data to our reconstruct buffer
841: 	const auto reconstruct_ptr = GetReconstructBuffer(gstate);
842: 	memcpy(reconstruct_ptr, part1_ptr, part1_size);
843: 
844: 	// We copied the object, so we are no longer reading the previous buffer
845: 	if (--previous_buffer_handle->readers == 0) {
846: 		current_reader->RemoveBuffer(*previous_buffer_handle);
847: 	}
848: 
849: 	if (part1_size == 1) {
850: 		// Just a newline
851: 		return false;
852: 	}
853: 
854: 	idx_t line_size = part1_size;
855: 	if (buffer_size != 0) {
856: 		// Now find the newline in the current block
857: 		auto line_end = NextNewline(buffer_ptr, buffer_size);
858: 		if (line_end == nullptr) {
859: 			ThrowObjectSizeError(buffer_size - buffer_offset);
860: 		} else {
861: 			line_end++;
862: 		}
863: 		idx_t part2_size = line_end - buffer_ptr;
864: 
865: 		line_size += part2_size;
866: 		if (line_size > bind_data.maximum_object_size) {
867: 			ThrowObjectSizeError(line_size);
868: 		}
869: 
870: 		// And copy the remainder of the line to the reconstruct buffer
871: 		memcpy(reconstruct_ptr + part1_size, buffer_ptr, part2_size);
872: 		memset(reconstruct_ptr + line_size, 0, YYJSON_PADDING_SIZE);
873: 		buffer_offset += part2_size;
874: 	}
875: 
876: 	ParseJSON(char_ptr_cast(reconstruct_ptr), line_size, line_size);
877: 
878: 	return true;
879: }
880: 
881: void JSONScanLocalState::ParseNextChunk(JSONScanGlobalState &gstate) {
882: 	auto buffer_offset_before = buffer_offset;
883: 
884: 	const auto format = current_reader->GetFormat();
885: 	for (; scan_count < STANDARD_VECTOR_SIZE; scan_count++) {
886: 		SkipWhitespace(buffer_ptr, buffer_offset, buffer_size);
887: 		auto json_start = buffer_ptr + buffer_offset;
888: 		idx_t remaining = buffer_size - buffer_offset;
889: 		if (remaining == 0) {
890: 			break;
891: 		}
892: 		D_ASSERT(format != JSONFormat::AUTO_DETECT);
893: 		const char *json_end = format == JSONFormat::NEWLINE_DELIMITED ? NextNewline(json_start, remaining)
894: 		                                                               : NextJSON(json_start, remaining);
895: 		if (json_end == nullptr) {
896: 			// We reached the end of the buffer
897: 			if (!is_last) {
898: 				// Last bit of data belongs to the next batch
899: 				if (format != JSONFormat::NEWLINE_DELIMITED) {
900: 					if (remaining > bind_data.maximum_object_size) {
901: 						ThrowObjectSizeError(remaining);
902: 					}
903: 					memcpy(GetReconstructBuffer(gstate), json_start, remaining);
904: 					prev_buffer_remainder = remaining;
905: 				}
906: 				buffer_offset = buffer_size;
907: 				break;
908: 			}
909: 			json_end = json_start + remaining;
910: 		}
911: 
912: 		idx_t json_size = json_end - json_start;
913: 		ParseJSON(json_start, json_size, remaining);
914: 		buffer_offset += json_size;
915: 
916: 		if (format == JSONFormat::ARRAY) {
917: 			SkipWhitespace(buffer_ptr, buffer_offset, buffer_size);
918: 			if (buffer_ptr[buffer_offset] == ',' || buffer_ptr[buffer_offset] == ']') {
919: 				buffer_offset++;
920: 			} else { // We can't ignore this error, even with 'ignore_errors'
921: 				yyjson_read_err err;
922: 				err.code = YYJSON_READ_ERROR_UNEXPECTED_CHARACTER;
923: 				err.msg = "unexpected character";
924: 				err.pos = json_size;
925: 				current_reader->ThrowParseError(current_buffer_handle->buffer_index, lines_or_objects_in_buffer, err);
926: 			}
927: 		}
928: 		SkipWhitespace(buffer_ptr, buffer_offset, buffer_size);
929: 	}
930: 
931: 	total_read_size += buffer_offset - buffer_offset_before;
932: 	total_tuple_count += scan_count;
933: }
934: 
935: yyjson_alc *JSONScanLocalState::GetAllocator() {
936: 	return allocator.GetYYAlc();
937: }
938: 
939: const MultiFileReaderData &JSONScanLocalState::GetReaderData() const {
940: 	return current_reader->reader_data;
941: }
942: 
943: void JSONScanLocalState::ThrowTransformError(idx_t object_index, const string &error_message) {
944: 	D_ASSERT(current_reader);
945: 	D_ASSERT(current_buffer_handle);
946: 	D_ASSERT(object_index != DConstants::INVALID_INDEX);
947: 	auto line_or_object_in_buffer = lines_or_objects_in_buffer - scan_count + object_index;
948: 	current_reader->ThrowTransformError(current_buffer_handle->buffer_index, line_or_object_in_buffer, error_message);
949: }
950: 
951: double JSONScan::ScanProgress(ClientContext &, const FunctionData *, const GlobalTableFunctionState *global_state) {
952: 	auto &gstate = global_state->Cast<JSONGlobalTableFunctionState>().state;
953: 	double progress = 0;
954: 	for (auto &reader : gstate.json_readers) {
955: 		progress += reader->GetProgress();
956: 	}
957: 	return progress / double(gstate.json_readers.size());
958: }
959: 
960: OperatorPartitionData JSONScan::GetPartitionData(ClientContext &, TableFunctionGetPartitionInput &input) {
961: 	if (input.partition_info.RequiresPartitionColumns()) {
962: 		throw InternalException("JSONScan::GetPartitionData: partition columns not supported");
963: 	}
964: 	auto &lstate = input.local_state->Cast<JSONLocalTableFunctionState>();
965: 	return OperatorPartitionData(lstate.GetBatchIndex());
966: }
967: 
968: unique_ptr<NodeStatistics> JSONScan::Cardinality(ClientContext &, const FunctionData *bind_data) {
969: 	auto &data = bind_data->Cast<JSONScanData>();
970: 	idx_t per_file_cardinality;
971: 	if (data.initial_reader && data.initial_reader->HasFileHandle()) {
972: 		per_file_cardinality = data.initial_reader->GetFileHandle().FileSize() / data.avg_tuple_size;
973: 	} else {
974: 		per_file_cardinality = 42; // The cardinality of an unknown JSON file is the almighty number 42
975: 	}
976: 	return make_uniq<NodeStatistics>(per_file_cardinality * data.files.size());
977: }
978: 
979: void JSONScan::ComplexFilterPushdown(ClientContext &context, LogicalGet &get, FunctionData *bind_data_p,
980:                                      vector<unique_ptr<Expression>> &filters) {
981: 	auto &data = bind_data_p->Cast<JSONScanData>();
982: 
983: 	SimpleMultiFileList file_list(std::move(data.files));
984: 
985: 	MultiFilePushdownInfo info(get);
986: 	auto filtered_list =
987: 	    MultiFileReader().ComplexFilterPushdown(context, file_list, data.options.file_options, info, filters);
988: 	if (filtered_list) {
989: 		MultiFileReader().PruneReaders(data, *filtered_list);
990: 		data.files = filtered_list->GetAllFiles();
991: 	} else {
992: 		data.files = file_list.GetAllFiles();
993: 	}
994: }
995: 
996: void JSONScan::Serialize(Serializer &serializer, const optional_ptr<FunctionData> bind_data_p, const TableFunction &) {
997: 	auto &bind_data = bind_data_p->Cast<JSONScanData>();
998: 	serializer.WriteProperty(100, "scan_data", &bind_data);
999: }
1000: 
1001: unique_ptr<FunctionData> JSONScan::Deserialize(Deserializer &deserializer, TableFunction &) {
1002: 	unique_ptr<JSONScanData> result;
1003: 	deserializer.ReadProperty(100, "scan_data", result);
1004: 	result->InitializeReaders(deserializer.Get<ClientContext &>());
1005: 	result->InitializeFormats();
1006: 	result->transform_options.date_format_map = &result->date_format_map;
1007: 	return std::move(result);
1008: }
1009: 
1010: void JSONScan::TableFunctionDefaults(TableFunction &table_function) {
1011: 	MultiFileReader().AddParameters(table_function);
1012: 
1013: 	table_function.named_parameters["maximum_object_size"] = LogicalType::UINTEGER;
1014: 	table_function.named_parameters["ignore_errors"] = LogicalType::BOOLEAN;
1015: 	table_function.named_parameters["format"] = LogicalType::VARCHAR;
1016: 	table_function.named_parameters["compression"] = LogicalType::VARCHAR;
1017: 
1018: 	table_function.table_scan_progress = ScanProgress;
1019: 	table_function.get_partition_data = GetPartitionData;
1020: 	table_function.cardinality = Cardinality;
1021: 
1022: 	table_function.serialize = Serialize;
1023: 	table_function.deserialize = Deserialize;
1024: 
1025: 	table_function.projection_pushdown = true;
1026: 	table_function.filter_pushdown = false;
1027: 	table_function.filter_prune = false;
1028: 	table_function.pushdown_complex_filter = ComplexFilterPushdown;
1029: }
1030: 
1031: } // namespace duckdb
[end of extension/json/json_scan.cpp]
[start of src/common/radix_partitioning.cpp]
1: #include "duckdb/common/radix_partitioning.hpp"
2: 
3: #include "duckdb/common/types/column/partitioned_column_data.hpp"
4: #include "duckdb/common/types/vector.hpp"
5: #include "duckdb/common/vector_operations/binary_executor.hpp"
6: #include "duckdb/common/vector_operations/unary_executor.hpp"
7: 
8: namespace duckdb {
9: 
10: //! Templated radix partitioning constants, can be templated to the number of radix bits
11: template <idx_t radix_bits>
12: struct RadixPartitioningConstants {
13: public:
14: 	//! Bitmask of the upper bits starting at the 5th byte
15: 	static constexpr idx_t NUM_PARTITIONS = RadixPartitioning::NumberOfPartitions(radix_bits);
16: 	static constexpr idx_t SHIFT = RadixPartitioning::Shift(radix_bits);
17: 	static constexpr hash_t MASK = RadixPartitioning::Mask(radix_bits);
18: 
19: public:
20: 	//! Apply bitmask and right shift to get a number between 0 and NUM_PARTITIONS
21: 	static hash_t ApplyMask(const hash_t hash) {
22: 		D_ASSERT((hash & MASK) >> SHIFT < NUM_PARTITIONS);
23: 		return (hash & MASK) >> SHIFT;
24: 	}
25: };
26: 
27: template <class OP, class RETURN_TYPE, typename... ARGS>
28: RETURN_TYPE RadixBitsSwitch(const idx_t radix_bits, ARGS &&... args) {
29: 	D_ASSERT(radix_bits <= RadixPartitioning::MAX_RADIX_BITS);
30: 	switch (radix_bits) {
31: 	case 0:
32: 		return OP::template Operation<0>(std::forward<ARGS>(args)...);
33: 	case 1:
34: 		return OP::template Operation<1>(std::forward<ARGS>(args)...);
35: 	case 2:
36: 		return OP::template Operation<2>(std::forward<ARGS>(args)...);
37: 	case 3:
38: 		return OP::template Operation<3>(std::forward<ARGS>(args)...);
39: 	case 4:
40: 		return OP::template Operation<4>(std::forward<ARGS>(args)...);
41: 	case 5: // LCOV_EXCL_START
42: 		return OP::template Operation<5>(std::forward<ARGS>(args)...);
43: 	case 6:
44: 		return OP::template Operation<6>(std::forward<ARGS>(args)...);
45: 	case 7:
46: 		return OP::template Operation<7>(std::forward<ARGS>(args)...);
47: 	case 8:
48: 		return OP::template Operation<8>(std::forward<ARGS>(args)...);
49: 	case 9:
50: 		return OP::template Operation<9>(std::forward<ARGS>(args)...);
51: 	case 10:
52: 		return OP::template Operation<10>(std::forward<ARGS>(args)...);
53: 	case 11:
54: 		return OP::template Operation<10>(std::forward<ARGS>(args)...);
55: 	case 12:
56: 		return OP::template Operation<10>(std::forward<ARGS>(args)...);
57: 	default:
58: 		throw InternalException(
59: 		    "radix_bits higher than RadixPartitioning::MAX_RADIX_BITS encountered in RadixBitsSwitch");
60: 	} // LCOV_EXCL_STOP
61: }
62: 
63: template <idx_t radix_bits>
64: struct RadixLessThan {
65: 	static inline bool Operation(hash_t hash, hash_t cutoff) {
66: 		using CONSTANTS = RadixPartitioningConstants<radix_bits>;
67: 		return CONSTANTS::ApplyMask(hash) < cutoff;
68: 	}
69: };
70: 
71: struct SelectFunctor {
72: 	template <idx_t radix_bits>
73: 	static idx_t Operation(Vector &hashes, const SelectionVector *sel, const idx_t count, const idx_t cutoff,
74: 	                       SelectionVector *true_sel, SelectionVector *false_sel) {
75: 		Vector cutoff_vector(Value::HASH(cutoff));
76: 		return BinaryExecutor::Select<hash_t, hash_t, RadixLessThan<radix_bits>>(hashes, cutoff_vector, sel, count,
77: 		                                                                         true_sel, false_sel);
78: 	}
79: };
80: 
81: idx_t RadixPartitioning::Select(Vector &hashes, const SelectionVector *sel, const idx_t count, const idx_t radix_bits,
82:                                 const idx_t cutoff, SelectionVector *true_sel, SelectionVector *false_sel) {
83: 	return RadixBitsSwitch<SelectFunctor, idx_t>(radix_bits, hashes, sel, count, cutoff, true_sel, false_sel);
84: }
85: 
86: struct ComputePartitionIndicesFunctor {
87: 	template <idx_t radix_bits>
88: 	static void Operation(Vector &hashes, Vector &partition_indices, const SelectionVector &append_sel,
89: 	                      const idx_t append_count) {
90: 		using CONSTANTS = RadixPartitioningConstants<radix_bits>;
91: 		if (append_sel.IsSet()) {
92: 			auto hashes_sliced = Vector(hashes, append_sel, append_count);
93: 			UnaryExecutor::Execute<hash_t, hash_t>(hashes_sliced, partition_indices, append_count,
94: 			                                       [&](hash_t hash) { return CONSTANTS::ApplyMask(hash); });
95: 		} else {
96: 			UnaryExecutor::Execute<hash_t, hash_t>(hashes, partition_indices, append_count,
97: 			                                       [&](hash_t hash) { return CONSTANTS::ApplyMask(hash); });
98: 		}
99: 	}
100: };
101: 
102: //===--------------------------------------------------------------------===//
103: // Column Data Partitioning
104: //===--------------------------------------------------------------------===//
105: RadixPartitionedColumnData::RadixPartitionedColumnData(ClientContext &context_p, vector<LogicalType> types_p,
106:                                                        idx_t radix_bits_p, idx_t hash_col_idx_p)
107:     : PartitionedColumnData(PartitionedColumnDataType::RADIX, context_p, std::move(types_p)), radix_bits(radix_bits_p),
108:       hash_col_idx(hash_col_idx_p) {
109: 	D_ASSERT(radix_bits <= RadixPartitioning::MAX_RADIX_BITS);
110: 	D_ASSERT(hash_col_idx < types.size());
111: 	const auto num_partitions = RadixPartitioning::NumberOfPartitions(radix_bits);
112: 	allocators->allocators.reserve(num_partitions);
113: 	for (idx_t i = 0; i < num_partitions; i++) {
114: 		CreateAllocator();
115: 	}
116: 	D_ASSERT(allocators->allocators.size() == num_partitions);
117: }
118: 
119: RadixPartitionedColumnData::RadixPartitionedColumnData(const RadixPartitionedColumnData &other)
120:     : PartitionedColumnData(other), radix_bits(other.radix_bits), hash_col_idx(other.hash_col_idx) {
121: 	for (idx_t i = 0; i < RadixPartitioning::NumberOfPartitions(radix_bits); i++) {
122: 		partitions.emplace_back(CreatePartitionCollection(i));
123: 	}
124: }
125: 
126: RadixPartitionedColumnData::~RadixPartitionedColumnData() {
127: }
128: 
129: void RadixPartitionedColumnData::InitializeAppendStateInternal(PartitionedColumnDataAppendState &state) const {
130: 	const auto num_partitions = RadixPartitioning::NumberOfPartitions(radix_bits);
131: 	state.partition_append_states.reserve(num_partitions);
132: 	state.partition_buffers.reserve(num_partitions);
133: 	for (idx_t i = 0; i < num_partitions; i++) {
134: 		state.partition_append_states.emplace_back(make_uniq<ColumnDataAppendState>());
135: 		partitions[i]->InitializeAppend(*state.partition_append_states[i]);
136: 		state.partition_buffers.emplace_back(CreatePartitionBuffer());
137: 	}
138: 
139: 	// Initialize fixed-size map
140: 	state.fixed_partition_entries.resize(RadixPartitioning::NumberOfPartitions(radix_bits));
141: }
142: 
143: void RadixPartitionedColumnData::ComputePartitionIndices(PartitionedColumnDataAppendState &state, DataChunk &input) {
144: 	D_ASSERT(partitions.size() == RadixPartitioning::NumberOfPartitions(radix_bits));
145: 	D_ASSERT(state.partition_buffers.size() == RadixPartitioning::NumberOfPartitions(radix_bits));
146: 	RadixBitsSwitch<ComputePartitionIndicesFunctor, void>(radix_bits, input.data[hash_col_idx], state.partition_indices,
147: 	                                                      *FlatVector::IncrementalSelectionVector(), input.size());
148: }
149: 
150: //===--------------------------------------------------------------------===//
151: // Tuple Data Partitioning
152: //===--------------------------------------------------------------------===//
153: RadixPartitionedTupleData::RadixPartitionedTupleData(BufferManager &buffer_manager, const TupleDataLayout &layout_p,
154:                                                      const idx_t radix_bits_p, const idx_t hash_col_idx_p)
155:     : PartitionedTupleData(PartitionedTupleDataType::RADIX, buffer_manager, layout_p.Copy()), radix_bits(radix_bits_p),
156:       hash_col_idx(hash_col_idx_p) {
157: 	D_ASSERT(radix_bits <= RadixPartitioning::MAX_RADIX_BITS);
158: 	D_ASSERT(hash_col_idx < layout.GetTypes().size());
159: 	const auto num_partitions = RadixPartitioning::NumberOfPartitions(radix_bits);
160: 	allocators->allocators.reserve(num_partitions);
161: 	for (idx_t i = 0; i < num_partitions; i++) {
162: 		CreateAllocator();
163: 	}
164: 	D_ASSERT(allocators->allocators.size() == num_partitions);
165: 	Initialize();
166: }
167: 
168: RadixPartitionedTupleData::RadixPartitionedTupleData(const RadixPartitionedTupleData &other)
169:     : PartitionedTupleData(other), radix_bits(other.radix_bits), hash_col_idx(other.hash_col_idx) {
170: 	Initialize();
171: }
172: 
173: RadixPartitionedTupleData::~RadixPartitionedTupleData() {
174: }
175: 
176: void RadixPartitionedTupleData::Initialize() {
177: 	for (idx_t i = 0; i < RadixPartitioning::NumberOfPartitions(radix_bits); i++) {
178: 		partitions.emplace_back(CreatePartitionCollection(i));
179: 	}
180: }
181: 
182: void RadixPartitionedTupleData::InitializeAppendStateInternal(PartitionedTupleDataAppendState &state,
183:                                                               const TupleDataPinProperties properties) const {
184: 	// Init pin state per partition
185: 	const auto num_partitions = RadixPartitioning::NumberOfPartitions(radix_bits);
186: 	state.partition_pin_states.reserve(num_partitions);
187: 	for (idx_t i = 0; i < num_partitions; i++) {
188: 		state.partition_pin_states.emplace_back(make_unsafe_uniq<TupleDataPinState>());
189: 		partitions[i]->InitializeAppend(*state.partition_pin_states[i], properties);
190: 	}
191: 
192: 	// Init single chunk state
193: 	auto column_count = layout.ColumnCount();
194: 	vector<column_t> column_ids;
195: 	column_ids.reserve(column_count);
196: 	for (idx_t col_idx = 0; col_idx < column_count; col_idx++) {
197: 		column_ids.emplace_back(col_idx);
198: 	}
199: 	partitions[0]->InitializeChunkState(state.chunk_state, std::move(column_ids));
200: 
201: 	// Initialize fixed-size map
202: 	state.fixed_partition_entries.resize(RadixPartitioning::NumberOfPartitions(radix_bits));
203: }
204: 
205: void RadixPartitionedTupleData::ComputePartitionIndices(PartitionedTupleDataAppendState &state, DataChunk &input,
206:                                                         const SelectionVector &append_sel, const idx_t append_count) {
207: 	D_ASSERT(partitions.size() == RadixPartitioning::NumberOfPartitions(radix_bits));
208: 	RadixBitsSwitch<ComputePartitionIndicesFunctor, void>(radix_bits, input.data[hash_col_idx], state.partition_indices,
209: 	                                                      append_sel, append_count);
210: }
211: 
212: void RadixPartitionedTupleData::ComputePartitionIndices(Vector &row_locations, idx_t count,
213:                                                         Vector &partition_indices) const {
214: 	Vector intermediate(LogicalType::HASH);
215: 	partitions[0]->Gather(row_locations, *FlatVector::IncrementalSelectionVector(), count, hash_col_idx, intermediate,
216: 	                      *FlatVector::IncrementalSelectionVector(), nullptr);
217: 	RadixBitsSwitch<ComputePartitionIndicesFunctor, void>(radix_bits, intermediate, partition_indices,
218: 	                                                      *FlatVector::IncrementalSelectionVector(), count);
219: }
220: 
221: void RadixPartitionedTupleData::RepartitionFinalizeStates(PartitionedTupleData &old_partitioned_data,
222:                                                           PartitionedTupleData &new_partitioned_data,
223:                                                           PartitionedTupleDataAppendState &state,
224:                                                           idx_t finished_partition_idx) const {
225: 	D_ASSERT(old_partitioned_data.GetType() == PartitionedTupleDataType::RADIX &&
226: 	         new_partitioned_data.GetType() == PartitionedTupleDataType::RADIX);
227: 	const auto &old_radix_partitions = old_partitioned_data.Cast<RadixPartitionedTupleData>();
228: 	const auto &new_radix_partitions = new_partitioned_data.Cast<RadixPartitionedTupleData>();
229: 	const auto old_radix_bits = old_radix_partitions.GetRadixBits();
230: 	const auto new_radix_bits = new_radix_partitions.GetRadixBits();
231: 	D_ASSERT(new_radix_bits > old_radix_bits);
232: 
233: 	// We take the most significant digits as the partition index
234: 	// When repartitioning, e.g., partition 0 from "old" goes into the first N partitions in "new"
235: 	// When partition 0 is done, we can already finalize the append states, unpinning blocks
236: 	const auto multiplier = RadixPartitioning::NumberOfPartitions(new_radix_bits - old_radix_bits);
237: 	const auto from_idx = finished_partition_idx * multiplier;
238: 	const auto to_idx = from_idx + multiplier;
239: 	auto &partitions = new_partitioned_data.GetPartitions();
240: 	for (idx_t partition_index = from_idx; partition_index < to_idx; partition_index++) {
241: 		auto &partition = *partitions[partition_index];
242: 		auto &partition_pin_state = *state.partition_pin_states[partition_index];
243: 		partition.FinalizePinState(partition_pin_state);
244: 	}
245: }
246: 
247: } // namespace duckdb
[end of src/common/radix_partitioning.cpp]
[start of src/common/types/column/column_data_allocator.cpp]
1: #include "duckdb/common/types/column/column_data_allocator.hpp"
2: 
3: #include "duckdb/common/types/column/column_data_collection_segment.hpp"
4: #include "duckdb/storage/buffer/block_handle.hpp"
5: #include "duckdb/storage/buffer/buffer_pool.hpp"
6: #include "duckdb/storage/buffer_manager.hpp"
7: 
8: namespace duckdb {
9: 
10: ColumnDataAllocator::ColumnDataAllocator(Allocator &allocator) : type(ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR) {
11: 	alloc.allocator = &allocator;
12: }
13: 
14: ColumnDataAllocator::ColumnDataAllocator(BufferManager &buffer_manager)
15:     : type(ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR) {
16: 	alloc.buffer_manager = &buffer_manager;
17: }
18: 
19: ColumnDataAllocator::ColumnDataAllocator(ClientContext &context, ColumnDataAllocatorType allocator_type)
20:     : type(allocator_type) {
21: 	switch (type) {
22: 	case ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR:
23: 	case ColumnDataAllocatorType::HYBRID:
24: 		alloc.buffer_manager = &BufferManager::GetBufferManager(context);
25: 		break;
26: 	case ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR:
27: 		alloc.allocator = &Allocator::Get(context);
28: 		break;
29: 	default:
30: 		throw InternalException("Unrecognized column data allocator type");
31: 	}
32: }
33: 
34: ColumnDataAllocator::ColumnDataAllocator(ColumnDataAllocator &other) {
35: 	type = other.GetType();
36: 	switch (type) {
37: 	case ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR:
38: 	case ColumnDataAllocatorType::HYBRID:
39: 		alloc.allocator = other.alloc.allocator;
40: 		break;
41: 	case ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR:
42: 		alloc.buffer_manager = other.alloc.buffer_manager;
43: 		break;
44: 	default:
45: 		throw InternalException("Unrecognized column data allocator type");
46: 	}
47: }
48: 
49: ColumnDataAllocator::~ColumnDataAllocator() {
50: 	if (type == ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR) {
51: 		return;
52: 	}
53: 	for (auto &block : blocks) {
54: 		block.handle->SetDestroyBufferUpon(DestroyBufferUpon::UNPIN);
55: 	}
56: 	const auto data_size = SizeInBytes();
57: 	blocks.clear();
58: 	if (Allocator::SupportsFlush() &&
59: 	    data_size > alloc.buffer_manager->GetBufferPool().GetAllocatorBulkDeallocationFlushThreshold()) {
60: 		Allocator::FlushAll();
61: 	}
62: }
63: 
64: BufferHandle ColumnDataAllocator::Pin(uint32_t block_id) {
65: 	D_ASSERT(type == ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR || type == ColumnDataAllocatorType::HYBRID);
66: 	shared_ptr<BlockHandle> handle;
67: 	if (shared) {
68: 		// we only need to grab the lock when accessing the vector, because vector access is not thread-safe:
69: 		// the vector can be resized by another thread while we try to access it
70: 		lock_guard<mutex> guard(lock);
71: 		handle = blocks[block_id].handle;
72: 	} else {
73: 		handle = blocks[block_id].handle;
74: 	}
75: 	return alloc.buffer_manager->Pin(handle);
76: }
77: 
78: BufferHandle ColumnDataAllocator::AllocateBlock(idx_t size) {
79: 	D_ASSERT(type == ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR || type == ColumnDataAllocatorType::HYBRID);
80: 	auto max_size = MaxValue<idx_t>(size, GetBufferManager().GetBlockSize());
81: 	BlockMetaData data;
82: 	data.size = 0;
83: 	data.capacity = NumericCast<uint32_t>(max_size);
84: 	auto pin = alloc.buffer_manager->Allocate(MemoryTag::COLUMN_DATA, max_size, false);
85: 	data.handle = pin.GetBlockHandle();
86: 	blocks.push_back(std::move(data));
87: 	allocated_size += max_size;
88: 	return pin;
89: }
90: 
91: void ColumnDataAllocator::AllocateEmptyBlock(idx_t size) {
92: 	auto allocation_amount = MaxValue<idx_t>(NextPowerOfTwo(size), 4096);
93: 	if (!blocks.empty()) {
94: 		idx_t last_capacity = blocks.back().capacity;
95: 		auto next_capacity = MinValue<idx_t>(last_capacity * 2, last_capacity + Storage::DEFAULT_BLOCK_SIZE);
96: 		allocation_amount = MaxValue<idx_t>(next_capacity, allocation_amount);
97: 	}
98: 	D_ASSERT(type == ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR);
99: 	BlockMetaData data;
100: 	data.size = 0;
101: 	data.capacity = NumericCast<uint32_t>(allocation_amount);
102: 	data.handle = nullptr;
103: 	blocks.push_back(std::move(data));
104: 	allocated_size += allocation_amount;
105: }
106: 
107: void ColumnDataAllocator::AssignPointer(uint32_t &block_id, uint32_t &offset, data_ptr_t pointer) {
108: 	auto pointer_value = uintptr_t(pointer);
109: 	if (sizeof(uintptr_t) == sizeof(uint32_t)) {
110: 		block_id = uint32_t(pointer_value);
111: 	} else if (sizeof(uintptr_t) == sizeof(uint64_t)) {
112: 		block_id = uint32_t(pointer_value & 0xFFFFFFFF);
113: 		offset = uint32_t(pointer_value >> 32);
114: 	} else {
115: 		throw InternalException("ColumnDataCollection: Architecture not supported!?");
116: 	}
117: }
118: 
119: void ColumnDataAllocator::AllocateBuffer(idx_t size, uint32_t &block_id, uint32_t &offset,
120:                                          ChunkManagementState *chunk_state) {
121: 	D_ASSERT(allocated_data.empty());
122: 	if (blocks.empty() || blocks.back().Capacity() < size) {
123: 		auto pinned_block = AllocateBlock(size);
124: 		if (chunk_state) {
125: 			D_ASSERT(!blocks.empty());
126: 			auto new_block_id = blocks.size() - 1;
127: 			chunk_state->handles[new_block_id] = std::move(pinned_block);
128: 		}
129: 	}
130: 	auto &block = blocks.back();
131: 	D_ASSERT(size <= block.capacity - block.size);
132: 	block_id = NumericCast<uint32_t>(blocks.size() - 1);
133: 	if (chunk_state && chunk_state->handles.find(block_id) == chunk_state->handles.end()) {
134: 		// not guaranteed to be pinned already by this thread (if shared allocator)
135: 		chunk_state->handles[block_id] = alloc.buffer_manager->Pin(blocks[block_id].handle);
136: 	}
137: 	offset = block.size;
138: 	block.size += size;
139: }
140: 
141: void ColumnDataAllocator::AllocateMemory(idx_t size, uint32_t &block_id, uint32_t &offset,
142:                                          ChunkManagementState *chunk_state) {
143: 	D_ASSERT(blocks.size() == allocated_data.size());
144: 	if (blocks.empty() || blocks.back().Capacity() < size) {
145: 		AllocateEmptyBlock(size);
146: 		auto &last_block = blocks.back();
147: 		auto allocated = alloc.allocator->Allocate(last_block.capacity);
148: 		allocated_data.push_back(std::move(allocated));
149: 	}
150: 	auto &block = blocks.back();
151: 	D_ASSERT(size <= block.capacity - block.size);
152: 	AssignPointer(block_id, offset, allocated_data.back().get() + block.size);
153: 	block.size += size;
154: }
155: 
156: void ColumnDataAllocator::AllocateData(idx_t size, uint32_t &block_id, uint32_t &offset,
157:                                        ChunkManagementState *chunk_state) {
158: 	switch (type) {
159: 	case ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR:
160: 	case ColumnDataAllocatorType::HYBRID:
161: 		if (shared) {
162: 			lock_guard<mutex> guard(lock);
163: 			AllocateBuffer(size, block_id, offset, chunk_state);
164: 		} else {
165: 			AllocateBuffer(size, block_id, offset, chunk_state);
166: 		}
167: 		break;
168: 	case ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR:
169: 		D_ASSERT(!shared);
170: 		AllocateMemory(size, block_id, offset, chunk_state);
171: 		break;
172: 	default:
173: 		throw InternalException("Unrecognized allocator type");
174: 	}
175: }
176: 
177: void ColumnDataAllocator::Initialize(ColumnDataAllocator &other) {
178: 	D_ASSERT(other.HasBlocks());
179: 	blocks.push_back(other.blocks.back());
180: }
181: 
182: data_ptr_t ColumnDataAllocator::GetDataPointer(ChunkManagementState &state, uint32_t block_id, uint32_t offset) {
183: 	if (type == ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR) {
184: 		// in-memory allocator: construct pointer from block_id and offset
185: 		if (sizeof(uintptr_t) == sizeof(uint32_t)) {
186: 			uintptr_t pointer_value = uintptr_t(block_id);
187: 			return (data_ptr_t)pointer_value; // NOLINT - convert from pointer value back to pointer
188: 		} else if (sizeof(uintptr_t) == sizeof(uint64_t)) {
189: 			uintptr_t pointer_value = (uintptr_t(offset) << 32) | uintptr_t(block_id);
190: 			return (data_ptr_t)pointer_value; // NOLINT - convert from pointer value back to pointer
191: 		} else {
192: 			throw InternalException("ColumnDataCollection: Architecture not supported!?");
193: 		}
194: 	}
195: 	D_ASSERT(state.handles.find(block_id) != state.handles.end());
196: 	return state.handles[block_id].Ptr() + offset;
197: }
198: 
199: void ColumnDataAllocator::UnswizzlePointers(ChunkManagementState &state, Vector &result, idx_t v_offset, uint16_t count,
200:                                             uint32_t block_id, uint32_t offset) {
201: 	D_ASSERT(result.GetType().InternalType() == PhysicalType::VARCHAR);
202: 	lock_guard<mutex> guard(lock);
203: 
204: 	auto &validity = FlatVector::Validity(result);
205: 	auto strings = FlatVector::GetData<string_t>(result);
206: 
207: 	// find first non-inlined string
208: 	auto i = NumericCast<uint32_t>(v_offset);
209: 	const uint32_t end = NumericCast<uint32_t>(v_offset + count);
210: 	for (; i < end; i++) {
211: 		if (!validity.RowIsValid(i)) {
212: 			continue;
213: 		}
214: 		if (!strings[i].IsInlined()) {
215: 			break;
216: 		}
217: 	}
218: 	// at least one string must be non-inlined, otherwise this function should not be called
219: 	D_ASSERT(i < end);
220: 
221: 	auto base_ptr = char_ptr_cast(GetDataPointer(state, block_id, offset));
222: 	if (strings[i].GetData() == base_ptr) {
223: 		// pointers are still valid
224: 		return;
225: 	}
226: 
227: 	// pointer mismatch! pointers are invalid, set them correctly
228: 	for (; i < end; i++) {
229: 		if (!validity.RowIsValid(i)) {
230: 			continue;
231: 		}
232: 		if (strings[i].IsInlined()) {
233: 			continue;
234: 		}
235: 		strings[i].SetPointer(base_ptr);
236: 		base_ptr += strings[i].GetSize();
237: 	}
238: }
239: 
240: void ColumnDataAllocator::SetDestroyBufferUponUnpin(uint32_t block_id) {
241: 	blocks[block_id].handle->SetDestroyBufferUpon(DestroyBufferUpon::UNPIN);
242: }
243: 
244: Allocator &ColumnDataAllocator::GetAllocator() {
245: 	if (type == ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR) {
246: 		return *alloc.allocator;
247: 	}
248: 	return alloc.buffer_manager->GetBufferAllocator();
249: }
250: 
251: BufferManager &ColumnDataAllocator::GetBufferManager() {
252: 	if (type == ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR) {
253: 		throw InternalException("cannot obtain the buffer manager for in memory allocations");
254: 	}
255: 	return *alloc.buffer_manager;
256: }
257: 
258: void ColumnDataAllocator::InitializeChunkState(ChunkManagementState &state, ChunkMetaData &chunk) {
259: 	if (type != ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR && type != ColumnDataAllocatorType::HYBRID) {
260: 		// nothing to pin
261: 		return;
262: 	}
263: 	// release any handles that are no longer required
264: 	bool found_handle;
265: 	do {
266: 		found_handle = false;
267: 		for (auto it = state.handles.begin(); it != state.handles.end(); it++) {
268: 			if (chunk.block_ids.find(NumericCast<uint32_t>(it->first)) != chunk.block_ids.end()) {
269: 				// still required: do not release
270: 				continue;
271: 			}
272: 			state.handles.erase(it);
273: 			found_handle = true;
274: 			break;
275: 		}
276: 	} while (found_handle);
277: 
278: 	// grab any handles that are now required
279: 	for (auto &block_id : chunk.block_ids) {
280: 		if (state.handles.find(block_id) != state.handles.end()) {
281: 			// already pinned: don't need to do anything
282: 			continue;
283: 		}
284: 		state.handles[block_id] = Pin(block_id);
285: 	}
286: }
287: 
288: uint32_t BlockMetaData::Capacity() {
289: 	D_ASSERT(size <= capacity);
290: 	return capacity - size;
291: }
292: 
293: } // namespace duckdb
[end of src/common/types/column/column_data_allocator.cpp]
[start of src/common/types/column/column_data_collection.cpp]
1: #include "duckdb/common/types/column/column_data_collection.hpp"
2: 
3: #include "duckdb/common/printer.hpp"
4: #include "duckdb/common/string_util.hpp"
5: #include "duckdb/common/types/column/column_data_collection_segment.hpp"
6: #include "duckdb/common/types/value_map.hpp"
7: #include "duckdb/common/uhugeint.hpp"
8: #include "duckdb/common/vector_operations/vector_operations.hpp"
9: #include "duckdb/storage/buffer_manager.hpp"
10: #include "duckdb/common/serializer/serializer.hpp"
11: #include "duckdb/common/serializer/deserializer.hpp"
12: 
13: namespace duckdb {
14: 
15: struct ColumnDataMetaData;
16: 
17: typedef void (*column_data_copy_function_t)(ColumnDataMetaData &meta_data, const UnifiedVectorFormat &source_data,
18:                                             Vector &source, idx_t offset, idx_t copy_count);
19: 
20: struct ColumnDataCopyFunction {
21: 	column_data_copy_function_t function;
22: 	vector<ColumnDataCopyFunction> child_functions;
23: };
24: 
25: struct ColumnDataMetaData {
26: 	ColumnDataMetaData(ColumnDataCopyFunction &copy_function, ColumnDataCollectionSegment &segment,
27: 	                   ColumnDataAppendState &state, ChunkMetaData &chunk_data, VectorDataIndex vector_data_index)
28: 	    : copy_function(copy_function), segment(segment), state(state), chunk_data(chunk_data),
29: 	      vector_data_index(vector_data_index) {
30: 	}
31: 	ColumnDataMetaData(ColumnDataCopyFunction &copy_function, ColumnDataMetaData &parent,
32: 	                   VectorDataIndex vector_data_index)
33: 	    : copy_function(copy_function), segment(parent.segment), state(parent.state), chunk_data(parent.chunk_data),
34: 	      vector_data_index(vector_data_index) {
35: 	}
36: 
37: 	ColumnDataCopyFunction &copy_function;
38: 	ColumnDataCollectionSegment &segment;
39: 	ColumnDataAppendState &state;
40: 	ChunkMetaData &chunk_data;
41: 	VectorDataIndex vector_data_index;
42: 	idx_t child_list_size = DConstants::INVALID_INDEX;
43: 
44: 	VectorMetaData &GetVectorMetaData() {
45: 		return segment.GetVectorData(vector_data_index);
46: 	}
47: };
48: 
49: //! Explicitly initialized without types
50: ColumnDataCollection::ColumnDataCollection(Allocator &allocator_p) {
51: 	types.clear();
52: 	count = 0;
53: 	this->finished_append = false;
54: 	allocator = make_shared_ptr<ColumnDataAllocator>(allocator_p);
55: }
56: 
57: ColumnDataCollection::ColumnDataCollection(Allocator &allocator_p, vector<LogicalType> types_p) {
58: 	Initialize(std::move(types_p));
59: 	allocator = make_shared_ptr<ColumnDataAllocator>(allocator_p);
60: }
61: 
62: ColumnDataCollection::ColumnDataCollection(BufferManager &buffer_manager, vector<LogicalType> types_p) {
63: 	Initialize(std::move(types_p));
64: 	allocator = make_shared_ptr<ColumnDataAllocator>(buffer_manager);
65: }
66: 
67: ColumnDataCollection::ColumnDataCollection(shared_ptr<ColumnDataAllocator> allocator_p, vector<LogicalType> types_p) {
68: 	Initialize(std::move(types_p));
69: 	this->allocator = std::move(allocator_p);
70: }
71: 
72: ColumnDataCollection::ColumnDataCollection(ClientContext &context, vector<LogicalType> types_p,
73:                                            ColumnDataAllocatorType type)
74:     : ColumnDataCollection(make_shared_ptr<ColumnDataAllocator>(context, type), std::move(types_p)) {
75: 	D_ASSERT(!types.empty());
76: }
77: 
78: ColumnDataCollection::ColumnDataCollection(ColumnDataCollection &other)
79:     : ColumnDataCollection(other.allocator, other.types) {
80: 	other.finished_append = true;
81: 	D_ASSERT(!types.empty());
82: }
83: 
84: ColumnDataCollection::~ColumnDataCollection() {
85: }
86: 
87: void ColumnDataCollection::Initialize(vector<LogicalType> types_p) {
88: 	this->types = std::move(types_p);
89: 	this->count = 0;
90: 	this->finished_append = false;
91: 	D_ASSERT(!types.empty());
92: 	copy_functions.reserve(types.size());
93: 	for (auto &type : types) {
94: 		copy_functions.push_back(GetCopyFunction(type));
95: 	}
96: }
97: 
98: void ColumnDataCollection::CreateSegment() {
99: 	segments.emplace_back(make_uniq<ColumnDataCollectionSegment>(allocator, types));
100: }
101: 
102: Allocator &ColumnDataCollection::GetAllocator() const {
103: 	return allocator->GetAllocator();
104: }
105: 
106: idx_t ColumnDataCollection::SizeInBytes() const {
107: 	idx_t total_size = 0;
108: 	for (const auto &segment : segments) {
109: 		total_size += segment->SizeInBytes();
110: 	}
111: 	return total_size;
112: }
113: 
114: idx_t ColumnDataCollection::AllocationSize() const {
115: 	idx_t total_size = 0;
116: 	for (const auto &segment : segments) {
117: 		total_size += segment->AllocationSize();
118: 	}
119: 	return total_size;
120: }
121: 
122: //===--------------------------------------------------------------------===//
123: // ColumnDataRow
124: //===--------------------------------------------------------------------===//
125: ColumnDataRow::ColumnDataRow(DataChunk &chunk_p, idx_t row_index, idx_t base_index)
126:     : chunk(chunk_p), row_index(row_index), base_index(base_index) {
127: }
128: 
129: Value ColumnDataRow::GetValue(idx_t column_index) const {
130: 	D_ASSERT(column_index < chunk.ColumnCount());
131: 	D_ASSERT(row_index < chunk.size());
132: 	return chunk.data[column_index].GetValue(row_index);
133: }
134: 
135: idx_t ColumnDataRow::RowIndex() const {
136: 	return base_index + row_index;
137: }
138: 
139: //===--------------------------------------------------------------------===//
140: // ColumnDataRowCollection
141: //===--------------------------------------------------------------------===//
142: ColumnDataRowCollection::ColumnDataRowCollection(const ColumnDataCollection &collection) {
143: 	if (collection.Count() == 0) {
144: 		return;
145: 	}
146: 	// read all the chunks
147: 	ColumnDataScanState temp_scan_state;
148: 	collection.InitializeScan(temp_scan_state, ColumnDataScanProperties::DISALLOW_ZERO_COPY);
149: 	while (true) {
150: 		auto chunk = make_uniq<DataChunk>();
151: 		collection.InitializeScanChunk(*chunk);
152: 		if (!collection.Scan(temp_scan_state, *chunk)) {
153: 			break;
154: 		}
155: 		chunks.push_back(std::move(chunk));
156: 	}
157: 	// now create all of the column data rows
158: 	rows.reserve(collection.Count());
159: 	idx_t base_row = 0;
160: 	for (auto &chunk : chunks) {
161: 		for (idx_t row_idx = 0; row_idx < chunk->size(); row_idx++) {
162: 			rows.emplace_back(*chunk, row_idx, base_row);
163: 		}
164: 		base_row += chunk->size();
165: 	}
166: }
167: 
168: ColumnDataRow &ColumnDataRowCollection::operator[](idx_t i) {
169: 	return rows[i];
170: }
171: 
172: const ColumnDataRow &ColumnDataRowCollection::operator[](idx_t i) const {
173: 	return rows[i];
174: }
175: 
176: Value ColumnDataRowCollection::GetValue(idx_t column, idx_t index) const {
177: 	return rows[index].GetValue(column);
178: }
179: 
180: //===--------------------------------------------------------------------===//
181: // ColumnDataChunkIterator
182: //===--------------------------------------------------------------------===//
183: ColumnDataChunkIterationHelper ColumnDataCollection::Chunks() const {
184: 	vector<column_t> column_ids;
185: 	for (idx_t i = 0; i < ColumnCount(); i++) {
186: 		column_ids.push_back(i);
187: 	}
188: 	return Chunks(column_ids);
189: }
190: 
191: ColumnDataChunkIterationHelper ColumnDataCollection::Chunks(vector<column_t> column_ids) const {
192: 	return ColumnDataChunkIterationHelper(*this, std::move(column_ids));
193: }
194: 
195: ColumnDataChunkIterationHelper::ColumnDataChunkIterationHelper(const ColumnDataCollection &collection_p,
196:                                                                vector<column_t> column_ids_p)
197:     : collection(collection_p), column_ids(std::move(column_ids_p)) {
198: }
199: 
200: ColumnDataChunkIterationHelper::ColumnDataChunkIterator::ColumnDataChunkIterator(
201:     const ColumnDataCollection *collection_p, vector<column_t> column_ids_p)
202:     : collection(collection_p), scan_chunk(make_shared_ptr<DataChunk>()), row_index(0) {
203: 	if (!collection) {
204: 		return;
205: 	}
206: 	collection->InitializeScan(scan_state, std::move(column_ids_p));
207: 	collection->InitializeScanChunk(scan_state, *scan_chunk);
208: 	collection->Scan(scan_state, *scan_chunk);
209: }
210: 
211: void ColumnDataChunkIterationHelper::ColumnDataChunkIterator::Next() {
212: 	if (!collection) {
213: 		return;
214: 	}
215: 	if (!collection->Scan(scan_state, *scan_chunk)) {
216: 		collection = nullptr;
217: 		row_index = 0;
218: 	} else {
219: 		row_index += scan_chunk->size();
220: 	}
221: }
222: 
223: ColumnDataChunkIterationHelper::ColumnDataChunkIterator &
224: ColumnDataChunkIterationHelper::ColumnDataChunkIterator::operator++() {
225: 	Next();
226: 	return *this;
227: }
228: 
229: bool ColumnDataChunkIterationHelper::ColumnDataChunkIterator::operator!=(const ColumnDataChunkIterator &other) const {
230: 	return collection != other.collection || row_index != other.row_index;
231: }
232: 
233: DataChunk &ColumnDataChunkIterationHelper::ColumnDataChunkIterator::operator*() const {
234: 	return *scan_chunk;
235: }
236: 
237: //===--------------------------------------------------------------------===//
238: // ColumnDataRowIterator
239: //===--------------------------------------------------------------------===//
240: ColumnDataRowIterationHelper ColumnDataCollection::Rows() const {
241: 	return ColumnDataRowIterationHelper(*this);
242: }
243: 
244: ColumnDataRowIterationHelper::ColumnDataRowIterationHelper(const ColumnDataCollection &collection_p)
245:     : collection(collection_p) {
246: }
247: 
248: ColumnDataRowIterationHelper::ColumnDataRowIterator::ColumnDataRowIterator(const ColumnDataCollection *collection_p)
249:     : collection(collection_p), scan_chunk(make_shared_ptr<DataChunk>()), current_row(*scan_chunk, 0, 0) {
250: 	if (!collection) {
251: 		return;
252: 	}
253: 	collection->InitializeScan(scan_state);
254: 	collection->InitializeScanChunk(*scan_chunk);
255: 	collection->Scan(scan_state, *scan_chunk);
256: }
257: 
258: void ColumnDataRowIterationHelper::ColumnDataRowIterator::Next() {
259: 	if (!collection) {
260: 		return;
261: 	}
262: 	current_row.row_index++;
263: 	if (current_row.row_index >= scan_chunk->size()) {
264: 		current_row.base_index += scan_chunk->size();
265: 		current_row.row_index = 0;
266: 		if (!collection->Scan(scan_state, *scan_chunk)) {
267: 			// exhausted collection: move iterator to nop state
268: 			current_row.base_index = 0;
269: 			collection = nullptr;
270: 		}
271: 	}
272: }
273: 
274: ColumnDataRowIterationHelper::ColumnDataRowIterator ColumnDataRowIterationHelper::begin() { // NOLINT
275: 	return ColumnDataRowIterationHelper::ColumnDataRowIterator(collection.Count() == 0 ? nullptr : &collection);
276: }
277: ColumnDataRowIterationHelper::ColumnDataRowIterator ColumnDataRowIterationHelper::end() { // NOLINT
278: 	return ColumnDataRowIterationHelper::ColumnDataRowIterator(nullptr);
279: }
280: 
281: ColumnDataRowIterationHelper::ColumnDataRowIterator &ColumnDataRowIterationHelper::ColumnDataRowIterator::operator++() {
282: 	Next();
283: 	return *this;
284: }
285: 
286: bool ColumnDataRowIterationHelper::ColumnDataRowIterator::operator!=(const ColumnDataRowIterator &other) const {
287: 	return collection != other.collection || current_row.row_index != other.current_row.row_index ||
288: 	       current_row.base_index != other.current_row.base_index;
289: }
290: 
291: const ColumnDataRow &ColumnDataRowIterationHelper::ColumnDataRowIterator::operator*() const {
292: 	return current_row;
293: }
294: 
295: //===--------------------------------------------------------------------===//
296: // Append
297: //===--------------------------------------------------------------------===//
298: void ColumnDataCollection::InitializeAppend(ColumnDataAppendState &state) {
299: 	D_ASSERT(!finished_append);
300: 	state.current_chunk_state.handles.clear();
301: 	state.vector_data.resize(types.size());
302: 	if (segments.empty()) {
303: 		CreateSegment();
304: 	}
305: 	auto &segment = *segments.back();
306: 	if (segment.chunk_data.empty()) {
307: 		segment.AllocateNewChunk();
308: 	}
309: 	segment.InitializeChunkState(segment.chunk_data.size() - 1, state.current_chunk_state);
310: }
311: 
312: void ColumnDataCopyValidity(const UnifiedVectorFormat &source_data, validity_t *target, idx_t source_offset,
313:                             idx_t target_offset, idx_t copy_count) {
314: 	ValidityMask validity(target);
315: 	if (target_offset == 0) {
316: 		// first time appending to this vector
317: 		// all data here is still uninitialized
318: 		// initialize the validity mask to set all to valid
319: 		validity.SetAllValid(STANDARD_VECTOR_SIZE);
320: 	}
321: 	// FIXME: we can do something more optimized here using bitshifts & bitwise ors
322: 	if (!source_data.validity.AllValid()) {
323: 		for (idx_t i = 0; i < copy_count; i++) {
324: 			auto idx = source_data.sel->get_index(source_offset + i);
325: 			if (!source_data.validity.RowIsValid(idx)) {
326: 				validity.SetInvalid(target_offset + i);
327: 			}
328: 		}
329: 	}
330: }
331: 
332: template <class T>
333: struct BaseValueCopy {
334: 	static idx_t TypeSize() {
335: 		return sizeof(T);
336: 	}
337: 
338: 	template <class OP>
339: 	static void Assign(ColumnDataMetaData &meta_data, data_ptr_t target, data_ptr_t source, idx_t target_idx,
340: 	                   idx_t source_idx) {
341: 		auto result_data = (T *)target;
342: 		auto source_data = (T *)source;
343: 		result_data[target_idx] = OP::Operation(meta_data, source_data[source_idx]);
344: 	}
345: };
346: 
347: template <class T>
348: struct StandardValueCopy : public BaseValueCopy<T> {
349: 	static T Operation(ColumnDataMetaData &, T input) {
350: 		return input;
351: 	}
352: };
353: 
354: struct StringValueCopy : public BaseValueCopy<string_t> {
355: 	static string_t Operation(ColumnDataMetaData &meta_data, string_t input) {
356: 		return input.IsInlined() ? input : meta_data.segment.heap->AddBlob(input);
357: 	}
358: };
359: 
360: struct ConstListValueCopy : public BaseValueCopy<list_entry_t> {
361: 	using TYPE = list_entry_t;
362: 
363: 	static TYPE Operation(ColumnDataMetaData &meta_data, TYPE input) {
364: 		input.offset = meta_data.child_list_size;
365: 		return input;
366: 	}
367: };
368: 
369: struct ListValueCopy : public BaseValueCopy<list_entry_t> {
370: 	using TYPE = list_entry_t;
371: 
372: 	static TYPE Operation(ColumnDataMetaData &meta_data, TYPE input) {
373: 		input.offset = meta_data.child_list_size;
374: 		meta_data.child_list_size += input.length;
375: 		return input;
376: 	}
377: };
378: 
379: struct StructValueCopy {
380: 	static idx_t TypeSize() {
381: 		return 0;
382: 	}
383: 
384: 	template <class OP>
385: 	static void Assign(ColumnDataMetaData &meta_data, data_ptr_t target, data_ptr_t source, idx_t target_idx,
386: 	                   idx_t source_idx) {
387: 	}
388: };
389: 
390: template <class OP>
391: static void TemplatedColumnDataCopy(ColumnDataMetaData &meta_data, const UnifiedVectorFormat &source_data,
392:                                     Vector &source, idx_t offset, idx_t count) {
393: 	auto &segment = meta_data.segment;
394: 	auto &append_state = meta_data.state;
395: 
396: 	auto current_index = meta_data.vector_data_index;
397: 	idx_t remaining = count;
398: 	while (remaining > 0) {
399: 		auto &current_segment = segment.GetVectorData(current_index);
400: 		idx_t append_count = MinValue<idx_t>(STANDARD_VECTOR_SIZE - current_segment.count, remaining);
401: 
402: 		auto base_ptr = segment.allocator->GetDataPointer(append_state.current_chunk_state, current_segment.block_id,
403: 		                                                  current_segment.offset);
404: 		auto validity_data = ColumnDataCollectionSegment::GetValidityPointer(base_ptr, OP::TypeSize());
405: 
406: 		ValidityMask result_validity(validity_data);
407: 		if (current_segment.count == 0) {
408: 			// first time appending to this vector
409: 			// all data here is still uninitialized
410: 			// initialize the validity mask to set all to valid
411: 			result_validity.SetAllValid(STANDARD_VECTOR_SIZE);
412: 		}
413: 		for (idx_t i = 0; i < append_count; i++) {
414: 			auto source_idx = source_data.sel->get_index(offset + i);
415: 			if (source_data.validity.RowIsValid(source_idx)) {
416: 				OP::template Assign<OP>(meta_data, base_ptr, source_data.data, current_segment.count + i, source_idx);
417: 			} else {
418: 				result_validity.SetInvalid(current_segment.count + i);
419: 			}
420: 		}
421: 		current_segment.count += append_count;
422: 		offset += append_count;
423: 		remaining -= append_count;
424: 		if (remaining > 0) {
425: 			// need to append more, check if we need to allocate a new vector or not
426: 			if (!current_segment.next_data.IsValid()) {
427: 				segment.AllocateVector(source.GetType(), meta_data.chunk_data, append_state, current_index);
428: 			}
429: 			D_ASSERT(segment.GetVectorData(current_index).next_data.IsValid());
430: 			current_index = segment.GetVectorData(current_index).next_data;
431: 		}
432: 	}
433: }
434: 
435: template <class T>
436: static void ColumnDataCopy(ColumnDataMetaData &meta_data, const UnifiedVectorFormat &source_data, Vector &source,
437:                            idx_t offset, idx_t copy_count) {
438: 	TemplatedColumnDataCopy<StandardValueCopy<T>>(meta_data, source_data, source, offset, copy_count);
439: }
440: 
441: template <>
442: void ColumnDataCopy<string_t>(ColumnDataMetaData &meta_data, const UnifiedVectorFormat &source_data, Vector &source,
443:                               idx_t offset, idx_t copy_count) {
444: 
445: 	const auto &allocator_type = meta_data.segment.allocator->GetType();
446: 	if (allocator_type == ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR ||
447: 	    allocator_type == ColumnDataAllocatorType::HYBRID) {
448: 		// strings cannot be spilled to disk - use StringHeap
449: 		TemplatedColumnDataCopy<StringValueCopy>(meta_data, source_data, source, offset, copy_count);
450: 		return;
451: 	}
452: 	D_ASSERT(allocator_type == ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR);
453: 
454: 	auto &segment = meta_data.segment;
455: 	auto &append_state = meta_data.state;
456: 
457: 	VectorDataIndex child_index;
458: 	if (meta_data.GetVectorMetaData().child_index.IsValid()) {
459: 		// find the last child index
460: 		child_index = segment.GetChildIndex(meta_data.GetVectorMetaData().child_index);
461: 		auto next_child_index = segment.GetVectorData(child_index).next_data;
462: 		while (next_child_index.IsValid()) {
463: 			child_index = next_child_index;
464: 			next_child_index = segment.GetVectorData(child_index).next_data;
465: 		}
466: 	}
467: 
468: 	auto current_index = meta_data.vector_data_index;
469: 	idx_t remaining = copy_count;
470: 	auto block_size = meta_data.segment.allocator->GetBufferManager().GetBlockSize();
471: 	while (remaining > 0) {
472: 		// how many values fit in the current string vector
473: 		idx_t vector_remaining =
474: 		    MinValue<idx_t>(STANDARD_VECTOR_SIZE - segment.GetVectorData(current_index).count, remaining);
475: 
476: 		// 'append_count' is less if we cannot fit that amount of non-inlined strings on one buffer-managed block
477: 		idx_t append_count;
478: 		idx_t heap_size = 0;
479: 		const auto source_entries = UnifiedVectorFormat::GetData<string_t>(source_data);
480: 		for (append_count = 0; append_count < vector_remaining; append_count++) {
481: 			auto source_idx = source_data.sel->get_index(offset + append_count);
482: 			if (!source_data.validity.RowIsValid(source_idx)) {
483: 				continue;
484: 			}
485: 			const auto &entry = source_entries[source_idx];
486: 			if (entry.IsInlined()) {
487: 				continue;
488: 			}
489: 			if (heap_size + entry.GetSize() > block_size) {
490: 				break;
491: 			}
492: 			heap_size += entry.GetSize();
493: 		}
494: 
495: 		if (vector_remaining != 0 && append_count == 0) {
496: 			// The string exceeds Storage::DEFAULT_BLOCK_SIZE, so we allocate one block at a time for long strings.
497: 			auto source_idx = source_data.sel->get_index(offset + append_count);
498: 			D_ASSERT(source_data.validity.RowIsValid(source_idx));
499: 			D_ASSERT(!source_entries[source_idx].IsInlined());
500: 			D_ASSERT(source_entries[source_idx].GetSize() > block_size);
501: 			heap_size += source_entries[source_idx].GetSize();
502: 			append_count++;
503: 		}
504: 
505: 		// allocate string heap for the next 'append_count' strings
506: 		data_ptr_t heap_ptr = nullptr;
507: 		if (heap_size != 0) {
508: 			child_index = segment.AllocateStringHeap(heap_size, meta_data.chunk_data, append_state, child_index);
509: 			if (!meta_data.GetVectorMetaData().child_index.IsValid()) {
510: 				meta_data.GetVectorMetaData().child_index = meta_data.segment.AddChildIndex(child_index);
511: 			}
512: 			auto &child_segment = segment.GetVectorData(child_index);
513: 			heap_ptr = segment.allocator->GetDataPointer(append_state.current_chunk_state, child_segment.block_id,
514: 			                                             child_segment.offset);
515: 		}
516: 
517: 		auto &current_segment = segment.GetVectorData(current_index);
518: 		auto base_ptr = segment.allocator->GetDataPointer(append_state.current_chunk_state, current_segment.block_id,
519: 		                                                  current_segment.offset);
520: 		auto validity_data = ColumnDataCollectionSegment::GetValidityPointer(base_ptr, sizeof(string_t));
521: 		ValidityMask target_validity(validity_data);
522: 		if (current_segment.count == 0) {
523: 			// first time appending to this vector
524: 			// all data here is still uninitialized
525: 			// initialize the validity mask to set all to valid
526: 			target_validity.SetAllValid(STANDARD_VECTOR_SIZE);
527: 		}
528: 
529: 		auto target_entries = reinterpret_cast<string_t *>(base_ptr);
530: 		for (idx_t i = 0; i < append_count; i++) {
531: 			auto source_idx = source_data.sel->get_index(offset + i);
532: 			auto target_idx = current_segment.count + i;
533: 			if (!source_data.validity.RowIsValid(source_idx)) {
534: 				target_validity.SetInvalid(target_idx);
535: 				continue;
536: 			}
537: 			const auto &source_entry = source_entries[source_idx];
538: 			auto &target_entry = target_entries[target_idx];
539: 			if (source_entry.IsInlined()) {
540: 				target_entry = source_entry;
541: 			} else {
542: 				D_ASSERT(heap_ptr != nullptr);
543: 				memcpy(heap_ptr, source_entry.GetData(), source_entry.GetSize());
544: 				target_entry =
545: 				    string_t(const_char_ptr_cast(heap_ptr), UnsafeNumericCast<uint32_t>(source_entry.GetSize()));
546: 				heap_ptr += source_entry.GetSize();
547: 			}
548: 		}
549: 
550: 		if (heap_size != 0) {
551: 			current_segment.swizzle_data.emplace_back(child_index, current_segment.count, append_count);
552: 		}
553: 
554: 		current_segment.count += append_count;
555: 		offset += append_count;
556: 		remaining -= append_count;
557: 
558: 		if (vector_remaining - append_count == 0) {
559: 			// need to append more, check if we need to allocate a new vector or not
560: 			if (!current_segment.next_data.IsValid()) {
561: 				segment.AllocateVector(source.GetType(), meta_data.chunk_data, append_state, current_index);
562: 			}
563: 			D_ASSERT(segment.GetVectorData(current_index).next_data.IsValid());
564: 			current_index = segment.GetVectorData(current_index).next_data;
565: 		}
566: 	}
567: }
568: 
569: template <>
570: void ColumnDataCopy<list_entry_t>(ColumnDataMetaData &meta_data, const UnifiedVectorFormat &source_data, Vector &source,
571:                                   idx_t offset, idx_t copy_count) {
572: 
573: 	auto &segment = meta_data.segment;
574: 
575: 	auto &child_vector = ListVector::GetEntry(source);
576: 	auto &child_type = child_vector.GetType();
577: 
578: 	if (!meta_data.GetVectorMetaData().child_index.IsValid()) {
579: 		auto child_index = segment.AllocateVector(child_type, meta_data.chunk_data, meta_data.state);
580: 		meta_data.GetVectorMetaData().child_index = meta_data.segment.AddChildIndex(child_index);
581: 	}
582: 
583: 	auto &child_function = meta_data.copy_function.child_functions[0];
584: 	auto child_index = segment.GetChildIndex(meta_data.GetVectorMetaData().child_index);
585: 
586: 	// figure out the current list size by traversing the set of child entries
587: 	idx_t current_list_size = 0;
588: 	auto current_child_index = child_index;
589: 	while (current_child_index.IsValid()) {
590: 		auto &child_vdata = segment.GetVectorData(current_child_index);
591: 		current_list_size += child_vdata.count;
592: 		current_child_index = child_vdata.next_data;
593: 	}
594: 
595: 	// set the child vector
596: 	UnifiedVectorFormat child_vector_data;
597: 	ColumnDataMetaData child_meta_data(child_function, meta_data, child_index);
598: 	auto info = ListVector::GetConsecutiveChildListInfo(source, offset, copy_count);
599: 
600: 	if (info.needs_slicing) {
601: 		SelectionVector sel(info.child_list_info.length);
602: 		ListVector::GetConsecutiveChildSelVector(source, sel, offset, copy_count);
603: 
604: 		auto sliced_child_vector = Vector(child_vector, sel, info.child_list_info.length);
605: 		sliced_child_vector.Flatten(info.child_list_info.length);
606: 		info.child_list_info.offset = 0;
607: 
608: 		sliced_child_vector.ToUnifiedFormat(info.child_list_info.length, child_vector_data);
609: 		child_function.function(child_meta_data, child_vector_data, sliced_child_vector, info.child_list_info.offset,
610: 		                        info.child_list_info.length);
611: 
612: 	} else {
613: 		child_vector.ToUnifiedFormat(info.child_list_info.length, child_vector_data);
614: 		child_function.function(child_meta_data, child_vector_data, child_vector, info.child_list_info.offset,
615: 		                        info.child_list_info.length);
616: 	}
617: 
618: 	// now copy the list entries
619: 	meta_data.child_list_size = current_list_size;
620: 	if (info.is_constant) {
621: 		TemplatedColumnDataCopy<ConstListValueCopy>(meta_data, source_data, source, offset, copy_count);
622: 	} else {
623: 		TemplatedColumnDataCopy<ListValueCopy>(meta_data, source_data, source, offset, copy_count);
624: 	}
625: }
626: 
627: void ColumnDataCopyStruct(ColumnDataMetaData &meta_data, const UnifiedVectorFormat &source_data, Vector &source,
628:                           idx_t offset, idx_t copy_count) {
629: 	auto &segment = meta_data.segment;
630: 
631: 	// copy the NULL values for the main struct vector
632: 	TemplatedColumnDataCopy<StructValueCopy>(meta_data, source_data, source, offset, copy_count);
633: 
634: 	auto &child_types = StructType::GetChildTypes(source.GetType());
635: 	// now copy all the child vectors
636: 	D_ASSERT(meta_data.GetVectorMetaData().child_index.IsValid());
637: 	auto &child_vectors = StructVector::GetEntries(source);
638: 	for (idx_t child_idx = 0; child_idx < child_types.size(); child_idx++) {
639: 		auto &child_function = meta_data.copy_function.child_functions[child_idx];
640: 		auto child_index = segment.GetChildIndex(meta_data.GetVectorMetaData().child_index, child_idx);
641: 		ColumnDataMetaData child_meta_data(child_function, meta_data, child_index);
642: 
643: 		UnifiedVectorFormat child_data;
644: 		child_vectors[child_idx]->ToUnifiedFormat(copy_count, child_data);
645: 
646: 		child_function.function(child_meta_data, child_data, *child_vectors[child_idx], offset, copy_count);
647: 	}
648: }
649: 
650: void ColumnDataCopyArray(ColumnDataMetaData &meta_data, const UnifiedVectorFormat &source_data, Vector &source,
651:                          idx_t offset, idx_t copy_count) {
652: 
653: 	auto &segment = meta_data.segment;
654: 
655: 	// copy the NULL values for the main array vector (the same as for a struct vector)
656: 	TemplatedColumnDataCopy<StructValueCopy>(meta_data, source_data, source, offset, copy_count);
657: 
658: 	auto &child_vector = ArrayVector::GetEntry(source);
659: 	auto &child_type = child_vector.GetType();
660: 	auto array_size = ArrayType::GetSize(source.GetType());
661: 
662: 	if (!meta_data.GetVectorMetaData().child_index.IsValid()) {
663: 		auto child_index = segment.AllocateVector(child_type, meta_data.chunk_data, meta_data.state);
664: 		meta_data.GetVectorMetaData().child_index = meta_data.segment.AddChildIndex(child_index);
665: 	}
666: 
667: 	auto &child_function = meta_data.copy_function.child_functions[0];
668: 	auto child_index = segment.GetChildIndex(meta_data.GetVectorMetaData().child_index);
669: 
670: 	auto current_child_index = child_index;
671: 	while (current_child_index.IsValid()) {
672: 		auto &child_vdata = segment.GetVectorData(current_child_index);
673: 		current_child_index = child_vdata.next_data;
674: 	}
675: 
676: 	UnifiedVectorFormat child_vector_data;
677: 	ColumnDataMetaData child_meta_data(child_function, meta_data, child_index);
678: 	child_vector.ToUnifiedFormat(copy_count * array_size, child_vector_data);
679: 
680: 	// Broadcast and sync the validity of the array vector to the child vector
681: 
682: 	if (source_data.validity.IsMaskSet()) {
683: 		for (idx_t i = 0; i < copy_count; i++) {
684: 			auto source_idx = source_data.sel->get_index(offset + i);
685: 			if (!source_data.validity.RowIsValid(source_idx)) {
686: 				for (idx_t j = 0; j < array_size; j++) {
687: 					child_vector_data.validity.SetInvalid(source_idx * array_size + j);
688: 				}
689: 			}
690: 		}
691: 	}
692: 
693: 	auto is_constant = source.GetVectorType() == VectorType::CONSTANT_VECTOR;
694: 	// If the array is constant, we need to copy the child vector n times
695: 	if (is_constant) {
696: 		for (idx_t i = 0; i < copy_count; i++) {
697: 			child_function.function(child_meta_data, child_vector_data, child_vector, 0, array_size);
698: 		}
699: 	} else {
700: 		child_function.function(child_meta_data, child_vector_data, child_vector, offset * array_size,
701: 		                        copy_count * array_size);
702: 	}
703: }
704: 
705: ColumnDataCopyFunction ColumnDataCollection::GetCopyFunction(const LogicalType &type) {
706: 	ColumnDataCopyFunction result;
707: 	column_data_copy_function_t function;
708: 	switch (type.InternalType()) {
709: 	case PhysicalType::BOOL:
710: 		function = ColumnDataCopy<bool>;
711: 		break;
712: 	case PhysicalType::INT8:
713: 		function = ColumnDataCopy<int8_t>;
714: 		break;
715: 	case PhysicalType::INT16:
716: 		function = ColumnDataCopy<int16_t>;
717: 		break;
718: 	case PhysicalType::INT32:
719: 		function = ColumnDataCopy<int32_t>;
720: 		break;
721: 	case PhysicalType::INT64:
722: 		function = ColumnDataCopy<int64_t>;
723: 		break;
724: 	case PhysicalType::INT128:
725: 		function = ColumnDataCopy<hugeint_t>;
726: 		break;
727: 	case PhysicalType::UINT8:
728: 		function = ColumnDataCopy<uint8_t>;
729: 		break;
730: 	case PhysicalType::UINT16:
731: 		function = ColumnDataCopy<uint16_t>;
732: 		break;
733: 	case PhysicalType::UINT32:
734: 		function = ColumnDataCopy<uint32_t>;
735: 		break;
736: 	case PhysicalType::UINT64:
737: 		function = ColumnDataCopy<uint64_t>;
738: 		break;
739: 	case PhysicalType::UINT128:
740: 		function = ColumnDataCopy<uhugeint_t>;
741: 		break;
742: 	case PhysicalType::FLOAT:
743: 		function = ColumnDataCopy<float>;
744: 		break;
745: 	case PhysicalType::DOUBLE:
746: 		function = ColumnDataCopy<double>;
747: 		break;
748: 	case PhysicalType::INTERVAL:
749: 		function = ColumnDataCopy<interval_t>;
750: 		break;
751: 	case PhysicalType::VARCHAR:
752: 		function = ColumnDataCopy<string_t>;
753: 		break;
754: 	case PhysicalType::STRUCT: {
755: 		function = ColumnDataCopyStruct;
756: 		auto &child_types = StructType::GetChildTypes(type);
757: 		for (auto &kv : child_types) {
758: 			result.child_functions.push_back(GetCopyFunction(kv.second));
759: 		}
760: 		break;
761: 	}
762: 	case PhysicalType::LIST: {
763: 		function = ColumnDataCopy<list_entry_t>;
764: 		auto child_function = GetCopyFunction(ListType::GetChildType(type));
765: 		result.child_functions.push_back(child_function);
766: 		break;
767: 	}
768: 	case PhysicalType::ARRAY: {
769: 		function = ColumnDataCopyArray;
770: 		auto child_function = GetCopyFunction(ArrayType::GetChildType(type));
771: 		result.child_functions.push_back(child_function);
772: 		break;
773: 	}
774: 	default:
775: 		throw InternalException("Unsupported type for ColumnDataCollection::GetCopyFunction");
776: 	}
777: 	result.function = function;
778: 	return result;
779: }
780: 
781: static bool IsComplexType(const LogicalType &type) {
782: 	switch (type.InternalType()) {
783: 	case PhysicalType::STRUCT:
784: 	case PhysicalType::LIST:
785: 	case PhysicalType::ARRAY:
786: 		return true;
787: 	default:
788: 		return false;
789: 	};
790: }
791: 
792: void ColumnDataCollection::Append(ColumnDataAppendState &state, DataChunk &input) {
793: 	D_ASSERT(!finished_append);
794: 	D_ASSERT(types == input.GetTypes());
795: 
796: 	auto &segment = *segments.back();
797: 	for (idx_t vector_idx = 0; vector_idx < types.size(); vector_idx++) {
798: 		if (IsComplexType(input.data[vector_idx].GetType())) {
799: 			input.data[vector_idx].Flatten(input.size());
800: 		}
801: 		input.data[vector_idx].ToUnifiedFormat(input.size(), state.vector_data[vector_idx]);
802: 	}
803: 
804: 	idx_t remaining = input.size();
805: 	while (remaining > 0) {
806: 		auto &chunk_data = segment.chunk_data.back();
807: 		idx_t append_amount = MinValue<idx_t>(remaining, STANDARD_VECTOR_SIZE - chunk_data.count);
808: 		if (append_amount > 0) {
809: 			idx_t offset = input.size() - remaining;
810: 			for (idx_t vector_idx = 0; vector_idx < types.size(); vector_idx++) {
811: 				ColumnDataMetaData meta_data(copy_functions[vector_idx], segment, state, chunk_data,
812: 				                             chunk_data.vector_data[vector_idx]);
813: 				copy_functions[vector_idx].function(meta_data, state.vector_data[vector_idx], input.data[vector_idx],
814: 				                                    offset, append_amount);
815: 			}
816: 			chunk_data.count += append_amount;
817: 		}
818: 		remaining -= append_amount;
819: 		if (remaining > 0) {
820: 			// more to do
821: 			// allocate a new chunk
822: 			segment.AllocateNewChunk();
823: 			segment.InitializeChunkState(segment.chunk_data.size() - 1, state.current_chunk_state);
824: 		}
825: 	}
826: 	segment.count += input.size();
827: 	count += input.size();
828: }
829: 
830: void ColumnDataCollection::Append(DataChunk &input) {
831: 	ColumnDataAppendState state;
832: 	InitializeAppend(state);
833: 	Append(state, input);
834: }
835: 
836: //===--------------------------------------------------------------------===//
837: // Scan
838: //===--------------------------------------------------------------------===//
839: void ColumnDataCollection::InitializeScan(ColumnDataScanState &state, ColumnDataScanProperties properties) const {
840: 	vector<column_t> column_ids;
841: 	column_ids.reserve(types.size());
842: 	for (idx_t i = 0; i < types.size(); i++) {
843: 		column_ids.push_back(i);
844: 	}
845: 	InitializeScan(state, std::move(column_ids), properties);
846: }
847: 
848: void ColumnDataCollection::InitializeScan(ColumnDataScanState &state, vector<column_t> column_ids,
849:                                           ColumnDataScanProperties properties) const {
850: 	state.chunk_index = 0;
851: 	state.segment_index = 0;
852: 	state.current_row_index = 0;
853: 	state.next_row_index = 0;
854: 	state.current_chunk_state.handles.clear();
855: 	state.properties = properties;
856: 	state.column_ids = std::move(column_ids);
857: }
858: 
859: void ColumnDataCollection::InitializeScan(ColumnDataParallelScanState &state,
860:                                           ColumnDataScanProperties properties) const {
861: 	InitializeScan(state.scan_state, properties);
862: }
863: 
864: void ColumnDataCollection::InitializeScan(ColumnDataParallelScanState &state, vector<column_t> column_ids,
865:                                           ColumnDataScanProperties properties) const {
866: 	InitializeScan(state.scan_state, std::move(column_ids), properties);
867: }
868: 
869: bool ColumnDataCollection::Scan(ColumnDataParallelScanState &state, ColumnDataLocalScanState &lstate,
870:                                 DataChunk &result) const {
871: 	result.Reset();
872: 
873: 	idx_t chunk_index;
874: 	idx_t segment_index;
875: 	idx_t row_index;
876: 	{
877: 		lock_guard<mutex> l(state.lock);
878: 		if (!NextScanIndex(state.scan_state, chunk_index, segment_index, row_index)) {
879: 			return false;
880: 		}
881: 	}
882: 	ScanAtIndex(state, lstate, result, chunk_index, segment_index, row_index);
883: 	return true;
884: }
885: 
886: void ColumnDataCollection::InitializeScanChunk(DataChunk &chunk) const {
887: 	chunk.Initialize(allocator->GetAllocator(), types);
888: }
889: 
890: void ColumnDataCollection::InitializeScanChunk(ColumnDataScanState &state, DataChunk &chunk) const {
891: 	D_ASSERT(!state.column_ids.empty());
892: 	vector<LogicalType> chunk_types;
893: 	chunk_types.reserve(state.column_ids.size());
894: 	for (idx_t i = 0; i < state.column_ids.size(); i++) {
895: 		auto column_idx = state.column_ids[i];
896: 		D_ASSERT(column_idx < types.size());
897: 		chunk_types.push_back(types[column_idx]);
898: 	}
899: 	chunk.Initialize(allocator->GetAllocator(), chunk_types);
900: }
901: 
902: bool ColumnDataCollection::NextScanIndex(ColumnDataScanState &state, idx_t &chunk_index, idx_t &segment_index,
903:                                          idx_t &row_index) const {
904: 	row_index = state.current_row_index = state.next_row_index;
905: 	// check if we still have collections to scan
906: 	if (state.segment_index >= segments.size()) {
907: 		// no more data left in the scan
908: 		return false;
909: 	}
910: 	// check within the current collection if we still have chunks to scan
911: 	while (state.chunk_index >= segments[state.segment_index]->chunk_data.size()) {
912: 		// exhausted all chunks for this internal data structure: move to the next one
913: 		state.chunk_index = 0;
914: 		state.segment_index++;
915: 		state.current_chunk_state.handles.clear();
916: 		if (state.segment_index >= segments.size()) {
917: 			return false;
918: 		}
919: 	}
920: 	state.next_row_index += segments[state.segment_index]->chunk_data[state.chunk_index].count;
921: 	segment_index = state.segment_index;
922: 	chunk_index = state.chunk_index++;
923: 	return true;
924: }
925: 
926: bool ColumnDataCollection::PrevScanIndex(ColumnDataScanState &state, idx_t &chunk_index, idx_t &segment_index,
927:                                          idx_t &row_index) const {
928: 	// check within the current segment if we still have chunks to scan
929: 	// Note that state.chunk_index is 1-indexed, with 0 as undefined.
930: 	while (state.chunk_index <= 1) {
931: 		if (!state.segment_index) {
932: 			return false;
933: 		}
934: 
935: 		--state.segment_index;
936: 		state.chunk_index = segments[state.segment_index]->chunk_data.size() + 1;
937: 		state.current_chunk_state.handles.clear();
938: 	}
939: 
940: 	--state.chunk_index;
941: 	segment_index = state.segment_index;
942: 	chunk_index = state.chunk_index - 1;
943: 	state.next_row_index = state.current_row_index;
944: 	state.current_row_index -= segments[state.segment_index]->chunk_data[chunk_index].count;
945: 	row_index = state.current_row_index;
946: 	return true;
947: }
948: 
949: void ColumnDataCollection::ScanAtIndex(ColumnDataParallelScanState &state, ColumnDataLocalScanState &lstate,
950:                                        DataChunk &result, idx_t chunk_index, idx_t segment_index,
951:                                        idx_t row_index) const {
952: 	if (segment_index != lstate.current_segment_index) {
953: 		lstate.current_chunk_state.handles.clear();
954: 		lstate.current_segment_index = segment_index;
955: 	}
956: 	auto &segment = *segments[segment_index];
957: 	lstate.current_chunk_state.properties = state.scan_state.properties;
958: 	segment.ReadChunk(chunk_index, lstate.current_chunk_state, result, state.scan_state.column_ids);
959: 	lstate.current_row_index = row_index;
960: 	result.Verify();
961: }
962: 
963: bool ColumnDataCollection::Scan(ColumnDataScanState &state, DataChunk &result) const {
964: 	result.Reset();
965: 
966: 	idx_t chunk_index;
967: 	idx_t segment_index;
968: 	idx_t row_index;
969: 	if (!NextScanIndex(state, chunk_index, segment_index, row_index)) {
970: 		return false;
971: 	}
972: 
973: 	// found a chunk to scan -> scan it
974: 	auto &segment = *segments[segment_index];
975: 	state.current_chunk_state.properties = state.properties;
976: 	segment.ReadChunk(chunk_index, state.current_chunk_state, result, state.column_ids);
977: 	result.Verify();
978: 	return true;
979: }
980: 
981: bool ColumnDataCollection::Seek(idx_t seek_idx, ColumnDataScanState &state, DataChunk &result) const {
982: 	//	Idempotency: Don't change anything if the row is already in range
983: 	if (state.current_row_index <= seek_idx && seek_idx < state.next_row_index) {
984: 		return true;
985: 	}
986: 
987: 	result.Reset();
988: 
989: 	//	Linear scan for now. We could use a current_row_index => chunk map at some point
990: 	//	but most use cases should be pretty local
991: 	idx_t chunk_index;
992: 	idx_t segment_index;
993: 	idx_t row_index;
994: 	while (seek_idx < state.current_row_index) {
995: 		if (!PrevScanIndex(state, chunk_index, segment_index, row_index)) {
996: 			return false;
997: 		}
998: 	}
999: 	while (state.next_row_index <= seek_idx) {
1000: 		if (!NextScanIndex(state, chunk_index, segment_index, row_index)) {
1001: 			return false;
1002: 		}
1003: 	}
1004: 
1005: 	// found a chunk to scan -> scan it
1006: 	auto &segment = *segments[segment_index];
1007: 	state.current_chunk_state.properties = state.properties;
1008: 	segment.ReadChunk(chunk_index, state.current_chunk_state, result, state.column_ids);
1009: 	result.Verify();
1010: 	return true;
1011: }
1012: 
1013: ColumnDataRowCollection ColumnDataCollection::GetRows() const {
1014: 	return ColumnDataRowCollection(*this);
1015: }
1016: 
1017: //===--------------------------------------------------------------------===//
1018: // Combine
1019: //===--------------------------------------------------------------------===//
1020: void ColumnDataCollection::Combine(ColumnDataCollection &other) {
1021: 	if (other.count == 0) {
1022: 		return;
1023: 	}
1024: 	if (types != other.types) {
1025: 		throw InternalException("Attempting to combine ColumnDataCollections with mismatching types");
1026: 	}
1027: 	this->count += other.count;
1028: 	this->segments.reserve(segments.size() + other.segments.size());
1029: 	for (auto &other_seg : other.segments) {
1030: 		segments.push_back(std::move(other_seg));
1031: 	}
1032: 	other.Reset();
1033: 	Verify();
1034: }
1035: 
1036: //===--------------------------------------------------------------------===//
1037: // Fetch
1038: //===--------------------------------------------------------------------===//
1039: idx_t ColumnDataCollection::ChunkCount() const {
1040: 	idx_t chunk_count = 0;
1041: 	for (auto &segment : segments) {
1042: 		chunk_count += segment->ChunkCount();
1043: 	}
1044: 	return chunk_count;
1045: }
1046: 
1047: void ColumnDataCollection::FetchChunk(idx_t chunk_idx, DataChunk &result) const {
1048: 	D_ASSERT(chunk_idx < ChunkCount());
1049: 	for (auto &segment : segments) {
1050: 		if (chunk_idx >= segment->ChunkCount()) {
1051: 			chunk_idx -= segment->ChunkCount();
1052: 		} else {
1053: 			segment->FetchChunk(chunk_idx, result);
1054: 			return;
1055: 		}
1056: 	}
1057: 	throw InternalException("Failed to find chunk in ColumnDataCollection");
1058: }
1059: 
1060: //===--------------------------------------------------------------------===//
1061: // Helpers
1062: //===--------------------------------------------------------------------===//
1063: void ColumnDataCollection::Verify() {
1064: #ifdef DEBUG
1065: 	// verify counts
1066: 	idx_t total_segment_count = 0;
1067: 	for (auto &segment : segments) {
1068: 		segment->Verify();
1069: 		total_segment_count += segment->count;
1070: 	}
1071: 	D_ASSERT(total_segment_count == this->count);
1072: #endif
1073: }
1074: 
1075: // LCOV_EXCL_START
1076: string ColumnDataCollection::ToString() const {
1077: 	DataChunk chunk;
1078: 	InitializeScanChunk(chunk);
1079: 
1080: 	ColumnDataScanState scan_state;
1081: 	InitializeScan(scan_state);
1082: 
1083: 	string result = StringUtil::Format("ColumnDataCollection - [%llu Chunks, %llu Rows]\n", ChunkCount(), Count());
1084: 	idx_t chunk_idx = 0;
1085: 	idx_t row_count = 0;
1086: 	while (Scan(scan_state, chunk)) {
1087: 		result +=
1088: 		    StringUtil::Format("Chunk %llu - [Rows %llu - %llu]\n", chunk_idx, row_count, row_count + chunk.size()) +
1089: 		    chunk.ToString();
1090: 		chunk_idx++;
1091: 		row_count += chunk.size();
1092: 	}
1093: 
1094: 	return result;
1095: }
1096: // LCOV_EXCL_STOP
1097: 
1098: void ColumnDataCollection::Print() const {
1099: 	Printer::Print(ToString());
1100: }
1101: 
1102: void ColumnDataCollection::Reset() {
1103: 	count = 0;
1104: 	segments.clear();
1105: 
1106: 	// Refreshes the ColumnDataAllocator to prevent holding on to allocated data unnecessarily
1107: 	allocator = make_shared_ptr<ColumnDataAllocator>(*allocator);
1108: }
1109: 
1110: struct ValueResultEquals {
1111: 	bool operator()(const Value &a, const Value &b) const {
1112: 		return Value::DefaultValuesAreEqual(a, b);
1113: 	}
1114: };
1115: 
1116: bool ColumnDataCollection::ResultEquals(const ColumnDataCollection &left, const ColumnDataCollection &right,
1117:                                         string &error_message, bool ordered) {
1118: 	if (left.ColumnCount() != right.ColumnCount()) {
1119: 		error_message = "Column count mismatch";
1120: 		return false;
1121: 	}
1122: 	if (left.Count() != right.Count()) {
1123: 		error_message = "Row count mismatch";
1124: 		return false;
1125: 	}
1126: 	auto left_rows = left.GetRows();
1127: 	auto right_rows = right.GetRows();
1128: 	for (idx_t r = 0; r < left.Count(); r++) {
1129: 		for (idx_t c = 0; c < left.ColumnCount(); c++) {
1130: 			auto lvalue = left_rows.GetValue(c, r);
1131: 			auto rvalue = right_rows.GetValue(c, r);
1132: 
1133: 			if (!Value::DefaultValuesAreEqual(lvalue, rvalue)) {
1134: 				error_message =
1135: 				    StringUtil::Format("%s <> %s (row: %lld, col: %lld)\n", lvalue.ToString(), rvalue.ToString(), r, c);
1136: 				break;
1137: 			}
1138: 		}
1139: 		if (!error_message.empty()) {
1140: 			if (ordered) {
1141: 				return false;
1142: 			} else {
1143: 				break;
1144: 			}
1145: 		}
1146: 	}
1147: 	if (!error_message.empty()) {
1148: 		// do an unordered comparison
1149: 		bool found_all = true;
1150: 		for (idx_t c = 0; c < left.ColumnCount(); c++) {
1151: 			std::unordered_multiset<Value, ValueHashFunction, ValueResultEquals> lvalues;
1152: 			for (idx_t r = 0; r < left.Count(); r++) {
1153: 				auto lvalue = left_rows.GetValue(c, r);
1154: 				lvalues.insert(lvalue);
1155: 			}
1156: 			for (idx_t r = 0; r < right.Count(); r++) {
1157: 				auto rvalue = right_rows.GetValue(c, r);
1158: 				auto entry = lvalues.find(rvalue);
1159: 				if (entry == lvalues.end()) {
1160: 					found_all = false;
1161: 					break;
1162: 				}
1163: 				lvalues.erase(entry);
1164: 			}
1165: 			if (!found_all) {
1166: 				break;
1167: 			}
1168: 		}
1169: 		if (!found_all) {
1170: 			return false;
1171: 		}
1172: 		error_message = string();
1173: 	}
1174: 	return true;
1175: }
1176: 
1177: vector<shared_ptr<StringHeap>> ColumnDataCollection::GetHeapReferences() {
1178: 	vector<shared_ptr<StringHeap>> result(segments.size(), nullptr);
1179: 	for (idx_t segment_idx = 0; segment_idx < segments.size(); segment_idx++) {
1180: 		result[segment_idx] = segments[segment_idx]->heap;
1181: 	}
1182: 	return result;
1183: }
1184: 
1185: ColumnDataAllocatorType ColumnDataCollection::GetAllocatorType() const {
1186: 	return allocator->GetType();
1187: }
1188: 
1189: const vector<unique_ptr<ColumnDataCollectionSegment>> &ColumnDataCollection::GetSegments() const {
1190: 	return segments;
1191: }
1192: 
1193: void ColumnDataCollection::Serialize(Serializer &serializer) const {
1194: 	vector<vector<Value>> values;
1195: 	values.resize(ColumnCount());
1196: 	for (auto &chunk : Chunks()) {
1197: 		for (idx_t c = 0; c < chunk.ColumnCount(); c++) {
1198: 			for (idx_t r = 0; r < chunk.size(); r++) {
1199: 				values[c].push_back(chunk.GetValue(c, r));
1200: 			}
1201: 		}
1202: 	}
1203: 	serializer.WriteProperty(100, "types", types);
1204: 	serializer.WriteProperty(101, "values", values);
1205: }
1206: 
1207: unique_ptr<ColumnDataCollection> ColumnDataCollection::Deserialize(Deserializer &deserializer) {
1208: 	auto types = deserializer.ReadProperty<vector<LogicalType>>(100, "types");
1209: 	auto values = deserializer.ReadProperty<vector<vector<Value>>>(101, "values");
1210: 
1211: 	auto collection = make_uniq<ColumnDataCollection>(Allocator::DefaultAllocator(), types);
1212: 	if (values.empty()) {
1213: 		return collection;
1214: 	}
1215: 	DataChunk chunk;
1216: 	chunk.Initialize(Allocator::DefaultAllocator(), types);
1217: 
1218: 	for (idx_t r = 0; r < values[0].size(); r++) {
1219: 		for (idx_t c = 0; c < types.size(); c++) {
1220: 			chunk.SetValue(c, chunk.size(), values[c][r]);
1221: 		}
1222: 		chunk.SetCardinality(chunk.size() + 1);
1223: 		if (chunk.size() == STANDARD_VECTOR_SIZE) {
1224: 			collection->Append(chunk);
1225: 			chunk.Reset();
1226: 		}
1227: 	}
1228: 	if (chunk.size() > 0) {
1229: 		collection->Append(chunk);
1230: 	}
1231: 	return collection;
1232: }
1233: 
1234: } // namespace duckdb
[end of src/common/types/column/column_data_collection.cpp]
[start of src/common/types/row/partitioned_tuple_data.cpp]
1: #include "duckdb/common/types/row/partitioned_tuple_data.hpp"
2: 
3: #include "duckdb/common/radix_partitioning.hpp"
4: #include "duckdb/common/types/row/tuple_data_iterator.hpp"
5: #include "duckdb/storage/buffer_manager.hpp"
6: 
7: namespace duckdb {
8: 
9: PartitionedTupleData::PartitionedTupleData(PartitionedTupleDataType type_p, BufferManager &buffer_manager_p,
10:                                            const TupleDataLayout &layout_p)
11:     : type(type_p), buffer_manager(buffer_manager_p), layout(layout_p.Copy()), count(0), data_size(0),
12:       allocators(make_shared_ptr<PartitionTupleDataAllocators>()) {
13: }
14: 
15: PartitionedTupleData::PartitionedTupleData(const PartitionedTupleData &other)
16:     : type(other.type), buffer_manager(other.buffer_manager), layout(other.layout.Copy()), count(0), data_size(0) {
17: }
18: 
19: PartitionedTupleData::~PartitionedTupleData() {
20: }
21: 
22: const TupleDataLayout &PartitionedTupleData::GetLayout() const {
23: 	return layout;
24: }
25: 
26: PartitionedTupleDataType PartitionedTupleData::GetType() const {
27: 	return type;
28: }
29: 
30: void PartitionedTupleData::InitializeAppendState(PartitionedTupleDataAppendState &state,
31:                                                  TupleDataPinProperties properties) const {
32: 	state.partition_sel.Initialize();
33: 	state.reverse_partition_sel.Initialize();
34: 
35: 	InitializeAppendStateInternal(state, properties);
36: }
37: 
38: void PartitionedTupleData::Append(PartitionedTupleDataAppendState &state, DataChunk &input,
39:                                   const SelectionVector &append_sel, const idx_t append_count) {
40: 	TupleDataCollection::ToUnifiedFormat(state.chunk_state, input);
41: 	AppendUnified(state, input, append_sel, append_count);
42: }
43: 
44: bool PartitionedTupleData::UseFixedSizeMap() const {
45: 	return MaxPartitionIndex() < PartitionedTupleDataAppendState::MAP_THRESHOLD;
46: }
47: 
48: void PartitionedTupleData::AppendUnified(PartitionedTupleDataAppendState &state, DataChunk &input,
49:                                          const SelectionVector &append_sel, const idx_t append_count) {
50: 	const idx_t actual_append_count = append_count == DConstants::INVALID_INDEX ? input.size() : append_count;
51: 
52: 	// Compute partition indices and store them in state.partition_indices
53: 	ComputePartitionIndices(state, input, append_sel, actual_append_count);
54: 
55: 	// Build the selection vector for the partitions
56: 	BuildPartitionSel(state, append_sel, actual_append_count);
57: 
58: 	// Early out: check if everything belongs to a single partition
59: 	const auto partition_index = state.GetPartitionIndexIfSinglePartition(UseFixedSizeMap());
60: 	if (partition_index.IsValid()) {
61: 		auto &partition = *partitions[partition_index.GetIndex()];
62: 		auto &partition_pin_state = *state.partition_pin_states[partition_index.GetIndex()];
63: 
64: 		const auto size_before = partition.SizeInBytes();
65: 		partition.AppendUnified(partition_pin_state, state.chunk_state, input, append_sel, actual_append_count);
66: 		data_size += partition.SizeInBytes() - size_before;
67: 	} else {
68: 		// Compute the heap sizes for the whole chunk
69: 		if (!layout.AllConstant()) {
70: 			TupleDataCollection::ComputeHeapSizes(state.chunk_state, input, state.partition_sel, actual_append_count);
71: 		}
72: 
73: 		// Build the buffer space
74: 		BuildBufferSpace(state);
75: 
76: 		// Now scatter everything in one go
77: 		partitions[0]->Scatter(state.chunk_state, input, state.partition_sel, actual_append_count);
78: 	}
79: 
80: 	count += actual_append_count;
81: 	Verify();
82: }
83: 
84: void PartitionedTupleData::Append(PartitionedTupleDataAppendState &state, TupleDataChunkState &input,
85:                                   const idx_t append_count) {
86: 	// Compute partition indices and store them in state.partition_indices
87: 	ComputePartitionIndices(input.row_locations, append_count, state.partition_indices);
88: 
89: 	// Build the selection vector for the partitions
90: 	BuildPartitionSel(state, *FlatVector::IncrementalSelectionVector(), append_count);
91: 
92: 	// Early out: check if everything belongs to a single partition
93: 	auto partition_index = state.GetPartitionIndexIfSinglePartition(UseFixedSizeMap());
94: 	if (partition_index.IsValid()) {
95: 		auto &partition = *partitions[partition_index.GetIndex()];
96: 		auto &partition_pin_state = *state.partition_pin_states[partition_index.GetIndex()];
97: 
98: 		state.chunk_state.heap_sizes.Reference(input.heap_sizes);
99: 
100: 		const auto size_before = partition.SizeInBytes();
101: 		partition.Build(partition_pin_state, state.chunk_state, 0, append_count);
102: 		data_size += partition.SizeInBytes() - size_before;
103: 
104: 		partition.CopyRows(state.chunk_state, input, *FlatVector::IncrementalSelectionVector(), append_count);
105: 	} else {
106: 		// Build the buffer space
107: 		state.chunk_state.heap_sizes.Slice(input.heap_sizes, state.partition_sel, append_count);
108: 		state.chunk_state.heap_sizes.Flatten(append_count);
109: 		BuildBufferSpace(state);
110: 
111: 		// Copy the rows
112: 		partitions[0]->CopyRows(state.chunk_state, input, state.partition_sel, append_count);
113: 	}
114: 
115: 	count += append_count;
116: 	Verify();
117: }
118: 
119: void PartitionedTupleData::BuildPartitionSel(PartitionedTupleDataAppendState &state, const SelectionVector &append_sel,
120:                                              const idx_t append_count) const {
121: 	if (UseFixedSizeMap()) {
122: 		BuildPartitionSel<true>(state, append_sel, append_count);
123: 	} else {
124: 		BuildPartitionSel<false>(state, append_sel, append_count);
125: 	}
126: }
127: 
128: template <bool fixed>
129: void PartitionedTupleData::BuildPartitionSel(PartitionedTupleDataAppendState &state, const SelectionVector &append_sel,
130:                                              const idx_t append_count) {
131: 	using GETTER = TemplatedMapGetter<list_entry_t, fixed>;
132: 	auto &partition_entries = state.GetMap<fixed>();
133: 	const auto partition_indices = FlatVector::GetData<idx_t>(state.partition_indices);
134: 	partition_entries.clear();
135: 	switch (state.partition_indices.GetVectorType()) {
136: 	case VectorType::FLAT_VECTOR:
137: 		for (idx_t i = 0; i < append_count; i++) {
138: 			const auto &partition_index = partition_indices[i];
139: 			auto partition_entry = partition_entries.find(partition_index);
140: 			if (partition_entry == partition_entries.end()) {
141: 				partition_entries[partition_index] = list_entry_t(0, 1);
142: 			} else {
143: 				GETTER::GetValue(partition_entry).length++;
144: 			}
145: 		}
146: 		break;
147: 	case VectorType::CONSTANT_VECTOR:
148: 		partition_entries[partition_indices[0]] = list_entry_t(0, append_count);
149: 		break;
150: 	default:
151: 		throw InternalException("Unexpected VectorType in PartitionedTupleData::Append");
152: 	}
153: 
154: 	// Early out: check if everything belongs to a single partition
155: 	if (partition_entries.size() == 1) {
156: 		// This needs to be initialized, even if we go the short path here
157: 		for (sel_t i = 0; i < append_count; i++) {
158: 			const auto index = append_sel.get_index(i);
159: 			state.reverse_partition_sel[index] = i;
160: 		}
161: 		return;
162: 	}
163: 
164: 	// Compute offsets from the counts
165: 	idx_t offset = 0;
166: 	for (auto it = partition_entries.begin(); it != partition_entries.end(); ++it) {
167: 		auto &partition_entry = GETTER::GetValue(it);
168: 		partition_entry.offset = offset;
169: 		offset += partition_entry.length;
170: 	}
171: 
172: 	// Now initialize a single selection vector that acts as a selection vector for every partition
173: 	auto &partition_sel = state.partition_sel;
174: 	auto &reverse_partition_sel = state.reverse_partition_sel;
175: 	for (idx_t i = 0; i < append_count; i++) {
176: 		const auto index = append_sel.get_index(i);
177: 		const auto &partition_index = partition_indices[i];
178: 		auto &partition_offset = partition_entries[partition_index].offset;
179: 		reverse_partition_sel[index] = UnsafeNumericCast<sel_t>(partition_offset);
180: 		partition_sel[partition_offset++] = UnsafeNumericCast<sel_t>(index);
181: 	}
182: }
183: 
184: void PartitionedTupleData::BuildBufferSpace(PartitionedTupleDataAppendState &state) {
185: 	if (UseFixedSizeMap()) {
186: 		BuildBufferSpace<true>(state);
187: 	} else {
188: 		BuildBufferSpace<false>(state);
189: 	}
190: }
191: 
192: template <bool fixed>
193: void PartitionedTupleData::BuildBufferSpace(PartitionedTupleDataAppendState &state) {
194: 	using GETTER = TemplatedMapGetter<list_entry_t, fixed>;
195: 	const auto &partition_entries = state.GetMap<fixed>();
196: 	for (auto it = partition_entries.begin(); it != partition_entries.end(); ++it) {
197: 		const auto &partition_index = GETTER::GetKey(it);
198: 
199: 		// Partition, pin state for this partition index
200: 		auto &partition = *partitions[partition_index];
201: 		auto &partition_pin_state = *state.partition_pin_states[partition_index];
202: 
203: 		// Length and offset for this partition
204: 		const auto &partition_entry = GETTER::GetValue(it);
205: 		const auto &partition_length = partition_entry.length;
206: 		const auto partition_offset = partition_entry.offset - partition_length;
207: 
208: 		// Build out the buffer space for this partition
209: 		const auto size_before = partition.SizeInBytes();
210: 		partition.Build(partition_pin_state, state.chunk_state, partition_offset, partition_length);
211: 		data_size += partition.SizeInBytes() - size_before;
212: 	}
213: }
214: 
215: void PartitionedTupleData::FlushAppendState(PartitionedTupleDataAppendState &state) {
216: 	for (idx_t partition_index = 0; partition_index < partitions.size(); partition_index++) {
217: 		auto &partition = *partitions[partition_index];
218: 		auto &partition_pin_state = *state.partition_pin_states[partition_index];
219: 		partition.FinalizePinState(partition_pin_state);
220: 	}
221: }
222: 
223: void PartitionedTupleData::Combine(PartitionedTupleData &other) {
224: 	if (other.Count() == 0) {
225: 		return;
226: 	}
227: 
228: 	// Now combine the state's partitions into this
229: 	lock_guard<mutex> guard(lock);
230: 	if (partitions.empty()) {
231: 		// This is the first merge, we just copy them over
232: 		partitions = std::move(other.partitions);
233: 	} else {
234: 		D_ASSERT(partitions.size() == other.partitions.size());
235: 		// Combine the append state's partitions into this PartitionedTupleData
236: 		for (idx_t i = 0; i < other.partitions.size(); i++) {
237: 			partitions[i]->Combine(*other.partitions[i]);
238: 		}
239: 	}
240: 	this->count += other.count;
241: 	this->data_size += other.data_size;
242: 	Verify();
243: }
244: 
245: void PartitionedTupleData::Reset() {
246: 	for (auto &partition : partitions) {
247: 		partition->Reset();
248: 	}
249: 	this->count = 0;
250: 	this->data_size = 0;
251: 	Verify();
252: }
253: 
254: void PartitionedTupleData::Repartition(PartitionedTupleData &new_partitioned_data) {
255: 	D_ASSERT(layout.GetTypes() == new_partitioned_data.layout.GetTypes());
256: 
257: 	if (partitions.size() == new_partitioned_data.partitions.size()) {
258: 		new_partitioned_data.Combine(*this);
259: 		return;
260: 	}
261: 
262: 	PartitionedTupleDataAppendState append_state;
263: 	new_partitioned_data.InitializeAppendState(append_state);
264: 
265: 	const auto reverse = RepartitionReverseOrder();
266: 	const idx_t start_idx = reverse ? partitions.size() : 0;
267: 	const idx_t end_idx = reverse ? 0 : partitions.size();
268: 	const int64_t update = reverse ? -1 : 1;
269: 	const int64_t adjustment = reverse ? -1 : 0;
270: 
271: 	for (idx_t partition_idx = start_idx; partition_idx != end_idx; partition_idx += idx_t(update)) {
272: 		auto actual_partition_idx = partition_idx + idx_t(adjustment);
273: 		auto &partition = *partitions[actual_partition_idx];
274: 
275: 		if (partition.Count() > 0) {
276: 			TupleDataChunkIterator iterator(partition, TupleDataPinProperties::DESTROY_AFTER_DONE, true);
277: 			auto &chunk_state = iterator.GetChunkState();
278: 			do {
279: 				new_partitioned_data.Append(append_state, chunk_state, iterator.GetCurrentChunkCount());
280: 			} while (iterator.Next());
281: 
282: 			RepartitionFinalizeStates(*this, new_partitioned_data, append_state, actual_partition_idx);
283: 		}
284: 		partitions[actual_partition_idx]->Reset();
285: 	}
286: 	new_partitioned_data.FlushAppendState(append_state);
287: 
288: 	count = 0;
289: 	data_size = 0;
290: 
291: 	Verify();
292: }
293: 
294: void PartitionedTupleData::Unpin() {
295: 	for (auto &partition : partitions) {
296: 		partition->Unpin();
297: 	}
298: }
299: 
300: unsafe_vector<unique_ptr<TupleDataCollection>> &PartitionedTupleData::GetPartitions() {
301: 	return partitions;
302: }
303: 
304: unique_ptr<TupleDataCollection> PartitionedTupleData::GetUnpartitioned() {
305: 	auto data_collection = std::move(partitions[0]);
306: 	partitions[0] = make_uniq<TupleDataCollection>(buffer_manager, layout);
307: 
308: 	for (idx_t i = 1; i < partitions.size(); i++) {
309: 		data_collection->Combine(*partitions[i]);
310: 	}
311: 	count = 0;
312: 	data_size = 0;
313: 
314: 	data_collection->Verify();
315: 	Verify();
316: 
317: 	return data_collection;
318: }
319: 
320: idx_t PartitionedTupleData::Count() const {
321: 	return count;
322: }
323: 
324: idx_t PartitionedTupleData::SizeInBytes() const {
325: 	idx_t total_size = 0;
326: 	for (auto &partition : partitions) {
327: 		total_size += partition->SizeInBytes();
328: 	}
329: 	return total_size;
330: }
331: 
332: idx_t PartitionedTupleData::PartitionCount() const {
333: 	return partitions.size();
334: }
335: 
336: void PartitionedTupleData::GetSizesAndCounts(vector<idx_t> &partition_sizes, vector<idx_t> &partition_counts) const {
337: 	D_ASSERT(partition_sizes.size() == PartitionCount());
338: 	D_ASSERT(partition_sizes.size() == partition_counts.size());
339: 	for (idx_t i = 0; i < PartitionCount(); i++) {
340: 		auto &partition = *partitions[i];
341: 		partition_sizes[i] += partition.SizeInBytes();
342: 		partition_counts[i] += partition.Count();
343: 	}
344: }
345: 
346: void PartitionedTupleData::Verify() const {
347: #ifdef DEBUG
348: 	idx_t total_count = 0;
349: 	idx_t total_size = 0;
350: 	for (auto &partition : partitions) {
351: 		partition->Verify();
352: 		total_count += partition->Count();
353: 		total_size += partition->SizeInBytes();
354: 	}
355: 	D_ASSERT(total_count == this->count);
356: 	D_ASSERT(total_size == this->data_size);
357: #endif
358: }
359: 
360: // LCOV_EXCL_START
361: string PartitionedTupleData::ToString() {
362: 	string result =
363: 	    StringUtil::Format("PartitionedTupleData - [%llu Partitions, %llu Rows]\n", partitions.size(), Count());
364: 	for (idx_t partition_idx = 0; partition_idx < partitions.size(); partition_idx++) {
365: 		result += StringUtil::Format("Partition %llu: ", partition_idx) + partitions[partition_idx]->ToString();
366: 	}
367: 	return result;
368: }
369: 
370: void PartitionedTupleData::Print() {
371: 	Printer::Print(ToString());
372: }
373: // LCOV_EXCL_STOP
374: 
375: void PartitionedTupleData::CreateAllocator() {
376: 	allocators->allocators.emplace_back(make_shared_ptr<TupleDataAllocator>(buffer_manager, layout));
377: }
378: 
379: } // namespace duckdb
[end of src/common/types/row/partitioned_tuple_data.cpp]
[start of src/common/types/row/tuple_data_allocator.cpp]
1: #include "duckdb/common/types/row/tuple_data_allocator.hpp"
2: 
3: #include "duckdb/common/fast_mem.hpp"
4: #include "duckdb/common/types/row/tuple_data_segment.hpp"
5: #include "duckdb/common/types/row/tuple_data_states.hpp"
6: #include "duckdb/storage/buffer/block_handle.hpp"
7: #include "duckdb/storage/buffer_manager.hpp"
8: 
9: namespace duckdb {
10: 
11: using ValidityBytes = TupleDataLayout::ValidityBytes;
12: 
13: TupleDataBlock::TupleDataBlock(BufferManager &buffer_manager, idx_t capacity_p) : capacity(capacity_p), size(0) {
14: 	auto buffer_handle = buffer_manager.Allocate(MemoryTag::HASH_TABLE, capacity, false);
15: 	handle = buffer_handle.GetBlockHandle();
16: }
17: 
18: TupleDataBlock::TupleDataBlock(TupleDataBlock &&other) noexcept : capacity(0), size(0) {
19: 	std::swap(handle, other.handle);
20: 	std::swap(capacity, other.capacity);
21: 	std::swap(size, other.size);
22: }
23: 
24: TupleDataBlock &TupleDataBlock::operator=(TupleDataBlock &&other) noexcept {
25: 	std::swap(handle, other.handle);
26: 	std::swap(capacity, other.capacity);
27: 	std::swap(size, other.size);
28: 	return *this;
29: }
30: 
31: TupleDataAllocator::TupleDataAllocator(BufferManager &buffer_manager, const TupleDataLayout &layout)
32:     : buffer_manager(buffer_manager), layout(layout.Copy()) {
33: }
34: 
35: TupleDataAllocator::TupleDataAllocator(TupleDataAllocator &allocator)
36:     : buffer_manager(allocator.buffer_manager), layout(allocator.layout.Copy()) {
37: }
38: 
39: void TupleDataAllocator::SetDestroyBufferUponUnpin() {
40: 	for (auto &block : row_blocks) {
41: 		if (block.handle) {
42: 			block.handle->SetDestroyBufferUpon(DestroyBufferUpon::UNPIN);
43: 		}
44: 	}
45: 	for (auto &block : heap_blocks) {
46: 		if (block.handle) {
47: 			block.handle->SetDestroyBufferUpon(DestroyBufferUpon::UNPIN);
48: 		}
49: 	}
50: }
51: 
52: TupleDataAllocator::~TupleDataAllocator() {
53: 	SetDestroyBufferUponUnpin();
54: }
55: 
56: BufferManager &TupleDataAllocator::GetBufferManager() {
57: 	return buffer_manager;
58: }
59: 
60: Allocator &TupleDataAllocator::GetAllocator() {
61: 	return buffer_manager.GetBufferAllocator();
62: }
63: 
64: const TupleDataLayout &TupleDataAllocator::GetLayout() const {
65: 	return layout;
66: }
67: 
68: idx_t TupleDataAllocator::RowBlockCount() const {
69: 	return row_blocks.size();
70: }
71: 
72: idx_t TupleDataAllocator::HeapBlockCount() const {
73: 	return heap_blocks.size();
74: }
75: 
76: void TupleDataAllocator::Build(TupleDataSegment &segment, TupleDataPinState &pin_state,
77:                                TupleDataChunkState &chunk_state, const idx_t append_offset, const idx_t append_count) {
78: 	D_ASSERT(this == segment.allocator.get());
79: 	auto &chunks = segment.chunks;
80: 	if (!chunks.empty()) {
81: 		ReleaseOrStoreHandles(pin_state, segment, chunks.back(), true);
82: 	}
83: 
84: 	// Build the chunk parts for the incoming data
85: 	chunk_part_indices.clear();
86: 	idx_t offset = 0;
87: 	while (offset != append_count) {
88: 		if (chunks.empty() || chunks.back().count == STANDARD_VECTOR_SIZE) {
89: 			chunks.emplace_back();
90: 		}
91: 		auto &chunk = chunks.back();
92: 
93: 		// Build the next part
94: 		auto next = MinValue<idx_t>(append_count - offset, STANDARD_VECTOR_SIZE - chunk.count);
95: 		chunk.AddPart(BuildChunkPart(pin_state, chunk_state, append_offset + offset, next, chunk), layout);
96: 		auto &chunk_part = chunk.parts.back();
97: 		next = chunk_part.count;
98: 
99: 		segment.count += next;
100: 		segment.data_size += chunk_part.count * layout.GetRowWidth();
101: 		if (!layout.AllConstant()) {
102: 			segment.data_size += chunk_part.total_heap_size;
103: 		}
104: 
105: 		if (layout.HasDestructor()) {
106: 			const auto base_row_ptr = GetRowPointer(pin_state, chunk_part);
107: 			for (auto &aggr_idx : layout.GetAggregateDestructorIndices()) {
108: 				const auto aggr_offset = layout.GetOffsets()[layout.ColumnCount() + aggr_idx];
109: 				auto &aggr_fun = layout.GetAggregates()[aggr_idx];
110: 				for (idx_t i = 0; i < next; i++) {
111: 					duckdb::FastMemset(base_row_ptr + i * layout.GetRowWidth() + aggr_offset, '\0',
112: 					                   aggr_fun.payload_size);
113: 				}
114: 			}
115: 		}
116: 
117: 		offset += next;
118: 		chunk_part_indices.emplace_back(chunks.size() - 1, chunk.parts.size() - 1);
119: 	}
120: 
121: 	// Now initialize the pointers to write the data to
122: 	chunk_parts.clear();
123: 	for (auto &indices : chunk_part_indices) {
124: 		chunk_parts.emplace_back(segment.chunks[indices.first].parts[indices.second]);
125: 	}
126: 	InitializeChunkStateInternal(pin_state, chunk_state, append_offset, false, true, false, chunk_parts);
127: 
128: 	// To reduce metadata, we try to merge chunk parts where possible
129: 	// Due to the way chunk parts are constructed, only the last part of the first chunk is eligible for merging
130: 	segment.chunks[chunk_part_indices[0].first].MergeLastChunkPart(layout);
131: 
132: 	segment.Verify();
133: }
134: 
135: TupleDataChunkPart TupleDataAllocator::BuildChunkPart(TupleDataPinState &pin_state, TupleDataChunkState &chunk_state,
136:                                                       const idx_t append_offset, const idx_t append_count,
137:                                                       TupleDataChunk &chunk) {
138: 	D_ASSERT(append_count != 0);
139: 	TupleDataChunkPart result(*chunk.lock);
140: 	const auto block_size = buffer_manager.GetBlockSize();
141: 
142: 	// Allocate row block (if needed)
143: 	if (row_blocks.empty() || row_blocks.back().RemainingCapacity() < layout.GetRowWidth()) {
144: 		row_blocks.emplace_back(buffer_manager, block_size);
145: 	}
146: 	result.row_block_index = NumericCast<uint32_t>(row_blocks.size() - 1);
147: 	auto &row_block = row_blocks[result.row_block_index];
148: 	result.row_block_offset = NumericCast<uint32_t>(row_block.size);
149: 
150: 	// Set count (might be reduced later when checking heap space)
151: 	result.count = NumericCast<uint32_t>(MinValue(row_block.RemainingCapacity(layout.GetRowWidth()), append_count));
152: 	if (!layout.AllConstant()) {
153: 		const auto heap_sizes = FlatVector::GetData<idx_t>(chunk_state.heap_sizes);
154: 
155: 		// Compute total heap size first
156: 		idx_t total_heap_size = 0;
157: 		for (idx_t i = 0; i < result.count; i++) {
158: 			const auto &heap_size = heap_sizes[append_offset + i];
159: 			total_heap_size += heap_size;
160: 		}
161: 
162: 		if (total_heap_size == 0) {
163: 			result.SetHeapEmpty();
164: 		} else {
165: 			const auto heap_remaining = MaxValue<idx_t>(
166: 			    heap_blocks.empty() ? block_size : heap_blocks.back().RemainingCapacity(), heap_sizes[append_offset]);
167: 
168: 			if (total_heap_size <= heap_remaining) {
169: 				// Everything fits
170: 				result.total_heap_size = NumericCast<uint32_t>(total_heap_size);
171: 			} else {
172: 				// Not everything fits - determine how many we can read next
173: 				result.total_heap_size = 0;
174: 				for (idx_t i = 0; i < result.count; i++) {
175: 					const auto &heap_size = heap_sizes[append_offset + i];
176: 					if (result.total_heap_size + heap_size > heap_remaining) {
177: 						result.count = NumericCast<uint32_t>(i);
178: 						break;
179: 					}
180: 					result.total_heap_size += heap_size;
181: 				}
182: 			}
183: 
184: 			if (result.total_heap_size == 0) {
185: 				result.SetHeapEmpty();
186: 			} else {
187: 				// Allocate heap block (if needed)
188: 				if (heap_blocks.empty() || heap_blocks.back().RemainingCapacity() < heap_sizes[append_offset]) {
189: 					const auto size = MaxValue<idx_t>(block_size, heap_sizes[append_offset]);
190: 					heap_blocks.emplace_back(buffer_manager, size);
191: 				}
192: 				result.heap_block_index = NumericCast<uint32_t>(heap_blocks.size() - 1);
193: 				auto &heap_block = heap_blocks[result.heap_block_index];
194: 				result.heap_block_offset = NumericCast<uint32_t>(heap_block.size);
195: 
196: 				// Mark this portion of the heap block as filled and set the pointer
197: 				heap_block.size += result.total_heap_size;
198: 				result.base_heap_ptr = GetBaseHeapPointer(pin_state, result);
199: 			}
200: 		}
201: 	}
202: 	D_ASSERT(result.count != 0 && result.count <= STANDARD_VECTOR_SIZE);
203: 
204: 	// Mark this portion of the row block as filled
205: 	row_block.size += result.count * layout.GetRowWidth();
206: 
207: 	return result;
208: }
209: 
210: void TupleDataAllocator::InitializeChunkState(TupleDataSegment &segment, TupleDataPinState &pin_state,
211:                                               TupleDataChunkState &chunk_state, idx_t chunk_idx, bool init_heap) {
212: 	D_ASSERT(this == segment.allocator.get());
213: 	D_ASSERT(chunk_idx < segment.ChunkCount());
214: 	auto &chunk = segment.chunks[chunk_idx];
215: 
216: 	// Release or store any handles that are no longer required:
217: 	// We can't release the heap here if the current chunk's heap_block_ids is empty, because if we are iterating with
218: 	// PinProperties::DESTROY_AFTER_DONE, we might destroy a heap block that is needed by a later chunk, e.g.,
219: 	// when chunk 0 needs heap block 0, chunk 1 does not need any heap blocks, and chunk 2 needs heap block 0 again
220: 	ReleaseOrStoreHandles(pin_state, segment, chunk, !chunk.heap_block_ids.empty());
221: 
222: 	unsafe_vector<reference<TupleDataChunkPart>> parts;
223: 	parts.reserve(chunk.parts.size());
224: 	for (auto &part : chunk.parts) {
225: 		parts.emplace_back(part);
226: 	}
227: 
228: 	InitializeChunkStateInternal(pin_state, chunk_state, 0, true, init_heap, init_heap, parts);
229: }
230: 
231: static inline void InitializeHeapSizes(const data_ptr_t row_locations[], idx_t heap_sizes[], const idx_t offset,
232:                                        const idx_t next, const TupleDataChunkPart &part, const idx_t heap_size_offset) {
233: 	// Read the heap sizes from the rows
234: 	for (idx_t i = 0; i < next; i++) {
235: 		auto idx = offset + i;
236: 		heap_sizes[idx] = Load<uint32_t>(row_locations[idx] + heap_size_offset);
237: 	}
238: 
239: 	// Verify total size
240: #ifdef DEBUG
241: 	idx_t total_heap_size = 0;
242: 	for (idx_t i = 0; i < next; i++) {
243: 		auto idx = offset + i;
244: 		total_heap_size += heap_sizes[idx];
245: 	}
246: 	D_ASSERT(total_heap_size == part.total_heap_size);
247: #endif
248: }
249: 
250: void TupleDataAllocator::InitializeChunkStateInternal(TupleDataPinState &pin_state, TupleDataChunkState &chunk_state,
251:                                                       idx_t offset, bool recompute, bool init_heap_pointers,
252:                                                       bool init_heap_sizes,
253:                                                       unsafe_vector<reference<TupleDataChunkPart>> &parts) {
254: 	auto row_locations = FlatVector::GetData<data_ptr_t>(chunk_state.row_locations);
255: 	auto heap_sizes = FlatVector::GetData<idx_t>(chunk_state.heap_sizes);
256: 	auto heap_locations = FlatVector::GetData<data_ptr_t>(chunk_state.heap_locations);
257: 
258: 	for (auto &part_ref : parts) {
259: 		auto &part = part_ref.get();
260: 		const auto next = part.count;
261: 
262: 		// Set up row locations for the scan
263: 		const auto row_width = layout.GetRowWidth();
264: 		const auto base_row_ptr = GetRowPointer(pin_state, part);
265: 		for (idx_t i = 0; i < next; i++) {
266: 			row_locations[offset + i] = base_row_ptr + i * row_width;
267: 		}
268: 
269: 		if (layout.AllConstant()) { // Can't have a heap
270: 			offset += next;
271: 			continue;
272: 		}
273: 
274: 		if (part.total_heap_size == 0) {
275: 			if (init_heap_sizes) { // No heap, but we need the heap sizes
276: 				InitializeHeapSizes(row_locations, heap_sizes, offset, next, part, layout.GetHeapSizeOffset());
277: 			}
278: 			offset += next;
279: 			continue;
280: 		}
281: 
282: 		// Check if heap block has changed - re-compute the pointers within each row if so
283: 		if (recompute && pin_state.properties != TupleDataPinProperties::ALREADY_PINNED) {
284: 			const auto new_base_heap_ptr = GetBaseHeapPointer(pin_state, part);
285: 			if (part.base_heap_ptr != new_base_heap_ptr) {
286: 				lock_guard<mutex> guard(part.lock);
287: 				const auto old_base_heap_ptr = part.base_heap_ptr;
288: 				if (old_base_heap_ptr != new_base_heap_ptr) {
289: 					Vector old_heap_ptrs(
290: 					    Value::POINTER(CastPointerToValue(old_base_heap_ptr + part.heap_block_offset)));
291: 					Vector new_heap_ptrs(
292: 					    Value::POINTER(CastPointerToValue(new_base_heap_ptr + part.heap_block_offset)));
293: 					RecomputeHeapPointers(old_heap_ptrs, *ConstantVector::ZeroSelectionVector(), row_locations,
294: 					                      new_heap_ptrs, offset, next, layout, 0);
295: 					part.base_heap_ptr = new_base_heap_ptr;
296: 				}
297: 			}
298: 		}
299: 
300: 		if (init_heap_sizes) {
301: 			InitializeHeapSizes(row_locations, heap_sizes, offset, next, part, layout.GetHeapSizeOffset());
302: 		}
303: 
304: 		if (init_heap_pointers) {
305: 			// Set the pointers where the heap data will be written (if needed)
306: 			heap_locations[offset] = part.base_heap_ptr + part.heap_block_offset;
307: 			for (idx_t i = 1; i < next; i++) {
308: 				auto idx = offset + i;
309: 				heap_locations[idx] = heap_locations[idx - 1] + heap_sizes[idx - 1];
310: 			}
311: 		}
312: 
313: 		offset += next;
314: 	}
315: 	D_ASSERT(offset <= STANDARD_VECTOR_SIZE);
316: }
317: 
318: static inline void VerifyStrings(const LogicalTypeId type_id, const data_ptr_t row_locations[], const idx_t col_idx,
319:                                  const idx_t base_col_offset, const idx_t col_offset, const idx_t offset,
320:                                  const idx_t count) {
321: #ifdef DEBUG
322: 	if (type_id != LogicalTypeId::VARCHAR) {
323: 		// Make sure we don't verify BLOB / AGGREGATE_STATE
324: 		return;
325: 	}
326: 	idx_t entry_idx;
327: 	idx_t idx_in_entry;
328: 	ValidityBytes::GetEntryIndex(col_idx, entry_idx, idx_in_entry);
329: 	for (idx_t i = 0; i < count; i++) {
330: 		const auto &row_location = row_locations[offset + i] + base_col_offset;
331: 		ValidityBytes row_mask(row_location);
332: 		if (row_mask.RowIsValid(row_mask.GetValidityEntryUnsafe(entry_idx), idx_in_entry)) {
333: 			auto recomputed_string = Load<string_t>(row_location + col_offset);
334: 			recomputed_string.Verify();
335: 		}
336: 	}
337: #endif
338: }
339: 
340: void TupleDataAllocator::RecomputeHeapPointers(Vector &old_heap_ptrs, const SelectionVector &old_heap_sel,
341:                                                const data_ptr_t row_locations[], Vector &new_heap_ptrs,
342:                                                const idx_t offset, const idx_t count, const TupleDataLayout &layout,
343:                                                const idx_t base_col_offset) {
344: 	const auto old_heap_locations = FlatVector::GetData<data_ptr_t>(old_heap_ptrs);
345: 
346: 	UnifiedVectorFormat new_heap_data;
347: 	new_heap_ptrs.ToUnifiedFormat(offset + count, new_heap_data);
348: 	const auto new_heap_locations = UnifiedVectorFormat::GetData<data_ptr_t>(new_heap_data);
349: 	const auto new_heap_sel = *new_heap_data.sel;
350: 
351: 	for (idx_t col_idx = 0; col_idx < layout.ColumnCount(); col_idx++) {
352: 		const auto &col_offset = layout.GetOffsets()[col_idx];
353: 
354: 		// Precompute mask indexes
355: 		idx_t entry_idx;
356: 		idx_t idx_in_entry;
357: 		ValidityBytes::GetEntryIndex(col_idx, entry_idx, idx_in_entry);
358: 
359: 		const auto &type = layout.GetTypes()[col_idx];
360: 		switch (type.InternalType()) {
361: 		case PhysicalType::VARCHAR: {
362: 			for (idx_t i = 0; i < count; i++) {
363: 				const auto idx = offset + i;
364: 				const auto &row_location = row_locations[idx] + base_col_offset;
365: 				ValidityBytes row_mask(row_location);
366: 				if (!row_mask.RowIsValid(row_mask.GetValidityEntryUnsafe(entry_idx), idx_in_entry)) {
367: 					continue;
368: 				}
369: 
370: 				const auto &old_heap_ptr = old_heap_locations[old_heap_sel.get_index(idx)];
371: 				const auto &new_heap_ptr = new_heap_locations[new_heap_sel.get_index(idx)];
372: 
373: 				const auto string_location = row_location + col_offset;
374: 				if (Load<uint32_t>(string_location) > string_t::INLINE_LENGTH) {
375: 					const auto string_ptr_location = string_location + string_t::HEADER_SIZE;
376: 					const auto string_ptr = Load<data_ptr_t>(string_ptr_location);
377: 					const auto diff = string_ptr - old_heap_ptr;
378: 					D_ASSERT(diff >= 0);
379: 					Store<data_ptr_t>(new_heap_ptr + diff, string_ptr_location);
380: 				}
381: 			}
382: 			VerifyStrings(type.id(), row_locations, col_idx, base_col_offset, col_offset, offset, count);
383: 			break;
384: 		}
385: 		case PhysicalType::LIST:
386: 		case PhysicalType::ARRAY: {
387: 			for (idx_t i = 0; i < count; i++) {
388: 				const auto idx = offset + i;
389: 				const auto &row_location = row_locations[idx] + base_col_offset;
390: 				ValidityBytes row_mask(row_location);
391: 				if (!row_mask.RowIsValid(row_mask.GetValidityEntryUnsafe(entry_idx), idx_in_entry)) {
392: 					continue;
393: 				}
394: 
395: 				const auto &old_heap_ptr = old_heap_locations[old_heap_sel.get_index(idx)];
396: 				const auto &new_heap_ptr = new_heap_locations[new_heap_sel.get_index(idx)];
397: 
398: 				const auto &list_ptr_location = row_location + col_offset;
399: 				const auto list_ptr = Load<data_ptr_t>(list_ptr_location);
400: 				const auto diff = list_ptr - old_heap_ptr;
401: 				D_ASSERT(diff >= 0);
402: 				Store<data_ptr_t>(new_heap_ptr + diff, list_ptr_location);
403: 			}
404: 			break;
405: 		}
406: 		case PhysicalType::STRUCT: {
407: 			const auto &struct_layout = layout.GetStructLayout(col_idx);
408: 			if (!struct_layout.AllConstant()) {
409: 				RecomputeHeapPointers(old_heap_ptrs, old_heap_sel, row_locations, new_heap_ptrs, offset, count,
410: 				                      struct_layout, base_col_offset + col_offset);
411: 			}
412: 			break;
413: 		}
414: 		default:
415: 			continue;
416: 		}
417: 	}
418: }
419: 
420: void TupleDataAllocator::ReleaseOrStoreHandles(TupleDataPinState &pin_state, TupleDataSegment &segment,
421:                                                TupleDataChunk &chunk, bool release_heap) {
422: 	D_ASSERT(this == segment.allocator.get());
423: 	ReleaseOrStoreHandlesInternal(segment, segment.pinned_row_handles, pin_state.row_handles, chunk.row_block_ids,
424: 	                              row_blocks, pin_state.properties);
425: 	if (!layout.AllConstant() && release_heap) {
426: 		ReleaseOrStoreHandlesInternal(segment, segment.pinned_heap_handles, pin_state.heap_handles,
427: 		                              chunk.heap_block_ids, heap_blocks, pin_state.properties);
428: 	}
429: }
430: 
431: void TupleDataAllocator::ReleaseOrStoreHandles(TupleDataPinState &pin_state, TupleDataSegment &segment) {
432: 	static TupleDataChunk DUMMY_CHUNK;
433: 	ReleaseOrStoreHandles(pin_state, segment, DUMMY_CHUNK, true);
434: }
435: 
436: void TupleDataAllocator::ReleaseOrStoreHandlesInternal(
437:     TupleDataSegment &segment, unsafe_vector<BufferHandle> &pinned_handles, perfect_map_t<BufferHandle> &handles,
438:     const perfect_set_t &block_ids, unsafe_vector<TupleDataBlock> &blocks, TupleDataPinProperties properties) {
439: 	bool found_handle;
440: 	do {
441: 		found_handle = false;
442: 		for (auto it = handles.begin(); it != handles.end(); it++) {
443: 			const auto block_id = it->first;
444: 			if (block_ids.find(block_id) != block_ids.end()) {
445: 				// still required: do not release
446: 				continue;
447: 			}
448: 			switch (properties) {
449: 			case TupleDataPinProperties::KEEP_EVERYTHING_PINNED: {
450: 				lock_guard<mutex> guard(segment.pinned_handles_lock);
451: 				const auto block_count = block_id + 1;
452: 				if (block_count > pinned_handles.size()) {
453: 					pinned_handles.resize(block_count);
454: 				}
455: 				pinned_handles[block_id] = std::move(it->second);
456: 				break;
457: 			}
458: 			case TupleDataPinProperties::UNPIN_AFTER_DONE:
459: 			case TupleDataPinProperties::ALREADY_PINNED:
460: 				break;
461: 			case TupleDataPinProperties::DESTROY_AFTER_DONE:
462: 				// Prevent it from being added to the eviction queue
463: 				blocks[block_id].handle->SetDestroyBufferUpon(DestroyBufferUpon::UNPIN);
464: 				// Destroy
465: 				blocks[block_id].handle.reset();
466: 				break;
467: 			default:
468: 				D_ASSERT(properties == TupleDataPinProperties::INVALID);
469: 				throw InternalException("Encountered TupleDataPinProperties::INVALID");
470: 			}
471: 			handles.erase(it);
472: 			found_handle = true;
473: 			break;
474: 		}
475: 	} while (found_handle);
476: }
477: 
478: BufferHandle &TupleDataAllocator::PinRowBlock(TupleDataPinState &pin_state, const TupleDataChunkPart &part) {
479: 	const auto &row_block_index = part.row_block_index;
480: 	auto it = pin_state.row_handles.find(row_block_index);
481: 	if (it == pin_state.row_handles.end()) {
482: 		D_ASSERT(row_block_index < row_blocks.size());
483: 		auto &row_block = row_blocks[row_block_index];
484: 		D_ASSERT(row_block.handle);
485: 		D_ASSERT(part.row_block_offset < row_block.size);
486: 		D_ASSERT(part.row_block_offset + part.count * layout.GetRowWidth() <= row_block.size);
487: 		it = pin_state.row_handles.emplace(row_block_index, buffer_manager.Pin(row_block.handle)).first;
488: 	}
489: 	return it->second;
490: }
491: 
492: BufferHandle &TupleDataAllocator::PinHeapBlock(TupleDataPinState &pin_state, const TupleDataChunkPart &part) {
493: 	const auto &heap_block_index = part.heap_block_index;
494: 	auto it = pin_state.heap_handles.find(heap_block_index);
495: 	if (it == pin_state.heap_handles.end()) {
496: 		D_ASSERT(heap_block_index < heap_blocks.size());
497: 		auto &heap_block = heap_blocks[heap_block_index];
498: 		D_ASSERT(heap_block.handle);
499: 		D_ASSERT(part.heap_block_offset < heap_block.size);
500: 		D_ASSERT(part.heap_block_offset + part.total_heap_size <= heap_block.size);
501: 		it = pin_state.heap_handles.emplace(heap_block_index, buffer_manager.Pin(heap_block.handle)).first;
502: 	}
503: 	return it->second;
504: }
505: 
506: data_ptr_t TupleDataAllocator::GetRowPointer(TupleDataPinState &pin_state, const TupleDataChunkPart &part) {
507: 	return PinRowBlock(pin_state, part).Ptr() + part.row_block_offset;
508: }
509: 
510: data_ptr_t TupleDataAllocator::GetBaseHeapPointer(TupleDataPinState &pin_state, const TupleDataChunkPart &part) {
511: 	return PinHeapBlock(pin_state, part).Ptr();
512: }
513: 
514: } // namespace duckdb
[end of src/common/types/row/tuple_data_allocator.cpp]
[start of src/common/types/row/tuple_data_collection.cpp]
1: #include "duckdb/common/types/row/tuple_data_collection.hpp"
2: 
3: #include "duckdb/common/fast_mem.hpp"
4: #include "duckdb/common/printer.hpp"
5: #include "duckdb/common/row_operations/row_operations.hpp"
6: #include "duckdb/common/type_visitor.hpp"
7: #include "duckdb/common/types/row/tuple_data_allocator.hpp"
8: 
9: #include <algorithm>
10: 
11: namespace duckdb {
12: 
13: using ValidityBytes = TupleDataLayout::ValidityBytes;
14: 
15: TupleDataCollection::TupleDataCollection(BufferManager &buffer_manager, const TupleDataLayout &layout_p)
16:     : layout(layout_p.Copy()), allocator(make_shared_ptr<TupleDataAllocator>(buffer_manager, layout)) {
17: 	Initialize();
18: }
19: 
20: TupleDataCollection::TupleDataCollection(shared_ptr<TupleDataAllocator> allocator)
21:     : layout(allocator->GetLayout().Copy()), allocator(std::move(allocator)) {
22: 	Initialize();
23: }
24: 
25: TupleDataCollection::~TupleDataCollection() {
26: }
27: 
28: void TupleDataCollection::Initialize() {
29: 	D_ASSERT(!layout.GetTypes().empty());
30: 	this->count = 0;
31: 	this->data_size = 0;
32: 	scatter_functions.reserve(layout.ColumnCount());
33: 	gather_functions.reserve(layout.ColumnCount());
34: 	for (idx_t col_idx = 0; col_idx < layout.ColumnCount(); col_idx++) {
35: 		auto &type = layout.GetTypes()[col_idx];
36: 		scatter_functions.emplace_back(GetScatterFunction(type));
37: 		gather_functions.emplace_back(GetGatherFunction(type));
38: 	}
39: }
40: 
41: void GetAllColumnIDsInternal(vector<column_t> &column_ids, const idx_t column_count) {
42: 	column_ids.reserve(column_count);
43: 	for (idx_t col_idx = 0; col_idx < column_count; col_idx++) {
44: 		column_ids.emplace_back(col_idx);
45: 	}
46: }
47: 
48: void TupleDataCollection::GetAllColumnIDs(vector<column_t> &column_ids) {
49: 	GetAllColumnIDsInternal(column_ids, layout.ColumnCount());
50: }
51: 
52: const TupleDataLayout &TupleDataCollection::GetLayout() const {
53: 	return layout;
54: }
55: 
56: const idx_t &TupleDataCollection::Count() const {
57: 	return count;
58: }
59: 
60: idx_t TupleDataCollection::ChunkCount() const {
61: 	idx_t total_chunk_count = 0;
62: 	for (const auto &segment : segments) {
63: 		total_chunk_count += segment.ChunkCount();
64: 	}
65: 	return total_chunk_count;
66: }
67: 
68: idx_t TupleDataCollection::SizeInBytes() const {
69: 	idx_t total_size = 0;
70: 	for (const auto &segment : segments) {
71: 		total_size += segment.SizeInBytes();
72: 	}
73: 	return total_size;
74: }
75: 
76: void TupleDataCollection::Unpin() {
77: 	for (auto &segment : segments) {
78: 		segment.Unpin();
79: 	}
80: }
81: 
82: // LCOV_EXCL_START
83: void VerifyAppendColumns(const TupleDataLayout &layout, const vector<column_t> &column_ids) {
84: #ifdef DEBUG
85: 	for (idx_t col_idx = 0; col_idx < layout.ColumnCount(); col_idx++) {
86: 		if (std::find(column_ids.begin(), column_ids.end(), col_idx) != column_ids.end()) {
87: 			continue;
88: 		}
89: 		// This column will not be appended in the first go - verify that it is fixed-size - we cannot resize heap after
90: 		const auto physical_type = layout.GetTypes()[col_idx].InternalType();
91: 		D_ASSERT(physical_type != PhysicalType::VARCHAR && physical_type != PhysicalType::LIST &&
92: 		         physical_type != PhysicalType::ARRAY);
93: 		if (physical_type == PhysicalType::STRUCT) {
94: 			const auto &struct_layout = layout.GetStructLayout(col_idx);
95: 			vector<column_t> struct_column_ids;
96: 			struct_column_ids.reserve(struct_layout.ColumnCount());
97: 			for (idx_t struct_col_idx = 0; struct_col_idx < struct_layout.ColumnCount(); struct_col_idx++) {
98: 				struct_column_ids.emplace_back(struct_col_idx);
99: 			}
100: 			VerifyAppendColumns(struct_layout, struct_column_ids);
101: 		}
102: 	}
103: #endif
104: }
105: // LCOV_EXCL_STOP
106: 
107: void TupleDataCollection::InitializeAppend(TupleDataAppendState &append_state, TupleDataPinProperties properties) {
108: 	vector<column_t> column_ids;
109: 	GetAllColumnIDs(column_ids);
110: 	InitializeAppend(append_state, std::move(column_ids), properties);
111: }
112: 
113: void TupleDataCollection::InitializeAppend(TupleDataAppendState &append_state, vector<column_t> column_ids,
114:                                            TupleDataPinProperties properties) {
115: 	VerifyAppendColumns(layout, column_ids);
116: 	InitializeAppend(append_state.pin_state, properties);
117: 	InitializeChunkState(append_state.chunk_state, std::move(column_ids));
118: }
119: 
120: void TupleDataCollection::InitializeAppend(TupleDataPinState &pin_state, TupleDataPinProperties properties) {
121: 	pin_state.properties = properties;
122: 	if (segments.empty()) {
123: 		segments.emplace_back(allocator);
124: 	}
125: }
126: 
127: static void InitializeVectorFormat(vector<TupleDataVectorFormat> &vector_data, const vector<LogicalType> &types) {
128: 	vector_data.resize(types.size());
129: 	for (idx_t col_idx = 0; col_idx < types.size(); col_idx++) {
130: 		const auto &type = types[col_idx];
131: 		switch (type.InternalType()) {
132: 		case PhysicalType::STRUCT: {
133: 			const auto &child_list = StructType::GetChildTypes(type);
134: 			vector<LogicalType> child_types;
135: 			child_types.reserve(child_list.size());
136: 			for (const auto &child_entry : child_list) {
137: 				child_types.emplace_back(child_entry.second);
138: 			}
139: 			InitializeVectorFormat(vector_data[col_idx].children, child_types);
140: 			break;
141: 		}
142: 		case PhysicalType::LIST:
143: 			InitializeVectorFormat(vector_data[col_idx].children, {ListType::GetChildType(type)});
144: 			break;
145: 		case PhysicalType::ARRAY:
146: 			InitializeVectorFormat(vector_data[col_idx].children, {ArrayType::GetChildType(type)});
147: 			break;
148: 		default:
149: 			break;
150: 		}
151: 	}
152: }
153: 
154: void TupleDataCollection::InitializeChunkState(TupleDataChunkState &chunk_state, vector<column_t> column_ids) {
155: 	TupleDataCollection::InitializeChunkState(chunk_state, layout.GetTypes(), std::move(column_ids));
156: }
157: 
158: void TupleDataCollection::InitializeChunkState(TupleDataChunkState &chunk_state, const vector<LogicalType> &types,
159:                                                vector<column_t> column_ids) {
160: 	if (column_ids.empty()) {
161: 		GetAllColumnIDsInternal(column_ids, types.size());
162: 	}
163: 	InitializeVectorFormat(chunk_state.vector_data, types);
164: 
165: 	for (auto &col : column_ids) {
166: 		auto &type = types[col];
167: 		if (TypeVisitor::Contains(type, LogicalTypeId::ARRAY)) {
168: 			auto cast_type = ArrayType::ConvertToList(type);
169: 			chunk_state.cached_cast_vector_cache.push_back(
170: 			    make_uniq<VectorCache>(Allocator::DefaultAllocator(), cast_type));
171: 			chunk_state.cached_cast_vectors.push_back(make_uniq<Vector>(*chunk_state.cached_cast_vector_cache.back()));
172: 		} else {
173: 			chunk_state.cached_cast_vectors.emplace_back();
174: 			chunk_state.cached_cast_vector_cache.emplace_back();
175: 		}
176: 	}
177: 
178: 	chunk_state.column_ids = std::move(column_ids);
179: }
180: 
181: void TupleDataCollection::Append(DataChunk &new_chunk, const SelectionVector &append_sel, idx_t append_count) {
182: 	TupleDataAppendState append_state;
183: 	InitializeAppend(append_state);
184: 	Append(append_state, new_chunk, append_sel, append_count);
185: }
186: 
187: void TupleDataCollection::Append(DataChunk &new_chunk, vector<column_t> column_ids, const SelectionVector &append_sel,
188:                                  const idx_t append_count) {
189: 	TupleDataAppendState append_state;
190: 	InitializeAppend(append_state, std::move(column_ids));
191: 	Append(append_state, new_chunk, append_sel, append_count);
192: }
193: 
194: void TupleDataCollection::Append(TupleDataAppendState &append_state, DataChunk &new_chunk,
195:                                  const SelectionVector &append_sel, const idx_t append_count) {
196: 	Append(append_state.pin_state, append_state.chunk_state, new_chunk, append_sel, append_count);
197: }
198: 
199: void TupleDataCollection::Append(TupleDataPinState &pin_state, TupleDataChunkState &chunk_state, DataChunk &new_chunk,
200:                                  const SelectionVector &append_sel, const idx_t append_count) {
201: 	TupleDataCollection::ToUnifiedFormat(chunk_state, new_chunk);
202: 	AppendUnified(pin_state, chunk_state, new_chunk, append_sel, append_count);
203: }
204: 
205: void TupleDataCollection::AppendUnified(TupleDataPinState &pin_state, TupleDataChunkState &chunk_state,
206:                                         DataChunk &new_chunk, const SelectionVector &append_sel,
207:                                         const idx_t append_count) {
208: 	const idx_t actual_append_count = append_count == DConstants::INVALID_INDEX ? new_chunk.size() : append_count;
209: 	if (actual_append_count == 0) {
210: 		return;
211: 	}
212: 
213: 	if (!layout.AllConstant()) {
214: 		TupleDataCollection::ComputeHeapSizes(chunk_state, new_chunk, append_sel, actual_append_count);
215: 	}
216: 
217: 	Build(pin_state, chunk_state, 0, actual_append_count);
218: 	Scatter(chunk_state, new_chunk, append_sel, actual_append_count);
219: }
220: 
221: static inline void ToUnifiedFormatInternal(TupleDataVectorFormat &format, Vector &vector, const idx_t count) {
222: 	vector.ToUnifiedFormat(count, format.unified);
223: 	format.original_sel = format.unified.sel;
224: 	format.original_owned_sel.Initialize(format.unified.owned_sel);
225: 	switch (vector.GetType().InternalType()) {
226: 	case PhysicalType::STRUCT: {
227: 		auto &entries = StructVector::GetEntries(vector);
228: 		D_ASSERT(format.children.size() == entries.size());
229: 		for (idx_t struct_col_idx = 0; struct_col_idx < entries.size(); struct_col_idx++) {
230: 			ToUnifiedFormatInternal(format.children[struct_col_idx], *entries[struct_col_idx], count);
231: 		}
232: 		break;
233: 	}
234: 	case PhysicalType::LIST:
235: 		D_ASSERT(format.children.size() == 1);
236: 		ToUnifiedFormatInternal(format.children[0], ListVector::GetEntry(vector), ListVector::GetListSize(vector));
237: 		break;
238: 	case PhysicalType::ARRAY: {
239: 		D_ASSERT(format.children.size() == 1);
240: 
241: 		// For arrays, we cheat a bit and pretend that they are lists by creating and assigning list_entry_t's to the
242: 		// vector This allows us to reuse all the list serialization functions for array types too.
243: 		auto array_size = ArrayType::GetSize(vector.GetType());
244: 
245: 		// How many list_entry_t's do we need to cover the whole child array?
246: 		// Make sure we round up so its all covered
247: 		auto child_array_total_size = ArrayVector::GetTotalSize(vector);
248: 		auto list_entry_t_count =
249: 		    MaxValue((child_array_total_size + array_size) / array_size, format.unified.validity.TargetCount());
250: 
251: 		// Create list entries!
252: 		format.array_list_entries = make_unsafe_uniq_array<list_entry_t>(list_entry_t_count);
253: 		for (idx_t i = 0; i < list_entry_t_count; i++) {
254: 			format.array_list_entries[i].length = array_size;
255: 			format.array_list_entries[i].offset = i * array_size;
256: 		}
257: 		format.unified.data = reinterpret_cast<data_ptr_t>(format.array_list_entries.get());
258: 
259: 		ToUnifiedFormatInternal(format.children[0], ArrayVector::GetEntry(vector), child_array_total_size);
260: 		break;
261: 	}
262: 	default:
263: 		break;
264: 	}
265: }
266: 
267: void TupleDataCollection::ToUnifiedFormat(TupleDataChunkState &chunk_state, DataChunk &new_chunk) {
268: 	D_ASSERT(chunk_state.vector_data.size() >= chunk_state.column_ids.size()); // Needs InitializeAppend
269: 	for (const auto &col_idx : chunk_state.column_ids) {
270: 		ToUnifiedFormatInternal(chunk_state.vector_data[col_idx], new_chunk.data[col_idx], new_chunk.size());
271: 	}
272: }
273: 
274: void TupleDataCollection::GetVectorData(const TupleDataChunkState &chunk_state, UnifiedVectorFormat result[]) {
275: 	const auto &vector_data = chunk_state.vector_data;
276: 	for (idx_t i = 0; i < vector_data.size(); i++) {
277: 		const auto &source = vector_data[i].unified;
278: 		auto &target = result[i];
279: 		target.sel = source.sel;
280: 		target.data = source.data;
281: 		target.validity = source.validity;
282: 	}
283: }
284: 
285: void TupleDataCollection::Build(TupleDataPinState &pin_state, TupleDataChunkState &chunk_state,
286:                                 const idx_t append_offset, const idx_t append_count) {
287: 	auto &segment = segments.back();
288: 	const auto size_before = segment.SizeInBytes();
289: 	segment.allocator->Build(segment, pin_state, chunk_state, append_offset, append_count);
290: 	data_size += segment.SizeInBytes() - size_before;
291: 	count += append_count;
292: 	Verify();
293: }
294: 
295: // LCOV_EXCL_START
296: void VerifyHeapSizes(const data_ptr_t source_locations[], const idx_t heap_sizes[], const SelectionVector &append_sel,
297:                      const idx_t append_count, const idx_t heap_size_offset) {
298: #ifdef DEBUG
299: 	for (idx_t i = 0; i < append_count; i++) {
300: 		auto idx = append_sel.get_index(i);
301: 		const auto stored_heap_size = Load<uint32_t>(source_locations[idx] + heap_size_offset);
302: 		D_ASSERT(stored_heap_size == heap_sizes[idx]);
303: 	}
304: #endif
305: }
306: // LCOV_EXCL_STOP
307: 
308: void TupleDataCollection::CopyRows(TupleDataChunkState &chunk_state, TupleDataChunkState &input,
309:                                    const SelectionVector &append_sel, const idx_t append_count) const {
310: 	const auto source_locations = FlatVector::GetData<data_ptr_t>(input.row_locations);
311: 	const auto target_locations = FlatVector::GetData<data_ptr_t>(chunk_state.row_locations);
312: 
313: 	// Copy rows
314: 	const auto row_width = layout.GetRowWidth();
315: 	for (idx_t i = 0; i < append_count; i++) {
316: 		auto idx = append_sel.get_index(i);
317: 		FastMemcpy(target_locations[i], source_locations[idx], row_width);
318: 	}
319: 
320: 	// Copy heap if we need to
321: 	if (!layout.AllConstant()) {
322: 		const auto source_heap_locations = FlatVector::GetData<data_ptr_t>(input.heap_locations);
323: 		const auto target_heap_locations = FlatVector::GetData<data_ptr_t>(chunk_state.heap_locations);
324: 		const auto heap_sizes = FlatVector::GetData<idx_t>(input.heap_sizes);
325: 		VerifyHeapSizes(source_locations, heap_sizes, append_sel, append_count, layout.GetHeapSizeOffset());
326: 
327: 		// Check if we need to copy anything at all
328: 		idx_t total_heap_size = 0;
329: 		for (idx_t i = 0; i < append_count; i++) {
330: 			auto idx = append_sel.get_index(i);
331: 			total_heap_size += heap_sizes[idx];
332: 		}
333: 		if (total_heap_size == 0) {
334: 			return;
335: 		}
336: 
337: 		// Copy heap
338: 		for (idx_t i = 0; i < append_count; i++) {
339: 			auto idx = append_sel.get_index(i);
340: 			FastMemcpy(target_heap_locations[i], source_heap_locations[idx], heap_sizes[idx]);
341: 		}
342: 
343: 		// Recompute pointers after copying the data
344: 		TupleDataAllocator::RecomputeHeapPointers(input.heap_locations, append_sel, target_locations,
345: 		                                          chunk_state.heap_locations, 0, append_count, layout, 0);
346: 	}
347: }
348: 
349: void TupleDataCollection::Combine(TupleDataCollection &other) {
350: 	if (other.count == 0) {
351: 		return;
352: 	}
353: 	if (this->layout.GetTypes() != other.GetLayout().GetTypes()) {
354: 		throw InternalException("Attempting to combine TupleDataCollection with mismatching types");
355: 	}
356: 	this->segments.reserve(this->segments.size() + other.segments.size());
357: 	for (auto &other_seg : other.segments) {
358: 		AddSegment(std::move(other_seg));
359: 	}
360: 	other.Reset();
361: }
362: 
363: void TupleDataCollection::AddSegment(TupleDataSegment &&segment) {
364: 	count += segment.count;
365: 	data_size += segment.data_size;
366: 	segments.emplace_back(std::move(segment));
367: 	Verify();
368: }
369: 
370: void TupleDataCollection::Combine(unique_ptr<TupleDataCollection> other) {
371: 	Combine(*other);
372: }
373: 
374: void TupleDataCollection::Reset() {
375: 	count = 0;
376: 	data_size = 0;
377: 	segments.clear();
378: 
379: 	// Refreshes the TupleDataAllocator to prevent holding on to allocated data unnecessarily
380: 	allocator = make_shared_ptr<TupleDataAllocator>(*allocator);
381: }
382: 
383: void TupleDataCollection::InitializeChunk(DataChunk &chunk) const {
384: 	chunk.Initialize(allocator->GetAllocator(), layout.GetTypes());
385: }
386: 
387: void TupleDataCollection::InitializeChunk(DataChunk &chunk, const vector<column_t> &columns) const {
388: 	vector<LogicalType> chunk_types(columns.size());
389: 	// keep the order of the columns
390: 	for (idx_t i = 0; i < columns.size(); i++) {
391: 		auto column_idx = columns[i];
392: 		D_ASSERT(column_idx < layout.ColumnCount());
393: 		chunk_types[i] = layout.GetTypes()[column_idx];
394: 	}
395: 	chunk.Initialize(allocator->GetAllocator(), chunk_types);
396: }
397: 
398: void TupleDataCollection::InitializeScanChunk(TupleDataScanState &state, DataChunk &chunk) const {
399: 	auto &column_ids = state.chunk_state.column_ids;
400: 	D_ASSERT(!column_ids.empty());
401: 	vector<LogicalType> chunk_types;
402: 	chunk_types.reserve(column_ids.size());
403: 	for (idx_t i = 0; i < column_ids.size(); i++) {
404: 		auto column_idx = column_ids[i];
405: 		D_ASSERT(column_idx < layout.ColumnCount());
406: 		chunk_types.push_back(layout.GetTypes()[column_idx]);
407: 	}
408: 	chunk.Initialize(allocator->GetAllocator(), chunk_types);
409: }
410: 
411: void TupleDataCollection::InitializeScan(TupleDataScanState &state, TupleDataPinProperties properties) const {
412: 	vector<column_t> column_ids;
413: 	column_ids.reserve(layout.ColumnCount());
414: 	for (idx_t i = 0; i < layout.ColumnCount(); i++) {
415: 		column_ids.push_back(i);
416: 	}
417: 	InitializeScan(state, std::move(column_ids), properties);
418: }
419: 
420: void TupleDataCollection::InitializeScan(TupleDataScanState &state, vector<column_t> column_ids,
421:                                          TupleDataPinProperties properties) const {
422: 	state.pin_state.row_handles.clear();
423: 	state.pin_state.heap_handles.clear();
424: 	state.pin_state.properties = properties;
425: 	state.segment_index = 0;
426: 	state.chunk_index = 0;
427: 
428: 	auto &chunk_state = state.chunk_state;
429: 
430: 	for (auto &col : column_ids) {
431: 		auto &type = layout.GetTypes()[col];
432: 
433: 		if (TypeVisitor::Contains(type, LogicalTypeId::ARRAY)) {
434: 			auto cast_type = ArrayType::ConvertToList(type);
435: 			chunk_state.cached_cast_vector_cache.push_back(
436: 			    make_uniq<VectorCache>(Allocator::DefaultAllocator(), cast_type));
437: 			chunk_state.cached_cast_vectors.push_back(make_uniq<Vector>(*chunk_state.cached_cast_vector_cache.back()));
438: 		} else {
439: 			chunk_state.cached_cast_vectors.emplace_back();
440: 			chunk_state.cached_cast_vector_cache.emplace_back();
441: 		}
442: 	}
443: 
444: 	state.chunk_state.column_ids = std::move(column_ids);
445: }
446: 
447: void TupleDataCollection::InitializeScan(TupleDataParallelScanState &gstate, TupleDataPinProperties properties) const {
448: 	InitializeScan(gstate.scan_state, properties);
449: }
450: 
451: void TupleDataCollection::InitializeScan(TupleDataParallelScanState &state, vector<column_t> column_ids,
452:                                          TupleDataPinProperties properties) const {
453: 	InitializeScan(state.scan_state, std::move(column_ids), properties);
454: }
455: 
456: bool TupleDataCollection::Scan(TupleDataScanState &state, DataChunk &result) {
457: 	const auto segment_index_before = state.segment_index;
458: 	idx_t segment_index;
459: 	idx_t chunk_index;
460: 	if (!NextScanIndex(state, segment_index, chunk_index)) {
461: 		if (!segments.empty()) {
462: 			FinalizePinState(state.pin_state, segments[segment_index_before]);
463: 		}
464: 		result.SetCardinality(0);
465: 		return false;
466: 	}
467: 	if (segment_index_before != DConstants::INVALID_INDEX && segment_index != segment_index_before) {
468: 		FinalizePinState(state.pin_state, segments[segment_index_before]);
469: 	}
470: 	ScanAtIndex(state.pin_state, state.chunk_state, state.chunk_state.column_ids, segment_index, chunk_index, result);
471: 	return true;
472: }
473: 
474: bool TupleDataCollection::Scan(TupleDataParallelScanState &gstate, TupleDataLocalScanState &lstate, DataChunk &result) {
475: 	lstate.pin_state.properties = gstate.scan_state.pin_state.properties;
476: 
477: 	const auto segment_index_before = lstate.segment_index;
478: 	{
479: 		lock_guard<mutex> guard(gstate.lock);
480: 		if (!NextScanIndex(gstate.scan_state, lstate.segment_index, lstate.chunk_index)) {
481: 			if (!segments.empty()) {
482: 				FinalizePinState(lstate.pin_state, segments[segment_index_before]);
483: 			}
484: 			result.SetCardinality(0);
485: 			return false;
486: 		}
487: 	}
488: 	if (segment_index_before != DConstants::INVALID_INDEX && segment_index_before != lstate.segment_index) {
489: 		FinalizePinState(lstate.pin_state, segments[lstate.segment_index]);
490: 	}
491: 	ScanAtIndex(lstate.pin_state, lstate.chunk_state, gstate.scan_state.chunk_state.column_ids, lstate.segment_index,
492: 	            lstate.chunk_index, result);
493: 	return true;
494: }
495: 
496: bool TupleDataCollection::ScanComplete(const TupleDataScanState &state) const {
497: 	if (Count() == 0) {
498: 		return true;
499: 	}
500: 	return state.segment_index == segments.size() - 1 && state.chunk_index == segments.back().ChunkCount();
501: }
502: 
503: void TupleDataCollection::FinalizePinState(TupleDataPinState &pin_state, TupleDataSegment &segment) {
504: 	segment.allocator->ReleaseOrStoreHandles(pin_state, segment);
505: }
506: 
507: void TupleDataCollection::FinalizePinState(TupleDataPinState &pin_state) {
508: 	D_ASSERT(!segments.empty());
509: 	FinalizePinState(pin_state, segments.back());
510: }
511: 
512: bool TupleDataCollection::NextScanIndex(TupleDataScanState &state, idx_t &segment_index, idx_t &chunk_index) {
513: 	// Check if we still have segments to scan
514: 	if (state.segment_index >= segments.size()) {
515: 		// No more data left in the scan
516: 		return false;
517: 	}
518: 	// Check within the current segment if we still have chunks to scan
519: 	while (state.chunk_index >= segments[state.segment_index].ChunkCount()) {
520: 		// Exhausted all chunks for this segment: Move to the next one
521: 		state.segment_index++;
522: 		state.chunk_index = 0;
523: 		if (state.segment_index >= segments.size()) {
524: 			return false;
525: 		}
526: 	}
527: 	segment_index = state.segment_index;
528: 	chunk_index = state.chunk_index++;
529: 	return true;
530: }
531: void TupleDataCollection::ScanAtIndex(TupleDataPinState &pin_state, TupleDataChunkState &chunk_state,
532:                                       const vector<column_t> &column_ids, idx_t segment_index, idx_t chunk_index,
533:                                       DataChunk &result) {
534: 	auto &segment = segments[segment_index];
535: 	auto &chunk = segment.chunks[chunk_index];
536: 	segment.allocator->InitializeChunkState(segment, pin_state, chunk_state, chunk_index, false);
537: 	result.Reset();
538: 
539: 	ResetCachedCastVectors(chunk_state, column_ids);
540: 	Gather(chunk_state.row_locations, *FlatVector::IncrementalSelectionVector(), chunk.count, column_ids, result,
541: 	       *FlatVector::IncrementalSelectionVector(), chunk_state.cached_cast_vectors);
542: 	result.SetCardinality(chunk.count);
543: }
544: 
545: void TupleDataCollection::ResetCachedCastVectors(TupleDataChunkState &chunk_state, const vector<column_t> &column_ids) {
546: 	for (idx_t i = 0; i < column_ids.size(); i++) {
547: 		if (chunk_state.cached_cast_vectors[i]) {
548: 			chunk_state.cached_cast_vectors[i]->ResetFromCache(*chunk_state.cached_cast_vector_cache[i]);
549: 		}
550: 	}
551: }
552: 
553: // LCOV_EXCL_START
554: string TupleDataCollection::ToString() {
555: 	DataChunk chunk;
556: 	InitializeChunk(chunk);
557: 
558: 	TupleDataScanState scan_state;
559: 	InitializeScan(scan_state);
560: 
561: 	string result = StringUtil::Format("TupleDataCollection - [%llu Chunks, %llu Rows]\n", ChunkCount(), Count());
562: 	idx_t chunk_idx = 0;
563: 	idx_t row_count = 0;
564: 	while (Scan(scan_state, chunk)) {
565: 		result +=
566: 		    StringUtil::Format("Chunk %llu - [Rows %llu - %llu]\n", chunk_idx, row_count, row_count + chunk.size()) +
567: 		    chunk.ToString();
568: 		chunk_idx++;
569: 		row_count += chunk.size();
570: 	}
571: 
572: 	return result;
573: }
574: 
575: void TupleDataCollection::Print() {
576: 	Printer::Print(ToString());
577: }
578: 
579: void TupleDataCollection::Verify() const {
580: #ifdef DEBUG
581: 	idx_t total_count = 0;
582: 	idx_t total_size = 0;
583: 	for (const auto &segment : segments) {
584: 		segment.Verify();
585: 		total_count += segment.count;
586: 		total_size += segment.data_size;
587: 	}
588: 	D_ASSERT(total_count == this->count);
589: 	D_ASSERT(total_size == this->data_size);
590: #endif
591: }
592: 
593: void TupleDataCollection::VerifyEverythingPinned() const {
594: #ifdef DEBUG
595: 	for (const auto &segment : segments) {
596: 		segment.VerifyEverythingPinned();
597: 	}
598: #endif
599: }
600: // LCOV_EXCL_STOP
601: 
602: } // namespace duckdb
[end of src/common/types/row/tuple_data_collection.cpp]
[start of src/execution/aggregate_hashtable.cpp]
1: #include "duckdb/execution/aggregate_hashtable.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/aggregate_function_catalog_entry.hpp"
4: #include "duckdb/common/algorithm.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/radix_partitioning.hpp"
7: #include "duckdb/common/row_operations/row_operations.hpp"
8: #include "duckdb/common/types/null_value.hpp"
9: #include "duckdb/common/types/row/tuple_data_iterator.hpp"
10: #include "duckdb/common/vector_operations/vector_operations.hpp"
11: #include "duckdb/execution/expression_executor.hpp"
12: #include "duckdb/execution/ht_entry.hpp"
13: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
14: 
15: namespace duckdb {
16: 
17: using ValidityBytes = TupleDataLayout::ValidityBytes;
18: 
19: GroupedAggregateHashTable::GroupedAggregateHashTable(ClientContext &context, Allocator &allocator,
20:                                                      vector<LogicalType> group_types, vector<LogicalType> payload_types,
21:                                                      const vector<BoundAggregateExpression *> &bindings,
22:                                                      idx_t initial_capacity, idx_t radix_bits)
23:     : GroupedAggregateHashTable(context, allocator, std::move(group_types), std::move(payload_types),
24:                                 AggregateObject::CreateAggregateObjects(bindings), initial_capacity, radix_bits) {
25: }
26: 
27: GroupedAggregateHashTable::GroupedAggregateHashTable(ClientContext &context, Allocator &allocator,
28:                                                      vector<LogicalType> group_types)
29:     : GroupedAggregateHashTable(context, allocator, std::move(group_types), {}, vector<AggregateObject>()) {
30: }
31: 
32: GroupedAggregateHashTable::AggregateHTAppendState::AggregateHTAppendState()
33:     : ht_offsets(LogicalType::UBIGINT), hash_salts(LogicalType::HASH), group_compare_vector(STANDARD_VECTOR_SIZE),
34:       no_match_vector(STANDARD_VECTOR_SIZE), empty_vector(STANDARD_VECTOR_SIZE), new_groups(STANDARD_VECTOR_SIZE),
35:       addresses(LogicalType::POINTER) {
36: }
37: 
38: GroupedAggregateHashTable::GroupedAggregateHashTable(ClientContext &context, Allocator &allocator,
39:                                                      vector<LogicalType> group_types_p,
40:                                                      vector<LogicalType> payload_types_p,
41:                                                      vector<AggregateObject> aggregate_objects_p,
42:                                                      idx_t initial_capacity, idx_t radix_bits)
43:     : BaseAggregateHashTable(context, allocator, aggregate_objects_p, std::move(payload_types_p)),
44:       radix_bits(radix_bits), count(0), capacity(0), aggregate_allocator(make_shared_ptr<ArenaAllocator>(allocator)) {
45: 
46: 	// Append hash column to the end and initialise the row layout
47: 	group_types_p.emplace_back(LogicalType::HASH);
48: 	layout.Initialize(std::move(group_types_p), std::move(aggregate_objects_p));
49: 
50: 	hash_offset = layout.GetOffsets()[layout.ColumnCount() - 1];
51: 
52: 	// Partitioned data and pointer table
53: 	InitializePartitionedData();
54: 	Resize(initial_capacity);
55: 
56: 	// Predicates
57: 	predicates.resize(layout.ColumnCount() - 1, ExpressionType::COMPARE_NOT_DISTINCT_FROM);
58: 	row_matcher.Initialize(true, layout, predicates);
59: }
60: 
61: void GroupedAggregateHashTable::InitializePartitionedData() {
62: 	if (!partitioned_data || RadixPartitioning::RadixBits(partitioned_data->PartitionCount()) != radix_bits) {
63: 		D_ASSERT(!partitioned_data || partitioned_data->Count() == 0);
64: 		partitioned_data =
65: 		    make_uniq<RadixPartitionedTupleData>(buffer_manager, layout, radix_bits, layout.ColumnCount() - 1);
66: 	} else {
67: 		partitioned_data->Reset();
68: 	}
69: 
70: 	D_ASSERT(GetLayout().GetAggrWidth() == layout.GetAggrWidth());
71: 	D_ASSERT(GetLayout().GetDataWidth() == layout.GetDataWidth());
72: 	D_ASSERT(GetLayout().GetRowWidth() == layout.GetRowWidth());
73: 
74: 	partitioned_data->InitializeAppendState(state.append_state, TupleDataPinProperties::KEEP_EVERYTHING_PINNED);
75: }
76: 
77: unique_ptr<PartitionedTupleData> &GroupedAggregateHashTable::GetPartitionedData() {
78: 	return partitioned_data;
79: }
80: 
81: shared_ptr<ArenaAllocator> GroupedAggregateHashTable::GetAggregateAllocator() {
82: 	return aggregate_allocator;
83: }
84: 
85: GroupedAggregateHashTable::~GroupedAggregateHashTable() {
86: 	Destroy();
87: }
88: 
89: void GroupedAggregateHashTable::Destroy() {
90: 	if (!partitioned_data || partitioned_data->Count() == 0 || !layout.HasDestructor()) {
91: 		return;
92: 	}
93: 
94: 	// There are aggregates with destructors: Call the destructor for each of the aggregates
95: 	// Currently does not happen because aggregate destructors are called while scanning in RadixPartitionedHashTable
96: 	// LCOV_EXCL_START
97: 	RowOperationsState row_state(*aggregate_allocator);
98: 	for (auto &data_collection : partitioned_data->GetPartitions()) {
99: 		if (data_collection->Count() == 0) {
100: 			continue;
101: 		}
102: 		TupleDataChunkIterator iterator(*data_collection, TupleDataPinProperties::DESTROY_AFTER_DONE, false);
103: 		auto &row_locations = iterator.GetChunkState().row_locations;
104: 		do {
105: 			RowOperations::DestroyStates(row_state, layout, row_locations, iterator.GetCurrentChunkCount());
106: 		} while (iterator.Next());
107: 		data_collection->Reset();
108: 	}
109: 	// LCOV_EXCL_STOP
110: }
111: 
112: const TupleDataLayout &GroupedAggregateHashTable::GetLayout() const {
113: 	return partitioned_data->GetLayout();
114: }
115: 
116: idx_t GroupedAggregateHashTable::Count() const {
117: 	return count;
118: }
119: 
120: idx_t GroupedAggregateHashTable::InitialCapacity() {
121: 	return STANDARD_VECTOR_SIZE * 2ULL;
122: }
123: 
124: idx_t GroupedAggregateHashTable::GetCapacityForCount(idx_t count) {
125: 	count = MaxValue<idx_t>(InitialCapacity(), count);
126: 	return NextPowerOfTwo(LossyNumericCast<uint64_t>(static_cast<double>(count) * LOAD_FACTOR));
127: }
128: 
129: idx_t GroupedAggregateHashTable::Capacity() const {
130: 	return capacity;
131: }
132: 
133: idx_t GroupedAggregateHashTable::ResizeThreshold() const {
134: 	return LossyNumericCast<idx_t>(static_cast<double>(Capacity()) / LOAD_FACTOR);
135: }
136: 
137: idx_t GroupedAggregateHashTable::ApplyBitMask(hash_t hash) const {
138: 	return hash & bitmask;
139: }
140: 
141: void GroupedAggregateHashTable::Verify() {
142: #ifdef DEBUG
143: 	idx_t total_count = 0;
144: 	for (idx_t i = 0; i < capacity; i++) {
145: 		const auto &entry = entries[i];
146: 		if (!entry.IsOccupied()) {
147: 			continue;
148: 		}
149: 		auto hash = Load<hash_t>(entry.GetPointer() + hash_offset);
150: 		D_ASSERT(entry.GetSalt() == ht_entry_t::ExtractSalt(hash));
151: 		total_count++;
152: 	}
153: 	D_ASSERT(total_count == Count());
154: #endif
155: }
156: 
157: void GroupedAggregateHashTable::ClearPointerTable() {
158: 	std::fill_n(entries, capacity, ht_entry_t::GetEmptyEntry());
159: }
160: 
161: void GroupedAggregateHashTable::ResetCount() {
162: 	count = 0;
163: }
164: 
165: void GroupedAggregateHashTable::SetRadixBits(idx_t radix_bits_p) {
166: 	radix_bits = radix_bits_p;
167: }
168: 
169: void GroupedAggregateHashTable::Resize(idx_t size) {
170: 	D_ASSERT(size >= STANDARD_VECTOR_SIZE);
171: 	D_ASSERT(IsPowerOfTwo(size));
172: 	if (size < capacity) {
173: 		throw InternalException("Cannot downsize a hash table!");
174: 	}
175: 
176: 	capacity = size;
177: 	hash_map = buffer_manager.GetBufferAllocator().Allocate(capacity * sizeof(ht_entry_t));
178: 	entries = reinterpret_cast<ht_entry_t *>(hash_map.get());
179: 	ClearPointerTable();
180: 	bitmask = capacity - 1;
181: 
182: 	if (Count() != 0) {
183: 		for (auto &data_collection : partitioned_data->GetPartitions()) {
184: 			if (data_collection->Count() == 0) {
185: 				continue;
186: 			}
187: 			TupleDataChunkIterator iterator(*data_collection, TupleDataPinProperties::ALREADY_PINNED, false);
188: 			const auto row_locations = iterator.GetRowLocations();
189: 			do {
190: 				for (idx_t i = 0; i < iterator.GetCurrentChunkCount(); i++) {
191: 					const auto &row_location = row_locations[i];
192: 					const auto hash = Load<hash_t>(row_location + hash_offset);
193: 
194: 					// Find an empty entry
195: 					auto entry_idx = ApplyBitMask(hash);
196: 					D_ASSERT(entry_idx == hash % capacity);
197: 					while (entries[entry_idx].IsOccupied()) {
198: 						entry_idx++;
199: 						if (entry_idx >= capacity) {
200: 							entry_idx = 0;
201: 						}
202: 					}
203: 					auto &entry = entries[entry_idx];
204: 					D_ASSERT(!entry.IsOccupied());
205: 					entry.SetSalt(ht_entry_t::ExtractSalt(hash));
206: 					entry.SetPointer(row_location);
207: 					D_ASSERT(entry.IsOccupied());
208: 				}
209: 			} while (iterator.Next());
210: 		}
211: 	}
212: 
213: 	Verify();
214: }
215: 
216: idx_t GroupedAggregateHashTable::AddChunk(DataChunk &groups, DataChunk &payload, AggregateType filter) {
217: 	unsafe_vector<idx_t> aggregate_filter;
218: 
219: 	auto &aggregates = layout.GetAggregates();
220: 	for (idx_t i = 0; i < aggregates.size(); i++) {
221: 		auto &aggregate = aggregates[i];
222: 		if (aggregate.aggr_type == filter) {
223: 			aggregate_filter.push_back(i);
224: 		}
225: 	}
226: 	return AddChunk(groups, payload, aggregate_filter);
227: }
228: 
229: idx_t GroupedAggregateHashTable::AddChunk(DataChunk &groups, DataChunk &payload, const unsafe_vector<idx_t> &filter) {
230: 	Vector hashes(LogicalType::HASH);
231: 	groups.Hash(hashes);
232: 
233: 	return AddChunk(groups, hashes, payload, filter);
234: }
235: 
236: idx_t GroupedAggregateHashTable::AddChunk(DataChunk &groups, Vector &group_hashes, DataChunk &payload,
237:                                           const unsafe_vector<idx_t> &filter) {
238: 	if (groups.size() == 0) {
239: 		return 0;
240: 	}
241: 
242: #ifdef DEBUG
243: 	D_ASSERT(groups.ColumnCount() + 1 == layout.ColumnCount());
244: 	for (idx_t i = 0; i < groups.ColumnCount(); i++) {
245: 		D_ASSERT(groups.GetTypes()[i] == layout.GetTypes()[i]);
246: 	}
247: #endif
248: 
249: 	const auto new_group_count = FindOrCreateGroups(groups, group_hashes, state.addresses, state.new_groups);
250: 	VectorOperations::AddInPlace(state.addresses, NumericCast<int64_t>(layout.GetAggrOffset()), payload.size());
251: 
252: 	// Now every cell has an entry, update the aggregates
253: 	auto &aggregates = layout.GetAggregates();
254: 	idx_t filter_idx = 0;
255: 	idx_t payload_idx = 0;
256: 	RowOperationsState row_state(*aggregate_allocator);
257: 	for (idx_t i = 0; i < aggregates.size(); i++) {
258: 		auto &aggr = aggregates[i];
259: 		if (filter_idx >= filter.size() || i < filter[filter_idx]) {
260: 			// Skip all the aggregates that are not in the filter
261: 			payload_idx += aggr.child_count;
262: 			VectorOperations::AddInPlace(state.addresses, NumericCast<int64_t>(aggr.payload_size), payload.size());
263: 			continue;
264: 		}
265: 		D_ASSERT(i == filter[filter_idx]);
266: 
267: 		if (aggr.aggr_type != AggregateType::DISTINCT && aggr.filter) {
268: 			RowOperations::UpdateFilteredStates(row_state, filter_set.GetFilterData(i), aggr, state.addresses, payload,
269: 			                                    payload_idx);
270: 		} else {
271: 			RowOperations::UpdateStates(row_state, aggr, state.addresses, payload, payload_idx, payload.size());
272: 		}
273: 
274: 		// Move to the next aggregate
275: 		payload_idx += aggr.child_count;
276: 		VectorOperations::AddInPlace(state.addresses, NumericCast<int64_t>(aggr.payload_size), payload.size());
277: 		filter_idx++;
278: 	}
279: 
280: 	Verify();
281: 	return new_group_count;
282: }
283: 
284: void GroupedAggregateHashTable::FetchAggregates(DataChunk &groups, DataChunk &result) {
285: #ifdef DEBUG
286: 	groups.Verify();
287: 	D_ASSERT(groups.ColumnCount() + 1 == layout.ColumnCount());
288: 	for (idx_t i = 0; i < result.ColumnCount(); i++) {
289: 		D_ASSERT(result.data[i].GetType() == payload_types[i]);
290: 	}
291: #endif
292: 
293: 	result.SetCardinality(groups);
294: 	if (groups.size() == 0) {
295: 		return;
296: 	}
297: 
298: 	// find the groups associated with the addresses
299: 	// FIXME: this should not use the FindOrCreateGroups, creating them is unnecessary
300: 	Vector addresses(LogicalType::POINTER);
301: 	FindOrCreateGroups(groups, addresses);
302: 	// now fetch the aggregates
303: 	RowOperationsState row_state(*aggregate_allocator);
304: 	RowOperations::FinalizeStates(row_state, layout, addresses, result, 0);
305: }
306: 
307: idx_t GroupedAggregateHashTable::FindOrCreateGroupsInternal(DataChunk &groups, Vector &group_hashes_v,
308:                                                             Vector &addresses_v, SelectionVector &new_groups_out) {
309: 	D_ASSERT(groups.ColumnCount() + 1 == layout.ColumnCount());
310: 	D_ASSERT(group_hashes_v.GetType() == LogicalType::HASH);
311: 	D_ASSERT(state.ht_offsets.GetVectorType() == VectorType::FLAT_VECTOR);
312: 	D_ASSERT(state.ht_offsets.GetType() == LogicalType::UBIGINT);
313: 	D_ASSERT(addresses_v.GetType() == LogicalType::POINTER);
314: 	D_ASSERT(state.hash_salts.GetType() == LogicalType::HASH);
315: 
316: 	// Need to fit the entire vector, and resize at threshold
317: 	if (Count() + groups.size() > capacity || Count() + groups.size() > ResizeThreshold()) {
318: 		Verify();
319: 		Resize(capacity * 2);
320: 	}
321: 	D_ASSERT(capacity - Count() >= groups.size()); // we need to be able to fit at least one vector of data
322: 
323: 	group_hashes_v.Flatten(groups.size());
324: 	auto hashes = FlatVector::GetData<hash_t>(group_hashes_v);
325: 
326: 	addresses_v.Flatten(groups.size());
327: 	auto addresses = FlatVector::GetData<data_ptr_t>(addresses_v);
328: 
329: 	// Compute the entry in the table based on the hash using a modulo,
330: 	// and precompute the hash salts for faster comparison below
331: 	auto ht_offsets = FlatVector::GetData<uint64_t>(state.ht_offsets);
332: 	const auto hash_salts = FlatVector::GetData<hash_t>(state.hash_salts);
333: 	for (idx_t r = 0; r < groups.size(); r++) {
334: 		const auto &hash = hashes[r];
335: 		ht_offsets[r] = ApplyBitMask(hash);
336: 		D_ASSERT(ht_offsets[r] == hash % capacity);
337: 		hash_salts[r] = ht_entry_t::ExtractSalt(hash);
338: 	}
339: 
340: 	// we start out with all entries [0, 1, 2, ..., groups.size()]
341: 	const SelectionVector *sel_vector = FlatVector::IncrementalSelectionVector();
342: 
343: 	// Make a chunk that references the groups and the hashes and convert to unified format
344: 	if (state.group_chunk.ColumnCount() == 0) {
345: 		state.group_chunk.InitializeEmpty(layout.GetTypes());
346: 	}
347: 	D_ASSERT(state.group_chunk.ColumnCount() == layout.GetTypes().size());
348: 	for (idx_t grp_idx = 0; grp_idx < groups.ColumnCount(); grp_idx++) {
349: 		state.group_chunk.data[grp_idx].Reference(groups.data[grp_idx]);
350: 	}
351: 	state.group_chunk.data[groups.ColumnCount()].Reference(group_hashes_v);
352: 	state.group_chunk.SetCardinality(groups);
353: 
354: 	// convert all vectors to unified format
355: 	auto &chunk_state = state.append_state.chunk_state;
356: 	TupleDataCollection::ToUnifiedFormat(chunk_state, state.group_chunk);
357: 	if (!state.group_data) {
358: 		state.group_data = make_unsafe_uniq_array_uninitialized<UnifiedVectorFormat>(state.group_chunk.ColumnCount());
359: 	}
360: 	TupleDataCollection::GetVectorData(chunk_state, state.group_data.get());
361: 
362: 	idx_t new_group_count = 0;
363: 	idx_t remaining_entries = groups.size();
364: 	idx_t iteration_count;
365: 	for (iteration_count = 0; remaining_entries > 0 && iteration_count < capacity; iteration_count++) {
366: 		idx_t new_entry_count = 0;
367: 		idx_t need_compare_count = 0;
368: 		idx_t no_match_count = 0;
369: 
370: 		// For each remaining entry, figure out whether or not it belongs to a full or empty group
371: 		for (idx_t i = 0; i < remaining_entries; i++) {
372: 			const auto index = sel_vector->get_index(i);
373: 			const auto &salt = hash_salts[index];
374: 			auto &ht_offset = ht_offsets[index];
375: 
376: 			idx_t inner_iteration_count;
377: 			for (inner_iteration_count = 0; inner_iteration_count < capacity; inner_iteration_count++) {
378: 				auto &entry = entries[ht_offset];
379: 				if (entry.IsOccupied()) { // Cell is occupied: Compare salts
380: 					if (entry.GetSalt() == salt) {
381: 						// Same salt, compare group keys
382: 						state.group_compare_vector.set_index(need_compare_count++, index);
383: 						break;
384: 					}
385: 					// Different salts, move to next entry (linear probing)
386: 					IncrementAndWrap(ht_offset, bitmask);
387: 				} else { // Cell is unoccupied, let's claim it
388: 					// Set salt (also marks as occupied)
389: 					entry.SetSalt(salt);
390: 					// Update selection lists for outer loops
391: 					state.empty_vector.set_index(new_entry_count++, index);
392: 					new_groups_out.set_index(new_group_count++, index);
393: 					break;
394: 				}
395: 			}
396: 			if (inner_iteration_count == capacity) {
397: 				throw InternalException("Maximum inner iteration count reached in GroupedAggregateHashTable");
398: 			}
399: 		}
400: 
401: 		if (new_entry_count != 0) {
402: 			// Append everything that belongs to an empty group
403: 			partitioned_data->AppendUnified(state.append_state, state.group_chunk, state.empty_vector, new_entry_count);
404: 			RowOperations::InitializeStates(layout, chunk_state.row_locations,
405: 			                                *FlatVector::IncrementalSelectionVector(), new_entry_count);
406: 
407: 			// Set the entry pointers in the 1st part of the HT now that the data has been appended
408: 			const auto row_locations = FlatVector::GetData<data_ptr_t>(chunk_state.row_locations);
409: 			const auto &row_sel = state.append_state.reverse_partition_sel;
410: 			for (idx_t new_entry_idx = 0; new_entry_idx < new_entry_count; new_entry_idx++) {
411: 				const auto index = state.empty_vector.get_index(new_entry_idx);
412: 				const auto row_idx = row_sel.get_index(index);
413: 				const auto &row_location = row_locations[row_idx];
414: 
415: 				auto &entry = entries[ht_offsets[index]];
416: 
417: 				entry.SetPointer(row_location);
418: 				addresses[index] = row_location;
419: 			}
420: 		}
421: 
422: 		if (need_compare_count != 0) {
423: 			// Get the pointers to the rows that need to be compared
424: 			for (idx_t need_compare_idx = 0; need_compare_idx < need_compare_count; need_compare_idx++) {
425: 				const auto index = state.group_compare_vector.get_index(need_compare_idx);
426: 				const auto &entry = entries[ht_offsets[index]];
427: 				addresses[index] = entry.GetPointer();
428: 			}
429: 
430: 			// Perform group comparisons
431: 			row_matcher.Match(state.group_chunk, chunk_state.vector_data, state.group_compare_vector,
432: 			                  need_compare_count, layout, addresses_v, &state.no_match_vector, no_match_count);
433: 		}
434: 
435: 		// Linear probing: each of the entries that do not match move to the next entry in the HT
436: 		for (idx_t i = 0; i < no_match_count; i++) {
437: 			const auto index = state.no_match_vector.get_index(i);
438: 			auto &ht_offset = ht_offsets[index];
439: 			IncrementAndWrap(ht_offset, bitmask);
440: 		}
441: 		sel_vector = &state.no_match_vector;
442: 		remaining_entries = no_match_count;
443: 	}
444: 	if (iteration_count == capacity) {
445: 		throw InternalException("Maximum outer iteration count reached in GroupedAggregateHashTable");
446: 	}
447: 
448: 	count += new_group_count;
449: 	return new_group_count;
450: }
451: 
452: // this is to support distinct aggregations where we need to record whether we
453: // have already seen a value for a group
454: idx_t GroupedAggregateHashTable::FindOrCreateGroups(DataChunk &groups, Vector &group_hashes, Vector &addresses_out,
455:                                                     SelectionVector &new_groups_out) {
456: 	return FindOrCreateGroupsInternal(groups, group_hashes, addresses_out, new_groups_out);
457: }
458: 
459: void GroupedAggregateHashTable::FindOrCreateGroups(DataChunk &groups, Vector &addresses) {
460: 	// create a dummy new_groups sel vector
461: 	FindOrCreateGroups(groups, addresses, state.new_groups);
462: }
463: 
464: idx_t GroupedAggregateHashTable::FindOrCreateGroups(DataChunk &groups, Vector &addresses_out,
465:                                                     SelectionVector &new_groups_out) {
466: 	Vector hashes(LogicalType::HASH);
467: 	groups.Hash(hashes);
468: 	return FindOrCreateGroups(groups, hashes, addresses_out, new_groups_out);
469: }
470: 
471: struct FlushMoveState {
472: 	explicit FlushMoveState(TupleDataCollection &collection_p)
473: 	    : collection(collection_p), hashes(LogicalType::HASH), group_addresses(LogicalType::POINTER),
474: 	      new_groups_sel(STANDARD_VECTOR_SIZE) {
475: 		const auto &layout = collection.GetLayout();
476: 		vector<column_t> column_ids;
477: 		column_ids.reserve(layout.ColumnCount() - 1);
478: 		for (idx_t col_idx = 0; col_idx < layout.ColumnCount() - 1; col_idx++) {
479: 			column_ids.emplace_back(col_idx);
480: 		}
481: 		collection.InitializeScan(scan_state, column_ids, TupleDataPinProperties::DESTROY_AFTER_DONE);
482: 		collection.InitializeScanChunk(scan_state, groups);
483: 		hash_col_idx = layout.ColumnCount() - 1;
484: 	}
485: 
486: 	bool Scan() {
487: 		if (collection.Scan(scan_state, groups)) {
488: 			collection.Gather(scan_state.chunk_state.row_locations, *FlatVector::IncrementalSelectionVector(),
489: 			                  groups.size(), hash_col_idx, hashes, *FlatVector::IncrementalSelectionVector(), nullptr);
490: 			return true;
491: 		}
492: 
493: 		collection.FinalizePinState(scan_state.pin_state);
494: 		return false;
495: 	}
496: 
497: 	TupleDataCollection &collection;
498: 	TupleDataScanState scan_state;
499: 	DataChunk groups;
500: 
501: 	idx_t hash_col_idx;
502: 	Vector hashes;
503: 
504: 	Vector group_addresses;
505: 	SelectionVector new_groups_sel;
506: };
507: 
508: void GroupedAggregateHashTable::Combine(GroupedAggregateHashTable &other) {
509: 	auto other_data = other.partitioned_data->GetUnpartitioned();
510: 	Combine(*other_data);
511: 
512: 	// Inherit ownership to all stored aggregate allocators
513: 	stored_allocators.emplace_back(other.aggregate_allocator);
514: 	for (const auto &stored_allocator : other.stored_allocators) {
515: 		stored_allocators.emplace_back(stored_allocator);
516: 	}
517: }
518: 
519: void GroupedAggregateHashTable::Combine(TupleDataCollection &other_data, optional_ptr<atomic<double>> progress) {
520: 	D_ASSERT(other_data.GetLayout().GetAggrWidth() == layout.GetAggrWidth());
521: 	D_ASSERT(other_data.GetLayout().GetDataWidth() == layout.GetDataWidth());
522: 	D_ASSERT(other_data.GetLayout().GetRowWidth() == layout.GetRowWidth());
523: 
524: 	if (other_data.Count() == 0) {
525: 		return;
526: 	}
527: 
528: 	FlushMoveState fm_state(other_data);
529: 	RowOperationsState row_state(*aggregate_allocator);
530: 
531: 	idx_t chunk_idx = 0;
532: 	const auto chunk_count = other_data.ChunkCount();
533: 	while (fm_state.Scan()) {
534: 		FindOrCreateGroups(fm_state.groups, fm_state.hashes, fm_state.group_addresses, fm_state.new_groups_sel);
535: 		RowOperations::CombineStates(row_state, layout, fm_state.scan_state.chunk_state.row_locations,
536: 		                             fm_state.group_addresses, fm_state.groups.size());
537: 		if (layout.HasDestructor()) {
538: 			RowOperations::DestroyStates(row_state, layout, fm_state.scan_state.chunk_state.row_locations,
539: 			                             fm_state.groups.size());
540: 		}
541: 
542: 		if (progress) {
543: 			*progress = double(++chunk_idx) / double(chunk_count);
544: 		}
545: 	}
546: 
547: 	Verify();
548: }
549: 
550: void GroupedAggregateHashTable::UnpinData() {
551: 	partitioned_data->FlushAppendState(state.append_state);
552: 	partitioned_data->Unpin();
553: }
554: 
555: } // namespace duckdb
[end of src/execution/aggregate_hashtable.cpp]
[start of src/execution/operator/join/physical_hash_join.cpp]
1: #include "duckdb/execution/operator/join/physical_hash_join.hpp"
2: 
3: #include "duckdb/common/radix_partitioning.hpp"
4: #include "duckdb/execution/expression_executor.hpp"
5: #include "duckdb/execution/operator/aggregate/ungrouped_aggregate_state.hpp"
6: #include "duckdb/function/aggregate/distributive_functions.hpp"
7: #include "duckdb/function/function_binder.hpp"
8: #include "duckdb/main/client_context.hpp"
9: #include "duckdb/main/query_profiler.hpp"
10: #include "duckdb/parallel/base_pipeline_event.hpp"
11: #include "duckdb/parallel/executor_task.hpp"
12: #include "duckdb/parallel/interrupt.hpp"
13: #include "duckdb/parallel/pipeline.hpp"
14: #include "duckdb/parallel/thread_context.hpp"
15: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
16: #include "duckdb/planner/expression/bound_reference_expression.hpp"
17: #include "duckdb/planner/filter/constant_filter.hpp"
18: #include "duckdb/planner/filter/null_filter.hpp"
19: #include "duckdb/planner/table_filter.hpp"
20: #include "duckdb/storage/buffer_manager.hpp"
21: #include "duckdb/storage/storage_manager.hpp"
22: #include "duckdb/storage/temporary_memory_manager.hpp"
23: 
24: namespace duckdb {
25: 
26: PhysicalHashJoin::PhysicalHashJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left,
27:                                    unique_ptr<PhysicalOperator> right, vector<JoinCondition> cond, JoinType join_type,
28:                                    const vector<idx_t> &left_projection_map, const vector<idx_t> &right_projection_map,
29:                                    vector<LogicalType> delim_types, idx_t estimated_cardinality,
30:                                    PerfectHashJoinStats perfect_join_stats,
31:                                    unique_ptr<JoinFilterPushdownInfo> pushdown_info_p)
32:     : PhysicalComparisonJoin(op, PhysicalOperatorType::HASH_JOIN, std::move(cond), join_type, estimated_cardinality),
33:       delim_types(std::move(delim_types)), perfect_join_statistics(std::move(perfect_join_stats)) {
34: 	D_ASSERT(left_projection_map.empty());
35: 
36: 	filter_pushdown = std::move(pushdown_info_p);
37: 
38: 	children.push_back(std::move(left));
39: 	children.push_back(std::move(right));
40: 
41: 	// Collect condition types, and which conditions are just references (so we won't duplicate them in the payload)
42: 	unordered_map<idx_t, idx_t> build_columns_in_conditions;
43: 	for (idx_t cond_idx = 0; cond_idx < conditions.size(); cond_idx++) {
44: 		auto &condition = conditions[cond_idx];
45: 		condition_types.push_back(condition.left->return_type);
46: 		if (condition.right->GetExpressionClass() == ExpressionClass::BOUND_REF) {
47: 			build_columns_in_conditions.emplace(condition.right->Cast<BoundReferenceExpression>().index, cond_idx);
48: 		}
49: 	}
50: 
51: 	// For ANTI, SEMI and MARK join, we only need to store the keys, so for these the payload/RHS types are empty
52: 	if (join_type == JoinType::ANTI || join_type == JoinType::SEMI || join_type == JoinType::MARK) {
53: 		return;
54: 	}
55: 
56: 	auto &rhs_input_types = children[1]->GetTypes();
57: 
58: 	// Create a projection map for the RHS (if it was empty), for convenience
59: 	auto right_projection_map_copy = right_projection_map;
60: 	if (right_projection_map_copy.empty()) {
61: 		right_projection_map_copy.reserve(rhs_input_types.size());
62: 		for (idx_t i = 0; i < rhs_input_types.size(); i++) {
63: 			right_projection_map_copy.emplace_back(i);
64: 		}
65: 	}
66: 
67: 	// Now fill payload expressions/types and RHS columns/types
68: 	for (auto &rhs_col : right_projection_map_copy) {
69: 		auto &rhs_col_type = rhs_input_types[rhs_col];
70: 
71: 		auto it = build_columns_in_conditions.find(rhs_col);
72: 		if (it == build_columns_in_conditions.end()) {
73: 			// This rhs column is not a join key
74: 			payload_column_idxs.push_back(rhs_col);
75: 			payload_types.push_back(rhs_col_type);
76: 			rhs_output_columns.push_back(condition_types.size() + payload_types.size() - 1);
77: 		} else {
78: 			// This rhs column is a join key
79: 			rhs_output_columns.push_back(it->second);
80: 		}
81: 		rhs_output_types.push_back(rhs_col_type);
82: 	}
83: }
84: 
85: PhysicalHashJoin::PhysicalHashJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left,
86:                                    unique_ptr<PhysicalOperator> right, vector<JoinCondition> cond, JoinType join_type,
87:                                    idx_t estimated_cardinality, PerfectHashJoinStats perfect_join_state)
88:     : PhysicalHashJoin(op, std::move(left), std::move(right), std::move(cond), join_type, {}, {}, {},
89:                        estimated_cardinality, std::move(perfect_join_state), nullptr) {
90: }
91: 
92: //===--------------------------------------------------------------------===//
93: // Sink
94: //===--------------------------------------------------------------------===//
95: JoinFilterGlobalState::~JoinFilterGlobalState() {
96: }
97: 
98: JoinFilterLocalState::~JoinFilterLocalState() {
99: }
100: 
101: unique_ptr<JoinFilterGlobalState> JoinFilterPushdownInfo::GetGlobalState(ClientContext &context,
102:                                                                          const PhysicalOperator &op) const {
103: 	// clear any previously set filters
104: 	// we can have previous filters for this operator in case of e.g. recursive CTEs
105: 	dynamic_filters->ClearFilters(op);
106: 	auto result = make_uniq<JoinFilterGlobalState>();
107: 	result->global_aggregate_state =
108: 	    make_uniq<GlobalUngroupedAggregateState>(BufferAllocator::Get(context), min_max_aggregates);
109: 	return result;
110: }
111: 
112: class HashJoinGlobalSinkState : public GlobalSinkState {
113: public:
114: 	HashJoinGlobalSinkState(const PhysicalHashJoin &op_p, ClientContext &context_p)
115: 	    : context(context_p), op(op_p),
116: 	      num_threads(NumericCast<idx_t>(TaskScheduler::GetScheduler(context).NumberOfThreads())),
117: 	      temporary_memory_state(TemporaryMemoryManager::Get(context).Register(context)), finalized(false),
118: 	      active_local_states(0), total_size(0), max_partition_size(0), max_partition_count(0), scanned_data(false) {
119: 		hash_table = op.InitializeHashTable(context);
120: 
121: 		// For perfect hash join
122: 		perfect_join_executor = make_uniq<PerfectHashJoinExecutor>(op, *hash_table, op.perfect_join_statistics);
123: 		// For external hash join
124: 		external = ClientConfig::GetConfig(context).GetSetting<DebugForceExternalSetting>(context);
125: 		// Set probe types
126: 		const auto &payload_types = op.children[0]->types;
127: 		probe_types.insert(probe_types.end(), op.condition_types.begin(), op.condition_types.end());
128: 		probe_types.insert(probe_types.end(), payload_types.begin(), payload_types.end());
129: 		probe_types.emplace_back(LogicalType::HASH);
130: 
131: 		if (op.filter_pushdown) {
132: 			global_filter_state = op.filter_pushdown->GetGlobalState(context, op);
133: 		}
134: 	}
135: 
136: 	void ScheduleFinalize(Pipeline &pipeline, Event &event);
137: 	void InitializeProbeSpill();
138: 
139: public:
140: 	ClientContext &context;
141: 	const PhysicalHashJoin &op;
142: 
143: 	const idx_t num_threads;
144: 	//! Temporary memory state for managing this operator's memory usage
145: 	unique_ptr<TemporaryMemoryState> temporary_memory_state;
146: 
147: 	//! Global HT used by the join
148: 	unique_ptr<JoinHashTable> hash_table;
149: 	//! The perfect hash join executor (if any)
150: 	unique_ptr<PerfectHashJoinExecutor> perfect_join_executor;
151: 	//! Whether or not the hash table has been finalized
152: 	bool finalized;
153: 	//! The number of active local states
154: 	atomic<idx_t> active_local_states;
155: 
156: 	//! Whether we are doing an external + some sizes
157: 	bool external;
158: 	idx_t total_size;
159: 	idx_t max_partition_size;
160: 	idx_t max_partition_count;
161: 
162: 	//! Hash tables built by each thread
163: 	vector<unique_ptr<JoinHashTable>> local_hash_tables;
164: 
165: 	//! Excess probe data gathered during Sink
166: 	vector<LogicalType> probe_types;
167: 	unique_ptr<JoinHashTable::ProbeSpill> probe_spill;
168: 
169: 	//! Whether or not we have started scanning data using GetData
170: 	atomic<bool> scanned_data;
171: 
172: 	unique_ptr<JoinFilterGlobalState> global_filter_state;
173: };
174: 
175: unique_ptr<JoinFilterLocalState> JoinFilterPushdownInfo::GetLocalState(JoinFilterGlobalState &gstate) const {
176: 	auto result = make_uniq<JoinFilterLocalState>();
177: 	result->local_aggregate_state = make_uniq<LocalUngroupedAggregateState>(*gstate.global_aggregate_state);
178: 	return result;
179: }
180: 
181: class HashJoinLocalSinkState : public LocalSinkState {
182: public:
183: 	HashJoinLocalSinkState(const PhysicalHashJoin &op, ClientContext &context, HashJoinGlobalSinkState &gstate)
184: 	    : join_key_executor(context) {
185: 		auto &allocator = BufferAllocator::Get(context);
186: 
187: 		for (auto &cond : op.conditions) {
188: 			join_key_executor.AddExpression(*cond.right);
189: 		}
190: 		join_keys.Initialize(allocator, op.condition_types);
191: 
192: 		if (!op.payload_types.empty()) {
193: 			payload_chunk.Initialize(allocator, op.payload_types);
194: 		}
195: 
196: 		hash_table = op.InitializeHashTable(context);
197: 		hash_table->GetSinkCollection().InitializeAppendState(append_state);
198: 
199: 		gstate.active_local_states++;
200: 
201: 		if (op.filter_pushdown) {
202: 			local_filter_state = op.filter_pushdown->GetLocalState(*gstate.global_filter_state);
203: 		}
204: 	}
205: 
206: public:
207: 	PartitionedTupleDataAppendState append_state;
208: 
209: 	ExpressionExecutor join_key_executor;
210: 	DataChunk join_keys;
211: 
212: 	DataChunk payload_chunk;
213: 
214: 	//! Thread-local HT
215: 	unique_ptr<JoinHashTable> hash_table;
216: 
217: 	unique_ptr<JoinFilterLocalState> local_filter_state;
218: };
219: 
220: unique_ptr<JoinHashTable> PhysicalHashJoin::InitializeHashTable(ClientContext &context) const {
221: 	auto result = make_uniq<JoinHashTable>(context, conditions, payload_types, join_type, rhs_output_columns);
222: 	if (!delim_types.empty() && join_type == JoinType::MARK) {
223: 		// correlated MARK join
224: 		if (delim_types.size() + 1 == conditions.size()) {
225: 			// the correlated MARK join has one more condition than the amount of correlated columns
226: 			// this is the case in a correlated ANY() expression
227: 			// in this case we need to keep track of additional entries, namely:
228: 			// - (1) the total amount of elements per group
229: 			// - (2) the amount of non-null elements per group
230: 			// we need these to correctly deal with the cases of either:
231: 			// - (1) the group being empty [in which case the result is always false, even if the comparison is NULL]
232: 			// - (2) the group containing a NULL value [in which case FALSE becomes NULL]
233: 			auto &info = result->correlated_mark_join_info;
234: 
235: 			vector<LogicalType> delim_payload_types;
236: 			vector<BoundAggregateExpression *> correlated_aggregates;
237: 			unique_ptr<BoundAggregateExpression> aggr;
238: 
239: 			// jury-rigging the GroupedAggregateHashTable
240: 			// we need a count_star and a count to get counts with and without NULLs
241: 
242: 			FunctionBinder function_binder(context);
243: 			aggr = function_binder.BindAggregateFunction(CountStarFun::GetFunction(), {}, nullptr,
244: 			                                             AggregateType::NON_DISTINCT);
245: 			correlated_aggregates.push_back(&*aggr);
246: 			delim_payload_types.push_back(aggr->return_type);
247: 			info.correlated_aggregates.push_back(std::move(aggr));
248: 
249: 			auto count_fun = CountFun::GetFunction();
250: 			vector<unique_ptr<Expression>> children;
251: 			// this is a dummy but we need it to make the hash table understand whats going on
252: 			children.push_back(make_uniq_base<Expression, BoundReferenceExpression>(count_fun.return_type, 0U));
253: 			aggr = function_binder.BindAggregateFunction(count_fun, std::move(children), nullptr,
254: 			                                             AggregateType::NON_DISTINCT);
255: 			correlated_aggregates.push_back(&*aggr);
256: 			delim_payload_types.push_back(aggr->return_type);
257: 			info.correlated_aggregates.push_back(std::move(aggr));
258: 
259: 			auto &allocator = BufferAllocator::Get(context);
260: 			info.correlated_counts = make_uniq<GroupedAggregateHashTable>(context, allocator, delim_types,
261: 			                                                              delim_payload_types, correlated_aggregates);
262: 			info.correlated_types = delim_types;
263: 			info.group_chunk.Initialize(allocator, delim_types);
264: 			info.result_chunk.Initialize(allocator, delim_payload_types);
265: 		}
266: 	}
267: 	return result;
268: }
269: 
270: unique_ptr<GlobalSinkState> PhysicalHashJoin::GetGlobalSinkState(ClientContext &context) const {
271: 	return make_uniq<HashJoinGlobalSinkState>(*this, context);
272: }
273: 
274: unique_ptr<LocalSinkState> PhysicalHashJoin::GetLocalSinkState(ExecutionContext &context) const {
275: 	auto &gstate = sink_state->Cast<HashJoinGlobalSinkState>();
276: 	return make_uniq<HashJoinLocalSinkState>(*this, context.client, gstate);
277: }
278: 
279: void JoinFilterPushdownInfo::Sink(DataChunk &chunk, JoinFilterLocalState &lstate) const {
280: 	// if we are pushing any filters into a probe-side, compute the min/max over the columns that we are pushing
281: 	for (idx_t pushdown_idx = 0; pushdown_idx < filters.size(); pushdown_idx++) {
282: 		auto &pushdown = filters[pushdown_idx];
283: 		for (idx_t i = 0; i < 2; i++) {
284: 			idx_t aggr_idx = pushdown_idx * 2 + i;
285: 			lstate.local_aggregate_state->Sink(chunk, pushdown.join_condition, aggr_idx);
286: 		}
287: 	}
288: }
289: 
290: SinkResultType PhysicalHashJoin::Sink(ExecutionContext &context, DataChunk &chunk, OperatorSinkInput &input) const {
291: 	auto &lstate = input.local_state.Cast<HashJoinLocalSinkState>();
292: 
293: 	// resolve the join keys for the right chunk
294: 	lstate.join_keys.Reset();
295: 	lstate.join_key_executor.Execute(chunk, lstate.join_keys);
296: 
297: 	if (filter_pushdown) {
298: 		filter_pushdown->Sink(lstate.join_keys, *lstate.local_filter_state);
299: 	}
300: 
301: 	// build the HT
302: 	auto &ht = *lstate.hash_table;
303: 	if (payload_types.empty()) {
304: 		// there are only keys: place an empty chunk in the payload
305: 		lstate.payload_chunk.SetCardinality(chunk.size());
306: 		ht.Build(lstate.append_state, lstate.join_keys, lstate.payload_chunk);
307: 	} else {
308: 		// there are payload columns
309: 		lstate.payload_chunk.Reset();
310: 		lstate.payload_chunk.SetCardinality(chunk);
311: 		for (idx_t i = 0; i < payload_column_idxs.size(); i++) {
312: 			lstate.payload_chunk.data[i].Reference(chunk.data[payload_column_idxs[i]]);
313: 		}
314: 		ht.Build(lstate.append_state, lstate.join_keys, lstate.payload_chunk);
315: 	}
316: 
317: 	return SinkResultType::NEED_MORE_INPUT;
318: }
319: 
320: void JoinFilterPushdownInfo::Combine(JoinFilterGlobalState &gstate, JoinFilterLocalState &lstate) const {
321: 	gstate.global_aggregate_state->Combine(*lstate.local_aggregate_state);
322: }
323: 
324: SinkCombineResultType PhysicalHashJoin::Combine(ExecutionContext &context, OperatorSinkCombineInput &input) const {
325: 	auto &gstate = input.global_state.Cast<HashJoinGlobalSinkState>();
326: 	auto &lstate = input.local_state.Cast<HashJoinLocalSinkState>();
327: 
328: 	lstate.hash_table->GetSinkCollection().FlushAppendState(lstate.append_state);
329: 	auto guard = gstate.Lock();
330: 	gstate.local_hash_tables.push_back(std::move(lstate.hash_table));
331: 	if (gstate.local_hash_tables.size() == gstate.active_local_states) {
332: 		// Set to 0 until PrepareFinalize
333: 		gstate.temporary_memory_state->SetZero();
334: 	}
335: 
336: 	auto &client_profiler = QueryProfiler::Get(context.client);
337: 	context.thread.profiler.Flush(*this);
338: 	client_profiler.Flush(context.thread.profiler);
339: 	if (filter_pushdown) {
340: 		filter_pushdown->Combine(*gstate.global_filter_state, *lstate.local_filter_state);
341: 	}
342: 
343: 	return SinkCombineResultType::FINISHED;
344: }
345: 
346: //===--------------------------------------------------------------------===//
347: // Finalize
348: //===--------------------------------------------------------------------===//
349: static idx_t GetTupleWidth(const vector<LogicalType> &types, bool &all_constant) {
350: 	idx_t tuple_width = 0;
351: 	all_constant = true;
352: 	for (auto &type : types) {
353: 		tuple_width += GetTypeIdSize(type.InternalType());
354: 		all_constant &= TypeIsConstantSize(type.InternalType());
355: 	}
356: 	return tuple_width + AlignValue(types.size()) / 8 + GetTypeIdSize(PhysicalType::UINT64);
357: }
358: 
359: static idx_t GetPartitioningSpaceRequirement(ClientContext &context, const vector<LogicalType> &types,
360:                                              const idx_t radix_bits, const idx_t num_threads) {
361: 	auto &buffer_manager = BufferManager::GetBufferManager(context);
362: 	bool all_constant;
363: 	idx_t tuple_width = GetTupleWidth(types, all_constant);
364: 
365: 	auto tuples_per_block = buffer_manager.GetBlockSize() / tuple_width;
366: 	auto blocks_per_chunk = (STANDARD_VECTOR_SIZE + tuples_per_block) / tuples_per_block + 1;
367: 	if (!all_constant) {
368: 		blocks_per_chunk += 2;
369: 	}
370: 	auto size_per_partition = blocks_per_chunk * buffer_manager.GetBlockAllocSize();
371: 	auto num_partitions = RadixPartitioning::NumberOfPartitions(radix_bits);
372: 
373: 	return num_threads * num_partitions * size_per_partition;
374: }
375: 
376: void PhysicalHashJoin::PrepareFinalize(ClientContext &context, GlobalSinkState &global_state) const {
377: 	auto &gstate = global_state.Cast<HashJoinGlobalSinkState>();
378: 	auto &ht = *gstate.hash_table;
379: 	gstate.total_size =
380: 	    ht.GetTotalSize(gstate.local_hash_tables, gstate.max_partition_size, gstate.max_partition_count);
381: 	bool all_constant;
382: 	gstate.temporary_memory_state->SetMaterializationPenalty(GetTupleWidth(children[0]->types, all_constant));
383: 	gstate.temporary_memory_state->SetRemainingSize(gstate.total_size);
384: }
385: 
386: class HashJoinFinalizeTask : public ExecutorTask {
387: public:
388: 	HashJoinFinalizeTask(shared_ptr<Event> event_p, ClientContext &context, HashJoinGlobalSinkState &sink_p,
389: 	                     idx_t chunk_idx_from_p, idx_t chunk_idx_to_p, bool parallel_p, const PhysicalOperator &op_p)
390: 	    : ExecutorTask(context, std::move(event_p), op_p), sink(sink_p), chunk_idx_from(chunk_idx_from_p),
391: 	      chunk_idx_to(chunk_idx_to_p), parallel(parallel_p) {
392: 	}
393: 
394: 	TaskExecutionResult ExecuteTask(TaskExecutionMode mode) override {
395: 		sink.hash_table->Finalize(chunk_idx_from, chunk_idx_to, parallel);
396: 		event->FinishTask();
397: 		return TaskExecutionResult::TASK_FINISHED;
398: 	}
399: 
400: private:
401: 	HashJoinGlobalSinkState &sink;
402: 	idx_t chunk_idx_from;
403: 	idx_t chunk_idx_to;
404: 	bool parallel;
405: };
406: 
407: class HashJoinFinalizeEvent : public BasePipelineEvent {
408: public:
409: 	HashJoinFinalizeEvent(Pipeline &pipeline_p, HashJoinGlobalSinkState &sink)
410: 	    : BasePipelineEvent(pipeline_p), sink(sink) {
411: 	}
412: 
413: 	HashJoinGlobalSinkState &sink;
414: 
415: public:
416: 	void Schedule() override {
417: 		auto &context = pipeline->GetClientContext();
418: 
419: 		vector<shared_ptr<Task>> finalize_tasks;
420: 		auto &ht = *sink.hash_table;
421: 		const auto chunk_count = ht.GetDataCollection().ChunkCount();
422: 		const auto num_threads = NumericCast<idx_t>(sink.num_threads);
423: 		if (num_threads == 1 || (ht.Count() < PARALLEL_CONSTRUCT_THRESHOLD && !context.config.verify_parallelism)) {
424: 			// Single-threaded finalize
425: 			finalize_tasks.push_back(
426: 			    make_uniq<HashJoinFinalizeTask>(shared_from_this(), context, sink, 0U, chunk_count, false, sink.op));
427: 		} else {
428: 			// Parallel finalize
429: 			auto chunks_per_thread = MaxValue<idx_t>((chunk_count + num_threads - 1) / num_threads, 1);
430: 
431: 			idx_t chunk_idx = 0;
432: 			for (idx_t thread_idx = 0; thread_idx < num_threads; thread_idx++) {
433: 				auto chunk_idx_from = chunk_idx;
434: 				auto chunk_idx_to = MinValue<idx_t>(chunk_idx_from + chunks_per_thread, chunk_count);
435: 				finalize_tasks.push_back(make_uniq<HashJoinFinalizeTask>(shared_from_this(), context, sink,
436: 				                                                         chunk_idx_from, chunk_idx_to, true, sink.op));
437: 				chunk_idx = chunk_idx_to;
438: 				if (chunk_idx == chunk_count) {
439: 					break;
440: 				}
441: 			}
442: 		}
443: 		SetTasks(std::move(finalize_tasks));
444: 	}
445: 
446: 	void FinishEvent() override {
447: 		sink.hash_table->GetDataCollection().VerifyEverythingPinned();
448: 		sink.hash_table->finalized = true;
449: 	}
450: 
451: 	static constexpr const idx_t PARALLEL_CONSTRUCT_THRESHOLD = 1048576;
452: };
453: 
454: void HashJoinGlobalSinkState::ScheduleFinalize(Pipeline &pipeline, Event &event) {
455: 	if (hash_table->Count() == 0) {
456: 		hash_table->finalized = true;
457: 		return;
458: 	}
459: 	hash_table->InitializePointerTable();
460: 	auto new_event = make_shared_ptr<HashJoinFinalizeEvent>(pipeline, *this);
461: 	event.InsertEvent(std::move(new_event));
462: }
463: 
464: void HashJoinGlobalSinkState::InitializeProbeSpill() {
465: 	auto guard = Lock();
466: 	if (!probe_spill) {
467: 		probe_spill = make_uniq<JoinHashTable::ProbeSpill>(*hash_table, context, probe_types);
468: 	}
469: }
470: 
471: class HashJoinRepartitionTask : public ExecutorTask {
472: public:
473: 	HashJoinRepartitionTask(shared_ptr<Event> event_p, ClientContext &context, JoinHashTable &global_ht,
474: 	                        JoinHashTable &local_ht, const PhysicalOperator &op_p)
475: 	    : ExecutorTask(context, std::move(event_p), op_p), global_ht(global_ht), local_ht(local_ht) {
476: 	}
477: 
478: 	TaskExecutionResult ExecuteTask(TaskExecutionMode mode) override {
479: 		local_ht.Repartition(global_ht);
480: 		event->FinishTask();
481: 		return TaskExecutionResult::TASK_FINISHED;
482: 	}
483: 
484: private:
485: 	JoinHashTable &global_ht;
486: 	JoinHashTable &local_ht;
487: };
488: 
489: class HashJoinRepartitionEvent : public BasePipelineEvent {
490: public:
491: 	HashJoinRepartitionEvent(Pipeline &pipeline_p, const PhysicalHashJoin &op_p, HashJoinGlobalSinkState &sink,
492: 	                         vector<unique_ptr<JoinHashTable>> &local_hts)
493: 	    : BasePipelineEvent(pipeline_p), op(op_p), sink(sink), local_hts(local_hts) {
494: 	}
495: 
496: 	const PhysicalHashJoin &op;
497: 	HashJoinGlobalSinkState &sink;
498: 	vector<unique_ptr<JoinHashTable>> &local_hts;
499: 
500: public:
501: 	void Schedule() override {
502: 		D_ASSERT(sink.hash_table->GetRadixBits() > JoinHashTable::INITIAL_RADIX_BITS);
503: 		auto block_size = sink.hash_table->buffer_manager.GetBlockSize();
504: 
505: 		idx_t total_size = 0;
506: 		idx_t total_count = 0;
507: 		for (auto &local_ht : local_hts) {
508: 			auto &sink_collection = local_ht->GetSinkCollection();
509: 			total_size += sink_collection.SizeInBytes();
510: 			total_count += sink_collection.Count();
511: 		}
512: 		auto total_blocks = (total_size + block_size - 1) / block_size;
513: 		auto count_per_block = total_count / total_blocks;
514: 		auto blocks_per_vector = MaxValue<idx_t>(STANDARD_VECTOR_SIZE / count_per_block, 2);
515: 
516: 		// Assume 8 blocks per partition per thread (4 input, 4 output)
517: 		auto partition_multiplier =
518: 		    RadixPartitioning::NumberOfPartitions(sink.hash_table->GetRadixBits() - JoinHashTable::INITIAL_RADIX_BITS);
519: 		auto thread_memory = 2 * blocks_per_vector * partition_multiplier * block_size;
520: 		auto repartition_threads = MaxValue<idx_t>(sink.temporary_memory_state->GetReservation() / thread_memory, 1);
521: 
522: 		if (repartition_threads < local_hts.size()) {
523: 			// Limit the number of threads working on repartitioning based on our memory reservation
524: 			for (idx_t thread_idx = repartition_threads; thread_idx < local_hts.size(); thread_idx++) {
525: 				local_hts[thread_idx % repartition_threads]->Merge(*local_hts[thread_idx]);
526: 			}
527: 			local_hts.resize(repartition_threads);
528: 		}
529: 
530: 		auto &context = pipeline->GetClientContext();
531: 
532: 		vector<shared_ptr<Task>> partition_tasks;
533: 		partition_tasks.reserve(local_hts.size());
534: 		for (auto &local_ht : local_hts) {
535: 			partition_tasks.push_back(
536: 			    make_uniq<HashJoinRepartitionTask>(shared_from_this(), context, *sink.hash_table, *local_ht, op));
537: 		}
538: 		SetTasks(std::move(partition_tasks));
539: 	}
540: 
541: 	void FinishEvent() override {
542: 		local_hts.clear();
543: 
544: 		// Minimum reservation is now the new smallest partition size
545: 		const auto num_partitions = RadixPartitioning::NumberOfPartitions(sink.hash_table->GetRadixBits());
546: 		vector<idx_t> partition_sizes(num_partitions, 0);
547: 		vector<idx_t> partition_counts(num_partitions, 0);
548: 		sink.total_size = sink.hash_table->GetTotalSize(partition_sizes, partition_counts, sink.max_partition_size,
549: 		                                                sink.max_partition_count);
550: 		const auto probe_side_requirement =
551: 		    GetPartitioningSpaceRequirement(sink.context, op.types, sink.hash_table->GetRadixBits(), sink.num_threads);
552: 
553: 		sink.temporary_memory_state->SetMinimumReservation(sink.max_partition_size +
554: 		                                                   JoinHashTable::PointerTableSize(sink.max_partition_count) +
555: 		                                                   probe_side_requirement);
556: 		sink.temporary_memory_state->UpdateReservation(executor.context);
557: 
558: 		sink.hash_table->PrepareExternalFinalize(sink.temporary_memory_state->GetReservation());
559: 		sink.ScheduleFinalize(*pipeline, *this);
560: 	}
561: };
562: 
563: void JoinFilterPushdownInfo::PushFilters(JoinFilterGlobalState &gstate, const PhysicalOperator &op) const {
564: 	// finalize the min/max aggregates
565: 	vector<LogicalType> min_max_types;
566: 	for (auto &aggr_expr : min_max_aggregates) {
567: 		min_max_types.push_back(aggr_expr->return_type);
568: 	}
569: 	DataChunk final_min_max;
570: 	final_min_max.Initialize(Allocator::DefaultAllocator(), min_max_types);
571: 
572: 	gstate.global_aggregate_state->Finalize(final_min_max);
573: 
574: 	// create a filter for each of the aggregates
575: 	for (idx_t filter_idx = 0; filter_idx < filters.size(); filter_idx++) {
576: 		auto &filter = filters[filter_idx];
577: 		auto filter_col_idx = filter.probe_column_index.column_index;
578: 		auto min_idx = filter_idx * 2;
579: 		auto max_idx = min_idx + 1;
580: 
581: 		auto min_val = final_min_max.data[min_idx].GetValue(0);
582: 		auto max_val = final_min_max.data[max_idx].GetValue(0);
583: 		if (min_val.IsNull() || max_val.IsNull()) {
584: 			// min/max is NULL
585: 			// this can happen in case all values in the RHS column are NULL, but they are still pushed into the hash
586: 			// table e.g. because they are part of a RIGHT join
587: 			continue;
588: 		}
589: 		if (Value::NotDistinctFrom(min_val, max_val)) {
590: 			// min = max - generate an equality filter
591: 			auto constant_filter = make_uniq<ConstantFilter>(ExpressionType::COMPARE_EQUAL, std::move(min_val));
592: 			dynamic_filters->PushFilter(op, filter_col_idx, std::move(constant_filter));
593: 		} else {
594: 			// min != max - generate a range filter
595: 			auto greater_equals =
596: 			    make_uniq<ConstantFilter>(ExpressionType::COMPARE_GREATERTHANOREQUALTO, std::move(min_val));
597: 			dynamic_filters->PushFilter(op, filter_col_idx, std::move(greater_equals));
598: 			auto less_equals = make_uniq<ConstantFilter>(ExpressionType::COMPARE_LESSTHANOREQUALTO, std::move(max_val));
599: 			dynamic_filters->PushFilter(op, filter_col_idx, std::move(less_equals));
600: 		}
601: 		// not null filter
602: 		dynamic_filters->PushFilter(op, filter_col_idx, make_uniq<IsNotNullFilter>());
603: 	}
604: }
605: 
606: SinkFinalizeType PhysicalHashJoin::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
607:                                             OperatorSinkFinalizeInput &input) const {
608: 	auto &sink = input.global_state.Cast<HashJoinGlobalSinkState>();
609: 	auto &ht = *sink.hash_table;
610: 
611: 	sink.temporary_memory_state->UpdateReservation(context);
612: 	sink.external = sink.temporary_memory_state->GetReservation() < sink.total_size;
613: 	if (sink.external) {
614: 		// External Hash Join
615: 		sink.perfect_join_executor.reset();
616: 
617: 		const auto max_partition_ht_size =
618: 		    sink.max_partition_size + JoinHashTable::PointerTableSize(sink.max_partition_count);
619: 		if (max_partition_ht_size > sink.temporary_memory_state->GetReservation()) {
620: 			// We have to repartition
621: 			ht.SetRepartitionRadixBits(sink.temporary_memory_state->GetReservation(), sink.max_partition_size,
622: 			                           sink.max_partition_count);
623: 			auto new_event = make_shared_ptr<HashJoinRepartitionEvent>(pipeline, *this, sink, sink.local_hash_tables);
624: 			event.InsertEvent(std::move(new_event));
625: 		} else {
626: 			// No repartitioning! We do need some space for partitioning the probe-side, though
627: 			const auto probe_side_requirement =
628: 			    GetPartitioningSpaceRequirement(context, children[0]->types, ht.GetRadixBits(), sink.num_threads);
629: 			sink.temporary_memory_state->SetMinimumReservation(max_partition_ht_size + probe_side_requirement);
630: 			for (auto &local_ht : sink.local_hash_tables) {
631: 				ht.Merge(*local_ht);
632: 			}
633: 			sink.local_hash_tables.clear();
634: 			sink.hash_table->PrepareExternalFinalize(sink.temporary_memory_state->GetReservation());
635: 			sink.ScheduleFinalize(pipeline, event);
636: 		}
637: 		sink.finalized = true;
638: 		return SinkFinalizeType::READY;
639: 	}
640: 
641: 	// In-memory Hash Join
642: 	for (auto &local_ht : sink.local_hash_tables) {
643: 		ht.Merge(*local_ht);
644: 	}
645: 	sink.local_hash_tables.clear();
646: 	ht.Unpartition();
647: 
648: 	if (filter_pushdown && ht.Count() > 0) {
649: 		filter_pushdown->PushFilters(*sink.global_filter_state, *this);
650: 	}
651: 
652: 	// check for possible perfect hash table
653: 	auto use_perfect_hash = sink.perfect_join_executor->CanDoPerfectHashJoin();
654: 	if (use_perfect_hash) {
655: 		D_ASSERT(ht.equality_types.size() == 1);
656: 		auto key_type = ht.equality_types[0];
657: 		use_perfect_hash = sink.perfect_join_executor->BuildPerfectHashTable(key_type);
658: 	}
659: 	// In case of a large build side or duplicates, use regular hash join
660: 	if (!use_perfect_hash) {
661: 		sink.perfect_join_executor.reset();
662: 		sink.ScheduleFinalize(pipeline, event);
663: 	}
664: 	sink.finalized = true;
665: 	if (ht.Count() == 0 && EmptyResultIfRHSIsEmpty()) {
666: 		return SinkFinalizeType::NO_OUTPUT_POSSIBLE;
667: 	}
668: 	return SinkFinalizeType::READY;
669: }
670: 
671: //===--------------------------------------------------------------------===//
672: // Operator
673: //===--------------------------------------------------------------------===//
674: class HashJoinOperatorState : public CachingOperatorState {
675: public:
676: 	explicit HashJoinOperatorState(ClientContext &context, HashJoinGlobalSinkState &sink)
677: 	    : probe_executor(context), scan_structure(*sink.hash_table, join_key_state) {
678: 	}
679: 
680: 	DataChunk join_keys;
681: 	TupleDataChunkState join_key_state;
682: 
683: 	ExpressionExecutor probe_executor;
684: 	JoinHashTable::ScanStructure scan_structure;
685: 	unique_ptr<OperatorState> perfect_hash_join_state;
686: 
687: 	JoinHashTable::ProbeSpillLocalAppendState spill_state;
688: 	JoinHashTable::ProbeState probe_state;
689: 	//! Chunk to sink data into for external join
690: 	DataChunk spill_chunk;
691: 
692: public:
693: 	void Finalize(const PhysicalOperator &op, ExecutionContext &context) override {
694: 		context.thread.profiler.Flush(op);
695: 	}
696: };
697: 
698: unique_ptr<OperatorState> PhysicalHashJoin::GetOperatorState(ExecutionContext &context) const {
699: 	auto &allocator = BufferAllocator::Get(context.client);
700: 	auto &sink = sink_state->Cast<HashJoinGlobalSinkState>();
701: 	auto state = make_uniq<HashJoinOperatorState>(context.client, sink);
702: 	if (sink.perfect_join_executor) {
703: 		state->perfect_hash_join_state = sink.perfect_join_executor->GetOperatorState(context);
704: 	} else {
705: 		state->join_keys.Initialize(allocator, condition_types);
706: 		for (auto &cond : conditions) {
707: 			state->probe_executor.AddExpression(*cond.left);
708: 		}
709: 		TupleDataCollection::InitializeChunkState(state->join_key_state, condition_types);
710: 	}
711: 	if (sink.external) {
712: 		state->spill_chunk.Initialize(allocator, sink.probe_types);
713: 		sink.InitializeProbeSpill();
714: 	}
715: 
716: 	return std::move(state);
717: }
718: 
719: OperatorResultType PhysicalHashJoin::ExecuteInternal(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
720:                                                      GlobalOperatorState &gstate, OperatorState &state_p) const {
721: 	auto &state = state_p.Cast<HashJoinOperatorState>();
722: 	auto &sink = sink_state->Cast<HashJoinGlobalSinkState>();
723: 	D_ASSERT(sink.finalized);
724: 	D_ASSERT(!sink.scanned_data);
725: 
726: 	if (sink.hash_table->Count() == 0) {
727: 		if (EmptyResultIfRHSIsEmpty()) {
728: 			return OperatorResultType::FINISHED;
729: 		}
730: 		ConstructEmptyJoinResult(sink.hash_table->join_type, sink.hash_table->has_null, input, chunk);
731: 		return OperatorResultType::NEED_MORE_INPUT;
732: 	}
733: 
734: 	if (sink.perfect_join_executor) {
735: 		D_ASSERT(!sink.external);
736: 		return sink.perfect_join_executor->ProbePerfectHashTable(context, input, chunk, *state.perfect_hash_join_state);
737: 	}
738: 
739: 	if (sink.external && !state.initialized) {
740: 		// some initialization for external hash join
741: 		if (!sink.probe_spill) {
742: 			sink.InitializeProbeSpill();
743: 		}
744: 		state.spill_state = sink.probe_spill->RegisterThread();
745: 		state.initialized = true;
746: 	}
747: 
748: 	if (state.scan_structure.is_null) {
749: 		// probe the HT, start by resolving the join keys for the left chunk
750: 		state.join_keys.Reset();
751: 		state.probe_executor.Execute(input, state.join_keys);
752: 
753: 		// perform the actual probe
754: 		if (sink.external) {
755: 			sink.hash_table->ProbeAndSpill(state.scan_structure, state.join_keys, state.join_key_state,
756: 			                               state.probe_state, input, *sink.probe_spill, state.spill_state,
757: 			                               state.spill_chunk);
758: 		} else {
759: 			sink.hash_table->Probe(state.scan_structure, state.join_keys, state.join_key_state, state.probe_state);
760: 		}
761: 	}
762: 	state.scan_structure.Next(state.join_keys, input, chunk);
763: 
764: 	if (state.scan_structure.PointersExhausted() && chunk.size() == 0) {
765: 		state.scan_structure.is_null = true;
766: 		return OperatorResultType::NEED_MORE_INPUT;
767: 	}
768: 	return OperatorResultType::HAVE_MORE_OUTPUT;
769: }
770: 
771: //===--------------------------------------------------------------------===//
772: // Source
773: //===--------------------------------------------------------------------===//
774: enum class HashJoinSourceStage : uint8_t { INIT, BUILD, PROBE, SCAN_HT, DONE };
775: 
776: class HashJoinLocalSourceState;
777: 
778: class HashJoinGlobalSourceState : public GlobalSourceState {
779: public:
780: 	HashJoinGlobalSourceState(const PhysicalHashJoin &op, const ClientContext &context);
781: 
782: 	//! Initialize this source state using the info in the sink
783: 	void Initialize(HashJoinGlobalSinkState &sink);
784: 	//! Try to prepare the next stage
785: 	bool TryPrepareNextStage(HashJoinGlobalSinkState &sink);
786: 	//! Prepare the next build/probe/scan_ht stage for external hash join (must hold lock)
787: 	void PrepareBuild(HashJoinGlobalSinkState &sink);
788: 	void PrepareProbe(HashJoinGlobalSinkState &sink);
789: 	void PrepareScanHT(HashJoinGlobalSinkState &sink);
790: 	//! Assigns a task to a local source state
791: 	bool AssignTask(HashJoinGlobalSinkState &sink, HashJoinLocalSourceState &lstate);
792: 
793: 	idx_t MaxThreads() override {
794: 		D_ASSERT(op.sink_state);
795: 		auto &gstate = op.sink_state->Cast<HashJoinGlobalSinkState>();
796: 
797: 		idx_t count;
798: 		if (gstate.probe_spill) {
799: 			count = probe_count;
800: 		} else if (PropagatesBuildSide(op.join_type)) {
801: 			count = gstate.hash_table->Count();
802: 		} else {
803: 			return 0;
804: 		}
805: 		return count / ((idx_t)STANDARD_VECTOR_SIZE * parallel_scan_chunk_count);
806: 	}
807: 
808: public:
809: 	const PhysicalHashJoin &op;
810: 
811: 	//! For synchronizing the external hash join
812: 	atomic<HashJoinSourceStage> global_stage;
813: 
814: 	//! For HT build synchronization
815: 	idx_t build_chunk_idx = DConstants::INVALID_INDEX;
816: 	idx_t build_chunk_count;
817: 	idx_t build_chunk_done;
818: 	idx_t build_chunks_per_thread = DConstants::INVALID_INDEX;
819: 
820: 	//! For probe synchronization
821: 	atomic<idx_t> probe_chunk_count;
822: 	idx_t probe_chunk_done;
823: 
824: 	//! To determine the number of threads
825: 	idx_t probe_count;
826: 	idx_t parallel_scan_chunk_count;
827: 
828: 	//! For full/outer synchronization
829: 	idx_t full_outer_chunk_idx = DConstants::INVALID_INDEX;
830: 	atomic<idx_t> full_outer_chunk_count;
831: 	atomic<idx_t> full_outer_chunk_done;
832: 	idx_t full_outer_chunks_per_thread = DConstants::INVALID_INDEX;
833: 
834: 	vector<InterruptState> blocked_tasks;
835: };
836: 
837: class HashJoinLocalSourceState : public LocalSourceState {
838: public:
839: 	HashJoinLocalSourceState(const PhysicalHashJoin &op, const HashJoinGlobalSinkState &sink, Allocator &allocator);
840: 
841: 	//! Do the work this thread has been assigned
842: 	void ExecuteTask(HashJoinGlobalSinkState &sink, HashJoinGlobalSourceState &gstate, DataChunk &chunk);
843: 	//! Whether this thread has finished the work it has been assigned
844: 	bool TaskFinished() const;
845: 	//! Build, probe and scan for external hash join
846: 	void ExternalBuild(HashJoinGlobalSinkState &sink, HashJoinGlobalSourceState &gstate);
847: 	void ExternalProbe(HashJoinGlobalSinkState &sink, HashJoinGlobalSourceState &gstate, DataChunk &chunk);
848: 	void ExternalScanHT(HashJoinGlobalSinkState &sink, HashJoinGlobalSourceState &gstate, DataChunk &chunk);
849: 
850: public:
851: 	//! The stage that this thread was assigned work for
852: 	HashJoinSourceStage local_stage;
853: 	//! Vector with pointers here so we don't have to re-initialize
854: 	Vector addresses;
855: 
856: 	//! Chunks assigned to this thread for building the pointer table
857: 	idx_t build_chunk_idx_from = DConstants::INVALID_INDEX;
858: 	idx_t build_chunk_idx_to = DConstants::INVALID_INDEX;
859: 
860: 	//! Local scan state for probe spill
861: 	ColumnDataConsumerScanState probe_local_scan;
862: 	//! Chunks for holding the scanned probe collection
863: 	DataChunk probe_chunk;
864: 	DataChunk join_keys;
865: 	DataChunk payload;
866: 	TupleDataChunkState join_key_state;
867: 
868: 	//! Column indices to easily reference the join keys/payload columns in probe_chunk
869: 	vector<idx_t> join_key_indices;
870: 	vector<idx_t> payload_indices;
871: 	//! Scan structure for the external probe
872: 	JoinHashTable::ScanStructure scan_structure;
873: 	JoinHashTable::ProbeState probe_state;
874: 	bool empty_ht_probe_in_progress = false;
875: 
876: 	//! Chunks assigned to this thread for a full/outer scan
877: 	idx_t full_outer_chunk_idx_from = DConstants::INVALID_INDEX;
878: 	idx_t full_outer_chunk_idx_to = DConstants::INVALID_INDEX;
879: 	unique_ptr<JoinHTScanState> full_outer_scan_state;
880: };
881: 
882: unique_ptr<GlobalSourceState> PhysicalHashJoin::GetGlobalSourceState(ClientContext &context) const {
883: 	return make_uniq<HashJoinGlobalSourceState>(*this, context);
884: }
885: 
886: unique_ptr<LocalSourceState> PhysicalHashJoin::GetLocalSourceState(ExecutionContext &context,
887:                                                                    GlobalSourceState &gstate) const {
888: 	return make_uniq<HashJoinLocalSourceState>(*this, sink_state->Cast<HashJoinGlobalSinkState>(),
889: 	                                           BufferAllocator::Get(context.client));
890: }
891: 
892: HashJoinGlobalSourceState::HashJoinGlobalSourceState(const PhysicalHashJoin &op, const ClientContext &context)
893:     : op(op), global_stage(HashJoinSourceStage::INIT), build_chunk_count(0), build_chunk_done(0), probe_chunk_count(0),
894:       probe_chunk_done(0), probe_count(op.children[0]->estimated_cardinality),
895:       parallel_scan_chunk_count(context.config.verify_parallelism ? 1 : 120) {
896: }
897: 
898: void HashJoinGlobalSourceState::Initialize(HashJoinGlobalSinkState &sink) {
899: 	auto guard = Lock();
900: 	if (global_stage != HashJoinSourceStage::INIT) {
901: 		// Another thread initialized
902: 		return;
903: 	}
904: 
905: 	// Finalize the probe spill
906: 	if (sink.probe_spill) {
907: 		sink.probe_spill->Finalize();
908: 	}
909: 
910: 	global_stage = HashJoinSourceStage::PROBE;
911: 	TryPrepareNextStage(sink);
912: }
913: 
914: bool HashJoinGlobalSourceState::TryPrepareNextStage(HashJoinGlobalSinkState &sink) {
915: 	switch (global_stage.load()) {
916: 	case HashJoinSourceStage::BUILD:
917: 		if (build_chunk_done == build_chunk_count) {
918: 			sink.hash_table->GetDataCollection().VerifyEverythingPinned();
919: 			sink.hash_table->finalized = true;
920: 			PrepareProbe(sink);
921: 			return true;
922: 		}
923: 		break;
924: 	case HashJoinSourceStage::PROBE:
925: 		if (probe_chunk_done == probe_chunk_count) {
926: 			if (PropagatesBuildSide(op.join_type)) {
927: 				PrepareScanHT(sink);
928: 			} else {
929: 				PrepareBuild(sink);
930: 			}
931: 			return true;
932: 		}
933: 		break;
934: 	case HashJoinSourceStage::SCAN_HT:
935: 		if (full_outer_chunk_done == full_outer_chunk_count) {
936: 			PrepareBuild(sink);
937: 			return true;
938: 		}
939: 		break;
940: 	default:
941: 		break;
942: 	}
943: 	return false;
944: }
945: 
946: void HashJoinGlobalSourceState::PrepareBuild(HashJoinGlobalSinkState &sink) {
947: 	D_ASSERT(global_stage != HashJoinSourceStage::BUILD);
948: 	auto &ht = *sink.hash_table;
949: 
950: 	// Update remaining size
951: 	sink.temporary_memory_state->SetRemainingSizeAndUpdateReservation(sink.context, ht.GetRemainingSize());
952: 
953: 	// Try to put the next partitions in the block collection of the HT
954: 	if (!sink.external || !ht.PrepareExternalFinalize(sink.temporary_memory_state->GetReservation())) {
955: 		global_stage = HashJoinSourceStage::DONE;
956: 		sink.temporary_memory_state->SetZero();
957: 		return;
958: 	}
959: 
960: 	auto &data_collection = ht.GetDataCollection();
961: 	if (data_collection.Count() == 0 && op.EmptyResultIfRHSIsEmpty()) {
962: 		PrepareBuild(sink);
963: 		return;
964: 	}
965: 
966: 	build_chunk_idx = 0;
967: 	build_chunk_count = data_collection.ChunkCount();
968: 	build_chunk_done = 0;
969: 
970: 	build_chunks_per_thread = MaxValue<idx_t>((build_chunk_count + sink.num_threads - 1) / sink.num_threads, 1);
971: 
972: 	ht.InitializePointerTable();
973: 
974: 	global_stage = HashJoinSourceStage::BUILD;
975: }
976: 
977: void HashJoinGlobalSourceState::PrepareProbe(HashJoinGlobalSinkState &sink) {
978: 	sink.probe_spill->PrepareNextProbe();
979: 	const auto &consumer = *sink.probe_spill->consumer;
980: 
981: 	probe_chunk_count = consumer.Count() == 0 ? 0 : consumer.ChunkCount();
982: 	probe_chunk_done = 0;
983: 
984: 	global_stage = HashJoinSourceStage::PROBE;
985: 	if (probe_chunk_count == 0) {
986: 		TryPrepareNextStage(sink);
987: 		return;
988: 	}
989: }
990: 
991: void HashJoinGlobalSourceState::PrepareScanHT(HashJoinGlobalSinkState &sink) {
992: 	D_ASSERT(global_stage != HashJoinSourceStage::SCAN_HT);
993: 	auto &ht = *sink.hash_table;
994: 
995: 	auto &data_collection = ht.GetDataCollection();
996: 	full_outer_chunk_idx = 0;
997: 	full_outer_chunk_count = data_collection.ChunkCount();
998: 	full_outer_chunk_done = 0;
999: 
1000: 	full_outer_chunks_per_thread =
1001: 	    MaxValue<idx_t>((full_outer_chunk_count + sink.num_threads - 1) / sink.num_threads, 1);
1002: 
1003: 	global_stage = HashJoinSourceStage::SCAN_HT;
1004: }
1005: 
1006: bool HashJoinGlobalSourceState::AssignTask(HashJoinGlobalSinkState &sink, HashJoinLocalSourceState &lstate) {
1007: 	D_ASSERT(lstate.TaskFinished());
1008: 
1009: 	auto guard = Lock();
1010: 	switch (global_stage.load()) {
1011: 	case HashJoinSourceStage::BUILD:
1012: 		if (build_chunk_idx != build_chunk_count) {
1013: 			lstate.local_stage = global_stage;
1014: 			lstate.build_chunk_idx_from = build_chunk_idx;
1015: 			build_chunk_idx = MinValue<idx_t>(build_chunk_count, build_chunk_idx + build_chunks_per_thread);
1016: 			lstate.build_chunk_idx_to = build_chunk_idx;
1017: 			return true;
1018: 		}
1019: 		break;
1020: 	case HashJoinSourceStage::PROBE:
1021: 		if (sink.probe_spill->consumer && sink.probe_spill->consumer->AssignChunk(lstate.probe_local_scan)) {
1022: 			lstate.local_stage = global_stage;
1023: 			lstate.empty_ht_probe_in_progress = false;
1024: 			return true;
1025: 		}
1026: 		break;
1027: 	case HashJoinSourceStage::SCAN_HT:
1028: 		if (full_outer_chunk_idx != full_outer_chunk_count) {
1029: 			lstate.local_stage = global_stage;
1030: 			lstate.full_outer_chunk_idx_from = full_outer_chunk_idx;
1031: 			full_outer_chunk_idx =
1032: 			    MinValue<idx_t>(full_outer_chunk_count, full_outer_chunk_idx + full_outer_chunks_per_thread);
1033: 			lstate.full_outer_chunk_idx_to = full_outer_chunk_idx;
1034: 			return true;
1035: 		}
1036: 		break;
1037: 	case HashJoinSourceStage::DONE:
1038: 		break;
1039: 	default:
1040: 		throw InternalException("Unexpected HashJoinSourceStage in AssignTask!");
1041: 	}
1042: 	return false;
1043: }
1044: 
1045: HashJoinLocalSourceState::HashJoinLocalSourceState(const PhysicalHashJoin &op, const HashJoinGlobalSinkState &sink,
1046:                                                    Allocator &allocator)
1047:     : local_stage(HashJoinSourceStage::INIT), addresses(LogicalType::POINTER),
1048:       scan_structure(*sink.hash_table, join_key_state) {
1049: 	auto &chunk_state = probe_local_scan.current_chunk_state;
1050: 	chunk_state.properties = ColumnDataScanProperties::ALLOW_ZERO_COPY;
1051: 
1052: 	probe_chunk.Initialize(allocator, sink.probe_types);
1053: 	join_keys.Initialize(allocator, op.condition_types);
1054: 	payload.Initialize(allocator, op.children[0]->types);
1055: 	TupleDataCollection::InitializeChunkState(join_key_state, op.condition_types);
1056: 
1057: 	// Store the indices of the columns to reference them easily
1058: 	idx_t col_idx = 0;
1059: 	for (; col_idx < op.condition_types.size(); col_idx++) {
1060: 		join_key_indices.push_back(col_idx);
1061: 	}
1062: 	for (; col_idx < sink.probe_types.size() - 1; col_idx++) {
1063: 		payload_indices.push_back(col_idx);
1064: 	}
1065: }
1066: 
1067: void HashJoinLocalSourceState::ExecuteTask(HashJoinGlobalSinkState &sink, HashJoinGlobalSourceState &gstate,
1068:                                            DataChunk &chunk) {
1069: 	switch (local_stage) {
1070: 	case HashJoinSourceStage::BUILD:
1071: 		ExternalBuild(sink, gstate);
1072: 		break;
1073: 	case HashJoinSourceStage::PROBE:
1074: 		ExternalProbe(sink, gstate, chunk);
1075: 		break;
1076: 	case HashJoinSourceStage::SCAN_HT:
1077: 		ExternalScanHT(sink, gstate, chunk);
1078: 		break;
1079: 	default:
1080: 		throw InternalException("Unexpected HashJoinSourceStage in ExecuteTask!");
1081: 	}
1082: }
1083: 
1084: bool HashJoinLocalSourceState::TaskFinished() const {
1085: 	switch (local_stage) {
1086: 	case HashJoinSourceStage::INIT:
1087: 	case HashJoinSourceStage::BUILD:
1088: 		return true;
1089: 	case HashJoinSourceStage::PROBE:
1090: 		return scan_structure.is_null && !empty_ht_probe_in_progress;
1091: 	case HashJoinSourceStage::SCAN_HT:
1092: 		return full_outer_scan_state == nullptr;
1093: 	default:
1094: 		throw InternalException("Unexpected HashJoinSourceStage in TaskFinished!");
1095: 	}
1096: }
1097: 
1098: void HashJoinLocalSourceState::ExternalBuild(HashJoinGlobalSinkState &sink, HashJoinGlobalSourceState &gstate) {
1099: 	D_ASSERT(local_stage == HashJoinSourceStage::BUILD);
1100: 
1101: 	auto &ht = *sink.hash_table;
1102: 	ht.Finalize(build_chunk_idx_from, build_chunk_idx_to, true);
1103: 
1104: 	auto guard = gstate.Lock();
1105: 	gstate.build_chunk_done += build_chunk_idx_to - build_chunk_idx_from;
1106: }
1107: 
1108: void HashJoinLocalSourceState::ExternalProbe(HashJoinGlobalSinkState &sink, HashJoinGlobalSourceState &gstate,
1109:                                              DataChunk &chunk) {
1110: 	D_ASSERT(local_stage == HashJoinSourceStage::PROBE && sink.hash_table->finalized);
1111: 
1112: 	if (!scan_structure.is_null) {
1113: 		// Still have elements remaining (i.e. we got >STANDARD_VECTOR_SIZE elements in the previous probe)
1114: 		scan_structure.Next(join_keys, payload, chunk);
1115: 		if (chunk.size() != 0 || !scan_structure.PointersExhausted()) {
1116: 			return;
1117: 		}
1118: 	}
1119: 
1120: 	if (!scan_structure.is_null || empty_ht_probe_in_progress) {
1121: 		// Previous probe is done
1122: 		scan_structure.is_null = true;
1123: 		empty_ht_probe_in_progress = false;
1124: 		sink.probe_spill->consumer->FinishChunk(probe_local_scan);
1125: 		auto guard = gstate.Lock();
1126: 		gstate.probe_chunk_done++;
1127: 		return;
1128: 	}
1129: 
1130: 	// Scan input chunk for next probe
1131: 	sink.probe_spill->consumer->ScanChunk(probe_local_scan, probe_chunk);
1132: 
1133: 	// Get the probe chunk columns/hashes
1134: 	join_keys.ReferenceColumns(probe_chunk, join_key_indices);
1135: 	payload.ReferenceColumns(probe_chunk, payload_indices);
1136: 	auto precomputed_hashes = &probe_chunk.data.back();
1137: 
1138: 	if (sink.hash_table->Count() == 0 && !gstate.op.EmptyResultIfRHSIsEmpty()) {
1139: 		gstate.op.ConstructEmptyJoinResult(sink.hash_table->join_type, sink.hash_table->has_null, payload, chunk);
1140: 		empty_ht_probe_in_progress = true;
1141: 		return;
1142: 	}
1143: 
1144: 	// Perform the probe
1145: 	sink.hash_table->Probe(scan_structure, join_keys, join_key_state, probe_state, precomputed_hashes);
1146: 	scan_structure.Next(join_keys, payload, chunk);
1147: }
1148: 
1149: void HashJoinLocalSourceState::ExternalScanHT(HashJoinGlobalSinkState &sink, HashJoinGlobalSourceState &gstate,
1150:                                               DataChunk &chunk) {
1151: 	D_ASSERT(local_stage == HashJoinSourceStage::SCAN_HT);
1152: 
1153: 	if (!full_outer_scan_state) {
1154: 		full_outer_scan_state = make_uniq<JoinHTScanState>(sink.hash_table->GetDataCollection(),
1155: 		                                                   full_outer_chunk_idx_from, full_outer_chunk_idx_to);
1156: 	}
1157: 	sink.hash_table->ScanFullOuter(*full_outer_scan_state, addresses, chunk);
1158: 
1159: 	if (chunk.size() == 0) {
1160: 		full_outer_scan_state = nullptr;
1161: 		auto guard = gstate.Lock();
1162: 		gstate.full_outer_chunk_done += full_outer_chunk_idx_to - full_outer_chunk_idx_from;
1163: 	}
1164: }
1165: 
1166: SourceResultType PhysicalHashJoin::GetData(ExecutionContext &context, DataChunk &chunk,
1167:                                            OperatorSourceInput &input) const {
1168: 	auto &sink = sink_state->Cast<HashJoinGlobalSinkState>();
1169: 	auto &gstate = input.global_state.Cast<HashJoinGlobalSourceState>();
1170: 	auto &lstate = input.local_state.Cast<HashJoinLocalSourceState>();
1171: 	sink.scanned_data = true;
1172: 
1173: 	if (!sink.external && !PropagatesBuildSide(join_type)) {
1174: 		auto guard = gstate.Lock();
1175: 		if (gstate.global_stage != HashJoinSourceStage::DONE) {
1176: 			gstate.global_stage = HashJoinSourceStage::DONE;
1177: 			sink.hash_table->Reset();
1178: 			sink.temporary_memory_state->SetZero();
1179: 		}
1180: 		return SourceResultType::FINISHED;
1181: 	}
1182: 
1183: 	if (gstate.global_stage == HashJoinSourceStage::INIT) {
1184: 		gstate.Initialize(sink);
1185: 	}
1186: 
1187: 	// Any call to GetData must produce tuples, otherwise the pipeline executor thinks that we're done
1188: 	// Therefore, we loop until we've produced tuples, or until the operator is actually done
1189: 	while (gstate.global_stage != HashJoinSourceStage::DONE && chunk.size() == 0) {
1190: 		if (!lstate.TaskFinished() || gstate.AssignTask(sink, lstate)) {
1191: 			lstate.ExecuteTask(sink, gstate, chunk);
1192: 		} else {
1193: 			auto guard = gstate.Lock();
1194: 			if (gstate.TryPrepareNextStage(sink) || gstate.global_stage == HashJoinSourceStage::DONE) {
1195: 				gstate.UnblockTasks(guard);
1196: 			} else {
1197: 				return gstate.BlockSource(guard, input.interrupt_state);
1198: 			}
1199: 		}
1200: 	}
1201: 
1202: 	return chunk.size() == 0 ? SourceResultType::FINISHED : SourceResultType::HAVE_MORE_OUTPUT;
1203: }
1204: 
1205: double PhysicalHashJoin::GetProgress(ClientContext &context, GlobalSourceState &gstate_p) const {
1206: 	auto &sink = sink_state->Cast<HashJoinGlobalSinkState>();
1207: 	auto &gstate = gstate_p.Cast<HashJoinGlobalSourceState>();
1208: 
1209: 	if (!sink.external) {
1210: 		if (PropagatesBuildSide(join_type)) {
1211: 			return static_cast<double>(gstate.full_outer_chunk_done) /
1212: 			       static_cast<double>(gstate.full_outer_chunk_count) * 100.0;
1213: 		}
1214: 		return 100.0;
1215: 	}
1216: 
1217: 	auto num_partitions = static_cast<double>(RadixPartitioning::NumberOfPartitions(sink.hash_table->GetRadixBits()));
1218: 	auto partition_start = static_cast<double>(sink.hash_table->GetPartitionStart());
1219: 	auto partition_end = static_cast<double>(sink.hash_table->GetPartitionEnd());
1220: 
1221: 	// This many partitions are fully done
1222: 	auto progress = partition_start / num_partitions;
1223: 
1224: 	auto probe_chunk_done = static_cast<double>(gstate.probe_chunk_done);
1225: 	auto probe_chunk_count = static_cast<double>(gstate.probe_chunk_count);
1226: 	if (probe_chunk_count != 0) {
1227: 		// Progress of the current round of probing, weighed by the number of partitions
1228: 		auto probe_progress = probe_chunk_done / probe_chunk_count;
1229: 		// Add it to the progress, weighed by the number of partitions in the current round
1230: 		progress += (partition_end - partition_start) / num_partitions * probe_progress;
1231: 	}
1232: 
1233: 	return progress * 100.0;
1234: }
1235: 
1236: InsertionOrderPreservingMap<string> PhysicalHashJoin::ParamsToString() const {
1237: 	InsertionOrderPreservingMap<string> result;
1238: 	result["Join Type"] = EnumUtil::ToString(join_type);
1239: 
1240: 	string condition_info;
1241: 	for (idx_t i = 0; i < conditions.size(); i++) {
1242: 		auto &join_condition = conditions[i];
1243: 		if (i > 0) {
1244: 			condition_info += "\n";
1245: 		}
1246: 		condition_info +=
1247: 		    StringUtil::Format("%s %s %s", join_condition.left->GetName(),
1248: 		                       ExpressionTypeToOperator(join_condition.comparison), join_condition.right->GetName());
1249: 	}
1250: 	result["Conditions"] = condition_info;
1251: 
1252: 	if (perfect_join_statistics.is_build_small) {
1253: 		// perfect hash join
1254: 		result["Build Min"] = perfect_join_statistics.build_min.ToString();
1255: 		result["Build Max"] = perfect_join_statistics.build_max.ToString();
1256: 	}
1257: 	SetEstimatedCardinality(result, estimated_cardinality);
1258: 	return result;
1259: }
1260: 
1261: } // namespace duckdb
[end of src/execution/operator/join/physical_hash_join.cpp]
[start of src/execution/radix_partitioned_hashtable.cpp]
1: #include "duckdb/execution/radix_partitioned_hashtable.hpp"
2: 
3: #include "duckdb/common/radix_partitioning.hpp"
4: #include "duckdb/common/row_operations/row_operations.hpp"
5: #include "duckdb/common/types/row/tuple_data_collection.hpp"
6: #include "duckdb/common/types/row/tuple_data_iterator.hpp"
7: #include "duckdb/execution/aggregate_hashtable.hpp"
8: #include "duckdb/execution/executor.hpp"
9: #include "duckdb/execution/ht_entry.hpp"
10: #include "duckdb/execution/operator/aggregate/physical_hash_aggregate.hpp"
11: #include "duckdb/main/config.hpp"
12: #include "duckdb/parallel/event.hpp"
13: #include "duckdb/planner/expression/bound_reference_expression.hpp"
14: #include "duckdb/storage/temporary_memory_manager.hpp"
15: 
16: namespace duckdb {
17: 
18: RadixPartitionedHashTable::RadixPartitionedHashTable(GroupingSet &grouping_set_p, const GroupedAggregateData &op_p)
19:     : grouping_set(grouping_set_p), op(op_p) {
20: 	auto groups_count = op.GroupCount();
21: 	for (idx_t i = 0; i < groups_count; i++) {
22: 		if (grouping_set.find(i) == grouping_set.end()) {
23: 			null_groups.push_back(i);
24: 		}
25: 	}
26: 	if (grouping_set.empty()) {
27: 		// Fake a single group with a constant value for aggregation without groups
28: 		group_types.emplace_back(LogicalType::TINYINT);
29: 	}
30: 	for (auto &entry : grouping_set) {
31: 		D_ASSERT(entry < op.group_types.size());
32: 		group_types.push_back(op.group_types[entry]);
33: 	}
34: 	SetGroupingValues();
35: 
36: 	auto group_types_copy = group_types;
37: 	group_types_copy.emplace_back(LogicalType::HASH);
38: 	layout.Initialize(std::move(group_types_copy), AggregateObject::CreateAggregateObjects(op.bindings));
39: }
40: 
41: void RadixPartitionedHashTable::SetGroupingValues() {
42: 	// Compute the GROUPING values:
43: 	// For each parameter to the GROUPING clause, we check if the hash table groups on this particular group
44: 	// If it does, we return 0, otherwise we return 1
45: 	// We then use bitshifts to combine these values
46: 	auto &grouping_functions = op.GetGroupingFunctions();
47: 	for (auto &grouping : grouping_functions) {
48: 		int64_t grouping_value = 0;
49: 		D_ASSERT(grouping.size() < sizeof(int64_t) * 8);
50: 		for (idx_t i = 0; i < grouping.size(); i++) {
51: 			if (grouping_set.find(grouping[i]) == grouping_set.end()) {
52: 				// We don't group on this value!
53: 				grouping_value += (int64_t)1 << (grouping.size() - (i + 1));
54: 			}
55: 		}
56: 		grouping_values.push_back(Value::BIGINT(grouping_value));
57: 	}
58: }
59: 
60: const TupleDataLayout &RadixPartitionedHashTable::GetLayout() const {
61: 	return layout;
62: }
63: 
64: unique_ptr<GroupedAggregateHashTable> RadixPartitionedHashTable::CreateHT(ClientContext &context, const idx_t capacity,
65:                                                                           const idx_t radix_bits) const {
66: 	return make_uniq<GroupedAggregateHashTable>(context, BufferAllocator::Get(context), group_types, op.payload_types,
67: 	                                            op.bindings, capacity, radix_bits);
68: }
69: 
70: //===--------------------------------------------------------------------===//
71: // Sink
72: //===--------------------------------------------------------------------===//
73: enum class AggregatePartitionState : uint8_t {
74: 	//! Can be finalized
75: 	READY_TO_FINALIZE = 0,
76: 	//! Finalize is in progress
77: 	FINALIZE_IN_PROGRESS = 1,
78: 	//! Finalized, ready to scan
79: 	READY_TO_SCAN = 2
80: };
81: 
82: struct AggregatePartition : StateWithBlockableTasks {
83: 	explicit AggregatePartition(unique_ptr<TupleDataCollection> data_p)
84: 	    : state(AggregatePartitionState::READY_TO_FINALIZE), data(std::move(data_p)), progress(0) {
85: 	}
86: 
87: 	AggregatePartitionState state;
88: 
89: 	unique_ptr<TupleDataCollection> data;
90: 	atomic<double> progress;
91: };
92: 
93: class RadixHTGlobalSinkState;
94: 
95: struct RadixHTConfig {
96: public:
97: 	explicit RadixHTConfig(ClientContext &context, RadixHTGlobalSinkState &sink);
98: 
99: 	void SetRadixBits(idx_t radix_bits_p);
100: 	bool SetRadixBitsToExternal();
101: 	idx_t GetRadixBits() const;
102: 
103: private:
104: 	void SetRadixBitsInternal(const idx_t radix_bits_p, bool external);
105: 	static idx_t InitialSinkRadixBits(ClientContext &context);
106: 	static idx_t MaximumSinkRadixBits(ClientContext &context);
107: 	static idx_t ExternalRadixBits(const idx_t &maximum_sink_radix_bits_p);
108: 	static idx_t SinkCapacity(ClientContext &context);
109: 
110: private:
111: 	//! Assume (1 << 15) = 32KB L1 cache per core, divided by two because hyperthreading
112: 	static constexpr const idx_t L1_CACHE_SIZE = 32768 / 2;
113: 	//! Assume (1 << 20) = 1MB L2 cache per core, divided by two because hyperthreading
114: 	static constexpr const idx_t L2_CACHE_SIZE = 1048576 / 2;
115: 	//! Assume (1 << 20) + (1 << 19) = 1.5MB L3 cache per core (shared), divided by two because hyperthreading
116: 	static constexpr const idx_t L3_CACHE_SIZE = 1572864 / 2;
117: 
118: 	//! Sink radix bits to initialize with
119: 	static constexpr const idx_t MAXIMUM_INITIAL_SINK_RADIX_BITS = 3;
120: 	//! Maximum Sink radix bits (independent of threads)
121: 	static constexpr const idx_t MAXIMUM_FINAL_SINK_RADIX_BITS = 7;
122: 	//! By how many radix bits to increment if we go external
123: 	static constexpr const idx_t EXTERNAL_RADIX_BITS_INCREMENT = 3;
124: 
125: 	//! The global sink state
126: 	RadixHTGlobalSinkState &sink;
127: 	//! Current thread-global sink radix bits
128: 	atomic<idx_t> sink_radix_bits;
129: 	//! Maximum Sink radix bits (set based on number of threads)
130: 	const idx_t maximum_sink_radix_bits;
131: 	//! Radix bits if we go external
132: 	const idx_t external_radix_bits;
133: 
134: public:
135: 	//! Capacity of HTs during the Sink
136: 	const idx_t sink_capacity;
137: 
138: 	//! If we fill this many blocks per partition, we trigger a repartition
139: 	static constexpr const double BLOCK_FILL_FACTOR = 1.8;
140: 	//! By how many bits to repartition if a repartition is triggered
141: 	static constexpr const idx_t REPARTITION_RADIX_BITS = 2;
142: };
143: 
144: class RadixHTGlobalSinkState : public GlobalSinkState {
145: public:
146: 	RadixHTGlobalSinkState(ClientContext &context, const RadixPartitionedHashTable &radix_ht);
147: 
148: 	//! Destroys aggregate states (if multi-scan)
149: 	~RadixHTGlobalSinkState() override;
150: 	void Destroy();
151: 
152: public:
153: 	ClientContext &context;
154: 	//! Temporary memory state for managing this hash table's memory usage
155: 	unique_ptr<TemporaryMemoryState> temporary_memory_state;
156: 
157: 	//! The radix HT
158: 	const RadixPartitionedHashTable &radix_ht;
159: 	//! Config for partitioning
160: 	RadixHTConfig config;
161: 
162: 	//! Whether we've called Finalize
163: 	bool finalized;
164: 	//! Whether we are doing an external aggregation
165: 	atomic<bool> external;
166: 	//! Threads that have called Sink
167: 	atomic<idx_t> active_threads;
168: 	//! Number of threads (from TaskScheduler)
169: 	const idx_t number_of_threads;
170: 	//! If any thread has called combine
171: 	atomic<bool> any_combined;
172: 
173: 	//! Uncombined partitioned data that will be put into the AggregatePartitions
174: 	unique_ptr<PartitionedTupleData> uncombined_data;
175: 	//! Allocators used during the Sink/Finalize
176: 	vector<shared_ptr<ArenaAllocator>> stored_allocators;
177: 
178: 	//! Partitions that are finalized during GetData
179: 	vector<unique_ptr<AggregatePartition>> partitions;
180: 	//! For keeping track of progress
181: 	atomic<idx_t> finalize_done;
182: 
183: 	//! Pin properties when scanning
184: 	TupleDataPinProperties scan_pin_properties;
185: 	//! Total count before combining
186: 	idx_t count_before_combining;
187: 	//! Maximum partition size if all unique
188: 	idx_t max_partition_size;
189: };
190: 
191: RadixHTGlobalSinkState::RadixHTGlobalSinkState(ClientContext &context_p, const RadixPartitionedHashTable &radix_ht_p)
192:     : context(context_p), temporary_memory_state(TemporaryMemoryManager::Get(context).Register(context)),
193:       radix_ht(radix_ht_p), config(context, *this), finalized(false), external(false), active_threads(0),
194:       number_of_threads(NumericCast<idx_t>(TaskScheduler::GetScheduler(context).NumberOfThreads())),
195:       any_combined(false), finalize_done(0), scan_pin_properties(TupleDataPinProperties::DESTROY_AFTER_DONE),
196:       count_before_combining(0), max_partition_size(0) {
197: 
198: 	// Compute minimum reservation
199: 	auto block_alloc_size = BufferManager::GetBufferManager(context).GetBlockAllocSize();
200: 	auto tuples_per_block = block_alloc_size / radix_ht.GetLayout().GetRowWidth();
201: 	idx_t ht_count =
202: 	    LossyNumericCast<idx_t>(static_cast<double>(config.sink_capacity) / GroupedAggregateHashTable::LOAD_FACTOR);
203: 	auto num_partitions = RadixPartitioning::NumberOfPartitions(config.GetRadixBits());
204: 	auto count_per_partition = ht_count / num_partitions;
205: 	auto blocks_per_partition = (count_per_partition + tuples_per_block) / tuples_per_block + 1;
206: 	if (!radix_ht.GetLayout().AllConstant()) {
207: 		blocks_per_partition += 2;
208: 	}
209: 	auto ht_size = blocks_per_partition * block_alloc_size + config.sink_capacity * sizeof(ht_entry_t);
210: 
211: 	// This really is the minimum reservation that we can do
212: 	auto num_threads = NumericCast<idx_t>(TaskScheduler::GetScheduler(context).NumberOfThreads());
213: 	auto minimum_reservation = num_threads * ht_size;
214: 
215: 	temporary_memory_state->SetMinimumReservation(minimum_reservation);
216: 	temporary_memory_state->SetRemainingSizeAndUpdateReservation(context, minimum_reservation);
217: }
218: 
219: RadixHTGlobalSinkState::~RadixHTGlobalSinkState() {
220: 	Destroy();
221: }
222: 
223: // LCOV_EXCL_START
224: void RadixHTGlobalSinkState::Destroy() {
225: 	if (scan_pin_properties == TupleDataPinProperties::DESTROY_AFTER_DONE || count_before_combining == 0 ||
226: 	    partitions.empty()) {
227: 		// Already destroyed / empty
228: 		return;
229: 	}
230: 
231: 	TupleDataLayout layout = partitions[0]->data->GetLayout().Copy();
232: 	if (!layout.HasDestructor()) {
233: 		return; // No destructors, exit
234: 	}
235: 
236: 	// There are aggregates with destructors: Call the destructor for each of the aggregates
237: 	auto guard = Lock();
238: 	RowOperationsState row_state(*stored_allocators.back());
239: 	for (auto &partition : partitions) {
240: 		auto &data_collection = *partition->data;
241: 		if (data_collection.Count() == 0) {
242: 			continue;
243: 		}
244: 		TupleDataChunkIterator iterator(data_collection, TupleDataPinProperties::DESTROY_AFTER_DONE, false);
245: 		auto &row_locations = iterator.GetChunkState().row_locations;
246: 		do {
247: 			RowOperations::DestroyStates(row_state, layout, row_locations, iterator.GetCurrentChunkCount());
248: 		} while (iterator.Next());
249: 		data_collection.Reset();
250: 	}
251: }
252: // LCOV_EXCL_STOP
253: 
254: RadixHTConfig::RadixHTConfig(ClientContext &context, RadixHTGlobalSinkState &sink_p)
255:     : sink(sink_p), sink_radix_bits(InitialSinkRadixBits(context)),
256:       maximum_sink_radix_bits(MaximumSinkRadixBits(context)),
257:       external_radix_bits(ExternalRadixBits(maximum_sink_radix_bits)), sink_capacity(SinkCapacity(context)) {
258: }
259: 
260: void RadixHTConfig::SetRadixBits(idx_t radix_bits_p) {
261: 	SetRadixBitsInternal(MinValue(radix_bits_p, maximum_sink_radix_bits), false);
262: }
263: 
264: bool RadixHTConfig::SetRadixBitsToExternal() {
265: 	SetRadixBitsInternal(external_radix_bits, true);
266: 	return sink.external;
267: }
268: 
269: idx_t RadixHTConfig::GetRadixBits() const {
270: 	return sink_radix_bits;
271: }
272: 
273: void RadixHTConfig::SetRadixBitsInternal(const idx_t radix_bits_p, bool external) {
274: 	if (sink_radix_bits >= radix_bits_p || sink.any_combined) {
275: 		return;
276: 	}
277: 
278: 	auto guard = sink.Lock();
279: 	if (sink_radix_bits >= radix_bits_p || sink.any_combined) {
280: 		return;
281: 	}
282: 
283: 	if (external) {
284: 		sink.external = true;
285: 	}
286: 	sink_radix_bits = radix_bits_p;
287: 	return;
288: }
289: 
290: idx_t RadixHTConfig::InitialSinkRadixBits(ClientContext &context) {
291: 	const auto active_threads = NumericCast<idx_t>(TaskScheduler::GetScheduler(context).NumberOfThreads());
292: 	return MinValue(RadixPartitioning::RadixBits(NextPowerOfTwo(active_threads)), MAXIMUM_INITIAL_SINK_RADIX_BITS);
293: }
294: 
295: idx_t RadixHTConfig::MaximumSinkRadixBits(ClientContext &context) {
296: 	const auto active_threads = NumericCast<idx_t>(TaskScheduler::GetScheduler(context).NumberOfThreads());
297: 	return MinValue(RadixPartitioning::RadixBits(NextPowerOfTwo(active_threads)), MAXIMUM_FINAL_SINK_RADIX_BITS);
298: }
299: 
300: idx_t RadixHTConfig::ExternalRadixBits(const idx_t &maximum_sink_radix_bits_p) {
301: 	return MinValue(maximum_sink_radix_bits_p + EXTERNAL_RADIX_BITS_INCREMENT, MAXIMUM_FINAL_SINK_RADIX_BITS);
302: }
303: 
304: idx_t RadixHTConfig::SinkCapacity(ClientContext &context) {
305: 	// Get active and maximum number of threads
306: 	const auto active_threads = NumericCast<idx_t>(TaskScheduler::GetScheduler(context).NumberOfThreads());
307: 
308: 	// Compute cache size per active thread (assuming cache is shared)
309: 	const auto total_shared_cache_size = active_threads * L3_CACHE_SIZE;
310: 	const auto cache_per_active_thread = L1_CACHE_SIZE + L2_CACHE_SIZE + total_shared_cache_size / active_threads;
311: 
312: 	// Divide cache per active thread by entry size, round up to next power of two, to get capacity
313: 	const auto size_per_entry = sizeof(ht_entry_t) * GroupedAggregateHashTable::LOAD_FACTOR;
314: 	const auto capacity =
315: 	    NextPowerOfTwo(LossyNumericCast<uint64_t>(static_cast<double>(cache_per_active_thread) / size_per_entry));
316: 
317: 	// Capacity must be at least the minimum capacity
318: 	return MaxValue<idx_t>(capacity, GroupedAggregateHashTable::InitialCapacity());
319: }
320: 
321: class RadixHTLocalSinkState : public LocalSinkState {
322: public:
323: 	RadixHTLocalSinkState(ClientContext &context, const RadixPartitionedHashTable &radix_ht);
324: 
325: public:
326: 	//! Thread-local HT that is re-used after abandoning
327: 	unique_ptr<GroupedAggregateHashTable> ht;
328: 	//! Chunk with group columns
329: 	DataChunk group_chunk;
330: 
331: 	//! Data that is abandoned ends up here (only if we're doing external aggregation)
332: 	unique_ptr<PartitionedTupleData> abandoned_data;
333: };
334: 
335: RadixHTLocalSinkState::RadixHTLocalSinkState(ClientContext &, const RadixPartitionedHashTable &radix_ht) {
336: 	// If there are no groups we create a fake group so everything has the same group
337: 	group_chunk.InitializeEmpty(radix_ht.group_types);
338: 	if (radix_ht.grouping_set.empty()) {
339: 		group_chunk.data[0].Reference(Value::TINYINT(42));
340: 	}
341: }
342: 
343: unique_ptr<GlobalSinkState> RadixPartitionedHashTable::GetGlobalSinkState(ClientContext &context) const {
344: 	return make_uniq<RadixHTGlobalSinkState>(context, *this);
345: }
346: 
347: unique_ptr<LocalSinkState> RadixPartitionedHashTable::GetLocalSinkState(ExecutionContext &context) const {
348: 	return make_uniq<RadixHTLocalSinkState>(context.client, *this);
349: }
350: 
351: void RadixPartitionedHashTable::PopulateGroupChunk(DataChunk &group_chunk, DataChunk &input_chunk) const {
352: 	idx_t chunk_index = 0;
353: 	// Populate the group_chunk
354: 	for (auto &group_idx : grouping_set) {
355: 		// Retrieve the expression containing the index in the input chunk
356: 		auto &group = op.groups[group_idx];
357: 		D_ASSERT(group->type == ExpressionType::BOUND_REF);
358: 		auto &bound_ref_expr = group->Cast<BoundReferenceExpression>();
359: 		// Reference from input_chunk[group.index] -> group_chunk[chunk_index]
360: 		group_chunk.data[chunk_index++].Reference(input_chunk.data[bound_ref_expr.index]);
361: 	}
362: 	group_chunk.SetCardinality(input_chunk.size());
363: 	group_chunk.Verify();
364: }
365: 
366: bool MaybeRepartition(ClientContext &context, RadixHTGlobalSinkState &gstate, RadixHTLocalSinkState &lstate) {
367: 	auto &config = gstate.config;
368: 	auto &ht = *lstate.ht;
369: 	auto &partitioned_data = ht.GetPartitionedData();
370: 
371: 	// Check if we're approaching the memory limit
372: 	auto &temporary_memory_state = *gstate.temporary_memory_state;
373: 	const auto total_size = partitioned_data->SizeInBytes() + ht.Capacity() * sizeof(ht_entry_t);
374: 	idx_t thread_limit = temporary_memory_state.GetReservation() / gstate.number_of_threads;
375: 	if (total_size > thread_limit) {
376: 		// We're over the thread memory limit
377: 		if (!gstate.external) {
378: 			// We haven't yet triggered out-of-core behavior, but maybe we don't have to, grab the lock and check again
379: 			auto guard = gstate.Lock();
380: 			thread_limit = temporary_memory_state.GetReservation() / gstate.number_of_threads;
381: 			if (total_size > thread_limit) {
382: 				// Out-of-core would be triggered below, try to increase the reservation
383: 				auto remaining_size =
384: 				    MaxValue<idx_t>(gstate.number_of_threads * total_size, temporary_memory_state.GetRemainingSize());
385: 				temporary_memory_state.SetRemainingSizeAndUpdateReservation(context, 2 * remaining_size);
386: 				thread_limit = temporary_memory_state.GetReservation() / gstate.number_of_threads;
387: 			}
388: 		}
389: 	}
390: 
391: 	if (total_size > thread_limit) {
392: 		if (gstate.config.SetRadixBitsToExternal()) {
393: 			// We're approaching the memory limit, unpin the data
394: 			if (!lstate.abandoned_data) {
395: 				lstate.abandoned_data = make_uniq<RadixPartitionedTupleData>(
396: 				    BufferManager::GetBufferManager(context), gstate.radix_ht.GetLayout(), config.GetRadixBits(),
397: 				    gstate.radix_ht.GetLayout().ColumnCount() - 1);
398: 			}
399: 
400: 			ht.UnpinData();
401: 			partitioned_data->Repartition(*lstate.abandoned_data);
402: 			ht.SetRadixBits(gstate.config.GetRadixBits());
403: 			ht.InitializePartitionedData();
404: 			return true;
405: 		}
406: 	}
407: 
408: 	// We can go external when there is only one active thread, but we shouldn't repartition here
409: 	if (gstate.number_of_threads < 2) {
410: 		return false;
411: 	}
412: 
413: 	const auto partition_count = partitioned_data->PartitionCount();
414: 	const auto current_radix_bits = RadixPartitioning::RadixBits(partition_count);
415: 	D_ASSERT(current_radix_bits <= config.GetRadixBits());
416: 
417: 	const auto block_size = BufferManager::GetBufferManager(context).GetBlockSize();
418: 	const auto row_size_per_partition =
419: 	    partitioned_data->Count() * partitioned_data->GetLayout().GetRowWidth() / partition_count;
420: 	if (row_size_per_partition > LossyNumericCast<idx_t>(config.BLOCK_FILL_FACTOR * static_cast<double>(block_size))) {
421: 		// We crossed our block filling threshold, try to increment radix bits
422: 		config.SetRadixBits(current_radix_bits + config.REPARTITION_RADIX_BITS);
423: 	}
424: 
425: 	const auto global_radix_bits = config.GetRadixBits();
426: 	if (current_radix_bits == global_radix_bits) {
427: 		return false; // We're already on the right number of radix bits
428: 	}
429: 
430: 	// We're out-of-sync with the global radix bits, repartition
431: 	ht.UnpinData();
432: 	auto old_partitioned_data = std::move(partitioned_data);
433: 	ht.SetRadixBits(global_radix_bits);
434: 	ht.InitializePartitionedData();
435: 	old_partitioned_data->Repartition(*ht.GetPartitionedData());
436: 	return true;
437: }
438: 
439: void RadixPartitionedHashTable::Sink(ExecutionContext &context, DataChunk &chunk, OperatorSinkInput &input,
440:                                      DataChunk &payload_input, const unsafe_vector<idx_t> &filter) const {
441: 	auto &gstate = input.global_state.Cast<RadixHTGlobalSinkState>();
442: 	auto &lstate = input.local_state.Cast<RadixHTLocalSinkState>();
443: 	if (!lstate.ht) {
444: 		lstate.ht = CreateHT(context.client, gstate.config.sink_capacity, gstate.config.GetRadixBits());
445: 		gstate.active_threads++;
446: 	}
447: 
448: 	auto &group_chunk = lstate.group_chunk;
449: 	PopulateGroupChunk(group_chunk, chunk);
450: 
451: 	auto &ht = *lstate.ht;
452: 	ht.AddChunk(group_chunk, payload_input, filter);
453: 
454: 	if (ht.Count() + STANDARD_VECTOR_SIZE < ht.ResizeThreshold()) {
455: 		return; // We can fit another chunk
456: 	}
457: 
458: 	if (gstate.number_of_threads > 2) {
459: 		// 'Reset' the HT without taking its data, we can just keep appending to the same collection
460: 		// This only works because we never resize the HT
461: 		ht.ClearPointerTable();
462: 		ht.ResetCount();
463: 		// We don't do this when running with 1 or 2 threads, it only makes sense when there's many threads
464: 	}
465: 
466: 	// Check if we need to repartition
467: 	auto repartitioned = MaybeRepartition(context.client, gstate, lstate);
468: 
469: 	if (repartitioned && ht.Count() != 0) {
470: 		// We repartitioned, but we didn't clear the pointer table / reset the count because we're on 1 or 2 threads
471: 		ht.ClearPointerTable();
472: 		ht.ResetCount();
473: 	}
474: 
475: 	// TODO: combine early and often
476: }
477: 
478: void RadixPartitionedHashTable::Combine(ExecutionContext &context, GlobalSinkState &gstate_p,
479:                                         LocalSinkState &lstate_p) const {
480: 	auto &gstate = gstate_p.Cast<RadixHTGlobalSinkState>();
481: 	auto &lstate = lstate_p.Cast<RadixHTLocalSinkState>();
482: 	if (!lstate.ht) {
483: 		return;
484: 	}
485: 
486: 	// Set any_combined, then check one last time whether we need to repartition
487: 	gstate.any_combined = true;
488: 	MaybeRepartition(context.client, gstate, lstate);
489: 
490: 	auto &ht = *lstate.ht;
491: 	ht.UnpinData();
492: 
493: 	if (lstate.abandoned_data) {
494: 		D_ASSERT(gstate.external);
495: 		D_ASSERT(lstate.abandoned_data->PartitionCount() == lstate.ht->GetPartitionedData()->PartitionCount());
496: 		D_ASSERT(lstate.abandoned_data->PartitionCount() ==
497: 		         RadixPartitioning::NumberOfPartitions(gstate.config.GetRadixBits()));
498: 		lstate.abandoned_data->Combine(*lstate.ht->GetPartitionedData());
499: 	} else {
500: 		lstate.abandoned_data = std::move(ht.GetPartitionedData());
501: 	}
502: 
503: 	auto guard = gstate.Lock();
504: 	if (gstate.uncombined_data) {
505: 		gstate.uncombined_data->Combine(*lstate.abandoned_data);
506: 	} else {
507: 		gstate.uncombined_data = std::move(lstate.abandoned_data);
508: 	}
509: 	gstate.stored_allocators.emplace_back(ht.GetAggregateAllocator());
510: }
511: 
512: void RadixPartitionedHashTable::Finalize(ClientContext &context, GlobalSinkState &gstate_p) const {
513: 	auto &gstate = gstate_p.Cast<RadixHTGlobalSinkState>();
514: 
515: 	if (gstate.uncombined_data) {
516: 		auto &uncombined_data = *gstate.uncombined_data;
517: 		gstate.count_before_combining = uncombined_data.Count();
518: 
519: 		// If true there is no need to combine, it was all done by a single thread in a single HT
520: 		const auto single_ht = !gstate.external && gstate.active_threads == 1 && gstate.number_of_threads == 1;
521: 
522: 		auto &uncombined_partition_data = uncombined_data.GetPartitions();
523: 		const auto n_partitions = uncombined_partition_data.size();
524: 		gstate.partitions.reserve(n_partitions);
525: 		for (idx_t i = 0; i < n_partitions; i++) {
526: 			auto &partition = uncombined_partition_data[i];
527: 			auto partition_size =
528: 			    partition->SizeInBytes() +
529: 			    GroupedAggregateHashTable::GetCapacityForCount(partition->Count()) * sizeof(ht_entry_t);
530: 			gstate.max_partition_size = MaxValue(gstate.max_partition_size, partition_size);
531: 
532: 			gstate.partitions.emplace_back(make_uniq<AggregatePartition>(std::move(partition)));
533: 			if (single_ht) {
534: 				gstate.finalize_done++;
535: 				gstate.partitions.back()->progress = 1;
536: 				gstate.partitions.back()->state = AggregatePartitionState::READY_TO_SCAN;
537: 			}
538: 		}
539: 	} else {
540: 		gstate.count_before_combining = 0;
541: 	}
542: 
543: 	// Minimum of combining one partition at a time
544: 	gstate.temporary_memory_state->SetMinimumReservation(gstate.max_partition_size);
545: 	// Set size to 0 until the scan actually starts
546: 	gstate.temporary_memory_state->SetZero();
547: 	gstate.finalized = true;
548: }
549: 
550: //===--------------------------------------------------------------------===//
551: // Source
552: //===--------------------------------------------------------------------===//
553: idx_t RadixPartitionedHashTable::MaxThreads(GlobalSinkState &sink_p) const {
554: 	auto &sink = sink_p.Cast<RadixHTGlobalSinkState>();
555: 	if (sink.partitions.empty()) {
556: 		return 0;
557: 	}
558: 
559: 	const auto max_threads = MinValue<idx_t>(
560: 	    NumericCast<idx_t>(TaskScheduler::GetScheduler(sink.context).NumberOfThreads()), sink.partitions.size());
561: 	sink.temporary_memory_state->SetRemainingSizeAndUpdateReservation(sink.context,
562: 	                                                                  max_threads * sink.max_partition_size);
563: 
564: 	// This many partitions will fit given our reservation (at least 1))
565: 	const auto partitions_fit =
566: 	    MaxValue<idx_t>(sink.temporary_memory_state->GetReservation() / sink.max_partition_size, 1);
567: 
568: 	// Mininum of the two
569: 	return MinValue<idx_t>(partitions_fit, max_threads);
570: }
571: 
572: void RadixPartitionedHashTable::SetMultiScan(GlobalSinkState &sink_p) {
573: 	auto &sink = sink_p.Cast<RadixHTGlobalSinkState>();
574: 	sink.scan_pin_properties = TupleDataPinProperties::UNPIN_AFTER_DONE;
575: }
576: 
577: enum class RadixHTSourceTaskType : uint8_t { NO_TASK, FINALIZE, SCAN };
578: 
579: class RadixHTLocalSourceState;
580: 
581: class RadixHTGlobalSourceState : public GlobalSourceState {
582: public:
583: 	RadixHTGlobalSourceState(ClientContext &context, const RadixPartitionedHashTable &radix_ht);
584: 
585: 	//! Assigns a task to a local source state
586: 	SourceResultType AssignTask(RadixHTGlobalSinkState &sink, RadixHTLocalSourceState &lstate,
587: 	                            InterruptState &interrupt_state);
588: 
589: public:
590: 	//! The client context
591: 	ClientContext &context;
592: 	//! For synchronizing the source phase
593: 	atomic<bool> finished;
594: 
595: 	//! Column ids for scanning
596: 	vector<column_t> column_ids;
597: 
598: 	//! For synchronizing tasks
599: 	idx_t task_idx;
600: 	atomic<idx_t> task_done;
601: };
602: 
603: enum class RadixHTScanStatus : uint8_t { INIT, IN_PROGRESS, DONE };
604: 
605: class RadixHTLocalSourceState : public LocalSourceState {
606: public:
607: 	explicit RadixHTLocalSourceState(ExecutionContext &context, const RadixPartitionedHashTable &radix_ht);
608: 
609: public:
610: 	//! Do the work this thread has been assigned
611: 	void ExecuteTask(RadixHTGlobalSinkState &sink, RadixHTGlobalSourceState &gstate, DataChunk &chunk);
612: 	//! Whether this thread has finished the work it has been assigned
613: 	bool TaskFinished();
614: 
615: private:
616: 	//! Execute the finalize or scan task
617: 	void Finalize(RadixHTGlobalSinkState &sink, RadixHTGlobalSourceState &gstate);
618: 	void Scan(RadixHTGlobalSinkState &sink, RadixHTGlobalSourceState &gstate, DataChunk &chunk);
619: 
620: public:
621: 	//! Current task and index
622: 	RadixHTSourceTaskType task;
623: 	idx_t task_idx;
624: 
625: 	//! Thread-local HT that is re-used to Finalize
626: 	unique_ptr<GroupedAggregateHashTable> ht;
627: 	//! Current status of a Scan
628: 	RadixHTScanStatus scan_status;
629: 
630: private:
631: 	//! Allocator and layout for finalizing state
632: 	TupleDataLayout layout;
633: 	ArenaAllocator aggregate_allocator;
634: 
635: 	//! State and chunk for scanning
636: 	TupleDataScanState scan_state;
637: 	DataChunk scan_chunk;
638: };
639: 
640: unique_ptr<GlobalSourceState> RadixPartitionedHashTable::GetGlobalSourceState(ClientContext &context) const {
641: 	return make_uniq<RadixHTGlobalSourceState>(context, *this);
642: }
643: 
644: unique_ptr<LocalSourceState> RadixPartitionedHashTable::GetLocalSourceState(ExecutionContext &context) const {
645: 	return make_uniq<RadixHTLocalSourceState>(context, *this);
646: }
647: 
648: RadixHTGlobalSourceState::RadixHTGlobalSourceState(ClientContext &context_p, const RadixPartitionedHashTable &radix_ht)
649:     : context(context_p), finished(false), task_idx(0), task_done(0) {
650: 	for (column_t column_id = 0; column_id < radix_ht.group_types.size(); column_id++) {
651: 		column_ids.push_back(column_id);
652: 	}
653: }
654: 
655: SourceResultType RadixHTGlobalSourceState::AssignTask(RadixHTGlobalSinkState &sink, RadixHTLocalSourceState &lstate,
656:                                                       InterruptState &interrupt_state) {
657: 	// First, try to get a partition index
658: 	auto guard = sink.Lock();
659: 	if (finished || task_idx == sink.partitions.size()) {
660: 		lstate.ht.reset();
661: 		return SourceResultType::FINISHED;
662: 	}
663: 	lstate.task_idx = task_idx++;
664: 
665: 	// We got a partition index
666: 	auto &partition = *sink.partitions[lstate.task_idx];
667: 	auto partition_guard = partition.Lock();
668: 	switch (partition.state) {
669: 	case AggregatePartitionState::READY_TO_FINALIZE:
670: 		partition.state = AggregatePartitionState::FINALIZE_IN_PROGRESS;
671: 		lstate.task = RadixHTSourceTaskType::FINALIZE;
672: 		return SourceResultType::HAVE_MORE_OUTPUT;
673: 	case AggregatePartitionState::FINALIZE_IN_PROGRESS:
674: 		lstate.task = RadixHTSourceTaskType::SCAN;
675: 		lstate.scan_status = RadixHTScanStatus::INIT;
676: 		return partition.BlockSource(partition_guard, interrupt_state);
677: 	case AggregatePartitionState::READY_TO_SCAN:
678: 		lstate.task = RadixHTSourceTaskType::SCAN;
679: 		lstate.scan_status = RadixHTScanStatus::INIT;
680: 		return SourceResultType::HAVE_MORE_OUTPUT;
681: 	default:
682: 		throw InternalException("Unexpected AggregatePartitionState in RadixHTLocalSourceState::Finalize!");
683: 	}
684: }
685: 
686: RadixHTLocalSourceState::RadixHTLocalSourceState(ExecutionContext &context, const RadixPartitionedHashTable &radix_ht)
687:     : task(RadixHTSourceTaskType::NO_TASK), scan_status(RadixHTScanStatus::DONE), layout(radix_ht.GetLayout().Copy()),
688:       aggregate_allocator(BufferAllocator::Get(context.client)) {
689: 	auto &allocator = BufferAllocator::Get(context.client);
690: 	auto scan_chunk_types = radix_ht.group_types;
691: 	for (auto &aggr_type : radix_ht.op.aggregate_return_types) {
692: 		scan_chunk_types.push_back(aggr_type);
693: 	}
694: 	scan_chunk.Initialize(allocator, scan_chunk_types);
695: }
696: 
697: void RadixHTLocalSourceState::ExecuteTask(RadixHTGlobalSinkState &sink, RadixHTGlobalSourceState &gstate,
698:                                           DataChunk &chunk) {
699: 	D_ASSERT(task != RadixHTSourceTaskType::NO_TASK);
700: 	switch (task) {
701: 	case RadixHTSourceTaskType::FINALIZE:
702: 		Finalize(sink, gstate);
703: 		break;
704: 	case RadixHTSourceTaskType::SCAN:
705: 		Scan(sink, gstate, chunk);
706: 		break;
707: 	default:
708: 		throw InternalException("Unexpected RadixHTSourceTaskType in ExecuteTask!");
709: 	}
710: }
711: 
712: void RadixHTLocalSourceState::Finalize(RadixHTGlobalSinkState &sink, RadixHTGlobalSourceState &gstate) {
713: 	D_ASSERT(task == RadixHTSourceTaskType::FINALIZE);
714: 	D_ASSERT(scan_status != RadixHTScanStatus::IN_PROGRESS);
715: 	auto &partition = *sink.partitions[task_idx];
716: 
717: 	if (!ht) {
718: 		// This capacity would always be sufficient for all data
719: 		const auto capacity = GroupedAggregateHashTable::GetCapacityForCount(partition.data->Count());
720: 
721: 		// However, we will limit the initial capacity so we don't do a huge over-allocation
722: 		const auto n_threads = NumericCast<idx_t>(TaskScheduler::GetScheduler(gstate.context).NumberOfThreads());
723: 		const auto memory_limit = BufferManager::GetBufferManager(gstate.context).GetMaxMemory();
724: 		const idx_t thread_limit = LossyNumericCast<idx_t>(0.6 * double(memory_limit) / double(n_threads));
725: 
726: 		const idx_t size_per_entry = partition.data->SizeInBytes() / MaxValue<idx_t>(partition.data->Count(), 1) +
727: 		                             idx_t(GroupedAggregateHashTable::LOAD_FACTOR * sizeof(ht_entry_t));
728: 		// but not lower than the initial capacity
729: 		const auto capacity_limit =
730: 		    MaxValue(NextPowerOfTwo(thread_limit / size_per_entry), GroupedAggregateHashTable::InitialCapacity());
731: 
732: 		ht = sink.radix_ht.CreateHT(gstate.context, MinValue<idx_t>(capacity, capacity_limit), 0);
733: 	} else {
734: 		// We may want to resize here to the size of this partition, but for now we just assume uniform partition sizes
735: 		ht->InitializePartitionedData();
736: 		ht->ClearPointerTable();
737: 		ht->ResetCount();
738: 	}
739: 
740: 	// Now combine the uncombined data using this thread's HT
741: 	ht->Combine(*partition.data, &partition.progress);
742: 	ht->UnpinData();
743: 	partition.progress = 1;
744: 
745: 	// Move the combined data back to the partition
746: 	partition.data =
747: 	    make_uniq<TupleDataCollection>(BufferManager::GetBufferManager(gstate.context), sink.radix_ht.GetLayout());
748: 	partition.data->Combine(*ht->GetPartitionedData()->GetPartitions()[0]);
749: 
750: 	// Update thread-global state
751: 	auto guard = sink.Lock();
752: 	sink.stored_allocators.emplace_back(ht->GetAggregateAllocator());
753: 	if (task_idx == sink.partitions.size()) {
754: 		ht.reset();
755: 	}
756: 	const auto finalizes_done = ++sink.finalize_done;
757: 	D_ASSERT(finalizes_done <= sink.partitions.size());
758: 	if (finalizes_done == sink.partitions.size()) {
759: 		// All finalizes are done, set remaining size to 0
760: 		sink.temporary_memory_state->SetZero();
761: 	}
762: 
763: 	// Update partition state
764: 	auto partition_guard = partition.Lock();
765: 	partition.state = AggregatePartitionState::READY_TO_SCAN;
766: 	partition.UnblockTasks(partition_guard);
767: 
768: 	// This thread will scan the partition
769: 	task = RadixHTSourceTaskType::SCAN;
770: 	scan_status = RadixHTScanStatus::INIT;
771: }
772: 
773: void RadixHTLocalSourceState::Scan(RadixHTGlobalSinkState &sink, RadixHTGlobalSourceState &gstate, DataChunk &chunk) {
774: 	D_ASSERT(task == RadixHTSourceTaskType::SCAN);
775: 	D_ASSERT(scan_status != RadixHTScanStatus::DONE);
776: 
777: 	auto &partition = *sink.partitions[task_idx];
778: 	D_ASSERT(partition.state == AggregatePartitionState::READY_TO_SCAN);
779: 	auto &data_collection = *partition.data;
780: 
781: 	if (scan_status == RadixHTScanStatus::INIT) {
782: 		data_collection.InitializeScan(scan_state, gstate.column_ids, sink.scan_pin_properties);
783: 		scan_status = RadixHTScanStatus::IN_PROGRESS;
784: 	}
785: 
786: 	if (!data_collection.Scan(scan_state, scan_chunk)) {
787: 		if (sink.scan_pin_properties == TupleDataPinProperties::DESTROY_AFTER_DONE) {
788: 			data_collection.Reset();
789: 		}
790: 		scan_status = RadixHTScanStatus::DONE;
791: 		auto guard = sink.Lock();
792: 		if (++gstate.task_done == sink.partitions.size()) {
793: 			gstate.finished = true;
794: 		}
795: 		return;
796: 	}
797: 
798: 	RowOperationsState row_state(aggregate_allocator);
799: 	const auto group_cols = layout.ColumnCount() - 1;
800: 	RowOperations::FinalizeStates(row_state, layout, scan_state.chunk_state.row_locations, scan_chunk, group_cols);
801: 
802: 	if (sink.scan_pin_properties == TupleDataPinProperties::DESTROY_AFTER_DONE && layout.HasDestructor()) {
803: 		RowOperations::DestroyStates(row_state, layout, scan_state.chunk_state.row_locations, scan_chunk.size());
804: 	}
805: 
806: 	auto &radix_ht = sink.radix_ht;
807: 	idx_t chunk_index = 0;
808: 	for (auto &entry : radix_ht.grouping_set) {
809: 		chunk.data[entry].Reference(scan_chunk.data[chunk_index++]);
810: 	}
811: 	for (auto null_group : radix_ht.null_groups) {
812: 		chunk.data[null_group].SetVectorType(VectorType::CONSTANT_VECTOR);
813: 		ConstantVector::SetNull(chunk.data[null_group], true);
814: 	}
815: 	D_ASSERT(radix_ht.grouping_set.size() + radix_ht.null_groups.size() == radix_ht.op.GroupCount());
816: 	for (idx_t col_idx = 0; col_idx < radix_ht.op.aggregates.size(); col_idx++) {
817: 		chunk.data[radix_ht.op.GroupCount() + col_idx].Reference(
818: 		    scan_chunk.data[radix_ht.group_types.size() + col_idx]);
819: 	}
820: 	D_ASSERT(radix_ht.op.grouping_functions.size() == radix_ht.grouping_values.size());
821: 	for (idx_t i = 0; i < radix_ht.op.grouping_functions.size(); i++) {
822: 		chunk.data[radix_ht.op.GroupCount() + radix_ht.op.aggregates.size() + i].Reference(radix_ht.grouping_values[i]);
823: 	}
824: 	chunk.SetCardinality(scan_chunk);
825: 	D_ASSERT(chunk.size() != 0);
826: }
827: 
828: bool RadixHTLocalSourceState::TaskFinished() {
829: 	switch (task) {
830: 	case RadixHTSourceTaskType::FINALIZE:
831: 		return true;
832: 	case RadixHTSourceTaskType::SCAN:
833: 		return scan_status == RadixHTScanStatus::DONE;
834: 	default:
835: 		D_ASSERT(task == RadixHTSourceTaskType::NO_TASK);
836: 		return true;
837: 	}
838: }
839: 
840: SourceResultType RadixPartitionedHashTable::GetData(ExecutionContext &context, DataChunk &chunk,
841:                                                     GlobalSinkState &sink_p, OperatorSourceInput &input) const {
842: 	auto &sink = sink_p.Cast<RadixHTGlobalSinkState>();
843: 	D_ASSERT(sink.finalized);
844: 
845: 	auto &gstate = input.global_state.Cast<RadixHTGlobalSourceState>();
846: 	auto &lstate = input.local_state.Cast<RadixHTLocalSourceState>();
847: 	D_ASSERT(sink.scan_pin_properties == TupleDataPinProperties::UNPIN_AFTER_DONE ||
848: 	         sink.scan_pin_properties == TupleDataPinProperties::DESTROY_AFTER_DONE);
849: 
850: 	if (gstate.finished) {
851: 		return SourceResultType::FINISHED;
852: 	}
853: 
854: 	if (sink.count_before_combining == 0) {
855: 		if (grouping_set.empty()) {
856: 			// Special case hack to sort out aggregating from empty intermediates for aggregations without groups
857: 			D_ASSERT(chunk.ColumnCount() == null_groups.size() + op.aggregates.size() + op.grouping_functions.size());
858: 			// For each column in the aggregates, set to initial state
859: 			chunk.SetCardinality(1);
860: 			for (auto null_group : null_groups) {
861: 				chunk.data[null_group].SetVectorType(VectorType::CONSTANT_VECTOR);
862: 				ConstantVector::SetNull(chunk.data[null_group], true);
863: 			}
864: 			ArenaAllocator allocator(BufferAllocator::Get(context.client));
865: 			for (idx_t i = 0; i < op.aggregates.size(); i++) {
866: 				D_ASSERT(op.aggregates[i]->GetExpressionClass() == ExpressionClass::BOUND_AGGREGATE);
867: 				auto &aggr = op.aggregates[i]->Cast<BoundAggregateExpression>();
868: 				auto aggr_state = make_unsafe_uniq_array_uninitialized<data_t>(aggr.function.state_size(aggr.function));
869: 				aggr.function.initialize(aggr.function, aggr_state.get());
870: 
871: 				AggregateInputData aggr_input_data(aggr.bind_info.get(), allocator);
872: 				Vector state_vector(Value::POINTER(CastPointerToValue(aggr_state.get())));
873: 				aggr.function.finalize(state_vector, aggr_input_data, chunk.data[null_groups.size() + i], 1, 0);
874: 				if (aggr.function.destructor) {
875: 					aggr.function.destructor(state_vector, aggr_input_data, 1);
876: 				}
877: 			}
878: 			// Place the grouping values (all the groups of the grouping_set condensed into a single value)
879: 			// Behind the null groups + aggregates
880: 			for (idx_t i = 0; i < op.grouping_functions.size(); i++) {
881: 				chunk.data[null_groups.size() + op.aggregates.size() + i].Reference(grouping_values[i]);
882: 			}
883: 		}
884: 		gstate.finished = true;
885: 		return SourceResultType::FINISHED;
886: 	}
887: 
888: 	while (!gstate.finished && chunk.size() == 0) {
889: 		if (lstate.TaskFinished()) {
890: 			const auto res = gstate.AssignTask(sink, lstate, input.interrupt_state);
891: 			if (res != SourceResultType::HAVE_MORE_OUTPUT) {
892: 				D_ASSERT(res == SourceResultType::FINISHED || res == SourceResultType::BLOCKED);
893: 				return res;
894: 			}
895: 		}
896: 		lstate.ExecuteTask(sink, gstate, chunk);
897: 	}
898: 
899: 	if (chunk.size() != 0) {
900: 		return SourceResultType::HAVE_MORE_OUTPUT;
901: 	} else {
902: 		return SourceResultType::FINISHED;
903: 	}
904: }
905: 
906: double RadixPartitionedHashTable::GetProgress(ClientContext &, GlobalSinkState &sink_p,
907:                                               GlobalSourceState &gstate_p) const {
908: 	auto &sink = sink_p.Cast<RadixHTGlobalSinkState>();
909: 	auto &gstate = gstate_p.Cast<RadixHTGlobalSourceState>();
910: 
911: 	// Get partition combine progress, weigh it 2x
912: 	double total_progress = 0;
913: 	for (auto &partition : sink.partitions) {
914: 		total_progress += 2.0 * partition->progress;
915: 	}
916: 
917: 	// Get scan progress, weigh it 1x
918: 	total_progress += 1.0 * double(gstate.task_done);
919: 
920: 	// Divide by 3x for the weights, and the number of partitions to get a value between 0 and 1 again
921: 	total_progress /= 3.0 * double(sink.partitions.size());
922: 
923: 	// Multiply by 100 to get a percentage
924: 	return 100.0 * total_progress;
925: }
926: 
927: } // namespace duckdb
[end of src/execution/radix_partitioned_hashtable.cpp]
[start of src/function/aggregate/distributive/first.cpp]
1: #include "duckdb/common/exception.hpp"
2: #include "duckdb/common/vector_operations/vector_operations.hpp"
3: #include "duckdb/function/create_sort_key.hpp"
4: #include "duckdb/function/aggregate/distributive_functions.hpp"
5: #include "duckdb/planner/expression.hpp"
6: 
7: namespace duckdb {
8: 
9: template <class T>
10: struct FirstState {
11: 	T value;
12: 	bool is_set;
13: 	bool is_null;
14: };
15: 
16: struct FirstFunctionBase {
17: 	template <class STATE>
18: 	static void Initialize(STATE &state) {
19: 		state.is_set = false;
20: 		state.is_null = false;
21: 	}
22: 
23: 	static bool IgnoreNull() {
24: 		return false;
25: 	}
26: };
27: 
28: template <bool LAST, bool SKIP_NULLS>
29: struct FirstFunction : public FirstFunctionBase {
30: 	template <class INPUT_TYPE, class STATE, class OP>
31: 	static void Operation(STATE &state, const INPUT_TYPE &input, AggregateUnaryInput &unary_input) {
32: 		if (LAST || !state.is_set) {
33: 			if (!unary_input.RowIsValid()) {
34: 				if (!SKIP_NULLS) {
35: 					state.is_set = true;
36: 				}
37: 				state.is_null = true;
38: 			} else {
39: 				state.is_set = true;
40: 				state.is_null = false;
41: 				state.value = input;
42: 			}
43: 		}
44: 	}
45: 
46: 	template <class INPUT_TYPE, class STATE, class OP>
47: 	static void ConstantOperation(STATE &state, const INPUT_TYPE &input, AggregateUnaryInput &unary_input,
48: 	                              idx_t count) {
49: 		Operation<INPUT_TYPE, STATE, OP>(state, input, unary_input);
50: 	}
51: 
52: 	template <class STATE, class OP>
53: 	static void Combine(const STATE &source, STATE &target, AggregateInputData &) {
54: 		if (!target.is_set) {
55: 			target = source;
56: 		}
57: 	}
58: 
59: 	template <class T, class STATE>
60: 	static void Finalize(STATE &state, T &target, AggregateFinalizeData &finalize_data) {
61: 		if (!state.is_set || state.is_null) {
62: 			finalize_data.ReturnNull();
63: 		} else {
64: 			target = state.value;
65: 		}
66: 	}
67: };
68: 
69: template <bool LAST, bool SKIP_NULLS>
70: struct FirstFunctionStringBase : public FirstFunctionBase {
71: 	template <class STATE>
72: 	static void SetValue(STATE &state, AggregateInputData &input_data, string_t value, bool is_null) {
73: 		if (LAST && state.is_set) {
74: 			Destroy(state, input_data);
75: 		}
76: 		if (is_null) {
77: 			if (!SKIP_NULLS) {
78: 				state.is_set = true;
79: 				state.is_null = true;
80: 			}
81: 		} else {
82: 			state.is_set = true;
83: 			state.is_null = false;
84: 			if (value.IsInlined()) {
85: 				state.value = value;
86: 			} else {
87: 				// non-inlined string, need to allocate space for it
88: 				auto len = value.GetSize();
89: 				auto ptr = LAST ? new char[len] : char_ptr_cast(input_data.allocator.Allocate(len));
90: 				memcpy(ptr, value.GetData(), len);
91: 
92: 				state.value = string_t(ptr, UnsafeNumericCast<uint32_t>(len));
93: 			}
94: 		}
95: 	}
96: 
97: 	template <class STATE, class OP>
98: 	static void Combine(const STATE &source, STATE &target, AggregateInputData &input_data) {
99: 		if (source.is_set && (LAST || !target.is_set)) {
100: 			SetValue(target, input_data, source.value, source.is_null);
101: 		}
102: 	}
103: 
104: 	template <class STATE>
105: 	static void Destroy(STATE &state, AggregateInputData &) {
106: 		if (state.is_set && !state.is_null && !state.value.IsInlined()) {
107: 			delete[] state.value.GetData();
108: 		}
109: 	}
110: };
111: 
112: template <bool LAST, bool SKIP_NULLS>
113: struct FirstFunctionString : FirstFunctionStringBase<LAST, SKIP_NULLS> {
114: 	template <class INPUT_TYPE, class STATE, class OP>
115: 	static void Operation(STATE &state, const INPUT_TYPE &input, AggregateUnaryInput &unary_input) {
116: 		if (LAST || !state.is_set) {
117: 			FirstFunctionStringBase<LAST, SKIP_NULLS>::template SetValue<STATE>(state, unary_input.input, input,
118: 			                                                                    !unary_input.RowIsValid());
119: 		}
120: 	}
121: 
122: 	template <class INPUT_TYPE, class STATE, class OP>
123: 	static void ConstantOperation(STATE &state, const INPUT_TYPE &input, AggregateUnaryInput &unary_input,
124: 	                              idx_t count) {
125: 		Operation<INPUT_TYPE, STATE, OP>(state, input, unary_input);
126: 	}
127: 
128: 	template <class T, class STATE>
129: 	static void Finalize(STATE &state, T &target, AggregateFinalizeData &finalize_data) {
130: 		if (!state.is_set || state.is_null) {
131: 			finalize_data.ReturnNull();
132: 		} else {
133: 			target = StringVector::AddStringOrBlob(finalize_data.result, state.value);
134: 		}
135: 	}
136: };
137: 
138: template <bool LAST, bool SKIP_NULLS>
139: struct FirstVectorFunction : FirstFunctionStringBase<LAST, SKIP_NULLS> {
140: 	using STATE = FirstState<string_t>;
141: 
142: 	static void Update(Vector inputs[], AggregateInputData &input_data, idx_t, Vector &state_vector, idx_t count) {
143: 		auto &input = inputs[0];
144: 		UnifiedVectorFormat idata;
145: 		input.ToUnifiedFormat(count, idata);
146: 
147: 		UnifiedVectorFormat sdata;
148: 		state_vector.ToUnifiedFormat(count, sdata);
149: 
150: 		sel_t assign_sel[STANDARD_VECTOR_SIZE];
151: 		idx_t assign_count = 0;
152: 
153: 		auto states = UnifiedVectorFormat::GetData<STATE *>(sdata);
154: 		for (idx_t i = 0; i < count; i++) {
155: 			const auto idx = idata.sel->get_index(i);
156: 			bool is_null = !idata.validity.RowIsValid(idx);
157: 			if (SKIP_NULLS && is_null) {
158: 				continue;
159: 			}
160: 			auto &state = *states[sdata.sel->get_index(i)];
161: 			if (!LAST && state.is_set) {
162: 				continue;
163: 			}
164: 			assign_sel[assign_count++] = NumericCast<sel_t>(i);
165: 		}
166: 		if (assign_count == 0) {
167: 			// fast path - nothing to set
168: 			return;
169: 		}
170: 
171: 		Vector sort_key(LogicalType::BLOB);
172: 		OrderModifiers modifiers(OrderType::ASCENDING, OrderByNullType::NULLS_LAST);
173: 		// slice with a selection vector and generate sort keys
174: 		if (assign_count == count) {
175: 			CreateSortKeyHelpers::CreateSortKey(input, count, modifiers, sort_key);
176: 		} else {
177: 			SelectionVector sel(assign_sel);
178: 			Vector sliced_input(input, sel, assign_count);
179: 			CreateSortKeyHelpers::CreateSortKey(sliced_input, assign_count, modifiers, sort_key);
180: 		}
181: 		auto sort_key_data = FlatVector::GetData<string_t>(sort_key);
182: 
183: 		// now assign sort keys
184: 		for (idx_t i = 0; i < assign_count; i++) {
185: 			const auto state_idx = sdata.sel->get_index(assign_sel[i]);
186: 			auto &state = *states[state_idx];
187: 			if (!LAST && state.is_set) {
188: 				continue;
189: 			}
190: 
191: 			const auto idx = idata.sel->get_index(assign_sel[i]);
192: 			bool is_null = !idata.validity.RowIsValid(idx);
193: 			FirstFunctionStringBase<LAST, SKIP_NULLS>::template SetValue<STATE>(state, input_data, sort_key_data[i],
194: 			                                                                    is_null);
195: 		}
196: 	}
197: 
198: 	template <class STATE>
199: 	static void Finalize(STATE &state, AggregateFinalizeData &finalize_data) {
200: 		if (!state.is_set || state.is_null) {
201: 			finalize_data.ReturnNull();
202: 		} else {
203: 			CreateSortKeyHelpers::DecodeSortKey(state.value, finalize_data.result, finalize_data.result_idx,
204: 			                                    OrderModifiers(OrderType::ASCENDING, OrderByNullType::NULLS_LAST));
205: 		}
206: 	}
207: 
208: 	static unique_ptr<FunctionData> Bind(ClientContext &context, AggregateFunction &function,
209: 	                                     vector<unique_ptr<Expression>> &arguments) {
210: 		function.arguments[0] = arguments[0]->return_type;
211: 		function.return_type = arguments[0]->return_type;
212: 		return nullptr;
213: 	}
214: };
215: 
216: template <class T, bool LAST, bool SKIP_NULLS>
217: static AggregateFunction GetFirstAggregateTemplated(LogicalType type) {
218: 	return AggregateFunction::UnaryAggregate<FirstState<T>, T, T, FirstFunction<LAST, SKIP_NULLS>>(type, type);
219: }
220: 
221: template <bool LAST, bool SKIP_NULLS>
222: static AggregateFunction GetFirstFunction(const LogicalType &type);
223: 
224: template <bool LAST, bool SKIP_NULLS>
225: AggregateFunction GetDecimalFirstFunction(const LogicalType &type) {
226: 	D_ASSERT(type.id() == LogicalTypeId::DECIMAL);
227: 	switch (type.InternalType()) {
228: 	case PhysicalType::INT16:
229: 		return GetFirstFunction<LAST, SKIP_NULLS>(LogicalType::SMALLINT);
230: 	case PhysicalType::INT32:
231: 		return GetFirstFunction<LAST, SKIP_NULLS>(LogicalType::INTEGER);
232: 	case PhysicalType::INT64:
233: 		return GetFirstFunction<LAST, SKIP_NULLS>(LogicalType::BIGINT);
234: 	default:
235: 		return GetFirstFunction<LAST, SKIP_NULLS>(LogicalType::HUGEINT);
236: 	}
237: }
238: template <bool LAST, bool SKIP_NULLS>
239: static AggregateFunction GetFirstFunction(const LogicalType &type) {
240: 	if (type.id() == LogicalTypeId::DECIMAL) {
241: 		type.Verify();
242: 		AggregateFunction function = GetDecimalFirstFunction<LAST, SKIP_NULLS>(type);
243: 		function.arguments[0] = type;
244: 		function.return_type = type;
245: 		return function;
246: 	}
247: 	switch (type.InternalType()) {
248: 	case PhysicalType::BOOL:
249: 	case PhysicalType::INT8:
250: 		return GetFirstAggregateTemplated<int8_t, LAST, SKIP_NULLS>(type);
251: 	case PhysicalType::INT16:
252: 		return GetFirstAggregateTemplated<int16_t, LAST, SKIP_NULLS>(type);
253: 	case PhysicalType::INT32:
254: 		return GetFirstAggregateTemplated<int32_t, LAST, SKIP_NULLS>(type);
255: 	case PhysicalType::INT64:
256: 		return GetFirstAggregateTemplated<int64_t, LAST, SKIP_NULLS>(type);
257: 	case PhysicalType::UINT8:
258: 		return GetFirstAggregateTemplated<uint8_t, LAST, SKIP_NULLS>(type);
259: 	case PhysicalType::UINT16:
260: 		return GetFirstAggregateTemplated<uint16_t, LAST, SKIP_NULLS>(type);
261: 	case PhysicalType::UINT32:
262: 		return GetFirstAggregateTemplated<uint32_t, LAST, SKIP_NULLS>(type);
263: 	case PhysicalType::UINT64:
264: 		return GetFirstAggregateTemplated<uint64_t, LAST, SKIP_NULLS>(type);
265: 	case PhysicalType::INT128:
266: 		return GetFirstAggregateTemplated<hugeint_t, LAST, SKIP_NULLS>(type);
267: 	case PhysicalType::UINT128:
268: 		return GetFirstAggregateTemplated<uhugeint_t, LAST, SKIP_NULLS>(type);
269: 	case PhysicalType::FLOAT:
270: 		return GetFirstAggregateTemplated<float, LAST, SKIP_NULLS>(type);
271: 	case PhysicalType::DOUBLE:
272: 		return GetFirstAggregateTemplated<double, LAST, SKIP_NULLS>(type);
273: 	case PhysicalType::INTERVAL:
274: 		return GetFirstAggregateTemplated<interval_t, LAST, SKIP_NULLS>(type);
275: 	case PhysicalType::VARCHAR:
276: 		if (LAST) {
277: 			return AggregateFunction::UnaryAggregateDestructor<FirstState<string_t>, string_t, string_t,
278: 			                                                   FirstFunctionString<LAST, SKIP_NULLS>>(type, type);
279: 		} else {
280: 			return AggregateFunction::UnaryAggregate<FirstState<string_t>, string_t, string_t,
281: 			                                         FirstFunctionString<LAST, SKIP_NULLS>>(type, type);
282: 		}
283: 	default: {
284: 		using OP = FirstVectorFunction<LAST, SKIP_NULLS>;
285: 		using STATE = FirstState<string_t>;
286: 		return AggregateFunction(
287: 		    {type}, type, AggregateFunction::StateSize<STATE>, AggregateFunction::StateInitialize<STATE, OP>,
288: 		    OP::Update, AggregateFunction::StateCombine<STATE, OP>, AggregateFunction::StateVoidFinalize<STATE, OP>,
289: 		    nullptr, OP::Bind, LAST ? AggregateFunction::StateDestroy<STATE, OP> : nullptr, nullptr, nullptr);
290: 	}
291: 	}
292: }
293: 
294: AggregateFunction FirstFun::GetFunction(const LogicalType &type) {
295: 	auto fun = GetFirstFunction<false, false>(type);
296: 	fun.name = "first";
297: 	return fun;
298: }
299: 
300: template <bool LAST, bool SKIP_NULLS>
301: unique_ptr<FunctionData> BindDecimalFirst(ClientContext &context, AggregateFunction &function,
302:                                           vector<unique_ptr<Expression>> &arguments) {
303: 	auto decimal_type = arguments[0]->return_type;
304: 	auto name = std::move(function.name);
305: 	function = GetFirstFunction<LAST, SKIP_NULLS>(decimal_type);
306: 	function.name = std::move(name);
307: 	function.return_type = decimal_type;
308: 	return nullptr;
309: }
310: 
311: template <bool LAST, bool SKIP_NULLS>
312: static AggregateFunction GetFirstOperator(const LogicalType &type) {
313: 	if (type.id() == LogicalTypeId::DECIMAL) {
314: 		throw InternalException("FIXME: this shouldn't happen...");
315: 	}
316: 	return GetFirstFunction<LAST, SKIP_NULLS>(type);
317: }
318: 
319: template <bool LAST, bool SKIP_NULLS>
320: unique_ptr<FunctionData> BindFirst(ClientContext &context, AggregateFunction &function,
321:                                    vector<unique_ptr<Expression>> &arguments) {
322: 	auto input_type = arguments[0]->return_type;
323: 	auto name = std::move(function.name);
324: 	function = GetFirstOperator<LAST, SKIP_NULLS>(input_type);
325: 	function.name = std::move(name);
326: 	if (function.bind) {
327: 		return function.bind(context, function, arguments);
328: 	} else {
329: 		return nullptr;
330: 	}
331: }
332: 
333: template <bool LAST, bool SKIP_NULLS>
334: static void AddFirstOperator(AggregateFunctionSet &set) {
335: 	set.AddFunction(AggregateFunction({LogicalTypeId::DECIMAL}, LogicalTypeId::DECIMAL, nullptr, nullptr, nullptr,
336: 	                                  nullptr, nullptr, nullptr, BindDecimalFirst<LAST, SKIP_NULLS>));
337: 	set.AddFunction(AggregateFunction({LogicalType::ANY}, LogicalType::ANY, nullptr, nullptr, nullptr, nullptr, nullptr,
338: 	                                  nullptr, BindFirst<LAST, SKIP_NULLS>));
339: }
340: 
341: void FirstFun::RegisterFunction(BuiltinFunctions &set) {
342: 	AggregateFunctionSet first("first");
343: 	AggregateFunctionSet last("last");
344: 	AggregateFunctionSet any_value("any_value");
345: 
346: 	AddFirstOperator<false, false>(first);
347: 	AddFirstOperator<true, false>(last);
348: 	AddFirstOperator<false, true>(any_value);
349: 
350: 	set.AddFunction(first);
351: 	first.name = "arbitrary";
352: 	set.AddFunction(first);
353: 
354: 	set.AddFunction(last);
355: 
356: 	set.AddFunction(any_value);
357: }
358: 
359: } // namespace duckdb
[end of src/function/aggregate/distributive/first.cpp]
[start of src/include/duckdb/common/file_buffer.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/file_buffer.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/constants.hpp"
12: #include "duckdb/common/enums/debug_initialize.hpp"
13: 
14: namespace duckdb {
15: class Allocator;
16: struct FileHandle;
17: 
18: enum class FileBufferType : uint8_t { BLOCK = 1, MANAGED_BUFFER = 2, TINY_BUFFER = 3 };
19: 
20: static constexpr const idx_t FILE_BUFFER_TYPE_COUNT = 3;
21: 
22: //! The FileBuffer represents a buffer that can be read or written to a Direct IO FileHandle.
23: class FileBuffer {
24: public:
25: 	//! Allocates a buffer of the specified size, with room for additional header bytes
26: 	//! (typically 8 bytes). On return, this->AllocSize() >= this->size >= user_size.
27: 	//! Our allocation size will always be page-aligned, which is necessary to support
28: 	//! DIRECT_IO
29: 	FileBuffer(Allocator &allocator, FileBufferType type, uint64_t user_size);
30: 	FileBuffer(FileBuffer &source, FileBufferType type);
31: 
32: 	virtual ~FileBuffer();
33: 
34: 	Allocator &allocator;
35: 	//! The type of the buffer
36: 	FileBufferType type;
37: 	//! The buffer that users can write to
38: 	data_ptr_t buffer;
39: 	//! The size of the portion that users can write to, this is equivalent to internal_size - BLOCK_HEADER_SIZE
40: 	uint64_t size;
41: 
42: public:
43: 	//! Read into the FileBuffer from the specified location.
44: 	void Read(FileHandle &handle, uint64_t location);
45: 	//! Write the contents of the FileBuffer to the specified location.
46: 	void Write(FileHandle &handle, uint64_t location);
47: 
48: 	void Clear();
49: 
50: 	// Same rules as the constructor. We will add room for a header, in additio to
51: 	// the requested user bytes. We will then sector-align the result.
52: 	void Resize(uint64_t user_size);
53: 
54: 	uint64_t AllocSize() const {
55: 		return internal_size;
56: 	}
57: 	data_ptr_t InternalBuffer() {
58: 		return internal_buffer;
59: 	}
60: 
61: 	struct MemoryRequirement {
62: 		idx_t alloc_size;
63: 		idx_t header_size;
64: 	};
65: 
66: 	MemoryRequirement CalculateMemory(uint64_t user_size);
67: 
68: 	void Initialize(DebugInitialize info);
69: 
70: protected:
71: 	//! The pointer to the internal buffer that will be read or written, including the buffer header
72: 	data_ptr_t internal_buffer;
73: 	//! The aligned size as passed to the constructor. This is the size that is read or written to disk.
74: 	uint64_t internal_size;
75: 
76: 	void ReallocBuffer(size_t malloc_size);
77: 	void Init();
78: };
79: 
80: } // namespace duckdb
[end of src/include/duckdb/common/file_buffer.hpp]
[start of src/include/duckdb/common/radix_partitioning.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/radix_partitioning.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/fast_mem.hpp"
12: #include "duckdb/common/types/column/partitioned_column_data.hpp"
13: #include "duckdb/common/types/row/partitioned_tuple_data.hpp"
14: 
15: namespace duckdb {
16: 
17: class BufferManager;
18: class Vector;
19: struct UnifiedVectorFormat;
20: struct SelectionVector;
21: 
22: //! Generic radix partitioning functions
23: struct RadixPartitioning {
24: public:
25: 	//! 4096 partitions ought to be enough to go out-of-core properly
26: 	static constexpr const idx_t MAX_RADIX_BITS = 12;
27: 
28: 	//! The number of partitions for a given number of radix bits
29: 	static inline constexpr idx_t NumberOfPartitions(idx_t radix_bits) {
30: 		return idx_t(1) << radix_bits;
31: 	}
32: 
33: 	//! Inverse of NumberOfPartitions, given a number of partitions, get the number of radix bits
34: 	static inline idx_t RadixBits(idx_t n_partitions) {
35: 		D_ASSERT(IsPowerOfTwo(n_partitions));
36: 		for (idx_t r = 0; r < sizeof(idx_t) * 8; r++) {
37: 			if (n_partitions == NumberOfPartitions(r)) {
38: 				return r;
39: 			}
40: 		}
41: 		throw InternalException("RadixPartitioning::RadixBits unable to find partition count!");
42: 	}
43: 
44: 	//! Radix bits begin after uint16_t because these bits are used as salt in the aggregate HT
45: 	static inline constexpr idx_t Shift(idx_t radix_bits) {
46: 		return (sizeof(hash_t) - sizeof(uint16_t)) * 8 - radix_bits;
47: 	}
48: 
49: 	//! Mask of the radix bits of the hash
50: 	static inline constexpr hash_t Mask(idx_t radix_bits) {
51: 		return (hash_t(1 << radix_bits) - 1) << Shift(radix_bits);
52: 	}
53: 
54: 	//! Select using a cutoff on the radix bits of the hash
55: 	static idx_t Select(Vector &hashes, const SelectionVector *sel, idx_t count, idx_t radix_bits, idx_t cutoff,
56: 	                    SelectionVector *true_sel, SelectionVector *false_sel);
57: };
58: 
59: //! RadixPartitionedColumnData is a PartitionedColumnData that partitions input based on the radix of a hash
60: class RadixPartitionedColumnData : public PartitionedColumnData {
61: public:
62: 	RadixPartitionedColumnData(ClientContext &context, vector<LogicalType> types, idx_t radix_bits, idx_t hash_col_idx);
63: 	RadixPartitionedColumnData(const RadixPartitionedColumnData &other);
64: 	~RadixPartitionedColumnData() override;
65: 
66: 	idx_t GetRadixBits() const {
67: 		return radix_bits;
68: 	}
69: 
70: protected:
71: 	//===--------------------------------------------------------------------===//
72: 	// Radix Partitioning interface implementation
73: 	//===--------------------------------------------------------------------===//
74: 	idx_t BufferSize() const override {
75: 		switch (radix_bits) {
76: 		case 1:
77: 		case 2:
78: 		case 3:
79: 		case 4:
80: 			return GetBufferSize(1 << 1);
81: 		case 5:
82: 			return GetBufferSize(1 << 2);
83: 		case 6:
84: 			return GetBufferSize(1 << 3);
85: 		default:
86: 			return GetBufferSize(1 << 4);
87: 		}
88: 	}
89: 
90: 	void InitializeAppendStateInternal(PartitionedColumnDataAppendState &state) const override;
91: 	void ComputePartitionIndices(PartitionedColumnDataAppendState &state, DataChunk &input) override;
92: 	idx_t MaxPartitionIndex() const override {
93: 		return RadixPartitioning::NumberOfPartitions(radix_bits) - 1;
94: 	}
95: 
96: 	static constexpr idx_t GetBufferSize(idx_t div) {
97: 		return STANDARD_VECTOR_SIZE / div == 0 ? 1 : STANDARD_VECTOR_SIZE / div;
98: 	}
99: 
100: private:
101: 	//! The number of radix bits
102: 	const idx_t radix_bits;
103: 	//! The index of the column holding the hashes
104: 	const idx_t hash_col_idx;
105: };
106: 
107: //! RadixPartitionedTupleData is a PartitionedTupleData that partitions input based on the radix of a hash
108: class RadixPartitionedTupleData : public PartitionedTupleData {
109: public:
110: 	RadixPartitionedTupleData(BufferManager &buffer_manager, const TupleDataLayout &layout, idx_t radix_bits_p,
111: 	                          idx_t hash_col_idx_p);
112: 	RadixPartitionedTupleData(const RadixPartitionedTupleData &other);
113: 	~RadixPartitionedTupleData() override;
114: 
115: 	idx_t GetRadixBits() const {
116: 		return radix_bits;
117: 	}
118: 
119: private:
120: 	void Initialize();
121: 
122: protected:
123: 	//===--------------------------------------------------------------------===//
124: 	// Radix Partitioning interface implementation
125: 	//===--------------------------------------------------------------------===//
126: 	void InitializeAppendStateInternal(PartitionedTupleDataAppendState &state,
127: 	                                   TupleDataPinProperties properties) const override;
128: 	void ComputePartitionIndices(PartitionedTupleDataAppendState &state, DataChunk &input,
129: 	                             const SelectionVector &append_sel, const idx_t append_count) override;
130: 	void ComputePartitionIndices(Vector &row_locations, idx_t count, Vector &partition_indices) const override;
131: 	idx_t MaxPartitionIndex() const override {
132: 		return RadixPartitioning::NumberOfPartitions(radix_bits) - 1;
133: 	}
134: 
135: 	bool RepartitionReverseOrder() const override {
136: 		return true;
137: 	}
138: 	void RepartitionFinalizeStates(PartitionedTupleData &old_partitioned_data,
139: 	                               PartitionedTupleData &new_partitioned_data, PartitionedTupleDataAppendState &state,
140: 	                               idx_t finished_partition_idx) const override;
141: 
142: private:
143: 	//! The number of radix bits
144: 	const idx_t radix_bits;
145: 	//! The index of the column holding the hashes
146: 	const idx_t hash_col_idx;
147: };
148: 
149: } // namespace duckdb
[end of src/include/duckdb/common/radix_partitioning.hpp]
[start of src/include/duckdb/common/types/column/column_data_allocator.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/types/column/column_data_allocator.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/column/column_data_collection.hpp"
12: 
13: namespace duckdb {
14: 
15: struct ChunkMetaData;
16: struct VectorMetaData;
17: 
18: struct BlockMetaData {
19: 	//! The underlying block handle
20: 	shared_ptr<BlockHandle> handle;
21: 	//! How much space is currently used within the block
22: 	uint32_t size;
23: 	//! How much space is available in the block
24: 	uint32_t capacity;
25: 
26: 	uint32_t Capacity();
27: };
28: 
29: class ColumnDataAllocator {
30: public:
31: 	explicit ColumnDataAllocator(Allocator &allocator);
32: 	explicit ColumnDataAllocator(BufferManager &buffer_manager);
33: 	ColumnDataAllocator(ClientContext &context, ColumnDataAllocatorType allocator_type);
34: 	ColumnDataAllocator(ColumnDataAllocator &allocator);
35: 	~ColumnDataAllocator();
36: 
37: 	//! Returns an allocator object to allocate with. This returns the allocator in IN_MEMORY_ALLOCATOR, and a buffer
38: 	//! allocator in case of BUFFER_MANAGER_ALLOCATOR.
39: 	Allocator &GetAllocator();
40: 	//! Returns the buffer manager, if this is not an in-memory allocation.
41: 	BufferManager &GetBufferManager();
42: 	//! Returns the allocator type
43: 	ColumnDataAllocatorType GetType() {
44: 		return type;
45: 	}
46: 	void MakeShared() {
47: 		shared = true;
48: 	}
49: 	bool IsShared() const {
50: 		return shared;
51: 	}
52: 	idx_t BlockCount() const {
53: 		return blocks.size();
54: 	}
55: 	idx_t SizeInBytes() const {
56: 		idx_t total_size = 0;
57: 		for (const auto &block : blocks) {
58: 			total_size += block.size;
59: 		}
60: 		return total_size;
61: 	}
62: 	idx_t AllocationSize() const {
63: 		return allocated_size;
64: 	}
65: 
66: public:
67: 	void AllocateData(idx_t size, uint32_t &block_id, uint32_t &offset, ChunkManagementState *chunk_state);
68: 
69: 	void Initialize(ColumnDataAllocator &other);
70: 	void InitializeChunkState(ChunkManagementState &state, ChunkMetaData &meta_data);
71: 	data_ptr_t GetDataPointer(ChunkManagementState &state, uint32_t block_id, uint32_t offset);
72: 	void UnswizzlePointers(ChunkManagementState &state, Vector &result, idx_t v_offset, uint16_t count,
73: 	                       uint32_t block_id, uint32_t offset);
74: 
75: 	//! Prevents the block with the given id from being added to the eviction queue
76: 	void SetDestroyBufferUponUnpin(uint32_t block_id);
77: 
78: private:
79: 	void AllocateEmptyBlock(idx_t size);
80: 	BufferHandle AllocateBlock(idx_t size);
81: 	BufferHandle Pin(uint32_t block_id);
82: 
83: 	bool HasBlocks() const {
84: 		return !blocks.empty();
85: 	}
86: 
87: private:
88: 	void AllocateBuffer(idx_t size, uint32_t &block_id, uint32_t &offset, ChunkManagementState *chunk_state);
89: 	void AllocateMemory(idx_t size, uint32_t &block_id, uint32_t &offset, ChunkManagementState *chunk_state);
90: 	void AssignPointer(uint32_t &block_id, uint32_t &offset, data_ptr_t pointer);
91: 
92: private:
93: 	ColumnDataAllocatorType type;
94: 	union {
95: 		//! The allocator object (if this is a IN_MEMORY_ALLOCATOR)
96: 		Allocator *allocator;
97: 		//! The buffer manager (if this is a BUFFER_MANAGER_ALLOCATOR)
98: 		BufferManager *buffer_manager;
99: 	} alloc;
100: 	//! The set of blocks used by the column data collection
101: 	vector<BlockMetaData> blocks;
102: 	//! The set of allocated data
103: 	vector<AllocatedData> allocated_data;
104: 	//! Whether this ColumnDataAllocator is shared across ColumnDataCollections that allocate in parallel
105: 	bool shared = false;
106: 	//! Lock used in case this ColumnDataAllocator is shared across threads
107: 	mutex lock;
108: 	//! Total allocated size
109: 	idx_t allocated_size = 0;
110: };
111: 
112: } // namespace duckdb
[end of src/include/duckdb/common/types/column/column_data_allocator.hpp]
[start of src/include/duckdb/common/types/column/column_data_collection.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/types/column/column_data_collection.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/pair.hpp"
12: #include "duckdb/common/types/column/column_data_collection_iterators.hpp"
13: 
14: namespace duckdb {
15: class BufferManager;
16: class BlockHandle;
17: class ClientContext;
18: struct ColumnDataCopyFunction;
19: class ColumnDataAllocator;
20: class ColumnDataCollection;
21: class ColumnDataCollectionSegment;
22: class ColumnDataRowCollection;
23: 
24: //! The ColumnDataCollection represents a set of (buffer-managed) data stored in columnar format
25: //! It is efficient to read and scan
26: class ColumnDataCollection {
27: public:
28: 	//! Constructs an in-memory column data collection from an allocator
29: 	DUCKDB_API ColumnDataCollection(Allocator &allocator, vector<LogicalType> types);
30: 	//! Constructs an empty (but valid) in-memory column data collection from an allocator
31: 	DUCKDB_API explicit ColumnDataCollection(Allocator &allocator);
32: 	//! Constructs a buffer-managed column data collection
33: 	DUCKDB_API ColumnDataCollection(BufferManager &buffer_manager, vector<LogicalType> types);
34: 	//! Constructs either an in-memory or a buffer-managed column data collection
35: 	DUCKDB_API ColumnDataCollection(ClientContext &context, vector<LogicalType> types,
36: 	                                ColumnDataAllocatorType type = ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR);
37: 	//! Creates a column data collection that inherits the blocks to write to. This allows blocks to be shared
38: 	//! between multiple column data collections and prevents wasting space.
39: 	//! Note that after one CDC inherits blocks from another, the other
40: 	//! cannot be written to anymore (i.e. we take ownership of the half-written blocks).
41: 	DUCKDB_API ColumnDataCollection(ColumnDataCollection &parent);
42: 	DUCKDB_API ColumnDataCollection(shared_ptr<ColumnDataAllocator> allocator, vector<LogicalType> types);
43: 	DUCKDB_API ~ColumnDataCollection();
44: 
45: public:
46: 	//! The types of columns in the ColumnDataCollection
47: 	vector<LogicalType> &Types() {
48: 		return types;
49: 	}
50: 	const vector<LogicalType> &Types() const {
51: 		return types;
52: 	}
53: 
54: 	//! The amount of rows in the ColumnDataCollection
55: 	const idx_t &Count() const {
56: 		return count;
57: 	}
58: 
59: 	//! The amount of columns in the ColumnDataCollection
60: 	idx_t ColumnCount() const {
61: 		return types.size();
62: 	}
63: 
64: 	//! The size (in bytes) of this ColumnDataCollection
65: 	idx_t SizeInBytes() const;
66: 	//! The allocation size (in bytes) of this ColumnDataCollection - this property is cached
67: 	idx_t AllocationSize() const;
68: 
69: 	//! Get the allocator
70: 	DUCKDB_API Allocator &GetAllocator() const;
71: 
72: 	//! Initializes an Append state - useful for optimizing many appends made to the same column data collection
73: 	DUCKDB_API void InitializeAppend(ColumnDataAppendState &state);
74: 	//! Append a DataChunk to this ColumnDataCollection using the specified append state
75: 	DUCKDB_API void Append(ColumnDataAppendState &state, DataChunk &new_chunk);
76: 
77: 	//! Initializes a chunk with the correct types that can be used to call Scan
78: 	DUCKDB_API void InitializeScanChunk(DataChunk &chunk) const;
79: 	//! Initializes a chunk with the correct types for a given scan state
80: 	DUCKDB_API void InitializeScanChunk(ColumnDataScanState &state, DataChunk &chunk) const;
81: 	//! Initializes a Scan state for scanning all columns
82: 	DUCKDB_API void
83: 	InitializeScan(ColumnDataScanState &state,
84: 	               ColumnDataScanProperties properties = ColumnDataScanProperties::ALLOW_ZERO_COPY) const;
85: 	//! Initializes a Scan state for scanning a subset of the columns
86: 	DUCKDB_API void
87: 	InitializeScan(ColumnDataScanState &state, vector<column_t> column_ids,
88: 	               ColumnDataScanProperties properties = ColumnDataScanProperties::ALLOW_ZERO_COPY) const;
89: 	//! Initialize a parallel scan over the column data collection over all columns
90: 	DUCKDB_API void
91: 	InitializeScan(ColumnDataParallelScanState &state,
92: 	               ColumnDataScanProperties properties = ColumnDataScanProperties::ALLOW_ZERO_COPY) const;
93: 	//! Initialize a parallel scan over the column data collection over a subset of the columns
94: 	DUCKDB_API void
95: 	InitializeScan(ColumnDataParallelScanState &state, vector<column_t> column_ids,
96: 	               ColumnDataScanProperties properties = ColumnDataScanProperties::ALLOW_ZERO_COPY) const;
97: 	//! Scans a DataChunk from the ColumnDataCollection
98: 	DUCKDB_API bool Scan(ColumnDataScanState &state, DataChunk &result) const;
99: 	//! Scans a DataChunk from the ColumnDataCollection
100: 	DUCKDB_API bool Scan(ColumnDataParallelScanState &state, ColumnDataLocalScanState &lstate, DataChunk &result) const;
101: 
102: 	//! Append a DataChunk directly to this ColumnDataCollection - calls InitializeAppend and Append internally
103: 	DUCKDB_API void Append(DataChunk &new_chunk);
104: 
105: 	//! Appends the other ColumnDataCollection to this, destroying the other data collection
106: 	DUCKDB_API void Combine(ColumnDataCollection &other);
107: 
108: 	DUCKDB_API void Verify();
109: 
110: 	DUCKDB_API string ToString() const;
111: 	DUCKDB_API void Print() const;
112: 
113: 	DUCKDB_API void Reset();
114: 
115: 	//! Returns the number of data chunks present in the ColumnDataCollection
116: 	DUCKDB_API idx_t ChunkCount() const;
117: 	//! Fetch an individual chunk from the ColumnDataCollection
118: 	DUCKDB_API void FetchChunk(idx_t chunk_idx, DataChunk &result) const;
119: 
120: 	//! Constructs a class that can be iterated over to fetch individual chunks
121: 	//! Iterating over this is syntactic sugar over just calling Scan
122: 	DUCKDB_API ColumnDataChunkIterationHelper Chunks() const;
123: 	//! Constructs a class that can be iterated over to fetch individual chunks
124: 	//! Only the column indexes specified in the column_ids list are scanned
125: 	DUCKDB_API ColumnDataChunkIterationHelper Chunks(vector<column_t> column_ids) const;
126: 
127: 	//! Constructs a class that can be iterated over to fetch individual rows
128: 	//! Note that row iteration is slow, and the `.Chunks()` method should be used instead
129: 	DUCKDB_API ColumnDataRowIterationHelper Rows() const;
130: 
131: 	//! Returns a materialized set of all of the rows in the column data collection
132: 	//! Note that usage of this is slow - avoid using this unless the amount of rows is small, or if you do not care
133: 	//! about performance
134: 	DUCKDB_API ColumnDataRowCollection GetRows() const;
135: 
136: 	//! Compare two column data collections to another. If they are equal according to result equality rules,
137: 	//! return true. That means null values are equal, and approx equality is used for floating point values.
138: 	//! If they are not equal, return false and fill in the error message.
139: 	static bool ResultEquals(const ColumnDataCollection &left, const ColumnDataCollection &right, string &error_message,
140: 	                         bool ordered = false);
141: 
142: 	//! Obtains the next scan index to scan from
143: 	bool NextScanIndex(ColumnDataScanState &state, idx_t &chunk_index, idx_t &segment_index, idx_t &row_index) const;
144: 	//! Obtains the previous scan index to scan from
145: 	bool PrevScanIndex(ColumnDataScanState &state, idx_t &chunk_index, idx_t &segment_index, idx_t &row_index) const;
146: 	//! Scans at the indices (obtained from NextScanIndex)
147: 	void ScanAtIndex(ColumnDataParallelScanState &state, ColumnDataLocalScanState &lstate, DataChunk &result,
148: 	                 idx_t chunk_index, idx_t segment_index, idx_t row_index) const;
149: 
150: 	//! Seeks to the chunk _containing_ the row. Returns false if it is past the end.
151: 	//! Note that the returned chunk will likely not be aligned to the given row
152: 	//! but the scan state will provide the actual range
153: 	bool Seek(idx_t row_idx, ColumnDataScanState &state, DataChunk &result) const;
154: 
155: 	//! Initialize the column data collection
156: 	void Initialize(vector<LogicalType> types);
157: 
158: 	//! Get references to the string heaps in this ColumnDataCollection
159: 	vector<shared_ptr<StringHeap>> GetHeapReferences();
160: 	//! Get the allocator type of this ColumnDataCollection
161: 	ColumnDataAllocatorType GetAllocatorType() const;
162: 
163: 	//! Get a vector of the segments in this ColumnDataCollection
164: 	const vector<unique_ptr<ColumnDataCollectionSegment>> &GetSegments() const;
165: 
166: 	void Serialize(Serializer &serializer) const;
167: 	static unique_ptr<ColumnDataCollection> Deserialize(Deserializer &deserializer);
168: 
169: private:
170: 	//! Creates a new segment within the ColumnDataCollection
171: 	void CreateSegment();
172: 
173: 	static ColumnDataCopyFunction GetCopyFunction(const LogicalType &type);
174: 
175: private:
176: 	//! The Column Data Allocator
177: 	buffer_ptr<ColumnDataAllocator> allocator;
178: 	//! The types of the stored entries
179: 	vector<LogicalType> types;
180: 	//! The number of entries stored in the column data collection
181: 	idx_t count;
182: 	//! The data segments of the column data collection
183: 	vector<unique_ptr<ColumnDataCollectionSegment>> segments;
184: 	//! The set of copy functions
185: 	vector<ColumnDataCopyFunction> copy_functions;
186: 	//! When the column data collection is marked as finished - new tuples can no longer be appended to it
187: 	bool finished_append;
188: };
189: 
190: //! The ColumnDataRowCollection represents a set of materialized rows, as obtained from the ColumnDataCollection
191: class ColumnDataRowCollection {
192: public:
193: 	DUCKDB_API explicit ColumnDataRowCollection(const ColumnDataCollection &collection);
194: 
195: public:
196: 	DUCKDB_API Value GetValue(idx_t column, idx_t index) const;
197: 
198: public:
199: 	// container API
200: 	bool empty() const {     // NOLINT: match stl API
201: 		return rows.empty(); // NOLINT
202: 	}
203: 	idx_t size() const { // NOLINT: match stl API
204: 		return rows.size();
205: 	}
206: 
207: 	DUCKDB_API ColumnDataRow &operator[](idx_t i);
208: 	DUCKDB_API const ColumnDataRow &operator[](idx_t i) const;
209: 
210: 	vector<ColumnDataRow>::iterator begin() { // NOLINT: match stl API
211: 		return rows.begin();
212: 	}
213: 	vector<ColumnDataRow>::iterator end() { // NOLINT: match stl API
214: 		return rows.end();
215: 	}
216: 	vector<ColumnDataRow>::const_iterator cbegin() const { // NOLINT: match stl API
217: 		return rows.cbegin();
218: 	}
219: 	vector<ColumnDataRow>::const_iterator cend() const { // NOLINT: match stl API
220: 		return rows.cend();
221: 	}
222: 	vector<ColumnDataRow>::const_iterator begin() const { // NOLINT: match stl API
223: 		return rows.begin();
224: 	}
225: 	vector<ColumnDataRow>::const_iterator end() const { // NOLINT: match stl API
226: 		return rows.end();
227: 	}
228: 
229: private:
230: 	vector<ColumnDataRow> rows;
231: 	vector<unique_ptr<DataChunk>> chunks;
232: 	ColumnDataScanState scan_state;
233: };
234: 
235: } // namespace duckdb
[end of src/include/duckdb/common/types/column/column_data_collection.hpp]
[start of src/include/duckdb/common/types/row/partitioned_tuple_data.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/types/row/partitioned_tuple_data.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/fixed_size_map.hpp"
12: #include "duckdb/common/optional_idx.hpp"
13: #include "duckdb/common/perfect_map_set.hpp"
14: #include "duckdb/common/types/row/tuple_data_allocator.hpp"
15: #include "duckdb/common/types/row/tuple_data_collection.hpp"
16: 
17: namespace duckdb {
18: 
19: //! Local state for parallel partitioning
20: struct PartitionedTupleDataAppendState {
21: public:
22: 	PartitionedTupleDataAppendState() : partition_indices(LogicalType::UBIGINT) {
23: 	}
24: 
25: public:
26: 	Vector partition_indices;
27: 	SelectionVector partition_sel;
28: 	SelectionVector reverse_partition_sel;
29: 
30: 	static constexpr idx_t MAP_THRESHOLD = 256;
31: 	perfect_map_t<list_entry_t> partition_entries;
32: 	fixed_size_map_t<list_entry_t> fixed_partition_entries;
33: 
34: 	unsafe_vector<unsafe_unique_ptr<TupleDataPinState>> partition_pin_states;
35: 	TupleDataChunkState chunk_state;
36: 
37: public:
38: 	template <bool fixed>
39: 	typename std::conditional<fixed, fixed_size_map_t<list_entry_t>, perfect_map_t<list_entry_t>>::type &GetMap() {
40: 		throw NotImplementedException("PartitionedTupleDataAppendState::GetMap for boolean value");
41: 	}
42: 
43: 	optional_idx GetPartitionIndexIfSinglePartition(const bool use_fixed_size_map) {
44: 		optional_idx result;
45: 		if (use_fixed_size_map) {
46: 			if (fixed_partition_entries.size() == 1) {
47: 				result = fixed_partition_entries.begin().GetKey();
48: 			}
49: 		} else {
50: 			if (partition_entries.size() == 1) {
51: 				result = partition_entries.begin()->first;
52: 			}
53: 		}
54: 		return result;
55: 	}
56: };
57: 
58: template <>
59: inline perfect_map_t<list_entry_t> &PartitionedTupleDataAppendState::GetMap<false>() {
60: 	return partition_entries;
61: }
62: 
63: template <>
64: inline fixed_size_map_t<list_entry_t> &PartitionedTupleDataAppendState::GetMap<true>() {
65: 	return fixed_partition_entries;
66: }
67: 
68: enum class PartitionedTupleDataType : uint8_t {
69: 	INVALID,
70: 	//! Radix partitioning on a hash column
71: 	RADIX
72: };
73: 
74: //! Shared allocators for parallel partitioning
75: struct PartitionTupleDataAllocators {
76: 	mutex lock;
77: 	vector<shared_ptr<TupleDataAllocator>> allocators;
78: };
79: 
80: //! PartitionedTupleData represents partitioned row data, which serves as an interface for different types of
81: //! partitioning, e.g., radix, hive
82: class PartitionedTupleData {
83: public:
84: 	virtual ~PartitionedTupleData();
85: 
86: public:
87: 	//! Get the layout of this PartitionedTupleData
88: 	const TupleDataLayout &GetLayout() const;
89: 	//! Get the partitioning type of this PartitionedTupleData
90: 	PartitionedTupleDataType GetType() const;
91: 	//! Initializes a local state for parallel partitioning that can be merged into this PartitionedTupleData
92: 	void InitializeAppendState(PartitionedTupleDataAppendState &state,
93: 	                           TupleDataPinProperties properties = TupleDataPinProperties::UNPIN_AFTER_DONE) const;
94: 	//! Appends a DataChunk to this PartitionedTupleData
95: 	void Append(PartitionedTupleDataAppendState &state, DataChunk &input,
96: 	            const SelectionVector &append_sel = *FlatVector::IncrementalSelectionVector(),
97: 	            const idx_t append_count = DConstants::INVALID_INDEX);
98: 	//! Appends a DataChunk to this PartitionedTupleData
99: 	//! - ToUnifiedFormat has already been called
100: 	void AppendUnified(PartitionedTupleDataAppendState &state, DataChunk &input,
101: 	                   const SelectionVector &append_sel = *FlatVector::IncrementalSelectionVector(),
102: 	                   const idx_t append_count = DConstants::INVALID_INDEX);
103: 	//! Appends rows to this PartitionedTupleData
104: 	void Append(PartitionedTupleDataAppendState &state, TupleDataChunkState &input, const idx_t count);
105: 	//! Flushes any remaining data in the append state into this PartitionedTupleData
106: 	void FlushAppendState(PartitionedTupleDataAppendState &state);
107: 	//! Combine another PartitionedTupleData into this PartitionedTupleData
108: 	void Combine(PartitionedTupleData &other);
109: 	//! Resets this PartitionedTupleData
110: 	void Reset();
111: 	//! Repartition this PartitionedTupleData into the new PartitionedTupleData
112: 	void Repartition(PartitionedTupleData &new_partitioned_data);
113: 	//! Unpins the data
114: 	void Unpin();
115: 	//! Get the partitions in this PartitionedTupleData
116: 	unsafe_vector<unique_ptr<TupleDataCollection>> &GetPartitions();
117: 	//! Get the data of this PartitionedTupleData as a single unpartitioned TupleDataCollection
118: 	unique_ptr<TupleDataCollection> GetUnpartitioned();
119: 	//! Get the count of this PartitionedTupleData
120: 	idx_t Count() const;
121: 	//! Get the size (in bytes) of this PartitionedTupleData
122: 	idx_t SizeInBytes() const;
123: 	//! Get the number of partitions of this PartitionedTupleData
124: 	idx_t PartitionCount() const;
125: 	//! Get the count and size of the largest partition
126: 	void GetSizesAndCounts(vector<idx_t> &partition_sizes, vector<idx_t> &partition_counts) const;
127: 	//! Converts this PartitionedTupleData to a string representation
128: 	string ToString();
129: 	//! Prints the string representation of this PartitionedTupleData
130: 	void Print();
131: 
132: protected:
133: 	//===--------------------------------------------------------------------===//
134: 	// Partitioning type implementation interface
135: 	//===--------------------------------------------------------------------===//
136: 	//! Initialize a PartitionedTupleDataAppendState for this type of partitioning (optional)
137: 	virtual void InitializeAppendStateInternal(PartitionedTupleDataAppendState &state,
138: 	                                           TupleDataPinProperties properties) const {
139: 	}
140: 	//! Compute the partition indices for this type of partitioning for the input DataChunk and store them in the
141: 	//! `partition_data` of the local state. If this type creates partitions on the fly (for, e.g., hive), this
142: 	//! function is also in charge of creating new partitions and mapping the input data to a partition index
143: 	virtual void ComputePartitionIndices(PartitionedTupleDataAppendState &state, DataChunk &input,
144: 	                                     const SelectionVector &append_sel, const idx_t append_count) {
145: 		throw NotImplementedException("ComputePartitionIndices for this type of PartitionedTupleData");
146: 	}
147: 	//! Compute partition indices from rows (similar to function above)
148: 	virtual void ComputePartitionIndices(Vector &row_locations, idx_t append_count, Vector &partition_indices) const {
149: 		throw NotImplementedException("ComputePartitionIndices for this type of PartitionedTupleData");
150: 	}
151: 	//! Maximum partition index (optional)
152: 	virtual idx_t MaxPartitionIndex() const {
153: 		return DConstants::INVALID_INDEX;
154: 	}
155: 
156: 	//! Whether or not to iterate over the original partitions in reverse order when repartitioning (optional)
157: 	virtual bool RepartitionReverseOrder() const {
158: 		return false;
159: 	}
160: 	//! Finalize states while repartitioning - useful for unpinning blocks that are no longer needed (optional)
161: 	virtual void RepartitionFinalizeStates(PartitionedTupleData &old_partitioned_data,
162: 	                                       PartitionedTupleData &new_partitioned_data,
163: 	                                       PartitionedTupleDataAppendState &state, idx_t finished_partition_idx) const {
164: 	}
165: 
166: protected:
167: 	//! PartitionedTupleData can only be instantiated by derived classes
168: 	PartitionedTupleData(PartitionedTupleDataType type, BufferManager &buffer_manager, const TupleDataLayout &layout);
169: 	PartitionedTupleData(const PartitionedTupleData &other);
170: 
171: 	//! Create a new shared allocator
172: 	void CreateAllocator();
173: 	//! Whether to use fixed size map or regular map
174: 	bool UseFixedSizeMap() const;
175: 	//! Builds a selection vector in the Append state for the partitions
176: 	//! - returns true if everything belongs to the same partition - stores partition index in single_partition_idx
177: 	void BuildPartitionSel(PartitionedTupleDataAppendState &state, const SelectionVector &append_sel,
178: 	                       const idx_t append_count) const;
179: 	template <bool fixed>
180: 	static void BuildPartitionSel(PartitionedTupleDataAppendState &state, const SelectionVector &append_sel,
181: 	                              const idx_t append_count);
182: 	//! Builds out the buffer space in the partitions
183: 	void BuildBufferSpace(PartitionedTupleDataAppendState &state);
184: 	template <bool fixed>
185: 	void BuildBufferSpace(PartitionedTupleDataAppendState &state);
186: 	//! Create a collection for a specific a partition
187: 	unique_ptr<TupleDataCollection> CreatePartitionCollection(idx_t partition_index) const {
188: 		if (allocators) {
189: 			return make_uniq<TupleDataCollection>(allocators->allocators[partition_index]);
190: 		} else {
191: 			return make_uniq<TupleDataCollection>(buffer_manager, layout);
192: 		}
193: 	}
194: 	//! Verify count/data size of this PartitionedTupleData
195: 	void Verify() const;
196: 
197: protected:
198: 	PartitionedTupleDataType type;
199: 	BufferManager &buffer_manager;
200: 	const TupleDataLayout layout;
201: 	idx_t count;
202: 	idx_t data_size;
203: 
204: 	mutex lock;
205: 	shared_ptr<PartitionTupleDataAllocators> allocators;
206: 	unsafe_vector<unique_ptr<TupleDataCollection>> partitions;
207: 
208: public:
209: 	template <class TARGET>
210: 	TARGET &Cast() {
211: 		DynamicCastCheck<TARGET>(this);
212: 		return reinterpret_cast<TARGET &>(*this);
213: 	}
214: 	template <class TARGET>
215: 	const TARGET &Cast() const {
216: 		DynamicCastCheck<TARGET>(this);
217: 		return reinterpret_cast<const TARGET &>(*this);
218: 	}
219: };
220: 
221: } // namespace duckdb
[end of src/include/duckdb/common/types/row/partitioned_tuple_data.hpp]
[start of src/include/duckdb/common/types/row/tuple_data_allocator.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/types/row/tuple_data_allocator.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/row/tuple_data_layout.hpp"
12: #include "duckdb/common/types/row/tuple_data_states.hpp"
13: 
14: namespace duckdb {
15: 
16: struct TupleDataSegment;
17: struct TupleDataChunk;
18: struct TupleDataChunkPart;
19: 
20: struct TupleDataBlock {
21: public:
22: 	TupleDataBlock(BufferManager &buffer_manager, idx_t capacity_p);
23: 
24: 	//! Disable copy constructors
25: 	TupleDataBlock(const TupleDataBlock &other) = delete;
26: 	TupleDataBlock &operator=(const TupleDataBlock &) = delete;
27: 
28: 	//! Enable move constructors
29: 	TupleDataBlock(TupleDataBlock &&other) noexcept;
30: 	TupleDataBlock &operator=(TupleDataBlock &&) noexcept;
31: 
32: public:
33: 	//! Remaining capacity (in bytes)
34: 	idx_t RemainingCapacity() const {
35: 		D_ASSERT(size <= capacity);
36: 		return capacity - size;
37: 	}
38: 
39: 	//! Remaining capacity (in rows)
40: 	idx_t RemainingCapacity(idx_t row_width) const {
41: 		return RemainingCapacity() / row_width;
42: 	}
43: 
44: public:
45: 	//! The underlying row block
46: 	shared_ptr<BlockHandle> handle;
47: 	//! Capacity (in bytes)
48: 	idx_t capacity;
49: 	//! Occupied size (in bytes)
50: 	idx_t size;
51: };
52: 
53: class TupleDataAllocator {
54: public:
55: 	TupleDataAllocator(BufferManager &buffer_manager, const TupleDataLayout &layout);
56: 	TupleDataAllocator(TupleDataAllocator &allocator);
57: 
58: 	~TupleDataAllocator();
59: 
60: 	//! Get the buffer manager
61: 	BufferManager &GetBufferManager();
62: 	//! Get the buffer allocator
63: 	Allocator &GetAllocator();
64: 	//! Get the layout
65: 	const TupleDataLayout &GetLayout() const;
66: 	//! Number of row blocks
67: 	idx_t RowBlockCount() const;
68: 	//! Number of heap blocks
69: 	idx_t HeapBlockCount() const;
70: 
71: public:
72: 	//! Builds out the chunks for next append, given the metadata in the append state
73: 	void Build(TupleDataSegment &segment, TupleDataPinState &pin_state, TupleDataChunkState &chunk_state,
74: 	           const idx_t append_offset, const idx_t append_count);
75: 	//! Initializes a chunk, making its pointers valid
76: 	void InitializeChunkState(TupleDataSegment &segment, TupleDataPinState &pin_state, TupleDataChunkState &chunk_state,
77: 	                          idx_t chunk_idx, bool init_heap);
78: 	static void RecomputeHeapPointers(Vector &old_heap_ptrs, const SelectionVector &old_heap_sel,
79: 	                                  const data_ptr_t row_locations[], Vector &new_heap_ptrs, const idx_t offset,
80: 	                                  const idx_t count, const TupleDataLayout &layout, const idx_t base_col_offset);
81: 	//! Releases or stores any handles in the management state that are no longer required
82: 	void ReleaseOrStoreHandles(TupleDataPinState &state, TupleDataSegment &segment, TupleDataChunk &chunk,
83: 	                           bool release_heap);
84: 	//! Releases or stores ALL handles in the management state
85: 	void ReleaseOrStoreHandles(TupleDataPinState &state, TupleDataSegment &segment);
86: 	//! Sets 'can_destroy' to true for all blocks so they aren't added to the eviction queue
87: 	void SetDestroyBufferUponUnpin();
88: 
89: private:
90: 	//! Builds out a single part (grabs the lock)
91: 	TupleDataChunkPart BuildChunkPart(TupleDataPinState &pin_state, TupleDataChunkState &chunk_state,
92: 	                                  const idx_t append_offset, const idx_t append_count, TupleDataChunk &chunk);
93: 	//! Internal function for InitializeChunkState
94: 	void InitializeChunkStateInternal(TupleDataPinState &pin_state, TupleDataChunkState &chunk_state, idx_t offset,
95: 	                                  bool recompute, bool init_heap_pointers, bool init_heap_sizes,
96: 	                                  unsafe_vector<reference<TupleDataChunkPart>> &parts);
97: 	//! Internal function for ReleaseOrStoreHandles
98: 	static void ReleaseOrStoreHandlesInternal(TupleDataSegment &segment,
99: 	                                          unsafe_vector<BufferHandle> &pinned_row_handles,
100: 	                                          perfect_map_t<BufferHandle> &handles, const perfect_set_t &block_ids,
101: 	                                          unsafe_vector<TupleDataBlock> &blocks, TupleDataPinProperties properties);
102: 	//! Pins the given row block
103: 	BufferHandle &PinRowBlock(TupleDataPinState &state, const TupleDataChunkPart &part);
104: 	//! Pins the given heap block
105: 	BufferHandle &PinHeapBlock(TupleDataPinState &state, const TupleDataChunkPart &part);
106: 	//! Gets the pointer to the rows for the given chunk part
107: 	data_ptr_t GetRowPointer(TupleDataPinState &state, const TupleDataChunkPart &part);
108: 	//! Gets the base pointer to the heap for the given chunk part
109: 	data_ptr_t GetBaseHeapPointer(TupleDataPinState &state, const TupleDataChunkPart &part);
110: 
111: private:
112: 	//! The buffer manager
113: 	BufferManager &buffer_manager;
114: 	//! The layout of the data
115: 	const TupleDataLayout layout;
116: 	//! Blocks storing the fixed-size rows
117: 	unsafe_vector<TupleDataBlock> row_blocks;
118: 	//! Blocks storing the variable-size data of the fixed-size rows (e.g., string, list)
119: 	unsafe_vector<TupleDataBlock> heap_blocks;
120: 
121: 	//! Re-usable arrays used while building buffer space
122: 	unsafe_vector<reference<TupleDataChunkPart>> chunk_parts;
123: 	unsafe_vector<pair<idx_t, idx_t>> chunk_part_indices;
124: };
125: 
126: } // namespace duckdb
[end of src/include/duckdb/common/types/row/tuple_data_allocator.hpp]
[start of src/include/duckdb/common/types/row/tuple_data_collection.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/types/row/tuple_data_collection.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/row/tuple_data_layout.hpp"
12: #include "duckdb/common/types/row/tuple_data_segment.hpp"
13: #include "duckdb/common/types/row/tuple_data_states.hpp"
14: 
15: namespace duckdb {
16: 
17: class TupleDataAllocator;
18: struct TupleDataScatterFunction;
19: struct TupleDataGatherFunction;
20: struct RowOperationsState;
21: 
22: typedef void (*tuple_data_scatter_function_t)(const Vector &source, const TupleDataVectorFormat &source_format,
23:                                               const SelectionVector &append_sel, const idx_t append_count,
24:                                               const TupleDataLayout &layout, const Vector &row_locations,
25:                                               Vector &heap_locations, const idx_t col_idx,
26:                                               const UnifiedVectorFormat &list_format,
27:                                               const vector<TupleDataScatterFunction> &child_functions);
28: 
29: struct TupleDataScatterFunction {
30: 	tuple_data_scatter_function_t function;
31: 	vector<TupleDataScatterFunction> child_functions;
32: };
33: 
34: typedef void (*tuple_data_gather_function_t)(const TupleDataLayout &layout, Vector &row_locations, const idx_t col_idx,
35:                                              const SelectionVector &scan_sel, const idx_t scan_count, Vector &target,
36:                                              const SelectionVector &target_sel, optional_ptr<Vector> list_vector,
37:                                              const vector<TupleDataGatherFunction> &child_functions);
38: 
39: struct TupleDataGatherFunction {
40: 	tuple_data_gather_function_t function;
41: 	vector<TupleDataGatherFunction> child_functions;
42: };
43: 
44: //! TupleDataCollection represents a set of buffer-managed data stored in row format
45: //! FIXME: rename to RowDataCollection after we phase it out
46: class TupleDataCollection {
47: 	friend class TupleDataChunkIterator;
48: 	friend class PartitionedTupleData;
49: 
50: public:
51: 	//! Constructs a TupleDataCollection with the specified layout
52: 	TupleDataCollection(BufferManager &buffer_manager, const TupleDataLayout &layout);
53: 	//! Constructs a TupleDataCollection with the same (shared) allocator
54: 	explicit TupleDataCollection(shared_ptr<TupleDataAllocator> allocator);
55: 
56: 	~TupleDataCollection();
57: 
58: public:
59: 	//! The layout of the stored rows
60: 	const TupleDataLayout &GetLayout() const;
61: 	//! The number of rows stored in the tuple data collection
62: 	const idx_t &Count() const;
63: 	//! The number of chunks stored in the tuple data collection
64: 	idx_t ChunkCount() const;
65: 	//! The size (in bytes) of the blocks held by this tuple data collection
66: 	idx_t SizeInBytes() const;
67: 	//! Unpins all held pins
68: 	void Unpin();
69: 
70: 	//! Gets the scatter function for the given type
71: 	static TupleDataScatterFunction GetScatterFunction(const LogicalType &type, bool within_collection = false);
72: 	//! Gets the gather function for the given type
73: 	static TupleDataGatherFunction GetGatherFunction(const LogicalType &type);
74: 
75: 	//! Initializes an Append state - useful for optimizing many appends made to the same tuple data collection
76: 	void InitializeAppend(TupleDataAppendState &append_state,
77: 	                      TupleDataPinProperties properties = TupleDataPinProperties::UNPIN_AFTER_DONE);
78: 	//! Initializes an Append state - useful for optimizing many appends made to the same tuple data collection
79: 	void InitializeAppend(TupleDataAppendState &append_state, vector<column_t> column_ids,
80: 	                      TupleDataPinProperties properties = TupleDataPinProperties::UNPIN_AFTER_DONE);
81: 	//! Initializes the Pin state of an Append state
82: 	//! - Useful for optimizing many appends made to the same tuple data collection
83: 	void InitializeAppend(TupleDataPinState &pin_state,
84: 	                      TupleDataPinProperties = TupleDataPinProperties::UNPIN_AFTER_DONE);
85: 	//! Initializes the Chunk state of an Append state
86: 	//! - Useful for optimizing many appends made to the same tuple data collection
87: 	void InitializeChunkState(TupleDataChunkState &chunk_state, vector<column_t> column_ids = {});
88: 	//! Initializes the Chunk state of an Append state
89: 	//! - Useful for optimizing many appends made to the same tuple data collection
90: 	static void InitializeChunkState(TupleDataChunkState &chunk_state, const vector<LogicalType> &types,
91: 	                                 vector<column_t> column_ids = {});
92: 	//! Append a DataChunk directly to this TupleDataCollection - calls InitializeAppend and Append internally
93: 	void Append(DataChunk &new_chunk, const SelectionVector &append_sel = *FlatVector::IncrementalSelectionVector(),
94: 	            idx_t append_count = DConstants::INVALID_INDEX);
95: 	//! Append a DataChunk directly to this TupleDataCollection - calls InitializeAppend and Append internally
96: 	void Append(DataChunk &new_chunk, vector<column_t> column_ids,
97: 	            const SelectionVector &append_sel = *FlatVector::IncrementalSelectionVector(),
98: 	            const idx_t append_count = DConstants::INVALID_INDEX);
99: 	//! Append a DataChunk to this TupleDataCollection using the specified Append state
100: 	void Append(TupleDataAppendState &append_state, DataChunk &new_chunk,
101: 	            const SelectionVector &append_sel = *FlatVector::IncrementalSelectionVector(),
102: 	            const idx_t append_count = DConstants::INVALID_INDEX);
103: 	//! Append a DataChunk to this TupleDataCollection using the specified pin and Chunk states
104: 	void Append(TupleDataPinState &pin_state, TupleDataChunkState &chunk_state, DataChunk &new_chunk,
105: 	            const SelectionVector &append_sel = *FlatVector::IncrementalSelectionVector(),
106: 	            const idx_t append_count = DConstants::INVALID_INDEX);
107: 	//! Append a DataChunk to this TupleDataCollection using the specified pin and Chunk states
108: 	//! - ToUnifiedFormat has already been called
109: 	void AppendUnified(TupleDataPinState &pin_state, TupleDataChunkState &chunk_state, DataChunk &new_chunk,
110: 	                   const SelectionVector &append_sel = *FlatVector::IncrementalSelectionVector(),
111: 	                   const idx_t append_count = DConstants::INVALID_INDEX);
112: 
113: 	//! Creates a UnifiedVectorFormat in the given Chunk state for the given DataChunk
114: 	static void ToUnifiedFormat(TupleDataChunkState &chunk_state, DataChunk &new_chunk);
115: 	//! Gets the UnifiedVectorFormat from the Chunk state as an array
116: 	static void GetVectorData(const TupleDataChunkState &chunk_state, UnifiedVectorFormat result[]);
117: 	//! Resets the cached cache vectors (used for ARRAY/LIST casts)
118: 	static void ResetCachedCastVectors(TupleDataChunkState &chunk_state, const vector<column_t> &column_ids);
119: 	//! Computes the heap sizes for the new DataChunk that will be appended
120: 	static void ComputeHeapSizes(TupleDataChunkState &chunk_state, const DataChunk &new_chunk,
121: 	                             const SelectionVector &append_sel, const idx_t append_count);
122: 
123: 	//! Builds out the buffer space for the specified Chunk state
124: 	void Build(TupleDataPinState &pin_state, TupleDataChunkState &chunk_state, const idx_t append_offset,
125: 	           const idx_t append_count);
126: 	//! Scatters the given DataChunk to the rows in the specified Chunk state
127: 	void Scatter(TupleDataChunkState &chunk_state, const DataChunk &new_chunk, const SelectionVector &append_sel,
128: 	             const idx_t append_count) const;
129: 	//! Scatters the given Vector to the given column id to the rows in the specified Chunk state
130: 	void Scatter(TupleDataChunkState &chunk_state, const Vector &source, const column_t column_id,
131: 	             const SelectionVector &append_sel, const idx_t append_count) const;
132: 	//! Copy rows from input to the built Chunk state
133: 	void CopyRows(TupleDataChunkState &chunk_state, TupleDataChunkState &input, const SelectionVector &append_sel,
134: 	              const idx_t append_count) const;
135: 
136: 	//! Finalizes the Pin state, releasing or storing blocks
137: 	void FinalizePinState(TupleDataPinState &pin_state, TupleDataSegment &segment);
138: 	//! Finalizes the Pin state, releasing or storing blocks
139: 	void FinalizePinState(TupleDataPinState &pin_state);
140: 
141: 	//! Appends the other TupleDataCollection to this, destroying the other data collection
142: 	void Combine(TupleDataCollection &other);
143: 	//! Appends the other TupleDataCollection to this, destroying the other data collection
144: 	void Combine(unique_ptr<TupleDataCollection> other);
145: 	//! Resets the TupleDataCollection, clearing all data
146: 	void Reset();
147: 
148: 	//! Initializes a chunk with the correct types that can be used to call Append/Scan
149: 	void InitializeChunk(DataChunk &chunk) const;
150: 	//! Initializes a chunk with the correct types that can be used to call Append/Scan for the given columns
151: 	void InitializeChunk(DataChunk &chunk, const vector<column_t> &columns) const;
152: 	//! Initializes a chunk with the correct types for a given scan state
153: 	void InitializeScanChunk(TupleDataScanState &state, DataChunk &chunk) const;
154: 	//! Initializes a Scan state for scanning all columns
155: 	void InitializeScan(TupleDataScanState &state,
156: 	                    TupleDataPinProperties properties = TupleDataPinProperties::UNPIN_AFTER_DONE) const;
157: 	//! Initializes a Scan state for scanning a subset of the columns
158: 	void InitializeScan(TupleDataScanState &state, vector<column_t> column_ids,
159: 	                    TupleDataPinProperties properties = TupleDataPinProperties::UNPIN_AFTER_DONE) const;
160: 	//! Initialize a parallel scan over the tuple data collection over all columns
161: 	void InitializeScan(TupleDataParallelScanState &state,
162: 	                    TupleDataPinProperties properties = TupleDataPinProperties::UNPIN_AFTER_DONE) const;
163: 	//! Initialize a parallel scan over the tuple data collection over a subset of the columns
164: 	void InitializeScan(TupleDataParallelScanState &gstate, vector<column_t> column_ids,
165: 	                    TupleDataPinProperties properties = TupleDataPinProperties::UNPIN_AFTER_DONE) const;
166: 	//! Scans a DataChunk from the TupleDataCollection
167: 	bool Scan(TupleDataScanState &state, DataChunk &result);
168: 	//! Scans a DataChunk from the TupleDataCollection
169: 	bool Scan(TupleDataParallelScanState &gstate, TupleDataLocalScanState &lstate, DataChunk &result);
170: 	//! Whether the last scan has been completed on this TupleDataCollection
171: 	bool ScanComplete(const TupleDataScanState &state) const;
172: 
173: 	//! Gathers a DataChunk from the TupleDataCollection, given the specific row locations (requires full pin)
174: 	void Gather(Vector &row_locations, const SelectionVector &scan_sel, const idx_t scan_count, DataChunk &result,
175: 	            const SelectionVector &target_sel, vector<unique_ptr<Vector>> &cached_cast_vectors) const;
176: 	//! Gathers a DataChunk (only the columns given by column_ids) from the TupleDataCollection,
177: 	//! given the specific row locations (requires full pin)
178: 	void Gather(Vector &row_locations, const SelectionVector &scan_sel, const idx_t scan_count,
179: 	            const vector<column_t> &column_ids, DataChunk &result, const SelectionVector &target_sel,
180: 	            vector<unique_ptr<Vector>> &cached_cast_vectors) const;
181: 	//! Gathers a Vector (from the given column id) from the TupleDataCollection
182: 	//! given the specific row locations (requires full pin)
183: 	void Gather(Vector &row_locations, const SelectionVector &sel, const idx_t scan_count, const column_t column_id,
184: 	            Vector &result, const SelectionVector &target_sel, optional_ptr<Vector> cached_cast_vector) const;
185: 
186: 	//! Converts this TupleDataCollection to a string representation
187: 	string ToString();
188: 	//! Prints the string representation of this TupleDataCollection
189: 	void Print();
190: 
191: 	//! Verify that all blocks are pinned
192: 	void VerifyEverythingPinned() const;
193: 
194: private:
195: 	//! Initializes the TupleDataCollection (called by the constructor)
196: 	void Initialize();
197: 	//! Gets all column ids
198: 	void GetAllColumnIDs(vector<column_t> &column_ids);
199: 	//! Adds a segment to this TupleDataCollection
200: 	void AddSegment(TupleDataSegment &&segment);
201: 
202: 	//! Computes the heap sizes for the specific Vector that will be appended
203: 	static void ComputeHeapSizes(Vector &heap_sizes_v, const Vector &source_v, TupleDataVectorFormat &source,
204: 	                             const SelectionVector &append_sel, const idx_t append_count);
205: 	//! Computes the heap sizes for the specific Vector that will be appended (within a list)
206: 	static void WithinCollectionComputeHeapSizes(Vector &heap_sizes_v, const Vector &source_v,
207: 	                                             TupleDataVectorFormat &source_format,
208: 	                                             const SelectionVector &append_sel, const idx_t append_count,
209: 	                                             const UnifiedVectorFormat &list_data);
210: 	//! Computes the heap sizes for the fixed-size type Vector that will be appended (within a list)
211: 	static void ComputeFixedWithinCollectionHeapSizes(Vector &heap_sizes_v, const Vector &source_v,
212: 	                                                  TupleDataVectorFormat &source_format,
213: 	                                                  const SelectionVector &append_sel, const idx_t append_count,
214: 	                                                  const UnifiedVectorFormat &list_data);
215: 	//! Computes the heap sizes for the string Vector that will be appended (within a list)
216: 	static void StringWithinCollectionComputeHeapSizes(Vector &heap_sizes_v, const Vector &source_v,
217: 	                                                   TupleDataVectorFormat &source_format,
218: 	                                                   const SelectionVector &append_sel, const idx_t append_count,
219: 	                                                   const UnifiedVectorFormat &list_data);
220: 	//! Computes the heap sizes for the struct Vector that will be appended (within a list)
221: 	static void StructWithinCollectionComputeHeapSizes(Vector &heap_sizes_v, const Vector &source_v,
222: 	                                                   TupleDataVectorFormat &source_format,
223: 	                                                   const SelectionVector &append_sel, const idx_t append_count,
224: 	                                                   const UnifiedVectorFormat &list_data);
225: 	//! Computes the heap sizes for the list Vector that will be appended (within a list)
226: 	static void CollectionWithinCollectionComputeHeapSizes(Vector &heap_sizes_v, const Vector &source_v,
227: 	                                                       TupleDataVectorFormat &source_format,
228: 	                                                       const SelectionVector &append_sel, const idx_t append_count,
229: 	                                                       const UnifiedVectorFormat &list_data);
230: 
231: 	//! Get the next segment/chunk index for the scan
232: 	bool NextScanIndex(TupleDataScanState &scan_state, idx_t &segment_index, idx_t &chunk_index);
233: 	//! Scans the chunk at the given segment/chunk indices
234: 	void ScanAtIndex(TupleDataPinState &pin_state, TupleDataChunkState &chunk_state, const vector<column_t> &column_ids,
235: 	                 idx_t segment_index, idx_t chunk_index, DataChunk &result);
236: 
237: 	//! Verify count/data size of this collection
238: 	void Verify() const;
239: 
240: private:
241: 	//! The layout of the TupleDataCollection
242: 	const TupleDataLayout layout;
243: 	//! The TupleDataAllocator
244: 	shared_ptr<TupleDataAllocator> allocator;
245: 	//! The number of entries stored in the TupleDataCollection
246: 	idx_t count;
247: 	//! The size (in bytes) of this TupleDataCollection
248: 	idx_t data_size;
249: 	//! The data segments of the TupleDataCollection
250: 	unsafe_vector<TupleDataSegment> segments;
251: 	//! The set of scatter functions
252: 	vector<TupleDataScatterFunction> scatter_functions;
253: 	//! The set of gather functions
254: 	vector<TupleDataGatherFunction> gather_functions;
255: };
256: 
257: } // namespace duckdb
[end of src/include/duckdb/common/types/row/tuple_data_collection.hpp]
[start of src/include/duckdb/execution/aggregate_hashtable.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/aggregate_hashtable.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/row_operations/row_matcher.hpp"
12: #include "duckdb/common/types/row/partitioned_tuple_data.hpp"
13: #include "duckdb/execution/base_aggregate_hashtable.hpp"
14: #include "duckdb/execution/ht_entry.hpp"
15: #include "duckdb/storage/arena_allocator.hpp"
16: #include "duckdb/storage/buffer/buffer_handle.hpp"
17: 
18: namespace duckdb {
19: 
20: class BlockHandle;
21: class BufferHandle;
22: 
23: struct FlushMoveState;
24: 
25: //! GroupedAggregateHashTable is a linear probing HT that is used for computing
26: //! aggregates
27: /*!
28:     GroupedAggregateHashTable is a HT that is used for computing aggregates. It takes
29:    as input the set of groups and the types of the aggregates to compute and
30:    stores them in the HT. It uses linear probing for collision resolution.
31: */
32: 
33: class GroupedAggregateHashTable : public BaseAggregateHashTable {
34: public:
35: 	GroupedAggregateHashTable(ClientContext &context, Allocator &allocator, vector<LogicalType> group_types,
36: 	                          vector<LogicalType> payload_types, const vector<BoundAggregateExpression *> &aggregates,
37: 	                          idx_t initial_capacity = InitialCapacity(), idx_t radix_bits = 0);
38: 	GroupedAggregateHashTable(ClientContext &context, Allocator &allocator, vector<LogicalType> group_types,
39: 	                          vector<LogicalType> payload_types, vector<AggregateObject> aggregates,
40: 	                          idx_t initial_capacity = InitialCapacity(), idx_t radix_bits = 0);
41: 	GroupedAggregateHashTable(ClientContext &context, Allocator &allocator, vector<LogicalType> group_types);
42: 	~GroupedAggregateHashTable() override;
43: 
44: public:
45: 	//! The hash table load factor, when a resize is triggered
46: 	constexpr static double LOAD_FACTOR = 1.5;
47: 
48: 	//! Get the layout of this HT
49: 	const TupleDataLayout &GetLayout() const;
50: 	//! Number of groups in the HT
51: 	idx_t Count() const;
52: 	//! Initial capacity of the HT
53: 	static idx_t InitialCapacity();
54: 	//! Capacity that can hold 'count' entries without resizing
55: 	static idx_t GetCapacityForCount(idx_t count);
56: 	//! Current capacity of the HT
57: 	idx_t Capacity() const;
58: 	//! Threshold at which to resize the HT
59: 	idx_t ResizeThreshold() const;
60: 
61: 	//! Add the given data to the HT, computing the aggregates grouped by the
62: 	//! data in the group chunk. When resize = true, aggregates will not be
63: 	//! computed but instead just assigned.
64: 	idx_t AddChunk(DataChunk &groups, DataChunk &payload, const unsafe_vector<idx_t> &filter);
65: 	idx_t AddChunk(DataChunk &groups, Vector &group_hashes, DataChunk &payload, const unsafe_vector<idx_t> &filter);
66: 	idx_t AddChunk(DataChunk &groups, DataChunk &payload, AggregateType filter);
67: 
68: 	//! Fetch the aggregates for specific groups from the HT and place them in the result
69: 	void FetchAggregates(DataChunk &groups, DataChunk &result);
70: 
71: 	//! Finds or creates groups in the hashtable using the specified group keys. The addresses vector will be filled
72: 	//! with pointers to the groups in the hash table, and the new_groups selection vector will point to the newly
73: 	//! created groups. The return value is the amount of newly created groups.
74: 	idx_t FindOrCreateGroups(DataChunk &groups, Vector &group_hashes, Vector &addresses_out,
75: 	                         SelectionVector &new_groups_out);
76: 	idx_t FindOrCreateGroups(DataChunk &groups, Vector &addresses_out, SelectionVector &new_groups_out);
77: 	void FindOrCreateGroups(DataChunk &groups, Vector &addresses_out);
78: 
79: 	unique_ptr<PartitionedTupleData> &GetPartitionedData();
80: 	shared_ptr<ArenaAllocator> GetAggregateAllocator();
81: 
82: 	//! Resize the HT to the specified size. Must be larger than the current size.
83: 	void Resize(idx_t size);
84: 	//! Resets the pointer table of the HT to all 0's
85: 	void ClearPointerTable();
86: 	//! Resets the group count to 0
87: 	void ResetCount();
88: 	//! Set the radix bits for this HT
89: 	void SetRadixBits(idx_t radix_bits);
90: 	//! Initializes the PartitionedTupleData
91: 	void InitializePartitionedData();
92: 
93: 	//! Executes the filter(if any) and update the aggregates
94: 	void Combine(GroupedAggregateHashTable &other);
95: 	void Combine(TupleDataCollection &other_data, optional_ptr<atomic<double>> progress = nullptr);
96: 
97: 	//! Unpins the data blocks
98: 	void UnpinData();
99: 
100: private:
101: 	//! Efficiently matches groups
102: 	RowMatcher row_matcher;
103: 
104: 	//! Append state
105: 	struct AggregateHTAppendState {
106: 		AggregateHTAppendState();
107: 
108: 		PartitionedTupleDataAppendState append_state;
109: 
110: 		Vector ht_offsets;
111: 		Vector hash_salts;
112: 		SelectionVector group_compare_vector;
113: 		SelectionVector no_match_vector;
114: 		SelectionVector empty_vector;
115: 		SelectionVector new_groups;
116: 		Vector addresses;
117: 		unsafe_unique_array<UnifiedVectorFormat> group_data;
118: 		DataChunk group_chunk;
119: 	} state;
120: 
121: 	//! The number of radix bits to partition by
122: 	idx_t radix_bits;
123: 	//! The data of the HT
124: 	unique_ptr<PartitionedTupleData> partitioned_data;
125: 
126: 	//! Predicates for matching groups (always ExpressionType::COMPARE_EQUAL)
127: 	vector<ExpressionType> predicates;
128: 
129: 	//! The number of groups in the HT
130: 	idx_t count;
131: 	//! The capacity of the HT. This can be increased using GroupedAggregateHashTable::Resize
132: 	idx_t capacity;
133: 	//! The hash map (pointer table) of the HT: allocated data and pointer into it
134: 	AllocatedData hash_map;
135: 	ht_entry_t *entries;
136: 	//! Offset of the hash column in the rows
137: 	idx_t hash_offset;
138: 	//! Bitmask for getting relevant bits from the hashes to determine the position
139: 	hash_t bitmask;
140: 
141: 	//! The active arena allocator used by the aggregates for their internal state
142: 	shared_ptr<ArenaAllocator> aggregate_allocator;
143: 	//! Owning arena allocators that this HT has data from
144: 	vector<shared_ptr<ArenaAllocator>> stored_allocators;
145: 
146: private:
147: 	//! Disabled the copy constructor
148: 	GroupedAggregateHashTable(const GroupedAggregateHashTable &) = delete;
149: 	//! Destroy the HT
150: 	void Destroy();
151: 
152: 	//! Apply bitmask to get the entry in the HT
153: 	inline idx_t ApplyBitMask(hash_t hash) const;
154: 
155: 	//! Does the actual group matching / creation
156: 	idx_t FindOrCreateGroupsInternal(DataChunk &groups, Vector &group_hashes, Vector &addresses,
157: 	                                 SelectionVector &new_groups);
158: 
159: 	//! Verify the pointer table of the HT
160: 	void Verify();
161: };
162: 
163: } // namespace duckdb
[end of src/include/duckdb/execution/aggregate_hashtable.hpp]
[start of src/include/duckdb/storage/buffer/block_handle.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/buffer/block_handle.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/atomic.hpp"
12: #include "duckdb/common/common.hpp"
13: #include "duckdb/common/enums/destroy_buffer_upon.hpp"
14: #include "duckdb/common/enums/memory_tag.hpp"
15: #include "duckdb/common/file_buffer.hpp"
16: #include "duckdb/common/mutex.hpp"
17: #include "duckdb/common/numeric_utils.hpp"
18: #include "duckdb/storage/storage_info.hpp"
19: 
20: namespace duckdb {
21: 
22: class BlockManager;
23: class BufferHandle;
24: class BufferPool;
25: class DatabaseInstance;
26: 
27: enum class BlockState : uint8_t { BLOCK_UNLOADED = 0, BLOCK_LOADED = 1 };
28: 
29: struct BufferPoolReservation {
30: 	MemoryTag tag;
31: 	idx_t size {0};
32: 	BufferPool &pool;
33: 
34: 	BufferPoolReservation(MemoryTag tag, BufferPool &pool);
35: 	BufferPoolReservation(const BufferPoolReservation &) = delete;
36: 	BufferPoolReservation &operator=(const BufferPoolReservation &) = delete;
37: 
38: 	BufferPoolReservation(BufferPoolReservation &&) noexcept;
39: 	BufferPoolReservation &operator=(BufferPoolReservation &&) noexcept;
40: 
41: 	~BufferPoolReservation();
42: 
43: 	void Resize(idx_t new_size);
44: 	void Merge(BufferPoolReservation src);
45: };
46: 
47: struct TempBufferPoolReservation : BufferPoolReservation {
48: 	TempBufferPoolReservation(MemoryTag tag, BufferPool &pool, idx_t size) : BufferPoolReservation(tag, pool) {
49: 		Resize(size);
50: 	}
51: 	TempBufferPoolReservation(TempBufferPoolReservation &&) = default;
52: 	~TempBufferPoolReservation() {
53: 		Resize(0);
54: 	}
55: };
56: 
57: class BlockHandle : public enable_shared_from_this<BlockHandle> {
58: 	friend class BlockManager;
59: 	friend struct BufferEvictionNode;
60: 	friend class BufferHandle;
61: 	friend class BufferManager;
62: 	friend class StandardBufferManager;
63: 	friend class BufferPool;
64: 	friend struct EvictionQueue;
65: 
66: public:
67: 	BlockHandle(BlockManager &block_manager, block_id_t block_id, MemoryTag tag);
68: 	BlockHandle(BlockManager &block_manager, block_id_t block_id, MemoryTag tag, unique_ptr<FileBuffer> buffer,
69: 	            DestroyBufferUpon destroy_buffer_upon, idx_t block_size, BufferPoolReservation &&reservation);
70: 	~BlockHandle();
71: 
72: 	BlockManager &block_manager;
73: 
74: public:
75: 	block_id_t BlockId() {
76: 		return block_id;
77: 	}
78: 
79: 	void ResizeBuffer(idx_t block_size, int64_t memory_delta) {
80: 		D_ASSERT(buffer);
81: 		// resize and adjust current memory
82: 		buffer->Resize(block_size);
83: 		memory_usage = NumericCast<idx_t>(NumericCast<int64_t>(memory_usage) + memory_delta);
84: 		D_ASSERT(memory_usage == buffer->AllocSize());
85: 	}
86: 
87: 	int32_t Readers() const {
88: 		return readers;
89: 	}
90: 
91: 	inline bool IsSwizzled() const {
92: 		return !unswizzled;
93: 	}
94: 
95: 	inline void SetSwizzling(const char *unswizzler) {
96: 		unswizzled = unswizzler;
97: 	}
98: 
99: 	MemoryTag GetMemoryTag() const {
100: 		return tag;
101: 	}
102: 
103: 	inline void SetDestroyBufferUpon(DestroyBufferUpon destroy_buffer_upon_p) {
104: 		lock_guard<mutex> guard(lock);
105: 		destroy_buffer_upon = destroy_buffer_upon_p;
106: 	}
107: 
108: 	inline bool MustAddToEvictionQueue() const {
109: 		return destroy_buffer_upon != DestroyBufferUpon::UNPIN;
110: 	}
111: 
112: 	inline bool MustWriteToTemporaryFile() const {
113: 		return destroy_buffer_upon == DestroyBufferUpon::BLOCK;
114: 	}
115: 
116: 	inline const idx_t &GetMemoryUsage() const {
117: 		return memory_usage;
118: 	}
119: 	bool IsUnloaded() {
120: 		return state == BlockState::BLOCK_UNLOADED;
121: 	}
122: 
123: private:
124: 	BufferHandle Load(unique_ptr<FileBuffer> buffer = nullptr);
125: 	BufferHandle LoadFromBuffer(data_ptr_t data, unique_ptr<FileBuffer> reusable_buffer);
126: 	unique_ptr<FileBuffer> UnloadAndTakeBlock();
127: 	void Unload();
128: 	bool CanUnload();
129: 
130: 	//! The block-level lock
131: 	mutex lock;
132: 	//! Whether or not the block is loaded/unloaded
133: 	atomic<BlockState> state;
134: 	//! Amount of concurrent readers
135: 	atomic<int32_t> readers;
136: 	//! The block id of the block
137: 	const block_id_t block_id;
138: 	//! Memory tag
139: 	MemoryTag tag;
140: 	//! Pointer to loaded data (if any)
141: 	unique_ptr<FileBuffer> buffer;
142: 	//! Internal eviction sequence number
143: 	atomic<idx_t> eviction_seq_num;
144: 	//! LRU timestamp (for age-based eviction)
145: 	atomic<int64_t> lru_timestamp_msec;
146: 	//! When to destroy the data buffer
147: 	DestroyBufferUpon destroy_buffer_upon;
148: 	//! The memory usage of the block (when loaded). If we are pinning/loading
149: 	//! an unloaded block, this tells us how much memory to reserve.
150: 	idx_t memory_usage;
151: 	//! Current memory reservation / usage
152: 	BufferPoolReservation memory_charge;
153: 	//! Does the block contain any memory pointers?
154: 	const char *unswizzled;
155: };
156: 
157: } // namespace duckdb
[end of src/include/duckdb/storage/buffer/block_handle.hpp]
[start of src/include/duckdb/storage/buffer/buffer_pool.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/buffer/buffer_pool.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/array.hpp"
12: #include "duckdb/common/enums/memory_tag.hpp"
13: #include "duckdb/common/file_buffer.hpp"
14: #include "duckdb/common/mutex.hpp"
15: #include "duckdb/common/typedefs.hpp"
16: #include "duckdb/storage/buffer/block_handle.hpp"
17: 
18: namespace duckdb {
19: 
20: class TemporaryMemoryManager;
21: struct EvictionQueue;
22: 
23: struct BufferEvictionNode {
24: 	BufferEvictionNode() {
25: 	}
26: 	BufferEvictionNode(weak_ptr<BlockHandle> handle_p, idx_t eviction_seq_num);
27: 
28: 	weak_ptr<BlockHandle> handle;
29: 	idx_t handle_sequence_number;
30: 
31: 	bool CanUnload(BlockHandle &handle_p);
32: 	shared_ptr<BlockHandle> TryGetBlockHandle();
33: };
34: 
35: //! The BufferPool is in charge of handling memory management for one or more databases. It defines memory limits
36: //! and implements priority eviction among all users of the pool.
37: class BufferPool {
38: 	friend class BlockHandle;
39: 	friend class BlockManager;
40: 	friend class BufferManager;
41: 	friend class StandardBufferManager;
42: 
43: public:
44: 	BufferPool(idx_t maximum_memory, bool track_eviction_timestamps, idx_t allocator_bulk_deallocation_flush_threshold);
45: 	virtual ~BufferPool();
46: 
47: 	//! Set a new memory limit to the buffer pool, throws an exception if the new limit is too low and not enough
48: 	//! blocks can be evicted
49: 	void SetLimit(idx_t limit, const char *exception_postscript);
50: 
51: 	//! If bulk deallocation larger than this occurs, flush outstanding allocations
52: 	void SetAllocatorBulkDeallocationFlushThreshold(idx_t threshold);
53: 	idx_t GetAllocatorBulkDeallocationFlushThreshold();
54: 
55: 	void UpdateUsedMemory(MemoryTag tag, int64_t size);
56: 
57: 	idx_t GetUsedMemory() const;
58: 
59: 	idx_t GetMaxMemory() const;
60: 
61: 	virtual idx_t GetQueryMaxMemory() const;
62: 
63: 	TemporaryMemoryManager &GetTemporaryMemoryManager();
64: 
65: protected:
66: 	//! Evict blocks until the currently used memory + extra_memory fit, returns false if this was not possible
67: 	//! (i.e. not enough blocks could be evicted)
68: 	//! If the "buffer" argument is specified AND the system can find a buffer to re-use for the given allocation size
69: 	//! "buffer" will be made to point to the re-usable memory. Note that this is not guaranteed.
70: 	//! Returns a pair. result.first indicates if eviction was successful. result.second contains the
71: 	//! reservation handle, which can be moved to the BlockHandle that will own the reservation.
72: 	struct EvictionResult {
73: 		bool success;
74: 		TempBufferPoolReservation reservation;
75: 	};
76: 	virtual EvictionResult EvictBlocks(MemoryTag tag, idx_t extra_memory, idx_t memory_limit,
77: 	                                   unique_ptr<FileBuffer> *buffer = nullptr);
78: 	virtual EvictionResult EvictBlocksInternal(EvictionQueue &queue, MemoryTag tag, idx_t extra_memory,
79: 	                                           idx_t memory_limit, unique_ptr<FileBuffer> *buffer = nullptr);
80: 
81: 	//! Purge all blocks that haven't been pinned within the last N seconds
82: 	idx_t PurgeAgedBlocks(uint32_t max_age_sec);
83: 	idx_t PurgeAgedBlocksInternal(EvictionQueue &queue, uint32_t max_age_sec, int64_t now, int64_t limit);
84: 	//! Garbage collect dead nodes in the eviction queue.
85: 	void PurgeQueue(FileBufferType type);
86: 	//! Add a buffer handle to the eviction queue. Returns true, if the queue is
87: 	//! ready to be purged, and false otherwise.
88: 	bool AddToEvictionQueue(shared_ptr<BlockHandle> &handle);
89: 	//! Gets the eviction queue for the specified type
90: 	EvictionQueue &GetEvictionQueueForType(FileBufferType type);
91: 	//! Increments the dead nodes for the queue with specified type
92: 	void IncrementDeadNodes(FileBufferType type);
93: 
94: protected:
95: 	enum class MemoryUsageCaches {
96: 		FLUSH,
97: 		NO_FLUSH,
98: 	};
99: 
100: 	struct MemoryUsage {
101: 		//! The maximum difference between memory statistics and actual usage is 2MB (64 * 32k)
102: 		static constexpr idx_t MEMORY_USAGE_CACHE_COUNT = 64;
103: 		static constexpr idx_t MEMORY_USAGE_CACHE_THRESHOLD = 32 << 10;
104: 		static constexpr idx_t TOTAL_MEMORY_USAGE_INDEX = MEMORY_TAG_COUNT;
105: 		using MemoryUsageCounters = array<atomic<int64_t>, MEMORY_TAG_COUNT + 1>;
106: 
107: 		//! global memory usage counters
108: 		MemoryUsageCounters memory_usage;
109: 		//! cache memory usage to improve performance
110: 		array<MemoryUsageCounters, MEMORY_USAGE_CACHE_COUNT> memory_usage_caches;
111: 
112: 		MemoryUsage();
113: 
114: 		idx_t GetUsedMemory(MemoryUsageCaches cache) {
115: 			return GetUsedMemory(TOTAL_MEMORY_USAGE_INDEX, cache);
116: 		}
117: 
118: 		idx_t GetUsedMemory(MemoryTag tag, MemoryUsageCaches cache) {
119: 			return GetUsedMemory((idx_t)tag, cache);
120: 		}
121: 
122: 		idx_t GetUsedMemory(idx_t index, MemoryUsageCaches cache) {
123: 			if (cache == MemoryUsageCaches::NO_FLUSH) {
124: 				auto used_memory = memory_usage[index].load(std::memory_order_relaxed);
125: 				return used_memory > 0 ? static_cast<idx_t>(used_memory) : 0;
126: 			}
127: 			int64_t cached = 0;
128: 			for (auto &cache : memory_usage_caches) {
129: 				cached += cache[index].exchange(0, std::memory_order_relaxed);
130: 			}
131: 			auto used_memory = memory_usage[index].fetch_add(cached, std::memory_order_relaxed) + cached;
132: 			return used_memory > 0 ? static_cast<idx_t>(used_memory) : 0;
133: 		}
134: 
135: 		void UpdateUsedMemory(MemoryTag tag, int64_t size);
136: 	};
137: 
138: 	//! The lock for changing the memory limit
139: 	mutex limit_lock;
140: 	//! The maximum amount of memory that the buffer manager can keep (in bytes)
141: 	atomic<idx_t> maximum_memory;
142: 	//! If bulk deallocation larger than this occurs, flush outstanding allocations
143: 	atomic<idx_t> allocator_bulk_deallocation_flush_threshold;
144: 	//! Record timestamps of buffer manager unpin() events. Usable by custom eviction policies.
145: 	bool track_eviction_timestamps;
146: 	//! Eviction queues
147: 	vector<unique_ptr<EvictionQueue>> queues;
148: 	//! Memory manager for concurrently used temporary memory, e.g., for physical operators
149: 	unique_ptr<TemporaryMemoryManager> temporary_memory_manager;
150: 	//! To improve performance, MemoryUsage maintains counter caches based on current cpu or thread id,
151: 	//! and only updates the global counter when the cache value exceeds a threshold.
152: 	//! Therefore, the statistics may have slight differences from the actual memory usage.
153: 	mutable MemoryUsage memory_usage;
154: };
155: 
156: } // namespace duckdb
[end of src/include/duckdb/storage/buffer/buffer_pool.hpp]
[start of src/include/duckdb/storage/buffer_manager.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/buffer_manager.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/storage/buffer_manager.hpp"
12: #include "duckdb/storage/buffer/buffer_handle.hpp"
13: #include "duckdb/storage/block_manager.hpp"
14: #include "duckdb/common/file_system.hpp"
15: #include "duckdb/common/enums/memory_tag.hpp"
16: #include "duckdb/storage/buffer/temporary_file_information.hpp"
17: #include "duckdb/main/config.hpp"
18: 
19: namespace duckdb {
20: 
21: class Allocator;
22: class BufferPool;
23: class TemporaryMemoryManager;
24: 
25: class BufferManager {
26: 	friend class BufferHandle;
27: 	friend class BlockHandle;
28: 	friend class BlockManager;
29: 
30: public:
31: 	BufferManager() {
32: 	}
33: 	virtual ~BufferManager() {
34: 	}
35: 
36: public:
37: 	virtual BufferHandle Allocate(MemoryTag tag, idx_t block_size, bool can_destroy = true) = 0;
38: 	//! Reallocate an in-memory buffer that is pinned.
39: 	virtual void ReAllocate(shared_ptr<BlockHandle> &handle, idx_t block_size) = 0;
40: 	virtual BufferHandle Pin(shared_ptr<BlockHandle> &handle) = 0;
41: 	//! Prefetch a series of blocks. Note that this is a performance suggestion.
42: 	virtual void Prefetch(vector<shared_ptr<BlockHandle>> &handles) = 0;
43: 	virtual void Unpin(shared_ptr<BlockHandle> &handle) = 0;
44: 
45: 	//! Returns the currently allocated memory
46: 	virtual idx_t GetUsedMemory() const = 0;
47: 	//! Returns the maximum available memory
48: 	virtual idx_t GetMaxMemory() const = 0;
49: 	//! Returns the currently used swap space
50: 	virtual idx_t GetUsedSwap() = 0;
51: 	//! Returns the maximum swap space that can be used
52: 	virtual optional_idx GetMaxSwap() const = 0;
53: 	//! Returns the block allocation size for buffer-managed blocks.
54: 	virtual idx_t GetBlockAllocSize() const = 0;
55: 	//! Returns the block size for buffer-managed blocks.
56: 	virtual idx_t GetBlockSize() const = 0;
57: 
58: 	//! Returns a new block of transient memory.
59: 	virtual shared_ptr<BlockHandle> RegisterTransientMemory(const idx_t size, const idx_t block_size);
60: 	//! Returns a new block of memory that is smaller than the block size setting.
61: 	virtual shared_ptr<BlockHandle> RegisterSmallMemory(const idx_t size);
62: 
63: 	virtual DUCKDB_API Allocator &GetBufferAllocator();
64: 	virtual DUCKDB_API void ReserveMemory(idx_t size);
65: 	virtual DUCKDB_API void FreeReservedMemory(idx_t size);
66: 	virtual vector<MemoryInformation> GetMemoryUsageInfo() const = 0;
67: 	//! Set a new memory limit to the buffer manager, throws an exception if the new limit is too low and not enough
68: 	//! blocks can be evicted
69: 	virtual void SetMemoryLimit(idx_t limit = (idx_t)-1);
70: 	virtual void SetSwapLimit(optional_idx limit = optional_idx());
71: 
72: 	virtual vector<TemporaryFileInformation> GetTemporaryFiles();
73: 	virtual const string &GetTemporaryDirectory() const;
74: 	virtual void SetTemporaryDirectory(const string &new_dir);
75: 	virtual bool HasTemporaryDirectory() const;
76: 
77: 	//! Construct a managed buffer.
78: 	virtual unique_ptr<FileBuffer> ConstructManagedBuffer(idx_t size, unique_ptr<FileBuffer> &&source,
79: 	                                                      FileBufferType type = FileBufferType::MANAGED_BUFFER);
80: 	//! Get the underlying buffer pool responsible for managing the buffers
81: 	virtual BufferPool &GetBufferPool() const;
82: 
83: 	virtual DatabaseInstance &GetDatabase() = 0;
84: 	// Static methods
85: 	DUCKDB_API static BufferManager &GetBufferManager(DatabaseInstance &db);
86: 	DUCKDB_API static const BufferManager &GetBufferManager(const DatabaseInstance &db);
87: 	DUCKDB_API static BufferManager &GetBufferManager(ClientContext &context);
88: 	DUCKDB_API static const BufferManager &GetBufferManager(const ClientContext &context);
89: 	DUCKDB_API static BufferManager &GetBufferManager(AttachedDatabase &db);
90: 
91: 	static idx_t GetAllocSize(const idx_t block_size) {
92: 		return AlignValue<idx_t, Storage::SECTOR_SIZE>(block_size + Storage::DEFAULT_BLOCK_HEADER_SIZE);
93: 	}
94: 	//! Returns the maximum available memory for a given query
95: 	idx_t GetQueryMaxMemory() const;
96: 
97: 	//! Get the manager that assigns reservations for temporary memory, e.g., for query intermediates
98: 	virtual TemporaryMemoryManager &GetTemporaryMemoryManager();
99: 
100: protected:
101: 	virtual void PurgeQueue(FileBufferType type) = 0;
102: 	virtual void AddToEvictionQueue(shared_ptr<BlockHandle> &handle);
103: 	virtual void WriteTemporaryBuffer(MemoryTag tag, block_id_t block_id, FileBuffer &buffer);
104: 	virtual unique_ptr<FileBuffer> ReadTemporaryBuffer(MemoryTag tag, BlockHandle &block,
105: 	                                                   unique_ptr<FileBuffer> buffer);
106: 	virtual void DeleteTemporaryFile(BlockHandle &block);
107: };
108: 
109: } // namespace duckdb
[end of src/include/duckdb/storage/buffer_manager.hpp]
[start of src/include/duckdb/storage/standard_buffer_manager.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/standard_buffer_manager.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/allocator.hpp"
12: #include "duckdb/common/atomic.hpp"
13: #include "duckdb/common/file_system.hpp"
14: #include "duckdb/common/mutex.hpp"
15: #include "duckdb/storage/block_manager.hpp"
16: #include "duckdb/storage/buffer/block_handle.hpp"
17: #include "duckdb/storage/buffer/buffer_pool.hpp"
18: #include "duckdb/storage/buffer_manager.hpp"
19: 
20: namespace duckdb {
21: 
22: class BlockManager;
23: class TemporaryMemoryManager;
24: class DatabaseInstance;
25: class TemporaryDirectoryHandle;
26: struct EvictionQueue;
27: 
28: //! The BufferManager is in charge of handling memory management for a single database. It cooperatively shares a
29: //! BufferPool with other BufferManagers, belonging to different databases. It hands out memory buffers that can
30: //! be used by the database internally, and offers configuration options specific to a database, which need not be
31: //! shared by the BufferPool, including whether to support swapping temp buffers to disk, and where to swap them to.
32: class StandardBufferManager : public BufferManager {
33: 	friend class BufferHandle;
34: 	friend class BlockHandle;
35: 	friend class BlockManager;
36: 
37: public:
38: 	StandardBufferManager(DatabaseInstance &db, string temp_directory);
39: 	~StandardBufferManager() override;
40: 
41: public:
42: 	static unique_ptr<StandardBufferManager> CreateBufferManager(DatabaseInstance &db, string temp_directory);
43: 	static unique_ptr<FileBuffer> ReadTemporaryBufferInternal(BufferManager &buffer_manager, FileHandle &handle,
44: 	                                                          idx_t position, idx_t size,
45: 	                                                          unique_ptr<FileBuffer> reusable_buffer);
46: 
47: 	//! Registers a transient memory buffer.
48: 	shared_ptr<BlockHandle> RegisterTransientMemory(const idx_t size, const idx_t block_size) final;
49: 	//! Registers an in-memory buffer that cannot be unloaded until it is destroyed.
50: 	//! This buffer can be small (smaller than the block size of the temporary block manager).
51: 	//! Unpin and Pin are NOPs on this block of memory.
52: 	shared_ptr<BlockHandle> RegisterSmallMemory(const idx_t size) final;
53: 
54: 	idx_t GetUsedMemory() const final;
55: 	idx_t GetMaxMemory() const final;
56: 	idx_t GetUsedSwap() final;
57: 	optional_idx GetMaxSwap() const final;
58: 	//! Returns the block allocation size for buffer-managed blocks.
59: 	idx_t GetBlockAllocSize() const final;
60: 	//! Returns the block size for buffer-managed blocks.
61: 	idx_t GetBlockSize() const final;
62: 
63: 	//! Allocate an in-memory buffer with a single pin.
64: 	//! The allocated memory is released when the buffer handle is destroyed.
65: 	DUCKDB_API BufferHandle Allocate(MemoryTag tag, idx_t block_size, bool can_destroy = true) final;
66: 
67: 	//! Reallocate an in-memory buffer that is pinned.
68: 	void ReAllocate(shared_ptr<BlockHandle> &handle, idx_t block_size) final;
69: 
70: 	BufferHandle Pin(shared_ptr<BlockHandle> &handle) final;
71: 	void Prefetch(vector<shared_ptr<BlockHandle>> &handles) final;
72: 	void Unpin(shared_ptr<BlockHandle> &handle) final;
73: 
74: 	//! Set a new memory limit to the buffer manager, throws an exception if the new limit is too low and not enough
75: 	//! blocks can be evicted
76: 	void SetMemoryLimit(idx_t limit = (idx_t)-1) final;
77: 	void SetSwapLimit(optional_idx limit = optional_idx()) final;
78: 
79: 	//! Returns informaton about memory usage
80: 	vector<MemoryInformation> GetMemoryUsageInfo() const override;
81: 
82: 	//! Returns a list of all temporary files
83: 	vector<TemporaryFileInformation> GetTemporaryFiles() final;
84: 
85: 	const string &GetTemporaryDirectory() const final {
86: 		return temporary_directory.path;
87: 	}
88: 
89: 	void SetTemporaryDirectory(const string &new_dir) final;
90: 
91: 	DUCKDB_API Allocator &GetBufferAllocator() final;
92: 
93: 	DatabaseInstance &GetDatabase() override {
94: 		return db;
95: 	}
96: 
97: 	//! Construct a managed buffer.
98: 	unique_ptr<FileBuffer> ConstructManagedBuffer(idx_t size, unique_ptr<FileBuffer> &&source,
99: 	                                              FileBufferType type = FileBufferType::MANAGED_BUFFER) override;
100: 
101: 	DUCKDB_API void ReserveMemory(idx_t size) final;
102: 	DUCKDB_API void FreeReservedMemory(idx_t size) final;
103: 	bool HasTemporaryDirectory() const final;
104: 
105: protected:
106: 	//! Helper
107: 	template <typename... ARGS>
108: 	TempBufferPoolReservation EvictBlocksOrThrow(MemoryTag tag, idx_t memory_delta, unique_ptr<FileBuffer> *buffer,
109: 	                                             ARGS...);
110: 
111: 	//! Register an in-memory buffer of arbitrary size, as long as it is >= BLOCK_SIZE. can_destroy signifies whether or
112: 	//! not the buffer can be destroyed instead of evicted,
113: 	//! if true, it will be destroyed,
114: 	//! if false, it will be written to a temporary file so it can be reloaded
115: 	//! If we want to change this, e.g., to immediately destroy the buffer upon unpinning,
116: 	//! we can call BlockHandle::SetDestroyBufferUpon
117: 	//! The resulting buffer will already be allocated, but needs to be pinned in order to be used.
118: 	//! This needs to be private to prevent creating blocks without ever pinning them:
119: 	//! blocks that are never pinned are never added to the eviction queue
120: 	shared_ptr<BlockHandle> RegisterMemory(MemoryTag tag, idx_t block_size, bool can_destroy);
121: 
122: 	//! Garbage collect eviction queue
123: 	void PurgeQueue(FileBufferType type) final;
124: 
125: 	BufferPool &GetBufferPool() const final;
126: 	TemporaryMemoryManager &GetTemporaryMemoryManager() final;
127: 
128: 	//! Write a temporary buffer to disk
129: 	void WriteTemporaryBuffer(MemoryTag tag, block_id_t block_id, FileBuffer &buffer) final;
130: 	//! Read a temporary buffer from disk
131: 	unique_ptr<FileBuffer> ReadTemporaryBuffer(MemoryTag tag, BlockHandle &block,
132: 	                                           unique_ptr<FileBuffer> buffer = nullptr) final;
133: 	//! Get the path of the temporary buffer
134: 	string GetTemporaryPath(block_id_t id);
135: 
136: 	void DeleteTemporaryFile(BlockHandle &block) final;
137: 
138: 	void RequireTemporaryDirectory();
139: 
140: 	void AddToEvictionQueue(shared_ptr<BlockHandle> &handle) final;
141: 
142: 	const char *InMemoryWarning();
143: 
144: 	static data_ptr_t BufferAllocatorAllocate(PrivateAllocatorData *private_data, idx_t size);
145: 	static void BufferAllocatorFree(PrivateAllocatorData *private_data, data_ptr_t pointer, idx_t size);
146: 	static data_ptr_t BufferAllocatorRealloc(PrivateAllocatorData *private_data, data_ptr_t pointer, idx_t old_size,
147: 	                                         idx_t size);
148: 
149: 	//! When the BlockHandle reaches 0 readers, this creates a new FileBuffer for this BlockHandle and
150: 	//! overwrites the data within with garbage. Any readers that do not hold the pin will notice
151: 	void VerifyZeroReaders(shared_ptr<BlockHandle> &handle);
152: 
153: 	void BatchRead(vector<shared_ptr<BlockHandle>> &handles, const map<block_id_t, idx_t> &load_map,
154: 	               block_id_t first_block, block_id_t last_block);
155: 
156: protected:
157: 	// These are stored here because temp_directory creation is lazy
158: 	// so we need to store information related to the temporary directory before it's created
159: 	struct TemporaryFileData {
160: 		//! The directory name where temporary files are stored
161: 		string path;
162: 		//! Lock for creating the temp handle (marked mutable so 'GetMaxSwap' can be const)
163: 		mutable mutex lock;
164: 		//! Handle for the temporary directory
165: 		unique_ptr<TemporaryDirectoryHandle> handle;
166: 		//! The maximum swap space that can be used
167: 		optional_idx maximum_swap_space = optional_idx();
168: 	};
169: 
170: protected:
171: 	//! The database instance
172: 	DatabaseInstance &db;
173: 	//! The buffer pool
174: 	BufferPool &buffer_pool;
175: 	//! The variables related to temporary file management
176: 	TemporaryFileData temporary_directory;
177: 	//! The temporary id used for managed buffers
178: 	atomic<block_id_t> temporary_id;
179: 	//! Allocator associated with the buffer manager, that passes all allocations through this buffer manager
180: 	Allocator buffer_allocator;
181: 	//! Block manager for temp data
182: 	unique_ptr<BlockManager> temp_block_manager;
183: 	//! Temporary evicted memory data per tag
184: 	atomic<idx_t> evicted_data_per_tag[MEMORY_TAG_COUNT];
185: };
186: 
187: } // namespace duckdb
[end of src/include/duckdb/storage/standard_buffer_manager.hpp]
[start of src/storage/buffer/block_handle.cpp]
1: #include "duckdb/storage/buffer/block_handle.hpp"
2: 
3: #include "duckdb/common/file_buffer.hpp"
4: #include "duckdb/storage/block.hpp"
5: #include "duckdb/storage/block_manager.hpp"
6: #include "duckdb/storage/buffer/buffer_handle.hpp"
7: #include "duckdb/storage/buffer/buffer_pool.hpp"
8: #include "duckdb/storage/buffer_manager.hpp"
9: 
10: namespace duckdb {
11: 
12: BlockHandle::BlockHandle(BlockManager &block_manager, block_id_t block_id_p, MemoryTag tag)
13:     : block_manager(block_manager), readers(0), block_id(block_id_p), tag(tag), buffer(nullptr), eviction_seq_num(0),
14:       destroy_buffer_upon(DestroyBufferUpon::BLOCK), memory_charge(tag, block_manager.buffer_manager.GetBufferPool()),
15:       unswizzled(nullptr) {
16: 	eviction_seq_num = 0;
17: 	state = BlockState::BLOCK_UNLOADED;
18: 	memory_usage = block_manager.GetBlockAllocSize();
19: }
20: 
21: BlockHandle::BlockHandle(BlockManager &block_manager, block_id_t block_id_p, MemoryTag tag,
22:                          unique_ptr<FileBuffer> buffer_p, DestroyBufferUpon destroy_buffer_upon_p, idx_t block_size,
23:                          BufferPoolReservation &&reservation)
24:     : block_manager(block_manager), readers(0), block_id(block_id_p), tag(tag), eviction_seq_num(0),
25:       destroy_buffer_upon(destroy_buffer_upon_p), memory_charge(tag, block_manager.buffer_manager.GetBufferPool()),
26:       unswizzled(nullptr) {
27: 	buffer = std::move(buffer_p);
28: 	state = BlockState::BLOCK_LOADED;
29: 	memory_usage = block_size;
30: 	memory_charge = std::move(reservation);
31: }
32: 
33: BlockHandle::~BlockHandle() { // NOLINT: allow internal exceptions
34: 	// being destroyed, so any unswizzled pointers are just binary junk now.
35: 	unswizzled = nullptr;
36: 	if (buffer && buffer->type != FileBufferType::TINY_BUFFER) {
37: 		// we kill the latest version in the eviction queue
38: 		auto &buffer_manager = block_manager.buffer_manager;
39: 		buffer_manager.GetBufferPool().IncrementDeadNodes(buffer->type);
40: 	}
41: 
42: 	// no references remain to this block: erase
43: 	if (buffer && state == BlockState::BLOCK_LOADED) {
44: 		D_ASSERT(memory_charge.size > 0);
45: 		// the block is still loaded in memory: erase it
46: 		buffer.reset();
47: 		memory_charge.Resize(0);
48: 	} else {
49: 		D_ASSERT(memory_charge.size == 0);
50: 	}
51: 
52: 	block_manager.UnregisterBlock(*this);
53: }
54: 
55: unique_ptr<Block> AllocateBlock(BlockManager &block_manager, unique_ptr<FileBuffer> reusable_buffer,
56:                                 block_id_t block_id) {
57: 	if (reusable_buffer) {
58: 		// re-usable buffer: re-use it
59: 		if (reusable_buffer->type == FileBufferType::BLOCK) {
60: 			// we can reuse the buffer entirely
61: 			auto &block = reinterpret_cast<Block &>(*reusable_buffer);
62: 			block.id = block_id;
63: 			return unique_ptr_cast<FileBuffer, Block>(std::move(reusable_buffer));
64: 		}
65: 		auto block = block_manager.CreateBlock(block_id, reusable_buffer.get());
66: 		reusable_buffer.reset();
67: 		return block;
68: 	} else {
69: 		// no re-usable buffer: allocate a new block
70: 		return block_manager.CreateBlock(block_id, nullptr);
71: 	}
72: }
73: 
74: BufferHandle BlockHandle::LoadFromBuffer(data_ptr_t data, unique_ptr<FileBuffer> reusable_buffer) {
75: 	D_ASSERT(state != BlockState::BLOCK_LOADED);
76: 	// copy over the data into the block from the file buffer
77: 	auto block = AllocateBlock(block_manager, std::move(reusable_buffer), block_id);
78: 	memcpy(block->InternalBuffer(), data, block->AllocSize());
79: 	buffer = std::move(block);
80: 	state = BlockState::BLOCK_LOADED;
81: 	return BufferHandle(shared_from_this());
82: }
83: 
84: BufferHandle BlockHandle::Load(unique_ptr<FileBuffer> reusable_buffer) {
85: 	if (state == BlockState::BLOCK_LOADED) {
86: 		// already loaded
87: 		D_ASSERT(buffer);
88: 		return BufferHandle(shared_from_this());
89: 	}
90: 
91: 	if (block_id < MAXIMUM_BLOCK) {
92: 		auto block = AllocateBlock(block_manager, std::move(reusable_buffer), block_id);
93: 		block_manager.Read(*block);
94: 		buffer = std::move(block);
95: 	} else {
96: 		if (MustWriteToTemporaryFile()) {
97: 			buffer = block_manager.buffer_manager.ReadTemporaryBuffer(tag, *this, std::move(reusable_buffer));
98: 		} else {
99: 			return BufferHandle(); // Destroyed upon unpin/evict, so there is no temp buffer to read
100: 		}
101: 	}
102: 	state = BlockState::BLOCK_LOADED;
103: 	return BufferHandle(shared_from_this());
104: }
105: 
106: unique_ptr<FileBuffer> BlockHandle::UnloadAndTakeBlock() {
107: 	if (state == BlockState::BLOCK_UNLOADED) {
108: 		// already unloaded: nothing to do
109: 		return nullptr;
110: 	}
111: 	D_ASSERT(!unswizzled);
112: 	D_ASSERT(CanUnload());
113: 
114: 	if (block_id >= MAXIMUM_BLOCK && MustWriteToTemporaryFile()) {
115: 		// temporary block that cannot be destroyed upon evict/unpin: write to temporary file
116: 		block_manager.buffer_manager.WriteTemporaryBuffer(tag, block_id, *buffer);
117: 	}
118: 	memory_charge.Resize(0);
119: 	state = BlockState::BLOCK_UNLOADED;
120: 	return std::move(buffer);
121: }
122: 
123: void BlockHandle::Unload() {
124: 	auto block = UnloadAndTakeBlock();
125: 	block.reset();
126: }
127: 
128: bool BlockHandle::CanUnload() {
129: 	if (state == BlockState::BLOCK_UNLOADED) {
130: 		// already unloaded
131: 		return false;
132: 	}
133: 	if (readers > 0) {
134: 		// there are active readers
135: 		return false;
136: 	}
137: 	if (block_id >= MAXIMUM_BLOCK && MustWriteToTemporaryFile() &&
138: 	    !block_manager.buffer_manager.HasTemporaryDirectory()) {
139: 		// this block cannot be destroyed upon evict/unpin
140: 		// in order to unload this block we need to write it to a temporary buffer
141: 		// however, no temporary directory is specified!
142: 		// hence we cannot unload the block
143: 		return false;
144: 	}
145: 	return true;
146: }
147: 
148: } // namespace duckdb
[end of src/storage/buffer/block_handle.cpp]
[start of src/storage/buffer/block_manager.cpp]
1: #include "duckdb/storage/block_manager.hpp"
2: #include "duckdb/storage/buffer_manager.hpp"
3: #include "duckdb/storage/buffer/block_handle.hpp"
4: #include "duckdb/storage/buffer/buffer_pool.hpp"
5: #include "duckdb/storage/metadata/metadata_manager.hpp"
6: 
7: namespace duckdb {
8: 
9: BlockManager::BlockManager(BufferManager &buffer_manager, const optional_idx block_alloc_size_p)
10:     : buffer_manager(buffer_manager), metadata_manager(make_uniq<MetadataManager>(*this, buffer_manager)),
11:       block_alloc_size(block_alloc_size_p) {
12: }
13: 
14: shared_ptr<BlockHandle> BlockManager::RegisterBlock(block_id_t block_id) {
15: 	lock_guard<mutex> lock(blocks_lock);
16: 	// check if the block already exists
17: 	auto entry = blocks.find(block_id);
18: 	if (entry != blocks.end()) {
19: 		// already exists: check if it hasn't expired yet
20: 		auto existing_ptr = entry->second.lock();
21: 		if (existing_ptr) {
22: 			//! it hasn't! return it
23: 			return existing_ptr;
24: 		}
25: 	}
26: 	// create a new block pointer for this block
27: 	auto result = make_shared_ptr<BlockHandle>(*this, block_id, MemoryTag::BASE_TABLE);
28: 	// register the block pointer in the set of blocks as a weak pointer
29: 	blocks[block_id] = weak_ptr<BlockHandle>(result);
30: 	return result;
31: }
32: 
33: shared_ptr<BlockHandle> BlockManager::ConvertToPersistent(block_id_t block_id, shared_ptr<BlockHandle> old_block) {
34: 	// pin the old block to ensure we have it loaded in memory
35: 	auto old_handle = buffer_manager.Pin(old_block);
36: 	D_ASSERT(old_block->state == BlockState::BLOCK_LOADED);
37: 	D_ASSERT(old_block->buffer);
38: 
39: 	// Temp buffers can be larger than the storage block size.
40: 	// But persistent buffers cannot.
41: 	D_ASSERT(old_block->buffer->AllocSize() <= GetBlockAllocSize());
42: 
43: 	// register a block with the new block id
44: 	auto new_block = RegisterBlock(block_id);
45: 	D_ASSERT(new_block->state == BlockState::BLOCK_UNLOADED);
46: 	D_ASSERT(new_block->readers == 0);
47: 
48: 	// move the data from the old block into data for the new block
49: 	new_block->state = BlockState::BLOCK_LOADED;
50: 	new_block->buffer = ConvertBlock(block_id, *old_block->buffer);
51: 	new_block->memory_usage = old_block->memory_usage;
52: 	new_block->memory_charge = std::move(old_block->memory_charge);
53: 
54: 	// clear the old buffer and unload it
55: 	old_block->buffer.reset();
56: 	old_block->state = BlockState::BLOCK_UNLOADED;
57: 	old_block->memory_usage = 0;
58: 	old_handle.Destroy();
59: 	old_block.reset();
60: 
61: 	// persist the new block to disk
62: 	Write(*new_block->buffer, block_id);
63: 
64: 	// potentially purge the queue
65: 	auto purge_queue = buffer_manager.GetBufferPool().AddToEvictionQueue(new_block);
66: 	if (purge_queue) {
67: 		buffer_manager.GetBufferPool().PurgeQueue(new_block->buffer->type);
68: 	}
69: 
70: 	return new_block;
71: }
72: 
73: void BlockManager::UnregisterBlock(block_id_t id) {
74: 	D_ASSERT(id < MAXIMUM_BLOCK);
75: 	lock_guard<mutex> lock(blocks_lock);
76: 	// on-disk block: erase from list of blocks in manager
77: 	blocks.erase(id);
78: }
79: 
80: void BlockManager::UnregisterBlock(BlockHandle &block) {
81: 	auto id = block.BlockId();
82: 	if (id >= MAXIMUM_BLOCK) {
83: 		// in-memory buffer: buffer could have been offloaded to disk: remove the file
84: 		buffer_manager.DeleteTemporaryFile(block);
85: 	} else {
86: 		lock_guard<mutex> lock(blocks_lock);
87: 		// on-disk block: erase from list of blocks in manager
88: 		blocks.erase(id);
89: 	}
90: }
91: 
92: MetadataManager &BlockManager::GetMetadataManager() {
93: 	return *metadata_manager;
94: }
95: 
96: void BlockManager::Truncate() {
97: }
98: 
99: } // namespace duckdb
[end of src/storage/buffer/block_manager.cpp]
[start of src/storage/buffer/buffer_pool.cpp]
1: #include "duckdb/storage/buffer/buffer_pool.hpp"
2: 
3: #include "duckdb/common/allocator.hpp"
4: #include "duckdb/common/chrono.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/typedefs.hpp"
7: #include "duckdb/parallel/concurrentqueue.hpp"
8: #include "duckdb/parallel/task_scheduler.hpp"
9: #include "duckdb/storage/temporary_memory_manager.hpp"
10: 
11: namespace duckdb {
12: 
13: BufferEvictionNode::BufferEvictionNode(weak_ptr<BlockHandle> handle_p, idx_t eviction_seq_num)
14:     : handle(std::move(handle_p)), handle_sequence_number(eviction_seq_num) {
15: 	D_ASSERT(!handle.expired());
16: }
17: 
18: bool BufferEvictionNode::CanUnload(BlockHandle &handle_p) {
19: 	if (handle_sequence_number != handle_p.eviction_seq_num) {
20: 		// handle was used in between
21: 		return false;
22: 	}
23: 	return handle_p.CanUnload();
24: }
25: 
26: shared_ptr<BlockHandle> BufferEvictionNode::TryGetBlockHandle() {
27: 	auto handle_p = handle.lock();
28: 	if (!handle_p) {
29: 		// BlockHandle has been destroyed
30: 		return nullptr;
31: 	}
32: 	if (!CanUnload(*handle_p)) {
33: 		// handle was used in between
34: 		return nullptr;
35: 	}
36: 	// this is the latest node in the queue with this handle
37: 	return handle_p;
38: }
39: 
40: typedef duckdb_moodycamel::ConcurrentQueue<BufferEvictionNode> eviction_queue_t;
41: 
42: struct EvictionQueue {
43: public:
44: 	EvictionQueue() : evict_queue_insertions(0), total_dead_nodes(0) {
45: 	}
46: 
47: public:
48: 	//! Add a buffer handle to the eviction queue. Returns true, if the queue is
49: 	//! ready to be purged, and false otherwise.
50: 	bool AddToEvictionQueue(BufferEvictionNode &&node);
51: 	//! Tries to dequeue an element from the eviction queue, but only after acquiring the purge queue lock.
52: 	bool TryDequeueWithLock(BufferEvictionNode &node);
53: 	//! Garbage collect dead nodes in the eviction queue.
54: 	void Purge();
55: 	template <typename FN>
56: 	void IterateUnloadableBlocks(FN fn);
57: 
58: 	//! Increment the dead node counter in the purge queue.
59: 	inline void IncrementDeadNodes() {
60: 		total_dead_nodes++;
61: 	}
62: 	//! Decrement the dead node counter in the purge queue.
63: 	inline void DecrementDeadNodes() {
64: 		total_dead_nodes--;
65: 	}
66: 
67: private:
68: 	//! Bulk purge dead nodes from the eviction queue. Then, enqueue those that are still alive.
69: 	void PurgeIteration(const idx_t purge_size);
70: 
71: public:
72: 	//! The concurrent queue
73: 	eviction_queue_t q;
74: 
75: private:
76: 	//! We trigger a purge of the eviction queue every INSERT_INTERVAL insertions
77: 	constexpr static idx_t INSERT_INTERVAL = 4096;
78: 	//! We multiply the base purge size by this value.
79: 	constexpr static idx_t PURGE_SIZE_MULTIPLIER = 2;
80: 	//! We multiply the purge size by this value to determine early-outs. This is the minimum queue size.
81: 	//! We never purge below this point.
82: 	constexpr static idx_t EARLY_OUT_MULTIPLIER = 4;
83: 	//! We multiply the approximate alive nodes by this value to test whether our total dead nodes
84: 	//! exceed their allowed ratio. Must be greater than 1.
85: 	constexpr static idx_t ALIVE_NODE_MULTIPLIER = 4;
86: 
87: private:
88: 	//! Total number of insertions into the eviction queue. This guides the schedule for calling PurgeQueue.
89: 	atomic<idx_t> evict_queue_insertions;
90: 	//! Total dead nodes in the eviction queue. There are two scenarios in which a node dies: (1) we destroy its block
91: 	//! handle, or (2) we insert a newer version into the eviction queue.
92: 	atomic<idx_t> total_dead_nodes;
93: 
94: 	//! Locked, if a queue purge is currently active or we're trying to forcefully evict a node.
95: 	//! Only lets a single thread enter the purge phase.
96: 	mutex purge_lock;
97: 	//! A pre-allocated vector of eviction nodes. We reuse this to keep the allocation overhead of purges small.
98: 	vector<BufferEvictionNode> purge_nodes;
99: };
100: 
101: bool EvictionQueue::AddToEvictionQueue(BufferEvictionNode &&node) {
102: 	q.enqueue(std::move(node));
103: 	return ++evict_queue_insertions % INSERT_INTERVAL == 0;
104: }
105: 
106: bool EvictionQueue::TryDequeueWithLock(BufferEvictionNode &node) {
107: 	lock_guard<mutex> lock(purge_lock);
108: 	return q.try_dequeue(node);
109: }
110: 
111: void EvictionQueue::Purge() {
112: 	// only one thread purges the queue, all other threads early-out
113: 	if (!purge_lock.try_lock()) {
114: 		return;
115: 	}
116: 	lock_guard<mutex> lock {purge_lock, std::adopt_lock};
117: 
118: 	// we purge INSERT_INTERVAL * PURGE_SIZE_MULTIPLIER nodes
119: 	idx_t purge_size = INSERT_INTERVAL * PURGE_SIZE_MULTIPLIER;
120: 
121: 	// get an estimate of the queue size as-of now
122: 	idx_t approx_q_size = q.size_approx();
123: 
124: 	// early-out, if the queue is not big enough to justify purging
125: 	// - we want to keep the LRU characteristic alive
126: 	if (approx_q_size < purge_size * EARLY_OUT_MULTIPLIER) {
127: 		return;
128: 	}
129: 
130: 	// There are two types of situations.
131: 
132: 	// For most scenarios, purging INSERT_INTERVAL * PURGE_SIZE_MULTIPLIER nodes is enough.
133: 	// Purging more nodes than we insert also counters oscillation for scenarios where most nodes are dead.
134: 	// If we always purge slightly more, we trigger a purge less often, as we purge below the trigger.
135: 
136: 	// However, if the pressure on the queue becomes too contested, we need to purge more aggressively,
137: 	// i.e., we actively seek a specific number of dead nodes to purge. We use the total number of existing dead nodes.
138: 	// We detect this situation by observing the queue's ratio between alive vs. dead nodes. If the ratio of alive vs.
139: 	// dead nodes grows faster than we can purge, we keep purging until we hit one of the following conditions.
140: 
141: 	// 2.1. We're back at an approximate queue size less than purge_size * EARLY_OUT_MULTIPLIER.
142: 	// 2.2. We're back at a ratio of 1*alive_node:ALIVE_NODE_MULTIPLIER*dead_nodes.
143: 	// 2.3. We've purged the entire queue: max_purges is zero. This is a worst-case scenario,
144: 	// guaranteeing that we always exit the loop.
145: 
146: 	idx_t max_purges = approx_q_size / purge_size;
147: 	while (max_purges != 0) {
148: 		PurgeIteration(purge_size);
149: 
150: 		// update relevant sizes and potentially early-out
151: 		approx_q_size = q.size_approx();
152: 
153: 		// early-out according to (2.1)
154: 		if (approx_q_size < purge_size * EARLY_OUT_MULTIPLIER) {
155: 			break;
156: 		}
157: 
158: 		idx_t approx_dead_nodes = total_dead_nodes;
159: 		approx_dead_nodes = approx_dead_nodes > approx_q_size ? approx_q_size : approx_dead_nodes;
160: 		idx_t approx_alive_nodes = approx_q_size - approx_dead_nodes;
161: 
162: 		// early-out according to (2.2)
163: 		if (approx_alive_nodes * (ALIVE_NODE_MULTIPLIER - 1) > approx_dead_nodes) {
164: 			break;
165: 		}
166: 
167: 		max_purges--;
168: 	}
169: }
170: 
171: void EvictionQueue::PurgeIteration(const idx_t purge_size) {
172: 	// if this purge is significantly smaller or bigger than the previous purge, then
173: 	// we need to resize the purge_nodes vector. Note that this barely happens, as we
174: 	// purge queue_insertions * PURGE_SIZE_MULTIPLIER nodes
175: 	idx_t previous_purge_size = purge_nodes.size();
176: 	if (purge_size < previous_purge_size / 2 || purge_size > previous_purge_size) {
177: 		purge_nodes.resize(purge_size);
178: 	}
179: 
180: 	// bulk purge
181: 	idx_t actually_dequeued = q.try_dequeue_bulk(purge_nodes.begin(), purge_size);
182: 
183: 	// retrieve all alive nodes that have been wrongly dequeued
184: 	idx_t alive_nodes = 0;
185: 	for (idx_t i = 0; i < actually_dequeued; i++) {
186: 		auto &node = purge_nodes[i];
187: 		auto handle = node.TryGetBlockHandle();
188: 		if (handle) {
189: 			q.enqueue(std::move(node));
190: 			alive_nodes++;
191: 		}
192: 	}
193: 
194: 	total_dead_nodes -= actually_dequeued - alive_nodes;
195: }
196: 
197: BufferPool::BufferPool(idx_t maximum_memory, bool track_eviction_timestamps,
198:                        idx_t allocator_bulk_deallocation_flush_threshold)
199:     : maximum_memory(maximum_memory),
200:       allocator_bulk_deallocation_flush_threshold(allocator_bulk_deallocation_flush_threshold),
201:       track_eviction_timestamps(track_eviction_timestamps),
202:       temporary_memory_manager(make_uniq<TemporaryMemoryManager>()) {
203: 	queues.reserve(FILE_BUFFER_TYPE_COUNT);
204: 	for (idx_t i = 0; i < FILE_BUFFER_TYPE_COUNT; i++) {
205: 		queues.push_back(make_uniq<EvictionQueue>());
206: 	}
207: }
208: BufferPool::~BufferPool() {
209: }
210: 
211: bool BufferPool::AddToEvictionQueue(shared_ptr<BlockHandle> &handle) {
212: 	auto &queue = GetEvictionQueueForType(handle->buffer->type);
213: 
214: 	// The block handle is locked during this operation (Unpin),
215: 	// or the block handle is still a local variable (ConvertToPersistent)
216: 	D_ASSERT(handle->readers == 0);
217: 	auto ts = ++handle->eviction_seq_num;
218: 	if (track_eviction_timestamps) {
219: 		handle->lru_timestamp_msec =
220: 		    std::chrono::time_point_cast<std::chrono::milliseconds>(std::chrono::steady_clock::now())
221: 		        .time_since_epoch()
222: 		        .count();
223: 	}
224: 
225: 	if (ts != 1) {
226: 		// we add a newer version, i.e., we kill exactly one previous version
227: 		queue.IncrementDeadNodes();
228: 	}
229: 
230: 	// Get the eviction queue for the buffer type and add it
231: 	return queue.AddToEvictionQueue(BufferEvictionNode(weak_ptr<BlockHandle>(handle), ts));
232: }
233: 
234: EvictionQueue &BufferPool::GetEvictionQueueForType(FileBufferType type) {
235: 	return *queues[uint8_t(type) - 1];
236: }
237: 
238: void BufferPool::IncrementDeadNodes(FileBufferType type) {
239: 	GetEvictionQueueForType(type).IncrementDeadNodes();
240: }
241: 
242: void BufferPool::UpdateUsedMemory(MemoryTag tag, int64_t size) {
243: 	memory_usage.UpdateUsedMemory(tag, size);
244: }
245: 
246: idx_t BufferPool::GetUsedMemory() const {
247: 	return memory_usage.GetUsedMemory(MemoryUsageCaches::FLUSH);
248: }
249: 
250: idx_t BufferPool::GetMaxMemory() const {
251: 	return maximum_memory;
252: }
253: 
254: idx_t BufferPool::GetQueryMaxMemory() const {
255: 	return GetMaxMemory();
256: }
257: 
258: TemporaryMemoryManager &BufferPool::GetTemporaryMemoryManager() {
259: 	return *temporary_memory_manager;
260: }
261: 
262: BufferPool::EvictionResult BufferPool::EvictBlocks(MemoryTag tag, idx_t extra_memory, idx_t memory_limit,
263:                                                    unique_ptr<FileBuffer> *buffer) {
264: 	// First, we try to evict persistent table data
265: 	auto block_result =
266: 	    EvictBlocksInternal(GetEvictionQueueForType(FileBufferType::BLOCK), tag, extra_memory, memory_limit, buffer);
267: 	if (block_result.success) {
268: 		return block_result;
269: 	}
270: 
271: 	// If that does not succeed, we try to evict temporary data
272: 	auto managed_buffer_result = EvictBlocksInternal(GetEvictionQueueForType(FileBufferType::MANAGED_BUFFER), tag,
273: 	                                                 extra_memory, memory_limit, buffer);
274: 	if (managed_buffer_result.success) {
275: 		return managed_buffer_result;
276: 	}
277: 
278: 	// Finally, we try to evict tiny buffers
279: 	return EvictBlocksInternal(GetEvictionQueueForType(FileBufferType::TINY_BUFFER), tag, extra_memory, memory_limit,
280: 	                           buffer);
281: }
282: 
283: BufferPool::EvictionResult BufferPool::EvictBlocksInternal(EvictionQueue &queue, MemoryTag tag, idx_t extra_memory,
284:                                                            idx_t memory_limit, unique_ptr<FileBuffer> *buffer) {
285: 	TempBufferPoolReservation r(tag, *this, extra_memory);
286: 	bool found = false;
287: 
288: 	if (memory_usage.GetUsedMemory(MemoryUsageCaches::NO_FLUSH) <= memory_limit) {
289: 		if (Allocator::SupportsFlush() && extra_memory > allocator_bulk_deallocation_flush_threshold) {
290: 			Allocator::FlushAll();
291: 		}
292: 		return {true, std::move(r)};
293: 	}
294: 
295: 	queue.IterateUnloadableBlocks([&](BufferEvictionNode &, const shared_ptr<BlockHandle> &handle) {
296: 		// hooray, we can unload the block
297: 		if (buffer && handle->buffer->AllocSize() == extra_memory) {
298: 			// we can re-use the memory directly
299: 			*buffer = handle->UnloadAndTakeBlock();
300: 			found = true;
301: 			return false;
302: 		}
303: 
304: 		// release the memory and mark the block as unloaded
305: 		handle->Unload();
306: 
307: 		if (memory_usage.GetUsedMemory(MemoryUsageCaches::NO_FLUSH) <= memory_limit) {
308: 			found = true;
309: 			return false;
310: 		}
311: 
312: 		// Continue iteration
313: 		return true;
314: 	});
315: 
316: 	if (!found) {
317: 		r.Resize(0);
318: 	} else if (Allocator::SupportsFlush() && extra_memory > allocator_bulk_deallocation_flush_threshold) {
319: 		Allocator::FlushAll();
320: 	}
321: 
322: 	return {found, std::move(r)};
323: }
324: 
325: idx_t BufferPool::PurgeAgedBlocks(uint32_t max_age_sec) {
326: 	int64_t now = std::chrono::time_point_cast<std::chrono::milliseconds>(std::chrono::steady_clock::now())
327: 	                  .time_since_epoch()
328: 	                  .count();
329: 	int64_t limit = now - (static_cast<int64_t>(max_age_sec) * 1000);
330: 	idx_t purged_bytes = 0;
331: 	for (auto &queue : queues) {
332: 		purged_bytes += PurgeAgedBlocksInternal(*queue, max_age_sec, now, limit);
333: 	}
334: 	return purged_bytes;
335: }
336: 
337: idx_t BufferPool::PurgeAgedBlocksInternal(EvictionQueue &queue, uint32_t max_age_sec, int64_t now, int64_t limit) {
338: 	idx_t purged_bytes = 0;
339: 	queue.IterateUnloadableBlocks([&](BufferEvictionNode &node, const shared_ptr<BlockHandle> &handle) {
340: 		// We will unload this block regardless. But stop the iteration immediately afterward if this
341: 		// block is younger than the age threshold.
342: 		bool is_fresh = handle->lru_timestamp_msec >= limit && handle->lru_timestamp_msec <= now;
343: 		purged_bytes += handle->GetMemoryUsage();
344: 		handle->Unload();
345: 		return is_fresh;
346: 	});
347: 	return purged_bytes;
348: }
349: 
350: template <typename FN>
351: void EvictionQueue::IterateUnloadableBlocks(FN fn) {
352: 	for (;;) {
353: 		// get a block to unpin from the queue
354: 		BufferEvictionNode node;
355: 		if (!q.try_dequeue(node)) {
356: 			// we could not dequeue any eviction node, so we try one more time,
357: 			// but more aggressively
358: 			if (!TryDequeueWithLock(node)) {
359: 				return;
360: 			}
361: 		}
362: 
363: 		// get a reference to the underlying block pointer
364: 		auto handle = node.TryGetBlockHandle();
365: 		if (!handle) {
366: 			DecrementDeadNodes();
367: 			continue;
368: 		}
369: 
370: 		// we might be able to free this block: grab the mutex and check if we can free it
371: 		lock_guard<mutex> lock(handle->lock);
372: 		if (!node.CanUnload(*handle)) {
373: 			// something changed in the mean-time, bail out
374: 			DecrementDeadNodes();
375: 			continue;
376: 		}
377: 
378: 		if (!fn(node, handle)) {
379: 			break;
380: 		}
381: 	}
382: }
383: 
384: void BufferPool::PurgeQueue(FileBufferType type) {
385: 	GetEvictionQueueForType(type).Purge();
386: }
387: 
388: void BufferPool::SetLimit(idx_t limit, const char *exception_postscript) {
389: 	lock_guard<mutex> l_lock(limit_lock);
390: 	// try to evict until the limit is reached
391: 	if (!EvictBlocks(MemoryTag::EXTENSION, 0, limit).success) {
392: 		throw OutOfMemoryException(
393: 		    "Failed to change memory limit to %lld: could not free up enough memory for the new limit%s", limit,
394: 		    exception_postscript);
395: 	}
396: 	idx_t old_limit = maximum_memory;
397: 	// set the global maximum memory to the new limit if successful
398: 	maximum_memory = limit;
399: 	// evict again
400: 	if (!EvictBlocks(MemoryTag::EXTENSION, 0, limit).success) {
401: 		// failed: go back to old limit
402: 		maximum_memory = old_limit;
403: 		throw OutOfMemoryException(
404: 		    "Failed to change memory limit to %lld: could not free up enough memory for the new limit%s", limit,
405: 		    exception_postscript);
406: 	}
407: 	if (Allocator::SupportsFlush()) {
408: 		Allocator::FlushAll();
409: 	}
410: }
411: 
412: void BufferPool::SetAllocatorBulkDeallocationFlushThreshold(idx_t threshold) {
413: 	allocator_bulk_deallocation_flush_threshold = threshold;
414: }
415: 
416: idx_t BufferPool::GetAllocatorBulkDeallocationFlushThreshold() {
417: 	return allocator_bulk_deallocation_flush_threshold;
418: }
419: 
420: BufferPool::MemoryUsage::MemoryUsage() {
421: 	for (auto &v : memory_usage) {
422: 		v = 0;
423: 	}
424: 	for (auto &cache : memory_usage_caches) {
425: 		for (auto &v : cache) {
426: 			v = 0;
427: 		}
428: 	}
429: }
430: 
431: void BufferPool::MemoryUsage::UpdateUsedMemory(MemoryTag tag, int64_t size) {
432: 	auto tag_idx = (idx_t)tag;
433: 	if ((idx_t)AbsValue(size) < MEMORY_USAGE_CACHE_THRESHOLD) {
434: 		// update cache and update global counter when cache exceeds threshold
435: 		// Get corresponding cache slot based on current CPU core index
436: 		// Two threads may access the same cache simultaneously,
437: 		// ensuring correctness through atomic operations
438: 		auto cache_idx = (idx_t)TaskScheduler::GetEstimatedCPUId() % MEMORY_USAGE_CACHE_COUNT;
439: 		auto &cache = memory_usage_caches[cache_idx];
440: 		auto new_tag_size = cache[tag_idx].fetch_add(size, std::memory_order_relaxed) + size;
441: 		if ((idx_t)AbsValue(new_tag_size) >= MEMORY_USAGE_CACHE_THRESHOLD) {
442: 			// cached tag memory usage exceeds threshold
443: 			auto tag_size = cache[tag_idx].exchange(0, std::memory_order_relaxed);
444: 			memory_usage[tag_idx].fetch_add(tag_size, std::memory_order_relaxed);
445: 		}
446: 		auto new_total_size = cache[TOTAL_MEMORY_USAGE_INDEX].fetch_add(size, std::memory_order_relaxed) + size;
447: 		if ((idx_t)AbsValue(new_total_size) >= MEMORY_USAGE_CACHE_THRESHOLD) {
448: 			// cached total memory usage exceeds threshold
449: 			auto total_size = cache[TOTAL_MEMORY_USAGE_INDEX].exchange(0, std::memory_order_relaxed);
450: 			memory_usage[TOTAL_MEMORY_USAGE_INDEX].fetch_add(total_size, std::memory_order_relaxed);
451: 		}
452: 	} else {
453: 		// update global counter
454: 		memory_usage[tag_idx].fetch_add(size, std::memory_order_relaxed);
455: 		memory_usage[TOTAL_MEMORY_USAGE_INDEX].fetch_add(size, std::memory_order_relaxed);
456: 	}
457: }
458: 
459: } // namespace duckdb
[end of src/storage/buffer/buffer_pool.cpp]
[start of src/storage/standard_buffer_manager.cpp]
1: #include "duckdb/storage/standard_buffer_manager.hpp"
2: 
3: #include "duckdb/common/allocator.hpp"
4: #include "duckdb/common/enums/memory_tag.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/set.hpp"
7: #include "duckdb/main/attached_database.hpp"
8: #include "duckdb/main/database.hpp"
9: #include "duckdb/storage/buffer/buffer_pool.hpp"
10: #include "duckdb/storage/in_memory_block_manager.hpp"
11: #include "duckdb/storage/storage_manager.hpp"
12: #include "duckdb/storage/temporary_file_manager.hpp"
13: #include "duckdb/storage/temporary_memory_manager.hpp"
14: 
15: namespace duckdb {
16: 
17: #ifdef DUCKDB_DEBUG_DESTROY_BLOCKS
18: static void WriteGarbageIntoBuffer(FileBuffer &buffer) {
19: 	memset(buffer.buffer, 0xa5, buffer.size); // 0xa5 is default memory in debug mode
20: }
21: #endif
22: 
23: struct BufferAllocatorData : PrivateAllocatorData {
24: 	explicit BufferAllocatorData(StandardBufferManager &manager) : manager(manager) {
25: 	}
26: 
27: 	StandardBufferManager &manager;
28: };
29: 
30: unique_ptr<FileBuffer> StandardBufferManager::ConstructManagedBuffer(idx_t size, unique_ptr<FileBuffer> &&source,
31:                                                                      FileBufferType type) {
32: 	unique_ptr<FileBuffer> result;
33: 	if (source) {
34: 		auto tmp = std::move(source);
35: 		D_ASSERT(tmp->AllocSize() == BufferManager::GetAllocSize(size));
36: 		result = make_uniq<FileBuffer>(*tmp, type);
37: 	} else {
38: 		// no re-usable buffer: allocate a new buffer
39: 		result = make_uniq<FileBuffer>(Allocator::Get(db), type, size);
40: 	}
41: 	result->Initialize(DBConfig::GetConfig(db).options.debug_initialize);
42: 	return result;
43: }
44: 
45: void StandardBufferManager::SetTemporaryDirectory(const string &new_dir) {
46: 	lock_guard<mutex> guard(temporary_directory.lock);
47: 	if (temporary_directory.handle) {
48: 		throw NotImplementedException("Cannot switch temporary directory after the current one has been used");
49: 	}
50: 	temporary_directory.path = new_dir;
51: }
52: 
53: StandardBufferManager::StandardBufferManager(DatabaseInstance &db, string tmp)
54:     : BufferManager(), db(db), buffer_pool(db.GetBufferPool()), temporary_id(MAXIMUM_BLOCK),
55:       buffer_allocator(BufferAllocatorAllocate, BufferAllocatorFree, BufferAllocatorRealloc,
56:                        make_uniq<BufferAllocatorData>(*this)) {
57: 	temp_block_manager = make_uniq<InMemoryBlockManager>(*this, DEFAULT_BLOCK_ALLOC_SIZE);
58: 	temporary_directory.path = std::move(tmp);
59: 	for (idx_t i = 0; i < MEMORY_TAG_COUNT; i++) {
60: 		evicted_data_per_tag[i] = 0;
61: 	}
62: }
63: 
64: StandardBufferManager::~StandardBufferManager() {
65: }
66: 
67: BufferPool &StandardBufferManager::GetBufferPool() const {
68: 	return buffer_pool;
69: }
70: 
71: TemporaryMemoryManager &StandardBufferManager::GetTemporaryMemoryManager() {
72: 	return buffer_pool.GetTemporaryMemoryManager();
73: }
74: 
75: idx_t StandardBufferManager::GetUsedMemory() const {
76: 	return buffer_pool.GetUsedMemory();
77: }
78: idx_t StandardBufferManager::GetMaxMemory() const {
79: 	return buffer_pool.GetMaxMemory();
80: }
81: 
82: idx_t StandardBufferManager::GetUsedSwap() {
83: 	lock_guard<mutex> guard(temporary_directory.lock);
84: 	if (!temporary_directory.handle) {
85: 		return 0;
86: 	}
87: 	return temporary_directory.handle->GetTempFile().GetTotalUsedSpaceInBytes();
88: }
89: 
90: optional_idx StandardBufferManager::GetMaxSwap() const {
91: 	lock_guard<mutex> guard(temporary_directory.lock);
92: 	if (!temporary_directory.handle) {
93: 		return optional_idx();
94: 	}
95: 	return temporary_directory.handle->GetTempFile().GetMaxSwapSpace();
96: }
97: 
98: idx_t StandardBufferManager::GetBlockAllocSize() const {
99: 	return temp_block_manager->GetBlockAllocSize();
100: }
101: 
102: idx_t StandardBufferManager::GetBlockSize() const {
103: 	return temp_block_manager->GetBlockSize();
104: }
105: 
106: template <typename... ARGS>
107: TempBufferPoolReservation StandardBufferManager::EvictBlocksOrThrow(MemoryTag tag, idx_t memory_delta,
108:                                                                     unique_ptr<FileBuffer> *buffer, ARGS... args) {
109: 	auto r = buffer_pool.EvictBlocks(tag, memory_delta, buffer_pool.maximum_memory, buffer);
110: 	if (!r.success) {
111: 		string extra_text = StringUtil::Format(" (%s/%s used)", StringUtil::BytesToHumanReadableString(GetUsedMemory()),
112: 		                                       StringUtil::BytesToHumanReadableString(GetMaxMemory()));
113: 		extra_text += InMemoryWarning();
114: 		throw OutOfMemoryException(args..., extra_text);
115: 	}
116: 	return std::move(r.reservation);
117: }
118: 
119: shared_ptr<BlockHandle> StandardBufferManager::RegisterTransientMemory(const idx_t size, const idx_t block_size) {
120: 	D_ASSERT(size <= block_size);
121: 
122: 	// This comparison is the reason behind passing block_size through transient memory creation.
123: 	// Otherwise, any non-default block size would register as small memory, causing problems when
124: 	// trying to convert that memory to consistent blocks later on.
125: 	if (size < block_size) {
126: 		return RegisterSmallMemory(size);
127: 	}
128: 
129: 	auto buffer_handle = Allocate(MemoryTag::IN_MEMORY_TABLE, size, false);
130: 	return buffer_handle.GetBlockHandle();
131: }
132: 
133: shared_ptr<BlockHandle> StandardBufferManager::RegisterSmallMemory(const idx_t size) {
134: 	D_ASSERT(size < GetBlockSize());
135: 	auto reservation = EvictBlocksOrThrow(MemoryTag::BASE_TABLE, size, nullptr, "could not allocate block of size %s%s",
136: 	                                      StringUtil::BytesToHumanReadableString(size));
137: 
138: 	auto buffer = ConstructManagedBuffer(size, nullptr, FileBufferType::TINY_BUFFER);
139: 
140: 	// Create a new block pointer for this block.
141: 	auto result =
142: 	    make_shared_ptr<BlockHandle>(*temp_block_manager, ++temporary_id, MemoryTag::BASE_TABLE, std::move(buffer),
143: 	                                 DestroyBufferUpon::BLOCK, size, std::move(reservation));
144: #ifdef DUCKDB_DEBUG_DESTROY_BLOCKS
145: 	// Initialize the memory with garbage data
146: 	WriteGarbageIntoBuffer(*result->buffer);
147: #endif
148: 	return result;
149: }
150: 
151: shared_ptr<BlockHandle> StandardBufferManager::RegisterMemory(MemoryTag tag, idx_t block_size, bool can_destroy) {
152: 	auto alloc_size = GetAllocSize(block_size);
153: 
154: 	// Evict blocks until there is enough memory to store the buffer.
155: 	unique_ptr<FileBuffer> reusable_buffer;
156: 	auto res = EvictBlocksOrThrow(tag, alloc_size, &reusable_buffer, "could not allocate block of size %s%s",
157: 	                              StringUtil::BytesToHumanReadableString(alloc_size));
158: 
159: 	// Create a new buffer and a block to hold the buffer.
160: 	auto buffer = ConstructManagedBuffer(block_size, std::move(reusable_buffer));
161: 	DestroyBufferUpon destroy_buffer_upon = can_destroy ? DestroyBufferUpon::EVICTION : DestroyBufferUpon::BLOCK;
162: 	return make_shared_ptr<BlockHandle>(*temp_block_manager, ++temporary_id, tag, std::move(buffer),
163: 	                                    destroy_buffer_upon, alloc_size, std::move(res));
164: }
165: 
166: BufferHandle StandardBufferManager::Allocate(MemoryTag tag, idx_t block_size, bool can_destroy) {
167: 	auto block = RegisterMemory(tag, block_size, can_destroy);
168: 
169: #ifdef DUCKDB_DEBUG_DESTROY_BLOCKS
170: 	// Initialize the memory with garbage data
171: 	WriteGarbageIntoBuffer(*block->buffer);
172: #endif
173: 	return Pin(block);
174: }
175: 
176: void StandardBufferManager::ReAllocate(shared_ptr<BlockHandle> &handle, idx_t block_size) {
177: 	D_ASSERT(block_size >= GetBlockSize());
178: 	unique_lock<mutex> lock(handle->lock);
179: 	D_ASSERT(handle->state == BlockState::BLOCK_LOADED);
180: 	D_ASSERT(handle->memory_usage == handle->buffer->AllocSize());
181: 	D_ASSERT(handle->memory_usage == handle->memory_charge.size);
182: 
183: 	auto req = handle->buffer->CalculateMemory(block_size);
184: 	int64_t memory_delta = NumericCast<int64_t>(req.alloc_size) - NumericCast<int64_t>(handle->memory_usage);
185: 
186: 	if (memory_delta == 0) {
187: 		return;
188: 	} else if (memory_delta > 0) {
189: 		// evict blocks until we have space to resize this block
190: 		// unlock the handle lock during the call to EvictBlocksOrThrow
191: 		lock.unlock();
192: 		auto reservation = EvictBlocksOrThrow(handle->tag, NumericCast<idx_t>(memory_delta), nullptr,
193: 		                                      "failed to resize block from %s to %s%s",
194: 		                                      StringUtil::BytesToHumanReadableString(handle->memory_usage),
195: 		                                      StringUtil::BytesToHumanReadableString(req.alloc_size));
196: 		lock.lock();
197: 
198: 		// EvictBlocks decrements 'current_memory' for us.
199: 		handle->memory_charge.Merge(std::move(reservation));
200: 	} else {
201: 		// no need to evict blocks, but we do need to decrement 'current_memory'.
202: 		handle->memory_charge.Resize(req.alloc_size);
203: 	}
204: 
205: 	handle->ResizeBuffer(block_size, memory_delta);
206: }
207: 
208: void StandardBufferManager::BatchRead(vector<shared_ptr<BlockHandle>> &handles, const map<block_id_t, idx_t> &load_map,
209:                                       block_id_t first_block, block_id_t last_block) {
210: 	auto &block_manager = handles[0]->block_manager;
211: 	idx_t block_count = NumericCast<idx_t>(last_block - first_block + 1);
212: #ifndef DUCKDB_ALTERNATIVE_VERIFY
213: 	if (block_count == 1) {
214: 		// prefetching with block_count == 1 has no performance impact since we can't batch reads
215: 		// skip the prefetch in this case
216: 		// we do it anyway if alternative_verify is on for extra testing
217: 		return;
218: 	}
219: #endif
220: 
221: 	// allocate a buffer to hold the data of all of the blocks
222: 	auto intermediate_buffer = Allocate(MemoryTag::BASE_TABLE, block_count * block_manager.GetBlockSize());
223: 	// perform a batch read of the blocks into the buffer
224: 	block_manager.ReadBlocks(intermediate_buffer.GetFileBuffer(), first_block, block_count);
225: 
226: 	// the blocks are read - now we need to assign them to the individual blocks
227: 	for (idx_t block_idx = 0; block_idx < block_count; block_idx++) {
228: 		block_id_t block_id = first_block + NumericCast<block_id_t>(block_idx);
229: 		auto entry = load_map.find(block_id);
230: 		D_ASSERT(entry != load_map.end()); // if we allow gaps we might not return true here
231: 		auto &handle = handles[entry->second];
232: 
233: 		// reserve memory for the block
234: 		idx_t required_memory = handle->memory_usage;
235: 		unique_ptr<FileBuffer> reusable_buffer;
236: 		auto reservation =
237: 		    EvictBlocksOrThrow(handle->tag, required_memory, &reusable_buffer, "failed to pin block of size %s%s",
238: 		                       StringUtil::BytesToHumanReadableString(required_memory));
239: 		// now load the block from the buffer
240: 		// note that we discard the buffer handle - we do not keep it around
241: 		// the prefetching relies on the block handle being pinned again during the actual read before it is evicted
242: 		BufferHandle buf;
243: 		{
244: 			lock_guard<mutex> lock(handle->lock);
245: 			if (handle->state == BlockState::BLOCK_LOADED) {
246: 				// the block is loaded already by another thread - free up the reservation and continue
247: 				reservation.Resize(0);
248: 				continue;
249: 			}
250: 			auto block_ptr =
251: 			    intermediate_buffer.GetFileBuffer().InternalBuffer() + block_idx * block_manager.GetBlockAllocSize();
252: 			buf = handle->LoadFromBuffer(block_ptr, std::move(reusable_buffer));
253: 			handle->readers = 1;
254: 			handle->memory_charge = std::move(reservation);
255: 		}
256: 	}
257: }
258: 
259: void StandardBufferManager::Prefetch(vector<shared_ptr<BlockHandle>> &handles) {
260: 	// figure out which set of blocks we should load
261: 	map<block_id_t, idx_t> to_be_loaded;
262: 	for (idx_t block_idx = 0; block_idx < handles.size(); block_idx++) {
263: 		auto &handle = handles[block_idx];
264: 		lock_guard<mutex> lock(handle->lock);
265: 		if (handle->state != BlockState::BLOCK_LOADED) {
266: 			// need to load this block - add it to the map
267: 			to_be_loaded.insert(make_pair(handle->BlockId(), block_idx));
268: 		}
269: 	}
270: 	if (to_be_loaded.empty()) {
271: 		// nothing to fetch
272: 		return;
273: 	}
274: 	// iterate over the blocks and perform bulk reads
275: 	block_id_t first_block = -1;
276: 	block_id_t previous_block_id = -1;
277: 	for (auto &entry : to_be_loaded) {
278: 		if (previous_block_id < 0) {
279: 			// this the first block we are seeing
280: 			first_block = entry.first;
281: 			previous_block_id = first_block;
282: 		} else if (previous_block_id + 1 == entry.first) {
283: 			// this block is adjacent to the previous block - add it to the batch read
284: 			previous_block_id = entry.first;
285: 		} else {
286: 			// this block is not adjacent to the previous block
287: 			// perform the batch read for the previous batch
288: 			BatchRead(handles, to_be_loaded, first_block, previous_block_id);
289: 
290: 			// set the first_block and previous_block_id to the current block
291: 			first_block = entry.first;
292: 			previous_block_id = entry.first;
293: 		}
294: 	}
295: 	// batch read the final batch
296: 	BatchRead(handles, to_be_loaded, first_block, previous_block_id);
297: }
298: 
299: BufferHandle StandardBufferManager::Pin(shared_ptr<BlockHandle> &handle) {
300: 	// we need to be careful not to return the BufferHandle to this block while holding the BlockHandle's lock
301: 	// as exiting this function's scope may cause the destructor of the BufferHandle to be called while holding the lock
302: 	// the destructor calls Unpin, which grabs the BlockHandle's lock again, causing a deadlock
303: 	BufferHandle buf;
304: 
305: 	idx_t required_memory;
306: 	{
307: 		// lock the block
308: 		lock_guard<mutex> lock(handle->lock);
309: 		// check if the block is already loaded
310: 		if (handle->state == BlockState::BLOCK_LOADED) {
311: 			// the block is loaded, increment the reader count and set the BufferHandle
312: 			handle->readers++;
313: 			buf = handle->Load();
314: 		}
315: 		required_memory = handle->memory_usage;
316: 	}
317: 
318: 	if (buf.IsValid()) {
319: 		return buf; // the block was already loaded, return it without holding the BlockHandle's lock
320: 	} else {
321: 		// evict blocks until we have space for the current block
322: 		unique_ptr<FileBuffer> reusable_buffer;
323: 		auto reservation =
324: 		    EvictBlocksOrThrow(handle->tag, required_memory, &reusable_buffer, "failed to pin block of size %s%s",
325: 		                       StringUtil::BytesToHumanReadableString(required_memory));
326: 
327: 		// lock the handle again and repeat the check (in case anybody loaded in the meantime)
328: 		lock_guard<mutex> lock(handle->lock);
329: 		// check if the block is already loaded
330: 		if (handle->state == BlockState::BLOCK_LOADED) {
331: 			// the block is loaded, increment the reader count and return a pointer to the handle
332: 			handle->readers++;
333: 			reservation.Resize(0);
334: 			buf = handle->Load();
335: 		} else {
336: 			// now we can actually load the current block
337: 			D_ASSERT(handle->readers == 0);
338: 			buf = handle->Load(std::move(reusable_buffer));
339: 			handle->readers = 1;
340: 			handle->memory_charge = std::move(reservation);
341: 			// in the case of a variable sized block, the buffer may be smaller than a full block.
342: 			int64_t delta =
343: 			    NumericCast<int64_t>(handle->buffer->AllocSize()) - NumericCast<int64_t>(handle->memory_usage);
344: 			if (delta) {
345: 				D_ASSERT(delta < 0);
346: 				handle->memory_usage += static_cast<idx_t>(delta);
347: 				handle->memory_charge.Resize(handle->memory_usage);
348: 			}
349: 			D_ASSERT(handle->memory_usage == handle->buffer->AllocSize());
350: 		}
351: 	}
352: 
353: 	// we should have a valid BufferHandle by now, either because the block was already loaded, or because we loaded it
354: 	// return it without holding the BlockHandle's lock
355: 	D_ASSERT(buf.IsValid());
356: 	return buf;
357: }
358: 
359: void StandardBufferManager::PurgeQueue(FileBufferType type) {
360: 	buffer_pool.PurgeQueue(type);
361: }
362: 
363: void StandardBufferManager::AddToEvictionQueue(shared_ptr<BlockHandle> &handle) {
364: 	buffer_pool.AddToEvictionQueue(handle);
365: }
366: 
367: void StandardBufferManager::VerifyZeroReaders(shared_ptr<BlockHandle> &handle) {
368: #ifdef DUCKDB_DEBUG_DESTROY_BLOCKS
369: 	auto replacement_buffer = make_uniq<FileBuffer>(Allocator::Get(db), handle->buffer->type,
370: 	                                                handle->memory_usage - Storage::DEFAULT_BLOCK_HEADER_SIZE);
371: 	memcpy(replacement_buffer->buffer, handle->buffer->buffer, handle->buffer->size);
372: 	WriteGarbageIntoBuffer(*handle->buffer);
373: 	handle->buffer = std::move(replacement_buffer);
374: #endif
375: }
376: 
377: void StandardBufferManager::Unpin(shared_ptr<BlockHandle> &handle) {
378: 	bool purge = false;
379: 	{
380: 		lock_guard<mutex> lock(handle->lock);
381: 		if (!handle->buffer || handle->buffer->type == FileBufferType::TINY_BUFFER) {
382: 			return;
383: 		}
384: 		D_ASSERT(handle->readers > 0);
385: 		handle->readers--;
386: 		if (handle->readers == 0) {
387: 			VerifyZeroReaders(handle);
388: 			if (handle->MustAddToEvictionQueue()) {
389: 				purge = buffer_pool.AddToEvictionQueue(handle);
390: 			} else {
391: 				handle->Unload();
392: 			}
393: 		}
394: 	}
395: 
396: 	// We do not have to keep the handle locked while purging.
397: 	if (purge) {
398: 		PurgeQueue(handle->buffer->type);
399: 	}
400: }
401: 
402: void StandardBufferManager::SetMemoryLimit(idx_t limit) {
403: 	buffer_pool.SetLimit(limit, InMemoryWarning());
404: }
405: 
406: void StandardBufferManager::SetSwapLimit(optional_idx limit) {
407: 	lock_guard<mutex> guard(temporary_directory.lock);
408: 	if (temporary_directory.handle) {
409: 		temporary_directory.handle->GetTempFile().SetMaxSwapSpace(limit);
410: 	} else {
411: 		temporary_directory.maximum_swap_space = limit;
412: 	}
413: }
414: 
415: vector<MemoryInformation> StandardBufferManager::GetMemoryUsageInfo() const {
416: 	vector<MemoryInformation> result;
417: 	for (idx_t k = 0; k < MEMORY_TAG_COUNT; k++) {
418: 		MemoryInformation info;
419: 		info.tag = MemoryTag(k);
420: 		info.size = buffer_pool.memory_usage.GetUsedMemory(MemoryTag(k), BufferPool::MemoryUsageCaches::FLUSH);
421: 		info.evicted_data = evicted_data_per_tag[k].load();
422: 		result.push_back(info);
423: 	}
424: 	return result;
425: }
426: 
427: unique_ptr<FileBuffer> StandardBufferManager::ReadTemporaryBufferInternal(BufferManager &buffer_manager,
428:                                                                           FileHandle &handle, idx_t position,
429:                                                                           idx_t size,
430:                                                                           unique_ptr<FileBuffer> reusable_buffer) {
431: 	auto buffer = buffer_manager.ConstructManagedBuffer(size, std::move(reusable_buffer));
432: 	buffer->Read(handle, position);
433: 	return buffer;
434: }
435: 
436: string StandardBufferManager::GetTemporaryPath(block_id_t id) {
437: 	auto &fs = FileSystem::GetFileSystem(db);
438: 	return fs.JoinPath(temporary_directory.path, "duckdb_temp_block-" + to_string(id) + ".block");
439: }
440: 
441: void StandardBufferManager::RequireTemporaryDirectory() {
442: 	if (temporary_directory.path.empty()) {
443: 		throw InvalidInputException(
444: 		    "Out-of-memory: cannot write buffer because no temporary directory is specified!\nTo enable "
445: 		    "temporary buffer eviction set a temporary directory using PRAGMA temp_directory='/path/to/tmp.tmp'");
446: 	}
447: 	lock_guard<mutex> guard(temporary_directory.lock);
448: 	if (!temporary_directory.handle) {
449: 		// temp directory has not been created yet: initialize it
450: 		temporary_directory.handle =
451: 		    make_uniq<TemporaryDirectoryHandle>(db, temporary_directory.path, temporary_directory.maximum_swap_space);
452: 	}
453: }
454: 
455: void StandardBufferManager::WriteTemporaryBuffer(MemoryTag tag, block_id_t block_id, FileBuffer &buffer) {
456: 
457: 	// WriteTemporaryBuffer assumes that we never write a buffer below DEFAULT_BLOCK_ALLOC_SIZE.
458: 	RequireTemporaryDirectory();
459: 
460: 	// Append to a few grouped files.
461: 	if (buffer.size == GetBlockSize()) {
462: 		evicted_data_per_tag[uint8_t(tag)] += GetBlockSize();
463: 		temporary_directory.handle->GetTempFile().WriteTemporaryBuffer(block_id, buffer);
464: 		return;
465: 	}
466: 
467: 	// Get the path to write to.
468: 	auto path = GetTemporaryPath(block_id);
469: 	evicted_data_per_tag[uint8_t(tag)] += buffer.size;
470: 
471: 	// Create the file and write the size followed by the buffer contents.
472: 	auto &fs = FileSystem::GetFileSystem(db);
473: 	auto handle = fs.OpenFile(path, FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE);
474: 	temporary_directory.handle->GetTempFile().IncreaseSizeOnDisk(buffer.AllocSize() + sizeof(idx_t));
475: 	handle->Write(&buffer.size, sizeof(idx_t), 0);
476: 	buffer.Write(*handle, sizeof(idx_t));
477: }
478: 
479: unique_ptr<FileBuffer> StandardBufferManager::ReadTemporaryBuffer(MemoryTag tag, BlockHandle &block,
480:                                                                   unique_ptr<FileBuffer> reusable_buffer) {
481: 	D_ASSERT(!temporary_directory.path.empty());
482: 	D_ASSERT(temporary_directory.handle.get());
483: 	auto id = block.BlockId();
484: 	if (temporary_directory.handle->GetTempFile().HasTemporaryBuffer(id)) {
485: 		// This is a block that was offloaded to a regular .tmp file, the file contains blocks of a fixed size
486: 		return temporary_directory.handle->GetTempFile().ReadTemporaryBuffer(id, std::move(reusable_buffer));
487: 	}
488: 
489: 	// This block contains data of variable size so we need to open it and read it to get its size.
490: 	idx_t block_size;
491: 	auto path = GetTemporaryPath(id);
492: 	auto &fs = FileSystem::GetFileSystem(db);
493: 	auto handle = fs.OpenFile(path, FileFlags::FILE_FLAGS_READ);
494: 	handle->Read(&block_size, sizeof(idx_t), 0);
495: 
496: 	// Allocate a buffer of the file's size and read the data into that buffer.
497: 	auto buffer = ReadTemporaryBufferInternal(*this, *handle, sizeof(idx_t), block_size, std::move(reusable_buffer));
498: 	handle.reset();
499: 
500: 	// Delete the file and return the buffer.
501: 	DeleteTemporaryFile(block);
502: 	return buffer;
503: }
504: 
505: void StandardBufferManager::DeleteTemporaryFile(BlockHandle &block) {
506: 	auto id = block.BlockId();
507: 	if (temporary_directory.path.empty()) {
508: 		// no temporary directory specified: nothing to delete
509: 		return;
510: 	}
511: 	{
512: 		lock_guard<mutex> guard(temporary_directory.lock);
513: 		if (!temporary_directory.handle) {
514: 			// temporary directory was not initialized yet: nothing to delete
515: 			return;
516: 		}
517: 	}
518: 	// check if we should delete the file from the shared pool of files, or from the general file system
519: 	if (temporary_directory.handle->GetTempFile().HasTemporaryBuffer(id)) {
520: 		evicted_data_per_tag[uint8_t(block.GetMemoryTag())] -= GetBlockSize();
521: 		temporary_directory.handle->GetTempFile().DeleteTemporaryBuffer(id);
522: 		return;
523: 	}
524: 
525: 	// The file is not in the shared pool of files.
526: 	auto &fs = FileSystem::GetFileSystem(db);
527: 	auto path = GetTemporaryPath(id);
528: 	if (fs.FileExists(path)) {
529: 		evicted_data_per_tag[uint8_t(block.GetMemoryTag())] -= block.GetMemoryUsage();
530: 		auto handle = fs.OpenFile(path, FileFlags::FILE_FLAGS_READ);
531: 		auto content_size = handle->GetFileSize();
532: 		handle.reset();
533: 		fs.RemoveFile(path);
534: 		temporary_directory.handle->GetTempFile().DecreaseSizeOnDisk(content_size);
535: 	}
536: }
537: 
538: bool StandardBufferManager::HasTemporaryDirectory() const {
539: 	return !temporary_directory.path.empty();
540: }
541: 
542: vector<TemporaryFileInformation> StandardBufferManager::GetTemporaryFiles() {
543: 	vector<TemporaryFileInformation> result;
544: 	if (temporary_directory.path.empty()) {
545: 		return result;
546: 	}
547: 	{
548: 		lock_guard<mutex> temp_handle_guard(temporary_directory.lock);
549: 		if (temporary_directory.handle) {
550: 			result = temporary_directory.handle->GetTempFile().GetTemporaryFiles();
551: 		}
552: 	}
553: 	auto &fs = FileSystem::GetFileSystem(db);
554: 	fs.ListFiles(temporary_directory.path, [&](const string &name, bool is_dir) {
555: 		if (is_dir) {
556: 			return;
557: 		}
558: 		if (!StringUtil::EndsWith(name, ".block")) {
559: 			return;
560: 		}
561: 
562: 		// Another process or thread can delete the file before we can get its file size.
563: 		auto handle = fs.OpenFile(name, FileFlags::FILE_FLAGS_READ | FileFlags::FILE_FLAGS_NULL_IF_NOT_EXISTS);
564: 		if (!handle) {
565: 			return;
566: 		}
567: 
568: 		TemporaryFileInformation info;
569: 		info.path = name;
570: 		info.size = NumericCast<idx_t>(fs.GetFileSize(*handle));
571: 		handle.reset();
572: 		result.push_back(info);
573: 	});
574: 	return result;
575: }
576: 
577: const char *StandardBufferManager::InMemoryWarning() {
578: 	if (!temporary_directory.path.empty()) {
579: 		return "";
580: 	}
581: 	return "\nDatabase is launched in in-memory mode and no temporary directory is specified."
582: 	       "\nUnused blocks cannot be offloaded to disk."
583: 	       "\n\nLaunch the database with a persistent storage back-end"
584: 	       "\nOr set SET temp_directory='/path/to/tmp.tmp'";
585: }
586: 
587: void StandardBufferManager::ReserveMemory(idx_t size) {
588: 	if (size == 0) {
589: 		return;
590: 	}
591: 	auto reservation =
592: 	    EvictBlocksOrThrow(MemoryTag::EXTENSION, size, nullptr, "failed to reserve memory data of size %s%s",
593: 	                       StringUtil::BytesToHumanReadableString(size));
594: 	reservation.size = 0;
595: }
596: 
597: void StandardBufferManager::FreeReservedMemory(idx_t size) {
598: 	if (size == 0) {
599: 		return;
600: 	}
601: 	buffer_pool.memory_usage.UpdateUsedMemory(MemoryTag::EXTENSION, -(int64_t)size);
602: }
603: 
604: //===--------------------------------------------------------------------===//
605: // Buffer Allocator
606: //===--------------------------------------------------------------------===//
607: data_ptr_t StandardBufferManager::BufferAllocatorAllocate(PrivateAllocatorData *private_data, idx_t size) {
608: 	auto &data = private_data->Cast<BufferAllocatorData>();
609: 	auto reservation =
610: 	    data.manager.EvictBlocksOrThrow(MemoryTag::ALLOCATOR, size, nullptr, "failed to allocate data of size %s%s",
611: 	                                    StringUtil::BytesToHumanReadableString(size));
612: 	// We rely on manual tracking of this one. :(
613: 	reservation.size = 0;
614: 	return Allocator::Get(data.manager.db).AllocateData(size);
615: }
616: 
617: void StandardBufferManager::BufferAllocatorFree(PrivateAllocatorData *private_data, data_ptr_t pointer, idx_t size) {
618: 	auto &data = private_data->Cast<BufferAllocatorData>();
619: 	BufferPoolReservation r(MemoryTag::ALLOCATOR, data.manager.GetBufferPool());
620: 	r.size = size;
621: 	r.Resize(0);
622: 	return Allocator::Get(data.manager.db).FreeData(pointer, size);
623: }
624: 
625: data_ptr_t StandardBufferManager::BufferAllocatorRealloc(PrivateAllocatorData *private_data, data_ptr_t pointer,
626:                                                          idx_t old_size, idx_t size) {
627: 	if (old_size == size) {
628: 		return pointer;
629: 	}
630: 	auto &data = private_data->Cast<BufferAllocatorData>();
631: 	BufferPoolReservation r(MemoryTag::ALLOCATOR, data.manager.GetBufferPool());
632: 	r.size = old_size;
633: 	r.Resize(size);
634: 	r.size = 0;
635: 	return Allocator::Get(data.manager.db).ReallocateData(pointer, old_size, size);
636: }
637: 
638: Allocator &BufferAllocator::Get(ClientContext &context) {
639: 	auto &manager = StandardBufferManager::GetBufferManager(context);
640: 	return manager.GetBufferAllocator();
641: }
642: 
643: Allocator &BufferAllocator::Get(DatabaseInstance &db) {
644: 	return StandardBufferManager::GetBufferManager(db).GetBufferAllocator();
645: }
646: 
647: Allocator &BufferAllocator::Get(AttachedDatabase &db) {
648: 	return BufferAllocator::Get(db.GetDatabase());
649: }
650: 
651: Allocator &StandardBufferManager::GetBufferAllocator() {
652: 	return buffer_allocator;
653: }
654: 
655: } // namespace duckdb
[end of src/storage/standard_buffer_manager.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: