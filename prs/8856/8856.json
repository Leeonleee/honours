{
  "repo": "duckdb/duckdb",
  "pull_number": 8856,
  "instance_id": "duckdb__duckdb-8856",
  "issue_numbers": [
    "8522"
  ],
  "base_commit": "ba71015ee711d95fe13886f372824143cae7493c",
  "patch": "diff --git a/src/function/table/arrow.cpp b/src/function/table/arrow.cpp\nindex 8604c779319a..1153948ce63c 100644\n--- a/src/function/table/arrow.cpp\n+++ b/src/function/table/arrow.cpp\n@@ -208,18 +208,10 @@ void ArrowTableFunction::RenameArrowColumns(vector<string> &names) {\n \t}\n }\n \n-unique_ptr<FunctionData> ArrowTableFunction::ArrowScanBind(ClientContext &context, TableFunctionBindInput &input,\n-                                                           vector<LogicalType> &return_types, vector<string> &names) {\n-\tauto stream_factory_ptr = input.inputs[0].GetPointer();\n-\tauto stream_factory_produce = (stream_factory_produce_t)input.inputs[1].GetPointer();       // NOLINT\n-\tauto stream_factory_get_schema = (stream_factory_get_schema_t)input.inputs[2].GetPointer(); // NOLINT\n-\n-\tauto res = make_uniq<ArrowScanFunctionData>(stream_factory_produce, stream_factory_ptr);\n-\n-\tauto &data = *res;\n-\tstream_factory_get_schema(stream_factory_ptr, data.schema_root);\n-\tfor (idx_t col_idx = 0; col_idx < (idx_t)data.schema_root.arrow_schema.n_children; col_idx++) {\n-\t\tauto &schema = *data.schema_root.arrow_schema.children[col_idx];\n+void ArrowTableFunction::PopulateArrowTableType(ArrowTableType &arrow_table, ArrowSchemaWrapper &schema_p,\n+                                                vector<string> &names, vector<LogicalType> &return_types) {\n+\tfor (idx_t col_idx = 0; col_idx < (idx_t)schema_p.arrow_schema.n_children; col_idx++) {\n+\t\tauto &schema = *schema_p.arrow_schema.children[col_idx];\n \t\tif (!schema.release) {\n \t\t\tthrow InvalidInputException(\"arrow_scan: released schema passed\");\n \t\t}\n@@ -233,7 +225,7 @@ unique_ptr<FunctionData> ArrowTableFunction::ArrowScanBind(ClientContext &contex\n \t\t} else {\n \t\t\treturn_types.emplace_back(arrow_type->GetDuckType());\n \t\t}\n-\t\tres->arrow_table.AddColumn(col_idx, std::move(arrow_type));\n+\t\tarrow_table.AddColumn(col_idx, std::move(arrow_type));\n \t\tauto format = string(schema.format);\n \t\tauto name = string(schema.name);\n \t\tif (name.empty()) {\n@@ -241,6 +233,19 @@ unique_ptr<FunctionData> ArrowTableFunction::ArrowScanBind(ClientContext &contex\n \t\t}\n \t\tnames.push_back(name);\n \t}\n+}\n+\n+unique_ptr<FunctionData> ArrowTableFunction::ArrowScanBind(ClientContext &context, TableFunctionBindInput &input,\n+                                                           vector<LogicalType> &return_types, vector<string> &names) {\n+\tauto stream_factory_ptr = input.inputs[0].GetPointer();\n+\tauto stream_factory_produce = (stream_factory_produce_t)input.inputs[1].GetPointer();       // NOLINT\n+\tauto stream_factory_get_schema = (stream_factory_get_schema_t)input.inputs[2].GetPointer(); // NOLINT\n+\n+\tauto res = make_uniq<ArrowScanFunctionData>(stream_factory_produce, stream_factory_ptr);\n+\n+\tauto &data = *res;\n+\tstream_factory_get_schema(stream_factory_ptr, data.schema_root);\n+\tPopulateArrowTableType(res->arrow_table, data.schema_root, names, return_types);\n \tRenameArrowColumns(names);\n \tres->all_types = return_types;\n \treturn std::move(res);\ndiff --git a/src/include/duckdb/function/table/arrow.hpp b/src/include/duckdb/function/table/arrow.hpp\nindex fd295edf620a..fae51810aae2 100644\n--- a/src/include/duckdb/function/table/arrow.hpp\n+++ b/src/include/duckdb/function/table/arrow.hpp\n@@ -129,6 +129,8 @@ struct ArrowTableFunction {\n \n \t//! Scan Function\n \tstatic void ArrowScanFunction(ClientContext &context, TableFunctionInput &data, DataChunk &output);\n+\tstatic void PopulateArrowTableType(ArrowTableType &arrow_table, ArrowSchemaWrapper &schema_p, vector<string> &names,\n+\t                                   vector<LogicalType> &return_types);\n \n protected:\n \t//! Defines Maximum Number of Threads\ndiff --git a/tools/pythonpkg/src/arrow/arrow_array_stream.cpp b/tools/pythonpkg/src/arrow/arrow_array_stream.cpp\nindex 6f2105f80b55..3d8e13454ee9 100644\n--- a/tools/pythonpkg/src/arrow/arrow_array_stream.cpp\n+++ b/tools/pythonpkg/src/arrow/arrow_array_stream.cpp\n@@ -11,6 +11,7 @@\n #include \"duckdb_python/pyconnection/pyconnection.hpp\"\n #include \"duckdb_python/pyrelation.hpp\"\n #include \"duckdb_python/pyresult.hpp\"\n+#include \"duckdb/function/table/arrow.hpp\"\n \n namespace duckdb {\n \n@@ -23,7 +24,10 @@ void TransformDuckToArrowChunk(ArrowSchema &arrow_schema, ArrowArray &data, py::\n \n void VerifyArrowDatasetLoaded() {\n \tauto &import_cache = *DuckDBPyConnection::ImportCache();\n-\tif (!import_cache.arrow_dataset().IsLoaded()) {\n+\tauto loaded = import_cache.arrow_dataset().IsLoaded();\n+\t// Have to check 'ModuleIsLoaded' because it could be monkeypatched\n+\t// so we think it's loaded, but it's not present in the sys modules\n+\tif (!loaded || !ModuleIsLoaded<ArrowDatasetCacheItem>()) {\n \t\tthrow InvalidInputException(\"Optional module 'pyarrow.dataset' is required to perform this action\");\n \t}\n }\n@@ -53,12 +57,20 @@ PyArrowObjectType GetArrowType(const py::handle &obj) {\n py::object PythonTableArrowArrayStreamFactory::ProduceScanner(py::object &arrow_scanner, py::handle &arrow_obj_handle,\n                                                               ArrowStreamParameters &parameters,\n                                                               const ClientProperties &client_properties) {\n+\tArrowSchemaWrapper schema;\n+\tPythonTableArrowArrayStreamFactory::GetSchemaInternal(arrow_obj_handle, schema);\n+\tvector<string> unused_names;\n+\tvector<LogicalType> unused_types;\n+\tArrowTableType arrow_table;\n+\tArrowTableFunction::PopulateArrowTableType(arrow_table, schema, unused_names, unused_types);\n+\n \tauto filters = parameters.filters;\n \tauto &column_list = parameters.projected_columns.columns;\n \tbool has_filter = filters && !filters->filters.empty();\n \tpy::list projection_list = py::cast(column_list);\n \tif (has_filter) {\n-\t\tauto filter = TransformFilter(*filters, parameters.projected_columns.projection_map, client_properties);\n+\t\tauto filter =\n+\t\t    TransformFilter(*filters, parameters.projected_columns.projection_map, client_properties, arrow_table);\n \t\tif (column_list.empty()) {\n \t\t\treturn arrow_scanner(arrow_obj_handle, py::arg(\"filter\") = filter);\n \t\t} else {\n@@ -80,11 +92,12 @@ unique_ptr<ArrowArrayStreamWrapper> PythonTableArrowArrayStreamFactory::Produce(\n \tpy::handle arrow_obj_handle(factory->arrow_object);\n \tauto arrow_object_type = GetArrowType(arrow_obj_handle);\n \n+\tauto &import_cache = *DuckDBPyConnection::ImportCache();\n \tpy::object scanner;\n-\tpy::object arrow_batch_scanner = py::module_::import(\"pyarrow.dataset\").attr(\"Scanner\").attr(\"from_batches\");\n+\tpy::object arrow_batch_scanner = import_cache.arrow_dataset().Scanner().attr(\"from_batches\");\n \tswitch (arrow_object_type) {\n \tcase PyArrowObjectType::Table: {\n-\t\tauto arrow_dataset = py::module_::import(\"pyarrow.dataset\").attr(\"dataset\");\n+\t\tauto arrow_dataset = import_cache.arrow_dataset()().attr(\"dataset\");\n \t\tauto dataset = arrow_dataset(arrow_obj_handle);\n \t\tpy::object arrow_scanner = dataset.attr(\"__class__\").attr(\"scanner\");\n \t\tscanner = ProduceScanner(arrow_scanner, dataset, parameters, factory->client_properties);\n@@ -119,12 +132,8 @@ unique_ptr<ArrowArrayStreamWrapper> PythonTableArrowArrayStreamFactory::Produce(\n \treturn res;\n }\n \n-void PythonTableArrowArrayStreamFactory::GetSchema(uintptr_t factory_ptr, ArrowSchemaWrapper &schema) {\n-\tpy::gil_scoped_acquire acquire;\n-\tauto factory = static_cast<PythonTableArrowArrayStreamFactory *>(reinterpret_cast<void *>(factory_ptr));\n-\tD_ASSERT(factory->arrow_object);\n+void PythonTableArrowArrayStreamFactory::GetSchemaInternal(py::handle arrow_obj_handle, ArrowSchemaWrapper &schema) {\n \tauto table_class = py::module::import(\"pyarrow\").attr(\"Table\");\n-\tpy::handle arrow_obj_handle(factory->arrow_object);\n \tif (py::isinstance(arrow_obj_handle, table_class)) {\n \t\tauto obj_schema = arrow_obj_handle.attr(\"schema\");\n \t\tauto export_to_c = obj_schema.attr(\"_export_to_c\");\n@@ -134,7 +143,8 @@ void PythonTableArrowArrayStreamFactory::GetSchema(uintptr_t factory_ptr, ArrowS\n \n \tVerifyArrowDatasetLoaded();\n \n-\tauto scanner_class = py::module::import(\"pyarrow.dataset\").attr(\"Scanner\");\n+\tauto &import_cache = *DuckDBPyConnection::ImportCache();\n+\tauto scanner_class = import_cache.arrow_dataset().Scanner();\n \n \tif (py::isinstance(arrow_obj_handle, scanner_class)) {\n \t\tauto obj_schema = arrow_obj_handle.attr(\"projected_schema\");\n@@ -147,9 +157,48 @@ void PythonTableArrowArrayStreamFactory::GetSchema(uintptr_t factory_ptr, ArrowS\n \t}\n }\n \n-py::object GetScalar(Value &constant, const string &timezone_config) {\n+void PythonTableArrowArrayStreamFactory::GetSchema(uintptr_t factory_ptr, ArrowSchemaWrapper &schema) {\n+\tpy::gil_scoped_acquire acquire;\n+\tauto factory = static_cast<PythonTableArrowArrayStreamFactory *>(reinterpret_cast<void *>(factory_ptr));\n+\tD_ASSERT(factory->arrow_object);\n+\tpy::handle arrow_obj_handle(factory->arrow_object);\n+\tGetSchemaInternal(arrow_obj_handle, schema);\n+}\n+\n+string ConvertTimestampUnit(ArrowDateTimeType unit) {\n+\tswitch (unit) {\n+\tcase ArrowDateTimeType::MICROSECONDS:\n+\t\treturn \"us\";\n+\tcase ArrowDateTimeType::MILLISECONDS:\n+\t\treturn \"ms\";\n+\tcase ArrowDateTimeType::NANOSECONDS:\n+\t\treturn \"ns\";\n+\tcase ArrowDateTimeType::SECONDS:\n+\t\treturn \"s\";\n+\tdefault:\n+\t\tthrow NotImplementedException(\"DatetimeType not recognized in ConvertTimestampUnit\");\n+\t}\n+}\n+\n+int64_t ConvertTimestampTZValue(int64_t base_value, ArrowDateTimeType datetime_type) {\n+\tswitch (datetime_type) {\n+\tcase ArrowDateTimeType::MICROSECONDS:\n+\t\treturn Timestamp::GetEpochMicroSeconds(timestamp_t(base_value));\n+\tcase ArrowDateTimeType::MILLISECONDS:\n+\t\treturn Timestamp::GetEpochMs(timestamp_t(base_value));\n+\tcase ArrowDateTimeType::NANOSECONDS:\n+\t\treturn Timestamp::GetEpochNanoSeconds(timestamp_t(base_value));\n+\tcase ArrowDateTimeType::SECONDS:\n+\t\treturn Timestamp::GetEpochSeconds(timestamp_t(base_value));\n+\tdefault:\n+\t\tthrow NotImplementedException(\"DatetimeType not recognized in ConvertTimestampTZValue\");\n+\t}\n+}\n+\n+py::object GetScalar(Value &constant, const string &timezone_config, const ArrowType &type) {\n \tpy::object scalar = py::module_::import(\"pyarrow\").attr(\"scalar\");\n-\tpy::object dataset_scalar = py::module_::import(\"pyarrow.dataset\").attr(\"scalar\");\n+\tauto &import_cache = *DuckDBPyConnection::ImportCache();\n+\tpy::object dataset_scalar = import_cache.arrow_dataset()().attr(\"scalar\");\n \tpy::object scalar_value;\n \tswitch (constant.type().id()) {\n \tcase LogicalTypeId::BOOLEAN:\n@@ -187,8 +236,12 @@ py::object GetScalar(Value &constant, const string &timezone_config) {\n \t\treturn dataset_scalar(scalar(constant.GetValue<int64_t>(), date_type(\"s\")));\n \t}\n \tcase LogicalTypeId::TIMESTAMP_TZ: {\n+\t\tauto base_value = constant.GetValue<int64_t>();\n+\t\tauto arrow_datetime_type = type.GetDateTimeType();\n+\t\tauto time_unit_string = ConvertTimestampUnit(arrow_datetime_type);\n+\t\tauto converted_value = ConvertTimestampTZValue(base_value, arrow_datetime_type);\n \t\tpy::object date_type = py::module_::import(\"pyarrow\").attr(\"timestamp\");\n-\t\treturn dataset_scalar(scalar(constant.GetValue<int64_t>(), date_type(\"us\", py::arg(\"tz\") = timezone_config)));\n+\t\treturn dataset_scalar(scalar(converted_value, date_type(time_unit_string, py::arg(\"tz\") = timezone_config)));\n \t}\n \tcase LogicalTypeId::UTINYINT:\n \t\treturn dataset_scalar(constant.GetValue<uint8_t>());\n@@ -233,13 +286,15 @@ py::object GetScalar(Value &constant, const string &timezone_config) {\n \t}\n }\n \n-py::object TransformFilterRecursive(TableFilter *filter, const string &column_name, const string &timezone_config) {\n-\tpy::object field = py::module_::import(\"pyarrow.dataset\").attr(\"field\");\n+py::object TransformFilterRecursive(TableFilter *filter, const string &column_name, const string &timezone_config,\n+                                    const ArrowType &type) {\n+\tauto &import_cache = *DuckDBPyConnection::ImportCache();\n+\tpy::object field = import_cache.arrow_dataset()().attr(\"field\");\n \tswitch (filter->filter_type) {\n \tcase TableFilterType::CONSTANT_COMPARISON: {\n \t\tauto &constant_filter = filter->Cast<ConstantFilter>();\n \t\tauto constant_field = field(column_name);\n-\t\tauto constant_value = GetScalar(constant_filter.constant, timezone_config);\n+\t\tauto constant_value = GetScalar(constant_filter.constant, timezone_config, type);\n \t\tswitch (constant_filter.comparison_type) {\n \t\tcase ExpressionType::COMPARE_EQUAL: {\n \t\t\treturn constant_field.attr(\"__eq__\")(constant_value);\n@@ -275,10 +330,10 @@ py::object TransformFilterRecursive(TableFilter *filter, const string &column_na\n \t\tauto &or_filter = filter->Cast<ConjunctionOrFilter>();\n \t\t//! Get first non null filter type\n \t\tauto child_filter = or_filter.child_filters[i++].get();\n-\t\tpy::object expression = TransformFilterRecursive(child_filter, column_name, timezone_config);\n+\t\tpy::object expression = TransformFilterRecursive(child_filter, column_name, timezone_config, type);\n \t\twhile (i < or_filter.child_filters.size()) {\n \t\t\tchild_filter = or_filter.child_filters[i++].get();\n-\t\t\tpy::object child_expression = TransformFilterRecursive(child_filter, column_name, timezone_config);\n+\t\t\tpy::object child_expression = TransformFilterRecursive(child_filter, column_name, timezone_config, type);\n \t\t\texpression = expression.attr(\"__or__\")(child_expression);\n \t\t}\n \t\treturn expression;\n@@ -287,10 +342,10 @@ py::object TransformFilterRecursive(TableFilter *filter, const string &column_na\n \t\tidx_t i = 0;\n \t\tauto &and_filter = filter->Cast<ConjunctionAndFilter>();\n \t\tauto child_filter = and_filter.child_filters[i++].get();\n-\t\tpy::object expression = TransformFilterRecursive(child_filter, column_name, timezone_config);\n+\t\tpy::object expression = TransformFilterRecursive(child_filter, column_name, timezone_config, type);\n \t\twhile (i < and_filter.child_filters.size()) {\n \t\t\tchild_filter = and_filter.child_filters[i++].get();\n-\t\t\tpy::object child_expression = TransformFilterRecursive(child_filter, column_name, timezone_config);\n+\t\t\tpy::object child_expression = TransformFilterRecursive(child_filter, column_name, timezone_config, type);\n \t\t\texpression = expression.attr(\"__and__\")(child_expression);\n \t\t}\n \t\treturn expression;\n@@ -302,13 +357,17 @@ py::object TransformFilterRecursive(TableFilter *filter, const string &column_na\n \n py::object PythonTableArrowArrayStreamFactory::TransformFilter(TableFilterSet &filter_collection,\n                                                                std::unordered_map<idx_t, string> &columns,\n-                                                               const ClientProperties &config) {\n+                                                               const ClientProperties &config,\n+                                                               const ArrowTableType &arrow_table) {\n \tauto filters_map = &filter_collection.filters;\n \tauto it = filters_map->begin();\n \tD_ASSERT(columns.find(it->first) != columns.end());\n-\tpy::object expression = TransformFilterRecursive(it->second.get(), columns[it->first], config.time_zone);\n+\tauto &arrow_type = *arrow_table.GetColumns().at(it->first);\n+\tpy::object expression =\n+\t    TransformFilterRecursive(it->second.get(), columns[it->first], config.time_zone, arrow_type);\n \twhile (it != filters_map->end()) {\n-\t\tpy::object child_expression = TransformFilterRecursive(it->second.get(), columns[it->first], config.time_zone);\n+\t\tpy::object child_expression =\n+\t\t    TransformFilterRecursive(it->second.get(), columns[it->first], config.time_zone, arrow_type);\n \t\texpression = expression.attr(\"__and__\")(child_expression);\n \t\tit++;\n \t}\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/arrow/arrow_array_stream.hpp b/tools/pythonpkg/src/include/duckdb_python/arrow/arrow_array_stream.hpp\nindex 95f9ce6b62d9..4ad36b6e606a 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/arrow/arrow_array_stream.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/arrow/arrow_array_stream.hpp\n@@ -65,6 +65,7 @@ class PythonTableArrowArrayStreamFactory {\n \tstatic unique_ptr<ArrowArrayStreamWrapper> Produce(uintptr_t factory, ArrowStreamParameters &parameters);\n \n \t//! Get the schema of the arrow object\n+\tstatic void GetSchemaInternal(py::handle arrow_object, ArrowSchemaWrapper &schema);\n \tstatic void GetSchema(uintptr_t factory_ptr, ArrowSchemaWrapper &schema);\n \n \t//! Arrow Object (i.e., Scanner, Record Batch Reader, Table, Dataset)\n@@ -75,7 +76,7 @@ class PythonTableArrowArrayStreamFactory {\n private:\n \t//! We transform a TableFilterSet to an Arrow Expression Object\n \tstatic py::object TransformFilter(TableFilterSet &filters, std::unordered_map<idx_t, string> &columns,\n-\t                                  const ClientProperties &client_properties);\n+\t                                  const ClientProperties &client_properties, const ArrowTableType &arrow_table);\n \n \tstatic py::object ProduceScanner(py::object &arrow_scanner, py::handle &arrow_obj_handle,\n \t                                 ArrowStreamParameters &parameters, const ClientProperties &client_properties);\ndiff --git a/tools/pythonpkg/src/python_import_cache.cpp b/tools/pythonpkg/src/python_import_cache.cpp\nindex b5b00e46a66e..3291e138888a 100644\n--- a/tools/pythonpkg/src/python_import_cache.cpp\n+++ b/tools/pythonpkg/src/python_import_cache.cpp\n@@ -17,7 +17,11 @@ bool PythonImportCacheItem::LoadSucceeded() const {\n \n bool PythonImportCacheItem::IsLoaded() const {\n \tauto type = (*this)();\n-\treturn type.ptr() != nullptr;\n+\tbool loaded = type.ptr() != nullptr;\n+\tif (!loaded) {\n+\t\treturn false;\n+\t}\n+\treturn true;\n }\n \n py::handle PythonImportCacheItem::AddCache(PythonImportCache &cache, py::object object) {\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/arrow/test_8522.py b/tools/pythonpkg/tests/fast/arrow/test_8522.py\nnew file mode 100644\nindex 000000000000..ef91b146e959\n--- /dev/null\n+++ b/tools/pythonpkg/tests/fast/arrow/test_8522.py\n@@ -0,0 +1,28 @@\n+import duckdb\n+import pytest\n+import string\n+import datetime as dt\n+\n+pa = pytest.importorskip(\"pyarrow\")\n+\n+\n+# Reconstruct filters when pushing down into arrow scan\n+# arrow supports timestamp_tz with different units than US, we only support US\n+# so we have to convert ConstantValues back to their native unit when pushing the filter expression containing them down to pyarrow\n+class Test8522(object):\n+    def test_8522(self):\n+        t_us = pa.Table.from_arrays(\n+            arrays=[pa.array([dt.datetime(2022, 1, 1)])],\n+            schema=pa.schema([pa.field(\"time\", pa.timestamp(\"us\", tz=\"UTC\"))]),\n+        )\n+\n+        t_ms = pa.Table.from_arrays(\n+            arrays=[pa.array([dt.datetime(2022, 1, 1)])],\n+            schema=pa.schema([pa.field(\"time\", pa.timestamp(\"ms\", tz=\"UTC\"))]),\n+        )\n+\n+        expected = duckdb.sql(\"FROM t_us\").filter(\"time>='2022-01-01'\").fetchall()\n+        assert len(expected) == 1\n+\n+        actual = duckdb.sql(\"FROM t_ms\").filter(\"time>='2022-01-01'\").fetchall()\n+        assert actual == expected\ndiff --git a/tools/pythonpkg/tests/fast/test_import_without_pyarrow_dataset.py b/tools/pythonpkg/tests/fast/test_import_without_pyarrow_dataset.py\ndeleted file mode 100644\nindex 57188b61e9a8..000000000000\n--- a/tools/pythonpkg/tests/fast/test_import_without_pyarrow_dataset.py\n+++ /dev/null\n@@ -1,18 +0,0 @@\n-import pytest\n-import sys\n-\n-pyarrow = pytest.importorskip(\"pyarrow\")\n-\n-\n-class TestImportWithoutPyArrowDataset:\n-    def test_import(self, monkeypatch: pytest.MonkeyPatch):\n-        monkeypatch.setitem(sys.modules, \"pyarrow.dataset\", None)\n-        import duckdb\n-\n-        # We should be able to import duckdb even when pyarrow.dataset is missing\n-        con = duckdb.connect()\n-        rel = con.query('select 1')\n-        arrow_record_batch = rel.record_batch()\n-        with pytest.raises(duckdb.InvalidInputException):\n-            # The replacement scan functionality relies on pyarrow.dataset\n-            con.query('select * from arrow_record_batch')\n",
  "problem_statement": "[python] InvalidInputException filtering pyarrow table timestamp column with timezone and unit is not 'us'\n### What happens?\r\n\r\nWhen filtering a timestamp column with a timezone and the unit is not 'us' of a pyarrow table the following error appears:\r\n\r\n\r\n```\r\nInvalidInputException: Invalid Input Error: Attempting to execute an unsuccessful or closed pending query result\r\n\r\nError: Invalid Error: ArrowNotImplementedError: Function 'greater_equal' has no kernel matching input types (timestamp[ms, tz=UTC], timestamp[s])\r\n\r\nAt:\r\n  pyarrow/error.pxi(121): pyarrow.lib.check_status\r\n```\r\npyarrow version is 12.0.1\r\n\r\n\r\n### To Reproduce\r\n\r\n```python\r\nimport pyarrow as pa\r\nimport datetime as dt\r\nimport duckdb\r\n\r\nt_us = pa.Table.from_arrays(\r\n    arrays=[pa.array([dt.datetime(2022,1,1)])], \r\n    schema=pa.schema([pa.field(\"time\", pa.timestamp(\"us\", tz=\"UTC\"))])\r\n)\r\n\r\nt_ms = pa.Table.from_arrays(\r\n    arrays=[pa.array([dt.datetime(2022,1,1)])], \r\n    schema=pa.schema([pa.field(\"time\", pa.timestamp(\"ms\", tz=\"UTC\"))])\r\n)\r\n```\r\n```sql\r\nduckdb.sql(\"FROM t_us\").filter(\"time>='2022-01-01'\") \r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502           time           \u2502\r\n\u2502 timestamp with time zone \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 2022-01-01 00:00:00+00   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n```sql\r\nduckdb.sql(\"FROM t_ms\").filter(\"time>='2022-01-01'\") \r\n\r\nInvalidInputException: Invalid Input Error: Attempting to execute an unsuccessful or closed pending query result\r\nError: Invalid Error: ArrowNotImplementedError: Function 'greater_equal' has no kernel matching input types (timestamp[ms, tz=UTC], timestamp[s])\r\n\r\nAt:\r\n  pyarrow/error.pxi(121): pyarrow.lib.check_status\r\n```\r\n\r\n### OS:\r\n\r\nUbuntu 22.04 x86_64\r\n\r\n### DuckDB Version:\r\n\r\n0.8.2-dev1435\r\n\r\n### DuckDB Client:\r\n\r\npython\r\n\r\n### Full Name:\r\n\r\nVolker Lorrmann\r\n\r\n### Affiliation:\r\n\r\nSiemens\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\nI have tested with a master build\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] Yes, I have\n",
  "hints_text": "@legout thanks for reporting this. I made some slight adjustments to your code and managed to reproduce it.",
  "created_at": "2023-09-08T15:45:14Z"
}