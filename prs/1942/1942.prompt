You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Invalid string encoding aborts Parquet import
I'm trying to import a series of parquet files into duckdb.  Unfortunately, the import aborts with the following error:

```python
In [51]: cursor.execute("INSERT INTO master_email SELECT * FROM 'master_email/*.parquet'").df()    
 ---------------------------------------------------------------------------                                                            RuntimeError                              Traceback (most recent call last)                                                            
<ipython-input-51-90580d0a8a82> in <module>                                                                                           
----> 1 cursor.execute("INSERT INTO master_email SELECT * FROM 'master_email/*.parquet'").df() 
                                                                                                                                                                               
RuntimeError: INTERNAL Error: Invalid string encoding found in Parquet file: value is not valid UTF8! 

```

Pandas (via pyarrow), Spark and Presto all read these same parquet files without issue.

Would be nice to have the option to either ignore invalid rows, or at least include details about the bad record in the error so they can be manually cleaned up.

</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
2: 
3: ![.github/workflows/main.yml](https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
6: 
7: 
8: ## Installation
9: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
10: 
11: ## Development
12: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
13: 
14: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
15: 
16: 
[end of README.md]
[start of extension/parquet/column_reader.cpp]
1: #include "column_reader.hpp"
2: #include "parquet_timestamp.hpp"
3: #include "utf8proc_wrapper.hpp"
4: #include "parquet_reader.hpp"
5: 
6: #include "boolean_column_reader.hpp"
7: #include "callback_column_reader.hpp"
8: #include "decimal_column_reader.hpp"
9: #include "list_column_reader.hpp"
10: #include "string_column_reader.hpp"
11: #include "struct_column_reader.hpp"
12: #include "templated_column_reader.hpp"
13: 
14: #include "snappy.h"
15: #include "miniz_wrapper.hpp"
16: #include "zstd.h"
17: #include <iostream>
18: 
19: #include "duckdb.hpp"
20: #ifndef DUCKDB_AMALGAMATION
21: #include "duckdb/common/types/chunk_collection.hpp"
22: #endif
23: 
24: namespace duckdb {
25: 
26: using duckdb_parquet::format::CompressionCodec;
27: using duckdb_parquet::format::ConvertedType;
28: using duckdb_parquet::format::Encoding;
29: using duckdb_parquet::format::PageType;
30: using duckdb_parquet::format::Type;
31: 
32: const uint32_t RleBpDecoder::BITPACK_MASKS[] = {
33:     0,       1,       3,        7,        15,       31,        63,        127,       255,        511,       1023,
34:     2047,    4095,    8191,     16383,    32767,    65535,     131071,    262143,    524287,     1048575,   2097151,
35:     4194303, 8388607, 16777215, 33554431, 67108863, 134217727, 268435455, 536870911, 1073741823, 2147483647};
36: 
37: const uint8_t RleBpDecoder::BITPACK_DLEN = 8;
38: 
39: ColumnReader::ColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
40:                            idx_t max_define_p, idx_t max_repeat_p)
41:     : schema(schema_p), file_idx(file_idx_p), max_define(max_define_p), max_repeat(max_repeat_p), reader(reader),
42:       type(move(type_p)), page_rows_available(0), dummy_result(type, nullptr) {
43: 
44: 	// dummies for Skip()
45: 	none_filter.none();
46: 	dummy_define.resize(reader.allocator, STANDARD_VECTOR_SIZE);
47: 	dummy_repeat.resize(reader.allocator, STANDARD_VECTOR_SIZE);
48: }
49: 
50: ColumnReader::~ColumnReader() {
51: }
52: 
53: unique_ptr<ColumnReader> ColumnReader::CreateReader(ParquetReader &reader, const LogicalType &type_p,
54:                                                     const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,
55:                                                     idx_t max_repeat) {
56: 	switch (type_p.id()) {
57: 	case LogicalTypeId::BOOLEAN:
58: 		return make_unique<BooleanColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
59: 	case LogicalTypeId::UTINYINT:
60: 		return make_unique<TemplatedColumnReader<uint8_t, TemplatedParquetValueConversion<uint32_t>>>(
61: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
62: 	case LogicalTypeId::USMALLINT:
63: 		return make_unique<TemplatedColumnReader<uint16_t, TemplatedParquetValueConversion<uint32_t>>>(
64: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
65: 	case LogicalTypeId::UINTEGER:
66: 		return make_unique<TemplatedColumnReader<uint32_t, TemplatedParquetValueConversion<uint32_t>>>(
67: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
68: 	case LogicalTypeId::UBIGINT:
69: 		return make_unique<TemplatedColumnReader<uint64_t, TemplatedParquetValueConversion<uint64_t>>>(
70: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
71: 	case LogicalTypeId::INTEGER:
72: 		return make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<int32_t>>>(
73: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
74: 	case LogicalTypeId::BIGINT:
75: 		return make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<int64_t>>>(
76: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
77: 	case LogicalTypeId::FLOAT:
78: 		return make_unique<TemplatedColumnReader<float, TemplatedParquetValueConversion<float>>>(
79: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
80: 	case LogicalTypeId::DOUBLE:
81: 		return make_unique<TemplatedColumnReader<double, TemplatedParquetValueConversion<double>>>(
82: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
83: 	case LogicalTypeId::TIMESTAMP:
84: 		switch (schema_p.type) {
85: 		case Type::INT96:
86: 			return make_unique<CallbackColumnReader<Int96, timestamp_t, ImpalaTimestampToTimestamp>>(
87: 			    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
88: 		case Type::INT64:
89: 			switch (schema_p.converted_type) {
90: 			case ConvertedType::TIMESTAMP_MICROS:
91: 				return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMicrosToTimestamp>>(
92: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
93: 			case ConvertedType::TIMESTAMP_MILLIS:
94: 				return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMsToTimestamp>>(
95: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
96: 			default:
97: 				break;
98: 			}
99: 		default:
100: 			break;
101: 		}
102: 		break;
103: 	case LogicalTypeId::DATE:
104: 		return make_unique<CallbackColumnReader<int32_t, date_t, ParquetIntToDate>>(reader, type_p, schema_p,
105: 		                                                                            file_idx_p, max_define, max_repeat);
106: 	case LogicalTypeId::BLOB:
107: 	case LogicalTypeId::VARCHAR:
108: 		return make_unique<StringColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
109: 	case LogicalTypeId::DECIMAL:
110: 		// we have to figure out what kind of int we need
111: 		switch (type_p.InternalType()) {
112: 		case PhysicalType::INT16:
113: 			return make_unique<DecimalColumnReader<int16_t>>(reader, type_p, schema_p, file_idx_p, max_define,
114: 			                                                 max_repeat);
115: 		case PhysicalType::INT32:
116: 			return make_unique<DecimalColumnReader<int32_t>>(reader, type_p, schema_p, file_idx_p, max_define,
117: 			                                                 max_repeat);
118: 		case PhysicalType::INT64:
119: 			return make_unique<DecimalColumnReader<int64_t>>(reader, type_p, schema_p, file_idx_p, max_define,
120: 			                                                 max_repeat);
121: 		case PhysicalType::INT128:
122: 			return make_unique<DecimalColumnReader<hugeint_t>>(reader, type_p, schema_p, file_idx_p, max_define,
123: 			                                                   max_repeat);
124: 
125: 		default:
126: 			break;
127: 		}
128: 		break;
129: 	default:
130: 		break;
131: 	}
132: 	throw NotImplementedException(type_p.ToString());
133: }
134: 
135: void ColumnReader::PrepareRead(parquet_filter_t &filter) {
136: 	dict_decoder.reset();
137: 	defined_decoder.reset();
138: 	block.reset();
139: 
140: 	PageHeader page_hdr;
141: 	page_hdr.read(protocol);
142: 
143: 	//	page_hdr.printTo(std::cout);
144: 	//	std::cout << '\n';
145: 
146: 	PreparePage(page_hdr.compressed_page_size, page_hdr.uncompressed_page_size);
147: 
148: 	switch (page_hdr.type) {
149: 	case PageType::DATA_PAGE_V2:
150: 	case PageType::DATA_PAGE:
151: 		PrepareDataPage(page_hdr);
152: 		break;
153: 	case PageType::DICTIONARY_PAGE:
154: 		Dictionary(move(block), page_hdr.dictionary_page_header.num_values);
155: 		break;
156: 	default:
157: 		break; // ignore INDEX page type and any other custom extensions
158: 	}
159: }
160: 
161: void ColumnReader::PreparePage(idx_t compressed_page_size, idx_t uncompressed_page_size) {
162: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
163: 
164: 	block = make_shared<ResizeableBuffer>(reader.allocator, compressed_page_size + 1);
165: 	trans.read((uint8_t *)block->ptr, compressed_page_size);
166: 
167: 	shared_ptr<ResizeableBuffer> unpacked_block;
168: 	if (chunk->meta_data.codec != CompressionCodec::UNCOMPRESSED) {
169: 		unpacked_block = make_shared<ResizeableBuffer>(reader.allocator, uncompressed_page_size + 1);
170: 	}
171: 
172: 	switch (chunk->meta_data.codec) {
173: 	case CompressionCodec::UNCOMPRESSED:
174: 		break;
175: 	case CompressionCodec::GZIP: {
176: 		MiniZStream s;
177: 
178: 		s.Decompress((const char *)block->ptr, compressed_page_size, (char *)unpacked_block->ptr,
179: 		             uncompressed_page_size);
180: 		block = move(unpacked_block);
181: 
182: 		break;
183: 	}
184: 	case CompressionCodec::SNAPPY: {
185: 		auto res = snappy::RawUncompress((const char *)block->ptr, compressed_page_size, (char *)unpacked_block->ptr);
186: 		if (!res) {
187: 			throw std::runtime_error("Decompression failure");
188: 		}
189: 		block = move(unpacked_block);
190: 		break;
191: 	}
192: 	case CompressionCodec::ZSTD: {
193: 		auto res = duckdb_zstd::ZSTD_decompress((char *)unpacked_block->ptr, uncompressed_page_size,
194: 		                                        (const char *)block->ptr, compressed_page_size);
195: 		if (duckdb_zstd::ZSTD_isError(res) || res != (size_t)uncompressed_page_size) {
196: 			throw std::runtime_error("ZSTD Decompression failure");
197: 		}
198: 		block = move(unpacked_block);
199: 		break;
200: 	}
201: 
202: 	default: {
203: 		std::stringstream codec_name;
204: 		codec_name << chunk->meta_data.codec;
205: 		throw std::runtime_error("Unsupported compression codec \"" + codec_name.str() +
206: 		                         "\". Supported options are uncompressed, gzip or snappy");
207: 		break;
208: 	}
209: 	}
210: }
211: 
212: static uint8_t ComputeBitWidth(idx_t val) {
213: 	if (val == 0) {
214: 		return 0;
215: 	}
216: 	uint8_t ret = 1;
217: 	while (((idx_t)(1 << ret) - 1) < val) {
218: 		ret++;
219: 	}
220: 	return ret;
221: }
222: 
223: void ColumnReader::PrepareDataPage(PageHeader &page_hdr) {
224: 	if (page_hdr.type == PageType::DATA_PAGE && !page_hdr.__isset.data_page_header) {
225: 		throw std::runtime_error("Missing data page header from data page");
226: 	}
227: 	if (page_hdr.type == PageType::DATA_PAGE_V2 && !page_hdr.__isset.data_page_header_v2) {
228: 		throw std::runtime_error("Missing data page header from data page v2");
229: 	}
230: 
231: 	page_rows_available = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.num_values
232: 	                                                           : page_hdr.data_page_header_v2.num_values;
233: 	auto page_encoding = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.encoding
234: 	                                                          : page_hdr.data_page_header_v2.encoding;
235: 
236: 	if (HasRepeats()) {
237: 		uint32_t rep_length = page_hdr.type == PageType::DATA_PAGE
238: 		                          ? block->read<uint32_t>()
239: 		                          : page_hdr.data_page_header_v2.repetition_levels_byte_length;
240: 		block->available(rep_length);
241: 		repeated_decoder =
242: 		    make_unique<RleBpDecoder>((const uint8_t *)block->ptr, rep_length, ComputeBitWidth(max_repeat));
243: 		block->inc(rep_length);
244: 	}
245: 
246: 	if (HasDefines()) {
247: 		uint32_t def_length = page_hdr.type == PageType::DATA_PAGE
248: 		                          ? block->read<uint32_t>()
249: 		                          : page_hdr.data_page_header_v2.definition_levels_byte_length;
250: 		block->available(def_length);
251: 		defined_decoder =
252: 		    make_unique<RleBpDecoder>((const uint8_t *)block->ptr, def_length, ComputeBitWidth(max_define));
253: 		block->inc(def_length);
254: 	}
255: 
256: 	switch (page_encoding) {
257: 	case Encoding::RLE_DICTIONARY:
258: 	case Encoding::PLAIN_DICTIONARY: {
259: 		// TODO there seems to be some confusion whether this is in the bytes for v2
260: 		// where is it otherwise??
261: 		auto dict_width = block->read<uint8_t>();
262: 		// TODO somehow dict_width can be 0 ?
263: 		dict_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, block->len, dict_width);
264: 		block->inc(block->len);
265: 		break;
266: 	}
267: 	case Encoding::PLAIN:
268: 		// nothing to do here, will be read directly below
269: 		break;
270: 
271: 	default:
272: 		throw std::runtime_error("Unsupported page encoding");
273: 	}
274: }
275: 
276: idx_t ColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
277:                          Vector &result) {
278: 	// we need to reset the location because multiple column readers share the same protocol
279: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
280: 	trans.SetLocation(chunk_read_offset);
281: 
282: 	idx_t result_offset = 0;
283: 	auto to_read = num_values;
284: 
285: 	while (to_read > 0) {
286: 		while (page_rows_available == 0) {
287: 			PrepareRead(filter);
288: 		}
289: 
290: 		D_ASSERT(block);
291: 		auto read_now = MinValue<idx_t>(to_read, page_rows_available);
292: 
293: 		D_ASSERT(read_now <= STANDARD_VECTOR_SIZE);
294: 
295: 		if (HasRepeats()) {
296: 			D_ASSERT(repeated_decoder);
297: 			repeated_decoder->GetBatch<uint8_t>((char *)repeat_out + result_offset, read_now);
298: 		}
299: 
300: 		if (HasDefines()) {
301: 			D_ASSERT(defined_decoder);
302: 			defined_decoder->GetBatch<uint8_t>((char *)define_out + result_offset, read_now);
303: 		}
304: 
305: 		if (dict_decoder) {
306: 			// we need the null count because the offsets and plain values have no entries for nulls
307: 			idx_t null_count = 0;
308: 			if (HasDefines()) {
309: 				for (idx_t i = 0; i < read_now; i++) {
310: 					if (define_out[i + result_offset] != max_define) {
311: 						null_count++;
312: 					}
313: 				}
314: 			}
315: 
316: 			offset_buffer.resize(reader.allocator, sizeof(uint32_t) * (read_now - null_count));
317: 			dict_decoder->GetBatch<uint32_t>(offset_buffer.ptr, read_now - null_count);
318: 			DictReference(result);
319: 			Offsets((uint32_t *)offset_buffer.ptr, define_out, read_now, filter, result_offset, result);
320: 		} else {
321: 			PlainReference(block, result);
322: 			Plain(block, define_out, read_now, filter, result_offset, result);
323: 		}
324: 
325: 		result_offset += read_now;
326: 		page_rows_available -= read_now;
327: 		to_read -= read_now;
328: 	}
329: 	group_rows_available -= num_values;
330: 	chunk_read_offset = trans.GetLocation();
331: 
332: 	return num_values;
333: }
334: 
335: void ColumnReader::Skip(idx_t num_values) {
336: 	dummy_define.zero();
337: 	dummy_repeat.zero();
338: 
339: 	// TODO this can be optimized, for example we dont actually have to bitunpack offsets
340: 	auto values_read =
341: 	    Read(num_values, none_filter, (uint8_t *)dummy_define.ptr, (uint8_t *)dummy_repeat.ptr, dummy_result);
342: 	if (values_read != num_values) {
343: 		throw std::runtime_error("Row count mismatch when skipping rows");
344: 	}
345: }
346: 
347: void StringColumnReader::VerifyString(const char *str_data, idx_t str_len) {
348: 	if (Type() != LogicalTypeId::VARCHAR) {
349: 		return;
350: 	}
351: 	// verify if a string is actually UTF8, and if there are no null bytes in the middle of the string
352: 	// technically Parquet should guarantee this, but reality is often disappointing
353: 	auto utf_type = Utf8Proc::Analyze(str_data, str_len);
354: 	if (utf_type == UnicodeType::INVALID) {
355: 		throw InternalException("Invalid string encoding found in Parquet file: value is not valid UTF8!");
356: 	}
357: }
358: 
359: void StringColumnReader::Dictionary(shared_ptr<ByteBuffer> data, idx_t num_entries) {
360: 	dict = move(data);
361: 	dict_strings = unique_ptr<string_t[]>(new string_t[num_entries]);
362: 	for (idx_t dict_idx = 0; dict_idx < num_entries; dict_idx++) {
363: 		uint32_t str_len = dict->read<uint32_t>();
364: 		dict->available(str_len);
365: 
366: 		VerifyString(dict->ptr, str_len);
367: 		dict_strings[dict_idx] = string_t(dict->ptr, str_len);
368: 		dict->inc(str_len);
369: 	}
370: }
371: 
372: class ParquetStringVectorBuffer : public VectorBuffer {
373: public:
374: 	explicit ParquetStringVectorBuffer(shared_ptr<ByteBuffer> buffer_p)
375: 	    : VectorBuffer(VectorBufferType::OPAQUE_BUFFER), buffer(move(buffer_p)) {
376: 	}
377: 
378: private:
379: 	shared_ptr<ByteBuffer> buffer;
380: };
381: 
382: void StringColumnReader::DictReference(Vector &result) {
383: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(dict));
384: }
385: void StringColumnReader::PlainReference(shared_ptr<ByteBuffer> plain_data, Vector &result) {
386: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(move(plain_data)));
387: }
388: 
389: string_t StringParquetValueConversion::DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
390: 	auto &dict_strings = ((StringColumnReader &)reader).dict_strings;
391: 	return dict_strings[offset];
392: }
393: 
394: string_t StringParquetValueConversion::PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
395: 	auto &scr = ((StringColumnReader &)reader);
396: 	uint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;
397: 	plain_data.available(str_len);
398: 	((StringColumnReader &)reader).VerifyString(plain_data.ptr, str_len);
399: 	auto ret_str = string_t(plain_data.ptr, str_len);
400: 	plain_data.inc(str_len);
401: 	return ret_str;
402: }
403: 
404: void StringParquetValueConversion::PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
405: 	auto &scr = ((StringColumnReader &)reader);
406: 	uint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;
407: 	plain_data.available(str_len);
408: 	plain_data.inc(str_len);
409: }
410: 
411: idx_t ListColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
412:                              Vector &result_out) {
413: 	idx_t result_offset = 0;
414: 	auto result_ptr = FlatVector::GetData<list_entry_t>(result_out);
415: 
416: 	child_result.Reset();
417: 	while (result_offset < num_values) {
418: 		auto child_req_num_values = MinValue<idx_t>(STANDARD_VECTOR_SIZE, child_column_reader->GroupRowsAvailable());
419: 		if (child_req_num_values == 0) {
420: 			break;
421: 		}
422: 
423: 		child_defines.zero();
424: 		child_repeats.zero();
425: 
426: 		idx_t child_actual_num_values = 0;
427: 
428: 		if (overflow_child_count == 0) {
429: 			child_actual_num_values = child_column_reader->Read(child_req_num_values, child_filter, child_defines_ptr,
430: 			                                                    child_repeats_ptr, child_result.data[0]);
431: 		} else {
432: 			child_actual_num_values = overflow_child_count;
433: 			overflow_child_count = 0;
434: 			child_result.data[0].Reference(child_result.data[1]);
435: 		}
436: 
437: 		child_result.data[0].Verify(child_actual_num_values);
438: 		idx_t current_chunk_offset = ListVector::GetListSize(result_out);
439: 		ListVector::Append(result_out, child_result.data[0], child_actual_num_values);
440: 
441: 		// hard-won piece of code this, modify at your own risk
442: 		// the intuition is that we have to only collapse values into lists that are repeated *on this level*
443: 		// the rest is pretty much handed up as-is as a single-valued list or NULL
444: 		idx_t child_idx;
445: 		for (child_idx = 0; child_idx < child_actual_num_values; child_idx++) {
446: 			if (child_repeats_ptr[child_idx] == max_repeat) { // value repeats on this level, append
447: 				D_ASSERT(result_offset > 0);
448: 				result_ptr[result_offset - 1].length++;
449: 				continue;
450: 			}
451: 			if (result_offset >= num_values) { // we ran out of output space
452: 				break;
453: 			}
454: 			if (child_defines_ptr[child_idx] >= max_define) {
455: 				// value has been defined down the stack, hence its NOT NULL
456: 				result_ptr[result_offset].offset = child_idx + current_chunk_offset;
457: 				result_ptr[result_offset].length = 1;
458: 			} else {
459: 				// value is NULL somewhere up the stack
460: 				FlatVector::SetNull(result_out, result_offset, true);
461: 				result_ptr[result_offset].offset = 0;
462: 				result_ptr[result_offset].length = 0;
463: 			}
464: 
465: 			repeat_out[result_offset] = child_repeats_ptr[child_idx];
466: 			define_out[result_offset] = child_defines_ptr[child_idx];
467: 
468: 			result_offset++;
469: 		}
470: 
471: 		// we have read more values from the child reader than we can fit into the result for this read
472: 		// we have to pass everything from child_idx to child_actual_num_values into the next call
473: 		if (child_idx < child_actual_num_values && result_offset == num_values) {
474: 			child_result.data[1].Slice(child_result.data[0], child_idx);
475: 			overflow_child_count = child_actual_num_values - child_idx;
476: 			child_result.data[1].Verify(overflow_child_count);
477: 
478: 			// move values in the child repeats and defines *backward* by child_idx
479: 			for (idx_t repdef_idx = 0; repdef_idx < overflow_child_count; repdef_idx++) {
480: 				child_defines_ptr[repdef_idx] = child_defines_ptr[child_idx + repdef_idx];
481: 				child_repeats_ptr[repdef_idx] = child_repeats_ptr[child_idx + repdef_idx];
482: 			}
483: 		}
484: 	}
485: 	result_out.Verify(result_offset);
486: 	return result_offset;
487: }
488: 
489: ListColumnReader::ListColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
490:                                    idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p,
491:                                    unique_ptr<ColumnReader> child_column_reader_p)
492:     : ColumnReader(reader, move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p),
493:       child_column_reader(move(child_column_reader_p)), overflow_child_count(0) {
494: 
495: 	child_defines.resize(reader.allocator, STANDARD_VECTOR_SIZE);
496: 	child_repeats.resize(reader.allocator, STANDARD_VECTOR_SIZE);
497: 	child_defines_ptr = (uint8_t *)child_defines.ptr;
498: 	child_repeats_ptr = (uint8_t *)child_repeats.ptr;
499: 
500: 	auto child_type = ListType::GetChildType(Type());
501: 
502: 	vector<LogicalType> child_result_types;
503: 	child_result_types.push_back(child_type);
504: 	child_result_types.push_back(child_type);
505: 	child_result.Initialize(child_result_types);
506: 
507: 	child_filter.set();
508: }
509: 
510: } // namespace duckdb
[end of extension/parquet/column_reader.cpp]
[start of extension/parquet/include/string_column_reader.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // string_column_reader.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "column_reader.hpp"
12: 
13: namespace duckdb {
14: 
15: struct StringParquetValueConversion {
16: 	static string_t DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader);
17: 
18: 	static string_t PlainRead(ByteBuffer &plain_data, ColumnReader &reader);
19: 
20: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader);
21: };
22: 
23: class StringColumnReader : public TemplatedColumnReader<string_t, StringParquetValueConversion> {
24: public:
25: 	StringColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t schema_idx_p,
26: 	                   idx_t max_define_p, idx_t max_repeat_p)
27: 	    : TemplatedColumnReader<string_t, StringParquetValueConversion>(reader, move(type_p), schema_p, schema_idx_p,
28: 	                                                                    max_define_p, max_repeat_p) {
29: 		fixed_width_string_length = 0;
30: 		if (schema_p.type == Type::FIXED_LEN_BYTE_ARRAY) {
31: 			D_ASSERT(schema_p.__isset.type_length);
32: 			fixed_width_string_length = schema_p.type_length;
33: 		}
34: 	};
35: 
36: 	void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) override;
37: 
38: 	unique_ptr<string_t[]> dict_strings;
39: 	void VerifyString(const char *str_data, idx_t str_len);
40: 	idx_t fixed_width_string_length;
41: 
42: protected:
43: 	void DictReference(Vector &result) override;
44: 	void PlainReference(shared_ptr<ByteBuffer> plain_data, Vector &result) override;
45: };
46: 
47: } // namespace duckdb
[end of extension/parquet/include/string_column_reader.hpp]
[start of src/main/materialized_query_result.cpp]
1: #include "duckdb/main/materialized_query_result.hpp"
2: #include "duckdb/common/to_string.hpp"
3: 
4: namespace duckdb {
5: 
6: MaterializedQueryResult::MaterializedQueryResult(StatementType statement_type)
7:     : QueryResult(QueryResultType::MATERIALIZED_RESULT, statement_type) {
8: }
9: 
10: MaterializedQueryResult::MaterializedQueryResult(StatementType statement_type, vector<LogicalType> types,
11:                                                  vector<string> names)
12:     : QueryResult(QueryResultType::MATERIALIZED_RESULT, statement_type, move(types), move(names)) {
13: }
14: 
15: MaterializedQueryResult::MaterializedQueryResult(string error)
16:     : QueryResult(QueryResultType::MATERIALIZED_RESULT, move(error)) {
17: }
18: 
19: Value MaterializedQueryResult::GetValue(idx_t column, idx_t index) {
20: 	auto &data = collection.GetChunkForRow(index).data[column];
21: 	auto offset_in_chunk = index % STANDARD_VECTOR_SIZE;
22: 	return data.GetValue(offset_in_chunk);
23: }
24: 
25: string MaterializedQueryResult::ToString() {
26: 	string result;
27: 	if (success) {
28: 		result = HeaderToString();
29: 		result += "[ Rows: " + to_string(collection.Count()) + "]\n";
30: 		for (idx_t j = 0; j < collection.Count(); j++) {
31: 			for (idx_t i = 0; i < collection.ColumnCount(); i++) {
32: 				auto val = collection.GetValue(i, j);
33: 				result += val.is_null ? "NULL" : val.ToString();
34: 				result += "\t";
35: 			}
36: 			result += "\n";
37: 		}
38: 		result += "\n";
39: 	} else {
40: 		result = error + "\n";
41: 	}
42: 	return result;
43: }
44: 
45: unique_ptr<DataChunk> MaterializedQueryResult::Fetch() {
46: 	return FetchRaw();
47: }
48: 
49: unique_ptr<DataChunk> MaterializedQueryResult::FetchRaw() {
50: 	if (!success) {
51: 		throw InvalidInputException("Attempting to fetch from an unsuccessful query result");
52: 	}
53: 	return collection.Fetch();
54: }
55: 
56: } // namespace duckdb
[end of src/main/materialized_query_result.cpp]
[start of src/main/stream_query_result.cpp]
1: #include "duckdb/main/stream_query_result.hpp"
2: 
3: #include "duckdb/main/client_context.hpp"
4: #include "duckdb/main/materialized_query_result.hpp"
5: 
6: namespace duckdb {
7: 
8: StreamQueryResult::StreamQueryResult(StatementType statement_type, shared_ptr<ClientContext> context,
9:                                      vector<LogicalType> types, vector<string> names,
10:                                      shared_ptr<PreparedStatementData> prepared)
11:     : QueryResult(QueryResultType::STREAM_RESULT, statement_type, move(types), move(names)), is_open(true),
12:       context(move(context)), prepared(move(prepared)) {
13: }
14: 
15: StreamQueryResult::~StreamQueryResult() {
16: 	Close();
17: }
18: 
19: string StreamQueryResult::ToString() {
20: 	string result;
21: 	if (success) {
22: 		result = HeaderToString();
23: 		result += "[[STREAM RESULT]]";
24: 	} else {
25: 		result = error + "\n";
26: 	}
27: 	return result;
28: }
29: 
30: unique_ptr<DataChunk> StreamQueryResult::FetchRaw() {
31: 	if (!success || !is_open) {
32: 		throw InvalidInputException("Attempting to fetch from an unsuccessful or closed streaming query result");
33: 	}
34: 	auto chunk = context->Fetch();
35: 	if (!chunk || chunk->ColumnCount() == 0 || chunk->size() == 0) {
36: 		Close();
37: 		return nullptr;
38: 	}
39: 	return chunk;
40: }
41: 
42: unique_ptr<MaterializedQueryResult> StreamQueryResult::Materialize() {
43: 	if (!success) {
44: 		return make_unique<MaterializedQueryResult>(error);
45: 	}
46: 	auto result = make_unique<MaterializedQueryResult>(statement_type, types, names);
47: 	while (true) {
48: 		auto chunk = Fetch();
49: 		if (!chunk || chunk->size() == 0) {
50: 			return result;
51: 		}
52: 		result->collection.Append(*chunk);
53: 	}
54: 	return result;
55: }
56: 
57: void StreamQueryResult::Close() {
58: 	if (!is_open) {
59: 		return;
60: 	}
61: 	is_open = false;
62: 	context->Cleanup();
63: }
64: 
65: } // namespace duckdb
[end of src/main/stream_query_result.cpp]
[start of third_party/utf8proc/include/utf8proc_wrapper.hpp]
1: #pragma once
2: 
3: #include <string>
4: #include <cassert>
5: #include <cstring>
6: 
7: namespace duckdb {
8: 
9: enum class UnicodeType {INVALID, ASCII, UNICODE};
10: 
11: 
12: class Utf8Proc {
13: public:
14: 	//! Distinguishes ASCII, Valid UTF8 and Invalid UTF8 strings
15: 	static UnicodeType Analyze(const char *s, size_t len);
16: 	//! Performs UTF NFC normalization of string, return value needs to be free'd
17: 	static char* Normalize(const char* s, size_t len);
18: 	//! Returns whether or not the UTF8 string is valid
19: 	static bool IsValid(const char *s, size_t len);
20: 	//! Returns the position (in bytes) of the next grapheme cluster
21: 	static size_t NextGraphemeCluster(const char *s, size_t len, size_t pos);
22: 	//! Returns the position (in bytes) of the previous grapheme cluster
23: 	static size_t PreviousGraphemeCluster(const char *s, size_t len, size_t pos);
24: 
25: 	//! Transform a codepoint to utf8 and writes it to "c", sets "sz" to the size of the codepoint
26: 	static bool CodepointToUtf8(int cp, int &sz, char *c);
27: 	//! Returns the codepoint length in bytes when encoded in UTF8
28: 	static int CodepointLength(int cp);
29: 	//! Transform a UTF8 string to a codepoint; returns the codepoint and writes the length of the codepoint (in UTF8) to sz
30: 	static int32_t UTF8ToCodepoint(const char *c, int &sz);
31:     static size_t RenderWidth(const char *s, size_t len, size_t pos);
32: 
33: };
34: 
35: }
[end of third_party/utf8proc/include/utf8proc_wrapper.hpp]
[start of third_party/utf8proc/utf8proc_wrapper.cpp]
1: #include "utf8proc_wrapper.hpp"
2: #include "utf8proc.hpp"
3: 
4: using namespace std;
5: 
6: namespace duckdb {
7: 
8: // This function efficiently checks if a string is valid UTF8.
9: // It was originally written by Sjoerd Mullender.
10: 
11: // Here is the table that makes it work:
12: 
13: // B 		= Number of Bytes in UTF8 encoding
14: // C_MIN 	= First Unicode code point
15: // C_MAX 	= Last Unicode code point
16: // B1 		= First Byte Prefix
17: 
18: // 	B	C_MIN		C_MAX		B1
19: //	1	U+000000	U+00007F		0xxxxxxx
20: //	2	U+000080	U+0007FF		110xxxxx
21: //	3	U+000800	U+00FFFF		1110xxxx
22: //	4	U+010000	U+10FFFF		11110xxx
23: 
24: UnicodeType Utf8Proc::Analyze(const char *s, size_t len) {
25: 	UnicodeType type = UnicodeType::ASCII;
26: 	char c;
27: 	for (size_t i = 0; i < len; i++) {
28: 		c = s[i];
29: 		if (c == '\0') {
30: 			return UnicodeType::INVALID;
31: 		}
32: 		// 1 Byte / ASCII
33: 		if ((c & 0x80) == 0)
34: 			continue;
35: 		type = UnicodeType::UNICODE;
36: 		if ((s[++i] & 0xC0) != 0x80)
37: 			return UnicodeType::INVALID;
38: 		if ((c & 0xE0) == 0xC0)
39: 			continue;
40: 		if ((s[++i] & 0xC0) != 0x80)
41: 			return UnicodeType::INVALID;
42: 		if ((c & 0xF0) == 0xE0)
43: 			continue;
44: 		if ((s[++i] & 0xC0) != 0x80)
45: 			return UnicodeType::INVALID;
46: 		if ((c & 0xF8) == 0xF0)
47: 			continue;
48: 		return UnicodeType::INVALID;
49: 	}
50: 
51: 	return type;
52: }
53: 
54: 
55: char* Utf8Proc::Normalize(const char *s, size_t len) {
56: 	assert(s);
57: 	assert(Utf8Proc::Analyze(s, len) != UnicodeType::INVALID);
58: 	return (char*) utf8proc_NFC((const utf8proc_uint8_t*) s, len);
59: }
60: 
61: bool Utf8Proc::IsValid(const char *s, size_t len) {
62: 	return Utf8Proc::Analyze(s, len) != UnicodeType::INVALID;
63: }
64: 
65: size_t Utf8Proc::NextGraphemeCluster(const char *s, size_t len, size_t cpos) {
66: 	return utf8proc_next_grapheme(s, len, cpos);
67: }
68: 
69: size_t Utf8Proc::PreviousGraphemeCluster(const char *s, size_t len, size_t cpos) {
70: 	if (!Utf8Proc::IsValid(s, len)) {
71: 		return cpos - 1;
72: 	}
73: 	size_t current_pos = 0;
74: 	while(true) {
75: 		size_t new_pos = NextGraphemeCluster(s, len, current_pos);
76: 		if (new_pos <= current_pos || new_pos >= cpos) {
77: 			return current_pos;
78: 		}
79: 		current_pos = new_pos;
80: 	}
81: }
82: 
83: bool Utf8Proc::CodepointToUtf8(int cp, int &sz, char *c) {
84: 	return utf8proc_codepoint_to_utf8(cp, sz, c);
85: }
86: 
87: int Utf8Proc::CodepointLength(int cp) {
88: 	return utf8proc_codepoint_length(cp);
89: }
90: 
91: int32_t Utf8Proc::UTF8ToCodepoint(const char *c, int &sz) {
92: 	return utf8proc_codepoint(c, sz);
93: }
94: 
95: size_t Utf8Proc::RenderWidth(const char *s, size_t len, size_t pos) {
96:     int sz;
97:     auto codepoint = duckdb::utf8proc_codepoint(s + pos, sz);
98:     auto properties = duckdb::utf8proc_get_property(codepoint);
99:     return properties->charwidth;
100: }
101: 
102: }
[end of third_party/utf8proc/utf8proc_wrapper.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: