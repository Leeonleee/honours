{
  "repo": "duckdb/duckdb",
  "pull_number": 1942,
  "instance_id": "duckdb__duckdb-1942",
  "issue_numbers": [
    "1932"
  ],
  "base_commit": "8d21bcf45697e36b24a0cd35bfd460daef348576",
  "patch": "diff --git a/extension/parquet/column_reader.cpp b/extension/parquet/column_reader.cpp\nindex 02eaa3e8badb..9e4b76f94d6b 100644\n--- a/extension/parquet/column_reader.cpp\n+++ b/extension/parquet/column_reader.cpp\n@@ -18,6 +18,7 @@\n \n #include \"duckdb.hpp\"\n #ifndef DUCKDB_AMALGAMATION\n+#include \"duckdb/common/types/blob.hpp\"\n #include \"duckdb/common/types/chunk_collection.hpp\"\n #endif\n \n@@ -344,16 +345,24 @@ void ColumnReader::Skip(idx_t num_values) {\n \t}\n }\n \n-void StringColumnReader::VerifyString(const char *str_data, idx_t str_len) {\n+uint32_t StringColumnReader::VerifyString(const char *str_data, uint32_t str_len) {\n \tif (Type() != LogicalTypeId::VARCHAR) {\n-\t\treturn;\n+\t\treturn str_len;\n \t}\n \t// verify if a string is actually UTF8, and if there are no null bytes in the middle of the string\n \t// technically Parquet should guarantee this, but reality is often disappointing\n-\tauto utf_type = Utf8Proc::Analyze(str_data, str_len);\n+\tUnicodeInvalidReason reason;\n+\tsize_t pos;\n+\tauto utf_type = Utf8Proc::Analyze(str_data, str_len, &reason, &pos);\n \tif (utf_type == UnicodeType::INVALID) {\n-\t\tthrow InternalException(\"Invalid string encoding found in Parquet file: value is not valid UTF8!\");\n+\t\tif (reason == UnicodeInvalidReason::NULL_BYTE) {\n+\t\t\t// for null bytes we just truncate the string\n+\t\t\treturn pos;\n+\t\t}\n+\t\tthrow InvalidInputException(\"Invalid string encoding found in Parquet file: value \\\"\" +\n+\t\t                            Blob::ToString(string_t(str_data, str_len)) + \"\\\" is not valid UTF8!\");\n \t}\n+\treturn str_len;\n }\n \n void StringColumnReader::Dictionary(shared_ptr<ByteBuffer> data, idx_t num_entries) {\n@@ -363,8 +372,8 @@ void StringColumnReader::Dictionary(shared_ptr<ByteBuffer> data, idx_t num_entri\n \t\tuint32_t str_len = dict->read<uint32_t>();\n \t\tdict->available(str_len);\n \n-\t\tVerifyString(dict->ptr, str_len);\n-\t\tdict_strings[dict_idx] = string_t(dict->ptr, str_len);\n+\t\tauto actual_str_len = VerifyString(dict->ptr, str_len);\n+\t\tdict_strings[dict_idx] = string_t(dict->ptr, actual_str_len);\n \t\tdict->inc(str_len);\n \t}\n }\n@@ -395,8 +404,8 @@ string_t StringParquetValueConversion::PlainRead(ByteBuffer &plain_data, ColumnR\n \tauto &scr = ((StringColumnReader &)reader);\n \tuint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;\n \tplain_data.available(str_len);\n-\t((StringColumnReader &)reader).VerifyString(plain_data.ptr, str_len);\n-\tauto ret_str = string_t(plain_data.ptr, str_len);\n+\tauto actual_str_len = ((StringColumnReader &)reader).VerifyString(plain_data.ptr, str_len);\n+\tauto ret_str = string_t(plain_data.ptr, actual_str_len);\n \tplain_data.inc(str_len);\n \treturn ret_str;\n }\ndiff --git a/extension/parquet/include/string_column_reader.hpp b/extension/parquet/include/string_column_reader.hpp\nindex d143fa4cd28a..7c53fd6b272c 100644\n--- a/extension/parquet/include/string_column_reader.hpp\n+++ b/extension/parquet/include/string_column_reader.hpp\n@@ -36,7 +36,7 @@ class StringColumnReader : public TemplatedColumnReader<string_t, StringParquetV\n \tvoid Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) override;\n \n \tunique_ptr<string_t[]> dict_strings;\n-\tvoid VerifyString(const char *str_data, idx_t str_len);\n+\tuint32_t VerifyString(const char *str_data, uint32_t str_len);\n \tidx_t fixed_width_string_length;\n \n protected:\ndiff --git a/src/main/materialized_query_result.cpp b/src/main/materialized_query_result.cpp\nindex 181172a0df82..fe74d24f6b26 100644\n--- a/src/main/materialized_query_result.cpp\n+++ b/src/main/materialized_query_result.cpp\n@@ -48,7 +48,7 @@ unique_ptr<DataChunk> MaterializedQueryResult::Fetch() {\n \n unique_ptr<DataChunk> MaterializedQueryResult::FetchRaw() {\n \tif (!success) {\n-\t\tthrow InvalidInputException(\"Attempting to fetch from an unsuccessful query result\");\n+\t\tthrow InvalidInputException(\"Attempting to fetch from an unsuccessful query result\\nError: %s\", error);\n \t}\n \treturn collection.Fetch();\n }\ndiff --git a/src/main/stream_query_result.cpp b/src/main/stream_query_result.cpp\nindex 24dbb453f6f1..7241da97d75a 100644\n--- a/src/main/stream_query_result.cpp\n+++ b/src/main/stream_query_result.cpp\n@@ -29,7 +29,8 @@ string StreamQueryResult::ToString() {\n \n unique_ptr<DataChunk> StreamQueryResult::FetchRaw() {\n \tif (!success || !is_open) {\n-\t\tthrow InvalidInputException(\"Attempting to fetch from an unsuccessful or closed streaming query result\");\n+\t\tthrow InvalidInputException(\n+\t\t    \"Attempting to fetch from an unsuccessful or closed streaming query result\\nError: %s\", error);\n \t}\n \tauto chunk = context->Fetch();\n \tif (!chunk || chunk->ColumnCount() == 0 || chunk->size() == 0) {\ndiff --git a/third_party/utf8proc/include/utf8proc_wrapper.hpp b/third_party/utf8proc/include/utf8proc_wrapper.hpp\nindex 94e0da5e1728..0190476cbf42 100644\n--- a/third_party/utf8proc/include/utf8proc_wrapper.hpp\n+++ b/third_party/utf8proc/include/utf8proc_wrapper.hpp\n@@ -6,13 +6,13 @@\n \n namespace duckdb {\n \n-enum class UnicodeType {INVALID, ASCII, UNICODE};\n-\n+enum class UnicodeType { INVALID, ASCII, UNICODE };\n+enum class UnicodeInvalidReason { BYTE_MISMATCH, NULL_BYTE };\n \n class Utf8Proc {\n public:\n \t//! Distinguishes ASCII, Valid UTF8 and Invalid UTF8 strings\n-\tstatic UnicodeType Analyze(const char *s, size_t len);\n+\tstatic UnicodeType Analyze(const char *s, size_t len, UnicodeInvalidReason *invalid_reason = nullptr, size_t *invalid_pos = nullptr);\n \t//! Performs UTF NFC normalization of string, return value needs to be free'd\n \tstatic char* Normalize(const char* s, size_t len);\n \t//! Returns whether or not the UTF8 string is valid\n@@ -28,7 +28,7 @@ class Utf8Proc {\n \tstatic int CodepointLength(int cp);\n \t//! Transform a UTF8 string to a codepoint; returns the codepoint and writes the length of the codepoint (in UTF8) to sz\n \tstatic int32_t UTF8ToCodepoint(const char *c, int &sz);\n-    static size_t RenderWidth(const char *s, size_t len, size_t pos);\n+\tstatic size_t RenderWidth(const char *s, size_t len, size_t pos);\n \n };\n \ndiff --git a/third_party/utf8proc/utf8proc_wrapper.cpp b/third_party/utf8proc/utf8proc_wrapper.cpp\nindex bfb41bcc935d..3dde822dfe09 100644\n--- a/third_party/utf8proc/utf8proc_wrapper.cpp\n+++ b/third_party/utf8proc/utf8proc_wrapper.cpp\n@@ -21,30 +21,51 @@ namespace duckdb {\n //\t3\tU+000800\tU+00FFFF\t\t1110xxxx\n //\t4\tU+010000\tU+10FFFF\t\t11110xxx\n \n-UnicodeType Utf8Proc::Analyze(const char *s, size_t len) {\n+static void AssignInvalidUTF8Reason(UnicodeInvalidReason *invalid_reason, size_t *invalid_pos, size_t pos, UnicodeInvalidReason reason) {\n+\tif (invalid_reason) {\n+\t\t*invalid_reason = reason;\n+\t}\n+\tif (invalid_pos) {\n+\t\t*invalid_pos = pos;\n+\t}\n+}\n+\n+UnicodeType Utf8Proc::Analyze(const char *s, size_t len, UnicodeInvalidReason *invalid_reason, size_t *invalid_pos) {\n \tUnicodeType type = UnicodeType::ASCII;\n \tchar c;\n \tfor (size_t i = 0; i < len; i++) {\n \t\tc = s[i];\n \t\tif (c == '\\0') {\n+\t\t\tAssignInvalidUTF8Reason(invalid_reason, invalid_pos, i, UnicodeInvalidReason::NULL_BYTE);\n \t\t\treturn UnicodeType::INVALID;\n \t\t}\n \t\t// 1 Byte / ASCII\n-\t\tif ((c & 0x80) == 0)\n+\t\tif ((c & 0x80) == 0) {\n \t\t\tcontinue;\n+\t\t}\n \t\ttype = UnicodeType::UNICODE;\n-\t\tif ((s[++i] & 0xC0) != 0x80)\n+\t\tif ((s[++i] & 0xC0) != 0x80) {\n+\t\t\tAssignInvalidUTF8Reason(invalid_reason, invalid_pos, i, UnicodeInvalidReason::BYTE_MISMATCH);\n \t\t\treturn UnicodeType::INVALID;\n-\t\tif ((c & 0xE0) == 0xC0)\n+\t\t}\n+\t\tif ((c & 0xE0) == 0xC0) {\n \t\t\tcontinue;\n-\t\tif ((s[++i] & 0xC0) != 0x80)\n+\t\t}\n+\t\tif ((s[++i] & 0xC0) != 0x80) {\n+\t\t\tAssignInvalidUTF8Reason(invalid_reason, invalid_pos, i, UnicodeInvalidReason::BYTE_MISMATCH);\n \t\t\treturn UnicodeType::INVALID;\n-\t\tif ((c & 0xF0) == 0xE0)\n+\t\t}\n+\t\tif ((c & 0xF0) == 0xE0) {\n \t\t\tcontinue;\n-\t\tif ((s[++i] & 0xC0) != 0x80)\n+\t\t}\n+\t\tif ((s[++i] & 0xC0) != 0x80) {\n+\t\t\tAssignInvalidUTF8Reason(invalid_reason, invalid_pos, i, UnicodeInvalidReason::BYTE_MISMATCH);\n \t\t\treturn UnicodeType::INVALID;\n-\t\tif ((c & 0xF8) == 0xF0)\n+\t\t}\n+\t\tif ((c & 0xF8) == 0xF0) {\n \t\t\tcontinue;\n+\t\t}\n+\t\tAssignInvalidUTF8Reason(invalid_reason, invalid_pos, i, UnicodeInvalidReason::BYTE_MISMATCH);\n \t\treturn UnicodeType::INVALID;\n \t}\n \n",
  "test_patch": "diff --git a/test/arrow/parquet_test.cpp b/test/arrow/parquet_test.cpp\nindex cb31760d6432..b70f927feeef 100644\n--- a/test/arrow/parquet_test.cpp\n+++ b/test/arrow/parquet_test.cpp\n@@ -152,7 +152,6 @@ TEST_CASE(\"Test Parquet Long Files\", \"[arrow]\") {\n }\n \n TEST_CASE(\"Test Parquet Files\", \"[arrow]\") {\n-\n \tstd::vector<std::string> skip {\"aws2.parquet\"};    //! Not supported by arrow\n \tskip.emplace_back(\"datapage_v2.snappy.parquet\");   //! Not supported by arrow\n \tskip.emplace_back(\"broken-arrow.parquet\");         //! Arrow can't read this\n@@ -160,6 +159,8 @@ TEST_CASE(\"Test Parquet Files\", \"[arrow]\") {\n \tskip.emplace_back(\"fixed.parquet\");                //! Can't roundtrip Fixed-size Binaries\n \tskip.emplace_back(\"leftdate3_192_loop_1.parquet\"); //! This is just crazy slow\n \tskip.emplace_back(\"bug687_nulls.parquet\");         //! This is just crazy slow\n+\tskip.emplace_back(\"nullbyte.parquet\");             //! Null byte in file\n+\tskip.emplace_back(\"nullbyte_multiple.parquet\");    //! Null byte in file\n \n \tduckdb::DuckDB db;\n \tduckdb::Connection conn {db};\ndiff --git a/test/sql/copy/parquet/data/nullbyte.parquet b/test/sql/copy/parquet/data/nullbyte.parquet\nnew file mode 100644\nindex 000000000000..4a478dd01d54\nBinary files /dev/null and b/test/sql/copy/parquet/data/nullbyte.parquet differ\ndiff --git a/test/sql/copy/parquet/data/nullbyte_multiple.parquet b/test/sql/copy/parquet/data/nullbyte_multiple.parquet\nnew file mode 100644\nindex 000000000000..6fcf7b42167d\nBinary files /dev/null and b/test/sql/copy/parquet/data/nullbyte_multiple.parquet differ\ndiff --git a/test/sql/copy/parquet/parquet_nullbyte.test b/test/sql/copy/parquet/parquet_nullbyte.test\nnew file mode 100644\nindex 000000000000..4f84d0a899f0\n--- /dev/null\n+++ b/test/sql/copy/parquet/parquet_nullbyte.test\n@@ -0,0 +1,20 @@\n+# name: test/sql/copy/parquet/parquet_nullbyte.test\n+# description: Test reading parquet files with null bytes in strings\n+# group: [parquet]\n+\n+require parquet\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+query II\n+select * from parquet_scan('test/sql/copy/parquet/data/nullbyte.parquet')\n+----\n+42\thello\n+\n+query II\n+select * from parquet_scan('test/sql/copy/parquet/data/nullbyte_multiple.parquet')\n+----\n+1\thello\n+2\tthis is a long\n+3\tthis string has no null bytes\n",
  "problem_statement": "Invalid string encoding aborts Parquet import\nI'm trying to import a series of parquet files into duckdb.  Unfortunately, the import aborts with the following error:\r\n\r\n```python\r\nIn [51]: cursor.execute(\"INSERT INTO master_email SELECT * FROM 'master_email/*.parquet'\").df()    \r\n ---------------------------------------------------------------------------                                                            RuntimeError                              Traceback (most recent call last)                                                            \r\n<ipython-input-51-90580d0a8a82> in <module>                                                                                           \r\n----> 1 cursor.execute(\"INSERT INTO master_email SELECT * FROM 'master_email/*.parquet'\").df() \r\n                                                                                                                                                                               \r\nRuntimeError: INTERNAL Error: Invalid string encoding found in Parquet file: value is not valid UTF8! \r\n\r\n```\r\n\r\nPandas (via pyarrow), Spark and Presto all read these same parquet files without issue.\r\n\r\nWould be nice to have the option to either ignore invalid rows, or at least include details about the bad record in the error so they can be manually cleaned up.\n",
  "hints_text": "Just in case it is helpful, it seems the following will clean up individual files.  I'm using it as a workaround for the time being.\r\n\r\n```python\r\nIn [22]: cleaned_up = []\r\n...: for fname in path.glob(\"part*\"):\r\n...:     try:\r\n...:         cursor.execute(f\"SELECT count(email) FROM '{fname}'\").df()\r\n...:     except RuntimeError as exc:\r\n...:         print(exc, fname)\r\n...:         df = pd.read_parquet(fname)\r\n...:         df.email = df.email.apply(lambda e: e.encode('utf-8'))\r\n...:         df.to_parquet(fname)\r\n...:         cleaned_up.append(fname)\r\n...:         # Test again to ensure file is clean.\r\n...:         cursor.execute(f\"SELECT count(email) FROM '{fname}'\").df()\r\n\r\n```\nThanks for the report! I think this is related to the [binaryAsString option](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html). In Spark this defaults to false, I believe we always assume this is always true. We should make this an actual option and probably default to false to match Spark\u2019s behavior.\nIf possible (i.e. the file is not private) could you share the file here so we can test/verify this behavior?\nThe file contains PII (email addresses).  I can share a sample of it with you privately.\nCould you send it to my email (m.raasveldt@cwi.nl)? Thanks!\nEmail sent.",
  "created_at": "2021-07-01T10:49:09Z"
}