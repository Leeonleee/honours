{
  "repo": "duckdb/duckdb",
  "pull_number": 9471,
  "instance_id": "duckdb__duckdb-9471",
  "issue_numbers": [
    "9314"
  ],
  "base_commit": "255857492791eef6b8ad71b4bf176e27c269ba52",
  "patch": "diff --git a/src/common/arrow/appender/CMakeLists.txt b/src/common/arrow/appender/CMakeLists.txt\nindex cc98b990417a..f7d91fa84e33 100644\n--- a/src/common/arrow/appender/CMakeLists.txt\n+++ b/src/common/arrow/appender/CMakeLists.txt\n@@ -1,11 +1,5 @@\n-add_library_unity(\n-  duckdb_common_arrow_appender\n-  OBJECT\n-  bool_data.cpp\n-  list_data.cpp\n-  map_data.cpp\n-  struct_data.cpp\n-  union_data.cpp)\n+add_library_unity(duckdb_common_arrow_appender OBJECT bool_data.cpp\n+                  struct_data.cpp union_data.cpp)\n set(ALL_OBJECT_FILES\n     ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:duckdb_common_arrow_appender>\n     PARENT_SCOPE)\ndiff --git a/src/common/arrow/appender/list_data.cpp b/src/common/arrow/appender/list_data.cpp\ndeleted file mode 100644\nindex 50ff8068763f..000000000000\n--- a/src/common/arrow/appender/list_data.cpp\n+++ /dev/null\n@@ -1,78 +0,0 @@\n-#include \"duckdb/common/arrow/arrow_appender.hpp\"\n-#include \"duckdb/common/arrow/appender/list_data.hpp\"\n-\n-namespace duckdb {\n-\n-//===--------------------------------------------------------------------===//\n-// Lists\n-//===--------------------------------------------------------------------===//\n-void ArrowListData::AppendOffsets(ArrowAppendData &append_data, UnifiedVectorFormat &format, idx_t from, idx_t to,\n-                                  vector<sel_t> &child_sel) {\n-\t// resize the offset buffer - the offset buffer holds the offsets into the child array\n-\tidx_t size = to - from;\n-\tappend_data.main_buffer.resize(append_data.main_buffer.size() + sizeof(uint32_t) * (size + 1));\n-\tauto data = UnifiedVectorFormat::GetData<list_entry_t>(format);\n-\tauto offset_data = append_data.main_buffer.GetData<uint32_t>();\n-\tif (append_data.row_count == 0) {\n-\t\t// first entry\n-\t\toffset_data[0] = 0;\n-\t}\n-\t// set up the offsets using the list entries\n-\tauto last_offset = offset_data[append_data.row_count];\n-\tfor (idx_t i = from; i < to; i++) {\n-\t\tauto source_idx = format.sel->get_index(i);\n-\t\tauto offset_idx = append_data.row_count + i + 1 - from;\n-\n-\t\tif (!format.validity.RowIsValid(source_idx)) {\n-\t\t\toffset_data[offset_idx] = last_offset;\n-\t\t\tcontinue;\n-\t\t}\n-\n-\t\t// append the offset data\n-\t\tauto list_length = data[source_idx].length;\n-\t\tlast_offset += list_length;\n-\t\toffset_data[offset_idx] = last_offset;\n-\n-\t\tfor (idx_t k = 0; k < list_length; k++) {\n-\t\t\tchild_sel.push_back(data[source_idx].offset + k);\n-\t\t}\n-\t}\n-}\n-\n-void ArrowListData::Initialize(ArrowAppendData &result, const LogicalType &type, idx_t capacity) {\n-\tauto &child_type = ListType::GetChildType(type);\n-\tresult.main_buffer.reserve((capacity + 1) * sizeof(uint32_t));\n-\tauto child_buffer = ArrowAppender::InitializeChild(child_type, capacity, result.options);\n-\tresult.child_data.push_back(std::move(child_buffer));\n-}\n-\n-void ArrowListData::Append(ArrowAppendData &append_data, Vector &input, idx_t from, idx_t to, idx_t input_size) {\n-\tUnifiedVectorFormat format;\n-\tinput.ToUnifiedFormat(input_size, format);\n-\tidx_t size = to - from;\n-\tvector<sel_t> child_indices;\n-\tAppendValidity(append_data, format, from, to);\n-\tArrowListData::AppendOffsets(append_data, format, from, to, child_indices);\n-\n-\t// append the child vector of the list\n-\tSelectionVector child_sel(child_indices.data());\n-\tauto &child = ListVector::GetEntry(input);\n-\tauto child_size = child_indices.size();\n-\tVector child_copy(child.GetType());\n-\tchild_copy.Slice(child, child_sel, child_size);\n-\tappend_data.child_data[0]->append_vector(*append_data.child_data[0], child_copy, 0, child_size, child_size);\n-\tappend_data.row_count += size;\n-}\n-\n-void ArrowListData::Finalize(ArrowAppendData &append_data, const LogicalType &type, ArrowArray *result) {\n-\tresult->n_buffers = 2;\n-\tresult->buffers[1] = append_data.main_buffer.data();\n-\n-\tauto &child_type = ListType::GetChildType(type);\n-\tArrowAppender::AddChildren(append_data, 1);\n-\tresult->children = append_data.child_pointers.data();\n-\tresult->n_children = 1;\n-\tappend_data.child_arrays[0] = *ArrowAppender::FinalizeChild(child_type, std::move(append_data.child_data[0]));\n-}\n-\n-} // namespace duckdb\ndiff --git a/src/common/arrow/appender/map_data.cpp b/src/common/arrow/appender/map_data.cpp\ndeleted file mode 100644\nindex 3bacf653cc9d..000000000000\n--- a/src/common/arrow/appender/map_data.cpp\n+++ /dev/null\n@@ -1,91 +0,0 @@\n-#include \"duckdb/common/arrow/arrow_appender.hpp\"\n-#include \"duckdb/common/arrow/appender/map_data.hpp\"\n-#include \"duckdb/common/arrow/appender/list_data.hpp\"\n-\n-namespace duckdb {\n-\n-//===--------------------------------------------------------------------===//\n-// Maps\n-//===--------------------------------------------------------------------===//\n-void ArrowMapData::Initialize(ArrowAppendData &result, const LogicalType &type, idx_t capacity) {\n-\t// map types are stored in a (too) clever way\n-\t// the main buffer holds the null values and the offsets\n-\t// then we have a single child, which is a struct of the map_type, and the key_type\n-\tresult.main_buffer.reserve((capacity + 1) * sizeof(uint32_t));\n-\n-\tauto &key_type = MapType::KeyType(type);\n-\tauto &value_type = MapType::ValueType(type);\n-\tauto internal_struct = make_uniq<ArrowAppendData>(result.options);\n-\tinternal_struct->child_data.push_back(ArrowAppender::InitializeChild(key_type, capacity, result.options));\n-\tinternal_struct->child_data.push_back(ArrowAppender::InitializeChild(value_type, capacity, result.options));\n-\n-\tresult.child_data.push_back(std::move(internal_struct));\n-}\n-\n-void ArrowMapData::Append(ArrowAppendData &append_data, Vector &input, idx_t from, idx_t to, idx_t input_size) {\n-\tUnifiedVectorFormat format;\n-\tinput.ToUnifiedFormat(input_size, format);\n-\tidx_t size = to - from;\n-\tAppendValidity(append_data, format, from, to);\n-\tvector<sel_t> child_indices;\n-\tArrowListData::AppendOffsets(append_data, format, from, to, child_indices);\n-\n-\tSelectionVector child_sel(child_indices.data());\n-\tauto &key_vector = MapVector::GetKeys(input);\n-\tauto &value_vector = MapVector::GetValues(input);\n-\tauto list_size = child_indices.size();\n-\n-\tauto &struct_data = *append_data.child_data[0];\n-\tauto &key_data = *struct_data.child_data[0];\n-\tauto &value_data = *struct_data.child_data[1];\n-\n-\tVector key_vector_copy(key_vector.GetType());\n-\tkey_vector_copy.Slice(key_vector, child_sel, list_size);\n-\tVector value_vector_copy(value_vector.GetType());\n-\tvalue_vector_copy.Slice(value_vector, child_sel, list_size);\n-\tkey_data.append_vector(key_data, key_vector_copy, 0, list_size, list_size);\n-\tvalue_data.append_vector(value_data, value_vector_copy, 0, list_size, list_size);\n-\n-\tappend_data.row_count += size;\n-\tstruct_data.row_count += size;\n-}\n-\n-void ArrowMapData::Finalize(ArrowAppendData &append_data, const LogicalType &type, ArrowArray *result) {\n-\t// set up the main map buffer\n-\tD_ASSERT(result);\n-\tresult->n_buffers = 2;\n-\tresult->buffers[1] = append_data.main_buffer.data();\n-\n-\t// the main map buffer has a single child: a struct\n-\tArrowAppender::AddChildren(append_data, 1);\n-\tresult->children = append_data.child_pointers.data();\n-\tresult->n_children = 1;\n-\n-\tauto &struct_data = *append_data.child_data[0];\n-\tauto struct_result = ArrowAppender::FinalizeChild(type, std::move(append_data.child_data[0]));\n-\n-\t// Initialize the struct array data\n-\tconst auto struct_child_count = 2;\n-\tArrowAppender::AddChildren(struct_data, struct_child_count);\n-\tstruct_result->children = struct_data.child_pointers.data();\n-\tstruct_result->n_buffers = 1;\n-\tstruct_result->n_children = struct_child_count;\n-\tstruct_result->length = struct_data.child_data[0]->row_count;\n-\n-\tappend_data.child_arrays[0] = *struct_result;\n-\n-\tD_ASSERT(struct_data.child_data[0]->row_count == struct_data.child_data[1]->row_count);\n-\n-\tauto &key_type = MapType::KeyType(type);\n-\tauto &value_type = MapType::ValueType(type);\n-\tauto key_data = ArrowAppender::FinalizeChild(key_type, std::move(struct_data.child_data[0]));\n-\tstruct_data.child_arrays[0] = *key_data;\n-\tstruct_data.child_arrays[1] = *ArrowAppender::FinalizeChild(value_type, std::move(struct_data.child_data[1]));\n-\n-\t// keys cannot have null values\n-\tif (key_data->null_count > 0) {\n-\t\tthrow std::runtime_error(\"Arrow doesn't accept NULL keys on Maps\");\n-\t}\n-}\n-\n-} // namespace duckdb\ndiff --git a/src/common/arrow/arrow_appender.cpp b/src/common/arrow/arrow_appender.cpp\nindex 10d1e39e35d1..bbca77756832 100644\n--- a/src/common/arrow/arrow_appender.cpp\n+++ b/src/common/arrow/arrow_appender.cpp\n@@ -193,26 +193,26 @@ static void InitializeFunctionPointers(ArrowAppendData &append_data, const Logic\n \t\tif (append_data.options.arrow_offset_size == ArrowOffsetSize::LARGE) {\n \t\t\tInitializeAppenderForType<ArrowVarcharData<string_t>>(append_data);\n \t\t} else {\n-\t\t\tInitializeAppenderForType<ArrowVarcharData<string_t, ArrowVarcharConverter, uint32_t>>(append_data);\n+\t\t\tInitializeAppenderForType<ArrowVarcharData<string_t, ArrowVarcharConverter, int32_t>>(append_data);\n \t\t}\n \t\tbreak;\n \tcase LogicalTypeId::UUID:\n \t\tif (append_data.options.arrow_offset_size == ArrowOffsetSize::LARGE) {\n \t\t\tInitializeAppenderForType<ArrowVarcharData<hugeint_t, ArrowUUIDConverter>>(append_data);\n \t\t} else {\n-\t\t\tInitializeAppenderForType<ArrowVarcharData<hugeint_t, ArrowUUIDConverter, uint32_t>>(append_data);\n+\t\t\tInitializeAppenderForType<ArrowVarcharData<hugeint_t, ArrowUUIDConverter, int32_t>>(append_data);\n \t\t}\n \t\tbreak;\n \tcase LogicalTypeId::ENUM:\n \t\tswitch (type.InternalType()) {\n \t\tcase PhysicalType::UINT8:\n-\t\t\tInitializeAppenderForType<ArrowEnumData<uint8_t>>(append_data);\n+\t\t\tInitializeAppenderForType<ArrowEnumData<int8_t>>(append_data);\n \t\t\tbreak;\n \t\tcase PhysicalType::UINT16:\n-\t\t\tInitializeAppenderForType<ArrowEnumData<uint16_t>>(append_data);\n+\t\t\tInitializeAppenderForType<ArrowEnumData<int16_t>>(append_data);\n \t\t\tbreak;\n \t\tcase PhysicalType::UINT32:\n-\t\t\tInitializeAppenderForType<ArrowEnumData<uint32_t>>(append_data);\n+\t\t\tInitializeAppenderForType<ArrowEnumData<int32_t>>(append_data);\n \t\t\tbreak;\n \t\tdefault:\n \t\t\tthrow InternalException(\"Unsupported internal enum type\");\n@@ -227,11 +227,20 @@ static void InitializeFunctionPointers(ArrowAppendData &append_data, const Logic\n \tcase LogicalTypeId::STRUCT:\n \t\tInitializeAppenderForType<ArrowStructData>(append_data);\n \t\tbreak;\n-\tcase LogicalTypeId::LIST:\n-\t\tInitializeAppenderForType<ArrowListData>(append_data);\n+\tcase LogicalTypeId::LIST: {\n+\t\tif (append_data.options.arrow_offset_size == ArrowOffsetSize::LARGE) {\n+\t\t\tInitializeAppenderForType<ArrowListData<int64_t>>(append_data);\n+\t\t} else {\n+\t\t\tInitializeAppenderForType<ArrowListData<int32_t>>(append_data);\n+\t\t}\n \t\tbreak;\n+\t}\n \tcase LogicalTypeId::MAP:\n-\t\tInitializeAppenderForType<ArrowMapData>(append_data);\n+\t\tif (append_data.options.arrow_offset_size == ArrowOffsetSize::LARGE) {\n+\t\t\tInitializeAppenderForType<ArrowMapData<int64_t>>(append_data);\n+\t\t} else {\n+\t\t\tInitializeAppenderForType<ArrowMapData<int32_t>>(append_data);\n+\t\t}\n \t\tbreak;\n \tdefault:\n \t\tthrow NotImplementedException(\"Unsupported type in DuckDB -> Arrow Conversion: %s\\n\", type.ToString());\ndiff --git a/src/common/arrow/arrow_converter.cpp b/src/common/arrow/arrow_converter.cpp\nindex 0ecc46e0fcca..d57bcc471bcb 100644\n--- a/src/common/arrow/arrow_converter.cpp\n+++ b/src/common/arrow/arrow_converter.cpp\n@@ -187,7 +187,11 @@ void SetArrowFormat(DuckDBArrowSchemaHolder &root_holder, ArrowSchema &child, co\n \t\tbreak;\n \t}\n \tcase LogicalTypeId::LIST: {\n-\t\tchild.format = \"+l\";\n+\t\tif (options.arrow_offset_size == ArrowOffsetSize::LARGE) {\n+\t\t\tchild.format = \"+L\";\n+\t\t} else {\n+\t\t\tchild.format = \"+l\";\n+\t\t}\n \t\tchild.n_children = 1;\n \t\troot_holder.nested_children.emplace_back();\n \t\troot_holder.nested_children.back().resize(1);\ndiff --git a/src/include/duckdb/common/arrow/appender/enum_data.hpp b/src/include/duckdb/common/arrow/appender/enum_data.hpp\nindex ffcf729fb44a..087c622e85ee 100644\n--- a/src/include/duckdb/common/arrow/appender/enum_data.hpp\n+++ b/src/include/duckdb/common/arrow/appender/enum_data.hpp\n@@ -8,14 +8,19 @@ namespace duckdb {\n //===--------------------------------------------------------------------===//\n // Enums\n //===--------------------------------------------------------------------===//\n+\n+// FIXME: support Large offsets (int64_t), this does not currently respect the 'arrow_large_buffer_size' setting\n+\n template <class TGT>\n struct ArrowEnumData : public ArrowScalarBaseData<TGT> {\n \tstatic idx_t GetLength(string_t input) {\n \t\treturn input.GetSize();\n \t}\n+\n \tstatic void WriteData(data_ptr_t target, string_t input) {\n \t\tmemcpy(target, input.GetData(), input.GetSize());\n \t}\n+\n \tstatic void EnumAppendVector(ArrowAppendData &append_data, const Vector &input, idx_t size) {\n \t\tD_ASSERT(input.GetVectorType() == VectorType::FLAT_VECTOR);\n \n@@ -23,9 +28,9 @@ struct ArrowEnumData : public ArrowScalarBaseData<TGT> {\n \t\tResizeValidity(append_data.validity, append_data.row_count + size);\n \n \t\t// resize the offset buffer - the offset buffer holds the offsets into the child array\n-\t\tappend_data.main_buffer.resize(append_data.main_buffer.size() + sizeof(uint32_t) * (size + 1));\n+\t\tappend_data.main_buffer.resize(append_data.main_buffer.size() + sizeof(int32_t) * (size + 1));\n \t\tauto data = FlatVector::GetData<string_t>(input);\n-\t\tauto offset_data = append_data.main_buffer.GetData<uint32_t>();\n+\t\tauto offset_data = append_data.main_buffer.GetData<int32_t>();\n \t\tif (append_data.row_count == 0) {\n \t\t\t// first entry\n \t\t\toffset_data[0] = 0;\n@@ -50,6 +55,7 @@ struct ArrowEnumData : public ArrowScalarBaseData<TGT> {\n \t\t}\n \t\tappend_data.row_count += size;\n \t}\n+\n \tstatic void Initialize(ArrowAppendData &result, const LogicalType &type, idx_t capacity) {\n \t\tresult.main_buffer.reserve(capacity * sizeof(TGT));\n \t\t// construct the enum child data\ndiff --git a/src/include/duckdb/common/arrow/appender/list_data.hpp b/src/include/duckdb/common/arrow/appender/list_data.hpp\nindex 9507ac729d95..534ddef2a368 100644\n--- a/src/include/duckdb/common/arrow/appender/list_data.hpp\n+++ b/src/include/duckdb/common/arrow/appender/list_data.hpp\n@@ -4,15 +4,85 @@\n \n namespace duckdb {\n \n+template <class BUFTYPE = int64_t>\n struct ArrowListData {\n public:\n-\tstatic void Initialize(ArrowAppendData &result, const LogicalType &type, idx_t capacity);\n-\tstatic void Append(ArrowAppendData &append_data, Vector &input, idx_t from, idx_t to, idx_t input_size);\n-\tstatic void Finalize(ArrowAppendData &append_data, const LogicalType &type, ArrowArray *result);\n+\tstatic void Initialize(ArrowAppendData &result, const LogicalType &type, idx_t capacity) {\n+\t\tauto &child_type = ListType::GetChildType(type);\n+\t\tresult.main_buffer.reserve((capacity + 1) * sizeof(BUFTYPE));\n+\t\tauto child_buffer = ArrowAppender::InitializeChild(child_type, capacity, result.options);\n+\t\tresult.child_data.push_back(std::move(child_buffer));\n+\t}\n+\n+\tstatic void Append(ArrowAppendData &append_data, Vector &input, idx_t from, idx_t to, idx_t input_size) {\n+\t\tUnifiedVectorFormat format;\n+\t\tinput.ToUnifiedFormat(input_size, format);\n+\t\tidx_t size = to - from;\n+\t\tvector<sel_t> child_indices;\n+\t\tAppendValidity(append_data, format, from, to);\n+\t\tAppendOffsets(append_data, format, from, to, child_indices);\n+\n+\t\t// append the child vector of the list\n+\t\tSelectionVector child_sel(child_indices.data());\n+\t\tauto &child = ListVector::GetEntry(input);\n+\t\tauto child_size = child_indices.size();\n+\t\tVector child_copy(child.GetType());\n+\t\tchild_copy.Slice(child, child_sel, child_size);\n+\t\tappend_data.child_data[0]->append_vector(*append_data.child_data[0], child_copy, 0, child_size, child_size);\n+\t\tappend_data.row_count += size;\n+\t}\n+\n+\tstatic void Finalize(ArrowAppendData &append_data, const LogicalType &type, ArrowArray *result) {\n+\t\tresult->n_buffers = 2;\n+\t\tresult->buffers[1] = append_data.main_buffer.data();\n+\n+\t\tauto &child_type = ListType::GetChildType(type);\n+\t\tArrowAppender::AddChildren(append_data, 1);\n+\t\tresult->children = append_data.child_pointers.data();\n+\t\tresult->n_children = 1;\n+\t\tappend_data.child_arrays[0] = *ArrowAppender::FinalizeChild(child_type, std::move(append_data.child_data[0]));\n+\t}\n \n public:\n \tstatic void AppendOffsets(ArrowAppendData &append_data, UnifiedVectorFormat &format, idx_t from, idx_t to,\n-\t                          vector<sel_t> &child_sel);\n+\t                          vector<sel_t> &child_sel) {\n+\t\t// resize the offset buffer - the offset buffer holds the offsets into the child array\n+\t\tidx_t size = to - from;\n+\t\tappend_data.main_buffer.resize(append_data.main_buffer.size() + sizeof(BUFTYPE) * (size + 1));\n+\t\tauto data = UnifiedVectorFormat::GetData<list_entry_t>(format);\n+\t\tauto offset_data = append_data.main_buffer.GetData<BUFTYPE>();\n+\t\tif (append_data.row_count == 0) {\n+\t\t\t// first entry\n+\t\t\toffset_data[0] = 0;\n+\t\t}\n+\t\t// set up the offsets using the list entries\n+\t\tauto last_offset = offset_data[append_data.row_count];\n+\t\tfor (idx_t i = from; i < to; i++) {\n+\t\t\tauto source_idx = format.sel->get_index(i);\n+\t\t\tauto offset_idx = append_data.row_count + i + 1 - from;\n+\n+\t\t\tif (!format.validity.RowIsValid(source_idx)) {\n+\t\t\t\toffset_data[offset_idx] = last_offset;\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// append the offset data\n+\t\t\tauto list_length = data[source_idx].length;\n+\t\t\tif (std::is_same<BUFTYPE, int32_t>::value == true &&\n+\t\t\t    (uint64_t)last_offset + list_length > NumericLimits<int32_t>::Maximum()) {\n+\t\t\t\tthrow InvalidInputException(\n+\t\t\t\t    \"Arrow Appender: The maximum combined list offset for regular list buffers is \"\n+\t\t\t\t    \"%u but the offset of %lu exceeds this.\",\n+\t\t\t\t    NumericLimits<int32_t>::Maximum(), last_offset);\n+\t\t\t}\n+\t\t\tlast_offset += list_length;\n+\t\t\toffset_data[offset_idx] = last_offset;\n+\n+\t\t\tfor (idx_t k = 0; k < list_length; k++) {\n+\t\t\t\tchild_sel.push_back(data[source_idx].offset + k);\n+\t\t\t}\n+\t\t}\n+\t}\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/common/arrow/appender/map_data.hpp b/src/include/duckdb/common/arrow/appender/map_data.hpp\nindex 9bb31c2fa006..e881c532a15a 100644\n--- a/src/include/duckdb/common/arrow/appender/map_data.hpp\n+++ b/src/include/duckdb/common/arrow/appender/map_data.hpp\n@@ -2,17 +2,96 @@\n \n #include \"duckdb/common/arrow/arrow_appender.hpp\"\n #include \"duckdb/common/arrow/appender/append_data.hpp\"\n+#include \"duckdb/common/arrow/appender/list_data.hpp\"\n \n namespace duckdb {\n \n //===--------------------------------------------------------------------===//\n // Maps\n //===--------------------------------------------------------------------===//\n+template <class BUFTYPE = int64_t>\n struct ArrowMapData {\n public:\n-\tstatic void Initialize(ArrowAppendData &result, const LogicalType &type, idx_t capacity);\n-\tstatic void Append(ArrowAppendData &append_data, Vector &input, idx_t from, idx_t to, idx_t input_size);\n-\tstatic void Finalize(ArrowAppendData &append_data, const LogicalType &type, ArrowArray *result);\n+\tstatic void Initialize(ArrowAppendData &result, const LogicalType &type, idx_t capacity) {\n+\t\t// map types are stored in a (too) clever way\n+\t\t// the main buffer holds the null values and the offsets\n+\t\t// then we have a single child, which is a struct of the map_type, and the key_type\n+\t\tresult.main_buffer.reserve((capacity + 1) * sizeof(BUFTYPE));\n+\n+\t\tauto &key_type = MapType::KeyType(type);\n+\t\tauto &value_type = MapType::ValueType(type);\n+\t\tauto internal_struct = make_uniq<ArrowAppendData>(result.options);\n+\t\tinternal_struct->child_data.push_back(ArrowAppender::InitializeChild(key_type, capacity, result.options));\n+\t\tinternal_struct->child_data.push_back(ArrowAppender::InitializeChild(value_type, capacity, result.options));\n+\n+\t\tresult.child_data.push_back(std::move(internal_struct));\n+\t}\n+\n+\tstatic void Append(ArrowAppendData &append_data, Vector &input, idx_t from, idx_t to, idx_t input_size) {\n+\t\tUnifiedVectorFormat format;\n+\t\tinput.ToUnifiedFormat(input_size, format);\n+\t\tidx_t size = to - from;\n+\t\tAppendValidity(append_data, format, from, to);\n+\t\tvector<sel_t> child_indices;\n+\t\tArrowListData<BUFTYPE>::AppendOffsets(append_data, format, from, to, child_indices);\n+\n+\t\tSelectionVector child_sel(child_indices.data());\n+\t\tauto &key_vector = MapVector::GetKeys(input);\n+\t\tauto &value_vector = MapVector::GetValues(input);\n+\t\tauto list_size = child_indices.size();\n+\n+\t\tauto &struct_data = *append_data.child_data[0];\n+\t\tauto &key_data = *struct_data.child_data[0];\n+\t\tauto &value_data = *struct_data.child_data[1];\n+\n+\t\tVector key_vector_copy(key_vector.GetType());\n+\t\tkey_vector_copy.Slice(key_vector, child_sel, list_size);\n+\t\tVector value_vector_copy(value_vector.GetType());\n+\t\tvalue_vector_copy.Slice(value_vector, child_sel, list_size);\n+\t\tkey_data.append_vector(key_data, key_vector_copy, 0, list_size, list_size);\n+\t\tvalue_data.append_vector(value_data, value_vector_copy, 0, list_size, list_size);\n+\n+\t\tappend_data.row_count += size;\n+\t\tstruct_data.row_count += size;\n+\t}\n+\n+\tstatic void Finalize(ArrowAppendData &append_data, const LogicalType &type, ArrowArray *result) {\n+\t\t// set up the main map buffer\n+\t\tD_ASSERT(result);\n+\t\tresult->n_buffers = 2;\n+\t\tresult->buffers[1] = append_data.main_buffer.data();\n+\n+\t\t// the main map buffer has a single child: a struct\n+\t\tArrowAppender::AddChildren(append_data, 1);\n+\t\tresult->children = append_data.child_pointers.data();\n+\t\tresult->n_children = 1;\n+\n+\t\tauto &struct_data = *append_data.child_data[0];\n+\t\tauto struct_result = ArrowAppender::FinalizeChild(type, std::move(append_data.child_data[0]));\n+\n+\t\t// Initialize the struct array data\n+\t\tconst auto struct_child_count = 2;\n+\t\tArrowAppender::AddChildren(struct_data, struct_child_count);\n+\t\tstruct_result->children = struct_data.child_pointers.data();\n+\t\tstruct_result->n_buffers = 1;\n+\t\tstruct_result->n_children = struct_child_count;\n+\t\tstruct_result->length = struct_data.child_data[0]->row_count;\n+\n+\t\tappend_data.child_arrays[0] = *struct_result;\n+\n+\t\tD_ASSERT(struct_data.child_data[0]->row_count == struct_data.child_data[1]->row_count);\n+\n+\t\tauto &key_type = MapType::KeyType(type);\n+\t\tauto &value_type = MapType::ValueType(type);\n+\t\tauto key_data = ArrowAppender::FinalizeChild(key_type, std::move(struct_data.child_data[0]));\n+\t\tstruct_data.child_arrays[0] = *key_data;\n+\t\tstruct_data.child_arrays[1] = *ArrowAppender::FinalizeChild(value_type, std::move(struct_data.child_data[1]));\n+\n+\t\t// keys cannot have null values\n+\t\tif (key_data->null_count > 0) {\n+\t\t\tthrow std::runtime_error(\"Arrow doesn't accept NULL keys on Maps\");\n+\t\t}\n+\t}\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/common/arrow/appender/varchar_data.hpp b/src/include/duckdb/common/arrow/appender/varchar_data.hpp\nindex 03984fc77461..fd2a2385dbe0 100644\n--- a/src/include/duckdb/common/arrow/appender/varchar_data.hpp\n+++ b/src/include/duckdb/common/arrow/appender/varchar_data.hpp\n@@ -32,7 +32,7 @@ struct ArrowUUIDConverter {\n \t}\n };\n \n-template <class SRC = string_t, class OP = ArrowVarcharConverter, class BUFTYPE = uint64_t>\n+template <class SRC = string_t, class OP = ArrowVarcharConverter, class BUFTYPE = int64_t>\n struct ArrowVarcharData {\n \tstatic void Initialize(ArrowAppendData &result, const LogicalType &type, idx_t capacity) {\n \t\tresult.main_buffer.reserve((capacity + 1) * sizeof(BUFTYPE));\n@@ -40,7 +40,8 @@ struct ArrowVarcharData {\n \t\tresult.aux_buffer.reserve(capacity);\n \t}\n \n-\tstatic void Append(ArrowAppendData &append_data, Vector &input, idx_t from, idx_t to, idx_t input_size) {\n+\ttemplate <bool LARGE_STRING>\n+\tstatic void AppendTemplated(ArrowAppendData &append_data, Vector &input, idx_t from, idx_t to, idx_t input_size) {\n \t\tidx_t size = to - from;\n \t\tUnifiedVectorFormat format;\n \t\tinput.ToUnifiedFormat(input_size, format);\n@@ -60,13 +61,6 @@ struct ArrowVarcharData {\n \t\t// now append the string data to the auxiliary buffer\n \t\t// the auxiliary buffer's length depends on the string lengths, so we resize as required\n \t\tauto last_offset = offset_data[append_data.row_count];\n-\t\tidx_t max_offset = append_data.row_count + to - from;\n-\t\tif (max_offset > NumericLimits<uint32_t>::Maximum() &&\n-\t\t    append_data.options.arrow_offset_size == ArrowOffsetSize::REGULAR) {\n-\t\t\tthrow InvalidInputException(\"Arrow Appender: The maximum total string size for regular string buffers is \"\n-\t\t\t                            \"%u but the offset of %lu exceeds this.\",\n-\t\t\t                            NumericLimits<uint32_t>::Maximum(), max_offset);\n-\t\t}\n \t\tfor (idx_t i = from; i < to; i++) {\n \t\t\tauto source_idx = format.sel->get_index(i);\n \t\t\tauto offset_idx = append_data.row_count + i + 1 - from;\n@@ -84,6 +78,13 @@ struct ArrowVarcharData {\n \n \t\t\t// append the offset data\n \t\t\tauto current_offset = last_offset + string_length;\n+\t\t\tif (!LARGE_STRING && (int64_t)last_offset + string_length > NumericLimits<int32_t>::Maximum()) {\n+\t\t\t\tD_ASSERT(append_data.options.arrow_offset_size == ArrowOffsetSize::REGULAR);\n+\t\t\t\tthrow InvalidInputException(\n+\t\t\t\t    \"Arrow Appender: The maximum total string size for regular string buffers is \"\n+\t\t\t\t    \"%u but the offset of %lu exceeds this.\",\n+\t\t\t\t    NumericLimits<int32_t>::Maximum(), current_offset);\n+\t\t\t}\n \t\t\toffset_data[offset_idx] = current_offset;\n \n \t\t\t// resize the string buffer if required, and write the string data\n@@ -95,6 +96,15 @@ struct ArrowVarcharData {\n \t\tappend_data.row_count += size;\n \t}\n \n+\tstatic void Append(ArrowAppendData &append_data, Vector &input, idx_t from, idx_t to, idx_t input_size) {\n+\t\tif (append_data.options.arrow_offset_size == ArrowOffsetSize::REGULAR) {\n+\t\t\t// Check if the offset exceeds the max supported value\n+\t\t\tAppendTemplated<false>(append_data, input, from, to, input_size);\n+\t\t} else {\n+\t\t\tAppendTemplated<true>(append_data, input, from, to, input_size);\n+\t\t}\n+\t}\n+\n \tstatic void Finalize(ArrowAppendData &append_data, const LogicalType &type, ArrowArray *result) {\n \t\tresult->n_buffers = 3;\n \t\tresult->buffers[1] = append_data.main_buffer.data();\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/arrow/test_large_offsets.py b/tools/pythonpkg/tests/fast/arrow/test_large_offsets.py\nnew file mode 100644\nindex 000000000000..03705e75e5a5\n--- /dev/null\n+++ b/tools/pythonpkg/tests/fast/arrow/test_large_offsets.py\n@@ -0,0 +1,43 @@\n+from re import S\n+import duckdb\n+import os\n+import pytest\n+import tempfile\n+from conftest import pandas_supports_arrow_backend\n+\n+pa = pytest.importorskip(\"pyarrow\")\n+pq = pytest.importorskip(\"pyarrow.parquet\")\n+ds = pytest.importorskip(\"pyarrow.dataset\")\n+np = pytest.importorskip(\"numpy\")\n+\n+\n+class TestArrowLargeOffsets(object):\n+    @pytest.mark.skip(reason=\"CI does not have enough memory to validate this\")\n+    def test_large_lists(self, duckdb_cursor):\n+        ary = pa.array([np.arange(start=0, stop=3000, dtype=np.uint8) for i in range(1_000_000)])\n+        tbl = pa.Table.from_pydict(dict(col=ary))\n+        with pytest.raises(\n+            duckdb.InvalidInputException,\n+            match='Arrow Appender: The maximum combined list offset for regular list buffers is 2147483647 but the offset of 2147481000 exceeds this.',\n+        ):\n+            res = duckdb_cursor.sql(\"SELECT col FROM tbl\").arrow()\n+\n+        tbl2 = pa.Table.from_pydict(dict(col=ary.cast(pa.large_list(pa.uint8()))))\n+        duckdb_cursor.sql(\"set arrow_large_buffer_size = true\")\n+        res2 = duckdb_cursor.sql(\"SELECT col FROM tbl2\").arrow()\n+        res2.validate()\n+\n+    @pytest.mark.skip(reason=\"CI does not have enough memory to validate this\")\n+    def test_large_maps(self, duckdb_cursor):\n+        ary = pa.array([np.arange(start=3000 * j, stop=3000 * (j + 1), dtype=np.uint64) for j in range(1_000_000)])\n+        tbl = pa.Table.from_pydict(dict(col=ary))\n+\n+        with pytest.raises(\n+            duckdb.InvalidInputException,\n+            match='Arrow Appender: The maximum combined list offset for regular list buffers is 2147483647 but the offset of 2147481000 exceeds this.',\n+        ):\n+            arrow_map = duckdb_cursor.sql(\"select map(col, col) from tbl\").arrow()\n+\n+        duckdb_cursor.sql(\"set arrow_large_buffer_size = true\")\n+        arrow_map_large = duckdb_cursor.sql(\"select map(col, col) from tbl\").arrow()\n+        arrow_map_large.validate()\n",
  "problem_statement": "DuckDB returns invalid Arrow array for lists with more than 2**31 elements\n### What happens?\r\n\r\nWhen a SELECT query returns a list with more than 2**31 elements, calling `.arrow()` (or `.pl()`) returns an invalid arrow table (message: `<Invalid array: Negative offsets in list array>`).\r\n\r\n### To Reproduce\r\n\r\n```python\r\nimport duckdb\r\nimport numpy as np\r\nimport pyarrow as pa\r\nary = pa.array([np.random.randint(0, 255, size=np.random.randint(2000, 4000), dtype=np.uint8) for i in trange(1_000_000)])\r\ntbl = pa.Table.from_pydict(dict(col=ary))\r\ntbl2 = pa.Table.from_pydict(dict(col=ary.cast(pa.large_list(pa.uint8()))))\r\nres = duckdb.sql(\"SELECT col FROM tbl\").arrow()\r\nres2 = duckdb.sql(\"SELECT col FROM tbl2\").arrow()\r\n```\r\n\r\nBoth `res` and `res2` display as:\r\n```\r\npyarrow.Table\r\ncol: list<l: uint8>\r\n  child 0, l: uint8\r\n----\r\ncol: [<Invalid array: Negative offsets in list array>]\r\n```\r\n\r\nCalling `.pl()` instead of `.arrow()` yields a similar error, while `.df()` works (and is slow, ~13min on my machine).\r\n\r\nThe underlying issue seems to be that DuckDB and/or Arrow is using int32 offsets (Arrow's List type) where int64 offsets (Arrow's LargeList type) are needed. Would love any tips on how to work around this. Ideally, DuckDB should automatically promote to int64 offsets when necessary (it shouldn't return an invalid arrow array).\r\n\r\n### OS:\r\n\r\nLinux x64\r\n\r\n### DuckDB Version:\r\n\r\n0.9.0\r\n\r\n### DuckDB Client:\r\n\r\nPython\r\n\r\n### Full Name:\r\n\r\nJacob Quinn Shenker\r\n\r\n### Affiliation:\r\n\r\nHarvard Medical School\r\n\r\n### Have you tried this on the latest `main` branch?\r\n\r\nI have tested with a release build (and could not test with a main build)\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] Yes, I have\n",
  "hints_text": "Confirming this is still an issue in 0.9.1.\nI can imagine this can happen if I look at our list conversion code, we always use the int32_t sized offsets when producing a list.\r\n\r\nWe have a `arrow_large_buffer_size` setting which switches our output buffer sizes for offsets (strings) to int64_t, we should respect this for lists as well",
  "created_at": "2023-10-25T12:23:08Z"
}