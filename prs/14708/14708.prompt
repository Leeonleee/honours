You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
DuckDB SEGV when loading 2 million entries to a table with STRUCT in schema
### What happens?

Loading 2 million entries to a table reliably crashes, if the table contains STRUCT in the schema. The STRUCT in the schema ifself is triggering the crash, the data does not have to be loaded in the column.
This is reproducible in the latest DuckDB 1.1.3 as well as earlier versions, except the stable 1.0.0 release.

bzipped crash_data.parquet file required for the code to run:
https://drive.google.com/file/d/1C4s5mTDTzoGewUaGMvFm6NE4h0X6xQvY/view?usp=share_link
(please ignore "large file" virus warnings from Google)

Python SEGFAULT report:
```
Fatal Python error: Segmentation fault

Thread 0x00000001e8e04f40 (most recent call first):
  File "/Users/dmitrykalinin/src/duckdb_crash/crash.py", line 19 in <module>
zsh: segmentation fault  python crash.py
```
Example C++ stack trace:
```
Thread 35 Crashed:
0   libsystem_platform.dylib      	       0x180fde1d4 _platform_memmove + 52
1   duckdb.cpython-312-darwin.so  	       0x1144cb538 duckdb::StringVector::AddStringOrBlob(duckdb::Vector&, duckdb::string_t) + 160
2   duckdb.cpython-312-darwin.so  	       0x113df914c duckdb::VectorOperations::Copy(duckdb::Vector const&, duckdb::Vector&, duckdb::SelectionVector const&, unsigned long long, unsigned long long, unsigned long long, unsigned long long) + 2492
3   duckdb.cpython-312-darwin.so  	       0x113df8fd8 duckdb::VectorOperations::Copy(duckdb::Vector const&, duckdb::Vector&, duckdb::SelectionVector const&, unsigned long long, unsigned long long, unsigned long long, unsigned long long) + 2120
4   duckdb.cpython-312-darwin.so  	       0x1144988c4 duckdb::Vector::Flatten(unsigned long long) + 480
5   duckdb.cpython-312-darwin.so  	       0x115436b58 duckdb::StructColumnData::Append(duckdb::BaseStatistics&, duckdb::ColumnAppendState&, duckdb::Vector&, unsigned long long) + 64
6   duckdb.cpython-312-darwin.so  	       0x11542c480 duckdb::RowGroupCollection::Append(duckdb::DataChunk&, duckdb::TableAppendState&) + 324
7   duckdb.cpython-312-darwin.so  	       0x114c7ab8c duckdb::PhysicalInsert::Sink(duckdb::ExecutionContext&, duckdb::DataChunk&, duckdb::OperatorSinkInput&) const + 432
8   duckdb.cpython-312-darwin.so  	       0x11518b8f0 duckdb::PipelineExecutor::ExecutePushInternal(duckdb::DataChunk&, unsigned long long) + 268
9   duckdb.cpython-312-darwin.so  	       0x1151886b4 duckdb::PipelineExecutor::Execute(unsigned long long) + 328
10  duckdb.cpython-312-darwin.so  	       0x1151883bc duckdb::PipelineTask::ExecuteTask(duckdb::TaskExecutionMode) + 328
11  duckdb.cpython-312-darwin.so  	       0x11518073c duckdb::ExecutorTask::Execute(duckdb::TaskExecutionMode) + 140
12  duckdb.cpython-312-darwin.so  	       0x11518ebf4 duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 612
13  duckdb.cpython-312-darwin.so  	       0x11519605c void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 56
14  libsystem_pthread.dylib       	       0x180fadf94 _pthread_start + 136
15  libsystem_pthread.dylib       	       0x180fa8d34 thread_start + 8
```

### To Reproduce

```
import duckdb
import faulthandler

faulthandler.enable()
conn = duckdb.connect()
cursor = conn.cursor()

conn.execute("""
    CREATE TABLE test_table (
        "id" VARCHAR,

        "str" STRUCT(
            a VARCHAR
        ),
        PRIMARY KEY (id)
    )
""")

conn.execute("""
INSERT INTO test_table(
    "id"   
)
(
    SELECT
        "id", 
    FROM
        read_parquet(['crash_data.parquet'])
    QUALIFY
        ROW_NUMBER() OVER (
            PARTITION BY id
        ) = 1
)
""")

```

### OS:

MacOS

### DuckDB Version:

1.1.3

### DuckDB Client:

Python 3.11.10

### Hardware:

Macbook M3 with 36GB RAM

### Full Name:

Dimi Kalyn

### Affiliation:

Exaforce, Inc

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have
DuckDB SEGV when loading 2 million entries to a table with STRUCT in schema
### What happens?

Loading 2 million entries to a table reliably crashes, if the table contains STRUCT in the schema. The STRUCT in the schema ifself is triggering the crash, the data does not have to be loaded in the column.
This is reproducible in the latest DuckDB 1.1.3 as well as earlier versions, except the stable 1.0.0 release.

bzipped crash_data.parquet file required for the code to run:
https://drive.google.com/file/d/1C4s5mTDTzoGewUaGMvFm6NE4h0X6xQvY/view?usp=share_link
(please ignore "large file" virus warnings from Google)

Python SEGFAULT report:
```
Fatal Python error: Segmentation fault

Thread 0x00000001e8e04f40 (most recent call first):
  File "/Users/dmitrykalinin/src/duckdb_crash/crash.py", line 19 in <module>
zsh: segmentation fault  python crash.py
```
Example C++ stack trace:
```
Thread 35 Crashed:
0   libsystem_platform.dylib      	       0x180fde1d4 _platform_memmove + 52
1   duckdb.cpython-312-darwin.so  	       0x1144cb538 duckdb::StringVector::AddStringOrBlob(duckdb::Vector&, duckdb::string_t) + 160
2   duckdb.cpython-312-darwin.so  	       0x113df914c duckdb::VectorOperations::Copy(duckdb::Vector const&, duckdb::Vector&, duckdb::SelectionVector const&, unsigned long long, unsigned long long, unsigned long long, unsigned long long) + 2492
3   duckdb.cpython-312-darwin.so  	       0x113df8fd8 duckdb::VectorOperations::Copy(duckdb::Vector const&, duckdb::Vector&, duckdb::SelectionVector const&, unsigned long long, unsigned long long, unsigned long long, unsigned long long) + 2120
4   duckdb.cpython-312-darwin.so  	       0x1144988c4 duckdb::Vector::Flatten(unsigned long long) + 480
5   duckdb.cpython-312-darwin.so  	       0x115436b58 duckdb::StructColumnData::Append(duckdb::BaseStatistics&, duckdb::ColumnAppendState&, duckdb::Vector&, unsigned long long) + 64
6   duckdb.cpython-312-darwin.so  	       0x11542c480 duckdb::RowGroupCollection::Append(duckdb::DataChunk&, duckdb::TableAppendState&) + 324
7   duckdb.cpython-312-darwin.so  	       0x114c7ab8c duckdb::PhysicalInsert::Sink(duckdb::ExecutionContext&, duckdb::DataChunk&, duckdb::OperatorSinkInput&) const + 432
8   duckdb.cpython-312-darwin.so  	       0x11518b8f0 duckdb::PipelineExecutor::ExecutePushInternal(duckdb::DataChunk&, unsigned long long) + 268
9   duckdb.cpython-312-darwin.so  	       0x1151886b4 duckdb::PipelineExecutor::Execute(unsigned long long) + 328
10  duckdb.cpython-312-darwin.so  	       0x1151883bc duckdb::PipelineTask::ExecuteTask(duckdb::TaskExecutionMode) + 328
11  duckdb.cpython-312-darwin.so  	       0x11518073c duckdb::ExecutorTask::Execute(duckdb::TaskExecutionMode) + 140
12  duckdb.cpython-312-darwin.so  	       0x11518ebf4 duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 612
13  duckdb.cpython-312-darwin.so  	       0x11519605c void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 56
14  libsystem_pthread.dylib       	       0x180fadf94 _pthread_start + 136
15  libsystem_pthread.dylib       	       0x180fa8d34 thread_start + 8
```

### To Reproduce

```
import duckdb
import faulthandler

faulthandler.enable()
conn = duckdb.connect()
cursor = conn.cursor()

conn.execute("""
    CREATE TABLE test_table (
        "id" VARCHAR,

        "str" STRUCT(
            a VARCHAR
        ),
        PRIMARY KEY (id)
    )
""")

conn.execute("""
INSERT INTO test_table(
    "id"   
)
(
    SELECT
        "id", 
    FROM
        read_parquet(['crash_data.parquet'])
    QUALIFY
        ROW_NUMBER() OVER (
            PARTITION BY id
        ) = 1
)
""")

```

### OS:

MacOS

### DuckDB Version:

1.1.3

### DuckDB Client:

Python 3.11.10

### Hardware:

Macbook M3 with 36GB RAM

### Full Name:

Dimi Kalyn

### Affiliation:

Exaforce, Inc

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://duckdb.org/docs/installation/) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/storage/table/array_column_data.cpp]
1: #include "duckdb/storage/table/array_column_data.hpp"
2: #include "duckdb/storage/statistics/array_stats.hpp"
3: #include "duckdb/common/serializer/serializer.hpp"
4: #include "duckdb/common/serializer/deserializer.hpp"
5: #include "duckdb/storage/table/column_checkpoint_state.hpp"
6: #include "duckdb/storage/table/append_state.hpp"
7: #include "duckdb/storage/table/scan_state.hpp"
8: 
9: namespace duckdb {
10: 
11: ArrayColumnData::ArrayColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index, idx_t start_row,
12:                                  LogicalType type_p, optional_ptr<ColumnData> parent)
13:     : ColumnData(block_manager, info, column_index, start_row, std::move(type_p), parent),
14:       validity(block_manager, info, 0, start_row, *this) {
15: 	D_ASSERT(type.InternalType() == PhysicalType::ARRAY);
16: 	auto &child_type = ArrayType::GetChildType(type);
17: 	// the child column, with column index 1 (0 is the validity mask)
18: 	child_column = ColumnData::CreateColumnUnique(block_manager, info, 1, start_row, child_type, this);
19: }
20: 
21: void ArrayColumnData::SetStart(idx_t new_start) {
22: 	this->start = new_start;
23: 	child_column->SetStart(new_start);
24: 	validity.SetStart(new_start);
25: }
26: 
27: FilterPropagateResult ArrayColumnData::CheckZonemap(ColumnScanState &state, TableFilter &filter) {
28: 	// FIXME: There is nothing preventing us from supporting this, but it's not implemented yet.
29: 	// table filters are not supported yet for fixed size list columns
30: 	return FilterPropagateResult::NO_PRUNING_POSSIBLE;
31: }
32: 
33: void ArrayColumnData::InitializePrefetch(PrefetchState &prefetch_state, ColumnScanState &scan_state, idx_t rows) {
34: 	ColumnData::InitializePrefetch(prefetch_state, scan_state, rows);
35: 	validity.InitializePrefetch(prefetch_state, scan_state.child_states[0], rows);
36: 	auto array_size = ArrayType::GetSize(type);
37: 	child_column->InitializePrefetch(prefetch_state, scan_state.child_states[1], rows * array_size);
38: }
39: 
40: void ArrayColumnData::InitializeScan(ColumnScanState &state) {
41: 	// initialize the validity segment
42: 	D_ASSERT(state.child_states.size() == 2);
43: 
44: 	state.row_index = 0;
45: 	state.current = nullptr;
46: 
47: 	validity.InitializeScan(state.child_states[0]);
48: 
49: 	// initialize the child scan
50: 	child_column->InitializeScan(state.child_states[1]);
51: }
52: 
53: void ArrayColumnData::InitializeScanWithOffset(ColumnScanState &state, idx_t row_idx) {
54: 	D_ASSERT(state.child_states.size() == 2);
55: 
56: 	if (row_idx == 0) {
57: 		// Trivial case, no offset
58: 		InitializeScan(state);
59: 		return;
60: 	}
61: 
62: 	state.row_index = row_idx;
63: 	state.current = nullptr;
64: 
65: 	// initialize the validity segment
66: 	validity.InitializeScanWithOffset(state.child_states[0], row_idx);
67: 
68: 	auto array_size = ArrayType::GetSize(type);
69: 	auto child_count = (row_idx - start) * array_size;
70: 
71: 	D_ASSERT(child_count <= child_column->GetMaxEntry());
72: 	if (child_count < child_column->GetMaxEntry()) {
73: 		const auto child_offset = start + child_count;
74: 		child_column->InitializeScanWithOffset(state.child_states[1], child_offset);
75: 	}
76: }
77: 
78: idx_t ArrayColumnData::Scan(TransactionData transaction, idx_t vector_index, ColumnScanState &state, Vector &result,
79:                             idx_t scan_count) {
80: 	return ScanCount(state, result, scan_count);
81: }
82: 
83: idx_t ArrayColumnData::ScanCommitted(idx_t vector_index, ColumnScanState &state, Vector &result, bool allow_updates,
84:                                      idx_t scan_count) {
85: 	return ScanCount(state, result, scan_count);
86: }
87: 
88: idx_t ArrayColumnData::ScanCount(ColumnScanState &state, Vector &result, idx_t count) {
89: 	// Scan validity
90: 	auto scan_count = validity.ScanCount(state.child_states[0], result, count);
91: 	auto array_size = ArrayType::GetSize(type);
92: 	// Scan child column
93: 	auto &child_vec = ArrayVector::GetEntry(result);
94: 	child_column->ScanCount(state.child_states[1], child_vec, count * array_size);
95: 	return scan_count;
96: }
97: 
98: void ArrayColumnData::Skip(ColumnScanState &state, idx_t count) {
99: 	// Skip validity
100: 	validity.Skip(state.child_states[0], count);
101: 	// Skip child column
102: 	auto array_size = ArrayType::GetSize(type);
103: 	child_column->Skip(state.child_states[1], count * array_size);
104: }
105: 
106: void ArrayColumnData::InitializeAppend(ColumnAppendState &state) {
107: 	ColumnAppendState validity_append;
108: 	validity.InitializeAppend(validity_append);
109: 	state.child_appends.push_back(std::move(validity_append));
110: 
111: 	ColumnAppendState child_append;
112: 	child_column->InitializeAppend(child_append);
113: 	state.child_appends.push_back(std::move(child_append));
114: }
115: 
116: void ArrayColumnData::Append(BaseStatistics &stats, ColumnAppendState &state, Vector &vector, idx_t count) {
117: 	vector.Flatten(count);
118: 	// Append validity
119: 	validity.Append(stats, state.child_appends[0], vector, count);
120: 	// Append child column
121: 	auto array_size = ArrayType::GetSize(type);
122: 	auto &child_vec = ArrayVector::GetEntry(vector);
123: 	child_column->Append(ArrayStats::GetChildStats(stats), state.child_appends[1], child_vec, count * array_size);
124: 
125: 	this->count += count;
126: }
127: 
128: void ArrayColumnData::RevertAppend(row_t start_row) {
129: 	// Revert validity
130: 	validity.RevertAppend(start_row);
131: 	// Revert child column
132: 	auto array_size = ArrayType::GetSize(type);
133: 	child_column->RevertAppend(start_row * UnsafeNumericCast<row_t>(array_size));
134: 
135: 	this->count = UnsafeNumericCast<idx_t>(start_row) - this->start;
136: }
137: 
138: idx_t ArrayColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &result) {
139: 	throw NotImplementedException("Array Fetch");
140: }
141: 
142: void ArrayColumnData::Update(TransactionData transaction, idx_t column_index, Vector &update_vector, row_t *row_ids,
143:                              idx_t update_count) {
144: 	throw NotImplementedException("Array Update is not supported.");
145: }
146: 
147: void ArrayColumnData::UpdateColumn(TransactionData transaction, const vector<column_t> &column_path,
148:                                    Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t depth) {
149: 	throw NotImplementedException("Array Update Column is not supported");
150: }
151: 
152: unique_ptr<BaseStatistics> ArrayColumnData::GetUpdateStatistics() {
153: 	return nullptr;
154: }
155: 
156: void ArrayColumnData::FetchRow(TransactionData transaction, ColumnFetchState &state, row_t row_id, Vector &result,
157:                                idx_t result_idx) {
158: 
159: 	// Create state for validity & child column
160: 	if (state.child_states.empty()) {
161: 		state.child_states.push_back(make_uniq<ColumnFetchState>());
162: 	}
163: 
164: 	// Fetch validity
165: 	validity.FetchRow(transaction, *state.child_states[0], row_id, result, result_idx);
166: 
167: 	// Fetch child column
168: 	auto &child_vec = ArrayVector::GetEntry(result);
169: 	auto &child_type = ArrayType::GetChildType(type);
170: 	auto array_size = ArrayType::GetSize(type);
171: 
172: 	// We need to fetch between [row_id * array_size, (row_id + 1) * array_size)
173: 	auto child_state = make_uniq<ColumnScanState>();
174: 	child_state->Initialize(child_type, nullptr);
175: 
176: 	const auto child_offset = start + (UnsafeNumericCast<idx_t>(row_id) - start) * array_size;
177: 
178: 	child_column->InitializeScanWithOffset(*child_state, child_offset);
179: 	Vector child_scan(child_type, array_size);
180: 	child_column->ScanCount(*child_state, child_scan, array_size);
181: 	VectorOperations::Copy(child_scan, child_vec, array_size, 0, result_idx * array_size);
182: }
183: 
184: void ArrayColumnData::CommitDropColumn() {
185: 	validity.CommitDropColumn();
186: 	child_column->CommitDropColumn();
187: }
188: 
189: struct ArrayColumnCheckpointState : public ColumnCheckpointState {
190: 	ArrayColumnCheckpointState(RowGroup &row_group, ColumnData &column_data, PartialBlockManager &partial_block_manager)
191: 	    : ColumnCheckpointState(row_group, column_data, partial_block_manager) {
192: 		global_stats = ArrayStats::CreateEmpty(column_data.type).ToUnique();
193: 	}
194: 
195: 	unique_ptr<ColumnCheckpointState> validity_state;
196: 	unique_ptr<ColumnCheckpointState> child_state;
197: 
198: public:
199: 	unique_ptr<BaseStatistics> GetStatistics() override {
200: 		auto stats = global_stats->Copy();
201: 		ArrayStats::SetChildStats(stats, child_state->GetStatistics());
202: 		return stats.ToUnique();
203: 	}
204: 
205: 	PersistentColumnData ToPersistentData() override {
206: 		PersistentColumnData data(PhysicalType::ARRAY);
207: 		data.child_columns.push_back(validity_state->ToPersistentData());
208: 		data.child_columns.push_back(child_state->ToPersistentData());
209: 		return data;
210: 	}
211: };
212: 
213: unique_ptr<ColumnCheckpointState> ArrayColumnData::CreateCheckpointState(RowGroup &row_group,
214:                                                                          PartialBlockManager &partial_block_manager) {
215: 	return make_uniq<ArrayColumnCheckpointState>(row_group, *this, partial_block_manager);
216: }
217: 
218: unique_ptr<ColumnCheckpointState> ArrayColumnData::Checkpoint(RowGroup &row_group,
219:                                                               ColumnCheckpointInfo &checkpoint_info) {
220: 
221: 	auto checkpoint_state = make_uniq<ArrayColumnCheckpointState>(row_group, *this, checkpoint_info.info.manager);
222: 	checkpoint_state->validity_state = validity.Checkpoint(row_group, checkpoint_info);
223: 	checkpoint_state->child_state = child_column->Checkpoint(row_group, checkpoint_info);
224: 	return std::move(checkpoint_state);
225: }
226: 
227: bool ArrayColumnData::IsPersistent() {
228: 	return validity.IsPersistent() && child_column->IsPersistent();
229: }
230: 
231: PersistentColumnData ArrayColumnData::Serialize() {
232: 	PersistentColumnData persistent_data(PhysicalType::ARRAY);
233: 	persistent_data.child_columns.push_back(validity.Serialize());
234: 	persistent_data.child_columns.push_back(child_column->Serialize());
235: 	return persistent_data;
236: }
237: 
238: void ArrayColumnData::InitializeColumn(PersistentColumnData &column_data, BaseStatistics &target_stats) {
239: 	D_ASSERT(column_data.pointers.empty());
240: 	validity.InitializeColumn(column_data.child_columns[0], target_stats);
241: 	auto &child_stats = ArrayStats::GetChildStats(target_stats);
242: 	child_column->InitializeColumn(column_data.child_columns[1], child_stats);
243: 	this->count = validity.count.load();
244: }
245: 
246: void ArrayColumnData::GetColumnSegmentInfo(idx_t row_group_index, vector<idx_t> col_path,
247:                                            vector<ColumnSegmentInfo> &result) {
248: 	col_path.push_back(0);
249: 	validity.GetColumnSegmentInfo(row_group_index, col_path, result);
250: 	col_path.back() = 1;
251: 	child_column->GetColumnSegmentInfo(row_group_index, col_path, result);
252: }
253: 
254: void ArrayColumnData::Verify(RowGroup &parent) {
255: #ifdef DEBUG
256: 	ColumnData::Verify(parent);
257: 	validity.Verify(parent);
258: 	child_column->Verify(parent);
259: #endif
260: }
261: 
262: } // namespace duckdb
[end of src/storage/table/array_column_data.cpp]
[start of src/storage/table/struct_column_data.cpp]
1: #include "duckdb/storage/table/struct_column_data.hpp"
2: #include "duckdb/storage/statistics/struct_stats.hpp"
3: #include "duckdb/common/serializer/serializer.hpp"
4: #include "duckdb/common/serializer/deserializer.hpp"
5: #include "duckdb/storage/table/column_checkpoint_state.hpp"
6: #include "duckdb/storage/table/append_state.hpp"
7: #include "duckdb/storage/table/scan_state.hpp"
8: #include "duckdb/storage/table/update_segment.hpp"
9: 
10: namespace duckdb {
11: 
12: StructColumnData::StructColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index,
13:                                    idx_t start_row, LogicalType type_p, optional_ptr<ColumnData> parent)
14:     : ColumnData(block_manager, info, column_index, start_row, std::move(type_p), parent),
15:       validity(block_manager, info, 0, start_row, *this) {
16: 	D_ASSERT(type.InternalType() == PhysicalType::STRUCT);
17: 	auto &child_types = StructType::GetChildTypes(type);
18: 	D_ASSERT(!child_types.empty());
19: 	if (type.id() != LogicalTypeId::UNION && StructType::IsUnnamed(type)) {
20: 		throw InvalidInputException("A table cannot be created from an unnamed struct");
21: 	}
22: 	// the sub column index, starting at 1 (0 is the validity mask)
23: 	idx_t sub_column_index = 1;
24: 	for (auto &child_type : child_types) {
25: 		sub_columns.push_back(
26: 		    ColumnData::CreateColumnUnique(block_manager, info, sub_column_index, start_row, child_type.second, this));
27: 		sub_column_index++;
28: 	}
29: }
30: 
31: void StructColumnData::SetStart(idx_t new_start) {
32: 	this->start = new_start;
33: 	for (auto &sub_column : sub_columns) {
34: 		sub_column->SetStart(new_start);
35: 	}
36: 	validity.SetStart(new_start);
37: }
38: 
39: idx_t StructColumnData::GetMaxEntry() {
40: 	return sub_columns[0]->GetMaxEntry();
41: }
42: 
43: void StructColumnData::InitializePrefetch(PrefetchState &prefetch_state, ColumnScanState &scan_state, idx_t rows) {
44: 	validity.InitializePrefetch(prefetch_state, scan_state.child_states[0], rows);
45: 	for (idx_t i = 0; i < sub_columns.size(); i++) {
46: 		sub_columns[i]->InitializePrefetch(prefetch_state, scan_state.child_states[i + 1], rows);
47: 	}
48: }
49: 
50: void StructColumnData::InitializeScan(ColumnScanState &state) {
51: 	D_ASSERT(state.child_states.size() == sub_columns.size() + 1);
52: 	state.row_index = 0;
53: 	state.current = nullptr;
54: 
55: 	// initialize the validity segment
56: 	validity.InitializeScan(state.child_states[0]);
57: 
58: 	// initialize the sub-columns
59: 	for (idx_t i = 0; i < sub_columns.size(); i++) {
60: 		sub_columns[i]->InitializeScan(state.child_states[i + 1]);
61: 	}
62: }
63: 
64: void StructColumnData::InitializeScanWithOffset(ColumnScanState &state, idx_t row_idx) {
65: 	D_ASSERT(state.child_states.size() == sub_columns.size() + 1);
66: 	state.row_index = row_idx;
67: 	state.current = nullptr;
68: 
69: 	// initialize the validity segment
70: 	validity.InitializeScanWithOffset(state.child_states[0], row_idx);
71: 
72: 	// initialize the sub-columns
73: 	for (idx_t i = 0; i < sub_columns.size(); i++) {
74: 		sub_columns[i]->InitializeScanWithOffset(state.child_states[i + 1], row_idx);
75: 	}
76: }
77: 
78: idx_t StructColumnData::Scan(TransactionData transaction, idx_t vector_index, ColumnScanState &state, Vector &result,
79:                              idx_t target_count) {
80: 	auto scan_count = validity.Scan(transaction, vector_index, state.child_states[0], result, target_count);
81: 	auto &child_entries = StructVector::GetEntries(result);
82: 	for (idx_t i = 0; i < sub_columns.size(); i++) {
83: 		sub_columns[i]->Scan(transaction, vector_index, state.child_states[i + 1], *child_entries[i], target_count);
84: 	}
85: 	return scan_count;
86: }
87: 
88: idx_t StructColumnData::ScanCommitted(idx_t vector_index, ColumnScanState &state, Vector &result, bool allow_updates,
89:                                       idx_t target_count) {
90: 	auto scan_count = validity.ScanCommitted(vector_index, state.child_states[0], result, allow_updates, target_count);
91: 	auto &child_entries = StructVector::GetEntries(result);
92: 	for (idx_t i = 0; i < sub_columns.size(); i++) {
93: 		sub_columns[i]->ScanCommitted(vector_index, state.child_states[i + 1], *child_entries[i], allow_updates,
94: 		                              target_count);
95: 	}
96: 	return scan_count;
97: }
98: 
99: idx_t StructColumnData::ScanCount(ColumnScanState &state, Vector &result, idx_t count) {
100: 	auto scan_count = validity.ScanCount(state.child_states[0], result, count);
101: 	auto &child_entries = StructVector::GetEntries(result);
102: 	for (idx_t i = 0; i < sub_columns.size(); i++) {
103: 		sub_columns[i]->ScanCount(state.child_states[i + 1], *child_entries[i], count);
104: 	}
105: 	return scan_count;
106: }
107: 
108: void StructColumnData::Skip(ColumnScanState &state, idx_t count) {
109: 	validity.Skip(state.child_states[0], count);
110: 
111: 	// skip inside the sub-columns
112: 	for (idx_t child_idx = 0; child_idx < sub_columns.size(); child_idx++) {
113: 		sub_columns[child_idx]->Skip(state.child_states[child_idx + 1], count);
114: 	}
115: }
116: 
117: void StructColumnData::InitializeAppend(ColumnAppendState &state) {
118: 	ColumnAppendState validity_append;
119: 	validity.InitializeAppend(validity_append);
120: 	state.child_appends.push_back(std::move(validity_append));
121: 
122: 	for (auto &sub_column : sub_columns) {
123: 		ColumnAppendState child_append;
124: 		sub_column->InitializeAppend(child_append);
125: 		state.child_appends.push_back(std::move(child_append));
126: 	}
127: }
128: 
129: void StructColumnData::Append(BaseStatistics &stats, ColumnAppendState &state, Vector &vector, idx_t count) {
130: 	vector.Flatten(count);
131: 
132: 	// append the null values
133: 	validity.Append(stats, state.child_appends[0], vector, count);
134: 
135: 	auto &child_entries = StructVector::GetEntries(vector);
136: 	for (idx_t i = 0; i < child_entries.size(); i++) {
137: 		sub_columns[i]->Append(StructStats::GetChildStats(stats, i), state.child_appends[i + 1], *child_entries[i],
138: 		                       count);
139: 	}
140: 	this->count += count;
141: }
142: 
143: void StructColumnData::RevertAppend(row_t start_row) {
144: 	validity.RevertAppend(start_row);
145: 	for (auto &sub_column : sub_columns) {
146: 		sub_column->RevertAppend(start_row);
147: 	}
148: 	this->count = UnsafeNumericCast<idx_t>(start_row) - this->start;
149: }
150: 
151: idx_t StructColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &result) {
152: 	// fetch validity mask
153: 	auto &child_entries = StructVector::GetEntries(result);
154: 	// insert any child states that are required
155: 	for (idx_t i = state.child_states.size(); i < child_entries.size() + 1; i++) {
156: 		ColumnScanState child_state;
157: 		child_state.scan_options = state.scan_options;
158: 		state.child_states.push_back(std::move(child_state));
159: 	}
160: 	// fetch the validity state
161: 	idx_t scan_count = validity.Fetch(state.child_states[0], row_id, result);
162: 	// fetch the sub-column states
163: 	for (idx_t i = 0; i < child_entries.size(); i++) {
164: 		sub_columns[i]->Fetch(state.child_states[i + 1], row_id, *child_entries[i]);
165: 	}
166: 	return scan_count;
167: }
168: 
169: void StructColumnData::Update(TransactionData transaction, idx_t column_index, Vector &update_vector, row_t *row_ids,
170:                               idx_t update_count) {
171: 	validity.Update(transaction, column_index, update_vector, row_ids, update_count);
172: 	auto &child_entries = StructVector::GetEntries(update_vector);
173: 	for (idx_t i = 0; i < child_entries.size(); i++) {
174: 		sub_columns[i]->Update(transaction, column_index, *child_entries[i], row_ids, update_count);
175: 	}
176: }
177: 
178: void StructColumnData::UpdateColumn(TransactionData transaction, const vector<column_t> &column_path,
179:                                     Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t depth) {
180: 	// we can never DIRECTLY update a struct column
181: 	if (depth >= column_path.size()) {
182: 		throw InternalException("Attempting to directly update a struct column - this should not be possible");
183: 	}
184: 	auto update_column = column_path[depth];
185: 	if (update_column == 0) {
186: 		// update the validity column
187: 		validity.UpdateColumn(transaction, column_path, update_vector, row_ids, update_count, depth + 1);
188: 	} else {
189: 		if (update_column > sub_columns.size()) {
190: 			throw InternalException("Update column_path out of range");
191: 		}
192: 		sub_columns[update_column - 1]->UpdateColumn(transaction, column_path, update_vector, row_ids, update_count,
193: 		                                             depth + 1);
194: 	}
195: }
196: 
197: unique_ptr<BaseStatistics> StructColumnData::GetUpdateStatistics() {
198: 	// check if any child column has updates
199: 	auto stats = BaseStatistics::CreateEmpty(type);
200: 	auto validity_stats = validity.GetUpdateStatistics();
201: 	if (validity_stats) {
202: 		stats.Merge(*validity_stats);
203: 	}
204: 	for (idx_t i = 0; i < sub_columns.size(); i++) {
205: 		auto child_stats = sub_columns[i]->GetUpdateStatistics();
206: 		if (child_stats) {
207: 			StructStats::SetChildStats(stats, i, std::move(child_stats));
208: 		}
209: 	}
210: 	return stats.ToUnique();
211: }
212: 
213: void StructColumnData::FetchRow(TransactionData transaction, ColumnFetchState &state, row_t row_id, Vector &result,
214:                                 idx_t result_idx) {
215: 	// fetch validity mask
216: 	auto &child_entries = StructVector::GetEntries(result);
217: 	// insert any child states that are required
218: 	for (idx_t i = state.child_states.size(); i < child_entries.size() + 1; i++) {
219: 		auto child_state = make_uniq<ColumnFetchState>();
220: 		state.child_states.push_back(std::move(child_state));
221: 	}
222: 	// fetch the validity state
223: 	validity.FetchRow(transaction, *state.child_states[0], row_id, result, result_idx);
224: 	// fetch the sub-column states
225: 	for (idx_t i = 0; i < child_entries.size(); i++) {
226: 		sub_columns[i]->FetchRow(transaction, *state.child_states[i + 1], row_id, *child_entries[i], result_idx);
227: 	}
228: }
229: 
230: void StructColumnData::CommitDropColumn() {
231: 	validity.CommitDropColumn();
232: 	for (auto &sub_column : sub_columns) {
233: 		sub_column->CommitDropColumn();
234: 	}
235: }
236: 
237: struct StructColumnCheckpointState : public ColumnCheckpointState {
238: 	StructColumnCheckpointState(RowGroup &row_group, ColumnData &column_data,
239: 	                            PartialBlockManager &partial_block_manager)
240: 	    : ColumnCheckpointState(row_group, column_data, partial_block_manager) {
241: 		global_stats = StructStats::CreateEmpty(column_data.type).ToUnique();
242: 	}
243: 
244: 	unique_ptr<ColumnCheckpointState> validity_state;
245: 	vector<unique_ptr<ColumnCheckpointState>> child_states;
246: 
247: public:
248: 	unique_ptr<BaseStatistics> GetStatistics() override {
249: 		D_ASSERT(global_stats);
250: 		for (idx_t i = 0; i < child_states.size(); i++) {
251: 			StructStats::SetChildStats(*global_stats, i, child_states[i]->GetStatistics());
252: 		}
253: 		return std::move(global_stats);
254: 	}
255: 
256: 	PersistentColumnData ToPersistentData() override {
257: 		PersistentColumnData data(PhysicalType::STRUCT);
258: 		data.child_columns.push_back(validity_state->ToPersistentData());
259: 		for (auto &child_state : child_states) {
260: 			data.child_columns.push_back(child_state->ToPersistentData());
261: 		}
262: 		return data;
263: 	}
264: };
265: 
266: unique_ptr<ColumnCheckpointState> StructColumnData::CreateCheckpointState(RowGroup &row_group,
267:                                                                           PartialBlockManager &partial_block_manager) {
268: 	return make_uniq<StructColumnCheckpointState>(row_group, *this, partial_block_manager);
269: }
270: 
271: unique_ptr<ColumnCheckpointState> StructColumnData::Checkpoint(RowGroup &row_group,
272:                                                                ColumnCheckpointInfo &checkpoint_info) {
273: 	auto checkpoint_state = make_uniq<StructColumnCheckpointState>(row_group, *this, checkpoint_info.info.manager);
274: 	checkpoint_state->validity_state = validity.Checkpoint(row_group, checkpoint_info);
275: 	for (auto &sub_column : sub_columns) {
276: 		checkpoint_state->child_states.push_back(sub_column->Checkpoint(row_group, checkpoint_info));
277: 	}
278: 	return std::move(checkpoint_state);
279: }
280: 
281: bool StructColumnData::IsPersistent() {
282: 	if (!validity.IsPersistent()) {
283: 		return false;
284: 	}
285: 	for (auto &child_col : sub_columns) {
286: 		if (!child_col->IsPersistent()) {
287: 			return false;
288: 		}
289: 	}
290: 	return true;
291: }
292: 
293: PersistentColumnData StructColumnData::Serialize() {
294: 	PersistentColumnData persistent_data(PhysicalType::STRUCT);
295: 	persistent_data.child_columns.push_back(validity.Serialize());
296: 	for (auto &sub_column : sub_columns) {
297: 		persistent_data.child_columns.push_back(sub_column->Serialize());
298: 	}
299: 	return persistent_data;
300: }
301: 
302: void StructColumnData::InitializeColumn(PersistentColumnData &column_data, BaseStatistics &target_stats) {
303: 	validity.InitializeColumn(column_data.child_columns[0], target_stats);
304: 	for (idx_t c_idx = 0; c_idx < sub_columns.size(); c_idx++) {
305: 		auto &child_stats = StructStats::GetChildStats(target_stats, c_idx);
306: 		sub_columns[c_idx]->InitializeColumn(column_data.child_columns[c_idx + 1], child_stats);
307: 	}
308: 	this->count = validity.count.load();
309: }
310: 
311: void StructColumnData::GetColumnSegmentInfo(duckdb::idx_t row_group_index, vector<duckdb::idx_t> col_path,
312:                                             vector<duckdb::ColumnSegmentInfo> &result) {
313: 	col_path.push_back(0);
314: 	validity.GetColumnSegmentInfo(row_group_index, col_path, result);
315: 	for (idx_t i = 0; i < sub_columns.size(); i++) {
316: 		col_path.back() = i + 1;
317: 		sub_columns[i]->GetColumnSegmentInfo(row_group_index, col_path, result);
318: 	}
319: }
320: 
321: void StructColumnData::Verify(RowGroup &parent) {
322: #ifdef DEBUG
323: 	ColumnData::Verify(parent);
324: 	validity.Verify(parent);
325: 	for (auto &sub_column : sub_columns) {
326: 		sub_column->Verify(parent);
327: 	}
328: #endif
329: }
330: 
331: } // namespace duckdb
[end of src/storage/table/struct_column_data.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: