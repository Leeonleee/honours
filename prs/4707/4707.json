{
  "repo": "duckdb/duckdb",
  "pull_number": 4707,
  "instance_id": "duckdb__duckdb-4707",
  "issue_numbers": [
    "4674",
    "4682"
  ],
  "base_commit": "a5b74ff1cdd60c1a64c6a5ba41e57a2dc20336c0",
  "patch": "diff --git a/src/catalog/catalog_entry/table_catalog_entry.cpp b/src/catalog/catalog_entry/table_catalog_entry.cpp\nindex f9e19aad4277..c3cf3db30ebd 100644\n--- a/src/catalog/catalog_entry/table_catalog_entry.cpp\n+++ b/src/catalog/catalog_entry/table_catalog_entry.cpp\n@@ -166,6 +166,19 @@ idx_t TableCatalogEntry::StandardColumnCount() const {\n \treturn count;\n }\n \n+unique_ptr<BaseStatistics> TableCatalogEntry::GetStatistics(ClientContext &context, column_t column_id) {\n+\tif (column_id == COLUMN_IDENTIFIER_ROW_ID) {\n+\t\treturn nullptr;\n+\t}\n+\tif (column_id >= columns.size()) {\n+\t\tthrow InternalException(\"TableCatalogEntry::GetStatistics column_id out of range\");\n+\t}\n+\tif (columns[column_id].Generated()) {\n+\t\treturn nullptr;\n+\t}\n+\treturn storage->GetStatistics(context, columns[column_id].StorageOid());\n+}\n+\n unique_ptr<CatalogEntry> TableCatalogEntry::AlterEntry(ClientContext &context, AlterInfo *info) {\n \tD_ASSERT(!internal);\n \tif (info->type != AlterType::ALTER_TABLE) {\n@@ -455,7 +468,9 @@ unique_ptr<CatalogEntry> TableCatalogEntry::SetDefault(ClientContext &context, S\n \t\tauto copy = columns[i].Copy();\n \t\tif (default_idx == i) {\n \t\t\t// set the default value of this column\n-\t\t\tD_ASSERT(!copy.Generated()); // Shouldnt reach here - DEFAULT value isn't supported for Generated Columns\n+\t\t\tif (copy.Generated()) {\n+\t\t\t\tthrow BinderException(\"Cannot SET DEFAULT for generated column \\\"%s\\\"\", columns[i].Name());\n+\t\t\t}\n \t\t\tcopy.SetDefaultValue(info.expression ? info.expression->Copy() : nullptr);\n \t\t}\n \t\tcreate_info->columns.push_back(move(copy));\ndiff --git a/src/execution/operator/set/physical_recursive_cte.cpp b/src/execution/operator/set/physical_recursive_cte.cpp\nindex 0c1b0fde7ec3..a42fecb8f75c 100644\n--- a/src/execution/operator/set/physical_recursive_cte.cpp\n+++ b/src/execution/operator/set/physical_recursive_cte.cpp\n@@ -166,6 +166,7 @@ void PhysicalRecursiveCTE::ExecuteRecursivePipelines(ExecutionContext &context)\n void PhysicalRecursiveCTE::BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) {\n \top_state.reset();\n \tsink_state.reset();\n+\tpipelines.clear();\n \n \t// recursive CTE\n \tstate.SetPipelineSource(current, this);\ndiff --git a/src/function/table/table_scan.cpp b/src/function/table/table_scan.cpp\nindex 819e2faa114e..e75674c85b55 100644\n--- a/src/function/table/table_scan.cpp\n+++ b/src/function/table/table_scan.cpp\n@@ -85,8 +85,7 @@ static unique_ptr<BaseStatistics> TableScanStatistics(ClientContext &context, co\n \t\t// we don't emit any statistics for tables that have outstanding transaction-local data\n \t\treturn nullptr;\n \t}\n-\tauto storage_idx = GetStorageIndex(*bind_data.table, column_id);\n-\treturn bind_data.table->storage->GetStatistics(context, storage_idx);\n+\treturn bind_data.table->GetStatistics(context, column_id);\n }\n \n static void TableScanFunc(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\ndiff --git a/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp b/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp\nindex 73733f239f74..affcd9094575 100644\n--- a/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp\n+++ b/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp\n@@ -65,6 +65,9 @@ class TableCatalogEntry : public StandardEntry {\n \tvector<LogicalType> GetTypes();\n \tstring ToSQL() override;\n \n+\t//! Get statistics of a column (physical or virtual) within the table\n+\tunique_ptr<BaseStatistics> GetStatistics(ClientContext &context, column_t column_id);\n+\n \t//! Serialize the meta information of the TableCatalogEntry a serializer\n \tvirtual void Serialize(Serializer &serializer);\n \t//! Deserializes to a CreateTableInfo\ndiff --git a/src/include/duckdb/common/types/cast_helpers.hpp b/src/include/duckdb/common/types/cast_helpers.hpp\nindex 8ef8b722b744..43903c285ca5 100644\n--- a/src/include/duckdb/common/types/cast_helpers.hpp\n+++ b/src/include/duckdb/common/types/cast_helpers.hpp\n@@ -522,14 +522,16 @@ struct IntervalToStringCast {\n \t\t\tif (micros < 0) {\n \t\t\t\t// negative time: append negative sign\n \t\t\t\tbuffer[length++] = '-';\n+\t\t\t} else {\n \t\t\t\tmicros = -micros;\n \t\t\t}\n-\t\t\tint64_t hour = micros / Interval::MICROS_PER_HOUR;\n-\t\t\tmicros -= hour * Interval::MICROS_PER_HOUR;\n-\t\t\tint64_t min = micros / Interval::MICROS_PER_MINUTE;\n-\t\t\tmicros -= min * Interval::MICROS_PER_MINUTE;\n-\t\t\tint64_t sec = micros / Interval::MICROS_PER_SEC;\n-\t\t\tmicros -= sec * Interval::MICROS_PER_SEC;\n+\t\t\tint64_t hour = -(micros / Interval::MICROS_PER_HOUR);\n+\t\t\tmicros += hour * Interval::MICROS_PER_HOUR;\n+\t\t\tint64_t min = -(micros / Interval::MICROS_PER_MINUTE);\n+\t\t\tmicros += min * Interval::MICROS_PER_MINUTE;\n+\t\t\tint64_t sec = -(micros / Interval::MICROS_PER_SEC);\n+\t\t\tmicros += sec * Interval::MICROS_PER_SEC;\n+\t\t\tmicros = -micros;\n \n \t\t\tif (hour < 10) {\n \t\t\t\tbuffer[length++] = '0';\ndiff --git a/src/include/duckdb/planner/expression_binder.hpp b/src/include/duckdb/planner/expression_binder.hpp\nindex a94be21a1ee3..95ee9c8396df 100644\n--- a/src/include/duckdb/planner/expression_binder.hpp\n+++ b/src/include/duckdb/planner/expression_binder.hpp\n@@ -142,6 +142,7 @@ class ExpressionBinder {\n \n \tvirtual string UnsupportedAggregateMessage();\n \tvirtual string UnsupportedUnnestMessage();\n+\tvirtual bool CanContainSubqueries();\n \n \tBinder &binder;\n \tClientContext &context;\ndiff --git a/src/include/duckdb/planner/expression_binder/constant_binder.hpp b/src/include/duckdb/planner/expression_binder/constant_binder.hpp\nindex c912aa941571..b0a699391f1d 100644\n--- a/src/include/duckdb/planner/expression_binder/constant_binder.hpp\n+++ b/src/include/duckdb/planner/expression_binder/constant_binder.hpp\n@@ -24,6 +24,7 @@ class ConstantBinder : public ExpressionBinder {\n \tBindResult BindExpression(unique_ptr<ParsedExpression> *expr, idx_t depth, bool root_expression = false) override;\n \n \tstring UnsupportedAggregateMessage() override;\n+\tbool CanContainSubqueries() override;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/storage/data_table.hpp b/src/include/duckdb/storage/data_table.hpp\nindex 58b78f71b5cc..2571977ffae1 100644\n--- a/src/include/duckdb/storage/data_table.hpp\n+++ b/src/include/duckdb/storage/data_table.hpp\n@@ -163,7 +163,9 @@ class DataTable {\n \t\tthis->is_root = true;\n \t}\n \n+\t//! Get statistics of a physical column within the table\n \tunique_ptr<BaseStatistics> GetStatistics(ClientContext &context, column_t column_id);\n+\t//! Sets statistics of a physical column within the table\n \tvoid SetStatistics(column_t column_id, const std::function<void(BaseStatistics &)> &set_fun);\n \n \t//! Checkpoint the table to the specified table data writer\ndiff --git a/src/include/duckdb/storage/table/chunk_info.hpp b/src/include/duckdb/storage/table/chunk_info.hpp\nindex 9285ed050abe..d9ba64c1c16c 100644\n--- a/src/include/duckdb/storage/table/chunk_info.hpp\n+++ b/src/include/duckdb/storage/table/chunk_info.hpp\n@@ -91,6 +91,11 @@ class ChunkVectorInfo : public ChunkInfo {\n \tvoid CommitAppend(transaction_t commit_id, idx_t start, idx_t end) override;\n \n \tvoid Append(idx_t start, idx_t end, transaction_t commit_id);\n+\t//! Performs a delete in the ChunkVectorInfo - returns how many tuples were actually deleted\n+\t//! The number of rows that were actually deleted might be lower than the input count\n+\t//! In case we delete rows that were already deleted\n+\t//! Note that \"rows\" is written to to reflect the row ids that were actually deleted\n+\t//! i.e. after calling this function, rows will hold [0..actual_delete_count] row ids of the actually deleted tuples\n \tidx_t Delete(Transaction &transaction, row_t rows[], idx_t count);\n \tvoid CommitDelete(transaction_t commit_id, row_t rows[], idx_t count);\n \ndiff --git a/src/optimizer/cardinality_estimator.cpp b/src/optimizer/cardinality_estimator.cpp\nindex 57bc2b6ebb40..ec0c501f5139 100644\n--- a/src/optimizer/cardinality_estimator.cpp\n+++ b/src/optimizer/cardinality_estimator.cpp\n@@ -392,9 +392,7 @@ void CardinalityEstimator::UpdateTotalDomains(JoinNode *node, LogicalOperator *o\n \t\t\t// Get HLL stats here\n \t\t\tauto actual_binding = relation_column_to_original_column[key];\n \n-\t\t\t// sometimes base stats is null (test_709.test) returns null for base stats while\n-\t\t\t// there is still a catalog table. Anybody know anything about this?\n-\t\t\tauto base_stats = catalog_table->storage->GetStatistics(context, actual_binding.column_index);\n+\t\t\tauto base_stats = catalog_table->GetStatistics(context, actual_binding.column_index);\n \t\t\tif (base_stats) {\n \t\t\t\tcount = base_stats->GetDistinctCount();\n \t\t\t}\ndiff --git a/src/optimizer/pushdown/pushdown_aggregate.cpp b/src/optimizer/pushdown/pushdown_aggregate.cpp\nindex 1ac9e95ef6ef..a79953365407 100644\n--- a/src/optimizer/pushdown/pushdown_aggregate.cpp\n+++ b/src/optimizer/pushdown/pushdown_aggregate.cpp\n@@ -32,21 +32,35 @@ unique_ptr<LogicalOperator> FilterPushdown::PushdownAggregate(unique_ptr<Logical\n \tFilterPushdown child_pushdown(optimizer);\n \tfor (idx_t i = 0; i < filters.size(); i++) {\n \t\tauto &f = *filters[i];\n-\t\t// check if any aggregate or GROUPING functions are in the set\n-\t\tif (f.bindings.find(aggr.aggregate_index) == f.bindings.end() &&\n-\t\t    f.bindings.find(aggr.groupings_index) == f.bindings.end()) {\n-\t\t\t// no aggregate! we can push this down\n-\t\t\t// rewrite any group bindings within the filter\n-\t\t\tf.filter = ReplaceGroupBindings(aggr, move(f.filter));\n-\t\t\t// add the filter to the child node\n-\t\t\tif (child_pushdown.AddFilter(move(f.filter)) == FilterResult::UNSATISFIABLE) {\n-\t\t\t\t// filter statically evaluates to false, strip tree\n-\t\t\t\treturn make_unique<LogicalEmptyResult>(move(op));\n+\t\tif (f.bindings.find(aggr.aggregate_index) != f.bindings.end()) {\n+\t\t\t// filter on aggregate: cannot pushdown\n+\t\t\tcontinue;\n+\t\t}\n+\t\tif (f.bindings.find(aggr.groupings_index) != f.bindings.end()) {\n+\t\t\t// filter on GROUPINGS function: cannot pushdown\n+\t\t\tcontinue;\n+\t\t}\n+\t\t// if there are any empty grouping sets, we cannot push down filters\n+\t\tbool has_empty_grouping_sets = false;\n+\t\tfor (auto &grp : aggr.grouping_sets) {\n+\t\t\tif (grp.empty()) {\n+\t\t\t\thas_empty_grouping_sets = true;\n \t\t\t}\n-\t\t\t// erase the filter from here\n-\t\t\tfilters.erase(filters.begin() + i);\n-\t\t\ti--;\n \t\t}\n+\t\tif (has_empty_grouping_sets) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\t// no aggregate! we can push this down\n+\t\t// rewrite any group bindings within the filter\n+\t\tf.filter = ReplaceGroupBindings(aggr, move(f.filter));\n+\t\t// add the filter to the child node\n+\t\tif (child_pushdown.AddFilter(move(f.filter)) == FilterResult::UNSATISFIABLE) {\n+\t\t\t// filter statically evaluates to false, strip tree\n+\t\t\treturn make_unique<LogicalEmptyResult>(move(op));\n+\t\t}\n+\t\t// erase the filter from here\n+\t\tfilters.erase(filters.begin() + i);\n+\t\ti--;\n \t}\n \tchild_pushdown.GenerateFilters();\n \ndiff --git a/src/planner/expression_binder.cpp b/src/planner/expression_binder.cpp\nindex d37f85344ea0..9e191595d950 100644\n--- a/src/planner/expression_binder.cpp\n+++ b/src/planner/expression_binder.cpp\n@@ -170,6 +170,9 @@ unique_ptr<Expression> ExpressionBinder::Bind(unique_ptr<ParsedExpression> &expr\n \t// bind the main expression\n \tauto error_msg = Bind(&expr, 0, root_expression);\n \tif (!error_msg.empty()) {\n+\t\tif (!CanContainSubqueries()) {\n+\t\t\tthrow BinderException(error_msg);\n+\t\t}\n \t\t// failed to bind: try to bind correlated columns in the expression (if any)\n \t\tbool success = BindCorrelatedColumns(expr);\n \t\tif (!success) {\n@@ -227,4 +230,8 @@ string ExpressionBinder::Bind(unique_ptr<ParsedExpression> *expr, idx_t depth, b\n \treturn string();\n }\n \n+bool ExpressionBinder::CanContainSubqueries() {\n+\treturn true;\n+}\n+\n } // namespace duckdb\ndiff --git a/src/planner/expression_binder/constant_binder.cpp b/src/planner/expression_binder/constant_binder.cpp\nindex a0829dc44ad7..e989aba63603 100644\n--- a/src/planner/expression_binder/constant_binder.cpp\n+++ b/src/planner/expression_binder/constant_binder.cpp\n@@ -26,4 +26,8 @@ string ConstantBinder::UnsupportedAggregateMessage() {\n \treturn clause + \" cannot contain aggregates!\";\n }\n \n+bool ConstantBinder::CanContainSubqueries() {\n+\treturn false;\n+}\n+\n } // namespace duckdb\ndiff --git a/src/storage/data_table.cpp b/src/storage/data_table.cpp\nindex 22af9e6c8433..a579838ed755 100644\n--- a/src/storage/data_table.cpp\n+++ b/src/storage/data_table.cpp\n@@ -1389,6 +1389,9 @@ unique_ptr<BaseStatistics> DataTable::GetStatistics(ClientContext &context, colu\n \t\treturn nullptr;\n \t}\n \tlock_guard<mutex> stats_guard(stats_lock);\n+\tif (column_id >= column_stats.size()) {\n+\t\tthrow InternalException(\"Call to GetStatistics is out of range\");\n+\t}\n \treturn column_stats[column_id]->stats->Copy();\n }\n \ndiff --git a/src/storage/table/chunk_info.cpp b/src/storage/table/chunk_info.cpp\nindex 28716e378ed0..efc8bcb54712 100644\n--- a/src/storage/table/chunk_info.cpp\n+++ b/src/storage/table/chunk_info.cpp\n@@ -185,6 +185,7 @@ idx_t ChunkVectorInfo::Delete(Transaction &transaction, row_t rows[], idx_t coun\n \t\t}\n \t\t// after verifying that there are no conflicts we mark the tuple as deleted\n \t\tdeleted[rows[i]] = transaction.transaction_id;\n+\t\trows[deleted_tuples] = rows[i];\n \t\tdeleted_tuples++;\n \t}\n \treturn deleted_tuples;\ndiff --git a/src/storage/table/row_group.cpp b/src/storage/table/row_group.cpp\nindex ba00a02d4027..17844d317519 100644\n--- a/src/storage/table/row_group.cpp\n+++ b/src/storage/table/row_group.cpp\n@@ -885,9 +885,15 @@ void VersionDeleteState::Flush() {\n \t\treturn;\n \t}\n \t// delete in the current info\n-\tdelete_count += current_info->Delete(transaction, rows, count);\n-\t// now push the delete into the undo buffer\n-\ttransaction.PushDelete(table, current_info, rows, count, base_row + chunk_row);\n+\t// it is possible for delete statements to delete the same tuple multiple times when combined with a USING clause\n+\t// in the current_info->Delete, we check which tuples are actually deleted (excluding duplicate deletions)\n+\t// this is returned in the actual_delete_count\n+\tauto actual_delete_count = current_info->Delete(transaction, rows, count);\n+\tdelete_count += actual_delete_count;\n+\tif (actual_delete_count > 0) {\n+\t\t// now push the delete into the undo buffer, but only if any deletes were actually performed\n+\t\ttransaction.PushDelete(table, current_info, rows, actual_delete_count, base_row + chunk_row);\n+\t}\n \tcount = 0;\n }\n \ndiff --git a/src/transaction/cleanup_state.cpp b/src/transaction/cleanup_state.cpp\nindex dca5cc539f7e..d96342177ccb 100644\n--- a/src/transaction/cleanup_state.cpp\n+++ b/src/transaction/cleanup_state.cpp\n@@ -50,6 +50,7 @@ void CleanupState::CleanupUpdate(UpdateInfo *info) {\n \n void CleanupState::CleanupDelete(DeleteInfo *info) {\n \tauto version_table = info->table;\n+\tD_ASSERT(version_table->info->cardinality >= info->count);\n \tversion_table->info->cardinality -= info->count;\n \tif (version_table->info->indexes.Empty()) {\n \t\t// this table has no indexes: no cleanup to be done\n",
  "test_patch": "diff --git a/test/api/capi/test_capi_data_chunk.cpp b/test/api/capi/test_capi_data_chunk.cpp\nindex 132481ebf356..02c9ddccf5df 100644\n--- a/test/api/capi/test_capi_data_chunk.cpp\n+++ b/test/api/capi/test_capi_data_chunk.cpp\n@@ -39,7 +39,6 @@ TEST_CASE(\"Test table_info incorrect 'is_valid' value for 'dflt_value' column\",\n \t\t\t\tuint64_t *validity = duckdb_vector_get_validity(vector);\n \t\t\t\tbool is_valid = duckdb_validity_row_is_valid(validity, row_idx);\n \n-\t\t\t\tprintf(\"Is row %d, col %d valid? %s\\n\", (int)row_idx, (int)col_idx, is_valid ? \"yes\" : \"no\");\n \t\t\t\tif (col_idx == 4) {\n \t\t\t\t\t//'dflt_value' column\n \t\t\t\t\tREQUIRE(is_valid == false);\ndiff --git a/test/fuzzer/pedro/cardinality_estimator_runtime_issue.test b/test/fuzzer/pedro/cardinality_estimator_runtime_issue.test\nnew file mode 100644\nindex 000000000000..5cbc4d0728c5\n--- /dev/null\n+++ b/test/fuzzer/pedro/cardinality_estimator_runtime_issue.test\n@@ -0,0 +1,19 @@\n+# name: test/fuzzer/pedro/cardinality_estimator_runtime_issue.test\n+# description: Issue #4682: Cardinality estimator runtime issue\n+# group: [pedro]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE t0 AS (SELECT 2);\n+\n+statement ok\n+DELETE FROM t0 USING generate_series(1);\n+\n+query I\n+SELECT * FROM t0\n+----\n+\n+statement ok\n+DELETE FROM t0 USING generate_series(1);\ndiff --git a/test/fuzzer/pedro/having_query_wrong_result.test b/test/fuzzer/pedro/having_query_wrong_result.test\nnew file mode 100644\nindex 000000000000..193498cf7aca\n--- /dev/null\n+++ b/test/fuzzer/pedro/having_query_wrong_result.test\n@@ -0,0 +1,25 @@\n+# name: test/fuzzer/pedro/having_query_wrong_result.test\n+# description: Issue #4680: Having query with wrong result?\n+# group: [pedro]\n+\n+require skip_reload\n+\n+statement ok\n+CREATE SEQUENCE seq;\n+\n+query I\n+SELECT nextval('seq');\n+----\n+1\n+\n+query I\n+SELECT currval('seq');\n+----\n+1\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+query I\n+SELECT 1 FROM (SELECT 1) t0(c0) GROUP BY CUBE (1) HAVING (currval('seq') IS NULL);\n+----\ndiff --git a/test/fuzzer/pedro/natural_join_generated_heap_overflow.test b/test/fuzzer/pedro/natural_join_generated_heap_overflow.test\nnew file mode 100644\nindex 000000000000..48a813d58358\n--- /dev/null\n+++ b/test/fuzzer/pedro/natural_join_generated_heap_overflow.test\n@@ -0,0 +1,25 @@\n+# name: test/fuzzer/pedro/natural_join_generated_heap_overflow.test\n+# description: Issue #4675: Natural join with generated column heap overflow\n+# group: [pedro]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE t0 (c1 AS (1), c0 INT);\n+\n+query I\n+SELECT 1 FROM t0 JOIN (SELECT 8) t1(c0) USING (c0);\n+----\n+\n+query I\n+SELECT 1 FROM t0 NATURAL INNER JOIN (SELECT 8) t1(c0);\n+----\n+\n+statement ok\n+INSERT INTO t0 VALUES (8)\n+\n+query I\n+SELECT 1 FROM t0 NATURAL INNER JOIN (SELECT 8) t1(c0);\n+----\n+1\ndiff --git a/test/fuzzer/pedro/nested_subquery_table_function.test b/test/fuzzer/pedro/nested_subquery_table_function.test\nnew file mode 100644\nindex 000000000000..50a6987059b5\n--- /dev/null\n+++ b/test/fuzzer/pedro/nested_subquery_table_function.test\n@@ -0,0 +1,12 @@\n+# name: test/fuzzer/pedro/nested_subquery_table_function.test\n+# description: Issue #4676: Subquery as Table UDF argument assertion error\n+# group: [pedro]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement error\n+SELECT 1 FROM range((SELECT 1) - 0);\n+\n+statement error\n+SELECT (SELECT 1 FROM range((SELECT 1) - 0));\ndiff --git a/test/fuzzer/pedro/overflow_parsing_interval.test b/test/fuzzer/pedro/overflow_parsing_interval.test\nnew file mode 100644\nindex 000000000000..b65042519b3c\n--- /dev/null\n+++ b/test/fuzzer/pedro/overflow_parsing_interval.test\n@@ -0,0 +1,23 @@\n+# name: test/fuzzer/pedro/overflow_parsing_interval.test\n+# group: [pedro]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+query I\n+SELECT INTERVAL '-9223372036854775808' MICROSECONDS;\n+----\n+-2562047788:00:54.775808\n+\n+query I\n+SELECT INTERVAL '-9223372036854775807' MICROSECONDS;\n+----\n+-2562047788:00:54.775807\n+\n+statement error\n+SELECT INTERVAL '9223372036854775808' MICROSECONDS;\n+\n+query I\n+SELECT INTERVAL '9223372036854775807' MICROSECONDS;\n+----\n+2562047788:00:54.775807\ndiff --git a/test/fuzzer/pedro/prepared_statement_recursive_cte.test b/test/fuzzer/pedro/prepared_statement_recursive_cte.test\nnew file mode 100644\nindex 000000000000..93789ec4b0aa\n--- /dev/null\n+++ b/test/fuzzer/pedro/prepared_statement_recursive_cte.test\n@@ -0,0 +1,25 @@\n+# name: test/fuzzer/pedro/prepared_statement_recursive_cte.test\n+# description: Issue #4681: Prepared statement recursive CTE heap-use-after-free\n+# group: [pedro]\n+\n+require skip_reload\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+PREPARE p0 AS WITH RECURSIVE t1(c2) AS ((SELECT 1) UNION DISTINCT (SELECT (c2 + 1) FROM t1 WHERE (c2 < 3))) SELECT * FROM t1 ORDER BY c2 NULLS LAST;\n+\n+query I\n+EXECUTE p0;\n+----\n+1\n+2\n+3\n+\n+query I\n+EXECUTE p0;\n+----\n+1\n+2\n+3\ndiff --git a/test/fuzzer/pedro/update_default_segv.test b/test/fuzzer/pedro/update_default_segv.test\nnew file mode 100644\nindex 000000000000..6ed4bec09e7a\n--- /dev/null\n+++ b/test/fuzzer/pedro/update_default_segv.test\n@@ -0,0 +1,19 @@\n+# name: test/fuzzer/pedro/update_default_segv.test\n+# description: Issue #4678: Update default SEGV\n+# group: [pedro]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE t0(c0 VARCHAR, c2 AS(1), c3 INT, UNIQUE(c0));\n+\n+statement ok\n+UPDATE t0 SET c0 = DEFAULT;\n+\n+statement ok\n+CREATE TABLE t1(c4 UUID, c0 REAL AS(0));\n+\n+# cannot set default for generated column\n+statement error\n+ALTER TABLE t1 ALTER c0 SET DEFAULT 0;\ndiff --git a/test/sql/aggregate/grouping_sets/grouping_sets_filter.test b/test/sql/aggregate/grouping_sets/grouping_sets_filter.test\nnew file mode 100644\nindex 000000000000..fcf7a2d3a713\n--- /dev/null\n+++ b/test/sql/aggregate/grouping_sets/grouping_sets_filter.test\n@@ -0,0 +1,46 @@\n+# name: test/sql/aggregate/grouping_sets/grouping_sets_filter.test\n+# description: Test grouping sets with filter pushdown\n+# group: [grouping_sets]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+create table students (course VARCHAR, type VARCHAR);\n+\n+statement ok\n+insert into students\n+\t\t(course, type)\n+\tvalues\n+\t\t('CS', 'Bachelor'),\n+\t\t('CS', 'Bachelor'),\n+\t\t('CS', 'PhD'),\n+\t\t('Math', 'Masters'),\n+\t\t('CS', NULL),\n+\t\t('CS', NULL),\n+\t\t('Math', NULL);\n+\n+query II\n+SELECT course, COUNT(*) FROM students GROUP BY GROUPING SETS ((), (course))  HAVING course LIKE 'C%' ORDER BY 1, 2;\n+----\n+CS\t5\n+\n+query II\n+SELECT course, COUNT(*) FROM students GROUP BY GROUPING SETS ((), (course)) HAVING course LIKE 'C%' OR course NOT LIKE 'C%' OR course IS NULL ORDER BY 1, 2;\n+----\n+NULL\t7\n+CS\t5\n+Math\t2\n+\n+# always true: random generates values between 0 and 1\n+query II\n+SELECT course, COUNT(*) FROM students GROUP BY GROUPING SETS ((), (course)) HAVING random()<1000;\n+----\n+NULL\t7\n+CS\t5\n+Math\t2\n+\n+# always false: random generates values between 0 and 1\n+query II\n+SELECT course, COUNT(*) FROM students GROUP BY GROUPING SETS ((), (course)) HAVING random()>1000;\n+----\ndiff --git a/test/sqlsmith/sql_reduce.test b/test/sqlsmith/sql_reduce.test\nindex def81b0c76cb..524dc564c114 100644\n--- a/test/sqlsmith/sql_reduce.test\n+++ b/test/sqlsmith/sql_reduce.test\n@@ -33,15 +33,15 @@ INSERT INTO tbl SELECT NULL FROM (VALUES (1, 2))\n query I\n SELECT * FROM reduce_sql_statement('UPDATE tbl SET i=3, j=4 WHERE z=5') ORDER BY 1\n ----\n-UPDATE tbl SET i=3 WHERE z = 5\n+UPDATE tbl SET i=3 WHERE (z) = (5)\n UPDATE tbl SET i=3, j=4\n UPDATE tbl SET i=3, j=4 WHERE NULL\n-UPDATE tbl SET j=4 WHERE z = 5\n+UPDATE tbl SET j=4 WHERE (z) = (5)\n \n query I\n SELECT * FROM reduce_sql_statement('DELETE FROM a WHERE i >= 2000 AND i < 5000;') ORDER BY 1\n ----\n DELETE FROM a\n+DELETE FROM a WHERE (i) < (5000)\n+DELETE FROM a WHERE (i) >= (2000)\n DELETE FROM a WHERE NULL\n-DELETE FROM a WHERE i < 5000\n-DELETE FROM a WHERE i >= 2000\n",
  "problem_statement": "[Fuzzer] Overflow while parsing interval value\n### What happens?\n\nRun this query:\r\n```\r\nSELECT INTERVAL '-9223372036854775808' MICROSECONDS;\r\n```\r\nThe undefined behavior sanitizer will report:\r\nsrc/include/duckdb/common/types/cast_helpers.hpp:525:14: runtime error: negation of -9223372036854775808 cannot be represented in type 'int64_t' (aka 'long'); cast to an unsigned type to negate this value to itself\r\nSUMMARY: UndefinedBehaviorSanitizer: undefined-behavior src/include/duckdb/common/types/cast_helpers.hpp:525:14 in\r\n\r\nThe negation of -9223372036854775808 cannot be represented in int64_t, maybe another parsing technique is needed.\n\n### To Reproduce\n\nRun the statement above.\n\n### OS:\n\nLinux\n\n### DuckDB Version:\n\nlatest from sources\n\n### DuckDB Client:\n\nShell\n\n### Full Name:\n\nPedro Ferreira\n\n### Affiliation:\n\nHuawei\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n[Fuzzer] Cardinality estimator runtime issue\n### What happens?\n\nRun the statements:\r\n```\r\nCREATE TABLE t0 AS (SELECT 2);\r\nDELETE FROM t0 USING generate_series(1);\r\nDELETE FROM t0 USING generate_series(1);\r\n```\r\nThe undefined behavior sanitizer will report: \r\nsrc/optimizer/cardinality_estimator.cpp:555:55: runtime error: 1.84467e+19 is outside the range of representable values of type 'unsigned long'\r\nSUMMARY: UndefinedBehaviorSanitizer: undefined-behavior src/optimizer/cardinality_estimator.cpp:555:55 in\r\n\r\nThe second DELETE statement should succeed.\n\n### To Reproduce\n\nRun the statements above.\n\n### OS:\n\nLinux\n\n### DuckDB Version:\n\nlatest from sources\n\n### DuckDB Client:\n\nShell\n\n### Full Name:\n\nPedro Ferreira\n\n### Affiliation:\n\nHuawei\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "\n",
  "created_at": "2022-09-14T09:09:06Z"
}