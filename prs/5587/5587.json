{
  "repo": "duckdb/duckdb",
  "pull_number": 5587,
  "instance_id": "duckdb__duckdb-5587",
  "issue_numbers": [
    "5502"
  ],
  "base_commit": "0367c35c9b72c4882c01330925ff76c6a6deca9c",
  "patch": "diff --git a/extension/httpfs/s3fs.cpp b/extension/httpfs/s3fs.cpp\nindex ea51765546f9..898c79db14a9 100644\n--- a/extension/httpfs/s3fs.cpp\n+++ b/extension/httpfs/s3fs.cpp\n@@ -558,7 +558,7 @@ string S3FileSystem::GetPayloadHash(char *buffer, idx_t buffer_len) {\n }\n \n static string get_full_s3_url(S3AuthParams &auth_params, ParsedS3Url parsed_url) {\n-\tstring full_url = parsed_url.http_proto + parsed_url.host + parsed_url.path;\n+\tstring full_url = parsed_url.http_proto + parsed_url.host + S3FileSystem::UrlEncode(parsed_url.path);\n \n \tif (!parsed_url.query_param.empty()) {\n \t\tfull_url += \"?\" + parsed_url.query_param;\n",
  "test_patch": "diff --git a/test/sql/copy/s3/url_encode.test b/test/sql/copy/s3/url_encode.test\nindex e22d2c3858c1..a88f0953f9af 100644\n--- a/test/sql/copy/s3/url_encode.test\n+++ b/test/sql/copy/s3/url_encode.test\n@@ -25,6 +25,9 @@ COPY test_1 TO 's3://test-bucket-public/url_encode/just because you can doesnt m\n statement ok\n COPY test_2 TO 's3://test-bucket-public/url_encode/just+dont+use+plus+or+spaces+please.parquet' (FORMAT 'parquet');\n \n+statement ok\n+COPY test_3 TO 's3://test-bucket-public/url_encode/should:avoid:using:colon:in:paths.parquet' (FORMAT 'parquet');\n+\n # For S3 urls spaces are fine\n query I\n SELECT * FROM \"s3://test-bucket-public/url_encode/just because you can doesnt mean you should.parquet\" LIMIT 1;\n@@ -37,6 +40,12 @@ SELECT * FROM \"s3://test-bucket-public/url_encode/just+dont+use+plus+or+spaces+p\n ----\n 2\n \n+# Colons in S3 urls are encoded by duckdb internaly like boto3 (issue #5502)\n+query I\n+SELECT * FROM \"s3://test-bucket-public/url_encode/should:avoid:using:colon:in:paths.parquet\" LIMIT 1;\n+----\n+3\n+\n # NOTE! For HTTP(s) urls, the + symbol is not encoded by duckdb, leaving it up to the server to decide if it should be interpreted\n # as a space or a plus. In the case of AWS S3, they are interpreted as encoded spaces, however Minio does not\n #query I\n",
  "problem_statement": "AWS S3 keys containing special characters return 404 error\n### What happens?\n\nWhen trying to query parquet files in AWS S3 bucket with HTTPFS extension I get an error (404 Not Found) for files that contain special characters like `+`,`:`.\r\n\r\nAccording to docs these characters are allowed but may need special handling: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n\n### To Reproduce\n\nSee example queries (files are in region eu-central-1):\r\ndoesn't work:\r\n```sql\r\nselect * from parquet_scan('s3://tt-osm-changesets/replication/dag_run_date=2022-11-26/2022-11-26T13:51:14.934709+00:00.parquet');\r\n```\r\nglob resolves the names correctly it's just the request to file that fails:\r\n```sql\r\nselect * from parquet_scan('s3://tt-osm-changesets/replication/*/*.parquet');\r\n```\r\nworks:\r\n```sql\r\nselect * from parquet_scan('s3://tt-osm-changesets/replication/dag_run_date=2022-11-26/20221126145858.parquet');\r\n```\n\n### OS:\n\nLinux\n\n### DuckDB Version:\n\nv0.6.0 2213f9c946\n\n### DuckDB Client:\n\nCLI, DBeaver\n\n### Full Name:\n\nTomasz Tara\u015b\n\n### Affiliation:\n\nDXC\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "",
  "created_at": "2022-12-04T07:45:23Z"
}