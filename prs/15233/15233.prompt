You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
"out of range" error when reading parquet with unsigned tinyint or unsigned smallint
### What happens?

"out of range" error when reading parquet with unsigned tinyint (unsigned int8) or unsigned smallint (unsigned int16).  

Error message for tinyint:
```
Invalid Input Error: Failed to cast value: Type UINT32 with value 4294967295 can't be cast because the value is out of range for the destination type UINT8
```

Error message for smallint:
```
Invalid Input Error: Failed to cast value: Type UINT32 with value 4294967295 can't be cast because the value is out of range for the destination type UINT16
```

### To Reproduce

Create a **parquet** file **containing at least 1 row** with this **schema**:
```
message schema {
  OPTIONAL INT32 hcol0 (INTEGER(8,false));
}
```
or
```
message schema {
  OPTIONAL INT32 hcol0 (INTEGER(16,false));
}
```

Use Duckdb to query the parquet:
```sql
select hcol0 from READ_PARQUET('data.parquet');
```

Example parquet files (the extension is put as `.txt` to be able to upload here)
[utinyint.txt](https://github.com/user-attachments/files/18060837/utinyint.txt)
[usmallint.txt](https://github.com/user-attachments/files/18060836/usmallint.txt)

```sql
select hcol0 from read_parquet('utinyint.txt');
select hcol0 from read_parquet('usmallint.txt');
```

### OS:

linux

### DuckDB Version:

1.1.3

### DuckDB Client:

CLI

### Hardware:

_No response_

### Full Name:

Dat Bui

### Affiliation:

holistics.io

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have
"out of range" error when reading parquet with unsigned tinyint or unsigned smallint
### What happens?

"out of range" error when reading parquet with unsigned tinyint (unsigned int8) or unsigned smallint (unsigned int16).  

Error message for tinyint:
```
Invalid Input Error: Failed to cast value: Type UINT32 with value 4294967295 can't be cast because the value is out of range for the destination type UINT8
```

Error message for smallint:
```
Invalid Input Error: Failed to cast value: Type UINT32 with value 4294967295 can't be cast because the value is out of range for the destination type UINT16
```

### To Reproduce

Create a **parquet** file **containing at least 1 row** with this **schema**:
```
message schema {
  OPTIONAL INT32 hcol0 (INTEGER(8,false));
}
```
or
```
message schema {
  OPTIONAL INT32 hcol0 (INTEGER(16,false));
}
```

Use Duckdb to query the parquet:
```sql
select hcol0 from READ_PARQUET('data.parquet');
```

Example parquet files (the extension is put as `.txt` to be able to upload here)
[utinyint.txt](https://github.com/user-attachments/files/18060837/utinyint.txt)
[usmallint.txt](https://github.com/user-attachments/files/18060836/usmallint.txt)

```sql
select hcol0 from read_parquet('utinyint.txt');
select hcol0 from read_parquet('usmallint.txt');
```

### OS:

linux

### DuckDB Version:

1.1.3

### DuckDB Client:

CLI

### Hardware:

_No response_

### Full Name:

Dat Bui

### Affiliation:

holistics.io

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://duckdb.org/docs/installation/) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of extension/parquet/include/parquet_statistics.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // parquet_statistics.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===/
8: 
9: #pragma once
10: 
11: #include "duckdb.hpp"
12: #ifndef DUCKDB_AMALGAMATION
13: #include "duckdb/storage/statistics/base_statistics.hpp"
14: #endif
15: #include "parquet_types.h"
16: 
17: namespace duckdb {
18: 
19: using duckdb_parquet::ColumnChunk;
20: using duckdb_parquet::SchemaElement;
21: 
22: struct LogicalType;
23: class ColumnReader;
24: class ResizeableBuffer;
25: 
26: struct ParquetStatisticsUtils {
27: 
28: 	static unique_ptr<BaseStatistics> TransformColumnStatistics(const ColumnReader &reader,
29: 	                                                            const vector<ColumnChunk> &columns);
30: 
31: 	static Value ConvertValue(const LogicalType &type, const duckdb_parquet::SchemaElement &schema_ele,
32: 	                          const std::string &stats);
33: 
34: 	static bool BloomFilterSupported(const LogicalTypeId &type_id);
35: 
36: 	static bool BloomFilterExcludes(const TableFilter &filter, const duckdb_parquet::ColumnMetaData &column_meta_data,
37: 	                                duckdb_apache::thrift::protocol::TProtocol &file_proto, Allocator &allocator);
38: };
39: 
40: class ParquetBloomFilter {
41: 	static constexpr const idx_t DEFAULT_BLOCK_COUNT = 32; // 4k filter
42: 
43: public:
44: 	ParquetBloomFilter(idx_t num_entries, double bloom_filter_false_positive_ratio);
45: 	ParquetBloomFilter(unique_ptr<ResizeableBuffer> data_p);
46: 	void FilterInsert(uint64_t x);
47: 	bool FilterCheck(uint64_t x);
48: 	void Shrink(idx_t new_block_count);
49: 	double OneRatio();
50: 	ResizeableBuffer *Get();
51: 
52: private:
53: 	unique_ptr<ResizeableBuffer> data;
54: 	idx_t block_count;
55: };
56: 
57: // see https://github.com/apache/parquet-format/blob/master/BloomFilter.md
58: 
59: struct ParquetBloomBlock {
60: 	struct ParquetBloomMaskResult {
61: 		uint8_t bit_set[8] = {0};
62: 	};
63: 
64: 	uint32_t block[8] = {0};
65: 
66: 	static bool check_bit(uint32_t &x, const uint8_t i) {
67: 		D_ASSERT(i < 32);
68: 		return (x >> i) & (uint32_t)1;
69: 	}
70: 
71: 	static void set_bit(uint32_t &x, const uint8_t i) {
72: 		D_ASSERT(i < 32);
73: 		x |= (uint32_t)1 << i;
74: 		D_ASSERT(check_bit(x, i));
75: 	}
76: 
77: 	static ParquetBloomMaskResult Mask(uint32_t x) {
78: 		static const uint32_t parquet_bloom_salt[8] = {0x47b6137bU, 0x44974d91U, 0x8824ad5bU, 0xa2b7289dU,
79: 		                                               0x705495c7U, 0x2df1424bU, 0x9efc4947U, 0x5c6bfb31U};
80: 		ParquetBloomMaskResult result;
81: 		for (idx_t i = 0; i < 8; i++) {
82: 			result.bit_set[i] = (x * parquet_bloom_salt[i]) >> 27;
83: 		}
84: 		return result;
85: 	}
86: 
87: 	static void BlockInsert(ParquetBloomBlock &b, uint32_t x) {
88: 		auto masked = Mask(x);
89: 		for (idx_t i = 0; i < 8; i++) {
90: 			set_bit(b.block[i], masked.bit_set[i]);
91: 			D_ASSERT(check_bit(b.block[i], masked.bit_set[i]));
92: 		}
93: 	}
94: 
95: 	static bool BlockCheck(ParquetBloomBlock &b, uint32_t x) {
96: 		auto masked = Mask(x);
97: 		for (idx_t i = 0; i < 8; i++) {
98: 			if (!check_bit(b.block[i], masked.bit_set[i])) {
99: 				return false;
100: 			}
101: 		}
102: 		return true;
103: 	}
104: };
105: 
106: } // namespace duckdb
[end of extension/parquet/include/parquet_statistics.hpp]
[start of extension/parquet/parquet_statistics.cpp]
1: #include "parquet_statistics.hpp"
2: 
3: #include "duckdb.hpp"
4: #include "parquet_decimal_utils.hpp"
5: #include "parquet_timestamp.hpp"
6: #include "string_column_reader.hpp"
7: #include "struct_column_reader.hpp"
8: #include "zstd/common/xxhash.hpp"
9: 
10: #ifndef DUCKDB_AMALGAMATION
11: #include "duckdb/common/types/blob.hpp"
12: #include "duckdb/common/types/time.hpp"
13: #include "duckdb/common/types/value.hpp"
14: #include "duckdb/storage/statistics/struct_stats.hpp"
15: #include "duckdb/planner/filter/constant_filter.hpp"
16: #endif
17: 
18: namespace duckdb {
19: 
20: using duckdb_parquet::ConvertedType;
21: using duckdb_parquet::Type;
22: 
23: static unique_ptr<BaseStatistics> CreateNumericStats(const LogicalType &type,
24:                                                      const duckdb_parquet::SchemaElement &schema_ele,
25:                                                      const duckdb_parquet::Statistics &parquet_stats) {
26: 	auto stats = NumericStats::CreateUnknown(type);
27: 
28: 	// for reasons unknown to science, Parquet defines *both* `min` and `min_value` as well as `max` and
29: 	// `max_value`. All are optional. such elegance.
30: 	Value min;
31: 	Value max;
32: 	if (parquet_stats.__isset.min_value) {
33: 		min = ParquetStatisticsUtils::ConvertValue(type, schema_ele, parquet_stats.min_value).DefaultCastAs(type);
34: 	} else if (parquet_stats.__isset.min) {
35: 		min = ParquetStatisticsUtils::ConvertValue(type, schema_ele, parquet_stats.min).DefaultCastAs(type);
36: 	} else {
37: 		min = Value(type);
38: 	}
39: 	if (parquet_stats.__isset.max_value) {
40: 		max = ParquetStatisticsUtils::ConvertValue(type, schema_ele, parquet_stats.max_value).DefaultCastAs(type);
41: 	} else if (parquet_stats.__isset.max) {
42: 		max = ParquetStatisticsUtils::ConvertValue(type, schema_ele, parquet_stats.max).DefaultCastAs(type);
43: 	} else {
44: 		max = Value(type);
45: 	}
46: 	NumericStats::SetMin(stats, min);
47: 	NumericStats::SetMax(stats, max);
48: 	return stats.ToUnique();
49: }
50: 
51: Value ParquetStatisticsUtils::ConvertValue(const LogicalType &type, const duckdb_parquet::SchemaElement &schema_ele,
52:                                            const std::string &stats) {
53: 	auto stats_data = const_data_ptr_cast(stats.c_str());
54: 	switch (type.id()) {
55: 	case LogicalTypeId::BOOLEAN: {
56: 		if (stats.size() != sizeof(bool)) {
57: 			throw InvalidInputException("Incorrect stats size for type BOOLEAN");
58: 		}
59: 		return Value::BOOLEAN(Load<bool>(stats_data));
60: 	}
61: 	case LogicalTypeId::UTINYINT:
62: 	case LogicalTypeId::USMALLINT:
63: 	case LogicalTypeId::UINTEGER:
64: 		if (stats.size() != sizeof(uint32_t)) {
65: 			throw InvalidInputException("Incorrect stats size for type UINTEGER");
66: 		}
67: 		return Value::UINTEGER(Load<uint32_t>(stats_data));
68: 	case LogicalTypeId::UBIGINT:
69: 		if (stats.size() != sizeof(uint64_t)) {
70: 			throw InvalidInputException("Incorrect stats size for type UBIGINT");
71: 		}
72: 		return Value::UBIGINT(Load<uint64_t>(stats_data));
73: 	case LogicalTypeId::TINYINT:
74: 	case LogicalTypeId::SMALLINT:
75: 	case LogicalTypeId::INTEGER:
76: 		if (stats.size() != sizeof(int32_t)) {
77: 			throw InvalidInputException("Incorrect stats size for type INTEGER");
78: 		}
79: 		return Value::INTEGER(Load<int32_t>(stats_data));
80: 	case LogicalTypeId::BIGINT:
81: 		if (stats.size() != sizeof(int64_t)) {
82: 			throw InvalidInputException("Incorrect stats size for type BIGINT");
83: 		}
84: 		return Value::BIGINT(Load<int64_t>(stats_data));
85: 	case LogicalTypeId::FLOAT: {
86: 		if (stats.size() != sizeof(float)) {
87: 			throw InvalidInputException("Incorrect stats size for type FLOAT");
88: 		}
89: 		auto val = Load<float>(stats_data);
90: 		if (!Value::FloatIsFinite(val)) {
91: 			return Value();
92: 		}
93: 		return Value::FLOAT(val);
94: 	}
95: 	case LogicalTypeId::DOUBLE: {
96: 		switch (schema_ele.type) {
97: 		case Type::FIXED_LEN_BYTE_ARRAY:
98: 		case Type::BYTE_ARRAY:
99: 			// decimals cast to double
100: 			return Value::DOUBLE(ParquetDecimalUtils::ReadDecimalValue<double>(stats_data, stats.size(), schema_ele));
101: 		default:
102: 			break;
103: 		}
104: 		if (stats.size() != sizeof(double)) {
105: 			throw InvalidInputException("Incorrect stats size for type DOUBLE");
106: 		}
107: 		auto val = Load<double>(stats_data);
108: 		if (!Value::DoubleIsFinite(val)) {
109: 			return Value();
110: 		}
111: 		return Value::DOUBLE(val);
112: 	}
113: 	case LogicalTypeId::DECIMAL: {
114: 		auto width = DecimalType::GetWidth(type);
115: 		auto scale = DecimalType::GetScale(type);
116: 		switch (schema_ele.type) {
117: 		case Type::INT32: {
118: 			if (stats.size() != sizeof(int32_t)) {
119: 				throw InvalidInputException("Incorrect stats size for type %s", type.ToString());
120: 			}
121: 			return Value::DECIMAL(Load<int32_t>(stats_data), width, scale);
122: 		}
123: 		case Type::INT64: {
124: 			if (stats.size() != sizeof(int64_t)) {
125: 				throw InvalidInputException("Incorrect stats size for type %s", type.ToString());
126: 			}
127: 			return Value::DECIMAL(Load<int64_t>(stats_data), width, scale);
128: 		}
129: 		case Type::BYTE_ARRAY:
130: 		case Type::FIXED_LEN_BYTE_ARRAY:
131: 			switch (type.InternalType()) {
132: 			case PhysicalType::INT16:
133: 				return Value::DECIMAL(
134: 				    ParquetDecimalUtils::ReadDecimalValue<int16_t>(stats_data, stats.size(), schema_ele), width, scale);
135: 			case PhysicalType::INT32:
136: 				return Value::DECIMAL(
137: 				    ParquetDecimalUtils::ReadDecimalValue<int32_t>(stats_data, stats.size(), schema_ele), width, scale);
138: 			case PhysicalType::INT64:
139: 				return Value::DECIMAL(
140: 				    ParquetDecimalUtils::ReadDecimalValue<int64_t>(stats_data, stats.size(), schema_ele), width, scale);
141: 			case PhysicalType::INT128:
142: 				return Value::DECIMAL(
143: 				    ParquetDecimalUtils::ReadDecimalValue<hugeint_t>(stats_data, stats.size(), schema_ele), width,
144: 				    scale);
145: 			default:
146: 				throw InvalidInputException("Unsupported internal type for decimal");
147: 			}
148: 		default:
149: 			throw InternalException("Unsupported internal type for decimal?..");
150: 		}
151: 	}
152: 	case LogicalTypeId::VARCHAR:
153: 	case LogicalTypeId::BLOB:
154: 		if (type.id() == LogicalTypeId::BLOB || !Value::StringIsValid(stats)) {
155: 			return Value(Blob::ToString(string_t(stats)));
156: 		}
157: 		return Value(stats);
158: 	case LogicalTypeId::DATE:
159: 		if (stats.size() != sizeof(int32_t)) {
160: 			throw InvalidInputException("Incorrect stats size for type DATE");
161: 		}
162: 		return Value::DATE(date_t(Load<int32_t>(stats_data)));
163: 	case LogicalTypeId::TIME: {
164: 		int64_t val;
165: 		if (stats.size() == sizeof(int32_t)) {
166: 			val = Load<int32_t>(stats_data);
167: 		} else if (stats.size() == sizeof(int64_t)) {
168: 			val = Load<int64_t>(stats_data);
169: 		} else {
170: 			throw InvalidInputException("Incorrect stats size for type TIME");
171: 		}
172: 		if (schema_ele.__isset.logicalType && schema_ele.logicalType.__isset.TIME) {
173: 			// logical type
174: 			if (schema_ele.logicalType.TIME.unit.__isset.MILLIS) {
175: 				return Value::TIME(Time::FromTimeMs(val));
176: 			} else if (schema_ele.logicalType.TIME.unit.__isset.NANOS) {
177: 				return Value::TIME(Time::FromTimeNs(val));
178: 			} else if (schema_ele.logicalType.TIME.unit.__isset.MICROS) {
179: 				return Value::TIME(dtime_t(val));
180: 			} else {
181: 				throw InternalException("Time logicalType is set but unit is not defined");
182: 			}
183: 		}
184: 		if (schema_ele.converted_type == duckdb_parquet::ConvertedType::TIME_MILLIS) {
185: 			return Value::TIME(Time::FromTimeMs(val));
186: 		} else {
187: 			return Value::TIME(dtime_t(val));
188: 		}
189: 	}
190: 	case LogicalTypeId::TIME_TZ: {
191: 		int64_t val;
192: 		if (stats.size() == sizeof(int32_t)) {
193: 			val = Load<int32_t>(stats_data);
194: 		} else if (stats.size() == sizeof(int64_t)) {
195: 			val = Load<int64_t>(stats_data);
196: 		} else {
197: 			throw InvalidInputException("Incorrect stats size for type TIMETZ");
198: 		}
199: 		if (schema_ele.__isset.logicalType && schema_ele.logicalType.__isset.TIME) {
200: 			// logical type
201: 			if (schema_ele.logicalType.TIME.unit.__isset.MILLIS) {
202: 				return Value::TIMETZ(ParquetIntToTimeMsTZ(NumericCast<int32_t>(val)));
203: 			} else if (schema_ele.logicalType.TIME.unit.__isset.MICROS) {
204: 				return Value::TIMETZ(ParquetIntToTimeTZ(val));
205: 			} else if (schema_ele.logicalType.TIME.unit.__isset.NANOS) {
206: 				return Value::TIMETZ(ParquetIntToTimeNsTZ(val));
207: 			} else {
208: 				throw InternalException("Time With Time Zone logicalType is set but unit is not defined");
209: 			}
210: 		}
211: 		return Value::TIMETZ(ParquetIntToTimeTZ(val));
212: 	}
213: 	case LogicalTypeId::TIMESTAMP:
214: 	case LogicalTypeId::TIMESTAMP_TZ: {
215: 		timestamp_t timestamp_value;
216: 		if (schema_ele.type == Type::INT96) {
217: 			if (stats.size() != sizeof(Int96)) {
218: 				throw InvalidInputException("Incorrect stats size for type TIMESTAMP");
219: 			}
220: 			timestamp_value = ImpalaTimestampToTimestamp(Load<Int96>(stats_data));
221: 		} else {
222: 			D_ASSERT(schema_ele.type == Type::INT64);
223: 			if (stats.size() != sizeof(int64_t)) {
224: 				throw InvalidInputException("Incorrect stats size for type TIMESTAMP");
225: 			}
226: 			auto val = Load<int64_t>(stats_data);
227: 			if (schema_ele.__isset.logicalType && schema_ele.logicalType.__isset.TIMESTAMP) {
228: 				// logical type
229: 				if (schema_ele.logicalType.TIMESTAMP.unit.__isset.MILLIS) {
230: 					timestamp_value = Timestamp::FromEpochMs(val);
231: 				} else if (schema_ele.logicalType.TIMESTAMP.unit.__isset.NANOS) {
232: 					timestamp_value = Timestamp::FromEpochNanoSeconds(val);
233: 				} else if (schema_ele.logicalType.TIMESTAMP.unit.__isset.MICROS) {
234: 					timestamp_value = timestamp_t(val);
235: 				} else {
236: 					throw InternalException("Timestamp logicalType is set but unit is not defined");
237: 				}
238: 			} else if (schema_ele.converted_type == duckdb_parquet::ConvertedType::TIMESTAMP_MILLIS) {
239: 				timestamp_value = Timestamp::FromEpochMs(val);
240: 			} else {
241: 				timestamp_value = timestamp_t(val);
242: 			}
243: 		}
244: 		if (type.id() == LogicalTypeId::TIMESTAMP_TZ) {
245: 			return Value::TIMESTAMPTZ(timestamp_tz_t(timestamp_value));
246: 		}
247: 		return Value::TIMESTAMP(timestamp_value);
248: 	}
249: 	case LogicalTypeId::TIMESTAMP_NS: {
250: 		timestamp_ns_t timestamp_value;
251: 		if (schema_ele.type == Type::INT96) {
252: 			if (stats.size() != sizeof(Int96)) {
253: 				throw InvalidInputException("Incorrect stats size for type TIMESTAMP_NS");
254: 			}
255: 			timestamp_value = ImpalaTimestampToTimestampNS(Load<Int96>(stats_data));
256: 		} else {
257: 			D_ASSERT(schema_ele.type == Type::INT64);
258: 			if (stats.size() != sizeof(int64_t)) {
259: 				throw InvalidInputException("Incorrect stats size for type TIMESTAMP_NS");
260: 			}
261: 			auto val = Load<int64_t>(stats_data);
262: 			if (schema_ele.__isset.logicalType && schema_ele.logicalType.__isset.TIMESTAMP) {
263: 				// logical type
264: 				if (schema_ele.logicalType.TIMESTAMP.unit.__isset.MILLIS) {
265: 					timestamp_value = ParquetTimestampMsToTimestampNs(val);
266: 				} else if (schema_ele.logicalType.TIMESTAMP.unit.__isset.NANOS) {
267: 					timestamp_value = ParquetTimestampNsToTimestampNs(val);
268: 				} else if (schema_ele.logicalType.TIMESTAMP.unit.__isset.MICROS) {
269: 					timestamp_value = ParquetTimestampUsToTimestampNs(val);
270: 				} else {
271: 					throw InternalException("Timestamp (NS) logicalType is set but unit is unknown");
272: 				}
273: 			} else if (schema_ele.converted_type == duckdb_parquet::ConvertedType::TIMESTAMP_MILLIS) {
274: 				timestamp_value = ParquetTimestampMsToTimestampNs(val);
275: 			} else {
276: 				timestamp_value = ParquetTimestampUsToTimestampNs(val);
277: 			}
278: 		}
279: 		return Value::TIMESTAMPNS(timestamp_value);
280: 	}
281: 	default:
282: 		throw InternalException("Unsupported type for stats %s", type.ToString());
283: 	}
284: }
285: 
286: unique_ptr<BaseStatistics> ParquetStatisticsUtils::TransformColumnStatistics(const ColumnReader &reader,
287:                                                                              const vector<ColumnChunk> &columns) {
288: 
289: 	// Not supported types
290: 	if (reader.Type().id() == LogicalTypeId::ARRAY || reader.Type().id() == LogicalTypeId::MAP ||
291: 	    reader.Type().id() == LogicalTypeId::LIST) {
292: 		return nullptr;
293: 	}
294: 
295: 	unique_ptr<BaseStatistics> row_group_stats;
296: 
297: 	// Structs are handled differently (they dont have stats)
298: 	if (reader.Type().id() == LogicalTypeId::STRUCT) {
299: 		auto struct_stats = StructStats::CreateUnknown(reader.Type());
300: 		auto &struct_reader = reader.Cast<StructColumnReader>();
301: 		// Recurse into child readers
302: 		for (idx_t i = 0; i < struct_reader.child_readers.size(); i++) {
303: 			if (!struct_reader.child_readers[i]) {
304: 				continue;
305: 			}
306: 			auto &child_reader = *struct_reader.child_readers[i];
307: 			auto child_stats = ParquetStatisticsUtils::TransformColumnStatistics(child_reader, columns);
308: 			StructStats::SetChildStats(struct_stats, i, std::move(child_stats));
309: 		}
310: 		row_group_stats = struct_stats.ToUnique();
311: 
312: 		// null count is generic
313: 		if (row_group_stats) {
314: 			row_group_stats->Set(StatsInfo::CAN_HAVE_NULL_AND_VALID_VALUES);
315: 		}
316: 		return row_group_stats;
317: 	}
318: 
319: 	// Otherwise, its a standard column with stats
320: 
321: 	auto &column_chunk = columns[reader.FileIdx()];
322: 	if (!column_chunk.__isset.meta_data || !column_chunk.meta_data.__isset.statistics) {
323: 		// no stats present for row group
324: 		return nullptr;
325: 	}
326: 	auto &parquet_stats = column_chunk.meta_data.statistics;
327: 
328: 	auto &type = reader.Type();
329: 	auto &s_ele = reader.Schema();
330: 
331: 	switch (type.id()) {
332: 	case LogicalTypeId::UTINYINT:
333: 	case LogicalTypeId::USMALLINT:
334: 	case LogicalTypeId::UINTEGER:
335: 	case LogicalTypeId::UBIGINT:
336: 	case LogicalTypeId::TINYINT:
337: 	case LogicalTypeId::SMALLINT:
338: 	case LogicalTypeId::INTEGER:
339: 	case LogicalTypeId::BIGINT:
340: 	case LogicalTypeId::FLOAT:
341: 	case LogicalTypeId::DOUBLE:
342: 	case LogicalTypeId::DATE:
343: 	case LogicalTypeId::TIME:
344: 	case LogicalTypeId::TIME_TZ:
345: 	case LogicalTypeId::TIMESTAMP:
346: 	case LogicalTypeId::TIMESTAMP_TZ:
347: 	case LogicalTypeId::TIMESTAMP_SEC:
348: 	case LogicalTypeId::TIMESTAMP_MS:
349: 	case LogicalTypeId::TIMESTAMP_NS:
350: 	case LogicalTypeId::DECIMAL:
351: 		row_group_stats = CreateNumericStats(type, s_ele, parquet_stats);
352: 		break;
353: 	case LogicalTypeId::VARCHAR: {
354: 		auto string_stats = StringStats::CreateEmpty(type);
355: 		if (parquet_stats.__isset.min_value) {
356: 			StringColumnReader::VerifyString(parquet_stats.min_value.c_str(), parquet_stats.min_value.size(), true);
357: 			StringStats::Update(string_stats, parquet_stats.min_value);
358: 		} else if (parquet_stats.__isset.min) {
359: 			StringColumnReader::VerifyString(parquet_stats.min.c_str(), parquet_stats.min.size(), true);
360: 			StringStats::Update(string_stats, parquet_stats.min);
361: 		} else {
362: 			return nullptr;
363: 		}
364: 		if (parquet_stats.__isset.max_value) {
365: 			StringColumnReader::VerifyString(parquet_stats.max_value.c_str(), parquet_stats.max_value.size(), true);
366: 			StringStats::Update(string_stats, parquet_stats.max_value);
367: 		} else if (parquet_stats.__isset.max) {
368: 			StringColumnReader::VerifyString(parquet_stats.max.c_str(), parquet_stats.max.size(), true);
369: 			StringStats::Update(string_stats, parquet_stats.max);
370: 		} else {
371: 			return nullptr;
372: 		}
373: 		StringStats::SetContainsUnicode(string_stats);
374: 		StringStats::ResetMaxStringLength(string_stats);
375: 		row_group_stats = string_stats.ToUnique();
376: 		break;
377: 	}
378: 	default:
379: 		// no stats for you
380: 		break;
381: 	} // end of type switch
382: 
383: 	// null count is generic
384: 	if (row_group_stats) {
385: 		row_group_stats->Set(StatsInfo::CAN_HAVE_NULL_AND_VALID_VALUES);
386: 		if (parquet_stats.__isset.null_count && parquet_stats.null_count == 0) {
387: 			row_group_stats->Set(StatsInfo::CANNOT_HAVE_NULL_VALUES);
388: 		}
389: 	}
390: 	return row_group_stats;
391: }
392: 
393: static bool HasFilterConstants(const TableFilter &duckdb_filter) {
394: 	switch (duckdb_filter.filter_type) {
395: 	case TableFilterType::CONSTANT_COMPARISON: {
396: 		auto &constant_filter = duckdb_filter.Cast<ConstantFilter>();
397: 		return (constant_filter.comparison_type == ExpressionType::COMPARE_EQUAL && !constant_filter.constant.IsNull());
398: 	}
399: 	case TableFilterType::CONJUNCTION_AND: {
400: 		auto &conjunction_and_filter = duckdb_filter.Cast<ConjunctionAndFilter>();
401: 		bool child_has_constant = false;
402: 		for (auto &child_filter : conjunction_and_filter.child_filters) {
403: 			child_has_constant |= HasFilterConstants(*child_filter);
404: 		}
405: 		return child_has_constant;
406: 	}
407: 	case TableFilterType::CONJUNCTION_OR: {
408: 		auto &conjunction_or_filter = duckdb_filter.Cast<ConjunctionOrFilter>();
409: 		bool child_has_constant = false;
410: 		for (auto &child_filter : conjunction_or_filter.child_filters) {
411: 			child_has_constant |= HasFilterConstants(*child_filter);
412: 		}
413: 		return child_has_constant;
414: 	}
415: 	default:
416: 		return false;
417: 	}
418: }
419: 
420: template <class T>
421: uint64_t ValueXH64FixedWidth(const Value &constant) {
422: 	T val = constant.GetValue<T>();
423: 	return duckdb_zstd::XXH64(&val, sizeof(val), 0);
424: }
425: 
426: // TODO we can only this if the parquet representation of the type exactly matches the duckdb rep!
427: // TODO TEST THIS!
428: // TODO perhaps we can re-use some writer infra here
429: static uint64_t ValueXXH64(const Value &constant) {
430: 	switch (constant.type().InternalType()) {
431: 	case PhysicalType::UINT8:
432: 		return ValueXH64FixedWidth<int32_t>(constant);
433: 	case PhysicalType::INT8:
434: 		return ValueXH64FixedWidth<int32_t>(constant);
435: 	case PhysicalType::UINT16:
436: 		return ValueXH64FixedWidth<int32_t>(constant);
437: 	case PhysicalType::INT16:
438: 		return ValueXH64FixedWidth<int32_t>(constant);
439: 	case PhysicalType::UINT32:
440: 		return ValueXH64FixedWidth<uint32_t>(constant);
441: 	case PhysicalType::INT32:
442: 		return ValueXH64FixedWidth<int32_t>(constant);
443: 	case PhysicalType::UINT64:
444: 		return ValueXH64FixedWidth<uint64_t>(constant);
445: 	case PhysicalType::INT64:
446: 		return ValueXH64FixedWidth<int64_t>(constant);
447: 	case PhysicalType::FLOAT:
448: 		return ValueXH64FixedWidth<float>(constant);
449: 	case PhysicalType::DOUBLE:
450: 		return ValueXH64FixedWidth<double>(constant);
451: 	case PhysicalType::VARCHAR: {
452: 		auto val = constant.GetValue<string>();
453: 		return duckdb_zstd::XXH64(val.c_str(), val.length(), 0);
454: 	}
455: 	default:
456: 		return 0;
457: 	}
458: }
459: 
460: static bool ApplyBloomFilter(const TableFilter &duckdb_filter, ParquetBloomFilter &bloom_filter) {
461: 	switch (duckdb_filter.filter_type) {
462: 	case TableFilterType::CONSTANT_COMPARISON: {
463: 		auto &constant_filter = duckdb_filter.Cast<ConstantFilter>();
464: 		auto is_compare_equal = constant_filter.comparison_type == ExpressionType::COMPARE_EQUAL;
465: 		D_ASSERT(!constant_filter.constant.IsNull());
466: 		auto hash = ValueXXH64(constant_filter.constant);
467: 		return hash > 0 && !bloom_filter.FilterCheck(hash) && is_compare_equal;
468: 	}
469: 	case TableFilterType::CONJUNCTION_AND: {
470: 		auto &conjunction_and_filter = duckdb_filter.Cast<ConjunctionAndFilter>();
471: 		bool any_children_true = false;
472: 		for (auto &child_filter : conjunction_and_filter.child_filters) {
473: 			any_children_true |= ApplyBloomFilter(*child_filter, bloom_filter);
474: 		}
475: 		return any_children_true;
476: 	}
477: 	case TableFilterType::CONJUNCTION_OR: {
478: 		auto &conjunction_or_filter = duckdb_filter.Cast<ConjunctionOrFilter>();
479: 		bool all_children_true = true;
480: 		for (auto &child_filter : conjunction_or_filter.child_filters) {
481: 			all_children_true &= ApplyBloomFilter(*child_filter, bloom_filter);
482: 		}
483: 		return all_children_true;
484: 	}
485: 	default:
486: 		return false;
487: 	}
488: }
489: 
490: bool ParquetStatisticsUtils::BloomFilterSupported(const LogicalTypeId &type_id) {
491: 	switch (type_id) {
492: 	case LogicalTypeId::TINYINT:
493: 	case LogicalTypeId::UTINYINT:
494: 	case LogicalTypeId::SMALLINT:
495: 	case LogicalTypeId::USMALLINT:
496: 	case LogicalTypeId::INTEGER:
497: 	case LogicalTypeId::UINTEGER:
498: 	case LogicalTypeId::BIGINT:
499: 	case LogicalTypeId::UBIGINT:
500: 	case LogicalTypeId::FLOAT:
501: 	case LogicalTypeId::DOUBLE:
502: 	case LogicalTypeId::VARCHAR:
503: 	case LogicalTypeId::BLOB:
504: 		return true;
505: 	default:
506: 		return false;
507: 	}
508: }
509: 
510: bool ParquetStatisticsUtils::BloomFilterExcludes(const TableFilter &duckdb_filter,
511:                                                  const duckdb_parquet::ColumnMetaData &column_meta_data,
512:                                                  TProtocol &file_proto, Allocator &allocator) {
513: 	if (!HasFilterConstants(duckdb_filter) || !column_meta_data.__isset.bloom_filter_offset ||
514: 	    column_meta_data.bloom_filter_offset <= 0) {
515: 		return false;
516: 	}
517: 	// TODO check length against file length!
518: 
519: 	auto &transport = reinterpret_cast<ThriftFileTransport &>(*file_proto.getTransport());
520: 	transport.SetLocation(column_meta_data.bloom_filter_offset);
521: 	if (column_meta_data.__isset.bloom_filter_length && column_meta_data.bloom_filter_length > 0) {
522: 		transport.Prefetch(column_meta_data.bloom_filter_offset, column_meta_data.bloom_filter_length);
523: 	}
524: 
525: 	duckdb_parquet::BloomFilterHeader filter_header;
526: 	// TODO the bloom filter could be encrypted, too, so need to double check that this is NOT the case
527: 	filter_header.read(&file_proto);
528: 	if (!filter_header.algorithm.__isset.BLOCK || !filter_header.compression.__isset.UNCOMPRESSED ||
529: 	    !filter_header.hash.__isset.XXHASH) {
530: 		return false;
531: 	}
532: 
533: 	auto new_buffer = make_uniq<ResizeableBuffer>(allocator, filter_header.numBytes);
534: 	transport.read(new_buffer->ptr, filter_header.numBytes);
535: 	ParquetBloomFilter bloom_filter(std::move(new_buffer));
536: 	return ApplyBloomFilter(duckdb_filter, bloom_filter);
537: }
538: 
539: ParquetBloomFilter::ParquetBloomFilter(idx_t num_entries, double bloom_filter_false_positive_ratio) {
540: 
541: 	// aim for hit ratio of 0.01%
542: 	// see http://tfk.mit.edu/pdf/bloom.pdf
543: 	double f = bloom_filter_false_positive_ratio;
544: 	double k = 8.0;
545: 	double n = num_entries;
546: 	double m = -k * n / std::log(1 - std::pow(f, 1 / k));
547: 	auto b = MaxValue<idx_t>(NextPowerOfTwo(m / k) / 32, 1);
548: 
549: 	D_ASSERT(b > 0 && IsPowerOfTwo(b));
550: 
551: 	data = make_uniq<ResizeableBuffer>(Allocator::DefaultAllocator(), sizeof(ParquetBloomBlock) * b);
552: 	data->zero();
553: 	block_count = data->len / sizeof(ParquetBloomBlock);
554: 	D_ASSERT(data->len % sizeof(ParquetBloomBlock) == 0);
555: }
556: 
557: ParquetBloomFilter::ParquetBloomFilter(unique_ptr<ResizeableBuffer> data_p) {
558: 	D_ASSERT(data_p->len % sizeof(ParquetBloomBlock) == 0);
559: 	data = std::move(data_p);
560: 	block_count = data->len / sizeof(ParquetBloomBlock);
561: 	D_ASSERT(data->len % sizeof(ParquetBloomBlock) == 0);
562: }
563: 
564: void ParquetBloomFilter::FilterInsert(uint64_t x) {
565: 	auto blocks = (ParquetBloomBlock *)(data->ptr);
566: 	uint64_t i = ((x >> 32) * block_count) >> 32;
567: 	auto &b = blocks[i];
568: 	ParquetBloomBlock::BlockInsert(b, x);
569: }
570: 
571: bool ParquetBloomFilter::FilterCheck(uint64_t x) {
572: 	auto blocks = (ParquetBloomBlock *)(data->ptr);
573: 	auto i = ((x >> 32) * block_count) >> 32;
574: 	return ParquetBloomBlock::BlockCheck(blocks[i], x);
575: }
576: 
577: // compiler optimizes this into a single instruction (popcnt)
578: static uint8_t PopCnt64(uint64_t n) {
579: 	uint8_t c = 0;
580: 	for (; n; ++c) {
581: 		n &= n - 1;
582: 	}
583: 	return c;
584: }
585: 
586: double ParquetBloomFilter::OneRatio() {
587: 	auto bloom_ptr = (uint64_t *)data->ptr;
588: 	idx_t one_count = 0;
589: 	for (idx_t b_idx = 0; b_idx < data->len / sizeof(uint64_t); ++b_idx) {
590: 		one_count += PopCnt64(bloom_ptr[b_idx]);
591: 	}
592: 	return one_count / (data->len * 8.0);
593: }
594: 
595: ResizeableBuffer *ParquetBloomFilter::Get() {
596: 	return data.get();
597: }
598: 
599: } // namespace duckdb
[end of extension/parquet/parquet_statistics.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: