{
  "repo": "duckdb/duckdb",
  "pull_number": 8387,
  "instance_id": "duckdb__duckdb-8387",
  "issue_numbers": [
    "8209"
  ],
  "base_commit": "ba71015ee711d95fe13886f372824143cae7493c",
  "patch": "diff --git a/.github/config/uncovered_files.csv b/.github/config/uncovered_files.csv\nindex 7f3d06ff0f3e..6b483efe0983 100644\n--- a/.github/config/uncovered_files.csv\n+++ b/.github/config/uncovered_files.csv\n@@ -200,10 +200,10 @@ execution/operator/join/physical_range_join.cpp\t62\n execution/operator/order/physical_top_n.cpp\t5\n execution/operator/csv_scanner/base_csv_reader.cpp\t31\n execution/operator/csv_scanner/buffered_csv_reader.cpp\t47\n+execution/operator/csv_scanner/csv_reader_options.cpp\t41\n execution/operator/persistent/csv_buffer.cpp\t2\n execution/operator/csv_scanner/csv_file_handle.cpp\t7\n-execution/operator/csv_scanner/csv_reader_options.cpp\t12\n-execution/operator/csv_scanner/parallel_csv_reader.cpp\t52\n+execution/operator/csv_scanner/parallel_csv_reader.cpp\t50\n execution/operator/persistent/physical_batch_copy_to_file.cpp\t7\n execution/operator/persistent/physical_batch_insert.cpp\t43\n execution/operator/persistent/physical_copy_to_file.cpp\t2\ndiff --git a/src/execution/operator/csv_scanner/csv_reader_options.cpp b/src/execution/operator/csv_scanner/csv_reader_options.cpp\nindex 524f3a500e56..ac0d07ba5780 100644\n--- a/src/execution/operator/csv_scanner/csv_reader_options.cpp\n+++ b/src/execution/operator/csv_scanner/csv_reader_options.cpp\n@@ -2,6 +2,8 @@\n #include \"duckdb/common/bind_helpers.hpp\"\n #include \"duckdb/common/vector_size.hpp\"\n #include \"duckdb/common/string_util.hpp\"\n+#include \"duckdb/common/enum_util.hpp\"\n+#include \"duckdb/common/multi_file_reader.hpp\"\n \n namespace duckdb {\n \n@@ -60,6 +62,10 @@ static int64_t ParseInteger(const Value &value, const string &loption) {\n \treturn value.GetValue<int64_t>();\n }\n \n+bool CSVReaderOptions::GetHeader() const {\n+\treturn this->dialect_options.header;\n+}\n+\n void CSVReaderOptions::SetHeader(bool input) {\n \tthis->dialect_options.header = input;\n \tthis->has_header = true;\n@@ -69,6 +75,10 @@ void CSVReaderOptions::SetCompression(const string &compression_p) {\n \tthis->compression = FileCompressionTypeFromString(compression_p);\n }\n \n+string CSVReaderOptions::GetEscape() const {\n+\treturn std::string(1, this->dialect_options.state_machine_options.escape);\n+}\n+\n void CSVReaderOptions::SetEscape(const string &input) {\n \tauto escape_str = input;\n \tif (escape_str.size() > 1) {\n@@ -81,6 +91,19 @@ void CSVReaderOptions::SetEscape(const string &input) {\n \tthis->has_escape = true;\n }\n \n+int64_t CSVReaderOptions::GetSkipRows() const {\n+\treturn this->dialect_options.skip_rows;\n+}\n+\n+void CSVReaderOptions::SetSkipRows(int64_t skip_rows) {\n+\tdialect_options.skip_rows = skip_rows;\n+\tskip_rows_set = true;\n+}\n+\n+string CSVReaderOptions::GetDelimiter() const {\n+\treturn std::string(1, this->dialect_options.state_machine_options.delimiter);\n+}\n+\n void CSVReaderOptions::SetDelimiter(const string &input) {\n \tauto delim_str = StringUtil::Replace(input, \"\\\\t\", \"\\t\");\n \tif (delim_str.size() > 1) {\n@@ -93,6 +116,10 @@ void CSVReaderOptions::SetDelimiter(const string &input) {\n \tthis->dialect_options.state_machine_options.delimiter = delim_str[0];\n }\n \n+string CSVReaderOptions::GetQuote() const {\n+\treturn std::string(1, this->dialect_options.state_machine_options.quote);\n+}\n+\n void CSVReaderOptions::SetQuote(const string &quote_p) {\n \tauto quote_str = quote_p;\n \tif (quote_str.size() > 1) {\n@@ -105,6 +132,10 @@ void CSVReaderOptions::SetQuote(const string &quote_p) {\n \tthis->has_quote = true;\n }\n \n+NewLineIdentifier CSVReaderOptions::GetNewline() const {\n+\treturn dialect_options.new_line;\n+}\n+\n void CSVReaderOptions::SetNewline(const string &input) {\n \tif (input == \"\\\\n\" || input == \"\\\\r\") {\n \t\tdialect_options.new_line = NewLineIdentifier::SINGLE;\n@@ -152,8 +183,7 @@ void CSVReaderOptions::SetReadOption(const string &loption, const Value &value,\n \t\t\tsample_chunks = sample_size / STANDARD_VECTOR_SIZE + 1;\n \t\t}\n \t} else if (loption == \"skip\") {\n-\t\tdialect_options.skip_rows = ParseInteger(value, loption);\n-\t\tskip_rows_set = true;\n+\t\tSetSkipRows(ParseInteger(value, loption));\n \t} else if (loption == \"max_line_size\" || loption == \"maximum_line_size\") {\n \t\tmaximum_line_size = ParseInteger(value, loption);\n \t} else if (loption == \"sample_chunk_size\") {\n@@ -296,4 +326,185 @@ string CSVReaderOptions::ToString() const {\n \t       \"\\n  ignore_errors=\" + std::to_string(ignore_errors) + \"\\n  all_varchar=\" + std::to_string(all_varchar);\n }\n \n+static Value StringVectorToValue(const vector<string> &vec) {\n+\tvector<Value> content;\n+\tcontent.reserve(vec.size());\n+\tfor (auto &item : vec) {\n+\t\tcontent.push_back(Value(item));\n+\t}\n+\treturn Value::LIST(std::move(content));\n+}\n+\n+static uint8_t GetCandidateSpecificity(const LogicalType &candidate_type) {\n+\t//! Const ht with accepted auto_types and their weights in specificity\n+\tconst duckdb::unordered_map<uint8_t, uint8_t> auto_type_candidates_specificity {\n+\t    {(uint8_t)LogicalTypeId::VARCHAR, 0},  {(uint8_t)LogicalTypeId::TIMESTAMP, 1},\n+\t    {(uint8_t)LogicalTypeId::DATE, 2},     {(uint8_t)LogicalTypeId::TIME, 3},\n+\t    {(uint8_t)LogicalTypeId::DOUBLE, 4},   {(uint8_t)LogicalTypeId::FLOAT, 5},\n+\t    {(uint8_t)LogicalTypeId::BIGINT, 6},   {(uint8_t)LogicalTypeId::INTEGER, 7},\n+\t    {(uint8_t)LogicalTypeId::SMALLINT, 8}, {(uint8_t)LogicalTypeId::TINYINT, 9},\n+\t    {(uint8_t)LogicalTypeId::BOOLEAN, 10}, {(uint8_t)LogicalTypeId::SQLNULL, 11}};\n+\n+\tauto id = (uint8_t)candidate_type.id();\n+\tauto it = auto_type_candidates_specificity.find(id);\n+\tif (it == auto_type_candidates_specificity.end()) {\n+\t\tthrow BinderException(\"Auto Type Candidate of type %s is not accepted as a valid input\",\n+\t\t                      EnumUtil::ToString(candidate_type.id()));\n+\t}\n+\treturn it->second;\n+}\n+\n+void CSVReaderOptions::FromNamedParameters(named_parameter_map_t &in, ClientContext &context,\n+                                           vector<LogicalType> &return_types, vector<string> &names) {\n+\tfor (auto &kv : in) {\n+\t\tif (MultiFileReader::ParseOption(kv.first, kv.second, file_options, context)) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tauto loption = StringUtil::Lower(kv.first);\n+\t\tif (loption == \"columns\") {\n+\t\t\texplicitly_set_columns = true;\n+\t\t\tauto &child_type = kv.second.type();\n+\t\t\tif (child_type.id() != LogicalTypeId::STRUCT) {\n+\t\t\t\tthrow BinderException(\"read_csv columns requires a struct as input\");\n+\t\t\t}\n+\t\t\tauto &struct_children = StructValue::GetChildren(kv.second);\n+\t\t\tD_ASSERT(StructType::GetChildCount(child_type) == struct_children.size());\n+\t\t\tfor (idx_t i = 0; i < struct_children.size(); i++) {\n+\t\t\t\tauto &name = StructType::GetChildName(child_type, i);\n+\t\t\t\tauto &val = struct_children[i];\n+\t\t\t\tnames.push_back(name);\n+\t\t\t\tif (val.type().id() != LogicalTypeId::VARCHAR) {\n+\t\t\t\t\tthrow BinderException(\"read_csv requires a type specification as string\");\n+\t\t\t\t}\n+\t\t\t\treturn_types.emplace_back(TransformStringToLogicalType(StringValue::Get(val), context));\n+\t\t\t}\n+\t\t\tif (names.empty()) {\n+\t\t\t\tthrow BinderException(\"read_csv requires at least a single column as input!\");\n+\t\t\t}\n+\t\t} else if (loption == \"auto_type_candidates\") {\n+\t\t\tauto_type_candidates.clear();\n+\t\t\tmap<uint8_t, LogicalType> candidate_types;\n+\t\t\t// We always have the extremes of Null and Varchar, so we can default to varchar if the\n+\t\t\t// sniffer is not able to confidently detect that column type\n+\t\t\tcandidate_types[GetCandidateSpecificity(LogicalType::VARCHAR)] = LogicalType::VARCHAR;\n+\t\t\tcandidate_types[GetCandidateSpecificity(LogicalType::SQLNULL)] = LogicalType::SQLNULL;\n+\n+\t\t\tauto &child_type = kv.second.type();\n+\t\t\tif (child_type.id() != LogicalTypeId::LIST) {\n+\t\t\t\tthrow BinderException(\"read_csv auto_types requires a list as input\");\n+\t\t\t}\n+\t\t\tauto &list_children = ListValue::GetChildren(kv.second);\n+\t\t\tif (list_children.empty()) {\n+\t\t\t\tthrow BinderException(\"auto_type_candidates requires at least one type\");\n+\t\t\t}\n+\t\t\tfor (auto &child : list_children) {\n+\t\t\t\tif (child.type().id() != LogicalTypeId::VARCHAR) {\n+\t\t\t\t\tthrow BinderException(\"auto_type_candidates requires a type specification as string\");\n+\t\t\t\t}\n+\t\t\t\tauto candidate_type = TransformStringToLogicalType(StringValue::Get(child), context);\n+\t\t\t\tcandidate_types[GetCandidateSpecificity(candidate_type)] = candidate_type;\n+\t\t\t}\n+\t\t\tfor (auto &candidate_type : candidate_types) {\n+\t\t\t\tauto_type_candidates.emplace_back(candidate_type.second);\n+\t\t\t}\n+\t\t} else if (loption == \"column_names\" || loption == \"names\") {\n+\t\t\tif (!name_list.empty()) {\n+\t\t\t\tthrow BinderException(\"read_csv_auto column_names/names can only be supplied once\");\n+\t\t\t}\n+\t\t\tif (kv.second.IsNull()) {\n+\t\t\t\tthrow BinderException(\"read_csv_auto %s cannot be NULL\", kv.first);\n+\t\t\t}\n+\t\t\tauto &children = ListValue::GetChildren(kv.second);\n+\t\t\tfor (auto &child : children) {\n+\t\t\t\tname_list.push_back(StringValue::Get(child));\n+\t\t\t}\n+\t\t} else if (loption == \"column_types\" || loption == \"types\" || loption == \"dtypes\") {\n+\t\t\tauto &child_type = kv.second.type();\n+\t\t\tif (child_type.id() != LogicalTypeId::STRUCT && child_type.id() != LogicalTypeId::LIST) {\n+\t\t\t\tthrow BinderException(\"read_csv_auto %s requires a struct or list as input\", kv.first);\n+\t\t\t}\n+\t\t\tif (!sql_type_list.empty()) {\n+\t\t\t\tthrow BinderException(\"read_csv_auto column_types/types/dtypes can only be supplied once\");\n+\t\t\t}\n+\t\t\tvector<string> sql_type_names;\n+\t\t\tif (child_type.id() == LogicalTypeId::STRUCT) {\n+\t\t\t\tauto &struct_children = StructValue::GetChildren(kv.second);\n+\t\t\t\tD_ASSERT(StructType::GetChildCount(child_type) == struct_children.size());\n+\t\t\t\tfor (idx_t i = 0; i < struct_children.size(); i++) {\n+\t\t\t\t\tauto &name = StructType::GetChildName(child_type, i);\n+\t\t\t\t\tauto &val = struct_children[i];\n+\t\t\t\t\tif (val.type().id() != LogicalTypeId::VARCHAR) {\n+\t\t\t\t\t\tthrow BinderException(\"read_csv_auto %s requires a type specification as string\", kv.first);\n+\t\t\t\t\t}\n+\t\t\t\t\tsql_type_names.push_back(StringValue::Get(val));\n+\t\t\t\t\tsql_types_per_column[name] = i;\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tauto &list_child = ListType::GetChildType(child_type);\n+\t\t\t\tif (list_child.id() != LogicalTypeId::VARCHAR) {\n+\t\t\t\t\tthrow BinderException(\"read_csv_auto %s requires a list of types (varchar) as input\", kv.first);\n+\t\t\t\t}\n+\t\t\t\tauto &children = ListValue::GetChildren(kv.second);\n+\t\t\t\tfor (auto &child : children) {\n+\t\t\t\t\tsql_type_names.push_back(StringValue::Get(child));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tsql_type_list.reserve(sql_type_names.size());\n+\t\t\tfor (auto &sql_type : sql_type_names) {\n+\t\t\t\tauto def_type = TransformStringToLogicalType(sql_type);\n+\t\t\t\tif (def_type.id() == LogicalTypeId::USER) {\n+\t\t\t\t\tthrow BinderException(\"Unrecognized type \\\"%s\\\" for read_csv_auto %s definition\", sql_type,\n+\t\t\t\t\t                      kv.first);\n+\t\t\t\t}\n+\t\t\t\tsql_type_list.push_back(std::move(def_type));\n+\t\t\t}\n+\t\t} else if (loption == \"all_varchar\") {\n+\t\t\tall_varchar = BooleanValue::Get(kv.second);\n+\t\t} else if (loption == \"normalize_names\") {\n+\t\t\tnormalize_names = BooleanValue::Get(kv.second);\n+\t\t} else {\n+\t\t\tSetReadOption(loption, kv.second, names);\n+\t\t}\n+\t}\n+}\n+\n+//! This function is used to remember options set by the sniffer, for use in ReadCSVRelation\n+void CSVReaderOptions::ToNamedParameters(named_parameter_map_t &named_params) {\n+\tif (has_delimiter) {\n+\t\tnamed_params[\"delim\"] = Value(GetDelimiter());\n+\t}\n+\tif (has_newline) {\n+\t\tnamed_params[\"newline\"] = Value(EnumUtil::ToString(GetNewline()));\n+\t}\n+\tif (has_quote) {\n+\t\tnamed_params[\"quote\"] = Value(GetQuote());\n+\t}\n+\tif (has_escape) {\n+\t\tnamed_params[\"escape\"] = Value(GetEscape());\n+\t}\n+\tif (has_header) {\n+\t\tnamed_params[\"header\"] = Value(GetHeader());\n+\t}\n+\tnamed_params[\"max_line_size\"] = Value::BIGINT(maximum_line_size);\n+\tif (skip_rows_set) {\n+\t\tnamed_params[\"skip\"] = Value::BIGINT(GetSkipRows());\n+\t}\n+\tnamed_params[\"sample_chunks\"] = Value::BIGINT(sample_chunks);\n+\tnamed_params[\"sample_chunk_size\"] = Value::BIGINT(sample_chunk_size);\n+\tnamed_params[\"null_padding\"] = Value::BOOLEAN(null_padding);\n+\tif (!date_format.at(LogicalType::DATE).format_specifier.empty()) {\n+\t\tnamed_params[\"dateformat\"] = Value(date_format.at(LogicalType::DATE).format_specifier);\n+\t}\n+\tif (!date_format.at(LogicalType::TIMESTAMP).format_specifier.empty()) {\n+\t\tnamed_params[\"timestampformat\"] = Value(date_format.at(LogicalType::TIMESTAMP).format_specifier);\n+\t}\n+\n+\tnamed_params[\"normalize_names\"] = Value::BOOLEAN(normalize_names);\n+\tif (!name_list.empty()) {\n+\t\tnamed_params[\"column_names\"] = StringVectorToValue(name_list);\n+\t}\n+\tnamed_params[\"all_varchar\"] = Value::BOOLEAN(all_varchar);\n+\tnamed_params[\"maximum_line_size\"] = Value::BIGINT(maximum_line_size);\n+}\n+\n } // namespace duckdb\ndiff --git a/src/function/table/read_csv.cpp b/src/function/table/read_csv.cpp\nindex 8408760bda82..227e68bea8b7 100644\n--- a/src/function/table/read_csv.cpp\n+++ b/src/function/table/read_csv.cpp\n@@ -85,25 +85,6 @@ void ReadCSVData::FinalizeRead(ClientContext &context) {\n \t}\n }\n \n-uint8_t GetCandidateSpecificity(const LogicalType &candidate_type) {\n-\t//! Const ht with accepted auto_types and their weights in specificity\n-\tconst duckdb::unordered_map<uint8_t, uint8_t> auto_type_candidates_specificity {\n-\t    {(uint8_t)LogicalTypeId::VARCHAR, 0},  {(uint8_t)LogicalTypeId::TIMESTAMP, 1},\n-\t    {(uint8_t)LogicalTypeId::DATE, 2},     {(uint8_t)LogicalTypeId::TIME, 3},\n-\t    {(uint8_t)LogicalTypeId::DOUBLE, 4},   {(uint8_t)LogicalTypeId::FLOAT, 5},\n-\t    {(uint8_t)LogicalTypeId::BIGINT, 6},   {(uint8_t)LogicalTypeId::INTEGER, 7},\n-\t    {(uint8_t)LogicalTypeId::SMALLINT, 8}, {(uint8_t)LogicalTypeId::TINYINT, 9},\n-\t    {(uint8_t)LogicalTypeId::BOOLEAN, 10}, {(uint8_t)LogicalTypeId::SQLNULL, 11}};\n-\n-\tauto id = (uint8_t)candidate_type.id();\n-\tauto it = auto_type_candidates_specificity.find(id);\n-\tif (it == auto_type_candidates_specificity.end()) {\n-\t\tthrow BinderException(\"Auto Type Candidate of type %s is not accepted as a valid input\",\n-\t\t                      EnumUtil::ToString(candidate_type.id()));\n-\t}\n-\treturn it->second;\n-}\n-\n static unique_ptr<FunctionData> ReadCSVBind(ClientContext &context, TableFunctionBindInput &input,\n                                             vector<LogicalType> &return_types, vector<string> &names) {\n \n@@ -111,117 +92,9 @@ static unique_ptr<FunctionData> ReadCSVBind(ClientContext &context, TableFunctio\n \tauto &options = result->options;\n \tresult->files = MultiFileReader::GetFileList(context, input.inputs[0], \"CSV\");\n \n-\tbool explicitly_set_columns = false;\n-\tfor (auto &kv : input.named_parameters) {\n-\t\tif (MultiFileReader::ParseOption(kv.first, kv.second, options.file_options, context)) {\n-\t\t\tcontinue;\n-\t\t}\n-\t\tauto loption = StringUtil::Lower(kv.first);\n-\t\tif (loption == \"columns\") {\n-\t\t\texplicitly_set_columns = true;\n-\t\t\tauto &child_type = kv.second.type();\n-\t\t\tif (child_type.id() != LogicalTypeId::STRUCT) {\n-\t\t\t\tthrow BinderException(\"read_csv columns requires a struct as input\");\n-\t\t\t}\n-\t\t\tauto &struct_children = StructValue::GetChildren(kv.second);\n-\t\t\tD_ASSERT(StructType::GetChildCount(child_type) == struct_children.size());\n-\t\t\tfor (idx_t i = 0; i < struct_children.size(); i++) {\n-\t\t\t\tauto &name = StructType::GetChildName(child_type, i);\n-\t\t\t\tauto &val = struct_children[i];\n-\t\t\t\tnames.push_back(name);\n-\t\t\t\tif (val.type().id() != LogicalTypeId::VARCHAR) {\n-\t\t\t\t\tthrow BinderException(\"read_csv requires a type specification as string\");\n-\t\t\t\t}\n-\t\t\t\treturn_types.emplace_back(TransformStringToLogicalType(StringValue::Get(val), context));\n-\t\t\t}\n-\t\t\tif (names.empty()) {\n-\t\t\t\tthrow BinderException(\"read_csv requires at least a single column as input!\");\n-\t\t\t}\n-\t\t} else if (loption == \"auto_type_candidates\") {\n-\t\t\toptions.auto_type_candidates.clear();\n-\t\t\tmap<uint8_t, LogicalType> candidate_types;\n-\t\t\t// We always have the extremes of Null and Varchar, so we can default to varchar if the\n-\t\t\t// sniffer is not able to confidently detect that column type\n-\t\t\tcandidate_types[GetCandidateSpecificity(LogicalType::VARCHAR)] = LogicalType::VARCHAR;\n-\t\t\tcandidate_types[GetCandidateSpecificity(LogicalType::SQLNULL)] = LogicalType::SQLNULL;\n-\n-\t\t\tauto &child_type = kv.second.type();\n-\t\t\tif (child_type.id() != LogicalTypeId::LIST) {\n-\t\t\t\tthrow BinderException(\"read_csv auto_types requires a list as input\");\n-\t\t\t}\n-\t\t\tauto &list_children = ListValue::GetChildren(kv.second);\n-\t\t\tif (list_children.empty()) {\n-\t\t\t\tthrow BinderException(\"auto_type_candidates requires at least one type\");\n-\t\t\t}\n-\t\t\tfor (auto &child : list_children) {\n-\t\t\t\tif (child.type().id() != LogicalTypeId::VARCHAR) {\n-\t\t\t\t\tthrow BinderException(\"auto_type_candidates requires a type specification as string\");\n-\t\t\t\t}\n-\t\t\t\tauto candidate_type = TransformStringToLogicalType(StringValue::Get(child), context);\n-\t\t\t\tcandidate_types[GetCandidateSpecificity(candidate_type)] = candidate_type;\n-\t\t\t}\n-\t\t\tfor (auto &candidate_type : candidate_types) {\n-\t\t\t\toptions.auto_type_candidates.emplace_back(candidate_type.second);\n-\t\t\t}\n-\t\t} else if (loption == \"column_names\" || loption == \"names\") {\n-\t\t\tif (!options.name_list.empty()) {\n-\t\t\t\tthrow BinderException(\"read_csv_auto column_names/names can only be supplied once\");\n-\t\t\t}\n-\t\t\tif (kv.second.IsNull()) {\n-\t\t\t\tthrow BinderException(\"read_csv_auto %s cannot be NULL\", kv.first);\n-\t\t\t}\n-\t\t\tauto &children = ListValue::GetChildren(kv.second);\n-\t\t\tfor (auto &child : children) {\n-\t\t\t\toptions.name_list.push_back(StringValue::Get(child));\n-\t\t\t}\n-\t\t} else if (loption == \"column_types\" || loption == \"types\" || loption == \"dtypes\") {\n-\t\t\tauto &child_type = kv.second.type();\n-\t\t\tif (child_type.id() != LogicalTypeId::STRUCT && child_type.id() != LogicalTypeId::LIST) {\n-\t\t\t\tthrow BinderException(\"read_csv_auto %s requires a struct or list as input\", kv.first);\n-\t\t\t}\n-\t\t\tif (!options.sql_type_list.empty()) {\n-\t\t\t\tthrow BinderException(\"read_csv_auto column_types/types/dtypes can only be supplied once\");\n-\t\t\t}\n-\t\t\tvector<string> sql_type_names;\n-\t\t\tif (child_type.id() == LogicalTypeId::STRUCT) {\n-\t\t\t\tauto &struct_children = StructValue::GetChildren(kv.second);\n-\t\t\t\tD_ASSERT(StructType::GetChildCount(child_type) == struct_children.size());\n-\t\t\t\tfor (idx_t i = 0; i < struct_children.size(); i++) {\n-\t\t\t\t\tauto &name = StructType::GetChildName(child_type, i);\n-\t\t\t\t\tauto &val = struct_children[i];\n-\t\t\t\t\tif (val.type().id() != LogicalTypeId::VARCHAR) {\n-\t\t\t\t\t\tthrow BinderException(\"read_csv_auto %s requires a type specification as string\", kv.first);\n-\t\t\t\t\t}\n-\t\t\t\t\tsql_type_names.push_back(StringValue::Get(val));\n-\t\t\t\t\toptions.sql_types_per_column[name] = i;\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tauto &list_child = ListType::GetChildType(child_type);\n-\t\t\t\tif (list_child.id() != LogicalTypeId::VARCHAR) {\n-\t\t\t\t\tthrow BinderException(\"read_csv_auto %s requires a list of types (varchar) as input\", kv.first);\n-\t\t\t\t}\n-\t\t\t\tauto &children = ListValue::GetChildren(kv.second);\n-\t\t\t\tfor (auto &child : children) {\n-\t\t\t\t\tsql_type_names.push_back(StringValue::Get(child));\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\toptions.sql_type_list.reserve(sql_type_names.size());\n-\t\t\tfor (auto &sql_type : sql_type_names) {\n-\t\t\t\tauto def_type = TransformStringToLogicalType(sql_type);\n-\t\t\t\tif (def_type.id() == LogicalTypeId::USER) {\n-\t\t\t\t\tthrow BinderException(\"Unrecognized type \\\"%s\\\" for read_csv_auto %s definition\", sql_type,\n-\t\t\t\t\t                      kv.first);\n-\t\t\t\t}\n-\t\t\t\toptions.sql_type_list.push_back(std::move(def_type));\n-\t\t\t}\n-\t\t} else if (loption == \"all_varchar\") {\n-\t\t\toptions.all_varchar = BooleanValue::Get(kv.second);\n-\t\t} else if (loption == \"normalize_names\") {\n-\t\t\toptions.normalize_names = BooleanValue::Get(kv.second);\n-\t\t} else {\n-\t\t\toptions.SetReadOption(loption, kv.second, names);\n-\t\t}\n-\t}\n+\toptions.FromNamedParameters(input.named_parameters, context, return_types, names);\n+\tbool explicitly_set_columns = options.explicitly_set_columns;\n+\n \toptions.file_options.AutoDetectHivePartitioning(result->files, context);\n \n \tif (!options.auto_detect && return_types.empty()) {\ndiff --git a/src/include/duckdb/execution/operator/scan/csv/csv_reader_options.hpp b/src/include/duckdb/execution/operator/scan/csv/csv_reader_options.hpp\nindex 491b2ba241d0..cd59c27933eb 100644\n--- a/src/include/duckdb/execution/operator/scan/csv/csv_reader_options.hpp\n+++ b/src/include/duckdb/execution/operator/scan/csv/csv_reader_options.hpp\n@@ -159,18 +159,33 @@ struct CSVReaderOptions {\n \tstring suffix;\n \tstring write_newline;\n \n+\t//! The date format to use (if any is specified)\n+\tmap<LogicalTypeId, StrpTimeFormat> date_format = {{LogicalTypeId::DATE, {}}, {LogicalTypeId::TIMESTAMP, {}}};\n \t//! The date format to use for writing (if any is specified)\n \tmap<LogicalTypeId, StrfTimeFormat> write_date_format = {{LogicalTypeId::DATE, {}}, {LogicalTypeId::TIMESTAMP, {}}};\n+\t//! Whether or not a type format is specified\n+\tmap<LogicalTypeId, bool> has_format = {{LogicalTypeId::DATE, false}, {LogicalTypeId::TIMESTAMP, false}};\n \n \tvoid Serialize(Serializer &serializer) const;\n \tstatic CSVReaderOptions Deserialize(Deserializer &deserializer);\n \n \tvoid SetCompression(const string &compression);\n+\n+\tbool GetHeader() const;\n \tvoid SetHeader(bool has_header);\n+\n+\tstring GetEscape() const;\n \tvoid SetEscape(const string &escape);\n+\n+\tint64_t GetSkipRows() const;\n+\tvoid SetSkipRows(int64_t rows);\n+\n+\tstring GetQuote() const;\n \tvoid SetQuote(const string &quote);\n \tvoid SetDelimiter(const string &delimiter);\n+\tstring GetDelimiter() const;\n \n+\tNewLineIdentifier GetNewline() const;\n \tvoid SetNewline(const string &input);\n \t//! Set an option that is supported by both reading and writing functions, called by\n \t//! the SetReadOption and SetWriteOption methods\n@@ -182,7 +197,16 @@ struct CSVReaderOptions {\n \tvoid SetReadOption(const string &loption, const Value &value, vector<string> &expected_names);\n \tvoid SetWriteOption(const string &loption, const Value &value);\n \tvoid SetDateFormat(LogicalTypeId type, const string &format, bool read_format);\n+\tvoid ToNamedParameters(named_parameter_map_t &out);\n+\tvoid FromNamedParameters(named_parameter_map_t &in, ClientContext &context, vector<LogicalType> &return_types,\n+\t                         vector<string> &names);\n \n \tstring ToString() const;\n+\n+\tnamed_parameter_map_t OutputReadSettings();\n+\n+public:\n+\t//! Whether columns were explicitly provided through named parameters\n+\tbool explicitly_set_columns = false;\n };\n } // namespace duckdb\ndiff --git a/src/include/duckdb/main/connection.hpp b/src/include/duckdb/main/connection.hpp\nindex 5df85444b161..29048a9e438e 100644\n--- a/src/include/duckdb/main/connection.hpp\n+++ b/src/include/duckdb/main/connection.hpp\n@@ -131,7 +131,7 @@ class Connection {\n \n \t//! Reads CSV file\n \tDUCKDB_API shared_ptr<Relation> ReadCSV(const string &csv_file);\n-\tDUCKDB_API shared_ptr<Relation> ReadCSV(const string &csv_file, CSVReaderOptions &options);\n+\tDUCKDB_API shared_ptr<Relation> ReadCSV(const string &csv_file, named_parameter_map_t &&options);\n \tDUCKDB_API shared_ptr<Relation> ReadCSV(const string &csv_file, const vector<string> &columns);\n \n \t//! Reads Parquet file\ndiff --git a/src/include/duckdb/main/relation/read_csv_relation.hpp b/src/include/duckdb/main/relation/read_csv_relation.hpp\nindex 938f243392cb..fc2f98181f7d 100644\n--- a/src/include/duckdb/main/relation/read_csv_relation.hpp\n+++ b/src/include/duckdb/main/relation/read_csv_relation.hpp\n@@ -10,16 +10,16 @@\n \n #include \"duckdb/execution/operator/scan/csv/csv_reader_options.hpp\"\n #include \"duckdb/main/relation/table_function_relation.hpp\"\n+#include \"duckdb/common/shared_ptr.hpp\"\n+#include \"duckdb/common/case_insensitive_map.hpp\"\n \n namespace duckdb {\n \n-struct CSVReaderOptions;\n-\n class ReadCSVRelation : public TableFunctionRelation {\n public:\n \tReadCSVRelation(const shared_ptr<ClientContext> &context, const string &csv_file, vector<ColumnDefinition> columns,\n \t                string alias = string());\n-\tReadCSVRelation(const shared_ptr<ClientContext> &context, const string &csv_file, CSVReaderOptions options,\n+\tReadCSVRelation(const shared_ptr<ClientContext> &context, const string &csv_file, named_parameter_map_t &&options,\n \t                string alias = string());\n \n \tstring alias;\ndiff --git a/src/include/duckdb/main/relation/table_function_relation.hpp b/src/include/duckdb/main/relation/table_function_relation.hpp\nindex 2e45d9a4d8ce..4dce6d951fe9 100644\n--- a/src/include/duckdb/main/relation/table_function_relation.hpp\n+++ b/src/include/duckdb/main/relation/table_function_relation.hpp\n@@ -35,6 +35,7 @@ class TableFunctionRelation : public Relation {\n \tstring ToString(idx_t depth) override;\n \tstring GetAlias() override;\n \tvoid AddNamedParameter(const string &name, Value argument);\n+\tvoid SetNamedParameters(named_parameter_map_t &&named_parameters);\n \n private:\n \tvoid InitializeColumns();\ndiff --git a/src/main/connection.cpp b/src/main/connection.cpp\nindex 12a222639af4..e225c643cd62 100644\n--- a/src/main/connection.cpp\n+++ b/src/main/connection.cpp\n@@ -219,14 +219,12 @@ shared_ptr<Relation> Connection::Values(const string &values, const vector<strin\n }\n \n shared_ptr<Relation> Connection::ReadCSV(const string &csv_file) {\n-\tCSVReaderOptions options;\n-\treturn ReadCSV(csv_file, options);\n+\tnamed_parameter_map_t options;\n+\treturn ReadCSV(csv_file, std::move(options));\n }\n \n-shared_ptr<Relation> Connection::ReadCSV(const string &csv_file, CSVReaderOptions &options) {\n-\toptions.file_path = csv_file;\n-\toptions.auto_detect = true;\n-\treturn make_shared<ReadCSVRelation>(context, csv_file, options);\n+shared_ptr<Relation> Connection::ReadCSV(const string &csv_file, named_parameter_map_t &&options) {\n+\treturn make_shared<ReadCSVRelation>(context, csv_file, std::move(options));\n }\n \n shared_ptr<Relation> Connection::ReadCSV(const string &csv_file, const vector<string> &columns) {\ndiff --git a/src/main/relation/read_csv_relation.cpp b/src/main/relation/read_csv_relation.cpp\nindex bde933bf8f26..fd9ecf5523e1 100644\n--- a/src/main/relation/read_csv_relation.cpp\n+++ b/src/main/relation/read_csv_relation.cpp\n@@ -1,6 +1,5 @@\n #include \"duckdb/main/relation/read_csv_relation.hpp\"\n \n-#include \"duckdb/common/string_util.hpp\"\n #include \"duckdb/execution/operator/scan/csv/buffered_csv_reader.hpp\"\n #include \"duckdb/execution/operator/scan/csv/csv_buffer_manager.hpp\"\n #include \"duckdb/execution/operator/scan/csv/csv_sniffer.hpp\"\n@@ -8,6 +7,9 @@\n #include \"duckdb/parser/expression/comparison_expression.hpp\"\n #include \"duckdb/parser/expression/constant_expression.hpp\"\n #include \"duckdb/parser/expression/function_expression.hpp\"\n+#include \"duckdb/common/string_util.hpp\"\n+#include \"duckdb/execution/operator/scan/csv/csv_reader_options.hpp\"\n+#include \"duckdb/common/multi_file_reader.hpp\"\n #include \"duckdb/parser/expression/star_expression.hpp\"\n #include \"duckdb/parser/query_node/select_node.hpp\"\n #include \"duckdb/parser/tableref/basetableref.hpp\"\n@@ -34,8 +36,8 @@ ReadCSVRelation::ReadCSVRelation(const shared_ptr<ClientContext> &context, const\n \tAddNamedParameter(\"columns\", Value::STRUCT(std::move(column_names)));\n }\n \n-ReadCSVRelation::ReadCSVRelation(const shared_ptr<ClientContext> &context, const string &csv_file,\n-                                 CSVReaderOptions options, string alias_p)\n+ReadCSVRelation::ReadCSVRelation(const std::shared_ptr<ClientContext> &context, const string &csv_file,\n+                                 named_parameter_map_t &&options, string alias_p)\n     : TableFunctionRelation(context, \"read_csv_auto\", {Value(csv_file)}, nullptr, false), alias(std::move(alias_p)),\n       auto_detect(true) {\n \n@@ -43,12 +45,24 @@ ReadCSVRelation::ReadCSVRelation(const shared_ptr<ClientContext> &context, const\n \t\talias = StringUtil::Split(csv_file, \".\")[0];\n \t}\n \n-\t// Force auto_detect for this constructor\n-\toptions.auto_detect = true;\n-\tauto bm_file_handle = BaseCSVReader::OpenCSV(*context, options);\n-\tauto buffer_manager = make_shared<CSVBufferManager>(*context, std::move(bm_file_handle), options);\n+\tauto files = MultiFileReader::GetFileList(*context, csv_file, \"CSV\");\n+\tD_ASSERT(!files.empty());\n+\n+\tauto &file_name = files[0];\n+\toptions[\"auto_detect\"] = Value::BOOLEAN(true);\n+\tCSVReaderOptions csv_options;\n+\tcsv_options.file_path = file_name;\n+\tvector<string> empty;\n+\n+\tvector<LogicalType> unused_types;\n+\tvector<string> unused_names;\n+\tcsv_options.FromNamedParameters(options, *context, unused_types, unused_names);\n+\t// Run the auto-detect, populating the options with the detected settings\n+\n+\tauto bm_file_handle = BaseCSVReader::OpenCSV(*context, csv_options);\n+\tauto buffer_manager = make_shared<CSVBufferManager>(*context, std::move(bm_file_handle), csv_options);\n \tCSVStateMachineCache state_machine_cache;\n-\tCSVSniffer sniffer(options, buffer_manager, state_machine_cache);\n+\tCSVSniffer sniffer(csv_options, buffer_manager, state_machine_cache);\n \tauto sniffer_result = sniffer.SniffCSV();\n \tauto &types = sniffer_result.return_types;\n \tauto &names = sniffer_result.names;\n@@ -56,7 +70,12 @@ ReadCSVRelation::ReadCSVRelation(const shared_ptr<ClientContext> &context, const\n \t\tcolumns.emplace_back(names[i], types[i]);\n \t}\n \n-\tAddNamedParameter(\"auto_detect\", Value::BOOLEAN(true));\n+\t//! Capture the options potentially set/altered by the auto detection phase\n+\tcsv_options.ToNamedParameters(options);\n+\n+\t// No need to auto-detect again\n+\toptions[\"auto_detect\"] = Value::BOOLEAN(false);\n+\tSetNamedParameters(std::move(options));\n }\n \n string ReadCSVRelation::GetAlias() {\ndiff --git a/src/main/relation/table_function_relation.cpp b/src/main/relation/table_function_relation.cpp\nindex 1a0f50b1bfd5..6933a36a245d 100644\n--- a/src/main/relation/table_function_relation.cpp\n+++ b/src/main/relation/table_function_relation.cpp\n@@ -9,6 +9,7 @@\n #include \"duckdb/main/client_context.hpp\"\n #include \"duckdb/parser/expression/comparison_expression.hpp\"\n #include \"duckdb/parser/expression/columnref_expression.hpp\"\n+#include \"duckdb/common/shared_ptr.hpp\"\n \n namespace duckdb {\n \n@@ -16,7 +17,12 @@ void TableFunctionRelation::AddNamedParameter(const string &name, Value argument\n \tnamed_parameters[name] = std::move(argument);\n }\n \n-TableFunctionRelation::TableFunctionRelation(const std::shared_ptr<ClientContext> &context, string name_p,\n+void TableFunctionRelation::SetNamedParameters(named_parameter_map_t &&options) {\n+\tD_ASSERT(named_parameters.empty());\n+\tnamed_parameters = std::move(options);\n+}\n+\n+TableFunctionRelation::TableFunctionRelation(const shared_ptr<ClientContext> &context, string name_p,\n                                              vector<Value> parameters_p, named_parameter_map_t named_parameters,\n                                              shared_ptr<Relation> input_relation_p, bool auto_init)\n     : Relation(context, RelationType::TABLE_FUNCTION_RELATION), name(std::move(name_p)),\n@@ -25,7 +31,7 @@ TableFunctionRelation::TableFunctionRelation(const std::shared_ptr<ClientContext\n \tInitializeColumns();\n }\n \n-TableFunctionRelation::TableFunctionRelation(const std::shared_ptr<ClientContext> &context, string name_p,\n+TableFunctionRelation::TableFunctionRelation(const shared_ptr<ClientContext> &context, string name_p,\n                                              vector<Value> parameters_p, shared_ptr<Relation> input_relation_p,\n                                              bool auto_init)\n     : Relation(context, RelationType::TABLE_FUNCTION_RELATION), name(std::move(name_p)),\ndiff --git a/tools/pythonpkg/src/pyconnection.cpp b/tools/pythonpkg/src/pyconnection.cpp\nindex 37c36bb18880..afe91a8e6cdf 100644\n--- a/tools/pythonpkg/src/pyconnection.cpp\n+++ b/tools/pythonpkg/src/pyconnection.cpp\n@@ -713,6 +713,7 @@ unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadCSV(\n \tauto path_like = GetPathLike(name_p);\n \tauto &name = path_like.str;\n \tauto file_like_object_wrapper = std::move(path_like.dependency);\n+\tnamed_parameter_map_t bind_parameters;\n \n \t// First check if the header is explicitly set\n \t// when false this affects the returned types, so it needs to be known at initialization of the relation\n@@ -721,42 +722,54 @@ unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadCSV(\n \t\tbool header_as_int = py::isinstance<py::int_>(header);\n \t\tbool header_as_bool = py::isinstance<py::bool_>(header);\n \n+\t\tbool header_value;\n \t\tif (header_as_bool) {\n-\t\t\toptions.SetHeader(py::bool_(header));\n+\t\t\theader_value = py::bool_(header);\n \t\t} else if (header_as_int) {\n \t\t\tif ((int)py::int_(header) != 0) {\n \t\t\t\tthrow InvalidInputException(\"read_csv only accepts 0 if 'header' is given as an integer\");\n \t\t\t}\n-\t\t\toptions.SetHeader(true);\n+\t\t\theader_value = true;\n \t\t} else {\n \t\t\tthrow InvalidInputException(\"read_csv only accepts 'header' as an integer, or a boolean\");\n \t\t}\n+\t\tbind_parameters[\"header\"] = Value::BOOLEAN(header_value);\n \t}\n \n-\t// We want to detect if the file can be opened, we set this in the options so we can detect this at bind time\n-\t// rather than only at execution time\n \tif (!py::none().is(compression)) {\n \t\tif (!py::isinstance<py::str>(compression)) {\n \t\t\tthrow InvalidInputException(\"read_csv only accepts 'compression' as a string\");\n \t\t}\n-\t\toptions.SetCompression(py::str(compression));\n+\t\tbind_parameters[\"compression\"] = Value(py::str(compression));\n \t}\n \n-\tauto read_csv_p = connection->ReadCSV(name, options);\n-\tauto &read_csv = read_csv_p->Cast<ReadCSVRelation>();\n-\tif (file_like_object_wrapper) {\n-\t\tD_ASSERT(!read_csv.extra_dependencies);\n-\t\tread_csv.extra_dependencies = std::move(file_like_object_wrapper);\n-\t}\n-\n-\tif (options.has_header) {\n-\t\t// 'options' is only used to initialize the ReadCSV relation\n-\t\t// we also need to set this in the arguments passed to the function\n-\t\tread_csv.AddNamedParameter(\"header\", Value::BOOLEAN(options.dialect_options.header));\n-\t}\n-\n-\tif (options.compression != FileCompressionType::AUTO_DETECT) {\n-\t\tread_csv.AddNamedParameter(\"compression\", Value(py::str(compression)));\n+\tif (!py::none().is(dtype)) {\n+\t\tif (py::isinstance<py::dict>(dtype)) {\n+\t\t\tchild_list_t<Value> struct_fields;\n+\t\t\tpy::dict dtype_dict = dtype;\n+\t\t\tfor (auto &kv : dtype_dict) {\n+\t\t\t\tshared_ptr<DuckDBPyType> sql_type;\n+\t\t\t\tif (!py::try_cast(kv.second, sql_type)) {\n+\t\t\t\t\tthrow py::value_error(\"The types provided to 'dtype' have to be DuckDBPyType\");\n+\t\t\t\t}\n+\t\t\t\tstruct_fields.emplace_back(py::str(kv.first), Value(py::str(kv.second)));\n+\t\t\t}\n+\t\t\tauto dtype_struct = Value::STRUCT(std::move(struct_fields));\n+\t\t\tbind_parameters[\"dtypes\"] = std::move(dtype_struct);\n+\t\t} else if (py::isinstance<py::list>(dtype)) {\n+\t\t\tvector<Value> list_values;\n+\t\t\tpy::list dtype_list = dtype;\n+\t\t\tfor (auto &child : dtype_list) {\n+\t\t\t\tshared_ptr<DuckDBPyType> sql_type;\n+\t\t\t\tif (!py::try_cast(child, sql_type)) {\n+\t\t\t\t\tthrow py::value_error(\"The types provided to 'dtype' have to be DuckDBPyType\");\n+\t\t\t\t}\n+\t\t\t\tlist_values.push_back(std::string(py::str(child)));\n+\t\t\t}\n+\t\t\tbind_parameters[\"dtypes\"] = Value::LIST(LogicalType::VARCHAR, std::move(list_values));\n+\t\t} else {\n+\t\t\tthrow InvalidInputException(\"read_csv only accepts 'dtype' as a dictionary or a list of strings\");\n+\t\t}\n \t}\n \n \tbool has_sep = !py::none().is(sep);\n@@ -765,9 +778,9 @@ unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadCSV(\n \t\tthrow InvalidInputException(\"read_csv takes either 'delimiter' or 'sep', not both\");\n \t}\n \tif (has_sep) {\n-\t\tread_csv.AddNamedParameter(\"delim\", Value(py::str(sep)));\n+\t\tbind_parameters[\"delim\"] = Value(py::str(sep));\n \t} else if (has_delimiter) {\n-\t\tread_csv.AddNamedParameter(\"delim\", Value(py::str(delimiter)));\n+\t\tbind_parameters[\"delim\"] = Value(py::str(delimiter));\n \t}\n \n \t// We don't support overriding the names of the header yet\n@@ -787,63 +800,39 @@ unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadCSV(\n \t//\t// FIXME: Check for uniqueness of 'names' ?\n \t//}\n \n-\tif (!py::none().is(dtype)) {\n-\t\tif (py::isinstance<py::dict>(dtype)) {\n-\t\t\tchild_list_t<Value> struct_fields;\n-\t\t\tpy::dict dtype_dict = dtype;\n-\t\t\tfor (auto &kv : dtype_dict) {\n-\t\t\t\tstruct_fields.emplace_back(py::str(kv.first), Value(py::str(kv.second)));\n-\t\t\t}\n-\t\t\tauto dtype_struct = Value::STRUCT(std::move(struct_fields));\n-\t\t\tread_csv.AddNamedParameter(\"dtypes\", std::move(dtype_struct));\n-\t\t} else if (py::isinstance<py::list>(dtype)) {\n-\t\t\tauto dtype_list = TransformPythonValue(py::list(dtype));\n-\t\t\tD_ASSERT(dtype_list.type().id() == LogicalTypeId::LIST);\n-\t\t\tauto &children = ListValue::GetChildren(dtype_list);\n-\t\t\tfor (auto &child : children) {\n-\t\t\t\tif (child.type().id() != LogicalTypeId::VARCHAR) {\n-\t\t\t\t\tthrow InvalidInputException(\"The types provided to 'dtype' have to be strings\");\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tread_csv.AddNamedParameter(\"dtypes\", std::move(dtype_list));\n-\t\t} else {\n-\t\t\tthrow InvalidInputException(\"read_csv only accepts 'dtype' as a dictionary or a list of strings\");\n-\t\t}\n-\t}\n-\n \tif (!py::none().is(na_values)) {\n \t\tif (!py::isinstance<py::str>(na_values)) {\n \t\t\tthrow InvalidInputException(\"read_csv only accepts 'na_values' as a string\");\n \t\t}\n-\t\tread_csv.AddNamedParameter(\"nullstr\", Value(py::str(na_values)));\n+\t\tbind_parameters[\"nullstr\"] = Value(py::str(na_values));\n \t}\n \n \tif (!py::none().is(skiprows)) {\n \t\tif (!py::isinstance<py::int_>(skiprows)) {\n \t\t\tthrow InvalidInputException(\"read_csv only accepts 'skiprows' as an integer\");\n \t\t}\n-\t\tread_csv.AddNamedParameter(\"skip\", Value::INTEGER(py::int_(skiprows)));\n+\t\tbind_parameters[\"skip\"] = Value::INTEGER(py::int_(skiprows));\n \t}\n \n \tif (!py::none().is(parallel)) {\n \t\tif (!py::isinstance<py::bool_>(parallel)) {\n \t\t\tthrow InvalidInputException(\"read_csv only accepts 'parallel' as a boolean\");\n \t\t}\n-\t\tread_csv.AddNamedParameter(\"parallel\", Value::BOOLEAN(py::bool_(parallel)));\n+\t\tbind_parameters[\"parallel\"] = Value::BOOLEAN(py::bool_(parallel));\n \t}\n \n \tif (!py::none().is(quotechar)) {\n \t\tif (!py::isinstance<py::str>(quotechar)) {\n \t\t\tthrow InvalidInputException(\"read_csv only accepts 'quotechar' as a string\");\n \t\t}\n-\t\tread_csv.AddNamedParameter(\"quote\", Value(py::str(quotechar)));\n+\t\tbind_parameters[\"quote\"] = Value(py::str(quotechar));\n \t}\n \n \tif (!py::none().is(escapechar)) {\n \t\tif (!py::isinstance<py::str>(escapechar)) {\n \t\t\tthrow InvalidInputException(\"read_csv only accepts 'escapechar' as a string\");\n \t\t}\n-\t\tread_csv.AddNamedParameter(\"escape\", Value(py::str(escapechar)));\n+\t\tbind_parameters[\"escape\"] = Value(py::str(escapechar));\n \t}\n \n \tif (!py::none().is(encoding)) {\n@@ -860,49 +849,58 @@ unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadCSV(\n \t\tif (!py::isinstance<py::str>(date_format)) {\n \t\t\tthrow InvalidInputException(\"read_csv only accepts 'date_format' as a string\");\n \t\t}\n-\t\tread_csv.AddNamedParameter(\"dateformat\", Value(py::str(date_format)));\n+\t\tbind_parameters[\"dateformat\"] = Value(py::str(date_format));\n \t}\n \n \tif (!py::none().is(timestamp_format)) {\n \t\tif (!py::isinstance<py::str>(timestamp_format)) {\n \t\t\tthrow InvalidInputException(\"read_csv only accepts 'timestamp_format' as a string\");\n \t\t}\n-\t\tread_csv.AddNamedParameter(\"timestampformat\", Value(py::str(timestamp_format)));\n+\t\tbind_parameters[\"timestampformat\"] = Value(py::str(timestamp_format));\n \t}\n \n \tif (!py::none().is(sample_size)) {\n \t\tif (!py::isinstance<py::int_>(sample_size)) {\n \t\t\tthrow InvalidInputException(\"read_csv only accepts 'sample_size' as an integer\");\n \t\t}\n-\t\tread_csv.AddNamedParameter(\"sample_size\", Value::INTEGER(py::int_(sample_size)));\n+\t\tbind_parameters[\"sample_size\"] = Value::INTEGER(py::int_(sample_size));\n \t}\n \n \tif (!py::none().is(all_varchar)) {\n \t\tif (!py::isinstance<py::bool_>(all_varchar)) {\n \t\t\tthrow InvalidInputException(\"read_csv only accepts 'all_varchar' as a boolean\");\n \t\t}\n-\t\tread_csv.AddNamedParameter(\"all_varchar\", Value::INTEGER(py::bool_(all_varchar)));\n+\t\tbind_parameters[\"all_varchar\"] = Value::BOOLEAN(py::bool_(all_varchar));\n \t}\n \n \tif (!py::none().is(normalize_names)) {\n \t\tif (!py::isinstance<py::bool_>(normalize_names)) {\n \t\t\tthrow InvalidInputException(\"read_csv only accepts 'normalize_names' as a boolean\");\n \t\t}\n-\t\tread_csv.AddNamedParameter(\"normalize_names\", Value::INTEGER(py::bool_(normalize_names)));\n+\t\tbind_parameters[\"normalize_names\"] = Value::BOOLEAN(py::bool_(normalize_names));\n \t}\n \n \tif (!py::none().is(filename)) {\n \t\tif (!py::isinstance<py::bool_>(filename)) {\n \t\t\tthrow InvalidInputException(\"read_csv only accepts 'filename' as a boolean\");\n \t\t}\n-\t\tread_csv.AddNamedParameter(\"filename\", Value::INTEGER(py::bool_(filename)));\n+\t\tbind_parameters[\"filename\"] = Value::BOOLEAN(py::bool_(filename));\n \t}\n \n \tif (!py::none().is(null_padding)) {\n \t\tif (!py::isinstance<py::bool_>(null_padding)) {\n \t\t\tthrow InvalidInputException(\"read_csv only accepts 'null_padding' as a boolean\");\n \t\t}\n-\t\tread_csv.AddNamedParameter(\"null_padding\", Value::INTEGER(py::bool_(null_padding)));\n+\t\tbind_parameters[\"null_padding\"] = Value::BOOLEAN(py::bool_(null_padding));\n+\t}\n+\n+\t// Create the ReadCSV Relation using the 'options'\n+\n+\tauto read_csv_p = connection->ReadCSV(name, std::move(bind_parameters));\n+\tauto &read_csv = read_csv_p->Cast<ReadCSVRelation>();\n+\tif (file_like_object_wrapper) {\n+\t\tD_ASSERT(!read_csv.extra_dependencies);\n+\t\tread_csv.extra_dependencies = std::move(file_like_object_wrapper);\n \t}\n \n \treturn make_uniq<DuckDBPyRelation>(read_csv_p->Alias(name));\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/api/test_read_csv.py b/tools/pythonpkg/tests/fast/api/test_read_csv.py\nindex 091b3836570f..4b35064d7365 100644\n--- a/tools/pythonpkg/tests/fast/api/test_read_csv.py\n+++ b/tools/pythonpkg/tests/fast/api/test_read_csv.py\n@@ -14,6 +14,24 @@ def TestFile(name):\n     return filename\n \n \n+@pytest.fixture\n+def create_temp_csv(tmp_path):\n+    # Create temporary CSV files\n+    file1_content = \"\"\"1\n+2\n+3\"\"\"\n+    file2_content = \"\"\"4\n+5\n+6\"\"\"\n+    file1_path = tmp_path / \"file1.csv\"\n+    file2_path = tmp_path / \"file2.csv\"\n+\n+    file1_path.write_text(file1_content)\n+    file2_path.write_text(file2_content)\n+\n+    return file1_path, file2_path\n+\n+\n class TestReadCSV(object):\n     def test_using_connection_wrapper(self):\n         rel = duckdb.read_csv(TestFile('category.csv'))\n@@ -438,3 +456,42 @@ def scoped_objects(duckdb_cursor):\n         assert CountedObject.instance_count == 0\n         scoped_objects(duckdb_cursor)\n         assert CountedObject.instance_count == 0\n+\n+    def test_read_csv_glob(self, tmp_path, create_temp_csv):\n+        file1_path, file2_path = create_temp_csv\n+\n+        # Use the temporary file paths to read CSV files\n+        con = duckdb.connect()\n+        rel = con.read_csv(f'{tmp_path}/file*.csv')\n+        res = con.sql(\"select * from rel order by all\").fetchall()\n+        assert res == [(1,), (2,), (3,), (4,), (5,), (6,)]\n+\n+    def test_read_csv_combined(self):\n+        CSV_FILE = TestFile('stress_test.csv')\n+        COLUMNS = {\n+            'result': 'VARCHAR',\n+            'table': 'BIGINT',\n+            '_time': 'TIMESTAMPTZ',\n+            '_measurement': 'VARCHAR',\n+            'bench_test': 'VARCHAR',\n+            'flight_id': 'VARCHAR',\n+            'flight_status': 'VARCHAR',\n+            'log_level': 'VARCHAR',\n+            'sys_uuid': 'VARCHAR',\n+            'message': 'VARCHAR',\n+        }\n+\n+        rel = duckdb.read_csv(\n+            CSV_FILE, header=True, skiprows=1, delimiter=\",\", quotechar='\"', escapechar=\"\\\\\", dtype=COLUMNS\n+        )\n+        res = rel.fetchall()\n+\n+        rel2 = duckdb.sql(rel.sql_query())\n+        res2 = rel2.fetchall()\n+\n+        # Assert that the results are the same\n+        assert res == res2\n+\n+        # And assert that the columns and types of the relations are the same\n+        assert rel.columns == rel2.columns\n+        assert rel.types == rel2.types\ndiff --git a/tools/pythonpkg/tests/fast/api/test_to_csv.py b/tools/pythonpkg/tests/fast/api/test_to_csv.py\nindex 8230df6aa60d..32b5830b52ae 100644\n--- a/tools/pythonpkg/tests/fast/api/test_to_csv.py\n+++ b/tools/pythonpkg/tests/fast/api/test_to_csv.py\n@@ -1,7 +1,6 @@\n import duckdb\n import tempfile\n import os\n-import tempfile\n import pandas._testing as tm\n import datetime\n import csv\ndiff --git a/tools/pythonpkg/tests/fast/data/stress_test.csv b/tools/pythonpkg/tests/fast/data/stress_test.csv\nnew file mode 100644\nindex 000000000000..ef7737096460\n--- /dev/null\n+++ b/tools/pythonpkg/tests/fast/data/stress_test.csv\n@@ -0,0 +1,51 @@\n+#datatype,string,long,dateTime:RFC3339,string,string,string,string,string,string,string\r\n+,result,table,_time,_measurement,bench_test,flight_id,flight_status,log_level,sys_uuid,message\r\n+,_result,0,2023-05-31T14:24:36.166678Z,logged_messages,False,01H1S35DSKVBFT2GRMX7S23BC9,PASS,INFO,0002000000003636353934385119003b0032,[commander] Takeoff detected\t\r\n+,_result,0,2023-05-31T14:25:04.12191Z,logged_messages,False,01H1S35DSKVBFT2GRMX7S23BC9,PASS,INFO,0002000000003636353934385119003b0032,[commander] Landing detected\t\r\n+,_result,0,2023-05-31T14:25:05.127837Z,logged_messages,False,01H1S35DSKVBFT2GRMX7S23BC9,PASS,INFO,0002000000003636353934385119003b0032,[commander] Disarmed by landing\t\r\n+,_result,1,2023-05-31T14:25:51.814473Z,logged_messages,False,01H1S37SZ4VRTH2HFWQ2PE3JN5,WARNING,INFO,0002000000003636353934385119003b0032,[commander] Takeoff detected\t\r\n+,_result,1,2023-05-31T14:26:24.849541Z,logged_messages,False,01H1S37SZ4VRTH2HFWQ2PE3JN5,WARNING,INFO,0002000000003636353934385119003b0032,[commander] Landing detected\t\r\n+,_result,1,2023-05-31T14:26:25.858165Z,logged_messages,False,01H1S37SZ4VRTH2HFWQ2PE3JN5,WARNING,INFO,0002000000003636353934385119003b0032,[vehicle_imu] ACC 1 (2490386) offset committed: [-0.101 0.195 -0.053]->[-0.123 0.107 -0.129])\r\n+,_result,2,2023-05-31T14:26:24.748908Z,logged_messages,False,01H1S37SZ4VRTH2HFWQ2PE3JN5,WARNING,WARNING,0002000000003636353934385119003b0032,[commander] Kill-switch engaged\t\r\n+,_result,3,2023-05-31T14:28:42.448271Z,logged_messages,False,01H1S3BY5EKMS4RRJWSEJ2PFK2,WARNING,INFO,0002000000003636353934385119003b0032,[commander] Takeoff detected\t\r\n+,_result,3,2023-05-31T14:31:19.948154Z,logged_messages,False,01H1S3BY5EKMS4RRJWSEJ2PFK2,WARNING,INFO,0002000000003636353934385119003b0032,[commander] Landing detected\t\r\n+,_result,3,2023-05-31T14:31:20.955221Z,logged_messages,False,01H1S3BY5EKMS4RRJWSEJ2PFK2,WARNING,INFO,0002000000003636353934385119003b0032,[commander] Disarmed by landing\t\r\n+,_result,4,2023-05-31T14:30:27.230708Z,logged_messages,False,01H1S3BY5EKMS4RRJWSEJ2PFK2,WARNING,WARNING,0002000000003636353934385119003b0032,[commander] Motor failure detected\t\r\n+,_result,4,2023-05-31T14:31:20.967383Z,logged_messages,False,01H1S3BY5EKMS4RRJWSEJ2PFK2,WARNING,WARNING,0002000000003636353934385119003b0032,[commander] Motor recovered\t\r\n+,_result,5,2023-05-31T14:37:25.583406Z,logged_messages,False,01H1S3WCMCRHWBS7FN6QZYRGCV,PASS,INFO,0002000000003636353934385119003b0032,[logger] Start mavlink log\r\n+,_result,5,2023-05-31T14:40:25.094086Z,logged_messages,False,01H1S3WCMCRHWBS7FN6QZYRGCV,PASS,INFO,0002000000003636353934385119003b0032,[commander] Landing detected\t\r\n+,_result,5,2023-05-31T14:40:26.104179Z,logged_messages,False,01H1S3WCMCRHWBS7FN6QZYRGCV,PASS,INFO,0002000000003636353934385119003b0032,[commander] Disarmed by landing\t\r\n+,_result,5,2023-05-31T14:40:26.106874Z,logged_messages,False,01H1S3WCMCRHWBS7FN6QZYRGCV,PASS,INFO,0002000000003636353934385119003b0032,[vehicle_imu] ACC 1 (2490386) offset committed: [-0.123 0.107 -0.129]->[-0.094 0.174 -0.106])\r\n+,_result,6,2023-05-31T14:45:49.739497Z,logged_messages,False,01H1S4BG6HSNCW3JZCGQYQDD8W,PASS,INFO,0002000000003636353934385119003b0032,[commander] Takeoff detected\t\r\n+,_result,6,2023-05-31T14:49:15.491698Z,logged_messages,False,01H1S4BG6HSNCW3JZCGQYQDD8W,PASS,INFO,0002000000003636353934385119003b0032,[commander] Landing detected\t\r\n+,_result,7,2023-05-31T15:02:34.912835Z,logged_messages,False,01H1S5A3TF3E2F8DFCXSAS1H7S,WARNING,INFO,0002000000003636353934385119003b0032,[commander] Takeoff detected\t\r\n+,_result,7,2023-05-31T15:04:52.167054Z,logged_messages,False,01H1S5A3TF3E2F8DFCXSAS1H7S,WARNING,INFO,0002000000003636353934385119003b0032,[commander] Landing detected\t\r\n+,_result,7,2023-05-31T15:04:53.175211Z,logged_messages,False,01H1S5A3TF3E2F8DFCXSAS1H7S,WARNING,INFO,0002000000003636353934385119003b0032,[commander] Disarmed by landing\t\r\n+,_result,7,2023-05-31T15:04:53.179601Z,logged_messages,False,01H1S5A3TF3E2F8DFCXSAS1H7S,WARNING,INFO,0002000000003636353934385119003b0032,[vehicle_imu] ACC 1 (2490386) offset committed: [-0.064 0.116 -0.096]->[-0.010 0.087 -0.095])\r\n+,_result,8,2023-05-31T15:03:37.487853Z,logged_messages,False,01H1S5A3TF3E2F8DFCXSAS1H7S,WARNING,WARNING,0002000000003636353934385119003b0032,[commander] Motor failure detected\t\r\n+,_result,8,2023-05-31T15:04:53.20197Z,logged_messages,False,01H1S5A3TF3E2F8DFCXSAS1H7S,WARNING,WARNING,0002000000003636353934385119003b0032,[commander] Motor recovered\t\r\n+,_result,9,2023-05-31T15:32:01.2761Z,logged_messages,False,01H1S5X09BG7H5V6QC75EEK88B,FAIL,INFO,0002000000003636353934385119003b0032,[commander] Landing detected\t\r\n+,_result,10,2023-05-31T15:16:25.09561Z,logged_messages,False,01H1S5X09BG7H5V6QC75EEK88B,FAIL,WARNING,0002000000003636353934385119003b0032,[commander] Motor failure detected\t\r\n+,_result,11,2023-05-31T15:58:50.86231Z,logged_messages,False,01H1S8HAYM5S7P1V8MGPV9T4VB,WARNING,INFO,0002000000003636353934385119003b0032,[commander] Takeoff detected\t\r\n+,_result,11,2023-05-31T16:00:43.036327Z,logged_messages,False,01H1S8HAYM5S7P1V8MGPV9T4VB,WARNING,INFO,0002000000003636353934385119003b0032,\"[commander] Low battery level, return advised\t\"\r\n+,_result,11,2023-05-31T16:00:43.670149Z,logged_messages,False,01H1S8HAYM5S7P1V8MGPV9T4VB,WARNING,INFO,0002000000003636353934385119003b0032,[commander] Landing detected\t\r\n+,_result,12,2023-05-31T15:59:36.010261Z,logged_messages,False,01H1S8HAYM5S7P1V8MGPV9T4VB,WARNING,WARNING,0002000000003636353934385119003b0032,[commander] Motor failure detected\t\r\n+,_result,12,2023-05-31T16:00:44.704726Z,logged_messages,False,01H1S8HAYM5S7P1V8MGPV9T4VB,WARNING,WARNING,0002000000003636353934385119003b0032,[commander] Motor recovered\t\r\n+,_result,13,2023-05-31T16:16:50.26879Z,logged_messages,False,01H1S9J63M323D6J51YJKGKPWX,WARNING,INFO,0002000000003636353934385119003b0032,[commander] Takeoff detected\t\r\n+,_result,13,2023-05-31T16:17:45.320423Z,logged_messages,False,01H1S9J63M323D6J51YJKGKPWX,WARNING,INFO,0002000000003636353934385119003b0032,[commander] Landing detected\t\r\n+,_result,13,2023-05-31T16:17:46.17427Z,logged_messages,False,01H1S9J63M323D6J51YJKGKPWX,WARNING,INFO,0002000000003636353934385119003b0032,[commander] Disarmed by RC\t\r\n+,_result,13,2023-05-31T16:17:46.178138Z,logged_messages,False,01H1S9J63M323D6J51YJKGKPWX,WARNING,INFO,0002000000003636353934385119003b0032,[vehicle_imu] ACC 1 (2490386) offset committed: [0.141 0.219 -0.129]->[0.192 0.261 -0.113])\r\n+,_result,14,2023-05-31T16:24:59.614529Z,logged_messages,False,01H1SA15A2ECGZ1KWAN4DH32M1,PASS,INFO,0002000000003636353934385119003b0032,[logger] Start mavlink log\r\n+,_result,14,2023-05-31T16:25:01.581388Z,logged_messages,False,01H1SA15A2ECGZ1KWAN4DH32M1,PASS,INFO,0002000000003636353934385119003b0032,[commander] Takeoff detected\t\r\n+,_result,14,2023-05-31T16:29:01.527536Z,logged_messages,False,01H1SA15A2ECGZ1KWAN4DH32M1,PASS,INFO,0002000000003636353934385119003b0032,[commander] Landing detected\t\r\n+,_result,15,2023-05-31T16:47:05.171501Z,logged_messages,False,01H1SAPT98GKTZYK0DH09943ZT,WARNING,INFO,0002000000003636353934385119003b0032,[commander] Landing detected\t\r\n+,_result,15,2023-05-31T16:47:05.428597Z,logged_messages,False,01H1SAPT98GKTZYK0DH09943ZT,WARNING,INFO,0002000000003636353934385119003b0032,[commander] Kill-switch engaged\t\r\n+,_result,16,2023-05-31T16:37:21.403343Z,logged_messages,False,01H1SAPT98GKTZYK0DH09943ZT,WARNING,WARNING,0002000000003636353934385119003b0032,[commander] Motor failure detected\t\r\n+,_result,16,2023-05-31T16:47:06.211374Z,logged_messages,False,01H1SAPT98GKTZYK0DH09943ZT,WARNING,WARNING,0002000000003636353934385119003b0032,[commander] Motor recovered\t\r\n+,_result,17,2023-05-31T17:12:01.608221Z,logged_messages,False,01H1SCQDVME5Q88HB7N7YVPC3H,WARNING,INFO,0002000000003636353934385119003b0032,[commander] Takeoff detected\t\r\n+,_result,17,2023-05-31T17:17:45.160216Z,logged_messages,False,01H1SCQDVME5Q88HB7N7YVPC3H,WARNING,INFO,0002000000003636353934385119003b0032,[commander] Landing detected\t\r\n+,_result,17,2023-05-31T17:17:46.173924Z,logged_messages,False,01H1SCQDVME5Q88HB7N7YVPC3H,WARNING,INFO,0002000000003636353934385119003b0032,[vehicle_imu] ACC 1 (2490386) offset committed: [0.068 -0.039 -0.127]->[0.074 0.162 -0.151])\r\n+,_result,18,2023-05-31T17:16:26.049831Z,logged_messages,False,01H1SCQDVME5Q88HB7N7YVPC3H,WARNING,WARNING,0002000000003636353934385119003b0032,[commander] Motor failure detected\t\r\n+,_result,18,2023-05-31T17:17:46.194896Z,logged_messages,False,01H1SCQDVME5Q88HB7N7YVPC3H,WARNING,WARNING,0002000000003636353934385119003b0032,[commander] Motor recovered\t\r\n+,_result,19,2023-05-31T17:25:58.746933Z,logged_messages,False,01H1SD7J6V62CN3YTJ7QS7715D,WARNING,INFO,0002000000003636353934385119003b0032,[commander] Landing detected\t\r\n+,_result,20,2023-05-31T17:22:26.896489Z,logged_messages,False,01H1SD7J6V62CN3YTJ7QS7715D,WARNING,WARNING,0002000000003636353934385119003b0032,[commander] Motor failure detected\t\r\n",
  "problem_statement": "[Python] relation.types and relation.columns values incorrect after creating relation with read_csv()\n### What happens?\n\nWhen using `duckdb.read_csv` to load a CSV file that has a date column in a format that uses an explicit date parsing template for, the value of the `types` property of the resulting relation is showing that the date column has type `VARCHAR`, even though inspecting the relation shows the parsing to have worked and produced a `DATE`.\n\n### To Reproduce\n\n```python\r\nwith open(\"test.csv\", \"w\") as f:\r\n    f.write(\"date,greeting\\n\")\r\n    f.write(\"July 11 2023,quack\\n\")\r\n\r\nrel = duckdb.read_csv(\r\n    \"test.csv\", dtype={\"date\": duckdb.typing.DATE}, date_format=\"%B %d %Y\"\r\n)\r\nprint(rel)\r\nrel.types\r\n```\r\n\r\nResulting output:\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502    date    \u2502 greeting \u2502\r\n\u2502    date    \u2502 varchar  \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 2023-07-11 \u2502 quack    \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n[VARCHAR, VARCHAR]\r\n```\n\n### OS:\n\nUbuntu 23.04\n\n### DuckDB Version:\n\n0.8.2.dev1435\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nNed Letcher\n\n### Affiliation:\n\nThoughtworks\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "Ahh, and I just realised that there's an issue with the column names of the relation produced from `read_csv`. They are set to `column0` and `column1`.\r\n\r\nOnce this relation is queried from, both the types and columns properties are set correctly in the subsequently created relation:\r\n\r\n```python\r\nnew_rel = duckdb.sql(\"FROM test\")\r\nprint(rel.types)\r\nprint(rel.columns)\r\n```\r\nOutput:\r\n```\r\n[DATE, VARCHAR]\r\n['date', 'greeting']\r\n```",
  "created_at": "2023-07-27T09:54:02Z"
}