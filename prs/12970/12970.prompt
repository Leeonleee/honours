You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Cannot filter for NULL value on a hive partitioned column from parquet
### What happens?

When attempting to filter for NULL values on a hive-partitioned parquet directory structure using `read_parquet` - DuckDB returns 0 rows.


### To Reproduce

```sql
CALL dbgen(sf = 0.01);

ALTER TABLE orders
    ALTER COLUMN o_orderpriority DROP NOT NULL;

UPDATE orders
SET o_orderpriority = NULL
WHERE o_orderkey BETWEEN 1 AND 10;

COPY orders TO '/tmp/orders'
    (FORMAT PARQUET, PARTITION_BY (o_orderpriority), OVERWRITE_OR_IGNORE, COMPRESSION zstd);

create or replace view orders_hive_view as
select *
from read_parquet('/tmp/orders/*/*.parquet'
    , hive_partitioning = true
    , hive_types = {'o_orderpriority': VARCHAR}
                   );

select coalesce(o_orderpriority, 'it is NULL') as o_orderpriority, count(*)
from orders_hive_view
group by all
order by 1;

select count(*)
  from orders_hive_view
where o_orderpriority IS NULL;
```

This produces output:
```text
duckdb
v1.0.0 1f98600c2c
Enter ".help" for usage hints.
Connected to a transient in-memory database.
Use ".open FILENAME" to reopen on a persistent database.
D CALL dbgen(sf = 0.01);
┌─────────┐
│ Success │
│ boolean │
├─────────┤
│ 0 rows  │
└─────────┘
D 
D ALTER TABLE orders
      ALTER COLUMN o_orderpriority DROP NOT NULL;
D 
D UPDATE orders
  SET o_orderpriority = NULL
  WHERE o_orderkey BETWEEN 1 AND 10;
D 
D COPY orders TO '/tmp/orders'
      (FORMAT PARQUET, PARTITION_BY (o_orderpriority), OVERWRITE_OR_IGNORE, COMPRESSION zstd);
D 
D create or replace view orders_hive_view as
  select *
  from read_parquet('/tmp/orders/*/*.parquet'
      , hive_partitioning = true
      , hive_types = {'o_orderpriority': VARCHAR}
                     );
D 
D select coalesce(o_orderpriority, 'it is NULL') as o_orderpriority, count(*)
  from orders_hive_view
  group by all
  order by 1;
┌─────────────────┬──────────────┐
│ o_orderpriority │ count_star() │
│     varchar     │    int64     │
├─────────────────┼──────────────┤
│ 1-URGENT        │         3019 │
│ 2-HIGH          │         3064 │
│ 3-MEDIUM        │         2941 │
│ 4-NOT SPECIFIED │         3023 │
│ 5-LOW           │         2946 │
│ it is NULL      │            7 │
└─────────────────┴──────────────┘
D 
D select count(*)
    from orders_hive_view
  where o_orderpriority IS NULL;
┌──────────────┐
│ count_star() │
│    int64     │
├──────────────┤
│            0 │
└──────────────┘
```

### OS:

Linux - x86-64

### DuckDB Version:

1.0.0

### DuckDB Client:

cli

### Full Name:

Philip Moore

### Affiliation:

Voltron Data

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Not applicable - the reproduction does not require a data set

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://www.duckdb.org/docs/installation) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/common/hive_partitioning.cpp]
1: #include "duckdb/common/hive_partitioning.hpp"
2: 
3: #include "duckdb/common/uhugeint.hpp"
4: #include "duckdb/execution/expression_executor.hpp"
5: #include "duckdb/optimizer/filter_combiner.hpp"
6: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
7: #include "duckdb/planner/expression/bound_constant_expression.hpp"
8: #include "duckdb/planner/expression/bound_reference_expression.hpp"
9: #include "duckdb/planner/expression_iterator.hpp"
10: #include "duckdb/planner/table_filter.hpp"
11: #include "duckdb/common/multi_file_list.hpp"
12: #include "re2/re2.h"
13: 
14: namespace duckdb {
15: 
16: static unordered_map<column_t, string> GetKnownColumnValues(string &filename,
17:                                                             unordered_map<string, column_t> &column_map,
18:                                                             duckdb_re2::RE2 &compiled_regex, bool filename_col,
19:                                                             bool hive_partition_cols) {
20: 	unordered_map<column_t, string> result;
21: 
22: 	if (filename_col) {
23: 		auto lookup_column_id = column_map.find("filename");
24: 		if (lookup_column_id != column_map.end()) {
25: 			result[lookup_column_id->second] = filename;
26: 		}
27: 	}
28: 
29: 	if (hive_partition_cols) {
30: 		auto partitions = HivePartitioning::Parse(filename, compiled_regex);
31: 		for (auto &partition : partitions) {
32: 			auto lookup_column_id = column_map.find(partition.first);
33: 			if (lookup_column_id != column_map.end()) {
34: 				result[lookup_column_id->second] = partition.second;
35: 			}
36: 		}
37: 	}
38: 
39: 	return result;
40: }
41: 
42: // Takes an expression and converts a list of known column_refs to constants
43: static void ConvertKnownColRefToConstants(unique_ptr<Expression> &expr,
44:                                           unordered_map<column_t, string> &known_column_values, idx_t table_index) {
45: 	if (expr->type == ExpressionType::BOUND_COLUMN_REF) {
46: 		auto &bound_colref = expr->Cast<BoundColumnRefExpression>();
47: 
48: 		// This bound column ref is for another table
49: 		if (table_index != bound_colref.binding.table_index) {
50: 			return;
51: 		}
52: 
53: 		auto lookup = known_column_values.find(bound_colref.binding.column_index);
54: 		if (lookup != known_column_values.end()) {
55: 			expr = make_uniq<BoundConstantExpression>(Value(lookup->second).DefaultCastAs(bound_colref.return_type));
56: 		}
57: 	} else {
58: 		ExpressionIterator::EnumerateChildren(*expr, [&](unique_ptr<Expression> &child) {
59: 			ConvertKnownColRefToConstants(child, known_column_values, table_index);
60: 		});
61: 	}
62: }
63: 
64: // matches hive partitions in file name. For example:
65: // 	- s3://bucket/var1=value1/bla/bla/var2=value2
66: //  - http(s)://domain(:port)/lala/kasdl/var1=value1/?not-a-var=not-a-value
67: //  - folder/folder/folder/../var1=value1/etc/.//var2=value2
68: const string &HivePartitioning::RegexString() {
69: 	static string REGEX = "[\\/\\\\]([^\\/\\?\\\\]+)=([^\\/\\n\\?\\\\]*)";
70: 	return REGEX;
71: }
72: 
73: std::map<string, string> HivePartitioning::Parse(const string &filename, duckdb_re2::RE2 &regex) {
74: 	std::map<string, string> result;
75: 	duckdb_re2::StringPiece input(filename); // Wrap a StringPiece around it
76: 
77: 	string var;
78: 	string value;
79: 	while (RE2::FindAndConsume(&input, regex, &var, &value)) {
80: 		result.insert(std::pair<string, string>(var, value));
81: 	}
82: 	return result;
83: }
84: 
85: std::map<string, string> HivePartitioning::Parse(const string &filename) {
86: 	duckdb_re2::RE2 regex(RegexString());
87: 	return Parse(filename, regex);
88: }
89: 
90: // TODO: this can still be improved by removing the parts of filter expressions that are true for all remaining files.
91: //		 currently, only expressions that cannot be evaluated during pushdown are removed.
92: void HivePartitioning::ApplyFiltersToFileList(ClientContext &context, vector<string> &files,
93:                                               vector<unique_ptr<Expression>> &filters,
94:                                               unordered_map<string, column_t> &column_map, MultiFilePushdownInfo &info,
95:                                               bool hive_enabled, bool filename_enabled) {
96: 
97: 	vector<string> pruned_files;
98: 	vector<bool> have_preserved_filter(filters.size(), false);
99: 	vector<unique_ptr<Expression>> pruned_filters;
100: 	unordered_set<idx_t> filters_applied_to_files;
101: 	duckdb_re2::RE2 regex(RegexString());
102: 	auto table_index = info.table_index;
103: 
104: 	if ((!filename_enabled && !hive_enabled) || filters.empty()) {
105: 		return;
106: 	}
107: 
108: 	for (idx_t i = 0; i < files.size(); i++) {
109: 		auto &file = files[i];
110: 		bool should_prune_file = false;
111: 		auto known_values = GetKnownColumnValues(file, column_map, regex, filename_enabled, hive_enabled);
112: 
113: 		FilterCombiner combiner(context);
114: 
115: 		for (idx_t j = 0; j < filters.size(); j++) {
116: 			auto &filter = filters[j];
117: 			unique_ptr<Expression> filter_copy = filter->Copy();
118: 			ConvertKnownColRefToConstants(filter_copy, known_values, table_index);
119: 			// Evaluate the filter, if it can be evaluated here, we can not prune this filter
120: 			Value result_value;
121: 
122: 			if (!filter_copy->IsScalar() || !filter_copy->IsFoldable() ||
123: 			    !ExpressionExecutor::TryEvaluateScalar(context, *filter_copy, result_value)) {
124: 				// can not be evaluated only with the filename/hive columns added, we can not prune this filter
125: 				if (!have_preserved_filter[j]) {
126: 					pruned_filters.emplace_back(filter->Copy());
127: 					have_preserved_filter[j] = true;
128: 				}
129: 			} else if (!result_value.GetValue<bool>()) {
130: 				// filter evaluates to false
131: 				should_prune_file = true;
132: 				// convert the filter to a table filter.
133: 				if (filters_applied_to_files.find(j) == filters_applied_to_files.end()) {
134: 					info.extra_info.file_filters += filter->ToString();
135: 					filters_applied_to_files.insert(j);
136: 				}
137: 			}
138: 		}
139: 
140: 		if (!should_prune_file) {
141: 			pruned_files.push_back(file);
142: 		}
143: 	}
144: 
145: 	D_ASSERT(filters.size() >= pruned_filters.size());
146: 
147: 	info.extra_info.total_files = files.size();
148: 	info.extra_info.filtered_files = pruned_files.size();
149: 
150: 	filters = std::move(pruned_filters);
151: 	files = std::move(pruned_files);
152: }
153: 
154: void HivePartitionedColumnData::InitializeKeys() {
155: 	keys.resize(STANDARD_VECTOR_SIZE);
156: 	for (idx_t i = 0; i < STANDARD_VECTOR_SIZE; i++) {
157: 		keys[i].values.resize(group_by_columns.size());
158: 	}
159: }
160: 
161: template <class T>
162: static inline Value GetHiveKeyValue(const T &val) {
163: 	return Value::CreateValue<T>(val);
164: }
165: 
166: template <class T>
167: static inline Value GetHiveKeyValue(const T &val, const LogicalType &type) {
168: 	auto result = GetHiveKeyValue(val);
169: 	result.Reinterpret(type);
170: 	return result;
171: }
172: 
173: static inline Value GetHiveKeyNullValue(const LogicalType &type) {
174: 	Value result;
175: 	result.Reinterpret(type);
176: 	return result;
177: }
178: 
179: template <class T>
180: static void TemplatedGetHivePartitionValues(Vector &input, vector<HivePartitionKey> &keys, const idx_t col_idx,
181:                                             const idx_t count) {
182: 	UnifiedVectorFormat format;
183: 	input.ToUnifiedFormat(count, format);
184: 
185: 	const auto &sel = *format.sel;
186: 	const auto data = UnifiedVectorFormat::GetData<T>(format);
187: 	const auto &validity = format.validity;
188: 
189: 	const auto &type = input.GetType();
190: 
191: 	const auto reinterpret = Value::CreateValue<T>(data[0]).GetTypeMutable() != type;
192: 	if (reinterpret) {
193: 		for (idx_t i = 0; i < count; i++) {
194: 			auto &key = keys[i];
195: 			const auto idx = sel.get_index(i);
196: 			if (validity.RowIsValid(idx)) {
197: 				key.values[col_idx] = GetHiveKeyValue(data[idx], type);
198: 			} else {
199: 				key.values[col_idx] = GetHiveKeyNullValue(type);
200: 			}
201: 		}
202: 	} else {
203: 		for (idx_t i = 0; i < count; i++) {
204: 			auto &key = keys[i];
205: 			const auto idx = sel.get_index(i);
206: 			if (validity.RowIsValid(idx)) {
207: 				key.values[col_idx] = GetHiveKeyValue(data[idx]);
208: 			} else {
209: 				key.values[col_idx] = GetHiveKeyNullValue(type);
210: 			}
211: 		}
212: 	}
213: }
214: 
215: static void GetNestedHivePartitionValues(Vector &input, vector<HivePartitionKey> &keys, const idx_t col_idx,
216:                                          const idx_t count) {
217: 	for (idx_t i = 0; i < count; i++) {
218: 		auto &key = keys[i];
219: 		key.values[col_idx] = input.GetValue(i);
220: 	}
221: }
222: 
223: static void GetHivePartitionValuesTypeSwitch(Vector &input, vector<HivePartitionKey> &keys, const idx_t col_idx,
224:                                              const idx_t count) {
225: 	const auto &type = input.GetType();
226: 	switch (type.InternalType()) {
227: 	case PhysicalType::BOOL:
228: 		TemplatedGetHivePartitionValues<bool>(input, keys, col_idx, count);
229: 		break;
230: 	case PhysicalType::INT8:
231: 		TemplatedGetHivePartitionValues<int8_t>(input, keys, col_idx, count);
232: 		break;
233: 	case PhysicalType::INT16:
234: 		TemplatedGetHivePartitionValues<int16_t>(input, keys, col_idx, count);
235: 		break;
236: 	case PhysicalType::INT32:
237: 		TemplatedGetHivePartitionValues<int32_t>(input, keys, col_idx, count);
238: 		break;
239: 	case PhysicalType::INT64:
240: 		TemplatedGetHivePartitionValues<int64_t>(input, keys, col_idx, count);
241: 		break;
242: 	case PhysicalType::INT128:
243: 		TemplatedGetHivePartitionValues<hugeint_t>(input, keys, col_idx, count);
244: 		break;
245: 	case PhysicalType::UINT8:
246: 		TemplatedGetHivePartitionValues<uint8_t>(input, keys, col_idx, count);
247: 		break;
248: 	case PhysicalType::UINT16:
249: 		TemplatedGetHivePartitionValues<uint16_t>(input, keys, col_idx, count);
250: 		break;
251: 	case PhysicalType::UINT32:
252: 		TemplatedGetHivePartitionValues<uint32_t>(input, keys, col_idx, count);
253: 		break;
254: 	case PhysicalType::UINT64:
255: 		TemplatedGetHivePartitionValues<uint64_t>(input, keys, col_idx, count);
256: 		break;
257: 	case PhysicalType::UINT128:
258: 		TemplatedGetHivePartitionValues<uhugeint_t>(input, keys, col_idx, count);
259: 		break;
260: 	case PhysicalType::FLOAT:
261: 		TemplatedGetHivePartitionValues<float>(input, keys, col_idx, count);
262: 		break;
263: 	case PhysicalType::DOUBLE:
264: 		TemplatedGetHivePartitionValues<double>(input, keys, col_idx, count);
265: 		break;
266: 	case PhysicalType::INTERVAL:
267: 		TemplatedGetHivePartitionValues<interval_t>(input, keys, col_idx, count);
268: 		break;
269: 	case PhysicalType::VARCHAR:
270: 		TemplatedGetHivePartitionValues<string_t>(input, keys, col_idx, count);
271: 		break;
272: 	case PhysicalType::STRUCT:
273: 	case PhysicalType::LIST:
274: 		GetNestedHivePartitionValues(input, keys, col_idx, count);
275: 		break;
276: 	default:
277: 		throw InternalException("Unsupported type for HivePartitionedColumnData::ComputePartitionIndices");
278: 	}
279: }
280: 
281: void HivePartitionedColumnData::ComputePartitionIndices(PartitionedColumnDataAppendState &state, DataChunk &input) {
282: 	const auto count = input.size();
283: 
284: 	input.Hash(group_by_columns, hashes_v);
285: 	hashes_v.Flatten(count);
286: 
287: 	for (idx_t col_idx = 0; col_idx < group_by_columns.size(); col_idx++) {
288: 		auto &group_by_col = input.data[group_by_columns[col_idx]];
289: 		GetHivePartitionValuesTypeSwitch(group_by_col, keys, col_idx, count);
290: 	}
291: 
292: 	const auto hashes = FlatVector::GetData<hash_t>(hashes_v);
293: 	const auto partition_indices = FlatVector::GetData<idx_t>(state.partition_indices);
294: 	for (idx_t i = 0; i < count; i++) {
295: 		auto &key = keys[i];
296: 		key.hash = hashes[i];
297: 		auto lookup = local_partition_map.find(key);
298: 		if (lookup == local_partition_map.end()) {
299: 			idx_t new_partition_id = RegisterNewPartition(key, state);
300: 			partition_indices[i] = new_partition_id;
301: 		} else {
302: 			partition_indices[i] = lookup->second;
303: 		}
304: 	}
305: }
306: 
307: std::map<idx_t, const HivePartitionKey *> HivePartitionedColumnData::GetReverseMap() {
308: 	std::map<idx_t, const HivePartitionKey *> ret;
309: 	for (const auto &pair : local_partition_map) {
310: 		ret[pair.second] = &(pair.first);
311: 	}
312: 	return ret;
313: }
314: 
315: HivePartitionedColumnData::HivePartitionedColumnData(ClientContext &context, vector<LogicalType> types,
316:                                                      vector<idx_t> partition_by_cols,
317:                                                      shared_ptr<GlobalHivePartitionState> global_state)
318:     : PartitionedColumnData(PartitionedColumnDataType::HIVE, context, std::move(types)),
319:       global_state(std::move(global_state)), group_by_columns(std::move(partition_by_cols)),
320:       hashes_v(LogicalType::HASH) {
321: 	InitializeKeys();
322: 	CreateAllocator();
323: }
324: 
325: void HivePartitionedColumnData::AddNewPartition(HivePartitionKey key, idx_t partition_id,
326:                                                 PartitionedColumnDataAppendState &state) {
327: 	local_partition_map.emplace(std::move(key), partition_id);
328: 
329: 	if (state.partition_append_states.size() <= partition_id) {
330: 		state.partition_append_states.resize(partition_id + 1);
331: 		state.partition_buffers.resize(partition_id + 1);
332: 		partitions.resize(partition_id + 1);
333: 	}
334: 	state.partition_append_states[partition_id] = make_uniq<ColumnDataAppendState>();
335: 	state.partition_buffers[partition_id] = CreatePartitionBuffer();
336: 	partitions[partition_id] = CreatePartitionCollection(0);
337: 	partitions[partition_id]->InitializeAppend(*state.partition_append_states[partition_id]);
338: }
339: 
340: idx_t HivePartitionedColumnData::RegisterNewPartition(HivePartitionKey key, PartitionedColumnDataAppendState &state) {
341: 	idx_t partition_id;
342: 	if (global_state) {
343: 		// Synchronize Global state with our local state with the newly discovered partition
344: 		unique_lock<mutex> lck_gstate(global_state->lock);
345: 
346: 		// Insert into global map, or return partition if already present
347: 		auto res = global_state->partition_map.emplace(std::make_pair(key, global_state->partition_map.size()));
348: 		partition_id = res.first->second;
349: 	} else {
350: 		partition_id = local_partition_map.size();
351: 	}
352: 	AddNewPartition(std::move(key), partition_id, state);
353: 	return partition_id;
354: }
355: 
356: } // namespace duckdb
[end of src/common/hive_partitioning.cpp]
[start of src/common/multi_file_list.cpp]
1: #include "duckdb/common/multi_file_reader.hpp"
2: 
3: #include "duckdb/common/exception.hpp"
4: #include "duckdb/common/hive_partitioning.hpp"
5: #include "duckdb/common/types.hpp"
6: #include "duckdb/function/function_set.hpp"
7: #include "duckdb/function/table_function.hpp"
8: #include "duckdb/main/config.hpp"
9: #include "duckdb/planner/operator/logical_get.hpp"
10: #include "duckdb/common/string_util.hpp"
11: 
12: #include <algorithm>
13: 
14: namespace duckdb {
15: 
16: MultiFilePushdownInfo::MultiFilePushdownInfo(LogicalGet &get)
17:     : table_index(get.table_index), column_names(get.names), column_ids(get.column_ids), extra_info(get.extra_info) {
18: }
19: 
20: MultiFilePushdownInfo::MultiFilePushdownInfo(idx_t table_index, const vector<string> &column_names,
21:                                              const vector<column_t> &column_ids, ExtraOperatorInfo &extra_info)
22:     : table_index(table_index), column_names(column_names), column_ids(column_ids), extra_info(extra_info) {
23: }
24: 
25: // Helper method to do Filter Pushdown into a MultiFileList
26: bool PushdownInternal(ClientContext &context, const MultiFileReaderOptions &options, MultiFilePushdownInfo &info,
27:                       vector<unique_ptr<Expression>> &filters, vector<string> &expanded_files) {
28: 	unordered_map<string, column_t> column_map;
29: 	for (idx_t i = 0; i < info.column_ids.size(); i++) {
30: 		if (!IsRowIdColumnId(info.column_ids[i])) {
31: 			column_map.insert({info.column_names[info.column_ids[i]], i});
32: 		}
33: 	}
34: 
35: 	auto start_files = expanded_files.size();
36: 	HivePartitioning::ApplyFiltersToFileList(context, expanded_files, filters, column_map, info,
37: 	                                         options.hive_partitioning, options.filename);
38: 
39: 	if (expanded_files.size() != start_files) {
40: 		return true;
41: 	}
42: 
43: 	return false;
44: }
45: 
46: bool PushdownInternal(ClientContext &context, const MultiFileReaderOptions &options, const vector<string> &names,
47:                       const vector<LogicalType> &types, const vector<column_t> &column_ids,
48:                       const TableFilterSet &filters, vector<string> &expanded_files) {
49: 	idx_t table_index = 0;
50: 	ExtraOperatorInfo extra_info;
51: 
52: 	// construct the pushdown info
53: 	MultiFilePushdownInfo info(table_index, names, column_ids, extra_info);
54: 
55: 	// construct the set of expressions from the table filters
56: 	vector<unique_ptr<Expression>> filter_expressions;
57: 	for (auto &entry : filters.filters) {
58: 		auto column_idx = column_ids[entry.first];
59: 		auto column_ref =
60: 		    make_uniq<BoundColumnRefExpression>(types[column_idx], ColumnBinding(table_index, entry.first));
61: 		auto filter_expr = entry.second->ToExpression(*column_ref);
62: 		filter_expressions.push_back(std::move(filter_expr));
63: 	}
64: 
65: 	// call the original PushdownInternal method
66: 	return PushdownInternal(context, options, info, filter_expressions, expanded_files);
67: }
68: 
69: //===--------------------------------------------------------------------===//
70: // MultiFileListIterator
71: //===--------------------------------------------------------------------===//
72: MultiFileListIterationHelper MultiFileList::Files() {
73: 	return MultiFileListIterationHelper(*this);
74: }
75: 
76: MultiFileListIterationHelper::MultiFileListIterationHelper(MultiFileList &file_list_p) : file_list(file_list_p) {
77: }
78: 
79: MultiFileListIterationHelper::MultiFileListIterator::MultiFileListIterator(MultiFileList *file_list_p)
80:     : file_list(file_list_p) {
81: 	if (!file_list) {
82: 		return;
83: 	}
84: 
85: 	file_list->InitializeScan(file_scan_data);
86: 	if (!file_list->Scan(file_scan_data, current_file)) {
87: 		// There is no first file: move iterator to nop state
88: 		file_list = nullptr;
89: 		file_scan_data.current_file_idx = DConstants::INVALID_INDEX;
90: 	}
91: }
92: 
93: void MultiFileListIterationHelper::MultiFileListIterator::Next() {
94: 	if (!file_list) {
95: 		return;
96: 	}
97: 
98: 	if (!file_list->Scan(file_scan_data, current_file)) {
99: 		// exhausted collection: move iterator to nop state
100: 		file_list = nullptr;
101: 		file_scan_data.current_file_idx = DConstants::INVALID_INDEX;
102: 	}
103: }
104: 
105: MultiFileListIterationHelper::MultiFileListIterator MultiFileListIterationHelper::begin() { // NOLINT: match stl API
106: 	return MultiFileListIterationHelper::MultiFileListIterator(
107: 	    file_list.GetExpandResult() == FileExpandResult::NO_FILES ? nullptr : &file_list);
108: }
109: MultiFileListIterationHelper::MultiFileListIterator MultiFileListIterationHelper::end() { // NOLINT: match stl API
110: 	return MultiFileListIterationHelper::MultiFileListIterator(nullptr);
111: }
112: 
113: MultiFileListIterationHelper::MultiFileListIterator &MultiFileListIterationHelper::MultiFileListIterator::operator++() {
114: 	Next();
115: 	return *this;
116: }
117: 
118: bool MultiFileListIterationHelper::MultiFileListIterator::operator!=(const MultiFileListIterator &other) const {
119: 	return file_list != other.file_list || file_scan_data.current_file_idx != other.file_scan_data.current_file_idx;
120: }
121: 
122: const string &MultiFileListIterationHelper::MultiFileListIterator::operator*() const {
123: 	return current_file;
124: }
125: 
126: //===--------------------------------------------------------------------===//
127: // MultiFileList
128: //===--------------------------------------------------------------------===//
129: MultiFileList::MultiFileList(vector<string> paths, FileGlobOptions options)
130:     : paths(std::move(paths)), glob_options(options) {
131: }
132: 
133: MultiFileList::~MultiFileList() {
134: }
135: 
136: const vector<string> MultiFileList::GetPaths() const {
137: 	return paths;
138: }
139: 
140: void MultiFileList::InitializeScan(MultiFileListScanData &iterator) {
141: 	iterator.current_file_idx = 0;
142: }
143: 
144: bool MultiFileList::Scan(MultiFileListScanData &iterator, string &result_file) {
145: 	D_ASSERT(iterator.current_file_idx != DConstants::INVALID_INDEX);
146: 	auto maybe_file = GetFile(iterator.current_file_idx);
147: 
148: 	if (maybe_file.empty()) {
149: 		D_ASSERT(iterator.current_file_idx >= GetTotalFileCount());
150: 		return false;
151: 	}
152: 
153: 	result_file = maybe_file;
154: 	iterator.current_file_idx++;
155: 	return true;
156: }
157: 
158: unique_ptr<MultiFileList> MultiFileList::ComplexFilterPushdown(ClientContext &context,
159:                                                                const MultiFileReaderOptions &options,
160:                                                                MultiFilePushdownInfo &info,
161:                                                                vector<unique_ptr<Expression>> &filters) {
162: 	// By default the filter pushdown into a multifilelist does nothing
163: 	return nullptr;
164: }
165: 
166: unique_ptr<MultiFileList>
167: MultiFileList::DynamicFilterPushdown(ClientContext &context, const MultiFileReaderOptions &options,
168:                                      const vector<string> &names, const vector<LogicalType> &types,
169:                                      const vector<column_t> &column_ids, TableFilterSet &filters) const {
170: 	// By default the filter pushdown into a multifilelist does nothing
171: 	return nullptr;
172: }
173: 
174: unique_ptr<NodeStatistics> MultiFileList::GetCardinality(ClientContext &context) {
175: 	return nullptr;
176: }
177: 
178: string MultiFileList::GetFirstFile() {
179: 	return GetFile(0);
180: }
181: 
182: bool MultiFileList::IsEmpty() {
183: 	return GetExpandResult() == FileExpandResult::NO_FILES;
184: }
185: 
186: //===--------------------------------------------------------------------===//
187: // SimpleMultiFileList
188: //===--------------------------------------------------------------------===//
189: SimpleMultiFileList::SimpleMultiFileList(vector<string> paths_p)
190:     : MultiFileList(std::move(paths_p), FileGlobOptions::ALLOW_EMPTY) {
191: }
192: 
193: unique_ptr<MultiFileList> SimpleMultiFileList::ComplexFilterPushdown(ClientContext &context_p,
194:                                                                      const MultiFileReaderOptions &options,
195:                                                                      MultiFilePushdownInfo &info,
196:                                                                      vector<unique_ptr<Expression>> &filters) {
197: 	if (!options.hive_partitioning && !options.filename) {
198: 		return nullptr;
199: 	}
200: 
201: 	// FIXME: don't copy list until first file is filtered
202: 	auto file_copy = paths;
203: 	auto res = PushdownInternal(context_p, options, info, filters, file_copy);
204: 
205: 	if (res) {
206: 		return make_uniq<SimpleMultiFileList>(file_copy);
207: 	}
208: 
209: 	return nullptr;
210: }
211: 
212: unique_ptr<MultiFileList>
213: SimpleMultiFileList::DynamicFilterPushdown(ClientContext &context, const MultiFileReaderOptions &options,
214:                                            const vector<string> &names, const vector<LogicalType> &types,
215:                                            const vector<column_t> &column_ids, TableFilterSet &filters) const {
216: 	if (!options.hive_partitioning && !options.filename) {
217: 		return nullptr;
218: 	}
219: 
220: 	// FIXME: don't copy list until first file is filtered
221: 	auto file_copy = paths;
222: 	auto res = PushdownInternal(context, options, names, types, column_ids, filters, file_copy);
223: 	if (res) {
224: 		return make_uniq<SimpleMultiFileList>(file_copy);
225: 	}
226: 
227: 	return nullptr;
228: }
229: 
230: vector<string> SimpleMultiFileList::GetAllFiles() {
231: 	return paths;
232: }
233: 
234: FileExpandResult SimpleMultiFileList::GetExpandResult() {
235: 	if (paths.size() > 1) {
236: 		return FileExpandResult::MULTIPLE_FILES;
237: 	} else if (paths.size() == 1) {
238: 		return FileExpandResult::SINGLE_FILE;
239: 	}
240: 
241: 	return FileExpandResult::NO_FILES;
242: }
243: 
244: string SimpleMultiFileList::GetFile(idx_t i) {
245: 	if (paths.empty() || i >= paths.size()) {
246: 		return "";
247: 	}
248: 
249: 	return paths[i];
250: }
251: 
252: idx_t SimpleMultiFileList::GetTotalFileCount() {
253: 	return paths.size();
254: }
255: 
256: //===--------------------------------------------------------------------===//
257: // GlobMultiFileList
258: //===--------------------------------------------------------------------===//
259: GlobMultiFileList::GlobMultiFileList(ClientContext &context_p, vector<string> paths_p, FileGlobOptions options)
260:     : MultiFileList(std::move(paths_p), options), context(context_p), current_path(0) {
261: }
262: 
263: unique_ptr<MultiFileList> GlobMultiFileList::ComplexFilterPushdown(ClientContext &context_p,
264:                                                                    const MultiFileReaderOptions &options,
265:                                                                    MultiFilePushdownInfo &info,
266:                                                                    vector<unique_ptr<Expression>> &filters) {
267: 	lock_guard<mutex> lck(lock);
268: 
269: 	// Expand all
270: 	// FIXME: lazy expansion
271: 	// FIXME: push down filters into glob
272: 	while (ExpandNextPath()) {
273: 	}
274: 
275: 	if (!options.hive_partitioning && !options.filename) {
276: 		return nullptr;
277: 	}
278: 	auto res = PushdownInternal(context, options, info, filters, expanded_files);
279: 	if (res) {
280: 		return make_uniq<SimpleMultiFileList>(expanded_files);
281: 	}
282: 
283: 	return nullptr;
284: }
285: 
286: unique_ptr<MultiFileList>
287: GlobMultiFileList::DynamicFilterPushdown(ClientContext &context, const MultiFileReaderOptions &options,
288:                                          const vector<string> &names, const vector<LogicalType> &types,
289:                                          const vector<column_t> &column_ids, TableFilterSet &filters) const {
290: 	if (!options.hive_partitioning && !options.filename) {
291: 		return nullptr;
292: 	}
293: 	lock_guard<mutex> lck(lock);
294: 
295: 	// Expand all paths into a copy
296: 	// FIXME: lazy expansion and push filters into glob
297: 	idx_t path_index = current_path;
298: 	auto file_list = expanded_files;
299: 	while (ExpandPathInternal(path_index, file_list)) {
300: 	}
301: 
302: 	auto res = PushdownInternal(context, options, names, types, column_ids, filters, file_list);
303: 	if (res) {
304: 		return make_uniq<SimpleMultiFileList>(file_list);
305: 	}
306: 
307: 	return nullptr;
308: }
309: 
310: vector<string> GlobMultiFileList::GetAllFiles() {
311: 	lock_guard<mutex> lck(lock);
312: 	while (ExpandNextPath()) {
313: 	}
314: 	return expanded_files;
315: }
316: 
317: idx_t GlobMultiFileList::GetTotalFileCount() {
318: 	lock_guard<mutex> lck(lock);
319: 	while (ExpandNextPath()) {
320: 	}
321: 	return expanded_files.size();
322: }
323: 
324: FileExpandResult GlobMultiFileList::GetExpandResult() {
325: 	// GetFile(1) will ensure at least the first 2 files are expanded if they are available
326: 	GetFile(1);
327: 
328: 	if (expanded_files.size() > 1) {
329: 		return FileExpandResult::MULTIPLE_FILES;
330: 	} else if (expanded_files.size() == 1) {
331: 		return FileExpandResult::SINGLE_FILE;
332: 	}
333: 
334: 	return FileExpandResult::NO_FILES;
335: }
336: 
337: string GlobMultiFileList::GetFile(idx_t i) {
338: 	lock_guard<mutex> lck(lock);
339: 	return GetFileInternal(i);
340: }
341: 
342: string GlobMultiFileList::GetFileInternal(idx_t i) {
343: 	while (expanded_files.size() <= i) {
344: 		if (!ExpandNextPath()) {
345: 			return "";
346: 		}
347: 	}
348: 	D_ASSERT(expanded_files.size() > i);
349: 	return expanded_files[i];
350: }
351: 
352: bool GlobMultiFileList::ExpandPathInternal(idx_t &current_path, vector<string> &result) const {
353: 	if (current_path >= paths.size()) {
354: 		return false;
355: 	}
356: 
357: 	auto &fs = FileSystem::GetFileSystem(context);
358: 	auto glob_files = fs.GlobFiles(paths[current_path], context, glob_options);
359: 	std::sort(glob_files.begin(), glob_files.end());
360: 	result.insert(result.end(), glob_files.begin(), glob_files.end());
361: 
362: 	current_path++;
363: 	return true;
364: }
365: 
366: bool GlobMultiFileList::ExpandNextPath() {
367: 	return ExpandPathInternal(current_path, expanded_files);
368: }
369: 
370: bool GlobMultiFileList::IsFullyExpanded() const {
371: 	return current_path == paths.size();
372: }
373: 
374: } // namespace duckdb
[end of src/common/multi_file_list.cpp]
[start of src/common/multi_file_reader.cpp]
1: #include "duckdb/common/multi_file_reader.hpp"
2: 
3: #include "duckdb/common/exception.hpp"
4: #include "duckdb/common/hive_partitioning.hpp"
5: #include "duckdb/common/types.hpp"
6: #include "duckdb/common/types/value.hpp"
7: #include "duckdb/function/function_set.hpp"
8: #include "duckdb/function/table_function.hpp"
9: #include "duckdb/main/config.hpp"
10: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
11: #include "duckdb/common/string_util.hpp"
12: 
13: #include <algorithm>
14: 
15: namespace duckdb {
16: 
17: MultiFileReaderGlobalState::~MultiFileReaderGlobalState() {
18: }
19: 
20: MultiFileReader::~MultiFileReader() {
21: }
22: 
23: unique_ptr<MultiFileReader> MultiFileReader::Create(const TableFunction &table_function) {
24: 	unique_ptr<MultiFileReader> res;
25: 	if (table_function.get_multi_file_reader) {
26: 		res = table_function.get_multi_file_reader();
27: 		res->function_name = table_function.name;
28: 	} else {
29: 		res = make_uniq<MultiFileReader>();
30: 		res->function_name = table_function.name;
31: 	}
32: 	return res;
33: }
34: 
35: unique_ptr<MultiFileReader> MultiFileReader::CreateDefault(const string &function_name) {
36: 	auto res = make_uniq<MultiFileReader>();
37: 	res->function_name = function_name;
38: 	return res;
39: }
40: 
41: void MultiFileReader::AddParameters(TableFunction &table_function) {
42: 	table_function.named_parameters["filename"] = LogicalType::ANY;
43: 	table_function.named_parameters["hive_partitioning"] = LogicalType::BOOLEAN;
44: 	table_function.named_parameters["union_by_name"] = LogicalType::BOOLEAN;
45: 	table_function.named_parameters["hive_types"] = LogicalType::ANY;
46: 	table_function.named_parameters["hive_types_autocast"] = LogicalType::BOOLEAN;
47: }
48: 
49: vector<string> MultiFileReader::ParsePaths(const Value &input) {
50: 	if (input.IsNull()) {
51: 		throw ParserException("%s cannot take NULL list as parameter", function_name);
52: 	}
53: 
54: 	if (input.type().id() == LogicalTypeId::VARCHAR) {
55: 		return {StringValue::Get(input)};
56: 	} else if (input.type().id() == LogicalTypeId::LIST) {
57: 		vector<string> paths;
58: 		for (auto &val : ListValue::GetChildren(input)) {
59: 			if (val.IsNull()) {
60: 				throw ParserException("%s reader cannot take NULL input as parameter", function_name);
61: 			}
62: 			if (val.type().id() != LogicalTypeId::VARCHAR) {
63: 				throw ParserException("%s reader can only take a list of strings as a parameter", function_name);
64: 			}
65: 			paths.push_back(StringValue::Get(val));
66: 		}
67: 		return paths;
68: 	} else {
69: 		throw InternalException("Unsupported type for MultiFileReader::ParsePaths called with: '%s'");
70: 	}
71: }
72: 
73: unique_ptr<MultiFileList> MultiFileReader::CreateFileList(ClientContext &context, const vector<string> &paths,
74:                                                           FileGlobOptions options) {
75: 	auto &config = DBConfig::GetConfig(context);
76: 	if (!config.options.enable_external_access) {
77: 		throw PermissionException("Scanning %s files is disabled through configuration", function_name);
78: 	}
79: 	vector<string> result_files;
80: 
81: 	auto res = make_uniq<GlobMultiFileList>(context, paths, options);
82: 	if (res->GetExpandResult() == FileExpandResult::NO_FILES && options == FileGlobOptions::DISALLOW_EMPTY) {
83: 		throw IOException("%s needs at least one file to read", function_name);
84: 	}
85: 	return std::move(res);
86: }
87: 
88: unique_ptr<MultiFileList> MultiFileReader::CreateFileList(ClientContext &context, const Value &input,
89:                                                           FileGlobOptions options) {
90: 	auto paths = ParsePaths(input);
91: 	return CreateFileList(context, paths, options);
92: }
93: 
94: bool MultiFileReader::ParseOption(const string &key, const Value &val, MultiFileReaderOptions &options,
95:                                   ClientContext &context) {
96: 	auto loption = StringUtil::Lower(key);
97: 	if (loption == "filename") {
98: 		if (val.type() == LogicalType::VARCHAR) {
99: 			// If not, we interpret it as the name of the column containing the filename
100: 			options.filename = true;
101: 			options.filename_column = StringValue::Get(val);
102: 		} else {
103: 			Value boolean_value;
104: 			string error_message;
105: 			if (val.DefaultTryCastAs(LogicalType::BOOLEAN, boolean_value, &error_message)) {
106: 				// If the argument can be cast to boolean, we just interpret it as a boolean
107: 				options.filename = BooleanValue::Get(boolean_value);
108: 			}
109: 		}
110: 	} else if (loption == "hive_partitioning") {
111: 		options.hive_partitioning = BooleanValue::Get(val);
112: 		options.auto_detect_hive_partitioning = false;
113: 	} else if (loption == "union_by_name") {
114: 		options.union_by_name = BooleanValue::Get(val);
115: 	} else if (loption == "hive_types_autocast" || loption == "hive_type_autocast") {
116: 		options.hive_types_autocast = BooleanValue::Get(val);
117: 	} else if (loption == "hive_types" || loption == "hive_type") {
118: 		if (val.type().id() != LogicalTypeId::STRUCT) {
119: 			throw InvalidInputException(
120: 			    "'hive_types' only accepts a STRUCT('name':VARCHAR, ...), but '%s' was provided",
121: 			    val.type().ToString());
122: 		}
123: 		// verify that that all the children of the struct value are VARCHAR
124: 		auto &children = StructValue::GetChildren(val);
125: 		for (idx_t i = 0; i < children.size(); i++) {
126: 			const Value &child = children[i];
127: 			if (child.type().id() != LogicalType::VARCHAR) {
128: 				throw InvalidInputException("hive_types: '%s' must be a VARCHAR, instead: '%s' was provided",
129: 				                            StructType::GetChildName(val.type(), i), child.type().ToString());
130: 			}
131: 			// for every child of the struct, get the logical type
132: 			LogicalType transformed_type = TransformStringToLogicalType(child.ToString(), context);
133: 			const string &name = StructType::GetChildName(val.type(), i);
134: 			options.hive_types_schema[name] = transformed_type;
135: 		}
136: 		D_ASSERT(!options.hive_types_schema.empty());
137: 	} else {
138: 		return false;
139: 	}
140: 	return true;
141: }
142: 
143: unique_ptr<MultiFileList> MultiFileReader::ComplexFilterPushdown(ClientContext &context, MultiFileList &files,
144:                                                                  const MultiFileReaderOptions &options,
145:                                                                  MultiFilePushdownInfo &info,
146:                                                                  vector<unique_ptr<Expression>> &filters) {
147: 	return files.ComplexFilterPushdown(context, options, info, filters);
148: }
149: 
150: unique_ptr<MultiFileList> MultiFileReader::DynamicFilterPushdown(ClientContext &context, const MultiFileList &files,
151:                                                                  const MultiFileReaderOptions &options,
152:                                                                  const vector<string> &names,
153:                                                                  const vector<LogicalType> &types,
154:                                                                  const vector<column_t> &column_ids,
155:                                                                  TableFilterSet &filters) {
156: 	return files.DynamicFilterPushdown(context, options, names, types, column_ids, filters);
157: }
158: 
159: bool MultiFileReader::Bind(MultiFileReaderOptions &options, MultiFileList &files, vector<LogicalType> &return_types,
160:                            vector<string> &names, MultiFileReaderBindData &bind_data) {
161: 	// The Default MultiFileReader can not perform any binding as it uses MultiFileLists with no schema information.
162: 	return false;
163: }
164: 
165: void MultiFileReader::BindOptions(MultiFileReaderOptions &options, MultiFileList &files,
166:                                   vector<LogicalType> &return_types, vector<string> &names,
167:                                   MultiFileReaderBindData &bind_data) {
168: 	// Add generated constant column for filename
169: 	if (options.filename) {
170: 		if (std::find(names.begin(), names.end(), options.filename_column) != names.end()) {
171: 			throw BinderException("Option filename adds column \"%s\", but a column with this name is also in the "
172: 			                      "file. Try setting a different name: filename='<filename column name>'",
173: 			                      options.filename_column);
174: 		}
175: 		bind_data.filename_idx = names.size();
176: 		return_types.emplace_back(LogicalType::VARCHAR);
177: 		names.emplace_back(options.filename_column);
178: 	}
179: 
180: 	// Add generated constant columns from hive partitioning scheme
181: 	if (options.hive_partitioning) {
182: 		D_ASSERT(files.GetExpandResult() != FileExpandResult::NO_FILES);
183: 		auto partitions = HivePartitioning::Parse(files.GetFirstFile());
184: 		// verify that all files have the same hive partitioning scheme
185: 		for (const auto &file : files.Files()) {
186: 			auto file_partitions = HivePartitioning::Parse(file);
187: 			for (auto &part_info : partitions) {
188: 				if (file_partitions.find(part_info.first) == file_partitions.end()) {
189: 					string error = "Hive partition mismatch between file \"%s\" and \"%s\": key \"%s\" not found";
190: 					if (options.auto_detect_hive_partitioning == true) {
191: 						throw InternalException(error + "(hive partitioning was autodetected)", files.GetFirstFile(),
192: 						                        file, part_info.first);
193: 					}
194: 					throw BinderException(error.c_str(), files.GetFirstFile(), file, part_info.first);
195: 				}
196: 			}
197: 			if (partitions.size() != file_partitions.size()) {
198: 				string error_msg = "Hive partition mismatch between file \"%s\" and \"%s\"";
199: 				if (options.auto_detect_hive_partitioning == true) {
200: 					throw InternalException(error_msg + "(hive partitioning was autodetected)", files.GetFirstFile(),
201: 					                        file);
202: 				}
203: 				throw BinderException(error_msg.c_str(), files.GetFirstFile(), file);
204: 			}
205: 		}
206: 
207: 		if (!options.hive_types_schema.empty()) {
208: 			// verify that all hive_types are existing partitions
209: 			options.VerifyHiveTypesArePartitions(partitions);
210: 		}
211: 
212: 		for (auto &part : partitions) {
213: 			idx_t hive_partitioning_index;
214: 			auto lookup = std::find(names.begin(), names.end(), part.first);
215: 			if (lookup != names.end()) {
216: 				// hive partitioning column also exists in file - override
217: 				auto idx = NumericCast<idx_t>(lookup - names.begin());
218: 				hive_partitioning_index = idx;
219: 				return_types[idx] = options.GetHiveLogicalType(part.first);
220: 			} else {
221: 				// hive partitioning column does not exist in file - add a new column containing the key
222: 				hive_partitioning_index = names.size();
223: 				return_types.emplace_back(options.GetHiveLogicalType(part.first));
224: 				names.emplace_back(part.first);
225: 			}
226: 			bind_data.hive_partitioning_indexes.emplace_back(part.first, hive_partitioning_index);
227: 		}
228: 	}
229: }
230: 
231: void MultiFileReader::FinalizeBind(const MultiFileReaderOptions &file_options, const MultiFileReaderBindData &options,
232:                                    const string &filename, const vector<string> &local_names,
233:                                    const vector<LogicalType> &global_types, const vector<string> &global_names,
234:                                    const vector<column_t> &global_column_ids, MultiFileReaderData &reader_data,
235:                                    ClientContext &context, optional_ptr<MultiFileReaderGlobalState> global_state) {
236: 
237: 	// create a map of name -> column index
238: 	case_insensitive_map_t<idx_t> name_map;
239: 	if (file_options.union_by_name) {
240: 		for (idx_t col_idx = 0; col_idx < local_names.size(); col_idx++) {
241: 			name_map[local_names[col_idx]] = col_idx;
242: 		}
243: 	}
244: 	for (idx_t i = 0; i < global_column_ids.size(); i++) {
245: 		auto column_id = global_column_ids[i];
246: 		if (IsRowIdColumnId(column_id)) {
247: 			// row-id
248: 			reader_data.constant_map.emplace_back(i, Value::BIGINT(42));
249: 			continue;
250: 		}
251: 		if (column_id == options.filename_idx) {
252: 			// filename
253: 			reader_data.constant_map.emplace_back(i, Value(filename));
254: 			continue;
255: 		}
256: 		if (!options.hive_partitioning_indexes.empty()) {
257: 			// hive partition constants
258: 			auto partitions = HivePartitioning::Parse(filename);
259: 			D_ASSERT(partitions.size() == options.hive_partitioning_indexes.size());
260: 			bool found_partition = false;
261: 			for (auto &entry : options.hive_partitioning_indexes) {
262: 				if (column_id == entry.index) {
263: 					Value value = file_options.GetHivePartitionValue(partitions[entry.value], entry.value, context);
264: 					reader_data.constant_map.emplace_back(i, value);
265: 					found_partition = true;
266: 					break;
267: 				}
268: 			}
269: 			if (found_partition) {
270: 				continue;
271: 			}
272: 		}
273: 		if (file_options.union_by_name) {
274: 			auto &global_name = global_names[column_id];
275: 			auto entry = name_map.find(global_name);
276: 			bool not_present_in_file = entry == name_map.end();
277: 			if (not_present_in_file) {
278: 				// we need to project a column with name \"global_name\" - but it does not exist in the current file
279: 				// push a NULL value of the specified type
280: 				reader_data.constant_map.emplace_back(i, Value(global_types[column_id]));
281: 				continue;
282: 			}
283: 		}
284: 	}
285: }
286: 
287: unique_ptr<MultiFileReaderGlobalState>
288: MultiFileReader::InitializeGlobalState(ClientContext &context, const MultiFileReaderOptions &file_options,
289:                                        const MultiFileReaderBindData &bind_data, const MultiFileList &file_list,
290:                                        const vector<LogicalType> &global_types, const vector<string> &global_names,
291:                                        const vector<column_t> &global_column_ids) {
292: 	// By default, the multifilereader does not require any global state
293: 	return nullptr;
294: }
295: 
296: void MultiFileReader::CreateNameMapping(const string &file_name, const vector<LogicalType> &local_types,
297:                                         const vector<string> &local_names, const vector<LogicalType> &global_types,
298:                                         const vector<string> &global_names, const vector<column_t> &global_column_ids,
299:                                         MultiFileReaderData &reader_data, const string &initial_file,
300:                                         optional_ptr<MultiFileReaderGlobalState> global_state) {
301: 	D_ASSERT(global_types.size() == global_names.size());
302: 	D_ASSERT(local_types.size() == local_names.size());
303: 	// we have expected types: create a map of name -> column index
304: 	case_insensitive_map_t<idx_t> name_map;
305: 	for (idx_t col_idx = 0; col_idx < local_names.size(); col_idx++) {
306: 		name_map[local_names[col_idx]] = col_idx;
307: 	}
308: 	for (idx_t i = 0; i < global_column_ids.size(); i++) {
309: 		// check if this is a constant column
310: 		bool constant = false;
311: 		for (auto &entry : reader_data.constant_map) {
312: 			if (entry.column_id == i) {
313: 				constant = true;
314: 				break;
315: 			}
316: 		}
317: 		if (constant) {
318: 			// this column is constant for this file
319: 			continue;
320: 		}
321: 		// not constant - look up the column in the name map
322: 		auto global_id = global_column_ids[i];
323: 		if (global_id >= global_types.size()) {
324: 			throw InternalException(
325: 			    "MultiFileReader::CreatePositionalMapping - global_id is out of range in global_types for this file");
326: 		}
327: 		auto &global_name = global_names[global_id];
328: 		auto entry = name_map.find(global_name);
329: 		if (entry == name_map.end()) {
330: 			string candidate_names;
331: 			for (auto &local_name : local_names) {
332: 				if (!candidate_names.empty()) {
333: 					candidate_names += ", ";
334: 				}
335: 				candidate_names += local_name;
336: 			}
337: 			throw IOException(
338: 			    StringUtil::Format("Failed to read file \"%s\": schema mismatch in glob: column \"%s\" was read from "
339: 			                       "the original file \"%s\", but could not be found in file \"%s\".\nCandidate names: "
340: 			                       "%s\nIf you are trying to "
341: 			                       "read files with different schemas, try setting union_by_name=True",
342: 			                       file_name, global_name, initial_file, file_name, candidate_names));
343: 		}
344: 		// we found the column in the local file - check if the types are the same
345: 		auto local_id = entry->second;
346: 		D_ASSERT(global_id < global_types.size());
347: 		D_ASSERT(local_id < local_types.size());
348: 		auto &global_type = global_types[global_id];
349: 		auto &local_type = local_types[local_id];
350: 		if (global_type != local_type) {
351: 			reader_data.cast_map[local_id] = global_type;
352: 		}
353: 		// the types are the same - create the mapping
354: 		reader_data.column_mapping.push_back(i);
355: 		reader_data.column_ids.push_back(local_id);
356: 	}
357: 
358: 	reader_data.empty_columns = reader_data.column_ids.empty();
359: }
360: 
361: void MultiFileReader::CreateMapping(const string &file_name, const vector<LogicalType> &local_types,
362:                                     const vector<string> &local_names, const vector<LogicalType> &global_types,
363:                                     const vector<string> &global_names, const vector<column_t> &global_column_ids,
364:                                     optional_ptr<TableFilterSet> filters, MultiFileReaderData &reader_data,
365:                                     const string &initial_file, const MultiFileReaderBindData &options,
366:                                     optional_ptr<MultiFileReaderGlobalState> global_state) {
367: 	CreateNameMapping(file_name, local_types, local_names, global_types, global_names, global_column_ids, reader_data,
368: 	                  initial_file, global_state);
369: 	CreateFilterMap(global_types, filters, reader_data, global_state);
370: }
371: 
372: void MultiFileReader::CreateFilterMap(const vector<LogicalType> &global_types, optional_ptr<TableFilterSet> filters,
373:                                       MultiFileReaderData &reader_data,
374:                                       optional_ptr<MultiFileReaderGlobalState> global_state) {
375: 	if (filters) {
376: 		auto filter_map_size = global_types.size();
377: 		if (global_state) {
378: 			filter_map_size += global_state->extra_columns.size();
379: 		}
380: 		reader_data.filter_map.resize(filter_map_size);
381: 
382: 		for (idx_t c = 0; c < reader_data.column_mapping.size(); c++) {
383: 			auto map_index = reader_data.column_mapping[c];
384: 			reader_data.filter_map[map_index].index = c;
385: 			reader_data.filter_map[map_index].is_constant = false;
386: 		}
387: 		for (idx_t c = 0; c < reader_data.constant_map.size(); c++) {
388: 			auto constant_index = reader_data.constant_map[c].column_id;
389: 			reader_data.filter_map[constant_index].index = c;
390: 			reader_data.filter_map[constant_index].is_constant = true;
391: 		}
392: 	}
393: }
394: 
395: void MultiFileReader::FinalizeChunk(ClientContext &context, const MultiFileReaderBindData &bind_data,
396:                                     const MultiFileReaderData &reader_data, DataChunk &chunk,
397:                                     optional_ptr<MultiFileReaderGlobalState> global_state) {
398: 	// reference all the constants set up in MultiFileReader::FinalizeBind
399: 	for (auto &entry : reader_data.constant_map) {
400: 		chunk.data[entry.column_id].Reference(entry.value);
401: 	}
402: 	chunk.Verify();
403: }
404: 
405: TableFunctionSet MultiFileReader::CreateFunctionSet(TableFunction table_function) {
406: 	TableFunctionSet function_set(table_function.name);
407: 	function_set.AddFunction(table_function);
408: 	D_ASSERT(table_function.arguments.size() == 1 && table_function.arguments[0] == LogicalType::VARCHAR);
409: 	table_function.arguments[0] = LogicalType::LIST(LogicalType::VARCHAR);
410: 	function_set.AddFunction(std::move(table_function));
411: 	return function_set;
412: }
413: 
414: HivePartitioningIndex::HivePartitioningIndex(string value_p, idx_t index) : value(std::move(value_p)), index(index) {
415: }
416: 
417: void MultiFileReaderOptions::AddBatchInfo(BindInfo &bind_info) const {
418: 	bind_info.InsertOption("filename", Value(filename_column));
419: 	bind_info.InsertOption("hive_partitioning", Value::BOOLEAN(hive_partitioning));
420: 	bind_info.InsertOption("auto_detect_hive_partitioning", Value::BOOLEAN(auto_detect_hive_partitioning));
421: 	bind_info.InsertOption("union_by_name", Value::BOOLEAN(union_by_name));
422: 	bind_info.InsertOption("hive_types_autocast", Value::BOOLEAN(hive_types_autocast));
423: }
424: 
425: void UnionByName::CombineUnionTypes(const vector<string> &col_names, const vector<LogicalType> &sql_types,
426:                                     vector<LogicalType> &union_col_types, vector<string> &union_col_names,
427:                                     case_insensitive_map_t<idx_t> &union_names_map) {
428: 	D_ASSERT(col_names.size() == sql_types.size());
429: 
430: 	for (idx_t col = 0; col < col_names.size(); ++col) {
431: 		auto union_find = union_names_map.find(col_names[col]);
432: 
433: 		if (union_find != union_names_map.end()) {
434: 			// given same name , union_col's type must compatible with col's type
435: 			auto &current_type = union_col_types[union_find->second];
436: 			auto compatible_type = LogicalType::ForceMaxLogicalType(current_type, sql_types[col]);
437: 			union_col_types[union_find->second] = compatible_type;
438: 		} else {
439: 			union_names_map[col_names[col]] = union_col_names.size();
440: 			union_col_names.emplace_back(col_names[col]);
441: 			union_col_types.emplace_back(sql_types[col]);
442: 		}
443: 	}
444: }
445: 
446: bool MultiFileReaderOptions::AutoDetectHivePartitioningInternal(MultiFileList &files, ClientContext &context) {
447: 	std::unordered_set<string> partitions;
448: 	auto &fs = FileSystem::GetFileSystem(context);
449: 
450: 	auto first_file = files.GetFirstFile();
451: 	auto splits_first_file = StringUtil::Split(first_file, fs.PathSeparator(first_file));
452: 	if (splits_first_file.size() < 2) {
453: 		return false;
454: 	}
455: 	for (auto &split : splits_first_file) {
456: 		auto partition = StringUtil::Split(split, "=");
457: 		if (partition.size() == 2) {
458: 			partitions.insert(partition.front());
459: 		}
460: 	}
461: 	if (partitions.empty()) {
462: 		return false;
463: 	}
464: 
465: 	for (const auto &file : files.Files()) {
466: 		auto splits = StringUtil::Split(file, fs.PathSeparator(file));
467: 		if (splits.size() != splits_first_file.size()) {
468: 			return false;
469: 		}
470: 		for (auto it = splits.begin(); it != std::prev(splits.end()); it++) {
471: 			auto part = StringUtil::Split(*it, "=");
472: 			if (part.size() != 2) {
473: 				continue;
474: 			}
475: 			if (partitions.find(part.front()) == partitions.end()) {
476: 				return false;
477: 			}
478: 		}
479: 	}
480: 	return true;
481: }
482: void MultiFileReaderOptions::AutoDetectHiveTypesInternal(MultiFileList &files, ClientContext &context) {
483: 	const LogicalType candidates[] = {LogicalType::DATE, LogicalType::TIMESTAMP, LogicalType::BIGINT};
484: 
485: 	auto &fs = FileSystem::GetFileSystem(context);
486: 
487: 	unordered_map<string, LogicalType> detected_types;
488: 	for (const auto &file : files.Files()) {
489: 		unordered_map<string, string> partitions;
490: 		auto splits = StringUtil::Split(file, fs.PathSeparator(file));
491: 		if (splits.size() < 2) {
492: 			return;
493: 		}
494: 		for (auto it = splits.begin(); it != std::prev(splits.end()); it++) {
495: 			auto part = StringUtil::Split(*it, "=");
496: 			if (part.size() == 2) {
497: 				partitions[part.front()] = part.back();
498: 			}
499: 		}
500: 		if (partitions.empty()) {
501: 			return;
502: 		}
503: 
504: 		for (auto &part : partitions) {
505: 			const string &name = part.first;
506: 			if (hive_types_schema.find(name) != hive_types_schema.end()) {
507: 				// type was explicitly provided by the user
508: 				continue;
509: 			}
510: 			LogicalType detected_type = LogicalType::VARCHAR;
511: 			Value value(part.second);
512: 			for (auto &candidate : candidates) {
513: 				const bool success = value.TryCastAs(context, candidate, true);
514: 				if (success) {
515: 					detected_type = candidate;
516: 					break;
517: 				}
518: 			}
519: 			auto entry = detected_types.find(name);
520: 			if (entry == detected_types.end()) {
521: 				// type was not yet detected - insert it
522: 				detected_types.insert(make_pair(name, std::move(detected_type)));
523: 			} else {
524: 				// type was already detected - check if the type matches
525: 				// if not promote to VARCHAR
526: 				if (entry->second != detected_type) {
527: 					entry->second = LogicalType::VARCHAR;
528: 				}
529: 			}
530: 		}
531: 	}
532: 	for (auto &entry : detected_types) {
533: 		hive_types_schema.insert(make_pair(entry.first, std::move(entry.second)));
534: 	}
535: }
536: void MultiFileReaderOptions::AutoDetectHivePartitioning(MultiFileList &files, ClientContext &context) {
537: 	D_ASSERT(files.GetExpandResult() != FileExpandResult::NO_FILES);
538: 	const bool hp_explicitly_disabled = !auto_detect_hive_partitioning && !hive_partitioning;
539: 	const bool ht_enabled = !hive_types_schema.empty();
540: 	if (hp_explicitly_disabled && ht_enabled) {
541: 		throw InvalidInputException("cannot disable hive_partitioning when hive_types is enabled");
542: 	}
543: 	if (ht_enabled && auto_detect_hive_partitioning && !hive_partitioning) {
544: 		// hive_types flag implies hive_partitioning
545: 		hive_partitioning = true;
546: 		auto_detect_hive_partitioning = false;
547: 	}
548: 	if (auto_detect_hive_partitioning) {
549: 		hive_partitioning = AutoDetectHivePartitioningInternal(files, context);
550: 	}
551: 	if (hive_partitioning && hive_types_autocast) {
552: 		AutoDetectHiveTypesInternal(files, context);
553: 	}
554: }
555: void MultiFileReaderOptions::VerifyHiveTypesArePartitions(const std::map<string, string> &partitions) const {
556: 	for (auto &hive_type : hive_types_schema) {
557: 		if (partitions.find(hive_type.first) == partitions.end()) {
558: 			throw InvalidInputException("Unknown hive_type: \"%s\" does not appear to be a partition", hive_type.first);
559: 		}
560: 	}
561: }
562: LogicalType MultiFileReaderOptions::GetHiveLogicalType(const string &hive_partition_column) const {
563: 	if (!hive_types_schema.empty()) {
564: 		auto it = hive_types_schema.find(hive_partition_column);
565: 		if (it != hive_types_schema.end()) {
566: 			return it->second;
567: 		}
568: 	}
569: 	return LogicalType::VARCHAR;
570: }
571: 
572: bool MultiFileReaderOptions::AnySet() {
573: 	return filename || hive_partitioning || union_by_name;
574: }
575: 
576: Value MultiFileReaderOptions::GetHivePartitionValue(const string &base, const string &entry,
577:                                                     ClientContext &context) const {
578: 	Value value(base);
579: 	auto it = hive_types_schema.find(entry);
580: 	if (it == hive_types_schema.end()) {
581: 		return value;
582: 	}
583: 
584: 	// Handle nulls
585: 	if (base.empty() || StringUtil::CIEquals(base, "NULL")) {
586: 		return Value(it->second);
587: 	}
588: 
589: 	if (!value.TryCastAs(context, it->second)) {
590: 		throw InvalidInputException("Unable to cast '%s' (from hive partition column '%s') to: '%s'", value.ToString(),
591: 		                            StringUtil::Upper(it->first), it->second.ToString());
592: 	}
593: 	return value;
594: }
595: 
596: } // namespace duckdb
[end of src/common/multi_file_reader.cpp]
[start of src/include/duckdb/common/hive_partitioning.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/hive_partitioning.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/column/partitioned_column_data.hpp"
12: #include "duckdb/execution/expression_executor.hpp"
13: #include "duckdb/optimizer/filter_combiner.hpp"
14: #include "duckdb/optimizer/statistics_propagator.hpp"
15: #include "duckdb/planner/expression_iterator.hpp"
16: #include "duckdb/planner/table_filter.hpp"
17: 
18: #include <iostream>
19: #include <sstream>
20: 
21: namespace duckdb_re2 {
22: class RE2;
23: } // namespace duckdb_re2
24: 
25: namespace duckdb {
26: struct MultiFilePushdownInfo;
27: 
28: class HivePartitioning {
29: public:
30: 	//! Parse a filename that follows the hive partitioning scheme
31: 	DUCKDB_API static std::map<string, string> Parse(const string &filename);
32: 	DUCKDB_API static std::map<string, string> Parse(const string &filename, duckdb_re2::RE2 &regex);
33: 	//! Prunes a list of filenames based on a set of filters, can be used by TableFunctions in the
34: 	//! pushdown_complex_filter function to skip files with filename-based filters. Also removes the filters that always
35: 	//! evaluate to true.
36: 	DUCKDB_API static void ApplyFiltersToFileList(ClientContext &context, vector<string> &files,
37: 	                                              vector<unique_ptr<Expression>> &filters,
38: 	                                              unordered_map<string, column_t> &column_map,
39: 	                                              MultiFilePushdownInfo &info, bool hive_enabled,
40: 	                                              bool filename_enabled);
41: 
42: 	//! Returns the compiled regex pattern to match hive partitions
43: 	DUCKDB_API static const string &RegexString();
44: };
45: 
46: struct HivePartitionKey {
47: 	//! Columns by which we want to partition
48: 	vector<Value> values;
49: 	//! Precomputed hash of values
50: 	hash_t hash;
51: 
52: 	struct Hash {
53: 		std::size_t operator()(const HivePartitionKey &k) const {
54: 			return k.hash;
55: 		}
56: 	};
57: 
58: 	struct Equality {
59: 		bool operator()(const HivePartitionKey &a, const HivePartitionKey &b) const {
60: 			if (a.values.size() != b.values.size()) {
61: 				return false;
62: 			}
63: 			for (idx_t i = 0; i < a.values.size(); i++) {
64: 				if (!Value::NotDistinctFrom(a.values[i], b.values[i])) {
65: 					return false;
66: 				}
67: 			}
68: 			return true;
69: 		}
70: 	};
71: };
72: 
73: //! Maps hive partitions to partition_ids
74: typedef unordered_map<HivePartitionKey, idx_t, HivePartitionKey::Hash, HivePartitionKey::Equality> hive_partition_map_t;
75: 
76: //! class shared between HivePartitionColumnData classes that synchronizes partition discovery between threads.
77: //! each HivePartitionedColumnData will hold a local copy of the key->partition map
78: class GlobalHivePartitionState {
79: public:
80: 	mutex lock;
81: 	hive_partition_map_t partition_map;
82: };
83: 
84: class HivePartitionedColumnData : public PartitionedColumnData {
85: public:
86: 	HivePartitionedColumnData(ClientContext &context, vector<LogicalType> types, vector<idx_t> partition_by_cols,
87: 	                          shared_ptr<GlobalHivePartitionState> global_state = nullptr);
88: 	void ComputePartitionIndices(PartitionedColumnDataAppendState &state, DataChunk &input) override;
89: 
90: 	//! Reverse lookup map to reconstruct keys from a partition id
91: 	std::map<idx_t, const HivePartitionKey *> GetReverseMap();
92: 
93: protected:
94: 	//! Register a newly discovered partition
95: 	idx_t RegisterNewPartition(HivePartitionKey key, PartitionedColumnDataAppendState &state);
96: 	//! Add a new partition with the given partition id
97: 	void AddNewPartition(HivePartitionKey key, idx_t partition_id, PartitionedColumnDataAppendState &state);
98: 
99: private:
100: 	void InitializeKeys();
101: 
102: protected:
103: 	//! Shared HivePartitionedColumnData should always have a global state to allow parallel key discovery
104: 	shared_ptr<GlobalHivePartitionState> global_state;
105: 	//! Thread-local copy of the partition map
106: 	hive_partition_map_t local_partition_map;
107: 	//! The columns that make up the key
108: 	vector<idx_t> group_by_columns;
109: 	//! Thread-local pre-allocated vector for hashes
110: 	Vector hashes_v;
111: 	//! Thread-local pre-allocated HivePartitionKeys
112: 	vector<HivePartitionKey> keys;
113: };
114: 
115: } // namespace duckdb
[end of src/include/duckdb/common/hive_partitioning.hpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: