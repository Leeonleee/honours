diff --git a/src/common/hive_partitioning.cpp b/src/common/hive_partitioning.cpp
index 743a3d46b47f..672e0ef93f90 100644
--- a/src/common/hive_partitioning.cpp
+++ b/src/common/hive_partitioning.cpp
@@ -2,36 +2,44 @@
 
 #include "duckdb/common/uhugeint.hpp"
 #include "duckdb/execution/expression_executor.hpp"
-#include "duckdb/optimizer/filter_combiner.hpp"
 #include "duckdb/planner/expression/bound_columnref_expression.hpp"
 #include "duckdb/planner/expression/bound_constant_expression.hpp"
 #include "duckdb/planner/expression/bound_reference_expression.hpp"
 #include "duckdb/planner/expression_iterator.hpp"
 #include "duckdb/planner/table_filter.hpp"
 #include "duckdb/common/multi_file_list.hpp"
-#include "re2/re2.h"
 
 namespace duckdb {
 
-static unordered_map<column_t, string> GetKnownColumnValues(string &filename,
-                                                            unordered_map<string, column_t> &column_map,
-                                                            duckdb_re2::RE2 &compiled_regex, bool filename_col,
-                                                            bool hive_partition_cols) {
-	unordered_map<column_t, string> result;
+struct PartitioningColumnValue {
+	explicit PartitioningColumnValue(string value_p) : value(std::move(value_p)) {
+	}
+	PartitioningColumnValue(string key_p, string value_p) : key(std::move(key_p)), value(std::move(value_p)) {
+	}
+
+	string key;
+	string value;
+};
+
+static unordered_map<column_t, PartitioningColumnValue>
+GetKnownColumnValues(const string &filename, const HivePartitioningFilterInfo &filter_info) {
+	unordered_map<column_t, PartitioningColumnValue> result;
 
-	if (filename_col) {
+	auto &column_map = filter_info.column_map;
+	if (filter_info.filename_enabled) {
 		auto lookup_column_id = column_map.find("filename");
 		if (lookup_column_id != column_map.end()) {
-			result[lookup_column_id->second] = filename;
+			result.insert(make_pair(lookup_column_id->second, PartitioningColumnValue(filename)));
 		}
 	}
 
-	if (hive_partition_cols) {
-		auto partitions = HivePartitioning::Parse(filename, compiled_regex);
+	if (filter_info.hive_enabled) {
+		auto partitions = HivePartitioning::Parse(filename);
 		for (auto &partition : partitions) {
 			auto lookup_column_id = column_map.find(partition.first);
 			if (lookup_column_id != column_map.end()) {
-				result[lookup_column_id->second] = partition.second;
+				result.insert(
+				    make_pair(lookup_column_id->second, PartitioningColumnValue(partition.first, partition.second)));
 			}
 		}
 	}
@@ -40,8 +48,9 @@ static unordered_map<column_t, string> GetKnownColumnValues(string &filename,
 }
 
 // Takes an expression and converts a list of known column_refs to constants
-static void ConvertKnownColRefToConstants(unique_ptr<Expression> &expr,
-                                          unordered_map<column_t, string> &known_column_values, idx_t table_index) {
+static void ConvertKnownColRefToConstants(ClientContext &context, unique_ptr<Expression> &expr,
+                                          const unordered_map<column_t, PartitioningColumnValue> &known_column_values,
+                                          idx_t table_index) {
 	if (expr->type == ExpressionType::BOUND_COLUMN_REF) {
 		auto &bound_colref = expr->Cast<BoundColumnRefExpression>();
 
@@ -52,11 +61,21 @@ static void ConvertKnownColRefToConstants(unique_ptr<Expression> &expr,
 
 		auto lookup = known_column_values.find(bound_colref.binding.column_index);
 		if (lookup != known_column_values.end()) {
-			expr = make_uniq<BoundConstantExpression>(Value(lookup->second).DefaultCastAs(bound_colref.return_type));
+			auto &partition_val = lookup->second;
+			Value result_val;
+			if (partition_val.key.empty()) {
+				// filename column - use directly
+				result_val = Value(partition_val.value);
+			} else {
+				// hive partitioning column - cast the value to the target type
+				result_val = HivePartitioning::GetValue(context, partition_val.key, partition_val.value,
+				                                        bound_colref.return_type);
+			}
+			expr = make_uniq<BoundConstantExpression>(std::move(result_val));
 		}
 	} else {
 		ExpressionIterator::EnumerateChildren(*expr, [&](unique_ptr<Expression> &child) {
-			ConvertKnownColRefToConstants(child, known_column_values, table_index);
+			ConvertKnownColRefToConstants(context, child, known_column_values, table_index);
 		});
 	}
 }
@@ -65,57 +84,87 @@ static void ConvertKnownColRefToConstants(unique_ptr<Expression> &expr,
 // 	- s3://bucket/var1=value1/bla/bla/var2=value2
 //  - http(s)://domain(:port)/lala/kasdl/var1=value1/?not-a-var=not-a-value
 //  - folder/folder/folder/../var1=value1/etc/.//var2=value2
-const string &HivePartitioning::RegexString() {
-	static string REGEX = "[\\/\\\\]([^\\/\\?\\\\]+)=([^\\/\
\\?\\\\]*)";
-	return REGEX;
-}
-
-std::map<string, string> HivePartitioning::Parse(const string &filename, duckdb_re2::RE2 &regex) {
+std::map<string, string> HivePartitioning::Parse(const string &filename) {
+	idx_t partition_start = 0;
+	idx_t equality_sign = 0;
+	bool candidate_partition = true;
 	std::map<string, string> result;
-	duckdb_re2::StringPiece input(filename); // Wrap a StringPiece around it
-
-	string var;
-	string value;
-	while (RE2::FindAndConsume(&input, regex, &var, &value)) {
-		result.insert(std::pair<string, string>(var, value));
+	for (idx_t c = 0; c < filename.size(); c++) {
+		if (filename[c] == '?' || filename[c] == '
') {
+			// get parameter or newline - not a partition
+			candidate_partition = false;
+		}
+		if (filename[c] == '\\' || filename[c] == '/') {
+			// separator
+			if (candidate_partition && equality_sign > partition_start) {
+				// we found a partition with an equality sign
+				string key = filename.substr(partition_start, equality_sign - partition_start);
+				string value = filename.substr(equality_sign + 1, c - equality_sign - 1);
+				result.insert(make_pair(std::move(key), std::move(value)));
+			}
+			partition_start = c + 1;
+			candidate_partition = true;
+		} else if (filename[c] == '=') {
+			if (equality_sign > partition_start) {
+				// multiple equality signs - not a partition
+				candidate_partition = false;
+			}
+			equality_sign = c;
+		}
 	}
 	return result;
 }
 
-std::map<string, string> HivePartitioning::Parse(const string &filename) {
-	duckdb_re2::RE2 regex(RegexString());
-	return Parse(filename, regex);
+Value HivePartitioning::GetValue(ClientContext &context, const string &key, const string &str_val,
+                                 const LogicalType &type) {
+	// Handle nulls
+	if (StringUtil::CIEquals(str_val, "NULL")) {
+		return Value(type);
+	}
+	if (type.id() == LogicalTypeId::VARCHAR) {
+		// for string values we can directly return the type
+		return Value(str_val);
+	}
+	if (str_val.empty()) {
+		// empty strings are NULL for non-string types
+		return Value(type);
+	}
+
+	// cast to the target type
+	Value value(str_val);
+	if (!value.TryCastAs(context, type)) {
+		throw InvalidInputException("Unable to cast '%s' (from hive partition column '%s') to: '%s'", value.ToString(),
+		                            StringUtil::Upper(key), type.ToString());
+	}
+	return value;
 }
 
 // TODO: this can still be improved by removing the parts of filter expressions that are true for all remaining files.
 //		 currently, only expressions that cannot be evaluated during pushdown are removed.
 void HivePartitioning::ApplyFiltersToFileList(ClientContext &context, vector<string> &files,
                                               vector<unique_ptr<Expression>> &filters,
-                                              unordered_map<string, column_t> &column_map, MultiFilePushdownInfo &info,
-                                              bool hive_enabled, bool filename_enabled) {
+                                              const HivePartitioningFilterInfo &filter_info,
+                                              MultiFilePushdownInfo &info) {
 
 	vector<string> pruned_files;
 	vector<bool> have_preserved_filter(filters.size(), false);
 	vector<unique_ptr<Expression>> pruned_filters;
 	unordered_set<idx_t> filters_applied_to_files;
-	duckdb_re2::RE2 regex(RegexString());
 	auto table_index = info.table_index;
 
-	if ((!filename_enabled && !hive_enabled) || filters.empty()) {
+	if ((!filter_info.filename_enabled && !filter_info.hive_enabled) || filters.empty()) {
 		return;
 	}
 
 	for (idx_t i = 0; i < files.size(); i++) {
 		auto &file = files[i];
 		bool should_prune_file = false;
-		auto known_values = GetKnownColumnValues(file, column_map, regex, filename_enabled, hive_enabled);
-
-		FilterCombiner combiner(context);
+		auto known_values = GetKnownColumnValues(file, filter_info);
 
 		for (idx_t j = 0; j < filters.size(); j++) {
 			auto &filter = filters[j];
 			unique_ptr<Expression> filter_copy = filter->Copy();
-			ConvertKnownColRefToConstants(filter_copy, known_values, table_index);
+			ConvertKnownColRefToConstants(context, filter_copy, known_values, table_index);
 			// Evaluate the filter, if it can be evaluated here, we can not prune this filter
 			Value result_value;
 
@@ -126,7 +175,7 @@ void HivePartitioning::ApplyFiltersToFileList(ClientContext &context, vector<str
 					pruned_filters.emplace_back(filter->Copy());
 					have_preserved_filter[j] = true;
 				}
-			} else if (!result_value.GetValue<bool>()) {
+			} else if (result_value.IsNull() || !result_value.GetValue<bool>()) {
 				// filter evaluates to false
 				should_prune_file = true;
 				// convert the filter to a table filter.
diff --git a/src/common/multi_file_list.cpp b/src/common/multi_file_list.cpp
index afd5af6f1e3e..e6483bb6c878 100644
--- a/src/common/multi_file_list.cpp
+++ b/src/common/multi_file_list.cpp
@@ -25,16 +25,17 @@ MultiFilePushdownInfo::MultiFilePushdownInfo(idx_t table_index, const vector<str
 // Helper method to do Filter Pushdown into a MultiFileList
 bool PushdownInternal(ClientContext &context, const MultiFileReaderOptions &options, MultiFilePushdownInfo &info,
                       vector<unique_ptr<Expression>> &filters, vector<string> &expanded_files) {
-	unordered_map<string, column_t> column_map;
+	HivePartitioningFilterInfo filter_info;
 	for (idx_t i = 0; i < info.column_ids.size(); i++) {
 		if (!IsRowIdColumnId(info.column_ids[i])) {
-			column_map.insert({info.column_names[info.column_ids[i]], i});
+			filter_info.column_map.insert({info.column_names[info.column_ids[i]], i});
 		}
 	}
+	filter_info.hive_enabled = options.hive_partitioning;
+	filter_info.filename_enabled = options.filename;
 
 	auto start_files = expanded_files.size();
-	HivePartitioning::ApplyFiltersToFileList(context, expanded_files, filters, column_map, info,
-	                                         options.hive_partitioning, options.filename);
+	HivePartitioning::ApplyFiltersToFileList(context, expanded_files, filters, filter_info, info);
 
 	if (expanded_files.size() != start_files) {
 		return true;
diff --git a/src/common/multi_file_reader.cpp b/src/common/multi_file_reader.cpp
index a8882b816644..6f4d2faa4485 100644
--- a/src/common/multi_file_reader.cpp
+++ b/src/common/multi_file_reader.cpp
@@ -444,35 +444,23 @@ void UnionByName::CombineUnionTypes(const vector<string> &col_names, const vecto
 }
 
 bool MultiFileReaderOptions::AutoDetectHivePartitioningInternal(MultiFileList &files, ClientContext &context) {
-	std::unordered_set<string> partitions;
-	auto &fs = FileSystem::GetFileSystem(context);
-
 	auto first_file = files.GetFirstFile();
-	auto splits_first_file = StringUtil::Split(first_file, fs.PathSeparator(first_file));
-	if (splits_first_file.size() < 2) {
-		return false;
-	}
-	for (auto &split : splits_first_file) {
-		auto partition = StringUtil::Split(split, "=");
-		if (partition.size() == 2) {
-			partitions.insert(partition.front());
-		}
-	}
+	auto partitions = HivePartitioning::Parse(first_file);
 	if (partitions.empty()) {
+		// no partitions found in first file
 		return false;
 	}
 
 	for (const auto &file : files.Files()) {
-		auto splits = StringUtil::Split(file, fs.PathSeparator(file));
-		if (splits.size() != splits_first_file.size()) {
+		auto new_partitions = HivePartitioning::Parse(file);
+		if (new_partitions.size() != partitions.size()) {
+			// partition count mismatch
 			return false;
 		}
-		for (auto it = splits.begin(); it != std::prev(splits.end()); it++) {
-			auto part = StringUtil::Split(*it, "=");
-			if (part.size() != 2) {
-				continue;
-			}
-			if (partitions.find(part.front()) == partitions.end()) {
+		for (auto &part : new_partitions) {
+			auto entry = partitions.find(part.first);
+			if (entry == partitions.end()) {
+				// differing partitions between files
 				return false;
 			}
 		}
@@ -482,21 +470,9 @@ bool MultiFileReaderOptions::AutoDetectHivePartitioningInternal(MultiFileList &f
 void MultiFileReaderOptions::AutoDetectHiveTypesInternal(MultiFileList &files, ClientContext &context) {
 	const LogicalType candidates[] = {LogicalType::DATE, LogicalType::TIMESTAMP, LogicalType::BIGINT};
 
-	auto &fs = FileSystem::GetFileSystem(context);
-
 	unordered_map<string, LogicalType> detected_types;
 	for (const auto &file : files.Files()) {
-		unordered_map<string, string> partitions;
-		auto splits = StringUtil::Split(file, fs.PathSeparator(file));
-		if (splits.size() < 2) {
-			return;
-		}
-		for (auto it = splits.begin(); it != std::prev(splits.end()); it++) {
-			auto part = StringUtil::Split(*it, "=");
-			if (part.size() == 2) {
-				partitions[part.front()] = part.back();
-			}
-		}
+		auto partitions = HivePartitioning::Parse(file);
 		if (partitions.empty()) {
 			return;
 		}
@@ -573,24 +549,13 @@ bool MultiFileReaderOptions::AnySet() {
 	return filename || hive_partitioning || union_by_name;
 }
 
-Value MultiFileReaderOptions::GetHivePartitionValue(const string &base, const string &entry,
+Value MultiFileReaderOptions::GetHivePartitionValue(const string &value, const string &key,
                                                     ClientContext &context) const {
-	Value value(base);
-	auto it = hive_types_schema.find(entry);
+	auto it = hive_types_schema.find(key);
 	if (it == hive_types_schema.end()) {
-		return value;
-	}
-
-	// Handle nulls
-	if (base.empty() || StringUtil::CIEquals(base, "NULL")) {
-		return Value(it->second);
-	}
-
-	if (!value.TryCastAs(context, it->second)) {
-		throw InvalidInputException("Unable to cast '%s' (from hive partition column '%s') to: '%s'", value.ToString(),
-		                            StringUtil::Upper(it->first), it->second.ToString());
+		return HivePartitioning::GetValue(context, key, value, LogicalType::VARCHAR);
 	}
-	return value;
+	return HivePartitioning::GetValue(context, key, value, it->second);
 }
 
 } // namespace duckdb
diff --git a/src/include/duckdb/common/hive_partitioning.hpp b/src/include/duckdb/common/hive_partitioning.hpp
index 867867759ec3..770dc6b50980 100644
--- a/src/include/duckdb/common/hive_partitioning.hpp
+++ b/src/include/duckdb/common/hive_partitioning.hpp
@@ -18,29 +18,29 @@
 #include <iostream>
 #include <sstream>
 
-namespace duckdb_re2 {
-class RE2;
-} // namespace duckdb_re2
-
 namespace duckdb {
 struct MultiFilePushdownInfo;
 
+struct HivePartitioningFilterInfo {
+	unordered_map<string, column_t> column_map;
+	bool hive_enabled;
+	bool filename_enabled;
+};
+
 class HivePartitioning {
 public:
 	//! Parse a filename that follows the hive partitioning scheme
 	DUCKDB_API static std::map<string, string> Parse(const string &filename);
-	DUCKDB_API static std::map<string, string> Parse(const string &filename, duckdb_re2::RE2 &regex);
 	//! Prunes a list of filenames based on a set of filters, can be used by TableFunctions in the
 	//! pushdown_complex_filter function to skip files with filename-based filters. Also removes the filters that always
 	//! evaluate to true.
 	DUCKDB_API static void ApplyFiltersToFileList(ClientContext &context, vector<string> &files,
 	                                              vector<unique_ptr<Expression>> &filters,
-	                                              unordered_map<string, column_t> &column_map,
-	                                              MultiFilePushdownInfo &info, bool hive_enabled,
-	                                              bool filename_enabled);
+	                                              const HivePartitioningFilterInfo &filter_info,
+	                                              MultiFilePushdownInfo &info);
 
-	//! Returns the compiled regex pattern to match hive partitions
-	DUCKDB_API static const string &RegexString();
+	DUCKDB_API static Value GetValue(ClientContext &context, const string &key, const string &value,
+	                                 const LogicalType &type);
 };
 
 struct HivePartitionKey {
