{
  "repo": "duckdb/duckdb",
  "pull_number": 12970,
  "instance_id": "duckdb__duckdb-12970",
  "issue_numbers": [
    "12921"
  ],
  "base_commit": "37fb136c0dbb7ff9832f2eec268c6371af565035",
  "patch": "diff --git a/src/common/hive_partitioning.cpp b/src/common/hive_partitioning.cpp\nindex 743a3d46b47f..672e0ef93f90 100644\n--- a/src/common/hive_partitioning.cpp\n+++ b/src/common/hive_partitioning.cpp\n@@ -2,36 +2,44 @@\n \n #include \"duckdb/common/uhugeint.hpp\"\n #include \"duckdb/execution/expression_executor.hpp\"\n-#include \"duckdb/optimizer/filter_combiner.hpp\"\n #include \"duckdb/planner/expression/bound_columnref_expression.hpp\"\n #include \"duckdb/planner/expression/bound_constant_expression.hpp\"\n #include \"duckdb/planner/expression/bound_reference_expression.hpp\"\n #include \"duckdb/planner/expression_iterator.hpp\"\n #include \"duckdb/planner/table_filter.hpp\"\n #include \"duckdb/common/multi_file_list.hpp\"\n-#include \"re2/re2.h\"\n \n namespace duckdb {\n \n-static unordered_map<column_t, string> GetKnownColumnValues(string &filename,\n-                                                            unordered_map<string, column_t> &column_map,\n-                                                            duckdb_re2::RE2 &compiled_regex, bool filename_col,\n-                                                            bool hive_partition_cols) {\n-\tunordered_map<column_t, string> result;\n+struct PartitioningColumnValue {\n+\texplicit PartitioningColumnValue(string value_p) : value(std::move(value_p)) {\n+\t}\n+\tPartitioningColumnValue(string key_p, string value_p) : key(std::move(key_p)), value(std::move(value_p)) {\n+\t}\n+\n+\tstring key;\n+\tstring value;\n+};\n+\n+static unordered_map<column_t, PartitioningColumnValue>\n+GetKnownColumnValues(const string &filename, const HivePartitioningFilterInfo &filter_info) {\n+\tunordered_map<column_t, PartitioningColumnValue> result;\n \n-\tif (filename_col) {\n+\tauto &column_map = filter_info.column_map;\n+\tif (filter_info.filename_enabled) {\n \t\tauto lookup_column_id = column_map.find(\"filename\");\n \t\tif (lookup_column_id != column_map.end()) {\n-\t\t\tresult[lookup_column_id->second] = filename;\n+\t\t\tresult.insert(make_pair(lookup_column_id->second, PartitioningColumnValue(filename)));\n \t\t}\n \t}\n \n-\tif (hive_partition_cols) {\n-\t\tauto partitions = HivePartitioning::Parse(filename, compiled_regex);\n+\tif (filter_info.hive_enabled) {\n+\t\tauto partitions = HivePartitioning::Parse(filename);\n \t\tfor (auto &partition : partitions) {\n \t\t\tauto lookup_column_id = column_map.find(partition.first);\n \t\t\tif (lookup_column_id != column_map.end()) {\n-\t\t\t\tresult[lookup_column_id->second] = partition.second;\n+\t\t\t\tresult.insert(\n+\t\t\t\t    make_pair(lookup_column_id->second, PartitioningColumnValue(partition.first, partition.second)));\n \t\t\t}\n \t\t}\n \t}\n@@ -40,8 +48,9 @@ static unordered_map<column_t, string> GetKnownColumnValues(string &filename,\n }\n \n // Takes an expression and converts a list of known column_refs to constants\n-static void ConvertKnownColRefToConstants(unique_ptr<Expression> &expr,\n-                                          unordered_map<column_t, string> &known_column_values, idx_t table_index) {\n+static void ConvertKnownColRefToConstants(ClientContext &context, unique_ptr<Expression> &expr,\n+                                          const unordered_map<column_t, PartitioningColumnValue> &known_column_values,\n+                                          idx_t table_index) {\n \tif (expr->type == ExpressionType::BOUND_COLUMN_REF) {\n \t\tauto &bound_colref = expr->Cast<BoundColumnRefExpression>();\n \n@@ -52,11 +61,21 @@ static void ConvertKnownColRefToConstants(unique_ptr<Expression> &expr,\n \n \t\tauto lookup = known_column_values.find(bound_colref.binding.column_index);\n \t\tif (lookup != known_column_values.end()) {\n-\t\t\texpr = make_uniq<BoundConstantExpression>(Value(lookup->second).DefaultCastAs(bound_colref.return_type));\n+\t\t\tauto &partition_val = lookup->second;\n+\t\t\tValue result_val;\n+\t\t\tif (partition_val.key.empty()) {\n+\t\t\t\t// filename column - use directly\n+\t\t\t\tresult_val = Value(partition_val.value);\n+\t\t\t} else {\n+\t\t\t\t// hive partitioning column - cast the value to the target type\n+\t\t\t\tresult_val = HivePartitioning::GetValue(context, partition_val.key, partition_val.value,\n+\t\t\t\t                                        bound_colref.return_type);\n+\t\t\t}\n+\t\t\texpr = make_uniq<BoundConstantExpression>(std::move(result_val));\n \t\t}\n \t} else {\n \t\tExpressionIterator::EnumerateChildren(*expr, [&](unique_ptr<Expression> &child) {\n-\t\t\tConvertKnownColRefToConstants(child, known_column_values, table_index);\n+\t\t\tConvertKnownColRefToConstants(context, child, known_column_values, table_index);\n \t\t});\n \t}\n }\n@@ -65,57 +84,87 @@ static void ConvertKnownColRefToConstants(unique_ptr<Expression> &expr,\n // \t- s3://bucket/var1=value1/bla/bla/var2=value2\n //  - http(s)://domain(:port)/lala/kasdl/var1=value1/?not-a-var=not-a-value\n //  - folder/folder/folder/../var1=value1/etc/.//var2=value2\n-const string &HivePartitioning::RegexString() {\n-\tstatic string REGEX = \"[\\\\/\\\\\\\\]([^\\\\/\\\\?\\\\\\\\]+)=([^\\\\/\\\\n\\\\?\\\\\\\\]*)\";\n-\treturn REGEX;\n-}\n-\n-std::map<string, string> HivePartitioning::Parse(const string &filename, duckdb_re2::RE2 &regex) {\n+std::map<string, string> HivePartitioning::Parse(const string &filename) {\n+\tidx_t partition_start = 0;\n+\tidx_t equality_sign = 0;\n+\tbool candidate_partition = true;\n \tstd::map<string, string> result;\n-\tduckdb_re2::StringPiece input(filename); // Wrap a StringPiece around it\n-\n-\tstring var;\n-\tstring value;\n-\twhile (RE2::FindAndConsume(&input, regex, &var, &value)) {\n-\t\tresult.insert(std::pair<string, string>(var, value));\n+\tfor (idx_t c = 0; c < filename.size(); c++) {\n+\t\tif (filename[c] == '?' || filename[c] == '\\n') {\n+\t\t\t// get parameter or newline - not a partition\n+\t\t\tcandidate_partition = false;\n+\t\t}\n+\t\tif (filename[c] == '\\\\' || filename[c] == '/') {\n+\t\t\t// separator\n+\t\t\tif (candidate_partition && equality_sign > partition_start) {\n+\t\t\t\t// we found a partition with an equality sign\n+\t\t\t\tstring key = filename.substr(partition_start, equality_sign - partition_start);\n+\t\t\t\tstring value = filename.substr(equality_sign + 1, c - equality_sign - 1);\n+\t\t\t\tresult.insert(make_pair(std::move(key), std::move(value)));\n+\t\t\t}\n+\t\t\tpartition_start = c + 1;\n+\t\t\tcandidate_partition = true;\n+\t\t} else if (filename[c] == '=') {\n+\t\t\tif (equality_sign > partition_start) {\n+\t\t\t\t// multiple equality signs - not a partition\n+\t\t\t\tcandidate_partition = false;\n+\t\t\t}\n+\t\t\tequality_sign = c;\n+\t\t}\n \t}\n \treturn result;\n }\n \n-std::map<string, string> HivePartitioning::Parse(const string &filename) {\n-\tduckdb_re2::RE2 regex(RegexString());\n-\treturn Parse(filename, regex);\n+Value HivePartitioning::GetValue(ClientContext &context, const string &key, const string &str_val,\n+                                 const LogicalType &type) {\n+\t// Handle nulls\n+\tif (StringUtil::CIEquals(str_val, \"NULL\")) {\n+\t\treturn Value(type);\n+\t}\n+\tif (type.id() == LogicalTypeId::VARCHAR) {\n+\t\t// for string values we can directly return the type\n+\t\treturn Value(str_val);\n+\t}\n+\tif (str_val.empty()) {\n+\t\t// empty strings are NULL for non-string types\n+\t\treturn Value(type);\n+\t}\n+\n+\t// cast to the target type\n+\tValue value(str_val);\n+\tif (!value.TryCastAs(context, type)) {\n+\t\tthrow InvalidInputException(\"Unable to cast '%s' (from hive partition column '%s') to: '%s'\", value.ToString(),\n+\t\t                            StringUtil::Upper(key), type.ToString());\n+\t}\n+\treturn value;\n }\n \n // TODO: this can still be improved by removing the parts of filter expressions that are true for all remaining files.\n //\t\t currently, only expressions that cannot be evaluated during pushdown are removed.\n void HivePartitioning::ApplyFiltersToFileList(ClientContext &context, vector<string> &files,\n                                               vector<unique_ptr<Expression>> &filters,\n-                                              unordered_map<string, column_t> &column_map, MultiFilePushdownInfo &info,\n-                                              bool hive_enabled, bool filename_enabled) {\n+                                              const HivePartitioningFilterInfo &filter_info,\n+                                              MultiFilePushdownInfo &info) {\n \n \tvector<string> pruned_files;\n \tvector<bool> have_preserved_filter(filters.size(), false);\n \tvector<unique_ptr<Expression>> pruned_filters;\n \tunordered_set<idx_t> filters_applied_to_files;\n-\tduckdb_re2::RE2 regex(RegexString());\n \tauto table_index = info.table_index;\n \n-\tif ((!filename_enabled && !hive_enabled) || filters.empty()) {\n+\tif ((!filter_info.filename_enabled && !filter_info.hive_enabled) || filters.empty()) {\n \t\treturn;\n \t}\n \n \tfor (idx_t i = 0; i < files.size(); i++) {\n \t\tauto &file = files[i];\n \t\tbool should_prune_file = false;\n-\t\tauto known_values = GetKnownColumnValues(file, column_map, regex, filename_enabled, hive_enabled);\n-\n-\t\tFilterCombiner combiner(context);\n+\t\tauto known_values = GetKnownColumnValues(file, filter_info);\n \n \t\tfor (idx_t j = 0; j < filters.size(); j++) {\n \t\t\tauto &filter = filters[j];\n \t\t\tunique_ptr<Expression> filter_copy = filter->Copy();\n-\t\t\tConvertKnownColRefToConstants(filter_copy, known_values, table_index);\n+\t\t\tConvertKnownColRefToConstants(context, filter_copy, known_values, table_index);\n \t\t\t// Evaluate the filter, if it can be evaluated here, we can not prune this filter\n \t\t\tValue result_value;\n \n@@ -126,7 +175,7 @@ void HivePartitioning::ApplyFiltersToFileList(ClientContext &context, vector<str\n \t\t\t\t\tpruned_filters.emplace_back(filter->Copy());\n \t\t\t\t\thave_preserved_filter[j] = true;\n \t\t\t\t}\n-\t\t\t} else if (!result_value.GetValue<bool>()) {\n+\t\t\t} else if (result_value.IsNull() || !result_value.GetValue<bool>()) {\n \t\t\t\t// filter evaluates to false\n \t\t\t\tshould_prune_file = true;\n \t\t\t\t// convert the filter to a table filter.\ndiff --git a/src/common/multi_file_list.cpp b/src/common/multi_file_list.cpp\nindex afd5af6f1e3e..e6483bb6c878 100644\n--- a/src/common/multi_file_list.cpp\n+++ b/src/common/multi_file_list.cpp\n@@ -25,16 +25,17 @@ MultiFilePushdownInfo::MultiFilePushdownInfo(idx_t table_index, const vector<str\n // Helper method to do Filter Pushdown into a MultiFileList\n bool PushdownInternal(ClientContext &context, const MultiFileReaderOptions &options, MultiFilePushdownInfo &info,\n                       vector<unique_ptr<Expression>> &filters, vector<string> &expanded_files) {\n-\tunordered_map<string, column_t> column_map;\n+\tHivePartitioningFilterInfo filter_info;\n \tfor (idx_t i = 0; i < info.column_ids.size(); i++) {\n \t\tif (!IsRowIdColumnId(info.column_ids[i])) {\n-\t\t\tcolumn_map.insert({info.column_names[info.column_ids[i]], i});\n+\t\t\tfilter_info.column_map.insert({info.column_names[info.column_ids[i]], i});\n \t\t}\n \t}\n+\tfilter_info.hive_enabled = options.hive_partitioning;\n+\tfilter_info.filename_enabled = options.filename;\n \n \tauto start_files = expanded_files.size();\n-\tHivePartitioning::ApplyFiltersToFileList(context, expanded_files, filters, column_map, info,\n-\t                                         options.hive_partitioning, options.filename);\n+\tHivePartitioning::ApplyFiltersToFileList(context, expanded_files, filters, filter_info, info);\n \n \tif (expanded_files.size() != start_files) {\n \t\treturn true;\ndiff --git a/src/common/multi_file_reader.cpp b/src/common/multi_file_reader.cpp\nindex a8882b816644..6f4d2faa4485 100644\n--- a/src/common/multi_file_reader.cpp\n+++ b/src/common/multi_file_reader.cpp\n@@ -444,35 +444,23 @@ void UnionByName::CombineUnionTypes(const vector<string> &col_names, const vecto\n }\n \n bool MultiFileReaderOptions::AutoDetectHivePartitioningInternal(MultiFileList &files, ClientContext &context) {\n-\tstd::unordered_set<string> partitions;\n-\tauto &fs = FileSystem::GetFileSystem(context);\n-\n \tauto first_file = files.GetFirstFile();\n-\tauto splits_first_file = StringUtil::Split(first_file, fs.PathSeparator(first_file));\n-\tif (splits_first_file.size() < 2) {\n-\t\treturn false;\n-\t}\n-\tfor (auto &split : splits_first_file) {\n-\t\tauto partition = StringUtil::Split(split, \"=\");\n-\t\tif (partition.size() == 2) {\n-\t\t\tpartitions.insert(partition.front());\n-\t\t}\n-\t}\n+\tauto partitions = HivePartitioning::Parse(first_file);\n \tif (partitions.empty()) {\n+\t\t// no partitions found in first file\n \t\treturn false;\n \t}\n \n \tfor (const auto &file : files.Files()) {\n-\t\tauto splits = StringUtil::Split(file, fs.PathSeparator(file));\n-\t\tif (splits.size() != splits_first_file.size()) {\n+\t\tauto new_partitions = HivePartitioning::Parse(file);\n+\t\tif (new_partitions.size() != partitions.size()) {\n+\t\t\t// partition count mismatch\n \t\t\treturn false;\n \t\t}\n-\t\tfor (auto it = splits.begin(); it != std::prev(splits.end()); it++) {\n-\t\t\tauto part = StringUtil::Split(*it, \"=\");\n-\t\t\tif (part.size() != 2) {\n-\t\t\t\tcontinue;\n-\t\t\t}\n-\t\t\tif (partitions.find(part.front()) == partitions.end()) {\n+\t\tfor (auto &part : new_partitions) {\n+\t\t\tauto entry = partitions.find(part.first);\n+\t\t\tif (entry == partitions.end()) {\n+\t\t\t\t// differing partitions between files\n \t\t\t\treturn false;\n \t\t\t}\n \t\t}\n@@ -482,21 +470,9 @@ bool MultiFileReaderOptions::AutoDetectHivePartitioningInternal(MultiFileList &f\n void MultiFileReaderOptions::AutoDetectHiveTypesInternal(MultiFileList &files, ClientContext &context) {\n \tconst LogicalType candidates[] = {LogicalType::DATE, LogicalType::TIMESTAMP, LogicalType::BIGINT};\n \n-\tauto &fs = FileSystem::GetFileSystem(context);\n-\n \tunordered_map<string, LogicalType> detected_types;\n \tfor (const auto &file : files.Files()) {\n-\t\tunordered_map<string, string> partitions;\n-\t\tauto splits = StringUtil::Split(file, fs.PathSeparator(file));\n-\t\tif (splits.size() < 2) {\n-\t\t\treturn;\n-\t\t}\n-\t\tfor (auto it = splits.begin(); it != std::prev(splits.end()); it++) {\n-\t\t\tauto part = StringUtil::Split(*it, \"=\");\n-\t\t\tif (part.size() == 2) {\n-\t\t\t\tpartitions[part.front()] = part.back();\n-\t\t\t}\n-\t\t}\n+\t\tauto partitions = HivePartitioning::Parse(file);\n \t\tif (partitions.empty()) {\n \t\t\treturn;\n \t\t}\n@@ -573,24 +549,13 @@ bool MultiFileReaderOptions::AnySet() {\n \treturn filename || hive_partitioning || union_by_name;\n }\n \n-Value MultiFileReaderOptions::GetHivePartitionValue(const string &base, const string &entry,\n+Value MultiFileReaderOptions::GetHivePartitionValue(const string &value, const string &key,\n                                                     ClientContext &context) const {\n-\tValue value(base);\n-\tauto it = hive_types_schema.find(entry);\n+\tauto it = hive_types_schema.find(key);\n \tif (it == hive_types_schema.end()) {\n-\t\treturn value;\n-\t}\n-\n-\t// Handle nulls\n-\tif (base.empty() || StringUtil::CIEquals(base, \"NULL\")) {\n-\t\treturn Value(it->second);\n-\t}\n-\n-\tif (!value.TryCastAs(context, it->second)) {\n-\t\tthrow InvalidInputException(\"Unable to cast '%s' (from hive partition column '%s') to: '%s'\", value.ToString(),\n-\t\t                            StringUtil::Upper(it->first), it->second.ToString());\n+\t\treturn HivePartitioning::GetValue(context, key, value, LogicalType::VARCHAR);\n \t}\n-\treturn value;\n+\treturn HivePartitioning::GetValue(context, key, value, it->second);\n }\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/common/hive_partitioning.hpp b/src/include/duckdb/common/hive_partitioning.hpp\nindex 867867759ec3..770dc6b50980 100644\n--- a/src/include/duckdb/common/hive_partitioning.hpp\n+++ b/src/include/duckdb/common/hive_partitioning.hpp\n@@ -18,29 +18,29 @@\n #include <iostream>\n #include <sstream>\n \n-namespace duckdb_re2 {\n-class RE2;\n-} // namespace duckdb_re2\n-\n namespace duckdb {\n struct MultiFilePushdownInfo;\n \n+struct HivePartitioningFilterInfo {\n+\tunordered_map<string, column_t> column_map;\n+\tbool hive_enabled;\n+\tbool filename_enabled;\n+};\n+\n class HivePartitioning {\n public:\n \t//! Parse a filename that follows the hive partitioning scheme\n \tDUCKDB_API static std::map<string, string> Parse(const string &filename);\n-\tDUCKDB_API static std::map<string, string> Parse(const string &filename, duckdb_re2::RE2 &regex);\n \t//! Prunes a list of filenames based on a set of filters, can be used by TableFunctions in the\n \t//! pushdown_complex_filter function to skip files with filename-based filters. Also removes the filters that always\n \t//! evaluate to true.\n \tDUCKDB_API static void ApplyFiltersToFileList(ClientContext &context, vector<string> &files,\n \t                                              vector<unique_ptr<Expression>> &filters,\n-\t                                              unordered_map<string, column_t> &column_map,\n-\t                                              MultiFilePushdownInfo &info, bool hive_enabled,\n-\t                                              bool filename_enabled);\n+\t                                              const HivePartitioningFilterInfo &filter_info,\n+\t                                              MultiFilePushdownInfo &info);\n \n-\t//! Returns the compiled regex pattern to match hive partitions\n-\tDUCKDB_API static const string &RegexString();\n+\tDUCKDB_API static Value GetValue(ClientContext &context, const string &key, const string &value,\n+\t                                 const LogicalType &type);\n };\n \n struct HivePartitionKey {\n",
  "test_patch": "diff --git a/data/parquet-testing/hive-partitioning/duplicate_names/x=1/x=2/test.parquet b/data/parquet-testing/hive-partitioning/duplicate_names/x=1/x=2/test.parquet\nnew file mode 100644\nindex 000000000000..ea9710486eac\nBinary files /dev/null and b/data/parquet-testing/hive-partitioning/duplicate_names/x=1/x=2/test.parquet differ\ndiff --git a/data/parquet-testing/hive-partitioning/duplicate_names/x=2/x=1/test.parquet b/data/parquet-testing/hive-partitioning/duplicate_names/x=2/x=1/test.parquet\nnew file mode 100644\nindex 000000000000..afa7bb5c0ed6\nBinary files /dev/null and b/data/parquet-testing/hive-partitioning/duplicate_names/x=2/x=1/test.parquet differ\ndiff --git a/data/parquet-testing/hive-partitioning/empty_string/key=NULL/part.parquet b/data/parquet-testing/hive-partitioning/empty_string/key=NULL/part.parquet\nnew file mode 100644\nindex 000000000000..97e54f67ee29\nBinary files /dev/null and b/data/parquet-testing/hive-partitioning/empty_string/key=NULL/part.parquet differ\ndiff --git a/test/sql/copy/parquet/parquet_hive_empty.test b/test/sql/copy/parquet/parquet_hive_empty.test\nindex ab11846371b7..4e7f0d2fc999 100644\n--- a/test/sql/copy/parquet/parquet_hive_empty.test\n+++ b/test/sql/copy/parquet/parquet_hive_empty.test\n@@ -6,9 +6,32 @@ require parquet\n \n query II\n select * \n-from parquet_scan('data/parquet-testing/hive-partitioning/empty_string/*/*.parquet', hive_partitioning=1)\n+from parquet_scan('data/parquet-testing/hive-partitioning/empty_string/*/*.parquet')\n ORDER BY ALL\n ----\n a\ta\n b\t(empty)\n+c\tNULL\n \n+# filter on hive partitioning with NULL values\n+query II\n+select *\n+from parquet_scan('data/parquet-testing/hive-partitioning/empty_string/*/*.parquet')\n+WHERE key IS NULL\n+----\n+c\tNULL\n+\n+\n+query II\n+select *\n+from parquet_scan('data/parquet-testing/hive-partitioning/empty_string/*/*.parquet')\n+WHERE key='a'\n+----\n+a\ta\n+\n+query II\n+select *\n+from parquet_scan('data/parquet-testing/hive-partitioning/empty_string/*/*.parquet')\n+WHERE key=''\n+----\n+b\t(empty)\ndiff --git a/test/sql/copy/partitioned/hive_partition_duplicate_name.test b/test/sql/copy/partitioned/hive_partition_duplicate_name.test\nnew file mode 100644\nindex 000000000000..2f3f52e8f043\n--- /dev/null\n+++ b/test/sql/copy/partitioned/hive_partition_duplicate_name.test\n@@ -0,0 +1,15 @@\n+# name: test/sql/copy/partitioned/hive_partition_duplicate_name.test\n+# description: Test partitioning names with duplicate keys\n+# group: [partitioned]\n+\n+require parquet\n+\n+# we just use the first partitioning key by default\n+query III\n+select *\n+from parquet_scan('data/parquet-testing/hive-partitioning/duplicate_names/**/*.parquet')\n+ORDER BY ALL\n+----\n+1\tvalue1\t1\n+2\tvalue2\t2\n+\n",
  "problem_statement": "Cannot filter for NULL value on a hive partitioned column from parquet\n### What happens?\n\nWhen attempting to filter for NULL values on a hive-partitioned parquet directory structure using `read_parquet` - DuckDB returns 0 rows.\r\n\n\n### To Reproduce\n\n```sql\r\nCALL dbgen(sf = 0.01);\r\n\r\nALTER TABLE orders\r\n    ALTER COLUMN o_orderpriority DROP NOT NULL;\r\n\r\nUPDATE orders\r\nSET o_orderpriority = NULL\r\nWHERE o_orderkey BETWEEN 1 AND 10;\r\n\r\nCOPY orders TO '/tmp/orders'\r\n    (FORMAT PARQUET, PARTITION_BY (o_orderpriority), OVERWRITE_OR_IGNORE, COMPRESSION zstd);\r\n\r\ncreate or replace view orders_hive_view as\r\nselect *\r\nfrom read_parquet('/tmp/orders/*/*.parquet'\r\n    , hive_partitioning = true\r\n    , hive_types = {'o_orderpriority': VARCHAR}\r\n                   );\r\n\r\nselect coalesce(o_orderpriority, 'it is NULL') as o_orderpriority, count(*)\r\nfrom orders_hive_view\r\ngroup by all\r\norder by 1;\r\n\r\nselect count(*)\r\n  from orders_hive_view\r\nwhere o_orderpriority IS NULL;\r\n```\r\n\r\nThis produces output:\r\n```text\r\nduckdb\r\nv1.0.0 1f98600c2c\r\nEnter \".help\" for usage hints.\r\nConnected to a transient in-memory database.\r\nUse \".open FILENAME\" to reopen on a persistent database.\r\nD CALL dbgen(sf = 0.01);\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Success \u2502\r\n\u2502 boolean \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 0 rows  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nD \r\nD ALTER TABLE orders\r\n      ALTER COLUMN o_orderpriority DROP NOT NULL;\r\nD \r\nD UPDATE orders\r\n  SET o_orderpriority = NULL\r\n  WHERE o_orderkey BETWEEN 1 AND 10;\r\nD \r\nD COPY orders TO '/tmp/orders'\r\n      (FORMAT PARQUET, PARTITION_BY (o_orderpriority), OVERWRITE_OR_IGNORE, COMPRESSION zstd);\r\nD \r\nD create or replace view orders_hive_view as\r\n  select *\r\n  from read_parquet('/tmp/orders/*/*.parquet'\r\n      , hive_partitioning = true\r\n      , hive_types = {'o_orderpriority': VARCHAR}\r\n                     );\r\nD \r\nD select coalesce(o_orderpriority, 'it is NULL') as o_orderpriority, count(*)\r\n  from orders_hive_view\r\n  group by all\r\n  order by 1;\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 o_orderpriority \u2502 count_star() \u2502\r\n\u2502     varchar     \u2502    int64     \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 1-URGENT        \u2502         3019 \u2502\r\n\u2502 2-HIGH          \u2502         3064 \u2502\r\n\u2502 3-MEDIUM        \u2502         2941 \u2502\r\n\u2502 4-NOT SPECIFIED \u2502         3023 \u2502\r\n\u2502 5-LOW           \u2502         2946 \u2502\r\n\u2502 it is NULL      \u2502            7 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nD \r\nD select count(*)\r\n    from orders_hive_view\r\n  where o_orderpriority IS NULL;\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 count_star() \u2502\r\n\u2502    int64     \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502            0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\n\n### OS:\n\nLinux - x86-64\n\n### DuckDB Version:\n\n1.0.0\n\n### DuckDB Client:\n\ncli\n\n### Full Name:\n\nPhilip Moore\n\n### Affiliation:\n\nVoltron Data\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nNot applicable - the reproduction does not require a data set\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\n",
  "hints_text": "@prmoore77 Thanks for the reproducer script, it was great. I could reproduce the issue in seconds. This indeed seems to be a bug, we'll take a look.\nThanks, @szarnyasg !",
  "created_at": "2024-07-12T13:20:09Z"
}