{
  "repo": "duckdb/duckdb",
  "pull_number": 16751,
  "instance_id": "duckdb__duckdb-16751",
  "issue_numbers": [
    "16554",
    "16554",
    "16719"
  ],
  "base_commit": "0b3d9e754a505641b56d11016198cdeacb032c77",
  "patch": "diff --git a/.github/workflows/BundleStaticLibs.yml b/.github/workflows/BundleStaticLibs.yml\nindex e9e4e6690d3b..3fbe60961d93 100644\n--- a/.github/workflows/BundleStaticLibs.yml\n+++ b/.github/workflows/BundleStaticLibs.yml\n@@ -80,7 +80,8 @@ jobs:\n \n       - name: Bundle static library\n         shell: bash\n-        run: make bundle-library-o\n+        run: |\n+          make gather-libs\n \n       - name: Print platform\n         shell: bash\n@@ -93,14 +94,14 @@ jobs:\n           AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_DUCKDB_STAGING_KEY }}\n         run: |\n           python3 scripts/amalgamation.py\n-          zip -j static-lib-osx-${{ matrix.architecture }}.zip src/include/duckdb.h build/release/libduckdb_bundle.a\n-          ./scripts/upload-assets-to-staging.sh github_release static-lib-osx-${{ matrix.architecture }}.zip\n+          zip -r -j static-libs-osx-${{ matrix.architecture }}.zip src/include/duckdb.h build/release/libs/\n+          ./scripts/upload-assets-to-staging.sh github_release static-libs-osx-${{ matrix.architecture }}.zip\n \n       - uses: actions/upload-artifact@v4\n         with:\n-          name: duckdb-static-lib-osx-${{ matrix.architecture }}\n+          name: duckdb-static-libs-osx-${{ matrix.architecture }}\n           path: |\n-            static-lib-osx-${{ matrix.architecture }}.zip\n+            static-libs-osx-${{ matrix.architecture }}.zip\n \n   bundle-mingw-static-lib:\n     name: Windows MingW static libs\n@@ -142,7 +143,7 @@ jobs:\n       - name: Bundle static library\n         shell: bash\n         run: |\n-          make bundle-library-obj\n+          make gather-libs\n \n       - name: Deploy\n         shell: bash\n@@ -150,14 +151,14 @@ jobs:\n           AWS_ACCESS_KEY_ID: ${{ secrets.S3_DUCKDB_STAGING_ID }}\n           AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_DUCKDB_STAGING_KEY }}\n         run: |\n-          zip -j static-lib-windows-mingw.zip src/include/duckdb.h build/release/libduckdb_bundle.a\n-          ./scripts/upload-assets-to-staging.sh github_release static-lib-windows-mingw.zip\n+          zip -r -j static-libs-windows-mingw.zip src/include/duckdb.h build/release/libs/\n+          ./scripts/upload-assets-to-staging.sh github_release static-libs-windows-mingw.zip\n \n       - uses: actions/upload-artifact@v4\n         with:\n-          name: duckdb-static-lib-windows-mingw\n+          name: duckdb-static-libs-windows-mingw\n           path: |\n-            static-lib-windows-mingw.zip\n+            static-libs-windows-mingw.zip\n   bundle-linux-arm64-static-libs:\n     name: Linux arm64 static libs\n     runs-on: ubuntu-latest\n@@ -183,7 +184,7 @@ jobs:\n           -e FORCE_WARN_UNUSED=1                                                 \\\n           -e DUCKDB_PLATFORM=linux_arm64                                         \\\n           ubuntu:18.04                                                           \\\n-          bash -c \"/duckdb/scripts/setup_ubuntu1804.sh && git config --global --add safe.directory /duckdb && make bundle-library -C /duckdb\"\n+          bash -c \"/duckdb/scripts/setup_ubuntu1804.sh && git config --global --add safe.directory /duckdb && make gather-libs -C /duckdb\"\n       - name: Deploy\n         shell: bash\n         env:\n@@ -191,13 +192,13 @@ jobs:\n           AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_DUCKDB_STAGING_KEY }}\n         run: |\n           python3 scripts/amalgamation.py\n-          zip -j static-lib-linux-arm64.zip src/include/duckdb.h build/release/libduckdb_bundle.a\n-          ./scripts/upload-assets-to-staging.sh github_release static-lib-linux-arm64.zip\n+          zip -r -j static-libs-linux-arm64.zip src/include/duckdb.h build/release/libs/\n+          ./scripts/upload-assets-to-staging.sh github_release static-libs-linux-arm64.zip\n       - uses: actions/upload-artifact@v4\n         with:\n-          name: duckdb-static-lib-linux-arm64\n+          name: duckdb-static-libs-linux-arm64\n           path: |\n-            static-lib-linux-arm64.zip\n+            static-libs-linux-arm64.zip\n   bundle-linux-amd64-static-libs:\n     name: Linux amd64 static libs\n     runs-on: ubuntu-latest\n@@ -225,7 +226,7 @@ jobs:\n           -e BUILD_BENCHMARK=1                                                   \\\n           -e FORCE_WARN_UNUSED=1                                                 \\\n           quay.io/pypa/manylinux2014_x86_64                                      \\\n-          bash -c \"yum install -y perl-IPC-Cmd && git config --global --add safe.directory $PWD && make bundle-library -C $PWD\"\n+          bash -c \"yum install -y perl-IPC-Cmd && git config --global --add safe.directory $PWD && make gather-libs -C $PWD\"\n       - name: Print platform\n         shell: bash\n         run: ./build/release/duckdb -c \"PRAGMA platform;\"\n@@ -237,10 +238,10 @@ jobs:\n           AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_DUCKDB_STAGING_KEY }}\n         run: |\n           python3 scripts/amalgamation.py\n-          zip -j static-lib-linux-amd64.zip src/include/duckdb.h build/release/libduckdb_bundle.a\n-          ./scripts/upload-assets-to-staging.sh github_release static-lib-linux-amd64.zip\n+          zip -r -j static-libs-linux-amd64.zip src/include/duckdb.h build/release/libs/\n+          ./scripts/upload-assets-to-staging.sh github_release static-libs-linux-amd64.zip\n       - uses: actions/upload-artifact@v4\n         with:\n-          name: duckdb-static-lib-linux-amd64\n+          name: duckdb-static-libs-linux-amd64\n           path: |\n-            static-lib-linux-amd64.zip\n\\ No newline at end of file\n+            static-libs-linux-amd64.zip\n\\ No newline at end of file\ndiff --git a/Makefile b/Makefile\nindex 38ee1c99dab5..b17c7e6ff66b 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -524,3 +524,11 @@ bundle-library-obj: bundle-setup\n \n bundle-library: release\n \tmake bundle-library-o\n+\n+gather-libs: release\n+\tcd build/release && \\\n+\trm -rf libs && \\\n+\tmkdir -p libs && \\\n+\tcp src/libduckdb_static.a libs/. && \\\n+\tcp third_party/*/libduckdb_*.a libs/. && \\\n+\tcp extension/*/lib*_extension.a libs/.\n\\ No newline at end of file\ndiff --git a/data/csv/afl/4086/case_1.csv b/data/csv/afl/4086/case_1.csv\nnew file mode 100644\nindex 000000000000..0b404fbd4d54\n--- /dev/null\n+++ b/data/csv/afl/4086/case_1.csv\n@@ -0,0 +1,2 @@\n+\ufffd\r\n+\ufffd\n\\ No newline at end of file\ndiff --git a/data/csv/afl/4086/case_2.csv b/data/csv/afl/4086/case_2.csv\nnew file mode 100644\nindex 000000000000..c9bf78e836f7\n--- /dev/null\n+++ b/data/csv/afl/4086/case_2.csv\n@@ -0,0 +1,149 @@\n+\r\n+>\r\n+?\r\n+@\r\n+A\r\n+B\r\n+C\r\n+D\r\n+E\r\n+F\r\n+G\r\n+H\r\n+I\r\n+J\r\n+K\r\n+L\r\n+M\r\n+N\r\n+O\r\n+P\r\n+Q\r\n+R\r\n+S\r\n+T\r\n+U\r\n+V\r\n+W\r\n+X\r\n+Y\r\n+Z\r\n+[\r\n+\\\r\n+]\r\n+^\r\n+_\r\n+`\r\n+a\r\n+b\r\n+c\r\n+j\r\n+k\r\n+l\r\n+\r\n+\ufffd\r\n+\ufffd\r\n+\n+F\r\n+G\r\n+H\r\n+I\r\n+J\r\n+K\r\n+L\r\n+M\r\n+N\r\n+O\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffdw\r\n+x\r\n+y\r\n+z\r\n+{\r\n+|\r\n+}\r\n+~\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd}}}}}}}}}}}}}}}}}}}}}}\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+2\r\n+3\r\n+4\r\n+5\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\r\n+\ufffd\n\\ No newline at end of file\ndiff --git a/data/csv/afl/4086/case_3.csv b/data/csv/afl/4086/case_3.csv\nnew file mode 100644\nindex 000000000000..501156bee996\nBinary files /dev/null and b/data/csv/afl/4086/case_3.csv differ\ndiff --git a/extension/core_functions/include/core_functions/aggregate/quantile_sort_tree.hpp b/extension/core_functions/include/core_functions/aggregate/quantile_sort_tree.hpp\nindex a330c0a4bbef..b6a088786b7e 100644\n--- a/extension/core_functions/include/core_functions/aggregate/quantile_sort_tree.hpp\n+++ b/extension/core_functions/include/core_functions/aggregate/quantile_sort_tree.hpp\n@@ -8,17 +8,13 @@\n \n #pragma once\n \n-#include \"duckdb/common/sort/sort.hpp\"\n #include \"duckdb/common/types/column/column_data_collection.hpp\"\n-#include \"duckdb/common/types/row/row_layout.hpp\"\n #include \"core_functions/aggregate/quantile_helpers.hpp\"\n-#include \"duckdb/execution/merge_sort_tree.hpp\"\n #include \"duckdb/common/operator/cast_operators.hpp\"\n #include \"duckdb/common/operator/multiply.hpp\"\n #include \"duckdb/planner/expression/bound_constant_expression.hpp\"\n #include \"duckdb/function/window/window_index_tree.hpp\"\n #include <algorithm>\n-#include <numeric>\n #include <stdlib.h>\n #include <utility>\n \n@@ -89,7 +85,7 @@ struct QuantileDirect {\n \tusing RESULT_TYPE = T;\n \n \tinline const INPUT_TYPE &operator()(const INPUT_TYPE &x) const {\n-\t\treturn x;\n+\t\treturn x; // NOLINT\n \t}\n };\n \n@@ -365,7 +361,7 @@ struct QuantileSortTree {\n \t}\n \n \tinline idx_t SelectNth(const SubFrames &frames, size_t n) const {\n-\t\treturn index_tree->SelectNth(frames, n);\n+\t\treturn index_tree->SelectNth(frames, n).first;\n \t}\n \n \ttemplate <typename INPUT_TYPE, typename RESULT_TYPE, bool DISCRETE>\ndiff --git a/extension/core_functions/include/core_functions/aggregate/quantile_state.hpp b/extension/core_functions/include/core_functions/aggregate/quantile_state.hpp\nindex 00f4baf77735..cdf242ae9c9c 100644\n--- a/extension/core_functions/include/core_functions/aggregate/quantile_state.hpp\n+++ b/extension/core_functions/include/core_functions/aggregate/quantile_state.hpp\n@@ -207,6 +207,9 @@ struct WindowQuantileState {\n \t\t\t\tdest[0] = skips[0].second;\n \t\t\t\tif (skips.size() > 1) {\n \t\t\t\t\tdest[1] = skips[1].second;\n+\t\t\t\t} else {\n+\t\t\t\t\t// Avoid UMA\n+\t\t\t\t\tdest[1] = skips[0].second;\n \t\t\t\t}\n \t\t\t\treturn interp.template Extract<INPUT_TYPE, RESULT_TYPE>(dest.data(), result);\n \t\t\t} catch (const duckdb_skiplistlib::skip_list::IndexError &idx_err) {\ndiff --git a/scripts/generate_extensions_function.py b/scripts/generate_extensions_function.py\nindex da2181121abe..d6e861bfc6e9 100644\n--- a/scripts/generate_extensions_function.py\n+++ b/scripts/generate_extensions_function.py\n@@ -742,6 +742,7 @@ def write_header(data: ExtensionData):\n }; // EXTENSION_SECRET_PROVIDERS\n \n static constexpr const char *AUTOLOADABLE_EXTENSIONS[] = {\n+    \"avro\",\n     \"aws\",\n     \"azure\",\n     \"autocomplete\",\ndiff --git a/src/common/types/row/tuple_data_segment.cpp b/src/common/types/row/tuple_data_segment.cpp\nindex d14c0e0bad4c..c3383f3cb26f 100644\n--- a/src/common/types/row/tuple_data_segment.cpp\n+++ b/src/common/types/row/tuple_data_segment.cpp\n@@ -15,23 +15,23 @@ void TupleDataChunkPart::SetHeapEmpty() {\n \tbase_heap_ptr = nullptr;\n }\n \n-void SwapTupleDataChunkPart(TupleDataChunkPart &a, TupleDataChunkPart &b) {\n-\tstd::swap(a.row_block_index, b.row_block_index);\n-\tstd::swap(a.row_block_offset, b.row_block_offset);\n-\tstd::swap(a.heap_block_index, b.heap_block_index);\n-\tstd::swap(a.heap_block_offset, b.heap_block_offset);\n-\tstd::swap(a.base_heap_ptr, b.base_heap_ptr);\n-\tstd::swap(a.total_heap_size, b.total_heap_size);\n-\tstd::swap(a.count, b.count);\n+void MoveTupleDataChunkPart(TupleDataChunkPart &a, TupleDataChunkPart &b) {\n+\ta.row_block_index = b.row_block_index;\n+\ta.row_block_offset = b.row_block_offset;\n+\ta.heap_block_index = b.heap_block_index;\n+\ta.heap_block_offset = b.heap_block_offset;\n+\ta.base_heap_ptr = b.base_heap_ptr;\n+\ta.total_heap_size = b.total_heap_size;\n+\ta.count = b.count;\n \tstd::swap(a.lock, b.lock);\n }\n \n TupleDataChunkPart::TupleDataChunkPart(TupleDataChunkPart &&other) noexcept : lock((other.lock)) {\n-\tSwapTupleDataChunkPart(*this, other);\n+\tMoveTupleDataChunkPart(*this, other);\n }\n \n TupleDataChunkPart &TupleDataChunkPart::operator=(TupleDataChunkPart &&other) noexcept {\n-\tSwapTupleDataChunkPart(*this, other);\n+\tMoveTupleDataChunkPart(*this, other);\n \treturn *this;\n }\n \ndiff --git a/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp b/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp\nindex 476e4f4bb468..386c5552cf76 100644\n--- a/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp\n+++ b/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp\n@@ -63,9 +63,9 @@ StringValueResult::StringValueResult(CSVStates &states, CSVStateMachine &state_m\n \t\t\t    \"Mismatch between the number of columns (%d) in the CSV file and what is expected in the scanner (%d).\",\n \t\t\t    number_of_columns, csv_file_scan->file_types.size());\n \t\t}\n-\t\tbool icu_loaded = csv_file_scan->buffer_manager->context.db->ExtensionIsLoaded(\"icu\");\n+\t\ticu_loaded = csv_file_scan->buffer_manager->context.db->ExtensionIsLoaded(\"icu\");\n \t\tfor (idx_t i = 0; i < csv_file_scan->file_types.size(); i++) {\n-\t\t\tauto &type = csv_file_scan->file_types[i];\n+\t\t\tauto type = csv_file_scan->file_types[i];\n \t\t\tif (type.IsJSONType()) {\n \t\t\t\ttype = LogicalType::VARCHAR;\n \t\t\t}\n@@ -436,6 +436,8 @@ void StringValueResult::AddValueToVector(const char *value_ptr, const idx_t size\n \t\t\t}\n \t\t\t// If we got here, we are ignoring errors, hence we must ignore this line.\n \t\t\tcurrent_errors.Insert(INVALID_UNICODE, cur_col_id, chunk_col_id, last_position);\n+\t\t\tstatic_cast<string_t *>(vector_ptr[chunk_col_id])[number_of_rows] = StringVector::AddStringOrBlob(\n+\t\t\t    parse_chunk.data[chunk_col_id], string_t(value_ptr, UnsafeNumericCast<uint32_t>(0)));\n \t\t\tbreak;\n \t\t}\n \t\tif (allocate) {\n@@ -606,7 +608,7 @@ void StringValueResult::AddValue(StringValueResult &result, const idx_t buffer_p\n \n void StringValueResult::HandleUnicodeError(idx_t col_idx, LinePosition &error_position) {\n \n-\tbool first_nl;\n+\tbool first_nl = false;\n \tauto borked_line = current_line_position.ReconstructCurrentLine(first_nl, buffer_handles, PrintErrorLine());\n \tLinesPerBoundary lines_per_batch(iterator.GetBoundaryIdx(), lines_read);\n \tif (current_line_position.begin == error_position) {\n@@ -673,6 +675,9 @@ bool LineError::HandleErrors(StringValueResult &result) {\n \t\t\t\t    result.current_line_position.begin.GetGlobalPosition(result.requested_size, first_nl),\n \t\t\t\t    line_pos.GetGlobalPosition(result.requested_size), result.path);\n \t\t\t}\n+\t\t\tif (!StringValueScanner::CanDirectlyCast(result.csv_file_scan->file_types[col_idx], result.icu_loaded)) {\n+\t\t\t\tresult.number_of_rows--;\n+\t\t\t}\n \t\t\tbreak;\n \t\t}\n \t\tcase UNTERMINATED_QUOTES:\n@@ -690,7 +695,7 @@ bool LineError::HandleErrors(StringValueResult &result) {\n \t\t\tbreak;\n \t\tcase CAST_ERROR: {\n \t\t\tstring column_name;\n-\t\t\tLogicalTypeId type_id;\n+\t\t\tLogicalTypeId type_id = LogicalTypeId::INVALID;\n \t\t\tif (cur_error.col_idx < result.names.size()) {\n \t\t\t\tcolumn_name = result.names[cur_error.col_idx];\n \t\t\t}\n@@ -768,7 +773,7 @@ void StringValueResult::NullPaddingQuotedNewlineCheck() const {\n string FullLinePosition::ReconstructCurrentLine(bool &first_char_nl,\n                                                 unordered_map<idx_t, shared_ptr<CSVBufferHandle>> &buffer_handles,\n                                                 bool reconstruct_line) const {\n-\tif (!reconstruct_line) {\n+\tif (!reconstruct_line || begin == end) {\n \t\treturn {};\n \t}\n \tstring result;\n@@ -822,6 +827,8 @@ bool StringValueResult::AddRowInternal() {\n \t}\n \n \tif (current_errors.HandleErrors(*this)) {\n+\t\tD_ASSERT(buffer_handles.find(current_line_position.begin.buffer_idx) != buffer_handles.end());\n+\t\tD_ASSERT(buffer_handles.find(current_line_position.end.buffer_idx) != buffer_handles.end());\n \t\tline_positions_per_row[static_cast<idx_t>(number_of_rows)] = current_line_position;\n \t\tnumber_of_rows++;\n \t\tif (static_cast<idx_t>(number_of_rows) >= result_size) {\n@@ -881,6 +888,8 @@ bool StringValueResult::AddRowInternal() {\n \t\t\tRemoveLastLine();\n \t\t}\n \t}\n+\tD_ASSERT(buffer_handles.find(current_line_position.begin.buffer_idx) != buffer_handles.end());\n+\tD_ASSERT(buffer_handles.find(current_line_position.end.buffer_idx) != buffer_handles.end());\n \tline_positions_per_row[static_cast<idx_t>(number_of_rows)] = current_line_position;\n \tcur_col_id = 0;\n \tchunk_col_id = 0;\n@@ -1024,6 +1033,7 @@ void StringValueScanner::Flush(DataChunk &insert_chunk) {\n \t\tauto &process_result = ParseChunk();\n \t\t// First Get Parsed Chunk\n \t\tauto &parse_chunk = process_result.ToChunk();\n+\t\tinsert_chunk.Reset();\n \t\t// We have to check if we got to error\n \t\terror_handler->ErrorIfNeeded();\n \t\tif (parse_chunk.size() == 0) {\n@@ -1086,7 +1096,7 @@ void StringValueScanner::Flush(DataChunk &insert_chunk) {\n \t\t\t\t\tif (!state_machine->options.IgnoreErrors()) {\n \t\t\t\t\t\tLinesPerBoundary lines_per_batch(iterator.GetBoundaryIdx(),\n \t\t\t\t\t\t                                 lines_read - parse_chunk.size() + line_error);\n-\t\t\t\t\t\tbool first_nl;\n+\t\t\t\t\t\tbool first_nl = false;\n \t\t\t\t\t\tauto borked_line = result.line_positions_per_row[line_error].ReconstructCurrentLine(\n \t\t\t\t\t\t    first_nl, result.buffer_handles, result.PrintErrorLine());\n \t\t\t\t\t\tstd::ostringstream error;\n@@ -1094,11 +1104,15 @@ void StringValueScanner::Flush(DataChunk &insert_chunk) {\n \t\t\t\t\t\t      << type.ToString() << \"\\'\";\n \t\t\t\t\t\tstring error_msg = error.str();\n \t\t\t\t\t\tSanitizeError(error_msg);\n+\t\t\t\t\t\tidx_t row_byte_pos = 0;\n+\t\t\t\t\t\tif (!(result.line_positions_per_row[line_error].begin ==\n+\t\t\t\t\t\t      result.line_positions_per_row[line_error].end)) {\n+\t\t\t\t\t\t\trow_byte_pos = result.line_positions_per_row[line_error].begin.GetGlobalPosition(\n+\t\t\t\t\t\t\t    result.result_size, first_nl);\n+\t\t\t\t\t\t}\n \t\t\t\t\t\tauto csv_error = CSVError::CastError(\n \t\t\t\t\t\t    state_machine->options, names[col_idx], error_msg, col_idx, borked_line, lines_per_batch,\n-\t\t\t\t\t\t    result.line_positions_per_row[line_error].begin.GetGlobalPosition(result.result_size,\n-\t\t\t\t\t\t                                                                      first_nl),\n-\t\t\t\t\t\t    optional_idx::Invalid(), result_vector.GetType().id(), result.path);\n+\t\t\t\t\t\t    row_byte_pos, optional_idx::Invalid(), result_vector.GetType().id(), result.path);\n \t\t\t\t\t\terror_handler->Error(csv_error);\n \t\t\t\t\t}\n \t\t\t\t}\n@@ -1116,7 +1130,7 @@ void StringValueScanner::Flush(DataChunk &insert_chunk) {\n \t\t\t\t\t\tif (!state_machine->options.IgnoreErrors()) {\n \t\t\t\t\t\t\tLinesPerBoundary lines_per_batch(iterator.GetBoundaryIdx(),\n \t\t\t\t\t\t\t                                 lines_read - parse_chunk.size() + line_error);\n-\t\t\t\t\t\t\tbool first_nl;\n+\t\t\t\t\t\t\tbool first_nl = false;\n \t\t\t\t\t\t\tauto borked_line = result.line_positions_per_row[line_error].ReconstructCurrentLine(\n \t\t\t\t\t\t\t    first_nl, result.buffer_handles, result.PrintErrorLine());\n \t\t\t\t\t\t\tstd::ostringstream error;\n@@ -1148,6 +1162,7 @@ void StringValueScanner::Flush(DataChunk &insert_chunk) {\n \t\t\t}\n \t\t\t// Now we slice the result\n \t\t\tinsert_chunk.Slice(successful_rows, sel_idx);\n+\t\t\tresult.borked_rows.clear();\n \t\t}\n \t\tif (insert_chunk.size() == 0 && cur_buffer_handle) {\n \t\t\tidx_t to_pos;\n@@ -1697,13 +1712,14 @@ bool StringValueScanner::CanDirectlyCast(const LogicalType &type, bool icu_loade\n \tcase LogicalTypeId::TIMESTAMP:\n \tcase LogicalTypeId::TIME:\n \tcase LogicalTypeId::DECIMAL:\n-\tcase LogicalType::VARCHAR:\n \tcase LogicalType::BOOLEAN:\n \t\treturn true;\n \tcase LogicalType::TIMESTAMP_TZ:\n \t\t// We only try to do direct cast of timestamp tz if the ICU extension is not loaded, otherwise, it needs to go\n \t\t// through string -> timestamp_tz casting\n \t\treturn !icu_loaded;\n+\tcase LogicalType::VARCHAR:\n+\t\treturn !type.IsJSONType();\n \tdefault:\n \t\treturn false;\n \t}\n@@ -1884,7 +1900,6 @@ void StringValueScanner::FinalizeChunkProcess() {\n \t\tif (result.current_errors.HandleErrors(result)) {\n \t\t\tresult.number_of_rows++;\n \t\t}\n-\n \t\tif (states.IsQuotedCurrent() && !found_error &&\n \t\t    state_machine->dialect_options.state_machine_options.strict_mode.GetValue()) {\n \t\t\t// If we finish the execution of a buffer, and we end in a quoted state, it means we have unterminated\ndiff --git a/src/execution/operator/csv_scanner/state_machine/csv_state_machine_cache.cpp b/src/execution/operator/csv_scanner/state_machine/csv_state_machine_cache.cpp\nindex 431b5ab6feca..332937d5bc55 100644\n--- a/src/execution/operator/csv_scanner/state_machine/csv_state_machine_cache.cpp\n+++ b/src/execution/operator/csv_scanner/state_machine/csv_state_machine_cache.cpp\n@@ -58,14 +58,19 @@ void CSVStateMachineCache::Insert(const CSVStateMachineOptions &state_machine_op\n \t}\n \n \tconst auto delimiter_value = state_machine_options.delimiter.GetValue();\n-\tconst auto delimiter_first_byte = static_cast<uint8_t>(delimiter_value[0]);\n+\tuint8_t delimiter_first_byte;\n+\tif (!delimiter_value.empty()) {\n+\t\tdelimiter_first_byte = static_cast<uint8_t>(delimiter_value[0]);\n+\t} else {\n+\t\tdelimiter_first_byte = static_cast<uint8_t>('\\0');\n+\t}\n \tconst auto quote = static_cast<uint8_t>(state_machine_options.quote.GetValue());\n \tconst auto escape = static_cast<uint8_t>(state_machine_options.escape.GetValue());\n \tconst auto comment = static_cast<uint8_t>(state_machine_options.comment.GetValue());\n \n \tconst auto new_line_id = state_machine_options.new_line.GetValue();\n \n-\tconst bool multi_byte_delimiter = delimiter_value.size() != 1;\n+\tconst bool multi_byte_delimiter = delimiter_value.size() > 1;\n \n \tconst bool enable_unquoted_escape = state_machine_options.strict_mode.GetValue() == false &&\n \t                                    state_machine_options.quote != state_machine_options.escape &&\n@@ -149,7 +154,7 @@ void CSVStateMachineCache::Insert(const CSVStateMachineOptions &state_machine_op\n \t\ttransition_array[static_cast<uint8_t>(delimiter_value[1])]\n \t\t                [static_cast<uint8_t>(CSVState::DELIMITER_FIRST_BYTE)] = CSVState::DELIMITER;\n \t} else if (delimiter_value.size() == 3) {\n-\t\tif (delimiter_value[0] == delimiter_value[1]) {\n+\t\tif (delimiter_first_byte == delimiter_value[1]) {\n \t\t\ttransition_array[static_cast<uint8_t>(delimiter_value[1])]\n \t\t\t                [static_cast<uint8_t>(CSVState::DELIMITER_SECOND_BYTE)] = CSVState::DELIMITER_SECOND_BYTE;\n \t\t}\n@@ -158,11 +163,11 @@ void CSVStateMachineCache::Insert(const CSVStateMachineOptions &state_machine_op\n \t\ttransition_array[static_cast<uint8_t>(delimiter_value[2])]\n \t\t                [static_cast<uint8_t>(CSVState::DELIMITER_SECOND_BYTE)] = CSVState::DELIMITER;\n \t} else if (delimiter_value.size() == 4) {\n-\t\tif (delimiter_value[0] == delimiter_value[2]) {\n+\t\tif (delimiter_first_byte == delimiter_value[2]) {\n \t\t\ttransition_array[static_cast<uint8_t>(delimiter_value[1])]\n \t\t\t                [static_cast<uint8_t>(CSVState::DELIMITER_THIRD_BYTE)] = CSVState::DELIMITER_SECOND_BYTE;\n \t\t}\n-\t\tif (delimiter_value[0] == delimiter_value[1] && delimiter_value[1] == delimiter_value[2]) {\n+\t\tif (delimiter_first_byte == delimiter_value[1] && delimiter_value[1] == delimiter_value[2]) {\n \t\t\ttransition_array[static_cast<uint8_t>(delimiter_value[1])]\n \t\t\t                [static_cast<uint8_t>(CSVState::DELIMITER_THIRD_BYTE)] = CSVState::DELIMITER_THIRD_BYTE;\n \t\t}\ndiff --git a/src/execution/operator/csv_scanner/util/csv_error.cpp b/src/execution/operator/csv_scanner/util/csv_error.cpp\nindex f0d9a8cf735b..fbc1f8cd5e99 100644\n--- a/src/execution/operator/csv_scanner/util/csv_error.cpp\n+++ b/src/execution/operator/csv_scanner/util/csv_error.cpp\n@@ -20,19 +20,32 @@ CSVErrorHandler::CSVErrorHandler(bool ignore_errors_p) : ignore_errors(ignore_er\n }\n \n void CSVErrorHandler::ThrowError(const CSVError &csv_error) {\n+\tauto error_to_throw = csv_error;\n+\tidx_t error_to_throw_row = GetLineInternal(error_to_throw.error_info);\n+\tif (PrintLineNumber(error_to_throw) && !errors.empty()) {\n+\t\t// We stored a previous error here, we pick the one that happens the earliest to throw\n+\t\tfor (const auto &error : errors) {\n+\t\t\tif (CanGetLine(error.GetBoundaryIndex())) {\n+\t\t\t\tidx_t cur_error_to_throw = GetLineInternal(error.error_info);\n+\t\t\t\tif (cur_error_to_throw < error_to_throw_row) {\n+\t\t\t\t\terror_to_throw = error;\n+\t\t\t\t\terror_to_throw_row = cur_error_to_throw;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n \tstd::ostringstream error;\n-\tif (PrintLineNumber(csv_error)) {\n-\t\terror << \"CSV Error on Line: \" << GetLineInternal(csv_error.error_info) << '\\n';\n-\t\tif (!csv_error.csv_row.empty()) {\n-\t\t\terror << \"Original Line: \" << csv_error.csv_row << '\\n';\n+\tif (PrintLineNumber(error_to_throw)) {\n+\t\terror << \"CSV Error on Line: \" << error_to_throw_row << '\\n';\n+\t\tif (!error_to_throw.csv_row.empty()) {\n+\t\t\terror << \"Original Line: \" << error_to_throw.csv_row << '\\n';\n \t\t}\n \t}\n-\tif (csv_error.full_error_message.empty()) {\n-\t\terror << csv_error.error_message;\n+\tif (error_to_throw.full_error_message.empty()) {\n+\t\terror << error_to_throw.error_message;\n \t} else {\n-\t\terror << csv_error.full_error_message;\n+\t\terror << error_to_throw.full_error_message;\n \t}\n-\n \tswitch (csv_error.type) {\n \tcase CAST_ERROR:\n \t\tthrow ConversionException(error.str());\ndiff --git a/src/execution/operator/csv_scanner/util/csv_reader_options.cpp b/src/execution/operator/csv_scanner/util/csv_reader_options.cpp\nindex 8fe70d0ac1db..70fec93ab1ec 100644\n--- a/src/execution/operator/csv_scanner/util/csv_reader_options.cpp\n+++ b/src/execution/operator/csv_scanner/util/csv_reader_options.cpp\n@@ -125,9 +125,6 @@ void CSVReaderOptions::SetDelimiter(const string &input) {\n \tif (delim_str.size() > 4) {\n \t\tthrow InvalidInputException(\"The delimiter option cannot exceed a size of 4 bytes.\");\n \t}\n-\tif (input.empty()) {\n-\t\tdelim_str = string(\"\\0\", 1);\n-\t}\n \tthis->dialect_options.state_machine_options.delimiter.Set(delim_str);\n }\n \ndiff --git a/src/execution/operator/persistent/physical_export.cpp b/src/execution/operator/persistent/physical_export.cpp\nindex 66afb4c97498..45e449aab64d 100644\n--- a/src/execution/operator/persistent/physical_export.cpp\n+++ b/src/execution/operator/persistent/physical_export.cpp\n@@ -41,7 +41,7 @@ static void WriteCatalogEntries(stringstream &ss, catalog_entry_vector_t &entrie\n \t\t} catch (const NotImplementedException &) {\n \t\t\tss << entry.get().ToSQL();\n \t\t}\n-\t\tss << '\\n';\n+\t\tss << \";\\n\";\n \t}\n \tss << '\\n';\n }\ndiff --git a/src/execution/physical_plan/plan_insert.cpp b/src/execution/physical_plan/plan_insert.cpp\nindex c960fe1130aa..d4cbc48801b9 100644\n--- a/src/execution/physical_plan/plan_insert.cpp\n+++ b/src/execution/physical_plan/plan_insert.cpp\n@@ -10,7 +10,7 @@\n \n namespace duckdb {\n \n-static OrderPreservationType OrderPreservationRecursive(PhysicalOperator &op) {\n+OrderPreservationType PhysicalPlanGenerator::OrderPreservationRecursive(PhysicalOperator &op) {\n \tif (op.IsSource()) {\n \t\treturn op.SourceOrder();\n \t}\ndiff --git a/src/function/aggregate/sorted_aggregate_function.cpp b/src/function/aggregate/sorted_aggregate_function.cpp\nindex 88941c040b7d..5e3747cb462f 100644\n--- a/src/function/aggregate/sorted_aggregate_function.cpp\n+++ b/src/function/aggregate/sorted_aggregate_function.cpp\n@@ -9,7 +9,6 @@\n #include \"duckdb/planner/expression/bound_aggregate_expression.hpp\"\n #include \"duckdb/planner/expression/bound_constant_expression.hpp\"\n #include \"duckdb/parser/expression_map.hpp\"\n-#include \"duckdb/function/aggregate/distributive_functions.hpp\"\n \n namespace duckdb {\n \n@@ -770,6 +769,17 @@ void FunctionBinder::BindSortedAggregate(ClientContext &context, BoundAggregateE\n }\n \n void FunctionBinder::BindSortedAggregate(ClientContext &context, BoundWindowExpression &expr) {\n+\t//\tMake implicit orderings explicit\n+\tauto &aggregate = *expr.aggregate;\n+\tif (aggregate.order_dependent == AggregateOrderDependent::ORDER_DEPENDENT && expr.arg_orders.empty()) {\n+\t\tfor (auto &order : expr.orders) {\n+\t\t\tconst auto type = order.type;\n+\t\t\tconst auto null_order = order.null_order;\n+\t\t\tauto expression = order.expression->Copy();\n+\t\t\texpr.arg_orders.emplace_back(BoundOrderByNode(type, null_order, std::move(expression)));\n+\t\t}\n+\t}\n+\n \tif (expr.arg_orders.empty() || expr.children.empty()) {\n \t\t// not a sorted aggregate: return\n \t\treturn;\n@@ -781,7 +791,6 @@ void FunctionBinder::BindSortedAggregate(ClientContext &context, BoundWindowExpr\n \t\t\treturn;\n \t\t}\n \t}\n-\tauto &aggregate = *expr.aggregate;\n \tauto &children = expr.children;\n \tauto &arg_orders = expr.arg_orders;\n \tauto sorted_bind = make_uniq<SortedAggregateBindData>(context, expr);\ndiff --git a/src/function/window/window_index_tree.cpp b/src/function/window/window_index_tree.cpp\nindex 5791b2af747f..48fb5b1bef0a 100644\n--- a/src/function/window/window_index_tree.cpp\n+++ b/src/function/window/window_index_tree.cpp\n@@ -1,6 +1,5 @@\n #include \"duckdb/function/window/window_index_tree.hpp\"\n \n-#include <thread>\n #include <utility>\n \n namespace duckdb {\n@@ -52,11 +51,21 @@ void WindowIndexTreeLocalState::BuildLeaves() {\n \t}\n }\n \n-idx_t WindowIndexTree::SelectNth(const SubFrames &frames, idx_t n) const {\n+pair<idx_t, idx_t> WindowIndexTree::SelectNth(const SubFrames &frames, idx_t n) const {\n \tif (mst32) {\n-\t\treturn mst32->NthElement(mst32->SelectNth(frames, n));\n+\t\tconst auto nth = mst32->SelectNth(frames, n);\n+\t\tif (nth.second) {\n+\t\t\treturn nth;\n+\t\t} else {\n+\t\t\treturn {mst32->NthElement(nth.first), 0};\n+\t\t}\n \t} else {\n-\t\treturn mst64->NthElement(mst64->SelectNth(frames, n));\n+\t\tconst auto nth = mst64->SelectNth(frames, n);\n+\t\tif (nth.second) {\n+\t\t\treturn nth;\n+\t\t} else {\n+\t\t\treturn {mst64->NthElement(nth.first), 0};\n+\t\t}\n \t}\n }\n \ndiff --git a/src/function/window/window_value_function.cpp b/src/function/window/window_value_function.cpp\nindex 6b8a7038ebb3..fbd1551a2e50 100644\n--- a/src/function/window/window_value_function.cpp\n+++ b/src/function/window/window_value_function.cpp\n@@ -311,7 +311,12 @@ void WindowLeadLagExecutor::EvaluateInternal(WindowExecutorGlobalState &gstate,\n \t\t\t\tconst auto n = NumericCast<idx_t>(val_idx);\n \t\t\t\tconst auto nth_index = glstate.value_tree->SelectNth(frames, n);\n \t\t\t\t// (4) evaluate the expression provided to LEAD/LAG on this row.\n-\t\t\t\tcursor.CopyCell(0, nth_index, result, i);\n+\t\t\t\tif (nth_index.second) {\n+\t\t\t\t\t//\tOverflow\n+\t\t\t\t\tFlatVector::SetNull(result, i, true);\n+\t\t\t\t} else {\n+\t\t\t\t\tcursor.CopyCell(0, nth_index.first, result, i);\n+\t\t\t\t}\n \t\t\t} else if (wexpr.default_expr) {\n \t\t\t\tleadlag_default.CopyCell(result, i);\n \t\t\t} else {\n@@ -425,7 +430,8 @@ void WindowFirstValueExecutor::EvaluateInternal(WindowExecutorGlobalState &gstat\n \n \t\t\tif (frame_width) {\n \t\t\t\tconst auto first_idx = gvstate.value_tree->SelectNth(frames, 0);\n-\t\t\t\tcursor.CopyCell(0, first_idx, result, i);\n+\t\t\t\tD_ASSERT(first_idx.second == 0);\n+\t\t\t\tcursor.CopyCell(0, first_idx.first, result, i);\n \t\t\t} else {\n \t\t\t\tFlatVector::SetNull(result, i, true);\n \t\t\t}\n@@ -474,8 +480,19 @@ void WindowLastValueExecutor::EvaluateInternal(WindowExecutorGlobalState &gstate\n \t\t\t}\n \n \t\t\tif (frame_width) {\n-\t\t\t\tconst auto last_idx = gvstate.value_tree->SelectNth(frames, frame_width - 1);\n-\t\t\t\tcursor.CopyCell(0, last_idx, result, i);\n+\t\t\t\tauto n = frame_width - 1;\n+\t\t\t\tauto last_idx = gvstate.value_tree->SelectNth(frames, n);\n+\t\t\t\tif (last_idx.second && last_idx.second <= n) {\n+\t\t\t\t\t//\tFrame larger than data. Since we want last, we back off by the overflow\n+\t\t\t\t\tn -= last_idx.second;\n+\t\t\t\t\tlast_idx = gvstate.value_tree->SelectNth(frames, n);\n+\t\t\t\t}\n+\t\t\t\tif (last_idx.second) {\n+\t\t\t\t\t//\tNo last value - give up.\n+\t\t\t\t\tFlatVector::SetNull(result, i, true);\n+\t\t\t\t} else {\n+\t\t\t\t\tcursor.CopyCell(0, last_idx.first, result, i);\n+\t\t\t\t}\n \t\t\t} else {\n \t\t\t\tFlatVector::SetNull(result, i, true);\n \t\t\t}\n@@ -541,7 +558,12 @@ void WindowNthValueExecutor::EvaluateInternal(WindowExecutorGlobalState &gstate,\n \n \t\t\tif (n < frame_width) {\n \t\t\t\tconst auto nth_index = gvstate.value_tree->SelectNth(frames, n - 1);\n-\t\t\t\tcursor.CopyCell(0, nth_index, result, i);\n+\t\t\t\tif (nth_index.second) {\n+\t\t\t\t\t// Past end of frame\n+\t\t\t\t\tFlatVector::SetNull(result, i, true);\n+\t\t\t\t} else {\n+\t\t\t\t\tcursor.CopyCell(0, nth_index.first, result, i);\n+\t\t\t\t}\n \t\t\t} else {\n \t\t\t\tFlatVector::SetNull(result, i, true);\n \t\t\t}\ndiff --git a/src/include/duckdb/execution/merge_sort_tree.hpp b/src/include/duckdb/execution/merge_sort_tree.hpp\nindex 672aaa56cd4a..8c04ecde03cf 100644\n--- a/src/include/duckdb/execution/merge_sort_tree.hpp\n+++ b/src/include/duckdb/execution/merge_sort_tree.hpp\n@@ -118,7 +118,8 @@ struct MergeSortTree {\n \n \tvoid Build();\n \n-\tidx_t SelectNth(const SubFrames &frames, idx_t n) const;\n+\t//\t{nth index, remainder}\n+\tpair<idx_t, idx_t> SelectNth(const SubFrames &frames, idx_t n) const;\n \n \tinline ElementType NthElement(idx_t i) const {\n \t\treturn tree.front().first[i];\n@@ -436,10 +437,10 @@ void MergeSortTree<E, O, CMP, F, C>::BuildRun(idx_t level_idx, idx_t run_idx) {\n }\n \n template <typename E, typename O, typename CMP, uint64_t F, uint64_t C>\n-idx_t MergeSortTree<E, O, CMP, F, C>::SelectNth(const SubFrames &frames, idx_t n) const {\n+pair<idx_t, idx_t> MergeSortTree<E, O, CMP, F, C>::SelectNth(const SubFrames &frames, idx_t n) const {\n \t// Handle special case of a one-element tree\n \tif (tree.size() < 2) {\n-\t\treturn 0;\n+\t\treturn {0, 0};\n \t}\n \n \t// \tThe first level contains a single run,\n@@ -566,7 +567,7 @@ idx_t MergeSortTree<E, O, CMP, F, C>::SelectNth(const SubFrames &frames, idx_t n\n \t\t}\n \t}\n \n-\treturn result;\n+\treturn {result, n};\n }\n \n template <typename E, typename O, typename CMP, uint64_t F, uint64_t C>\ndiff --git a/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp b/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp\nindex a18a0eb5f0bd..dc73e17fc32c 100644\n--- a/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp\n+++ b/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp\n@@ -195,6 +195,8 @@ class StringValueResult : public ScannerResult {\n \tbool projecting_columns = false;\n \tidx_t chunk_col_id = 0;\n \n+\tbool icu_loaded = false;\n+\n \t//! We must ensure that we keep the buffers alive until processing the query result\n \tunordered_map<idx_t, shared_ptr<CSVBufferHandle>> buffer_handles;\n \ndiff --git a/src/include/duckdb/execution/physical_plan_generator.hpp b/src/include/duckdb/execution/physical_plan_generator.hpp\nindex ebd172492bc5..ee635c16adcd 100644\n--- a/src/include/duckdb/execution/physical_plan_generator.hpp\n+++ b/src/include/duckdb/execution/physical_plan_generator.hpp\n@@ -74,6 +74,8 @@ class PhysicalPlanGenerator {\n \tstatic bool UseBatchIndex(ClientContext &context, PhysicalOperator &plan);\n \t//! Whether or not we should preserve insertion order for executing the given sink\n \tstatic bool PreserveInsertionOrder(ClientContext &context, PhysicalOperator &plan);\n+\t//! The order preservation type of the given operator decided by recursively looking at its children\n+\tstatic OrderPreservationType OrderPreservationRecursive(PhysicalOperator &op);\n \n \ttemplate <class T, class... ARGS>\n \tPhysicalOperator &Make(ARGS &&... args) {\ndiff --git a/src/include/duckdb/function/window/window_index_tree.hpp b/src/include/duckdb/function/window/window_index_tree.hpp\nindex e9f9f4014188..e95b522747d6 100644\n--- a/src/include/duckdb/function/window/window_index_tree.hpp\n+++ b/src/include/duckdb/function/window/window_index_tree.hpp\n@@ -36,7 +36,8 @@ class WindowIndexTree : public WindowMergeSortTree {\n \tunique_ptr<WindowAggregatorState> GetLocalState() override;\n \n \t//! Find the Nth index in the set of subframes\n-\tidx_t SelectNth(const SubFrames &frames, idx_t n) const;\n+\t//! Returns {nth index, 0} or {nth offset, overflow}\n+\tpair<idx_t, idx_t> SelectNth(const SubFrames &frames, idx_t n) const;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/main/extension_entries.hpp b/src/include/duckdb/main/extension_entries.hpp\nindex 89108ef34bca..6f86738a66a6 100644\n--- a/src/include/duckdb/main/extension_entries.hpp\n+++ b/src/include/duckdb/main/extension_entries.hpp\n@@ -478,6 +478,7 @@ static constexpr ExtensionFunctionEntry EXTENSION_FUNCTIONS[] = {\n     {\"quantile_disc\", \"core_functions\", CatalogType::AGGREGATE_FUNCTION_ENTRY},\n     {\"radians\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n     {\"random\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n+    {\"read_avro\", \"avro\", CatalogType::TABLE_FUNCTION_ENTRY},\n     {\"read_json\", \"json\", CatalogType::TABLE_FUNCTION_ENTRY},\n     {\"read_json_auto\", \"json\", CatalogType::TABLE_FUNCTION_ENTRY},\n     {\"read_json_objects\", \"json\", CatalogType::TABLE_FUNCTION_ENTRY},\n@@ -1086,16 +1087,28 @@ static constexpr ExtensionEntry EXTENSION_SECRET_PROVIDERS[] = {\n     {\"mysql/config\", \"mysql_scanner\"},\n     {\"postgres/config\", \"postgres_scanner\"}}; // EXTENSION_SECRET_PROVIDERS\n \n-static constexpr const char *AUTOLOADABLE_EXTENSIONS[] = {\"aws\",          \"azure\",\n-                                                          \"autocomplete\", \"core_functions\",\n-                                                          \"delta\",        \"excel\",\n-                                                          \"fts\",          \"httpfs\",\n-                                                          \"iceberg\",      \"inet\",\n-                                                          \"icu\",          \"json\",\n-                                                          \"motherduck\",   \"mysql_scanner\",\n-                                                          \"parquet\",      \"sqlite_scanner\",\n-                                                          \"sqlsmith\",     \"postgres_scanner\",\n-                                                          \"tpcds\",        \"tpch\",\n-                                                          \"uc_catalog\",   \"ui\"}; // END_OF_AUTOLOADABLE_EXTENSIONS\n+static constexpr const char *AUTOLOADABLE_EXTENSIONS[] = {\"avro\",\n+                                                          \"aws\",\n+                                                          \"azure\",\n+                                                          \"autocomplete\",\n+                                                          \"core_functions\",\n+                                                          \"delta\",\n+                                                          \"excel\",\n+                                                          \"fts\",\n+                                                          \"httpfs\",\n+                                                          \"iceberg\",\n+                                                          \"inet\",\n+                                                          \"icu\",\n+                                                          \"json\",\n+                                                          \"motherduck\",\n+                                                          \"mysql_scanner\",\n+                                                          \"parquet\",\n+                                                          \"sqlite_scanner\",\n+                                                          \"sqlsmith\",\n+                                                          \"postgres_scanner\",\n+                                                          \"tpcds\",\n+                                                          \"tpch\",\n+                                                          \"uc_catalog\",\n+                                                          \"ui\"}; // END_OF_AUTOLOADABLE_EXTENSIONS\n \n } // namespace duckdb\ndiff --git a/src/parser/parsed_data/create_type_info.cpp b/src/parser/parsed_data/create_type_info.cpp\nindex 1ce0327c3c31..4ee92f525a3f 100644\n--- a/src/parser/parsed_data/create_type_info.cpp\n+++ b/src/parser/parsed_data/create_type_info.cpp\n@@ -61,6 +61,7 @@ string CreateTypeInfo::ToString() const {\n \t\tresult += \" AS \";\n \t\tresult += type.ToString();\n \t}\n+\tresult += \";\";\n \treturn result;\n }\n \ndiff --git a/src/storage/statistics/numeric_stats.cpp b/src/storage/statistics/numeric_stats.cpp\nindex a9379812292e..4283ea78988e 100644\n--- a/src/storage/statistics/numeric_stats.cpp\n+++ b/src/storage/statistics/numeric_stats.cpp\n@@ -147,6 +147,7 @@ FilterPropagateResult CheckZonemapTemplated(const BaseStatistics &stats, Express\n                                             T max_value, T constant) {\n \tswitch (comparison_type) {\n \tcase ExpressionType::COMPARE_EQUAL:\n+\tcase ExpressionType::COMPARE_NOT_DISTINCT_FROM:\n \t\tif (ConstantExactRange(min_value, max_value, constant)) {\n \t\t\treturn FilterPropagateResult::FILTER_ALWAYS_TRUE;\n \t\t}\n@@ -155,6 +156,7 @@ FilterPropagateResult CheckZonemapTemplated(const BaseStatistics &stats, Express\n \t\t}\n \t\treturn FilterPropagateResult::FILTER_ALWAYS_FALSE;\n \tcase ExpressionType::COMPARE_NOTEQUAL:\n+\tcase ExpressionType::COMPARE_DISTINCT_FROM:\n \t\tif (!ConstantValueInRange(min_value, max_value, constant)) {\n \t\t\treturn FilterPropagateResult::FILTER_ALWAYS_TRUE;\n \t\t} else if (ConstantExactRange(min_value, max_value, constant)) {\ndiff --git a/src/storage/statistics/string_stats.cpp b/src/storage/statistics/string_stats.cpp\nindex 62262d448349..230944ab1446 100644\n--- a/src/storage/statistics/string_stats.cpp\n+++ b/src/storage/statistics/string_stats.cpp\n@@ -216,12 +216,14 @@ FilterPropagateResult StringStats::CheckZonemap(const_data_ptr_t min_data, idx_t\n \tint max_comp = StringValueComparison(data, MinValue(max_len, size), max_data);\n \tswitch (comparison_type) {\n \tcase ExpressionType::COMPARE_EQUAL:\n+\tcase ExpressionType::COMPARE_NOT_DISTINCT_FROM:\n \t\tif (min_comp >= 0 && max_comp <= 0) {\n \t\t\treturn FilterPropagateResult::NO_PRUNING_POSSIBLE;\n \t\t} else {\n \t\t\treturn FilterPropagateResult::FILTER_ALWAYS_FALSE;\n \t\t}\n \tcase ExpressionType::COMPARE_NOTEQUAL:\n+\tcase ExpressionType::COMPARE_DISTINCT_FROM:\n \t\tif (min_comp < 0 || max_comp > 0) {\n \t\t\treturn FilterPropagateResult::FILTER_ALWAYS_TRUE;\n \t\t}\ndiff --git a/third_party/fsst/fsst.h b/third_party/fsst/fsst.h\nindex 8e143db8782a..86909be5b5f2 100644\n--- a/third_party/fsst/fsst.h\n+++ b/third_party/fsst/fsst.h\n@@ -196,7 +196,7 @@ duckdb_fsst_decompress(\n          }\n       }\n    }\n-   if (posOut+24 <= size) { // handle the possibly 3 last bytes without a loop\n+   if (posOut+32 <= size) { // handle the possibly 3 last bytes without a loop\n       if (posIn+2 <= lenIn) { \n \t strOut[posOut] = strIn[posIn+1]; \n          if (strIn[posIn] != FSST_ESC) {\ndiff --git a/tools/shell/shell.cpp b/tools/shell/shell.cpp\nindex 72c7cf55f020..ba31af324940 100644\n--- a/tools/shell/shell.cpp\n+++ b/tools/shell/shell.cpp\n@@ -1736,11 +1736,7 @@ void ShellState::ExecutePreparedStatement(sqlite3_stmt *pStmt) {\n \t\t/* extract the data and data types */\n \t\tfor (int i = 0; i < nCol; i++) {\n \t\t\tresult.types[i] = sqlite3_column_type(pStmt, i);\n-\t\t\tif (result.types[i] == SQLITE_BLOB && cMode == RenderMode::INSERT) {\n-\t\t\t\tresult.data[i] = \"\";\n-\t\t\t} else {\n-\t\t\t\tresult.data[i] = (const char *)sqlite3_column_text(pStmt, i);\n-\t\t\t}\n+\t\t\tresult.data[i] = (const char *)sqlite3_column_text(pStmt, i);\n \t\t\tif (!result.data[i] && result.types[i] != SQLITE_NULL) {\n \t\t\t\t// OOM\n \t\t\t\trc = SQLITE_NOMEM;\n",
  "test_patch": "diff --git a/test/optimizer/pushdown/distinct_from_pushdown.test b/test/optimizer/pushdown/distinct_from_pushdown.test\nnew file mode 100644\nindex 000000000000..fa50f3094948\n--- /dev/null\n+++ b/test/optimizer/pushdown/distinct_from_pushdown.test\n@@ -0,0 +1,27 @@\n+# name: test/optimizer/pushdown/distinct_from_pushdown.test\n+# description: Test DISTINCT FROM pushed down into scans\n+# group: [pushdown]\n+\n+statement ok\n+create table test as select 'tst' as tst;\n+\n+query I\n+select * from test where tst is not distinct from 'a' or tst is not distinct from 'b';\n+----\n+\n+query I\n+select * from test where tst is distinct from 'a' or tst is distinct from 'b';\n+----\n+tst\n+\n+statement ok\n+create table test2 as select 42 as tst;\n+\n+query I\n+select * from test2 where tst is not distinct from 12 or tst is not distinct from 13;\n+----\n+\n+query I\n+select * from test2 where tst is distinct from 12 or tst is distinct from 13\n+----\n+42\ndiff --git a/test/sql/copy/csv/afl/test_fuzz_4086.test b/test/sql/copy/csv/afl/test_fuzz_4086.test\nnew file mode 100644\nindex 000000000000..08d2d36a80aa\n--- /dev/null\n+++ b/test/sql/copy/csv/afl/test_fuzz_4086.test\n@@ -0,0 +1,20 @@\n+# name: test/sql/copy/csv/afl/test_fuzz_4086.test\n+# description: fuzzer generated csv files - should not raise internal exception (by failed assertion).\n+# group: [afl]\n+\n+require json\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/4086/case_1.csv', auto_detect=false, columns={'json': 'JSON'}, delim=NULL, buffer_size=42, store_rejects=true, rejects_limit=658694493994253607);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/4086/case_2.csv', auto_detect=false, columns={'json': 'JSON'}, delim=NULL, buffer_size=42, store_rejects=true, rejects_limit=658694493994253607);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/4086/case_3.csv', auto_detect=false, columns={'json': 'JSON'}, delim='\\0', buffer_size=42, store_rejects=true, rejects_limit=658694493994253607);\n+----\ndiff --git a/test/sql/window/test_value_orderby.test b/test/sql/window/test_value_orderby.test\nindex 8b51198f277c..8fbb3e9e4cd1 100644\n--- a/test/sql/window/test_value_orderby.test\n+++ b/test/sql/window/test_value_orderby.test\n@@ -29,3 +29,19 @@ ORDER BY 2\n 9\t8\t9\t1\t7\n 6\t9\t9\t1\t6\n 3\t10\t9\t1\t6\n+\n+# Frame larger than data\n+query I\n+with IDS as (\n+    select * as idx from generate_series(1,4)\n+),DATA as (\n+    select *, (case when idx != 3 then idx * 1.0 else NULL end) as value from IDS\n+)\n+SELECT \n+ last(value ORDER BY idx IGNORE NULLS) OVER (ORDER BY idx ROWS BETWEEN UNBOUNDED PRECEDING AND 0 FOLLOWING)\n+FROM DATA\n+----\n+1.0\n+2.0\n+2.0\n+4.0\ndiff --git a/test/sql/window/test_window_constant_aggregate.test b/test/sql/window/test_window_constant_aggregate.test\nindex 6aafb23d0742..f16d221152b9 100644\n--- a/test/sql/window/test_window_constant_aggregate.test\n+++ b/test/sql/window/test_window_constant_aggregate.test\n@@ -205,7 +205,7 @@ ORDER BY ALL\n statement ok\n pragma threads=2\n \n-loop i 0 100\n+loop i 0 20\n \n query III\n with table_1 AS (\n@@ -285,3 +285,27 @@ fb30cf47-6f6b-42ef-dec2-3f984479a2aa\t2024-04-01 00:00:00\t12\n 7d1cc557-2d45-6900-a1ed-b2c64f5d9200\t2024-02-01 00:00:00\tNULL\n \n endloop\n+\n+# Test implicit ordering for aggregates\n+loop i 0 20\n+\n+query I\n+with repro2 AS (\n+\tSELECT range // 59 AS id, random() AS value\n+\tFROM range(1475)\n+), X AS (\n+\tSELECT\n+\t\tlist(value) OVER (\n+\t\t\tPARTITION BY id \n+\t\t\tORDER BY value \n+\t\t\tROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n+\t\t\t) AS values\n+\tFROM repro2\n+)\n+select count(*) \n+from X \n+where values[1] != list_aggregate(values, 'min')\n+----\n+0\n+\n+endloop\ndiff --git a/tools/shell/tests/test_shell_basics.py b/tools/shell/tests/test_shell_basics.py\nindex e2c554bb5a2d..1523ac2ccef2 100644\n--- a/tools/shell/tests/test_shell_basics.py\n+++ b/tools/shell/tests/test_shell_basics.py\n@@ -834,6 +834,17 @@ def test_dump_mixed(shell):\n     result = test.run()\n     result.check_stdout('CREATE TABLE a(d DATE, k FLOAT, t TIMESTAMP);')\n \n+def test_dump_blobs(shell):\n+    test = (\n+        ShellTest(shell)\n+        .statement(\"create table test(t VARCHAR, b BLOB);\")\n+        .statement(\".changes off\")\n+        .statement(\"insert into test values('literal blob', '\\\\x07\\\\x08\\\\x09');\")\n+        .statement(\".dump\")\n+    )\n+    result = test.run()\n+    result.check_stdout(\"'\\\\x07\\\\x08\\\\x09'\")\n+\n def test_invalid_csv(shell, tmp_path):\n     file = tmp_path / 'nonsencsv.csv'\n     with open(file, 'wb+') as f:\n@@ -869,18 +880,6 @@ def test_mode_trash(shell):\n     result = test.run()\n     result.check_stdout('')\n \n-@pytest.mark.skip(reason=\"Broken test, ported directly, was commented out\")\n-def test_dump_blobs(shell):\n-    test = (\n-        ShellTest(shell)\n-        .statement(\"CREATE TABLE a (b BLOB);\")\n-        .statement(\".changes off\")\n-        .statement(\"INSERT INTO a VALUES (DATE '1992-01-01', 0.3, NOW());\")\n-        .statement(\".dump\")\n-    )\n-    result = test.run()\n-    result.check_stdout('COMMIT')\n-\n def test_sqlite_comments(shell):\n     # Using /* <comment> */\n     test = (\n",
  "problem_statement": "[CLI] Empty strings for BLOB values in .mode insert / .dump\n### What happens?\n\nFor tables (or queries) with BLOB columns, `.dump` (or `.mode insert`) only produce empty strings instead of some kind \"literal BLOB value\".\n\nIn duckdb-cli 1.2.0 and 1.2.1.  \nAlso checked with 1.1.3, there the BLOBs are emitted as sqlite style BLOB literal ( `X'01F32B'` ) which is just wrong (as it is somewhat strangly interpreded as string with a prefixed X `'x01F32B'` - parser glitch ?)\n\nIt would make sense to emit these as eg `'\\x01\\xF3\\x2B'::BLOB`.\n\n### To Reproduce\n\n#### code\n```\ncreate table test(t VARCHAR, b BLOB);\ninsert into test values('literal blob', '\\x07\\x08\\x09');\ninsert into test values('text-as-blob', 'ABC'::BLOB);\ninsert into test values('unhex', unhex('040506'));\n\nselect * from test;\n\n.mode insert TEST\nselect * from test;\n\n.dump\n```\n\n#### results on 1.2.0/1.2.1 / BLOBs missing\n```\n-- select * from test;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      t       \u2502      b       \u2502\n\u2502   varchar    \u2502     blob     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 literal blob \u2502 \\x07\\x08\\x09 \u2502\n\u2502 text-as-blob \u2502 ABC          \u2502\n\u2502 unhex        \u2502 \\x04\\x05\\x06 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n-- .mode insert TEST\n-- select * from test;\nINSERT INTO TEST(t,b) VALUES('literal blob','');\nINSERT INTO TEST(t,b) VALUES('text-as-blob','');\nINSERT INTO TEST(t,b) VALUES('unhex','');\n\n-- .dump\nBEGIN TRANSACTION;\nCREATE TABLE test(t VARCHAR, b BLOB);;\nINSERT INTO test VALUES('literal blob','');\nINSERT INTO test VALUES('text-as-blob','');\nINSERT INTO test VALUES('unhex','');\nCOMMIT;\n```\n\n#### results on 1.1.3 / BLOBs as invalid (sqlite-style) literals\n```\n-- select * from test;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      t       \u2502      b       \u2502\n\u2502   varchar    \u2502     blob     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 literal blob \u2502 \\x07\\x08\\x09 \u2502\n\u2502 text-as-blob \u2502 ABC          \u2502\n\u2502 unhex        \u2502 \\x04\\x05\\x06 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n-- .mode insert TEST\n-- select * from test;\nINSERT INTO TEST(t,b) VALUES('literal blob',X'070809');\nINSERT INTO TEST(t,b) VALUES('text-as-blob',X'414243');\nINSERT INTO TEST(t,b) VALUES('unhex',X'040506');\n\n-- .dump\nPRAGMA foreign_keys=OFF;\nBEGIN TRANSACTION;\nCREATE TABLE test(t VARCHAR, b BLOB);;\nINSERT INTO test VALUES('literal blob',X'070809');\nINSERT INTO test VALUES('text-as-blob',X'414243');\nINSERT INTO test VALUES('unhex',X'040506');\nCOMMIT;\n```\n\n### OS:\n\nLinux (Ubuntu 24.04), x86_x64\n\n### DuckDB Version:\n\n1.2.1\n\n### DuckDB Client:\n\nCLI\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nMarc Gerber\n\n### Affiliation:\n\nprivate\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [ ] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [x] Yes, I have\n[CLI] Empty strings for BLOB values in .mode insert / .dump\n### What happens?\n\nFor tables (or queries) with BLOB columns, `.dump` (or `.mode insert`) only produce empty strings instead of some kind \"literal BLOB value\".\n\nIn duckdb-cli 1.2.0 and 1.2.1.  \nAlso checked with 1.1.3, there the BLOBs are emitted as sqlite style BLOB literal ( `X'01F32B'` ) which is just wrong (as it is somewhat strangly interpreded as string with a prefixed X `'x01F32B'` - parser glitch ?)\n\nIt would make sense to emit these as eg `'\\x01\\xF3\\x2B'::BLOB`.\n\n### To Reproduce\n\n#### code\n```\ncreate table test(t VARCHAR, b BLOB);\ninsert into test values('literal blob', '\\x07\\x08\\x09');\ninsert into test values('text-as-blob', 'ABC'::BLOB);\ninsert into test values('unhex', unhex('040506'));\n\nselect * from test;\n\n.mode insert TEST\nselect * from test;\n\n.dump\n```\n\n#### results on 1.2.0/1.2.1 / BLOBs missing\n```\n-- select * from test;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      t       \u2502      b       \u2502\n\u2502   varchar    \u2502     blob     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 literal blob \u2502 \\x07\\x08\\x09 \u2502\n\u2502 text-as-blob \u2502 ABC          \u2502\n\u2502 unhex        \u2502 \\x04\\x05\\x06 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n-- .mode insert TEST\n-- select * from test;\nINSERT INTO TEST(t,b) VALUES('literal blob','');\nINSERT INTO TEST(t,b) VALUES('text-as-blob','');\nINSERT INTO TEST(t,b) VALUES('unhex','');\n\n-- .dump\nBEGIN TRANSACTION;\nCREATE TABLE test(t VARCHAR, b BLOB);;\nINSERT INTO test VALUES('literal blob','');\nINSERT INTO test VALUES('text-as-blob','');\nINSERT INTO test VALUES('unhex','');\nCOMMIT;\n```\n\n#### results on 1.1.3 / BLOBs as invalid (sqlite-style) literals\n```\n-- select * from test;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      t       \u2502      b       \u2502\n\u2502   varchar    \u2502     blob     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 literal blob \u2502 \\x07\\x08\\x09 \u2502\n\u2502 text-as-blob \u2502 ABC          \u2502\n\u2502 unhex        \u2502 \\x04\\x05\\x06 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n-- .mode insert TEST\n-- select * from test;\nINSERT INTO TEST(t,b) VALUES('literal blob',X'070809');\nINSERT INTO TEST(t,b) VALUES('text-as-blob',X'414243');\nINSERT INTO TEST(t,b) VALUES('unhex',X'040506');\n\n-- .dump\nPRAGMA foreign_keys=OFF;\nBEGIN TRANSACTION;\nCREATE TABLE test(t VARCHAR, b BLOB);;\nINSERT INTO test VALUES('literal blob',X'070809');\nINSERT INTO test VALUES('text-as-blob',X'414243');\nINSERT INTO test VALUES('unhex',X'040506');\nCOMMIT;\n```\n\n### OS:\n\nLinux (Ubuntu 24.04), x86_x64\n\n### DuckDB Version:\n\n1.2.1\n\n### DuckDB Client:\n\nCLI\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nMarc Gerber\n\n### Affiliation:\n\nprivate\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [ ] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [x] Yes, I have\nEXPORT DATABASE containing types misses semicolon\n### What happens?\n\nHi and thanks for this amazing project.\n\nSeems like exporting databases containing type declarations is currently broken.\n\n### To Reproduce\n\nTo reproduce the issue:\n\n```\ncreate type one as text;\ncreate type two as text;\nexport database 'typeissue';\n```\n\nThen in a new DB:\n\n```\nimport database 'typeissue';\n```\n\nThis results in:\n\n\n```\nParser Error:\nsyntax error at or near \"CREATE\"\n\nLINE 1: import database 'typeissue';\n```\n\nSemicolons are missing in `schema.sql`. Contents:\n\n```\nCREATE TYPE one AS VARCHAR\nCREATE TYPE two AS VARCHAR\n```\n\n### OS:\n\nx86_64\n\n### DuckDB Version:\n\n`v1.2.0 5f5512b827`\n\n### DuckDB Client:\n\nCLI\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nJorin Vogel\n\n### Affiliation:\n\ntaleshape.com\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [x] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [x] Yes, I have\n",
  "hints_text": "\n\n",
  "created_at": "2025-03-20T11:48:17Z"
}