{
  "repo": "duckdb/duckdb",
  "pull_number": 10826,
  "instance_id": "duckdb__duckdb-10826",
  "issue_numbers": [
    "8623"
  ],
  "base_commit": "6634735e57f65c4aaeddb5c93f7c56f3739f50c0",
  "patch": "diff --git a/CMakeLists.txt b/CMakeLists.txt\nindex cf19696d563e..fd523bff42f7 100644\n--- a/CMakeLists.txt\n+++ b/CMakeLists.txt\n@@ -1118,9 +1118,9 @@ if(BUILD_PYTHON)\n \t)\n \n   if(PYTHON_EDITABLE_BUILD)\n-    set(PIP_COMMAND ${PIP_COMMAND} pip install --editable .)\n+    set(PIP_COMMAND ${PIP_COMMAND} python3 -m pip install --editable .)\n   else()\n-    set(PIP_COMMAND ${PIP_COMMAND} pip install .)\n+    set(PIP_COMMAND ${PIP_COMMAND} python3 -m pip install .)\n   endif()\n \n   if(USER_SPACE)\ndiff --git a/tools/pythonpkg/duckdb-stubs/__init__.pyi b/tools/pythonpkg/duckdb-stubs/__init__.pyi\nindex 891a57717b08..7198a9889884 100644\n--- a/tools/pythonpkg/duckdb-stubs/__init__.pyi\n+++ b/tools/pythonpkg/duckdb-stubs/__init__.pyi\n@@ -448,6 +448,7 @@ class DuckDBPyRelation:\n             file_name: str,\n             compression: Optional[str]\n     ) -> None: ...\n+    def fetch_df_chunk(self, *args, **kwargs) -> pandas.DataFrame: ...\n     def to_table(self, table_name: str) -> None: ...\n     def to_view(self, view_name: str, replace: bool = ...) -> DuckDBPyRelation: ...\n     def torch(self, connection: DuckDBPyConnection = ...) -> dict: ...\ndiff --git a/tools/pythonpkg/scripts/cache_data.json b/tools/pythonpkg/scripts/cache_data.json\nindex b7f57409d67d..32a13a6e4bbd 100644\n--- a/tools/pythonpkg/scripts/cache_data.json\n+++ b/tools/pythonpkg/scripts/cache_data.json\n@@ -214,7 +214,8 @@\n         \"full_path\": \"numpy\",\n         \"name\": \"numpy\",\n         \"children\": [\n-\t\t\t\"numpy.core\",\n+            \"numpy.core\",\n+            \"numpy.ma\",\n             \"numpy.ndarray\",\n             \"numpy.datetime64\",\n             \"numpy.generic\",\n@@ -253,6 +254,20 @@\n         \"name\": \"multiarray\",\n         \"children\": []\n     },\n+    \"numpy.ma\": {\n+        \"type\": \"attribute\",\n+        \"full_path\": \"numpy.ma\",\n+        \"name\": \"ma\",\n+        \"children\": [\n+            \"numpy.ma.masked\"\n+        ]\n+    },\n+    \"numpy.ma.masked\": {\n+        \"type\": \"attribute\",\n+        \"full_path\": \"numpy.ma.masked\",\n+        \"name\": \"masked\",\n+        \"children\": []\n+    },\n     \"numpy.ndarray\": {\n         \"type\": \"attribute\",\n         \"full_path\": \"numpy.ndarray\",\ndiff --git a/tools/pythonpkg/scripts/imports.py b/tools/pythonpkg/scripts/imports.py\nindex 13b5f21aacf5..cfac8266bcac 100644\n--- a/tools/pythonpkg/scripts/imports.py\n+++ b/tools/pythonpkg/scripts/imports.py\n@@ -43,6 +43,7 @@\n import numpy\n \n numpy.core.multiarray\n+numpy.ma.masked\n numpy.ndarray\n numpy.datetime64\n numpy.generic\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/numpy_module.hpp b/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/numpy_module.hpp\nindex 733df11fae80..a4a75ecd84dc 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/numpy_module.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/numpy_module.hpp\n@@ -13,6 +13,18 @@\n \n namespace duckdb {\n \n+struct NumpyMaCacheItem : public PythonImportCacheItem {\n+\n+public:\n+\tNumpyMaCacheItem(optional_ptr<PythonImportCacheItem> parent)\n+\t    : PythonImportCacheItem(\"ma\", parent), masked(\"masked\", this) {\n+\t}\n+\t~NumpyMaCacheItem() override {\n+\t}\n+\n+\tPythonImportCacheItem masked;\n+};\n+\n struct NumpyCoreCacheItem : public PythonImportCacheItem {\n \n public:\n@@ -32,9 +44,9 @@ struct NumpyCacheItem : public PythonImportCacheItem {\n \n public:\n \tNumpyCacheItem()\n-\t    : PythonImportCacheItem(\"numpy\"), core(this), ndarray(\"ndarray\", this), datetime64(\"datetime64\", this),\n-\t      generic(\"generic\", this), int64(\"int64\", this), bool_(\"bool_\", this), byte(\"byte\", this),\n-\t      ubyte(\"ubyte\", this), short_(\"short\", this), ushort_(\"ushort\", this), intc(\"intc\", this),\n+\t    : PythonImportCacheItem(\"numpy\"), core(this), ma(this), ndarray(\"ndarray\", this),\n+\t      datetime64(\"datetime64\", this), generic(\"generic\", this), int64(\"int64\", this), bool_(\"bool_\", this),\n+\t      byte(\"byte\", this), ubyte(\"ubyte\", this), short_(\"short\", this), ushort_(\"ushort\", this), intc(\"intc\", this),\n \t      uintc(\"uintc\", this), int_(\"int_\", this), uint(\"uint\", this), longlong(\"longlong\", this),\n \t      ulonglong(\"ulonglong\", this), half(\"half\", this), float16(\"float16\", this), single(\"single\", this),\n \t      longdouble(\"longdouble\", this), csingle(\"csingle\", this), cdouble(\"cdouble\", this),\n@@ -44,6 +56,7 @@ struct NumpyCacheItem : public PythonImportCacheItem {\n \t}\n \n \tNumpyCoreCacheItem core;\n+\tNumpyMaCacheItem ma;\n \tPythonImportCacheItem ndarray;\n \tPythonImportCacheItem datetime64;\n \tPythonImportCacheItem generic;\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/numpy/array_wrapper.hpp b/tools/pythonpkg/src/include/duckdb_python/numpy/array_wrapper.hpp\nindex 8c262671bd7d..a9740e2cbed3 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/numpy/array_wrapper.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/numpy/array_wrapper.hpp\n@@ -28,10 +28,12 @@ struct NumpyAppendData {\n \tconst ClientProperties &client_properties;\n \tVector &input;\n \n+\tidx_t source_offset;\n \tidx_t target_offset;\n \tdata_ptr_t target_data;\n \tbool *target_mask;\n \tidx_t count;\n+\tidx_t source_size;\n \tPhysicalType physical_type = PhysicalType::INVALID;\n \tbool pandas = false;\n };\n@@ -48,8 +50,9 @@ struct ArrayWrapper {\n public:\n \tvoid Initialize(idx_t capacity);\n \tvoid Resize(idx_t new_capacity);\n-\tvoid Append(idx_t current_offset, Vector &input, idx_t count);\n-\tpy::object ToArray(idx_t count) const;\n+\tvoid Append(idx_t current_offset, Vector &input, idx_t source_size, idx_t source_offset = 0,\n+\t            idx_t count = DConstants::INVALID_INDEX);\n+\tpy::object ToArray() const;\n };\n \n } // namespace duckdb\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_result_conversion.hpp b/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_result_conversion.hpp\nindex e53b6add5729..e7922fae3b4f 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_result_conversion.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_result_conversion.hpp\n@@ -22,7 +22,7 @@ class NumpyResultConversion {\n \tvoid Append(DataChunk &chunk);\n \n \tpy::object ToArray(idx_t col_idx) {\n-\t\treturn owned_data[col_idx].ToArray(count);\n+\t\treturn owned_data[col_idx].ToArray();\n \t}\n \n private:\ndiff --git a/tools/pythonpkg/src/native/python_conversion.cpp b/tools/pythonpkg/src/native/python_conversion.cpp\nindex 77993b88b41b..3d6df0435b78 100644\n--- a/tools/pythonpkg/src/native/python_conversion.cpp\n+++ b/tools/pythonpkg/src/native/python_conversion.cpp\n@@ -436,6 +436,8 @@ PythonObjectType GetPythonObjectType(py::handle &ele) {\n \t\treturn PythonObjectType::Tuple;\n \t} else if (py::isinstance<py::dict>(ele)) {\n \t\treturn PythonObjectType::Dict;\n+\t} else if (ele.is(import_cache.numpy.ma.masked())) {\n+\t\treturn PythonObjectType::None;\n \t} else if (py::isinstance(ele, import_cache.numpy.ndarray())) {\n \t\treturn PythonObjectType::NdArray;\n \t} else if (py::isinstance(ele, import_cache.numpy.datetime64())) {\ndiff --git a/tools/pythonpkg/src/numpy/array_wrapper.cpp b/tools/pythonpkg/src/numpy/array_wrapper.cpp\nindex a731276c1148..649b9548bdec 100644\n--- a/tools/pythonpkg/src/numpy/array_wrapper.cpp\n+++ b/tools/pythonpkg/src/numpy/array_wrapper.cpp\n@@ -273,36 +273,62 @@ struct UUIDConvert {\n \t}\n };\n \n+static py::object InternalCreateList(Vector &input, idx_t total_size, idx_t offset, idx_t size,\n+                                     NumpyAppendData &append_data) {\n+\t// Initialize the array we'll append the list data to\n+\tauto &type = input.GetType();\n+\tArrayWrapper result(type, append_data.client_properties, append_data.pandas);\n+\tresult.Initialize(size);\n+\n+\tD_ASSERT(offset + size <= total_size);\n+\tresult.Append(0, input, total_size, offset, size);\n+\treturn result.ToArray();\n+}\n+\n struct ListConvert {\n-\tstatic py::list ConvertValue(Vector &input, idx_t chunk_offset, const ClientProperties &client_properties) {\n-\t\tauto val = input.GetValue(chunk_offset);\n-\t\tauto &list_children = ListValue::GetChildren(val);\n-\t\tpy::list list;\n-\t\tfor (auto &list_elem : list_children) {\n-\t\t\tlist.append(PythonObject::FromValue(list_elem, ListType::GetChildType(input.GetType()), client_properties));\n-\t\t}\n-\t\treturn list;\n+\tstatic py::object ConvertValue(Vector &input, idx_t chunk_offset, NumpyAppendData &append_data) {\n+\t\tauto &client_properties = append_data.client_properties;\n+\t\tauto &list_data = append_data.idata;\n+\n+\t\t// Get the list entry information from the parent\n+\t\tconst auto list_sel = *list_data.sel;\n+\t\tconst auto list_entries = UnifiedVectorFormat::GetData<list_entry_t>(list_data);\n+\t\tauto list_index = list_sel.get_index(chunk_offset);\n+\t\tauto list_entry = list_entries[list_index];\n+\n+\t\tauto list_size = list_entry.length;\n+\t\tauto list_offset = list_entry.offset;\n+\t\tauto child_size = ListVector::GetListSize(input);\n+\t\tauto &child_vector = ListVector::GetEntry(input);\n+\n+\t\treturn InternalCreateList(child_vector, child_size, list_offset, list_size, append_data);\n \t}\n };\n \n struct ArrayConvert {\n-\tstatic py::tuple ConvertValue(Vector &input, idx_t chunk_offset, const ClientProperties &client_properties) {\n-\t\tauto val = input.GetValue(chunk_offset);\n-\t\tauto &array_values = ArrayValue::GetChildren(val);\n+\tstatic py::object ConvertValue(Vector &input, idx_t chunk_offset, NumpyAppendData &append_data) {\n+\t\tauto &array_data = append_data.idata;\n+\n+\t\t// Get the list entry information from the parent\n+\t\tconst auto array_sel = *array_data.sel;\n+\t\tauto array_index = array_sel.get_index(chunk_offset);\n+\n \t\tauto &array_type = input.GetType();\n+\t\tD_ASSERT(array_type.id() == LogicalTypeId::ARRAY);\n+\n \t\tauto array_size = ArrayType::GetSize(array_type);\n-\t\tauto &child_type = ArrayType::GetChildType(array_type);\n+\t\tauto array_offset = array_index * array_size;\n+\t\tauto child_size = ArrayVector::GetTotalSize(input);\n+\t\tauto &child_vector = ArrayVector::GetEntry(input);\n \n-\t\tpy::tuple arr(array_size);\n-\t\tfor (idx_t elem_idx = 0; elem_idx < array_size; elem_idx++) {\n-\t\t\tarr[elem_idx] = PythonObject::FromValue(array_values[elem_idx], child_type, client_properties);\n-\t\t}\n-\t\treturn arr;\n+\t\treturn InternalCreateList(child_vector, child_size, array_offset, array_size, append_data);\n \t}\n };\n \n struct StructConvert {\n-\tstatic py::dict ConvertValue(Vector &input, idx_t chunk_offset, const ClientProperties &client_properties) {\n+\tstatic py::dict ConvertValue(Vector &input, idx_t chunk_offset, NumpyAppendData &append_data) {\n+\t\tauto &client_properties = append_data.client_properties;\n+\n \t\tpy::dict py_struct;\n \t\tauto val = input.GetValue(chunk_offset);\n \t\tauto &child_types = StructType::GetChildTypes(input.GetType());\n@@ -319,7 +345,8 @@ struct StructConvert {\n };\n \n struct UnionConvert {\n-\tstatic py::object ConvertValue(Vector &input, idx_t chunk_offset, const ClientProperties &client_properties) {\n+\tstatic py::object ConvertValue(Vector &input, idx_t chunk_offset, NumpyAppendData &append_data) {\n+\t\tauto &client_properties = append_data.client_properties;\n \t\tauto val = input.GetValue(chunk_offset);\n \t\tauto value = UnionValue::GetValue(val);\n \n@@ -328,7 +355,8 @@ struct UnionConvert {\n };\n \n struct MapConvert {\n-\tstatic py::dict ConvertValue(Vector &input, idx_t chunk_offset, const ClientProperties &client_properties) {\n+\tstatic py::dict ConvertValue(Vector &input, idx_t chunk_offset, NumpyAppendData &append_data) {\n+\t\tauto &client_properties = append_data.client_properties;\n \t\tauto val = input.GetValue(chunk_offset);\n \t\tauto &list_children = ListValue::GetChildren(val);\n \n@@ -386,13 +414,14 @@ static bool ConvertColumn(NumpyAppendData &append_data) {\n \tauto target_mask = append_data.target_mask;\n \tauto &idata = append_data.idata;\n \tauto count = append_data.count;\n+\tauto source_offset = append_data.source_offset;\n \n \tauto src_ptr = UnifiedVectorFormat::GetData<DUCKDB_T>(idata);\n \tauto out_ptr = reinterpret_cast<NUMPY_T *>(target_data);\n \tif (!idata.validity.AllValid()) {\n \t\tbool mask_is_set = false;\n \t\tfor (idx_t i = 0; i < count; i++) {\n-\t\t\tidx_t src_idx = idata.sel->get_index(i);\n+\t\t\tidx_t src_idx = idata.sel->get_index(i + source_offset);\n \t\t\tidx_t offset = target_offset + i;\n \t\t\tif (!idata.validity.RowIsValidUnsafe(src_idx)) {\n \t\t\t\tif (append_data.pandas) {\n@@ -409,7 +438,7 @@ static bool ConvertColumn(NumpyAppendData &append_data) {\n \t\treturn mask_is_set;\n \t} else {\n \t\tfor (idx_t i = 0; i < count; i++) {\n-\t\t\tidx_t src_idx = idata.sel->get_index(i);\n+\t\t\tidx_t src_idx = idata.sel->get_index(i + source_offset);\n \t\t\tidx_t offset = target_offset + i;\n \t\t\tout_ptr[offset] = CONVERT::template ConvertValue<DUCKDB_T, NUMPY_T>(src_ptr[src_idx]);\n \t\t\ttarget_mask[offset] = false;\n@@ -424,12 +453,13 @@ static bool ConvertColumnCategoricalTemplate(NumpyAppendData &append_data) {\n \tauto target_data = append_data.target_data;\n \tauto &idata = append_data.idata;\n \tauto count = append_data.count;\n+\tauto source_offset = append_data.source_offset;\n \n \tauto src_ptr = UnifiedVectorFormat::GetData<DUCKDB_T>(idata);\n \tauto out_ptr = reinterpret_cast<NUMPY_T *>(target_data);\n \tif (!idata.validity.AllValid()) {\n \t\tfor (idx_t i = 0; i < count; i++) {\n-\t\t\tidx_t src_idx = idata.sel->get_index(i);\n+\t\t\tidx_t src_idx = idata.sel->get_index(i + source_offset);\n \t\t\tidx_t offset = target_offset + i;\n \t\t\tif (!idata.validity.RowIsValidUnsafe(src_idx)) {\n \t\t\t\tout_ptr[offset] = static_cast<NUMPY_T>(-1);\n@@ -440,7 +470,7 @@ static bool ConvertColumnCategoricalTemplate(NumpyAppendData &append_data) {\n \t\t}\n \t} else {\n \t\tfor (idx_t i = 0; i < count; i++) {\n-\t\t\tidx_t src_idx = idata.sel->get_index(i);\n+\t\t\tidx_t src_idx = idata.sel->get_index(i + source_offset);\n \t\t\tidx_t offset = target_offset + i;\n \t\t\tout_ptr[offset] =\n \t\t\t    duckdb_py_convert::RegularConvert::template ConvertValue<DUCKDB_T, NUMPY_T>(src_ptr[src_idx]);\n@@ -459,24 +489,32 @@ static bool ConvertNested(NumpyAppendData &append_data) {\n \tauto &idata = append_data.idata;\n \tauto &client_properties = append_data.client_properties;\n \tauto count = append_data.count;\n+\tauto source_offset = append_data.source_offset;\n \n \tauto out_ptr = reinterpret_cast<NUMPY_T *>(target_data);\n \tif (!idata.validity.AllValid()) {\n+\t\tbool requires_mask = false;\n \t\tfor (idx_t i = 0; i < count; i++) {\n-\t\t\tidx_t src_idx = idata.sel->get_index(i);\n+\t\t\tidx_t index = i + source_offset;\n+\t\t\tidx_t src_idx = idata.sel->get_index(index);\n \t\t\tidx_t offset = target_offset + i;\n \t\t\tif (!idata.validity.RowIsValidUnsafe(src_idx)) {\n+\t\t\t\tout_ptr[offset] = py::none();\n+\t\t\t\trequires_mask = true;\n \t\t\t\ttarget_mask[offset] = true;\n \t\t\t} else {\n-\t\t\t\tout_ptr[offset] = CONVERT::ConvertValue(input, i, client_properties);\n+\t\t\t\tout_ptr[offset] = CONVERT::ConvertValue(input, index, append_data);\n \t\t\t\ttarget_mask[offset] = false;\n \t\t\t}\n \t\t}\n-\t\treturn true;\n+\t\treturn requires_mask;\n \t} else {\n \t\tfor (idx_t i = 0; i < count; i++) {\n+\t\t\t// NOTE: we do not apply the selection vector here,\n+\t\t\t// because we use GetValue inside ConvertValue, which *also* applies the selection vector\n+\t\t\tidx_t index = i + source_offset;\n \t\t\tidx_t offset = target_offset + i;\n-\t\t\tout_ptr[offset] = CONVERT::ConvertValue(input, i, client_properties);\n+\t\t\tout_ptr[offset] = CONVERT::ConvertValue(input, index, append_data);\n \t\t\ttarget_mask[offset] = false;\n \t\t}\n \t\treturn false;\n@@ -510,12 +548,13 @@ static bool ConvertDecimalInternal(NumpyAppendData &append_data, double division\n \tauto target_mask = append_data.target_mask;\n \tauto &idata = append_data.idata;\n \tauto count = append_data.count;\n+\tauto source_offset = append_data.source_offset;\n \n \tauto src_ptr = UnifiedVectorFormat::GetData<DUCKDB_T>(idata);\n \tauto out_ptr = reinterpret_cast<double *>(target_data);\n \tif (!idata.validity.AllValid()) {\n \t\tfor (idx_t i = 0; i < count; i++) {\n-\t\t\tidx_t src_idx = idata.sel->get_index(i);\n+\t\t\tidx_t src_idx = idata.sel->get_index(i + source_offset);\n \t\t\tidx_t offset = target_offset + i;\n \t\t\tif (!idata.validity.RowIsValidUnsafe(src_idx)) {\n \t\t\t\ttarget_mask[offset] = true;\n@@ -528,7 +567,7 @@ static bool ConvertDecimalInternal(NumpyAppendData &append_data, double division\n \t\treturn true;\n \t} else {\n \t\tfor (idx_t i = 0; i < count; i++) {\n-\t\t\tidx_t src_idx = idata.sel->get_index(i);\n+\t\t\tidx_t src_idx = idata.sel->get_index(i + source_offset);\n \t\t\tidx_t offset = target_offset + i;\n \t\t\tout_ptr[offset] =\n \t\t\t    duckdb_py_convert::IntegralConvert::ConvertValue<DUCKDB_T, double>(src_ptr[src_idx]) / division;\n@@ -572,7 +611,7 @@ void ArrayWrapper::Resize(idx_t new_capacity) {\n \tmask->Resize(new_capacity);\n }\n \n-void ArrayWrapper::Append(idx_t current_offset, Vector &input, idx_t count) {\n+void ArrayWrapper::Append(idx_t current_offset, Vector &input, idx_t source_size, idx_t source_offset, idx_t count) {\n \tauto dataptr = data->data;\n \tauto maskptr = reinterpret_cast<bool *>(mask->data);\n \tD_ASSERT(dataptr);\n@@ -581,11 +620,18 @@ void ArrayWrapper::Append(idx_t current_offset, Vector &input, idx_t count) {\n \tbool may_have_null;\n \n \tUnifiedVectorFormat idata;\n-\tinput.ToUnifiedFormat(count, idata);\n+\tinput.ToUnifiedFormat(source_size, idata);\n+\n+\tif (count == DConstants::INVALID_INDEX) {\n+\t\tD_ASSERT(source_size != DConstants::INVALID_INDEX);\n+\t\tcount = source_size;\n+\t}\n \n \tNumpyAppendData append_data(idata, client_properties, input);\n \tappend_data.target_offset = current_offset;\n \tappend_data.target_data = dataptr;\n+\tappend_data.source_offset = source_offset;\n+\tappend_data.source_size = source_size;\n \tappend_data.count = count;\n \tappend_data.target_mask = maskptr;\n \tappend_data.pandas = pandas;\n@@ -672,19 +718,19 @@ void ArrayWrapper::Append(idx_t current_offset, Vector &input, idx_t count) {\n \t\tmay_have_null = ConvertColumn<string_t, PyObject *, duckdb_py_convert::BitConvert>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::LIST:\n-\t\tmay_have_null = ConvertNested<py::list, duckdb_py_convert::ListConvert>(append_data);\n+\t\tmay_have_null = ConvertNested<py::object, duckdb_py_convert::ListConvert>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::ARRAY:\n-\t\tmay_have_null = ConvertNested<py::tuple, duckdb_py_convert::ArrayConvert>(append_data);\n+\t\tmay_have_null = ConvertNested<py::object, duckdb_py_convert::ArrayConvert>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::MAP:\n-\t\tmay_have_null = ConvertNested<py::dict, duckdb_py_convert::MapConvert>(append_data);\n+\t\tmay_have_null = ConvertNested<py::object, duckdb_py_convert::MapConvert>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::UNION:\n \t\tmay_have_null = ConvertNested<py::object, duckdb_py_convert::UnionConvert>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::STRUCT:\n-\t\tmay_have_null = ConvertNested<py::dict, duckdb_py_convert::StructConvert>(append_data);\n+\t\tmay_have_null = ConvertNested<py::object, duckdb_py_convert::StructConvert>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::UUID:\n \t\tmay_have_null = ConvertColumn<hugeint_t, PyObject *, duckdb_py_convert::UUIDConvert>(append_data);\n@@ -700,7 +746,7 @@ void ArrayWrapper::Append(idx_t current_offset, Vector &input, idx_t count) {\n \tmask->count += count;\n }\n \n-py::object ArrayWrapper::ToArray(idx_t count) const {\n+py::object ArrayWrapper::ToArray() const {\n \tD_ASSERT(data->array && mask->array);\n \tdata->Resize(data->count);\n \tif (!requires_mask) {\ndiff --git a/tools/pythonpkg/src/numpy/numpy_result_conversion.cpp b/tools/pythonpkg/src/numpy/numpy_result_conversion.cpp\nindex b8d832e0ea53..c2105baec2e7 100644\n--- a/tools/pythonpkg/src/numpy/numpy_result_conversion.cpp\n+++ b/tools/pythonpkg/src/numpy/numpy_result_conversion.cpp\n@@ -31,10 +31,13 @@ void NumpyResultConversion::Append(DataChunk &chunk) {\n \t\tResize(capacity * 2);\n \t}\n \tauto chunk_types = chunk.GetTypes();\n+\tauto source_offset = 0;\n+\tauto source_size = chunk.size();\n+\tauto to_append = chunk.size();\n \tfor (idx_t col_idx = 0; col_idx < owned_data.size(); col_idx++) {\n-\t\towned_data[col_idx].Append(count, chunk.data[col_idx], chunk.size());\n+\t\towned_data[col_idx].Append(count, chunk.data[col_idx], source_size, source_offset, to_append);\n \t}\n-\tcount += chunk.size();\n+\tcount += to_append;\n #ifdef DEBUG\n \tfor (auto &data : owned_data) {\n \t\tD_ASSERT(data.data->count == count);\ndiff --git a/tools/pythonpkg/src/pandas/analyzer.cpp b/tools/pythonpkg/src/pandas/analyzer.cpp\nindex df1ddb6be080..15f9dbed1c90 100644\n--- a/tools/pythonpkg/src/pandas/analyzer.cpp\n+++ b/tools/pythonpkg/src/pandas/analyzer.cpp\n@@ -11,8 +11,10 @@ namespace duckdb {\n static bool TypeIsNested(LogicalTypeId id) {\n \tswitch (id) {\n \tcase LogicalTypeId::STRUCT:\n+\tcase LogicalTypeId::UNION:\n \tcase LogicalTypeId::LIST:\n \tcase LogicalTypeId::MAP:\n+\tcase LogicalTypeId::ARRAY:\n \t\treturn true;\n \tdefault:\n \t\treturn false;\ndiff --git a/tools/pythonpkg/src/pandas/scan.cpp b/tools/pythonpkg/src/pandas/scan.cpp\nindex 16ad8b4dd7cd..1bd35fe7319f 100644\n--- a/tools/pythonpkg/src/pandas/scan.cpp\n+++ b/tools/pythonpkg/src/pandas/scan.cpp\n@@ -194,7 +194,7 @@ unique_ptr<NodeStatistics> PandasScanFunction::PandasScanCardinality(ClientConte\n }\n \n py::object PandasScanFunction::PandasReplaceCopiedNames(const py::object &original_df) {\n-\tauto copy_df = original_df.attr(\"copy\")(false);\n+\tpy::object copy_df = original_df.attr(\"copy\")(false);\n \tauto df_columns = py::list(original_df.attr(\"columns\"));\n \tvector<string> columns;\n \tfor (const auto &str : df_columns) {\n@@ -202,7 +202,12 @@ py::object PandasScanFunction::PandasReplaceCopiedNames(const py::object &origin\n \t}\n \tQueryResult::DeduplicateColumns(columns);\n \n-\tcopy_df.attr(\"columns\") = columns;\n+\tpy::list new_columns(columns.size());\n+\tfor (idx_t i = 0; i < columns.size(); i++) {\n+\t\tnew_columns[i] = std::move(columns[i]);\n+\t}\n+\tcopy_df.attr(\"columns\") = std::move(new_columns);\n+\tcolumns.clear();\n \treturn copy_df;\n }\n \ndiff --git a/tools/pythonpkg/src/pyconnection.cpp b/tools/pythonpkg/src/pyconnection.cpp\nindex 4fe5e1529c06..48de0b994e5f 100644\n--- a/tools/pythonpkg/src/pyconnection.cpp\n+++ b/tools/pythonpkg/src/pyconnection.cpp\n@@ -1232,9 +1232,11 @@ void DuckDBPyConnection::Close() {\n \tresult = nullptr;\n \tconnection = nullptr;\n \tdatabase = nullptr;\n+\ttemporary_views.clear();\n \tfor (auto &cur : cursors) {\n \t\tcur->Close();\n \t}\n+\tregistered_functions.clear();\n \tcursors.clear();\n }\n \ndiff --git a/tools/pythonpkg/src/pyrelation/initialize.cpp b/tools/pythonpkg/src/pyrelation/initialize.cpp\nindex cb959435b35a..87a3c538dc22 100644\n--- a/tools/pythonpkg/src/pyrelation/initialize.cpp\n+++ b/tools/pythonpkg/src/pyrelation/initialize.cpp\n@@ -53,6 +53,8 @@ static void InitializeConsumers(py::class_<DuckDBPyRelation> &m) {\n \t         py::arg(\"date_as_object\") = false)\n \t    .def(\"to_df\", &DuckDBPyRelation::FetchDF, \"Execute and fetch all rows as a pandas DataFrame\", py::kw_only(),\n \t         py::arg(\"date_as_object\") = false)\n+\t    .def(\"fetch_df_chunk\", &DuckDBPyRelation::FetchDFChunk, \"Execute and fetch a chunk of the rows\", py::kw_only(),\n+\t         py::arg(\"vectors_per_chunk\"), py::arg(\"date_as_object\"))\n \t    .def(\"arrow\", &DuckDBPyRelation::ToArrowTable, \"Execute and fetch all rows as an Arrow Table\",\n \t         py::arg(\"batch_size\") = 1000000)\n \t    .def(\"fetch_arrow_table\", &DuckDBPyRelation::ToArrowTable, \"Execute and fetch all rows as an Arrow Table\",\ndiff --git a/tools/pythonpkg/src/pyresult.cpp b/tools/pythonpkg/src/pyresult.cpp\nindex baa9880aacdd..eb3b651cc88a 100644\n--- a/tools/pythonpkg/src/pyresult.cpp\n+++ b/tools/pythonpkg/src/pyresult.cpp\n@@ -266,7 +266,7 @@ void DuckDBPyResult::ChangeDateToDatetime(PandasDataFrame &df) {\n }\n \n PandasDataFrame DuckDBPyResult::FrameFromNumpy(bool date_as_object, const py::handle &o) {\n-\tauto df = py::cast<PandasDataFrame>(py::module::import(\"pandas\").attr(\"DataFrame\").attr(\"from_dict\")(o));\n+\tPandasDataFrame df = py::cast<PandasDataFrame>(py::module::import(\"pandas\").attr(\"DataFrame\").attr(\"from_dict\")(o));\n \t// Unfortunately we have to do a type change here for timezones since these types are not supported by numpy\n \tChangeToTZType(df);\n \tif (date_as_object) {\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py b/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py\nindex 9c6834ff8d23..6207fd4e82af 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py\n@@ -368,9 +368,9 @@ def test_list_starts_with_null(self, pandas, duckdb_cursor):\n     @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n     def test_list_value_upgrade(self, pandas, duckdb_cursor):\n         x = pandas.DataFrame([{'0': [['5'], [34], [-245]]}])\n-        duckdb_col = duckdb_cursor.sql(\"select [['5'], ['34'], ['-245']] as '0'\").df()\n+        duckdb_rel = duckdb_cursor.sql(\"select [['5'], ['34'], ['-245']] as '0'\")\n+        duckdb_col = duckdb_rel.df()\n         converted_col = duckdb_cursor.sql(\"select * from x\").df()\n-        duckdb_cursor.sql(\"drop view if exists tbl\")\n         pandas.testing.assert_frame_equal(duckdb_col, converted_col)\n \n     @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_fetch_nested.py b/tools/pythonpkg/tests/fast/pandas/test_fetch_nested.py\nindex 7c3957a450f9..af23ae97a5a8 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_fetch_nested.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_fetch_nested.py\n@@ -1,94 +1,170 @@\n import pytest\n import duckdb\n-import pandas as pd\n+import sys\n+\n+pd = pytest.importorskip(\"pandas\")\n import numpy as np\n \n \n-def compare_results(query, list_values=[], list_values_2=[]):\n-    con = duckdb.connect()\n-    df_duck = con.query(query).df()\n-    duck_values = df_duck['a']\n-    for duck_value in duck_values:\n-        is_in_list_value = False\n-        for value in list_values:\n-            if duck_value == value:\n-                is_in_list_value = True\n-        for value in list_values_2:\n-            if duck_value == value:\n-                is_in_list_value = True\n-        assert is_in_list_value\n+def compare_results(con, query, expected):\n+    expected = pd.DataFrame.from_dict(expected)\n+\n+    unsorted_res = con.query(query).df()\n+    print(unsorted_res, unsorted_res['a'][0].__class__)\n+    df_duck = con.query(\"select * from unsorted_res order by all\").df()\n+    print(df_duck, df_duck['a'][0].__class__)\n+    print(expected, expected['a'][0].__class__)\n+    pd.testing.assert_frame_equal(df_duck, expected)\n+\n+\n+def list_test_cases():\n+    # fmt: off\n+    test_cases = [\n+        (\"SELECT list_value(3,5,10) as a\", {\n+            'a': [\n+                [3, 5, 10]\n+            ]\n+        }),\n+        (\"SELECT list_value(3,5,NULL) as a\", {\n+            'a': [\n+                np.ma.array(\n+                    [3, 5, 0],\n+                mask=[0, 0, 1],\n+            )\n+            ]\n+        }),\n+        (\"SELECT list_value(NULL,NULL,NULL) as a\", {\n+            'a': [\n+                np.ma.array(\n+                    [0, 0, 0],\n+                mask=[1, 1, 1],\n+            )\n+            ]\n+        }),\n+        (\"SELECT list_value() as a\", {\n+            'a': [\n+                np.array([])\n+            ]\n+        }),\n+        (\"SELECT list_value('test','test_one','test_two') as a\", {\n+            'a': [\n+                np.array([\n+                    'test', 'test_one', 'test_two'\n+                ])\n+            ]\n+        }),\n+        (\"SELECT a from (SELECT LIST(i) as a FROM range(10000) tbl(i)) as t\", {\n+            'a': [\n+                list(range(0, 10000))\n+            ]\n+        }),\n+        (\"SELECT LIST(i) as a FROM range(5) tbl(i) group by i%2 order by all\", {\n+            'a': [\n+                [0, 2, 4],\n+                [1, 3]\n+            ]\n+        }),\n+        (\"SELECT list_value(1) as a FROM range(5) tbl(i)\", {\n+            'a': [\n+                [1],\n+                [1],\n+                [1],\n+                [1],\n+                [1]\n+            ]\n+        }),\n+        (\"\"\"\n+            SELECT\n+                list(st) AS a\n+            FROM\n+            (\n+                SELECT\n+                    i,\n+                    CASE WHEN i%5\n+                        THEN NULL\n+                        ELSE i::VARCHAR\n+                    END AS st\n+                FROM range(10) tbl(i)\n+            ) AS t\n+            GROUP BY i%2\n+            ORDER BY all\n+        \"\"\", {\n+            'a': [\n+                ['0', None, None, None, None],\n+                [None, None, '5', None, None]\n+            ]\n+        }),\n+    ]\n+\n+    return test_cases\n+    # These tests are problematic\n+    # They cause errors on highest NumPy version supported by 3.7\n+    # And a crash on manylinux 2014, x86_64 AMD, python 3.12.1\n+    test_cases.extend([\n+        (\"\"\"\n+            SELECT * from values\n+                ([[1, 3], [0,2,4]])\n+            t(a)\n+        \"\"\", {\n+            'a': [\n+                [\n+                    [1, 3],\n+                    [0, 2, 4]\n+                ]\n+            ]\n+        }),\n+        (\"\"\"\n+            SELECT * from values\n+                ([[[[[0, 2, 4], [1, 3]]]]])\n+            t(a)\n+        \"\"\", {\n+            'a': [\n+                [[[[[0, 2, 4], [1, 3]]]]]\n+            ]\n+        }),\n+        (\"\"\"\n+            select * from values\n+                ([\n+                    [NULL],\n+                    [NULL],\n+                    [2, 6],\n+                    [3]\n+                ])\n+            t(a)\n+        \"\"\", {\n+            'a': [\n+                np.array([\n+                    np.ma.array([0], mask=[1], dtype=np.int32()),\n+                    np.ma.array([0], mask=[1], dtype=np.int32()),\n+                    np.array([2, 6]),\n+                    np.array([3]),\n+                ], dtype='object')\n+            ]\n+        }),\n+    ])\n+    # fmt: on\n+    return test_cases\n \n \n class TestFetchNested(object):\n-    @pytest.mark.skip(reason=\"sporadically breaks in CI\")\n-    def test_fetch_df_list(self):\n-        con = duckdb.connect()\n-        # Integers\n-        compare_results(\"SELECT a from (select list_value(3,5,10) as a) as t\", [[3, 5, 10]])\n-        compare_results(\"SELECT a from (select list_value(3,5,NULL) as a) as t\", [[3, 5, None]])\n-        compare_results(\"SELECT a from (select list_value(NULL,NULL,NULL) as a) as t\", [[None, None, None]])\n-        compare_results(\"SELECT a from (select list_value() as a) as t\", [[]])\n-\n-        # Strings\n-        compare_results(\n-            \"SELECT a from (select list_value('test','test_one','test_two') as a) as t\",\n-            [['test', 'test_one', 'test_two']],\n-        )\n-        compare_results(\n-            \"SELECT a from (select list_value('test','test_one',NULL) as a) as t\", [['test', 'test_one', None]]\n-        )\n-\n-        # Big Lists\n-        compare_results(\"SELECT a from (SELECT LIST(i) as a FROM range(10000) tbl(i)) as t\", [list(range(0, 10000))])\n-\n-        # #Multiple Lists\n-        compare_results(\n-            \"SELECT a from (SELECT LIST(i) as a FROM range(5) tbl(i) group by i%2 order by all) as t\",\n-            [[0, 2, 4], [1, 3]],\n-        )\n-\n-        # Unique Constants\n-        compare_results(\n-            \"SELECT a from (SELECT list_value(1) as a FROM range(5) tbl(i)) as t\", [[1], [1], [1], [1], [1]]\n-        )\n-\n-        # Nested Lists\n-        compare_results(\n-            \"SELECT LIST(le) as a FROM (SELECT LIST(i) le from range(5) tbl(i) group by i%2 order by all) as t order by all\",\n-            [[[0, 2, 4], [1, 3]]],\n-            [[[1, 3], [0, 2, 4]]],\n-        )\n-\n-        # LIST[LIST[LIST[LIST[LIST[INTEGER]]]]]]\n-        compare_results(\n-            \"SELECT list (lllle)  as a from (SELECT list (llle) lllle from (SELECT list(lle) llle from (SELECT LIST(le) lle FROM (SELECT LIST(i) le from range(5) tbl(i) group by i%2 order by all) as t order by all) as t1 order by all) as t2 order by all) as t3 order by all\",\n-            [[[[[[0, 2, 4], [1, 3]]]]]],\n-            [[[[[[1, 3], [0, 2, 4]]]]]],\n-        )\n-\n-        compare_results(\n-            '''SELECT grp,lst,a FROM (select grp, lst, case when grp>1 then lst else list_value(null) end as a\n-                         from (SELECT a_1%4 as grp, list(a_1) as lst FROM range(7) tbl(a_1) group by grp order by all) as lst_tbl) as T;''',\n-            [[None], [None], [2, 6], [3]],\n-        )\n-\n-        # Tests for converting multiple lists to/from Pandas with NULL values and/or strings\n-        compare_results(\n-            \"SELECT list(st) as a from (select i, case when i%5 then NULL else i::VARCHAR end as st from range(10) tbl(i)) as t group by i%2 order by all\",\n-            [['0', None, None, None, None], [None, None, '5', None, None]],\n-        )\n-\n-    @pytest.mark.skip(reason=\"sporadically breaks in CI\")\n-    def test_struct_df(self):\n-        compare_results(\"SELECT a from (SELECT STRUCT_PACK(a := 42, b := 43) as a) as t\", [{'a': 42, 'b': 43}])\n-\n-        compare_results(\"SELECT a from (SELECT STRUCT_PACK(a := NULL, b := 43) as a) as t\", [{'a': None, 'b': 43}])\n-\n-        compare_results(\"SELECT a from (SELECT STRUCT_PACK(a := NULL) as a) as t\", [{'a': None}])\n-\n-        compare_results(\n-            \"SELECT a from (SELECT STRUCT_PACK(a := i, b := i) as a FROM range(10) tbl(i)) as t\",\n-            [\n+    @pytest.mark.parametrize('query, expected', list_test_cases())\n+    def test_fetch_df_list(self, duckdb_cursor, query, expected):\n+        compare_results(duckdb_cursor, query, expected)\n+\n+    # fmt: off\n+    @pytest.mark.parametrize('query, expected', [\n+        (\"SELECT a from (SELECT STRUCT_PACK(a := 42, b := 43) as a) as t\", {\n+            'a': [\n+                {'a': 42, 'b': 43}\n+            ]\n+        }),\n+        (\"SELECT a from (SELECT STRUCT_PACK(a := NULL) as a) as t\", {\n+            'a': [\n+                {'a': None}\n+            ]\n+        }),\n+        (\"SELECT a from (SELECT STRUCT_PACK(a := i, b := i) as a FROM range(10) tbl(i)) as t\", {\n+            'a': [\n                 {'a': 0, 'b': 0},\n                 {'a': 1, 'b': 1},\n                 {'a': 2, 'b': 2},\n@@ -99,12 +175,10 @@ def test_struct_df(self):\n                 {'a': 7, 'b': 7},\n                 {'a': 8, 'b': 8},\n                 {'a': 9, 'b': 9},\n-            ],\n-        )\n-\n-        compare_results(\n-            \"SELECT a from (SELECT STRUCT_PACK(a := LIST_VALUE(1,2,3), b := i) as a FROM range(10) tbl(i)) as t\",\n-            [\n+            ]\n+        }),\n+        (\"SELECT a from (SELECT STRUCT_PACK(a := LIST_VALUE(1,2,3), b := i) as a FROM range(10) tbl(i)) as t\", {\n+            'a': [\n                 {'a': [1, 2, 3], 'b': 0},\n                 {'a': [1, 2, 3], 'b': 1},\n                 {'a': [1, 2, 3], 'b': 2},\n@@ -115,125 +189,217 @@ def test_struct_df(self):\n                 {'a': [1, 2, 3], 'b': 7},\n                 {'a': [1, 2, 3], 'b': 8},\n                 {'a': [1, 2, 3], 'b': 9},\n-            ],\n-        )\n-\n-    @pytest.mark.skip(reason=\"sporadically breaks in CI\")\n-    def test_map_df(self):\n-        compare_results(\n-            \"SELECT a from (select MAP(LIST_VALUE(1, 2, 3, 4),LIST_VALUE(10, 9, 8, 7)) as a) as t\",\n-            [{'key': [1, 2, 3, 4], 'value': [10, 9, 8, 7]}],\n-        )\n-\n-        with pytest.raises(duckdb.InvalidInputException, match=\"Map keys have to be unique\"):\n-            compare_results(\n-                \"SELECT a from (select MAP(LIST_VALUE(1, 2, 3, 4,2, NULL),LIST_VALUE(10, 9, 8, 7,11,42)) as a) as t\",\n-                [{'key': [1, 2, 3, 4, 2, None], 'value': [10, 9, 8, 7, 11, 42]}],\n-            )\n-\n-        compare_results(\"SELECT a from (select MAP(LIST_VALUE(),LIST_VALUE()) as a) as t\", [{'key': [], 'value': []}])\n-\n-        with pytest.raises(duckdb.InvalidInputException, match=\"Map keys have to be unique\"):\n-            compare_results(\n-                \"SELECT a from (select MAP(LIST_VALUE('Jon Lajoie', 'Backstreet Boys', 'Tenacious D','Jon Lajoie' ),LIST_VALUE(10,9,10,11)) as a) as t\",\n-                [{'key': ['Jon Lajoie', 'Backstreet Boys', 'Tenacious D', 'Jon Lajoie'], 'value': [10, 9, 10, 11]}],\n-            )\n-\n-        with pytest.raises(duckdb.InvalidInputException, match=\"Map keys can not be NULL\"):\n-            compare_results(\n-                \"SELECT a from (select MAP(LIST_VALUE('Jon Lajoie', NULL, 'Tenacious D',NULL,NULL ),LIST_VALUE(10,9,10,11,13)) as a) as t\",\n-                [{'key': ['Jon Lajoie', None, 'Tenacious D', None, None], 'value': [10, 9, 10, 11, 13]}],\n-            )\n-\n-        with pytest.raises(duckdb.InvalidInputException, match=\"Map keys can not be NULL\"):\n-            compare_results(\n-                \"SELECT a from (select MAP(LIST_VALUE(NULL, NULL, NULL,NULL,NULL ),LIST_VALUE(10,9,10,11,13)) as a) as t\",\n-                [{'key': [None, None, None, None, None], 'value': [10, 9, 10, 11, 13]}],\n-            )\n-\n-        with pytest.raises(duckdb.InvalidInputException, match=\"Map keys can not be NULL\"):\n-            compare_results(\n-                \"SELECT a from (select MAP(LIST_VALUE(NULL, NULL, NULL,NULL,NULL ),LIST_VALUE(NULL, NULL, NULL,NULL,NULL )) as a) as t\",\n-                [{'key': [None, None, None, None, None], 'value': [None, None, None, None, None]}],\n-            )\n-\n-        compare_results(\n-            \"SELECT m as a from (select MAP(list_value(1), list_value(2)) from range(5) tbl(i)) tbl(m)\",\n-            [\n+            ]\n+        }),\n+    ])\n+    # fmt: on\n+    def test_struct_df(self, duckdb_cursor, query, expected):\n+        compare_results(duckdb_cursor, query, expected)\n+\n+    # fmt: off\n+    @pytest.mark.parametrize('query, expected, expected_error', [\n+        (\"SELECT a from (select MAP(LIST_VALUE(1, 2, 3, 4),LIST_VALUE(10, 9, 8, 7)) as a) as t\", {\n+            'a': [\n+                {\n+                    'key': [1, 2, 3, 4],\n+                    'value': [10, 9, 8, 7]\n+                }\n+            ]\n+        }, \"\"),\n+        (\"SELECT a from (select MAP(LIST_VALUE(1, 2, 3, 4),LIST_VALUE(10, 9, 8, 7)) as a) as t\", {\n+            'a': [\n+                {\n+                    'key': [1, 2, 3, 4],\n+                    'value': [10, 9, 8, 7]\n+                }\n+            ]\n+        }, \"\"),\n+        (\"SELECT a from (select MAP(LIST_VALUE(),LIST_VALUE()) as a) as t\", {\n+            'a': [\n+                {\n+                    'key': [],\n+                    'value': []\n+                }\n+            ]\n+        }, \"\"),\n+        (\"SELECT m as a from (select MAP(list_value(1), list_value(2)) from range(5) tbl(i)) tbl(m)\", {\n+            'a': [\n                 {'key': [1], 'value': [2]},\n                 {'key': [1], 'value': [2]},\n                 {'key': [1], 'value': [2]},\n                 {'key': [1], 'value': [2]},\n                 {'key': [1], 'value': [2]},\n-            ],\n-        )\n-\n-        compare_results(\n-            \"SELECT m as a from (select MAP(lsta,lstb) as m from (SELECT list(i) as lsta, list(i) as lstb from range(10) tbl(i) group by i%5 order by all) as lst_tbl) as T\",\n-            [\n+            ]\n+        }, \"\"),\n+        (\"SELECT m as a from (select MAP(lsta,lstb) as m from (SELECT list(i) as lsta, list(i) as lstb from range(10) tbl(i) group by i%5 order by all) as lst_tbl) as T\", {\n+            'a': [\n                 {'key': [0, 5], 'value': [0, 5]},\n                 {'key': [1, 6], 'value': [1, 6]},\n                 {'key': [2, 7], 'value': [2, 7]},\n                 {'key': [3, 8], 'value': [3, 8]},\n                 {'key': [4, 9], 'value': [4, 9]},\n-            ],\n-        )\n-\n-    @pytest.mark.skip(reason=\"sporadically breaks in CI\")\n-    def test_nested_mix(self):\n-        con = duckdb.connect()\n-        # List of structs W/ Struct that is NULL entirely\n-        compare_results(\n-            \"SELECT [{'i':1,'j':2},NULL,{'i':2,'j':NULL}] as a\", [[{'i': 1, 'j': 2}, None, {'i': 2, 'j': None}]]\n-        )\n-\n-        # Lists of structs with lists\n-        compare_results(\"SELECT [{'i':1,'j':[2,3]},NULL] as a\", [[{'i': 1, 'j': [2, 3]}, None]])\n-\n-        # Maps embedded in a struct\n-        compare_results(\n-            \"SELECT {'i':mp,'j':mp2} as a FROM (SELECT MAP(LIST_VALUE(1, 2, 3, 4),LIST_VALUE(10, 9, 8, 7)) as mp, MAP(LIST_VALUE(1, 2, 3, 5),LIST_VALUE(10, 9, 8, 7)) as mp2) as t\",\n-            [{'i': {'key': [1, 2, 3, 4], 'value': [10, 9, 8, 7]}, 'j': {'key': [1, 2, 3, 5], 'value': [10, 9, 8, 7]}}],\n-        )\n-\n-        # List of maps\n-        compare_results(\n-            \"SELECT [mp,mp2] as a FROM (SELECT MAP(LIST_VALUE(1, 2, 3, 4),LIST_VALUE(10, 9, 8, 7)) as mp, MAP(LIST_VALUE(1, 2, 3, 5),LIST_VALUE(10, 9, 8, 7)) as mp2) as t\",\n-            [[{'key': [1, 2, 3, 4], 'value': [10, 9, 8, 7]}, {'key': [1, 2, 3, 5], 'value': [10, 9, 8, 7]}]],\n-        )\n-\n-        # Map with list as key and/or value\n-        compare_results(\n-            \"SELECT MAP(LIST_VALUE([1,2],[3,4],[5,4]),LIST_VALUE([1,2],[3,4],[5,4])) as a\",\n-            [{'key': [[1, 2], [3, 4], [5, 4]], 'value': [[1, 2], [3, 4], [5, 4]]}],\n-        )\n-\n-        # Map with struct as key and/or value\n-        compare_results(\n-            \"SELECT MAP(LIST_VALUE({'i':1,'j':2},{'i':3,'j':4}),LIST_VALUE({'i':1,'j':2},{'i':3,'j':4})) as a\",\n-            [{'key': [{'i': 1, 'j': 2}, {'i': 3, 'j': 4}], 'value': [{'i': 1, 'j': 2}, {'i': 3, 'j': 4}]}],\n-        )\n-\n-        # Null checks on lists with structs\n-        compare_results(\n-            \"SELECT [{'i':1,'j':[2,3]},NULL,{'i':1,'j':[2,3]}] as a\",\n-            [[{'i': 1, 'j': [2, 3]}, None, {'i': 1, 'j': [2, 3]}]],\n-        )\n-\n-        # Struct that is NULL entirely\n-        df_duck = con.query(\"SELECT col0 as a FROM (VALUES ({'i':1,'j':2}), (NULL), ({'i':1,'j':2}), (NULL))\").df()\n-        duck_values = df_duck['a']\n-        assert duck_values[0] == {'i': 1, 'j': 2}\n-        assert np.isnan(duck_values[1])\n-        assert duck_values[2] == {'i': 1, 'j': 2}\n-        assert np.isnan(duck_values[3])\n-\n-        # MAP that is NULL entirely\n-        df_duck = con.query(\n-            \"SELECT col0 as a FROM (VALUES (MAP(LIST_VALUE(1,2),LIST_VALUE(3,4))),(NULL), (MAP(LIST_VALUE(1,2),LIST_VALUE(3,4))), (NULL))\"\n-        ).df()\n-        duck_values = df_duck['a']\n-        assert duck_values[0] == {'key': [1, 2], 'value': [3, 4]}\n-        assert np.isnan(duck_values[1])\n-        assert duck_values[2] == {'key': [1, 2], 'value': [3, 4]}\n-        assert np.isnan(duck_values[3])\n+            ]\n+        }, \"\"),\n+        (\"SELECT a from (select MAP(LIST_VALUE(1, 2, 3, 4,2, NULL),LIST_VALUE(10, 9, 8, 7,11,42)) as a) as t\", {\n+            'a': [\n+                {\n+                    'key': [1, 2, 3, 4, 2, None],\n+                    'value': [10, 9, 8, 7, 11, 42]\n+                }\n+            ]\n+        }, \"Map keys must be unique\"),\n+        (\"SELECT a from (select MAP(LIST_VALUE('Jon Lajoie', 'Backstreet Boys', 'Tenacious D','Jon Lajoie' ),LIST_VALUE(10,9,10,11)) as a) as t\", {\n+            'a': [\n+                {\n+                    'key': ['Jon Lajoie', 'Backstreet Boys', 'Tenacious D', 'Jon Lajoie'],\n+                    'value': [10, 9, 10, 11]\n+                }\n+            ]\n+        }, \"Map keys must be unique\"),\n+        (\"SELECT a from (select MAP(LIST_VALUE('Jon Lajoie', NULL, 'Tenacious D',NULL,NULL ),LIST_VALUE(10,9,10,11,13)) as a) as t\", {\n+            'a': [\n+                {\n+                    'key': ['Jon Lajoie', None, 'Tenacious D', None, None],\n+                    'value': [10, 9, 10, 11, 13]\n+                }\n+            ]\n+        }, \"Map keys can not be NULL\"),\n+        (\"SELECT a from (select MAP(LIST_VALUE(NULL, NULL, NULL,NULL,NULL ),LIST_VALUE(10,9,10,11,13)) as a) as t\", {\n+            'a': [\n+                {\n+                    'key': [None, None, None, None, None],\n+                    'value': [10, 9, 10, 11, 13]\n+                }\n+            ]\n+        }, \"Map keys can not be NULL\"),\n+        (\"SELECT a from (select MAP(LIST_VALUE(NULL, NULL, NULL,NULL,NULL ),LIST_VALUE(NULL, NULL, NULL,NULL,NULL )) as a) as t\", {\n+            'a': [\n+                {\n+                    'key': [None, None, None, None, None],\n+                    'value': [None, None, None, None, None]\n+                }\n+            ]\n+        }, \"Map keys can not be NULL\"),\n+    ])\n+    # fmt: on\n+    def test_map_df(self, duckdb_cursor, query, expected, expected_error):\n+        if not expected_error:\n+            compare_results(duckdb_cursor, query, expected)\n+        else:\n+            with pytest.raises(duckdb.InvalidInputException, match=expected_error):\n+                compare_results(duckdb_cursor, query, expected)\n+\n+    # fmt: off\n+    @pytest.mark.parametrize('query, expected', [\n+        (\"\"\"\n+            SELECT [\n+                {'i':1,'j':2},\n+                NULL,\n+                {'i':2,'j':NULL}\n+            ] as a\n+        \"\"\", {\n+            'a': [\n+                np.ma.array([\n+                    {'i': 1, 'j': 2},\n+                    None,\n+                    {'i': 2, 'j': None}\n+                ], mask=[0, 1, 0])\n+            ]\n+        }),\n+        (\"\"\"\n+            SELECT [{'i':1,'j':2},NULL,{'i':2,'j':NULL}] as a\n+        \"\"\", {\n+            'a': [\n+                np.ma.array([\n+                    {'i': 1, 'j': 2},\n+                    None,\n+                    {'i': 2, 'j': None}\n+                ], mask=[0, 1, 0])\n+            ]\n+        }),\n+        (\"\"\"\n+            SELECT [{'i':1,'j':[2,3]},NULL] as a\n+        \"\"\", {\n+            'a': [\n+                np.ma.array([\n+                    {'i': 1, 'j': [2, 3]},\n+                    None,\n+                ], mask=[0, 1])\n+            ]\n+        }),\n+        (\"\"\"\n+            SELECT {'i':mp,'j':mp2} as a FROM (SELECT MAP(LIST_VALUE(1, 2, 3, 4),LIST_VALUE(10, 9, 8, 7)) as mp, MAP(LIST_VALUE(1, 2, 3, 5),LIST_VALUE(10, 9, 8, 7)) as mp2) as t\n+        \"\"\", {\n+            'a': [\n+                {\n+                    'i': {'key': [1, 2, 3, 4], 'value': [10, 9, 8, 7]},\n+                    'j': {'key': [1, 2, 3, 5], 'value': [10, 9, 8, 7]}\n+                }\n+            ]\n+        }),\n+        (\"\"\"\n+            SELECT [mp,mp2] as a FROM (SELECT MAP(LIST_VALUE(1, 2, 3, 4),LIST_VALUE(10, 9, 8, 7)) as mp, MAP(LIST_VALUE(1, 2, 3, 5),LIST_VALUE(10, 9, 8, 7)) as mp2) as t\n+        \"\"\", {\n+            'a': [\n+                [{'key': [1, 2, 3, 4], 'value': [10, 9, 8, 7]}, {'key': [1, 2, 3, 5], 'value': [10, 9, 8, 7]}]\n+            ]\n+        }),\n+        (\"\"\"\n+            SELECT MAP(LIST_VALUE([1,2],[3,4],[5,4]),LIST_VALUE([1,2],[3,4],[5,4])) as a\n+        \"\"\", {\n+            'a': [\n+                {'key': [[1, 2], [3, 4], [5, 4]], 'value': [[1, 2], [3, 4], [5, 4]]}\n+            ]\n+        }),\n+        (\"\"\"\n+            SELECT MAP(LIST_VALUE({'i':1,'j':2},{'i':3,'j':4}),LIST_VALUE({'i':1,'j':2},{'i':3,'j':4})) as a\n+        \"\"\", {\n+            'a': [\n+                {'key': [{'i': 1, 'j': 2}, {'i': 3, 'j': 4}], 'value': [{'i': 1, 'j': 2}, {'i': 3, 'j': 4}]}\n+            ]\n+        }),\n+        (\"\"\"\n+            SELECT [{'i':1,'j':[2,3]},NULL,{'i':1,'j':[2,3]}] as a\n+        \"\"\", {\n+            'a': [\n+                np.ma.array([\n+                    {'i': 1, 'j': [2, 3]},\n+                    None,\n+                    {'i': 1, 'j': [2, 3]}\n+                ], mask=[0, 1, 0])\n+            ]\n+        }),\n+        (\"\"\"\n+            SELECT * FROM (VALUES\n+            ({'i':1,'j':2}),\n+            ({'i':1,'j':2}),\n+            (NULL),\n+            (NULL)\n+        ) t(a)\n+        \"\"\", {\n+            'a': [\n+                {'i': 1, 'j': 2},\n+                {'i': 1, 'j': 2},\n+                np.nan,\n+                np.nan\n+            ]\n+        }),\n+        (\"\"\"\n+            SELECT a FROM (VALUES\n+                (MAP(LIST_VALUE(1,2),LIST_VALUE(3,4))),\n+                (MAP(LIST_VALUE(1,2),LIST_VALUE(3,4))),\n+                (NULL),\n+                (NULL)\n+            ) t(a)\n+        \"\"\", {\n+            'a': [\n+                {'key': [1, 2], 'value': [3, 4]},\n+                {'key': [1, 2], 'value': [3, 4]},\n+                np.nan,\n+                np.nan\n+            ]\n+        }),\n+    ])\n+    # fmt: on\n+    def test_nested_mix(self, duckdb_cursor, query, expected):\n+        compare_results(duckdb_cursor, query, expected)\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_pandas_update.py b/tools/pythonpkg/tests/fast/pandas/test_pandas_update.py\nindex d12fa72781e3..663d6da201f7 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_pandas_update.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_pandas_update.py\n@@ -8,4 +8,6 @@ def test_pandas_update_list(self, duckdb_cursor):\n         duckdb_cursor.execute('create table t (l int[])')\n         duckdb_cursor.execute('insert into t values ([1, 2]), ([3,4])')\n         duckdb_cursor.execute('update t set l = [5, 6]')\n-        assert duckdb_cursor.execute('select * from t').fetchdf()['l'].tolist() == [[5, 6], [5, 6]]\n+        expected = pd.DataFrame({'l': [[5, 6], [5, 6]]})\n+        res = duckdb_cursor.execute('select * from t').fetchdf()\n+        pd.testing.assert_frame_equal(expected, res)\ndiff --git a/tools/pythonpkg/tests/fast/test_all_types.py b/tools/pythonpkg/tests/fast/test_all_types.py\nindex 89a0f67b1267..4a2ceb105a3e 100644\n--- a/tools/pythonpkg/tests/fast/test_all_types.py\n+++ b/tools/pythonpkg/tests/fast/test_all_types.py\n@@ -9,10 +9,22 @@\n import pytest\n \n \n+def replace_with_ndarray(obj):\n+    if hasattr(obj, '__getitem__'):\n+        if isinstance(obj, dict):\n+            for key, value in obj.items():\n+                obj[key] = replace_with_ndarray(value)\n+        elif isinstance(obj, list):\n+            for i, item in enumerate(obj):\n+                obj[i] = replace_with_ndarray(item)\n+        return np.array(obj)\n+    return obj\n+\n+\n # we need to write our own equality function that considers nan==nan for testing purposes\n def recursive_equality(o1, o2):\n-    if o1 == o2:\n-        return True\n+    import math\n+\n     if type(o1) != type(o2):\n         return False\n     if type(o1) == float and math.isnan(o1) and math.isnan(o2):\n@@ -27,7 +39,7 @@ def recursive_equality(o1, o2):\n                 return False\n         return True\n     except:\n-        return False\n+        return o1 == o2\n \n \n # Regenerate the 'all_types' list using:\n@@ -481,6 +493,7 @@ def test_fetchnumpy(self, cur_type):\n             ),\n             'union': np.ma.array(['Frank', 5, None], mask=[0, 0, 1], dtype=object),\n         }\n+        correct_answer_map = replace_with_ndarray(correct_answer_map)\n \n         # The following types don't have a numpy equivalent, and are coerced to\n         # floating point types by fetchnumpy():\n",
  "problem_statement": "High memory usage when reading boolean list column from parquet\n### What happens?\n\nMemory usage explodes when I try to materialize a 100k row BOOL[] column(ie each row value is [ False, True....1000 elements]) as a pandas dataframe from parquet files. \r\nAt same time, if I try to materialize the column as a arrow table first and then convert it to a pandas dataframe, memory usage doesn't spike.\r\n\n\n### To Reproduce\n\n```\r\n parquet_path = '<path>'\r\n\r\n query = \"select \\\"boolean_list_column\\\" from read_parquet('{}/*.parquet')\".format(parquet_path)\r\n#memory usage explodes to like 30GB\r\n column = duckdb.sql(query).df()\r\n\r\n#memory usage stays level around 2GB\r\n column = duckdb.sql(query).arrow().to_pandas()\r\n```\n\n### OS:\n\nmac\n\n### DuckDB Version:\n\n0.8.1\n\n### DuckDB Client:\n\npython\n\n### Full Name:\n\nPaul Cherian\n\n### Affiliation:\n\nqualtrics\n\n### Have you tried this on the latest `master` branch?\n\nI have tested with a master build\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] Yes, I have\n",
  "hints_text": "Hi @PaulCherian, thanks for reporting this. Could you please provide a script that generates a Parquet file that reproduces this issue?\nYou can use the snippet below to generate the parquet files\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nparquet_file_path = '/tmp/test.parquet'\r\n\r\nrows = 1000000\r\nwidth = 1000\r\ncolumn = np.random.choice([False, True], size=(rows, width))\r\ndf = pd.DataFrame({'bool_list_column': np.zeros(rows, dtype=object)})\r\nfor i in range(rows):\r\n    df.iloc[i, 0] = column[i]\r\ndf.to_parquet(parquet_file_path)\r\n```\nHere is the memory usage graph I generated using `memray` when running\r\n```python\r\nimport duckdb\r\nimport time\r\n\r\nparquet_file_path = '/tmp/test.parquet'\r\n\r\nprint('reading parquet using duckdb')\r\nstart = time.time()\r\nquery = \"select bool_list_column from read_parquet('{}')\".format(\r\n    parquet_file_path)\r\ncheckbox = duckdb.sql(query).df()\r\nprint('took {} seconds'.format(time.time() - start))\r\n```\r\nThe whole process took ~200s to run\r\n![newplot (16)](https://github.com/duckdb/duckdb/assets/13074900/f5efe72f-61d2-4344-a806-313bf259ae45)\r\n\r\n\nHere is the memory usage graph I generated using memray when running\r\n```\r\nprint('reading parquet using duckdb')\r\nstart = time.time()\r\nquery = \"select bool_list_column from read_parquet('{}')\".format(\r\n    parquet_file_path)\r\ncheckbox = duckdb.sql(query).arrow().to_pandas()\r\nprint('took {} seconds'.format(time.time() - start))\r\n```\r\nThe whole process took only ~20s(significantly faster than .df())\r\n![newplot (15)](https://github.com/duckdb/duckdb/assets/13074900/a354dbba-7012-47d8-b66e-cbeeafb845d8)\r\n\nThis issue is stale because it has been open 90 days with no activity. Remove stale label or comment or this will be closed in 30 days.\nThis issue was closed because it has been stale for 30 days with no activity.",
  "created_at": "2024-02-23T17:00:37Z"
}