You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Nested selecting max from column values gives dependent join internal error
#### What happens?

The reproduction query results in an internal error.

```
Error: INTERNAL Error: Logical operator type "EXPRESSION_GET" for dependent join
```

#### To Reproduce

```sql
CREATE TABLE test AS SELECT (i % 100)::INTEGER AS i, ((i * 2) % 100)::INTEGER AS j FROM range(0, 100000) tbl(i);
SELECT i, j, (SELECT max(x) FROM (VALUES (i), (j)) AS X(x)) as maxn FROM test;
```

#### Environment (please complete the following information):
 - OS: Windows 10
 - DuckDB Version: 0.3.1
 - DuckDB Client: CLI

#### Before Submitting

- [x] **Have you tried this on the latest `master` branch?**
* **Python**: `pip install duckdb --upgrade --pre`
* **R**: `install.packages("https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz", repos = NULL)`
* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.

- [x] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**


</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16: </p>
17: 
18: ## DuckDB
19: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/docs/why_duckdb.html).
20: 
21: ## Installation
22: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
23: 
24: ## Data Import
25: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
26: 
27: ```sql
28: SELECT * FROM 'myfile.csv';
29: SELECT * FROM 'myfile.parquet';
30: ```
31: 
32: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
33: 
34: ## SQL Reference
35: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
36: 
37: ## Development
38: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
39: 
40: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
41: 
42: 
[end of README.md]
[start of src/execution/aggregate_hashtable.cpp]
1: #include "duckdb/execution/aggregate_hashtable.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/aggregate_function_catalog_entry.hpp"
4: #include "duckdb/common/algorithm.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/operator/comparison_operators.hpp"
7: #include "duckdb/common/row_operations/row_operations.hpp"
8: #include "duckdb/common/types/null_value.hpp"
9: #include "duckdb/common/types/row_data_collection.hpp"
10: #include "duckdb/common/vector_operations/unary_executor.hpp"
11: #include "duckdb/common/vector_operations/vector_operations.hpp"
12: #include "duckdb/execution/expression_executor.hpp"
13: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
14: #include "duckdb/storage/buffer_manager.hpp"
15: 
16: #include <cmath>
17: #include <map>
18: 
19: namespace duckdb {
20: 
21: using ValidityBytes = RowLayout::ValidityBytes;
22: 
23: GroupedAggregateHashTable::GroupedAggregateHashTable(BufferManager &buffer_manager, vector<LogicalType> group_types,
24:                                                      vector<LogicalType> payload_types,
25:                                                      const vector<BoundAggregateExpression *> &bindings,
26:                                                      HtEntryType entry_type)
27:     : GroupedAggregateHashTable(buffer_manager, move(group_types), move(payload_types),
28:                                 AggregateObject::CreateAggregateObjects(bindings), entry_type) {
29: }
30: 
31: GroupedAggregateHashTable::GroupedAggregateHashTable(BufferManager &buffer_manager, vector<LogicalType> group_types)
32:     : GroupedAggregateHashTable(buffer_manager, move(group_types), {}, vector<AggregateObject>()) {
33: }
34: 
35: GroupedAggregateHashTable::GroupedAggregateHashTable(BufferManager &buffer_manager, vector<LogicalType> group_types_p,
36:                                                      vector<LogicalType> payload_types_p,
37:                                                      vector<AggregateObject> aggregate_objects_p,
38:                                                      HtEntryType entry_type)
39:     : BaseAggregateHashTable(buffer_manager, move(payload_types_p)), entry_type(entry_type), capacity(0), entries(0),
40:       payload_page_offset(0), is_finalized(false), ht_offsets(LogicalTypeId::BIGINT),
41:       hash_salts(LogicalTypeId::SMALLINT), group_compare_vector(STANDARD_VECTOR_SIZE),
42:       no_match_vector(STANDARD_VECTOR_SIZE), empty_vector(STANDARD_VECTOR_SIZE) {
43: 
44: 	// Append hash column to the end and initialise the row layout
45: 	group_types_p.emplace_back(LogicalType::HASH);
46: 	layout.Initialize(move(group_types_p), move(aggregate_objects_p));
47: 
48: 	// HT layout
49: 	hash_offset = layout.GetOffsets()[layout.ColumnCount() - 1];
50: 
51: 	tuple_size = layout.GetRowWidth();
52: 
53: 	D_ASSERT(tuple_size <= Storage::BLOCK_SIZE);
54: 	tuples_per_block = Storage::BLOCK_SIZE / tuple_size;
55: 	hashes_hdl = buffer_manager.Allocate(Storage::BLOCK_SIZE);
56: 	hashes_hdl_ptr = hashes_hdl->Ptr();
57: 
58: 	switch (entry_type) {
59: 	case HtEntryType::HT_WIDTH_64: {
60: 		hash_prefix_shift = (HASH_WIDTH - sizeof(aggr_ht_entry_64::salt)) * 8;
61: 		Resize<aggr_ht_entry_64>(STANDARD_VECTOR_SIZE * 2);
62: 		break;
63: 	}
64: 	case HtEntryType::HT_WIDTH_32: {
65: 		hash_prefix_shift = (HASH_WIDTH - sizeof(aggr_ht_entry_32::salt)) * 8;
66: 		Resize<aggr_ht_entry_32>(STANDARD_VECTOR_SIZE * 2);
67: 		break;
68: 	}
69: 	default:
70: 		throw InternalException("Unknown HT entry width");
71: 	}
72: 
73: 	// create additional hash tables for distinct aggrs
74: 	auto &aggregates = layout.GetAggregates();
75: 	distinct_hashes.resize(aggregates.size());
76: 
77: 	idx_t payload_idx = 0;
78: 	for (idx_t i = 0; i < aggregates.size(); i++) {
79: 		auto &aggr = aggregates[i];
80: 		if (aggr.distinct) {
81: 			// layout types minus hash column plus aggr return type
82: 			vector<LogicalType> distinct_group_types(layout.GetTypes());
83: 			(void)distinct_group_types.pop_back();
84: 			for (idx_t child_idx = 0; child_idx < aggr.child_count; child_idx++) {
85: 				distinct_group_types.push_back(payload_types[payload_idx]);
86: 			}
87: 			distinct_hashes[i] = make_unique<GroupedAggregateHashTable>(buffer_manager, distinct_group_types);
88: 		}
89: 		payload_idx += aggr.child_count;
90: 	}
91: 	predicates.resize(layout.ColumnCount() - 1, ExpressionType::COMPARE_EQUAL);
92: 	string_heap = make_unique<RowDataCollection>(buffer_manager, (idx_t)Storage::BLOCK_SIZE, 1, true);
93: }
94: 
95: GroupedAggregateHashTable::~GroupedAggregateHashTable() {
96: 	Destroy();
97: }
98: 
99: template <class FUNC>
100: void GroupedAggregateHashTable::PayloadApply(FUNC fun) {
101: 	if (entries == 0) {
102: 		return;
103: 	}
104: 	idx_t apply_entries = entries;
105: 	idx_t page_nr = 0;
106: 	idx_t page_offset = 0;
107: 
108: 	for (auto &payload_chunk_ptr : payload_hds_ptrs) {
109: 		auto this_entries = MinValue(tuples_per_block, apply_entries);
110: 		page_offset = 0;
111: 		for (data_ptr_t ptr = payload_chunk_ptr, end = payload_chunk_ptr + this_entries * tuple_size; ptr < end;
112: 		     ptr += tuple_size) {
113: 			fun(page_nr, page_offset++, ptr);
114: 		}
115: 		apply_entries -= this_entries;
116: 		page_nr++;
117: 	}
118: 	D_ASSERT(apply_entries == 0);
119: }
120: 
121: void GroupedAggregateHashTable::NewBlock() {
122: 	auto pin = buffer_manager.Allocate(Storage::BLOCK_SIZE);
123: 	payload_hds.push_back(move(pin));
124: 	payload_hds_ptrs.push_back(payload_hds.back()->Ptr());
125: 	payload_page_offset = 0;
126: }
127: 
128: void GroupedAggregateHashTable::Destroy() {
129: 	// check if there is a destructor
130: 	bool has_destructor = false;
131: 	for (auto &aggr : layout.GetAggregates()) {
132: 		if (aggr.function.destructor) {
133: 			has_destructor = true;
134: 		}
135: 	}
136: 	if (!has_destructor) {
137: 		return;
138: 	}
139: 	// there are aggregates with destructors: loop over the hash table
140: 	// and call the destructor method for each of the aggregates
141: 	data_ptr_t data_pointers[STANDARD_VECTOR_SIZE];
142: 	Vector state_vector(LogicalType::POINTER, (data_ptr_t)data_pointers);
143: 	idx_t count = 0;
144: 
145: 	PayloadApply([&](idx_t page_nr, idx_t page_offset, data_ptr_t ptr) {
146: 		data_pointers[count++] = ptr;
147: 		if (count == STANDARD_VECTOR_SIZE) {
148: 			RowOperations::DestroyStates(layout, state_vector, count);
149: 			count = 0;
150: 		}
151: 	});
152: 	RowOperations::DestroyStates(layout, state_vector, count);
153: }
154: 
155: template <class ENTRY>
156: void GroupedAggregateHashTable::VerifyInternal() {
157: 	auto hashes_ptr = (ENTRY *)hashes_hdl_ptr;
158: 	D_ASSERT(payload_hds.size() == payload_hds_ptrs.size());
159: 	idx_t count = 0;
160: 	for (idx_t i = 0; i < capacity; i++) {
161: 		if (hashes_ptr[i].page_nr > 0) {
162: 			D_ASSERT(hashes_ptr[i].page_offset < tuples_per_block);
163: 			D_ASSERT(hashes_ptr[i].page_nr <= payload_hds.size());
164: 			auto ptr = payload_hds_ptrs[hashes_ptr[i].page_nr - 1] + ((hashes_ptr[i].page_offset) * tuple_size);
165: 			auto hash = Load<hash_t>(ptr + hash_offset);
166: 			D_ASSERT((hashes_ptr[i].salt) == (hash >> hash_prefix_shift));
167: 
168: 			count++;
169: 		}
170: 	}
171: 	D_ASSERT(count == entries);
172: }
173: 
174: idx_t GroupedAggregateHashTable::MaxCapacity() {
175: 	idx_t max_pages = 0;
176: 	idx_t max_tuples = 0;
177: 
178: 	switch (entry_type) {
179: 	case HtEntryType::HT_WIDTH_32:
180: 		max_pages = NumericLimits<uint8_t>::Maximum();
181: 		max_tuples = NumericLimits<uint16_t>::Maximum();
182: 		break;
183: 	default:
184: 		D_ASSERT(entry_type == HtEntryType::HT_WIDTH_64);
185: 		max_pages = NumericLimits<uint32_t>::Maximum();
186: 		max_tuples = NumericLimits<uint16_t>::Maximum();
187: 		break;
188: 	}
189: 
190: 	return max_pages * MinValue(max_tuples, (idx_t)Storage::BLOCK_SIZE / tuple_size);
191: }
192: 
193: void GroupedAggregateHashTable::Verify() {
194: #ifdef DEBUG
195: 	switch (entry_type) {
196: 	case HtEntryType::HT_WIDTH_32:
197: 		VerifyInternal<aggr_ht_entry_32>();
198: 		break;
199: 	case HtEntryType::HT_WIDTH_64:
200: 		VerifyInternal<aggr_ht_entry_64>();
201: 		break;
202: 	}
203: #endif
204: }
205: 
206: template <class ENTRY>
207: void GroupedAggregateHashTable::Resize(idx_t size) {
208: 	Verify();
209: 
210: 	D_ASSERT(!is_finalized);
211: 
212: 	if (size <= capacity) {
213: 		throw InternalException("Cannot downsize a hash table!");
214: 	}
215: 	D_ASSERT(size >= STANDARD_VECTOR_SIZE);
216: 
217: 	// size needs to be a power of 2
218: 	D_ASSERT((size & (size - 1)) == 0);
219: 	bitmask = size - 1;
220: 
221: 	auto byte_size = size * sizeof(ENTRY);
222: 	if (byte_size > (idx_t)Storage::BLOCK_SIZE) {
223: 		hashes_hdl = buffer_manager.Allocate(byte_size);
224: 		hashes_hdl_ptr = hashes_hdl->Ptr();
225: 	}
226: 	memset(hashes_hdl_ptr, 0, byte_size);
227: 	hashes_end_ptr = hashes_hdl_ptr + byte_size;
228: 	capacity = size;
229: 
230: 	auto hashes_arr = (ENTRY *)hashes_hdl_ptr;
231: 
232: 	PayloadApply([&](idx_t page_nr, idx_t page_offset, data_ptr_t ptr) {
233: 		auto hash = Load<hash_t>(ptr + hash_offset);
234: 		D_ASSERT((hash & bitmask) == (hash % capacity));
235: 		auto entry_idx = (idx_t)hash & bitmask;
236: 		while (hashes_arr[entry_idx].page_nr > 0) {
237: 			entry_idx++;
238: 			if (entry_idx >= capacity) {
239: 				entry_idx = 0;
240: 			}
241: 		}
242: 
243: 		D_ASSERT(!hashes_arr[entry_idx].page_nr);
244: 		D_ASSERT(hash >> hash_prefix_shift <= NumericLimits<uint16_t>::Maximum());
245: 
246: 		hashes_arr[entry_idx].salt = hash >> hash_prefix_shift;
247: 		hashes_arr[entry_idx].page_nr = page_nr + 1;
248: 		hashes_arr[entry_idx].page_offset = page_offset;
249: 	});
250: 
251: 	Verify();
252: }
253: 
254: idx_t GroupedAggregateHashTable::AddChunk(DataChunk &groups, DataChunk &payload) {
255: 	Vector hashes(LogicalType::HASH);
256: 	groups.Hash(hashes);
257: 
258: 	return AddChunk(groups, hashes, payload);
259: }
260: 
261: idx_t GroupedAggregateHashTable::AddChunk(DataChunk &groups, Vector &group_hashes, DataChunk &payload) {
262: 	D_ASSERT(!is_finalized);
263: 
264: 	if (groups.size() == 0) {
265: 		return 0;
266: 	}
267: 	// dummy
268: 	SelectionVector new_groups(STANDARD_VECTOR_SIZE);
269: 
270: 	D_ASSERT(groups.ColumnCount() + 1 == layout.ColumnCount());
271: 	for (idx_t i = 0; i < groups.ColumnCount(); i++) {
272: 		D_ASSERT(groups.GetTypes()[i] == layout.GetTypes()[i]);
273: 	}
274: 
275: 	Vector addresses(LogicalType::POINTER);
276: 	auto new_group_count = FindOrCreateGroups(groups, group_hashes, addresses, new_groups);
277: 	VectorOperations::AddInPlace(addresses, layout.GetAggrOffset(), payload.size());
278: 
279: 	// now every cell has an entry
280: 	// update the aggregates
281: 	idx_t payload_idx = 0;
282: 
283: 	auto &aggregates = layout.GetAggregates();
284: 	for (idx_t aggr_idx = 0; aggr_idx < aggregates.size(); aggr_idx++) {
285: 		// for any entries for which a group was found, update the aggregate
286: 		auto &aggr = aggregates[aggr_idx];
287: 		if (aggr.distinct) {
288: 			// construct chunk for secondary hash table probing
289: 			vector<LogicalType> probe_types(groups.GetTypes());
290: 			for (idx_t i = 0; i < aggr.child_count; i++) {
291: 				probe_types.push_back(payload_types[payload_idx]);
292: 			}
293: 			DataChunk probe_chunk;
294: 			probe_chunk.Initialize(probe_types);
295: 			for (idx_t group_idx = 0; group_idx < groups.ColumnCount(); group_idx++) {
296: 				probe_chunk.data[group_idx].Reference(groups.data[group_idx]);
297: 			}
298: 			for (idx_t i = 0; i < aggr.child_count; i++) {
299: 				probe_chunk.data[groups.ColumnCount() + i].Reference(payload.data[payload_idx + i]);
300: 			}
301: 			probe_chunk.SetCardinality(groups);
302: 			probe_chunk.Verify();
303: 
304: 			Vector dummy_addresses(LogicalType::POINTER);
305: 			// this is the actual meat, find out which groups plus payload
306: 			// value have not been seen yet
307: 			idx_t new_group_count =
308: 			    distinct_hashes[aggr_idx]->FindOrCreateGroups(probe_chunk, dummy_addresses, new_groups);
309: 
310: 			// now fix up the payload and addresses accordingly by creating
311: 			// a selection vector
312: 			if (new_group_count > 0) {
313: 				if (aggr.filter) {
314: 					Vector distinct_addresses(addresses, new_groups, new_group_count);
315: 					DataChunk distinct_payload;
316: 					auto pay_types = payload.GetTypes();
317: 					distinct_payload.Initialize(pay_types);
318: 					distinct_payload.Slice(payload, new_groups, new_group_count);
319: 					distinct_addresses.Verify(new_group_count);
320: 					distinct_addresses.Normalify(new_group_count);
321: 					RowOperations::UpdateFilteredStates(aggr, distinct_addresses, distinct_payload, payload_idx);
322: 				} else {
323: 					Vector distinct_addresses(addresses, new_groups, new_group_count);
324: 					for (idx_t i = 0; i < aggr.child_count; i++) {
325: 						payload.data[payload_idx + i].Slice(new_groups, new_group_count);
326: 						payload.data[payload_idx + i].Verify(new_group_count);
327: 					}
328: 					distinct_addresses.Verify(new_group_count);
329: 
330: 					RowOperations::UpdateStates(aggr, distinct_addresses, payload, payload_idx, new_group_count);
331: 				}
332: 			}
333: 		} else if (aggr.filter) {
334: 			RowOperations::UpdateFilteredStates(aggr, addresses, payload, payload_idx);
335: 		} else {
336: 			RowOperations::UpdateStates(aggr, addresses, payload, payload_idx, payload.size());
337: 		}
338: 
339: 		// move to the next aggregate
340: 		payload_idx += aggr.child_count;
341: 		VectorOperations::AddInPlace(addresses, aggr.payload_size, payload.size());
342: 	}
343: 
344: 	Verify();
345: 	return new_group_count;
346: }
347: 
348: void GroupedAggregateHashTable::FetchAggregates(DataChunk &groups, DataChunk &result) {
349: 	groups.Verify();
350: 	D_ASSERT(groups.ColumnCount() + 1 == layout.ColumnCount());
351: 	for (idx_t i = 0; i < result.ColumnCount(); i++) {
352: 		D_ASSERT(result.data[i].GetType() == payload_types[i]);
353: 	}
354: 	result.SetCardinality(groups);
355: 	if (groups.size() == 0) {
356: 		return;
357: 	}
358: 	// find the groups associated with the addresses
359: 	// FIXME: this should not use the FindOrCreateGroups, creating them is unnecessary
360: 	Vector addresses(LogicalType::POINTER);
361: 	FindOrCreateGroups(groups, addresses);
362: 	// now fetch the aggregates
363: 	RowOperations::FinalizeStates(layout, addresses, result, 0);
364: }
365: 
366: template <class ENTRY>
367: idx_t GroupedAggregateHashTable::FindOrCreateGroupsInternal(DataChunk &groups, Vector &group_hashes, Vector &addresses,
368:                                                             SelectionVector &new_groups_out) {
369: 	D_ASSERT(!is_finalized);
370: 
371: 	if (entries + groups.size() > MaxCapacity()) {
372: 		throw InternalException("Hash table capacity reached");
373: 	}
374: 
375: 	// resize at 50% capacity, also need to fit the entire vector
376: 	if (capacity - entries <= groups.size() || entries > capacity / LOAD_FACTOR) {
377: 		Resize<ENTRY>(capacity * 2);
378: 	}
379: 
380: 	D_ASSERT(capacity - entries >= groups.size());
381: 	D_ASSERT(groups.ColumnCount() + 1 == layout.ColumnCount());
382: 	// we need to be able to fit at least one vector of data
383: 	D_ASSERT(capacity - entries >= groups.size());
384: 	D_ASSERT(group_hashes.GetType() == LogicalType::HASH);
385: 
386: 	group_hashes.Normalify(groups.size());
387: 	auto group_hashes_ptr = FlatVector::GetData<hash_t>(group_hashes);
388: 
389: 	D_ASSERT(ht_offsets.GetVectorType() == VectorType::FLAT_VECTOR);
390: 	D_ASSERT(ht_offsets.GetType() == LogicalType::BIGINT);
391: 
392: 	D_ASSERT(addresses.GetType() == LogicalType::POINTER);
393: 	addresses.Normalify(groups.size());
394: 	auto addresses_ptr = FlatVector::GetData<data_ptr_t>(addresses);
395: 
396: 	// now compute the entry in the table based on the hash using a modulo
397: 	UnaryExecutor::Execute<hash_t, uint64_t>(group_hashes, ht_offsets, groups.size(), [&](hash_t element) {
398: 		D_ASSERT((element & bitmask) == (element % capacity));
399: 		return (element & bitmask);
400: 	});
401: 	auto ht_offsets_ptr = FlatVector::GetData<uint64_t>(ht_offsets);
402: 
403: 	// precompute the hash salts for faster comparison below
404: 	D_ASSERT(hash_salts.GetType() == LogicalType::SMALLINT);
405: 	UnaryExecutor::Execute<hash_t, uint16_t>(group_hashes, hash_salts, groups.size(),
406: 	                                         [&](hash_t element) { return (element >> hash_prefix_shift); });
407: 	auto hash_salts_ptr = FlatVector::GetData<uint16_t>(hash_salts);
408: 
409: 	// we start out with all entries [0, 1, 2, ..., groups.size()]
410: 	const SelectionVector *sel_vector = &FlatVector::INCREMENTAL_SELECTION_VECTOR;
411: 
412: 	idx_t remaining_entries = groups.size();
413: 
414: 	// make a chunk that references the groups and the hashes
415: 	DataChunk group_chunk;
416: 	group_chunk.InitializeEmpty(layout.GetTypes());
417: 	for (idx_t grp_idx = 0; grp_idx < groups.ColumnCount(); grp_idx++) {
418: 		group_chunk.data[grp_idx].Reference(groups.data[grp_idx]);
419: 	}
420: 	group_chunk.data[groups.ColumnCount()].Reference(group_hashes);
421: 	group_chunk.SetCardinality(groups);
422: 
423: 	// orrify all the groups
424: 	auto group_data = group_chunk.Orrify();
425: 
426: 	idx_t new_group_count = 0;
427: 	while (remaining_entries > 0) {
428: 		idx_t new_entry_count = 0;
429: 		idx_t need_compare_count = 0;
430: 		idx_t no_match_count = 0;
431: 
432: 		// first figure out for each remaining whether or not it belongs to a full or empty group
433: 		for (idx_t i = 0; i < remaining_entries; i++) {
434: 			const idx_t index = sel_vector->get_index(i);
435: 			const auto ht_entry_ptr = ((ENTRY *)this->hashes_hdl_ptr) + ht_offsets_ptr[index];
436: 			if (ht_entry_ptr->page_nr == 0) { // we use page number 0 as a "unused marker"
437: 				// cell is empty; setup the new entry
438: 				if (payload_page_offset == tuples_per_block || payload_hds.empty()) {
439: 					NewBlock();
440: 				}
441: 
442: 				auto entry_payload_ptr = payload_hds_ptrs.back() + (payload_page_offset * tuple_size);
443: 
444: 				D_ASSERT(group_hashes_ptr[index] >> hash_prefix_shift <= NumericLimits<uint16_t>::Maximum());
445: 				D_ASSERT(payload_page_offset < tuples_per_block);
446: 				D_ASSERT(payload_hds.size() < NumericLimits<uint32_t>::Maximum());
447: 				D_ASSERT(payload_page_offset + 1 < NumericLimits<uint16_t>::Maximum());
448: 
449: 				ht_entry_ptr->salt = group_hashes_ptr[index] >> hash_prefix_shift;
450: 
451: 				// page numbers start at one so we can use 0 as empty flag
452: 				// GetPtr undoes this
453: 				ht_entry_ptr->page_nr = payload_hds.size();
454: 				ht_entry_ptr->page_offset = payload_page_offset++;
455: 
456: 				// update selection lists for outer loops
457: 				empty_vector.set_index(new_entry_count++, index);
458: 				new_groups_out.set_index(new_group_count++, index);
459: 				entries++;
460: 
461: 				addresses_ptr[index] = entry_payload_ptr;
462: 
463: 			} else {
464: 				// cell is occupied: add to check list
465: 				// only need to check if hash salt in ptr == prefix of hash in payload
466: 				if (ht_entry_ptr->salt == hash_salts_ptr[index]) {
467: 					group_compare_vector.set_index(need_compare_count++, index);
468: 
469: 					auto page_ptr = payload_hds_ptrs[ht_entry_ptr->page_nr - 1];
470: 					auto page_offset = ht_entry_ptr->page_offset * tuple_size;
471: 					addresses_ptr[index] = page_ptr + page_offset;
472: 
473: 				} else {
474: 					no_match_vector.set_index(no_match_count++, index);
475: 				}
476: 			}
477: 		}
478: 
479: 		// for each of the locations that are empty, serialize the group columns to the locations
480: 		RowOperations::Scatter(group_chunk, group_data.get(), layout, addresses, *string_heap, empty_vector,
481: 		                       new_entry_count);
482: 		RowOperations::InitializeStates(layout, addresses, empty_vector, new_entry_count);
483: 
484: 		// now we have only the tuples remaining that might match to an existing group
485: 		// start performing comparisons with each of the groups
486: 		RowOperations::Match(group_chunk, group_data.get(), layout, addresses, predicates, group_compare_vector,
487: 		                     need_compare_count, &no_match_vector, no_match_count);
488: 
489: 		// each of the entries that do not match we move them to the next entry in the HT
490: 		for (idx_t i = 0; i < no_match_count; i++) {
491: 			idx_t index = no_match_vector.get_index(i);
492: 			ht_offsets_ptr[index]++;
493: 			if (ht_offsets_ptr[index] >= capacity) {
494: 				ht_offsets_ptr[index] = 0;
495: 			}
496: 		}
497: 		sel_vector = &no_match_vector;
498: 		remaining_entries = no_match_count;
499: 	}
500: 
501: 	return new_group_count;
502: }
503: 
504: // this is to support distinct aggregations where we need to record whether we
505: // have already seen a value for a group
506: idx_t GroupedAggregateHashTable::FindOrCreateGroups(DataChunk &groups, Vector &group_hashes, Vector &addresses_out,
507:                                                     SelectionVector &new_groups_out) {
508: 	switch (entry_type) {
509: 	case HtEntryType::HT_WIDTH_64:
510: 		return FindOrCreateGroupsInternal<aggr_ht_entry_64>(groups, group_hashes, addresses_out, new_groups_out);
511: 	case HtEntryType::HT_WIDTH_32:
512: 		return FindOrCreateGroupsInternal<aggr_ht_entry_32>(groups, group_hashes, addresses_out, new_groups_out);
513: 	default:
514: 		throw InternalException("Unknown HT entry width");
515: 	}
516: }
517: 
518: void GroupedAggregateHashTable::FindOrCreateGroups(DataChunk &groups, Vector &addresses) {
519: 	// create a dummy new_groups sel vector
520: 	SelectionVector new_groups(STANDARD_VECTOR_SIZE);
521: 	FindOrCreateGroups(groups, addresses, new_groups);
522: }
523: 
524: idx_t GroupedAggregateHashTable::FindOrCreateGroups(DataChunk &groups, Vector &addresses_out,
525:                                                     SelectionVector &new_groups_out) {
526: 	Vector hashes(LogicalType::HASH);
527: 	groups.Hash(hashes);
528: 	return FindOrCreateGroups(groups, hashes, addresses_out, new_groups_out);
529: }
530: 
531: void GroupedAggregateHashTable::FlushMove(Vector &source_addresses, Vector &source_hashes, idx_t count) {
532: 	D_ASSERT(source_addresses.GetType() == LogicalType::POINTER);
533: 	D_ASSERT(source_hashes.GetType() == LogicalType::HASH);
534: 
535: 	DataChunk groups;
536: 	groups.Initialize(vector<LogicalType>(layout.GetTypes().begin(), layout.GetTypes().end() - 1));
537: 	groups.SetCardinality(count);
538: 	for (idx_t i = 0; i < groups.ColumnCount(); i++) {
539: 		auto &column = groups.data[i];
540: 		const auto col_offset = layout.GetOffsets()[i];
541: 		RowOperations::Gather(source_addresses, FlatVector::INCREMENTAL_SELECTION_VECTOR, column,
542: 		                      FlatVector::INCREMENTAL_SELECTION_VECTOR, count, col_offset, i);
543: 	}
544: 
545: 	SelectionVector new_groups(STANDARD_VECTOR_SIZE);
546: 	Vector group_addresses(LogicalType::POINTER);
547: 	SelectionVector new_groups_sel(STANDARD_VECTOR_SIZE);
548: 
549: 	FindOrCreateGroups(groups, source_hashes, group_addresses, new_groups_sel);
550: 
551: 	RowOperations::CombineStates(layout, source_addresses, group_addresses, count);
552: }
553: 
554: void GroupedAggregateHashTable::Combine(GroupedAggregateHashTable &other) {
555: 
556: 	D_ASSERT(!is_finalized);
557: 
558: 	D_ASSERT(other.layout.GetAggrWidth() == layout.GetAggrWidth());
559: 	D_ASSERT(other.layout.GetDataWidth() == layout.GetDataWidth());
560: 	D_ASSERT(other.layout.GetRowWidth() == layout.GetRowWidth());
561: 	D_ASSERT(other.tuples_per_block == tuples_per_block);
562: 
563: 	if (other.entries == 0) {
564: 		return;
565: 	}
566: 
567: 	Vector addresses(LogicalType::POINTER);
568: 	auto addresses_ptr = FlatVector::GetData<data_ptr_t>(addresses);
569: 
570: 	Vector hashes(LogicalType::HASH);
571: 	auto hashes_ptr = FlatVector::GetData<hash_t>(hashes);
572: 
573: 	idx_t group_idx = 0;
574: 
575: 	other.PayloadApply([&](idx_t page_nr, idx_t page_offset, data_ptr_t ptr) {
576: 		auto hash = Load<hash_t>(ptr + hash_offset);
577: 
578: 		hashes_ptr[group_idx] = hash;
579: 		addresses_ptr[group_idx] = ptr;
580: 		group_idx++;
581: 		if (group_idx == STANDARD_VECTOR_SIZE) {
582: 			FlushMove(addresses, hashes, group_idx);
583: 			group_idx = 0;
584: 		}
585: 	});
586: 	FlushMove(addresses, hashes, group_idx);
587: 	string_heap->Merge(*other.string_heap);
588: 	Verify();
589: }
590: 
591: struct PartitionInfo {
592: 	PartitionInfo() : addresses(LogicalType::POINTER), hashes(LogicalType::HASH), group_count(0) {
593: 		addresses_ptr = FlatVector::GetData<data_ptr_t>(addresses);
594: 		hashes_ptr = FlatVector::GetData<hash_t>(hashes);
595: 	};
596: 	Vector addresses;
597: 	Vector hashes;
598: 	idx_t group_count;
599: 	data_ptr_t *addresses_ptr;
600: 	hash_t *hashes_ptr;
601: };
602: 
603: void GroupedAggregateHashTable::Partition(vector<GroupedAggregateHashTable *> &partition_hts, hash_t mask,
604:                                           idx_t shift) {
605: 	D_ASSERT(partition_hts.size() > 1);
606: 	vector<PartitionInfo> partition_info(partition_hts.size());
607: 
608: 	PayloadApply([&](idx_t page_nr, idx_t page_offset, data_ptr_t ptr) {
609: 		auto hash = Load<hash_t>(ptr + hash_offset);
610: 
611: 		idx_t partition = (hash & mask) >> shift;
612: 		D_ASSERT(partition < partition_hts.size());
613: 
614: 		auto &info = partition_info[partition];
615: 
616: 		info.hashes_ptr[info.group_count] = hash;
617: 		info.addresses_ptr[info.group_count] = ptr;
618: 		info.group_count++;
619: 		if (info.group_count == STANDARD_VECTOR_SIZE) {
620: 			D_ASSERT(partition_hts[partition]);
621: 			partition_hts[partition]->FlushMove(info.addresses, info.hashes, info.group_count);
622: 			info.group_count = 0;
623: 		}
624: 	});
625: 
626: 	idx_t info_idx = 0;
627: 	idx_t total_count = 0;
628: 	for (auto &partition_entry : partition_hts) {
629: 		auto &info = partition_info[info_idx++];
630: 		partition_entry->FlushMove(info.addresses, info.hashes, info.group_count);
631: 
632: 		partition_entry->string_heap->Merge(*string_heap);
633: 		partition_entry->Verify();
634: 		total_count += partition_entry->Size();
635: 	}
636: 	D_ASSERT(total_count == entries);
637: }
638: 
639: idx_t GroupedAggregateHashTable::Scan(idx_t &scan_position, DataChunk &result) {
640: 	Vector addresses(LogicalType::POINTER);
641: 	auto data_pointers = FlatVector::GetData<data_ptr_t>(addresses);
642: 
643: 	auto remaining = entries - scan_position;
644: 	if (remaining == 0) {
645: 		return 0;
646: 	}
647: 	auto this_n = MinValue((idx_t)STANDARD_VECTOR_SIZE, remaining);
648: 
649: 	auto chunk_idx = scan_position / tuples_per_block;
650: 	auto chunk_offset = (scan_position % tuples_per_block) * tuple_size;
651: 	D_ASSERT(chunk_offset + tuple_size <= Storage::BLOCK_SIZE);
652: 
653: 	auto read_ptr = payload_hds_ptrs[chunk_idx++];
654: 	for (idx_t i = 0; i < this_n; i++) {
655: 		data_pointers[i] = read_ptr + chunk_offset;
656: 		chunk_offset += tuple_size;
657: 		if (chunk_offset >= tuples_per_block * tuple_size) {
658: 			read_ptr = payload_hds_ptrs[chunk_idx++];
659: 			chunk_offset = 0;
660: 		}
661: 	}
662: 
663: 	result.SetCardinality(this_n);
664: 	// fetch the group columns (ignoring the final hash column
665: 	const auto group_cols = layout.ColumnCount() - 1;
666: 	for (idx_t i = 0; i < group_cols; i++) {
667: 		auto &column = result.data[i];
668: 		const auto col_offset = layout.GetOffsets()[i];
669: 		RowOperations::Gather(addresses, FlatVector::INCREMENTAL_SELECTION_VECTOR, column,
670: 		                      FlatVector::INCREMENTAL_SELECTION_VECTOR, result.size(), col_offset, i);
671: 	}
672: 
673: 	RowOperations::FinalizeStates(layout, addresses, result, group_cols);
674: 
675: 	scan_position += this_n;
676: 	return this_n;
677: }
678: 
679: void GroupedAggregateHashTable::Finalize() {
680: 	D_ASSERT(!is_finalized);
681: 
682: 	// early release hashes, not needed for partition/scan
683: 	hashes_hdl.reset();
684: 	is_finalized = true;
685: }
686: 
687: } // namespace duckdb
[end of src/execution/aggregate_hashtable.cpp]
[start of src/execution/operator/scan/physical_expression_scan.cpp]
1: #include "duckdb/execution/operator/scan/physical_expression_scan.hpp"
2: #include "duckdb/parallel/thread_context.hpp"
3: #include "duckdb/execution/expression_executor.hpp"
4: 
5: namespace duckdb {
6: 
7: class ExpressionScanState : public GlobalSourceState {
8: public:
9: 	explicit ExpressionScanState(const PhysicalExpressionScan &op) : expression_index(0) {
10: 		temp_chunk.Initialize(op.GetTypes());
11: 	}
12: 
13: 	//! The current position in the scan
14: 	idx_t expression_index;
15: 	//! Temporary chunk for evaluating expressions
16: 	DataChunk temp_chunk;
17: };
18: 
19: class ExpressionSinkState : public GlobalSinkState {
20: public:
21: 	ExpressionSinkState() {
22: 	}
23: 
24: 	DataChunk child_chunk;
25: };
26: 
27: unique_ptr<GlobalSourceState> PhysicalExpressionScan::GetGlobalSourceState(ClientContext &context) const {
28: 	return make_unique<ExpressionScanState>(*this);
29: }
30: 
31: void PhysicalExpressionScan::EvaluateExpression(idx_t expression_idx, DataChunk *child_chunk, DataChunk &result) const {
32: 	ExpressionExecutor executor(expressions[expression_idx]);
33: 	if (child_chunk) {
34: 		child_chunk->Verify();
35: 		executor.Execute(*child_chunk, result);
36: 	} else {
37: 		executor.Execute(result);
38: 	}
39: }
40: 
41: void PhysicalExpressionScan::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate_p,
42:                                      LocalSourceState &lstate) const {
43: 	D_ASSERT(sink_state);
44: 	auto &state = (ExpressionScanState &)gstate_p;
45: 	auto &gstate = (ExpressionSinkState &)*sink_state;
46: 
47: 	for (; chunk.size() < STANDARD_VECTOR_SIZE && state.expression_index < expressions.size();
48: 	     state.expression_index++) {
49: 		state.temp_chunk.Reset();
50: 		EvaluateExpression(state.expression_index, &gstate.child_chunk, state.temp_chunk);
51: 		chunk.Append(state.temp_chunk);
52: 	}
53: }
54: 
55: bool PhysicalExpressionScan::IsFoldable() const {
56: 	for (auto &expr_list : expressions) {
57: 		for (auto &expr : expr_list) {
58: 			if (!expr->IsFoldable()) {
59: 				return false;
60: 			}
61: 		}
62: 	}
63: 	return true;
64: }
65: 
66: SinkResultType PhysicalExpressionScan::Sink(ExecutionContext &context, GlobalSinkState &gstate_p,
67:                                             LocalSinkState &lstate, DataChunk &input) const {
68: 	auto &gstate = (ExpressionSinkState &)gstate_p;
69: 
70: 	D_ASSERT(children.size() == 1);
71: 	D_ASSERT(gstate.child_chunk.size() == 0);
72: 	if (input.size() != 1) {
73: 		throw InternalException("Expected expression scan child to have exactly one element");
74: 	}
75: 	gstate.child_chunk.Move(input);
76: 	gstate.child_chunk.Verify();
77: 	return SinkResultType::FINISHED;
78: }
79: 
80: unique_ptr<GlobalSinkState> PhysicalExpressionScan::GetGlobalSinkState(ClientContext &context) const {
81: 	return make_unique<ExpressionSinkState>();
82: }
83: 
84: } // namespace duckdb
[end of src/execution/operator/scan/physical_expression_scan.cpp]
[start of src/include/duckdb/execution/operator/scan/physical_expression_scan.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/scan/physical_expression_scan.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/chunk_collection.hpp"
12: #include "duckdb/execution/physical_operator.hpp"
13: #include "duckdb/planner/expression.hpp"
14: 
15: namespace duckdb {
16: 
17: //! The PhysicalExpressionScan scans a set of expressions
18: class PhysicalExpressionScan : public PhysicalOperator {
19: public:
20: 	PhysicalExpressionScan(vector<LogicalType> types, vector<vector<unique_ptr<Expression>>> expressions,
21: 	                       idx_t estimated_cardinality)
22: 	    : PhysicalOperator(PhysicalOperatorType::EXPRESSION_SCAN, move(types), estimated_cardinality),
23: 	      expressions(move(expressions)) {
24: 	}
25: 
26: 	//! The set of expressions to scan
27: 	vector<vector<unique_ptr<Expression>>> expressions;
28: 
29: public:
30: 	unique_ptr<GlobalSourceState> GetGlobalSourceState(ClientContext &context) const override;
31: 	void GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,
32: 	             LocalSourceState &lstate) const override;
33: 
34: public:
35: 	// Sink interface
36: 	SinkResultType Sink(ExecutionContext &context, GlobalSinkState &gstate, LocalSinkState &lstate,
37: 	                    DataChunk &input) const override;
38: 
39: 	unique_ptr<GlobalSinkState> GetGlobalSinkState(ClientContext &context) const override;
40: 
41: 	bool IsSink() const override {
42: 		return true;
43: 	}
44: 
45: public:
46: 	bool IsFoldable() const;
47: 	void EvaluateExpression(idx_t expression_idx, DataChunk *child_chunk, DataChunk &result) const;
48: };
49: 
50: } // namespace duckdb
[end of src/include/duckdb/execution/operator/scan/physical_expression_scan.hpp]
[start of src/parallel/executor.cpp]
1: #include "duckdb/execution/executor.hpp"
2: 
3: #include "duckdb/execution/operator/helper/physical_execute.hpp"
4: #include "duckdb/execution/operator/join/physical_delim_join.hpp"
5: #include "duckdb/execution/operator/scan/physical_chunk_scan.hpp"
6: #include "duckdb/execution/operator/set/physical_recursive_cte.hpp"
7: #include "duckdb/execution/physical_operator.hpp"
8: #include "duckdb/main/client_context.hpp"
9: #include "duckdb/execution/execution_context.hpp"
10: #include "duckdb/parallel/thread_context.hpp"
11: #include "duckdb/parallel/task_scheduler.hpp"
12: #include "duckdb/parallel/pipeline_executor.hpp"
13: 
14: #include "duckdb/parallel/pipeline_event.hpp"
15: #include "duckdb/parallel/pipeline_finish_event.hpp"
16: #include "duckdb/parallel/pipeline_complete_event.hpp"
17: 
18: #include <algorithm>
19: 
20: namespace duckdb {
21: 
22: Executor::Executor(ClientContext &context) : context(context) {
23: }
24: 
25: Executor::~Executor() {
26: }
27: 
28: Executor &Executor::Get(ClientContext &context) {
29: 	return context.executor;
30: }
31: 
32: void Executor::AddEvent(shared_ptr<Event> event) {
33: 	lock_guard<mutex> elock(executor_lock);
34: 	events.push_back(move(event));
35: }
36: 
37: struct PipelineEventStack {
38: 	Event *pipeline_event;
39: 	Event *pipeline_finish_event;
40: 	Event *pipeline_complete_event;
41: };
42: 
43: Pipeline *Executor::ScheduleUnionPipeline(const shared_ptr<Pipeline> &pipeline, const Pipeline *parent,
44:                                           event_map_t &event_map, vector<shared_ptr<Event>> &events) {
45: 	pipeline->Ready();
46: 
47: 	D_ASSERT(pipeline);
48: 	auto pipeline_event = make_shared<PipelineEvent>(pipeline);
49: 
50: 	auto parent_stack_entry = event_map.find(parent);
51: 	D_ASSERT(parent_stack_entry != event_map.end());
52: 
53: 	auto &parent_stack = parent_stack_entry->second;
54: 
55: 	PipelineEventStack stack;
56: 	stack.pipeline_event = pipeline_event.get();
57: 	stack.pipeline_finish_event = parent_stack.pipeline_finish_event;
58: 	stack.pipeline_complete_event = parent_stack.pipeline_complete_event;
59: 
60: 	stack.pipeline_event->AddDependency(*parent_stack.pipeline_event);
61: 	parent_stack.pipeline_finish_event->AddDependency(*pipeline_event);
62: 
63: 	events.push_back(move(pipeline_event));
64: 	event_map.insert(make_pair(pipeline.get(), stack));
65: 
66: 	auto parent_pipeline = pipeline.get();
67: 
68: 	auto union_entry = union_pipelines.find(pipeline.get());
69: 	if (union_entry != union_pipelines.end()) {
70: 		for (auto &entry : union_entry->second) {
71: 			parent_pipeline = ScheduleUnionPipeline(entry, parent_pipeline, event_map, events);
72: 		}
73: 	}
74: 
75: 	return parent_pipeline;
76: }
77: 
78: void Executor::ScheduleChildPipeline(Pipeline *parent, const shared_ptr<Pipeline> &pipeline, event_map_t &event_map,
79:                                      vector<shared_ptr<Event>> &events) {
80: 	pipeline->Ready();
81: 
82: 	auto child_ptr = pipeline.get();
83: 	auto dependencies = child_dependencies.find(child_ptr);
84: 	D_ASSERT(union_pipelines.find(child_ptr) == union_pipelines.end());
85: 	D_ASSERT(dependencies != child_dependencies.end());
86: 	// create the pipeline event and the event stack
87: 	auto pipeline_event = make_shared<PipelineEvent>(pipeline);
88: 
89: 	auto parent_entry = event_map.find(parent);
90: 	PipelineEventStack stack;
91: 	stack.pipeline_event = pipeline_event.get();
92: 	stack.pipeline_finish_event = parent_entry->second.pipeline_finish_event;
93: 	stack.pipeline_complete_event = parent_entry->second.pipeline_complete_event;
94: 
95: 	// set up the dependencies for this child pipeline
96: 	unordered_set<Event *> finish_events;
97: 	for (auto &dep : dependencies->second) {
98: 		auto dep_entry = event_map.find(dep);
99: 		D_ASSERT(dep_entry != event_map.end());
100: 		D_ASSERT(dep_entry->second.pipeline_event);
101: 		D_ASSERT(dep_entry->second.pipeline_finish_event);
102: 
103: 		auto finish_event = dep_entry->second.pipeline_finish_event;
104: 		stack.pipeline_event->AddDependency(*dep_entry->second.pipeline_event);
105: 		if (finish_events.find(finish_event) == finish_events.end()) {
106: 			finish_event->AddDependency(*stack.pipeline_event);
107: 			finish_events.insert(finish_event);
108: 		}
109: 	}
110: 
111: 	events.push_back(move(pipeline_event));
112: 	event_map.insert(make_pair(child_ptr, stack));
113: }
114: 
115: void Executor::SchedulePipeline(const shared_ptr<Pipeline> &pipeline, event_map_t &event_map,
116:                                 vector<shared_ptr<Event>> &events, bool complete_pipeline) {
117: 	D_ASSERT(pipeline);
118: 
119: 	pipeline->Ready();
120: 
121: 	auto pipeline_event = make_shared<PipelineEvent>(pipeline);
122: 	auto pipeline_finish_event = make_shared<PipelineFinishEvent>(pipeline);
123: 	auto pipeline_complete_event = make_shared<PipelineCompleteEvent>(pipeline->executor, complete_pipeline);
124: 
125: 	PipelineEventStack stack;
126: 	stack.pipeline_event = pipeline_event.get();
127: 	stack.pipeline_finish_event = pipeline_finish_event.get();
128: 	stack.pipeline_complete_event = pipeline_complete_event.get();
129: 
130: 	pipeline_finish_event->AddDependency(*pipeline_event);
131: 	pipeline_complete_event->AddDependency(*pipeline_finish_event);
132: 
133: 	events.push_back(move(pipeline_event));
134: 	events.push_back(move(pipeline_finish_event));
135: 	events.push_back(move(pipeline_complete_event));
136: 
137: 	event_map.insert(make_pair(pipeline.get(), stack));
138: 
139: 	auto union_entry = union_pipelines.find(pipeline.get());
140: 	if (union_entry != union_pipelines.end()) {
141: 		auto parent_pipeline = pipeline.get();
142: 		for (auto &entry : union_entry->second) {
143: 			parent_pipeline = ScheduleUnionPipeline(entry, parent_pipeline, event_map, events);
144: 		}
145: 	}
146: }
147: 
148: void Executor::ScheduleEventsInternal(const vector<shared_ptr<Pipeline>> &pipelines,
149:                                       unordered_map<Pipeline *, vector<shared_ptr<Pipeline>>> &child_pipelines,
150:                                       vector<shared_ptr<Event>> &events, bool main_schedule) {
151: 	D_ASSERT(events.empty());
152: 	// create all the required pipeline events
153: 	event_map_t event_map;
154: 	for (auto &pipeline : pipelines) {
155: 		SchedulePipeline(pipeline, event_map, events, main_schedule);
156: 	}
157: 	// schedule child pipelines
158: 	for (auto &entry : child_pipelines) {
159: 		// iterate in reverse order
160: 		// since child entries are added from top to bottom
161: 		// dependencies are in reverse order (bottom to top)
162: 		for (idx_t i = entry.second.size(); i > 0; i--) {
163: 			auto &child_entry = entry.second[i - 1];
164: 			ScheduleChildPipeline(entry.first, child_entry, event_map, events);
165: 		}
166: 	}
167: 	// set up the dependencies between pipeline events
168: 	for (auto &entry : event_map) {
169: 		auto pipeline = entry.first;
170: 		for (auto &dependency : pipeline->dependencies) {
171: 			auto dep = dependency.lock();
172: 			D_ASSERT(dep);
173: 			auto event_map_entry = event_map.find(dep.get());
174: 			D_ASSERT(event_map_entry != event_map.end());
175: 			auto &dep_entry = event_map_entry->second;
176: 			D_ASSERT(dep_entry.pipeline_complete_event);
177: 			entry.second.pipeline_event->AddDependency(*dep_entry.pipeline_complete_event);
178: 		}
179: 	}
180: 	// schedule the pipelines that do not have dependencies
181: 	for (auto &event : events) {
182: 		if (!event->HasDependencies()) {
183: 			event->Schedule();
184: 		}
185: 	}
186: }
187: 
188: void Executor::ScheduleEvents() {
189: 	ScheduleEventsInternal(pipelines, child_pipelines, events);
190: }
191: 
192: void Executor::ReschedulePipelines(const vector<shared_ptr<Pipeline>> &pipelines, vector<shared_ptr<Event>> &events) {
193: 	unordered_map<Pipeline *, vector<shared_ptr<Pipeline>>> child_pipelines;
194: 	ScheduleEventsInternal(pipelines, child_pipelines, events, false);
195: }
196: 
197: void Executor::ExtractPipelines(shared_ptr<Pipeline> &pipeline, vector<shared_ptr<Pipeline>> &result) {
198: 	pipeline->Ready();
199: 
200: 	auto pipeline_ptr = pipeline.get();
201: 	result.push_back(move(pipeline));
202: 	auto union_entry = union_pipelines.find(pipeline_ptr);
203: 	if (union_entry != union_pipelines.end()) {
204: 		auto &union_pipeline_list = union_entry->second;
205: 		for (auto &pipeline : union_pipeline_list) {
206: 			ExtractPipelines(pipeline, result);
207: 		}
208: 		union_pipelines.erase(pipeline_ptr);
209: 	}
210: 	auto child_entry = child_pipelines.find(pipeline_ptr);
211: 	if (child_entry != child_pipelines.end()) {
212: 		for (auto &entry : child_entry->second) {
213: 			ExtractPipelines(entry, result);
214: 		}
215: 		child_pipelines.erase(pipeline_ptr);
216: 	}
217: }
218: 
219: bool Executor::NextExecutor() {
220: 	if (root_pipeline_idx >= root_pipelines.size()) {
221: 		return false;
222: 	}
223: 	root_executor = make_unique<PipelineExecutor>(context, *root_pipelines[root_pipeline_idx]);
224: 	root_pipeline_idx++;
225: 	return true;
226: }
227: 
228: void Executor::VerifyPipeline(Pipeline &pipeline) {
229: 	D_ASSERT(!pipeline.ToString().empty());
230: 	auto operators = pipeline.GetOperators();
231: 	for (auto &other_pipeline : pipelines) {
232: 		auto other_operators = other_pipeline->GetOperators();
233: 		for (idx_t op_idx = 0; op_idx < operators.size(); op_idx++) {
234: 			for (idx_t other_idx = 0; other_idx < other_operators.size(); other_idx++) {
235: 				auto &left = *operators[op_idx];
236: 				auto &right = *other_operators[other_idx];
237: 				if (left.Equals(right)) {
238: 					D_ASSERT(right.Equals(left));
239: 				} else {
240: 					D_ASSERT(!right.Equals(left));
241: 				}
242: 			}
243: 		}
244: 	}
245: }
246: 
247: void Executor::VerifyPipelines() {
248: #ifdef DEBUG
249: 	for (auto &pipeline : pipelines) {
250: 		VerifyPipeline(*pipeline);
251: 	}
252: 	for (auto &pipeline : root_pipelines) {
253: 		VerifyPipeline(*pipeline);
254: 	}
255: #endif
256: }
257: 
258: void Executor::WorkOnTasks() {
259: 	auto &scheduler = TaskScheduler::GetScheduler(context);
260: 
261: 	unique_ptr<Task> task;
262: 	while (scheduler.GetTaskFromProducer(*producer, task)) {
263: 		task->Execute();
264: 		task.reset();
265: 	}
266: }
267: 
268: void Executor::Initialize(PhysicalOperator *plan) {
269: 	Reset();
270: 
271: 	auto &scheduler = TaskScheduler::GetScheduler(context);
272: 	{
273: 		lock_guard<mutex> elock(executor_lock);
274: 		physical_plan = plan;
275: 
276: 		context.profiler->Initialize(physical_plan);
277: 		this->producer = scheduler.CreateProducer();
278: 
279: 		auto root_pipeline = make_shared<Pipeline>(*this);
280: 		root_pipeline->sink = nullptr;
281: 		BuildPipelines(physical_plan, root_pipeline.get());
282: 
283: 		this->total_pipelines = pipelines.size();
284: 
285: 		root_pipeline_idx = 0;
286: 		ExtractPipelines(root_pipeline, root_pipelines);
287: 
288: 		VerifyPipelines();
289: 
290: 		ScheduleEvents();
291: 	}
292: 
293: 	// now execute tasks from this producer until all pipelines are completed
294: 	while (completed_pipelines < total_pipelines) {
295: 		WorkOnTasks();
296: 		if (!HasError()) {
297: 			// no exceptions: continue
298: 			continue;
299: 		}
300: 
301: 		// an exception has occurred executing one of the pipelines
302: 		// we need to wait until all threads are finished
303: 		// we do this by creating weak pointers to all pipelines
304: 		// then clearing our references to the pipelines
305: 		// and waiting until all pipelines have been destroyed
306: 		vector<weak_ptr<Pipeline>> weak_references;
307: 		{
308: 			lock_guard<mutex> elock(executor_lock);
309: 			weak_references.reserve(pipelines.size());
310: 			for (auto &pipeline : pipelines) {
311: 				weak_references.push_back(weak_ptr<Pipeline>(pipeline));
312: 			}
313: 			for (auto &kv : union_pipelines) {
314: 				for (auto &pipeline : kv.second) {
315: 					weak_references.push_back(weak_ptr<Pipeline>(pipeline));
316: 				}
317: 			}
318: 			for (auto &kv : child_pipelines) {
319: 				for (auto &pipeline : kv.second) {
320: 					weak_references.push_back(weak_ptr<Pipeline>(pipeline));
321: 				}
322: 			}
323: 			pipelines.clear();
324: 			union_pipelines.clear();
325: 			child_pipelines.clear();
326: 			events.clear();
327: 		}
328: 		for (auto &weak_ref : weak_references) {
329: 			while (true) {
330: 				auto weak = weak_ref.lock();
331: 				if (!weak) {
332: 					break;
333: 				}
334: 			}
335: 		}
336: 		ThrowException();
337: 	}
338: 
339: 	lock_guard<mutex> elock(executor_lock);
340: 	pipelines.clear();
341: 	NextExecutor();
342: 	if (!exceptions.empty()) { // LCOV_EXCL_START
343: 		// an exception has occurred executing one of the pipelines
344: 		ThrowExceptionInternal();
345: 	} // LCOV_EXCL_STOP
346: }
347: 
348: void Executor::Reset() {
349: 	lock_guard<mutex> elock(executor_lock);
350: 	delim_join_dependencies.clear();
351: 	recursive_cte = nullptr;
352: 	physical_plan = nullptr;
353: 	root_executor.reset();
354: 	root_pipelines.clear();
355: 	root_pipeline_idx = 0;
356: 	completed_pipelines = 0;
357: 	total_pipelines = 0;
358: 	exceptions.clear();
359: 	pipelines.clear();
360: 	events.clear();
361: 	union_pipelines.clear();
362: 	child_pipelines.clear();
363: 	child_dependencies.clear();
364: }
365: 
366: void Executor::AddChildPipeline(Pipeline *current) {
367: 	D_ASSERT(!current->operators.empty());
368: 	// found another operator that is a source
369: 	// schedule a child pipeline
370: 	auto child_pipeline = make_shared<Pipeline>(*this);
371: 	auto child_pipeline_ptr = child_pipeline.get();
372: 	child_pipeline->sink = current->sink;
373: 	child_pipeline->operators = current->operators;
374: 	child_pipeline->source = current->operators.back();
375: 	D_ASSERT(child_pipeline->source->IsSource());
376: 	child_pipeline->operators.pop_back();
377: 
378: 	vector<Pipeline *> dependencies;
379: 	dependencies.push_back(current);
380: 	auto child_entry = child_pipelines.find(current);
381: 	if (child_entry != child_pipelines.end()) {
382: 		for (auto &current_child : child_entry->second) {
383: 			D_ASSERT(child_dependencies.find(current_child.get()) != child_dependencies.end());
384: 			child_dependencies[current_child.get()].push_back(child_pipeline_ptr);
385: 		}
386: 	}
387: 	D_ASSERT(child_dependencies.find(child_pipeline_ptr) == child_dependencies.end());
388: 	child_dependencies.insert(make_pair(child_pipeline_ptr, move(dependencies)));
389: 	child_pipelines[current].push_back(move(child_pipeline));
390: }
391: 
392: void Executor::BuildPipelines(PhysicalOperator *op, Pipeline *current) {
393: 	D_ASSERT(current);
394: 	if (op->IsSink()) {
395: 		// operator is a sink, build a pipeline
396: 		op->sink_state.reset();
397: 
398: 		PhysicalOperator *pipeline_child = nullptr;
399: 		switch (op->type) {
400: 		case PhysicalOperatorType::CREATE_TABLE_AS:
401: 		case PhysicalOperatorType::INSERT:
402: 		case PhysicalOperatorType::DELETE_OPERATOR:
403: 		case PhysicalOperatorType::UPDATE:
404: 		case PhysicalOperatorType::HASH_GROUP_BY:
405: 		case PhysicalOperatorType::SIMPLE_AGGREGATE:
406: 		case PhysicalOperatorType::PERFECT_HASH_GROUP_BY:
407: 		case PhysicalOperatorType::WINDOW:
408: 		case PhysicalOperatorType::ORDER_BY:
409: 		case PhysicalOperatorType::RESERVOIR_SAMPLE:
410: 		case PhysicalOperatorType::TOP_N:
411: 		case PhysicalOperatorType::COPY_TO_FILE:
412: 		case PhysicalOperatorType::LIMIT:
413: 		case PhysicalOperatorType::EXPRESSION_SCAN:
414: 		case PhysicalOperatorType::EXPLAIN_ANALYZE:
415: 			D_ASSERT(op->children.size() == 1);
416: 			// single operator:
417: 			// the operator becomes the data source of the current pipeline
418: 			current->source = op;
419: 			// we create a new pipeline starting from the child
420: 			pipeline_child = op->children[0].get();
421: 			break;
422: 		case PhysicalOperatorType::EXPORT:
423: 			// EXPORT has an optional child
424: 			// we only need to schedule child pipelines if there is a child
425: 			current->source = op;
426: 			if (op->children.empty()) {
427: 				return;
428: 			}
429: 			D_ASSERT(op->children.size() == 1);
430: 			pipeline_child = op->children[0].get();
431: 			break;
432: 		case PhysicalOperatorType::NESTED_LOOP_JOIN:
433: 		case PhysicalOperatorType::BLOCKWISE_NL_JOIN:
434: 		case PhysicalOperatorType::HASH_JOIN:
435: 		case PhysicalOperatorType::PIECEWISE_MERGE_JOIN:
436: 		case PhysicalOperatorType::CROSS_PRODUCT:
437: 			// regular join, create a pipeline with RHS source that sinks into this pipeline
438: 			pipeline_child = op->children[1].get();
439: 			// on the LHS (probe child), the operator becomes a regular operator
440: 			current->operators.push_back(op);
441: 			if (op->IsSource()) {
442: 				// FULL or RIGHT outer join
443: 				// schedule a scan of the node as a child pipeline
444: 				// this scan has to be performed AFTER all the probing has happened
445: 				if (recursive_cte) {
446: 					throw NotImplementedException("FULL and RIGHT outer joins are not supported in recursive CTEs yet");
447: 				}
448: 				AddChildPipeline(current);
449: 			}
450: 			BuildPipelines(op->children[0].get(), current);
451: 			break;
452: 		case PhysicalOperatorType::DELIM_JOIN: {
453: 			// duplicate eliminated join
454: 			// for delim joins, recurse into the actual join
455: 			pipeline_child = op->children[0].get();
456: 			break;
457: 		}
458: 		case PhysicalOperatorType::RECURSIVE_CTE: {
459: 			auto &cte_node = (PhysicalRecursiveCTE &)*op;
460: 
461: 			// recursive CTE
462: 			current->source = op;
463: 			// the LHS of the recursive CTE is our initial state
464: 			// we build this pipeline as normal
465: 			pipeline_child = op->children[0].get();
466: 			// for the RHS, we gather all pipelines that depend on the recursive cte
467: 			// these pipelines need to be rerun
468: 			if (recursive_cte) {
469: 				throw InternalException("Recursive CTE detected WITHIN a recursive CTE node");
470: 			}
471: 			recursive_cte = op;
472: 
473: 			auto recursive_pipeline = make_shared<Pipeline>(*this);
474: 			recursive_pipeline->sink = op;
475: 			op->sink_state.reset();
476: 			BuildPipelines(op->children[1].get(), recursive_pipeline.get());
477: 
478: 			cte_node.pipelines.push_back(move(recursive_pipeline));
479: 
480: 			recursive_cte = nullptr;
481: 			break;
482: 		}
483: 		default:
484: 			throw InternalException("Unimplemented sink type!");
485: 		}
486: 		// the current is dependent on this pipeline to complete
487: 		auto pipeline = make_shared<Pipeline>(*this);
488: 		pipeline->sink = op;
489: 		current->AddDependency(pipeline);
490: 		D_ASSERT(pipeline_child);
491: 		// recurse into the pipeline child
492: 		BuildPipelines(pipeline_child, pipeline.get());
493: 		if (op->type == PhysicalOperatorType::DELIM_JOIN) {
494: 			// for delim joins, recurse into the actual join
495: 			// any pipelines in there depend on the main pipeline
496: 			auto &delim_join = (PhysicalDelimJoin &)*op;
497: 			// any scan of the duplicate eliminated data on the RHS depends on this pipeline
498: 			// we add an entry to the mapping of (PhysicalOperator*) -> (Pipeline*)
499: 			for (auto &delim_scan : delim_join.delim_scans) {
500: 				delim_join_dependencies[delim_scan] = pipeline.get();
501: 			}
502: 			BuildPipelines(delim_join.join.get(), current);
503: 		}
504: 		if (!recursive_cte) {
505: 			// regular pipeline: schedule it
506: 			pipelines.push_back(move(pipeline));
507: 		} else {
508: 			// CTE pipeline! add it to the CTE pipelines
509: 			D_ASSERT(recursive_cte);
510: 			auto &cte = (PhysicalRecursiveCTE &)*recursive_cte;
511: 			cte.pipelines.push_back(move(pipeline));
512: 		}
513: 	} else {
514: 		// operator is not a sink! recurse in children
515: 		// first check if there is any additional action we need to do depending on the type
516: 		switch (op->type) {
517: 		case PhysicalOperatorType::DELIM_SCAN: {
518: 			D_ASSERT(op->children.empty());
519: 			auto entry = delim_join_dependencies.find(op);
520: 			D_ASSERT(entry != delim_join_dependencies.end());
521: 			// this chunk scan introduces a dependency to the current pipeline
522: 			// namely a dependency on the duplicate elimination pipeline to finish
523: 			auto delim_dependency = entry->second->shared_from_this();
524: 			D_ASSERT(delim_dependency->sink->type == PhysicalOperatorType::DELIM_JOIN);
525: 			auto &delim_join = (PhysicalDelimJoin &)*delim_dependency->sink;
526: 			current->AddDependency(delim_dependency);
527: 			current->source = (PhysicalOperator *)delim_join.distinct.get();
528: 			return;
529: 		}
530: 		case PhysicalOperatorType::EXECUTE: {
531: 			// EXECUTE statement: build pipeline on child
532: 			auto &execute = (PhysicalExecute &)*op;
533: 			BuildPipelines(execute.plan, current);
534: 			return;
535: 		}
536: 		case PhysicalOperatorType::RECURSIVE_CTE_SCAN: {
537: 			if (!recursive_cte) {
538: 				throw InternalException("Recursive CTE scan found without recursive CTE node");
539: 			}
540: 			break;
541: 		}
542: 		case PhysicalOperatorType::INDEX_JOIN: {
543: 			// index join: we only continue into the LHS
544: 			// the right side is probed by the index join
545: 			// so we don't need to do anything in the pipeline with this child
546: 			current->operators.push_back(op);
547: 			BuildPipelines(op->children[0].get(), current);
548: 			return;
549: 		}
550: 		case PhysicalOperatorType::UNION: {
551: 			if (recursive_cte) {
552: 				throw NotImplementedException("UNIONS are not supported in recursive CTEs yet");
553: 			}
554: 			auto union_pipeline = make_shared<Pipeline>(*this);
555: 			auto pipeline_ptr = union_pipeline.get();
556: 			// set up dependencies for any child pipelines to this union pipeline
557: 			auto child_entry = child_pipelines.find(current);
558: 			if (child_entry != child_pipelines.end()) {
559: 				for (auto &current_child : child_entry->second) {
560: 					D_ASSERT(child_dependencies.find(current_child.get()) != child_dependencies.end());
561: 					child_dependencies[current_child.get()].push_back(pipeline_ptr);
562: 				}
563: 			}
564: 			// for the current pipeline, continue building on the LHS
565: 			union_pipeline->operators = current->operators;
566: 			BuildPipelines(op->children[0].get(), current);
567: 			// insert the union pipeline as a union pipeline of the current node
568: 			union_pipelines[current].push_back(move(union_pipeline));
569: 
570: 			// for the union pipeline, build on the RHS
571: 			pipeline_ptr->sink = current->sink;
572: 			BuildPipelines(op->children[1].get(), pipeline_ptr);
573: 			return;
574: 		}
575: 		default:
576: 			break;
577: 		}
578: 		if (op->children.empty()) {
579: 			// source
580: 			current->source = op;
581: 		} else {
582: 			if (op->children.size() != 1) {
583: 				throw InternalException("Operator not supported yet");
584: 			}
585: 			current->operators.push_back(op);
586: 			BuildPipelines(op->children[0].get(), current);
587: 		}
588: 	}
589: }
590: 
591: vector<LogicalType> Executor::GetTypes() {
592: 	D_ASSERT(physical_plan);
593: 	return physical_plan->GetTypes();
594: }
595: 
596: void Executor::PushError(ExceptionType type, const string &exception) {
597: 	lock_guard<mutex> elock(executor_lock);
598: 	// interrupt execution of any other pipelines that belong to this executor
599: 	context.interrupted = true;
600: 	// push the exception onto the stack
601: 	exceptions.emplace_back(type, exception);
602: }
603: 
604: bool Executor::HasError() {
605: 	lock_guard<mutex> elock(executor_lock);
606: 	return !exceptions.empty();
607: }
608: 
609: void Executor::ThrowException() {
610: 	lock_guard<mutex> elock(executor_lock);
611: 	ThrowExceptionInternal();
612: }
613: 
614: void Executor::ThrowExceptionInternal() { // LCOV_EXCL_START
615: 	D_ASSERT(!exceptions.empty());
616: 	auto &entry = exceptions[0];
617: 	switch (entry.first) {
618: 	case ExceptionType::TRANSACTION:
619: 		throw TransactionException(entry.second);
620: 	case ExceptionType::CATALOG:
621: 		throw CatalogException(entry.second);
622: 	case ExceptionType::PARSER:
623: 		throw ParserException(entry.second);
624: 	case ExceptionType::BINDER:
625: 		throw BinderException(entry.second);
626: 	case ExceptionType::INTERRUPT:
627: 		throw InterruptException();
628: 	case ExceptionType::FATAL:
629: 		throw FatalException(entry.second);
630: 	case ExceptionType::INTERNAL:
631: 		throw InternalException(entry.second);
632: 	case ExceptionType::IO:
633: 		throw IOException(entry.second);
634: 	case ExceptionType::CONSTRAINT:
635: 		throw ConstraintException(entry.second);
636: 	case ExceptionType::CONVERSION:
637: 		throw ConversionException(entry.second);
638: 	default:
639: 		throw Exception(entry.second);
640: 	}
641: } // LCOV_EXCL_STOP
642: 
643: void Executor::Flush(ThreadContext &tcontext) {
644: 	lock_guard<mutex> elock(executor_lock);
645: 	context.profiler->Flush(tcontext.profiler);
646: }
647: 
648: bool Executor::GetPipelinesProgress(int &current_progress) { // LCOV_EXCL_START
649: 	lock_guard<mutex> elock(executor_lock);
650: 
651: 	if (!pipelines.empty()) {
652: 		return pipelines.back()->GetProgress(current_progress);
653: 	} else {
654: 		current_progress = -1;
655: 		return true;
656: 	}
657: } // LCOV_EXCL_STOP
658: 
659: unique_ptr<DataChunk> Executor::FetchChunk() {
660: 	D_ASSERT(physical_plan);
661: 
662: 	auto chunk = make_unique<DataChunk>();
663: 	root_executor->InitializeChunk(*chunk);
664: 	while (true) {
665: 		root_executor->ExecutePull(*chunk);
666: 		if (chunk->size() == 0) {
667: 			root_executor->PullFinalize();
668: 			if (NextExecutor()) {
669: 				continue;
670: 			}
671: 			break;
672: 		} else {
673: 			break;
674: 		}
675: 	}
676: 	return chunk;
677: }
678: 
679: } // namespace duckdb
[end of src/parallel/executor.cpp]
[start of src/planner/expression_iterator.cpp]
1: #include "duckdb/planner/expression_iterator.hpp"
2: 
3: #include "duckdb/planner/bound_query_node.hpp"
4: #include "duckdb/planner/expression/list.hpp"
5: #include "duckdb/planner/query_node/bound_select_node.hpp"
6: #include "duckdb/planner/query_node/bound_set_operation_node.hpp"
7: #include "duckdb/planner/tableref/list.hpp"
8: 
9: namespace duckdb {
10: 
11: void ExpressionIterator::EnumerateChildren(const Expression &expr,
12:                                            const std::function<void(const Expression &child)> &callback) {
13: 	EnumerateChildren((Expression &)expr, [&](unique_ptr<Expression> &child) { callback(*child); });
14: }
15: 
16: void ExpressionIterator::EnumerateChildren(Expression &expr, const std::function<void(Expression &child)> &callback) {
17: 	EnumerateChildren(expr, [&](unique_ptr<Expression> &child) { callback(*child); });
18: }
19: 
20: void ExpressionIterator::EnumerateChildren(Expression &expr,
21:                                            const std::function<void(unique_ptr<Expression> &child)> &callback) {
22: 	switch (expr.expression_class) {
23: 	case ExpressionClass::BOUND_AGGREGATE: {
24: 		auto &aggr_expr = (BoundAggregateExpression &)expr;
25: 		for (auto &child : aggr_expr.children) {
26: 			callback(child);
27: 		}
28: 		if (aggr_expr.filter) {
29: 			callback(aggr_expr.filter);
30: 		}
31: 		break;
32: 	}
33: 	case ExpressionClass::BOUND_BETWEEN: {
34: 		auto &between_expr = (BoundBetweenExpression &)expr;
35: 		callback(between_expr.input);
36: 		callback(between_expr.lower);
37: 		callback(between_expr.upper);
38: 		break;
39: 	}
40: 	case ExpressionClass::BOUND_CASE: {
41: 		auto &case_expr = (BoundCaseExpression &)expr;
42: 		for (auto &case_check : case_expr.case_checks) {
43: 			callback(case_check.when_expr);
44: 			callback(case_check.then_expr);
45: 		}
46: 		callback(case_expr.else_expr);
47: 		break;
48: 	}
49: 	case ExpressionClass::BOUND_CAST: {
50: 		auto &cast_expr = (BoundCastExpression &)expr;
51: 		callback(cast_expr.child);
52: 		break;
53: 	}
54: 	case ExpressionClass::BOUND_COMPARISON: {
55: 		auto &comp_expr = (BoundComparisonExpression &)expr;
56: 		callback(comp_expr.left);
57: 		callback(comp_expr.right);
58: 		break;
59: 	}
60: 	case ExpressionClass::BOUND_CONJUNCTION: {
61: 		auto &conj_expr = (BoundConjunctionExpression &)expr;
62: 		for (auto &child : conj_expr.children) {
63: 			callback(child);
64: 		}
65: 		break;
66: 	}
67: 	case ExpressionClass::BOUND_FUNCTION: {
68: 		auto &func_expr = (BoundFunctionExpression &)expr;
69: 		for (auto &child : func_expr.children) {
70: 			callback(child);
71: 		}
72: 		break;
73: 	}
74: 	case ExpressionClass::BOUND_OPERATOR: {
75: 		auto &op_expr = (BoundOperatorExpression &)expr;
76: 		for (auto &child : op_expr.children) {
77: 			callback(child);
78: 		}
79: 		break;
80: 	}
81: 	case ExpressionClass::BOUND_SUBQUERY: {
82: 		auto &subquery_expr = (BoundSubqueryExpression &)expr;
83: 		if (subquery_expr.child) {
84: 			callback(subquery_expr.child);
85: 		}
86: 		break;
87: 	}
88: 	case ExpressionClass::BOUND_WINDOW: {
89: 		auto &window_expr = (BoundWindowExpression &)expr;
90: 		for (auto &partition : window_expr.partitions) {
91: 			callback(partition);
92: 		}
93: 		for (auto &order : window_expr.orders) {
94: 			callback(order.expression);
95: 		}
96: 		for (auto &child : window_expr.children) {
97: 			callback(child);
98: 		}
99: 		if (window_expr.start_expr) {
100: 			callback(window_expr.start_expr);
101: 		}
102: 		if (window_expr.end_expr) {
103: 			callback(window_expr.end_expr);
104: 		}
105: 		if (window_expr.offset_expr) {
106: 			callback(window_expr.offset_expr);
107: 		}
108: 		if (window_expr.default_expr) {
109: 			callback(window_expr.default_expr);
110: 		}
111: 		break;
112: 	}
113: 	case ExpressionClass::BOUND_UNNEST: {
114: 		auto &unnest_expr = (BoundUnnestExpression &)expr;
115: 		callback(unnest_expr.child);
116: 		break;
117: 	}
118: 	case ExpressionClass::BOUND_COLUMN_REF:
119: 	case ExpressionClass::BOUND_CONSTANT:
120: 	case ExpressionClass::BOUND_DEFAULT:
121: 	case ExpressionClass::BOUND_PARAMETER:
122: 	case ExpressionClass::BOUND_REF:
123: 		// these node types have no children
124: 		break;
125: 	default:
126: 		throw InternalException("ExpressionIterator used on unbound expression");
127: 	}
128: }
129: 
130: void ExpressionIterator::EnumerateExpression(unique_ptr<Expression> &expr,
131:                                              const std::function<void(Expression &child)> &callback) {
132: 	if (!expr) {
133: 		return;
134: 	}
135: 	callback(*expr);
136: 	ExpressionIterator::EnumerateChildren(*expr,
137: 	                                      [&](unique_ptr<Expression> &child) { EnumerateExpression(child, callback); });
138: }
139: 
140: void ExpressionIterator::EnumerateTableRefChildren(BoundTableRef &ref,
141:                                                    const std::function<void(Expression &child)> &callback) {
142: 	switch (ref.type) {
143: 	case TableReferenceType::CROSS_PRODUCT: {
144: 		auto &bound_crossproduct = (BoundCrossProductRef &)ref;
145: 		EnumerateTableRefChildren(*bound_crossproduct.left, callback);
146: 		EnumerateTableRefChildren(*bound_crossproduct.right, callback);
147: 		break;
148: 	}
149: 	case TableReferenceType::JOIN: {
150: 		auto &bound_join = (BoundJoinRef &)ref;
151: 		EnumerateExpression(bound_join.condition, callback);
152: 		EnumerateTableRefChildren(*bound_join.left, callback);
153: 		EnumerateTableRefChildren(*bound_join.right, callback);
154: 		break;
155: 	}
156: 	case TableReferenceType::SUBQUERY: {
157: 		auto &bound_subquery = (BoundSubqueryRef &)ref;
158: 		EnumerateQueryNodeChildren(*bound_subquery.subquery, callback);
159: 		break;
160: 	}
161: 	default:
162: 		D_ASSERT(ref.type == TableReferenceType::TABLE_FUNCTION || ref.type == TableReferenceType::BASE_TABLE ||
163: 		         ref.type == TableReferenceType::EMPTY);
164: 		break;
165: 	}
166: }
167: 
168: void ExpressionIterator::EnumerateQueryNodeChildren(BoundQueryNode &node,
169:                                                     const std::function<void(Expression &child)> &callback) {
170: 	switch (node.type) {
171: 	case QueryNodeType::SET_OPERATION_NODE: {
172: 		auto &bound_setop = (BoundSetOperationNode &)node;
173: 		EnumerateQueryNodeChildren(*bound_setop.left, callback);
174: 		EnumerateQueryNodeChildren(*bound_setop.right, callback);
175: 		break;
176: 	}
177: 	default:
178: 		D_ASSERT(node.type == QueryNodeType::SELECT_NODE);
179: 		auto &bound_select = (BoundSelectNode &)node;
180: 		for (idx_t i = 0; i < bound_select.select_list.size(); i++) {
181: 			EnumerateExpression(bound_select.select_list[i], callback);
182: 		}
183: 		EnumerateExpression(bound_select.where_clause, callback);
184: 		for (idx_t i = 0; i < bound_select.groups.group_expressions.size(); i++) {
185: 			EnumerateExpression(bound_select.groups.group_expressions[i], callback);
186: 		}
187: 		EnumerateExpression(bound_select.having, callback);
188: 		for (idx_t i = 0; i < bound_select.aggregates.size(); i++) {
189: 			EnumerateExpression(bound_select.aggregates[i], callback);
190: 		}
191: 		for (idx_t i = 0; i < bound_select.unnests.size(); i++) {
192: 			EnumerateExpression(bound_select.unnests[i], callback);
193: 		}
194: 		for (idx_t i = 0; i < bound_select.windows.size(); i++) {
195: 			EnumerateExpression(bound_select.windows[i], callback);
196: 		}
197: 		if (bound_select.from_table) {
198: 			EnumerateTableRefChildren(*bound_select.from_table, callback);
199: 		}
200: 		break;
201: 	}
202: 	for (idx_t i = 0; i < node.modifiers.size(); i++) {
203: 		switch (node.modifiers[i]->type) {
204: 		case ResultModifierType::DISTINCT_MODIFIER:
205: 			for (auto &expr : ((BoundDistinctModifier &)*node.modifiers[i]).target_distincts) {
206: 				EnumerateExpression(expr, callback);
207: 			}
208: 			break;
209: 		case ResultModifierType::ORDER_MODIFIER:
210: 			for (auto &order : ((BoundOrderModifier &)*node.modifiers[i]).orders) {
211: 				EnumerateExpression(order.expression, callback);
212: 			}
213: 			break;
214: 		default:
215: 			break;
216: 		}
217: 	}
218: }
219: 
220: } // namespace duckdb
[end of src/planner/expression_iterator.cpp]
[start of src/planner/subquery/flatten_dependent_join.cpp]
1: #include "duckdb/planner/subquery/flatten_dependent_join.hpp"
2: 
3: #include "duckdb/planner/binder.hpp"
4: #include "duckdb/planner/expression/list.hpp"
5: #include "duckdb/planner/logical_operator_visitor.hpp"
6: #include "duckdb/planner/binder.hpp"
7: #include "duckdb/planner/operator/list.hpp"
8: #include "duckdb/planner/subquery/has_correlated_expressions.hpp"
9: #include "duckdb/planner/subquery/rewrite_correlated_expressions.hpp"
10: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
11: #include "duckdb/catalog/catalog_entry/aggregate_function_catalog_entry.hpp"
12: #include "duckdb/function/aggregate/distributive_functions.hpp"
13: 
14: namespace duckdb {
15: 
16: FlattenDependentJoins::FlattenDependentJoins(Binder &binder, const vector<CorrelatedColumnInfo> &correlated)
17:     : binder(binder), correlated_columns(correlated) {
18: 	for (idx_t i = 0; i < correlated_columns.size(); i++) {
19: 		auto &col = correlated_columns[i];
20: 		correlated_map[col.binding] = i;
21: 		delim_types.push_back(col.type);
22: 	}
23: }
24: 
25: bool FlattenDependentJoins::DetectCorrelatedExpressions(LogicalOperator *op) {
26: 	D_ASSERT(op);
27: 	// check if this entry has correlated expressions
28: 	HasCorrelatedExpressions visitor(correlated_columns);
29: 	visitor.VisitOperator(*op);
30: 	bool has_correlation = visitor.has_correlated_expressions;
31: 	// now visit the children of this entry and check if they have correlated expressions
32: 	for (auto &child : op->children) {
33: 		// we OR the property with its children such that has_correlation is true if either
34: 		// (1) this node has a correlated expression or
35: 		// (2) one of its children has a correlated expression
36: 		if (DetectCorrelatedExpressions(child.get())) {
37: 			has_correlation = true;
38: 		}
39: 	}
40: 	// set the entry in the map
41: 	has_correlated_expressions[op] = has_correlation;
42: 	return has_correlation;
43: }
44: 
45: unique_ptr<LogicalOperator> FlattenDependentJoins::PushDownDependentJoin(unique_ptr<LogicalOperator> plan) {
46: 	auto result = PushDownDependentJoinInternal(move(plan));
47: 	if (!replacement_map.empty()) {
48: 		// check if we have to replace any COUNT aggregates into "CASE WHEN X IS NULL THEN 0 ELSE COUNT END"
49: 		RewriteCountAggregates aggr(replacement_map);
50: 		aggr.VisitOperator(*result);
51: 	}
52: 	return result;
53: }
54: 
55: unique_ptr<LogicalOperator> FlattenDependentJoins::PushDownDependentJoinInternal(unique_ptr<LogicalOperator> plan) {
56: 	// first check if the logical operator has correlated expressions
57: 	auto entry = has_correlated_expressions.find(plan.get());
58: 	D_ASSERT(entry != has_correlated_expressions.end());
59: 	if (!entry->second) {
60: 		// we reached a node without correlated expressions
61: 		// we can eliminate the dependent join now and create a simple cross product
62: 		auto cross_product = make_unique<LogicalCrossProduct>();
63: 		// now create the duplicate eliminated scan for this node
64: 		auto delim_index = binder.GenerateTableIndex();
65: 		this->base_binding = ColumnBinding(delim_index, 0);
66: 		auto delim_scan = make_unique<LogicalDelimGet>(delim_index, delim_types);
67: 		cross_product->children.push_back(move(delim_scan));
68: 		cross_product->children.push_back(move(plan));
69: 		return move(cross_product);
70: 	}
71: 	switch (plan->type) {
72: 	case LogicalOperatorType::LOGICAL_UNNEST:
73: 	case LogicalOperatorType::LOGICAL_FILTER: {
74: 		// filter
75: 		// first we flatten the dependent join in the child of the filter
76: 		plan->children[0] = PushDownDependentJoinInternal(move(plan->children[0]));
77: 		// then we replace any correlated expressions with the corresponding entry in the correlated_map
78: 		RewriteCorrelatedExpressions rewriter(base_binding, correlated_map);
79: 		rewriter.VisitOperator(*plan);
80: 		return plan;
81: 	}
82: 	case LogicalOperatorType::LOGICAL_PROJECTION: {
83: 		// projection
84: 		// first we flatten the dependent join in the child of the projection
85: 		plan->children[0] = PushDownDependentJoinInternal(move(plan->children[0]));
86: 		// then we replace any correlated expressions with the corresponding entry in the correlated_map
87: 		RewriteCorrelatedExpressions rewriter(base_binding, correlated_map);
88: 		rewriter.VisitOperator(*plan);
89: 		// now we add all the columns of the delim_scan to the projection list
90: 		auto proj = (LogicalProjection *)plan.get();
91: 		for (idx_t i = 0; i < correlated_columns.size(); i++) {
92: 			auto colref = make_unique<BoundColumnRefExpression>(
93: 			    correlated_columns[i].type, ColumnBinding(base_binding.table_index, base_binding.column_index + i));
94: 			plan->expressions.push_back(move(colref));
95: 		}
96: 
97: 		base_binding.table_index = proj->table_index;
98: 		this->delim_offset = base_binding.column_index = plan->expressions.size() - correlated_columns.size();
99: 		this->data_offset = 0;
100: 		return plan;
101: 	}
102: 	case LogicalOperatorType::LOGICAL_AGGREGATE_AND_GROUP_BY: {
103: 		auto &aggr = (LogicalAggregate &)*plan;
104: 		// aggregate and group by
105: 		// first we flatten the dependent join in the child of the projection
106: 		plan->children[0] = PushDownDependentJoinInternal(move(plan->children[0]));
107: 		// then we replace any correlated expressions with the corresponding entry in the correlated_map
108: 		RewriteCorrelatedExpressions rewriter(base_binding, correlated_map);
109: 		rewriter.VisitOperator(*plan);
110: 		// now we add all the columns of the delim_scan to the grouping operators AND the projection list
111: 		for (idx_t i = 0; i < correlated_columns.size(); i++) {
112: 			auto colref = make_unique<BoundColumnRefExpression>(
113: 			    correlated_columns[i].type, ColumnBinding(base_binding.table_index, base_binding.column_index + i));
114: 			for (auto &set : aggr.grouping_sets) {
115: 				set.insert(aggr.groups.size());
116: 			}
117: 			aggr.groups.push_back(move(colref));
118: 		}
119: 		if (aggr.groups.size() == correlated_columns.size()) {
120: 			// we have to perform a LEFT OUTER JOIN between the result of this aggregate and the delim scan
121: 			// FIXME: this does not always have to be a LEFT OUTER JOIN, depending on whether aggr.expressions return
122: 			// NULL or a value
123: 			auto left_outer_join = make_unique<LogicalComparisonJoin>(JoinType::LEFT);
124: 			auto left_index = binder.GenerateTableIndex();
125: 			auto delim_scan = make_unique<LogicalDelimGet>(left_index, delim_types);
126: 			left_outer_join->children.push_back(move(delim_scan));
127: 			left_outer_join->children.push_back(move(plan));
128: 			for (idx_t i = 0; i < correlated_columns.size(); i++) {
129: 				JoinCondition cond;
130: 				cond.left =
131: 				    make_unique<BoundColumnRefExpression>(correlated_columns[i].type, ColumnBinding(left_index, i));
132: 				cond.right = make_unique<BoundColumnRefExpression>(
133: 				    correlated_columns[i].type,
134: 				    ColumnBinding(aggr.group_index, (aggr.groups.size() - correlated_columns.size()) + i));
135: 				cond.comparison = ExpressionType::COMPARE_EQUAL;
136: 				cond.null_values_are_equal = true;
137: 				left_outer_join->conditions.push_back(move(cond));
138: 			}
139: 			// for any COUNT aggregate we replace references to the column with: CASE WHEN COUNT(*) IS NULL THEN 0
140: 			// ELSE COUNT(*) END
141: 			for (idx_t i = 0; i < aggr.expressions.size(); i++) {
142: 				D_ASSERT(aggr.expressions[i]->GetExpressionClass() == ExpressionClass::BOUND_AGGREGATE);
143: 				auto bound = (BoundAggregateExpression *)&*aggr.expressions[i];
144: 				vector<LogicalType> arguments;
145: 				if (bound->function == CountFun::GetFunction() || bound->function == CountStarFun::GetFunction()) {
146: 					// have to replace this ColumnBinding with the CASE expression
147: 					replacement_map[ColumnBinding(aggr.aggregate_index, i)] = i;
148: 				}
149: 			}
150: 			// now we update the delim_index
151: 
152: 			base_binding.table_index = left_index;
153: 			this->delim_offset = base_binding.column_index = 0;
154: 			this->data_offset = 0;
155: 			return move(left_outer_join);
156: 		} else {
157: 			// update the delim_index
158: 			base_binding.table_index = aggr.group_index;
159: 			this->delim_offset = base_binding.column_index = aggr.groups.size() - correlated_columns.size();
160: 			this->data_offset = aggr.groups.size();
161: 			return plan;
162: 		}
163: 	}
164: 	case LogicalOperatorType::LOGICAL_CROSS_PRODUCT: {
165: 		// cross product
166: 		// push into both sides of the plan
167: 		bool left_has_correlation = has_correlated_expressions.find(plan->children[0].get())->second;
168: 		bool right_has_correlation = has_correlated_expressions.find(plan->children[1].get())->second;
169: 		if (!right_has_correlation) {
170: 			// only left has correlation: push into left
171: 			plan->children[0] = PushDownDependentJoinInternal(move(plan->children[0]));
172: 			return plan;
173: 		}
174: 		if (!left_has_correlation) {
175: 			// only right has correlation: push into right
176: 			plan->children[1] = PushDownDependentJoinInternal(move(plan->children[1]));
177: 			return plan;
178: 		}
179: 		// both sides have correlation
180: 		// turn into an inner join
181: 		auto join = make_unique<LogicalComparisonJoin>(JoinType::INNER);
182: 		plan->children[0] = PushDownDependentJoinInternal(move(plan->children[0]));
183: 		auto left_binding = this->base_binding;
184: 		plan->children[1] = PushDownDependentJoinInternal(move(plan->children[1]));
185: 		// add the correlated columns to the join conditions
186: 		for (idx_t i = 0; i < correlated_columns.size(); i++) {
187: 			JoinCondition cond;
188: 			cond.left = make_unique<BoundColumnRefExpression>(
189: 			    correlated_columns[i].type, ColumnBinding(left_binding.table_index, left_binding.column_index + i));
190: 			cond.right = make_unique<BoundColumnRefExpression>(
191: 			    correlated_columns[i].type, ColumnBinding(base_binding.table_index, base_binding.column_index + i));
192: 			cond.comparison = ExpressionType::COMPARE_EQUAL;
193: 			cond.null_values_are_equal = true;
194: 			join->conditions.push_back(move(cond));
195: 		}
196: 		join->children.push_back(move(plan->children[0]));
197: 		join->children.push_back(move(plan->children[1]));
198: 		return move(join);
199: 	}
200: 	case LogicalOperatorType::LOGICAL_COMPARISON_JOIN: {
201: 		auto &join = (LogicalComparisonJoin &)*plan;
202: 		D_ASSERT(plan->children.size() == 2);
203: 		// check the correlated expressions in the children of the join
204: 		bool left_has_correlation = has_correlated_expressions.find(plan->children[0].get())->second;
205: 		bool right_has_correlation = has_correlated_expressions.find(plan->children[1].get())->second;
206: 
207: 		if (join.join_type == JoinType::INNER) {
208: 			// inner join
209: 			if (!right_has_correlation) {
210: 				// only left has correlation: push into left
211: 				plan->children[0] = PushDownDependentJoinInternal(move(plan->children[0]));
212: 				return plan;
213: 			}
214: 			if (!left_has_correlation) {
215: 				// only right has correlation: push into right
216: 				plan->children[1] = PushDownDependentJoinInternal(move(plan->children[1]));
217: 				return plan;
218: 			}
219: 		} else if (join.join_type == JoinType::LEFT) {
220: 			// left outer join
221: 			if (!right_has_correlation) {
222: 				// only left has correlation: push into left
223: 				plan->children[0] = PushDownDependentJoinInternal(move(plan->children[0]));
224: 				return plan;
225: 			}
226: 		} else if (join.join_type == JoinType::MARK) {
227: 			if (right_has_correlation) {
228: 				throw Exception("MARK join with correlation in RHS not supported");
229: 			}
230: 			// push the child into the LHS
231: 			plan->children[0] = PushDownDependentJoinInternal(move(plan->children[0]));
232: 			// rewrite expressions in the join conditions
233: 			RewriteCorrelatedExpressions rewriter(base_binding, correlated_map);
234: 			rewriter.VisitOperator(*plan);
235: 			return plan;
236: 		} else {
237: 			throw Exception("Unsupported join type for flattening correlated subquery");
238: 		}
239: 		// both sides have correlation
240: 		// push into both sides
241: 		// NOTE: for OUTER JOINS it matters what the BASE BINDING is after the join
242: 		// for the LEFT OUTER JOIN, we want the LEFT side to be the base binding after we push
243: 		// because the RIGHT binding might contain NULL values
244: 		plan->children[0] = PushDownDependentJoinInternal(move(plan->children[0]));
245: 		auto left_binding = this->base_binding;
246: 		plan->children[1] = PushDownDependentJoinInternal(move(plan->children[1]));
247: 		auto right_binding = this->base_binding;
248: 		if (join.join_type == JoinType::LEFT) {
249: 			this->base_binding = left_binding;
250: 		}
251: 		// add the correlated columns to the join conditions
252: 		for (idx_t i = 0; i < correlated_columns.size(); i++) {
253: 			JoinCondition cond;
254: 
255: 			cond.left = make_unique<BoundColumnRefExpression>(
256: 			    correlated_columns[i].type, ColumnBinding(left_binding.table_index, left_binding.column_index + i));
257: 			cond.right = make_unique<BoundColumnRefExpression>(
258: 			    correlated_columns[i].type, ColumnBinding(right_binding.table_index, right_binding.column_index + i));
259: 			cond.comparison = ExpressionType::COMPARE_EQUAL;
260: 			cond.null_values_are_equal = true;
261: 			join.conditions.push_back(move(cond));
262: 		}
263: 		// then we replace any correlated expressions with the corresponding entry in the correlated_map
264: 		RewriteCorrelatedExpressions rewriter(right_binding, correlated_map);
265: 		rewriter.VisitOperator(*plan);
266: 		return plan;
267: 	}
268: 	case LogicalOperatorType::LOGICAL_LIMIT: {
269: 		auto &limit = (LogicalLimit &)*plan;
270: 		if (limit.offset_val > 0) {
271: 			throw ParserException("OFFSET not supported in correlated subquery");
272: 		}
273: 		if (limit.limit) {
274: 			throw ParserException("Non-constant limit not supported in correlated subquery");
275: 		}
276: 		plan->children[0] = PushDownDependentJoinInternal(move(plan->children[0]));
277: 		if (limit.limit_val == 0) {
278: 			// limit = 0 means we return zero columns here
279: 			return plan;
280: 		} else {
281: 			// limit > 0 does nothing
282: 			return move(plan->children[0]);
283: 		}
284: 	}
285: 	case LogicalOperatorType::LOGICAL_WINDOW: {
286: 		auto &window = (LogicalWindow &)*plan;
287: 		// push into children
288: 		plan->children[0] = PushDownDependentJoinInternal(move(plan->children[0]));
289: 		// add the correlated columns to the PARTITION BY clauses in the Window
290: 		for (auto &expr : window.expressions) {
291: 			D_ASSERT(expr->GetExpressionClass() == ExpressionClass::BOUND_WINDOW);
292: 			auto &w = (BoundWindowExpression &)*expr;
293: 			for (idx_t i = 0; i < correlated_columns.size(); i++) {
294: 				w.partitions.push_back(make_unique<BoundColumnRefExpression>(
295: 				    correlated_columns[i].type,
296: 				    ColumnBinding(base_binding.table_index, base_binding.column_index + i)));
297: 			}
298: 		}
299: 		return plan;
300: 	}
301: 	case LogicalOperatorType::LOGICAL_EXCEPT:
302: 	case LogicalOperatorType::LOGICAL_INTERSECT:
303: 	case LogicalOperatorType::LOGICAL_UNION: {
304: 		auto &setop = (LogicalSetOperation &)*plan;
305: 		// set operator, push into both children
306: 		plan->children[0] = PushDownDependentJoin(move(plan->children[0]));
307: 		plan->children[1] = PushDownDependentJoin(move(plan->children[1]));
308: 		// we have to refer to the setop index now
309: 		base_binding.table_index = setop.table_index;
310: 		base_binding.column_index = setop.column_count;
311: 		setop.column_count += correlated_columns.size();
312: 		return plan;
313: 	}
314: 	case LogicalOperatorType::LOGICAL_DISTINCT:
315: 		plan->children[0] = PushDownDependentJoin(move(plan->children[0]));
316: 		return plan;
317: 	case LogicalOperatorType::LOGICAL_ORDER_BY:
318: 		throw ParserException("ORDER BY not supported in correlated subquery");
319: 	default:
320: 		throw InternalException("Logical operator type \"%s\" for dependent join", LogicalOperatorToString(plan->type));
321: 	}
322: }
323: 
324: } // namespace duckdb
[end of src/planner/subquery/flatten_dependent_join.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: