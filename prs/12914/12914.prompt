You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
information_schema.table_constraints and key_column_usage always list "memory" for table_catalog on ATTACHed databases
### What happens?

When using the `ATTACH` statement, the `information_schema.table_constraints` and `information_schema.key_column_usage` tables will always list `memory` for the `constraint_catalog` and `table_catalog`.

This makes it impossible to determine which attached database the constraints come from when multiple database files are attached, especially when some have some tables or constraints with the same names.

Thus making it not possible to read the full database schema from the `information_schema` of those attached databases.

### To Reproduce

First test behavior when starting duckdb with specified database which works correctly:
1. From command line run `duckdb test.db`  where test.db is a DuckDB file with some tables.
2. Run `select * from information_schema.table_constraints;`
3. Notice `constraint_catalog` and `table_catalog` for all tables from `test.db` are correctly listed as `test`
4. Same with `select * from information_schema.key_column_usage;`
5. Now try `select * from information_schema.tables;`
6. Notice `table_catalog` is correctly listed as `test`

Then check behavior when using `ATTACH`
1. From command line run `duckdb` 
2. Attach a `duckdb` database eg. `ATTACH 'test.db' as test`
3. Run `select * from information_schema.table_constraints;`
4. Notice `constraint_catalog` and `table_catalog` for all tables are listed as `memory`
7. Same with `select * from information_schema.key_column_usage;`
8. Now try `select * from information_schema.tables;`
9. Notice in this case `table_catalog` is correctly listed as `test`

(Didn't share data set because it is reproducible with any DuckDB database file that has tables with constraints.)

### OS:

Tested on Mac OS (ARM64) and Windows (x64) with same behavior through both command line and Node.js sdk.

### DuckDB Version:

v1.0.0 1f98600c2c

### DuckDB Client:

Command Line, Node.js SDK

### Full Name:

Chanon Sajamanochai

### Affiliation:

Independent developer

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - Other reason (please specify in the issue body)

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://www.duckdb.org/docs/installation) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/catalog/catalog_entry/table_catalog_entry.cpp]
1: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/common/algorithm.hpp"
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/main/database.hpp"
8: #include "duckdb/parser/constraints/list.hpp"
9: #include "duckdb/parser/parsed_data/create_table_info.hpp"
10: #include "duckdb/storage/table_storage_info.hpp"
11: #include "duckdb/planner/operator/logical_update.hpp"
12: #include "duckdb/planner/operator/logical_get.hpp"
13: #include "duckdb/planner/constraints/bound_check_constraint.hpp"
14: #include "duckdb/planner/operator/logical_projection.hpp"
15: #include "duckdb/common/extra_type_info.hpp"
16: #include "duckdb/parser/expression/cast_expression.hpp"
17: 
18: #include <sstream>
19: 
20: namespace duckdb {
21: 
22: TableCatalogEntry::TableCatalogEntry(Catalog &catalog, SchemaCatalogEntry &schema, CreateTableInfo &info)
23:     : StandardEntry(CatalogType::TABLE_ENTRY, schema, catalog, info.table), columns(std::move(info.columns)),
24:       constraints(std::move(info.constraints)) {
25: 	this->temporary = info.temporary;
26: 	this->dependencies = info.dependencies;
27: 	this->comment = info.comment;
28: 	this->tags = info.tags;
29: }
30: 
31: bool TableCatalogEntry::HasGeneratedColumns() const {
32: 	return columns.LogicalColumnCount() != columns.PhysicalColumnCount();
33: }
34: 
35: LogicalIndex TableCatalogEntry::GetColumnIndex(string &column_name, bool if_exists) {
36: 	auto entry = columns.GetColumnIndex(column_name);
37: 	if (!entry.IsValid()) {
38: 		if (if_exists) {
39: 			return entry;
40: 		}
41: 		throw BinderException("Table \"%s\" does not have a column with name \"%s\"", name, column_name);
42: 	}
43: 	return entry;
44: }
45: 
46: bool TableCatalogEntry::ColumnExists(const string &name) {
47: 	return columns.ColumnExists(name);
48: }
49: 
50: const ColumnDefinition &TableCatalogEntry::GetColumn(const string &name) {
51: 	return columns.GetColumn(name);
52: }
53: 
54: vector<LogicalType> TableCatalogEntry::GetTypes() {
55: 	vector<LogicalType> types;
56: 	for (auto &col : columns.Physical()) {
57: 		types.push_back(col.Type());
58: 	}
59: 	return types;
60: }
61: 
62: unique_ptr<CreateInfo> TableCatalogEntry::GetInfo() const {
63: 	auto result = make_uniq<CreateTableInfo>();
64: 	result->catalog = catalog.GetName();
65: 	result->schema = schema.name;
66: 	result->table = name;
67: 	result->columns = columns.Copy();
68: 	result->constraints.reserve(constraints.size());
69: 	result->dependencies = dependencies;
70: 	std::for_each(constraints.begin(), constraints.end(),
71: 	              [&result](const unique_ptr<Constraint> &c) { result->constraints.emplace_back(c->Copy()); });
72: 	result->comment = comment;
73: 	result->tags = tags;
74: 	return std::move(result);
75: }
76: 
77: string TableCatalogEntry::ColumnsToSQL(const ColumnList &columns, const vector<unique_ptr<Constraint>> &constraints) {
78: 	std::stringstream ss;
79: 
80: 	ss << "(";
81: 
82: 	// find all columns that have NOT NULL specified, but are NOT primary key columns
83: 	logical_index_set_t not_null_columns;
84: 	logical_index_set_t unique_columns;
85: 	logical_index_set_t pk_columns;
86: 	unordered_set<string> multi_key_pks;
87: 	vector<string> extra_constraints;
88: 	for (auto &constraint : constraints) {
89: 		if (constraint->type == ConstraintType::NOT_NULL) {
90: 			auto &not_null = constraint->Cast<NotNullConstraint>();
91: 			not_null_columns.insert(not_null.index);
92: 		} else if (constraint->type == ConstraintType::UNIQUE) {
93: 			auto &pk = constraint->Cast<UniqueConstraint>();
94: 			if (pk.HasIndex()) {
95: 				// no columns specified: single column constraint
96: 				if (pk.IsPrimaryKey()) {
97: 					pk_columns.insert(pk.GetIndex());
98: 				} else {
99: 					unique_columns.insert(pk.GetIndex());
100: 				}
101: 			} else {
102: 				// multi-column constraint, this constraint needs to go at the end after all columns
103: 				if (pk.IsPrimaryKey()) {
104: 					// multi key pk column: insert set of columns into multi_key_pks
105: 					for (auto &col : pk.GetColumnNames()) {
106: 						multi_key_pks.insert(col);
107: 					}
108: 				}
109: 				extra_constraints.push_back(constraint->ToString());
110: 			}
111: 		} else if (constraint->type == ConstraintType::FOREIGN_KEY) {
112: 			auto &fk = constraint->Cast<ForeignKeyConstraint>();
113: 			if (fk.info.type == ForeignKeyType::FK_TYPE_FOREIGN_KEY_TABLE ||
114: 			    fk.info.type == ForeignKeyType::FK_TYPE_SELF_REFERENCE_TABLE) {
115: 				extra_constraints.push_back(constraint->ToString());
116: 			}
117: 		} else {
118: 			extra_constraints.push_back(constraint->ToString());
119: 		}
120: 	}
121: 
122: 	for (auto &column : columns.Logical()) {
123: 		if (column.Oid() > 0) {
124: 			ss << ", ";
125: 		}
126: 		ss << KeywordHelper::WriteOptionallyQuoted(column.Name()) << " ";
127: 		auto &column_type = column.Type();
128: 		if (column_type.id() != LogicalTypeId::ANY) {
129: 			ss << column.Type().ToString();
130: 		}
131: 		auto extra_type_info = column_type.AuxInfo();
132: 		if (extra_type_info && extra_type_info->type == ExtraTypeInfoType::STRING_TYPE_INFO) {
133: 			auto &string_info = extra_type_info->Cast<StringTypeInfo>();
134: 			if (!string_info.collation.empty()) {
135: 				ss << " COLLATE " + string_info.collation;
136: 			}
137: 		}
138: 		bool not_null = not_null_columns.find(column.Logical()) != not_null_columns.end();
139: 		bool is_single_key_pk = pk_columns.find(column.Logical()) != pk_columns.end();
140: 		bool is_multi_key_pk = multi_key_pks.find(column.Name()) != multi_key_pks.end();
141: 		bool is_unique = unique_columns.find(column.Logical()) != unique_columns.end();
142: 		if (column.Generated()) {
143: 			reference<const ParsedExpression> generated_expression = column.GeneratedExpression();
144: 			if (column_type.id() != LogicalTypeId::ANY) {
145: 				// We artificially add a cast if the type is specified, need to strip it
146: 				auto &expr = generated_expression.get();
147: 				D_ASSERT(expr.type == ExpressionType::OPERATOR_CAST);
148: 				auto &cast_expr = expr.Cast<CastExpression>();
149: 				D_ASSERT(cast_expr.cast_type.id() == column_type.id());
150: 				generated_expression = *cast_expr.child;
151: 			}
152: 			ss << " GENERATED ALWAYS AS(" << generated_expression.get().ToString() << ")";
153: 		} else if (column.HasDefaultValue()) {
154: 			ss << " DEFAULT(" << column.DefaultValue().ToString() << ")";
155: 		}
156: 		if (not_null && !is_single_key_pk && !is_multi_key_pk) {
157: 			// NOT NULL but not a primary key column
158: 			ss << " NOT NULL";
159: 		}
160: 		if (is_single_key_pk) {
161: 			// single column pk: insert constraint here
162: 			ss << " PRIMARY KEY";
163: 		}
164: 		if (is_unique) {
165: 			// single column unique: insert constraint here
166: 			ss << " UNIQUE";
167: 		}
168: 	}
169: 	// print any extra constraints that still need to be printed
170: 	for (auto &extra_constraint : extra_constraints) {
171: 		ss << ", ";
172: 		ss << extra_constraint;
173: 	}
174: 
175: 	ss << ")";
176: 	return ss.str();
177: }
178: 
179: string TableCatalogEntry::ToSQL() const {
180: 	auto create_info = GetInfo();
181: 	return create_info->ToString();
182: }
183: 
184: const ColumnList &TableCatalogEntry::GetColumns() const {
185: 	return columns;
186: }
187: 
188: const ColumnDefinition &TableCatalogEntry::GetColumn(LogicalIndex idx) {
189: 	return columns.GetColumn(idx);
190: }
191: 
192: const vector<unique_ptr<Constraint>> &TableCatalogEntry::GetConstraints() const {
193: 	return constraints;
194: }
195: 
196: // LCOV_EXCL_START
197: DataTable &TableCatalogEntry::GetStorage() {
198: 	throw InternalException("Calling GetStorage on a TableCatalogEntry that is not a DuckTableEntry");
199: }
200: // LCOV_EXCL_STOP
201: 
202: static void BindExtraColumns(TableCatalogEntry &table, LogicalGet &get, LogicalProjection &proj, LogicalUpdate &update,
203:                              physical_index_set_t &bound_columns) {
204: 	if (bound_columns.size() <= 1) {
205: 		return;
206: 	}
207: 	idx_t found_column_count = 0;
208: 	physical_index_set_t found_columns;
209: 	for (idx_t i = 0; i < update.columns.size(); i++) {
210: 		if (bound_columns.find(update.columns[i]) != bound_columns.end()) {
211: 			// this column is referenced in the CHECK constraint
212: 			found_column_count++;
213: 			found_columns.insert(update.columns[i]);
214: 		}
215: 	}
216: 	if (found_column_count > 0 && found_column_count != bound_columns.size()) {
217: 		// columns in this CHECK constraint were referenced, but not all were part of the UPDATE
218: 		// add them to the scan and update set
219: 		for (auto &check_column_id : bound_columns) {
220: 			if (found_columns.find(check_column_id) != found_columns.end()) {
221: 				// column is already projected
222: 				continue;
223: 			}
224: 			// column is not projected yet: project it by adding the clause "i=i" to the set of updated columns
225: 			auto &column = table.GetColumns().GetColumn(check_column_id);
226: 			update.expressions.push_back(make_uniq<BoundColumnRefExpression>(
227: 			    column.Type(), ColumnBinding(proj.table_index, proj.expressions.size())));
228: 			proj.expressions.push_back(make_uniq<BoundColumnRefExpression>(
229: 			    column.Type(), ColumnBinding(get.table_index, get.column_ids.size())));
230: 			get.column_ids.push_back(check_column_id.index);
231: 			update.columns.push_back(check_column_id);
232: 		}
233: 	}
234: }
235: 
236: static bool TypeSupportsRegularUpdate(const LogicalType &type) {
237: 	switch (type.id()) {
238: 	case LogicalTypeId::LIST:
239: 	case LogicalTypeId::ARRAY:
240: 	case LogicalTypeId::MAP:
241: 	case LogicalTypeId::UNION:
242: 		// lists and maps and unions don't support updates directly
243: 		return false;
244: 	case LogicalTypeId::STRUCT: {
245: 		auto &child_types = StructType::GetChildTypes(type);
246: 		for (auto &entry : child_types) {
247: 			if (!TypeSupportsRegularUpdate(entry.second)) {
248: 				return false;
249: 			}
250: 		}
251: 		return true;
252: 	}
253: 	default:
254: 		return true;
255: 	}
256: }
257: 
258: vector<ColumnSegmentInfo> TableCatalogEntry::GetColumnSegmentInfo() {
259: 	return {};
260: }
261: 
262: void TableCatalogEntry::BindUpdateConstraints(Binder &binder, LogicalGet &get, LogicalProjection &proj,
263:                                               LogicalUpdate &update, ClientContext &context) {
264: 	// check the constraints and indexes of the table to see if we need to project any additional columns
265: 	// we do this for indexes with multiple columns and CHECK constraints in the UPDATE clause
266: 	// suppose we have a constraint CHECK(i + j < 10); now we need both i and j to check the constraint
267: 	// if we are only updating one of the two columns we add the other one to the UPDATE set
268: 	// with a "useless" update (i.e. i=i) so we can verify that the CHECK constraint is not violated
269: 	auto bound_constraints = binder.BindConstraints(constraints, name, columns);
270: 	for (auto &constraint : bound_constraints) {
271: 		if (constraint->type == ConstraintType::CHECK) {
272: 			auto &check = constraint->Cast<BoundCheckConstraint>();
273: 			// check constraint! check if we need to add any extra columns to the UPDATE clause
274: 			BindExtraColumns(*this, get, proj, update, check.bound_columns);
275: 		}
276: 	}
277: 	if (update.return_chunk) {
278: 		physical_index_set_t all_columns;
279: 		for (auto &column : GetColumns().Physical()) {
280: 			all_columns.insert(column.Physical());
281: 		}
282: 		BindExtraColumns(*this, get, proj, update, all_columns);
283: 	}
284: 	// for index updates we always turn any update into an insert and a delete
285: 	// we thus need all the columns to be available, hence we check if the update touches any index columns
286: 	// If the returning keyword is used, we need access to the whole row in case the user requests it.
287: 	// Therefore switch the update to a delete and insert.
288: 	update.update_is_del_and_insert = false;
289: 	TableStorageInfo table_storage_info = GetStorageInfo(context);
290: 	for (auto index : table_storage_info.index_info) {
291: 		for (auto &column : update.columns) {
292: 			if (index.column_set.find(column.index) != index.column_set.end()) {
293: 				update.update_is_del_and_insert = true;
294: 				break;
295: 			}
296: 		}
297: 	};
298: 
299: 	// we also convert any updates on LIST columns into delete + insert
300: 	for (auto &col_index : update.columns) {
301: 		auto &column = GetColumns().GetColumn(col_index);
302: 		if (!TypeSupportsRegularUpdate(column.Type())) {
303: 			update.update_is_del_and_insert = true;
304: 			break;
305: 		}
306: 	}
307: 
308: 	if (update.update_is_del_and_insert) {
309: 		// the update updates a column required by an index or requires returning the updated rows,
310: 		// push projections for all columns
311: 		physical_index_set_t all_columns;
312: 		for (auto &column : GetColumns().Physical()) {
313: 			all_columns.insert(column.Physical());
314: 		}
315: 		BindExtraColumns(*this, get, proj, update, all_columns);
316: 	}
317: }
318: 
319: } // namespace duckdb
[end of src/catalog/catalog_entry/table_catalog_entry.cpp]
[start of src/catalog/default/default_views.cpp]
1: #include "duckdb/catalog/default/default_views.hpp"
2: #include "duckdb/planner/binder.hpp"
3: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
4: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
5: #include "duckdb/common/string_util.hpp"
6: 
7: namespace duckdb {
8: 
9: struct DefaultView {
10: 	const char *schema;
11: 	const char *name;
12: 	const char *sql;
13: };
14: 
15: static const DefaultView internal_views[] = {
16:     {DEFAULT_SCHEMA, "pragma_database_list", "SELECT database_oid AS seq, database_name AS name, path AS file FROM duckdb_databases() WHERE NOT internal ORDER BY 1"},
17:     {DEFAULT_SCHEMA, "sqlite_master", "select 'table' \"type\", table_name \"name\", table_name \"tbl_name\", 0 rootpage, sql from duckdb_tables union all select 'view' \"type\", view_name \"name\", view_name \"tbl_name\", 0 rootpage, sql from duckdb_views union all select 'index' \"type\", index_name \"name\", table_name \"tbl_name\", 0 rootpage, sql from duckdb_indexes;"},
18:     {DEFAULT_SCHEMA, "sqlite_schema", "SELECT * FROM sqlite_master"},
19:     {DEFAULT_SCHEMA, "sqlite_temp_master", "SELECT * FROM sqlite_master"},
20:     {DEFAULT_SCHEMA, "sqlite_temp_schema", "SELECT * FROM sqlite_master"},
21:     {DEFAULT_SCHEMA, "duckdb_constraints", "SELECT * FROM duckdb_constraints()"},
22:     {DEFAULT_SCHEMA, "duckdb_columns", "SELECT * FROM duckdb_columns() WHERE NOT internal"},
23:     {DEFAULT_SCHEMA, "duckdb_databases", "SELECT * FROM duckdb_databases() WHERE NOT internal"},
24:     {DEFAULT_SCHEMA, "duckdb_indexes", "SELECT * FROM duckdb_indexes()"},
25:     {DEFAULT_SCHEMA, "duckdb_schemas", "SELECT * FROM duckdb_schemas() WHERE NOT internal"},
26:     {DEFAULT_SCHEMA, "duckdb_tables", "SELECT * FROM duckdb_tables() WHERE NOT internal"},
27:     {DEFAULT_SCHEMA, "duckdb_types", "SELECT * FROM duckdb_types()"},
28:     {DEFAULT_SCHEMA, "duckdb_views", "SELECT * FROM duckdb_views() WHERE NOT internal"},
29:     {"pg_catalog", "pg_am", "SELECT 0 oid, 'art' amname, NULL amhandler, 'i' amtype"},
30:     {"pg_catalog", "pg_attribute", "SELECT table_oid attrelid, column_name attname, data_type_id atttypid, 0 attstattarget, NULL attlen, column_index attnum, 0 attndims, -1 attcacheoff, case when data_type ilike '%decimal%' then numeric_precision*1000+numeric_scale else -1 end atttypmod, false attbyval, NULL attstorage, NULL attalign, NOT is_nullable attnotnull, column_default IS NOT NULL atthasdef, false atthasmissing, '' attidentity, '' attgenerated, false attisdropped, true attislocal, 0 attinhcount, 0 attcollation, NULL attcompression, NULL attacl, NULL attoptions, NULL attfdwoptions, NULL attmissingval FROM duckdb_columns()"},
31:     {"pg_catalog", "pg_attrdef", "SELECT column_index oid, table_oid adrelid, column_index adnum, column_default adbin from duckdb_columns() where column_default is not null;"},
32:     {"pg_catalog", "pg_class", "SELECT table_oid oid, table_name relname, schema_oid relnamespace, 0 reltype, 0 reloftype, 0 relowner, 0 relam, 0 relfilenode, 0 reltablespace, 0 relpages, estimated_size::real reltuples, 0 relallvisible, 0 reltoastrelid, 0 reltoastidxid, index_count > 0 relhasindex, false relisshared, case when temporary then 't' else 'p' end relpersistence, 'r' relkind, column_count relnatts, check_constraint_count relchecks, false relhasoids, has_primary_key relhaspkey, false relhasrules, false relhastriggers, false relhassubclass, false relrowsecurity, true relispopulated, NULL relreplident, false relispartition, 0 relrewrite, 0 relfrozenxid, NULL relminmxid, NULL relacl, NULL reloptions, NULL relpartbound FROM duckdb_tables() UNION ALL SELECT view_oid oid, view_name relname, schema_oid relnamespace, 0 reltype, 0 reloftype, 0 relowner, 0 relam, 0 relfilenode, 0 reltablespace, 0 relpages, 0 reltuples, 0 relallvisible, 0 reltoastrelid, 0 reltoastidxid, false relhasindex, false relisshared, case when temporary then 't' else 'p' end relpersistence, 'v' relkind, column_count relnatts, 0 relchecks, false relhasoids, false relhaspkey, false relhasrules, false relhastriggers, false relhassubclass, false relrowsecurity, true relispopulated, NULL relreplident, false relispartition, 0 relrewrite, 0 relfrozenxid, NULL relminmxid, NULL relacl, NULL reloptions, NULL relpartbound FROM duckdb_views() UNION ALL SELECT sequence_oid oid, sequence_name relname, schema_oid relnamespace, 0 reltype, 0 reloftype, 0 relowner, 0 relam, 0 relfilenode, 0 reltablespace, 0 relpages, 0 reltuples, 0 relallvisible, 0 reltoastrelid, 0 reltoastidxid, false relhasindex, false relisshared, case when temporary then 't' else 'p' end relpersistence, 'S' relkind, 0 relnatts, 0 relchecks, false relhasoids, false relhaspkey, false relhasrules, false relhastriggers, false relhassubclass, false relrowsecurity, true relispopulated, NULL relreplident, false relispartition, 0 relrewrite, 0 relfrozenxid, NULL relminmxid, NULL relacl, NULL reloptions, NULL relpartbound FROM duckdb_sequences() UNION ALL SELECT index_oid oid, index_name relname, schema_oid relnamespace, 0 reltype, 0 reloftype, 0 relowner, 0 relam, 0 relfilenode, 0 reltablespace, 0 relpages, 0 reltuples, 0 relallvisible, 0 reltoastrelid, 0 reltoastidxid, false relhasindex, false relisshared, 't' relpersistence, 'i' relkind, NULL relnatts, 0 relchecks, false relhasoids, false relhaspkey, false relhasrules, false relhastriggers, false relhassubclass, false relrowsecurity, true relispopulated, NULL relreplident, false relispartition, 0 relrewrite, 0 relfrozenxid, NULL relminmxid, NULL relacl, NULL reloptions, NULL relpartbound FROM duckdb_indexes()"},
33:     {"pg_catalog", "pg_constraint", "SELECT table_oid*1000000+constraint_index oid, constraint_text conname, schema_oid connamespace, CASE constraint_type WHEN 'CHECK' then 'c' WHEN 'UNIQUE' then 'u' WHEN 'PRIMARY KEY' THEN 'p' WHEN 'FOREIGN KEY' THEN 'f' ELSE 'x' END contype, false condeferrable, false condeferred, true convalidated, table_oid conrelid, 0 contypid, 0 conindid, 0 conparentid, 0 confrelid, NULL confupdtype, NULL confdeltype, NULL confmatchtype, true conislocal, 0 coninhcount, false connoinherit, constraint_column_indexes conkey, NULL confkey, NULL conpfeqop, NULL conppeqop, NULL conffeqop, NULL conexclop, expression conbin FROM duckdb_constraints()"},
34: 	{"pg_catalog", "pg_database", "SELECT database_oid oid, database_name datname FROM duckdb_databases()"},
35:     {"pg_catalog", "pg_depend", "SELECT * FROM duckdb_dependencies()"},
36: 	{"pg_catalog", "pg_description", "SELECT table_oid AS objoid, database_oid AS classoid, 0 AS objsubid, comment AS description FROM duckdb_tables() WHERE NOT internal UNION ALL SELECT table_oid AS objoid, database_oid AS classoid, column_index AS objsubid, comment AS description FROM duckdb_columns() WHERE NOT internal UNION ALL SELECT view_oid AS objoid, database_oid AS classoid, 0 AS objsubid, comment AS description FROM duckdb_views() WHERE NOT internal UNION ALL SELECT index_oid AS objoid, database_oid AS classoid, 0 AS objsubid, comment AS description FROM duckdb_indexes UNION ALL SELECT sequence_oid AS objoid, database_oid AS classoid, 0 AS objsubid, comment AS description FROM duckdb_sequences() UNION ALL SELECT type_oid AS objoid, database_oid AS classoid, 0 AS objsubid, comment AS description FROM duckdb_types() WHERE NOT internal UNION ALL SELECT function_oid AS objoid, database_oid AS classoid, 0 AS objsubid, comment AS description FROM duckdb_functions() WHERE NOT internal;"},
37:     {"pg_catalog", "pg_enum", "SELECT NULL oid, a.type_oid enumtypid, list_position(b.labels, a.elabel) enumsortorder, a.elabel enumlabel FROM (SELECT UNNEST(labels) elabel, type_oid FROM duckdb_types() WHERE logical_type='ENUM') a JOIN duckdb_types() b ON a.type_oid=b.type_oid;"},
38:     {"pg_catalog", "pg_index", "SELECT index_oid indexrelid, table_oid indrelid, 0 indnatts, 0 indnkeyatts, is_unique indisunique, is_primary indisprimary, false indisexclusion, true indimmediate, false indisclustered, true indisvalid, false indcheckxmin, true indisready, true indislive, false indisreplident, NULL::INT[] indkey, NULL::OID[] indcollation, NULL::OID[] indclass, NULL::INT[] indoption, expressions indexprs, NULL indpred FROM duckdb_indexes()"},
39:     {"pg_catalog", "pg_indexes", "SELECT schema_name schemaname, table_name tablename, index_name indexname, NULL \"tablespace\", sql indexdef FROM duckdb_indexes()"},
40:     {"pg_catalog", "pg_namespace", "SELECT oid, schema_name nspname, 0 nspowner, NULL nspacl FROM duckdb_schemas()"},
41: 	{"pg_catalog", "pg_proc", "SELECT f.function_oid oid, function_name proname, s.oid pronamespace,  NULL proowner, NULL prolang, 0 procost, 0 prorows, varargs provariadic,  0 prosupport, CASE function_type WHEN 'aggregate' THEN 'a' ELSE 'f' END prokind, false prosecdef, false proleakproof, false proisstrict, function_type = 'table' proretset,  case (stability) when 'CONSISTENT' then 'i' when 'CONSISTENT_WITHIN_QUERY' then 's' when 'VOLATILE' then 'v' end provolatile, 'u' proparallel, length(parameters)  pronargs, 0 pronargdefaults, return_type prorettype,  parameter_types proargtypes,  NULL proallargtypes, NULL proargmodes, parameters proargnames, NULL proargdefaults, NULL protrftypes, NULL prosrc, NULL probin, macro_definition prosqlbody, NULL proconfig, NULL proacl, function_type = 'aggregate' proisagg,  FROM duckdb_functions() f LEFT JOIN duckdb_schemas() s USING (database_name, schema_name)"},
42:     {"pg_catalog", "pg_sequence", "SELECT sequence_oid seqrelid, 0 seqtypid, start_value seqstart, increment_by seqincrement, max_value seqmax, min_value seqmin, 0 seqcache, cycle seqcycle FROM duckdb_sequences()"},
43: 	{"pg_catalog", "pg_sequences", "SELECT schema_name schemaname, sequence_name sequencename, 'duckdb' sequenceowner, 0 data_type, start_value, min_value, max_value, increment_by, cycle, 0 cache_size, last_value FROM duckdb_sequences()"},
44: 	{"pg_catalog", "pg_settings", "SELECT name, value setting, description short_desc, CASE WHEN input_type = 'VARCHAR' THEN 'string' WHEN input_type = 'BOOLEAN' THEN 'bool' WHEN input_type IN ('BIGINT', 'UBIGINT') THEN 'integer' ELSE input_type END vartype FROM duckdb_settings()"},
45:     {"pg_catalog", "pg_tables", "SELECT schema_name schemaname, table_name tablename, 'duckdb' tableowner, NULL \"tablespace\", index_count > 0 hasindexes, false hasrules, false hastriggers FROM duckdb_tables()"},
46:     {"pg_catalog", "pg_tablespace", "SELECT 0 oid, 'pg_default' spcname, 0 spcowner, NULL spcacl, NULL spcoptions"},
47:     {"pg_catalog", "pg_type", "SELECT CASE WHEN type_oid IS NULL THEN NULL WHEN logical_type = 'ENUM' AND type_name <> 'enum' THEN type_oid ELSE map_to_pg_oid(type_name) END oid, format_pg_type(logical_type, type_name) typname, schema_oid typnamespace, 0 typowner, type_size typlen, false typbyval, CASE WHEN logical_type='ENUM' THEN 'e' else 'b' end typtype, CASE WHEN type_category='NUMERIC' THEN 'N' WHEN type_category='STRING' THEN 'S' WHEN type_category='DATETIME' THEN 'D' WHEN type_category='BOOLEAN' THEN 'B' WHEN type_category='COMPOSITE' THEN 'C' WHEN type_category='USER' THEN 'U' ELSE 'X' END typcategory, false typispreferred, true typisdefined, NULL typdelim, NULL typrelid, NULL typsubscript, NULL typelem, NULL typarray, NULL typinput, NULL typoutput, NULL typreceive, NULL typsend, NULL typmodin, NULL typmodout, NULL typanalyze, 'd' typalign, 'p' typstorage, NULL typnotnull, NULL typbasetype, NULL typtypmod, NULL typndims, NULL typcollation, NULL typdefaultbin, NULL typdefault, NULL typacl FROM duckdb_types() WHERE type_oid IS NOT NULL;"},
48:     {"pg_catalog", "pg_views", "SELECT schema_name schemaname, view_name viewname, 'duckdb' viewowner, sql definition FROM duckdb_views()"},
49:     {"information_schema", "columns", "SELECT database_name table_catalog, schema_name table_schema, table_name, column_name, column_index ordinal_position, column_default, CASE WHEN is_nullable THEN 'YES' ELSE 'NO' END is_nullable, data_type, character_maximum_length, NULL::INT character_octet_length, numeric_precision, numeric_precision_radix, numeric_scale, NULL::INT datetime_precision, NULL::VARCHAR interval_type, NULL::INT interval_precision, NULL::VARCHAR character_set_catalog, NULL::VARCHAR character_set_schema, NULL::VARCHAR character_set_name, NULL::VARCHAR collation_catalog, NULL::VARCHAR collation_schema, NULL::VARCHAR collation_name, NULL::VARCHAR domain_catalog, NULL::VARCHAR domain_schema, NULL::VARCHAR domain_name, NULL::VARCHAR udt_catalog, NULL::VARCHAR udt_schema, NULL::VARCHAR udt_name, NULL::VARCHAR scope_catalog, NULL::VARCHAR scope_schema, NULL::VARCHAR scope_name, NULL::BIGINT maximum_cardinality, NULL::VARCHAR dtd_identifier, NULL::BOOL is_self_referencing, NULL::BOOL is_identity, NULL::VARCHAR identity_generation, NULL::VARCHAR identity_start, NULL::VARCHAR identity_increment, NULL::VARCHAR identity_maximum, NULL::VARCHAR identity_minimum, NULL::BOOL identity_cycle, NULL::VARCHAR is_generated, NULL::VARCHAR generation_expression, NULL::BOOL is_updatable, comment AS COLUMN_COMMENT FROM duckdb_columns;"},
50:     {"information_schema", "schemata", "SELECT database_name catalog_name, schema_name, 'duckdb' schema_owner, NULL::VARCHAR default_character_set_catalog, NULL::VARCHAR default_character_set_schema, NULL::VARCHAR default_character_set_name, sql sql_path FROM duckdb_schemas()"},
51:     {"information_schema", "tables", "SELECT database_name table_catalog, schema_name table_schema, table_name, CASE WHEN temporary THEN 'LOCAL TEMPORARY' ELSE 'BASE TABLE' END table_type, NULL::VARCHAR self_referencing_column_name, NULL::VARCHAR reference_generation, NULL::VARCHAR user_defined_type_catalog, NULL::VARCHAR user_defined_type_schema, NULL::VARCHAR user_defined_type_name, 'YES' is_insertable_into, 'NO' is_typed, CASE WHEN temporary THEN 'PRESERVE' ELSE NULL END commit_action, comment AS TABLE_COMMENT FROM duckdb_tables() UNION ALL SELECT database_name table_catalog, schema_name table_schema, view_name table_name, 'VIEW' table_type, NULL self_referencing_column_name, NULL reference_generation, NULL user_defined_type_catalog, NULL user_defined_type_schema, NULL user_defined_type_name, 'NO' is_insertable_into, 'NO' is_typed, NULL commit_action, comment AS TABLE_COMMENT FROM duckdb_views;"},
52: 	{"information_schema", "character_sets", "SELECT NULL::VARCHAR character_set_catalog, NULL::VARCHAR character_set_schema, 'UTF8' character_set_name, 'UCS' character_repertoire, 'UTF8' form_of_use, current_database() default_collate_catalog, 'pg_catalog' default_collate_schema, 'ucs_basic' default_collate_name;"},
53: 	{"information_schema", "referential_constraints", "SELECT f.database_name constraint_catalog, f.schema_name constraint_schema, concat(f.table_name, '_', f.source, '_fkey') constraint_name, current_database() unique_constraint_catalog, c.schema_name unique_constraint_schema, concat(c.table_name, '_', f.target_column, '_', CASE WHEN c.constraint_type == 'UNIQUE' THEN 'key' ELSE 'pkey' END) unique_constraint_name, 'NONE' match_option, 'NO ACTION' update_rule, 'NO ACTION' delete_rule FROM duckdb_constraints() c JOIN (SELECT *, name_extract['source'] as source, name_extract['target'] as target, name_extract['target_column'] as target_column FROM (SELECT *, regexp_extract(constraint_text, 'FOREIGN KEY \\(([a-zA-Z_0-9]+)\\) REFERENCES ([a-zA-Z_0-9]+)\\(([a-zA-Z_0-9]+)\\)', ['source', 'target', 'target_column']) name_extract FROM duckdb_constraints() WHERE constraint_type = 'FOREIGN KEY')) f ON name_extract['target'] = c.table_name AND (c.constraint_type = 'UNIQUE' OR c.constraint_type = 'PRIMARY KEY')"},
54: 	{"information_schema", "key_column_usage", "SELECT current_database() constraint_catalog, schema_name constraint_schema, concat(table_name, '_', constraint_column_names[1], CASE constraint_type WHEN 'FOREIGN KEY' THEN '_fkey' WHEN 'PRIMARY KEY' THEN '_pkey' ELSE '_key' END) constraint_name, current_database() table_catalog, schema_name table_schema, table_name, constraint_column_names[1] column_name, 1 ordinal_position, CASE constraint_type WHEN 'FOREIGN KEY' THEN 1 ELSE NULL END position_in_unique_constraint FROM duckdb_constraints() WHERE constraint_type = 'FOREIGN KEY' OR constraint_type = 'PRIMARY KEY' OR constraint_type = 'UNIQUE';"},
55: 	{"information_schema", "table_constraints", "SELECT current_database() constraint_catalog, schema_name constraint_schema, concat(table_name, '_', CASE WHEN length(constraint_column_names) > 1 THEN NULL ELSE constraint_column_names[1] || '_' END, CASE constraint_type WHEN 'FOREIGN KEY' THEN 'fkey' WHEN 'PRIMARY KEY' THEN 'pkey' WHEN 'UNIQUE' THEN 'key' WHEN 'CHECK' THEN 'check' WHEN 'NOT NULL' THEN 'not_null'  END) constraint_name, current_database() table_catalog, schema_name table_schema, table_name, CASE constraint_type WHEN 'NOT NULL' THEN 'CHECK' ELSE constraint_type END constraint_type, 'NO' is_deferrable, 'NO' initially_deferred, 'YES' enforced, 'YES' nulls_distinct FROM duckdb_constraints() WHERE constraint_type = 'PRIMARY KEY' OR constraint_type = 'FOREIGN KEY' OR constraint_type = 'UNIQUE' OR constraint_type = 'CHECK' OR constraint_type = 'NOT NULL';"},
56:     {nullptr, nullptr, nullptr}};
57: 
58: static unique_ptr<CreateViewInfo> GetDefaultView(ClientContext &context, const string &input_schema, const string &input_name) {
59: 	auto schema = StringUtil::Lower(input_schema);
60: 	auto name = StringUtil::Lower(input_name);
61: 	for (idx_t index = 0; internal_views[index].name != nullptr; index++) {
62: 		if (internal_views[index].schema == schema && internal_views[index].name == name) {
63: 			auto result = make_uniq<CreateViewInfo>();
64: 			result->schema = schema;
65: 			result->view_name = name;
66: 			result->sql = internal_views[index].sql;
67: 			result->temporary = true;
68: 			result->internal = true;
69: 
70: 			return CreateViewInfo::FromSelect(context, std::move(result));
71: 		}
72: 	}
73: 	return nullptr;
74: }
75: 
76: DefaultViewGenerator::DefaultViewGenerator(Catalog &catalog, SchemaCatalogEntry &schema)
77:     : DefaultGenerator(catalog), schema(schema) {
78: }
79: 
80: unique_ptr<CatalogEntry> DefaultViewGenerator::CreateDefaultEntry(ClientContext &context, const string &entry_name) {
81: 	auto info = GetDefaultView(context, schema.name, entry_name);
82: 	if (info) {
83: 		return make_uniq_base<CatalogEntry, ViewCatalogEntry>(catalog, schema, *info);
84: 	}
85: 	return nullptr;
86: }
87: 
88: vector<string> DefaultViewGenerator::GetDefaultEntries() {
89: 	vector<string> result;
90: 	for (idx_t index = 0; internal_views[index].name != nullptr; index++) {
91: 		if (internal_views[index].schema == schema.name) {
92: 			result.emplace_back(internal_views[index].name);
93: 		}
94: 	}
95: 	return result;
96: }
97: 
98: } // namespace duckdb
[end of src/catalog/default/default_views.cpp]
[start of src/function/table/system/duckdb_constraints.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/duck_table_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
6: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
7: #include "duckdb/common/exception.hpp"
8: #include "duckdb/main/client_context.hpp"
9: #include "duckdb/main/client_data.hpp"
10: #include "duckdb/parser/constraint.hpp"
11: #include "duckdb/parser/constraints/check_constraint.hpp"
12: #include "duckdb/parser/constraints/unique_constraint.hpp"
13: #include "duckdb/planner/constraints/bound_unique_constraint.hpp"
14: #include "duckdb/planner/constraints/bound_check_constraint.hpp"
15: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
16: #include "duckdb/planner/constraints/bound_foreign_key_constraint.hpp"
17: #include "duckdb/storage/data_table.hpp"
18: #include "duckdb/planner/binder.hpp"
19: 
20: namespace duckdb {
21: 
22: struct UniqueKeyInfo {
23: 	string schema;
24: 	string table;
25: 	vector<LogicalIndex> columns;
26: 
27: 	bool operator==(const UniqueKeyInfo &other) const {
28: 		return (schema == other.schema) && (table == other.table) && (columns == other.columns);
29: 	}
30: };
31: 
32: } // namespace duckdb
33: 
34: namespace std {
35: 
36: template <>
37: struct hash<duckdb::UniqueKeyInfo> {
38: 	template <class X>
39: 	static size_t ComputeHash(const X &x) {
40: 		return hash<X>()(x);
41: 	}
42: 
43: 	size_t operator()(const duckdb::UniqueKeyInfo &j) const {
44: 		D_ASSERT(j.columns.size() > 0);
45: 		return ComputeHash(j.schema) + ComputeHash(j.table) + ComputeHash(j.columns[0].index);
46: 	}
47: };
48: 
49: } // namespace std
50: 
51: namespace duckdb {
52: 
53: struct ConstraintEntry {
54: 	ConstraintEntry(ClientContext &context, TableCatalogEntry &table) : table(table) {
55: 		if (!table.IsDuckTable()) {
56: 			return;
57: 		}
58: 		auto binder = Binder::CreateBinder(context);
59: 		bound_constraints = binder->BindConstraints(table.GetConstraints(), table.name, table.GetColumns());
60: 	}
61: 
62: 	TableCatalogEntry &table;
63: 	vector<unique_ptr<BoundConstraint>> bound_constraints;
64: };
65: 
66: struct DuckDBConstraintsData : public GlobalTableFunctionState {
67: 	DuckDBConstraintsData() : offset(0), constraint_offset(0), unique_constraint_offset(0) {
68: 	}
69: 
70: 	vector<ConstraintEntry> entries;
71: 	idx_t offset;
72: 	idx_t constraint_offset;
73: 	idx_t unique_constraint_offset;
74: 	unordered_map<UniqueKeyInfo, idx_t> known_fk_unique_constraint_offsets;
75: };
76: 
77: static unique_ptr<FunctionData> DuckDBConstraintsBind(ClientContext &context, TableFunctionBindInput &input,
78:                                                       vector<LogicalType> &return_types, vector<string> &names) {
79: 	names.emplace_back("database_name");
80: 	return_types.emplace_back(LogicalType::VARCHAR);
81: 
82: 	names.emplace_back("database_oid");
83: 	return_types.emplace_back(LogicalType::BIGINT);
84: 
85: 	names.emplace_back("schema_name");
86: 	return_types.emplace_back(LogicalType::VARCHAR);
87: 
88: 	names.emplace_back("schema_oid");
89: 	return_types.emplace_back(LogicalType::BIGINT);
90: 
91: 	names.emplace_back("table_name");
92: 	return_types.emplace_back(LogicalType::VARCHAR);
93: 
94: 	names.emplace_back("table_oid");
95: 	return_types.emplace_back(LogicalType::BIGINT);
96: 
97: 	names.emplace_back("constraint_index");
98: 	return_types.emplace_back(LogicalType::BIGINT);
99: 
100: 	// CHECK, PRIMARY KEY or UNIQUE
101: 	names.emplace_back("constraint_type");
102: 	return_types.emplace_back(LogicalType::VARCHAR);
103: 
104: 	names.emplace_back("constraint_text");
105: 	return_types.emplace_back(LogicalType::VARCHAR);
106: 
107: 	names.emplace_back("expression");
108: 	return_types.emplace_back(LogicalType::VARCHAR);
109: 
110: 	names.emplace_back("constraint_column_indexes");
111: 	return_types.push_back(LogicalType::LIST(LogicalType::BIGINT));
112: 
113: 	names.emplace_back("constraint_column_names");
114: 	return_types.push_back(LogicalType::LIST(LogicalType::VARCHAR));
115: 
116: 	return nullptr;
117: }
118: 
119: unique_ptr<GlobalTableFunctionState> DuckDBConstraintsInit(ClientContext &context, TableFunctionInitInput &input) {
120: 	auto result = make_uniq<DuckDBConstraintsData>();
121: 
122: 	// scan all the schemas for tables and collect them
123: 	auto schemas = Catalog::GetAllSchemas(context);
124: 
125: 	for (auto &schema : schemas) {
126: 		vector<reference<CatalogEntry>> entries;
127: 
128: 		schema.get().Scan(context, CatalogType::TABLE_ENTRY, [&](CatalogEntry &entry) {
129: 			if (entry.type == CatalogType::TABLE_ENTRY) {
130: 				entries.push_back(entry);
131: 			}
132: 		});
133: 
134: 		sort(entries.begin(), entries.end(), [&](CatalogEntry &x, CatalogEntry &y) { return (x.name < y.name); });
135: 		for (auto &entry : entries) {
136: 			result->entries.emplace_back(context, entry.get().Cast<TableCatalogEntry>());
137: 		}
138: 	};
139: 
140: 	return std::move(result);
141: }
142: 
143: void DuckDBConstraintsFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {
144: 	auto &data = data_p.global_state->Cast<DuckDBConstraintsData>();
145: 	if (data.offset >= data.entries.size()) {
146: 		// finished returning values
147: 		return;
148: 	}
149: 	// start returning values
150: 	// either fill up the chunk or return all the remaining columns
151: 	idx_t count = 0;
152: 	while (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {
153: 		auto &entry = data.entries[data.offset];
154: 
155: 		auto &table = entry.table;
156: 		auto &constraints = table.GetConstraints();
157: 		bool is_duck_table = table.IsDuckTable();
158: 		for (; data.constraint_offset < constraints.size() && count < STANDARD_VECTOR_SIZE; data.constraint_offset++) {
159: 			auto &constraint = constraints[data.constraint_offset];
160: 			// return values:
161: 			// constraint_type, VARCHAR
162: 			// Processing this first due to shortcut (early continue)
163: 			string constraint_type;
164: 			switch (constraint->type) {
165: 			case ConstraintType::CHECK:
166: 				constraint_type = "CHECK";
167: 				break;
168: 			case ConstraintType::UNIQUE: {
169: 				auto &unique = constraint->Cast<UniqueConstraint>();
170: 				constraint_type = unique.IsPrimaryKey() ? "PRIMARY KEY" : "UNIQUE";
171: 				break;
172: 			}
173: 			case ConstraintType::NOT_NULL:
174: 				constraint_type = "NOT NULL";
175: 				break;
176: 			case ConstraintType::FOREIGN_KEY: {
177: 				if (!is_duck_table) {
178: 					continue;
179: 				}
180: 				auto &bound_constraints = entry.bound_constraints;
181: 				auto &bound_foreign_key = bound_constraints[data.constraint_offset]->Cast<BoundForeignKeyConstraint>();
182: 				if (bound_foreign_key.info.type == ForeignKeyType::FK_TYPE_PRIMARY_KEY_TABLE) {
183: 					// Those are already covered by PRIMARY KEY and UNIQUE entries
184: 					continue;
185: 				}
186: 				constraint_type = "FOREIGN KEY";
187: 				break;
188: 			}
189: 			default:
190: 				throw NotImplementedException("Unimplemented constraint for duckdb_constraints");
191: 			}
192: 
193: 			idx_t col = 0;
194: 			// database_name, LogicalType::VARCHAR
195: 			output.SetValue(col++, count, Value(table.schema.catalog.GetName()));
196: 			// database_oid, LogicalType::BIGINT
197: 			output.SetValue(col++, count, Value::BIGINT(NumericCast<int64_t>(table.schema.catalog.GetOid())));
198: 			// schema_name, LogicalType::VARCHAR
199: 			output.SetValue(col++, count, Value(table.schema.name));
200: 			// schema_oid, LogicalType::BIGINT
201: 			output.SetValue(col++, count, Value::BIGINT(NumericCast<int64_t>(table.schema.oid)));
202: 			// table_name, LogicalType::VARCHAR
203: 			output.SetValue(col++, count, Value(table.name));
204: 			// table_oid, LogicalType::BIGINT
205: 			output.SetValue(col++, count, Value::BIGINT(NumericCast<int64_t>(table.oid)));
206: 
207: 			// constraint_index, BIGINT
208: 			UniqueKeyInfo uk_info;
209: 
210: 			if (is_duck_table) {
211: 				auto &bound_constraint = *entry.bound_constraints[data.constraint_offset];
212: 				switch (bound_constraint.type) {
213: 				case ConstraintType::UNIQUE: {
214: 					auto &bound_unique = bound_constraint.Cast<BoundUniqueConstraint>();
215: 					uk_info = {table.schema.name, table.name, bound_unique.keys};
216: 					break;
217: 				}
218: 				case ConstraintType::FOREIGN_KEY: {
219: 					const auto &bound_foreign_key = bound_constraint.Cast<BoundForeignKeyConstraint>();
220: 					const auto &info = bound_foreign_key.info;
221: 					// find the other table
222: 					auto table_entry = Catalog::GetEntry<TableCatalogEntry>(
223: 					    context, table.catalog.GetName(), info.schema, info.table, OnEntryNotFound::RETURN_NULL);
224: 					if (!table_entry) {
225: 						throw InternalException("dukdb_constraints: entry %s.%s referenced in foreign key not found",
226: 						                        info.schema, info.table);
227: 					}
228: 					vector<LogicalIndex> index;
229: 					for (auto &key : info.pk_keys) {
230: 						index.push_back(table_entry->GetColumns().PhysicalToLogical(key));
231: 					}
232: 					uk_info = {table_entry->schema.name, table_entry->name, index};
233: 					break;
234: 				}
235: 				default:
236: 					break;
237: 				}
238: 			}
239: 
240: 			if (uk_info.columns.empty()) {
241: 				output.SetValue(col++, count, Value::BIGINT(NumericCast<int64_t>(data.unique_constraint_offset++)));
242: 			} else {
243: 				auto known_unique_constraint_offset = data.known_fk_unique_constraint_offsets.find(uk_info);
244: 				if (known_unique_constraint_offset == data.known_fk_unique_constraint_offsets.end()) {
245: 					data.known_fk_unique_constraint_offsets.insert(make_pair(uk_info, data.unique_constraint_offset));
246: 					output.SetValue(col++, count, Value::BIGINT(NumericCast<int64_t>(data.unique_constraint_offset)));
247: 					data.unique_constraint_offset++;
248: 				} else {
249: 					output.SetValue(col++, count,
250: 					                Value::BIGINT(NumericCast<int64_t>(known_unique_constraint_offset->second)));
251: 				}
252: 			}
253: 			output.SetValue(col++, count, Value(constraint_type));
254: 
255: 			// constraint_text, VARCHAR
256: 			output.SetValue(col++, count, Value(constraint->ToString()));
257: 
258: 			// expression, VARCHAR
259: 			Value expression_text;
260: 			if (constraint->type == ConstraintType::CHECK) {
261: 				auto &check = constraint->Cast<CheckConstraint>();
262: 				expression_text = Value(check.expression->ToString());
263: 			}
264: 			output.SetValue(col++, count, expression_text);
265: 
266: 			vector<LogicalIndex> column_index_list;
267: 			if (is_duck_table) {
268: 				auto &bound_constraint = *entry.bound_constraints[data.constraint_offset];
269: 				switch (bound_constraint.type) {
270: 				case ConstraintType::CHECK: {
271: 					auto &bound_check = bound_constraint.Cast<BoundCheckConstraint>();
272: 					for (auto &col_idx : bound_check.bound_columns) {
273: 						column_index_list.push_back(table.GetColumns().PhysicalToLogical(col_idx));
274: 					}
275: 					break;
276: 				}
277: 				case ConstraintType::UNIQUE: {
278: 					auto &bound_unique = bound_constraint.Cast<BoundUniqueConstraint>();
279: 					for (auto &col_idx : bound_unique.keys) {
280: 						column_index_list.push_back(col_idx);
281: 					}
282: 					break;
283: 				}
284: 				case ConstraintType::NOT_NULL: {
285: 					auto &bound_not_null = bound_constraint.Cast<BoundNotNullConstraint>();
286: 					column_index_list.push_back(table.GetColumns().PhysicalToLogical(bound_not_null.index));
287: 					break;
288: 				}
289: 				case ConstraintType::FOREIGN_KEY: {
290: 					auto &bound_foreign_key = bound_constraint.Cast<BoundForeignKeyConstraint>();
291: 					for (auto &col_idx : bound_foreign_key.info.fk_keys) {
292: 						column_index_list.push_back(table.GetColumns().PhysicalToLogical(col_idx));
293: 					}
294: 					break;
295: 				}
296: 				default:
297: 					throw NotImplementedException("Unimplemented constraint for duckdb_constraints");
298: 				}
299: 			}
300: 
301: 			vector<Value> index_list;
302: 			vector<Value> column_name_list;
303: 			for (auto column_index : column_index_list) {
304: 				index_list.push_back(Value::BIGINT(NumericCast<int64_t>(column_index.index)));
305: 				column_name_list.emplace_back(table.GetColumn(column_index).Name());
306: 			}
307: 
308: 			// constraint_column_indexes, LIST
309: 			output.SetValue(col++, count, Value::LIST(LogicalType::BIGINT, std::move(index_list)));
310: 
311: 			// constraint_column_names, LIST
312: 			output.SetValue(col++, count, Value::LIST(LogicalType::VARCHAR, std::move(column_name_list)));
313: 
314: 			count++;
315: 		}
316: 		if (data.constraint_offset >= constraints.size()) {
317: 			data.constraint_offset = 0;
318: 			data.offset++;
319: 		}
320: 	}
321: 	output.SetCardinality(count);
322: }
323: 
324: void DuckDBConstraintsFun::RegisterFunction(BuiltinFunctions &set) {
325: 	set.AddFunction(TableFunction("duckdb_constraints", {}, DuckDBConstraintsFunction, DuckDBConstraintsBind,
326: 	                              DuckDBConstraintsInit));
327: }
328: 
329: } // namespace duckdb
[end of src/function/table/system/duckdb_constraints.cpp]
[start of src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/catalog/catalog_entry/table_catalog_entry.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/catalog/standard_entry.hpp"
12: #include "duckdb/common/unordered_map.hpp"
13: #include "duckdb/parser/column_list.hpp"
14: #include "duckdb/parser/constraint.hpp"
15: #include "duckdb/planner/bound_constraint.hpp"
16: #include "duckdb/planner/expression.hpp"
17: #include "duckdb/common/case_insensitive_map.hpp"
18: #include "duckdb/catalog/catalog_entry/table_column_type.hpp"
19: #include "duckdb/catalog/catalog_entry/column_dependency_manager.hpp"
20: 
21: namespace duckdb {
22: 
23: class DataTable;
24: struct CreateTableInfo;
25: struct BoundCreateTableInfo;
26: 
27: struct RenameColumnInfo;
28: struct AddColumnInfo;
29: struct RemoveColumnInfo;
30: struct SetDefaultInfo;
31: struct ChangeColumnTypeInfo;
32: struct AlterForeignKeyInfo;
33: struct SetNotNullInfo;
34: struct DropNotNullInfo;
35: struct SetColumnCommentInfo;
36: 
37: class TableFunction;
38: struct FunctionData;
39: 
40: class Binder;
41: class TableColumnInfo;
42: struct ColumnSegmentInfo;
43: class TableStorageInfo;
44: 
45: class LogicalGet;
46: class LogicalProjection;
47: class LogicalUpdate;
48: 
49: //! A table catalog entry
50: class TableCatalogEntry : public StandardEntry {
51: public:
52: 	static constexpr const CatalogType Type = CatalogType::TABLE_ENTRY;
53: 	static constexpr const char *Name = "table";
54: 
55: public:
56: 	//! Create a TableCatalogEntry and initialize storage for it
57: 	DUCKDB_API TableCatalogEntry(Catalog &catalog, SchemaCatalogEntry &schema, CreateTableInfo &info);
58: 
59: public:
60: 	DUCKDB_API unique_ptr<CreateInfo> GetInfo() const override;
61: 
62: 	DUCKDB_API bool HasGeneratedColumns() const;
63: 
64: 	//! Returns whether or not a column with the given name exists
65: 	DUCKDB_API bool ColumnExists(const string &name);
66: 	//! Returns a reference to the column of the specified name. Throws an
67: 	//! exception if the column does not exist.
68: 	DUCKDB_API const ColumnDefinition &GetColumn(const string &name);
69: 	//! Returns a reference to the column of the specified logical index. Throws an
70: 	//! exception if the column does not exist.
71: 	DUCKDB_API const ColumnDefinition &GetColumn(LogicalIndex idx);
72: 	//! Returns a list of types of the table, excluding generated columns
73: 	DUCKDB_API vector<LogicalType> GetTypes();
74: 	//! Returns a list of the columns of the table
75: 	DUCKDB_API const ColumnList &GetColumns() const;
76: 	//! Returns the underlying storage of the table
77: 	virtual DataTable &GetStorage();
78: 
79: 	//! Returns a list of the constraints of the table
80: 	DUCKDB_API const vector<unique_ptr<Constraint>> &GetConstraints() const;
81: 	DUCKDB_API string ToSQL() const override;
82: 
83: 	//! Get statistics of a column (physical or virtual) within the table
84: 	virtual unique_ptr<BaseStatistics> GetStatistics(ClientContext &context, column_t column_id) = 0;
85: 
86: 	//! Returns the column index of the specified column name.
87: 	//! If the column does not exist:
88: 	//! If if_column_exists is true, returns DConstants::INVALID_INDEX
89: 	//! If if_column_exists is false, throws an exception
90: 	DUCKDB_API LogicalIndex GetColumnIndex(string &name, bool if_exists = false);
91: 
92: 	//! Returns the scan function that can be used to scan the given table
93: 	virtual TableFunction GetScanFunction(ClientContext &context, unique_ptr<FunctionData> &bind_data) = 0;
94: 
95: 	virtual bool IsDuckTable() const {
96: 		return false;
97: 	}
98: 
99: 	DUCKDB_API static string ColumnsToSQL(const ColumnList &columns, const vector<unique_ptr<Constraint>> &constraints);
100: 
101: 	//! Returns a list of segment information for this table, if exists
102: 	virtual vector<ColumnSegmentInfo> GetColumnSegmentInfo();
103: 
104: 	//! Returns the storage info of this table
105: 	virtual TableStorageInfo GetStorageInfo(ClientContext &context) = 0;
106: 
107: 	virtual void BindUpdateConstraints(Binder &binder, LogicalGet &get, LogicalProjection &proj, LogicalUpdate &update,
108: 	                                   ClientContext &context);
109: 
110: protected:
111: 	//! A list of columns that are part of this table
112: 	ColumnList columns;
113: 	//! A list of constraints that are part of this table
114: 	vector<unique_ptr<Constraint>> constraints;
115: };
116: } // namespace duckdb
[end of src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: