{
  "repo": "duckdb/duckdb",
  "pull_number": 4211,
  "instance_id": "duckdb__duckdb-4211",
  "issue_numbers": [
    "2186"
  ],
  "base_commit": "e95fb4c307a478d3f58a1e35dd7e6232c9ac129f",
  "patch": "diff --git a/extension/parquet/include/parquet_reader.hpp b/extension/parquet/include/parquet_reader.hpp\nindex ef1ff8388051..4bfed934b1f1 100644\n--- a/extension/parquet/include/parquet_reader.hpp\n+++ b/extension/parquet/include/parquet_reader.hpp\n@@ -10,6 +10,10 @@\n \n #include \"duckdb.hpp\"\n #ifndef DUCKDB_AMALGAMATION\n+#include \"duckdb/planner/table_filter.hpp\"\n+#include \"duckdb/planner/filter/constant_filter.hpp\"\n+#include \"duckdb/planner/filter/null_filter.hpp\"\n+#include \"duckdb/planner/filter/conjunction_filter.hpp\"\n #include \"duckdb/common/common.hpp\"\n #include \"duckdb/common/exception.hpp\"\n #include \"duckdb/common/string_util.hpp\"\ndiff --git a/extension/parquet/parquet-extension.cpp b/extension/parquet/parquet-extension.cpp\nindex 4f6933eddfa5..fb992ab42bf5 100644\n--- a/extension/parquet/parquet-extension.cpp\n+++ b/extension/parquet/parquet-extension.cpp\n@@ -4,6 +4,7 @@\n #include <vector>\n #include <fstream>\n #include <iostream>\n+#include <numeric>\n \n #include \"parquet-extension.hpp\"\n #include \"parquet_reader.hpp\"\n@@ -13,6 +14,7 @@\n \n #include \"duckdb.hpp\"\n #ifndef DUCKDB_AMALGAMATION\n+#include \"duckdb/common/hive_partitioning.hpp\"\n #include \"duckdb/common/constants.hpp\"\n #include \"duckdb/common/file_system.hpp\"\n #include \"duckdb/common/types/chunk_collection.hpp\"\n@@ -32,6 +34,8 @@\n \n #include \"duckdb/main/client_context.hpp\"\n #include \"duckdb/catalog/catalog.hpp\"\n+\n+#include \"duckdb/planner/operator/logical_get.hpp\"\n #endif\n \n namespace duckdb {\n@@ -44,6 +48,18 @@ struct ParquetReadBindData : public TableFunctionData {\n \tatomic<idx_t> cur_file;\n \tvector<string> names;\n \tvector<LogicalType> types;\n+\n+\t// These come from the initial_reader, but need to be stored in case the initial_reader is removed by a filter\n+\tidx_t initial_file_cardinality;\n+\tidx_t initial_file_row_groups;\n+\tParquetOptions parquet_options;\n+\n+\tvoid SetInitialReader(shared_ptr<ParquetReader> reader) {\n+\t\tinitial_reader = std::move(reader);\n+\t\tinitial_file_cardinality = initial_reader->NumRows();\n+\t\tinitial_file_row_groups = initial_reader->NumRowGroups();\n+\t\tparquet_options = initial_reader->parquet_options;\n+\t}\n };\n \n struct ParquetReadLocalState : public LocalTableFunctionState {\n@@ -69,6 +85,26 @@ struct ParquetReadGlobalState : public GlobalTableFunctionState {\n \t}\n };\n \n+struct ParquetWriteBindData : public TableFunctionData {\n+\tvector<LogicalType> sql_types;\n+\tstring file_name;\n+\tvector<string> column_names;\n+\tduckdb_parquet::format::CompressionCodec::type codec = duckdb_parquet::format::CompressionCodec::SNAPPY;\n+\tidx_t row_group_size = 100000;\n+};\n+\n+struct ParquetWriteGlobalState : public GlobalFunctionData {\n+\tunique_ptr<ParquetWriter> writer;\n+};\n+\n+struct ParquetWriteLocalState : public LocalFunctionData {\n+\texplicit ParquetWriteLocalState(Allocator &allocator) {\n+\t\tbuffer = make_unique<ChunkCollection>(allocator);\n+\t}\n+\n+\tunique_ptr<ChunkCollection> buffer;\n+};\n+\n class ParquetScanFunction {\n public:\n \tstatic TableFunctionSet GetFunctionSet() {\n@@ -84,6 +120,7 @@ class ParquetScanFunction {\n \t\ttable_function.get_batch_index = ParquetScanGetBatchIndex;\n \t\ttable_function.projection_pushdown = true;\n \t\ttable_function.filter_pushdown = true;\n+\t\ttable_function.pushdown_complex_filter = ParquetComplexFilterPushdown;\n \t\tset.AddFunction(table_function);\n \t\ttable_function.arguments = {LogicalType::LIST(LogicalType::VARCHAR)};\n \t\ttable_function.bind = ParquetScanBindList;\n@@ -120,7 +157,8 @@ class ParquetScanFunction {\n \t\tif (result->files.empty()) {\n \t\t\tthrow IOException(\"No files found that match the pattern \\\"%s\\\"\", info.file_path);\n \t\t}\n-\t\tresult->initial_reader = make_shared<ParquetReader>(context, result->files[0], expected_types, parquet_options);\n+\t\tresult->SetInitialReader(\n+\t\t    make_shared<ParquetReader>(context, result->files[0], expected_types, parquet_options));\n \t\tresult->names = result->initial_reader->names;\n \t\tresult->types = result->initial_reader->return_types;\n \t\treturn move(result);\n@@ -134,27 +172,32 @@ class ParquetScanFunction {\n \t\t\treturn nullptr;\n \t\t}\n \n-\t\t// we do not want to parse the Parquet metadata for the sole purpose of getting column statistics\n-\n-\t\t// We already parsed the metadata for the first file in a glob because we need some type info.\n-\t\tauto overall_stats = ParquetReader::ReadStatistics(\n-\t\t    *bind_data.initial_reader, bind_data.initial_reader->return_types[column_index], column_index,\n-\t\t    bind_data.initial_reader->metadata->metadata.get());\n-\n-\t\tif (!overall_stats) {\n-\t\t\treturn nullptr;\n-\t\t}\n+\t\t// NOTE: we do not want to parse the Parquet metadata for the sole purpose of getting column statistics\n \n-\t\t// if there is only one file in the glob (quite common case), we are done\n \t\tauto &config = DBConfig::GetConfig(context);\n \t\tif (bind_data.files.size() < 2) {\n-\t\t\treturn overall_stats;\n+\t\t\tif (bind_data.initial_reader) {\n+\t\t\t\t// most common path, scanning single parquet file\n+\t\t\t\treturn ParquetReader::ReadStatistics(*bind_data.initial_reader,\n+\t\t\t\t                                     bind_data.initial_reader->return_types[column_index], column_index,\n+\t\t\t\t                                     bind_data.initial_reader->metadata->metadata.get());\n+\t\t\t} else if (!config.options.object_cache_enable) {\n+\t\t\t\t// our initial reader was reset\n+\t\t\t\treturn nullptr;\n+\t\t\t}\n \t\t} else if (config.options.object_cache_enable) {\n+\t\t\t// multiple files, object cache enabled: merge statistics\n+\t\t\tunique_ptr<BaseStatistics> overall_stats;\n+\n \t\t\tauto &cache = ObjectCache::GetObjectCache(context);\n \t\t\t// for more than one file, we could be lucky and metadata for *every* file is in the object cache (if\n \t\t\t// enabled at all)\n \t\t\tFileSystem &fs = FileSystem::GetFileSystem(context);\n-\t\t\tfor (idx_t file_idx = 1; file_idx < bind_data.files.size(); file_idx++) {\n+\n+\t\t\t// If we don't have an initial_reader anymore, we may need to allocate a new one here.\n+\t\t\tshared_ptr<ParquetReader> reader;\n+\n+\t\t\tfor (idx_t file_idx = 0; file_idx < bind_data.files.size(); file_idx++) {\n \t\t\t\tauto &file_name = bind_data.files[file_idx];\n \t\t\t\tauto metadata = cache.Get<ParquetFileMetadataCache>(file_name);\n \t\t\t\tif (!metadata) {\n@@ -168,19 +211,36 @@ class ParquetScanFunction {\n \t\t\t\t\t// missing or invalid metadata entry in cache, no usable stats overall\n \t\t\t\t\treturn nullptr;\n \t\t\t\t}\n+\n+\t\t\t\t// If we don't have an initial reader anymore we need to create a reader\n+\t\t\t\tauto &current_reader = bind_data.initial_reader ? bind_data.initial_reader : reader;\n+\t\t\t\tif (!current_reader) {\n+\t\t\t\t\tstd::vector<column_t> ids(bind_data.names.size());\n+\t\t\t\t\tstd::iota(std::begin(ids), std::end(ids), 0); // fill with 0,1,2,3.. etc\n+\n+\t\t\t\t\tcurrent_reader =\n+\t\t\t\t\t    make_shared<ParquetReader>(context, bind_data.files[0], bind_data.names, bind_data.types, ids,\n+\t\t\t\t\t                               bind_data.parquet_options, bind_data.files[0]);\n+\t\t\t\t}\n+\n \t\t\t\t// get and merge stats for file\n-\t\t\t\tauto file_stats = ParquetReader::ReadStatistics(*bind_data.initial_reader,\n-\t\t\t\t                                                bind_data.initial_reader->return_types[column_index],\n-\t\t\t\t                                                column_index, metadata->metadata.get());\n+\t\t\t\tauto file_stats =\n+\t\t\t\t    ParquetReader::ReadStatistics(*current_reader, current_reader->return_types[column_index],\n+\t\t\t\t                                  column_index, metadata->metadata.get());\n \t\t\t\tif (!file_stats) {\n \t\t\t\t\treturn nullptr;\n \t\t\t\t}\n-\t\t\t\toverall_stats->Merge(*file_stats);\n+\t\t\t\tif (overall_stats) {\n+\t\t\t\t\toverall_stats->Merge(*file_stats);\n+\t\t\t\t} else {\n+\t\t\t\t\toverall_stats = std::move(file_stats);\n+\t\t\t\t}\n \t\t\t}\n \t\t\t// success!\n \t\t\treturn overall_stats;\n \t\t}\n-\t\t// we have more than one file and no object cache so no statistics overall\n+\n+\t\t// multiple files and no object cache, no luck!\n \t\treturn nullptr;\n \t}\n \n@@ -190,7 +250,7 @@ class ParquetScanFunction {\n \t\tauto result = make_unique<ParquetReadBindData>();\n \t\tresult->files = move(files);\n \n-\t\tresult->initial_reader = make_shared<ParquetReader>(context, result->files[0], parquet_options);\n+\t\tresult->SetInitialReader(make_shared<ParquetReader>(context, result->files[0], parquet_options));\n \t\treturn_types = result->types = result->initial_reader->return_types;\n \t\tnames = result->names = result->initial_reader->names;\n \t\treturn move(result);\n@@ -259,10 +319,13 @@ class ParquetScanFunction {\n \tstatic double ParquetProgress(ClientContext &context, const FunctionData *bind_data_p,\n \t                              const GlobalTableFunctionState *global_state) {\n \t\tauto &bind_data = (ParquetReadBindData &)*bind_data_p;\n-\t\tif (bind_data.initial_reader->NumRows() == 0) {\n+\t\tif (bind_data.files.empty()) {\n+\t\t\treturn 100.0;\n+\t\t}\n+\t\tif (bind_data.initial_file_cardinality == 0) {\n \t\t\treturn (100.0 * (bind_data.cur_file + 1)) / bind_data.files.size();\n \t\t}\n-\t\tauto percentage = (bind_data.chunk_count * STANDARD_VECTOR_SIZE * 100.0 / bind_data.initial_reader->NumRows()) /\n+\t\tauto percentage = (bind_data.chunk_count * STANDARD_VECTOR_SIZE * 100.0 / bind_data.initial_file_cardinality) /\n \t\t                  bind_data.files.size();\n \t\tpercentage += 100.0 * bind_data.cur_file / bind_data.files.size();\n \t\treturn percentage;\n@@ -287,8 +350,21 @@ class ParquetScanFunction {\n \tstatic unique_ptr<GlobalTableFunctionState> ParquetScanInitGlobal(ClientContext &context,\n \t                                                                  TableFunctionInitInput &input) {\n \t\tauto &bind_data = (ParquetReadBindData &)*input.bind_data;\n+\n \t\tauto result = make_unique<ParquetReadGlobalState>();\n-\t\tresult->current_reader = bind_data.initial_reader;\n+\n+\t\tif (bind_data.initial_reader) {\n+\t\t\tresult->current_reader = bind_data.initial_reader;\n+\t\t} else {\n+\t\t\tif (bind_data.files.empty()) {\n+\t\t\t\tresult->current_reader = nullptr;\n+\t\t\t} else {\n+\t\t\t\tresult->current_reader =\n+\t\t\t\t    make_shared<ParquetReader>(context, bind_data.files[0], bind_data.names, bind_data.types,\n+\t\t\t\t                               input.column_ids, bind_data.parquet_options, bind_data.files[0]);\n+\t\t\t}\n+\t\t}\n+\n \t\tresult->row_group_index = 0;\n \t\tresult->file_index = 0;\n \t\tresult->batch_index = 0;\n@@ -325,18 +401,22 @@ class ParquetScanFunction {\n \n \tstatic unique_ptr<NodeStatistics> ParquetCardinality(ClientContext &context, const FunctionData *bind_data) {\n \t\tauto &data = (ParquetReadBindData &)*bind_data;\n-\t\treturn make_unique<NodeStatistics>(data.initial_reader->NumRows() * data.files.size());\n+\t\treturn make_unique<NodeStatistics>(data.initial_file_cardinality * data.files.size());\n \t}\n \n \tstatic idx_t ParquetScanMaxThreads(ClientContext &context, const FunctionData *bind_data) {\n \t\tauto &data = (ParquetReadBindData &)*bind_data;\n-\t\treturn data.initial_reader->NumRowGroups() * data.files.size();\n+\t\treturn data.initial_file_row_groups * data.files.size();\n \t}\n \n \tstatic bool ParquetParallelStateNext(ClientContext &context, const ParquetReadBindData &bind_data,\n \t                                     ParquetReadLocalState &scan_data, ParquetReadGlobalState &parallel_state) {\n-\n \t\tlock_guard<mutex> parallel_lock(parallel_state.lock);\n+\n+\t\tif (parallel_state.current_reader == nullptr) {\n+\t\t\treturn false;\n+\t\t}\n+\n \t\tif (parallel_state.row_group_index < parallel_state.current_reader->NumRowGroups()) {\n \t\t\t// groups remain in the current parquet file: read the next group\n \t\t\tscan_data.reader = parallel_state.current_reader;\n@@ -352,8 +432,7 @@ class ParquetScanFunction {\n \t\t\twhile (parallel_state.file_index + 1 < bind_data.files.size()) {\n \t\t\t\t// read the next file\n \t\t\t\tstring file = bind_data.files[++parallel_state.file_index];\n-\t\t\t\t// TODO check if any of the hivepartitioning/filename columns are in a filter, in this case we may be\n-\t\t\t\t// \t\table to skip the file here.\n+\n \t\t\t\tparallel_state.current_reader =\n \t\t\t\t    make_shared<ParquetReader>(context, file, bind_data.names, bind_data.types, scan_data.column_ids,\n \t\t\t\t                               parallel_state.current_reader->parquet_options, bind_data.files[0]);\n@@ -374,26 +453,28 @@ class ParquetScanFunction {\n \t\t}\n \t\treturn false;\n \t}\n-};\n \n-struct ParquetWriteBindData : public TableFunctionData {\n-\tvector<LogicalType> sql_types;\n-\tstring file_name;\n-\tvector<string> column_names;\n-\tduckdb_parquet::format::CompressionCodec::type codec = duckdb_parquet::format::CompressionCodec::SNAPPY;\n-\tidx_t row_group_size = 100000;\n-};\n+\tstatic void ParquetComplexFilterPushdown(ClientContext &context, LogicalGet &get, FunctionData *bind_data_p,\n+\t                                         vector<unique_ptr<Expression>> &filters) {\n+\t\tauto data = (ParquetReadBindData *)bind_data_p;\n+\t\tauto initial_filename = data->files[0];\n \n-struct ParquetWriteGlobalState : public GlobalFunctionData {\n-\tunique_ptr<ParquetWriter> writer;\n-};\n+\t\tif (data->parquet_options.hive_partitioning || data->parquet_options.filename) {\n+\t\t\tunordered_map<string, column_t> column_map;\n+\t\t\tfor (idx_t i = 0; i < get.column_ids.size(); i++) {\n+\t\t\t\tcolumn_map.insert({get.names[get.column_ids[i]], i});\n+\t\t\t}\n \n-struct ParquetWriteLocalState : public LocalFunctionData {\n-\texplicit ParquetWriteLocalState(Allocator &allocator) {\n-\t\tbuffer = make_unique<ChunkCollection>(allocator);\n-\t}\n+\t\t\tHivePartitioning::ApplyFiltersToFileList(data->files, filters, column_map, get.table_index,\n+\t\t\t                                         data->parquet_options.hive_partitioning,\n+\t\t\t                                         data->parquet_options.filename);\n \n-\tunique_ptr<ChunkCollection> buffer;\n+\t\t\tif (data->files.empty() || initial_filename != data->files[0]) {\n+\t\t\t\t// Remove initial reader in case the first file gets filtered out\n+\t\t\t\tdata->initial_reader.reset();\n+\t\t\t}\n+\t\t}\n+\t}\n };\n \n unique_ptr<FunctionData> ParquetWriteBind(ClientContext &context, CopyInfo &info, vector<string> &names,\ndiff --git a/extension/parquet/parquet_reader.cpp b/extension/parquet/parquet_reader.cpp\nindex 04ebdfe45af7..3150c9b52e75 100644\n--- a/extension/parquet/parquet_reader.cpp\n+++ b/extension/parquet/parquet_reader.cpp\n@@ -363,7 +363,7 @@ unique_ptr<ColumnReader> ParquetReader::CreateReader(const duckdb_parquet::forma\n \t}\n \n \tif (parquet_options.hive_partitioning) {\n-\t\tauto res = ParseHivePartitions(file_name);\n+\t\tauto res = HivePartitioning::Parse(file_name);\n \n \t\tfor (auto &partition : res) {\n \t\t\tValue val = Value(partition.second);\n@@ -411,7 +411,7 @@ void ParquetReader::InitializeSchema(const vector<string> &expected_names, const\n \n \t// Add generated constant column for filename\n \tif (parquet_options.hive_partitioning) {\n-\t\tauto partitions = ParseHivePartitions(file_name);\n+\t\tauto partitions = HivePartitioning::Parse(file_name);\n \t\tfor (auto &part : partitions) {\n \t\t\treturn_types.emplace_back(LogicalType::VARCHAR);\n \t\t\tnames.emplace_back(part.first);\ndiff --git a/scripts/amalgamation.py b/scripts/amalgamation.py\nindex 232be0622fc9..6060464cea38 100644\n--- a/scripts/amalgamation.py\n+++ b/scripts/amalgamation.py\n@@ -67,6 +67,7 @@ def add_include_dir(dirpath):\n         \"duckdb/planner/filter/null_filter.hpp\",\n         \"duckdb/common/arrow_wrapper.hpp\",\n         \"duckdb/common/hive_partitioning.hpp\",\n+        \"duckdb/planner/operator/logical_get.hpp\",\n         \"duckdb/common/compressed_file_system.hpp\"]]\n     main_header_files += add_include_dir(os.path.join(include_dir, 'duckdb/parser/expression'))\n     main_header_files += add_include_dir(os.path.join(include_dir, 'duckdb/parser/parsed_data'))\ndiff --git a/src/common/CMakeLists.txt b/src/common/CMakeLists.txt\nindex 8a19adc81734..b2ae7ca01095 100644\n--- a/src/common/CMakeLists.txt\n+++ b/src/common/CMakeLists.txt\n@@ -29,6 +29,7 @@ add_library_unity(\n   file_buffer.cpp\n   file_system.cpp\n   gzip_file_system.cpp\n+  hive_partitioning.cpp\n   pipe_file_system.cpp\n   limits.cpp\n   local_file_system.cpp\ndiff --git a/src/common/hive_partitioning.cpp b/src/common/hive_partitioning.cpp\nnew file mode 100644\nindex 000000000000..a402f3d87000\n--- /dev/null\n+++ b/src/common/hive_partitioning.cpp\n@@ -0,0 +1,125 @@\n+#include \"duckdb/common/hive_partitioning.hpp\"\n+#include \"duckdb/optimizer/statistics_propagator.hpp\"\n+#include \"duckdb/planner/table_filter.hpp\"\n+#include \"duckdb/execution/expression_executor.hpp\"\n+#include \"duckdb/optimizer/filter_combiner.hpp\"\n+#include \"duckdb/planner/expression_iterator.hpp\"\n+#include \"re2/re2.h\"\n+\n+#include <iostream>\n+\n+namespace duckdb {\n+\n+static unordered_map<column_t, string> GetKnownColumnValues(string &filename,\n+                                                            unordered_map<string, column_t> &column_map,\n+                                                            bool filename_col, bool hive_partition_cols) {\n+\tunordered_map<column_t, string> result;\n+\n+\tif (filename_col) {\n+\t\tauto lookup_column_id = column_map.find(\"filename\");\n+\t\tif (lookup_column_id != column_map.end()) {\n+\t\t\tresult[lookup_column_id->second] = filename;\n+\t\t}\n+\t}\n+\n+\tif (hive_partition_cols) {\n+\t\tauto partitions = HivePartitioning::Parse(filename);\n+\t\tfor (auto &partition : partitions) {\n+\t\t\tauto lookup_column_id = column_map.find(partition.first);\n+\t\t\tif (lookup_column_id != column_map.end()) {\n+\t\t\t\tresult[lookup_column_id->second] = partition.second;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\treturn result;\n+}\n+\n+// Takes an expression and converts a list of known column_refs to constants\n+static void ConvertKnownColRefToConstants(unique_ptr<Expression> &expr,\n+                                          unordered_map<column_t, string> &known_column_values, idx_t table_index) {\n+\tif (expr->type == ExpressionType::BOUND_COLUMN_REF) {\n+\t\tauto &bound_colref = (BoundColumnRefExpression &)*expr;\n+\n+\t\t// This bound column ref is for another table\n+\t\tif (table_index != bound_colref.binding.table_index) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tauto lookup = known_column_values.find(bound_colref.binding.column_index);\n+\t\tif (lookup != known_column_values.end()) {\n+\t\t\texpr = make_unique<BoundConstantExpression>(Value(lookup->second));\n+\t\t}\n+\t} else {\n+\t\tExpressionIterator::EnumerateChildren(*expr, [&](unique_ptr<Expression> &child) {\n+\t\t\tConvertKnownColRefToConstants(child, known_column_values, table_index);\n+\t\t});\n+\t}\n+}\n+\n+// matches hive partitions in file name. For example:\n+// \t- s3://bucket/var1=value1/bla/bla/var2=value2\n+//  - http(s)://domain(:port)/lala/kasdl/var1=value1/?not-a-var=not-a-value\n+//  - folder/folder/folder/../var1=value1/etc/.//var2=value2\n+std::map<string, string> HivePartitioning::Parse(string &filename) {\n+\tstd::map<string, string> result;\n+\n+\tstring regex = \"[\\\\/\\\\\\\\]([^\\\\/\\\\?\\\\\\\\]+)=([^\\\\/\\\\n\\\\?\\\\\\\\]+)\";\n+\tduckdb_re2::StringPiece input(filename); // Wrap a StringPiece around it\n+\n+\tstring var;\n+\tstring value;\n+\twhile (RE2::FindAndConsume(&input, regex, &var, &value)) {\n+\t\tresult.insert(std::pair<string, string>(var, value));\n+\t}\n+\treturn result;\n+}\n+\n+// TODO: this can still be improved by removing the parts of filter expressions that are true for all remaining files.\n+//\t\t currently, only expressions that cannot be evaluated during pushdown are removed.\n+void HivePartitioning::ApplyFiltersToFileList(vector<string> &files, vector<unique_ptr<Expression>> &filters,\n+                                              unordered_map<string, column_t> &column_map, idx_t table_index,\n+                                              bool hive_enabled, bool filename_enabled) {\n+\tvector<string> pruned_files;\n+\tvector<unique_ptr<Expression>> pruned_filters;\n+\n+\tif ((!filename_enabled && !hive_enabled) || filters.empty()) {\n+\t\treturn;\n+\t}\n+\n+\tfor (idx_t i = 0; i < files.size(); i++) {\n+\t\tauto &file = files[i];\n+\t\tbool should_prune_file = false;\n+\t\tauto known_values = GetKnownColumnValues(file, column_map, filename_enabled, hive_enabled);\n+\n+\t\tFilterCombiner combiner;\n+\t\tfor (auto &filter : filters) {\n+\t\t\tunique_ptr<Expression> filter_copy = filter->Copy();\n+\t\t\tConvertKnownColRefToConstants(filter_copy, known_values, table_index);\n+\t\t\t// Evaluate the filter, if it can be evaluated here, we can not prune this filter\n+\t\t\tValue result_value;\n+\t\t\tif (!filter_copy->IsScalar() || !filter_copy->IsFoldable() ||\n+\t\t\t    !ExpressionExecutor::TryEvaluateScalar(*filter_copy, result_value)) {\n+\t\t\t\t// can not be evaluated only with the filename/hive columns added, we can not prune this filter\n+\t\t\t\tpruned_filters.emplace_back(filter->Copy());\n+\t\t\t} else if (!result_value.GetValue<bool>()) {\n+\t\t\t\t// filter evaluates to false\n+\t\t\t\tshould_prune_file = true;\n+\t\t\t}\n+\n+\t\t\t// Use filter combiner to determine that this filter makes\n+\t\t\tif (!should_prune_file && combiner.AddFilter(std::move(filter_copy)) == FilterResult::UNSATISFIABLE) {\n+\t\t\t\tshould_prune_file = true;\n+\t\t\t}\n+\t\t}\n+\n+\t\tif (!should_prune_file) {\n+\t\t\tpruned_files.push_back(file);\n+\t\t}\n+\t}\n+\n+\tfilters = std::move(pruned_filters);\n+\tfiles = std::move(pruned_files);\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/function/table/read_csv.cpp b/src/function/table/read_csv.cpp\nindex 218ace282e00..49c0ae3fb7c1 100644\n--- a/src/function/table/read_csv.cpp\n+++ b/src/function/table/read_csv.cpp\n@@ -9,6 +9,7 @@\n #include \"duckdb/parser/expression/constant_expression.hpp\"\n #include \"duckdb/parser/expression/function_expression.hpp\"\n #include \"duckdb/parser/tableref/table_function_ref.hpp\"\n+#include \"duckdb/planner/operator/logical_get.hpp\"\n \n #include <limits>\n \n@@ -90,7 +91,7 @@ static unique_ptr<FunctionData> ReadCSVBind(ClientContext &context, TableFunctio\n \t}\n \n \tif (result->options.include_parsed_hive_partitions) {\n-\t\tauto partitions = ParseHivePartitions(result->files[0]);\n+\t\tauto partitions = HivePartitioning::Parse(result->files[0]);\n \t\tresult->hive_partition_col_idx = names.size();\n \t\tfor (auto &part : partitions) {\n \t\t\treturn_types.emplace_back(LogicalType::VARCHAR);\n@@ -117,6 +118,9 @@ static unique_ptr<GlobalTableFunctionState> ReadCSVInit(ClientContext &context,\n \tauto result = make_unique<ReadCSVOperatorData>();\n \tif (bind_data.initial_reader) {\n \t\tresult->csv_reader = move(bind_data.initial_reader);\n+\t} else if (bind_data.files.empty()) {\n+\t\t// This can happen when a filename based filter pushdown has eliminated all possible files for this scan.\n+\t\treturn move(result);\n \t} else {\n \t\tbind_data.options.file_path = bind_data.files[0];\n \t\tresult->csv_reader = make_unique<BufferedCSVReader>(context, bind_data.options, bind_data.sql_types);\n@@ -135,6 +139,12 @@ static unique_ptr<FunctionData> ReadCSVAutoBind(ClientContext &context, TableFun\n static void ReadCSVFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n \tauto &bind_data = (ReadCSVData &)*data_p.bind_data;\n \tauto &data = (ReadCSVOperatorData &)*data_p.global_state;\n+\n+\tif (!data.csv_reader) {\n+\t\t// no csv_reader was set, this can happen when a filename-based filter has filtered out all possible files\n+\t\treturn;\n+\t}\n+\n \tdo {\n \t\tdata.csv_reader->ParseCSV(output);\n \t\tdata.bytes_read = data.csv_reader->bytes_in_chunk;\n@@ -155,7 +165,7 @@ static void ReadCSVFunction(ClientContext &context, TableFunctionInput &data_p,\n \t\tcol.SetVectorType(VectorType::CONSTANT_VECTOR);\n \t}\n \tif (bind_data.options.include_parsed_hive_partitions) {\n-\t\tauto partitions = ParseHivePartitions(data.csv_reader->options.file_path);\n+\t\tauto partitions = HivePartitioning::Parse(data.csv_reader->options.file_path);\n \n \t\tidx_t i = bind_data.hive_partition_col_idx;\n \n@@ -212,9 +222,32 @@ double CSVReaderProgress(ClientContext &context, const FunctionData *bind_data_p\n \treturn percentage;\n }\n \n+void CSVComplexFilterPushdown(ClientContext &context, LogicalGet &get, FunctionData *bind_data_p,\n+                              vector<unique_ptr<Expression>> &filters) {\n+\tauto data = (ReadCSVData *)bind_data_p;\n+\n+\tif (data->options.include_parsed_hive_partitions || data->options.include_file_name) {\n+\t\tstring first_file = data->files[0];\n+\n+\t\tunordered_map<string, column_t> column_map;\n+\t\tfor (idx_t i = 0; i < get.column_ids.size(); i++) {\n+\t\t\tcolumn_map.insert({get.names[get.column_ids[i]], i});\n+\t\t}\n+\n+\t\tHivePartitioning::ApplyFiltersToFileList(data->files, filters, column_map, get.table_index,\n+\t\t                                         data->options.include_parsed_hive_partitions,\n+\t\t                                         data->options.include_file_name);\n+\n+\t\tif (data->files.empty() || data->files[0] != first_file) {\n+\t\t\tdata->initial_reader.reset();\n+\t\t}\n+\t}\n+}\n+\n TableFunction ReadCSVTableFunction::GetFunction() {\n \tTableFunction read_csv(\"read_csv\", {LogicalType::VARCHAR}, ReadCSVFunction, ReadCSVBind, ReadCSVInit);\n \tread_csv.table_scan_progress = CSVReaderProgress;\n+\tread_csv.pushdown_complex_filter = CSVComplexFilterPushdown;\n \tReadCSVAddNamedParameters(read_csv);\n \treturn read_csv;\n }\n@@ -224,6 +257,7 @@ void ReadCSVTableFunction::RegisterFunction(BuiltinFunctions &set) {\n \n \tTableFunction read_csv_auto(\"read_csv_auto\", {LogicalType::VARCHAR}, ReadCSVFunction, ReadCSVAutoBind, ReadCSVInit);\n \tread_csv_auto.table_scan_progress = CSVReaderProgress;\n+\tread_csv_auto.pushdown_complex_filter = CSVComplexFilterPushdown;\n \tReadCSVAddNamedParameters(read_csv_auto);\n \tset.AddFunction(read_csv_auto);\n }\ndiff --git a/src/include/duckdb/common/hive_partitioning.hpp b/src/include/duckdb/common/hive_partitioning.hpp\nindex 94db7ca190de..cc9c3015fd21 100644\n--- a/src/include/duckdb/common/hive_partitioning.hpp\n+++ b/src/include/duckdb/common/hive_partitioning.hpp\n@@ -8,25 +8,25 @@\n \n #pragma once\n \n+#include \"duckdb/optimizer/statistics_propagator.hpp\"\n+#include \"duckdb/planner/table_filter.hpp\"\n+#include \"duckdb/execution/expression_executor.hpp\"\n+#include \"duckdb/optimizer/filter_combiner.hpp\"\n+#include \"duckdb/planner/expression_iterator.hpp\"\n #include \"re2/re2.h\"\n \n namespace duckdb {\n-// matches hive partitions in file name. For example:\n-// \t- s3://bucket/var1=value1/bla/bla/var2=value2\n-//  - http(s)://domain(:port)/lala/kasdl/var1=value1/?not-a-var=not-a-value\n-//  - folder/folder/folder/../var1=value1/etc/.//var2=value2\n-inline std::map<string, string> ParseHivePartitions(string filename) {\n-\tstd::map<string, string> result;\n \n-\tstring regex = \"[\\\\/\\\\\\\\]([^\\\\/\\\\?\\\\\\\\]+)=([^\\\\/\\\\n\\\\?\\\\\\\\]+)\";\n-\tduckdb_re2::StringPiece input(filename); // Wrap a StringPiece around it\n-\n-\tstring var;\n-\tstring value;\n-\twhile (RE2::FindAndConsume(&input, regex, &var, &value)) {\n-\t\tresult.insert(std::pair<string, string>(var, value));\n-\t}\n-\treturn result;\n-}\n+class HivePartitioning {\n+public:\n+\t//! Parse a filename that follows the hive partitioning scheme\n+\tstatic std::map<string, string> Parse(string &filename);\n+\t//! Prunes a list of filenames based on a set of filters, can be used by TableFunctions in the\n+\t//! pushdown_complex_filter function to skip files with filename-based filters. Also removes the filters that always\n+\t//! evaluate to true.\n+\tstatic void ApplyFiltersToFileList(vector<string> &files, vector<unique_ptr<Expression>> &filters,\n+\t                                   unordered_map<string, column_t> &column_map, idx_t table_index,\n+\t                                   bool hive_enabled, bool filename_enabled);\n+};\n \n } // namespace duckdb\n",
  "test_patch": "diff --git a/data/csv/hive-partitioning/types/part=1000/date=2012-01-01/test.csv b/data/csv/hive-partitioning/types/part=1000/date=2012-01-01/test.csv\nnew file mode 100644\nindex 000000000000..6b0c8805998d\n--- /dev/null\n+++ b/data/csv/hive-partitioning/types/part=1000/date=2012-01-01/test.csv\n@@ -0,0 +1,2 @@\n+id,value\n+1,value1\n\\ No newline at end of file\ndiff --git a/data/csv/hive-partitioning/types/part=9000/date=2013-01-01/test.csv b/data/csv/hive-partitioning/types/part=9000/date=2013-01-01/test.csv\nnew file mode 100644\nindex 000000000000..3ac77ccf8ef1\n--- /dev/null\n+++ b/data/csv/hive-partitioning/types/part=9000/date=2013-01-01/test.csv\n@@ -0,0 +1,2 @@\n+id,value\n+2,value2\n\\ No newline at end of file\ndiff --git a/data/parquet-testing/hive-partitioning/simple/part=a/date=2012-01-01/test.csv b/data/parquet-testing/hive-partitioning/simple/part=a/date=2012-01-01/test.csv\nnew file mode 100644\nindex 000000000000..2c91b1d0a5da\n--- /dev/null\n+++ b/data/parquet-testing/hive-partitioning/simple/part=a/date=2012-01-01/test.csv\n@@ -0,0 +1,1 @@\n+1,value1\ndiff --git a/data/parquet-testing/hive-partitioning/simple/part=b/date=2013-01-01/test.csv b/data/parquet-testing/hive-partitioning/simple/part=b/date=2013-01-01/test.csv\nnew file mode 100644\nindex 000000000000..7935626de62f\n--- /dev/null\n+++ b/data/parquet-testing/hive-partitioning/simple/part=b/date=2013-01-01/test.csv\n@@ -0,0 +1,1 @@\n+2,value2\ndiff --git a/test/sql/copy/csv/csv_hive.test b/test/sql/copy/csv/csv_hive.test\nindex 7d43adbd0dc2..1309549ed371 100644\n--- a/test/sql/copy/csv/csv_hive.test\n+++ b/test/sql/copy/csv/csv_hive.test\n@@ -33,4 +33,92 @@ select * from read_csv_auto('data/csv/hive-partitioning/mismatching_names/*/*/te\n \n # If the key names don't add up, we throw\n statement error\n-select * from read_csv_auto('data/csv/hive-partitioning/mismatching_count/*/*/test.csv', HIVE_PARTITIONING=1, HEADER=1)\n\\ No newline at end of file\n+select * from read_csv_auto('data/csv/hive-partitioning/mismatching_count/*/*/test.csv', HIVE_PARTITIONING=1, HEADER=1)\n+\n+# Now we do a bunch of filtering on the partitions, to test the file skipping mechanism\n+query IIII\n+select id, value, part, date from read_csv_auto('data/csv/hive-partitioning/different_order/*/*/test.csv', HIVE_PARTITIONING=1, HEADER=1) where part='a'\n+----\n+1\tvalue1\ta\t2012-01-01\n+\n+query IIII\n+select id, value, part, date from read_csv_auto('data/csv/hive-partitioning/different_order/*/*/test.csv', HIVE_PARTITIONING=1, HEADER=1) where part='b'\n+----\n+2\tvalue2\tb\t2013-01-01\n+\n+query IIII\n+select id, value, CAST(part AS INT) as part_cast, CAST(date AS DATE) as date_cast from read_csv_auto('data/csv/hive-partitioning/types/*/*/test.csv', HIVE_PARTITIONING=1, HEADER=1) where part_cast > 0 and part_cast < 5000;\n+----\n+1\tvalue1\t1000\t2012-01-01\n+\n+query IIII\n+select id, value, CAST(part AS INT) as part_cast, CAST(date AS DATE) as date_cast from read_csv_auto('data/csv/hive-partitioning/types/*/*/test.csv', HIVE_PARTITIONING=1, HEADER=1) where part_cast > 5000;\n+----\n+2\tvalue2\t9000\t2013-01-01\n+\n+query IIII\n+select id, value, CAST(part AS INT) as part_cast, CAST(date AS DATE) as date_cast from read_csv_auto('data/csv/hive-partitioning/types/*/*/test.csv', HIVE_PARTITIONING=1, HEADER=1) where date_cast > CAST('2000-01-01' as DATE) and date_cast < CAST('2012-12-12' as DATE);\n+----\n+1\tvalue1\t1000\t2012-01-01\n+\n+query IIII\n+select id, value, CAST(part AS INT) as part_cast, CAST(date AS DATE) as date_cast from read_csv_auto('data/csv/hive-partitioning/types/*/*/test.csv', HIVE_PARTITIONING=1, HEADER=1) where date_cast > CAST('2000-01-01' as DATE) order by date_cast;\n+----\n+1\tvalue1\t1000\t2012-01-01\n+2\tvalue2\t9000\t2013-01-01\n+\n+query IIII\n+select id, value, CAST(part AS INT) as part_cast, CAST(date AS DATE) as date_cast from read_csv_auto('data/csv/hive-partitioning/types/*/*/test.csv', HIVE_PARTITIONING=1, HEADER=1) where date_cast=CAST('2012-01-01' as DATE) OR part_cast=9000 ORDER BY date_cast;\n+----\n+1\tvalue1\t1000\t2012-01-01\n+2\tvalue2\t9000\t2013-01-01\n+\n+## Filter expressions we can calculate during pushdown using filenames/hive partitions should be pruned\n+\n+# Filtering out 0/2 files\n+query IIII\n+select id, value, CAST(part AS INT) as part_cast, CAST(date AS DATE) as date_cast from read_csv_auto('data/csv/hive-partitioning/types/*/*/test.csv', HIVE_PARTITIONING=1, HEADER=1) where (date_cast=CAST('2012-01-01' as DATE) AND concat(date_cast::VARCHAR, part_cast::VARCHAR) == '2012-01-011000') OR (part_cast=9000) ORDER BY date_cast;\n+----\n+1\tvalue1\t1000\t2012-01-01\n+2\tvalue2\t9000\t2013-01-01\n+\n+# There should not be any filter operation remaining since it can be handled completely during pushdown by pruning file list\n+query II\n+EXPLAIN select id, value, CAST(part AS INT) as part_cast, CAST(date AS DATE) as date_cast from read_csv_auto('data/csv/hive-partitioning/types/*/*/test.csv', HIVE_PARTITIONING=1, HEADER=1) where (date_cast=CAST('2012-01-01' as DATE) AND concat(date_cast::VARCHAR, part_cast::VARCHAR) == '2012-01-011000') OR (part_cast=9000) ORDER BY date_cast;\n+----\n+physical_plan\t<!REGEX>:.*FILTER.*\n+\n+# Query filtering out first file\n+query IIII\n+select id, value, CAST(part AS INT) as part_cast, CAST(date AS DATE) as date_cast from read_csv_auto('data/csv/hive-partitioning/types/*/*/test.csv', HIVE_PARTITIONING=1, HEADER=1) where (date_cast=CAST('2012-01-01' as DATE) AND concat(date_cast::VARCHAR, part_cast::VARCHAR) == 'foobar') OR (part_cast=9000) ORDER BY date_cast;\n+----\n+2\tvalue2\t9000\t2013-01-01\n+\n+# Again, we should not have a filter operator here\n+query II\n+explain select id, value, CAST(part AS INT) as part_cast, CAST(date AS DATE) as date_cast from read_csv_auto('data/csv/hive-partitioning/types/*/*/test.csv', HIVE_PARTITIONING=1, HEADER=1) where (date_cast=CAST('2012-01-01' as DATE) AND concat(date_cast::VARCHAR, part_cast::VARCHAR) == 'foobar') OR (part_cast=9000) ORDER BY date_cast;\n+----\n+physical_plan\t<!REGEX>:.*FILTER.*\n+\n+# Query filtering out second file\n+query IIII\n+select id, value, CAST(part AS INT) as part_cast, CAST(date AS DATE) as date_cast from read_csv_auto('data/csv/hive-partitioning/types/*/*/test.csv', HIVE_PARTITIONING=1, HEADER=1) where (date_cast=CAST('2012-01-01' as DATE) AND concat(date_cast::VARCHAR, part_cast::VARCHAR) == '2012-01-011000') OR (part_cast=1337) ORDER BY date_cast;\n+----\n+1\tvalue1\t1000\t2012-01-01\n+\n+# Again, we should not have a filter operator here\n+query II\n+explain select id, value, CAST(part AS INT) as part_cast, CAST(date AS DATE) as date_cast from read_csv_auto('data/csv/hive-partitioning/types/*/*/test.csv', HIVE_PARTITIONING=1, HEADER=1) where (date_cast=CAST('2012-01-01' as DATE) AND concat(date_cast::VARCHAR, part_cast::VARCHAR) == '2012-01-011000') OR (part_cast=1337) ORDER BY date_cast;\n+----\n+physical_plan\t<!REGEX>:.*FILTER.*\n+\n+# Filtering out both files\n+query IIII\n+select id, value, CAST(part AS INT) as part_cast, CAST(date AS DATE) as date_cast from read_csv_auto('data/csv/hive-partitioning/types/*/*/test.csv', HIVE_PARTITIONING=1, HEADER=1) where (date_cast=CAST('2012-01-01' as DATE) AND concat(date_cast::VARCHAR, part_cast::VARCHAR) == 'foobar') OR (part_cast=1337) ORDER BY date_cast;\n+----\n+\n+# Again, we should not have a filter operator here\n+query II\n+EXPLAIN select id, value, CAST(part AS INT) as part_cast, CAST(date AS DATE) as date_cast from read_csv_auto('data/csv/hive-partitioning/types/*/*/test.csv', HIVE_PARTITIONING=1, HEADER=1) where (date_cast=CAST('2012-01-01' as DATE) AND concat(date_cast::VARCHAR, part_cast::VARCHAR) == 'foobar') OR (part_cast=1337) ORDER BY date_cast;\n+----\n+physical_plan\t<!REGEX>:.*FILTER.*\n\\ No newline at end of file\ndiff --git a/test/sql/copy/parquet/parquet_filename_filter.test b/test/sql/copy/parquet/parquet_filename_filter.test\nnew file mode 100644\nindex 000000000000..25df7de87946\n--- /dev/null\n+++ b/test/sql/copy/parquet/parquet_filename_filter.test\n@@ -0,0 +1,94 @@\n+# name: test/sql/copy/parquet/parquet_filename_filter.test\n+# description: Test the filename filter pushdown\n+# group: [parquet]\n+\n+require parquet\n+\n+# requires notwindows for windows-style path backslash reasons\n+require notwindows\n+\n+query III\n+select i, j, filename from parquet_scan('data/parquet-testing/glob*/t?.parquet', FILENAME=1) order by i;\n+----\n+1\ta\tdata/parquet-testing/glob/t1.parquet\n+2\tb\tdata/parquet-testing/glob/t2.parquet\n+3\tc\tdata/parquet-testing/glob2/t1.parquet\n+\n+query III\n+select i, j, filename as file from parquet_scan('data/parquet-testing/glob*/t?.parquet', FILENAME=1) where file='data/parquet-testing/glob2/t1.parquet' or file='data/parquet-testing/glob/t2.parquet' order by i;\n+----\n+2\tb\tdata/parquet-testing/glob/t2.parquet\n+3\tc\tdata/parquet-testing/glob2/t1.parquet\n+\n+query III\n+select i, j, filename as file from parquet_scan('data/parquet-testing/glob*/t?.parquet', FILENAME=1) where file='data/parquet-testing/glob2/t1.parquet' and i=3 order by i;\n+----\n+3\tc\tdata/parquet-testing/glob2/t1.parquet\n+\n+query III\n+select i, j, filename as file from parquet_scan('data/parquet-testing/glob*/t?.parquet', FILENAME=1) where file='data/parquet-testing/glob2/t1.parquet' and i=2 order by i;\n+----\n+\n+# This query should trigger the file skipping mechanism, which prevents reading metadata for files that are not scanned\n+query IIII\n+select id, value, date, filename from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1, FILENAME=1) order by id;\n+----\n+1\tvalue1\t2012-01-01\tdata/parquet-testing/hive-partitioning/different_order/date=2012-01-01/part=a/test.parquet\n+2\tvalue2\t2013-01-01\tdata/parquet-testing/hive-partitioning/different_order/part=b/date=2013-01-01/test.parquet\n+\n+# These queries test that the file skipping mechanism works even for complex filters on multiple filename-based filters\n+query IIII\n+select id, value, date, filename from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1, FILENAME=1) where concat(date,filename)='2013-01-01data/parquet-testing/hive-partitioning/different_order/part=b/date=2013-01-01/test.parquet';\n+----\n+2\tvalue2\t2013-01-01\tdata/parquet-testing/hive-partitioning/different_order/part=b/date=2013-01-01/test.parquet\n+\n+query IIII\n+select id, value, date, filename from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1, FILENAME=1) where concat(date,filename)='2012-01-01data/parquet-testing/hive-partitioning/different_order/date=2012-01-01/part=a/test.parquet';\n+----\n+1\tvalue1\t2012-01-01\tdata/parquet-testing/hive-partitioning/different_order/date=2012-01-01/part=a/test.parquet\n+\n+# Ensure we don't somehow endup mixing things up\n+query III\n+select id, value as filename, date from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1) where filename='value2';\n+----\n+2\tvalue2\t2013-01-01\n+\n+query III\n+select id, value as filename, date from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1) where filename='value1';\n+----\n+1\tvalue1\t2012-01-01\n+\n+# These tests confirm that the ParquetScanStats will properly handle the pruned files list\n+\n+statement ok\n+pragma enable_object_cache\n+\n+query II\n+select id, value from parquet_scan('data/parquet-testing/hive-partitioning/*/*/*/test.parquet', FILENAME=1) where filename like '%mismatching_count%' and id > 1;\n+----\n+2\tvalue2\n+\n+query II\n+select id, value from parquet_scan('data/parquet-testing/hive-partitioning/*/*/*/test.parquet', FILENAME=1) where filename like '%mismatching_count%' and id > 1;\n+----\n+2\tvalue2\n+\n+query II\n+select id, value from parquet_scan('data/parquet-testing/hive-partitioning/*/*/*/test.parquet', FILENAME=1) where filename like '%mismatching_count%' and value = 'value1';\n+----\n+1\tvalue1\n+\n+query II\n+select id, value from parquet_scan('data/parquet-testing/hive-partitioning/*/*/*/test.parquet', FILENAME=1) where filename like '%mismatching_count%' and value = 'value2';\n+----\n+2\tvalue2\n+\n+query II\n+select id, value from parquet_scan('data/parquet-testing/hive-partitioning/*/*/*/test.parquet', FILENAME=1) where filename like '%simple%' and value = 'value1';\n+----\n+1\tvalue1\n+\n+query II\n+select id, value from parquet_scan('data/parquet-testing/hive-partitioning/*/*/*/test.parquet', FILENAME=1) where filename like '%simple%' and value = 'value2';\n+----\n+2\tvalue2\n\\ No newline at end of file\ndiff --git a/test/sql/copy/parquet/parquet_hive.test b/test/sql/copy/parquet/parquet_hive.test\nindex d4c61e7a55d3..b147e1f6ed5a 100644\n--- a/test/sql/copy/parquet/parquet_hive.test\n+++ b/test/sql/copy/parquet/parquet_hive.test\n@@ -6,7 +6,7 @@ require parquet\n \n # test parsing hive partitioning scheme\n query IIII\n-select id, value, part, date from parquet_scan('data/parquet-testing/hive-partitioning/simple/*/*/test.parquet', HIVE_PARTITIONING=1)  order by id\n+select id, value, part, date from parquet_scan('data/parquet-testing/hive-partitioning/simple/*/*/test.parquet', HIVE_PARTITIONING=1) order by id\n ----\n 1\tvalue1\ta\t2012-01-01\n 2\tvalue2\tb\t2013-01-01\n@@ -24,9 +24,119 @@ select id, date from parquet_scan('data/parquet-testing/hive-partitioning/differ\n ----\n 2\t2013-01-01\n \n+query II\n+select id, date from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1) where date = '2012-01-01';\n+----\n+1\t2012-01-01\n+\n+query II\n+select id, date from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1) where date = '2018-01-01';\n+----\n+\n+query IIII\n+select id, value, part, date from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1) where part='a' OR part='b' order by id;\n+----\n+1\tvalue1\ta\t2012-01-01\n+2\tvalue2\tb\t2013-01-01\n+\n+query II\n+select id, date from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1) where date = '2013-01-01' and id = 2;\n+----\n+2\t2013-01-01\n+\n+query II\n+select id, date from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1) where date = '2013-01-01' and id = 1;\n+----\n+\n+# This query should trigger the file skipping mechanism, which prevents reading metadata for files that are not scanned\n+query III\n+select id, value, date from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1) where date = '2012-01-01' and id = 1;\n+----\n+1\tvalue1\t2012-01-01\n+\n+query III\n+select id, value, date from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1) where date = '2012-01-01' or id <= 2 order by id;\n+----\n+1\tvalue1\t2012-01-01\n+2\tvalue2\t2013-01-01\n+\n # If the key names don't add up, there's nothing we can do\n statement error\n select * from parquet_scan('data/parquet-testing/hive-partitioning/mismatching_names/*/*/test.parquet', HIVE_PARTITIONING=1)\n \n statement error\n-select * from parquet_scan('data/parquet-testing/hive-partitioning/mismatching_count/*/*/test.parquet', HIVE_PARTITIONING=1) WHERE part=b\n\\ No newline at end of file\n+select * from parquet_scan('data/parquet-testing/hive-partitioning/mismatching_count/*/*/test.parquet', HIVE_PARTITIONING=1) WHERE part=b\n+\n+# Verify that no filters remain, both pushed-down or as operators\n+query II\n+EXPLAIN select id, date from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1) where date = '2012-01-01';\n+----\n+physical_plan\t<!REGEX>:.*(PARQUET_SCAN.*Filters:|FILTER).*\n+\n+query II\n+EXPLAIN select id, date from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1) where date = '2013-01-01';\n+----\n+physical_plan\t<!REGEX>:.*(PARQUET_SCAN.*Filters:|FILTER).*\n+\n+query II\n+EXPLAIN select id, date from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1) where date = '2018-01-01';\n+----\n+physical_plan\t<!REGEX>:.*(PARQUET_SCAN.*Filters:|FILTER).*\n+\n+query II\n+EXPLAIN select id, value, part, date from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1) where part='a' OR part='b' order by id;\n+----\n+physical_plan\t<!REGEX>:.*(PARQUET_SCAN.*Filters:|FILTER).*\n+\n+query II\n+EXPLAIN select id, date from parquet_scan('data/parquet-testing/hive-partitioning/simple/*/*/test.parquet', HIVE_PARTITIONING=1) where date = '2012-01-01' and id < 10;\n+----\n+physical_plan\t<!REGEX>:.*(PARQUET_SCAN.*Filters:.*2012\\-01\\-01|FILTER).*\n+\n+query II\n+EXPLAIN select id, date from parquet_scan('data/parquet-testing/hive-partitioning/simple/*/*/test.parquet', HIVE_PARTITIONING=1) where date = '2013-01-01' and id < 10;\n+----\n+physical_plan\t<!REGEX>:.*(PARQUET_SCAN.*Filters:.*2013\\-01\\-01|FILTER).*\n+\n+# There should still not be a date filter though\n+query II\n+EXPLAIN select id, date from parquet_scan('data/parquet-testing/hive-partitioning/simple/*/*/test.parquet', HIVE_PARTITIONING=1) where date = '2013-01-01' and id < 10;\n+----\n+physical_plan\t<!REGEX>:.*(2013\\-01\\-01).*\n+\n+# Complex filter filtering first file\n+query IIII\n+select id, value, part, CAST(date AS DATE) as date_cast from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1) where concat(date_cast::VARCHAR, part) == '2013-01-01b';\n+----\n+2\tvalue2\tb\t2013-01-01\n+\n+# Complex filter filtering first file, filter should be pruned completely\n+query II\n+explain select id, value, part, CAST(date AS DATE) as date_cast from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1) where concat(date_cast::VARCHAR, part) == '2013-01-01b';\n+----\n+physical_plan\t<!REGEX>:.*(PARQUET_SCAN.*Filters:|FILTER).*\n+\n+# Complex filter filtering second file\n+query IIII\n+select id, value, part, CAST(date AS DATE) as date_cast from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1) where concat(date_cast::VARCHAR, part) == '2012-01-01a';\n+----\n+1\tvalue1\ta\t2012-01-01\n+\n+# Complex filter filtering second file, filter should be pruned completely\n+query II\n+explain select id, value, part, CAST(date AS DATE) as date_cast from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1) where concat(date_cast::VARCHAR, part) == '2012-01-01a';\n+----\n+physical_plan\t<!REGEX>:.*(PARQUET_SCAN.*Filters:|FILTER).*\n+\n+# Currently, complex fiters combining hive columns and regular columns, can prevent filter pushdown for some situations\n+# TODO: we want to support filter pushdown here too\n+query II\n+explain select id, value, part, CAST(date AS DATE) as date_cast from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1) where (date_cast=CAST('2013-01-01' as DATE) AND (value='value1' OR concat(date_cast::VARCHAR, part) == '2013-01-01b'));\n+----\n+physical_plan\t<REGEX>:.*(FILTER).*\n+\n+# Idem\n+query II\n+explain select id, value, part, CAST(date AS DATE) as date_cast from parquet_scan('data/parquet-testing/hive-partitioning/different_order/*/*/test.parquet', HIVE_PARTITIONING=1) where (date_cast=CAST('2012-01-01' as DATE) AND (value='value2' OR concat(date_cast::VARCHAR, part) == '2012-01-01a'));\n+----\n+physical_plan\t<REGEX>:.*(FILTER).*\n\\ No newline at end of file\n",
  "problem_statement": "Support hive-style partitioning of parquet archives\nThis has been mentioned tangentially in other issues (#1420, #2046, #773) but not identified directly as a feature request so just thought it might help to establish a specific thread for this.  In particular, while the globbing support (#773) makes it possible to read in these files (with a bit of extra munging to handle the partition columns), proper treatment of hive partitioning should be able to dramatically reduce the time for many queries to run, as it already does in other implementations such as `arrow` (e.g. https://ursalabs.org/blog/2021-r-benchmarks-part-1/).  Thanks for considering!\n",
  "hints_text": "Yes, this is indeed something we will need to add at some point. For now this can be read through the Arrow dataset API, which supports it. The main issue is that the people that came up with this format decided that the partitioning columns should *only* exist encoded in the path and *not* in the Parquet files themselves, which would have simplified this a lot. But well.\nThis should already work with the example from https://arrow.apache.org/docs/r/articles/dataset.html:\r\n\r\n````R\r\nlibrary(\"DBI\")\r\n \r\ncon <- dbConnect(duckdb::duckdb())\r\n\r\nds <- arrow::open_dataset(\"nyc-taxi\", partitioning = c(\"year\", \"month\"))\r\nduckdb::duckdb_register_arrow(con, \"taxi\", ds)\r\n\r\ndbGetQuery(con, \"select * from taxi limit 10\") # or whatever\r\n\r\n\r\n````\nWow, had no idea about `duckdb::duckdb_register_arrow`.  By the way, with this way, do we still have support for the full SQL range of duckdb (windowed operations, mutates, joins, regex etc)?  My understanding at least from the R side was that the `arrow` API was much more limited....  Going to have to play around with this more.  \r\n\r\n(Currently `arrow` 5.0.0 is crashing with OOM errors on me but that's another issue).\r\n\r\nThanks for the quick reply (and even providing via the R API for me :) ).  I'm with you :100: on why not just include the partitioning columns (since it's a compressed format and partition columns are usually pretty compressible omitting them seems like a pretty minimal saving).  Oh well.  \r\n\r\nWould still  be great to have native support though.  \nYes, still the full range of querying is possible. And yes, we will eventually add native support.\nA side comment. If you consider implementing partitioning support, please also consider the case of hive bucketing. In essence only open parquet files that match with bucketed column filters (e.g. bucketed by timestamp column in day=/ partition/folder).\nAnother side comment along @dforsber's lines... \r\n\r\nPossibly something to consider is the interaction not only with Pandas/Parquet but also with netCDF4/HDF5? And especially (at least in the Python ecosystem) libraries that enable processing of extremely large datasets (e.g. `dask` and `xarray` - `zarr` too).\r\n\r\nThe ability of `dask` to chunk large datasets is really important - Often, I find myself working on datasets that are highly compressable (so, on disk there isn't a problem) but are way too large to be loaded in memory all at once. \r\n\r\nJust a thought :)\n@danieldjewell also interested in ncdf/hdf5, but I think array data models are distinct from SQL-based relational databases like duckdb; like you say, `xarray` and `zarr` already provide good options there, no?  \nIt's awesome ! Thanks for your work. DuckDB + parquet is **the law**. If you could add this through the dbplyr interface with **tbl()** (like for parquet) it would be the best thing ever.\n> It's awesome ! Thanks for your work. DuckDB + parquet is **the law**. If you could add this through the dbplyr interface with **tbl()** (like for parquet) it would be the best thing ever. \r\n\r\nWe literally just merged a PR from @rsund for this\nAwesome ! Now that's the fastest feature request implementation ever !\nHi all\r\n\r\nI've found an strange behaviour with duckdb and pyarrow.\r\n\r\nI want to use duckdb through an arrow dataset (with data in parquet with hive partitioning and in an S3 like minio).\r\n\r\nThis would be the code:\r\n\r\n#In arrow\r\n...\r\nminio_conn = fs.S3FileSystem(access_key='accessKey1', secret_key='verySecretKey1', endpoint_override=\"https://s3.scality.test:8000\", scheme='https')\r\ndataset = ds.dataset(\"dummydata\", filesystem=minio_conn, partitioning=\"hive\")\r\n\r\n#In duckdb\r\nduckdb_dataset = duckdb.arrow(dataset)\r\n\r\n#Check pyarrow memory pool\r\nfor i in range(0,10):\r\n print(convert_size(pa.total_allocated_bytes()))\r\n time.sleep(1)\r\n\r\n\r\nIt seems that loads all the entire dataset in arrow memory and then deallocate it, just with the arrow() method, no query!!!!\r\n\r\n\r\nAny comment? \r\n\nSame happens for me with R @LuisMoralesAlonso . For now, i rather load datasets with the glob notation, which doesn't bring memory overflow. But no partitioning variables allowed\nI was running into the same issue when using duckdb=0.3.1 and calling `con.register_arrow()` on a partitioned dataset (huge memory spike after the function call had completed, as measured by @LuisMoralesAlonso's code). With duckdb=0.3.4 and `con.register()` the problem seems to have disappeared (I was using pyarrow=8.0.0 in both scenarios).\nNote that a basic hive partition parsing option was added. Will leave this issue open for a bit because I still want to implement an optimisation that essentially achieves what @dforsber described: where we can actually apply filters on the hive partitions before the files are even opened (currently the parquet metadata of all files is scanned, instead of only the ones matching the filter).",
  "created_at": "2022-07-26T13:16:13Z"
}