{
  "repo": "duckdb/duckdb",
  "pull_number": 6467,
  "instance_id": "duckdb__duckdb-6467",
  "issue_numbers": [
    "5310"
  ],
  "base_commit": "6525767cf115f7996eaab67641a5eae3a41ab2fd",
  "patch": "diff --git a/.github/workflows/LinuxRelease.yml b/.github/workflows/LinuxRelease.yml\nindex 448a97eb79fe..8e13e00c63b8 100644\n--- a/.github/workflows/LinuxRelease.yml\n+++ b/.github/workflows/LinuxRelease.yml\n@@ -690,7 +690,8 @@ jobs:\n       shell: bash\n       run: |\n         sudo ./scripts/install_s3_test_server.sh\n-        ./scripts/run_s3_test_server.sh\n+        ./scripts/generate_presigned_url.sh\n+        source ./scripts/run_s3_test_server.sh\n         sleep 60\n \n     - name: Test\ndiff --git a/extension/httpfs/httpfs.cpp b/extension/httpfs/httpfs.cpp\nindex d51eb344ae73..ab038eeb456c 100644\n--- a/extension/httpfs/httpfs.cpp\n+++ b/extension/httpfs/httpfs.cpp\n@@ -338,8 +338,10 @@ unique_ptr<ResponseWrapper> HTTPFileSystem::GetRangeRequest(FileHandle &handle,\n \t\t\t    if (hfs.state) {\n \t\t\t\t    hfs.state->total_bytes_received += data_length;\n \t\t\t    }\n-\t\t\t    memcpy(buffer_out + out_offset, data, data_length);\n-\t\t\t    out_offset += data_length;\n+\t\t\t    if (buffer_out != nullptr) {\n+\t\t\t\t    memcpy(buffer_out + out_offset, data, data_length);\n+\t\t\t\t    out_offset += data_length;\n+\t\t\t    }\n \t\t\t    return true;\n \t\t    });\n \t});\n@@ -556,6 +558,7 @@ void HTTPFileHandle::Initialize(FileOpener *opener) {\n \t}\n \n \tauto res = hfs.HeadRequest(*this, path, {});\n+\tstring range_length;\n \n \tif (res->code != 200) {\n \t\tif ((flags & FileFlags::FILE_FLAGS_WRITE) && res->code == 404) {\n@@ -566,9 +569,30 @@ void HTTPFileHandle::Initialize(FileOpener *opener) {\n \t\t\tlength = 0;\n \t\t\treturn;\n \t\t} else {\n-\t\t\tthrow HTTPException(res->code, res->error,\n-\t\t\t                    Exception::ConstructMessage(\"Unable to connect to URL \\\"%s\\\": %s (%s)\", res->http_url,\n-\t\t\t                                                to_string(res->code), res->error));\n+\t\t\t// HEAD request fail, use Range request for another try (read only one byte)\n+\t\t\tif ((flags & FileFlags::FILE_FLAGS_READ) && res->code != 404) {\n+\t\t\t\tauto range_res = hfs.GetRangeRequest(*this, path, {}, 0, nullptr, 2);\n+\t\t\t\tif (range_res->code != 206) {\n+\t\t\t\t\tthrow IOException(\"Unable to connect to URL \\\"%s\\\": %d (%s)\", path, res->code, res->error);\n+\t\t\t\t}\n+\t\t\t\tauto range_find = range_res->headers[\"Content-Range\"].find(\"/\");\n+\n+\t\t\t\tif (range_find == std::string::npos || range_res->headers[\"Content-Range\"].size() < range_find + 1) {\n+\t\t\t\t\tthrow IOException(\"Unknown Content-Range Header \\\"The value of Content-Range Header\\\":  (%s)\",\n+\t\t\t\t\t                  range_res->headers[\"Content-Range\"]);\n+\t\t\t\t}\n+\n+\t\t\t\trange_length = range_res->headers[\"Content-Range\"].substr(range_find + 1);\n+\t\t\t\tif (range_length == \"*\") {\n+\t\t\t\t\tthrow IOException(\"Unknown total length of the document \\\"%s\\\": %d (%s)\", path, res->code,\n+\t\t\t\t\t                  res->error);\n+\t\t\t\t}\n+\t\t\t\tres = std::move(range_res);\n+\t\t\t} else {\n+\t\t\t\tthrow HTTPException(res->code, res->error,\n+\t\t\t\t                    Exception::ConstructMessage(\"Unable to connect to URL \\\"%s\\\": %s (%s)\",\n+\t\t\t\t                                                res->http_url, to_string(res->code), res->error));\n+\t\t\t}\n \t\t}\n \t}\n \n@@ -582,7 +606,11 @@ void HTTPFileHandle::Initialize(FileOpener *opener) {\n \t\tlength = 0;\n \t} else {\n \t\ttry {\n-\t\t\tlength = std::stoll(res->headers[\"Content-Length\"]);\n+\t\t\tif (res->headers.find(\"Content-Range\") == res->headers.end() || res->headers[\"Content-Range\"].empty()) {\n+\t\t\t\tlength = std::stoll(res->headers[\"Content-Length\"]);\n+\t\t\t} else {\n+\t\t\t\tlength = std::stoll(range_length);\n+\t\t\t}\n \t\t} catch (std::invalid_argument &e) {\n \t\t\tthrow IOException(\"Invalid Content-Length header received: %s\", res->headers[\"Content-Length\"]);\n \t\t} catch (std::out_of_range &e) {\n@@ -590,7 +618,6 @@ void HTTPFileHandle::Initialize(FileOpener *opener) {\n \t\t}\n \t}\n \tif (length == 0 || http_params.force_download) {\n-\n \t\tlock_guard<mutex> lock(state->cached_files_mutex);\n \t\tauto &cached_file = state->cached_files[path];\n \ndiff --git a/scripts/generate_presigned_url.sh b/scripts/generate_presigned_url.sh\nnew file mode 100755\nindex 000000000000..338c643b2716\n--- /dev/null\n+++ b/scripts/generate_presigned_url.sh\n@@ -0,0 +1,13 @@\n+#!/usr/bin/env bash\n+#Note: DONT run as root\n+\n+mkdir -p data/parquet-testing/presigned\n+\n+generate_large_parquet_query=$(cat <<EOF\n+\n+CALL DBGEN(sf=1);\n+COPY lineitem TO 'data/parquet-testing/presigned/presigned-url-lineitem.parquet' (FORMAT 'parquet');\n+\n+EOF\n+)\n+build/release/duckdb -c \"$generate_large_parquet_query\"\n\\ No newline at end of file\ndiff --git a/scripts/minio_s3.yml b/scripts/minio_s3.yml\nindex dce2467b6a15..89b43b76644c 100644\n--- a/scripts/minio_s3.yml\n+++ b/scripts/minio_s3.yml\n@@ -22,6 +22,8 @@ services:\n       - minio\n     links:\n       - minio\n+    volumes:\n+      - ${PWD}/data:/duckdb/data\n     entrypoint: >\n       /bin/sh -c \"\n         while true; do\n@@ -54,5 +56,18 @@ services:\n         /usr/bin/mc mb myminio/test-bucket-public\n         /usr/bin/mc policy set download myminio/test-bucket-public\n         /usr/bin/mc policy get myminio/test-bucket-public\n+\n+        # This is for the test of presigned URLs\n+        # small file upload\n+        /usr/bin/mc cp /duckdb/data/csv/phonenumbers.csv myminio/test-bucket/presigned/phonenumbers.csv\n+        /usr/bin/mc cp /duckdb/data/parquet-testing/glob/t1.parquet myminio/test-bucket/presigned/t1.parquet\n+\n+        # large file upload\n+        /usr/bin/mc cp /duckdb/data/parquet-testing/presigned/presigned-url-lineitem.parquet myminio/test-bucket/presigned/lineitem_large.parquet\n+\n+        /usr/bin/mc share download myminio/test-bucket/presigned/phonenumbers.csv\n+        /usr/bin/mc share download myminio/test-bucket/presigned/t1.parquet\n+        /usr/bin/mc share download myminio/test-bucket/presigned/lineitem_large.parquet\n+\n         exit 0;\n       \"\n\\ No newline at end of file\n",
  "test_patch": "diff --git a/scripts/run_s3_test_server.sh b/scripts/run_s3_test_server.sh\nindex 347eb47548c4..cf4d76cbc17e 100755\n--- a/scripts/run_s3_test_server.sh\n+++ b/scripts/run_s3_test_server.sh\n@@ -4,3 +4,17 @@\n mkdir -p /tmp/minio_test_data\n mkdir -p /tmp/minio_root_data\n docker compose -f scripts/minio_s3.yml -p duckdb-minio up -d\n+\n+# for testing presigned url \n+sleep 3\n+container_name=$(docker ps -a --format '{{.Names}}' | grep -m 1 \"duckdb-minio\")\n+echo $container_name\n+\n+export S3_SMALL_CSV_PRESIGNED_URL=$(docker logs $container_name | grep -m 1 'Share:.*phonenumbers\\.csv' | grep -o 'http[s]\\?://[^ ]\\+')\n+echo $S3_SMALL_CSV_PRESIGNED_URL\n+\n+export S3_SMALL_PARQUET_PRESIGNED_URL=$(docker logs $container_name | grep -m 1 'Share:.*t1\\.parquet' | grep -o 'http[s]\\?://[^ ]\\+')\n+echo $S3_SMALL_PARQUET_PRESIGNED_URL\n+\n+export S3_LARGE_PARQUET_PRESIGNED_URL=$(docker logs $container_name | grep -m 1 'Share:.*lineitem_large\\.parquet' | grep -o 'http[s]\\?://[^ ]\\+')\n+echo $S3_LARGE_PARQUET_PRESIGNED_URL\ndiff --git a/test/sql/copy/s3/s3_presigned_read.test b/test/sql/copy/s3/s3_presigned_read.test\nnew file mode 100644\nindex 000000000000..2bf244df496b\n--- /dev/null\n+++ b/test/sql/copy/s3/s3_presigned_read.test\n@@ -0,0 +1,40 @@\n+# name: test/sql/copy/s3/s3_presigned_read.test\n+# description: Read small csv/parquet files from S3 Presigned URL.\n+# group: [s3]\n+\n+require parquet\n+\n+require httpfs\n+\n+require-env S3_TEST_SERVER_AVAILABLE 1\n+\n+# Require that these environment variables are also set\n+\n+require-env AWS_DEFAULT_REGION\n+\n+require-env AWS_ACCESS_KEY_ID\n+\n+require-env AWS_SECRET_ACCESS_KEY\n+\n+require-env DUCKDB_S3_ENDPOINT\n+\n+require-env DUCKDB_S3_USE_SSL\n+\n+require-env S3_SMALL_CSV_PRESIGNED_URL\n+\n+require-env S3_SMALL_PARQUET_PRESIGNED_URL\n+\n+# override the default behaviour of skipping HTTP errors and connection failures: this test fails on connection issues\n+set ignore_error_messages\n+\n+query I\n+SELECT phone FROM read_csv_auto('${S3_SMALL_CSV_PRESIGNED_URL}', header=1);\n+----\n++318855443322\n++552244331122\n++12233445567\n+\n+query I\n+SELECT i FROM '${S3_SMALL_PARQUET_PRESIGNED_URL}';\n+----\n+1\n\\ No newline at end of file\ndiff --git a/test/sql/copy/s3/s3_presigned_read.test_slow b/test/sql/copy/s3/s3_presigned_read.test_slow\nnew file mode 100644\nindex 000000000000..d2ccf9f4d504\n--- /dev/null\n+++ b/test/sql/copy/s3/s3_presigned_read.test_slow\n@@ -0,0 +1,42 @@\n+# name: test/sql/copy/s3/s3_presigned_read.test_slow\n+# description: Read large csv/parquet files from S3 Presigned URL.\n+# group: [s3]\n+\n+require parquet\n+\n+require httpfs\n+\n+require-env S3_TEST_SERVER_AVAILABLE 1\n+\n+# Require that these environment variables are also set\n+\n+require-env AWS_DEFAULT_REGION\n+\n+require-env AWS_ACCESS_KEY_ID\n+\n+require-env AWS_SECRET_ACCESS_KEY\n+\n+require-env DUCKDB_S3_ENDPOINT\n+\n+require-env DUCKDB_S3_USE_SSL\n+\n+\n+require-env S3_LARGE_PARQUET_PRESIGNED_URL\n+\n+# override the default behaviour of skipping HTTP errors and connection failures: this test fails on connection issues\n+set ignore_error_messages\n+\n+\n+query I\n+SELECT\n+    sum(l_extendedprice * l_discount) AS revenue\n+FROM\n+    '${S3_LARGE_PARQUET_PRESIGNED_URL}' \n+WHERE\n+    l_shipdate >= CAST('1994-01-01' AS date)\n+    AND l_shipdate < CAST('1995-01-01' AS date)\n+    AND l_discount BETWEEN 0.05\n+    AND 0.07\n+    AND l_quantity < 24;\n+----\n+123141078.2283\n\\ No newline at end of file\n",
  "problem_statement": "'403 Forbidden' for AWS s3 presigned URL\nHi. I am creating an Amazon s3 presigned URL to a CSV file using their Javascript SDK. The URL downloads the file fine in a browser which confirms it's OK, but if I use it in the duck cli or via the npm package with:\r\n\r\n`create view test as select * from read_csv_auto('...presigned url...');`\r\n\r\nI get `403 (Forbidden)`.\r\n\r\nYou can also test this by locating a CSV file in s3 via the AWS admin console and creating the URL with 'Object actions' ->  'Share with a presigned URL'.\n",
  "hints_text": "Can you please include all the version information requested in the issue template?\nHi, sorry - it's:\r\n\r\nOS: Ubuntu 20.04.5 LTS\r\nDuckDB Version: 0.5.1\r\nDuckDB Client: npm\r\n\r\nAlso tried duck db cli but that's actually 0.5.0.\nOkay, can you please try with the latest master version instead? You can install it with `duckdb@next`\nHi @Mause - Now I am getting a new errror:\r\n\r\n```\r\nError: Connection created on database that was not yet initialized\r\nat default_connection (node_modules/duckdb/lib/duckdb.js:384:32)\r\nat Database.exec (node_modules/duckdb/lib/duckdb.js:558:5)\r\n```\r\n\r\nThis at the place in my javascript where I am issuing the:\r\n```\r\nINSTALL httpfs;\r\nLOAD httpfs;\r\n```\r\n\r\nRequests ahead of creating the view.\r\n\r\nIf I pause the code slightly before running this it then proceeds to give me the same `403 (Forbidden)` error as before.\nAre your sure the cors configuration on the bucket is set correctly?\n> Hi @Mause - Now I am getting a new errror:\n> \n> ```\n> Error: Connection created on database that was not yet initialized\n> at default_connection (node_modules/duckdb/lib/duckdb.js:384:32)\n> at Database.exec (node_modules/duckdb/lib/duckdb.js:558:5)\n> ```\n> \n> This at the place in my javascript where I am issuing the:\n> ```\n> INSTALL httpfs;\n> LOAD httpfs;\n> ```\n> \n> Requests ahead of creating the view.\n> \n> If I pause the code slightly before running this it then proceeds to give me the same `403 (Forbidden)` error as before.\n\nCan you please confirm that you're waiting for the database to startup (with the callback) before you call `exec`?\n> Are your sure the cors configuration on the bucket is set correctly?\r\n\r\nActually no, I am not doing anything with CORS. I may be missing something but can't quite see why this would be relevant, I thought CORS was a protection mechanism related to browser requests.\r\n\r\nI can curl request the file using the presigned URL OK.\n> Can you please confirm that you're waiting for the database to startup (with the callback) before you call `exec`?\r\n\r\nYes I think I can confirm that - I am issuing those commands using the duckdb.exec method and waiting the callback. Also I have not seen that new error before (i.e. on 0.5.0 and 0.5.1, only when I tried the @next).\n> > Can you please confirm that you're waiting for the database to startup (with the callback) before you call `exec`?\n> \n> Yes I think I can confirm that - I am issuing those commands using the duckdb.exec method and waiting the callback. Also I have not seen that new error before (i.e. on 0.5.0 and 0.5.1, only when I tried the @next).\n\nAre you waiting for the callback you passed to the `Database` constructor?\n@Mause OK I sorry didn't realise you had to do that now, that looks like a change in @next relative to 0.5.1 (and not in docs - e.g. https://duckdb.org/docs/api/nodejs/overview).\r\n\r\nBut now I am doing that (i.e. waiting on callback as 3rd param to .ctor) the `Connection created on database that was not yet initialized` error is no more but I still get the `403 (Forbidden)`.\r\n\r\nIf this is CORS related then I am stil confused as this is not running in a browser and if it were I would expect the browser to enforce this on any http requests coming from the page. This is happening running from the CLI or duckdb running server side in nodejs.\n@chrisbrain it's not a new requirement sorry, but a workaround for a bug that needs to be fixed\n\nI'm not sure if we've done any/much testing with presigned urls to be honest, I'll see if I have time to investigate this week\nAh I see - OK well I think I confirmed it's still happening on the latest pre release version.\r\n\r\nFWIW Dropbox has a kind-of-similar feature where they let you generate a URL which is valid for a few hours and that appears to be working fine with duckdb.\r\n\r\nThanks and appreciate anything you can do!\n@Mause I am suspecting now that this happens because in the background duck is making both HTTP HEAD and GET requests and the presigned URLs are only for GET.\r\n\r\nIf that's the case it's quite unfortunate as this seems it would have provided a neat way to connect duck views to s3 files disparate buckets with different credentials.\r\n\r\nNot sure if this is worth any thought but perhaps duckdb could be put into a mode where it uses GET to simulate HEAD requests as I saw mentioned here:\r\nhttps://stackoverflow.com/a/39663152\r\n\r\nBut it may have other implications.\nI also bumped into this problem, I think it's failing on the HEAD request as you mention, the error I was getting was:\r\n\r\n`Invalid Error: Unable to connect to URL \"https://....`\r\n\r\nand that looks to be thrown when a non 200 response is returned from the HEAD request: https://github.com/duckdb/duckdb/blob/master/extension/httpfs/httpfs.cpp#L490\r\n\r\nIt would have been easier to spot what was going wrong if the error message had have said:\r\n\r\n`Invalid Error: Unable to make HEAD request to URL \"https://...`\r\n\r\nI'll just workaround this by downloading the files and then loading them locally.",
  "created_at": "2023-02-26T04:36:40Z"
}