You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
ART not correctly deleting values when dealing with many negative values
The ART index does not appear to correctly remove values when there are many negative values stored. See the test case below. In the test we create a table with a single entry (1), and then insert a large number of values. Then within the same transaction we insert a duplicate value, which causes us to force a rollback.

This works, but leaves some dangling entries in the ART index, which causes the next batch of the test to fail. This also trips the assertion added in #1901. The specific violating value seems to be `-122882`. 


```sql
# name: test/sql/index/art/index_large_abort.test
# description: Test abort of large insertion of negative values into index and verify that all elements are correctly deleted
# group: [art]

statement ok
PRAGMA enable_verification

statement ok
CREATE TABLE a(id INTEGER PRIMARY KEY, c INT);

statement ok
INSERT INTO a VALUES (1, 4)

statement ok
BEGIN TRANSACTION

statement ok
INSERT INTO a SELECT i id, NULL c FROM range(-2, -250000, -1) tbl(i)

statement error
INSERT INTO a VALUES (1, 5)

statement ok
ROLLBACK

query I
SELECT c FROM a WHERE id=1
----
4

query II
SELECT * FROM a
----
1	4

# now with non-null values
statement ok
BEGIN TRANSACTION

statement ok
INSERT INTO a SELECT i id, -i c FROM range(-2, -250000, -1) tbl(i)

statement error
INSERT INTO a VALUES (1, 5)

statement ok
ROLLBACK

query I
SELECT c FROM a WHERE id=1
----
4

query II
SELECT * FROM a
----
1	4

```

</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
2: 
3: ![.github/workflows/main.yml](https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
6: 
7: 
8: ## Installation
9: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
10: 
11: ## Development
12: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
13: 
14: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
15: 
16: 
[end of README.md]
[start of src/catalog/catalog_entry/table_catalog_entry.cpp]
1: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/serializer.hpp"
7: #include "duckdb/main/connection.hpp"
8: #include "duckdb/main/database.hpp"
9: #include "duckdb/parser/constraints/list.hpp"
10: #include "duckdb/parser/parsed_data/alter_table_info.hpp"
11: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
12: #include "duckdb/planner/constraints/bound_unique_constraint.hpp"
13: #include "duckdb/planner/constraints/bound_check_constraint.hpp"
14: #include "duckdb/planner/expression/bound_constant_expression.hpp"
15: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
16: #include "duckdb/storage/storage_manager.hpp"
17: #include "duckdb/planner/binder.hpp"
18: 
19: #include "duckdb/execution/index/art/art.hpp"
20: #include "duckdb/parser/expression/columnref_expression.hpp"
21: #include "duckdb/planner/expression/bound_reference_expression.hpp"
22: #include "duckdb/parser/parsed_expression_iterator.hpp"
23: #include "duckdb/planner/expression_binder/alter_binder.hpp"
24: #include "duckdb/parser/keyword_helper.hpp"
25: 
26: #include <algorithm>
27: #include <sstream>
28: 
29: namespace duckdb {
30: 
31: void TableCatalogEntry::AddLowerCaseAliases(unordered_map<string, column_t> &name_map) {
32: 	unordered_map<string, column_t> extra_lowercase_names;
33: 	for (auto &entry : name_map) {
34: 		auto lcase = StringUtil::Lower(entry.first);
35: 		// check the lowercase name map if there already exists a lowercase version
36: 		if (extra_lowercase_names.find(lcase) == extra_lowercase_names.end()) {
37: 			// not yet: add the mapping
38: 			extra_lowercase_names[lcase] = entry.second;
39: 		} else {
40: 			// the lowercase already exists: set it to invalid index
41: 			extra_lowercase_names[lcase] = INVALID_INDEX;
42: 		}
43: 	}
44: 	// for any new lowercase names, add them to the original name map
45: 	for (auto &entry : extra_lowercase_names) {
46: 		if (entry.second != INVALID_INDEX) {
47: 			name_map[entry.first] = entry.second;
48: 		}
49: 	}
50: }
51: 
52: TableCatalogEntry::TableCatalogEntry(Catalog *catalog, SchemaCatalogEntry *schema, BoundCreateTableInfo *info,
53:                                      std::shared_ptr<DataTable> inherited_storage)
54:     : StandardEntry(CatalogType::TABLE_ENTRY, schema, catalog, info->Base().table), storage(move(inherited_storage)),
55:       columns(move(info->Base().columns)), constraints(move(info->Base().constraints)),
56:       bound_constraints(move(info->bound_constraints)), name_map(info->name_map) {
57: 	this->temporary = info->Base().temporary;
58: 	// add lower case aliases
59: 	AddLowerCaseAliases(name_map);
60: 	// add the "rowid" alias, if there is no rowid column specified in the table
61: 	if (name_map.find("rowid") == name_map.end()) {
62: 		name_map["rowid"] = COLUMN_IDENTIFIER_ROW_ID;
63: 	}
64: 	if (!storage) {
65: 		// create the physical storage
66: 		storage = make_shared<DataTable>(catalog->db, schema->name, name, GetTypes(), move(info->data));
67: 
68: 		// create the unique indexes for the UNIQUE and PRIMARY KEY constraints
69: 		for (idx_t i = 0; i < bound_constraints.size(); i++) {
70: 			auto &constraint = bound_constraints[i];
71: 			if (constraint->type == ConstraintType::UNIQUE) {
72: 				// unique constraint: create a unique index
73: 				auto &unique = (BoundUniqueConstraint &)*constraint;
74: 				// fetch types and create expressions for the index from the columns
75: 				vector<column_t> column_ids;
76: 				vector<unique_ptr<Expression>> unbound_expressions;
77: 				vector<unique_ptr<Expression>> bound_expressions;
78: 				idx_t key_nr = 0;
79: 				for (auto &key : unique.keys) {
80: 					D_ASSERT(key < columns.size());
81: 
82: 					unbound_expressions.push_back(make_unique<BoundColumnRefExpression>(
83: 					    columns[key].name, columns[key].type, ColumnBinding(0, column_ids.size())));
84: 
85: 					bound_expressions.push_back(make_unique<BoundReferenceExpression>(columns[key].type, key_nr++));
86: 					column_ids.push_back(key);
87: 				}
88: 				// create an adaptive radix tree around the expressions
89: 				auto art = make_unique<ART>(column_ids, move(unbound_expressions), true, unique.is_primary_key);
90: 				storage->AddIndex(move(art), bound_expressions);
91: 			}
92: 		}
93: 	}
94: }
95: 
96: bool TableCatalogEntry::ColumnExists(const string &name) {
97: 	return name_map.find(name) != name_map.end();
98: }
99: 
100: unique_ptr<CatalogEntry> TableCatalogEntry::AlterEntry(ClientContext &context, AlterInfo *info) {
101: 	D_ASSERT(!internal);
102: 	if (info->type != AlterType::ALTER_TABLE) {
103: 		throw CatalogException("Can only modify table with ALTER TABLE statement");
104: 	}
105: 	auto table_info = (AlterTableInfo *)info;
106: 	switch (table_info->alter_table_type) {
107: 	case AlterTableType::RENAME_COLUMN: {
108: 		auto rename_info = (RenameColumnInfo *)table_info;
109: 		return RenameColumn(context, *rename_info);
110: 	}
111: 	case AlterTableType::RENAME_TABLE: {
112: 		auto rename_info = (RenameTableInfo *)table_info;
113: 		auto copied_table = Copy(context);
114: 		copied_table->name = rename_info->new_table_name;
115: 		return copied_table;
116: 	}
117: 	case AlterTableType::ADD_COLUMN: {
118: 		auto add_info = (AddColumnInfo *)table_info;
119: 		return AddColumn(context, *add_info);
120: 	}
121: 	case AlterTableType::REMOVE_COLUMN: {
122: 		auto remove_info = (RemoveColumnInfo *)table_info;
123: 		return RemoveColumn(context, *remove_info);
124: 	}
125: 	case AlterTableType::SET_DEFAULT: {
126: 		auto set_default_info = (SetDefaultInfo *)table_info;
127: 		return SetDefault(context, *set_default_info);
128: 	}
129: 	case AlterTableType::ALTER_COLUMN_TYPE: {
130: 		auto change_type_info = (ChangeColumnTypeInfo *)table_info;
131: 		return ChangeColumnType(context, *change_type_info);
132: 	}
133: 	default:
134: 		throw InternalException("Unrecognized alter table type!");
135: 	}
136: }
137: 
138: static void RenameExpression(ParsedExpression &expr, RenameColumnInfo &info) {
139: 	if (expr.type == ExpressionType::COLUMN_REF) {
140: 		auto &colref = (ColumnRefExpression &)expr;
141: 		if (colref.column_name == info.old_name) {
142: 			colref.column_name = info.new_name;
143: 		}
144: 	}
145: 	ParsedExpressionIterator::EnumerateChildren(
146: 	    expr, [&](const ParsedExpression &child) { RenameExpression((ParsedExpression &)child, info); });
147: }
148: 
149: unique_ptr<CatalogEntry> TableCatalogEntry::RenameColumn(ClientContext &context, RenameColumnInfo &info) {
150: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
151: 	create_info->temporary = temporary;
152: 	bool found = false;
153: 	for (idx_t i = 0; i < columns.size(); i++) {
154: 		ColumnDefinition copy = columns[i].Copy();
155: 
156: 		create_info->columns.push_back(move(copy));
157: 		if (info.old_name == columns[i].name) {
158: 			D_ASSERT(!found);
159: 			create_info->columns[i].name = info.new_name;
160: 			found = true;
161: 		}
162: 	}
163: 	if (!found) {
164: 		throw CatalogException("Table does not have a column with name \"%s\"", info.name);
165: 	}
166: 	for (idx_t c_idx = 0; c_idx < constraints.size(); c_idx++) {
167: 		auto copy = constraints[c_idx]->Copy();
168: 		switch (copy->type) {
169: 		case ConstraintType::NOT_NULL:
170: 			// NOT NULL constraint: no adjustments necessary
171: 			break;
172: 		case ConstraintType::CHECK: {
173: 			// CHECK constraint: need to rename column references that refer to the renamed column
174: 			auto &check = (CheckConstraint &)*copy;
175: 			RenameExpression(*check.expression, info);
176: 			break;
177: 		}
178: 		case ConstraintType::UNIQUE: {
179: 			// UNIQUE constraint: possibly need to rename columns
180: 			auto &unique = (UniqueConstraint &)*copy;
181: 			for (idx_t i = 0; i < unique.columns.size(); i++) {
182: 				if (unique.columns[i] == info.old_name) {
183: 					unique.columns[i] = info.new_name;
184: 				}
185: 			}
186: 			break;
187: 		}
188: 		default:
189: 			throw CatalogException("Unsupported constraint for entry!");
190: 		}
191: 		create_info->constraints.push_back(move(copy));
192: 	}
193: 	auto binder = Binder::CreateBinder(context);
194: 	auto bound_create_info = binder->BindCreateTableInfo(move(create_info));
195: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
196: }
197: 
198: unique_ptr<CatalogEntry> TableCatalogEntry::AddColumn(ClientContext &context, AddColumnInfo &info) {
199: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
200: 	create_info->temporary = temporary;
201: 	for (idx_t i = 0; i < columns.size(); i++) {
202: 		create_info->columns.push_back(columns[i].Copy());
203: 	}
204: 	info.new_column.oid = columns.size();
205: 	create_info->columns.push_back(info.new_column.Copy());
206: 
207: 	auto binder = Binder::CreateBinder(context);
208: 	auto bound_create_info = binder->BindCreateTableInfo(move(create_info));
209: 	auto new_storage =
210: 	    make_shared<DataTable>(context, *storage, info.new_column, bound_create_info->bound_defaults.back().get());
211: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
212: 	                                      new_storage);
213: }
214: 
215: unique_ptr<CatalogEntry> TableCatalogEntry::RemoveColumn(ClientContext &context, RemoveColumnInfo &info) {
216: 	idx_t removed_index = INVALID_INDEX;
217: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
218: 	create_info->temporary = temporary;
219: 	for (idx_t i = 0; i < columns.size(); i++) {
220: 		if (columns[i].name == info.removed_column) {
221: 			D_ASSERT(removed_index == INVALID_INDEX);
222: 			removed_index = i;
223: 			continue;
224: 		}
225: 		create_info->columns.push_back(columns[i].Copy());
226: 	}
227: 	if (removed_index == INVALID_INDEX) {
228: 		if (!info.if_exists) {
229: 			throw CatalogException("Table does not have a column with name \"%s\"", info.removed_column);
230: 		}
231: 		return nullptr;
232: 	}
233: 	if (create_info->columns.empty()) {
234: 		throw CatalogException("Cannot drop column: table only has one column remaining!");
235: 	}
236: 	// handle constraints for the new table
237: 	D_ASSERT(constraints.size() == bound_constraints.size());
238: 	for (idx_t constr_idx = 0; constr_idx < constraints.size(); constr_idx++) {
239: 		auto &constraint = constraints[constr_idx];
240: 		auto &bound_constraint = bound_constraints[constr_idx];
241: 		switch (bound_constraint->type) {
242: 		case ConstraintType::NOT_NULL: {
243: 			auto &not_null_constraint = (BoundNotNullConstraint &)*bound_constraint;
244: 			if (not_null_constraint.index != removed_index) {
245: 				// the constraint is not about this column: we need to copy it
246: 				// we might need to shift the index back by one though, to account for the removed column
247: 				idx_t new_index = not_null_constraint.index;
248: 				if (not_null_constraint.index > removed_index) {
249: 					new_index -= 1;
250: 				}
251: 				create_info->constraints.push_back(make_unique<NotNullConstraint>(new_index));
252: 			}
253: 			break;
254: 		}
255: 		case ConstraintType::CHECK: {
256: 			// CHECK constraint
257: 			auto &bound_check = (BoundCheckConstraint &)*bound_constraint;
258: 			// check if the removed column is part of the check constraint
259: 			if (bound_check.bound_columns.find(removed_index) != bound_check.bound_columns.end()) {
260: 				if (bound_check.bound_columns.size() > 1) {
261: 					// CHECK constraint that concerns mult
262: 					throw CatalogException(
263: 					    "Cannot drop column \"%s\" because there is a CHECK constraint that depends on it",
264: 					    info.removed_column);
265: 				} else {
266: 					// CHECK constraint that ONLY concerns this column, strip the constraint
267: 				}
268: 			} else {
269: 				// check constraint does not concern the removed column: simply re-add it
270: 				create_info->constraints.push_back(constraint->Copy());
271: 			}
272: 			break;
273: 		}
274: 		case ConstraintType::UNIQUE: {
275: 			auto copy = constraint->Copy();
276: 			auto &unique = (UniqueConstraint &)*copy;
277: 			if (unique.index != INVALID_INDEX) {
278: 				if (unique.index == removed_index) {
279: 					throw CatalogException(
280: 					    "Cannot drop column \"%s\" because there is a UNIQUE constraint that depends on it",
281: 					    info.removed_column);
282: 				} else if (unique.index > removed_index) {
283: 					unique.index--;
284: 				}
285: 			}
286: 			create_info->constraints.push_back(move(copy));
287: 			break;
288: 		}
289: 		default:
290: 			throw InternalException("Unsupported constraint for entry!");
291: 		}
292: 	}
293: 
294: 	auto binder = Binder::CreateBinder(context);
295: 	auto bound_create_info = binder->BindCreateTableInfo(move(create_info));
296: 	auto new_storage = make_shared<DataTable>(context, *storage, removed_index);
297: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
298: 	                                      new_storage);
299: }
300: 
301: unique_ptr<CatalogEntry> TableCatalogEntry::SetDefault(ClientContext &context, SetDefaultInfo &info) {
302: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
303: 	bool found = false;
304: 	for (idx_t i = 0; i < columns.size(); i++) {
305: 		auto copy = columns[i].Copy();
306: 		if (info.column_name == copy.name) {
307: 			// set the default value of this column
308: 			copy.default_value = info.expression ? info.expression->Copy() : nullptr;
309: 			found = true;
310: 		}
311: 		create_info->columns.push_back(move(copy));
312: 	}
313: 	if (!found) {
314: 		throw BinderException("Table \"%s\" does not have a column with name \"%s\"", info.name, info.column_name);
315: 	}
316: 
317: 	for (idx_t i = 0; i < constraints.size(); i++) {
318: 		auto constraint = constraints[i]->Copy();
319: 		create_info->constraints.push_back(move(constraint));
320: 	}
321: 
322: 	auto binder = Binder::CreateBinder(context);
323: 	auto bound_create_info = binder->BindCreateTableInfo(move(create_info));
324: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
325: }
326: 
327: unique_ptr<CatalogEntry> TableCatalogEntry::ChangeColumnType(ClientContext &context, ChangeColumnTypeInfo &info) {
328: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
329: 	idx_t change_idx = INVALID_INDEX;
330: 	for (idx_t i = 0; i < columns.size(); i++) {
331: 		auto copy = columns[i].Copy();
332: 		if (info.column_name == copy.name) {
333: 			// set the default value of this column
334: 			change_idx = i;
335: 			copy.type = info.target_type;
336: 		}
337: 		create_info->columns.push_back(move(copy));
338: 	}
339: 	if (change_idx == INVALID_INDEX) {
340: 		throw BinderException("Table \"%s\" does not have a column with name \"%s\"", info.name, info.column_name);
341: 	}
342: 
343: 	for (idx_t i = 0; i < constraints.size(); i++) {
344: 		auto constraint = constraints[i]->Copy();
345: 		switch (constraint->type) {
346: 		case ConstraintType::CHECK: {
347: 			auto &bound_check = (BoundCheckConstraint &)*bound_constraints[i];
348: 			if (bound_check.bound_columns.find(change_idx) != bound_check.bound_columns.end()) {
349: 				throw BinderException("Cannot change the type of a column that has a CHECK constraint specified");
350: 			}
351: 			break;
352: 		}
353: 		case ConstraintType::NOT_NULL:
354: 			break;
355: 		case ConstraintType::UNIQUE: {
356: 			auto &bound_unique = (BoundUniqueConstraint &)*bound_constraints[i];
357: 			if (bound_unique.keys.find(change_idx) != bound_unique.keys.end()) {
358: 				throw BinderException(
359: 				    "Cannot change the type of a column that has a UNIQUE or PRIMARY KEY constraint specified");
360: 			}
361: 			break;
362: 		}
363: 		default:
364: 			throw InternalException("Unsupported constraint for entry!");
365: 		}
366: 		create_info->constraints.push_back(move(constraint));
367: 	}
368: 
369: 	auto binder = Binder::CreateBinder(context);
370: 	// bind the specified expression
371: 	vector<column_t> bound_columns;
372: 	AlterBinder expr_binder(*binder, context, name, columns, bound_columns, info.target_type);
373: 	auto expression = info.expression->Copy();
374: 	auto bound_expression = expr_binder.Bind(expression);
375: 	auto bound_create_info = binder->BindCreateTableInfo(move(create_info));
376: 	if (bound_columns.empty()) {
377: 		bound_columns.push_back(COLUMN_IDENTIFIER_ROW_ID);
378: 	}
379: 
380: 	auto new_storage =
381: 	    make_shared<DataTable>(context, *storage, change_idx, info.target_type, move(bound_columns), *bound_expression);
382: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
383: 	                                      new_storage);
384: }
385: 
386: ColumnDefinition &TableCatalogEntry::GetColumn(const string &name) {
387: 	auto entry = name_map.find(name);
388: 	if (entry == name_map.end() || entry->second == COLUMN_IDENTIFIER_ROW_ID) {
389: 		throw CatalogException("Column with name %s does not exist!", name);
390: 	}
391: 	return columns[entry->second];
392: }
393: 
394: vector<LogicalType> TableCatalogEntry::GetTypes() {
395: 	vector<LogicalType> types;
396: 	for (auto &it : columns) {
397: 		types.push_back(it.type);
398: 	}
399: 	return types;
400: }
401: 
402: vector<LogicalType> TableCatalogEntry::GetTypes(const vector<column_t> &column_ids) {
403: 	vector<LogicalType> result;
404: 	for (auto &index : column_ids) {
405: 		if (index == COLUMN_IDENTIFIER_ROW_ID) {
406: 			result.push_back(LOGICAL_ROW_TYPE);
407: 		} else {
408: 			result.push_back(columns[index].type);
409: 		}
410: 	}
411: 	return result;
412: }
413: 
414: void TableCatalogEntry::Serialize(Serializer &serializer) {
415: 	serializer.WriteString(schema->name);
416: 	serializer.WriteString(name);
417: 	D_ASSERT(columns.size() <= NumericLimits<uint32_t>::Maximum());
418: 	serializer.Write<uint32_t>((uint32_t)columns.size());
419: 	for (auto &column : columns) {
420: 		column.Serialize(serializer);
421: 	}
422: 	D_ASSERT(constraints.size() <= NumericLimits<uint32_t>::Maximum());
423: 	serializer.Write<uint32_t>((uint32_t)constraints.size());
424: 	for (auto &constraint : constraints) {
425: 		constraint->Serialize(serializer);
426: 	}
427: }
428: 
429: string TableCatalogEntry::ToSQL() {
430: 	std::stringstream ss;
431: 
432: 	ss << "CREATE TABLE ";
433: 
434: 	if (schema->name != DEFAULT_SCHEMA) {
435: 		ss << KeywordHelper::WriteOptionallyQuoted(schema->name) << ".";
436: 	}
437: 
438: 	ss << KeywordHelper::WriteOptionallyQuoted(name) << "(";
439: 
440: 	// find all columns that have NOT NULL specified, but are NOT primary key columns
441: 	unordered_set<idx_t> not_null_columns;
442: 	unordered_set<idx_t> unique_columns;
443: 	unordered_set<idx_t> pk_columns;
444: 	unordered_set<string> multi_key_pks;
445: 	vector<string> extra_constraints;
446: 	for (auto &constraint : constraints) {
447: 		if (constraint->type == ConstraintType::NOT_NULL) {
448: 			auto &not_null = (NotNullConstraint &)*constraint;
449: 			not_null_columns.insert(not_null.index);
450: 		} else if (constraint->type == ConstraintType::UNIQUE) {
451: 			auto &pk = (UniqueConstraint &)*constraint;
452: 			vector<string> constraint_columns = pk.columns;
453: 			if (pk.index != INVALID_INDEX) {
454: 				// no columns specified: single column constraint
455: 				if (pk.is_primary_key) {
456: 					pk_columns.insert(pk.index);
457: 				} else {
458: 					unique_columns.insert(pk.index);
459: 				}
460: 			} else {
461: 				// multi-column constraint, this constraint needs to go at the end after all columns
462: 				if (pk.is_primary_key) {
463: 					// multi key pk column: insert set of columns into multi_key_pks
464: 					for (auto &col : pk.columns) {
465: 						multi_key_pks.insert(col);
466: 					}
467: 				}
468: 				extra_constraints.push_back(constraint->ToString());
469: 			}
470: 		} else {
471: 			extra_constraints.push_back(constraint->ToString());
472: 		}
473: 	}
474: 
475: 	for (idx_t i = 0; i < columns.size(); i++) {
476: 		if (i > 0) {
477: 			ss << ", ";
478: 		}
479: 		auto &column = columns[i];
480: 		ss << KeywordHelper::WriteOptionallyQuoted(column.name) << " " << column.type.ToString();
481: 		bool not_null = not_null_columns.find(column.oid) != not_null_columns.end();
482: 		bool is_single_key_pk = pk_columns.find(column.oid) != pk_columns.end();
483: 		bool is_multi_key_pk = multi_key_pks.find(column.name) != multi_key_pks.end();
484: 		bool is_unique = unique_columns.find(column.oid) != unique_columns.end();
485: 		if (not_null && !is_single_key_pk && !is_multi_key_pk) {
486: 			// NOT NULL but not a primary key column
487: 			ss << " NOT NULL";
488: 		}
489: 		if (is_single_key_pk) {
490: 			// single column pk: insert constraint here
491: 			ss << " PRIMARY KEY";
492: 		}
493: 		if (is_unique) {
494: 			// single column unique: insert constraint here
495: 			ss << " UNIQUE";
496: 		}
497: 		if (column.default_value) {
498: 			ss << " DEFAULT(" << column.default_value->ToString() << ")";
499: 		}
500: 	}
501: 	// print any extra constraints that still need to be printed
502: 	for (auto &extra_constraint : extra_constraints) {
503: 		ss << ", ";
504: 		ss << extra_constraint;
505: 	}
506: 
507: 	ss << ");";
508: 	return ss.str();
509: }
510: 
511: unique_ptr<CreateTableInfo> TableCatalogEntry::Deserialize(Deserializer &source) {
512: 	auto info = make_unique<CreateTableInfo>();
513: 
514: 	info->schema = source.Read<string>();
515: 	info->table = source.Read<string>();
516: 	auto column_count = source.Read<uint32_t>();
517: 
518: 	for (uint32_t i = 0; i < column_count; i++) {
519: 		auto column = ColumnDefinition::Deserialize(source);
520: 		info->columns.push_back(move(column));
521: 	}
522: 	auto constraint_count = source.Read<uint32_t>();
523: 
524: 	for (uint32_t i = 0; i < constraint_count; i++) {
525: 		auto constraint = Constraint::Deserialize(source);
526: 		info->constraints.push_back(move(constraint));
527: 	}
528: 	return info;
529: }
530: 
531: unique_ptr<CatalogEntry> TableCatalogEntry::Copy(ClientContext &context) {
532: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
533: 	for (idx_t i = 0; i < columns.size(); i++) {
534: 		create_info->columns.push_back(columns[i].Copy());
535: 	}
536: 
537: 	for (idx_t i = 0; i < constraints.size(); i++) {
538: 		auto constraint = constraints[i]->Copy();
539: 		create_info->constraints.push_back(move(constraint));
540: 	}
541: 
542: 	auto binder = Binder::CreateBinder(context);
543: 	auto bound_create_info = binder->BindCreateTableInfo(move(create_info));
544: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
545: }
546: 
547: void TableCatalogEntry::SetAsRoot() {
548: 	storage->SetAsRoot();
549: }
550: 
551: void TableCatalogEntry::CommitAlter(AlterInfo &info) {
552: 	D_ASSERT(info.type == AlterType::ALTER_TABLE);
553: 	auto &alter_table = (AlterTableInfo &)info;
554: 	string column_name;
555: 	switch (alter_table.alter_table_type) {
556: 	case AlterTableType::REMOVE_COLUMN: {
557: 		auto &remove_info = (RemoveColumnInfo &)alter_table;
558: 		column_name = remove_info.removed_column;
559: 		break;
560: 	}
561: 	case AlterTableType::ALTER_COLUMN_TYPE: {
562: 		auto &change_info = (ChangeColumnTypeInfo &)alter_table;
563: 		column_name = change_info.column_name;
564: 		break;
565: 	}
566: 	default:
567: 		break;
568: 	}
569: 	if (column_name.empty()) {
570: 		return;
571: 	}
572: 	idx_t removed_index = INVALID_INDEX;
573: 	for (idx_t i = 0; i < columns.size(); i++) {
574: 		if (columns[i].name == column_name) {
575: 			D_ASSERT(removed_index == INVALID_INDEX);
576: 			removed_index = i;
577: 			continue;
578: 		}
579: 	}
580: 	D_ASSERT(removed_index != INVALID_INDEX);
581: 	storage->CommitDropColumn(removed_index);
582: }
583: 
584: void TableCatalogEntry::CommitDrop() {
585: 	storage->CommitDropTable();
586: }
587: 
588: } // namespace duckdb
[end of src/catalog/catalog_entry/table_catalog_entry.cpp]
[start of src/execution/index/art/art.cpp]
1: #include "duckdb/execution/index/art/art.hpp"
2: #include "duckdb/execution/expression_executor.hpp"
3: #include "duckdb/common/vector_operations/vector_operations.hpp"
4: #include "duckdb/common/bit_operations.hpp"
5: #include <algorithm>
6: #include <ctgmath>
7: #include <cstring>
8: 
9: namespace duckdb {
10: 
11: ART::ART(const vector<column_t> &column_ids, const vector<unique_ptr<Expression>> &unbound_expressions, bool is_unique,
12:          bool is_primary)
13:     : Index(IndexType::ART, column_ids, unbound_expressions, is_unique, is_primary) {
14: 	tree = nullptr;
15: 	expression_result.Initialize(logical_types);
16: 	is_little_endian = IsLittleEndian();
17: 	switch (types[0]) {
18: 	case PhysicalType::BOOL:
19: 	case PhysicalType::INT8:
20: 	case PhysicalType::INT16:
21: 	case PhysicalType::INT32:
22: 	case PhysicalType::INT64:
23: 	case PhysicalType::UINT8:
24: 	case PhysicalType::UINT16:
25: 	case PhysicalType::UINT32:
26: 	case PhysicalType::UINT64:
27: 	case PhysicalType::FLOAT:
28: 	case PhysicalType::DOUBLE:
29: 	case PhysicalType::VARCHAR:
30: 		break;
31: 	default:
32: 		throw InvalidTypeException(types[0], "Invalid type for index");
33: 	}
34: }
35: 
36: ART::~ART() {
37: }
38: 
39: bool ART::LeafMatches(Node *node, Key &key, unsigned depth) {
40: 	auto leaf = static_cast<Leaf *>(node);
41: 	Key &leaf_key = *leaf->value;
42: 	for (idx_t i = depth; i < leaf_key.len; i++) {
43: 		if (leaf_key[i] != key[i]) {
44: 			return false;
45: 		}
46: 	}
47: 
48: 	return true;
49: }
50: 
51: unique_ptr<IndexScanState> ART::InitializeScanSinglePredicate(Transaction &transaction, Value value,
52:                                                               ExpressionType expression_type) {
53: 	auto result = make_unique<ARTIndexScanState>();
54: 	result->values[0] = value;
55: 	result->expressions[0] = expression_type;
56: 	return move(result);
57: }
58: 
59: unique_ptr<IndexScanState> ART::InitializeScanTwoPredicates(Transaction &transaction, Value low_value,
60:                                                             ExpressionType low_expression_type, Value high_value,
61:                                                             ExpressionType high_expression_type) {
62: 	auto result = make_unique<ARTIndexScanState>();
63: 	result->values[0] = low_value;
64: 	result->expressions[0] = low_expression_type;
65: 	result->values[1] = high_value;
66: 	result->expressions[1] = high_expression_type;
67: 	return move(result);
68: }
69: 
70: //===--------------------------------------------------------------------===//
71: // Insert
72: //===--------------------------------------------------------------------===//
73: template <class T>
74: static void TemplatedGenerateKeys(Vector &input, idx_t count, vector<unique_ptr<Key>> &keys, bool is_little_endian) {
75: 	VectorData idata;
76: 	input.Orrify(count, idata);
77: 
78: 	auto input_data = (T *)idata.data;
79: 	for (idx_t i = 0; i < count; i++) {
80: 		auto idx = idata.sel->get_index(i);
81: 		if (idata.validity.RowIsValid(idx)) {
82: 			keys.push_back(Key::CreateKey<T>(input_data[idx], is_little_endian));
83: 		} else {
84: 			keys.push_back(nullptr);
85: 		}
86: 	}
87: }
88: 
89: template <class T>
90: static void ConcatenateKeys(Vector &input, idx_t count, vector<unique_ptr<Key>> &keys, bool is_little_endian) {
91: 	VectorData idata;
92: 	input.Orrify(count, idata);
93: 
94: 	auto input_data = (T *)idata.data;
95: 	for (idx_t i = 0; i < count; i++) {
96: 		auto idx = idata.sel->get_index(i);
97: 		if (!idata.validity.RowIsValid(idx) || !keys[i]) {
98: 			// either this column is NULL, or the previous column is NULL!
99: 			keys[i] = nullptr;
100: 		} else {
101: 			// concatenate the keys
102: 			auto old_key = move(keys[i]);
103: 			auto new_key = Key::CreateKey<T>(input_data[idx], is_little_endian);
104: 			auto key_len = old_key->len + new_key->len;
105: 			auto compound_data = unique_ptr<data_t[]>(new data_t[key_len]);
106: 			memcpy(compound_data.get(), old_key->data.get(), old_key->len);
107: 			memcpy(compound_data.get() + old_key->len, new_key->data.get(), new_key->len);
108: 			keys[i] = make_unique<Key>(move(compound_data), key_len);
109: 		}
110: 	}
111: }
112: 
113: void ART::GenerateKeys(DataChunk &input, vector<unique_ptr<Key>> &keys) {
114: 	keys.reserve(STANDARD_VECTOR_SIZE);
115: 	// generate keys for the first input column
116: 	switch (input.data[0].GetType().InternalType()) {
117: 	case PhysicalType::BOOL:
118: 		TemplatedGenerateKeys<bool>(input.data[0], input.size(), keys, is_little_endian);
119: 		break;
120: 	case PhysicalType::INT8:
121: 		TemplatedGenerateKeys<int8_t>(input.data[0], input.size(), keys, is_little_endian);
122: 		break;
123: 	case PhysicalType::INT16:
124: 		TemplatedGenerateKeys<int16_t>(input.data[0], input.size(), keys, is_little_endian);
125: 		break;
126: 	case PhysicalType::INT32:
127: 		TemplatedGenerateKeys<int32_t>(input.data[0], input.size(), keys, is_little_endian);
128: 		break;
129: 	case PhysicalType::INT64:
130: 		TemplatedGenerateKeys<int64_t>(input.data[0], input.size(), keys, is_little_endian);
131: 		break;
132: 	case PhysicalType::UINT8:
133: 		TemplatedGenerateKeys<uint8_t>(input.data[0], input.size(), keys, is_little_endian);
134: 		break;
135: 	case PhysicalType::UINT16:
136: 		TemplatedGenerateKeys<uint16_t>(input.data[0], input.size(), keys, is_little_endian);
137: 		break;
138: 	case PhysicalType::UINT32:
139: 		TemplatedGenerateKeys<uint32_t>(input.data[0], input.size(), keys, is_little_endian);
140: 		break;
141: 	case PhysicalType::UINT64:
142: 		TemplatedGenerateKeys<uint64_t>(input.data[0], input.size(), keys, is_little_endian);
143: 		break;
144: 	case PhysicalType::FLOAT:
145: 		TemplatedGenerateKeys<float>(input.data[0], input.size(), keys, is_little_endian);
146: 		break;
147: 	case PhysicalType::DOUBLE:
148: 		TemplatedGenerateKeys<double>(input.data[0], input.size(), keys, is_little_endian);
149: 		break;
150: 	case PhysicalType::VARCHAR:
151: 		TemplatedGenerateKeys<string_t>(input.data[0], input.size(), keys, is_little_endian);
152: 		break;
153: 	default:
154: 		throw InvalidTypeException(input.data[0].GetType(), "Invalid type for index");
155: 	}
156: 	for (idx_t i = 1; i < input.ColumnCount(); i++) {
157: 		// for each of the remaining columns, concatenate
158: 		switch (input.data[i].GetType().InternalType()) {
159: 		case PhysicalType::BOOL:
160: 			ConcatenateKeys<bool>(input.data[i], input.size(), keys, is_little_endian);
161: 			break;
162: 		case PhysicalType::INT8:
163: 			ConcatenateKeys<int8_t>(input.data[i], input.size(), keys, is_little_endian);
164: 			break;
165: 		case PhysicalType::INT16:
166: 			ConcatenateKeys<int16_t>(input.data[i], input.size(), keys, is_little_endian);
167: 			break;
168: 		case PhysicalType::INT32:
169: 			ConcatenateKeys<int32_t>(input.data[i], input.size(), keys, is_little_endian);
170: 			break;
171: 		case PhysicalType::INT64:
172: 			ConcatenateKeys<int64_t>(input.data[i], input.size(), keys, is_little_endian);
173: 			break;
174: 		case PhysicalType::UINT8:
175: 			ConcatenateKeys<uint8_t>(input.data[i], input.size(), keys, is_little_endian);
176: 			break;
177: 		case PhysicalType::UINT16:
178: 			ConcatenateKeys<uint16_t>(input.data[i], input.size(), keys, is_little_endian);
179: 			break;
180: 		case PhysicalType::UINT32:
181: 			ConcatenateKeys<uint32_t>(input.data[i], input.size(), keys, is_little_endian);
182: 			break;
183: 		case PhysicalType::UINT64:
184: 			ConcatenateKeys<uint64_t>(input.data[i], input.size(), keys, is_little_endian);
185: 			break;
186: 		case PhysicalType::FLOAT:
187: 			ConcatenateKeys<float>(input.data[i], input.size(), keys, is_little_endian);
188: 			break;
189: 		case PhysicalType::DOUBLE:
190: 			ConcatenateKeys<double>(input.data[i], input.size(), keys, is_little_endian);
191: 			break;
192: 		case PhysicalType::VARCHAR:
193: 			ConcatenateKeys<string_t>(input.data[i], input.size(), keys, is_little_endian);
194: 			break;
195: 		default:
196: 			throw InvalidTypeException(input.data[0].GetType(), "Invalid type for index");
197: 		}
198: 	}
199: }
200: 
201: bool ART::Insert(IndexLock &lock, DataChunk &input, Vector &row_ids) {
202: 	D_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);
203: 	D_ASSERT(logical_types[0] == input.data[0].GetType());
204: 
205: 	// generate the keys for the given input
206: 	vector<unique_ptr<Key>> keys;
207: 	GenerateKeys(input, keys);
208: 
209: 	// now insert the elements into the index
210: 	row_ids.Normalify(input.size());
211: 	auto row_identifiers = FlatVector::GetData<row_t>(row_ids);
212: 	idx_t failed_index = INVALID_INDEX;
213: 	for (idx_t i = 0; i < input.size(); i++) {
214: 		if (!keys[i]) {
215: 			continue;
216: 		}
217: 
218: 		row_t row_id = row_identifiers[i];
219: 		if (!Insert(tree, move(keys[i]), 0, row_id)) {
220: 			// failed to insert because of constraint violation
221: 			failed_index = i;
222: 			break;
223: 		}
224: 	}
225: 	if (failed_index != INVALID_INDEX) {
226: 		// failed to insert because of constraint violation: remove previously inserted entries
227: 		// generate keys again
228: 		keys.clear();
229: 		GenerateKeys(input, keys);
230: 		unique_ptr<Key> key;
231: 
232: 		// now erase the entries
233: 		for (idx_t i = 0; i < failed_index; i++) {
234: 			if (!keys[i]) {
235: 				continue;
236: 			}
237: 			row_t row_id = row_identifiers[i];
238: 			Erase(tree, *keys[i], 0, row_id);
239: 		}
240: 		return false;
241: 	}
242: 	return true;
243: }
244: 
245: bool ART::Append(IndexLock &lock, DataChunk &appended_data, Vector &row_identifiers) {
246: 	DataChunk expression_result;
247: 	expression_result.Initialize(logical_types);
248: 
249: 	// first resolve the expressions for the index
250: 	ExecuteExpressions(appended_data, expression_result);
251: 
252: 	// now insert into the index
253: 	return Insert(lock, expression_result, row_identifiers);
254: }
255: 
256: void ART::VerifyAppend(DataChunk &chunk) {
257: 	if (!is_unique) {
258: 		return;
259: 	}
260: 
261: 	DataChunk expression_result;
262: 	expression_result.Initialize(logical_types);
263: 
264: 	// unique index, check
265: 	lock_guard<mutex> l(lock);
266: 	// first resolve the expressions for the index
267: 	ExecuteExpressions(chunk, expression_result);
268: 
269: 	// generate the keys for the given input
270: 	vector<unique_ptr<Key>> keys;
271: 	GenerateKeys(expression_result, keys);
272: 
273: 	for (idx_t i = 0; i < chunk.size(); i++) {
274: 		if (!keys[i]) {
275: 			continue;
276: 		}
277: 		if (Lookup(tree, *keys[i], 0) != nullptr) {
278: 			// node already exists in tree
279: 			throw ConstraintException("duplicate key value violates primary key or unique constraint");
280: 		}
281: 	}
282: }
283: 
284: bool ART::InsertToLeaf(Leaf &leaf, row_t row_id) {
285: 	if (is_unique && leaf.num_elements != 0) {
286: 		return false;
287: 	}
288: 	leaf.Insert(row_id);
289: 	return true;
290: }
291: 
292: bool ART::Insert(unique_ptr<Node> &node, unique_ptr<Key> value, unsigned depth, row_t row_id) {
293: 	Key &key = *value;
294: 	if (!node) {
295: 		// node is currently empty, create a leaf here with the key
296: 		node = make_unique<Leaf>(*this, move(value), row_id);
297: 		return true;
298: 	}
299: 
300: 	if (node->type == NodeType::NLeaf) {
301: 		// Replace leaf with Node4 and store both leaves in it
302: 		auto leaf = static_cast<Leaf *>(node.get());
303: 
304: 		Key &existing_key = *leaf->value;
305: 		uint32_t new_prefix_length = 0;
306: 		// Leaf node is already there, update row_id vector
307: 		if (depth + new_prefix_length == existing_key.len && existing_key.len == key.len) {
308: 			return InsertToLeaf(*leaf, row_id);
309: 		}
310: 		while (existing_key[depth + new_prefix_length] == key[depth + new_prefix_length]) {
311: 			new_prefix_length++;
312: 			// Leaf node is already there, update row_id vector
313: 			if (depth + new_prefix_length == existing_key.len && existing_key.len == key.len) {
314: 				return InsertToLeaf(*leaf, row_id);
315: 			}
316: 		}
317: 
318: 		unique_ptr<Node> new_node = make_unique<Node4>(*this, new_prefix_length);
319: 		new_node->prefix_length = new_prefix_length;
320: 		memcpy(new_node->prefix.get(), &key[depth], new_prefix_length);
321: 		Node4::Insert(*this, new_node, existing_key[depth + new_prefix_length], node);
322: 		unique_ptr<Node> leaf_node = make_unique<Leaf>(*this, move(value), row_id);
323: 		Node4::Insert(*this, new_node, key[depth + new_prefix_length], leaf_node);
324: 		node = move(new_node);
325: 		return true;
326: 	}
327: 
328: 	// Handle prefix of inner node
329: 	if (node->prefix_length) {
330: 		uint32_t mismatch_pos = Node::PrefixMismatch(*this, node.get(), key, depth);
331: 		if (mismatch_pos != node->prefix_length) {
332: 			// Prefix differs, create new node
333: 			unique_ptr<Node> new_node = make_unique<Node4>(*this, mismatch_pos);
334: 			new_node->prefix_length = mismatch_pos;
335: 			memcpy(new_node->prefix.get(), node->prefix.get(), mismatch_pos);
336: 			// Break up prefix
337: 			auto node_ptr = node.get();
338: 			Node4::Insert(*this, new_node, node->prefix[mismatch_pos], node);
339: 			node_ptr->prefix_length -= (mismatch_pos + 1);
340: 			memmove(node_ptr->prefix.get(), node_ptr->prefix.get() + mismatch_pos + 1, node_ptr->prefix_length);
341: 			unique_ptr<Node> leaf_node = make_unique<Leaf>(*this, move(value), row_id);
342: 			Node4::Insert(*this, new_node, key[depth + mismatch_pos], leaf_node);
343: 			node = move(new_node);
344: 			return true;
345: 		}
346: 		depth += node->prefix_length;
347: 	}
348: 
349: 	// Recurse
350: 	idx_t pos = node->GetChildPos(key[depth]);
351: 	if (pos != INVALID_INDEX) {
352: 		auto child = node->GetChild(pos);
353: 		return Insert(*child, move(value), depth + 1, row_id);
354: 	}
355: 	unique_ptr<Node> new_node = make_unique<Leaf>(*this, move(value), row_id);
356: 	Node::InsertLeaf(*this, node, key[depth], new_node);
357: 	return true;
358: }
359: 
360: //===--------------------------------------------------------------------===//
361: // Delete
362: //===--------------------------------------------------------------------===//
363: void ART::Delete(IndexLock &state, DataChunk &input, Vector &row_ids) {
364: 	DataChunk expression_result;
365: 	expression_result.Initialize(logical_types);
366: 
367: 	// first resolve the expressions
368: 	ExecuteExpressions(input, expression_result);
369: 
370: 	// then generate the keys for the given input
371: 	vector<unique_ptr<Key>> keys;
372: 	GenerateKeys(expression_result, keys);
373: 
374: 	// now erase the elements from the database
375: 	row_ids.Normalify(input.size());
376: 	auto row_identifiers = FlatVector::GetData<row_t>(row_ids);
377: 
378: 	for (idx_t i = 0; i < input.size(); i++) {
379: 		if (!keys[i]) {
380: 			continue;
381: 		}
382: 		Erase(tree, *keys[i], 0, row_identifiers[i]);
383: 	}
384: }
385: 
386: void ART::Erase(unique_ptr<Node> &node, Key &key, unsigned depth, row_t row_id) {
387: 	if (!node) {
388: 		return;
389: 	}
390: 	// Delete a leaf from a tree
391: 	if (node->type == NodeType::NLeaf) {
392: 		// Make sure we have the right leaf
393: 		if (ART::LeafMatches(node.get(), key, depth)) {
394: 			auto leaf = static_cast<Leaf *>(node.get());
395: 			leaf->Remove(row_id);
396: 			if (leaf->num_elements == 0) {
397: 				node.reset();
398: 			}
399: 		}
400: 		return;
401: 	}
402: 
403: 	// Handle prefix
404: 	if (node->prefix_length) {
405: 		if (Node::PrefixMismatch(*this, node.get(), key, depth) != node->prefix_length) {
406: 			return;
407: 		}
408: 		depth += node->prefix_length;
409: 	}
410: 	idx_t pos = node->GetChildPos(key[depth]);
411: 	if (pos != INVALID_INDEX) {
412: 		auto child = node->GetChild(pos);
413: 		D_ASSERT(child);
414: 
415: 		unique_ptr<Node> &child_ref = *child;
416: 		if (child_ref->type == NodeType::NLeaf && LeafMatches(child_ref.get(), key, depth)) {
417: 			// Leaf found, remove entry
418: 			auto leaf = static_cast<Leaf *>(child_ref.get());
419: 			leaf->Remove(row_id);
420: 			if (leaf->num_elements == 0) {
421: 				// Leaf is empty, delete leaf, decrement node counter and maybe shrink node
422: 				Node::Erase(*this, node, pos);
423: 			}
424: 		} else {
425: 			// Recurse
426: 			Erase(*child, key, depth + 1, row_id);
427: 		}
428: 	}
429: }
430: 
431: //===--------------------------------------------------------------------===//
432: // Point Query
433: //===--------------------------------------------------------------------===//
434: static unique_ptr<Key> CreateKey(ART &art, PhysicalType type, Value &value) {
435: 	D_ASSERT(type == value.type().InternalType());
436: 	switch (type) {
437: 	case PhysicalType::BOOL:
438: 		return Key::CreateKey<bool>(value.value_.boolean, art.is_little_endian);
439: 	case PhysicalType::INT8:
440: 		return Key::CreateKey<int8_t>(value.value_.tinyint, art.is_little_endian);
441: 	case PhysicalType::INT16:
442: 		return Key::CreateKey<int16_t>(value.value_.smallint, art.is_little_endian);
443: 	case PhysicalType::INT32:
444: 		return Key::CreateKey<int32_t>(value.value_.integer, art.is_little_endian);
445: 	case PhysicalType::INT64:
446: 		return Key::CreateKey<int64_t>(value.value_.bigint, art.is_little_endian);
447: 	case PhysicalType::UINT8:
448: 		return Key::CreateKey<uint8_t>(value.value_.utinyint, art.is_little_endian);
449: 	case PhysicalType::UINT16:
450: 		return Key::CreateKey<uint16_t>(value.value_.usmallint, art.is_little_endian);
451: 	case PhysicalType::UINT32:
452: 		return Key::CreateKey<uint32_t>(value.value_.uinteger, art.is_little_endian);
453: 	case PhysicalType::UINT64:
454: 		return Key::CreateKey<uint64_t>(value.value_.ubigint, art.is_little_endian);
455: 	case PhysicalType::INT128:
456: 		return Key::CreateKey<hugeint_t>(value.value_.hugeint, art.is_little_endian);
457: 	case PhysicalType::FLOAT:
458: 		return Key::CreateKey<float>(value.value_.float_, art.is_little_endian);
459: 	case PhysicalType::DOUBLE:
460: 		return Key::CreateKey<double>(value.value_.double_, art.is_little_endian);
461: 	case PhysicalType::VARCHAR:
462: 		return Key::CreateKey<string_t>(string_t(value.str_value.c_str(), value.str_value.size()),
463: 		                                art.is_little_endian);
464: 	default:
465: 		throw InvalidTypeException(type, "Invalid type for index");
466: 	}
467: }
468: 
469: bool ART::SearchEqual(ARTIndexScanState *state, idx_t max_count, vector<row_t> &result_ids) {
470: 	auto key = CreateKey(*this, types[0], state->values[0]);
471: 	auto leaf = static_cast<Leaf *>(Lookup(tree, *key, 0));
472: 	if (!leaf) {
473: 		return true;
474: 	}
475: 	if (leaf->num_elements > max_count) {
476: 		return false;
477: 	}
478: 	for (idx_t i = 0; i < leaf->num_elements; i++) {
479: 		row_t row_id = leaf->GetRowId(i);
480: 		result_ids.push_back(row_id);
481: 	}
482: 	return true;
483: }
484: 
485: void ART::SearchEqualJoinNoFetch(Value &equal_value, idx_t &result_size) {
486: 	//! We need to look for a leaf
487: 	auto key = CreateKey(*this, types[0], equal_value);
488: 	auto leaf = static_cast<Leaf *>(Lookup(tree, *key, 0));
489: 	if (!leaf) {
490: 		return;
491: 	}
492: 	result_size = leaf->num_elements;
493: }
494: 
495: Node *ART::Lookup(unique_ptr<Node> &node, Key &key, unsigned depth) {
496: 	auto node_val = node.get();
497: 
498: 	while (node_val) {
499: 		if (node_val->type == NodeType::NLeaf) {
500: 			auto leaf = static_cast<Leaf *>(node_val);
501: 			Key &leaf_key = *leaf->value;
502: 			//! Check leaf
503: 			for (idx_t i = depth; i < leaf_key.len; i++) {
504: 				if (leaf_key[i] != key[i]) {
505: 					return nullptr;
506: 				}
507: 			}
508: 			return node_val;
509: 		}
510: 		if (node_val->prefix_length) {
511: 			for (idx_t pos = 0; pos < node_val->prefix_length; pos++) {
512: 				if (key[depth + pos] != node_val->prefix[pos]) {
513: 					return nullptr;
514: 				}
515: 			}
516: 			depth += node_val->prefix_length;
517: 		}
518: 		idx_t pos = node_val->GetChildPos(key[depth]);
519: 		if (pos == INVALID_INDEX) {
520: 			return nullptr;
521: 		}
522: 		node_val = node_val->GetChild(pos)->get();
523: 		D_ASSERT(node_val);
524: 
525: 		depth++;
526: 	}
527: 
528: 	return nullptr;
529: }
530: 
531: //===--------------------------------------------------------------------===//
532: // Iterator scans
533: //===--------------------------------------------------------------------===//
534: template <bool HAS_BOUND, bool INCLUSIVE>
535: bool ART::IteratorScan(ARTIndexScanState *state, Iterator *it, Key *bound, idx_t max_count, vector<row_t> &result_ids) {
536: 	bool has_next;
537: 	do {
538: 		if (HAS_BOUND) {
539: 			D_ASSERT(bound);
540: 			if (INCLUSIVE) {
541: 				if (*it->node->value > *bound) {
542: 					break;
543: 				}
544: 			} else {
545: 				if (*it->node->value >= *bound) {
546: 					break;
547: 				}
548: 			}
549: 		}
550: 		if (result_ids.size() + it->node->num_elements > max_count) {
551: 			// adding these elements would exceed the max count
552: 			return false;
553: 		}
554: 		for (idx_t i = 0; i < it->node->num_elements; i++) {
555: 			row_t row_id = it->node->GetRowId(i);
556: 			result_ids.push_back(row_id);
557: 		}
558: 		has_next = ART::IteratorNext(*it);
559: 	} while (has_next);
560: 	return true;
561: }
562: 
563: void Iterator::SetEntry(idx_t entry_depth, IteratorEntry entry) {
564: 	if (stack.size() < entry_depth + 1) {
565: 		stack.resize(MaxValue<idx_t>(8, MaxValue<idx_t>(entry_depth + 1, stack.size() * 2)));
566: 	}
567: 	stack[entry_depth] = entry;
568: }
569: 
570: bool ART::IteratorNext(Iterator &it) {
571: 	// Skip leaf
572: 	if ((it.depth) && ((it.stack[it.depth - 1].node)->type == NodeType::NLeaf)) {
573: 		it.depth--;
574: 	}
575: 
576: 	// Look for the next leaf
577: 	while (it.depth > 0) {
578: 		auto &top = it.stack[it.depth - 1];
579: 		Node *node = top.node;
580: 
581: 		if (node->type == NodeType::NLeaf) {
582: 			// found a leaf: move to next node
583: 			it.node = (Leaf *)node;
584: 			return true;
585: 		}
586: 
587: 		// Find next node
588: 		top.pos = node->GetNextPos(top.pos);
589: 		if (top.pos != INVALID_INDEX) {
590: 			// next node found: go there
591: 			it.SetEntry(it.depth, IteratorEntry(node->GetChild(top.pos)->get(), INVALID_INDEX));
592: 			it.depth++;
593: 		} else {
594: 			// no node found: move up the tree
595: 			it.depth--;
596: 		}
597: 	}
598: 	return false;
599: }
600: 
601: //===--------------------------------------------------------------------===//
602: // Greater Than
603: // Returns: True (If found leaf >= key)
604: //          False (Otherwise)
605: //===--------------------------------------------------------------------===//
606: bool ART::Bound(unique_ptr<Node> &n, Key &key, Iterator &it, bool inclusive) {
607: 	it.depth = 0;
608: 	bool equal = false;
609: 	if (!n) {
610: 		return false;
611: 	}
612: 	Node *node = n.get();
613: 
614: 	idx_t depth = 0;
615: 	while (true) {
616: 		it.SetEntry(it.depth, IteratorEntry(node, 0));
617: 		auto &top = it.stack[it.depth];
618: 		it.depth++;
619: 		if (!equal) {
620: 			while (node->type != NodeType::NLeaf) {
621: 				node = node->GetChild(node->GetMin())->get();
622: 				auto &c_top = it.stack[it.depth];
623: 				c_top.node = node;
624: 				it.depth++;
625: 			}
626: 		}
627: 		if (node->type == NodeType::NLeaf) {
628: 			// found a leaf node: check if it is bigger or equal than the current key
629: 			auto leaf = static_cast<Leaf *>(node);
630: 			it.node = leaf;
631: 			// if the search is not inclusive the leaf node could still be equal to the current value
632: 			// check if leaf is equal to the current key
633: 			if (*leaf->value == key) {
634: 				// if its not inclusive check if there is a next leaf
635: 				if (!inclusive && !IteratorNext(it)) {
636: 					return false;
637: 				} else {
638: 					return true;
639: 				}
640: 			}
641: 
642: 			if (*leaf->value > key) {
643: 				return true;
644: 			}
645: 			// Leaf is lower than key
646: 			// Check if next leaf is still lower than key
647: 			while (IteratorNext(it)) {
648: 				if (*it.node->value == key) {
649: 					// if its not inclusive check if there is a next leaf
650: 					if (!inclusive && !IteratorNext(it)) {
651: 						return false;
652: 					} else {
653: 						return true;
654: 					}
655: 				} else if (*it.node->value > key) {
656: 					// if its not inclusive check if there is a next leaf
657: 					return true;
658: 				}
659: 			}
660: 			return false;
661: 		}
662: 		uint32_t mismatch_pos = Node::PrefixMismatch(*this, node, key, depth);
663: 		if (mismatch_pos != node->prefix_length) {
664: 			if (node->prefix[mismatch_pos] < key[depth + mismatch_pos]) {
665: 				// Less
666: 				it.depth--;
667: 				return IteratorNext(it);
668: 			} else {
669: 				// Greater
670: 				top.pos = INVALID_INDEX;
671: 				return IteratorNext(it);
672: 			}
673: 		}
674: 		// prefix matches, search inside the child for the key
675: 		depth += node->prefix_length;
676: 
677: 		top.pos = node->GetChildGreaterEqual(key[depth], equal);
678: 		if (top.pos == INVALID_INDEX) {
679: 			// Find min leaf
680: 			top.pos = node->GetMin();
681: 		}
682: 		node = node->GetChild(top.pos)->get();
683: 		//! This means all children of this node qualify as geq
684: 
685: 		depth++;
686: 	}
687: }
688: 
689: bool ART::SearchGreater(ARTIndexScanState *state, bool inclusive, idx_t max_count, vector<row_t> &result_ids) {
690: 	Iterator *it = &state->iterator;
691: 	auto key = CreateKey(*this, types[0], state->values[0]);
692: 
693: 	// greater than scan: first set the iterator to the node at which we will start our scan by finding the lowest node
694: 	// that satisfies our requirement
695: 	if (!it->start) {
696: 		bool found = ART::Bound(tree, *key, *it, inclusive);
697: 		if (!found) {
698: 			return true;
699: 		}
700: 		it->start = true;
701: 	}
702: 	// after that we continue the scan; we don't need to check the bounds as any value following this value is
703: 	// automatically bigger and hence satisfies our predicate
704: 	return IteratorScan<false, false>(state, it, nullptr, max_count, result_ids);
705: }
706: 
707: //===--------------------------------------------------------------------===//
708: // Less Than
709: //===--------------------------------------------------------------------===//
710: static Leaf &FindMinimum(Iterator &it, Node &node) {
711: 	Node *next = nullptr;
712: 	idx_t pos = 0;
713: 	switch (node.type) {
714: 	case NodeType::NLeaf:
715: 		it.node = (Leaf *)&node;
716: 		return (Leaf &)node;
717: 	case NodeType::N4:
718: 		next = ((Node4 &)node).child[0].get();
719: 		break;
720: 	case NodeType::N16:
721: 		next = ((Node16 &)node).child[0].get();
722: 		break;
723: 	case NodeType::N48: {
724: 		auto &n48 = (Node48 &)node;
725: 		while (n48.child_index[pos] == Node::EMPTY_MARKER) {
726: 			pos++;
727: 		}
728: 		next = n48.child[n48.child_index[pos]].get();
729: 		break;
730: 	}
731: 	case NodeType::N256: {
732: 		auto &n256 = (Node256 &)node;
733: 		while (!n256.child[pos]) {
734: 			pos++;
735: 		}
736: 		next = n256.child[pos].get();
737: 		break;
738: 	}
739: 	}
740: 	it.SetEntry(it.depth, IteratorEntry(&node, pos));
741: 	it.depth++;
742: 	return FindMinimum(it, *next);
743: }
744: 
745: bool ART::SearchLess(ARTIndexScanState *state, bool inclusive, idx_t max_count, vector<row_t> &result_ids) {
746: 	if (!tree) {
747: 		return true;
748: 	}
749: 
750: 	Iterator *it = &state->iterator;
751: 	auto upper_bound = CreateKey(*this, types[0], state->values[0]);
752: 
753: 	if (!it->start) {
754: 		// first find the minimum value in the ART: we start scanning from this value
755: 		auto &minimum = FindMinimum(state->iterator, *tree);
756: 		// early out min value higher than upper bound query
757: 		if (*minimum.value > *upper_bound) {
758: 			return true;
759: 		}
760: 		it->start = true;
761: 	}
762: 	// now continue the scan until we reach the upper bound
763: 	if (inclusive) {
764: 		return IteratorScan<true, true>(state, it, upper_bound.get(), max_count, result_ids);
765: 	} else {
766: 		return IteratorScan<true, false>(state, it, upper_bound.get(), max_count, result_ids);
767: 	}
768: }
769: 
770: //===--------------------------------------------------------------------===//
771: // Closed Range Query
772: //===--------------------------------------------------------------------===//
773: bool ART::SearchCloseRange(ARTIndexScanState *state, bool left_inclusive, bool right_inclusive, idx_t max_count,
774:                            vector<row_t> &result_ids) {
775: 	auto lower_bound = CreateKey(*this, types[0], state->values[0]);
776: 	auto upper_bound = CreateKey(*this, types[0], state->values[1]);
777: 	Iterator *it = &state->iterator;
778: 	// first find the first node that satisfies the left predicate
779: 	if (!it->start) {
780: 		bool found = ART::Bound(tree, *lower_bound, *it, left_inclusive);
781: 		if (!found) {
782: 			return true;
783: 		}
784: 		it->start = true;
785: 	}
786: 	// now continue the scan until we reach the upper bound
787: 	if (right_inclusive) {
788: 		return IteratorScan<true, true>(state, it, upper_bound.get(), max_count, result_ids);
789: 	} else {
790: 		return IteratorScan<true, false>(state, it, upper_bound.get(), max_count, result_ids);
791: 	}
792: }
793: 
794: bool ART::Scan(Transaction &transaction, DataTable &table, IndexScanState &table_state, idx_t max_count,
795:                vector<row_t> &result_ids) {
796: 	auto state = (ARTIndexScanState *)&table_state;
797: 
798: 	D_ASSERT(state->values[0].type().InternalType() == types[0]);
799: 
800: 	vector<row_t> row_ids;
801: 	bool success = true;
802: 	if (state->values[1].is_null) {
803: 		lock_guard<mutex> l(lock);
804: 		// single predicate
805: 		switch (state->expressions[0]) {
806: 		case ExpressionType::COMPARE_EQUAL:
807: 			success = SearchEqual(state, max_count, row_ids);
808: 			break;
809: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
810: 			success = SearchGreater(state, true, max_count, row_ids);
811: 			break;
812: 		case ExpressionType::COMPARE_GREATERTHAN:
813: 			success = SearchGreater(state, false, max_count, row_ids);
814: 			break;
815: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
816: 			success = SearchLess(state, true, max_count, row_ids);
817: 			break;
818: 		case ExpressionType::COMPARE_LESSTHAN:
819: 			success = SearchLess(state, false, max_count, row_ids);
820: 			break;
821: 		default:
822: 			throw NotImplementedException("Operation not implemented");
823: 		}
824: 	} else {
825: 		lock_guard<mutex> l(lock);
826: 		// two predicates
827: 		D_ASSERT(state->values[1].type().InternalType() == types[0]);
828: 		bool left_inclusive = state->expressions[0] == ExpressionType ::COMPARE_GREATERTHANOREQUALTO;
829: 		bool right_inclusive = state->expressions[1] == ExpressionType ::COMPARE_LESSTHANOREQUALTO;
830: 		success = SearchCloseRange(state, left_inclusive, right_inclusive, max_count, row_ids);
831: 	}
832: 	if (!success) {
833: 		return false;
834: 	}
835: 	if (row_ids.empty()) {
836: 		return true;
837: 	}
838: 	// sort the row ids
839: 	sort(row_ids.begin(), row_ids.end());
840: 	// duplicate eliminate the row ids and append them to the row ids of the state
841: 	result_ids.reserve(row_ids.size());
842: 
843: 	result_ids.push_back(row_ids[0]);
844: 	for (idx_t i = 1; i < row_ids.size(); i++) {
845: 		if (row_ids[i] != row_ids[i - 1]) {
846: 			result_ids.push_back(row_ids[i]);
847: 		}
848: 	}
849: 	return true;
850: }
851: 
852: } // namespace duckdb
[end of src/execution/index/art/art.cpp]
[start of src/function/table/system/pragma_table_info.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
6: #include "duckdb/parser/qualified_name.hpp"
7: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
8: #include "duckdb/planner/constraints/bound_unique_constraint.hpp"
9: 
10: #include "duckdb/common/exception.hpp"
11: #include "duckdb/common/limits.hpp"
12: 
13: #include <algorithm>
14: 
15: namespace duckdb {
16: 
17: struct PragmaTableFunctionData : public TableFunctionData {
18: 	explicit PragmaTableFunctionData(CatalogEntry *entry_p) : entry(entry_p) {
19: 	}
20: 
21: 	CatalogEntry *entry;
22: };
23: 
24: struct PragmaTableOperatorData : public FunctionOperatorData {
25: 	PragmaTableOperatorData() : offset(0) {
26: 	}
27: 	idx_t offset;
28: };
29: 
30: static unique_ptr<FunctionData> PragmaTableInfoBind(ClientContext &context, vector<Value> &inputs,
31:                                                     unordered_map<string, Value> &named_parameters,
32:                                                     vector<LogicalType> &input_table_types,
33:                                                     vector<string> &input_table_names,
34:                                                     vector<LogicalType> &return_types, vector<string> &names) {
35: 	names.emplace_back("cid");
36: 	return_types.push_back(LogicalType::INTEGER);
37: 
38: 	names.emplace_back("name");
39: 	return_types.push_back(LogicalType::VARCHAR);
40: 
41: 	names.emplace_back("type");
42: 	return_types.push_back(LogicalType::VARCHAR);
43: 
44: 	names.emplace_back("notnull");
45: 	return_types.push_back(LogicalType::BOOLEAN);
46: 
47: 	names.emplace_back("dflt_value");
48: 	return_types.push_back(LogicalType::VARCHAR);
49: 
50: 	names.emplace_back("pk");
51: 	return_types.push_back(LogicalType::BOOLEAN);
52: 
53: 	auto qname = QualifiedName::Parse(inputs[0].GetValue<string>());
54: 
55: 	// look up the table name in the catalog
56: 	auto &catalog = Catalog::GetCatalog(context);
57: 	auto entry = catalog.GetEntry(context, CatalogType::TABLE_ENTRY, qname.schema, qname.name);
58: 	return make_unique<PragmaTableFunctionData>(entry);
59: }
60: 
61: unique_ptr<FunctionOperatorData> PragmaTableInfoInit(ClientContext &context, const FunctionData *bind_data,
62:                                                      const vector<column_t> &column_ids,
63:                                                      TableFilterCollection *filters) {
64: 	return make_unique<PragmaTableOperatorData>();
65: }
66: 
67: static void CheckConstraints(TableCatalogEntry *table, idx_t oid, bool &out_not_null, bool &out_pk) {
68: 	out_not_null = false;
69: 	out_pk = false;
70: 	// check all constraints
71: 	// FIXME: this is pretty inefficient, it probably doesn't matter
72: 	for (auto &constraint : table->bound_constraints) {
73: 		switch (constraint->type) {
74: 		case ConstraintType::NOT_NULL: {
75: 			auto &not_null = (BoundNotNullConstraint &)*constraint;
76: 			if (not_null.index == oid) {
77: 				out_not_null = true;
78: 			}
79: 			break;
80: 		}
81: 		case ConstraintType::UNIQUE: {
82: 			auto &unique = (BoundUniqueConstraint &)*constraint;
83: 			if (unique.is_primary_key && unique.keys.find(oid) != unique.keys.end()) {
84: 				out_pk = true;
85: 			}
86: 			break;
87: 		}
88: 		default:
89: 			break;
90: 		}
91: 	}
92: }
93: 
94: static void PragmaTableInfoTable(PragmaTableOperatorData &data, TableCatalogEntry *table, DataChunk &output) {
95: 	if (data.offset >= table->columns.size()) {
96: 		// finished returning values
97: 		return;
98: 	}
99: 	// start returning values
100: 	// either fill up the chunk or return all the remaining columns
101: 	idx_t next = MinValue<idx_t>(data.offset + STANDARD_VECTOR_SIZE, table->columns.size());
102: 	output.SetCardinality(next - data.offset);
103: 
104: 	for (idx_t i = data.offset; i < next; i++) {
105: 		bool not_null, pk;
106: 		auto index = i - data.offset;
107: 		auto &column = table->columns[i];
108: 		D_ASSERT(column.oid < (idx_t)NumericLimits<int32_t>::Maximum());
109: 		CheckConstraints(table, column.oid, not_null, pk);
110: 
111: 		// return values:
112: 		// "cid", PhysicalType::INT32
113: 		output.SetValue(0, index, Value::INTEGER((int32_t)column.oid));
114: 		// "name", PhysicalType::VARCHAR
115: 		output.SetValue(1, index, Value(column.name));
116: 		// "type", PhysicalType::VARCHAR
117: 		output.SetValue(2, index, Value(column.type.ToString()));
118: 		// "notnull", PhysicalType::BOOL
119: 		output.SetValue(3, index, Value::BOOLEAN(not_null));
120: 		// "dflt_value", PhysicalType::VARCHAR
121: 		Value def_value = column.default_value ? Value(column.default_value->ToString()) : Value();
122: 		output.SetValue(4, index, def_value);
123: 		// "pk", PhysicalType::BOOL
124: 		output.SetValue(5, index, Value::BOOLEAN(pk));
125: 	}
126: 	data.offset = next;
127: }
128: 
129: static void PragmaTableInfoView(PragmaTableOperatorData &data, ViewCatalogEntry *view, DataChunk &output) {
130: 	if (data.offset >= view->types.size()) {
131: 		// finished returning values
132: 		return;
133: 	}
134: 	// start returning values
135: 	// either fill up the chunk or return all the remaining columns
136: 	idx_t next = MinValue<idx_t>(data.offset + STANDARD_VECTOR_SIZE, view->types.size());
137: 	output.SetCardinality(next - data.offset);
138: 
139: 	for (idx_t i = data.offset; i < next; i++) {
140: 		auto index = i - data.offset;
141: 		auto type = view->types[index];
142: 		auto &name = view->aliases[index];
143: 		// return values:
144: 		// "cid", PhysicalType::INT32
145: 
146: 		output.SetValue(0, index, Value::INTEGER((int32_t)index));
147: 		// "name", PhysicalType::VARCHAR
148: 		output.SetValue(1, index, Value(name));
149: 		// "type", PhysicalType::VARCHAR
150: 		output.SetValue(2, index, Value(type.ToString()));
151: 		// "notnull", PhysicalType::BOOL
152: 		output.SetValue(3, index, Value::BOOLEAN(false));
153: 		// "dflt_value", PhysicalType::VARCHAR
154: 		output.SetValue(4, index, Value());
155: 		// "pk", PhysicalType::BOOL
156: 		output.SetValue(5, index, Value::BOOLEAN(false));
157: 	}
158: 	data.offset = next;
159: }
160: 
161: static void PragmaTableInfoFunction(ClientContext &context, const FunctionData *bind_data_p,
162:                                     FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
163: 	auto &bind_data = (PragmaTableFunctionData &)*bind_data_p;
164: 	auto &state = (PragmaTableOperatorData &)*operator_state;
165: 	switch (bind_data.entry->type) {
166: 	case CatalogType::TABLE_ENTRY:
167: 		PragmaTableInfoTable(state, (TableCatalogEntry *)bind_data.entry, output);
168: 		break;
169: 	case CatalogType::VIEW_ENTRY:
170: 		PragmaTableInfoView(state, (ViewCatalogEntry *)bind_data.entry, output);
171: 		break;
172: 	default:
173: 		throw NotImplementedException("Unimplemented catalog type for pragma_table_info");
174: 	}
175: }
176: 
177: void PragmaTableInfo::RegisterFunction(BuiltinFunctions &set) {
178: 	set.AddFunction(TableFunction("pragma_table_info", {LogicalType::VARCHAR}, PragmaTableInfoFunction,
179: 	                              PragmaTableInfoBind, PragmaTableInfoInit));
180: }
181: 
182: } // namespace duckdb
[end of src/function/table/system/pragma_table_info.cpp]
[start of src/include/duckdb/planner/constraints/bound_unique_constraint.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/constraints/bound_unique_constraint.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/unordered_set.hpp"
12: #include "duckdb/planner/bound_constraint.hpp"
13: 
14: namespace duckdb {
15: 
16: class BoundUniqueConstraint : public BoundConstraint {
17: public:
18: 	BoundUniqueConstraint(unordered_set<idx_t> keys, bool is_primary_key)
19: 	    : BoundConstraint(ConstraintType::UNIQUE), keys(keys), is_primary_key(is_primary_key) {
20: 	}
21: 
22: 	//! The same keys but represented as an unordered set
23: 	unordered_set<idx_t> keys;
24: 	//! Whether or not the unique constraint is a primary key
25: 	bool is_primary_key;
26: };
27: 
28: } // namespace duckdb
[end of src/include/duckdb/planner/constraints/bound_unique_constraint.hpp]
[start of src/planner/binder/statement/bind_create_table.cpp]
1: #include "duckdb/parser/constraints/list.hpp"
2: #include "duckdb/parser/expression/cast_expression.hpp"
3: #include "duckdb/planner/binder.hpp"
4: #include "duckdb/planner/constraints/list.hpp"
5: #include "duckdb/planner/expression/bound_constant_expression.hpp"
6: #include "duckdb/planner/expression_binder/check_binder.hpp"
7: #include "duckdb/planner/expression_binder/constant_binder.hpp"
8: #include "duckdb/parser/parsed_data/create_table_info.hpp"
9: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
10: #include <algorithm>
11: 
12: namespace duckdb {
13: 
14: static void CreateColumnMap(BoundCreateTableInfo &info) {
15: 	auto &base = (CreateTableInfo &)*info.base;
16: 
17: 	for (uint64_t oid = 0; oid < base.columns.size(); oid++) {
18: 		auto &col = base.columns[oid];
19: 		if (info.name_map.find(col.name) != info.name_map.end()) {
20: 			throw CatalogException("Column with name %s already exists!", col.name);
21: 		}
22: 
23: 		info.name_map[col.name] = oid;
24: 		col.oid = oid;
25: 	}
26: }
27: 
28: static void BindConstraints(Binder &binder, BoundCreateTableInfo &info) {
29: 	auto &base = (CreateTableInfo &)*info.base;
30: 
31: 	bool has_primary_key = false;
32: 	unordered_set<idx_t> primary_keys;
33: 	for (idx_t i = 0; i < base.constraints.size(); i++) {
34: 		auto &cond = base.constraints[i];
35: 		switch (cond->type) {
36: 		case ConstraintType::CHECK: {
37: 			auto bound_constraint = make_unique<BoundCheckConstraint>();
38: 			// check constraint: bind the expression
39: 			CheckBinder check_binder(binder, binder.context, base.table, base.columns, bound_constraint->bound_columns);
40: 			auto &check = (CheckConstraint &)*cond;
41: 			// create a copy of the unbound expression because the binding destroys the constraint
42: 			auto unbound_expression = check.expression->Copy();
43: 			// now bind the constraint and create a new BoundCheckConstraint
44: 			bound_constraint->expression = check_binder.Bind(check.expression);
45: 			info.bound_constraints.push_back(move(bound_constraint));
46: 			// move the unbound constraint back into the original check expression
47: 			check.expression = move(unbound_expression);
48: 			break;
49: 		}
50: 		case ConstraintType::NOT_NULL: {
51: 			auto &not_null = (NotNullConstraint &)*cond;
52: 			info.bound_constraints.push_back(make_unique<BoundNotNullConstraint>(not_null.index));
53: 			break;
54: 		}
55: 		case ConstraintType::UNIQUE: {
56: 			auto &unique = (UniqueConstraint &)*cond;
57: 			// have to resolve columns of the unique constraint
58: 			unordered_set<idx_t> keys;
59: 			if (unique.index != INVALID_INDEX) {
60: 				D_ASSERT(unique.index < base.columns.size());
61: 				// unique constraint is given by single index
62: 				unique.columns.push_back(base.columns[unique.index].name);
63: 				keys.insert(unique.index);
64: 			} else {
65: 				// unique constraint is given by list of names
66: 				// have to resolve names
67: 				D_ASSERT(unique.columns.size() > 0);
68: 				for (auto &keyname : unique.columns) {
69: 					auto entry = info.name_map.find(keyname);
70: 					if (entry == info.name_map.end()) {
71: 						throw ParserException("column \"%s\" named in key does not exist", keyname);
72: 					}
73: 					if (keys.find(entry->second) != keys.end()) {
74: 						throw ParserException("column \"%s\" appears twice in "
75: 						                      "primary key constraint",
76: 						                      keyname);
77: 					}
78: 					keys.insert(entry->second);
79: 				}
80: 			}
81: 
82: 			if (unique.is_primary_key) {
83: 				// we can only have one primary key per table
84: 				if (has_primary_key) {
85: 					throw ParserException("table \"%s\" has more than one primary key", base.table);
86: 				}
87: 				has_primary_key = true;
88: 				primary_keys = keys;
89: 			}
90: 			info.bound_constraints.push_back(make_unique<BoundUniqueConstraint>(keys, unique.is_primary_key));
91: 			break;
92: 		}
93: 		default:
94: 			throw NotImplementedException("unrecognized constraint type in bind");
95: 		}
96: 	}
97: 	if (has_primary_key) {
98: 		// if there is a primary key index, also create a NOT NULL constraint for each of the columns
99: 		for (auto &column_index : primary_keys) {
100: 			base.constraints.push_back(make_unique<NotNullConstraint>(column_index));
101: 			info.bound_constraints.push_back(make_unique<BoundNotNullConstraint>(column_index));
102: 		}
103: 	}
104: }
105: 
106: void Binder::BindDefaultValues(vector<ColumnDefinition> &columns, vector<unique_ptr<Expression>> &bound_defaults) {
107: 	for (idx_t i = 0; i < columns.size(); i++) {
108: 		unique_ptr<Expression> bound_default;
109: 		if (columns[i].default_value) {
110: 			// we bind a copy of the DEFAULT value because binding is destructive
111: 			// and we want to keep the original expression around for serialization
112: 			auto default_copy = columns[i].default_value->Copy();
113: 			ConstantBinder default_binder(*this, context, "DEFAULT value");
114: 			default_binder.target_type = columns[i].type;
115: 			bound_default = default_binder.Bind(default_copy);
116: 		} else {
117: 			// no default value specified: push a default value of constant null
118: 			bound_default = make_unique<BoundConstantExpression>(Value(columns[i].type));
119: 		}
120: 		bound_defaults.push_back(move(bound_default));
121: 	}
122: }
123: 
124: unique_ptr<BoundCreateTableInfo> Binder::BindCreateTableInfo(unique_ptr<CreateInfo> info) {
125: 	auto &base = (CreateTableInfo &)*info;
126: 
127: 	auto result = make_unique<BoundCreateTableInfo>(move(info));
128: 	result->schema = BindSchema(*result->base);
129: 	if (base.query) {
130: 		// construct the result object
131: 		auto query_obj = Bind(*base.query);
132: 		result->query = move(query_obj.plan);
133: 
134: 		// construct the set of columns based on the names and types of the query
135: 		auto &names = query_obj.names;
136: 		auto &sql_types = query_obj.types;
137: 		D_ASSERT(names.size() == sql_types.size());
138: 		for (idx_t i = 0; i < names.size(); i++) {
139: 			base.columns.emplace_back(names[i], sql_types[i]);
140: 		}
141: 		// create the name map for the statement
142: 		CreateColumnMap(*result);
143: 	} else {
144: 		// create the name map for the statement
145: 		CreateColumnMap(*result);
146: 		// bind any constraints
147: 		BindConstraints(*this, *result);
148: 		// bind the default values
149: 		BindDefaultValues(base.columns, result->bound_defaults);
150: 	}
151: 	// bind collations to detect any unsupported collation errors
152: 	for (auto &column : base.columns) {
153: 		ExpressionBinder::TestCollation(context, StringType::GetCollation(column.type));
154: 	}
155: 	return result;
156: }
157: 
158: } // namespace duckdb
[end of src/planner/binder/statement/bind_create_table.cpp]
[start of src/storage/data_table.cpp]
1: #include "duckdb/storage/data_table.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/exception.hpp"
5: #include "duckdb/common/helper.hpp"
6: #include "duckdb/common/vector_operations/vector_operations.hpp"
7: #include "duckdb/execution/expression_executor.hpp"
8: #include "duckdb/main/client_context.hpp"
9: #include "duckdb/planner/constraints/list.hpp"
10: #include "duckdb/planner/table_filter.hpp"
11: #include "duckdb/storage/storage_manager.hpp"
12: #include "duckdb/storage/table/row_group.hpp"
13: #include "duckdb/storage/table/persistent_table_data.hpp"
14: #include "duckdb/storage/table/transient_segment.hpp"
15: #include "duckdb/transaction/transaction.hpp"
16: #include "duckdb/transaction/transaction_manager.hpp"
17: #include "duckdb/storage/checkpoint/table_data_writer.hpp"
18: #include "duckdb/storage/table/standard_column_data.hpp"
19: 
20: #include "duckdb/common/chrono.hpp"
21: 
22: namespace duckdb {
23: 
24: DataTable::DataTable(DatabaseInstance &db, const string &schema, const string &table, vector<LogicalType> types_p,
25:                      unique_ptr<PersistentTableData> data)
26:     : info(make_shared<DataTableInfo>(db, schema, table)), types(move(types_p)), db(db), total_rows(0), is_root(true) {
27: 	// initialize the table with the existing data from disk, if any
28: 	this->row_groups = make_shared<SegmentTree>();
29: 	if (data && !data->row_groups.empty()) {
30: 		for (auto &row_group_pointer : data->row_groups) {
31: 			auto new_row_group = make_unique<RowGroup>(db, *info, types, row_group_pointer);
32: 			auto row_group_count = new_row_group->start + new_row_group->count;
33: 			if (row_group_count > total_rows) {
34: 				total_rows = row_group_count;
35: 			}
36: 			row_groups->AppendSegment(move(new_row_group));
37: 		}
38: 		column_stats = move(data->column_stats);
39: 		if (column_stats.size() != types.size()) {
40: 			throw IOException("Table statistics column count is not aligned with table column count. Corrupt file?");
41: 		}
42: 	}
43: 	if (column_stats.empty()) {
44: 		D_ASSERT(total_rows == 0);
45: 
46: 		AppendRowGroup(0);
47: 		for (auto &type : types) {
48: 			column_stats.push_back(BaseStatistics::CreateEmpty(type));
49: 		}
50: 	} else {
51: 		D_ASSERT(column_stats.size() == types.size());
52: 		D_ASSERT(row_groups->GetRootSegment() != nullptr);
53: 	}
54: }
55: 
56: void DataTable::AppendRowGroup(idx_t start_row) {
57: 	auto new_row_group = make_unique<RowGroup>(db, *info, start_row, 0);
58: 	new_row_group->InitializeEmpty(types);
59: 	row_groups->AppendSegment(move(new_row_group));
60: }
61: 
62: DataTable::DataTable(ClientContext &context, DataTable &parent, ColumnDefinition &new_column, Expression *default_value)
63:     : info(parent.info), types(parent.types), db(parent.db), total_rows(parent.total_rows.load()), is_root(true) {
64: 	// prevent any new tuples from being added to the parent
65: 	lock_guard<mutex> parent_lock(parent.append_lock);
66: 	// add the new column to this DataTable
67: 	auto new_column_type = new_column.type;
68: 	auto new_column_idx = parent.types.size();
69: 
70: 	types.push_back(new_column_type);
71: 
72: 	// set up the statistics
73: 	for (idx_t i = 0; i < parent.column_stats.size(); i++) {
74: 		column_stats.push_back(parent.column_stats[i]->Copy());
75: 	}
76: 	column_stats.push_back(BaseStatistics::CreateEmpty(new_column_type));
77: 
78: 	auto &transaction = Transaction::GetTransaction(context);
79: 
80: 	ExpressionExecutor executor;
81: 	DataChunk dummy_chunk;
82: 	Vector result(new_column_type);
83: 	if (!default_value) {
84: 		FlatVector::Validity(result).SetAllInvalid(STANDARD_VECTOR_SIZE);
85: 	} else {
86: 		executor.AddExpression(*default_value);
87: 	}
88: 
89: 	// fill the column with its DEFAULT value, or NULL if none is specified
90: 	auto new_stats = make_unique<SegmentStatistics>(new_column.type);
91: 	this->row_groups = make_shared<SegmentTree>();
92: 	auto current_row_group = (RowGroup *)parent.row_groups->GetRootSegment();
93: 	while (current_row_group) {
94: 		auto new_row_group = current_row_group->AddColumn(context, new_column, executor, default_value, result);
95: 		// merge in the statistics
96: 		column_stats[new_column_idx]->Merge(*new_row_group->GetStatistics(new_column_idx));
97: 
98: 		row_groups->AppendSegment(move(new_row_group));
99: 		current_row_group = (RowGroup *)current_row_group->next.get();
100: 	}
101: 
102: 	// also add this column to client local storage
103: 	transaction.storage.AddColumn(&parent, this, new_column, default_value);
104: 
105: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
106: 	parent.is_root = false;
107: }
108: 
109: DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t removed_column)
110:     : info(parent.info), types(parent.types), db(parent.db), total_rows(parent.total_rows.load()), is_root(true) {
111: 	// prevent any new tuples from being added to the parent
112: 	lock_guard<mutex> parent_lock(parent.append_lock);
113: 	// first check if there are any indexes that exist that point to the removed column
114: 	info->indexes.Scan([&](Index &index) {
115: 		for (auto &column_id : index.column_ids) {
116: 			if (column_id == removed_column) {
117: 				throw CatalogException("Cannot drop this column: an index depends on it!");
118: 			} else if (column_id > removed_column) {
119: 				throw CatalogException("Cannot drop this column: an index depends on a column after it!");
120: 			}
121: 		}
122: 		return false;
123: 	});
124: 
125: 	// erase the stats and type from this DataTable
126: 	D_ASSERT(removed_column < types.size());
127: 	types.erase(types.begin() + removed_column);
128: 	for (idx_t i = 0; i < parent.column_stats.size(); i++) {
129: 		if (i != removed_column) {
130: 			column_stats.push_back(parent.column_stats[i]->Copy());
131: 		}
132: 	}
133: 
134: 	// alter the row_groups and remove the column from each of them
135: 	this->row_groups = make_shared<SegmentTree>();
136: 	auto current_row_group = (RowGroup *)parent.row_groups->GetRootSegment();
137: 	while (current_row_group) {
138: 		auto new_row_group = current_row_group->RemoveColumn(removed_column);
139: 		row_groups->AppendSegment(move(new_row_group));
140: 		current_row_group = (RowGroup *)current_row_group->next.get();
141: 	}
142: 
143: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
144: 	parent.is_root = false;
145: }
146: 
147: DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t changed_idx, const LogicalType &target_type,
148:                      vector<column_t> bound_columns, Expression &cast_expr)
149:     : info(parent.info), types(parent.types), db(parent.db), total_rows(parent.total_rows.load()), is_root(true) {
150: 	// prevent any tuples from being added to the parent
151: 	lock_guard<mutex> lock(append_lock);
152: 
153: 	// first check if there are any indexes that exist that point to the changed column
154: 	info->indexes.Scan([&](Index &index) {
155: 		for (auto &column_id : index.column_ids) {
156: 			if (column_id == changed_idx) {
157: 				throw CatalogException("Cannot change the type of this column: an index depends on it!");
158: 			}
159: 		}
160: 		return false;
161: 	});
162: 
163: 	// change the type in this DataTable
164: 	types[changed_idx] = target_type;
165: 
166: 	// set up the statistics for the table
167: 	// the column that had its type changed will have the new statistics computed during conversion
168: 	for (idx_t i = 0; i < types.size(); i++) {
169: 		if (i == changed_idx) {
170: 			column_stats.push_back(BaseStatistics::CreateEmpty(types[i]));
171: 		} else {
172: 			column_stats.push_back(parent.column_stats[i]->Copy());
173: 		}
174: 	}
175: 
176: 	// scan the original table, and fill the new column with the transformed value
177: 	auto &transaction = Transaction::GetTransaction(context);
178: 
179: 	vector<LogicalType> scan_types;
180: 	for (idx_t i = 0; i < bound_columns.size(); i++) {
181: 		if (bound_columns[i] == COLUMN_IDENTIFIER_ROW_ID) {
182: 			scan_types.push_back(LOGICAL_ROW_TYPE);
183: 		} else {
184: 			scan_types.push_back(parent.types[bound_columns[i]]);
185: 		}
186: 	}
187: 	DataChunk scan_chunk;
188: 	scan_chunk.Initialize(scan_types);
189: 
190: 	ExpressionExecutor executor;
191: 	executor.AddExpression(cast_expr);
192: 
193: 	TableScanState scan_state;
194: 	scan_state.column_ids = bound_columns;
195: 	scan_state.max_row = total_rows;
196: 
197: 	// now alter the type of the column within all of the row_groups individually
198: 	this->row_groups = make_shared<SegmentTree>();
199: 	auto current_row_group = (RowGroup *)parent.row_groups->GetRootSegment();
200: 	while (current_row_group) {
201: 		auto new_row_group =
202: 		    current_row_group->AlterType(context, target_type, changed_idx, executor, scan_state, scan_chunk);
203: 		column_stats[changed_idx]->Merge(*new_row_group->GetStatistics(changed_idx));
204: 		row_groups->AppendSegment(move(new_row_group));
205: 		current_row_group = (RowGroup *)current_row_group->next.get();
206: 	}
207: 
208: 	transaction.storage.ChangeType(&parent, this, changed_idx, target_type, bound_columns, cast_expr);
209: 
210: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
211: 	parent.is_root = false;
212: }
213: 
214: //===--------------------------------------------------------------------===//
215: // Scan
216: //===--------------------------------------------------------------------===//
217: void DataTable::InitializeScan(TableScanState &state, const vector<column_t> &column_ids,
218:                                TableFilterSet *table_filters) {
219: 	// initialize a column scan state for each column
220: 	// initialize the chunk scan state
221: 	auto row_group = (RowGroup *)row_groups->GetRootSegment();
222: 	state.column_ids = column_ids;
223: 	state.max_row = total_rows;
224: 	state.table_filters = table_filters;
225: 	if (table_filters) {
226: 		D_ASSERT(table_filters->filters.size() > 0);
227: 		state.adaptive_filter = make_unique<AdaptiveFilter>(table_filters);
228: 	}
229: 	while (row_group && !row_group->InitializeScan(state.row_group_scan_state)) {
230: 		row_group = (RowGroup *)row_group->next.get();
231: 	}
232: }
233: 
234: void DataTable::InitializeScan(Transaction &transaction, TableScanState &state, const vector<column_t> &column_ids,
235:                                TableFilterSet *table_filters) {
236: 	InitializeScan(state, column_ids, table_filters);
237: 	transaction.storage.InitializeScan(this, state.local_state, table_filters);
238: }
239: 
240: void DataTable::InitializeScanWithOffset(TableScanState &state, const vector<column_t> &column_ids, idx_t start_row,
241:                                          idx_t end_row) {
242: 
243: 	auto row_group = (RowGroup *)row_groups->GetSegment(start_row);
244: 	state.column_ids = column_ids;
245: 	state.max_row = end_row;
246: 	state.table_filters = nullptr;
247: 	idx_t start_vector = (start_row - row_group->start) / STANDARD_VECTOR_SIZE;
248: 	if (!row_group->InitializeScanWithOffset(state.row_group_scan_state, start_vector)) {
249: 		throw InternalException("Failed to initialize row group scan with offset");
250: 	}
251: }
252: 
253: bool DataTable::InitializeScanInRowGroup(TableScanState &state, const vector<column_t> &column_ids,
254:                                          TableFilterSet *table_filters, RowGroup *row_group, idx_t vector_index,
255:                                          idx_t max_row) {
256: 	state.column_ids = column_ids;
257: 	state.max_row = max_row;
258: 	state.table_filters = table_filters;
259: 	if (table_filters) {
260: 		D_ASSERT(table_filters->filters.size() > 0);
261: 		state.adaptive_filter = make_unique<AdaptiveFilter>(table_filters);
262: 	}
263: 	return row_group->InitializeScanWithOffset(state.row_group_scan_state, vector_index);
264: }
265: 
266: idx_t DataTable::MaxThreads(ClientContext &context) {
267: 	idx_t parallel_scan_vector_count = RowGroup::ROW_GROUP_VECTOR_COUNT;
268: 	if (context.force_parallelism) {
269: 		parallel_scan_vector_count = 1;
270: 	}
271: 	idx_t parallel_scan_tuple_count = STANDARD_VECTOR_SIZE * parallel_scan_vector_count;
272: 
273: 	return total_rows / parallel_scan_tuple_count + 1;
274: }
275: 
276: void DataTable::InitializeParallelScan(ParallelTableScanState &state) {
277: 	state.current_row_group = (RowGroup *)row_groups->GetRootSegment();
278: 	state.transaction_local_data = false;
279: }
280: 
281: bool DataTable::NextParallelScan(ClientContext &context, ParallelTableScanState &state, TableScanState &scan_state,
282:                                  const vector<column_t> &column_ids) {
283: 	while (state.current_row_group) {
284: 		idx_t vector_index;
285: 		idx_t max_row;
286: 		if (context.force_parallelism) {
287: 			vector_index = state.vector_index;
288: 			max_row = state.current_row_group->start +
289: 			          MinValue<idx_t>(state.current_row_group->count,
290: 			                          STANDARD_VECTOR_SIZE * state.vector_index + STANDARD_VECTOR_SIZE);
291: 		} else {
292: 			vector_index = 0;
293: 			max_row = state.current_row_group->start + state.current_row_group->count;
294: 		}
295: 		bool need_to_scan = InitializeScanInRowGroup(scan_state, column_ids, scan_state.table_filters,
296: 		                                             state.current_row_group, vector_index, max_row);
297: 		if (context.force_parallelism) {
298: 			state.vector_index++;
299: 			if (state.vector_index * STANDARD_VECTOR_SIZE >= state.current_row_group->count) {
300: 				state.current_row_group = (RowGroup *)state.current_row_group->next.get();
301: 				state.vector_index = 0;
302: 			}
303: 		} else {
304: 			state.current_row_group = (RowGroup *)state.current_row_group->next.get();
305: 		}
306: 		if (!need_to_scan) {
307: 			// filters allow us to skip this row group: move to the next row group
308: 			continue;
309: 		}
310: 		return true;
311: 	}
312: 	if (!state.transaction_local_data) {
313: 		auto &transaction = Transaction::GetTransaction(context);
314: 		// create a task for scanning the local data
315: 		scan_state.row_group_scan_state.max_row = 0;
316: 		scan_state.max_row = 0;
317: 		transaction.storage.InitializeScan(this, scan_state.local_state, scan_state.table_filters);
318: 		state.transaction_local_data = true;
319: 		return true;
320: 	} else {
321: 		// finished all scans: no more scans remaining
322: 		return false;
323: 	}
324: }
325: 
326: void DataTable::Scan(Transaction &transaction, DataChunk &result, TableScanState &state, vector<column_t> &column_ids) {
327: 	// scan the persistent segments
328: 	if (ScanBaseTable(transaction, result, state)) {
329: 		D_ASSERT(result.size() > 0);
330: 		return;
331: 	}
332: 
333: 	// scan the transaction-local segments
334: 	transaction.storage.Scan(state.local_state, column_ids, result);
335: }
336: 
337: bool DataTable::ScanBaseTable(Transaction &transaction, DataChunk &result, TableScanState &state) {
338: 	auto current_row_group = state.row_group_scan_state.row_group;
339: 	while (current_row_group) {
340: 		current_row_group->Scan(transaction, state.row_group_scan_state, result);
341: 		if (result.size() > 0) {
342: 			return true;
343: 		} else {
344: 			do {
345: 				current_row_group = state.row_group_scan_state.row_group = (RowGroup *)current_row_group->next.get();
346: 				if (current_row_group) {
347: 					bool scan_row_group = current_row_group->InitializeScan(state.row_group_scan_state);
348: 					if (scan_row_group) {
349: 						// skip this row group
350: 						break;
351: 					}
352: 				}
353: 			} while (current_row_group);
354: 		}
355: 	}
356: 	return false;
357: }
358: 
359: //===--------------------------------------------------------------------===//
360: // Fetch
361: //===--------------------------------------------------------------------===//
362: void DataTable::Fetch(Transaction &transaction, DataChunk &result, const vector<column_t> &column_ids,
363:                       Vector &row_identifiers, idx_t fetch_count, ColumnFetchState &state) {
364: 	// figure out which row_group to fetch from
365: 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
366: 	idx_t count = 0;
367: 	for (idx_t i = 0; i < fetch_count; i++) {
368: 		auto row_id = row_ids[i];
369: 		auto row_group = (RowGroup *)row_groups->GetSegment(row_id);
370: 		if (!row_group->Fetch(transaction, row_id - row_group->start)) {
371: 			continue;
372: 		}
373: 		row_group->FetchRow(transaction, state, column_ids, row_id, result, count);
374: 		count++;
375: 	}
376: 	result.SetCardinality(count);
377: }
378: 
379: //===--------------------------------------------------------------------===//
380: // Append
381: //===--------------------------------------------------------------------===//
382: static void VerifyNotNullConstraint(TableCatalogEntry &table, Vector &vector, idx_t count, string &col_name) {
383: 	if (VectorOperations::HasNull(vector, count)) {
384: 		throw ConstraintException("NOT NULL constraint failed: %s.%s", table.name, col_name);
385: 	}
386: }
387: 
388: static void VerifyCheckConstraint(TableCatalogEntry &table, Expression &expr, DataChunk &chunk) {
389: 	ExpressionExecutor executor(expr);
390: 	Vector result(LogicalType::INTEGER);
391: 	try {
392: 		executor.ExecuteExpression(chunk, result);
393: 	} catch (Exception &ex) {
394: 		throw ConstraintException("CHECK constraint failed: %s (Error: %s)", table.name, ex.what());
395: 	} catch (...) {
396: 		throw ConstraintException("CHECK constraint failed: %s (Unknown Error)", table.name);
397: 	}
398: 	VectorData vdata;
399: 	result.Orrify(chunk.size(), vdata);
400: 
401: 	auto dataptr = (int32_t *)vdata.data;
402: 	for (idx_t i = 0; i < chunk.size(); i++) {
403: 		auto idx = vdata.sel->get_index(i);
404: 		if (vdata.validity.RowIsValid(idx) && dataptr[idx] == 0) {
405: 			throw ConstraintException("CHECK constraint failed: %s", table.name);
406: 		}
407: 	}
408: }
409: 
410: void DataTable::VerifyAppendConstraints(TableCatalogEntry &table, DataChunk &chunk) {
411: 	for (auto &constraint : table.bound_constraints) {
412: 		switch (constraint->type) {
413: 		case ConstraintType::NOT_NULL: {
414: 			auto &not_null = *reinterpret_cast<BoundNotNullConstraint *>(constraint.get());
415: 			VerifyNotNullConstraint(table, chunk.data[not_null.index], chunk.size(),
416: 			                        table.columns[not_null.index].name);
417: 			break;
418: 		}
419: 		case ConstraintType::CHECK: {
420: 			auto &check = *reinterpret_cast<BoundCheckConstraint *>(constraint.get());
421: 			VerifyCheckConstraint(table, *check.expression, chunk);
422: 			break;
423: 		}
424: 		case ConstraintType::UNIQUE: {
425: 			//! check whether or not the chunk can be inserted into the indexes
426: 			info->indexes.Scan([&](Index &index) {
427: 				index.VerifyAppend(chunk);
428: 				return false;
429: 			});
430: 			break;
431: 		}
432: 		case ConstraintType::FOREIGN_KEY:
433: 		default:
434: 			throw NotImplementedException("Constraint type not implemented!");
435: 		}
436: 	}
437: }
438: 
439: void DataTable::Append(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk) {
440: 	if (chunk.size() == 0) {
441: 		return;
442: 	}
443: 	if (chunk.ColumnCount() != table.columns.size()) {
444: 		throw CatalogException("Mismatch in column count for append");
445: 	}
446: 	if (!is_root) {
447: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
448: 	}
449: 
450: 	chunk.Verify();
451: 
452: 	// verify any constraints on the new chunk
453: 	VerifyAppendConstraints(table, chunk);
454: 
455: 	// append to the transaction local data
456: 	auto &transaction = Transaction::GetTransaction(context);
457: 	transaction.storage.Append(this, chunk);
458: }
459: 
460: void DataTable::InitializeAppend(Transaction &transaction, TableAppendState &state, idx_t append_count) {
461: 	// obtain the append lock for this table
462: 	state.append_lock = unique_lock<mutex>(append_lock);
463: 	if (!is_root) {
464: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
465: 	}
466: 	state.row_start = total_rows;
467: 	state.current_row = state.row_start;
468: 	state.remaining_append_count = append_count;
469: 
470: 	// start writing to the row_groups
471: 	lock_guard<mutex> row_group_lock(row_groups->node_lock);
472: 	auto last_row_group = (RowGroup *)row_groups->GetLastSegment();
473: 	D_ASSERT(total_rows == last_row_group->start + last_row_group->count);
474: 	last_row_group->InitializeAppend(transaction, state.row_group_append_state, state.remaining_append_count);
475: 	total_rows += append_count;
476: }
477: 
478: void DataTable::Append(Transaction &transaction, DataChunk &chunk, TableAppendState &state) {
479: 	D_ASSERT(is_root);
480: 	D_ASSERT(chunk.ColumnCount() == types.size());
481: 	chunk.Verify();
482: 
483: 	idx_t remaining = chunk.size();
484: 	while (true) {
485: 		auto current_row_group = state.row_group_append_state.row_group;
486: 		// check how much we can fit into the current row_group
487: 		idx_t append_count =
488: 		    MinValue<idx_t>(remaining, RowGroup::ROW_GROUP_SIZE - state.row_group_append_state.offset_in_row_group);
489: 		if (append_count > 0) {
490: 			current_row_group->Append(state.row_group_append_state, chunk, append_count);
491: 			// merge the stats
492: 			lock_guard<mutex> stats_guard(stats_lock);
493: 			for (idx_t i = 0; i < types.size(); i++) {
494: 				column_stats[i]->Merge(*current_row_group->GetStatistics(i));
495: 			}
496: 		}
497: 		state.remaining_append_count -= append_count;
498: 		remaining -= append_count;
499: 		if (remaining > 0) {
500: 			// we expect max 1 iteration of this loop (i.e. a single chunk should never overflow more than one
501: 			// row_group)
502: 			D_ASSERT(chunk.size() == remaining + append_count);
503: 			// slice the input chunk
504: 			if (remaining < chunk.size()) {
505: 				SelectionVector sel(STANDARD_VECTOR_SIZE);
506: 				for (idx_t i = 0; i < remaining; i++) {
507: 					sel.set_index(i, append_count + i);
508: 				}
509: 				chunk.Slice(sel, remaining);
510: 			}
511: 			// append a new row_group
512: 			AppendRowGroup(current_row_group->start + current_row_group->count);
513: 			// set up the append state for this row_group
514: 			lock_guard<mutex> row_group_lock(row_groups->node_lock);
515: 			auto last_row_group = (RowGroup *)row_groups->GetLastSegment();
516: 			last_row_group->InitializeAppend(transaction, state.row_group_append_state, state.remaining_append_count);
517: 			continue;
518: 		} else {
519: 			break;
520: 		}
521: 	}
522: 	state.current_row += chunk.size();
523: }
524: 
525: void DataTable::ScanTableSegment(idx_t row_start, idx_t count, const std::function<void(DataChunk &chunk)> &function) {
526: 	idx_t end = row_start + count;
527: 
528: 	vector<column_t> column_ids;
529: 	vector<LogicalType> types;
530: 	for (idx_t i = 0; i < this->types.size(); i++) {
531: 		column_ids.push_back(i);
532: 		types.push_back(this->types[i]);
533: 	}
534: 	DataChunk chunk;
535: 	chunk.Initialize(types);
536: 
537: 	CreateIndexScanState state;
538: 
539: 	idx_t row_start_aligned = row_start / STANDARD_VECTOR_SIZE * STANDARD_VECTOR_SIZE;
540: 	InitializeScanWithOffset(state, column_ids, row_start_aligned, row_start + count);
541: 
542: 	idx_t current_row = row_start_aligned;
543: 	while (current_row < end) {
544: 		CreateIndexScan(state, column_ids, chunk, true);
545: 		if (chunk.size() == 0) {
546: 			break;
547: 		}
548: 		idx_t end_row = current_row + chunk.size();
549: 		// figure out if we need to write the entire chunk or just part of it
550: 		idx_t chunk_start = MaxValue<idx_t>(current_row, row_start);
551: 		idx_t chunk_end = MinValue<idx_t>(end_row, end);
552: 		D_ASSERT(chunk_start < chunk_end);
553: 		idx_t chunk_count = chunk_end - chunk_start;
554: 		if (chunk_count != chunk.size()) {
555: 			// need to slice the chunk before insert
556: 			auto start_in_chunk = chunk_start % STANDARD_VECTOR_SIZE;
557: 			SelectionVector sel(start_in_chunk, chunk_count);
558: 			chunk.Slice(sel, chunk_count);
559: 			chunk.Verify();
560: 		}
561: 		function(chunk);
562: 		chunk.Reset();
563: 		current_row = end_row;
564: 	}
565: }
566: 
567: void DataTable::WriteToLog(WriteAheadLog &log, idx_t row_start, idx_t count) {
568: 	log.WriteSetTable(info->schema, info->table);
569: 	ScanTableSegment(row_start, count, [&](DataChunk &chunk) { log.WriteInsert(chunk); });
570: }
571: 
572: void DataTable::CommitAppend(transaction_t commit_id, idx_t row_start, idx_t count) {
573: 	lock_guard<mutex> lock(append_lock);
574: 
575: 	auto row_group = (RowGroup *)row_groups->GetSegment(row_start);
576: 	idx_t current_row = row_start;
577: 	idx_t remaining = count;
578: 	while (true) {
579: 		idx_t start_in_row_group = current_row - row_group->start;
580: 		idx_t append_count = MinValue<idx_t>(row_group->count - start_in_row_group, remaining);
581: 
582: 		row_group->CommitAppend(commit_id, start_in_row_group, append_count);
583: 
584: 		current_row += append_count;
585: 		remaining -= append_count;
586: 		if (remaining == 0) {
587: 			break;
588: 		}
589: 		row_group = (RowGroup *)row_group->next.get();
590: 	}
591: 	info->cardinality += count;
592: }
593: 
594: void DataTable::RevertAppendInternal(idx_t start_row, idx_t count) {
595: 	if (count == 0) {
596: 		// nothing to revert!
597: 		return;
598: 	}
599: 	if (total_rows != start_row + count) {
600: 		// interleaved append: don't do anything
601: 		// in this case the rows will stay as "inserted by transaction X", but will never be committed
602: 		// they will never be used by any other transaction and will essentially leave a gap
603: 		// this situation is rare, and as such we don't care about optimizing it (yet?)
604: 		// it only happens if C1 appends a lot of data -> C2 appends a lot of data -> C1 rolls back
605: 		return;
606: 	}
607: 	// adjust the cardinality
608: 	info->cardinality = start_row;
609: 	total_rows = start_row;
610: 	D_ASSERT(is_root);
611: 	// revert appends made to row_groups
612: 	lock_guard<mutex> tree_lock(row_groups->node_lock);
613: 	// find the segment index that the current row belongs to
614: 	idx_t segment_index = row_groups->GetSegmentIndex(start_row);
615: 	auto segment = row_groups->nodes[segment_index].node;
616: 	auto &info = (RowGroup &)*segment;
617: 
618: 	// remove any segments AFTER this segment: they should be deleted entirely
619: 	if (segment_index < row_groups->nodes.size() - 1) {
620: 		row_groups->nodes.erase(row_groups->nodes.begin() + segment_index + 1, row_groups->nodes.end());
621: 	}
622: 	info.next = nullptr;
623: 	info.RevertAppend(start_row);
624: }
625: 
626: void DataTable::RevertAppend(idx_t start_row, idx_t count) {
627: 	lock_guard<mutex> lock(append_lock);
628: 
629: 	if (!info->indexes.Empty()) {
630: 		idx_t current_row_base = start_row;
631: 		row_t row_data[STANDARD_VECTOR_SIZE];
632: 		Vector row_identifiers(LOGICAL_ROW_TYPE, (data_ptr_t)row_data);
633: 		ScanTableSegment(start_row, count, [&](DataChunk &chunk) {
634: 			for (idx_t i = 0; i < chunk.size(); i++) {
635: 				row_data[i] = current_row_base + i;
636: 			}
637: 			info->indexes.Scan([&](Index &index) {
638: 				index.Delete(chunk, row_identifiers);
639: 				return false;
640: 			});
641: 			current_row_base += chunk.size();
642: 		});
643: 	}
644: 	RevertAppendInternal(start_row, count);
645: }
646: 
647: //===--------------------------------------------------------------------===//
648: // Indexes
649: //===--------------------------------------------------------------------===//
650: bool DataTable::AppendToIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
651: 	D_ASSERT(is_root);
652: 	if (info->indexes.Empty()) {
653: 		return true;
654: 	}
655: 	// first generate the vector of row identifiers
656: 	Vector row_identifiers(LOGICAL_ROW_TYPE);
657: 	VectorOperations::GenerateSequence(row_identifiers, chunk.size(), row_start, 1);
658: 
659: 	vector<Index *> already_appended;
660: 	bool append_failed = false;
661: 	// now append the entries to the indices
662: 	info->indexes.Scan([&](Index &index) {
663: 		if (!index.Append(chunk, row_identifiers)) {
664: 			append_failed = true;
665: 			return true;
666: 		}
667: 		already_appended.push_back(&index);
668: 		return false;
669: 	});
670: 
671: 	if (append_failed) {
672: 		// constraint violation!
673: 		// remove any appended entries from previous indexes (if any)
674: 
675: 		for (auto *index : already_appended) {
676: 			index->Delete(chunk, row_identifiers);
677: 		}
678: 
679: 		return false;
680: 	}
681: 	return true;
682: }
683: 
684: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
685: 	D_ASSERT(is_root);
686: 	if (info->indexes.Empty()) {
687: 		return;
688: 	}
689: 	// first generate the vector of row identifiers
690: 	Vector row_identifiers(LOGICAL_ROW_TYPE);
691: 	VectorOperations::GenerateSequence(row_identifiers, chunk.size(), row_start, 1);
692: 
693: 	// now remove the entries from the indices
694: 	RemoveFromIndexes(state, chunk, row_identifiers);
695: }
696: 
697: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, Vector &row_identifiers) {
698: 	D_ASSERT(is_root);
699: 	info->indexes.Scan([&](Index &index) {
700: 		index.Delete(chunk, row_identifiers);
701: 		return false;
702: 	});
703: }
704: 
705: void DataTable::RemoveFromIndexes(Vector &row_identifiers, idx_t count) {
706: 	D_ASSERT(is_root);
707: 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
708: 
709: 	// figure out which row_group to fetch from
710: 	auto row_group = (RowGroup *)row_groups->GetSegment(row_ids[0]);
711: 	auto row_group_vector_idx = (row_ids[0] - row_group->start) / STANDARD_VECTOR_SIZE;
712: 	auto base_row_id = row_group_vector_idx * STANDARD_VECTOR_SIZE + row_group->start;
713: 
714: 	// create a selection vector from the row_ids
715: 	SelectionVector sel(STANDARD_VECTOR_SIZE);
716: 	for (idx_t i = 0; i < count; i++) {
717: 		auto row_in_vector = row_ids[i] - base_row_id;
718: 		D_ASSERT(row_in_vector < STANDARD_VECTOR_SIZE);
719: 		sel.set_index(i, row_in_vector);
720: 	}
721: 
722: 	// now fetch the columns from that row_group
723: 	// FIXME: we do not need to fetch all columns, only the columns required by the indices!
724: 	TableScanState state;
725: 	state.max_row = total_rows;
726: 	for (idx_t i = 0; i < types.size(); i++) {
727: 		state.column_ids.push_back(i);
728: 	}
729: 	DataChunk result;
730: 	result.Initialize(types);
731: 
732: 	row_group->InitializeScanWithOffset(state.row_group_scan_state, row_group_vector_idx);
733: 	row_group->IndexScan(state.row_group_scan_state, result, false);
734: 	result.Slice(sel, count);
735: 
736: 	info->indexes.Scan([&](Index &index) {
737: 		index.Delete(result, row_identifiers);
738: 		return false;
739: 	});
740: }
741: 
742: //===--------------------------------------------------------------------===//
743: // Delete
744: //===--------------------------------------------------------------------===//
745: idx_t DataTable::Delete(TableCatalogEntry &table, ClientContext &context, Vector &row_identifiers, idx_t count) {
746: 	D_ASSERT(row_identifiers.GetType().InternalType() == ROW_TYPE);
747: 	if (count == 0) {
748: 		return 0;
749: 	}
750: 
751: 	auto &transaction = Transaction::GetTransaction(context);
752: 
753: 	row_identifiers.Normalify(count);
754: 	auto ids = FlatVector::GetData<row_t>(row_identifiers);
755: 	auto first_id = ids[0];
756: 
757: 	if (first_id >= MAX_ROW_ID) {
758: 		// deletion is in transaction-local storage: push delete into local chunk collection
759: 		return transaction.storage.Delete(this, row_identifiers, count);
760: 	} else {
761: 		idx_t delete_count = 0;
762: 		// delete is in the row groups
763: 		// we need to figure out for each id to which row group it belongs
764: 		// usually all (or many) ids belong to the same row group
765: 		// we iterate over the ids and check for every id if it belongs to the same row group as their predecessor
766: 		idx_t pos = 0;
767: 		do {
768: 			idx_t start = pos;
769: 			auto row_group = (RowGroup *)row_groups->GetSegment(ids[pos]);
770: 			for (pos++; pos < count; pos++) {
771: 				D_ASSERT(ids[pos] >= 0);
772: 				// check if this id still belongs to this row group
773: 				if (idx_t(ids[pos]) < row_group->start) {
774: 					// id is before row_group start -> it does not
775: 					break;
776: 				}
777: 				if (idx_t(ids[pos]) >= row_group->start + row_group->count) {
778: 					// id is after row group end -> it does not
779: 					break;
780: 				}
781: 			}
782: 			delete_count += row_group->Delete(transaction, this, ids + start, pos - start);
783: 		} while (pos < count);
784: 		return delete_count;
785: 	}
786: }
787: 
788: //===--------------------------------------------------------------------===//
789: // Update
790: //===--------------------------------------------------------------------===//
791: static void CreateMockChunk(vector<LogicalType> &types, const vector<column_t> &column_ids, DataChunk &chunk,
792:                             DataChunk &mock_chunk) {
793: 	// construct a mock DataChunk
794: 	mock_chunk.InitializeEmpty(types);
795: 	for (column_t i = 0; i < column_ids.size(); i++) {
796: 		mock_chunk.data[column_ids[i]].Reference(chunk.data[i]);
797: 	}
798: 	mock_chunk.SetCardinality(chunk.size());
799: }
800: 
801: static bool CreateMockChunk(TableCatalogEntry &table, const vector<column_t> &column_ids,
802:                             unordered_set<column_t> &desired_column_ids, DataChunk &chunk, DataChunk &mock_chunk) {
803: 	idx_t found_columns = 0;
804: 	// check whether the desired columns are present in the UPDATE clause
805: 	for (column_t i = 0; i < column_ids.size(); i++) {
806: 		if (desired_column_ids.find(column_ids[i]) != desired_column_ids.end()) {
807: 			found_columns++;
808: 		}
809: 	}
810: 	if (found_columns == 0) {
811: 		// no columns were found: no need to check the constraint again
812: 		return false;
813: 	}
814: 	if (found_columns != desired_column_ids.size()) {
815: 		// FIXME: not all columns in UPDATE clause are present!
816: 		// this should not be triggered at all as the binder should add these columns
817: 		throw InternalException("Not all columns required for the CHECK constraint are present in the UPDATED chunk!");
818: 	}
819: 	// construct a mock DataChunk
820: 	auto types = table.GetTypes();
821: 	CreateMockChunk(types, column_ids, chunk, mock_chunk);
822: 	return true;
823: }
824: 
825: void DataTable::VerifyUpdateConstraints(TableCatalogEntry &table, DataChunk &chunk,
826:                                         const vector<column_t> &column_ids) {
827: 	for (auto &constraint : table.bound_constraints) {
828: 		switch (constraint->type) {
829: 		case ConstraintType::NOT_NULL: {
830: 			auto &not_null = *reinterpret_cast<BoundNotNullConstraint *>(constraint.get());
831: 			// check if the constraint is in the list of column_ids
832: 			for (idx_t i = 0; i < column_ids.size(); i++) {
833: 				if (column_ids[i] == not_null.index) {
834: 					// found the column id: check the data in
835: 					VerifyNotNullConstraint(table, chunk.data[i], chunk.size(), table.columns[not_null.index].name);
836: 					break;
837: 				}
838: 			}
839: 			break;
840: 		}
841: 		case ConstraintType::CHECK: {
842: 			auto &check = *reinterpret_cast<BoundCheckConstraint *>(constraint.get());
843: 
844: 			DataChunk mock_chunk;
845: 			if (CreateMockChunk(table, column_ids, check.bound_columns, chunk, mock_chunk)) {
846: 				VerifyCheckConstraint(table, *check.expression, mock_chunk);
847: 			}
848: 			break;
849: 		}
850: 		case ConstraintType::UNIQUE:
851: 		case ConstraintType::FOREIGN_KEY:
852: 			break;
853: 		default:
854: 			throw NotImplementedException("Constraint type not implemented!");
855: 		}
856: 	}
857: 	// update should not be called for indexed columns!
858: 	// instead update should have been rewritten to delete + update on higher layer
859: #ifdef DEBUG
860: 	info->indexes.Scan([&](Index &index) {
861: 		D_ASSERT(!index.IndexIsUpdated(column_ids));
862: 		return false;
863: 	});
864: 
865: #endif
866: }
867: 
868: void DataTable::Update(TableCatalogEntry &table, ClientContext &context, Vector &row_ids,
869:                        const vector<column_t> &column_ids, DataChunk &updates) {
870: 	D_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);
871: 
872: 	auto count = updates.size();
873: 	updates.Verify();
874: 	if (count == 0) {
875: 		return;
876: 	}
877: 
878: 	if (!is_root) {
879: 		throw TransactionException("Transaction conflict: cannot update a table that has been altered!");
880: 	}
881: 
882: 	// first verify that no constraints are violated
883: 	VerifyUpdateConstraints(table, updates, column_ids);
884: 
885: 	// now perform the actual update
886: 	auto &transaction = Transaction::GetTransaction(context);
887: 
888: 	updates.Normalify();
889: 	row_ids.Normalify(count);
890: 	auto ids = FlatVector::GetData<row_t>(row_ids);
891: 	auto first_id = FlatVector::GetValue<row_t>(row_ids, 0);
892: 	if (first_id >= MAX_ROW_ID) {
893: 		// update is in transaction-local storage: push update into local storage
894: 		transaction.storage.Update(this, row_ids, column_ids, updates);
895: 		return;
896: 	}
897: 
898: 	// update is in the row groups
899: 	// we need to figure out for each id to which row group it belongs
900: 	// usually all (or many) ids belong to the same row group
901: 	// we iterate over the ids and check for every id if it belongs to the same row group as their predecessor
902: 	idx_t pos = 0;
903: 	do {
904: 		idx_t start = pos;
905: 		auto row_group = (RowGroup *)row_groups->GetSegment(ids[pos]);
906: 		row_t base_id =
907: 		    row_group->start + ((ids[pos] - row_group->start) / STANDARD_VECTOR_SIZE * STANDARD_VECTOR_SIZE);
908: 		for (pos++; pos < count; pos++) {
909: 			D_ASSERT(ids[pos] >= 0);
910: 			// check if this id still belongs to this vector
911: 			if (ids[pos] < base_id) {
912: 				// id is before vector start -> it does not
913: 				break;
914: 			}
915: 			if (ids[pos] >= base_id + STANDARD_VECTOR_SIZE) {
916: 				// id is after vector end -> it does not
917: 				break;
918: 			}
919: 		}
920: 		row_group->Update(transaction, updates, ids, start, pos - start, column_ids);
921: 
922: 		lock_guard<mutex> stats_guard(stats_lock);
923: 		for (idx_t i = 0; i < column_ids.size(); i++) {
924: 			auto column_id = column_ids[i];
925: 			column_stats[column_id]->Merge(*row_group->GetStatistics(column_id));
926: 		}
927: 	} while (pos < count);
928: }
929: 
930: void DataTable::UpdateColumn(TableCatalogEntry &table, ClientContext &context, Vector &row_ids,
931:                              const vector<column_t> &column_path, DataChunk &updates) {
932: 	D_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);
933: 	D_ASSERT(updates.ColumnCount() == 1);
934: 	updates.Verify();
935: 	if (updates.size() == 0) {
936: 		return;
937: 	}
938: 
939: 	if (!is_root) {
940: 		throw TransactionException("Transaction conflict: cannot update a table that has been altered!");
941: 	}
942: 
943: 	// now perform the actual update
944: 	auto &transaction = Transaction::GetTransaction(context);
945: 
946: 	updates.Normalify();
947: 	row_ids.Normalify(updates.size());
948: 	auto first_id = FlatVector::GetValue<row_t>(row_ids, 0);
949: 	if (first_id >= MAX_ROW_ID) {
950: 		throw NotImplementedException("Cannot update a column-path on transaction local data");
951: 	}
952: 	// find the row_group this id belongs to
953: 	auto primary_column_idx = column_path[0];
954: 	auto row_group = (RowGroup *)row_groups->GetSegment(first_id);
955: 	row_group->UpdateColumn(transaction, updates, row_ids, column_path);
956: 
957: 	lock_guard<mutex> stats_guard(stats_lock);
958: 	column_stats[primary_column_idx]->Merge(*row_group->GetStatistics(primary_column_idx));
959: }
960: 
961: //===--------------------------------------------------------------------===//
962: // Create Index Scan
963: //===--------------------------------------------------------------------===//
964: void DataTable::InitializeCreateIndexScan(CreateIndexScanState &state, const vector<column_t> &column_ids) {
965: 	// we grab the append lock to make sure nothing is appended until AFTER we finish the index scan
966: 	state.append_lock = std::unique_lock<mutex>(append_lock);
967: 	state.delete_lock = std::unique_lock<mutex>(row_groups->node_lock);
968: 
969: 	InitializeScan(state, column_ids);
970: }
971: 
972: void DataTable::CreateIndexScan(CreateIndexScanState &state, const vector<column_t> &column_ids, DataChunk &result,
973:                                 bool allow_pending_updates) {
974: 	// scan the persistent segments
975: 	if (ScanCreateIndex(state, result, allow_pending_updates)) {
976: 		return;
977: 	}
978: }
979: 
980: bool DataTable::ScanCreateIndex(CreateIndexScanState &state, DataChunk &result, bool allow_pending_updates) {
981: 	auto current_row_group = state.row_group_scan_state.row_group;
982: 	while (current_row_group) {
983: 		current_row_group->IndexScan(state.row_group_scan_state, result, allow_pending_updates);
984: 		if (result.size() > 0) {
985: 			return true;
986: 		} else {
987: 			current_row_group = state.row_group_scan_state.row_group = (RowGroup *)current_row_group->next.get();
988: 			if (current_row_group) {
989: 				current_row_group->InitializeScan(state.row_group_scan_state);
990: 			}
991: 		}
992: 	}
993: 	return false;
994: }
995: 
996: void DataTable::AddIndex(unique_ptr<Index> index, const vector<unique_ptr<Expression>> &expressions) {
997: 	DataChunk result;
998: 	result.Initialize(index->logical_types);
999: 
1000: 	DataChunk intermediate;
1001: 	vector<LogicalType> intermediate_types;
1002: 	auto column_ids = index->column_ids;
1003: 	column_ids.push_back(COLUMN_IDENTIFIER_ROW_ID);
1004: 	for (auto &id : index->column_ids) {
1005: 		intermediate_types.push_back(types[id]);
1006: 	}
1007: 	intermediate_types.push_back(LOGICAL_ROW_TYPE);
1008: 	intermediate.Initialize(intermediate_types);
1009: 
1010: 	// initialize an index scan
1011: 	CreateIndexScanState state;
1012: 	InitializeCreateIndexScan(state, column_ids);
1013: 
1014: 	if (!is_root) {
1015: 		throw TransactionException("Transaction conflict: cannot add an index to a table that has been altered!");
1016: 	}
1017: 
1018: 	// now start incrementally building the index
1019: 	{
1020: 		IndexLock lock;
1021: 		index->InitializeLock(lock);
1022: 		ExpressionExecutor executor(expressions);
1023: 		while (true) {
1024: 			intermediate.Reset();
1025: 			// scan a new chunk from the table to index
1026: 			CreateIndexScan(state, column_ids, intermediate);
1027: 			if (intermediate.size() == 0) {
1028: 				// finished scanning for index creation
1029: 				// release all locks
1030: 				break;
1031: 			}
1032: 			// resolve the expressions for this chunk
1033: 			executor.Execute(intermediate, result);
1034: 
1035: 			// insert into the index
1036: 			if (!index->Insert(lock, result, intermediate.data[intermediate.ColumnCount() - 1])) {
1037: 				throw ConstraintException(
1038: 				    "Cant create unique index, table contains duplicate data on indexed column(s)");
1039: 			}
1040: 		}
1041: 	}
1042: 	info->indexes.AddIndex(move(index));
1043: }
1044: 
1045: unique_ptr<BaseStatistics> DataTable::GetStatistics(ClientContext &context, column_t column_id) {
1046: 	if (column_id == COLUMN_IDENTIFIER_ROW_ID) {
1047: 		return nullptr;
1048: 	}
1049: 	lock_guard<mutex> stats_guard(stats_lock);
1050: 	return column_stats[column_id]->Copy();
1051: }
1052: 
1053: //===--------------------------------------------------------------------===//
1054: // Checkpoint
1055: //===--------------------------------------------------------------------===//
1056: BlockPointer DataTable::Checkpoint(TableDataWriter &writer) {
1057: 	// checkpoint each individual row group
1058: 	// FIXME: we might want to combine adjacent row groups in case they have had deletions...
1059: 	vector<unique_ptr<BaseStatistics>> global_stats;
1060: 	for (idx_t i = 0; i < types.size(); i++) {
1061: 		global_stats.push_back(BaseStatistics::CreateEmpty(types[i]));
1062: 	}
1063: 
1064: 	auto row_group = (RowGroup *)row_groups->GetRootSegment();
1065: 	vector<RowGroupPointer> row_group_pointers;
1066: 	while (row_group) {
1067: 		auto pointer = row_group->Checkpoint(writer, global_stats);
1068: 		row_group_pointers.push_back(move(pointer));
1069: 		row_group = (RowGroup *)row_group->next.get();
1070: 	}
1071: 	// store the current position in the metadata writer
1072: 	// this is where the row groups for this table start
1073: 	auto &meta_writer = writer.GetMetaWriter();
1074: 	auto pointer = meta_writer.GetBlockPointer();
1075: 
1076: 	for (auto &stats : global_stats) {
1077: 		stats->Serialize(meta_writer);
1078: 	}
1079: 	// now start writing the row group pointers to disk
1080: 	meta_writer.Write<uint64_t>(row_group_pointers.size());
1081: 	for (auto &row_group_pointer : row_group_pointers) {
1082: 		RowGroup::Serialize(row_group_pointer, meta_writer);
1083: 	}
1084: 	return pointer;
1085: }
1086: 
1087: void DataTable::CommitDropColumn(idx_t index) {
1088: 	auto segment = (RowGroup *)row_groups->GetRootSegment();
1089: 	while (segment) {
1090: 		segment->CommitDropColumn(index);
1091: 		segment = (RowGroup *)segment->next.get();
1092: 	}
1093: }
1094: 
1095: idx_t DataTable::GetTotalRows() {
1096: 	return total_rows;
1097: }
1098: 
1099: void DataTable::CommitDropTable() {
1100: 	// commit a drop of this table: mark all blocks as modified so they can be reclaimed later on
1101: 	auto segment = (RowGroup *)row_groups->GetRootSegment();
1102: 	while (segment) {
1103: 		segment->CommitDrop();
1104: 		segment = (RowGroup *)segment->next.get();
1105: 	}
1106: }
1107: 
1108: //===--------------------------------------------------------------------===//
1109: // GetStorageInfo
1110: //===--------------------------------------------------------------------===//
1111: vector<vector<Value>> DataTable::GetStorageInfo() {
1112: 	vector<vector<Value>> result;
1113: 
1114: 	auto row_group = (RowGroup *)row_groups->GetRootSegment();
1115: 	idx_t row_group_index = 0;
1116: 	while (row_group) {
1117: 		row_group->GetStorageInfo(row_group_index, result);
1118: 		row_group_index++;
1119: 
1120: 		row_group = (RowGroup *)row_group->next.get();
1121: 	}
1122: 
1123: 	return result;
1124: }
1125: 
1126: } // namespace duckdb
[end of src/storage/data_table.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: