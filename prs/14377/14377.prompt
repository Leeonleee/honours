You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Can not cast field type to Int64 when querying nullable Int64 fields from a Pandas.DataFrame
### What happens?

As indicated in the title and attachment, even when using the cast function, it is impossible to maintain the cc_id in the dataframe returned by duckdb.query as Int64; the cc_id has been converted to a float64 type.
![image](https://github.com/user-attachments/assets/70ca1382-f7c2-4c7b-9510-c436e947ed3d)


### To Reproduce

```
import pandas as pd
import duckdb

df = pd.DataFrame({
    'cc_id': pd.Series([1844743171901218817, 1844691776961568771, None, None], dtype='Int64'),
    'other_column': [10, 20, 30, 40]
})

result = duckdb.query("SELECT CAST(cc_id AS Int64) AS cc_id, other_column FROM df WHERE other_column > 10").df()
result['cc_id']
```

### OS:

macOS 12.7.6

### DuckDB Version:

0.10.2, 1.1.1

### DuckDB Client:

Python

### Hardware:

_No response_

### Full Name:

Kafka Liu

### Affiliation:

Freelancer

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [x] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [x] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://duckdb.org/docs/installation/) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/include/duckdb/common/types/validity_mask.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/types/validity_mask.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/to_string.hpp"
13: #include "duckdb/common/types.hpp"
14: #include "duckdb/common/vector_size.hpp"
15: 
16: namespace duckdb {
17: struct SelectionVector;
18: struct ValidityMask;
19: 
20: template <typename V>
21: struct TemplatedValidityData {
22: 	static constexpr const idx_t BITS_PER_VALUE = sizeof(V) * 8;
23: 	static constexpr const V MAX_ENTRY = V(~V(0));
24: 
25: public:
26: 	inline explicit TemplatedValidityData(idx_t count) {
27: 		auto entry_count = EntryCount(count);
28: 		owned_data = make_unsafe_uniq_array_uninitialized<V>(entry_count);
29: 		for (idx_t entry_idx = 0; entry_idx < entry_count; entry_idx++) {
30: 			owned_data[entry_idx] = MAX_ENTRY;
31: 		}
32: 	}
33: 	inline TemplatedValidityData(const V *validity_mask, idx_t count) {
34: 		D_ASSERT(validity_mask);
35: 		auto entry_count = EntryCount(count);
36: 		owned_data = make_unsafe_uniq_array_uninitialized<V>(entry_count);
37: 		for (idx_t entry_idx = 0; entry_idx < entry_count; entry_idx++) {
38: 			owned_data[entry_idx] = validity_mask[entry_idx];
39: 		}
40: 	}
41: 
42: 	unsafe_unique_array<V> owned_data;
43: 
44: public:
45: 	static inline idx_t EntryCount(idx_t count) {
46: 		return (count + (BITS_PER_VALUE - 1)) / BITS_PER_VALUE;
47: 	}
48: };
49: 
50: using validity_t = uint64_t;
51: 
52: struct ValidityData : TemplatedValidityData<validity_t> {
53: public:
54: 	DUCKDB_API explicit ValidityData(idx_t count);
55: 	DUCKDB_API ValidityData(const ValidityMask &original, idx_t count);
56: };
57: 
58: //! Type used for validity masks
59: template <typename V>
60: struct TemplatedValidityMask {
61: 	using ValidityBuffer = TemplatedValidityData<V>;
62: 
63: public:
64: 	static constexpr const idx_t BITS_PER_VALUE = ValidityBuffer::BITS_PER_VALUE;
65: 	static constexpr const idx_t STANDARD_ENTRY_COUNT = (STANDARD_VECTOR_SIZE + (BITS_PER_VALUE - 1)) / BITS_PER_VALUE;
66: 	static constexpr const idx_t STANDARD_MASK_SIZE = STANDARD_ENTRY_COUNT * sizeof(validity_t);
67: 
68: public:
69: 	inline TemplatedValidityMask() : validity_mask(nullptr), target_count(STANDARD_VECTOR_SIZE) {
70: 	}
71: 	inline explicit TemplatedValidityMask(idx_t target_count) : validity_mask(nullptr), target_count(target_count) {
72: 	}
73: 	inline explicit TemplatedValidityMask(V *ptr) : validity_mask(ptr), target_count(STANDARD_VECTOR_SIZE) {
74: 	}
75: 	inline TemplatedValidityMask(const TemplatedValidityMask &original, idx_t count) {
76: 		Copy(original, count);
77: 	}
78: 
79: 	static inline idx_t ValidityMaskSize(idx_t count = STANDARD_VECTOR_SIZE) {
80: 		return ValidityBuffer::EntryCount(count) * sizeof(V);
81: 	}
82: 	inline bool AllValid() const {
83: 		return !validity_mask;
84: 	}
85: 	inline bool CheckAllValid(idx_t count) const {
86: 		return CountValid(count) == count;
87: 	}
88: 
89: 	inline bool CheckAllValid(idx_t to, idx_t from) const {
90: 		if (AllValid()) {
91: 			return true;
92: 		}
93: 		for (idx_t i = from; i < to; i++) {
94: 			if (!RowIsValid(i)) {
95: 				return false;
96: 			}
97: 		}
98: 		return true;
99: 	}
100: 
101: 	idx_t CountValid(const idx_t count) const {
102: 		if (AllValid() || count == 0) {
103: 			return count;
104: 		}
105: 
106: 		idx_t valid = 0;
107: 		const auto entry_count = EntryCount(count);
108: 		for (idx_t entry_idx = 0; entry_idx < entry_count;) {
109: 			auto entry = GetValidityEntry(entry_idx++);
110: 			// Handle ragged end (if not exactly multiple of BITS_PER_VALUE)
111: 			if (entry_idx == entry_count && count % BITS_PER_VALUE != 0) {
112: 				idx_t idx_in_entry;
113: 				GetEntryIndex(count, entry_idx, idx_in_entry);
114: 				for (idx_t i = 0; i < idx_in_entry; ++i) {
115: 					valid += idx_t(RowIsValid(entry, i));
116: 				}
117: 				break;
118: 			}
119: 
120: 			// Handle all set
121: 			if (AllValid(entry)) {
122: 				valid += BITS_PER_VALUE;
123: 				continue;
124: 			}
125: 
126: 			// Count partial entry (Kernighan's algorithm)
127: 			while (entry) {
128: 				entry &= (entry - 1);
129: 				++valid;
130: 			}
131: 		}
132: 
133: 		return valid;
134: 	}
135: 
136: 	inline V *GetData() const {
137: 		return validity_mask;
138: 	}
139: 	inline void Reset(idx_t target_count_p = STANDARD_VECTOR_SIZE) {
140: 		validity_mask = nullptr;
141: 		validity_data.reset();
142: 		target_count = target_count_p;
143: 	}
144: 
145: 	static inline idx_t EntryCount(idx_t count) {
146: 		return ValidityBuffer::EntryCount(count);
147: 	}
148: 	inline V GetValidityEntry(idx_t entry_idx) const {
149: 		if (!validity_mask) {
150: 			return ValidityBuffer::MAX_ENTRY;
151: 		}
152: 		return GetValidityEntryUnsafe(entry_idx);
153: 	}
154: 	inline V &GetValidityEntryUnsafe(idx_t entry_idx) const {
155: 		return validity_mask[entry_idx];
156: 	}
157: 	static inline bool AllValid(V entry) {
158: 		return entry == ValidityBuffer::MAX_ENTRY;
159: 	}
160: 	static inline bool NoneValid(V entry) {
161: 		return entry == 0;
162: 	}
163: 	static inline bool RowIsValid(const V &entry, const idx_t &idx_in_entry) {
164: 		return entry & (V(1) << V(idx_in_entry));
165: 	}
166: 	static inline void GetEntryIndex(idx_t row_idx, idx_t &entry_idx, idx_t &idx_in_entry) {
167: 		entry_idx = row_idx / BITS_PER_VALUE;
168: 		idx_in_entry = row_idx % BITS_PER_VALUE;
169: 	}
170: 	//! Get an entry that has first-n bits set as valid and rest set as invalid
171: 	static inline V EntryWithValidBits(idx_t n) {
172: 		if (n == 0) {
173: 			return V(0);
174: 		}
175: 		return ValidityBuffer::MAX_ENTRY >> (BITS_PER_VALUE - n);
176: 	}
177: 	static inline idx_t SizeInBytes(idx_t n) {
178: 		return (n + BITS_PER_VALUE - 1) / BITS_PER_VALUE;
179: 	}
180: 
181: 	//! RowIsValidUnsafe should only be used if AllValid() is false: it achieves the same as RowIsValid but skips a
182: 	//! not-null check
183: 	inline bool RowIsValidUnsafe(idx_t row_idx) const {
184: 		D_ASSERT(validity_mask);
185: 		idx_t entry_idx, idx_in_entry;
186: 		GetEntryIndex(row_idx, entry_idx, idx_in_entry);
187: 		auto entry = GetValidityEntry(entry_idx);
188: 		return RowIsValid(entry, idx_in_entry);
189: 	}
190: 
191: 	//! Returns true if a row is valid (i.e. not null), false otherwise
192: 	inline bool RowIsValid(idx_t row_idx) const {
193: 		if (!validity_mask) {
194: 			return true;
195: 		}
196: 		return RowIsValidUnsafe(row_idx);
197: 	}
198: 
199: 	//! Same as SetValid, but skips a null check on validity_mask
200: 	inline void SetValidUnsafe(idx_t row_idx) {
201: 		D_ASSERT(validity_mask);
202: 		idx_t entry_idx, idx_in_entry;
203: 		GetEntryIndex(row_idx, entry_idx, idx_in_entry);
204: 		validity_mask[entry_idx] |= (V(1) << V(idx_in_entry));
205: 	}
206: 
207: 	//! Marks the entry at the specified row index as valid (i.e. not-null)
208: 	inline void SetValid(idx_t row_idx) {
209: 		if (!validity_mask) {
210: 			// if AllValid() we don't need to do anything
211: 			// the row is already valid
212: 			return;
213: 		}
214: 		SetValidUnsafe(row_idx);
215: 	}
216: 
217: 	//! Marks the bit at the specified entry as invalid (i.e. null)
218: 	inline void SetInvalidUnsafe(idx_t entry_idx, idx_t idx_in_entry) {
219: 		D_ASSERT(validity_mask);
220: 		validity_mask[entry_idx] &= ~(V(1) << V(idx_in_entry));
221: 	}
222: 
223: 	//! Marks the bit at the specified row index as invalid (i.e. null)
224: 	inline void SetInvalidUnsafe(idx_t row_idx) {
225: 		idx_t entry_idx, idx_in_entry;
226: 		GetEntryIndex(row_idx, entry_idx, idx_in_entry);
227: 		SetInvalidUnsafe(entry_idx, idx_in_entry);
228: 	}
229: 
230: 	//! Marks the entry at the specified row index as invalid (i.e. null)
231: 	inline void SetInvalid(idx_t row_idx) {
232: 		if (!validity_mask) {
233: 			D_ASSERT(row_idx <= target_count);
234: 			Initialize(target_count);
235: 		}
236: 		SetInvalidUnsafe(row_idx);
237: 	}
238: 
239: 	//! Mark the entry at the specified index as either valid or invalid (non-null or null)
240: 	inline void Set(idx_t row_idx, bool valid) {
241: 		if (valid) {
242: 			SetValid(row_idx);
243: 		} else {
244: 			SetInvalid(row_idx);
245: 		}
246: 	}
247: 
248: 	//! Ensure the validity mask is writable, allocating space if it is not initialized
249: 	inline void EnsureWritable() {
250: 		if (!validity_mask) {
251: 			Initialize();
252: 		}
253: 	}
254: 
255: 	//! Marks exactly "count" bits in the validity mask as invalid (null)
256: 	inline void SetAllInvalid(idx_t count) {
257: 		EnsureWritable();
258: 		if (count == 0) {
259: 			return;
260: 		}
261: 		auto last_entry_index = ValidityBuffer::EntryCount(count) - 1;
262: 		for (idx_t i = 0; i < last_entry_index; i++) {
263: 			validity_mask[i] = 0;
264: 		}
265: 		auto last_entry_bits = count % BITS_PER_VALUE;
266: 		validity_mask[last_entry_index] =
267: 		    (last_entry_bits == 0) ? 0 : static_cast<V>(ValidityBuffer::MAX_ENTRY << (last_entry_bits));
268: 	}
269: 
270: 	//! Marks exactly "count" bits in the validity mask as valid (not null)
271: 	inline void SetAllValid(idx_t count) {
272: 		EnsureWritable();
273: 		if (count == 0) {
274: 			return;
275: 		}
276: 		auto last_entry_index = ValidityBuffer::EntryCount(count) - 1;
277: 		for (idx_t i = 0; i < last_entry_index; i++) {
278: 			validity_mask[i] = ValidityBuffer::MAX_ENTRY;
279: 		}
280: 		auto last_entry_bits = count % BITS_PER_VALUE;
281: 		validity_mask[last_entry_index] |= (last_entry_bits == 0)
282: 		                                       ? ValidityBuffer::MAX_ENTRY
283: 		                                       : ~static_cast<V>(ValidityBuffer::MAX_ENTRY << (last_entry_bits));
284: 	}
285: 
286: 	inline bool IsMaskSet() const {
287: 		if (validity_mask) {
288: 			return true;
289: 		}
290: 		return false;
291: 	}
292: 
293: public:
294: 	inline void Initialize(validity_t *validity) {
295: 		validity_data.reset();
296: 		validity_mask = validity;
297: 	}
298: 	inline void Initialize(const TemplatedValidityMask &other) {
299: 		validity_mask = other.validity_mask;
300: 		validity_data = other.validity_data;
301: 		target_count = other.target_count;
302: 	}
303: 	inline void Initialize(idx_t count) {
304: 		target_count = count;
305: 		validity_data = make_buffer<ValidityBuffer>(count);
306: 		validity_mask = validity_data->owned_data.get();
307: 	}
308: 	inline void Initialize() {
309: 		Initialize(target_count);
310: 	}
311: 	inline void Copy(const TemplatedValidityMask &other, idx_t count) {
312: 		target_count = count;
313: 		if (other.AllValid()) {
314: 			validity_data = nullptr;
315: 			validity_mask = nullptr;
316: 		} else {
317: 			validity_data = make_buffer<ValidityBuffer>(other.validity_mask, count);
318: 			validity_mask = validity_data->owned_data.get();
319: 		}
320: 	}
321: 
322: protected:
323: 	V *validity_mask;
324: 	buffer_ptr<ValidityBuffer> validity_data;
325: 	// The size to initialize the validity mask to when/if the mask is lazily initialized
326: 	idx_t target_count;
327: };
328: 
329: struct ValidityMask : public TemplatedValidityMask<validity_t> {
330: public:
331: 	inline ValidityMask() : TemplatedValidityMask(nullptr) {
332: 	}
333: 	inline explicit ValidityMask(idx_t target_count) : TemplatedValidityMask(target_count) {
334: 	}
335: 	inline explicit ValidityMask(validity_t *ptr) : TemplatedValidityMask(ptr) {
336: 	}
337: 	inline ValidityMask(const ValidityMask &original, idx_t count) : TemplatedValidityMask(original, count) {
338: 	}
339: 
340: public:
341: 	DUCKDB_API void Resize(idx_t old_size, idx_t new_size);
342: 	DUCKDB_API idx_t TargetCount() const;
343: 	DUCKDB_API void SliceInPlace(const ValidityMask &other, idx_t target_offset, idx_t source_offset, idx_t count);
344: 	DUCKDB_API void Slice(const ValidityMask &other, idx_t source_offset, idx_t count);
345: 	DUCKDB_API void CopySel(const ValidityMask &other, const SelectionVector &sel, idx_t source_offset,
346: 	                        idx_t target_offset, idx_t count);
347: 	DUCKDB_API void Combine(const ValidityMask &other, idx_t count);
348: 	DUCKDB_API string ToString(idx_t count) const;
349: 
350: 	DUCKDB_API static bool IsAligned(idx_t count);
351: 
352: 	void Write(WriteStream &writer, idx_t count);
353: 	void Read(ReadStream &reader, idx_t count);
354: };
355: 
356: //===--------------------------------------------------------------------===//
357: // ValidityArray
358: //===--------------------------------------------------------------------===//
359: struct ValidityArray {
360: 	inline ValidityArray() {
361: 	}
362: 
363: 	inline bool AllValid() const {
364: 		return !validity_mask;
365: 	}
366: 
367: 	inline void Initialize(idx_t count, bool initial = true) {
368: 		target_count = count;
369: 		validity_data = make_unsafe_uniq_array<bool>(count);
370: 		validity_mask = validity_data.get();
371: 		memset(validity_mask, initial, sizeof(bool) * count);
372: 	}
373: 
374: 	//! RowIsValidUnsafe should only be used if AllValid() is false: it achieves the same as RowIsValid but skips a
375: 	//! not-null check
376: 	inline bool RowIsValidUnsafe(idx_t row_idx) const {
377: 		D_ASSERT(validity_mask);
378: 		return validity_mask[row_idx];
379: 	}
380: 
381: 	//! Returns true if a row is valid (i.e. not null), false otherwise
382: 	inline bool RowIsValid(idx_t row_idx) const {
383: 		if (!validity_mask) {
384: 			return true;
385: 		}
386: 		return RowIsValidUnsafe(row_idx);
387: 	}
388: 
389: 	//! Same as SetValid, but skips a null check on validity_mask
390: 	inline void SetValidUnsafe(idx_t row_idx) {
391: 		D_ASSERT(validity_mask);
392: 		validity_mask[row_idx] = true;
393: 	}
394: 
395: 	//! Marks the entry at the specified row index as valid (i.e. not-null)
396: 	inline void SetValid(idx_t row_idx) {
397: 		if (!validity_mask) {
398: 			// if AllValid() we don't need to do anything
399: 			// the row is already valid
400: 			return;
401: 		}
402: 
403: 		SetValidUnsafe(row_idx);
404: 	}
405: 
406: 	inline void Pack(ValidityMask &mask, const idx_t count) const {
407: 		if (AllValid()) {
408: 			mask.Reset();
409: 			return;
410: 		}
411: 		mask.Initialize(count);
412: 
413: 		const auto entire_entries = count / ValidityMask::BITS_PER_VALUE;
414: 		const auto ragged = count % ValidityMask::BITS_PER_VALUE;
415: 		auto bits = mask.GetData();
416: 		idx_t row_idx = 0;
417: 		for (idx_t i = 0; i < entire_entries; ++i) {
418: 			validity_t entry = 0;
419: 			for (idx_t j = 0; j < ValidityMask::BITS_PER_VALUE; ++j) {
420: 				if (RowIsValidUnsafe(row_idx++)) {
421: 					entry |= validity_t(1) << j;
422: 				}
423: 			}
424: 			*bits++ = entry;
425: 		}
426: 		if (ragged) {
427: 			validity_t entry = 0;
428: 			for (idx_t j = 0; j < ragged; ++j) {
429: 				if (RowIsValidUnsafe(row_idx++)) {
430: 					entry |= validity_t(1) << j;
431: 				}
432: 			}
433: 			*bits++ = entry;
434: 		}
435: 	}
436: 
437: 	bool *validity_mask = nullptr;
438: 	unsafe_unique_array<bool> validity_data;
439: 	idx_t target_count = 0;
440: };
441: 
442: } // namespace duckdb
[end of src/include/duckdb/common/types/validity_mask.hpp]
[start of tools/pythonpkg/scripts/cache_data.json]
1: {
2:     "pyarrow": {
3:         "type": "module",
4:         "full_path": "pyarrow",
5:         "name": "pyarrow",
6:         "children": [
7:             "pyarrow.dataset",
8:             "pyarrow.Table",
9:             "pyarrow.RecordBatchReader"
10:         ]
11:     },
12:     "pyarrow.dataset": {
13:         "type": "module",
14:         "full_path": "pyarrow.dataset",
15:         "name": "dataset",
16:         "children": [
17:             "pyarrow.dataset.Scanner",
18:             "pyarrow.dataset.Dataset"
19:         ]
20:     },
21:     "pyarrow.dataset.Scanner": {
22:         "type": "attribute",
23:         "full_path": "pyarrow.dataset.Scanner",
24:         "name": "Scanner",
25:         "children": []
26:     },
27:     "pyarrow.dataset.Dataset": {
28:         "type": "attribute",
29:         "full_path": "pyarrow.dataset.Dataset",
30:         "name": "Dataset",
31:         "children": []
32:     },
33:     "pyarrow.Table": {
34:         "type": "attribute",
35:         "full_path": "pyarrow.Table",
36:         "name": "Table",
37:         "children": []
38:     },
39:     "pyarrow.RecordBatchReader": {
40:         "type": "attribute",
41:         "full_path": "pyarrow.RecordBatchReader",
42:         "name": "RecordBatchReader",
43:         "children": []
44:     },
45:     "pandas": {
46:         "type": "module",
47:         "full_path": "pandas",
48:         "name": "pandas",
49:         "children": [
50:             "pandas.DataFrame",
51:             "pandas.isnull",
52:             "pandas.ArrowDtype",
53:             "pandas.NaT",
54:             "pandas.NA"
55:         ],
56:         "required": false
57:     },
58:     "pandas.DataFrame": {
59:         "type": "attribute",
60:         "full_path": "pandas.DataFrame",
61:         "name": "DataFrame",
62:         "children": []
63:     },
64:     "pandas.NaT": {
65:         "type": "attribute",
66:         "full_path": "pandas.NaT",
67:         "name": "NaT",
68:         "children": []
69:     },
70:     "pandas.NA": {
71:         "type": "attribute",
72:         "full_path": "pandas.NA",
73:         "name": "NA",
74:         "children": []
75:     },
76:     "pandas.isnull": {
77:         "type": "attribute",
78:         "full_path": "pandas.isnull",
79:         "name": "isnull",
80:         "children": []
81:     },
82:     "pandas.ArrowDtype": {
83:         "type": "attribute",
84:         "full_path": "pandas.ArrowDtype",
85:         "name": "ArrowDtype",
86:         "children": []
87:     },
88:     "datetime": {
89:         "type": "module",
90:         "full_path": "datetime",
91:         "name": "datetime",
92:         "children": [
93:             "datetime.date",
94:             "datetime.time",
95:             "datetime.timedelta",
96:             "datetime.timezone",
97:             "datetime.datetime"
98:         ]
99:     },
100:     "datetime.date": {
101:         "type": "attribute",
102:         "full_path": "datetime.date",
103:         "name": "date",
104:         "children": [
105:             "datetime.date.max",
106:             "datetime.date.min"
107:         ]
108:     },
109:     "datetime.date.max": {
110:         "type": "attribute",
111:         "full_path": "datetime.date.max",
112:         "name": "max",
113:         "children": []
114:     },
115:     "datetime.date.min": {
116:         "type": "attribute",
117:         "full_path": "datetime.date.min",
118:         "name": "min",
119:         "children": []
120:     },
121:     "datetime.time": {
122:         "type": "attribute",
123:         "full_path": "datetime.time",
124:         "name": "time",
125:         "children": []
126:     },
127:     "datetime.timedelta": {
128:         "type": "attribute",
129:         "full_path": "datetime.timedelta",
130:         "name": "timedelta",
131:         "children": []
132:     },
133:     "datetime.datetime": {
134:         "type": "attribute",
135:         "full_path": "datetime.datetime",
136:         "name": "datetime",
137:         "children": [
138:             "datetime.datetime.min",
139:             "datetime.datetime.max",
140:             "datetime.datetime.combine"
141:         ]
142:     },
143:     "datetime.datetime.min": {
144:         "type": "attribute",
145:         "full_path": "datetime.datetime.min",
146:         "name": "min",
147:         "children": []
148:     },
149:     "datetime.datetime.max": {
150:         "type": "attribute",
151:         "full_path": "datetime.datetime.max",
152:         "name": "max",
153:         "children": []
154:     },
155:     "datetime.datetime.combine": {
156:         "type": "attribute",
157:         "full_path": "datetime.datetime.combine",
158:         "name": "combine",
159:         "children": []
160:     },
161:     "datetime.timezone": {
162:         "type": "attribute",
163:         "full_path": "datetime.timezone",
164:         "name": "timezone",
165:         "children": []
166:     },
167:     "decimal": {
168:         "type": "module",
169:         "full_path": "decimal",
170:         "name": "decimal",
171:         "children": [
172:             "decimal.Decimal"
173:         ]
174:     },
175:     "decimal.Decimal": {
176:         "type": "attribute",
177:         "full_path": "decimal.Decimal",
178:         "name": "Decimal",
179:         "children": []
180:     },
181:     "IPython": {
182:         "type": "module",
183:         "full_path": "IPython",
184:         "name": "IPython",
185:         "children": [
186:             "IPython.get_ipython",
187:             "IPython.display"
188:         ],
189:         "required": false
190:     },
191:     "IPython.get_ipython": {
192:         "type": "attribute",
193:         "full_path": "IPython.get_ipython",
194:         "name": "get_ipython",
195:         "children": []
196:     },
197:     "IPython.display": {
198:         "type": "attribute",
199:         "full_path": "IPython.display",
200:         "name": "display",
201:         "children": [
202:             "IPython.display.display",
203:             "IPython.display.HTML"
204:         ]
205:     },
206:     "IPython.display.display": {
207:         "type": "attribute",
208:         "full_path": "IPython.display.display",
209:         "name": "display",
210:         "children": []
211:     },
212:     "IPython.display.HTML": {
213:         "type": "attribute",
214:         "full_path": "IPython.display.HTML",
215:         "name": "HTML",
216:         "children": []
217:     },
218:     "ipywidgets": {
219:         "type": "module",
220:         "full_path": "ipywidgets",
221:         "name": "ipywidgets",
222:         "children": [
223:             "ipywidgets.FloatProgress"
224:         ],
225:         "required": false
226:     },
227:     "ipywidgets.FloatProgress": {
228:         "type": "attribute",
229:         "full_path": "ipywidgets.FloatProgress",
230:         "name": "FloatProgress",
231:         "children": []
232:     },
233:     "numpy": {
234:         "type": "module",
235:         "full_path": "numpy",
236:         "name": "numpy",
237:         "children": [
238:             "numpy.core",
239:             "numpy.ma",
240:             "numpy.ndarray",
241:             "numpy.datetime64",
242:             "numpy.generic",
243:             "numpy.int64",
244:             "numpy.bool_",
245:             "numpy.byte",
246:             "numpy.ubyte",
247:             "numpy.short",
248:             "numpy.ushort",
249:             "numpy.intc",
250:             "numpy.uintc",
251:             "numpy.int_",
252:             "numpy.uint",
253:             "numpy.longlong",
254:             "numpy.ulonglong",
255:             "numpy.half",
256:             "numpy.float16",
257:             "numpy.single",
258:             "numpy.longdouble",
259:             "numpy.csingle",
260:             "numpy.cdouble",
261:             "numpy.clongdouble"
262:         ],
263:         "required": false
264:     },
265:     "numpy.core": {
266:         "type": "attribute",
267:         "full_path": "numpy.core",
268:         "name": "core",
269:         "children": [
270:             "numpy.core.multiarray"
271:         ]
272:     },
273:     "numpy.core.multiarray": {
274:         "type": "attribute",
275:         "full_path": "numpy.core.multiarray",
276:         "name": "multiarray",
277:         "children": []
278:     },
279:     "numpy.ma": {
280:         "type": "attribute",
281:         "full_path": "numpy.ma",
282:         "name": "ma",
283:         "children": [
284:             "numpy.ma.masked"
285:         ]
286:     },
287:     "numpy.ma.masked": {
288:         "type": "attribute",
289:         "full_path": "numpy.ma.masked",
290:         "name": "masked",
291:         "children": []
292:     },
293:     "numpy.ndarray": {
294:         "type": "attribute",
295:         "full_path": "numpy.ndarray",
296:         "name": "ndarray",
297:         "children": []
298:     },
299:     "numpy.datetime64": {
300:         "type": "attribute",
301:         "full_path": "numpy.datetime64",
302:         "name": "datetime64",
303:         "children": []
304:     },
305:     "numpy.generic": {
306:         "type": "attribute",
307:         "full_path": "numpy.generic",
308:         "name": "generic",
309:         "children": []
310:     },
311:     "numpy.int64": {
312:         "type": "attribute",
313:         "full_path": "numpy.int64",
314:         "name": "int64",
315:         "children": []
316:     },
317:     "numpy.bool_": {
318:         "type": "attribute",
319:         "full_path": "numpy.bool_",
320:         "name": "bool_",
321:         "children": []
322:     },
323:     "numpy.byte": {
324:         "type": "attribute",
325:         "full_path": "numpy.byte",
326:         "name": "byte",
327:         "children": []
328:     },
329:     "numpy.ubyte": {
330:         "type": "attribute",
331:         "full_path": "numpy.ubyte",
332:         "name": "ubyte",
333:         "children": []
334:     },
335:     "numpy.short": {
336:         "type": "attribute",
337:         "full_path": "numpy.short",
338:         "name": "short",
339:         "children": []
340:     },
341:     "numpy.ushort": {
342:         "type": "attribute",
343:         "full_path": "numpy.ushort",
344:         "name": "ushort",
345:         "children": []
346:     },
347:     "numpy.intc": {
348:         "type": "attribute",
349:         "full_path": "numpy.intc",
350:         "name": "intc",
351:         "children": []
352:     },
353:     "numpy.uintc": {
354:         "type": "attribute",
355:         "full_path": "numpy.uintc",
356:         "name": "uintc",
357:         "children": []
358:     },
359:     "numpy.int_": {
360:         "type": "attribute",
361:         "full_path": "numpy.int_",
362:         "name": "int_",
363:         "children": []
364:     },
365:     "numpy.uint": {
366:         "type": "attribute",
367:         "full_path": "numpy.uint",
368:         "name": "uint",
369:         "children": []
370:     },
371:     "numpy.longlong": {
372:         "type": "attribute",
373:         "full_path": "numpy.longlong",
374:         "name": "longlong",
375:         "children": []
376:     },
377:     "numpy.ulonglong": {
378:         "type": "attribute",
379:         "full_path": "numpy.ulonglong",
380:         "name": "ulonglong",
381:         "children": []
382:     },
383:     "numpy.half": {
384:         "type": "attribute",
385:         "full_path": "numpy.half",
386:         "name": "half",
387:         "children": []
388:     },
389:     "numpy.float16": {
390:         "type": "attribute",
391:         "full_path": "numpy.float16",
392:         "name": "float16",
393:         "children": []
394:     },
395:     "numpy.single": {
396:         "type": "attribute",
397:         "full_path": "numpy.single",
398:         "name": "single",
399:         "children": []
400:     },
401:     "numpy.longdouble": {
402:         "type": "attribute",
403:         "full_path": "numpy.longdouble",
404:         "name": "longdouble",
405:         "children": []
406:     },
407:     "numpy.csingle": {
408:         "type": "attribute",
409:         "full_path": "numpy.csingle",
410:         "name": "csingle",
411:         "children": []
412:     },
413:     "numpy.cdouble": {
414:         "type": "attribute",
415:         "full_path": "numpy.cdouble",
416:         "name": "cdouble",
417:         "children": []
418:     },
419:     "numpy.clongdouble": {
420:         "type": "attribute",
421:         "full_path": "numpy.clongdouble",
422:         "name": "clongdouble",
423:         "children": []
424:     },
425:     "pathlib": {
426:         "type": "module",
427:         "full_path": "pathlib",
428:         "name": "pathlib",
429:         "children": [
430:             "pathlib.Path"
431:         ],
432:         "required": false
433:     },
434:     "pathlib.Path": {
435:         "type": "attribute",
436:         "full_path": "pathlib.Path",
437:         "name": "Path",
438:         "children": []
439:     },
440:     "polars": {
441:         "type": "module",
442:         "full_path": "polars",
443:         "name": "polars",
444:         "children": [
445:             "polars.DataFrame",
446:             "polars.LazyFrame"
447:         ],
448:         "required": false
449:     },
450:     "polars.DataFrame": {
451:         "type": "attribute",
452:         "full_path": "polars.DataFrame",
453:         "name": "DataFrame",
454:         "children": []
455:     },
456:     "polars.LazyFrame": {
457:         "type": "attribute",
458:         "full_path": "polars.LazyFrame",
459:         "name": "LazyFrame",
460:         "children": []
461:     },
462:     "duckdb": {
463:         "type": "module",
464:         "full_path": "duckdb",
465:         "name": "duckdb",
466:         "children": [
467:             "duckdb.filesystem",
468:             "duckdb.Value"
469:         ]
470:     },
471:     "duckdb.filesystem": {
472:         "type": "module",
473:         "full_path": "duckdb.filesystem",
474:         "name": "filesystem",
475:         "children": [
476:             "duckdb.filesystem.ModifiedMemoryFileSystem"
477:         ],
478:         "required": false
479:     },
480:     "duckdb.filesystem.ModifiedMemoryFileSystem": {
481:         "type": "attribute",
482:         "full_path": "duckdb.filesystem.ModifiedMemoryFileSystem",
483:         "name": "ModifiedMemoryFileSystem",
484:         "children": []
485:     },
486:     "duckdb.Value": {
487:         "type": "attribute",
488:         "full_path": "duckdb.Value",
489:         "name": "Value",
490:         "children": []
491:     },
492:     "pytz": {
493:         "type": "module",
494:         "full_path": "pytz",
495:         "name": "pytz",
496:         "children": [
497:             "pytz.timezone"
498:         ]
499:     },
500:     "pytz.timezone": {
501:         "type": "attribute",
502:         "full_path": "pytz.timezone",
503:         "name": "timezone",
504:         "children": []
505:     },
506:     "types": {
507:         "type": "module",
508:         "full_path": "types",
509:         "name": "types",
510:         "children": [
511:             "types.UnionType",
512:             "types.GenericAlias",
513:             "types.BuiltinFunctionType"
514:         ]
515:     },
516:     "types.UnionType": {
517:         "type": "attribute",
518:         "full_path": "types.UnionType",
519:         "name": "UnionType",
520:         "children": []
521:     },
522:     "types.GenericAlias": {
523:         "type": "attribute",
524:         "full_path": "types.GenericAlias",
525:         "name": "GenericAlias",
526:         "children": []
527:     },
528:     "types.BuiltinFunctionType": {
529:         "type": "attribute",
530:         "full_path": "types.BuiltinFunctionType",
531:         "name": "BuiltinFunctionType",
532:         "children": []
533:     },
534:     "typing": {
535:         "type": "module",
536:         "full_path": "typing",
537:         "name": "typing",
538:         "children": [
539:             "typing._UnionGenericAlias"
540:         ]
541:     },
542:     "typing._UnionGenericAlias": {
543:         "type": "attribute",
544:         "full_path": "typing._UnionGenericAlias",
545:         "name": "_UnionGenericAlias",
546:         "children": []
547:     },
548:     "uuid": {
549:         "type": "module",
550:         "full_path": "uuid",
551:         "name": "uuid",
552:         "children": [
553:             "uuid.UUID"
554:         ]
555:     },
556:     "uuid.UUID": {
557:         "type": "attribute",
558:         "full_path": "uuid.UUID",
559:         "name": "UUID",
560:         "children": []
561:     },
562:     "collections": {
563:         "type": "module",
564:         "full_path": "collections",
565:         "name": "collections",
566:         "children": [
567:             "collections.abc"
568:         ]
569:     },
570:     "collections.abc": {
571:         "type": "module",
572:         "full_path": "collections.abc",
573:         "name": "abc",
574:         "children": [
575:             "collections.abc.Iterable",
576:             "collections.abc.Mapping"
577:         ]
578:     },
579:     "collections.abc.Iterable": {
580:         "type": "attribute",
581:         "full_path": "collections.abc.Iterable",
582:         "name": "Iterable",
583:         "children": []
584:     },
585:     "collections.abc.Mapping": {
586:         "type": "attribute",
587:         "full_path": "collections.abc.Mapping",
588:         "name": "Mapping",
589:         "children": []
590:     }
591: }
[end of tools/pythonpkg/scripts/cache_data.json]
[start of tools/pythonpkg/scripts/imports.py]
1: import pyarrow
2: import pyarrow.dataset
3: 
4: pyarrow.dataset.Scanner
5: pyarrow.dataset.Dataset
6: pyarrow.Table
7: pyarrow.RecordBatchReader
8: 
9: import pandas
10: 
11: pandas.DataFrame
12: pandas.NaT
13: pandas.NA
14: pandas.isnull
15: pandas.ArrowDtype
16: 
17: import datetime
18: 
19: datetime.date
20: datetime.date.max
21: datetime.date.min
22: datetime.time
23: datetime.timedelta
24: datetime.datetime
25: datetime.datetime.min
26: datetime.datetime.max
27: datetime.datetime.combine
28: datetime.timezone
29: 
30: import decimal
31: 
32: decimal.Decimal
33: 
34: import IPython
35: 
36: IPython.get_ipython
37: IPython.display
38: IPython.display.display
39: IPython.display.HTML
40: 
41: import ipywidgets
42: 
43: ipywidgets.FloatProgress
44: 
45: import numpy
46: 
47: numpy.core.multiarray
48: numpy.ma.masked
49: numpy.ndarray
50: numpy.datetime64
51: numpy.generic
52: numpy.int64
53: numpy.bool_
54: numpy.byte
55: numpy.ubyte
56: numpy.short
57: numpy.ushort
58: numpy.intc
59: numpy.uintc
60: numpy.int_
61: numpy.uint
62: numpy.longlong
63: numpy.ulonglong
64: numpy.half
65: numpy.float16
66: numpy.single
67: numpy.longdouble
68: numpy.csingle
69: numpy.cdouble
70: numpy.clongdouble
71: 
72: import pathlib
73: 
74: pathlib.Path
75: 
76: import polars
77: 
78: polars.DataFrame
79: polars.LazyFrame
80: 
81: import duckdb
82: import duckdb.filesystem
83: 
84: duckdb.filesystem.ModifiedMemoryFileSystem
85: duckdb.Value
86: 
87: import pytz
88: 
89: pytz.timezone
90: 
91: import types
92: 
93: types.UnionType
94: types.GenericAlias
95: types.BuiltinFunctionType
96: 
97: import typing
98: 
99: typing._UnionGenericAlias
100: 
101: import uuid
102: 
103: uuid.UUID
104: 
105: import collections
106: import collections.abc
107: 
108: collections.abc.Iterable
109: collections.abc.Mapping
[end of tools/pythonpkg/scripts/imports.py]
[start of tools/pythonpkg/src/include/duckdb_python/import_cache/modules/numpy_module.hpp]
1: 
2: //===----------------------------------------------------------------------===//
3: //                         DuckDB
4: //
5: // duckdb_python/import_cache/modules/numpy_module.hpp
6: //
7: //
8: //===----------------------------------------------------------------------===//
9: 
10: #pragma once
11: 
12: #include "duckdb_python/import_cache/python_import_cache_item.hpp"
13: 
14: namespace duckdb {
15: 
16: struct NumpyMaCacheItem : public PythonImportCacheItem {
17: 
18: public:
19: 	NumpyMaCacheItem(optional_ptr<PythonImportCacheItem> parent)
20: 	    : PythonImportCacheItem("ma", parent), masked("masked", this) {
21: 	}
22: 	~NumpyMaCacheItem() override {
23: 	}
24: 
25: 	PythonImportCacheItem masked;
26: };
27: 
28: struct NumpyCoreCacheItem : public PythonImportCacheItem {
29: 
30: public:
31: 	NumpyCoreCacheItem(optional_ptr<PythonImportCacheItem> parent)
32: 	    : PythonImportCacheItem("core", parent), multiarray("multiarray", this) {
33: 	}
34: 	~NumpyCoreCacheItem() override {
35: 	}
36: 
37: 	PythonImportCacheItem multiarray;
38: };
39: 
40: struct NumpyCacheItem : public PythonImportCacheItem {
41: 
42: public:
43: 	static constexpr const char *Name = "numpy";
44: 
45: public:
46: 	NumpyCacheItem()
47: 	    : PythonImportCacheItem("numpy"), core(this), ma(this), ndarray("ndarray", this),
48: 	      datetime64("datetime64", this), generic("generic", this), int64("int64", this), bool_("bool_", this),
49: 	      byte("byte", this), ubyte("ubyte", this), short_("short", this), ushort_("ushort", this), intc("intc", this),
50: 	      uintc("uintc", this), int_("int_", this), uint("uint", this), longlong("longlong", this),
51: 	      ulonglong("ulonglong", this), half("half", this), float16("float16", this), single("single", this),
52: 	      longdouble("longdouble", this), csingle("csingle", this), cdouble("cdouble", this),
53: 	      clongdouble("clongdouble", this) {
54: 	}
55: 	~NumpyCacheItem() override {
56: 	}
57: 
58: 	NumpyCoreCacheItem core;
59: 	NumpyMaCacheItem ma;
60: 	PythonImportCacheItem ndarray;
61: 	PythonImportCacheItem datetime64;
62: 	PythonImportCacheItem generic;
63: 	PythonImportCacheItem int64;
64: 	PythonImportCacheItem bool_;
65: 	PythonImportCacheItem byte;
66: 	PythonImportCacheItem ubyte;
67: 	PythonImportCacheItem short_;
68: 	PythonImportCacheItem ushort_;
69: 	PythonImportCacheItem intc;
70: 	PythonImportCacheItem uintc;
71: 	PythonImportCacheItem int_;
72: 	PythonImportCacheItem uint;
73: 	PythonImportCacheItem longlong;
74: 	PythonImportCacheItem ulonglong;
75: 	PythonImportCacheItem half;
76: 	PythonImportCacheItem float16;
77: 	PythonImportCacheItem single;
78: 	PythonImportCacheItem longdouble;
79: 	PythonImportCacheItem csingle;
80: 	PythonImportCacheItem cdouble;
81: 	PythonImportCacheItem clongdouble;
82: 
83: protected:
84: 	bool IsRequired() const override final {
85: 		return false;
86: 	}
87: };
88: 
89: } // namespace duckdb
[end of tools/pythonpkg/src/include/duckdb_python/import_cache/modules/numpy_module.hpp]
[start of tools/pythonpkg/src/include/duckdb_python/import_cache/modules/pandas_module.hpp]
1: 
2: //===----------------------------------------------------------------------===//
3: //                         DuckDB
4: //
5: // duckdb_python/import_cache/modules/pandas_module.hpp
6: //
7: //
8: //===----------------------------------------------------------------------===//
9: 
10: #pragma once
11: 
12: #include "duckdb_python/import_cache/python_import_cache_item.hpp"
13: 
14: namespace duckdb {
15: 
16: struct PandasCacheItem : public PythonImportCacheItem {
17: 
18: public:
19: 	static constexpr const char *Name = "pandas";
20: 
21: public:
22: 	PandasCacheItem()
23: 	    : PythonImportCacheItem("pandas"), DataFrame("DataFrame", this), isnull("isnull", this),
24: 	      ArrowDtype("ArrowDtype", this), NaT("NaT", this), NA("NA", this) {
25: 	}
26: 	~PandasCacheItem() override {
27: 	}
28: 
29: 	PythonImportCacheItem DataFrame;
30: 	PythonImportCacheItem isnull;
31: 	PythonImportCacheItem ArrowDtype;
32: 	PythonImportCacheItem NaT;
33: 	PythonImportCacheItem NA;
34: 
35: protected:
36: 	bool IsRequired() const override final {
37: 		return false;
38: 	}
39: };
40: 
41: } // namespace duckdb
[end of tools/pythonpkg/src/include/duckdb_python/import_cache/modules/pandas_module.hpp]
[start of tools/pythonpkg/src/include/duckdb_python/pyresult.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb_python/pyresult.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb_python/numpy/numpy_result_conversion.hpp"
12: #include "duckdb.hpp"
13: #include "duckdb/main/chunk_scan_state.hpp"
14: #include "duckdb_python/pybind11/pybind_wrapper.hpp"
15: #include "duckdb_python/python_objects.hpp"
16: #include "duckdb_python/pybind11/dataframe.hpp"
17: 
18: namespace duckdb {
19: 
20: struct DuckDBPyResult {
21: public:
22: 	explicit DuckDBPyResult(unique_ptr<QueryResult> result);
23: 	~DuckDBPyResult();
24: 
25: public:
26: 	Optional<py::tuple> Fetchone();
27: 
28: 	py::list Fetchmany(idx_t size);
29: 
30: 	py::list Fetchall();
31: 
32: 	py::dict FetchNumpy();
33: 
34: 	py::dict FetchNumpyInternal(bool stream = false, idx_t vectors_per_chunk = 1,
35: 	                            unique_ptr<NumpyResultConversion> conversion = nullptr);
36: 
37: 	PandasDataFrame FetchDF(bool date_as_object);
38: 
39: 	duckdb::pyarrow::Table FetchArrowTable(idx_t rows_per_batch, bool to_polars);
40: 
41: 	PandasDataFrame FetchDFChunk(const idx_t vectors_per_chunk = 1, bool date_as_object = false);
42: 
43: 	py::dict FetchPyTorch();
44: 
45: 	py::dict FetchTF();
46: 
47: 	ArrowArrayStream FetchArrowArrayStream(idx_t rows_per_batch = 1000000);
48: 	duckdb::pyarrow::RecordBatchReader FetchRecordBatchReader(idx_t rows_per_batch = 1000000);
49: 	py::object FetchArrowCapsule(idx_t rows_per_batch = 1000000);
50: 
51: 	static py::list GetDescription(const vector<string> &names, const vector<LogicalType> &types);
52: 
53: 	void Close();
54: 
55: 	bool IsClosed() const;
56: 
57: 	unique_ptr<DataChunk> FetchChunk();
58: 
59: 	const vector<string> &GetNames();
60: 	const vector<LogicalType> &GetTypes();
61: 
62: private:
63: 	void FillNumpy(py::dict &res, idx_t col_idx, NumpyResultConversion &conversion, const char *name);
64: 
65: 	PandasDataFrame FrameFromNumpy(bool date_as_object, const py::handle &o);
66: 
67: 	void ChangeToTZType(PandasDataFrame &df);
68: 	void ChangeDateToDatetime(PandasDataFrame &df);
69: 	unique_ptr<DataChunk> FetchNext(QueryResult &result);
70: 	unique_ptr<DataChunk> FetchNextRaw(QueryResult &result);
71: 	unique_ptr<NumpyResultConversion> InitializeNumpyConversion(bool pandas = false);
72: 
73: private:
74: 	idx_t chunk_offset = 0;
75: 
76: 	unique_ptr<QueryResult> result;
77: 	unique_ptr<DataChunk> current_chunk;
78: 	// Holds the categories of Categorical/ENUM types
79: 	unordered_map<idx_t, py::list> categories;
80: 	// Holds the categorical type of Categorical/ENUM types
81: 	unordered_map<idx_t, py::object> categories_type;
82: 	bool result_closed = false;
83: };
84: 
85: } // namespace duckdb
[end of tools/pythonpkg/src/include/duckdb_python/pyresult.hpp]
[start of tools/pythonpkg/src/numpy/array_wrapper.cpp]
1: #include "duckdb_python/numpy/array_wrapper.hpp"
2: #include "duckdb/common/types/date.hpp"
3: #include "duckdb/common/types/hugeint.hpp"
4: #include "duckdb/common/types/time.hpp"
5: #include "duckdb/common/types/timestamp.hpp"
6: #include "utf8proc_wrapper.hpp"
7: #include "duckdb/common/types/interval.hpp"
8: #include "duckdb_python/pyrelation.hpp"
9: #include "duckdb_python/python_objects.hpp"
10: #include "duckdb_python/pyconnection/pyconnection.hpp"
11: #include "duckdb_python/pyresult.hpp"
12: #include "duckdb/common/types/uuid.hpp"
13: 
14: namespace duckdb {
15: 
16: namespace duckdb_py_convert {
17: 
18: struct RegularConvert {
19: 	template <class DUCKDB_T, class NUMPY_T>
20: 	static NUMPY_T ConvertValue(DUCKDB_T val, NumpyAppendData &append_data) {
21: 		(void)append_data;
22: 		return (NUMPY_T)val;
23: 	}
24: 
25: 	template <class NUMPY_T, bool PANDAS>
26: 	static NUMPY_T NullValue(bool &set_mask) {
27: 		set_mask = true;
28: 		return 0;
29: 	}
30: };
31: 
32: struct TimestampConvert {
33: 	template <class DUCKDB_T, class NUMPY_T>
34: 	static int64_t ConvertValue(timestamp_t val, NumpyAppendData &append_data) {
35: 		(void)append_data;
36: 		if (!Timestamp::IsFinite(val)) {
37: 			return val.value;
38: 		}
39: 		return Timestamp::GetEpochNanoSeconds(val);
40: 	}
41: 
42: 	template <class NUMPY_T, bool PANDAS>
43: 	static NUMPY_T NullValue(bool &set_mask) {
44: 		set_mask = true;
45: 		return 0;
46: 	}
47: };
48: 
49: struct TimestampConvertSec {
50: 	template <class DUCKDB_T, class NUMPY_T>
51: 	static int64_t ConvertValue(timestamp_t val, NumpyAppendData &append_data) {
52: 		(void)append_data;
53: 		if (!Timestamp::IsFinite(val)) {
54: 			return val.value;
55: 		}
56: 		return Timestamp::GetEpochNanoSeconds(Timestamp::FromEpochSeconds(val.value));
57: 	}
58: 
59: 	template <class NUMPY_T, bool PANDAS>
60: 	static NUMPY_T NullValue(bool &set_mask) {
61: 		set_mask = true;
62: 		return 0;
63: 	}
64: };
65: 
66: struct TimestampConvertMilli {
67: 	template <class DUCKDB_T, class NUMPY_T>
68: 	static int64_t ConvertValue(timestamp_t val, NumpyAppendData &append_data) {
69: 		(void)append_data;
70: 		if (!Timestamp::IsFinite(val)) {
71: 			return val.value;
72: 		}
73: 		return Timestamp::GetEpochNanoSeconds(Timestamp::FromEpochMs(val.value));
74: 	}
75: 
76: 	template <class NUMPY_T, bool PANDAS>
77: 	static NUMPY_T NullValue(bool &set_mask) {
78: 		set_mask = true;
79: 		return 0;
80: 	}
81: };
82: 
83: struct TimestampConvertNano {
84: 	template <class DUCKDB_T, class NUMPY_T>
85: 	static int64_t ConvertValue(timestamp_t val, NumpyAppendData &append_data) {
86: 		(void)append_data;
87: 		return val.value;
88: 	}
89: 
90: 	template <class NUMPY_T, bool PANDAS>
91: 	static NUMPY_T NullValue(bool &set_mask) {
92: 		set_mask = true;
93: 		return 0;
94: 	}
95: };
96: 
97: struct DateConvert {
98: 	template <class DUCKDB_T, class NUMPY_T>
99: 	static int64_t ConvertValue(date_t val, NumpyAppendData &append_data) {
100: 		(void)append_data;
101: 		return Date::EpochMicroseconds(val);
102: 	}
103: 
104: 	template <class NUMPY_T, bool PANDAS>
105: 	static NUMPY_T NullValue(bool &set_mask) {
106: 		set_mask = true;
107: 		return 0;
108: 	}
109: };
110: 
111: struct IntervalConvert {
112: 	template <class DUCKDB_T, class NUMPY_T>
113: 	static int64_t ConvertValue(interval_t val, NumpyAppendData &append_data) {
114: 		(void)append_data;
115: 		return Interval::GetNanoseconds(val);
116: 	}
117: 
118: 	template <class NUMPY_T, bool PANDAS>
119: 	static NUMPY_T NullValue(bool &set_mask) {
120: 		set_mask = true;
121: 		return 0;
122: 	}
123: };
124: 
125: struct TimeConvert {
126: 	template <class DUCKDB_T, class NUMPY_T>
127: 	static PyObject *ConvertValue(dtime_t val, NumpyAppendData &append_data) {
128: 		auto &client_properties = append_data.client_properties;
129: 		auto value = Value::TIME(val);
130: 		auto py_obj = PythonObject::FromValue(value, LogicalType::TIME, client_properties);
131: 		// Release ownership of the PyObject* without decreasing refcount
132: 		// this returns a handle, of which we take the ptr to get the PyObject*
133: 		return py_obj.release().ptr();
134: 	}
135: 
136: 	template <class NUMPY_T, bool PANDAS>
137: 	static NUMPY_T NullValue(bool &set_mask) {
138: 		set_mask = true;
139: 		return nullptr;
140: 	}
141: };
142: 
143: struct StringConvert {
144: 	template <class T>
145: 	static void ConvertUnicodeValueTemplated(T *result, int32_t *codepoints, idx_t codepoint_count, const char *data,
146: 	                                         idx_t ascii_count) {
147: 		// we first fill in the batch of ascii characters directly
148: 		for (idx_t i = 0; i < ascii_count; i++) {
149: 			result[i] = data[i];
150: 		}
151: 		// then we fill in the remaining codepoints from our codepoint array
152: 		for (idx_t i = 0; i < codepoint_count; i++) {
153: 			result[ascii_count + i] = codepoints[i];
154: 		}
155: 	}
156: 
157: 	static PyObject *ConvertUnicodeValue(const char *data, idx_t len, idx_t start_pos) {
158: 		// slow path: check the code points
159: 		// we know that all characters before "start_pos" were ascii characters, so we don't need to check those
160: 
161: 		// allocate an array of code points so we only have to convert the codepoints once
162: 		// short-string optimization
163: 		// we know that the max amount of codepoints is the length of the string
164: 		// for short strings (less than 64 bytes) we simply statically allocate an array of 256 bytes (64x int32)
165: 		// this avoids memory allocation for small strings (common case)
166: 		idx_t remaining = len - start_pos;
167: 		unique_ptr<int32_t[]> allocated_codepoints;
168: 		int32_t static_codepoints[64];
169: 		int32_t *codepoints;
170: 		if (remaining > 64) {
171: 			allocated_codepoints = unique_ptr<int32_t[]>(new int32_t[remaining]);
172: 			codepoints = allocated_codepoints.get();
173: 		} else {
174: 			codepoints = static_codepoints;
175: 		}
176: 		// now we iterate over the remainder of the string to convert the UTF8 string into a sequence of codepoints
177: 		// and to find the maximum codepoint
178: 		int32_t max_codepoint = 127;
179: 		int sz;
180: 		idx_t pos = start_pos;
181: 		idx_t codepoint_count = 0;
182: 		while (pos < len) {
183: 			codepoints[codepoint_count] = Utf8Proc::UTF8ToCodepoint(data + pos, sz);
184: 			pos += sz;
185: 			if (codepoints[codepoint_count] > max_codepoint) {
186: 				max_codepoint = codepoints[codepoint_count];
187: 			}
188: 			codepoint_count++;
189: 		}
190: 		// based on the max codepoint, we construct the result string
191: 		auto result = PyUnicode_New(start_pos + codepoint_count, max_codepoint);
192: 		// based on the resulting unicode kind, we fill in the code points
193: 		auto result_handle = py::handle(result);
194: 		auto kind = PyUtil::PyUnicodeKind(result_handle);
195: 		switch (kind) {
196: 		case PyUnicode_1BYTE_KIND:
197: 			ConvertUnicodeValueTemplated<Py_UCS1>(PyUtil::PyUnicode1ByteData(result_handle), codepoints,
198: 			                                      codepoint_count, data, start_pos);
199: 			break;
200: 		case PyUnicode_2BYTE_KIND:
201: 			ConvertUnicodeValueTemplated<Py_UCS2>(PyUtil::PyUnicode2ByteData(result_handle), codepoints,
202: 			                                      codepoint_count, data, start_pos);
203: 			break;
204: 		case PyUnicode_4BYTE_KIND:
205: 			ConvertUnicodeValueTemplated<Py_UCS4>(PyUtil::PyUnicode4ByteData(result_handle), codepoints,
206: 			                                      codepoint_count, data, start_pos);
207: 			break;
208: 		default:
209: 			throw NotImplementedException("Unsupported typekind constant '%d' for Python Unicode Compact decode", kind);
210: 		}
211: 		return result;
212: 	}
213: 
214: 	template <class DUCKDB_T, class NUMPY_T>
215: 	static PyObject *ConvertValue(string_t val, NumpyAppendData &append_data) {
216: 		(void)append_data;
217: 		// we could use PyUnicode_FromStringAndSize here, but it does a lot of verification that we don't need
218: 		// because of that it is a lot slower than it needs to be
219: 		auto data = const_data_ptr_cast(val.GetData());
220: 		auto len = val.GetSize();
221: 		// check if there are any non-ascii characters in there
222: 		for (idx_t i = 0; i < len; i++) {
223: 			if (data[i] > 127) {
224: 				// there are! fallback to slower case
225: 				return ConvertUnicodeValue(const_char_ptr_cast(data), len, i);
226: 			}
227: 		}
228: 		// no unicode: fast path
229: 		// directly construct the string and memcpy it
230: 		auto result = PyUnicode_New(len, 127);
231: 		auto result_handle = py::handle(result);
232: 		auto target_data = PyUtil::PyUnicodeDataMutable(result_handle);
233: 		memcpy(target_data, data, len);
234: 		return result;
235: 	}
236: 	template <class NUMPY_T, bool PANDAS>
237: 	static NUMPY_T NullValue(bool &set_mask) {
238: 		if (PANDAS) {
239: 			set_mask = false;
240: 			Py_RETURN_NONE;
241: 		}
242: 		set_mask = true;
243: 		return nullptr;
244: 	}
245: };
246: 
247: struct BlobConvert {
248: 	template <class DUCKDB_T, class NUMPY_T>
249: 	static PyObject *ConvertValue(string_t val, NumpyAppendData &append_data) {
250: 		(void)append_data;
251: 		return PyByteArray_FromStringAndSize(val.GetData(), val.GetSize());
252: 	}
253: 
254: 	template <class NUMPY_T, bool PANDAS>
255: 	static NUMPY_T NullValue(bool &set_mask) {
256: 		set_mask = true;
257: 		return nullptr;
258: 	}
259: };
260: 
261: struct BitConvert {
262: 	template <class DUCKDB_T, class NUMPY_T>
263: 	static PyObject *ConvertValue(string_t val, NumpyAppendData &append_data) {
264: 		(void)append_data;
265: 		return PyBytes_FromStringAndSize(val.GetData(), val.GetSize());
266: 	}
267: 
268: 	template <class NUMPY_T, bool PANDAS>
269: 	static NUMPY_T NullValue(bool &set_mask) {
270: 		set_mask = true;
271: 		return nullptr;
272: 	}
273: };
274: 
275: struct UUIDConvert {
276: 	template <class DUCKDB_T, class NUMPY_T>
277: 	static PyObject *ConvertValue(hugeint_t val, NumpyAppendData &append_data) {
278: 		(void)append_data;
279: 		auto &import_cache = *DuckDBPyConnection::ImportCache();
280: 		py::handle h = import_cache.uuid.UUID()(UUID::ToString(val)).release();
281: 		return h.ptr();
282: 	}
283: 
284: 	template <class NUMPY_T, bool PANDAS>
285: 	static NUMPY_T NullValue(bool &set_mask) {
286: 		set_mask = true;
287: 		return nullptr;
288: 	}
289: };
290: 
291: static py::object InternalCreateList(Vector &input, idx_t total_size, idx_t offset, idx_t size,
292:                                      NumpyAppendData &append_data) {
293: 	// Initialize the array we'll append the list data to
294: 	auto &type = input.GetType();
295: 	ArrayWrapper result(type, append_data.client_properties, append_data.pandas);
296: 	result.Initialize(size);
297: 
298: 	D_ASSERT(offset + size <= total_size);
299: 	result.Append(0, input, total_size, offset, size);
300: 	return result.ToArray();
301: }
302: 
303: struct ListConvert {
304: 	static py::object ConvertValue(Vector &input, idx_t chunk_offset, NumpyAppendData &append_data) {
305: 		auto &client_properties = append_data.client_properties;
306: 		auto &list_data = append_data.idata;
307: 
308: 		// Get the list entry information from the parent
309: 		const auto list_sel = *list_data.sel;
310: 		const auto list_entries = UnifiedVectorFormat::GetData<list_entry_t>(list_data);
311: 		auto list_index = list_sel.get_index(chunk_offset);
312: 		auto list_entry = list_entries[list_index];
313: 
314: 		auto list_size = list_entry.length;
315: 		auto list_offset = list_entry.offset;
316: 		auto child_size = ListVector::GetListSize(input);
317: 		auto &child_vector = ListVector::GetEntry(input);
318: 
319: 		return InternalCreateList(child_vector, child_size, list_offset, list_size, append_data);
320: 	}
321: };
322: 
323: struct ArrayConvert {
324: 	static py::object ConvertValue(Vector &input, idx_t chunk_offset, NumpyAppendData &append_data) {
325: 		auto &array_data = append_data.idata;
326: 
327: 		// Get the list entry information from the parent
328: 		const auto array_sel = *array_data.sel;
329: 		auto array_index = array_sel.get_index(chunk_offset);
330: 
331: 		auto &array_type = input.GetType();
332: 		D_ASSERT(array_type.id() == LogicalTypeId::ARRAY);
333: 
334: 		auto array_size = ArrayType::GetSize(array_type);
335: 		auto array_offset = array_index * array_size;
336: 		auto child_size = ArrayVector::GetTotalSize(input);
337: 		auto &child_vector = ArrayVector::GetEntry(input);
338: 
339: 		return InternalCreateList(child_vector, child_size, array_offset, array_size, append_data);
340: 	}
341: };
342: 
343: struct StructConvert {
344: 	static py::dict ConvertValue(Vector &input, idx_t chunk_offset, NumpyAppendData &append_data) {
345: 		auto &client_properties = append_data.client_properties;
346: 
347: 		py::dict py_struct;
348: 		auto val = input.GetValue(chunk_offset);
349: 		auto &child_types = StructType::GetChildTypes(input.GetType());
350: 		auto &struct_children = StructValue::GetChildren(val);
351: 
352: 		for (idx_t i = 0; i < struct_children.size(); i++) {
353: 			auto &child_entry = child_types[i];
354: 			auto &child_name = child_entry.first;
355: 			auto &child_type = child_entry.second;
356: 			py_struct[child_name.c_str()] = PythonObject::FromValue(struct_children[i], child_type, client_properties);
357: 		}
358: 		return py_struct;
359: 	}
360: };
361: 
362: struct UnionConvert {
363: 	static py::object ConvertValue(Vector &input, idx_t chunk_offset, NumpyAppendData &append_data) {
364: 		auto &client_properties = append_data.client_properties;
365: 		auto val = input.GetValue(chunk_offset);
366: 		auto value = UnionValue::GetValue(val);
367: 
368: 		return PythonObject::FromValue(value, UnionValue::GetType(val), client_properties);
369: 	}
370: };
371: 
372: struct MapConvert {
373: 	static py::dict ConvertValue(Vector &input, idx_t chunk_offset, NumpyAppendData &append_data) {
374: 		auto &client_properties = append_data.client_properties;
375: 		auto val = input.GetValue(chunk_offset);
376: 		return PythonObject::FromValue(val, input.GetType(), client_properties);
377: 	}
378: };
379: 
380: struct IntegralConvert {
381: 	template <class DUCKDB_T, class NUMPY_T>
382: 	static NUMPY_T ConvertValue(DUCKDB_T val, NumpyAppendData &append_data) {
383: 		(void)append_data;
384: 		return NUMPY_T(val);
385: 	}
386: 
387: 	template <class NUMPY_T, bool PANDAS>
388: 	static NUMPY_T NullValue(bool &set_mask) {
389: 		set_mask = true;
390: 		return 0;
391: 	}
392: };
393: 
394: template <>
395: double IntegralConvert::ConvertValue(hugeint_t val, NumpyAppendData &append_data) {
396: 	(void)append_data;
397: 	double result;
398: 	Hugeint::TryCast(val, result);
399: 	return result;
400: }
401: 
402: template <>
403: double IntegralConvert::ConvertValue(uhugeint_t val, NumpyAppendData &append_data) {
404: 	(void)append_data;
405: 	double result;
406: 	Uhugeint::TryCast(val, result);
407: 	return result;
408: }
409: 
410: } // namespace duckdb_py_convert
411: 
412: template <class DUCKDB_T, class NUMPY_T, class CONVERT, bool HAS_NULLS, bool PANDAS>
413: static bool ConvertColumnTemplated(NumpyAppendData &append_data) {
414: 	auto target_offset = append_data.target_offset;
415: 	auto target_data = append_data.target_data;
416: 	auto target_mask = append_data.target_mask;
417: 	auto &idata = append_data.idata;
418: 	auto count = append_data.count;
419: 	auto source_offset = append_data.source_offset;
420: 
421: 	auto src_ptr = UnifiedVectorFormat::GetData<DUCKDB_T>(idata);
422: 	auto out_ptr = reinterpret_cast<NUMPY_T *>(target_data);
423: 	bool mask_is_set = false;
424: 	for (idx_t i = 0; i < count; i++) {
425: 		idx_t src_idx = idata.sel->get_index(i + source_offset);
426: 		idx_t offset = target_offset + i;
427: 		if (HAS_NULLS && !idata.validity.RowIsValidUnsafe(src_idx)) {
428: 			out_ptr[offset] = CONVERT::template NullValue<NUMPY_T, PANDAS>(target_mask[offset]);
429: 			mask_is_set = mask_is_set || target_mask[offset];
430: 		} else {
431: 			out_ptr[offset] = CONVERT::template ConvertValue<DUCKDB_T, NUMPY_T>(src_ptr[src_idx], append_data);
432: 			target_mask[offset] = false;
433: 		}
434: 	}
435: 	return mask_is_set;
436: }
437: 
438: template <class DUCKDB_T, class NUMPY_T, class CONVERT>
439: static bool ConvertColumn(NumpyAppendData &append_data) {
440: 	auto target_offset = append_data.target_offset;
441: 	auto target_data = append_data.target_data;
442: 	auto &idata = append_data.idata;
443: 
444: 	auto src_ptr = UnifiedVectorFormat::GetData<DUCKDB_T>(idata);
445: 	auto out_ptr = reinterpret_cast<NUMPY_T *>(target_data);
446: 	if (!idata.validity.AllValid()) {
447: 		if (append_data.pandas) {
448: 			return ConvertColumnTemplated<DUCKDB_T, NUMPY_T, CONVERT, /*has_nulls=*/true, /*pandas=*/true>(append_data);
449: 		} else {
450: 			return ConvertColumnTemplated<DUCKDB_T, NUMPY_T, CONVERT, /*has_nulls=*/true, /*pandas=*/false>(
451: 			    append_data);
452: 		}
453: 	} else {
454: 		if (append_data.pandas) {
455: 			return ConvertColumnTemplated<DUCKDB_T, NUMPY_T, CONVERT, /*has_nulls=*/false, /*pandas=*/true>(
456: 			    append_data);
457: 		} else {
458: 			return ConvertColumnTemplated<DUCKDB_T, NUMPY_T, CONVERT, /*has_nulls=*/false, /*pandas=*/false>(
459: 			    append_data);
460: 		}
461: 	}
462: }
463: 
464: template <class DUCKDB_T, class NUMPY_T>
465: static bool ConvertColumnCategoricalTemplate(NumpyAppendData &append_data) {
466: 	auto target_offset = append_data.target_offset;
467: 	auto target_data = append_data.target_data;
468: 	auto &idata = append_data.idata;
469: 	auto count = append_data.count;
470: 	auto source_offset = append_data.source_offset;
471: 
472: 	auto src_ptr = UnifiedVectorFormat::GetData<DUCKDB_T>(idata);
473: 	auto out_ptr = reinterpret_cast<NUMPY_T *>(target_data);
474: 	if (!idata.validity.AllValid()) {
475: 		for (idx_t i = 0; i < count; i++) {
476: 			idx_t src_idx = idata.sel->get_index(i + source_offset);
477: 			idx_t offset = target_offset + i;
478: 			if (!idata.validity.RowIsValidUnsafe(src_idx)) {
479: 				out_ptr[offset] = static_cast<NUMPY_T>(-1);
480: 			} else {
481: 				out_ptr[offset] = duckdb_py_convert::RegularConvert::template ConvertValue<DUCKDB_T, NUMPY_T>(
482: 				    src_ptr[src_idx], append_data);
483: 			}
484: 		}
485: 	} else {
486: 		for (idx_t i = 0; i < count; i++) {
487: 			idx_t src_idx = idata.sel->get_index(i + source_offset);
488: 			idx_t offset = target_offset + i;
489: 			out_ptr[offset] = duckdb_py_convert::RegularConvert::template ConvertValue<DUCKDB_T, NUMPY_T>(
490: 			    src_ptr[src_idx], append_data);
491: 		}
492: 	}
493: 	// Null values are encoded in the data itself
494: 	return false;
495: }
496: 
497: template <class NUMPY_T, class CONVERT>
498: static bool ConvertNested(NumpyAppendData &append_data) {
499: 	auto target_offset = append_data.target_offset;
500: 	auto target_data = append_data.target_data;
501: 	auto target_mask = append_data.target_mask;
502: 	auto &input = append_data.input;
503: 	auto &idata = append_data.idata;
504: 	auto &client_properties = append_data.client_properties;
505: 	auto count = append_data.count;
506: 	auto source_offset = append_data.source_offset;
507: 
508: 	auto out_ptr = reinterpret_cast<NUMPY_T *>(target_data);
509: 	if (!idata.validity.AllValid()) {
510: 		bool requires_mask = false;
511: 		for (idx_t i = 0; i < count; i++) {
512: 			idx_t index = i + source_offset;
513: 			idx_t src_idx = idata.sel->get_index(index);
514: 			idx_t offset = target_offset + i;
515: 			if (!idata.validity.RowIsValidUnsafe(src_idx)) {
516: 				out_ptr[offset] = py::none();
517: 				requires_mask = true;
518: 				target_mask[offset] = true;
519: 			} else {
520: 				out_ptr[offset] = CONVERT::ConvertValue(input, index, append_data);
521: 				target_mask[offset] = false;
522: 			}
523: 		}
524: 		return requires_mask;
525: 	} else {
526: 		for (idx_t i = 0; i < count; i++) {
527: 			// NOTE: we do not apply the selection vector here,
528: 			// because we use GetValue inside ConvertValue, which *also* applies the selection vector
529: 			idx_t index = i + source_offset;
530: 			idx_t offset = target_offset + i;
531: 			out_ptr[offset] = CONVERT::ConvertValue(input, index, append_data);
532: 			target_mask[offset] = false;
533: 		}
534: 		return false;
535: 	}
536: }
537: 
538: template <class NUMPY_T>
539: static bool ConvertColumnCategorical(NumpyAppendData &append_data) {
540: 	auto physical_type = append_data.physical_type;
541: 	switch (physical_type) {
542: 	case PhysicalType::UINT8:
543: 		return ConvertColumnCategoricalTemplate<uint8_t, NUMPY_T>(append_data);
544: 	case PhysicalType::UINT16:
545: 		return ConvertColumnCategoricalTemplate<uint16_t, NUMPY_T>(append_data);
546: 	case PhysicalType::UINT32:
547: 		return ConvertColumnCategoricalTemplate<uint32_t, NUMPY_T>(append_data);
548: 	default:
549: 		throw InternalException("Enum Physical Type not Allowed");
550: 	}
551: }
552: 
553: template <class T>
554: static bool ConvertColumnRegular(NumpyAppendData &append_data) {
555: 	return ConvertColumn<T, T, duckdb_py_convert::RegularConvert>(append_data);
556: }
557: 
558: template <class DUCKDB_T>
559: static bool ConvertDecimalInternal(NumpyAppendData &append_data, double division) {
560: 	auto target_offset = append_data.target_offset;
561: 	auto target_data = append_data.target_data;
562: 	auto target_mask = append_data.target_mask;
563: 	auto &idata = append_data.idata;
564: 	auto count = append_data.count;
565: 	auto source_offset = append_data.source_offset;
566: 
567: 	auto src_ptr = UnifiedVectorFormat::GetData<DUCKDB_T>(idata);
568: 	auto out_ptr = reinterpret_cast<double *>(target_data);
569: 	if (!idata.validity.AllValid()) {
570: 		for (idx_t i = 0; i < count; i++) {
571: 			idx_t src_idx = idata.sel->get_index(i + source_offset);
572: 			idx_t offset = target_offset + i;
573: 			if (!idata.validity.RowIsValidUnsafe(src_idx)) {
574: 				target_mask[offset] = true;
575: 			} else {
576: 				out_ptr[offset] =
577: 				    duckdb_py_convert::IntegralConvert::ConvertValue<DUCKDB_T, double>(src_ptr[src_idx], append_data) /
578: 				    division;
579: 				target_mask[offset] = false;
580: 			}
581: 		}
582: 		return true;
583: 	} else {
584: 		for (idx_t i = 0; i < count; i++) {
585: 			idx_t src_idx = idata.sel->get_index(i + source_offset);
586: 			idx_t offset = target_offset + i;
587: 			out_ptr[offset] =
588: 			    duckdb_py_convert::IntegralConvert::ConvertValue<DUCKDB_T, double>(src_ptr[src_idx], append_data) /
589: 			    division;
590: 			target_mask[offset] = false;
591: 		}
592: 		return false;
593: 	}
594: }
595: 
596: static bool ConvertDecimal(NumpyAppendData &append_data) {
597: 	auto &decimal_type = append_data.input.GetType();
598: 	auto dec_scale = DecimalType::GetScale(decimal_type);
599: 	double division = pow(10, dec_scale);
600: 	switch (decimal_type.InternalType()) {
601: 	case PhysicalType::INT16:
602: 		return ConvertDecimalInternal<int16_t>(append_data, division);
603: 	case PhysicalType::INT32:
604: 		return ConvertDecimalInternal<int32_t>(append_data, division);
605: 	case PhysicalType::INT64:
606: 		return ConvertDecimalInternal<int64_t>(append_data, division);
607: 	case PhysicalType::INT128:
608: 		return ConvertDecimalInternal<hugeint_t>(append_data, division);
609: 	default:
610: 		throw NotImplementedException("Unimplemented internal type for DECIMAL");
611: 	}
612: }
613: 
614: ArrayWrapper::ArrayWrapper(const LogicalType &type, const ClientProperties &client_properties_p, bool pandas)
615:     : requires_mask(false), client_properties(client_properties_p), pandas(pandas) {
616: 	data = make_uniq<RawArrayWrapper>(type);
617: 	mask = make_uniq<RawArrayWrapper>(LogicalType::BOOLEAN);
618: }
619: 
620: void ArrayWrapper::Initialize(idx_t capacity) {
621: 	data->Initialize(capacity);
622: 	mask->Initialize(capacity);
623: }
624: 
625: void ArrayWrapper::Resize(idx_t new_capacity) {
626: 	data->Resize(new_capacity);
627: 	mask->Resize(new_capacity);
628: }
629: 
630: void ArrayWrapper::Append(idx_t current_offset, Vector &input, idx_t source_size, idx_t source_offset, idx_t count) {
631: 	auto dataptr = data->data;
632: 	auto maskptr = reinterpret_cast<bool *>(mask->data);
633: 	D_ASSERT(dataptr);
634: 	D_ASSERT(maskptr);
635: 	D_ASSERT(input.GetType() == data->type);
636: 	bool may_have_null;
637: 
638: 	UnifiedVectorFormat idata;
639: 	input.ToUnifiedFormat(source_size, idata);
640: 
641: 	if (count == DConstants::INVALID_INDEX) {
642: 		D_ASSERT(source_size != DConstants::INVALID_INDEX);
643: 		count = source_size;
644: 	}
645: 
646: 	NumpyAppendData append_data(idata, client_properties, input);
647: 	append_data.target_offset = current_offset;
648: 	append_data.target_data = dataptr;
649: 	append_data.source_offset = source_offset;
650: 	append_data.source_size = source_size;
651: 	append_data.count = count;
652: 	append_data.target_mask = maskptr;
653: 	append_data.pandas = pandas;
654: 
655: 	switch (input.GetType().id()) {
656: 	case LogicalTypeId::ENUM: {
657: 		auto size = EnumType::GetSize(input.GetType());
658: 		append_data.physical_type = input.GetType().InternalType();
659: 		if (size <= (idx_t)NumericLimits<int8_t>::Maximum()) {
660: 			may_have_null = ConvertColumnCategorical<int8_t>(append_data);
661: 		} else if (size <= (idx_t)NumericLimits<int16_t>::Maximum()) {
662: 			may_have_null = ConvertColumnCategorical<int16_t>(append_data);
663: 		} else if (size <= (idx_t)NumericLimits<int32_t>::Maximum()) {
664: 			may_have_null = ConvertColumnCategorical<int32_t>(append_data);
665: 		} else {
666: 			throw InternalException("Size not supported on ENUM types");
667: 		}
668: 		break;
669: 	}
670: 	case LogicalTypeId::BOOLEAN:
671: 		may_have_null = ConvertColumnRegular<bool>(append_data);
672: 		break;
673: 	case LogicalTypeId::TINYINT:
674: 		may_have_null = ConvertColumnRegular<int8_t>(append_data);
675: 		break;
676: 	case LogicalTypeId::SMALLINT:
677: 		may_have_null = ConvertColumnRegular<int16_t>(append_data);
678: 		break;
679: 	case LogicalTypeId::INTEGER:
680: 		may_have_null = ConvertColumnRegular<int32_t>(append_data);
681: 		break;
682: 	case LogicalTypeId::BIGINT:
683: 		may_have_null = ConvertColumnRegular<int64_t>(append_data);
684: 		break;
685: 	case LogicalTypeId::UTINYINT:
686: 		may_have_null = ConvertColumnRegular<uint8_t>(append_data);
687: 		break;
688: 	case LogicalTypeId::USMALLINT:
689: 		may_have_null = ConvertColumnRegular<uint16_t>(append_data);
690: 		break;
691: 	case LogicalTypeId::UINTEGER:
692: 		may_have_null = ConvertColumnRegular<uint32_t>(append_data);
693: 		break;
694: 	case LogicalTypeId::UBIGINT:
695: 		may_have_null = ConvertColumnRegular<uint64_t>(append_data);
696: 		break;
697: 	case LogicalTypeId::HUGEINT:
698: 		may_have_null = ConvertColumn<hugeint_t, double, duckdb_py_convert::IntegralConvert>(append_data);
699: 		break;
700: 	case LogicalTypeId::UHUGEINT:
701: 		may_have_null = ConvertColumn<uhugeint_t, double, duckdb_py_convert::IntegralConvert>(append_data);
702: 		break;
703: 	case LogicalTypeId::FLOAT:
704: 		may_have_null = ConvertColumnRegular<float>(append_data);
705: 		break;
706: 	case LogicalTypeId::DOUBLE:
707: 		may_have_null = ConvertColumnRegular<double>(append_data);
708: 		break;
709: 	case LogicalTypeId::DECIMAL:
710: 		may_have_null = ConvertDecimal(append_data);
711: 		break;
712: 	case LogicalTypeId::TIMESTAMP:
713: 	case LogicalTypeId::TIMESTAMP_TZ:
714: 	case LogicalTypeId::TIMESTAMP_SEC:
715: 	case LogicalTypeId::TIMESTAMP_MS:
716: 	case LogicalTypeId::TIMESTAMP_NS:
717: 		may_have_null = ConvertColumn<timestamp_t, int64_t, duckdb_py_convert::TimestampConvertNano>(append_data);
718: 		break;
719: 	case LogicalTypeId::DATE:
720: 		may_have_null = ConvertColumn<date_t, int64_t, duckdb_py_convert::DateConvert>(append_data);
721: 		break;
722: 	case LogicalTypeId::TIME:
723: 		may_have_null = ConvertColumn<dtime_t, PyObject *, duckdb_py_convert::TimeConvert>(append_data);
724: 		break;
725: 	case LogicalTypeId::INTERVAL:
726: 		may_have_null = ConvertColumn<interval_t, int64_t, duckdb_py_convert::IntervalConvert>(append_data);
727: 		break;
728: 	case LogicalTypeId::VARCHAR:
729: 		may_have_null = ConvertColumn<string_t, PyObject *, duckdb_py_convert::StringConvert>(append_data);
730: 		break;
731: 	case LogicalTypeId::BLOB:
732: 		may_have_null = ConvertColumn<string_t, PyObject *, duckdb_py_convert::BlobConvert>(append_data);
733: 		break;
734: 	case LogicalTypeId::BIT:
735: 		may_have_null = ConvertColumn<string_t, PyObject *, duckdb_py_convert::BitConvert>(append_data);
736: 		break;
737: 	case LogicalTypeId::LIST:
738: 		may_have_null = ConvertNested<py::object, duckdb_py_convert::ListConvert>(append_data);
739: 		break;
740: 	case LogicalTypeId::ARRAY:
741: 		may_have_null = ConvertNested<py::object, duckdb_py_convert::ArrayConvert>(append_data);
742: 		break;
743: 	case LogicalTypeId::MAP:
744: 		may_have_null = ConvertNested<py::object, duckdb_py_convert::MapConvert>(append_data);
745: 		break;
746: 	case LogicalTypeId::UNION:
747: 		may_have_null = ConvertNested<py::object, duckdb_py_convert::UnionConvert>(append_data);
748: 		break;
749: 	case LogicalTypeId::STRUCT:
750: 		may_have_null = ConvertNested<py::object, duckdb_py_convert::StructConvert>(append_data);
751: 		break;
752: 	case LogicalTypeId::UUID:
753: 		may_have_null = ConvertColumn<hugeint_t, PyObject *, duckdb_py_convert::UUIDConvert>(append_data);
754: 		break;
755: 
756: 	default:
757: 		throw NotImplementedException("Unsupported type \"%s\"", input.GetType().ToString());
758: 	}
759: 	if (may_have_null) {
760: 		requires_mask = true;
761: 	}
762: 	data->count += count;
763: 	mask->count += count;
764: }
765: 
766: py::object ArrayWrapper::ToArray() const {
767: 	D_ASSERT(data->array && mask->array);
768: 	data->Resize(data->count);
769: 	if (!requires_mask) {
770: 		return std::move(data->array);
771: 	}
772: 	mask->Resize(mask->count);
773: 	// construct numpy arrays from the data and the mask
774: 	auto values = std::move(data->array);
775: 	auto nullmask = std::move(mask->array);
776: 
777: 	// create masked array and return it
778: 	auto masked_array = py::module::import("numpy.ma").attr("masked_array")(values, nullmask);
779: 	return masked_array;
780: }
781: 
782: } // namespace duckdb
[end of tools/pythonpkg/src/numpy/array_wrapper.cpp]
[start of tools/pythonpkg/src/pyresult.cpp]
1: #include "duckdb_python/pyrelation.hpp"
2: #include "duckdb_python/pyconnection/pyconnection.hpp"
3: #include "duckdb_python/pyresult.hpp"
4: #include "duckdb_python/python_objects.hpp"
5: 
6: #include "duckdb_python/arrow/arrow_array_stream.hpp"
7: #include "duckdb/common/arrow/arrow.hpp"
8: #include "duckdb/common/arrow/arrow_util.hpp"
9: #include "duckdb/common/arrow/arrow_converter.hpp"
10: #include "duckdb/common/arrow/arrow_wrapper.hpp"
11: #include "duckdb/common/arrow/result_arrow_wrapper.hpp"
12: #include "duckdb/common/types/date.hpp"
13: #include "duckdb/common/types/hugeint.hpp"
14: #include "duckdb/common/types/uhugeint.hpp"
15: #include "duckdb/common/types/time.hpp"
16: #include "duckdb/common/types/timestamp.hpp"
17: #include "duckdb/common/types/uuid.hpp"
18: #include "duckdb_python/numpy/array_wrapper.hpp"
19: #include "duckdb/common/exception.hpp"
20: #include "duckdb/common/enums/stream_execution_result.hpp"
21: #include "duckdb_python/arrow/arrow_export_utils.hpp"
22: #include "duckdb/main/chunk_scan_state/query_result.hpp"
23: #include "duckdb/common/arrow/arrow_query_result.hpp"
24: 
25: namespace duckdb {
26: 
27: DuckDBPyResult::DuckDBPyResult(unique_ptr<QueryResult> result_p) : result(std::move(result_p)) {
28: 	if (!result) {
29: 		throw InternalException("PyResult created without a result object");
30: 	}
31: }
32: 
33: DuckDBPyResult::~DuckDBPyResult() {
34: 	try {
35: 		D_ASSERT(py::gil_check());
36: 		py::gil_scoped_release gil;
37: 		result.reset();
38: 		current_chunk.reset();
39: 	} catch (...) { // NOLINT
40: 	}
41: }
42: 
43: const vector<string> &DuckDBPyResult::GetNames() {
44: 	if (!result) {
45: 		throw InternalException("Calling GetNames without a result object");
46: 	}
47: 	return result->names;
48: }
49: 
50: const vector<LogicalType> &DuckDBPyResult::GetTypes() {
51: 	if (!result) {
52: 		throw InternalException("Calling GetTypes without a result object");
53: 	}
54: 	return result->types;
55: }
56: 
57: unique_ptr<DataChunk> DuckDBPyResult::FetchChunk() {
58: 	if (!result) {
59: 		throw InternalException("FetchChunk called without a result object");
60: 	}
61: 	return FetchNext(*result);
62: }
63: 
64: unique_ptr<DataChunk> DuckDBPyResult::FetchNext(QueryResult &query_result) {
65: 	if (!result_closed && query_result.type == QueryResultType::STREAM_RESULT &&
66: 	    !query_result.Cast<StreamQueryResult>().IsOpen()) {
67: 		result_closed = true;
68: 		return nullptr;
69: 	}
70: 	if (query_result.type == QueryResultType::STREAM_RESULT) {
71: 		auto &stream_result = query_result.Cast<StreamQueryResult>();
72: 		StreamExecutionResult execution_result;
73: 		while (!StreamQueryResult::IsChunkReady(execution_result = stream_result.ExecuteTask())) {
74: 			{
75: 				py::gil_scoped_acquire gil;
76: 				if (PyErr_CheckSignals() != 0) {
77: 					throw std::runtime_error("Query interrupted");
78: 				}
79: 			}
80: 			if (execution_result == StreamExecutionResult::BLOCKED) {
81: 				stream_result.WaitForTask();
82: 			}
83: 		}
84: 		if (execution_result == StreamExecutionResult::EXECUTION_CANCELLED) {
85: 			throw InvalidInputException("The execution of the query was cancelled before it could finish, likely "
86: 			                            "caused by executing a different query");
87: 		}
88: 		if (execution_result == StreamExecutionResult::EXECUTION_ERROR) {
89: 			stream_result.ThrowError();
90: 		}
91: 	}
92: 	auto chunk = query_result.Fetch();
93: 	if (query_result.HasError()) {
94: 		query_result.ThrowError();
95: 	}
96: 	return chunk;
97: }
98: 
99: unique_ptr<DataChunk> DuckDBPyResult::FetchNextRaw(QueryResult &query_result) {
100: 	if (!result_closed && query_result.type == QueryResultType::STREAM_RESULT &&
101: 	    !query_result.Cast<StreamQueryResult>().IsOpen()) {
102: 		result_closed = true;
103: 		return nullptr;
104: 	}
105: 	auto chunk = query_result.FetchRaw();
106: 	if (query_result.HasError()) {
107: 		query_result.ThrowError();
108: 	}
109: 	return chunk;
110: }
111: 
112: Optional<py::tuple> DuckDBPyResult::Fetchone() {
113: 	{
114: 		D_ASSERT(py::gil_check());
115: 		py::gil_scoped_release release;
116: 		if (!result) {
117: 			throw InvalidInputException("result closed");
118: 		}
119: 		if (!current_chunk || chunk_offset >= current_chunk->size()) {
120: 			current_chunk = FetchNext(*result);
121: 			chunk_offset = 0;
122: 		}
123: 	}
124: 
125: 	if (!current_chunk || current_chunk->size() == 0) {
126: 		return py::none();
127: 	}
128: 	py::tuple res(result->types.size());
129: 
130: 	for (idx_t col_idx = 0; col_idx < result->types.size(); col_idx++) {
131: 		auto &mask = FlatVector::Validity(current_chunk->data[col_idx]);
132: 		if (!mask.RowIsValid(chunk_offset)) {
133: 			res[col_idx] = py::none();
134: 			continue;
135: 		}
136: 		auto val = current_chunk->data[col_idx].GetValue(chunk_offset);
137: 		res[col_idx] = PythonObject::FromValue(val, result->types[col_idx], result->client_properties);
138: 	}
139: 	chunk_offset++;
140: 	return res;
141: }
142: 
143: py::list DuckDBPyResult::Fetchmany(idx_t size) {
144: 	py::list res;
145: 	for (idx_t i = 0; i < size; i++) {
146: 		auto fres = Fetchone();
147: 		if (fres.is_none()) {
148: 			break;
149: 		}
150: 		res.append(fres);
151: 	}
152: 	return res;
153: }
154: 
155: py::list DuckDBPyResult::Fetchall() {
156: 	py::list res;
157: 	while (true) {
158: 		auto fres = Fetchone();
159: 		if (fres.is_none()) {
160: 			break;
161: 		}
162: 		res.append(fres);
163: 	}
164: 	return res;
165: }
166: 
167: py::dict DuckDBPyResult::FetchNumpy() {
168: 	return FetchNumpyInternal();
169: }
170: 
171: void DuckDBPyResult::FillNumpy(py::dict &res, idx_t col_idx, NumpyResultConversion &conversion, const char *name) {
172: 	if (result->types[col_idx].id() == LogicalTypeId::ENUM) {
173: 		// first we (might) need to create the categorical type
174: 		if (categories_type.find(col_idx) == categories_type.end()) {
175: 			// Equivalent to: pandas.CategoricalDtype(['a', 'b'], ordered=True)
176: 			categories_type[col_idx] = py::module::import("pandas").attr("CategoricalDtype")(categories[col_idx], true);
177: 		}
178: 		// Equivalent to: pandas.Categorical.from_codes(codes=[0, 1, 0, 1], dtype=dtype)
179: 		res[name] = py::module::import("pandas")
180: 		                .attr("Categorical")
181: 		                .attr("from_codes")(conversion.ToArray(col_idx), py::arg("dtype") = categories_type[col_idx]);
182: 		if (!conversion.ToPandas()) {
183: 			res[name] = res[name].attr("to_numpy")();
184: 		}
185: 	} else {
186: 		res[name] = conversion.ToArray(col_idx);
187: 	}
188: }
189: 
190: void InsertCategory(QueryResult &result, unordered_map<idx_t, py::list> &categories) {
191: 	for (idx_t col_idx = 0; col_idx < result.types.size(); col_idx++) {
192: 		auto &type = result.types[col_idx];
193: 		if (type.id() == LogicalTypeId::ENUM) {
194: 			// It's an ENUM type, in addition to converting the codes we must convert the categories
195: 			if (categories.find(col_idx) == categories.end()) {
196: 				auto &categories_list = EnumType::GetValuesInsertOrder(type);
197: 				auto categories_size = EnumType::GetSize(type);
198: 				for (idx_t i = 0; i < categories_size; i++) {
199: 					categories[col_idx].append(py::cast(categories_list.GetValue(i).ToString()));
200: 				}
201: 			}
202: 		}
203: 	}
204: }
205: 
206: unique_ptr<NumpyResultConversion> DuckDBPyResult::InitializeNumpyConversion(bool pandas) {
207: 	if (!result) {
208: 		throw InvalidInputException("result closed");
209: 	}
210: 
211: 	idx_t initial_capacity = STANDARD_VECTOR_SIZE * 2ULL;
212: 	if (result->type == QueryResultType::MATERIALIZED_RESULT) {
213: 		// materialized query result: we know exactly how much space we need
214: 		auto &materialized = result->Cast<MaterializedQueryResult>();
215: 		initial_capacity = materialized.RowCount();
216: 	}
217: 
218: 	auto conversion =
219: 	    make_uniq<NumpyResultConversion>(result->types, initial_capacity, result->client_properties, pandas);
220: 	return conversion;
221: }
222: 
223: py::dict DuckDBPyResult::FetchNumpyInternal(bool stream, idx_t vectors_per_chunk,
224:                                             unique_ptr<NumpyResultConversion> conversion_p) {
225: 	if (!result) {
226: 		throw InvalidInputException("result closed");
227: 	}
228: 	if (!conversion_p) {
229: 		conversion_p = InitializeNumpyConversion();
230: 	}
231: 	auto &conversion = *conversion_p;
232: 
233: 	if (result->type == QueryResultType::MATERIALIZED_RESULT) {
234: 		auto &materialized = result->Cast<MaterializedQueryResult>();
235: 		for (auto &chunk : materialized.Collection().Chunks()) {
236: 			conversion.Append(chunk);
237: 		}
238: 		InsertCategory(materialized, categories);
239: 		materialized.Collection().Reset();
240: 	} else {
241: 		D_ASSERT(result->type == QueryResultType::STREAM_RESULT);
242: 		if (!stream) {
243: 			vectors_per_chunk = NumericLimits<idx_t>::Maximum();
244: 		}
245: 		auto &stream_result = result->Cast<StreamQueryResult>();
246: 		for (idx_t count_vec = 0; count_vec < vectors_per_chunk; count_vec++) {
247: 			if (!stream_result.IsOpen()) {
248: 				break;
249: 			}
250: 			unique_ptr<DataChunk> chunk;
251: 			{
252: 				D_ASSERT(py::gil_check());
253: 				py::gil_scoped_release release;
254: 				chunk = FetchNextRaw(stream_result);
255: 			}
256: 			if (!chunk || chunk->size() == 0) {
257: 				//! finished
258: 				break;
259: 			}
260: 			conversion.Append(*chunk);
261: 			InsertCategory(stream_result, categories);
262: 		}
263: 	}
264: 
265: 	// now that we have materialized the result in contiguous arrays, construct the actual NumPy arrays or categorical
266: 	// types
267: 	py::dict res;
268: 	auto names = result->names;
269: 	QueryResult::DeduplicateColumns(names);
270: 	for (idx_t col_idx = 0; col_idx < result->names.size(); col_idx++) {
271: 		auto &name = names[col_idx];
272: 		FillNumpy(res, col_idx, conversion, name.c_str());
273: 	}
274: 	return res;
275: }
276: 
277: // TODO: unify these with an enum/flag to indicate which conversions to do
278: void DuckDBPyResult::ChangeToTZType(PandasDataFrame &df) {
279: 	auto names = df.attr("columns").cast<vector<string>>();
280: 
281: 	for (idx_t i = 0; i < result->ColumnCount(); i++) {
282: 		if (result->types[i] == LogicalType::TIMESTAMP_TZ) {
283: 			// first localize to UTC then convert to timezone_config
284: 			auto utc_local = df[names[i].c_str()].attr("dt").attr("tz_localize")("UTC");
285: 			df.attr("__setitem__")(names[i].c_str(),
286: 			                       utc_local.attr("dt").attr("tz_convert")(result->client_properties.time_zone));
287: 		}
288: 	}
289: }
290: 
291: // TODO: unify these with an enum/flag to indicate which conversions to perform
292: void DuckDBPyResult::ChangeDateToDatetime(PandasDataFrame &df) {
293: 	auto names = df.attr("columns").cast<vector<string>>();
294: 
295: 	for (idx_t i = 0; i < result->ColumnCount(); i++) {
296: 		if (result->types[i] == LogicalType::DATE) {
297: 			df.attr("__setitem__")(names[i].c_str(), df[names[i].c_str()].attr("dt").attr("date"));
298: 		}
299: 	}
300: }
301: 
302: PandasDataFrame DuckDBPyResult::FrameFromNumpy(bool date_as_object, const py::handle &o) {
303: 	PandasDataFrame df = py::cast<PandasDataFrame>(py::module::import("pandas").attr("DataFrame").attr("from_dict")(o));
304: 	// Unfortunately we have to do a type change here for timezones since these types are not supported by numpy
305: 	ChangeToTZType(df);
306: 	if (date_as_object) {
307: 		ChangeDateToDatetime(df);
308: 	}
309: 	return df;
310: }
311: 
312: PandasDataFrame DuckDBPyResult::FetchDF(bool date_as_object) {
313: 	auto conversion = InitializeNumpyConversion(true);
314: 	return FrameFromNumpy(date_as_object, FetchNumpyInternal(false, 1, std::move(conversion)));
315: }
316: 
317: PandasDataFrame DuckDBPyResult::FetchDFChunk(idx_t num_of_vectors, bool date_as_object) {
318: 	auto conversion = InitializeNumpyConversion(true);
319: 	return FrameFromNumpy(date_as_object, FetchNumpyInternal(true, num_of_vectors, std::move(conversion)));
320: }
321: 
322: py::dict DuckDBPyResult::FetchPyTorch() {
323: 	auto result_dict = FetchNumpyInternal();
324: 	auto from_numpy = py::module::import("torch").attr("from_numpy");
325: 	for (auto &item : result_dict) {
326: 		result_dict[item.first] = from_numpy(item.second);
327: 	}
328: 	return result_dict;
329: }
330: 
331: py::dict DuckDBPyResult::FetchTF() {
332: 	auto result_dict = FetchNumpyInternal();
333: 	auto convert_to_tensor = py::module::import("tensorflow").attr("convert_to_tensor");
334: 	for (auto &item : result_dict) {
335: 		result_dict[item.first] = convert_to_tensor(item.second);
336: 	}
337: 	return result_dict;
338: }
339: 
340: duckdb::pyarrow::Table DuckDBPyResult::FetchArrowTable(idx_t rows_per_batch, bool to_polars) {
341: 	if (!result) {
342: 		throw InvalidInputException("There is no query result");
343: 	}
344: 	auto names = result->names;
345: 	if (to_polars) {
346: 		QueryResult::DeduplicateColumns(names);
347: 	}
348: 
349: 	if (!result) {
350: 		throw InvalidInputException("result closed");
351: 	}
352: 	auto pyarrow_lib_module = py::module::import("pyarrow").attr("lib");
353: 
354: 	py::list batches;
355: 	if (result->type == QueryResultType::ARROW_RESULT) {
356: 		auto &arrow_result = result->Cast<ArrowQueryResult>();
357: 		auto arrays = arrow_result.ConsumeArrays();
358: 		for (auto &array : arrays) {
359: 			ArrowSchema arrow_schema;
360: 			auto names = arrow_result.names;
361: 			if (to_polars) {
362: 				QueryResult::DeduplicateColumns(names);
363: 			}
364: 			ArrowArray data = array->arrow_array;
365: 			array->arrow_array.release = nullptr;
366: 			ArrowConverter::ToArrowSchema(&arrow_schema, arrow_result.types, names, arrow_result.client_properties);
367: 			TransformDuckToArrowChunk(arrow_schema, data, batches);
368: 		}
369: 	} else {
370: 		QueryResultChunkScanState scan_state(*result.get());
371: 		while (true) {
372: 			ArrowArray data;
373: 			idx_t count;
374: 			auto &query_result = *result.get();
375: 			{
376: 				D_ASSERT(py::gil_check());
377: 				py::gil_scoped_release release;
378: 				count = ArrowUtil::FetchChunk(scan_state, query_result.client_properties, rows_per_batch, &data);
379: 			}
380: 			if (count == 0) {
381: 				break;
382: 			}
383: 			ArrowSchema arrow_schema;
384: 			auto names = query_result.names;
385: 			if (to_polars) {
386: 				QueryResult::DeduplicateColumns(names);
387: 			}
388: 			ArrowConverter::ToArrowSchema(&arrow_schema, query_result.types, names, query_result.client_properties);
389: 			TransformDuckToArrowChunk(arrow_schema, data, batches);
390: 		}
391: 	}
392: 
393: 	return pyarrow::ToArrowTable(result->types, names, std::move(batches), result->client_properties);
394: }
395: 
396: ArrowArrayStream DuckDBPyResult::FetchArrowArrayStream(idx_t rows_per_batch) {
397: 	if (!result) {
398: 		throw InvalidInputException("There is no query result");
399: 	}
400: 	ResultArrowArrayStreamWrapper *result_stream = new ResultArrowArrayStreamWrapper(std::move(result), rows_per_batch);
401: 	// The 'result_stream' is part of the 'private_data' of the ArrowArrayStream and its lifetime is bound to that of
402: 	// the ArrowArrayStream.
403: 	return result_stream->stream;
404: }
405: 
406: duckdb::pyarrow::RecordBatchReader DuckDBPyResult::FetchRecordBatchReader(idx_t rows_per_batch) {
407: 	if (!result) {
408: 		throw InvalidInputException("There is no query result");
409: 	}
410: 	py::gil_scoped_acquire acquire;
411: 	auto pyarrow_lib_module = py::module::import("pyarrow").attr("lib");
412: 	auto record_batch_reader_func = pyarrow_lib_module.attr("RecordBatchReader").attr("_import_from_c");
413: 	auto stream = FetchArrowArrayStream(rows_per_batch);
414: 	py::object record_batch_reader = record_batch_reader_func((uint64_t)&stream); // NOLINT
415: 	return py::cast<duckdb::pyarrow::RecordBatchReader>(record_batch_reader);
416: }
417: 
418: static void ArrowArrayStreamPyCapsuleDestructor(PyObject *object) {
419: 	auto data = PyCapsule_GetPointer(object, "arrow_array_stream");
420: 	if (!data) {
421: 		return;
422: 	}
423: 	auto stream = reinterpret_cast<ArrowArrayStream *>(data);
424: 	if (stream->release) {
425: 		stream->release(stream);
426: 	}
427: 	delete stream;
428: }
429: 
430: py::object DuckDBPyResult::FetchArrowCapsule(idx_t rows_per_batch) {
431: 	auto stream_p = FetchArrowArrayStream(rows_per_batch);
432: 	auto stream = new ArrowArrayStream();
433: 	*stream = stream_p;
434: 	return py::capsule(stream, "arrow_array_stream", ArrowArrayStreamPyCapsuleDestructor);
435: }
436: 
437: py::str GetTypeToPython(const LogicalType &type) {
438: 	switch (type.id()) {
439: 	case LogicalTypeId::BOOLEAN:
440: 		return py::str("bool");
441: 	case LogicalTypeId::TINYINT:
442: 	case LogicalTypeId::SMALLINT:
443: 	case LogicalTypeId::INTEGER:
444: 	case LogicalTypeId::BIGINT:
445: 	case LogicalTypeId::UTINYINT:
446: 	case LogicalTypeId::USMALLINT:
447: 	case LogicalTypeId::UINTEGER:
448: 	case LogicalTypeId::UBIGINT:
449: 	case LogicalTypeId::HUGEINT:
450: 	case LogicalTypeId::UHUGEINT:
451: 	case LogicalTypeId::FLOAT:
452: 	case LogicalTypeId::DOUBLE:
453: 	case LogicalTypeId::DECIMAL: {
454: 		return py::str("NUMBER");
455: 	}
456: 	case LogicalTypeId::VARCHAR: {
457: 		if (type.HasAlias() && type.GetAlias() == "JSON") {
458: 			return py::str("JSON");
459: 		} else {
460: 			return py::str("STRING");
461: 		}
462: 	}
463: 	case LogicalTypeId::BLOB:
464: 	case LogicalTypeId::BIT:
465: 		return py::str("BINARY");
466: 	case LogicalTypeId::TIMESTAMP:
467: 	case LogicalTypeId::TIMESTAMP_TZ:
468: 	case LogicalTypeId::TIMESTAMP_MS:
469: 	case LogicalTypeId::TIMESTAMP_NS:
470: 	case LogicalTypeId::TIMESTAMP_SEC: {
471: 		return py::str("DATETIME");
472: 	}
473: 	case LogicalTypeId::TIME:
474: 	case LogicalTypeId::TIME_TZ: {
475: 		return py::str("Time");
476: 	}
477: 	case LogicalTypeId::DATE: {
478: 		return py::str("Date");
479: 	}
480: 	case LogicalTypeId::STRUCT:
481: 	case LogicalTypeId::MAP:
482: 		return py::str("dict");
483: 	case LogicalTypeId::LIST: {
484: 		return py::str("list");
485: 	}
486: 	case LogicalTypeId::INTERVAL: {
487: 		return py::str("TIMEDELTA");
488: 	}
489: 	case LogicalTypeId::UUID: {
490: 		return py::str("UUID");
491: 	}
492: 	default:
493: 		return py::str(type.ToString());
494: 	}
495: }
496: 
497: py::list DuckDBPyResult::GetDescription(const vector<string> &names, const vector<LogicalType> &types) {
498: 	py::list desc;
499: 
500: 	for (idx_t col_idx = 0; col_idx < names.size(); col_idx++) {
501: 		auto py_name = py::str(names[col_idx]);
502: 		auto py_type = GetTypeToPython(types[col_idx]);
503: 		desc.append(py::make_tuple(py_name, py_type, py::none(), py::none(), py::none(), py::none(), py::none()));
504: 	}
505: 	return desc;
506: }
507: 
508: void DuckDBPyResult::Close() {
509: 	result = nullptr;
510: }
511: 
512: bool DuckDBPyResult::IsClosed() const {
513: 	return result_closed;
514: }
515: 
516: } // namespace duckdb
[end of tools/pythonpkg/src/pyresult.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: