You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
v1.2 broke select from gzipped json
### What happens?

After upgrading to v1.2, reading a gzipped json file demanded a large increase in maximum object size and subsequently failed with invalid character. The same json without gzip compression reads fine in both v1.x and v1.2.

### To Reproduce

```
SELECT * FROM 'fundos_list.json.gz';
```
[fundos_list.json.gz](https://github.com/user-attachments/files/18707966/fundos_list.json.gz)

### OS:

Windows 11

### DuckDB Version:

1.2

### DuckDB Client:

python

### Hardware:

_No response_

### Full Name:

Leonardo Horta

### Affiliation:

Sparta Fundos de Investimento

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [x] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [x] Yes, I have
Large limits are much slower in v1.2.0
### What happens?

After upgrading to duckdb 1.2.0 certain parquet export operations are taking much longer than in 1.1.3, especially for large(?) tables. On an example table with ~150M rows and 20 cols We see performance like:

```
./duckdb-1.1.3 test_parquet_writes.db -c "set preserve_insertion_order=false; copy (from t1 limit 100_000_000) to '/dev/null' (format parquet)"
```
takes 9s on v1.1.3 whereas

```
./duckdb-nightly test_parquet_writes.db -c "set preserve_insertion_order=false; copy (from t1 limit 100_000_000) to '/dev/null' (format parquet)"
```
takes 26s on nightly, a 3x slowdown. On wider tables the difference can be much worse.

### To Reproduce

here is a dropbox link to a database that demonstrates the issue https://www.dropbox.com/scl/fi/jd0y5cjdabwfgriuwhick/test_parquet_writes.db.gz?rlkey=tfhu5mxtsm9ng4yv0bhzcwtok&st=l8blewpt&dl=1

after downloading/unzipping run:

```time duckdb test_parquet_writes.db -c "set preserve_insertion_order=false; copy (from t1 limit 100_000_000) to '/dev/null' (format parquet)"```

on 1.2.0 vs 1.1.3

### OS:

macos arm / apple silicon

### DuckDB Version:

1.2.0

### DuckDB Client:

cli

### Hardware:

repros with 16core / 128 GB and 8 core / 8 GB

### Full Name:

Matt Hanlon

### Affiliation:

PwC

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a nightly build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [x] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [x] Yes, I have
Out of Memory Error When Unnesting Large Array Field
### What happens?

I have large dataset that contains a deeply nested json field I am attempting to unnest. It is the ingredients column that is part of this dataset:
https://huggingface.co/datasets/openfoodfacts/product-database

I have loaded this parquet file into my database and I am attempting to unnest it with the following query:
```sql
with
    parsed_ingredients as (
        select code, ingredients::json[] as json_array
        from "mydb"."main_raw"."raw__open_food_facts"
    ),
    ingredient_hierarchy as (
        select
            code,
            unnest(json_array, max_depth := 2) as ingredient_name,
            generate_subscripts(json_array, 1) as ingredient_order,
        from parsed_ingredients
    )

select *
from ingredient_hierarchy
order by code, ingredient_order
```

When running it results in the following error (note I am limiting my duckdb instance to only use 8gb to run on my hardware):
`Out of Memory Error: could not allocate block of size 256.0 KiB (7.4 GiB/7.4 GiB used)`

I have also tried converting the field to a struct first but this causes it run out of memory before I even begin unnesting. How can i properly process this field without running OOM?

### To Reproduce

Download the parquet data and run the code above to reporoduce.

### OS:

Windows

### DuckDB Version:

1.7.0

### DuckDB Client:

DuckDB-DBT

### Hardware:

_No response_

### Full Name:

Kyle Burke

### Affiliation:

None

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [x] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [x] Yes, I have
Filter with an error-throwing function getting incorrectly reordered
### What happens?

Simple `where abs(col_double) < 1 and acos(col_double) > 0` expressions can throw an error as of 1.2.0 because the filters might get reordered with `acos` being executed first while the `abs` expression is guarding against `acos` errors.

### To Reproduce

```sql
copy (select random() * 2 col_double from generate_series(1,100)) to '/tmp/r.parquet';
select * from (select * from  '/tmp/r.parquet') where abs(col_double) < 1 and acos(col_double) > 0;
```

On 1.1.3:
```
┌──────────────────────┐
│      col_double      │
│        double        │
├──────────────────────┤
│  0.19490873625576743 │
│   0.9895238863469309 │  
...
```

On 1.2.0:
```
Invalid Input Error:
ACOS is undefined outside [-1,1]
```

### OS:

Ubuntu 22.04

### DuckDB Version:

1.2.0

### DuckDB Client:

CLI

### Hardware:

_No response_

### Full Name:

Marco Slot

### Affiliation:

Crunchy Data

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [x] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [x] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdb.org/docs/api/r#duckplyr-dplyr-api).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://duckdb.org/docs/installation/) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of .github/actions/build_extensions/action.yml]
1: name: "Build Extensions"
2: description: "Build, test and deploy the DuckDB extensions"
3: inputs:
4:   # Test config
5:   run_tests:
6:     description: 'Run extension tests after build'
7:     default: 1
8:   run_autoload_tests:
9:     description: 'Runs the autoloading tests'
10:     default: 1
11: 
12:   # Deploy config
13:   deploy_as:
14:     description: 'Binary architecture name for deploy step'
15:     default: ''
16:   deploy_version:
17:     description: 'Version tag or commit short hash for deploy step'
18:     default: ''
19:   s3_id:
20:     description: 'S3 key ID'
21:     default: ''
22:   s3_key:
23:     description: 'S3 key secret'
24:     default: ''
25:   signing_pk:
26:     description: 'Extension signing RSA private key'
27:     default: ''
28: 
29:   # Build config
30:   duckdb_arch:
31:     description: 'Provide DUCKDB_PLATFORM to build system for cross compilation'
32:     default: ''
33:   static_link_build:
34:     description: 'Links DuckDB statically to the loadable extensions'
35:     default: 1
36:   no_static_linking:
37:     description: 'Disables linking extensions into DuckDB for testing'
38:     default: 0
39:   vcpkg_build:
40:     description: 'Installs vcpkg and pass its toolchain to CMakes'
41:     default: 1
42:   build_dir:
43:     description: 'DuckDB source directory to run the build in'
44:     default: '.'
45:   ninja:
46:     description: 'Use ninja for building'
47:     default: 0
48:   openssl_path:
49:     description: 'Directory of OpenSSL installation'
50:     default: ''
51:   post_install:
52:     description: 'Post-install scripts to run'
53:     default: ''
54:   treat_warn_as_error:
55:     description: 'Treat compilation warnings as errors'
56:     default: 1
57:   build_in_tree_extensions:
58:     description: 'Build in-tree extensions'
59:     default: 1
60:   build_out_of_tree_extensions:
61:     description: 'Build out-of-tree extensions'
62:     default: 1
63:   osx_universal:
64:     description: 'Build Universal Binary for OSX'
65:     default: 0
66:   osx_arch:
67:     description: 'Build specific architecture for OSX'
68:     default: ''
69:   aarch64_cross_compile:
70:     description: 'Enable Linux aarch64 cross-compiling'
71:     default: 0
72:   vcpkg_target_triplet:
73:     description: 'Target triplet for installing vcpkg dependencies'
74:     default: ''
75:   override_cc:
76:     description: 'Override C Compiler'
77:     default: ''
78:   override_cxx:
79:     description: 'Override CXX Compiler'
80:     default: ''
81:   unittest_script:
82:     description: 'Script/program to execute the unittests'
83:     default: 'python3 scripts/run_tests_one_by_one.py ./build/release/test/unittest'
84:   cmake_flags:
85:     description: 'Flags to be passed to cmake'
86:     default: ''
87: 
88: runs:
89:   using: "composite"
90:   steps:
91:     - name: Setup DuckDB extension build config
92:       shell: bash
93:       run: |
94:         export EXTENSION_CONFIGS="$EXTENSION_CONFIGS;${{ inputs.build_in_tree_extensions == 1 && '.github/config/in_tree_extensions.cmake' || ''}}"
95:         export EXTENSION_CONFIGS="$EXTENSION_CONFIGS;${{ inputs.build_out_of_tree_extensions == 1 && '.github/config/out_of_tree_extensions.cmake' || '' }}"
96:         echo "EXTENSION_CONFIGS=$EXTENSION_CONFIGS" >> $GITHUB_ENV
97: 
98:     - name: Setup vcpkg
99:       if: inputs.vcpkg_build == 1
100:       uses: lukka/run-vcpkg@v11.1
101:       with:
102:         vcpkgGitCommitId: 5e5d0e1cd7785623065e77eff011afdeec1a3574
103: 
104:     - name: Set vcpkg env variables
105:       if: inputs.vcpkg_build == 1
106:       shell: bash
107:       run: |
108:         echo "VCPKG_TOOLCHAIN_PATH=$VCPKG_ROOT/scripts/buildsystems/vcpkg.cmake" >> $GITHUB_ENV
109:         echo "VCPKG_TARGET_TRIPLET=${{ inputs.vcpkg_target_triplet }}" >> $GITHUB_ENV
110: 
111:     - name: workaround for https://github.com/duckdb/duckdb/issues/8360
112:       if: inputs.vcpkg_target_triplet == 'x64-windows-static-md'
113:       shell: bash
114:       run: |
115:         cd $VCPKG_ROOT
116:         mkdir -p downloads
117:         cd downloads
118:         curl -O -L https://github.com/duckdb/duckdb-data/releases/download/v1.0/nasm-2.16.01-win64.zip
119:         ls -al
120:         pwd
121: 
122:     - name: Set Openssl dir
123:       if: inputs.openssl_path != ''
124:       shell: bash
125:       run: |
126:         echo "OPENSSL_ROOT_DIR=${{ inputs.openssl_path }}" >> $GITHUB_ENV
127: 
128:     - name: Create combined vcpkg manifest
129:       if: inputs.vcpkg_build == 1 && inputs.build_out_of_tree_extensions == 1
130:       shell: bash
131:       run: |
132:         make extension_configuration
133: 
134:     - name: Build
135:       shell: bash
136:       env:
137:         TREAT_WARNINGS_AS_ERRORS: ${{ inputs.treat_warn_as_error}}
138:         FORCE_WARN_UNUSED: 1
139:         STATIC_OPENSSL: 1
140:         EXTENSION_STATIC_BUILD: ${{ inputs.static_link_build }}
141:         OSX_BUILD_UNIVERSAL: ${{ inputs.osx_universal }}
142:         OSX_BUILD_ARCH: ${{ inputs.osx_arch }}
143:         DISABLE_BUILTIN_EXTENSIONS: ${{ inputs.no_static_linking}}
144:         CC: ${{ inputs.aarch64_cross_compile == 1 && 'aarch64-linux-gnu-gcc' || inputs.override_cc }}
145:         CXX: ${{ inputs.aarch64_cross_compile == 1 && 'aarch64-linux-gnu-g++' || inputs.override_cxx}}
146:         LOCAL_EXTENSION_REPO: ${{ inputs.run_autoload_tests == 1 && github.workspace || ''}}
147:         GEN: ${{ inputs.ninja == 1 && 'ninja' || '' }}
148:         USE_MERGED_VCPKG_MANIFEST: 1
149:         DUCKDB_PLATFORM: ${{ inputs.duckdb_arch }}
150:         CMAKE_VARS_BUILD: ${{ inputs.cmake_flags }}
151: 
152:       run: |
153:         ls
154:         mkdir -p ~/.ssh
155:         ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts
156:         cd  ${{ inputs.build_dir}}
157:         ls -al
158:         pwd
159:         echo "$USER"
160:         git config --global --add safe.directory '*'
161:         make
162:         ls
163: 
164:     # Tests extension using regular require mechanism:
165:     # - statically linked extensions are disable on startup but a sqlogictest require will call their load function
166:     # - loadable-only extensions have their loadable extension loaded with the LOAD statement on a sqlogictest require
167:     - name: Test statically linked extensions
168:       if: ${{ inputs.run_tests == 1 && inputs.no_static_linking == 0}}
169:       shell: bash
170:       run: |
171:         ${{ inputs.unittest_script }}
172: 
173:     - name: Run post-install scripts
174:       if: ${{ inputs.post_install != '' }}
175:       shell: bash
176:       run: |
177:         ls
178:         cd  ${{ inputs.build_dir}}
179:         ${{ inputs.post_install }}
180: 
181:     # The reason we need to rebuild is we need to test auto-loading extensions: this is only possible without the other
182:     # extensions linked
183:     - name: Rebuild DuckDB without any extensions, but with all extension tests
184:       if: ${{ inputs.run_autoload_tests == 1 }}
185:       shell: bash
186:       env:
187:         EXTENSION_TESTS_ONLY: 1
188:         ENABLE_EXTENSION_AUTOLOADING: 1
189:         ENABLE_EXTENSION_AUTOINSTALL: 1
190:         GEN: ${{ inputs.ninja == 1 && 'ninja' || '' }}
191:         USE_MERGED_VCPKG_MANIFEST: 1
192:       run: |
193:         cd  ${{ inputs.build_dir}}
194:         rm -rf duckdb_unittest_tempdir/*
195:         mv build/release/extension build/extension_tmp
196:         rm -rf build/release
197:         VCPKG_TOOLCHAIN_PATH="" make
198: 
199:     # Run all unittests (including the out-of-tree tests) without any extensions linked, relying on the autoloader
200:     - name: Run tests with auto loading
201:       if: ${{ inputs.run_autoload_tests == 1 }}
202:       shell: bash
203:       env:
204:         LOCAL_EXTENSION_REPO: ${{ inputs.run_autoload_tests == 1 && github.workspace || ''}}
205:       run: |
206:         cd  ${{ inputs.build_dir}}
207:         python3 scripts/get_test_list.py --file-contains 'require ' --list '"*.test"' > test.list
208:         python3 scripts/get_test_list.py --file-contains 'require-env LOCAL_EXTENSION_REPO' --list '"*.test"' >> test.list
209:         ${{ inputs.unittest_script }} '-f test.list'
210:         rm -rf build/release/extension
211:         mv build/extension_tmp build/release/extension
212: 
213:     - name: Deploy
214:       if: ${{ inputs.deploy_as != '' }}
215:       shell: bash
216:       env:
217:         AWS_ACCESS_KEY_ID: ${{ inputs.s3_id }}
218:         AWS_SECRET_ACCESS_KEY: ${{ inputs.s3_key }}
219:         DUCKDB_EXTENSION_SIGNING_PK: ${{ inputs.signing_pk }}
220:         AWS_DEFAULT_REGION: us-east-1
221:         DUCKDB_DEPLOY_SCRIPT_MODE: for_real
222:       run: |
223:         cd  ${{ inputs.build_dir}}
224:         if [[ "$GITHUB_REPOSITORY" = "duckdb/duckdb" ]] ; then
225:           if [[ ! -z "${{ inputs.deploy_version }}" ]] ; then
226:             ./scripts/extension-upload-all.sh ${{ inputs.deploy_as }} ${{ inputs.deploy_version }}
227:           elif [[ "$GITHUB_REF" =~ ^(refs/tags/v.+)$ ]] ; then
228:             ./scripts/extension-upload-all.sh ${{ inputs.deploy_as }} ${{ github.ref_name }}
229:           elif [[ "$GITHUB_REF" =~ ^(refs/heads/main)$ ]] ; then
230:             ./scripts/extension-upload-all.sh ${{ inputs.deploy_as }} `git log -1 --format=%h`
231:           fi
232:         fi
233: 
234:     # Run the unittests (excluding the out-of-tree tests) with the extensions that we deployed to S3
235:     - name: Test deployed extensions
236:       if: ${{ inputs.deploy_as != '' && inputs.run_tests == 1 }}
237:       shell: bash
238:       env:
239:         AWS_ACCESS_KEY_ID: ${{ inputs.s3_id }}
240:         AWS_SECRET_ACCESS_KEY: ${{ inputs.s3_key }}
241:         AWS_DEFAULT_REGION: us-east-1
242:       run: |
243:         rm -rf ~/.duckdb
244:         cd  ${{ inputs.build_dir}}
245:         if [[ "$GITHUB_REF" =~ ^(refs/heads/main|refs/tags/v.+)$ && "$GITHUB_REPOSITORY" = "duckdb/duckdb" ]] ; then
246:           ./scripts/extension-upload-test.sh
247:         fi
[end of .github/actions/build_extensions/action.yml]
[start of .github/config/out_of_tree_extensions.cmake]
1: #
2: # This config file holds all out-of-tree extension that are built with DuckDB's CI
3: #
4: # to build duckdb with this configuration run:
5: #   EXTENSION_CONFIGS=.github/config/out_of_tree_extensions.cmake make
6: #
7: #  Note that many of these packages require vcpkg, and a merged manifest must be created to
8: #  compile multiple of them.
9: #
10: #  After setting up vcpkg, build using e.g. the following commands:
11: #  USE_MERGED_VCPKG_MANIFEST=1 BUILD_ALL_EXT=1 make extension_configuration
12: #  USE_MERGED_VCPKG_MANIFEST=1 BUILD_ALL_EXT=1 make debug
13: #
14: #  Make sure the VCPKG_TOOLCHAIN_PATH and VCPKG_TARGET_TRIPLET are set. For example:
15: #  VCPKG_TOOLCHAIN_PATH=~/vcpkg/scripts/buildsystems/vcpkg.cmake
16: #  VCPKG_TARGET_TRIPLET=arm64-osx
17: 
18: ################# HTTPFS
19: duckdb_extension_load(httpfs
20:     LOAD_TESTS
21:     GIT_URL https://github.com/duckdb/duckdb-httpfs
22:     GIT_TAG 85ac4667bcb0d868199e156f8dd918b0278db7b9
23:     INCLUDE_DIR extension/httpfs/include
24:     )
25: 
26: ################# ARROW
27: if (NOT MINGW AND NOT ${WASM_ENABLED} AND NOT ${MUSL_ENABLED})
28:     duckdb_extension_load(arrow
29:             LOAD_TESTS DONT_LINK
30:             GIT_URL https://github.com/duckdb/arrow
31:             GIT_TAG cff2f0e21b1608e38640e15b4cf0693dd52dd0eb
32:             )
33: endif()
34: 
35: ################## AWS
36: if (NOT MINGW AND NOT ${WASM_ENABLED})
37:     duckdb_extension_load(aws
38:             LOAD_TESTS
39:             GIT_URL https://github.com/duckdb/duckdb-aws
40:             GIT_TAG b3050f35c6e99fa35465230493eeab14a78a0409
41:             )
42: endif()
43: 
44: ################# AZURE
45: if (NOT MINGW AND NOT ${WASM_ENABLED})
46:     duckdb_extension_load(azure
47:             LOAD_TESTS
48:             GIT_URL https://github.com/duckdb/duckdb-azure
49:             GIT_TAG e707cf361d76358743969cddf3acf97cfc87677b
50:             )
51: endif()
52: 
53: ################# DELTA
54: # MinGW build is not available, and our current manylinux ci does not have enough storage space to run the rust build
55: # for Delta
56: if (NOT MINGW AND NOT "${OS_NAME}" STREQUAL "linux" AND NOT ${WASM_ENABLED})
57:     duckdb_extension_load(delta
58:             GIT_URL https://github.com/duckdb/duckdb-delta
59:             GIT_TAG 846019edcc27000721ff9c4281e85a63d1aa10de
60:     )
61: endif()
62: 
63: ################# EXCEL
64: duckdb_extension_load(excel
65:     LOAD_TESTS
66:     GIT_URL https://github.com/duckdb/duckdb-excel
67:     GIT_TAG e243577956f36a898d5485189e5288839c2c4b73
68:     INCLUDE_DIR src/excel/include
69:     )
70: 
71: ################# ICEBERG
72: # Windows tests for iceberg currently not working
73: if (NOT WIN32)
74:     set(LOAD_ICEBERG_TESTS "LOAD_TESTS")
75: else ()
76:     set(LOAD_ICEBERG_TESTS "")
77: endif()
78: 
79: if (NOT MINGW AND NOT ${WASM_ENABLED} AND NOT ${MUSL_ENABLED})
80:     duckdb_extension_load(iceberg
81: #            ${LOAD_ICEBERG_TESTS} TODO: re-enable once autoloading test is fixed
82:             GIT_URL https://github.com/duckdb/duckdb-iceberg
83:             GIT_TAG 43b4e37f6e859d6c1c67b787ac511659e9e0b6fb
84:             )
85: endif()
86: 
87: ################# INET
88: duckdb_extension_load(inet
89:     LOAD_TESTS
90:     GIT_URL https://github.com/duckdb/duckdb-inet
91:     GIT_TAG a8b361ab5d43f6390d7cb48c9a9f0638e9581cf9
92:     INCLUDE_DIR src/include
93:     TEST_DIR test/sql
94:     )
95: 
96: ################# POSTGRES_SCANNER
97: # Note: tests for postgres_scanner are currently not run. All of them need a postgres server running. One test
98: #       uses a remote rds server but that's not something we want to run here.
99: if (NOT MINGW AND NOT ${WASM_ENABLED})
100:     duckdb_extension_load(postgres_scanner
101:             DONT_LINK
102:             GIT_URL https://github.com/duckdb/duckdb-postgres
103:             GIT_TAG 79fcce4a7478d245189d851ce289def2b42f4f93
104:             )
105: endif()
106: 
107: # mingw CI with all extensions at once is somehow not happy
108: if (NOT MINGW)
109: ################# SPATIAL
110: duckdb_extension_load(spatial
111:     DONT_LINK LOAD_TESTS
112:     GIT_URL https://github.com/duckdb/duckdb-spatial
113:     GIT_TAG 79bf2b6f55db3bf7201f375662616663b4b0954a
114:     INCLUDE_DIR spatial/include
115:     TEST_DIR test/sql
116:     )
117: endif()
118: 
119: ################# SQLITE_SCANNER
120: # Static linking on windows does not properly work due to symbol collision
121: if (WIN32)
122:     set(STATIC_LINK_SQLITE "DONT_LINK")
123: else ()
124:     set(STATIC_LINK_SQLITE "")
125: endif()
126: 
127: duckdb_extension_load(sqlite_scanner
128:         ${STATIC_LINK_SQLITE} LOAD_TESTS
129:         GIT_URL https://github.com/duckdb/duckdb-sqlite
130:         GIT_TAG 96e451c043afa40ee39b7581009ba0c72a523a12
131:         )
132: 
133: duckdb_extension_load(sqlsmith
134:         DONT_LINK LOAD_TESTS
135:         GIT_URL https://github.com/duckdb/duckdb-sqlsmith
136:         GIT_TAG b13723fe701f1e38d2cd65b3b6eb587c6553a251
137:         )
138: 
139: ################# VSS
140: duckdb_extension_load(vss
141:         LOAD_TESTS
142:         DONT_LINK
143:         GIT_URL https://github.com/duckdb/duckdb-vss
144:         GIT_TAG 580e8918eb89f478cf2d233ca908ffbd3ec752c5
145:         TEST_DIR test/sql
146:     )
147: 
148: ################# MYSQL
149: if (NOT MINGW AND NOT ${WASM_ENABLED} AND NOT ${MUSL_ENABLED})
150:     duckdb_extension_load(mysql_scanner
151:             DONT_LINK
152:             LOAD_TESTS
153:             GIT_URL https://github.com/duckdb/duckdb-mysql
154:             GIT_TAG c2a56813a9fe9cb8c24c424be646d41ab2f8e64f
155:             )
156: endif()
157: 
158: ################# FTS
159: duckdb_extension_load(fts
160:         LOAD_TESTS
161:         DONT_LINK
162:         GIT_URL https://github.com/duckdb/duckdb-fts
163:         GIT_TAG 0477abaf2484aa7b9aabf8ace9dc0bde80a15554
164:         TEST_DIR test/sql
165: )
[end of .github/config/out_of_tree_extensions.cmake]
[start of .github/workflows/BundleStaticLibs.yml]
1: name: Bundle Static Libraries
2: on:
3:   workflow_call:
4:     inputs:
5:       override_git_describe:
6:         type: string
7:       git_ref:
8:         type: string
9:       skip_tests:
10:         type: string
11:   workflow_dispatch:
12:     inputs:
13:       override_git_describe:
14:         type: string
15:       git_ref:
16:         type: string
17:       skip_tests:
18:         type: string
19:   push:
20:     branches-ignore:
21:       - 'main'
22:       - 'feature'
23:       - 'v*.*-*'
24:     paths-ignore:
25:       - '**'
26:       - '!.github/workflows/BundleStaticLibs.yml'
27:   pull_request:
28:     types: [opened, reopened, ready_for_review]
29:     paths-ignore:
30:       - '**'
31:       - '!.github/workflows/BundleStaticLibs.yml'
32: 
33: concurrency:
34:   group: ${{ github.workflow }}-${{ github.ref }}-${{ github.head_ref || '' }}-${{ github.base_ref || '' }}-${{ github.ref != 'refs/heads/main' || github.sha }}-${{ inputs.override_git_describe }}
35:   cancel-in-progress: true
36: 
37: env:
38:   GH_TOKEN: ${{ secrets.GH_TOKEN }}
39:   OVERRIDE_GIT_DESCRIBE: ${{ inputs.override_git_describe }}
40: 
41: jobs:
42:   bundle-osx-static-libs:
43:     name: OSX static libs
44:     strategy:
45:       matrix:
46:         include:
47:           - version: "macos-13"
48:             architecture: "amd64"
49:           - version: "macos-14"
50:             architecture: "arm64"
51:     runs-on: ${{ matrix.version }}
52:     env:
53:       EXTENSION_CONFIGS: '${GITHUB_WORKSPACE}/.github/config/bundled_extensions.cmake'
54:       ENABLE_EXTENSION_AUTOLOADING: 1
55:       ENABLE_EXTENSION_AUTOINSTALL: 1
56:       GEN: ninja
57: 
58:     steps:
59:       - uses: actions/checkout@v4
60:         with:
61:           fetch-depth: 0
62:           ref: ${{ inputs.git_ref }}
63: 
64:       - uses: actions/setup-python@v5
65:         with:
66:           python-version: '3.12'
67: 
68:       - name: Install Ninja
69:         run: brew install ninja
70: 
71:       - name: Setup Ccache
72:         uses: hendrikmuhs/ccache-action@main
73:         with:
74:           key: ${{ github.job }}
75:           save: ${{ github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' }}
76: 
77:       - name: Build
78:         shell: bash
79:         run: make
80: 
81:       - name: Bundle static library
82:         shell: bash
83:         run: make bundle-library-o
84: 
85:       - name: Print platform
86:         shell: bash
87:         run: ./build/release/duckdb -c "PRAGMA platform;"
88: 
89:       - name: Deploy
90:         shell: bash
91:         env:
92:           AWS_ACCESS_KEY_ID: ${{ secrets.S3_DUCKDB_STAGING_ID }}
93:           AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_DUCKDB_STAGING_KEY }}
94:         run: |
95:           python3 scripts/amalgamation.py
96:           zip -j static-lib-osx-${{ matrix.architecture }}.zip src/include/duckdb.h build/release/libduckdb_bundle.a
97:           ./scripts/upload-assets-to-staging.sh github_release static-lib-osx-${{ matrix.architecture }}.zip
98: 
99:       - uses: actions/upload-artifact@v4
100:         with:
101:           name: duckdb-static-lib-osx-${{ matrix.architecture }}
102:           path: |
103:             static-lib-osx-${{ matrix.architecture }}.zip
104: 
105:   bundle-mingw-static-lib:
106:     name: Windows MingW static libs
107:     runs-on: windows-2019
108:     steps:
109:       - uses: actions/checkout@v4
110:         with:
111:           fetch-depth: 0
112:           ref: ${{ inputs.git_ref }}
113: 
114:       - uses: actions/setup-python@v5
115:         with:
116:           python-version: '3.12'
117: 
118:       - uses: r-lib/actions/setup-r@v2
119:         with:
120:           r-version: 'devel'
121:           update-rtools: true
122:           rtools-version: '42' # linker bug in 43 ^^
123: 
124:       - uses: ./.github/actions/build_extensions
125:         with:
126:           duckdb_arch: windows_amd64_mingw
127:           vcpkg_target_triplet: x64-mingw-static
128:           treat_warn_as_error: 0
129:           override_cc: gcc
130:           override_cxx: g++
131:           vcpkg_build: 1
132:           no_static_linking: 1
133:           run_tests: 0
134:           run_autoload_tests: 0
135: 
136:       - name: Bundle static library
137:         shell: bash
138:         run: |
139:           make bundle-library-obj
140: 
141:       - name: Deploy
142:         shell: bash
143:         env:
144:           AWS_ACCESS_KEY_ID: ${{ secrets.S3_DUCKDB_STAGING_ID }}
145:           AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_DUCKDB_STAGING_KEY }}
146:         run: |
147:           zip -j static-lib-windows-mingw.zip src/include/duckdb.h build/release/libduckdb_bundle.a
148:           ./scripts/upload-assets-to-staging.sh github_release static-lib-windows-mingw.zip
149: 
150:       - uses: actions/upload-artifact@v4
151:         with:
152:           name: duckdb-static-lib-windows-mingw
153:           path: |
154:             static-lib-windows-mingw.zip
155:   bundle-linux-arm64-static-libs:
156:     name: Linux arm64 static libs
157:     runs-on: ubuntu-latest
158: 
159:     steps:
160:       - uses: actions/checkout@v4
161:         with:
162:           fetch-depth: 0
163:           ref: ${{ inputs.git_ref }}
164: 
165:       - name: Build
166:         shell: bash
167:         run: |
168:           docker run                                                             \
169:           -v.:/duckdb                                                            \
170:           -e CC=aarch64-linux-gnu-gcc                                            \
171:           -e CXX=aarch64-linux-gnu-g++                                           \
172:           -e CMAKE_BUILD_PARALLEL_LEVEL=2                                        \
173:           -e OVERRIDE_GIT_DESCRIBE=$OVERRIDE_GIT_DESCRIBE                        \
174:           -e EXTENSION_CONFIGS='/duckdb/.github/config/bundled_extensions.cmake' \
175:           -e ENABLE_EXTENSION_AUTOLOADING=1                                      \
176:           -e ENABLE_EXTENSION_AUTOINSTALL=1                                      \
177:           -e FORCE_WARN_UNUSED=1                                                 \
178:           -e DUCKDB_PLATFORM=linux_arm64                                         \
179:           ubuntu:18.04                                                           \
180:           bash -c "/duckdb/scripts/setup_ubuntu1804.sh && git config --global --add safe.directory /duckdb && make bundle-library -C /duckdb"
181:       - name: Deploy
182:         shell: bash
183:         env:
184:           AWS_ACCESS_KEY_ID: ${{ secrets.S3_DUCKDB_STAGING_ID }}
185:           AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_DUCKDB_STAGING_KEY }}
186:         run: |
187:           python3 scripts/amalgamation.py
188:           zip -j static-lib-linux-arm64.zip src/include/duckdb.h build/release/libduckdb_bundle.a
189:           ./scripts/upload-assets-to-staging.sh github_release static-lib-linux-arm64.zip
190:       - uses: actions/upload-artifact@v4
191:         with:
192:           name: duckdb-static-lib-linux-arm64
193:           path: |
194:             static-lib-linux-arm64.zip
195:   bundle-linux-amd64-static-libs:
196:     name: Linux amd64 static libs
197:     runs-on: ubuntu-latest
198: 
199:     steps:
200:       - uses: actions/checkout@v4
201:         with:
202:           fetch-depth: 0
203:           ref: ${{ inputs.git_ref }}
204: 
205:       - name: Install pytest
206:         run: |
207:           python3 -m pip install pytest
208:       - name: Build
209:         shell: bash
210:         run: |
211:           export PWD=`pwd`
212:           docker run                                                             \
213:           -v$PWD:$PWD                                                            \
214:           -e CMAKE_BUILD_PARALLEL_LEVEL=2                                        \
215:           -e OVERRIDE_GIT_DESCRIBE=$OVERRIDE_GIT_DESCRIBE                        \
216:           -e EXTENSION_CONFIGS="$PWD/.github/config/bundled_extensions.cmake"    \
217:           -e ENABLE_EXTENSION_AUTOLOADING=1                                      \
218:           -e ENABLE_EXTENSION_AUTOINSTALL=1                                      \
219:           -e BUILD_BENCHMARK=1                                                   \
220:           -e FORCE_WARN_UNUSED=1                                                 \
221:           quay.io/pypa/manylinux2014_x86_64                                      \
222:           bash -c "yum install -y perl-IPC-Cmd && git config --global --add safe.directory $PWD && make bundle-library -C $PWD"
223:       - name: Print platform
224:         shell: bash
225:         run: ./build/release/duckdb -c "PRAGMA platform;"
226: 
227:       - name: Deploy
228:         shell: bash
229:         env:
230:           AWS_ACCESS_KEY_ID: ${{ secrets.S3_DUCKDB_STAGING_ID }}
231:           AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_DUCKDB_STAGING_KEY }}
232:         run: |
233:           python3 scripts/amalgamation.py
234:           zip -j static-lib-linux-amd64.zip src/include/duckdb.h build/release/libduckdb_bundle.a
235:           ./scripts/upload-assets-to-staging.sh github_release static-lib-linux-amd64.zip
236:       - uses: actions/upload-artifact@v4
237:         with:
238:           name: duckdb-static-lib-linux-amd64
239:           path: |
240:             static-lib-linux-amd64.zip
[end of .github/workflows/BundleStaticLibs.yml]
[start of extension/core_functions/aggregate/distributive/string_agg.cpp]
1: #include "core_functions/aggregate/distributive_functions.hpp"
2: #include "duckdb/common/exception.hpp"
3: #include "duckdb/common/types/null_value.hpp"
4: #include "duckdb/common/vector_operations/vector_operations.hpp"
5: #include "duckdb/common/algorithm.hpp"
6: #include "duckdb/execution/expression_executor.hpp"
7: #include "duckdb/planner/expression/bound_constant_expression.hpp"
8: #include "duckdb/common/serializer/serializer.hpp"
9: #include "duckdb/common/serializer/deserializer.hpp"
10: 
11: namespace duckdb {
12: 
13: struct StringAggState {
14: 	idx_t size;
15: 	idx_t alloc_size;
16: 	char *dataptr;
17: };
18: 
19: struct StringAggBindData : public FunctionData {
20: 	explicit StringAggBindData(string sep_p) : sep(std::move(sep_p)) {
21: 	}
22: 
23: 	string sep;
24: 
25: 	unique_ptr<FunctionData> Copy() const override {
26: 		return make_uniq<StringAggBindData>(sep);
27: 	}
28: 	bool Equals(const FunctionData &other_p) const override {
29: 		auto &other = other_p.Cast<StringAggBindData>();
30: 		return sep == other.sep;
31: 	}
32: };
33: 
34: struct StringAggFunction {
35: 	template <class STATE>
36: 	static void Initialize(STATE &state) {
37: 		state.dataptr = nullptr;
38: 		state.alloc_size = 0;
39: 		state.size = 0;
40: 	}
41: 
42: 	template <class T, class STATE>
43: 	static void Finalize(STATE &state, T &target, AggregateFinalizeData &finalize_data) {
44: 		if (!state.dataptr) {
45: 			finalize_data.ReturnNull();
46: 		} else {
47: 			target = StringVector::AddString(finalize_data.result, state.dataptr, state.size);
48: 		}
49: 	}
50: 
51: 	template <class STATE>
52: 	static void Destroy(STATE &state, AggregateInputData &aggr_input_data) {
53: 		if (state.dataptr) {
54: 			delete[] state.dataptr;
55: 		}
56: 	}
57: 
58: 	static bool IgnoreNull() {
59: 		return true;
60: 	}
61: 
62: 	static inline void PerformOperation(StringAggState &state, const char *str, const char *sep, idx_t str_size,
63: 	                                    idx_t sep_size) {
64: 		if (!state.dataptr) {
65: 			// first iteration: allocate space for the string and copy it into the state
66: 			state.alloc_size = MaxValue<idx_t>(8, NextPowerOfTwo(str_size));
67: 			state.dataptr = new char[state.alloc_size];
68: 			state.size = str_size;
69: 			memcpy(state.dataptr, str, str_size);
70: 		} else {
71: 			// subsequent iteration: first check if we have space to place the string and separator
72: 			idx_t required_size = state.size + str_size + sep_size;
73: 			if (required_size > state.alloc_size) {
74: 				// no space! allocate extra space
75: 				while (state.alloc_size < required_size) {
76: 					state.alloc_size *= 2;
77: 				}
78: 				auto new_data = new char[state.alloc_size];
79: 				memcpy(new_data, state.dataptr, state.size);
80: 				delete[] state.dataptr;
81: 				state.dataptr = new_data;
82: 			}
83: 			// copy the separator
84: 			memcpy(state.dataptr + state.size, sep, sep_size);
85: 			state.size += sep_size;
86: 			// copy the string
87: 			memcpy(state.dataptr + state.size, str, str_size);
88: 			state.size += str_size;
89: 		}
90: 	}
91: 
92: 	static inline void PerformOperation(StringAggState &state, string_t str, optional_ptr<FunctionData> data_p) {
93: 		auto &data = data_p->Cast<StringAggBindData>();
94: 		PerformOperation(state, str.GetData(), data.sep.c_str(), str.GetSize(), data.sep.size());
95: 	}
96: 
97: 	template <class INPUT_TYPE, class STATE, class OP>
98: 	static void Operation(STATE &state, const INPUT_TYPE &input, AggregateUnaryInput &unary_input) {
99: 		PerformOperation(state, input, unary_input.input.bind_data);
100: 	}
101: 
102: 	template <class INPUT_TYPE, class STATE, class OP>
103: 	static void ConstantOperation(STATE &state, const INPUT_TYPE &input, AggregateUnaryInput &unary_input,
104: 	                              idx_t count) {
105: 		for (idx_t i = 0; i < count; i++) {
106: 			Operation<INPUT_TYPE, STATE, OP>(state, input, unary_input);
107: 		}
108: 	}
109: 
110: 	template <class STATE, class OP>
111: 	static void Combine(const STATE &source, STATE &target, AggregateInputData &aggr_input_data) {
112: 		if (!source.dataptr) {
113: 			// source is not set: skip combining
114: 			return;
115: 		}
116: 		PerformOperation(target, string_t(source.dataptr, UnsafeNumericCast<uint32_t>(source.size)),
117: 		                 aggr_input_data.bind_data);
118: 	}
119: };
120: 
121: unique_ptr<FunctionData> StringAggBind(ClientContext &context, AggregateFunction &function,
122:                                        vector<unique_ptr<Expression>> &arguments) {
123: 	if (arguments.size() == 1) {
124: 		// single argument: default to comma
125: 		return make_uniq<StringAggBindData>(",");
126: 	}
127: 	D_ASSERT(arguments.size() == 2);
128: 	if (arguments[1]->HasParameter()) {
129: 		throw ParameterNotResolvedException();
130: 	}
131: 	if (!arguments[1]->IsFoldable()) {
132: 		throw BinderException("Separator argument to StringAgg must be a constant");
133: 	}
134: 	auto separator_val = ExpressionExecutor::EvaluateScalar(context, *arguments[1]);
135: 	string separator_string = ",";
136: 	if (separator_val.IsNull()) {
137: 		arguments[0] = make_uniq<BoundConstantExpression>(Value(LogicalType::VARCHAR));
138: 	} else {
139: 		separator_string = separator_val.ToString();
140: 	}
141: 	Function::EraseArgument(function, arguments, arguments.size() - 1);
142: 	return make_uniq<StringAggBindData>(std::move(separator_string));
143: }
144: 
145: static void StringAggSerialize(Serializer &serializer, const optional_ptr<FunctionData> bind_data_p,
146:                                const AggregateFunction &function) {
147: 	auto bind_data = bind_data_p->Cast<StringAggBindData>();
148: 	serializer.WriteProperty(100, "separator", bind_data.sep);
149: }
150: 
151: unique_ptr<FunctionData> StringAggDeserialize(Deserializer &deserializer, AggregateFunction &bound_function) {
152: 	auto sep = deserializer.ReadProperty<string>(100, "separator");
153: 	return make_uniq<StringAggBindData>(std::move(sep));
154: }
155: 
156: AggregateFunctionSet StringAggFun::GetFunctions() {
157: 	AggregateFunctionSet string_agg;
158: 	AggregateFunction string_agg_param(
159: 	    {LogicalType::ANY_PARAMS(LogicalType::VARCHAR)}, LogicalType::VARCHAR,
160: 	    AggregateFunction::StateSize<StringAggState>,
161: 	    AggregateFunction::StateInitialize<StringAggState, StringAggFunction>,
162: 	    AggregateFunction::UnaryScatterUpdate<StringAggState, string_t, StringAggFunction>,
163: 	    AggregateFunction::StateCombine<StringAggState, StringAggFunction>,
164: 	    AggregateFunction::StateFinalize<StringAggState, string_t, StringAggFunction>,
165: 	    AggregateFunction::UnaryUpdate<StringAggState, string_t, StringAggFunction>, StringAggBind,
166: 	    AggregateFunction::StateDestroy<StringAggState, StringAggFunction>);
167: 	string_agg_param.serialize = StringAggSerialize;
168: 	string_agg_param.deserialize = StringAggDeserialize;
169: 	string_agg.AddFunction(string_agg_param);
170: 	string_agg_param.arguments.emplace_back(LogicalType::VARCHAR);
171: 	string_agg.AddFunction(string_agg_param);
172: 	return string_agg;
173: }
174: 
175: } // namespace duckdb
[end of extension/core_functions/aggregate/distributive/string_agg.cpp]
[start of extension/core_functions/aggregate/nested/list.cpp]
1: #include "duckdb/common/pair.hpp"
2: #include "duckdb/common/types/list_segment.hpp"
3: #include "core_functions/aggregate/nested_functions.hpp"
4: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
5: 
6: namespace duckdb {
7: 
8: struct ListBindData : public FunctionData {
9: 	explicit ListBindData(const LogicalType &stype_p);
10: 	~ListBindData() override;
11: 
12: 	LogicalType stype;
13: 	ListSegmentFunctions functions;
14: 
15: 	unique_ptr<FunctionData> Copy() const override {
16: 		return make_uniq<ListBindData>(stype);
17: 	}
18: 
19: 	bool Equals(const FunctionData &other_p) const override {
20: 		auto &other = other_p.Cast<ListBindData>();
21: 		return stype == other.stype;
22: 	}
23: };
24: 
25: ListBindData::ListBindData(const LogicalType &stype_p) : stype(stype_p) {
26: 	// always unnest once because the result vector is of type LIST
27: 	auto type = ListType::GetChildType(stype_p);
28: 	GetSegmentDataFunctions(functions, type);
29: }
30: 
31: ListBindData::~ListBindData() {
32: }
33: 
34: struct ListAggState {
35: 	LinkedList linked_list;
36: };
37: 
38: struct ListFunction {
39: 	template <class STATE>
40: 	static void Initialize(STATE &state) {
41: 		state.linked_list.total_capacity = 0;
42: 		state.linked_list.first_segment = nullptr;
43: 		state.linked_list.last_segment = nullptr;
44: 	}
45: 	static bool IgnoreNull() {
46: 		return false;
47: 	}
48: };
49: 
50: static void ListUpdateFunction(Vector inputs[], AggregateInputData &aggr_input_data, idx_t input_count,
51:                                Vector &state_vector, idx_t count) {
52: 
53: 	D_ASSERT(input_count == 1);
54: 	auto &input = inputs[0];
55: 	RecursiveUnifiedVectorFormat input_data;
56: 	Vector::RecursiveToUnifiedFormat(input, count, input_data);
57: 
58: 	UnifiedVectorFormat states_data;
59: 	state_vector.ToUnifiedFormat(count, states_data);
60: 	auto states = UnifiedVectorFormat::GetData<ListAggState *>(states_data);
61: 
62: 	auto &list_bind_data = aggr_input_data.bind_data->Cast<ListBindData>();
63: 
64: 	for (idx_t i = 0; i < count; i++) {
65: 		auto &state = *states[states_data.sel->get_index(i)];
66: 		aggr_input_data.allocator.AlignNext();
67: 		list_bind_data.functions.AppendRow(aggr_input_data.allocator, state.linked_list, input_data, i);
68: 	}
69: }
70: 
71: static void ListAbsorbFunction(Vector &states_vector, Vector &combined, AggregateInputData &aggr_input_data,
72:                                idx_t count) {
73: 	D_ASSERT(aggr_input_data.combine_type == AggregateCombineType::ALLOW_DESTRUCTIVE);
74: 
75: 	UnifiedVectorFormat states_data;
76: 	states_vector.ToUnifiedFormat(count, states_data);
77: 	auto states_ptr = UnifiedVectorFormat::GetData<ListAggState *>(states_data);
78: 
79: 	auto combined_ptr = FlatVector::GetData<ListAggState *>(combined);
80: 	for (idx_t i = 0; i < count; i++) {
81: 
82: 		auto &state = *states_ptr[states_data.sel->get_index(i)];
83: 		if (state.linked_list.total_capacity == 0) {
84: 			// NULL, no need to append
85: 			// this can happen when adding a FILTER to the grouping, e.g.,
86: 			// LIST(i) FILTER (WHERE i <> 3)
87: 			continue;
88: 		}
89: 
90: 		if (combined_ptr[i]->linked_list.total_capacity == 0) {
91: 			combined_ptr[i]->linked_list = state.linked_list;
92: 			continue;
93: 		}
94: 
95: 		// append the linked list
96: 		combined_ptr[i]->linked_list.last_segment->next = state.linked_list.first_segment;
97: 		combined_ptr[i]->linked_list.last_segment = state.linked_list.last_segment;
98: 		combined_ptr[i]->linked_list.total_capacity += state.linked_list.total_capacity;
99: 	}
100: }
101: 
102: static void ListFinalize(Vector &states_vector, AggregateInputData &aggr_input_data, Vector &result, idx_t count,
103:                          idx_t offset) {
104: 
105: 	UnifiedVectorFormat states_data;
106: 	states_vector.ToUnifiedFormat(count, states_data);
107: 	auto states = UnifiedVectorFormat::GetData<ListAggState *>(states_data);
108: 
109: 	D_ASSERT(result.GetType().id() == LogicalTypeId::LIST);
110: 
111: 	auto &mask = FlatVector::Validity(result);
112: 	auto result_data = FlatVector::GetData<list_entry_t>(result);
113: 	size_t total_len = ListVector::GetListSize(result);
114: 
115: 	auto &list_bind_data = aggr_input_data.bind_data->Cast<ListBindData>();
116: 
117: 	// first iterate over all entries and set up the list entries, and get the newly required total length
118: 	for (idx_t i = 0; i < count; i++) {
119: 
120: 		auto &state = *states[states_data.sel->get_index(i)];
121: 		const auto rid = i + offset;
122: 		result_data[rid].offset = total_len;
123: 		if (state.linked_list.total_capacity == 0) {
124: 			mask.SetInvalid(rid);
125: 			result_data[rid].length = 0;
126: 			continue;
127: 		}
128: 
129: 		// set the length and offset of this list in the result vector
130: 		auto total_capacity = state.linked_list.total_capacity;
131: 		result_data[rid].length = total_capacity;
132: 		total_len += total_capacity;
133: 	}
134: 
135: 	// reserve capacity, then iterate over all entries again and copy over the data to the child vector
136: 	ListVector::Reserve(result, total_len);
137: 	auto &result_child = ListVector::GetEntry(result);
138: 	for (idx_t i = 0; i < count; i++) {
139: 
140: 		auto &state = *states[states_data.sel->get_index(i)];
141: 		const auto rid = i + offset;
142: 		if (state.linked_list.total_capacity == 0) {
143: 			continue;
144: 		}
145: 
146: 		idx_t current_offset = result_data[rid].offset;
147: 		list_bind_data.functions.BuildListVector(state.linked_list, result_child, current_offset);
148: 	}
149: 
150: 	ListVector::SetListSize(result, total_len);
151: }
152: 
153: static void ListCombineFunction(Vector &states_vector, Vector &combined, AggregateInputData &aggr_input_data,
154:                                 idx_t count) {
155: 
156: 	//	Can we use destructive combining?
157: 	if (aggr_input_data.combine_type == AggregateCombineType::ALLOW_DESTRUCTIVE) {
158: 		ListAbsorbFunction(states_vector, combined, aggr_input_data, count);
159: 		return;
160: 	}
161: 
162: 	UnifiedVectorFormat states_data;
163: 	states_vector.ToUnifiedFormat(count, states_data);
164: 	auto states_ptr = UnifiedVectorFormat::GetData<const ListAggState *>(states_data);
165: 	auto combined_ptr = FlatVector::GetData<ListAggState *>(combined);
166: 
167: 	auto &list_bind_data = aggr_input_data.bind_data->Cast<ListBindData>();
168: 	auto result_type = ListType::GetChildType(list_bind_data.stype);
169: 
170: 	for (idx_t i = 0; i < count; i++) {
171: 		auto &source = *states_ptr[states_data.sel->get_index(i)];
172: 		auto &target = *combined_ptr[i];
173: 
174: 		const auto entry_count = source.linked_list.total_capacity;
175: 		Vector input(result_type, source.linked_list.total_capacity);
176: 		list_bind_data.functions.BuildListVector(source.linked_list, input, 0);
177: 
178: 		RecursiveUnifiedVectorFormat input_data;
179: 		Vector::RecursiveToUnifiedFormat(input, entry_count, input_data);
180: 
181: 		for (idx_t entry_idx = 0; entry_idx < entry_count; ++entry_idx) {
182: 			aggr_input_data.allocator.AlignNext();
183: 			list_bind_data.functions.AppendRow(aggr_input_data.allocator, target.linked_list, input_data, entry_idx);
184: 		}
185: 	}
186: }
187: 
188: unique_ptr<FunctionData> ListBindFunction(ClientContext &context, AggregateFunction &function,
189:                                           vector<unique_ptr<Expression>> &arguments) {
190: 	D_ASSERT(arguments.size() == 1);
191: 	D_ASSERT(function.arguments.size() == 1);
192: 
193: 	if (arguments[0]->return_type.id() == LogicalTypeId::UNKNOWN) {
194: 		function.arguments[0] = LogicalTypeId::UNKNOWN;
195: 		function.return_type = LogicalType::SQLNULL;
196: 		return nullptr;
197: 	}
198: 
199: 	function.return_type = LogicalType::LIST(arguments[0]->return_type);
200: 	return make_uniq<ListBindData>(function.return_type);
201: }
202: 
203: AggregateFunction ListFun::GetFunction() {
204: 	auto func =
205: 	    AggregateFunction({LogicalType::ANY}, LogicalTypeId::LIST, AggregateFunction::StateSize<ListAggState>,
206: 	                      AggregateFunction::StateInitialize<ListAggState, ListFunction>, ListUpdateFunction,
207: 	                      ListCombineFunction, ListFinalize, nullptr, ListBindFunction, nullptr, nullptr, nullptr);
208: 
209: 	return func;
210: }
211: 
212: } // namespace duckdb
[end of extension/core_functions/aggregate/nested/list.cpp]
[start of extension/core_functions/scalar/list/list_aggregates.cpp]
1: #include "core_functions/scalar/list_functions.hpp"
2: #include "core_functions/aggregate/nested_functions.hpp"
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/aggregate_function_catalog_entry.hpp"
5: #include "duckdb/execution/expression_executor.hpp"
6: #include "duckdb/function/scalar/nested_functions.hpp"
7: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
8: #include "duckdb/planner/expression/bound_constant_expression.hpp"
9: #include "duckdb/planner/expression/bound_function_expression.hpp"
10: #include "duckdb/planner/expression/bound_cast_expression.hpp"
11: #include "duckdb/planner/expression_binder.hpp"
12: #include "duckdb/function/function_binder.hpp"
13: #include "duckdb/function/create_sort_key.hpp"
14: #include "duckdb/common/owning_string_map.hpp"
15: 
16: namespace duckdb {
17: 
18: // FIXME: use a local state for each thread to increase performance?
19: // FIXME: benchmark the use of simple_update against using update (if applicable)
20: 
21: static unique_ptr<FunctionData> ListAggregatesBindFailure(ScalarFunction &bound_function) {
22: 	bound_function.arguments[0] = LogicalType::SQLNULL;
23: 	bound_function.return_type = LogicalType::SQLNULL;
24: 	return make_uniq<VariableReturnBindData>(LogicalType::SQLNULL);
25: }
26: 
27: struct ListAggregatesBindData : public FunctionData {
28: 	ListAggregatesBindData(const LogicalType &stype_p, unique_ptr<Expression> aggr_expr_p);
29: 	~ListAggregatesBindData() override;
30: 
31: 	LogicalType stype;
32: 	unique_ptr<Expression> aggr_expr;
33: 
34: 	unique_ptr<FunctionData> Copy() const override {
35: 		return make_uniq<ListAggregatesBindData>(stype, aggr_expr->Copy());
36: 	}
37: 
38: 	bool Equals(const FunctionData &other_p) const override {
39: 		auto &other = other_p.Cast<ListAggregatesBindData>();
40: 		return stype == other.stype && aggr_expr->Equals(*other.aggr_expr);
41: 	}
42: 	void Serialize(Serializer &serializer) const {
43: 		serializer.WriteProperty(1, "stype", stype);
44: 		serializer.WriteProperty(2, "aggr_expr", aggr_expr);
45: 	}
46: 	static unique_ptr<ListAggregatesBindData> Deserialize(Deserializer &deserializer) {
47: 		auto stype = deserializer.ReadProperty<LogicalType>(1, "stype");
48: 		auto aggr_expr = deserializer.ReadProperty<unique_ptr<Expression>>(2, "aggr_expr");
49: 		auto result = make_uniq<ListAggregatesBindData>(std::move(stype), std::move(aggr_expr));
50: 		return result;
51: 	}
52: 
53: 	static void SerializeFunction(Serializer &serializer, const optional_ptr<FunctionData> bind_data_p,
54: 	                              const ScalarFunction &function) {
55: 		auto bind_data = dynamic_cast<const ListAggregatesBindData *>(bind_data_p.get());
56: 		serializer.WritePropertyWithDefault(100, "bind_data", bind_data, (const ListAggregatesBindData *)nullptr);
57: 	}
58: 
59: 	static unique_ptr<FunctionData> DeserializeFunction(Deserializer &deserializer, ScalarFunction &bound_function) {
60: 		auto result = deserializer.ReadPropertyWithExplicitDefault<unique_ptr<ListAggregatesBindData>>(
61: 		    100, "bind_data", unique_ptr<ListAggregatesBindData>(nullptr));
62: 		if (!result) {
63: 			return ListAggregatesBindFailure(bound_function);
64: 		}
65: 		return std::move(result);
66: 	}
67: };
68: 
69: ListAggregatesBindData::ListAggregatesBindData(const LogicalType &stype_p, unique_ptr<Expression> aggr_expr_p)
70:     : stype(stype_p), aggr_expr(std::move(aggr_expr_p)) {
71: }
72: 
73: ListAggregatesBindData::~ListAggregatesBindData() {
74: }
75: 
76: struct StateVector {
77: 	StateVector(idx_t count_p, unique_ptr<Expression> aggr_expr_p)
78: 	    : count(count_p), aggr_expr(std::move(aggr_expr_p)), state_vector(Vector(LogicalType::POINTER, count_p)) {
79: 	}
80: 
81: 	~StateVector() { // NOLINT
82: 		// destroy objects within the aggregate states
83: 		auto &aggr = aggr_expr->Cast<BoundAggregateExpression>();
84: 		if (aggr.function.destructor) {
85: 			ArenaAllocator allocator(Allocator::DefaultAllocator());
86: 			AggregateInputData aggr_input_data(aggr.bind_info.get(), allocator);
87: 			aggr.function.destructor(state_vector, aggr_input_data, count);
88: 		}
89: 	}
90: 
91: 	idx_t count;
92: 	unique_ptr<Expression> aggr_expr;
93: 	Vector state_vector;
94: };
95: 
96: struct FinalizeValueFunctor {
97: 	template <class T>
98: 	static void HistogramFinalize(T value, Vector &result, idx_t offset) {
99: 		FlatVector::GetData<T>(result)[offset] = value;
100: 	}
101: };
102: 
103: struct FinalizeStringValueFunctor {
104: 	template <class T>
105: 	static void HistogramFinalize(T value, Vector &result, idx_t offset) {
106: 		FlatVector::GetData<string_t>(result)[offset] = StringVector::AddStringOrBlob(result, value);
107: 	}
108: };
109: 
110: struct FinalizeGenericValueFunctor {
111: 	template <class T>
112: 	static void HistogramFinalize(T value, Vector &result, idx_t offset) {
113: 		CreateSortKeyHelpers::DecodeSortKey(value, result, offset,
114: 		                                    OrderModifiers(OrderType::ASCENDING, OrderByNullType::NULLS_LAST));
115: 	}
116: };
117: 
118: struct AggregateFunctor {
119: 	template <class OP, class T, class MAP_TYPE = unordered_map<T, idx_t>>
120: 	static void ListExecuteFunction(Vector &result, Vector &state_vector, idx_t count) {
121: 	}
122: };
123: 
124: struct DistinctFunctor {
125: 	template <class OP, class T, class MAP_TYPE = unordered_map<T, idx_t>>
126: 	static void ListExecuteFunction(Vector &result, Vector &state_vector, idx_t count) {
127: 		UnifiedVectorFormat sdata;
128: 		state_vector.ToUnifiedFormat(count, sdata);
129: 		auto states = UnifiedVectorFormat::GetData<HistogramAggState<T, MAP_TYPE> *>(sdata);
130: 
131: 		auto old_len = ListVector::GetListSize(result);
132: 		idx_t new_entries = 0;
133: 		// figure out how much space we need
134: 		for (idx_t i = 0; i < count; i++) {
135: 			auto &state = *states[sdata.sel->get_index(i)];
136: 			if (!state.hist) {
137: 				continue;
138: 			}
139: 			new_entries += state.hist->size();
140: 		}
141: 		// reserve space in the list vector
142: 		ListVector::Reserve(result, old_len + new_entries);
143: 		auto &child_elements = ListVector::GetEntry(result);
144: 		auto list_entries = FlatVector::GetData<list_entry_t>(result);
145: 
146: 		idx_t current_offset = old_len;
147: 		for (idx_t i = 0; i < count; i++) {
148: 			const auto rid = i;
149: 			auto &state = *states[sdata.sel->get_index(i)];
150: 			auto &list_entry = list_entries[rid];
151: 			list_entry.offset = current_offset;
152: 			if (!state.hist) {
153: 				list_entry.length = 0;
154: 				continue;
155: 			}
156: 
157: 			for (auto &entry : *state.hist) {
158: 				OP::template HistogramFinalize<T>(entry.first, child_elements, current_offset);
159: 				current_offset++;
160: 			}
161: 			list_entry.length = current_offset - list_entry.offset;
162: 		}
163: 		D_ASSERT(current_offset == old_len + new_entries);
164: 		ListVector::SetListSize(result, current_offset);
165: 		result.Verify(count);
166: 	}
167: };
168: 
169: struct UniqueFunctor {
170: 	template <class OP, class T, class MAP_TYPE = unordered_map<T, idx_t>>
171: 	static void ListExecuteFunction(Vector &result, Vector &state_vector, idx_t count) {
172: 		UnifiedVectorFormat sdata;
173: 		state_vector.ToUnifiedFormat(count, sdata);
174: 		auto states = UnifiedVectorFormat::GetData<HistogramAggState<T, MAP_TYPE> *>(sdata);
175: 
176: 		auto result_data = FlatVector::GetData<uint64_t>(result);
177: 		for (idx_t i = 0; i < count; i++) {
178: 
179: 			auto state = states[sdata.sel->get_index(i)];
180: 
181: 			if (!state->hist) {
182: 				result_data[i] = 0;
183: 				continue;
184: 			}
185: 			result_data[i] = state->hist->size();
186: 		}
187: 		result.Verify(count);
188: 	}
189: };
190: 
191: template <class FUNCTION_FUNCTOR, bool IS_AGGR = false>
192: static void ListAggregatesFunction(DataChunk &args, ExpressionState &state, Vector &result) {
193: 	auto count = args.size();
194: 	Vector &lists = args.data[0];
195: 
196: 	// set the result vector
197: 	result.SetVectorType(VectorType::FLAT_VECTOR);
198: 	auto &result_validity = FlatVector::Validity(result);
199: 
200: 	if (lists.GetType().id() == LogicalTypeId::SQLNULL) {
201: 		result.SetVectorType(VectorType::CONSTANT_VECTOR);
202: 		ConstantVector::SetNull(result, true);
203: 		return;
204: 	}
205: 
206: 	// get the aggregate function
207: 	auto &func_expr = state.expr.Cast<BoundFunctionExpression>();
208: 	auto &info = func_expr.bind_info->Cast<ListAggregatesBindData>();
209: 	auto &aggr = info.aggr_expr->Cast<BoundAggregateExpression>();
210: 	ArenaAllocator allocator(Allocator::DefaultAllocator());
211: 	AggregateInputData aggr_input_data(aggr.bind_info.get(), allocator);
212: 
213: 	D_ASSERT(aggr.function.update);
214: 
215: 	auto lists_size = ListVector::GetListSize(lists);
216: 	auto &child_vector = ListVector::GetEntry(lists);
217: 	child_vector.Flatten(lists_size);
218: 
219: 	UnifiedVectorFormat child_data;
220: 	child_vector.ToUnifiedFormat(lists_size, child_data);
221: 
222: 	UnifiedVectorFormat lists_data;
223: 	lists.ToUnifiedFormat(count, lists_data);
224: 	auto list_entries = UnifiedVectorFormat::GetData<list_entry_t>(lists_data);
225: 
226: 	// state_buffer holds the state for each list of this chunk
227: 	idx_t size = aggr.function.state_size(aggr.function);
228: 	auto state_buffer = make_unsafe_uniq_array_uninitialized<data_t>(size * count);
229: 
230: 	// state vector for initialize and finalize
231: 	StateVector state_vector(count, info.aggr_expr->Copy());
232: 	auto states = FlatVector::GetData<data_ptr_t>(state_vector.state_vector);
233: 
234: 	// state vector of STANDARD_VECTOR_SIZE holds the pointers to the states
235: 	Vector state_vector_update = Vector(LogicalType::POINTER);
236: 	auto states_update = FlatVector::GetData<data_ptr_t>(state_vector_update);
237: 
238: 	// selection vector pointing to the data
239: 	SelectionVector sel_vector(STANDARD_VECTOR_SIZE);
240: 	idx_t states_idx = 0;
241: 
242: 	for (idx_t i = 0; i < count; i++) {
243: 
244: 		// initialize the state for this list
245: 		auto state_ptr = state_buffer.get() + size * i;
246: 		states[i] = state_ptr;
247: 		aggr.function.initialize(aggr.function, states[i]);
248: 
249: 		auto lists_index = lists_data.sel->get_index(i);
250: 		const auto &list_entry = list_entries[lists_index];
251: 
252: 		// nothing to do for this list
253: 		if (!lists_data.validity.RowIsValid(lists_index)) {
254: 			result_validity.SetInvalid(i);
255: 			continue;
256: 		}
257: 
258: 		// skip empty list
259: 		if (list_entry.length == 0) {
260: 			continue;
261: 		}
262: 
263: 		for (idx_t child_idx = 0; child_idx < list_entry.length; child_idx++) {
264: 			// states vector is full, update
265: 			if (states_idx == STANDARD_VECTOR_SIZE) {
266: 				// update the aggregate state(s)
267: 				Vector slice(child_vector, sel_vector, states_idx);
268: 				aggr.function.update(&slice, aggr_input_data, 1, state_vector_update, states_idx);
269: 
270: 				// reset values
271: 				states_idx = 0;
272: 			}
273: 
274: 			auto source_idx = child_data.sel->get_index(list_entry.offset + child_idx);
275: 			sel_vector.set_index(states_idx, source_idx);
276: 			states_update[states_idx] = state_ptr;
277: 			states_idx++;
278: 		}
279: 	}
280: 
281: 	// update the remaining elements of the last list(s)
282: 	if (states_idx != 0) {
283: 		Vector slice(child_vector, sel_vector, states_idx);
284: 		aggr.function.update(&slice, aggr_input_data, 1, state_vector_update, states_idx);
285: 	}
286: 
287: 	if (IS_AGGR) {
288: 		// finalize all the aggregate states
289: 		aggr.function.finalize(state_vector.state_vector, aggr_input_data, result, count, 0);
290: 
291: 	} else {
292: 		// finalize manually to use the map
293: 		D_ASSERT(aggr.function.arguments.size() == 1);
294: 		auto key_type = aggr.function.arguments[0];
295: 
296: 		switch (key_type.InternalType()) {
297: #ifndef DUCKDB_SMALLER_BINARY
298: 		case PhysicalType::BOOL:
299: 			FUNCTION_FUNCTOR::template ListExecuteFunction<FinalizeValueFunctor, bool>(
300: 			    result, state_vector.state_vector, count);
301: 			break;
302: 		case PhysicalType::UINT8:
303: 			FUNCTION_FUNCTOR::template ListExecuteFunction<FinalizeValueFunctor, uint8_t>(
304: 			    result, state_vector.state_vector, count);
305: 			break;
306: 		case PhysicalType::UINT16:
307: 			FUNCTION_FUNCTOR::template ListExecuteFunction<FinalizeValueFunctor, uint16_t>(
308: 			    result, state_vector.state_vector, count);
309: 			break;
310: 		case PhysicalType::UINT32:
311: 			FUNCTION_FUNCTOR::template ListExecuteFunction<FinalizeValueFunctor, uint32_t>(
312: 			    result, state_vector.state_vector, count);
313: 			break;
314: 		case PhysicalType::UINT64:
315: 			FUNCTION_FUNCTOR::template ListExecuteFunction<FinalizeValueFunctor, uint64_t>(
316: 			    result, state_vector.state_vector, count);
317: 			break;
318: 		case PhysicalType::INT8:
319: 			FUNCTION_FUNCTOR::template ListExecuteFunction<FinalizeValueFunctor, int8_t>(
320: 			    result, state_vector.state_vector, count);
321: 			break;
322: 		case PhysicalType::INT16:
323: 			FUNCTION_FUNCTOR::template ListExecuteFunction<FinalizeValueFunctor, int16_t>(
324: 			    result, state_vector.state_vector, count);
325: 			break;
326: 		case PhysicalType::INT32:
327: 			FUNCTION_FUNCTOR::template ListExecuteFunction<FinalizeValueFunctor, int32_t>(
328: 			    result, state_vector.state_vector, count);
329: 			break;
330: 		case PhysicalType::INT64:
331: 			FUNCTION_FUNCTOR::template ListExecuteFunction<FinalizeValueFunctor, int64_t>(
332: 			    result, state_vector.state_vector, count);
333: 			break;
334: 		case PhysicalType::FLOAT:
335: 			FUNCTION_FUNCTOR::template ListExecuteFunction<FinalizeValueFunctor, float>(
336: 			    result, state_vector.state_vector, count);
337: 			break;
338: 		case PhysicalType::DOUBLE:
339: 			FUNCTION_FUNCTOR::template ListExecuteFunction<FinalizeValueFunctor, double>(
340: 			    result, state_vector.state_vector, count);
341: 			break;
342: 		case PhysicalType::VARCHAR:
343: 			FUNCTION_FUNCTOR::template ListExecuteFunction<FinalizeStringValueFunctor, string_t,
344: 			                                               OwningStringMap<idx_t>>(result, state_vector.state_vector,
345: 			                                                                       count);
346: 			break;
347: #endif
348: 		default:
349: 			FUNCTION_FUNCTOR::template ListExecuteFunction<FinalizeGenericValueFunctor, string_t,
350: 			                                               OwningStringMap<idx_t>>(result, state_vector.state_vector,
351: 			                                                                       count);
352: 			break;
353: 		}
354: 	}
355: 
356: 	if (args.AllConstant()) {
357: 		result.SetVectorType(VectorType::CONSTANT_VECTOR);
358: 	}
359: }
360: 
361: static void ListAggregateFunction(DataChunk &args, ExpressionState &state, Vector &result) {
362: 	D_ASSERT(args.ColumnCount() >= 2);
363: 	ListAggregatesFunction<AggregateFunctor, true>(args, state, result);
364: }
365: 
366: static void ListDistinctFunction(DataChunk &args, ExpressionState &state, Vector &result) {
367: 	D_ASSERT(args.ColumnCount() == 1);
368: 	ListAggregatesFunction<DistinctFunctor>(args, state, result);
369: }
370: 
371: static void ListUniqueFunction(DataChunk &args, ExpressionState &state, Vector &result) {
372: 	D_ASSERT(args.ColumnCount() == 1);
373: 	ListAggregatesFunction<UniqueFunctor>(args, state, result);
374: }
375: 
376: template <bool IS_AGGR = false>
377: static unique_ptr<FunctionData>
378: ListAggregatesBindFunction(ClientContext &context, ScalarFunction &bound_function, const LogicalType &list_child_type,
379:                            AggregateFunction &aggr_function, vector<unique_ptr<Expression>> &arguments) {
380: 
381: 	// create the child expression and its type
382: 	vector<unique_ptr<Expression>> children;
383: 	auto expr = make_uniq<BoundConstantExpression>(Value(list_child_type));
384: 	children.push_back(std::move(expr));
385: 	// push any extra arguments into the list aggregate bind
386: 	if (arguments.size() > 2) {
387: 		for (idx_t i = 2; i < arguments.size(); i++) {
388: 			children.push_back(std::move(arguments[i]));
389: 		}
390: 		arguments.resize(2);
391: 	}
392: 
393: 	FunctionBinder function_binder(context);
394: 	auto bound_aggr_function = function_binder.BindAggregateFunction(aggr_function, std::move(children));
395: 	bound_function.arguments[0] = LogicalType::LIST(bound_aggr_function->function.arguments[0]);
396: 
397: 	if (IS_AGGR) {
398: 		bound_function.return_type = bound_aggr_function->function.return_type;
399: 	}
400: 	// check if the aggregate function consumed all the extra input arguments
401: 	if (bound_aggr_function->children.size() > 1) {
402: 		throw InvalidInputException(
403: 		    "Aggregate function %s is not supported for list_aggr: extra arguments were not removed during bind",
404: 		    bound_aggr_function->ToString());
405: 	}
406: 
407: 	return make_uniq<ListAggregatesBindData>(bound_function.return_type, std::move(bound_aggr_function));
408: }
409: 
410: template <bool IS_AGGR = false>
411: static unique_ptr<FunctionData> ListAggregatesBind(ClientContext &context, ScalarFunction &bound_function,
412:                                                    vector<unique_ptr<Expression>> &arguments) {
413: 
414: 	arguments[0] = BoundCastExpression::AddArrayCastToList(context, std::move(arguments[0]));
415: 
416: 	if (arguments[0]->return_type.id() == LogicalTypeId::SQLNULL) {
417: 		return ListAggregatesBindFailure(bound_function);
418: 	}
419: 
420: 	bool is_parameter = arguments[0]->return_type.id() == LogicalTypeId::UNKNOWN;
421: 	LogicalType child_type;
422: 	if (is_parameter) {
423: 		child_type = LogicalType::ANY;
424: 	} else if (arguments[0]->return_type.id() == LogicalTypeId::LIST ||
425: 	           arguments[0]->return_type.id() == LogicalTypeId::MAP) {
426: 		child_type = ListType::GetChildType(arguments[0]->return_type);
427: 	} else {
428: 		// Unreachable
429: 		throw InvalidInputException("First argument of list aggregate must be a list, map or array");
430: 	}
431: 
432: 	string function_name = "histogram";
433: 	if (IS_AGGR) { // get the name of the aggregate function
434: 		if (!arguments[1]->IsFoldable()) {
435: 			throw InvalidInputException("Aggregate function name must be a constant");
436: 		}
437: 		// get the function name
438: 		Value function_value = ExpressionExecutor::EvaluateScalar(context, *arguments[1]);
439: 		function_name = function_value.ToString();
440: 	}
441: 
442: 	// look up the aggregate function in the catalog
443: 	auto &func = Catalog::GetSystemCatalog(context).GetEntry<AggregateFunctionCatalogEntry>(context, DEFAULT_SCHEMA,
444: 	                                                                                        function_name);
445: 	D_ASSERT(func.type == CatalogType::AGGREGATE_FUNCTION_ENTRY);
446: 
447: 	if (is_parameter) {
448: 		bound_function.arguments[0] = LogicalTypeId::UNKNOWN;
449: 		bound_function.return_type = LogicalType::SQLNULL;
450: 		return nullptr;
451: 	}
452: 
453: 	// find a matching aggregate function
454: 	ErrorData error;
455: 	vector<LogicalType> types;
456: 	types.push_back(child_type);
457: 	// push any extra arguments into the type list
458: 	for (idx_t i = 2; i < arguments.size(); i++) {
459: 		types.push_back(arguments[i]->return_type);
460: 	}
461: 
462: 	FunctionBinder function_binder(context);
463: 	auto best_function_idx = function_binder.BindFunction(func.name, func.functions, types, error);
464: 	if (!best_function_idx.IsValid()) {
465: 		throw BinderException("No matching aggregate function\n%s", error.Message());
466: 	}
467: 
468: 	// found a matching function, bind it as an aggregate
469: 	auto best_function = func.functions.GetFunctionByOffset(best_function_idx.GetIndex());
470: 	if (IS_AGGR) {
471: 		bound_function.errors = best_function.errors;
472: 		return ListAggregatesBindFunction<IS_AGGR>(context, bound_function, child_type, best_function, arguments);
473: 	}
474: 
475: 	// create the unordered map histogram function
476: 	D_ASSERT(best_function.arguments.size() == 1);
477: 	auto aggr_function = HistogramFun::GetHistogramUnorderedMap(child_type);
478: 	return ListAggregatesBindFunction<IS_AGGR>(context, bound_function, child_type, aggr_function, arguments);
479: }
480: 
481: static unique_ptr<FunctionData> ListAggregateBind(ClientContext &context, ScalarFunction &bound_function,
482:                                                   vector<unique_ptr<Expression>> &arguments) {
483: 
484: 	// the list column and the name of the aggregate function
485: 	D_ASSERT(bound_function.arguments.size() >= 2);
486: 	D_ASSERT(arguments.size() >= 2);
487: 
488: 	return ListAggregatesBind<true>(context, bound_function, arguments);
489: }
490: 
491: static unique_ptr<FunctionData> ListDistinctBind(ClientContext &context, ScalarFunction &bound_function,
492:                                                  vector<unique_ptr<Expression>> &arguments) {
493: 
494: 	D_ASSERT(bound_function.arguments.size() == 1);
495: 	D_ASSERT(arguments.size() == 1);
496: 
497: 	arguments[0] = BoundCastExpression::AddArrayCastToList(context, std::move(arguments[0]));
498: 	bound_function.return_type = arguments[0]->return_type;
499: 
500: 	return ListAggregatesBind<>(context, bound_function, arguments);
501: }
502: 
503: static unique_ptr<FunctionData> ListUniqueBind(ClientContext &context, ScalarFunction &bound_function,
504:                                                vector<unique_ptr<Expression>> &arguments) {
505: 
506: 	D_ASSERT(bound_function.arguments.size() == 1);
507: 	D_ASSERT(arguments.size() == 1);
508: 	bound_function.return_type = LogicalType::UBIGINT;
509: 
510: 	return ListAggregatesBind<>(context, bound_function, arguments);
511: }
512: 
513: ScalarFunction ListAggregateFun::GetFunction() {
514: 	auto result = ScalarFunction({LogicalType::LIST(LogicalType::ANY), LogicalType::VARCHAR}, LogicalType::ANY,
515: 	                             ListAggregateFunction, ListAggregateBind);
516: 	BaseScalarFunction::SetReturnsError(result);
517: 	result.null_handling = FunctionNullHandling::SPECIAL_HANDLING;
518: 	result.varargs = LogicalType::ANY;
519: 	result.serialize = ListAggregatesBindData::SerializeFunction;
520: 	result.deserialize = ListAggregatesBindData::DeserializeFunction;
521: 	return result;
522: }
523: 
524: ScalarFunction ListDistinctFun::GetFunction() {
525: 	return ScalarFunction({LogicalType::LIST(LogicalType::ANY)}, LogicalType::LIST(LogicalType::ANY),
526: 	                      ListDistinctFunction, ListDistinctBind);
527: }
528: 
529: ScalarFunction ListUniqueFun::GetFunction() {
530: 	return ScalarFunction({LogicalType::LIST(LogicalType::ANY)}, LogicalType::UBIGINT, ListUniqueFunction,
531: 	                      ListUniqueBind);
532: }
533: 
534: } // namespace duckdb
[end of extension/core_functions/scalar/list/list_aggregates.cpp]
[start of extension/json/buffered_json_reader.cpp]
1: #include "buffered_json_reader.hpp"
2: 
3: #include "duckdb/common/file_opener.hpp"
4: #include "duckdb/common/serializer/deserializer.hpp"
5: #include "duckdb/common/serializer/serializer.hpp"
6: 
7: #include <utility>
8: 
9: namespace duckdb {
10: 
11: JSONBufferHandle::JSONBufferHandle(idx_t buffer_index_p, idx_t readers_p, AllocatedData &&buffer_p, idx_t buffer_size_p)
12:     : buffer_index(buffer_index_p), readers(readers_p), buffer(std::move(buffer_p)), buffer_size(buffer_size_p) {
13: }
14: 
15: JSONFileHandle::JSONFileHandle(unique_ptr<FileHandle> file_handle_p, Allocator &allocator_p)
16:     : file_handle(std::move(file_handle_p)), allocator(allocator_p), can_seek(file_handle->CanSeek()),
17:       file_size(file_handle->GetFileSize()), read_position(0), requested_reads(0), actual_reads(0),
18:       last_read_requested(false), cached_size(0) {
19: }
20: 
21: bool JSONFileHandle::IsOpen() const {
22: 	return file_handle != nullptr;
23: }
24: 
25: void JSONFileHandle::Close() {
26: 	if (IsOpen() && !file_handle->IsPipe()) {
27: 		file_handle->Close();
28: 		file_handle = nullptr;
29: 	}
30: }
31: 
32: void JSONFileHandle::Reset() {
33: 	D_ASSERT(RequestedReadsComplete());
34: 	read_position = 0;
35: 	requested_reads = 0;
36: 	actual_reads = 0;
37: 	last_read_requested = false;
38: 	if (IsOpen() && !file_handle->IsPipe()) {
39: 		file_handle->Reset();
40: 	}
41: }
42: 
43: bool JSONFileHandle::RequestedReadsComplete() {
44: 	return requested_reads == actual_reads;
45: }
46: 
47: bool JSONFileHandle::LastReadRequested() const {
48: 	return last_read_requested;
49: }
50: 
51: idx_t JSONFileHandle::FileSize() const {
52: 	return file_size;
53: }
54: 
55: idx_t JSONFileHandle::Remaining() const {
56: 	return file_size - read_position;
57: }
58: 
59: bool JSONFileHandle::CanSeek() const {
60: 	return can_seek;
61: }
62: 
63: bool JSONFileHandle::IsPipe() const {
64: 	return file_handle->IsPipe();
65: }
66: 
67: FileHandle &JSONFileHandle::GetHandle() {
68: 	return *file_handle;
69: }
70: 
71: bool JSONFileHandle::GetPositionAndSize(idx_t &position, idx_t &size, idx_t requested_size) {
72: 	D_ASSERT(requested_size != 0);
73: 	if (last_read_requested) {
74: 		return false;
75: 	}
76: 
77: 	position = read_position;
78: 	size = MinValue<idx_t>(requested_size, Remaining());
79: 	read_position += size;
80: 
81: 	requested_reads++;
82: 	if (size == 0) {
83: 		last_read_requested = true;
84: 	}
85: 
86: 	return true;
87: }
88: 
89: void JSONFileHandle::ReadAtPosition(char *pointer, idx_t size, idx_t position, bool &file_done, bool sample_run,
90:                                     optional_ptr<FileHandle> override_handle) {
91: 	if (size != 0) {
92: 		auto &handle = override_handle ? *override_handle.get() : *file_handle.get();
93: 		if (can_seek) {
94: 			handle.Read(pointer, size, position);
95: 		} else if (sample_run) { // Cache the buffer
96: 			handle.Read(pointer, size, position);
97: 
98: 			cached_buffers.emplace_back(allocator.Allocate(size));
99: 			memcpy(cached_buffers.back().get(), pointer, size);
100: 			cached_size += size;
101: 		} else {
102: 			if (!cached_buffers.empty() || position < cached_size) {
103: 				ReadFromCache(pointer, size, position);
104: 			}
105: 
106: 			if (size != 0) {
107: 				handle.Read(pointer, size, position);
108: 			}
109: 		}
110: 	}
111: 
112: 	const auto incremented_actual_reads = ++actual_reads;
113: 	if (incremented_actual_reads > requested_reads) {
114: 		throw InternalException("JSONFileHandle performed more actual reads than requested reads");
115: 	}
116: 
117: 	if (last_read_requested && incremented_actual_reads == requested_reads) {
118: 		file_done = true;
119: 	}
120: }
121: 
122: bool JSONFileHandle::Read(char *pointer, idx_t &read_size, idx_t requested_size, bool &file_done, bool sample_run) {
123: 	D_ASSERT(requested_size != 0);
124: 	if (last_read_requested) {
125: 		return false;
126: 	}
127: 
128: 	if (can_seek) {
129: 		read_size = ReadInternal(pointer, requested_size);
130: 		read_position += read_size;
131: 	} else if (sample_run) { // Cache the buffer
132: 		read_size = ReadInternal(pointer, requested_size);
133: 		if (read_size > 0) {
134: 			cached_buffers.emplace_back(allocator.Allocate(read_size));
135: 			memcpy(cached_buffers.back().get(), pointer, read_size);
136: 		}
137: 		cached_size += read_size;
138: 		read_position += read_size;
139: 	} else {
140: 		read_size = 0;
141: 		if (!cached_buffers.empty() || read_position < cached_size) {
142: 			read_size += ReadFromCache(pointer, requested_size, read_position);
143: 		}
144: 		if (requested_size != 0) {
145: 			read_size += ReadInternal(pointer, requested_size);
146: 		}
147: 	}
148: 
149: 	if (read_size == 0) {
150: 		last_read_requested = true;
151: 		file_done = true;
152: 	}
153: 
154: 	return true;
155: }
156: 
157: idx_t JSONFileHandle::ReadInternal(char *pointer, const idx_t requested_size) {
158: 	// Deal with reading from pipes
159: 	idx_t total_read_size = 0;
160: 	while (total_read_size < requested_size) {
161: 		auto read_size = file_handle->Read(pointer + total_read_size, requested_size - total_read_size);
162: 		if (read_size == 0) {
163: 			break;
164: 		}
165: 		total_read_size += read_size;
166: 	}
167: 	return total_read_size;
168: }
169: 
170: idx_t JSONFileHandle::ReadFromCache(char *&pointer, idx_t &size, idx_t &position) {
171: 	idx_t read_size = 0;
172: 	idx_t total_offset = 0;
173: 
174: 	idx_t cached_buffer_idx;
175: 	for (cached_buffer_idx = 0; cached_buffer_idx < cached_buffers.size(); cached_buffer_idx++) {
176: 		auto &cached_buffer = cached_buffers[cached_buffer_idx];
177: 		if (size == 0) {
178: 			break;
179: 		}
180: 		if (position < total_offset + cached_buffer.GetSize()) {
181: 			idx_t within_buffer_offset = position - total_offset;
182: 			idx_t copy_size = MinValue<idx_t>(size, cached_buffer.GetSize() - within_buffer_offset);
183: 			memcpy(pointer, cached_buffer.get() + within_buffer_offset, copy_size);
184: 
185: 			read_size += copy_size;
186: 			pointer += copy_size;
187: 			size -= copy_size;
188: 			position += copy_size;
189: 		}
190: 		total_offset += cached_buffer.GetSize();
191: 	}
192: 
193: 	return read_size;
194: }
195: 
196: BufferedJSONReader::BufferedJSONReader(ClientContext &context, BufferedJSONReaderOptions options_p, string file_name_p)
197:     : context(context), options(std::move(options_p)), file_name(std::move(file_name_p)), buffer_index(0),
198:       thrown(false) {
199: }
200: 
201: void BufferedJSONReader::OpenJSONFile() {
202: 	lock_guard<mutex> guard(lock);
203: 	if (!IsOpen()) {
204: 		auto &fs = FileSystem::GetFileSystem(context);
205: 		auto regular_file_handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ | options.compression);
206: 		file_handle = make_uniq<JSONFileHandle>(std::move(regular_file_handle), BufferAllocator::Get(context));
207: 	}
208: 	Reset();
209: }
210: 
211: void BufferedJSONReader::Reset() {
212: 	buffer_index = 0;
213: 	buffer_map.clear();
214: 	buffer_line_or_object_counts.clear();
215: 	if (HasFileHandle()) {
216: 		file_handle->Reset();
217: 	}
218: }
219: 
220: bool BufferedJSONReader::HasFileHandle() const {
221: 	return file_handle != nullptr;
222: }
223: 
224: bool BufferedJSONReader::IsOpen() const {
225: 	if (HasFileHandle()) {
226: 		return file_handle->IsOpen();
227: 	}
228: 	return false;
229: }
230: 
231: BufferedJSONReaderOptions &BufferedJSONReader::GetOptions() {
232: 	return options;
233: }
234: 
235: JSONFormat BufferedJSONReader::GetFormat() const {
236: 	return options.format;
237: }
238: 
239: void BufferedJSONReader::SetFormat(JSONFormat format) {
240: 	D_ASSERT(options.format == JSONFormat::AUTO_DETECT);
241: 	options.format = format;
242: }
243: 
244: JSONRecordType BufferedJSONReader::GetRecordType() const {
245: 	return options.record_type;
246: }
247: 
248: void BufferedJSONReader::SetRecordType(duckdb::JSONRecordType type) {
249: 	D_ASSERT(options.record_type == JSONRecordType::AUTO_DETECT);
250: 	options.record_type = type;
251: }
252: 
253: const string &BufferedJSONReader::GetFileName() const {
254: 	return file_name;
255: }
256: 
257: JSONFileHandle &BufferedJSONReader::GetFileHandle() const {
258: 	D_ASSERT(HasFileHandle());
259: 	return *file_handle;
260: }
261: 
262: void BufferedJSONReader::InsertBuffer(idx_t buffer_idx, unique_ptr<JSONBufferHandle> &&buffer) {
263: 	lock_guard<mutex> guard(lock);
264: 	buffer_map.insert(make_pair(buffer_idx, std::move(buffer)));
265: }
266: 
267: optional_ptr<JSONBufferHandle> BufferedJSONReader::GetBuffer(idx_t buffer_idx) {
268: 	lock_guard<mutex> guard(lock);
269: 	auto it = buffer_map.find(buffer_idx);
270: 	return it == buffer_map.end() ? nullptr : it->second.get();
271: }
272: 
273: AllocatedData BufferedJSONReader::RemoveBuffer(JSONBufferHandle &handle) {
274: 	lock_guard<mutex> guard(lock);
275: 	auto it = buffer_map.find(handle.buffer_index);
276: 	D_ASSERT(it != buffer_map.end());
277: 	D_ASSERT(RefersToSameObject(handle, *it->second));
278: 	auto result = std::move(it->second->buffer);
279: 	buffer_map.erase(it);
280: 	return result;
281: }
282: 
283: idx_t BufferedJSONReader::GetBufferIndex() {
284: 	buffer_line_or_object_counts.push_back(-1);
285: 	return buffer_index++;
286: }
287: 
288: void BufferedJSONReader::SetBufferLineOrObjectCount(JSONBufferHandle &handle, idx_t count) {
289: 	lock_guard<mutex> guard(lock);
290: 	D_ASSERT(buffer_map.find(handle.buffer_index) != buffer_map.end());
291: 	D_ASSERT(RefersToSameObject(handle, *buffer_map.find(handle.buffer_index)->second));
292: 	D_ASSERT(buffer_line_or_object_counts[handle.buffer_index] == -1);
293: 	buffer_line_or_object_counts[handle.buffer_index] = count;
294: }
295: 
296: idx_t BufferedJSONReader::GetLineNumber(idx_t buf_index, idx_t line_or_object_in_buf) {
297: 	D_ASSERT(options.format != JSONFormat::AUTO_DETECT);
298: 	while (true) {
299: 		idx_t line = line_or_object_in_buf;
300: 		bool can_throw = true;
301: 		{
302: 			lock_guard<mutex> guard(lock);
303: 			if (thrown) {
304: 				return DConstants::INVALID_INDEX;
305: 			}
306: 			for (idx_t b_idx = 0; b_idx < buf_index; b_idx++) {
307: 				if (buffer_line_or_object_counts[b_idx] == -1) {
308: 					can_throw = false;
309: 					break;
310: 				} else {
311: 					line += buffer_line_or_object_counts[b_idx];
312: 				}
313: 			}
314: 			if (can_throw) {
315: 				thrown = true;
316: 				// SQL uses 1-based indexing so I guess we will do that in our exception here as well
317: 				return line + 1;
318: 			}
319: 		}
320: 		TaskScheduler::YieldThread();
321: 	}
322: }
323: 
324: void BufferedJSONReader::ThrowParseError(idx_t buf_index, idx_t line_or_object_in_buf, yyjson_read_err &err,
325:                                          const string &extra) {
326: 	string unit = options.format == JSONFormat::NEWLINE_DELIMITED ? "line" : "record/value";
327: 	auto line = GetLineNumber(buf_index, line_or_object_in_buf);
328: 	throw InvalidInputException("Malformed JSON in file \"%s\", at byte %llu in %s %llu: %s. %s", file_name,
329: 	                            err.pos + 1, unit, line + 1, err.msg, extra);
330: }
331: 
332: void BufferedJSONReader::ThrowTransformError(idx_t buf_index, idx_t line_or_object_in_buf,
333:                                              const string &error_message) {
334: 	string unit = options.format == JSONFormat::NEWLINE_DELIMITED ? "line" : "record/value";
335: 	auto line = GetLineNumber(buf_index, line_or_object_in_buf);
336: 	throw InvalidInputException("JSON transform error in file \"%s\", in %s %llu: %s", file_name, unit, line,
337: 	                            error_message);
338: }
339: 
340: bool BufferedJSONReader::HasThrown() {
341: 	lock_guard<mutex> guard(lock);
342: 	return thrown;
343: }
344: 
345: double BufferedJSONReader::GetProgress() const {
346: 	lock_guard<mutex> guard(lock);
347: 	if (HasFileHandle()) {
348: 		return 100.0 - 100.0 * double(file_handle->Remaining()) / double(file_handle->FileSize());
349: 	} else {
350: 		return 0;
351: 	}
352: }
353: 
354: } // namespace duckdb
[end of extension/json/buffered_json_reader.cpp]
[start of extension/json/json_extension.cpp]
1: #define DUCKDB_EXTENSION_MAIN
2: #include "json_extension.hpp"
3: 
4: #include "duckdb/catalog/catalog_entry/macro_catalog_entry.hpp"
5: #include "duckdb/catalog/default/default_functions.hpp"
6: #include "duckdb/common/string_util.hpp"
7: #include "duckdb/function/copy_function.hpp"
8: #include "duckdb/main/extension_util.hpp"
9: #include "duckdb/parser/expression/constant_expression.hpp"
10: #include "duckdb/parser/expression/function_expression.hpp"
11: #include "duckdb/parser/parsed_data/create_pragma_function_info.hpp"
12: #include "duckdb/parser/parsed_data/create_type_info.hpp"
13: #include "duckdb/parser/tableref/table_function_ref.hpp"
14: #include "json_common.hpp"
15: #include "json_functions.hpp"
16: 
17: namespace duckdb {
18: 
19: static DefaultMacro json_macros[] = {
20:     {DEFAULT_SCHEMA, "json_group_array", {"x", nullptr}, {{nullptr, nullptr}}, "to_json(list(x))"},
21:     {DEFAULT_SCHEMA,
22:      "json_group_object",
23:      {"name", "value", nullptr},
24:      {{nullptr, nullptr}},
25:      "to_json(map(list(name), list(value)))"},
26:     {DEFAULT_SCHEMA,
27:      "json_group_structure",
28:      {"x", nullptr},
29:      {{nullptr, nullptr}},
30:      "json_structure(json_group_array(x))->0"},
31:     {DEFAULT_SCHEMA, "json", {"x", nullptr}, {{nullptr, nullptr}}, "json_extract(x, '$')"},
32:     {nullptr, nullptr, {nullptr}, {{nullptr, nullptr}}, nullptr}};
33: 
34: void JsonExtension::Load(DuckDB &db) {
35: 	auto &db_instance = *db.instance;
36: 	// JSON type
37: 	auto json_type = LogicalType::JSON();
38: 	ExtensionUtil::RegisterType(db_instance, LogicalType::JSON_TYPE_NAME, std::move(json_type));
39: 
40: 	// JSON casts
41: 	JSONFunctions::RegisterSimpleCastFunctions(DBConfig::GetConfig(db_instance).GetCastFunctions());
42: 	JSONFunctions::RegisterJSONCreateCastFunctions(DBConfig::GetConfig(db_instance).GetCastFunctions());
43: 	JSONFunctions::RegisterJSONTransformCastFunctions(DBConfig::GetConfig(db_instance).GetCastFunctions());
44: 
45: 	// JSON scalar functions
46: 	for (auto &fun : JSONFunctions::GetScalarFunctions()) {
47: 		ExtensionUtil::RegisterFunction(db_instance, fun);
48: 	}
49: 
50: 	// JSON table functions
51: 	for (auto &fun : JSONFunctions::GetTableFunctions()) {
52: 		ExtensionUtil::RegisterFunction(db_instance, fun);
53: 	}
54: 
55: 	// JSON pragma functions
56: 	for (auto &fun : JSONFunctions::GetPragmaFunctions()) {
57: 		ExtensionUtil::RegisterFunction(db_instance, fun);
58: 	}
59: 
60: 	// JSON replacement scan
61: 	auto &config = DBConfig::GetConfig(*db.instance);
62: 	config.replacement_scans.emplace_back(JSONFunctions::ReadJSONReplacement);
63: 
64: 	// JSON copy function
65: 	auto copy_fun = JSONFunctions::GetJSONCopyFunction();
66: 	ExtensionUtil::RegisterFunction(db_instance, std::move(copy_fun));
67: 
68: 	// JSON macro's
69: 	for (idx_t index = 0; json_macros[index].name != nullptr; index++) {
70: 		auto info = DefaultFunctionGenerator::CreateInternalMacroInfo(json_macros[index]);
71: 		ExtensionUtil::RegisterFunction(db_instance, *info);
72: 	}
73: }
74: 
75: std::string JsonExtension::Name() {
76: 	return "json";
77: }
78: 
79: std::string JsonExtension::Version() const {
80: #ifdef EXT_VERSION_JSON
81: 	return EXT_VERSION_JSON;
82: #else
83: 	return "";
84: #endif
85: }
86: 
87: } // namespace duckdb
88: 
89: extern "C" {
90: 
91: DUCKDB_EXTENSION_API void json_init(duckdb::DatabaseInstance &db) {
92: 	duckdb::DuckDB db_wrapper(db);
93: 	db_wrapper.LoadExtension<duckdb::JsonExtension>();
94: }
95: 
96: DUCKDB_EXTENSION_API const char *json_version() {
97: 	return duckdb::DuckDB::LibraryVersion();
98: }
99: }
100: 
101: #ifndef DUCKDB_EXTENSION_MAIN
102: #error DUCKDB_EXTENSION_MAIN not defined
103: #endif
[end of extension/json/json_extension.cpp]
[start of src/common/compressed_file_system.cpp]
1: #include "duckdb/common/compressed_file_system.hpp"
2: #include "duckdb/common/numeric_utils.hpp"
3: 
4: namespace duckdb {
5: 
6: StreamWrapper::~StreamWrapper() {
7: }
8: 
9: CompressedFile::CompressedFile(CompressedFileSystem &fs, unique_ptr<FileHandle> child_handle_p, const string &path)
10:     : FileHandle(fs, path, child_handle_p->GetFlags()), compressed_fs(fs), child_handle(std::move(child_handle_p)) {
11: }
12: 
13: CompressedFile::~CompressedFile() {
14: 	try {
15: 		// stream_wrapper->Close() might throw
16: 		CompressedFile::Close();
17: 	} catch (...) { // NOLINT - cannot throw in exception
18: 	}
19: }
20: 
21: void CompressedFile::Initialize(bool write) {
22: 	Close();
23: 
24: 	this->write = write;
25: 	stream_data.in_buf_size = compressed_fs.InBufferSize();
26: 	stream_data.out_buf_size = compressed_fs.OutBufferSize();
27: 	stream_data.in_buff = make_unsafe_uniq_array<data_t>(stream_data.in_buf_size);
28: 	stream_data.in_buff_start = stream_data.in_buff.get();
29: 	stream_data.in_buff_end = stream_data.in_buff.get();
30: 	stream_data.out_buff = make_unsafe_uniq_array<data_t>(stream_data.out_buf_size);
31: 	stream_data.out_buff_start = stream_data.out_buff.get();
32: 	stream_data.out_buff_end = stream_data.out_buff.get();
33: 
34: 	stream_wrapper = compressed_fs.CreateStream();
35: 	stream_wrapper->Initialize(*this, write);
36: }
37: 
38: idx_t CompressedFile::GetProgress() {
39: 	return current_position;
40: }
41: 
42: int64_t CompressedFile::ReadData(void *buffer, int64_t remaining) {
43: 	idx_t total_read = 0;
44: 	while (true) {
45: 		// first check if there are input bytes available in the output buffers
46: 		if (stream_data.out_buff_start != stream_data.out_buff_end) {
47: 			// there is! copy it into the output buffer
48: 			auto available =
49: 			    MinValue<idx_t>(UnsafeNumericCast<idx_t>(remaining),
50: 			                    UnsafeNumericCast<idx_t>(stream_data.out_buff_end - stream_data.out_buff_start));
51: 			memcpy(static_cast<data_ptr_t>(buffer) + total_read, stream_data.out_buff_start, available);
52: 
53: 			// increment the total read variables as required
54: 			stream_data.out_buff_start += available;
55: 			total_read += available;
56: 			remaining = UnsafeNumericCast<int64_t>(UnsafeNumericCast<idx_t>(remaining) - available);
57: 			if (remaining == 0) {
58: 				// done! read enough
59: 				return UnsafeNumericCast<int64_t>(total_read);
60: 			}
61: 		}
62: 		if (!stream_wrapper) {
63: 			return UnsafeNumericCast<int64_t>(total_read);
64: 		}
65: 		current_position += static_cast<idx_t>(stream_data.in_buff_end - stream_data.in_buff_start);
66: 		// ran out of buffer: read more data from the child stream
67: 		stream_data.out_buff_start = stream_data.out_buff.get();
68: 		stream_data.out_buff_end = stream_data.out_buff.get();
69: 		D_ASSERT(stream_data.in_buff_start <= stream_data.in_buff_end);
70: 		D_ASSERT(stream_data.in_buff_end <= stream_data.in_buff_start + stream_data.in_buf_size);
71: 
72: 		// read more input when requested and still data in the input stream
73: 		if (stream_data.refresh && (stream_data.in_buff_end == stream_data.in_buff.get() + stream_data.in_buf_size)) {
74: 			auto bufrem = stream_data.in_buff_end - stream_data.in_buff_start;
75: 			// buffer not empty, move remaining bytes to the beginning
76: 			memmove(stream_data.in_buff.get(), stream_data.in_buff_start, UnsafeNumericCast<size_t>(bufrem));
77: 			stream_data.in_buff_start = stream_data.in_buff.get();
78: 			// refill the rest of input buffer
79: 			auto sz = child_handle->Read(stream_data.in_buff_start + bufrem,
80: 			                             stream_data.in_buf_size - UnsafeNumericCast<idx_t>(bufrem));
81: 			stream_data.in_buff_end = stream_data.in_buff_start + bufrem + sz;
82: 			if (sz <= 0) {
83: 				stream_wrapper.reset();
84: 				break;
85: 			}
86: 		}
87: 
88: 		// read more input if none available
89: 		if (stream_data.in_buff_start == stream_data.in_buff_end) {
90: 			// empty input buffer: refill from the start
91: 			stream_data.in_buff_start = stream_data.in_buff.get();
92: 			stream_data.in_buff_end = stream_data.in_buff_start;
93: 			auto sz = child_handle->Read(stream_data.in_buff.get(), stream_data.in_buf_size);
94: 			if (sz <= 0) {
95: 				stream_wrapper.reset();
96: 				break;
97: 			}
98: 			stream_data.in_buff_end = stream_data.in_buff_start + sz;
99: 		}
100: 
101: 		auto finished = stream_wrapper->Read(stream_data);
102: 		if (finished) {
103: 			stream_wrapper.reset();
104: 		}
105: 	}
106: 	return UnsafeNumericCast<int64_t>(total_read);
107: }
108: 
109: int64_t CompressedFile::WriteData(data_ptr_t buffer, int64_t nr_bytes) {
110: 	stream_wrapper->Write(*this, stream_data, buffer, nr_bytes);
111: 	return nr_bytes;
112: }
113: 
114: void CompressedFile::Close() {
115: 	if (stream_wrapper) {
116: 		stream_wrapper->Close();
117: 		stream_wrapper.reset();
118: 	}
119: 	stream_data.in_buff.reset();
120: 	stream_data.out_buff.reset();
121: 	stream_data.out_buff_start = nullptr;
122: 	stream_data.out_buff_end = nullptr;
123: 	stream_data.in_buff_start = nullptr;
124: 	stream_data.in_buff_end = nullptr;
125: 	stream_data.in_buf_size = 0;
126: 	stream_data.out_buf_size = 0;
127: 	stream_data.refresh = false;
128: }
129: 
130: int64_t CompressedFileSystem::Read(FileHandle &handle, void *buffer, int64_t nr_bytes) {
131: 	auto &compressed_file = handle.Cast<CompressedFile>();
132: 	return compressed_file.ReadData(buffer, nr_bytes);
133: }
134: 
135: int64_t CompressedFileSystem::Write(FileHandle &handle, void *buffer, int64_t nr_bytes) {
136: 	auto &compressed_file = handle.Cast<CompressedFile>();
137: 	return compressed_file.WriteData(data_ptr_cast(buffer), nr_bytes);
138: }
139: 
140: void CompressedFileSystem::Reset(FileHandle &handle) {
141: 	auto &compressed_file = handle.Cast<CompressedFile>();
142: 	compressed_file.child_handle->Reset();
143: 	compressed_file.Initialize(compressed_file.write);
144: }
145: 
146: int64_t CompressedFileSystem::GetFileSize(FileHandle &handle) {
147: 	auto &compressed_file = handle.Cast<CompressedFile>();
148: 	return NumericCast<int64_t>(compressed_file.child_handle->GetFileSize());
149: }
150: 
151: bool CompressedFileSystem::OnDiskFile(FileHandle &handle) {
152: 	auto &compressed_file = handle.Cast<CompressedFile>();
153: 	return compressed_file.child_handle->OnDiskFile();
154: }
155: 
156: bool CompressedFileSystem::CanSeek() {
157: 	return false;
158: }
159: 
160: } // namespace duckdb
[end of src/common/compressed_file_system.cpp]
[start of src/execution/operator/aggregate/physical_window.cpp]
1: #include "duckdb/execution/operator/aggregate/physical_window.hpp"
2: 
3: #include "duckdb/common/sort/partition_state.hpp"
4: #include "duckdb/function/window/window_aggregate_function.hpp"
5: #include "duckdb/function/window/window_executor.hpp"
6: #include "duckdb/function/window/window_rank_function.hpp"
7: #include "duckdb/function/window/window_rownumber_function.hpp"
8: #include "duckdb/function/window/window_shared_expressions.hpp"
9: #include "duckdb/function/window/window_value_function.hpp"
10: #include "duckdb/planner/expression/bound_window_expression.hpp"
11: //
12: #include <numeric>
13: 
14: namespace duckdb {
15: 
16: //	Global sink state
17: class WindowGlobalSinkState;
18: 
19: enum WindowGroupStage : uint8_t { SINK, FINALIZE, GETDATA, DONE };
20: 
21: class WindowHashGroup {
22: public:
23: 	using HashGroupPtr = unique_ptr<PartitionGlobalHashGroup>;
24: 	using OrderMasks = PartitionGlobalHashGroup::OrderMasks;
25: 	using ExecutorGlobalStatePtr = unique_ptr<WindowExecutorGlobalState>;
26: 	using ExecutorGlobalStates = vector<ExecutorGlobalStatePtr>;
27: 	using ExecutorLocalStatePtr = unique_ptr<WindowExecutorLocalState>;
28: 	using ExecutorLocalStates = vector<ExecutorLocalStatePtr>;
29: 	using ThreadLocalStates = vector<ExecutorLocalStates>;
30: 
31: 	WindowHashGroup(WindowGlobalSinkState &gstate, const idx_t hash_bin_p);
32: 
33: 	ExecutorGlobalStates &Initialize(WindowGlobalSinkState &gstate);
34: 
35: 	// Scan all of the blocks during the build phase
36: 	unique_ptr<RowDataCollectionScanner> GetBuildScanner(idx_t block_idx) const {
37: 		if (!rows) {
38: 			return nullptr;
39: 		}
40: 		return make_uniq<RowDataCollectionScanner>(*rows, *heap, layout, external, block_idx, false);
41: 	}
42: 
43: 	// Scan a single block during the evaluate phase
44: 	unique_ptr<RowDataCollectionScanner> GetEvaluateScanner(idx_t block_idx) const {
45: 		//	Second pass can flush
46: 		D_ASSERT(rows);
47: 		return make_uniq<RowDataCollectionScanner>(*rows, *heap, layout, external, block_idx, true);
48: 	}
49: 
50: 	// The processing stage for this group
51: 	WindowGroupStage GetStage() const {
52: 		return stage;
53: 	}
54: 
55: 	bool TryPrepareNextStage() {
56: 		lock_guard<mutex> prepare_guard(lock);
57: 		switch (stage.load()) {
58: 		case WindowGroupStage::SINK:
59: 			if (sunk == count) {
60: 				stage = WindowGroupStage::FINALIZE;
61: 				return true;
62: 			}
63: 			return false;
64: 		case WindowGroupStage::FINALIZE:
65: 			if (finalized == blocks) {
66: 				stage = WindowGroupStage::GETDATA;
67: 				return true;
68: 			}
69: 			return false;
70: 		default:
71: 			// never block in GETDATA
72: 			return true;
73: 		}
74: 	}
75: 
76: 	//! The hash partition data
77: 	HashGroupPtr hash_group;
78: 	//! The size of the group
79: 	idx_t count = 0;
80: 	//! The number of blocks in the group
81: 	idx_t blocks = 0;
82: 	unique_ptr<RowDataCollection> rows;
83: 	unique_ptr<RowDataCollection> heap;
84: 	RowLayout layout;
85: 	//! The partition boundary mask
86: 	ValidityMask partition_mask;
87: 	//! The order boundary mask
88: 	OrderMasks order_masks;
89: 	//! The fully materialised data collection
90: 	unique_ptr<WindowCollection> collection;
91: 	//! External paging
92: 	bool external;
93: 	// The processing stage for this group
94: 	atomic<WindowGroupStage> stage;
95: 	//! The function global states for this hash group
96: 	ExecutorGlobalStates gestates;
97: 	//! Executor local states, one per thread
98: 	ThreadLocalStates thread_states;
99: 
100: 	//! The bin number
101: 	idx_t hash_bin;
102: 	//! Single threading lock
103: 	mutex lock;
104: 	//! Count of sunk rows
105: 	std::atomic<idx_t> sunk;
106: 	//! Count of finalized blocks
107: 	std::atomic<idx_t> finalized;
108: 	//! The number of tasks left before we should be deleted
109: 	std::atomic<idx_t> tasks_remaining;
110: 	//! The output ordering batch index this hash group starts at
111: 	idx_t batch_base;
112: 
113: private:
114: 	void MaterializeSortedData();
115: };
116: 
117: class WindowPartitionGlobalSinkState;
118: 
119: class WindowGlobalSinkState : public GlobalSinkState {
120: public:
121: 	using ExecutorPtr = unique_ptr<WindowExecutor>;
122: 	using Executors = vector<ExecutorPtr>;
123: 
124: 	WindowGlobalSinkState(const PhysicalWindow &op, ClientContext &context);
125: 
126: 	//! Parent operator
127: 	const PhysicalWindow &op;
128: 	//! Execution context
129: 	ClientContext &context;
130: 	//! The partitioned sunk data
131: 	unique_ptr<WindowPartitionGlobalSinkState> global_partition;
132: 	//! The execution functions
133: 	Executors executors;
134: 	//! The shared expressions library
135: 	WindowSharedExpressions shared;
136: };
137: 
138: class WindowPartitionGlobalSinkState : public PartitionGlobalSinkState {
139: public:
140: 	using WindowHashGroupPtr = unique_ptr<WindowHashGroup>;
141: 
142: 	WindowPartitionGlobalSinkState(WindowGlobalSinkState &gsink, const BoundWindowExpression &wexpr)
143: 	    : PartitionGlobalSinkState(gsink.context, wexpr.partitions, wexpr.orders, gsink.op.children[0]->types,
144: 	                               wexpr.partitions_stats, gsink.op.estimated_cardinality),
145: 	      gsink(gsink) {
146: 	}
147: 	~WindowPartitionGlobalSinkState() override = default;
148: 
149: 	void OnBeginMerge() override {
150: 		PartitionGlobalSinkState::OnBeginMerge();
151: 		window_hash_groups.resize(hash_groups.size());
152: 	}
153: 
154: 	void OnSortedPartition(const idx_t group_idx) override {
155: 		PartitionGlobalSinkState::OnSortedPartition(group_idx);
156: 		window_hash_groups[group_idx] = make_uniq<WindowHashGroup>(gsink, group_idx);
157: 	}
158: 
159: 	//! Operator global sink state
160: 	WindowGlobalSinkState &gsink;
161: 	//! The sorted hash groups
162: 	vector<WindowHashGroupPtr> window_hash_groups;
163: };
164: 
165: //	Per-thread sink state
166: class WindowLocalSinkState : public LocalSinkState {
167: public:
168: 	WindowLocalSinkState(ClientContext &context, const WindowGlobalSinkState &gstate)
169: 	    : local_partition(context, *gstate.global_partition) {
170: 	}
171: 
172: 	void Sink(DataChunk &input_chunk) {
173: 		local_partition.Sink(input_chunk);
174: 	}
175: 
176: 	void Combine() {
177: 		local_partition.Combine();
178: 	}
179: 
180: 	PartitionLocalSinkState local_partition;
181: };
182: 
183: // this implements a sorted window functions variant
184: PhysicalWindow::PhysicalWindow(vector<LogicalType> types, vector<unique_ptr<Expression>> select_list_p,
185:                                idx_t estimated_cardinality, PhysicalOperatorType type)
186:     : PhysicalOperator(type, std::move(types), estimated_cardinality), select_list(std::move(select_list_p)),
187:       order_idx(0), is_order_dependent(false) {
188: 
189: 	idx_t max_orders = 0;
190: 	for (idx_t i = 0; i < select_list.size(); ++i) {
191: 		auto &expr = select_list[i];
192: 		D_ASSERT(expr->GetExpressionClass() == ExpressionClass::BOUND_WINDOW);
193: 		auto &bound_window = expr->Cast<BoundWindowExpression>();
194: 		if (bound_window.partitions.empty() && bound_window.orders.empty()) {
195: 			is_order_dependent = true;
196: 		}
197: 
198: 		if (bound_window.orders.size() > max_orders) {
199: 			order_idx = i;
200: 			max_orders = bound_window.orders.size();
201: 		}
202: 	}
203: }
204: 
205: static unique_ptr<WindowExecutor> WindowExecutorFactory(BoundWindowExpression &wexpr, ClientContext &context,
206:                                                         WindowSharedExpressions &shared, WindowAggregationMode mode) {
207: 	switch (wexpr.GetExpressionType()) {
208: 	case ExpressionType::WINDOW_AGGREGATE:
209: 		return make_uniq<WindowAggregateExecutor>(wexpr, context, shared, mode);
210: 	case ExpressionType::WINDOW_ROW_NUMBER:
211: 		return make_uniq<WindowRowNumberExecutor>(wexpr, context, shared);
212: 	case ExpressionType::WINDOW_RANK_DENSE:
213: 		return make_uniq<WindowDenseRankExecutor>(wexpr, context, shared);
214: 	case ExpressionType::WINDOW_RANK:
215: 		return make_uniq<WindowRankExecutor>(wexpr, context, shared);
216: 	case ExpressionType::WINDOW_PERCENT_RANK:
217: 		return make_uniq<WindowPercentRankExecutor>(wexpr, context, shared);
218: 	case ExpressionType::WINDOW_CUME_DIST:
219: 		return make_uniq<WindowCumeDistExecutor>(wexpr, context, shared);
220: 	case ExpressionType::WINDOW_NTILE:
221: 		return make_uniq<WindowNtileExecutor>(wexpr, context, shared);
222: 	case ExpressionType::WINDOW_LEAD:
223: 	case ExpressionType::WINDOW_LAG:
224: 		return make_uniq<WindowLeadLagExecutor>(wexpr, context, shared);
225: 	case ExpressionType::WINDOW_FIRST_VALUE:
226: 		return make_uniq<WindowFirstValueExecutor>(wexpr, context, shared);
227: 	case ExpressionType::WINDOW_LAST_VALUE:
228: 		return make_uniq<WindowLastValueExecutor>(wexpr, context, shared);
229: 	case ExpressionType::WINDOW_NTH_VALUE:
230: 		return make_uniq<WindowNthValueExecutor>(wexpr, context, shared);
231: 		break;
232: 	default:
233: 		throw InternalException("Window aggregate type %s", ExpressionTypeToString(wexpr.GetExpressionType()));
234: 	}
235: }
236: 
237: WindowGlobalSinkState::WindowGlobalSinkState(const PhysicalWindow &op, ClientContext &context)
238:     : op(op), context(context) {
239: 
240: 	D_ASSERT(op.select_list[op.order_idx]->GetExpressionClass() == ExpressionClass::BOUND_WINDOW);
241: 	auto &wexpr = op.select_list[op.order_idx]->Cast<BoundWindowExpression>();
242: 
243: 	const auto mode = DBConfig::GetConfig(context).options.window_mode;
244: 	for (idx_t expr_idx = 0; expr_idx < op.select_list.size(); ++expr_idx) {
245: 		D_ASSERT(op.select_list[expr_idx]->GetExpressionClass() == ExpressionClass::BOUND_WINDOW);
246: 		auto &wexpr = op.select_list[expr_idx]->Cast<BoundWindowExpression>();
247: 		auto wexec = WindowExecutorFactory(wexpr, context, shared, mode);
248: 		executors.emplace_back(std::move(wexec));
249: 	}
250: 
251: 	global_partition = make_uniq<WindowPartitionGlobalSinkState>(*this, wexpr);
252: }
253: 
254: //===--------------------------------------------------------------------===//
255: // Sink
256: //===--------------------------------------------------------------------===//
257: SinkResultType PhysicalWindow::Sink(ExecutionContext &context, DataChunk &chunk, OperatorSinkInput &input) const {
258: 	auto &lstate = input.local_state.Cast<WindowLocalSinkState>();
259: 
260: 	lstate.Sink(chunk);
261: 
262: 	return SinkResultType::NEED_MORE_INPUT;
263: }
264: 
265: SinkCombineResultType PhysicalWindow::Combine(ExecutionContext &context, OperatorSinkCombineInput &input) const {
266: 	auto &lstate = input.local_state.Cast<WindowLocalSinkState>();
267: 	lstate.Combine();
268: 
269: 	return SinkCombineResultType::FINISHED;
270: }
271: 
272: unique_ptr<LocalSinkState> PhysicalWindow::GetLocalSinkState(ExecutionContext &context) const {
273: 	auto &gstate = sink_state->Cast<WindowGlobalSinkState>();
274: 	return make_uniq<WindowLocalSinkState>(context.client, gstate);
275: }
276: 
277: unique_ptr<GlobalSinkState> PhysicalWindow::GetGlobalSinkState(ClientContext &context) const {
278: 	return make_uniq<WindowGlobalSinkState>(*this, context);
279: }
280: 
281: SinkFinalizeType PhysicalWindow::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
282:                                           OperatorSinkFinalizeInput &input) const {
283: 	auto &state = input.global_state.Cast<WindowGlobalSinkState>();
284: 
285: 	//	Did we get any data?
286: 	if (!state.global_partition->count) {
287: 		return SinkFinalizeType::NO_OUTPUT_POSSIBLE;
288: 	}
289: 
290: 	// Do we have any sorting to schedule?
291: 	if (state.global_partition->rows) {
292: 		D_ASSERT(!state.global_partition->grouping_data);
293: 		return state.global_partition->rows->count ? SinkFinalizeType::READY : SinkFinalizeType::NO_OUTPUT_POSSIBLE;
294: 	}
295: 
296: 	// Find the first group to sort
297: 	if (!state.global_partition->HasMergeTasks()) {
298: 		// Empty input!
299: 		return SinkFinalizeType::NO_OUTPUT_POSSIBLE;
300: 	}
301: 
302: 	// Schedule all the sorts for maximum thread utilisation
303: 	auto new_event = make_shared_ptr<PartitionMergeEvent>(*state.global_partition, pipeline, *this);
304: 	event.InsertEvent(std::move(new_event));
305: 
306: 	return SinkFinalizeType::READY;
307: }
308: 
309: //===--------------------------------------------------------------------===//
310: // Source
311: //===--------------------------------------------------------------------===//
312: class WindowGlobalSourceState : public GlobalSourceState {
313: public:
314: 	using ScannerPtr = unique_ptr<RowDataCollectionScanner>;
315: 
316: 	struct Task {
317: 		Task(WindowGroupStage stage, idx_t group_idx, idx_t max_idx)
318: 		    : stage(stage), group_idx(group_idx), thread_idx(0), max_idx(max_idx) {
319: 		}
320: 		WindowGroupStage stage;
321: 		//! The hash group
322: 		idx_t group_idx;
323: 		//! The thread index (for local state)
324: 		idx_t thread_idx;
325: 		//! The total block index count
326: 		idx_t max_idx;
327: 		//! The first block index count
328: 		idx_t begin_idx = 0;
329: 		//! The end block index count
330: 		idx_t end_idx = 0;
331: 	};
332: 	using TaskPtr = optional_ptr<Task>;
333: 
334: 	WindowGlobalSourceState(ClientContext &context_p, WindowGlobalSinkState &gsink_p);
335: 
336: 	//! Build task list
337: 	void CreateTaskList();
338: 
339: 	//! Are there any more tasks?
340: 	bool HasMoreTasks() const {
341: 		return !stopped && next_task < tasks.size();
342: 	}
343: 	bool HasUnfinishedTasks() const {
344: 		return !stopped && finished < tasks.size();
345: 	}
346: 	//! Try to advance the group stage
347: 	bool TryPrepareNextStage();
348: 	//! Get the next task given the current state
349: 	bool TryNextTask(TaskPtr &task);
350: 	//! Finish a task
351: 	void FinishTask(TaskPtr task);
352: 
353: 	//! Context for executing computations
354: 	ClientContext &context;
355: 	//! All the sunk data
356: 	WindowGlobalSinkState &gsink;
357: 	//! The total number of blocks to process;
358: 	idx_t total_blocks = 0;
359: 	//! The number of local states
360: 	atomic<idx_t> locals;
361: 	//! The list of tasks
362: 	vector<Task> tasks;
363: 	//! The the next task
364: 	atomic<idx_t> next_task;
365: 	//! The the number of finished tasks
366: 	atomic<idx_t> finished;
367: 	//! Stop producing tasks
368: 	atomic<bool> stopped;
369: 	//! The number of rows returned
370: 	atomic<idx_t> returned;
371: 
372: public:
373: 	idx_t MaxThreads() override {
374: 		return total_blocks;
375: 	}
376: };
377: 
378: WindowGlobalSourceState::WindowGlobalSourceState(ClientContext &context_p, WindowGlobalSinkState &gsink_p)
379:     : context(context_p), gsink(gsink_p), locals(0), next_task(0), finished(0), stopped(false), returned(0) {
380: 	auto &gpart = gsink.global_partition;
381: 	auto &window_hash_groups = gsink.global_partition->window_hash_groups;
382: 
383: 	if (window_hash_groups.empty()) {
384: 		//	OVER()
385: 		if (gpart->rows && !gpart->rows->blocks.empty()) {
386: 			// We need to construct the single WindowHashGroup here because the sort tasks will not be run.
387: 			window_hash_groups.emplace_back(make_uniq<WindowHashGroup>(gsink, idx_t(0)));
388: 			total_blocks = gpart->rows->blocks.size();
389: 		}
390: 	} else {
391: 		idx_t batch_base = 0;
392: 		for (auto &window_hash_group : window_hash_groups) {
393: 			if (!window_hash_group) {
394: 				continue;
395: 			}
396: 			auto &rows = window_hash_group->rows;
397: 			if (!rows) {
398: 				continue;
399: 			}
400: 
401: 			const auto block_count = window_hash_group->rows->blocks.size();
402: 			window_hash_group->batch_base = batch_base;
403: 			batch_base += block_count;
404: 		}
405: 		total_blocks = batch_base;
406: 	}
407: }
408: 
409: void WindowGlobalSourceState::CreateTaskList() {
410: 	//	Check whether we have a task list outside the mutex.
411: 	if (next_task.load()) {
412: 		return;
413: 	}
414: 
415: 	auto guard = Lock();
416: 
417: 	auto &window_hash_groups = gsink.global_partition->window_hash_groups;
418: 	if (!tasks.empty()) {
419: 		return;
420: 	}
421: 
422: 	//    Sort the groups from largest to smallest
423: 	if (window_hash_groups.empty()) {
424: 		return;
425: 	}
426: 
427: 	using PartitionBlock = std::pair<idx_t, idx_t>;
428: 	vector<PartitionBlock> partition_blocks;
429: 	for (idx_t group_idx = 0; group_idx < window_hash_groups.size(); ++group_idx) {
430: 		auto &window_hash_group = window_hash_groups[group_idx];
431: 		partition_blocks.emplace_back(window_hash_group->rows->blocks.size(), group_idx);
432: 	}
433: 	std::sort(partition_blocks.begin(), partition_blocks.end(), std::greater<PartitionBlock>());
434: 
435: 	//	Schedule the largest group on as many threads as possible
436: 	const auto threads = locals.load();
437: 	const auto &max_block = partition_blocks.front();
438: 	const auto per_thread = (max_block.first + threads - 1) / threads;
439: 	if (!per_thread) {
440: 		throw InternalException("No blocks per thread! %ld threads, %ld groups, %ld blocks, %ld hash group", threads,
441: 		                        partition_blocks.size(), max_block.first, max_block.second);
442: 	}
443: 
444: 	//	TODO: Generate dynamically instead of building a big list?
445: 	vector<WindowGroupStage> states {WindowGroupStage::SINK, WindowGroupStage::FINALIZE, WindowGroupStage::GETDATA};
446: 	for (const auto &b : partition_blocks) {
447: 		auto &window_hash_group = *window_hash_groups[b.second];
448: 		for (const auto &state : states) {
449: 			idx_t thread_count = 0;
450: 			for (Task task(state, b.second, b.first); task.begin_idx < task.max_idx; task.begin_idx += per_thread) {
451: 				task.end_idx = MinValue<idx_t>(task.begin_idx + per_thread, task.max_idx);
452: 				tasks.emplace_back(task);
453: 				window_hash_group.tasks_remaining++;
454: 				thread_count = ++task.thread_idx;
455: 			}
456: 			window_hash_group.thread_states.resize(thread_count);
457: 		}
458: 	}
459: }
460: 
461: void WindowHashGroup::MaterializeSortedData() {
462: 	auto &global_sort_state = *hash_group->global_sort;
463: 	if (global_sort_state.sorted_blocks.empty()) {
464: 		return;
465: 	}
466: 
467: 	// scan the sorted row data
468: 	D_ASSERT(global_sort_state.sorted_blocks.size() == 1);
469: 	auto &sb = *global_sort_state.sorted_blocks[0];
470: 
471: 	// Free up some memory before allocating more
472: 	sb.radix_sorting_data.clear();
473: 	sb.blob_sorting_data = nullptr;
474: 
475: 	// Move the sorting row blocks into our RDCs
476: 	auto &buffer_manager = global_sort_state.buffer_manager;
477: 	auto &sd = *sb.payload_data;
478: 
479: 	// Data blocks are required
480: 	D_ASSERT(!sd.data_blocks.empty());
481: 	auto &block = sd.data_blocks[0];
482: 	rows = make_uniq<RowDataCollection>(buffer_manager, block->capacity, block->entry_size);
483: 	rows->blocks = std::move(sd.data_blocks);
484: 	rows->count = std::accumulate(rows->blocks.begin(), rows->blocks.end(), idx_t(0),
485: 	                              [&](idx_t c, const unique_ptr<RowDataBlock> &b) { return c + b->count; });
486: 
487: 	// Heap blocks are optional, but we want both for iteration.
488: 	if (!sd.heap_blocks.empty()) {
489: 		auto &block = sd.heap_blocks[0];
490: 		heap = make_uniq<RowDataCollection>(buffer_manager, block->capacity, block->entry_size);
491: 		heap->blocks = std::move(sd.heap_blocks);
492: 		hash_group.reset();
493: 	} else {
494: 		heap = make_uniq<RowDataCollection>(buffer_manager, buffer_manager.GetBlockSize(), 1U, true);
495: 	}
496: 	heap->count = std::accumulate(heap->blocks.begin(), heap->blocks.end(), idx_t(0),
497: 	                              [&](idx_t c, const unique_ptr<RowDataBlock> &b) { return c + b->count; });
498: }
499: 
500: WindowHashGroup::WindowHashGroup(WindowGlobalSinkState &gstate, const idx_t hash_bin_p)
501:     : count(0), blocks(0), stage(WindowGroupStage::SINK), hash_bin(hash_bin_p), sunk(0), finalized(0),
502:       tasks_remaining(0), batch_base(0) {
503: 	// There are three types of partitions:
504: 	// 1. No partition (no sorting)
505: 	// 2. One partition (sorting, but no hashing)
506: 	// 3. Multiple partitions (sorting and hashing)
507: 
508: 	//	How big is the partition?
509: 	auto &gpart = *gstate.global_partition;
510: 	layout.Initialize(gpart.payload_types);
511: 	if (hash_bin < gpart.hash_groups.size() && gpart.hash_groups[hash_bin]) {
512: 		count = gpart.hash_groups[hash_bin]->count;
513: 	} else if (gpart.rows && !hash_bin) {
514: 		count = gpart.count;
515: 	} else {
516: 		return;
517: 	}
518: 
519: 	//	Initialise masks to false
520: 	partition_mask.Initialize(count);
521: 	partition_mask.SetAllInvalid(count);
522: 
523: 	const auto &executors = gstate.executors;
524: 	for (auto &wexec : executors) {
525: 		auto &wexpr = wexec->wexpr;
526: 		auto &order_mask = order_masks[wexpr.partitions.size() + wexpr.orders.size()];
527: 		if (order_mask.IsMaskSet()) {
528: 			continue;
529: 		}
530: 		order_mask.Initialize(count);
531: 		order_mask.SetAllInvalid(count);
532: 	}
533: 
534: 	// Scan the sorted data into new Collections
535: 	external = gpart.external;
536: 	if (gpart.rows && !hash_bin) {
537: 		// Simple mask
538: 		partition_mask.SetValidUnsafe(0);
539: 		for (auto &order_mask : order_masks) {
540: 			order_mask.second.SetValidUnsafe(0);
541: 		}
542: 		//	No partition - align the heap blocks with the row blocks
543: 		rows = gpart.rows->CloneEmpty(gpart.rows->keep_pinned);
544: 		heap = gpart.strings->CloneEmpty(gpart.strings->keep_pinned);
545: 		RowDataCollectionScanner::AlignHeapBlocks(*rows, *heap, *gpart.rows, *gpart.strings, layout);
546: 		external = true;
547: 	} else if (hash_bin < gpart.hash_groups.size()) {
548: 		// Overwrite the collections with the sorted data
549: 		D_ASSERT(gpart.hash_groups[hash_bin].get());
550: 		hash_group = std::move(gpart.hash_groups[hash_bin]);
551: 		hash_group->ComputeMasks(partition_mask, order_masks);
552: 		external = hash_group->global_sort->external;
553: 		MaterializeSortedData();
554: 	}
555: 
556: 	if (rows) {
557: 		blocks = rows->blocks.size();
558: 	}
559: 
560: 	// Set up the collection for any fully materialised data
561: 	const auto &shared = WindowSharedExpressions::GetSortedExpressions(gstate.shared.coll_shared);
562: 	vector<LogicalType> types;
563: 	for (auto &expr : shared) {
564: 		types.emplace_back(expr->return_type);
565: 	}
566: 	auto &buffer_manager = BufferManager::GetBufferManager(gstate.context);
567: 	collection = make_uniq<WindowCollection>(buffer_manager, count, types);
568: }
569: 
570: // Per-thread scan state
571: class WindowLocalSourceState : public LocalSourceState {
572: public:
573: 	using Task = WindowGlobalSourceState::Task;
574: 	using TaskPtr = optional_ptr<Task>;
575: 
576: 	explicit WindowLocalSourceState(WindowGlobalSourceState &gsource);
577: 
578: 	//! Does the task have more work to do?
579: 	bool TaskFinished() const {
580: 		return !task || task->begin_idx == task->end_idx;
581: 	}
582: 	//! Assign the next task
583: 	bool TryAssignTask();
584: 	//! Execute a step in the current task
585: 	void ExecuteTask(DataChunk &chunk);
586: 
587: 	//! The shared source state
588: 	WindowGlobalSourceState &gsource;
589: 	//! The current batch index (for output reordering)
590: 	idx_t batch_index;
591: 	//! The task this thread is working on
592: 	TaskPtr task;
593: 	//! The current source being processed
594: 	optional_ptr<WindowHashGroup> window_hash_group;
595: 	//! The scan cursor
596: 	unique_ptr<RowDataCollectionScanner> scanner;
597: 	//! Buffer for the inputs
598: 	DataChunk input_chunk;
599: 	//! Buffer for window results
600: 	DataChunk output_chunk;
601: 
602: protected:
603: 	void Sink();
604: 	void Finalize();
605: 	void GetData(DataChunk &chunk);
606: 
607: 	//! Storage and evaluation for the fully materialised data
608: 	unique_ptr<WindowBuilder> builder;
609: 	ExpressionExecutor coll_exec;
610: 	DataChunk coll_chunk;
611: 
612: 	//! Storage and evaluation for chunks used in the sink/build phase
613: 	ExpressionExecutor sink_exec;
614: 	DataChunk sink_chunk;
615: 
616: 	//! Storage and evaluation for chunks used in the evaluate phase
617: 	ExpressionExecutor eval_exec;
618: 	DataChunk eval_chunk;
619: };
620: 
621: WindowHashGroup::ExecutorGlobalStates &WindowHashGroup::Initialize(WindowGlobalSinkState &gsink) {
622: 	//	Single-threaded building as this is mostly memory allocation
623: 	lock_guard<mutex> gestate_guard(lock);
624: 	const auto &executors = gsink.executors;
625: 	if (gestates.size() == executors.size()) {
626: 		return gestates;
627: 	}
628: 
629: 	// These can be large so we defer building them until we are ready.
630: 	for (auto &wexec : executors) {
631: 		auto &wexpr = wexec->wexpr;
632: 		auto &order_mask = order_masks[wexpr.partitions.size() + wexpr.orders.size()];
633: 		gestates.emplace_back(wexec->GetGlobalState(count, partition_mask, order_mask));
634: 	}
635: 
636: 	return gestates;
637: }
638: 
639: void WindowLocalSourceState::Sink() {
640: 	D_ASSERT(task);
641: 	D_ASSERT(task->stage == WindowGroupStage::SINK);
642: 
643: 	auto &gsink = gsource.gsink;
644: 	const auto &executors = gsink.executors;
645: 
646: 	// Create the global state for each function
647: 	// These can be large so we defer building them until we are ready.
648: 	auto &gestates = window_hash_group->Initialize(gsink);
649: 
650: 	//	Set up the local states
651: 	auto &local_states = window_hash_group->thread_states.at(task->thread_idx);
652: 	if (local_states.empty()) {
653: 		for (idx_t w = 0; w < executors.size(); ++w) {
654: 			local_states.emplace_back(executors[w]->GetLocalState(*gestates[w]));
655: 		}
656: 	}
657: 
658: 	//	First pass over the input without flushing
659: 	for (; task->begin_idx < task->end_idx; ++task->begin_idx) {
660: 		scanner = window_hash_group->GetBuildScanner(task->begin_idx);
661: 		if (!scanner) {
662: 			break;
663: 		}
664: 		while (true) {
665: 			//	TODO: Try to align on validity mask boundaries by starting ragged?
666: 			idx_t input_idx = scanner->Scanned();
667: 			input_chunk.Reset();
668: 			scanner->Scan(input_chunk);
669: 			if (input_chunk.size() == 0) {
670: 				break;
671: 			}
672: 
673: 			//	Compute fully materialised expressions
674: 			if (coll_chunk.data.empty()) {
675: 				coll_chunk.SetCardinality(input_chunk);
676: 			} else {
677: 				coll_chunk.Reset();
678: 				coll_exec.Execute(input_chunk, coll_chunk);
679: 				auto collection = window_hash_group->collection.get();
680: 				if (!builder || &builder->collection != collection) {
681: 					builder = make_uniq<WindowBuilder>(*collection);
682: 				}
683: 
684: 				builder->Sink(coll_chunk, input_idx);
685: 			}
686: 
687: 			// Compute sink expressions
688: 			if (sink_chunk.data.empty()) {
689: 				sink_chunk.SetCardinality(input_chunk);
690: 			} else {
691: 				sink_chunk.Reset();
692: 				sink_exec.Execute(input_chunk, sink_chunk);
693: 			}
694: 
695: 			for (idx_t w = 0; w < executors.size(); ++w) {
696: 				executors[w]->Sink(sink_chunk, coll_chunk, input_idx, *gestates[w], *local_states[w]);
697: 			}
698: 
699: 			window_hash_group->sunk += input_chunk.size();
700: 		}
701: 
702: 		// External scanning assumes all blocks are swizzled.
703: 		scanner->SwizzleBlock(task->begin_idx);
704: 		scanner.reset();
705: 	}
706: }
707: 
708: void WindowLocalSourceState::Finalize() {
709: 	D_ASSERT(task);
710: 	D_ASSERT(task->stage == WindowGroupStage::FINALIZE);
711: 
712: 	// First finalize the collection (so the executors can use it)
713: 	auto &gsink = gsource.gsink;
714: 	if (window_hash_group->collection) {
715: 		window_hash_group->collection->Combine(gsink.shared.coll_validity);
716: 	}
717: 
718: 	// Finalize all the executors.
719: 	// Parallel finalisation is handled internally by the executor,
720: 	// and should not return until all threads have completed work.
721: 	const auto &executors = gsink.executors;
722: 	auto &gestates = window_hash_group->gestates;
723: 	auto &local_states = window_hash_group->thread_states.at(task->thread_idx);
724: 	for (idx_t w = 0; w < executors.size(); ++w) {
725: 		executors[w]->Finalize(*gestates[w], *local_states[w], window_hash_group->collection);
726: 	}
727: 
728: 	//	Mark this range as done
729: 	window_hash_group->finalized += (task->end_idx - task->begin_idx);
730: 	task->begin_idx = task->end_idx;
731: }
732: 
733: WindowLocalSourceState::WindowLocalSourceState(WindowGlobalSourceState &gsource)
734:     : gsource(gsource), batch_index(0), coll_exec(gsource.context), sink_exec(gsource.context),
735:       eval_exec(gsource.context) {
736: 	auto &gsink = gsource.gsink;
737: 	auto &global_partition = *gsink.global_partition;
738: 
739: 	input_chunk.Initialize(global_partition.allocator, global_partition.payload_types);
740: 
741: 	vector<LogicalType> output_types;
742: 	for (auto &wexec : gsink.executors) {
743: 		auto &wexpr = wexec->wexpr;
744: 		output_types.emplace_back(wexpr.return_type);
745: 	}
746: 	output_chunk.Initialize(global_partition.allocator, output_types);
747: 
748: 	auto &shared = gsink.shared;
749: 	shared.PrepareCollection(coll_exec, coll_chunk);
750: 	shared.PrepareSink(sink_exec, sink_chunk);
751: 	shared.PrepareEvaluate(eval_exec, eval_chunk);
752: 
753: 	++gsource.locals;
754: }
755: 
756: bool WindowGlobalSourceState::TryNextTask(TaskPtr &task) {
757: 	auto guard = Lock();
758: 	if (next_task >= tasks.size() || stopped) {
759: 		task = nullptr;
760: 		return false;
761: 	}
762: 
763: 	//	If the next task matches the current state of its group, then we can use it
764: 	//	Otherwise block.
765: 	task = &tasks[next_task];
766: 
767: 	auto &gpart = *gsink.global_partition;
768: 	auto &window_hash_group = gpart.window_hash_groups[task->group_idx];
769: 	auto group_stage = window_hash_group->GetStage();
770: 
771: 	if (task->stage == group_stage) {
772: 		++next_task;
773: 		return true;
774: 	}
775: 
776: 	task = nullptr;
777: 	return false;
778: }
779: 
780: void WindowGlobalSourceState::FinishTask(TaskPtr task) {
781: 	if (!task) {
782: 		return;
783: 	}
784: 
785: 	auto &gpart = *gsink.global_partition;
786: 	auto &finished_hash_group = gpart.window_hash_groups[task->group_idx];
787: 	D_ASSERT(finished_hash_group);
788: 
789: 	if (!--finished_hash_group->tasks_remaining) {
790: 		finished_hash_group.reset();
791: 	}
792: }
793: 
794: bool WindowLocalSourceState::TryAssignTask() {
795: 	// Because downstream operators may be using our internal buffers,
796: 	// we can't "finish" a task until we are about to get the next one.
797: 
798: 	// Scanner first, as it may be referencing sort blocks in the hash group
799: 	scanner.reset();
800: 	gsource.FinishTask(task);
801: 
802: 	return gsource.TryNextTask(task);
803: }
804: 
805: bool WindowGlobalSourceState::TryPrepareNextStage() {
806: 	if (next_task >= tasks.size() || stopped) {
807: 		return true;
808: 	}
809: 
810: 	auto task = &tasks[next_task];
811: 	auto window_hash_group = gsink.global_partition->window_hash_groups[task->group_idx].get();
812: 	return window_hash_group->TryPrepareNextStage();
813: }
814: 
815: void WindowLocalSourceState::ExecuteTask(DataChunk &result) {
816: 	auto &gsink = gsource.gsink;
817: 
818: 	// Update the hash group
819: 	window_hash_group = gsink.global_partition->window_hash_groups[task->group_idx].get();
820: 
821: 	// Process the new state
822: 	switch (task->stage) {
823: 	case WindowGroupStage::SINK:
824: 		Sink();
825: 		D_ASSERT(TaskFinished());
826: 		break;
827: 	case WindowGroupStage::FINALIZE:
828: 		Finalize();
829: 		D_ASSERT(TaskFinished());
830: 		break;
831: 	case WindowGroupStage::GETDATA:
832: 		D_ASSERT(!TaskFinished());
833: 		GetData(result);
834: 		break;
835: 	default:
836: 		throw InternalException("Invalid window source state.");
837: 	}
838: 
839: 	// Count this task as finished.
840: 	if (TaskFinished()) {
841: 		++gsource.finished;
842: 	}
843: }
844: 
845: void WindowLocalSourceState::GetData(DataChunk &result) {
846: 	D_ASSERT(window_hash_group->GetStage() == WindowGroupStage::GETDATA);
847: 
848: 	if (!scanner || !scanner->Remaining()) {
849: 		scanner = window_hash_group->GetEvaluateScanner(task->begin_idx);
850: 		batch_index = window_hash_group->batch_base + task->begin_idx;
851: 	}
852: 
853: 	const auto position = scanner->Scanned();
854: 	input_chunk.Reset();
855: 	scanner->Scan(input_chunk);
856: 
857: 	const auto &executors = gsource.gsink.executors;
858: 	auto &gestates = window_hash_group->gestates;
859: 	auto &local_states = window_hash_group->thread_states.at(task->thread_idx);
860: 	output_chunk.Reset();
861: 	for (idx_t expr_idx = 0; expr_idx < executors.size(); ++expr_idx) {
862: 		auto &executor = *executors[expr_idx];
863: 		auto &gstate = *gestates[expr_idx];
864: 		auto &lstate = *local_states[expr_idx];
865: 		auto &result = output_chunk.data[expr_idx];
866: 		if (eval_chunk.data.empty()) {
867: 			eval_chunk.SetCardinality(input_chunk);
868: 		} else {
869: 			eval_chunk.Reset();
870: 			eval_exec.Execute(input_chunk, eval_chunk);
871: 		}
872: 		executor.Evaluate(position, eval_chunk, result, lstate, gstate);
873: 	}
874: 	output_chunk.SetCardinality(input_chunk);
875: 	output_chunk.Verify();
876: 
877: 	idx_t out_idx = 0;
878: 	result.SetCardinality(input_chunk);
879: 	for (idx_t col_idx = 0; col_idx < input_chunk.ColumnCount(); col_idx++) {
880: 		result.data[out_idx++].Reference(input_chunk.data[col_idx]);
881: 	}
882: 	for (idx_t col_idx = 0; col_idx < output_chunk.ColumnCount(); col_idx++) {
883: 		result.data[out_idx++].Reference(output_chunk.data[col_idx]);
884: 	}
885: 
886: 	// If we done with this block, move to the next one
887: 	if (!scanner->Remaining()) {
888: 		++task->begin_idx;
889: 	}
890: 
891: 	// If that was the last block, release out local state memory.
892: 	if (TaskFinished()) {
893: 		local_states.clear();
894: 	}
895: 	result.Verify();
896: }
897: 
898: unique_ptr<LocalSourceState> PhysicalWindow::GetLocalSourceState(ExecutionContext &context,
899:                                                                  GlobalSourceState &gsource_p) const {
900: 	auto &gsource = gsource_p.Cast<WindowGlobalSourceState>();
901: 	return make_uniq<WindowLocalSourceState>(gsource);
902: }
903: 
904: unique_ptr<GlobalSourceState> PhysicalWindow::GetGlobalSourceState(ClientContext &context) const {
905: 	auto &gsink = sink_state->Cast<WindowGlobalSinkState>();
906: 	return make_uniq<WindowGlobalSourceState>(context, gsink);
907: }
908: 
909: bool PhysicalWindow::SupportsPartitioning(const OperatorPartitionInfo &partition_info) const {
910: 	if (partition_info.RequiresPartitionColumns()) {
911: 		return false;
912: 	}
913: 	//	We can only preserve order for single partitioning
914: 	//	or work stealing causes out of order batch numbers
915: 	auto &wexpr = select_list[order_idx]->Cast<BoundWindowExpression>();
916: 	return wexpr.partitions.empty(); // NOLINT
917: }
918: 
919: OrderPreservationType PhysicalWindow::SourceOrder() const {
920: 	auto &wexpr = select_list[order_idx]->Cast<BoundWindowExpression>();
921: 	if (!wexpr.partitions.empty()) {
922: 		// if we have partitions the window order is not defined
923: 		return OrderPreservationType::NO_ORDER;
924: 	}
925: 	// without partitions we can maintain order
926: 	if (wexpr.orders.empty()) {
927: 		// if we have no orders we maintain insertion order
928: 		return OrderPreservationType::INSERTION_ORDER;
929: 	}
930: 	// otherwise we can maintain the fixed order
931: 	return OrderPreservationType::FIXED_ORDER;
932: }
933: 
934: ProgressData PhysicalWindow::GetProgress(ClientContext &context, GlobalSourceState &gsource_p) const {
935: 	auto &gsource = gsource_p.Cast<WindowGlobalSourceState>();
936: 	const auto returned = gsource.returned.load();
937: 
938: 	auto &gsink = gsource.gsink;
939: 	const auto count = gsink.global_partition->count.load();
940: 	ProgressData res;
941: 	if (count) {
942: 		res.done = double(returned);
943: 		res.total = double(count);
944: 	} else {
945: 		res.SetInvalid();
946: 	}
947: 	return res;
948: }
949: 
950: OperatorPartitionData PhysicalWindow::GetPartitionData(ExecutionContext &context, DataChunk &chunk,
951:                                                        GlobalSourceState &gstate_p, LocalSourceState &lstate_p,
952:                                                        const OperatorPartitionInfo &partition_info) const {
953: 	if (partition_info.RequiresPartitionColumns()) {
954: 		throw InternalException("PhysicalWindow::GetPartitionData: partition columns not supported");
955: 	}
956: 	auto &lstate = lstate_p.Cast<WindowLocalSourceState>();
957: 	return OperatorPartitionData(lstate.batch_index);
958: }
959: 
960: SourceResultType PhysicalWindow::GetData(ExecutionContext &context, DataChunk &chunk,
961:                                          OperatorSourceInput &input) const {
962: 	auto &gsource = input.global_state.Cast<WindowGlobalSourceState>();
963: 	auto &lsource = input.local_state.Cast<WindowLocalSourceState>();
964: 
965: 	gsource.CreateTaskList();
966: 
967: 	while (gsource.HasUnfinishedTasks() && chunk.size() == 0) {
968: 		if (!lsource.TaskFinished() || lsource.TryAssignTask()) {
969: 			try {
970: 				lsource.ExecuteTask(chunk);
971: 			} catch (...) {
972: 				gsource.stopped = true;
973: 				throw;
974: 			}
975: 		} else {
976: 			auto guard = gsource.Lock();
977: 			if (!gsource.HasMoreTasks()) {
978: 				// no more tasks - exit
979: 				gsource.UnblockTasks(guard);
980: 				break;
981: 			}
982: 			if (gsource.TryPrepareNextStage()) {
983: 				// we successfully prepared the next stage - unblock tasks
984: 				gsource.UnblockTasks(guard);
985: 			} else {
986: 				// there are more tasks available, but we can't execute them yet
987: 				// block the source
988: 				return gsource.BlockSource(guard, input.interrupt_state);
989: 			}
990: 		}
991: 	}
992: 
993: 	gsource.returned += chunk.size();
994: 
995: 	if (chunk.size() == 0) {
996: 		return SourceResultType::FINISHED;
997: 	}
998: 	return SourceResultType::HAVE_MORE_OUTPUT;
999: }
1000: 
1001: InsertionOrderPreservingMap<string> PhysicalWindow::ParamsToString() const {
1002: 	InsertionOrderPreservingMap<string> result;
1003: 	string projections;
1004: 	for (idx_t i = 0; i < select_list.size(); i++) {
1005: 		if (i > 0) {
1006: 			projections += "\n";
1007: 		}
1008: 		projections += select_list[i]->GetName();
1009: 	}
1010: 	result["Projections"] = projections;
1011: 	return result;
1012: }
1013: 
1014: } // namespace duckdb
[end of src/execution/operator/aggregate/physical_window.cpp]
[start of src/execution/operator/csv_scanner/sniffer/dialect_detection.cpp]
1: #include "duckdb/common/shared_ptr.hpp"
2: #include "duckdb/execution/operator/csv_scanner/sniffer/csv_sniffer.hpp"
3: #include "duckdb/main/client_data.hpp"
4: #include "duckdb/execution/operator/csv_scanner/csv_reader_options.hpp"
5: 
6: namespace duckdb {
7: 
8: constexpr idx_t CSVReaderOptions::sniff_size;
9: 
10: bool IsQuoteDefault(char quote) {
11: 	if (quote == '\"' || quote == '\'' || quote == '\0') {
12: 		return true;
13: 	}
14: 	return false;
15: }
16: 
17: vector<string> DialectCandidates::GetDefaultDelimiter() {
18: 	return {",", "|", ";", "\t"};
19: }
20: 
21: vector<vector<char>> DialectCandidates::GetDefaultQuote() {
22: 	return {{'\0'}, {'\"', '\''}, {'\"'}};
23: }
24: 
25: vector<QuoteRule> DialectCandidates::GetDefaultQuoteRule() {
26: 	return {QuoteRule::NO_QUOTES, QuoteRule::QUOTES_OTHER, QuoteRule::QUOTES_RFC};
27: }
28: 
29: vector<vector<char>> DialectCandidates::GetDefaultEscape() {
30: 	return {{'\0'}, {'\\'}, {'\"', '\0', '\''}};
31: }
32: 
33: vector<char> DialectCandidates::GetDefaultComment() {
34: 	return {'#', '\0'};
35: }
36: 
37: string DialectCandidates::Print() {
38: 	std::ostringstream search_space;
39: 
40: 	search_space << "Delimiter Candidates: ";
41: 	for (idx_t i = 0; i < delim_candidates.size(); i++) {
42: 		search_space << "\'" << delim_candidates[i] << "\'";
43: 		if (i < delim_candidates.size() - 1) {
44: 			search_space << ", ";
45: 		}
46: 	}
47: 	search_space << "\n";
48: 	search_space << "Quote/Escape Candidates: ";
49: 	for (uint8_t i = 0; i < static_cast<uint8_t>(quote_rule_candidates.size()); i++) {
50: 		auto quote_candidate = quote_candidates_map[i];
51: 		auto escape_candidate = escape_candidates_map[i];
52: 		for (idx_t j = 0; j < quote_candidate.size(); j++) {
53: 			for (idx_t k = 0; k < escape_candidate.size(); k++) {
54: 				search_space << "[\'";
55: 				if (quote_candidate[j] == '\0') {
56: 					search_space << "(no quote)";
57: 				} else {
58: 					search_space << quote_candidate[j];
59: 				}
60: 				search_space << "\',\'";
61: 				if (escape_candidate[k] == '\0') {
62: 					search_space << "(no escape)";
63: 				} else {
64: 					search_space << escape_candidate[k];
65: 				}
66: 				search_space << "\']";
67: 				if (k < escape_candidate.size() - 1) {
68: 					search_space << ",";
69: 				}
70: 			}
71: 			if (j < quote_candidate.size() - 1) {
72: 				search_space << ",";
73: 			}
74: 		}
75: 		if (i < quote_rule_candidates.size() - 1) {
76: 			search_space << ",";
77: 		}
78: 	}
79: 	search_space << "\n";
80: 
81: 	search_space << "Comment Candidates: ";
82: 	for (idx_t i = 0; i < comment_candidates.size(); i++) {
83: 		search_space << "\'" << comment_candidates[i] << "\'";
84: 		if (i < comment_candidates.size() - 1) {
85: 			search_space << ", ";
86: 		}
87: 	}
88: 	search_space << "\n";
89: 
90: 	return search_space.str();
91: }
92: 
93: DialectCandidates::DialectCandidates(const CSVStateMachineOptions &options) {
94: 	// assert that quotes escapes and rules have equal size
95: 	const auto default_quote = GetDefaultQuote();
96: 	const auto default_escape = GetDefaultEscape();
97: 	const auto default_quote_rule = GetDefaultQuoteRule();
98: 	const auto default_delimiter = GetDefaultDelimiter();
99: 	const auto default_comment = GetDefaultComment();
100: 
101: 	D_ASSERT(default_quote.size() == default_quote_rule.size() && default_quote_rule.size() == default_escape.size());
102: 	// fill the escapes
103: 	for (idx_t i = 0; i < default_quote_rule.size(); i++) {
104: 		escape_candidates_map[static_cast<uint8_t>(default_quote_rule[i])] = default_escape[i];
105: 	}
106: 
107: 	if (options.delimiter.IsSetByUser()) {
108: 		// user provided a delimiter: use that delimiter
109: 		delim_candidates = {options.delimiter.GetValue()};
110: 	} else {
111: 		// no delimiter provided: try standard/common delimiters
112: 		delim_candidates = default_delimiter;
113: 	}
114: 	if (options.comment.IsSetByUser()) {
115: 		// user provided comment character: use that as a comment
116: 		comment_candidates = {options.comment.GetValue()};
117: 	} else {
118: 		// no comment provided: try standard/common comments
119: 		comment_candidates = default_comment;
120: 	}
121: 	if (options.quote.IsSetByUser()) {
122: 		// user provided quote: use that quote rule
123: 		for (auto &quote_rule : default_quote_rule) {
124: 			quote_candidates_map[static_cast<uint8_t>(quote_rule)] = {options.quote.GetValue()};
125: 		}
126: 		// also add it as an escape rule
127: 		if (!IsQuoteDefault(options.quote.GetValue())) {
128: 			escape_candidates_map[static_cast<uint8_t>(QuoteRule::QUOTES_RFC)].emplace_back(options.quote.GetValue());
129: 		}
130: 	} else {
131: 		// no quote rule provided: use standard/common quotes
132: 		for (idx_t i = 0; i < default_quote_rule.size(); i++) {
133: 			quote_candidates_map[static_cast<uint8_t>(default_quote_rule[i])] = {default_quote[i]};
134: 		}
135: 	}
136: 	if (options.escape.IsSetByUser()) {
137: 		// user provided escape: use that escape rule
138: 		if (options.escape == '\0') {
139: 			quote_rule_candidates = {QuoteRule::QUOTES_RFC};
140: 		} else {
141: 			quote_rule_candidates = {QuoteRule::QUOTES_OTHER};
142: 		}
143: 		escape_candidates_map[static_cast<uint8_t>(quote_rule_candidates[0])] = {options.escape.GetValue()};
144: 	} else {
145: 		// no escape provided: try standard/common escapes
146: 		quote_rule_candidates = default_quote_rule;
147: 	}
148: }
149: 
150: void CSVSniffer::GenerateStateMachineSearchSpace(vector<unique_ptr<ColumnCountScanner>> &column_count_scanners,
151:                                                  const DialectCandidates &dialect_candidates) {
152: 	// Generate state machines for all option combinations
153: 	NewLineIdentifier new_line_id;
154: 	if (options.dialect_options.state_machine_options.new_line.IsSetByUser()) {
155: 		new_line_id = options.dialect_options.state_machine_options.new_line.GetValue();
156: 	} else {
157: 		new_line_id = DetectNewLineDelimiter(*buffer_manager);
158: 	}
159: 	CSVIterator first_iterator;
160: 	bool iterator_set = false;
161: 	for (const auto quote_rule : dialect_candidates.quote_rule_candidates) {
162: 		const auto &quote_candidates = dialect_candidates.quote_candidates_map.at(static_cast<uint8_t>(quote_rule));
163: 		for (const auto &quote : quote_candidates) {
164: 			for (const auto &delimiter : dialect_candidates.delim_candidates) {
165: 				const auto &escape_candidates =
166: 				    dialect_candidates.escape_candidates_map.at(static_cast<uint8_t>(quote_rule));
167: 				for (const auto &escape : escape_candidates) {
168: 					for (const auto &comment : dialect_candidates.comment_candidates) {
169: 						D_ASSERT(buffer_manager);
170: 						CSVStateMachineOptions state_machine_options(
171: 						    delimiter, quote, escape, comment, new_line_id,
172: 						    options.dialect_options.state_machine_options.strict_mode.GetValue());
173: 						auto sniffing_state_machine =
174: 						    make_shared_ptr<CSVStateMachine>(options, state_machine_options, state_machine_cache);
175: 						if (options.dialect_options.skip_rows.IsSetByUser()) {
176: 							if (!iterator_set) {
177: 								first_iterator = BaseScanner::SkipCSVRows(buffer_manager, sniffing_state_machine,
178: 								                                          options.dialect_options.skip_rows.GetValue());
179: 								iterator_set = true;
180: 							}
181: 							column_count_scanners.emplace_back(make_uniq<ColumnCountScanner>(
182: 							    buffer_manager, std::move(sniffing_state_machine), detection_error_handler,
183: 							    CSVReaderOptions::sniff_size, first_iterator));
184: 							continue;
185: 						}
186: 						column_count_scanners.emplace_back(
187: 						    make_uniq<ColumnCountScanner>(buffer_manager, std::move(sniffing_state_machine),
188: 						                                  detection_error_handler, CSVReaderOptions::sniff_size));
189: 					}
190: 				}
191: 			}
192: 		}
193: 	}
194: }
195: 
196: // Returns true if a comment is acceptable
197: bool AreCommentsAcceptable(const ColumnCountResult &result, idx_t num_cols, bool comment_set_by_user) {
198: 	if (comment_set_by_user) {
199: 		return true;
200: 	}
201: 	// For a comment to be acceptable, we want 3/5th's the majority of unmatched in the columns
202: 	constexpr double min_majority = 0.6;
203: 	// detected comments, are all lines that started with a comment character.
204: 	double detected_comments = 0;
205: 	// If at least one comment is a full line comment
206: 	bool has_full_line_comment = false;
207: 	// valid comments are all lines where the number of columns does not fit our expected number of columns.
208: 	double valid_comments = 0;
209: 	for (idx_t i = 0; i < result.result_position; i++) {
210: 		if (result.column_counts[i].is_comment || result.column_counts[i].is_mid_comment) {
211: 			detected_comments++;
212: 			if (result.column_counts[i].number_of_columns != num_cols && result.column_counts[i].is_comment) {
213: 				has_full_line_comment = true;
214: 				valid_comments++;
215: 			}
216: 			if (result.column_counts[i].number_of_columns == num_cols && result.column_counts[i].is_mid_comment) {
217: 				valid_comments++;
218: 			}
219: 		}
220: 	}
221: 	// If we do not encounter at least one full line comment, we do not consider this comment option.
222: 	if (valid_comments == 0 || !has_full_line_comment) {
223: 		// this is only valid if our comment character is \0
224: 		if (result.state_machine.state_machine_options.comment.GetValue() == '\0') {
225: 			return true;
226: 		}
227: 		return false;
228: 	}
229: 
230: 	return valid_comments / detected_comments >= min_majority;
231: }
232: 
233: void CSVSniffer::AnalyzeDialectCandidate(unique_ptr<ColumnCountScanner> scanner, idx_t &rows_read,
234:                                          idx_t &best_consistent_rows, idx_t &prev_padding_count,
235:                                          idx_t &min_ignored_rows) {
236: 	// The sniffed_column_counts variable keeps track of the number of columns found for each row
237: 	auto &sniffed_column_counts = scanner->ParseChunk();
238: 	idx_t dirty_notes = 0;
239: 	idx_t dirty_notes_minus_comments = 0;
240: 	if (sniffed_column_counts.error) {
241: 		// This candidate has an error (i.e., over maximum line size or never unquoting quoted values)
242: 		return;
243: 	}
244: 	idx_t consistent_rows = 0;
245: 	idx_t num_cols = sniffed_column_counts.result_position == 0 ? 1 : sniffed_column_counts[0].number_of_columns;
246: 	const bool ignore_errors = options.ignore_errors.GetValue();
247: 	// If we are ignoring errors and not null_padding , we pick the most frequent number of columns as the right one
248: 	const bool use_most_frequent_columns = ignore_errors && !options.null_padding;
249: 	if (use_most_frequent_columns) {
250: 		num_cols = sniffed_column_counts.GetMostFrequentColumnCount();
251: 	}
252: 	idx_t padding_count = 0;
253: 	idx_t comment_rows = 0;
254: 	idx_t ignored_rows = 0;
255: 	const bool allow_padding = options.null_padding;
256: 	bool first_valid = false;
257: 	if (sniffed_column_counts.result_position > rows_read) {
258: 		rows_read = sniffed_column_counts.result_position;
259: 	}
260: 	if (set_columns.IsCandidateUnacceptable(num_cols, options.null_padding, ignore_errors,
261: 	                                        sniffed_column_counts[0].last_value_always_empty)) {
262: 		// Not acceptable
263: 		return;
264: 	}
265: 	idx_t header_idx = 0;
266: 	for (idx_t row = 0; row < sniffed_column_counts.result_position; row++) {
267: 		if (set_columns.IsCandidateUnacceptable(sniffed_column_counts[row].number_of_columns, options.null_padding,
268: 		                                        ignore_errors, sniffed_column_counts[row].last_value_always_empty)) {
269: 			// Not acceptable
270: 			return;
271: 		}
272: 		if (sniffed_column_counts[row].is_comment) {
273: 			comment_rows++;
274: 		} else if (sniffed_column_counts[row].last_value_always_empty &&
275: 		           sniffed_column_counts[row].number_of_columns ==
276: 		               sniffed_column_counts[header_idx].number_of_columns + 1) {
277: 			// we allow for the first row to miss one column IF last_value_always_empty is true
278: 			// This is so we can sniff files that have an extra delimiter on the data part.
279: 			// e.g., C1|C2\n1|2|\n3|4|
280: 			consistent_rows++;
281: 		} else if (num_cols < sniffed_column_counts[row].number_of_columns &&
282: 		           (!options.dialect_options.skip_rows.IsSetByUser() || comment_rows > 0) &&
283: 		           (!set_columns.IsSet() || options.null_padding) && (!first_valid || (!use_most_frequent_columns))) {
284: 			// all rows up to this point will need padding
285: 			if (!first_valid) {
286: 				first_valid = true;
287: 				sniffed_column_counts.state_machine.dialect_options.rows_until_header = row;
288: 			}
289: 			padding_count = 0;
290: 			// we use the maximum amount of num_cols that we find
291: 			num_cols = sniffed_column_counts[row].number_of_columns;
292: 			dirty_notes = row;
293: 			dirty_notes_minus_comments = dirty_notes - comment_rows;
294: 			header_idx = row;
295: 			consistent_rows = 1;
296: 		} else if (sniffed_column_counts[row].number_of_columns == num_cols || (use_most_frequent_columns)) {
297: 			if (!first_valid) {
298: 				first_valid = true;
299: 				sniffed_column_counts.state_machine.dialect_options.rows_until_header = row;
300: 				dirty_notes = row;
301: 			}
302: 			if (sniffed_column_counts[row].number_of_columns != num_cols) {
303: 				ignored_rows++;
304: 			}
305: 			consistent_rows++;
306: 		} else if (num_cols >= sniffed_column_counts[row].number_of_columns) {
307: 			// we are missing some columns, we can parse this as long as we add padding
308: 			padding_count++;
309: 		}
310: 	}
311: 
312: 	if (sniffed_column_counts.state_machine.options.dialect_options.skip_rows.IsSetByUser()) {
313: 		sniffed_column_counts.state_machine.dialect_options.rows_until_header +=
314: 		    sniffed_column_counts.state_machine.options.dialect_options.skip_rows.GetValue();
315: 	}
316: 	// Calculate the total number of consistent rows after adding padding.
317: 	consistent_rows += padding_count;
318: 
319: 	// Whether there are more values (rows) available that are consistent, exceeding the current best.
320: 	const bool more_values = consistent_rows > best_consistent_rows && num_cols >= max_columns_found;
321: 
322: 	const bool more_columns = consistent_rows == best_consistent_rows && num_cols > max_columns_found;
323: 
324: 	// If additional padding is required when compared to the previous padding count.
325: 	const bool require_more_padding = padding_count > prev_padding_count;
326: 
327: 	// If less padding is now required when compared to the previous padding count.
328: 	const bool require_less_padding = padding_count < prev_padding_count;
329: 
330: 	// If there was only a single column before, and the new number of columns exceeds that.
331: 	const bool single_column_before = max_columns_found < 2 && num_cols > max_columns_found * candidates.size();
332: 
333: 	// If the number of rows is consistent with the calculated value after accounting for skipped rows and the
334: 	// start row.
335: 	const bool rows_consistent =
336: 	    consistent_rows + (dirty_notes_minus_comments - options.dialect_options.skip_rows.GetValue()) + comment_rows ==
337: 	    sniffed_column_counts.result_position - options.dialect_options.skip_rows.GetValue();
338: 	// If there are more than one consistent row.
339: 	const bool more_than_one_row = consistent_rows > 1;
340: 
341: 	// If there are more than one column.
342: 	const bool more_than_one_column = num_cols > 1;
343: 
344: 	// If the start position is valid.
345: 	const bool start_good = !candidates.empty() &&
346: 	                        dirty_notes <= candidates.front()->GetStateMachine().dialect_options.skip_rows.GetValue();
347: 
348: 	// If padding happened but it is not allowed.
349: 	const bool invalid_padding = !allow_padding && padding_count > 0;
350: 
351: 	const bool comments_are_acceptable = AreCommentsAcceptable(
352: 	    sniffed_column_counts, num_cols, options.dialect_options.state_machine_options.comment.IsSetByUser());
353: 
354: 	const bool quoted =
355: 	    scanner->ever_quoted &&
356: 	    sniffed_column_counts.state_machine.dialect_options.state_machine_options.quote.GetValue() != '\0';
357: 
358: 	// For our columns to match, we either don't have them manually set, or they match in value with the sniffed value
359: 	const bool columns_match_set =
360: 	    num_cols == set_columns.Size() ||
361: 	    (num_cols == set_columns.Size() + 1 && sniffed_column_counts[0].last_value_always_empty) ||
362: 	    !set_columns.IsSet();
363: 
364: 	// If rows are consistent and no invalid padding happens, this is the best suitable candidate if one of the
365: 	// following is valid:
366: 	// - There's a single column before.
367: 	// - There are more values and no additional padding is required.
368: 	// - There's more than one column and less padding is required.
369: 	if (columns_match_set && (rows_consistent || (set_columns.IsSet() && ignore_errors)) &&
370: 	    (single_column_before || ((more_values || more_columns) && !require_more_padding) ||
371: 	     (more_than_one_column && require_less_padding) || quoted) &&
372: 	    !invalid_padding && comments_are_acceptable) {
373: 		if (!candidates.empty() && set_columns.IsSet() && max_columns_found == set_columns.Size() &&
374: 		    consistent_rows <= best_consistent_rows) {
375: 			// We have a candidate that fits our requirements better
376: 			if (candidates.front()->ever_quoted || !scanner->ever_quoted) {
377: 				return;
378: 			}
379: 		}
380: 		auto &sniffing_state_machine = scanner->GetStateMachine();
381: 
382: 		if (!candidates.empty() && candidates.front()->ever_quoted) {
383: 			// Give preference to quoted boys.
384: 			if (!scanner->ever_quoted) {
385: 				return;
386: 			} else {
387: 				// Give preference to one that got escaped
388: 				if (!scanner->ever_escaped && candidates.front()->ever_escaped) {
389: 					return;
390: 				}
391: 				if (best_consistent_rows == consistent_rows && num_cols >= max_columns_found) {
392: 					// If both have not been escaped, this might get solved later on.
393: 					sniffing_state_machine.dialect_options.num_cols = num_cols;
394: 					candidates.emplace_back(std::move(scanner));
395: 					max_columns_found = num_cols;
396: 					return;
397: 				}
398: 			}
399: 		}
400: 		if (max_columns_found == num_cols && ignored_rows > min_ignored_rows) {
401: 			return;
402: 		}
403: 		if (quoted && num_cols < max_columns_found) {
404: 			for (auto &candidate : candidates) {
405: 				if (candidate->ever_quoted) {
406: 					return;
407: 				}
408: 			}
409: 		}
410: 		best_consistent_rows = consistent_rows;
411: 		max_columns_found = num_cols;
412: 		prev_padding_count = padding_count;
413: 		min_ignored_rows = ignored_rows;
414: 
415: 		if (options.dialect_options.skip_rows.IsSetByUser()) {
416: 			// If skip rows is set by user, and we found dirty notes, we only accept it if either null_padding or
417: 			// ignore_errors is set we have comments
418: 			if (dirty_notes != 0 && !options.null_padding && !options.ignore_errors.GetValue() && comment_rows == 0) {
419: 				return;
420: 			}
421: 			sniffing_state_machine.dialect_options.skip_rows = options.dialect_options.skip_rows.GetValue();
422: 		} else if (!options.null_padding) {
423: 			sniffing_state_machine.dialect_options.skip_rows = dirty_notes;
424: 		}
425: 
426: 		candidates.clear();
427: 		sniffing_state_machine.dialect_options.num_cols = num_cols;
428: 		lines_sniffed = sniffed_column_counts.result_position;
429: 		candidates.emplace_back(std::move(scanner));
430: 		return;
431: 	}
432: 	// If there's more than one row and column, the start is good, rows are consistent,
433: 	// no additional padding is required, and there is no invalid padding, and there is not yet a candidate
434: 	// with the same quote, we add this state_machine as a suitable candidate.
435: 	if (columns_match_set && more_than_one_row && more_than_one_column && start_good && rows_consistent &&
436: 	    !require_more_padding && !invalid_padding && num_cols == max_columns_found && comments_are_acceptable) {
437: 		auto &sniffing_state_machine = scanner->GetStateMachine();
438: 
439: 		bool same_quote_is_candidate = false;
440: 		for (const auto &candidate : candidates) {
441: 			if (sniffing_state_machine.dialect_options.state_machine_options.quote ==
442: 			    candidate->GetStateMachine().dialect_options.state_machine_options.quote) {
443: 				same_quote_is_candidate = true;
444: 			}
445: 		}
446: 		if (!same_quote_is_candidate) {
447: 			if (options.dialect_options.skip_rows.IsSetByUser()) {
448: 				// If skip rows is set by user, and we found dirty notes, we only accept it if either null_padding or
449: 				// ignore_errors is set
450: 				if (dirty_notes != 0 && !options.null_padding && !options.ignore_errors.GetValue()) {
451: 					return;
452: 				}
453: 				sniffing_state_machine.dialect_options.skip_rows = options.dialect_options.skip_rows.GetValue();
454: 			} else if (!options.null_padding) {
455: 				sniffing_state_machine.dialect_options.skip_rows = dirty_notes;
456: 			}
457: 			sniffing_state_machine.dialect_options.num_cols = num_cols;
458: 			lines_sniffed = sniffed_column_counts.result_position;
459: 			candidates.emplace_back(std::move(scanner));
460: 		}
461: 	}
462: }
463: 
464: bool CSVSniffer::RefineCandidateNextChunk(ColumnCountScanner &candidate) const {
465: 	auto &sniffed_column_counts = candidate.ParseChunk();
466: 	for (idx_t i = 0; i < sniffed_column_counts.result_position; i++) {
467: 		if (set_columns.IsSet()) {
468: 			return !set_columns.IsCandidateUnacceptable(sniffed_column_counts[i].number_of_columns,
469: 			                                            options.null_padding, options.ignore_errors.GetValue(),
470: 			                                            sniffed_column_counts[i].last_value_always_empty);
471: 		}
472: 		if (max_columns_found != sniffed_column_counts[i].number_of_columns &&
473: 		    (!options.null_padding && !options.ignore_errors.GetValue() && !sniffed_column_counts[i].is_comment)) {
474: 			return false;
475: 		}
476: 	}
477: 	return true;
478: }
479: 
480: void CSVSniffer::RefineCandidates() {
481: 	// It's very frequent that more than one dialect can parse a csv file, hence here we run one state machine
482: 	// fully on the whole sample dataset, when/if it fails we go to the next one.
483: 	if (candidates.empty()) {
484: 		// No candidates to refine
485: 		return;
486: 	}
487: 	if (candidates.size() == 1 || candidates[0]->FinishedFile()) {
488: 		// Only one candidate nothing to refine or all candidates already checked
489: 		return;
490: 	}
491: 
492: 	for (idx_t i = 1; i <= options.sample_size_chunks; i++) {
493: 		vector<unique_ptr<ColumnCountScanner>> successful_candidates;
494: 		bool done = false;
495: 		for (auto &cur_candidate : candidates) {
496: 			const bool finished_file = cur_candidate->FinishedFile();
497: 			if (successful_candidates.empty()) {
498: 				lines_sniffed += cur_candidate->GetResult().result_position;
499: 			}
500: 			if (finished_file || i == options.sample_size_chunks) {
501: 				// we finished the file or our chunk sample successfully
502: 				if (!cur_candidate->GetResult().error) {
503: 					successful_candidates.push_back(std::move(cur_candidate));
504: 				}
505: 				done = true;
506: 				continue;
507: 			}
508: 			if (RefineCandidateNextChunk(*cur_candidate) && !cur_candidate->GetResult().error) {
509: 				successful_candidates.push_back(std::move(cur_candidate));
510: 			}
511: 		}
512: 		candidates = std::move(successful_candidates);
513: 		if (done) {
514: 			break;
515: 		}
516: 	}
517: 	// If we have multiple candidates with quotes set, we will give the preference to ones
518: 	// that have actually quoted values, otherwise we will choose quotes = \0
519: 	vector<unique_ptr<ColumnCountScanner>> successful_candidates = std::move(candidates);
520: 	if (!successful_candidates.empty()) {
521: 		for (idx_t i = 0; i < successful_candidates.size(); i++) {
522: 			unique_ptr<ColumnCountScanner> cc_best_candidate = std::move(successful_candidates[i]);
523: 			if (cc_best_candidate->state_machine->state_machine_options.quote != '\0' &&
524: 			    cc_best_candidate->ever_quoted) {
525: 				candidates.clear();
526: 				candidates.push_back(std::move(cc_best_candidate));
527: 				return;
528: 			}
529: 			candidates.push_back(std::move(cc_best_candidate));
530: 		}
531: 	}
532: }
533: 
534: NewLineIdentifier CSVSniffer::DetectNewLineDelimiter(CSVBufferManager &buffer_manager) {
535: 	// Get first buffer
536: 	auto buffer = buffer_manager.GetBuffer(0);
537: 	auto buffer_ptr = buffer->Ptr();
538: 	bool carriage_return = false;
539: 	bool n = false;
540: 	for (idx_t i = 0; i < buffer->actual_size; i++) {
541: 		if (buffer_ptr[i] == '\r') {
542: 			carriage_return = true;
543: 		} else if (buffer_ptr[i] == '\n') {
544: 			n = true;
545: 			break;
546: 		} else if (carriage_return) {
547: 			break;
548: 		}
549: 	}
550: 	if (carriage_return && n) {
551: 		return NewLineIdentifier::CARRY_ON;
552: 	}
553: 	if (carriage_return) {
554: 		return NewLineIdentifier::SINGLE_R;
555: 	}
556: 	return NewLineIdentifier::SINGLE_N;
557: }
558: 
559: // Dialect Detection consists of five steps:
560: // 1. Generate a search space of all possible dialects
561: // 2. Generate a state machine for each dialect
562: // 3. Analyze the first chunk of the file and find the best dialect candidates
563: // 4. Analyze the remaining chunks of the file and find the best dialect candidate
564: void CSVSniffer::DetectDialect() {
565: 	// Variables for Dialect Detection
566: 	DialectCandidates dialect_candidates(options.dialect_options.state_machine_options);
567: 	// Number of rows read
568: 	idx_t rows_read = 0;
569: 	// Best Number of consistent rows (i.e., presenting all columns)
570: 	idx_t best_consistent_rows = 0;
571: 	// If padding was necessary (i.e., rows are missing some columns, how many)
572: 	idx_t prev_padding_count = 0;
573: 	// Min number of ignores rows
574: 	idx_t best_ignored_rows = 0;
575: 	// Vector of CSV State Machines
576: 	vector<unique_ptr<ColumnCountScanner>> csv_state_machines;
577: 	// Step 1: Generate state machines
578: 	GenerateStateMachineSearchSpace(csv_state_machines, dialect_candidates);
579: 	// Step 2: Analyze all candidates on the first chunk
580: 	for (auto &state_machine : csv_state_machines) {
581: 		AnalyzeDialectCandidate(std::move(state_machine), rows_read, best_consistent_rows, prev_padding_count,
582: 		                        best_ignored_rows);
583: 	}
584: 	// Step 3: Loop over candidates and find if they can still produce good results for the remaining chunks
585: 	RefineCandidates();
586: 
587: 	// if no dialect candidate was found, we throw an exception
588: 	if (candidates.empty()) {
589: 		auto error = CSVError::SniffingError(options, dialect_candidates.Print());
590: 		error_handler->Error(error, true);
591: 	}
592: }
593: } // namespace duckdb
[end of src/execution/operator/csv_scanner/sniffer/dialect_detection.cpp]
[start of src/execution/operator/helper/physical_reservoir_sample.cpp]
1: #include "duckdb/execution/operator/helper/physical_reservoir_sample.hpp"
2: #include "duckdb/execution/reservoir_sample.hpp"
3: 
4: namespace duckdb {
5: 
6: //===--------------------------------------------------------------------===//
7: // Sink
8: //===--------------------------------------------------------------------===//
9: 
10: class SampleGlobalSinkState : public GlobalSinkState {
11: public:
12: 	explicit SampleGlobalSinkState(Allocator &allocator, SampleOptions &options) {
13: 		if (options.is_percentage) {
14: 			auto percentage = options.sample_size.GetValue<double>();
15: 			if (percentage == 0) {
16: 				return;
17: 			}
18: 			sample = make_uniq<ReservoirSamplePercentage>(allocator, percentage,
19: 			                                              static_cast<int64_t>(options.seed.GetIndex()));
20: 		} else {
21: 			auto size = NumericCast<idx_t>(options.sample_size.GetValue<int64_t>());
22: 			if (size == 0) {
23: 				return;
24: 			}
25: 			sample = make_uniq<ReservoirSample>(allocator, size, static_cast<int64_t>(options.seed.GetIndex()));
26: 		}
27: 	}
28: 
29: 	//! The lock for updating the global aggoregate state
30: 	//! Also used to update the global sample when percentages are used
31: 	mutex lock;
32: 	//! The reservoir sample
33: 	unique_ptr<BlockingSample> sample;
34: };
35: 
36: unique_ptr<GlobalSinkState> PhysicalReservoirSample::GetGlobalSinkState(ClientContext &context) const {
37: 	return make_uniq<SampleGlobalSinkState>(Allocator::Get(context), *options);
38: }
39: 
40: SinkResultType PhysicalReservoirSample::Sink(ExecutionContext &context, DataChunk &chunk,
41:                                              OperatorSinkInput &input) const {
42: 	auto &global_state = input.global_state.Cast<SampleGlobalSinkState>();
43: 	// Percentage only has a global sample.
44: 	lock_guard<mutex> glock(global_state.lock);
45: 	if (!global_state.sample) {
46: 		// always gather full thread percentage
47: 		auto &allocator = Allocator::Get(context.client);
48: 		if (options->is_percentage) {
49: 			double percentage = options->sample_size.GetValue<double>();
50: 			if (percentage == 0) {
51: 				return SinkResultType::FINISHED;
52: 			}
53: 			global_state.sample = make_uniq<ReservoirSamplePercentage>(allocator, percentage,
54: 			                                                           static_cast<int64_t>(options->seed.GetIndex()));
55: 		} else {
56: 			idx_t num_samples = options->sample_size.GetValue<idx_t>();
57: 			if (num_samples == 0) {
58: 				return SinkResultType::FINISHED;
59: 			}
60: 			global_state.sample =
61: 			    make_uniq<ReservoirSample>(allocator, num_samples, static_cast<int64_t>(options->seed.GetIndex()));
62: 		}
63: 	}
64: 	global_state.sample->AddToReservoir(chunk);
65: 	return SinkResultType::NEED_MORE_INPUT;
66: }
67: 
68: SinkCombineResultType PhysicalReservoirSample::Combine(ExecutionContext &context,
69:                                                        OperatorSinkCombineInput &input) const {
70: 	return SinkCombineResultType::FINISHED;
71: }
72: 
73: SinkFinalizeType PhysicalReservoirSample::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
74:                                                    OperatorSinkFinalizeInput &input) const {
75: 	return SinkFinalizeType::READY;
76: }
77: 
78: //===--------------------------------------------------------------------===//
79: // Source
80: //===--------------------------------------------------------------------===//
81: SourceResultType PhysicalReservoirSample::GetData(ExecutionContext &context, DataChunk &chunk,
82:                                                   OperatorSourceInput &input) const {
83: 	auto &sink = this->sink_state->Cast<SampleGlobalSinkState>();
84: 	lock_guard<mutex> glock(sink.lock);
85: 	if (!sink.sample) {
86: 		return SourceResultType::FINISHED;
87: 	}
88: 	auto sample_chunk = sink.sample->GetChunk();
89: 	if (!sample_chunk) {
90: 		return SourceResultType::FINISHED;
91: 	}
92: 	chunk.Move(*sample_chunk);
93: 
94: 	return SourceResultType::HAVE_MORE_OUTPUT;
95: }
96: 
97: InsertionOrderPreservingMap<string> PhysicalReservoirSample::ParamsToString() const {
98: 	InsertionOrderPreservingMap<string> result;
99: 	result["Sample Size"] = options->sample_size.ToString() + (options->is_percentage ? "%" : " rows");
100: 	return result;
101: }
102: 
103: } // namespace duckdb
[end of src/execution/operator/helper/physical_reservoir_sample.cpp]
[start of src/execution/operator/schema/physical_create_art_index.cpp]
1: #include "duckdb/execution/operator/schema/physical_create_art_index.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/duck_index_entry.hpp"
4: #include "duckdb/catalog/catalog_entry/duck_table_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
6: #include "duckdb/execution/index/art/art_key.hpp"
7: #include "duckdb/execution/index/bound_index.hpp"
8: #include "duckdb/main/client_context.hpp"
9: #include "duckdb/main/database_manager.hpp"
10: #include "duckdb/storage/storage_manager.hpp"
11: #include "duckdb/storage/table/append_state.hpp"
12: #include "duckdb/common/exception/transaction_exception.hpp"
13: 
14: namespace duckdb {
15: 
16: PhysicalCreateARTIndex::PhysicalCreateARTIndex(LogicalOperator &op, TableCatalogEntry &table_p,
17:                                                const vector<column_t> &column_ids, unique_ptr<CreateIndexInfo> info,
18:                                                vector<unique_ptr<Expression>> unbound_expressions,
19:                                                idx_t estimated_cardinality, const bool sorted,
20:                                                unique_ptr<AlterTableInfo> alter_table_info)
21:     : PhysicalOperator(PhysicalOperatorType::CREATE_INDEX, op.types, estimated_cardinality),
22:       table(table_p.Cast<DuckTableEntry>()), info(std::move(info)), unbound_expressions(std::move(unbound_expressions)),
23:       sorted(sorted), alter_table_info(std::move(alter_table_info)) {
24: 
25: 	// Convert the logical column ids to physical column ids.
26: 	for (auto &column_id : column_ids) {
27: 		storage_ids.push_back(table.GetColumns().LogicalToPhysical(LogicalIndex(column_id)).index);
28: 	}
29: }
30: 
31: //===--------------------------------------------------------------------===//
32: // Sink
33: //===--------------------------------------------------------------------===//
34: 
35: class CreateARTIndexGlobalSinkState : public GlobalSinkState {
36: public:
37: 	unique_ptr<BoundIndex> global_index;
38: };
39: 
40: class CreateARTIndexLocalSinkState : public LocalSinkState {
41: public:
42: 	explicit CreateARTIndexLocalSinkState(ClientContext &context) : arena_allocator(Allocator::Get(context)) {};
43: 
44: 	unique_ptr<BoundIndex> local_index;
45: 	ArenaAllocator arena_allocator;
46: 
47: 	DataChunk key_chunk;
48: 	unsafe_vector<ARTKey> keys;
49: 	vector<column_t> key_column_ids;
50: 
51: 	DataChunk row_id_chunk;
52: 	unsafe_vector<ARTKey> row_ids;
53: };
54: 
55: unique_ptr<GlobalSinkState> PhysicalCreateARTIndex::GetGlobalSinkState(ClientContext &context) const {
56: 	// Create the global sink state and add the global index.
57: 	auto state = make_uniq<CreateARTIndexGlobalSinkState>();
58: 	auto &storage = table.GetStorage();
59: 	state->global_index = make_uniq<ART>(info->index_name, info->constraint_type, storage_ids,
60: 	                                     TableIOManager::Get(storage), unbound_expressions, storage.db);
61: 	return (std::move(state));
62: }
63: 
64: unique_ptr<LocalSinkState> PhysicalCreateARTIndex::GetLocalSinkState(ExecutionContext &context) const {
65: 	// Create the local sink state and add the local index.
66: 	auto state = make_uniq<CreateARTIndexLocalSinkState>(context.client);
67: 	auto &storage = table.GetStorage();
68: 	state->local_index = make_uniq<ART>(info->index_name, info->constraint_type, storage_ids,
69: 	                                    TableIOManager::Get(storage), unbound_expressions, storage.db);
70: 
71: 	// Initialize the local sink state.
72: 	state->keys.resize(STANDARD_VECTOR_SIZE);
73: 	state->row_ids.resize(STANDARD_VECTOR_SIZE);
74: 	state->key_chunk.Initialize(Allocator::Get(context.client), state->local_index->logical_types);
75: 	state->row_id_chunk.Initialize(Allocator::Get(context.client), vector<LogicalType> {LogicalType::ROW_TYPE});
76: 	for (idx_t i = 0; i < state->key_chunk.ColumnCount(); i++) {
77: 		state->key_column_ids.push_back(i);
78: 	}
79: 	return std::move(state);
80: }
81: 
82: SinkResultType PhysicalCreateARTIndex::SinkUnsorted(OperatorSinkInput &input) const {
83: 
84: 	auto &l_state = input.local_state.Cast<CreateARTIndexLocalSinkState>();
85: 	auto row_count = l_state.key_chunk.size();
86: 	auto &art = l_state.local_index->Cast<ART>();
87: 
88: 	// Insert each key and its corresponding row ID.
89: 	for (idx_t i = 0; i < row_count; i++) {
90: 		auto status = art.tree.GetGateStatus();
91: 		auto conflict_type =
92: 		    art.Insert(art.tree, l_state.keys[i], 0, l_state.row_ids[i], status, nullptr, IndexAppendMode::DEFAULT);
93: 		D_ASSERT(conflict_type != ARTConflictType::TRANSACTION);
94: 		if (conflict_type == ARTConflictType::CONSTRAINT) {
95: 			throw ConstraintException("Data contains duplicates on indexed column(s)");
96: 		}
97: 	}
98: 
99: 	return SinkResultType::NEED_MORE_INPUT;
100: }
101: 
102: SinkResultType PhysicalCreateARTIndex::SinkSorted(OperatorSinkInput &input) const {
103: 
104: 	auto &l_state = input.local_state.Cast<CreateARTIndexLocalSinkState>();
105: 	auto &storage = table.GetStorage();
106: 	auto &l_index = l_state.local_index;
107: 
108: 	// Construct an ART for this chunk.
109: 	auto art = make_uniq<ART>(info->index_name, l_index->GetConstraintType(), l_index->GetColumnIds(),
110: 	                          l_index->table_io_manager, l_index->unbound_expressions, storage.db,
111: 	                          l_index->Cast<ART>().allocators);
112: 	if (!art->Construct(l_state.keys, l_state.row_ids, l_state.key_chunk.size())) {
113: 		throw ConstraintException("Data contains duplicates on indexed column(s)");
114: 	}
115: 
116: 	// Merge the ART into the local ART.
117: 	if (!l_index->MergeIndexes(*art)) {
118: 		throw ConstraintException("Data contains duplicates on indexed column(s)");
119: 	}
120: 
121: 	return SinkResultType::NEED_MORE_INPUT;
122: }
123: 
124: SinkResultType PhysicalCreateARTIndex::Sink(ExecutionContext &context, DataChunk &chunk,
125:                                             OperatorSinkInput &input) const {
126: 
127: 	D_ASSERT(chunk.ColumnCount() >= 2);
128: 	auto &l_state = input.local_state.Cast<CreateARTIndexLocalSinkState>();
129: 	l_state.arena_allocator.Reset();
130: 	l_state.key_chunk.ReferenceColumns(chunk, l_state.key_column_ids);
131: 
132: 	// Check for NULLs, if we are creating a PRIMARY KEY.
133: 	// FIXME: Later, we want to ensure that we skip the NULL check for any non-PK alter.
134: 	if (alter_table_info) {
135: 		auto row_count = l_state.key_chunk.size();
136: 		for (idx_t i = 0; i < l_state.key_chunk.ColumnCount(); i++) {
137: 			if (VectorOperations::HasNull(l_state.key_chunk.data[i], row_count)) {
138: 				throw ConstraintException("NOT NULL constraint failed: %s", info->index_name);
139: 			}
140: 		}
141: 	}
142: 
143: 	ART::GenerateKeyVectors(l_state.arena_allocator, l_state.key_chunk, chunk.data[chunk.ColumnCount() - 1],
144: 	                        l_state.keys, l_state.row_ids);
145: 
146: 	if (sorted) {
147: 		return SinkSorted(input);
148: 	}
149: 	return SinkUnsorted(input);
150: }
151: 
152: SinkCombineResultType PhysicalCreateARTIndex::Combine(ExecutionContext &context,
153:                                                       OperatorSinkCombineInput &input) const {
154: 
155: 	auto &g_state = input.global_state.Cast<CreateARTIndexGlobalSinkState>();
156: 	auto &l_state = input.local_state.Cast<CreateARTIndexLocalSinkState>();
157: 
158: 	// Merge the local index into the global index.
159: 	if (!g_state.global_index->MergeIndexes(*l_state.local_index)) {
160: 		throw ConstraintException("Data contains duplicates on indexed column(s)");
161: 	}
162: 
163: 	return SinkCombineResultType::FINISHED;
164: }
165: 
166: SinkFinalizeType PhysicalCreateARTIndex::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
167:                                                   OperatorSinkFinalizeInput &input) const {
168: 
169: 	// Here, we set the resulting global index as the newly created index of the table.
170: 	auto &state = input.global_state.Cast<CreateARTIndexGlobalSinkState>();
171: 
172: 	// Vacuum excess memory and verify.
173: 	state.global_index->Vacuum();
174: 	D_ASSERT(!state.global_index->VerifyAndToString(true).empty());
175: 	state.global_index->VerifyAllocations();
176: 
177: 	auto &storage = table.GetStorage();
178: 	if (!storage.IsRoot()) {
179: 		throw TransactionException("cannot add an index to a table that has been altered");
180: 	}
181: 
182: 	auto &schema = table.schema;
183: 	info->column_ids = storage_ids;
184: 
185: 	// FIXME: We should check for catalog exceptions prior to index creation, and later double-check.
186: 	if (!alter_table_info) {
187: 		// Ensure that the index does not yet exist in the catalog.
188: 		auto entry = schema.GetEntry(schema.GetCatalogTransaction(context), CatalogType::INDEX_ENTRY, info->index_name);
189: 		if (entry) {
190: 			if (info->on_conflict != OnCreateConflict::IGNORE_ON_CONFLICT) {
191: 				throw CatalogException("Index with name \"%s\" already exists!", info->index_name);
192: 			}
193: 			// IF NOT EXISTS on existing index. We are done.
194: 			return SinkFinalizeType::READY;
195: 		}
196: 
197: 		auto index_entry = schema.CreateIndex(schema.GetCatalogTransaction(context), *info, table).get();
198: 		D_ASSERT(index_entry);
199: 		auto &index = index_entry->Cast<DuckIndexEntry>();
200: 		index.initial_index_size = state.global_index->GetInMemorySize();
201: 
202: 	} else {
203: 		// Ensure that there are no other indexes with that name on this table.
204: 		auto &indexes = storage.GetDataTableInfo()->GetIndexes();
205: 		indexes.Scan([&](Index &index) {
206: 			if (index.GetIndexName() == info->index_name) {
207: 				throw CatalogException("an index with that name already exists for this table: %s", info->index_name);
208: 			}
209: 			return false;
210: 		});
211: 
212: 		auto &catalog = Catalog::GetCatalog(context, info->catalog);
213: 		catalog.Alter(context, *alter_table_info);
214: 	}
215: 
216: 	// Add the index to the storage.
217: 	storage.AddIndex(std::move(state.global_index));
218: 	return SinkFinalizeType::READY;
219: }
220: 
221: //===--------------------------------------------------------------------===//
222: // Source
223: //===--------------------------------------------------------------------===//
224: 
225: SourceResultType PhysicalCreateARTIndex::GetData(ExecutionContext &context, DataChunk &chunk,
226:                                                  OperatorSourceInput &input) const {
227: 	return SourceResultType::FINISHED;
228: }
229: 
230: } // namespace duckdb
[end of src/execution/operator/schema/physical_create_art_index.cpp]
[start of src/execution/physical_plan/plan_create_index.cpp]
1: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
2: #include "duckdb/execution/physical_plan_generator.hpp"
3: #include "duckdb/main/client_context.hpp"
4: #include "duckdb/main/database.hpp"
5: #include "duckdb/planner/expression/bound_operator_expression.hpp"
6: #include "duckdb/planner/expression/bound_reference_expression.hpp"
7: #include "duckdb/planner/operator/logical_create_index.hpp"
8: #include "duckdb/planner/operator/logical_get.hpp"
9: 
10: namespace duckdb {
11: 
12: unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalCreateIndex &op) {
13: 	// Ensure that all expressions contain valid scalar functions.
14: 	// E.g., get_current_timestamp(), random(), and sequence values cannot be index keys.
15: 	for (idx_t i = 0; i < op.unbound_expressions.size(); i++) {
16: 		auto &expr = op.unbound_expressions[i];
17: 		if (!expr->IsConsistent()) {
18: 			throw BinderException("Index keys cannot contain expressions with side effects.");
19: 		}
20: 	}
21: 
22: 	// If we get here and the index type is not valid index type, we throw an exception.
23: 	const auto index_type = context.db->config.GetIndexTypes().FindByName(op.info->index_type);
24: 	if (!index_type) {
25: 		throw BinderException("Unknown index type: " + op.info->index_type);
26: 	}
27: 	if (!index_type->create_plan) {
28: 		throw InternalException("Index type '%s' is missing a create_plan function", op.info->index_type);
29: 	}
30: 
31: 	// Add a dependency for the entire table on which we create the index.
32: 	dependencies.AddDependency(op.table);
33: 	D_ASSERT(op.info->scan_types.size() - 1 <= op.info->names.size());
34: 	D_ASSERT(op.info->scan_types.size() - 1 <= op.info->column_ids.size());
35: 
36: 	// Generate a physical plan for the parallel index creation.
37: 	// TABLE SCAN - PROJECTION - (optional) NOT NULL FILTER - (optional) ORDER BY - CREATE INDEX
38: 	D_ASSERT(op.children.size() == 1);
39: 	auto table_scan = CreatePlan(*op.children[0]);
40: 
41: 	PlanIndexInput input(context, op, table_scan);
42: 	return index_type->create_plan(input);
43: }
44: 
45: } // namespace duckdb
[end of src/execution/physical_plan/plan_create_index.cpp]
[start of src/execution/sample/reservoir_sample.cpp]
1: #include "duckdb/execution/reservoir_sample.hpp"
2: #include "duckdb/common/types/data_chunk.hpp"
3: #include "duckdb/common/vector_operations/vector_operations.hpp"
4: #include <unordered_set>
5: 
6: namespace duckdb {
7: 
8: std::pair<double, idx_t> BlockingSample::PopFromWeightQueue() {
9: 	D_ASSERT(base_reservoir_sample && !base_reservoir_sample->reservoir_weights.empty());
10: 	auto ret = base_reservoir_sample->reservoir_weights.top();
11: 	base_reservoir_sample->reservoir_weights.pop();
12: 
13: 	base_reservoir_sample->UpdateMinWeightThreshold();
14: 	D_ASSERT(base_reservoir_sample->min_weight_threshold > 0);
15: 	return ret;
16: }
17: 
18: double BlockingSample::GetMinWeightThreshold() {
19: 	return base_reservoir_sample->min_weight_threshold;
20: }
21: 
22: idx_t BlockingSample::GetPriorityQueueSize() {
23: 	return base_reservoir_sample->reservoir_weights.size();
24: }
25: 
26: void BlockingSample::Destroy() {
27: 	destroyed = true;
28: }
29: 
30: void ReservoirChunk::Serialize(Serializer &serializer) const {
31: 	chunk.Serialize(serializer);
32: }
33: 
34: unique_ptr<ReservoirChunk> ReservoirChunk::Deserialize(Deserializer &deserializer) {
35: 	auto result = make_uniq<ReservoirChunk>();
36: 	result->chunk.Deserialize(deserializer);
37: 	return result;
38: }
39: 
40: unique_ptr<ReservoirChunk> ReservoirChunk::Copy() const {
41: 	auto copy = make_uniq<ReservoirChunk>();
42: 	copy->chunk.Initialize(Allocator::DefaultAllocator(), chunk.GetTypes());
43: 
44: 	chunk.Copy(copy->chunk);
45: 	return copy;
46: }
47: 
48: ReservoirSample::ReservoirSample(idx_t sample_count, unique_ptr<ReservoirChunk> reservoir_chunk)
49:     : ReservoirSample(Allocator::DefaultAllocator(), sample_count, 1) {
50: 	if (reservoir_chunk) {
51: 		this->reservoir_chunk = std::move(reservoir_chunk);
52: 		sel_size = this->reservoir_chunk->chunk.size();
53: 		sel = SelectionVector(FIXED_SAMPLE_SIZE);
54: 		for (idx_t i = 0; i < sel_size; i++) {
55: 			sel.set_index(i, i);
56: 		}
57: 		ExpandSerializedSample();
58: 	}
59: 	stats_sample = true;
60: }
61: 
62: ReservoirSample::ReservoirSample(Allocator &allocator, idx_t sample_count, int64_t seed)
63:     : BlockingSample(seed), sample_count(sample_count), allocator(allocator) {
64: 	base_reservoir_sample = make_uniq<BaseReservoirSampling>(seed);
65: 	type = SampleType::RESERVOIR_SAMPLE;
66: 	reservoir_chunk = nullptr;
67: 	stats_sample = false;
68: 	sel = SelectionVector(sample_count);
69: 	sel_size = 0;
70: }
71: 
72: idx_t ReservoirSample::GetSampleCount() {
73: 	return sample_count;
74: }
75: 
76: idx_t ReservoirSample::NumSamplesCollected() const {
77: 	if (!reservoir_chunk) {
78: 		return 0;
79: 	}
80: 	return reservoir_chunk->chunk.size();
81: }
82: 
83: SamplingState ReservoirSample::GetSamplingState() const {
84: 	if (base_reservoir_sample->reservoir_weights.empty()) {
85: 		return SamplingState::RANDOM;
86: 	}
87: 	return SamplingState::RESERVOIR;
88: }
89: 
90: idx_t ReservoirSample::GetActiveSampleCount() const {
91: 	switch (GetSamplingState()) {
92: 	case SamplingState::RANDOM:
93: 		return sel_size;
94: 	case SamplingState::RESERVOIR:
95: 		return base_reservoir_sample->reservoir_weights.size();
96: 	default:
97: 		throw InternalException("Sampling State is INVALID");
98: 	}
99: }
100: 
101: idx_t ReservoirSample::GetTuplesSeen() const {
102: 	return base_reservoir_sample->num_entries_seen_total;
103: }
104: 
105: DataChunk &ReservoirSample::Chunk() {
106: 	D_ASSERT(reservoir_chunk);
107: 	return reservoir_chunk->chunk;
108: }
109: 
110: unique_ptr<DataChunk> ReservoirSample::GetChunk() {
111: 	if (destroyed || !reservoir_chunk || Chunk().size() == 0) {
112: 		return nullptr;
113: 	}
114: 	// cannot destory internal samples.
115: 	auto ret = make_uniq<DataChunk>();
116: 
117: 	SelectionVector ret_sel(STANDARD_VECTOR_SIZE);
118: 	idx_t collected_samples = GetActiveSampleCount();
119: 
120: 	if (collected_samples == 0) {
121: 		return nullptr;
122: 	}
123: 
124: 	idx_t samples_remaining;
125: 	idx_t return_chunk_size;
126: 	if (collected_samples > STANDARD_VECTOR_SIZE) {
127: 		samples_remaining = collected_samples - STANDARD_VECTOR_SIZE;
128: 		return_chunk_size = STANDARD_VECTOR_SIZE;
129: 	} else {
130: 		samples_remaining = 0;
131: 		return_chunk_size = collected_samples;
132: 	}
133: 
134: 	for (idx_t i = samples_remaining; i < collected_samples; i++) {
135: 		// pop samples and reduce size of selection vector.
136: 		if (GetSamplingState() == SamplingState::RESERVOIR) {
137: 			auto top = PopFromWeightQueue();
138: 			ret_sel.set_index(i - samples_remaining, sel.get_index(top.second));
139: 		} else {
140: 			ret_sel.set_index(i - samples_remaining, sel.get_index(i));
141: 		}
142: 		sel_size -= 1;
143: 	}
144: 
145: 	auto reservoir_types = Chunk().GetTypes();
146: 
147: 	ret->Initialize(allocator, reservoir_types, STANDARD_VECTOR_SIZE);
148: 	ret->Slice(Chunk(), ret_sel, return_chunk_size);
149: 	ret->SetCardinality(return_chunk_size);
150: 	return ret;
151: }
152: 
153: unique_ptr<ReservoirChunk> ReservoirSample::CreateNewSampleChunk(vector<LogicalType> &types, idx_t size) const {
154: 	auto new_sample_chunk = make_uniq<ReservoirChunk>();
155: 	new_sample_chunk->chunk.Initialize(Allocator::DefaultAllocator(), types, size);
156: 
157: 	// set the NULL columns correctly
158: 	for (idx_t col_idx = 0; col_idx < types.size(); col_idx++) {
159: 		if (!ValidSampleType(types[col_idx]) && stats_sample) {
160: 			new_sample_chunk->chunk.data[col_idx].SetVectorType(VectorType::CONSTANT_VECTOR);
161: 			ConstantVector::SetNull(new_sample_chunk->chunk.data[col_idx], true);
162: 		}
163: 	}
164: 	return new_sample_chunk;
165: }
166: 
167: void ReservoirSample::Vacuum() {
168: 	Verify();
169: 	if (NumSamplesCollected() <= FIXED_SAMPLE_SIZE || !reservoir_chunk || destroyed) {
170: 		// sample is destroyed or too small to shrink
171: 		return;
172: 	}
173: 
174: 	auto ret = Copy();
175: 	auto ret_reservoir = duckdb::unique_ptr_cast<BlockingSample, ReservoirSample>(std::move(ret));
176: 	reservoir_chunk = std::move(ret_reservoir->reservoir_chunk);
177: 	sel = std::move(ret_reservoir->sel);
178: 	sel_size = ret_reservoir->sel_size;
179: 
180: 	Verify();
181: 	// We should only have one sample chunk now.
182: 	D_ASSERT(Chunk().size() > 0 && Chunk().size() <= sample_count);
183: }
184: 
185: unique_ptr<BlockingSample> ReservoirSample::Copy() const {
186: 
187: 	auto ret = make_uniq<ReservoirSample>(sample_count);
188: 	ret->stats_sample = stats_sample;
189: 
190: 	ret->base_reservoir_sample = base_reservoir_sample->Copy();
191: 	ret->destroyed = destroyed;
192: 
193: 	if (!reservoir_chunk || destroyed) {
194: 		return unique_ptr_cast<ReservoirSample, BlockingSample>(std::move(ret));
195: 	}
196: 
197: 	D_ASSERT(reservoir_chunk);
198: 
199: 	// create a new sample chunk to store new samples
200: 	auto types = reservoir_chunk->chunk.GetTypes();
201: 	// how many values should be copied
202: 	idx_t values_to_copy = MinValue<idx_t>(GetActiveSampleCount(), sample_count);
203: 
204: 	auto new_sample_chunk = CreateNewSampleChunk(types, GetReservoirChunkCapacity());
205: 
206: 	SelectionVector sel_copy(sel);
207: 
208: 	ret->reservoir_chunk = std::move(new_sample_chunk);
209: 	ret->UpdateSampleAppend(ret->reservoir_chunk->chunk, reservoir_chunk->chunk, sel_copy, values_to_copy);
210: 	ret->sel = SelectionVector(values_to_copy);
211: 	for (idx_t i = 0; i < values_to_copy; i++) {
212: 		ret->sel.set_index(i, i);
213: 	}
214: 	ret->sel_size = sel_size;
215: 	D_ASSERT(ret->reservoir_chunk->chunk.size() <= sample_count);
216: 	ret->Verify();
217: 	return unique_ptr_cast<ReservoirSample, BlockingSample>(std::move(ret));
218: }
219: 
220: void ReservoirSample::ConvertToReservoirSample() {
221: 	D_ASSERT(sel_size <= sample_count);
222: 	base_reservoir_sample->FillWeights(sel, sel_size);
223: }
224: 
225: vector<uint32_t> ReservoirSample::GetRandomizedVector(uint32_t range, uint32_t size) const {
226: 	vector<uint32_t> ret;
227: 	ret.reserve(range);
228: 	for (uint32_t i = 0; i < range; i++) {
229: 		ret.push_back(i);
230: 	}
231: 	for (uint32_t i = 0; i < size; i++) {
232: 		uint32_t random_shuffle = base_reservoir_sample->random.NextRandomInteger32(i, range);
233: 		if (random_shuffle == i) {
234: 			// leave the value where it is
235: 			continue;
236: 		}
237: 		uint32_t tmp = ret[random_shuffle];
238: 		// basically replacing the tuple that was at index actual_sample_indexes[random_shuffle]
239: 		ret[random_shuffle] = ret[i];
240: 		ret[i] = tmp;
241: 	}
242: 	return ret;
243: }
244: 
245: void ReservoirSample::SimpleMerge(ReservoirSample &other) {
246: 	D_ASSERT(GetPriorityQueueSize() == 0);
247: 	D_ASSERT(other.GetPriorityQueueSize() == 0);
248: 	D_ASSERT(GetSamplingState() == SamplingState::RANDOM);
249: 	D_ASSERT(other.GetSamplingState() == SamplingState::RANDOM);
250: 
251: 	if (other.GetActiveSampleCount() == 0 && other.GetTuplesSeen() == 0) {
252: 		return;
253: 	}
254: 
255: 	if (GetActiveSampleCount() == 0 && GetTuplesSeen() == 0) {
256: 		sel = SelectionVector(other.sel);
257: 		sel_size = other.sel_size;
258: 		base_reservoir_sample->num_entries_seen_total = other.GetTuplesSeen();
259: 		return;
260: 	}
261: 
262: 	idx_t total_seen = GetTuplesSeen() + other.GetTuplesSeen();
263: 
264: 	auto weight_tuples_this = static_cast<double>(GetTuplesSeen()) / static_cast<double>(total_seen);
265: 	auto weight_tuples_other = static_cast<double>(other.GetTuplesSeen()) / static_cast<double>(total_seen);
266: 
267: 	// If weights don't add up to 1, most likely a simple merge occured and no new samples were added.
268: 	// if that is the case, add the missing weight to the lower weighted sample to adjust.
269: 	// this is to avoid cases where if you have a 20k row table and add another 20k rows row by row
270: 	// then eventually the missing weights will add up, and get you a more even distribution
271: 	if (weight_tuples_this + weight_tuples_other < 1) {
272: 		weight_tuples_other += 1 - (weight_tuples_other + weight_tuples_this);
273: 	}
274: 
275: 	idx_t keep_from_this = 0;
276: 	idx_t keep_from_other = 0;
277: 	D_ASSERT(stats_sample);
278: 	D_ASSERT(sample_count == FIXED_SAMPLE_SIZE);
279: 	D_ASSERT(sample_count == other.sample_count);
280: 	auto sample_count_double = static_cast<double>(sample_count);
281: 
282: 	if (weight_tuples_this > weight_tuples_other) {
283: 		keep_from_this = MinValue<idx_t>(static_cast<idx_t>(round(sample_count_double * weight_tuples_this)),
284: 		                                 GetActiveSampleCount());
285: 		keep_from_other = MinValue<idx_t>(sample_count - keep_from_this, other.GetActiveSampleCount());
286: 	} else {
287: 		keep_from_other = MinValue<idx_t>(static_cast<idx_t>(round(sample_count_double * weight_tuples_other)),
288: 		                                  other.GetActiveSampleCount());
289: 		keep_from_this = MinValue<idx_t>(sample_count - keep_from_other, GetActiveSampleCount());
290: 	}
291: 
292: 	D_ASSERT(keep_from_this <= GetActiveSampleCount());
293: 	D_ASSERT(keep_from_other <= other.GetActiveSampleCount());
294: 	D_ASSERT(keep_from_other + keep_from_this <= FIXED_SAMPLE_SIZE);
295: 	idx_t size_after_merge = MinValue<idx_t>(keep_from_other + keep_from_this, FIXED_SAMPLE_SIZE);
296: 
297: 	// Check if appending the other samples to this will go over the sample chunk size
298: 	if (reservoir_chunk->chunk.size() + keep_from_other > GetReservoirChunkCapacity()) {
299: 		Vacuum();
300: 	}
301: 
302: 	D_ASSERT(size_after_merge <= other.GetActiveSampleCount() + GetActiveSampleCount());
303: 	SelectionVector chunk_sel(keep_from_other);
304: 	auto offset = reservoir_chunk->chunk.size();
305: 	for (idx_t i = keep_from_this; i < size_after_merge; i++) {
306: 		if (i >= GetActiveSampleCount()) {
307: 			D_ASSERT(sel_size >= GetActiveSampleCount());
308: 			sel.set_index(GetActiveSampleCount(), offset);
309: 			sel_size += 1;
310: 		} else {
311: 			sel.set_index(i, offset);
312: 		}
313: 		chunk_sel.set_index(i - keep_from_this, other.sel.get_index(i - keep_from_this));
314: 		offset += 1;
315: 	}
316: 
317: 	D_ASSERT(GetActiveSampleCount() == size_after_merge);
318: 
319: 	// Copy the rows that make it to the sample from other and put them into this.
320: 	UpdateSampleAppend(reservoir_chunk->chunk, other.reservoir_chunk->chunk, chunk_sel, keep_from_other);
321: 	base_reservoir_sample->num_entries_seen_total += other.GetTuplesSeen();
322: 
323: 	// if THIS has too many samples now, we conver it to a slower sample.
324: 	if (GetTuplesSeen() >= FIXED_SAMPLE_SIZE * FAST_TO_SLOW_THRESHOLD) {
325: 		ConvertToReservoirSample();
326: 	}
327: 	Verify();
328: }
329: 
330: void ReservoirSample::WeightedMerge(ReservoirSample &other_sample) {
331: 	D_ASSERT(GetSamplingState() == SamplingState::RESERVOIR);
332: 	D_ASSERT(other_sample.GetSamplingState() == SamplingState::RESERVOIR);
333: 
334: 	// Find out how many samples we want to keep.
335: 	idx_t total_samples = GetActiveSampleCount() + other_sample.GetActiveSampleCount();
336: 	idx_t total_samples_seen =
337: 	    base_reservoir_sample->num_entries_seen_total + other_sample.base_reservoir_sample->num_entries_seen_total;
338: 	idx_t num_samples_to_keep = MinValue<idx_t>(total_samples, MinValue<idx_t>(sample_count, total_samples_seen));
339: 
340: 	D_ASSERT(GetActiveSampleCount() <= num_samples_to_keep);
341: 	D_ASSERT(total_samples <= FIXED_SAMPLE_SIZE * 2);
342: 
343: 	// pop from base base_reservoir weights until there are num_samples_to_keep left.
344: 	vector<idx_t> this_indexes_to_replace;
345: 	for (idx_t i = num_samples_to_keep; i < total_samples; i++) {
346: 		auto min_weight_this = base_reservoir_sample->min_weight_threshold;
347: 		auto min_weight_other = other_sample.base_reservoir_sample->min_weight_threshold;
348: 		// min weight threshol is always positive
349: 		if (min_weight_this > min_weight_other) {
350: 			// pop from other
351: 			other_sample.base_reservoir_sample->reservoir_weights.pop();
352: 			other_sample.base_reservoir_sample->UpdateMinWeightThreshold();
353: 		} else {
354: 			auto top_this = PopFromWeightQueue();
355: 			this_indexes_to_replace.push_back(top_this.second);
356: 			base_reservoir_sample->UpdateMinWeightThreshold();
357: 		}
358: 	}
359: 
360: 	D_ASSERT(other_sample.GetPriorityQueueSize() + GetPriorityQueueSize() <= FIXED_SAMPLE_SIZE);
361: 	D_ASSERT(other_sample.GetPriorityQueueSize() + GetPriorityQueueSize() == num_samples_to_keep);
362: 	D_ASSERT(other_sample.reservoir_chunk->chunk.GetTypes() == reservoir_chunk->chunk.GetTypes());
363: 
364: 	// Prepare a selection vector to copy data from the other sample chunk to this sample chunk
365: 	SelectionVector sel_other(other_sample.GetPriorityQueueSize());
366: 	D_ASSERT(GetPriorityQueueSize() <= num_samples_to_keep);
367: 	D_ASSERT(other_sample.GetPriorityQueueSize() >= this_indexes_to_replace.size());
368: 	idx_t chunk_offset = 0;
369: 
370: 	// Now push weights from other.base_reservoir_sample to this
371: 	// Depending on how many sample values "this" has, we either need to add to the selection vector
372: 	// Or replace values in "this'" selection vector
373: 	idx_t i = 0;
374: 	while (other_sample.GetPriorityQueueSize() > 0) {
375: 		auto other_top = other_sample.PopFromWeightQueue();
376: 		idx_t index_for_new_pair = chunk_offset + reservoir_chunk->chunk.size();
377: 
378: 		// update the sel used to copy values from other to this
379: 		sel_other.set_index(chunk_offset, other_top.second);
380: 		if (i < this_indexes_to_replace.size()) {
381: 			auto replacement_index = this_indexes_to_replace[i];
382: 			sel.set_index(replacement_index, index_for_new_pair);
383: 			other_top.second = replacement_index;
384: 		} else {
385: 			sel.set_index(sel_size, index_for_new_pair);
386: 			other_top.second = sel_size;
387: 			sel_size += 1;
388: 		}
389: 
390: 		// make sure that the sample indexes are (this.sample_chunk.size() + chunk_offfset)
391: 		base_reservoir_sample->reservoir_weights.push(other_top);
392: 		chunk_offset += 1;
393: 		i += 1;
394: 	}
395: 
396: 	D_ASSERT(GetPriorityQueueSize() == num_samples_to_keep);
397: 
398: 	base_reservoir_sample->UpdateMinWeightThreshold();
399: 	D_ASSERT(base_reservoir_sample->min_weight_threshold > 0);
400: 	base_reservoir_sample->num_entries_seen_total = GetTuplesSeen() + other_sample.GetTuplesSeen();
401: 
402: 	UpdateSampleAppend(reservoir_chunk->chunk, other_sample.reservoir_chunk->chunk, sel_other, chunk_offset);
403: 	if (reservoir_chunk->chunk.size() > FIXED_SAMPLE_SIZE * (FIXED_SAMPLE_SIZE_MULTIPLIER - 3)) {
404: 		Vacuum();
405: 	}
406: 
407: 	Verify();
408: }
409: 
410: void ReservoirSample::Merge(unique_ptr<BlockingSample> other) {
411: 	if (destroyed || other->destroyed) {
412: 		Destroy();
413: 		return;
414: 	}
415: 
416: 	D_ASSERT(other->type == SampleType::RESERVOIR_SAMPLE);
417: 	auto &other_sample = other->Cast<ReservoirSample>();
418: 
419: 	// if the other sample has not collected anything yet return
420: 	if (!other_sample.reservoir_chunk || other_sample.reservoir_chunk->chunk.size() == 0) {
421: 		return;
422: 	}
423: 
424: 	// this has not collected samples, take over the other
425: 	if (!reservoir_chunk || reservoir_chunk->chunk.size() == 0) {
426: 		base_reservoir_sample = std::move(other->base_reservoir_sample);
427: 		reservoir_chunk = std::move(other_sample.reservoir_chunk);
428: 		sel = SelectionVector(other_sample.sel);
429: 		sel_size = other_sample.sel_size;
430: 		Verify();
431: 		return;
432: 	}
433: 	//! Both samples are still in "fast sampling" method
434: 	if (GetSamplingState() == SamplingState::RANDOM && other_sample.GetSamplingState() == SamplingState::RANDOM) {
435: 		SimpleMerge(other_sample);
436: 		return;
437: 	}
438: 
439: 	// One or none of the samples are in "Fast Sampling" method.
440: 	// When this is the case, switch both to slow sampling
441: 	ConvertToReservoirSample();
442: 	other_sample.ConvertToReservoirSample();
443: 	WeightedMerge(other_sample);
444: }
445: 
446: void ReservoirSample::ShuffleSel(SelectionVector &sel, idx_t range, idx_t size) const {
447: 	auto randomized = GetRandomizedVector(static_cast<uint32_t>(range), static_cast<uint32_t>(size));
448: 	SelectionVector original_sel(range);
449: 	for (idx_t i = 0; i < range; i++) {
450: 		original_sel.set_index(i, sel.get_index(i));
451: 	}
452: 	for (idx_t i = 0; i < size; i++) {
453: 		sel.set_index(i, original_sel.get_index(randomized[i]));
454: 	}
455: }
456: 
457: void ReservoirSample::NormalizeWeights() {
458: 	vector<std::pair<double, idx_t>> tmp_weights;
459: 	while (!base_reservoir_sample->reservoir_weights.empty()) {
460: 		auto top = base_reservoir_sample->reservoir_weights.top();
461: 		tmp_weights.push_back(std::move(top));
462: 		base_reservoir_sample->reservoir_weights.pop();
463: 	}
464: 	std::sort(tmp_weights.begin(), tmp_weights.end(),
465: 	          [&](std::pair<double, idx_t> a, std::pair<double, idx_t> b) { return a.second < b.second; });
466: 	for (idx_t i = 0; i < tmp_weights.size(); i++) {
467: 		base_reservoir_sample->reservoir_weights.emplace(tmp_weights.at(i).first, i);
468: 	}
469: 	base_reservoir_sample->SetNextEntry();
470: }
471: 
472: void ReservoirSample::EvictOverBudgetSamples() {
473: 	Verify();
474: 	if (!reservoir_chunk || destroyed) {
475: 		return;
476: 	}
477: 
478: 	// since this is for serialization, we really need to make sure keep a
479: 	// minimum of 1% of the rows or 2048 rows
480: 	idx_t num_samples_to_keep =
481: 	    MinValue<idx_t>(FIXED_SAMPLE_SIZE, static_cast<idx_t>(SAVE_PERCENTAGE * static_cast<double>(GetTuplesSeen())));
482: 
483: 	if (num_samples_to_keep <= 0) {
484: 		reservoir_chunk->chunk.SetCardinality(0);
485: 		return;
486: 	}
487: 
488: 	if (num_samples_to_keep == sample_count) {
489: 		return;
490: 	}
491: 
492: 	// if we over sampled, make sure we only keep the highest percentage samples
493: 	std::unordered_set<idx_t> selections_to_delete;
494: 
495: 	while (num_samples_to_keep < GetPriorityQueueSize()) {
496: 		auto top = PopFromWeightQueue();
497: 		D_ASSERT(top.second < sel_size);
498: 		selections_to_delete.emplace(top.second);
499: 	}
500: 
501: 	// set up reservoir chunk for the reservoir sample
502: 	D_ASSERT(reservoir_chunk->chunk.size() <= sample_count);
503: 	// create a new sample chunk to store new samples
504: 	auto types = reservoir_chunk->chunk.GetTypes();
505: 	D_ASSERT(num_samples_to_keep <= sample_count);
506: 	D_ASSERT(stats_sample);
507: 	D_ASSERT(sample_count == FIXED_SAMPLE_SIZE);
508: 	auto new_reservoir_chunk = CreateNewSampleChunk(types, sample_count);
509: 
510: 	// The current selection vector can potentially have 2048 valid mappings.
511: 	// If we need to save a sample with less rows than that, we need to do the following
512: 	// 1. Create a new selection vector that doesn't point to the rows we are evicting
513: 	SelectionVector new_sel(num_samples_to_keep);
514: 	idx_t offset = 0;
515: 	for (idx_t i = 0; i < num_samples_to_keep + selections_to_delete.size(); i++) {
516: 		if (selections_to_delete.find(i) == selections_to_delete.end()) {
517: 			D_ASSERT(i - offset < num_samples_to_keep);
518: 			new_sel.set_index(i - offset, sel.get_index(i));
519: 		} else {
520: 			offset += 1;
521: 		}
522: 	}
523: 	// 2. Update row_ids in our weights so that they don't store rows ids to
524: 	//    indexes in the selection vector that have been evicted.
525: 	if (!selections_to_delete.empty()) {
526: 		NormalizeWeights();
527: 	}
528: 
529: 	D_ASSERT(reservoir_chunk->chunk.GetTypes() == new_reservoir_chunk->chunk.GetTypes());
530: 
531: 	UpdateSampleAppend(new_reservoir_chunk->chunk, reservoir_chunk->chunk, new_sel, num_samples_to_keep);
532: 	// set the cardinality
533: 	new_reservoir_chunk->chunk.SetCardinality(num_samples_to_keep);
534: 	reservoir_chunk = std::move(new_reservoir_chunk);
535: 	sel_size = num_samples_to_keep;
536: 	base_reservoir_sample->UpdateMinWeightThreshold();
537: }
538: 
539: void ReservoirSample::ExpandSerializedSample() {
540: 	if (!reservoir_chunk) {
541: 		return;
542: 	}
543: 
544: 	auto types = reservoir_chunk->chunk.GetTypes();
545: 	auto new_res_chunk = CreateNewSampleChunk(types, GetReservoirChunkCapacity());
546: 	auto copy_count = reservoir_chunk->chunk.size();
547: 	SelectionVector tmp_sel = SelectionVector(0, copy_count);
548: 	UpdateSampleAppend(new_res_chunk->chunk, reservoir_chunk->chunk, tmp_sel, copy_count);
549: 	new_res_chunk->chunk.SetCardinality(copy_count);
550: 	std::swap(reservoir_chunk, new_res_chunk);
551: }
552: 
553: idx_t ReservoirSample::GetReservoirChunkCapacity() const {
554: 	return sample_count + (FIXED_SAMPLE_SIZE_MULTIPLIER * MinValue<idx_t>(sample_count, FIXED_SAMPLE_SIZE));
555: }
556: 
557: idx_t ReservoirSample::FillReservoir(DataChunk &chunk) {
558: 
559: 	idx_t ingested_count = 0;
560: 	if (!reservoir_chunk) {
561: 		if (chunk.size() > FIXED_SAMPLE_SIZE) {
562: 			throw InternalException("Creating sample with DataChunk that is larger than the fixed sample size");
563: 		}
564: 		auto types = chunk.GetTypes();
565: 		// create a new sample chunk to store new samples
566: 		reservoir_chunk = CreateNewSampleChunk(types, GetReservoirChunkCapacity());
567: 	}
568: 
569: 	idx_t actual_sample_index_start = GetActiveSampleCount();
570: 	D_ASSERT(reservoir_chunk->chunk.ColumnCount() == chunk.ColumnCount());
571: 
572: 	if (reservoir_chunk->chunk.size() < sample_count) {
573: 		ingested_count = MinValue<idx_t>(sample_count - reservoir_chunk->chunk.size(), chunk.size());
574: 		auto random_other_sel =
575: 		    GetRandomizedVector(static_cast<uint32_t>(ingested_count), static_cast<uint32_t>(ingested_count));
576: 		SelectionVector sel_for_input_chunk(ingested_count);
577: 		for (idx_t i = 0; i < ingested_count; i++) {
578: 			sel.set_index(actual_sample_index_start + i, actual_sample_index_start + i);
579: 			sel_for_input_chunk.set_index(i, random_other_sel[i]);
580: 		}
581: 		UpdateSampleAppend(reservoir_chunk->chunk, chunk, sel_for_input_chunk, ingested_count);
582: 		sel_size += ingested_count;
583: 	}
584: 	D_ASSERT(GetActiveSampleCount() <= sample_count);
585: 	D_ASSERT(GetActiveSampleCount() >= ingested_count);
586: 	// always return how many tuples were ingested
587: 	return ingested_count;
588: }
589: 
590: void ReservoirSample::Destroy() {
591: 	destroyed = true;
592: }
593: 
594: SelectionVectorHelper ReservoirSample::GetReplacementIndexes(idx_t sample_chunk_offset,
595:                                                              idx_t theoretical_chunk_length) {
596: 	if (GetSamplingState() == SamplingState::RANDOM) {
597: 		return GetReplacementIndexesFast(sample_chunk_offset, theoretical_chunk_length);
598: 	}
599: 	return GetReplacementIndexesSlow(sample_chunk_offset, theoretical_chunk_length);
600: }
601: 
602: SelectionVectorHelper ReservoirSample::GetReplacementIndexesFast(idx_t sample_chunk_offset, idx_t chunk_length) {
603: 
604: 	// how much weight to the other tuples have compared to the ones in this chunk?
605: 	auto weight_tuples_other = static_cast<double>(chunk_length) / static_cast<double>(GetTuplesSeen() + chunk_length);
606: 	auto num_to_pop = static_cast<uint32_t>(round(weight_tuples_other * static_cast<double>(sample_count)));
607: 	D_ASSERT(num_to_pop <= sample_count);
608: 	D_ASSERT(num_to_pop <= sel_size);
609: 	SelectionVectorHelper ret;
610: 
611: 	if (num_to_pop == 0) {
612: 		ret.sel = SelectionVector(num_to_pop);
613: 		ret.size = 0;
614: 		return ret;
615: 	}
616: 	std::unordered_map<idx_t, idx_t> replacement_indexes;
617: 	SelectionVector chunk_sel(num_to_pop);
618: 
619: 	auto random_indexes_chunk = GetRandomizedVector(static_cast<uint32_t>(chunk_length), num_to_pop);
620: 	auto random_sel_indexes = GetRandomizedVector(static_cast<uint32_t>(sel_size), num_to_pop);
621: 	for (idx_t i = 0; i < num_to_pop; i++) {
622: 		// update the selection vector for the reservoir sample
623: 		chunk_sel.set_index(i, random_indexes_chunk[i]);
624: 		// sel is not guaratneed to be random, so we update the indexes according to our
625: 		// random sel indexes.
626: 		sel.set_index(random_sel_indexes[i], sample_chunk_offset + i);
627: 	}
628: 
629: 	D_ASSERT(sel_size == sample_count);
630: 
631: 	ret.sel = SelectionVector(chunk_sel);
632: 	ret.size = num_to_pop;
633: 	return ret;
634: }
635: 
636: SelectionVectorHelper ReservoirSample::GetReplacementIndexesSlow(const idx_t sample_chunk_offset,
637:                                                                  const idx_t chunk_length) {
638: 	idx_t remaining = chunk_length;
639: 	std::unordered_map<idx_t, idx_t> ret_map;
640: 	idx_t sample_chunk_index = 0;
641: 
642: 	idx_t base_offset = 0;
643: 
644: 	while (true) {
645: 		idx_t offset =
646: 		    base_reservoir_sample->next_index_to_sample - base_reservoir_sample->num_entries_to_skip_b4_next_sample;
647: 		if (offset >= remaining) {
648: 			// not in this chunk! increment current count and go to the next chunk
649: 			base_reservoir_sample->num_entries_to_skip_b4_next_sample += remaining;
650: 			break;
651: 		}
652: 		// in this chunk! replace the element
653: 		// ret[index_in_new_chunk] = index_in_sample_chunk (the sample chunk offset will be applied later)
654: 		// D_ASSERT(sample_chunk_index == ret.size());
655: 		ret_map[base_offset + offset] = sample_chunk_index;
656: 		double r2 = base_reservoir_sample->random.NextRandom32(base_reservoir_sample->min_weight_threshold, 1);
657: 		// replace element in our max_heap
658: 		// first get the top most pair
659: 		const auto top = PopFromWeightQueue();
660: 		const auto index = top.second;
661: 		const auto index_in_sample_chunk = sample_chunk_offset + sample_chunk_index;
662: 		sel.set_index(index, index_in_sample_chunk);
663: 		base_reservoir_sample->ReplaceElementWithIndex(index, r2, false);
664: 
665: 		sample_chunk_index += 1;
666: 		// shift the chunk forward
667: 		remaining -= offset;
668: 		base_offset += offset;
669: 	}
670: 
671: 	// create selection vector to return
672: 	SelectionVector ret_sel(ret_map.size());
673: 	D_ASSERT(sel_size == sample_count);
674: 	for (auto &kv : ret_map) {
675: 		ret_sel.set_index(kv.second, kv.first);
676: 	}
677: 	SelectionVectorHelper ret;
678: 	ret.sel = SelectionVector(ret_sel);
679: 	ret.size = static_cast<uint32_t>(ret_map.size());
680: 	return ret;
681: }
682: 
683: void ReservoirSample::Finalize() {
684: }
685: 
686: bool ReservoirSample::ValidSampleType(const LogicalType &type) {
687: 	return type.IsNumeric();
688: }
689: 
690: void ReservoirSample::UpdateSampleAppend(DataChunk &this_, DataChunk &other, SelectionVector &other_sel,
691:                                          idx_t append_count) const {
692: 	idx_t new_size = this_.size() + append_count;
693: 	if (other.size() == 0) {
694: 		return;
695: 	}
696: 	D_ASSERT(this_.GetTypes() == other.GetTypes());
697: 
698: 	// UpdateSampleAppend(this_, other, other_sel, append_count);
699: 	D_ASSERT(this_.GetTypes() == other.GetTypes());
700: 	auto types = reservoir_chunk->chunk.GetTypes();
701: 
702: 	for (idx_t i = 0; i < reservoir_chunk->chunk.ColumnCount(); i++) {
703: 		auto col_type = types[i];
704: 		if (ValidSampleType(col_type) || !stats_sample) {
705: 			D_ASSERT(this_.data[i].GetVectorType() == VectorType::FLAT_VECTOR);
706: 			VectorOperations::Copy(other.data[i], this_.data[i], other_sel, append_count, 0, this_.size());
707: 		}
708: 	}
709: 	this_.SetCardinality(new_size);
710: }
711: 
712: void ReservoirSample::AddToReservoir(DataChunk &chunk) {
713: 	if (destroyed || chunk.size() == 0) {
714: 		return;
715: 	}
716: 
717: 	idx_t tuples_consumed = FillReservoir(chunk);
718: 	base_reservoir_sample->num_entries_seen_total += tuples_consumed;
719: 	D_ASSERT(sample_count == 0 || reservoir_chunk->chunk.size() >= 1);
720: 
721: 	if (tuples_consumed == chunk.size()) {
722: 		return;
723: 	}
724: 
725: 	// the chunk filled the first FIXED_SAMPLE_SIZE chunk but still has tuples remaining
726: 	// slice the chunk and call AddToReservoir again.
727: 	if (tuples_consumed != chunk.size() && tuples_consumed != 0) {
728: 		// Fill reservoir consumed some of the chunk to reach FIXED_SAMPLE_SIZE
729: 		// now we need to
730: 		// So we slice it and call AddToReservoir
731: 		auto slice = make_uniq<DataChunk>();
732: 		auto samples_remaining = chunk.size() - tuples_consumed;
733: 		auto types = chunk.GetTypes();
734: 		SelectionVector input_sel(samples_remaining);
735: 		for (idx_t i = 0; i < samples_remaining; i++) {
736: 			input_sel.set_index(i, tuples_consumed + i);
737: 		}
738: 		slice->Initialize(Allocator::DefaultAllocator(), types, samples_remaining);
739: 		slice->Slice(chunk, input_sel, samples_remaining);
740: 		slice->SetCardinality(samples_remaining);
741: 		AddToReservoir(*slice);
742: 		return;
743: 	}
744: 
745: 	// at this point we should have collected at least sample count samples
746: 	D_ASSERT(GetActiveSampleCount() >= sample_count);
747: 
748: 	auto chunk_sel = GetReplacementIndexes(reservoir_chunk->chunk.size(), chunk.size());
749: 
750: 	if (chunk_sel.size == 0) {
751: 		// not adding any samples
752: 		base_reservoir_sample->num_entries_seen_total += chunk.size();
753: 		return;
754: 	}
755: 	idx_t size = chunk_sel.size;
756: 	D_ASSERT(size <= chunk.size());
757: 
758: 	UpdateSampleAppend(reservoir_chunk->chunk, chunk, chunk_sel.sel, size);
759: 
760: 	base_reservoir_sample->num_entries_seen_total += chunk.size();
761: 	D_ASSERT(base_reservoir_sample->reservoir_weights.size() == 0 ||
762: 	         base_reservoir_sample->reservoir_weights.size() == sample_count);
763: 
764: 	Verify();
765: 
766: 	// if we are over the threshold, we ned to swith to slow sampling.
767: 	if (GetSamplingState() == SamplingState::RANDOM && GetTuplesSeen() >= FIXED_SAMPLE_SIZE * FAST_TO_SLOW_THRESHOLD) {
768: 		ConvertToReservoirSample();
769: 	}
770: 	if (reservoir_chunk->chunk.size() >= (GetReservoirChunkCapacity() - (static_cast<idx_t>(FIXED_SAMPLE_SIZE) * 3))) {
771: 		Vacuum();
772: 	}
773: }
774: 
775: void ReservoirSample::Verify() {
776: #ifdef DEBUG
777: 	if (destroyed) {
778: 		return;
779: 	}
780: 	if (GetPriorityQueueSize() == 0) {
781: 		D_ASSERT(GetActiveSampleCount() <= sample_count);
782: 		D_ASSERT(GetTuplesSeen() >= GetActiveSampleCount());
783: 		return;
784: 	}
785: 	if (NumSamplesCollected() > sample_count) {
786: 		D_ASSERT(GetPriorityQueueSize() == sample_count);
787: 	} else if (NumSamplesCollected() <= sample_count && GetPriorityQueueSize() > 0) {
788: 		// it's possible to collect more samples than your priority queue size.
789: 		// see sample_converts_to_reservoir_sample.test
790: 		D_ASSERT(NumSamplesCollected() >= GetPriorityQueueSize());
791: 	}
792: 	auto base_reservoir_copy = base_reservoir_sample->Copy();
793: 	std::unordered_map<idx_t, idx_t> index_count;
794: 	while (!base_reservoir_copy->reservoir_weights.empty()) {
795: 		auto &pair = base_reservoir_copy->reservoir_weights.top();
796: 		if (index_count.find(pair.second) == index_count.end()) {
797: 			index_count[pair.second] = 1;
798: 			base_reservoir_copy->reservoir_weights.pop();
799: 		} else {
800: 			index_count[pair.second] += 1;
801: 			base_reservoir_copy->reservoir_weights.pop();
802: 			throw InternalException("Duplicate selection index in reservoir weights");
803: 		}
804: 	}
805: 	// TODO: Verify the Sel as well. No duplicate indices.
806: 
807: 	if (reservoir_chunk) {
808: 		reservoir_chunk->chunk.Verify();
809: 	}
810: #endif
811: }
812: 
813: ReservoirSamplePercentage::ReservoirSamplePercentage(double percentage, int64_t seed, idx_t reservoir_sample_size)
814:     : BlockingSample(seed), allocator(Allocator::DefaultAllocator()), sample_percentage(percentage / 100.0),
815:       reservoir_sample_size(reservoir_sample_size), current_count(0), is_finalized(false) {
816: 	current_sample = make_uniq<ReservoirSample>(allocator, reservoir_sample_size, base_reservoir_sample->random());
817: 	type = SampleType::RESERVOIR_PERCENTAGE_SAMPLE;
818: }
819: 
820: ReservoirSamplePercentage::ReservoirSamplePercentage(Allocator &allocator, double percentage, int64_t seed)
821:     : BlockingSample(seed), allocator(allocator), sample_percentage(percentage / 100.0), current_count(0),
822:       is_finalized(false) {
823: 	reservoir_sample_size = (idx_t)(sample_percentage * RESERVOIR_THRESHOLD);
824: 	current_sample = make_uniq<ReservoirSample>(allocator, reservoir_sample_size, base_reservoir_sample->random());
825: 	type = SampleType::RESERVOIR_PERCENTAGE_SAMPLE;
826: }
827: 
828: ReservoirSamplePercentage::ReservoirSamplePercentage(double percentage, int64_t seed)
829:     : ReservoirSamplePercentage(Allocator::DefaultAllocator(), percentage, seed) {
830: }
831: 
832: void ReservoirSamplePercentage::AddToReservoir(DataChunk &input) {
833: 	base_reservoir_sample->num_entries_seen_total += input.size();
834: 	if (current_count + input.size() > RESERVOIR_THRESHOLD) {
835: 		// we don't have enough space in our current reservoir
836: 		// first check what we still need to append to the current sample
837: 		idx_t append_to_current_sample_count = RESERVOIR_THRESHOLD - current_count;
838: 		idx_t append_to_next_sample = input.size() - append_to_current_sample_count;
839: 		if (append_to_current_sample_count > 0) {
840: 			// we have elements remaining, first add them to the current sample
841: 			if (append_to_next_sample > 0) {
842: 				// we need to also add to the next sample
843: 				DataChunk new_chunk;
844: 				new_chunk.InitializeEmpty(input.GetTypes());
845: 				new_chunk.Slice(input, *FlatVector::IncrementalSelectionVector(), append_to_current_sample_count);
846: 				new_chunk.Flatten();
847: 				current_sample->AddToReservoir(new_chunk);
848: 			} else {
849: 				input.Flatten();
850: 				input.SetCardinality(append_to_current_sample_count);
851: 				current_sample->AddToReservoir(input);
852: 			}
853: 		}
854: 		if (append_to_next_sample > 0) {
855: 			// slice the input for the remainder
856: 			SelectionVector sel(append_to_next_sample);
857: 			for (idx_t i = append_to_current_sample_count; i < append_to_next_sample + append_to_current_sample_count;
858: 			     i++) {
859: 				sel.set_index(i - append_to_current_sample_count, i);
860: 			}
861: 			input.Slice(sel, append_to_next_sample);
862: 		}
863: 		// now our first sample is filled: append it to the set of finished samples
864: 		finished_samples.push_back(std::move(current_sample));
865: 
866: 		// allocate a new sample, and potentially add the remainder of the current input to that sample
867: 		current_sample = make_uniq<ReservoirSample>(allocator, reservoir_sample_size, base_reservoir_sample->random());
868: 		if (append_to_next_sample > 0) {
869: 			current_sample->AddToReservoir(input);
870: 		}
871: 		current_count = append_to_next_sample;
872: 	} else {
873: 		// we can just append to the current sample
874: 		current_count += input.size();
875: 		current_sample->AddToReservoir(input);
876: 	}
877: }
878: 
879: unique_ptr<DataChunk> ReservoirSamplePercentage::GetChunk() {
880: 	// reservoir sample percentage should never stay
881: 	if (!is_finalized) {
882: 		Finalize();
883: 	}
884: 	while (!finished_samples.empty()) {
885: 		auto &front = finished_samples.front();
886: 		auto chunk = front->GetChunk();
887: 		if (chunk && chunk->size() > 0) {
888: 			return chunk;
889: 		}
890: 		// move to the next sample
891: 		finished_samples.erase(finished_samples.begin());
892: 	}
893: 	return nullptr;
894: }
895: 
896: unique_ptr<BlockingSample> ReservoirSamplePercentage::Copy() const {
897: 	throw InternalException("Cannot call Copy on ReservoirSample Percentage");
898: }
899: 
900: void ReservoirSamplePercentage::Finalize() {
901: 	// need to finalize the current sample, if any
902: 	// we are finializing, so we are starting to return chunks. Our last chunk has
903: 	// sample_percentage * RESERVOIR_THRESHOLD entries that hold samples.
904: 	// if our current count is less than the sample_percentage * RESERVOIR_THRESHOLD
905: 	// then we have sampled too much for the current_sample and we need to redo the sample
906: 	// otherwise we can just push the current sample back
907: 	// Imagine sampling 70% of 100 rows (so 70 rows). We allocate sample_percentage * RESERVOIR_THRESHOLD
908: 	// -----------------------------------------
909: 	auto sampled_more_than_required =
910: 	    static_cast<double>(current_count) > sample_percentage * RESERVOIR_THRESHOLD || finished_samples.empty();
911: 	if (current_count > 0 && sampled_more_than_required) {
912: 		// create a new sample
913: 		auto new_sample_size = static_cast<idx_t>(round(sample_percentage * static_cast<double>(current_count)));
914: 		auto new_sample = make_uniq<ReservoirSample>(allocator, new_sample_size, base_reservoir_sample->random());
915: 		while (true) {
916: 			auto chunk = current_sample->GetChunk();
917: 			if (!chunk || chunk->size() == 0) {
918: 				break;
919: 			}
920: 			new_sample->AddToReservoir(*chunk);
921: 		}
922: 		finished_samples.push_back(std::move(new_sample));
923: 	} else {
924: 		finished_samples.push_back(std::move(current_sample));
925: 	}
926: 	// when finalizing, current_sample is null. All samples are now in finished samples.
927: 	current_sample = nullptr;
928: 	is_finalized = true;
929: }
930: 
931: } // namespace duckdb
[end of src/execution/sample/reservoir_sample.cpp]
[start of src/function/window/window_constant_aggregator.cpp]
1: #include "duckdb/function/window/window_constant_aggregator.hpp"
2: 
3: #include "duckdb/function/function_binder.hpp"
4: #include "duckdb/function/window/window_aggregate_states.hpp"
5: #include "duckdb/function/window/window_shared_expressions.hpp"
6: #include "duckdb/planner/expression/bound_window_expression.hpp"
7: 
8: namespace duckdb {
9: 
10: //===--------------------------------------------------------------------===//
11: // WindowConstantAggregatorGlobalState
12: //===--------------------------------------------------------------------===//
13: 
14: class WindowConstantAggregatorGlobalState : public WindowAggregatorGlobalState {
15: public:
16: 	WindowConstantAggregatorGlobalState(ClientContext &context, const WindowConstantAggregator &aggregator, idx_t count,
17: 	                                    const ValidityMask &partition_mask);
18: 
19: 	void Finalize(const FrameStats &stats);
20: 
21: 	//! Partition starts
22: 	vector<idx_t> partition_offsets;
23: 	//! Reused result state container for the window functions
24: 	WindowAggregateStates statef;
25: 	//! Aggregate results
26: 	unique_ptr<Vector> results;
27: };
28: 
29: WindowConstantAggregatorGlobalState::WindowConstantAggregatorGlobalState(ClientContext &context,
30:                                                                          const WindowConstantAggregator &aggregator,
31:                                                                          idx_t group_count,
32:                                                                          const ValidityMask &partition_mask)
33:     : WindowAggregatorGlobalState(context, aggregator, STANDARD_VECTOR_SIZE), statef(aggr) {
34: 
35: 	// Locate the partition boundaries
36: 	if (partition_mask.AllValid()) {
37: 		partition_offsets.emplace_back(0);
38: 	} else {
39: 		idx_t entry_idx;
40: 		idx_t shift;
41: 		for (idx_t start = 0; start < group_count;) {
42: 			partition_mask.GetEntryIndex(start, entry_idx, shift);
43: 
44: 			//	If start is aligned with the start of a block,
45: 			//	and the block is blank, then skip forward one block.
46: 			const auto block = partition_mask.GetValidityEntry(entry_idx);
47: 			if (partition_mask.NoneValid(block) && !shift) {
48: 				start += ValidityMask::BITS_PER_VALUE;
49: 				continue;
50: 			}
51: 
52: 			// Loop over the block
53: 			for (; shift < ValidityMask::BITS_PER_VALUE && start < group_count; ++shift, ++start) {
54: 				if (partition_mask.RowIsValid(block, shift)) {
55: 					partition_offsets.emplace_back(start);
56: 				}
57: 			}
58: 		}
59: 	}
60: 
61: 	//	Initialise the vector for caching the results
62: 	results = make_uniq<Vector>(aggregator.result_type, partition_offsets.size());
63: 
64: 	//	Initialise the final states
65: 	statef.Initialize(partition_offsets.size());
66: 
67: 	// Add final guard
68: 	partition_offsets.emplace_back(group_count);
69: }
70: 
71: //===--------------------------------------------------------------------===//
72: // WindowConstantAggregatorLocalState
73: //===--------------------------------------------------------------------===//
74: class WindowConstantAggregatorLocalState : public WindowAggregatorLocalState {
75: public:
76: 	explicit WindowConstantAggregatorLocalState(const WindowConstantAggregatorGlobalState &gstate);
77: 	~WindowConstantAggregatorLocalState() override {
78: 	}
79: 
80: 	void Sink(DataChunk &sink_chunk, DataChunk &coll_chunk, idx_t input_idx, optional_ptr<SelectionVector> filter_sel,
81: 	          idx_t filtered);
82: 	void Combine(WindowConstantAggregatorGlobalState &gstate);
83: 
84: public:
85: 	//! The global state we are sharing
86: 	const WindowConstantAggregatorGlobalState &gstate;
87: 	//! Reusable chunk for sinking
88: 	DataChunk inputs;
89: 	//! Chunk for referencing the input columns
90: 	DataChunk payload_chunk;
91: 	//! A vector of pointers to "state", used for intermediate window segment aggregation
92: 	Vector statep;
93: 	//! Reused result state container for the window functions
94: 	WindowAggregateStates statef;
95: 	//! The current result partition being read
96: 	idx_t partition;
97: 	//! Shared SV for evaluation
98: 	SelectionVector matches;
99: };
100: 
101: WindowConstantAggregatorLocalState::WindowConstantAggregatorLocalState(
102:     const WindowConstantAggregatorGlobalState &gstate)
103:     : gstate(gstate), statep(Value::POINTER(0)), statef(gstate.statef.aggr), partition(0) {
104: 	matches.Initialize();
105: 
106: 	//	Start the aggregates
107: 	auto &partition_offsets = gstate.partition_offsets;
108: 	auto &aggregator = gstate.aggregator;
109: 	statef.Initialize(partition_offsets.size() - 1);
110: 
111: 	// Set up shared buffer
112: 	inputs.Initialize(Allocator::DefaultAllocator(), aggregator.arg_types);
113: 	payload_chunk.InitializeEmpty(inputs.GetTypes());
114: 
115: 	gstate.locals++;
116: }
117: 
118: //===--------------------------------------------------------------------===//
119: // WindowConstantAggregator
120: //===--------------------------------------------------------------------===//
121: bool WindowConstantAggregator::CanAggregate(const BoundWindowExpression &wexpr) {
122: 	if (!wexpr.aggregate) {
123: 		return false;
124: 	}
125: 	// window exclusion cannot be handled by constant aggregates
126: 	if (wexpr.exclude_clause != WindowExcludeMode::NO_OTHER) {
127: 		return false;
128: 	}
129: 
130: 	// 	DISTINCT aggregation cannot be handled by constant aggregation
131: 	if (wexpr.distinct) {
132: 		return false;
133: 	}
134: 
135: 	//	COUNT(*) is already handled efficiently by segment trees.
136: 	if (wexpr.children.empty()) {
137: 		return false;
138: 	}
139: 
140: 	/*
141: 	    The default framing option is RANGE UNBOUNDED PRECEDING, which
142: 	    is the same as RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT
143: 	    ROW; it sets the frame to be all rows from the partition start
144: 	    up through the current row's last peer (a row that the window's
145: 	    ORDER BY clause considers equivalent to the current row; all
146: 	    rows are peers if there is no ORDER BY). In general, UNBOUNDED
147: 	    PRECEDING means that the frame starts with the first row of the
148: 	    partition, and similarly UNBOUNDED FOLLOWING means that the
149: 	    frame ends with the last row of the partition, regardless of
150: 	    RANGE, ROWS or GROUPS mode. In ROWS mode, CURRENT ROW means that
151: 	    the frame starts or ends with the current row; but in RANGE or
152: 	    GROUPS mode it means that the frame starts or ends with the
153: 	    current row's first or last peer in the ORDER BY ordering. The
154: 	    offset PRECEDING and offset FOLLOWING options vary in meaning
155: 	    depending on the frame mode.
156: 	*/
157: 	switch (wexpr.start) {
158: 	case WindowBoundary::UNBOUNDED_PRECEDING:
159: 		break;
160: 	case WindowBoundary::CURRENT_ROW_RANGE:
161: 		if (!wexpr.orders.empty()) {
162: 			return false;
163: 		}
164: 		break;
165: 	default:
166: 		return false;
167: 	}
168: 
169: 	switch (wexpr.end) {
170: 	case WindowBoundary::UNBOUNDED_FOLLOWING:
171: 		break;
172: 	case WindowBoundary::CURRENT_ROW_RANGE:
173: 		if (!wexpr.orders.empty()) {
174: 			return false;
175: 		}
176: 		break;
177: 	default:
178: 		return false;
179: 	}
180: 
181: 	return true;
182: }
183: 
184: BoundWindowExpression &WindowConstantAggregator::RebindAggregate(ClientContext &context, BoundWindowExpression &wexpr) {
185: 	FunctionBinder::BindSortedAggregate(context, wexpr);
186: 
187: 	return wexpr;
188: }
189: 
190: WindowConstantAggregator::WindowConstantAggregator(BoundWindowExpression &wexpr, WindowSharedExpressions &shared,
191:                                                    ClientContext &context)
192:     : WindowAggregator(RebindAggregate(context, wexpr)) {
193: 
194: 	// We only need these values for Sink
195: 	for (auto &child : wexpr.children) {
196: 		child_idx.emplace_back(shared.RegisterSink(child));
197: 	}
198: }
199: 
200: unique_ptr<WindowAggregatorState> WindowConstantAggregator::GetGlobalState(ClientContext &context, idx_t group_count,
201:                                                                            const ValidityMask &partition_mask) const {
202: 	return make_uniq<WindowConstantAggregatorGlobalState>(context, *this, group_count, partition_mask);
203: }
204: 
205: void WindowConstantAggregator::Sink(WindowAggregatorState &gsink, WindowAggregatorState &lstate, DataChunk &sink_chunk,
206:                                     DataChunk &coll_chunk, idx_t input_idx, optional_ptr<SelectionVector> filter_sel,
207:                                     idx_t filtered) {
208: 	auto &lastate = lstate.Cast<WindowConstantAggregatorLocalState>();
209: 
210: 	lastate.Sink(sink_chunk, coll_chunk, input_idx, filter_sel, filtered);
211: }
212: 
213: void WindowConstantAggregatorLocalState::Sink(DataChunk &sink_chunk, DataChunk &coll_chunk, idx_t row,
214:                                               optional_ptr<SelectionVector> filter_sel, idx_t filtered) {
215: 	auto &partition_offsets = gstate.partition_offsets;
216: 	const auto &aggr = gstate.aggr;
217: 	const auto chunk_begin = row;
218: 	const auto chunk_end = chunk_begin + sink_chunk.size();
219: 	idx_t partition =
220: 	    idx_t(std::upper_bound(partition_offsets.begin(), partition_offsets.end(), row) - partition_offsets.begin()) -
221: 	    1;
222: 
223: 	auto state_f_data = statef.GetData();
224: 	auto state_p_data = FlatVector::GetData<data_ptr_t>(statep);
225: 
226: 	auto &child_idx = gstate.aggregator.child_idx;
227: 	for (column_t c = 0; c < child_idx.size(); ++c) {
228: 		payload_chunk.data[c].Reference(sink_chunk.data[child_idx[c]]);
229: 	}
230: 
231: 	AggregateInputData aggr_input_data(aggr.GetFunctionData(), allocator);
232: 	idx_t begin = 0;
233: 	idx_t filter_idx = 0;
234: 	auto partition_end = partition_offsets[partition + 1];
235: 	while (row < chunk_end) {
236: 		if (row == partition_end) {
237: 			++partition;
238: 			partition_end = partition_offsets[partition + 1];
239: 		}
240: 		partition_end = MinValue(partition_end, chunk_end);
241: 		auto end = partition_end - chunk_begin;
242: 
243: 		inputs.Reset();
244: 		if (filter_sel) {
245: 			// 	Slice to any filtered rows in [begin, end)
246: 			SelectionVector sel;
247: 
248: 			//	Find the first value in [begin, end)
249: 			for (; filter_idx < filtered; ++filter_idx) {
250: 				auto idx = filter_sel->get_index(filter_idx);
251: 				if (idx >= begin) {
252: 					break;
253: 				}
254: 			}
255: 
256: 			//	Find the first value in [end, filtered)
257: 			sel.Initialize(filter_sel->data() + filter_idx);
258: 			idx_t nsel = 0;
259: 			for (; filter_idx < filtered; ++filter_idx, ++nsel) {
260: 				auto idx = filter_sel->get_index(filter_idx);
261: 				if (idx >= end) {
262: 					break;
263: 				}
264: 			}
265: 
266: 			if (nsel != inputs.size()) {
267: 				inputs.Slice(payload_chunk, sel, nsel);
268: 			}
269: 		} else {
270: 			//	Slice to [begin, end)
271: 			if (begin) {
272: 				for (idx_t c = 0; c < payload_chunk.ColumnCount(); ++c) {
273: 					inputs.data[c].Slice(payload_chunk.data[c], begin, end);
274: 				}
275: 			} else {
276: 				inputs.Reference(payload_chunk);
277: 			}
278: 			inputs.SetCardinality(end - begin);
279: 		}
280: 
281: 		//	Aggregate the filtered rows into a single state
282: 		const auto count = inputs.size();
283: 		auto state = state_f_data[partition];
284: 		if (aggr.function.simple_update) {
285: 			aggr.function.simple_update(inputs.data.data(), aggr_input_data, inputs.ColumnCount(), state, count);
286: 		} else {
287: 			state_p_data[0] = state_f_data[partition];
288: 			aggr.function.update(inputs.data.data(), aggr_input_data, inputs.ColumnCount(), statep, count);
289: 		}
290: 
291: 		//	Skip filtered rows too!
292: 		row += end - begin;
293: 		begin = end;
294: 	}
295: }
296: 
297: void WindowConstantAggregator::Finalize(WindowAggregatorState &gstate, WindowAggregatorState &lstate,
298:                                         CollectionPtr collection, const FrameStats &stats) {
299: 	auto &gastate = gstate.Cast<WindowConstantAggregatorGlobalState>();
300: 	auto &lastate = lstate.Cast<WindowConstantAggregatorLocalState>();
301: 
302: 	//	Single-threaded combine
303: 	lock_guard<mutex> finalize_guard(gastate.lock);
304: 	lastate.statef.Combine(gastate.statef);
305: 	lastate.statef.Destroy();
306: 
307: 	//	Last one out turns off the lights!
308: 	if (++gastate.finalized == gastate.locals) {
309: 		gastate.statef.Finalize(*gastate.results);
310: 		gastate.statef.Destroy();
311: 	}
312: }
313: 
314: unique_ptr<WindowAggregatorState> WindowConstantAggregator::GetLocalState(const WindowAggregatorState &gstate) const {
315: 	return make_uniq<WindowConstantAggregatorLocalState>(gstate.Cast<WindowConstantAggregatorGlobalState>());
316: }
317: 
318: void WindowConstantAggregator::Evaluate(const WindowAggregatorState &gsink, WindowAggregatorState &lstate,
319:                                         const DataChunk &bounds, Vector &result, idx_t count, idx_t row_idx) const {
320: 	auto &gasink = gsink.Cast<WindowConstantAggregatorGlobalState>();
321: 	const auto &partition_offsets = gasink.partition_offsets;
322: 	const auto &results = *gasink.results;
323: 
324: 	auto begins = FlatVector::GetData<const idx_t>(bounds.data[FRAME_BEGIN]);
325: 	//	Chunk up the constants and copy them one at a time
326: 	auto &lcstate = lstate.Cast<WindowConstantAggregatorLocalState>();
327: 	idx_t matched = 0;
328: 	idx_t target_offset = 0;
329: 	for (idx_t i = 0; i < count; ++i) {
330: 		const auto begin = begins[i];
331: 		//	Find the partition containing [begin, end)
332: 		while (partition_offsets[lcstate.partition + 1] <= begin) {
333: 			//	Flush the previous partition's data
334: 			if (matched) {
335: 				VectorOperations::Copy(results, result, lcstate.matches, matched, 0, target_offset);
336: 				target_offset += matched;
337: 				matched = 0;
338: 			}
339: 			++lcstate.partition;
340: 		}
341: 
342: 		lcstate.matches.set_index(matched++, lcstate.partition);
343: 	}
344: 
345: 	//	Flush the last partition
346: 	if (matched) {
347: 		// Optimize constant result
348: 		if (target_offset == 0 && matched == count) {
349: 			VectorOperations::Copy(results, result, lcstate.matches, 1, 0, target_offset);
350: 			result.SetVectorType(VectorType::CONSTANT_VECTOR);
351: 		} else {
352: 			VectorOperations::Copy(results, result, lcstate.matches, matched, 0, target_offset);
353: 		}
354: 	}
355: }
356: 
357: } // namespace duckdb
[end of src/function/window/window_constant_aggregator.cpp]
[start of src/function/window/window_distinct_aggregator.cpp]
1: #include "duckdb/function/window/window_distinct_aggregator.hpp"
2: 
3: #include "duckdb/common/sort/partition_state.hpp"
4: #include "duckdb/common/sort/sort.hpp"
5: #include "duckdb/execution/merge_sort_tree.hpp"
6: #include "duckdb/function/window/window_aggregate_states.hpp"
7: #include "duckdb/planner/bound_result_modifier.hpp"
8: #include "duckdb/planner/expression/bound_constant_expression.hpp"
9: #include "duckdb/planner/expression/bound_window_expression.hpp"
10: 
11: #include <numeric>
12: #include <thread>
13: 
14: namespace duckdb {
15: 
16: //===--------------------------------------------------------------------===//
17: // WindowDistinctAggregator
18: //===--------------------------------------------------------------------===//
19: bool WindowDistinctAggregator::CanAggregate(const BoundWindowExpression &wexpr) {
20: 	if (!wexpr.aggregate) {
21: 		return false;
22: 	}
23: 
24: 	return wexpr.distinct && wexpr.exclude_clause == WindowExcludeMode::NO_OTHER && wexpr.arg_orders.empty();
25: }
26: 
27: WindowDistinctAggregator::WindowDistinctAggregator(const BoundWindowExpression &wexpr, WindowSharedExpressions &shared,
28:                                                    ClientContext &context)
29:     : WindowAggregator(wexpr, shared), context(context) {
30: }
31: 
32: class WindowDistinctAggregatorLocalState;
33: 
34: class WindowDistinctAggregatorGlobalState;
35: 
36: class WindowDistinctSortTree : public MergeSortTree<idx_t, idx_t> {
37: public:
38: 	// prev_idx, input_idx
39: 	using ZippedTuple = std::tuple<idx_t, idx_t>;
40: 	using ZippedElements = vector<ZippedTuple>;
41: 
42: 	explicit WindowDistinctSortTree(WindowDistinctAggregatorGlobalState &gdastate, idx_t count) : gdastate(gdastate) {
43: 		//	Set up for parallel build
44: 		build_level = 0;
45: 		build_complete = 0;
46: 		build_run = 0;
47: 		build_run_length = 1;
48: 		build_num_runs = count;
49: 	}
50: 
51: 	void Build(WindowDistinctAggregatorLocalState &ldastate);
52: 
53: protected:
54: 	bool TryNextRun(idx_t &level_idx, idx_t &run_idx);
55: 	void BuildRun(idx_t level_nr, idx_t i, WindowDistinctAggregatorLocalState &ldastate);
56: 
57: 	WindowDistinctAggregatorGlobalState &gdastate;
58: };
59: 
60: class WindowDistinctAggregatorGlobalState : public WindowAggregatorGlobalState {
61: public:
62: 	using GlobalSortStatePtr = unique_ptr<GlobalSortState>;
63: 	using LocalSortStatePtr = unique_ptr<LocalSortState>;
64: 	using ZippedTuple = WindowDistinctSortTree::ZippedTuple;
65: 	using ZippedElements = WindowDistinctSortTree::ZippedElements;
66: 
67: 	WindowDistinctAggregatorGlobalState(ClientContext &context, const WindowDistinctAggregator &aggregator,
68: 	                                    idx_t group_count);
69: 
70: 	//! Compute the block starts
71: 	void MeasurePayloadBlocks();
72: 	//! Create a new local sort
73: 	optional_ptr<LocalSortState> InitializeLocalSort() const;
74: 
75: 	//! Patch up the previous index block boundaries
76: 	void PatchPrevIdcs();
77: 	bool TryPrepareNextStage(WindowDistinctAggregatorLocalState &lstate);
78: 
79: 	//	Single threaded sorting for now
80: 	ClientContext &context;
81: 	idx_t memory_per_thread;
82: 
83: 	//! Finalize guard
84: 	mutable mutex lock;
85: 	//! Finalize stage
86: 	atomic<PartitionSortStage> stage;
87: 	//! Tasks launched
88: 	idx_t total_tasks = 0;
89: 	//! Tasks launched
90: 	mutable idx_t tasks_assigned;
91: 	//! Tasks landed
92: 	mutable atomic<idx_t> tasks_completed;
93: 
94: 	//! The sorted payload data types (partition index)
95: 	vector<LogicalType> payload_types;
96: 	//! The aggregate arguments + partition index
97: 	vector<LogicalType> sort_types;
98: 
99: 	//! Sorting operations
100: 	GlobalSortStatePtr global_sort;
101: 	//! Local sort set
102: 	mutable vector<LocalSortStatePtr> local_sorts;
103: 	//! The block starts (the scanner doesn't know this) plus the total count
104: 	vector<idx_t> block_starts;
105: 
106: 	//! The block boundary seconds
107: 	mutable ZippedElements seconds;
108: 	//! The MST with the distinct back pointers
109: 	mutable MergeSortTree<ZippedTuple> zipped_tree;
110: 	//! The merge sort tree for the aggregate.
111: 	WindowDistinctSortTree merge_sort_tree;
112: 
113: 	//! The actual window segment tree: an array of aggregate states that represent all the intermediate nodes
114: 	WindowAggregateStates levels_flat_native;
115: 	//! For each level, the starting location in the levels_flat_native array
116: 	vector<idx_t> levels_flat_start;
117: };
118: 
119: WindowDistinctAggregatorGlobalState::WindowDistinctAggregatorGlobalState(ClientContext &context,
120:                                                                          const WindowDistinctAggregator &aggregator,
121:                                                                          idx_t group_count)
122:     : WindowAggregatorGlobalState(context, aggregator, group_count), context(aggregator.context),
123:       stage(PartitionSortStage::INIT), tasks_assigned(0), tasks_completed(0), merge_sort_tree(*this, group_count),
124:       levels_flat_native(aggr) {
125: 	payload_types.emplace_back(LogicalType::UBIGINT);
126: 
127: 	//	1:	functionComputePrevIdcs(𝑖𝑛)
128: 	//	2:		sorted ← []
129: 	//	We sort the aggregate arguments and use the partition index as a tie-breaker.
130: 	//	TODO: Use a hash table?
131: 	sort_types = aggregator.arg_types;
132: 	for (const auto &type : payload_types) {
133: 		sort_types.emplace_back(type);
134: 	}
135: 
136: 	vector<BoundOrderByNode> orders;
137: 	for (const auto &type : sort_types) {
138: 		auto expr = make_uniq<BoundConstantExpression>(Value(type));
139: 		orders.emplace_back(BoundOrderByNode(OrderType::ASCENDING, OrderByNullType::NULLS_FIRST, std::move(expr)));
140: 	}
141: 
142: 	RowLayout payload_layout;
143: 	payload_layout.Initialize(payload_types);
144: 
145: 	global_sort = make_uniq<GlobalSortState>(BufferManager::GetBufferManager(context), orders, payload_layout);
146: 
147: 	memory_per_thread = PhysicalOperator::GetMaxThreadMemory(context);
148: 
149: 	//	6:	prevIdcs ← []
150: 	//	7:	prevIdcs[0] ← “-”
151: 	auto &prev_idcs = zipped_tree.Allocate(group_count);
152: 
153: 	//	To handle FILTER clauses we make the missing elements
154: 	//	point to themselves so they won't be counted.
155: 	for (idx_t i = 0; i < group_count; ++i) {
156: 		prev_idcs[i] = ZippedTuple(i + 1, i);
157: 	}
158: 
159: 	// compute space required to store aggregation states of merge sort tree
160: 	// this is one aggregate state per entry per level
161: 	idx_t internal_nodes = 0;
162: 	levels_flat_start.push_back(internal_nodes);
163: 	for (idx_t level_nr = 0; level_nr < zipped_tree.tree.size(); ++level_nr) {
164: 		internal_nodes += zipped_tree.tree[level_nr].first.size();
165: 		levels_flat_start.push_back(internal_nodes);
166: 	}
167: 	levels_flat_native.Initialize(internal_nodes);
168: 
169: 	merge_sort_tree.tree.reserve(zipped_tree.tree.size());
170: 	for (idx_t level_nr = 0; level_nr < zipped_tree.tree.size(); ++level_nr) {
171: 		auto &zipped_level = zipped_tree.tree[level_nr].first;
172: 		WindowDistinctSortTree::Elements level;
173: 		WindowDistinctSortTree::Offsets cascades;
174: 		level.resize(zipped_level.size());
175: 		merge_sort_tree.tree.emplace_back(std::move(level), std::move(cascades));
176: 	}
177: }
178: 
179: optional_ptr<LocalSortState> WindowDistinctAggregatorGlobalState::InitializeLocalSort() const {
180: 	lock_guard<mutex> local_sort_guard(lock);
181: 	auto local_sort = make_uniq<LocalSortState>();
182: 	local_sort->Initialize(*global_sort, global_sort->buffer_manager);
183: 	++tasks_assigned;
184: 	local_sorts.emplace_back(std::move(local_sort));
185: 
186: 	return local_sorts.back().get();
187: }
188: 
189: class WindowDistinctAggregatorLocalState : public WindowAggregatorLocalState {
190: public:
191: 	explicit WindowDistinctAggregatorLocalState(const WindowDistinctAggregatorGlobalState &aggregator);
192: 
193: 	void Sink(DataChunk &sink_chunk, DataChunk &coll_chunk, idx_t input_idx, optional_ptr<SelectionVector> filter_sel,
194: 	          idx_t filtered);
195: 	void Finalize(WindowAggregatorGlobalState &gastate, CollectionPtr collection) override;
196: 	void Sorted();
197: 	void ExecuteTask();
198: 	void Evaluate(const WindowDistinctAggregatorGlobalState &gdstate, const DataChunk &bounds, Vector &result,
199: 	              idx_t count, idx_t row_idx);
200: 
201: 	//! Thread-local sorting data
202: 	optional_ptr<LocalSortState> local_sort;
203: 	//! Finalize stage
204: 	PartitionSortStage stage = PartitionSortStage::INIT;
205: 	//! Finalize scan block index
206: 	idx_t block_idx;
207: 	//! Thread-local tree aggregation
208: 	Vector update_v;
209: 	Vector source_v;
210: 	Vector target_v;
211: 	DataChunk leaves;
212: 	SelectionVector sel;
213: 
214: protected:
215: 	//! Flush the accumulated intermediate states into the result states
216: 	void FlushStates();
217: 
218: 	//! The aggregator we are working with
219: 	const WindowDistinctAggregatorGlobalState &gastate;
220: 	DataChunk sort_chunk;
221: 	DataChunk payload_chunk;
222: 	//! Reused result state container for the window functions
223: 	WindowAggregateStates statef;
224: 	//! A vector of pointers to "state", used for buffering intermediate aggregates
225: 	Vector statep;
226: 	//! Reused state pointers for combining tree elements
227: 	Vector statel;
228: 	//! Count of buffered values
229: 	idx_t flush_count;
230: 	//! The frame boundaries, used for the window functions
231: 	SubFrames frames;
232: };
233: 
234: WindowDistinctAggregatorLocalState::WindowDistinctAggregatorLocalState(
235:     const WindowDistinctAggregatorGlobalState &gastate)
236:     : update_v(LogicalType::POINTER), source_v(LogicalType::POINTER), target_v(LogicalType::POINTER), gastate(gastate),
237:       statef(gastate.aggr), statep(LogicalType::POINTER), statel(LogicalType::POINTER), flush_count(0) {
238: 	InitSubFrames(frames, gastate.aggregator.exclude_mode);
239: 	payload_chunk.Initialize(Allocator::DefaultAllocator(), gastate.payload_types);
240: 
241: 	sort_chunk.Initialize(Allocator::DefaultAllocator(), gastate.sort_types);
242: 	sort_chunk.data.back().Reference(payload_chunk.data[0]);
243: 
244: 	gastate.locals++;
245: }
246: 
247: unique_ptr<WindowAggregatorState> WindowDistinctAggregator::GetGlobalState(ClientContext &context, idx_t group_count,
248:                                                                            const ValidityMask &partition_mask) const {
249: 	return make_uniq<WindowDistinctAggregatorGlobalState>(context, *this, group_count);
250: }
251: 
252: void WindowDistinctAggregator::Sink(WindowAggregatorState &gsink, WindowAggregatorState &lstate, DataChunk &sink_chunk,
253:                                     DataChunk &coll_chunk, idx_t input_idx, optional_ptr<SelectionVector> filter_sel,
254:                                     idx_t filtered) {
255: 	WindowAggregator::Sink(gsink, lstate, sink_chunk, coll_chunk, input_idx, filter_sel, filtered);
256: 
257: 	auto &ldstate = lstate.Cast<WindowDistinctAggregatorLocalState>();
258: 	ldstate.Sink(sink_chunk, coll_chunk, input_idx, filter_sel, filtered);
259: }
260: 
261: void WindowDistinctAggregatorLocalState::Sink(DataChunk &sink_chunk, DataChunk &coll_chunk, idx_t input_idx,
262:                                               optional_ptr<SelectionVector> filter_sel, idx_t filtered) {
263: 	//	3: 	for i ← 0 to in.size do
264: 	//	4: 		sorted[i] ← (in[i], i)
265: 	const auto count = sink_chunk.size();
266: 	payload_chunk.Reset();
267: 	auto &sorted_vec = payload_chunk.data[0];
268: 	auto sorted = FlatVector::GetData<idx_t>(sorted_vec);
269: 	std::iota(sorted, sorted + count, input_idx);
270: 
271: 	// Our arguments are being fully materialised,
272: 	// but we also need them as sort keys.
273: 	auto &child_idx = gastate.aggregator.child_idx;
274: 	for (column_t c = 0; c < child_idx.size(); ++c) {
275: 		sort_chunk.data[c].Reference(coll_chunk.data[child_idx[c]]);
276: 	}
277: 	sort_chunk.data.back().Reference(sorted_vec);
278: 	sort_chunk.SetCardinality(sink_chunk);
279: 	payload_chunk.SetCardinality(sort_chunk);
280: 
281: 	//	Apply FILTER clause, if any
282: 	if (filter_sel) {
283: 		sort_chunk.Slice(*filter_sel, filtered);
284: 		payload_chunk.Slice(*filter_sel, filtered);
285: 	}
286: 
287: 	if (!local_sort) {
288: 		local_sort = gastate.InitializeLocalSort();
289: 	}
290: 
291: 	local_sort->SinkChunk(sort_chunk, payload_chunk);
292: 
293: 	if (local_sort->SizeInBytes() > gastate.memory_per_thread) {
294: 		local_sort->Sort(*gastate.global_sort, true);
295: 	}
296: }
297: 
298: void WindowDistinctAggregatorLocalState::Finalize(WindowAggregatorGlobalState &gastate, CollectionPtr collection) {
299: 	WindowAggregatorLocalState::Finalize(gastate, collection);
300: 
301: 	//! Input data chunk, used for leaf segment aggregation
302: 	leaves.Initialize(Allocator::DefaultAllocator(), cursor->chunk.GetTypes());
303: 	sel.Initialize();
304: }
305: 
306: void WindowDistinctAggregatorLocalState::ExecuteTask() {
307: 	auto &global_sort = *gastate.global_sort;
308: 	switch (stage) {
309: 	case PartitionSortStage::SCAN:
310: 		global_sort.AddLocalState(*gastate.local_sorts[block_idx]);
311: 		break;
312: 	case PartitionSortStage::MERGE: {
313: 		MergeSorter merge_sorter(global_sort, global_sort.buffer_manager);
314: 		merge_sorter.PerformInMergeRound();
315: 		break;
316: 	}
317: 	case PartitionSortStage::SORTED:
318: 		Sorted();
319: 		break;
320: 	default:
321: 		break;
322: 	}
323: 
324: 	++gastate.tasks_completed;
325: }
326: 
327: void WindowDistinctAggregatorGlobalState::MeasurePayloadBlocks() {
328: 	const auto &blocks = global_sort->sorted_blocks[0]->payload_data->data_blocks;
329: 	idx_t count = 0;
330: 	for (const auto &block : blocks) {
331: 		block_starts.emplace_back(count);
332: 		count += block->count;
333: 	}
334: 	block_starts.emplace_back(count);
335: }
336: 
337: bool WindowDistinctAggregatorGlobalState::TryPrepareNextStage(WindowDistinctAggregatorLocalState &lstate) {
338: 	lock_guard<mutex> stage_guard(lock);
339: 
340: 	switch (stage.load()) {
341: 	case PartitionSortStage::INIT:
342: 		//	5: Sort sorted lexicographically increasing
343: 		total_tasks = local_sorts.size();
344: 		tasks_assigned = 0;
345: 		tasks_completed = 0;
346: 		lstate.stage = stage = PartitionSortStage::SCAN;
347: 		lstate.block_idx = tasks_assigned++;
348: 		return true;
349: 	case PartitionSortStage::SCAN:
350: 		// Process all the local sorts
351: 		if (tasks_assigned < total_tasks) {
352: 			lstate.stage = PartitionSortStage::SCAN;
353: 			lstate.block_idx = tasks_assigned++;
354: 			return true;
355: 		} else if (tasks_completed < tasks_assigned) {
356: 			return false;
357: 		}
358: 		global_sort->PrepareMergePhase();
359: 		if (!(global_sort->sorted_blocks.size() / 2)) {
360: 			if (global_sort->sorted_blocks.empty()) {
361: 				lstate.stage = stage = PartitionSortStage::FINISHED;
362: 				return true;
363: 			}
364: 			MeasurePayloadBlocks();
365: 			seconds.resize(block_starts.size() - 1);
366: 			total_tasks = seconds.size();
367: 			tasks_completed = 0;
368: 			tasks_assigned = 0;
369: 			lstate.stage = stage = PartitionSortStage::SORTED;
370: 			lstate.block_idx = tasks_assigned++;
371: 			return true;
372: 		}
373: 		global_sort->InitializeMergeRound();
374: 		lstate.stage = stage = PartitionSortStage::MERGE;
375: 		total_tasks = locals;
376: 		tasks_assigned = 1;
377: 		tasks_completed = 0;
378: 		return true;
379: 	case PartitionSortStage::MERGE:
380: 		if (tasks_assigned < total_tasks) {
381: 			lstate.stage = PartitionSortStage::MERGE;
382: 			++tasks_assigned;
383: 			return true;
384: 		} else if (tasks_completed < tasks_assigned) {
385: 			return false;
386: 		}
387: 		global_sort->CompleteMergeRound(true);
388: 		if (!(global_sort->sorted_blocks.size() / 2)) {
389: 			MeasurePayloadBlocks();
390: 			seconds.resize(block_starts.size() - 1);
391: 			total_tasks = seconds.size();
392: 			tasks_completed = 0;
393: 			tasks_assigned = 0;
394: 			lstate.stage = stage = PartitionSortStage::SORTED;
395: 			lstate.block_idx = tasks_assigned++;
396: 			return true;
397: 		}
398: 		global_sort->InitializeMergeRound();
399: 		lstate.stage = PartitionSortStage::MERGE;
400: 		total_tasks = locals;
401: 		tasks_assigned = 1;
402: 		tasks_completed = 0;
403: 		return true;
404: 	case PartitionSortStage::SORTED:
405: 		if (tasks_assigned < total_tasks) {
406: 			lstate.stage = PartitionSortStage::SORTED;
407: 			lstate.block_idx = tasks_assigned++;
408: 			return true;
409: 		} else if (tasks_completed < tasks_assigned) {
410: 			lstate.stage = PartitionSortStage::FINISHED;
411: 			// Sleep while other tasks finish
412: 			return false;
413: 		}
414: 		// Last task patches the boundaries
415: 		PatchPrevIdcs();
416: 		break;
417: 	default:
418: 		break;
419: 	}
420: 
421: 	lstate.stage = stage = PartitionSortStage::FINISHED;
422: 
423: 	return true;
424: }
425: 
426: void WindowDistinctAggregator::Finalize(WindowAggregatorState &gsink, WindowAggregatorState &lstate,
427:                                         CollectionPtr collection, const FrameStats &stats) {
428: 	auto &gdsink = gsink.Cast<WindowDistinctAggregatorGlobalState>();
429: 	auto &ldstate = lstate.Cast<WindowDistinctAggregatorLocalState>();
430: 	ldstate.Finalize(gdsink, collection);
431: 
432: 	// Sort, merge and build the tree in parallel
433: 	while (gdsink.stage.load() != PartitionSortStage::FINISHED) {
434: 		if (gdsink.TryPrepareNextStage(ldstate)) {
435: 			ldstate.ExecuteTask();
436: 		} else {
437: 			std::this_thread::yield();
438: 		}
439: 	}
440: 
441: 	//	These are a parallel implementations,
442: 	//	so every thread can call them.
443: 	gdsink.zipped_tree.Build();
444: 	gdsink.merge_sort_tree.Build(ldstate);
445: 
446: 	++gdsink.finalized;
447: }
448: 
449: void WindowDistinctAggregatorLocalState::Sorted() {
450: 	using ZippedTuple = WindowDistinctAggregatorGlobalState::ZippedTuple;
451: 	auto &global_sort = gastate.global_sort;
452: 	auto &prev_idcs = gastate.zipped_tree.LowestLevel();
453: 	auto &aggregator = gastate.aggregator;
454: 	auto &scan_chunk = payload_chunk;
455: 
456: 	auto scanner = make_uniq<PayloadScanner>(*global_sort, block_idx);
457: 	const auto in_size = gastate.block_starts.at(block_idx + 1);
458: 	scanner->Scan(scan_chunk);
459: 	idx_t scan_idx = 0;
460: 
461: 	auto *input_idx = FlatVector::GetData<idx_t>(scan_chunk.data[0]);
462: 	idx_t i = 0;
463: 
464: 	SBIterator curr(*global_sort, ExpressionType::COMPARE_LESSTHAN);
465: 	SBIterator prev(*global_sort, ExpressionType::COMPARE_LESSTHAN);
466: 	auto prefix_layout = global_sort->sort_layout.GetPrefixComparisonLayout(aggregator.arg_types.size());
467: 
468: 	const auto block_begin = gastate.block_starts.at(block_idx);
469: 	if (!block_begin) {
470: 		// First block, so set up initial sentinel
471: 		i = input_idx[scan_idx++];
472: 		prev_idcs[i] = ZippedTuple(0, i);
473: 		std::get<0>(gastate.seconds[block_idx]) = i;
474: 	} else {
475: 		// Move to the to end of the previous block
476: 		// so we can record the comparison result for the first row
477: 		curr.SetIndex(block_begin - 1);
478: 		prev.SetIndex(block_begin - 1);
479: 		scan_idx = 0;
480: 		std::get<0>(gastate.seconds[block_idx]) = input_idx[scan_idx];
481: 	}
482: 
483: 	//	8:	for i ← 1 to in.size do
484: 	for (++curr; curr.GetIndex() < in_size; ++curr, ++prev) {
485: 		//	Scan second one chunk at a time
486: 		//	Note the scan is one behind the iterators
487: 		if (scan_idx >= scan_chunk.size()) {
488: 			scan_chunk.Reset();
489: 			scanner->Scan(scan_chunk);
490: 			scan_idx = 0;
491: 			input_idx = FlatVector::GetData<idx_t>(scan_chunk.data[0]);
492: 		}
493: 		auto second = i;
494: 		i = input_idx[scan_idx++];
495: 
496: 		int lt = 0;
497: 		if (prefix_layout.all_constant) {
498: 			lt = FastMemcmp(prev.entry_ptr, curr.entry_ptr, prefix_layout.comparison_size);
499: 		} else {
500: 			lt = Comparators::CompareTuple(prev.scan, curr.scan, prev.entry_ptr, curr.entry_ptr, prefix_layout,
501: 			                               prev.external);
502: 		}
503: 
504: 		//	9:	if sorted[i].first == sorted[i-1].first then
505: 		//	10:		prevIdcs[i] ← sorted[i-1].second
506: 		//	11:	else
507: 		//	12:		prevIdcs[i] ← “-”
508: 		if (!lt) {
509: 			prev_idcs[i] = ZippedTuple(second + 1, i);
510: 		} else {
511: 			prev_idcs[i] = ZippedTuple(0, i);
512: 		}
513: 	}
514: 
515: 	// Save the last value of i for patching up the block boundaries
516: 	std::get<1>(gastate.seconds[block_idx]) = i;
517: }
518: 
519: void WindowDistinctAggregatorGlobalState::PatchPrevIdcs() {
520: 	//	13:	return prevIdcs
521: 
522: 	// Patch up the indices at block boundaries
523: 	// (We don't need to patch block 0.)
524: 	auto &prev_idcs = zipped_tree.LowestLevel();
525: 	for (idx_t block_idx = 1; block_idx < seconds.size(); ++block_idx) {
526: 		// We only need to patch if the first index in the block
527: 		// was a back link to the previous block (10:)
528: 		auto i = std::get<0>(seconds.at(block_idx));
529: 		if (std::get<0>(prev_idcs[i])) {
530: 			auto second = std::get<1>(seconds.at(block_idx - 1));
531: 			prev_idcs[i] = ZippedTuple(second + 1, i);
532: 		}
533: 	}
534: }
535: 
536: bool WindowDistinctSortTree::TryNextRun(idx_t &level_idx, idx_t &run_idx) {
537: 	const auto fanout = FANOUT;
538: 
539: 	lock_guard<mutex> stage_guard(build_lock);
540: 
541: 	//	Verify we are not done
542: 	if (build_level >= tree.size()) {
543: 		return false;
544: 	}
545: 
546: 	// Finished with this level?
547: 	if (build_complete >= build_num_runs) {
548: 		auto &zipped_tree = gdastate.zipped_tree;
549: 		std::swap(tree[build_level].second, zipped_tree.tree[build_level].second);
550: 
551: 		++build_level;
552: 		if (build_level >= tree.size()) {
553: 			zipped_tree.tree.clear();
554: 			return false;
555: 		}
556: 
557: 		const auto count = LowestLevel().size();
558: 		build_run_length *= fanout;
559: 		build_num_runs = (count + build_run_length - 1) / build_run_length;
560: 		build_run = 0;
561: 		build_complete = 0;
562: 	}
563: 
564: 	// If all runs are in flight,
565: 	// yield until the next level is ready
566: 	if (build_run >= build_num_runs) {
567: 		return false;
568: 	}
569: 
570: 	level_idx = build_level;
571: 	run_idx = build_run++;
572: 
573: 	return true;
574: }
575: 
576: void WindowDistinctSortTree::Build(WindowDistinctAggregatorLocalState &ldastate) {
577: 	//	Fan in parent levels until we are at the top
578: 	//	Note that we don't build the top layer as that would just be all the data.
579: 	while (build_level.load() < tree.size()) {
580: 		idx_t level_idx;
581: 		idx_t run_idx;
582: 		if (TryNextRun(level_idx, run_idx)) {
583: 			BuildRun(level_idx, run_idx, ldastate);
584: 		} else {
585: 			std::this_thread::yield();
586: 		}
587: 	}
588: }
589: 
590: void WindowDistinctSortTree::BuildRun(idx_t level_nr, idx_t run_idx, WindowDistinctAggregatorLocalState &ldastate) {
591: 	auto &aggr = gdastate.aggr;
592: 	auto &allocator = gdastate.allocator;
593: 	auto &inputs = ldastate.cursor->chunk;
594: 	auto &levels_flat_native = gdastate.levels_flat_native;
595: 
596: 	//! Input data chunk, used for leaf segment aggregation
597: 	auto &leaves = ldastate.leaves;
598: 	auto &sel = ldastate.sel;
599: 
600: 	AggregateInputData aggr_input_data(aggr.GetFunctionData(), allocator);
601: 
602: 	//! The states to update
603: 	auto &update_v = ldastate.update_v;
604: 	auto updates = FlatVector::GetData<data_ptr_t>(update_v);
605: 
606: 	auto &source_v = ldastate.source_v;
607: 	auto sources = FlatVector::GetData<data_ptr_t>(source_v);
608: 	auto &target_v = ldastate.target_v;
609: 	auto targets = FlatVector::GetData<data_ptr_t>(target_v);
610: 
611: 	auto &zipped_tree = gdastate.zipped_tree;
612: 	auto &zipped_level = zipped_tree.tree[level_nr].first;
613: 	auto &level = tree[level_nr].first;
614: 
615: 	//	Reset the combine state
616: 	idx_t nupdate = 0;
617: 	idx_t ncombine = 0;
618: 	data_ptr_t prev_state = nullptr;
619: 	idx_t i = run_idx * build_run_length;
620: 	auto next_limit = MinValue<idx_t>(zipped_level.size(), i + build_run_length);
621: 	idx_t levels_flat_offset = level_nr * zipped_level.size() + i;
622: 	for (auto j = i; j < next_limit; ++j) {
623: 		//	Initialise the next aggregate
624: 		auto curr_state = levels_flat_native.GetStatePtr(levels_flat_offset++);
625: 
626: 		//	Update this state (if it matches)
627: 		const auto prev_idx = std::get<0>(zipped_level[j]);
628: 		level[j] = prev_idx;
629: 		if (prev_idx < i + 1) {
630: 			const auto update_idx = std::get<1>(zipped_level[j]);
631: 			if (!ldastate.cursor->RowIsVisible(update_idx)) {
632: 				// 	Flush if we have to move the cursor
633: 				//	Push the updates first so they propagate
634: 				leaves.Reference(inputs);
635: 				leaves.Slice(sel, nupdate);
636: 				aggr.function.update(leaves.data.data(), aggr_input_data, leaves.ColumnCount(), update_v, nupdate);
637: 				nupdate = 0;
638: 
639: 				//	Combine the states sequentially
640: 				aggr.function.combine(source_v, target_v, aggr_input_data, ncombine);
641: 				ncombine = 0;
642: 
643: 				// Move the update into range.
644: 				ldastate.cursor->Seek(update_idx);
645: 			}
646: 
647: 			updates[nupdate] = curr_state;
648: 			//	input_idx
649: 			sel[nupdate] = ldastate.cursor->RowOffset(update_idx);
650: 			++nupdate;
651: 		}
652: 
653: 		//	Merge the previous state (if any)
654: 		if (prev_state) {
655: 			sources[ncombine] = prev_state;
656: 			targets[ncombine] = curr_state;
657: 			++ncombine;
658: 		}
659: 		prev_state = curr_state;
660: 
661: 		//	Flush the states if one is maxed out.
662: 		if (MaxValue<idx_t>(ncombine, nupdate) >= STANDARD_VECTOR_SIZE) {
663: 			//	Push the updates first so they propagate
664: 			leaves.Reference(inputs);
665: 			leaves.Slice(sel, nupdate);
666: 			aggr.function.update(leaves.data.data(), aggr_input_data, leaves.ColumnCount(), update_v, nupdate);
667: 			nupdate = 0;
668: 
669: 			//	Combine the states sequentially
670: 			aggr.function.combine(source_v, target_v, aggr_input_data, ncombine);
671: 			ncombine = 0;
672: 		}
673: 	}
674: 
675: 	//	Flush any remaining states
676: 	if (ncombine || nupdate) {
677: 		//	Push  the updates
678: 		leaves.Reference(inputs);
679: 		leaves.Slice(sel, nupdate);
680: 		aggr.function.update(leaves.data.data(), aggr_input_data, leaves.ColumnCount(), update_v, nupdate);
681: 		nupdate = 0;
682: 
683: 		//	Combine the states sequentially
684: 		aggr.function.combine(source_v, target_v, aggr_input_data, ncombine);
685: 		ncombine = 0;
686: 	}
687: 
688: 	++build_complete;
689: }
690: 
691: void WindowDistinctAggregatorLocalState::FlushStates() {
692: 	if (!flush_count) {
693: 		return;
694: 	}
695: 
696: 	const auto &aggr = gastate.aggr;
697: 	AggregateInputData aggr_input_data(aggr.GetFunctionData(), allocator);
698: 	statel.Verify(flush_count);
699: 	aggr.function.combine(statel, statep, aggr_input_data, flush_count);
700: 
701: 	flush_count = 0;
702: }
703: 
704: void WindowDistinctAggregatorLocalState::Evaluate(const WindowDistinctAggregatorGlobalState &gdstate,
705:                                                   const DataChunk &bounds, Vector &result, idx_t count, idx_t row_idx) {
706: 	auto ldata = FlatVector::GetData<const_data_ptr_t>(statel);
707: 	auto pdata = FlatVector::GetData<data_ptr_t>(statep);
708: 
709: 	const auto &merge_sort_tree = gdstate.merge_sort_tree;
710: 	const auto &levels_flat_native = gdstate.levels_flat_native;
711: 	const auto exclude_mode = gdstate.aggregator.exclude_mode;
712: 
713: 	//	Build the finalise vector that just points to the result states
714: 	statef.Initialize(count);
715: 
716: 	WindowAggregator::EvaluateSubFrames(bounds, exclude_mode, count, row_idx, frames, [&](idx_t rid) {
717: 		auto agg_state = statef.GetStatePtr(rid);
718: 
719: 		//	TODO: Extend AggregateLowerBound to handle subframes, just like SelectNth.
720: 		const auto lower = frames[0].start;
721: 		const auto upper = frames[0].end;
722: 		merge_sort_tree.AggregateLowerBound(lower, upper, lower + 1,
723: 		                                    [&](idx_t level, const idx_t run_begin, const idx_t run_pos) {
724: 			                                    if (run_pos != run_begin) {
725: 				                                    //	Find the source aggregate
726: 				                                    // Buffer a merge of the indicated state into the current state
727: 				                                    const auto agg_idx = gdstate.levels_flat_start[level] + run_pos - 1;
728: 				                                    const auto running_agg = levels_flat_native.GetStatePtr(agg_idx);
729: 				                                    pdata[flush_count] = agg_state;
730: 				                                    ldata[flush_count++] = running_agg;
731: 				                                    if (flush_count >= STANDARD_VECTOR_SIZE) {
732: 					                                    FlushStates();
733: 				                                    }
734: 			                                    }
735: 		                                    });
736: 	});
737: 
738: 	//	Flush the final states
739: 	FlushStates();
740: 
741: 	//	Finalise the result aggregates and write to the result
742: 	statef.Finalize(result);
743: 	statef.Destroy();
744: }
745: 
746: unique_ptr<WindowAggregatorState> WindowDistinctAggregator::GetLocalState(const WindowAggregatorState &gstate) const {
747: 	return make_uniq<WindowDistinctAggregatorLocalState>(gstate.Cast<const WindowDistinctAggregatorGlobalState>());
748: }
749: 
750: void WindowDistinctAggregator::Evaluate(const WindowAggregatorState &gsink, WindowAggregatorState &lstate,
751:                                         const DataChunk &bounds, Vector &result, idx_t count, idx_t row_idx) const {
752: 
753: 	const auto &gdstate = gsink.Cast<WindowDistinctAggregatorGlobalState>();
754: 	auto &ldstate = lstate.Cast<WindowDistinctAggregatorLocalState>();
755: 	ldstate.Evaluate(gdstate, bounds, result, count, row_idx);
756: }
757: 
758: } // namespace duckdb
[end of src/function/window/window_distinct_aggregator.cpp]
[start of src/include/duckdb.h]
1: //===----------------------------------------------------------------------===//
2: //
3: //                         DuckDB
4: //
5: // duckdb.h
6: //
7: //
8: //===----------------------------------------------------------------------===//
9: //
10: // !!!!!!!
11: // WARNING: this file is autogenerated by scripts/generate_c_api.py, manual changes will be overwritten
12: // !!!!!!!
13: 
14: #pragma once
15: 
16: //! duplicate of duckdb/main/winapi.hpp
17: #ifndef DUCKDB_API
18: #ifdef _WIN32
19: #ifdef DUCKDB_STATIC_BUILD
20: #define DUCKDB_API
21: #else
22: #if defined(DUCKDB_BUILD_LIBRARY) && !defined(DUCKDB_BUILD_LOADABLE_EXTENSION)
23: #define DUCKDB_API __declspec(dllexport)
24: #else
25: #define DUCKDB_API __declspec(dllimport)
26: #endif
27: #endif
28: #else
29: #define DUCKDB_API
30: #endif
31: #endif
32: 
33: //! duplicate of duckdb/main/winapi.hpp
34: #ifndef DUCKDB_EXTENSION_API
35: #ifdef _WIN32
36: #ifdef DUCKDB_STATIC_BUILD
37: #define DUCKDB_EXTENSION_API
38: #else
39: #ifdef DUCKDB_BUILD_LOADABLE_EXTENSION
40: #define DUCKDB_EXTENSION_API __declspec(dllexport)
41: #else
42: #define DUCKDB_EXTENSION_API
43: #endif
44: #endif
45: #else
46: #define DUCKDB_EXTENSION_API __attribute__((visibility("default")))
47: #endif
48: #endif
49: 
50: #include <stdbool.h>
51: #include <stdint.h>
52: #include <stddef.h>
53: 
54: #ifdef __cplusplus
55: extern "C" {
56: #endif
57: 
58: //===--------------------------------------------------------------------===//
59: // Enums
60: //===--------------------------------------------------------------------===//
61: // WARNING: the numbers of these enums should not be changed, as changing the numbers breaks ABI compatibility
62: // Always add enums at the END of the enum
63: //! An enum over DuckDB's internal types.
64: typedef enum DUCKDB_TYPE {
65: 	DUCKDB_TYPE_INVALID = 0,
66: 	// bool
67: 	DUCKDB_TYPE_BOOLEAN = 1,
68: 	// int8_t
69: 	DUCKDB_TYPE_TINYINT = 2,
70: 	// int16_t
71: 	DUCKDB_TYPE_SMALLINT = 3,
72: 	// int32_t
73: 	DUCKDB_TYPE_INTEGER = 4,
74: 	// int64_t
75: 	DUCKDB_TYPE_BIGINT = 5,
76: 	// uint8_t
77: 	DUCKDB_TYPE_UTINYINT = 6,
78: 	// uint16_t
79: 	DUCKDB_TYPE_USMALLINT = 7,
80: 	// uint32_t
81: 	DUCKDB_TYPE_UINTEGER = 8,
82: 	// uint64_t
83: 	DUCKDB_TYPE_UBIGINT = 9,
84: 	// float
85: 	DUCKDB_TYPE_FLOAT = 10,
86: 	// double
87: 	DUCKDB_TYPE_DOUBLE = 11,
88: 	// duckdb_timestamp (microseconds)
89: 	DUCKDB_TYPE_TIMESTAMP = 12,
90: 	// duckdb_date
91: 	DUCKDB_TYPE_DATE = 13,
92: 	// duckdb_time
93: 	DUCKDB_TYPE_TIME = 14,
94: 	// duckdb_interval
95: 	DUCKDB_TYPE_INTERVAL = 15,
96: 	// duckdb_hugeint
97: 	DUCKDB_TYPE_HUGEINT = 16,
98: 	// duckdb_uhugeint
99: 	DUCKDB_TYPE_UHUGEINT = 32,
100: 	// const char*
101: 	DUCKDB_TYPE_VARCHAR = 17,
102: 	// duckdb_blob
103: 	DUCKDB_TYPE_BLOB = 18,
104: 	// duckdb_decimal
105: 	DUCKDB_TYPE_DECIMAL = 19,
106: 	// duckdb_timestamp_s (seconds)
107: 	DUCKDB_TYPE_TIMESTAMP_S = 20,
108: 	// duckdb_timestamp_ms (milliseconds)
109: 	DUCKDB_TYPE_TIMESTAMP_MS = 21,
110: 	// duckdb_timestamp_ns (nanoseconds)
111: 	DUCKDB_TYPE_TIMESTAMP_NS = 22,
112: 	// enum type, only useful as logical type
113: 	DUCKDB_TYPE_ENUM = 23,
114: 	// list type, only useful as logical type
115: 	DUCKDB_TYPE_LIST = 24,
116: 	// struct type, only useful as logical type
117: 	DUCKDB_TYPE_STRUCT = 25,
118: 	// map type, only useful as logical type
119: 	DUCKDB_TYPE_MAP = 26,
120: 	// duckdb_array, only useful as logical type
121: 	DUCKDB_TYPE_ARRAY = 33,
122: 	// duckdb_hugeint
123: 	DUCKDB_TYPE_UUID = 27,
124: 	// union type, only useful as logical type
125: 	DUCKDB_TYPE_UNION = 28,
126: 	// duckdb_bit
127: 	DUCKDB_TYPE_BIT = 29,
128: 	// duckdb_time_tz
129: 	DUCKDB_TYPE_TIME_TZ = 30,
130: 	// duckdb_timestamp (microseconds)
131: 	DUCKDB_TYPE_TIMESTAMP_TZ = 31,
132: 	// ANY type
133: 	DUCKDB_TYPE_ANY = 34,
134: 	// duckdb_varint
135: 	DUCKDB_TYPE_VARINT = 35,
136: 	// SQLNULL type
137: 	DUCKDB_TYPE_SQLNULL = 36,
138: } duckdb_type;
139: //! An enum over the returned state of different functions.
140: typedef enum duckdb_state { DuckDBSuccess = 0, DuckDBError = 1 } duckdb_state;
141: //! An enum over the pending state of a pending query result.
142: typedef enum duckdb_pending_state {
143: 	DUCKDB_PENDING_RESULT_READY = 0,
144: 	DUCKDB_PENDING_RESULT_NOT_READY = 1,
145: 	DUCKDB_PENDING_ERROR = 2,
146: 	DUCKDB_PENDING_NO_TASKS_AVAILABLE = 3
147: } duckdb_pending_state;
148: //! An enum over DuckDB's different result types.
149: typedef enum duckdb_result_type {
150: 	DUCKDB_RESULT_TYPE_INVALID = 0,
151: 	DUCKDB_RESULT_TYPE_CHANGED_ROWS = 1,
152: 	DUCKDB_RESULT_TYPE_NOTHING = 2,
153: 	DUCKDB_RESULT_TYPE_QUERY_RESULT = 3,
154: } duckdb_result_type;
155: //! An enum over DuckDB's different statement types.
156: typedef enum duckdb_statement_type {
157: 	DUCKDB_STATEMENT_TYPE_INVALID = 0,
158: 	DUCKDB_STATEMENT_TYPE_SELECT = 1,
159: 	DUCKDB_STATEMENT_TYPE_INSERT = 2,
160: 	DUCKDB_STATEMENT_TYPE_UPDATE = 3,
161: 	DUCKDB_STATEMENT_TYPE_EXPLAIN = 4,
162: 	DUCKDB_STATEMENT_TYPE_DELETE = 5,
163: 	DUCKDB_STATEMENT_TYPE_PREPARE = 6,
164: 	DUCKDB_STATEMENT_TYPE_CREATE = 7,
165: 	DUCKDB_STATEMENT_TYPE_EXECUTE = 8,
166: 	DUCKDB_STATEMENT_TYPE_ALTER = 9,
167: 	DUCKDB_STATEMENT_TYPE_TRANSACTION = 10,
168: 	DUCKDB_STATEMENT_TYPE_COPY = 11,
169: 	DUCKDB_STATEMENT_TYPE_ANALYZE = 12,
170: 	DUCKDB_STATEMENT_TYPE_VARIABLE_SET = 13,
171: 	DUCKDB_STATEMENT_TYPE_CREATE_FUNC = 14,
172: 	DUCKDB_STATEMENT_TYPE_DROP = 15,
173: 	DUCKDB_STATEMENT_TYPE_EXPORT = 16,
174: 	DUCKDB_STATEMENT_TYPE_PRAGMA = 17,
175: 	DUCKDB_STATEMENT_TYPE_VACUUM = 18,
176: 	DUCKDB_STATEMENT_TYPE_CALL = 19,
177: 	DUCKDB_STATEMENT_TYPE_SET = 20,
178: 	DUCKDB_STATEMENT_TYPE_LOAD = 21,
179: 	DUCKDB_STATEMENT_TYPE_RELATION = 22,
180: 	DUCKDB_STATEMENT_TYPE_EXTENSION = 23,
181: 	DUCKDB_STATEMENT_TYPE_LOGICAL_PLAN = 24,
182: 	DUCKDB_STATEMENT_TYPE_ATTACH = 25,
183: 	DUCKDB_STATEMENT_TYPE_DETACH = 26,
184: 	DUCKDB_STATEMENT_TYPE_MULTI = 27,
185: } duckdb_statement_type;
186: //! An enum over DuckDB's different result types.
187: typedef enum duckdb_error_type {
188: 	DUCKDB_ERROR_INVALID = 0,
189: 	DUCKDB_ERROR_OUT_OF_RANGE = 1,
190: 	DUCKDB_ERROR_CONVERSION = 2,
191: 	DUCKDB_ERROR_UNKNOWN_TYPE = 3,
192: 	DUCKDB_ERROR_DECIMAL = 4,
193: 	DUCKDB_ERROR_MISMATCH_TYPE = 5,
194: 	DUCKDB_ERROR_DIVIDE_BY_ZERO = 6,
195: 	DUCKDB_ERROR_OBJECT_SIZE = 7,
196: 	DUCKDB_ERROR_INVALID_TYPE = 8,
197: 	DUCKDB_ERROR_SERIALIZATION = 9,
198: 	DUCKDB_ERROR_TRANSACTION = 10,
199: 	DUCKDB_ERROR_NOT_IMPLEMENTED = 11,
200: 	DUCKDB_ERROR_EXPRESSION = 12,
201: 	DUCKDB_ERROR_CATALOG = 13,
202: 	DUCKDB_ERROR_PARSER = 14,
203: 	DUCKDB_ERROR_PLANNER = 15,
204: 	DUCKDB_ERROR_SCHEDULER = 16,
205: 	DUCKDB_ERROR_EXECUTOR = 17,
206: 	DUCKDB_ERROR_CONSTRAINT = 18,
207: 	DUCKDB_ERROR_INDEX = 19,
208: 	DUCKDB_ERROR_STAT = 20,
209: 	DUCKDB_ERROR_CONNECTION = 21,
210: 	DUCKDB_ERROR_SYNTAX = 22,
211: 	DUCKDB_ERROR_SETTINGS = 23,
212: 	DUCKDB_ERROR_BINDER = 24,
213: 	DUCKDB_ERROR_NETWORK = 25,
214: 	DUCKDB_ERROR_OPTIMIZER = 26,
215: 	DUCKDB_ERROR_NULL_POINTER = 27,
216: 	DUCKDB_ERROR_IO = 28,
217: 	DUCKDB_ERROR_INTERRUPT = 29,
218: 	DUCKDB_ERROR_FATAL = 30,
219: 	DUCKDB_ERROR_INTERNAL = 31,
220: 	DUCKDB_ERROR_INVALID_INPUT = 32,
221: 	DUCKDB_ERROR_OUT_OF_MEMORY = 33,
222: 	DUCKDB_ERROR_PERMISSION = 34,
223: 	DUCKDB_ERROR_PARAMETER_NOT_RESOLVED = 35,
224: 	DUCKDB_ERROR_PARAMETER_NOT_ALLOWED = 36,
225: 	DUCKDB_ERROR_DEPENDENCY = 37,
226: 	DUCKDB_ERROR_HTTP = 38,
227: 	DUCKDB_ERROR_MISSING_EXTENSION = 39,
228: 	DUCKDB_ERROR_AUTOLOAD = 40,
229: 	DUCKDB_ERROR_SEQUENCE = 41,
230: 	DUCKDB_INVALID_CONFIGURATION = 42
231: } duckdb_error_type;
232: //! An enum over DuckDB's different cast modes.
233: typedef enum duckdb_cast_mode { DUCKDB_CAST_NORMAL = 0, DUCKDB_CAST_TRY = 1 } duckdb_cast_mode;
234: 
235: //===--------------------------------------------------------------------===//
236: // General type definitions
237: //===--------------------------------------------------------------------===//
238: 
239: //! DuckDB's index type.
240: typedef uint64_t idx_t;
241: 
242: //! The callback that will be called to destroy data, e.g.,
243: //! bind data (if any), init data (if any), extra data for replacement scans (if any)
244: typedef void (*duckdb_delete_callback_t)(void *data);
245: 
246: //! Used for threading, contains a task state. Must be destroyed with `duckdb_destroy_state`.
247: typedef void *duckdb_task_state;
248: 
249: //===--------------------------------------------------------------------===//
250: // Types (no explicit freeing)
251: //===--------------------------------------------------------------------===//
252: 
253: //! Days are stored as days since 1970-01-01
254: //! Use the duckdb_from_date/duckdb_to_date function to extract individual information
255: typedef struct {
256: 	int32_t days;
257: } duckdb_date;
258: typedef struct {
259: 	int32_t year;
260: 	int8_t month;
261: 	int8_t day;
262: } duckdb_date_struct;
263: 
264: //! Time is stored as microseconds since 00:00:00
265: //! Use the duckdb_from_time/duckdb_to_time function to extract individual information
266: typedef struct {
267: 	int64_t micros;
268: } duckdb_time;
269: typedef struct {
270: 	int8_t hour;
271: 	int8_t min;
272: 	int8_t sec;
273: 	int32_t micros;
274: } duckdb_time_struct;
275: 
276: //! TIME_TZ is stored as 40 bits for int64_t micros, and 24 bits for int32_t offset
277: typedef struct {
278: 	uint64_t bits;
279: } duckdb_time_tz;
280: typedef struct {
281: 	duckdb_time_struct time;
282: 	int32_t offset;
283: } duckdb_time_tz_struct;
284: 
285: //! TIMESTAMP values are stored as microseconds since 1970-01-01.
286: //! Use the duckdb_from_timestamp and duckdb_to_timestamp functions to extract individual information.
287: typedef struct {
288: 	int64_t micros;
289: } duckdb_timestamp;
290: 
291: //! TIMESTAMP_S values are stored as seconds since 1970-01-01.
292: typedef struct {
293: 	int64_t seconds;
294: } duckdb_timestamp_s;
295: 
296: //! TIMESTAMP_MS values are stored as milliseconds since 1970-01-01.
297: typedef struct {
298: 	int64_t millis;
299: } duckdb_timestamp_ms;
300: 
301: //! TIMESTAMP_NS values are stored as nanoseconds since 1970-01-01.
302: typedef struct {
303: 	int64_t nanos;
304: } duckdb_timestamp_ns;
305: 
306: typedef struct {
307: 	duckdb_date_struct date;
308: 	duckdb_time_struct time;
309: } duckdb_timestamp_struct;
310: 
311: typedef struct {
312: 	int32_t months;
313: 	int32_t days;
314: 	int64_t micros;
315: } duckdb_interval;
316: 
317: //! Hugeints are composed of a (lower, upper) component
318: //! The value of the hugeint is upper * 2^64 + lower
319: //! For easy usage, the functions duckdb_hugeint_to_double/duckdb_double_to_hugeint are recommended
320: typedef struct {
321: 	uint64_t lower;
322: 	int64_t upper;
323: } duckdb_hugeint;
324: typedef struct {
325: 	uint64_t lower;
326: 	uint64_t upper;
327: } duckdb_uhugeint;
328: 
329: //! Decimals are composed of a width and a scale, and are stored in a hugeint
330: typedef struct {
331: 	uint8_t width;
332: 	uint8_t scale;
333: 	duckdb_hugeint value;
334: } duckdb_decimal;
335: 
336: //! A type holding information about the query execution progress
337: typedef struct {
338: 	double percentage;
339: 	uint64_t rows_processed;
340: 	uint64_t total_rows_to_process;
341: } duckdb_query_progress_type;
342: 
343: //! The internal representation of a VARCHAR (string_t). If the VARCHAR does not
344: //! exceed 12 characters, then we inline it. Otherwise, we inline a prefix for faster
345: //! string comparisons and store a pointer to the remaining characters. This is a non-
346: //! owning structure, i.e., it does not have to be freed.
347: typedef struct {
348: 	union {
349: 		struct {
350: 			uint32_t length;
351: 			char prefix[4];
352: 			char *ptr;
353: 		} pointer;
354: 		struct {
355: 			uint32_t length;
356: 			char inlined[12];
357: 		} inlined;
358: 	} value;
359: } duckdb_string_t;
360: 
361: //! The internal representation of a list metadata entry contains the list's offset in
362: //! the child vector, and its length. The parent vector holds these metadata entries,
363: //! whereas the child vector holds the data
364: typedef struct {
365: 	uint64_t offset;
366: 	uint64_t length;
367: } duckdb_list_entry;
368: 
369: //! A column consists of a pointer to its internal data. Don't operate on this type directly.
370: //! Instead, use functions such as duckdb_column_data, duckdb_nullmask_data,
371: //! duckdb_column_type, and duckdb_column_name, which take the result and the column index
372: //! as their parameters
373: typedef struct {
374: 	// deprecated, use duckdb_column_data
375: 	void *deprecated_data;
376: 	// deprecated, use duckdb_nullmask_data
377: 	bool *deprecated_nullmask;
378: 	// deprecated, use duckdb_column_type
379: 	duckdb_type deprecated_type;
380: 	// deprecated, use duckdb_column_name
381: 	char *deprecated_name;
382: 	void *internal_data;
383: } duckdb_column;
384: 
385: //! A vector to a specified column in a data chunk. Lives as long as the
386: //! data chunk lives, i.e., must not be destroyed.
387: typedef struct _duckdb_vector {
388: 	void *internal_ptr;
389: } * duckdb_vector;
390: 
391: //===--------------------------------------------------------------------===//
392: // Types (explicit freeing/destroying)
393: //===--------------------------------------------------------------------===//
394: 
395: //! Strings are composed of a char pointer and a size. You must free string.data
396: //! with `duckdb_free`.
397: typedef struct {
398: 	char *data;
399: 	idx_t size;
400: } duckdb_string;
401: 
402: //! BLOBs are composed of a byte pointer and a size. You must free blob.data
403: //! with `duckdb_free`.
404: typedef struct {
405: 	void *data;
406: 	idx_t size;
407: } duckdb_blob;
408: 
409: //! BITs are composed of a byte pointer and a size.
410: //! BIT byte data has 0 to 7 bits of padding.
411: //! The first byte contains the number of padding bits.
412: //! This number of bits of the second byte are set to 1, starting from the MSB.
413: //! You must free `data` with `duckdb_free`.
414: typedef struct {
415: 	uint8_t *data;
416: 	idx_t size;
417: } duckdb_bit;
418: 
419: //! VARINTs are composed of a byte pointer, a size, and an is_negative bool.
420: //! The absolute value of the number is stored in `data` in little endian format.
421: //! You must free `data` with `duckdb_free`.
422: typedef struct {
423: 	uint8_t *data;
424: 	idx_t size;
425: 	bool is_negative;
426: } duckdb_varint;
427: 
428: //! A query result consists of a pointer to its internal data.
429: //! Must be freed with 'duckdb_destroy_result'.
430: typedef struct {
431: 	// deprecated, use duckdb_column_count
432: 	idx_t deprecated_column_count;
433: 	// deprecated, use duckdb_row_count
434: 	idx_t deprecated_row_count;
435: 	// deprecated, use duckdb_rows_changed
436: 	idx_t deprecated_rows_changed;
437: 	// deprecated, use duckdb_column_*-family of functions
438: 	duckdb_column *deprecated_columns;
439: 	// deprecated, use duckdb_result_error
440: 	char *deprecated_error_message;
441: 	void *internal_data;
442: } duckdb_result;
443: 
444: //! A database instance cache object. Must be destroyed with `duckdb_destroy_instance_cache`.
445: typedef struct _duckdb_instance_cache {
446: 	void *internal_ptr;
447: } * duckdb_instance_cache;
448: 
449: //! A database object. Must be closed with `duckdb_close`.
450: typedef struct _duckdb_database {
451: 	void *internal_ptr;
452: } * duckdb_database;
453: 
454: //! A connection to a duckdb database. Must be closed with `duckdb_disconnect`.
455: typedef struct _duckdb_connection {
456: 	void *internal_ptr;
457: } * duckdb_connection;
458: 
459: //! A prepared statement is a parameterized query that allows you to bind parameters to it.
460: //! Must be destroyed with `duckdb_destroy_prepare`.
461: typedef struct _duckdb_prepared_statement {
462: 	void *internal_ptr;
463: } * duckdb_prepared_statement;
464: 
465: //! Extracted statements. Must be destroyed with `duckdb_destroy_extracted`.
466: typedef struct _duckdb_extracted_statements {
467: 	void *internal_ptr;
468: } * duckdb_extracted_statements;
469: 
470: //! The pending result represents an intermediate structure for a query that is not yet fully executed.
471: //! Must be destroyed with `duckdb_destroy_pending`.
472: typedef struct _duckdb_pending_result {
473: 	void *internal_ptr;
474: } * duckdb_pending_result;
475: 
476: //! The appender enables fast data loading into DuckDB.
477: //! Must be destroyed with `duckdb_appender_destroy`.
478: typedef struct _duckdb_appender {
479: 	void *internal_ptr;
480: } * duckdb_appender;
481: 
482: //! The table description allows querying info about the table.
483: //! Must be destroyed with `duckdb_table_description_destroy`.
484: typedef struct _duckdb_table_description {
485: 	void *internal_ptr;
486: } * duckdb_table_description;
487: 
488: //! Can be used to provide start-up options for the DuckDB instance.
489: //! Must be destroyed with `duckdb_destroy_config`.
490: typedef struct _duckdb_config {
491: 	void *internal_ptr;
492: } * duckdb_config;
493: 
494: //! Holds an internal logical type.
495: //! Must be destroyed with `duckdb_destroy_logical_type`.
496: typedef struct _duckdb_logical_type {
497: 	void *internal_ptr;
498: } * duckdb_logical_type;
499: 
500: //! Holds extra information used when registering a custom logical type.
501: //! Reserved for future use.
502: typedef struct _duckdb_create_type_info {
503: 	void *internal_ptr;
504: } * duckdb_create_type_info;
505: 
506: //! Contains a data chunk from a duckdb_result.
507: //! Must be destroyed with `duckdb_destroy_data_chunk`.
508: typedef struct _duckdb_data_chunk {
509: 	void *internal_ptr;
510: } * duckdb_data_chunk;
511: 
512: //! Holds a DuckDB value, which wraps a type.
513: //! Must be destroyed with `duckdb_destroy_value`.
514: typedef struct _duckdb_value {
515: 	void *internal_ptr;
516: } * duckdb_value;
517: 
518: //! Holds a recursive tree that matches the query plan.
519: typedef struct _duckdb_profiling_info {
520: 	void *internal_ptr;
521: } * duckdb_profiling_info;
522: 
523: //===--------------------------------------------------------------------===//
524: // C API Extension info
525: //===--------------------------------------------------------------------===//
526: //! Holds state during the C API extension intialization process
527: typedef struct _duckdb_extension_info {
528: 	void *internal_ptr;
529: } * duckdb_extension_info;
530: 
531: //===--------------------------------------------------------------------===//
532: // Function types
533: //===--------------------------------------------------------------------===//
534: //! Additional function info. When setting this info, it is necessary to pass a destroy-callback function.
535: typedef struct _duckdb_function_info {
536: 	void *internal_ptr;
537: } * duckdb_function_info;
538: 
539: //===--------------------------------------------------------------------===//
540: // Scalar function types
541: //===--------------------------------------------------------------------===//
542: //! A scalar function. Must be destroyed with `duckdb_destroy_scalar_function`.
543: typedef struct _duckdb_scalar_function {
544: 	void *internal_ptr;
545: } * duckdb_scalar_function;
546: 
547: //! A scalar function set. Must be destroyed with `duckdb_destroy_scalar_function_set`.
548: typedef struct _duckdb_scalar_function_set {
549: 	void *internal_ptr;
550: } * duckdb_scalar_function_set;
551: 
552: //! The main function of the scalar function.
553: typedef void (*duckdb_scalar_function_t)(duckdb_function_info info, duckdb_data_chunk input, duckdb_vector output);
554: 
555: //===--------------------------------------------------------------------===//
556: // Aggregate function types
557: //===--------------------------------------------------------------------===//
558: //! An aggregate function. Must be destroyed with `duckdb_destroy_aggregate_function`.
559: typedef struct _duckdb_aggregate_function {
560: 	void *internal_ptr;
561: } * duckdb_aggregate_function;
562: 
563: //! A aggregate function set. Must be destroyed with `duckdb_destroy_aggregate_function_set`.
564: typedef struct _duckdb_aggregate_function_set {
565: 	void *internal_ptr;
566: } * duckdb_aggregate_function_set;
567: 
568: //! Aggregate state
569: typedef struct _duckdb_aggregate_state {
570: 	void *internal_ptr;
571: } * duckdb_aggregate_state;
572: 
573: //! Returns the aggregate state size
574: typedef idx_t (*duckdb_aggregate_state_size)(duckdb_function_info info);
575: //! Initialize the aggregate state
576: typedef void (*duckdb_aggregate_init_t)(duckdb_function_info info, duckdb_aggregate_state state);
577: //! Destroy aggregate state (optional)
578: typedef void (*duckdb_aggregate_destroy_t)(duckdb_aggregate_state *states, idx_t count);
579: //! Update a set of aggregate states with new values
580: typedef void (*duckdb_aggregate_update_t)(duckdb_function_info info, duckdb_data_chunk input,
581:                                           duckdb_aggregate_state *states);
582: //! Combine aggregate states
583: typedef void (*duckdb_aggregate_combine_t)(duckdb_function_info info, duckdb_aggregate_state *source,
584:                                            duckdb_aggregate_state *target, idx_t count);
585: //! Finalize aggregate states into a result vector
586: typedef void (*duckdb_aggregate_finalize_t)(duckdb_function_info info, duckdb_aggregate_state *source,
587:                                             duckdb_vector result, idx_t count, idx_t offset);
588: 
589: //===--------------------------------------------------------------------===//
590: // Table function types
591: //===--------------------------------------------------------------------===//
592: 
593: //! A table function. Must be destroyed with `duckdb_destroy_table_function`.
594: typedef struct _duckdb_table_function {
595: 	void *internal_ptr;
596: } * duckdb_table_function;
597: 
598: //! The bind info of the function. When setting this info, it is necessary to pass a destroy-callback function.
599: typedef struct _duckdb_bind_info {
600: 	void *internal_ptr;
601: } * duckdb_bind_info;
602: 
603: //! Additional function init info. When setting this info, it is necessary to pass a destroy-callback function.
604: typedef struct _duckdb_init_info {
605: 	void *internal_ptr;
606: } * duckdb_init_info;
607: 
608: //! The bind function of the table function.
609: typedef void (*duckdb_table_function_bind_t)(duckdb_bind_info info);
610: 
611: //! The (possibly thread-local) init function of the table function.
612: typedef void (*duckdb_table_function_init_t)(duckdb_init_info info);
613: 
614: //! The main function of the table function.
615: typedef void (*duckdb_table_function_t)(duckdb_function_info info, duckdb_data_chunk output);
616: 
617: //===--------------------------------------------------------------------===//
618: // Cast types
619: //===--------------------------------------------------------------------===//
620: 
621: //! A cast function. Must be destroyed with `duckdb_destroy_cast_function`.
622: typedef struct _duckdb_cast_function {
623: 	void *internal_ptr;
624: } * duckdb_cast_function;
625: 
626: typedef bool (*duckdb_cast_function_t)(duckdb_function_info info, idx_t count, duckdb_vector input,
627:                                        duckdb_vector output);
628: 
629: //===--------------------------------------------------------------------===//
630: // Replacement scan types
631: //===--------------------------------------------------------------------===//
632: 
633: //! Additional replacement scan info. When setting this info, it is necessary to pass a destroy-callback function.
634: typedef struct _duckdb_replacement_scan_info {
635: 	void *internal_ptr;
636: } * duckdb_replacement_scan_info;
637: 
638: //! A replacement scan function that can be added to a database.
639: typedef void (*duckdb_replacement_callback_t)(duckdb_replacement_scan_info info, const char *table_name, void *data);
640: 
641: //===--------------------------------------------------------------------===//
642: // Arrow-related types
643: //===--------------------------------------------------------------------===//
644: 
645: //! Holds an arrow query result. Must be destroyed with `duckdb_destroy_arrow`.
646: typedef struct _duckdb_arrow {
647: 	void *internal_ptr;
648: } * duckdb_arrow;
649: 
650: //! Holds an arrow array stream. Must be destroyed with `duckdb_destroy_arrow_stream`.
651: typedef struct _duckdb_arrow_stream {
652: 	void *internal_ptr;
653: } * duckdb_arrow_stream;
654: 
655: //! Holds an arrow schema. Remember to release the respective ArrowSchema object.
656: typedef struct _duckdb_arrow_schema {
657: 	void *internal_ptr;
658: } * duckdb_arrow_schema;
659: 
660: //! Holds an arrow array. Remember to release the respective ArrowArray object.
661: typedef struct _duckdb_arrow_array {
662: 	void *internal_ptr;
663: } * duckdb_arrow_array;
664: 
665: //===--------------------------------------------------------------------===//
666: // DuckDB extension access
667: //===--------------------------------------------------------------------===//
668: //! Passed to C API extension as parameter to the entrypoint
669: struct duckdb_extension_access {
670: 	//! Indicate that an error has occurred
671: 	void (*set_error)(duckdb_extension_info info, const char *error);
672: 	//! Fetch the database from duckdb to register extensions to
673: 	duckdb_database *(*get_database)(duckdb_extension_info info);
674: 	//! Fetch the API
675: 	const void *(*get_api)(duckdb_extension_info info, const char *version);
676: };
677: 
678: #ifndef DUCKDB_API_EXCLUDE_FUNCTIONS
679: 
680: //===--------------------------------------------------------------------===//
681: // Functions
682: //===--------------------------------------------------------------------===//
683: 
684: //===--------------------------------------------------------------------===//
685: // Open Connect
686: //===--------------------------------------------------------------------===//
687: 
688: /*!
689: Creates a new database instance cache.
690: The instance cache is necessary if a client/program (re)opens multiple databases to the same file within the same
691: process. Must be destroyed with 'duckdb_destroy_instance_cache'.
692: 
693: * @return The database instance cache.
694: */
695: DUCKDB_API duckdb_instance_cache duckdb_create_instance_cache();
696: 
697: /*!
698: Creates a new database instance in the instance cache, or retrieves an existing database instance.
699: Must be closed with 'duckdb_close'.
700: 
701: * @param instance_cache The instance cache in which to create the database, or from which to take the database.
702: * @param path Path to the database file on disk. Both `nullptr` and `:memory:` open or retrieve an in-memory database.
703: * @param out_database The resulting cached database.
704: * @param config (Optional) configuration used to create the database.
705: * @param out_error If set and the function returns `DuckDBError`, this contains the error message.
706: Note that the error message must be freed using `duckdb_free`.
707: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
708: */
709: DUCKDB_API duckdb_state duckdb_get_or_create_from_cache(duckdb_instance_cache instance_cache, const char *path,
710:                                                         duckdb_database *out_database, duckdb_config config,
711:                                                         char **out_error);
712: 
713: /*!
714: Destroys an existing database instance cache and de-allocates its memory.
715: 
716: * @param instance_cache The instance cache to destroy.
717: */
718: DUCKDB_API void duckdb_destroy_instance_cache(duckdb_instance_cache *instance_cache);
719: 
720: /*!
721: Creates a new database or opens an existing database file stored at the given path.
722: If no path is given a new in-memory database is created instead.
723: The database must be closed with 'duckdb_close'.
724: 
725: * @param path Path to the database file on disk. Both `nullptr` and `:memory:` open an in-memory database.
726: * @param out_database The result database object.
727: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
728: */
729: DUCKDB_API duckdb_state duckdb_open(const char *path, duckdb_database *out_database);
730: 
731: /*!
732: Extended version of duckdb_open. Creates a new database or opens an existing database file stored at the given path.
733: The database must be closed with 'duckdb_close'.
734: 
735: * @param path Path to the database file on disk. Both `nullptr` and `:memory:` open an in-memory database.
736: * @param out_database The result database object.
737: * @param config (Optional) configuration used to start up the database.
738: * @param out_error If set and the function returns `DuckDBError`, this contains the error message.
739: Note that the error message must be freed using `duckdb_free`.
740: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
741: */
742: DUCKDB_API duckdb_state duckdb_open_ext(const char *path, duckdb_database *out_database, duckdb_config config,
743:                                         char **out_error);
744: 
745: /*!
746: Closes the specified database and de-allocates all memory allocated for that database.
747: This should be called after you are done with any database allocated through `duckdb_open` or `duckdb_open_ext`.
748: Note that failing to call `duckdb_close` (in case of e.g. a program crash) will not cause data corruption.
749: Still, it is recommended to always correctly close a database object after you are done with it.
750: 
751: * @param database The database object to shut down.
752: */
753: DUCKDB_API void duckdb_close(duckdb_database *database);
754: 
755: /*!
756: Opens a connection to a database. Connections are required to query the database, and store transactional state
757: associated with the connection.
758: The instantiated connection should be closed using 'duckdb_disconnect'.
759: 
760: * @param database The database file to connect to.
761: * @param out_connection The result connection object.
762: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
763: */
764: DUCKDB_API duckdb_state duckdb_connect(duckdb_database database, duckdb_connection *out_connection);
765: 
766: /*!
767: Interrupt running query
768: 
769: * @param connection The connection to interrupt
770: */
771: DUCKDB_API void duckdb_interrupt(duckdb_connection connection);
772: 
773: /*!
774: Get progress of the running query
775: 
776: * @param connection The working connection
777: * @return -1 if no progress or a percentage of the progress
778: */
779: DUCKDB_API duckdb_query_progress_type duckdb_query_progress(duckdb_connection connection);
780: 
781: /*!
782: Closes the specified connection and de-allocates all memory allocated for that connection.
783: 
784: * @param connection The connection to close.
785: */
786: DUCKDB_API void duckdb_disconnect(duckdb_connection *connection);
787: 
788: /*!
789: Returns the version of the linked DuckDB, with a version postfix for dev versions
790: 
791: Usually used for developing C extensions that must return this for a compatibility check.
792: */
793: DUCKDB_API const char *duckdb_library_version();
794: 
795: //===--------------------------------------------------------------------===//
796: // Configuration
797: //===--------------------------------------------------------------------===//
798: 
799: /*!
800: Initializes an empty configuration object that can be used to provide start-up options for the DuckDB instance
801: through `duckdb_open_ext`.
802: The duckdb_config must be destroyed using 'duckdb_destroy_config'
803: 
804: This will always succeed unless there is a malloc failure.
805: 
806: Note that `duckdb_destroy_config` should always be called on the resulting config, even if the function returns
807: `DuckDBError`.
808: 
809: * @param out_config The result configuration object.
810: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
811: */
812: DUCKDB_API duckdb_state duckdb_create_config(duckdb_config *out_config);
813: 
814: /*!
815: This returns the total amount of configuration options available for usage with `duckdb_get_config_flag`.
816: 
817: This should not be called in a loop as it internally loops over all the options.
818: 
819: * @return The amount of config options available.
820: */
821: DUCKDB_API size_t duckdb_config_count();
822: 
823: /*!
824: Obtains a human-readable name and description of a specific configuration option. This can be used to e.g.
825: display configuration options. This will succeed unless `index` is out of range (i.e. `>= duckdb_config_count`).
826: 
827: The result name or description MUST NOT be freed.
828: 
829: * @param index The index of the configuration option (between 0 and `duckdb_config_count`)
830: * @param out_name A name of the configuration flag.
831: * @param out_description A description of the configuration flag.
832: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
833: */
834: DUCKDB_API duckdb_state duckdb_get_config_flag(size_t index, const char **out_name, const char **out_description);
835: 
836: /*!
837: Sets the specified option for the specified configuration. The configuration option is indicated by name.
838: To obtain a list of config options, see `duckdb_get_config_flag`.
839: 
840: In the source code, configuration options are defined in `config.cpp`.
841: 
842: This can fail if either the name is invalid, or if the value provided for the option is invalid.
843: 
844: * @param config The configuration object to set the option on.
845: * @param name The name of the configuration flag to set.
846: * @param option The value to set the configuration flag to.
847: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
848: */
849: DUCKDB_API duckdb_state duckdb_set_config(duckdb_config config, const char *name, const char *option);
850: 
851: /*!
852: Destroys the specified configuration object and de-allocates all memory allocated for the object.
853: 
854: * @param config The configuration object to destroy.
855: */
856: DUCKDB_API void duckdb_destroy_config(duckdb_config *config);
857: 
858: //===--------------------------------------------------------------------===//
859: // Query Execution
860: //===--------------------------------------------------------------------===//
861: 
862: /*!
863: Executes a SQL query within a connection and stores the full (materialized) result in the out_result pointer.
864: If the query fails to execute, DuckDBError is returned and the error message can be retrieved by calling
865: `duckdb_result_error`.
866: 
867: Note that after running `duckdb_query`, `duckdb_destroy_result` must be called on the result object even if the
868: query fails, otherwise the error stored within the result will not be freed correctly.
869: 
870: * @param connection The connection to perform the query in.
871: * @param query The SQL query to run.
872: * @param out_result The query result.
873: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
874: */
875: DUCKDB_API duckdb_state duckdb_query(duckdb_connection connection, const char *query, duckdb_result *out_result);
876: 
877: /*!
878: Closes the result and de-allocates all memory allocated for that connection.
879: 
880: * @param result The result to destroy.
881: */
882: DUCKDB_API void duckdb_destroy_result(duckdb_result *result);
883: 
884: /*!
885: Returns the column name of the specified column. The result should not need to be freed; the column names will
886: automatically be destroyed when the result is destroyed.
887: 
888: Returns `NULL` if the column is out of range.
889: 
890: * @param result The result object to fetch the column name from.
891: * @param col The column index.
892: * @return The column name of the specified column.
893: */
894: DUCKDB_API const char *duckdb_column_name(duckdb_result *result, idx_t col);
895: 
896: /*!
897: Returns the column type of the specified column.
898: 
899: Returns `DUCKDB_TYPE_INVALID` if the column is out of range.
900: 
901: * @param result The result object to fetch the column type from.
902: * @param col The column index.
903: * @return The column type of the specified column.
904: */
905: DUCKDB_API duckdb_type duckdb_column_type(duckdb_result *result, idx_t col);
906: 
907: /*!
908: Returns the statement type of the statement that was executed
909: 
910: * @param result The result object to fetch the statement type from.
911: * @return duckdb_statement_type value or DUCKDB_STATEMENT_TYPE_INVALID
912: */
913: DUCKDB_API duckdb_statement_type duckdb_result_statement_type(duckdb_result result);
914: 
915: /*!
916: Returns the logical column type of the specified column.
917: 
918: The return type of this call should be destroyed with `duckdb_destroy_logical_type`.
919: 
920: Returns `NULL` if the column is out of range.
921: 
922: * @param result The result object to fetch the column type from.
923: * @param col The column index.
924: * @return The logical column type of the specified column.
925: */
926: DUCKDB_API duckdb_logical_type duckdb_column_logical_type(duckdb_result *result, idx_t col);
927: 
928: /*!
929: Returns the number of columns present in a the result object.
930: 
931: * @param result The result object.
932: * @return The number of columns present in the result object.
933: */
934: DUCKDB_API idx_t duckdb_column_count(duckdb_result *result);
935: 
936: #ifndef DUCKDB_API_NO_DEPRECATED
937: /*!
938: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
939: 
940: Returns the number of rows present in the result object.
941: 
942: * @param result The result object.
943: * @return The number of rows present in the result object.
944: */
945: DUCKDB_API idx_t duckdb_row_count(duckdb_result *result);
946: 
947: #endif
948: /*!
949: Returns the number of rows changed by the query stored in the result. This is relevant only for INSERT/UPDATE/DELETE
950: queries. For other queries the rows_changed will be 0.
951: 
952: * @param result The result object.
953: * @return The number of rows changed.
954: */
955: DUCKDB_API idx_t duckdb_rows_changed(duckdb_result *result);
956: 
957: #ifndef DUCKDB_API_NO_DEPRECATED
958: /*!
959: **DEPRECATED**: Prefer using `duckdb_result_get_chunk` instead.
960: 
961: Returns the data of a specific column of a result in columnar format.
962: 
963: The function returns a dense array which contains the result data. The exact type stored in the array depends on the
964: corresponding duckdb_type (as provided by `duckdb_column_type`). For the exact type by which the data should be
965: accessed, see the comments in [the types section](types) or the `DUCKDB_TYPE` enum.
966: 
967: For example, for a column of type `DUCKDB_TYPE_INTEGER`, rows can be accessed in the following manner:
968: ```c
969: int32_t *data = (int32_t *) duckdb_column_data(&result, 0);
970: printf("Data for row %d: %d\n", row, data[row]);
971: ```
972: 
973: * @param result The result object to fetch the column data from.
974: * @param col The column index.
975: * @return The column data of the specified column.
976: */
977: DUCKDB_API void *duckdb_column_data(duckdb_result *result, idx_t col);
978: 
979: /*!
980: **DEPRECATED**: Prefer using `duckdb_result_get_chunk` instead.
981: 
982: Returns the nullmask of a specific column of a result in columnar format. The nullmask indicates for every row
983: whether or not the corresponding row is `NULL`. If a row is `NULL`, the values present in the array provided
984: by `duckdb_column_data` are undefined.
985: 
986: ```c
987: int32_t *data = (int32_t *) duckdb_column_data(&result, 0);
988: bool *nullmask = duckdb_nullmask_data(&result, 0);
989: if (nullmask[row]) {
990:     printf("Data for row %d: NULL\n", row);
991: } else {
992:     printf("Data for row %d: %d\n", row, data[row]);
993: }
994: ```
995: 
996: * @param result The result object to fetch the nullmask from.
997: * @param col The column index.
998: * @return The nullmask of the specified column.
999: */
1000: DUCKDB_API bool *duckdb_nullmask_data(duckdb_result *result, idx_t col);
1001: 
1002: #endif
1003: /*!
1004: Returns the error message contained within the result. The error is only set if `duckdb_query` returns `DuckDBError`.
1005: 
1006: The result of this function must not be freed. It will be cleaned up when `duckdb_destroy_result` is called.
1007: 
1008: * @param result The result object to fetch the error from.
1009: * @return The error of the result.
1010: */
1011: DUCKDB_API const char *duckdb_result_error(duckdb_result *result);
1012: 
1013: /*!
1014: Returns the result error type contained within the result. The error is only set if `duckdb_query` returns
1015: `DuckDBError`.
1016: 
1017: * @param result The result object to fetch the error from.
1018: * @return The error type of the result.
1019: */
1020: DUCKDB_API duckdb_error_type duckdb_result_error_type(duckdb_result *result);
1021: 
1022: //===--------------------------------------------------------------------===//
1023: // Result Functions
1024: //===--------------------------------------------------------------------===//
1025: 
1026: #ifndef DUCKDB_API_NO_DEPRECATED
1027: /*!
1028: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1029: 
1030: Fetches a data chunk from the duckdb_result. This function should be called repeatedly until the result is exhausted.
1031: 
1032: The result must be destroyed with `duckdb_destroy_data_chunk`.
1033: 
1034: This function supersedes all `duckdb_value` functions, as well as the `duckdb_column_data` and `duckdb_nullmask_data`
1035: functions. It results in significantly better performance, and should be preferred in newer code-bases.
1036: 
1037: If this function is used, none of the other result functions can be used and vice versa (i.e. this function cannot be
1038: mixed with the legacy result functions).
1039: 
1040: Use `duckdb_result_chunk_count` to figure out how many chunks there are in the result.
1041: 
1042: * @param result The result object to fetch the data chunk from.
1043: * @param chunk_index The chunk index to fetch from.
1044: * @return The resulting data chunk. Returns `NULL` if the chunk index is out of bounds.
1045: */
1046: DUCKDB_API duckdb_data_chunk duckdb_result_get_chunk(duckdb_result result, idx_t chunk_index);
1047: 
1048: /*!
1049: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1050: 
1051: Checks if the type of the internal result is StreamQueryResult.
1052: 
1053: * @param result The result object to check.
1054: * @return Whether or not the result object is of the type StreamQueryResult
1055: */
1056: DUCKDB_API bool duckdb_result_is_streaming(duckdb_result result);
1057: 
1058: /*!
1059: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1060: 
1061: Returns the number of data chunks present in the result.
1062: 
1063: * @param result The result object
1064: * @return Number of data chunks present in the result.
1065: */
1066: DUCKDB_API idx_t duckdb_result_chunk_count(duckdb_result result);
1067: 
1068: #endif
1069: /*!
1070: Returns the return_type of the given result, or DUCKDB_RETURN_TYPE_INVALID on error
1071: 
1072: * @param result The result object
1073: * @return The return_type
1074: */
1075: DUCKDB_API duckdb_result_type duckdb_result_return_type(duckdb_result result);
1076: 
1077: //===--------------------------------------------------------------------===//
1078: // Safe Fetch Functions
1079: //===--------------------------------------------------------------------===//
1080: 
1081: // These functions will perform conversions if necessary.
1082: // On failure (e.g. if conversion cannot be performed or if the value is NULL) a default value is returned.
1083: // Note that these functions are slow since they perform bounds checking and conversion
1084: // For fast access of values prefer using `duckdb_result_get_chunk`
1085: #ifndef DUCKDB_API_NO_DEPRECATED
1086: /*!
1087: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1088: 
1089: * @return The boolean value at the specified location, or false if the value cannot be converted.
1090: */
1091: DUCKDB_API bool duckdb_value_boolean(duckdb_result *result, idx_t col, idx_t row);
1092: 
1093: /*!
1094: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1095: 
1096: * @return The int8_t value at the specified location, or 0 if the value cannot be converted.
1097: */
1098: DUCKDB_API int8_t duckdb_value_int8(duckdb_result *result, idx_t col, idx_t row);
1099: 
1100: /*!
1101: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1102: 
1103: * @return The int16_t value at the specified location, or 0 if the value cannot be converted.
1104: */
1105: DUCKDB_API int16_t duckdb_value_int16(duckdb_result *result, idx_t col, idx_t row);
1106: 
1107: /*!
1108: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1109: 
1110: * @return The int32_t value at the specified location, or 0 if the value cannot be converted.
1111: */
1112: DUCKDB_API int32_t duckdb_value_int32(duckdb_result *result, idx_t col, idx_t row);
1113: 
1114: /*!
1115: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1116: 
1117: * @return The int64_t value at the specified location, or 0 if the value cannot be converted.
1118: */
1119: DUCKDB_API int64_t duckdb_value_int64(duckdb_result *result, idx_t col, idx_t row);
1120: 
1121: /*!
1122: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1123: 
1124: * @return The duckdb_hugeint value at the specified location, or 0 if the value cannot be converted.
1125: */
1126: DUCKDB_API duckdb_hugeint duckdb_value_hugeint(duckdb_result *result, idx_t col, idx_t row);
1127: 
1128: /*!
1129: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1130: 
1131: * @return The duckdb_uhugeint value at the specified location, or 0 if the value cannot be converted.
1132: */
1133: DUCKDB_API duckdb_uhugeint duckdb_value_uhugeint(duckdb_result *result, idx_t col, idx_t row);
1134: 
1135: /*!
1136: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1137: 
1138: * @return The duckdb_decimal value at the specified location, or 0 if the value cannot be converted.
1139: */
1140: DUCKDB_API duckdb_decimal duckdb_value_decimal(duckdb_result *result, idx_t col, idx_t row);
1141: 
1142: /*!
1143: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1144: 
1145: * @return The uint8_t value at the specified location, or 0 if the value cannot be converted.
1146: */
1147: DUCKDB_API uint8_t duckdb_value_uint8(duckdb_result *result, idx_t col, idx_t row);
1148: 
1149: /*!
1150: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1151: 
1152: * @return The uint16_t value at the specified location, or 0 if the value cannot be converted.
1153: */
1154: DUCKDB_API uint16_t duckdb_value_uint16(duckdb_result *result, idx_t col, idx_t row);
1155: 
1156: /*!
1157: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1158: 
1159: * @return The uint32_t value at the specified location, or 0 if the value cannot be converted.
1160: */
1161: DUCKDB_API uint32_t duckdb_value_uint32(duckdb_result *result, idx_t col, idx_t row);
1162: 
1163: /*!
1164: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1165: 
1166: * @return The uint64_t value at the specified location, or 0 if the value cannot be converted.
1167: */
1168: DUCKDB_API uint64_t duckdb_value_uint64(duckdb_result *result, idx_t col, idx_t row);
1169: 
1170: /*!
1171: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1172: 
1173: * @return The float value at the specified location, or 0 if the value cannot be converted.
1174: */
1175: DUCKDB_API float duckdb_value_float(duckdb_result *result, idx_t col, idx_t row);
1176: 
1177: /*!
1178: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1179: 
1180: * @return The double value at the specified location, or 0 if the value cannot be converted.
1181: */
1182: DUCKDB_API double duckdb_value_double(duckdb_result *result, idx_t col, idx_t row);
1183: 
1184: /*!
1185: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1186: 
1187: * @return The duckdb_date value at the specified location, or 0 if the value cannot be converted.
1188: */
1189: DUCKDB_API duckdb_date duckdb_value_date(duckdb_result *result, idx_t col, idx_t row);
1190: 
1191: /*!
1192: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1193: 
1194: * @return The duckdb_time value at the specified location, or 0 if the value cannot be converted.
1195: */
1196: DUCKDB_API duckdb_time duckdb_value_time(duckdb_result *result, idx_t col, idx_t row);
1197: 
1198: /*!
1199: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1200: 
1201: * @return The duckdb_timestamp value at the specified location, or 0 if the value cannot be converted.
1202: */
1203: DUCKDB_API duckdb_timestamp duckdb_value_timestamp(duckdb_result *result, idx_t col, idx_t row);
1204: 
1205: /*!
1206: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1207: 
1208: * @return The duckdb_interval value at the specified location, or 0 if the value cannot be converted.
1209: */
1210: DUCKDB_API duckdb_interval duckdb_value_interval(duckdb_result *result, idx_t col, idx_t row);
1211: 
1212: /*!
1213: **DEPRECATED**: Use duckdb_value_string instead. This function does not work correctly if the string contains null
1214: bytes.
1215: 
1216: * @return The text value at the specified location as a null-terminated string, or nullptr if the value cannot be
1217: converted. The result must be freed with `duckdb_free`.
1218: */
1219: DUCKDB_API char *duckdb_value_varchar(duckdb_result *result, idx_t col, idx_t row);
1220: 
1221: /*!
1222: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1223: 
1224: No support for nested types, and for other complex types.
1225: The resulting field "string.data" must be freed with `duckdb_free.`
1226: 
1227: * @return The string value at the specified location. Attempts to cast the result value to string.
1228: */
1229: DUCKDB_API duckdb_string duckdb_value_string(duckdb_result *result, idx_t col, idx_t row);
1230: 
1231: /*!
1232: **DEPRECATED**: Use duckdb_value_string_internal instead. This function does not work correctly if the string contains
1233: null bytes.
1234: 
1235: * @return The char* value at the specified location. ONLY works on VARCHAR columns and does not auto-cast.
1236: If the column is NOT a VARCHAR column this function will return NULL.
1237: 
1238: The result must NOT be freed.
1239: */
1240: DUCKDB_API char *duckdb_value_varchar_internal(duckdb_result *result, idx_t col, idx_t row);
1241: 
1242: /*!
1243: **DEPRECATED**: Use duckdb_value_string_internal instead. This function does not work correctly if the string contains
1244: null bytes.
1245: * @return The char* value at the specified location. ONLY works on VARCHAR columns and does not auto-cast.
1246: If the column is NOT a VARCHAR column this function will return NULL.
1247: 
1248: The result must NOT be freed.
1249: */
1250: DUCKDB_API duckdb_string duckdb_value_string_internal(duckdb_result *result, idx_t col, idx_t row);
1251: 
1252: /*!
1253: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1254: 
1255: * @return The duckdb_blob value at the specified location. Returns a blob with blob.data set to nullptr if the
1256: value cannot be converted. The resulting field "blob.data" must be freed with `duckdb_free.`
1257: */
1258: DUCKDB_API duckdb_blob duckdb_value_blob(duckdb_result *result, idx_t col, idx_t row);
1259: 
1260: /*!
1261: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1262: 
1263: * @return Returns true if the value at the specified index is NULL, and false otherwise.
1264: */
1265: DUCKDB_API bool duckdb_value_is_null(duckdb_result *result, idx_t col, idx_t row);
1266: 
1267: #endif
1268: //===--------------------------------------------------------------------===//
1269: // Helpers
1270: //===--------------------------------------------------------------------===//
1271: 
1272: /*!
1273: Allocate `size` bytes of memory using the duckdb internal malloc function. Any memory allocated in this manner
1274: should be freed using `duckdb_free`.
1275: 
1276: * @param size The number of bytes to allocate.
1277: * @return A pointer to the allocated memory region.
1278: */
1279: DUCKDB_API void *duckdb_malloc(size_t size);
1280: 
1281: /*!
1282: Free a value returned from `duckdb_malloc`, `duckdb_value_varchar`, `duckdb_value_blob`, or
1283: `duckdb_value_string`.
1284: 
1285: * @param ptr The memory region to de-allocate.
1286: */
1287: DUCKDB_API void duckdb_free(void *ptr);
1288: 
1289: /*!
1290: The internal vector size used by DuckDB.
1291: This is the amount of tuples that will fit into a data chunk created by `duckdb_create_data_chunk`.
1292: 
1293: * @return The vector size.
1294: */
1295: DUCKDB_API idx_t duckdb_vector_size();
1296: 
1297: /*!
1298: Whether or not the duckdb_string_t value is inlined.
1299: This means that the data of the string does not have a separate allocation.
1300: 
1301: */
1302: DUCKDB_API bool duckdb_string_is_inlined(duckdb_string_t string);
1303: 
1304: /*!
1305: Get the string length of a string_t
1306: 
1307: * @param string The string to get the length of.
1308: * @return The length.
1309: */
1310: DUCKDB_API uint32_t duckdb_string_t_length(duckdb_string_t string);
1311: 
1312: /*!
1313: Get a pointer to the string data of a string_t
1314: 
1315: * @param string The string to get the pointer to.
1316: * @return The pointer.
1317: */
1318: DUCKDB_API const char *duckdb_string_t_data(duckdb_string_t *string);
1319: 
1320: //===--------------------------------------------------------------------===//
1321: // Date Time Timestamp Helpers
1322: //===--------------------------------------------------------------------===//
1323: 
1324: /*!
1325: Decompose a `duckdb_date` object into year, month and date (stored as `duckdb_date_struct`).
1326: 
1327: * @param date The date object, as obtained from a `DUCKDB_TYPE_DATE` column.
1328: * @return The `duckdb_date_struct` with the decomposed elements.
1329: */
1330: DUCKDB_API duckdb_date_struct duckdb_from_date(duckdb_date date);
1331: 
1332: /*!
1333: Re-compose a `duckdb_date` from year, month and date (`duckdb_date_struct`).
1334: 
1335: * @param date The year, month and date stored in a `duckdb_date_struct`.
1336: * @return The `duckdb_date` element.
1337: */
1338: DUCKDB_API duckdb_date duckdb_to_date(duckdb_date_struct date);
1339: 
1340: /*!
1341: Test a `duckdb_date` to see if it is a finite value.
1342: 
1343: * @param date The date object, as obtained from a `DUCKDB_TYPE_DATE` column.
1344: * @return True if the date is finite, false if it is ±infinity.
1345: */
1346: DUCKDB_API bool duckdb_is_finite_date(duckdb_date date);
1347: 
1348: /*!
1349: Decompose a `duckdb_time` object into hour, minute, second and microsecond (stored as `duckdb_time_struct`).
1350: 
1351: * @param time The time object, as obtained from a `DUCKDB_TYPE_TIME` column.
1352: * @return The `duckdb_time_struct` with the decomposed elements.
1353: */
1354: DUCKDB_API duckdb_time_struct duckdb_from_time(duckdb_time time);
1355: 
1356: /*!
1357: Create a `duckdb_time_tz` object from micros and a timezone offset.
1358: 
1359: * @param micros The microsecond component of the time.
1360: * @param offset The timezone offset component of the time.
1361: * @return The `duckdb_time_tz` element.
1362: */
1363: DUCKDB_API duckdb_time_tz duckdb_create_time_tz(int64_t micros, int32_t offset);
1364: 
1365: /*!
1366: Decompose a TIME_TZ objects into micros and a timezone offset.
1367: 
1368: Use `duckdb_from_time` to further decompose the micros into hour, minute, second and microsecond.
1369: 
1370: * @param micros The time object, as obtained from a `DUCKDB_TYPE_TIME_TZ` column.
1371: */
1372: DUCKDB_API duckdb_time_tz_struct duckdb_from_time_tz(duckdb_time_tz micros);
1373: 
1374: /*!
1375: Re-compose a `duckdb_time` from hour, minute, second and microsecond (`duckdb_time_struct`).
1376: 
1377: * @param time The hour, minute, second and microsecond in a `duckdb_time_struct`.
1378: * @return The `duckdb_time` element.
1379: */
1380: DUCKDB_API duckdb_time duckdb_to_time(duckdb_time_struct time);
1381: 
1382: /*!
1383: Decompose a `duckdb_timestamp` object into a `duckdb_timestamp_struct`.
1384: 
1385: * @param ts The ts object, as obtained from a `DUCKDB_TYPE_TIMESTAMP` column.
1386: * @return The `duckdb_timestamp_struct` with the decomposed elements.
1387: */
1388: DUCKDB_API duckdb_timestamp_struct duckdb_from_timestamp(duckdb_timestamp ts);
1389: 
1390: /*!
1391: Re-compose a `duckdb_timestamp` from a duckdb_timestamp_struct.
1392: 
1393: * @param ts The de-composed elements in a `duckdb_timestamp_struct`.
1394: * @return The `duckdb_timestamp` element.
1395: */
1396: DUCKDB_API duckdb_timestamp duckdb_to_timestamp(duckdb_timestamp_struct ts);
1397: 
1398: /*!
1399: Test a `duckdb_timestamp` to see if it is a finite value.
1400: 
1401: * @param ts The duckdb_timestamp object, as obtained from a `DUCKDB_TYPE_TIMESTAMP` column.
1402: * @return True if the timestamp is finite, false if it is ±infinity.
1403: */
1404: DUCKDB_API bool duckdb_is_finite_timestamp(duckdb_timestamp ts);
1405: 
1406: /*!
1407: Test a `duckdb_timestamp_s` to see if it is a finite value.
1408: 
1409: * @param ts The duckdb_timestamp_s object, as obtained from a `DUCKDB_TYPE_TIMESTAMP_S` column.
1410: * @return True if the timestamp is finite, false if it is ±infinity.
1411: */
1412: DUCKDB_API bool duckdb_is_finite_timestamp_s(duckdb_timestamp_s ts);
1413: 
1414: /*!
1415: Test a `duckdb_timestamp_ms` to see if it is a finite value.
1416: 
1417: * @param ts The duckdb_timestamp_ms object, as obtained from a `DUCKDB_TYPE_TIMESTAMP_MS` column.
1418: * @return True if the timestamp is finite, false if it is ±infinity.
1419: */
1420: DUCKDB_API bool duckdb_is_finite_timestamp_ms(duckdb_timestamp_ms ts);
1421: 
1422: /*!
1423: Test a `duckdb_timestamp_ns` to see if it is a finite value.
1424: 
1425: * @param ts The duckdb_timestamp_ns object, as obtained from a `DUCKDB_TYPE_TIMESTAMP_NS` column.
1426: * @return True if the timestamp is finite, false if it is ±infinity.
1427: */
1428: DUCKDB_API bool duckdb_is_finite_timestamp_ns(duckdb_timestamp_ns ts);
1429: 
1430: //===--------------------------------------------------------------------===//
1431: // Hugeint Helpers
1432: //===--------------------------------------------------------------------===//
1433: 
1434: /*!
1435: Converts a duckdb_hugeint object (as obtained from a `DUCKDB_TYPE_HUGEINT` column) into a double.
1436: 
1437: * @param val The hugeint value.
1438: * @return The converted `double` element.
1439: */
1440: DUCKDB_API double duckdb_hugeint_to_double(duckdb_hugeint val);
1441: 
1442: /*!
1443: Converts a double value to a duckdb_hugeint object.
1444: 
1445: If the conversion fails because the double value is too big the result will be 0.
1446: 
1447: * @param val The double value.
1448: * @return The converted `duckdb_hugeint` element.
1449: */
1450: DUCKDB_API duckdb_hugeint duckdb_double_to_hugeint(double val);
1451: 
1452: //===--------------------------------------------------------------------===//
1453: // Unsigned Hugeint Helpers
1454: //===--------------------------------------------------------------------===//
1455: 
1456: /*!
1457: Converts a duckdb_uhugeint object (as obtained from a `DUCKDB_TYPE_UHUGEINT` column) into a double.
1458: 
1459: * @param val The uhugeint value.
1460: * @return The converted `double` element.
1461: */
1462: DUCKDB_API double duckdb_uhugeint_to_double(duckdb_uhugeint val);
1463: 
1464: /*!
1465: Converts a double value to a duckdb_uhugeint object.
1466: 
1467: If the conversion fails because the double value is too big the result will be 0.
1468: 
1469: * @param val The double value.
1470: * @return The converted `duckdb_uhugeint` element.
1471: */
1472: DUCKDB_API duckdb_uhugeint duckdb_double_to_uhugeint(double val);
1473: 
1474: //===--------------------------------------------------------------------===//
1475: // Decimal Helpers
1476: //===--------------------------------------------------------------------===//
1477: 
1478: /*!
1479: Converts a double value to a duckdb_decimal object.
1480: 
1481: If the conversion fails because the double value is too big, or the width/scale are invalid the result will be 0.
1482: 
1483: * @param val The double value.
1484: * @return The converted `duckdb_decimal` element.
1485: */
1486: DUCKDB_API duckdb_decimal duckdb_double_to_decimal(double val, uint8_t width, uint8_t scale);
1487: 
1488: /*!
1489: Converts a duckdb_decimal object (as obtained from a `DUCKDB_TYPE_DECIMAL` column) into a double.
1490: 
1491: * @param val The decimal value.
1492: * @return The converted `double` element.
1493: */
1494: DUCKDB_API double duckdb_decimal_to_double(duckdb_decimal val);
1495: 
1496: //===--------------------------------------------------------------------===//
1497: // Prepared Statements
1498: //===--------------------------------------------------------------------===//
1499: 
1500: // A prepared statement is a parameterized query that allows you to bind parameters to it.
1501: // * This is useful to easily supply parameters to functions and avoid SQL injection attacks.
1502: // * This is useful to speed up queries that you will execute several times with different parameters.
1503: // Because the query will only be parsed, bound, optimized and planned once during the prepare stage,
1504: // rather than once per execution.
1505: // For example:
1506: //   SELECT * FROM tbl WHERE id=?
1507: // Or a query with multiple parameters:
1508: //   SELECT * FROM tbl WHERE id=$1 OR name=$2
1509: /*!
1510: Create a prepared statement object from a query.
1511: 
1512: Note that after calling `duckdb_prepare`, the prepared statement should always be destroyed using
1513: `duckdb_destroy_prepare`, even if the prepare fails.
1514: 
1515: If the prepare fails, `duckdb_prepare_error` can be called to obtain the reason why the prepare failed.
1516: 
1517: * @param connection The connection object
1518: * @param query The SQL query to prepare
1519: * @param out_prepared_statement The resulting prepared statement object
1520: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
1521: */
1522: DUCKDB_API duckdb_state duckdb_prepare(duckdb_connection connection, const char *query,
1523:                                        duckdb_prepared_statement *out_prepared_statement);
1524: 
1525: /*!
1526: Closes the prepared statement and de-allocates all memory allocated for the statement.
1527: 
1528: * @param prepared_statement The prepared statement to destroy.
1529: */
1530: DUCKDB_API void duckdb_destroy_prepare(duckdb_prepared_statement *prepared_statement);
1531: 
1532: /*!
1533: Returns the error message associated with the given prepared statement.
1534: If the prepared statement has no error message, this returns `nullptr` instead.
1535: 
1536: The error message should not be freed. It will be de-allocated when `duckdb_destroy_prepare` is called.
1537: 
1538: * @param prepared_statement The prepared statement to obtain the error from.
1539: * @return The error message, or `nullptr` if there is none.
1540: */
1541: DUCKDB_API const char *duckdb_prepare_error(duckdb_prepared_statement prepared_statement);
1542: 
1543: /*!
1544: Returns the number of parameters that can be provided to the given prepared statement.
1545: 
1546: Returns 0 if the query was not successfully prepared.
1547: 
1548: * @param prepared_statement The prepared statement to obtain the number of parameters for.
1549: */
1550: DUCKDB_API idx_t duckdb_nparams(duckdb_prepared_statement prepared_statement);
1551: 
1552: /*!
1553: Returns the name used to identify the parameter
1554: The returned string should be freed using `duckdb_free`.
1555: 
1556: Returns NULL if the index is out of range for the provided prepared statement.
1557: 
1558: * @param prepared_statement The prepared statement for which to get the parameter name from.
1559: */
1560: DUCKDB_API const char *duckdb_parameter_name(duckdb_prepared_statement prepared_statement, idx_t index);
1561: 
1562: /*!
1563: Returns the parameter type for the parameter at the given index.
1564: 
1565: Returns `DUCKDB_TYPE_INVALID` if the parameter index is out of range or the statement was not successfully prepared.
1566: 
1567: * @param prepared_statement The prepared statement.
1568: * @param param_idx The parameter index.
1569: * @return The parameter type
1570: */
1571: DUCKDB_API duckdb_type duckdb_param_type(duckdb_prepared_statement prepared_statement, idx_t param_idx);
1572: 
1573: /*!
1574: Returns the logical type for the parameter at the given index.
1575: 
1576: Returns `nullptr` if the parameter index is out of range or the statement was not successfully prepared.
1577: 
1578: The return type of this call should be destroyed with `duckdb_destroy_logical_type`.
1579: 
1580: * @param prepared_statement The prepared statement.
1581: * @param param_idx The parameter index.
1582: * @return The logical type of the parameter
1583: */
1584: DUCKDB_API duckdb_logical_type duckdb_param_logical_type(duckdb_prepared_statement prepared_statement, idx_t param_idx);
1585: 
1586: /*!
1587: Clear the params bind to the prepared statement.
1588: */
1589: DUCKDB_API duckdb_state duckdb_clear_bindings(duckdb_prepared_statement prepared_statement);
1590: 
1591: /*!
1592: Returns the statement type of the statement to be executed
1593: 
1594: * @param statement The prepared statement.
1595: * @return duckdb_statement_type value or DUCKDB_STATEMENT_TYPE_INVALID
1596: */
1597: DUCKDB_API duckdb_statement_type duckdb_prepared_statement_type(duckdb_prepared_statement statement);
1598: 
1599: //===--------------------------------------------------------------------===//
1600: // Bind Values To Prepared Statements
1601: //===--------------------------------------------------------------------===//
1602: 
1603: /*!
1604: Binds a value to the prepared statement at the specified index.
1605: */
1606: DUCKDB_API duckdb_state duckdb_bind_value(duckdb_prepared_statement prepared_statement, idx_t param_idx,
1607:                                           duckdb_value val);
1608: 
1609: /*!
1610: Retrieve the index of the parameter for the prepared statement, identified by name
1611: */
1612: DUCKDB_API duckdb_state duckdb_bind_parameter_index(duckdb_prepared_statement prepared_statement, idx_t *param_idx_out,
1613:                                                     const char *name);
1614: 
1615: /*!
1616: Binds a bool value to the prepared statement at the specified index.
1617: */
1618: DUCKDB_API duckdb_state duckdb_bind_boolean(duckdb_prepared_statement prepared_statement, idx_t param_idx, bool val);
1619: 
1620: /*!
1621: Binds an int8_t value to the prepared statement at the specified index.
1622: */
1623: DUCKDB_API duckdb_state duckdb_bind_int8(duckdb_prepared_statement prepared_statement, idx_t param_idx, int8_t val);
1624: 
1625: /*!
1626: Binds an int16_t value to the prepared statement at the specified index.
1627: */
1628: DUCKDB_API duckdb_state duckdb_bind_int16(duckdb_prepared_statement prepared_statement, idx_t param_idx, int16_t val);
1629: 
1630: /*!
1631: Binds an int32_t value to the prepared statement at the specified index.
1632: */
1633: DUCKDB_API duckdb_state duckdb_bind_int32(duckdb_prepared_statement prepared_statement, idx_t param_idx, int32_t val);
1634: 
1635: /*!
1636: Binds an int64_t value to the prepared statement at the specified index.
1637: */
1638: DUCKDB_API duckdb_state duckdb_bind_int64(duckdb_prepared_statement prepared_statement, idx_t param_idx, int64_t val);
1639: 
1640: /*!
1641: Binds a duckdb_hugeint value to the prepared statement at the specified index.
1642: */
1643: DUCKDB_API duckdb_state duckdb_bind_hugeint(duckdb_prepared_statement prepared_statement, idx_t param_idx,
1644:                                             duckdb_hugeint val);
1645: 
1646: /*!
1647: Binds an duckdb_uhugeint value to the prepared statement at the specified index.
1648: */
1649: DUCKDB_API duckdb_state duckdb_bind_uhugeint(duckdb_prepared_statement prepared_statement, idx_t param_idx,
1650:                                              duckdb_uhugeint val);
1651: 
1652: /*!
1653: Binds a duckdb_decimal value to the prepared statement at the specified index.
1654: */
1655: DUCKDB_API duckdb_state duckdb_bind_decimal(duckdb_prepared_statement prepared_statement, idx_t param_idx,
1656:                                             duckdb_decimal val);
1657: 
1658: /*!
1659: Binds an uint8_t value to the prepared statement at the specified index.
1660: */
1661: DUCKDB_API duckdb_state duckdb_bind_uint8(duckdb_prepared_statement prepared_statement, idx_t param_idx, uint8_t val);
1662: 
1663: /*!
1664: Binds an uint16_t value to the prepared statement at the specified index.
1665: */
1666: DUCKDB_API duckdb_state duckdb_bind_uint16(duckdb_prepared_statement prepared_statement, idx_t param_idx, uint16_t val);
1667: 
1668: /*!
1669: Binds an uint32_t value to the prepared statement at the specified index.
1670: */
1671: DUCKDB_API duckdb_state duckdb_bind_uint32(duckdb_prepared_statement prepared_statement, idx_t param_idx, uint32_t val);
1672: 
1673: /*!
1674: Binds an uint64_t value to the prepared statement at the specified index.
1675: */
1676: DUCKDB_API duckdb_state duckdb_bind_uint64(duckdb_prepared_statement prepared_statement, idx_t param_idx, uint64_t val);
1677: 
1678: /*!
1679: Binds a float value to the prepared statement at the specified index.
1680: */
1681: DUCKDB_API duckdb_state duckdb_bind_float(duckdb_prepared_statement prepared_statement, idx_t param_idx, float val);
1682: 
1683: /*!
1684: Binds a double value to the prepared statement at the specified index.
1685: */
1686: DUCKDB_API duckdb_state duckdb_bind_double(duckdb_prepared_statement prepared_statement, idx_t param_idx, double val);
1687: 
1688: /*!
1689: Binds a duckdb_date value to the prepared statement at the specified index.
1690: */
1691: DUCKDB_API duckdb_state duckdb_bind_date(duckdb_prepared_statement prepared_statement, idx_t param_idx,
1692:                                          duckdb_date val);
1693: 
1694: /*!
1695: Binds a duckdb_time value to the prepared statement at the specified index.
1696: */
1697: DUCKDB_API duckdb_state duckdb_bind_time(duckdb_prepared_statement prepared_statement, idx_t param_idx,
1698:                                          duckdb_time val);
1699: 
1700: /*!
1701: Binds a duckdb_timestamp value to the prepared statement at the specified index.
1702: */
1703: DUCKDB_API duckdb_state duckdb_bind_timestamp(duckdb_prepared_statement prepared_statement, idx_t param_idx,
1704:                                               duckdb_timestamp val);
1705: 
1706: /*!
1707: Binds a duckdb_timestamp value to the prepared statement at the specified index.
1708: */
1709: DUCKDB_API duckdb_state duckdb_bind_timestamp_tz(duckdb_prepared_statement prepared_statement, idx_t param_idx,
1710:                                                  duckdb_timestamp val);
1711: 
1712: /*!
1713: Binds a duckdb_interval value to the prepared statement at the specified index.
1714: */
1715: DUCKDB_API duckdb_state duckdb_bind_interval(duckdb_prepared_statement prepared_statement, idx_t param_idx,
1716:                                              duckdb_interval val);
1717: 
1718: /*!
1719: Binds a null-terminated varchar value to the prepared statement at the specified index.
1720: */
1721: DUCKDB_API duckdb_state duckdb_bind_varchar(duckdb_prepared_statement prepared_statement, idx_t param_idx,
1722:                                             const char *val);
1723: 
1724: /*!
1725: Binds a varchar value to the prepared statement at the specified index.
1726: */
1727: DUCKDB_API duckdb_state duckdb_bind_varchar_length(duckdb_prepared_statement prepared_statement, idx_t param_idx,
1728:                                                    const char *val, idx_t length);
1729: 
1730: /*!
1731: Binds a blob value to the prepared statement at the specified index.
1732: */
1733: DUCKDB_API duckdb_state duckdb_bind_blob(duckdb_prepared_statement prepared_statement, idx_t param_idx,
1734:                                          const void *data, idx_t length);
1735: 
1736: /*!
1737: Binds a NULL value to the prepared statement at the specified index.
1738: */
1739: DUCKDB_API duckdb_state duckdb_bind_null(duckdb_prepared_statement prepared_statement, idx_t param_idx);
1740: 
1741: //===--------------------------------------------------------------------===//
1742: // Execute Prepared Statements
1743: //===--------------------------------------------------------------------===//
1744: 
1745: /*!
1746: Executes the prepared statement with the given bound parameters, and returns a materialized query result.
1747: 
1748: This method can be called multiple times for each prepared statement, and the parameters can be modified
1749: between calls to this function.
1750: 
1751: Note that the result must be freed with `duckdb_destroy_result`.
1752: 
1753: * @param prepared_statement The prepared statement to execute.
1754: * @param out_result The query result.
1755: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
1756: */
1757: DUCKDB_API duckdb_state duckdb_execute_prepared(duckdb_prepared_statement prepared_statement,
1758:                                                 duckdb_result *out_result);
1759: 
1760: #ifndef DUCKDB_API_NO_DEPRECATED
1761: /*!
1762: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1763: 
1764: Executes the prepared statement with the given bound parameters, and returns an optionally-streaming query result.
1765: To determine if the resulting query was in fact streamed, use `duckdb_result_is_streaming`
1766: 
1767: This method can be called multiple times for each prepared statement, and the parameters can be modified
1768: between calls to this function.
1769: 
1770: Note that the result must be freed with `duckdb_destroy_result`.
1771: 
1772: * @param prepared_statement The prepared statement to execute.
1773: * @param out_result The query result.
1774: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
1775: */
1776: DUCKDB_API duckdb_state duckdb_execute_prepared_streaming(duckdb_prepared_statement prepared_statement,
1777:                                                           duckdb_result *out_result);
1778: 
1779: #endif
1780: //===--------------------------------------------------------------------===//
1781: // Extract Statements
1782: //===--------------------------------------------------------------------===//
1783: 
1784: // A query string can be extracted into multiple SQL statements. Each statement can be prepared and executed separately.
1785: /*!
1786: Extract all statements from a query.
1787: Note that after calling `duckdb_extract_statements`, the extracted statements should always be destroyed using
1788: `duckdb_destroy_extracted`, even if no statements were extracted.
1789: 
1790: If the extract fails, `duckdb_extract_statements_error` can be called to obtain the reason why the extract failed.
1791: 
1792: * @param connection The connection object
1793: * @param query The SQL query to extract
1794: * @param out_extracted_statements The resulting extracted statements object
1795: * @return The number of extracted statements or 0 on failure.
1796: */
1797: DUCKDB_API idx_t duckdb_extract_statements(duckdb_connection connection, const char *query,
1798:                                            duckdb_extracted_statements *out_extracted_statements);
1799: 
1800: /*!
1801: Prepare an extracted statement.
1802: Note that after calling `duckdb_prepare_extracted_statement`, the prepared statement should always be destroyed using
1803: `duckdb_destroy_prepare`, even if the prepare fails.
1804: 
1805: If the prepare fails, `duckdb_prepare_error` can be called to obtain the reason why the prepare failed.
1806: 
1807: * @param connection The connection object
1808: * @param extracted_statements The extracted statements object
1809: * @param index The index of the extracted statement to prepare
1810: * @param out_prepared_statement The resulting prepared statement object
1811: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
1812: */
1813: DUCKDB_API duckdb_state duckdb_prepare_extracted_statement(duckdb_connection connection,
1814:                                                            duckdb_extracted_statements extracted_statements,
1815:                                                            idx_t index,
1816:                                                            duckdb_prepared_statement *out_prepared_statement);
1817: 
1818: /*!
1819: Returns the error message contained within the extracted statements.
1820: The result of this function must not be freed. It will be cleaned up when `duckdb_destroy_extracted` is called.
1821: 
1822: * @param extracted_statements The extracted statements to fetch the error from.
1823: * @return The error of the extracted statements.
1824: */
1825: DUCKDB_API const char *duckdb_extract_statements_error(duckdb_extracted_statements extracted_statements);
1826: 
1827: /*!
1828: De-allocates all memory allocated for the extracted statements.
1829: * @param extracted_statements The extracted statements to destroy.
1830: */
1831: DUCKDB_API void duckdb_destroy_extracted(duckdb_extracted_statements *extracted_statements);
1832: 
1833: //===--------------------------------------------------------------------===//
1834: // Pending Result Interface
1835: //===--------------------------------------------------------------------===//
1836: 
1837: /*!
1838: Executes the prepared statement with the given bound parameters, and returns a pending result.
1839: The pending result represents an intermediate structure for a query that is not yet fully executed.
1840: The pending result can be used to incrementally execute a query, returning control to the client between tasks.
1841: 
1842: Note that after calling `duckdb_pending_prepared`, the pending result should always be destroyed using
1843: `duckdb_destroy_pending`, even if this function returns DuckDBError.
1844: 
1845: * @param prepared_statement The prepared statement to execute.
1846: * @param out_result The pending query result.
1847: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
1848: */
1849: DUCKDB_API duckdb_state duckdb_pending_prepared(duckdb_prepared_statement prepared_statement,
1850:                                                 duckdb_pending_result *out_result);
1851: 
1852: #ifndef DUCKDB_API_NO_DEPRECATED
1853: /*!
1854: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
1855: 
1856: Executes the prepared statement with the given bound parameters, and returns a pending result.
1857: This pending result will create a streaming duckdb_result when executed.
1858: The pending result represents an intermediate structure for a query that is not yet fully executed.
1859: 
1860: Note that after calling `duckdb_pending_prepared_streaming`, the pending result should always be destroyed using
1861: `duckdb_destroy_pending`, even if this function returns DuckDBError.
1862: 
1863: * @param prepared_statement The prepared statement to execute.
1864: * @param out_result The pending query result.
1865: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
1866: */
1867: DUCKDB_API duckdb_state duckdb_pending_prepared_streaming(duckdb_prepared_statement prepared_statement,
1868:                                                           duckdb_pending_result *out_result);
1869: 
1870: #endif
1871: /*!
1872: Closes the pending result and de-allocates all memory allocated for the result.
1873: 
1874: * @param pending_result The pending result to destroy.
1875: */
1876: DUCKDB_API void duckdb_destroy_pending(duckdb_pending_result *pending_result);
1877: 
1878: /*!
1879: Returns the error message contained within the pending result.
1880: 
1881: The result of this function must not be freed. It will be cleaned up when `duckdb_destroy_pending` is called.
1882: 
1883: * @param pending_result The pending result to fetch the error from.
1884: * @return The error of the pending result.
1885: */
1886: DUCKDB_API const char *duckdb_pending_error(duckdb_pending_result pending_result);
1887: 
1888: /*!
1889: Executes a single task within the query, returning whether or not the query is ready.
1890: 
1891: If this returns DUCKDB_PENDING_RESULT_READY, the duckdb_execute_pending function can be called to obtain the result.
1892: If this returns DUCKDB_PENDING_RESULT_NOT_READY, the duckdb_pending_execute_task function should be called again.
1893: If this returns DUCKDB_PENDING_ERROR, an error occurred during execution.
1894: 
1895: The error message can be obtained by calling duckdb_pending_error on the pending_result.
1896: 
1897: * @param pending_result The pending result to execute a task within.
1898: * @return The state of the pending result after the execution.
1899: */
1900: DUCKDB_API duckdb_pending_state duckdb_pending_execute_task(duckdb_pending_result pending_result);
1901: 
1902: /*!
1903: If this returns DUCKDB_PENDING_RESULT_READY, the duckdb_execute_pending function can be called to obtain the result.
1904: If this returns DUCKDB_PENDING_RESULT_NOT_READY, the duckdb_pending_execute_check_state function should be called again.
1905: If this returns DUCKDB_PENDING_ERROR, an error occurred during execution.
1906: 
1907: The error message can be obtained by calling duckdb_pending_error on the pending_result.
1908: 
1909: * @param pending_result The pending result.
1910: * @return The state of the pending result.
1911: */
1912: DUCKDB_API duckdb_pending_state duckdb_pending_execute_check_state(duckdb_pending_result pending_result);
1913: 
1914: /*!
1915: Fully execute a pending query result, returning the final query result.
1916: 
1917: If duckdb_pending_execute_task has been called until DUCKDB_PENDING_RESULT_READY was returned, this will return fast.
1918: Otherwise, all remaining tasks must be executed first.
1919: 
1920: Note that the result must be freed with `duckdb_destroy_result`.
1921: 
1922: * @param pending_result The pending result to execute.
1923: * @param out_result The result object.
1924: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
1925: */
1926: DUCKDB_API duckdb_state duckdb_execute_pending(duckdb_pending_result pending_result, duckdb_result *out_result);
1927: 
1928: /*!
1929: Returns whether a duckdb_pending_state is finished executing. For example if `pending_state` is
1930: DUCKDB_PENDING_RESULT_READY, this function will return true.
1931: 
1932: * @param pending_state The pending state on which to decide whether to finish execution.
1933: * @return Boolean indicating pending execution should be considered finished.
1934: */
1935: DUCKDB_API bool duckdb_pending_execution_is_finished(duckdb_pending_state pending_state);
1936: 
1937: //===--------------------------------------------------------------------===//
1938: // Value Interface
1939: //===--------------------------------------------------------------------===//
1940: 
1941: /*!
1942: Destroys the value and de-allocates all memory allocated for that type.
1943: 
1944: * @param value The value to destroy.
1945: */
1946: DUCKDB_API void duckdb_destroy_value(duckdb_value *value);
1947: 
1948: /*!
1949: Creates a value from a null-terminated string
1950: 
1951: * @param text The null-terminated string
1952: * @return The value. This must be destroyed with `duckdb_destroy_value`.
1953: */
1954: DUCKDB_API duckdb_value duckdb_create_varchar(const char *text);
1955: 
1956: /*!
1957: Creates a value from a string
1958: 
1959: * @param text The text
1960: * @param length The length of the text
1961: * @return The value. This must be destroyed with `duckdb_destroy_value`.
1962: */
1963: DUCKDB_API duckdb_value duckdb_create_varchar_length(const char *text, idx_t length);
1964: 
1965: /*!
1966: Creates a value from a boolean
1967: 
1968: * @param input The boolean value
1969: * @return The value. This must be destroyed with `duckdb_destroy_value`.
1970: */
1971: DUCKDB_API duckdb_value duckdb_create_bool(bool input);
1972: 
1973: /*!
1974: Creates a value from a int8_t (a tinyint)
1975: 
1976: * @param input The tinyint value
1977: * @return The value. This must be destroyed with `duckdb_destroy_value`.
1978: */
1979: DUCKDB_API duckdb_value duckdb_create_int8(int8_t input);
1980: 
1981: /*!
1982: Creates a value from a uint8_t (a utinyint)
1983: 
1984: * @param input The utinyint value
1985: * @return The value. This must be destroyed with `duckdb_destroy_value`.
1986: */
1987: DUCKDB_API duckdb_value duckdb_create_uint8(uint8_t input);
1988: 
1989: /*!
1990: Creates a value from a int16_t (a smallint)
1991: 
1992: * @param input The smallint value
1993: * @return The value. This must be destroyed with `duckdb_destroy_value`.
1994: */
1995: DUCKDB_API duckdb_value duckdb_create_int16(int16_t input);
1996: 
1997: /*!
1998: Creates a value from a uint16_t (a usmallint)
1999: 
2000: * @param input The usmallint value
2001: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2002: */
2003: DUCKDB_API duckdb_value duckdb_create_uint16(uint16_t input);
2004: 
2005: /*!
2006: Creates a value from a int32_t (an integer)
2007: 
2008: * @param input The integer value
2009: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2010: */
2011: DUCKDB_API duckdb_value duckdb_create_int32(int32_t input);
2012: 
2013: /*!
2014: Creates a value from a uint32_t (a uinteger)
2015: 
2016: * @param input The uinteger value
2017: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2018: */
2019: DUCKDB_API duckdb_value duckdb_create_uint32(uint32_t input);
2020: 
2021: /*!
2022: Creates a value from a uint64_t (a ubigint)
2023: 
2024: * @param input The ubigint value
2025: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2026: */
2027: DUCKDB_API duckdb_value duckdb_create_uint64(uint64_t input);
2028: 
2029: /*!
2030: Creates a value from an int64
2031: 
2032: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2033: */
2034: DUCKDB_API duckdb_value duckdb_create_int64(int64_t val);
2035: 
2036: /*!
2037: Creates a value from a hugeint
2038: 
2039: * @param input The hugeint value
2040: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2041: */
2042: DUCKDB_API duckdb_value duckdb_create_hugeint(duckdb_hugeint input);
2043: 
2044: /*!
2045: Creates a value from a uhugeint
2046: 
2047: * @param input The uhugeint value
2048: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2049: */
2050: DUCKDB_API duckdb_value duckdb_create_uhugeint(duckdb_uhugeint input);
2051: 
2052: /*!
2053: Creates a VARINT value from a duckdb_varint
2054: 
2055: * @param input The duckdb_varint value
2056: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2057: */
2058: DUCKDB_API duckdb_value duckdb_create_varint(duckdb_varint input);
2059: 
2060: /*!
2061: Creates a DECIMAL value from a duckdb_decimal
2062: 
2063: * @param input The duckdb_decimal value
2064: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2065: */
2066: DUCKDB_API duckdb_value duckdb_create_decimal(duckdb_decimal input);
2067: 
2068: /*!
2069: Creates a value from a float
2070: 
2071: * @param input The float value
2072: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2073: */
2074: DUCKDB_API duckdb_value duckdb_create_float(float input);
2075: 
2076: /*!
2077: Creates a value from a double
2078: 
2079: * @param input The double value
2080: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2081: */
2082: DUCKDB_API duckdb_value duckdb_create_double(double input);
2083: 
2084: /*!
2085: Creates a value from a date
2086: 
2087: * @param input The date value
2088: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2089: */
2090: DUCKDB_API duckdb_value duckdb_create_date(duckdb_date input);
2091: 
2092: /*!
2093: Creates a value from a time
2094: 
2095: * @param input The time value
2096: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2097: */
2098: DUCKDB_API duckdb_value duckdb_create_time(duckdb_time input);
2099: 
2100: /*!
2101: Creates a value from a time_tz.
2102: Not to be confused with `duckdb_create_time_tz`, which creates a duckdb_time_tz_t.
2103: 
2104: * @param value The time_tz value
2105: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2106: */
2107: DUCKDB_API duckdb_value duckdb_create_time_tz_value(duckdb_time_tz value);
2108: 
2109: /*!
2110: Creates a TIMESTAMP value from a duckdb_timestamp
2111: 
2112: * @param input The duckdb_timestamp value
2113: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2114: */
2115: DUCKDB_API duckdb_value duckdb_create_timestamp(duckdb_timestamp input);
2116: 
2117: /*!
2118: Creates a TIMESTAMP_TZ value from a duckdb_timestamp
2119: 
2120: * @param input The duckdb_timestamp value
2121: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2122: */
2123: DUCKDB_API duckdb_value duckdb_create_timestamp_tz(duckdb_timestamp input);
2124: 
2125: /*!
2126: Creates a TIMESTAMP_S value from a duckdb_timestamp_s
2127: 
2128: * @param input The duckdb_timestamp_s value
2129: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2130: */
2131: DUCKDB_API duckdb_value duckdb_create_timestamp_s(duckdb_timestamp_s input);
2132: 
2133: /*!
2134: Creates a TIMESTAMP_MS value from a duckdb_timestamp_ms
2135: 
2136: * @param input The duckdb_timestamp_ms value
2137: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2138: */
2139: DUCKDB_API duckdb_value duckdb_create_timestamp_ms(duckdb_timestamp_ms input);
2140: 
2141: /*!
2142: Creates a TIMESTAMP_NS value from a duckdb_timestamp_ns
2143: 
2144: * @param input The duckdb_timestamp_ns value
2145: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2146: */
2147: DUCKDB_API duckdb_value duckdb_create_timestamp_ns(duckdb_timestamp_ns input);
2148: 
2149: /*!
2150: Creates a value from an interval
2151: 
2152: * @param input The interval value
2153: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2154: */
2155: DUCKDB_API duckdb_value duckdb_create_interval(duckdb_interval input);
2156: 
2157: /*!
2158: Creates a value from a blob
2159: 
2160: * @param data The blob data
2161: * @param length The length of the blob data
2162: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2163: */
2164: DUCKDB_API duckdb_value duckdb_create_blob(const uint8_t *data, idx_t length);
2165: 
2166: /*!
2167: Creates a BIT value from a duckdb_bit
2168: 
2169: * @param input The duckdb_bit value
2170: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2171: */
2172: DUCKDB_API duckdb_value duckdb_create_bit(duckdb_bit input);
2173: 
2174: /*!
2175: Creates a UUID value from a uhugeint
2176: 
2177: * @param input The duckdb_uhugeint containing the UUID
2178: * @return The value. This must be destroyed with `duckdb_destroy_value`.
2179: */
2180: DUCKDB_API duckdb_value duckdb_create_uuid(duckdb_uhugeint input);
2181: 
2182: /*!
2183: Returns the boolean value of the given value.
2184: 
2185: * @param val A duckdb_value containing a boolean
2186: * @return A boolean, or false if the value cannot be converted
2187: */
2188: DUCKDB_API bool duckdb_get_bool(duckdb_value val);
2189: 
2190: /*!
2191: Returns the int8_t value of the given value.
2192: 
2193: * @param val A duckdb_value containing a tinyint
2194: * @return A int8_t, or MinValue<int8> if the value cannot be converted
2195: */
2196: DUCKDB_API int8_t duckdb_get_int8(duckdb_value val);
2197: 
2198: /*!
2199: Returns the uint8_t value of the given value.
2200: 
2201: * @param val A duckdb_value containing a utinyint
2202: * @return A uint8_t, or MinValue<uint8> if the value cannot be converted
2203: */
2204: DUCKDB_API uint8_t duckdb_get_uint8(duckdb_value val);
2205: 
2206: /*!
2207: Returns the int16_t value of the given value.
2208: 
2209: * @param val A duckdb_value containing a smallint
2210: * @return A int16_t, or MinValue<int16> if the value cannot be converted
2211: */
2212: DUCKDB_API int16_t duckdb_get_int16(duckdb_value val);
2213: 
2214: /*!
2215: Returns the uint16_t value of the given value.
2216: 
2217: * @param val A duckdb_value containing a usmallint
2218: * @return A uint16_t, or MinValue<uint16> if the value cannot be converted
2219: */
2220: DUCKDB_API uint16_t duckdb_get_uint16(duckdb_value val);
2221: 
2222: /*!
2223: Returns the int32_t value of the given value.
2224: 
2225: * @param val A duckdb_value containing a integer
2226: * @return A int32_t, or MinValue<int32> if the value cannot be converted
2227: */
2228: DUCKDB_API int32_t duckdb_get_int32(duckdb_value val);
2229: 
2230: /*!
2231: Returns the uint32_t value of the given value.
2232: 
2233: * @param val A duckdb_value containing a uinteger
2234: * @return A uint32_t, or MinValue<uint32> if the value cannot be converted
2235: */
2236: DUCKDB_API uint32_t duckdb_get_uint32(duckdb_value val);
2237: 
2238: /*!
2239: Returns the int64_t value of the given value.
2240: 
2241: * @param val A duckdb_value containing a bigint
2242: * @return A int64_t, or MinValue<int64> if the value cannot be converted
2243: */
2244: DUCKDB_API int64_t duckdb_get_int64(duckdb_value val);
2245: 
2246: /*!
2247: Returns the uint64_t value of the given value.
2248: 
2249: * @param val A duckdb_value containing a ubigint
2250: * @return A uint64_t, or MinValue<uint64> if the value cannot be converted
2251: */
2252: DUCKDB_API uint64_t duckdb_get_uint64(duckdb_value val);
2253: 
2254: /*!
2255: Returns the hugeint value of the given value.
2256: 
2257: * @param val A duckdb_value containing a hugeint
2258: * @return A duckdb_hugeint, or MinValue<hugeint> if the value cannot be converted
2259: */
2260: DUCKDB_API duckdb_hugeint duckdb_get_hugeint(duckdb_value val);
2261: 
2262: /*!
2263: Returns the uhugeint value of the given value.
2264: 
2265: * @param val A duckdb_value containing a uhugeint
2266: * @return A duckdb_uhugeint, or MinValue<uhugeint> if the value cannot be converted
2267: */
2268: DUCKDB_API duckdb_uhugeint duckdb_get_uhugeint(duckdb_value val);
2269: 
2270: /*!
2271: Returns the duckdb_varint value of the given value.
2272: The `data` field must be destroyed with `duckdb_free`.
2273: 
2274: * @param val A duckdb_value containing a VARINT
2275: * @return A duckdb_varint. The `data` field must be destroyed with `duckdb_free`.
2276: */
2277: DUCKDB_API duckdb_varint duckdb_get_varint(duckdb_value val);
2278: 
2279: /*!
2280: Returns the duckdb_decimal value of the given value.
2281: 
2282: * @param val A duckdb_value containing a DECIMAL
2283: * @return A duckdb_decimal, or MinValue<decimal> if the value cannot be converted
2284: */
2285: DUCKDB_API duckdb_decimal duckdb_get_decimal(duckdb_value val);
2286: 
2287: /*!
2288: Returns the float value of the given value.
2289: 
2290: * @param val A duckdb_value containing a float
2291: * @return A float, or NAN if the value cannot be converted
2292: */
2293: DUCKDB_API float duckdb_get_float(duckdb_value val);
2294: 
2295: /*!
2296: Returns the double value of the given value.
2297: 
2298: * @param val A duckdb_value containing a double
2299: * @return A double, or NAN if the value cannot be converted
2300: */
2301: DUCKDB_API double duckdb_get_double(duckdb_value val);
2302: 
2303: /*!
2304: Returns the date value of the given value.
2305: 
2306: * @param val A duckdb_value containing a date
2307: * @return A duckdb_date, or MinValue<date> if the value cannot be converted
2308: */
2309: DUCKDB_API duckdb_date duckdb_get_date(duckdb_value val);
2310: 
2311: /*!
2312: Returns the time value of the given value.
2313: 
2314: * @param val A duckdb_value containing a time
2315: * @return A duckdb_time, or MinValue<time> if the value cannot be converted
2316: */
2317: DUCKDB_API duckdb_time duckdb_get_time(duckdb_value val);
2318: 
2319: /*!
2320: Returns the time_tz value of the given value.
2321: 
2322: * @param val A duckdb_value containing a time_tz
2323: * @return A duckdb_time_tz, or MinValue<time_tz> if the value cannot be converted
2324: */
2325: DUCKDB_API duckdb_time_tz duckdb_get_time_tz(duckdb_value val);
2326: 
2327: /*!
2328: Returns the TIMESTAMP value of the given value.
2329: 
2330: * @param val A duckdb_value containing a TIMESTAMP
2331: * @return A duckdb_timestamp, or MinValue<timestamp> if the value cannot be converted
2332: */
2333: DUCKDB_API duckdb_timestamp duckdb_get_timestamp(duckdb_value val);
2334: 
2335: /*!
2336: Returns the TIMESTAMP_TZ value of the given value.
2337: 
2338: * @param val A duckdb_value containing a TIMESTAMP_TZ
2339: * @return A duckdb_timestamp, or MinValue<timestamp_tz> if the value cannot be converted
2340: */
2341: DUCKDB_API duckdb_timestamp duckdb_get_timestamp_tz(duckdb_value val);
2342: 
2343: /*!
2344: Returns the duckdb_timestamp_s value of the given value.
2345: 
2346: * @param val A duckdb_value containing a TIMESTAMP_S
2347: * @return A duckdb_timestamp_s, or MinValue<timestamp_s> if the value cannot be converted
2348: */
2349: DUCKDB_API duckdb_timestamp_s duckdb_get_timestamp_s(duckdb_value val);
2350: 
2351: /*!
2352: Returns the duckdb_timestamp_ms value of the given value.
2353: 
2354: * @param val A duckdb_value containing a TIMESTAMP_MS
2355: * @return A duckdb_timestamp_ms, or MinValue<timestamp_ms> if the value cannot be converted
2356: */
2357: DUCKDB_API duckdb_timestamp_ms duckdb_get_timestamp_ms(duckdb_value val);
2358: 
2359: /*!
2360: Returns the duckdb_timestamp_ns value of the given value.
2361: 
2362: * @param val A duckdb_value containing a TIMESTAMP_NS
2363: * @return A duckdb_timestamp_ns, or MinValue<timestamp_ns> if the value cannot be converted
2364: */
2365: DUCKDB_API duckdb_timestamp_ns duckdb_get_timestamp_ns(duckdb_value val);
2366: 
2367: /*!
2368: Returns the interval value of the given value.
2369: 
2370: * @param val A duckdb_value containing a interval
2371: * @return A duckdb_interval, or MinValue<interval> if the value cannot be converted
2372: */
2373: DUCKDB_API duckdb_interval duckdb_get_interval(duckdb_value val);
2374: 
2375: /*!
2376: Returns the type of the given value. The type is valid as long as the value is not destroyed.
2377: The type itself must not be destroyed.
2378: 
2379: * @param val A duckdb_value
2380: * @return A duckdb_logical_type.
2381: */
2382: DUCKDB_API duckdb_logical_type duckdb_get_value_type(duckdb_value val);
2383: 
2384: /*!
2385: Returns the blob value of the given value.
2386: 
2387: * @param val A duckdb_value containing a blob
2388: * @return A duckdb_blob
2389: */
2390: DUCKDB_API duckdb_blob duckdb_get_blob(duckdb_value val);
2391: 
2392: /*!
2393: Returns the duckdb_bit value of the given value.
2394: The `data` field must be destroyed with `duckdb_free`.
2395: 
2396: * @param val A duckdb_value containing a BIT
2397: * @return A duckdb_bit
2398: */
2399: DUCKDB_API duckdb_bit duckdb_get_bit(duckdb_value val);
2400: 
2401: /*!
2402: Returns a duckdb_uhugeint representing the UUID value of the given value.
2403: 
2404: * @param val A duckdb_value containing a UUID
2405: * @return A duckdb_uhugeint representing the UUID value
2406: */
2407: DUCKDB_API duckdb_uhugeint duckdb_get_uuid(duckdb_value val);
2408: 
2409: /*!
2410: Obtains a string representation of the given value.
2411: The result must be destroyed with `duckdb_free`.
2412: 
2413: * @param value The value
2414: * @return The string value. This must be destroyed with `duckdb_free`.
2415: */
2416: DUCKDB_API char *duckdb_get_varchar(duckdb_value value);
2417: 
2418: /*!
2419: Creates a struct value from a type and an array of values. Must be destroyed with `duckdb_destroy_value`.
2420: 
2421: * @param type The type of the struct
2422: * @param values The values for the struct fields
2423: * @return The struct value, or nullptr, if any child type is `DUCKDB_TYPE_ANY` or `DUCKDB_TYPE_INVALID`.
2424: */
2425: DUCKDB_API duckdb_value duckdb_create_struct_value(duckdb_logical_type type, duckdb_value *values);
2426: 
2427: /*!
2428: Creates a list value from a child (element) type and an array of values of length `value_count`.
2429: Must be destroyed with `duckdb_destroy_value`.
2430: 
2431: * @param type The type of the list
2432: * @param values The values for the list
2433: * @param value_count The number of values in the list
2434: * @return The list value, or nullptr, if the child type is `DUCKDB_TYPE_ANY` or `DUCKDB_TYPE_INVALID`.
2435: */
2436: DUCKDB_API duckdb_value duckdb_create_list_value(duckdb_logical_type type, duckdb_value *values, idx_t value_count);
2437: 
2438: /*!
2439: Creates an array value from a child (element) type and an array of values of length `value_count`.
2440: Must be destroyed with `duckdb_destroy_value`.
2441: 
2442: * @param type The type of the array
2443: * @param values The values for the array
2444: * @param value_count The number of values in the array
2445: * @return The array value, or nullptr, if the child type is `DUCKDB_TYPE_ANY` or `DUCKDB_TYPE_INVALID`.
2446: */
2447: DUCKDB_API duckdb_value duckdb_create_array_value(duckdb_logical_type type, duckdb_value *values, idx_t value_count);
2448: 
2449: /*!
2450: Returns the number of elements in a MAP value.
2451: 
2452: * @param value The MAP value.
2453: * @return The number of elements in the map.
2454: */
2455: DUCKDB_API idx_t duckdb_get_map_size(duckdb_value value);
2456: 
2457: /*!
2458: Returns the MAP key at index as a duckdb_value.
2459: 
2460: * @param value The MAP value.
2461: * @param index The index of the key.
2462: * @return The key as a duckdb_value.
2463: */
2464: DUCKDB_API duckdb_value duckdb_get_map_key(duckdb_value value, idx_t index);
2465: 
2466: /*!
2467: Returns the MAP value at index as a duckdb_value.
2468: 
2469: * @param value The MAP value.
2470: * @param index The index of the value.
2471: * @return The value as a duckdb_value.
2472: */
2473: DUCKDB_API duckdb_value duckdb_get_map_value(duckdb_value value, idx_t index);
2474: 
2475: /*!
2476: Returns whether the value's type is SQLNULL or not.
2477: 
2478: * @param value The value to check.
2479: * @return True, if the value's type is SQLNULL, otherwise false.
2480: */
2481: DUCKDB_API bool duckdb_is_null_value(duckdb_value value);
2482: 
2483: /*!
2484: Creates a value of type SQLNULL.
2485: 
2486: * @return The duckdb_value representing SQLNULL. This must be destroyed with `duckdb_destroy_value`.
2487: */
2488: DUCKDB_API duckdb_value duckdb_create_null_value();
2489: 
2490: /*!
2491: Returns the number of elements in a LIST value.
2492: 
2493: * @param value The LIST value.
2494: * @return The number of elements in the list.
2495: */
2496: DUCKDB_API idx_t duckdb_get_list_size(duckdb_value value);
2497: 
2498: /*!
2499: Returns the LIST child at index as a duckdb_value.
2500: 
2501: * @param value The LIST value.
2502: * @param index The index of the child.
2503: * @return The child as a duckdb_value.
2504: */
2505: DUCKDB_API duckdb_value duckdb_get_list_child(duckdb_value value, idx_t index);
2506: 
2507: /*!
2508: Creates an enum value from a type and a value. Must be destroyed with `duckdb_destroy_value`.
2509: 
2510: * @param type The type of the enum
2511: * @param value The value for the enum
2512: * @return The enum value, or nullptr.
2513: */
2514: DUCKDB_API duckdb_value duckdb_create_enum_value(duckdb_logical_type type, uint64_t value);
2515: 
2516: /*!
2517: Returns the enum value of the given value.
2518: 
2519: * @param value A duckdb_value containing an enum
2520: * @return A uint64_t, or MinValue<uint64> if the value cannot be converted
2521: */
2522: DUCKDB_API uint64_t duckdb_get_enum_value(duckdb_value value);
2523: 
2524: /*!
2525: Returns the STRUCT child at index as a duckdb_value.
2526: 
2527: * @param value The STRUCT value.
2528: * @param index The index of the child.
2529: * @return The child as a duckdb_value.
2530: */
2531: DUCKDB_API duckdb_value duckdb_get_struct_child(duckdb_value value, idx_t index);
2532: 
2533: //===--------------------------------------------------------------------===//
2534: // Logical Type Interface
2535: //===--------------------------------------------------------------------===//
2536: 
2537: /*!
2538: Creates a `duckdb_logical_type` from a primitive type.
2539: The resulting logical type must be destroyed with `duckdb_destroy_logical_type`.
2540: 
2541: Returns an invalid logical type, if type is: `DUCKDB_TYPE_INVALID`, `DUCKDB_TYPE_DECIMAL`, `DUCKDB_TYPE_ENUM`,
2542: `DUCKDB_TYPE_LIST`, `DUCKDB_TYPE_STRUCT`, `DUCKDB_TYPE_MAP`, `DUCKDB_TYPE_ARRAY`, or `DUCKDB_TYPE_UNION`.
2543: 
2544: * @param type The primitive type to create.
2545: * @return The logical type.
2546: */
2547: DUCKDB_API duckdb_logical_type duckdb_create_logical_type(duckdb_type type);
2548: 
2549: /*!
2550: Returns the alias of a duckdb_logical_type, if set, else `nullptr`.
2551: The result must be destroyed with `duckdb_free`.
2552: 
2553: * @param type The logical type
2554: * @return The alias or `nullptr`
2555: */
2556: DUCKDB_API char *duckdb_logical_type_get_alias(duckdb_logical_type type);
2557: 
2558: /*!
2559: Sets the alias of a duckdb_logical_type.
2560: 
2561: * @param type The logical type
2562: * @param alias The alias to set
2563: */
2564: DUCKDB_API void duckdb_logical_type_set_alias(duckdb_logical_type type, const char *alias);
2565: 
2566: /*!
2567: Creates a LIST type from its child type.
2568: The return type must be destroyed with `duckdb_destroy_logical_type`.
2569: 
2570: * @param type The child type of the list
2571: * @return The logical type.
2572: */
2573: DUCKDB_API duckdb_logical_type duckdb_create_list_type(duckdb_logical_type type);
2574: 
2575: /*!
2576: Creates an ARRAY type from its child type.
2577: The return type must be destroyed with `duckdb_destroy_logical_type`.
2578: 
2579: * @param type The child type of the array.
2580: * @param array_size The number of elements in the array.
2581: * @return The logical type.
2582: */
2583: DUCKDB_API duckdb_logical_type duckdb_create_array_type(duckdb_logical_type type, idx_t array_size);
2584: 
2585: /*!
2586: Creates a MAP type from its key type and value type.
2587: The return type must be destroyed with `duckdb_destroy_logical_type`.
2588: 
2589: * @param key_type The map's key type.
2590: * @param value_type The map's value type.
2591: * @return The logical type.
2592: */
2593: DUCKDB_API duckdb_logical_type duckdb_create_map_type(duckdb_logical_type key_type, duckdb_logical_type value_type);
2594: 
2595: /*!
2596: Creates a UNION type from the passed arrays.
2597: The return type must be destroyed with `duckdb_destroy_logical_type`.
2598: 
2599: * @param member_types The array of union member types.
2600: * @param member_names The union member names.
2601: * @param member_count The number of union members.
2602: * @return The logical type.
2603: */
2604: DUCKDB_API duckdb_logical_type duckdb_create_union_type(duckdb_logical_type *member_types, const char **member_names,
2605:                                                         idx_t member_count);
2606: 
2607: /*!
2608: Creates a STRUCT type based on the member types and names.
2609: The resulting type must be destroyed with `duckdb_destroy_logical_type`.
2610: 
2611: * @param member_types The array of types of the struct members.
2612: * @param member_names The array of names of the struct members.
2613: * @param member_count The number of members of the struct.
2614: * @return The logical type.
2615: */
2616: DUCKDB_API duckdb_logical_type duckdb_create_struct_type(duckdb_logical_type *member_types, const char **member_names,
2617:                                                          idx_t member_count);
2618: 
2619: /*!
2620: Creates an ENUM type from the passed member name array.
2621: The resulting type should be destroyed with `duckdb_destroy_logical_type`.
2622: 
2623: * @param member_names The array of names that the enum should consist of.
2624: * @param member_count The number of elements that were specified in the array.
2625: * @return The logical type.
2626: */
2627: DUCKDB_API duckdb_logical_type duckdb_create_enum_type(const char **member_names, idx_t member_count);
2628: 
2629: /*!
2630: Creates a DECIMAL type with the specified width and scale.
2631: The resulting type should be destroyed with `duckdb_destroy_logical_type`.
2632: 
2633: * @param width The width of the decimal type
2634: * @param scale The scale of the decimal type
2635: * @return The logical type.
2636: */
2637: DUCKDB_API duckdb_logical_type duckdb_create_decimal_type(uint8_t width, uint8_t scale);
2638: 
2639: /*!
2640: Retrieves the enum `duckdb_type` of a `duckdb_logical_type`.
2641: 
2642: * @param type The logical type.
2643: * @return The `duckdb_type` id.
2644: */
2645: DUCKDB_API duckdb_type duckdb_get_type_id(duckdb_logical_type type);
2646: 
2647: /*!
2648: Retrieves the width of a decimal type.
2649: 
2650: * @param type The logical type object
2651: * @return The width of the decimal type
2652: */
2653: DUCKDB_API uint8_t duckdb_decimal_width(duckdb_logical_type type);
2654: 
2655: /*!
2656: Retrieves the scale of a decimal type.
2657: 
2658: * @param type The logical type object
2659: * @return The scale of the decimal type
2660: */
2661: DUCKDB_API uint8_t duckdb_decimal_scale(duckdb_logical_type type);
2662: 
2663: /*!
2664: Retrieves the internal storage type of a decimal type.
2665: 
2666: * @param type The logical type object
2667: * @return The internal type of the decimal type
2668: */
2669: DUCKDB_API duckdb_type duckdb_decimal_internal_type(duckdb_logical_type type);
2670: 
2671: /*!
2672: Retrieves the internal storage type of an enum type.
2673: 
2674: * @param type The logical type object
2675: * @return The internal type of the enum type
2676: */
2677: DUCKDB_API duckdb_type duckdb_enum_internal_type(duckdb_logical_type type);
2678: 
2679: /*!
2680: Retrieves the dictionary size of the enum type.
2681: 
2682: * @param type The logical type object
2683: * @return The dictionary size of the enum type
2684: */
2685: DUCKDB_API uint32_t duckdb_enum_dictionary_size(duckdb_logical_type type);
2686: 
2687: /*!
2688: Retrieves the dictionary value at the specified position from the enum.
2689: 
2690: The result must be freed with `duckdb_free`.
2691: 
2692: * @param type The logical type object
2693: * @param index The index in the dictionary
2694: * @return The string value of the enum type. Must be freed with `duckdb_free`.
2695: */
2696: DUCKDB_API char *duckdb_enum_dictionary_value(duckdb_logical_type type, idx_t index);
2697: 
2698: /*!
2699: Retrieves the child type of the given LIST type. Also accepts MAP types.
2700: The result must be freed with `duckdb_destroy_logical_type`.
2701: 
2702: * @param type The logical type, either LIST or MAP.
2703: * @return The child type of the LIST or MAP type.
2704: */
2705: DUCKDB_API duckdb_logical_type duckdb_list_type_child_type(duckdb_logical_type type);
2706: 
2707: /*!
2708: Retrieves the child type of the given ARRAY type.
2709: 
2710: The result must be freed with `duckdb_destroy_logical_type`.
2711: 
2712: * @param type The logical type. Must be ARRAY.
2713: * @return The child type of the ARRAY type.
2714: */
2715: DUCKDB_API duckdb_logical_type duckdb_array_type_child_type(duckdb_logical_type type);
2716: 
2717: /*!
2718: Retrieves the array size of the given array type.
2719: 
2720: * @param type The logical type object
2721: * @return The fixed number of elements the values of this array type can store.
2722: */
2723: DUCKDB_API idx_t duckdb_array_type_array_size(duckdb_logical_type type);
2724: 
2725: /*!
2726: Retrieves the key type of the given map type.
2727: 
2728: The result must be freed with `duckdb_destroy_logical_type`.
2729: 
2730: * @param type The logical type object
2731: * @return The key type of the map type. Must be destroyed with `duckdb_destroy_logical_type`.
2732: */
2733: DUCKDB_API duckdb_logical_type duckdb_map_type_key_type(duckdb_logical_type type);
2734: 
2735: /*!
2736: Retrieves the value type of the given map type.
2737: 
2738: The result must be freed with `duckdb_destroy_logical_type`.
2739: 
2740: * @param type The logical type object
2741: * @return The value type of the map type. Must be destroyed with `duckdb_destroy_logical_type`.
2742: */
2743: DUCKDB_API duckdb_logical_type duckdb_map_type_value_type(duckdb_logical_type type);
2744: 
2745: /*!
2746: Returns the number of children of a struct type.
2747: 
2748: * @param type The logical type object
2749: * @return The number of children of a struct type.
2750: */
2751: DUCKDB_API idx_t duckdb_struct_type_child_count(duckdb_logical_type type);
2752: 
2753: /*!
2754: Retrieves the name of the struct child.
2755: 
2756: The result must be freed with `duckdb_free`.
2757: 
2758: * @param type The logical type object
2759: * @param index The child index
2760: * @return The name of the struct type. Must be freed with `duckdb_free`.
2761: */
2762: DUCKDB_API char *duckdb_struct_type_child_name(duckdb_logical_type type, idx_t index);
2763: 
2764: /*!
2765: Retrieves the child type of the given struct type at the specified index.
2766: 
2767: The result must be freed with `duckdb_destroy_logical_type`.
2768: 
2769: * @param type The logical type object
2770: * @param index The child index
2771: * @return The child type of the struct type. Must be destroyed with `duckdb_destroy_logical_type`.
2772: */
2773: DUCKDB_API duckdb_logical_type duckdb_struct_type_child_type(duckdb_logical_type type, idx_t index);
2774: 
2775: /*!
2776: Returns the number of members that the union type has.
2777: 
2778: * @param type The logical type (union) object
2779: * @return The number of members of a union type.
2780: */
2781: DUCKDB_API idx_t duckdb_union_type_member_count(duckdb_logical_type type);
2782: 
2783: /*!
2784: Retrieves the name of the union member.
2785: 
2786: The result must be freed with `duckdb_free`.
2787: 
2788: * @param type The logical type object
2789: * @param index The child index
2790: * @return The name of the union member. Must be freed with `duckdb_free`.
2791: */
2792: DUCKDB_API char *duckdb_union_type_member_name(duckdb_logical_type type, idx_t index);
2793: 
2794: /*!
2795: Retrieves the child type of the given union member at the specified index.
2796: 
2797: The result must be freed with `duckdb_destroy_logical_type`.
2798: 
2799: * @param type The logical type object
2800: * @param index The child index
2801: * @return The child type of the union member. Must be destroyed with `duckdb_destroy_logical_type`.
2802: */
2803: DUCKDB_API duckdb_logical_type duckdb_union_type_member_type(duckdb_logical_type type, idx_t index);
2804: 
2805: /*!
2806: Destroys the logical type and de-allocates all memory allocated for that type.
2807: 
2808: * @param type The logical type to destroy.
2809: */
2810: DUCKDB_API void duckdb_destroy_logical_type(duckdb_logical_type *type);
2811: 
2812: /*!
2813: Registers a custom type within the given connection.
2814: The type must have an alias
2815: 
2816: * @param con The connection to use
2817: * @param type The custom type to register
2818: * @return Whether or not the registration was successful.
2819: */
2820: DUCKDB_API duckdb_state duckdb_register_logical_type(duckdb_connection con, duckdb_logical_type type,
2821:                                                      duckdb_create_type_info info);
2822: 
2823: //===--------------------------------------------------------------------===//
2824: // Data Chunk Interface
2825: //===--------------------------------------------------------------------===//
2826: 
2827: /*!
2828: Creates an empty data chunk with the specified column types.
2829: The result must be destroyed with `duckdb_destroy_data_chunk`.
2830: 
2831: * @param types An array of column types. Column types can not contain ANY and INVALID types.
2832: * @param column_count The number of columns.
2833: * @return The data chunk.
2834: */
2835: DUCKDB_API duckdb_data_chunk duckdb_create_data_chunk(duckdb_logical_type *types, idx_t column_count);
2836: 
2837: /*!
2838: Destroys the data chunk and de-allocates all memory allocated for that chunk.
2839: 
2840: * @param chunk The data chunk to destroy.
2841: */
2842: DUCKDB_API void duckdb_destroy_data_chunk(duckdb_data_chunk *chunk);
2843: 
2844: /*!
2845: Resets a data chunk, clearing the validity masks and setting the cardinality of the data chunk to 0.
2846: After calling this method, you must call `duckdb_vector_get_validity` and `duckdb_vector_get_data` to obtain current
2847: data and validity pointers
2848: 
2849: * @param chunk The data chunk to reset.
2850: */
2851: DUCKDB_API void duckdb_data_chunk_reset(duckdb_data_chunk chunk);
2852: 
2853: /*!
2854: Retrieves the number of columns in a data chunk.
2855: 
2856: * @param chunk The data chunk to get the data from
2857: * @return The number of columns in the data chunk
2858: */
2859: DUCKDB_API idx_t duckdb_data_chunk_get_column_count(duckdb_data_chunk chunk);
2860: 
2861: /*!
2862: Retrieves the vector at the specified column index in the data chunk.
2863: 
2864: The pointer to the vector is valid for as long as the chunk is alive.
2865: It does NOT need to be destroyed.
2866: 
2867: * @param chunk The data chunk to get the data from
2868: * @return The vector
2869: */
2870: DUCKDB_API duckdb_vector duckdb_data_chunk_get_vector(duckdb_data_chunk chunk, idx_t col_idx);
2871: 
2872: /*!
2873: Retrieves the current number of tuples in a data chunk.
2874: 
2875: * @param chunk The data chunk to get the data from
2876: * @return The number of tuples in the data chunk
2877: */
2878: DUCKDB_API idx_t duckdb_data_chunk_get_size(duckdb_data_chunk chunk);
2879: 
2880: /*!
2881: Sets the current number of tuples in a data chunk.
2882: 
2883: * @param chunk The data chunk to set the size in
2884: * @param size The number of tuples in the data chunk
2885: */
2886: DUCKDB_API void duckdb_data_chunk_set_size(duckdb_data_chunk chunk, idx_t size);
2887: 
2888: //===--------------------------------------------------------------------===//
2889: // Vector Interface
2890: //===--------------------------------------------------------------------===//
2891: 
2892: /*!
2893: Retrieves the column type of the specified vector.
2894: 
2895: The result must be destroyed with `duckdb_destroy_logical_type`.
2896: 
2897: * @param vector The vector get the data from
2898: * @return The type of the vector
2899: */
2900: DUCKDB_API duckdb_logical_type duckdb_vector_get_column_type(duckdb_vector vector);
2901: 
2902: /*!
2903: Retrieves the data pointer of the vector.
2904: 
2905: The data pointer can be used to read or write values from the vector.
2906: How to read or write values depends on the type of the vector.
2907: 
2908: * @param vector The vector to get the data from
2909: * @return The data pointer
2910: */
2911: DUCKDB_API void *duckdb_vector_get_data(duckdb_vector vector);
2912: 
2913: /*!
2914: Retrieves the validity mask pointer of the specified vector.
2915: 
2916: If all values are valid, this function MIGHT return NULL!
2917: 
2918: The validity mask is a bitset that signifies null-ness within the data chunk.
2919: It is a series of uint64_t values, where each uint64_t value contains validity for 64 tuples.
2920: The bit is set to 1 if the value is valid (i.e. not NULL) or 0 if the value is invalid (i.e. NULL).
2921: 
2922: Validity of a specific value can be obtained like this:
2923: 
2924: idx_t entry_idx = row_idx / 64;
2925: idx_t idx_in_entry = row_idx % 64;
2926: bool is_valid = validity_mask[entry_idx] & (1 << idx_in_entry);
2927: 
2928: Alternatively, the (slower) duckdb_validity_row_is_valid function can be used.
2929: 
2930: * @param vector The vector to get the data from
2931: * @return The pointer to the validity mask, or NULL if no validity mask is present
2932: */
2933: DUCKDB_API uint64_t *duckdb_vector_get_validity(duckdb_vector vector);
2934: 
2935: /*!
2936: Ensures the validity mask is writable by allocating it.
2937: 
2938: After this function is called, `duckdb_vector_get_validity` will ALWAYS return non-NULL.
2939: This allows NULL values to be written to the vector, regardless of whether a validity mask was present before.
2940: 
2941: * @param vector The vector to alter
2942: */
2943: DUCKDB_API void duckdb_vector_ensure_validity_writable(duckdb_vector vector);
2944: 
2945: /*!
2946: Assigns a string element in the vector at the specified location.
2947: 
2948: * @param vector The vector to alter
2949: * @param index The row position in the vector to assign the string to
2950: * @param str The null-terminated string
2951: */
2952: DUCKDB_API void duckdb_vector_assign_string_element(duckdb_vector vector, idx_t index, const char *str);
2953: 
2954: /*!
2955: Assigns a string element in the vector at the specified location. You may also use this function to assign BLOBs.
2956: 
2957: * @param vector The vector to alter
2958: * @param index The row position in the vector to assign the string to
2959: * @param str The string
2960: * @param str_len The length of the string (in bytes)
2961: */
2962: DUCKDB_API void duckdb_vector_assign_string_element_len(duckdb_vector vector, idx_t index, const char *str,
2963:                                                         idx_t str_len);
2964: 
2965: /*!
2966: Retrieves the child vector of a list vector.
2967: 
2968: The resulting vector is valid as long as the parent vector is valid.
2969: 
2970: * @param vector The vector
2971: * @return The child vector
2972: */
2973: DUCKDB_API duckdb_vector duckdb_list_vector_get_child(duckdb_vector vector);
2974: 
2975: /*!
2976: Returns the size of the child vector of the list.
2977: 
2978: * @param vector The vector
2979: * @return The size of the child list
2980: */
2981: DUCKDB_API idx_t duckdb_list_vector_get_size(duckdb_vector vector);
2982: 
2983: /*!
2984: Sets the total size of the underlying child-vector of a list vector.
2985: 
2986: * @param vector The list vector.
2987: * @param size The size of the child list.
2988: * @return The duckdb state. Returns DuckDBError if the vector is nullptr.
2989: */
2990: DUCKDB_API duckdb_state duckdb_list_vector_set_size(duckdb_vector vector, idx_t size);
2991: 
2992: /*!
2993: Sets the total capacity of the underlying child-vector of a list.
2994: 
2995: After calling this method, you must call `duckdb_vector_get_validity` and `duckdb_vector_get_data` to obtain current
2996: data and validity pointers
2997: 
2998: * @param vector The list vector.
2999: * @param required_capacity the total capacity to reserve.
3000: * @return The duckdb state. Returns DuckDBError if the vector is nullptr.
3001: */
3002: DUCKDB_API duckdb_state duckdb_list_vector_reserve(duckdb_vector vector, idx_t required_capacity);
3003: 
3004: /*!
3005: Retrieves the child vector of a struct vector.
3006: 
3007: The resulting vector is valid as long as the parent vector is valid.
3008: 
3009: * @param vector The vector
3010: * @param index The child index
3011: * @return The child vector
3012: */
3013: DUCKDB_API duckdb_vector duckdb_struct_vector_get_child(duckdb_vector vector, idx_t index);
3014: 
3015: /*!
3016: Retrieves the child vector of a array vector.
3017: 
3018: The resulting vector is valid as long as the parent vector is valid.
3019: The resulting vector has the size of the parent vector multiplied by the array size.
3020: 
3021: * @param vector The vector
3022: * @return The child vector
3023: */
3024: DUCKDB_API duckdb_vector duckdb_array_vector_get_child(duckdb_vector vector);
3025: 
3026: //===--------------------------------------------------------------------===//
3027: // Validity Mask Functions
3028: //===--------------------------------------------------------------------===//
3029: 
3030: /*!
3031: Returns whether or not a row is valid (i.e. not NULL) in the given validity mask.
3032: 
3033: * @param validity The validity mask, as obtained through `duckdb_vector_get_validity`
3034: * @param row The row index
3035: * @return true if the row is valid, false otherwise
3036: */
3037: DUCKDB_API bool duckdb_validity_row_is_valid(uint64_t *validity, idx_t row);
3038: 
3039: /*!
3040: In a validity mask, sets a specific row to either valid or invalid.
3041: 
3042: Note that `duckdb_vector_ensure_validity_writable` should be called before calling `duckdb_vector_get_validity`,
3043: to ensure that there is a validity mask to write to.
3044: 
3045: * @param validity The validity mask, as obtained through `duckdb_vector_get_validity`.
3046: * @param row The row index
3047: * @param valid Whether or not to set the row to valid, or invalid
3048: */
3049: DUCKDB_API void duckdb_validity_set_row_validity(uint64_t *validity, idx_t row, bool valid);
3050: 
3051: /*!
3052: In a validity mask, sets a specific row to invalid.
3053: 
3054: Equivalent to `duckdb_validity_set_row_validity` with valid set to false.
3055: 
3056: * @param validity The validity mask
3057: * @param row The row index
3058: */
3059: DUCKDB_API void duckdb_validity_set_row_invalid(uint64_t *validity, idx_t row);
3060: 
3061: /*!
3062: In a validity mask, sets a specific row to valid.
3063: 
3064: Equivalent to `duckdb_validity_set_row_validity` with valid set to true.
3065: 
3066: * @param validity The validity mask
3067: * @param row The row index
3068: */
3069: DUCKDB_API void duckdb_validity_set_row_valid(uint64_t *validity, idx_t row);
3070: 
3071: //===--------------------------------------------------------------------===//
3072: // Scalar Functions
3073: //===--------------------------------------------------------------------===//
3074: 
3075: /*!
3076: Creates a new empty scalar function.
3077: 
3078: The return value should be destroyed with `duckdb_destroy_scalar_function`.
3079: 
3080: * @return The scalar function object.
3081: */
3082: DUCKDB_API duckdb_scalar_function duckdb_create_scalar_function();
3083: 
3084: /*!
3085: Destroys the given scalar function object.
3086: 
3087: * @param scalar_function The scalar function to destroy
3088: */
3089: DUCKDB_API void duckdb_destroy_scalar_function(duckdb_scalar_function *scalar_function);
3090: 
3091: /*!
3092: Sets the name of the given scalar function.
3093: 
3094: * @param scalar_function The scalar function
3095: * @param name The name of the scalar function
3096: */
3097: DUCKDB_API void duckdb_scalar_function_set_name(duckdb_scalar_function scalar_function, const char *name);
3098: 
3099: /*!
3100: Sets the parameters of the given scalar function to varargs. Does not require adding parameters with
3101: duckdb_scalar_function_add_parameter.
3102: 
3103: * @param scalar_function The scalar function.
3104: * @param type The type of the arguments.
3105: * @return The parameter type. Cannot contain INVALID.
3106: */
3107: DUCKDB_API void duckdb_scalar_function_set_varargs(duckdb_scalar_function scalar_function, duckdb_logical_type type);
3108: 
3109: /*!
3110: Sets the parameters of the given scalar function to varargs. Does not require adding parameters with
3111: duckdb_scalar_function_add_parameter.
3112: 
3113: * @param scalar_function The scalar function.
3114: */
3115: DUCKDB_API void duckdb_scalar_function_set_special_handling(duckdb_scalar_function scalar_function);
3116: 
3117: /*!
3118: Sets the Function Stability of the scalar function to VOLATILE, indicating the function should be re-run for every row.
3119: This limits optimization that can be performed for the function.
3120: 
3121: * @param scalar_function The scalar function.
3122: */
3123: DUCKDB_API void duckdb_scalar_function_set_volatile(duckdb_scalar_function scalar_function);
3124: 
3125: /*!
3126: Adds a parameter to the scalar function.
3127: 
3128: * @param scalar_function The scalar function.
3129: * @param type The parameter type. Cannot contain INVALID.
3130: */
3131: DUCKDB_API void duckdb_scalar_function_add_parameter(duckdb_scalar_function scalar_function, duckdb_logical_type type);
3132: 
3133: /*!
3134: Sets the return type of the scalar function.
3135: 
3136: * @param scalar_function The scalar function
3137: * @param type Cannot contain INVALID or ANY.
3138: */
3139: DUCKDB_API void duckdb_scalar_function_set_return_type(duckdb_scalar_function scalar_function,
3140:                                                        duckdb_logical_type type);
3141: 
3142: /*!
3143: Assigns extra information to the scalar function that can be fetched during binding, etc.
3144: 
3145: * @param scalar_function The scalar function
3146: * @param extra_info The extra information
3147: * @param destroy The callback that will be called to destroy the bind data (if any)
3148: */
3149: DUCKDB_API void duckdb_scalar_function_set_extra_info(duckdb_scalar_function scalar_function, void *extra_info,
3150:                                                       duckdb_delete_callback_t destroy);
3151: 
3152: /*!
3153: Sets the main function of the scalar function.
3154: 
3155: * @param scalar_function The scalar function
3156: * @param function The function
3157: */
3158: DUCKDB_API void duckdb_scalar_function_set_function(duckdb_scalar_function scalar_function,
3159:                                                     duckdb_scalar_function_t function);
3160: 
3161: /*!
3162: Register the scalar function object within the given connection.
3163: 
3164: The function requires at least a name, a function and a return type.
3165: 
3166: If the function is incomplete or a function with this name already exists DuckDBError is returned.
3167: 
3168: * @param con The connection to register it in.
3169: * @param scalar_function The function pointer
3170: * @return Whether or not the registration was successful.
3171: */
3172: DUCKDB_API duckdb_state duckdb_register_scalar_function(duckdb_connection con, duckdb_scalar_function scalar_function);
3173: 
3174: /*!
3175: Retrieves the extra info of the function as set in `duckdb_scalar_function_set_extra_info`.
3176: 
3177: * @param info The info object.
3178: * @return The extra info.
3179: */
3180: DUCKDB_API void *duckdb_scalar_function_get_extra_info(duckdb_function_info info);
3181: 
3182: /*!
3183: Report that an error has occurred while executing the scalar function.
3184: 
3185: * @param info The info object.
3186: * @param error The error message
3187: */
3188: DUCKDB_API void duckdb_scalar_function_set_error(duckdb_function_info info, const char *error);
3189: 
3190: /*!
3191: Creates a new empty scalar function set.
3192: 
3193: The return value should be destroyed with `duckdb_destroy_scalar_function_set`.
3194: 
3195: * @return The scalar function set object.
3196: */
3197: DUCKDB_API duckdb_scalar_function_set duckdb_create_scalar_function_set(const char *name);
3198: 
3199: /*!
3200: Destroys the given scalar function set object.
3201: 
3202: */
3203: DUCKDB_API void duckdb_destroy_scalar_function_set(duckdb_scalar_function_set *scalar_function_set);
3204: 
3205: /*!
3206: Adds the scalar function as a new overload to the scalar function set.
3207: 
3208: Returns DuckDBError if the function could not be added, for example if the overload already exists.
3209: 
3210: * @param set The scalar function set
3211: * @param function The function to add
3212: */
3213: DUCKDB_API duckdb_state duckdb_add_scalar_function_to_set(duckdb_scalar_function_set set,
3214:                                                           duckdb_scalar_function function);
3215: 
3216: /*!
3217: Register the scalar function set within the given connection.
3218: 
3219: The set requires at least a single valid overload.
3220: 
3221: If the set is incomplete or a function with this name already exists DuckDBError is returned.
3222: 
3223: * @param con The connection to register it in.
3224: * @param set The function set to register
3225: * @return Whether or not the registration was successful.
3226: */
3227: DUCKDB_API duckdb_state duckdb_register_scalar_function_set(duckdb_connection con, duckdb_scalar_function_set set);
3228: 
3229: //===--------------------------------------------------------------------===//
3230: // Aggregate Functions
3231: //===--------------------------------------------------------------------===//
3232: 
3233: /*!
3234: Creates a new empty aggregate function.
3235: 
3236: The return value should be destroyed with `duckdb_destroy_aggregate_function`.
3237: 
3238: * @return The aggregate function object.
3239: */
3240: DUCKDB_API duckdb_aggregate_function duckdb_create_aggregate_function();
3241: 
3242: /*!
3243: Destroys the given aggregate function object.
3244: 
3245: */
3246: DUCKDB_API void duckdb_destroy_aggregate_function(duckdb_aggregate_function *aggregate_function);
3247: 
3248: /*!
3249: Sets the name of the given aggregate function.
3250: 
3251: * @param aggregate_function The aggregate function
3252: * @param name The name of the aggregate function
3253: */
3254: DUCKDB_API void duckdb_aggregate_function_set_name(duckdb_aggregate_function aggregate_function, const char *name);
3255: 
3256: /*!
3257: Adds a parameter to the aggregate function.
3258: 
3259: * @param aggregate_function The aggregate function.
3260: * @param type The parameter type. Cannot contain INVALID.
3261: */
3262: DUCKDB_API void duckdb_aggregate_function_add_parameter(duckdb_aggregate_function aggregate_function,
3263:                                                         duckdb_logical_type type);
3264: 
3265: /*!
3266: Sets the return type of the aggregate function.
3267: 
3268: * @param aggregate_function The aggregate function.
3269: * @param type The return type. Cannot contain INVALID or ANY.
3270: */
3271: DUCKDB_API void duckdb_aggregate_function_set_return_type(duckdb_aggregate_function aggregate_function,
3272:                                                           duckdb_logical_type type);
3273: 
3274: /*!
3275: Sets the main functions of the aggregate function.
3276: 
3277: * @param aggregate_function The aggregate function
3278: * @param state_size state size
3279: * @param state_init state init function
3280: * @param update update states
3281: * @param combine combine states
3282: * @param finalize finalize states
3283: */
3284: DUCKDB_API void duckdb_aggregate_function_set_functions(duckdb_aggregate_function aggregate_function,
3285:                                                         duckdb_aggregate_state_size state_size,
3286:                                                         duckdb_aggregate_init_t state_init,
3287:                                                         duckdb_aggregate_update_t update,
3288:                                                         duckdb_aggregate_combine_t combine,
3289:                                                         duckdb_aggregate_finalize_t finalize);
3290: 
3291: /*!
3292: Sets the state destructor callback of the aggregate function (optional)
3293: 
3294: * @param aggregate_function The aggregate function
3295: * @param destroy state destroy callback
3296: */
3297: DUCKDB_API void duckdb_aggregate_function_set_destructor(duckdb_aggregate_function aggregate_function,
3298:                                                          duckdb_aggregate_destroy_t destroy);
3299: 
3300: /*!
3301: Register the aggregate function object within the given connection.
3302: 
3303: The function requires at least a name, functions and a return type.
3304: 
3305: If the function is incomplete or a function with this name already exists DuckDBError is returned.
3306: 
3307: * @param con The connection to register it in.
3308: * @return Whether or not the registration was successful.
3309: */
3310: DUCKDB_API duckdb_state duckdb_register_aggregate_function(duckdb_connection con,
3311:                                                            duckdb_aggregate_function aggregate_function);
3312: 
3313: /*!
3314: Sets the NULL handling of the aggregate function to SPECIAL_HANDLING.
3315: 
3316: * @param aggregate_function The aggregate function
3317: */
3318: DUCKDB_API void duckdb_aggregate_function_set_special_handling(duckdb_aggregate_function aggregate_function);
3319: 
3320: /*!
3321: Assigns extra information to the scalar function that can be fetched during binding, etc.
3322: 
3323: * @param aggregate_function The aggregate function
3324: * @param extra_info The extra information
3325: * @param destroy The callback that will be called to destroy the bind data (if any)
3326: */
3327: DUCKDB_API void duckdb_aggregate_function_set_extra_info(duckdb_aggregate_function aggregate_function, void *extra_info,
3328:                                                          duckdb_delete_callback_t destroy);
3329: 
3330: /*!
3331: Retrieves the extra info of the function as set in `duckdb_aggregate_function_set_extra_info`.
3332: 
3333: * @param info The info object
3334: * @return The extra info
3335: */
3336: DUCKDB_API void *duckdb_aggregate_function_get_extra_info(duckdb_function_info info);
3337: 
3338: /*!
3339: Report that an error has occurred while executing the aggregate function.
3340: 
3341: * @param info The info object
3342: * @param error The error message
3343: */
3344: DUCKDB_API void duckdb_aggregate_function_set_error(duckdb_function_info info, const char *error);
3345: 
3346: /*!
3347: Creates a new empty aggregate function set.
3348: 
3349: The return value should be destroyed with `duckdb_destroy_aggregate_function_set`.
3350: 
3351: * @return The aggregate function set object.
3352: */
3353: DUCKDB_API duckdb_aggregate_function_set duckdb_create_aggregate_function_set(const char *name);
3354: 
3355: /*!
3356: Destroys the given aggregate function set object.
3357: 
3358: */
3359: DUCKDB_API void duckdb_destroy_aggregate_function_set(duckdb_aggregate_function_set *aggregate_function_set);
3360: 
3361: /*!
3362: Adds the aggregate function as a new overload to the aggregate function set.
3363: 
3364: Returns DuckDBError if the function could not be added, for example if the overload already exists.
3365: 
3366: * @param set The aggregate function set
3367: * @param function The function to add
3368: */
3369: DUCKDB_API duckdb_state duckdb_add_aggregate_function_to_set(duckdb_aggregate_function_set set,
3370:                                                              duckdb_aggregate_function function);
3371: 
3372: /*!
3373: Register the aggregate function set within the given connection.
3374: 
3375: The set requires at least a single valid overload.
3376: 
3377: If the set is incomplete or a function with this name already exists DuckDBError is returned.
3378: 
3379: * @param con The connection to register it in.
3380: * @param set The function set to register
3381: * @return Whether or not the registration was successful.
3382: */
3383: DUCKDB_API duckdb_state duckdb_register_aggregate_function_set(duckdb_connection con,
3384:                                                                duckdb_aggregate_function_set set);
3385: 
3386: //===--------------------------------------------------------------------===//
3387: // Table Functions
3388: //===--------------------------------------------------------------------===//
3389: 
3390: /*!
3391: Creates a new empty table function.
3392: 
3393: The return value should be destroyed with `duckdb_destroy_table_function`.
3394: 
3395: * @return The table function object.
3396: */
3397: DUCKDB_API duckdb_table_function duckdb_create_table_function();
3398: 
3399: /*!
3400: Destroys the given table function object.
3401: 
3402: * @param table_function The table function to destroy
3403: */
3404: DUCKDB_API void duckdb_destroy_table_function(duckdb_table_function *table_function);
3405: 
3406: /*!
3407: Sets the name of the given table function.
3408: 
3409: * @param table_function The table function
3410: * @param name The name of the table function
3411: */
3412: DUCKDB_API void duckdb_table_function_set_name(duckdb_table_function table_function, const char *name);
3413: 
3414: /*!
3415: Adds a parameter to the table function.
3416: 
3417: * @param table_function The table function.
3418: * @param type The parameter type. Cannot contain INVALID.
3419: */
3420: DUCKDB_API void duckdb_table_function_add_parameter(duckdb_table_function table_function, duckdb_logical_type type);
3421: 
3422: /*!
3423: Adds a named parameter to the table function.
3424: 
3425: * @param table_function The table function.
3426: * @param name The parameter name.
3427: * @param type The parameter type. Cannot contain INVALID.
3428: */
3429: DUCKDB_API void duckdb_table_function_add_named_parameter(duckdb_table_function table_function, const char *name,
3430:                                                           duckdb_logical_type type);
3431: 
3432: /*!
3433: Assigns extra information to the table function that can be fetched during binding, etc.
3434: 
3435: * @param table_function The table function
3436: * @param extra_info The extra information
3437: * @param destroy The callback that will be called to destroy the bind data (if any)
3438: */
3439: DUCKDB_API void duckdb_table_function_set_extra_info(duckdb_table_function table_function, void *extra_info,
3440:                                                      duckdb_delete_callback_t destroy);
3441: 
3442: /*!
3443: Sets the bind function of the table function.
3444: 
3445: * @param table_function The table function
3446: * @param bind The bind function
3447: */
3448: DUCKDB_API void duckdb_table_function_set_bind(duckdb_table_function table_function, duckdb_table_function_bind_t bind);
3449: 
3450: /*!
3451: Sets the init function of the table function.
3452: 
3453: * @param table_function The table function
3454: * @param init The init function
3455: */
3456: DUCKDB_API void duckdb_table_function_set_init(duckdb_table_function table_function, duckdb_table_function_init_t init);
3457: 
3458: /*!
3459: Sets the thread-local init function of the table function.
3460: 
3461: * @param table_function The table function
3462: * @param init The init function
3463: */
3464: DUCKDB_API void duckdb_table_function_set_local_init(duckdb_table_function table_function,
3465:                                                      duckdb_table_function_init_t init);
3466: 
3467: /*!
3468: Sets the main function of the table function.
3469: 
3470: * @param table_function The table function
3471: * @param function The function
3472: */
3473: DUCKDB_API void duckdb_table_function_set_function(duckdb_table_function table_function,
3474:                                                    duckdb_table_function_t function);
3475: 
3476: /*!
3477: Sets whether or not the given table function supports projection pushdown.
3478: 
3479: If this is set to true, the system will provide a list of all required columns in the `init` stage through
3480: the `duckdb_init_get_column_count` and `duckdb_init_get_column_index` functions.
3481: If this is set to false (the default), the system will expect all columns to be projected.
3482: 
3483: * @param table_function The table function
3484: * @param pushdown True if the table function supports projection pushdown, false otherwise.
3485: */
3486: DUCKDB_API void duckdb_table_function_supports_projection_pushdown(duckdb_table_function table_function, bool pushdown);
3487: 
3488: /*!
3489: Register the table function object within the given connection.
3490: 
3491: The function requires at least a name, a bind function, an init function and a main function.
3492: 
3493: If the function is incomplete or a function with this name already exists DuckDBError is returned.
3494: 
3495: * @param con The connection to register it in.
3496: * @param function The function pointer
3497: * @return Whether or not the registration was successful.
3498: */
3499: DUCKDB_API duckdb_state duckdb_register_table_function(duckdb_connection con, duckdb_table_function function);
3500: 
3501: //===--------------------------------------------------------------------===//
3502: // Table Function Bind
3503: //===--------------------------------------------------------------------===//
3504: 
3505: /*!
3506: Retrieves the extra info of the function as set in `duckdb_table_function_set_extra_info`.
3507: 
3508: * @param info The info object
3509: * @return The extra info
3510: */
3511: DUCKDB_API void *duckdb_bind_get_extra_info(duckdb_bind_info info);
3512: 
3513: /*!
3514: Adds a result column to the output of the table function.
3515: 
3516: * @param info The table function's bind info.
3517: * @param name The column name.
3518: * @param type The logical column type.
3519: */
3520: DUCKDB_API void duckdb_bind_add_result_column(duckdb_bind_info info, const char *name, duckdb_logical_type type);
3521: 
3522: /*!
3523: Retrieves the number of regular (non-named) parameters to the function.
3524: 
3525: * @param info The info object
3526: * @return The number of parameters
3527: */
3528: DUCKDB_API idx_t duckdb_bind_get_parameter_count(duckdb_bind_info info);
3529: 
3530: /*!
3531: Retrieves the parameter at the given index.
3532: 
3533: The result must be destroyed with `duckdb_destroy_value`.
3534: 
3535: * @param info The info object
3536: * @param index The index of the parameter to get
3537: * @return The value of the parameter. Must be destroyed with `duckdb_destroy_value`.
3538: */
3539: DUCKDB_API duckdb_value duckdb_bind_get_parameter(duckdb_bind_info info, idx_t index);
3540: 
3541: /*!
3542: Retrieves a named parameter with the given name.
3543: 
3544: The result must be destroyed with `duckdb_destroy_value`.
3545: 
3546: * @param info The info object
3547: * @param name The name of the parameter
3548: * @return The value of the parameter. Must be destroyed with `duckdb_destroy_value`.
3549: */
3550: DUCKDB_API duckdb_value duckdb_bind_get_named_parameter(duckdb_bind_info info, const char *name);
3551: 
3552: /*!
3553: Sets the user-provided bind data in the bind object. This object can be retrieved again during execution.
3554: 
3555: * @param info The info object
3556: * @param bind_data The bind data object.
3557: * @param destroy The callback that will be called to destroy the bind data (if any)
3558: */
3559: DUCKDB_API void duckdb_bind_set_bind_data(duckdb_bind_info info, void *bind_data, duckdb_delete_callback_t destroy);
3560: 
3561: /*!
3562: Sets the cardinality estimate for the table function, used for optimization.
3563: 
3564: * @param info The bind data object.
3565: * @param is_exact Whether or not the cardinality estimate is exact, or an approximation
3566: */
3567: DUCKDB_API void duckdb_bind_set_cardinality(duckdb_bind_info info, idx_t cardinality, bool is_exact);
3568: 
3569: /*!
3570: Report that an error has occurred while calling bind.
3571: 
3572: * @param info The info object
3573: * @param error The error message
3574: */
3575: DUCKDB_API void duckdb_bind_set_error(duckdb_bind_info info, const char *error);
3576: 
3577: //===--------------------------------------------------------------------===//
3578: // Table Function Init
3579: //===--------------------------------------------------------------------===//
3580: 
3581: /*!
3582: Retrieves the extra info of the function as set in `duckdb_table_function_set_extra_info`.
3583: 
3584: * @param info The info object
3585: * @return The extra info
3586: */
3587: DUCKDB_API void *duckdb_init_get_extra_info(duckdb_init_info info);
3588: 
3589: /*!
3590: Gets the bind data set by `duckdb_bind_set_bind_data` during the bind.
3591: 
3592: Note that the bind data should be considered as read-only.
3593: For tracking state, use the init data instead.
3594: 
3595: * @param info The info object
3596: * @return The bind data object
3597: */
3598: DUCKDB_API void *duckdb_init_get_bind_data(duckdb_init_info info);
3599: 
3600: /*!
3601: Sets the user-provided init data in the init object. This object can be retrieved again during execution.
3602: 
3603: * @param info The info object
3604: * @param init_data The init data object.
3605: * @param destroy The callback that will be called to destroy the init data (if any)
3606: */
3607: DUCKDB_API void duckdb_init_set_init_data(duckdb_init_info info, void *init_data, duckdb_delete_callback_t destroy);
3608: 
3609: /*!
3610: Returns the number of projected columns.
3611: 
3612: This function must be used if projection pushdown is enabled to figure out which columns to emit.
3613: 
3614: * @param info The info object
3615: * @return The number of projected columns.
3616: */
3617: DUCKDB_API idx_t duckdb_init_get_column_count(duckdb_init_info info);
3618: 
3619: /*!
3620: Returns the column index of the projected column at the specified position.
3621: 
3622: This function must be used if projection pushdown is enabled to figure out which columns to emit.
3623: 
3624: * @param info The info object
3625: * @param column_index The index at which to get the projected column index, from 0..duckdb_init_get_column_count(info)
3626: * @return The column index of the projected column.
3627: */
3628: DUCKDB_API idx_t duckdb_init_get_column_index(duckdb_init_info info, idx_t column_index);
3629: 
3630: /*!
3631: Sets how many threads can process this table function in parallel (default: 1)
3632: 
3633: * @param info The info object
3634: * @param max_threads The maximum amount of threads that can process this table function
3635: */
3636: DUCKDB_API void duckdb_init_set_max_threads(duckdb_init_info info, idx_t max_threads);
3637: 
3638: /*!
3639: Report that an error has occurred while calling init.
3640: 
3641: * @param info The info object
3642: * @param error The error message
3643: */
3644: DUCKDB_API void duckdb_init_set_error(duckdb_init_info info, const char *error);
3645: 
3646: //===--------------------------------------------------------------------===//
3647: // Table Function
3648: //===--------------------------------------------------------------------===//
3649: 
3650: /*!
3651: Retrieves the extra info of the function as set in `duckdb_table_function_set_extra_info`.
3652: 
3653: * @param info The info object
3654: * @return The extra info
3655: */
3656: DUCKDB_API void *duckdb_function_get_extra_info(duckdb_function_info info);
3657: 
3658: /*!
3659: Gets the bind data set by `duckdb_bind_set_bind_data` during the bind.
3660: 
3661: Note that the bind data should be considered as read-only.
3662: For tracking state, use the init data instead.
3663: 
3664: * @param info The info object
3665: * @return The bind data object
3666: */
3667: DUCKDB_API void *duckdb_function_get_bind_data(duckdb_function_info info);
3668: 
3669: /*!
3670: Gets the init data set by `duckdb_init_set_init_data` during the init.
3671: 
3672: * @param info The info object
3673: * @return The init data object
3674: */
3675: DUCKDB_API void *duckdb_function_get_init_data(duckdb_function_info info);
3676: 
3677: /*!
3678: Gets the thread-local init data set by `duckdb_init_set_init_data` during the local_init.
3679: 
3680: * @param info The info object
3681: * @return The init data object
3682: */
3683: DUCKDB_API void *duckdb_function_get_local_init_data(duckdb_function_info info);
3684: 
3685: /*!
3686: Report that an error has occurred while executing the function.
3687: 
3688: * @param info The info object
3689: * @param error The error message
3690: */
3691: DUCKDB_API void duckdb_function_set_error(duckdb_function_info info, const char *error);
3692: 
3693: //===--------------------------------------------------------------------===//
3694: // Replacement Scans
3695: //===--------------------------------------------------------------------===//
3696: 
3697: /*!
3698: Add a replacement scan definition to the specified database.
3699: 
3700: * @param db The database object to add the replacement scan to
3701: * @param replacement The replacement scan callback
3702: * @param extra_data Extra data that is passed back into the specified callback
3703: * @param delete_callback The delete callback to call on the extra data, if any
3704: */
3705: DUCKDB_API void duckdb_add_replacement_scan(duckdb_database db, duckdb_replacement_callback_t replacement,
3706:                                             void *extra_data, duckdb_delete_callback_t delete_callback);
3707: 
3708: /*!
3709: Sets the replacement function name. If this function is called in the replacement callback,
3710: the replacement scan is performed. If it is not called, the replacement callback is not performed.
3711: 
3712: * @param info The info object
3713: * @param function_name The function name to substitute.
3714: */
3715: DUCKDB_API void duckdb_replacement_scan_set_function_name(duckdb_replacement_scan_info info, const char *function_name);
3716: 
3717: /*!
3718: Adds a parameter to the replacement scan function.
3719: 
3720: * @param info The info object
3721: * @param parameter The parameter to add.
3722: */
3723: DUCKDB_API void duckdb_replacement_scan_add_parameter(duckdb_replacement_scan_info info, duckdb_value parameter);
3724: 
3725: /*!
3726: Report that an error has occurred while executing the replacement scan.
3727: 
3728: * @param info The info object
3729: * @param error The error message
3730: */
3731: DUCKDB_API void duckdb_replacement_scan_set_error(duckdb_replacement_scan_info info, const char *error);
3732: 
3733: //===--------------------------------------------------------------------===//
3734: // Profiling Info
3735: //===--------------------------------------------------------------------===//
3736: 
3737: /*!
3738: Returns the root node of the profiling information. Returns nullptr, if profiling is not enabled.
3739: 
3740: * @param connection A connection object.
3741: * @return A profiling information object.
3742: */
3743: DUCKDB_API duckdb_profiling_info duckdb_get_profiling_info(duckdb_connection connection);
3744: 
3745: /*!
3746: Returns the value of the metric of the current profiling info node. Returns nullptr, if the metric does
3747:  not exist or is not enabled. Currently, the value holds a string, and you can retrieve the string
3748:  by calling the corresponding function: char *duckdb_get_varchar(duckdb_value value).
3749: 
3750: * @param info A profiling information object.
3751: * @param key The name of the requested metric.
3752: * @return The value of the metric. Must be freed with `duckdb_destroy_value`
3753: */
3754: DUCKDB_API duckdb_value duckdb_profiling_info_get_value(duckdb_profiling_info info, const char *key);
3755: 
3756: /*!
3757: Returns the key-value metric map of this profiling node as a MAP duckdb_value.
3758: The individual elements are accessible via the duckdb_value MAP functions.
3759: 
3760: * @param info A profiling information object.
3761: * @return The key-value metric map as a MAP duckdb_value.
3762: */
3763: DUCKDB_API duckdb_value duckdb_profiling_info_get_metrics(duckdb_profiling_info info);
3764: 
3765: /*!
3766: Returns the number of children in the current profiling info node.
3767: 
3768: * @param info A profiling information object.
3769: * @return The number of children in the current node.
3770: */
3771: DUCKDB_API idx_t duckdb_profiling_info_get_child_count(duckdb_profiling_info info);
3772: 
3773: /*!
3774: Returns the child node at the specified index.
3775: 
3776: * @param info A profiling information object.
3777: * @param index The index of the child node.
3778: * @return The child node at the specified index.
3779: */
3780: DUCKDB_API duckdb_profiling_info duckdb_profiling_info_get_child(duckdb_profiling_info info, idx_t index);
3781: 
3782: //===--------------------------------------------------------------------===//
3783: // Appender
3784: //===--------------------------------------------------------------------===//
3785: 
3786: // Appenders are the most efficient way of loading data into DuckDB from within the C API.
3787: // They are recommended for fast data loading as they perform better than prepared statements or individual `INSERT
3788: // INTO` statements.
3789: 
3790: // Appends are possible in row-wise format, and by appending entire data chunks.
3791: 
3792: // Row-wise: for every column, a `duckdb_append_[type]` call should be made. After finishing all appends to a row, call
3793: // `duckdb_appender_end_row`.
3794: 
3795: // Chunk-wise: Consecutively call `duckdb_append_data_chunk` until all chunks have been appended.
3796: 
3797: // After all data has been appended, call `duckdb_appender_close` to finalize the appender followed by
3798: // `duckdb_appender_destroy` to clean up the memory.
3799: 
3800: /*!
3801: Creates an appender object.
3802: 
3803: Note that the object must be destroyed with `duckdb_appender_destroy`.
3804: 
3805: * @param connection The connection context to create the appender in.
3806: * @param schema The schema of the table to append to, or `nullptr` for the default schema.
3807: * @param table The table name to append to.
3808: * @param out_appender The resulting appender object.
3809: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
3810: */
3811: DUCKDB_API duckdb_state duckdb_appender_create(duckdb_connection connection, const char *schema, const char *table,
3812:                                                duckdb_appender *out_appender);
3813: 
3814: /*!
3815: Creates an appender object.
3816: 
3817: Note that the object must be destroyed with `duckdb_appender_destroy`.
3818: 
3819: * @param connection The connection context to create the appender in.
3820: * @param catalog The catalog of the table to append to, or `nullptr` for the default catalog.
3821: * @param schema The schema of the table to append to, or `nullptr` for the default schema.
3822: * @param table The table name to append to.
3823: * @param out_appender The resulting appender object.
3824: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
3825: */
3826: DUCKDB_API duckdb_state duckdb_appender_create_ext(duckdb_connection connection, const char *catalog,
3827:                                                    const char *schema, const char *table,
3828:                                                    duckdb_appender *out_appender);
3829: 
3830: /*!
3831: Returns the number of columns that belong to the appender.
3832: If there is no active column list, then this equals the table's physical columns.
3833: 
3834: * @param appender The appender to get the column count from.
3835: * @return The number of columns in the data chunks.
3836: */
3837: DUCKDB_API idx_t duckdb_appender_column_count(duckdb_appender appender);
3838: 
3839: /*!
3840: Returns the type of the column at the specified index. This is either a type in the active column list, or the same type
3841: as a column in the receiving table.
3842: 
3843: Note: The resulting type must be destroyed with `duckdb_destroy_logical_type`.
3844: 
3845: * @param appender The appender to get the column type from.
3846: * @param col_idx The index of the column to get the type of.
3847: * @return The `duckdb_logical_type` of the column.
3848: */
3849: DUCKDB_API duckdb_logical_type duckdb_appender_column_type(duckdb_appender appender, idx_t col_idx);
3850: 
3851: /*!
3852: Returns the error message associated with the given appender.
3853: If the appender has no error message, this returns `nullptr` instead.
3854: 
3855: The error message should not be freed. It will be de-allocated when `duckdb_appender_destroy` is called.
3856: 
3857: * @param appender The appender to get the error from.
3858: * @return The error message, or `nullptr` if there is none.
3859: */
3860: DUCKDB_API const char *duckdb_appender_error(duckdb_appender appender);
3861: 
3862: /*!
3863: Flush the appender to the table, forcing the cache of the appender to be cleared. If flushing the data triggers a
3864: constraint violation or any other error, then all data is invalidated, and this function returns DuckDBError.
3865: It is not possible to append more values. Call duckdb_appender_error to obtain the error message followed by
3866: duckdb_appender_destroy to destroy the invalidated appender.
3867: 
3868: * @param appender The appender to flush.
3869: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
3870: */
3871: DUCKDB_API duckdb_state duckdb_appender_flush(duckdb_appender appender);
3872: 
3873: /*!
3874: Closes the appender by flushing all intermediate states and closing it for further appends. If flushing the data
3875: triggers a constraint violation or any other error, then all data is invalidated, and this function returns DuckDBError.
3876: Call duckdb_appender_error to obtain the error message followed by duckdb_appender_destroy to destroy the invalidated
3877: appender.
3878: 
3879: * @param appender The appender to flush and close.
3880: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
3881: */
3882: DUCKDB_API duckdb_state duckdb_appender_close(duckdb_appender appender);
3883: 
3884: /*!
3885: Closes the appender by flushing all intermediate states to the table and destroying it. By destroying it, this function
3886: de-allocates all memory associated with the appender. If flushing the data triggers a constraint violation,
3887: then all data is invalidated, and this function returns DuckDBError. Due to the destruction of the appender, it is no
3888: longer possible to obtain the specific error message with duckdb_appender_error. Therefore, call duckdb_appender_close
3889: before destroying the appender, if you need insights into the specific error.
3890: 
3891: * @param appender The appender to flush, close and destroy.
3892: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
3893: */
3894: DUCKDB_API duckdb_state duckdb_appender_destroy(duckdb_appender *appender);
3895: 
3896: /*!
3897: Appends a column to the active column list of the appender. Immediately flushes all previous data.
3898: 
3899: The active column list specifies all columns that are expected when flushing the data. Any non-active columns are filled
3900: with their default values, or NULL.
3901: 
3902: * @param appender The appender to add the column to.
3903: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
3904: */
3905: DUCKDB_API duckdb_state duckdb_appender_add_column(duckdb_appender appender, const char *name);
3906: 
3907: /*!
3908: Removes all columns from the active column list of the appender, resetting the appender to treat all columns as active.
3909: Immediately flushes all previous data.
3910: 
3911: * @param appender The appender to clear the columns from.
3912: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
3913: */
3914: DUCKDB_API duckdb_state duckdb_appender_clear_columns(duckdb_appender appender);
3915: 
3916: /*!
3917: A nop function, provided for backwards compatibility reasons. Does nothing. Only `duckdb_appender_end_row` is required.
3918: */
3919: DUCKDB_API duckdb_state duckdb_appender_begin_row(duckdb_appender appender);
3920: 
3921: /*!
3922: Finish the current row of appends. After end_row is called, the next row can be appended.
3923: 
3924: * @param appender The appender.
3925: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
3926: */
3927: DUCKDB_API duckdb_state duckdb_appender_end_row(duckdb_appender appender);
3928: 
3929: /*!
3930: Append a DEFAULT value (NULL if DEFAULT not available for column) to the appender.
3931: */
3932: DUCKDB_API duckdb_state duckdb_append_default(duckdb_appender appender);
3933: 
3934: /*!
3935: Append a DEFAULT value, at the specified row and column, (NULL if DEFAULT not available for column) to the chunk created
3936: from the specified appender. The default value of the column must be a constant value. Non-deterministic expressions
3937: like nextval('seq') or random() are not supported.
3938: 
3939: * @param appender The appender to get the default value from.
3940: * @param chunk The data chunk to append the default value to.
3941: * @param col The chunk column index to append the default value to.
3942: * @param row The chunk row index to append the default value to.
3943: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
3944: */
3945: DUCKDB_API duckdb_state duckdb_append_default_to_chunk(duckdb_appender appender, duckdb_data_chunk chunk, idx_t col,
3946:                                                        idx_t row);
3947: 
3948: /*!
3949: Append a bool value to the appender.
3950: */
3951: DUCKDB_API duckdb_state duckdb_append_bool(duckdb_appender appender, bool value);
3952: 
3953: /*!
3954: Append an int8_t value to the appender.
3955: */
3956: DUCKDB_API duckdb_state duckdb_append_int8(duckdb_appender appender, int8_t value);
3957: 
3958: /*!
3959: Append an int16_t value to the appender.
3960: */
3961: DUCKDB_API duckdb_state duckdb_append_int16(duckdb_appender appender, int16_t value);
3962: 
3963: /*!
3964: Append an int32_t value to the appender.
3965: */
3966: DUCKDB_API duckdb_state duckdb_append_int32(duckdb_appender appender, int32_t value);
3967: 
3968: /*!
3969: Append an int64_t value to the appender.
3970: */
3971: DUCKDB_API duckdb_state duckdb_append_int64(duckdb_appender appender, int64_t value);
3972: 
3973: /*!
3974: Append a duckdb_hugeint value to the appender.
3975: */
3976: DUCKDB_API duckdb_state duckdb_append_hugeint(duckdb_appender appender, duckdb_hugeint value);
3977: 
3978: /*!
3979: Append a uint8_t value to the appender.
3980: */
3981: DUCKDB_API duckdb_state duckdb_append_uint8(duckdb_appender appender, uint8_t value);
3982: 
3983: /*!
3984: Append a uint16_t value to the appender.
3985: */
3986: DUCKDB_API duckdb_state duckdb_append_uint16(duckdb_appender appender, uint16_t value);
3987: 
3988: /*!
3989: Append a uint32_t value to the appender.
3990: */
3991: DUCKDB_API duckdb_state duckdb_append_uint32(duckdb_appender appender, uint32_t value);
3992: 
3993: /*!
3994: Append a uint64_t value to the appender.
3995: */
3996: DUCKDB_API duckdb_state duckdb_append_uint64(duckdb_appender appender, uint64_t value);
3997: 
3998: /*!
3999: Append a duckdb_uhugeint value to the appender.
4000: */
4001: DUCKDB_API duckdb_state duckdb_append_uhugeint(duckdb_appender appender, duckdb_uhugeint value);
4002: 
4003: /*!
4004: Append a float value to the appender.
4005: */
4006: DUCKDB_API duckdb_state duckdb_append_float(duckdb_appender appender, float value);
4007: 
4008: /*!
4009: Append a double value to the appender.
4010: */
4011: DUCKDB_API duckdb_state duckdb_append_double(duckdb_appender appender, double value);
4012: 
4013: /*!
4014: Append a duckdb_date value to the appender.
4015: */
4016: DUCKDB_API duckdb_state duckdb_append_date(duckdb_appender appender, duckdb_date value);
4017: 
4018: /*!
4019: Append a duckdb_time value to the appender.
4020: */
4021: DUCKDB_API duckdb_state duckdb_append_time(duckdb_appender appender, duckdb_time value);
4022: 
4023: /*!
4024: Append a duckdb_timestamp value to the appender.
4025: */
4026: DUCKDB_API duckdb_state duckdb_append_timestamp(duckdb_appender appender, duckdb_timestamp value);
4027: 
4028: /*!
4029: Append a duckdb_interval value to the appender.
4030: */
4031: DUCKDB_API duckdb_state duckdb_append_interval(duckdb_appender appender, duckdb_interval value);
4032: 
4033: /*!
4034: Append a varchar value to the appender.
4035: */
4036: DUCKDB_API duckdb_state duckdb_append_varchar(duckdb_appender appender, const char *val);
4037: 
4038: /*!
4039: Append a varchar value to the appender.
4040: */
4041: DUCKDB_API duckdb_state duckdb_append_varchar_length(duckdb_appender appender, const char *val, idx_t length);
4042: 
4043: /*!
4044: Append a blob value to the appender.
4045: */
4046: DUCKDB_API duckdb_state duckdb_append_blob(duckdb_appender appender, const void *data, idx_t length);
4047: 
4048: /*!
4049: Append a NULL value to the appender (of any type).
4050: */
4051: DUCKDB_API duckdb_state duckdb_append_null(duckdb_appender appender);
4052: 
4053: /*!
4054: Append a duckdb_value to the appender.
4055: */
4056: DUCKDB_API duckdb_state duckdb_append_value(duckdb_appender appender, duckdb_value value);
4057: 
4058: /*!
4059: Appends a pre-filled data chunk to the specified appender.
4060:  Attempts casting, if the data chunk types do not match the active appender types.
4061: 
4062: * @param appender The appender to append to.
4063: * @param chunk The data chunk to append.
4064: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
4065: */
4066: DUCKDB_API duckdb_state duckdb_append_data_chunk(duckdb_appender appender, duckdb_data_chunk chunk);
4067: 
4068: //===--------------------------------------------------------------------===//
4069: // Table Description
4070: //===--------------------------------------------------------------------===//
4071: 
4072: /*!
4073: Creates a table description object. Note that `duckdb_table_description_destroy` should always be called on the
4074: resulting table_description, even if the function returns `DuckDBError`.
4075: 
4076: * @param connection The connection context.
4077: * @param schema The schema of the table, or `nullptr` for the default schema.
4078: * @param table The table name.
4079: * @param out The resulting table description object.
4080: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
4081: */
4082: DUCKDB_API duckdb_state duckdb_table_description_create(duckdb_connection connection, const char *schema,
4083:                                                         const char *table, duckdb_table_description *out);
4084: 
4085: /*!
4086: Creates a table description object. Note that `duckdb_table_description_destroy` must be called on the resulting
4087: table_description, even if the function returns `DuckDBError`.
4088: 
4089: * @param connection The connection context.
4090: * @param catalog The catalog (database) name of the table, or `nullptr` for the default catalog.
4091: * @param schema The schema of the table, or `nullptr` for the default schema.
4092: * @param table The table name.
4093: * @param out The resulting table description object.
4094: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
4095: */
4096: DUCKDB_API duckdb_state duckdb_table_description_create_ext(duckdb_connection connection, const char *catalog,
4097:                                                             const char *schema, const char *table,
4098:                                                             duckdb_table_description *out);
4099: 
4100: /*!
4101: Destroy the TableDescription object.
4102: 
4103: * @param table_description The table_description to destroy.
4104: */
4105: DUCKDB_API void duckdb_table_description_destroy(duckdb_table_description *table_description);
4106: 
4107: /*!
4108: Returns the error message associated with the given table_description.
4109: If the table_description has no error message, this returns `nullptr` instead.
4110: The error message should not be freed. It will be de-allocated when `duckdb_table_description_destroy` is called.
4111: 
4112: * @param table_description The table_description to get the error from.
4113: * @return The error message, or `nullptr` if there is none.
4114: */
4115: DUCKDB_API const char *duckdb_table_description_error(duckdb_table_description table_description);
4116: 
4117: /*!
4118: Check if the column at 'index' index of the table has a DEFAULT expression.
4119: 
4120: * @param table_description The table_description to query.
4121: * @param index The index of the column to query.
4122: * @param out The out-parameter used to store the result.
4123: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
4124: */
4125: DUCKDB_API duckdb_state duckdb_column_has_default(duckdb_table_description table_description, idx_t index, bool *out);
4126: 
4127: /*!
4128: Obtain the column name at 'index'.
4129: The out result must be destroyed with `duckdb_free`.
4130: 
4131: * @param table_description The table_description to query.
4132: * @param index The index of the column to query.
4133: * @return The column name.
4134: */
4135: DUCKDB_API char *duckdb_table_description_get_column_name(duckdb_table_description table_description, idx_t index);
4136: 
4137: //===--------------------------------------------------------------------===//
4138: // Arrow Interface
4139: //===--------------------------------------------------------------------===//
4140: 
4141: #ifndef DUCKDB_API_NO_DEPRECATED
4142: /*!
4143: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
4144: 
4145: Executes a SQL query within a connection and stores the full (materialized) result in an arrow structure.
4146: If the query fails to execute, DuckDBError is returned and the error message can be retrieved by calling
4147: `duckdb_query_arrow_error`.
4148: 
4149: Note that after running `duckdb_query_arrow`, `duckdb_destroy_arrow` must be called on the result object even if the
4150: query fails, otherwise the error stored within the result will not be freed correctly.
4151: 
4152: * @param connection The connection to perform the query in.
4153: * @param query The SQL query to run.
4154: * @param out_result The query result.
4155: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
4156: */
4157: DUCKDB_API duckdb_state duckdb_query_arrow(duckdb_connection connection, const char *query, duckdb_arrow *out_result);
4158: 
4159: /*!
4160: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
4161: 
4162: Fetch the internal arrow schema from the arrow result. Remember to call release on the respective
4163: ArrowSchema object.
4164: 
4165: * @param result The result to fetch the schema from.
4166: * @param out_schema The output schema.
4167: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
4168: */
4169: DUCKDB_API duckdb_state duckdb_query_arrow_schema(duckdb_arrow result, duckdb_arrow_schema *out_schema);
4170: 
4171: /*!
4172: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
4173: 
4174: Fetch the internal arrow schema from the prepared statement. Remember to call release on the respective
4175: ArrowSchema object.
4176: 
4177: * @param prepared The prepared statement to fetch the schema from.
4178: * @param out_schema The output schema.
4179: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
4180: */
4181: DUCKDB_API duckdb_state duckdb_prepared_arrow_schema(duckdb_prepared_statement prepared,
4182:                                                      duckdb_arrow_schema *out_schema);
4183: 
4184: /*!
4185: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
4186: 
4187: Convert a data chunk into an arrow struct array. Remember to call release on the respective
4188: ArrowArray object.
4189: 
4190: * @param result The result object the data chunk have been fetched from.
4191: * @param chunk The data chunk to convert.
4192: * @param out_array The output array.
4193: */
4194: DUCKDB_API void duckdb_result_arrow_array(duckdb_result result, duckdb_data_chunk chunk, duckdb_arrow_array *out_array);
4195: 
4196: /*!
4197: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
4198: 
4199: Fetch an internal arrow struct array from the arrow result. Remember to call release on the respective
4200: ArrowArray object.
4201: 
4202: This function can be called multiple time to get next chunks, which will free the previous out_array.
4203: So consume the out_array before calling this function again.
4204: 
4205: * @param result The result to fetch the array from.
4206: * @param out_array The output array.
4207: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
4208: */
4209: DUCKDB_API duckdb_state duckdb_query_arrow_array(duckdb_arrow result, duckdb_arrow_array *out_array);
4210: 
4211: /*!
4212: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
4213: 
4214: Returns the number of columns present in the arrow result object.
4215: 
4216: * @param result The result object.
4217: * @return The number of columns present in the result object.
4218: */
4219: DUCKDB_API idx_t duckdb_arrow_column_count(duckdb_arrow result);
4220: 
4221: /*!
4222: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
4223: 
4224: Returns the number of rows present in the arrow result object.
4225: 
4226: * @param result The result object.
4227: * @return The number of rows present in the result object.
4228: */
4229: DUCKDB_API idx_t duckdb_arrow_row_count(duckdb_arrow result);
4230: 
4231: /*!
4232: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
4233: 
4234: Returns the number of rows changed by the query stored in the arrow result. This is relevant only for
4235: INSERT/UPDATE/DELETE queries. For other queries the rows_changed will be 0.
4236: 
4237: * @param result The result object.
4238: * @return The number of rows changed.
4239: */
4240: DUCKDB_API idx_t duckdb_arrow_rows_changed(duckdb_arrow result);
4241: 
4242: /*!
4243: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
4244: 
4245:  Returns the error message contained within the result. The error is only set if `duckdb_query_arrow` returns
4246: `DuckDBError`.
4247: 
4248: The error message should not be freed. It will be de-allocated when `duckdb_destroy_arrow` is called.
4249: 
4250: * @param result The result object to fetch the error from.
4251: * @return The error of the result.
4252: */
4253: DUCKDB_API const char *duckdb_query_arrow_error(duckdb_arrow result);
4254: 
4255: /*!
4256: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
4257: 
4258: Closes the result and de-allocates all memory allocated for the arrow result.
4259: 
4260: * @param result The result to destroy.
4261: */
4262: DUCKDB_API void duckdb_destroy_arrow(duckdb_arrow *result);
4263: 
4264: /*!
4265: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
4266: 
4267: Releases the arrow array stream and de-allocates its memory.
4268: 
4269: * @param stream_p The arrow array stream to destroy.
4270: */
4271: DUCKDB_API void duckdb_destroy_arrow_stream(duckdb_arrow_stream *stream_p);
4272: 
4273: /*!
4274: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
4275: 
4276: Executes the prepared statement with the given bound parameters, and returns an arrow query result.
4277: Note that after running `duckdb_execute_prepared_arrow`, `duckdb_destroy_arrow` must be called on the result object.
4278: 
4279: * @param prepared_statement The prepared statement to execute.
4280: * @param out_result The query result.
4281: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
4282: */
4283: DUCKDB_API duckdb_state duckdb_execute_prepared_arrow(duckdb_prepared_statement prepared_statement,
4284:                                                       duckdb_arrow *out_result);
4285: 
4286: /*!
4287: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
4288: 
4289: Scans the Arrow stream and creates a view with the given name.
4290: 
4291: * @param connection The connection on which to execute the scan.
4292: * @param table_name Name of the temporary view to create.
4293: * @param arrow Arrow stream wrapper.
4294: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
4295: */
4296: DUCKDB_API duckdb_state duckdb_arrow_scan(duckdb_connection connection, const char *table_name,
4297:                                           duckdb_arrow_stream arrow);
4298: 
4299: /*!
4300: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
4301: 
4302: Scans the Arrow array and creates a view with the given name.
4303: Note that after running `duckdb_arrow_array_scan`, `duckdb_destroy_arrow_stream` must be called on the out stream.
4304: 
4305: * @param connection The connection on which to execute the scan.
4306: * @param table_name Name of the temporary view to create.
4307: * @param arrow_schema Arrow schema wrapper.
4308: * @param arrow_array Arrow array wrapper.
4309: * @param out_stream Output array stream that wraps around the passed schema, for releasing/deleting once done.
4310: * @return `DuckDBSuccess` on success or `DuckDBError` on failure.
4311: */
4312: DUCKDB_API duckdb_state duckdb_arrow_array_scan(duckdb_connection connection, const char *table_name,
4313:                                                 duckdb_arrow_schema arrow_schema, duckdb_arrow_array arrow_array,
4314:                                                 duckdb_arrow_stream *out_stream);
4315: 
4316: #endif
4317: //===--------------------------------------------------------------------===//
4318: // Threading Information
4319: //===--------------------------------------------------------------------===//
4320: 
4321: /*!
4322: Execute DuckDB tasks on this thread.
4323: 
4324: Will return after `max_tasks` have been executed, or if there are no more tasks present.
4325: 
4326: * @param database The database object to execute tasks for
4327: * @param max_tasks The maximum amount of tasks to execute
4328: */
4329: DUCKDB_API void duckdb_execute_tasks(duckdb_database database, idx_t max_tasks);
4330: 
4331: /*!
4332: Creates a task state that can be used with duckdb_execute_tasks_state to execute tasks until
4333: `duckdb_finish_execution` is called on the state.
4334: 
4335: `duckdb_destroy_state` must be called on the result.
4336: 
4337: * @param database The database object to create the task state for
4338: * @return The task state that can be used with duckdb_execute_tasks_state.
4339: */
4340: DUCKDB_API duckdb_task_state duckdb_create_task_state(duckdb_database database);
4341: 
4342: /*!
4343: Execute DuckDB tasks on this thread.
4344: 
4345: The thread will keep on executing tasks forever, until duckdb_finish_execution is called on the state.
4346: Multiple threads can share the same duckdb_task_state.
4347: 
4348: * @param state The task state of the executor
4349: */
4350: DUCKDB_API void duckdb_execute_tasks_state(duckdb_task_state state);
4351: 
4352: /*!
4353: Execute DuckDB tasks on this thread.
4354: 
4355: The thread will keep on executing tasks until either duckdb_finish_execution is called on the state,
4356: max_tasks tasks have been executed or there are no more tasks to be executed.
4357: 
4358: Multiple threads can share the same duckdb_task_state.
4359: 
4360: * @param state The task state of the executor
4361: * @param max_tasks The maximum amount of tasks to execute
4362: * @return The amount of tasks that have actually been executed
4363: */
4364: DUCKDB_API idx_t duckdb_execute_n_tasks_state(duckdb_task_state state, idx_t max_tasks);
4365: 
4366: /*!
4367: Finish execution on a specific task.
4368: 
4369: * @param state The task state to finish execution
4370: */
4371: DUCKDB_API void duckdb_finish_execution(duckdb_task_state state);
4372: 
4373: /*!
4374: Check if the provided duckdb_task_state has finished execution
4375: 
4376: * @param state The task state to inspect
4377: * @return Whether or not duckdb_finish_execution has been called on the task state
4378: */
4379: DUCKDB_API bool duckdb_task_state_is_finished(duckdb_task_state state);
4380: 
4381: /*!
4382: Destroys the task state returned from duckdb_create_task_state.
4383: 
4384: Note that this should not be called while there is an active duckdb_execute_tasks_state running
4385: on the task state.
4386: 
4387: * @param state The task state to clean up
4388: */
4389: DUCKDB_API void duckdb_destroy_task_state(duckdb_task_state state);
4390: 
4391: /*!
4392: Returns true if the execution of the current query is finished.
4393: 
4394: * @param con The connection on which to check
4395: */
4396: DUCKDB_API bool duckdb_execution_is_finished(duckdb_connection con);
4397: 
4398: //===--------------------------------------------------------------------===//
4399: // Streaming Result Interface
4400: //===--------------------------------------------------------------------===//
4401: 
4402: #ifndef DUCKDB_API_NO_DEPRECATED
4403: /*!
4404: **DEPRECATION NOTICE**: This method is scheduled for removal in a future release.
4405: 
4406: Fetches a data chunk from the (streaming) duckdb_result. This function should be called repeatedly until the result is
4407: exhausted.
4408: 
4409: The result must be destroyed with `duckdb_destroy_data_chunk`.
4410: 
4411: This function can only be used on duckdb_results created with 'duckdb_pending_prepared_streaming'
4412: 
4413: If this function is used, none of the other result functions can be used and vice versa (i.e. this function cannot be
4414: mixed with the legacy result functions or the materialized result functions).
4415: 
4416: It is not known beforehand how many chunks will be returned by this result.
4417: 
4418: * @param result The result object to fetch the data chunk from.
4419: * @return The resulting data chunk. Returns `NULL` if the result has an error.
4420: */
4421: DUCKDB_API duckdb_data_chunk duckdb_stream_fetch_chunk(duckdb_result result);
4422: 
4423: #endif
4424: /*!
4425: Fetches a data chunk from a duckdb_result. This function should be called repeatedly until the result is exhausted.
4426: 
4427: The result must be destroyed with `duckdb_destroy_data_chunk`.
4428: 
4429: It is not known beforehand how many chunks will be returned by this result.
4430: 
4431: * @param result The result object to fetch the data chunk from.
4432: * @return The resulting data chunk. Returns `NULL` if the result has an error.
4433: */
4434: DUCKDB_API duckdb_data_chunk duckdb_fetch_chunk(duckdb_result result);
4435: 
4436: //===--------------------------------------------------------------------===//
4437: // Cast Functions
4438: //===--------------------------------------------------------------------===//
4439: 
4440: /*!
4441: Creates a new cast function object.
4442: 
4443: * @return The cast function object.
4444: */
4445: DUCKDB_API duckdb_cast_function duckdb_create_cast_function();
4446: 
4447: /*!
4448: Sets the source type of the cast function.
4449: 
4450: * @param cast_function The cast function object.
4451: * @param source_type The source type to set.
4452: */
4453: DUCKDB_API void duckdb_cast_function_set_source_type(duckdb_cast_function cast_function,
4454:                                                      duckdb_logical_type source_type);
4455: 
4456: /*!
4457: Sets the target type of the cast function.
4458: 
4459: * @param cast_function The cast function object.
4460: * @param target_type The target type to set.
4461: */
4462: DUCKDB_API void duckdb_cast_function_set_target_type(duckdb_cast_function cast_function,
4463:                                                      duckdb_logical_type target_type);
4464: 
4465: /*!
4466: Sets the "cost" of implicitly casting the source type to the target type using this function.
4467: 
4468: * @param cast_function The cast function object.
4469: * @param cost The cost to set.
4470: */
4471: DUCKDB_API void duckdb_cast_function_set_implicit_cast_cost(duckdb_cast_function cast_function, int64_t cost);
4472: 
4473: /*!
4474: Sets the actual cast function to use.
4475: 
4476: * @param cast_function The cast function object.
4477: * @param function The function to set.
4478: */
4479: DUCKDB_API void duckdb_cast_function_set_function(duckdb_cast_function cast_function, duckdb_cast_function_t function);
4480: 
4481: /*!
4482: Assigns extra information to the cast function that can be fetched during execution, etc.
4483: 
4484: * @param extra_info The extra information
4485: * @param destroy The callback that will be called to destroy the extra information (if any)
4486: */
4487: DUCKDB_API void duckdb_cast_function_set_extra_info(duckdb_cast_function cast_function, void *extra_info,
4488:                                                     duckdb_delete_callback_t destroy);
4489: 
4490: /*!
4491: Retrieves the extra info of the function as set in `duckdb_cast_function_set_extra_info`.
4492: 
4493: * @param info The info object.
4494: * @return The extra info.
4495: */
4496: DUCKDB_API void *duckdb_cast_function_get_extra_info(duckdb_function_info info);
4497: 
4498: /*!
4499: Get the cast execution mode from the given function info.
4500: 
4501: * @param info The info object.
4502: * @return The cast mode.
4503: */
4504: DUCKDB_API duckdb_cast_mode duckdb_cast_function_get_cast_mode(duckdb_function_info info);
4505: 
4506: /*!
4507: Report that an error has occurred while executing the cast function.
4508: 
4509: * @param info The info object.
4510: * @param error The error message.
4511: */
4512: DUCKDB_API void duckdb_cast_function_set_error(duckdb_function_info info, const char *error);
4513: 
4514: /*!
4515: Report that an error has occurred while executing the cast function, setting the corresponding output row to NULL.
4516: 
4517: * @param info The info object.
4518: * @param error The error message.
4519: * @param row The index of the row within the output vector to set to NULL.
4520: * @param output The output vector.
4521: */
4522: DUCKDB_API void duckdb_cast_function_set_row_error(duckdb_function_info info, const char *error, idx_t row,
4523:                                                    duckdb_vector output);
4524: 
4525: /*!
4526: Registers a cast function within the given connection.
4527: 
4528: * @param con The connection to use.
4529: * @param cast_function The cast function to register.
4530: * @return Whether or not the registration was successful.
4531: */
4532: DUCKDB_API duckdb_state duckdb_register_cast_function(duckdb_connection con, duckdb_cast_function cast_function);
4533: 
4534: /*!
4535: Destroys the cast function object.
4536: 
4537: * @param cast_function The cast function object.
4538: */
4539: DUCKDB_API void duckdb_destroy_cast_function(duckdb_cast_function *cast_function);
4540: 
4541: #endif
4542: 
4543: #ifdef __cplusplus
4544: }
4545: #endif
[end of src/include/duckdb.h]
[start of src/include/duckdb/execution/reservoir_sample.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/reservoir_sample.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/allocator.hpp"
12: #include "duckdb/common/common.hpp"
13: #include "duckdb/common/random_engine.hpp"
14: #include "duckdb/common/types/data_chunk.hpp"
15: #include "duckdb/common/windows_undefs.hpp"
16: 
17: #include "duckdb/common/queue.hpp"
18: 
19: // Originally intended to be the vector size, but in order to run on
20: // vector size = 2, we had to change it.
21: #define FIXED_SAMPLE_SIZE 2048
22: 
23: namespace duckdb {
24: 
25: enum class SampleType : uint8_t { BLOCKING_SAMPLE = 0, RESERVOIR_SAMPLE = 1, RESERVOIR_PERCENTAGE_SAMPLE = 2 };
26: 
27: enum class SamplingState : uint8_t { RANDOM = 0, RESERVOIR = 1 };
28: 
29: class ReservoirRNG : public RandomEngine {
30: public:
31: 	// return type must be called result type to be a valid URNG
32: 	typedef uint32_t result_type;
33: 
34: 	explicit ReservoirRNG(int64_t seed) : RandomEngine(seed) {};
35: 
36: 	result_type operator()() {
37: 		return NextRandomInteger();
38: 	};
39: 
40: 	static constexpr result_type min() {
41: 		return NumericLimits<result_type>::Minimum();
42: 	};
43: 	static constexpr result_type max() {
44: 		return NumericLimits<result_type>::Maximum();
45: 	};
46: };
47: 
48: //! Resevoir sampling is based on the 2005 paper "Weighted Random Sampling" by Efraimidis and Spirakis
49: class BaseReservoirSampling {
50: public:
51: 	explicit BaseReservoirSampling(int64_t seed);
52: 	BaseReservoirSampling();
53: 
54: 	void InitializeReservoirWeights(idx_t cur_size, idx_t sample_size);
55: 
56: 	void SetNextEntry();
57: 
58: 	void ReplaceElementWithIndex(idx_t entry_index, double with_weight, bool pop = true);
59: 	void ReplaceElement(double with_weight = -1);
60: 
61: 	void UpdateMinWeightThreshold();
62: 
63: 	//! Go from the naive sampling to the reservoir sampling
64: 	//! Naive samping will not collect weights, but when we serialize
65: 	//! we need to serialize weights again.
66: 	void FillWeights(SelectionVector &sel, idx_t &sel_size);
67: 
68: 	unique_ptr<BaseReservoirSampling> Copy();
69: 
70: 	//! The random generator
71: 	ReservoirRNG random;
72: 
73: 	//! The next element to sample
74: 	idx_t next_index_to_sample;
75: 	//! The reservoir threshold of the current min entry
76: 	double min_weight_threshold;
77: 	//! The reservoir index of the current min entry
78: 	idx_t min_weighted_entry_index;
79: 	//! The current count towards next index (i.e. we will replace an entry in next_index - current_count tuples)
80: 	//! The number of entries "seen" before choosing one that will go in our reservoir sample.
81: 	idx_t num_entries_to_skip_b4_next_sample;
82: 	//! when collecting a sample in parallel, we want to know how many values each thread has seen
83: 	//! so we can collect the samples from the thread local states in a uniform manner
84: 	idx_t num_entries_seen_total;
85: 	//! Priority queue of [random element, index] for each of the elements in the sample
86: 	std::priority_queue<std::pair<double, idx_t>> reservoir_weights;
87: 
88: 	void Serialize(Serializer &serializer) const;
89: 	static unique_ptr<BaseReservoirSampling> Deserialize(Deserializer &deserializer);
90: 
91: 	static double GetMinWeightFromTuplesSeen(idx_t rows_seen_total);
92: 	// static unordered_map<idx_t, double> tuples_to_min_weight_map;
93: 	// Blocking sample is a virtual class. It should be allowed to see the weights and
94: 	// of tuples in the sample. The blocking sample can then easily maintain statisitcal properties
95: 	// from the sample point of view.
96: 	friend class BlockingSample;
97: };
98: 
99: class BlockingSample {
100: public:
101: 	static constexpr const SampleType TYPE = SampleType::BLOCKING_SAMPLE;
102: 
103: 	unique_ptr<BaseReservoirSampling> base_reservoir_sample;
104: 	//! The sample type
105: 	SampleType type;
106: 	//! has the sample been destroyed due to updates to the referenced table
107: 	bool destroyed;
108: 
109: public:
110: 	explicit BlockingSample(int64_t seed = -1)
111: 	    : base_reservoir_sample(make_uniq<BaseReservoirSampling>(seed)), type(SampleType::BLOCKING_SAMPLE),
112: 	      destroyed(false) {
113: 	}
114: 	virtual ~BlockingSample() {
115: 	}
116: 
117: 	//! Add a chunk of data to the sample
118: 	virtual void AddToReservoir(DataChunk &input) = 0;
119: 	virtual unique_ptr<BlockingSample> Copy() const = 0;
120: 	virtual void Finalize() = 0;
121: 	virtual void Destroy();
122: 
123: 	//! Fetches a chunk from the sample. destroy = true should only be used when
124: 	//! querying from a sample defined in a query and not a duckdb_table_sample.
125: 	virtual unique_ptr<DataChunk> GetChunk() = 0;
126: 
127: 	virtual void Serialize(Serializer &serializer) const;
128: 	static unique_ptr<BlockingSample> Deserialize(Deserializer &deserializer);
129: 
130: 	//! Helper functions needed to merge two reservoirs while respecting weights of sampled rows
131: 	std::pair<double, idx_t> PopFromWeightQueue();
132: 	double GetMinWeightThreshold();
133: 	idx_t GetPriorityQueueSize();
134: 
135: public:
136: 	template <class TARGET>
137: 	TARGET &Cast() {
138: 		if (type != TARGET::TYPE && TARGET::TYPE != SampleType::BLOCKING_SAMPLE) {
139: 			throw InternalException("Failed to cast sample to type - sample type mismatch");
140: 		}
141: 		return reinterpret_cast<TARGET &>(*this);
142: 	}
143: 
144: 	template <class TARGET>
145: 	const TARGET &Cast() const {
146: 		if (type != TARGET::TYPE && TARGET::TYPE != SampleType::BLOCKING_SAMPLE) {
147: 			throw InternalException("Failed to cast sample to type - sample type mismatch");
148: 		}
149: 		return reinterpret_cast<const TARGET &>(*this);
150: 	}
151: };
152: 
153: class ReservoirChunk {
154: public:
155: 	ReservoirChunk() {
156: 	}
157: 
158: 	DataChunk chunk;
159: 	void Serialize(Serializer &serializer) const;
160: 	static unique_ptr<ReservoirChunk> Deserialize(Deserializer &deserializer);
161: 
162: 	unique_ptr<ReservoirChunk> Copy() const;
163: };
164: 
165: struct SelectionVectorHelper {
166: 	SelectionVector sel;
167: 	uint32_t size;
168: };
169: 
170: class ReservoirSample : public BlockingSample {
171: public:
172: 	static constexpr const SampleType TYPE = SampleType::RESERVOIR_SAMPLE;
173: 
174: 	constexpr static idx_t FIXED_SAMPLE_SIZE_MULTIPLIER = 10;
175: 	// size is small enough, then the threshold to switch
176: 	// MinValue between std vec size and fixed sample size.
177: 	// During 'fast' sampling, we want every new vector to have the potential
178: 	// to add to the sample. If the threshold is too far below the standard vector size, then
179: 	// samples in the sample have a higher weight than new samples coming in.
180: 	// i.e during vector_size=2, 2 new samples will not be significant compared 2048 samples from 204800 tuples.
181: 	constexpr static idx_t FAST_TO_SLOW_THRESHOLD = MinValue<idx_t>(STANDARD_VECTOR_SIZE, 60);
182: 
183: 	// If the table has less than 204800 rows, this is the percentage
184: 	// of values we save when serializing/returning a sample.
185: 	constexpr static double SAVE_PERCENTAGE = 0.01;
186: 
187: 	ReservoirSample(Allocator &allocator, idx_t sample_count, int64_t seed = 1);
188: 	explicit ReservoirSample(idx_t sample_count, unique_ptr<ReservoirChunk> = nullptr);
189: 
190: 	//! methods used to help with serializing and deserializing
191: 	void EvictOverBudgetSamples();
192: 	void ExpandSerializedSample();
193: 
194: 	SamplingState GetSamplingState() const;
195: 
196: 	//! Vacuum the Reservoir Sample so it throws away tuples that are not in the
197: 	//! reservoir weights or in the selection vector
198: 	void Vacuum();
199: 
200: 	//! Transform To sample based on reservoir sampling paper
201: 	void ConvertToReservoirSample();
202: 
203: 	//! Get the capactiy of the data chunk reserved for storing samples
204: 	idx_t GetReservoirChunkCapacity() const;
205: 
206: 	//! If for_serialization=true then the sample_chunk is not padded with extra spaces for
207: 	//! future sampling values
208: 	unique_ptr<BlockingSample> Copy() const override;
209: 
210: 	//! create the first chunk called by AddToReservoir()
211: 	idx_t FillReservoir(DataChunk &chunk);
212: 	//! Add a chunk of data to the sample
213: 	void AddToReservoir(DataChunk &input) override;
214: 	//! Merge two Reservoir Samples. Other must be a reservoir sample
215: 	void Merge(unique_ptr<BlockingSample> other);
216: 
217: 	void ShuffleSel(SelectionVector &sel, idx_t range, idx_t size) const;
218: 
219: 	//! Update the sample by pushing new sample rows to the end of the sample_chunk.
220: 	//! The new sample rows are the tuples rows resulting from applying sel to other
221: 	void UpdateSampleAppend(DataChunk &this_, DataChunk &other, SelectionVector &other_sel, idx_t append_count) const;
222: 
223: 	idx_t GetTuplesSeen() const;
224: 	idx_t NumSamplesCollected() const;
225: 	idx_t GetActiveSampleCount() const;
226: 	static bool ValidSampleType(const LogicalType &type);
227: 
228: 	// get the chunk from Reservoir chunk
229: 	DataChunk &Chunk();
230: 
231: 	//! Fetches a chunk from the sample. Note that this method is destructive and should only be used after the
232: 	//! sample is completely built.
233: 	// unique_ptr<DataChunk> GetChunkAndDestroy() override;
234: 	unique_ptr<DataChunk> GetChunk() override;
235: 	void Destroy() override;
236: 	void Finalize() override;
237: 	void Verify();
238: 
239: 	idx_t GetSampleCount();
240: 
241: 	// map is [index in input chunk] -> [index in sample chunk]. Both are zero-based
242: 	// [index in sample chunk] is incremented by 1
243: 	// index in input chunk have random values, however, they are increasing.
244: 	// The base_reservoir_sampling gets updated however, so the indexes point to (sample_chunk_offset +
245: 	// index_in_sample_chunk) this data is used to make a selection vector to copy samples from the input chunk to the
246: 	// sample chunk
247: 	//! Get indexes from current sample that can be replaced.
248: 	SelectionVectorHelper GetReplacementIndexes(idx_t sample_chunk_offset, idx_t theoretical_chunk_length);
249: 
250: 	void Serialize(Serializer &serializer) const override;
251: 	static unique_ptr<BlockingSample> Deserialize(Deserializer &deserializer);
252: 
253: private:
254: 	// when we serialize, we may have collected too many samples since we fill a standard vector size, then
255: 	// truncate if the table is still <=204800 values. The problem is, in our weights, we store indexes into
256: 	// the selection vector. If throw away values at selection vector index i = 5 , we need to update all indexes
257: 	// i > 5. Otherwise we will have indexes in the weights that are greater than the length of our sample.
258: 	void NormalizeWeights();
259: 
260: 	SelectionVectorHelper GetReplacementIndexesSlow(const idx_t sample_chunk_offset, const idx_t chunk_length);
261: 	SelectionVectorHelper GetReplacementIndexesFast(const idx_t sample_chunk_offset, const idx_t chunk_length);
262: 	void SimpleMerge(ReservoirSample &other);
263: 	void WeightedMerge(ReservoirSample &other_sample);
264: 
265: 	// Helper methods for Shrink().
266: 	// Shrink has different logic depending on if the Reservoir sample is still in
267: 	// "Random" mode or in "reservoir" mode. This function creates a new sample chunk
268: 	// to copy the old sample chunk into
269: 	unique_ptr<ReservoirChunk> CreateNewSampleChunk(vector<LogicalType> &types, idx_t size) const;
270: 
271: 	// Get a vector where each index is a random int in the range 0, size.
272: 	// This is used to shuffle selection vector indexes
273: 	vector<uint32_t> GetRandomizedVector(uint32_t range, uint32_t size) const;
274: 
275: 	idx_t sample_count;
276: 	Allocator &allocator;
277: 	unique_ptr<ReservoirChunk> reservoir_chunk;
278: 	bool stats_sample;
279: 	SelectionVector sel;
280: 	idx_t sel_size;
281: };
282: 
283: //! The reservoir sample sample_size class maintains a streaming sample of variable size
284: class ReservoirSamplePercentage : public BlockingSample {
285: 	constexpr static idx_t RESERVOIR_THRESHOLD = 100000;
286: 
287: public:
288: 	static constexpr const SampleType TYPE = SampleType::RESERVOIR_PERCENTAGE_SAMPLE;
289: 
290: 	ReservoirSamplePercentage(Allocator &allocator, double percentage, int64_t seed = -1);
291: 	ReservoirSamplePercentage(double percentage, int64_t seed, idx_t reservoir_sample_size);
292: 	explicit ReservoirSamplePercentage(double percentage, int64_t seed = -1);
293: 
294: 	//! Add a chunk of data to the sample
295: 	void AddToReservoir(DataChunk &input) override;
296: 
297: 	unique_ptr<BlockingSample> Copy() const override;
298: 
299: 	//! Fetches a chunk from the sample. If destory = true this method is descructive
300: 	unique_ptr<DataChunk> GetChunk() override;
301: 	void Finalize() override;
302: 
303: 	void Serialize(Serializer &serializer) const override;
304: 	static unique_ptr<BlockingSample> Deserialize(Deserializer &deserializer);
305: 
306: private:
307: 	Allocator &allocator;
308: 	//! The sample_size to sample
309: 	double sample_percentage;
310: 	//! The fixed sample size of the sub-reservoirs
311: 	idx_t reservoir_sample_size;
312: 
313: 	//! The current sample
314: 	unique_ptr<ReservoirSample> current_sample;
315: 
316: 	//! The set of finished samples of the reservoir sample
317: 	vector<unique_ptr<ReservoirSample>> finished_samples;
318: 
319: 	//! The amount of tuples that have been processed so far (not put in the reservoir, just processed)
320: 	idx_t current_count = 0;
321: 	//! Whether or not the stream is finalized. The stream is automatically finalized on the first call to
322: 	//! GetChunkAndShrink();
323: 	bool is_finalized;
324: };
325: 
326: } // namespace duckdb
[end of src/include/duckdb/execution/reservoir_sample.hpp]
[start of src/include/duckdb/main/capi/header_generation/header_base.hpp.template]
1: // !!!!!!!
2: // WARNING: this file is used for header generation by scripts/generate_c_api.py after modifying the code below, rerun
3: //          the script to apply changes to the generated files
4: // !!!!!!!
5: 
6: // DUCKDB_START_OF_HEADER
7: 
8: #pragma once
9: 
10: //! duplicate of duckdb/main/winapi.hpp
11: #ifndef DUCKDB_API
12: #ifdef _WIN32
13: #ifdef DUCKDB_STATIC_BUILD
14: #define DUCKDB_API
15: #else
16: #if defined(DUCKDB_BUILD_LIBRARY) && !defined(DUCKDB_BUILD_LOADABLE_EXTENSION)
17: #define DUCKDB_API __declspec(dllexport)
18: #else
19: #define DUCKDB_API __declspec(dllimport)
20: #endif
21: #endif
22: #else
23: #define DUCKDB_API
24: #endif
25: #endif
26: 
27: //! duplicate of duckdb/main/winapi.hpp
28: #ifndef DUCKDB_EXTENSION_API
29: #ifdef _WIN32
30: #ifdef DUCKDB_STATIC_BUILD
31: #define DUCKDB_EXTENSION_API
32: #else
33: #ifdef DUCKDB_BUILD_LOADABLE_EXTENSION
34: #define DUCKDB_EXTENSION_API __declspec(dllexport)
35: #else
36: #define DUCKDB_EXTENSION_API
37: #endif
38: #endif
39: #else
40: #define DUCKDB_EXTENSION_API __attribute__((visibility("default")))
41: #endif
42: #endif
43: 
44: #include <stdbool.h>
45: #include <stdint.h>
46: #include <stddef.h>
47: 
48: #ifdef __cplusplus
49: extern "C" {
50: #endif
51: 
52: //===--------------------------------------------------------------------===//
53: // Enums
54: //===--------------------------------------------------------------------===//
55: // WARNING: the numbers of these enums should not be changed, as changing the numbers breaks ABI compatibility
56: // Always add enums at the END of the enum
57: //! An enum over DuckDB's internal types.
58: typedef enum DUCKDB_TYPE {
59: 	DUCKDB_TYPE_INVALID = 0,
60: 	// bool
61: 	DUCKDB_TYPE_BOOLEAN = 1,
62: 	// int8_t
63: 	DUCKDB_TYPE_TINYINT = 2,
64: 	// int16_t
65: 	DUCKDB_TYPE_SMALLINT = 3,
66: 	// int32_t
67: 	DUCKDB_TYPE_INTEGER = 4,
68: 	// int64_t
69: 	DUCKDB_TYPE_BIGINT = 5,
70: 	// uint8_t
71: 	DUCKDB_TYPE_UTINYINT = 6,
72: 	// uint16_t
73: 	DUCKDB_TYPE_USMALLINT = 7,
74: 	// uint32_t
75: 	DUCKDB_TYPE_UINTEGER = 8,
76: 	// uint64_t
77: 	DUCKDB_TYPE_UBIGINT = 9,
78: 	// float
79: 	DUCKDB_TYPE_FLOAT = 10,
80: 	// double
81: 	DUCKDB_TYPE_DOUBLE = 11,
82: 	// duckdb_timestamp (microseconds)
83: 	DUCKDB_TYPE_TIMESTAMP = 12,
84: 	// duckdb_date
85: 	DUCKDB_TYPE_DATE = 13,
86: 	// duckdb_time
87: 	DUCKDB_TYPE_TIME = 14,
88: 	// duckdb_interval
89: 	DUCKDB_TYPE_INTERVAL = 15,
90: 	// duckdb_hugeint
91: 	DUCKDB_TYPE_HUGEINT = 16,
92: 	// duckdb_uhugeint
93: 	DUCKDB_TYPE_UHUGEINT = 32,
94: 	// const char*
95: 	DUCKDB_TYPE_VARCHAR = 17,
96: 	// duckdb_blob
97: 	DUCKDB_TYPE_BLOB = 18,
98: 	// duckdb_decimal
99: 	DUCKDB_TYPE_DECIMAL = 19,
100: 	// duckdb_timestamp_s (seconds)
101: 	DUCKDB_TYPE_TIMESTAMP_S = 20,
102: 	// duckdb_timestamp_ms (milliseconds)
103: 	DUCKDB_TYPE_TIMESTAMP_MS = 21,
104: 	// duckdb_timestamp_ns (nanoseconds)
105: 	DUCKDB_TYPE_TIMESTAMP_NS = 22,
106: 	// enum type, only useful as logical type
107: 	DUCKDB_TYPE_ENUM = 23,
108: 	// list type, only useful as logical type
109: 	DUCKDB_TYPE_LIST = 24,
110: 	// struct type, only useful as logical type
111: 	DUCKDB_TYPE_STRUCT = 25,
112: 	// map type, only useful as logical type
113: 	DUCKDB_TYPE_MAP = 26,
114: 	// duckdb_array, only useful as logical type
115: 	DUCKDB_TYPE_ARRAY = 33,
116: 	// duckdb_hugeint
117: 	DUCKDB_TYPE_UUID = 27,
118: 	// union type, only useful as logical type
119: 	DUCKDB_TYPE_UNION = 28,
120: 	// duckdb_bit
121: 	DUCKDB_TYPE_BIT = 29,
122: 	// duckdb_time_tz
123: 	DUCKDB_TYPE_TIME_TZ = 30,
124: 	// duckdb_timestamp (microseconds)
125: 	DUCKDB_TYPE_TIMESTAMP_TZ = 31,
126: 	// ANY type
127: 	DUCKDB_TYPE_ANY = 34,
128: 	// duckdb_varint
129: 	DUCKDB_TYPE_VARINT = 35,
130: 	// SQLNULL type
131: 	DUCKDB_TYPE_SQLNULL = 36,
132: } duckdb_type;
133: //! An enum over the returned state of different functions.
134: typedef enum duckdb_state { DuckDBSuccess = 0, DuckDBError = 1 } duckdb_state;
135: //! An enum over the pending state of a pending query result.
136: typedef enum duckdb_pending_state {
137: 	DUCKDB_PENDING_RESULT_READY = 0,
138: 	DUCKDB_PENDING_RESULT_NOT_READY = 1,
139: 	DUCKDB_PENDING_ERROR = 2,
140: 	DUCKDB_PENDING_NO_TASKS_AVAILABLE = 3
141: } duckdb_pending_state;
142: //! An enum over DuckDB's different result types.
143: typedef enum duckdb_result_type {
144: 	DUCKDB_RESULT_TYPE_INVALID = 0,
145: 	DUCKDB_RESULT_TYPE_CHANGED_ROWS = 1,
146: 	DUCKDB_RESULT_TYPE_NOTHING = 2,
147: 	DUCKDB_RESULT_TYPE_QUERY_RESULT = 3,
148: } duckdb_result_type;
149: //! An enum over DuckDB's different statement types.
150: typedef enum duckdb_statement_type {
151: 	DUCKDB_STATEMENT_TYPE_INVALID = 0,
152: 	DUCKDB_STATEMENT_TYPE_SELECT = 1,
153: 	DUCKDB_STATEMENT_TYPE_INSERT = 2,
154: 	DUCKDB_STATEMENT_TYPE_UPDATE = 3,
155: 	DUCKDB_STATEMENT_TYPE_EXPLAIN = 4,
156: 	DUCKDB_STATEMENT_TYPE_DELETE = 5,
157: 	DUCKDB_STATEMENT_TYPE_PREPARE = 6,
158: 	DUCKDB_STATEMENT_TYPE_CREATE = 7,
159: 	DUCKDB_STATEMENT_TYPE_EXECUTE = 8,
160: 	DUCKDB_STATEMENT_TYPE_ALTER = 9,
161: 	DUCKDB_STATEMENT_TYPE_TRANSACTION = 10,
162: 	DUCKDB_STATEMENT_TYPE_COPY = 11,
163: 	DUCKDB_STATEMENT_TYPE_ANALYZE = 12,
164: 	DUCKDB_STATEMENT_TYPE_VARIABLE_SET = 13,
165: 	DUCKDB_STATEMENT_TYPE_CREATE_FUNC = 14,
166: 	DUCKDB_STATEMENT_TYPE_DROP = 15,
167: 	DUCKDB_STATEMENT_TYPE_EXPORT = 16,
168: 	DUCKDB_STATEMENT_TYPE_PRAGMA = 17,
169: 	DUCKDB_STATEMENT_TYPE_VACUUM = 18,
170: 	DUCKDB_STATEMENT_TYPE_CALL = 19,
171: 	DUCKDB_STATEMENT_TYPE_SET = 20,
172: 	DUCKDB_STATEMENT_TYPE_LOAD = 21,
173: 	DUCKDB_STATEMENT_TYPE_RELATION = 22,
174: 	DUCKDB_STATEMENT_TYPE_EXTENSION = 23,
175: 	DUCKDB_STATEMENT_TYPE_LOGICAL_PLAN = 24,
176: 	DUCKDB_STATEMENT_TYPE_ATTACH = 25,
177: 	DUCKDB_STATEMENT_TYPE_DETACH = 26,
178: 	DUCKDB_STATEMENT_TYPE_MULTI = 27,
179: } duckdb_statement_type;
180: //! An enum over DuckDB's different result types.
181: typedef enum duckdb_error_type {
182: 	DUCKDB_ERROR_INVALID = 0,
183: 	DUCKDB_ERROR_OUT_OF_RANGE = 1,
184: 	DUCKDB_ERROR_CONVERSION = 2,
185: 	DUCKDB_ERROR_UNKNOWN_TYPE = 3,
186: 	DUCKDB_ERROR_DECIMAL = 4,
187: 	DUCKDB_ERROR_MISMATCH_TYPE = 5,
188: 	DUCKDB_ERROR_DIVIDE_BY_ZERO = 6,
189: 	DUCKDB_ERROR_OBJECT_SIZE = 7,
190: 	DUCKDB_ERROR_INVALID_TYPE = 8,
191: 	DUCKDB_ERROR_SERIALIZATION = 9,
192: 	DUCKDB_ERROR_TRANSACTION = 10,
193: 	DUCKDB_ERROR_NOT_IMPLEMENTED = 11,
194: 	DUCKDB_ERROR_EXPRESSION = 12,
195: 	DUCKDB_ERROR_CATALOG = 13,
196: 	DUCKDB_ERROR_PARSER = 14,
197: 	DUCKDB_ERROR_PLANNER = 15,
198: 	DUCKDB_ERROR_SCHEDULER = 16,
199: 	DUCKDB_ERROR_EXECUTOR = 17,
200: 	DUCKDB_ERROR_CONSTRAINT = 18,
201: 	DUCKDB_ERROR_INDEX = 19,
202: 	DUCKDB_ERROR_STAT = 20,
203: 	DUCKDB_ERROR_CONNECTION = 21,
204: 	DUCKDB_ERROR_SYNTAX = 22,
205: 	DUCKDB_ERROR_SETTINGS = 23,
206: 	DUCKDB_ERROR_BINDER = 24,
207: 	DUCKDB_ERROR_NETWORK = 25,
208: 	DUCKDB_ERROR_OPTIMIZER = 26,
209: 	DUCKDB_ERROR_NULL_POINTER = 27,
210: 	DUCKDB_ERROR_IO = 28,
211: 	DUCKDB_ERROR_INTERRUPT = 29,
212: 	DUCKDB_ERROR_FATAL = 30,
213: 	DUCKDB_ERROR_INTERNAL = 31,
214: 	DUCKDB_ERROR_INVALID_INPUT = 32,
215: 	DUCKDB_ERROR_OUT_OF_MEMORY = 33,
216: 	DUCKDB_ERROR_PERMISSION = 34,
217: 	DUCKDB_ERROR_PARAMETER_NOT_RESOLVED = 35,
218: 	DUCKDB_ERROR_PARAMETER_NOT_ALLOWED = 36,
219: 	DUCKDB_ERROR_DEPENDENCY = 37,
220: 	DUCKDB_ERROR_HTTP = 38,
221: 	DUCKDB_ERROR_MISSING_EXTENSION = 39,
222: 	DUCKDB_ERROR_AUTOLOAD = 40,
223: 	DUCKDB_ERROR_SEQUENCE = 41,
224: 	DUCKDB_INVALID_CONFIGURATION = 42
225: } duckdb_error_type;
226: //! An enum over DuckDB's different cast modes.
227: typedef enum duckdb_cast_mode {
228: 	DUCKDB_CAST_NORMAL = 0,
229: 	DUCKDB_CAST_TRY = 1
230: } duckdb_cast_mode;
231: 
232: //===--------------------------------------------------------------------===//
233: // General type definitions
234: //===--------------------------------------------------------------------===//
235: 
236: //! DuckDB's index type.
237: typedef uint64_t idx_t;
238: 
239: //! The callback that will be called to destroy data, e.g.,
240: //! bind data (if any), init data (if any), extra data for replacement scans (if any)
241: typedef void (*duckdb_delete_callback_t)(void *data);
242: 
243: //! Used for threading, contains a task state. Must be destroyed with `duckdb_destroy_state`.
244: typedef void *duckdb_task_state;
245: 
246: //===--------------------------------------------------------------------===//
247: // Types (no explicit freeing)
248: //===--------------------------------------------------------------------===//
249: 
250: //! Days are stored as days since 1970-01-01
251: //! Use the duckdb_from_date/duckdb_to_date function to extract individual information
252: typedef struct {
253: 	int32_t days;
254: } duckdb_date;
255: typedef struct {
256: 	int32_t year;
257: 	int8_t month;
258: 	int8_t day;
259: } duckdb_date_struct;
260: 
261: //! Time is stored as microseconds since 00:00:00
262: //! Use the duckdb_from_time/duckdb_to_time function to extract individual information
263: typedef struct {
264: 	int64_t micros;
265: } duckdb_time;
266: typedef struct {
267: 	int8_t hour;
268: 	int8_t min;
269: 	int8_t sec;
270: 	int32_t micros;
271: } duckdb_time_struct;
272: 
273: //! TIME_TZ is stored as 40 bits for int64_t micros, and 24 bits for int32_t offset
274: typedef struct {
275: 	uint64_t bits;
276: } duckdb_time_tz;
277: typedef struct {
278: 	duckdb_time_struct time;
279: 	int32_t offset;
280: } duckdb_time_tz_struct;
281: 
282: //! TIMESTAMP values are stored as microseconds since 1970-01-01.
283: //! Use the duckdb_from_timestamp and duckdb_to_timestamp functions to extract individual information.
284: typedef struct {
285: 	int64_t micros;
286: } duckdb_timestamp;
287: 
288: //! TIMESTAMP_S values are stored as seconds since 1970-01-01.
289: typedef struct {
290: 	int64_t seconds;
291: } duckdb_timestamp_s;
292: 
293: //! TIMESTAMP_MS values are stored as milliseconds since 1970-01-01.
294: typedef struct {
295: 	int64_t millis;
296: } duckdb_timestamp_ms;
297: 
298: //! TIMESTAMP_NS values are stored as nanoseconds since 1970-01-01.
299: typedef struct {
300: 	int64_t nanos;
301: } duckdb_timestamp_ns;
302: 
303: typedef struct {
304: 	duckdb_date_struct date;
305: 	duckdb_time_struct time;
306: } duckdb_timestamp_struct;
307: 
308: typedef struct {
309: 	int32_t months;
310: 	int32_t days;
311: 	int64_t micros;
312: } duckdb_interval;
313: 
314: //! Hugeints are composed of a (lower, upper) component
315: //! The value of the hugeint is upper * 2^64 + lower
316: //! For easy usage, the functions duckdb_hugeint_to_double/duckdb_double_to_hugeint are recommended
317: typedef struct {
318: 	uint64_t lower;
319: 	int64_t upper;
320: } duckdb_hugeint;
321: typedef struct {
322: 	uint64_t lower;
323: 	uint64_t upper;
324: } duckdb_uhugeint;
325: 
326: //! Decimals are composed of a width and a scale, and are stored in a hugeint
327: typedef struct {
328: 	uint8_t width;
329: 	uint8_t scale;
330: 	duckdb_hugeint value;
331: } duckdb_decimal;
332: 
333: //! A type holding information about the query execution progress
334: typedef struct {
335: 	double percentage;
336: 	uint64_t rows_processed;
337: 	uint64_t total_rows_to_process;
338: } duckdb_query_progress_type;
339: 
340: //! The internal representation of a VARCHAR (string_t). If the VARCHAR does not
341: //! exceed 12 characters, then we inline it. Otherwise, we inline a prefix for faster
342: //! string comparisons and store a pointer to the remaining characters. This is a non-
343: //! owning structure, i.e., it does not have to be freed.
344: typedef struct {
345: 	union {
346: 		struct {
347: 			uint32_t length;
348: 			char prefix[4];
349: 			char *ptr;
350: 		} pointer;
351: 		struct {
352: 			uint32_t length;
353: 			char inlined[12];
354: 		} inlined;
355: 	} value;
356: } duckdb_string_t;
357: 
358: //! The internal representation of a list metadata entry contains the list's offset in
359: //! the child vector, and its length. The parent vector holds these metadata entries,
360: //! whereas the child vector holds the data
361: typedef struct {
362: 	uint64_t offset;
363: 	uint64_t length;
364: } duckdb_list_entry;
365: 
366: //! A column consists of a pointer to its internal data. Don't operate on this type directly.
367: //! Instead, use functions such as duckdb_column_data, duckdb_nullmask_data,
368: //! duckdb_column_type, and duckdb_column_name, which take the result and the column index
369: //! as their parameters
370: typedef struct {
371: 	// deprecated, use duckdb_column_data
372: 	void *deprecated_data;
373: 	// deprecated, use duckdb_nullmask_data
374: 	bool *deprecated_nullmask;
375: 	// deprecated, use duckdb_column_type
376: 	duckdb_type deprecated_type;
377: 	// deprecated, use duckdb_column_name
378: 	char *deprecated_name;
379: 	void *internal_data;
380: } duckdb_column;
381: 
382: //! A vector to a specified column in a data chunk. Lives as long as the
383: //! data chunk lives, i.e., must not be destroyed.
384: typedef struct _duckdb_vector {
385: 	void *internal_ptr;
386: } * duckdb_vector;
387: 
388: //===--------------------------------------------------------------------===//
389: // Types (explicit freeing/destroying)
390: //===--------------------------------------------------------------------===//
391: 
392: //! Strings are composed of a char pointer and a size. You must free string.data
393: //! with `duckdb_free`.
394: typedef struct {
395: 	char *data;
396: 	idx_t size;
397: } duckdb_string;
398: 
399: //! BLOBs are composed of a byte pointer and a size. You must free blob.data
400: //! with `duckdb_free`.
401: typedef struct {
402: 	void *data;
403: 	idx_t size;
404: } duckdb_blob;
405: 
406: //! BITs are composed of a byte pointer and a size.
407: //! BIT byte data has 0 to 7 bits of padding.
408: //! The first byte contains the number of padding bits.
409: //! This number of bits of the second byte are set to 1, starting from the MSB.
410: //! You must free `data` with `duckdb_free`.
411: typedef struct {
412: 	uint8_t *data;
413: 	idx_t size;
414: } duckdb_bit;
415: 
416: //! VARINTs are composed of a byte pointer, a size, and an is_negative bool.
417: //! The absolute value of the number is stored in `data` in little endian format.
418: //! You must free `data` with `duckdb_free`.
419: typedef struct {
420: 	uint8_t *data;
421: 	idx_t size;
422: 	bool is_negative;
423: } duckdb_varint;
424: 
425: //! A query result consists of a pointer to its internal data.
426: //! Must be freed with 'duckdb_destroy_result'.
427: typedef struct {
428: 	// deprecated, use duckdb_column_count
429: 	idx_t deprecated_column_count;
430: 	// deprecated, use duckdb_row_count
431: 	idx_t deprecated_row_count;
432: 	// deprecated, use duckdb_rows_changed
433: 	idx_t deprecated_rows_changed;
434: 	// deprecated, use duckdb_column_*-family of functions
435: 	duckdb_column *deprecated_columns;
436: 	// deprecated, use duckdb_result_error
437: 	char *deprecated_error_message;
438: 	void *internal_data;
439: } duckdb_result;
440: 
441: //! A database instance cache object. Must be destroyed with `duckdb_destroy_instance_cache`.
442: typedef struct _duckdb_instance_cache {
443: 	void *internal_ptr;
444: } * duckdb_instance_cache;
445: 
446: //! A database object. Must be closed with `duckdb_close`.
447: typedef struct _duckdb_database {
448: 	void *internal_ptr;
449: } * duckdb_database;
450: 
451: //! A connection to a duckdb database. Must be closed with `duckdb_disconnect`.
452: typedef struct _duckdb_connection {
453: 	void *internal_ptr;
454: } * duckdb_connection;
455: 
456: //! A prepared statement is a parameterized query that allows you to bind parameters to it.
457: //! Must be destroyed with `duckdb_destroy_prepare`.
458: typedef struct _duckdb_prepared_statement {
459: 	void *internal_ptr;
460: } * duckdb_prepared_statement;
461: 
462: //! Extracted statements. Must be destroyed with `duckdb_destroy_extracted`.
463: typedef struct _duckdb_extracted_statements {
464: 	void *internal_ptr;
465: } * duckdb_extracted_statements;
466: 
467: //! The pending result represents an intermediate structure for a query that is not yet fully executed.
468: //! Must be destroyed with `duckdb_destroy_pending`.
469: typedef struct _duckdb_pending_result {
470: 	void *internal_ptr;
471: } * duckdb_pending_result;
472: 
473: //! The appender enables fast data loading into DuckDB.
474: //! Must be destroyed with `duckdb_appender_destroy`.
475: typedef struct _duckdb_appender {
476: 	void *internal_ptr;
477: } * duckdb_appender;
478: 
479: //! The table description allows querying info about the table.
480: //! Must be destroyed with `duckdb_table_description_destroy`.
481: typedef struct _duckdb_table_description {
482: 	void *internal_ptr;
483: } * duckdb_table_description;
484: 
485: //! Can be used to provide start-up options for the DuckDB instance.
486: //! Must be destroyed with `duckdb_destroy_config`.
487: typedef struct _duckdb_config {
488: 	void *internal_ptr;
489: } * duckdb_config;
490: 
491: //! Holds an internal logical type.
492: //! Must be destroyed with `duckdb_destroy_logical_type`.
493: typedef struct _duckdb_logical_type {
494: 	void *internal_ptr;
495: } * duckdb_logical_type;
496: 
497: //! Holds extra information used when registering a custom logical type.
498: //! Reserved for future use.
499: typedef struct _duckdb_create_type_info {
500: 	void *internal_ptr;
501: } * duckdb_create_type_info;
502: 
503: //! Contains a data chunk from a duckdb_result.
504: //! Must be destroyed with `duckdb_destroy_data_chunk`.
505: typedef struct _duckdb_data_chunk {
506: 	void *internal_ptr;
507: } * duckdb_data_chunk;
508: 
509: //! Holds a DuckDB value, which wraps a type.
510: //! Must be destroyed with `duckdb_destroy_value`.
511: typedef struct _duckdb_value {
512: 	void *internal_ptr;
513: } * duckdb_value;
514: 
515: //! Holds a recursive tree that matches the query plan.
516: typedef struct _duckdb_profiling_info {
517: 	void *internal_ptr;
518: } * duckdb_profiling_info;
519: 
520: //===--------------------------------------------------------------------===//
521: // C API Extension info
522: //===--------------------------------------------------------------------===//
523: //! Holds state during the C API extension intialization process
524: typedef struct _duckdb_extension_info {
525: 	void *internal_ptr;
526: } * duckdb_extension_info;
527: 
528: //===--------------------------------------------------------------------===//
529: // Function types
530: //===--------------------------------------------------------------------===//
531: //! Additional function info. When setting this info, it is necessary to pass a destroy-callback function.
532: typedef struct _duckdb_function_info {
533: 	void *internal_ptr;
534: } * duckdb_function_info;
535: 
536: //===--------------------------------------------------------------------===//
537: // Scalar function types
538: //===--------------------------------------------------------------------===//
539: //! A scalar function. Must be destroyed with `duckdb_destroy_scalar_function`.
540: typedef struct _duckdb_scalar_function {
541: 	void *internal_ptr;
542: } * duckdb_scalar_function;
543: 
544: //! A scalar function set. Must be destroyed with `duckdb_destroy_scalar_function_set`.
545: typedef struct _duckdb_scalar_function_set {
546: 	void *internal_ptr;
547: } * duckdb_scalar_function_set;
548: 
549: //! The main function of the scalar function.
550: typedef void (*duckdb_scalar_function_t)(duckdb_function_info info, duckdb_data_chunk input, duckdb_vector output);
551: 
552: //===--------------------------------------------------------------------===//
553: // Aggregate function types
554: //===--------------------------------------------------------------------===//
555: //! An aggregate function. Must be destroyed with `duckdb_destroy_aggregate_function`.
556: typedef struct _duckdb_aggregate_function {
557: 	void *internal_ptr;
558: } * duckdb_aggregate_function;
559: 
560: //! A aggregate function set. Must be destroyed with `duckdb_destroy_aggregate_function_set`.
561: typedef struct _duckdb_aggregate_function_set {
562: 	void *internal_ptr;
563: } * duckdb_aggregate_function_set;
564: 
565: //! Aggregate state
566: typedef struct _duckdb_aggregate_state {
567: 	void *internal_ptr;
568: } * duckdb_aggregate_state;
569: 
570: //! Returns the aggregate state size
571: typedef idx_t (*duckdb_aggregate_state_size)(duckdb_function_info info);
572: //! Initialize the aggregate state
573: typedef void (*duckdb_aggregate_init_t)(duckdb_function_info info, duckdb_aggregate_state state);
574: //! Destroy aggregate state (optional)
575: typedef void (*duckdb_aggregate_destroy_t)(duckdb_aggregate_state *states, idx_t count);
576: //! Update a set of aggregate states with new values
577: typedef void (*duckdb_aggregate_update_t)(duckdb_function_info info, duckdb_data_chunk input,
578:                                           duckdb_aggregate_state *states);
579: //! Combine aggregate states
580: typedef void (*duckdb_aggregate_combine_t)(duckdb_function_info info, duckdb_aggregate_state *source,
581:                                            duckdb_aggregate_state *target, idx_t count);
582: //! Finalize aggregate states into a result vector
583: typedef void (*duckdb_aggregate_finalize_t)(duckdb_function_info info, duckdb_aggregate_state *source,
584:                                             duckdb_vector result, idx_t count, idx_t offset);
585: 
586: //===--------------------------------------------------------------------===//
587: // Table function types
588: //===--------------------------------------------------------------------===//
589: 
590: //! A table function. Must be destroyed with `duckdb_destroy_table_function`.
591: typedef struct _duckdb_table_function {
592: 	void *internal_ptr;
593: } * duckdb_table_function;
594: 
595: //! The bind info of the function. When setting this info, it is necessary to pass a destroy-callback function.
596: typedef struct _duckdb_bind_info {
597: 	void *internal_ptr;
598: } * duckdb_bind_info;
599: 
600: //! Additional function init info. When setting this info, it is necessary to pass a destroy-callback function.
601: typedef struct _duckdb_init_info {
602: 	void *internal_ptr;
603: } * duckdb_init_info;
604: 
605: //! The bind function of the table function.
606: typedef void (*duckdb_table_function_bind_t)(duckdb_bind_info info);
607: 
608: //! The (possibly thread-local) init function of the table function.
609: typedef void (*duckdb_table_function_init_t)(duckdb_init_info info);
610: 
611: //! The main function of the table function.
612: typedef void (*duckdb_table_function_t)(duckdb_function_info info, duckdb_data_chunk output);
613: 
614: //===--------------------------------------------------------------------===//
615: // Cast types
616: //===--------------------------------------------------------------------===//
617: 
618: //! A cast function. Must be destroyed with `duckdb_destroy_cast_function`.
619: typedef struct _duckdb_cast_function {
620: 	void *internal_ptr;
621: } * duckdb_cast_function;
622: 
623: typedef bool (*duckdb_cast_function_t)(duckdb_function_info info, idx_t count, duckdb_vector input, duckdb_vector output);
624: 
625: //===--------------------------------------------------------------------===//
626: // Replacement scan types
627: //===--------------------------------------------------------------------===//
628: 
629: //! Additional replacement scan info. When setting this info, it is necessary to pass a destroy-callback function.
630: typedef struct _duckdb_replacement_scan_info {
631: 	void *internal_ptr;
632: } * duckdb_replacement_scan_info;
633: 
634: //! A replacement scan function that can be added to a database.
635: typedef void (*duckdb_replacement_callback_t)(duckdb_replacement_scan_info info, const char *table_name, void *data);
636: 
637: //===--------------------------------------------------------------------===//
638: // Arrow-related types
639: //===--------------------------------------------------------------------===//
640: 
641: //! Holds an arrow query result. Must be destroyed with `duckdb_destroy_arrow`.
642: typedef struct _duckdb_arrow {
643: 	void *internal_ptr;
644: } * duckdb_arrow;
645: 
646: //! Holds an arrow array stream. Must be destroyed with `duckdb_destroy_arrow_stream`.
647: typedef struct _duckdb_arrow_stream {
648: 	void *internal_ptr;
649: } * duckdb_arrow_stream;
650: 
651: //! Holds an arrow schema. Remember to release the respective ArrowSchema object.
652: typedef struct _duckdb_arrow_schema {
653: 	void *internal_ptr;
654: } * duckdb_arrow_schema;
655: 
656: //! Holds an arrow array. Remember to release the respective ArrowArray object.
657: typedef struct _duckdb_arrow_array {
658: 	void *internal_ptr;
659: } * duckdb_arrow_array;
660: 
661: //===--------------------------------------------------------------------===//
662: // DuckDB extension access
663: //===--------------------------------------------------------------------===//
664: //! Passed to C API extension as parameter to the entrypoint
665: struct duckdb_extension_access {
666: 	//! Indicate that an error has occurred
667: 	void (*set_error)(duckdb_extension_info info, const char *error);
668: 	//! Fetch the database from duckdb to register extensions to
669: 	duckdb_database *(*get_database)(duckdb_extension_info info);
670: 	//! Fetch the API
671: 	const void *(*get_api)(duckdb_extension_info info, const char *version);
672: };
673: 
674: // FILE_CONTENT_SECTION
675: 
676: #ifdef __cplusplus
677: }
678: #endif
[end of src/include/duckdb/main/capi/header_generation/header_base.hpp.template]
[start of src/include/duckdb/main/extension_entries.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/main/extension_entries.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/unordered_map.hpp"
12: #include "duckdb/common/enums/catalog_type.hpp"
13: 
14: // NOTE: this file is generated by scripts/generate_extensions_function.py.
15: // Example usage to refresh one extension (replace "icu" with the desired extension):
16: // GENERATE_EXTENSION_ENTRIES=1 make debug
17: // python3 scripts/generate_extensions_function.py --extensions icu --shell build/debug/duckdb --extension_dir
18: // build/debug
19: 
20: // Check out the check-load-install-extensions  job in .github/workflows/LinuxRelease.yml for more details
21: 
22: namespace duckdb {
23: 
24: struct ExtensionEntry {
25: 	char name[48];
26: 	char extension[48];
27: };
28: 
29: struct ExtensionFunctionEntry {
30: 	char name[48];
31: 	char extension[48];
32: 	CatalogType type;
33: };
34: 
35: struct ExtensionFunctionOverloadEntry {
36: 	char name[48];
37: 	char extension[48];
38: 	CatalogType type;
39: 	char signature[96];
40: };
41: 
42: static constexpr ExtensionFunctionEntry EXTENSION_FUNCTIONS[] = {
43:     {"!__postfix", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
44:     {"&", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
45:     {"&&", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
46:     {"**", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
47:     {"->>", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
48:     {"<->", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
49:     {"<<", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
50:     {"<<=", "inet", CatalogType::SCALAR_FUNCTION_ENTRY},
51:     {"<=>", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
52:     {"<@", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
53:     {">>", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
54:     {">>=", "inet", CatalogType::SCALAR_FUNCTION_ENTRY},
55:     {"@", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
56:     {"@>", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
57:     {"^", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
58:     {"^@", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
59:     {"abs", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
60:     {"acos", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
61:     {"acosh", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
62:     {"add_numbers_together", "demo_capi", CatalogType::SCALAR_FUNCTION_ENTRY},
63:     {"add_parquet_key", "parquet", CatalogType::PRAGMA_FUNCTION_ENTRY},
64:     {"aggregate", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
65:     {"alias", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
66:     {"apply", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
67:     {"approx_count_distinct", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
68:     {"approx_quantile", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
69:     {"approx_top_k", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
70:     {"arg_max", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
71:     {"arg_max_null", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
72:     {"arg_min", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
73:     {"arg_min_null", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
74:     {"argmax", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
75:     {"argmin", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
76:     {"array_agg", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
77:     {"array_aggr", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
78:     {"array_aggregate", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
79:     {"array_apply", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
80:     {"array_cosine_distance", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
81:     {"array_cosine_similarity", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
82:     {"array_cross_product", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
83:     {"array_distance", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
84:     {"array_distinct", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
85:     {"array_dot_product", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
86:     {"array_filter", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
87:     {"array_grade_up", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
88:     {"array_has_all", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
89:     {"array_has_any", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
90:     {"array_inner_product", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
91:     {"array_negative_dot_product", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
92:     {"array_negative_inner_product", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
93:     {"array_reduce", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
94:     {"array_reverse_sort", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
95:     {"array_slice", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
96:     {"array_sort", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
97:     {"array_to_json", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
98:     {"array_transform", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
99:     {"array_unique", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
100:     {"array_value", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
101:     {"ascii", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
102:     {"asin", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
103:     {"asinh", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
104:     {"atan", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
105:     {"atan2", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
106:     {"atanh", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
107:     {"avg", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
108:     {"bar", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
109:     {"base64", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
110:     {"bin", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
111:     {"bit_and", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
112:     {"bit_count", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
113:     {"bit_or", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
114:     {"bit_position", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
115:     {"bit_xor", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
116:     {"bitstring", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
117:     {"bitstring_agg", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
118:     {"bool_and", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
119:     {"bool_or", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
120:     {"broadcast", "inet", CatalogType::SCALAR_FUNCTION_ENTRY},
121:     {"can_cast_implicitly", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
122:     {"cardinality", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
123:     {"cbrt", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
124:     {"ceil", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
125:     {"ceiling", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
126:     {"check_peg_parser", "autocomplete", CatalogType::TABLE_FUNCTION_ENTRY},
127:     {"chr", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
128:     {"corr", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
129:     {"cos", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
130:     {"cosh", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
131:     {"cot", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
132:     {"count_if", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
133:     {"countif", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
134:     {"covar_pop", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
135:     {"covar_samp", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
136:     {"create_fts_index", "fts", CatalogType::PRAGMA_FUNCTION_ENTRY},
137:     {"current_database", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
138:     {"current_date", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
139:     {"current_localtime", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
140:     {"current_localtimestamp", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
141:     {"current_query", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
142:     {"current_schema", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
143:     {"current_schemas", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
144:     {"current_setting", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
145:     {"damerau_levenshtein", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
146:     {"dbgen", "tpch", CatalogType::TABLE_FUNCTION_ENTRY},
147:     {"decode", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
148:     {"degrees", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
149:     {"delta_scan", "delta", CatalogType::TABLE_FUNCTION_ENTRY},
150:     {"drop_fts_index", "fts", CatalogType::PRAGMA_FUNCTION_ENTRY},
151:     {"dsdgen", "tpcds", CatalogType::TABLE_FUNCTION_ENTRY},
152:     {"editdist3", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
153:     {"element_at", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
154:     {"encode", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
155:     {"entropy", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
156:     {"enum_code", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
157:     {"enum_first", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
158:     {"enum_last", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
159:     {"enum_range", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
160:     {"enum_range_boundary", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
161:     {"epoch_ms", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
162:     {"epoch_ns", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
163:     {"epoch_us", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
164:     {"equi_width_bins", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
165:     {"even", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
166:     {"excel_text", "excel", CatalogType::SCALAR_FUNCTION_ENTRY},
167:     {"exp", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
168:     {"factorial", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
169:     {"family", "inet", CatalogType::SCALAR_FUNCTION_ENTRY},
170:     {"favg", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
171:     {"filter", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
172:     {"flatten", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
173:     {"floor", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
174:     {"format", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
175:     {"format_bytes", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
176:     {"formatreadabledecimalsize", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
177:     {"formatreadablesize", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
178:     {"from_base64", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
179:     {"from_binary", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
180:     {"from_hex", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
181:     {"from_json", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
182:     {"from_json_strict", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
183:     {"fsum", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
184:     {"fuzz_all_functions", "sqlsmith", CatalogType::TABLE_FUNCTION_ENTRY},
185:     {"fuzzyduck", "sqlsmith", CatalogType::TABLE_FUNCTION_ENTRY},
186:     {"gamma", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
187:     {"gcd", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
188:     {"gen_random_uuid", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
189:     {"get_bit", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
190:     {"get_current_time", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
191:     {"get_current_timestamp", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
192:     {"grade_up", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
193:     {"greatest", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
194:     {"greatest_common_divisor", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
195:     {"group_concat", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
196:     {"hamming", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
197:     {"hash", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
198:     {"hex", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
199:     {"histogram", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
200:     {"histogram_exact", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
201:     {"hnsw_compact_index", "vss", CatalogType::PRAGMA_FUNCTION_ENTRY},
202:     {"hnsw_index_scan", "vss", CatalogType::TABLE_FUNCTION_ENTRY},
203:     {"host", "inet", CatalogType::SCALAR_FUNCTION_ENTRY},
204:     {"html_escape", "inet", CatalogType::SCALAR_FUNCTION_ENTRY},
205:     {"html_unescape", "inet", CatalogType::SCALAR_FUNCTION_ENTRY},
206:     {"iceberg_metadata", "iceberg", CatalogType::TABLE_FUNCTION_ENTRY},
207:     {"iceberg_scan", "iceberg", CatalogType::TABLE_FUNCTION_ENTRY},
208:     {"iceberg_snapshots", "iceberg", CatalogType::TABLE_FUNCTION_ENTRY},
209:     {"icu_calendar_names", "icu", CatalogType::TABLE_FUNCTION_ENTRY},
210:     {"icu_collate_af", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
211:     {"icu_collate_am", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
212:     {"icu_collate_ar", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
213:     {"icu_collate_ar_sa", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
214:     {"icu_collate_as", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
215:     {"icu_collate_az", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
216:     {"icu_collate_be", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
217:     {"icu_collate_bg", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
218:     {"icu_collate_bn", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
219:     {"icu_collate_bo", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
220:     {"icu_collate_br", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
221:     {"icu_collate_bs", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
222:     {"icu_collate_ca", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
223:     {"icu_collate_ceb", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
224:     {"icu_collate_chr", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
225:     {"icu_collate_cs", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
226:     {"icu_collate_cy", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
227:     {"icu_collate_da", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
228:     {"icu_collate_de", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
229:     {"icu_collate_de_at", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
230:     {"icu_collate_dsb", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
231:     {"icu_collate_dz", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
232:     {"icu_collate_ee", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
233:     {"icu_collate_el", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
234:     {"icu_collate_en", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
235:     {"icu_collate_en_us", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
236:     {"icu_collate_eo", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
237:     {"icu_collate_es", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
238:     {"icu_collate_et", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
239:     {"icu_collate_fa", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
240:     {"icu_collate_fa_af", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
241:     {"icu_collate_ff", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
242:     {"icu_collate_fi", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
243:     {"icu_collate_fil", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
244:     {"icu_collate_fo", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
245:     {"icu_collate_fr", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
246:     {"icu_collate_fr_ca", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
247:     {"icu_collate_fy", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
248:     {"icu_collate_ga", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
249:     {"icu_collate_gl", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
250:     {"icu_collate_gu", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
251:     {"icu_collate_ha", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
252:     {"icu_collate_haw", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
253:     {"icu_collate_he", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
254:     {"icu_collate_he_il", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
255:     {"icu_collate_hi", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
256:     {"icu_collate_hr", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
257:     {"icu_collate_hsb", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
258:     {"icu_collate_hu", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
259:     {"icu_collate_hy", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
260:     {"icu_collate_id", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
261:     {"icu_collate_id_id", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
262:     {"icu_collate_ig", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
263:     {"icu_collate_is", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
264:     {"icu_collate_it", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
265:     {"icu_collate_ja", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
266:     {"icu_collate_ka", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
267:     {"icu_collate_kk", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
268:     {"icu_collate_kl", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
269:     {"icu_collate_km", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
270:     {"icu_collate_kn", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
271:     {"icu_collate_ko", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
272:     {"icu_collate_kok", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
273:     {"icu_collate_ku", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
274:     {"icu_collate_ky", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
275:     {"icu_collate_lb", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
276:     {"icu_collate_lkt", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
277:     {"icu_collate_ln", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
278:     {"icu_collate_lo", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
279:     {"icu_collate_lt", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
280:     {"icu_collate_lv", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
281:     {"icu_collate_mk", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
282:     {"icu_collate_ml", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
283:     {"icu_collate_mn", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
284:     {"icu_collate_mr", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
285:     {"icu_collate_ms", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
286:     {"icu_collate_mt", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
287:     {"icu_collate_my", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
288:     {"icu_collate_nb", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
289:     {"icu_collate_nb_no", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
290:     {"icu_collate_ne", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
291:     {"icu_collate_nl", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
292:     {"icu_collate_nn", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
293:     {"icu_collate_noaccent", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
294:     {"icu_collate_om", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
295:     {"icu_collate_or", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
296:     {"icu_collate_pa", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
297:     {"icu_collate_pa_in", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
298:     {"icu_collate_pl", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
299:     {"icu_collate_ps", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
300:     {"icu_collate_pt", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
301:     {"icu_collate_ro", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
302:     {"icu_collate_ru", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
303:     {"icu_collate_sa", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
304:     {"icu_collate_se", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
305:     {"icu_collate_si", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
306:     {"icu_collate_sk", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
307:     {"icu_collate_sl", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
308:     {"icu_collate_smn", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
309:     {"icu_collate_sq", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
310:     {"icu_collate_sr", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
311:     {"icu_collate_sr_ba", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
312:     {"icu_collate_sr_me", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
313:     {"icu_collate_sr_rs", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
314:     {"icu_collate_sv", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
315:     {"icu_collate_sw", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
316:     {"icu_collate_ta", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
317:     {"icu_collate_te", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
318:     {"icu_collate_th", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
319:     {"icu_collate_tk", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
320:     {"icu_collate_to", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
321:     {"icu_collate_tr", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
322:     {"icu_collate_ug", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
323:     {"icu_collate_uk", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
324:     {"icu_collate_ur", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
325:     {"icu_collate_uz", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
326:     {"icu_collate_vi", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
327:     {"icu_collate_wae", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
328:     {"icu_collate_wo", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
329:     {"icu_collate_xh", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
330:     {"icu_collate_yi", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
331:     {"icu_collate_yo", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
332:     {"icu_collate_yue", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
333:     {"icu_collate_yue_cn", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
334:     {"icu_collate_zh", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
335:     {"icu_collate_zh_cn", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
336:     {"icu_collate_zh_hk", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
337:     {"icu_collate_zh_mo", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
338:     {"icu_collate_zh_sg", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
339:     {"icu_collate_zh_tw", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
340:     {"icu_collate_zu", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
341:     {"icu_sort_key", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
342:     {"in_search_path", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
343:     {"instr", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
344:     {"is_histogram_other_bin", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
345:     {"isfinite", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
346:     {"isinf", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
347:     {"isnan", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
348:     {"jaccard", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
349:     {"jaro_similarity", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
350:     {"jaro_winkler_similarity", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
351:     {"json", "json", CatalogType::MACRO_ENTRY},
352:     {"json_array", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
353:     {"json_array_length", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
354:     {"json_contains", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
355:     {"json_deserialize_sql", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
356:     {"json_execute_serialized_sql", "json", CatalogType::PRAGMA_FUNCTION_ENTRY},
357:     {"json_execute_serialized_sql", "json", CatalogType::TABLE_FUNCTION_ENTRY},
358:     {"json_exists", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
359:     {"json_extract", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
360:     {"json_extract_path", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
361:     {"json_extract_path_text", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
362:     {"json_extract_string", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
363:     {"json_group_array", "json", CatalogType::MACRO_ENTRY},
364:     {"json_group_object", "json", CatalogType::MACRO_ENTRY},
365:     {"json_group_structure", "json", CatalogType::MACRO_ENTRY},
366:     {"json_keys", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
367:     {"json_merge_patch", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
368:     {"json_object", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
369:     {"json_pretty", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
370:     {"json_quote", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
371:     {"json_serialize_plan", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
372:     {"json_serialize_sql", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
373:     {"json_structure", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
374:     {"json_transform", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
375:     {"json_transform_strict", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
376:     {"json_type", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
377:     {"json_valid", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
378:     {"json_value", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
379:     {"kahan_sum", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
380:     {"kurtosis", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
381:     {"kurtosis_pop", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
382:     {"lcm", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
383:     {"least", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
384:     {"least_common_multiple", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
385:     {"left", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
386:     {"left_grapheme", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
387:     {"levenshtein", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
388:     {"lgamma", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
389:     {"list", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
390:     {"list_aggr", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
391:     {"list_aggregate", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
392:     {"list_apply", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
393:     {"list_cosine_distance", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
394:     {"list_cosine_similarity", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
395:     {"list_distance", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
396:     {"list_distinct", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
397:     {"list_dot_product", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
398:     {"list_filter", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
399:     {"list_grade_up", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
400:     {"list_has_all", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
401:     {"list_has_any", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
402:     {"list_inner_product", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
403:     {"list_negative_dot_product", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
404:     {"list_negative_inner_product", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
405:     {"list_pack", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
406:     {"list_reduce", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
407:     {"list_reverse_sort", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
408:     {"list_slice", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
409:     {"list_sort", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
410:     {"list_transform", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
411:     {"list_unique", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
412:     {"list_value", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
413:     {"listagg", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
414:     {"ln", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
415:     {"load_aws_credentials", "aws", CatalogType::TABLE_FUNCTION_ENTRY},
416:     {"log", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
417:     {"log10", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
418:     {"log2", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
419:     {"lpad", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
420:     {"ltrim", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
421:     {"mad", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
422:     {"make_date", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
423:     {"make_time", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
424:     {"make_timestamp", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
425:     {"make_timestamp_ns", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
426:     {"make_timestamptz", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
427:     {"map", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
428:     {"map_concat", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
429:     {"map_entries", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
430:     {"map_extract", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
431:     {"map_extract_value", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
432:     {"map_from_entries", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
433:     {"map_keys", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
434:     {"map_values", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
435:     {"max_by", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
436:     {"mean", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
437:     {"median", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
438:     {"min_by", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
439:     {"mismatches", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
440:     {"mode", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
441:     {"mysql_clear_cache", "mysql_scanner", CatalogType::TABLE_FUNCTION_ENTRY},
442:     {"mysql_execute", "mysql_scanner", CatalogType::TABLE_FUNCTION_ENTRY},
443:     {"mysql_query", "mysql_scanner", CatalogType::TABLE_FUNCTION_ENTRY},
444:     {"nanosecond", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
445:     {"netmask", "inet", CatalogType::SCALAR_FUNCTION_ENTRY},
446:     {"network", "inet", CatalogType::SCALAR_FUNCTION_ENTRY},
447:     {"nextafter", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
448:     {"normalized_interval", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
449:     {"now", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
450:     {"ord", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
451:     {"parquet_bloom_probe", "parquet", CatalogType::TABLE_FUNCTION_ENTRY},
452:     {"parquet_file_metadata", "parquet", CatalogType::TABLE_FUNCTION_ENTRY},
453:     {"parquet_kv_metadata", "parquet", CatalogType::TABLE_FUNCTION_ENTRY},
454:     {"parquet_metadata", "parquet", CatalogType::TABLE_FUNCTION_ENTRY},
455:     {"parquet_scan", "parquet", CatalogType::TABLE_FUNCTION_ENTRY},
456:     {"parquet_schema", "parquet", CatalogType::TABLE_FUNCTION_ENTRY},
457:     {"parse_dirname", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
458:     {"parse_dirpath", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
459:     {"parse_filename", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
460:     {"parse_path", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
461:     {"pg_clear_cache", "postgres_scanner", CatalogType::TABLE_FUNCTION_ENTRY},
462:     {"pg_timezone_names", "icu", CatalogType::TABLE_FUNCTION_ENTRY},
463:     {"pi", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
464:     {"position", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
465:     {"postgres_attach", "postgres_scanner", CatalogType::TABLE_FUNCTION_ENTRY},
466:     {"postgres_execute", "postgres_scanner", CatalogType::TABLE_FUNCTION_ENTRY},
467:     {"postgres_query", "postgres_scanner", CatalogType::TABLE_FUNCTION_ENTRY},
468:     {"postgres_scan", "postgres_scanner", CatalogType::TABLE_FUNCTION_ENTRY},
469:     {"postgres_scan_pushdown", "postgres_scanner", CatalogType::TABLE_FUNCTION_ENTRY},
470:     {"pow", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
471:     {"power", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
472:     {"pragma_hnsw_index_info", "vss", CatalogType::TABLE_FUNCTION_ENTRY},
473:     {"pragma_rtree_index_info", "spatial", CatalogType::TABLE_FUNCTION_ENTRY},
474:     {"printf", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
475:     {"product", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
476:     {"quantile", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
477:     {"quantile_cont", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
478:     {"quantile_disc", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
479:     {"radians", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
480:     {"random", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
481:     {"read_json", "json", CatalogType::TABLE_FUNCTION_ENTRY},
482:     {"read_json_auto", "json", CatalogType::TABLE_FUNCTION_ENTRY},
483:     {"read_json_objects", "json", CatalogType::TABLE_FUNCTION_ENTRY},
484:     {"read_json_objects_auto", "json", CatalogType::TABLE_FUNCTION_ENTRY},
485:     {"read_ndjson", "json", CatalogType::TABLE_FUNCTION_ENTRY},
486:     {"read_ndjson_auto", "json", CatalogType::TABLE_FUNCTION_ENTRY},
487:     {"read_ndjson_objects", "json", CatalogType::TABLE_FUNCTION_ENTRY},
488:     {"read_parquet", "parquet", CatalogType::TABLE_FUNCTION_ENTRY},
489:     {"read_xlsx", "excel", CatalogType::TABLE_FUNCTION_ENTRY},
490:     {"reduce", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
491:     {"reduce_sql_statement", "sqlsmith", CatalogType::TABLE_FUNCTION_ENTRY},
492:     {"regr_avgx", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
493:     {"regr_avgy", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
494:     {"regr_count", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
495:     {"regr_intercept", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
496:     {"regr_r2", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
497:     {"regr_slope", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
498:     {"regr_sxx", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
499:     {"regr_sxy", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
500:     {"regr_syy", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
501:     {"repeat", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
502:     {"replace", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
503:     {"reservoir_quantile", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
504:     {"reverse", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
505:     {"right", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
506:     {"right_grapheme", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
507:     {"round", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
508:     {"row_to_json", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
509:     {"rpad", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
510:     {"rtree_index_dump", "spatial", CatalogType::TABLE_FUNCTION_ENTRY},
511:     {"rtree_index_scan", "spatial", CatalogType::TABLE_FUNCTION_ENTRY},
512:     {"rtrim", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
513:     {"scan_arrow_ipc", "arrow", CatalogType::TABLE_FUNCTION_ENTRY},
514:     {"sem", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
515:     {"set_bit", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
516:     {"setseed", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
517:     {"shapefile_meta", "spatial", CatalogType::TABLE_FUNCTION_ENTRY},
518:     {"sign", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
519:     {"signbit", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
520:     {"sin", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
521:     {"sinh", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
522:     {"skewness", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
523:     {"sql_auto_complete", "autocomplete", CatalogType::TABLE_FUNCTION_ENTRY},
524:     {"sqlite_attach", "sqlite_scanner", CatalogType::TABLE_FUNCTION_ENTRY},
525:     {"sqlite_query", "sqlite_scanner", CatalogType::TABLE_FUNCTION_ENTRY},
526:     {"sqlite_scan", "sqlite_scanner", CatalogType::TABLE_FUNCTION_ENTRY},
527:     {"sqlsmith", "sqlsmith", CatalogType::TABLE_FUNCTION_ENTRY},
528:     {"sqrt", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
529:     {"st_area", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
530:     {"st_area_spheroid", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
531:     {"st_asgeojson", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
532:     {"st_ashexwkb", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
533:     {"st_assvg", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
534:     {"st_astext", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
535:     {"st_aswkb", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
536:     {"st_boundary", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
537:     {"st_buffer", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
538:     {"st_centroid", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
539:     {"st_collect", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
540:     {"st_collectionextract", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
541:     {"st_contains", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
542:     {"st_containsproperly", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
543:     {"st_convexhull", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
544:     {"st_coveredby", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
545:     {"st_covers", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
546:     {"st_crosses", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
547:     {"st_difference", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
548:     {"st_dimension", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
549:     {"st_disjoint", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
550:     {"st_distance", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
551:     {"st_distance_sphere", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
552:     {"st_distance_spheroid", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
553:     {"st_drivers", "spatial", CatalogType::TABLE_FUNCTION_ENTRY},
554:     {"st_dump", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
555:     {"st_dwithin", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
556:     {"st_dwithin_spheroid", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
557:     {"st_endpoint", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
558:     {"st_envelope", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
559:     {"st_envelope_agg", "spatial", CatalogType::AGGREGATE_FUNCTION_ENTRY},
560:     {"st_equals", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
561:     {"st_extent", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
562:     {"st_extent_agg", "spatial", CatalogType::AGGREGATE_FUNCTION_ENTRY},
563:     {"st_extent_approx", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
564:     {"st_exteriorring", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
565:     {"st_flipcoordinates", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
566:     {"st_force2d", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
567:     {"st_force3dm", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
568:     {"st_force3dz", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
569:     {"st_force4d", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
570:     {"st_generatepoints", "spatial", CatalogType::TABLE_FUNCTION_ENTRY},
571:     {"st_geometrytype", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
572:     {"st_geomfromgeojson", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
573:     {"st_geomfromhexewkb", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
574:     {"st_geomfromhexwkb", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
575:     {"st_geomfromtext", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
576:     {"st_geomfromwkb", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
577:     {"st_hasm", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
578:     {"st_hasz", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
579:     {"st_hilbert", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
580:     {"st_intersection", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
581:     {"st_intersection_agg", "spatial", CatalogType::AGGREGATE_FUNCTION_ENTRY},
582:     {"st_intersects", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
583:     {"st_intersects_extent", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
584:     {"st_isclosed", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
585:     {"st_isempty", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
586:     {"st_isring", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
587:     {"st_issimple", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
588:     {"st_isvalid", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
589:     {"st_length", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
590:     {"st_length_spheroid", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
591:     {"st_linemerge", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
592:     {"st_linestring2dfromwkb", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
593:     {"st_list_proj_crs", "spatial", CatalogType::TABLE_FUNCTION_ENTRY},
594:     {"st_m", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
595:     {"st_makeenvelope", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
596:     {"st_makeline", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
597:     {"st_makepolygon", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
598:     {"st_makevalid", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
599:     {"st_mmax", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
600:     {"st_mmin", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
601:     {"st_multi", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
602:     {"st_ngeometries", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
603:     {"st_ninteriorrings", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
604:     {"st_normalize", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
605:     {"st_npoints", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
606:     {"st_numgeometries", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
607:     {"st_numinteriorrings", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
608:     {"st_numpoints", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
609:     {"st_overlaps", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
610:     {"st_perimeter", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
611:     {"st_perimeter_spheroid", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
612:     {"st_point", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
613:     {"st_point2d", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
614:     {"st_point2dfromwkb", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
615:     {"st_point3d", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
616:     {"st_point4d", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
617:     {"st_pointn", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
618:     {"st_pointonsurface", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
619:     {"st_points", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
620:     {"st_polygon2dfromwkb", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
621:     {"st_quadkey", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
622:     {"st_read", "spatial", CatalogType::TABLE_FUNCTION_ENTRY},
623:     {"st_read_meta", "spatial", CatalogType::TABLE_FUNCTION_ENTRY},
624:     {"st_readosm", "spatial", CatalogType::TABLE_FUNCTION_ENTRY},
625:     {"st_readshp", "spatial", CatalogType::TABLE_FUNCTION_ENTRY},
626:     {"st_reduceprecision", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
627:     {"st_removerepeatedpoints", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
628:     {"st_reverse", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
629:     {"st_shortestline", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
630:     {"st_simplify", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
631:     {"st_simplifypreservetopology", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
632:     {"st_startpoint", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
633:     {"st_touches", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
634:     {"st_transform", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
635:     {"st_union", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
636:     {"st_union_agg", "spatial", CatalogType::AGGREGATE_FUNCTION_ENTRY},
637:     {"st_within", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
638:     {"st_x", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
639:     {"st_xmax", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
640:     {"st_xmin", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
641:     {"st_y", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
642:     {"st_ymax", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
643:     {"st_ymin", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
644:     {"st_z", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
645:     {"st_zmax", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
646:     {"st_zmflag", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
647:     {"st_zmin", "spatial", CatalogType::SCALAR_FUNCTION_ENTRY},
648:     {"start_ui", "motherduck", CatalogType::TABLE_FUNCTION_ENTRY},
649:     {"starts_with", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
650:     {"stats", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
651:     {"stddev", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
652:     {"stddev_pop", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
653:     {"stddev_samp", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
654:     {"stem", "fts", CatalogType::SCALAR_FUNCTION_ENTRY},
655:     {"string_agg", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
656:     {"strpos", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
657:     {"struct_insert", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
658:     {"sum", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
659:     {"sum_no_overflow", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
660:     {"sumkahan", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
661:     {"tan", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
662:     {"tanh", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
663:     {"text", "excel", CatalogType::SCALAR_FUNCTION_ENTRY},
664:     {"timetz_byte_comparable", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
665:     {"to_arrow_ipc", "arrow", CatalogType::TABLE_FUNCTION_ENTRY},
666:     {"to_base", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
667:     {"to_base64", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
668:     {"to_binary", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
669:     {"to_centuries", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
670:     {"to_days", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
671:     {"to_decades", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
672:     {"to_hex", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
673:     {"to_hours", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
674:     {"to_json", "json", CatalogType::SCALAR_FUNCTION_ENTRY},
675:     {"to_microseconds", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
676:     {"to_millennia", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
677:     {"to_milliseconds", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
678:     {"to_minutes", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
679:     {"to_months", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
680:     {"to_quarters", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
681:     {"to_seconds", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
682:     {"to_timestamp", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
683:     {"to_weeks", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
684:     {"to_years", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
685:     {"today", "icu", CatalogType::SCALAR_FUNCTION_ENTRY},
686:     {"tpcds", "tpcds", CatalogType::PRAGMA_FUNCTION_ENTRY},
687:     {"tpcds_answers", "tpcds", CatalogType::TABLE_FUNCTION_ENTRY},
688:     {"tpcds_queries", "tpcds", CatalogType::TABLE_FUNCTION_ENTRY},
689:     {"tpch", "tpch", CatalogType::PRAGMA_FUNCTION_ENTRY},
690:     {"tpch_answers", "tpch", CatalogType::TABLE_FUNCTION_ENTRY},
691:     {"tpch_queries", "tpch", CatalogType::TABLE_FUNCTION_ENTRY},
692:     {"transaction_timestamp", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
693:     {"translate", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
694:     {"trim", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
695:     {"trunc", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
696:     {"txid_current", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
697:     {"typeof", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
698:     {"unbin", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
699:     {"unhex", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
700:     {"unicode", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
701:     {"union_extract", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
702:     {"union_tag", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
703:     {"union_value", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
704:     {"unpivot_list", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
705:     {"url_decode", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
706:     {"url_encode", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
707:     {"uuid", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
708:     {"var_pop", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
709:     {"var_samp", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
710:     {"variance", "core_functions", CatalogType::AGGREGATE_FUNCTION_ENTRY},
711:     {"vector_type", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
712:     {"version", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
713:     {"vss_join", "vss", CatalogType::TABLE_MACRO_ENTRY},
714:     {"vss_match", "vss", CatalogType::TABLE_MACRO_ENTRY},
715:     {"xor", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
716:     {"|", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
717:     {"~", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY},
718: }; // END_OF_EXTENSION_FUNCTIONS
719: 
720: static constexpr ExtensionFunctionOverloadEntry EXTENSION_FUNCTION_OVERLOADS[] = {
721:     {"age", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>INTERVAL"},
722:     {"age", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP,TIMESTAMP>INTERVAL"},
723:     {"age", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>INTERVAL"},
724:     {"age", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ,TIMESTAMPTZ>INTERVAL"},
725:     {"century", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
726:     {"century", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
727:     {"century", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
728:     {"century", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
729:     {"date_diff", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,DATE,DATE>BIGINT"},
730:     {"date_diff", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIME,TIME>BIGINT"},
731:     {"date_diff", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMESTAMP,TIMESTAMP>BIGINT"},
732:     {"date_diff", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMESTAMPTZ,TIMESTAMPTZ>BIGINT"},
733:     {"date_part", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,DATE>BIGINT"},
734:     {"date_part", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,INTERVAL>BIGINT"},
735:     {"date_part", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIME>BIGINT"},
736:     {"date_part", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMESTAMP>BIGINT"},
737:     {"date_part", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMETZ>BIGINT"},
738:     {"date_part", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR[],DATE>STRUCT()"},
739:     {"date_part", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR[],INTERVAL>STRUCT()"},
740:     {"date_part", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR[],TIME>STRUCT()"},
741:     {"date_part", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR[],TIMESTAMP>STRUCT()"},
742:     {"date_part", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR[],TIMETZ>STRUCT()"},
743:     {"date_part", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMESTAMPTZ>BIGINT"},
744:     {"date_part", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR[],TIMESTAMPTZ>STRUCT()"},
745:     {"date_sub", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,DATE,DATE>BIGINT"},
746:     {"date_sub", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIME,TIME>BIGINT"},
747:     {"date_sub", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMESTAMP,TIMESTAMP>BIGINT"},
748:     {"date_sub", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMESTAMPTZ,TIMESTAMPTZ>BIGINT"},
749:     {"date_trunc", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,DATE>TIMESTAMP"},
750:     {"date_trunc", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,INTERVAL>INTERVAL"},
751:     {"date_trunc", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMESTAMP>TIMESTAMP"},
752:     {"date_trunc", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMESTAMPTZ>TIMESTAMPTZ"},
753:     {"datediff", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,DATE,DATE>BIGINT"},
754:     {"datediff", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIME,TIME>BIGINT"},
755:     {"datediff", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMESTAMP,TIMESTAMP>BIGINT"},
756:     {"datediff", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMESTAMPTZ,TIMESTAMPTZ>BIGINT"},
757:     {"datepart", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,DATE>BIGINT"},
758:     {"datepart", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,INTERVAL>BIGINT"},
759:     {"datepart", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIME>BIGINT"},
760:     {"datepart", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMESTAMP>BIGINT"},
761:     {"datepart", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMETZ>BIGINT"},
762:     {"datepart", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR[],DATE>STRUCT()"},
763:     {"datepart", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR[],INTERVAL>STRUCT()"},
764:     {"datepart", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR[],TIME>STRUCT()"},
765:     {"datepart", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR[],TIMESTAMP>STRUCT()"},
766:     {"datepart", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR[],TIMETZ>STRUCT()"},
767:     {"datepart", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMESTAMPTZ>BIGINT"},
768:     {"datepart", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR[],TIMESTAMPTZ>STRUCT()"},
769:     {"datesub", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,DATE,DATE>BIGINT"},
770:     {"datesub", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIME,TIME>BIGINT"},
771:     {"datesub", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMESTAMP,TIMESTAMP>BIGINT"},
772:     {"datesub", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMESTAMPTZ,TIMESTAMPTZ>BIGINT"},
773:     {"datetrunc", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,DATE>TIMESTAMP"},
774:     {"datetrunc", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,INTERVAL>INTERVAL"},
775:     {"datetrunc", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMESTAMP>TIMESTAMP"},
776:     {"datetrunc", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMESTAMPTZ>TIMESTAMPTZ"},
777:     {"day", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
778:     {"day", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
779:     {"day", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
780:     {"day", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
781:     {"dayname", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>VARCHAR"},
782:     {"dayname", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>VARCHAR"},
783:     {"dayname", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>VARCHAR"},
784:     {"dayofmonth", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
785:     {"dayofmonth", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
786:     {"dayofmonth", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
787:     {"dayofmonth", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
788:     {"dayofweek", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
789:     {"dayofweek", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
790:     {"dayofweek", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
791:     {"dayofweek", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
792:     {"dayofyear", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
793:     {"dayofyear", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
794:     {"dayofyear", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
795:     {"dayofyear", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
796:     {"decade", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
797:     {"decade", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
798:     {"decade", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
799:     {"decade", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
800:     {"epoch", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>DOUBLE"},
801:     {"epoch", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>DOUBLE"},
802:     {"epoch", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIME>DOUBLE"},
803:     {"epoch", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>DOUBLE"},
804:     {"epoch", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMETZ>DOUBLE"},
805:     {"epoch", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>DOUBLE"},
806:     {"era", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
807:     {"era", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
808:     {"era", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
809:     {"era", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
810:     {"generate_series", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "BIGINT>BIGINT[]"},
811:     {"generate_series", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "BIGINT,BIGINT>BIGINT[]"},
812:     {"generate_series", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "BIGINT,BIGINT,BIGINT>BIGINT[]"},
813:     {"generate_series", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY,
814:      "TIMESTAMP,TIMESTAMP,INTERVAL>TIMESTAMP[]"},
815:     {"generate_series", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ,TIMESTAMPTZ,INTERVAL>TIMESTAMPTZ[]"},
816:     {"hour", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
817:     {"hour", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
818:     {"hour", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIME>BIGINT"},
819:     {"hour", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
820:     {"hour", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMETZ>BIGINT"},
821:     {"hour", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
822:     {"isodow", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
823:     {"isodow", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
824:     {"isodow", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
825:     {"isodow", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
826:     {"isoyear", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
827:     {"isoyear", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
828:     {"isoyear", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
829:     {"isoyear", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
830:     {"julian", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>DOUBLE"},
831:     {"julian", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>DOUBLE"},
832:     {"julian", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>DOUBLE"},
833:     {"last_day", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>DATE"},
834:     {"last_day", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>DATE"},
835:     {"last_day", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>DATE"},
836:     {"microsecond", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
837:     {"microsecond", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
838:     {"microsecond", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIME>BIGINT"},
839:     {"microsecond", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
840:     {"microsecond", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMETZ>BIGINT"},
841:     {"microsecond", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
842:     {"millennium", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
843:     {"millennium", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
844:     {"millennium", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
845:     {"millennium", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
846:     {"millisecond", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
847:     {"millisecond", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
848:     {"millisecond", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIME>BIGINT"},
849:     {"millisecond", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
850:     {"millisecond", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMETZ>BIGINT"},
851:     {"millisecond", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
852:     {"minute", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
853:     {"minute", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
854:     {"minute", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIME>BIGINT"},
855:     {"minute", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
856:     {"minute", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMETZ>BIGINT"},
857:     {"minute", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
858:     {"month", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
859:     {"month", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
860:     {"month", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
861:     {"month", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
862:     {"monthname", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>VARCHAR"},
863:     {"monthname", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>VARCHAR"},
864:     {"monthname", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>VARCHAR"},
865:     {"quarter", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
866:     {"quarter", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
867:     {"quarter", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
868:     {"quarter", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
869:     {"range", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "BIGINT>BIGINT[]"},
870:     {"range", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "BIGINT,BIGINT>BIGINT[]"},
871:     {"range", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "BIGINT,BIGINT,BIGINT>BIGINT[]"},
872:     {"range", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP,TIMESTAMP,INTERVAL>TIMESTAMP[]"},
873:     {"range", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ,TIMESTAMPTZ,INTERVAL>TIMESTAMPTZ[]"},
874:     {"second", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
875:     {"second", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
876:     {"second", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIME>BIGINT"},
877:     {"second", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
878:     {"second", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMETZ>BIGINT"},
879:     {"second", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
880:     {"time_bucket", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL,DATE>DATE"},
881:     {"time_bucket", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL,DATE,DATE>DATE"},
882:     {"time_bucket", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL,DATE,INTERVAL>DATE"},
883:     {"time_bucket", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL,TIMESTAMP>TIMESTAMP"},
884:     {"time_bucket", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL,TIMESTAMP,INTERVAL>TIMESTAMP"},
885:     {"time_bucket", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL,TIMESTAMP,TIMESTAMP>TIMESTAMP"},
886:     {"time_bucket", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL,TIMESTAMPTZ>TIMESTAMPTZ"},
887:     {"time_bucket", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL,TIMESTAMPTZ,INTERVAL>TIMESTAMPTZ"},
888:     {"time_bucket", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL,TIMESTAMPTZ,TIMESTAMPTZ>TIMESTAMPTZ"},
889:     {"time_bucket", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL,TIMESTAMPTZ,VARCHAR>TIMESTAMPTZ"},
890:     {"timezone", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
891:     {"timezone", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
892:     {"timezone", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL,TIMETZ>TIMETZ"},
893:     {"timezone", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
894:     {"timezone", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
895:     {"timezone", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMESTAMP>TIMESTAMPTZ"},
896:     {"timezone", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMESTAMPTZ>TIMESTAMP"},
897:     {"timezone", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "VARCHAR,TIMETZ>TIMETZ"},
898:     {"timezone_hour", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
899:     {"timezone_hour", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
900:     {"timezone_hour", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
901:     {"timezone_hour", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
902:     {"timezone_minute", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
903:     {"timezone_minute", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
904:     {"timezone_minute", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
905:     {"timezone_minute", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
906:     {"week", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
907:     {"week", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
908:     {"week", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
909:     {"week", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
910:     {"weekday", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
911:     {"weekday", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
912:     {"weekday", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
913:     {"weekday", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
914:     {"weekofyear", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
915:     {"weekofyear", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
916:     {"weekofyear", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
917:     {"weekofyear", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
918:     {"year", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
919:     {"year", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
920:     {"year", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
921:     {"year", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
922:     {"yearweek", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "DATE>BIGINT"},
923:     {"yearweek", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "INTERVAL>BIGINT"},
924:     {"yearweek", "core_functions", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMP>BIGINT"},
925:     {"yearweek", "icu", CatalogType::SCALAR_FUNCTION_ENTRY, "TIMESTAMPTZ>BIGINT"},
926: }; // END_OF_EXTENSION_FUNCTION_OVERLOADS
927: 
928: static constexpr ExtensionEntry EXTENSION_SETTINGS[] = {
929:     {"azure_account_name", "azure"},
930:     {"azure_context_caching", "azure"},
931:     {"azure_credential_chain", "azure"},
932:     {"azure_endpoint", "azure"},
933:     {"azure_http_proxy", "azure"},
934:     {"azure_http_stats", "azure"},
935:     {"azure_proxy_password", "azure"},
936:     {"azure_proxy_user_name", "azure"},
937:     {"azure_read_buffer_size", "azure"},
938:     {"azure_read_transfer_chunk_size", "azure"},
939:     {"azure_read_transfer_concurrency", "azure"},
940:     {"azure_storage_connection_string", "azure"},
941:     {"azure_transport_option_type", "azure"},
942:     {"binary_as_string", "parquet"},
943:     {"ca_cert_file", "httpfs"},
944:     {"calendar", "icu"},
945:     {"disable_parquet_prefetching", "parquet"},
946:     {"enable_geoparquet_conversion", "parquet"},
947:     {"enable_server_cert_verification", "httpfs"},
948:     {"force_download", "httpfs"},
949:     {"hf_max_per_page", "httpfs"},
950:     {"hnsw_ef_search", "vss"},
951:     {"hnsw_enable_experimental_persistence", "vss"},
952:     {"http_keep_alive", "httpfs"},
953:     {"http_retries", "httpfs"},
954:     {"http_retry_backoff", "httpfs"},
955:     {"http_retry_wait_ms", "httpfs"},
956:     {"http_timeout", "httpfs"},
957:     {"mysql_bit1_as_boolean", "mysql_scanner"},
958:     {"mysql_debug_show_queries", "mysql_scanner"},
959:     {"mysql_experimental_filter_pushdown", "mysql_scanner"},
960:     {"mysql_tinyint1_as_boolean", "mysql_scanner"},
961:     {"parquet_metadata_cache", "parquet"},
962:     {"pg_array_as_varchar", "postgres_scanner"},
963:     {"pg_connection_cache", "postgres_scanner"},
964:     {"pg_connection_limit", "postgres_scanner"},
965:     {"pg_debug_show_queries", "postgres_scanner"},
966:     {"pg_experimental_filter_pushdown", "postgres_scanner"},
967:     {"pg_null_byte_replacement", "postgres_scanner"},
968:     {"pg_pages_per_task", "postgres_scanner"},
969:     {"pg_use_binary_copy", "postgres_scanner"},
970:     {"pg_use_ctid_scan", "postgres_scanner"},
971:     {"prefetch_all_parquet_files", "parquet"},
972:     {"s3_access_key_id", "httpfs"},
973:     {"s3_endpoint", "httpfs"},
974:     {"s3_region", "httpfs"},
975:     {"s3_secret_access_key", "httpfs"},
976:     {"s3_session_token", "httpfs"},
977:     {"s3_uploader_max_filesize", "httpfs"},
978:     {"s3_uploader_max_parts_per_file", "httpfs"},
979:     {"s3_uploader_thread_limit", "httpfs"},
980:     {"s3_url_compatibility_mode", "httpfs"},
981:     {"s3_url_style", "httpfs"},
982:     {"s3_use_ssl", "httpfs"},
983:     {"sqlite_all_varchar", "sqlite_scanner"},
984:     {"sqlite_debug_show_queries", "sqlite_scanner"},
985:     {"timezone", "icu"},
986:     {"unsafe_enable_version_guessing", "iceberg"},
987: }; // END_OF_EXTENSION_SETTINGS
988: 
989: // Note: these are currently hardcoded in scripts/generate_extensions_function.py
990: // TODO: automate by passing though to script via duckdb
991: static constexpr ExtensionEntry EXTENSION_COPY_FUNCTIONS[] = {{"parquet", "parquet"},
992:                                                               {"json", "json"}}; // END_OF_EXTENSION_COPY_FUNCTIONS
993: 
994: // Note: these are currently hardcoded in scripts/generate_extensions_function.py
995: // TODO: automate by passing though to script via duckdb
996: static constexpr ExtensionEntry EXTENSION_TYPES[] = {
997:     {"json", "json"}, {"inet", "inet"}, {"geometry", "spatial"}}; // END_OF_EXTENSION_TYPES
998: 
999: // Note: these are currently hardcoded in scripts/generate_extensions_function.py
1000: // TODO: automate by passing though to script via duckdb
1001: static constexpr ExtensionEntry EXTENSION_COLLATIONS[] = {
1002:     {"af", "icu"},    {"am", "icu"},    {"ar", "icu"},     {"ar_sa", "icu"}, {"as", "icu"},    {"az", "icu"},
1003:     {"be", "icu"},    {"bg", "icu"},    {"bn", "icu"},     {"bo", "icu"},    {"br", "icu"},    {"bs", "icu"},
1004:     {"ca", "icu"},    {"ceb", "icu"},   {"chr", "icu"},    {"cs", "icu"},    {"cy", "icu"},    {"da", "icu"},
1005:     {"de", "icu"},    {"de_at", "icu"}, {"dsb", "icu"},    {"dz", "icu"},    {"ee", "icu"},    {"el", "icu"},
1006:     {"en", "icu"},    {"en_us", "icu"}, {"eo", "icu"},     {"es", "icu"},    {"et", "icu"},    {"fa", "icu"},
1007:     {"fa_af", "icu"}, {"ff", "icu"},    {"fi", "icu"},     {"fil", "icu"},   {"fo", "icu"},    {"fr", "icu"},
1008:     {"fr_ca", "icu"}, {"fy", "icu"},    {"ga", "icu"},     {"gl", "icu"},    {"gu", "icu"},    {"ha", "icu"},
1009:     {"haw", "icu"},   {"he", "icu"},    {"he_il", "icu"},  {"hi", "icu"},    {"hr", "icu"},    {"hsb", "icu"},
1010:     {"hu", "icu"},    {"hy", "icu"},    {"id", "icu"},     {"id_id", "icu"}, {"ig", "icu"},    {"is", "icu"},
1011:     {"it", "icu"},    {"ja", "icu"},    {"ka", "icu"},     {"kk", "icu"},    {"kl", "icu"},    {"km", "icu"},
1012:     {"kn", "icu"},    {"ko", "icu"},    {"kok", "icu"},    {"ku", "icu"},    {"ky", "icu"},    {"lb", "icu"},
1013:     {"lkt", "icu"},   {"ln", "icu"},    {"lo", "icu"},     {"lt", "icu"},    {"lv", "icu"},    {"mk", "icu"},
1014:     {"ml", "icu"},    {"mn", "icu"},    {"mr", "icu"},     {"ms", "icu"},    {"mt", "icu"},    {"my", "icu"},
1015:     {"nb", "icu"},    {"nb_no", "icu"}, {"ne", "icu"},     {"nl", "icu"},    {"nn", "icu"},    {"om", "icu"},
1016:     {"or", "icu"},    {"pa", "icu"},    {"pa_in", "icu"},  {"pl", "icu"},    {"ps", "icu"},    {"pt", "icu"},
1017:     {"ro", "icu"},    {"ru", "icu"},    {"sa", "icu"},     {"se", "icu"},    {"si", "icu"},    {"sk", "icu"},
1018:     {"sl", "icu"},    {"smn", "icu"},   {"sq", "icu"},     {"sr", "icu"},    {"sr_ba", "icu"}, {"sr_me", "icu"},
1019:     {"sr_rs", "icu"}, {"sv", "icu"},    {"sw", "icu"},     {"ta", "icu"},    {"te", "icu"},    {"th", "icu"},
1020:     {"tk", "icu"},    {"to", "icu"},    {"tr", "icu"},     {"ug", "icu"},    {"uk", "icu"},    {"ur", "icu"},
1021:     {"uz", "icu"},    {"vi", "icu"},    {"wae", "icu"},    {"wo", "icu"},    {"xh", "icu"},    {"yi", "icu"},
1022:     {"yo", "icu"},    {"yue", "icu"},   {"yue_cn", "icu"}, {"zh", "icu"},    {"zh_cn", "icu"}, {"zh_hk", "icu"},
1023:     {"zh_mo", "icu"}, {"zh_sg", "icu"}, {"zh_tw", "icu"},  {"zu", "icu"}}; // END_OF_EXTENSION_COLLATIONS
1024: 
1025: // Note: these are currently hardcoded in scripts/generate_extensions_function.py
1026: // TODO: automate by passing though to script via duckdb
1027: static constexpr ExtensionEntry EXTENSION_FILE_PREFIXES[] = {
1028:     {"http://", "httpfs"}, {"https://", "httpfs"}, {"s3://", "httpfs"}, {"s3a://", "httpfs"},  {"s3n://", "httpfs"},
1029:     {"gcs://", "httpfs"},  {"gs://", "httpfs"},    {"r2://", "httpfs"}, {"azure://", "azure"}, {"az://", "azure"},
1030:     {"abfss://", "azure"}, {"hf://", "httpfs"}}; // END_OF_EXTENSION_FILE_PREFIXES
1031: 
1032: // Note: these are currently hardcoded in scripts/generate_extensions_function.py
1033: // TODO: automate by passing though to script via duckdb
1034: static constexpr ExtensionEntry EXTENSION_FILE_POSTFIXES[] = {
1035:     {".parquet", "parquet"}, {".json", "json"},    {".jsonl", "json"},  {".ndjson", "json"},
1036:     {".shp", "spatial"},     {".gpkg", "spatial"}, {".fgb", "spatial"}, {".xlsx", "excel"},
1037: }; // END_OF_EXTENSION_FILE_POSTFIXES
1038: 
1039: // Note: these are currently hardcoded in scripts/generate_extensions_function.py
1040: // TODO: automate by passing though to script via duckdb
1041: static constexpr ExtensionEntry EXTENSION_FILE_CONTAINS[] = {{".parquet?", "parquet"},
1042:                                                              {".json?", "json"},
1043:                                                              {".ndjson?", ".jsonl?"},
1044:                                                              {".jsonl?", ".ndjson?"}}; // EXTENSION_FILE_CONTAINS
1045: 
1046: // Note: these are currently hardcoded in scripts/generate_extensions_function.py
1047: // TODO: automate by passing though to script via duckdb
1048: static constexpr ExtensionEntry EXTENSION_SECRET_TYPES[] = {
1049:     {"s3", "httpfs"},           {"r2", "httpfs"},
1050:     {"gcs", "httpfs"},          {"azure", "azure"},
1051:     {"huggingface", "httpfs"},  {"bearer", "httpfs"},
1052:     {"mysql", "mysql_scanner"}, {"postgres", "postgres_scanner"}}; // EXTENSION_SECRET_TYPES
1053: 
1054: // Note: these are currently hardcoded in scripts/generate_extensions_function.py
1055: // TODO: automate by passing though to script via duckdb
1056: static constexpr ExtensionEntry EXTENSION_SECRET_PROVIDERS[] = {
1057:     {"s3/config", "httpfs"},
1058:     {"gcs/config", "httpfs"},
1059:     {"r2/config", "httpfs"},
1060:     {"s3/credential_chain", "aws"},
1061:     {"gcs/credential_chain", "aws"},
1062:     {"r2/credential_chain", "aws"},
1063:     {"azure/access_token", "azure"},
1064:     {"azure/config", "azure"},
1065:     {"azure/credential_chain", "azure"},
1066:     {"azure/service_principal", "azure"},
1067:     {"huggingface/config", "httfps"},
1068:     {"huggingface/credential_chain", "httpfs"},
1069:     {"bearer/config", "httpfs"},
1070:     {"mysql/config", "mysql_scanner"},
1071:     {"postgres/config", "postgres_scanner"}}; // EXTENSION_SECRET_PROVIDERS
1072: 
1073: static constexpr const char *AUTOLOADABLE_EXTENSIONS[] = {
1074:     "aws",        "azure",         "autocomplete", "core_functions", "delta",    "excel",
1075:     "fts",        "httpfs",        "iceberg",      "inet",           "icu",      "json",
1076:     "motherduck", "mysql_scanner", "parquet",      "sqlite_scanner", "sqlsmith", "postgres_scanner",
1077:     "tpcds",      "tpch",          "uc_catalog"}; // END_OF_AUTOLOADABLE_EXTENSIONS
1078: 
1079: } // namespace duckdb
[end of src/include/duckdb/main/extension_entries.hpp]
[start of src/include/duckdb/main/query_profiler.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/main/query_profiler.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/deque.hpp"
13: #include "duckdb/common/enums/profiler_format.hpp"
14: #include "duckdb/common/enums/explain_format.hpp"
15: #include "duckdb/common/pair.hpp"
16: #include "duckdb/common/profiler.hpp"
17: #include "duckdb/common/reference_map.hpp"
18: #include "duckdb/common/string_util.hpp"
19: #include "duckdb/common/types/data_chunk.hpp"
20: #include "duckdb/common/unordered_map.hpp"
21: #include "duckdb/common/winapi.hpp"
22: #include "duckdb/execution/expression_executor_state.hpp"
23: #include "duckdb/execution/physical_operator.hpp"
24: #include "duckdb/main/profiling_info.hpp"
25: #include "duckdb/main/profiling_node.hpp"
26: 
27: #include <stack>
28: 
29: namespace duckdb {
30: class ClientContext;
31: class ExpressionExecutor;
32: class ProfilingNode;
33: class PhysicalOperator;
34: class SQLStatement;
35: 
36: struct OperatorInformation {
37: 	explicit OperatorInformation(double time_p = 0, idx_t elements_returned_p = 0, idx_t elements_scanned_p = 0,
38: 	                             idx_t result_set_size_p = 0)
39: 	    : time(time_p), elements_returned(elements_returned_p), result_set_size(result_set_size_p) {
40: 	}
41: 
42: 	double time;
43: 	idx_t elements_returned;
44: 	idx_t result_set_size;
45: 	string name;
46: 	InsertionOrderPreservingMap<string> extra_info;
47: 
48: 	void AddTime(double n_time) {
49: 		time += n_time;
50: 	}
51: 
52: 	void AddReturnedElements(idx_t n_elements) {
53: 		elements_returned += n_elements;
54: 	}
55: 
56: 	void AddResultSetSize(idx_t n_result_set_size) {
57: 		result_set_size += n_result_set_size;
58: 	}
59: };
60: 
61: //! The OperatorProfiler measures timings of individual operators
62: //! This class exists once for all operators and collects `OperatorInfo` for each operator
63: class OperatorProfiler {
64: 	friend class QueryProfiler;
65: 
66: public:
67: 	DUCKDB_API explicit OperatorProfiler(ClientContext &context);
68: 	~OperatorProfiler() {
69: 	}
70: 
71: public:
72: 	DUCKDB_API void StartOperator(optional_ptr<const PhysicalOperator> phys_op);
73: 	DUCKDB_API void EndOperator(optional_ptr<DataChunk> chunk);
74: 
75: 	//! Adds the timings in the OperatorProfiler (tree) to the QueryProfiler (tree).
76: 	DUCKDB_API void Flush(const PhysicalOperator &phys_op);
77: 	DUCKDB_API OperatorInformation &GetOperatorInfo(const PhysicalOperator &phys_op);
78: 
79: public:
80: 	ClientContext &context;
81: 
82: private:
83: 	//! Whether or not the profiler is enabled
84: 	bool enabled;
85: 	//! Sub-settings for the operator profiler
86: 	profiler_settings_t settings;
87: 
88: 	//! The timer used to time the execution time of the individual Physical Operators
89: 	Profiler op;
90: 	//! The stack of Physical Operators that are currently active
91: 	optional_ptr<const PhysicalOperator> active_operator;
92: 	//! A mapping of physical operators to profiled operator information.
93: 	reference_map_t<const PhysicalOperator, OperatorInformation> operator_infos;
94: };
95: 
96: struct QueryInfo {
97: 	QueryInfo() : blocked_thread_time(0) {};
98: 	string query_name;
99: 	double blocked_thread_time;
100: };
101: 
102: //! The QueryProfiler can be used to measure timings of queries
103: class QueryProfiler {
104: public:
105: 	DUCKDB_API explicit QueryProfiler(ClientContext &context);
106: 
107: public:
108: 	// Propagate save_location, enabled, detailed_enabled and automatic_print_format.
109: 	void Propagate(QueryProfiler &qp);
110: 
111: 	using TreeMap = reference_map_t<const PhysicalOperator, reference<ProfilingNode>>;
112: 
113: private:
114: 	unique_ptr<ProfilingNode> CreateTree(const PhysicalOperator &root, const profiler_settings_t &settings,
115: 	                                     const idx_t depth = 0);
116: 	void Render(const ProfilingNode &node, std::ostream &str) const;
117: 	string RenderDisabledMessage(ProfilerPrintFormat format) const;
118: 
119: public:
120: 	DUCKDB_API bool IsEnabled() const;
121: 	DUCKDB_API bool IsDetailedEnabled() const;
122: 	DUCKDB_API ProfilerPrintFormat GetPrintFormat(ExplainFormat format = ExplainFormat::DEFAULT) const;
123: 	DUCKDB_API bool PrintOptimizerOutput() const;
124: 	DUCKDB_API string GetSaveLocation() const;
125: 
126: 	DUCKDB_API static QueryProfiler &Get(ClientContext &context);
127: 
128: 	DUCKDB_API void StartQuery(string query, bool is_explain_analyze = false, bool start_at_optimizer = false);
129: 	DUCKDB_API void EndQuery();
130: 
131: 	DUCKDB_API void StartExplainAnalyze();
132: 
133: 	//! Adds the timings gathered by an OperatorProfiler to this query profiler
134: 	DUCKDB_API void Flush(OperatorProfiler &profiler);
135: 	//! Adds the top level query information to the global profiler.
136: 	DUCKDB_API void SetInfo(const double &blocked_thread_time);
137: 
138: 	DUCKDB_API void StartPhase(MetricsType phase_metric);
139: 	DUCKDB_API void EndPhase();
140: 
141: 	DUCKDB_API void Initialize(const PhysicalOperator &root);
142: 
143: 	DUCKDB_API string QueryTreeToString() const;
144: 	DUCKDB_API void QueryTreeToStream(std::ostream &str) const;
145: 	DUCKDB_API void Print();
146: 
147: 	//! return the printed as a string. Unlike ToString, which is always formatted as a string,
148: 	//! the return value is formatted based on the current print format (see GetPrintFormat()).
149: 	DUCKDB_API string ToString(ExplainFormat format = ExplainFormat::DEFAULT) const;
150: 	DUCKDB_API string ToString(ProfilerPrintFormat format) const;
151: 
152: 	static InsertionOrderPreservingMap<string> JSONSanitize(const InsertionOrderPreservingMap<string> &input);
153: 	static string JSONSanitize(const string &text);
154: 	static string DrawPadded(const string &str, idx_t width);
155: 	DUCKDB_API string ToJSON() const;
156: 	DUCKDB_API void WriteToFile(const char *path, string &info) const;
157: 
158: 	idx_t OperatorSize() {
159: 		return tree_map.size();
160: 	}
161: 
162: 	void Finalize(ProfilingNode &node);
163: 
164: 	//! Return the root of the query tree
165: 	optional_ptr<ProfilingNode> GetRoot() {
166: 		return root.get();
167: 	}
168: 
169: private:
170: 	ClientContext &context;
171: 
172: 	//! Whether or not the query profiler is running
173: 	bool running;
174: 	//! The lock used for accessing the global query profiler or flushing information to it from a thread
175: 	mutable std::mutex lock;
176: 
177: 	//! Whether or not the query requires profiling
178: 	bool query_requires_profiling;
179: 
180: 	//! The root of the query tree
181: 	unique_ptr<ProfilingNode> root;
182: 
183: 	//! Top level query information.
184: 	QueryInfo query_info;
185: 	//! The timer used to time the execution time of the entire query
186: 	Profiler main_query;
187: 	//! A map of a Physical Operator pointer to a tree node
188: 	TreeMap tree_map;
189: 	//! Whether or not we are running as part of a explain_analyze query
190: 	bool is_explain_analyze;
191: 
192: public:
193: 	const TreeMap &GetTreeMap() const {
194: 		return tree_map;
195: 	}
196: 
197: private:
198: 	//! The timer used to time the individual phases of the planning process
199: 	Profiler phase_profiler;
200: 	//! A mapping of the phase names to the timings
201: 	using PhaseTimingStorage = unordered_map<MetricsType, double, MetricsTypeHashFunction>;
202: 	PhaseTimingStorage phase_timings;
203: 	using PhaseTimingItem = PhaseTimingStorage::value_type;
204: 	//! The stack of currently active phases
205: 	vector<MetricsType> phase_stack;
206: 
207: private:
208: 	void MoveOptimizerPhasesToRoot();
209: 
210: 	//! Check whether or not an operator type requires query profiling. If none of the ops in a query require profiling
211: 	//! no profiling information is output.
212: 	bool OperatorRequiresProfiling(PhysicalOperatorType op_type);
213: 	ExplainFormat GetExplainFormat(ProfilerPrintFormat format) const;
214: };
215: 
216: } // namespace duckdb
[end of src/include/duckdb/main/query_profiler.hpp]
[start of src/include/duckdb/optimizer/filter_combiner.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/optimizer/filter_combiner.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/value.hpp"
12: #include "duckdb/common/unordered_map.hpp"
13: #include "duckdb/parser/expression_map.hpp"
14: #include "duckdb/planner/expression.hpp"
15: #include "duckdb/planner/filter/conjunction_filter.hpp"
16: #include "duckdb/planner/filter/constant_filter.hpp"
17: 
18: #include "duckdb/storage/data_table.hpp"
19: #include <functional>
20: #include <map>
21: 
22: namespace duckdb {
23: class Optimizer;
24: 
25: enum class ValueComparisonResult { PRUNE_LEFT, PRUNE_RIGHT, UNSATISFIABLE_CONDITION, PRUNE_NOTHING };
26: enum class FilterResult { UNSATISFIABLE, SUCCESS, UNSUPPORTED };
27: 
28: //! The FilterCombiner combines several filters and generates a logically equivalent set that is more efficient
29: //! Amongst others:
30: //! (1) it prunes obsolete filter conditions: i.e. [X > 5 and X > 7] => [X > 7]
31: //! (2) it generates new filters for expressions in the same equivalence set: i.e. [X = Y and X = 500] => [Y = 500]
32: //! (3) it prunes branches that have unsatisfiable filters: i.e. [X = 5 AND X > 6] => FALSE, prune branch
33: class FilterCombiner {
34: public:
35: 	explicit FilterCombiner(ClientContext &context);
36: 	explicit FilterCombiner(Optimizer &optimizer);
37: 
38: 	ClientContext &context;
39: 
40: public:
41: 	struct ExpressionValueInformation {
42: 		Value constant;
43: 		ExpressionType comparison_type;
44: 	};
45: 
46: 	FilterResult AddFilter(unique_ptr<Expression> expr);
47: 
48: 	//! Returns whether or not a set of integral values is a dense range (i.e. 1, 2, 3, 4, 5)
49: 	//! If this returns true - this sorts "in_list" as a side-effect
50: 	static bool IsDenseRange(vector<Value> &in_list);
51: 	static bool ContainsNull(vector<Value> &in_list);
52: 
53: 	void GenerateFilters(const std::function<void(unique_ptr<Expression> filter)> &callback);
54: 	bool HasFilters();
55: 	TableFilterSet GenerateTableScanFilters(const vector<ColumnIndex> &column_ids);
56: 	// vector<unique_ptr<TableFilter>> GenerateZonemapChecks(vector<idx_t> &column_ids, vector<unique_ptr<TableFilter>>
57: 	// &pushed_filters);
58: 
59: private:
60: 	FilterResult AddFilter(Expression &expr);
61: 	FilterResult AddBoundComparisonFilter(Expression &expr);
62: 	FilterResult AddTransitiveFilters(BoundComparisonExpression &comparison, bool is_root = true);
63: 	unique_ptr<Expression> FindTransitiveFilter(Expression &expr);
64: 	// unordered_map<idx_t, std::pair<Value *, Value *>>
65: 	// FindZonemapChecks(vector<idx_t> &column_ids, unordered_set<idx_t> &not_constants, Expression *filter);
66: 	Expression &GetNode(Expression &expr);
67: 	idx_t GetEquivalenceSet(Expression &expr);
68: 	FilterResult AddConstantComparison(vector<ExpressionValueInformation> &info_list, ExpressionValueInformation info);
69: 	//
70: 	//	//! Functions used to push and generate OR Filters
71: 	//	void LookUpConjunctions(Expression *expr);
72: 	//	bool BFSLookUpConjunctions(BoundConjunctionExpression *conjunction);
73: 	//	void VerifyOrsToPush(Expression &expr);
74: 	//
75: 	//	bool UpdateConjunctionFilter(BoundComparisonExpression *comparison_expr);
76: 	//	bool UpdateFilterByColumn(BoundColumnRefExpression *column_ref, BoundComparisonExpression *comparison_expr);
77: 	//	void GenerateORFilters(TableFilterSet &table_filter, vector<idx_t> &column_ids);
78: 	//
79: 	//	template <typename CONJUNCTION_TYPE>
80: 	//	void GenerateConjunctionFilter(BoundConjunctionExpression *conjunction, ConjunctionFilter *last_conj_filter) {
81: 	//		auto new_filter = NextConjunctionFilter<CONJUNCTION_TYPE>(conjunction);
82: 	//		auto conj_filter_ptr = (ConjunctionFilter *)new_filter.get();
83: 	//		last_conj_filter->child_filters.push_back(std::move(new_filter));
84: 	//		last_conj_filter = conj_filter_ptr;
85: 	//	}
86: 	//
87: 	//	template <typename CONJUNCTION_TYPE>
88: 	//	unique_ptr<TableFilter> NextConjunctionFilter(BoundConjunctionExpression *conjunction) {
89: 	//		unique_ptr<ConjunctionFilter> conj_filter = make_uniq<CONJUNCTION_TYPE>();
90: 	//		for (auto &expr : conjunction->children) {
91: 	//			auto comp_expr = (BoundComparisonExpression *)expr.get();
92: 	//			auto &const_expr =
93: 	//			    (comp_expr->left->type == ExpressionType::VALUE_CONSTANT) ? *comp_expr->left : *comp_expr->right;
94: 	//			auto const_value = ExpressionExecutor::EvaluateScalar(const_expr);
95: 	//			auto const_filter = make_uniq<ConstantFilter>(comp_expr->type, const_value);
96: 	//			conj_filter->child_filters.push_back(std::move(const_filter));
97: 	//		}
98: 	//		return std::move(conj_filter);
99: 	//	}
100: 
101: private:
102: 	vector<unique_ptr<Expression>> remaining_filters;
103: 
104: 	expression_map_t<unique_ptr<Expression>> stored_expressions;
105: 	expression_map_t<idx_t> equivalence_set_map;
106: 	unordered_map<idx_t, vector<ExpressionValueInformation>> constant_values;
107: 	unordered_map<idx_t, vector<reference<Expression>>> equivalence_map;
108: 	idx_t set_index = 0;
109: 	//
110: 	//	//! Structures used for OR Filters
111: 	//
112: 	//	struct ConjunctionsToPush {
113: 	//		BoundConjunctionExpression *root_or;
114: 	//
115: 	//		// only preserve AND if there is a single column in the expression
116: 	//		bool preserve_and = true;
117: 	//
118: 	//		// conjunction chain for this column
119: 	//		vector<unique_ptr<BoundConjunctionExpression>> conjunctions;
120: 	//	};
121: 	//
122: 	//	expression_map_t<vector<unique_ptr<ConjunctionsToPush>>> map_col_conjunctions;
123: 	//	vector<BoundColumnRefExpression *> vec_colref_insertion_order;
124: 	//
125: 	//	BoundConjunctionExpression *cur_root_or;
126: 	//	BoundConjunctionExpression *cur_conjunction;
127: 	//
128: 	//	BoundColumnRefExpression *cur_colref_to_push;
129: };
130: 
131: } // namespace duckdb
[end of src/include/duckdb/optimizer/filter_combiner.hpp]
[start of src/include/duckdb/optimizer/late_materialization.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/optimizer/late_materialization.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/constants.hpp"
12: #include "duckdb/optimizer/remove_unused_columns.hpp"
13: 
14: namespace duckdb {
15: class LogicalOperator;
16: class LogicalGet;
17: class Optimizer;
18: 
19: //! Transform
20: class LateMaterialization : public BaseColumnPruner {
21: public:
22: 	explicit LateMaterialization(Optimizer &optimizer);
23: 
24: 	unique_ptr<LogicalOperator> Optimize(unique_ptr<LogicalOperator> op);
25: 
26: private:
27: 	bool TryLateMaterialization(unique_ptr<LogicalOperator> &op);
28: 
29: 	unique_ptr<LogicalGet> ConstructLHS(LogicalGet &get);
30: 	ColumnBinding ConstructRHS(unique_ptr<LogicalOperator> &op);
31: 	idx_t GetOrInsertRowId(LogicalGet &get);
32: 
33: 	void ReplaceTopLevelTableIndex(LogicalOperator &op, idx_t new_index);
34: 	void ReplaceTableReferences(Expression &expr, idx_t new_table_index);
35: 	unique_ptr<Expression> GetExpression(LogicalOperator &op, idx_t column_index);
36: 	void ReplaceExpressionReferences(LogicalOperator &next_op, unique_ptr<Expression> &expr);
37: 	bool OptimizeLargeLimit(LogicalOperator &child);
38: 
39: private:
40: 	Optimizer &optimizer;
41: 	//! The max row count for which we will consider late materialization
42: 	idx_t max_row_count;
43: 	//! The type of the row id column
44: 	LogicalType row_id_type;
45: };
46: 
47: } // namespace duckdb
[end of src/include/duckdb/optimizer/late_materialization.hpp]
[start of src/main/capi/helper-c.cpp]
1: #include "duckdb/main/capi/capi_internal.hpp"
2: 
3: namespace duckdb {
4: 
5: LogicalTypeId ConvertCTypeToCPP(duckdb_type c_type) {
6: 	switch (c_type) {
7: 	case DUCKDB_TYPE_INVALID:
8: 		return LogicalTypeId::INVALID;
9: 	case DUCKDB_TYPE_BOOLEAN:
10: 		return LogicalTypeId::BOOLEAN;
11: 	case DUCKDB_TYPE_TINYINT:
12: 		return LogicalTypeId::TINYINT;
13: 	case DUCKDB_TYPE_SMALLINT:
14: 		return LogicalTypeId::SMALLINT;
15: 	case DUCKDB_TYPE_INTEGER:
16: 		return LogicalTypeId::INTEGER;
17: 	case DUCKDB_TYPE_BIGINT:
18: 		return LogicalTypeId::BIGINT;
19: 	case DUCKDB_TYPE_UTINYINT:
20: 		return LogicalTypeId::UTINYINT;
21: 	case DUCKDB_TYPE_USMALLINT:
22: 		return LogicalTypeId::USMALLINT;
23: 	case DUCKDB_TYPE_UINTEGER:
24: 		return LogicalTypeId::UINTEGER;
25: 	case DUCKDB_TYPE_UBIGINT:
26: 		return LogicalTypeId::UBIGINT;
27: 	case DUCKDB_TYPE_FLOAT:
28: 		return LogicalTypeId::FLOAT;
29: 	case DUCKDB_TYPE_DOUBLE:
30: 		return LogicalTypeId::DOUBLE;
31: 	case DUCKDB_TYPE_TIMESTAMP:
32: 		return LogicalTypeId::TIMESTAMP;
33: 	case DUCKDB_TYPE_DATE:
34: 		return LogicalTypeId::DATE;
35: 	case DUCKDB_TYPE_TIME:
36: 		return LogicalTypeId::TIME;
37: 	case DUCKDB_TYPE_INTERVAL:
38: 		return LogicalTypeId::INTERVAL;
39: 	case DUCKDB_TYPE_HUGEINT:
40: 		return LogicalTypeId::HUGEINT;
41: 	case DUCKDB_TYPE_UHUGEINT:
42: 		return LogicalTypeId::UHUGEINT;
43: 	case DUCKDB_TYPE_VARCHAR:
44: 		return LogicalTypeId::VARCHAR;
45: 	case DUCKDB_TYPE_BLOB:
46: 		return LogicalTypeId::BLOB;
47: 	case DUCKDB_TYPE_DECIMAL:
48: 		return LogicalTypeId::DECIMAL;
49: 	case DUCKDB_TYPE_TIMESTAMP_S:
50: 		return LogicalTypeId::TIMESTAMP_SEC;
51: 	case DUCKDB_TYPE_TIMESTAMP_MS:
52: 		return LogicalTypeId::TIMESTAMP_MS;
53: 	case DUCKDB_TYPE_TIMESTAMP_NS:
54: 		return LogicalTypeId::TIMESTAMP_NS;
55: 	case DUCKDB_TYPE_ENUM:
56: 		return LogicalTypeId::ENUM;
57: 	case DUCKDB_TYPE_LIST:
58: 		return LogicalTypeId::LIST;
59: 	case DUCKDB_TYPE_STRUCT:
60: 		return LogicalTypeId::STRUCT;
61: 	case DUCKDB_TYPE_MAP:
62: 		return LogicalTypeId::MAP;
63: 	case DUCKDB_TYPE_ARRAY:
64: 		return LogicalTypeId::ARRAY;
65: 	case DUCKDB_TYPE_UUID:
66: 		return LogicalTypeId::UUID;
67: 	case DUCKDB_TYPE_UNION:
68: 		return LogicalTypeId::UNION;
69: 	case DUCKDB_TYPE_BIT:
70: 		return LogicalTypeId::BIT;
71: 	case DUCKDB_TYPE_TIME_TZ:
72: 		return LogicalTypeId::TIME_TZ;
73: 	case DUCKDB_TYPE_TIMESTAMP_TZ:
74: 		return LogicalTypeId::TIMESTAMP_TZ;
75: 	case DUCKDB_TYPE_ANY:
76: 		return LogicalTypeId::ANY;
77: 	case DUCKDB_TYPE_VARINT:
78: 		return LogicalTypeId::VARINT;
79: 	case DUCKDB_TYPE_SQLNULL:
80: 		return LogicalTypeId::SQLNULL;
81: 	default: // LCOV_EXCL_START
82: 		D_ASSERT(0);
83: 		return LogicalTypeId::INVALID;
84: 	} // LCOV_EXCL_STOP
85: }
86: 
87: duckdb_type ConvertCPPTypeToC(const LogicalType &sql_type) {
88: 	switch (sql_type.id()) {
89: 	case LogicalTypeId::INVALID:
90: 		return DUCKDB_TYPE_INVALID;
91: 	case LogicalTypeId::BOOLEAN:
92: 		return DUCKDB_TYPE_BOOLEAN;
93: 	case LogicalTypeId::TINYINT:
94: 		return DUCKDB_TYPE_TINYINT;
95: 	case LogicalTypeId::SMALLINT:
96: 		return DUCKDB_TYPE_SMALLINT;
97: 	case LogicalTypeId::INTEGER:
98: 		return DUCKDB_TYPE_INTEGER;
99: 	case LogicalTypeId::BIGINT:
100: 		return DUCKDB_TYPE_BIGINT;
101: 	case LogicalTypeId::UTINYINT:
102: 		return DUCKDB_TYPE_UTINYINT;
103: 	case LogicalTypeId::USMALLINT:
104: 		return DUCKDB_TYPE_USMALLINT;
105: 	case LogicalTypeId::UINTEGER:
106: 		return DUCKDB_TYPE_UINTEGER;
107: 	case LogicalTypeId::UBIGINT:
108: 		return DUCKDB_TYPE_UBIGINT;
109: 	case LogicalTypeId::HUGEINT:
110: 		return DUCKDB_TYPE_HUGEINT;
111: 	case LogicalTypeId::UHUGEINT:
112: 		return DUCKDB_TYPE_UHUGEINT;
113: 	case LogicalTypeId::FLOAT:
114: 		return DUCKDB_TYPE_FLOAT;
115: 	case LogicalTypeId::DOUBLE:
116: 		return DUCKDB_TYPE_DOUBLE;
117: 	case LogicalTypeId::TIMESTAMP:
118: 		return DUCKDB_TYPE_TIMESTAMP;
119: 	case LogicalTypeId::TIMESTAMP_TZ:
120: 		return DUCKDB_TYPE_TIMESTAMP_TZ;
121: 	case LogicalTypeId::TIMESTAMP_SEC:
122: 		return DUCKDB_TYPE_TIMESTAMP_S;
123: 	case LogicalTypeId::TIMESTAMP_MS:
124: 		return DUCKDB_TYPE_TIMESTAMP_MS;
125: 	case LogicalTypeId::TIMESTAMP_NS:
126: 		return DUCKDB_TYPE_TIMESTAMP_NS;
127: 	case LogicalTypeId::DATE:
128: 		return DUCKDB_TYPE_DATE;
129: 	case LogicalTypeId::TIME:
130: 		return DUCKDB_TYPE_TIME;
131: 	case LogicalTypeId::TIME_TZ:
132: 		return DUCKDB_TYPE_TIME_TZ;
133: 	case LogicalTypeId::VARCHAR:
134: 		return DUCKDB_TYPE_VARCHAR;
135: 	case LogicalTypeId::BLOB:
136: 		return DUCKDB_TYPE_BLOB;
137: 	case LogicalTypeId::BIT:
138: 		return DUCKDB_TYPE_BIT;
139: 	case LogicalTypeId::VARINT:
140: 		return DUCKDB_TYPE_VARINT;
141: 	case LogicalTypeId::INTERVAL:
142: 		return DUCKDB_TYPE_INTERVAL;
143: 	case LogicalTypeId::DECIMAL:
144: 		return DUCKDB_TYPE_DECIMAL;
145: 	case LogicalTypeId::ENUM:
146: 		return DUCKDB_TYPE_ENUM;
147: 	case LogicalTypeId::LIST:
148: 		return DUCKDB_TYPE_LIST;
149: 	case LogicalTypeId::STRUCT:
150: 		return DUCKDB_TYPE_STRUCT;
151: 	case LogicalTypeId::MAP:
152: 		return DUCKDB_TYPE_MAP;
153: 	case LogicalTypeId::UNION:
154: 		return DUCKDB_TYPE_UNION;
155: 	case LogicalTypeId::UUID:
156: 		return DUCKDB_TYPE_UUID;
157: 	case LogicalTypeId::ARRAY:
158: 		return DUCKDB_TYPE_ARRAY;
159: 	case LogicalTypeId::ANY:
160: 		return DUCKDB_TYPE_ANY;
161: 	case LogicalTypeId::SQLNULL:
162: 		return DUCKDB_TYPE_SQLNULL;
163: 	default: // LCOV_EXCL_START
164: 		D_ASSERT(0);
165: 		return DUCKDB_TYPE_INVALID;
166: 	} // LCOV_EXCL_STOP
167: }
168: 
169: idx_t GetCTypeSize(duckdb_type type) {
170: 	switch (type) {
171: 	case DUCKDB_TYPE_BOOLEAN:
172: 		return sizeof(bool);
173: 	case DUCKDB_TYPE_TINYINT:
174: 		return sizeof(int8_t);
175: 	case DUCKDB_TYPE_SMALLINT:
176: 		return sizeof(int16_t);
177: 	case DUCKDB_TYPE_INTEGER:
178: 		return sizeof(int32_t);
179: 	case DUCKDB_TYPE_BIGINT:
180: 		return sizeof(int64_t);
181: 	case DUCKDB_TYPE_UTINYINT:
182: 		return sizeof(uint8_t);
183: 	case DUCKDB_TYPE_USMALLINT:
184: 		return sizeof(uint16_t);
185: 	case DUCKDB_TYPE_UINTEGER:
186: 		return sizeof(uint32_t);
187: 	case DUCKDB_TYPE_UBIGINT:
188: 		return sizeof(uint64_t);
189: 	case DUCKDB_TYPE_UHUGEINT:
190: 	case DUCKDB_TYPE_HUGEINT:
191: 	case DUCKDB_TYPE_UUID:
192: 		return sizeof(duckdb_hugeint);
193: 	case DUCKDB_TYPE_FLOAT:
194: 		return sizeof(float);
195: 	case DUCKDB_TYPE_DOUBLE:
196: 		return sizeof(double);
197: 	case DUCKDB_TYPE_DATE:
198: 		return sizeof(duckdb_date);
199: 	case DUCKDB_TYPE_TIME:
200: 		return sizeof(duckdb_time);
201: 	case DUCKDB_TYPE_TIMESTAMP:
202: 	case DUCKDB_TYPE_TIMESTAMP_TZ:
203: 	case DUCKDB_TYPE_TIMESTAMP_S:
204: 	case DUCKDB_TYPE_TIMESTAMP_MS:
205: 	case DUCKDB_TYPE_TIMESTAMP_NS:
206: 		return sizeof(duckdb_timestamp);
207: 	case DUCKDB_TYPE_VARCHAR:
208: 		return sizeof(const char *);
209: 	case DUCKDB_TYPE_BLOB:
210: 		return sizeof(duckdb_blob);
211: 	case DUCKDB_TYPE_INTERVAL:
212: 		return sizeof(duckdb_interval);
213: 	case DUCKDB_TYPE_DECIMAL:
214: 		return sizeof(duckdb_hugeint);
215: 	default: // LCOV_EXCL_START
216: 		// Unsupported nested or complex type. Internally, we set the null mask to NULL.
217: 		// This is a deprecated code path. Use the Vector Interface for nested and complex types.
218: 		return 0;
219: 	} // LCOV_EXCL_STOP
220: }
221: 
222: duckdb_statement_type StatementTypeToC(duckdb::StatementType statement_type) {
223: 	switch (statement_type) {
224: 	case duckdb::StatementType::SELECT_STATEMENT:
225: 		return DUCKDB_STATEMENT_TYPE_SELECT;
226: 	case duckdb::StatementType::INVALID_STATEMENT:
227: 		return DUCKDB_STATEMENT_TYPE_INVALID;
228: 	case duckdb::StatementType::INSERT_STATEMENT:
229: 		return DUCKDB_STATEMENT_TYPE_INSERT;
230: 	case duckdb::StatementType::UPDATE_STATEMENT:
231: 		return DUCKDB_STATEMENT_TYPE_UPDATE;
232: 	case duckdb::StatementType::EXPLAIN_STATEMENT:
233: 		return DUCKDB_STATEMENT_TYPE_EXPLAIN;
234: 	case duckdb::StatementType::DELETE_STATEMENT:
235: 		return DUCKDB_STATEMENT_TYPE_DELETE;
236: 	case duckdb::StatementType::PREPARE_STATEMENT:
237: 		return DUCKDB_STATEMENT_TYPE_PREPARE;
238: 	case duckdb::StatementType::CREATE_STATEMENT:
239: 		return DUCKDB_STATEMENT_TYPE_CREATE;
240: 	case duckdb::StatementType::EXECUTE_STATEMENT:
241: 		return DUCKDB_STATEMENT_TYPE_EXECUTE;
242: 	case duckdb::StatementType::ALTER_STATEMENT:
243: 		return DUCKDB_STATEMENT_TYPE_ALTER;
244: 	case duckdb::StatementType::TRANSACTION_STATEMENT:
245: 		return DUCKDB_STATEMENT_TYPE_TRANSACTION;
246: 	case duckdb::StatementType::COPY_STATEMENT:
247: 		return DUCKDB_STATEMENT_TYPE_COPY;
248: 	case duckdb::StatementType::ANALYZE_STATEMENT:
249: 		return DUCKDB_STATEMENT_TYPE_ANALYZE;
250: 	case duckdb::StatementType::VARIABLE_SET_STATEMENT:
251: 		return DUCKDB_STATEMENT_TYPE_VARIABLE_SET;
252: 	case duckdb::StatementType::CREATE_FUNC_STATEMENT:
253: 		return DUCKDB_STATEMENT_TYPE_CREATE_FUNC;
254: 	case duckdb::StatementType::DROP_STATEMENT:
255: 		return DUCKDB_STATEMENT_TYPE_DROP;
256: 	case duckdb::StatementType::EXPORT_STATEMENT:
257: 		return DUCKDB_STATEMENT_TYPE_EXPORT;
258: 	case duckdb::StatementType::PRAGMA_STATEMENT:
259: 		return DUCKDB_STATEMENT_TYPE_PRAGMA;
260: 	case duckdb::StatementType::VACUUM_STATEMENT:
261: 		return DUCKDB_STATEMENT_TYPE_VACUUM;
262: 	case duckdb::StatementType::CALL_STATEMENT:
263: 		return DUCKDB_STATEMENT_TYPE_CALL;
264: 	case duckdb::StatementType::SET_STATEMENT:
265: 		return DUCKDB_STATEMENT_TYPE_SET;
266: 	case duckdb::StatementType::LOAD_STATEMENT:
267: 		return DUCKDB_STATEMENT_TYPE_LOAD;
268: 	case duckdb::StatementType::RELATION_STATEMENT:
269: 		return DUCKDB_STATEMENT_TYPE_RELATION;
270: 	case duckdb::StatementType::EXTENSION_STATEMENT:
271: 		return DUCKDB_STATEMENT_TYPE_EXTENSION;
272: 	case duckdb::StatementType::LOGICAL_PLAN_STATEMENT:
273: 		return DUCKDB_STATEMENT_TYPE_LOGICAL_PLAN;
274: 	case duckdb::StatementType::ATTACH_STATEMENT:
275: 		return DUCKDB_STATEMENT_TYPE_ATTACH;
276: 	case duckdb::StatementType::DETACH_STATEMENT:
277: 		return DUCKDB_STATEMENT_TYPE_DETACH;
278: 	case duckdb::StatementType::MULTI_STATEMENT:
279: 		return DUCKDB_STATEMENT_TYPE_MULTI;
280: 	default:
281: 		return DUCKDB_STATEMENT_TYPE_INVALID;
282: 	}
283: }
284: 
285: } // namespace duckdb
286: 
287: void *duckdb_malloc(size_t size) {
288: 	return malloc(size);
289: }
290: 
291: void duckdb_free(void *ptr) {
292: 	free(ptr);
293: }
294: 
295: idx_t duckdb_vector_size() {
296: 	return STANDARD_VECTOR_SIZE;
297: }
298: 
299: bool duckdb_string_is_inlined(duckdb_string_t string_p) {
300: 	static_assert(sizeof(duckdb_string_t) == sizeof(duckdb::string_t),
301: 	              "duckdb_string_t should have the same memory layout as duckdb::string_t");
302: 	auto &string = *reinterpret_cast<duckdb::string_t *>(&string_p);
303: 	return string.IsInlined();
304: }
305: 
306: uint32_t duckdb_string_t_length(duckdb_string_t string_p) {
307: 	static_assert(sizeof(duckdb_string_t) == sizeof(duckdb::string_t),
308: 	              "duckdb_string_t should have the same memory layout as duckdb::string_t");
309: 	auto &string = *reinterpret_cast<duckdb::string_t *>(&string_p);
310: 	return static_cast<uint32_t>(string.GetSize());
311: }
312: 
313: const char *duckdb_string_t_data(duckdb_string_t *string_p) {
314: 	static_assert(sizeof(duckdb_string_t) == sizeof(duckdb::string_t),
315: 	              "duckdb_string_t should have the same memory layout as duckdb::string_t");
316: 	auto &string = *reinterpret_cast<duckdb::string_t *>(string_p);
317: 	return string.GetData();
318: }
[end of src/main/capi/helper-c.cpp]
[start of src/optimizer/column_lifetime_analyzer.cpp]
1: #include "duckdb/optimizer/column_lifetime_analyzer.hpp"
2: 
3: #include "duckdb/main/client_context.hpp"
4: #include "duckdb/optimizer/column_binding_replacer.hpp"
5: #include "duckdb/optimizer/optimizer.hpp"
6: #include "duckdb/optimizer/topn_optimizer.hpp"
7: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
8: #include "duckdb/planner/expression/bound_constant_expression.hpp"
9: #include "duckdb/planner/expression_iterator.hpp"
10: #include "duckdb/planner/operator/logical_comparison_join.hpp"
11: #include "duckdb/planner/operator/logical_filter.hpp"
12: #include "duckdb/planner/operator/logical_order.hpp"
13: #include "duckdb/planner/operator/logical_projection.hpp"
14: 
15: namespace duckdb {
16: 
17: void ColumnLifetimeAnalyzer::ExtractUnusedColumnBindings(const vector<ColumnBinding> &bindings,
18:                                                          column_binding_set_t &unused_bindings) {
19: 	for (idx_t i = 0; i < bindings.size(); i++) {
20: 		if (column_references.find(bindings[i]) == column_references.end()) {
21: 			unused_bindings.insert(bindings[i]);
22: 		}
23: 	}
24: }
25: 
26: void ColumnLifetimeAnalyzer::GenerateProjectionMap(vector<ColumnBinding> bindings,
27:                                                    column_binding_set_t &unused_bindings,
28:                                                    vector<idx_t> &projection_map) {
29: 	projection_map.clear();
30: 	if (unused_bindings.empty()) {
31: 		return;
32: 	}
33: 	// now iterate over the result bindings of the child
34: 	for (idx_t i = 0; i < bindings.size(); i++) {
35: 		// if this binding does not belong to the unused bindings, add it to the projection map
36: 		if (unused_bindings.find(bindings[i]) == unused_bindings.end()) {
37: 			projection_map.push_back(i);
38: 		}
39: 	}
40: 	if (projection_map.size() == bindings.size()) {
41: 		projection_map.clear();
42: 	}
43: }
44: 
45: void ColumnLifetimeAnalyzer::StandardVisitOperator(LogicalOperator &op) {
46: 	VisitOperatorExpressions(op);
47: 	VisitOperatorChildren(op);
48: }
49: 
50: void ExtractColumnBindings(Expression &expr, vector<ColumnBinding> &bindings) {
51: 	if (expr.GetExpressionType() == ExpressionType::BOUND_COLUMN_REF) {
52: 		auto &bound_ref = expr.Cast<BoundColumnRefExpression>();
53: 		bindings.push_back(bound_ref.binding);
54: 	}
55: 	ExpressionIterator::EnumerateChildren(expr, [&](Expression &child) { ExtractColumnBindings(child, bindings); });
56: }
57: 
58: void ColumnLifetimeAnalyzer::VisitOperator(LogicalOperator &op) {
59: 	Verify(op);
60: 	if (TopN::CanOptimize(op) && op.children[0]->type == LogicalOperatorType::LOGICAL_ORDER_BY) {
61: 		// Let's not mess with this, TopN is more important than projection maps
62: 		// TopN does not support a projection map like Order does
63: 		VisitOperatorExpressions(op);                        // Visit LIMIT
64: 		VisitOperatorExpressions(*op.children[0]);           // Visit ORDER
65: 		StandardVisitOperator(*op.children[0]->children[0]); // Recurse into child of ORDER
66: 		return;
67: 	}
68: 	switch (op.type) {
69: 	case LogicalOperatorType::LOGICAL_AGGREGATE_AND_GROUP_BY: {
70: 		// FIXME: groups that are not referenced can be removed from projection
71: 		// recurse into the children of the aggregate
72: 		ColumnLifetimeAnalyzer analyzer(optimizer, root);
73: 		analyzer.StandardVisitOperator(op);
74: 		return;
75: 	}
76: 	case LogicalOperatorType::LOGICAL_ASOF_JOIN:
77: 	case LogicalOperatorType::LOGICAL_DELIM_JOIN:
78: 	case LogicalOperatorType::LOGICAL_COMPARISON_JOIN: {
79: 		auto &comp_join = op.Cast<LogicalComparisonJoin>();
80: 		if (everything_referenced) {
81: 			break;
82: 		}
83: 
84: 		// FIXME: for now, we only push into the projection map for equality (hash) joins
85: 		idx_t has_range = 0;
86: 		if (!comp_join.HasEquality(has_range) || optimizer.context.config.prefer_range_joins) {
87: 			return;
88: 		}
89: 
90: 		column_binding_set_t lhs_unused;
91: 		column_binding_set_t rhs_unused;
92: 		ExtractUnusedColumnBindings(op.children[0]->GetColumnBindings(), lhs_unused);
93: 		ExtractUnusedColumnBindings(op.children[1]->GetColumnBindings(), rhs_unused);
94: 
95: 		StandardVisitOperator(op);
96: 
97: 		// then generate the projection map
98: 		if (op.type != LogicalOperatorType::LOGICAL_ASOF_JOIN) {
99: 			// FIXME: left_projection_map in ASOF join
100: 			GenerateProjectionMap(op.children[0]->GetColumnBindings(), lhs_unused, comp_join.left_projection_map);
101: 		}
102: 		GenerateProjectionMap(op.children[1]->GetColumnBindings(), rhs_unused, comp_join.right_projection_map);
103: 		return;
104: 	}
105: 	case LogicalOperatorType::LOGICAL_INSERT:
106: 	case LogicalOperatorType::LOGICAL_UPDATE:
107: 	case LogicalOperatorType::LOGICAL_DELETE:
108: 		//! When RETURNING is used, a PROJECTION is the top level operator for INSERTS, UPDATES, and DELETES
109: 		//! We still need to project all values from these operators so the projection
110: 		//! on top of them can select from only the table values being inserted.
111: 	case LogicalOperatorType::LOGICAL_UNION:
112: 	case LogicalOperatorType::LOGICAL_EXCEPT:
113: 	case LogicalOperatorType::LOGICAL_INTERSECT:
114: 	case LogicalOperatorType::LOGICAL_MATERIALIZED_CTE: {
115: 		// for set operations/materialized CTEs we don't remove anything, just recursively visit the children
116: 		// FIXME: for UNION we can remove unreferenced columns as long as everything_referenced is false (i.e. we
117: 		// encounter a UNION node that is not preceded by a DISTINCT)
118: 		ColumnLifetimeAnalyzer analyzer(optimizer, root, true);
119: 		analyzer.StandardVisitOperator(op);
120: 		return;
121: 	}
122: 	case LogicalOperatorType::LOGICAL_PROJECTION: {
123: 		// then recurse into the children of this projection
124: 		ColumnLifetimeAnalyzer analyzer(optimizer, root);
125: 		analyzer.StandardVisitOperator(op);
126: 		return;
127: 	}
128: 	case LogicalOperatorType::LOGICAL_ORDER_BY: {
129: 		auto &order = op.Cast<LogicalOrder>();
130: 		if (everything_referenced) {
131: 			break;
132: 		}
133: 
134: 		column_binding_set_t unused_bindings;
135: 		ExtractUnusedColumnBindings(op.children[0]->GetColumnBindings(), unused_bindings);
136: 
137: 		StandardVisitOperator(op);
138: 
139: 		GenerateProjectionMap(op.children[0]->GetColumnBindings(), unused_bindings, order.projection_map);
140: 		return;
141: 	}
142: 	case LogicalOperatorType::LOGICAL_DISTINCT: {
143: 		// distinct, all projected columns are used for the DISTINCT computation
144: 		// mark all columns as used and continue to the children
145: 		// FIXME: DISTINCT with expression list does not implicitly reference everything
146: 		everything_referenced = true;
147: 		break;
148: 	}
149: 	case LogicalOperatorType::LOGICAL_FILTER: {
150: 		auto &filter = op.Cast<LogicalFilter>();
151: 		if (everything_referenced) {
152: 			break;
153: 		}
154: 
155: 		// filter, figure out which columns are not needed after the filter
156: 		column_binding_set_t unused_bindings;
157: 		ExtractUnusedColumnBindings(op.children[0]->GetColumnBindings(), unused_bindings);
158: 
159: 		StandardVisitOperator(op);
160: 
161: 		// then generate the projection map
162: 		GenerateProjectionMap(op.children[0]->GetColumnBindings(), unused_bindings, filter.projection_map);
163: 
164: 		return;
165: 	}
166: 	default:
167: 		break;
168: 	}
169: 	StandardVisitOperator(op);
170: }
171: 
172: void ColumnLifetimeAnalyzer::Verify(LogicalOperator &op) {
173: #ifdef DEBUG
174: 	if (everything_referenced) {
175: 		return;
176: 	}
177: 	switch (op.type) {
178: 	case LogicalOperatorType::LOGICAL_ASOF_JOIN:
179: 	case LogicalOperatorType::LOGICAL_DELIM_JOIN:
180: 	case LogicalOperatorType::LOGICAL_COMPARISON_JOIN:
181: 		AddVerificationProjection(op.children[0]);
182: 		if (op.Cast<LogicalComparisonJoin>().join_type != JoinType::MARK) { // Can't mess up the mark_index
183: 			AddVerificationProjection(op.children[1]);
184: 		}
185: 		break;
186: 	case LogicalOperatorType::LOGICAL_ORDER_BY:
187: 	case LogicalOperatorType::LOGICAL_FILTER:
188: 		AddVerificationProjection(op.children[0]);
189: 		break;
190: 	default:
191: 		break;
192: 	}
193: #endif
194: }
195: 
196: void ColumnLifetimeAnalyzer::AddVerificationProjection(unique_ptr<LogicalOperator> &child) {
197: 	child->ResolveOperatorTypes();
198: 	const auto child_types = child->types;
199: 	const auto child_bindings = child->GetColumnBindings();
200: 	const auto column_count = child_bindings.size();
201: 
202: 	// If our child has columns [i, j], we will generate a projection like so [NULL, j, NULL, i, NULL]
203: 	const auto projection_column_count = column_count * 2 + 1;
204: 	vector<unique_ptr<Expression>> expressions;
205: 	expressions.reserve(projection_column_count);
206: 
207: 	// First fill with all NULLs
208: 	for (idx_t col_idx = 0; col_idx < projection_column_count; col_idx++) {
209: 		expressions.emplace_back(make_uniq<BoundConstantExpression>(Value(LogicalType::UTINYINT)));
210: 	}
211: 
212: 	// Now place the "real" columns in their respective positions, while keeping track of which column becomes which
213: 	const auto table_index = optimizer.binder.GenerateTableIndex();
214: 	ColumnBindingReplacer replacer;
215: 	for (idx_t col_idx = 0; col_idx < column_count; col_idx++) {
216: 		const auto &old_binding = child_bindings[col_idx];
217: 		const auto new_col_idx = projection_column_count - 2 - col_idx * 2;
218: 		expressions[new_col_idx] = make_uniq<BoundColumnRefExpression>(child_types[col_idx], old_binding);
219: 		replacer.replacement_bindings.emplace_back(old_binding, ColumnBinding(table_index, new_col_idx));
220: 	}
221: 
222: 	// Create a projection and swap the operators accordingly
223: 	auto projection = make_uniq<LogicalProjection>(table_index, std::move(expressions));
224: 	projection->children.emplace_back(std::move(child));
225: 	child = std::move(projection);
226: 
227: 	// Replace references to the old binding (higher up in the plan) with references to the new binding
228: 	replacer.stop_operator = child.get();
229: 	replacer.VisitOperator(root);
230: 
231: 	// Add new bindings to column_references, else they are considered "unused"
232: 	for (const auto &replacement_binding : replacer.replacement_bindings) {
233: 		if (column_references.find(replacement_binding.old_binding) != column_references.end()) {
234: 			column_references.insert(replacement_binding.new_binding);
235: 		}
236: 	}
237: }
238: 
239: unique_ptr<Expression> ColumnLifetimeAnalyzer::VisitReplace(BoundColumnRefExpression &expr,
240:                                                             unique_ptr<Expression> *expr_ptr) {
241: 	column_references.insert(expr.binding);
242: 	return nullptr;
243: }
244: 
245: unique_ptr<Expression> ColumnLifetimeAnalyzer::VisitReplace(BoundReferenceExpression &expr,
246:                                                             unique_ptr<Expression> *expr_ptr) {
247: 	// BoundReferenceExpression should not be used here yet, they only belong in the physical plan
248: 	throw InternalException("BoundReferenceExpression should not be used here yet!");
249: }
250: 
251: } // namespace duckdb
[end of src/optimizer/column_lifetime_analyzer.cpp]
[start of src/optimizer/late_materialization.cpp]
1: #include "duckdb/optimizer/late_materialization.hpp"
2: #include "duckdb/planner/operator/logical_comparison_join.hpp"
3: #include "duckdb/planner/operator/logical_filter.hpp"
4: #include "duckdb/planner/operator/logical_get.hpp"
5: #include "duckdb/planner/operator/logical_limit.hpp"
6: #include "duckdb/planner/operator/logical_order.hpp"
7: #include "duckdb/planner/operator/logical_projection.hpp"
8: #include "duckdb/planner/operator/logical_sample.hpp"
9: #include "duckdb/planner/operator/logical_top_n.hpp"
10: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
11: #include "duckdb/planner/binder.hpp"
12: #include "duckdb/optimizer/optimizer.hpp"
13: #include "duckdb/planner/expression_iterator.hpp"
14: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
15: #include "duckdb/main/client_config.hpp"
16: 
17: namespace duckdb {
18: 
19: LateMaterialization::LateMaterialization(Optimizer &optimizer) : optimizer(optimizer) {
20: 	max_row_count = ClientConfig::GetConfig(optimizer.context).late_materialization_max_rows;
21: }
22: 
23: idx_t LateMaterialization::GetOrInsertRowId(LogicalGet &get) {
24: 	auto &column_ids = get.GetMutableColumnIds();
25: 	// check if it is already projected
26: 	for (idx_t i = 0; i < column_ids.size(); ++i) {
27: 		if (column_ids[i].IsRowIdColumn()) {
28: 			// already projected - return the id
29: 			return i;
30: 		}
31: 	}
32: 	// row id is not yet projected - push it and return the new index
33: 	column_ids.push_back(ColumnIndex(COLUMN_IDENTIFIER_ROW_ID));
34: 	if (!get.projection_ids.empty()) {
35: 		get.projection_ids.push_back(column_ids.size() - 1);
36: 	}
37: 	if (!get.types.empty()) {
38: 		get.types.push_back(row_id_type);
39: 	}
40: 	return column_ids.size() - 1;
41: }
42: 
43: unique_ptr<LogicalGet> LateMaterialization::ConstructLHS(LogicalGet &get) {
44: 	// we need to construct a new scan of the same table
45: 	auto table_index = optimizer.binder.GenerateTableIndex();
46: 	auto new_get = make_uniq<LogicalGet>(table_index, get.function, get.bind_data->Copy(), get.returned_types,
47: 	                                     get.names, get.virtual_columns);
48: 	new_get->GetMutableColumnIds() = get.GetColumnIds();
49: 	new_get->projection_ids = get.projection_ids;
50: 	return new_get;
51: }
52: 
53: ColumnBinding LateMaterialization::ConstructRHS(unique_ptr<LogicalOperator> &op) {
54: 	// traverse down until we reach the LogicalGet
55: 	vector<reference<LogicalOperator>> stack;
56: 	reference<LogicalOperator> child = *op->children[0];
57: 	while (child.get().type != LogicalOperatorType::LOGICAL_GET) {
58: 		stack.push_back(child);
59: 		D_ASSERT(child.get().children.size() == 1);
60: 		child = *child.get().children[0];
61: 	}
62: 	// we have reached the logical get - now we need to push the row-id column (if it is not yet projected out)
63: 	auto &get = child.get().Cast<LogicalGet>();
64: 	auto row_id_idx = GetOrInsertRowId(get);
65: 	idx_t column_count = get.projection_ids.empty() ? get.GetColumnIds().size() : get.projection_ids.size();
66: 	D_ASSERT(column_count == get.GetColumnBindings().size());
67: 
68: 	// the row id has been projected - now project it up the stack
69: 	ColumnBinding row_id_binding(get.table_index, row_id_idx);
70: 	for (idx_t i = stack.size(); i > 0; i--) {
71: 		auto &op = stack[i - 1].get();
72: 		switch (op.type) {
73: 		case LogicalOperatorType::LOGICAL_PROJECTION: {
74: 			auto &proj = op.Cast<LogicalProjection>();
75: 			// push a projection of the row-id column
76: 			proj.expressions.push_back(make_uniq<BoundColumnRefExpression>("rowid", row_id_type, row_id_binding));
77: 			// modify the row-id-binding to push to the new projection
78: 			row_id_binding = ColumnBinding(proj.table_index, proj.expressions.size() - 1);
79: 			column_count = proj.expressions.size();
80: 			break;
81: 		}
82: 		case LogicalOperatorType::LOGICAL_FILTER: {
83: 			auto &filter = op.Cast<LogicalFilter>();
84: 			// column bindings pass-through this operator as-is UNLESS the filter has a projection map
85: 			if (filter.HasProjectionMap()) {
86: 				// if the filter has a projection map, we need to project the new column
87: 				filter.projection_map.push_back(column_count - 1);
88: 			}
89: 			break;
90: 		}
91: 		default:
92: 			throw InternalException("Unsupported logical operator in LateMaterialization::ConstructRHS");
93: 		}
94: 	}
95: 	return row_id_binding;
96: }
97: 
98: void LateMaterialization::ReplaceTopLevelTableIndex(LogicalOperator &root, idx_t new_index) {
99: 	reference<LogicalOperator> current_op = root;
100: 	while (true) {
101: 		auto &op = current_op.get();
102: 		switch (op.type) {
103: 		case LogicalOperatorType::LOGICAL_PROJECTION: {
104: 			// reached a projection - modify the table index and return
105: 			auto &proj = op.Cast<LogicalProjection>();
106: 			proj.table_index = new_index;
107: 			return;
108: 		}
109: 		case LogicalOperatorType::LOGICAL_GET: {
110: 			// reached the root get - modify the table index and return
111: 			auto &get = op.Cast<LogicalGet>();
112: 			get.table_index = new_index;
113: 			return;
114: 		}
115: 		case LogicalOperatorType::LOGICAL_TOP_N: {
116: 			// visit the expressions of the operator and continue into the child node
117: 			auto &top_n = op.Cast<LogicalTopN>();
118: 			for (auto &order : top_n.orders) {
119: 				ReplaceTableReferences(*order.expression, new_index);
120: 			}
121: 			current_op = *op.children[0];
122: 			break;
123: 		}
124: 		case LogicalOperatorType::LOGICAL_FILTER:
125: 		case LogicalOperatorType::LOGICAL_SAMPLE:
126: 		case LogicalOperatorType::LOGICAL_LIMIT: {
127: 			// visit the expressions of the operator and continue into the child node
128: 			for (auto &expr : op.expressions) {
129: 				ReplaceTableReferences(*expr, new_index);
130: 			}
131: 			current_op = *op.children[0];
132: 			break;
133: 		}
134: 		default:
135: 			throw InternalException("Unsupported operator type in LateMaterialization::ReplaceTopLevelTableIndex");
136: 		}
137: 	}
138: }
139: 
140: void LateMaterialization::ReplaceTableReferences(Expression &expr, idx_t new_table_index) {
141: 	if (expr.GetExpressionType() == ExpressionType::BOUND_COLUMN_REF) {
142: 		auto &bound_column_ref = expr.Cast<BoundColumnRefExpression>();
143: 		bound_column_ref.binding.table_index = new_table_index;
144: 	}
145: 
146: 	ExpressionIterator::EnumerateChildren(expr,
147: 	                                      [&](Expression &child) { ReplaceTableReferences(child, new_table_index); });
148: }
149: 
150: unique_ptr<Expression> LateMaterialization::GetExpression(LogicalOperator &op, idx_t column_index) {
151: 	switch (op.type) {
152: 	case LogicalOperatorType::LOGICAL_GET: {
153: 		auto &get = op.Cast<LogicalGet>();
154: 		auto &column_id = get.GetColumnIds()[column_index];
155: 		auto column_name = get.GetColumnName(column_id);
156: 		auto &column_type = get.GetColumnType(column_id);
157: 		auto expr =
158: 		    make_uniq<BoundColumnRefExpression>(column_name, column_type, ColumnBinding(get.table_index, column_index));
159: 		return std::move(expr);
160: 	}
161: 	case LogicalOperatorType::LOGICAL_PROJECTION:
162: 		return op.expressions[column_index]->Copy();
163: 	default:
164: 		throw InternalException("Unsupported operator type for LateMaterialization::GetExpression");
165: 	}
166: }
167: 
168: void LateMaterialization::ReplaceExpressionReferences(LogicalOperator &next_op, unique_ptr<Expression> &expr) {
169: 	if (expr->GetExpressionType() == ExpressionType::BOUND_COLUMN_REF) {
170: 		auto &bound_column_ref = expr->Cast<BoundColumnRefExpression>();
171: 		expr = GetExpression(next_op, bound_column_ref.binding.column_index);
172: 		return;
173: 	}
174: 
175: 	ExpressionIterator::EnumerateChildren(
176: 	    *expr, [&](unique_ptr<Expression> &child) { ReplaceExpressionReferences(next_op, child); });
177: }
178: 
179: bool LateMaterialization::TryLateMaterialization(unique_ptr<LogicalOperator> &op) {
180: 	// check if we can benefit from late materialization
181: 	// we need to see how many columns we require in the pipeline versus how many columns we emit in the scan
182: 	// for example, in a query like SELECT * FROM tbl ORDER BY ts LIMIT 5, the top-n only needs the "ts" column
183: 	// the other columns can be fetched later on using late materialization
184: 	// we can only push late materialization through a subset of operators
185: 	// and we can only do it for scans that support the row-id pushdown (currently only DuckDB table scans)
186: 
187: 	// visit the expressions for each operator in the chain
188: 	vector<reference<LogicalOperator>> source_operators;
189: 
190: 	VisitOperatorExpressions(*op);
191: 	reference<LogicalOperator> child = *op->children[0];
192: 	while (child.get().type != LogicalOperatorType::LOGICAL_GET) {
193: 		switch (child.get().type) {
194: 		case LogicalOperatorType::LOGICAL_PROJECTION: {
195: 			// recurse into the child node - but ONLY visit expressions that are referenced
196: 			auto &proj = child.get().Cast<LogicalProjection>();
197: 			source_operators.push_back(child);
198: 
199: 			for (auto &expr : proj.expressions) {
200: 				if (expr->IsVolatile()) {
201: 					// we cannot do this optimization if any of the columns are volatile
202: 					return false;
203: 				}
204: 			}
205: 
206: 			// figure out which projection expressions we are currently referencing
207: 			set<idx_t> referenced_columns;
208: 			for (auto &entry : column_references) {
209: 				auto &column_binding = entry.first;
210: 				if (column_binding.table_index == proj.table_index) {
211: 					referenced_columns.insert(column_binding.column_index);
212: 				}
213: 			}
214: 			// clear the list of referenced expressions and visit those columns
215: 			column_references.clear();
216: 			for (auto &col_idx : referenced_columns) {
217: 				VisitExpression(&proj.expressions[col_idx]);
218: 			}
219: 			// continue into child
220: 			child = *child.get().children[0];
221: 			break;
222: 		}
223: 		case LogicalOperatorType::LOGICAL_FILTER: {
224: 			// visit filter expressions - we need these columns
225: 			VisitOperatorExpressions(child.get());
226: 			// continue into child
227: 			child = *child.get().children[0];
228: 			break;
229: 		}
230: 		default:
231: 			// unsupported operator for late materialization
232: 			return false;
233: 		}
234: 	}
235: 	auto &get = child.get().Cast<LogicalGet>();
236: 	if (column_references.size() >= get.GetColumnIds().size()) {
237: 		// we do not benefit from late materialization
238: 		// we need all of the columns to compute the root node anyway (Top-N/Limit/etc)
239: 		return false;
240: 	}
241: 	if (!get.function.late_materialization) {
242: 		// this function does not support late materialization
243: 		return false;
244: 	}
245: 	auto entry = get.virtual_columns.find(COLUMN_IDENTIFIER_ROW_ID);
246: 	if (entry == get.virtual_columns.end()) {
247: 		throw InternalException("Table function supports late materialization but does not expose a rowid column");
248: 	}
249: 	row_id_type = entry->second.type;
250: 	// we benefit from late materialization
251: 	// we need to transform this plan into a semi-join with the row-id
252: 	// we need to ensure the operator returns exactly the same column bindings as before
253: 
254: 	// construct the LHS from the LogicalGet
255: 	auto lhs = ConstructLHS(get);
256: 	// insert the row-id column on the left hand side
257: 	auto &lhs_get = *lhs;
258: 	auto lhs_index = lhs_get.table_index;
259: 	auto lhs_columns = lhs_get.GetColumnIds().size();
260: 	auto lhs_row_idx = GetOrInsertRowId(lhs_get);
261: 	ColumnBinding lhs_binding(lhs_index, lhs_row_idx);
262: 
263: 	// after constructing the LHS but before constructing the RHS we construct the final projections/orders
264: 	// - we do this before constructing the RHS because that alter the original plan
265: 	vector<unique_ptr<Expression>> final_proj_list;
266: 	// construct the final projection list from either (1) the root projection, or (2) the logical get
267: 	if (!source_operators.empty()) {
268: 		// construct the columns from the root projection
269: 		auto &root_proj = source_operators[0].get();
270: 		for (auto &expr : root_proj.expressions) {
271: 			final_proj_list.push_back(expr->Copy());
272: 		}
273: 		// now we need to "flatten" the projection list by traversing the set of projections and inlining them
274: 		for (idx_t i = 0; i < source_operators.size(); i++) {
275: 			auto &next_operator = i + 1 < source_operators.size() ? source_operators[i + 1].get() : lhs_get;
276: 			for (auto &expr : final_proj_list) {
277: 				ReplaceExpressionReferences(next_operator, expr);
278: 			}
279: 		}
280: 	} else {
281: 		// if we have no projection directly construct the columns from the root get
282: 		for (idx_t i = 0; i < lhs_columns; i++) {
283: 			final_proj_list.push_back(GetExpression(lhs_get, i));
284: 		}
285: 	}
286: 
287: 	// we need to re-order again at the end
288: 	vector<BoundOrderByNode> final_orders;
289: 	auto root_type = op->type;
290: 	if (root_type == LogicalOperatorType::LOGICAL_TOP_N) {
291: 		// for top-n we need to re-order by the top-n conditions
292: 		auto &top_n = op->Cast<LogicalTopN>();
293: 		for (auto &order : top_n.orders) {
294: 			auto expr = order.expression->Copy();
295: 			final_orders.emplace_back(order.type, order.null_order, std::move(expr));
296: 		}
297: 	} else {
298: 		// for limit/sample we order by row-id
299: 		auto row_id_expr = make_uniq<BoundColumnRefExpression>("rowid", row_id_type, lhs_binding);
300: 		final_orders.emplace_back(OrderType::ASCENDING, OrderByNullType::NULLS_LAST, std::move(row_id_expr));
301: 	}
302: 
303: 	// construct the RHS for the join
304: 	// this is essentially the old pipeline, but with the `rowid` column added
305: 	// note that the purpose of this optimization is to remove columns from the RHS
306: 	// we don't do that here yet though - we do this in a later step using the RemoveUnusedColumns optimizer
307: 	auto rhs_binding = ConstructRHS(op);
308: 
309: 	// the final table index emitted must be the table index of the original operator
310: 	// this ensures any upstream operators that refer to the original get will keep on referring to the correct columns
311: 	auto final_index = rhs_binding.table_index;
312: 
313: 	// we need to replace any references to "rhs_binding.table_index" in the rhs to a new table index
314: 	rhs_binding.table_index = optimizer.binder.GenerateTableIndex();
315: 	ReplaceTopLevelTableIndex(*op, rhs_binding.table_index);
316: 
317: 	// construct a semi join between the lhs and rhs
318: 	auto join = make_uniq<LogicalComparisonJoin>(JoinType::SEMI);
319: 	join->children.push_back(std::move(lhs));
320: 	join->children.push_back(std::move(op));
321: 	JoinCondition condition;
322: 	condition.comparison = ExpressionType::COMPARE_EQUAL;
323: 	condition.left = make_uniq<BoundColumnRefExpression>("rowid", row_id_type, lhs_binding);
324: 	condition.right = make_uniq<BoundColumnRefExpression>("rowid", row_id_type, rhs_binding);
325: 	join->conditions.push_back(std::move(condition));
326: 
327: 	// push a projection that removes the row id again from the lhs
328: 	// this is the final projection - so it should have the final table index
329: 	auto proj_index = final_index;
330: 	if (root_type == LogicalOperatorType::LOGICAL_TOP_N) {
331: 		// for top-n we need to order on expressions, so we need to order AFTER the final projection
332: 		auto proj = make_uniq<LogicalProjection>(proj_index, std::move(final_proj_list));
333: 		proj->children.push_back(std::move(join));
334: 
335: 		for (auto &order : final_orders) {
336: 			ReplaceTableReferences(*order.expression, proj_index);
337: 		}
338: 		auto order = make_uniq<LogicalOrder>(std::move(final_orders));
339: 		order->children.push_back(std::move(proj));
340: 
341: 		op = std::move(order);
342: 	} else {
343: 		// for limit/sample we order on row-id, so we need to order BEFORE the final projection
344: 		// because the final projection removes row-ids
345: 		auto order = make_uniq<LogicalOrder>(std::move(final_orders));
346: 		order->children.push_back(std::move(join));
347: 
348: 		auto proj = make_uniq<LogicalProjection>(proj_index, std::move(final_proj_list));
349: 		proj->children.push_back(std::move(order));
350: 
351: 		op = std::move(proj);
352: 	}
353: 
354: 	// run the RemoveUnusedColumns optimizer to prune the (now) unused columns the plan
355: 	RemoveUnusedColumns unused_optimizer(optimizer.binder, optimizer.context, true);
356: 	unused_optimizer.VisitOperator(*op);
357: 	return true;
358: }
359: 
360: bool LateMaterialization::OptimizeLargeLimit(LogicalOperator &child) {
361: 	// we only support large limits if the only
362: 	reference<LogicalOperator> current_op = child;
363: 	while (current_op.get().type != LogicalOperatorType::LOGICAL_GET) {
364: 		if (current_op.get().type != LogicalOperatorType::LOGICAL_PROJECTION) {
365: 			return false;
366: 		}
367: 		current_op = *current_op.get().children[0];
368: 	}
369: 	return true;
370: }
371: 
372: unique_ptr<LogicalOperator> LateMaterialization::Optimize(unique_ptr<LogicalOperator> op) {
373: 	switch (op->type) {
374: 	case LogicalOperatorType::LOGICAL_LIMIT: {
375: 		auto &limit = op->Cast<LogicalLimit>();
376: 		if (limit.limit_val.Type() != LimitNodeType::CONSTANT_VALUE) {
377: 			break;
378: 		}
379: 		if (limit.limit_val.GetConstantValue() > max_row_count) {
380: 			// for large limits - we may still want to do this optimization if the limit is consecutive
381: 			// this is the case if there are only projections/get below the limit
382: 			// if the row-ids are not consecutive doing the join can worsen performance
383: 			if (!OptimizeLargeLimit(*limit.children[0])) {
384: 				break;
385: 			}
386: 		}
387: 		if (TryLateMaterialization(op)) {
388: 			return op;
389: 		}
390: 		break;
391: 	}
392: 	case LogicalOperatorType::LOGICAL_TOP_N: {
393: 		auto &top_n = op->Cast<LogicalTopN>();
394: 		if (top_n.limit > max_row_count) {
395: 			break;
396: 		}
397: 		// for the top-n we need to visit the order elements
398: 		if (TryLateMaterialization(op)) {
399: 			return op;
400: 		}
401: 		break;
402: 	}
403: 	case LogicalOperatorType::LOGICAL_SAMPLE: {
404: 		auto &sample = op->Cast<LogicalSample>();
405: 		if (sample.sample_options->is_percentage) {
406: 			break;
407: 		}
408: 		if (sample.sample_options->sample_size.GetValue<uint64_t>() > max_row_count) {
409: 			break;
410: 		}
411: 		if (TryLateMaterialization(op)) {
412: 			return op;
413: 		}
414: 		break;
415: 	}
416: 	default:
417: 		break;
418: 	}
419: 	for (auto &child : op->children) {
420: 		child = Optimize(std::move(child));
421: 	}
422: 	return op;
423: }
424: 
425: } // namespace duckdb
[end of src/optimizer/late_materialization.cpp]
[start of src/storage/compression/string_uncompressed.cpp]
1: #include "duckdb/storage/string_uncompressed.hpp"
2: 
3: #include "duckdb/common/pair.hpp"
4: #include "duckdb/common/serializer/deserializer.hpp"
5: #include "duckdb/common/serializer/serializer.hpp"
6: #include "duckdb/storage/checkpoint/write_overflow_strings_to_disk.hpp"
7: #include "duckdb/storage/table/column_data.hpp"
8: 
9: namespace duckdb {
10: 
11: //===--------------------------------------------------------------------===//
12: // Storage Class
13: //===--------------------------------------------------------------------===//
14: UncompressedStringSegmentState::~UncompressedStringSegmentState() {
15: 	while (head) {
16: 		// prevent deep recursion here
17: 		head = std::move(head->next);
18: 	}
19: }
20: 
21: //===--------------------------------------------------------------------===//
22: // Analyze
23: //===--------------------------------------------------------------------===//
24: struct StringAnalyzeState : public AnalyzeState {
25: 	explicit StringAnalyzeState(const CompressionInfo &info)
26: 	    : AnalyzeState(info), count(0), total_string_size(0), overflow_strings(0) {
27: 	}
28: 
29: 	idx_t count;
30: 	idx_t total_string_size;
31: 	idx_t overflow_strings;
32: };
33: 
34: unique_ptr<AnalyzeState> UncompressedStringStorage::StringInitAnalyze(ColumnData &col_data, PhysicalType type) {
35: 	CompressionInfo info(col_data.GetBlockManager().GetBlockSize());
36: 	return make_uniq<StringAnalyzeState>(info);
37: }
38: 
39: bool UncompressedStringStorage::StringAnalyze(AnalyzeState &state_p, Vector &input, idx_t count) {
40: 	auto &state = state_p.Cast<StringAnalyzeState>();
41: 	UnifiedVectorFormat vdata;
42: 	input.ToUnifiedFormat(count, vdata);
43: 
44: 	state.count += count;
45: 	auto data = UnifiedVectorFormat::GetData<string_t>(vdata);
46: 	for (idx_t i = 0; i < count; i++) {
47: 		auto idx = vdata.sel->get_index(i);
48: 		if (vdata.validity.RowIsValid(idx)) {
49: 			auto string_size = data[idx].GetSize();
50: 			state.total_string_size += string_size;
51: 			if (string_size >= StringUncompressed::GetStringBlockLimit(state.info.GetBlockSize())) {
52: 				state.overflow_strings++;
53: 			}
54: 		}
55: 	}
56: 	return true;
57: }
58: 
59: idx_t UncompressedStringStorage::StringFinalAnalyze(AnalyzeState &state_p) {
60: 	auto &state = state_p.Cast<StringAnalyzeState>();
61: 	return state.count * sizeof(int32_t) + state.total_string_size + state.overflow_strings * BIG_STRING_MARKER_SIZE;
62: }
63: 
64: //===--------------------------------------------------------------------===//
65: // Scan
66: //===--------------------------------------------------------------------===//
67: void UncompressedStringInitPrefetch(ColumnSegment &segment, PrefetchState &prefetch_state) {
68: 	prefetch_state.AddBlock(segment.block);
69: 	auto segment_state = segment.GetSegmentState();
70: 	if (segment_state) {
71: 		auto &state = segment_state->Cast<UncompressedStringSegmentState>();
72: 		auto &block_manager = segment.GetBlockManager();
73: 		for (auto &block_id : state.on_disk_blocks) {
74: 			auto block_handle = state.GetHandle(block_manager, block_id);
75: 			prefetch_state.AddBlock(block_handle);
76: 		}
77: 	}
78: }
79: 
80: unique_ptr<SegmentScanState> UncompressedStringStorage::StringInitScan(ColumnSegment &segment) {
81: 	auto result = make_uniq<StringScanState>();
82: 	auto &buffer_manager = BufferManager::GetBufferManager(segment.db);
83: 	result->handle = buffer_manager.Pin(segment.block);
84: 	return std::move(result);
85: }
86: 
87: //===--------------------------------------------------------------------===//
88: // Scan base data
89: //===--------------------------------------------------------------------===//
90: void UncompressedStringStorage::StringScanPartial(ColumnSegment &segment, ColumnScanState &state, idx_t scan_count,
91:                                                   Vector &result, idx_t result_offset) {
92: 	// clear any previously locked buffers and get the primary buffer handle
93: 	auto &scan_state = state.scan_state->Cast<StringScanState>();
94: 	auto start = segment.GetRelativeIndex(state.row_index);
95: 
96: 	auto baseptr = scan_state.handle.Ptr() + segment.GetBlockOffset();
97: 	auto dict_end = GetDictionaryEnd(segment, scan_state.handle);
98: 	auto base_data = reinterpret_cast<int32_t *>(baseptr + DICTIONARY_HEADER_SIZE);
99: 	auto result_data = FlatVector::GetData<string_t>(result);
100: 
101: 	int32_t previous_offset = start > 0 ? base_data[start - 1] : 0;
102: 
103: 	for (idx_t i = 0; i < scan_count; i++) {
104: 		// std::abs used since offsets can be negative to indicate big strings
105: 		auto current_offset = base_data[start + i];
106: 		auto string_length = UnsafeNumericCast<uint32_t>(std::abs(current_offset) - std::abs(previous_offset));
107: 		result_data[result_offset + i] =
108: 		    FetchStringFromDict(segment, dict_end, result, baseptr, current_offset, string_length);
109: 		previous_offset = base_data[start + i];
110: 	}
111: }
112: 
113: void UncompressedStringStorage::StringScan(ColumnSegment &segment, ColumnScanState &state, idx_t scan_count,
114:                                            Vector &result) {
115: 	StringScanPartial(segment, state, scan_count, result, 0);
116: }
117: 
118: //===--------------------------------------------------------------------===//
119: // Select
120: //===--------------------------------------------------------------------===//
121: void UncompressedStringStorage::Select(ColumnSegment &segment, ColumnScanState &state, idx_t vector_count,
122:                                        Vector &result, const SelectionVector &sel, idx_t sel_count) {
123: 	// clear any previously locked buffers and get the primary buffer handle
124: 	auto &scan_state = state.scan_state->Cast<StringScanState>();
125: 	auto start = segment.GetRelativeIndex(state.row_index);
126: 
127: 	auto baseptr = scan_state.handle.Ptr() + segment.GetBlockOffset();
128: 	auto dict_end = GetDictionaryEnd(segment, scan_state.handle);
129: 	auto base_data = reinterpret_cast<int32_t *>(baseptr + DICTIONARY_HEADER_SIZE);
130: 	auto result_data = FlatVector::GetData<string_t>(result);
131: 
132: 	for (idx_t i = 0; i < sel_count; i++) {
133: 		idx_t index = start + sel.get_index(i);
134: 		auto current_offset = base_data[index];
135: 		auto prev_offset = index > 0 ? base_data[index - 1] : 0;
136: 		auto string_length = UnsafeNumericCast<uint32_t>(std::abs(current_offset) - std::abs(prev_offset));
137: 		result_data[i] = FetchStringFromDict(segment, dict_end, result, baseptr, current_offset, string_length);
138: 	}
139: }
140: 
141: //===--------------------------------------------------------------------===//
142: // Fetch
143: //===--------------------------------------------------------------------===//
144: BufferHandle &ColumnFetchState::GetOrInsertHandle(ColumnSegment &segment) {
145: 	auto primary_id = segment.block->BlockId();
146: 
147: 	auto entry = handles.find(primary_id);
148: 	if (entry == handles.end()) {
149: 		// not pinned yet: pin it
150: 		auto &buffer_manager = BufferManager::GetBufferManager(segment.db);
151: 		auto handle = buffer_manager.Pin(segment.block);
152: 		auto pinned_entry = handles.insert(make_pair(primary_id, std::move(handle)));
153: 		return pinned_entry.first->second;
154: 	} else {
155: 		// already pinned: use the pinned handle
156: 		return entry->second;
157: 	}
158: }
159: 
160: void UncompressedStringStorage::StringFetchRow(ColumnSegment &segment, ColumnFetchState &state, row_t row_id,
161:                                                Vector &result, idx_t result_idx) {
162: 	// fetch a single row from the string segment
163: 	// first pin the main buffer if it is not already pinned
164: 	auto &handle = state.GetOrInsertHandle(segment);
165: 
166: 	auto baseptr = handle.Ptr() + segment.GetBlockOffset();
167: 	auto dict_end = GetDictionaryEnd(segment, handle);
168: 	auto base_data = reinterpret_cast<int32_t *>(baseptr + DICTIONARY_HEADER_SIZE);
169: 	auto result_data = FlatVector::GetData<string_t>(result);
170: 
171: 	auto dict_offset = base_data[row_id];
172: 	uint32_t string_length;
173: 	if (DUCKDB_UNLIKELY(row_id == 0LL)) {
174: 		// edge case where this is the first string in the dict
175: 		string_length = NumericCast<uint32_t>(std::abs(dict_offset));
176: 	} else {
177: 		string_length = NumericCast<uint32_t>(std::abs(dict_offset) - std::abs(base_data[row_id - 1]));
178: 	}
179: 	result_data[result_idx] = FetchStringFromDict(segment, dict_end, result, baseptr, dict_offset, string_length);
180: }
181: 
182: //===--------------------------------------------------------------------===//
183: // Append
184: //===--------------------------------------------------------------------===//
185: SerializedStringSegmentState::SerializedStringSegmentState() {
186: }
187: 
188: SerializedStringSegmentState::SerializedStringSegmentState(vector<block_id_t> blocks_p) {
189: 	blocks = std::move(blocks_p);
190: }
191: 
192: void SerializedStringSegmentState::Serialize(Serializer &serializer) const {
193: 	serializer.WriteProperty(1, "overflow_blocks", blocks);
194: }
195: 
196: unique_ptr<CompressedSegmentState>
197: UncompressedStringStorage::StringInitSegment(ColumnSegment &segment, block_id_t block_id,
198:                                              optional_ptr<ColumnSegmentState> segment_state) {
199: 	auto &buffer_manager = BufferManager::GetBufferManager(segment.db);
200: 	if (block_id == INVALID_BLOCK) {
201: 		auto handle = buffer_manager.Pin(segment.block);
202: 		StringDictionaryContainer dictionary;
203: 		dictionary.size = 0;
204: 		dictionary.end = UnsafeNumericCast<uint32_t>(segment.SegmentSize());
205: 		SetDictionary(segment, handle, dictionary);
206: 	}
207: 	auto result = make_uniq<UncompressedStringSegmentState>();
208: 	if (segment_state) {
209: 		auto &serialized_state = segment_state->Cast<SerializedStringSegmentState>();
210: 		result->on_disk_blocks = std::move(serialized_state.blocks);
211: 	}
212: 	return std::move(result);
213: }
214: 
215: idx_t UncompressedStringStorage::FinalizeAppend(ColumnSegment &segment, SegmentStatistics &) {
216: 	auto &buffer_manager = BufferManager::GetBufferManager(segment.db);
217: 	auto handle = buffer_manager.Pin(segment.block);
218: 	auto dict = GetDictionary(segment, handle);
219: 	D_ASSERT(dict.end == segment.SegmentSize());
220: 	// compute the total size required to store this segment
221: 	auto offset_size = DICTIONARY_HEADER_SIZE + segment.count * sizeof(int32_t);
222: 	auto total_size = offset_size + dict.size;
223: 
224: 	CompressionInfo info(segment.GetBlockManager().GetBlockSize());
225: 	if (total_size >= info.GetCompactionFlushLimit()) {
226: 		// the block is full enough, don't bother moving around the dictionary
227: 		return segment.SegmentSize();
228: 	}
229: 
230: 	// the block has space left: figure out how much space we can save
231: 	auto move_amount = segment.SegmentSize() - total_size;
232: 	// move the dictionary so it lines up exactly with the offsets
233: 	auto dataptr = handle.Ptr();
234: 	memmove(dataptr + offset_size, dataptr + dict.end - dict.size, dict.size);
235: 	dict.end -= move_amount;
236: 	D_ASSERT(dict.end == total_size);
237: 	// write the new dictionary (with the updated "end")
238: 	SetDictionary(segment, handle, dict);
239: 	return total_size;
240: }
241: 
242: //===--------------------------------------------------------------------===//
243: // Serialization & Cleanup
244: //===--------------------------------------------------------------------===//
245: unique_ptr<ColumnSegmentState> UncompressedStringStorage::SerializeState(ColumnSegment &segment) {
246: 	auto &state = segment.GetSegmentState()->Cast<UncompressedStringSegmentState>();
247: 	if (state.on_disk_blocks.empty()) {
248: 		// no on-disk blocks - nothing to write
249: 		return nullptr;
250: 	}
251: 	return make_uniq<SerializedStringSegmentState>(state.on_disk_blocks);
252: }
253: 
254: unique_ptr<ColumnSegmentState> UncompressedStringStorage::DeserializeState(Deserializer &deserializer) {
255: 	auto result = make_uniq<SerializedStringSegmentState>();
256: 	deserializer.ReadProperty(1, "overflow_blocks", result->blocks);
257: 	return std::move(result);
258: }
259: 
260: void UncompressedStringStorage::CleanupState(ColumnSegment &segment) {
261: 	auto &state = segment.GetSegmentState()->Cast<UncompressedStringSegmentState>();
262: 	auto &block_manager = segment.GetBlockManager();
263: 	for (auto &block_id : state.on_disk_blocks) {
264: 		block_manager.MarkBlockAsModified(block_id);
265: 	}
266: }
267: 
268: //===--------------------------------------------------------------------===//
269: // Get Function
270: //===--------------------------------------------------------------------===//
271: CompressionFunction StringUncompressed::GetFunction(PhysicalType data_type) {
272: 	D_ASSERT(data_type == PhysicalType::VARCHAR);
273: 	return CompressionFunction(
274: 	    CompressionType::COMPRESSION_UNCOMPRESSED, data_type, UncompressedStringStorage::StringInitAnalyze,
275: 	    UncompressedStringStorage::StringAnalyze, UncompressedStringStorage::StringFinalAnalyze,
276: 	    UncompressedFunctions::InitCompression, UncompressedFunctions::Compress,
277: 	    UncompressedFunctions::FinalizeCompress, UncompressedStringStorage::StringInitScan,
278: 	    UncompressedStringStorage::StringScan, UncompressedStringStorage::StringScanPartial,
279: 	    UncompressedStringStorage::StringFetchRow, UncompressedFunctions::EmptySkip,
280: 	    UncompressedStringStorage::StringInitSegment, UncompressedStringStorage::StringInitAppend,
281: 	    UncompressedStringStorage::StringAppend, UncompressedStringStorage::FinalizeAppend, nullptr,
282: 	    UncompressedStringStorage::SerializeState, UncompressedStringStorage::DeserializeState,
283: 	    UncompressedStringStorage::CleanupState, UncompressedStringInitPrefetch, UncompressedStringStorage::Select);
284: }
285: 
286: //===--------------------------------------------------------------------===//
287: // Helper Functions
288: //===--------------------------------------------------------------------===//
289: void UncompressedStringStorage::SetDictionary(ColumnSegment &segment, BufferHandle &handle,
290:                                               StringDictionaryContainer container) {
291: 	auto startptr = handle.Ptr() + segment.GetBlockOffset();
292: 	Store<uint32_t>(container.size, startptr);
293: 	Store<uint32_t>(container.end, startptr + sizeof(uint32_t));
294: }
295: 
296: StringDictionaryContainer UncompressedStringStorage::GetDictionary(ColumnSegment &segment, BufferHandle &handle) {
297: 	auto startptr = handle.Ptr() + segment.GetBlockOffset();
298: 	StringDictionaryContainer container;
299: 	container.size = Load<uint32_t>(startptr);
300: 	container.end = Load<uint32_t>(startptr + sizeof(uint32_t));
301: 	return container;
302: }
303: 
304: uint32_t UncompressedStringStorage::GetDictionaryEnd(ColumnSegment &segment, BufferHandle &handle) {
305: 	auto startptr = handle.Ptr() + segment.GetBlockOffset();
306: 	return Load<uint32_t>(startptr + sizeof(uint32_t));
307: }
308: 
309: idx_t UncompressedStringStorage::RemainingSpace(ColumnSegment &segment, BufferHandle &handle) {
310: 	auto dictionary = GetDictionary(segment, handle);
311: 	D_ASSERT(dictionary.end == segment.SegmentSize());
312: 	idx_t used_space = dictionary.size + segment.count * sizeof(int32_t) + DICTIONARY_HEADER_SIZE;
313: 	D_ASSERT(segment.SegmentSize() >= used_space);
314: 	return segment.SegmentSize() - used_space;
315: }
316: 
317: void UncompressedStringStorage::WriteString(ColumnSegment &segment, string_t string, block_id_t &result_block,
318:                                             int32_t &result_offset) {
319: 	auto &state = segment.GetSegmentState()->Cast<UncompressedStringSegmentState>();
320: 	if (state.overflow_writer) {
321: 		// overflow writer is set: write string there
322: 		state.overflow_writer->WriteString(state, string, result_block, result_offset);
323: 	} else {
324: 		// default overflow behavior: use in-memory buffer to store the overflow string
325: 		WriteStringMemory(segment, string, result_block, result_offset);
326: 	}
327: }
328: 
329: void UncompressedStringStorage::WriteStringMemory(ColumnSegment &segment, string_t string, block_id_t &result_block,
330:                                                   int32_t &result_offset) {
331: 	auto total_length = UnsafeNumericCast<uint32_t>(string.GetSize() + sizeof(uint32_t));
332: 	shared_ptr<BlockHandle> block;
333: 	BufferHandle handle;
334: 
335: 	auto &buffer_manager = BufferManager::GetBufferManager(segment.db);
336: 	auto &state = segment.GetSegmentState()->Cast<UncompressedStringSegmentState>();
337: 	// check if the string fits in the current block
338: 	if (!state.head || state.head->offset + total_length >= state.head->size) {
339: 		// string does not fit, allocate space for it
340: 		// create a new string block
341: 		auto alloc_size = MaxValue<idx_t>(total_length, segment.GetBlockManager().GetBlockSize());
342: 		auto new_block = make_uniq<StringBlock>();
343: 		new_block->offset = 0;
344: 		new_block->size = alloc_size;
345: 		// allocate an in-memory buffer for it
346: 		handle = buffer_manager.Allocate(MemoryTag::OVERFLOW_STRINGS, alloc_size, false);
347: 		block = handle.GetBlockHandle();
348: 		state.overflow_blocks.insert(make_pair(block->BlockId(), reference<StringBlock>(*new_block)));
349: 		new_block->block = std::move(block);
350: 		new_block->next = std::move(state.head);
351: 		state.head = std::move(new_block);
352: 	} else {
353: 		// string fits, copy it into the current block
354: 		handle = buffer_manager.Pin(state.head->block);
355: 	}
356: 
357: 	result_block = state.head->block->BlockId();
358: 	result_offset = UnsafeNumericCast<int32_t>(state.head->offset);
359: 
360: 	// copy the string and the length there
361: 	auto ptr = handle.Ptr() + state.head->offset;
362: 	Store<uint32_t>(UnsafeNumericCast<uint32_t>(string.GetSize()), ptr);
363: 	ptr += sizeof(uint32_t);
364: 	memcpy(ptr, string.GetData(), string.GetSize());
365: 	state.head->offset += total_length;
366: }
367: 
368: string_t UncompressedStringStorage::ReadOverflowString(ColumnSegment &segment, Vector &result, block_id_t block,
369:                                                        int32_t offset) {
370: 	auto &block_manager = segment.GetBlockManager();
371: 	auto &buffer_manager = block_manager.buffer_manager;
372: 	auto &state = segment.GetSegmentState()->Cast<UncompressedStringSegmentState>();
373: 
374: 	D_ASSERT(block != INVALID_BLOCK);
375: 	D_ASSERT(offset < NumericCast<int32_t>(block_manager.GetBlockSize()));
376: 
377: 	if (block < MAXIMUM_BLOCK) {
378: 		// read the overflow string from disk
379: 		// pin the initial handle and read the length
380: 		auto block_handle = state.GetHandle(block_manager, block);
381: 		auto handle = buffer_manager.Pin(block_handle);
382: 
383: 		// read header
384: 		uint32_t length = Load<uint32_t>(handle.Ptr() + offset);
385: 		uint32_t remaining = length;
386: 		offset += sizeof(uint32_t);
387: 
388: 		// allocate a buffer to store the string
389: 		auto alloc_size = MaxValue<idx_t>(block_manager.GetBlockSize(), length);
390: 		// allocate a buffer to store the compressed string
391: 		// TODO: profile this to check if we need to reuse buffer
392: 		auto target_handle = buffer_manager.Allocate(MemoryTag::OVERFLOW_STRINGS, alloc_size);
393: 		auto target_ptr = target_handle.Ptr();
394: 
395: 		// now append the string to the single buffer
396: 		while (remaining > 0) {
397: 			idx_t to_write = MinValue<idx_t>(remaining, block_manager.GetBlockSize() - sizeof(block_id_t) -
398: 			                                                UnsafeNumericCast<idx_t>(offset));
399: 			memcpy(target_ptr, handle.Ptr() + offset, to_write);
400: 			remaining -= to_write;
401: 			offset += UnsafeNumericCast<int32_t>(to_write);
402: 			target_ptr += to_write;
403: 			if (remaining > 0) {
404: 				// read the next block
405: 				block_id_t next_block = Load<block_id_t>(handle.Ptr() + offset);
406: 				block_handle = state.GetHandle(block_manager, next_block);
407: 				handle = buffer_manager.Pin(block_handle);
408: 				offset = 0;
409: 			}
410: 		}
411: 
412: 		auto final_buffer = target_handle.Ptr();
413: 		StringVector::AddHandle(result, std::move(target_handle));
414: 		return ReadString(final_buffer, 0, length);
415: 	}
416: 
417: 	// read the overflow string from memory
418: 	// first pin the handle, if it is not pinned yet
419: 	auto entry = state.overflow_blocks.find(block);
420: 	D_ASSERT(entry != state.overflow_blocks.end());
421: 	auto handle = buffer_manager.Pin(entry->second.get().block);
422: 	auto final_buffer = handle.Ptr();
423: 	StringVector::AddHandle(result, std::move(handle));
424: 	return ReadStringWithLength(final_buffer, offset);
425: }
426: 
427: string_t UncompressedStringStorage::ReadString(data_ptr_t target, int32_t offset, uint32_t string_length) {
428: 	auto ptr = target + offset;
429: 	auto str_ptr = char_ptr_cast(ptr);
430: 	return string_t(str_ptr, string_length);
431: }
432: 
433: string_t UncompressedStringStorage::ReadStringWithLength(data_ptr_t target, int32_t offset) {
434: 	auto ptr = target + offset;
435: 	auto str_length = Load<uint32_t>(ptr);
436: 	auto str_ptr = char_ptr_cast(ptr + sizeof(uint32_t));
437: 	return string_t(str_ptr, str_length);
438: }
439: 
440: void UncompressedStringStorage::WriteStringMarker(data_ptr_t target, block_id_t block_id, int32_t offset) {
441: 	memcpy(target, &block_id, sizeof(block_id_t));
442: 	target += sizeof(block_id_t);
443: 	memcpy(target, &offset, sizeof(int32_t));
444: }
445: 
446: void UncompressedStringStorage::ReadStringMarker(data_ptr_t target, block_id_t &block_id, int32_t &offset) {
447: 	memcpy(&block_id, target, sizeof(block_id_t));
448: 	target += sizeof(block_id_t);
449: 	memcpy(&offset, target, sizeof(int32_t));
450: }
451: 
452: } // namespace duckdb
[end of src/storage/compression/string_uncompressed.cpp]
[start of tools/pythonpkg/duckdb-stubs/__init__.pyi]
1: # to regenerate this from scratch, run scripts/regenerate_python_stubs.sh .
2: # be warned - currently there are still tweaks needed after this file is
3: # generated. These should be annotated with a comment like
4: # # stubgen override
5: # to help the sanity of maintainers.
6: 
7: import duckdb.typing as typing
8: import duckdb.functional as functional
9: from duckdb.typing import DuckDBPyType
10: from duckdb.functional import FunctionNullHandling, PythonUDFType
11: from duckdb.value.constant import (
12:     Value,
13:     NullValue,
14:     BooleanValue,
15:     UnsignedBinaryValue,
16:     UnsignedShortValue,
17:     UnsignedIntegerValue,
18:     UnsignedLongValue,
19:     BinaryValue,
20:     ShortValue,
21:     IntegerValue,
22:     LongValue,
23:     HugeIntegerValue,
24:     FloatValue,
25:     DoubleValue,
26:     DecimalValue,
27:     StringValue,
28:     UUIDValue,
29:     BitValue,
30:     BlobValue,
31:     DateValue,
32:     IntervalValue,
33:     TimestampValue,
34:     TimestampSecondValue,
35:     TimestampMilisecondValue,
36:     TimestampNanosecondValue,
37:     TimestampTimeZoneValue,
38:     TimeValue,
39:     TimeTimeZoneValue,
40: )
41: 
42: # We also run this in python3.7, where this is needed
43: from typing_extensions import Literal
44: # stubgen override - missing import of Set
45: from typing import Any, ClassVar, Set, Optional, Callable
46: from io import StringIO, TextIOBase
47: from pathlib import Path
48: 
49: from typing import overload, Dict, List, Union, Tuple
50: import pandas
51: # stubgen override - unfortunately we need this for version checks
52: import sys
53: import fsspec
54: import pyarrow.lib
55: import polars
56: # stubgen override - This should probably not be exposed
57: apilevel: str
58: comment: token_type
59: identifier: token_type
60: keyword: token_type
61: numeric_const: token_type
62: operator: token_type
63: paramstyle: str
64: string_const: token_type
65: threadsafety: int
66: __standard_vector_size__: int
67: STANDARD: ExplainType
68: ANALYZE: ExplainType
69: DEFAULT: PythonExceptionHandling
70: RETURN_NULL: PythonExceptionHandling
71: ROWS: RenderMode
72: COLUMNS: RenderMode
73: 
74: __version__: str
75: 
76: __interactive__: bool
77: __jupyter__: bool
78: 
79: class BinderException(ProgrammingError): ...
80: 
81: class CatalogException(ProgrammingError): ...
82: 
83: class ConnectionException(OperationalError): ...
84: 
85: class ConstraintException(IntegrityError): ...
86: 
87: class ConversionException(DataError): ...
88: 
89: class DataError(Error): ...
90: 
91: class ExplainType:
92:     STANDARD: ExplainType
93:     ANALYZE: ExplainType
94:     def __int__(self) -> int: ...
95:     def __index__(self) -> int: ...
96:     @property
97:     def __members__(self) -> Dict[str, ExplainType]: ...
98:     @property
99:     def name(self) -> str: ...
100:     @property
101:     def value(self) -> int: ...
102: 
103: class RenderMode:
104:     ROWS: RenderMode
105:     COLUMNS: RenderMode
106:     def __int__(self) -> int: ...
107:     def __index__(self) -> int: ...
108:     @property
109:     def __members__(self) -> Dict[str, RenderMode]: ...
110:     @property
111:     def name(self) -> str: ...
112:     @property
113:     def value(self) -> int: ...
114: 
115: class PythonExceptionHandling:
116:     DEFAULT: PythonExceptionHandling
117:     RETURN_NULL: PythonExceptionHandling
118:     def __int__(self) -> int: ...
119:     def __index__(self) -> int: ...
120:     @property
121:     def __members__(self) -> Dict[str, PythonExceptionHandling]: ...
122:     @property
123:     def name(self) -> str: ...
124:     @property
125:     def value(self) -> int: ...
126: 
127: class CSVLineTerminator:
128:     LINE_FEED: CSVLineTerminator
129:     CARRIAGE_RETURN_LINE_FEED: CSVLineTerminator
130:     def __int__(self) -> int: ...
131:     def __index__(self) -> int: ...
132:     @property
133:     def __members__(self) -> Dict[str, CSVLineTerminator]: ...
134:     @property
135:     def name(self) -> str: ...
136:     @property
137:     def value(self) -> int: ...
138: 
139: class ExpectedResultType:
140:     QUERY_RESULT: ExpectedResultType
141:     CHANGED_ROWS: ExpectedResultType
142:     NOTHING: ExpectedResultType
143:     def __int__(self) -> int: ...
144:     def __index__(self) -> int: ...
145:     @property
146:     def __members__(self) -> Dict[str, ExpectedResultType]: ...
147:     @property
148:     def name(self) -> str: ...
149:     @property
150:     def value(self) -> int: ...
151: 
152: class StatementType:
153:     INVALID: StatementType
154:     SELECT: StatementType
155:     INSERT: StatementType
156:     UPDATE: StatementType
157:     CREATE: StatementType
158:     DELETE: StatementType
159:     PREPARE: StatementType
160:     EXECUTE: StatementType
161:     ALTER: StatementType
162:     TRANSACTION: StatementType
163:     COPY: StatementType
164:     ANALYZE: StatementType
165:     VARIABLE_SET: StatementType
166:     CREATE_FUNC: StatementType
167:     EXPLAIN: StatementType
168:     DROP: StatementType
169:     EXPORT: StatementType
170:     PRAGMA: StatementType
171:     VACUUM: StatementType
172:     CALL: StatementType
173:     SET: StatementType
174:     LOAD: StatementType
175:     RELATION: StatementType
176:     EXTENSION: StatementType
177:     LOGICAL_PLAN: StatementType
178:     ATTACH: StatementType
179:     DETACH: StatementType
180:     MULTI: StatementType
181:     COPY_DATABASE: StatementType
182:     def __int__(self) -> int: ...
183:     def __index__(self) -> int: ...
184:     @property
185:     def __members__(self) -> Dict[str, StatementType]: ...
186:     @property
187:     def name(self) -> str: ...
188:     @property
189:     def value(self) -> int: ...
190: 
191: class Statement:
192:     def __init__(self, *args, **kwargs) -> None: ...
193:     @property
194:     def query(self) -> str: ...
195:     @property
196:     def named_parameters(self) -> Set[str]: ...
197:     @property
198:     def expected_result_type(self) -> List[ExpectedResultType]: ...
199:     @property
200:     def type(self) -> StatementType: ...
201: 
202: class Expression:
203:     def __init__(self, *args, **kwargs) -> None: ...
204:     def __neg__(self) -> "Expression": ...
205: 
206:     def __add__(self, expr: "Expression") -> "Expression": ...
207:     def __radd__(self, expr: "Expression") -> "Expression": ...
208: 
209:     def __sub__(self, expr: "Expression") -> "Expression": ...
210:     def __rsub__(self, expr: "Expression") -> "Expression": ...
211: 
212:     def __mul__(self, expr: "Expression") -> "Expression": ...
213:     def __rmul__(self, expr: "Expression") -> "Expression": ...
214: 
215:     def __div__(self, expr: "Expression") -> "Expression": ...
216:     def __rdiv__(self, expr: "Expression") -> "Expression": ...
217: 
218:     def __truediv__(self, expr: "Expression") -> "Expression": ...
219:     def __rtruediv__(self, expr: "Expression") -> "Expression": ...
220: 
221:     def __floordiv__(self, expr: "Expression") -> "Expression": ...
222:     def __rfloordiv__(self, expr: "Expression") -> "Expression": ...
223: 
224:     def __mod__(self, expr: "Expression") -> "Expression": ...
225:     def __rmod__(self, expr: "Expression") -> "Expression": ...
226: 
227:     def __pow__(self, expr: "Expression") -> "Expression": ...
228:     def __rpow__(self, expr: "Expression") -> "Expression": ...
229: 
230:     def __and__(self, expr: "Expression") -> "Expression": ...
231:     def __rand__(self, expr: "Expression") -> "Expression": ...
232:     def __or__(self, expr: "Expression") -> "Expression": ...
233:     def __ror__(self, expr: "Expression") -> "Expression": ...
234:     def __invert__(self) -> "Expression": ...
235: 
236:     def __eq__(# type: ignore[override]
237:         self, expr: "Expression") -> "Expression": ...
238:     def __ne__(# type: ignore[override]
239:         self, expr: "Expression") -> "Expression": ...
240:     def __gt__(self, expr: "Expression") -> "Expression": ...
241:     def __ge__(self, expr: "Expression") -> "Expression": ...
242:     def __lt__(self, expr: "Expression") -> "Expression": ...
243:     def __le__(self, expr: "Expression") -> "Expression": ...
244: 
245:     def show(self, max_width: Optional[int] = None, max_rows: Optional[int] = None, max_col_width: Optional[int] = None, null_value: Optional[str] = None, render_mode: Optional[RenderMode] = None) -> None: ...
246:     def __repr__(self) -> str: ...
247:     def get_name(self) -> str: ...
248:     def alias(self, alias: str) -> "Expression": ...
249:     def when(self, condition: "Expression", value: "Expression") -> "Expression": ...
250:     def otherwise(self, value: "Expression") -> "Expression": ...
251:     def cast(self, type: DuckDBPyType) -> "Expression": ...
252:     def between(self, lower: "Expression", upper: "Expression") -> "Expression": ...
253:     def collate(self, collation: str) -> "Expression": ...
254:     def asc(self) -> "Expression": ...
255:     def desc(self) -> "Expression": ...
256:     def nulls_first(self) -> "Expression": ...
257:     def nulls_last(self) -> "Expression": ...
258:     def isnull(self) -> "Expression": ...
259:     def isnotnull(self) -> "Expression": ...
260:     def isin(self, *cols: "Expression") -> "Expression": ...
261:     def isnotin(self, *cols: "Expression") -> "Expression": ...
262: 
263: def StarExpression(exclude: Optional[List[str]] = None) -> Expression: ...
264: def ColumnExpression(column: str) -> Expression: ...
265: def DefaultExpression() -> Expression: ...
266: def ConstantExpression(val: Any) -> Expression: ...
267: def CaseExpression(condition: Expression, value: Expression) -> Expression: ...
268: def FunctionExpression(function: str, *cols: Expression) -> Expression: ...
269: def CoalesceOperator(*cols: Expression) -> Expression: ...
270: def LambdaExpression(lhs: Union[Tuple["Expression", ...], str]) -> Expression: ...
271: 
272: class DuckDBPyConnection:
273:     def __init__(self, *args, **kwargs) -> None: ...
274:     def __enter__(self) -> DuckDBPyConnection: ...
275:     def __exit__(self, exc_type: object, exc: object, traceback: object) -> None: ...
276:     def __del__(self) -> None: ...
277:     @property
278:     def description(self) -> Optional[List[Any]]: ...
279:     @property
280:     def rowcount(self) -> int: ...
281: 
282:     # NOTE: this section is generated by tools/pythonpkg/scripts/generate_connection_stubs.py.
283:     # Do not edit this section manually, your changes will be overwritten!
284: 
285:     # START OF CONNECTION METHODS
286:     def cursor(self) -> DuckDBPyConnection: ...
287:     def register_filesystem(self, filesystem: str) -> None: ...
288:     def unregister_filesystem(self, name: str) -> None: ...
289:     def list_filesystems(self) -> list: ...
290:     def filesystem_is_registered(self, name: str) -> bool: ...
291:     def create_function(self, name: str, function: function, parameters: Optional[List[DuckDBPyType]] = None, return_type: Optional[DuckDBPyType] = None, *, type: Optional[PythonUDFType] = PythonUDFType.NATIVE, null_handling: Optional[FunctionNullHandling] = FunctionNullHandling.DEFAULT, exception_handling: Optional[PythonExceptionHandling] = PythonExceptionHandling.DEFAULT, side_effects: bool = False) -> DuckDBPyConnection: ...
292:     def remove_function(self, name: str) -> DuckDBPyConnection: ...
293:     def sqltype(self, type_str: str) -> DuckDBPyType: ...
294:     def dtype(self, type_str: str) -> DuckDBPyType: ...
295:     def type(self, type_str: str) -> DuckDBPyType: ...
296:     def array_type(self, type: DuckDBPyType, size: int) -> DuckDBPyType: ...
297:     def list_type(self, type: DuckDBPyType) -> DuckDBPyType: ...
298:     def union_type(self, members: DuckDBPyType) -> DuckDBPyType: ...
299:     def string_type(self, collation: str = "") -> DuckDBPyType: ...
300:     def enum_type(self, name: str, type: DuckDBPyType, values: List[Any]) -> DuckDBPyType: ...
301:     def decimal_type(self, width: int, scale: int) -> DuckDBPyType: ...
302:     def struct_type(self, fields: Union[Dict[str, DuckDBPyType], List[str]]) -> DuckDBPyType: ...
303:     def row_type(self, fields: Union[Dict[str, DuckDBPyType], List[str]]) -> DuckDBPyType: ...
304:     def map_type(self, key: DuckDBPyType, value: DuckDBPyType) -> DuckDBPyType: ...
305:     def duplicate(self) -> DuckDBPyConnection: ...
306:     def execute(self, query: object, parameters: object = None) -> DuckDBPyConnection: ...
307:     def executemany(self, query: object, parameters: object = None) -> DuckDBPyConnection: ...
308:     def close(self) -> None: ...
309:     def interrupt(self) -> None: ...
310:     def fetchone(self) -> Optional[tuple]: ...
311:     def fetchmany(self, size: int = 1) -> List[Any]: ...
312:     def fetchall(self) -> List[Any]: ...
313:     def fetchnumpy(self) -> dict: ...
314:     def fetchdf(self, *, date_as_object: bool = False) -> pandas.DataFrame: ...
315:     def fetch_df(self, *, date_as_object: bool = False) -> pandas.DataFrame: ...
316:     def df(self, *, date_as_object: bool = False) -> pandas.DataFrame: ...
317:     def fetch_df_chunk(self, vectors_per_chunk: int = 1, *, date_as_object: bool = False) -> pandas.DataFrame: ...
318:     def pl(self, rows_per_batch: int = 1000000) -> polars.DataFrame: ...
319:     def fetch_arrow_table(self, rows_per_batch: int = 1000000) -> pyarrow.lib.Table: ...
320:     def arrow(self, rows_per_batch: int = 1000000) -> pyarrow.lib.Table: ...
321:     def fetch_record_batch(self, rows_per_batch: int = 1000000) -> pyarrow.lib.RecordBatchReader: ...
322:     def torch(self) -> dict: ...
323:     def tf(self) -> dict: ...
324:     def begin(self) -> DuckDBPyConnection: ...
325:     def commit(self) -> DuckDBPyConnection: ...
326:     def rollback(self) -> DuckDBPyConnection: ...
327:     def checkpoint(self) -> DuckDBPyConnection: ...
328:     def append(self, table_name: str, df: pandas.DataFrame, *, by_name: bool = False) -> DuckDBPyConnection: ...
329:     def register(self, view_name: str, python_object: object) -> DuckDBPyConnection: ...
330:     def unregister(self, view_name: str) -> DuckDBPyConnection: ...
331:     def table(self, table_name: str) -> DuckDBPyRelation: ...
332:     def view(self, view_name: str) -> DuckDBPyRelation: ...
333:     def values(self, *args: Union[List[Any],Expression, Tuple[Expression]]) -> DuckDBPyRelation: ...
334:     def table_function(self, name: str, parameters: object = None) -> DuckDBPyRelation: ...
335:     def read_json(self, path_or_buffer: Union[str, StringIO, TextIOBase], *, columns: Optional[Dict[str,str]] = None, sample_size: Optional[int] = None, maximum_depth: Optional[int] = None, records: Optional[str] = None, format: Optional[str] = None, date_format: Optional[str] = None, timestamp_format: Optional[str] = None, compression: Optional[str] = None, maximum_object_size: Optional[int] = None, ignore_errors: Optional[bool] = None, convert_strings_to_integers: Optional[bool] = None, field_appearance_threshold: Optional[float] = None, map_inference_threshold: Optional[int] = None, maximum_sample_files: Optional[int] = None, filename: Optional[bool | str] = None, hive_partitioning: Optional[bool] = None, union_by_name: Optional[bool] = None, hive_types: Optional[Dict[str, str]] = None, hive_types_autocast: Optional[bool] = None) -> DuckDBPyRelation: ...
336:     def extract_statements(self, query: str) -> List[Statement]: ...
337:     def sql(self, query: str, *, alias: str = "", params: object = None) -> DuckDBPyRelation: ...
338:     def query(self, query: str, *, alias: str = "", params: object = None) -> DuckDBPyRelation: ...
339:     def from_query(self, query: str, *, alias: str = "", params: object = None) -> DuckDBPyRelation: ...
340:     def read_csv(self, path_or_buffer: Union[str, StringIO, TextIOBase], *, header: Optional[bool | int] = None, compression: Optional[str] = None, sep: Optional[str] = None, delimiter: Optional[str] = None, dtype: Optional[Dict[str, str] | List[str]] = None, na_values: Optional[str| List[str]] = None, skiprows: Optional[int] = None, quotechar: Optional[str] = None, escapechar: Optional[str] = None, encoding: Optional[str] = None, parallel: Optional[bool] = None, date_format: Optional[str] = None, timestamp_format: Optional[str] = None, sample_size: Optional[int] = None, all_varchar: Optional[bool] = None, normalize_names: Optional[bool] = None, null_padding: Optional[bool] = None, names: Optional[List[str]] = None, lineterminator: Optional[str] = None, columns: Optional[Dict[str, str]] = None, auto_type_candidates: Optional[List[str]] = None, max_line_size: Optional[int] = None, ignore_errors: Optional[bool] = None, store_rejects: Optional[bool] = None, rejects_table: Optional[str] = None, rejects_scan: Optional[str] = None, rejects_limit: Optional[int] = None, force_not_null: Optional[List[str]] = None, buffer_size: Optional[int] = None, decimal: Optional[str] = None, allow_quoted_nulls: Optional[bool] = None, filename: Optional[bool | str] = None, hive_partitioning: Optional[bool] = None, union_by_name: Optional[bool] = None, hive_types: Optional[Dict[str, str]] = None, hive_types_autocast: Optional[bool] = None) -> DuckDBPyRelation: ...
341:     def from_csv_auto(self, path_or_buffer: Union[str, StringIO, TextIOBase], *, header: Optional[bool | int] = None, compression: Optional[str] = None, sep: Optional[str] = None, delimiter: Optional[str] = None, dtype: Optional[Dict[str, str] | List[str]] = None, na_values: Optional[str| List[str]] = None, skiprows: Optional[int] = None, quotechar: Optional[str] = None, escapechar: Optional[str] = None, encoding: Optional[str] = None, parallel: Optional[bool] = None, date_format: Optional[str] = None, timestamp_format: Optional[str] = None, sample_size: Optional[int] = None, all_varchar: Optional[bool] = None, normalize_names: Optional[bool] = None, null_padding: Optional[bool] = None, names: Optional[List[str]] = None, lineterminator: Optional[str] = None, columns: Optional[Dict[str, str]] = None, auto_type_candidates: Optional[List[str]] = None, max_line_size: Optional[int] = None, ignore_errors: Optional[bool] = None, store_rejects: Optional[bool] = None, rejects_table: Optional[str] = None, rejects_scan: Optional[str] = None, rejects_limit: Optional[int] = None, force_not_null: Optional[List[str]] = None, buffer_size: Optional[int] = None, decimal: Optional[str] = None, allow_quoted_nulls: Optional[bool] = None, filename: Optional[bool | str] = None, hive_partitioning: Optional[bool] = None, union_by_name: Optional[bool] = None, hive_types: Optional[Dict[str, str]] = None, hive_types_autocast: Optional[bool] = None) -> DuckDBPyRelation: ...
342:     def from_df(self, df: pandas.DataFrame) -> DuckDBPyRelation: ...
343:     def from_arrow(self, arrow_object: object) -> DuckDBPyRelation: ...
344:     def from_parquet(self, file_glob: str, binary_as_string: bool = False, *, file_row_number: bool = False, filename: bool = False, hive_partitioning: bool = False, union_by_name: bool = False, compression: Optional[str] = None) -> DuckDBPyRelation: ...
345:     def read_parquet(self, file_glob: str, binary_as_string: bool = False, *, file_row_number: bool = False, filename: bool = False, hive_partitioning: bool = False, union_by_name: bool = False, compression: Optional[str] = None) -> DuckDBPyRelation: ...
346:     def get_table_names(self, query: str) -> Set[str]: ...
347:     def install_extension(self, extension: str, *, force_install: bool = False, repository: Optional[str] = None, repository_url: Optional[str] = None, version: Optional[str] = None) -> None: ...
348:     def load_extension(self, extension: str) -> None: ...
349:     # END OF CONNECTION METHODS
350: 
351: class DuckDBPyRelation:
352:     def close(self) -> None: ...
353:     def __getattr__(self, name: str) -> DuckDBPyRelation: ...
354:     def __getitem__(self, name: str) -> DuckDBPyRelation: ...
355:     def __init__(self, *args, **kwargs) -> None: ...
356:     def __contains__(self, name: str) -> bool: ...
357:     def aggregate(self, aggr_expr: str, group_expr: str = ...) -> DuckDBPyRelation: ...
358:     def apply(self, function_name: str, function_aggr: str, group_expr: str = ..., function_parameter: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
359: 
360:     def cume_dist(self, window_spec: str, projected_columns: str = ...) -> DuckDBPyRelation: ...
361:     def dense_rank(self, window_spec: str, projected_columns: str = ...) -> DuckDBPyRelation: ...
362:     def percent_rank(self, window_spec: str, projected_columns: str = ...) -> DuckDBPyRelation: ...
363:     def rank(self, window_spec: str, projected_columns: str = ...) -> DuckDBPyRelation: ...
364:     def rank_dense(self, window_spec: str, projected_columns: str = ...) -> DuckDBPyRelation: ...
365:     def row_number(self, window_spec: str, projected_columns: str = ...) -> DuckDBPyRelation: ...
366: 
367:     def lag(self, column: str, window_spec: str, offset: int, default_value: str, ignore_nulls: bool, projected_columns: str = ...) -> DuckDBPyRelation: ...
368:     def lead(self, column: str, window_spec: str, offset: int, default_value: str, ignore_nulls: bool, projected_columns: str = ...) -> DuckDBPyRelation: ...
369:     def nth_value(self, column: str, window_spec: str, offset: int, ignore_nulls: bool = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
370: 
371:     def value_counts(self, column: str, groups: str = ...) -> DuckDBPyRelation: ...
372:     def geomean(self, column: str, groups: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
373:     def first(self, column: str, groups: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
374:     def first_value(self, column: str, window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
375:     def last(self, column: str, groups: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
376:     def last_value(self, column: str, window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
377:     def mode(self, aggregation_columns: str, group_columns: str = ...) -> DuckDBPyRelation: ...
378:     def n_tile(self, window_spec: str, num_buckets: int, projected_columns: str = ...) -> DuckDBPyRelation: ...
379:     def quantile_cont(self, column: str, q: Union[float, List[float]] = ..., groups: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
380:     def quantile_disc(self, column: str, q: Union[float, List[float]] = ..., groups: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
381:     def sum(self, sum_aggr: str, group_expr: str = ...) -> DuckDBPyRelation: ...
382: 
383:     def any_value(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
384:     def arg_max(self, arg_column: str, value_column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
385:     def arg_min(self, arg_column: str, value_column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
386:     def avg(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
387:     def bit_and(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
388:     def bit_or(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
389:     def bit_xor(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
390:     def bitstring_agg(self, column: str, min: Optional[int], max: Optional[int], groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
391:     def bool_and(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
392:     def bool_or(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
393:     def count(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
394:     def favg(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
395:     def fsum(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
396:     def histogram(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
397:     def max(self, max_aggr: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
398:     def min(self, min_aggr: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
399:     def mean(self, mean_aggr: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
400:     def median(self, median_aggr: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
401:     def product(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
402:     def quantile(self, q: str, quantile_aggr: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
403:     def std(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
404:     def stddev(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
405:     def stddev_pop(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
406:     def stddev_samp(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
407:     def string_agg(self, column: str, sep: str = ..., groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
408:     def var(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
409:     def var_pop(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
410:     def var_samp(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
411:     def variance(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
412:     def list(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
413: 
414:     def arrow(self, batch_size: int = ...) -> pyarrow.lib.Table: ...
415:     def __arrow_c_stream__(self, requested_schema: Optional[object] = None) -> object: ...
416:     def create(self, table_name: str) -> None: ...
417:     def create_view(self, view_name: str, replace: bool = ...) -> DuckDBPyRelation: ...
418:     def describe(self) -> DuckDBPyRelation: ...
419:     def df(self, *args, **kwargs) -> pandas.DataFrame: ...
420:     def distinct(self) -> DuckDBPyRelation: ...
421:     def except_(self, other_rel: DuckDBPyRelation) -> DuckDBPyRelation: ...
422:     def execute(self, *args, **kwargs) -> DuckDBPyRelation: ...
423:     def explain(self, type: Optional[Literal['standard', 'analyze'] | int] = 'standard') -> str: ...
424:     def fetchall(self) -> List[Any]: ...
425:     def fetchmany(self, size: int = ...) -> List[Any]: ...
426:     def fetchnumpy(self) -> dict: ...
427:     def fetchone(self) -> Optional[tuple]: ...
428:     def fetchdf(self, *args, **kwargs) -> Any: ...
429:     def fetch_arrow_reader(self, batch_size: int = ...) -> pyarrow.lib.RecordBatchReader: ...
430:     def fetch_arrow_table(self, rows_per_batch: int = ...) -> pyarrow.lib.Table: ...
431:     def filter(self, filter_expr: Union[Expression, str]) -> DuckDBPyRelation: ...
432:     def insert(self, values: List[Any]) -> None: ...
433:     def update(self, set: Dict[str, Expression], condition: Optional[Expression] = None) -> None: ...
434:     def insert_into(self, table_name: str) -> None: ...
435:     def intersect(self, other_rel: DuckDBPyRelation) -> DuckDBPyRelation: ...
436:     def join(self, other_rel: DuckDBPyRelation, condition: str, how: str = ...) -> DuckDBPyRelation: ...
437:     def cross(self, other_rel: DuckDBPyRelation) -> DuckDBPyRelation: ...
438:     def limit(self, n: int, offset: int = ...) -> DuckDBPyRelation: ...
439:     def map(self, map_function: function, schema: Optional[Dict[str, DuckDBPyType]] = None) -> DuckDBPyRelation: ...
440:     def order(self, order_expr: str) -> DuckDBPyRelation: ...
441:     def sort(self, *cols: Expression) -> DuckDBPyRelation: ...
442:     def project(self, *cols: Union[str, Expression]) -> DuckDBPyRelation: ...
443:     def select(self, *cols: Union[str, Expression]) -> DuckDBPyRelation: ...
444:     def pl(self, rows_per_batch: int = ..., connection: DuckDBPyConnection = ...) -> polars.DataFrame: ...
445:     def query(self, virtual_table_name: str, sql_query: str) -> DuckDBPyRelation: ...
446:     def record_batch(self, batch_size: int = ...) -> pyarrow.lib.RecordBatchReader: ...
447:     def select_types(self, types: List[Union[str, DuckDBPyType]]) -> DuckDBPyRelation: ...
448:     def select_dtypes(self, types: List[Union[str, DuckDBPyType]]) -> DuckDBPyRelation: ...
449:     def set_alias(self, alias: str) -> DuckDBPyRelation: ...
450:     def show(self) -> None: ...
451:     def sql_query(self) -> str: ...
452:     def to_arrow_table(self, batch_size: int = ...) -> pyarrow.lib.Table: ...
453:     def to_csv(
454:             self,
455:             file_name: str,
456:             sep: Optional[str] = None,
457:             na_rep: Optional[str] = None,
458:             header: Optional[bool] = None,
459:             quotechar: Optional[str] = None,
460:             escapechar: Optional[str] = None,
461:             date_format: Optional[str] = None,
462:             timestamp_format: Optional[str] = None,
463:             quoting: Optional[str | int] = None,
464:             encoding: Optional[str] = None,
465:             compression: Optional[str] = None,
466:             write_partition_columns: Optional[bool] = None,
467:             overwrite: Optional[bool] = None,
468:             per_thread_output: Optional[bool] = None,
469:             use_tmp_file: Optional[bool] = None,
470:             partition_by: Optional[List[str]] = None
471:     ) -> None: ...
472:     def to_df(self, *args, **kwargs) -> pandas.DataFrame: ...
473:     def to_parquet(
474:             self,
475:             file_name: str,
476:             compression: Optional[str] = None,
477:             field_ids: Optional[dict | str] = None,
478:             row_group_size_bytes: Optional[int | str] = None,
479:             row_group_size: Optional[int] = None,
480:             partition_by: Optional[List[str]] = None,
481:             write_partition_columns: Optional[bool] = None,
482:             overwrite: Optional[bool] = None,
483:             per_thread_output: Optional[bool] = None,
484:             use_tmp_file: Optional[bool] = None,
485:             append: Optional[bool] = None
486:     ) -> None: ...
487:     def fetch_df_chunk(self, vectors_per_chunk: int = 1, *, date_as_object: bool = False) -> pandas.DataFrame: ...
488:     def to_table(self, table_name: str) -> None: ...
489:     def to_view(self, view_name: str, replace: bool = ...) -> DuckDBPyRelation: ...
490:     def torch(self, connection: DuckDBPyConnection = ...) -> dict: ...
491:     def tf(self, connection: DuckDBPyConnection = ...) -> dict: ...
492:     def union(self, union_rel: DuckDBPyRelation) -> DuckDBPyRelation: ...
493:     def unique(self, unique_aggr: str) -> DuckDBPyRelation: ...
494:     def write_csv(
495:             self,
496:             file_name: str,
497:             sep: Optional[str] = None,
498:             na_rep: Optional[str] = None,
499:             header: Optional[bool] = None,
500:             quotechar: Optional[str] = None,
501:             escapechar: Optional[str] = None,
502:             date_format: Optional[str] = None,
503:             timestamp_format: Optional[str] = None,
504:             quoting: Optional[str | int] = None,
505:             encoding: Optional[str] = None,
506:             compression: Optional[str] = None,
507:             write_partition_columns: Optional[bool] = None,
508:             overwrite: Optional[bool] = None,
509:             per_thread_output: Optional[bool] = None,
510:             use_tmp_file: Optional[bool] = None,
511:             partition_by: Optional[List[str]] = None
512:     ) -> None: ...
513:     def write_parquet(
514:             self,
515:             file_name: str,
516:             compression: Optional[str] = None,
517:             field_ids: Optional[dict | str] = None,
518:             row_group_size_bytes: Optional[int | str] = None,
519:             row_group_size: Optional[int] = None,
520:             partition_by: Optional[List[str]] = None,
521:             write_partition_columns: Optional[bool] = None,
522:             overwrite: Optional[bool] = None,
523:             per_thread_output: Optional[bool] = None,
524:             use_tmp_file: Optional[bool] = None,
525:             append: Optional[bool] = None
526:     ) -> None: ...
527:     def __len__(self) -> int: ...
528:     @property
529:     def alias(self) -> str: ...
530:     @property
531:     def columns(self) -> List[Any]: ...
532:     @property
533:     def dtypes(self) -> List[DuckDBPyType]: ...
534:     @property
535:     def description(self) -> List[Any]: ...
536:     @property
537:     def shape(self) -> tuple: ...
538:     @property
539:     def type(self) -> str: ...
540:     @property
541:     def types(self) -> List[DuckDBPyType]: ...
542: 
543: class Error(Exception): ...
544: 
545: class FatalException(Error): ...
546: 
547: class HTTPException(IOException):
548:     status_code: int
549:     body: str
550:     reason: str
551:     headers: Dict[str, str]
552: 
553: class IOException(OperationalError): ...
554: 
555: class IntegrityError(Error): ...
556: 
557: class InternalError(Error): ...
558: 
559: class InternalException(InternalError): ...
560: 
561: class InterruptException(Error): ...
562: 
563: class InvalidInputException(ProgrammingError): ...
564: 
565: class InvalidTypeException(ProgrammingError): ...
566: 
567: class NotImplementedException(NotSupportedError): ...
568: 
569: class NotSupportedError(Error): ...
570: 
571: class OperationalError(Error): ...
572: 
573: class OutOfMemoryException(OperationalError): ...
574: 
575: class OutOfRangeException(DataError): ...
576: 
577: class ParserException(ProgrammingError): ...
578: 
579: class PermissionException(Error): ...
580: 
581: class ProgrammingError(Error): ...
582: 
583: class SequenceException(Error): ...
584: 
585: class SerializationException(OperationalError): ...
586: 
587: class SyntaxException(ProgrammingError): ...
588: 
589: class TransactionException(OperationalError): ...
590: 
591: class TypeMismatchException(DataError): ...
592: 
593: class Warning(Exception): ...
594: 
595: class token_type:
596:     # stubgen override - these make mypy sad
597:     #__doc__: ClassVar[str] = ...  # read-only
598:     #__members__: ClassVar[dict] = ...  # read-only
599:     __entries: ClassVar[dict] = ...
600:     comment: ClassVar[token_type] = ...
601:     identifier: ClassVar[token_type] = ...
602:     keyword: ClassVar[token_type] = ...
603:     numeric_const: ClassVar[token_type] = ...
604:     operator: ClassVar[token_type] = ...
605:     string_const: ClassVar[token_type] = ...
606:     def __init__(self, value: int) -> None: ...
607:     def __eq__(self, other: object) -> bool: ...
608:     def __getstate__(self) -> int: ...
609:     def __hash__(self) -> int: ...
610:     # stubgen override - pybind only puts index in python >= 3.8: https://github.com/EricCousineau-TRI/pybind11/blob/54430436/include/pybind11/pybind11.h#L1789
611:     if sys.version_info >= (3, 7):
612:         def __index__(self) -> int: ...
613:     def __int__(self) -> int: ...
614:     def __ne__(self, other: object) -> bool: ...
615:     def __setstate__(self, state: int) -> None: ...
616:     @property
617:     def name(self) -> str: ...
618:     @property
619:     def value(self) -> int: ...
620:     @property
621:     # stubgen override - this gets removed by stubgen but it shouldn't
622:     def __members__(self) -> object: ...
623: 
624: def connect(database: Union[str, Path] = ..., read_only: bool = ..., config: dict = ...) -> DuckDBPyConnection: ...
625: def default_connection() -> DuckDBPyConnection: ...
626: def set_default_connection(connection: DuckDBPyConnection) -> None: ...
627: def tokenize(query: str) -> List[Any]: ...
628: 
629: # NOTE: this section is generated by tools/pythonpkg/scripts/generate_connection_wrapper_stubs.py.
630: # Do not edit this section manually, your changes will be overwritten!
631: 
632: # START OF CONNECTION WRAPPER
633: def cursor(*, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
634: def register_filesystem(filesystem: str, *, connection: DuckDBPyConnection = ...) -> None: ...
635: def unregister_filesystem(name: str, *, connection: DuckDBPyConnection = ...) -> None: ...
636: def list_filesystems(*, connection: DuckDBPyConnection = ...) -> list: ...
637: def filesystem_is_registered(name: str, *, connection: DuckDBPyConnection = ...) -> bool: ...
638: def create_function(name: str, function: function, parameters: Optional[List[DuckDBPyType]] = None, return_type: Optional[DuckDBPyType] = None, *, type: Optional[PythonUDFType] = PythonUDFType.NATIVE, null_handling: Optional[FunctionNullHandling] = FunctionNullHandling.DEFAULT, exception_handling: Optional[PythonExceptionHandling] = PythonExceptionHandling.DEFAULT, side_effects: bool = False, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
639: def remove_function(name: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
640: def sqltype(type_str: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
641: def dtype(type_str: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
642: def type(type_str: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
643: def array_type(type: DuckDBPyType, size: int, *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
644: def list_type(type: DuckDBPyType, *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
645: def union_type(members: DuckDBPyType, *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
646: def string_type(collation: str = "", *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
647: def enum_type(name: str, type: DuckDBPyType, values: List[Any], *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
648: def decimal_type(width: int, scale: int, *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
649: def struct_type(fields: Union[Dict[str, DuckDBPyType], List[str]], *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
650: def row_type(fields: Union[Dict[str, DuckDBPyType], List[str]], *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
651: def map_type(key: DuckDBPyType, value: DuckDBPyType, *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
652: def duplicate(*, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
653: def execute(query: object, parameters: object = None, *, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
654: def executemany(query: object, parameters: object = None, *, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
655: def close(*, connection: DuckDBPyConnection = ...) -> None: ...
656: def interrupt(*, connection: DuckDBPyConnection = ...) -> None: ...
657: def fetchone(*, connection: DuckDBPyConnection = ...) -> Optional[tuple]: ...
658: def fetchmany(size: int = 1, *, connection: DuckDBPyConnection = ...) -> List[Any]: ...
659: def fetchall(*, connection: DuckDBPyConnection = ...) -> List[Any]: ...
660: def fetchnumpy(*, connection: DuckDBPyConnection = ...) -> dict: ...
661: def fetchdf(*, date_as_object: bool = False, connection: DuckDBPyConnection = ...) -> pandas.DataFrame: ...
662: def fetch_df(*, date_as_object: bool = False, connection: DuckDBPyConnection = ...) -> pandas.DataFrame: ...
663: def df(*, date_as_object: bool = False, connection: DuckDBPyConnection = ...) -> pandas.DataFrame: ...
664: def fetch_df_chunk(vectors_per_chunk: int = 1, *, date_as_object: bool = False, connection: DuckDBPyConnection = ...) -> pandas.DataFrame: ...
665: def pl(rows_per_batch: int = 1000000, *, connection: DuckDBPyConnection = ...) -> polars.DataFrame: ...
666: def fetch_arrow_table(rows_per_batch: int = 1000000, *, connection: DuckDBPyConnection = ...) -> pyarrow.lib.Table: ...
667: def arrow(rows_per_batch: int = 1000000, *, connection: DuckDBPyConnection = ...) -> pyarrow.lib.Table: ...
668: def fetch_record_batch(rows_per_batch: int = 1000000, *, connection: DuckDBPyConnection = ...) -> pyarrow.lib.RecordBatchReader: ...
669: def torch(*, connection: DuckDBPyConnection = ...) -> dict: ...
670: def tf(*, connection: DuckDBPyConnection = ...) -> dict: ...
671: def begin(*, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
672: def commit(*, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
673: def rollback(*, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
674: def checkpoint(*, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
675: def append(table_name: str, df: pandas.DataFrame, *, by_name: bool = False, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
676: def register(view_name: str, python_object: object, *, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
677: def unregister(view_name: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
678: def table(table_name: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
679: def view(view_name: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
680: def values(*args: Union[List[Any],Expression, Tuple[Expression]], connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
681: def table_function(name: str, parameters: object = None, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
682: def read_json(path_or_buffer: Union[str, StringIO, TextIOBase], *, columns: Optional[Dict[str,str]] = None, sample_size: Optional[int] = None, maximum_depth: Optional[int] = None, records: Optional[str] = None, format: Optional[str] = None, date_format: Optional[str] = None, timestamp_format: Optional[str] = None, compression: Optional[str] = None, maximum_object_size: Optional[int] = None, ignore_errors: Optional[bool] = None, convert_strings_to_integers: Optional[bool] = None, field_appearance_threshold: Optional[float] = None, map_inference_threshold: Optional[int] = None, maximum_sample_files: Optional[int] = None, filename: Optional[bool | str] = None, hive_partitioning: Optional[bool] = None, union_by_name: Optional[bool] = None, hive_types: Optional[Dict[str, str]] = None, hive_types_autocast: Optional[bool] = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
683: def extract_statements(query: str, *, connection: DuckDBPyConnection = ...) -> List[Statement]: ...
684: def sql(query: str, *, alias: str = "", params: object = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
685: def query(query: str, *, alias: str = "", params: object = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
686: def from_query(query: str, *, alias: str = "", params: object = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
687: def read_csv(path_or_buffer: Union[str, StringIO, TextIOBase], *, header: Optional[bool | int] = None, compression: Optional[str] = None, sep: Optional[str] = None, delimiter: Optional[str] = None, dtype: Optional[Dict[str, str] | List[str]] = None, na_values: Optional[str| List[str]] = None, skiprows: Optional[int] = None, quotechar: Optional[str] = None, escapechar: Optional[str] = None, encoding: Optional[str] = None, parallel: Optional[bool] = None, date_format: Optional[str] = None, timestamp_format: Optional[str] = None, sample_size: Optional[int] = None, all_varchar: Optional[bool] = None, normalize_names: Optional[bool] = None, null_padding: Optional[bool] = None, names: Optional[List[str]] = None, lineterminator: Optional[str] = None, columns: Optional[Dict[str, str]] = None, auto_type_candidates: Optional[List[str]] = None, max_line_size: Optional[int] = None, ignore_errors: Optional[bool] = None, store_rejects: Optional[bool] = None, rejects_table: Optional[str] = None, rejects_scan: Optional[str] = None, rejects_limit: Optional[int] = None, force_not_null: Optional[List[str]] = None, buffer_size: Optional[int] = None, decimal: Optional[str] = None, allow_quoted_nulls: Optional[bool] = None, filename: Optional[bool | str] = None, hive_partitioning: Optional[bool] = None, union_by_name: Optional[bool] = None, hive_types: Optional[Dict[str, str]] = None, hive_types_autocast: Optional[bool] = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
688: def from_csv_auto(path_or_buffer: Union[str, StringIO, TextIOBase], *, header: Optional[bool | int] = None, compression: Optional[str] = None, sep: Optional[str] = None, delimiter: Optional[str] = None, dtype: Optional[Dict[str, str] | List[str]] = None, na_values: Optional[str| List[str]] = None, skiprows: Optional[int] = None, quotechar: Optional[str] = None, escapechar: Optional[str] = None, encoding: Optional[str] = None, parallel: Optional[bool] = None, date_format: Optional[str] = None, timestamp_format: Optional[str] = None, sample_size: Optional[int] = None, all_varchar: Optional[bool] = None, normalize_names: Optional[bool] = None, null_padding: Optional[bool] = None, names: Optional[List[str]] = None, lineterminator: Optional[str] = None, columns: Optional[Dict[str, str]] = None, auto_type_candidates: Optional[List[str]] = None, max_line_size: Optional[int] = None, ignore_errors: Optional[bool] = None, store_rejects: Optional[bool] = None, rejects_table: Optional[str] = None, rejects_scan: Optional[str] = None, rejects_limit: Optional[int] = None, force_not_null: Optional[List[str]] = None, buffer_size: Optional[int] = None, decimal: Optional[str] = None, allow_quoted_nulls: Optional[bool] = None, filename: Optional[bool | str] = None, hive_partitioning: Optional[bool] = None, union_by_name: Optional[bool] = None, hive_types: Optional[Dict[str, str]] = None, hive_types_autocast: Optional[bool] = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
689: def from_df(df: pandas.DataFrame, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
690: def from_arrow(arrow_object: object, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
691: def from_parquet(file_glob: str, binary_as_string: bool = False, *, file_row_number: bool = False, filename: bool = False, hive_partitioning: bool = False, union_by_name: bool = False, compression: Optional[str] = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
692: def read_parquet(file_glob: str, binary_as_string: bool = False, *, file_row_number: bool = False, filename: bool = False, hive_partitioning: bool = False, union_by_name: bool = False, compression: Optional[str] = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
693: def get_table_names(query: str, *, connection: DuckDBPyConnection = ...) -> Set[str]: ...
694: def install_extension(extension: str, *, force_install: bool = False, repository: Optional[str] = None, repository_url: Optional[str] = None, version: Optional[str] = None, connection: DuckDBPyConnection = ...) -> None: ...
695: def load_extension(extension: str, *, connection: DuckDBPyConnection = ...) -> None: ...
696: def project(df: pandas.DataFrame, *args: str, groups: str = "", connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
697: def distinct(df: pandas.DataFrame, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
698: def write_csv(df: pandas.DataFrame, filename: str, *, sep: Optional[str] = None, na_rep: Optional[str] = None, header: Optional[bool] = None, quotechar: Optional[str] = None, escapechar: Optional[str] = None, date_format: Optional[str] = None, timestamp_format: Optional[str] = None, quoting: Optional[str | int] = None, encoding: Optional[str] = None, compression: Optional[str] = None, overwrite: Optional[bool] = None, per_thread_output: Optional[bool] = None, use_tmp_file: Optional[bool] = None, partition_by: Optional[List[str]] = None, write_partition_columns: Optional[bool] = None, connection: DuckDBPyConnection = ...) -> None: ...
699: def aggregate(df: pandas.DataFrame, aggr_expr: str | List[Expression], group_expr: str = "", *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
700: def alias(df: pandas.DataFrame, alias: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
701: def filter(df: pandas.DataFrame, filter_expr: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
702: def limit(df: pandas.DataFrame, n: int, offset: int = 0, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
703: def order(df: pandas.DataFrame, order_expr: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
704: def query_df(df: pandas.DataFrame, virtual_table_name: str, sql_query: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
705: def description(*, connection: DuckDBPyConnection = ...) -> Optional[List[Any]]: ...
706: def rowcount(*, connection: DuckDBPyConnection = ...) -> int: ...
707: # END OF CONNECTION WRAPPER
[end of tools/pythonpkg/duckdb-stubs/__init__.pyi]
[start of tools/pythonpkg/scripts/connection_methods.json]
1: [
2: 	{
3: 		"name": "cursor",
4: 		"function": "Cursor",
5: 		"docs": "Create a duplicate of the current connection",
6: 		"return": "DuckDBPyConnection"
7: 	},
8: 	{
9: 		"name": "register_filesystem",
10: 		"function": "RegisterFilesystem",
11: 		"docs": "Register a fsspec compliant filesystem",
12: 		"args": [
13: 			{
14: 				"name": "filesystem",
15: 				"type": "str"
16: 			}
17: 		],
18: 		"return": "None"
19: 	},
20: 	{
21: 		"name": "unregister_filesystem",
22: 		"function": "UnregisterFilesystem",
23: 		"docs": "Unregister a filesystem",
24: 		"args": [
25: 			{
26: 				"name": "name",
27: 				"type": "str"
28: 			}
29: 		],
30: 		"return": "None"
31: 	},
32: 	{
33: 		"name": "list_filesystems",
34: 		"function": "ListFilesystems",
35: 		"docs": "List registered filesystems, including builtin ones",
36: 		"return": "list"
37: 	},
38: 	{
39: 		"name": "filesystem_is_registered",
40: 		"function": "FileSystemIsRegistered",
41: 		"docs": "Check if a filesystem with the provided name is currently registered",
42: 		"args": [
43: 			{
44: 				"name": "name",
45: 				"type": "str"
46: 			}
47: 		],
48: 		"return": "bool"
49: 	},
50: 	{
51: 		"name": "create_function",
52: 		"function": "RegisterScalarUDF",
53: 		"docs": "Create a DuckDB function out of the passing in Python function so it can be used in queries",
54: 		"args": [
55: 			{
56: 				"name": "name",
57: 				"type": "str"
58: 			},
59: 			{
60: 				"name": "function",
61: 				"type": "function"
62: 			},
63: 			{
64: 				"name": "parameters",
65: 				"type": "Optional[List[DuckDBPyType]]",
66: 				"default": "None"
67: 			},
68: 			{
69: 				"name": "return_type",
70: 				"type": "Optional[DuckDBPyType]",
71: 				"default": "None"
72: 			}
73: 		],
74: 		"kwargs": [
75: 			{
76: 				"name": "type",
77: 				"type": "Optional[PythonUDFType]",
78: 				"default": "PythonUDFType.NATIVE"
79: 			},
80: 			{
81: 				"name": "null_handling",
82: 				"type": "Optional[FunctionNullHandling]",
83: 				"default": "FunctionNullHandling.DEFAULT"
84: 			},
85: 			{
86: 				"name": "exception_handling",
87: 				"type": "Optional[PythonExceptionHandling]",
88: 				"default": "PythonExceptionHandling.DEFAULT"
89: 			},
90: 			{
91: 				"name": "side_effects",
92: 				"type": "bool",
93: 				"default": "False"
94: 			}
95: 		],
96: 		"return": "DuckDBPyConnection"
97: 	},
98: 	{
99: 		"name": "remove_function",
100: 		"function": "UnregisterUDF",
101: 		"docs": "Remove a previously created function",
102: 		"args": [
103: 			{
104: 				"name": "name",
105: 				"type": "str"
106: 			}
107: 		],
108: 		"return": "DuckDBPyConnection"
109: 	},
110: 	{
111: 		"name": ["sqltype", "dtype", "type"],
112: 		"function": "Type",
113: 		"docs": "Create a type object by parsing the 'type_str' string",
114: 		"args": [
115: 			{
116: 				"name": "type_str",
117: 				"type": "str"
118: 			}
119: 		],
120: 		"return": "DuckDBPyType"
121: 	},
122: 	{
123: 		"name": "array_type",
124: 		"function": "ArrayType",
125: 		"docs": "Create an array type object of 'type'",
126: 		"args": [
127: 			{
128: 				"name": "type",
129: 				"type": "DuckDBPyType",
130: 				"allow_none": false
131: 			},
132: 			{
133: 				"name": "size",
134: 				"type": "int"
135: 			}
136: 		],
137: 		"return": "DuckDBPyType"
138: 	},
139: 	{
140: 		"name": "list_type",
141: 		"function": "ListType",
142: 		"docs": "Create a list type object of 'type'",
143: 		"args": [
144: 			{
145: 				"name": "type",
146: 				"type": "DuckDBPyType",
147: 				"allow_none": false
148: 			}
149: 		],
150: 		"return": "DuckDBPyType"
151: 	},
152: 	{
153: 		"name": "union_type",
154: 		"function": "UnionType",
155: 		"docs": "Create a union type object from 'members'",
156: 		"args": [
157: 			{
158: 				"name": "members",
159: 				"type": "DuckDBPyType",
160: 				"allow_none": false
161: 			}
162: 		],
163: 		"return": "DuckDBPyType"
164: 	},
165: 	{
166: 		"name": "string_type",
167: 		"function": "StringType",
168: 		"docs": "Create a string type with an optional collation",
169: 		"args": [
170: 			{
171: 				"name": "collation",
172: 				"type": "str",
173: 				"default": "\"\""
174: 			}
175: 		],
176: 		"return": "DuckDBPyType"
177: 	},
178: 	{
179: 		"name": "enum_type",
180: 		"function": "EnumType",
181: 		"docs": "Create an enum type of underlying 'type', consisting of the list of 'values'",
182: 		"args": [
183: 			{
184: 				"name": "name",
185: 				"type": "str"
186: 			},
187: 			{
188: 				"name": "type",
189: 				"type": "DuckDBPyType"
190: 			},
191: 			{
192: 				"name": "values",
193: 				"type": "List[Any]"
194: 			}
195: 		],
196: 		"return": "DuckDBPyType"
197: 	},
198: 	{
199: 		"name": "decimal_type",
200: 		"function": "DecimalType",
201: 		"docs": "Create a decimal type with 'width' and 'scale'",
202: 		"args": [
203: 			{
204: 				"name": "width",
205: 				"type": "int"
206: 			},
207: 			{
208: 				"name": "scale",
209: 				"type": "int"
210: 			}
211: 		],
212: 		"return": "DuckDBPyType"
213: 	},
214: 	{
215: 		"name": ["struct_type", "row_type"],
216: 		"function": "StructType",
217: 		"docs": "Create a struct type object from 'fields'",
218: 		"args": [
219: 			{
220: 				"name": "fields",
221: 				"type": "Union[Dict[str, DuckDBPyType], List[str]]"
222: 			}
223: 		],
224: 		"return": "DuckDBPyType"
225: 	},
226: 	{
227: 		"name": "map_type",
228: 		"function": "MapType",
229: 		"docs": "Create a map type object from 'key_type' and 'value_type'",
230: 		"args": [
231: 			{
232: 				"name": "key",
233: 				"allow_none": false,
234: 				"type": "DuckDBPyType"
235: 			},
236: 			{
237: 				"name": "value",
238: 				"allow_none": false,
239: 				"type": "DuckDBPyType"
240: 			}
241: 		],
242: 		"return": "DuckDBPyType"
243: 	},
244: 	{
245: 		"name": "duplicate",
246: 		"function": "Cursor",
247: 		"docs": "Create a duplicate of the current connection",
248: 		"return": "DuckDBPyConnection"
249: 	},
250: 	{
251: 		"name": "execute",
252: 		"function": "Execute",
253: 		"docs": "Execute the given SQL query, optionally using prepared statements with parameters set",
254: 		"args": [
255: 			{
256: 				"name": "query",
257: 				"type": "object"
258: 			},
259: 			{
260: 				"name": "parameters",
261: 				"default": "None",
262: 				"type": "object"
263: 			}
264: 		],
265: 		"return": "DuckDBPyConnection"
266: 	},
267: 	{
268: 		"name": "executemany",
269: 		"function": "ExecuteMany",
270: 		"docs": "Execute the given prepared statement multiple times using the list of parameter sets in parameters",
271: 		"args": [
272: 			{
273: 				"name": "query",
274: 				"type": "object"
275: 			},
276: 			{
277: 				"name": "parameters",
278: 				"default": "None",
279: 				"type": "object"
280: 			}
281: 		],
282: 		"return": "DuckDBPyConnection"
283: 	},
284: 	{
285: 		"name": "close",
286: 		"function": "Close",
287: 		"docs": "Close the connection",
288: 		"return": "None"
289: 	},
290: 	{
291: 		"name": "interrupt",
292: 		"function": "Interrupt",
293: 		"docs": "Interrupt pending operations",
294: 		"return": "None"
295: 	},
296: 	{
297: 		"name": "fetchone",
298: 		"function": "FetchOne",
299: 		"docs": "Fetch a single row from a result following execute",
300: 		"return": "Optional[tuple]"
301: 	},
302: 	{
303: 		"name": "fetchmany",
304: 		"function": "FetchMany",
305: 		"docs": "Fetch the next set of rows from a result following execute",
306: 		"args": [
307: 			{
308: 				"name": "size",
309: 				"default": "1",
310: 				"type": "int"
311: 			}
312: 		],
313: 		"return": "List[Any]"
314: 	},
315: 	{
316: 		"name": "fetchall",
317: 		"function": "FetchAll",
318: 		"docs": "Fetch all rows from a result following execute",
319: 		"return": "List[Any]"
320: 	},
321: 	{
322: 		"name": "fetchnumpy",
323: 		"function": "FetchNumpy",
324: 		"docs": "Fetch a result as list of NumPy arrays following execute",
325: 		"return": "dict"
326: 	},
327: 	{
328: 		"name": ["fetchdf", "fetch_df", "df"],
329: 		"function": "FetchDF",
330: 		"docs": "Fetch a result as DataFrame following execute()",
331: 		"kwargs": [
332: 			{
333: 				"name": "date_as_object",
334: 				"default": "False",
335: 				"type": "bool"
336: 			}
337: 		],
338: 		"return": "pandas.DataFrame"
339: 	},
340: 	{
341: 		"name": "fetch_df_chunk",
342: 		"function": "FetchDFChunk",
343: 		"docs": "Fetch a chunk of the result as DataFrame following execute()",
344: 		"args": [
345: 			{
346: 				"name": "vectors_per_chunk",
347: 				"default": "1",
348: 				"type": "int"
349: 			}
350: 		],
351: 		"kwargs": [
352: 			{
353: 				"name": "date_as_object",
354: 				"default": "False",
355: 				"type": "bool"
356: 			}
357: 		],
358: 		"return": "pandas.DataFrame"
359: 	},
360: 	{
361: 		"name": "pl",
362: 		"function": "FetchPolars",
363: 		"docs": "Fetch a result as Polars DataFrame following execute()",
364: 		"args": [
365: 			{
366: 				"name": "rows_per_batch",
367: 				"default": "1000000",
368: 				"type": "int"
369: 			}
370: 		],
371: 		"return": "polars.DataFrame"
372: 	},
373: 	{
374: 		"name": ["fetch_arrow_table", "arrow"],
375: 		"function": "FetchArrow",
376: 		"docs": "Fetch a result as Arrow table following execute()",
377: 		"args": [
378: 			{
379: 				"name": "rows_per_batch",
380: 				"default": "1000000",
381: 				"type": "int"
382: 			}
383: 		],
384: 		"return": "pyarrow.lib.Table"
385: 	},
386: 	{
387: 		"name": "fetch_record_batch",
388: 		"function": "FetchRecordBatchReader",
389: 		"docs": "Fetch an Arrow RecordBatchReader following execute()",
390: 		"args": [
391: 			{
392: 				"name": "rows_per_batch",
393: 				"default": "1000000",
394: 				"type": "int"
395: 			}
396: 		],
397: 		"return": "pyarrow.lib.RecordBatchReader"
398: 	},
399: 	{
400: 		"name": "torch",
401: 		"function": "FetchPyTorch",
402: 		"docs": "Fetch a result as dict of PyTorch Tensors following execute()",
403: 		"return": "dict"
404: 	},
405: 	{
406: 		"name": "tf",
407: 		"function": "FetchTF",
408: 		"docs": "Fetch a result as dict of TensorFlow Tensors following execute()",
409: 		"return": "dict"
410: 	},
411: 	{
412: 		"name": "begin",
413: 		"function": "Begin",
414: 		"docs": "Start a new transaction",
415: 		"return": "DuckDBPyConnection"
416: 	},
417: 	{
418: 		"name": "commit",
419: 		"function": "Commit",
420: 		"docs": "Commit changes performed within a transaction",
421: 		"return": "DuckDBPyConnection"
422: 	},
423: 	{
424: 		"name": "rollback",
425: 		"function": "Rollback",
426: 		"docs": "Roll back changes performed within a transaction",
427: 		"return": "DuckDBPyConnection"
428: 	},
429: 	{
430: 		"name": "checkpoint",
431: 		"function": "Checkpoint",
432: 		"docs": "Synchronizes data in the write-ahead log (WAL) to the database data file (no-op for in-memory connections)",
433: 		"return": "DuckDBPyConnection"
434: 	},
435: 	{
436: 		"name": "append",
437: 		"function": "Append",
438: 		"docs": "Append the passed DataFrame to the named table",
439: 		"args": [
440: 			{
441: 				"name": "table_name",
442: 				"type": "str"
443: 			},
444: 			{
445: 				"name": "df",
446: 				"type": "pandas.DataFrame"
447: 			}
448: 		],
449: 		"kwargs": [
450: 			{
451: 				"name": "by_name",
452: 				"default": "False",
453: 				"type": "bool"
454: 			}
455: 		],
456: 		"return": "DuckDBPyConnection"
457: 	},
458: 	{
459: 		"name": "register",
460: 		"function": "RegisterPythonObject",
461: 		"docs": "Register the passed Python Object value for querying with a view",
462: 		"args": [
463: 			{
464: 				"name": "view_name",
465: 				"type": "str"
466: 			},
467: 			{
468: 				"name": "python_object",
469: 				"type": "object"
470: 			}
471: 		],
472: 		"return": "DuckDBPyConnection"
473: 	},
474: 	{
475: 		"name": "unregister",
476: 		"function": "UnregisterPythonObject",
477: 		"docs": "Unregister the view name",
478: 		"args": [
479: 			{
480: 				"name": "view_name",
481: 				"type": "str"
482: 			}
483: 		],
484: 		"return": "DuckDBPyConnection"
485: 	},
486: 	{
487: 		"name": "table",
488: 		"function": "Table",
489: 		"docs": "Create a relation object for the named table",
490: 		"args": [
491: 			{
492: 				"name": "table_name",
493: 				"type": "str"
494: 			}
495: 		],
496: 		"return": "DuckDBPyRelation"
497: 	},
498: 	{
499: 		"name": "view",
500: 		"function": "View",
501: 		"docs": "Create a relation object for the named view",
502: 		"args": [
503: 			{
504: 				"name": "view_name",
505: 				"type": "str"
506: 			}
507: 		],
508: 		"return": "DuckDBPyRelation"
509: 	},
510: 	{
511: 		"name": "values",
512: 		"function": "Values",
513: 		"docs": "Create a relation object from the passed values",
514: 		"args": [
515: 			{
516: 				"name": "*args",
517: 				"type": "Union[List[Any],Expression, Tuple[Expression]]"
518: 			}
519: 		],
520: 		"return": "DuckDBPyRelation"
521: 	},
522: 	{
523: 		"name": "table_function",
524: 		"function": "TableFunction",
525: 		"docs": "Create a relation object from the named table function with given parameters",
526: 		"args": [
527: 			{
528: 				"name": "name",
529: 				"type": "str"
530: 			},
531: 			{
532: 				"name": "parameters",
533: 				"default": "None",
534: 				"type": "object"
535: 			}
536: 		],
537: 		"return": "DuckDBPyRelation"
538: 	},
539: 	{
540: 		"name": "read_json",
541: 		"function": "ReadJSON",
542: 		"docs": "Create a relation object from the JSON file in 'name'",
543: 		"args": [
544: 			{
545: 				"name": "path_or_buffer",
546: 				"type": "Union[str, StringIO, TextIOBase]"
547: 			}
548: 		],
549: 		"kwargs": [
550: 			{
551: 				"name": "columns",
552: 				"default": "None",
553: 				"type": "Optional[Dict[str,str]]"
554: 			},
555: 			{
556: 				"name": "sample_size",
557: 				"default": "None",
558: 				"type": "Optional[int]"
559: 			},
560: 			{
561: 				"name": "maximum_depth",
562: 				"default": "None",
563: 				"type": "Optional[int]"
564: 			},
565: 			{
566: 				"name": "records",
567: 				"default": "None",
568: 				"type": "Optional[str]"
569: 			},
570: 			{
571: 				"name": "format",
572: 				"default": "None",
573: 				"type": "Optional[str]"
574: 			},
575: 			{
576: 				"name": "date_format",
577: 				"default": "None",
578: 				"type": "Optional[str]"
579: 			},
580: 			{
581: 				"name": "timestamp_format",
582: 				"default": "None",
583: 				"type": "Optional[str]"
584: 			},
585: 			{
586: 				"name": "compression",
587: 				"default": "None",
588: 				"type": "Optional[str]"
589: 			},
590: 			{
591: 				"name": "maximum_object_size",
592: 				"default": "None",
593: 				"type": "Optional[int]"
594: 			},
595: 			{
596: 				"name": "ignore_errors",
597: 				"default": "None",
598: 				"type": "Optional[bool]"
599: 			},
600: 			{
601: 				"name": "convert_strings_to_integers",
602: 				"default": "None",
603: 				"type": "Optional[bool]"
604: 			},
605: 			{
606: 				"name": "field_appearance_threshold",
607: 				"default": "None",
608: 				"type": "Optional[float]"
609: 			},
610: 			{
611: 				"name": "map_inference_threshold",
612: 				"default": "None",
613: 				"type": "Optional[int]"
614: 			},
615: 			{
616: 				"name": "maximum_sample_files",
617: 				"default": "None",
618: 				"type": "Optional[int]"
619: 			},
620: 			{
621: 				"name": "filename",
622: 				"default": "None",
623: 				"type": "Optional[bool | str]"
624: 			},
625: 			{
626: 				"name": "hive_partitioning",
627: 				"default": "None",
628: 				"type": "Optional[bool]"
629: 			},
630: 			{
631: 				"name": "union_by_name",
632: 				"default": "None",
633: 				"type": "Optional[bool]"
634: 			},
635: 			{
636: 				"name": "hive_types",
637: 				"default": "None",
638: 				"type": "Optional[Dict[str, str]]"
639: 			},
640: 			{
641: 				"name": "hive_types_autocast",
642: 				"default": "None",
643: 				"type": "Optional[bool]"
644: 			}
645: 		],
646: 		"return": "DuckDBPyRelation"
647: 	},
648: 	{
649: 		"name": "extract_statements",
650: 		"function": "ExtractStatements",
651: 		"docs": "Parse the query string and extract the Statement object(s) produced",
652: 		"args": [
653: 			{
654: 				"name": "query",
655: 				"type": "str"
656: 			}
657: 		],
658: 		"return": "List[Statement]"
659: 	},
660: 	{
661: 		"name": ["sql", "query", "from_query"],
662: 		"function": "RunQuery",
663: 		"docs": "Run a SQL query. If it is a SELECT statement, create a relation object from the given SQL query, otherwise run the query as-is.",
664: 		"args": [
665: 			{
666: 				"name": "query",
667: 				"type": "str"
668: 			}
669: 		],
670: 		"kwargs": [
671: 			{
672: 				"name": "alias",
673: 				"default": "\"\"",
674: 				"type": "str"
675: 			},
676: 			{
677: 				"name": "params",
678: 				"default": "None",
679: 				"type": "object"
680: 			}
681: 		],
682: 		"return": "DuckDBPyRelation"
683: 	},
684: 	{
685: 		"name": ["read_csv", "from_csv_auto"],
686: 		"function": "ReadCSV",
687: 		"docs": "Create a relation object from the CSV file in 'name'",
688: 		"args": [
689: 			{
690: 				"name": "path_or_buffer",
691: 				"type": "Union[str, StringIO, TextIOBase]"
692: 			}
693: 		],
694: 		"kwargs_as_dict": true,
695: 		"kwargs": [
696: 			{
697: 				"name": "header",
698: 				"default": "None",
699: 				"type": "Optional[bool | int]"
700: 			},
701: 			{
702: 				"name": "compression",
703: 				"default": "None",
704: 				"type": "Optional[str]"
705: 			},
706: 			{
707: 				"name": "sep",
708: 				"default": "None",
709: 				"type": "Optional[str]"
710: 			},
711: 			{
712: 				"name": "delimiter",
713: 				"default": "None",
714: 				"type": "Optional[str]"
715: 			},
716: 			{
717: 				"name": "dtype",
718: 				"default": "None",
719: 				"type": "Optional[Dict[str, str] | List[str]]"
720: 			},
721: 			{
722: 				"name": "na_values",
723: 				"default": "None",
724: 				"type": "Optional[str| List[str]]"
725: 			},
726: 			{
727: 				"name": "skiprows",
728: 				"default": "None",
729: 				"type": "Optional[int]"
730: 			},
731: 			{
732: 				"name": "quotechar",
733: 				"default": "None",
734: 				"type": "Optional[str]"
735: 			},
736: 			{
737: 				"name": "escapechar",
738: 				"default": "None",
739: 				"type": "Optional[str]"
740: 			},
741: 			{
742: 				"name": "encoding",
743: 				"default": "None",
744: 				"type": "Optional[str]"
745: 			},
746: 			{
747: 				"name": "parallel",
748: 				"default": "None",
749: 				"type": "Optional[bool]"
750: 			},
751: 			{
752: 				"name": "date_format",
753: 				"default": "None",
754: 				"type": "Optional[str]"
755: 			},
756: 			{
757: 				"name": "timestamp_format",
758: 				"default": "None",
759: 				"type": "Optional[str]"
760: 			},
761: 			{
762: 				"name": "sample_size",
763: 				"default": "None",
764: 				"type": "Optional[int]"
765: 			},
766: 			{
767: 				"name": "all_varchar",
768: 				"default": "None",
769: 				"type": "Optional[bool]"
770: 			},
771: 			{
772: 				"name": "normalize_names",
773: 				"default": "None",
774: 				"type": "Optional[bool]"
775: 			},
776: 			{
777: 				"name": "null_padding",
778: 				"default": "None",
779: 				"type": "Optional[bool]"
780: 			},
781: 			{
782: 				"name": "names",
783: 				"default": "None",
784: 				"type": "Optional[List[str]]"
785: 			},
786: 			{
787: 				"name": "lineterminator",
788: 				"default": "None",
789: 				"type": "Optional[str]"
790: 			},
791: 			{
792: 				"name": "columns",
793: 				"default": "None",
794: 				"type": "Optional[Dict[str, str]]"
795: 			},
796: 			{
797: 				"name": "auto_type_candidates",
798: 				"default": "None",
799: 				"type": "Optional[List[str]]"
800: 			},
801: 			{
802: 				"name": "max_line_size",
803: 				"default": "None",
804: 				"type": "Optional[int]"
805: 			},
806: 			{
807: 				"name": "ignore_errors",
808: 				"default": "None",
809: 				"type": "Optional[bool]"
810: 			},
811: 			{
812: 				"name": "store_rejects",
813: 				"default": "None",
814: 				"type": "Optional[bool]"
815: 			},
816: 			{
817: 				"name": "rejects_table",
818: 				"default": "None",
819: 				"type": "Optional[str]"
820: 			},
821: 			{
822: 				"name": "rejects_scan",
823: 				"default": "None",
824: 				"type": "Optional[str]"
825: 			},
826: 			{
827: 				"name": "rejects_limit",
828: 				"default": "None",
829: 				"type": "Optional[int]"
830: 			},
831: 			{
832: 				"name": "force_not_null",
833: 				"default": "None",
834: 				"type": "Optional[List[str]]"
835: 			},
836: 			{
837: 				"name": "buffer_size",
838: 				"default": "None",
839: 				"type": "Optional[int]"
840: 			},
841: 			{
842: 				"name": "decimal",
843: 				"default": "None",
844: 				"type": "Optional[str]"
845: 			},
846: 			{
847: 				"name": "allow_quoted_nulls",
848: 				"default": "None",
849: 				"type": "Optional[bool]"
850: 			},
851: 			{
852: 				"name": "filename",
853: 				"default": "None",
854: 				"type": "Optional[bool | str]"
855: 			},
856: 			{
857: 				"name": "hive_partitioning",
858: 				"default": "None",
859: 				"type": "Optional[bool]"
860: 			},
861: 			{
862: 				"name": "union_by_name",
863: 				"default": "None",
864: 				"type": "Optional[bool]"
865: 			},
866: 			{
867: 				"name": "hive_types",
868: 				"default": "None",
869: 				"type": "Optional[Dict[str, str]]"
870: 			},
871: 			{
872: 				"name": "hive_types_autocast",
873: 				"default": "None",
874: 				"type": "Optional[bool]"
875: 			}
876: 		],
877: 		"return": "DuckDBPyRelation"
878: 	},
879: 	{
880: 		"name": "from_df",
881: 		"function": "FromDF",
882: 		"docs": "Create a relation object from the DataFrame in df",
883: 		"args": [
884: 			{
885: 				"name": "df",
886: 				"type": "pandas.DataFrame"
887: 			}
888: 		],
889: 		"return": "DuckDBPyRelation"
890: 	},
891: 	{
892: 		"name": "from_arrow",
893: 		"function": "FromArrow",
894: 		"docs": "Create a relation object from an Arrow object",
895: 		"args": [
896: 			{
897: 				"name": "arrow_object",
898: 				"type": "object"
899: 			}
900: 		],
901: 		"return": "DuckDBPyRelation"
902: 	},
903: 	{
904: 		"name": ["from_parquet", "read_parquet"],
905: 		"function": "FromParquet",
906: 		"docs": "Create a relation object from the Parquet files in file_glob",
907: 		"args": [
908: 			{
909: 				"name": "file_glob",
910: 				"type": "str"
911: 			},
912: 			{
913: 				"name": "binary_as_string",
914: 				"default": "False",
915: 				"type": "bool"
916: 			}
917: 		],
918: 		"kwargs": [
919: 			{
920: 				"name": "file_row_number",
921: 				"default": "False",
922: 				"type": "bool"
923: 			},
924: 			{
925: 				"name": "filename",
926: 				"default": "False",
927: 				"type": "bool"
928: 			},
929: 			{
930: 				"name": "hive_partitioning",
931: 				"default": "False",
932: 				"type": "bool"
933: 			},
934: 			{
935: 				"name": "union_by_name",
936: 				"default": "False",
937: 				"type": "bool"
938: 			},
939: 			{
940: 				"name": "compression",
941: 				"default": "None",
942: 				"type": "Optional[str]"
943: 			}
944: 		],
945: 		"return": "DuckDBPyRelation"
946: 	},
947: 	{
948: 		"name": ["from_parquet", "read_parquet"],
949: 		"function": "FromParquets",
950: 		"docs": "Create a relation object from the Parquet files in file_globs",
951: 		"args": [
952: 			{
953: 				"name": "file_globs",
954: 				"type": "str"
955: 			},
956: 			{
957: 				"name": "binary_as_string",
958: 				"default": "False",
959: 				"type": "bool"
960: 			}
961: 		],
962: 		"kwargs": [
963: 			{
964: 				"name": "file_row_number",
965: 				"default": "False",
966: 				"type": "bool"
967: 			},
968: 			{
969: 				"name": "filename",
970: 				"default": "False",
971: 				"type": "bool"
972: 			},
973: 			{
974: 				"name": "hive_partitioning",
975: 				"default": "False",
976: 				"type": "bool"
977: 			},
978: 			{
979: 				"name": "union_by_name",
980: 				"default": "False",
981: 				"type": "bool"
982: 			},
983: 			{
984: 				"name": "compression",
985: 				"default": "None",
986: 				"type": "str"
987: 			}
988: 		],
989: 		"return": "DuckDBPyRelation"
990: 	},
991: 	{
992: 		"name": "get_table_names",
993: 		"function": "GetTableNames",
994: 		"docs": "Extract the required table names from a query",
995: 		"args": [
996: 			{
997: 				"name": "query",
998: 				"type": "str"
999: 			}
1000: 		],
1001: 		"return": "Set[str]"
1002: 	},
1003: 	{
1004: 		"name": "install_extension",
1005: 		"function": "InstallExtension",
1006: 		"docs": "Install an extension by name, with an optional version and/or repository to get the extension from",
1007: 		"args": [
1008: 			{
1009: 				"name": "extension",
1010: 				"type": "str"
1011: 			}
1012: 		],
1013: 		"kwargs": [
1014: 			{
1015: 				"name": "force_install",
1016: 				"default": "False",
1017: 				"type": "bool"
1018: 			},
1019: 			{
1020: 				"name": "repository",
1021: 				"default": "None",
1022: 				"type": "Optional[str]"
1023: 			},
1024: 			{
1025: 				"name": "repository_url",
1026: 				"default": "None",
1027: 				"type": "Optional[str]"
1028: 			},
1029: 			{
1030: 				"name": "version",
1031: 				"default": "None",
1032: 				"type": "Optional[str]"
1033: 			}
1034: 		],
1035: 		"return": "None"
1036: 	},
1037: 	{
1038: 		"name": "load_extension",
1039: 		"function": "LoadExtension",
1040: 		"docs": "Load an installed extension",
1041: 		"args": [
1042: 			{
1043: 				"name": "extension",
1044: 				"type": "str"
1045: 			}
1046: 		],
1047: 		"return": "None"
1048: 	}
1049: ]
[end of tools/pythonpkg/scripts/connection_methods.json]
[start of tools/pythonpkg/src/include/duckdb_python/expression/pyexpression.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb_python/expression/pyexpression.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb_python/pybind11/pybind_wrapper.hpp"
12: #include "duckdb.hpp"
13: #include "duckdb/common/string.hpp"
14: #include "duckdb/parser/parsed_expression.hpp"
15: #include "duckdb/parser/expression/constant_expression.hpp"
16: #include "duckdb/parser/expression/columnref_expression.hpp"
17: #include "duckdb/parser/expression/function_expression.hpp"
18: #include "duckdb_python/python_conversion.hpp"
19: #include "duckdb_python/pyconnection/pyconnection.hpp"
20: #include "duckdb_python/pytype.hpp"
21: #include "duckdb/common/enums/order_type.hpp"
22: 
23: namespace duckdb {
24: 
25: struct DuckDBPyExpression : public enable_shared_from_this<DuckDBPyExpression> {
26: public:
27: 	explicit DuckDBPyExpression(unique_ptr<ParsedExpression> expr, OrderType order_type = OrderType::ORDER_DEFAULT,
28: 	                            OrderByNullType null_order = OrderByNullType::ORDER_DEFAULT);
29: 
30: public:
31: 	shared_ptr<DuckDBPyExpression> shared_from_this() {
32: 		return enable_shared_from_this<DuckDBPyExpression>::shared_from_this();
33: 	}
34: 
35: public:
36: 	static void Initialize(py::module_ &m);
37: 
38: 	string Type() const;
39: 
40: 	string ToString() const;
41: 	string GetName() const;
42: 	void Print() const;
43: 	shared_ptr<DuckDBPyExpression> Add(const DuckDBPyExpression &other);
44: 	shared_ptr<DuckDBPyExpression> Negate();
45: 	shared_ptr<DuckDBPyExpression> Subtract(const DuckDBPyExpression &other);
46: 	shared_ptr<DuckDBPyExpression> Multiply(const DuckDBPyExpression &other);
47: 	shared_ptr<DuckDBPyExpression> Division(const DuckDBPyExpression &other);
48: 	shared_ptr<DuckDBPyExpression> FloorDivision(const DuckDBPyExpression &other);
49: 	shared_ptr<DuckDBPyExpression> Modulo(const DuckDBPyExpression &other);
50: 	shared_ptr<DuckDBPyExpression> Power(const DuckDBPyExpression &other);
51: 
52: 	// Equality operations
53: 
54: 	shared_ptr<DuckDBPyExpression> Equality(const DuckDBPyExpression &other);
55: 	shared_ptr<DuckDBPyExpression> Inequality(const DuckDBPyExpression &other);
56: 	shared_ptr<DuckDBPyExpression> GreaterThan(const DuckDBPyExpression &other);
57: 	shared_ptr<DuckDBPyExpression> GreaterThanOrEqual(const DuckDBPyExpression &other);
58: 	shared_ptr<DuckDBPyExpression> LessThan(const DuckDBPyExpression &other);
59: 	shared_ptr<DuckDBPyExpression> LessThanOrEqual(const DuckDBPyExpression &other);
60: 
61: 	shared_ptr<DuckDBPyExpression> SetAlias(const string &alias) const;
62: 	shared_ptr<DuckDBPyExpression> When(const DuckDBPyExpression &condition, const DuckDBPyExpression &value);
63: 	shared_ptr<DuckDBPyExpression> Else(const DuckDBPyExpression &value);
64: 
65: 	shared_ptr<DuckDBPyExpression> Cast(const DuckDBPyType &type) const;
66: 	shared_ptr<DuckDBPyExpression> Between(const DuckDBPyExpression &lower, const DuckDBPyExpression &upper);
67: 	shared_ptr<DuckDBPyExpression> Collate(const string &collation);
68: 
69: 	// AND, OR and NOT
70: 
71: 	shared_ptr<DuckDBPyExpression> Not();
72: 	shared_ptr<DuckDBPyExpression> And(const DuckDBPyExpression &other);
73: 	shared_ptr<DuckDBPyExpression> Or(const DuckDBPyExpression &other);
74: 
75: 	// IS NULL / IS NOT NULL
76: 
77: 	shared_ptr<DuckDBPyExpression> IsNull();
78: 	shared_ptr<DuckDBPyExpression> IsNotNull();
79: 
80: 	// IN / NOT IN
81: 
82: 	shared_ptr<DuckDBPyExpression> In(const py::args &args);
83: 	shared_ptr<DuckDBPyExpression> NotIn(const py::args &args);
84: 
85: 	// Order modifiers
86: 
87: 	shared_ptr<DuckDBPyExpression> Ascending();
88: 	shared_ptr<DuckDBPyExpression> Descending();
89: 
90: 	// Null order modifiers
91: 
92: 	shared_ptr<DuckDBPyExpression> NullsFirst();
93: 	shared_ptr<DuckDBPyExpression> NullsLast();
94: 
95: public:
96: 	const ParsedExpression &GetExpression() const;
97: 	shared_ptr<DuckDBPyExpression> Copy() const;
98: 
99: public:
100: 	static shared_ptr<DuckDBPyExpression> StarExpression(py::object exclude = py::none());
101: 	static shared_ptr<DuckDBPyExpression> ColumnExpression(const py::args &column_name);
102: 	static shared_ptr<DuckDBPyExpression> DefaultExpression();
103: 	static shared_ptr<DuckDBPyExpression> ConstantExpression(const py::object &value);
104: 	static shared_ptr<DuckDBPyExpression> LambdaExpression(const py::object &lhs, const DuckDBPyExpression &rhs);
105: 	static shared_ptr<DuckDBPyExpression> CaseExpression(const DuckDBPyExpression &condition,
106: 	                                                     const DuckDBPyExpression &value);
107: 	static shared_ptr<DuckDBPyExpression> FunctionExpression(const string &function_name, const py::args &args);
108: 	static shared_ptr<DuckDBPyExpression> Coalesce(const py::args &args);
109: 
110: public:
111: 	// Internal functions (not exposed to Python)
112: 	static shared_ptr<DuckDBPyExpression> InternalFunctionExpression(const string &function_name,
113: 	                                                                 vector<unique_ptr<ParsedExpression>> children,
114: 	                                                                 bool is_operator = false);
115: 
116: 	static shared_ptr<DuckDBPyExpression> InternalUnaryOperator(ExpressionType type, const DuckDBPyExpression &arg);
117: 	static shared_ptr<DuckDBPyExpression> InternalConjunction(ExpressionType type, const DuckDBPyExpression &arg,
118: 	                                                          const DuckDBPyExpression &other);
119: 	static shared_ptr<DuckDBPyExpression> InternalConstantExpression(Value value);
120: 	static shared_ptr<DuckDBPyExpression> BinaryOperator(const string &function_name, const DuckDBPyExpression &arg_one,
121: 	                                                     const DuckDBPyExpression &arg_two);
122: 	static shared_ptr<DuckDBPyExpression> ComparisonExpression(ExpressionType type, const DuckDBPyExpression &left,
123: 	                                                           const DuckDBPyExpression &right);
124: 	static shared_ptr<DuckDBPyExpression> InternalWhen(unique_ptr<duckdb::CaseExpression> expr,
125: 	                                                   const DuckDBPyExpression &condition,
126: 	                                                   const DuckDBPyExpression &value);
127: 	void AssertCaseExpression() const;
128: 
129: private:
130: 	unique_ptr<ParsedExpression> expression;
131: 
132: public:
133: 	OrderByNullType null_order = OrderByNullType::ORDER_DEFAULT;
134: 	OrderType order_type = OrderType::ORDER_DEFAULT;
135: };
136: 
137: } // namespace duckdb
[end of tools/pythonpkg/src/include/duckdb_python/expression/pyexpression.hpp]
[start of tools/pythonpkg/src/pyexpression.cpp]
1: #include "duckdb_python/expression/pyexpression.hpp"
2: #include "duckdb/parser/expression/comparison_expression.hpp"
3: #include "duckdb/parser/expression/star_expression.hpp"
4: #include "duckdb/parser/expression/case_expression.hpp"
5: #include "duckdb/parser/expression/cast_expression.hpp"
6: #include "duckdb/parser/expression/between_expression.hpp"
7: #include "duckdb/parser/expression/conjunction_expression.hpp"
8: #include "duckdb/parser/expression/lambda_expression.hpp"
9: #include "duckdb/parser/expression/operator_expression.hpp"
10: #include "duckdb/parser/expression/default_expression.hpp"
11: #include "duckdb/parser/expression/collate_expression.hpp"
12: 
13: namespace duckdb {
14: 
15: DuckDBPyExpression::DuckDBPyExpression(unique_ptr<ParsedExpression> expr_p, OrderType order_type,
16:                                        OrderByNullType null_order)
17:     : expression(std::move(expr_p)), null_order(null_order), order_type(order_type) {
18: 	if (!expression) {
19: 		throw InternalException("DuckDBPyExpression created without an expression");
20: 	}
21: }
22: 
23: string DuckDBPyExpression::Type() const {
24: 	return ExpressionTypeToString(expression->type);
25: }
26: 
27: string DuckDBPyExpression::ToString() const {
28: 	return expression->ToString();
29: }
30: 
31: string DuckDBPyExpression::GetName() const {
32: 	return expression->GetName();
33: }
34: 
35: void DuckDBPyExpression::Print() const {
36: 	Printer::Print(expression->ToString());
37: }
38: 
39: const ParsedExpression &DuckDBPyExpression::GetExpression() const {
40: 	return *expression;
41: }
42: 
43: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::Copy() const {
44: 	auto expr = GetExpression().Copy();
45: 	return make_shared_ptr<DuckDBPyExpression>(std::move(expr), order_type, null_order);
46: }
47: 
48: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::SetAlias(const string &name) const {
49: 	auto copied_expression = GetExpression().Copy();
50: 	copied_expression->alias = name;
51: 	return make_shared_ptr<DuckDBPyExpression>(std::move(copied_expression));
52: }
53: 
54: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::Cast(const DuckDBPyType &type) const {
55: 	auto copied_expression = GetExpression().Copy();
56: 	auto case_expr = make_uniq<duckdb::CastExpression>(type.Type(), std::move(copied_expression));
57: 	return make_shared_ptr<DuckDBPyExpression>(std::move(case_expr));
58: }
59: 
60: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::Between(const DuckDBPyExpression &lower,
61:                                                            const DuckDBPyExpression &upper) {
62: 	auto copied_expression = GetExpression().Copy();
63: 	auto between_expr = make_uniq<BetweenExpression>(std::move(copied_expression), lower.GetExpression().Copy(),
64: 	                                                 upper.GetExpression().Copy());
65: 	return make_shared_ptr<DuckDBPyExpression>(std::move(between_expr));
66: }
67: 
68: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::Collate(const string &collation) {
69: 	auto copied_expression = GetExpression().Copy();
70: 	auto collation_expression = make_uniq<CollateExpression>(collation, std::move(copied_expression));
71: 	return make_shared_ptr<DuckDBPyExpression>(std::move(collation_expression));
72: }
73: 
74: // Case Expression modifiers
75: 
76: void DuckDBPyExpression::AssertCaseExpression() const {
77: 	if (expression->GetExpressionType() != ExpressionType::CASE_EXPR) {
78: 		throw py::value_error("This method can only be used on a Expression resulting from CaseExpression or When");
79: 	}
80: }
81: 
82: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::InternalWhen(unique_ptr<duckdb::CaseExpression> expr,
83:                                                                 const DuckDBPyExpression &condition,
84:                                                                 const DuckDBPyExpression &value) {
85: 	CaseCheck check;
86: 	check.when_expr = condition.GetExpression().Copy();
87: 	check.then_expr = value.GetExpression().Copy();
88: 	expr->case_checks.push_back(std::move(check));
89: 	return make_shared_ptr<DuckDBPyExpression>(std::move(expr));
90: }
91: 
92: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::When(const DuckDBPyExpression &condition,
93:                                                         const DuckDBPyExpression &value) {
94: 	AssertCaseExpression();
95: 	auto expr_p = expression->Copy();
96: 	auto expr = unique_ptr_cast<ParsedExpression, duckdb::CaseExpression>(std::move(expr_p));
97: 
98: 	return InternalWhen(std::move(expr), condition, value);
99: }
100: 
101: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::Else(const DuckDBPyExpression &value) {
102: 	AssertCaseExpression();
103: 	auto expr_p = expression->Copy();
104: 	auto expr = unique_ptr_cast<ParsedExpression, duckdb::CaseExpression>(std::move(expr_p));
105: 
106: 	expr->else_expr = value.GetExpression().Copy();
107: 	return make_shared_ptr<DuckDBPyExpression>(std::move(expr));
108: }
109: 
110: // Binary operators
111: 
112: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::Add(const DuckDBPyExpression &other) {
113: 	return DuckDBPyExpression::BinaryOperator("+", *this, other);
114: }
115: 
116: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::Subtract(const DuckDBPyExpression &other) {
117: 	return DuckDBPyExpression::BinaryOperator("-", *this, other);
118: }
119: 
120: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::Multiply(const DuckDBPyExpression &other) {
121: 	return DuckDBPyExpression::BinaryOperator("*", *this, other);
122: }
123: 
124: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::Division(const DuckDBPyExpression &other) {
125: 	return DuckDBPyExpression::BinaryOperator("/", *this, other);
126: }
127: 
128: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::FloorDivision(const DuckDBPyExpression &other) {
129: 	return DuckDBPyExpression::BinaryOperator("//", *this, other);
130: }
131: 
132: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::Modulo(const DuckDBPyExpression &other) {
133: 	return DuckDBPyExpression::BinaryOperator("%", *this, other);
134: }
135: 
136: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::Power(const DuckDBPyExpression &other) {
137: 	return DuckDBPyExpression::BinaryOperator("**", *this, other);
138: }
139: 
140: // Comparison expressions
141: 
142: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::Equality(const DuckDBPyExpression &other) {
143: 	return ComparisonExpression(ExpressionType::COMPARE_EQUAL, *this, other);
144: }
145: 
146: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::Inequality(const DuckDBPyExpression &other) {
147: 	return ComparisonExpression(ExpressionType::COMPARE_NOTEQUAL, *this, other);
148: }
149: 
150: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::GreaterThan(const DuckDBPyExpression &other) {
151: 	return ComparisonExpression(ExpressionType::COMPARE_GREATERTHAN, *this, other);
152: }
153: 
154: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::GreaterThanOrEqual(const DuckDBPyExpression &other) {
155: 	return ComparisonExpression(ExpressionType::COMPARE_GREATERTHANOREQUALTO, *this, other);
156: }
157: 
158: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::LessThan(const DuckDBPyExpression &other) {
159: 	return ComparisonExpression(ExpressionType::COMPARE_LESSTHAN, *this, other);
160: }
161: 
162: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::LessThanOrEqual(const DuckDBPyExpression &other) {
163: 	return ComparisonExpression(ExpressionType::COMPARE_LESSTHANOREQUALTO, *this, other);
164: }
165: 
166: // AND, OR and NOT
167: 
168: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::Not() {
169: 	return DuckDBPyExpression::InternalUnaryOperator(ExpressionType::OPERATOR_NOT, *this);
170: }
171: 
172: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::And(const DuckDBPyExpression &other) {
173: 	return DuckDBPyExpression::InternalConjunction(ExpressionType::CONJUNCTION_AND, *this, other);
174: }
175: 
176: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::Or(const DuckDBPyExpression &other) {
177: 	return DuckDBPyExpression::InternalConjunction(ExpressionType::CONJUNCTION_OR, *this, other);
178: }
179: 
180: // NULL
181: 
182: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::IsNull() {
183: 	return DuckDBPyExpression::InternalUnaryOperator(ExpressionType::OPERATOR_IS_NULL, *this);
184: }
185: 
186: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::IsNotNull() {
187: 	return DuckDBPyExpression::InternalUnaryOperator(ExpressionType::OPERATOR_IS_NOT_NULL, *this);
188: }
189: 
190: // IN
191: 
192: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::In(const py::args &args) {
193: 	vector<unique_ptr<ParsedExpression>> expressions;
194: 	expressions.reserve(args.size() + 1);
195: 	expressions.push_back(GetExpression().Copy());
196: 
197: 	for (auto arg : args) {
198: 		shared_ptr<DuckDBPyExpression> py_expr;
199: 		if (!py::try_cast<shared_ptr<DuckDBPyExpression>>(arg, py_expr)) {
200: 			throw InvalidInputException("Please provide arguments of type Expression!");
201: 		}
202: 		auto expr = py_expr->GetExpression().Copy();
203: 		expressions.push_back(std::move(expr));
204: 	}
205: 	auto operator_expr = make_uniq<OperatorExpression>(ExpressionType::COMPARE_IN, std::move(expressions));
206: 	return make_shared_ptr<DuckDBPyExpression>(std::move(operator_expr));
207: }
208: 
209: // COALESCE
210: 
211: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::Coalesce(const py::args &args) {
212: 	vector<unique_ptr<ParsedExpression>> expressions;
213: 	expressions.reserve(args.size());
214: 
215: 	for (auto arg : args) {
216: 		shared_ptr<DuckDBPyExpression> py_expr;
217: 		if (!py::try_cast<shared_ptr<DuckDBPyExpression>>(arg, py_expr)) {
218: 			throw InvalidInputException("Please provide arguments of type Expression!");
219: 		}
220: 		auto expr = py_expr->GetExpression().Copy();
221: 		expressions.push_back(std::move(expr));
222: 	}
223: 	if (expressions.empty()) {
224: 		throw InvalidInputException("Please provide at least one argument");
225: 	}
226: 	auto operator_expr = make_uniq<OperatorExpression>(ExpressionType::OPERATOR_COALESCE, std::move(expressions));
227: 	return make_shared_ptr<DuckDBPyExpression>(std::move(operator_expr));
228: }
229: 
230: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::NotIn(const py::args &args) {
231: 	auto in_expr = In(args);
232: 	return in_expr->Not();
233: }
234: 
235: // Order modifiers
236: 
237: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::Ascending() {
238: 	auto py_expr = Copy();
239: 	py_expr->order_type = OrderType::ASCENDING;
240: 	return py_expr;
241: }
242: 
243: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::Descending() {
244: 	auto py_expr = Copy();
245: 	py_expr->order_type = OrderType::DESCENDING;
246: 	return py_expr;
247: }
248: 
249: // Null order modifiers
250: 
251: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::NullsFirst() {
252: 	auto py_expr = Copy();
253: 	py_expr->null_order = OrderByNullType::NULLS_FIRST;
254: 	return py_expr;
255: }
256: 
257: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::NullsLast() {
258: 	auto py_expr = Copy();
259: 	py_expr->null_order = OrderByNullType::NULLS_LAST;
260: 	return py_expr;
261: }
262: 
263: // Unary operators
264: 
265: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::Negate() {
266: 	vector<unique_ptr<ParsedExpression>> children;
267: 	children.push_back(GetExpression().Copy());
268: 	return DuckDBPyExpression::InternalFunctionExpression("-", std::move(children), true);
269: }
270: 
271: // Static creation methods
272: 
273: static void PopulateExcludeList(qualified_column_set_t &exclude, py::object list_p) {
274: 	if (py::none().is(list_p)) {
275: 		list_p = py::list();
276: 	}
277: 	py::list list = py::cast<py::list>(list_p);
278: 	for (auto item : list) {
279: 		if (py::isinstance<py::str>(item)) {
280: 			exclude.insert(QualifiedColumnName(std::string(py::str(item))));
281: 			continue;
282: 		}
283: 		shared_ptr<DuckDBPyExpression> expr;
284: 		if (!py::try_cast(item, expr)) {
285: 			throw py::value_error("Items in the exclude list should either be 'str' or Expression");
286: 		}
287: 		if (expr->GetExpression().GetExpressionType() != ExpressionType::COLUMN_REF) {
288: 			throw py::value_error("Only ColumnExpressions are accepted Expression types here");
289: 		}
290: 		auto &column = expr->GetExpression().Cast<ColumnRefExpression>();
291: 		exclude.insert(QualifiedColumnName(column.GetColumnName()));
292: 	}
293: }
294: 
295: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::StarExpression(py::object exclude_list) {
296: 	case_insensitive_set_t exclude;
297: 	auto star = make_uniq<duckdb::StarExpression>();
298: 	PopulateExcludeList(star->exclude_list, std::move(exclude_list));
299: 	return make_shared_ptr<DuckDBPyExpression>(std::move(star));
300: }
301: 
302: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::ColumnExpression(const py::args &names) {
303: 	vector<string> column_names;
304: 	if (names.size() == 1) {
305: 		string column_name = std::string(py::str(names[0]));
306: 		if (column_name == "*") {
307: 			return StarExpression();
308: 		}
309: 
310: 		auto qualified_name = QualifiedName::Parse(column_name);
311: 		if (!qualified_name.catalog.empty()) {
312: 			column_names.push_back(qualified_name.catalog);
313: 		}
314: 		if (!qualified_name.schema.empty()) {
315: 			column_names.push_back(qualified_name.schema);
316: 		}
317: 		column_names.push_back(qualified_name.name);
318: 	} else {
319: 		for (auto &part : names) {
320: 			column_names.push_back(std::string(py::str(part)));
321: 		}
322: 	}
323: 	auto column_ref = make_uniq<duckdb::ColumnRefExpression>(std::move(column_names));
324: 	return make_shared_ptr<DuckDBPyExpression>(std::move(column_ref));
325: }
326: 
327: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::DefaultExpression() {
328: 	return make_shared_ptr<DuckDBPyExpression>(make_uniq<duckdb::DefaultExpression>());
329: }
330: 
331: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::ConstantExpression(const py::object &value) {
332: 	auto val = TransformPythonValue(value);
333: 	return InternalConstantExpression(std::move(val));
334: }
335: 
336: static py::args CreateArgsFromItem(py::handle item) {
337: 	if (py::isinstance<py::tuple>(item)) {
338: 		return py::cast<py::args>(item);
339: 	} else {
340: 		return py::make_tuple(item);
341: 	}
342: }
343: 
344: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::LambdaExpression(const py::object &lhs_p,
345:                                                                     const DuckDBPyExpression &rhs) {
346: 	unique_ptr<ParsedExpression> lhs;
347: 	if (py::isinstance<py::tuple>(lhs_p)) {
348: 		// LambdaExpression(lhs=(<item>, <item>, <item>))
349: 		auto lhs_tuple = py::cast<py::tuple>(lhs_p);
350: 		vector<unique_ptr<ParsedExpression>> children;
351: 		for (auto &item : lhs_tuple) {
352: 			unique_ptr<ParsedExpression> column;
353: 			if (py::isinstance<DuckDBPyExpression>(item)) {
354: 				// 'item' is already an Expression, check its type and use it
355: 				auto column_expr = py::cast<shared_ptr<DuckDBPyExpression>>(item);
356: 				if (column_expr->GetExpression().GetExpressionType() != ExpressionType::COLUMN_REF) {
357: 					throw py::value_error("'lhs' was provided as a tuple of columns, but one of the columns is not of "
358: 					                      "type ColumnExpression");
359: 				}
360: 				column = column_expr->GetExpression().Copy();
361: 			} else {
362: 				// 'item' is a tuple[str, ...] or str, construct a ColumnExpression from it
363: 				auto args = CreateArgsFromItem(item);
364: 				auto column_expr = ColumnExpression(args);
365: 				if (column_expr->GetExpression().GetExpressionType() != ExpressionType::COLUMN_REF) {
366: 					throw py::value_error("'lhs' was provided as a tuple of columns, but one of the columns is not of "
367: 					                      "type ColumnExpression");
368: 				}
369: 				column = std::move(column_expr->expression);
370: 			}
371: 			children.push_back(std::move(column));
372: 		}
373: 		auto row_function = InternalFunctionExpression("row", std::move(children), false);
374: 		lhs = std::move(row_function->expression);
375: 	} else if (py::isinstance<py::str>(lhs_p)) {
376: 		// LambdaExpression(lhs=str)
377: 		auto args = CreateArgsFromItem(lhs_p);
378: 		auto column_expr = ColumnExpression(args);
379: 		if (column_expr->GetExpression().GetExpressionType() != ExpressionType::COLUMN_REF) {
380: 			throw py::value_error("'lhs' should be a valid ColumnExpression (or be used to create one)");
381: 		}
382: 		lhs = std::move(column_expr->expression);
383: 	} else if (py::isinstance<DuckDBPyExpression>(lhs_p)) {
384: 		// LambdaExpression(lhs=Expression)
385: 		// 'lhs_p' is already an Expression, check its type and use it
386: 		auto column_expr = py::cast<shared_ptr<DuckDBPyExpression>>(lhs_p);
387: 		if (column_expr->GetExpression().GetExpressionType() != ExpressionType::COLUMN_REF) {
388: 			throw py::value_error("'lhs' was an Expression, but is not of type ColumnExpression");
389: 		}
390: 		lhs = column_expr->GetExpression().Copy();
391: 	} else {
392: 		throw py::value_error("Please provide 'lhs' as either a tuple containing strings, or a single string");
393: 	}
394: 	auto lambda_expression = make_uniq<duckdb::LambdaExpression>(std::move(lhs), rhs.GetExpression().Copy());
395: 	return make_shared_ptr<DuckDBPyExpression>(std::move(lambda_expression));
396: }
397: 
398: // Private methods
399: 
400: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::BinaryOperator(const string &function_name,
401:                                                                   const DuckDBPyExpression &arg_one,
402:                                                                   const DuckDBPyExpression &arg_two) {
403: 	vector<unique_ptr<ParsedExpression>> children;
404: 
405: 	children.push_back(arg_one.GetExpression().Copy());
406: 	children.push_back(arg_two.GetExpression().Copy());
407: 	return InternalFunctionExpression(function_name, std::move(children), true);
408: }
409: 
410: shared_ptr<DuckDBPyExpression>
411: DuckDBPyExpression::InternalFunctionExpression(const string &function_name,
412:                                                vector<unique_ptr<ParsedExpression>> children, bool is_operator) {
413: 	auto function_expression =
414: 	    make_uniq<duckdb::FunctionExpression>(function_name, std::move(children), nullptr, nullptr, false, is_operator);
415: 	return make_shared_ptr<DuckDBPyExpression>(std::move(function_expression));
416: }
417: 
418: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::InternalUnaryOperator(ExpressionType type,
419:                                                                          const DuckDBPyExpression &arg) {
420: 	auto expr = arg.GetExpression().Copy();
421: 	auto operator_expression = make_uniq<OperatorExpression>(type, std::move(expr));
422: 	return make_shared_ptr<DuckDBPyExpression>(std::move(operator_expression));
423: }
424: 
425: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::InternalConjunction(ExpressionType type,
426:                                                                        const DuckDBPyExpression &arg,
427:                                                                        const DuckDBPyExpression &other) {
428: 	vector<unique_ptr<ParsedExpression>> children;
429: 	children.reserve(2);
430: 	children.push_back(arg.GetExpression().Copy());
431: 	children.push_back(other.GetExpression().Copy());
432: 
433: 	auto operator_expression = make_uniq<ConjunctionExpression>(type, std::move(children));
434: 	return make_shared_ptr<DuckDBPyExpression>(std::move(operator_expression));
435: }
436: 
437: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::InternalConstantExpression(Value val) {
438: 	return make_shared_ptr<DuckDBPyExpression>(make_uniq<duckdb::ConstantExpression>(std::move(val)));
439: }
440: 
441: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::ComparisonExpression(ExpressionType type,
442:                                                                         const DuckDBPyExpression &left_p,
443:                                                                         const DuckDBPyExpression &right_p) {
444: 	auto left = left_p.GetExpression().Copy();
445: 	auto right = right_p.GetExpression().Copy();
446: 	return make_shared_ptr<DuckDBPyExpression>(
447: 	    make_uniq<duckdb::ComparisonExpression>(type, std::move(left), std::move(right)));
448: }
449: 
450: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::CaseExpression(const DuckDBPyExpression &condition,
451:                                                                   const DuckDBPyExpression &value) {
452: 	auto expr = make_uniq<duckdb::CaseExpression>();
453: 	auto case_expr = InternalWhen(std::move(expr), condition, value);
454: 
455: 	// Add NULL as default Else expression
456: 	auto &internal_expression = reinterpret_cast<duckdb::CaseExpression &>(*case_expr->expression);
457: 	internal_expression.else_expr = make_uniq<duckdb::ConstantExpression>(Value(LogicalTypeId::SQLNULL));
458: 	return case_expr;
459: }
460: 
461: shared_ptr<DuckDBPyExpression> DuckDBPyExpression::FunctionExpression(const string &function_name,
462:                                                                       const py::args &args) {
463: 	vector<unique_ptr<ParsedExpression>> expressions;
464: 	for (auto arg : args) {
465: 		shared_ptr<DuckDBPyExpression> py_expr;
466: 		if (!py::try_cast<shared_ptr<DuckDBPyExpression>>(arg, py_expr)) {
467: 			string actual_type = py::str(arg.get_type());
468: 			throw InvalidInputException("Expected argument of type Expression, received '%s' instead", actual_type);
469: 		}
470: 		auto expr = py_expr->GetExpression().Copy();
471: 		expressions.push_back(std::move(expr));
472: 	}
473: 	return InternalFunctionExpression(function_name, std::move(expressions));
474: }
475: 
476: } // namespace duckdb
[end of tools/pythonpkg/src/pyexpression.cpp]
[start of tools/pythonpkg/src/pyexpression/initialize.cpp]
1: #include "duckdb_python/pybind11/pybind_wrapper.hpp"
2: #include "duckdb_python/expression/pyexpression.hpp"
3: #include "duckdb/common/helper.hpp"
4: #include "duckdb/common/vector.hpp"
5: #include "duckdb_python/python_conversion.hpp"
6: 
7: namespace duckdb {
8: 
9: void InitializeStaticMethods(py::module_ &m) {
10: 	const char *docs;
11: 
12: 	// Constant Expression
13: 	docs = "Create a constant expression from the provided value";
14: 	m.def("ConstantExpression", &DuckDBPyExpression::ConstantExpression, py::arg("value"), docs);
15: 
16: 	// ColumnRef Expression
17: 	docs = "Create a column reference from the provided column name";
18: 	m.def("ColumnExpression", &DuckDBPyExpression::ColumnExpression, docs);
19: 
20: 	// Default Expression
21: 	docs = "";
22: 	m.def("DefaultExpression", &DuckDBPyExpression::DefaultExpression, docs);
23: 
24: 	// Case Expression
25: 	docs = "";
26: 	m.def("CaseExpression", &DuckDBPyExpression::CaseExpression, py::arg("condition"), py::arg("value"), docs);
27: 
28: 	// Star Expression
29: 	docs = "";
30: 	m.def("StarExpression", &DuckDBPyExpression::StarExpression, py::kw_only(), py::arg("exclude") = py::none(), docs);
31: 	m.def(
32: 	    "StarExpression", []() { return DuckDBPyExpression::StarExpression(); }, docs);
33: 
34: 	// Function Expression
35: 	docs = "";
36: 	m.def("FunctionExpression", &DuckDBPyExpression::FunctionExpression, py::arg("function_name"), docs);
37: 
38: 	// Coalesce Operator
39: 	docs = "";
40: 	m.def("CoalesceOperator", &DuckDBPyExpression::Coalesce, docs);
41: 
42: 	// Lambda Expression
43: 	docs = "";
44: 	m.def("LambdaExpression", &DuckDBPyExpression::LambdaExpression, py::arg("lhs"), py::arg("rhs"), docs);
45: }
46: 
47: static void InitializeDunderMethods(py::class_<DuckDBPyExpression, shared_ptr<DuckDBPyExpression>> &m) {
48: 	const char *docs;
49: 
50: 	docs = R"(
51: 		Add expr to self
52: 
53: 		Parameters:
54: 			expr: The expression to add together with
55: 
56: 		Returns:
57: 			FunctionExpression: self '+' expr
58: 	)";
59: 
60: 	m.def("__add__", &DuckDBPyExpression::Add, py::arg("expr"), docs);
61: 	m.def("__radd__", &DuckDBPyExpression::Add, py::arg("expr"), docs);
62: 
63: 	docs = R"(
64: 		Negate the expression.
65: 
66: 		Returns:
67: 			FunctionExpression: -self
68: 	)";
69: 	m.def("__neg__", &DuckDBPyExpression::Negate, docs);
70: 
71: 	docs = R"(
72: 		Subtract expr from self
73: 
74: 		Parameters:
75: 			expr: The expression to subtract from
76: 
77: 		Returns:
78: 			FunctionExpression: self '-' expr
79: 	)";
80: 	m.def("__sub__", &DuckDBPyExpression::Subtract, docs);
81: 	m.def("__rsub__", &DuckDBPyExpression::Subtract, docs);
82: 
83: 	docs = R"(
84: 		Multiply self by expr
85: 
86: 		Parameters:
87: 			expr: The expression to multiply by
88: 
89: 		Returns:
90: 			FunctionExpression: self '*' expr
91: 	)";
92: 	m.def("__mul__", &DuckDBPyExpression::Multiply, docs);
93: 	m.def("__rmul__", &DuckDBPyExpression::Multiply, docs);
94: 
95: 	docs = R"(
96: 		Divide self by expr
97: 
98: 		Parameters:
99: 			expr: The expression to divide by
100: 
101: 		Returns:
102: 			FunctionExpression: self '/' expr
103: 	)";
104: 	m.def("__div__", &DuckDBPyExpression::Division, docs);
105: 	m.def("__rdiv__", &DuckDBPyExpression::Division, docs);
106: 
107: 	m.def("__truediv__", &DuckDBPyExpression::Division, docs);
108: 	m.def("__rtruediv__", &DuckDBPyExpression::Division, docs);
109: 
110: 	docs = R"(
111: 		(Floor) Divide self by expr
112: 
113: 		Parameters:
114: 			expr: The expression to (floor) divide by
115: 
116: 		Returns:
117: 			FunctionExpression: self '//' expr
118: 	)";
119: 	m.def("__floordiv__", &DuckDBPyExpression::FloorDivision, docs);
120: 	m.def("__rfloordiv__", &DuckDBPyExpression::FloorDivision, docs);
121: 
122: 	docs = R"(
123: 		Modulo self by expr
124: 
125: 		Parameters:
126: 			expr: The expression to modulo by
127: 
128: 		Returns:
129: 			FunctionExpression: self '%' expr
130: 	)";
131: 	m.def("__mod__", &DuckDBPyExpression::Modulo, docs);
132: 	m.def("__rmod__", &DuckDBPyExpression::Modulo, docs);
133: 
134: 	docs = R"(
135: 		Power self by expr
136: 
137: 		Parameters:
138: 			expr: The expression to power by
139: 
140: 		Returns:
141: 			FunctionExpression: self '**' expr
142: 	)";
143: 	m.def("__pow__", &DuckDBPyExpression::Power, docs);
144: 	m.def("__rpow__", &DuckDBPyExpression::Power, docs);
145: 
146: 	docs = R"(
147: 		Create an equality expression between two expressions
148: 
149: 		Parameters:
150: 			expr: The expression to check equality with
151: 
152: 		Returns:
153: 			FunctionExpression: self '=' expr
154: 	)";
155: 	m.def("__eq__", &DuckDBPyExpression::Equality, docs);
156: 
157: 	docs = R"(
158: 		Create an inequality expression between two expressions
159: 
160: 		Parameters:
161: 			expr: The expression to check inequality with
162: 
163: 		Returns:
164: 			FunctionExpression: self '!=' expr
165: 	)";
166: 	m.def("__ne__", &DuckDBPyExpression::Inequality, docs);
167: 
168: 	docs = R"(
169: 		Create a greater than expression between two expressions
170: 
171: 		Parameters:
172: 			expr: The expression to check
173: 
174: 		Returns:
175: 			FunctionExpression: self '>' expr
176: 	)";
177: 	m.def("__gt__", &DuckDBPyExpression::GreaterThan, docs);
178: 
179: 	docs = R"(
180: 		Create a greater than or equal expression between two expressions
181: 
182: 		Parameters:
183: 			expr: The expression to check
184: 
185: 		Returns:
186: 			FunctionExpression: self '>=' expr
187: 	)";
188: 	m.def("__ge__", &DuckDBPyExpression::GreaterThanOrEqual, docs);
189: 
190: 	docs = R"(
191: 		Create a less than expression between two expressions
192: 
193: 		Parameters:
194: 			expr: The expression to check
195: 
196: 		Returns:
197: 			FunctionExpression: self '<' expr
198: 	)";
199: 	m.def("__lt__", &DuckDBPyExpression::LessThan, docs);
200: 
201: 	docs = R"(
202: 		Create a less than or equal expression between two expressions
203: 
204: 		Parameters:
205: 			expr: The expression to check
206: 
207: 		Returns:
208: 			FunctionExpression: self '<=' expr
209: 	)";
210: 	m.def("__le__", &DuckDBPyExpression::LessThanOrEqual, docs);
211: 
212: 	// AND, NOT and OR
213: 
214: 	docs = R"(
215: 		Binary-and self together with expr
216: 
217: 		Parameters:
218: 			expr: The expression to AND together with self
219: 
220: 		Returns:
221: 			FunctionExpression: self '&' expr
222: 	)";
223: 	m.def("__and__", &DuckDBPyExpression::And, docs);
224: 
225: 	docs = R"(
226: 		Binary-or self together with expr
227: 
228: 		Parameters:
229: 			expr: The expression to OR together with self
230: 
231: 		Returns:
232: 			FunctionExpression: self '|' expr
233: 	)";
234: 	m.def("__or__", &DuckDBPyExpression::Or, docs);
235: 
236: 	docs = R"(
237: 		Create a binary-not expression from self
238: 
239: 		Returns:
240: 			FunctionExpression: ~self
241: 	)";
242: 	m.def("__invert__", &DuckDBPyExpression::Not, docs);
243: 
244: 	docs = R"(
245: 		Binary-and self together with expr
246: 
247: 		Parameters:
248: 			expr: The expression to AND together with self
249: 
250: 		Returns:
251: 			FunctionExpression: expr '&' self
252: 	)";
253: 	m.def("__rand__", &DuckDBPyExpression::And, docs);
254: 
255: 	docs = R"(
256: 		Binary-or self together with expr
257: 
258: 		Parameters:
259: 			expr: The expression to OR together with self
260: 
261: 		Returns:
262: 			FunctionExpression: expr '|' self
263: 	)";
264: 	m.def("__ror__", &DuckDBPyExpression::Or, docs);
265: }
266: 
267: static void InitializeImplicitConversion(py::class_<DuckDBPyExpression, shared_ptr<DuckDBPyExpression>> &m) {
268: 	m.def(py::init<>([](const string &name) {
269: 		auto names = py::make_tuple(py::str(name));
270: 		return DuckDBPyExpression::ColumnExpression(names);
271: 	}));
272: 	m.def(py::init<>([](const py::object &obj) {
273: 		auto val = TransformPythonValue(obj);
274: 		return DuckDBPyExpression::InternalConstantExpression(std::move(val));
275: 	}));
276: 	py::implicitly_convertible<py::str, DuckDBPyExpression>();
277: 	py::implicitly_convertible<py::object, DuckDBPyExpression>();
278: }
279: 
280: void DuckDBPyExpression::Initialize(py::module_ &m) {
281: 	auto expression =
282: 	    py::class_<DuckDBPyExpression, shared_ptr<DuckDBPyExpression>>(m, "Expression", py::module_local());
283: 
284: 	InitializeStaticMethods(m);
285: 	InitializeDunderMethods(expression);
286: 	InitializeImplicitConversion(expression);
287: 
288: 	const char *docs;
289: 
290: 	docs = R"(
291: 		Print the stringified version of the expression.
292: 	)";
293: 	expression.def("show", &DuckDBPyExpression::Print, docs);
294: 
295: 	docs = R"(
296: 		Set the order by modifier to ASCENDING.
297: 	)";
298: 	expression.def("asc", &DuckDBPyExpression::Ascending, docs);
299: 
300: 	docs = R"(
301: 		Set the order by modifier to DESCENDING.
302: 	)";
303: 	expression.def("desc", &DuckDBPyExpression::Descending, docs);
304: 
305: 	docs = R"(
306: 		Set the NULL order by modifier to NULLS FIRST.
307: 	)";
308: 	expression.def("nulls_first", &DuckDBPyExpression::NullsFirst, docs);
309: 
310: 	docs = R"(
311: 		Set the NULL order by modifier to NULLS LAST.
312: 	)";
313: 	expression.def("nulls_last", &DuckDBPyExpression::NullsLast, docs);
314: 
315: 	docs = R"(
316: 		Create a binary IS NULL expression from self
317: 
318: 		Returns:
319: 			DuckDBPyExpression: self IS NULL
320: 	)";
321: 	expression.def("isnull", &DuckDBPyExpression::IsNull, docs);
322: 
323: 	docs = R"(
324: 		Create a binary IS NOT NULL expression from self
325: 
326: 		Returns:
327: 			DuckDBPyExpression: self IS NOT NULL
328: 	)";
329: 	expression.def("isnotnull", &DuckDBPyExpression::IsNotNull, docs);
330: 
331: 	docs = R"(
332: 		Return an IN expression comparing self to the input arguments.
333: 
334: 		Returns:
335: 			DuckDBPyExpression: The compare IN expression
336: 	)";
337: 	expression.def("isin", &DuckDBPyExpression::In, docs);
338: 
339: 	docs = R"(
340: 		Return a NOT IN expression comparing self to the input arguments.
341: 
342: 		Returns:
343: 			DuckDBPyExpression: The compare NOT IN expression
344: 	)";
345: 	expression.def("isnotin", &DuckDBPyExpression::NotIn, docs);
346: 
347: 	docs = R"(
348: 		Return the stringified version of the expression.
349: 
350: 		Returns:
351: 			str: The string representation.
352: 	)";
353: 	expression.def("__repr__", &DuckDBPyExpression::ToString, docs);
354: 
355: 	expression.def("get_name", &DuckDBPyExpression::GetName, docs);
356: 
357: 	docs = R"(
358: 		Create a copy of this expression with the given alias.
359: 
360: 		Parameters:
361: 			name: The alias to use for the expression, this will affect how it can be referenced.
362: 
363: 		Returns:
364: 			Expression: self with an alias.
365: 	)";
366: 	expression.def("alias", &DuckDBPyExpression::SetAlias, docs);
367: 
368: 	docs = R"(
369: 		Add an additional WHEN <condition> THEN <value> clause to the CaseExpression.
370: 
371: 		Parameters:
372: 			condition: The condition that must be met.
373: 			value: The value to use if the condition is met.
374: 
375: 		Returns:
376: 			CaseExpression: self with an additional WHEN clause.
377: 	)";
378: 	expression.def("when", &DuckDBPyExpression::When, py::arg("condition"), py::arg("value"), docs);
379: 
380: 	docs = R"(
381: 		Add an ELSE <value> clause to the CaseExpression.
382: 
383: 		Parameters:
384: 			value: The value to use if none of the WHEN conditions are met.
385: 
386: 		Returns:
387: 			CaseExpression: self with an ELSE clause.
388: 	)";
389: 	expression.def("otherwise", &DuckDBPyExpression::Else, py::arg("value"), docs);
390: 
391: 	docs = R"(
392: 		Create a CastExpression to type from self
393: 
394: 		Parameters:
395: 			type: The type to cast to
396: 
397: 		Returns:
398: 			CastExpression: self::type
399: 	)";
400: 	expression.def("cast", &DuckDBPyExpression::Cast, py::arg("type"), docs);
401: 
402: 	docs = "";
403: 	expression.def("between", &DuckDBPyExpression::Between, py::arg("lower"), py::arg("upper"), docs);
404: 
405: 	docs = "";
406: 	expression.def("collate", &DuckDBPyExpression::Collate, py::arg("collation"), docs);
407: }
408: 
409: } // namespace duckdb
[end of tools/pythonpkg/src/pyexpression/initialize.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: