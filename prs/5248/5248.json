{
  "repo": "duckdb/duckdb",
  "pull_number": 5248,
  "instance_id": "duckdb__duckdb-5248",
  "issue_numbers": [
    "4975"
  ],
  "base_commit": "be1bd1704f73afcfef9bfbc794f98eb11e36002b",
  "patch": "diff --git a/.github/workflows/LinuxRelease.yml b/.github/workflows/LinuxRelease.yml\nindex b12b2f9ca1c5..52f67178ed90 100644\n--- a/.github/workflows/LinuxRelease.yml\n+++ b/.github/workflows/LinuxRelease.yml\n@@ -552,6 +552,8 @@ jobs:\n     env:\n       BUILD_VISUALIZER: 1\n       BUILD_HTTPFS: 1\n+      BUILD_TPCH: 1\n+      BUILD_TPCDS: 1\n       S3_TEST_SERVER_AVAILABLE: 1\n       GEN: ninja\n \ndiff --git a/extension/httpfs/s3fs.cpp b/extension/httpfs/s3fs.cpp\nindex 046287d923e3..20376c9989ef 100644\n--- a/extension/httpfs/s3fs.cpp\n+++ b/extension/httpfs/s3fs.cpp\n@@ -912,7 +912,7 @@ string AWSListObjectV2::Request(string &path, HTTPParams &http_params, S3AuthPar\n \n \tstring req_params = \"\";\n \tif (!continuation_token.empty()) {\n-\t\treq_params += \"continuation-token=\" + S3FileSystem::UrlEncode(continuation_token);\n+\t\treq_params += \"continuation-token=\" + S3FileSystem::UrlEncode(continuation_token, true);\n \t\treq_params += \"&\";\n \t}\n \treq_params += \"encoding-type=url&list-type=2\";\n",
  "test_patch": "diff --git a/test/sql/copy/s3/parquet_s3_tpcds.test_slow b/test/sql/copy/s3/parquet_s3_tpcds.test_slow\nindex 31f93f99caf1..fce4e97e75eb 100644\n--- a/test/sql/copy/s3/parquet_s3_tpcds.test_slow\n+++ b/test/sql/copy/s3/parquet_s3_tpcds.test_slow\n@@ -14,7 +14,7 @@ require-env S3_TEST_SERVER_AVAILABLE 1\n set ignore_error_messages\n \n statement ok\n-SET s3_secret_access_key='S3RVER';SET s3_access_key_id='S3RVER';SET s3_region='eu-west-1'; SET s3_endpoint='s3.s3rver-endpoint.com:4923';SET s3_use_ssl=false;\n+SET s3_secret_access_key='minio_duckdb_user_password';SET s3_access_key_id='minio_duckdb_user';SET s3_region='eu-west-1'; SET s3_endpoint='duckdb-minio.com:9000';SET s3_use_ssl=false;\n \n # answers are generated from postgres\n # hence check with NULLS LAST flag\ndiff --git a/test/sql/copy/s3/parquet_s3_tpch.test_slow b/test/sql/copy/s3/parquet_s3_tpch.test_slow\nindex c83368027e4e..1a87fa21a5d5 100644\n--- a/test/sql/copy/s3/parquet_s3_tpch.test_slow\n+++ b/test/sql/copy/s3/parquet_s3_tpch.test_slow\n@@ -14,7 +14,7 @@ require-env S3_TEST_SERVER_AVAILABLE 1\n set ignore_error_messages\n \n statement ok\n-SET s3_secret_access_key='S3RVER';SET s3_access_key_id='S3RVER';SET s3_region='eu-west-1'; SET s3_endpoint='s3.s3rver-endpoint.com:4923';SET s3_use_ssl=false;\n+SET s3_secret_access_key='minio_duckdb_user_password';SET s3_access_key_id='minio_duckdb_user';SET s3_region='eu-west-1'; SET s3_endpoint='duckdb-minio.com:9000';SET s3_use_ssl=false;\n \n # Copy files to S3 before beginning tests\n statement ok\n",
  "problem_statement": "HTTP 403 (Access Denied) when using read_parquet() with glob matching >1000 S3 objects\n### What happens?\r\n\r\nDuckDB throws an error when I do a glob-style query that matches over 1000 S3 objects.\r\n\r\nHere's the query I'm using:\r\n\r\n```\r\nSELECT *\r\nFROM read_parquet('s3://prod-ingestion-github-repositories/2022-09_*');\r\n```\r\n\r\n\r\nAnd here is the error message I'm getting:\r\n\r\n```\r\nInvalid Error: HTTP GET error on 'https://prod-ingestion-github-repositories.s3.amazonaws.com/?continuation-token=1ZU0D5N%2Bv2Dm9hhTi2JQvSAphFHiPRSnPegvosjQD3klo92WJYFwF8yrnJdtFhDHfsIka/QOCYNdQQTHC6f/3H8mudjEJscJydhNUTBJevcvcZlqgunsNdyhSHXsAZLbc&encoding-type=url&list-type=2&prefix=2022-09_' (HTTP 403)\r\n```\r\n\r\nI confirmed that the same query works if the glob pattern is modified to match fewer objects. E.g., the following returns the expected result:\r\n\r\n```\r\nSELECT *\r\nFROM read_parquet('s3://prod-ingestion-github-repositories/2022-09_06-*');\r\n```\r\n\r\n\r\n### To Reproduce\r\n\r\nTough for me to provide the exact dataset, but the general gist is:\r\n\r\n1) Create N parquet files in S3 with the same prefix, where N is some number >1000\r\n2) Query them with `read_parquet('your-prefix-*')`\r\n\r\n### OS:\r\n\r\nOSX\r\n\r\n### DuckDB Version:\r\n\r\n0.5.1\r\n\r\n### DuckDB Client:\r\n\r\nPython\r\n\r\n### Full Name:\r\n\r\nNathan Gould\r\n\r\n### Affiliation:\r\n\r\nEndeavor Labs LLC\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\n- [X] I agree\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] I agree\n",
  "hints_text": "Ran into this as well using the [NOAA GSOD](https://registry.opendata.aws/noaa-gsod/) dataset. \r\n\r\nThis is a public dataset, so you can easily repro:\r\n\r\n```sql\r\nSELECT * FROM read_csv_auto('s3://noaa-gsod-pds/2021/*.csv', header=True) LIMIT 10;\r\n```\nThis is probably the same error I am getting with this one: https://github.com/duckdb/duckdb/issues/5190. I noticed that it does _sometimes_ work... perhaps related the order in which the data is paginated?\nI've been meaning to look into this, expect to have time for this next week",
  "created_at": "2022-11-08T12:11:36Z"
}