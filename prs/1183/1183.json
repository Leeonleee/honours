{
  "repo": "duckdb/duckdb",
  "pull_number": 1183,
  "instance_id": "duckdb__duckdb-1183",
  "issue_numbers": [
    "1152"
  ],
  "base_commit": "9cbfca659942c724f21204eda58defd6689644b5",
  "patch": "diff --git a/extension/parquet/include/parquet_reader.hpp b/extension/parquet/include/parquet_reader.hpp\nindex 872da56dda9a..5eff933eca4b 100644\n--- a/extension/parquet/include/parquet_reader.hpp\n+++ b/extension/parquet/include/parquet_reader.hpp\n@@ -16,6 +16,7 @@\n \n #include \"parquet_file_metadata_cache.hpp\"\n #include \"parquet_types.h\"\n+#include \"parquet_rle_bp_decoder.hpp\"\n \n #include <exception>\n \n@@ -27,9 +28,9 @@ class FileMetaData;\n \n namespace duckdb {\n class ClientContext;\n-class RleBpDecoder;\n class ChunkCollection;\n class BaseStatistics;\n+struct TableFilterSet;\n \n struct ParquetReaderColumnData {\n \t~ParquetReaderColumnData();\n@@ -68,8 +69,12 @@ struct ParquetReaderScanState {\n \tidx_t group_offset;\n \tvector<unique_ptr<ParquetReaderColumnData>> column_data;\n \tbool finished;\n+\tTableFilterSet *filters;\n+\tSelectionVector sel;\n };\n \n+typedef nullmask_t parquet_filter_t;\n+\n class ParquetReader {\n public:\n \tParquetReader(ClientContext &context, string file_name, vector<LogicalType> expected_types,\n@@ -85,8 +90,9 @@ class ParquetReader {\n \tshared_ptr<ParquetFileMetadataCache> metadata;\n \n public:\n-\tvoid Initialize(ParquetReaderScanState &state, vector<column_t> column_ids, vector<idx_t> groups_to_read);\n-\tvoid ReadChunk(ParquetReaderScanState &state, DataChunk &output);\n+\tvoid Initialize(ParquetReaderScanState &state, vector<column_t> column_ids, vector<idx_t> groups_to_read,\n+\t                TableFilterSet *table_filters);\n+\tvoid Scan(ParquetReaderScanState &state, DataChunk &output);\n \n \tidx_t NumRows();\n \tidx_t NumRowGroups();\n@@ -97,8 +103,12 @@ class ParquetReader {\n \t                                                 const parquet::format::FileMetaData *file_meta_data);\n \n private:\n+\tvoid ScanColumn(ParquetReaderScanState &state, parquet_filter_t &filter_mask, idx_t count, idx_t out_col_idx,\n+\t                Vector &out);\n+\tbool ScanInternal(ParquetReaderScanState &state, DataChunk &output);\n+\n \tconst parquet::format::RowGroup &GetGroup(ParquetReaderScanState &state);\n-\tvoid PrepareChunkBuffer(ParquetReaderScanState &state, idx_t col_idx);\n+\tvoid PrepareRowGroupBuffer(ParquetReaderScanState &state, idx_t col_idx, LogicalType &type);\n \tbool PreparePageBuffers(ParquetReaderScanState &state, idx_t col_idx);\n \tvoid VerifyString(LogicalTypeId id, const char *str_data, idx_t str_len);\n \n@@ -107,11 +117,6 @@ class ParquetReader {\n \t\t                          \"\\\": \" + StringUtil::Format(fmt_str, params...));\n \t}\n \n-\ttemplate <class T>\n-\tvoid fill_from_dict(ParquetReaderColumnData &col_data, idx_t count, Vector &target, idx_t target_offset);\n-\ttemplate <class T>\n-\tvoid fill_from_plain(ParquetReaderColumnData &col_data, idx_t count, Vector &target, idx_t target_offset);\n-\n private:\n \tClientContext &context;\n };\ndiff --git a/extension/parquet/include/parquet_rle_bp_decoder.hpp b/extension/parquet/include/parquet_rle_bp_decoder.hpp\nnew file mode 100644\nindex 000000000000..3b09b2706766\n--- /dev/null\n+++ b/extension/parquet/include/parquet_rle_bp_decoder.hpp\n@@ -0,0 +1,137 @@\n+#pragma once\n+namespace duckdb {\n+class RleBpDecoder {\n+public:\n+\t/// Create a decoder object. buffer/buffer_len is the decoded data.\n+\t/// bit_width is the width of each value (before encoding).\n+\tRleBpDecoder(const uint8_t *buffer, uint32_t buffer_len, uint32_t bit_width)\n+\t    : buffer(buffer), bit_width_(bit_width), current_value_(0), repeat_count_(0), literal_count_(0) {\n+\t\tif (bit_width >= 64) {\n+\t\t\tthrow std::runtime_error(\"Decode bit width too large\");\n+\t\t}\n+\t\tbyte_encoded_len = ((bit_width_ + 7) / 8);\n+\t\tmax_val = (1 << bit_width_) - 1;\n+\t}\n+\n+\t/// Gets a batch of values.  Returns the number of decoded elements.\n+\ttemplate <typename T> void GetBatch(char *values_target_ptr, uint32_t batch_size) {\n+\t\tauto values = (T *)values_target_ptr;\n+\t\tuint32_t values_read = 0;\n+\n+\t\twhile (values_read < batch_size) {\n+\t\t\tif (repeat_count_ > 0) {\n+\t\t\t\tint repeat_batch = std::min(batch_size - values_read, static_cast<uint32_t>(repeat_count_));\n+\t\t\t\tstd::fill(values + values_read, values + values_read + repeat_batch, static_cast<T>(current_value_));\n+\t\t\t\trepeat_count_ -= repeat_batch;\n+\t\t\t\tvalues_read += repeat_batch;\n+\t\t\t} else if (literal_count_ > 0) {\n+\t\t\t\tuint32_t literal_batch = std::min(batch_size - values_read, static_cast<uint32_t>(literal_count_));\n+\t\t\t\tuint32_t actual_read = BitUnpack<T>(values + values_read, literal_batch);\n+\t\t\t\tif (literal_batch != actual_read) {\n+\t\t\t\t\tthrow std::runtime_error(\"Did not find enough values\");\n+\t\t\t\t}\n+\t\t\t\tliteral_count_ -= literal_batch;\n+\t\t\t\tvalues_read += literal_batch;\n+\t\t\t} else {\n+\t\t\t\tif (!NextCounts<T>()) {\n+\t\t\t\t\tif (values_read != batch_size) {\n+\t\t\t\t\t\tthrow std::runtime_error(\"RLE decode did not find enough values\");\n+\t\t\t\t\t}\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tif (values_read != batch_size) {\n+\t\t\tthrow std::runtime_error(\"RLE decode did not find enough values\");\n+\t\t}\n+\t}\n+\n+private:\n+\tconst uint8_t *buffer;\n+\n+\t/// Number of bits needed to encode the value. Must be between 0 and 64.\n+\tint bit_width_;\n+\tuint64_t current_value_;\n+\tuint32_t repeat_count_;\n+\tuint32_t literal_count_;\n+\tuint8_t byte_encoded_len;\n+\tuint32_t max_val;\n+\n+\tint8_t bitpack_pos = 0;\n+\n+\t// this is slow but whatever, calls are rare\n+\tstatic uint8_t VarintDecode(const uint8_t *source, uint32_t *result_out) {\n+\t\tuint32_t result = 0;\n+\t\tuint8_t shift = 0;\n+\t\tuint8_t len = 0;\n+\t\twhile (true) {\n+\t\t\tauto byte = *source++;\n+\t\t\tlen++;\n+\t\t\tresult |= (byte & 127) << shift;\n+\t\t\tif ((byte & 128) == 0)\n+\t\t\t\tbreak;\n+\t\t\tshift += 7;\n+\t\t\tif (shift > 32) {\n+\t\t\t\tthrow std::runtime_error(\"Varint-decoding found too large number\");\n+\t\t\t}\n+\t\t}\n+\t\t*result_out = result;\n+\t\treturn len;\n+\t}\n+\n+\t/// Fills literal_count_ and repeat_count_ with next values. Returns false if there\n+\t/// are no more.\n+\ttemplate <typename T> bool NextCounts() {\n+\t\t// Read the next run's indicator int, it could be a literal or repeated run.\n+\t\t// The int is encoded as a vlq-encoded value.\n+\t\tuint32_t indicator_value;\n+\t\tif (bitpack_pos != 0) {\n+\t\t\tbuffer++;\n+\t\t\tbitpack_pos = 0;\n+\t\t}\n+\t\tbuffer += VarintDecode(buffer, &indicator_value);\n+\n+\t\t// lsb indicates if it is a literal run or repeated run\n+\t\tbool is_literal = indicator_value & 1;\n+\t\tif (is_literal) {\n+\t\t\tliteral_count_ = (indicator_value >> 1) * 8;\n+\t\t} else {\n+\t\t\trepeat_count_ = indicator_value >> 1;\n+\t\t\t// (ARROW-4018) this is not big-endian compatible, lol\n+\t\t\tcurrent_value_ = 0;\n+\t\t\tfor (auto i = 0; i < byte_encoded_len; i++) {\n+\t\t\t\tcurrent_value_ |= ((uint8_t)*buffer++) << (i * 8);\n+\t\t\t}\n+\t\t\t// sanity check\n+\t\t\tif (repeat_count_ > 0 && current_value_ > max_val) {\n+\t\t\t\tthrow std::runtime_error(\"Payload value bigger than allowed. Corrupted file?\");\n+\t\t\t}\n+\t\t}\n+\t\t// TODO complain if we run out of buffer\n+\t\treturn true;\n+\t}\n+\n+\t// somewhat optimized implementation that avoids non-alignment\n+\n+\tstatic const uint32_t BITPACK_MASKS[];\n+\tstatic const uint8_t BITPACK_DLEN;\n+\n+\ttemplate <typename T> uint32_t BitUnpack(T *dest, uint32_t count) {\n+\t\tD_ASSERT(bit_width_ < 32);\n+\n+\t\t// auto source = buffer;\n+\t\tauto mask = BITPACK_MASKS[bit_width_];\n+\n+\t\tfor (uint32_t i = 0; i < count; i++) {\n+\t\t\tT val = (*buffer >> bitpack_pos) & mask;\n+\t\t\tbitpack_pos += bit_width_;\n+\t\t\twhile (bitpack_pos > BITPACK_DLEN) {\n+\t\t\t\tval |= (*++buffer << (BITPACK_DLEN - (bitpack_pos - bit_width_))) & mask;\n+\t\t\t\tbitpack_pos -= BITPACK_DLEN;\n+\t\t\t}\n+\t\t\tdest[i] = val;\n+\t\t}\n+\t\treturn count;\n+\t}\n+};\n+} // namespace duckdb\ndiff --git a/extension/parquet/parquet-extension.cpp b/extension/parquet/parquet-extension.cpp\nindex 132802df4a98..e132e3ed2d57 100644\n--- a/extension/parquet/parquet-extension.cpp\n+++ b/extension/parquet/parquet-extension.cpp\n@@ -38,6 +38,7 @@ struct ParquetReadOperatorData : public FunctionOperatorData {\n \tbool is_parallel;\n \tidx_t file_index;\n \tvector<column_t> column_ids;\n+\tTableFilterSet *table_filters;\n };\n \n struct ParquetReadParallelState : public ParallelState {\n@@ -56,6 +57,7 @@ class ParquetScanFunction : public TableFunction {\n \t                    /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr, parquet_max_threads,\n \t                    parquet_init_parallel_state, parquet_scan_parallel_init, parquet_parallel_state_next) {\n \t\tprojection_pushdown = true;\n+\t\tfilter_pushdown = true;\n \t}\n \n \tstatic unique_ptr<FunctionData> parquet_read_bind(ClientContext &context, CopyInfo &info,\n@@ -154,13 +156,14 @@ class ParquetScanFunction : public TableFunction {\n \n \t\tresult->is_parallel = false;\n \t\tresult->file_index = 0;\n+\t\tresult->table_filters = table_filters;\n \t\t// single-threaded: one thread has to read all groups\n \t\tvector<idx_t> group_ids;\n \t\tfor (idx_t i = 0; i < bind_data.initial_reader->NumRowGroups(); i++) {\n \t\t\tgroup_ids.push_back(i);\n \t\t}\n \t\tresult->reader = bind_data.initial_reader;\n-\t\tresult->reader->Initialize(result->scan_state, column_ids, move(group_ids));\n+\t\tresult->reader->Initialize(result->scan_state, column_ids, move(group_ids), table_filters);\n \t\treturn move(result);\n \t}\n \n@@ -170,6 +173,7 @@ class ParquetScanFunction : public TableFunction {\n \t\tauto result = make_unique<ParquetReadOperatorData>();\n \t\tresult->column_ids = column_ids;\n \t\tresult->is_parallel = true;\n+\t\tresult->table_filters = table_filters;\n \t\tif (!parquet_parallel_state_next(context, bind_data_, result.get(), parallel_state_)) {\n \t\t\treturn nullptr;\n \t\t}\n@@ -180,7 +184,7 @@ class ParquetScanFunction : public TableFunction {\n \t                                  FunctionOperatorData *operator_state, DataChunk &output) {\n \t\tauto &data = (ParquetReadOperatorData &)*operator_state;\n \t\tdo {\n-\t\t\tdata.reader->ReadChunk(data.scan_state, output);\n+\t\t\tdata.reader->Scan(data.scan_state, output);\n \t\t\tif (output.size() == 0 && !data.is_parallel) {\n \t\t\t\tauto &bind_data = (ParquetReadBindData &)*bind_data_;\n \t\t\t\t// check if there is another file\n@@ -194,7 +198,7 @@ class ParquetScanFunction : public TableFunction {\n \t\t\t\t\tfor (idx_t i = 0; i < data.reader->NumRowGroups(); i++) {\n \t\t\t\t\t\tgroup_ids.push_back(i);\n \t\t\t\t\t}\n-\t\t\t\t\tdata.reader->Initialize(data.scan_state, data.column_ids, move(group_ids));\n+\t\t\t\t\tdata.reader->Initialize(data.scan_state, data.column_ids, move(group_ids), data.table_filters);\n \t\t\t\t} else {\n \t\t\t\t\t// exhausted all the files: done\n \t\t\t\t\tbreak;\n@@ -236,7 +240,8 @@ class ParquetScanFunction : public TableFunction {\n \t\t\t// groups remain in the current parquet file: read the next group\n \t\t\tscan_data.reader = parallel_state.current_reader;\n \t\t\tvector<idx_t> group_indexes{parallel_state.row_group_index};\n-\t\t\tscan_data.reader->Initialize(scan_data.scan_state, scan_data.column_ids, group_indexes);\n+\t\t\tscan_data.reader->Initialize(scan_data.scan_state, scan_data.column_ids, group_indexes,\n+\t\t\t                             scan_data.table_filters);\n \t\t\tparallel_state.row_group_index++;\n \t\t\treturn true;\n \t\t} else {\n@@ -253,7 +258,8 @@ class ParquetScanFunction : public TableFunction {\n \t\t\t\t// set up the scan state to read the first group\n \t\t\t\tscan_data.reader = parallel_state.current_reader;\n \t\t\t\tvector<idx_t> group_indexes{0};\n-\t\t\t\tscan_data.reader->Initialize(scan_data.scan_state, scan_data.column_ids, group_indexes);\n+\t\t\t\tscan_data.reader->Initialize(scan_data.scan_state, scan_data.column_ids, group_indexes,\n+\t\t\t\t                             scan_data.table_filters);\n \t\t\t\tparallel_state.row_group_index = 1;\n \t\t\t\treturn true;\n \t\t\t}\n@@ -319,8 +325,8 @@ unique_ptr<GlobalFunctionData> parquet_write_initialize_global(ClientContext &co\n \tauto &parquet_bind = (ParquetWriteBindData &)bind_data;\n \n \tauto &fs = FileSystem::GetFileSystem(context);\n-\tglobal_state->writer =\n-\t    make_unique<ParquetWriter>(fs, parquet_bind.file_name, parquet_bind.sql_types, parquet_bind.column_names, parquet_bind.codec);\n+\tglobal_state->writer = make_unique<ParquetWriter>(fs, parquet_bind.file_name, parquet_bind.sql_types,\n+\t                                                  parquet_bind.column_names, parquet_bind.codec);\n \treturn move(global_state);\n }\n \ndiff --git a/extension/parquet/parquet_reader.cpp b/extension/parquet/parquet_reader.cpp\nindex ffec4a333e87..e7b011e7bd88 100644\n--- a/extension/parquet/parquet_reader.cpp\n+++ b/extension/parquet/parquet_reader.cpp\n@@ -5,6 +5,9 @@\n #include \"duckdb/function/table_function.hpp\"\n #include \"duckdb/parser/parsed_data/create_table_function_info.hpp\"\n #include \"duckdb/parser/parsed_data/create_copy_function_info.hpp\"\n+\n+#include \"duckdb/planner/table_filter.hpp\"\n+\n #include \"duckdb/main/client_context.hpp\"\n #include \"duckdb/main/connection.hpp\"\n #include \"duckdb/main/database.hpp\"\n@@ -37,11 +40,19 @@\n \n namespace duckdb {\n \n+const uint32_t RleBpDecoder::BITPACK_MASKS[] = {\n+    0,       1,       3,        7,        15,       31,        63,        127,       255,        511,       1023,\n+    2047,    4095,    8191,     16383,    32767,    65535,     131071,    262143,    524287,     1048575,   2097151,\n+    4194303, 8388607, 16777215, 33554431, 67108863, 134217727, 268435455, 536870911, 1073741823, 2147483647};\n+\n+const uint8_t RleBpDecoder::BITPACK_DLEN = 8;\n+\n using namespace parquet;\n using namespace apache::thrift;\n using namespace apache::thrift::protocol;\n using namespace apache::thrift::transport;\n \n+using parquet::format::ColumnChunk;\n using parquet::format::CompressionCodec;\n using parquet::format::ConvertedType;\n using parquet::format::Encoding;\n@@ -50,152 +61,10 @@ using parquet::format::FileMetaData;\n using parquet::format::PageHeader;\n using parquet::format::PageType;\n using parquet::format::RowGroup;\n+using parquet::format::SchemaElement;\n+using parquet::format::Statistics;\n using parquet::format::Type;\n \n-// adapted from arrow parquet reader\n-class RleBpDecoder {\n-public:\n-\t/// Create a decoder object. buffer/buffer_len is the decoded data.\n-\t/// bit_width is the width of each value (before encoding).\n-\tRleBpDecoder(const uint8_t *buffer, uint32_t buffer_len, uint32_t bit_width)\n-\t    : buffer(buffer), bit_width_(bit_width), current_value_(0), repeat_count_(0), literal_count_(0) {\n-\n-\t\tif (bit_width >= 64) {\n-\t\t\tthrow runtime_error(\"Decode bit width too large\");\n-\t\t}\n-\t\tbyte_encoded_len = ((bit_width_ + 7) / 8);\n-\t\tmax_val = (1 << bit_width_) - 1;\n-\t}\n-\n-\t/// Gets a batch of values.  Returns the number of decoded elements.\n-\ttemplate <typename T> void GetBatch(char *values_target_ptr, uint32_t batch_size) {\n-\t\tauto values = (T *)values_target_ptr;\n-\t\tuint32_t values_read = 0;\n-\n-\t\twhile (values_read < batch_size) {\n-\t\t\tif (repeat_count_ > 0) {\n-\t\t\t\tint repeat_batch = std::min(batch_size - values_read, static_cast<uint32_t>(repeat_count_));\n-\t\t\t\tstd::fill(values + values_read, values + values_read + repeat_batch, static_cast<T>(current_value_));\n-\t\t\t\trepeat_count_ -= repeat_batch;\n-\t\t\t\tvalues_read += repeat_batch;\n-\t\t\t} else if (literal_count_ > 0) {\n-\t\t\t\tuint32_t literal_batch = std::min(batch_size - values_read, static_cast<uint32_t>(literal_count_));\n-\t\t\t\tuint32_t actual_read = BitUnpack<T>(values + values_read, literal_batch);\n-\t\t\t\tif (literal_batch != actual_read) {\n-\t\t\t\t\tthrow runtime_error(\"Did not find enough values\");\n-\t\t\t\t}\n-\t\t\t\tliteral_count_ -= literal_batch;\n-\t\t\t\tvalues_read += literal_batch;\n-\t\t\t} else {\n-\t\t\t\tif (!NextCounts<T>()) {\n-\t\t\t\t\tif (values_read != batch_size) {\n-\t\t\t\t\t\tthrow runtime_error(\"RLE decode did not find enough values\");\n-\t\t\t\t\t}\n-\t\t\t\t\treturn;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\tif (values_read != batch_size) {\n-\t\t\tthrow runtime_error(\"RLE decode did not find enough values\");\n-\t\t}\n-\t}\n-\n-private:\n-\tconst uint8_t *buffer;\n-\n-\t/// Number of bits needed to encode the value. Must be between 0 and 64.\n-\tint bit_width_;\n-\tuint64_t current_value_;\n-\tuint32_t repeat_count_;\n-\tuint32_t literal_count_;\n-\tuint8_t byte_encoded_len;\n-\tuint32_t max_val;\n-\n-\tint8_t bitpack_pos = 0;\n-\n-\t// this is slow but whatever, calls are rare\n-\tstatic uint8_t VarintDecode(const uint8_t *source, uint32_t *result_out) {\n-\t\tuint32_t result = 0;\n-\t\tuint8_t shift = 0;\n-\t\tuint8_t len = 0;\n-\t\twhile (true) {\n-\t\t\tauto byte = *source++;\n-\t\t\tlen++;\n-\t\t\tresult |= (byte & 127) << shift;\n-\t\t\tif ((byte & 128) == 0)\n-\t\t\t\tbreak;\n-\t\t\tshift += 7;\n-\t\t\tif (shift > 32) {\n-\t\t\t\tthrow runtime_error(\"Varint-decoding found too large number\");\n-\t\t\t}\n-\t\t}\n-\t\t*result_out = result;\n-\t\treturn len;\n-\t}\n-\n-\t/// Fills literal_count_ and repeat_count_ with next values. Returns false if there\n-\t/// are no more.\n-\ttemplate <typename T> bool NextCounts() {\n-\t\t// Read the next run's indicator int, it could be a literal or repeated run.\n-\t\t// The int is encoded as a vlq-encoded value.\n-\t\tuint32_t indicator_value;\n-\t\tif (bitpack_pos != 0) {\n-\t\t\tbuffer++;\n-\t\t\tbitpack_pos = 0;\n-\t\t}\n-\t\tbuffer += VarintDecode(buffer, &indicator_value);\n-\n-\t\t// lsb indicates if it is a literal run or repeated run\n-\t\tbool is_literal = indicator_value & 1;\n-\t\tif (is_literal) {\n-\t\t\tliteral_count_ = (indicator_value >> 1) * 8;\n-\t\t} else {\n-\t\t\trepeat_count_ = indicator_value >> 1;\n-\t\t\t// (ARROW-4018) this is not big-endian compatible, lol\n-\t\t\tcurrent_value_ = 0;\n-\t\t\tfor (auto i = 0; i < byte_encoded_len; i++) {\n-\t\t\t\tcurrent_value_ |= ((uint8_t)*buffer++) << (i * 8);\n-\t\t\t}\n-\t\t\t// sanity check\n-\t\t\tif (repeat_count_ > 0 && current_value_ > max_val) {\n-\t\t\t\tthrow runtime_error(\"Payload value bigger than allowed. Corrupted file?\");\n-\t\t\t}\n-\t\t}\n-\t\t// TODO complain if we run out of buffer\n-\t\treturn true;\n-\t}\n-\n-\t// somewhat optimized implementation that avoids non-alignment\n-\n-\tstatic const uint32_t BITPACK_MASKS[];\n-\tstatic const uint8_t BITPACK_DLEN;\n-\n-\ttemplate <typename T> uint32_t BitUnpack(T *dest, uint32_t count) {\n-\t\tD_ASSERT(bit_width_ < 32);\n-\n-\t\t// auto source = buffer;\n-\t\tauto mask = BITPACK_MASKS[bit_width_];\n-\n-\t\tfor (uint32_t i = 0; i < count; i++) {\n-\t\t\tT val = (*buffer >> bitpack_pos) & mask;\n-\t\t\tbitpack_pos += bit_width_;\n-\t\t\twhile (bitpack_pos > BITPACK_DLEN) {\n-\t\t\t\tval |= (*++buffer << (BITPACK_DLEN - (bitpack_pos - bit_width_))) & mask;\n-\t\t\t\tbitpack_pos -= BITPACK_DLEN;\n-\t\t\t}\n-\t\t\tdest[i] = val;\n-\t\t}\n-\t\treturn count;\n-\t}\n-};\n-\n-const uint32_t RleBpDecoder::BITPACK_MASKS[] = {\n-    0,       1,       3,        7,        15,       31,        63,        127,       255,        511,       1023,\n-    2047,    4095,    8191,     16383,    32767,    65535,     131071,    262143,    524287,     1048575,   2097151,\n-    4194303, 8388607, 16777215, 33554431, 67108863, 134217727, 268435455, 536870911, 1073741823, 2147483647};\n-\n-const uint8_t RleBpDecoder::BITPACK_DLEN = 8;\n-\n static TCompactProtocolFactoryT<TMemoryBuffer> tproto_factory;\n \n template <class T> static void thrift_unpack(const uint8_t *buf, uint32_t *len, T *deserialized_msg) {\n@@ -212,9 +81,9 @@ template <class T> static void thrift_unpack(const uint8_t *buf, uint32_t *len,\n \t*len = *len - bytes_left;\n }\n \n-static unique_ptr<parquet::format::FileMetaData> read_metadata(duckdb::FileSystem &fs, duckdb::FileHandle *handle,\n-                                                               uint32_t footer_len, uint64_t file_size) {\n-\tauto metadata = make_unique<parquet::format::FileMetaData>();\n+static unique_ptr<FileMetaData> read_metadata(duckdb::FileSystem &fs, duckdb::FileHandle *handle, uint32_t footer_len,\n+                                              uint64_t file_size) {\n+\tauto metadata = make_unique<FileMetaData>();\n \t// read footer into buffer and de-thrift\n \tResizeableBuffer buf;\n \tbuf.resize(footer_len);\n@@ -381,7 +250,7 @@ ParquetReader::ParquetReader(ClientContext &context, string file_name_, vector<L\n ParquetReader::~ParquetReader() {\n }\n \n-const parquet::format::FileMetaData *ParquetReader::GetFileMetadata() {\n+const FileMetaData *ParquetReader::GetFileMetadata() {\n \tD_ASSERT(metadata);\n \tD_ASSERT(metadata->metadata);\n \treturn metadata->metadata.get();\n@@ -404,12 +273,190 @@ template <> bool ValueIsValid::Operation(double value) {\n \treturn Value::DoubleIsValid(value);\n }\n \n+timestamp_t arrow_timestamp_micros_to_timestamp(const int64_t &raw_ts) {\n+\treturn Timestamp::FromEpochMicroSeconds(raw_ts);\n+}\n+timestamp_t arrow_timestamp_ms_to_timestamp(const int64_t &raw_ts) {\n+\treturn Timestamp::FromEpochMs(raw_ts);\n+}\n+\n+// statistics handling\n+\n+template <Value (*FUNC)(const_data_ptr_t input)>\n+static unique_ptr<BaseStatistics> templated_get_numeric_stats(const LogicalType &type,\n+                                                              const Statistics &parquet_stats) {\n+\tauto stats = make_unique<NumericStatistics>(type);\n+\n+\t// for reasons unknown to science, Parquet defines *both* `min` and `min_value` as well as `max` and\n+\t// `max_value`. All are optional. such elegance.\n+\tif (parquet_stats.__isset.min) {\n+\t\tstats->min = FUNC((const_data_ptr_t)parquet_stats.min.data());\n+\t} else if (parquet_stats.__isset.min_value) {\n+\t\tstats->min = FUNC((const_data_ptr_t)parquet_stats.min_value.data());\n+\t} else {\n+\t\tstats->min.is_null = true;\n+\t}\n+\tif (parquet_stats.__isset.max) {\n+\t\tstats->max = FUNC((const_data_ptr_t)parquet_stats.max.data());\n+\t} else if (parquet_stats.__isset.max_value) {\n+\t\tstats->max = FUNC((const_data_ptr_t)parquet_stats.max_value.data());\n+\t} else {\n+\t\tstats->max.is_null = true;\n+\t}\n+\t// GCC 4.x insists on a move() here\n+\treturn move(stats);\n+}\n+\n+template <class T> static Value transform_statistics_plain(const_data_ptr_t input) {\n+\treturn Value(Load<T>(input));\n+}\n+\n+static Value transform_statistics_timestamp_ms(const_data_ptr_t input) {\n+\treturn Value::TIMESTAMP(arrow_timestamp_ms_to_timestamp(Load<int64_t>(input)));\n+}\n+\n+static Value transform_statistics_timestamp_micros(const_data_ptr_t input) {\n+\treturn Value::TIMESTAMP(arrow_timestamp_micros_to_timestamp(Load<int64_t>(input)));\n+}\n+\n+static Value transform_statistics_timestamp_impala(const_data_ptr_t input) {\n+\treturn Value::TIMESTAMP(impala_timestamp_to_timestamp_t(Load<Int96>(input)));\n+}\n+\n+static unique_ptr<BaseStatistics> get_col_chunk_stats(const SchemaElement &s_ele, const LogicalType &type,\n+                                                      const ColumnChunk &column_chunk) {\n+\tif (!column_chunk.__isset.meta_data || !column_chunk.meta_data.__isset.statistics) {\n+\t\t// no stats present for row group\n+\t\treturn nullptr;\n+\t}\n+\tauto &parquet_stats = column_chunk.meta_data.statistics;\n+\tunique_ptr<BaseStatistics> row_group_stats;\n+\n+\tswitch (type.id()) {\n+\tcase LogicalTypeId::INTEGER:\n+\t\trow_group_stats = templated_get_numeric_stats<transform_statistics_plain<int32_t>>(type, parquet_stats);\n+\t\tbreak;\n+\n+\tcase LogicalTypeId::BIGINT:\n+\t\trow_group_stats = templated_get_numeric_stats<transform_statistics_plain<int64_t>>(type, parquet_stats);\n+\t\tbreak;\n+\n+\tcase LogicalTypeId::FLOAT:\n+\t\trow_group_stats = templated_get_numeric_stats<transform_statistics_plain<float>>(type, parquet_stats);\n+\t\tbreak;\n+\n+\tcase LogicalTypeId::DOUBLE:\n+\t\trow_group_stats = templated_get_numeric_stats<transform_statistics_plain<double>>(type, parquet_stats);\n+\t\tbreak;\n+\n+\t\t// here we go, our favorite type\n+\tcase LogicalTypeId::TIMESTAMP: {\n+\t\tswitch (s_ele.type) {\n+\t\tcase Type::INT64:\n+\t\t\t// arrow timestamp\n+\t\t\tswitch (s_ele.converted_type) {\n+\t\t\tcase ConvertedType::TIMESTAMP_MICROS:\n+\t\t\t\trow_group_stats =\n+\t\t\t\t    templated_get_numeric_stats<transform_statistics_timestamp_micros>(type, parquet_stats);\n+\t\t\t\tbreak;\n+\t\t\tcase ConvertedType::TIMESTAMP_MILLIS:\n+\t\t\t\trow_group_stats = templated_get_numeric_stats<transform_statistics_timestamp_ms>(type, parquet_stats);\n+\t\t\t\tbreak;\n+\t\t\tdefault:\n+\t\t\t\treturn nullptr;\n+\t\t\t}\n+\t\t\tbreak;\n+\t\tcase Type::INT96:\n+\t\t\t// impala timestamp\n+\t\t\trow_group_stats = templated_get_numeric_stats<transform_statistics_timestamp_impala>(type, parquet_stats);\n+\t\t\tbreak;\n+\t\tdefault:\n+\t\t\treturn nullptr;\n+\t\t}\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::VARCHAR: {\n+\t\tauto string_stats = make_unique<StringStatistics>(type);\n+\t\tif (parquet_stats.__isset.min) {\n+\t\t\tmemcpy(string_stats->min, (data_ptr_t)parquet_stats.min.data(),\n+\t\t\t       MinValue<idx_t>(parquet_stats.min.size(), StringStatistics::MAX_STRING_MINMAX_SIZE));\n+\t\t} else if (parquet_stats.__isset.min_value) {\n+\t\t\tmemcpy(string_stats->min, (data_ptr_t)parquet_stats.min_value.data(),\n+\t\t\t       MinValue<idx_t>(parquet_stats.min_value.size(), StringStatistics::MAX_STRING_MINMAX_SIZE));\n+\t\t} else {\n+\t\t\treturn nullptr;\n+\t\t}\n+\t\tif (parquet_stats.__isset.max) {\n+\t\t\tmemcpy(string_stats->max, (data_ptr_t)parquet_stats.max.data(),\n+\t\t\t       MinValue<idx_t>(parquet_stats.max.size(), StringStatistics::MAX_STRING_MINMAX_SIZE));\n+\t\t} else if (parquet_stats.__isset.max_value) {\n+\t\t\tmemcpy(string_stats->max, (data_ptr_t)parquet_stats.max_value.data(),\n+\t\t\t       MinValue<idx_t>(parquet_stats.max_value.size(), StringStatistics::MAX_STRING_MINMAX_SIZE));\n+\t\t} else {\n+\t\t\treturn nullptr;\n+\t\t}\n+\n+\t\tstring_stats->has_unicode = true; // we dont know better\n+\t\trow_group_stats = move(string_stats);\n+\t\tbreak;\n+\t}\n+\tdefault:\n+\t\t// no stats for you\n+\t\tbreak;\n+\t} // end of type switch\n+\n+\t// null count is generic\n+\tif (row_group_stats) {\n+\t\tif (parquet_stats.__isset.null_count) {\n+\t\t\trow_group_stats->has_null = parquet_stats.null_count != 0;\n+\t\t} else {\n+\t\t\trow_group_stats->has_null = true;\n+\t\t}\n+\t} else {\n+\t\t// if stats are missing from any row group we know squat\n+\t\treturn nullptr;\n+\t}\n+\n+\treturn row_group_stats;\n+}\n+\n+unique_ptr<BaseStatistics> ParquetReader::ReadStatistics(LogicalType &type, column_t column_index,\n+                                                         const FileMetaData *file_meta_data) {\n+\tunique_ptr<BaseStatistics> column_stats;\n+\n+\tfor (auto &row_group : file_meta_data->row_groups) {\n+\n+\t\tD_ASSERT(column_index < row_group.columns.size());\n+\t\tauto &column_chunk = row_group.columns[column_index];\n+\t\tauto &s_ele = file_meta_data->schema[column_index + 1];\n+\n+\t\tauto chunk_stats = get_col_chunk_stats(s_ele, type, column_chunk);\n+\n+\t\tif (!column_stats) {\n+\t\t\tcolumn_stats = move(chunk_stats);\n+\t\t} else {\n+\t\t\tcolumn_stats->Merge(*chunk_stats);\n+\t\t}\n+\t}\n+\treturn column_stats;\n+}\n+\n template <class T>\n-void ParquetReader::fill_from_dict(ParquetReaderColumnData &col_data, idx_t count, Vector &target,\n-                                   idx_t target_offset) {\n+static void fill_from_dict(ParquetReaderColumnData &col_data, idx_t count, parquet_filter_t &filter_mask,\n+                           Vector &target, idx_t target_offset) {\n+\n+\tif (!col_data.has_nulls && filter_mask.none()) {\n+\t\tcol_data.offset_buf.inc(sizeof(uint32_t) * count);\n+\t\treturn;\n+\t}\n+\n \tfor (idx_t i = 0; i < count; i++) {\n \t\tif (!col_data.has_nulls || col_data.defined_buf.ptr[i]) {\n \t\t\tauto offset = col_data.offset_buf.read<uint32_t>();\n+\t\t\tif (!filter_mask[i + target_offset]) {\n+\t\t\t\tcontinue; // early out if this value is skipped\n+\t\t\t}\n+\n \t\t\tif (offset > col_data.dict_size) {\n \t\t\t\tthrow runtime_error(\"Offset \" + to_string(offset) + \" greater than dictionary size \" +\n \t\t\t\t                    to_string(col_data.dict_size) + \" at \" + to_string(i + target_offset) +\n@@ -428,11 +475,21 @@ void ParquetReader::fill_from_dict(ParquetReaderColumnData &col_data, idx_t coun\n }\n \n template <class T>\n-void ParquetReader::fill_from_plain(ParquetReaderColumnData &col_data, idx_t count, Vector &target,\n-                                    idx_t target_offset) {\n+static void fill_from_plain(ParquetReaderColumnData &col_data, idx_t count, parquet_filter_t &filter_mask,\n+                            Vector &target, idx_t target_offset) {\n+\tif (!col_data.has_nulls && filter_mask.none()) {\n+\t\tcol_data.payload.inc(sizeof(T) * count);\n+\t\treturn;\n+\t}\n+\n \tfor (idx_t i = 0; i < count; i++) {\n \t\tif (!col_data.has_nulls || col_data.defined_buf.ptr[i]) {\n \t\t\tauto value = col_data.payload.read<T>();\n+\n+\t\t\tif (!filter_mask[i + target_offset]) {\n+\t\t\t\tcontinue; // early out if this value is skipped\n+\t\t\t}\n+\n \t\t\tif (ValueIsValid::Operation(value)) {\n \t\t\t\t((T *)FlatVector::GetData(target))[i + target_offset] = value;\n \t\t\t} else {\n@@ -445,10 +502,21 @@ void ParquetReader::fill_from_plain(ParquetReaderColumnData &col_data, idx_t cou\n }\n \n template <class T, timestamp_t (*FUNC)(const T &input)>\n-static void fill_timestamp_plain(ParquetReaderColumnData &col_data, idx_t count, Vector &target, idx_t target_offset) {\n+static void fill_timestamp_plain(ParquetReaderColumnData &col_data, idx_t count, parquet_filter_t &filter_mask,\n+                                 Vector &target, idx_t target_offset) {\n+\tif (!col_data.has_nulls && filter_mask.none()) {\n+\t\tcol_data.payload.inc(sizeof(T) * count);\n+\t\treturn;\n+\t}\n+\n \tfor (idx_t i = 0; i < count; i++) {\n \t\tif (!col_data.has_nulls || col_data.defined_buf.ptr[i]) {\n \t\t\tauto value = col_data.payload.read<T>();\n+\n+\t\t\tif (!filter_mask[i + target_offset]) {\n+\t\t\t\tcontinue; // early out if this value is skipped\n+\t\t\t}\n+\n \t\t\t((timestamp_t *)FlatVector::GetData(target))[i + target_offset] = FUNC(value);\n \t\t} else {\n \t\t\tFlatVector::SetNull(target, i + target_offset, true);\n@@ -464,13 +532,6 @@ const RowGroup &ParquetReader::GetGroup(ParquetReaderScanState &state) {\n \treturn file_meta_data->row_groups[state.group_idx_list[state.current_group]];\n }\n \n-timestamp_t arrow_timestamp_micros_to_timestamp(const int64_t &raw_ts) {\n-\treturn Timestamp::FromEpochMicroSeconds(raw_ts);\n-}\n-timestamp_t arrow_timestamp_ms_to_timestamp(const int64_t &raw_ts) {\n-\treturn Timestamp::FromEpochMs(raw_ts);\n-}\n-\n template <class T, timestamp_t (*FUNC)(const T &input)>\n static void fill_timestamp_dict(ParquetReaderColumnData &col_data) {\n \t// immediately convert timestamps to duckdb format, potentially fewer conversions\n@@ -657,7 +718,6 @@ bool ParquetReader::PreparePageBuffers(ParquetReaderScanState &state, idx_t col_\n \t}\n \tcase PageType::DATA_PAGE:\n \tcase PageType::DATA_PAGE_V2: {\n-\n \t\tif (page_hdr.type == PageType::DATA_PAGE) {\n \t\t\tD_ASSERT(page_hdr.__isset.data_page_header);\n \t\t}\n@@ -720,8 +780,9 @@ bool ParquetReader::PreparePageBuffers(ParquetReaderScanState &state, idx_t col_\n \treturn true;\n }\n \n-void ParquetReader::PrepareChunkBuffer(ParquetReaderScanState &state, idx_t col_idx) {\n-\tauto &chunk = GetGroup(state).columns[col_idx];\n+void ParquetReader::PrepareRowGroupBuffer(ParquetReaderScanState &state, idx_t col_idx, LogicalType &type) {\n+\tauto &group = GetGroup(state);\n+\tauto &chunk = group.columns[col_idx];\n \tif (chunk.__isset.file_path) {\n \t\tthrow FormatException(\"Only inlined data files are supported (no references)\");\n \t}\n@@ -730,6 +791,49 @@ void ParquetReader::PrepareChunkBuffer(ParquetReaderScanState &state, idx_t col_\n \t\tthrow FormatException(\"Only flat tables are supported (no nesting)\");\n \t}\n \n+\tif (state.filters) {\n+\t\tauto &s_ele = GetFileMetadata()->schema[col_idx + 1];\n+\t\tauto stats = get_col_chunk_stats(s_ele, type, group.columns[col_idx]);\n+\t\tauto filter_entry = state.filters->filters.find(col_idx);\n+\t\tif (stats && filter_entry != state.filters->filters.end()) {\n+\t\t\tbool skip_chunk = false;\n+\t\t\tswitch (type.id()) {\n+\t\t\tcase LogicalTypeId::INTEGER:\n+\t\t\tcase LogicalTypeId::BIGINT:\n+\t\t\tcase LogicalTypeId::FLOAT:\n+\t\t\tcase LogicalTypeId::TIMESTAMP:\n+\t\t\tcase LogicalTypeId::DOUBLE: {\n+\t\t\t\tauto num_stats = (NumericStatistics &)*stats;\n+\t\t\t\tfor (auto &filter : filter_entry->second) {\n+\t\t\t\t\tskip_chunk = !num_stats.CheckZonemap(filter.comparison_type, filter.constant);\n+\t\t\t\t\tif (skip_chunk) {\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tcase LogicalTypeId::BLOB:\n+\t\t\tcase LogicalTypeId::VARCHAR: {\n+\t\t\t\tauto str_stats = (StringStatistics &)*stats;\n+\t\t\t\tfor (auto &filter : filter_entry->second) {\n+\t\t\t\t\tskip_chunk = !str_stats.CheckZonemap(filter.comparison_type, filter.constant.str_value);\n+\t\t\t\t\tif (skip_chunk) {\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tdefault:\n+\t\t\t\tD_ASSERT(0);\n+\t\t\t}\n+\t\t\tif (skip_chunk) {\n+\t\t\t\tstate.group_offset = group.num_rows;\n+\t\t\t\treturn;\n+\t\t\t\t// this effectively will skip this chunk\n+\t\t\t}\n+\t\t}\n+\t}\n+\n \t// ugh. sometimes there is an extra offset for the dict. sometimes it's wrong.\n \tauto chunk_start = chunk.meta_data.data_page_offset;\n \tif (chunk.meta_data.__isset.dictionary_page_offset && chunk.meta_data.dictionary_page_offset >= 4) {\n@@ -747,6 +851,7 @@ void ParquetReader::PrepareChunkBuffer(ParquetReaderScanState &state, idx_t col_\n \t// read entire chunk into RAM\n \tstate.column_data[col_idx]->buf.resize(chunk_len);\n \tfs.Read(*handle, state.column_data[col_idx]->buf.ptr, chunk_len, chunk_start);\n+\treturn;\n }\n \n idx_t ParquetReader::NumRows() {\n@@ -757,402 +862,403 @@ idx_t ParquetReader::NumRowGroups() {\n \treturn GetFileMetadata()->row_groups.size();\n }\n \n-void ParquetReader::Initialize(ParquetReaderScanState &state, vector<column_t> column_ids,\n-                               vector<idx_t> groups_to_read) {\n+void ParquetReader::Initialize(ParquetReaderScanState &state, vector<column_t> column_ids, vector<idx_t> groups_to_read,\n+                               TableFilterSet *filters) {\n \tstate.current_group = -1;\n \tstate.finished = false;\n \tstate.column_ids = move(column_ids);\n \tstate.group_offset = 0;\n \tstate.group_idx_list = move(groups_to_read);\n+\tstate.filters = filters;\n \tfor (idx_t i = 0; i < return_types.size(); i++) {\n \t\tstate.column_data.push_back(make_unique<ParquetReaderColumnData>());\n \t}\n+\tstate.sel.Initialize(STANDARD_VECTOR_SIZE);\n }\n \n-void ParquetReader::ReadChunk(ParquetReaderScanState &state, DataChunk &output) {\n-\tif (state.finished) {\n+void ParquetReader::ScanColumn(ParquetReaderScanState &state, parquet_filter_t &filter_mask, idx_t count,\n+                               idx_t out_col_idx, Vector &out) {\n+\tauto file_col_idx = state.column_ids[out_col_idx];\n+\tif (file_col_idx == COLUMN_IDENTIFIER_ROW_ID) {\n+\t\tValue constant_42 = Value::BIGINT(42);\n+\t\tout.Reference(constant_42);\n \t\treturn;\n \t}\n-\n-\t// see if we have to switch to the next row group in the parquet file\n-\tif (state.current_group < 0 || (int64_t)state.group_offset >= GetGroup(state).num_rows) {\n-\t\tstate.current_group++;\n-\t\tstate.group_offset = 0;\n-\n-\t\tif ((idx_t)state.current_group == state.group_idx_list.size()) {\n-\t\t\tstate.finished = true;\n-\t\t\treturn;\n-\t\t}\n-\n-\t\tfor (idx_t out_col_idx = 0; out_col_idx < output.ColumnCount(); out_col_idx++) {\n-\t\t\tauto file_col_idx = state.column_ids[out_col_idx];\n-\n-\t\t\t// this is a special case where we are not interested in the actual contents of the file\n-\t\t\tif (file_col_idx == COLUMN_IDENTIFIER_ROW_ID) {\n+\tauto &s_ele = GetFileMetadata()->schema[file_col_idx + 1];\n+\tauto &col_data = *state.column_data[file_col_idx];\n+\n+\t// we might need to read multiple pages to fill the data chunk\n+\tidx_t output_offset = 0;\n+\twhile (output_offset < count) {\n+\t\t// do this unpack business only if we run out of stuff from the current page\n+\t\tif (col_data.page_offset >= col_data.page_value_count) {\n+\t\t\t// read dictionaries and data page headers so that we are ready to go for scan\n+\t\t\tif (!PreparePageBuffers(state, file_col_idx)) {\n \t\t\t\tcontinue;\n \t\t\t}\n-\n-\t\t\tPrepareChunkBuffer(state, file_col_idx);\n-\t\t\t// trigger the reading of a new page below\n-\t\t\tstate.column_data[file_col_idx]->page_value_count = 0;\n+\t\t\tcol_data.page_offset = 0;\n \t\t}\n-\t}\n \n-\toutput.SetCardinality(MinValue<int64_t>(STANDARD_VECTOR_SIZE, GetGroup(state).num_rows - state.group_offset));\n+\t\tauto current_batch_size =\n+\t\t    MinValue<idx_t>(col_data.page_value_count - col_data.page_offset, count - output_offset);\n \n-\tif (output.size() == 0) {\n-\t\treturn;\n-\t}\n+\t\tD_ASSERT(current_batch_size > 0);\n \n-\tauto file_meta_data = GetFileMetadata();\n-\n-\tfor (idx_t out_col_idx = 0; out_col_idx < output.ColumnCount(); out_col_idx++) {\n-\t\tauto file_col_idx = state.column_ids[out_col_idx];\n-\t\tif (file_col_idx == COLUMN_IDENTIFIER_ROW_ID) {\n-\t\t\tValue constant_42 = Value::BIGINT(42);\n-\t\t\toutput.data[out_col_idx].Reference(constant_42);\n-\t\t\tcontinue;\n+\t\tif (col_data.has_nulls) {\n+\t\t\tcol_data.defined_buf.resize(current_batch_size);\n+\t\t\tcol_data.defined_decoder->GetBatch<uint8_t>(col_data.defined_buf.ptr, current_batch_size);\n \t\t}\n-\t\tauto &s_ele = file_meta_data->schema[file_col_idx + 1];\n-\t\tauto &col_data = *state.column_data[file_col_idx];\n-\n-\t\t// we might need to read multiple pages to fill the data chunk\n-\t\tidx_t output_offset = 0;\n-\t\twhile (output_offset < output.size()) {\n-\t\t\t// do this unpack business only if we run out of stuff from the current page\n-\t\t\tif (col_data.page_offset >= col_data.page_value_count) {\n-\t\t\t\t// read dictionaries and data page headers so that we are ready to go for scan\n-\t\t\t\tif (!PreparePageBuffers(state, file_col_idx)) {\n-\t\t\t\t\tcontinue;\n+\n+\t\tswitch (col_data.page_encoding) {\n+\t\tcase Encoding::RLE_DICTIONARY:\n+\t\tcase Encoding::PLAIN_DICTIONARY: {\n+\t\t\tidx_t null_count = 0;\n+\t\t\tif (col_data.has_nulls) {\n+\t\t\t\tfor (idx_t i = 0; i < current_batch_size; i++) {\n+\t\t\t\t\tif (!col_data.defined_buf.ptr[i]) {\n+\t\t\t\t\t\tnull_count++;\n+\t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t\tcol_data.page_offset = 0;\n \t\t\t}\n \n-\t\t\tauto current_batch_size =\n-\t\t\t    MinValue<idx_t>(col_data.page_value_count - col_data.page_offset, output.size() - output_offset);\n+\t\t\tcol_data.offset_buf.resize(current_batch_size * sizeof(uint32_t));\n+\t\t\tcol_data.dict_decoder->GetBatch<uint32_t>(col_data.offset_buf.ptr, current_batch_size - null_count);\n \n-\t\t\tD_ASSERT(current_batch_size > 0);\n+\t\t\t// TODO ensure we had seen a dict page IN THIS CHUNK before getting here\n \n-\t\t\tif (col_data.has_nulls) {\n-\t\t\t\tcol_data.defined_buf.resize(current_batch_size);\n-\t\t\t\tcol_data.defined_decoder->GetBatch<uint8_t>(col_data.defined_buf.ptr, current_batch_size);\n-\t\t\t}\n+\t\t\tswitch (return_types[file_col_idx].id()) {\n+\t\t\tcase LogicalTypeId::BOOLEAN:\n+\t\t\t\tfill_from_dict<bool>(col_data, current_batch_size, filter_mask, out, output_offset);\n+\t\t\t\tbreak;\n+\t\t\tcase LogicalTypeId::INTEGER:\n+\t\t\t\tfill_from_dict<int32_t>(col_data, current_batch_size, filter_mask, out, output_offset);\n+\t\t\t\tbreak;\n+\t\t\tcase LogicalTypeId::BIGINT:\n+\t\t\t\tfill_from_dict<int64_t>(col_data, current_batch_size, filter_mask, out, output_offset);\n+\t\t\t\tbreak;\n+\t\t\tcase LogicalTypeId::FLOAT:\n+\t\t\t\tfill_from_dict<float>(col_data, current_batch_size, filter_mask, out, output_offset);\n+\t\t\t\tbreak;\n+\t\t\tcase LogicalTypeId::DOUBLE:\n+\t\t\t\tfill_from_dict<double>(col_data, current_batch_size, filter_mask, out, output_offset);\n+\t\t\t\tbreak;\n+\t\t\tcase LogicalTypeId::TIMESTAMP:\n+\t\t\t\tfill_from_dict<timestamp_t>(col_data, current_batch_size, filter_mask, out, output_offset);\n+\t\t\t\tbreak;\n+\t\t\tcase LogicalTypeId::BLOB:\n+\t\t\tcase LogicalTypeId::VARCHAR: {\n+\t\t\t\tif (!col_data.string_collection) {\n+\t\t\t\t\tthrow FormatException(\"Did not see a dictionary for strings. Corrupt file?\");\n+\t\t\t\t}\n \n-\t\t\tswitch (col_data.page_encoding) {\n-\t\t\tcase Encoding::RLE_DICTIONARY:\n-\t\t\tcase Encoding::PLAIN_DICTIONARY: {\n-\t\t\t\tidx_t null_count = 0;\n-\t\t\t\tif (col_data.has_nulls) {\n-\t\t\t\t\tfor (idx_t i = 0; i < current_batch_size; i++) {\n-\t\t\t\t\t\tif (!col_data.defined_buf.ptr[i]) {\n-\t\t\t\t\t\t\tnull_count++;\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n+\t\t\t\tif (!col_data.has_nulls && filter_mask.none()) {\n+\t\t\t\t\tcol_data.offset_buf.inc(sizeof(uint32_t) * count);\n+\t\t\t\t\tbreak;\n \t\t\t\t}\n \n-\t\t\t\tcol_data.offset_buf.resize(current_batch_size * sizeof(uint32_t));\n-\t\t\t\tcol_data.dict_decoder->GetBatch<uint32_t>(col_data.offset_buf.ptr, current_batch_size - null_count);\n+\t\t\t\t// the strings can be anywhere in the collection so just reference it all\n+\t\t\t\tfor (auto &chunk : col_data.string_collection->Chunks()) {\n+\t\t\t\t\tStringVector::AddHeapReference(out, chunk->data[0]);\n+\t\t\t\t}\n \n-\t\t\t\t// TODO ensure we had seen a dict page IN THIS CHUNK before getting here\n+\t\t\t\tauto out_data_ptr = FlatVector::GetData<string_t>(out);\n \n-\t\t\t\tswitch (return_types[file_col_idx].id()) {\n-\t\t\t\tcase LogicalTypeId::BOOLEAN:\n-\t\t\t\t\tfill_from_dict<bool>(col_data, current_batch_size, output.data[out_col_idx], output_offset);\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase LogicalTypeId::INTEGER:\n-\t\t\t\t\tfill_from_dict<int32_t>(col_data, current_batch_size, output.data[out_col_idx], output_offset);\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase LogicalTypeId::BIGINT:\n-\t\t\t\t\tfill_from_dict<int64_t>(col_data, current_batch_size, output.data[out_col_idx], output_offset);\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase LogicalTypeId::FLOAT:\n-\t\t\t\t\tfill_from_dict<float>(col_data, current_batch_size, output.data[out_col_idx], output_offset);\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase LogicalTypeId::DOUBLE:\n-\t\t\t\t\tfill_from_dict<double>(col_data, current_batch_size, output.data[out_col_idx], output_offset);\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase LogicalTypeId::TIMESTAMP:\n-\t\t\t\t\tfill_from_dict<timestamp_t>(col_data, current_batch_size, output.data[out_col_idx], output_offset);\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase LogicalTypeId::BLOB:\n-\t\t\t\tcase LogicalTypeId::VARCHAR: {\n-\t\t\t\t\tif (!col_data.string_collection) {\n-\t\t\t\t\t\tthrow FormatException(\"Did not see a dictionary for strings. Corrupt file?\");\n-\t\t\t\t\t}\n+\t\t\t\tfor (idx_t i = 0; i < current_batch_size; i++) {\n+\t\t\t\t\tif (!col_data.has_nulls || col_data.defined_buf.ptr[i]) {\n+\t\t\t\t\t\tauto offset = col_data.offset_buf.read<uint32_t>();\n \n-\t\t\t\t\t// the strings can be anywhere in the collection so just reference it all\n-\t\t\t\t\tfor (auto &chunk : col_data.string_collection->Chunks()) {\n-\t\t\t\t\t\tStringVector::AddHeapReference(output.data[out_col_idx], chunk->data[0]);\n-\t\t\t\t\t}\n+\t\t\t\t\t\tif (!filter_mask[i + output_offset]) {\n+\t\t\t\t\t\t\tcontinue; // early out if this value is skipped\n+\t\t\t\t\t\t}\n \n-\t\t\t\t\tauto out_data_ptr = FlatVector::GetData<string_t>(output.data[out_col_idx]);\n-\t\t\t\t\tfor (idx_t i = 0; i < current_batch_size; i++) {\n-\t\t\t\t\t\tif (!col_data.has_nulls || col_data.defined_buf.ptr[i]) {\n-\t\t\t\t\t\t\tauto offset = col_data.offset_buf.read<uint32_t>();\n-\t\t\t\t\t\t\tif (offset >= col_data.string_collection->Count()) {\n-\t\t\t\t\t\t\t\tthrow FormatException(\"string dictionary offset out of bounds\");\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\tauto &chunk = col_data.string_collection->GetChunk(offset / STANDARD_VECTOR_SIZE);\n-\t\t\t\t\t\t\tauto &vec = chunk.data[0];\n-\n-\t\t\t\t\t\t\tout_data_ptr[i + output_offset] =\n-\t\t\t\t\t\t\t    FlatVector::GetData<string_t>(vec)[offset % STANDARD_VECTOR_SIZE];\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tFlatVector::SetNull(output.data[out_col_idx], i + output_offset, true);\n+\t\t\t\t\t\tif (offset >= col_data.string_collection->Count()) {\n+\t\t\t\t\t\t\tthrow FormatException(\"string dictionary offset out of bounds\");\n \t\t\t\t\t\t}\n+\t\t\t\t\t\tauto &chunk = col_data.string_collection->GetChunk(offset / STANDARD_VECTOR_SIZE);\n+\t\t\t\t\t\tauto &vec = chunk.data[0];\n+\n+\t\t\t\t\t\tout_data_ptr[i + output_offset] =\n+\t\t\t\t\t\t    FlatVector::GetData<string_t>(vec)[offset % STANDARD_VECTOR_SIZE];\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tFlatVector::SetNull(out, i + output_offset, true);\n \t\t\t\t\t}\n-\t\t\t\t} break;\n-\t\t\t\tdefault:\n-\t\t\t\t\tthrow FormatException(return_types[file_col_idx].ToString());\n \t\t\t\t}\n-\n-\t\t\t\tbreak;\n+\t\t\t} break;\n+\t\t\tdefault:\n+\t\t\t\tthrow FormatException(return_types[file_col_idx].ToString());\n \t\t\t}\n-\t\t\tcase Encoding::PLAIN:\n-\t\t\t\tD_ASSERT(col_data.payload.ptr);\n-\t\t\t\tswitch (return_types[file_col_idx].id()) {\n-\t\t\t\tcase LogicalTypeId::BOOLEAN: {\n-\t\t\t\t\t// bit packed this\n-\t\t\t\t\tauto target_ptr = FlatVector::GetData<bool>(output.data[out_col_idx]);\n-\t\t\t\t\tfor (idx_t i = 0; i < current_batch_size; i++) {\n-\t\t\t\t\t\tif (col_data.has_nulls && !col_data.defined_buf.ptr[i]) {\n-\t\t\t\t\t\t\tFlatVector::SetNull(output.data[out_col_idx], i + output_offset, true);\n-\t\t\t\t\t\t\tcontinue;\n-\t\t\t\t\t\t}\n-\t\t\t\t\t\tcol_data.payload.available(1);\n-\t\t\t\t\t\ttarget_ptr[i + output_offset] = (*col_data.payload.ptr >> col_data.byte_pos) & 1;\n-\t\t\t\t\t\tcol_data.byte_pos++;\n-\t\t\t\t\t\tif (col_data.byte_pos == 8) {\n-\t\t\t\t\t\t\tcol_data.byte_pos = 0;\n-\t\t\t\t\t\t\tcol_data.payload.inc(1);\n-\t\t\t\t\t\t}\n+\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase Encoding::PLAIN:\n+\t\t\tD_ASSERT(col_data.payload.ptr);\n+\t\t\tswitch (return_types[file_col_idx].id()) {\n+\t\t\tcase LogicalTypeId::BOOLEAN: {\n+\t\t\t\t// bit packed this\n+\t\t\t\tauto target_ptr = FlatVector::GetData<bool>(out);\n+\t\t\t\tfor (idx_t i = 0; i < current_batch_size; i++) {\n+\t\t\t\t\tif (col_data.has_nulls && !col_data.defined_buf.ptr[i]) {\n+\t\t\t\t\t\tFlatVector::SetNull(out, i + output_offset, true);\n+\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t}\n+\t\t\t\t\tcol_data.payload.available(1);\n+\t\t\t\t\ttarget_ptr[i + output_offset] = (*col_data.payload.ptr >> col_data.byte_pos) & 1;\n+\t\t\t\t\tcol_data.byte_pos++;\n+\t\t\t\t\tif (col_data.byte_pos == 8) {\n+\t\t\t\t\t\tcol_data.byte_pos = 0;\n+\t\t\t\t\t\tcol_data.payload.inc(1);\n \t\t\t\t\t}\n-\t\t\t\t\tbreak;\n \t\t\t\t}\n-\t\t\t\tcase LogicalTypeId::INTEGER:\n-\t\t\t\t\tfill_from_plain<int32_t>(col_data, current_batch_size, output.data[out_col_idx], output_offset);\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase LogicalTypeId::BIGINT:\n-\t\t\t\t\tfill_from_plain<int64_t>(col_data, current_batch_size, output.data[out_col_idx], output_offset);\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase LogicalTypeId::FLOAT:\n-\t\t\t\t\tfill_from_plain<float>(col_data, current_batch_size, output.data[out_col_idx], output_offset);\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase LogicalTypeId::DOUBLE:\n-\t\t\t\t\tfill_from_plain<double>(col_data, current_batch_size, output.data[out_col_idx], output_offset);\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase LogicalTypeId::TIMESTAMP:\n-\t\t\t\t\tswitch (s_ele.type) {\n-\t\t\t\t\tcase Type::INT64:\n-\t\t\t\t\t\t// arrow timestamp\n-\t\t\t\t\t\tswitch (s_ele.converted_type) {\n-\t\t\t\t\t\tcase ConvertedType::TIMESTAMP_MICROS:\n-\t\t\t\t\t\t\tfill_timestamp_plain<int64_t, arrow_timestamp_micros_to_timestamp>(\n-\t\t\t\t\t\t\t    col_data, current_batch_size, output.data[out_col_idx], output_offset);\n-\t\t\t\t\t\t\tbreak;\n-\t\t\t\t\t\tcase ConvertedType::TIMESTAMP_MILLIS:\n-\t\t\t\t\t\t\tfill_timestamp_plain<int64_t, arrow_timestamp_ms_to_timestamp>(\n-\t\t\t\t\t\t\t    col_data, current_batch_size, output.data[out_col_idx], output_offset);\n-\t\t\t\t\t\t\tbreak;\n-\t\t\t\t\t\tdefault:\n-\t\t\t\t\t\t\tthrow InternalException(\"Unsupported converted type for timestamp\");\n-\t\t\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tcase LogicalTypeId::INTEGER:\n+\t\t\t\tfill_from_plain<int32_t>(col_data, current_batch_size, filter_mask, out, output_offset);\n+\t\t\t\tbreak;\n+\t\t\tcase LogicalTypeId::BIGINT:\n+\t\t\t\tfill_from_plain<int64_t>(col_data, current_batch_size, filter_mask, out, output_offset);\n+\t\t\t\tbreak;\n+\t\t\tcase LogicalTypeId::FLOAT:\n+\t\t\t\tfill_from_plain<float>(col_data, current_batch_size, filter_mask, out, output_offset);\n+\t\t\t\tbreak;\n+\t\t\tcase LogicalTypeId::DOUBLE:\n+\t\t\t\tfill_from_plain<double>(col_data, current_batch_size, filter_mask, out, output_offset);\n+\t\t\t\tbreak;\n+\t\t\tcase LogicalTypeId::TIMESTAMP:\n+\t\t\t\tswitch (s_ele.type) {\n+\t\t\t\tcase Type::INT64:\n+\t\t\t\t\t// arrow timestamp\n+\t\t\t\t\tswitch (s_ele.converted_type) {\n+\t\t\t\t\tcase ConvertedType::TIMESTAMP_MICROS:\n+\t\t\t\t\t\tfill_timestamp_plain<int64_t, arrow_timestamp_micros_to_timestamp>(\n+\t\t\t\t\t\t    col_data, current_batch_size, filter_mask, out, output_offset);\n \t\t\t\t\t\tbreak;\n-\t\t\t\t\tcase Type::INT96:\n-\t\t\t\t\t\t// impala timestamp\n-\t\t\t\t\t\tfill_timestamp_plain<Int96, impala_timestamp_to_timestamp_t>(\n-\t\t\t\t\t\t    col_data, current_batch_size, output.data[out_col_idx], output_offset);\n+\t\t\t\t\tcase ConvertedType::TIMESTAMP_MILLIS:\n+\t\t\t\t\t\tfill_timestamp_plain<int64_t, arrow_timestamp_ms_to_timestamp>(col_data, current_batch_size,\n+\t\t\t\t\t\t                                                               filter_mask, out, output_offset);\n \t\t\t\t\t\tbreak;\n \t\t\t\t\tdefault:\n-\t\t\t\t\t\tthrow InternalException(\"Unsupported type for timestamp\");\n+\t\t\t\t\t\tthrow InternalException(\"Unsupported converted type for timestamp\");\n \t\t\t\t\t}\n \t\t\t\t\tbreak;\n-\t\t\t\tcase LogicalTypeId::BLOB:\n-\t\t\t\tcase LogicalTypeId::VARCHAR: {\n-\t\t\t\t\tfor (idx_t i = 0; i < current_batch_size; i++) {\n-\t\t\t\t\t\tif (!col_data.has_nulls || col_data.defined_buf.ptr[i]) {\n-\t\t\t\t\t\t\tuint32_t str_len = col_data.payload.read<uint32_t>();\n-\t\t\t\t\t\t\tcol_data.payload.available(str_len);\n-\t\t\t\t\t\t\tVerifyString(return_types[file_col_idx].id(), col_data.payload.ptr, str_len);\n-\t\t\t\t\t\t\tFlatVector::GetData<string_t>(output.data[out_col_idx])[i + output_offset] =\n-\t\t\t\t\t\t\t    StringVector::AddStringOrBlob(output.data[out_col_idx],\n-\t\t\t\t\t\t\t                                  string_t(col_data.payload.ptr, str_len));\n-\t\t\t\t\t\t\tcol_data.payload.inc(str_len);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tFlatVector::SetNull(output.data[out_col_idx], i + output_offset, true);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n+\t\t\t\tcase Type::INT96:\n+\t\t\t\t\t// impala timestamp\n+\t\t\t\t\tfill_timestamp_plain<Int96, impala_timestamp_to_timestamp_t>(col_data, current_batch_size,\n+\t\t\t\t\t                                                             filter_mask, out, output_offset);\n \t\t\t\t\tbreak;\n-\t\t\t\t}\n \t\t\t\tdefault:\n-\t\t\t\t\tthrow FormatException(return_types[file_col_idx].ToString());\n+\t\t\t\t\tthrow InternalException(\"Unsupported type for timestamp\");\n \t\t\t\t}\n-\n \t\t\t\tbreak;\n+\t\t\tcase LogicalTypeId::BLOB:\n+\t\t\tcase LogicalTypeId::VARCHAR: {\n+\t\t\t\tfor (idx_t i = 0; i < current_batch_size; i++) {\n+\t\t\t\t\tif (!col_data.has_nulls || col_data.defined_buf.ptr[i]) {\n+\t\t\t\t\t\tuint32_t str_len = col_data.payload.read<uint32_t>();\n+\n+\t\t\t\t\t\tif (!filter_mask[i + output_offset]) {\n+\t\t\t\t\t\t\tcontinue; // early out if this value is skipped\n+\t\t\t\t\t\t}\n \n+\t\t\t\t\t\tcol_data.payload.available(str_len);\n+\t\t\t\t\t\tVerifyString(return_types[file_col_idx].id(), col_data.payload.ptr, str_len);\n+\t\t\t\t\t\tFlatVector::GetData<string_t>(out)[i + output_offset] =\n+\t\t\t\t\t\t    StringVector::AddStringOrBlob(out, string_t(col_data.payload.ptr, str_len));\n+\t\t\t\t\t\tcol_data.payload.inc(str_len);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tFlatVector::SetNull(out, i + output_offset, true);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\t}\n \t\t\tdefault:\n-\t\t\t\tthrow FormatException(\"Data page has unsupported/invalid encoding\");\n+\t\t\t\tthrow FormatException(return_types[file_col_idx].ToString());\n \t\t\t}\n \n-\t\t\toutput_offset += current_batch_size;\n-\t\t\tcol_data.page_offset += current_batch_size;\n+\t\t\tbreak;\n+\n+\t\tdefault:\n+\t\t\tthrow FormatException(\"Data page has unsupported/invalid encoding\");\n \t\t}\n+\n+\t\toutput_offset += current_batch_size;\n+\t\tcol_data.page_offset += current_batch_size;\n \t}\n-\tstate.group_offset += output.size();\n }\n \n-// statistics handling\n+template <class T, class OP>\n+void templated_filter_operation2(Vector &v, T constant, parquet_filter_t &filter_mask, idx_t count) {\n+\tD_ASSERT(v.vector_type == VectorType::FLAT_VECTOR); // we just created the damn thing it better be\n \n-template <Value (*FUNC)(const_data_ptr_t input)>\n-static unique_ptr<BaseStatistics> templated_get_numeric_stats(const LogicalType &type,\n-                                                              const parquet::format::Statistics &parquet_stats) {\n-\tauto stats = make_unique<NumericStatistics>(type);\n+\tauto v_ptr = FlatVector::GetData<T>(v);\n+\tauto &nullmask = FlatVector::Nullmask(v);\n \n-\t// for reasons unknown to science, Parquet defines *both* `min` and `min_value` as well as `max` and\n-\t// `max_value`. All are optional. such elegance.\n-\tif (parquet_stats.__isset.min) {\n-\t\tstats->min = FUNC((const_data_ptr_t)parquet_stats.min.data());\n-\t} else if (parquet_stats.__isset.min_value) {\n-\t\tstats->min = FUNC((const_data_ptr_t)parquet_stats.min_value.data());\n-\t} else {\n-\t\tstats->min.is_null = true;\n-\t}\n-\tif (parquet_stats.__isset.max) {\n-\t\tstats->max = FUNC((const_data_ptr_t)parquet_stats.max.data());\n-\t} else if (parquet_stats.__isset.max_value) {\n-\t\tstats->max = FUNC((const_data_ptr_t)parquet_stats.max_value.data());\n+\tif (nullmask.any()) {\n+\t\tfor (idx_t i = 0; i < count; i++) {\n+\t\t\tfilter_mask[i] = filter_mask[i] && !(nullmask)[i] && OP::Operation(v_ptr[i], constant);\n+\t\t}\n \t} else {\n-\t\tstats->max.is_null = true;\n+\t\tfor (idx_t i = 0; i < count; i++) {\n+\t\t\tfilter_mask[i] = filter_mask[i] && OP::Operation(v_ptr[i], constant);\n+\t\t}\n \t}\n-    // GCC 4.x insists on a move() here\n-\treturn move(stats);\n }\n \n-template <class T> static Value transform_statistics_plain(const_data_ptr_t input) {\n-\treturn Value(Load<T>(input));\n-}\n+template <class OP>\n+static void templated_filter_operation(Vector &v, Value &constant, parquet_filter_t &filter_mask, idx_t count) {\n+\tif (filter_mask.none() || count == 0) {\n+\t\treturn;\n+\t}\n+\tswitch (v.type.id()) {\n+\tcase LogicalTypeId::BOOLEAN:\n+\t\ttemplated_filter_operation2<bool, OP>(v, constant.value_.boolean, filter_mask, count);\n+\t\tbreak;\n \n-static Value transform_statistics_timestamp_ms(const_data_ptr_t input) {\n-\treturn Value::TIMESTAMP(arrow_timestamp_ms_to_timestamp(Load<int64_t>(input)));\n-}\n+\tcase LogicalTypeId::INTEGER:\n+\t\ttemplated_filter_operation2<int32_t, OP>(v, constant.value_.integer, filter_mask, count);\n+\t\tbreak;\n \n-static Value transform_statistics_timestamp_micros(const_data_ptr_t input) {\n-\treturn Value::TIMESTAMP(arrow_timestamp_micros_to_timestamp(Load<int64_t>(input)));\n+\tcase LogicalTypeId::BIGINT:\n+\t\ttemplated_filter_operation2<int64_t, OP>(v, constant.value_.bigint, filter_mask, count);\n+\t\tbreak;\n+\n+\tcase LogicalTypeId::FLOAT:\n+\t\ttemplated_filter_operation2<float, OP>(v, constant.value_.float_, filter_mask, count);\n+\t\tbreak;\n+\n+\tcase LogicalTypeId::DOUBLE:\n+\t\ttemplated_filter_operation2<double, OP>(v, constant.value_.double_, filter_mask, count);\n+\t\tbreak;\n+\n+\tcase LogicalTypeId::TIMESTAMP:\n+\t\ttemplated_filter_operation2<timestamp_t, OP>(v, constant.value_.bigint, filter_mask, count);\n+\t\tbreak;\n+\n+\tcase LogicalTypeId::BLOB:\n+\tcase LogicalTypeId::VARCHAR:\n+\t\ttemplated_filter_operation2<string_t, OP>(v, string_t(constant.str_value), filter_mask, count);\n+\t\tbreak;\n+\n+\tdefault:\n+\t\tthrow NotImplementedException(\"Unsupported type for filter %s\", v.ToString());\n+\t}\n }\n \n-static Value transform_statistics_timestamp_impala(const_data_ptr_t input) {\n-\treturn Value::TIMESTAMP(impala_timestamp_to_timestamp_t(Load<Int96>(input)));\n+void ParquetReader::Scan(ParquetReaderScanState &state, DataChunk &result) {\n+\twhile (ScanInternal(state, result)) {\n+\t\tif (result.size() > 0) {\n+\t\t\tbreak;\n+\t\t}\n+\t\tresult.Reset();\n+\t}\n }\n \n-unique_ptr<BaseStatistics> ParquetReader::ReadStatistics(LogicalType &type, column_t column_index,\n-                                                         const parquet::format::FileMetaData *file_meta_data) {\n-\tunique_ptr<BaseStatistics> column_stats;\n+bool ParquetReader::ScanInternal(ParquetReaderScanState &state, DataChunk &result) {\n+\tif (state.finished) {\n+\t\treturn false;\n+\t}\n \n-\tfor (auto &row_group : file_meta_data->row_groups) {\n-\t\tD_ASSERT(column_index < row_group.columns.size());\n-\t\tauto &column_chunk = row_group.columns[column_index];\n-\t\tif (!column_chunk.__isset.meta_data || !column_chunk.meta_data.__isset.statistics) {\n-\t\t\t// no stats present for row group\n-\t\t\treturn nullptr;\n+\t// see if we have to switch to the next row group in the parquet file\n+\tif (state.current_group < 0 || (int64_t)state.group_offset >= GetGroup(state).num_rows) {\n+\t\tstate.current_group++;\n+\t\tstate.group_offset = 0;\n+\n+\t\tif ((idx_t)state.current_group == state.group_idx_list.size()) {\n+\t\t\tstate.finished = true;\n+\t\t\treturn false;\n \t\t}\n-\t\tauto &parquet_stats = column_chunk.meta_data.statistics;\n-\t\tunique_ptr<BaseStatistics> row_group_stats;\n \n-\t\tswitch (type.id()) {\n-\t\tcase LogicalTypeId::INTEGER:\n-\t\t\trow_group_stats = templated_get_numeric_stats<transform_statistics_plain<int32_t>>(type, parquet_stats);\n-\t\t\tbreak;\n+\t\tfor (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {\n+\t\t\tauto file_col_idx = state.column_ids[out_col_idx];\n \n-\t\tcase LogicalTypeId::BIGINT:\n-\t\t\trow_group_stats = templated_get_numeric_stats<transform_statistics_plain<int64_t>>(type, parquet_stats);\n-\t\t\tbreak;\n+\t\t\t// this is a special case where we are not interested in the actual contents of the file\n+\t\t\tif (file_col_idx == COLUMN_IDENTIFIER_ROW_ID) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n \n-\t\tcase LogicalTypeId::FLOAT:\n-\t\t\trow_group_stats = templated_get_numeric_stats<transform_statistics_plain<float>>(type, parquet_stats);\n-\t\t\tbreak;\n+\t\t\tPrepareRowGroupBuffer(state, file_col_idx, result.GetTypes()[out_col_idx]);\n+\t\t\t// trigger the reading of a new page in FillColumn\n+\t\t\tstate.column_data[file_col_idx]->page_value_count = 0;\n+\t\t}\n+\t\treturn true;\n+\t}\n \n-\t\tcase LogicalTypeId::DOUBLE:\n-\t\t\trow_group_stats = templated_get_numeric_stats<transform_statistics_plain<double>>(type, parquet_stats);\n-\t\t\tbreak;\n+\tauto this_output_chunk_rows = MinValue<idx_t>(STANDARD_VECTOR_SIZE, GetGroup(state).num_rows - state.group_offset);\n+\tresult.SetCardinality(this_output_chunk_rows);\n \n-\t\t\t// here we go, our favorite type\n-\t\tcase LogicalTypeId::TIMESTAMP: {\n-\t\t\tauto &s_ele = file_meta_data->schema[column_index + 1];\n-\t\t\tswitch (s_ele.type) {\n-\t\t\tcase Type::INT64:\n-\t\t\t\t// arrow timestamp\n-\t\t\t\tswitch (s_ele.converted_type) {\n-\t\t\t\tcase ConvertedType::TIMESTAMP_MICROS:\n-\t\t\t\t\trow_group_stats =\n-\t\t\t\t\t    templated_get_numeric_stats<transform_statistics_timestamp_micros>(type, parquet_stats);\n+\tif (this_output_chunk_rows == 0) {\n+\t\tstate.finished = true;\n+\t\treturn false; // end of last group, we are done\n+\t}\n+\n+\t// we evaluate simple table filters directly in this scan so we can skip decoding column data that's never going to\n+\t// be relevant\n+\tparquet_filter_t filter_mask;\n+\tfilter_mask.set();\n+\n+\tif (state.filters) {\n+\t\tvector<bool> need_to_read(result.ColumnCount(), true);\n+\n+\t\t// first load the columns that are used in filters\n+\t\tfor (auto &filter_col : state.filters->filters) {\n+\t\t\tif (filter_mask.none()) { // if no rows are left we can stop checking filters\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tScanColumn(state, filter_mask, result.size(), filter_col.first, result.data[filter_col.first]);\n+\t\t\tneed_to_read[filter_col.first] = false;\n+\n+\t\t\tfor (auto &filter : filter_col.second) {\n+\t\t\t\tswitch (filter.comparison_type) {\n+\t\t\t\tcase ExpressionType::COMPARE_EQUAL:\n+\t\t\t\t\ttemplated_filter_operation<Equals>(result.data[filter_col.first], filter.constant, filter_mask,\n+\t\t\t\t\t                                   this_output_chunk_rows);\n \t\t\t\t\tbreak;\n-\t\t\t\tcase ConvertedType::TIMESTAMP_MILLIS:\n-\t\t\t\t\trow_group_stats =\n-\t\t\t\t\t    templated_get_numeric_stats<transform_statistics_timestamp_ms>(type, parquet_stats);\n+\t\t\t\tcase ExpressionType::COMPARE_LESSTHAN:\n+\t\t\t\t\ttemplated_filter_operation<LessThan>(result.data[filter_col.first], filter.constant, filter_mask,\n+\t\t\t\t\t                                     this_output_chunk_rows);\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase ExpressionType::COMPARE_LESSTHANOREQUALTO:\n+\t\t\t\t\ttemplated_filter_operation<LessThanEquals>(result.data[filter_col.first], filter.constant,\n+\t\t\t\t\t                                           filter_mask, this_output_chunk_rows);\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase ExpressionType::COMPARE_GREATERTHAN:\n+\t\t\t\t\ttemplated_filter_operation<GreaterThan>(result.data[filter_col.first], filter.constant, filter_mask,\n+\t\t\t\t\t                                        this_output_chunk_rows);\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase ExpressionType::COMPARE_GREATERTHANOREQUALTO:\n+\t\t\t\t\ttemplated_filter_operation<GreaterThanEquals>(result.data[filter_col.first], filter.constant,\n+\t\t\t\t\t                                              filter_mask, this_output_chunk_rows);\n \t\t\t\t\tbreak;\n \t\t\t\tdefault:\n-\t\t\t\t\treturn nullptr;\n+\t\t\t\t\tD_ASSERT(0);\n \t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase Type::INT96:\n-\t\t\t\t// impala timestamp\n-\t\t\t\trow_group_stats =\n-\t\t\t\t    templated_get_numeric_stats<transform_statistics_timestamp_impala>(type, parquet_stats);\n-\t\t\t\tbreak;\n-\t\t\tdefault:\n-\t\t\t\treturn nullptr;\n \t\t\t}\n-\t\t\tbreak;\n \t\t}\n-\t\tcase LogicalTypeId::VARCHAR: {\n-\t\t\tauto string_stats = make_unique<StringStatistics>(type);\n-\t\t\tif (parquet_stats.__isset.min) {\n-\t\t\t\tmemcpy(string_stats->min, (data_ptr_t)parquet_stats.min.data(),\n-\t\t\t\t       MinValue<idx_t>(parquet_stats.min.size(), StringStatistics::MAX_STRING_MINMAX_SIZE));\n-\t\t\t} else if (parquet_stats.__isset.min_value) {\n-\t\t\t\tmemcpy(string_stats->min, (data_ptr_t)parquet_stats.min_value.data(),\n-\t\t\t\t       MinValue<idx_t>(parquet_stats.min_value.size(), StringStatistics::MAX_STRING_MINMAX_SIZE));\n-\t\t\t} else {\n-\t\t\t\treturn nullptr;\n-\t\t\t}\n-\t\t\tif (parquet_stats.__isset.max) {\n-\t\t\t\tmemcpy(string_stats->max, (data_ptr_t)parquet_stats.max.data(),\n-\t\t\t\t       MinValue<idx_t>(parquet_stats.max.size(), StringStatistics::MAX_STRING_MINMAX_SIZE));\n-\t\t\t} else if (parquet_stats.__isset.max_value) {\n-\t\t\t\tmemcpy(string_stats->max, (data_ptr_t)parquet_stats.max_value.data(),\n-\t\t\t\t       MinValue<idx_t>(parquet_stats.max_value.size(), StringStatistics::MAX_STRING_MINMAX_SIZE));\n-\t\t\t} else {\n-\t\t\t\treturn nullptr;\n-\t\t\t}\n \n-\t\t\tstring_stats->has_unicode = true; // we dont know better\n-\t\t\trow_group_stats = move(string_stats);\n-\t\t\tbreak;\n+\t\t// we still may have to read some cols\n+\t\tfor (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {\n+\t\t\tif (need_to_read[out_col_idx]) {\n+\t\t\t\tScanColumn(state, filter_mask, result.size(), out_col_idx, result.data[out_col_idx]);\n+\t\t\t}\n \t\t}\n-\t\tdefault:\n-\t\t\t// no stats for you\n-\t\t\tbreak;\n-\t\t} // end of type switch\n \n-\t\t// null count is generic\n-\t\tif (row_group_stats) {\n-\t\t\tif (parquet_stats.__isset.null_count) {\n-\t\t\t\trow_group_stats->has_null = parquet_stats.null_count > 0;\n-\t\t\t} else {\n-\t\t\t\trow_group_stats->has_null = true;\n+\t\tidx_t sel_size = 0;\n+\t\tfor (idx_t i = 0; i < this_output_chunk_rows; i++) {\n+\t\t\tif (filter_mask[i]) {\n+\t\t\t\tstate.sel.set_index(sel_size++, i);\n \t\t\t}\n-\t\t} else {\n-\t\t\t// if stats are missing from any row group we know squat\n-\t\t\treturn nullptr;\n \t\t}\n \n-\t\tif (!column_stats) {\n-\t\t\tcolumn_stats = move(row_group_stats);\n-\t\t} else {\n-\t\t\tcolumn_stats->Merge(*row_group_stats);\n+\t\tresult.Slice(state.sel, sel_size);\n+\t\tresult.Verify();\n+\n+\t} else { // just fricking load the data\n+\t\tfor (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {\n+\t\t\tScanColumn(state, filter_mask, result.size(), out_col_idx, result.data[out_col_idx]);\n \t\t}\n \t}\n-\treturn column_stats;\n+\n+\tstate.group_offset += this_output_chunk_rows;\n+\treturn true; // thank you scan again\n }\n \n } // namespace duckdb\n",
  "test_patch": "diff --git a/test/sql/copy/parquet/test_parquet_filter_pushdown.test b/test/sql/copy/parquet/test_parquet_filter_pushdown.test\nnew file mode 100644\nindex 000000000000..cbd5f44cb37c\n--- /dev/null\n+++ b/test/sql/copy/parquet/test_parquet_filter_pushdown.test\n@@ -0,0 +1,72 @@\n+# name: test/sql/copy/parquet/test_parquet_filter_pushdown.test\n+# description: Test basic parquet reading\n+# group: [parquet]\n+\n+require parquet\n+require vector_size 512\n+\n+statement ok\n+pragma enable_verification\n+\n+# userdata1.parquet\n+query I\n+SELECT COUNT(*) FROM parquet_scan('test/sql/copy/parquet/data/userdata1.parquet') where id > 500\n+----\n+500\n+\n+query I\n+SELECT COUNT(*) FROM parquet_scan('test/sql/copy/parquet/data/userdata1.parquet') where id < 500\n+----\n+499\n+\n+query I\n+SELECT COUNT(*) FROM parquet_scan('test/sql/copy/parquet/data/userdata1.parquet') where id > 100 and id < 900\n+----\n+799\n+\n+query I\n+SELECT COUNT(*) FROM parquet_scan('test/sql/copy/parquet/data/userdata1.parquet') where id between 100 and 900\n+----\n+801\n+\n+query I\n+SELECT COUNT(*) FROM parquet_scan('test/sql/copy/parquet/data/userdata1.parquet') where id = 42\n+----\n+1\n+\n+query I\n+SELECT COUNT(*) FROM parquet_scan('test/sql/copy/parquet/data/userdata1.parquet') where salary < 1000\n+----\n+0\n+\n+query I\n+SELECT COUNT(*) FROM parquet_scan('test/sql/copy/parquet/data/userdata1.parquet') where gender = 'Male' and first_name = 'Mark'\n+----\n+10\n+\n+\n+query I\n+SELECT last_name FROM parquet_scan('test/sql/copy/parquet/data/userdata1.parquet') where first_name > 'Mark' and country > 'Germany' and salary > 0 order by last_name limit 10\n+----\n+Adams\n+Adams\n+Allen\n+Allen\n+Allen\n+Alvarez\n+Alvarez\n+Alvarez\n+Arnold\n+Arnold\n+\n+\n+\n+query I\n+SELECT length(l_comment) FROM parquet_scan('test/sql/copy/parquet/data/lineitem-top10000.gzip.parquet') where l_orderkey = 1 order by l_comment\n+----\n+24\n+17\n+23\n+23\n+34\n+29\n\\ No newline at end of file\ndiff --git a/test/sql/copy/parquet/test_parquet_stats.test b/test/sql/copy/parquet/test_parquet_stats.test\nindex fdb1e7c991bd..719cc3ac6c26 100644\n--- a/test/sql/copy/parquet/test_parquet_stats.test\n+++ b/test/sql/copy/parquet/test_parquet_stats.test\n@@ -18,7 +18,6 @@ query I nosort empty\n explain select * from parquet_scan('test/sql/copy/parquet/data/userdata1.parquet') where id is null;\n ----\n \n-\n # verify min/max stats on int cols work\n query I nosort empty\n explain select * from parquet_scan('test/sql/copy/parquet/data/userdata1.parquet') where id < 1;\n",
  "problem_statement": "Filter pushdown in Parquet reader\nCurrently the Parquet reader does not evaluate filter conditions directly in the scan. This should be addressed.\r\n\r\nWe already have all the infrastructure for this in place, and we will also compare the filters to the Parquet column statistics that we now can read since #1167, and possibly skip entire row groups. We should also do that for individual pages. \n",
  "hints_text": "",
  "created_at": "2020-12-03T10:13:38Z"
}