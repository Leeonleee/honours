You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Filter pushdown in Parquet reader
Currently the Parquet reader does not evaluate filter conditions directly in the scan. This should be addressed.

We already have all the infrastructure for this in place, and we will also compare the filters to the Parquet column statistics that we now can read since #1167, and possibly skip entire row groups. We should also do that for individual pages. 

</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
2: 
3: ![.github/workflows/main.yml](https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
6: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3901452.svg)](https://zenodo.org/record/3901452)
7: 
8: 
9: ## Installation
10: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
11: 
12: ## Development
13: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
14: 
15: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
16: 
17: 
[end of README.md]
[start of extension/parquet/include/parquet_reader.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // parquet_reader.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/exception.hpp"
13: #include "duckdb/common/string_util.hpp"
14: #include "duckdb/common/types/data_chunk.hpp"
15: #include "resizable_buffer.hpp"
16: 
17: #include "parquet_file_metadata_cache.hpp"
18: #include "parquet_types.h"
19: 
20: #include <exception>
21: 
22: namespace parquet {
23: namespace format {
24: class FileMetaData;
25: }
26: } // namespace parquet
27: 
28: namespace duckdb {
29: class ClientContext;
30: class RleBpDecoder;
31: class ChunkCollection;
32: class BaseStatistics;
33: 
34: struct ParquetReaderColumnData {
35: 	~ParquetReaderColumnData();
36: 
37: 	idx_t chunk_offset;
38: 
39: 	idx_t page_offset;
40: 	idx_t page_value_count = 0;
41: 
42: 	idx_t dict_size;
43: 
44: 	uint8_t byte_pos = 0; // to decode plain booleans from bit fields
45: 
46: 	ResizeableBuffer buf;
47: 	ResizeableBuffer decompressed_buf; // only used for compressed files
48: 	ResizeableBuffer dict;
49: 	ResizeableBuffer offset_buf;
50: 	ResizeableBuffer defined_buf;
51: 
52: 	ByteBuffer payload;
53: 
54: 	parquet::format::Encoding::type page_encoding;
55: 	// these point into buf or decompressed_buf
56: 	unique_ptr<RleBpDecoder> defined_decoder;
57: 	unique_ptr<RleBpDecoder> dict_decoder;
58: 
59: 	unique_ptr<ChunkCollection> string_collection;
60: 
61: 	bool has_nulls;
62: };
63: 
64: struct ParquetReaderScanState {
65: 	vector<idx_t> group_idx_list;
66: 	int64_t current_group;
67: 	vector<column_t> column_ids;
68: 	idx_t group_offset;
69: 	vector<unique_ptr<ParquetReaderColumnData>> column_data;
70: 	bool finished;
71: };
72: 
73: class ParquetReader {
74: public:
75: 	ParquetReader(ClientContext &context, string file_name, vector<LogicalType> expected_types,
76: 	              string initial_filename = string());
77: 	ParquetReader(ClientContext &context, string file_name) : ParquetReader(context, file_name, vector<LogicalType>()) {
78: 	}
79: 	~ParquetReader();
80: 
81: 	string file_name;
82: 	vector<LogicalType> return_types;
83: 	vector<string> names;
84: 
85: 	shared_ptr<ParquetFileMetadataCache> metadata;
86: 
87: public:
88: 	void Initialize(ParquetReaderScanState &state, vector<column_t> column_ids, vector<idx_t> groups_to_read);
89: 	void ReadChunk(ParquetReaderScanState &state, DataChunk &output);
90: 
91: 	idx_t NumRows();
92: 	idx_t NumRowGroups();
93: 
94: 	const parquet::format::FileMetaData *GetFileMetadata();
95: 
96: 	static unique_ptr<BaseStatistics> ReadStatistics(LogicalType &type, column_t column_index,
97: 	                                                 const parquet::format::FileMetaData *file_meta_data);
98: 
99: private:
100: 	const parquet::format::RowGroup &GetGroup(ParquetReaderScanState &state);
101: 	void PrepareChunkBuffer(ParquetReaderScanState &state, idx_t col_idx);
102: 	bool PreparePageBuffers(ParquetReaderScanState &state, idx_t col_idx);
103: 	void VerifyString(LogicalTypeId id, const char *str_data, idx_t str_len);
104: 
105: 	template <typename... Args> std::runtime_error FormatException(const string fmt_str, Args... params) {
106: 		return std::runtime_error("Failed to read Parquet file \"" + file_name +
107: 		                          "\": " + StringUtil::Format(fmt_str, params...));
108: 	}
109: 
110: 	template <class T>
111: 	void fill_from_dict(ParquetReaderColumnData &col_data, idx_t count, Vector &target, idx_t target_offset);
112: 	template <class T>
113: 	void fill_from_plain(ParquetReaderColumnData &col_data, idx_t count, Vector &target, idx_t target_offset);
114: 
115: private:
116: 	ClientContext &context;
117: };
118: 
119: } // namespace duckdb
[end of extension/parquet/include/parquet_reader.hpp]
[start of extension/parquet/parquet-extension.cpp]
1: #include <string>
2: #include <vector>
3: #include <bitset>
4: #include <fstream>
5: #include <cstring>
6: #include <iostream>
7: #include <sstream>
8: 
9: #include "parquet-extension.hpp"
10: #include "parquet_reader.hpp"
11: #include "parquet_writer.hpp"
12: 
13: #include "duckdb.hpp"
14: #include "duckdb/common/types/chunk_collection.hpp"
15: #include "duckdb/function/copy_function.hpp"
16: #include "duckdb/function/table_function.hpp"
17: #include "duckdb/common/file_system.hpp"
18: #include "duckdb/parallel/parallel_state.hpp"
19: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
20: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
21: 
22: #include "duckdb/storage/statistics/base_statistics.hpp"
23: 
24: #include "duckdb/main/client_context.hpp"
25: #include "duckdb/catalog/catalog.hpp"
26: 
27: namespace duckdb {
28: 
29: struct ParquetReadBindData : public FunctionData {
30: 	shared_ptr<ParquetReader> initial_reader;
31: 	vector<string> files;
32: 	vector<column_t> column_ids;
33: };
34: 
35: struct ParquetReadOperatorData : public FunctionOperatorData {
36: 	shared_ptr<ParquetReader> reader;
37: 	ParquetReaderScanState scan_state;
38: 	bool is_parallel;
39: 	idx_t file_index;
40: 	vector<column_t> column_ids;
41: };
42: 
43: struct ParquetReadParallelState : public ParallelState {
44: 	std::mutex lock;
45: 	shared_ptr<ParquetReader> current_reader;
46: 	idx_t file_index;
47: 	idx_t row_group_index;
48: };
49: 
50: class ParquetScanFunction : public TableFunction {
51: public:
52: 	ParquetScanFunction()
53: 	    : TableFunction("parquet_scan", {LogicalType::VARCHAR}, parquet_scan_function, parquet_scan_bind,
54: 	                    parquet_scan_init, /* statistics */ parquet_scan_stats, /* cleanup */ nullptr,
55: 	                    /* dependency */ nullptr, parquet_cardinality,
56: 	                    /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr, parquet_max_threads,
57: 	                    parquet_init_parallel_state, parquet_scan_parallel_init, parquet_parallel_state_next) {
58: 		projection_pushdown = true;
59: 	}
60: 
61: 	static unique_ptr<FunctionData> parquet_read_bind(ClientContext &context, CopyInfo &info,
62: 	                                                  vector<string> &expected_names,
63: 	                                                  vector<LogicalType> &expected_types) {
64: 		for (auto &option : info.options) {
65: 			throw NotImplementedException("Unsupported option for COPY FROM parquet: %s", option.first);
66: 		}
67: 		auto result = make_unique<ParquetReadBindData>();
68: 
69: 		FileSystem &fs = FileSystem::GetFileSystem(context);
70: 		result->files = fs.Glob(info.file_path);
71: 		if (result->files.empty()) {
72: 			throw IOException("No files found that match the pattern \"%s\"", info.file_path);
73: 		}
74: 		result->initial_reader = make_shared<ParquetReader>(context, result->files[0], expected_types);
75: 		return move(result);
76: 	}
77: 
78: 	static unique_ptr<BaseStatistics> parquet_scan_stats(ClientContext &context, const FunctionData *bind_data_,
79: 	                                                     column_t column_index) {
80: 		auto &bind_data = (ParquetReadBindData &)*bind_data_;
81: 
82: 		if (column_index == COLUMN_IDENTIFIER_ROW_ID) {
83: 			return nullptr;
84: 		}
85: 
86: 		// we do not want to parse the Parquet metadata for the sole purpose of getting column statistics
87: 
88: 		// We already parsed the metadata for the first file in a glob because we need some type info.
89: 		auto overall_stats =
90: 		    ParquetReader::ReadStatistics(bind_data.initial_reader->return_types[column_index], column_index,
91: 		                                  bind_data.initial_reader->metadata->metadata.get());
92: 
93: 		if (!overall_stats) {
94: 			return nullptr;
95: 		}
96: 
97: 		// if there is only one file in the glob (quite common case), we are done
98: 		if (bind_data.files.size() < 2) {
99: 			return overall_stats;
100: 		} else if (context.db.config.object_cache_enable) {
101: 			// for more than one file, we could be lucky and metadata for *every* file is in the object cache (if
102: 			// enabled at all)
103: 			FileSystem &fs = FileSystem::GetFileSystem(context);
104: 			for (idx_t file_idx = 1; file_idx < bind_data.files.size(); file_idx++) {
105: 				auto &file_name = bind_data.files[file_idx];
106: 				auto metadata = dynamic_pointer_cast<ParquetFileMetadataCache>(context.db.object_cache->Get(file_name));
107: 				auto handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ);
108: 				// but we need to check if the metadata cache entries are current
109: 				if (!metadata || (fs.GetLastModifiedTime(*handle) >= metadata->read_time)) {
110: 					// missing or invalid metadata entry in cache, no usable stats overall
111: 					return nullptr;
112: 				}
113: 				// get and merge stats for file
114: 				auto file_stats = ParquetReader::ReadStatistics(bind_data.initial_reader->return_types[column_index],
115: 				                                                column_index, metadata->metadata.get());
116: 				if (!file_stats) {
117: 					return nullptr;
118: 				}
119: 				overall_stats->Merge(*file_stats);
120: 			}
121: 			// success!
122: 			return overall_stats;
123: 		}
124: 		// we have more than one file and no object cache so no statistics overall
125: 		return nullptr;
126: 	}
127: 
128: 	static unique_ptr<FunctionData> parquet_scan_bind(ClientContext &context, vector<Value> &inputs,
129: 	                                                  unordered_map<string, Value> &named_parameters,
130: 	                                                  vector<LogicalType> &return_types, vector<string> &names) {
131: 		auto file_name = inputs[0].GetValue<string>();
132: 		auto result = make_unique<ParquetReadBindData>();
133: 
134: 		FileSystem &fs = FileSystem::GetFileSystem(context);
135: 		result->files = fs.Glob(file_name);
136: 		if (result->files.empty()) {
137: 			throw IOException("No files found that match the pattern \"%s\"", file_name);
138: 		}
139: 
140: 		result->initial_reader = make_shared<ParquetReader>(context, result->files[0]);
141: 		return_types = result->initial_reader->return_types;
142: 
143: 		names = result->initial_reader->names;
144: 		return move(result);
145: 	}
146: 
147: 	static unique_ptr<FunctionOperatorData> parquet_scan_init(ClientContext &context, const FunctionData *bind_data_,
148: 	                                                          vector<column_t> &column_ids,
149: 	                                                          TableFilterSet *table_filters) {
150: 		auto &bind_data = (ParquetReadBindData &)*bind_data_;
151: 
152: 		auto result = make_unique<ParquetReadOperatorData>();
153: 		result->column_ids = column_ids;
154: 
155: 		result->is_parallel = false;
156: 		result->file_index = 0;
157: 		// single-threaded: one thread has to read all groups
158: 		vector<idx_t> group_ids;
159: 		for (idx_t i = 0; i < bind_data.initial_reader->NumRowGroups(); i++) {
160: 			group_ids.push_back(i);
161: 		}
162: 		result->reader = bind_data.initial_reader;
163: 		result->reader->Initialize(result->scan_state, column_ids, move(group_ids));
164: 		return move(result);
165: 	}
166: 
167: 	static unique_ptr<FunctionOperatorData>
168: 	parquet_scan_parallel_init(ClientContext &context, const FunctionData *bind_data_, ParallelState *parallel_state_,
169: 	                           vector<column_t> &column_ids, TableFilterSet *table_filters) {
170: 		auto result = make_unique<ParquetReadOperatorData>();
171: 		result->column_ids = column_ids;
172: 		result->is_parallel = true;
173: 		if (!parquet_parallel_state_next(context, bind_data_, result.get(), parallel_state_)) {
174: 			return nullptr;
175: 		}
176: 		return move(result);
177: 	}
178: 
179: 	static void parquet_scan_function(ClientContext &context, const FunctionData *bind_data_,
180: 	                                  FunctionOperatorData *operator_state, DataChunk &output) {
181: 		auto &data = (ParquetReadOperatorData &)*operator_state;
182: 		do {
183: 			data.reader->ReadChunk(data.scan_state, output);
184: 			if (output.size() == 0 && !data.is_parallel) {
185: 				auto &bind_data = (ParquetReadBindData &)*bind_data_;
186: 				// check if there is another file
187: 				if (data.file_index + 1 < bind_data.files.size()) {
188: 					data.file_index++;
189: 					string file = bind_data.files[data.file_index];
190: 					// move to the next file
191: 					data.reader =
192: 					    make_shared<ParquetReader>(context, file, data.reader->return_types, bind_data.files[0]);
193: 					vector<idx_t> group_ids;
194: 					for (idx_t i = 0; i < data.reader->NumRowGroups(); i++) {
195: 						group_ids.push_back(i);
196: 					}
197: 					data.reader->Initialize(data.scan_state, data.column_ids, move(group_ids));
198: 				} else {
199: 					// exhausted all the files: done
200: 					break;
201: 				}
202: 			} else {
203: 				break;
204: 			}
205: 		} while (true);
206: 	}
207: 
208: 	static unique_ptr<NodeStatistics> parquet_cardinality(ClientContext &context, const FunctionData *bind_data) {
209: 		auto &data = (ParquetReadBindData &)*bind_data;
210: 		return make_unique<NodeStatistics>(data.initial_reader->NumRows() * data.files.size());
211: 	}
212: 
213: 	static idx_t parquet_max_threads(ClientContext &context, const FunctionData *bind_data) {
214: 		auto &data = (ParquetReadBindData &)*bind_data;
215: 		return data.initial_reader->NumRowGroups() * data.files.size();
216: 	}
217: 
218: 	static unique_ptr<ParallelState> parquet_init_parallel_state(ClientContext &context,
219: 	                                                             const FunctionData *bind_data_) {
220: 		auto &bind_data = (ParquetReadBindData &)*bind_data_;
221: 		auto result = make_unique<ParquetReadParallelState>();
222: 		result->current_reader = bind_data.initial_reader;
223: 		result->row_group_index = 0;
224: 		result->file_index = 0;
225: 		return move(result);
226: 	}
227: 
228: 	static bool parquet_parallel_state_next(ClientContext &context, const FunctionData *bind_data_,
229: 	                                        FunctionOperatorData *state_, ParallelState *parallel_state_) {
230: 		auto &bind_data = (ParquetReadBindData &)*bind_data_;
231: 		auto &parallel_state = (ParquetReadParallelState &)*parallel_state_;
232: 		auto &scan_data = (ParquetReadOperatorData &)*state_;
233: 
234: 		lock_guard<mutex> parallel_lock(parallel_state.lock);
235: 		if (parallel_state.row_group_index < parallel_state.current_reader->NumRowGroups()) {
236: 			// groups remain in the current parquet file: read the next group
237: 			scan_data.reader = parallel_state.current_reader;
238: 			vector<idx_t> group_indexes{parallel_state.row_group_index};
239: 			scan_data.reader->Initialize(scan_data.scan_state, scan_data.column_ids, group_indexes);
240: 			parallel_state.row_group_index++;
241: 			return true;
242: 		} else {
243: 			// no groups remain in the current parquet file: check if there are more files to read
244: 			while (parallel_state.file_index + 1 < bind_data.files.size()) {
245: 				// read the next file
246: 				string file = bind_data.files[++parallel_state.file_index];
247: 				parallel_state.current_reader =
248: 				    make_shared<ParquetReader>(context, file, parallel_state.current_reader->return_types);
249: 				if (parallel_state.current_reader->NumRowGroups() == 0) {
250: 					// empty parquet file, move to next file
251: 					continue;
252: 				}
253: 				// set up the scan state to read the first group
254: 				scan_data.reader = parallel_state.current_reader;
255: 				vector<idx_t> group_indexes{0};
256: 				scan_data.reader->Initialize(scan_data.scan_state, scan_data.column_ids, group_indexes);
257: 				parallel_state.row_group_index = 1;
258: 				return true;
259: 			}
260: 		}
261: 		return false;
262: 	}
263: };
264: 
265: struct ParquetWriteBindData : public FunctionData {
266: 	vector<LogicalType> sql_types;
267: 	string file_name;
268: 	vector<string> column_names;
269: 	parquet::format::CompressionCodec::type codec = parquet::format::CompressionCodec::SNAPPY;
270: };
271: 
272: struct ParquetWriteGlobalState : public GlobalFunctionData {
273: 	unique_ptr<ParquetWriter> writer;
274: };
275: 
276: struct ParquetWriteLocalState : public LocalFunctionData {
277: 	ParquetWriteLocalState() {
278: 		buffer = make_unique<ChunkCollection>();
279: 	}
280: 
281: 	unique_ptr<ChunkCollection> buffer;
282: };
283: 
284: unique_ptr<FunctionData> parquet_write_bind(ClientContext &context, CopyInfo &info, vector<string> &names,
285:                                             vector<LogicalType> &sql_types) {
286: 	auto bind_data = make_unique<ParquetWriteBindData>();
287: 	for (auto &option : info.options) {
288: 		auto loption = StringUtil::Lower(option.first);
289: 		if (loption == "compression" || loption == "codec") {
290: 			if (option.second.size() > 0) {
291: 				auto roption = StringUtil::Lower(option.second[0].ToString());
292: 				if (roption == "uncompressed") {
293: 					bind_data->codec = parquet::format::CompressionCodec::UNCOMPRESSED;
294: 					continue;
295: 				} else if (roption == "snappy") {
296: 					bind_data->codec = parquet::format::CompressionCodec::SNAPPY;
297: 					continue;
298: 				} else if (roption == "gzip") {
299: 					bind_data->codec = parquet::format::CompressionCodec::GZIP;
300: 					continue;
301: 				} else if (roption == "zstd") {
302: 					bind_data->codec = parquet::format::CompressionCodec::ZSTD;
303: 					continue;
304: 				}
305: 			}
306: 			throw ParserException("Expected %s argument to be either [uncompressed, snappy, gzip or zstd]", loption);
307: 		} else {
308: 			throw NotImplementedException("Unrecognized option for PARQUET: %s", option.first.c_str());
309: 		}
310: 	}
311: 	bind_data->sql_types = sql_types;
312: 	bind_data->column_names = names;
313: 	bind_data->file_name = info.file_path;
314: 	return move(bind_data);
315: }
316: 
317: unique_ptr<GlobalFunctionData> parquet_write_initialize_global(ClientContext &context, FunctionData &bind_data) {
318: 	auto global_state = make_unique<ParquetWriteGlobalState>();
319: 	auto &parquet_bind = (ParquetWriteBindData &)bind_data;
320: 
321: 	auto &fs = FileSystem::GetFileSystem(context);
322: 	global_state->writer =
323: 	    make_unique<ParquetWriter>(fs, parquet_bind.file_name, parquet_bind.sql_types, parquet_bind.column_names, parquet_bind.codec);
324: 	return move(global_state);
325: }
326: 
327: void parquet_write_sink(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
328:                         LocalFunctionData &lstate, DataChunk &input) {
329: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
330: 	auto &local_state = (ParquetWriteLocalState &)lstate;
331: 
332: 	// append data to the local (buffered) chunk collection
333: 	local_state.buffer->Append(input);
334: 	if (local_state.buffer->Count() > 100000) {
335: 		// if the chunk collection exceeds a certain size we flush it to the parquet file
336: 		global_state.writer->Flush(*local_state.buffer);
337: 		// and reset the buffer
338: 		local_state.buffer = make_unique<ChunkCollection>();
339: 	}
340: }
341: 
342: void parquet_write_combine(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
343:                            LocalFunctionData &lstate) {
344: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
345: 	auto &local_state = (ParquetWriteLocalState &)lstate;
346: 	// flush any data left in the local state to the file
347: 	global_state.writer->Flush(*local_state.buffer);
348: }
349: 
350: void parquet_write_finalize(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate) {
351: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
352: 	// finalize: write any additional metadata to the file here
353: 	global_state.writer->Finalize();
354: }
355: 
356: unique_ptr<LocalFunctionData> parquet_write_initialize_local(ClientContext &context, FunctionData &bind_data) {
357: 	return make_unique<ParquetWriteLocalState>();
358: }
359: 
360: void ParquetExtension::Load(DuckDB &db) {
361: 	ParquetScanFunction scan_fun;
362: 	CreateTableFunctionInfo cinfo(scan_fun);
363: 	cinfo.name = "read_parquet";
364: 	CreateTableFunctionInfo pq_scan = cinfo;
365: 	pq_scan.name = "parquet_scan";
366: 
367: 	CopyFunction function("parquet");
368: 	function.copy_to_bind = parquet_write_bind;
369: 	function.copy_to_initialize_global = parquet_write_initialize_global;
370: 	function.copy_to_initialize_local = parquet_write_initialize_local;
371: 	function.copy_to_sink = parquet_write_sink;
372: 	function.copy_to_combine = parquet_write_combine;
373: 	function.copy_to_finalize = parquet_write_finalize;
374: 	function.copy_from_bind = ParquetScanFunction::parquet_read_bind;
375: 	function.copy_from_function = scan_fun;
376: 
377: 	function.extension = "parquet";
378: 	CreateCopyFunctionInfo info(function);
379: 
380: 	Connection conn(db);
381: 	conn.context->transaction.BeginTransaction();
382: 	db.catalog->CreateCopyFunction(*conn.context, &info);
383: 	db.catalog->CreateTableFunction(*conn.context, &cinfo);
384: 	db.catalog->CreateTableFunction(*conn.context, &pq_scan);
385: 
386: 	conn.context->transaction.Commit();
387: }
388: 
389: } // namespace duckdb
[end of extension/parquet/parquet-extension.cpp]
[start of extension/parquet/parquet_reader.cpp]
1: #include "parquet_reader.hpp"
2: #include "parquet_timestamp.hpp"
3: #include "parquet_file_metadata_cache.hpp"
4: 
5: #include "duckdb/function/table_function.hpp"
6: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
7: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
8: #include "duckdb/main/client_context.hpp"
9: #include "duckdb/main/connection.hpp"
10: #include "duckdb/main/database.hpp"
11: 
12: #include "duckdb/common/file_system.hpp"
13: #include "duckdb/common/string_util.hpp"
14: #include "duckdb/common/types/date.hpp"
15: #include "duckdb/common/types/time.hpp"
16: #include "duckdb/common/types/timestamp.hpp"
17: 
18: #include "duckdb/common/serializer/buffered_file_writer.hpp"
19: #include "duckdb/common/serializer/buffered_serializer.hpp"
20: #include "duckdb/storage/object_cache.hpp"
21: 
22: #include "duckdb/storage/statistics/string_statistics.hpp"
23: #include "duckdb/storage/statistics/numeric_statistics.hpp"
24: 
25: #include "thrift/protocol/TCompactProtocol.h"
26: #include "thrift/transport/TBufferTransports.h"
27: #include "snappy.h"
28: #include "miniz_wrapper.hpp"
29: 
30: #include "zstd.h"
31: 
32: #include "utf8proc_wrapper.hpp"
33: 
34: #include <sstream>
35: #include <cassert>
36: #include <chrono>
37: 
38: namespace duckdb {
39: 
40: using namespace parquet;
41: using namespace apache::thrift;
42: using namespace apache::thrift::protocol;
43: using namespace apache::thrift::transport;
44: 
45: using parquet::format::CompressionCodec;
46: using parquet::format::ConvertedType;
47: using parquet::format::Encoding;
48: using parquet::format::FieldRepetitionType;
49: using parquet::format::FileMetaData;
50: using parquet::format::PageHeader;
51: using parquet::format::PageType;
52: using parquet::format::RowGroup;
53: using parquet::format::Type;
54: 
55: // adapted from arrow parquet reader
56: class RleBpDecoder {
57: public:
58: 	/// Create a decoder object. buffer/buffer_len is the decoded data.
59: 	/// bit_width is the width of each value (before encoding).
60: 	RleBpDecoder(const uint8_t *buffer, uint32_t buffer_len, uint32_t bit_width)
61: 	    : buffer(buffer), bit_width_(bit_width), current_value_(0), repeat_count_(0), literal_count_(0) {
62: 
63: 		if (bit_width >= 64) {
64: 			throw runtime_error("Decode bit width too large");
65: 		}
66: 		byte_encoded_len = ((bit_width_ + 7) / 8);
67: 		max_val = (1 << bit_width_) - 1;
68: 	}
69: 
70: 	/// Gets a batch of values.  Returns the number of decoded elements.
71: 	template <typename T> void GetBatch(char *values_target_ptr, uint32_t batch_size) {
72: 		auto values = (T *)values_target_ptr;
73: 		uint32_t values_read = 0;
74: 
75: 		while (values_read < batch_size) {
76: 			if (repeat_count_ > 0) {
77: 				int repeat_batch = std::min(batch_size - values_read, static_cast<uint32_t>(repeat_count_));
78: 				std::fill(values + values_read, values + values_read + repeat_batch, static_cast<T>(current_value_));
79: 				repeat_count_ -= repeat_batch;
80: 				values_read += repeat_batch;
81: 			} else if (literal_count_ > 0) {
82: 				uint32_t literal_batch = std::min(batch_size - values_read, static_cast<uint32_t>(literal_count_));
83: 				uint32_t actual_read = BitUnpack<T>(values + values_read, literal_batch);
84: 				if (literal_batch != actual_read) {
85: 					throw runtime_error("Did not find enough values");
86: 				}
87: 				literal_count_ -= literal_batch;
88: 				values_read += literal_batch;
89: 			} else {
90: 				if (!NextCounts<T>()) {
91: 					if (values_read != batch_size) {
92: 						throw runtime_error("RLE decode did not find enough values");
93: 					}
94: 					return;
95: 				}
96: 			}
97: 		}
98: 		if (values_read != batch_size) {
99: 			throw runtime_error("RLE decode did not find enough values");
100: 		}
101: 	}
102: 
103: private:
104: 	const uint8_t *buffer;
105: 
106: 	/// Number of bits needed to encode the value. Must be between 0 and 64.
107: 	int bit_width_;
108: 	uint64_t current_value_;
109: 	uint32_t repeat_count_;
110: 	uint32_t literal_count_;
111: 	uint8_t byte_encoded_len;
112: 	uint32_t max_val;
113: 
114: 	int8_t bitpack_pos = 0;
115: 
116: 	// this is slow but whatever, calls are rare
117: 	static uint8_t VarintDecode(const uint8_t *source, uint32_t *result_out) {
118: 		uint32_t result = 0;
119: 		uint8_t shift = 0;
120: 		uint8_t len = 0;
121: 		while (true) {
122: 			auto byte = *source++;
123: 			len++;
124: 			result |= (byte & 127) << shift;
125: 			if ((byte & 128) == 0)
126: 				break;
127: 			shift += 7;
128: 			if (shift > 32) {
129: 				throw runtime_error("Varint-decoding found too large number");
130: 			}
131: 		}
132: 		*result_out = result;
133: 		return len;
134: 	}
135: 
136: 	/// Fills literal_count_ and repeat_count_ with next values. Returns false if there
137: 	/// are no more.
138: 	template <typename T> bool NextCounts() {
139: 		// Read the next run's indicator int, it could be a literal or repeated run.
140: 		// The int is encoded as a vlq-encoded value.
141: 		uint32_t indicator_value;
142: 		if (bitpack_pos != 0) {
143: 			buffer++;
144: 			bitpack_pos = 0;
145: 		}
146: 		buffer += VarintDecode(buffer, &indicator_value);
147: 
148: 		// lsb indicates if it is a literal run or repeated run
149: 		bool is_literal = indicator_value & 1;
150: 		if (is_literal) {
151: 			literal_count_ = (indicator_value >> 1) * 8;
152: 		} else {
153: 			repeat_count_ = indicator_value >> 1;
154: 			// (ARROW-4018) this is not big-endian compatible, lol
155: 			current_value_ = 0;
156: 			for (auto i = 0; i < byte_encoded_len; i++) {
157: 				current_value_ |= ((uint8_t)*buffer++) << (i * 8);
158: 			}
159: 			// sanity check
160: 			if (repeat_count_ > 0 && current_value_ > max_val) {
161: 				throw runtime_error("Payload value bigger than allowed. Corrupted file?");
162: 			}
163: 		}
164: 		// TODO complain if we run out of buffer
165: 		return true;
166: 	}
167: 
168: 	// somewhat optimized implementation that avoids non-alignment
169: 
170: 	static const uint32_t BITPACK_MASKS[];
171: 	static const uint8_t BITPACK_DLEN;
172: 
173: 	template <typename T> uint32_t BitUnpack(T *dest, uint32_t count) {
174: 		D_ASSERT(bit_width_ < 32);
175: 
176: 		// auto source = buffer;
177: 		auto mask = BITPACK_MASKS[bit_width_];
178: 
179: 		for (uint32_t i = 0; i < count; i++) {
180: 			T val = (*buffer >> bitpack_pos) & mask;
181: 			bitpack_pos += bit_width_;
182: 			while (bitpack_pos > BITPACK_DLEN) {
183: 				val |= (*++buffer << (BITPACK_DLEN - (bitpack_pos - bit_width_))) & mask;
184: 				bitpack_pos -= BITPACK_DLEN;
185: 			}
186: 			dest[i] = val;
187: 		}
188: 		return count;
189: 	}
190: };
191: 
192: const uint32_t RleBpDecoder::BITPACK_MASKS[] = {
193:     0,       1,       3,        7,        15,       31,        63,        127,       255,        511,       1023,
194:     2047,    4095,    8191,     16383,    32767,    65535,     131071,    262143,    524287,     1048575,   2097151,
195:     4194303, 8388607, 16777215, 33554431, 67108863, 134217727, 268435455, 536870911, 1073741823, 2147483647};
196: 
197: const uint8_t RleBpDecoder::BITPACK_DLEN = 8;
198: 
199: static TCompactProtocolFactoryT<TMemoryBuffer> tproto_factory;
200: 
201: template <class T> static void thrift_unpack(const uint8_t *buf, uint32_t *len, T *deserialized_msg) {
202: 	shared_ptr<TMemoryBuffer> tmem_transport(new TMemoryBuffer(const_cast<uint8_t *>(buf), *len));
203: 	shared_ptr<TProtocol> tproto = tproto_factory.getProtocol(tmem_transport);
204: 	try {
205: 		deserialized_msg->read(tproto.get());
206: 	} catch (std::exception &e) {
207: 		std::stringstream ss;
208: 		ss << "Couldn't deserialize thrift: " << e.what() << "\n";
209: 		throw std::runtime_error(ss.str());
210: 	}
211: 	uint32_t bytes_left = tmem_transport->available_read();
212: 	*len = *len - bytes_left;
213: }
214: 
215: static unique_ptr<parquet::format::FileMetaData> read_metadata(duckdb::FileSystem &fs, duckdb::FileHandle *handle,
216:                                                                uint32_t footer_len, uint64_t file_size) {
217: 	auto metadata = make_unique<parquet::format::FileMetaData>();
218: 	// read footer into buffer and de-thrift
219: 	ResizeableBuffer buf;
220: 	buf.resize(footer_len);
221: 	fs.Read(*handle, buf.ptr, footer_len, file_size - (footer_len + 8));
222: 	thrift_unpack((const uint8_t *)buf.ptr, &footer_len, metadata.get());
223: 	return metadata;
224: }
225: 
226: static shared_ptr<ParquetFileMetadataCache> load_metadata(duckdb::FileSystem &fs, duckdb::FileHandle *handle,
227:                                                           uint32_t footer_len, uint64_t file_size) {
228: 	auto current_time = chrono::system_clock::to_time_t(chrono::system_clock::now());
229: 	return make_shared<ParquetFileMetadataCache>(read_metadata(fs, handle, footer_len, file_size), current_time);
230: }
231: 
232: ParquetReader::ParquetReader(ClientContext &context, string file_name_, vector<LogicalType> expected_types,
233:                              string initial_filename)
234:     : file_name(move(file_name_)), context(context) {
235: 	auto &fs = FileSystem::GetFileSystem(context);
236: 
237: 	auto handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ);
238: 
239: 	ResizeableBuffer buf;
240: 	buf.resize(4);
241: 	memset(buf.ptr, '\0', 4);
242: 	// check for magic bytes at start of file
243: 	fs.Read(*handle, buf.ptr, 4);
244: 	if (strncmp(buf.ptr, "PAR1", 4) != 0) {
245: 		throw FormatException("Missing magic bytes in front of Parquet file");
246: 	}
247: 
248: 	// check for magic bytes at end of file
249: 	auto file_size_signed = fs.GetFileSize(*handle);
250: 	if (file_size_signed < 12) {
251: 		throw FormatException("File too small to be a Parquet file");
252: 	}
253: 	auto file_size = (uint64_t)file_size_signed;
254: 	fs.Read(*handle, buf.ptr, 4, file_size - 4);
255: 	if (strncmp(buf.ptr, "PAR1", 4) != 0) {
256: 		throw FormatException("No magic bytes found at end of file");
257: 	}
258: 
259: 	// read four-byte footer length from just before the end magic bytes
260: 	fs.Read(*handle, buf.ptr, 4, file_size - 8);
261: 	auto footer_len = *(uint32_t *)buf.ptr;
262: 	if (footer_len <= 0) {
263: 		throw FormatException("Footer length can't be 0");
264: 	}
265: 	if (file_size < 12 + footer_len) {
266: 		throw FormatException("Footer length %d is too big for the file of size %d", footer_len, file_size);
267: 	}
268: 
269: 	// If object cached is disabled
270: 	// or if this file has cached metadata
271: 	// or if the cached version already expired
272: 	if (!context.db.config.object_cache_enable) {
273: 		metadata = load_metadata(fs, handle.get(), footer_len, file_size);
274: 	} else {
275: 		metadata = dynamic_pointer_cast<ParquetFileMetadataCache>(context.db.object_cache->Get(file_name));
276: 		if (!metadata || (fs.GetLastModifiedTime(*handle) + 10 >= metadata->read_time)) {
277: 			metadata = load_metadata(fs, handle.get(), footer_len, file_size);
278: 			context.db.object_cache->Put(file_name, dynamic_pointer_cast<ObjectCacheEntry>(metadata));
279: 		}
280: 	}
281: 
282: 	auto file_meta_data = GetFileMetadata();
283: 
284: 	if (file_meta_data->__isset.encryption_algorithm) {
285: 		throw FormatException("Encrypted Parquet files are not supported");
286: 	}
287: 	// check if we like this schema
288: 	if (file_meta_data->schema.size() < 2) {
289: 		throw FormatException("Need at least one column in the file");
290: 	}
291: 	if (file_meta_data->schema[0].num_children != (int32_t)(file_meta_data->schema.size() - 1)) {
292: 		throw FormatException("Only flat tables are supported (no nesting)");
293: 	}
294: 
295: 	this->return_types = expected_types;
296: 	bool has_expected_types = expected_types.size() > 0;
297: 
298: 	// skip the first column its the root and otherwise useless
299: 	for (uint64_t col_idx = 1; col_idx < file_meta_data->schema.size(); col_idx++) {
300: 		auto &s_ele = file_meta_data->schema[col_idx];
301: 		if (!s_ele.__isset.type || s_ele.num_children > 0) {
302: 			throw FormatException("Only flat tables are supported (no nesting)");
303: 		}
304: 		// if this is REQUIRED, there are no defined levels in file
305: 		// if field is REPEATED, no bueno
306: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
307: 			throw FormatException("REPEATED fields are not supported");
308: 		}
309: 
310: 		LogicalType type;
311: 		switch (s_ele.type) {
312: 		case Type::BOOLEAN:
313: 			type = LogicalType::BOOLEAN;
314: 			break;
315: 		case Type::INT32:
316: 			type = LogicalType::INTEGER;
317: 			break;
318: 		case Type::INT64:
319: 			if (s_ele.__isset.converted_type) {
320: 				switch (s_ele.converted_type) {
321: 				case ConvertedType::TIMESTAMP_MICROS:
322: 				case ConvertedType::TIMESTAMP_MILLIS:
323: 					type = LogicalType::TIMESTAMP;
324: 					break;
325: 				default:
326: 					type = LogicalType::BIGINT;
327: 					break;
328: 				}
329: 			} else {
330: 				type = LogicalType::BIGINT;
331: 			}
332: 			break;
333: 		case Type::INT96: // always a timestamp?
334: 			type = LogicalType::TIMESTAMP;
335: 			break;
336: 		case Type::FLOAT:
337: 			type = LogicalType::FLOAT;
338: 			break;
339: 		case Type::DOUBLE:
340: 			type = LogicalType::DOUBLE;
341: 			break;
342: 			//			case parquet::format::Type::FIXED_LEN_BYTE_ARRAY: {
343: 			// TODO some decimals yuck
344: 		case Type::BYTE_ARRAY:
345: 			if (s_ele.__isset.converted_type) {
346: 				switch (s_ele.converted_type) {
347: 				case ConvertedType::UTF8:
348: 					type = LogicalType::VARCHAR;
349: 					break;
350: 				default:
351: 					type = LogicalType::BLOB;
352: 					break;
353: 				}
354: 			} else {
355: 				type = LogicalType::BLOB;
356: 			}
357: 			break;
358: 		default:
359: 			throw FormatException("Unsupported type");
360: 		}
361: 		if (has_expected_types) {
362: 			if (return_types[col_idx - 1] != type) {
363: 				if (initial_filename.empty()) {
364: 					throw FormatException("column \"%s\" in parquet file is of type %s, could not auto cast to "
365: 					                      "expected type %s for this column",
366: 					                      s_ele.name, type.ToString(), return_types[col_idx - 1].ToString());
367: 				} else {
368: 					throw FormatException("schema mismatch in Parquet glob: column \"%s\" in parquet file is of type "
369: 					                      "%s, but in the original file \"%s\" this column is of type \"%s\"",
370: 					                      s_ele.name, type.ToString(), initial_filename,
371: 					                      return_types[col_idx - 1].ToString());
372: 				}
373: 			}
374: 		} else {
375: 			names.push_back(s_ele.name);
376: 			return_types.push_back(type);
377: 		}
378: 	}
379: }
380: 
381: ParquetReader::~ParquetReader() {
382: }
383: 
384: const parquet::format::FileMetaData *ParquetReader::GetFileMetadata() {
385: 	D_ASSERT(metadata);
386: 	D_ASSERT(metadata->metadata);
387: 	return metadata->metadata.get();
388: }
389: 
390: ParquetReaderColumnData::~ParquetReaderColumnData() {
391: }
392: 
393: struct ValueIsValid {
394: 	template <class T> static bool Operation(T value) {
395: 		return true;
396: 	}
397: };
398: 
399: template <> bool ValueIsValid::Operation(float value) {
400: 	return Value::FloatIsValid(value);
401: }
402: 
403: template <> bool ValueIsValid::Operation(double value) {
404: 	return Value::DoubleIsValid(value);
405: }
406: 
407: template <class T>
408: void ParquetReader::fill_from_dict(ParquetReaderColumnData &col_data, idx_t count, Vector &target,
409:                                    idx_t target_offset) {
410: 	for (idx_t i = 0; i < count; i++) {
411: 		if (!col_data.has_nulls || col_data.defined_buf.ptr[i]) {
412: 			auto offset = col_data.offset_buf.read<uint32_t>();
413: 			if (offset > col_data.dict_size) {
414: 				throw runtime_error("Offset " + to_string(offset) + " greater than dictionary size " +
415: 				                    to_string(col_data.dict_size) + " at " + to_string(i + target_offset) +
416: 				                    ". Corrupt file?");
417: 			}
418: 			auto value = ((const T *)col_data.dict.ptr)[offset];
419: 			if (ValueIsValid::Operation(value)) {
420: 				((T *)FlatVector::GetData(target))[i + target_offset] = value;
421: 			} else {
422: 				FlatVector::SetNull(target, i + target_offset, true);
423: 			}
424: 		} else {
425: 			FlatVector::SetNull(target, i + target_offset, true);
426: 		}
427: 	}
428: }
429: 
430: template <class T>
431: void ParquetReader::fill_from_plain(ParquetReaderColumnData &col_data, idx_t count, Vector &target,
432:                                     idx_t target_offset) {
433: 	for (idx_t i = 0; i < count; i++) {
434: 		if (!col_data.has_nulls || col_data.defined_buf.ptr[i]) {
435: 			auto value = col_data.payload.read<T>();
436: 			if (ValueIsValid::Operation(value)) {
437: 				((T *)FlatVector::GetData(target))[i + target_offset] = value;
438: 			} else {
439: 				FlatVector::SetNull(target, i + target_offset, true);
440: 			}
441: 		} else {
442: 			FlatVector::SetNull(target, i + target_offset, true);
443: 		}
444: 	}
445: }
446: 
447: template <class T, timestamp_t (*FUNC)(const T &input)>
448: static void fill_timestamp_plain(ParquetReaderColumnData &col_data, idx_t count, Vector &target, idx_t target_offset) {
449: 	for (idx_t i = 0; i < count; i++) {
450: 		if (!col_data.has_nulls || col_data.defined_buf.ptr[i]) {
451: 			auto value = col_data.payload.read<T>();
452: 			((timestamp_t *)FlatVector::GetData(target))[i + target_offset] = FUNC(value);
453: 		} else {
454: 			FlatVector::SetNull(target, i + target_offset, true);
455: 		}
456: 	}
457: }
458: 
459: const RowGroup &ParquetReader::GetGroup(ParquetReaderScanState &state) {
460: 	auto file_meta_data = GetFileMetadata();
461: 	D_ASSERT(state.current_group >= 0 && (idx_t)state.current_group < state.group_idx_list.size());
462: 	D_ASSERT(state.group_idx_list[state.current_group] >= 0 &&
463: 	         state.group_idx_list[state.current_group] < file_meta_data->row_groups.size());
464: 	return file_meta_data->row_groups[state.group_idx_list[state.current_group]];
465: }
466: 
467: timestamp_t arrow_timestamp_micros_to_timestamp(const int64_t &raw_ts) {
468: 	return Timestamp::FromEpochMicroSeconds(raw_ts);
469: }
470: timestamp_t arrow_timestamp_ms_to_timestamp(const int64_t &raw_ts) {
471: 	return Timestamp::FromEpochMs(raw_ts);
472: }
473: 
474: template <class T, timestamp_t (*FUNC)(const T &input)>
475: static void fill_timestamp_dict(ParquetReaderColumnData &col_data) {
476: 	// immediately convert timestamps to duckdb format, potentially fewer conversions
477: 	for (idx_t dict_index = 0; dict_index < col_data.dict_size; dict_index++) {
478: 		auto impala_ts = Load<T>((data_ptr_t)(col_data.payload.ptr + dict_index * sizeof(T)));
479: 		((timestamp_t *)col_data.dict.ptr)[dict_index] = FUNC(impala_ts);
480: 	}
481: }
482: 
483: void ParquetReader::VerifyString(LogicalTypeId id, const char *str_data, idx_t str_len) {
484: 	if (id != LogicalTypeId::VARCHAR) {
485: 		return;
486: 	}
487: 	// verify if a string is actually UTF8, and if there are no null bytes in the middle of the string
488: 	// technically Parquet should guarantee this, but reality is often disappointing
489: 	auto utf_type = Utf8Proc::Analyze(str_data, str_len);
490: 	if (utf_type == UnicodeType::INVALID) {
491: 		throw FormatException("Invalid string encoding found in Parquet file: value is not valid UTF8!");
492: 	}
493: }
494: 
495: bool ParquetReader::PreparePageBuffers(ParquetReaderScanState &state, idx_t col_idx) {
496: 	auto &col_data = *state.column_data[col_idx];
497: 	auto &s_ele = GetFileMetadata()->schema[col_idx + 1];
498: 	auto &chunk = GetGroup(state).columns[col_idx];
499: 
500: 	// clean up a bit to avoid nasty surprises
501: 	col_data.payload.ptr = nullptr;
502: 	col_data.payload.len = 0;
503: 	col_data.dict_decoder = nullptr;
504: 	col_data.defined_decoder = nullptr;
505: 	col_data.byte_pos = 0;
506: 
507: 	auto page_header_len = col_data.buf.len;
508: 	if (page_header_len < 1) {
509: 		throw FormatException("Ran out of bytes to read header from. File corrupt?");
510: 	}
511: 	PageHeader page_hdr;
512: 	thrift_unpack((const uint8_t *)col_data.buf.ptr + col_data.chunk_offset, (uint32_t *)&page_header_len, &page_hdr);
513: 
514: 	// the payload starts behind the header, obvsl.
515: 	col_data.buf.inc(page_header_len);
516: 
517: 	col_data.payload.len = page_hdr.uncompressed_page_size;
518: 
519: 	// handle compression, in the end we expect a pointer to uncompressed parquet data in payload_ptr
520: 	switch (chunk.meta_data.codec) {
521: 	case CompressionCodec::UNCOMPRESSED:
522: 		col_data.payload.ptr = col_data.buf.ptr;
523: 		break;
524: 	case CompressionCodec::SNAPPY: {
525: 		col_data.decompressed_buf.resize(page_hdr.uncompressed_page_size);
526: 		auto res =
527: 		    snappy::RawUncompress(col_data.buf.ptr, page_hdr.compressed_page_size, col_data.decompressed_buf.ptr);
528: 		if (!res) {
529: 			throw FormatException("Decompression failure");
530: 		}
531: 		col_data.payload.ptr = col_data.decompressed_buf.ptr;
532: 		break;
533: 	}
534: 	case CompressionCodec::GZIP: {
535: 		MiniZStream s;
536: 
537: 		col_data.decompressed_buf.resize(page_hdr.uncompressed_page_size);
538: 		s.Decompress(col_data.buf.ptr, page_hdr.compressed_page_size, col_data.decompressed_buf.ptr,
539: 		             page_hdr.uncompressed_page_size);
540: 
541: 		col_data.payload.ptr = col_data.decompressed_buf.ptr;
542: 		break;
543: 	}
544: 	case CompressionCodec::ZSTD: {
545: 		col_data.decompressed_buf.resize(page_hdr.uncompressed_page_size);
546: 		auto res = duckdb_zstd::ZSTD_decompress(col_data.decompressed_buf.ptr, page_hdr.uncompressed_page_size,
547: 		                                        col_data.buf.ptr, page_hdr.compressed_page_size);
548: 		if (duckdb_zstd::ZSTD_isError(res) || res != (size_t)page_hdr.uncompressed_page_size) {
549: 			throw FormatException("ZSTD Decompression failure");
550: 		}
551: 		col_data.payload.ptr = col_data.decompressed_buf.ptr;
552: 		break;
553: 	}
554: 	default: {
555: 		std::stringstream codec_name;
556: 		codec_name << chunk.meta_data.codec;
557: 		throw FormatException("Unsupported compression codec \"" + codec_name.str() +
558: 		                      "\". Supported options are uncompressed, gzip or snappy");
559: 	}
560: 	}
561: 	col_data.buf.inc(page_hdr.compressed_page_size);
562: 
563: 	// handle page contents
564: 	switch (page_hdr.type) {
565: 	case PageType::DICTIONARY_PAGE: {
566: 		// fill the dictionary vector
567: 
568: 		if (page_hdr.__isset.data_page_header || !page_hdr.__isset.dictionary_page_header) {
569: 			throw FormatException("Dictionary page header mismatch");
570: 		}
571: 
572: 		// make sure we like the encoding
573: 		switch (page_hdr.dictionary_page_header.encoding) {
574: 		case Encoding::PLAIN:
575: 		case Encoding::PLAIN_DICTIONARY: // deprecated
576: 			break;
577: 
578: 		default:
579: 			throw FormatException("Dictionary page has unsupported/invalid encoding");
580: 		}
581: 
582: 		col_data.dict_size = page_hdr.dictionary_page_header.num_values;
583: 		auto dict_byte_size = col_data.dict_size * GetTypeIdSize(return_types[col_idx].InternalType());
584: 
585: 		col_data.dict.resize(dict_byte_size);
586: 
587: 		switch (return_types[col_idx].id()) {
588: 		case LogicalTypeId::BOOLEAN:
589: 		case LogicalTypeId::INTEGER:
590: 		case LogicalTypeId::BIGINT:
591: 		case LogicalTypeId::FLOAT:
592: 		case LogicalTypeId::DOUBLE:
593: 			col_data.payload.available(dict_byte_size);
594: 			// TODO this copy could be avoided if we use different buffers for dicts
595: 			col_data.payload.copy_to(col_data.dict.ptr, dict_byte_size);
596: 			break;
597: 		case LogicalTypeId::TIMESTAMP:
598: 			col_data.payload.available(dict_byte_size);
599: 			switch (s_ele.type) {
600: 			case Type::INT64:
601: 				// arrow timestamp
602: 				switch (s_ele.converted_type) {
603: 				case ConvertedType::TIMESTAMP_MICROS:
604: 					fill_timestamp_dict<int64_t, arrow_timestamp_micros_to_timestamp>(col_data);
605: 					break;
606: 				case ConvertedType::TIMESTAMP_MILLIS:
607: 					fill_timestamp_dict<int64_t, arrow_timestamp_ms_to_timestamp>(col_data);
608: 					break;
609: 				default:
610: 					throw InternalException("Unsupported converted type for timestamp");
611: 				}
612: 				break;
613: 			case Type::INT96:
614: 				// impala timestamp
615: 				fill_timestamp_dict<Int96, impala_timestamp_to_timestamp_t>(col_data);
616: 				break;
617: 			default:
618: 				throw InternalException("Unsupported type for timestamp");
619: 			}
620: 			break;
621: 		case LogicalTypeId::BLOB:
622: 		case LogicalTypeId::VARCHAR: {
623: 			// strings we directly fill a string heap that we can use for the vectors later
624: 			col_data.string_collection = make_unique<ChunkCollection>();
625: 
626: 			auto append_chunk = make_unique<DataChunk>();
627: 			vector<LogicalType> types = {return_types[col_idx]};
628: 			append_chunk->Initialize(types);
629: 
630: 			for (idx_t dict_index = 0; dict_index < col_data.dict_size; dict_index++) {
631: 				uint32_t str_len = col_data.payload.read<uint32_t>();
632: 				col_data.payload.available(str_len);
633: 
634: 				if (append_chunk->size() == STANDARD_VECTOR_SIZE) {
635: 					col_data.string_collection->Append(*append_chunk);
636: 					append_chunk->SetCardinality(0);
637: 				}
638: 
639: 				VerifyString(return_types[col_idx].id(), col_data.payload.ptr, str_len);
640: 				FlatVector::GetData<string_t>(append_chunk->data[0])[append_chunk->size()] =
641: 				    StringVector::AddStringOrBlob(append_chunk->data[0], string_t(col_data.payload.ptr, str_len));
642: 
643: 				append_chunk->SetCardinality(append_chunk->size() + 1);
644: 				col_data.payload.inc(str_len);
645: 			}
646: 			// FLUSH last chunk!
647: 			if (append_chunk->size() > 0) {
648: 				col_data.string_collection->Append(*append_chunk);
649: 			}
650: 			col_data.string_collection->Verify();
651: 		} break;
652: 		default:
653: 			throw FormatException(return_types[col_idx].ToString());
654: 		}
655: 		// important, move to next page which should be a data page
656: 		return false;
657: 	}
658: 	case PageType::DATA_PAGE:
659: 	case PageType::DATA_PAGE_V2: {
660: 
661: 		if (page_hdr.type == PageType::DATA_PAGE) {
662: 			D_ASSERT(page_hdr.__isset.data_page_header);
663: 		}
664: 		if (page_hdr.type == PageType::DATA_PAGE_V2) {
665: 			D_ASSERT(page_hdr.__isset.data_page_header_v2);
666: 		}
667: 
668: 		col_data.page_value_count = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.num_values
669: 		                                                                 : page_hdr.data_page_header_v2.num_values;
670: 		col_data.page_encoding = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.encoding
671: 		                                                              : page_hdr.data_page_header_v2.encoding;
672: 
673: 		if (!col_data.has_nulls && page_hdr.type == PageType::DATA_PAGE &&
674: 		    page_hdr.data_page_header.__isset.statistics && page_hdr.data_page_header.statistics.__isset.null_count &&
675: 		    page_hdr.data_page_header.statistics.null_count > 0) {
676: 			throw FormatException("Column is defined as REQUIRED but statistics still claim NULL present");
677: 		}
678: 		if (!col_data.has_nulls && page_hdr.type == PageType::DATA_PAGE_V2 &&
679: 		    page_hdr.data_page_header_v2.num_nulls > 0) {
680: 			throw FormatException("Column is defined as REQUIRED but statistics still claim NULL present");
681: 		}
682: 
683: 		if (col_data.has_nulls) {
684: 			// we have to first decode the define levels
685: 			switch (page_hdr.data_page_header.definition_level_encoding) {
686: 			case Encoding::RLE: {
687: 				// read length of define payload, always
688: 				uint32_t def_length = col_data.payload.read<uint32_t>();
689: 				col_data.payload.available(def_length);
690: 				col_data.defined_decoder =
691: 				    make_unique<RleBpDecoder>((const uint8_t *)col_data.payload.ptr, def_length, 1);
692: 				col_data.payload.inc(def_length);
693: 			} break;
694: 			default:
695: 				throw FormatException("Definition levels have unsupported/invalid encoding");
696: 			}
697: 		}
698: 
699: 		switch (col_data.page_encoding) {
700: 		case Encoding::RLE_DICTIONARY:
701: 		case Encoding::PLAIN_DICTIONARY: {
702: 			auto enc_length = col_data.payload.read<uint8_t>();
703: 			col_data.dict_decoder =
704: 			    make_unique<RleBpDecoder>((const uint8_t *)col_data.payload.ptr, col_data.payload.len, enc_length);
705: 			break;
706: 		}
707: 		case Encoding::PLAIN:
708: 			// nothing here, see below
709: 			break;
710: 
711: 		default:
712: 			throw FormatException("Data page has unsupported/invalid encoding");
713: 		}
714: 
715: 		break;
716: 	}
717: 	default:
718: 		break; // ignore INDEX page type and any other custom extensions
719: 	}
720: 	return true;
721: }
722: 
723: void ParquetReader::PrepareChunkBuffer(ParquetReaderScanState &state, idx_t col_idx) {
724: 	auto &chunk = GetGroup(state).columns[col_idx];
725: 	if (chunk.__isset.file_path) {
726: 		throw FormatException("Only inlined data files are supported (no references)");
727: 	}
728: 
729: 	if (chunk.meta_data.path_in_schema.size() != 1) {
730: 		throw FormatException("Only flat tables are supported (no nesting)");
731: 	}
732: 
733: 	// ugh. sometimes there is an extra offset for the dict. sometimes it's wrong.
734: 	auto chunk_start = chunk.meta_data.data_page_offset;
735: 	if (chunk.meta_data.__isset.dictionary_page_offset && chunk.meta_data.dictionary_page_offset >= 4) {
736: 		// this assumes the data pages follow the dict pages directly.
737: 		chunk_start = chunk.meta_data.dictionary_page_offset;
738: 	}
739: 	auto chunk_len = chunk.meta_data.total_compressed_size;
740: 
741: 	auto &fs = FileSystem::GetFileSystem(context);
742: 	auto handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ);
743: 
744: 	state.column_data[col_idx]->has_nulls =
745: 	    GetFileMetadata()->schema[col_idx + 1].repetition_type == FieldRepetitionType::OPTIONAL;
746: 
747: 	// read entire chunk into RAM
748: 	state.column_data[col_idx]->buf.resize(chunk_len);
749: 	fs.Read(*handle, state.column_data[col_idx]->buf.ptr, chunk_len, chunk_start);
750: }
751: 
752: idx_t ParquetReader::NumRows() {
753: 	return GetFileMetadata()->num_rows;
754: }
755: 
756: idx_t ParquetReader::NumRowGroups() {
757: 	return GetFileMetadata()->row_groups.size();
758: }
759: 
760: void ParquetReader::Initialize(ParquetReaderScanState &state, vector<column_t> column_ids,
761:                                vector<idx_t> groups_to_read) {
762: 	state.current_group = -1;
763: 	state.finished = false;
764: 	state.column_ids = move(column_ids);
765: 	state.group_offset = 0;
766: 	state.group_idx_list = move(groups_to_read);
767: 	for (idx_t i = 0; i < return_types.size(); i++) {
768: 		state.column_data.push_back(make_unique<ParquetReaderColumnData>());
769: 	}
770: }
771: 
772: void ParquetReader::ReadChunk(ParquetReaderScanState &state, DataChunk &output) {
773: 	if (state.finished) {
774: 		return;
775: 	}
776: 
777: 	// see if we have to switch to the next row group in the parquet file
778: 	if (state.current_group < 0 || (int64_t)state.group_offset >= GetGroup(state).num_rows) {
779: 		state.current_group++;
780: 		state.group_offset = 0;
781: 
782: 		if ((idx_t)state.current_group == state.group_idx_list.size()) {
783: 			state.finished = true;
784: 			return;
785: 		}
786: 
787: 		for (idx_t out_col_idx = 0; out_col_idx < output.ColumnCount(); out_col_idx++) {
788: 			auto file_col_idx = state.column_ids[out_col_idx];
789: 
790: 			// this is a special case where we are not interested in the actual contents of the file
791: 			if (file_col_idx == COLUMN_IDENTIFIER_ROW_ID) {
792: 				continue;
793: 			}
794: 
795: 			PrepareChunkBuffer(state, file_col_idx);
796: 			// trigger the reading of a new page below
797: 			state.column_data[file_col_idx]->page_value_count = 0;
798: 		}
799: 	}
800: 
801: 	output.SetCardinality(MinValue<int64_t>(STANDARD_VECTOR_SIZE, GetGroup(state).num_rows - state.group_offset));
802: 
803: 	if (output.size() == 0) {
804: 		return;
805: 	}
806: 
807: 	auto file_meta_data = GetFileMetadata();
808: 
809: 	for (idx_t out_col_idx = 0; out_col_idx < output.ColumnCount(); out_col_idx++) {
810: 		auto file_col_idx = state.column_ids[out_col_idx];
811: 		if (file_col_idx == COLUMN_IDENTIFIER_ROW_ID) {
812: 			Value constant_42 = Value::BIGINT(42);
813: 			output.data[out_col_idx].Reference(constant_42);
814: 			continue;
815: 		}
816: 		auto &s_ele = file_meta_data->schema[file_col_idx + 1];
817: 		auto &col_data = *state.column_data[file_col_idx];
818: 
819: 		// we might need to read multiple pages to fill the data chunk
820: 		idx_t output_offset = 0;
821: 		while (output_offset < output.size()) {
822: 			// do this unpack business only if we run out of stuff from the current page
823: 			if (col_data.page_offset >= col_data.page_value_count) {
824: 				// read dictionaries and data page headers so that we are ready to go for scan
825: 				if (!PreparePageBuffers(state, file_col_idx)) {
826: 					continue;
827: 				}
828: 				col_data.page_offset = 0;
829: 			}
830: 
831: 			auto current_batch_size =
832: 			    MinValue<idx_t>(col_data.page_value_count - col_data.page_offset, output.size() - output_offset);
833: 
834: 			D_ASSERT(current_batch_size > 0);
835: 
836: 			if (col_data.has_nulls) {
837: 				col_data.defined_buf.resize(current_batch_size);
838: 				col_data.defined_decoder->GetBatch<uint8_t>(col_data.defined_buf.ptr, current_batch_size);
839: 			}
840: 
841: 			switch (col_data.page_encoding) {
842: 			case Encoding::RLE_DICTIONARY:
843: 			case Encoding::PLAIN_DICTIONARY: {
844: 				idx_t null_count = 0;
845: 				if (col_data.has_nulls) {
846: 					for (idx_t i = 0; i < current_batch_size; i++) {
847: 						if (!col_data.defined_buf.ptr[i]) {
848: 							null_count++;
849: 						}
850: 					}
851: 				}
852: 
853: 				col_data.offset_buf.resize(current_batch_size * sizeof(uint32_t));
854: 				col_data.dict_decoder->GetBatch<uint32_t>(col_data.offset_buf.ptr, current_batch_size - null_count);
855: 
856: 				// TODO ensure we had seen a dict page IN THIS CHUNK before getting here
857: 
858: 				switch (return_types[file_col_idx].id()) {
859: 				case LogicalTypeId::BOOLEAN:
860: 					fill_from_dict<bool>(col_data, current_batch_size, output.data[out_col_idx], output_offset);
861: 					break;
862: 				case LogicalTypeId::INTEGER:
863: 					fill_from_dict<int32_t>(col_data, current_batch_size, output.data[out_col_idx], output_offset);
864: 					break;
865: 				case LogicalTypeId::BIGINT:
866: 					fill_from_dict<int64_t>(col_data, current_batch_size, output.data[out_col_idx], output_offset);
867: 					break;
868: 				case LogicalTypeId::FLOAT:
869: 					fill_from_dict<float>(col_data, current_batch_size, output.data[out_col_idx], output_offset);
870: 					break;
871: 				case LogicalTypeId::DOUBLE:
872: 					fill_from_dict<double>(col_data, current_batch_size, output.data[out_col_idx], output_offset);
873: 					break;
874: 				case LogicalTypeId::TIMESTAMP:
875: 					fill_from_dict<timestamp_t>(col_data, current_batch_size, output.data[out_col_idx], output_offset);
876: 					break;
877: 				case LogicalTypeId::BLOB:
878: 				case LogicalTypeId::VARCHAR: {
879: 					if (!col_data.string_collection) {
880: 						throw FormatException("Did not see a dictionary for strings. Corrupt file?");
881: 					}
882: 
883: 					// the strings can be anywhere in the collection so just reference it all
884: 					for (auto &chunk : col_data.string_collection->Chunks()) {
885: 						StringVector::AddHeapReference(output.data[out_col_idx], chunk->data[0]);
886: 					}
887: 
888: 					auto out_data_ptr = FlatVector::GetData<string_t>(output.data[out_col_idx]);
889: 					for (idx_t i = 0; i < current_batch_size; i++) {
890: 						if (!col_data.has_nulls || col_data.defined_buf.ptr[i]) {
891: 							auto offset = col_data.offset_buf.read<uint32_t>();
892: 							if (offset >= col_data.string_collection->Count()) {
893: 								throw FormatException("string dictionary offset out of bounds");
894: 							}
895: 							auto &chunk = col_data.string_collection->GetChunk(offset / STANDARD_VECTOR_SIZE);
896: 							auto &vec = chunk.data[0];
897: 
898: 							out_data_ptr[i + output_offset] =
899: 							    FlatVector::GetData<string_t>(vec)[offset % STANDARD_VECTOR_SIZE];
900: 						} else {
901: 							FlatVector::SetNull(output.data[out_col_idx], i + output_offset, true);
902: 						}
903: 					}
904: 				} break;
905: 				default:
906: 					throw FormatException(return_types[file_col_idx].ToString());
907: 				}
908: 
909: 				break;
910: 			}
911: 			case Encoding::PLAIN:
912: 				D_ASSERT(col_data.payload.ptr);
913: 				switch (return_types[file_col_idx].id()) {
914: 				case LogicalTypeId::BOOLEAN: {
915: 					// bit packed this
916: 					auto target_ptr = FlatVector::GetData<bool>(output.data[out_col_idx]);
917: 					for (idx_t i = 0; i < current_batch_size; i++) {
918: 						if (col_data.has_nulls && !col_data.defined_buf.ptr[i]) {
919: 							FlatVector::SetNull(output.data[out_col_idx], i + output_offset, true);
920: 							continue;
921: 						}
922: 						col_data.payload.available(1);
923: 						target_ptr[i + output_offset] = (*col_data.payload.ptr >> col_data.byte_pos) & 1;
924: 						col_data.byte_pos++;
925: 						if (col_data.byte_pos == 8) {
926: 							col_data.byte_pos = 0;
927: 							col_data.payload.inc(1);
928: 						}
929: 					}
930: 					break;
931: 				}
932: 				case LogicalTypeId::INTEGER:
933: 					fill_from_plain<int32_t>(col_data, current_batch_size, output.data[out_col_idx], output_offset);
934: 					break;
935: 				case LogicalTypeId::BIGINT:
936: 					fill_from_plain<int64_t>(col_data, current_batch_size, output.data[out_col_idx], output_offset);
937: 					break;
938: 				case LogicalTypeId::FLOAT:
939: 					fill_from_plain<float>(col_data, current_batch_size, output.data[out_col_idx], output_offset);
940: 					break;
941: 				case LogicalTypeId::DOUBLE:
942: 					fill_from_plain<double>(col_data, current_batch_size, output.data[out_col_idx], output_offset);
943: 					break;
944: 				case LogicalTypeId::TIMESTAMP:
945: 					switch (s_ele.type) {
946: 					case Type::INT64:
947: 						// arrow timestamp
948: 						switch (s_ele.converted_type) {
949: 						case ConvertedType::TIMESTAMP_MICROS:
950: 							fill_timestamp_plain<int64_t, arrow_timestamp_micros_to_timestamp>(
951: 							    col_data, current_batch_size, output.data[out_col_idx], output_offset);
952: 							break;
953: 						case ConvertedType::TIMESTAMP_MILLIS:
954: 							fill_timestamp_plain<int64_t, arrow_timestamp_ms_to_timestamp>(
955: 							    col_data, current_batch_size, output.data[out_col_idx], output_offset);
956: 							break;
957: 						default:
958: 							throw InternalException("Unsupported converted type for timestamp");
959: 						}
960: 						break;
961: 					case Type::INT96:
962: 						// impala timestamp
963: 						fill_timestamp_plain<Int96, impala_timestamp_to_timestamp_t>(
964: 						    col_data, current_batch_size, output.data[out_col_idx], output_offset);
965: 						break;
966: 					default:
967: 						throw InternalException("Unsupported type for timestamp");
968: 					}
969: 					break;
970: 				case LogicalTypeId::BLOB:
971: 				case LogicalTypeId::VARCHAR: {
972: 					for (idx_t i = 0; i < current_batch_size; i++) {
973: 						if (!col_data.has_nulls || col_data.defined_buf.ptr[i]) {
974: 							uint32_t str_len = col_data.payload.read<uint32_t>();
975: 							col_data.payload.available(str_len);
976: 							VerifyString(return_types[file_col_idx].id(), col_data.payload.ptr, str_len);
977: 							FlatVector::GetData<string_t>(output.data[out_col_idx])[i + output_offset] =
978: 							    StringVector::AddStringOrBlob(output.data[out_col_idx],
979: 							                                  string_t(col_data.payload.ptr, str_len));
980: 							col_data.payload.inc(str_len);
981: 						} else {
982: 							FlatVector::SetNull(output.data[out_col_idx], i + output_offset, true);
983: 						}
984: 					}
985: 					break;
986: 				}
987: 				default:
988: 					throw FormatException(return_types[file_col_idx].ToString());
989: 				}
990: 
991: 				break;
992: 
993: 			default:
994: 				throw FormatException("Data page has unsupported/invalid encoding");
995: 			}
996: 
997: 			output_offset += current_batch_size;
998: 			col_data.page_offset += current_batch_size;
999: 		}
1000: 	}
1001: 	state.group_offset += output.size();
1002: }
1003: 
1004: // statistics handling
1005: 
1006: template <Value (*FUNC)(const_data_ptr_t input)>
1007: static unique_ptr<BaseStatistics> templated_get_numeric_stats(const LogicalType &type,
1008:                                                               const parquet::format::Statistics &parquet_stats) {
1009: 	auto stats = make_unique<NumericStatistics>(type);
1010: 
1011: 	// for reasons unknown to science, Parquet defines *both* `min` and `min_value` as well as `max` and
1012: 	// `max_value`. All are optional. such elegance.
1013: 	if (parquet_stats.__isset.min) {
1014: 		stats->min = FUNC((const_data_ptr_t)parquet_stats.min.data());
1015: 	} else if (parquet_stats.__isset.min_value) {
1016: 		stats->min = FUNC((const_data_ptr_t)parquet_stats.min_value.data());
1017: 	} else {
1018: 		stats->min.is_null = true;
1019: 	}
1020: 	if (parquet_stats.__isset.max) {
1021: 		stats->max = FUNC((const_data_ptr_t)parquet_stats.max.data());
1022: 	} else if (parquet_stats.__isset.max_value) {
1023: 		stats->max = FUNC((const_data_ptr_t)parquet_stats.max_value.data());
1024: 	} else {
1025: 		stats->max.is_null = true;
1026: 	}
1027:     // GCC 4.x insists on a move() here
1028: 	return move(stats);
1029: }
1030: 
1031: template <class T> static Value transform_statistics_plain(const_data_ptr_t input) {
1032: 	return Value(Load<T>(input));
1033: }
1034: 
1035: static Value transform_statistics_timestamp_ms(const_data_ptr_t input) {
1036: 	return Value::TIMESTAMP(arrow_timestamp_ms_to_timestamp(Load<int64_t>(input)));
1037: }
1038: 
1039: static Value transform_statistics_timestamp_micros(const_data_ptr_t input) {
1040: 	return Value::TIMESTAMP(arrow_timestamp_micros_to_timestamp(Load<int64_t>(input)));
1041: }
1042: 
1043: static Value transform_statistics_timestamp_impala(const_data_ptr_t input) {
1044: 	return Value::TIMESTAMP(impala_timestamp_to_timestamp_t(Load<Int96>(input)));
1045: }
1046: 
1047: unique_ptr<BaseStatistics> ParquetReader::ReadStatistics(LogicalType &type, column_t column_index,
1048:                                                          const parquet::format::FileMetaData *file_meta_data) {
1049: 	unique_ptr<BaseStatistics> column_stats;
1050: 
1051: 	for (auto &row_group : file_meta_data->row_groups) {
1052: 		D_ASSERT(column_index < row_group.columns.size());
1053: 		auto &column_chunk = row_group.columns[column_index];
1054: 		if (!column_chunk.__isset.meta_data || !column_chunk.meta_data.__isset.statistics) {
1055: 			// no stats present for row group
1056: 			return nullptr;
1057: 		}
1058: 		auto &parquet_stats = column_chunk.meta_data.statistics;
1059: 		unique_ptr<BaseStatistics> row_group_stats;
1060: 
1061: 		switch (type.id()) {
1062: 		case LogicalTypeId::INTEGER:
1063: 			row_group_stats = templated_get_numeric_stats<transform_statistics_plain<int32_t>>(type, parquet_stats);
1064: 			break;
1065: 
1066: 		case LogicalTypeId::BIGINT:
1067: 			row_group_stats = templated_get_numeric_stats<transform_statistics_plain<int64_t>>(type, parquet_stats);
1068: 			break;
1069: 
1070: 		case LogicalTypeId::FLOAT:
1071: 			row_group_stats = templated_get_numeric_stats<transform_statistics_plain<float>>(type, parquet_stats);
1072: 			break;
1073: 
1074: 		case LogicalTypeId::DOUBLE:
1075: 			row_group_stats = templated_get_numeric_stats<transform_statistics_plain<double>>(type, parquet_stats);
1076: 			break;
1077: 
1078: 			// here we go, our favorite type
1079: 		case LogicalTypeId::TIMESTAMP: {
1080: 			auto &s_ele = file_meta_data->schema[column_index + 1];
1081: 			switch (s_ele.type) {
1082: 			case Type::INT64:
1083: 				// arrow timestamp
1084: 				switch (s_ele.converted_type) {
1085: 				case ConvertedType::TIMESTAMP_MICROS:
1086: 					row_group_stats =
1087: 					    templated_get_numeric_stats<transform_statistics_timestamp_micros>(type, parquet_stats);
1088: 					break;
1089: 				case ConvertedType::TIMESTAMP_MILLIS:
1090: 					row_group_stats =
1091: 					    templated_get_numeric_stats<transform_statistics_timestamp_ms>(type, parquet_stats);
1092: 					break;
1093: 				default:
1094: 					return nullptr;
1095: 				}
1096: 				break;
1097: 			case Type::INT96:
1098: 				// impala timestamp
1099: 				row_group_stats =
1100: 				    templated_get_numeric_stats<transform_statistics_timestamp_impala>(type, parquet_stats);
1101: 				break;
1102: 			default:
1103: 				return nullptr;
1104: 			}
1105: 			break;
1106: 		}
1107: 		case LogicalTypeId::VARCHAR: {
1108: 			auto string_stats = make_unique<StringStatistics>(type);
1109: 			if (parquet_stats.__isset.min) {
1110: 				memcpy(string_stats->min, (data_ptr_t)parquet_stats.min.data(),
1111: 				       MinValue<idx_t>(parquet_stats.min.size(), StringStatistics::MAX_STRING_MINMAX_SIZE));
1112: 			} else if (parquet_stats.__isset.min_value) {
1113: 				memcpy(string_stats->min, (data_ptr_t)parquet_stats.min_value.data(),
1114: 				       MinValue<idx_t>(parquet_stats.min_value.size(), StringStatistics::MAX_STRING_MINMAX_SIZE));
1115: 			} else {
1116: 				return nullptr;
1117: 			}
1118: 			if (parquet_stats.__isset.max) {
1119: 				memcpy(string_stats->max, (data_ptr_t)parquet_stats.max.data(),
1120: 				       MinValue<idx_t>(parquet_stats.max.size(), StringStatistics::MAX_STRING_MINMAX_SIZE));
1121: 			} else if (parquet_stats.__isset.max_value) {
1122: 				memcpy(string_stats->max, (data_ptr_t)parquet_stats.max_value.data(),
1123: 				       MinValue<idx_t>(parquet_stats.max_value.size(), StringStatistics::MAX_STRING_MINMAX_SIZE));
1124: 			} else {
1125: 				return nullptr;
1126: 			}
1127: 
1128: 			string_stats->has_unicode = true; // we dont know better
1129: 			row_group_stats = move(string_stats);
1130: 			break;
1131: 		}
1132: 		default:
1133: 			// no stats for you
1134: 			break;
1135: 		} // end of type switch
1136: 
1137: 		// null count is generic
1138: 		if (row_group_stats) {
1139: 			if (parquet_stats.__isset.null_count) {
1140: 				row_group_stats->has_null = parquet_stats.null_count > 0;
1141: 			} else {
1142: 				row_group_stats->has_null = true;
1143: 			}
1144: 		} else {
1145: 			// if stats are missing from any row group we know squat
1146: 			return nullptr;
1147: 		}
1148: 
1149: 		if (!column_stats) {
1150: 			column_stats = move(row_group_stats);
1151: 		} else {
1152: 			column_stats->Merge(*row_group_stats);
1153: 		}
1154: 	}
1155: 	return column_stats;
1156: }
1157: 
1158: } // namespace duckdb
[end of extension/parquet/parquet_reader.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: