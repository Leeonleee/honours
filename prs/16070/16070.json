{
  "repo": "duckdb/duckdb",
  "pull_number": 16070,
  "instance_id": "duckdb__duckdb-16070",
  "issue_numbers": [
    "16052",
    "16052",
    "15791"
  ],
  "base_commit": "3240832cddb955d4240cf2d3adcc053b77c88717",
  "patch": "diff --git a/.github/actions/build_extensions/action.yml b/.github/actions/build_extensions/action.yml\nindex 3163dff31ff2..c98528ff6a6c 100644\n--- a/.github/actions/build_extensions/action.yml\n+++ b/.github/actions/build_extensions/action.yml\n@@ -233,7 +233,7 @@ runs:\n \n     # Run the unittests (excluding the out-of-tree tests) with the extensions that we deployed to S3\n     - name: Test deployed extensions\n-      if: ${{ inputs.run_tests == 1 }}\n+      if: ${{ inputs.deploy_as != '' && inputs.run_tests == 1 }}\n       shell: bash\n       env:\n         AWS_ACCESS_KEY_ID: ${{ inputs.s3_id }}\ndiff --git a/.github/workflows/BundleStaticLibs.yml b/.github/workflows/BundleStaticLibs.yml\nindex 4757eec2e163..97acc0efebe9 100644\n--- a/.github/workflows/BundleStaticLibs.yml\n+++ b/.github/workflows/BundleStaticLibs.yml\n@@ -93,7 +93,7 @@ jobs:\n           AWS_ACCESS_KEY_ID: ${{ secrets.S3_DUCKDB_STAGING_ID }}\n           AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_DUCKDB_STAGING_KEY }}\n         run: |\n-          python scripts/amalgamation.py\n+          python3 scripts/amalgamation.py\n           zip -j static-lib-osx-${{ matrix.architecture }}.zip src/include/duckdb.h build/release/libduckdb_bundle.a\n           ./scripts/upload-assets-to-staging.sh github_release static-lib-osx-${{ matrix.architecture }}.zip\n \n@@ -124,7 +124,6 @@ jobs:\n \n       - uses: ./.github/actions/build_extensions\n         with:\n-          deploy_as: windows_amd64_mingw\n           duckdb_arch: windows_amd64_mingw\n           vcpkg_target_triplet: x64-mingw-static\n           treat_warn_as_error: 0\n@@ -153,4 +152,91 @@ jobs:\n         with:\n           name: duckdb-static-lib-windows-mingw\n           path: |\n-            static-lib-windows-mingw.zip\n\\ No newline at end of file\n+            static-lib-windows-mingw.zip\n+  bundle-linux-arm64-static-libs:\n+    name: Linux arm64 static libs\n+    runs-on: ubuntu-latest\n+\n+    steps:\n+      - uses: actions/checkout@v4\n+        with:\n+          fetch-depth: 0\n+          ref: ${{ inputs.git_ref }}\n+\n+      - name: Build\n+        shell: bash\n+        run: |\n+          docker run                                                             \\\n+          -v.:/duckdb                                                            \\\n+          -e CC=aarch64-linux-gnu-gcc                                            \\\n+          -e CXX=aarch64-linux-gnu-g++                                           \\\n+          -e CMAKE_BUILD_PARALLEL_LEVEL=2                                        \\\n+          -e OVERRIDE_GIT_DESCRIBE=$OVERRIDE_GIT_DESCRIBE                        \\\n+          -e EXTENSION_CONFIGS='/duckdb/.github/config/bundled_extensions.cmake' \\\n+          -e ENABLE_EXTENSION_AUTOLOADING=1                                      \\\n+          -e ENABLE_EXTENSION_AUTOINSTALL=1                                      \\\n+          -e FORCE_WARN_UNUSED=1                                                 \\\n+          -e DUCKDB_PLATFORM=linux_arm64                                         \\\n+          ubuntu:18.04                                                           \\\n+          bash -c \"/duckdb/scripts/setup_ubuntu1804.sh && git config --global --add safe.directory /duckdb && make bundle-library -C /duckdb\"\n+      - name: Deploy\n+        shell: bash\n+        env:\n+          AWS_ACCESS_KEY_ID: ${{ secrets.S3_DUCKDB_STAGING_ID }}\n+          AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_DUCKDB_STAGING_KEY }}\n+        run: |\n+          python3 scripts/amalgamation.py\n+          zip -j static-lib-linux-arm64.zip src/include/duckdb.h build/release/libduckdb_bundle.a\n+          ./scripts/upload-assets-to-staging.sh github_release static-lib-linux-arm64.zip\n+      - uses: actions/upload-artifact@v4\n+        with:\n+          name: duckdb-static-lib-linux-arm64\n+          path: |\n+            static-lib-linux-arm64.zip\n+  bundle-linux-amd64-static-libs:\n+    name: Linux amd64 static libs\n+    runs-on: ubuntu-latest\n+\n+    steps:\n+      - uses: actions/checkout@v4\n+        with:\n+          fetch-depth: 0\n+          ref: ${{ inputs.git_ref }}\n+\n+      - name: Install pytest\n+        run: |\n+          python3 -m pip install pytest\n+      - name: Build\n+        shell: bash\n+        run: |\n+          export PWD=`pwd`\n+          docker run                                                             \\\n+          -v$PWD:$PWD                                                            \\\n+          -e CMAKE_BUILD_PARALLEL_LEVEL=2                                        \\\n+          -e OVERRIDE_GIT_DESCRIBE=$OVERRIDE_GIT_DESCRIBE                        \\\n+          -e EXTENSION_CONFIGS=\"$PWD/.github/config/bundled_extensions.cmake\"    \\\n+          -e ENABLE_EXTENSION_AUTOLOADING=1                                      \\\n+          -e ENABLE_EXTENSION_AUTOINSTALL=1                                      \\\n+          -e BUILD_BENCHMARK=1                                                   \\\n+          -e FORCE_WARN_UNUSED=1                                                 \\\n+          -e DUCKDB_RUN_PARALLEL_CSV_TESTS=1                                     \\\n+          quay.io/pypa/manylinux2014_x86_64                                      \\\n+          bash -c \"yum install -y perl-IPC-Cmd && git config --global --add safe.directory $PWD && make bundle-library -C $PWD\"\n+      - name: Print platform\n+        shell: bash\n+        run: ./build/release/duckdb -c \"PRAGMA platform;\"\n+\n+      - name: Deploy\n+        shell: bash\n+        env:\n+          AWS_ACCESS_KEY_ID: ${{ secrets.S3_DUCKDB_STAGING_ID }}\n+          AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_DUCKDB_STAGING_KEY }}\n+        run: |\n+          python3 scripts/amalgamation.py\n+          zip -j static-lib-linux-amd64.zip src/include/duckdb.h build/release/libduckdb_bundle.a\n+          ./scripts/upload-assets-to-staging.sh github_release static-lib-linux-amd64.zip\n+      - uses: actions/upload-artifact@v4\n+        with:\n+          name: duckdb-static-lib-linux-amd64\n+          path: |\n+            static-lib-linux-amd64.zip\n\\ No newline at end of file\ndiff --git a/.github/workflows/LinuxRelease.yml b/.github/workflows/LinuxRelease.yml\nindex 7facfe26c94a..2b5f7a7d3769 100644\n--- a/.github/workflows/LinuxRelease.yml\n+++ b/.github/workflows/LinuxRelease.yml\n@@ -123,7 +123,7 @@ jobs:\n       shell: bash\n       if: ${{ inputs.skip_tests != 'true' }}\n       run: |\n-        build/release/benchmark/benchmark_runner benchmark/tpch/sf1/q01.benchmark\n+        build/release/benchmark/benchmark_runner benchmark/micro/update/update_with_join.benchmark\n         build/release/duckdb -c \"COPY (SELECT 42) TO '/dev/stdout' (FORMAT PARQUET)\" | cat\n \n  linux-release-aarch64:\ndiff --git a/.github/workflows/R.yml b/.github/workflows/R.yml\nindex 36400179a812..21afdd29009b 100644\n--- a/.github/workflows/R.yml\n+++ b/.github/workflows/R.yml\n@@ -62,7 +62,6 @@ jobs:\n \n       - uses: ./.github/actions/build_extensions\n         with:\n-          deploy_as: windows_amd64_mingw\n           duckdb_arch: windows_amd64_mingw\n           vcpkg_target_triplet: x64-mingw-static\n           treat_warn_as_error: 0\ndiff --git a/.github/workflows/SwiftRelease.yml b/.github/workflows/SwiftRelease.yml\nindex 9708ae55faf2..0d0d3c4bf875 100644\n--- a/.github/workflows/SwiftRelease.yml\n+++ b/.github/workflows/SwiftRelease.yml\n@@ -61,5 +61,9 @@ jobs:\n           cd source-repo\n           export TAG_NAME=`python3 -c \"import sys, os; sys.path.append(os.path.join('scripts')); import package_build; print(package_build.git_dev_version())\"`\n           cd ..\n-          git -C updated-repo tag -a $TAG_NAME -m \"Release $TAG_NAME\"\n-          git -C updated-repo push origin $TAG_NAME\n+          if [[ $(git -C updated-repo tag -l $TAG_NAME) ]]; then\n+            echo 'Tag '$TAG_NAME' already exists - skipping'\n+          else\n+            git -C updated-repo tag -a $TAG_NAME -m \"Release $TAG_NAME\"\n+            git -C updated-repo push origin $TAG_NAME\n+          fi\ndiff --git a/.github/workflows/Windows.yml b/.github/workflows/Windows.yml\nindex 57b9b41ded0b..c1db5b18b8d3 100644\n--- a/.github/workflows/Windows.yml\n+++ b/.github/workflows/Windows.yml\n@@ -280,7 +280,6 @@ jobs:\n      - uses: ./.github/actions/build_extensions\n        with:\n          vcpkg_target_triplet: x64-windows-static-md\n-         deploy_as: windows_amd64\n          treat_warn_as_error: 0\n          run_tests: ${{ inputs.skip_tests != 'true' && 1 || 0 }}\n          run_autoload_tests: ${{ inputs.skip_tests != 'true' && 1 || 0 }}\ndiff --git a/extension/jemalloc/jemalloc/README.md b/extension/jemalloc/jemalloc/README.md\nindex 81b6669327e9..49bb632e4685 100644\n--- a/extension/jemalloc/jemalloc/README.md\n+++ b/extension/jemalloc/jemalloc/README.md\n@@ -79,6 +79,10 @@ jemalloc_constructor(void) {\n \tif (cpu_count == 0) {\n \t\tcpu_count = duckdb_malloc_ncpus();\n \t}\n+\tunsigned long long narenas = cpu_count / 2;\n+\tif (narenas == 0) {\n+\t    narenas = 1;\n+\t}\n \tunsigned long long bgt_count = cpu_count / 16;\n \tif (bgt_count == 0) {\n \t\tbgt_count = 1;\n@@ -86,9 +90,9 @@ jemalloc_constructor(void) {\n \t// decay is in ms\n \tunsigned long long decay = DUCKDB_JEMALLOC_DECAY * 1000;\n #ifdef DEBUG\n-\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"junk:true,oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count / 2, bgt_count);\n+\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"junk:true,oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, narenas, bgt_count);\n #else\n-\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count / 2, bgt_count);\n+\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, narenas, bgt_count);\n #endif\n \tje_malloc_conf = JE_MALLOC_CONF_BUFFER;\n \tmalloc_init();\ndiff --git a/extension/jemalloc/jemalloc/src/jemalloc.c b/extension/jemalloc/jemalloc/src/jemalloc.c\nindex 292d553aa360..64dca839ecad 100644\n--- a/extension/jemalloc/jemalloc/src/jemalloc.c\n+++ b/extension/jemalloc/jemalloc/src/jemalloc.c\n@@ -4275,6 +4275,10 @@ jemalloc_constructor(void) {\n \tif (cpu_count == 0) {\n \t\tcpu_count = duckdb_malloc_ncpus();\n \t}\n+\tunsigned long long narenas = cpu_count / 2;\n+\tif (narenas == 0) {\n+\t    narenas = 1;\n+\t}\n \tunsigned long long bgt_count = cpu_count / 16;\n \tif (bgt_count == 0) {\n \t\tbgt_count = 1;\n@@ -4282,9 +4286,9 @@ jemalloc_constructor(void) {\n \t// decay is in ms\n \tunsigned long long decay = DUCKDB_JEMALLOC_DECAY * 1000;\n #ifdef DEBUG\n-\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"junk:true,oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count / 2, bgt_count);\n+\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"junk:true,oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, narenas, bgt_count);\n #else\n-\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count / 2, bgt_count);\n+\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, narenas, bgt_count);\n #endif\n \tje_malloc_conf = JE_MALLOC_CONF_BUFFER;\n \tmalloc_init();\ndiff --git a/extension/parquet/include/parquet_reader.hpp b/extension/parquet/include/parquet_reader.hpp\nindex df537ea04b32..0061ede2dd14 100644\n--- a/extension/parquet/include/parquet_reader.hpp\n+++ b/extension/parquet/include/parquet_reader.hpp\n@@ -216,12 +216,6 @@ class ParquetReader {\n \tvoid PrepareRowGroupBuffer(ParquetReaderScanState &state, idx_t out_col_idx);\n \tLogicalType DeriveLogicalType(const SchemaElement &s_ele);\n \n-\ttemplate <typename... Args>\n-\tstd::runtime_error FormatException(const string fmt_str, Args... params) {\n-\t\treturn std::runtime_error(\"Failed to read Parquet file \\\"\" + file_name +\n-\t\t                          \"\\\": \" + StringUtil::Format(fmt_str, params...));\n-\t}\n-\n private:\n \tunique_ptr<FileHandle> file_handle;\n };\ndiff --git a/extension/parquet/parquet_reader.cpp b/extension/parquet/parquet_reader.cpp\nindex 6c5fd8f714c6..c16c3b183a9c 100644\n--- a/extension/parquet/parquet_reader.cpp\n+++ b/extension/parquet/parquet_reader.cpp\n@@ -492,7 +492,8 @@ void ParquetReader::InitializeSchema(ClientContext &context) {\n \t}\n \t// check if we like this schema\n \tif (file_meta_data->schema.size() < 2) {\n-\t\tthrow FormatException(\"Need at least one non-root column in the file\");\n+\t\tthrow InvalidInputException(\"Failed to read Parquet file '%s': Need at least one non-root column in the file\",\n+\t\t                            file_name);\n \t}\n \troot_reader = CreateReader(context);\n \tauto &root_type = root_reader->Type();\ndiff --git a/src/common/types/column/column_data_collection.cpp b/src/common/types/column/column_data_collection.cpp\nindex d6e01e5a663d..17be6722389b 100644\n--- a/src/common/types/column/column_data_collection.cpp\n+++ b/src/common/types/column/column_data_collection.cpp\n@@ -799,7 +799,10 @@ static bool IsComplexType(const LogicalType &type) {\n \n void ColumnDataCollection::Append(ColumnDataAppendState &state, DataChunk &input) {\n \tD_ASSERT(!finished_append);\n-\tD_ASSERT(types == input.GetTypes());\n+\t{\n+\t\tauto input_types = input.GetTypes();\n+\t\tD_ASSERT(types == input_types);\n+\t}\n \n \tauto &segment = *segments.back();\n \tfor (idx_t vector_idx = 0; vector_idx < types.size(); vector_idx++) {\ndiff --git a/src/execution/index/fixed_size_allocator.cpp b/src/execution/index/fixed_size_allocator.cpp\nindex 860e45d46ff5..7da8b547899d 100644\n--- a/src/execution/index/fixed_size_allocator.cpp\n+++ b/src/execution/index/fixed_size_allocator.cpp\n@@ -253,6 +253,12 @@ FixedSizeAllocatorInfo FixedSizeAllocator::GetInfo() const {\n \n \tfor (const auto &buffer : buffers) {\n \t\tinfo.buffer_ids.push_back(buffer.first);\n+\n+\t\t// Memory safety check.\n+\t\tif (buffer.first > idx_t(MAX_ROW_ID)) {\n+\t\t\tthrow InternalException(\"Initializing invalid buffer ID in FixedSizeAllocator::GetInfo\");\n+\t\t}\n+\n \t\tinfo.block_pointers.push_back(buffer.second->block_pointer);\n \t\tinfo.segment_counts.push_back(buffer.second->segment_count);\n \t\tinfo.allocation_sizes.push_back(buffer.second->allocation_size);\n@@ -289,6 +295,12 @@ void FixedSizeAllocator::Init(const FixedSizeAllocatorInfo &info) {\n \n \t\t// read all FixedSizeBuffer data\n \t\tauto buffer_id = info.buffer_ids[i];\n+\n+\t\t// Memory safety check.\n+\t\tif (buffer_id > idx_t(MAX_ROW_ID)) {\n+\t\t\tthrow InternalException(\"Initializing invalid buffer ID in FixedSizeAllocator::Init\");\n+\t\t}\n+\n \t\tauto buffer_block_pointer = info.block_pointers[i];\n \t\tauto segment_count = info.segment_counts[i];\n \t\tauto allocation_size = info.allocation_sizes[i];\ndiff --git a/src/execution/index/unbound_index.cpp b/src/execution/index/unbound_index.cpp\nindex b8173d751243..86f224feede4 100644\n--- a/src/execution/index/unbound_index.cpp\n+++ b/src/execution/index/unbound_index.cpp\n@@ -14,6 +14,16 @@ UnboundIndex::UnboundIndex(unique_ptr<CreateInfo> create_info, IndexStorageInfo\n                            TableIOManager &table_io_manager, AttachedDatabase &db)\n     : Index(create_info->Cast<CreateIndexInfo>().column_ids, table_io_manager, db), create_info(std::move(create_info)),\n       storage_info(std::move(storage_info_p)) {\n+\n+\t// Memory safety check.\n+\tfor (idx_t info_idx = 0; info_idx < storage_info.allocator_infos.size(); info_idx++) {\n+\t\tauto &info = storage_info.allocator_infos[info_idx];\n+\t\tfor (idx_t buffer_idx = 0; buffer_idx < info.buffer_ids.size(); buffer_idx++) {\n+\t\t\tif (info.buffer_ids[buffer_idx] > idx_t(MAX_ROW_ID)) {\n+\t\t\t\tthrow InternalException(\"Found invalid buffer ID in UnboundIndex constructor\");\n+\t\t\t}\n+\t\t}\n+\t}\n }\n \n void UnboundIndex::CommitDrop() {\ndiff --git a/src/execution/operator/persistent/physical_insert.cpp b/src/execution/operator/persistent/physical_insert.cpp\nindex a3c6619ed5e9..161fa8f58a20 100644\n--- a/src/execution/operator/persistent/physical_insert.cpp\n+++ b/src/execution/operator/persistent/physical_insert.cpp\n@@ -253,6 +253,7 @@ static void CreateUpdateChunk(ExecutionContext &context, DataChunk &chunk, Table\n \t\t\tchunk.SetCardinality(selection.Count());\n \t\t\t// Also apply this Slice to the to-update row_ids\n \t\t\trow_ids.Slice(selection.Selection(), selection.Count());\n+\t\t\trow_ids.Flatten(selection.Count());\n \t\t}\n \t}\n \n@@ -271,8 +272,9 @@ static void CreateUpdateChunk(ExecutionContext &context, DataChunk &chunk, Table\n }\n \n template <bool GLOBAL>\n-static idx_t PerformOnConflictAction(InsertLocalState &lstate, ExecutionContext &context, DataChunk &chunk,\n-                                     TableCatalogEntry &table, Vector &row_ids, const PhysicalInsert &op) {\n+static idx_t PerformOnConflictAction(InsertLocalState &lstate, InsertGlobalState &gstate, ExecutionContext &context,\n+                                     DataChunk &chunk, TableCatalogEntry &table, Vector &row_ids,\n+                                     const PhysicalInsert &op) {\n \t// Early-out, if we do nothing on conflicting rows.\n \tif (op.action_type == OnConflictAction::NOTHING) {\n \t\treturn 0;\n@@ -283,15 +285,8 @@ static idx_t PerformOnConflictAction(InsertLocalState &lstate, ExecutionContext\n \tCreateUpdateChunk(context, chunk, table, row_ids, update_chunk, op);\n \tauto &data_table = table.GetStorage();\n \n-\t// Perform the UPDATE on the (global) storage.\n-\tif (!op.update_is_del_and_insert) {\n-\t\tif (GLOBAL) {\n-\t\t\tauto update_state = data_table.InitializeUpdate(table, context.client, op.bound_constraints);\n-\t\t\tdata_table.Update(*update_state, context.client, row_ids, set_columns, update_chunk);\n-\t\t\treturn update_chunk.size();\n-\t\t}\n-\t\tauto &local_storage = LocalStorage::Get(context.client, data_table.db);\n-\t\tlocal_storage.Update(data_table, row_ids, set_columns, update_chunk);\n+\tif (update_chunk.size() == 0) {\n+\t\t// Nothing to do\n \t\treturn update_chunk.size();\n \t}\n \n@@ -305,6 +300,27 @@ static idx_t PerformOnConflictAction(InsertLocalState &lstate, ExecutionContext\n \t\tappend_chunk.data[set_columns[i].index].Reference(update_chunk.data[i]);\n \t}\n \n+\t// Perform the UPDATE on the (global) storage.\n+\tif (!op.update_is_del_and_insert) {\n+\t\tif (!op.parallel && op.return_chunk) {\n+\t\t\tgstate.return_collection.Append(append_chunk);\n+\t\t}\n+\n+\t\tif (GLOBAL) {\n+\t\t\tauto update_state = data_table.InitializeUpdate(table, context.client, op.bound_constraints);\n+\t\t\tdata_table.Update(*update_state, context.client, row_ids, set_columns, update_chunk);\n+\t\t\treturn update_chunk.size();\n+\t\t}\n+\t\tauto &local_storage = LocalStorage::Get(context.client, data_table.db);\n+\t\tif (gstate.initialized) {\n+\t\t\t// Flush the data first, it might be referenced by the Update\n+\t\t\tdata_table.FinalizeLocalAppend(gstate.append_state);\n+\t\t\tgstate.initialized = false;\n+\t\t}\n+\t\tlocal_storage.Update(data_table, row_ids, set_columns, update_chunk);\n+\t\treturn update_chunk.size();\n+\t}\n+\n \tif (GLOBAL) {\n \t\tauto &delete_state = lstate.GetDeleteState(data_table, table, context.client);\n \t\tdata_table.Delete(delete_state, context.client, row_ids, update_chunk.size());\n@@ -313,6 +329,9 @@ static idx_t PerformOnConflictAction(InsertLocalState &lstate, ExecutionContext\n \t\tlocal_storage.Delete(data_table, row_ids, update_chunk.size());\n \t}\n \n+\tif (!op.parallel && op.return_chunk) {\n+\t\tgstate.return_collection.Append(append_chunk);\n+\t}\n \tdata_table.LocalAppend(table, context.client, append_chunk, op.bound_constraints, row_ids, append_chunk);\n \treturn update_chunk.size();\n }\n@@ -365,8 +384,8 @@ static void CheckDistinctnessInternal(ValidityMask &valid, vector<reference<Vect\n \t}\n }\n \n-void PrepareSortKeys(DataChunk &input, unordered_map<column_t, unique_ptr<Vector>> &sort_keys,\n-                     const unordered_set<column_t> &column_ids) {\n+static void PrepareSortKeys(DataChunk &input, unordered_map<column_t, unique_ptr<Vector>> &sort_keys,\n+                            const unordered_set<column_t> &column_ids) {\n \tOrderModifiers order_modifiers(OrderType::ASCENDING, OrderByNullType::NULLS_LAST);\n \tfor (auto &it : column_ids) {\n \t\tauto &sort_key = sort_keys[it];\n@@ -448,7 +467,7 @@ static void VerifyOnConflictCondition(ExecutionContext &context, DataChunk &comb\n \n template <bool GLOBAL>\n static idx_t HandleInsertConflicts(TableCatalogEntry &table, ExecutionContext &context, InsertLocalState &lstate,\n-                                   DataChunk &tuples, const PhysicalInsert &op) {\n+                                   InsertGlobalState &gstate, DataChunk &tuples, const PhysicalInsert &op) {\n \tauto &types_to_fetch = op.types_to_fetch;\n \tauto &on_conflict_condition = op.on_conflict_condition;\n \tauto &conflict_target = op.conflict_target;\n@@ -518,7 +537,7 @@ static idx_t HandleInsertConflicts(TableCatalogEntry &table, ExecutionContext &c\n \t\tRegisterUpdatedRows(lstate, row_ids, combined_chunk.size());\n \t}\n \n-\taffected_tuples += PerformOnConflictAction<GLOBAL>(lstate, context, combined_chunk, table, row_ids, op);\n+\taffected_tuples += PerformOnConflictAction<GLOBAL>(lstate, gstate, context, combined_chunk, table, row_ids, op);\n \n \t// Remove the conflicting tuples from the insert chunk\n \tSelectionVector sel_vec(tuples.size());\n@@ -598,6 +617,11 @@ idx_t PhysicalInsert::OnConflictHandling(TableCatalogEntry &table, ExecutionCont\n \t\t\t}\n \t\t}\n \t\tif (action_type == OnConflictAction::UPDATE) {\n+\t\t\tif (do_update_condition) {\n+\t\t\t\t//! See https://github.com/duckdblabs/duckdb-internal/issues/4090 for context\n+\t\t\t\tthrow NotImplementedException(\"Inner conflicts detected with a conditional DO UPDATE on-conflict \"\n+\t\t\t\t                              \"action, not fully implemented yet\");\n+\t\t\t}\n \t\t\tManagedSelection last_occurrences(last_occurrences_of_conflict.size());\n \t\t\tfor (auto &idx : last_occurrences_of_conflict) {\n \t\t\t\tlast_occurrences.Append(idx);\n@@ -615,9 +639,9 @@ idx_t PhysicalInsert::OnConflictHandling(TableCatalogEntry &table, ExecutionCont\n \t// Check whether any conflicts arise, and if they all meet the conflict_target + condition\n \t// If that's not the case - We throw the first error\n \tidx_t updated_tuples = 0;\n-\tupdated_tuples += HandleInsertConflicts<true>(table, context, lstate, lstate.insert_chunk, *this);\n+\tupdated_tuples += HandleInsertConflicts<true>(table, context, lstate, gstate, lstate.insert_chunk, *this);\n \t// Also check the transaction-local storage+ART so we can detect conflicts within this transaction\n-\tupdated_tuples += HandleInsertConflicts<false>(table, context, lstate, lstate.insert_chunk, *this);\n+\tupdated_tuples += HandleInsertConflicts<false>(table, context, lstate, gstate, lstate.insert_chunk, *this);\n \n \treturn updated_tuples;\n }\n@@ -636,31 +660,22 @@ SinkResultType PhysicalInsert::Sink(ExecutionContext &context, DataChunk &chunk,\n \t\t\tgstate.initialized = true;\n \t\t}\n \n-\t\tif (action_type != OnConflictAction::NOTHING && return_chunk) {\n-\t\t\t// If the action is UPDATE or REPLACE, we will always create either an APPEND or an INSERT\n-\t\t\t// for NOTHING we don't create either an APPEND or an INSERT for the tuple\n-\t\t\t// so it should not be added to the RETURNING chunk\n-\t\t\tgstate.return_collection.Append(lstate.insert_chunk);\n-\t\t}\n \t\tidx_t updated_tuples = OnConflictHandling(table, context, gstate, lstate);\n-\t\tif (action_type == OnConflictAction::NOTHING && return_chunk) {\n-\t\t\t// Because we didn't add to the RETURNING chunk yet\n-\t\t\t// we add the tuples that did not get filtered out now\n-\t\t\tgstate.return_collection.Append(lstate.insert_chunk);\n-\t\t}\n+\n \t\tgstate.insert_count += lstate.insert_chunk.size();\n \t\tgstate.insert_count += updated_tuples;\n+\t\tif (!parallel && return_chunk) {\n+\t\t\tgstate.return_collection.Append(lstate.insert_chunk);\n+\t\t}\n \t\tstorage.LocalAppend(gstate.append_state, context.client, lstate.insert_chunk, true);\n \t\tif (action_type == OnConflictAction::UPDATE && lstate.update_chunk.size() != 0) {\n-\t\t\t// Flush the append so we can target the data we just appended with the update\n-\t\t\tstorage.FinalizeLocalAppend(gstate.append_state);\n-\t\t\tgstate.initialized = false;\n-\t\t\t(void)HandleInsertConflicts<true>(table, context, lstate, lstate.update_chunk, *this);\n-\t\t\t(void)HandleInsertConflicts<false>(table, context, lstate, lstate.update_chunk, *this);\n+\t\t\t(void)HandleInsertConflicts<true>(table, context, lstate, gstate, lstate.update_chunk, *this);\n+\t\t\t(void)HandleInsertConflicts<false>(table, context, lstate, gstate, lstate.update_chunk, *this);\n \t\t\t// All of the tuples should have been turned into an update, leaving the chunk empty afterwards\n \t\t\tD_ASSERT(lstate.update_chunk.size() == 0);\n \t\t}\n \t} else {\n+\t\t//! FIXME: can't we enable this by using a BatchedDataCollection ?\n \t\tD_ASSERT(!return_chunk);\n \t\t// parallel append\n \t\tif (!lstate.local_collection) {\ndiff --git a/src/execution/operator/schema/physical_attach.cpp b/src/execution/operator/schema/physical_attach.cpp\nindex 3d3d7b187d44..523f0d57ea32 100644\n--- a/src/execution/operator/schema/physical_attach.cpp\n+++ b/src/execution/operator/schema/physical_attach.cpp\n@@ -25,9 +25,6 @@ SourceResultType PhysicalAttach::GetData(ExecutionContext &context, DataChunk &c\n \tif (options.db_type.empty()) {\n \t\tDBPathAndType::ExtractExtensionPrefix(path, options.db_type);\n \t}\n-\tif (!config.options.enable_external_access && !options.db_type.empty()) {\n-\t\tthrow PermissionException(\"Attaching external databases is disabled through configuration\");\n-\t}\n \tif (name.empty()) {\n \t\tauto &fs = FileSystem::GetFileSystem(context.client);\n \t\tname = AttachedDatabase::ExtractDatabaseName(path, fs);\ndiff --git a/src/execution/sample/reservoir_sample.cpp b/src/execution/sample/reservoir_sample.cpp\nindex 58056920eaf8..b28bed81b0e9 100644\n--- a/src/execution/sample/reservoir_sample.cpp\n+++ b/src/execution/sample/reservoir_sample.cpp\n@@ -551,7 +551,7 @@ void ReservoirSample::ExpandSerializedSample() {\n }\n \n idx_t ReservoirSample::GetReservoirChunkCapacity() const {\n-\treturn sample_count + (FIXED_SAMPLE_SIZE_MULTIPLIER * FIXED_SAMPLE_SIZE);\n+\treturn sample_count + (FIXED_SAMPLE_SIZE_MULTIPLIER * MinValue<idx_t>(sample_count, FIXED_SAMPLE_SIZE));\n }\n \n idx_t ReservoirSample::FillReservoir(DataChunk &chunk) {\ndiff --git a/src/function/table/table_scan.cpp b/src/function/table/table_scan.cpp\nindex 5317f49ffa52..0cf85d64a2b9 100644\n--- a/src/function/table/table_scan.cpp\n+++ b/src/function/table/table_scan.cpp\n@@ -576,7 +576,8 @@ unique_ptr<GlobalTableFunctionState> TableScanInitGlobal(ClientContext &context,\n \t}\n \n \t// The checkpoint lock ensures that we do not checkpoint while scanning this table.\n-\tauto checkpoint_lock = storage.GetSharedCheckpointLock();\n+\tauto &transaction = DuckTransaction::Get(context, storage.db);\n+\tauto checkpoint_lock = transaction.SharedLockTable(*storage.GetDataTableInfo());\n \tauto &info = storage.GetDataTableInfo();\n \tauto &indexes = info->GetIndexes();\n \tif (indexes.Empty()) {\ndiff --git a/src/include/duckdb/execution/index/fixed_size_allocator.hpp b/src/include/duckdb/execution/index/fixed_size_allocator.hpp\nindex d4a1b708def8..3b2e4c28781c 100644\n--- a/src/include/duckdb/execution/index/fixed_size_allocator.hpp\n+++ b/src/include/duckdb/execution/index/fixed_size_allocator.hpp\n@@ -54,8 +54,9 @@ class FixedSizeAllocator {\n \t\tD_ASSERT(ptr.GetOffset() < available_segments_per_buffer);\n \t\tD_ASSERT(buffers.find(ptr.GetBufferId()) != buffers.end());\n \n-\t\tauto &buffer = buffers.find(ptr.GetBufferId())->second;\n-\t\tauto buffer_ptr = buffer->Get(dirty);\n+\t\tauto buffer_it = buffers.find(ptr.GetBufferId());\n+\t\tD_ASSERT(buffer_it != buffers.end());\n+\t\tauto buffer_ptr = buffer_it->second->Get(dirty);\n \t\treturn buffer_ptr + ptr.GetOffset() * segment_size + bitmask_offset;\n \t}\n \ndiff --git a/src/include/duckdb/main/extension_helper.hpp b/src/include/duckdb/main/extension_helper.hpp\nindex 7a7c3ed9e9cf..3c5e04cbe4f8 100644\n--- a/src/include/duckdb/main/extension_helper.hpp\n+++ b/src/include/duckdb/main/extension_helper.hpp\n@@ -236,6 +236,7 @@ class ExtensionHelper {\n \n \tstatic bool IsRelease(const string &version_tag);\n \tstatic bool CreateSuggestions(const string &extension_name, string &message);\n+\tstatic string ExtensionInstallDocumentationLink(const string &extension_name);\n \n private:\n \tstatic unique_ptr<ExtensionInstallInfo> InstallExtensionInternal(DatabaseInstance &db, FileSystem &fs,\ndiff --git a/src/include/duckdb/planner/table_filter.hpp b/src/include/duckdb/planner/table_filter.hpp\nindex b39d529b0873..290f789da1b8 100644\n--- a/src/include/duckdb/planner/table_filter.hpp\n+++ b/src/include/duckdb/planner/table_filter.hpp\n@@ -52,7 +52,7 @@ class TableFilter {\n \tstring DebugToString();\n \tvirtual unique_ptr<TableFilter> Copy() const = 0;\n \tvirtual bool Equals(const TableFilter &other) const {\n-\t\treturn filter_type != other.filter_type;\n+\t\treturn filter_type == other.filter_type;\n \t}\n \tvirtual unique_ptr<Expression> ToExpression(const Expression &column) const = 0;\n \ndiff --git a/src/main/extension/extension_install.cpp b/src/main/extension/extension_install.cpp\nindex e8ab595ab0b7..b36e1b46f9f2 100644\n--- a/src/main/extension/extension_install.cpp\n+++ b/src/main/extension/extension_install.cpp\n@@ -56,6 +56,18 @@ const vector<string> ExtensionHelper::PathComponents() {\n \treturn vector<string> {GetVersionDirectoryName(), DuckDB::Platform()};\n }\n \n+string ExtensionHelper::ExtensionInstallDocumentationLink(const string &extension_name) {\n+\tauto components = PathComponents();\n+\n+\tstring link = \"https://duckdb.org/docs/extensions/troubleshooting\";\n+\n+\tif (components.size() >= 2) {\n+\t\tlink += \"/?version=\" + components[0] + \"&platform=\" + components[1] + \"&extension=\" + extension_name;\n+\t}\n+\n+\treturn link;\n+}\n+\n duckdb::string ExtensionHelper::DefaultExtensionFolder(FileSystem &fs) {\n \tstring home_directory = fs.GetHomeDirectory();\n \t// exception if the home directory does not exist, don't create whatever we think is home\n@@ -444,9 +456,11 @@ static unique_ptr<ExtensionInstallInfo> InstallFromHttpUrl(DatabaseInstance &db,\n \t\tif (!should_retry || retry_count >= MAX_RETRY_COUNT) {\n \t\t\t// if we should not retry or exceeded the number of retries - bubble up the error\n \t\t\tstring message;\n-\t\t\tauto exact_match = ExtensionHelper::CreateSuggestions(extension_name, message);\n-\t\t\tif (exact_match && !ExtensionHelper::IsRelease(DuckDB::LibraryVersion())) {\n-\t\t\t\tmessage += \"\\nAre you using a development build? In this case, extensions might not (yet) be uploaded.\";\n+\t\t\tExtensionHelper::CreateSuggestions(extension_name, message);\n+\n+\t\t\tauto documentation_link = ExtensionHelper::ExtensionInstallDocumentationLink(extension_name);\n+\t\t\tif (!documentation_link.empty()) {\n+\t\t\t\tmessage += \"\\nFor more info, visit \" + documentation_link;\n \t\t\t}\n \t\t\tif (res.error() == duckdb_httplib::Error::Success) {\n \t\t\t\tthrow HTTPException(res.value(), \"Failed to download extension \\\"%s\\\" at URL \\\"%s%s\\\" (HTTP %n)\\n%s\",\ndiff --git a/src/optimizer/remove_unused_columns.cpp b/src/optimizer/remove_unused_columns.cpp\nindex b197a4f5f986..48ea7cbffe83 100644\n--- a/src/optimizer/remove_unused_columns.cpp\n+++ b/src/optimizer/remove_unused_columns.cpp\n@@ -261,9 +261,6 @@ void RemoveUnusedColumns::VisitOperator(LogicalOperator &op) {\n \t\t\t\tif (entry == column_references.end()) {\n \t\t\t\t\tthrow InternalException(\"RemoveUnusedColumns - could not find referenced column\");\n \t\t\t\t}\n-\t\t\t\tif (final_column_ids[col_sel_idx].HasChildren()) {\n-\t\t\t\t\tthrow InternalException(\"RemoveUnusedColumns - LogicalGet::column_ids already has children\");\n-\t\t\t\t}\n \t\t\t\tColumnIndex new_index(final_column_ids[col_sel_idx].GetPrimaryIndex(), entry->second.child_columns);\n \t\t\t\tcolumn_ids.emplace_back(new_index);\n \t\t\t}\ndiff --git a/src/planner/binder/expression/bind_star_expression.cpp b/src/planner/binder/expression/bind_star_expression.cpp\nindex 179fad7da38a..fa9f004ee53d 100644\n--- a/src/planner/binder/expression/bind_star_expression.cpp\n+++ b/src/planner/binder/expression/bind_star_expression.cpp\n@@ -145,7 +145,7 @@ void TryTransformStarLike(unique_ptr<ParsedExpression> &root) {\n \t\treturn;\n \t}\n \tauto &function = root->Cast<FunctionExpression>();\n-\tif (function.children.size() != 2) {\n+\tif (function.children.size() < 2 || function.children.size() > 3) {\n \t\treturn;\n \t}\n \tauto &left = function.children[0];\n@@ -158,7 +158,17 @@ void TryTransformStarLike(unique_ptr<ParsedExpression> &root) {\n \t\t// COLUMNS(*) has different semantics\n \t\treturn;\n \t}\n-\tunordered_set<string> supported_ops {\"~~\", \"!~~\", \"~~~\", \"!~~~\", \"~~*\", \"!~~*\", \"regexp_full_match\"};\n+\tunordered_set<string> supported_ops {\"~~\",\n+\t                                     \"!~~\",\n+\t                                     \"~~~\",\n+\t                                     \"!~~~\",\n+\t                                     \"~~*\",\n+\t                                     \"!~~*\",\n+\t                                     \"regexp_full_match\",\n+\t                                     \"not_like_escape\",\n+\t                                     \"ilike_escape\",\n+\t                                     \"not_ilike_escape\",\n+\t                                     \"like_escape\"};\n \tif (supported_ops.count(function.function_name) == 0) {\n \t\t// unsupported op for * expression\n \t\tthrow BinderException(*root, \"Function \\\"%s\\\" cannot be applied to a star expression\", function.function_name);\ndiff --git a/src/planner/binder/tableref/bind_table_function.cpp b/src/planner/binder/tableref/bind_table_function.cpp\nindex 29d68f3fc3ab..ace96207bf98 100644\n--- a/src/planner/binder/tableref/bind_table_function.cpp\n+++ b/src/planner/binder/tableref/bind_table_function.cpp\n@@ -203,8 +203,12 @@ unique_ptr<LogicalOperator> Binder::BindTableFunctionInternal(TableFunction &tab\n \t\tif (table_function.bind_replace) {\n \t\t\tauto new_plan = table_function.bind_replace(context, bind_input);\n \t\t\tif (new_plan) {\n-\t\t\t\tnew_plan->alias = ref.alias;\n-\t\t\t\tnew_plan->column_name_alias = ref.column_name_alias;\n+\t\t\t\tif (!ref.alias.empty()) {\n+\t\t\t\t\tnew_plan->alias = ref.alias;\n+\t\t\t\t}\n+\t\t\t\tif (!ref.column_name_alias.empty()) {\n+\t\t\t\t\tnew_plan->column_name_alias = ref.column_name_alias;\n+\t\t\t\t}\n \t\t\t\treturn CreatePlan(*Bind(*new_plan));\n \t\t\t} else if (!table_function.bind) {\n \t\t\t\tthrow BinderException(\"Failed to bind \\\"%s\\\": nullptr returned from bind_replace without bind function\",\ndiff --git a/src/storage/local_storage.cpp b/src/storage/local_storage.cpp\nindex cbbf64df28f8..93dccfcba763 100644\n--- a/src/storage/local_storage.cpp\n+++ b/src/storage/local_storage.cpp\n@@ -474,6 +474,7 @@ idx_t LocalStorage::Delete(DataTable &table, Vector &row_ids, idx_t count) {\n \n void LocalStorage::Update(DataTable &table, Vector &row_ids, const vector<PhysicalIndex> &column_ids,\n                           DataChunk &updates) {\n+\tD_ASSERT(updates.size() >= 1);\n \tauto storage = table_manager.GetStorage(table);\n \tD_ASSERT(storage);\n \ndiff --git a/src/storage/table/row_group_collection.cpp b/src/storage/table/row_group_collection.cpp\nindex 2629800a4be8..dc0a7eb47eb7 100644\n--- a/src/storage/table/row_group_collection.cpp\n+++ b/src/storage/table/row_group_collection.cpp\n@@ -593,6 +593,7 @@ idx_t RowGroupCollection::Delete(TransactionData transaction, DataTable &table,\n //===--------------------------------------------------------------------===//\n void RowGroupCollection::Update(TransactionData transaction, row_t *ids, const vector<PhysicalIndex> &column_ids,\n                                 DataChunk &updates) {\n+\tD_ASSERT(updates.size() >= 1);\n \tidx_t pos = 0;\n \tdo {\n \t\tidx_t start = pos;\ndiff --git a/third_party/fmt/include/fmt/format.h b/third_party/fmt/include/fmt/format.h\nindex e92437a2ed8e..ad4fe83ab438 100644\n--- a/third_party/fmt/include/fmt/format.h\n+++ b/third_party/fmt/include/fmt/format.h\n@@ -548,7 +548,7 @@ class u8string_view : public basic_string_view<fmt_char8_t> {\n \n #if FMT_USE_USER_DEFINED_LITERALS\n inline namespace literals {\n-inline u8string_view operator\"\"_u(const char* s, std::size_t n) {\n+inline u8string_view operator\"\" _u(const char* s, std::size_t n) {\n   return {s, n};\n }\n }  // namespace literals\n@@ -3342,11 +3342,11 @@ FMT_CONSTEXPR internal::udl_formatter<Char, CHARS...> operator\"\"_format() {\n     std::string message = \"The answer is {}\"_format(42);\n   \\endrst\n  */\n-FMT_CONSTEXPR internal::udl_formatter<char> operator\"\"_format(const char* s,\n+FMT_CONSTEXPR internal::udl_formatter<char> operator\"\" _format(const char* s,\n                                                                std::size_t n) {\n   return {{s, n}};\n }\n-FMT_CONSTEXPR internal::udl_formatter<wchar_t> operator\"\"_format(\n+FMT_CONSTEXPR internal::udl_formatter<wchar_t> operator\"\" _format(\n     const wchar_t* s, std::size_t n) {\n   return {{s, n}};\n }\n@@ -3362,11 +3362,11 @@ FMT_CONSTEXPR internal::udl_formatter<wchar_t> operator\"\"_format(\n     fmt::print(\"Elapsed time: {s:.2f} seconds\", \"s\"_a=1.23);\n   \\endrst\n  */\n-FMT_CONSTEXPR internal::udl_arg<char> operator\"\"_a(const char* s,\n+FMT_CONSTEXPR internal::udl_arg<char> operator\"\" _a(const char* s,\n                                                     std::size_t n) {\n   return {{s, n}};\n }\n-FMT_CONSTEXPR internal::udl_arg<wchar_t> operator\"\"_a(const wchar_t* s,\n+FMT_CONSTEXPR internal::udl_arg<wchar_t> operator\"\" _a(const wchar_t* s,\n                                                        std::size_t n) {\n   return {{s, n}};\n }\ndiff --git a/tools/juliapkg/src/result.jl b/tools/juliapkg/src/result.jl\nindex 59bf6953a78a..f3ce9a8ee225 100644\n--- a/tools/juliapkg/src/result.jl\n+++ b/tools/juliapkg/src/result.jl\n@@ -870,7 +870,14 @@ The resultset iterator supports the [Tables.jl](https://github.com/JuliaData/Tab\n like `DataFrame(results)`, `CSV.write(\"results.csv\", results)`, etc.\n \"\"\"\n DBInterface.execute(stmt::Stmt, params::DBInterface.StatementParams) = execute(stmt, params)\n-DBInterface.execute(con::Connection, sql::AbstractString, result_type::Type) = execute(Stmt(con, sql, result_type))\n+function DBInterface.execute(con::Connection, sql::AbstractString, result_type::Type)\n+    stmt = Stmt(con, sql, result_type)\n+    try\n+        return execute(stmt)\n+    finally\n+        _close_stmt(stmt) # immediately close, don't wait for GC\n+    end\n+end\n DBInterface.execute(con::Connection, sql::AbstractString) = DBInterface.execute(con, sql, MaterializedResult)\n DBInterface.execute(db::DB, sql::AbstractString, result_type::Type) =\n     DBInterface.execute(db.main_connection, sql, result_type)\n",
  "test_patch": "diff --git a/test/extension/install_extension.test b/test/extension/install_extension.test\nindex 1c20345e374b..35acda8ef6a9 100644\n--- a/test/extension/install_extension.test\n+++ b/test/extension/install_extension.test\n@@ -37,3 +37,9 @@ statement error\n INSTALL will_never_exist FROM 'core';\n ----\n IO Error: Failed to copy local extension \"will_never_exist\" at PATH \n+\n+# Error message should point to extensions troubleshooting page\n+statement error\n+INSTALL will_never_exist FROM core;\n+----\n+For more info, visit https://duckdb.org/docs/extensions/troubleshooting\ndiff --git a/test/sql/constraints/primarykey/test_pk_updel_multi_column.test b/test/sql/constraints/primarykey/test_pk_updel_multi_column.test\nindex 077fa824f38c..083a10ca9011 100644\n--- a/test/sql/constraints/primarykey/test_pk_updel_multi_column.test\n+++ b/test/sql/constraints/primarykey/test_pk_updel_multi_column.test\n@@ -2,9 +2,6 @@\n # description: PRIMARY KEY and update/delete on multiple columns\n # group: [primarykey]\n \n-# See test/sql/index/art/constraints/test_art_tx_returning.test.\n-require vector_size 2048\n-\n statement ok\n PRAGMA enable_verification;\n \n@@ -25,17 +22,17 @@ SELECT * FROM test ORDER BY ALL;\n 13\tpandas\n \n statement ok\n-UPDATE test SET a = a + 1;\n+UPDATE test SET a = a + 3;\n \n query IT\n SELECT * FROM test ORDER BY ALL;\n ----\n-12\tpandas\n-13\tpandas\n 14\tpandas\n+15\tpandas\n+16\tpandas\n \n statement error\n-UPDATE test SET a = 13 WHERE a = 12;\n+UPDATE test SET a = 15 WHERE a = 14;\n ----\n <REGEX>:Constraint Error.*violates primary key constraint.*\n \n@@ -47,12 +44,12 @@ UPDATE test SET a = 4;\n query IT\n SELECT * FROM test ORDER BY a;\n ----\n-12\tpandas\n-13\tpandas\n 14\tpandas\n+15\tpandas\n+16\tpandas\n \n statement ok\n-UPDATE test SET a = a - 1;\n+UPDATE test SET a = a - 3;\n \n query IT\n SELECT * FROM test ORDER BY ALL;\ndiff --git a/test/sql/index/art/constraints/test_art_tx_returning.test b/test/sql/index/art/constraints/test_art_tx_returning.test\nindex de9e8963f04c..f414b71cbb15 100644\n--- a/test/sql/index/art/constraints/test_art_tx_returning.test\n+++ b/test/sql/index/art/constraints/test_art_tx_returning.test\n@@ -2,13 +2,6 @@\n # description: Test updates on the primary key containing RETURNING.\n # group: [constraints]\n \n-# For each incoming chunk, we add the row IDs to the delete index.\n-# For standard_vector_size = 2, we delete [0, 1], and then try to insert value [1, 2].\n-# This is expected to throw a constraint violation.\n-# The value 2 is not yet in the delete index, as the chunk that would add that value hasn't been processed, yet.\n-# This scenario is a known limitation (also in postgres).\n-require vector_size 2048\n-\n statement ok\n PRAGMA enable_verification\n \n@@ -22,10 +15,24 @@ statement ok\n INSERT INTO tbl_list SELECT range, [range || ' payload'] FROM range(5);\n \n query II\n+UPDATE tbl_list SET id = id + 5 RETURNING id, payload;\n+----\n+5\t[0 payload]\n+6\t[1 payload]\n+7\t[2 payload]\n+8\t[3 payload]\n+9\t[4 payload]\n+\n+statement ok\n+INSERT INTO tbl_list SELECT range + 10, [(range + 10) || ' payload'] FROM range(3000);\n+\n+# For each incoming chunk, we add the row IDs to the delete index.\n+# For standard_vector_size = 2048, we delete the first chunk, and then try to insert the incremented values.\n+# This is expected to throw a constraint violation.\n+# The value going into the next chunk is not yet in the delete index, as the chunk that would add that value hasn't been processed, yet.\n+# This scenario is a known limitation (also in postgres).\n+\n+statement error\n UPDATE tbl_list SET id = id + 1 RETURNING id, payload;\n ----\n-1\t[0 payload]\n-2\t[1 payload]\n-3\t[2 payload]\n-4\t[3 payload]\n-5\t[4 payload]\n+<REGEX>:Constraint Error.*violates primary key constraint.*\ndiff --git a/test/sql/projection/select_star_like.test b/test/sql/projection/select_star_like.test\nindex 40969744dfa1..f31e68f9df80 100644\n--- a/test/sql/projection/select_star_like.test\n+++ b/test/sql/projection/select_star_like.test\n@@ -61,6 +61,27 @@ SELECT * NOT LIKE '%number%' AS val FROM (SELECT 1 AS number1, 2 AS number2, 3 A\n ----\n 3\n \n+# ESCAPE\n+query I\n+SELECT * LIKE '\\_%' ESCAPE '\\' AS val FROM (SELECT 1 AS number1, 2 AS _number2)\n+----\n+2\n+\n+query I\n+SELECT * NOT LIKE '\\_%' ESCAPE '\\' AS val FROM (SELECT 1 AS number1, 2 AS _number2)\n+----\n+1\n+\n+query I\n+SELECT * ILIKE '\\_NUM%' ESCAPE '\\' AS val FROM (SELECT 1 AS number1, 2 AS _number2)\n+----\n+2\n+\n+query I\n+SELECT * NOT ILIKE '\\_NUM%' ESCAPE '\\' AS val FROM (SELECT 1 AS number1, 2 AS _number2)\n+----\n+1\n+\n # non-constant pattern\n statement error\n SELECT * SIMILAR TO pattern FROM integers, (SELECT '.*col.*') t(pattern)\ndiff --git a/test/sql/topn/test_top_n_nested_struct.test b/test/sql/topn/test_top_n_nested_struct.test\nnew file mode 100644\nindex 000000000000..beaad014bf82\n--- /dev/null\n+++ b/test/sql/topn/test_top_n_nested_struct.test\n@@ -0,0 +1,28 @@\n+# name: test/sql/topn/test_top_n_nested_struct.test\n+# description: Test Top-N on nested structs\n+# group: [topn]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE nested_struct(cik BIGINT, entityName VARCHAR, a STRUCT(b STRUCT(c INT, d INT), c STRUCT(e INT, f INT)));\n+\n+statement ok\n+INSERT INTO nested_struct VALUES (42, 'entity', {'b': {'c': 42, 'd': 43}, 'c': {'e': 44, 'f': 45}}),\n+                                 (142, 'entity2', {'b': {'c': 142, 'd': 143}, 'c': {'e': 144, 'f': 145}});\n+\n+query IIII\n+select unnest(a, recursive:=true) from nested_struct limit 1\n+----\n+42\t43\t44\t45\n+\n+query IIII\n+SELECT unnest(a, recursive := true) FROM nested_struct ORDER BY a.b.c LIMIT 1\n+----\n+42\t43\t44\t45\n+\n+query IIII\n+SELECT unnest(a, recursive := true) FROM nested_struct ORDER BY a.b.c DESC LIMIT 1\n+----\n+142\t143\t144\t145\ndiff --git a/test/sql/types/nested/struct/lineitem_struct.test_slow b/test/sql/types/nested/struct/lineitem_struct.test_slow\nindex 14b515071c10..9dd375bc3df8 100644\n--- a/test/sql/types/nested/struct/lineitem_struct.test_slow\n+++ b/test/sql/types/nested/struct/lineitem_struct.test_slow\n@@ -36,3 +36,22 @@ query I\n PRAGMA tpch(1)\n ----\n <FILE>:extension/tpch/dbgen/answers/sf0.01/q01.csv\n+\n+# top-n\n+query IIIIIIIIIIIIIII\n+SELECT l_orderkey, l_partkey, l_suppkey, l_linenumber, l_quantity, l_extendedprice, l_discount, l_tax, l_returnflag, l_linestatus, l_shipdate, l_commitdate, l_receiptdate, l_shipinstruct, l_shipmode FROM lineitem ORDER BY l_shipdate, l_orderkey LIMIT 5;\n+----\n+27137\t1673\t56\t3\t24.00\t37792.08\t0.06\t0.03\tR\tF\t1992-01-04\t1992-02-18\t1992-01-22\tDELIVER IN PERSON\tRAIL\n+27137\t590\t51\t5\t37.00\t55151.83\t0.03\t0.03\tA\tF\t1992-01-06\t1992-02-24\t1992-01-13\tDELIVER IN PERSON\tMAIL\n+47591\t676\t39\t1\t38.00\t59913.46\t0.05\t0.04\tA\tF\t1992-01-06\t1992-03-19\t1992-01-11\tCOLLECT COD\tREG AIR\n+5601\t723\t24\t3\t38.00\t61701.36\t0.07\t0.00\tA\tF\t1992-01-08\t1992-03-01\t1992-01-09\tTAKE BACK RETURN\tREG AIR\n+9379\t556\t57\t4\t13.00\t18935.15\t0.00\t0.01\tA\tF\t1992-01-09\t1992-03-20\t1992-01-12\tCOLLECT COD\tTRUCK\n+\n+query IIIIIIIIIIIIIII\n+SELECT l_orderkey,l_partkey,l_suppkey,l_linenumber,l_quantity,l_extendedprice,l_discount,l_tax,l_returnflag,l_linestatus,l_shipdate,l_commitdate,l_receiptdate,l_shipinstruct,l_shipmode FROM lineitem ORDER BY l_orderkey DESC, l_shipdate DESC LIMIT 5;\n+----\n+60000\t1843\t44\t2\t23.00\t40131.32\t0.05\t0.03\tN\tO\t1995-08-09\t1995-06-08\t1995-08-23\tCOLLECT COD\tFOB\n+60000\t585\t16\t5\t31.00\t46052.98\t0.00\t0.05\tN\tO\t1995-08-06\t1995-07-18\t1995-08-19\tTAKE BACK RETURN\tTRUCK\n+60000\t271\t53\t4\t29.00\t33966.83\t0.02\t0.01\tN\tO\t1995-07-25\t1995-06-07\t1995-08-17\tCOLLECT COD\tSHIP\n+60000\t836\t3\t6\t45.00\t78157.35\t0.04\t0.08\tN\tO\t1995-07-23\t1995-07-17\t1995-07-24\tDELIVER IN PERSON\tTRUCK\n+60000\t292\t93\t1\t45.00\t53653.05\t0.05\t0.06\tN\tO\t1995-07-13\t1995-05-29\t1995-08-10\tTAKE BACK RETURN\tMAIL\ndiff --git a/test/sql/upsert/test_big_insert.test b/test/sql/upsert/test_big_insert.test\nindex 1bba1ba5fd17..eaf476447f57 100644\n--- a/test/sql/upsert/test_big_insert.test\n+++ b/test/sql/upsert/test_big_insert.test\n@@ -3,53 +3,45 @@\n # group: [upsert]\n \n statement ok\n-pragma enable_verification;\n+PRAGMA enable_verification;\n \n statement ok\n-SET preserve_insertion_order=false;\n+SET preserve_insertion_order = false;\n \n-# big insert\n statement ok\n CREATE TABLE integers(\n-\ti INTEGER unique,\n-\tj INTEGER DEFAULT 0,\n-\tk INTEGER DEFAULT 0\n+\ti INT UNIQUE,\n+\tj INT DEFAULT 0,\n+\tk INT DEFAULT 0\n );\n \n statement ok\n-INSERT INTO integers(i) SELECT i from range(5000) tbl(i);\n+INSERT INTO integers(i) SELECT i FROM range(5000) tbl(i);\n \n query I\n SELECT COUNT(*) FROM integers\n ----\n 5000\n \n-# All tuples hit a conflict - Do nothing\n+# All tuples hit a conflict - DO NOTHING.\n statement ok\n-INSERT INTO integers SELECT * FROM integers on conflict do nothing;\n+INSERT INTO integers SELECT * FROM integers ON CONFLICT DO NOTHING;\n \n-# All tuples hit a conflict - Do Update\n+# All tuples hit a conflict - DO UPDATE.\n statement ok\n-INSERT INTO integers SELECT * FROM integers on conflict do update set j = 10;\n+INSERT INTO integers SELECT * FROM integers ON CONFLICT DO UPDATE SET j = 10;\n \n-# All 'j' entries are changed to 10\n+# All 'j' entries are changed to 10.\n query I\n SELECT COUNT(*) FILTER (WHERE j = 10) FROM integers\n ----\n 5000\n \n-# All insert tuples cause a conflict on the same row\n-statement error\n-INSERT INTO integers(i,j) select i%5,i from range(5000) tbl(i) on conflict do update set j = excluded.j, k = excluded.i;\n-----\n-Invalid Input Error: ON CONFLICT DO UPDATE can not update the same row twice in the same command. Ensure that no rows proposed for insertion within the same command have duplicate constrained values\n-\n statement ok\n-INSERT INTO integers(i,j) select i%5,i from range(4995, 5000) tbl(i) on conflict do update set j = excluded.j, k = excluded.i;\n+INSERT INTO integers(i, j) SELECT i % 5, i FROM range(4995, 5000) tbl(i) ON CONFLICT DO UPDATE SET j = excluded.j, k = excluded.i;\n \n-# This is what we might expect the previous result to look like as well\n query I\n-select j from integers limit 5;\n+SELECT j FROM integers LIMIT 5;\n ----\n 4995\n 4996\n@@ -57,25 +49,21 @@ select j from integers limit 5;\n 4998\n 4999\n \n-# This is the worst conflicting rowid pattern we could have\n-# Every odd-indexed insert tuple conflicts with a row at the start of the existing tuples\n-# And every even-indexed insert tuple conflicts with a row at the end of the existing tuples\n+# This is the worst conflicting rowid pattern we could have.\n+# Every odd-indexed insert tuple conflicts with a row at the start of the existing tuples.\n+# And every even-indexed insert tuple conflicts with a row at the end of the existing tuples.\n statement ok\n-insert into integers(i,j)\n-\tselect\n-\t\tCASE WHEN i % 2 = 0\n-\t\t\tTHEN\n-\t\t\t\t4999 - (i//2)\n-\t\t\tELSE\n-\t\t\t\ti - ((i//2)+1)\n-\t\tEND,\n-\t\ti\n-\tfrom range(5000) tbl(i)\n-on conflict do update set j = excluded.j;\n-\n-# This shows that the odd-indexed insert tuples conflicted with the first rows\n+INSERT INTO integers(i, j)\n+\tSELECT CASE WHEN i % 2 = 0\n+\t\tTHEN 4999 - (i // 2)\n+\t\tELSE i - ((i // 2) + 1)\n+\t\tEND, i\n+\tFROM range(5000) tbl(i)\n+ON CONFLICT DO UPDATE SET j = excluded.j;\n+\n+# This shows that the odd-indexed insert tuples conflicted with the first rows.\n query I\n-select j from integers limit 5;\n+SELECT j FROM integers LIMIT 5;\n ----\n 1\n 3\n@@ -83,9 +71,9 @@ select j from integers limit 5;\n 7\n 9\n \n-# This shows that the even-indexed insert tuples conflicted with the last rows\n+# This shows that the even-indexed insert tuples conflicted with the last rows.\n query I\n-select j from integers limit 5 offset 4995;\n+SELECT j FROM integers LIMIT 5 OFFSET 4995;\n ----\n 8\n 6\n@@ -93,26 +81,23 @@ select j from integers limit 5 offset 4995;\n 2\n 0\n \n-# Reset j\n+# Reset j.\n statement ok\n-update integers set j = 0;\n+UPDATE integers SET j = 0;\n \n-# Only set j if both the existing tuple and the insert tuple are even\n+# Only set j if both the existing tuple and the insert tuple are even.\n statement ok\n-insert into integers(i,j)\n-\tselect\n-\t\tCASE WHEN i % 2 = 0\n-\t\t\tTHEN\n-\t\t\t\t4999 - (i//2)\n-\t\t\tELSE\n-\t\t\t\ti - ((i//2)+1)\n-\t\tEND,\n-\t\ti\n-\tfrom range(5000) tbl(i)\n-on conflict do update set j = excluded.j where i % 2 = 0 AND excluded.j % 2 = 0;\n-\n-# The DO UPDATE where clause is only true for a quarter of the cases\n+INSERT INTO integers(i, j)\n+\tSELECT CASE WHEN i % 2 = 0\n+\t\tTHEN 4999 - (i // 2)\n+\t\tELSE i - ((i // 2) + 1)\n+\t\tEND, i\n+\tFROM range(5000) tbl(i)\n+ON CONFLICT DO UPDATE SET j = excluded.j\n+WHERE i % 2 = 0 AND excluded.j % 2 = 0;\n+\n+# The DO UPDATE WHERE clause is only true for a quarter of the cases.\n query I\n-select COUNT(j) filter (where j != 0) from integers;\n+SELECT COUNT(j) FILTER (WHERE j != 0) FROM integers;\n ----\n 1250\ndiff --git a/test/sql/upsert/test_big_insert_no_vector_verification.test b/test/sql/upsert/test_big_insert_no_vector_verification.test\nnew file mode 100644\nindex 000000000000..3f4b849d9774\n--- /dev/null\n+++ b/test/sql/upsert/test_big_insert_no_vector_verification.test\n@@ -0,0 +1,32 @@\n+# name: test/sql/upsert/test_big_insert_no_vector_verification.test\n+# description: Test ON CONFLICT statement on the same conflicting row.\n+# group: [upsert]\n+\n+# The constant operator verification ensures that we have only one row per data chunk.\n+# Thus, the below insert succeeds, as we no longer see the same row within a chunk.\n+require no_vector_verification\n+\n+statement ok\n+PRAGMA enable_verification;\n+\n+statement ok\n+SET preserve_insertion_order = false;\n+\n+statement ok\n+CREATE TABLE integers(\n+\ti INT UNIQUE,\n+\tj INT DEFAULT 0,\n+\tk INT DEFAULT 0\n+);\n+\n+statement ok\n+INSERT INTO integers(i) SELECT i FROM range(5000) tbl(i);\n+\n+statement error\n+INSERT INTO integers(i, j)\n+SELECT i % 5, i\n+FROM range(5000) tbl(i) ON CONFLICT DO UPDATE SET\n+\tj = excluded.j,\n+\tk = excluded.i;\n+----\n+<REGEX>:Invalid Input Error:.*ON CONFLICT DO UPDATE can not update the same row twice in the same command.*\ndiff --git a/test/sql/upsert/test_problematic_conditional_do_update.test b/test/sql/upsert/test_problematic_conditional_do_update.test\nnew file mode 100644\nindex 000000000000..ade3cd1bde07\n--- /dev/null\n+++ b/test/sql/upsert/test_problematic_conditional_do_update.test\n@@ -0,0 +1,40 @@\n+# name: test/sql/upsert/test_problematic_conditional_do_update.test\n+# group: [upsert]\n+\n+statement ok\n+CREATE TABLE users (\n+\tid BIGINT PRIMARY KEY,\n+\tusername TEXT UNIQUE,\n+\temail TEXT\n+);\n+\n+# FIXME: not consistent\n+mode skip\n+\n+# The condition skips the last tuple\n+statement error\n+INSERT INTO users (id, username, email)\n+VALUES\n+\t(3, 'inner_conflict', 'test'),\n+\t(3, 'inner_conflict2', 'other_test'),\n+\t(3, 'inner_conflict3', 'filtered_out')\n+ON CONFLICT (id) DO\n+    UPDATE SET email = EXCLUDED.email\n+    WHERE EXCLUDED.email != 'filtered_out'\n+----\n+Not implemented Error: Inner conflicts detected with a conditional DO UPDATE on-conflict action, not fully implemented yet\n+\n+# The result of the condition can also be influenced based on previous updates\n+statement error\n+INSERT INTO users (id, username, email)\n+VALUES\n+\t(3, 'inner_conflict', 'test'),\n+\t(3, 'inner_conflict2', 'other_test'),\n+\t(3, 'inner_conflict3', 'yet_another_test'),\n+\t(3, 'inner_conflict4', 'dont_skip_me')\n+ON CONFLICT (id) DO\n+    UPDATE SET email = EXCLUDED.email\n+    WHERE email != 'other_test' OR EXCLUDED.email == 'dont_skip_me'\n+RETURNING *;\n+----\n+Not implemented Error: Inner conflicts detected with a conditional DO UPDATE on-conflict action, not fully implemented yet\ndiff --git a/test/sql/upsert/upsert_returning.test b/test/sql/upsert/upsert_returning.test\nnew file mode 100644\nindex 000000000000..cbb3c90126d9\n--- /dev/null\n+++ b/test/sql/upsert/upsert_returning.test\n@@ -0,0 +1,83 @@\n+# name: test/sql/upsert/upsert_returning.test\n+# group: [upsert]\n+\n+require vector_size 2048\n+\n+statement ok\n+CREATE TABLE users (\n+    id BIGINT PRIMARY KEY,\n+    username TEXT UNIQUE,\n+    email TEXT\n+);\n+\n+query III\n+INSERT INTO users (id, username, email) VALUES\n+\t(1, 'john_doe', 'john@example.com')\n+ON CONFLICT (username) DO NOTHING\n+RETURNING *;\n+----\n+1\tjohn_doe\tjohn@example.com\n+\n+query III\n+INSERT INTO users (id, username, email) VALUES\n+\t(1, 'john_doe', 'john@example.com')\n+ON CONFLICT (username) DO NOTHING\n+RETURNING *;\n+----\n+\n+# We create a conflict, with a where clause that filters out this conflict\n+# Because the where clause filters it out, the DO UPDATE becomes a DO NOTHING for this row instead\n+# So it does not get added to the returning clause.\n+query III\n+INSERT INTO users (id, username, email)\n+VALUES (1, 'john_doe', 'john_new@example.com')\n+ON CONFLICT (id) DO\n+    UPDATE SET email = EXCLUDED.email\n+    WHERE EXCLUDED.email != 'john_new@example.com'\n+RETURNING *;\n+----\n+\n+# Verify that the *other* tuple does get added to the returning clause\n+query III\n+INSERT INTO users (id, username, email)\n+VALUES\n+\t(1, 'john_doe', 'john_new@example.com'),\n+\t(2, 'not_john_doe', 'not_john_new@example.com')\n+ON CONFLICT (id) DO\n+    UPDATE SET email = EXCLUDED.email\n+    WHERE EXCLUDED.email != 'john_new@example.com'\n+RETURNING *;\n+----\n+2\tnot_john_doe\tnot_john_new@example.com\n+\n+\n+# FIXME: not consistent\n+mode skip\n+\n+# Here we have conflicts within the inserted data\n+# Only the last occurrence of this conflict should be present in the returning clause.\n+query III\n+INSERT INTO users (id, username, email)\n+VALUES\n+\t(3, 'inner_conflict', 'test'),\n+\t(4, 'a', ''),\n+\t(5, 'b', ''),\n+\t(6, 'c', ''),\n+\t(3, 'inner_conflict2', 'other_test'),\n+\t(7, 'd', ''),\n+\t(8, 'e', ''),\n+\t(9, 'f', ''),\n+\t(3, 'inner_conflict3', 'yet_another_test')\n+ON CONFLICT (id) DO\n+    UPDATE SET email = EXCLUDED.email\n+    WHERE EXCLUDED.email != 'test'\n+RETURNING *;\n+----\n+3\tinner_conflict\ttest\n+4\ta\t(empty)\n+5\tb\t(empty)\n+6\tc\t(empty)\n+7\td\t(empty)\n+8\te\t(empty)\n+9\tf\t(empty)\n+3\tinner_conflict3\tyet_another_test\ndiff --git a/tools/juliapkg/test/test_connection.jl b/tools/juliapkg/test/test_connection.jl\nindex c09848b27450..0258788a6dfe 100644\n--- a/tools/juliapkg/test/test_connection.jl\n+++ b/tools/juliapkg/test/test_connection.jl\n@@ -17,3 +17,39 @@ end\n @testset \"Test opening a bogus directory\" begin\n     @test_throws DuckDB.ConnectionException DBInterface.connect(DuckDB.DB, \"/path/to/bogus/directory\")\n end\n+\n+\n+@testset \"Test opening and closing an on-disk database\" begin\n+    # This checks for an issue where the DB and the connection are \n+    # closed but the actual db is not (and subsequently cannot be opened\n+    # in a different process). To check this, we create a DB, write some\n+    # data to it, close the connection and check if the WAL file exists.\n+    #\n+    # Ideally, the WAL file should not exist, but Garbage Collection of Julia\n+    # may not have run yet, so open database handles may still exist, preventing\n+    # the database from being closed properly.\n+\n+    db_path = joinpath(mktempdir(), \"duckdata.db\")\n+    db_path_wal = db_path * \".wal\"\n+\n+    function write_data(dbfile::String)\n+        db = DuckDB.DB(dbfile)\n+        conn = DBInterface.connect(db)\n+        DBInterface.execute(conn, \"CREATE OR REPLACE TABLE test (a INTEGER, b INTEGER);\")\n+        DBInterface.execute(conn, \"INSERT INTO test VALUES (1, 2);\")\n+        DBInterface.close!(conn)\n+        DuckDB.close_database(db)\n+        return true\n+    end\n+    write_data(db_path) # call the function\n+    @test isfile(db_path_wal) === false # WAL file should not exist\n+\n+    @test isfile(db_path) # check if the database file exists\n+\n+    # check if the database can be opened\n+    if haskey(ENV, \"JULIA_DUCKDB_LIBRARY\")\n+        duckdb_binary = joinpath(dirname(ENV[\"JULIA_DUCKDB_LIBRARY\"]), \"..\", \"duckdb\")\n+        result = run(`$duckdb_binary $db_path -c \"SELECT * FROM test LIMIT 1\"`) # check if the database can be opened\n+        @test success(result)\n+    end\n+end\ndiff --git a/tools/pythonpkg/scripts/sqllogictest_python.py b/tools/pythonpkg/scripts/sqllogictest_python.py\nindex c3821f581c75..554e3a5a489e 100644\n--- a/tools/pythonpkg/scripts/sqllogictest_python.py\n+++ b/tools/pythonpkg/scripts/sqllogictest_python.py\n@@ -38,6 +38,9 @@ def __init__(self, build_directory: Optional[str] = None):\n                 'test/sql/json/table/read_json_objects.test',  # <-- Python client is always loaded with JSON available\n                 'test/sql/copy/csv/zstd_crash.test',  # <-- Python client is always loaded with Parquet available\n                 'test/sql/error/extension_function_error.test',  # <-- Python client is always loaded with TPCH available\n+                'test/optimizer/joins/tpcds_nofail.test',  # <-- Python client is always loaded with TPCDS available\n+                'test/sql/settings/errors_as_json.test',  # <-- errors_as_json not currently supported in Python\n+                'test/sql/parallelism/intraquery/depth_first_evaluation_union_and_join.test',  # <-- Python client is always loaded with TPCDS available\n                 'test/sql/types/timestamp/test_timestamp_tz.test',  # <-- Python client is always loaded wih ICU available - making the TIMESTAMPTZ::DATE cast pass\n                 'test/sql/parser/invisible_spaces.test',  # <-- Parser is getting tripped up on the invisible spaces\n                 'test/sql/copy/csv/code_cov/csv_state_machine_invalid_utf.test',  # <-- ConversionException is empty, see Python Mega Issue (duckdb-internal #1488)\n",
  "problem_statement": "Aliases being set inside bind_replace are overwritten\n### What happens?\n\nWhen setting the alias of a TableRef inside a bind_replace function, it is overwritten by the `ref.alias` even if this `ref.alias` is empty. \nSpecifically here: \nhttps://github.com/duckdb/duckdb/blob/6264c5c95b9a77d333d04fa9a268e04c52015766/src/planner/binder/tableref/bind_table_function.cpp#L204-L209\nI can set the `ref.alias` earlier like this: https://github.com/cwida/duckpgq-extension/blob/8de232b5ba4df00afdaf7e1f3c0870dfee21c54f/src/core/parser/duckpgq_parser.cpp#L50, but that means I need to check for every possible bind_replace function I have during the initial bind phase, which feels quite hacky. I think the ability to set the alias during the bind_replace and it not being overwritten is a better solution.\n\nPerhaps I am missing something regarding the `ref.alias`, if that's the case please let me know :) \n\n### To Reproduce\n\nTo reproduce: \n\n`git clone --recurse-submodules git@github.com:cwida/duckpgq-extension.git`\n`make GEN=ninja`\n\n```sql\nCREATE TABLE Student(id BIGINT, name VARCHAR);INSERT INTO Student VALUES (0, 'Daniel'), (1, 'Tavneet'), (2, 'Gabor'), (3, 'Peter'), (4, 'David');\nCREATE TABLE know(src BIGINT, dst BIGINT, createDate BIGINT);INSERT INTO know VALUES (0,1, 10), (0,2, 11), (0,3, 12), (3,0, 13), (1,2, 14), (1,3, 15), (2,3, 16), (4,3, 17);\n-CREATE PROPERTY GRAPH pg\nVERTEX TABLES (\n    Student,\n    Foo\n    )\nEDGE TABLES (\n    know    SOURCE KEY ( src ) REFERENCES Student ( id )\n            DESTINATION KEY ( dst ) REFERENCES Student ( id )\n    );\n\nselect a.id, a.name, local_clustering_coefficient from local_clustering_coefficient(pg, student, know), student a where a.id = lcc.id;\n```\n```\nBinder Error: Referenced table \"lcc\" not found!\nCandidate tables: \"a\"\n```\n\n### OS:\n\nmacOS\n\n### DuckDB Version:\n\nv1.2-histrionicus\n\n### DuckDB Client:\n\nCLI\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nDaniel ten Wolde\n\n### Affiliation:\n\nCentrum Wiskunde & Informatica\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a source build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [x] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [x] Yes, I have\nAliases being set inside bind_replace are overwritten\n### What happens?\n\nWhen setting the alias of a TableRef inside a bind_replace function, it is overwritten by the `ref.alias` even if this `ref.alias` is empty. \nSpecifically here: \nhttps://github.com/duckdb/duckdb/blob/6264c5c95b9a77d333d04fa9a268e04c52015766/src/planner/binder/tableref/bind_table_function.cpp#L204-L209\nI can set the `ref.alias` earlier like this: https://github.com/cwida/duckpgq-extension/blob/8de232b5ba4df00afdaf7e1f3c0870dfee21c54f/src/core/parser/duckpgq_parser.cpp#L50, but that means I need to check for every possible bind_replace function I have during the initial bind phase, which feels quite hacky. I think the ability to set the alias during the bind_replace and it not being overwritten is a better solution.\n\nPerhaps I am missing something regarding the `ref.alias`, if that's the case please let me know :) \n\n### To Reproduce\n\nTo reproduce: \n\n`git clone --recurse-submodules git@github.com:cwida/duckpgq-extension.git`\n`make GEN=ninja`\n\n```sql\nCREATE TABLE Student(id BIGINT, name VARCHAR);INSERT INTO Student VALUES (0, 'Daniel'), (1, 'Tavneet'), (2, 'Gabor'), (3, 'Peter'), (4, 'David');\nCREATE TABLE know(src BIGINT, dst BIGINT, createDate BIGINT);INSERT INTO know VALUES (0,1, 10), (0,2, 11), (0,3, 12), (3,0, 13), (1,2, 14), (1,3, 15), (2,3, 16), (4,3, 17);\n-CREATE PROPERTY GRAPH pg\nVERTEX TABLES (\n    Student,\n    Foo\n    )\nEDGE TABLES (\n    know    SOURCE KEY ( src ) REFERENCES Student ( id )\n            DESTINATION KEY ( dst ) REFERENCES Student ( id )\n    );\n\nselect a.id, a.name, local_clustering_coefficient from local_clustering_coefficient(pg, student, know), student a where a.id = lcc.id;\n```\n```\nBinder Error: Referenced table \"lcc\" not found!\nCandidate tables: \"a\"\n```\n\n### OS:\n\nmacOS\n\n### DuckDB Version:\n\nv1.2-histrionicus\n\n### DuckDB Client:\n\nCLI\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nDaniel ten Wolde\n\n### Affiliation:\n\nCentrum Wiskunde & Informatica\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a source build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [x] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [x] Yes, I have\nSurprising upsert with returning and where condition result\n### What happens?\n\nGiven I have a table with a primary key and I perform an upsert with a where condition in the update I would expect rows that don't match the condition not be present in the returned rows, this is not the case.\n\n### To Reproduce\n\n```sql\ncreate table foo(id int primary key, bar text);\ninsert into foo select 1, 'zoo';\ninsert into foo select 1, 'zoo' returning *;\n```\n```\nConstraint Error: Duplicate key \"id: 1\" violates primary key constraint. If this is an unexpected constraint violation please double check with the known index limitations section in our documentation (https://duckdb.org/docs/sql/indexes).\n```\n```sql\ninsert into foo select 1, 'zoo'\non conflict (id) do\n    update set bar = excluded.bar\n    where excluded.bar != 'zoo'\nreturning *;\n```\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  id   \u2502   bar   \u2502\n\u2502 int32 \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     1 \u2502 zoo     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n```sql\ninsert into foo select 1, 'zoom'\non conflict (id) do\n    update set bar = excluded.bar\n    where excluded.bar != 'zoom'\nreturning *;\n```\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  id   \u2502   bar   \u2502\n\u2502 int32 \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     1 \u2502 zoom    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n```sql\ninsert into foo select 1, 'zoom'\non conflict (id) do\n    update set bar = excluded.bar\n    where id != 1\nreturning *;\n```\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  id   \u2502   bar   \u2502\n\u2502 int32 \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     1 \u2502 zoom    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n```sql\nselect * from foo;\n```\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  id   \u2502   bar   \u2502\n\u2502 int32 \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     1 \u2502 zoo     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nSee in the upserts the where condition prevents the update and I would expect it to prevent anything from being returned, the stored data was never changed.\n\nThe documentation only states\n> The RETURNING clause may be used to return the contents of the rows that were inserted\n\nI think it would be fair in an upsert scenario to assume it would \"return the contents of the rows that were upserted\"\n\n### OS:\n\nMacOs 13.6.7 (aarch64)\n\n### DuckDB Version:\n\nv1.1.3\n\n### DuckDB Client:\n\nCLI (mac via homebrew)\n\n### Hardware:\n\nM1, 16Gb ram\n\n### Full Name:\n\nStephen Flavin\n\n### Affiliation:\n\nN/A\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [x] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [x] Yes, I have\n",
  "hints_text": "\n\n",
  "created_at": "2025-02-04T23:02:22Z"
}