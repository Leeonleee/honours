diff --git a/scripts/sqllogictest/__init__.py b/scripts/sqllogictest/__init__.py
index 02ae67a6f99c..6053ce836ec1 100644
--- a/scripts/sqllogictest/__init__.py
+++ b/scripts/sqllogictest/__init__.py
@@ -20,6 +20,7 @@
     Sleep,
     SleepUnit,
     Skip,
+    Unzip,
     Unskip,
 )
 from .decorator import SkipIf, OnlyIf
@@ -50,6 +51,7 @@
     Sleep,
     SleepUnit,
     Skip,
+    Unzip,
     Unskip,
     SkipIf,
     OnlyIf,
diff --git a/scripts/sqllogictest/parser/parser.py b/scripts/sqllogictest/parser/parser.py
index 349d63cb1e34..37049dc6a1fe 100644
--- a/scripts/sqllogictest/parser/parser.py
+++ b/scripts/sqllogictest/parser/parser.py
@@ -23,6 +23,7 @@
     Reconnect,
     Sleep,
     Skip,
+    Unzip,
     Unskip,
     SortStyle,
 )
@@ -86,6 +87,7 @@ def __init__(self):
             TokenType.SQLLOGIC_RESTART: self.statement_restart,
             TokenType.SQLLOGIC_RECONNECT: self.statement_reconnect,
             TokenType.SQLLOGIC_SLEEP: self.statement_sleep,
+            TokenType.SQLLOGIC_UNZIP: self.statement_unzip,
             TokenType.SQLLOGIC_INVALID: None,
         }
         self.DECORATORS = {
@@ -405,6 +407,29 @@ def statement_sleep(self, header: Token) -> Optional[BaseStatement]:
             raise self.fail(f"Unrecognized sleep mode - expected {create_formatted_list(options)}")
         return Sleep(header, self.current_line + 1, sleep_duration, sleep_unit)
 
+    def statement_unzip(self, header: Token) -> Optional[BaseStatement]:
+        params = header.parameters
+        if len(params) != 1 and len(params) != 2:
+            docs = """
+                unzip requires 1 parameter, the path to a (g)zipped file.
+                Optionally a destination location can be provided, defaulting to '__TEST_DIR__/<base_name>'
+            """
+            self.fail(docs)
+
+        source = params[0]
+
+        accepted_filetypes = {'.gz'}
+
+        basename = os.path.basename(source)
+        stem, extension = os.path.splitext(basename)
+        if extension not in accepted_filetypes:
+            accepted_options = ", ".join(list(accepted_filetypes))
+            self.fail(
+                f"unzip: input does not end in a valid file extension ({extension}), accepted options are: {accepted_options}"
+            )
+        destination = params[1] if len(params) == 2 else f'__TEST_DIR__/{stem}'
+        return Unzip(header, self.current_line + 1, source, destination)
+
     # Decorators
 
     def decorator_skipif(self, token: Token) -> Optional[BaseDecorator]:
@@ -531,6 +556,7 @@ def is_single_line_statement(self, token):
             TokenType.SQLLOGIC_RESTART,
             TokenType.SQLLOGIC_RECONNECT,
             TokenType.SQLLOGIC_SLEEP,
+            TokenType.SQLLOGIC_UNZIP,
         ]
 
         if token.type in single_line_statements:
@@ -566,6 +592,7 @@ def command_to_token(self, token):
             "load": TokenType.SQLLOGIC_LOAD,
             "restart": TokenType.SQLLOGIC_RESTART,
             "reconnect": TokenType.SQLLOGIC_RECONNECT,
+            "unzip": TokenType.SQLLOGIC_UNZIP,
             "sleep": TokenType.SQLLOGIC_SLEEP,
         }
 
diff --git a/scripts/sqllogictest/result.py b/scripts/sqllogictest/result.py
index aa4e8158bd47..1893389b1d1a 100644
--- a/scripts/sqllogictest/result.py
+++ b/scripts/sqllogictest/result.py
@@ -21,6 +21,7 @@
     Sleep,
     SleepUnit,
     Skip,
+    Unzip,
     SortStyle,
     Unskip,
 )
@@ -764,6 +765,7 @@ def __init__(
             Restart: self.execute_restart,
             HashThreshold: self.execute_hash_threshold,
             Set: self.execute_set,
+            Unzip: self.execute_unzip,
             Loop: self.execute_loop,
             Foreach: self.execute_foreach,
             Endloop: None,  # <-- should never be encountered outside of Loop/Foreach
@@ -900,6 +902,18 @@ def is_query_result(sql_query, statement) -> bool:
     def execute_skip(self, statement: Skip):
         self.runner.skip()
 
+    def execute_unzip(self, statement: Unzip):
+        import gzip
+        import shutil
+
+        source = self.replace_keywords(statement.source)
+        destination = self.replace_keywords(statement.destination)
+
+        with gzip.open(source, 'rb') as f_in:
+            with open(destination, 'wb') as f_out:
+                shutil.copyfileobj(f_in, f_out)
+        print(f"Extracted to '{destination}'")
+
     def execute_unskip(self, statement: Unskip):
         self.runner.unskip()
 
diff --git a/scripts/sqllogictest/statement/__init__.py b/scripts/sqllogictest/statement/__init__.py
index daeeee0a1d52..2600fe110963 100644
--- a/scripts/sqllogictest/statement/__init__.py
+++ b/scripts/sqllogictest/statement/__init__.py
@@ -14,6 +14,7 @@
 from .restart import Restart
 from .reconnect import Reconnect
 from .sleep import Sleep, SleepUnit
+from .unzip import Unzip
 
 from .skip import Skip, Unskip
 
@@ -35,6 +36,7 @@
     Sleep,
     SleepUnit,
     Skip,
+    Unzip,
     Unskip,
     SortStyle,
 ]
diff --git a/scripts/sqllogictest/statement/unzip.py b/scripts/sqllogictest/statement/unzip.py
new file mode 100644
index 000000000000..030e1b7b4e70
--- /dev/null
+++ b/scripts/sqllogictest/statement/unzip.py
@@ -0,0 +1,9 @@
+from sqllogictest.base_statement import BaseStatement
+from sqllogictest.token import Token
+
+
+class Unzip(BaseStatement):
+    def __init__(self, header: Token, line: int, source: str, destination: str):
+        super().__init__(header, line)
+        self.source = source
+        self.destination = destination
diff --git a/scripts/sqllogictest/token.py b/scripts/sqllogictest/token.py
index 20928735d950..88d42d156d38 100644
--- a/scripts/sqllogictest/token.py
+++ b/scripts/sqllogictest/token.py
@@ -22,6 +22,7 @@ class TokenType(Enum):
     SQLLOGIC_RESTART = auto()
     SQLLOGIC_RECONNECT = auto()
     SQLLOGIC_SLEEP = auto()
+    SQLLOGIC_UNZIP = auto()
 
 
 class Token:
diff --git a/test/fuzzer/duckfuzz/late_materialization_filter.test b/test/fuzzer/duckfuzz/late_materialization_filter.test
new file mode 100644
index 000000000000..30bdea187cca
--- /dev/null
+++ b/test/fuzzer/duckfuzz/late_materialization_filter.test
@@ -0,0 +1,17 @@
+# name: test/fuzzer/duckfuzz/late_materialization_filter.test
+# description: NULL input to json_extract_string - found by fuzzer
+# group: [duckfuzz]
+
+statement ok
+create table tbl(z int, i bool, j bool, k uhugeint);
+
+statement ok
+insert into tbl (i, j, k) values (true, 'true', 3), (NULL, NULL, NULL), (false, 'false', 1);
+
+query I
+select k
+from tbl
+where i and j
+limit 30;
+----
+3
diff --git a/test/fuzzer/public/insert_returning.test b/test/fuzzer/public/insert_returning.test
new file mode 100644
index 000000000000..4f4cfb9acea5
--- /dev/null
+++ b/test/fuzzer/public/insert_returning.test
@@ -0,0 +1,17 @@
+# name: test/fuzzer/public/insert_returning.test
+# description: Test INSERT OR REPLACE with DEFAULT VALUES
+# group: [public]
+
+statement ok
+pragma enable_verification
+
+statement ok
+CREATE TABLE v00 (c01 INT, c02 STRING);
+
+statement ok
+INSERT INTO v00 (c01, c02) VALUES (0, 'abc');
+
+query I
+INSERT INTO v00 FROM v00 ORDER BY ALL DESC RETURNING 'abc';
+----
+abc
diff --git a/test/fuzzer/public/lateral_in_right_side_of_join.test b/test/fuzzer/public/lateral_in_right_side_of_join.test
new file mode 100644
index 000000000000..aab7706bef98
--- /dev/null
+++ b/test/fuzzer/public/lateral_in_right_side_of_join.test
@@ -0,0 +1,28 @@
+# name: test/fuzzer/public/lateral_in_right_side_of_join.test
+# description: Lateral correlation in right side of join
+# group: [public]
+
+statement ok
+pragma enable_verification
+
+statement ok
+CREATE TABLE t0(c0 INT , c1 VARCHAR);
+
+statement ok
+CREATE TABLE t1( c1 INT);
+
+statement ok
+INSERT INTO t0 VALUES(4, 3);
+
+statement ok
+INSERT INTO t1 VALUES(2);
+
+query IIII
+SELECT * FROM t1, t0 JOIN LATERAL (SELECT t1.c1 AS col0) _subq ON true;
+----
+2	4	3	2
+
+query IIII
+SELECT * FROM t1, t0 INNER JOIN (SELECT t1.c1 AS col0) ON true;
+----
+2	4	3	2
diff --git a/test/fuzzer/public/unsatisfiable_filter_prune.test b/test/fuzzer/public/unsatisfiable_filter_prune.test
new file mode 100644
index 000000000000..b1f3e1a67f3e
--- /dev/null
+++ b/test/fuzzer/public/unsatisfiable_filter_prune.test
@@ -0,0 +1,20 @@
+# name: test/fuzzer/public/unsatisfiable_filter_prune.test
+# description: Test SEMI JOIN with USING clause
+# group: [public]
+
+statement ok
+pragma enable_verification
+
+statement ok
+CREATE TABLE  t0(c0 INT, c1 INT);
+
+statement ok
+INSERT INTO t0( c0, c1) VALUES ( -1, -1);
+
+query I
+SELECT c0 FROM t0 WHERE (((CASE t0.c0 WHEN t0.c0 THEN t0.c0 END ) BETWEEN 1 AND t0.c0)AND(t0.c0 <= 0)) ;
+----
+
+query II
+SELECT * FROM t0 WHERE c0 >= 1 AND c0 <= t0.c1 AND t0.c1 <= 0;
+----
diff --git a/test/sql/catalog/sequence/test_sequence_google_fuzz.test b/test/sql/catalog/sequence/test_sequence_google_fuzz.test
new file mode 100644
index 000000000000..a439abbf30e3
--- /dev/null
+++ b/test/sql/catalog/sequence/test_sequence_google_fuzz.test
@@ -0,0 +1,16 @@
+# name: test/sql/catalog/sequence/test_sequence_google_fuzz.test
+# description: Test Sequence on results produce by the google fuzzer
+# group: [sequence]
+
+statement error
+;creAte
+sequence
+uGeõó±õõõõ.õõõ.õ;creAte
+sequence
+uGeõõõõõ..õ;creAte
+sequence
+uGeõõõõõ..õ;creAte
+sequence
+uGeõõõõ
+----
+Catalog with name uGeõó±õõõõ does not exist!
\ No newline at end of file
diff --git a/test/sql/constraints/primarykey/test_pk_updel_multi_column.test b/test/sql/constraints/primarykey/test_pk_updel_multi_column.test
index 5357eaa69d46..077fa824f38c 100644
--- a/test/sql/constraints/primarykey/test_pk_updel_multi_column.test
+++ b/test/sql/constraints/primarykey/test_pk_updel_multi_column.test
@@ -2,6 +2,9 @@
 # description: PRIMARY KEY and update/delete on multiple columns
 # group: [primarykey]
 
+# See test/sql/index/art/constraints/test_art_tx_returning.test.
+require vector_size 2048
+
 statement ok
 PRAGMA enable_verification;
 
diff --git a/test/sql/copy/csv/14512.test b/test/sql/copy/csv/14512.test
index 97df786ccb28..79af7b8399c0 100644
--- a/test/sql/copy/csv/14512.test
+++ b/test/sql/copy/csv/14512.test
@@ -8,7 +8,7 @@ PRAGMA enable_verification
 query II
 FROM read_csv('data/csv/14512.csv', strict_mode=TRUE);
 ----
-onions 	,
+onions	,
 
 query I
 select columns FROM sniff_csv('data/csv/14512.csv')
@@ -20,7 +20,7 @@ FROM 'data/csv/14512_og.csv';
 ----
 00000579000098	13.99	EA	PINE RIDGE CHENIN VOIGNIER	750.0	ML	1	13	NULL	1	NULL	NULL	NULL	NULL	NULL	NULL	DEFAULT BRAND	NULL	NULL	NULL	NULL	BEER & WINE	NULL	NULL	7.25	{"sales_tax":{ "tax_type": "rate_percent", "value" :0.0725}}
 00000609082001	3.99	EA	MADELAINE MINI MILK CHOCOLATE TURKEY	1.0	OZ	1	13	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	MADELEINE	NULL	NULL	NULL	NULL	CANDY	NULL	NULL	7.25	{"sales_tax":{ "tax_type": "rate_percent", "value" :0.0725}}
-00817566020096	9.99	EA	COTSWOLD EW	5.3	OZ	1	13	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	LONG CLAWSON	NULL	NULL	NULL	NULL	DELI	INGREDIENTS: DOUBLE GLOUCESTER CHEESE (PASTEURIZED MILK  SALT  ENZYMES  DAIRY CULTURES  ANNATTO  EXTRACT AS A COLOR)  RECONSTITUTED MINCED ONIONS (2%)  DRIED CHIVES. CONTAINS: MILK     THIS PRODUCT WAS PRODUCED IN AN ENVIRONMENT THAT ALSO USES PEANUTS  TREE NUTS  EGGS  MILK  WHEAT  SOY  FISH  SHELLFISH  AND SESAME. 	NULL	2.0	{"sales_tax":{ "tax_type": "rate_percent", "value" :0.02}}
+00817566020096	9.99	EA	COTSWOLD EW	5.3	OZ	1	13	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	LONG CLAWSON	NULL	NULL	NULL	NULL	DELI	INGREDIENTS: DOUBLE GLOUCESTER CHEESE (PASTEURIZED MILK  SALT  ENZYMES  DAIRY CULTURES  ANNATTO  EXTRACT AS A COLOR)  RECONSTITUTED MINCED ONIONS (2%)  DRIED CHIVES. CONTAINS: MILK     THIS PRODUCT WAS PRODUCED IN AN ENVIRONMENT THAT ALSO USES PEANUTS  TREE NUTS  EGGS  MILK  WHEAT  SOY  FISH  SHELLFISH  AND SESAME.	NULL	2.0	{"sales_tax":{ "tax_type": "rate_percent", "value" :0.02}}
 
 query I
 select columns FROM sniff_csv('data/csv/14512_og.csv')
diff --git a/test/sql/copy/csv/parallel/test_parallel_csv.test b/test/sql/copy/csv/parallel/test_parallel_csv.test
index b903844ccf41..ce3cfefede61 100644
--- a/test/sql/copy/csv/parallel/test_parallel_csv.test
+++ b/test/sql/copy/csv/parallel/test_parallel_csv.test
@@ -5,6 +5,13 @@
 statement ok
 PRAGMA enable_verification
 
+query IIIIIIIIIIIIIIIIIIIIIIIIII
+FROM read_csv('data/csv/14512_og.csv', buffer_size = 473)
+----
+00000579000098	13.99	EA	PINE RIDGE CHENIN VOIGNIER	750.0	ML	1	13	NULL	1	NULL	NULL	NULL	NULL	NULL	NULL	DEFAULT BRAND	NULL	NULL	NULL	NULL	BEER & WINE	NULL	NULL	7.25	{"sales_tax":{ "tax_type": "rate_percent", "value" :0.0725}}
+00000609082001	3.99	EA	MADELAINE MINI MILK CHOCOLATE TURKEY	1.0	OZ	1	13	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	MADELEINE	NULL	NULL	NULL	NULL	CANDY	NULL	NULL	7.25	{"sales_tax":{ "tax_type": "rate_percent", "value" :0.0725}}
+00817566020096	9.99	EA	COTSWOLD EW	5.3	OZ	1	13	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	LONG CLAWSON	NULL	NULL	NULL	NULL	DELI	INGREDIENTS: DOUBLE GLOUCESTER CHEESE (PASTEURIZED MILK  SALT  ENZYMES  DAIRY CULTURES  ANNATTO  EXTRACT AS A COLOR)  RECONSTITUTED MINCED ONIONS (2%)  DRIED CHIVES. CONTAINS: MILK     THIS PRODUCT WAS PRODUCED IN AN ENVIRONMENT THAT ALSO USES PEANUTS  TREE NUTS  EGGS  MILK  WHEAT  SOY  FISH  SHELLFISH  AND SESAME.	NULL	2.0	{"sales_tax":{ "tax_type": "rate_percent", "value" :0.02}}
+
 
 query III
 select * from read_csv_auto('data/csv/dirty_line.csv',  skip = 1)
diff --git a/test/sql/copy/csv/test_thijs_unquoted_file.test b/test/sql/copy/csv/test_thijs_unquoted_file.test
index b40e9af98897..64a2fc222192 100644
--- a/test/sql/copy/csv/test_thijs_unquoted_file.test
+++ b/test/sql/copy/csv/test_thijs_unquoted_file.test
@@ -14,7 +14,7 @@ from read_csv('data/csv/thijs_unquoted.csv', quote='"', sep='|', escape='"', col
 query III
 from read_csv('data/csv/thijs_unquoted.csv', quote='"', sep='|', escape='"', columns={'a':'varchar', 'b': 'varchar', 'c': 'integer'}, auto_detect=false, strict_mode = False);
 ----
-HYDRONIC GESELLSCHAFT FÜR WASSERTECHNIK MBH                                                     	 	2011
+HYDRONIC GESELLSCHAFT FÜR WASSERTECHNIK MBH	 	2011
 ANTON SONNENSCHUTZSYSTEME GESELLSCHAFT MIT BESCHRÄNKTER HAFTUN	 	2012
 ENERGYS MAINTENANCE S	 	2015
 SYSTEMAT BELGIUM  S	 	2013
diff --git a/test/sql/copy_database/copy_database_index.test b/test/sql/copy_database/copy_database_index.test
index 0c7db26eafe9..e8f7351b7d60 100644
--- a/test/sql/copy_database/copy_database_index.test
+++ b/test/sql/copy_database/copy_database_index.test
@@ -43,13 +43,10 @@ SELECT * FROM test WHERE a = 42;
 ----
 42	88	hello
 
-# FIXME: We do not yet copy indexes.
-# See issue 14909.
-
 query II
 EXPLAIN ANALYZE SELECT * FROM test WHERE a = 42;
 ----
-analyzed_plan	<REGEX>:.*Type: Sequential Scan.*
+analyzed_plan	<REGEX>:.*Type: Index Scan.*
 
 statement ok
 DROP INDEX i_index;
diff --git a/test/sql/copy_database/copy_database_with_index.test b/test/sql/copy_database/copy_database_with_index.test
new file mode 100644
index 000000000000..b4daa7003bc3
--- /dev/null
+++ b/test/sql/copy_database/copy_database_with_index.test
@@ -0,0 +1,42 @@
+# name: test/sql/copy_database/copy_database_with_index.test
+# description: Test the COPY DATABASE statement with an index.
+# group: [copy_database]
+
+require noforcestorage
+
+statement ok
+PRAGMA enable_verification;
+
+statement ok
+ATTACH ':memory:' AS db1;
+
+statement ok
+USE db1;
+
+statement ok
+CREATE TABLE data AS
+SELECT i, hash(i)::VARCHAR AS value FROM generate_series(1, 10000) s(i);
+
+statement ok
+ALTER TABLE data ALTER COLUMN value SET NOT NULL;
+
+statement ok
+CREATE INDEX data_value ON data(value);
+
+statement ok
+ATTACH ':memory:' AS db2;
+
+statement ok
+COPY FROM DATABASE db1 TO db2;
+
+query III
+SELECT database_name, table_name, index_name FROM duckdb_indexes ORDER BY ALL;
+----
+db1	data	data_value
+db2	data	data_value
+
+query III
+SELECT database_name, table_name, index_count FROM duckdb_tables ORDER BY ALL;
+----
+db1	data	1
+db2	data	1
\ No newline at end of file
diff --git a/test/sql/copy_database/copy_database_with_unique_index.test b/test/sql/copy_database/copy_database_with_unique_index.test
new file mode 100644
index 000000000000..9c0e00123cf8
--- /dev/null
+++ b/test/sql/copy_database/copy_database_with_unique_index.test
@@ -0,0 +1,62 @@
+# name: test/sql/copy_database/copy_database_with_unique_index.test
+# description: Test the COPY DATABASE statement with unique indexes.
+# group: [copy_database]
+
+require noforcestorage
+
+statement ok
+PRAGMA enable_verification;
+
+statement ok
+ATTACH '__TEST_DIR__/copy_db_old.db' AS old;
+
+statement ok
+CREATE TABLE old.items (id INT, uniq INT UNIQUE);
+
+statement ok
+INSERT INTO old.items VALUES (1, 1), (2, 2), (3, 3);
+
+statement ok
+CREATE UNIQUE INDEX idx_id ON old.items(id);
+
+statement ok
+ATTACH '__TEST_DIR__/copy_db_new1.db' AS new1;
+
+statement ok
+COPY FROM DATABASE old TO new1 (SCHEMA);
+
+statement ok
+COPY FROM DATABASE old TO new1 (DATA);
+
+query II
+SELECT id, uniq FROM new1.items ORDER BY ALL;
+----
+1	1
+2	2
+3	3
+
+statement error
+INSERT INTO new1.items VALUES (1, 4);
+----
+<REGEX>:Constraint Error.*violates unique constraint.*
+
+statement error
+INSERT INTO new1.items VALUES (4, 1);
+----
+<REGEX>:Constraint Error.*violates unique constraint.*
+
+statement ok
+DETACH new1;
+
+statement ok
+ATTACH '__TEST_DIR__/copy_db_new1.db' AS new1;
+
+statement error
+INSERT INTO new1.items VALUES (1, 4);
+----
+<REGEX>:Constraint Error.*violates unique constraint.*
+
+statement error
+INSERT INTO new1.items VALUES (4, 1);
+----
+<REGEX>:Constraint Error.*violates unique constraint.*
diff --git a/test/sql/function/list/flatten.test b/test/sql/function/list/flatten.test
index e0c894b802ae..4869cb2233ce 100644
--- a/test/sql/function/list/flatten.test
+++ b/test/sql/function/list/flatten.test
@@ -150,3 +150,13 @@ query I
 select flatten(NULL);
 ----
 NULL
+
+query IIII rowsort
+with v_data (col, list) as ( select * FROM (VALUES ('a', [1,2,3]), ('b', [4,5]), ('a', [2,6])) ),
+        v_list_of_lists ( col, list, list_of_lists ) as ( select v.*, array_agg(v.list) over (partition by v.col) from v_data v )
+select v.*, flatten(v.list_of_lists) from v_list_of_lists v;
+----
+a	[1, 2, 3]	[[1, 2, 3], [2, 6]]	[1, 2, 3, 2, 6]
+a	[2, 6]	[[1, 2, 3], [2, 6]]	[1, 2, 3, 2, 6]
+b	[4, 5]	[[4, 5]]	[4, 5]
+
diff --git a/test/sql/index/art/constraints/test_art_eager_with_wal.test b/test/sql/index/art/constraints/test_art_eager_with_wal.test
new file mode 100644
index 000000000000..4d211adf7734
--- /dev/null
+++ b/test/sql/index/art/constraints/test_art_eager_with_wal.test
@@ -0,0 +1,39 @@
+# name: test/sql/index/art/constraints/test_art_eager_with_wal.test
+# description: Test eager constraint checking with WAL replay.
+# group: [constraints]
+
+load __TEST_DIR__/pk_duplicate_key_wal.db
+
+statement ok
+PRAGMA enable_verification;
+
+statement ok
+SET checkpoint_threshold = '10.0 GB';
+
+statement ok
+PRAGMA disable_checkpoint_on_shutdown;
+
+statement ok
+CREATE TABLE tbl (id INT PRIMARY KEY);
+
+statement ok
+INSERT INTO tbl VALUES (1);
+
+statement ok
+BEGIN;
+
+statement ok
+DELETE FROM tbl WHERE id = 1;
+
+statement ok
+INSERT INTO tbl VALUES (1);
+
+statement ok
+COMMIT;
+
+restart
+
+statement error
+INSERT INTO tbl VALUES (1);
+----
+<REGEX>:Constraint Error.*violates primary key constraint.*
\ No newline at end of file
diff --git a/test/sql/index/art/constraints/test_art_tx_returning.test b/test/sql/index/art/constraints/test_art_tx_returning.test
index 318d68b08844..de9e8963f04c 100644
--- a/test/sql/index/art/constraints/test_art_tx_returning.test
+++ b/test/sql/index/art/constraints/test_art_tx_returning.test
@@ -2,6 +2,13 @@
 # description: Test updates on the primary key containing RETURNING.
 # group: [constraints]
 
+# For each incoming chunk, we add the row IDs to the delete index.
+# For standard_vector_size = 2, we delete [0, 1], and then try to insert value [1, 2].
+# This is expected to throw a constraint violation.
+# The value 2 is not yet in the delete index, as the chunk that would add that value hasn't been processed, yet.
+# This scenario is a known limitation (also in postgres).
+require vector_size 2048
+
 statement ok
 PRAGMA enable_verification
 
diff --git a/test/sql/projection/select_star_rename.test b/test/sql/projection/select_star_rename.test
index f52f4a9e7b2a..7f3be601c2e0 100644
--- a/test/sql/projection/select_star_rename.test
+++ b/test/sql/projection/select_star_rename.test
@@ -17,6 +17,12 @@ SELECT renamed_col FROM (SELECT * RENAME i AS renamed_col FROM integers)
 ----
 1
 
+# rename with COLUMNS
+query I
+SELECT renamed_col FROM (SELECT COLUMNS(* RENAME i AS renamed_col) FROM integers)
+----
+1
+
 # qualified
 query I
 SELECT renamed_col FROM (SELECT * RENAME integers.i AS renamed_col FROM integers)
diff --git a/test/sql/sample/table_samples/basic_sample_tests.test b/test/sql/sample/table_samples/basic_sample_tests.test
index 1bb07364c9c3..733b14b6827a 100644
--- a/test/sql/sample/table_samples/basic_sample_tests.test
+++ b/test/sql/sample/table_samples/basic_sample_tests.test
@@ -1,6 +1,11 @@
 # name: test/sql/sample/table_samples/basic_sample_tests.test
 # group: [table_samples]
 
+# currently require fixed vector size since the "randomness" of the sample depends on
+# the vector size. If the vector size decreases, the randomness of the sample decreases
+# This is especially noticeable for small tables and their samples
+require vector_size 2048
+
 statement ok
 PRAGMA enable_verification
 
diff --git a/test/sql/sample/table_samples/sample_stores_rows_from_later_on.test_slow b/test/sql/sample/table_samples/sample_stores_rows_from_later_on.test_slow
index c88b9cd74a67..7f883d9fa19b 100644
--- a/test/sql/sample/table_samples/sample_stores_rows_from_later_on.test_slow
+++ b/test/sql/sample/table_samples/sample_stores_rows_from_later_on.test_slow
@@ -2,6 +2,7 @@
 # description: Test sampling of larger relations
 # group: [table_samples]
 
+# required when testing table samples. See basic_sample_test.test
 require vector_size 2048
 
 require noforcestorage
diff --git a/test/sql/sample/table_samples/table_sample_converts_to_block_sample.test b/test/sql/sample/table_samples/table_sample_converts_to_block_sample.test
index 0ffca61f31ef..3e810c325a3b 100644
--- a/test/sql/sample/table_samples/table_sample_converts_to_block_sample.test
+++ b/test/sql/sample/table_samples/table_sample_converts_to_block_sample.test
@@ -2,6 +2,7 @@
 # description: Test sampling of larger relations
 # group: [table_samples]
 
+# required when testing table samples. See basic_sample_test.test
 require vector_size 2048
 
 require noforcestorage
diff --git a/test/sql/sample/table_samples/table_sample_is_stored.test_slow b/test/sql/sample/table_samples/table_sample_is_stored.test_slow
index f8370557d2a5..aef613355f64 100644
--- a/test/sql/sample/table_samples/table_sample_is_stored.test_slow
+++ b/test/sql/sample/table_samples/table_sample_is_stored.test_slow
@@ -2,6 +2,7 @@
 # description: Test sampling of larger relations
 # group: [table_samples]
 
+# required when testing table samples. See basic_sample_test.test
 require vector_size 2048
 
 require noforcestorage
diff --git a/test/sql/sample/table_samples/test_sample_is_destroyed_on_updates.test b/test/sql/sample/table_samples/test_sample_is_destroyed_on_updates.test
index 410d9004e87c..5041df626617 100644
--- a/test/sql/sample/table_samples/test_sample_is_destroyed_on_updates.test
+++ b/test/sql/sample/table_samples/test_sample_is_destroyed_on_updates.test
@@ -2,6 +2,7 @@
 # description: Test sampling of larger relations
 # group: [table_samples]
 
+# required when testing table samples. See basic_sample_test.test
 require vector_size 2048
 
 load __TEST_DIR__/test_sample_is_destroyed_on_update.db
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_functions_date.py b/tools/pythonpkg/tests/fast/spark/test_spark_functions_date.py
index fda95752a714..8a03fd68169c 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_functions_date.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_functions_date.py
@@ -177,20 +177,13 @@ def test_to_date(self, spark):
     def test_to_timestamp(self, spark):
         df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])
         res = df.select(F.to_timestamp(df.t).alias('dt')).collect()
-        # FIXME: Fix difference between DuckDB and Spark
-        if USE_ACTUAL_SPARK:
-            assert res == [Row(dt=datetime(1997, 2, 28, 10, 30))]
-        else:
-            assert res == [Row(dt=datetime(1997, 2, 28, 10, 30, tzinfo=timezone.utc))]
+        assert res == [Row(dt=datetime(1997, 2, 28, 10, 30))]
 
     def test_to_timestamp_ltz(self, spark):
         df = spark.createDataFrame([("2016-12-31",)], ["e"])
         res = df.select(F.to_timestamp_ltz(df.e).alias('r')).collect()
-        # FIXME: Fix difference between DuckDB and Spark
-        if USE_ACTUAL_SPARK:
-            assert res == [Row(r=datetime(2016, 12, 31, 0, 0))]
-        else:
-            assert res == [Row(r=datetime(2016, 12, 31, 0, 0, tzinfo=timezone.utc))]
+
+        assert res == [Row(r=datetime(2016, 12, 31, 0, 0))]
 
     def test_to_timestamp_ntz(self, spark):
         df = spark.createDataFrame([("2016-04-08",)], ["e"])
