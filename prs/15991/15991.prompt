You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
DuckDB Trigger Assertion Failure: result.data[storage_idx].GetType() == chunk.data[mapped_index].GetType()
### What happens?

The latest version of the DuckDB (latest main: v1.1.4-dev3741 ab8c909857) triggers Internal Error when running the following SQL statement: 

```sql
CREATE TABLE v00 (c01 INT, c02 STRING);
INSERT INTO v00 BY POSITION ( c02 ) OVERRIDING USER VALUE FROM LATERAL ( SELECT 'simple_string' ) AS ta507400 GROUP BY ALL WINDOW EVENT AS ( GROUPS BETWEEN 'abc' IS NOT UNKNOWN IN CASE WHEN 'abc' THEN 0 ELSE 'abc' END FOLLOWING AND UNBOUNDED FOLLOWING ) ORDER BY ALL RETURNING 'anything';
```

The code is working fine from the latest release: v1.1.3 19864453f7. Maybe just a faulty assertion? 

Here is the stack from ab8c909857:

```
Assertion triggered in file "/home/duckdb/duckdb/src/execution/operator/persistent/physical_insert.cpp" on line 150: result.data[storage_idx].GetType() == chunk.data[mapped_index].GetType()

#0  duckdb::InternalException::InternalException (this=0x60d000018ba0, Python Exception <class 'gdb.error'> There is no member named _M_dataplus.:
msg=) at /home/duckdb/duckdb/src/common/exception.cpp:320
#1  0x00000000020c1089 in duckdb::InternalException::InternalException<char const*, int, char const*> (this=0x60d000018ba0, msg=...,
    params=<optimized out>, params=<optimized out>, params=<optimized out>) at ../../src/include/duckdb/common/exception.hpp:313
#2  0x0000000001e672b1 in duckdb::DuckDBAssertInternal (condition=<optimized out>, condition_name=<optimized out>, file=<optimized out>,
    linenr=<optimized out>) at /home/duckdb/duckdb/src/common/assert.cpp:13
#3  0x000000000a69cf33 in duckdb::PhysicalInsert::ResolveDefaults (table=..., chunk=..., column_index_map=..., default_executor=..., result=...)
    at /home/duckdb/duckdb/src/execution/operator/persistent/physical_insert.cpp:150
#4  0x000000000a6f5aaa in duckdb::PhysicalInsert::Sink (this=0x6150000e4a00, context=..., chunk=..., input=...)
    at /home/duckdb/duckdb/src/execution/operator/persistent/physical_insert.cpp:623
#5  0x0000000003cf87a8 in duckdb::PipelineExecutor::ExecutePushInternal (this=<optimized out>, input=..., chunk_budget=..., initial_idx=<optimized out>)
    at /home/duckdb/duckdb/src/parallel/pipeline_executor.cpp:332
#6  0x0000000003cdbda6 in duckdb::PipelineExecutor::Execute (this=0x615000083980, max_chunks=<optimized out>)
    at /home/duckdb/duckdb/src/parallel/pipeline_executor.cpp:250
#7  0x0000000003cdd7e0 in duckdb::PipelineExecutor::Execute (this=<optimized out>) at /home/duckdb/duckdb/src/parallel/pipeline_executor.cpp:278
#8  0x0000000003cd8c05 in duckdb::PipelineTask::ExecuteTask (this=0x60700008ace0, mode=<optimized out>) at /home/duckdb/duckdb/src/parallel/pipeline.cpp:51
#9  0x0000000003ca595c in duckdb::ExecutorTask::Execute (this=0x60700008ace0, mode=<optimized out>)
    at /home/duckdb/duckdb/src/parallel/executor_task.cpp:49
#10 0x0000000003d0f68f in duckdb::TaskScheduler::ExecuteForever (this=<optimized out>, marker=<optimized out>)
    at /home/duckdb/duckdb/src/parallel/task_scheduler.cpp:189
#11 0x000079e0d73d5df4 in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6
#12 0x000079e0d7195609 in start_thread (arg=<optimized out>) at pthread_create.c:477
#13 0x000079e0d7093353 in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95
```

### To Reproduce

1. Clone the DuckDB Git from the official repo.
2. Checkout to the latest main (v1.1.4-dev3741 ab8c909857).
3. Compile the DuckDB binary by using `make relassert` or `make debug`.
4. Run the compiled DuckDB and input the following SQL:

```sql
CREATE TABLE v00 (c01 INT, c02 STRING);
INSERT INTO v00 BY POSITION ( c02 ) OVERRIDING USER VALUE FROM LATERAL ( SELECT 'simple_string' ) AS ta507400 GROUP BY ALL WINDOW EVENT AS ( GROUPS BETWEEN 'abc' IS NOT UNKNOWN IN CASE WHEN 'abc' THEN 0 ELSE 'abc' END FOLLOWING AND UNBOUNDED FOLLOWING ) ORDER BY ALL RETURNING 'anything';
```

### OS:

Ubuntu 24.04 LTS

### DuckDB Version:

v1.1.4-dev3741 ab8c909857

### DuckDB Client:

cli

### Hardware:

_No response_

### Full Name:

Yu Liang

### Affiliation:

Pennsylvania State University

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a source build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have
DuckDB Trigger Assertion Failure: result.data[storage_idx].GetType() == chunk.data[mapped_index].GetType()
### What happens?

The latest version of the DuckDB (latest main: v1.1.4-dev3741 ab8c909857) triggers Internal Error when running the following SQL statement: 

```sql
CREATE TABLE v00 (c01 INT, c02 STRING);
INSERT INTO v00 BY POSITION ( c02 ) OVERRIDING USER VALUE FROM LATERAL ( SELECT 'simple_string' ) AS ta507400 GROUP BY ALL WINDOW EVENT AS ( GROUPS BETWEEN 'abc' IS NOT UNKNOWN IN CASE WHEN 'abc' THEN 0 ELSE 'abc' END FOLLOWING AND UNBOUNDED FOLLOWING ) ORDER BY ALL RETURNING 'anything';
```

The code is working fine from the latest release: v1.1.3 19864453f7. Maybe just a faulty assertion? 

Here is the stack from ab8c909857:

```
Assertion triggered in file "/home/duckdb/duckdb/src/execution/operator/persistent/physical_insert.cpp" on line 150: result.data[storage_idx].GetType() == chunk.data[mapped_index].GetType()

#0  duckdb::InternalException::InternalException (this=0x60d000018ba0, Python Exception <class 'gdb.error'> There is no member named _M_dataplus.:
msg=) at /home/duckdb/duckdb/src/common/exception.cpp:320
#1  0x00000000020c1089 in duckdb::InternalException::InternalException<char const*, int, char const*> (this=0x60d000018ba0, msg=...,
    params=<optimized out>, params=<optimized out>, params=<optimized out>) at ../../src/include/duckdb/common/exception.hpp:313
#2  0x0000000001e672b1 in duckdb::DuckDBAssertInternal (condition=<optimized out>, condition_name=<optimized out>, file=<optimized out>,
    linenr=<optimized out>) at /home/duckdb/duckdb/src/common/assert.cpp:13
#3  0x000000000a69cf33 in duckdb::PhysicalInsert::ResolveDefaults (table=..., chunk=..., column_index_map=..., default_executor=..., result=...)
    at /home/duckdb/duckdb/src/execution/operator/persistent/physical_insert.cpp:150
#4  0x000000000a6f5aaa in duckdb::PhysicalInsert::Sink (this=0x6150000e4a00, context=..., chunk=..., input=...)
    at /home/duckdb/duckdb/src/execution/operator/persistent/physical_insert.cpp:623
#5  0x0000000003cf87a8 in duckdb::PipelineExecutor::ExecutePushInternal (this=<optimized out>, input=..., chunk_budget=..., initial_idx=<optimized out>)
    at /home/duckdb/duckdb/src/parallel/pipeline_executor.cpp:332
#6  0x0000000003cdbda6 in duckdb::PipelineExecutor::Execute (this=0x615000083980, max_chunks=<optimized out>)
    at /home/duckdb/duckdb/src/parallel/pipeline_executor.cpp:250
#7  0x0000000003cdd7e0 in duckdb::PipelineExecutor::Execute (this=<optimized out>) at /home/duckdb/duckdb/src/parallel/pipeline_executor.cpp:278
#8  0x0000000003cd8c05 in duckdb::PipelineTask::ExecuteTask (this=0x60700008ace0, mode=<optimized out>) at /home/duckdb/duckdb/src/parallel/pipeline.cpp:51
#9  0x0000000003ca595c in duckdb::ExecutorTask::Execute (this=0x60700008ace0, mode=<optimized out>)
    at /home/duckdb/duckdb/src/parallel/executor_task.cpp:49
#10 0x0000000003d0f68f in duckdb::TaskScheduler::ExecuteForever (this=<optimized out>, marker=<optimized out>)
    at /home/duckdb/duckdb/src/parallel/task_scheduler.cpp:189
#11 0x000079e0d73d5df4 in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6
#12 0x000079e0d7195609 in start_thread (arg=<optimized out>) at pthread_create.c:477
#13 0x000079e0d7093353 in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95
```

### To Reproduce

1. Clone the DuckDB Git from the official repo.
2. Checkout to the latest main (v1.1.4-dev3741 ab8c909857).
3. Compile the DuckDB binary by using `make relassert` or `make debug`.
4. Run the compiled DuckDB and input the following SQL:

```sql
CREATE TABLE v00 (c01 INT, c02 STRING);
INSERT INTO v00 BY POSITION ( c02 ) OVERRIDING USER VALUE FROM LATERAL ( SELECT 'simple_string' ) AS ta507400 GROUP BY ALL WINDOW EVENT AS ( GROUPS BETWEEN 'abc' IS NOT UNKNOWN IN CASE WHEN 'abc' THEN 0 ELSE 'abc' END FOLLOWING AND UNBOUNDED FOLLOWING ) ORDER BY ALL RETURNING 'anything';
```

### OS:

Ubuntu 24.04 LTS

### DuckDB Version:

v1.1.4-dev3741 ab8c909857

### DuckDB Client:

cli

### Hardware:

_No response_

### Full Name:

Yu Liang

### Affiliation:

Pennsylvania State University

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a source build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=master" alt="Github Actions Badge">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The detail of benchmarks is in our [Benchmark Guide](benchmark/README.md).
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
[end of README.md]
[start of benchmark/tpch/startup.cpp]
1: #include "benchmark_runner.hpp"
2: #include "compare_result.hpp"
3: #include "tpch-extension.hpp"
4: #include "duckdb_benchmark_macro.hpp"
5: 
6: using namespace duckdb;
7: 
8: #define SF 1
9: 
10: #define TPCHStartup(QUERY)                                                                                             \
11: 	string db_path = "duckdb_benchmark_db.db";                                                                         \
12: 	void Load(DuckDBBenchmarkState *state) override {                                                                  \
13: 		DeleteDatabase(db_path);                                                                                       \
14: 		{                                                                                                              \
15: 			DuckDB db(db_path);                                                                                        \
16: 			Connection con(db);                                                                                        \
17: 			con.Query("CALL dbgen(sf=" + std::to_string(SF) + ")");                                                    \
18: 		}                                                                                                              \
19: 		{                                                                                                              \
20: 			auto config = GetConfig();                                                                                 \
21: 			config->options.checkpoint_wal_size = 0;                                                                   \
22: 			DuckDB db(db_path, config.get());                                                                          \
23: 		}                                                                                                              \
24: 	}                                                                                                                  \
25: 	void RunBenchmark(DuckDBBenchmarkState *state) override {                                                          \
26: 		auto config = GetConfig();                                                                                     \
27: 		DuckDB db(db_path, config.get());                                                                              \
28: 		Connection con(db);                                                                                            \
29: 		state->result = con.Query(QUERY);                                                                              \
30: 	}                                                                                                                  \
31: 	string BenchmarkInfo() override {                                                                                  \
32: 		return string("Start a TPC-H SF1 database and run ") + QUERY + string(" in the database");                     \
33: 	}
34: 
35: #define NormalConfig()                                                                                                 \
36: 	unique_ptr<DBConfig> GetConfig() {                                                                                 \
37: 		return make_unique<DBConfig>();                                                                                \
38: 	}
39: 
40: DUCKDB_BENCHMARK(TPCHEmptyStartup, "[startup]")
41: TPCHStartup("SELECT * FROM lineitem WHERE 1=0") NormalConfig() string VerifyResult(QueryResult *result) override {
42: 	if (result->HasError()) {
43: 		return result->GetError();
44: 	}
45: 	return string();
46: }
47: FINISH_BENCHMARK(TPCHEmptyStartup)
48: 
49: DUCKDB_BENCHMARK(TPCHCount, "[startup]")
50: TPCHStartup("SELECT COUNT(*) FROM lineitem") NormalConfig() string VerifyResult(QueryResult *result) override {
51: 	if (result->HasError()) {
52: 		return result->GetError();
53: 	}
54: 	return string();
55: }
56: FINISH_BENCHMARK(TPCHCount)
57: 
58: DUCKDB_BENCHMARK(TPCHSimpleAggr, "[startup]")
59: TPCHStartup("SELECT SUM(l_extendedprice) FROM lineitem") NormalConfig() string
60:     VerifyResult(QueryResult *result) override {
61: 	if (result->HasError()) {
62: 		return result->GetError();
63: 	}
64: 	return string();
65: }
66: FINISH_BENCHMARK(TPCHSimpleAggr)
67: 
68: DUCKDB_BENCHMARK(TPCHQ1, "[startup]")
69: TPCHStartup("PRAGMA tpch(1)") NormalConfig() string VerifyResult(QueryResult *result) override {
70: 	if (result->HasError()) {
71: 		return result->GetError();
72: 	}
73: 	return compare_csv(*result, TPCHExtension::GetAnswer(SF, 1), true);
74: }
75: FINISH_BENCHMARK(TPCHQ1)
[end of benchmark/tpch/startup.cpp]
[start of extension/parquet/column_writer.cpp]
1: #include "column_writer.hpp"
2: 
3: #include "duckdb.hpp"
4: #include "geo_parquet.hpp"
5: #include "parquet_dbp_encoder.hpp"
6: #include "parquet_dlba_encoder.hpp"
7: #include "parquet_rle_bp_decoder.hpp"
8: #include "parquet_rle_bp_encoder.hpp"
9: #include "parquet_bss_encoder.hpp"
10: #include "parquet_statistics.hpp"
11: #include "parquet_writer.hpp"
12: #ifndef DUCKDB_AMALGAMATION
13: #include "duckdb/common/exception.hpp"
14: #include "duckdb/common/operator/comparison_operators.hpp"
15: #include "duckdb/common/serializer/buffered_file_writer.hpp"
16: #include "duckdb/common/serializer/memory_stream.hpp"
17: #include "duckdb/common/serializer/write_stream.hpp"
18: #include "duckdb/common/string_map_set.hpp"
19: #include "duckdb/common/types/hugeint.hpp"
20: #include "duckdb/common/types/time.hpp"
21: #include "duckdb/common/types/timestamp.hpp"
22: #include "duckdb/common/types/uhugeint.hpp"
23: #include "duckdb/execution/expression_executor.hpp"
24: #endif
25: 
26: #include "brotli/encode.h"
27: #include "lz4.hpp"
28: #include "miniz_wrapper.hpp"
29: #include "snappy.h"
30: #include "zstd.h"
31: #include "zstd/common/xxhash.hpp"
32: 
33: #include <cmath>
34: 
35: namespace duckdb {
36: 
37: using namespace duckdb_parquet; // NOLINT
38: using namespace duckdb_miniz;   // NOLINT
39: 
40: using duckdb_parquet::CompressionCodec;
41: using duckdb_parquet::ConvertedType;
42: using duckdb_parquet::Encoding;
43: using duckdb_parquet::FieldRepetitionType;
44: using duckdb_parquet::FileMetaData;
45: using duckdb_parquet::PageHeader;
46: using duckdb_parquet::PageType;
47: using ParquetRowGroup = duckdb_parquet::RowGroup;
48: using duckdb_parquet::Type;
49: 
50: #define PARQUET_DEFINE_VALID 65535
51: 
52: //===--------------------------------------------------------------------===//
53: // ColumnWriterStatistics
54: //===--------------------------------------------------------------------===//
55: ColumnWriterStatistics::~ColumnWriterStatistics() {
56: }
57: 
58: bool ColumnWriterStatistics::HasStats() {
59: 	return false;
60: }
61: 
62: string ColumnWriterStatistics::GetMin() {
63: 	return string();
64: }
65: 
66: string ColumnWriterStatistics::GetMax() {
67: 	return string();
68: }
69: 
70: string ColumnWriterStatistics::GetMinValue() {
71: 	return string();
72: }
73: 
74: string ColumnWriterStatistics::GetMaxValue() {
75: 	return string();
76: }
77: 
78: //===--------------------------------------------------------------------===//
79: // RleBpEncoder
80: //===--------------------------------------------------------------------===//
81: RleBpEncoder::RleBpEncoder(uint32_t bit_width)
82:     : byte_width((bit_width + 7) / 8), byte_count(idx_t(-1)), run_count(idx_t(-1)) {
83: }
84: 
85: // we always RLE everything (for now)
86: void RleBpEncoder::BeginPrepare(uint32_t first_value) {
87: 	byte_count = 0;
88: 	run_count = 1;
89: 	current_run_count = 1;
90: 	last_value = first_value;
91: }
92: 
93: void RleBpEncoder::FinishRun() {
94: 	// last value, or value has changed
95: 	// write out the current run
96: 	byte_count += ParquetDecodeUtils::GetVarintSize(current_run_count << 1) + byte_width;
97: 	current_run_count = 1;
98: 	run_count++;
99: }
100: 
101: void RleBpEncoder::PrepareValue(uint32_t value) {
102: 	if (value != last_value) {
103: 		FinishRun();
104: 		last_value = value;
105: 	} else {
106: 		current_run_count++;
107: 	}
108: }
109: 
110: void RleBpEncoder::FinishPrepare() {
111: 	FinishRun();
112: }
113: 
114: idx_t RleBpEncoder::GetByteCount() {
115: 	D_ASSERT(byte_count != idx_t(-1));
116: 	return byte_count;
117: }
118: 
119: void RleBpEncoder::BeginWrite(WriteStream &writer, uint32_t first_value) {
120: 	// start the RLE runs
121: 	last_value = first_value;
122: 	current_run_count = 1;
123: }
124: 
125: void RleBpEncoder::WriteRun(WriteStream &writer) {
126: 	// write the header of the run
127: 	ParquetDecodeUtils::VarintEncode(current_run_count << 1, writer);
128: 	// now write the value
129: 	D_ASSERT(last_value >> (byte_width * 8) == 0);
130: 	switch (byte_width) {
131: 	case 1:
132: 		writer.Write<uint8_t>(last_value);
133: 		break;
134: 	case 2:
135: 		writer.Write<uint16_t>(last_value);
136: 		break;
137: 	case 3:
138: 		writer.Write<uint8_t>(last_value & 0xFF);
139: 		writer.Write<uint8_t>((last_value >> 8) & 0xFF);
140: 		writer.Write<uint8_t>((last_value >> 16) & 0xFF);
141: 		break;
142: 	case 4:
143: 		writer.Write<uint32_t>(last_value);
144: 		break;
145: 	default:
146: 		throw InternalException("unsupported byte width for RLE encoding");
147: 	}
148: 	current_run_count = 1;
149: }
150: 
151: void RleBpEncoder::WriteValue(WriteStream &writer, uint32_t value) {
152: 	if (value != last_value) {
153: 		WriteRun(writer);
154: 		last_value = value;
155: 	} else {
156: 		current_run_count++;
157: 	}
158: }
159: 
160: void RleBpEncoder::FinishWrite(WriteStream &writer) {
161: 	WriteRun(writer);
162: }
163: 
164: //===--------------------------------------------------------------------===//
165: // ColumnWriter
166: //===--------------------------------------------------------------------===//
167: ColumnWriter::ColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,
168:                            idx_t max_define, bool can_have_nulls)
169:     : writer(writer), schema_idx(schema_idx), schema_path(std::move(schema_path_p)), max_repeat(max_repeat),
170:       max_define(max_define), can_have_nulls(can_have_nulls) {
171: }
172: ColumnWriter::~ColumnWriter() {
173: }
174: 
175: ColumnWriterState::~ColumnWriterState() {
176: }
177: 
178: void ColumnWriter::CompressPage(MemoryStream &temp_writer, size_t &compressed_size, data_ptr_t &compressed_data,
179:                                 unique_ptr<data_t[]> &compressed_buf) {
180: 	switch (writer.GetCodec()) {
181: 	case CompressionCodec::UNCOMPRESSED:
182: 		compressed_size = temp_writer.GetPosition();
183: 		compressed_data = temp_writer.GetData();
184: 		break;
185: 
186: 	case CompressionCodec::SNAPPY: {
187: 		compressed_size = duckdb_snappy::MaxCompressedLength(temp_writer.GetPosition());
188: 		compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
189: 		duckdb_snappy::RawCompress(const_char_ptr_cast(temp_writer.GetData()), temp_writer.GetPosition(),
190: 		                           char_ptr_cast(compressed_buf.get()), &compressed_size);
191: 		compressed_data = compressed_buf.get();
192: 		D_ASSERT(compressed_size <= duckdb_snappy::MaxCompressedLength(temp_writer.GetPosition()));
193: 		break;
194: 	}
195: 	case CompressionCodec::LZ4_RAW: {
196: 		compressed_size = duckdb_lz4::LZ4_compressBound(UnsafeNumericCast<int32_t>(temp_writer.GetPosition()));
197: 		compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
198: 		compressed_size = duckdb_lz4::LZ4_compress_default(
199: 		    const_char_ptr_cast(temp_writer.GetData()), char_ptr_cast(compressed_buf.get()),
200: 		    UnsafeNumericCast<int32_t>(temp_writer.GetPosition()), UnsafeNumericCast<int32_t>(compressed_size));
201: 		compressed_data = compressed_buf.get();
202: 		break;
203: 	}
204: 	case CompressionCodec::GZIP: {
205: 		MiniZStream s;
206: 		compressed_size = s.MaxCompressedLength(temp_writer.GetPosition());
207: 		compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
208: 		s.Compress(const_char_ptr_cast(temp_writer.GetData()), temp_writer.GetPosition(),
209: 		           char_ptr_cast(compressed_buf.get()), &compressed_size);
210: 		compressed_data = compressed_buf.get();
211: 		break;
212: 	}
213: 	case CompressionCodec::ZSTD: {
214: 		compressed_size = duckdb_zstd::ZSTD_compressBound(temp_writer.GetPosition());
215: 		compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
216: 		compressed_size = duckdb_zstd::ZSTD_compress((void *)compressed_buf.get(), compressed_size,
217: 		                                             (const void *)temp_writer.GetData(), temp_writer.GetPosition(),
218: 		                                             UnsafeNumericCast<int32_t>(writer.CompressionLevel()));
219: 		compressed_data = compressed_buf.get();
220: 		break;
221: 	}
222: 	case CompressionCodec::BROTLI: {
223: 
224: 		compressed_size = duckdb_brotli::BrotliEncoderMaxCompressedSize(temp_writer.GetPosition());
225: 		compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
226: 
227: 		duckdb_brotli::BrotliEncoderCompress(BROTLI_DEFAULT_QUALITY, BROTLI_DEFAULT_WINDOW, BROTLI_DEFAULT_MODE,
228: 		                                     temp_writer.GetPosition(), temp_writer.GetData(), &compressed_size,
229: 		                                     compressed_buf.get());
230: 		compressed_data = compressed_buf.get();
231: 
232: 		break;
233: 	}
234: 	default:
235: 		throw InternalException("Unsupported codec for Parquet Writer");
236: 	}
237: 
238: 	if (compressed_size > idx_t(NumericLimits<int32_t>::Maximum())) {
239: 		throw InternalException("Parquet writer: %d compressed page size out of range for type integer",
240: 		                        temp_writer.GetPosition());
241: 	}
242: }
243: 
244: void ColumnWriter::HandleRepeatLevels(ColumnWriterState &state, ColumnWriterState *parent, idx_t count,
245:                                       idx_t max_repeat) const {
246: 	if (!parent) {
247: 		// no repeat levels without a parent node
248: 		return;
249: 	}
250: 	while (state.repetition_levels.size() < parent->repetition_levels.size()) {
251: 		state.repetition_levels.push_back(parent->repetition_levels[state.repetition_levels.size()]);
252: 	}
253: }
254: 
255: void ColumnWriter::HandleDefineLevels(ColumnWriterState &state, ColumnWriterState *parent, const ValidityMask &validity,
256:                                       const idx_t count, const uint16_t define_value, const uint16_t null_value) const {
257: 	if (parent) {
258: 		// parent node: inherit definition level from the parent
259: 		idx_t vector_index = 0;
260: 		while (state.definition_levels.size() < parent->definition_levels.size()) {
261: 			idx_t current_index = state.definition_levels.size();
262: 			if (parent->definition_levels[current_index] != PARQUET_DEFINE_VALID) {
263: 				state.definition_levels.push_back(parent->definition_levels[current_index]);
264: 			} else if (validity.RowIsValid(vector_index)) {
265: 				state.definition_levels.push_back(define_value);
266: 			} else {
267: 				if (!can_have_nulls) {
268: 					throw IOException("Parquet writer: map key column is not allowed to contain NULL values");
269: 				}
270: 				state.null_count++;
271: 				state.definition_levels.push_back(null_value);
272: 			}
273: 			if (parent->is_empty.empty() || !parent->is_empty[current_index]) {
274: 				vector_index++;
275: 			}
276: 		}
277: 	} else {
278: 		// no parent: set definition levels only from this validity mask
279: 		for (idx_t i = 0; i < count; i++) {
280: 			const auto is_null = !validity.RowIsValid(i);
281: 			state.definition_levels.emplace_back(is_null ? null_value : define_value);
282: 			state.null_count += is_null;
283: 		}
284: 		if (!can_have_nulls && state.null_count != 0) {
285: 			throw IOException("Parquet writer: map key column is not allowed to contain NULL values");
286: 		}
287: 	}
288: }
289: 
290: class ColumnWriterPageState {
291: public:
292: 	virtual ~ColumnWriterPageState() {
293: 	}
294: 
295: public:
296: 	template <class TARGET>
297: 	TARGET &Cast() {
298: 		DynamicCastCheck<TARGET>(this);
299: 		return reinterpret_cast<TARGET &>(*this);
300: 	}
301: 	template <class TARGET>
302: 	const TARGET &Cast() const {
303: 		D_ASSERT(dynamic_cast<const TARGET *>(this));
304: 		return reinterpret_cast<const TARGET &>(*this);
305: 	}
306: };
307: 
308: struct PageInformation {
309: 	idx_t offset = 0;
310: 	idx_t row_count = 0;
311: 	idx_t empty_count = 0;
312: 	idx_t estimated_page_size = 0;
313: };
314: 
315: struct PageWriteInformation {
316: 	PageHeader page_header;
317: 	unique_ptr<MemoryStream> temp_writer;
318: 	unique_ptr<ColumnWriterPageState> page_state;
319: 	idx_t write_page_idx = 0;
320: 	idx_t write_count = 0;
321: 	idx_t max_write_count = 0;
322: 	size_t compressed_size;
323: 	data_ptr_t compressed_data;
324: 	unique_ptr<data_t[]> compressed_buf;
325: };
326: 
327: class BasicColumnWriterState : public ColumnWriterState {
328: public:
329: 	BasicColumnWriterState(duckdb_parquet::RowGroup &row_group, idx_t col_idx)
330: 	    : row_group(row_group), col_idx(col_idx) {
331: 		page_info.emplace_back();
332: 	}
333: 	~BasicColumnWriterState() override = default;
334: 
335: 	duckdb_parquet::RowGroup &row_group;
336: 	idx_t col_idx;
337: 	vector<PageInformation> page_info;
338: 	vector<PageWriteInformation> write_info;
339: 	unique_ptr<ColumnWriterStatistics> stats_state;
340: 	idx_t current_page = 0;
341: 
342: 	unique_ptr<ParquetBloomFilter> bloom_filter;
343: };
344: 
345: //===--------------------------------------------------------------------===//
346: // BasicColumnWriter
347: // A base class for writing all non-compound types (ex. numerics, strings)
348: //===--------------------------------------------------------------------===//
349: class BasicColumnWriter : public ColumnWriter {
350: public:
351: 	BasicColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path, idx_t max_repeat,
352: 	                  idx_t max_define, bool can_have_nulls)
353: 	    : ColumnWriter(writer, schema_idx, std::move(schema_path), max_repeat, max_define, can_have_nulls) {
354: 	}
355: 
356: 	~BasicColumnWriter() override = default;
357: 
358: 	//! We limit the uncompressed page size to 100MB
359: 	//! The max size in Parquet is 2GB, but we choose a more conservative limit
360: 	static constexpr const idx_t MAX_UNCOMPRESSED_PAGE_SIZE = 100000000;
361: 	//! Dictionary pages must be below 2GB. Unlike data pages, there's only one dictionary page.
362: 	//! For this reason we go with a much higher, but still a conservative upper bound of 1GB;
363: 	static constexpr const idx_t MAX_UNCOMPRESSED_DICT_PAGE_SIZE = 1e9;
364: 	//! If the dictionary has this many entries, we stop creating the dictionary
365: 	static constexpr const idx_t DICTIONARY_ANALYZE_THRESHOLD = 1e4;
366: 	//! The maximum size a key entry in an RLE page takes
367: 	static constexpr const idx_t MAX_DICTIONARY_KEY_SIZE = sizeof(uint32_t);
368: 	//! The size of encoding the string length
369: 	static constexpr const idx_t STRING_LENGTH_SIZE = sizeof(uint32_t);
370: 
371: public:
372: 	unique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::RowGroup &row_group) override;
373: 	void Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;
374: 	void BeginWrite(ColumnWriterState &state) override;
375: 	void Write(ColumnWriterState &state, Vector &vector, idx_t count) override;
376: 	void FinalizeWrite(ColumnWriterState &state) override;
377: 
378: protected:
379: 	static void WriteLevels(WriteStream &temp_writer, const unsafe_vector<uint16_t> &levels, idx_t max_value,
380: 	                        idx_t start_offset, idx_t count);
381: 
382: 	virtual duckdb_parquet::Encoding::type GetEncoding(BasicColumnWriterState &state);
383: 
384: 	void NextPage(BasicColumnWriterState &state);
385: 	void FlushPage(BasicColumnWriterState &state);
386: 
387: 	//! Initializes the state used to track statistics during writing. Only used for scalar types.
388: 	virtual unique_ptr<ColumnWriterStatistics> InitializeStatsState();
389: 
390: 	//! Initialize the writer for a specific page. Only used for scalar types.
391: 	virtual unique_ptr<ColumnWriterPageState> InitializePageState(BasicColumnWriterState &state);
392: 
393: 	//! Flushes the writer for a specific page. Only used for scalar types.
394: 	virtual void FlushPageState(WriteStream &temp_writer, ColumnWriterPageState *state);
395: 
396: 	//! Retrieves the row size of a vector at the specified location. Only used for scalar types.
397: 	virtual idx_t GetRowSize(const Vector &vector, const idx_t index, const BasicColumnWriterState &state) const;
398: 	//! Writes a (subset of a) vector to the specified serializer. Only used for scalar types.
399: 	virtual void WriteVector(WriteStream &temp_writer, ColumnWriterStatistics *stats, ColumnWriterPageState *page_state,
400: 	                         Vector &vector, idx_t chunk_start, idx_t chunk_end) = 0;
401: 
402: 	virtual bool HasDictionary(BasicColumnWriterState &state_p) {
403: 		return false;
404: 	}
405: 	//! The number of elements in the dictionary
406: 	virtual idx_t DictionarySize(BasicColumnWriterState &state_p);
407: 	void WriteDictionary(BasicColumnWriterState &state, unique_ptr<MemoryStream> temp_writer, idx_t row_count);
408: 	virtual void FlushDictionary(BasicColumnWriterState &state, ColumnWriterStatistics *stats);
409: 
410: 	void SetParquetStatistics(BasicColumnWriterState &state, duckdb_parquet::ColumnChunk &column);
411: 	void RegisterToRowGroup(duckdb_parquet::RowGroup &row_group);
412: };
413: 
414: unique_ptr<ColumnWriterState> BasicColumnWriter::InitializeWriteState(duckdb_parquet::RowGroup &row_group) {
415: 	auto result = make_uniq<BasicColumnWriterState>(row_group, row_group.columns.size());
416: 	RegisterToRowGroup(row_group);
417: 	return std::move(result);
418: }
419: 
420: void BasicColumnWriter::RegisterToRowGroup(duckdb_parquet::RowGroup &row_group) {
421: 	duckdb_parquet::ColumnChunk column_chunk;
422: 	column_chunk.__isset.meta_data = true;
423: 	column_chunk.meta_data.codec = writer.GetCodec();
424: 	column_chunk.meta_data.path_in_schema = schema_path;
425: 	column_chunk.meta_data.num_values = 0;
426: 	column_chunk.meta_data.type = writer.GetType(schema_idx);
427: 	row_group.columns.push_back(std::move(column_chunk));
428: }
429: 
430: unique_ptr<ColumnWriterPageState> BasicColumnWriter::InitializePageState(BasicColumnWriterState &state) {
431: 	return nullptr;
432: }
433: 
434: void BasicColumnWriter::FlushPageState(WriteStream &temp_writer, ColumnWriterPageState *state) {
435: }
436: 
437: void BasicColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
438: 	auto &state = state_p.Cast<BasicColumnWriterState>();
439: 	auto &col_chunk = state.row_group.columns[state.col_idx];
440: 
441: 	idx_t start = 0;
442: 	idx_t vcount = parent ? parent->definition_levels.size() - state.definition_levels.size() : count;
443: 	idx_t parent_index = state.definition_levels.size();
444: 	auto &validity = FlatVector::Validity(vector);
445: 	HandleRepeatLevels(state, parent, count, max_repeat);
446: 	HandleDefineLevels(state, parent, validity, count, max_define, max_define - 1);
447: 
448: 	idx_t vector_index = 0;
449: 	reference<PageInformation> page_info_ref = state.page_info.back();
450: 	for (idx_t i = start; i < vcount; i++) {
451: 		auto &page_info = page_info_ref.get();
452: 		page_info.row_count++;
453: 		col_chunk.meta_data.num_values++;
454: 		if (parent && !parent->is_empty.empty() && parent->is_empty[parent_index + i]) {
455: 			page_info.empty_count++;
456: 			continue;
457: 		}
458: 		if (validity.RowIsValid(vector_index)) {
459: 			page_info.estimated_page_size += GetRowSize(vector, vector_index, state);
460: 			if (page_info.estimated_page_size >= MAX_UNCOMPRESSED_PAGE_SIZE) {
461: 				PageInformation new_info;
462: 				new_info.offset = page_info.offset + page_info.row_count;
463: 				state.page_info.push_back(new_info);
464: 				page_info_ref = state.page_info.back();
465: 			}
466: 		}
467: 		vector_index++;
468: 	}
469: }
470: 
471: duckdb_parquet::Encoding::type BasicColumnWriter::GetEncoding(BasicColumnWriterState &state) {
472: 	return Encoding::PLAIN;
473: }
474: 
475: void BasicColumnWriter::BeginWrite(ColumnWriterState &state_p) {
476: 	auto &state = state_p.Cast<BasicColumnWriterState>();
477: 
478: 	// set up the page write info
479: 	state.stats_state = InitializeStatsState();
480: 	for (idx_t page_idx = 0; page_idx < state.page_info.size(); page_idx++) {
481: 		auto &page_info = state.page_info[page_idx];
482: 		if (page_info.row_count == 0) {
483: 			D_ASSERT(page_idx + 1 == state.page_info.size());
484: 			state.page_info.erase_at(page_idx);
485: 			break;
486: 		}
487: 		PageWriteInformation write_info;
488: 		// set up the header
489: 		auto &hdr = write_info.page_header;
490: 		hdr.compressed_page_size = 0;
491: 		hdr.uncompressed_page_size = 0;
492: 		hdr.type = PageType::DATA_PAGE;
493: 		hdr.__isset.data_page_header = true;
494: 
495: 		hdr.data_page_header.num_values = UnsafeNumericCast<int32_t>(page_info.row_count);
496: 		hdr.data_page_header.encoding = GetEncoding(state);
497: 		hdr.data_page_header.definition_level_encoding = Encoding::RLE;
498: 		hdr.data_page_header.repetition_level_encoding = Encoding::RLE;
499: 
500: 		write_info.temp_writer = make_uniq<MemoryStream>(
501: 		    MaxValue<idx_t>(NextPowerOfTwo(page_info.estimated_page_size), MemoryStream::DEFAULT_INITIAL_CAPACITY));
502: 		write_info.write_count = page_info.empty_count;
503: 		write_info.max_write_count = page_info.row_count;
504: 		write_info.page_state = InitializePageState(state);
505: 
506: 		write_info.compressed_size = 0;
507: 		write_info.compressed_data = nullptr;
508: 
509: 		state.write_info.push_back(std::move(write_info));
510: 	}
511: 
512: 	// start writing the first page
513: 	NextPage(state);
514: }
515: 
516: void BasicColumnWriter::WriteLevels(WriteStream &temp_writer, const unsafe_vector<uint16_t> &levels, idx_t max_value,
517:                                     idx_t offset, idx_t count) {
518: 	if (levels.empty() || count == 0) {
519: 		return;
520: 	}
521: 
522: 	// write the levels using the RLE-BP encoding
523: 	auto bit_width = RleBpDecoder::ComputeBitWidth((max_value));
524: 	RleBpEncoder rle_encoder(bit_width);
525: 
526: 	rle_encoder.BeginPrepare(levels[offset]);
527: 	for (idx_t i = offset + 1; i < offset + count; i++) {
528: 		rle_encoder.PrepareValue(levels[i]);
529: 	}
530: 	rle_encoder.FinishPrepare();
531: 
532: 	// start off by writing the byte count as a uint32_t
533: 	temp_writer.Write<uint32_t>(rle_encoder.GetByteCount());
534: 	rle_encoder.BeginWrite(temp_writer, levels[offset]);
535: 	for (idx_t i = offset + 1; i < offset + count; i++) {
536: 		rle_encoder.WriteValue(temp_writer, levels[i]);
537: 	}
538: 	rle_encoder.FinishWrite(temp_writer);
539: }
540: 
541: void BasicColumnWriter::NextPage(BasicColumnWriterState &state) {
542: 	if (state.current_page > 0) {
543: 		// need to flush the current page
544: 		FlushPage(state);
545: 	}
546: 	if (state.current_page >= state.write_info.size()) {
547: 		state.current_page = state.write_info.size() + 1;
548: 		return;
549: 	}
550: 	auto &page_info = state.page_info[state.current_page];
551: 	auto &write_info = state.write_info[state.current_page];
552: 	state.current_page++;
553: 
554: 	auto &temp_writer = *write_info.temp_writer;
555: 
556: 	// write the repetition levels
557: 	WriteLevels(temp_writer, state.repetition_levels, max_repeat, page_info.offset, page_info.row_count);
558: 
559: 	// write the definition levels
560: 	WriteLevels(temp_writer, state.definition_levels, max_define, page_info.offset, page_info.row_count);
561: }
562: 
563: void BasicColumnWriter::FlushPage(BasicColumnWriterState &state) {
564: 	D_ASSERT(state.current_page > 0);
565: 	if (state.current_page > state.write_info.size()) {
566: 		return;
567: 	}
568: 
569: 	// compress the page info
570: 	auto &write_info = state.write_info[state.current_page - 1];
571: 	auto &temp_writer = *write_info.temp_writer;
572: 	auto &hdr = write_info.page_header;
573: 
574: 	FlushPageState(temp_writer, write_info.page_state.get());
575: 
576: 	// now that we have finished writing the data we know the uncompressed size
577: 	if (temp_writer.GetPosition() > idx_t(NumericLimits<int32_t>::Maximum())) {
578: 		throw InternalException("Parquet writer: %d uncompressed page size out of range for type integer",
579: 		                        temp_writer.GetPosition());
580: 	}
581: 	hdr.uncompressed_page_size = UnsafeNumericCast<int32_t>(temp_writer.GetPosition());
582: 
583: 	// compress the data
584: 	CompressPage(temp_writer, write_info.compressed_size, write_info.compressed_data, write_info.compressed_buf);
585: 	hdr.compressed_page_size = UnsafeNumericCast<int32_t>(write_info.compressed_size);
586: 	D_ASSERT(hdr.uncompressed_page_size > 0);
587: 	D_ASSERT(hdr.compressed_page_size > 0);
588: 
589: 	if (write_info.compressed_buf) {
590: 		// if the data has been compressed, we no longer need the uncompressed data
591: 		D_ASSERT(write_info.compressed_buf.get() == write_info.compressed_data);
592: 		write_info.temp_writer.reset();
593: 	}
594: }
595: 
596: unique_ptr<ColumnWriterStatistics> BasicColumnWriter::InitializeStatsState() {
597: 	return make_uniq<ColumnWriterStatistics>();
598: }
599: 
600: idx_t BasicColumnWriter::GetRowSize(const Vector &vector, const idx_t index,
601:                                     const BasicColumnWriterState &state) const {
602: 	throw InternalException("GetRowSize unsupported for struct/list column writers");
603: }
604: 
605: void BasicColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count) {
606: 	auto &state = state_p.Cast<BasicColumnWriterState>();
607: 
608: 	idx_t remaining = count;
609: 	idx_t offset = 0;
610: 	while (remaining > 0) {
611: 		auto &write_info = state.write_info[state.current_page - 1];
612: 		if (!write_info.temp_writer) {
613: 			throw InternalException("Writes are not correctly aligned!?");
614: 		}
615: 		auto &temp_writer = *write_info.temp_writer;
616: 		idx_t write_count = MinValue<idx_t>(remaining, write_info.max_write_count - write_info.write_count);
617: 		D_ASSERT(write_count > 0);
618: 
619: 		WriteVector(temp_writer, state.stats_state.get(), write_info.page_state.get(), vector, offset,
620: 		            offset + write_count);
621: 
622: 		write_info.write_count += write_count;
623: 		if (write_info.write_count == write_info.max_write_count) {
624: 			NextPage(state);
625: 		}
626: 		offset += write_count;
627: 		remaining -= write_count;
628: 	}
629: }
630: 
631: void BasicColumnWriter::SetParquetStatistics(BasicColumnWriterState &state, duckdb_parquet::ColumnChunk &column_chunk) {
632: 	if (!state.stats_state) {
633: 		return;
634: 	}
635: 	if (max_repeat == 0) {
636: 		column_chunk.meta_data.statistics.null_count = NumericCast<int64_t>(state.null_count);
637: 		column_chunk.meta_data.statistics.__isset.null_count = true;
638: 		column_chunk.meta_data.__isset.statistics = true;
639: 	}
640: 	// set min/max/min_value/max_value
641: 	// this code is not going to win any beauty contests, but well
642: 	auto min = state.stats_state->GetMin();
643: 	if (!min.empty()) {
644: 		column_chunk.meta_data.statistics.min = std::move(min);
645: 		column_chunk.meta_data.statistics.__isset.min = true;
646: 		column_chunk.meta_data.__isset.statistics = true;
647: 	}
648: 	auto max = state.stats_state->GetMax();
649: 	if (!max.empty()) {
650: 		column_chunk.meta_data.statistics.max = std::move(max);
651: 		column_chunk.meta_data.statistics.__isset.max = true;
652: 		column_chunk.meta_data.__isset.statistics = true;
653: 	}
654: 	if (state.stats_state->HasStats()) {
655: 		column_chunk.meta_data.statistics.min_value = state.stats_state->GetMinValue();
656: 		column_chunk.meta_data.statistics.__isset.min_value = true;
657: 		column_chunk.meta_data.__isset.statistics = true;
658: 
659: 		column_chunk.meta_data.statistics.max_value = state.stats_state->GetMaxValue();
660: 		column_chunk.meta_data.statistics.__isset.max_value = true;
661: 		column_chunk.meta_data.__isset.statistics = true;
662: 	}
663: 	if (HasDictionary(state)) {
664: 		column_chunk.meta_data.statistics.distinct_count = UnsafeNumericCast<int64_t>(DictionarySize(state));
665: 		column_chunk.meta_data.statistics.__isset.distinct_count = true;
666: 		column_chunk.meta_data.__isset.statistics = true;
667: 	}
668: 	for (const auto &write_info : state.write_info) {
669: 		// only care about data page encodings, data_page_header.encoding is meaningless for dict
670: 		if (write_info.page_header.type != PageType::DATA_PAGE &&
671: 		    write_info.page_header.type != PageType::DATA_PAGE_V2) {
672: 			continue;
673: 		}
674: 		column_chunk.meta_data.encodings.push_back(write_info.page_header.data_page_header.encoding);
675: 	}
676: }
677: 
678: void BasicColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {
679: 	auto &state = state_p.Cast<BasicColumnWriterState>();
680: 	auto &column_chunk = state.row_group.columns[state.col_idx];
681: 
682: 	// flush the last page (if any remains)
683: 	FlushPage(state);
684: 
685: 	auto &column_writer = writer.GetWriter();
686: 	auto start_offset = column_writer.GetTotalWritten();
687: 	// flush the dictionary
688: 	if (HasDictionary(state)) {
689: 		column_chunk.meta_data.statistics.distinct_count = UnsafeNumericCast<int64_t>(DictionarySize(state));
690: 		column_chunk.meta_data.statistics.__isset.distinct_count = true;
691: 		column_chunk.meta_data.dictionary_page_offset = UnsafeNumericCast<int64_t>(column_writer.GetTotalWritten());
692: 		column_chunk.meta_data.__isset.dictionary_page_offset = true;
693: 		FlushDictionary(state, state.stats_state.get());
694: 	}
695: 
696: 	// record the start position of the pages for this column
697: 	column_chunk.meta_data.data_page_offset = 0;
698: 	SetParquetStatistics(state, column_chunk);
699: 
700: 	// write the individual pages to disk
701: 	idx_t total_uncompressed_size = 0;
702: 	for (auto &write_info : state.write_info) {
703: 		// set the data page offset whenever we see the *first* data page
704: 		if (column_chunk.meta_data.data_page_offset == 0 && (write_info.page_header.type == PageType::DATA_PAGE ||
705: 		                                                     write_info.page_header.type == PageType::DATA_PAGE_V2)) {
706: 			column_chunk.meta_data.data_page_offset = UnsafeNumericCast<int64_t>(column_writer.GetTotalWritten());
707: 			;
708: 		}
709: 		D_ASSERT(write_info.page_header.uncompressed_page_size > 0);
710: 		auto header_start_offset = column_writer.GetTotalWritten();
711: 		writer.Write(write_info.page_header);
712: 		// total uncompressed size in the column chunk includes the header size (!)
713: 		total_uncompressed_size += column_writer.GetTotalWritten() - header_start_offset;
714: 		total_uncompressed_size += write_info.page_header.uncompressed_page_size;
715: 		writer.WriteData(write_info.compressed_data, write_info.compressed_size);
716: 	}
717: 	column_chunk.meta_data.total_compressed_size =
718: 	    UnsafeNumericCast<int64_t>(column_writer.GetTotalWritten() - start_offset);
719: 	column_chunk.meta_data.total_uncompressed_size = UnsafeNumericCast<int64_t>(total_uncompressed_size);
720: 
721: 	if (state.bloom_filter) {
722: 		writer.BufferBloomFilter(state.col_idx, std::move(state.bloom_filter));
723: 	}
724: 	// which row group is this?
725: }
726: 
727: void BasicColumnWriter::FlushDictionary(BasicColumnWriterState &state, ColumnWriterStatistics *stats) {
728: 	throw InternalException("This page does not have a dictionary");
729: }
730: 
731: idx_t BasicColumnWriter::DictionarySize(BasicColumnWriterState &state) {
732: 	throw InternalException("This page does not have a dictionary");
733: }
734: 
735: void BasicColumnWriter::WriteDictionary(BasicColumnWriterState &state, unique_ptr<MemoryStream> temp_writer,
736:                                         idx_t row_count) {
737: 	D_ASSERT(temp_writer);
738: 	D_ASSERT(temp_writer->GetPosition() > 0);
739: 
740: 	// write the dictionary page header
741: 	PageWriteInformation write_info;
742: 	// set up the header
743: 	auto &hdr = write_info.page_header;
744: 	hdr.uncompressed_page_size = UnsafeNumericCast<int32_t>(temp_writer->GetPosition());
745: 	hdr.type = PageType::DICTIONARY_PAGE;
746: 	hdr.__isset.dictionary_page_header = true;
747: 
748: 	hdr.dictionary_page_header.encoding = Encoding::PLAIN;
749: 	hdr.dictionary_page_header.is_sorted = false;
750: 	hdr.dictionary_page_header.num_values = UnsafeNumericCast<int32_t>(row_count);
751: 
752: 	write_info.temp_writer = std::move(temp_writer);
753: 	write_info.write_count = 0;
754: 	write_info.max_write_count = 0;
755: 
756: 	// compress the contents of the dictionary page
757: 	CompressPage(*write_info.temp_writer, write_info.compressed_size, write_info.compressed_data,
758: 	             write_info.compressed_buf);
759: 	hdr.compressed_page_size = UnsafeNumericCast<int32_t>(write_info.compressed_size);
760: 
761: 	// insert the dictionary page as the first page to write for this column
762: 	state.write_info.insert(state.write_info.begin(), std::move(write_info));
763: }
764: 
765: //===--------------------------------------------------------------------===//
766: // Standard Column Writer
767: //===--------------------------------------------------------------------===//
768: template <class SRC, class T, class OP>
769: class NumericStatisticsState : public ColumnWriterStatistics {
770: public:
771: 	NumericStatisticsState() : min(NumericLimits<T>::Maximum()), max(NumericLimits<T>::Minimum()) {
772: 	}
773: 
774: 	T min;
775: 	T max;
776: 
777: public:
778: 	bool HasStats() override {
779: 		return min <= max;
780: 	}
781: 
782: 	string GetMin() override {
783: 		return NumericLimits<SRC>::IsSigned() ? GetMinValue() : string();
784: 	}
785: 	string GetMax() override {
786: 		return NumericLimits<SRC>::IsSigned() ? GetMaxValue() : string();
787: 	}
788: 	string GetMinValue() override {
789: 		return HasStats() ? string(char_ptr_cast(&min), sizeof(T)) : string();
790: 	}
791: 	string GetMaxValue() override {
792: 		return HasStats() ? string(char_ptr_cast(&max), sizeof(T)) : string();
793: 	}
794: };
795: 
796: struct BaseParquetOperator {
797: 
798: 	template <class SRC, class TGT>
799: 	static void WriteToStream(const TGT &input, WriteStream &ser) {
800: 		ser.WriteData(const_data_ptr_cast(&input), sizeof(TGT));
801: 	}
802: 
803: 	template <class SRC, class TGT>
804: 	static uint64_t XXHash64(const TGT &target_value) {
805: 		return duckdb_zstd::XXH64(&target_value, sizeof(target_value), 0);
806: 	}
807: 
808: 	template <class SRC, class TGT>
809: 	static unique_ptr<ColumnWriterStatistics> InitializeStats() {
810: 		return nullptr;
811: 	}
812: 
813: 	template <class SRC, class TGT>
814: 	static void HandleStats(ColumnWriterStatistics *stats, TGT target_value) {
815: 	}
816: };
817: 
818: struct ParquetCastOperator : public BaseParquetOperator {
819: 	template <class SRC, class TGT>
820: 	static TGT Operation(SRC input) {
821: 		return TGT(input);
822: 	}
823: 	template <class SRC, class TGT>
824: 	static unique_ptr<ColumnWriterStatistics> InitializeStats() {
825: 		return make_uniq<NumericStatisticsState<SRC, TGT, BaseParquetOperator>>();
826: 	}
827: 
828: 	template <class SRC, class TGT>
829: 	static void HandleStats(ColumnWriterStatistics *stats, TGT target_value) {
830: 		auto &numeric_stats = (NumericStatisticsState<SRC, TGT, BaseParquetOperator> &)*stats;
831: 		if (LessThan::Operation(target_value, numeric_stats.min)) {
832: 			numeric_stats.min = target_value;
833: 		}
834: 		if (GreaterThan::Operation(target_value, numeric_stats.max)) {
835: 			numeric_stats.max = target_value;
836: 		}
837: 	}
838: };
839: 
840: struct ParquetTimestampNSOperator : public ParquetCastOperator {
841: 	template <class SRC, class TGT>
842: 	static TGT Operation(SRC input) {
843: 		return TGT(input);
844: 	}
845: };
846: 
847: struct ParquetTimestampSOperator : public ParquetCastOperator {
848: 	template <class SRC, class TGT>
849: 	static TGT Operation(SRC input) {
850: 		return Timestamp::FromEpochSecondsPossiblyInfinite(input).value;
851: 	}
852: };
853: 
854: class StringStatisticsState : public ColumnWriterStatistics {
855: 	static constexpr const idx_t MAX_STRING_STATISTICS_SIZE = 10000;
856: 
857: public:
858: 	StringStatisticsState() : has_stats(false), values_too_big(false), min(), max() {
859: 	}
860: 
861: 	bool has_stats;
862: 	bool values_too_big;
863: 	string min;
864: 	string max;
865: 
866: public:
867: 	bool HasStats() override {
868: 		return has_stats;
869: 	}
870: 
871: 	void Update(const string_t &val) {
872: 		if (values_too_big) {
873: 			return;
874: 		}
875: 		auto str_len = val.GetSize();
876: 		if (str_len > MAX_STRING_STATISTICS_SIZE) {
877: 			// we avoid gathering stats when individual string values are too large
878: 			// this is because the statistics are copied into the Parquet file meta data in uncompressed format
879: 			// ideally we avoid placing several mega or giga-byte long strings there
880: 			// we put a threshold of 10KB, if we see strings that exceed this threshold we avoid gathering stats
881: 			values_too_big = true;
882: 			has_stats = false;
883: 			min = string();
884: 			max = string();
885: 			return;
886: 		}
887: 		if (!has_stats || LessThan::Operation(val, string_t(min))) {
888: 			min = val.GetString();
889: 		}
890: 		if (!has_stats || GreaterThan::Operation(val, string_t(max))) {
891: 			max = val.GetString();
892: 		}
893: 		has_stats = true;
894: 	}
895: 
896: 	string GetMin() override {
897: 		return GetMinValue();
898: 	}
899: 	string GetMax() override {
900: 		return GetMaxValue();
901: 	}
902: 	string GetMinValue() override {
903: 		return HasStats() ? min : string();
904: 	}
905: 	string GetMaxValue() override {
906: 		return HasStats() ? max : string();
907: 	}
908: };
909: 
910: struct ParquetStringOperator : public BaseParquetOperator {
911: 	template <class SRC, class TGT>
912: 	static TGT Operation(SRC input) {
913: 		return input;
914: 	}
915: 
916: 	template <class SRC, class TGT>
917: 	static unique_ptr<ColumnWriterStatistics> InitializeStats() {
918: 		return make_uniq<StringStatisticsState>();
919: 	}
920: 
921: 	template <class SRC, class TGT>
922: 	static void HandleStats(ColumnWriterStatistics *stats, TGT target_value) {
923: 		auto &string_stats = stats->Cast<StringStatisticsState>();
924: 		string_stats.Update(target_value);
925: 	}
926: 
927: 	template <class SRC, class TGT>
928: 	static void WriteToStream(const TGT &target_value, WriteStream &ser) {
929: 		ser.Write<uint32_t>(target_value.GetSize());
930: 		ser.WriteData(const_data_ptr_cast(target_value.GetData()), target_value.GetSize());
931: 	}
932: 
933: 	template <class SRC, class TGT>
934: 	static uint64_t XXHash64(const TGT &target_value) {
935: 		return duckdb_zstd::XXH64(target_value.GetData(), target_value.GetSize(), 0);
936: 	}
937: };
938: 
939: struct ParquetIntervalTargetType {
940: 	static constexpr const idx_t PARQUET_INTERVAL_SIZE = 12;
941: 	data_t bytes[PARQUET_INTERVAL_SIZE];
942: };
943: 
944: struct ParquetIntervalOperator : public BaseParquetOperator {
945: 	template <class SRC, class TGT>
946: 	static TGT Operation(SRC input) {
947: 
948: 		if (input.days < 0 || input.months < 0 || input.micros < 0) {
949: 			throw IOException("Parquet files do not support negative intervals");
950: 		}
951: 		TGT result;
952: 		Store<uint32_t>(input.months, result.bytes);
953: 		Store<uint32_t>(input.days, result.bytes + sizeof(uint32_t));
954: 		Store<uint32_t>(input.micros / 1000, result.bytes + sizeof(uint32_t) * 2);
955: 		return result;
956: 	}
957: 
958: 	template <class SRC, class TGT>
959: 	static void WriteToStream(const TGT &target_value, WriteStream &ser) {
960: 		ser.WriteData(target_value.bytes, ParquetIntervalTargetType::PARQUET_INTERVAL_SIZE);
961: 	}
962: 
963: 	template <class SRC, class TGT>
964: 	static uint64_t XXHash64(const TGT &target_value) {
965: 		return duckdb_zstd::XXH64(target_value.bytes, ParquetIntervalTargetType::PARQUET_INTERVAL_SIZE, 0);
966: 	}
967: };
968: 
969: struct ParquetUUIDTargetType {
970: 	static constexpr const idx_t PARQUET_UUID_SIZE = 16;
971: 	data_t bytes[PARQUET_UUID_SIZE];
972: };
973: 
974: struct ParquetUUIDOperator : public BaseParquetOperator {
975: 	template <class SRC, class TGT>
976: 	static TGT Operation(SRC input) {
977: 		TGT result;
978: 		uint64_t high_bytes = input.upper ^ (int64_t(1) << 63);
979: 		uint64_t low_bytes = input.lower;
980: 		for (idx_t i = 0; i < sizeof(uint64_t); i++) {
981: 			auto shift_count = (sizeof(uint64_t) - i - 1) * 8;
982: 			result.bytes[i] = (high_bytes >> shift_count) & 0xFF;
983: 		}
984: 		for (idx_t i = 0; i < sizeof(uint64_t); i++) {
985: 			auto shift_count = (sizeof(uint64_t) - i - 1) * 8;
986: 			result.bytes[sizeof(uint64_t) + i] = (low_bytes >> shift_count) & 0xFF;
987: 		}
988: 		return result;
989: 	}
990: 
991: 	template <class SRC, class TGT>
992: 	static void WriteToStream(const TGT &target_value, WriteStream &ser) {
993: 		ser.WriteData(target_value.bytes, ParquetUUIDTargetType::PARQUET_UUID_SIZE);
994: 	}
995: 
996: 	template <class SRC, class TGT>
997: 	static uint64_t XXHash64(const TGT &target_value) {
998: 		return duckdb_zstd::XXH64(target_value.bytes, ParquetUUIDTargetType::PARQUET_UUID_SIZE, 0);
999: 	}
1000: };
1001: 
1002: struct ParquetTimeTZOperator : public BaseParquetOperator {
1003: 	template <class SRC, class TGT>
1004: 	static TGT Operation(SRC input) {
1005: 		return input.time().micros;
1006: 	}
1007: };
1008: 
1009: struct ParquetHugeintOperator : public BaseParquetOperator {
1010: 	template <class SRC, class TGT>
1011: 	static TGT Operation(SRC input) {
1012: 		return Hugeint::Cast<double>(input);
1013: 	}
1014: 
1015: 	template <class SRC, class TGT>
1016: 	static unique_ptr<ColumnWriterStatistics> InitializeStats() {
1017: 		return make_uniq<ColumnWriterStatistics>();
1018: 	}
1019: 
1020: 	template <class SRC, class TGT>
1021: 	static void HandleStats(ColumnWriterStatistics *stats, TGT target_value) {
1022: 	}
1023: };
1024: 
1025: struct ParquetUhugeintOperator : public BaseParquetOperator {
1026: 	template <class SRC, class TGT>
1027: 	static TGT Operation(SRC input) {
1028: 		return Uhugeint::Cast<double>(input);
1029: 	}
1030: 
1031: 	template <class SRC, class TGT>
1032: 	static unique_ptr<ColumnWriterStatistics> InitializeStats() {
1033: 		return make_uniq<ColumnWriterStatistics>();
1034: 	}
1035: 
1036: 	template <class SRC, class TGT>
1037: 	static void HandleStats(ColumnWriterStatistics *stats, TGT target_value) {
1038: 	}
1039: };
1040: 
1041: template <class SRC, class TGT, class OP = ParquetCastOperator>
1042: static void TemplatedWritePlain(Vector &col, ColumnWriterStatistics *stats, const idx_t chunk_start,
1043:                                 const idx_t chunk_end, const ValidityMask &mask, WriteStream &ser) {
1044: 
1045: 	const auto *ptr = FlatVector::GetData<SRC>(col);
1046: 	for (idx_t r = chunk_start; r < chunk_end; r++) {
1047: 		if (!mask.RowIsValid(r)) {
1048: 			continue;
1049: 		}
1050: 		TGT target_value = OP::template Operation<SRC, TGT>(ptr[r]);
1051: 		OP::template HandleStats<SRC, TGT>(stats, target_value);
1052: 		OP::template WriteToStream<SRC, TGT>(target_value, ser);
1053: 	}
1054: }
1055: 
1056: template <class T>
1057: class StandardColumnWriterState : public BasicColumnWriterState {
1058: public:
1059: 	StandardColumnWriterState(duckdb_parquet::RowGroup &row_group, idx_t col_idx)
1060: 	    : BasicColumnWriterState(row_group, col_idx) {
1061: 	}
1062: 	~StandardColumnWriterState() override = default;
1063: 
1064: 	// analysis state for integer values for DELTA_BINARY_PACKED/DELTA_LENGTH_BYTE_ARRAY
1065: 	idx_t total_value_count = 0;
1066: 	idx_t total_string_size = 0;
1067: 
1068: 	unordered_map<T, uint32_t> dictionary;
1069: 	duckdb_parquet::Encoding::type encoding;
1070: };
1071: 
1072: template <class SRC, class TGT>
1073: class StandardWriterPageState : public ColumnWriterPageState {
1074: public:
1075: 	explicit StandardWriterPageState(const idx_t total_value_count, const idx_t total_string_size,
1076: 	                                 Encoding::type encoding_p, const unordered_map<SRC, uint32_t> &dictionary_p)
1077: 	    : encoding(encoding_p), dbp_initialized(false), dbp_encoder(total_value_count), dlba_initialized(false),
1078: 	      dlba_encoder(total_value_count, total_string_size), bss_encoder(total_value_count, sizeof(TGT)),
1079: 	      dictionary(dictionary_p), dict_written_value(false),
1080: 	      dict_bit_width(RleBpDecoder::ComputeBitWidth(dictionary.size())), dict_encoder(dict_bit_width) {
1081: 	}
1082: 	duckdb_parquet::Encoding::type encoding;
1083: 
1084: 	bool dbp_initialized;
1085: 	DbpEncoder dbp_encoder;
1086: 
1087: 	bool dlba_initialized;
1088: 	DlbaEncoder dlba_encoder;
1089: 
1090: 	BssEncoder bss_encoder;
1091: 
1092: 	const unordered_map<SRC, uint32_t> &dictionary;
1093: 	bool dict_written_value;
1094: 	uint32_t dict_bit_width;
1095: 	RleBpEncoder dict_encoder;
1096: };
1097: 
1098: namespace dbp_encoder {
1099: 
1100: template <class T>
1101: void BeginWrite(DbpEncoder &encoder, WriteStream &writer, const T &first_value) {
1102: 	throw InternalException("Can't write type to DELTA_BINARY_PACKED column");
1103: }
1104: 
1105: template <>
1106: void BeginWrite(DbpEncoder &encoder, WriteStream &writer, const int64_t &first_value) {
1107: 	encoder.BeginWrite(writer, first_value);
1108: }
1109: 
1110: template <>
1111: void BeginWrite(DbpEncoder &encoder, WriteStream &writer, const int32_t &first_value) {
1112: 	BeginWrite(encoder, writer, UnsafeNumericCast<int64_t>(first_value));
1113: }
1114: 
1115: template <>
1116: void BeginWrite(DbpEncoder &encoder, WriteStream &writer, const uint64_t &first_value) {
1117: 	encoder.BeginWrite(writer, UnsafeNumericCast<int64_t>(first_value));
1118: }
1119: 
1120: template <>
1121: void BeginWrite(DbpEncoder &encoder, WriteStream &writer, const uint32_t &first_value) {
1122: 	BeginWrite(encoder, writer, UnsafeNumericCast<int64_t>(first_value));
1123: }
1124: 
1125: template <class T>
1126: void WriteValue(DbpEncoder &encoder, WriteStream &writer, const T &value) {
1127: 	throw InternalException("Can't write type to DELTA_BINARY_PACKED column");
1128: }
1129: 
1130: template <>
1131: void WriteValue(DbpEncoder &encoder, WriteStream &writer, const int64_t &value) {
1132: 	encoder.WriteValue(writer, value);
1133: }
1134: 
1135: template <>
1136: void WriteValue(DbpEncoder &encoder, WriteStream &writer, const int32_t &value) {
1137: 	WriteValue(encoder, writer, UnsafeNumericCast<int64_t>(value));
1138: }
1139: 
1140: template <>
1141: void WriteValue(DbpEncoder &encoder, WriteStream &writer, const uint64_t &value) {
1142: 	encoder.WriteValue(writer, UnsafeNumericCast<int64_t>(value));
1143: }
1144: 
1145: template <>
1146: void WriteValue(DbpEncoder &encoder, WriteStream &writer, const uint32_t &value) {
1147: 	WriteValue(encoder, writer, UnsafeNumericCast<int64_t>(value));
1148: }
1149: 
1150: } // namespace dbp_encoder
1151: 
1152: namespace dlba_encoder {
1153: 
1154: template <class T>
1155: void BeginWrite(DlbaEncoder &encoder, WriteStream &writer, const T &first_value) {
1156: 	throw InternalException("Can't write type to DELTA_LENGTH_BYTE_ARRAY column");
1157: }
1158: 
1159: template <>
1160: void BeginWrite(DlbaEncoder &encoder, WriteStream &writer, const string_t &first_value) {
1161: 	encoder.BeginWrite(writer, first_value);
1162: }
1163: 
1164: template <class T>
1165: void WriteValue(DlbaEncoder &encoder, WriteStream &writer, const T &value) {
1166: 	throw InternalException("Can't write type to DELTA_LENGTH_BYTE_ARRAY column");
1167: }
1168: 
1169: template <>
1170: void WriteValue(DlbaEncoder &encoder, WriteStream &writer, const string_t &value) {
1171: 	encoder.WriteValue(writer, value);
1172: }
1173: 
1174: // helpers to get size from strings
1175: template <class SRC>
1176: static idx_t GetDlbaStringSize(const SRC &src_value) {
1177: 	return 0;
1178: }
1179: 
1180: template <>
1181: idx_t GetDlbaStringSize(const string_t &src_value) {
1182: 	return src_value.GetSize();
1183: }
1184: 
1185: } // namespace dlba_encoder
1186: 
1187: namespace bss_encoder {
1188: 
1189: template <class T>
1190: void WriteValue(BssEncoder &encoder, const T &value) {
1191: 	throw InternalException("Can't write type to BYTE_STREAM_SPLIT column");
1192: }
1193: 
1194: template <>
1195: void WriteValue(BssEncoder &encoder, const float &value) {
1196: 	encoder.WriteValue(value);
1197: }
1198: 
1199: template <>
1200: void WriteValue(BssEncoder &encoder, const double &value) {
1201: 	encoder.WriteValue(value);
1202: }
1203: 
1204: } // namespace bss_encoder
1205: 
1206: template <class SRC, class TGT, class OP = ParquetCastOperator>
1207: class StandardColumnWriter : public BasicColumnWriter {
1208: public:
1209: 	StandardColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, // NOLINT
1210: 	                     idx_t max_repeat, idx_t max_define, bool can_have_nulls)
1211: 	    : BasicColumnWriter(writer, schema_idx, std::move(schema_path_p), max_repeat, max_define, can_have_nulls) {
1212: 	}
1213: 	~StandardColumnWriter() override = default;
1214: 
1215: public:
1216: 	unique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::RowGroup &row_group) override {
1217: 		auto result = make_uniq<StandardColumnWriterState<SRC>>(row_group, row_group.columns.size());
1218: 		result->encoding = Encoding::RLE_DICTIONARY;
1219: 		RegisterToRowGroup(row_group);
1220: 		return std::move(result);
1221: 	}
1222: 
1223: 	unique_ptr<ColumnWriterPageState> InitializePageState(BasicColumnWriterState &state_p) override {
1224: 		auto &state = state_p.Cast<StandardColumnWriterState<SRC>>();
1225: 
1226: 		auto result = make_uniq<StandardWriterPageState<SRC, TGT>>(state.total_value_count, state.total_string_size,
1227: 		                                                           state.encoding, state.dictionary);
1228: 		return std::move(result);
1229: 	}
1230: 
1231: 	void FlushPageState(WriteStream &temp_writer, ColumnWriterPageState *state_p) override {
1232: 		auto &page_state = state_p->Cast<StandardWriterPageState<SRC, TGT>>();
1233: 		switch (page_state.encoding) {
1234: 		case Encoding::DELTA_BINARY_PACKED:
1235: 			if (!page_state.dbp_initialized) {
1236: 				dbp_encoder::BeginWrite<int64_t>(page_state.dbp_encoder, temp_writer, 0);
1237: 			}
1238: 			page_state.dbp_encoder.FinishWrite(temp_writer);
1239: 			break;
1240: 		case Encoding::RLE_DICTIONARY:
1241: 			D_ASSERT(page_state.dict_bit_width != 0);
1242: 			if (!page_state.dict_written_value) {
1243: 				// all values are null
1244: 				// just write the bit width
1245: 				temp_writer.Write<uint8_t>(page_state.dict_bit_width);
1246: 				return;
1247: 			}
1248: 			page_state.dict_encoder.FinishWrite(temp_writer);
1249: 			break;
1250: 		case Encoding::DELTA_LENGTH_BYTE_ARRAY:
1251: 			if (!page_state.dlba_initialized) {
1252: 				dlba_encoder::BeginWrite<string_t>(page_state.dlba_encoder, temp_writer, string_t(""));
1253: 			}
1254: 			page_state.dlba_encoder.FinishWrite(temp_writer);
1255: 			break;
1256: 		case Encoding::BYTE_STREAM_SPLIT:
1257: 			page_state.bss_encoder.FinishWrite(temp_writer);
1258: 			break;
1259: 		case Encoding::PLAIN:
1260: 			break;
1261: 		default:
1262: 			throw InternalException("Unknown encoding");
1263: 		}
1264: 	}
1265: 
1266: 	Encoding::type GetEncoding(BasicColumnWriterState &state_p) override {
1267: 		auto &state = state_p.Cast<StandardColumnWriterState<SRC>>();
1268: 		return state.encoding;
1269: 	}
1270: 
1271: 	bool HasAnalyze() override {
1272: 		return true;
1273: 	}
1274: 
1275: 	void Analyze(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) override {
1276: 		auto &state = state_p.Cast<StandardColumnWriterState<SRC>>();
1277: 
1278: 		auto data_ptr = FlatVector::GetData<SRC>(vector);
1279: 		idx_t vector_index = 0;
1280: 		uint32_t new_value_index = state.dictionary.size();
1281: 
1282: 		const bool check_parent_empty = parent && !parent->is_empty.empty();
1283: 		const idx_t parent_index = state.definition_levels.size();
1284: 
1285: 		const idx_t vcount =
1286: 		    check_parent_empty ? parent->definition_levels.size() - state.definition_levels.size() : count;
1287: 
1288: 		const auto &validity = FlatVector::Validity(vector);
1289: 
1290: 		for (idx_t i = 0; i < vcount; i++) {
1291: 			if (check_parent_empty && parent->is_empty[parent_index + i]) {
1292: 				continue;
1293: 			}
1294: 			if (validity.RowIsValid(vector_index)) {
1295: 				const auto &src_value = data_ptr[vector_index];
1296: 				if (state.dictionary.size() <= writer.DictionarySizeLimit()) {
1297: 					if (state.dictionary.find(src_value) == state.dictionary.end()) {
1298: 						state.dictionary[src_value] = new_value_index;
1299: 						new_value_index++;
1300: 					}
1301: 				}
1302: 				state.total_value_count++;
1303: 				state.total_string_size += dlba_encoder::GetDlbaStringSize(src_value);
1304: 			}
1305: 			vector_index++;
1306: 		}
1307: 	}
1308: 
1309: 	void FinalizeAnalyze(ColumnWriterState &state_p) override {
1310: 		const auto type = writer.GetType(schema_idx);
1311: 
1312: 		auto &state = state_p.Cast<StandardColumnWriterState<SRC>>();
1313: 		if (state.dictionary.size() == 0 || state.dictionary.size() > writer.DictionarySizeLimit()) {
1314: 			if (writer.GetParquetVersion() == ParquetVersion::V1) {
1315: 				// Can't do the cool stuff for V1
1316: 				state.encoding = Encoding::PLAIN;
1317: 			} else {
1318: 				// If we aren't doing dictionary encoding, these encodings are virtually always better than PLAIN
1319: 				switch (type) {
1320: 				case Type::type::INT32:
1321: 				case Type::type::INT64:
1322: 					state.encoding = Encoding::DELTA_BINARY_PACKED;
1323: 					break;
1324: 				case Type::type::BYTE_ARRAY:
1325: 					state.encoding = Encoding::DELTA_LENGTH_BYTE_ARRAY;
1326: 					break;
1327: 				case Type::type::FLOAT:
1328: 				case Type::type::DOUBLE:
1329: 					state.encoding = Encoding::BYTE_STREAM_SPLIT;
1330: 					break;
1331: 				default:
1332: 					state.encoding = Encoding::PLAIN;
1333: 				}
1334: 			}
1335: 			state.dictionary.clear();
1336: 		}
1337: 	}
1338: 
1339: 	unique_ptr<ColumnWriterStatistics> InitializeStatsState() override {
1340: 		return OP::template InitializeStats<SRC, TGT>();
1341: 	}
1342: 
1343: 	bool HasDictionary(BasicColumnWriterState &state_p) override {
1344: 		auto &state = state_p.Cast<StandardColumnWriterState<SRC>>();
1345: 		return state.encoding == Encoding::RLE_DICTIONARY;
1346: 	}
1347: 
1348: 	idx_t DictionarySize(BasicColumnWriterState &state_p) override {
1349: 		auto &state = state_p.Cast<StandardColumnWriterState<SRC>>();
1350: 		return state.dictionary.size();
1351: 	}
1352: 
1353: 	void WriteVector(WriteStream &temp_writer, ColumnWriterStatistics *stats, ColumnWriterPageState *page_state_p,
1354: 	                 Vector &input_column, idx_t chunk_start, idx_t chunk_end) override {
1355: 		auto &page_state = page_state_p->Cast<StandardWriterPageState<SRC, TGT>>();
1356: 
1357: 		const auto &mask = FlatVector::Validity(input_column);
1358: 		const auto *data_ptr = FlatVector::GetData<SRC>(input_column);
1359: 
1360: 		switch (page_state.encoding) {
1361: 		case Encoding::RLE_DICTIONARY: {
1362: 			for (idx_t r = chunk_start; r < chunk_end; r++) {
1363: 				if (!mask.RowIsValid(r)) {
1364: 					continue;
1365: 				}
1366: 				auto &src_val = data_ptr[r];
1367: 				auto value_index = page_state.dictionary.at(src_val);
1368: 				if (!page_state.dict_written_value) {
1369: 					// first value
1370: 					// write the bit-width as a one-byte entry
1371: 					temp_writer.Write<uint8_t>(page_state.dict_bit_width);
1372: 					// now begin writing the actual value
1373: 					page_state.dict_encoder.BeginWrite(temp_writer, value_index);
1374: 					page_state.dict_written_value = true;
1375: 				} else {
1376: 					page_state.dict_encoder.WriteValue(temp_writer, value_index);
1377: 				}
1378: 			}
1379: 			break;
1380: 		}
1381: 		case Encoding::DELTA_BINARY_PACKED: {
1382: 			idx_t r = chunk_start;
1383: 			if (!page_state.dbp_initialized) {
1384: 				// find first non-null value
1385: 				for (; r < chunk_end; r++) {
1386: 					if (!mask.RowIsValid(r)) {
1387: 						continue;
1388: 					}
1389: 					const TGT target_value = OP::template Operation<SRC, TGT>(data_ptr[r]);
1390: 					OP::template HandleStats<SRC, TGT>(stats, target_value);
1391: 					dbp_encoder::BeginWrite(page_state.dbp_encoder, temp_writer, target_value);
1392: 					page_state.dbp_initialized = true;
1393: 					r++; // skip over
1394: 					break;
1395: 				}
1396: 			}
1397: 
1398: 			for (; r < chunk_end; r++) {
1399: 				if (!mask.RowIsValid(r)) {
1400: 					continue;
1401: 				}
1402: 				const TGT target_value = OP::template Operation<SRC, TGT>(data_ptr[r]);
1403: 				OP::template HandleStats<SRC, TGT>(stats, target_value);
1404: 				dbp_encoder::WriteValue(page_state.dbp_encoder, temp_writer, target_value);
1405: 			}
1406: 			break;
1407: 		}
1408: 		case Encoding::DELTA_LENGTH_BYTE_ARRAY: {
1409: 			idx_t r = chunk_start;
1410: 			if (!page_state.dlba_initialized) {
1411: 				// find first non-null value
1412: 				for (; r < chunk_end; r++) {
1413: 					if (!mask.RowIsValid(r)) {
1414: 						continue;
1415: 					}
1416: 					const TGT target_value = OP::template Operation<SRC, TGT>(data_ptr[r]);
1417: 					OP::template HandleStats<SRC, TGT>(stats, target_value);
1418: 					dlba_encoder::BeginWrite(page_state.dlba_encoder, temp_writer, target_value);
1419: 					page_state.dlba_initialized = true;
1420: 					r++; // skip over
1421: 					break;
1422: 				}
1423: 			}
1424: 
1425: 			for (; r < chunk_end; r++) {
1426: 				if (!mask.RowIsValid(r)) {
1427: 					continue;
1428: 				}
1429: 				const TGT target_value = OP::template Operation<SRC, TGT>(data_ptr[r]);
1430: 				OP::template HandleStats<SRC, TGT>(stats, target_value);
1431: 				dlba_encoder::WriteValue(page_state.dlba_encoder, temp_writer, target_value);
1432: 			}
1433: 			break;
1434: 		}
1435: 		case Encoding::BYTE_STREAM_SPLIT: {
1436: 			for (idx_t r = chunk_start; r < chunk_end; r++) {
1437: 				if (!mask.RowIsValid(r)) {
1438: 					continue;
1439: 				}
1440: 				const TGT target_value = OP::template Operation<SRC, TGT>(data_ptr[r]);
1441: 				OP::template HandleStats<SRC, TGT>(stats, target_value);
1442: 				bss_encoder::WriteValue(page_state.bss_encoder, target_value);
1443: 			}
1444: 			break;
1445: 		}
1446: 		case Encoding::PLAIN: {
1447: 			D_ASSERT(page_state.encoding == Encoding::PLAIN);
1448: 			TemplatedWritePlain<SRC, TGT, OP>(input_column, stats, chunk_start, chunk_end, mask, temp_writer);
1449: 			break;
1450: 		}
1451: 		default:
1452: 			throw InternalException("Unknown encoding");
1453: 		}
1454: 	}
1455: 
1456: 	void FlushDictionary(BasicColumnWriterState &state_p, ColumnWriterStatistics *stats) override {
1457: 		auto &state = state_p.Cast<StandardColumnWriterState<SRC>>();
1458: 
1459: 		D_ASSERT(state.encoding == Encoding::RLE_DICTIONARY);
1460: 
1461: 		// first we need to sort the values in index order
1462: 		auto values = vector<SRC>(state.dictionary.size());
1463: 		for (const auto &entry : state.dictionary) {
1464: 			values[entry.second] = entry.first;
1465: 		}
1466: 
1467: 		state.bloom_filter =
1468: 		    make_uniq<ParquetBloomFilter>(state.dictionary.size(), writer.BloomFilterFalsePositiveRatio());
1469: 
1470: 		// first write the contents of the dictionary page to a temporary buffer
1471: 		auto temp_writer = make_uniq<MemoryStream>(MaxValue<idx_t>(
1472: 		    NextPowerOfTwo(state.dictionary.size() * sizeof(TGT)), MemoryStream::DEFAULT_INITIAL_CAPACITY));
1473: 		for (idx_t r = 0; r < values.size(); r++) {
1474: 			const TGT target_value = OP::template Operation<SRC, TGT>(values[r]);
1475: 			// update the statistics
1476: 			OP::template HandleStats<SRC, TGT>(stats, target_value);
1477: 			// update the bloom filter
1478: 			auto hash = OP::template XXHash64<SRC, TGT>(target_value);
1479: 			state.bloom_filter->FilterInsert(hash);
1480: 			// actually write the dictionary value
1481: 			OP::template WriteToStream<SRC, TGT>(target_value, *temp_writer);
1482: 		}
1483: 		// flush the dictionary page and add it to the to-be-written pages
1484: 		WriteDictionary(state, std::move(temp_writer), values.size());
1485: 		// bloom filter will be queued for writing in ParquetWriter::BufferBloomFilter one level up
1486: 	}
1487: 
1488: 	// TODO this now vastly over-estimates the page size
1489: 	idx_t GetRowSize(const Vector &vector, const idx_t index, const BasicColumnWriterState &state_p) const override {
1490: 		return sizeof(TGT);
1491: 	}
1492: };
1493: 
1494: //===--------------------------------------------------------------------===//
1495: // Boolean Column Writer
1496: //===--------------------------------------------------------------------===//
1497: class BooleanStatisticsState : public ColumnWriterStatistics {
1498: public:
1499: 	BooleanStatisticsState() : min(true), max(false) {
1500: 	}
1501: 
1502: 	bool min;
1503: 	bool max;
1504: 
1505: public:
1506: 	bool HasStats() override {
1507: 		return !(min && !max);
1508: 	}
1509: 
1510: 	string GetMin() override {
1511: 		return GetMinValue();
1512: 	}
1513: 	string GetMax() override {
1514: 		return GetMaxValue();
1515: 	}
1516: 	string GetMinValue() override {
1517: 		return HasStats() ? string(const_char_ptr_cast(&min), sizeof(bool)) : string();
1518: 	}
1519: 	string GetMaxValue() override {
1520: 		return HasStats() ? string(const_char_ptr_cast(&max), sizeof(bool)) : string();
1521: 	}
1522: };
1523: 
1524: class BooleanWriterPageState : public ColumnWriterPageState {
1525: public:
1526: 	uint8_t byte = 0;
1527: 	uint8_t byte_pos = 0;
1528: };
1529: 
1530: class BooleanColumnWriter : public BasicColumnWriter {
1531: public:
1532: 	BooleanColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,
1533: 	                    idx_t max_define, bool can_have_nulls)
1534: 	    : BasicColumnWriter(writer, schema_idx, std::move(schema_path_p), max_repeat, max_define, can_have_nulls) {
1535: 	}
1536: 	~BooleanColumnWriter() override = default;
1537: 
1538: public:
1539: 	unique_ptr<ColumnWriterStatistics> InitializeStatsState() override {
1540: 		return make_uniq<BooleanStatisticsState>();
1541: 	}
1542: 
1543: 	void WriteVector(WriteStream &temp_writer, ColumnWriterStatistics *stats_p, ColumnWriterPageState *state_p,
1544: 	                 Vector &input_column, idx_t chunk_start, idx_t chunk_end) override {
1545: 		auto &stats = stats_p->Cast<BooleanStatisticsState>();
1546: 		auto &state = state_p->Cast<BooleanWriterPageState>();
1547: 		auto &mask = FlatVector::Validity(input_column);
1548: 
1549: 		auto *ptr = FlatVector::GetData<bool>(input_column);
1550: 		for (idx_t r = chunk_start; r < chunk_end; r++) {
1551: 			if (mask.RowIsValid(r)) {
1552: 				// only encode if non-null
1553: 				if (ptr[r]) {
1554: 					stats.max = true;
1555: 					state.byte |= 1 << state.byte_pos;
1556: 				} else {
1557: 					stats.min = false;
1558: 				}
1559: 				state.byte_pos++;
1560: 
1561: 				if (state.byte_pos == 8) {
1562: 					temp_writer.Write<uint8_t>(state.byte);
1563: 					state.byte = 0;
1564: 					state.byte_pos = 0;
1565: 				}
1566: 			}
1567: 		}
1568: 	}
1569: 
1570: 	unique_ptr<ColumnWriterPageState> InitializePageState(BasicColumnWriterState &state) override {
1571: 		return make_uniq<BooleanWriterPageState>();
1572: 	}
1573: 
1574: 	void FlushPageState(WriteStream &temp_writer, ColumnWriterPageState *state_p) override {
1575: 		auto &state = state_p->Cast<BooleanWriterPageState>();
1576: 		if (state.byte_pos > 0) {
1577: 			temp_writer.Write<uint8_t>(state.byte);
1578: 			state.byte = 0;
1579: 			state.byte_pos = 0;
1580: 		}
1581: 	}
1582: 
1583: 	idx_t GetRowSize(const Vector &vector, const idx_t index, const BasicColumnWriterState &state) const override {
1584: 		return sizeof(bool);
1585: 	}
1586: };
1587: 
1588: //===--------------------------------------------------------------------===//
1589: // Decimal Column Writer
1590: //===--------------------------------------------------------------------===//
1591: static void WriteParquetDecimal(hugeint_t input, data_ptr_t result) {
1592: 	bool positive = input >= 0;
1593: 	// numbers are stored as two's complement so some muckery is required
1594: 	if (!positive) {
1595: 		input = NumericLimits<hugeint_t>::Maximum() + input + 1;
1596: 	}
1597: 	uint64_t high_bytes = uint64_t(input.upper);
1598: 	uint64_t low_bytes = input.lower;
1599: 
1600: 	for (idx_t i = 0; i < sizeof(uint64_t); i++) {
1601: 		auto shift_count = (sizeof(uint64_t) - i - 1) * 8;
1602: 		result[i] = (high_bytes >> shift_count) & 0xFF;
1603: 	}
1604: 	for (idx_t i = 0; i < sizeof(uint64_t); i++) {
1605: 		auto shift_count = (sizeof(uint64_t) - i - 1) * 8;
1606: 		result[sizeof(uint64_t) + i] = (low_bytes >> shift_count) & 0xFF;
1607: 	}
1608: 	if (!positive) {
1609: 		result[0] |= 0x80;
1610: 	}
1611: }
1612: 
1613: class FixedDecimalStatistics : public ColumnWriterStatistics {
1614: public:
1615: 	FixedDecimalStatistics() : min(NumericLimits<hugeint_t>::Maximum()), max(NumericLimits<hugeint_t>::Minimum()) {
1616: 	}
1617: 
1618: 	hugeint_t min;
1619: 	hugeint_t max;
1620: 
1621: public:
1622: 	string GetStats(hugeint_t &input) {
1623: 		data_t buffer[16];
1624: 		WriteParquetDecimal(input, buffer);
1625: 		return string(const_char_ptr_cast(buffer), 16);
1626: 	}
1627: 
1628: 	bool HasStats() override {
1629: 		return min <= max;
1630: 	}
1631: 
1632: 	void Update(hugeint_t &val) {
1633: 		if (LessThan::Operation(val, min)) {
1634: 			min = val;
1635: 		}
1636: 		if (GreaterThan::Operation(val, max)) {
1637: 			max = val;
1638: 		}
1639: 	}
1640: 
1641: 	string GetMin() override {
1642: 		return GetMinValue();
1643: 	}
1644: 	string GetMax() override {
1645: 		return GetMaxValue();
1646: 	}
1647: 	string GetMinValue() override {
1648: 		return HasStats() ? GetStats(min) : string();
1649: 	}
1650: 	string GetMaxValue() override {
1651: 		return HasStats() ? GetStats(max) : string();
1652: 	}
1653: };
1654: 
1655: class FixedDecimalColumnWriter : public BasicColumnWriter {
1656: public:
1657: 	FixedDecimalColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,
1658: 	                         idx_t max_define, bool can_have_nulls)
1659: 	    : BasicColumnWriter(writer, schema_idx, std::move(schema_path_p), max_repeat, max_define, can_have_nulls) {
1660: 	}
1661: 	~FixedDecimalColumnWriter() override = default;
1662: 
1663: public:
1664: 	unique_ptr<ColumnWriterStatistics> InitializeStatsState() override {
1665: 		return make_uniq<FixedDecimalStatistics>();
1666: 	}
1667: 
1668: 	void WriteVector(WriteStream &temp_writer, ColumnWriterStatistics *stats_p, ColumnWriterPageState *page_state,
1669: 	                 Vector &input_column, idx_t chunk_start, idx_t chunk_end) override {
1670: 		auto &mask = FlatVector::Validity(input_column);
1671: 		auto *ptr = FlatVector::GetData<hugeint_t>(input_column);
1672: 		auto &stats = stats_p->Cast<FixedDecimalStatistics>();
1673: 
1674: 		data_t temp_buffer[16];
1675: 		for (idx_t r = chunk_start; r < chunk_end; r++) {
1676: 			if (mask.RowIsValid(r)) {
1677: 				stats.Update(ptr[r]);
1678: 				WriteParquetDecimal(ptr[r], temp_buffer);
1679: 				temp_writer.WriteData(temp_buffer, 16);
1680: 			}
1681: 		}
1682: 	}
1683: 
1684: 	idx_t GetRowSize(const Vector &vector, const idx_t index, const BasicColumnWriterState &state) const override {
1685: 		return sizeof(hugeint_t);
1686: 	}
1687: };
1688: 
1689: //===--------------------------------------------------------------------===//
1690: // WKB Column Writer
1691: //===--------------------------------------------------------------------===//
1692: // Used to store the metadata for a WKB-encoded geometry column when writing
1693: // GeoParquet files.
1694: class WKBColumnWriterState final : public StandardColumnWriterState<string_t> {
1695: public:
1696: 	WKBColumnWriterState(ClientContext &context, duckdb_parquet::RowGroup &row_group, idx_t col_idx)
1697: 	    : StandardColumnWriterState(row_group, col_idx), geo_data(), geo_data_writer(context) {
1698: 	}
1699: 
1700: 	GeoParquetColumnMetadata geo_data;
1701: 	GeoParquetColumnMetadataWriter geo_data_writer;
1702: };
1703: 
1704: class WKBColumnWriter final : public StandardColumnWriter<string_t, string_t, ParquetStringOperator> {
1705: public:
1706: 	WKBColumnWriter(ClientContext &context_p, ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p,
1707: 	                idx_t max_repeat, idx_t max_define, bool can_have_nulls, string name)
1708: 	    : StandardColumnWriter(writer, schema_idx, std::move(schema_path_p), max_repeat, max_define, can_have_nulls),
1709: 	      column_name(std::move(name)), context(context_p) {
1710: 
1711: 		this->writer.GetGeoParquetData().RegisterGeometryColumn(column_name);
1712: 	}
1713: 
1714: 	unique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::RowGroup &row_group) override {
1715: 		auto result = make_uniq<WKBColumnWriterState>(context, row_group, row_group.columns.size());
1716: 		result->encoding = Encoding::RLE_DICTIONARY;
1717: 		RegisterToRowGroup(row_group);
1718: 		return std::move(result);
1719: 	}
1720: 
1721: 	void Write(ColumnWriterState &state, Vector &vector, idx_t count) override {
1722: 		StandardColumnWriter::Write(state, vector, count);
1723: 
1724: 		auto &geo_state = state.Cast<WKBColumnWriterState>();
1725: 		geo_state.geo_data_writer.Update(geo_state.geo_data, vector, count);
1726: 	}
1727: 
1728: 	void FinalizeWrite(ColumnWriterState &state) override {
1729: 		StandardColumnWriter::FinalizeWrite(state);
1730: 
1731: 		// Add the geodata object to the writer
1732: 		const auto &geo_state = state.Cast<WKBColumnWriterState>();
1733: 
1734: 		// Merge this state's geo column data with the writer's geo column data
1735: 		writer.GetGeoParquetData().FlushColumnMeta(column_name, geo_state.geo_data);
1736: 	}
1737: 
1738: private:
1739: 	string column_name;
1740: 	ClientContext &context;
1741: };
1742: 
1743: //===--------------------------------------------------------------------===//
1744: // Enum Column Writer
1745: //===--------------------------------------------------------------------===//
1746: class EnumWriterPageState : public ColumnWriterPageState {
1747: public:
1748: 	explicit EnumWriterPageState(uint32_t bit_width) : encoder(bit_width), written_value(false) {
1749: 	}
1750: 
1751: 	RleBpEncoder encoder;
1752: 	bool written_value;
1753: };
1754: 
1755: class EnumColumnWriter : public BasicColumnWriter {
1756: public:
1757: 	EnumColumnWriter(ParquetWriter &writer, LogicalType enum_type_p, idx_t schema_idx, vector<string> schema_path_p,
1758: 	                 idx_t max_repeat, idx_t max_define, bool can_have_nulls)
1759: 	    : BasicColumnWriter(writer, schema_idx, std::move(schema_path_p), max_repeat, max_define, can_have_nulls),
1760: 	      enum_type(std::move(enum_type_p)) {
1761: 		bit_width = RleBpDecoder::ComputeBitWidth(EnumType::GetSize(enum_type));
1762: 	}
1763: 	~EnumColumnWriter() override = default;
1764: 
1765: 	LogicalType enum_type;
1766: 	uint32_t bit_width;
1767: 
1768: public:
1769: 	unique_ptr<ColumnWriterStatistics> InitializeStatsState() override {
1770: 		return make_uniq<StringStatisticsState>();
1771: 	}
1772: 
1773: 	template <class T>
1774: 	void WriteEnumInternal(WriteStream &temp_writer, Vector &input_column, idx_t chunk_start, idx_t chunk_end,
1775: 	                       EnumWriterPageState &page_state) {
1776: 		auto &mask = FlatVector::Validity(input_column);
1777: 		auto *ptr = FlatVector::GetData<T>(input_column);
1778: 		for (idx_t r = chunk_start; r < chunk_end; r++) {
1779: 			if (mask.RowIsValid(r)) {
1780: 				if (!page_state.written_value) {
1781: 					// first value
1782: 					// write the bit-width as a one-byte entry
1783: 					temp_writer.Write<uint8_t>(bit_width);
1784: 					// now begin writing the actual value
1785: 					page_state.encoder.BeginWrite(temp_writer, ptr[r]);
1786: 					page_state.written_value = true;
1787: 				} else {
1788: 					page_state.encoder.WriteValue(temp_writer, ptr[r]);
1789: 				}
1790: 			}
1791: 		}
1792: 	}
1793: 
1794: 	void WriteVector(WriteStream &temp_writer, ColumnWriterStatistics *stats_p, ColumnWriterPageState *page_state_p,
1795: 	                 Vector &input_column, idx_t chunk_start, idx_t chunk_end) override {
1796: 		auto &page_state = page_state_p->Cast<EnumWriterPageState>();
1797: 		switch (enum_type.InternalType()) {
1798: 		case PhysicalType::UINT8:
1799: 			WriteEnumInternal<uint8_t>(temp_writer, input_column, chunk_start, chunk_end, page_state);
1800: 			break;
1801: 		case PhysicalType::UINT16:
1802: 			WriteEnumInternal<uint16_t>(temp_writer, input_column, chunk_start, chunk_end, page_state);
1803: 			break;
1804: 		case PhysicalType::UINT32:
1805: 			WriteEnumInternal<uint32_t>(temp_writer, input_column, chunk_start, chunk_end, page_state);
1806: 			break;
1807: 		default:
1808: 			throw InternalException("Unsupported internal enum type");
1809: 		}
1810: 	}
1811: 
1812: 	unique_ptr<ColumnWriterPageState> InitializePageState(BasicColumnWriterState &state) override {
1813: 		return make_uniq<EnumWriterPageState>(bit_width);
1814: 	}
1815: 
1816: 	void FlushPageState(WriteStream &temp_writer, ColumnWriterPageState *state_p) override {
1817: 		auto &page_state = state_p->Cast<EnumWriterPageState>();
1818: 		if (!page_state.written_value) {
1819: 			// all values are null
1820: 			// just write the bit width
1821: 			temp_writer.Write<uint8_t>(bit_width);
1822: 			return;
1823: 		}
1824: 		page_state.encoder.FinishWrite(temp_writer);
1825: 	}
1826: 
1827: 	duckdb_parquet::Encoding::type GetEncoding(BasicColumnWriterState &state) override {
1828: 		return Encoding::RLE_DICTIONARY;
1829: 	}
1830: 
1831: 	bool HasDictionary(BasicColumnWriterState &state) override {
1832: 		return true;
1833: 	}
1834: 
1835: 	idx_t DictionarySize(BasicColumnWriterState &state_p) override {
1836: 		return EnumType::GetSize(enum_type);
1837: 	}
1838: 
1839: 	void FlushDictionary(BasicColumnWriterState &state, ColumnWriterStatistics *stats_p) override {
1840: 		auto &stats = stats_p->Cast<StringStatisticsState>();
1841: 		// write the enum values to a dictionary page
1842: 		auto &enum_values = EnumType::GetValuesInsertOrder(enum_type);
1843: 		auto enum_count = EnumType::GetSize(enum_type);
1844: 		auto string_values = FlatVector::GetData<string_t>(enum_values);
1845: 		// first write the contents of the dictionary page to a temporary buffer
1846: 		auto temp_writer = make_uniq<MemoryStream>();
1847: 		for (idx_t r = 0; r < enum_count; r++) {
1848: 			D_ASSERT(!FlatVector::IsNull(enum_values, r));
1849: 			// update the statistics
1850: 			stats.Update(string_values[r]);
1851: 			// write this string value to the dictionary
1852: 			temp_writer->Write<uint32_t>(string_values[r].GetSize());
1853: 			temp_writer->WriteData(const_data_ptr_cast(string_values[r].GetData()), string_values[r].GetSize());
1854: 		}
1855: 		// flush the dictionary page and add it to the to-be-written pages
1856: 		WriteDictionary(state, std::move(temp_writer), enum_count);
1857: 	}
1858: 
1859: 	idx_t GetRowSize(const Vector &vector, const idx_t index, const BasicColumnWriterState &state) const override {
1860: 		return (bit_width + 7) / 8;
1861: 	}
1862: };
1863: 
1864: //===--------------------------------------------------------------------===//
1865: // Struct Column Writer
1866: //===--------------------------------------------------------------------===//
1867: class StructColumnWriter : public ColumnWriter {
1868: public:
1869: 	StructColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,
1870: 	                   idx_t max_define, vector<unique_ptr<ColumnWriter>> child_writers_p, bool can_have_nulls)
1871: 	    : ColumnWriter(writer, schema_idx, std::move(schema_path_p), max_repeat, max_define, can_have_nulls),
1872: 	      child_writers(std::move(child_writers_p)) {
1873: 	}
1874: 	~StructColumnWriter() override = default;
1875: 
1876: 	vector<unique_ptr<ColumnWriter>> child_writers;
1877: 
1878: public:
1879: 	unique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::RowGroup &row_group) override;
1880: 	bool HasAnalyze() override;
1881: 	void Analyze(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;
1882: 	void FinalizeAnalyze(ColumnWriterState &state) override;
1883: 	void Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;
1884: 
1885: 	void BeginWrite(ColumnWriterState &state) override;
1886: 	void Write(ColumnWriterState &state, Vector &vector, idx_t count) override;
1887: 	void FinalizeWrite(ColumnWriterState &state) override;
1888: };
1889: 
1890: class StructColumnWriterState : public ColumnWriterState {
1891: public:
1892: 	StructColumnWriterState(duckdb_parquet::RowGroup &row_group, idx_t col_idx)
1893: 	    : row_group(row_group), col_idx(col_idx) {
1894: 	}
1895: 	~StructColumnWriterState() override = default;
1896: 
1897: 	duckdb_parquet::RowGroup &row_group;
1898: 	idx_t col_idx;
1899: 	vector<unique_ptr<ColumnWriterState>> child_states;
1900: };
1901: 
1902: unique_ptr<ColumnWriterState> StructColumnWriter::InitializeWriteState(duckdb_parquet::RowGroup &row_group) {
1903: 	auto result = make_uniq<StructColumnWriterState>(row_group, row_group.columns.size());
1904: 
1905: 	result->child_states.reserve(child_writers.size());
1906: 	for (auto &child_writer : child_writers) {
1907: 		result->child_states.push_back(child_writer->InitializeWriteState(row_group));
1908: 	}
1909: 	return std::move(result);
1910: }
1911: 
1912: bool StructColumnWriter::HasAnalyze() {
1913: 	for (auto &child_writer : child_writers) {
1914: 		if (child_writer->HasAnalyze()) {
1915: 			return true;
1916: 		}
1917: 	}
1918: 	return false;
1919: }
1920: 
1921: void StructColumnWriter::Analyze(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
1922: 	auto &state = state_p.Cast<StructColumnWriterState>();
1923: 	auto &child_vectors = StructVector::GetEntries(vector);
1924: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
1925: 		// Need to check again. It might be that just one child needs it but the rest not
1926: 		if (child_writers[child_idx]->HasAnalyze()) {
1927: 			child_writers[child_idx]->Analyze(*state.child_states[child_idx], &state_p, *child_vectors[child_idx],
1928: 			                                  count);
1929: 		}
1930: 	}
1931: }
1932: 
1933: void StructColumnWriter::FinalizeAnalyze(ColumnWriterState &state_p) {
1934: 	auto &state = state_p.Cast<StructColumnWriterState>();
1935: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
1936: 		// Need to check again. It might be that just one child needs it but the rest not
1937: 		if (child_writers[child_idx]->HasAnalyze()) {
1938: 			child_writers[child_idx]->FinalizeAnalyze(*state.child_states[child_idx]);
1939: 		}
1940: 	}
1941: }
1942: 
1943: void StructColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
1944: 	auto &state = state_p.Cast<StructColumnWriterState>();
1945: 
1946: 	auto &validity = FlatVector::Validity(vector);
1947: 	if (parent) {
1948: 		// propagate empty entries from the parent
1949: 		while (state.is_empty.size() < parent->is_empty.size()) {
1950: 			state.is_empty.push_back(parent->is_empty[state.is_empty.size()]);
1951: 		}
1952: 	}
1953: 	HandleRepeatLevels(state_p, parent, count, max_repeat);
1954: 	HandleDefineLevels(state_p, parent, validity, count, PARQUET_DEFINE_VALID, max_define - 1);
1955: 	auto &child_vectors = StructVector::GetEntries(vector);
1956: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
1957: 		child_writers[child_idx]->Prepare(*state.child_states[child_idx], &state_p, *child_vectors[child_idx], count);
1958: 	}
1959: }
1960: 
1961: void StructColumnWriter::BeginWrite(ColumnWriterState &state_p) {
1962: 	auto &state = state_p.Cast<StructColumnWriterState>();
1963: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
1964: 		child_writers[child_idx]->BeginWrite(*state.child_states[child_idx]);
1965: 	}
1966: }
1967: 
1968: void StructColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count) {
1969: 	auto &state = state_p.Cast<StructColumnWriterState>();
1970: 	auto &child_vectors = StructVector::GetEntries(vector);
1971: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
1972: 		child_writers[child_idx]->Write(*state.child_states[child_idx], *child_vectors[child_idx], count);
1973: 	}
1974: }
1975: 
1976: void StructColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {
1977: 	auto &state = state_p.Cast<StructColumnWriterState>();
1978: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
1979: 		// we add the null count of the struct to the null count of the children
1980: 		state.child_states[child_idx]->null_count += state_p.null_count;
1981: 		child_writers[child_idx]->FinalizeWrite(*state.child_states[child_idx]);
1982: 	}
1983: }
1984: 
1985: //===--------------------------------------------------------------------===//
1986: // List Column Writer
1987: //===--------------------------------------------------------------------===//
1988: class ListColumnWriter : public ColumnWriter {
1989: public:
1990: 	ListColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,
1991: 	                 idx_t max_define, unique_ptr<ColumnWriter> child_writer_p, bool can_have_nulls)
1992: 	    : ColumnWriter(writer, schema_idx, std::move(schema_path_p), max_repeat, max_define, can_have_nulls),
1993: 	      child_writer(std::move(child_writer_p)) {
1994: 	}
1995: 	~ListColumnWriter() override = default;
1996: 
1997: 	unique_ptr<ColumnWriter> child_writer;
1998: 
1999: public:
2000: 	unique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::RowGroup &row_group) override;
2001: 	bool HasAnalyze() override;
2002: 	void Analyze(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;
2003: 	void FinalizeAnalyze(ColumnWriterState &state) override;
2004: 	void Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;
2005: 
2006: 	void BeginWrite(ColumnWriterState &state) override;
2007: 	void Write(ColumnWriterState &state, Vector &vector, idx_t count) override;
2008: 	void FinalizeWrite(ColumnWriterState &state) override;
2009: };
2010: 
2011: class ListColumnWriterState : public ColumnWriterState {
2012: public:
2013: 	ListColumnWriterState(duckdb_parquet::RowGroup &row_group, idx_t col_idx) : row_group(row_group), col_idx(col_idx) {
2014: 	}
2015: 	~ListColumnWriterState() override = default;
2016: 
2017: 	duckdb_parquet::RowGroup &row_group;
2018: 	idx_t col_idx;
2019: 	unique_ptr<ColumnWriterState> child_state;
2020: 	idx_t parent_index = 0;
2021: };
2022: 
2023: unique_ptr<ColumnWriterState> ListColumnWriter::InitializeWriteState(duckdb_parquet::RowGroup &row_group) {
2024: 	auto result = make_uniq<ListColumnWriterState>(row_group, row_group.columns.size());
2025: 	result->child_state = child_writer->InitializeWriteState(row_group);
2026: 	return std::move(result);
2027: }
2028: 
2029: bool ListColumnWriter::HasAnalyze() {
2030: 	return child_writer->HasAnalyze();
2031: }
2032: void ListColumnWriter::Analyze(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
2033: 	auto &state = state_p.Cast<ListColumnWriterState>();
2034: 	auto &list_child = ListVector::GetEntry(vector);
2035: 	auto list_count = ListVector::GetListSize(vector);
2036: 	child_writer->Analyze(*state.child_state, &state_p, list_child, list_count);
2037: }
2038: 
2039: void ListColumnWriter::FinalizeAnalyze(ColumnWriterState &state_p) {
2040: 	auto &state = state_p.Cast<ListColumnWriterState>();
2041: 	child_writer->FinalizeAnalyze(*state.child_state);
2042: }
2043: 
2044: idx_t GetConsecutiveChildList(Vector &list, Vector &result, idx_t offset, idx_t count) {
2045: 	// returns a consecutive child list that fully flattens and repeats all required elements
2046: 	auto &validity = FlatVector::Validity(list);
2047: 	auto list_entries = FlatVector::GetData<list_entry_t>(list);
2048: 	bool is_consecutive = true;
2049: 	idx_t total_length = 0;
2050: 	for (idx_t c = offset; c < offset + count; c++) {
2051: 		if (!validity.RowIsValid(c)) {
2052: 			continue;
2053: 		}
2054: 		if (list_entries[c].offset != total_length) {
2055: 			is_consecutive = false;
2056: 		}
2057: 		total_length += list_entries[c].length;
2058: 	}
2059: 	if (is_consecutive) {
2060: 		// already consecutive - leave it as-is
2061: 		return total_length;
2062: 	}
2063: 	SelectionVector sel(total_length);
2064: 	idx_t index = 0;
2065: 	for (idx_t c = offset; c < offset + count; c++) {
2066: 		if (!validity.RowIsValid(c)) {
2067: 			continue;
2068: 		}
2069: 		for (idx_t k = 0; k < list_entries[c].length; k++) {
2070: 			sel.set_index(index++, list_entries[c].offset + k);
2071: 		}
2072: 	}
2073: 	result.Slice(sel, total_length);
2074: 	result.Flatten(total_length);
2075: 	return total_length;
2076: }
2077: 
2078: void ListColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
2079: 	auto &state = state_p.Cast<ListColumnWriterState>();
2080: 
2081: 	auto list_data = FlatVector::GetData<list_entry_t>(vector);
2082: 	auto &validity = FlatVector::Validity(vector);
2083: 
2084: 	// write definition levels and repeats
2085: 	idx_t start = 0;
2086: 	idx_t vcount = parent ? parent->definition_levels.size() - state.parent_index : count;
2087: 	idx_t vector_index = 0;
2088: 	for (idx_t i = start; i < vcount; i++) {
2089: 		idx_t parent_index = state.parent_index + i;
2090: 		if (parent && !parent->is_empty.empty() && parent->is_empty[parent_index]) {
2091: 			state.definition_levels.push_back(parent->definition_levels[parent_index]);
2092: 			state.repetition_levels.push_back(parent->repetition_levels[parent_index]);
2093: 			state.is_empty.push_back(true);
2094: 			continue;
2095: 		}
2096: 		auto first_repeat_level =
2097: 		    parent && !parent->repetition_levels.empty() ? parent->repetition_levels[parent_index] : max_repeat;
2098: 		if (parent && parent->definition_levels[parent_index] != PARQUET_DEFINE_VALID) {
2099: 			state.definition_levels.push_back(parent->definition_levels[parent_index]);
2100: 			state.repetition_levels.push_back(first_repeat_level);
2101: 			state.is_empty.push_back(true);
2102: 		} else if (validity.RowIsValid(vector_index)) {
2103: 			// push the repetition levels
2104: 			if (list_data[vector_index].length == 0) {
2105: 				state.definition_levels.push_back(max_define);
2106: 				state.is_empty.push_back(true);
2107: 			} else {
2108: 				state.definition_levels.push_back(PARQUET_DEFINE_VALID);
2109: 				state.is_empty.push_back(false);
2110: 			}
2111: 			state.repetition_levels.push_back(first_repeat_level);
2112: 			for (idx_t k = 1; k < list_data[vector_index].length; k++) {
2113: 				state.repetition_levels.push_back(max_repeat + 1);
2114: 				state.definition_levels.push_back(PARQUET_DEFINE_VALID);
2115: 				state.is_empty.push_back(false);
2116: 			}
2117: 		} else {
2118: 			if (!can_have_nulls) {
2119: 				throw IOException("Parquet writer: map key column is not allowed to contain NULL values");
2120: 			}
2121: 			state.definition_levels.push_back(max_define - 1);
2122: 			state.repetition_levels.push_back(first_repeat_level);
2123: 			state.is_empty.push_back(true);
2124: 		}
2125: 		vector_index++;
2126: 	}
2127: 	state.parent_index += vcount;
2128: 
2129: 	auto &list_child = ListVector::GetEntry(vector);
2130: 	Vector child_list(list_child);
2131: 	auto child_length = GetConsecutiveChildList(vector, child_list, 0, count);
2132: 	child_writer->Prepare(*state.child_state, &state_p, child_list, child_length);
2133: }
2134: 
2135: void ListColumnWriter::BeginWrite(ColumnWriterState &state_p) {
2136: 	auto &state = state_p.Cast<ListColumnWriterState>();
2137: 	child_writer->BeginWrite(*state.child_state);
2138: }
2139: 
2140: void ListColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count) {
2141: 	auto &state = state_p.Cast<ListColumnWriterState>();
2142: 
2143: 	auto &list_child = ListVector::GetEntry(vector);
2144: 	Vector child_list(list_child);
2145: 	auto child_length = GetConsecutiveChildList(vector, child_list, 0, count);
2146: 	child_writer->Write(*state.child_state, child_list, child_length);
2147: }
2148: 
2149: void ListColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {
2150: 	auto &state = state_p.Cast<ListColumnWriterState>();
2151: 	child_writer->FinalizeWrite(*state.child_state);
2152: }
2153: 
2154: //===--------------------------------------------------------------------===//
2155: // Array Column Writer
2156: //===--------------------------------------------------------------------===//
2157: class ArrayColumnWriter : public ListColumnWriter {
2158: public:
2159: 	ArrayColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,
2160: 	                  idx_t max_define, unique_ptr<ColumnWriter> child_writer_p, bool can_have_nulls)
2161: 	    : ListColumnWriter(writer, schema_idx, std::move(schema_path_p), max_repeat, max_define,
2162: 	                       std::move(child_writer_p), can_have_nulls) {
2163: 	}
2164: 	~ArrayColumnWriter() override = default;
2165: 
2166: public:
2167: 	void Analyze(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;
2168: 	void Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;
2169: 	void Write(ColumnWriterState &state, Vector &vector, idx_t count) override;
2170: };
2171: 
2172: void ArrayColumnWriter::Analyze(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
2173: 	auto &state = state_p.Cast<ListColumnWriterState>();
2174: 	auto &array_child = ArrayVector::GetEntry(vector);
2175: 	auto array_size = ArrayType::GetSize(vector.GetType());
2176: 	child_writer->Analyze(*state.child_state, &state_p, array_child, array_size * count);
2177: }
2178: 
2179: void ArrayColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
2180: 	auto &state = state_p.Cast<ListColumnWriterState>();
2181: 
2182: 	auto array_size = ArrayType::GetSize(vector.GetType());
2183: 	auto &validity = FlatVector::Validity(vector);
2184: 
2185: 	// write definition levels and repeats
2186: 	// the main difference between this and ListColumnWriter::Prepare is that we need to make sure to write out
2187: 	// repetition levels and definitions for the child elements of the array even if the array itself is NULL.
2188: 	idx_t start = 0;
2189: 	idx_t vcount = parent ? parent->definition_levels.size() - state.parent_index : count;
2190: 	idx_t vector_index = 0;
2191: 	for (idx_t i = start; i < vcount; i++) {
2192: 		idx_t parent_index = state.parent_index + i;
2193: 		if (parent && !parent->is_empty.empty() && parent->is_empty[parent_index]) {
2194: 			state.definition_levels.push_back(parent->definition_levels[parent_index]);
2195: 			state.repetition_levels.push_back(parent->repetition_levels[parent_index]);
2196: 			state.is_empty.push_back(true);
2197: 			continue;
2198: 		}
2199: 		auto first_repeat_level =
2200: 		    parent && !parent->repetition_levels.empty() ? parent->repetition_levels[parent_index] : max_repeat;
2201: 		if (parent && parent->definition_levels[parent_index] != PARQUET_DEFINE_VALID) {
2202: 			state.definition_levels.push_back(parent->definition_levels[parent_index]);
2203: 			state.repetition_levels.push_back(first_repeat_level);
2204: 			state.is_empty.push_back(false);
2205: 			for (idx_t k = 1; k < array_size; k++) {
2206: 				state.repetition_levels.push_back(max_repeat + 1);
2207: 				state.definition_levels.push_back(parent->definition_levels[parent_index]);
2208: 				state.is_empty.push_back(false);
2209: 			}
2210: 		} else if (validity.RowIsValid(vector_index)) {
2211: 			// push the repetition levels
2212: 			state.definition_levels.push_back(PARQUET_DEFINE_VALID);
2213: 			state.is_empty.push_back(false);
2214: 
2215: 			state.repetition_levels.push_back(first_repeat_level);
2216: 			for (idx_t k = 1; k < array_size; k++) {
2217: 				state.repetition_levels.push_back(max_repeat + 1);
2218: 				state.definition_levels.push_back(PARQUET_DEFINE_VALID);
2219: 				state.is_empty.push_back(false);
2220: 			}
2221: 		} else {
2222: 			state.definition_levels.push_back(max_define - 1);
2223: 			state.repetition_levels.push_back(first_repeat_level);
2224: 			state.is_empty.push_back(false);
2225: 			for (idx_t k = 1; k < array_size; k++) {
2226: 				state.repetition_levels.push_back(max_repeat + 1);
2227: 				state.definition_levels.push_back(max_define - 1);
2228: 				state.is_empty.push_back(false);
2229: 			}
2230: 		}
2231: 		vector_index++;
2232: 	}
2233: 	state.parent_index += vcount;
2234: 
2235: 	auto &array_child = ArrayVector::GetEntry(vector);
2236: 	child_writer->Prepare(*state.child_state, &state_p, array_child, count * array_size);
2237: }
2238: 
2239: void ArrayColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count) {
2240: 	auto &state = state_p.Cast<ListColumnWriterState>();
2241: 	auto array_size = ArrayType::GetSize(vector.GetType());
2242: 	auto &array_child = ArrayVector::GetEntry(vector);
2243: 	child_writer->Write(*state.child_state, array_child, count * array_size);
2244: }
2245: 
2246: // special double/float class to deal with dictionary encoding and NaN equality
2247: struct double_na_equal {
2248: 	double_na_equal() : val(0) {
2249: 	}
2250: 	explicit double_na_equal(const double val_p) : val(val_p) {
2251: 	}
2252: 	// NOLINTNEXTLINE: allow implicit conversion to double
2253: 	operator double() const {
2254: 		return val;
2255: 	}
2256: 
2257: 	bool operator==(const double &right) const {
2258: 		if (std::isnan(val) && std::isnan(right)) {
2259: 			return true;
2260: 		}
2261: 		return val == right;
2262: 	}
2263: 	double val;
2264: };
2265: 
2266: struct float_na_equal {
2267: 	float_na_equal() : val(0) {
2268: 	}
2269: 	explicit float_na_equal(const float val_p) : val(val_p) {
2270: 	}
2271: 	// NOLINTNEXTLINE: allow implicit conversion to float
2272: 	operator float() const {
2273: 		return val;
2274: 	}
2275: 	bool operator==(const float &right) const {
2276: 		if (std::isnan(val) && std::isnan(right)) {
2277: 			return true;
2278: 		}
2279: 		return val == right;
2280: 	}
2281: 	float val;
2282: };
2283: 
2284: //===--------------------------------------------------------------------===//
2285: // Create Column Writer
2286: //===--------------------------------------------------------------------===//
2287: 
2288: unique_ptr<ColumnWriter> ColumnWriter::CreateWriterRecursive(ClientContext &context,
2289:                                                              vector<duckdb_parquet::SchemaElement> &schemas,
2290:                                                              ParquetWriter &writer, const LogicalType &type,
2291:                                                              const string &name, vector<string> schema_path,
2292:                                                              optional_ptr<const ChildFieldIDs> field_ids,
2293:                                                              idx_t max_repeat, idx_t max_define, bool can_have_nulls) {
2294: 	auto null_type = can_have_nulls ? FieldRepetitionType::OPTIONAL : FieldRepetitionType::REQUIRED;
2295: 	if (!can_have_nulls) {
2296: 		max_define--;
2297: 	}
2298: 	idx_t schema_idx = schemas.size();
2299: 
2300: 	optional_ptr<const FieldID> field_id;
2301: 	optional_ptr<const ChildFieldIDs> child_field_ids;
2302: 	if (field_ids) {
2303: 		auto field_id_it = field_ids->ids->find(name);
2304: 		if (field_id_it != field_ids->ids->end()) {
2305: 			field_id = &field_id_it->second;
2306: 			child_field_ids = &field_id->child_field_ids;
2307: 		}
2308: 	}
2309: 
2310: 	if (type.id() == LogicalTypeId::STRUCT || type.id() == LogicalTypeId::UNION) {
2311: 		auto &child_types = StructType::GetChildTypes(type);
2312: 		// set up the schema element for this struct
2313: 		duckdb_parquet::SchemaElement schema_element;
2314: 		schema_element.repetition_type = null_type;
2315: 		schema_element.num_children = UnsafeNumericCast<int32_t>(child_types.size());
2316: 		schema_element.__isset.num_children = true;
2317: 		schema_element.__isset.type = false;
2318: 		schema_element.__isset.repetition_type = true;
2319: 		schema_element.name = name;
2320: 		if (field_id && field_id->set) {
2321: 			schema_element.__isset.field_id = true;
2322: 			schema_element.field_id = field_id->field_id;
2323: 		}
2324: 		schemas.push_back(std::move(schema_element));
2325: 		schema_path.push_back(name);
2326: 
2327: 		// construct the child types recursively
2328: 		vector<unique_ptr<ColumnWriter>> child_writers;
2329: 		child_writers.reserve(child_types.size());
2330: 		for (auto &child_type : child_types) {
2331: 			child_writers.push_back(CreateWriterRecursive(context, schemas, writer, child_type.second, child_type.first,
2332: 			                                              schema_path, child_field_ids, max_repeat, max_define + 1));
2333: 		}
2334: 		return make_uniq<StructColumnWriter>(writer, schema_idx, std::move(schema_path), max_repeat, max_define,
2335: 		                                     std::move(child_writers), can_have_nulls);
2336: 	}
2337: 	if (type.id() == LogicalTypeId::LIST || type.id() == LogicalTypeId::ARRAY) {
2338: 		auto is_list = type.id() == LogicalTypeId::LIST;
2339: 		auto &child_type = is_list ? ListType::GetChildType(type) : ArrayType::GetChildType(type);
2340: 		// set up the two schema elements for the list
2341: 		// for some reason we only set the converted type in the OPTIONAL element
2342: 		// first an OPTIONAL element
2343: 		duckdb_parquet::SchemaElement optional_element;
2344: 		optional_element.repetition_type = null_type;
2345: 		optional_element.num_children = 1;
2346: 		optional_element.converted_type = ConvertedType::LIST;
2347: 		optional_element.__isset.num_children = true;
2348: 		optional_element.__isset.type = false;
2349: 		optional_element.__isset.repetition_type = true;
2350: 		optional_element.__isset.converted_type = true;
2351: 		optional_element.name = name;
2352: 		if (field_id && field_id->set) {
2353: 			optional_element.__isset.field_id = true;
2354: 			optional_element.field_id = field_id->field_id;
2355: 		}
2356: 		schemas.push_back(std::move(optional_element));
2357: 		schema_path.push_back(name);
2358: 
2359: 		// then a REPEATED element
2360: 		duckdb_parquet::SchemaElement repeated_element;
2361: 		repeated_element.repetition_type = FieldRepetitionType::REPEATED;
2362: 		repeated_element.num_children = 1;
2363: 		repeated_element.__isset.num_children = true;
2364: 		repeated_element.__isset.type = false;
2365: 		repeated_element.__isset.repetition_type = true;
2366: 		repeated_element.name = is_list ? "list" : "array";
2367: 		schemas.push_back(std::move(repeated_element));
2368: 		schema_path.emplace_back(is_list ? "list" : "array");
2369: 
2370: 		auto child_writer = CreateWriterRecursive(context, schemas, writer, child_type, "element", schema_path,
2371: 		                                          child_field_ids, max_repeat + 1, max_define + 2);
2372: 		if (is_list) {
2373: 			return make_uniq<ListColumnWriter>(writer, schema_idx, std::move(schema_path), max_repeat, max_define,
2374: 			                                   std::move(child_writer), can_have_nulls);
2375: 		} else {
2376: 			return make_uniq<ArrayColumnWriter>(writer, schema_idx, std::move(schema_path), max_repeat, max_define,
2377: 			                                    std::move(child_writer), can_have_nulls);
2378: 		}
2379: 	}
2380: 	if (type.id() == LogicalTypeId::MAP) {
2381: 		// map type
2382: 		// maps are stored as follows:
2383: 		// <map-repetition> group <name> (MAP) {
2384: 		// 	repeated group key_value {
2385: 		// 		required <key-type> key;
2386: 		// 		<value-repetition> <value-type> value;
2387: 		// 	}
2388: 		// }
2389: 		// top map element
2390: 		duckdb_parquet::SchemaElement top_element;
2391: 		top_element.repetition_type = null_type;
2392: 		top_element.num_children = 1;
2393: 		top_element.converted_type = ConvertedType::MAP;
2394: 		top_element.__isset.repetition_type = true;
2395: 		top_element.__isset.num_children = true;
2396: 		top_element.__isset.converted_type = true;
2397: 		top_element.__isset.type = false;
2398: 		top_element.name = name;
2399: 		if (field_id && field_id->set) {
2400: 			top_element.__isset.field_id = true;
2401: 			top_element.field_id = field_id->field_id;
2402: 		}
2403: 		schemas.push_back(std::move(top_element));
2404: 		schema_path.push_back(name);
2405: 
2406: 		// key_value element
2407: 		duckdb_parquet::SchemaElement kv_element;
2408: 		kv_element.repetition_type = FieldRepetitionType::REPEATED;
2409: 		kv_element.num_children = 2;
2410: 		kv_element.__isset.repetition_type = true;
2411: 		kv_element.__isset.num_children = true;
2412: 		kv_element.__isset.type = false;
2413: 		kv_element.name = "key_value";
2414: 		schemas.push_back(std::move(kv_element));
2415: 		schema_path.emplace_back("key_value");
2416: 
2417: 		// construct the child types recursively
2418: 		vector<LogicalType> kv_types {MapType::KeyType(type), MapType::ValueType(type)};
2419: 		vector<string> kv_names {"key", "value"};
2420: 		vector<unique_ptr<ColumnWriter>> child_writers;
2421: 		child_writers.reserve(2);
2422: 		for (idx_t i = 0; i < 2; i++) {
2423: 			// key needs to be marked as REQUIRED
2424: 			bool is_key = i == 0;
2425: 			auto child_writer = CreateWriterRecursive(context, schemas, writer, kv_types[i], kv_names[i], schema_path,
2426: 			                                          child_field_ids, max_repeat + 1, max_define + 2, !is_key);
2427: 
2428: 			child_writers.push_back(std::move(child_writer));
2429: 		}
2430: 		auto struct_writer = make_uniq<StructColumnWriter>(writer, schema_idx, schema_path, max_repeat, max_define,
2431: 		                                                   std::move(child_writers), can_have_nulls);
2432: 		return make_uniq<ListColumnWriter>(writer, schema_idx, schema_path, max_repeat, max_define,
2433: 		                                   std::move(struct_writer), can_have_nulls);
2434: 	}
2435: 	duckdb_parquet::SchemaElement schema_element;
2436: 	schema_element.type = ParquetWriter::DuckDBTypeToParquetType(type);
2437: 	schema_element.repetition_type = null_type;
2438: 	schema_element.__isset.num_children = false;
2439: 	schema_element.__isset.type = true;
2440: 	schema_element.__isset.repetition_type = true;
2441: 	schema_element.name = name;
2442: 	if (field_id && field_id->set) {
2443: 		schema_element.__isset.field_id = true;
2444: 		schema_element.field_id = field_id->field_id;
2445: 	}
2446: 	ParquetWriter::SetSchemaProperties(type, schema_element);
2447: 	schemas.push_back(std::move(schema_element));
2448: 	schema_path.push_back(name);
2449: 	if (type.id() == LogicalTypeId::BLOB && type.GetAlias() == "WKB_BLOB" &&
2450: 	    GeoParquetFileMetadata::IsGeoParquetConversionEnabled(context)) {
2451: 		return make_uniq<WKBColumnWriter>(context, writer, schema_idx, std::move(schema_path), max_repeat, max_define,
2452: 		                                  can_have_nulls, name);
2453: 	}
2454: 
2455: 	switch (type.id()) {
2456: 	case LogicalTypeId::BOOLEAN:
2457: 		return make_uniq<BooleanColumnWriter>(writer, schema_idx, std::move(schema_path), max_repeat, max_define,
2458: 		                                      can_have_nulls);
2459: 	case LogicalTypeId::TINYINT:
2460: 		return make_uniq<StandardColumnWriter<int8_t, int32_t>>(writer, schema_idx, std::move(schema_path), max_repeat,
2461: 		                                                        max_define, can_have_nulls);
2462: 	case LogicalTypeId::SMALLINT:
2463: 		return make_uniq<StandardColumnWriter<int16_t, int32_t>>(writer, schema_idx, std::move(schema_path), max_repeat,
2464: 		                                                         max_define, can_have_nulls);
2465: 	case LogicalTypeId::INTEGER:
2466: 	case LogicalTypeId::DATE:
2467: 		return make_uniq<StandardColumnWriter<int32_t, int32_t>>(writer, schema_idx, std::move(schema_path), max_repeat,
2468: 		                                                         max_define, can_have_nulls);
2469: 	case LogicalTypeId::BIGINT:
2470: 	case LogicalTypeId::TIME:
2471: 	case LogicalTypeId::TIMESTAMP:
2472: 	case LogicalTypeId::TIMESTAMP_TZ:
2473: 	case LogicalTypeId::TIMESTAMP_MS:
2474: 		return make_uniq<StandardColumnWriter<int64_t, int64_t>>(writer, schema_idx, std::move(schema_path), max_repeat,
2475: 		                                                         max_define, can_have_nulls);
2476: 	case LogicalTypeId::TIME_TZ:
2477: 		return make_uniq<StandardColumnWriter<dtime_tz_t, int64_t, ParquetTimeTZOperator>>(
2478: 		    writer, schema_idx, std::move(schema_path), max_repeat, max_define, can_have_nulls);
2479: 	case LogicalTypeId::HUGEINT:
2480: 		return make_uniq<StandardColumnWriter<hugeint_t, double, ParquetHugeintOperator>>(
2481: 		    writer, schema_idx, std::move(schema_path), max_repeat, max_define, can_have_nulls);
2482: 	case LogicalTypeId::UHUGEINT:
2483: 		return make_uniq<StandardColumnWriter<uhugeint_t, double, ParquetUhugeintOperator>>(
2484: 		    writer, schema_idx, std::move(schema_path), max_repeat, max_define, can_have_nulls);
2485: 	case LogicalTypeId::TIMESTAMP_NS:
2486: 		return make_uniq<StandardColumnWriter<int64_t, int64_t, ParquetTimestampNSOperator>>(
2487: 		    writer, schema_idx, std::move(schema_path), max_repeat, max_define, can_have_nulls);
2488: 	case LogicalTypeId::TIMESTAMP_SEC:
2489: 		return make_uniq<StandardColumnWriter<int64_t, int64_t, ParquetTimestampSOperator>>(
2490: 		    writer, schema_idx, std::move(schema_path), max_repeat, max_define, can_have_nulls);
2491: 	case LogicalTypeId::UTINYINT:
2492: 		return make_uniq<StandardColumnWriter<uint8_t, int32_t>>(writer, schema_idx, std::move(schema_path), max_repeat,
2493: 		                                                         max_define, can_have_nulls);
2494: 	case LogicalTypeId::USMALLINT:
2495: 		return make_uniq<StandardColumnWriter<uint16_t, int32_t>>(writer, schema_idx, std::move(schema_path),
2496: 		                                                          max_repeat, max_define, can_have_nulls);
2497: 	case LogicalTypeId::UINTEGER:
2498: 		return make_uniq<StandardColumnWriter<uint32_t, uint32_t>>(writer, schema_idx, std::move(schema_path),
2499: 		                                                           max_repeat, max_define, can_have_nulls);
2500: 	case LogicalTypeId::UBIGINT:
2501: 		return make_uniq<StandardColumnWriter<uint64_t, uint64_t>>(writer, schema_idx, std::move(schema_path),
2502: 		                                                           max_repeat, max_define, can_have_nulls);
2503: 	case LogicalTypeId::FLOAT:
2504: 		return make_uniq<StandardColumnWriter<float_na_equal, float>>(writer, schema_idx, std::move(schema_path),
2505: 		                                                              max_repeat, max_define, can_have_nulls);
2506: 	case LogicalTypeId::DOUBLE:
2507: 		return make_uniq<StandardColumnWriter<double_na_equal, double>>(writer, schema_idx, std::move(schema_path),
2508: 		                                                                max_repeat, max_define, can_have_nulls);
2509: 	case LogicalTypeId::DECIMAL:
2510: 		switch (type.InternalType()) {
2511: 		case PhysicalType::INT16:
2512: 			return make_uniq<StandardColumnWriter<int16_t, int32_t>>(writer, schema_idx, std::move(schema_path),
2513: 			                                                         max_repeat, max_define, can_have_nulls);
2514: 		case PhysicalType::INT32:
2515: 			return make_uniq<StandardColumnWriter<int32_t, int32_t>>(writer, schema_idx, std::move(schema_path),
2516: 			                                                         max_repeat, max_define, can_have_nulls);
2517: 		case PhysicalType::INT64:
2518: 			return make_uniq<StandardColumnWriter<int64_t, int64_t>>(writer, schema_idx, std::move(schema_path),
2519: 			                                                         max_repeat, max_define, can_have_nulls);
2520: 		default:
2521: 			return make_uniq<FixedDecimalColumnWriter>(writer, schema_idx, std::move(schema_path), max_repeat,
2522: 			                                           max_define, can_have_nulls);
2523: 		}
2524: 	case LogicalTypeId::BLOB:
2525: 	case LogicalTypeId::VARCHAR:
2526: 		return make_uniq<StandardColumnWriter<string_t, string_t, ParquetStringOperator>>(
2527: 		    writer, schema_idx, std::move(schema_path), max_repeat, max_define, can_have_nulls);
2528: 	case LogicalTypeId::UUID:
2529: 		return make_uniq<StandardColumnWriter<hugeint_t, ParquetUUIDTargetType, ParquetUUIDOperator>>(
2530: 		    writer, schema_idx, std::move(schema_path), max_repeat, max_define, can_have_nulls);
2531: 	case LogicalTypeId::INTERVAL:
2532: 		return make_uniq<StandardColumnWriter<interval_t, ParquetIntervalTargetType, ParquetIntervalOperator>>(
2533: 		    writer, schema_idx, std::move(schema_path), max_repeat, max_define, can_have_nulls);
2534: 	case LogicalTypeId::ENUM:
2535: 		return make_uniq<EnumColumnWriter>(writer, type, schema_idx, std::move(schema_path), max_repeat, max_define,
2536: 		                                   can_have_nulls);
2537: 	default:
2538: 		throw InternalException("Unsupported type \"%s\" in Parquet writer", type.ToString());
2539: 	}
2540: }
2541: 
2542: template <>
2543: struct NumericLimits<float_na_equal> {
2544: 	static constexpr float Minimum() {
2545: 		return std::numeric_limits<float>::lowest();
2546: 	};
2547: 	static constexpr float Maximum() {
2548: 		return std::numeric_limits<float>::max();
2549: 	};
2550: 	static constexpr bool IsSigned() {
2551: 		return std::is_signed<float>::value;
2552: 	}
2553: 	static constexpr bool IsIntegral() {
2554: 		return std::is_integral<float>::value;
2555: 	}
2556: };
2557: 
2558: template <>
2559: struct NumericLimits<double_na_equal> {
2560: 	static constexpr double Minimum() {
2561: 		return std::numeric_limits<double>::lowest();
2562: 	};
2563: 	static constexpr double Maximum() {
2564: 		return std::numeric_limits<double>::max();
2565: 	};
2566: 	static constexpr bool IsSigned() {
2567: 		return std::is_signed<double>::value;
2568: 	}
2569: 	static constexpr bool IsIntegral() {
2570: 		return std::is_integral<double>::value;
2571: 	}
2572: };
2573: 
2574: } // namespace duckdb
2575: 
2576: namespace std {
2577: template <>
2578: struct hash<duckdb::ParquetIntervalTargetType> {
2579: 	size_t operator()(const duckdb::ParquetIntervalTargetType &val) const {
2580: 		return duckdb::Hash(duckdb::const_char_ptr_cast(val.bytes),
2581: 		                    duckdb::ParquetIntervalTargetType::PARQUET_INTERVAL_SIZE);
2582: 	}
2583: };
2584: 
2585: template <>
2586: struct hash<duckdb::ParquetUUIDTargetType> {
2587: 	size_t operator()(const duckdb::ParquetUUIDTargetType &val) const {
2588: 		return duckdb::Hash(duckdb::const_char_ptr_cast(val.bytes), duckdb::ParquetUUIDTargetType::PARQUET_UUID_SIZE);
2589: 	}
2590: };
2591: 
2592: template <>
2593: struct hash<duckdb::float_na_equal> {
2594: 	size_t operator()(const duckdb::float_na_equal &val) const {
2595: 		if (std::isnan(val.val)) {
2596: 			return duckdb::Hash<float>(std::numeric_limits<float>::quiet_NaN());
2597: 		}
2598: 		return duckdb::Hash<float>(val.val);
2599: 	}
2600: };
2601: 
2602: template <>
2603: struct hash<duckdb::double_na_equal> {
2604: 	inline size_t operator()(const duckdb::double_na_equal &val) const {
2605: 		if (std::isnan(val.val)) {
2606: 			return duckdb::Hash<double>(std::numeric_limits<double>::quiet_NaN());
2607: 		}
2608: 		return duckdb::Hash<double>(val.val);
2609: 	}
2610: };
2611: } // namespace std
[end of extension/parquet/column_writer.cpp]
[start of extension/parquet/parquet_writer.cpp]
1: #include "parquet_writer.hpp"
2: 
3: #include "duckdb.hpp"
4: #include "mbedtls_wrapper.hpp"
5: #include "parquet_crypto.hpp"
6: #include "parquet_timestamp.hpp"
7: #include "resizable_buffer.hpp"
8: 
9: #ifndef DUCKDB_AMALGAMATION
10: #include "duckdb/common/file_system.hpp"
11: #include "duckdb/common/serializer/buffered_file_writer.hpp"
12: #include "duckdb/common/serializer/deserializer.hpp"
13: #include "duckdb/common/serializer/serializer.hpp"
14: #include "duckdb/common/serializer/write_stream.hpp"
15: #include "duckdb/common/string_util.hpp"
16: #include "duckdb/function/table_function.hpp"
17: #include "duckdb/main/client_context.hpp"
18: #include "duckdb/main/connection.hpp"
19: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
20: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
21: #endif
22: 
23: namespace duckdb {
24: 
25: using namespace duckdb_apache::thrift;            // NOLINT
26: using namespace duckdb_apache::thrift::protocol;  // NOLINT
27: using namespace duckdb_apache::thrift::transport; // NOLINT
28: 
29: using duckdb_parquet::CompressionCodec;
30: using duckdb_parquet::ConvertedType;
31: using duckdb_parquet::Encoding;
32: using duckdb_parquet::FieldRepetitionType;
33: using duckdb_parquet::FileCryptoMetaData;
34: using duckdb_parquet::FileMetaData;
35: using duckdb_parquet::PageHeader;
36: using duckdb_parquet::PageType;
37: using ParquetRowGroup = duckdb_parquet::RowGroup;
38: using duckdb_parquet::Type;
39: 
40: ChildFieldIDs::ChildFieldIDs() : ids(make_uniq<case_insensitive_map_t<FieldID>>()) {
41: }
42: 
43: ChildFieldIDs ChildFieldIDs::Copy() const {
44: 	ChildFieldIDs result;
45: 	for (const auto &id : *ids) {
46: 		result.ids->emplace(id.first, id.second.Copy());
47: 	}
48: 	return result;
49: }
50: 
51: FieldID::FieldID() : set(false) {
52: }
53: 
54: FieldID::FieldID(int32_t field_id_p) : set(true), field_id(field_id_p) {
55: }
56: 
57: FieldID FieldID::Copy() const {
58: 	auto result = set ? FieldID(field_id) : FieldID();
59: 	result.child_field_ids = child_field_ids.Copy();
60: 	return result;
61: }
62: 
63: class MyTransport : public TTransport {
64: public:
65: 	explicit MyTransport(WriteStream &serializer) : serializer(serializer) {
66: 	}
67: 
68: 	bool isOpen() const override {
69: 		return true;
70: 	}
71: 
72: 	void open() override {
73: 	}
74: 
75: 	void close() override {
76: 	}
77: 
78: 	void write_virt(const uint8_t *buf, uint32_t len) override {
79: 		serializer.WriteData(const_data_ptr_cast(buf), len);
80: 	}
81: 
82: private:
83: 	WriteStream &serializer;
84: };
85: 
86: bool ParquetWriter::TryGetParquetType(const LogicalType &duckdb_type, optional_ptr<Type::type> parquet_type_ptr) {
87: 	Type::type parquet_type;
88: 	switch (duckdb_type.id()) {
89: 	case LogicalTypeId::BOOLEAN:
90: 		parquet_type = Type::BOOLEAN;
91: 		break;
92: 	case LogicalTypeId::TINYINT:
93: 	case LogicalTypeId::SMALLINT:
94: 	case LogicalTypeId::INTEGER:
95: 	case LogicalTypeId::DATE:
96: 		parquet_type = Type::INT32;
97: 		break;
98: 	case LogicalTypeId::BIGINT:
99: 		parquet_type = Type::INT64;
100: 		break;
101: 	case LogicalTypeId::FLOAT:
102: 		parquet_type = Type::FLOAT;
103: 		break;
104: 	case LogicalTypeId::DOUBLE:
105: 		parquet_type = Type::DOUBLE;
106: 		break;
107: 	case LogicalTypeId::UHUGEINT:
108: 	case LogicalTypeId::HUGEINT:
109: 		parquet_type = Type::DOUBLE;
110: 		break;
111: 	case LogicalTypeId::ENUM:
112: 	case LogicalTypeId::BLOB:
113: 	case LogicalTypeId::VARCHAR:
114: 		parquet_type = Type::BYTE_ARRAY;
115: 		break;
116: 	case LogicalTypeId::TIME:
117: 	case LogicalTypeId::TIME_TZ:
118: 	case LogicalTypeId::TIMESTAMP:
119: 	case LogicalTypeId::TIMESTAMP_TZ:
120: 	case LogicalTypeId::TIMESTAMP_MS:
121: 	case LogicalTypeId::TIMESTAMP_NS:
122: 	case LogicalTypeId::TIMESTAMP_SEC:
123: 		parquet_type = Type::INT64;
124: 		break;
125: 	case LogicalTypeId::UTINYINT:
126: 	case LogicalTypeId::USMALLINT:
127: 	case LogicalTypeId::UINTEGER:
128: 		parquet_type = Type::INT32;
129: 		break;
130: 	case LogicalTypeId::UBIGINT:
131: 		parquet_type = Type::INT64;
132: 		break;
133: 	case LogicalTypeId::INTERVAL:
134: 	case LogicalTypeId::UUID:
135: 		parquet_type = Type::FIXED_LEN_BYTE_ARRAY;
136: 		break;
137: 	case LogicalTypeId::DECIMAL:
138: 		switch (duckdb_type.InternalType()) {
139: 		case PhysicalType::INT16:
140: 		case PhysicalType::INT32:
141: 			parquet_type = Type::INT32;
142: 			break;
143: 		case PhysicalType::INT64:
144: 			parquet_type = Type::INT64;
145: 			break;
146: 		case PhysicalType::INT128:
147: 			parquet_type = Type::FIXED_LEN_BYTE_ARRAY;
148: 			break;
149: 		default:
150: 			throw InternalException("Unsupported internal decimal type");
151: 		}
152: 		break;
153: 	default:
154: 		// Anything that is not supported
155: 		return false;
156: 	}
157: 	if (parquet_type_ptr) {
158: 		*parquet_type_ptr = parquet_type;
159: 	}
160: 	return true;
161: }
162: 
163: Type::type ParquetWriter::DuckDBTypeToParquetType(const LogicalType &duckdb_type) {
164: 	Type::type result;
165: 	if (TryGetParquetType(duckdb_type, &result)) {
166: 		return result;
167: 	}
168: 	throw NotImplementedException("Unimplemented type for Parquet \"%s\"", duckdb_type.ToString());
169: }
170: 
171: void ParquetWriter::SetSchemaProperties(const LogicalType &duckdb_type, duckdb_parquet::SchemaElement &schema_ele) {
172: 	if (duckdb_type.IsJSONType()) {
173: 		schema_ele.converted_type = ConvertedType::JSON;
174: 		schema_ele.__isset.converted_type = true;
175: 		schema_ele.__isset.logicalType = true;
176: 		schema_ele.logicalType.__set_JSON(duckdb_parquet::JsonType());
177: 		return;
178: 	}
179: 	switch (duckdb_type.id()) {
180: 	case LogicalTypeId::TINYINT:
181: 		schema_ele.converted_type = ConvertedType::INT_8;
182: 		schema_ele.__isset.converted_type = true;
183: 		break;
184: 	case LogicalTypeId::SMALLINT:
185: 		schema_ele.converted_type = ConvertedType::INT_16;
186: 		schema_ele.__isset.converted_type = true;
187: 		break;
188: 	case LogicalTypeId::INTEGER:
189: 		schema_ele.converted_type = ConvertedType::INT_32;
190: 		schema_ele.__isset.converted_type = true;
191: 		break;
192: 	case LogicalTypeId::BIGINT:
193: 		schema_ele.converted_type = ConvertedType::INT_64;
194: 		schema_ele.__isset.converted_type = true;
195: 		break;
196: 	case LogicalTypeId::UTINYINT:
197: 		schema_ele.converted_type = ConvertedType::UINT_8;
198: 		schema_ele.__isset.converted_type = true;
199: 		break;
200: 	case LogicalTypeId::USMALLINT:
201: 		schema_ele.converted_type = ConvertedType::UINT_16;
202: 		schema_ele.__isset.converted_type = true;
203: 		break;
204: 	case LogicalTypeId::UINTEGER:
205: 		schema_ele.converted_type = ConvertedType::UINT_32;
206: 		schema_ele.__isset.converted_type = true;
207: 		break;
208: 	case LogicalTypeId::UBIGINT:
209: 		schema_ele.converted_type = ConvertedType::UINT_64;
210: 		schema_ele.__isset.converted_type = true;
211: 		break;
212: 	case LogicalTypeId::DATE:
213: 		schema_ele.converted_type = ConvertedType::DATE;
214: 		schema_ele.__isset.converted_type = true;
215: 		break;
216: 	case LogicalTypeId::TIME_TZ:
217: 	case LogicalTypeId::TIME:
218: 		schema_ele.converted_type = ConvertedType::TIME_MICROS;
219: 		schema_ele.__isset.converted_type = true;
220: 		schema_ele.__isset.logicalType = true;
221: 		schema_ele.logicalType.__isset.TIME = true;
222: 		schema_ele.logicalType.TIME.isAdjustedToUTC = (duckdb_type.id() == LogicalTypeId::TIME_TZ);
223: 		schema_ele.logicalType.TIME.unit.__isset.MICROS = true;
224: 		break;
225: 	case LogicalTypeId::TIMESTAMP_TZ:
226: 	case LogicalTypeId::TIMESTAMP:
227: 	case LogicalTypeId::TIMESTAMP_SEC:
228: 		schema_ele.converted_type = ConvertedType::TIMESTAMP_MICROS;
229: 		schema_ele.__isset.converted_type = true;
230: 		schema_ele.__isset.logicalType = true;
231: 		schema_ele.logicalType.__isset.TIMESTAMP = true;
232: 		schema_ele.logicalType.TIMESTAMP.isAdjustedToUTC = (duckdb_type.id() == LogicalTypeId::TIMESTAMP_TZ);
233: 		schema_ele.logicalType.TIMESTAMP.unit.__isset.MICROS = true;
234: 		break;
235: 	case LogicalTypeId::TIMESTAMP_NS:
236: 		schema_ele.__isset.converted_type = false;
237: 		schema_ele.__isset.logicalType = true;
238: 		schema_ele.logicalType.__isset.TIMESTAMP = true;
239: 		schema_ele.logicalType.TIMESTAMP.isAdjustedToUTC = false;
240: 		schema_ele.logicalType.TIMESTAMP.unit.__isset.NANOS = true;
241: 		break;
242: 	case LogicalTypeId::TIMESTAMP_MS:
243: 		schema_ele.converted_type = ConvertedType::TIMESTAMP_MILLIS;
244: 		schema_ele.__isset.converted_type = true;
245: 		schema_ele.__isset.logicalType = true;
246: 		schema_ele.logicalType.__isset.TIMESTAMP = true;
247: 		schema_ele.logicalType.TIMESTAMP.isAdjustedToUTC = false;
248: 		schema_ele.logicalType.TIMESTAMP.unit.__isset.MILLIS = true;
249: 		break;
250: 	case LogicalTypeId::ENUM:
251: 	case LogicalTypeId::VARCHAR:
252: 		schema_ele.converted_type = ConvertedType::UTF8;
253: 		schema_ele.__isset.converted_type = true;
254: 		break;
255: 	case LogicalTypeId::INTERVAL:
256: 		schema_ele.type_length = 12;
257: 		schema_ele.converted_type = ConvertedType::INTERVAL;
258: 		schema_ele.__isset.type_length = true;
259: 		schema_ele.__isset.converted_type = true;
260: 		break;
261: 	case LogicalTypeId::UUID:
262: 		schema_ele.type_length = 16;
263: 		schema_ele.__isset.type_length = true;
264: 		schema_ele.__isset.logicalType = true;
265: 		schema_ele.logicalType.__isset.UUID = true;
266: 		break;
267: 	case LogicalTypeId::DECIMAL:
268: 		schema_ele.converted_type = ConvertedType::DECIMAL;
269: 		schema_ele.precision = DecimalType::GetWidth(duckdb_type);
270: 		schema_ele.scale = DecimalType::GetScale(duckdb_type);
271: 		schema_ele.__isset.converted_type = true;
272: 		schema_ele.__isset.precision = true;
273: 		schema_ele.__isset.scale = true;
274: 		if (duckdb_type.InternalType() == PhysicalType::INT128) {
275: 			schema_ele.type_length = 16;
276: 			schema_ele.__isset.type_length = true;
277: 		}
278: 		schema_ele.__isset.logicalType = true;
279: 		schema_ele.logicalType.__isset.DECIMAL = true;
280: 		schema_ele.logicalType.DECIMAL.precision = schema_ele.precision;
281: 		schema_ele.logicalType.DECIMAL.scale = schema_ele.scale;
282: 		break;
283: 	default:
284: 		break;
285: 	}
286: }
287: 
288: uint32_t ParquetWriter::Write(const duckdb_apache::thrift::TBase &object) {
289: 	if (encryption_config) {
290: 		return ParquetCrypto::Write(object, *protocol, encryption_config->GetFooterKey(), *encryption_util);
291: 	} else {
292: 		return object.write(protocol.get());
293: 	}
294: }
295: 
296: uint32_t ParquetWriter::WriteData(const const_data_ptr_t buffer, const uint32_t buffer_size) {
297: 	if (encryption_config) {
298: 		return ParquetCrypto::WriteData(*protocol, buffer, buffer_size, encryption_config->GetFooterKey(),
299: 		                                *encryption_util);
300: 	} else {
301: 		protocol->getTransport()->write(buffer, buffer_size);
302: 		return buffer_size;
303: 	}
304: }
305: 
306: void VerifyUniqueNames(const vector<string> &names) {
307: #ifdef DEBUG
308: 	unordered_set<string> name_set;
309: 	name_set.reserve(names.size());
310: 	for (auto &column : names) {
311: 		auto res = name_set.insert(column);
312: 		D_ASSERT(res.second == true);
313: 	}
314: 	// If there would be duplicates, these sizes would differ
315: 	D_ASSERT(name_set.size() == names.size());
316: #endif
317: }
318: 
319: ParquetWriter::ParquetWriter(ClientContext &context, FileSystem &fs, string file_name_p, vector<LogicalType> types_p,
320:                              vector<string> names_p, CompressionCodec::type codec, ChildFieldIDs field_ids_p,
321:                              const vector<pair<string, string>> &kv_metadata,
322:                              shared_ptr<ParquetEncryptionConfig> encryption_config_p, idx_t dictionary_size_limit_p,
323:                              double bloom_filter_false_positive_ratio_p, int64_t compression_level_p,
324:                              bool debug_use_openssl_p, ParquetVersion parquet_version)
325:     : file_name(std::move(file_name_p)), sql_types(std::move(types_p)), column_names(std::move(names_p)), codec(codec),
326:       field_ids(std::move(field_ids_p)), encryption_config(std::move(encryption_config_p)),
327:       dictionary_size_limit(dictionary_size_limit_p),
328:       bloom_filter_false_positive_ratio(bloom_filter_false_positive_ratio_p), compression_level(compression_level_p),
329:       debug_use_openssl(debug_use_openssl_p), parquet_version(parquet_version) {
330: 
331: 	// initialize the file writer
332: 	writer = make_uniq<BufferedFileWriter>(fs, file_name.c_str(),
333: 	                                       FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE_NEW);
334: 	if (encryption_config) {
335: 		auto &config = DBConfig::GetConfig(context);
336: 		if (config.encryption_util && debug_use_openssl) {
337: 			// Use OpenSSL
338: 			encryption_util = config.encryption_util;
339: 		} else {
340: 			encryption_util = make_shared_ptr<duckdb_mbedtls::MbedTlsWrapper::AESGCMStateMBEDTLSFactory>();
341: 		}
342: 		// encrypted parquet files start with the string "PARE"
343: 		writer->WriteData(const_data_ptr_cast("PARE"), 4);
344: 		// we only support this one for now, not "AES_GCM_CTR_V1"
345: 		file_meta_data.encryption_algorithm.__isset.AES_GCM_V1 = true;
346: 	} else {
347: 		// parquet files start with the string "PAR1"
348: 		writer->WriteData(const_data_ptr_cast("PAR1"), 4);
349: 	}
350: 	TCompactProtocolFactoryT<MyTransport> tproto_factory;
351: 	protocol = tproto_factory.getProtocol(std::make_shared<MyTransport>(*writer));
352: 
353: 	file_meta_data.num_rows = 0;
354: 	file_meta_data.version = 1;
355: 
356: 	file_meta_data.__isset.created_by = true;
357: 	file_meta_data.created_by =
358: 	    StringUtil::Format("DuckDB version %s (build %s)", DuckDB::LibraryVersion(), DuckDB::SourceID());
359: 
360: 	file_meta_data.schema.resize(1);
361: 
362: 	for (auto &kv_pair : kv_metadata) {
363: 		duckdb_parquet::KeyValue kv;
364: 		kv.__set_key(kv_pair.first);
365: 		kv.__set_value(kv_pair.second);
366: 		file_meta_data.key_value_metadata.push_back(kv);
367: 		file_meta_data.__isset.key_value_metadata = true;
368: 	}
369: 
370: 	// populate root schema object
371: 	file_meta_data.schema[0].name = "duckdb_schema";
372: 	file_meta_data.schema[0].num_children = NumericCast<int32_t>(sql_types.size());
373: 	file_meta_data.schema[0].__isset.num_children = true;
374: 	file_meta_data.schema[0].repetition_type = duckdb_parquet::FieldRepetitionType::REQUIRED;
375: 	file_meta_data.schema[0].__isset.repetition_type = true;
376: 
377: 	auto &unique_names = column_names;
378: 	VerifyUniqueNames(unique_names);
379: 
380: 	vector<string> schema_path;
381: 	for (idx_t i = 0; i < sql_types.size(); i++) {
382: 		column_writers.push_back(ColumnWriter::CreateWriterRecursive(
383: 		    context, file_meta_data.schema, *this, sql_types[i], unique_names[i], schema_path, &field_ids));
384: 	}
385: }
386: 
387: void ParquetWriter::PrepareRowGroup(ColumnDataCollection &buffer, PreparedRowGroup &result) {
388: 	// We write 8 columns at a time so that iterating over ColumnDataCollection is more efficient
389: 	static constexpr idx_t COLUMNS_PER_PASS = 8;
390: 
391: 	// We want these to be in-memory/hybrid so we don't have to copy over strings to the dictionary
392: 	D_ASSERT(buffer.GetAllocatorType() == ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR ||
393: 	         buffer.GetAllocatorType() == ColumnDataAllocatorType::HYBRID);
394: 
395: 	// set up a new row group for this chunk collection
396: 	auto &row_group = result.row_group;
397: 	row_group.num_rows = NumericCast<int64_t>(buffer.Count());
398: 	row_group.total_byte_size = NumericCast<int64_t>(buffer.SizeInBytes());
399: 	row_group.__isset.file_offset = true;
400: 
401: 	auto &states = result.states;
402: 	// iterate over each of the columns of the chunk collection and write them
403: 	D_ASSERT(buffer.ColumnCount() == column_writers.size());
404: 	for (idx_t col_idx = 0; col_idx < buffer.ColumnCount(); col_idx += COLUMNS_PER_PASS) {
405: 		const auto next = MinValue<idx_t>(buffer.ColumnCount() - col_idx, COLUMNS_PER_PASS);
406: 		vector<column_t> column_ids;
407: 		vector<reference<ColumnWriter>> col_writers;
408: 		vector<unique_ptr<ColumnWriterState>> write_states;
409: 		for (idx_t i = 0; i < next; i++) {
410: 			column_ids.emplace_back(col_idx + i);
411: 			col_writers.emplace_back(*column_writers[column_ids.back()]);
412: 			write_states.emplace_back(col_writers.back().get().InitializeWriteState(row_group));
413: 		}
414: 
415: 		for (auto &chunk : buffer.Chunks({column_ids})) {
416: 			for (idx_t i = 0; i < next; i++) {
417: 				if (col_writers[i].get().HasAnalyze()) {
418: 					col_writers[i].get().Analyze(*write_states[i], nullptr, chunk.data[i], chunk.size());
419: 				}
420: 			}
421: 		}
422: 
423: 		for (idx_t i = 0; i < next; i++) {
424: 			if (col_writers[i].get().HasAnalyze()) {
425: 				col_writers[i].get().FinalizeAnalyze(*write_states[i]);
426: 			}
427: 		}
428: 
429: 		// Reserving these once at the start really pays off
430: 		for (auto &write_state : write_states) {
431: 			write_state->definition_levels.reserve(buffer.Count());
432: 		}
433: 
434: 		for (auto &chunk : buffer.Chunks({column_ids})) {
435: 			for (idx_t i = 0; i < next; i++) {
436: 				col_writers[i].get().Prepare(*write_states[i], nullptr, chunk.data[i], chunk.size());
437: 			}
438: 		}
439: 
440: 		for (idx_t i = 0; i < next; i++) {
441: 			col_writers[i].get().BeginWrite(*write_states[i]);
442: 		}
443: 
444: 		for (auto &chunk : buffer.Chunks({column_ids})) {
445: 			for (idx_t i = 0; i < next; i++) {
446: 				col_writers[i].get().Write(*write_states[i], chunk.data[i], chunk.size());
447: 			}
448: 		}
449: 
450: 		for (auto &write_state : write_states) {
451: 			states.push_back(std::move(write_state));
452: 		}
453: 	}
454: 	result.heaps = buffer.GetHeapReferences();
455: }
456: 
457: // Validation code adapted from Impala
458: static void ValidateOffsetInFile(const string &filename, idx_t col_idx, idx_t file_length, idx_t offset,
459:                                  const string &offset_name) {
460: 	if (offset >= file_length) {
461: 		throw IOException("File '%s': metadata is corrupt. Column %d has invalid "
462: 		                  "%s (offset=%llu file_size=%llu).",
463: 		                  filename, col_idx, offset_name, offset, file_length);
464: 	}
465: }
466: 
467: static void ValidateColumnOffsets(const string &filename, idx_t file_length, const ParquetRowGroup &row_group) {
468: 	for (idx_t i = 0; i < row_group.columns.size(); ++i) {
469: 		const auto &col_chunk = row_group.columns[i];
470: 		ValidateOffsetInFile(filename, i, file_length, col_chunk.meta_data.data_page_offset, "data page offset");
471: 		auto col_start = NumericCast<idx_t>(col_chunk.meta_data.data_page_offset);
472: 		// The file format requires that if a dictionary page exists, it be before data pages.
473: 		if (col_chunk.meta_data.__isset.dictionary_page_offset) {
474: 			ValidateOffsetInFile(filename, i, file_length, col_chunk.meta_data.dictionary_page_offset,
475: 			                     "dictionary page offset");
476: 			if (NumericCast<idx_t>(col_chunk.meta_data.dictionary_page_offset) >= col_start) {
477: 				throw IOException("Parquet file '%s': metadata is corrupt. Dictionary "
478: 				                  "page (offset=%llu) must come before any data pages (offset=%llu).",
479: 				                  filename, col_chunk.meta_data.dictionary_page_offset, col_start);
480: 			}
481: 			col_start = col_chunk.meta_data.dictionary_page_offset;
482: 		}
483: 		auto col_len = NumericCast<idx_t>(col_chunk.meta_data.total_compressed_size);
484: 		auto col_end = col_start + col_len;
485: 		if (col_end <= 0 || col_end > file_length) {
486: 			throw IOException("Parquet file '%s': metadata is corrupt. Column %llu has "
487: 			                  "invalid column offsets (offset=%llu, size=%llu, file_size=%llu).",
488: 			                  filename, i, col_start, col_len, file_length);
489: 		}
490: 	}
491: }
492: 
493: void ParquetWriter::FlushRowGroup(PreparedRowGroup &prepared) {
494: 	lock_guard<mutex> glock(lock);
495: 	auto &row_group = prepared.row_group;
496: 	auto &states = prepared.states;
497: 	if (states.empty()) {
498: 		throw InternalException("Attempting to flush a row group with no rows");
499: 	}
500: 	row_group.file_offset = NumericCast<int64_t>(writer->GetTotalWritten());
501: 	for (idx_t col_idx = 0; col_idx < states.size(); col_idx++) {
502: 		const auto &col_writer = column_writers[col_idx];
503: 		auto write_state = std::move(states[col_idx]);
504: 		col_writer->FinalizeWrite(*write_state);
505: 	}
506: 	// let's make sure all offsets are ay-okay
507: 	ValidateColumnOffsets(file_name, writer->GetTotalWritten(), row_group);
508: 
509: 	// append the row group to the file meta data
510: 	file_meta_data.row_groups.push_back(row_group);
511: 	file_meta_data.num_rows += row_group.num_rows;
512: 
513: 	prepared.heaps.clear();
514: }
515: 
516: void ParquetWriter::Flush(ColumnDataCollection &buffer) {
517: 	if (buffer.Count() == 0) {
518: 		return;
519: 	}
520: 
521: 	PreparedRowGroup prepared_row_group;
522: 	PrepareRowGroup(buffer, prepared_row_group);
523: 	buffer.Reset();
524: 
525: 	FlushRowGroup(prepared_row_group);
526: }
527: 
528: void ParquetWriter::Finalize() {
529: 
530: 	// dump the bloom filters right before footer, not if stuff is encrypted
531: 
532: 	for (auto &bloom_filter_entry : bloom_filters) {
533: 		D_ASSERT(!encryption_config);
534: 		// write nonsense bloom filter header
535: 		duckdb_parquet::BloomFilterHeader filter_header;
536: 		auto bloom_filter_bytes = bloom_filter_entry.bloom_filter->Get();
537: 		filter_header.numBytes = NumericCast<int32_t>(bloom_filter_bytes->len);
538: 		filter_header.algorithm.__set_BLOCK(duckdb_parquet::SplitBlockAlgorithm());
539: 		filter_header.compression.__set_UNCOMPRESSED(duckdb_parquet::Uncompressed());
540: 		filter_header.hash.__set_XXHASH(duckdb_parquet::XxHash());
541: 
542: 		// set metadata flags
543: 		auto &column_chunk =
544: 		    file_meta_data.row_groups[bloom_filter_entry.row_group_idx].columns[bloom_filter_entry.column_idx];
545: 
546: 		column_chunk.meta_data.__isset.bloom_filter_offset = true;
547: 		column_chunk.meta_data.bloom_filter_offset = NumericCast<int64_t>(writer->GetTotalWritten());
548: 
549: 		auto bloom_filter_header_size = Write(filter_header);
550: 		// write actual data
551: 		WriteData(bloom_filter_bytes->ptr, bloom_filter_bytes->len);
552: 
553: 		column_chunk.meta_data.__isset.bloom_filter_length = true;
554: 		column_chunk.meta_data.bloom_filter_length =
555: 		    NumericCast<int32_t>(bloom_filter_header_size + bloom_filter_bytes->len);
556: 	}
557: 
558: 	const auto metadata_start_offset = writer->GetTotalWritten();
559: 	if (encryption_config) {
560: 		// Crypto metadata is written unencrypted
561: 		FileCryptoMetaData crypto_metadata;
562: 		duckdb_parquet::AesGcmV1 aes_gcm_v1;
563: 		duckdb_parquet::EncryptionAlgorithm alg;
564: 		alg.__set_AES_GCM_V1(aes_gcm_v1);
565: 		crypto_metadata.__set_encryption_algorithm(alg);
566: 		crypto_metadata.write(protocol.get());
567: 	}
568: 
569: 	// Add geoparquet metadata to the file metadata
570: 	if (geoparquet_data) {
571: 		geoparquet_data->Write(file_meta_data);
572: 	}
573: 
574: 	Write(file_meta_data);
575: 
576: 	writer->Write<uint32_t>(writer->GetTotalWritten() - metadata_start_offset);
577: 
578: 	if (encryption_config) {
579: 		// encrypted parquet files also end with the string "PARE"
580: 		writer->WriteData(const_data_ptr_cast("PARE"), 4);
581: 	} else {
582: 		// parquet files also end with the string "PAR1"
583: 		writer->WriteData(const_data_ptr_cast("PAR1"), 4);
584: 	}
585: 
586: 	// flush to disk
587: 	writer->Close();
588: 	writer.reset();
589: }
590: 
591: GeoParquetFileMetadata &ParquetWriter::GetGeoParquetData() {
592: 	if (!geoparquet_data) {
593: 		geoparquet_data = make_uniq<GeoParquetFileMetadata>();
594: 	}
595: 	return *geoparquet_data;
596: }
597: 
598: void ParquetWriter::BufferBloomFilter(idx_t col_idx, unique_ptr<ParquetBloomFilter> bloom_filter) {
599: 	if (encryption_config) {
600: 		return;
601: 	}
602: 	ParquetBloomFilterEntry new_entry;
603: 	new_entry.bloom_filter = std::move(bloom_filter);
604: 	new_entry.column_idx = col_idx;
605: 	new_entry.row_group_idx = file_meta_data.row_groups.size();
606: 	bloom_filters.push_back(std::move(new_entry));
607: }
608: 
609: } // namespace duckdb
[end of extension/parquet/parquet_writer.cpp]
[start of src/catalog/catalog.cpp]
1: #include "duckdb/catalog/catalog.hpp"
2: 
3: #include "duckdb/catalog/catalog_search_path.hpp"
4: #include "duckdb/catalog/catalog_entry/list.hpp"
5: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
6: #include "duckdb/catalog/catalog_set.hpp"
7: #include "duckdb/catalog/default/default_schemas.hpp"
8: #include "duckdb/catalog/catalog_entry/type_catalog_entry.hpp"
9: #include "duckdb/common/exception.hpp"
10: #include "duckdb/main/client_context.hpp"
11: #include "duckdb/main/client_data.hpp"
12: #include "duckdb/main/database.hpp"
13: #include "duckdb/parser/expression/function_expression.hpp"
14: #include "duckdb/main/extension_helper.hpp"
15: #include "duckdb/parser/parsed_data/alter_table_info.hpp"
16: #include "duckdb/parser/parsed_data/create_aggregate_function_info.hpp"
17: #include "duckdb/parser/parsed_data/create_collation_info.hpp"
18: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
19: #include "duckdb/parser/parsed_data/create_index_info.hpp"
20: #include "duckdb/parser/parsed_data/create_pragma_function_info.hpp"
21: #include "duckdb/parser/parsed_data/create_secret_info.hpp"
22: #include "duckdb/parser/parsed_data/create_scalar_function_info.hpp"
23: #include "duckdb/parser/parsed_data/create_schema_info.hpp"
24: #include "duckdb/parser/parsed_data/create_sequence_info.hpp"
25: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
26: #include "duckdb/parser/parsed_data/create_type_info.hpp"
27: #include "duckdb/parser/parsed_data/create_view_info.hpp"
28: #include "duckdb/parser/parsed_data/drop_info.hpp"
29: #include "duckdb/parser/statement/create_statement.hpp"
30: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
31: #include "duckdb/planner/binder.hpp"
32: #include "duckdb/planner/expression_binder/index_binder.hpp"
33: #include "duckdb/catalog/default/default_types.hpp"
34: #include "duckdb/main/extension_entries.hpp"
35: #include "duckdb/main/extension/generated_extension_loader.hpp"
36: #include "duckdb/main/connection.hpp"
37: #include "duckdb/main/attached_database.hpp"
38: #include "duckdb/main/database_manager.hpp"
39: #include "duckdb/function/built_in_functions.hpp"
40: #include "duckdb/catalog/similar_catalog_entry.hpp"
41: #include "duckdb/storage/database_size.hpp"
42: #include <algorithm>
43: 
44: namespace duckdb {
45: 
46: Catalog::Catalog(AttachedDatabase &db) : db(db) {
47: }
48: 
49: Catalog::~Catalog() {
50: }
51: 
52: DatabaseInstance &Catalog::GetDatabase() {
53: 	return db.GetDatabase();
54: }
55: 
56: AttachedDatabase &Catalog::GetAttached() {
57: 	return db;
58: }
59: 
60: const AttachedDatabase &Catalog::GetAttached() const {
61: 	return db;
62: }
63: 
64: const string &Catalog::GetName() const {
65: 	return GetAttached().GetName();
66: }
67: 
68: idx_t Catalog::GetOid() {
69: 	return GetAttached().oid;
70: }
71: 
72: Catalog &Catalog::GetSystemCatalog(ClientContext &context) {
73: 	return Catalog::GetSystemCatalog(*context.db);
74: }
75: 
76: const string &GetDefaultCatalog(CatalogEntryRetriever &retriever) {
77: 	return DatabaseManager::GetDefaultDatabase(retriever.GetContext());
78: }
79: 
80: optional_ptr<Catalog> Catalog::GetCatalogEntry(CatalogEntryRetriever &retriever, const string &catalog_name) {
81: 	auto &context = retriever.GetContext();
82: 	auto &db_manager = DatabaseManager::Get(context);
83: 	if (catalog_name == TEMP_CATALOG) {
84: 		return &ClientData::Get(context).temporary_objects->GetCatalog();
85: 	}
86: 	if (catalog_name == SYSTEM_CATALOG) {
87: 		return &GetSystemCatalog(context);
88: 	}
89: 	auto entry =
90: 	    db_manager.GetDatabase(context, IsInvalidCatalog(catalog_name) ? GetDefaultCatalog(retriever) : catalog_name);
91: 	if (!entry) {
92: 		return nullptr;
93: 	}
94: 	return &entry->GetCatalog();
95: }
96: 
97: optional_ptr<Catalog> Catalog::GetCatalogEntry(ClientContext &context, const string &catalog_name) {
98: 	CatalogEntryRetriever entry_retriever(context);
99: 	return GetCatalogEntry(entry_retriever, catalog_name);
100: }
101: 
102: Catalog &Catalog::GetCatalog(CatalogEntryRetriever &retriever, const string &catalog_name) {
103: 	auto catalog = Catalog::GetCatalogEntry(retriever, catalog_name);
104: 	if (!catalog) {
105: 		throw BinderException("Catalog \"%s\" does not exist!", catalog_name);
106: 	}
107: 	return *catalog;
108: }
109: 
110: Catalog &Catalog::GetCatalog(ClientContext &context, const string &catalog_name) {
111: 	CatalogEntryRetriever entry_retriever(context);
112: 	return GetCatalog(entry_retriever, catalog_name);
113: }
114: 
115: //===--------------------------------------------------------------------===//
116: // Schema
117: //===--------------------------------------------------------------------===//
118: optional_ptr<CatalogEntry> Catalog::CreateSchema(ClientContext &context, CreateSchemaInfo &info) {
119: 	return CreateSchema(GetCatalogTransaction(context), info);
120: }
121: 
122: CatalogTransaction Catalog::GetCatalogTransaction(ClientContext &context) {
123: 	return CatalogTransaction(*this, context);
124: }
125: 
126: //===--------------------------------------------------------------------===//
127: // Table
128: //===--------------------------------------------------------------------===//
129: optional_ptr<CatalogEntry> Catalog::CreateTable(ClientContext &context, BoundCreateTableInfo &info) {
130: 	return CreateTable(GetCatalogTransaction(context), info);
131: }
132: 
133: optional_ptr<CatalogEntry> Catalog::CreateTable(ClientContext &context, unique_ptr<CreateTableInfo> info) {
134: 	auto binder = Binder::CreateBinder(context);
135: 	auto bound_info = binder->BindCreateTableInfo(std::move(info));
136: 	return CreateTable(context, *bound_info);
137: }
138: 
139: optional_ptr<CatalogEntry> Catalog::CreateTable(CatalogTransaction transaction, SchemaCatalogEntry &schema,
140:                                                 BoundCreateTableInfo &info) {
141: 	return schema.CreateTable(transaction, info);
142: }
143: 
144: optional_ptr<CatalogEntry> Catalog::CreateTable(CatalogTransaction transaction, BoundCreateTableInfo &info) {
145: 	auto &schema = GetSchema(transaction, info.base->schema);
146: 	return CreateTable(transaction, schema, info);
147: }
148: 
149: //===--------------------------------------------------------------------===//
150: // View
151: //===--------------------------------------------------------------------===//
152: optional_ptr<CatalogEntry> Catalog::CreateView(CatalogTransaction transaction, CreateViewInfo &info) {
153: 	auto &schema = GetSchema(transaction, info.schema);
154: 	return CreateView(transaction, schema, info);
155: }
156: 
157: optional_ptr<CatalogEntry> Catalog::CreateView(ClientContext &context, CreateViewInfo &info) {
158: 	return CreateView(GetCatalogTransaction(context), info);
159: }
160: 
161: optional_ptr<CatalogEntry> Catalog::CreateView(CatalogTransaction transaction, SchemaCatalogEntry &schema,
162:                                                CreateViewInfo &info) {
163: 	return schema.CreateView(transaction, info);
164: }
165: 
166: //===--------------------------------------------------------------------===//
167: // Sequence
168: //===--------------------------------------------------------------------===//
169: optional_ptr<CatalogEntry> Catalog::CreateSequence(CatalogTransaction transaction, CreateSequenceInfo &info) {
170: 	auto &schema = GetSchema(transaction, info.schema);
171: 	return CreateSequence(transaction, schema, info);
172: }
173: 
174: optional_ptr<CatalogEntry> Catalog::CreateSequence(ClientContext &context, CreateSequenceInfo &info) {
175: 	return CreateSequence(GetCatalogTransaction(context), info);
176: }
177: 
178: optional_ptr<CatalogEntry> Catalog::CreateSequence(CatalogTransaction transaction, SchemaCatalogEntry &schema,
179:                                                    CreateSequenceInfo &info) {
180: 	return schema.CreateSequence(transaction, info);
181: }
182: 
183: //===--------------------------------------------------------------------===//
184: // Type
185: //===--------------------------------------------------------------------===//
186: optional_ptr<CatalogEntry> Catalog::CreateType(CatalogTransaction transaction, CreateTypeInfo &info) {
187: 	auto &schema = GetSchema(transaction, info.schema);
188: 	return CreateType(transaction, schema, info);
189: }
190: 
191: optional_ptr<CatalogEntry> Catalog::CreateType(ClientContext &context, CreateTypeInfo &info) {
192: 	return CreateType(GetCatalogTransaction(context), info);
193: }
194: 
195: optional_ptr<CatalogEntry> Catalog::CreateType(CatalogTransaction transaction, SchemaCatalogEntry &schema,
196:                                                CreateTypeInfo &info) {
197: 	return schema.CreateType(transaction, info);
198: }
199: 
200: //===--------------------------------------------------------------------===//
201: // Table Function
202: //===--------------------------------------------------------------------===//
203: optional_ptr<CatalogEntry> Catalog::CreateTableFunction(CatalogTransaction transaction, CreateTableFunctionInfo &info) {
204: 	auto &schema = GetSchema(transaction, info.schema);
205: 	return CreateTableFunction(transaction, schema, info);
206: }
207: 
208: optional_ptr<CatalogEntry> Catalog::CreateTableFunction(ClientContext &context, CreateTableFunctionInfo &info) {
209: 	return CreateTableFunction(GetCatalogTransaction(context), info);
210: }
211: 
212: optional_ptr<CatalogEntry> Catalog::CreateTableFunction(CatalogTransaction transaction, SchemaCatalogEntry &schema,
213:                                                         CreateTableFunctionInfo &info) {
214: 	return schema.CreateTableFunction(transaction, info);
215: }
216: 
217: optional_ptr<CatalogEntry> Catalog::CreateTableFunction(ClientContext &context,
218:                                                         optional_ptr<CreateTableFunctionInfo> info) {
219: 	return CreateTableFunction(context, *info);
220: }
221: 
222: //===--------------------------------------------------------------------===//
223: // Copy Function
224: //===--------------------------------------------------------------------===//
225: optional_ptr<CatalogEntry> Catalog::CreateCopyFunction(CatalogTransaction transaction, CreateCopyFunctionInfo &info) {
226: 	auto &schema = GetSchema(transaction, info.schema);
227: 	return CreateCopyFunction(transaction, schema, info);
228: }
229: 
230: optional_ptr<CatalogEntry> Catalog::CreateCopyFunction(ClientContext &context, CreateCopyFunctionInfo &info) {
231: 	return CreateCopyFunction(GetCatalogTransaction(context), info);
232: }
233: 
234: optional_ptr<CatalogEntry> Catalog::CreateCopyFunction(CatalogTransaction transaction, SchemaCatalogEntry &schema,
235:                                                        CreateCopyFunctionInfo &info) {
236: 	return schema.CreateCopyFunction(transaction, info);
237: }
238: 
239: //===--------------------------------------------------------------------===//
240: // Pragma Function
241: //===--------------------------------------------------------------------===//
242: optional_ptr<CatalogEntry> Catalog::CreatePragmaFunction(CatalogTransaction transaction,
243:                                                          CreatePragmaFunctionInfo &info) {
244: 	auto &schema = GetSchema(transaction, info.schema);
245: 	return CreatePragmaFunction(transaction, schema, info);
246: }
247: 
248: optional_ptr<CatalogEntry> Catalog::CreatePragmaFunction(ClientContext &context, CreatePragmaFunctionInfo &info) {
249: 	return CreatePragmaFunction(GetCatalogTransaction(context), info);
250: }
251: 
252: optional_ptr<CatalogEntry> Catalog::CreatePragmaFunction(CatalogTransaction transaction, SchemaCatalogEntry &schema,
253:                                                          CreatePragmaFunctionInfo &info) {
254: 	return schema.CreatePragmaFunction(transaction, info);
255: }
256: 
257: //===--------------------------------------------------------------------===//
258: // Function
259: //===--------------------------------------------------------------------===//
260: optional_ptr<CatalogEntry> Catalog::CreateFunction(CatalogTransaction transaction, CreateFunctionInfo &info) {
261: 	auto &schema = GetSchema(transaction, info.schema);
262: 	return CreateFunction(transaction, schema, info);
263: }
264: 
265: optional_ptr<CatalogEntry> Catalog::CreateFunction(ClientContext &context, CreateFunctionInfo &info) {
266: 	return CreateFunction(GetCatalogTransaction(context), info);
267: }
268: 
269: optional_ptr<CatalogEntry> Catalog::CreateFunction(CatalogTransaction transaction, SchemaCatalogEntry &schema,
270:                                                    CreateFunctionInfo &info) {
271: 	return schema.CreateFunction(transaction, info);
272: }
273: 
274: optional_ptr<CatalogEntry> Catalog::AddFunction(ClientContext &context, CreateFunctionInfo &info) {
275: 	info.on_conflict = OnCreateConflict::ALTER_ON_CONFLICT;
276: 	return CreateFunction(context, info);
277: }
278: 
279: //===--------------------------------------------------------------------===//
280: // Collation
281: //===--------------------------------------------------------------------===//
282: optional_ptr<CatalogEntry> Catalog::CreateCollation(CatalogTransaction transaction, CreateCollationInfo &info) {
283: 	auto &schema = GetSchema(transaction, info.schema);
284: 	return CreateCollation(transaction, schema, info);
285: }
286: 
287: optional_ptr<CatalogEntry> Catalog::CreateCollation(ClientContext &context, CreateCollationInfo &info) {
288: 	return CreateCollation(GetCatalogTransaction(context), info);
289: }
290: 
291: optional_ptr<CatalogEntry> Catalog::CreateCollation(CatalogTransaction transaction, SchemaCatalogEntry &schema,
292:                                                     CreateCollationInfo &info) {
293: 	return schema.CreateCollation(transaction, info);
294: }
295: 
296: //===--------------------------------------------------------------------===//
297: // Index
298: //===--------------------------------------------------------------------===//
299: optional_ptr<CatalogEntry> Catalog::CreateIndex(CatalogTransaction transaction, CreateIndexInfo &info) {
300: 	auto &schema = GetSchema(transaction, info.schema);
301: 	auto &table = schema.GetEntry(transaction, CatalogType::TABLE_ENTRY, info.table)->Cast<TableCatalogEntry>();
302: 	return schema.CreateIndex(transaction, info, table);
303: }
304: 
305: optional_ptr<CatalogEntry> Catalog::CreateIndex(ClientContext &context, CreateIndexInfo &info) {
306: 	return CreateIndex(GetCatalogTransaction(context), info);
307: }
308: 
309: unique_ptr<LogicalOperator> Catalog::BindCreateIndex(Binder &binder, CreateStatement &stmt, TableCatalogEntry &table,
310:                                                      unique_ptr<LogicalOperator> plan) {
311: 	D_ASSERT(plan->type == LogicalOperatorType::LOGICAL_GET);
312: 	auto create_index_info = unique_ptr_cast<CreateInfo, CreateIndexInfo>(std::move(stmt.info));
313: 	IndexBinder index_binder(binder, binder.context);
314: 	return index_binder.BindCreateIndex(binder.context, std::move(create_index_info), table, std::move(plan), nullptr);
315: }
316: 
317: unique_ptr<LogicalOperator> Catalog::BindAlterAddIndex(Binder &binder, TableCatalogEntry &table_entry,
318:                                                        unique_ptr<LogicalOperator> plan,
319:                                                        unique_ptr<CreateIndexInfo> create_info,
320:                                                        unique_ptr<AlterTableInfo> alter_info) {
321: 	throw NotImplementedException("BindAlterAddIndex not supported by this catalog");
322: }
323: 
324: //===--------------------------------------------------------------------===//
325: // Lookup Structures
326: //===--------------------------------------------------------------------===//
327: struct CatalogLookup {
328: 	CatalogLookup(Catalog &catalog, string schema_p, string name_p)
329: 	    : catalog(catalog), schema(std::move(schema_p)), name(std::move(name_p)) {
330: 	}
331: 
332: 	Catalog &catalog;
333: 	string schema;
334: 	string name;
335: };
336: 
337: //===--------------------------------------------------------------------===//
338: // Generic
339: //===--------------------------------------------------------------------===//
340: void Catalog::DropEntry(ClientContext &context, DropInfo &info) {
341: 	if (info.type == CatalogType::SCHEMA_ENTRY) {
342: 		// DROP SCHEMA
343: 		DropSchema(context, info);
344: 		return;
345: 	}
346: 
347: 	CatalogEntryRetriever retriever(context);
348: 	auto lookup = LookupEntry(retriever, info.type, info.schema, info.name, info.if_not_found);
349: 	if (!lookup.Found()) {
350: 		return;
351: 	}
352: 
353: 	lookup.schema->DropEntry(context, info);
354: }
355: 
356: SchemaCatalogEntry &Catalog::GetSchema(ClientContext &context, const string &name, QueryErrorContext error_context) {
357: 	return *Catalog::GetSchema(context, name, OnEntryNotFound::THROW_EXCEPTION, error_context);
358: }
359: 
360: optional_ptr<SchemaCatalogEntry> Catalog::GetSchema(ClientContext &context, const string &schema_name,
361:                                                     OnEntryNotFound if_not_found, QueryErrorContext error_context) {
362: 	return GetSchema(GetCatalogTransaction(context), schema_name, if_not_found, error_context);
363: }
364: 
365: SchemaCatalogEntry &Catalog::GetSchema(ClientContext &context, const string &catalog_name, const string &schema_name,
366:                                        QueryErrorContext error_context) {
367: 	return *Catalog::GetSchema(context, catalog_name, schema_name, OnEntryNotFound::THROW_EXCEPTION, error_context);
368: }
369: 
370: SchemaCatalogEntry &Catalog::GetSchema(CatalogTransaction transaction, const string &name,
371:                                        QueryErrorContext error_context) {
372: 	return *GetSchema(transaction, name, OnEntryNotFound::THROW_EXCEPTION, error_context);
373: }
374: 
375: //===--------------------------------------------------------------------===//
376: // Lookup
377: //===--------------------------------------------------------------------===//
378: vector<SimilarCatalogEntry> Catalog::SimilarEntriesInSchemas(ClientContext &context, const string &entry_name,
379:                                                              CatalogType type,
380:                                                              const reference_set_t<SchemaCatalogEntry> &schemas) {
381: 	vector<SimilarCatalogEntry> results;
382: 	for (auto schema_ref : schemas) {
383: 		auto &schema = schema_ref.get();
384: 		auto transaction = schema.catalog.GetCatalogTransaction(context);
385: 		auto entry = schema.GetSimilarEntry(transaction, type, entry_name);
386: 		if (!entry.Found()) {
387: 			// no similar entry found
388: 			continue;
389: 		}
390: 		if (results.empty() || results[0].score <= entry.score) {
391: 			if (!results.empty() && results[0].score < entry.score) {
392: 				results.clear();
393: 			}
394: 
395: 			results.push_back(entry);
396: 			results.back().schema = &schema;
397: 		}
398: 	}
399: 	return results;
400: }
401: 
402: vector<CatalogSearchEntry> GetCatalogEntries(CatalogEntryRetriever &retriever, const string &catalog,
403:                                              const string &schema) {
404: 	auto &context = retriever.GetContext();
405: 	vector<CatalogSearchEntry> entries;
406: 	auto &search_path = retriever.GetSearchPath();
407: 	if (IsInvalidCatalog(catalog) && IsInvalidSchema(schema)) {
408: 		// no catalog or schema provided - scan the entire search path
409: 		entries = search_path.Get();
410: 	} else if (IsInvalidCatalog(catalog)) {
411: 		auto catalogs = search_path.GetCatalogsForSchema(schema);
412: 		for (auto &catalog_name : catalogs) {
413: 			entries.emplace_back(catalog_name, schema);
414: 		}
415: 		if (entries.empty()) {
416: 			auto &default_entry = search_path.GetDefault();
417: 			if (!IsInvalidCatalog(default_entry.catalog)) {
418: 				entries.emplace_back(default_entry.catalog, schema);
419: 			} else {
420: 				entries.emplace_back(DatabaseManager::GetDefaultDatabase(context), schema);
421: 			}
422: 		}
423: 	} else if (IsInvalidSchema(schema)) {
424: 		auto schemas = search_path.GetSchemasForCatalog(catalog);
425: 		for (auto &schema_name : schemas) {
426: 			entries.emplace_back(catalog, schema_name);
427: 		}
428: 		if (entries.empty()) {
429: 			entries.emplace_back(catalog, DEFAULT_SCHEMA);
430: 		}
431: 	} else {
432: 		// specific catalog and schema provided
433: 		entries.emplace_back(catalog, schema);
434: 	}
435: 	return entries;
436: }
437: 
438: void FindMinimalQualification(CatalogEntryRetriever &retriever, const string &catalog_name, const string &schema_name,
439:                               bool &qualify_database, bool &qualify_schema) {
440: 	// check if we can we qualify ONLY the schema
441: 	bool found = false;
442: 	auto entries = GetCatalogEntries(retriever, INVALID_CATALOG, schema_name);
443: 	for (auto &entry : entries) {
444: 		if (entry.catalog == catalog_name && entry.schema == schema_name) {
445: 			found = true;
446: 			break;
447: 		}
448: 	}
449: 	if (found) {
450: 		qualify_database = false;
451: 		qualify_schema = true;
452: 		return;
453: 	}
454: 	// check if we can qualify ONLY the catalog
455: 	found = false;
456: 	entries = GetCatalogEntries(retriever, catalog_name, INVALID_SCHEMA);
457: 	for (auto &entry : entries) {
458: 		if (entry.catalog == catalog_name && entry.schema == schema_name) {
459: 			found = true;
460: 			break;
461: 		}
462: 	}
463: 	if (found) {
464: 		qualify_database = true;
465: 		qualify_schema = false;
466: 		return;
467: 	}
468: 	// need to qualify both catalog and schema
469: 	qualify_database = true;
470: 	qualify_schema = true;
471: }
472: 
473: bool Catalog::TryAutoLoad(ClientContext &context, const string &original_name) noexcept {
474: 	string extension_name = ExtensionHelper::ApplyExtensionAlias(original_name);
475: 	if (context.db->ExtensionIsLoaded(extension_name)) {
476: 		return true;
477: 	}
478: #ifndef DUCKDB_DISABLE_EXTENSION_LOAD
479: 	auto &dbconfig = DBConfig::GetConfig(context);
480: 	if (!dbconfig.options.autoload_known_extensions) {
481: 		return false;
482: 	}
483: 	try {
484: 		if (ExtensionHelper::CanAutoloadExtension(extension_name)) {
485: 			return ExtensionHelper::TryAutoLoadExtension(context, extension_name);
486: 		}
487: 	} catch (...) {
488: 		return false;
489: 	}
490: #endif
491: 	return false;
492: }
493: 
494: void Catalog::AutoloadExtensionByConfigName(ClientContext &context, const string &configuration_name) {
495: #ifndef DUCKDB_DISABLE_EXTENSION_LOAD
496: 	auto &dbconfig = DBConfig::GetConfig(context);
497: 	if (dbconfig.options.autoload_known_extensions) {
498: 		auto extension_name = ExtensionHelper::FindExtensionInEntries(configuration_name, EXTENSION_SETTINGS);
499: 		if (ExtensionHelper::CanAutoloadExtension(extension_name)) {
500: 			ExtensionHelper::AutoLoadExtension(context, extension_name);
501: 			return;
502: 		}
503: 	}
504: #endif
505: 
506: 	throw Catalog::UnrecognizedConfigurationError(context, configuration_name);
507: }
508: 
509: static bool IsAutoloadableFunction(CatalogType type) {
510: 	return (type == CatalogType::TABLE_FUNCTION_ENTRY || type == CatalogType::SCALAR_FUNCTION_ENTRY ||
511: 	        type == CatalogType::AGGREGATE_FUNCTION_ENTRY || type == CatalogType::PRAGMA_FUNCTION_ENTRY);
512: }
513: 
514: bool IsTableFunction(CatalogType type) {
515: 	switch (type) {
516: 	case CatalogType::TABLE_FUNCTION_ENTRY:
517: 	case CatalogType::TABLE_MACRO_ENTRY:
518: 	case CatalogType::PRAGMA_FUNCTION_ENTRY:
519: 		return true;
520: 	default:
521: 		return false;
522: 	}
523: }
524: 
525: bool IsScalarFunction(CatalogType type) {
526: 	switch (type) {
527: 	case CatalogType::SCALAR_FUNCTION_ENTRY:
528: 	case CatalogType::AGGREGATE_FUNCTION_ENTRY:
529: 	case CatalogType::MACRO_ENTRY:
530: 		return true;
531: 	default:
532: 		return false;
533: 	}
534: }
535: 
536: static bool CompareCatalogTypes(CatalogType type_a, CatalogType type_b) {
537: 	if (type_a == type_b) {
538: 		// Types are same
539: 		return true;
540: 	}
541: 	if (IsScalarFunction(type_a) && IsScalarFunction(type_b)) {
542: 		return true;
543: 	}
544: 	if (IsTableFunction(type_a) && IsTableFunction(type_b)) {
545: 		return true;
546: 	}
547: 	return false;
548: }
549: 
550: bool Catalog::AutoLoadExtensionByCatalogEntry(DatabaseInstance &db, CatalogType type, const string &entry_name) {
551: #ifndef DUCKDB_DISABLE_EXTENSION_LOAD
552: 	auto &dbconfig = DBConfig::GetConfig(db);
553: 	if (dbconfig.options.autoload_known_extensions) {
554: 		string extension_name;
555: 		if (IsAutoloadableFunction(type)) {
556: 			auto lookup_result = ExtensionHelper::FindExtensionInFunctionEntries(entry_name, EXTENSION_FUNCTIONS);
557: 			if (lookup_result.empty()) {
558: 				return false;
559: 			}
560: 			for (auto &function : lookup_result) {
561: 				auto function_type = function.second;
562: 				// FIXME: what if there are two functions with the same name, from different extensions?
563: 				if (CompareCatalogTypes(type, function_type)) {
564: 					extension_name = function.first;
565: 					break;
566: 				}
567: 			}
568: 		} else if (type == CatalogType::COPY_FUNCTION_ENTRY) {
569: 			extension_name = ExtensionHelper::FindExtensionInEntries(entry_name, EXTENSION_COPY_FUNCTIONS);
570: 		} else if (type == CatalogType::TYPE_ENTRY) {
571: 			extension_name = ExtensionHelper::FindExtensionInEntries(entry_name, EXTENSION_TYPES);
572: 		} else if (type == CatalogType::COLLATION_ENTRY) {
573: 			extension_name = ExtensionHelper::FindExtensionInEntries(entry_name, EXTENSION_COLLATIONS);
574: 		}
575: 
576: 		if (!extension_name.empty() && ExtensionHelper::CanAutoloadExtension(extension_name)) {
577: 			ExtensionHelper::AutoLoadExtension(db, extension_name);
578: 			return true;
579: 		}
580: 	}
581: #endif
582: 
583: 	return false;
584: }
585: 
586: CatalogException Catalog::UnrecognizedConfigurationError(ClientContext &context, const string &name) {
587: 	// check if the setting exists in any extensions
588: 	auto extension_name = ExtensionHelper::FindExtensionInEntries(name, EXTENSION_SETTINGS);
589: 	if (!extension_name.empty()) {
590: 		auto error_message = "Setting with name \"" + name + "\" is not in the catalog, but it exists in the " +
591: 		                     extension_name + " extension.";
592: 		error_message = ExtensionHelper::AddExtensionInstallHintToErrorMsg(context, error_message, extension_name);
593: 		return CatalogException(error_message);
594: 	}
595: 	// the setting is not in an extension
596: 	// get a list of all options
597: 	vector<string> potential_names = DBConfig::GetOptionNames();
598: 	for (auto &entry : DBConfig::GetConfig(context).extension_parameters) {
599: 		potential_names.push_back(entry.first);
600: 	}
601: 	throw CatalogException::MissingEntry("configuration parameter", name, potential_names);
602: }
603: 
604: CatalogException Catalog::CreateMissingEntryException(CatalogEntryRetriever &retriever, const string &entry_name,
605:                                                       CatalogType type,
606:                                                       const reference_set_t<SchemaCatalogEntry> &schemas,
607:                                                       QueryErrorContext error_context) {
608: 	auto &context = retriever.GetContext();
609: 	auto entries = SimilarEntriesInSchemas(context, entry_name, type, schemas);
610: 
611: 	reference_set_t<SchemaCatalogEntry> unseen_schemas;
612: 	auto &db_manager = DatabaseManager::Get(context);
613: 	auto databases = db_manager.GetDatabases(context);
614: 	auto &config = DBConfig::GetConfig(context);
615: 
616: 	auto max_schema_count = config.GetSetting<CatalogErrorMaxSchemasSetting>(context);
617: 	for (auto database : databases) {
618: 		if (unseen_schemas.size() >= max_schema_count) {
619: 			break;
620: 		}
621: 		auto &catalog = database.get().GetCatalog();
622: 		auto current_schemas = catalog.GetAllSchemas(context);
623: 		for (auto &current_schema : current_schemas) {
624: 			if (unseen_schemas.size() >= max_schema_count) {
625: 				break;
626: 			}
627: 			unseen_schemas.insert(current_schema.get());
628: 		}
629: 	}
630: 	// check if the entry exists in any extension
631: 	string extension_name;
632: 	if (type == CatalogType::TABLE_FUNCTION_ENTRY || type == CatalogType::SCALAR_FUNCTION_ENTRY ||
633: 	    type == CatalogType::AGGREGATE_FUNCTION_ENTRY || type == CatalogType::PRAGMA_FUNCTION_ENTRY) {
634: 		auto lookup_result = ExtensionHelper::FindExtensionInFunctionEntries(entry_name, EXTENSION_FUNCTIONS);
635: 		do {
636: 			if (lookup_result.empty()) {
637: 				break;
638: 			}
639: 			vector<string> other_types;
640: 			string extension_for_error;
641: 			for (auto &function : lookup_result) {
642: 				auto function_type = function.second;
643: 				if (CompareCatalogTypes(type, function_type)) {
644: 					extension_name = function.first;
645: 					break;
646: 				}
647: 				extension_for_error = function.first;
648: 				other_types.push_back(CatalogTypeToString(function_type));
649: 			}
650: 			if (!extension_name.empty()) {
651: 				break;
652: 			}
653: 			if (other_types.size() == 1) {
654: 				auto &function_type = other_types[0];
655: 				auto error =
656: 				    CatalogException("%s with name \"%s\" is not in the catalog, a function by this name exists "
657: 				                     "in the %s extension, but it's of a different type, namely %s",
658: 				                     CatalogTypeToString(type), entry_name, extension_for_error, function_type);
659: 				return error;
660: 			} else {
661: 				D_ASSERT(!other_types.empty());
662: 				auto list_of_types = StringUtil::Join(other_types, ", ");
663: 				auto error =
664: 				    CatalogException("%s with name \"%s\" is not in the catalog, functions with this name exist "
665: 				                     "in the %s extension, but they are of different types, namely %s",
666: 				                     CatalogTypeToString(type), entry_name, extension_for_error, list_of_types);
667: 				return error;
668: 			}
669: 		} while (false);
670: 	} else if (type == CatalogType::TYPE_ENTRY) {
671: 		extension_name = ExtensionHelper::FindExtensionInEntries(entry_name, EXTENSION_TYPES);
672: 	} else if (type == CatalogType::COPY_FUNCTION_ENTRY) {
673: 		extension_name = ExtensionHelper::FindExtensionInEntries(entry_name, EXTENSION_COPY_FUNCTIONS);
674: 	} else if (type == CatalogType::COLLATION_ENTRY) {
675: 		extension_name = ExtensionHelper::FindExtensionInEntries(entry_name, EXTENSION_COLLATIONS);
676: 	}
677: 
678: 	// if we found an extension that can handle this catalog entry, create an error hinting the user
679: 	if (!extension_name.empty()) {
680: 		auto error_message = CatalogTypeToString(type) + " with name \"" + entry_name +
681: 		                     "\" is not in the catalog, but it exists in the " + extension_name + " extension.";
682: 		error_message = ExtensionHelper::AddExtensionInstallHintToErrorMsg(context, error_message, extension_name);
683: 		return CatalogException(error_message);
684: 	}
685: 
686: 	// entries in other schemas get a penalty
687: 	// however, if there is an exact match in another schema, we will always show it
688: 	static constexpr const double UNSEEN_PENALTY = 0.2;
689: 	auto unseen_entries = SimilarEntriesInSchemas(context, entry_name, type, unseen_schemas);
690: 	vector<string> suggestions;
691: 	if (!unseen_entries.empty() && (unseen_entries[0].score == 1.0 || unseen_entries[0].score - UNSEEN_PENALTY >
692: 	                                                                      (entries.empty() ? 0.0 : entries[0].score))) {
693: 		// the closest matching entry requires qualification as it is not in the default search path
694: 		// check how to minimally qualify this entry
695: 		for (auto &unseen_entry : unseen_entries) {
696: 			auto catalog_name = unseen_entry.schema->catalog.GetName();
697: 			auto schema_name = unseen_entry.schema->name;
698: 			bool qualify_database;
699: 			bool qualify_schema;
700: 			FindMinimalQualification(retriever, catalog_name, schema_name, qualify_database, qualify_schema);
701: 			suggestions.push_back(unseen_entry.GetQualifiedName(qualify_database, qualify_schema));
702: 		}
703: 	} else if (!entries.empty()) {
704: 		for (auto &entry : entries) {
705: 			suggestions.push_back(entry.name);
706: 		}
707: 	}
708: 
709: 	string did_you_mean;
710: 	std::sort(suggestions.begin(), suggestions.end());
711: 	if (suggestions.size() > 2) {
712: 		auto last = suggestions.back();
713: 		suggestions.pop_back();
714: 		did_you_mean = StringUtil::Join(suggestions, ", ") + ", or " + last;
715: 	} else {
716: 		did_you_mean = StringUtil::Join(suggestions, " or ");
717: 	}
718: 
719: 	return CatalogException::MissingEntry(type, entry_name, did_you_mean, error_context);
720: }
721: 
722: CatalogEntryLookup Catalog::TryLookupEntryInternal(CatalogTransaction transaction, CatalogType type,
723:                                                    const string &schema, const string &name) {
724: 	auto schema_entry = GetSchema(transaction, schema, OnEntryNotFound::RETURN_NULL);
725: 	if (!schema_entry) {
726: 		return {nullptr, nullptr, ErrorData()};
727: 	}
728: 	auto entry = schema_entry->GetEntry(transaction, type, name);
729: 	if (!entry) {
730: 		return {schema_entry, nullptr, ErrorData()};
731: 	}
732: 	return {schema_entry, entry, ErrorData()};
733: }
734: 
735: CatalogEntryLookup Catalog::TryLookupEntry(CatalogEntryRetriever &retriever, CatalogType type, const string &schema,
736:                                            const string &name, OnEntryNotFound if_not_found,
737:                                            QueryErrorContext error_context) {
738: 	auto &context = retriever.GetContext();
739: 	reference_set_t<SchemaCatalogEntry> schemas;
740: 	if (IsInvalidSchema(schema)) {
741: 		// try all schemas for this catalog
742: 		auto entries = GetCatalogEntries(retriever, GetName(), INVALID_SCHEMA);
743: 		for (auto &entry : entries) {
744: 			auto &candidate_schema = entry.schema;
745: 			auto transaction = GetCatalogTransaction(context);
746: 			auto result = TryLookupEntryInternal(transaction, type, candidate_schema, name);
747: 			if (result.Found()) {
748: 				return result;
749: 			}
750: 			if (result.schema) {
751: 				schemas.insert(*result.schema);
752: 			}
753: 		}
754: 	} else {
755: 		auto transaction = GetCatalogTransaction(context);
756: 		auto result = TryLookupEntryInternal(transaction, type, schema, name);
757: 		if (result.Found()) {
758: 			return result;
759: 		}
760: 		if (result.schema) {
761: 			schemas.insert(*result.schema);
762: 		}
763: 	}
764: 
765: 	if (if_not_found == OnEntryNotFound::RETURN_NULL) {
766: 		return {nullptr, nullptr, ErrorData()};
767: 	} else {
768: 		auto except = CreateMissingEntryException(retriever, name, type, schemas, error_context);
769: 		return {nullptr, nullptr, ErrorData(except)};
770: 	}
771: }
772: 
773: CatalogEntryLookup Catalog::LookupEntry(CatalogEntryRetriever &retriever, CatalogType type, const string &schema,
774:                                         const string &name, OnEntryNotFound if_not_found,
775:                                         QueryErrorContext error_context) {
776: 	auto res = TryLookupEntry(retriever, type, schema, name, if_not_found, error_context);
777: 
778: 	if (res.error.HasError()) {
779: 		res.error.Throw();
780: 	}
781: 
782: 	return res;
783: }
784: 
785: CatalogEntryLookup Catalog::TryLookupEntry(CatalogEntryRetriever &retriever, vector<CatalogLookup> &lookups,
786:                                            CatalogType type, const string &name, OnEntryNotFound if_not_found,
787:                                            QueryErrorContext error_context) {
788: 	auto &context = retriever.GetContext();
789: 	reference_set_t<SchemaCatalogEntry> schemas;
790: 	for (auto &lookup : lookups) {
791: 		auto transaction = lookup.catalog.GetCatalogTransaction(context);
792: 		auto result = lookup.catalog.TryLookupEntryInternal(transaction, type, lookup.schema, lookup.name);
793: 		if (result.Found()) {
794: 			return result;
795: 		}
796: 		if (result.schema) {
797: 			schemas.insert(*result.schema);
798: 		}
799: 	}
800: 
801: 	if (if_not_found == OnEntryNotFound::RETURN_NULL) {
802: 		return {nullptr, nullptr, ErrorData()};
803: 	} else {
804: 		auto except = CreateMissingEntryException(retriever, name, type, schemas, error_context);
805: 		return {nullptr, nullptr, ErrorData(except)};
806: 	}
807: }
808: 
809: CatalogEntryLookup Catalog::TryLookupDefaultTable(CatalogEntryRetriever &retriever, CatalogType type,
810:                                                   const string &catalog, const string &schema, const string &name,
811:                                                   OnEntryNotFound if_not_found, QueryErrorContext error_context) {
812: 	// Default tables of catalogs can only be accessed by the catalog name directly
813: 	if (!schema.empty() || !catalog.empty()) {
814: 		return {nullptr, nullptr, ErrorData()};
815: 	}
816: 
817: 	vector<CatalogLookup> catalog_by_name_lookups;
818: 	auto catalog_by_name = GetCatalogEntry(retriever, name);
819: 	if (catalog_by_name && catalog_by_name->HasDefaultTable()) {
820: 		catalog_by_name_lookups.emplace_back(*catalog_by_name, catalog_by_name->GetDefaultTableSchema(),
821: 		                                     catalog_by_name->GetDefaultTable());
822: 	}
823: 
824: 	return TryLookupEntry(retriever, catalog_by_name_lookups, type, name, if_not_found, error_context);
825: }
826: 
827: static void ThrowDefaultTableAmbiguityException(CatalogEntryLookup &base_lookup, CatalogEntryLookup &default_table,
828:                                                 const string &name) {
829: 	auto entry_type = CatalogTypeToString(base_lookup.entry->type);
830: 	string fully_qualified_name_hint;
831: 	if (base_lookup.schema) {
832: 		fully_qualified_name_hint = StringUtil::Format(": '%s.%s.%s'", base_lookup.schema->catalog.GetName(),
833: 		                                               base_lookup.schema->name, base_lookup.entry->name);
834: 	}
835: 	string fully_qualified_catalog_name_hint = StringUtil::Format(
836: 	    ": '%s.%s.%s'", default_table.schema->catalog.GetName(), default_table.schema->name, default_table.entry->name);
837: 	throw CatalogException(
838: 	    "Ambiguity detected for '%s': this could either refer to the '%s' '%s', or the "
839: 	    "attached catalog '%s' which has a default table. To avoid this error, either detach the catalog and "
840: 	    "reattach under a different name, or use a fully qualified name for the '%s'%s or for the Catalog "
841: 	    "Default Table%s.",
842: 	    name, entry_type, name, name, entry_type, fully_qualified_name_hint, fully_qualified_catalog_name_hint);
843: }
844: 
845: CatalogEntryLookup Catalog::TryLookupEntry(CatalogEntryRetriever &retriever, CatalogType type, const string &catalog,
846:                                            const string &schema, const string &name, OnEntryNotFound if_not_found,
847:                                            QueryErrorContext error_context) {
848: 	auto entries = GetCatalogEntries(retriever, catalog, schema);
849: 	vector<CatalogLookup> lookups;
850: 	vector<CatalogLookup> final_lookups;
851: 	lookups.reserve(entries.size());
852: 	for (auto &entry : entries) {
853: 		optional_ptr<Catalog> catalog_entry;
854: 		if (if_not_found == OnEntryNotFound::RETURN_NULL) {
855: 			catalog_entry = Catalog::GetCatalogEntry(retriever, entry.catalog);
856: 		} else {
857: 			catalog_entry = &Catalog::GetCatalog(retriever, entry.catalog);
858: 		}
859: 		if (!catalog_entry) {
860: 			return {nullptr, nullptr, ErrorData()};
861: 		}
862: 		D_ASSERT(catalog_entry);
863: 		auto lookup_behavior = catalog_entry->CatalogTypeLookupRule(type);
864: 		if (lookup_behavior == CatalogLookupBehavior::STANDARD) {
865: 			lookups.emplace_back(*catalog_entry, entry.schema, name);
866: 		} else if (lookup_behavior == CatalogLookupBehavior::LOWER_PRIORITY) {
867: 			final_lookups.emplace_back(*catalog_entry, entry.schema, name);
868: 		}
869: 	}
870: 
871: 	for (auto &lookup : final_lookups) {
872: 		lookups.emplace_back(std::move(lookup));
873: 	}
874: 
875: 	// Do the main lookup
876: 	auto lookup_result = TryLookupEntry(retriever, lookups, type, name, if_not_found, error_context);
877: 
878: 	// Special case for tables: we do a second lookup searching for catalogs with default tables that also match this
879: 	// lookup
880: 	if (type == CatalogType::TABLE_ENTRY) {
881: 		auto lookup_result_default_table =
882: 		    TryLookupDefaultTable(retriever, type, catalog, schema, name, OnEntryNotFound::RETURN_NULL, error_context);
883: 
884: 		if (lookup_result_default_table.Found() && lookup_result.Found()) {
885: 			ThrowDefaultTableAmbiguityException(lookup_result, lookup_result_default_table, name);
886: 		}
887: 
888: 		if (lookup_result_default_table.Found()) {
889: 			return lookup_result_default_table;
890: 		}
891: 	}
892: 
893: 	return lookup_result;
894: }
895: 
896: optional_ptr<CatalogEntry> Catalog::GetEntry(CatalogEntryRetriever &retriever, CatalogType type,
897:                                              const string &schema_name, const string &name,
898:                                              OnEntryNotFound if_not_found, QueryErrorContext error_context) {
899: 	auto lookup_entry = TryLookupEntry(retriever, type, schema_name, name, if_not_found, error_context);
900: 
901: 	// Try autoloading extension to resolve lookup
902: 	if (!lookup_entry.Found()) {
903: 		if (AutoLoadExtensionByCatalogEntry(*retriever.GetContext().db, type, name)) {
904: 			lookup_entry = TryLookupEntry(retriever, type, schema_name, name, if_not_found, error_context);
905: 		}
906: 	}
907: 
908: 	if (lookup_entry.error.HasError()) {
909: 		lookup_entry.error.Throw();
910: 	}
911: 
912: 	return lookup_entry.entry.get();
913: }
914: 
915: optional_ptr<CatalogEntry> Catalog::GetEntry(ClientContext &context, CatalogType type, const string &schema_name,
916:                                              const string &name, OnEntryNotFound if_not_found,
917:                                              QueryErrorContext error_context) {
918: 	CatalogEntryRetriever retriever(context);
919: 	return GetEntry(retriever, type, schema_name, name, if_not_found, error_context);
920: }
921: 
922: CatalogEntry &Catalog::GetEntry(ClientContext &context, CatalogType type, const string &schema, const string &name,
923:                                 QueryErrorContext error_context) {
924: 	return *Catalog::GetEntry(context, type, schema, name, OnEntryNotFound::THROW_EXCEPTION, error_context);
925: }
926: 
927: optional_ptr<CatalogEntry> Catalog::GetEntry(CatalogEntryRetriever &retriever, CatalogType type, const string &catalog,
928:                                              const string &schema, const string &name, OnEntryNotFound if_not_found,
929:                                              QueryErrorContext error_context) {
930: 	auto result = TryLookupEntry(retriever, type, catalog, schema, name, if_not_found, error_context);
931: 
932: 	// Try autoloading extension to resolve lookup
933: 	if (!result.Found()) {
934: 		if (AutoLoadExtensionByCatalogEntry(*retriever.GetContext().db, type, name)) {
935: 			result = TryLookupEntry(retriever, type, catalog, schema, name, if_not_found, error_context);
936: 		}
937: 	}
938: 
939: 	if (result.error.HasError()) {
940: 		result.error.Throw();
941: 	}
942: 
943: 	if (!result.Found()) {
944: 		D_ASSERT(if_not_found == OnEntryNotFound::RETURN_NULL);
945: 		return nullptr;
946: 	}
947: 	return result.entry.get();
948: }
949: optional_ptr<CatalogEntry> Catalog::GetEntry(ClientContext &context, CatalogType type, const string &catalog,
950:                                              const string &schema, const string &name, OnEntryNotFound if_not_found,
951:                                              QueryErrorContext error_context) {
952: 	CatalogEntryRetriever retriever(context);
953: 	return GetEntry(retriever, type, catalog, schema, name, if_not_found, error_context);
954: }
955: 
956: CatalogEntry &Catalog::GetEntry(ClientContext &context, CatalogType type, const string &catalog, const string &schema,
957:                                 const string &name, QueryErrorContext error_context) {
958: 	return *Catalog::GetEntry(context, type, catalog, schema, name, OnEntryNotFound::THROW_EXCEPTION, error_context);
959: }
960: 
961: optional_ptr<SchemaCatalogEntry> Catalog::GetSchema(CatalogEntryRetriever &retriever, const string &catalog_name,
962:                                                     const string &schema_name, OnEntryNotFound if_not_found,
963:                                                     QueryErrorContext error_context) {
964: 	auto entries = GetCatalogEntries(retriever, catalog_name, schema_name);
965: 	for (idx_t i = 0; i < entries.size(); i++) {
966: 		auto catalog = Catalog::GetCatalogEntry(retriever, entries[i].catalog);
967: 		if (!catalog) {
968: 			// skip if it is not an attached database
969: 			continue;
970: 		}
971: 		auto on_not_found = i + 1 == entries.size() ? if_not_found : OnEntryNotFound::RETURN_NULL;
972: 		auto result = catalog->GetSchema(retriever.GetContext(), schema_name, on_not_found, error_context);
973: 		if (result) {
974: 			return result;
975: 		}
976: 	}
977: 	return nullptr;
978: }
979: 
980: optional_ptr<SchemaCatalogEntry> Catalog::GetSchema(ClientContext &context, const string &catalog_name,
981:                                                     const string &schema_name, OnEntryNotFound if_not_found,
982:                                                     QueryErrorContext error_context) {
983: 	CatalogEntryRetriever retriever(context);
984: 	return GetSchema(retriever, catalog_name, schema_name, if_not_found, error_context);
985: }
986: 
987: vector<reference<SchemaCatalogEntry>> Catalog::GetSchemas(ClientContext &context) {
988: 	vector<reference<SchemaCatalogEntry>> schemas;
989: 	ScanSchemas(context, [&](SchemaCatalogEntry &entry) { schemas.push_back(entry); });
990: 	return schemas;
991: }
992: 
993: vector<reference<SchemaCatalogEntry>> Catalog::GetSchemas(CatalogEntryRetriever &retriever,
994:                                                           const string &catalog_name) {
995: 	vector<reference<Catalog>> catalogs;
996: 	if (IsInvalidCatalog(catalog_name)) {
997: 		reference_set_t<Catalog> inserted_catalogs;
998: 
999: 		auto &search_path = retriever.GetSearchPath();
1000: 		for (auto &entry : search_path.Get()) {
1001: 			auto &catalog = Catalog::GetCatalog(retriever, entry.catalog);
1002: 			if (inserted_catalogs.find(catalog) != inserted_catalogs.end()) {
1003: 				continue;
1004: 			}
1005: 			inserted_catalogs.insert(catalog);
1006: 			catalogs.push_back(catalog);
1007: 		}
1008: 	} else {
1009: 		catalogs.push_back(Catalog::GetCatalog(retriever, catalog_name));
1010: 	}
1011: 	vector<reference<SchemaCatalogEntry>> result;
1012: 	for (auto catalog : catalogs) {
1013: 		auto schemas = catalog.get().GetSchemas(retriever.GetContext());
1014: 		result.insert(result.end(), schemas.begin(), schemas.end());
1015: 	}
1016: 	return result;
1017: }
1018: 
1019: vector<reference<SchemaCatalogEntry>> Catalog::GetSchemas(ClientContext &context, const string &catalog_name) {
1020: 	CatalogEntryRetriever retriever(context);
1021: 	return GetSchemas(retriever, catalog_name);
1022: }
1023: 
1024: vector<reference<SchemaCatalogEntry>> Catalog::GetAllSchemas(ClientContext &context) {
1025: 	vector<reference<SchemaCatalogEntry>> result;
1026: 
1027: 	auto &db_manager = DatabaseManager::Get(context);
1028: 	auto databases = db_manager.GetDatabases(context);
1029: 	for (auto database : databases) {
1030: 		auto &catalog = database.get().GetCatalog();
1031: 		auto new_schemas = catalog.GetSchemas(context);
1032: 		result.insert(result.end(), new_schemas.begin(), new_schemas.end());
1033: 	}
1034: 	sort(result.begin(), result.end(),
1035: 	     [&](reference<SchemaCatalogEntry> left_p, reference<SchemaCatalogEntry> right_p) {
1036: 		     auto &left = left_p.get();
1037: 		     auto &right = right_p.get();
1038: 		     if (left.catalog.GetName() < right.catalog.GetName()) {
1039: 			     return true;
1040: 		     }
1041: 		     if (left.catalog.GetName() == right.catalog.GetName()) {
1042: 			     return left.name < right.name;
1043: 		     }
1044: 		     return false;
1045: 	     });
1046: 
1047: 	return result;
1048: }
1049: 
1050: void Catalog::Alter(CatalogTransaction transaction, AlterInfo &info) {
1051: 	if (transaction.HasContext()) {
1052: 		CatalogEntryRetriever retriever(transaction.GetContext());
1053: 		auto lookup = LookupEntry(retriever, info.GetCatalogType(), info.schema, info.name, info.if_not_found);
1054: 		if (!lookup.Found()) {
1055: 			return;
1056: 		}
1057: 		return lookup.schema->Alter(transaction, info);
1058: 	}
1059: 	D_ASSERT(info.if_not_found == OnEntryNotFound::THROW_EXCEPTION);
1060: 	auto &schema = GetSchema(transaction, info.schema);
1061: 	return schema.Alter(transaction, info);
1062: }
1063: 
1064: void Catalog::Alter(ClientContext &context, AlterInfo &info) {
1065: 	Alter(GetCatalogTransaction(context), info);
1066: }
1067: 
1068: vector<MetadataBlockInfo> Catalog::GetMetadataInfo(ClientContext &context) {
1069: 	return vector<MetadataBlockInfo>();
1070: }
1071: 
1072: optional_ptr<DependencyManager> Catalog::GetDependencyManager() {
1073: 	return nullptr;
1074: }
1075: 
1076: //! Whether this catalog has a default table. Catalogs with a default table can be queries by their catalog name
1077: bool Catalog::HasDefaultTable() const {
1078: 	return !default_table.empty();
1079: }
1080: 
1081: void Catalog::SetDefaultTable(const string &schema, const string &name) {
1082: 	default_table = name;
1083: 	default_table_schema = schema;
1084: }
1085: 
1086: string Catalog::GetDefaultTable() const {
1087: 	return default_table;
1088: }
1089: 
1090: string Catalog::GetDefaultTableSchema() const {
1091: 	return !default_table_schema.empty() ? default_table_schema : DEFAULT_SCHEMA;
1092: }
1093: 
1094: void Catalog::Verify() {
1095: }
1096: 
1097: bool Catalog::IsSystemCatalog() const {
1098: 	return db.IsSystem();
1099: }
1100: 
1101: bool Catalog::IsTemporaryCatalog() const {
1102: 	return db.IsTemporary();
1103: }
1104: 
1105: } // namespace duckdb
[end of src/catalog/catalog.cpp]
[start of src/catalog/catalog_search_path.cpp]
1: #include "duckdb/catalog/catalog_search_path.hpp"
2: #include "duckdb/catalog/default/default_schemas.hpp"
3: 
4: #include "duckdb/catalog/catalog.hpp"
5: #include "duckdb/common/constants.hpp"
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/common/string_util.hpp"
8: #include "duckdb/main/client_context.hpp"
9: #include "duckdb/main/database_manager.hpp"
10: 
11: namespace duckdb {
12: 
13: CatalogSearchEntry::CatalogSearchEntry(string catalog_p, string schema_p)
14:     : catalog(std::move(catalog_p)), schema(std::move(schema_p)) {
15: }
16: 
17: string CatalogSearchEntry::ToString() const {
18: 	if (catalog.empty()) {
19: 		return WriteOptionallyQuoted(schema);
20: 	} else {
21: 		return WriteOptionallyQuoted(catalog) + "." + WriteOptionallyQuoted(schema);
22: 	}
23: }
24: 
25: string CatalogSearchEntry::WriteOptionallyQuoted(const string &input) {
26: 	for (idx_t i = 0; i < input.size(); i++) {
27: 		if (input[i] == '.' || input[i] == ',') {
28: 			return "\"" + input + "\"";
29: 		}
30: 	}
31: 	return input;
32: }
33: 
34: string CatalogSearchEntry::ListToString(const vector<CatalogSearchEntry> &input) {
35: 	string result;
36: 	for (auto &entry : input) {
37: 		if (!result.empty()) {
38: 			result += ",";
39: 		}
40: 		result += entry.ToString();
41: 	}
42: 	return result;
43: }
44: 
45: CatalogSearchEntry CatalogSearchEntry::ParseInternal(const string &input, idx_t &idx) {
46: 	string catalog;
47: 	string schema;
48: 	string entry;
49: 	bool finished = false;
50: normal:
51: 	for (; idx < input.size(); idx++) {
52: 		if (input[idx] == '"') {
53: 			idx++;
54: 			goto quoted;
55: 		} else if (input[idx] == '.') {
56: 			goto separator;
57: 		} else if (input[idx] == ',') {
58: 			finished = true;
59: 			goto separator;
60: 		}
61: 		entry += input[idx];
62: 	}
63: 	finished = true;
64: 	goto separator;
65: quoted:
66: 	//! look for another quote
67: 	for (; idx < input.size(); idx++) {
68: 		if (input[idx] == '"') {
69: 			//! unquote
70: 			idx++;
71: 			if (idx < input.size() && input[idx] == '"') {
72: 				// escaped quote
73: 				entry += input[idx];
74: 				continue;
75: 			}
76: 			goto normal;
77: 		}
78: 		entry += input[idx];
79: 	}
80: 	throw ParserException("Unterminated quote in qualified name!");
81: separator:
82: 	if (entry.empty()) {
83: 		throw ParserException("Unexpected dot - empty CatalogSearchEntry");
84: 	}
85: 	if (schema.empty()) {
86: 		// if we parse one entry it is the schema
87: 		schema = std::move(entry);
88: 	} else if (catalog.empty()) {
89: 		// if we parse two entries it is [catalog.schema]
90: 		catalog = std::move(schema);
91: 		schema = std::move(entry);
92: 	} else {
93: 		throw ParserException("Too many dots - expected [schema] or [catalog.schema] for CatalogSearchEntry");
94: 	}
95: 	entry = "";
96: 	idx++;
97: 	if (finished) {
98: 		goto final;
99: 	}
100: 	goto normal;
101: final:
102: 	if (schema.empty()) {
103: 		throw ParserException("Unexpected end of entry - empty CatalogSearchEntry");
104: 	}
105: 	return CatalogSearchEntry(std::move(catalog), std::move(schema));
106: }
107: 
108: CatalogSearchEntry CatalogSearchEntry::Parse(const string &input) {
109: 	idx_t pos = 0;
110: 	auto result = ParseInternal(input, pos);
111: 	if (pos < input.size()) {
112: 		throw ParserException("Failed to convert entry \"%s\" to CatalogSearchEntry - expected a single entry", input);
113: 	}
114: 	return result;
115: }
116: 
117: vector<CatalogSearchEntry> CatalogSearchEntry::ParseList(const string &input) {
118: 	idx_t pos = 0;
119: 	vector<CatalogSearchEntry> result;
120: 	while (pos < input.size()) {
121: 		auto entry = ParseInternal(input, pos);
122: 		result.push_back(entry);
123: 	}
124: 	return result;
125: }
126: 
127: CatalogSearchPath::CatalogSearchPath(ClientContext &context_p, vector<CatalogSearchEntry> entries)
128:     : context(context_p) {
129: 	SetPathsInternal(std::move(entries));
130: }
131: 
132: CatalogSearchPath::CatalogSearchPath(ClientContext &context_p) : CatalogSearchPath(context_p, {}) {
133: }
134: 
135: void CatalogSearchPath::Reset() {
136: 	vector<CatalogSearchEntry> empty;
137: 	SetPathsInternal(empty);
138: }
139: 
140: string CatalogSearchPath::GetSetName(CatalogSetPathType set_type) {
141: 	switch (set_type) {
142: 	case CatalogSetPathType::SET_SCHEMA:
143: 		return "SET schema";
144: 	case CatalogSetPathType::SET_SCHEMAS:
145: 		return "SET search_path";
146: 	default:
147: 		throw InternalException("Unrecognized CatalogSetPathType");
148: 	}
149: }
150: 
151: void CatalogSearchPath::Set(vector<CatalogSearchEntry> new_paths, CatalogSetPathType set_type) {
152: 	if (set_type != CatalogSetPathType::SET_SCHEMAS && new_paths.size() != 1) {
153: 		throw CatalogException("%s can set only 1 schema. This has %d", GetSetName(set_type), new_paths.size());
154: 	}
155: 	for (auto &path : new_paths) {
156: 		auto schema_entry = Catalog::GetSchema(context, path.catalog, path.schema, OnEntryNotFound::RETURN_NULL);
157: 		if (schema_entry) {
158: 			// we are setting a schema - update the catalog and schema
159: 			if (path.catalog.empty()) {
160: 				path.catalog = GetDefault().catalog;
161: 			}
162: 			continue;
163: 		}
164: 		// only schema supplied - check if this is a catalog instead
165: 		if (path.catalog.empty()) {
166: 			auto catalog = Catalog::GetCatalogEntry(context, path.schema);
167: 			if (catalog) {
168: 				auto schema = catalog->GetSchema(context, DEFAULT_SCHEMA, OnEntryNotFound::RETURN_NULL);
169: 				if (schema) {
170: 					path.catalog = std::move(path.schema);
171: 					path.schema = schema->name;
172: 					continue;
173: 				}
174: 			}
175: 		}
176: 		throw CatalogException("%s: No catalog + schema named \"%s\" found.", GetSetName(set_type), path.ToString());
177: 	}
178: 	if (set_type == CatalogSetPathType::SET_SCHEMA) {
179: 		if (new_paths[0].catalog == TEMP_CATALOG || new_paths[0].catalog == SYSTEM_CATALOG) {
180: 			throw CatalogException("%s cannot be set to internal schema \"%s\"", GetSetName(set_type),
181: 			                       new_paths[0].catalog);
182: 		}
183: 	}
184: 	SetPathsInternal(std::move(new_paths));
185: }
186: 
187: void CatalogSearchPath::Set(CatalogSearchEntry new_value, CatalogSetPathType set_type) {
188: 	vector<CatalogSearchEntry> new_paths {std::move(new_value)};
189: 	Set(std::move(new_paths), set_type);
190: }
191: 
192: const vector<CatalogSearchEntry> &CatalogSearchPath::Get() {
193: 	return paths;
194: }
195: 
196: string CatalogSearchPath::GetDefaultSchema(const string &catalog) {
197: 	for (auto &path : paths) {
198: 		if (path.catalog == TEMP_CATALOG) {
199: 			continue;
200: 		}
201: 		if (StringUtil::CIEquals(path.catalog, catalog)) {
202: 			return path.schema;
203: 		}
204: 	}
205: 	return DEFAULT_SCHEMA;
206: }
207: 
208: string CatalogSearchPath::GetDefaultCatalog(const string &schema) {
209: 	if (DefaultSchemaGenerator::IsDefaultSchema(schema)) {
210: 		return SYSTEM_CATALOG;
211: 	}
212: 	for (auto &path : paths) {
213: 		if (path.catalog == TEMP_CATALOG) {
214: 			continue;
215: 		}
216: 		if (StringUtil::CIEquals(path.schema, schema)) {
217: 			return path.catalog;
218: 		}
219: 	}
220: 	return INVALID_CATALOG;
221: }
222: 
223: vector<string> CatalogSearchPath::GetCatalogsForSchema(const string &schema) {
224: 	vector<string> catalogs;
225: 	if (DefaultSchemaGenerator::IsDefaultSchema(schema)) {
226: 		catalogs.push_back(SYSTEM_CATALOG);
227: 	} else {
228: 		for (auto &path : paths) {
229: 			if (StringUtil::CIEquals(path.schema, schema)) {
230: 				catalogs.push_back(path.catalog);
231: 			}
232: 		}
233: 	}
234: 	return catalogs;
235: }
236: 
237: vector<string> CatalogSearchPath::GetSchemasForCatalog(const string &catalog) {
238: 	vector<string> schemas;
239: 	for (auto &path : paths) {
240: 		if (StringUtil::CIEquals(path.catalog, catalog)) {
241: 			schemas.push_back(path.schema);
242: 		}
243: 	}
244: 	return schemas;
245: }
246: 
247: const CatalogSearchEntry &CatalogSearchPath::GetDefault() {
248: 	const auto &paths = Get();
249: 	D_ASSERT(paths.size() >= 2);
250: 	return paths[1];
251: }
252: 
253: void CatalogSearchPath::SetPathsInternal(vector<CatalogSearchEntry> new_paths) {
254: 	this->set_paths = std::move(new_paths);
255: 
256: 	paths.clear();
257: 	paths.reserve(set_paths.size() + 3);
258: 	paths.emplace_back(TEMP_CATALOG, DEFAULT_SCHEMA);
259: 	for (auto &path : set_paths) {
260: 		paths.push_back(path);
261: 	}
262: 	paths.emplace_back(INVALID_CATALOG, DEFAULT_SCHEMA);
263: 	paths.emplace_back(SYSTEM_CATALOG, DEFAULT_SCHEMA);
264: 	paths.emplace_back(SYSTEM_CATALOG, "pg_catalog");
265: }
266: 
267: bool CatalogSearchPath::SchemaInSearchPath(ClientContext &context, const string &catalog_name,
268:                                            const string &schema_name) {
269: 	for (auto &path : paths) {
270: 		if (!StringUtil::CIEquals(path.schema, schema_name)) {
271: 			continue;
272: 		}
273: 		if (StringUtil::CIEquals(path.catalog, catalog_name)) {
274: 			return true;
275: 		}
276: 		if (IsInvalidCatalog(path.catalog) &&
277: 		    StringUtil::CIEquals(catalog_name, DatabaseManager::GetDefaultDatabase(context))) {
278: 			return true;
279: 		}
280: 	}
281: 	return false;
282: }
283: 
284: } // namespace duckdb
[end of src/catalog/catalog_search_path.cpp]
[start of src/common/random_engine.cpp]
1: #include "duckdb/common/random_engine.hpp"
2: #include "duckdb/common/numeric_utils.hpp"
3: #include "pcg_random.hpp"
4: 
5: #ifdef __linux__
6: #include <sys/syscall.h>
7: #include <unistd.h>
8: #else
9: #include <random>
10: #endif
11: namespace duckdb {
12: 
13: struct RandomState {
14: 	RandomState() {
15: 	}
16: 
17: 	pcg32 pcg;
18: };
19: 
20: RandomEngine::RandomEngine(int64_t seed) : random_state(make_uniq<RandomState>()) {
21: 	if (seed < 0) {
22: #ifdef __linux__
23: 		idx_t random_seed = 0;
24: 		auto result = syscall(SYS_getrandom, &random_seed, sizeof(random_seed), 0);
25: 		if (result == -1) {
26: 			// Something went wrong with the syscall, we use chrono
27: 			const auto now = std::chrono::high_resolution_clock::now();
28: 			random_seed = now.time_since_epoch().count();
29: 		}
30: 		random_state->pcg.seed(random_seed);
31: #else
32: 		random_state->pcg.seed(pcg_extras::seed_seq_from<std::random_device>());
33: #endif
34: 	} else {
35: 		random_state->pcg.seed(NumericCast<uint64_t>(seed));
36: 	}
37: }
38: 
39: RandomEngine::~RandomEngine() {
40: }
41: 
42: double RandomEngine::NextRandom(double min, double max) {
43: 	D_ASSERT(max >= min);
44: 	return min + (NextRandom() * (max - min));
45: }
46: 
47: double RandomEngine::NextRandom() {
48: 	auto uint64 = NextRandomInteger64();
49: 	return std::ldexp(uint64, -64);
50: }
51: 
52: double RandomEngine::NextRandom32(double min, double max) {
53: 	D_ASSERT(max >= min);
54: 	return min + (NextRandom32() * (max - min));
55: }
56: 
57: double RandomEngine::NextRandom32() {
58: 	auto uint32 = NextRandomInteger();
59: 	return std::ldexp(uint32, -32);
60: }
61: 
62: uint32_t RandomEngine::NextRandomInteger() {
63: 	return random_state->pcg();
64: }
65: 
66: uint64_t RandomEngine::NextRandomInteger64() {
67: 	return (static_cast<uint64_t>(NextRandomInteger()) << UINT64_C(32)) | static_cast<uint64_t>(NextRandomInteger());
68: }
69: 
70: uint32_t RandomEngine::NextRandomInteger(uint32_t min, uint32_t max) {
71: 	return min + static_cast<uint32_t>(NextRandom() * double(max - min));
72: }
73: 
74: uint32_t RandomEngine::NextRandomInteger32(uint32_t min, uint32_t max) {
75: 	return min + static_cast<uint32_t>(NextRandom32() * double(max - min));
76: }
77: 
78: void RandomEngine::SetSeed(uint64_t seed) {
79: 	random_state->pcg.seed(seed);
80: }
81: 
82: } // namespace duckdb
[end of src/common/random_engine.cpp]
[start of src/execution/index/art/art.cpp]
1: #include "duckdb/execution/index/art/art.hpp"
2: 
3: #include "duckdb/common/types/conflict_manager.hpp"
4: #include "duckdb/common/unordered_map.hpp"
5: #include "duckdb/common/vector_operations/vector_operations.hpp"
6: #include "duckdb/execution/expression_executor.hpp"
7: #include "duckdb/execution/index/art/art_key.hpp"
8: #include "duckdb/execution/index/art/base_leaf.hpp"
9: #include "duckdb/execution/index/art/base_node.hpp"
10: #include "duckdb/execution/index/art/iterator.hpp"
11: #include "duckdb/execution/index/art/leaf.hpp"
12: #include "duckdb/execution/index/art/node256.hpp"
13: #include "duckdb/execution/index/art/node256_leaf.hpp"
14: #include "duckdb/execution/index/art/node48.hpp"
15: #include "duckdb/execution/index/art/prefix.hpp"
16: #include "duckdb/optimizer/matcher/expression_matcher.hpp"
17: #include "duckdb/planner/expression/bound_between_expression.hpp"
18: #include "duckdb/planner/expression/bound_comparison_expression.hpp"
19: #include "duckdb/planner/expression/bound_constant_expression.hpp"
20: #include "duckdb/storage/arena_allocator.hpp"
21: #include "duckdb/storage/metadata/metadata_reader.hpp"
22: #include "duckdb/storage/table/scan_state.hpp"
23: #include "duckdb/storage/table_io_manager.hpp"
24: 
25: namespace duckdb {
26: 
27: struct ARTIndexScanState : public IndexScanState {
28: 	//! The predicates to scan.
29: 	//! A single predicate for point lookups, and two predicates for range scans.
30: 	Value values[2];
31: 	//! The expressions over the scan predicates.
32: 	ExpressionType expressions[2];
33: 	bool checked = false;
34: 	//! All scanned row IDs.
35: 	unsafe_vector<row_t> row_ids;
36: };
37: 
38: //===--------------------------------------------------------------------===//
39: // ART
40: //===--------------------------------------------------------------------===//
41: 
42: ART::ART(const string &name, const IndexConstraintType index_constraint_type, const vector<column_t> &column_ids,
43:          TableIOManager &table_io_manager, const vector<unique_ptr<Expression>> &unbound_expressions,
44:          AttachedDatabase &db,
45:          const shared_ptr<array<unsafe_unique_ptr<FixedSizeAllocator>, ALLOCATOR_COUNT>> &allocators_ptr,
46:          const IndexStorageInfo &info)
47:     : BoundIndex(name, ART::TYPE_NAME, index_constraint_type, column_ids, table_io_manager, unbound_expressions, db),
48:       allocators(allocators_ptr), owns_data(false), append_mode(ARTAppendMode::DEFAULT) {
49: 
50: 	// FIXME: Use the new byte representation function to support nested types.
51: 	for (idx_t i = 0; i < types.size(); i++) {
52: 		switch (types[i]) {
53: 		case PhysicalType::BOOL:
54: 		case PhysicalType::INT8:
55: 		case PhysicalType::INT16:
56: 		case PhysicalType::INT32:
57: 		case PhysicalType::INT64:
58: 		case PhysicalType::INT128:
59: 		case PhysicalType::UINT8:
60: 		case PhysicalType::UINT16:
61: 		case PhysicalType::UINT32:
62: 		case PhysicalType::UINT64:
63: 		case PhysicalType::UINT128:
64: 		case PhysicalType::FLOAT:
65: 		case PhysicalType::DOUBLE:
66: 		case PhysicalType::VARCHAR:
67: 			break;
68: 		default:
69: 			throw InvalidTypeException(logical_types[i], "Invalid type for index key.");
70: 		}
71: 	}
72: 
73: 	// Initialize the allocators.
74: 	SetPrefixCount(info);
75: 	if (!allocators) {
76: 		owns_data = true;
77: 		auto prefix_size = NumericCast<idx_t>(prefix_count) + NumericCast<idx_t>(Prefix::METADATA_SIZE);
78: 		auto &block_manager = table_io_manager.GetIndexBlockManager();
79: 
80: 		array<unsafe_unique_ptr<FixedSizeAllocator>, ALLOCATOR_COUNT> allocator_array = {
81: 		    make_unsafe_uniq<FixedSizeAllocator>(prefix_size, block_manager),
82: 		    make_unsafe_uniq<FixedSizeAllocator>(sizeof(Leaf), block_manager),
83: 		    make_unsafe_uniq<FixedSizeAllocator>(sizeof(Node4), block_manager),
84: 		    make_unsafe_uniq<FixedSizeAllocator>(sizeof(Node16), block_manager),
85: 		    make_unsafe_uniq<FixedSizeAllocator>(sizeof(Node48), block_manager),
86: 		    make_unsafe_uniq<FixedSizeAllocator>(sizeof(Node256), block_manager),
87: 		    make_unsafe_uniq<FixedSizeAllocator>(sizeof(Node7Leaf), block_manager),
88: 		    make_unsafe_uniq<FixedSizeAllocator>(sizeof(Node15Leaf), block_manager),
89: 		    make_unsafe_uniq<FixedSizeAllocator>(sizeof(Node256Leaf), block_manager),
90: 		};
91: 		allocators =
92: 		    make_shared_ptr<array<unsafe_unique_ptr<FixedSizeAllocator>, ALLOCATOR_COUNT>>(std::move(allocator_array));
93: 	}
94: 
95: 	if (!info.IsValid()) {
96: 		// We create a new ART.
97: 		return;
98: 	}
99: 
100: 	if (info.root_block_ptr.IsValid()) {
101: 		// Backwards compatibility.
102: 		Deserialize(info.root_block_ptr);
103: 		return;
104: 	}
105: 
106: 	// Set the root node and initialize the allocators.
107: 	tree.Set(info.root);
108: 	InitAllocators(info);
109: }
110: 
111: //===--------------------------------------------------------------------===//
112: // Initialize Scans
113: //===--------------------------------------------------------------------===//
114: 
115: static unique_ptr<IndexScanState> InitializeScanSinglePredicate(const Value &value,
116:                                                                 const ExpressionType expression_type) {
117: 	auto result = make_uniq<ARTIndexScanState>();
118: 	result->values[0] = value;
119: 	result->expressions[0] = expression_type;
120: 	return std::move(result);
121: }
122: 
123: static unique_ptr<IndexScanState> InitializeScanTwoPredicates(const Value &low_value,
124:                                                               const ExpressionType low_expression_type,
125:                                                               const Value &high_value,
126:                                                               const ExpressionType high_expression_type) {
127: 	auto result = make_uniq<ARTIndexScanState>();
128: 	result->values[0] = low_value;
129: 	result->expressions[0] = low_expression_type;
130: 	result->values[1] = high_value;
131: 	result->expressions[1] = high_expression_type;
132: 	return std::move(result);
133: }
134: 
135: unique_ptr<IndexScanState> ART::TryInitializeScan(const Expression &expr, const Expression &filter_expr) {
136: 	Value low_value, high_value, equal_value;
137: 	ExpressionType low_comparison_type = ExpressionType::INVALID, high_comparison_type = ExpressionType::INVALID;
138: 
139: 	// Try to find a matching index for any of the filter expressions.
140: 	ComparisonExpressionMatcher matcher;
141: 
142: 	// Match on a comparison type.
143: 	matcher.expr_type = make_uniq<ComparisonExpressionTypeMatcher>();
144: 
145: 	// Match on a constant comparison with the indexed expression.
146: 	matcher.matchers.push_back(make_uniq<ExpressionEqualityMatcher>(expr));
147: 	matcher.matchers.push_back(make_uniq<ConstantExpressionMatcher>());
148: 	matcher.policy = SetMatcher::Policy::UNORDERED;
149: 
150: 	vector<reference<Expression>> bindings;
151: 	auto filter_match =
152: 	    matcher.Match(const_cast<Expression &>(filter_expr), bindings); // NOLINT: Match does not alter the expr.
153: 	if (filter_match) {
154: 		// This is a range or equality comparison with a constant value, so we can use the index.
155: 		// 		bindings[0] = the expression
156: 		// 		bindings[1] = the index expression
157: 		// 		bindings[2] = the constant
158: 		auto &comparison = bindings[0].get().Cast<BoundComparisonExpression>();
159: 		auto constant_value = bindings[2].get().Cast<BoundConstantExpression>().value;
160: 		auto comparison_type = comparison.GetExpressionType();
161: 
162: 		if (comparison.left->GetExpressionType() == ExpressionType::VALUE_CONSTANT) {
163: 			// The expression is on the right side, we flip the comparison expression.
164: 			comparison_type = FlipComparisonExpression(comparison_type);
165: 		}
166: 
167: 		if (comparison_type == ExpressionType::COMPARE_EQUAL) {
168: 			// An equality value overrides any other bounds.
169: 			equal_value = constant_value;
170: 		} else if (comparison_type == ExpressionType::COMPARE_GREATERTHANOREQUALTO ||
171: 		           comparison_type == ExpressionType::COMPARE_GREATERTHAN) {
172: 			// This is a lower bound.
173: 			low_value = constant_value;
174: 			low_comparison_type = comparison_type;
175: 		} else {
176: 			// This is an upper bound.
177: 			high_value = constant_value;
178: 			high_comparison_type = comparison_type;
179: 		}
180: 
181: 	} else if (filter_expr.GetExpressionType() == ExpressionType::COMPARE_BETWEEN) {
182: 		auto &between = filter_expr.Cast<BoundBetweenExpression>();
183: 		if (!between.input->Equals(expr)) {
184: 			// The expression does not match the index expression.
185: 			return nullptr;
186: 		}
187: 
188: 		if (between.lower->GetExpressionType() != ExpressionType::VALUE_CONSTANT ||
189: 		    between.upper->GetExpressionType() != ExpressionType::VALUE_CONSTANT) {
190: 			// Not a constant expression.
191: 			return nullptr;
192: 		}
193: 
194: 		low_value = between.lower->Cast<BoundConstantExpression>().value;
195: 		low_comparison_type = between.lower_inclusive ? ExpressionType::COMPARE_GREATERTHANOREQUALTO
196: 		                                              : ExpressionType::COMPARE_GREATERTHAN;
197: 		high_value = (between.upper->Cast<BoundConstantExpression>()).value;
198: 		high_comparison_type =
199: 		    between.upper_inclusive ? ExpressionType::COMPARE_LESSTHANOREQUALTO : ExpressionType::COMPARE_LESSTHAN;
200: 	}
201: 
202: 	// We cannot use an index scan.
203: 	if (equal_value.IsNull() && low_value.IsNull() && high_value.IsNull()) {
204: 		return nullptr;
205: 	}
206: 
207: 	// Initialize the index scan state and return it.
208: 	if (!equal_value.IsNull()) {
209: 		// Equality predicate.
210: 		return InitializeScanSinglePredicate(equal_value, ExpressionType::COMPARE_EQUAL);
211: 	}
212: 	if (!low_value.IsNull() && !high_value.IsNull()) {
213: 		// Two-sided predicate.
214: 		return InitializeScanTwoPredicates(low_value, low_comparison_type, high_value, high_comparison_type);
215: 	}
216: 	if (!low_value.IsNull()) {
217: 		// Less-than predicate.
218: 		return InitializeScanSinglePredicate(low_value, low_comparison_type);
219: 	}
220: 	// Greater-than predicate.
221: 	return InitializeScanSinglePredicate(high_value, high_comparison_type);
222: }
223: 
224: //===--------------------------------------------------------------------===//
225: // ART Keys
226: //===--------------------------------------------------------------------===//
227: 
228: template <class T, bool IS_NOT_NULL>
229: static void TemplatedGenerateKeys(ArenaAllocator &allocator, Vector &input, idx_t count, unsafe_vector<ARTKey> &keys) {
230: 	D_ASSERT(keys.size() >= count);
231: 
232: 	UnifiedVectorFormat data;
233: 	input.ToUnifiedFormat(count, data);
234: 	auto input_data = UnifiedVectorFormat::GetData<T>(data);
235: 
236: 	for (idx_t i = 0; i < count; i++) {
237: 		auto idx = data.sel->get_index(i);
238: 		if (IS_NOT_NULL || data.validity.RowIsValid(idx)) {
239: 			ARTKey::CreateARTKey<T>(allocator, keys[i], input_data[idx]);
240: 			continue;
241: 		}
242: 
243: 		// We need to reset the key value in the reusable keys vector.
244: 		keys[i] = ARTKey();
245: 	}
246: }
247: 
248: template <class T, bool IS_NOT_NULL>
249: static void ConcatenateKeys(ArenaAllocator &allocator, Vector &input, idx_t count, unsafe_vector<ARTKey> &keys) {
250: 	UnifiedVectorFormat data;
251: 	input.ToUnifiedFormat(count, data);
252: 	auto input_data = UnifiedVectorFormat::GetData<T>(data);
253: 
254: 	for (idx_t i = 0; i < count; i++) {
255: 		auto idx = data.sel->get_index(i);
256: 
257: 		if (IS_NOT_NULL) {
258: 			auto other_key = ARTKey::CreateARTKey<T>(allocator, input_data[idx]);
259: 			keys[i].Concat(allocator, other_key);
260: 			continue;
261: 		}
262: 
263: 		// A previous column entry was NULL.
264: 		if (keys[i].Empty()) {
265: 			continue;
266: 		}
267: 
268: 		// This column entry is NULL, so we set the whole key to NULL.
269: 		if (!data.validity.RowIsValid(idx)) {
270: 			keys[i] = ARTKey();
271: 			continue;
272: 		}
273: 
274: 		// Concatenate the keys.
275: 		auto other_key = ARTKey::CreateARTKey<T>(allocator, input_data[idx]);
276: 		keys[i].Concat(allocator, other_key);
277: 	}
278: }
279: 
280: template <bool IS_NOT_NULL>
281: void GenerateKeysInternal(ArenaAllocator &allocator, DataChunk &input, unsafe_vector<ARTKey> &keys) {
282: 	switch (input.data[0].GetType().InternalType()) {
283: 	case PhysicalType::BOOL:
284: 		TemplatedGenerateKeys<bool, IS_NOT_NULL>(allocator, input.data[0], input.size(), keys);
285: 		break;
286: 	case PhysicalType::INT8:
287: 		TemplatedGenerateKeys<int8_t, IS_NOT_NULL>(allocator, input.data[0], input.size(), keys);
288: 		break;
289: 	case PhysicalType::INT16:
290: 		TemplatedGenerateKeys<int16_t, IS_NOT_NULL>(allocator, input.data[0], input.size(), keys);
291: 		break;
292: 	case PhysicalType::INT32:
293: 		TemplatedGenerateKeys<int32_t, IS_NOT_NULL>(allocator, input.data[0], input.size(), keys);
294: 		break;
295: 	case PhysicalType::INT64:
296: 		TemplatedGenerateKeys<int64_t, IS_NOT_NULL>(allocator, input.data[0], input.size(), keys);
297: 		break;
298: 	case PhysicalType::INT128:
299: 		TemplatedGenerateKeys<hugeint_t, IS_NOT_NULL>(allocator, input.data[0], input.size(), keys);
300: 		break;
301: 	case PhysicalType::UINT8:
302: 		TemplatedGenerateKeys<uint8_t, IS_NOT_NULL>(allocator, input.data[0], input.size(), keys);
303: 		break;
304: 	case PhysicalType::UINT16:
305: 		TemplatedGenerateKeys<uint16_t, IS_NOT_NULL>(allocator, input.data[0], input.size(), keys);
306: 		break;
307: 	case PhysicalType::UINT32:
308: 		TemplatedGenerateKeys<uint32_t, IS_NOT_NULL>(allocator, input.data[0], input.size(), keys);
309: 		break;
310: 	case PhysicalType::UINT64:
311: 		TemplatedGenerateKeys<uint64_t, IS_NOT_NULL>(allocator, input.data[0], input.size(), keys);
312: 		break;
313: 	case PhysicalType::UINT128:
314: 		TemplatedGenerateKeys<uhugeint_t, IS_NOT_NULL>(allocator, input.data[0], input.size(), keys);
315: 		break;
316: 	case PhysicalType::FLOAT:
317: 		TemplatedGenerateKeys<float, IS_NOT_NULL>(allocator, input.data[0], input.size(), keys);
318: 		break;
319: 	case PhysicalType::DOUBLE:
320: 		TemplatedGenerateKeys<double, IS_NOT_NULL>(allocator, input.data[0], input.size(), keys);
321: 		break;
322: 	case PhysicalType::VARCHAR:
323: 		TemplatedGenerateKeys<string_t, IS_NOT_NULL>(allocator, input.data[0], input.size(), keys);
324: 		break;
325: 	default:
326: 		throw InternalException("Invalid type for index");
327: 	}
328: 
329: 	// We concatenate the keys for each remaining column of a compound key.
330: 	for (idx_t i = 1; i < input.ColumnCount(); i++) {
331: 		switch (input.data[i].GetType().InternalType()) {
332: 		case PhysicalType::BOOL:
333: 			ConcatenateKeys<bool, IS_NOT_NULL>(allocator, input.data[i], input.size(), keys);
334: 			break;
335: 		case PhysicalType::INT8:
336: 			ConcatenateKeys<int8_t, IS_NOT_NULL>(allocator, input.data[i], input.size(), keys);
337: 			break;
338: 		case PhysicalType::INT16:
339: 			ConcatenateKeys<int16_t, IS_NOT_NULL>(allocator, input.data[i], input.size(), keys);
340: 			break;
341: 		case PhysicalType::INT32:
342: 			ConcatenateKeys<int32_t, IS_NOT_NULL>(allocator, input.data[i], input.size(), keys);
343: 			break;
344: 		case PhysicalType::INT64:
345: 			ConcatenateKeys<int64_t, IS_NOT_NULL>(allocator, input.data[i], input.size(), keys);
346: 			break;
347: 		case PhysicalType::INT128:
348: 			ConcatenateKeys<hugeint_t, IS_NOT_NULL>(allocator, input.data[i], input.size(), keys);
349: 			break;
350: 		case PhysicalType::UINT8:
351: 			ConcatenateKeys<uint8_t, IS_NOT_NULL>(allocator, input.data[i], input.size(), keys);
352: 			break;
353: 		case PhysicalType::UINT16:
354: 			ConcatenateKeys<uint16_t, IS_NOT_NULL>(allocator, input.data[i], input.size(), keys);
355: 			break;
356: 		case PhysicalType::UINT32:
357: 			ConcatenateKeys<uint32_t, IS_NOT_NULL>(allocator, input.data[i], input.size(), keys);
358: 			break;
359: 		case PhysicalType::UINT64:
360: 			ConcatenateKeys<uint64_t, IS_NOT_NULL>(allocator, input.data[i], input.size(), keys);
361: 			break;
362: 		case PhysicalType::UINT128:
363: 			ConcatenateKeys<uhugeint_t, IS_NOT_NULL>(allocator, input.data[i], input.size(), keys);
364: 			break;
365: 		case PhysicalType::FLOAT:
366: 			ConcatenateKeys<float, IS_NOT_NULL>(allocator, input.data[i], input.size(), keys);
367: 			break;
368: 		case PhysicalType::DOUBLE:
369: 			ConcatenateKeys<double, IS_NOT_NULL>(allocator, input.data[i], input.size(), keys);
370: 			break;
371: 		case PhysicalType::VARCHAR:
372: 			ConcatenateKeys<string_t, IS_NOT_NULL>(allocator, input.data[i], input.size(), keys);
373: 			break;
374: 		default:
375: 			throw InternalException("Invalid type for index");
376: 		}
377: 	}
378: }
379: 
380: template <>
381: void ART::GenerateKeys<>(ArenaAllocator &allocator, DataChunk &input, unsafe_vector<ARTKey> &keys) {
382: 	GenerateKeysInternal<false>(allocator, input, keys);
383: }
384: 
385: template <>
386: void ART::GenerateKeys<true>(ArenaAllocator &allocator, DataChunk &input, unsafe_vector<ARTKey> &keys) {
387: 	GenerateKeysInternal<true>(allocator, input, keys);
388: }
389: 
390: void ART::GenerateKeyVectors(ArenaAllocator &allocator, DataChunk &input, Vector &row_ids, unsafe_vector<ARTKey> &keys,
391:                              unsafe_vector<ARTKey> &row_id_keys) {
392: 	GenerateKeys<>(allocator, input, keys);
393: 
394: 	DataChunk row_id_chunk;
395: 	row_id_chunk.Initialize(Allocator::DefaultAllocator(), vector<LogicalType> {LogicalType::ROW_TYPE}, input.size());
396: 	row_id_chunk.data[0].Reference(row_ids);
397: 	row_id_chunk.SetCardinality(input.size());
398: 	GenerateKeys<>(allocator, row_id_chunk, row_id_keys);
399: }
400: 
401: //===--------------------------------------------------------------------===//
402: // Construct from sorted data.
403: //===--------------------------------------------------------------------===//
404: 
405: bool ART::ConstructInternal(const unsafe_vector<ARTKey> &keys, const unsafe_vector<ARTKey> &row_ids, Node &node,
406:                             ARTKeySection &section) {
407: 	D_ASSERT(section.start < keys.size());
408: 	D_ASSERT(section.end < keys.size());
409: 	D_ASSERT(section.start <= section.end);
410: 
411: 	auto &start = keys[section.start];
412: 	auto &end = keys[section.end];
413: 	D_ASSERT(start.len != 0);
414: 
415: 	// Increment the depth until we reach a leaf or find a mismatching byte.
416: 	auto prefix_depth = section.depth;
417: 	while (start.len != section.depth && start.ByteMatches(end, section.depth)) {
418: 		section.depth++;
419: 	}
420: 
421: 	if (start.len == section.depth) {
422: 		// We reached a leaf. All the bytes of start_key and end_key match.
423: 		auto row_id_count = section.end - section.start + 1;
424: 		if (IsUnique() && row_id_count != 1) {
425: 			return false;
426: 		}
427: 
428: 		reference<Node> ref(node);
429: 		auto count = UnsafeNumericCast<uint8_t>(start.len - prefix_depth);
430: 		Prefix::New(*this, ref, start, prefix_depth, count);
431: 		if (row_id_count == 1) {
432: 			Leaf::New(ref, row_ids[section.start].GetRowId());
433: 		} else {
434: 			Leaf::New(*this, ref, row_ids, section.start, row_id_count);
435: 		}
436: 		return true;
437: 	}
438: 
439: 	// Create a new node and recurse.
440: 	unsafe_vector<ARTKeySection> children;
441: 	section.GetChildSections(children, keys);
442: 
443: 	// Create the prefix.
444: 	reference<Node> ref(node);
445: 	auto prefix_length = section.depth - prefix_depth;
446: 	Prefix::New(*this, ref, start, prefix_depth, prefix_length);
447: 
448: 	// Create the node.
449: 	Node::New(*this, ref, Node::GetNodeType(children.size()));
450: 	for (auto &child : children) {
451: 		Node new_child;
452: 		auto success = ConstructInternal(keys, row_ids, new_child, child);
453: 		Node::InsertChild(*this, ref, child.key_byte, new_child);
454: 		if (!success) {
455: 			return false;
456: 		}
457: 	}
458: 	return true;
459: }
460: 
461: bool ART::Construct(unsafe_vector<ARTKey> &keys, unsafe_vector<ARTKey> &row_ids, const idx_t row_count) {
462: 	ARTKeySection section(0, row_count - 1, 0, 0);
463: 	if (!ConstructInternal(keys, row_ids, tree, section)) {
464: 		return false;
465: 	}
466: 
467: #ifdef DEBUG
468: 	unsafe_vector<row_t> row_ids_debug;
469: 	Iterator it(*this);
470: 	it.FindMinimum(tree);
471: 	ARTKey empty_key = ARTKey();
472: 	it.Scan(empty_key, NumericLimits<row_t>().Maximum(), row_ids_debug, false);
473: 	D_ASSERT(row_count == row_ids_debug.size());
474: #endif
475: 	return true;
476: }
477: 
478: //===--------------------------------------------------------------------===//
479: // Insert and Constraint Checking
480: //===--------------------------------------------------------------------===//
481: 
482: ErrorData ART::Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids) {
483: 	return Insert(l, chunk, row_ids, nullptr);
484: }
485: 
486: ErrorData ART::Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids, optional_ptr<BoundIndex> delete_index) {
487: 	D_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);
488: 	auto row_count = chunk.size();
489: 
490: 	ArenaAllocator allocator(BufferAllocator::Get(db));
491: 	unsafe_vector<ARTKey> keys(row_count);
492: 	unsafe_vector<ARTKey> row_id_keys(row_count);
493: 	GenerateKeyVectors(allocator, chunk, row_ids, keys, row_id_keys);
494: 
495: 	optional_ptr<ART> delete_art;
496: 	if (delete_index) {
497: 		delete_art = delete_index->Cast<ART>();
498: 	}
499: 
500: 	auto conflict_type = ARTConflictType::NO_CONFLICT;
501: 	optional_idx conflict_idx;
502: 	auto was_empty = !tree.HasMetadata();
503: 
504: 	// Insert the entries into the index.
505: 	for (idx_t i = 0; i < row_count; i++) {
506: 		if (keys[i].Empty()) {
507: 			continue;
508: 		}
509: 		conflict_type = Insert(tree, keys[i], 0, row_id_keys[i], tree.GetGateStatus(), delete_art);
510: 		if (conflict_type != ARTConflictType::NO_CONFLICT) {
511: 			conflict_idx = i;
512: 			break;
513: 		}
514: 	}
515: 
516: 	// Remove any previously inserted entries.
517: 	if (conflict_type != ARTConflictType::NO_CONFLICT) {
518: 		D_ASSERT(conflict_idx.IsValid());
519: 		for (idx_t i = 0; i < conflict_idx.GetIndex(); i++) {
520: 			if (keys[i].Empty()) {
521: 				continue;
522: 			}
523: 			Erase(tree, keys[i], 0, row_id_keys[i], tree.GetGateStatus());
524: 		}
525: 	}
526: 
527: 	if (was_empty) {
528: 		// All nodes are in-memory.
529: 		VerifyAllocationsInternal();
530: 	}
531: 
532: 	if (conflict_type == ARTConflictType::TRANSACTION) {
533: 		auto msg = AppendRowError(chunk, conflict_idx.GetIndex());
534: 		return ErrorData(TransactionException("write-write conflict on key: \"%s\"", msg));
535: 	}
536: 
537: 	if (conflict_type == ARTConflictType::CONSTRAINT) {
538: 		auto msg = AppendRowError(chunk, conflict_idx.GetIndex());
539: 		return ErrorData(ConstraintException("PRIMARY KEY or UNIQUE constraint violation: duplicate key \"%s\"", msg));
540: 	}
541: 
542: #ifdef DEBUG
543: 	for (idx_t i = 0; i < row_count; i++) {
544: 		if (keys[i].Empty()) {
545: 			continue;
546: 		}
547: 		D_ASSERT(Lookup(tree, keys[i], 0));
548: 	}
549: #endif
550: 	return ErrorData();
551: }
552: 
553: ErrorData ART::Append(IndexLock &l, DataChunk &chunk, Vector &row_ids) {
554: 	// Execute all column expressions before inserting the data chunk.
555: 	DataChunk expr_chunk;
556: 	expr_chunk.Initialize(Allocator::DefaultAllocator(), logical_types);
557: 	ExecuteExpressions(chunk, expr_chunk);
558: 
559: 	// Now insert the data chunk.
560: 	return Insert(l, expr_chunk, row_ids, nullptr);
561: }
562: 
563: ErrorData ART::AppendWithDeleteIndex(IndexLock &l, DataChunk &chunk, Vector &row_ids,
564:                                      optional_ptr<BoundIndex> delete_index) {
565: 	// Execute all column expressions before inserting the data chunk.
566: 	DataChunk expr_chunk;
567: 	expr_chunk.Initialize(Allocator::DefaultAllocator(), logical_types);
568: 	ExecuteExpressions(chunk, expr_chunk);
569: 
570: 	// Now insert the data chunk.
571: 	return Insert(l, expr_chunk, row_ids, delete_index);
572: }
573: 
574: void ART::VerifyAppend(DataChunk &chunk, optional_ptr<BoundIndex> delete_index, optional_ptr<ConflictManager> manager) {
575: 	if (manager) {
576: 		D_ASSERT(manager->LookupType() == VerifyExistenceType::APPEND);
577: 		return VerifyConstraint(chunk, delete_index, *manager);
578: 	}
579: 	ConflictManager local_manager(VerifyExistenceType::APPEND, chunk.size());
580: 	VerifyConstraint(chunk, delete_index, local_manager);
581: }
582: 
583: void ART::InsertIntoEmpty(Node &node, const ARTKey &key, const idx_t depth, const ARTKey &row_id,
584:                           const GateStatus status) {
585: 	D_ASSERT(depth <= key.len);
586: 	D_ASSERT(!node.HasMetadata());
587: 
588: 	if (status == GateStatus::GATE_SET) {
589: 		Leaf::New(node, row_id.GetRowId());
590: 		return;
591: 	}
592: 
593: 	reference<Node> ref(node);
594: 	auto count = key.len - depth;
595: 
596: 	Prefix::New(*this, ref, key, depth, count);
597: 	Leaf::New(ref, row_id.GetRowId());
598: }
599: 
600: ARTConflictType ART::InsertIntoInlined(Node &node, const ARTKey &key, const idx_t depth, const ARTKey &row_id,
601:                                        const GateStatus status, optional_ptr<ART> delete_art) {
602: 
603: 	if (!IsUnique() || append_mode == ARTAppendMode::INSERT_DUPLICATES) {
604: 		Leaf::InsertIntoInlined(*this, node, row_id, depth, status);
605: 		return ARTConflictType::NO_CONFLICT;
606: 	}
607: 
608: 	if (!delete_art) {
609: 		if (append_mode == ARTAppendMode::IGNORE_DUPLICATES) {
610: 			return ARTConflictType::NO_CONFLICT;
611: 		}
612: 		return ARTConflictType::CONSTRAINT;
613: 	}
614: 
615: 	// Lookup in the delete_art.
616: 	auto delete_leaf = delete_art->Lookup(delete_art->tree, key, 0);
617: 	if (!delete_leaf) {
618: 		return ARTConflictType::CONSTRAINT;
619: 	}
620: 
621: 	// The row ID has changed.
622: 	// Thus, the local index has a newer (local) row ID, and this is a constraint violation.
623: 	D_ASSERT(delete_leaf->GetType() == NType::LEAF_INLINED);
624: 	auto deleted_row_id = delete_leaf->GetRowId();
625: 	auto this_row_id = node.GetRowId();
626: 	if (deleted_row_id != this_row_id) {
627: 		return ARTConflictType::CONSTRAINT;
628: 	}
629: 
630: 	// The deleted key and its row ID match the current key and its row ID.
631: 	Leaf::InsertIntoInlined(*this, node, row_id, depth, status);
632: 	return ARTConflictType::NO_CONFLICT;
633: }
634: 
635: ARTConflictType ART::InsertIntoNode(Node &node, const ARTKey &key, const idx_t depth, const ARTKey &row_id,
636:                                     const GateStatus status, optional_ptr<ART> delete_art) {
637: 	D_ASSERT(depth < key.len);
638: 	auto child = node.GetChildMutable(*this, key[depth]);
639: 
640: 	// Recurse, if a child exists at key[depth].
641: 	if (child) {
642: 		D_ASSERT(child->HasMetadata());
643: 		auto conflict_type = Insert(*child, key, depth + 1, row_id, status, delete_art);
644: 		node.ReplaceChild(*this, key[depth], *child);
645: 		return conflict_type;
646: 	}
647: 
648: 	// Create an inlined prefix at key[depth].
649: 	if (status == GateStatus::GATE_SET) {
650: 		Node remainder;
651: 		auto byte = key[depth];
652: 		auto conflict_type = Insert(remainder, key, depth + 1, row_id, status, delete_art);
653: 		Node::InsertChild(*this, node, byte, remainder);
654: 		return conflict_type;
655: 	}
656: 
657: 	// Insert an inlined leaf at key[depth].
658: 	Node leaf;
659: 	reference<Node> ref(leaf);
660: 
661: 	// Create the prefix.
662: 	if (depth + 1 < key.len) {
663: 		auto count = key.len - depth - 1;
664: 		Prefix::New(*this, ref, key, depth + 1, count);
665: 	}
666: 
667: 	// Create the inlined leaf.
668: 	Leaf::New(ref, row_id.GetRowId());
669: 	Node::InsertChild(*this, node, key[depth], leaf);
670: 	return ARTConflictType::NO_CONFLICT;
671: }
672: 
673: ARTConflictType ART::Insert(Node &node, const ARTKey &key, idx_t depth, const ARTKey &row_id, const GateStatus status,
674:                             optional_ptr<ART> delete_art) {
675: 	if (!node.HasMetadata()) {
676: 		InsertIntoEmpty(node, key, depth, row_id, status);
677: 		return ARTConflictType::NO_CONFLICT;
678: 	}
679: 
680: 	// Enter a nested leaf.
681: 	if (status == GateStatus::GATE_NOT_SET && node.GetGateStatus() == GateStatus::GATE_SET) {
682: 		if (IsUnique()) {
683: 			// Unique indexes can have duplicates, if another transaction DELETE + INSERT
684: 			// the same key. In that case, the previous value must be kept alive until all
685: 			// other transactions do not depend on it anymore.
686: 
687: 			// We restrict this transactionality to two-value leaves, so any subsequent
688: 			// incoming transaction must fail here.
689: 			return ARTConflictType::TRANSACTION;
690: 		}
691: 		return Insert(node, row_id, 0, row_id, GateStatus::GATE_SET, delete_art);
692: 	}
693: 
694: 	auto type = node.GetType();
695: 	switch (type) {
696: 	case NType::LEAF_INLINED: {
697: 		return InsertIntoInlined(node, key, depth, row_id, status, delete_art);
698: 	}
699: 	case NType::LEAF: {
700: 		Leaf::TransformToNested(*this, node);
701: 		return Insert(node, key, depth, row_id, status, delete_art);
702: 	}
703: 	case NType::NODE_7_LEAF:
704: 	case NType::NODE_15_LEAF:
705: 	case NType::NODE_256_LEAF: {
706: 		// Row IDs are unique, so there are never any duplicate byte conflicts here.
707: 		auto byte = key[Prefix::ROW_ID_COUNT];
708: 		Node::InsertChild(*this, node, byte);
709: 		return ARTConflictType::NO_CONFLICT;
710: 	}
711: 	case NType::NODE_4:
712: 	case NType::NODE_16:
713: 	case NType::NODE_48:
714: 	case NType::NODE_256:
715: 		return InsertIntoNode(node, key, depth, row_id, status, delete_art);
716: 	case NType::PREFIX:
717: 		return Prefix::Insert(*this, node, key, depth, row_id, status, delete_art);
718: 	default:
719: 		throw InternalException("Invalid node type for ART::Insert.");
720: 	}
721: }
722: 
723: //===--------------------------------------------------------------------===//
724: // Drop and Delete
725: //===--------------------------------------------------------------------===//
726: 
727: void ART::CommitDrop(IndexLock &index_lock) {
728: 	for (auto &allocator : *allocators) {
729: 		allocator->Reset();
730: 	}
731: 	tree.Clear();
732: }
733: 
734: void ART::Delete(IndexLock &state, DataChunk &input, Vector &row_ids) {
735: 	auto row_count = input.size();
736: 
737: 	DataChunk expr_chunk;
738: 	expr_chunk.Initialize(Allocator::DefaultAllocator(), logical_types);
739: 	ExecuteExpressions(input, expr_chunk);
740: 
741: 	ArenaAllocator allocator(BufferAllocator::Get(db));
742: 	unsafe_vector<ARTKey> keys(row_count);
743: 	unsafe_vector<ARTKey> row_id_keys(row_count);
744: 	GenerateKeyVectors(allocator, expr_chunk, row_ids, keys, row_id_keys);
745: 
746: 	for (idx_t i = 0; i < row_count; i++) {
747: 		if (keys[i].Empty()) {
748: 			continue;
749: 		}
750: 		Erase(tree, keys[i], 0, row_id_keys[i], tree.GetGateStatus());
751: 	}
752: 
753: 	if (!tree.HasMetadata()) {
754: 		// No more allocations.
755: 		VerifyAllocationsInternal();
756: 	}
757: 
758: #ifdef DEBUG
759: 	for (idx_t i = 0; i < row_count; i++) {
760: 		if (keys[i].Empty()) {
761: 			continue;
762: 		}
763: 		auto leaf = Lookup(tree, keys[i], 0);
764: 		if (leaf && leaf->GetType() == NType::LEAF_INLINED) {
765: 			D_ASSERT(leaf->GetRowId() != row_id_keys[i].GetRowId());
766: 		}
767: 	}
768: #endif
769: }
770: 
771: void ART::Erase(Node &node, reference<const ARTKey> key, idx_t depth, reference<const ARTKey> row_id,
772:                 GateStatus status) {
773: 	if (!node.HasMetadata()) {
774: 		return;
775: 	}
776: 
777: 	// Traverse the prefix.
778: 	reference<Node> next(node);
779: 	if (next.get().GetType() == NType::PREFIX) {
780: 		Prefix::TraverseMutable(*this, next, key, depth);
781: 
782: 		// Prefixes don't match: nothing to erase.
783: 		if (next.get().GetType() == NType::PREFIX && next.get().GetGateStatus() == GateStatus::GATE_NOT_SET) {
784: 			return;
785: 		}
786: 	}
787: 
788: 	//	Delete the row ID from the leaf.
789: 	//	This is the root node, which can be a leaf with possible prefix nodes.
790: 	if (next.get().GetType() == NType::LEAF_INLINED) {
791: 		if (next.get().GetRowId() == row_id.get().GetRowId()) {
792: 			Node::Free(*this, node);
793: 		}
794: 		return;
795: 	}
796: 
797: 	// Transform a deprecated leaf.
798: 	if (next.get().GetType() == NType::LEAF) {
799: 		D_ASSERT(status == GateStatus::GATE_NOT_SET);
800: 		Leaf::TransformToNested(*this, next);
801: 	}
802: 
803: 	// Enter a nested leaf.
804: 	if (status == GateStatus::GATE_NOT_SET && next.get().GetGateStatus() == GateStatus::GATE_SET) {
805: 		return Erase(next, row_id, 0, row_id, GateStatus::GATE_SET);
806: 	}
807: 
808: 	D_ASSERT(depth < key.get().len);
809: 	if (next.get().IsLeafNode()) {
810: 		auto byte = key.get()[depth];
811: 		if (next.get().HasByte(*this, byte)) {
812: 			Node::DeleteChild(*this, next, node, key.get()[depth], status, key.get());
813: 		}
814: 		return;
815: 	}
816: 
817: 	auto child = next.get().GetChildMutable(*this, key.get()[depth]);
818: 	if (!child) {
819: 		// No child at the byte: nothing to erase.
820: 		return;
821: 	}
822: 
823: 	// Transform a deprecated leaf.
824: 	if (child->GetType() == NType::LEAF) {
825: 		D_ASSERT(status == GateStatus::GATE_NOT_SET);
826: 		Leaf::TransformToNested(*this, *child);
827: 	}
828: 
829: 	// Enter a nested leaf.
830: 	if (status == GateStatus::GATE_NOT_SET && child->GetGateStatus() == GateStatus::GATE_SET) {
831: 		Erase(*child, row_id, 0, row_id, GateStatus::GATE_SET);
832: 		if (!child->HasMetadata()) {
833: 			Node::DeleteChild(*this, next, node, key.get()[depth], status, key.get());
834: 		} else {
835: 			next.get().ReplaceChild(*this, key.get()[depth], *child);
836: 		}
837: 		return;
838: 	}
839: 
840: 	auto temp_depth = depth + 1;
841: 	reference<Node> ref(*child);
842: 
843: 	if (ref.get().GetType() == NType::PREFIX) {
844: 		Prefix::TraverseMutable(*this, ref, key, temp_depth);
845: 
846: 		// Prefixes don't match: nothing to erase.
847: 		if (ref.get().GetType() == NType::PREFIX && ref.get().GetGateStatus() == GateStatus::GATE_NOT_SET) {
848: 			return;
849: 		}
850: 	}
851: 
852: 	if (ref.get().GetType() == NType::LEAF_INLINED) {
853: 		if (ref.get().GetRowId() == row_id.get().GetRowId()) {
854: 			Node::DeleteChild(*this, next, node, key.get()[depth], status, key.get());
855: 		}
856: 		return;
857: 	}
858: 
859: 	// Recurse.
860: 	Erase(*child, key, depth + 1, row_id, status);
861: 	if (!child->HasMetadata()) {
862: 		Node::DeleteChild(*this, next, node, key.get()[depth], status, key.get());
863: 	} else {
864: 		next.get().ReplaceChild(*this, key.get()[depth], *child);
865: 	}
866: }
867: 
868: //===--------------------------------------------------------------------===//
869: // Point and range lookups
870: //===--------------------------------------------------------------------===//
871: 
872: const unsafe_optional_ptr<const Node> ART::Lookup(const Node &node, const ARTKey &key, idx_t depth) {
873: 	reference<const Node> ref(node);
874: 	while (ref.get().HasMetadata()) {
875: 
876: 		// Return the leaf.
877: 		if (ref.get().IsAnyLeaf() || ref.get().GetGateStatus() == GateStatus::GATE_SET) {
878: 			return unsafe_optional_ptr<const Node>(ref.get());
879: 		}
880: 
881: 		// Traverse the prefix.
882: 		if (ref.get().GetType() == NType::PREFIX) {
883: 			Prefix::Traverse(*this, ref, key, depth);
884: 			if (ref.get().GetType() == NType::PREFIX && ref.get().GetGateStatus() == GateStatus::GATE_NOT_SET) {
885: 				// Prefix mismatch, return nullptr.
886: 				return nullptr;
887: 			}
888: 			continue;
889: 		}
890: 
891: 		// Get the child node.
892: 		D_ASSERT(depth < key.len);
893: 		auto child = ref.get().GetChild(*this, key[depth]);
894: 
895: 		// No child at the matching byte, return nullptr.
896: 		if (!child) {
897: 			return nullptr;
898: 		}
899: 
900: 		// Continue in the child.
901: 		ref = *child;
902: 		D_ASSERT(ref.get().HasMetadata());
903: 		depth++;
904: 	}
905: 
906: 	return nullptr;
907: }
908: 
909: bool ART::SearchEqual(ARTKey &key, idx_t max_count, unsafe_vector<row_t> &row_ids) {
910: 	auto leaf = Lookup(tree, key, 0);
911: 	if (!leaf) {
912: 		return true;
913: 	}
914: 
915: 	Iterator it(*this);
916: 	it.FindMinimum(*leaf);
917: 	ARTKey empty_key = ARTKey();
918: 	return it.Scan(empty_key, max_count, row_ids, false);
919: }
920: 
921: bool ART::SearchGreater(ARTKey &key, bool equal, idx_t max_count, unsafe_vector<row_t> &row_ids) {
922: 	if (!tree.HasMetadata()) {
923: 		return true;
924: 	}
925: 
926: 	// Find the lowest value that satisfies the predicate.
927: 	Iterator it(*this);
928: 
929: 	// Early-out, if the maximum value in the ART is lower than the lower bound.
930: 	if (!it.LowerBound(tree, key, equal, 0)) {
931: 		return true;
932: 	}
933: 
934: 	// We continue the scan. We do not check the bounds as any value following this value is
935: 	// greater and satisfies our predicate.
936: 	return it.Scan(ARTKey(), max_count, row_ids, false);
937: }
938: 
939: bool ART::SearchLess(ARTKey &upper_bound, bool equal, idx_t max_count, unsafe_vector<row_t> &row_ids) {
940: 	if (!tree.HasMetadata()) {
941: 		return true;
942: 	}
943: 
944: 	// Find the minimum value in the ART: we start scanning from this value.
945: 	Iterator it(*this);
946: 	it.FindMinimum(tree);
947: 
948: 	// Early-out, if the minimum value is higher than the upper bound.
949: 	if (it.current_key.GreaterThan(upper_bound, equal, it.GetNestedDepth())) {
950: 		return true;
951: 	}
952: 
953: 	// Continue the scan until we reach the upper bound.
954: 	return it.Scan(upper_bound, max_count, row_ids, equal);
955: }
956: 
957: bool ART::SearchCloseRange(ARTKey &lower_bound, ARTKey &upper_bound, bool left_equal, bool right_equal, idx_t max_count,
958:                            unsafe_vector<row_t> &row_ids) {
959: 	// Find the first node that satisfies the left predicate.
960: 	Iterator it(*this);
961: 
962: 	// Early-out, if the maximum value in the ART is lower than the lower bound.
963: 	if (!it.LowerBound(tree, lower_bound, left_equal, 0)) {
964: 		return true;
965: 	}
966: 
967: 	// Continue the scan until we reach the upper bound.
968: 	return it.Scan(upper_bound, max_count, row_ids, right_equal);
969: }
970: 
971: bool ART::Scan(IndexScanState &state, const idx_t max_count, unsafe_vector<row_t> &row_ids) {
972: 	auto &scan_state = state.Cast<ARTIndexScanState>();
973: 	D_ASSERT(scan_state.values[0].type().InternalType() == types[0]);
974: 	ArenaAllocator arena_allocator(Allocator::Get(db));
975: 	auto key = ARTKey::CreateKey(arena_allocator, types[0], scan_state.values[0]);
976: 
977: 	if (scan_state.values[1].IsNull()) {
978: 		// Single predicate.
979: 		lock_guard<mutex> l(lock);
980: 		switch (scan_state.expressions[0]) {
981: 		case ExpressionType::COMPARE_EQUAL:
982: 			return SearchEqual(key, max_count, row_ids);
983: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
984: 			return SearchGreater(key, true, max_count, row_ids);
985: 		case ExpressionType::COMPARE_GREATERTHAN:
986: 			return SearchGreater(key, false, max_count, row_ids);
987: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
988: 			return SearchLess(key, true, max_count, row_ids);
989: 		case ExpressionType::COMPARE_LESSTHAN:
990: 			return SearchLess(key, false, max_count, row_ids);
991: 		default:
992: 			throw InternalException("Index scan type not implemented");
993: 		}
994: 	}
995: 
996: 	// Two predicates.
997: 	lock_guard<mutex> l(lock);
998: 	D_ASSERT(scan_state.values[1].type().InternalType() == types[0]);
999: 	auto upper_bound = ARTKey::CreateKey(arena_allocator, types[0], scan_state.values[1]);
1000: 	bool left_equal = scan_state.expressions[0] == ExpressionType ::COMPARE_GREATERTHANOREQUALTO;
1001: 	bool right_equal = scan_state.expressions[1] == ExpressionType ::COMPARE_LESSTHANOREQUALTO;
1002: 	return SearchCloseRange(key, upper_bound, left_equal, right_equal, max_count, row_ids);
1003: }
1004: 
1005: //===--------------------------------------------------------------------===//
1006: // More Constraint Checking
1007: //===--------------------------------------------------------------------===//
1008: 
1009: string ART::GenerateErrorKeyName(DataChunk &input, idx_t row_idx) {
1010: 	DataChunk expr_chunk;
1011: 	expr_chunk.Initialize(Allocator::DefaultAllocator(), logical_types);
1012: 	ExecuteExpressions(input, expr_chunk);
1013: 
1014: 	string key_name;
1015: 	for (idx_t k = 0; k < expr_chunk.ColumnCount(); k++) {
1016: 		if (k > 0) {
1017: 			key_name += ", ";
1018: 		}
1019: 		key_name += unbound_expressions[k]->GetName() + ": " + expr_chunk.data[k].GetValue(row_idx).ToString();
1020: 	}
1021: 	return key_name;
1022: }
1023: 
1024: string ART::GenerateConstraintErrorMessage(VerifyExistenceType verify_type, const string &key_name) {
1025: 	switch (verify_type) {
1026: 	case VerifyExistenceType::APPEND: {
1027: 		// APPEND to PK/UNIQUE table, but node/key already exists in PK/UNIQUE table.
1028: 		string type = IsPrimary() ? "primary key" : "unique";
1029: 		return StringUtil::Format("Duplicate key \"%s\" violates %s constraint.", key_name, type);
1030: 	}
1031: 	case VerifyExistenceType::APPEND_FK: {
1032: 		// APPEND_FK to FK table, node/key does not exist in PK/UNIQUE table.
1033: 		return StringUtil::Format(
1034: 		    "Violates foreign key constraint because key \"%s\" does not exist in the referenced table", key_name);
1035: 	}
1036: 	case VerifyExistenceType::DELETE_FK: {
1037: 		// DELETE_FK that still exists in a FK table, i.e., not a valid delete.
1038: 		return StringUtil::Format("Violates foreign key constraint because key \"%s\" is still referenced by a foreign "
1039: 		                          "key in a different table",
1040: 		                          key_name);
1041: 	}
1042: 	default:
1043: 		throw NotImplementedException("Type not implemented for VerifyExistenceType");
1044: 	}
1045: }
1046: 
1047: void ART::VerifyLeaf(const Node &leaf, const ARTKey &key, optional_ptr<ART> delete_art, ConflictManager &manager,
1048:                      optional_idx &conflict_idx, idx_t i) {
1049: 	// Fast path, the leaf is inlined, and the delete ART does not exist.
1050: 	if (leaf.GetType() == NType::LEAF_INLINED && !delete_art) {
1051: 		if (manager.AddHit(i, leaf.GetRowId())) {
1052: 			conflict_idx = i;
1053: 		}
1054: 		return;
1055: 	}
1056: 
1057: 	// Get the delete_leaf.
1058: 	// All leaves in the delete ART are inlined.
1059: 	unsafe_optional_ptr<const Node> deleted_leaf;
1060: 	if (delete_art) {
1061: 		deleted_leaf = delete_art->Lookup(delete_art->tree, key, 0);
1062: 	}
1063: 
1064: 	// The leaf is inlined, and there is no deleted leaf with the same key.
1065: 	if (leaf.GetType() == NType::LEAF_INLINED && !deleted_leaf) {
1066: 		if (manager.AddHit(i, leaf.GetRowId())) {
1067: 			conflict_idx = i;
1068: 		}
1069: 		return;
1070: 	}
1071: 
1072: 	// The leaf is inlined, and the same key exists in the delete ART.
1073: 	if (leaf.GetType() == NType::LEAF_INLINED && deleted_leaf) {
1074: 		D_ASSERT(deleted_leaf->GetType() == NType::LEAF_INLINED);
1075: 		auto deleted_row_id = deleted_leaf->GetRowId();
1076: 		auto this_row_id = leaf.GetRowId();
1077: 
1078: 		if (deleted_row_id == this_row_id) {
1079: 			if (manager.AddMiss(i)) {
1080: 				conflict_idx = i;
1081: 			}
1082: 			return;
1083: 		}
1084: 
1085: 		if (manager.AddHit(i, this_row_id)) {
1086: 			conflict_idx = i;
1087: 		}
1088: 		return;
1089: 	}
1090: 
1091: 	// FIXME: proper foreign key + delete ART support.
1092: 	// This implicitly works for foreign keys, as we do not have to consider the actual row IDs.
1093: 	// We only need to know that there are conflicts (for now), as we still perform over-eager constraint checking.
1094: 
1095: 	// Scan the two row IDs in the leaf.
1096: 	Iterator it(*this);
1097: 	it.FindMinimum(leaf);
1098: 	ARTKey empty_key = ARTKey();
1099: 	unsafe_vector<row_t> row_ids;
1100: 	it.Scan(empty_key, 2, row_ids, false);
1101: 
1102: 	if (!deleted_leaf) {
1103: 		if (manager.AddHit(i, row_ids[0]) || manager.AddHit(i, row_ids[1])) {
1104: 			conflict_idx = i;
1105: 		}
1106: 		return;
1107: 	}
1108: 
1109: 	auto deleted_row_id = deleted_leaf->GetRowId();
1110: 	if (deleted_row_id == row_ids[0] || deleted_row_id == row_ids[1]) {
1111: 		if (manager.AddMiss(i)) {
1112: 			conflict_idx = i;
1113: 		}
1114: 		return;
1115: 	}
1116: 
1117: 	if (manager.AddHit(i, row_ids[0]) || manager.AddHit(i, row_ids[1])) {
1118: 		conflict_idx = i;
1119: 	}
1120: }
1121: 
1122: void ART::VerifyConstraint(DataChunk &chunk, optional_ptr<BoundIndex> delete_index, ConflictManager &manager) {
1123: 	// Lock the index during constraint checking.
1124: 	lock_guard<mutex> l(lock);
1125: 
1126: 	DataChunk expr_chunk;
1127: 	expr_chunk.Initialize(Allocator::DefaultAllocator(), logical_types);
1128: 	ExecuteExpressions(chunk, expr_chunk);
1129: 
1130: 	ArenaAllocator arena_allocator(BufferAllocator::Get(db));
1131: 	unsafe_vector<ARTKey> keys(expr_chunk.size());
1132: 	GenerateKeys<>(arena_allocator, expr_chunk, keys);
1133: 
1134: 	optional_ptr<ART> delete_art;
1135: 	if (delete_index) {
1136: 		delete_art = delete_index->Cast<ART>();
1137: 	}
1138: 
1139: 	optional_idx conflict_idx;
1140: 	for (idx_t i = 0; !conflict_idx.IsValid() && i < chunk.size(); i++) {
1141: 		if (keys[i].Empty()) {
1142: 			if (manager.AddNull(i)) {
1143: 				conflict_idx = i;
1144: 			}
1145: 			continue;
1146: 		}
1147: 
1148: 		auto leaf = Lookup(tree, keys[i], 0);
1149: 		if (!leaf) {
1150: 			if (manager.AddMiss(i)) {
1151: 				conflict_idx = i;
1152: 			}
1153: 			continue;
1154: 		}
1155: 		VerifyLeaf(*leaf, keys[i], delete_art, manager, conflict_idx, i);
1156: 	}
1157: 
1158: 	manager.FinishLookup();
1159: 	if (!conflict_idx.IsValid()) {
1160: 		return;
1161: 	}
1162: 
1163: 	auto key_name = GenerateErrorKeyName(chunk, conflict_idx.GetIndex());
1164: 	auto exception_msg = GenerateConstraintErrorMessage(manager.LookupType(), key_name);
1165: 	throw ConstraintException(exception_msg);
1166: }
1167: 
1168: string ART::GetConstraintViolationMessage(VerifyExistenceType verify_type, idx_t failed_index, DataChunk &input) {
1169: 	auto key_name = GenerateErrorKeyName(input, failed_index);
1170: 	auto exception_msg = GenerateConstraintErrorMessage(verify_type, key_name);
1171: 	return exception_msg;
1172: }
1173: 
1174: //===--------------------------------------------------------------------===//
1175: // Storage and Memory
1176: //===--------------------------------------------------------------------===//
1177: 
1178: void ART::TransformToDeprecated() {
1179: 	auto idx = Node::GetAllocatorIdx(NType::PREFIX);
1180: 	auto &block_manager = (*allocators)[idx]->block_manager;
1181: 	unsafe_unique_ptr<FixedSizeAllocator> deprecated_allocator;
1182: 
1183: 	if (prefix_count != Prefix::DEPRECATED_COUNT) {
1184: 		auto prefix_size = NumericCast<idx_t>(Prefix::DEPRECATED_COUNT) + NumericCast<idx_t>(Prefix::METADATA_SIZE);
1185: 		deprecated_allocator = make_unsafe_uniq<FixedSizeAllocator>(prefix_size, block_manager);
1186: 	}
1187: 
1188: 	// Transform all leaves, and possibly the prefixes.
1189: 	if (tree.HasMetadata()) {
1190: 		Node::TransformToDeprecated(*this, tree, deprecated_allocator);
1191: 	}
1192: 
1193: 	// Replace the prefix allocator with the deprecated allocator.
1194: 	if (deprecated_allocator) {
1195: 		prefix_count = Prefix::DEPRECATED_COUNT;
1196: 
1197: 		D_ASSERT((*allocators)[idx]->IsEmpty());
1198: 		(*allocators)[idx]->Reset();
1199: 		(*allocators)[idx] = std::move(deprecated_allocator);
1200: 	}
1201: }
1202: 
1203: IndexStorageInfo ART::GetStorageInfo(const case_insensitive_map_t<Value> &options, const bool to_wal) {
1204: 	// If the storage format uses deprecated leaf storage,
1205: 	// then we need to transform all nested leaves before serialization.
1206: 	auto v1_0_0_option = options.find("v1_0_0_storage");
1207: 	bool v1_0_0_storage = v1_0_0_option == options.end() || v1_0_0_option->second != Value(false);
1208: 	if (v1_0_0_storage) {
1209: 		TransformToDeprecated();
1210: 	}
1211: 
1212: 	IndexStorageInfo info(name);
1213: 	info.root = tree.Get();
1214: 	info.options = options;
1215: 
1216: 	for (auto &allocator : *allocators) {
1217: 		allocator->RemoveEmptyBuffers();
1218: 	}
1219: 
1220: #ifdef DEBUG
1221: 	if (v1_0_0_storage) {
1222: 		D_ASSERT((*allocators)[Node::GetAllocatorIdx(NType::NODE_7_LEAF)]->IsEmpty());
1223: 		D_ASSERT((*allocators)[Node::GetAllocatorIdx(NType::NODE_15_LEAF)]->IsEmpty());
1224: 		D_ASSERT((*allocators)[Node::GetAllocatorIdx(NType::NODE_256_LEAF)]->IsEmpty());
1225: 		D_ASSERT((*allocators)[Node::GetAllocatorIdx(NType::PREFIX)]->GetSegmentSize() ==
1226: 		         Prefix::DEPRECATED_COUNT + Prefix::METADATA_SIZE);
1227: 	}
1228: #endif
1229: 
1230: 	auto allocator_count = v1_0_0_storage ? DEPRECATED_ALLOCATOR_COUNT : ALLOCATOR_COUNT;
1231: 	if (!to_wal) {
1232: 		// Store the data on disk as partial blocks and set the block ids.
1233: 		WritePartialBlocks(v1_0_0_storage);
1234: 
1235: 	} else {
1236: 		// Set the correct allocation sizes and get the map containing all buffers.
1237: 		for (idx_t i = 0; i < allocator_count; i++) {
1238: 			info.buffers.push_back((*allocators)[i]->InitSerializationToWAL());
1239: 		}
1240: 	}
1241: 
1242: 	for (idx_t i = 0; i < allocator_count; i++) {
1243: 		info.allocator_infos.push_back((*allocators)[i]->GetInfo());
1244: 	}
1245: 	return info;
1246: }
1247: 
1248: void ART::WritePartialBlocks(const bool v1_0_0_storage) {
1249: 	auto &block_manager = table_io_manager.GetIndexBlockManager();
1250: 	PartialBlockManager partial_block_manager(block_manager, PartialBlockType::FULL_CHECKPOINT);
1251: 
1252: 	idx_t allocator_count = v1_0_0_storage ? DEPRECATED_ALLOCATOR_COUNT : ALLOCATOR_COUNT;
1253: 	for (idx_t i = 0; i < allocator_count; i++) {
1254: 		(*allocators)[i]->SerializeBuffers(partial_block_manager);
1255: 	}
1256: 	partial_block_manager.FlushPartialBlocks();
1257: }
1258: 
1259: void ART::InitAllocators(const IndexStorageInfo &info) {
1260: 	for (idx_t i = 0; i < info.allocator_infos.size(); i++) {
1261: 		(*allocators)[i]->Init(info.allocator_infos[i]);
1262: 	}
1263: }
1264: 
1265: void ART::Deserialize(const BlockPointer &pointer) {
1266: 	D_ASSERT(pointer.IsValid());
1267: 
1268: 	auto &metadata_manager = table_io_manager.GetMetadataManager();
1269: 	MetadataReader reader(metadata_manager, pointer);
1270: 	tree = reader.Read<Node>();
1271: 
1272: 	for (idx_t i = 0; i < DEPRECATED_ALLOCATOR_COUNT; i++) {
1273: 		(*allocators)[i]->Deserialize(metadata_manager, reader.Read<BlockPointer>());
1274: 	}
1275: }
1276: 
1277: void ART::SetPrefixCount(const IndexStorageInfo &info) {
1278: 	auto numeric_max = NumericLimits<uint8_t>().Maximum();
1279: 	auto max_aligned = AlignValueFloor<uint8_t>(numeric_max - Prefix::METADATA_SIZE);
1280: 
1281: 	if (info.IsValid() && info.root_block_ptr.IsValid()) {
1282: 		prefix_count = Prefix::DEPRECATED_COUNT;
1283: 		return;
1284: 	}
1285: 
1286: 	if (info.IsValid()) {
1287: 		auto serialized_count = info.allocator_infos[0].segment_size - Prefix::METADATA_SIZE;
1288: 		prefix_count = NumericCast<uint8_t>(serialized_count);
1289: 		return;
1290: 	}
1291: 
1292: 	if (!IsUnique()) {
1293: 		prefix_count = Prefix::ROW_ID_COUNT;
1294: 		return;
1295: 	}
1296: 
1297: 	idx_t compound_size = 0;
1298: 	for (const auto &type : types) {
1299: 		compound_size += GetTypeIdSize(type);
1300: 	}
1301: 
1302: 	auto aligned = AlignValue(compound_size) - 1;
1303: 	if (aligned > NumericCast<idx_t>(max_aligned)) {
1304: 		prefix_count = max_aligned;
1305: 		return;
1306: 	}
1307: 
1308: 	prefix_count = NumericCast<uint8_t>(aligned);
1309: }
1310: 
1311: idx_t ART::GetInMemorySize(IndexLock &index_lock) {
1312: 	D_ASSERT(owns_data);
1313: 
1314: 	idx_t in_memory_size = 0;
1315: 	for (auto &allocator : *allocators) {
1316: 		in_memory_size += allocator->GetInMemorySize();
1317: 	}
1318: 	return in_memory_size;
1319: }
1320: 
1321: //===--------------------------------------------------------------------===//
1322: // Vacuum
1323: //===--------------------------------------------------------------------===//
1324: 
1325: void ART::InitializeVacuum(unordered_set<uint8_t> &indexes) {
1326: 	for (idx_t i = 0; i < allocators->size(); i++) {
1327: 		if ((*allocators)[i]->InitializeVacuum()) {
1328: 			indexes.insert(NumericCast<uint8_t>(i));
1329: 		}
1330: 	}
1331: }
1332: 
1333: void ART::FinalizeVacuum(const unordered_set<uint8_t> &indexes) {
1334: 	for (const auto &idx : indexes) {
1335: 		(*allocators)[idx]->FinalizeVacuum();
1336: 	}
1337: }
1338: 
1339: void ART::Vacuum(IndexLock &state) {
1340: 	D_ASSERT(owns_data);
1341: 
1342: 	if (!tree.HasMetadata()) {
1343: 		for (auto &allocator : *allocators) {
1344: 			allocator->Reset();
1345: 		}
1346: 		return;
1347: 	}
1348: 
1349: 	// True, if an allocator needs a vacuum, false otherwise.
1350: 	unordered_set<uint8_t> indexes;
1351: 	InitializeVacuum(indexes);
1352: 
1353: 	// Skip vacuum, if no allocators require it.
1354: 	if (indexes.empty()) {
1355: 		return;
1356: 	}
1357: 
1358: 	// Traverse the allocated memory of the tree to perform a vacuum.
1359: 	tree.Vacuum(*this, indexes);
1360: 
1361: 	// Finalize the vacuum operation.
1362: 	FinalizeVacuum(indexes);
1363: }
1364: 
1365: //===--------------------------------------------------------------------===//
1366: // Merging
1367: //===--------------------------------------------------------------------===//
1368: 
1369: void ART::InitializeMerge(unsafe_vector<idx_t> &upper_bounds) {
1370: 	D_ASSERT(owns_data);
1371: 	for (auto &allocator : *allocators) {
1372: 		upper_bounds.emplace_back(allocator->GetUpperBoundBufferId());
1373: 	}
1374: }
1375: 
1376: bool ART::MergeIndexes(IndexLock &state, BoundIndex &other_index) {
1377: 	auto &other_art = other_index.Cast<ART>();
1378: 	if (!other_art.tree.HasMetadata()) {
1379: 		return true;
1380: 	}
1381: 
1382: 	if (other_art.owns_data) {
1383: 		if (tree.HasMetadata()) {
1384: 			// Fully deserialize other_index, and traverse it to increment its buffer IDs.
1385: 			unsafe_vector<idx_t> upper_bounds;
1386: 			InitializeMerge(upper_bounds);
1387: 			other_art.tree.InitMerge(other_art, upper_bounds);
1388: 		}
1389: 
1390: 		// Merge the node storage.
1391: 		for (idx_t i = 0; i < allocators->size(); i++) {
1392: 			(*allocators)[i]->Merge(*(*other_art.allocators)[i]);
1393: 		}
1394: 	}
1395: 
1396: 	// Merge the ARTs.
1397: 	D_ASSERT(tree.GetGateStatus() == other_art.tree.GetGateStatus());
1398: 	if (!tree.Merge(*this, other_art.tree, tree.GetGateStatus())) {
1399: 		return false;
1400: 	}
1401: 	return true;
1402: }
1403: 
1404: //===--------------------------------------------------------------------===//
1405: // Verification
1406: //===--------------------------------------------------------------------===//
1407: 
1408: string ART::VerifyAndToString(IndexLock &state, const bool only_verify) {
1409: 	return VerifyAndToStringInternal(only_verify);
1410: }
1411: 
1412: string ART::VerifyAndToStringInternal(const bool only_verify) {
1413: 	if (tree.HasMetadata()) {
1414: 		return "ART: " + tree.VerifyAndToString(*this, only_verify);
1415: 	}
1416: 	return "[empty]";
1417: }
1418: 
1419: void ART::VerifyAllocations(IndexLock &state) {
1420: 	return VerifyAllocationsInternal();
1421: }
1422: 
1423: void ART::VerifyAllocationsInternal() {
1424: #ifdef DEBUG
1425: 	unordered_map<uint8_t, idx_t> node_counts;
1426: 	for (idx_t i = 0; i < allocators->size(); i++) {
1427: 		node_counts[NumericCast<uint8_t>(i)] = 0;
1428: 	}
1429: 
1430: 	if (tree.HasMetadata()) {
1431: 		tree.VerifyAllocations(*this, node_counts);
1432: 	}
1433: 
1434: 	for (idx_t i = 0; i < allocators->size(); i++) {
1435: 		auto segment_count = (*allocators)[i]->GetSegmentCount();
1436: 		D_ASSERT(segment_count == node_counts[NumericCast<uint8_t>(i)]);
1437: 	}
1438: #endif
1439: }
1440: 
1441: constexpr const char *ART::TYPE_NAME;
1442: 
1443: } // namespace duckdb
[end of src/execution/index/art/art.cpp]
[start of src/execution/index/art/leaf.cpp]
1: #include "duckdb/execution/index/art/leaf.hpp"
2: 
3: #include "duckdb/common/types.hpp"
4: #include "duckdb/execution/index/art/art.hpp"
5: #include "duckdb/execution/index/art/art_key.hpp"
6: #include "duckdb/execution/index/art/base_leaf.hpp"
7: #include "duckdb/execution/index/art/base_node.hpp"
8: #include "duckdb/execution/index/art/iterator.hpp"
9: #include "duckdb/execution/index/art/node.hpp"
10: #include "duckdb/execution/index/art/prefix.hpp"
11: 
12: namespace duckdb {
13: 
14: void Leaf::New(Node &node, const row_t row_id) {
15: 	D_ASSERT(row_id < MAX_ROW_ID_LOCAL);
16: 	node.Clear();
17: 	node.SetMetadata(static_cast<uint8_t>(INLINED));
18: 	node.SetRowId(row_id);
19: }
20: 
21: void Leaf::New(ART &art, reference<Node> &node, const unsafe_vector<ARTKey> &row_ids, const idx_t start,
22:                const idx_t count) {
23: 	D_ASSERT(count > 1);
24: 	D_ASSERT(!node.get().HasMetadata());
25: 
26: 	// We cannot recurse into the leaf during Construct(...) because row IDs are not sorted.
27: 	for (idx_t i = 0; i < count; i++) {
28: 		idx_t offset = start + i;
29: 		art.Insert(node, row_ids[offset], 0, row_ids[offset], GateStatus::GATE_SET, nullptr);
30: 	}
31: 	node.get().SetGateStatus(GateStatus::GATE_SET);
32: }
33: 
34: void Leaf::MergeInlined(ART &art, Node &l_node, Node &r_node) {
35: 	D_ASSERT(r_node.GetType() == INLINED);
36: 
37: 	ArenaAllocator arena_allocator(Allocator::Get(art.db));
38: 	auto key = ARTKey::CreateARTKey<row_t>(arena_allocator, r_node.GetRowId());
39: 	art.Insert(l_node, key, 0, key, l_node.GetGateStatus(), nullptr);
40: 	r_node.Clear();
41: }
42: 
43: void Leaf::InsertIntoInlined(ART &art, Node &node, const ARTKey &row_id, idx_t depth, const GateStatus status) {
44: 	D_ASSERT(node.GetType() == INLINED);
45: 
46: 	ArenaAllocator allocator(Allocator::Get(art.db));
47: 	auto key = ARTKey::CreateARTKey<row_t>(allocator, node.GetRowId());
48: 
49: 	GateStatus new_status;
50: 	if (status == GateStatus::GATE_NOT_SET || node.GetGateStatus() == GateStatus::GATE_SET) {
51: 		new_status = GateStatus::GATE_SET;
52: 	} else {
53: 		new_status = GateStatus::GATE_NOT_SET;
54: 	}
55: 
56: 	if (new_status == GateStatus::GATE_SET) {
57: 		depth = 0;
58: 	}
59: 	node.Clear();
60: 
61: 	// Get the mismatching position.
62: 	D_ASSERT(row_id.len == key.len);
63: 	auto pos = row_id.GetMismatchPos(key, depth);
64: 	D_ASSERT(pos != DConstants::INVALID_INDEX);
65: 	D_ASSERT(pos >= depth);
66: 	auto byte = row_id.data[pos];
67: 
68: 	// Create the (optional) prefix and the node.
69: 	reference<Node> next(node);
70: 	auto count = pos - depth;
71: 	if (count != 0) {
72: 		Prefix::New(art, next, row_id, depth, count);
73: 	}
74: 	if (pos == Prefix::ROW_ID_COUNT) {
75: 		Node7Leaf::New(art, next);
76: 	} else {
77: 		Node4::New(art, next);
78: 	}
79: 
80: 	// Create the children.
81: 	Node row_id_node;
82: 	Leaf::New(row_id_node, row_id.GetRowId());
83: 	Node remainder;
84: 	if (pos != Prefix::ROW_ID_COUNT) {
85: 		Leaf::New(remainder, key.GetRowId());
86: 	}
87: 
88: 	Node::InsertChild(art, next, key[pos], remainder);
89: 	Node::InsertChild(art, next, byte, row_id_node);
90: 	node.SetGateStatus(new_status);
91: }
92: 
93: void Leaf::TransformToNested(ART &art, Node &node) {
94: 	D_ASSERT(node.GetType() == LEAF);
95: 
96: 	ArenaAllocator allocator(Allocator::Get(art.db));
97: 	Node root = Node();
98: 
99: 	// Temporarily disable constraint checking.
100: 	if (art.IsUnique() && art.append_mode == ARTAppendMode::DEFAULT) {
101: 		art.append_mode = ARTAppendMode::INSERT_DUPLICATES;
102: 	}
103: 
104: 	// Move all row IDs into the nested leaf.
105: 	reference<const Node> leaf_ref(node);
106: 	while (leaf_ref.get().HasMetadata()) {
107: 		auto &leaf = Node::Ref<const Leaf>(art, leaf_ref, LEAF);
108: 		for (uint8_t i = 0; i < leaf.count; i++) {
109: 			auto row_id = ARTKey::CreateARTKey<row_t>(allocator, leaf.row_ids[i]);
110: 			auto conflict_type = art.Insert(root, row_id, 0, row_id, GateStatus::GATE_SET, nullptr);
111: 			if (conflict_type != ARTConflictType::NO_CONFLICT) {
112: 				throw InternalException("invalid conflict type in Leaf::TransformToNested");
113: 			}
114: 		}
115: 		leaf_ref = leaf.ptr;
116: 	}
117: 
118: 	art.append_mode = ARTAppendMode::DEFAULT;
119: 	root.SetGateStatus(GateStatus::GATE_SET);
120: 	Node::Free(art, node);
121: 	node = root;
122: }
123: 
124: void Leaf::TransformToDeprecated(ART &art, Node &node) {
125: 	D_ASSERT(node.GetGateStatus() == GateStatus::GATE_SET || node.GetType() == LEAF);
126: 
127: 	// Early-out, if we never transformed this leaf.
128: 	if (node.GetGateStatus() == GateStatus::GATE_NOT_SET) {
129: 		return;
130: 	}
131: 
132: 	// Collect all row IDs and free the nested leaf.
133: 	unsafe_vector<row_t> row_ids;
134: 	Iterator it(art);
135: 	it.FindMinimum(node);
136: 	ARTKey empty_key = ARTKey();
137: 	it.Scan(empty_key, NumericLimits<row_t>().Maximum(), row_ids, false);
138: 	Node::Free(art, node);
139: 	D_ASSERT(row_ids.size() > 1);
140: 
141: 	// Create the deprecated leaves.
142: 	idx_t remaining = row_ids.size();
143: 	idx_t copy_count = 0;
144: 	reference<Node> ref(node);
145: 	while (remaining) {
146: 		ref.get() = Node::GetAllocator(art, LEAF).New();
147: 		ref.get().SetMetadata(static_cast<uint8_t>(LEAF));
148: 
149: 		auto &leaf = Node::Ref<Leaf>(art, ref, LEAF);
150: 		auto min = MinValue(UnsafeNumericCast<idx_t>(LEAF_SIZE), remaining);
151: 		leaf.count = UnsafeNumericCast<uint8_t>(min);
152: 
153: 		for (uint8_t i = 0; i < leaf.count; i++) {
154: 			leaf.row_ids[i] = row_ids[copy_count + i];
155: 		}
156: 
157: 		copy_count += leaf.count;
158: 		remaining -= leaf.count;
159: 
160: 		ref = leaf.ptr;
161: 		leaf.ptr.Clear();
162: 	}
163: }
164: 
165: //===--------------------------------------------------------------------===//
166: // Deprecated code paths.
167: //===--------------------------------------------------------------------===//
168: 
169: void Leaf::DeprecatedFree(ART &art, Node &node) {
170: 	D_ASSERT(node.GetType() == LEAF);
171: 
172: 	Node next;
173: 	while (node.HasMetadata()) {
174: 		next = Node::Ref<Leaf>(art, node, LEAF).ptr;
175: 		Node::GetAllocator(art, LEAF).Free(node);
176: 		node = next;
177: 	}
178: 	node.Clear();
179: }
180: 
181: bool Leaf::DeprecatedGetRowIds(ART &art, const Node &node, unsafe_vector<row_t> &row_ids, const idx_t max_count) {
182: 	D_ASSERT(node.GetType() == LEAF);
183: 
184: 	reference<const Node> ref(node);
185: 	while (ref.get().HasMetadata()) {
186: 
187: 		auto &leaf = Node::Ref<const Leaf>(art, ref, LEAF);
188: 		if (row_ids.size() + leaf.count > max_count) {
189: 			return false;
190: 		}
191: 		for (uint8_t i = 0; i < leaf.count; i++) {
192: 			row_ids.push_back(leaf.row_ids[i]);
193: 		}
194: 		ref = leaf.ptr;
195: 	}
196: 	return true;
197: }
198: 
199: void Leaf::DeprecatedVacuum(ART &art, Node &node) {
200: 	D_ASSERT(node.HasMetadata());
201: 	D_ASSERT(node.GetType() == LEAF);
202: 
203: 	auto &allocator = Node::GetAllocator(art, LEAF);
204: 	reference<Node> ref(node);
205: 	while (ref.get().HasMetadata()) {
206: 		if (allocator.NeedsVacuum(ref)) {
207: 			ref.get() = allocator.VacuumPointer(ref);
208: 			ref.get().SetMetadata(static_cast<uint8_t>(LEAF));
209: 		}
210: 		auto &leaf = Node::Ref<Leaf>(art, ref, LEAF);
211: 		ref = leaf.ptr;
212: 	}
213: }
214: 
215: string Leaf::DeprecatedVerifyAndToString(ART &art, const Node &node, const bool only_verify) {
216: 	D_ASSERT(node.GetType() == LEAF);
217: 
218: 	string str = "";
219: 	reference<const Node> ref(node);
220: 
221: 	while (ref.get().HasMetadata()) {
222: 		auto &leaf = Node::Ref<const Leaf>(art, ref, LEAF);
223: 		D_ASSERT(leaf.count <= LEAF_SIZE);
224: 
225: 		str += "Leaf [count: " + to_string(leaf.count) + ", row IDs: ";
226: 		for (uint8_t i = 0; i < leaf.count; i++) {
227: 			str += to_string(leaf.row_ids[i]) + "-";
228: 		}
229: 		str += "] ";
230: 		ref = leaf.ptr;
231: 	}
232: 
233: 	return only_verify ? "" : str;
234: }
235: 
236: void Leaf::DeprecatedVerifyAllocations(ART &art, unordered_map<uint8_t, idx_t> &node_counts) const {
237: 	auto idx = Node::GetAllocatorIdx(LEAF);
238: 	node_counts[idx]++;
239: 
240: 	reference<const Node> ref(ptr);
241: 	while (ref.get().HasMetadata()) {
242: 		auto &leaf = Node::Ref<const Leaf>(art, ref, LEAF);
243: 		node_counts[idx]++;
244: 		ref = leaf.ptr;
245: 	}
246: }
247: 
248: } // namespace duckdb
[end of src/execution/index/art/leaf.cpp]
[start of src/execution/index/art/node.cpp]
1: #include "duckdb/execution/index/art/node.hpp"
2: 
3: #include "duckdb/common/limits.hpp"
4: #include "duckdb/common/swap.hpp"
5: #include "duckdb/execution/index/art/art.hpp"
6: #include "duckdb/execution/index/art/art_key.hpp"
7: #include "duckdb/execution/index/art/base_leaf.hpp"
8: #include "duckdb/execution/index/art/base_node.hpp"
9: #include "duckdb/execution/index/art/iterator.hpp"
10: #include "duckdb/execution/index/art/leaf.hpp"
11: #include "duckdb/execution/index/art/node256.hpp"
12: #include "duckdb/execution/index/art/node256_leaf.hpp"
13: #include "duckdb/execution/index/art/node48.hpp"
14: #include "duckdb/execution/index/art/prefix.hpp"
15: #include "duckdb/storage/table_io_manager.hpp"
16: 
17: namespace duckdb {
18: 
19: //===--------------------------------------------------------------------===//
20: // New and free
21: //===--------------------------------------------------------------------===//
22: 
23: void Node::New(ART &art, Node &node, NType type) {
24: 	switch (type) {
25: 	case NType::NODE_7_LEAF:
26: 		Node7Leaf::New(art, node);
27: 		break;
28: 	case NType::NODE_15_LEAF:
29: 		Node15Leaf::New(art, node);
30: 		break;
31: 	case NType::NODE_256_LEAF:
32: 		Node256Leaf::New(art, node);
33: 		break;
34: 	case NType::NODE_4:
35: 		Node4::New(art, node);
36: 		break;
37: 	case NType::NODE_16:
38: 		Node16::New(art, node);
39: 		break;
40: 	case NType::NODE_48:
41: 		Node48::New(art, node);
42: 		break;
43: 	case NType::NODE_256:
44: 		Node256::New(art, node);
45: 		break;
46: 	default:
47: 		throw InternalException("Invalid node type for New: %d.", static_cast<uint8_t>(type));
48: 	}
49: }
50: 
51: void Node::Free(ART &art, Node &node) {
52: 	if (!node.HasMetadata()) {
53: 		return node.Clear();
54: 	}
55: 
56: 	// Free the children.
57: 	auto type = node.GetType();
58: 	switch (type) {
59: 	case NType::PREFIX:
60: 		return Prefix::Free(art, node);
61: 	case NType::LEAF:
62: 		return Leaf::DeprecatedFree(art, node);
63: 	case NType::NODE_4:
64: 		Node4::Free(art, node);
65: 		break;
66: 	case NType::NODE_16:
67: 		Node16::Free(art, node);
68: 		break;
69: 	case NType::NODE_48:
70: 		Node48::Free(art, node);
71: 		break;
72: 	case NType::NODE_256:
73: 		Node256::Free(art, node);
74: 		break;
75: 	case NType::LEAF_INLINED:
76: 		return node.Clear();
77: 	case NType::NODE_7_LEAF:
78: 	case NType::NODE_15_LEAF:
79: 	case NType::NODE_256_LEAF:
80: 		break;
81: 	}
82: 
83: 	GetAllocator(art, type).Free(node);
84: 	node.Clear();
85: }
86: 
87: //===--------------------------------------------------------------------===//
88: // Allocators
89: //===--------------------------------------------------------------------===//
90: 
91: FixedSizeAllocator &Node::GetAllocator(const ART &art, const NType type) {
92: 	return *(*art.allocators)[GetAllocatorIdx(type)];
93: }
94: 
95: uint8_t Node::GetAllocatorIdx(const NType type) {
96: 	switch (type) {
97: 	case NType::PREFIX:
98: 		return 0;
99: 	case NType::LEAF:
100: 		return 1;
101: 	case NType::NODE_4:
102: 		return 2;
103: 	case NType::NODE_16:
104: 		return 3;
105: 	case NType::NODE_48:
106: 		return 4;
107: 	case NType::NODE_256:
108: 		return 5;
109: 	case NType::NODE_7_LEAF:
110: 		return 6;
111: 	case NType::NODE_15_LEAF:
112: 		return 7;
113: 	case NType::NODE_256_LEAF:
114: 		return 8;
115: 	default:
116: 		throw InternalException("Invalid node type for GetAllocatorIdx: %d.", static_cast<uint8_t>(type));
117: 	}
118: }
119: 
120: //===--------------------------------------------------------------------===//
121: // Inserts
122: //===--------------------------------------------------------------------===//
123: 
124: void Node::ReplaceChild(const ART &art, const uint8_t byte, const Node child) const {
125: 	D_ASSERT(HasMetadata());
126: 
127: 	auto type = GetType();
128: 	switch (type) {
129: 	case NType::NODE_4:
130: 		return Node4::ReplaceChild(Ref<Node4>(art, *this, type), byte, child);
131: 	case NType::NODE_16:
132: 		return Node16::ReplaceChild(Ref<Node16>(art, *this, type), byte, child);
133: 	case NType::NODE_48:
134: 		return Ref<Node48>(art, *this, type).ReplaceChild(byte, child);
135: 	case NType::NODE_256:
136: 		return Ref<Node256>(art, *this, type).ReplaceChild(byte, child);
137: 	default:
138: 		throw InternalException("Invalid node type for ReplaceChild: %d.", static_cast<uint8_t>(type));
139: 	}
140: }
141: 
142: void Node::InsertChild(ART &art, Node &node, const uint8_t byte, const Node child) {
143: 	D_ASSERT(node.HasMetadata());
144: 
145: 	auto type = node.GetType();
146: 	switch (type) {
147: 	case NType::NODE_4:
148: 		return Node4::InsertChild(art, node, byte, child);
149: 	case NType::NODE_16:
150: 		return Node16::InsertChild(art, node, byte, child);
151: 	case NType::NODE_48:
152: 		return Node48::InsertChild(art, node, byte, child);
153: 	case NType::NODE_256:
154: 		return Node256::InsertChild(art, node, byte, child);
155: 	case NType::NODE_7_LEAF:
156: 		return Node7Leaf::InsertByte(art, node, byte);
157: 	case NType::NODE_15_LEAF:
158: 		return Node15Leaf::InsertByte(art, node, byte);
159: 	case NType::NODE_256_LEAF:
160: 		return Node256Leaf::InsertByte(art, node, byte);
161: 	default:
162: 		throw InternalException("Invalid node type for InsertChild: %d.", static_cast<uint8_t>(type));
163: 	}
164: }
165: 
166: //===--------------------------------------------------------------------===//
167: // Delete
168: //===--------------------------------------------------------------------===//
169: 
170: void Node::DeleteChild(ART &art, Node &node, Node &prefix, const uint8_t byte, const GateStatus status,
171:                        const ARTKey &row_id) {
172: 	D_ASSERT(node.HasMetadata());
173: 
174: 	auto type = node.GetType();
175: 	switch (type) {
176: 	case NType::NODE_4:
177: 		return Node4::DeleteChild(art, node, prefix, byte, status);
178: 	case NType::NODE_16:
179: 		return Node16::DeleteChild(art, node, byte);
180: 	case NType::NODE_48:
181: 		return Node48::DeleteChild(art, node, byte);
182: 	case NType::NODE_256:
183: 		return Node256::DeleteChild(art, node, byte);
184: 	case NType::NODE_7_LEAF:
185: 		return Node7Leaf::DeleteByte(art, node, prefix, byte, row_id);
186: 	case NType::NODE_15_LEAF:
187: 		return Node15Leaf::DeleteByte(art, node, byte);
188: 	case NType::NODE_256_LEAF:
189: 		return Node256Leaf::DeleteByte(art, node, byte);
190: 	default:
191: 		throw InternalException("Invalid node type for DeleteChild: %d.", static_cast<uint8_t>(type));
192: 	}
193: }
194: 
195: //===--------------------------------------------------------------------===//
196: // Get child and byte.
197: //===--------------------------------------------------------------------===//
198: 
199: template <class NODE>
200: unsafe_optional_ptr<Node> GetChildInternal(ART &art, NODE &node, const uint8_t byte) {
201: 	D_ASSERT(node.HasMetadata());
202: 
203: 	auto type = node.GetType();
204: 	switch (type) {
205: 	case NType::NODE_4:
206: 		return Node4::GetChild(Node::Ref<Node4>(art, node, type), byte);
207: 	case NType::NODE_16:
208: 		return Node16::GetChild(Node::Ref<Node16>(art, node, type), byte);
209: 	case NType::NODE_48:
210: 		return Node48::GetChild(Node::Ref<Node48>(art, node, type), byte);
211: 	case NType::NODE_256: {
212: 		return Node256::GetChild(Node::Ref<Node256>(art, node, type), byte);
213: 	}
214: 	default:
215: 		throw InternalException("Invalid node type for GetChildInternal: %d.", static_cast<uint8_t>(type));
216: 	}
217: }
218: 
219: const unsafe_optional_ptr<Node> Node::GetChild(ART &art, const uint8_t byte) const {
220: 	return GetChildInternal(art, *this, byte);
221: }
222: 
223: unsafe_optional_ptr<Node> Node::GetChildMutable(ART &art, const uint8_t byte) const {
224: 	return GetChildInternal(art, *this, byte);
225: }
226: 
227: template <class NODE>
228: unsafe_optional_ptr<Node> GetNextChildInternal(ART &art, NODE &node, uint8_t &byte) {
229: 	D_ASSERT(node.HasMetadata());
230: 
231: 	auto type = node.GetType();
232: 	switch (type) {
233: 	case NType::NODE_4:
234: 		return Node4::GetNextChild(Node::Ref<Node4>(art, node, type), byte);
235: 	case NType::NODE_16:
236: 		return Node16::GetNextChild(Node::Ref<Node16>(art, node, type), byte);
237: 	case NType::NODE_48:
238: 		return Node48::GetNextChild(Node::Ref<Node48>(art, node, type), byte);
239: 	case NType::NODE_256:
240: 		return Node256::GetNextChild(Node::Ref<Node256>(art, node, type), byte);
241: 	default:
242: 		throw InternalException("Invalid node type for GetNextChildInternal: %d.", static_cast<uint8_t>(type));
243: 	}
244: }
245: 
246: const unsafe_optional_ptr<Node> Node::GetNextChild(ART &art, uint8_t &byte) const {
247: 	return GetNextChildInternal(art, *this, byte);
248: }
249: 
250: unsafe_optional_ptr<Node> Node::GetNextChildMutable(ART &art, uint8_t &byte) const {
251: 	return GetNextChildInternal(art, *this, byte);
252: }
253: 
254: bool Node::HasByte(ART &art, uint8_t &byte) const {
255: 	D_ASSERT(HasMetadata());
256: 
257: 	auto type = GetType();
258: 	switch (type) {
259: 	case NType::NODE_7_LEAF:
260: 		return Ref<const Node7Leaf>(art, *this, NType::NODE_7_LEAF).HasByte(byte);
261: 	case NType::NODE_15_LEAF:
262: 		return Ref<const Node15Leaf>(art, *this, NType::NODE_15_LEAF).HasByte(byte);
263: 	case NType::NODE_256_LEAF:
264: 		return Ref<Node256Leaf>(art, *this, NType::NODE_256_LEAF).HasByte(byte);
265: 	default:
266: 		throw InternalException("Invalid node type for GetNextByte: %d.", static_cast<uint8_t>(type));
267: 	}
268: }
269: 
270: bool Node::GetNextByte(ART &art, uint8_t &byte) const {
271: 	D_ASSERT(HasMetadata());
272: 
273: 	auto type = GetType();
274: 	switch (type) {
275: 	case NType::NODE_7_LEAF:
276: 		return Ref<const Node7Leaf>(art, *this, NType::NODE_7_LEAF).GetNextByte(byte);
277: 	case NType::NODE_15_LEAF:
278: 		return Ref<const Node15Leaf>(art, *this, NType::NODE_15_LEAF).GetNextByte(byte);
279: 	case NType::NODE_256_LEAF:
280: 		return Ref<Node256Leaf>(art, *this, NType::NODE_256_LEAF).GetNextByte(byte);
281: 	default:
282: 		throw InternalException("Invalid node type for GetNextByte: %d.", static_cast<uint8_t>(type));
283: 	}
284: }
285: 
286: //===--------------------------------------------------------------------===//
287: // Utility
288: //===--------------------------------------------------------------------===//
289: 
290: idx_t GetCapacity(NType type) {
291: 	switch (type) {
292: 	case NType::NODE_4:
293: 		return Node4::CAPACITY;
294: 	case NType::NODE_7_LEAF:
295: 		return Node7Leaf::CAPACITY;
296: 	case NType::NODE_15_LEAF:
297: 		return Node15Leaf::CAPACITY;
298: 	case NType::NODE_16:
299: 		return Node16::CAPACITY;
300: 	case NType::NODE_48:
301: 		return Node48::CAPACITY;
302: 	case NType::NODE_256_LEAF:
303: 		return Node256::CAPACITY;
304: 	case NType::NODE_256:
305: 		return Node256::CAPACITY;
306: 	default:
307: 		throw InternalException("Invalid node type for GetCapacity: %d.", static_cast<uint8_t>(type));
308: 	}
309: }
310: 
311: NType Node::GetNodeType(idx_t count) {
312: 	if (count <= Node4::CAPACITY) {
313: 		return NType::NODE_4;
314: 	} else if (count <= Node16::CAPACITY) {
315: 		return NType::NODE_16;
316: 	} else if (count <= Node48::CAPACITY) {
317: 		return NType::NODE_48;
318: 	}
319: 	return NType::NODE_256;
320: }
321: 
322: bool Node::IsNode() const {
323: 	switch (GetType()) {
324: 	case NType::NODE_4:
325: 	case NType::NODE_16:
326: 	case NType::NODE_48:
327: 	case NType::NODE_256:
328: 		return true;
329: 	default:
330: 		return false;
331: 	}
332: }
333: 
334: bool Node::IsLeafNode() const {
335: 	switch (GetType()) {
336: 	case NType::NODE_7_LEAF:
337: 	case NType::NODE_15_LEAF:
338: 	case NType::NODE_256_LEAF:
339: 		return true;
340: 	default:
341: 		return false;
342: 	}
343: }
344: 
345: bool Node::IsAnyLeaf() const {
346: 	if (IsLeafNode()) {
347: 		return true;
348: 	}
349: 
350: 	switch (GetType()) {
351: 	case NType::LEAF_INLINED:
352: 	case NType::LEAF:
353: 		return true;
354: 	default:
355: 		return false;
356: 	}
357: }
358: 
359: //===--------------------------------------------------------------------===//
360: // Merge
361: //===--------------------------------------------------------------------===//
362: 
363: void Node::InitMerge(ART &art, const unsafe_vector<idx_t> &upper_bounds) {
364: 	D_ASSERT(HasMetadata());
365: 	auto type = GetType();
366: 
367: 	switch (type) {
368: 	case NType::PREFIX:
369: 		return Prefix::InitializeMerge(art, *this, upper_bounds);
370: 	case NType::LEAF:
371: 		throw InternalException("Failed to initialize merge due to deprecated ART storage.");
372: 	case NType::NODE_4:
373: 		InitMergeInternal(art, Ref<Node4>(art, *this, type), upper_bounds);
374: 		break;
375: 	case NType::NODE_16:
376: 		InitMergeInternal(art, Ref<Node16>(art, *this, type), upper_bounds);
377: 		break;
378: 	case NType::NODE_48:
379: 		InitMergeInternal(art, Ref<Node48>(art, *this, type), upper_bounds);
380: 		break;
381: 	case NType::NODE_256:
382: 		InitMergeInternal(art, Ref<Node256>(art, *this, type), upper_bounds);
383: 		break;
384: 	case NType::LEAF_INLINED:
385: 		return;
386: 	case NType::NODE_7_LEAF:
387: 	case NType::NODE_15_LEAF:
388: 	case NType::NODE_256_LEAF:
389: 		break;
390: 	}
391: 
392: 	auto idx = GetAllocatorIdx(type);
393: 	IncreaseBufferId(upper_bounds[idx]);
394: }
395: 
396: bool Node::MergeNormalNodes(ART &art, Node &l_node, Node &r_node, uint8_t &byte, const GateStatus status) {
397: 	// Merge N4, N16, N48, N256 nodes.
398: 	D_ASSERT(l_node.IsNode() && r_node.IsNode());
399: 	D_ASSERT(l_node.GetGateStatus() == r_node.GetGateStatus());
400: 
401: 	auto r_child = r_node.GetNextChildMutable(art, byte);
402: 	while (r_child) {
403: 		auto l_child = l_node.GetChildMutable(art, byte);
404: 		if (!l_child) {
405: 			Node::InsertChild(art, l_node, byte, *r_child);
406: 			r_node.ReplaceChild(art, byte);
407: 		} else {
408: 			if (!l_child->MergeInternal(art, *r_child, status)) {
409: 				return false;
410: 			}
411: 		}
412: 
413: 		if (byte == NumericLimits<uint8_t>::Maximum()) {
414: 			break;
415: 		}
416: 		byte++;
417: 		r_child = r_node.GetNextChildMutable(art, byte);
418: 	}
419: 
420: 	Node::Free(art, r_node);
421: 	return true;
422: }
423: 
424: void Node::MergeLeafNodes(ART &art, Node &l_node, Node &r_node, uint8_t &byte) {
425: 	// Merge N7, N15, N256 leaf nodes.
426: 	D_ASSERT(l_node.IsLeafNode() && r_node.IsLeafNode());
427: 	D_ASSERT(l_node.GetGateStatus() == GateStatus::GATE_NOT_SET);
428: 	D_ASSERT(r_node.GetGateStatus() == GateStatus::GATE_NOT_SET);
429: 
430: 	auto has_next = r_node.GetNextByte(art, byte);
431: 	while (has_next) {
432: 		// Row IDs are always unique.
433: 		Node::InsertChild(art, l_node, byte);
434: 		if (byte == NumericLimits<uint8_t>::Maximum()) {
435: 			break;
436: 		}
437: 		byte++;
438: 		has_next = r_node.GetNextByte(art, byte);
439: 	}
440: 
441: 	Node::Free(art, r_node);
442: }
443: 
444: bool Node::MergeNodes(ART &art, Node &other, GateStatus status) {
445: 	// Merge the smaller node into the bigger node.
446: 	if (GetType() < other.GetType()) {
447: 		swap(*this, other);
448: 	}
449: 
450: 	uint8_t byte = 0;
451: 	if (IsNode()) {
452: 		return MergeNormalNodes(art, *this, other, byte, status);
453: 	}
454: 	MergeLeafNodes(art, *this, other, byte);
455: 	return true;
456: }
457: 
458: bool Node::Merge(ART &art, Node &other, const GateStatus status) {
459: 	if (HasMetadata()) {
460: 		return MergeInternal(art, other, status);
461: 	}
462: 
463: 	*this = other;
464: 	other = Node();
465: 	return true;
466: }
467: 
468: bool Node::PrefixContainsOther(ART &art, Node &l_node, Node &r_node, const uint8_t pos, const GateStatus status) {
469: 	// r_node's prefix contains l_node's prefix. l_node must be a node with child nodes.
470: 	D_ASSERT(l_node.IsNode());
471: 
472: 	// Check if the next byte (pos) in r_node exists in l_node.
473: 	auto byte = Prefix::GetByte(art, r_node, pos);
474: 	auto child = l_node.GetChildMutable(art, byte);
475: 
476: 	// Reduce r_node's prefix to the bytes after pos.
477: 	Prefix::Reduce(art, r_node, pos);
478: 	if (child) {
479: 		return child->MergeInternal(art, r_node, status);
480: 	}
481: 
482: 	Node::InsertChild(art, l_node, byte, r_node);
483: 	r_node.Clear();
484: 	return true;
485: }
486: 
487: void Node::MergeIntoNode4(ART &art, Node &l_node, Node &r_node, const uint8_t pos) {
488: 	Node l_child;
489: 	auto l_byte = Prefix::GetByte(art, l_node, pos);
490: 
491: 	reference<Node> ref(l_node);
492: 	auto status = Prefix::Split(art, ref, l_child, pos);
493: 	Node4::New(art, ref);
494: 	ref.get().SetGateStatus(status);
495: 
496: 	Node4::InsertChild(art, ref, l_byte, l_child);
497: 
498: 	auto r_byte = Prefix::GetByte(art, r_node, pos);
499: 	Prefix::Reduce(art, r_node, pos);
500: 	Node4::InsertChild(art, ref, r_byte, r_node);
501: 	r_node.Clear();
502: }
503: 
504: bool Node::MergePrefixes(ART &art, Node &other, const GateStatus status) {
505: 	reference<Node> l_node(*this);
506: 	reference<Node> r_node(other);
507: 	auto pos = DConstants::INVALID_INDEX;
508: 
509: 	if (l_node.get().GetType() == NType::PREFIX && r_node.get().GetType() == NType::PREFIX) {
510: 		// Traverse prefixes. Possibly change the referenced nodes.
511: 		if (!Prefix::Traverse(art, l_node, r_node, pos, status)) {
512: 			return false;
513: 		}
514: 		if (pos == DConstants::INVALID_INDEX) {
515: 			return true;
516: 		}
517: 
518: 	} else {
519: 		// l_prefix contains r_prefix.
520: 		if (l_node.get().GetType() == NType::PREFIX) {
521: 			swap(*this, other);
522: 		}
523: 		pos = 0;
524: 	}
525: 
526: 	D_ASSERT(pos != DConstants::INVALID_INDEX);
527: 	if (l_node.get().GetType() != NType::PREFIX && r_node.get().GetType() == NType::PREFIX) {
528: 		return PrefixContainsOther(art, l_node, r_node, UnsafeNumericCast<uint8_t>(pos), status);
529: 	}
530: 
531: 	// The prefixes differ.
532: 	MergeIntoNode4(art, l_node, r_node, UnsafeNumericCast<uint8_t>(pos));
533: 	return true;
534: }
535: 
536: bool Node::MergeInternal(ART &art, Node &other, const GateStatus status) {
537: 	D_ASSERT(HasMetadata());
538: 	D_ASSERT(other.HasMetadata());
539: 
540: 	// Merge inlined leaves.
541: 	if (GetType() == NType::LEAF_INLINED) {
542: 		swap(*this, other);
543: 	}
544: 	if (other.GetType() == NType::LEAF_INLINED) {
545: 		D_ASSERT(status == GateStatus::GATE_NOT_SET);
546: 		D_ASSERT(other.GetGateStatus() == GateStatus::GATE_SET || other.GetType() == NType::LEAF_INLINED);
547: 		D_ASSERT(GetType() == NType::LEAF_INLINED || GetGateStatus() == GateStatus::GATE_SET);
548: 
549: 		if (art.IsUnique()) {
550: 			return false;
551: 		}
552: 		Leaf::MergeInlined(art, *this, other);
553: 		return true;
554: 	}
555: 
556: 	// Enter a gate.
557: 	if (GetGateStatus() == GateStatus::GATE_SET && status == GateStatus::GATE_NOT_SET) {
558: 		D_ASSERT(other.GetGateStatus() == GateStatus::GATE_SET);
559: 		D_ASSERT(GetType() != NType::LEAF_INLINED);
560: 		D_ASSERT(other.GetType() != NType::LEAF_INLINED);
561: 
562: 		// Get all row IDs.
563: 		unsafe_vector<row_t> row_ids;
564: 		Iterator it(art);
565: 		it.FindMinimum(other);
566: 		ARTKey empty_key = ARTKey();
567: 		it.Scan(empty_key, NumericLimits<row_t>().Maximum(), row_ids, false);
568: 		Node::Free(art, other);
569: 		D_ASSERT(row_ids.size() > 1);
570: 
571: 		// Insert all row IDs.
572: 		ArenaAllocator allocator(Allocator::Get(art.db));
573: 		for (idx_t i = 0; i < row_ids.size(); i++) {
574: 			auto row_id = ARTKey::CreateARTKey<row_t>(allocator, row_ids[i]);
575: 			art.Insert(*this, row_id, 0, row_id, GateStatus::GATE_SET, nullptr);
576: 		}
577: 		return true;
578: 	}
579: 
580: 	// Merge N4, N16, N48, N256 nodes.
581: 	if (IsNode() && other.IsNode()) {
582: 		return MergeNodes(art, other, status);
583: 	}
584: 	// Merge N7, N15, N256 leaf nodes.
585: 	if (IsLeafNode() && other.IsLeafNode()) {
586: 		D_ASSERT(status == GateStatus::GATE_SET);
587: 		return MergeNodes(art, other, status);
588: 	}
589: 
590: 	// Merge prefixes.
591: 	return MergePrefixes(art, other, status);
592: }
593: 
594: //===--------------------------------------------------------------------===//
595: // Vacuum
596: //===--------------------------------------------------------------------===//
597: 
598: void Node::Vacuum(ART &art, const unordered_set<uint8_t> &indexes) {
599: 	D_ASSERT(HasMetadata());
600: 
601: 	auto type = GetType();
602: 	switch (type) {
603: 	case NType::LEAF_INLINED:
604: 		return;
605: 	case NType::PREFIX:
606: 		return Prefix::Vacuum(art, *this, indexes);
607: 	case NType::LEAF:
608: 		if (indexes.find(GetAllocatorIdx(type)) == indexes.end()) {
609: 			return;
610: 		}
611: 		return Leaf::DeprecatedVacuum(art, *this);
612: 	default:
613: 		break;
614: 	}
615: 
616: 	auto idx = GetAllocatorIdx(type);
617: 	auto &allocator = GetAllocator(art, type);
618: 	auto needs_vacuum = indexes.find(idx) != indexes.end() && allocator.NeedsVacuum(*this);
619: 	if (needs_vacuum) {
620: 		auto status = GetGateStatus();
621: 		*this = allocator.VacuumPointer(*this);
622: 		SetMetadata(static_cast<uint8_t>(type));
623: 		SetGateStatus(status);
624: 	}
625: 
626: 	switch (type) {
627: 	case NType::NODE_4:
628: 		return VacuumInternal(art, Ref<Node4>(art, *this, type), indexes);
629: 	case NType::NODE_16:
630: 		return VacuumInternal(art, Ref<Node16>(art, *this, type), indexes);
631: 	case NType::NODE_48:
632: 		return VacuumInternal(art, Ref<Node48>(art, *this, type), indexes);
633: 	case NType::NODE_256:
634: 		return VacuumInternal(art, Ref<Node256>(art, *this, type), indexes);
635: 	case NType::NODE_7_LEAF:
636: 	case NType::NODE_15_LEAF:
637: 	case NType::NODE_256_LEAF:
638: 		return;
639: 	default:
640: 		throw InternalException("Invalid node type for Vacuum: %d.", static_cast<uint8_t>(type));
641: 	}
642: }
643: 
644: //===--------------------------------------------------------------------===//
645: // TransformToDeprecated
646: //===--------------------------------------------------------------------===//
647: 
648: void Node::TransformToDeprecated(ART &art, Node &node, unsafe_unique_ptr<FixedSizeAllocator> &allocator) {
649: 	D_ASSERT(node.HasMetadata());
650: 
651: 	if (node.GetGateStatus() == GateStatus::GATE_SET) {
652: 		D_ASSERT(node.GetType() != NType::LEAF_INLINED);
653: 		return Leaf::TransformToDeprecated(art, node);
654: 	}
655: 
656: 	auto type = node.GetType();
657: 	switch (type) {
658: 	case NType::PREFIX:
659: 		return Prefix::TransformToDeprecated(art, node, allocator);
660: 	case NType::LEAF_INLINED:
661: 		return;
662: 	case NType::LEAF:
663: 		return;
664: 	case NType::NODE_4:
665: 		return TransformToDeprecatedInternal(art, InMemoryRef<Node4>(art, node, type), allocator);
666: 	case NType::NODE_16:
667: 		return TransformToDeprecatedInternal(art, InMemoryRef<Node16>(art, node, type), allocator);
668: 	case NType::NODE_48:
669: 		return TransformToDeprecatedInternal(art, InMemoryRef<Node48>(art, node, type), allocator);
670: 	case NType::NODE_256:
671: 		return TransformToDeprecatedInternal(art, InMemoryRef<Node256>(art, node, type), allocator);
672: 	default:
673: 		throw InternalException("Invalid node type for TransformToDeprecated: %d.", static_cast<uint8_t>(type));
674: 	}
675: }
676: 
677: //===--------------------------------------------------------------------===//
678: // Verification
679: //===--------------------------------------------------------------------===//
680: 
681: string Node::VerifyAndToString(ART &art, const bool only_verify) const {
682: 	D_ASSERT(HasMetadata());
683: 
684: 	auto type = GetType();
685: 	switch (type) {
686: 	case NType::LEAF_INLINED:
687: 		return only_verify ? "" : "Inlined Leaf [row ID: " + to_string(GetRowId()) + "]";
688: 	case NType::LEAF:
689: 		return Leaf::DeprecatedVerifyAndToString(art, *this, only_verify);
690: 	case NType::PREFIX: {
691: 		auto str = Prefix::VerifyAndToString(art, *this, only_verify);
692: 		if (GetGateStatus() == GateStatus::GATE_SET) {
693: 			str = "Gate [ " + str + " ]";
694: 		}
695: 		return only_verify ? "" : "\n" + str;
696: 	}
697: 	default:
698: 		break;
699: 	}
700: 
701: 	string str = "Node" + to_string(GetCapacity(type)) + ": [ ";
702: 	uint8_t byte = 0;
703: 
704: 	if (IsLeafNode()) {
705: 		str = "Leaf " + str;
706: 		auto has_byte = GetNextByte(art, byte);
707: 		while (has_byte) {
708: 			str += to_string(byte) + "-";
709: 			if (byte == NumericLimits<uint8_t>::Maximum()) {
710: 				break;
711: 			}
712: 			byte++;
713: 			has_byte = GetNextByte(art, byte);
714: 		}
715: 	} else {
716: 		auto child = GetNextChild(art, byte);
717: 		while (child) {
718: 			str += "(" + to_string(byte) + ", " + child->VerifyAndToString(art, only_verify) + ")";
719: 			if (byte == NumericLimits<uint8_t>::Maximum()) {
720: 				break;
721: 			}
722: 			byte++;
723: 			child = GetNextChild(art, byte);
724: 		}
725: 	}
726: 
727: 	if (GetGateStatus() == GateStatus::GATE_SET) {
728: 		str = "Gate [ " + str + " ]";
729: 	}
730: 	return only_verify ? "" : "\n" + str + "]";
731: }
732: 
733: void Node::VerifyAllocations(ART &art, unordered_map<uint8_t, idx_t> &node_counts) const {
734: 	D_ASSERT(HasMetadata());
735: 
736: 	auto type = GetType();
737: 	switch (type) {
738: 	case NType::PREFIX:
739: 		return Prefix::VerifyAllocations(art, *this, node_counts);
740: 	case NType::LEAF:
741: 		return Ref<Leaf>(art, *this, type).DeprecatedVerifyAllocations(art, node_counts);
742: 	case NType::LEAF_INLINED:
743: 		return;
744: 	case NType::NODE_4:
745: 		VerifyAllocationsInternal(art, Ref<Node4>(art, *this, type), node_counts);
746: 		break;
747: 	case NType::NODE_16:
748: 		VerifyAllocationsInternal(art, Ref<Node16>(art, *this, type), node_counts);
749: 		break;
750: 	case NType::NODE_48:
751: 		VerifyAllocationsInternal(art, Ref<Node48>(art, *this, type), node_counts);
752: 		break;
753: 	case NType::NODE_256:
754: 		VerifyAllocationsInternal(art, Ref<Node256>(art, *this, type), node_counts);
755: 		break;
756: 	case NType::NODE_7_LEAF:
757: 	case NType::NODE_15_LEAF:
758: 	case NType::NODE_256_LEAF:
759: 		break;
760: 	}
761: 
762: 	node_counts[GetAllocatorIdx(type)]++;
763: }
764: 
765: } // namespace duckdb
[end of src/execution/index/art/node.cpp]
[start of src/execution/index/art/prefix.cpp]
1: #include "duckdb/execution/index/art/prefix.hpp"
2: 
3: #include "duckdb/common/swap.hpp"
4: #include "duckdb/execution/index/art/art.hpp"
5: #include "duckdb/execution/index/art/art_key.hpp"
6: #include "duckdb/execution/index/art/base_leaf.hpp"
7: #include "duckdb/execution/index/art/base_node.hpp"
8: #include "duckdb/execution/index/art/leaf.hpp"
9: #include "duckdb/execution/index/art/node.hpp"
10: 
11: namespace duckdb {
12: 
13: Prefix::Prefix(const ART &art, const Node ptr_p, const bool is_mutable, const bool set_in_memory) {
14: 	if (!set_in_memory) {
15: 		data = Node::GetAllocator(art, PREFIX).Get(ptr_p, is_mutable);
16: 	} else {
17: 		data = Node::GetAllocator(art, PREFIX).GetIfLoaded(ptr_p);
18: 		if (!data) {
19: 			ptr = nullptr;
20: 			in_memory = false;
21: 			return;
22: 		}
23: 	}
24: 	ptr = reinterpret_cast<Node *>(data + Count(art) + 1);
25: 	in_memory = true;
26: }
27: 
28: Prefix::Prefix(unsafe_unique_ptr<FixedSizeAllocator> &allocator, const Node ptr_p, const idx_t count) {
29: 	data = allocator->Get(ptr_p, true);
30: 	ptr = reinterpret_cast<Node *>(data + count + 1);
31: 	in_memory = true;
32: }
33: 
34: idx_t Prefix::GetMismatchWithOther(const Prefix &l_prefix, const Prefix &r_prefix, const idx_t max_count) {
35: 	for (idx_t i = 0; i < max_count; i++) {
36: 		if (l_prefix.data[i] != r_prefix.data[i]) {
37: 			return i;
38: 		}
39: 	}
40: 	return DConstants::INVALID_INDEX;
41: }
42: 
43: idx_t Prefix::GetMismatchWithKey(ART &art, const Node &node, const ARTKey &key, idx_t &depth) {
44: 	Prefix prefix(art, node);
45: 	for (idx_t i = 0; i < prefix.data[Prefix::Count(art)]; i++) {
46: 		if (prefix.data[i] != key[depth]) {
47: 			return i;
48: 		}
49: 		depth++;
50: 	}
51: 	return DConstants::INVALID_INDEX;
52: }
53: 
54: uint8_t Prefix::GetByte(const ART &art, const Node &node, const uint8_t pos) {
55: 	D_ASSERT(node.GetType() == PREFIX);
56: 	Prefix prefix(art, node);
57: 	return prefix.data[pos];
58: }
59: 
60: Prefix Prefix::NewInternal(ART &art, Node &node, const data_ptr_t data, const uint8_t count, const idx_t offset,
61:                            const NType type) {
62: 	node = Node::GetAllocator(art, type).New();
63: 	node.SetMetadata(static_cast<uint8_t>(type));
64: 
65: 	Prefix prefix(art, node, true);
66: 	prefix.data[Count(art)] = count;
67: 	if (data) {
68: 		D_ASSERT(count);
69: 		memcpy(prefix.data, data + offset, count);
70: 	}
71: 	return prefix;
72: }
73: 
74: void Prefix::New(ART &art, reference<Node> &ref, const ARTKey &key, const idx_t depth, idx_t count) {
75: 	idx_t offset = 0;
76: 
77: 	while (count) {
78: 		auto min = MinValue(UnsafeNumericCast<idx_t>(Count(art)), count);
79: 		auto this_count = UnsafeNumericCast<uint8_t>(min);
80: 		auto prefix = NewInternal(art, ref, key.data, this_count, offset + depth, PREFIX);
81: 
82: 		ref = *prefix.ptr;
83: 		offset += this_count;
84: 		count -= this_count;
85: 	}
86: }
87: 
88: void Prefix::Free(ART &art, Node &node) {
89: 	Node next;
90: 
91: 	while (node.HasMetadata() && node.GetType() == PREFIX) {
92: 		Prefix prefix(art, node, true);
93: 		next = *prefix.ptr;
94: 		Node::GetAllocator(art, PREFIX).Free(node);
95: 		node = next;
96: 	}
97: 
98: 	Node::Free(art, node);
99: 	node.Clear();
100: }
101: 
102: void Prefix::InitializeMerge(ART &art, Node &node, const unsafe_vector<idx_t> &upper_bounds) {
103: 	auto buffer_count = upper_bounds[Node::GetAllocatorIdx(PREFIX)];
104: 	Node next = node;
105: 	Prefix prefix(art, next, true);
106: 
107: 	while (next.GetType() == PREFIX) {
108: 		next = *prefix.ptr;
109: 		if (prefix.ptr->GetType() == PREFIX) {
110: 			prefix.ptr->IncreaseBufferId(buffer_count);
111: 			prefix = Prefix(art, next, true);
112: 		}
113: 	}
114: 
115: 	node.IncreaseBufferId(buffer_count);
116: 	prefix.ptr->InitMerge(art, upper_bounds);
117: }
118: 
119: void Prefix::Concat(ART &art, Node &parent, uint8_t byte, const GateStatus old_status, const Node &child,
120:                     const GateStatus status) {
121: 	D_ASSERT(!parent.IsAnyLeaf());
122: 	D_ASSERT(child.HasMetadata());
123: 
124: 	if (old_status == GateStatus::GATE_SET) {
125: 		// Concat Node4.
126: 		D_ASSERT(status == GateStatus::GATE_SET);
127: 		return ConcatGate(art, parent, byte, child);
128: 	}
129: 	if (child.GetGateStatus() == GateStatus::GATE_SET) {
130: 		// Concat Node4.
131: 		D_ASSERT(status == GateStatus::GATE_NOT_SET);
132: 		return ConcatChildIsGate(art, parent, byte, child);
133: 	}
134: 
135: 	if (status == GateStatus::GATE_SET && child.GetType() == NType::LEAF_INLINED) {
136: 		auto row_id = child.GetRowId();
137: 		Free(art, parent);
138: 		Leaf::New(parent, row_id);
139: 		return;
140: 	}
141: 
142: 	if (parent.GetType() != PREFIX) {
143: 		auto prefix = NewInternal(art, parent, &byte, 1, 0, PREFIX);
144: 		if (child.GetType() == PREFIX) {
145: 			prefix.Append(art, child);
146: 		} else {
147: 			*prefix.ptr = child;
148: 		}
149: 		return;
150: 	}
151: 
152: 	auto tail = GetTail(art, parent);
153: 	tail = tail.Append(art, byte);
154: 
155: 	if (child.GetType() == PREFIX) {
156: 		tail.Append(art, child);
157: 	} else {
158: 		*tail.ptr = child;
159: 	}
160: }
161: 
162: template <class NODE>
163: idx_t TraverseInternal(ART &art, reference<NODE> &node, const ARTKey &key, idx_t &depth,
164:                        const bool is_mutable = false) {
165: 	D_ASSERT(node.get().HasMetadata());
166: 	D_ASSERT(node.get().GetType() == NType::PREFIX);
167: 
168: 	while (node.get().GetType() == NType::PREFIX) {
169: 		auto pos = Prefix::GetMismatchWithKey(art, node, key, depth);
170: 		if (pos != DConstants::INVALID_INDEX) {
171: 			return pos;
172: 		}
173: 
174: 		Prefix prefix(art, node, is_mutable);
175: 		node = *prefix.ptr;
176: 		if (node.get().GetGateStatus() == GateStatus::GATE_SET) {
177: 			break;
178: 		}
179: 	}
180: 	return DConstants::INVALID_INDEX;
181: }
182: 
183: idx_t Prefix::Traverse(ART &art, reference<const Node> &node, const ARTKey &key, idx_t &depth) {
184: 	return TraverseInternal<const Node>(art, node, key, depth);
185: }
186: 
187: idx_t Prefix::TraverseMutable(ART &art, reference<Node> &node, const ARTKey &key, idx_t &depth) {
188: 	return TraverseInternal<Node>(art, node, key, depth, true);
189: }
190: 
191: bool Prefix::Traverse(ART &art, reference<Node> &l_node, reference<Node> &r_node, idx_t &pos, const GateStatus status) {
192: 	D_ASSERT(l_node.get().HasMetadata());
193: 	D_ASSERT(r_node.get().HasMetadata());
194: 
195: 	Prefix l_prefix(art, l_node, true);
196: 	Prefix r_prefix(art, r_node, true);
197: 
198: 	idx_t max_count = MinValue(l_prefix.data[Count(art)], r_prefix.data[Count(art)]);
199: 	pos = GetMismatchWithOther(l_prefix, r_prefix, max_count);
200: 	if (pos != DConstants::INVALID_INDEX) {
201: 		return true;
202: 	}
203: 
204: 	// Match.
205: 	if (l_prefix.data[Count(art)] == r_prefix.data[Count(art)]) {
206: 		auto r_child = *r_prefix.ptr;
207: 		r_prefix.ptr->Clear();
208: 		Node::Free(art, r_node);
209: 		return l_prefix.ptr->MergeInternal(art, r_child, status);
210: 	}
211: 
212: 	pos = max_count;
213: 	if (r_prefix.ptr->GetType() != PREFIX && r_prefix.data[Count(art)] == max_count) {
214: 		// l_prefix contains r_prefix.
215: 		swap(l_node.get(), r_node.get());
216: 		l_node = *r_prefix.ptr;
217: 		return true;
218: 	}
219: 	// r_prefix contains l_prefix.
220: 	l_node = *l_prefix.ptr;
221: 	return true;
222: }
223: 
224: void Prefix::Reduce(ART &art, Node &node, const idx_t pos) {
225: 	D_ASSERT(node.HasMetadata());
226: 	D_ASSERT(pos < Count(art));
227: 
228: 	Prefix prefix(art, node);
229: 	if (pos == idx_t(prefix.data[Count(art)] - 1)) {
230: 		auto next = *prefix.ptr;
231: 		prefix.ptr->Clear();
232: 		Node::Free(art, node);
233: 		node = next;
234: 		return;
235: 	}
236: 
237: 	for (idx_t i = 0; i < Count(art) - pos - 1; i++) {
238: 		prefix.data[i] = prefix.data[pos + i + 1];
239: 	}
240: 
241: 	prefix.data[Count(art)] -= pos + 1;
242: 	prefix.Append(art, *prefix.ptr);
243: }
244: 
245: GateStatus Prefix::Split(ART &art, reference<Node> &node, Node &child, const uint8_t pos) {
246: 	D_ASSERT(node.get().HasMetadata());
247: 
248: 	Prefix prefix(art, node, true);
249: 
250: 	// The split is at the last prefix byte. Decrease the count and return.
251: 	if (pos + 1 == Count(art)) {
252: 		prefix.data[Count(art)]--;
253: 		node = *prefix.ptr;
254: 		child = *prefix.ptr;
255: 		return GateStatus::GATE_NOT_SET;
256: 	}
257: 
258: 	if (pos + 1 < prefix.data[Count(art)]) {
259: 		// Create a new prefix and
260: 		// 1. copy the remaining bytes of this prefix.
261: 		// 2. append remaining prefix nodes.
262: 		auto new_prefix = NewInternal(art, child, nullptr, 0, 0, PREFIX);
263: 		new_prefix.data[Count(art)] = prefix.data[Count(art)] - pos - 1;
264: 		memcpy(new_prefix.data, prefix.data + pos + 1, new_prefix.data[Count(art)]);
265: 
266: 		if (prefix.ptr->GetType() == PREFIX && prefix.ptr->GetGateStatus() == GateStatus::GATE_NOT_SET) {
267: 			new_prefix.Append(art, *prefix.ptr);
268: 		} else {
269: 			*new_prefix.ptr = *prefix.ptr;
270: 		}
271: 
272: 	} else if (pos + 1 == prefix.data[Count(art)]) {
273: 		// No prefix bytes after the split.
274: 		child = *prefix.ptr;
275: 	}
276: 
277: 	// Set the new count of this node.
278: 	prefix.data[Count(art)] = pos;
279: 
280: 	// No bytes left before the split, free this node.
281: 	if (pos == 0) {
282: 		auto old_status = node.get().GetGateStatus();
283: 		prefix.ptr->Clear();
284: 		Node::Free(art, node);
285: 		return old_status;
286: 	}
287: 
288: 	// There are bytes left before the split.
289: 	// The subsequent node replaces the split byte.
290: 	node = *prefix.ptr;
291: 	return GateStatus::GATE_NOT_SET;
292: }
293: 
294: ARTConflictType Prefix::Insert(ART &art, Node &node, const ARTKey &key, idx_t depth, const ARTKey &row_id,
295:                                const GateStatus status, optional_ptr<ART> delete_art) {
296: 	reference<Node> next(node);
297: 	auto pos = TraverseMutable(art, next, key, depth);
298: 
299: 	// We recurse into the next node, if
300: 	// (1) the prefix matches the key.
301: 	// (2) we reach a gate.
302: 	if (pos == DConstants::INVALID_INDEX) {
303: 		if (next.get().GetType() != NType::PREFIX || next.get().GetGateStatus() == GateStatus::GATE_SET) {
304: 			return art.Insert(next, key, depth, row_id, status, delete_art);
305: 		}
306: 	}
307: 
308: 	Node remainder;
309: 	auto byte = GetByte(art, next, UnsafeNumericCast<uint8_t>(pos));
310: 	auto split_status = Split(art, next, remainder, UnsafeNumericCast<uint8_t>(pos));
311: 	Node4::New(art, next);
312: 	next.get().SetGateStatus(split_status);
313: 
314: 	// Insert the remaining prefix into the new Node4.
315: 	Node4::InsertChild(art, next, byte, remainder);
316: 
317: 	if (status == GateStatus::GATE_SET) {
318: 		D_ASSERT(pos != ROW_ID_COUNT);
319: 		Node new_row_id;
320: 		Leaf::New(new_row_id, key.GetRowId());
321: 		Node::InsertChild(art, next, key[depth], new_row_id);
322: 		return ARTConflictType::NO_CONFLICT;
323: 	}
324: 
325: 	Node leaf;
326: 	reference<Node> ref(leaf);
327: 	if (depth + 1 < key.len) {
328: 		// Create the prefix.
329: 		auto count = key.len - depth - 1;
330: 		Prefix::New(art, ref, key, depth + 1, count);
331: 	}
332: 	// Create the inlined leaf.
333: 	Leaf::New(ref, row_id.GetRowId());
334: 	Node4::InsertChild(art, next, key[depth], leaf);
335: 	return ARTConflictType::NO_CONFLICT;
336: }
337: 
338: string Prefix::VerifyAndToString(ART &art, const Node &node, const bool only_verify) {
339: 	string str = "";
340: 	reference<const Node> ref(node);
341: 
342: 	Iterator(art, ref, true, false, [&](Prefix &prefix) {
343: 		D_ASSERT(prefix.data[Count(art)] != 0);
344: 		D_ASSERT(prefix.data[Count(art)] <= Count(art));
345: 
346: 		str += " Prefix :[ ";
347: 		for (idx_t i = 0; i < prefix.data[Count(art)]; i++) {
348: 			str += to_string(prefix.data[i]) + "-";
349: 		}
350: 		str += " ] ";
351: 	});
352: 
353: 	auto child = ref.get().VerifyAndToString(art, only_verify);
354: 	return only_verify ? "" : str + child;
355: }
356: 
357: void Prefix::VerifyAllocations(ART &art, const Node &node, unordered_map<uint8_t, idx_t> &node_counts) {
358: 	auto idx = Node::GetAllocatorIdx(PREFIX);
359: 	reference<const Node> ref(node);
360: 	Iterator(art, ref, false, false, [&](Prefix &prefix) { node_counts[idx]++; });
361: 	return ref.get().VerifyAllocations(art, node_counts);
362: }
363: 
364: void Prefix::Vacuum(ART &art, Node &node, const unordered_set<uint8_t> &indexes) {
365: 	bool set = indexes.find(Node::GetAllocatorIdx(PREFIX)) != indexes.end();
366: 	auto &allocator = Node::GetAllocator(art, PREFIX);
367: 
368: 	reference<Node> ref(node);
369: 	while (ref.get().GetType() == PREFIX) {
370: 		if (set && allocator.NeedsVacuum(ref)) {
371: 			auto status = ref.get().GetGateStatus();
372: 			ref.get() = allocator.VacuumPointer(ref);
373: 			ref.get().SetMetadata(static_cast<uint8_t>(PREFIX));
374: 			ref.get().SetGateStatus(status);
375: 		}
376: 		Prefix prefix(art, ref, true);
377: 		ref = *prefix.ptr;
378: 	}
379: 
380: 	ref.get().Vacuum(art, indexes);
381: }
382: 
383: void Prefix::TransformToDeprecated(ART &art, Node &node, unsafe_unique_ptr<FixedSizeAllocator> &allocator) {
384: 	// Early-out, if we do not need any transformations.
385: 	if (!allocator) {
386: 		reference<Node> ref(node);
387: 		while (ref.get().GetType() == PREFIX && ref.get().GetGateStatus() == GateStatus::GATE_NOT_SET) {
388: 			Prefix prefix(art, ref, true, true);
389: 			if (!prefix.in_memory) {
390: 				return;
391: 			}
392: 			ref = *prefix.ptr;
393: 		}
394: 		return Node::TransformToDeprecated(art, ref, allocator);
395: 	}
396: 
397: 	// We need to create a new prefix (chain).
398: 	Node new_node;
399: 	new_node = allocator->New();
400: 	new_node.SetMetadata(static_cast<uint8_t>(PREFIX));
401: 	Prefix new_prefix(allocator, new_node, DEPRECATED_COUNT);
402: 
403: 	Node current_node = node;
404: 	while (current_node.GetType() == PREFIX && current_node.GetGateStatus() == GateStatus::GATE_NOT_SET) {
405: 		Prefix prefix(art, current_node, true, true);
406: 		if (!prefix.in_memory) {
407: 			return;
408: 		}
409: 
410: 		for (idx_t i = 0; i < prefix.data[Count(art)]; i++) {
411: 			new_prefix = new_prefix.TransformToDeprecatedAppend(art, allocator, prefix.data[i]);
412: 		}
413: 
414: 		*new_prefix.ptr = *prefix.ptr;
415: 		prefix.ptr->Clear();
416: 		Node::Free(art, current_node);
417: 		current_node = *new_prefix.ptr;
418: 	}
419: 
420: 	node = new_node;
421: 	return Node::TransformToDeprecated(art, *new_prefix.ptr, allocator);
422: }
423: 
424: Prefix Prefix::Append(ART &art, const uint8_t byte) {
425: 	if (data[Count(art)] != Count(art)) {
426: 		data[data[Count(art)]] = byte;
427: 		data[Count(art)]++;
428: 		return *this;
429: 	}
430: 
431: 	auto prefix = NewInternal(art, *ptr, nullptr, 0, 0, PREFIX);
432: 	return prefix.Append(art, byte);
433: }
434: 
435: void Prefix::Append(ART &art, Node other) {
436: 	D_ASSERT(other.HasMetadata());
437: 
438: 	Prefix prefix = *this;
439: 	while (other.GetType() == PREFIX) {
440: 		if (other.GetGateStatus() == GateStatus::GATE_SET) {
441: 			*prefix.ptr = other;
442: 			return;
443: 		}
444: 
445: 		Prefix other_prefix(art, other, true);
446: 		for (idx_t i = 0; i < other_prefix.data[Count(art)]; i++) {
447: 			prefix = prefix.Append(art, other_prefix.data[i]);
448: 		}
449: 
450: 		*prefix.ptr = *other_prefix.ptr;
451: 		Node::GetAllocator(art, PREFIX).Free(other);
452: 		other = *prefix.ptr;
453: 	}
454: }
455: 
456: Prefix Prefix::GetTail(ART &art, const Node &node) {
457: 	Prefix prefix(art, node, true);
458: 	while (prefix.ptr->GetType() == PREFIX) {
459: 		prefix = Prefix(art, *prefix.ptr, true);
460: 	}
461: 	return prefix;
462: }
463: 
464: void Prefix::ConcatGate(ART &art, Node &parent, uint8_t byte, const Node &child) {
465: 	D_ASSERT(child.HasMetadata());
466: 	Node new_prefix = Node();
467: 
468: 	// Inside gates, inlined row IDs are not prefixed.
469: 	if (child.GetType() == NType::LEAF_INLINED) {
470: 		Leaf::New(new_prefix, child.GetRowId());
471: 
472: 	} else if (child.GetType() == PREFIX) {
473: 		// At least one more row ID in this gate.
474: 		auto prefix = NewInternal(art, new_prefix, &byte, 1, 0, PREFIX);
475: 		prefix.ptr->Clear();
476: 		prefix.Append(art, child);
477: 		new_prefix.SetGateStatus(GateStatus::GATE_SET);
478: 
479: 	} else {
480: 		// At least one more row ID in this gate.
481: 		auto prefix = NewInternal(art, new_prefix, &byte, 1, 0, PREFIX);
482: 		*prefix.ptr = child;
483: 		new_prefix.SetGateStatus(GateStatus::GATE_SET);
484: 	}
485: 
486: 	if (parent.GetType() != PREFIX) {
487: 		parent = new_prefix;
488: 		return;
489: 	}
490: 	*GetTail(art, parent).ptr = new_prefix;
491: }
492: 
493: void Prefix::ConcatChildIsGate(ART &art, Node &parent, uint8_t byte, const Node &child) {
494: 	// Create a new prefix and point it to the gate.
495: 	if (parent.GetType() != PREFIX) {
496: 		auto prefix = NewInternal(art, parent, &byte, 1, 0, PREFIX);
497: 		*prefix.ptr = child;
498: 		return;
499: 	}
500: 
501: 	auto tail = GetTail(art, parent);
502: 	tail = tail.Append(art, byte);
503: 	*tail.ptr = child;
504: }
505: 
506: Prefix Prefix::TransformToDeprecatedAppend(ART &art, unsafe_unique_ptr<FixedSizeAllocator> &allocator, uint8_t byte) {
507: 	if (data[DEPRECATED_COUNT] != DEPRECATED_COUNT) {
508: 		data[data[DEPRECATED_COUNT]] = byte;
509: 		data[DEPRECATED_COUNT]++;
510: 		return *this;
511: 	}
512: 
513: 	*ptr = allocator->New();
514: 	ptr->SetMetadata(static_cast<uint8_t>(PREFIX));
515: 	Prefix prefix(allocator, *ptr, DEPRECATED_COUNT);
516: 	return prefix.TransformToDeprecatedAppend(art, allocator, byte);
517: }
518: 
519: } // namespace duckdb
[end of src/execution/index/art/prefix.cpp]
[start of src/execution/operator/persistent/physical_delete.cpp]
1: #include "duckdb/execution/operator/persistent/physical_delete.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/atomic.hpp"
5: #include "duckdb/common/types/column/column_data_collection.hpp"
6: #include "duckdb/execution/expression_executor.hpp"
7: #include "duckdb/execution/index/bound_index.hpp"
8: #include "duckdb/storage/data_table.hpp"
9: #include "duckdb/storage/table/delete_state.hpp"
10: #include "duckdb/storage/table/scan_state.hpp"
11: #include "duckdb/transaction/duck_transaction.hpp"
12: 
13: namespace duckdb {
14: 
15: PhysicalDelete::PhysicalDelete(vector<LogicalType> types, TableCatalogEntry &tableref, DataTable &table,
16:                                vector<unique_ptr<BoundConstraint>> bound_constraints, idx_t row_id_index,
17:                                idx_t estimated_cardinality, bool return_chunk)
18:     : PhysicalOperator(PhysicalOperatorType::DELETE_OPERATOR, std::move(types), estimated_cardinality),
19:       tableref(tableref), table(table), bound_constraints(std::move(bound_constraints)), row_id_index(row_id_index),
20:       return_chunk(return_chunk) {
21: }
22: //===--------------------------------------------------------------------===//
23: // Sink
24: //===--------------------------------------------------------------------===//
25: class DeleteGlobalState : public GlobalSinkState {
26: public:
27: 	explicit DeleteGlobalState(ClientContext &context, const vector<LogicalType> &return_types,
28: 	                           TableCatalogEntry &table, const vector<unique_ptr<BoundConstraint>> &bound_constraints)
29: 	    : deleted_count(0), return_collection(context, return_types), has_unique_indexes(false) {
30: 
31: 		// We need to append deletes to the local delete-ART.
32: 		auto &storage = table.GetStorage();
33: 		if (storage.HasUniqueIndexes()) {
34: 			storage.InitializeLocalStorage(delete_index_append_state, table, context, bound_constraints);
35: 			has_unique_indexes = true;
36: 		}
37: 	}
38: 
39: 	mutex delete_lock;
40: 	idx_t deleted_count;
41: 	ColumnDataCollection return_collection;
42: 	LocalAppendState delete_index_append_state;
43: 	bool has_unique_indexes;
44: };
45: 
46: class DeleteLocalState : public LocalSinkState {
47: public:
48: 	DeleteLocalState(ClientContext &context, TableCatalogEntry &table,
49: 	                 const vector<unique_ptr<BoundConstraint>> &bound_constraints) {
50: 		const auto &types = table.GetTypes();
51: 		auto initialize = vector<bool>(types.size(), false);
52: 		delete_chunk.Initialize(Allocator::Get(context), types, initialize);
53: 
54: 		auto &storage = table.GetStorage();
55: 		delete_state = storage.InitializeDelete(table, context, bound_constraints);
56: 	}
57: 
58: public:
59: 	DataChunk delete_chunk;
60: 	unique_ptr<TableDeleteState> delete_state;
61: };
62: 
63: SinkResultType PhysicalDelete::Sink(ExecutionContext &context, DataChunk &chunk, OperatorSinkInput &input) const {
64: 	auto &g_state = input.global_state.Cast<DeleteGlobalState>();
65: 	auto &l_state = input.local_state.Cast<DeleteLocalState>();
66: 
67: 	auto &transaction = DuckTransaction::Get(context.client, table.db);
68: 	auto &row_ids = chunk.data[row_id_index];
69: 
70: 	lock_guard<mutex> delete_guard(g_state.delete_lock);
71: 	if (!return_chunk && !g_state.has_unique_indexes) {
72: 		g_state.deleted_count += table.Delete(*l_state.delete_state, context.client, row_ids, chunk.size());
73: 		return SinkResultType::NEED_MORE_INPUT;
74: 	}
75: 
76: 	auto types = table.GetTypes();
77: 	auto to_be_fetched = vector<bool>(types.size(), return_chunk);
78: 	vector<StorageIndex> column_ids;
79: 	vector<LogicalType> column_types;
80: 	if (return_chunk) {
81: 		// Fetch all columns.
82: 		column_types = types;
83: 		for (idx_t i = 0; i < table.ColumnCount(); i++) {
84: 			column_ids.emplace_back(i);
85: 		}
86: 
87: 	} else {
88: 		// Fetch only the required columns for updating the delete indexes.
89: 		auto &local_storage = LocalStorage::Get(context.client, table.db);
90: 		auto storage = local_storage.GetStorage(table);
91: 		unordered_set<column_t> indexed_column_id_set;
92: 		storage->delete_indexes.Scan([&](Index &index) {
93: 			if (!index.IsBound() || !index.IsUnique()) {
94: 				return false;
95: 			}
96: 			auto &set = index.GetColumnIdSet();
97: 			indexed_column_id_set.insert(set.begin(), set.end());
98: 			return false;
99: 		});
100: 		for (auto &col : indexed_column_id_set) {
101: 			column_ids.emplace_back(col);
102: 		}
103: 		sort(column_ids.begin(), column_ids.end());
104: 		for (auto &col : column_ids) {
105: 			auto i = col.GetPrimaryIndex();
106: 			to_be_fetched[i] = true;
107: 			column_types.push_back(types[i]);
108: 		}
109: 	}
110: 
111: 	l_state.delete_chunk.Reset();
112: 	row_ids.Flatten(chunk.size());
113: 
114: 	// Fetch the to-be-deleted chunk.
115: 	DataChunk fetch_chunk;
116: 	fetch_chunk.Initialize(Allocator::Get(context.client), column_types, chunk.size());
117: 	auto fetch_state = ColumnFetchState();
118: 	table.Fetch(transaction, fetch_chunk, column_ids, row_ids, chunk.size(), fetch_state);
119: 
120: 	// Reference the necessary columns of the fetch_chunk.
121: 	idx_t fetch_idx = 0;
122: 	for (idx_t i = 0; i < table.ColumnCount(); i++) {
123: 		if (to_be_fetched[i]) {
124: 			l_state.delete_chunk.data[i].Reference(fetch_chunk.data[fetch_idx++]);
125: 			continue;
126: 		}
127: 		l_state.delete_chunk.data[i].Reference(Value(types[i]));
128: 	}
129: 	l_state.delete_chunk.SetCardinality(fetch_chunk);
130: 
131: 	// Append the deleted row IDs to the delete indexes.
132: 	// If we only delete local row IDs, then the delete_chunk is empty.
133: 	if (g_state.has_unique_indexes && l_state.delete_chunk.size() != 0) {
134: 		auto &local_storage = LocalStorage::Get(context.client, table.db);
135: 		auto storage = local_storage.GetStorage(table);
136: 		storage->delete_indexes.Scan([&](Index &index) {
137: 			if (!index.IsBound() || !index.IsUnique()) {
138: 				return false;
139: 			}
140: 			auto &bound_index = index.Cast<BoundIndex>();
141: 			auto error = bound_index.Append(l_state.delete_chunk, row_ids);
142: 			if (error.HasError()) {
143: 				throw InternalException("failed to update delete ART in physical delete: ", error.Message());
144: 			}
145: 			return false;
146: 		});
147: 	}
148: 
149: 	// Append the return_chunk to the return collection.
150: 	if (return_chunk) {
151: 		g_state.return_collection.Append(l_state.delete_chunk);
152: 	}
153: 
154: 	g_state.deleted_count += table.Delete(*l_state.delete_state, context.client, row_ids, chunk.size());
155: 	return SinkResultType::NEED_MORE_INPUT;
156: }
157: 
158: unique_ptr<GlobalSinkState> PhysicalDelete::GetGlobalSinkState(ClientContext &context) const {
159: 	return make_uniq<DeleteGlobalState>(context, GetTypes(), tableref, bound_constraints);
160: }
161: 
162: unique_ptr<LocalSinkState> PhysicalDelete::GetLocalSinkState(ExecutionContext &context) const {
163: 	return make_uniq<DeleteLocalState>(context.client, tableref, bound_constraints);
164: }
165: 
166: //===--------------------------------------------------------------------===//
167: // Source
168: //===--------------------------------------------------------------------===//
169: class DeleteSourceState : public GlobalSourceState {
170: public:
171: 	explicit DeleteSourceState(const PhysicalDelete &op) {
172: 		if (op.return_chunk) {
173: 			D_ASSERT(op.sink_state);
174: 			auto &g = op.sink_state->Cast<DeleteGlobalState>();
175: 			g.return_collection.InitializeScan(scan_state);
176: 		}
177: 	}
178: 
179: 	ColumnDataScanState scan_state;
180: };
181: 
182: unique_ptr<GlobalSourceState> PhysicalDelete::GetGlobalSourceState(ClientContext &context) const {
183: 	return make_uniq<DeleteSourceState>(*this);
184: }
185: 
186: SourceResultType PhysicalDelete::GetData(ExecutionContext &context, DataChunk &chunk,
187:                                          OperatorSourceInput &input) const {
188: 	auto &state = input.global_state.Cast<DeleteSourceState>();
189: 	auto &g = sink_state->Cast<DeleteGlobalState>();
190: 	if (!return_chunk) {
191: 		chunk.SetCardinality(1);
192: 		chunk.SetValue(0, 0, Value::BIGINT(NumericCast<int64_t>(g.deleted_count)));
193: 		return SourceResultType::FINISHED;
194: 	}
195: 
196: 	g.return_collection.Scan(state.scan_state, chunk);
197: 	return chunk.size() == 0 ? SourceResultType::FINISHED : SourceResultType::HAVE_MORE_OUTPUT;
198: }
199: 
200: } // namespace duckdb
[end of src/execution/operator/persistent/physical_delete.cpp]
[start of src/execution/operator/persistent/physical_insert.cpp]
1: #include "duckdb/execution/operator/persistent/physical_insert.hpp"
2: #include "duckdb/parallel/thread_context.hpp"
3: #include "duckdb/catalog/catalog_entry/duck_table_entry.hpp"
4: #include "duckdb/common/types/column/column_data_collection.hpp"
5: #include "duckdb/common/vector_operations/vector_operations.hpp"
6: #include "duckdb/execution/expression_executor.hpp"
7: #include "duckdb/storage/data_table.hpp"
8: #include "duckdb/main/client_context.hpp"
9: #include "duckdb/parser/parsed_data/create_table_info.hpp"
10: #include "duckdb/planner/expression/bound_constant_expression.hpp"
11: #include "duckdb/storage/table_io_manager.hpp"
12: #include "duckdb/transaction/local_storage.hpp"
13: #include "duckdb/parser/statement/insert_statement.hpp"
14: #include "duckdb/parser/statement/update_statement.hpp"
15: #include "duckdb/storage/table/scan_state.hpp"
16: #include "duckdb/common/types/conflict_manager.hpp"
17: #include "duckdb/execution/index/art/art.hpp"
18: #include "duckdb/transaction/duck_transaction.hpp"
19: #include "duckdb/storage/table/append_state.hpp"
20: #include "duckdb/storage/table/update_state.hpp"
21: #include "duckdb/function/create_sort_key.hpp"
22: 
23: namespace duckdb {
24: 
25: PhysicalInsert::PhysicalInsert(
26:     vector<LogicalType> types_p, TableCatalogEntry &table, physical_index_vector_t<idx_t> column_index_map,
27:     vector<unique_ptr<Expression>> bound_defaults, vector<unique_ptr<BoundConstraint>> bound_constraints_p,
28:     vector<unique_ptr<Expression>> set_expressions, vector<PhysicalIndex> set_columns, vector<LogicalType> set_types,
29:     idx_t estimated_cardinality, bool return_chunk, bool parallel, OnConflictAction action_type,
30:     unique_ptr<Expression> on_conflict_condition_p, unique_ptr<Expression> do_update_condition_p,
31:     unordered_set<column_t> conflict_target_p, vector<column_t> columns_to_fetch_p, bool update_is_del_and_insert)
32:     : PhysicalOperator(PhysicalOperatorType::INSERT, std::move(types_p), estimated_cardinality),
33:       column_index_map(std::move(column_index_map)), insert_table(&table), insert_types(table.GetTypes()),
34:       bound_defaults(std::move(bound_defaults)), bound_constraints(std::move(bound_constraints_p)),
35:       return_chunk(return_chunk), parallel(parallel), action_type(action_type),
36:       set_expressions(std::move(set_expressions)), set_columns(std::move(set_columns)), set_types(std::move(set_types)),
37:       on_conflict_condition(std::move(on_conflict_condition_p)), do_update_condition(std::move(do_update_condition_p)),
38:       conflict_target(std::move(conflict_target_p)), update_is_del_and_insert(update_is_del_and_insert) {
39: 
40: 	if (action_type == OnConflictAction::THROW) {
41: 		return;
42: 	}
43: 
44: 	D_ASSERT(this->set_expressions.size() == this->set_columns.size());
45: 
46: 	// One or more columns are referenced from the existing table,
47: 	// we use the 'insert_types' to figure out which types these columns have
48: 	types_to_fetch = vector<LogicalType>(columns_to_fetch_p.size(), LogicalType::SQLNULL);
49: 	for (idx_t i = 0; i < columns_to_fetch_p.size(); i++) {
50: 		auto &id = columns_to_fetch_p[i];
51: 		D_ASSERT(id < insert_types.size());
52: 		types_to_fetch[i] = insert_types[id];
53: 		columns_to_fetch.emplace_back(id);
54: 	}
55: }
56: 
57: PhysicalInsert::PhysicalInsert(LogicalOperator &op, SchemaCatalogEntry &schema, unique_ptr<BoundCreateTableInfo> info_p,
58:                                idx_t estimated_cardinality, bool parallel)
59:     : PhysicalOperator(PhysicalOperatorType::CREATE_TABLE_AS, op.types, estimated_cardinality), insert_table(nullptr),
60:       return_chunk(false), schema(&schema), info(std::move(info_p)), parallel(parallel),
61:       action_type(OnConflictAction::THROW), update_is_del_and_insert(false) {
62: 	GetInsertInfo(*info, insert_types, bound_defaults);
63: }
64: 
65: void PhysicalInsert::GetInsertInfo(const BoundCreateTableInfo &info, vector<LogicalType> &insert_types,
66:                                    vector<unique_ptr<Expression>> &bound_defaults) {
67: 	auto &create_info = info.base->Cast<CreateTableInfo>();
68: 	for (auto &col : create_info.columns.Physical()) {
69: 		insert_types.push_back(col.GetType());
70: 		bound_defaults.push_back(make_uniq<BoundConstantExpression>(Value(col.GetType())));
71: 	}
72: }
73: 
74: //===--------------------------------------------------------------------===//
75: // Sink
76: //===--------------------------------------------------------------------===//
77: 
78: InsertGlobalState::InsertGlobalState(ClientContext &context, const vector<LogicalType> &return_types,
79:                                      DuckTableEntry &table)
80:     : table(table), insert_count(0), initialized(false), return_collection(context, return_types) {
81: }
82: 
83: InsertLocalState::InsertLocalState(ClientContext &context, const vector<LogicalType> &types_p,
84:                                    const vector<unique_ptr<Expression>> &bound_defaults,
85:                                    const vector<unique_ptr<BoundConstraint>> &bound_constraints)
86:     : default_executor(context, bound_defaults), bound_constraints(bound_constraints) {
87: 
88: 	auto &allocator = Allocator::Get(context);
89: 
90: 	types = types_p;
91: 	auto initialize = vector<bool>(types.size(), false);
92: 	update_chunk.Initialize(allocator, types, initialize);
93: 	append_chunk.Initialize(allocator, types, initialize);
94: }
95: 
96: ConstraintState &InsertLocalState::GetConstraintState(DataTable &table, TableCatalogEntry &table_ref) {
97: 	if (!constraint_state) {
98: 		constraint_state = table.InitializeConstraintState(table_ref, bound_constraints);
99: 	}
100: 	return *constraint_state;
101: }
102: 
103: TableDeleteState &InsertLocalState::GetDeleteState(DataTable &table, TableCatalogEntry &table_ref,
104:                                                    ClientContext &context) {
105: 	if (!delete_state) {
106: 		delete_state = table.InitializeDelete(table_ref, context, bound_constraints);
107: 	}
108: 	return *delete_state;
109: }
110: 
111: unique_ptr<GlobalSinkState> PhysicalInsert::GetGlobalSinkState(ClientContext &context) const {
112: 	optional_ptr<TableCatalogEntry> table;
113: 	if (info) {
114: 		// CREATE TABLE AS
115: 		D_ASSERT(!insert_table);
116: 		auto &catalog = schema->catalog;
117: 		table = &catalog.CreateTable(catalog.GetCatalogTransaction(context), *schema.get_mutable(), *info)
118: 		             ->Cast<TableCatalogEntry>();
119: 	} else {
120: 		D_ASSERT(insert_table);
121: 		D_ASSERT(insert_table->IsDuckTable());
122: 		table = insert_table.get_mutable();
123: 	}
124: 	auto result = make_uniq<InsertGlobalState>(context, GetTypes(), table->Cast<DuckTableEntry>());
125: 	return std::move(result);
126: }
127: 
128: unique_ptr<LocalSinkState> PhysicalInsert::GetLocalSinkState(ExecutionContext &context) const {
129: 	return make_uniq<InsertLocalState>(context.client, insert_types, bound_defaults, bound_constraints);
130: }
131: 
132: void PhysicalInsert::ResolveDefaults(const TableCatalogEntry &table, DataChunk &chunk,
133:                                      const physical_index_vector_t<idx_t> &column_index_map,
134:                                      ExpressionExecutor &default_executor, DataChunk &result) {
135: 	chunk.Flatten();
136: 	default_executor.SetChunk(chunk);
137: 
138: 	result.Reset();
139: 	result.SetCardinality(chunk);
140: 
141: 	if (!column_index_map.empty()) {
142: 		// columns specified by the user, use column_index_map
143: 		for (auto &col : table.GetColumns().Physical()) {
144: 			auto storage_idx = col.StorageOid();
145: 			auto mapped_index = column_index_map[col.Physical()];
146: 			if (mapped_index == DConstants::INVALID_INDEX) {
147: 				// insert default value
148: 				default_executor.ExecuteExpression(storage_idx, result.data[storage_idx]);
149: 			} else {
150: 				// get value from child chunk
151: 				D_ASSERT((idx_t)mapped_index < chunk.ColumnCount());
152: 				D_ASSERT(result.data[storage_idx].GetType() == chunk.data[mapped_index].GetType());
153: 				result.data[storage_idx].Reference(chunk.data[mapped_index]);
154: 			}
155: 		}
156: 	} else {
157: 		// no columns specified, just append directly
158: 		for (idx_t i = 0; i < result.ColumnCount(); i++) {
159: 			D_ASSERT(result.data[i].GetType() == chunk.data[i].GetType());
160: 			result.data[i].Reference(chunk.data[i]);
161: 		}
162: 	}
163: }
164: 
165: bool AllConflictsMeetCondition(DataChunk &result) {
166: 	result.Flatten();
167: 	auto data = FlatVector::GetData<bool>(result.data[0]);
168: 	for (idx_t i = 0; i < result.size(); i++) {
169: 		if (!data[i]) {
170: 			return false;
171: 		}
172: 	}
173: 	return true;
174: }
175: 
176: void CheckOnConflictCondition(ExecutionContext &context, DataChunk &conflicts, const unique_ptr<Expression> &condition,
177:                               DataChunk &result) {
178: 	ExpressionExecutor executor(context.client, *condition);
179: 	result.Initialize(context.client, {LogicalType::BOOLEAN});
180: 	executor.Execute(conflicts, result);
181: 	result.SetCardinality(conflicts.size());
182: }
183: 
184: static void CombineExistingAndInsertTuples(DataChunk &result, DataChunk &scan_chunk, DataChunk &input_chunk,
185:                                            ClientContext &client, const PhysicalInsert &op) {
186: 	auto &types_to_fetch = op.types_to_fetch;
187: 	auto &insert_types = op.insert_types;
188: 
189: 	if (types_to_fetch.empty()) {
190: 		// We have not scanned the initial table, so we duplicate the initial chunk.
191: 		const auto &types = input_chunk.GetTypes();
192: 		auto initialize = vector<bool>(types.size(), false);
193: 		result.Initialize(client, types, initialize, input_chunk.size());
194: 		result.Reference(input_chunk);
195: 		result.SetCardinality(input_chunk);
196: 		return;
197: 	}
198: 	vector<LogicalType> combined_types;
199: 	combined_types.reserve(insert_types.size() + types_to_fetch.size());
200: 	combined_types.insert(combined_types.end(), insert_types.begin(), insert_types.end());
201: 	combined_types.insert(combined_types.end(), types_to_fetch.begin(), types_to_fetch.end());
202: 
203: 	result.Initialize(client, combined_types, input_chunk.size());
204: 	result.Reset();
205: 	// Add the VALUES list
206: 	for (idx_t i = 0; i < insert_types.size(); i++) {
207: 		idx_t col_idx = i;
208: 		auto &other_col = input_chunk.data[i];
209: 		auto &this_col = result.data[col_idx];
210: 		D_ASSERT(other_col.GetType() == this_col.GetType());
211: 		this_col.Reference(other_col);
212: 	}
213: 	// Add the columns from the original conflicting tuples
214: 	for (idx_t i = 0; i < types_to_fetch.size(); i++) {
215: 		idx_t col_idx = i + insert_types.size();
216: 		auto &other_col = scan_chunk.data[i];
217: 		auto &this_col = result.data[col_idx];
218: 		D_ASSERT(other_col.GetType() == this_col.GetType());
219: 		this_col.Reference(other_col);
220: 	}
221: 	// This is guaranteed by the requirement of a conflict target to have a condition or set expressions
222: 	// Only when we have any sort of condition or SET expression that references the existing table is this possible
223: 	// to not be true.
224: 	// We can have a SET expression without a conflict target ONLY if there is only 1 Index on the table
225: 	// In which case this also can't cause a discrepancy between existing tuple count and insert tuple count
226: 	D_ASSERT(input_chunk.size() == scan_chunk.size());
227: 	result.SetCardinality(input_chunk.size());
228: }
229: 
230: static void CreateUpdateChunk(ExecutionContext &context, DataChunk &chunk, Vector &row_ids, DataChunk &update_chunk,
231:                               const PhysicalInsert &op) {
232: 
233: 	auto &do_update_condition = op.do_update_condition;
234: 	auto &set_types = op.set_types;
235: 	auto &set_expressions = op.set_expressions;
236: 
237: 	// Check the optional condition for the DO UPDATE clause, to filter which rows will be updated
238: 	if (do_update_condition) {
239: 		DataChunk do_update_filter_result;
240: 		do_update_filter_result.Initialize(context.client, {LogicalType::BOOLEAN});
241: 		ExpressionExecutor where_executor(context.client, *do_update_condition);
242: 		where_executor.Execute(chunk, do_update_filter_result);
243: 		do_update_filter_result.SetCardinality(chunk.size());
244: 		do_update_filter_result.Flatten();
245: 
246: 		ManagedSelection selection(chunk.size());
247: 
248: 		auto where_data = FlatVector::GetData<bool>(do_update_filter_result.data[0]);
249: 		for (idx_t i = 0; i < chunk.size(); i++) {
250: 			if (where_data[i]) {
251: 				selection.Append(i);
252: 			}
253: 		}
254: 		if (selection.Count() != selection.Size()) {
255: 			// Not all conflicts met the condition, need to filter out the ones that don't
256: 			chunk.Slice(selection.Selection(), selection.Count());
257: 			chunk.SetCardinality(selection.Count());
258: 			// Also apply this Slice to the to-update row_ids
259: 			row_ids.Slice(selection.Selection(), selection.Count());
260: 		}
261: 	}
262: 
263: 	if (chunk.size() == 0) {
264: 		auto initialize = vector<bool>(set_types.size(), false);
265: 		update_chunk.Initialize(context.client, set_types, initialize, chunk.size());
266: 		update_chunk.SetCardinality(chunk);
267: 		return;
268: 	}
269: 
270: 	// Execute the SET expressions.
271: 	update_chunk.Initialize(context.client, set_types, chunk.size());
272: 	ExpressionExecutor executor(context.client, set_expressions);
273: 	executor.Execute(chunk, update_chunk);
274: 	update_chunk.SetCardinality(chunk);
275: }
276: 
277: template <bool GLOBAL>
278: static idx_t PerformOnConflictAction(InsertLocalState &lstate, ExecutionContext &context, DataChunk &chunk,
279:                                      TableCatalogEntry &table, Vector &row_ids, const PhysicalInsert &op) {
280: 	// Early-out, if we do nothing on conflicting rows.
281: 	if (op.action_type == OnConflictAction::NOTHING) {
282: 		return 0;
283: 	}
284: 
285: 	auto &set_columns = op.set_columns;
286: 	DataChunk update_chunk;
287: 	CreateUpdateChunk(context, chunk, row_ids, update_chunk, op);
288: 	auto &data_table = table.GetStorage();
289: 
290: 	// Perform the UPDATE on the (global) storage.
291: 	if (!op.update_is_del_and_insert) {
292: 		if (GLOBAL) {
293: 			auto update_state = data_table.InitializeUpdate(table, context.client, op.bound_constraints);
294: 			data_table.Update(*update_state, context.client, row_ids, set_columns, update_chunk);
295: 			return update_chunk.size();
296: 		}
297: 		auto &local_storage = LocalStorage::Get(context.client, data_table.db);
298: 		local_storage.Update(data_table, row_ids, set_columns, update_chunk);
299: 		return update_chunk.size();
300: 	}
301: 
302: 	// Arrange the columns in the standard table order.
303: 	DataChunk &append_chunk = lstate.append_chunk;
304: 	append_chunk.SetCardinality(update_chunk);
305: 	for (idx_t i = 0; i < append_chunk.ColumnCount(); i++) {
306: 		append_chunk.data[i].Reference(chunk.data[i]);
307: 	}
308: 	for (idx_t i = 0; i < set_columns.size(); i++) {
309: 		append_chunk.data[set_columns[i].index].Reference(update_chunk.data[i]);
310: 	}
311: 
312: 	if (GLOBAL) {
313: 		auto &delete_state = lstate.GetDeleteState(data_table, table, context.client);
314: 		data_table.Delete(delete_state, context.client, row_ids, update_chunk.size());
315: 	} else {
316: 		auto &local_storage = LocalStorage::Get(context.client, data_table.db);
317: 		local_storage.Delete(data_table, row_ids, update_chunk.size());
318: 	}
319: 
320: 	data_table.LocalAppend(table, context.client, append_chunk, op.bound_constraints, row_ids, append_chunk);
321: 	return update_chunk.size();
322: }
323: 
324: // TODO: should we use a hash table to keep track of this instead?
325: static void RegisterUpdatedRows(InsertLocalState &lstate, const Vector &row_ids, idx_t count) {
326: 	// Insert all rows, if any of the rows has already been updated before, we throw an error
327: 	auto data = FlatVector::GetData<row_t>(row_ids);
328: 
329: 	auto &updated_rows = lstate.updated_rows;
330: 	for (idx_t i = 0; i < count; i++) {
331: 		auto result = updated_rows.insert(data[i]);
332: 		if (result.second == false) {
333: 			// This is following postgres behavior:
334: 			throw InvalidInputException(
335: 			    "ON CONFLICT DO UPDATE can not update the same row twice in the same command. Ensure that no rows "
336: 			    "proposed for insertion within the same command have duplicate constrained values");
337: 		}
338: 	}
339: }
340: 
341: static void CheckDistinctnessInternal(ValidityMask &valid, vector<reference<Vector>> &sort_keys, idx_t count,
342:                                       map<idx_t, vector<idx_t>> &result) {
343: 	for (idx_t i = 0; i < count; i++) {
344: 		bool has_conflicts = false;
345: 		for (idx_t j = i + 1; j < count; j++) {
346: 			if (!valid.RowIsValid(j)) {
347: 				// Already a conflict
348: 				continue;
349: 			}
350: 			bool matches = true;
351: 			for (auto &sort_key : sort_keys) {
352: 				auto &this_row = FlatVector::GetData<string_t>(sort_key.get())[i];
353: 				auto &other_row = FlatVector::GetData<string_t>(sort_key.get())[j];
354: 				if (this_row != other_row) {
355: 					matches = false;
356: 					break;
357: 				}
358: 			}
359: 			if (matches) {
360: 				auto &row_ids = result[i];
361: 				has_conflicts = true;
362: 				row_ids.push_back(j);
363: 				valid.SetInvalid(j);
364: 			}
365: 		}
366: 		if (has_conflicts) {
367: 			valid.SetInvalid(i);
368: 		}
369: 	}
370: }
371: 
372: void PrepareSortKeys(DataChunk &input, unordered_map<column_t, unique_ptr<Vector>> &sort_keys,
373:                      const unordered_set<column_t> &column_ids) {
374: 	OrderModifiers order_modifiers(OrderType::ASCENDING, OrderByNullType::NULLS_LAST);
375: 	for (auto &it : column_ids) {
376: 		auto &sort_key = sort_keys[it];
377: 		if (sort_key != nullptr) {
378: 			continue;
379: 		}
380: 		auto &column = input.data[it];
381: 		sort_key = make_uniq<Vector>(LogicalType::BLOB);
382: 		CreateSortKeyHelpers::CreateSortKey(column, input.size(), order_modifiers, *sort_key);
383: 	}
384: }
385: 
386: static map<idx_t, vector<idx_t>> CheckDistinctness(DataChunk &input, ConflictInfo &info,
387:                                                    unordered_set<BoundIndex *> &matched_indexes) {
388: 	map<idx_t, vector<idx_t>> conflicts;
389: 	unordered_map<idx_t, unique_ptr<Vector>> sort_keys;
390: 	//! Register which rows have already caused a conflict
391: 	ValidityMask valid(input.size());
392: 
393: 	auto &column_ids = info.column_ids;
394: 	if (column_ids.empty()) {
395: 		for (auto index : matched_indexes) {
396: 			auto &index_column_ids = index->GetColumnIdSet();
397: 			PrepareSortKeys(input, sort_keys, index_column_ids);
398: 			vector<reference<Vector>> columns;
399: 			for (auto &idx : index_column_ids) {
400: 				columns.push_back(*sort_keys[idx]);
401: 			}
402: 			CheckDistinctnessInternal(valid, columns, input.size(), conflicts);
403: 		}
404: 	} else {
405: 		PrepareSortKeys(input, sort_keys, column_ids);
406: 		vector<reference<Vector>> columns;
407: 		for (auto &idx : column_ids) {
408: 			columns.push_back(*sort_keys[idx]);
409: 		}
410: 		CheckDistinctnessInternal(valid, columns, input.size(), conflicts);
411: 	}
412: 	return conflicts;
413: }
414: 
415: template <bool GLOBAL>
416: static void VerifyOnConflictCondition(ExecutionContext &context, DataChunk &combined_chunk,
417:                                       const unique_ptr<Expression> &on_conflict_condition,
418:                                       ConstraintState &constraint_state, DataChunk &tuples, DataTable &data_table,
419:                                       LocalStorage &local_storage) {
420: 	if (!on_conflict_condition) {
421: 		return;
422: 	}
423: 	DataChunk conflict_condition_result;
424: 	CheckOnConflictCondition(context, combined_chunk, on_conflict_condition, conflict_condition_result);
425: 	bool conditions_met = AllConflictsMeetCondition(conflict_condition_result);
426: 	if (conditions_met) {
427: 		return;
428: 	}
429: 
430: 	// We need to throw. Filter all tuples that passed, and verify again with those that violate the constraint.
431: 	ManagedSelection sel(combined_chunk.size());
432: 	auto data = FlatVector::GetData<bool>(conflict_condition_result.data[0]);
433: 	for (idx_t i = 0; i < combined_chunk.size(); i++) {
434: 		if (!data[i]) {
435: 			// This tuple did not meet the condition.
436: 			sel.Append(i);
437: 		}
438: 	}
439: 	combined_chunk.Slice(sel.Selection(), sel.Count());
440: 
441: 	// Verify and throw.
442: 	if (GLOBAL) {
443: 		data_table.VerifyAppendConstraints(constraint_state, context.client, combined_chunk, nullptr, nullptr);
444: 		throw InternalException("VerifyAppendConstraints was expected to throw but didn't");
445: 	}
446: 
447: 	auto &indexes = local_storage.GetIndexes(data_table);
448: 	auto storage = local_storage.GetStorage(data_table);
449: 	DataTable::VerifyUniqueIndexes(indexes, storage, tuples, nullptr);
450: 	throw InternalException("VerifyUniqueIndexes was expected to throw but didn't");
451: }
452: 
453: template <bool GLOBAL>
454: static idx_t HandleInsertConflicts(TableCatalogEntry &table, ExecutionContext &context, InsertLocalState &lstate,
455:                                    DataChunk &tuples, const PhysicalInsert &op) {
456: 	auto &types_to_fetch = op.types_to_fetch;
457: 	auto &on_conflict_condition = op.on_conflict_condition;
458: 	auto &conflict_target = op.conflict_target;
459: 	auto &columns_to_fetch = op.columns_to_fetch;
460: 	auto &data_table = table.GetStorage();
461: 
462: 	auto &local_storage = LocalStorage::Get(context.client, data_table.db);
463: 
464: 	ConflictInfo conflict_info(conflict_target);
465: 	ConflictManager conflict_manager(VerifyExistenceType::APPEND, tuples.size(), &conflict_info);
466: 	if (GLOBAL) {
467: 		auto &constraint_state = lstate.GetConstraintState(data_table, table);
468: 		auto storage = local_storage.GetStorage(data_table);
469: 		data_table.VerifyAppendConstraints(constraint_state, context.client, tuples, storage, &conflict_manager);
470: 	} else {
471: 		auto &indexes = local_storage.GetIndexes(data_table);
472: 		auto storage = local_storage.GetStorage(data_table);
473: 		DataTable::VerifyUniqueIndexes(indexes, storage, tuples, &conflict_manager);
474: 	}
475: 
476: 	conflict_manager.Finalize();
477: 	if (conflict_manager.ConflictCount() == 0) {
478: 		// No conflicts found, 0 updates performed
479: 		return 0;
480: 	}
481: 	idx_t affected_tuples = 0;
482: 
483: 	auto &conflicts = conflict_manager.Conflicts();
484: 	auto &row_ids = conflict_manager.RowIds();
485: 
486: 	DataChunk conflict_chunk; // contains only the conflicting values
487: 	DataChunk scan_chunk;     // contains the original values, that caused the conflict
488: 	DataChunk combined_chunk; // contains conflict_chunk + scan_chunk (wide)
489: 
490: 	// Filter out everything but the conflicting rows
491: 	const auto &types = tuples.GetTypes();
492: 	auto initialize = vector<bool>(types.size(), false);
493: 	conflict_chunk.Initialize(context.client, types, initialize, tuples.size());
494: 	conflict_chunk.Reference(tuples);
495: 	conflict_chunk.Slice(conflicts.Selection(), conflicts.Count());
496: 	conflict_chunk.SetCardinality(conflicts.Count());
497: 
498: 	// Holds the pins for the fetched rows
499: 	unique_ptr<ColumnFetchState> fetch_state;
500: 	if (!types_to_fetch.empty()) {
501: 		D_ASSERT(scan_chunk.size() == 0);
502: 		// When these values are required for the conditions or the SET expressions,
503: 		// then we scan the existing table for the conflicting tuples, using the rowids
504: 		scan_chunk.Initialize(context.client, types_to_fetch, conflicts.Count());
505: 		fetch_state = make_uniq<ColumnFetchState>();
506: 		if (GLOBAL) {
507: 			auto &transaction = DuckTransaction::Get(context.client, table.catalog);
508: 			data_table.Fetch(transaction, scan_chunk, columns_to_fetch, row_ids, conflicts.Count(), *fetch_state);
509: 		} else {
510: 			local_storage.FetchChunk(data_table, row_ids, conflicts.Count(), columns_to_fetch, scan_chunk,
511: 			                         *fetch_state);
512: 		}
513: 	}
514: 
515: 	// Splice the Input chunk and the fetched chunk together
516: 	CombineExistingAndInsertTuples(combined_chunk, scan_chunk, conflict_chunk, context.client, op);
517: 
518: 	auto &constraint_state = lstate.GetConstraintState(data_table, table);
519: 	VerifyOnConflictCondition<GLOBAL>(context, combined_chunk, on_conflict_condition, constraint_state, tuples,
520: 	                                  data_table, local_storage);
521: 
522: 	if (&tuples == &lstate.update_chunk) {
523: 		// Allow updating duplicate rows for the 'update_chunk'
524: 		RegisterUpdatedRows(lstate, row_ids, combined_chunk.size());
525: 	}
526: 
527: 	affected_tuples += PerformOnConflictAction<GLOBAL>(lstate, context, combined_chunk, table, row_ids, op);
528: 
529: 	// Remove the conflicting tuples from the insert chunk
530: 	SelectionVector sel_vec(tuples.size());
531: 	idx_t new_size = SelectionVector::Inverted(conflicts.Selection(), sel_vec, conflicts.Count(), tuples.size());
532: 	tuples.Slice(sel_vec, new_size);
533: 	tuples.SetCardinality(new_size);
534: 	return affected_tuples;
535: }
536: 
537: idx_t PhysicalInsert::OnConflictHandling(TableCatalogEntry &table, ExecutionContext &context,
538:                                          InsertLocalState &lstate) const {
539: 	auto &data_table = table.GetStorage();
540: 	auto &local_storage = LocalStorage::Get(context.client, data_table.db);
541: 
542: 	if (action_type == OnConflictAction::THROW) {
543: 		auto &constraint_state = lstate.GetConstraintState(data_table, table);
544: 		auto storage = local_storage.GetStorage(data_table);
545: 		data_table.VerifyAppendConstraints(constraint_state, context.client, lstate.insert_chunk, storage, nullptr);
546: 		return 0;
547: 	}
548: 
549: 	ConflictInfo conflict_info(conflict_target);
550: 
551: 	auto &global_indexes = data_table.GetDataTableInfo()->GetIndexes();
552: 	auto &local_indexes = local_storage.GetIndexes(data_table);
553: 
554: 	unordered_set<BoundIndex *> matched_indexes;
555: 	if (conflict_info.column_ids.empty()) {
556: 		// We care about every index that applies to the table if no ON CONFLICT (...) target is given
557: 		global_indexes.Scan([&](Index &index) {
558: 			if (!index.IsUnique()) {
559: 				return false;
560: 			}
561: 			if (conflict_info.ConflictTargetMatches(index)) {
562: 				D_ASSERT(index.IsBound());
563: 				auto &bound_index = index.Cast<BoundIndex>();
564: 				matched_indexes.insert(&bound_index);
565: 			}
566: 			return false;
567: 		});
568: 		local_indexes.Scan([&](Index &index) {
569: 			if (!index.IsUnique()) {
570: 				return false;
571: 			}
572: 			if (conflict_info.ConflictTargetMatches(index)) {
573: 				D_ASSERT(index.IsBound());
574: 				auto &bound_index = index.Cast<BoundIndex>();
575: 				matched_indexes.insert(&bound_index);
576: 			}
577: 			return false;
578: 		});
579: 	}
580: 
581: 	auto inner_conflicts = CheckDistinctness(lstate.insert_chunk, conflict_info, matched_indexes);
582: 	idx_t count = lstate.insert_chunk.size();
583: 	if (!inner_conflicts.empty()) {
584: 		// We have at least one inner conflict, filter it out
585: 		ManagedSelection sel_vec(count);
586: 		ValidityMask not_a_conflict(count);
587: 		set<idx_t> last_occurrences_of_conflict;
588: 		for (idx_t i = 0; i < count; i++) {
589: 			auto it = inner_conflicts.find(i);
590: 			if (it != inner_conflicts.end()) {
591: 				auto &conflicts = it->second;
592: 				auto conflict_it = conflicts.begin();
593: 				for (; conflict_it != conflicts.end();) {
594: 					auto &idx = *conflict_it;
595: 					not_a_conflict.SetInvalid(idx);
596: 					conflict_it++;
597: 					if (conflict_it == conflicts.end()) {
598: 						last_occurrences_of_conflict.insert(idx);
599: 					}
600: 				}
601: 			}
602: 			if (not_a_conflict.RowIsValid(i)) {
603: 				sel_vec.Append(i);
604: 			}
605: 		}
606: 		if (action_type == OnConflictAction::UPDATE) {
607: 			ManagedSelection last_occurrences(last_occurrences_of_conflict.size());
608: 			for (auto &idx : last_occurrences_of_conflict) {
609: 				last_occurrences.Append(idx);
610: 			}
611: 
612: 			lstate.update_chunk.Reference(lstate.insert_chunk);
613: 			lstate.update_chunk.Slice(last_occurrences.Selection(), last_occurrences.Count());
614: 			lstate.update_chunk.SetCardinality(last_occurrences.Count());
615: 		}
616: 
617: 		lstate.insert_chunk.Slice(sel_vec.Selection(), sel_vec.Count());
618: 		lstate.insert_chunk.SetCardinality(sel_vec.Count());
619: 	}
620: 
621: 	// Check whether any conflicts arise, and if they all meet the conflict_target + condition
622: 	// If that's not the case - We throw the first error
623: 	idx_t updated_tuples = 0;
624: 	updated_tuples += HandleInsertConflicts<true>(table, context, lstate, lstate.insert_chunk, *this);
625: 	// Also check the transaction-local storage+ART so we can detect conflicts within this transaction
626: 	updated_tuples += HandleInsertConflicts<false>(table, context, lstate, lstate.insert_chunk, *this);
627: 
628: 	return updated_tuples;
629: }
630: 
631: SinkResultType PhysicalInsert::Sink(ExecutionContext &context, DataChunk &chunk, OperatorSinkInput &input) const {
632: 	auto &gstate = input.global_state.Cast<InsertGlobalState>();
633: 	auto &lstate = input.local_state.Cast<InsertLocalState>();
634: 
635: 	auto &table = gstate.table;
636: 	auto &storage = table.GetStorage();
637: 	if (lstate.init_insert_chunk) {
638: 		auto initialize = vector<bool>(lstate.types.size(), false);
639: 		if (!column_index_map.empty()) {
640: 			for (auto &col : table.GetColumns().Physical()) {
641: 				auto storage_idx = col.StorageOid();
642: 				auto mapped_index = column_index_map[col.Physical()];
643: 				if (mapped_index == DConstants::INVALID_INDEX) {
644: 					initialize[storage_idx] = true;
645: 				}
646: 			}
647: 		}
648: 		auto &allocator = Allocator::Get(context.client);
649: 		lstate.insert_chunk.Initialize(allocator, lstate.types, initialize, chunk.size());
650: 		lstate.init_insert_chunk = false;
651: 	}
652: 	PhysicalInsert::ResolveDefaults(table, chunk, column_index_map, lstate.default_executor, lstate.insert_chunk);
653: 
654: 	if (!parallel) {
655: 		if (!gstate.initialized) {
656: 			storage.InitializeLocalAppend(gstate.append_state, table, context.client, bound_constraints);
657: 			gstate.initialized = true;
658: 		}
659: 
660: 		if (action_type != OnConflictAction::NOTHING && return_chunk) {
661: 			// If the action is UPDATE or REPLACE, we will always create either an APPEND or an INSERT
662: 			// for NOTHING we don't create either an APPEND or an INSERT for the tuple
663: 			// so it should not be added to the RETURNING chunk
664: 			gstate.return_collection.Append(lstate.insert_chunk);
665: 		}
666: 		idx_t updated_tuples = OnConflictHandling(table, context, lstate);
667: 		if (action_type == OnConflictAction::NOTHING && return_chunk) {
668: 			// Because we didn't add to the RETURNING chunk yet
669: 			// we add the tuples that did not get filtered out now
670: 			gstate.return_collection.Append(lstate.insert_chunk);
671: 		}
672: 		gstate.insert_count += lstate.insert_chunk.size();
673: 		gstate.insert_count += updated_tuples;
674: 		storage.LocalAppend(gstate.append_state, context.client, lstate.insert_chunk, true);
675: 		if (action_type == OnConflictAction::UPDATE && lstate.update_chunk.size() != 0) {
676: 			// Flush the append so we can target the data we just appended with the update
677: 			storage.FinalizeLocalAppend(gstate.append_state);
678: 			gstate.initialized = false;
679: 			(void)HandleInsertConflicts<true>(table, context, lstate, lstate.update_chunk, *this);
680: 			(void)HandleInsertConflicts<false>(table, context, lstate, lstate.update_chunk, *this);
681: 			// All of the tuples should have been turned into an update, leaving the chunk empty afterwards
682: 			D_ASSERT(lstate.update_chunk.size() == 0);
683: 		}
684: 	} else {
685: 		D_ASSERT(!return_chunk);
686: 		// parallel append
687: 		if (!lstate.local_collection) {
688: 			lock_guard<mutex> l(gstate.lock);
689: 			auto table_info = storage.GetDataTableInfo();
690: 			auto &io_manager = TableIOManager::Get(table.GetStorage());
691: 			lstate.local_collection = make_uniq<RowGroupCollection>(std::move(table_info), io_manager, insert_types,
692: 			                                                        NumericCast<idx_t>(MAX_ROW_ID));
693: 			lstate.local_collection->InitializeEmpty();
694: 			lstate.local_collection->InitializeAppend(lstate.local_append_state);
695: 			lstate.writer = &gstate.table.GetStorage().CreateOptimisticWriter(context.client);
696: 		}
697: 		OnConflictHandling(table, context, lstate);
698: 		D_ASSERT(action_type != OnConflictAction::UPDATE);
699: 
700: 		auto new_row_group = lstate.local_collection->Append(lstate.insert_chunk, lstate.local_append_state);
701: 		if (new_row_group) {
702: 			lstate.writer->WriteNewRowGroup(*lstate.local_collection);
703: 		}
704: 	}
705: 
706: 	return SinkResultType::NEED_MORE_INPUT;
707: }
708: 
709: SinkCombineResultType PhysicalInsert::Combine(ExecutionContext &context, OperatorSinkCombineInput &input) const {
710: 	auto &gstate = input.global_state.Cast<InsertGlobalState>();
711: 	auto &lstate = input.local_state.Cast<InsertLocalState>();
712: 	auto &client_profiler = QueryProfiler::Get(context.client);
713: 	context.thread.profiler.Flush(*this);
714: 	client_profiler.Flush(context.thread.profiler);
715: 
716: 	if (!parallel || !lstate.local_collection) {
717: 		return SinkCombineResultType::FINISHED;
718: 	}
719: 
720: 	auto &table = gstate.table;
721: 	auto &storage = table.GetStorage();
722: 	const idx_t row_group_size = storage.GetRowGroupSize();
723: 
724: 	// parallel append: finalize the append
725: 	TransactionData tdata(0, 0);
726: 	lstate.local_collection->FinalizeAppend(tdata, lstate.local_append_state);
727: 
728: 	auto append_count = lstate.local_collection->GetTotalRows();
729: 
730: 	lock_guard<mutex> lock(gstate.lock);
731: 	gstate.insert_count += append_count;
732: 	if (append_count < row_group_size) {
733: 		// we have few rows - append to the local storage directly
734: 		storage.InitializeLocalAppend(gstate.append_state, table, context.client, bound_constraints);
735: 		auto &transaction = DuckTransaction::Get(context.client, table.catalog);
736: 		lstate.local_collection->Scan(transaction, [&](DataChunk &insert_chunk) {
737: 			storage.LocalAppend(gstate.append_state, context.client, insert_chunk, false);
738: 			return true;
739: 		});
740: 		storage.FinalizeLocalAppend(gstate.append_state);
741: 	} else {
742: 		// we have written rows to disk optimistically - merge directly into the transaction-local storage
743: 		lstate.writer->WriteLastRowGroup(*lstate.local_collection);
744: 		lstate.writer->FinalFlush();
745: 		gstate.table.GetStorage().LocalMerge(context.client, *lstate.local_collection);
746: 		gstate.table.GetStorage().FinalizeOptimisticWriter(context.client, *lstate.writer);
747: 	}
748: 
749: 	return SinkCombineResultType::FINISHED;
750: }
751: 
752: SinkFinalizeType PhysicalInsert::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
753:                                           OperatorSinkFinalizeInput &input) const {
754: 	auto &gstate = input.global_state.Cast<InsertGlobalState>();
755: 	if (!parallel && gstate.initialized) {
756: 		auto &table = gstate.table;
757: 		auto &storage = table.GetStorage();
758: 		storage.FinalizeLocalAppend(gstate.append_state);
759: 	}
760: 	return SinkFinalizeType::READY;
761: }
762: 
763: //===--------------------------------------------------------------------===//
764: // Source
765: //===--------------------------------------------------------------------===//
766: class InsertSourceState : public GlobalSourceState {
767: public:
768: 	explicit InsertSourceState(const PhysicalInsert &op) {
769: 		if (op.return_chunk) {
770: 			D_ASSERT(op.sink_state);
771: 			auto &g = op.sink_state->Cast<InsertGlobalState>();
772: 			g.return_collection.InitializeScan(scan_state);
773: 		}
774: 	}
775: 
776: 	ColumnDataScanState scan_state;
777: };
778: 
779: unique_ptr<GlobalSourceState> PhysicalInsert::GetGlobalSourceState(ClientContext &context) const {
780: 	return make_uniq<InsertSourceState>(*this);
781: }
782: 
783: SourceResultType PhysicalInsert::GetData(ExecutionContext &context, DataChunk &chunk,
784:                                          OperatorSourceInput &input) const {
785: 	auto &state = input.global_state.Cast<InsertSourceState>();
786: 	auto &insert_gstate = sink_state->Cast<InsertGlobalState>();
787: 	if (!return_chunk) {
788: 		chunk.SetCardinality(1);
789: 		chunk.SetValue(0, 0, Value::BIGINT(NumericCast<int64_t>(insert_gstate.insert_count)));
790: 		return SourceResultType::FINISHED;
791: 	}
792: 
793: 	insert_gstate.return_collection.Scan(state.scan_state, chunk);
794: 	return chunk.size() == 0 ? SourceResultType::FINISHED : SourceResultType::HAVE_MORE_OUTPUT;
795: }
796: 
797: } // namespace duckdb
[end of src/execution/operator/persistent/physical_insert.cpp]
[start of src/include/duckdb/catalog/catalog.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/catalog/catalog.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/catalog/catalog_entry.hpp"
12: #include "duckdb/catalog/catalog_transaction.hpp"
13: #include "duckdb/common/atomic.hpp"
14: #include "duckdb/common/enums/catalog_lookup_behavior.hpp"
15: #include "duckdb/common/enums/on_entry_not_found.hpp"
16: #include "duckdb/common/error_data.hpp"
17: #include "duckdb/common/exception/catalog_exception.hpp"
18: #include "duckdb/common/map.hpp"
19: #include "duckdb/common/mutex.hpp"
20: #include "duckdb/common/optional_ptr.hpp"
21: #include "duckdb/common/reference_map.hpp"
22: #include "duckdb/parser/query_error_context.hpp"
23: 
24: #include <functional>
25: 
26: namespace duckdb {
27: struct CreateSchemaInfo;
28: struct DropInfo;
29: struct BoundCreateTableInfo;
30: struct AlterTableInfo;
31: struct CreateTableFunctionInfo;
32: struct CreateCopyFunctionInfo;
33: struct CreatePragmaFunctionInfo;
34: struct CreateFunctionInfo;
35: struct CreateViewInfo;
36: struct CreateSequenceInfo;
37: struct CreateCollationInfo;
38: struct CreateIndexInfo;
39: struct CreateTypeInfo;
40: struct CreateTableInfo;
41: struct DatabaseSize;
42: struct MetadataBlockInfo;
43: 
44: class AttachedDatabase;
45: class ClientContext;
46: class Transaction;
47: 
48: class AggregateFunctionCatalogEntry;
49: class CollateCatalogEntry;
50: class SchemaCatalogEntry;
51: class TableCatalogEntry;
52: class ViewCatalogEntry;
53: class SequenceCatalogEntry;
54: class TableFunctionCatalogEntry;
55: class CopyFunctionCatalogEntry;
56: class PragmaFunctionCatalogEntry;
57: class CatalogSet;
58: class DatabaseInstance;
59: class DependencyManager;
60: 
61: struct CatalogLookup;
62: struct CatalogEntryLookup;
63: struct SimilarCatalogEntry;
64: 
65: class Binder;
66: class LogicalOperator;
67: class PhysicalOperator;
68: class LogicalCreateIndex;
69: class LogicalCreateTable;
70: class LogicalInsert;
71: class LogicalDelete;
72: class LogicalUpdate;
73: class CreateStatement;
74: class CatalogEntryRetriever;
75: 
76: //! Return value of Catalog::LookupEntry
77: struct CatalogEntryLookup {
78: 	optional_ptr<SchemaCatalogEntry> schema;
79: 	optional_ptr<CatalogEntry> entry;
80: 	ErrorData error;
81: 
82: 	DUCKDB_API bool Found() const {
83: 		return entry;
84: 	}
85: };
86: 
87: //! The Catalog object represents the catalog of the database.
88: class Catalog {
89: public:
90: 	explicit Catalog(AttachedDatabase &db);
91: 	virtual ~Catalog();
92: 
93: public:
94: 	//! Get the SystemCatalog from the ClientContext
95: 	DUCKDB_API static Catalog &GetSystemCatalog(ClientContext &context);
96: 	//! Get the SystemCatalog from the DatabaseInstance
97: 	DUCKDB_API static Catalog &GetSystemCatalog(DatabaseInstance &db);
98: 	//! Get the specified Catalog from the ClientContext
99: 	DUCKDB_API static Catalog &GetCatalog(ClientContext &context, const string &catalog_name);
100: 	//! Get the specified Catalog from the ClientContext
101: 	DUCKDB_API static Catalog &GetCatalog(CatalogEntryRetriever &retriever, const string &catalog_name);
102: 	//! Get the specified Catalog from the DatabaseInstance
103: 	DUCKDB_API static Catalog &GetCatalog(DatabaseInstance &db, const string &catalog_name);
104: 	//! Gets the specified Catalog from the database if it exists
105: 	DUCKDB_API static optional_ptr<Catalog> GetCatalogEntry(ClientContext &context, const string &catalog_name);
106: 	//! Gets the specified Catalog from the database if it exists
107: 	DUCKDB_API static optional_ptr<Catalog> GetCatalogEntry(CatalogEntryRetriever &retriever,
108: 	                                                        const string &catalog_name);
109: 	//! Get the specific Catalog from the AttachedDatabase
110: 	DUCKDB_API static Catalog &GetCatalog(AttachedDatabase &db);
111: 
112: 	DUCKDB_API AttachedDatabase &GetAttached();
113: 	DUCKDB_API const AttachedDatabase &GetAttached() const;
114: 	DUCKDB_API DatabaseInstance &GetDatabase();
115: 
116: 	virtual bool IsDuckCatalog() {
117: 		return false;
118: 	}
119: 	virtual void Initialize(bool load_builtin) = 0;
120: 
121: 	bool IsSystemCatalog() const;
122: 	bool IsTemporaryCatalog() const;
123: 
124: 	//! Returns a version number that uniquely characterizes the current catalog snapshot.
125: 	//! If there are transaction-local changes, the version returned is >= TRANSACTION_START, o.w. it is a simple number
126: 	//! starting at 0 that is incremented at each commit that has had catalog changes.
127: 	//! If the catalog does not support versioning, no index is returned.
128: 	DUCKDB_API virtual optional_idx GetCatalogVersion(ClientContext &context) {
129: 		return {}; // don't return anything by default
130: 	}
131: 
132: 	//! Returns the catalog name - based on how the catalog was attached
133: 	DUCKDB_API const string &GetName() const;
134: 	DUCKDB_API idx_t GetOid();
135: 	DUCKDB_API virtual string GetCatalogType() = 0;
136: 
137: 	DUCKDB_API CatalogTransaction GetCatalogTransaction(ClientContext &context);
138: 
139: 	//! Creates a schema in the catalog.
140: 	DUCKDB_API virtual optional_ptr<CatalogEntry> CreateSchema(CatalogTransaction transaction,
141: 	                                                           CreateSchemaInfo &info) = 0;
142: 	DUCKDB_API optional_ptr<CatalogEntry> CreateSchema(ClientContext &context, CreateSchemaInfo &info);
143: 	//! Creates a table in the catalog.
144: 	DUCKDB_API optional_ptr<CatalogEntry> CreateTable(CatalogTransaction transaction, BoundCreateTableInfo &info);
145: 	DUCKDB_API optional_ptr<CatalogEntry> CreateTable(ClientContext &context, BoundCreateTableInfo &info);
146: 	//! Creates a table in the catalog.
147: 	DUCKDB_API optional_ptr<CatalogEntry> CreateTable(ClientContext &context, unique_ptr<CreateTableInfo> info);
148: 	//! Create a table function in the catalog
149: 	DUCKDB_API optional_ptr<CatalogEntry> CreateTableFunction(CatalogTransaction transaction,
150: 	                                                          CreateTableFunctionInfo &info);
151: 	DUCKDB_API optional_ptr<CatalogEntry> CreateTableFunction(ClientContext &context, CreateTableFunctionInfo &info);
152: 	// Kept for backwards compatibility
153: 	DUCKDB_API optional_ptr<CatalogEntry> CreateTableFunction(ClientContext &context,
154: 	                                                          optional_ptr<CreateTableFunctionInfo> info);
155: 	//! Create a copy function in the catalog
156: 	DUCKDB_API optional_ptr<CatalogEntry> CreateCopyFunction(CatalogTransaction transaction,
157: 	                                                         CreateCopyFunctionInfo &info);
158: 	DUCKDB_API optional_ptr<CatalogEntry> CreateCopyFunction(ClientContext &context, CreateCopyFunctionInfo &info);
159: 	//! Create a pragma function in the catalog
160: 	DUCKDB_API optional_ptr<CatalogEntry> CreatePragmaFunction(CatalogTransaction transaction,
161: 	                                                           CreatePragmaFunctionInfo &info);
162: 	DUCKDB_API optional_ptr<CatalogEntry> CreatePragmaFunction(ClientContext &context, CreatePragmaFunctionInfo &info);
163: 	//! Create a scalar or aggregate function in the catalog
164: 	DUCKDB_API optional_ptr<CatalogEntry> CreateFunction(CatalogTransaction transaction, CreateFunctionInfo &info);
165: 	DUCKDB_API optional_ptr<CatalogEntry> CreateFunction(ClientContext &context, CreateFunctionInfo &info);
166: 	//! Creates a table in the catalog.
167: 	DUCKDB_API optional_ptr<CatalogEntry> CreateView(CatalogTransaction transaction, CreateViewInfo &info);
168: 	DUCKDB_API optional_ptr<CatalogEntry> CreateView(ClientContext &context, CreateViewInfo &info);
169: 	//! Creates a sequence in the catalog.
170: 	DUCKDB_API optional_ptr<CatalogEntry> CreateSequence(CatalogTransaction transaction, CreateSequenceInfo &info);
171: 	DUCKDB_API optional_ptr<CatalogEntry> CreateSequence(ClientContext &context, CreateSequenceInfo &info);
172: 	//! Creates a Enum in the catalog.
173: 	DUCKDB_API optional_ptr<CatalogEntry> CreateType(CatalogTransaction transaction, CreateTypeInfo &info);
174: 	DUCKDB_API optional_ptr<CatalogEntry> CreateType(ClientContext &context, CreateTypeInfo &info);
175: 	//! Creates a collation in the catalog
176: 	DUCKDB_API optional_ptr<CatalogEntry> CreateCollation(CatalogTransaction transaction, CreateCollationInfo &info);
177: 	DUCKDB_API optional_ptr<CatalogEntry> CreateCollation(ClientContext &context, CreateCollationInfo &info);
178: 	//! Creates an index in the catalog
179: 	DUCKDB_API optional_ptr<CatalogEntry> CreateIndex(CatalogTransaction transaction, CreateIndexInfo &info);
180: 	DUCKDB_API optional_ptr<CatalogEntry> CreateIndex(ClientContext &context, CreateIndexInfo &info);
181: 
182: 	//! Creates a table in the catalog.
183: 	DUCKDB_API optional_ptr<CatalogEntry> CreateTable(CatalogTransaction transaction, SchemaCatalogEntry &schema,
184: 	                                                  BoundCreateTableInfo &info);
185: 	//! Create a table function in the catalog
186: 	DUCKDB_API optional_ptr<CatalogEntry>
187: 	CreateTableFunction(CatalogTransaction transaction, SchemaCatalogEntry &schema, CreateTableFunctionInfo &info);
188: 	//! Create a copy function in the catalog
189: 	DUCKDB_API optional_ptr<CatalogEntry> CreateCopyFunction(CatalogTransaction transaction, SchemaCatalogEntry &schema,
190: 	                                                         CreateCopyFunctionInfo &info);
191: 	//! Create a pragma function in the catalog
192: 	DUCKDB_API optional_ptr<CatalogEntry>
193: 	CreatePragmaFunction(CatalogTransaction transaction, SchemaCatalogEntry &schema, CreatePragmaFunctionInfo &info);
194: 	//! Create a scalar or aggregate function in the catalog
195: 	DUCKDB_API optional_ptr<CatalogEntry> CreateFunction(CatalogTransaction transaction, SchemaCatalogEntry &schema,
196: 	                                                     CreateFunctionInfo &info);
197: 	//! Creates a view in the catalog
198: 	DUCKDB_API optional_ptr<CatalogEntry> CreateView(CatalogTransaction transaction, SchemaCatalogEntry &schema,
199: 	                                                 CreateViewInfo &info);
200: 	//! Creates a table in the catalog.
201: 	DUCKDB_API optional_ptr<CatalogEntry> CreateSequence(CatalogTransaction transaction, SchemaCatalogEntry &schema,
202: 	                                                     CreateSequenceInfo &info);
203: 	//! Creates a enum in the catalog.
204: 	DUCKDB_API optional_ptr<CatalogEntry> CreateType(CatalogTransaction transaction, SchemaCatalogEntry &schema,
205: 	                                                 CreateTypeInfo &info);
206: 	//! Creates a collation in the catalog
207: 	DUCKDB_API optional_ptr<CatalogEntry> CreateCollation(CatalogTransaction transaction, SchemaCatalogEntry &schema,
208: 	                                                      CreateCollationInfo &info);
209: 
210: 	//! Drops an entry from the catalog
211: 	DUCKDB_API void DropEntry(ClientContext &context, DropInfo &info);
212: 
213: 	//! Returns the schema object with the specified name, or throws an exception if it does not exist
214: 	DUCKDB_API SchemaCatalogEntry &GetSchema(ClientContext &context, const string &name,
215: 	                                         QueryErrorContext error_context = QueryErrorContext());
216: 	DUCKDB_API optional_ptr<SchemaCatalogEntry> GetSchema(ClientContext &context, const string &name,
217: 	                                                      OnEntryNotFound if_not_found,
218: 	                                                      QueryErrorContext error_context = QueryErrorContext());
219: 	//! Overloadable method for giving warnings on ambiguous naming id.tab due to a database and schema with name id
220: 	DUCKDB_API virtual bool CheckAmbiguousCatalogOrSchema(ClientContext &context, const string &name) {
221: 		return !!GetSchema(context, name, OnEntryNotFound::RETURN_NULL);
222: 	}
223: 	DUCKDB_API SchemaCatalogEntry &GetSchema(CatalogTransaction transaction, const string &name,
224: 	                                         QueryErrorContext error_context = QueryErrorContext());
225: 	DUCKDB_API virtual optional_ptr<SchemaCatalogEntry>
226: 	GetSchema(CatalogTransaction transaction, const string &schema_name, OnEntryNotFound if_not_found,
227: 	          QueryErrorContext error_context = QueryErrorContext()) = 0;
228: 	DUCKDB_API static SchemaCatalogEntry &GetSchema(ClientContext &context, const string &catalog_name,
229: 	                                                const string &schema_name,
230: 	                                                QueryErrorContext error_context = QueryErrorContext());
231: 	DUCKDB_API static optional_ptr<SchemaCatalogEntry> GetSchema(ClientContext &context, const string &catalog_name,
232: 	                                                             const string &schema_name,
233: 	                                                             OnEntryNotFound if_not_found,
234: 	                                                             QueryErrorContext error_context = QueryErrorContext());
235: 	DUCKDB_API static optional_ptr<SchemaCatalogEntry> GetSchema(CatalogEntryRetriever &retriever,
236: 	                                                             const string &catalog_name, const string &schema_name,
237: 	                                                             OnEntryNotFound if_not_found,
238: 	                                                             QueryErrorContext error_context = QueryErrorContext());
239: 	//! Scans all the schemas in the system one-by-one, invoking the callback for each entry
240: 	DUCKDB_API virtual void ScanSchemas(ClientContext &context, std::function<void(SchemaCatalogEntry &)> callback) = 0;
241: 
242: 	//! Gets the "schema.name" entry of the specified type, if entry does not exist behavior depends on OnEntryNotFound
243: 	DUCKDB_API optional_ptr<CatalogEntry> GetEntry(ClientContext &context, CatalogType type, const string &schema,
244: 	                                               const string &name, OnEntryNotFound if_not_found,
245: 	                                               QueryErrorContext error_context = QueryErrorContext());
246: 	DUCKDB_API optional_ptr<CatalogEntry> GetEntry(CatalogEntryRetriever &retriever, CatalogType type,
247: 	                                               const string &schema, const string &name,
248: 	                                               OnEntryNotFound if_not_found,
249: 	                                               QueryErrorContext error_context = QueryErrorContext());
250: 	DUCKDB_API CatalogEntry &GetEntry(ClientContext &context, CatalogType type, const string &schema,
251: 	                                  const string &name, QueryErrorContext error_context = QueryErrorContext());
252: 	//! Gets the "catalog.schema.name" entry of the specified type, if entry does not exist behavior depends on
253: 	//! OnEntryNotFound
254: 	DUCKDB_API static optional_ptr<CatalogEntry> GetEntry(ClientContext &context, CatalogType type,
255: 	                                                      const string &catalog, const string &schema,
256: 	                                                      const string &name, OnEntryNotFound if_not_found,
257: 	                                                      QueryErrorContext error_context = QueryErrorContext());
258: 	DUCKDB_API static optional_ptr<CatalogEntry> GetEntry(CatalogEntryRetriever &retriever, CatalogType type,
259: 	                                                      const string &catalog, const string &schema,
260: 	                                                      const string &name, OnEntryNotFound if_not_found,
261: 	                                                      QueryErrorContext error_context = QueryErrorContext());
262: 	DUCKDB_API static CatalogEntry &GetEntry(ClientContext &context, CatalogType type, const string &catalog,
263: 	                                         const string &schema, const string &name,
264: 	                                         QueryErrorContext error_context = QueryErrorContext());
265: 
266: 	template <class T>
267: 	optional_ptr<T> GetEntry(ClientContext &context, const string &schema_name, const string &name,
268: 	                         OnEntryNotFound if_not_found, QueryErrorContext error_context = QueryErrorContext()) {
269: 		auto entry = GetEntry(context, T::Type, schema_name, name, if_not_found, error_context);
270: 		if (!entry) {
271: 			return nullptr;
272: 		}
273: 		if (entry->type != T::Type) {
274: 			throw CatalogException(error_context, "%s is not an %s", name, T::Name);
275: 		}
276: 		return &entry->template Cast<T>();
277: 	}
278: 	template <class T>
279: 	T &GetEntry(ClientContext &context, const string &schema_name, const string &name,
280: 	            QueryErrorContext error_context = QueryErrorContext()) {
281: 		auto entry = GetEntry<T>(context, schema_name, name, OnEntryNotFound::THROW_EXCEPTION, error_context);
282: 		return *entry;
283: 	}
284: 
285: 	//! Append a scalar or aggregate function to the catalog
286: 	DUCKDB_API optional_ptr<CatalogEntry> AddFunction(ClientContext &context, CreateFunctionInfo &info);
287: 
288: 	//! Alter an existing entry in the catalog.
289: 	DUCKDB_API void Alter(CatalogTransaction transaction, AlterInfo &info);
290: 	DUCKDB_API void Alter(ClientContext &context, AlterInfo &info);
291: 
292: 	virtual unique_ptr<PhysicalOperator> PlanCreateTableAs(ClientContext &context, LogicalCreateTable &op,
293: 	                                                       unique_ptr<PhysicalOperator> plan) = 0;
294: 	virtual unique_ptr<PhysicalOperator> PlanInsert(ClientContext &context, LogicalInsert &op,
295: 	                                                unique_ptr<PhysicalOperator> plan) = 0;
296: 	virtual unique_ptr<PhysicalOperator> PlanDelete(ClientContext &context, LogicalDelete &op,
297: 	                                                unique_ptr<PhysicalOperator> plan) = 0;
298: 	virtual unique_ptr<PhysicalOperator> PlanUpdate(ClientContext &context, LogicalUpdate &op,
299: 	                                                unique_ptr<PhysicalOperator> plan) = 0;
300: 	virtual unique_ptr<LogicalOperator> BindCreateIndex(Binder &binder, CreateStatement &stmt, TableCatalogEntry &table,
301: 	                                                    unique_ptr<LogicalOperator> plan);
302: 	virtual unique_ptr<LogicalOperator> BindAlterAddIndex(Binder &binder, TableCatalogEntry &table_entry,
303: 	                                                      unique_ptr<LogicalOperator> plan,
304: 	                                                      unique_ptr<CreateIndexInfo> create_info,
305: 	                                                      unique_ptr<AlterTableInfo> alter_info);
306: 
307: 	virtual DatabaseSize GetDatabaseSize(ClientContext &context) = 0;
308: 	virtual vector<MetadataBlockInfo> GetMetadataInfo(ClientContext &context);
309: 
310: 	virtual bool InMemory() = 0;
311: 	virtual string GetDBPath() = 0;
312: 
313: 	//! Whether or not this catalog should search a specific type with the standard priority
314: 	DUCKDB_API virtual CatalogLookupBehavior CatalogTypeLookupRule(CatalogType type) const {
315: 		return CatalogLookupBehavior::STANDARD;
316: 	}
317: 
318: 	//! The default table is used for `SELECT * FROM <catalog_name>;`
319: 	DUCKDB_API bool HasDefaultTable() const;
320: 	DUCKDB_API void SetDefaultTable(const string &schema, const string &name);
321: 	DUCKDB_API string GetDefaultTable() const;
322: 	DUCKDB_API string GetDefaultTableSchema() const;
323: 
324: 	//! Returns the dependency manager of this catalog - if the catalog has anye
325: 	virtual optional_ptr<DependencyManager> GetDependencyManager();
326: 
327: public:
328: 	template <class T>
329: 	static optional_ptr<T> GetEntry(ClientContext &context, const string &catalog_name, const string &schema_name,
330: 	                                const string &name, OnEntryNotFound if_not_found,
331: 	                                QueryErrorContext error_context = QueryErrorContext()) {
332: 		auto entry = GetEntry(context, T::Type, catalog_name, schema_name, name, if_not_found, error_context);
333: 		if (!entry) {
334: 			return nullptr;
335: 		}
336: 		if (entry->type != T::Type) {
337: 			throw CatalogException(error_context, "%s is not an %s", name, T::Name);
338: 		}
339: 		return &entry->template Cast<T>();
340: 	}
341: 	template <class T>
342: 	static T &GetEntry(ClientContext &context, const string &catalog_name, const string &schema_name,
343: 	                   const string &name, QueryErrorContext error_context = QueryErrorContext()) {
344: 		auto entry =
345: 		    GetEntry<T>(context, catalog_name, schema_name, name, OnEntryNotFound::THROW_EXCEPTION, error_context);
346: 		return *entry;
347: 	}
348: 
349: 	DUCKDB_API vector<reference<SchemaCatalogEntry>> GetSchemas(ClientContext &context);
350: 	DUCKDB_API static vector<reference<SchemaCatalogEntry>> GetSchemas(ClientContext &context,
351: 	                                                                   const string &catalog_name);
352: 	DUCKDB_API static vector<reference<SchemaCatalogEntry>> GetSchemas(CatalogEntryRetriever &retriever,
353: 	                                                                   const string &catalog_name);
354: 	DUCKDB_API static vector<reference<SchemaCatalogEntry>> GetAllSchemas(ClientContext &context);
355: 
356: 	virtual void Verify();
357: 
358: 	static CatalogException UnrecognizedConfigurationError(ClientContext &context, const string &name);
359: 
360: 	//! Autoload the extension required for `configuration_name` or throw a CatalogException
361: 	static void AutoloadExtensionByConfigName(ClientContext &context, const string &configuration_name);
362: 	//! Autoload the extension required for `function_name` or throw a CatalogException
363: 	static bool AutoLoadExtensionByCatalogEntry(DatabaseInstance &db, CatalogType type, const string &entry_name);
364: 	DUCKDB_API static bool TryAutoLoad(ClientContext &context, const string &extension_name) noexcept;
365: 
366: protected:
367: 	//! Reference to the database
368: 	AttachedDatabase &db;
369: 
370: 	//! (optionally) a default table to query for `SELECT * FROM <catalog_name>;`
371: 	string default_table;
372: 	string default_table_schema;
373: 
374: public:
375: 	//! Lookup an entry using TryLookupEntry, throws if entry not found and if_not_found == THROW_EXCEPTION
376: 	CatalogEntryLookup LookupEntry(CatalogEntryRetriever &retriever, CatalogType type, const string &schema,
377: 	                               const string &name, OnEntryNotFound if_not_found,
378: 	                               QueryErrorContext error_context = QueryErrorContext());
379: 
380: private:
381: 	//! Lookup an entry in the schema, returning a lookup with the entry and schema if they exist
382: 	CatalogEntryLookup TryLookupEntryInternal(CatalogTransaction transaction, CatalogType type, const string &schema,
383: 	                                          const string &name);
384: 	//! Calls LookupEntryInternal on the schema, trying other schemas if the schema is invalid. Sets
385: 	//! CatalogEntryLookup->error depending on if_not_found when no entry is found
386: 	CatalogEntryLookup TryLookupEntry(CatalogEntryRetriever &retriever, CatalogType type, const string &schema,
387: 	                                  const string &name, OnEntryNotFound if_not_found,
388: 	                                  QueryErrorContext error_context = QueryErrorContext());
389: 	static CatalogEntryLookup TryLookupEntry(CatalogEntryRetriever &retriever, vector<CatalogLookup> &lookups,
390: 	                                         CatalogType type, const string &name, OnEntryNotFound if_not_found,
391: 	                                         QueryErrorContext error_context = QueryErrorContext());
392: 	static CatalogEntryLookup TryLookupEntry(CatalogEntryRetriever &retriever, CatalogType type, const string &catalog,
393: 	                                         const string &schema, const string &name, OnEntryNotFound if_not_found,
394: 	                                         QueryErrorContext error_context);
395: 
396: 	//! Looks for a Catalog with a DefaultTable that matches the lookup
397: 	static CatalogEntryLookup TryLookupDefaultTable(CatalogEntryRetriever &retriever, CatalogType type,
398: 	                                                const string &catalog, const string &schema, const string &name,
399: 	                                                OnEntryNotFound if_not_found, QueryErrorContext error_context);
400: 
401: 	//! Return an exception with did-you-mean suggestion.
402: 	static CatalogException CreateMissingEntryException(CatalogEntryRetriever &retriever, const string &entry_name,
403: 	                                                    CatalogType type,
404: 	                                                    const reference_set_t<SchemaCatalogEntry> &schemas,
405: 	                                                    QueryErrorContext error_context);
406: 
407: 	//! Return the close entry name, the distance and the belonging schema.
408: 	static vector<SimilarCatalogEntry> SimilarEntriesInSchemas(ClientContext &context, const string &entry_name,
409: 	                                                           CatalogType type,
410: 	                                                           const reference_set_t<SchemaCatalogEntry> &schemas);
411: 
412: 	virtual void DropSchema(ClientContext &context, DropInfo &info) = 0;
413: 
414: public:
415: 	template <class TARGET>
416: 	TARGET &Cast() {
417: 		DynamicCastCheck<TARGET>(this);
418: 		return reinterpret_cast<TARGET &>(*this);
419: 	}
420: 
421: 	template <class TARGET>
422: 	const TARGET &Cast() const {
423: 		DynamicCastCheck<TARGET>(this);
424: 		return reinterpret_cast<const TARGET &>(*this);
425: 	}
426: };
427: 
428: } // namespace duckdb
[end of src/include/duckdb/catalog/catalog.hpp]
[start of src/include/duckdb/catalog/catalog_search_path.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/catalog/catalog_search_path.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include <functional>
12: #include "duckdb/common/enums/catalog_type.hpp"
13: #include "duckdb/common/string.hpp"
14: #include "duckdb/common/vector.hpp"
15: #include "duckdb/common/types/value.hpp"
16: 
17: namespace duckdb {
18: 
19: class ClientContext;
20: 
21: struct CatalogSearchEntry {
22: 	CatalogSearchEntry(string catalog, string schema);
23: 
24: 	string catalog;
25: 	string schema;
26: 
27: public:
28: 	string ToString() const;
29: 	static string ListToString(const vector<CatalogSearchEntry> &input);
30: 	static CatalogSearchEntry Parse(const string &input);
31: 	static vector<CatalogSearchEntry> ParseList(const string &input);
32: 
33: private:
34: 	static CatalogSearchEntry ParseInternal(const string &input, idx_t &pos);
35: 	static string WriteOptionallyQuoted(const string &input);
36: };
37: 
38: enum class CatalogSetPathType { SET_SCHEMA, SET_SCHEMAS };
39: 
40: //! The schema search path, in order by which entries are searched if no schema entry is provided
41: class CatalogSearchPath {
42: public:
43: 	DUCKDB_API explicit CatalogSearchPath(ClientContext &client_p);
44: 	DUCKDB_API CatalogSearchPath(ClientContext &client_p, vector<CatalogSearchEntry> entries);
45: 	CatalogSearchPath(const CatalogSearchPath &other) = delete;
46: 
47: 	DUCKDB_API void Set(CatalogSearchEntry new_value, CatalogSetPathType set_type);
48: 	DUCKDB_API void Set(vector<CatalogSearchEntry> new_paths, CatalogSetPathType set_type);
49: 	DUCKDB_API void Reset();
50: 
51: 	DUCKDB_API const vector<CatalogSearchEntry> &Get();
52: 	const vector<CatalogSearchEntry> &GetSetPaths() {
53: 		return set_paths;
54: 	}
55: 	DUCKDB_API const CatalogSearchEntry &GetDefault();
56: 	DUCKDB_API string GetDefaultSchema(const string &catalog);
57: 	DUCKDB_API string GetDefaultCatalog(const string &schema);
58: 
59: 	DUCKDB_API vector<string> GetSchemasForCatalog(const string &catalog);
60: 	DUCKDB_API vector<string> GetCatalogsForSchema(const string &schema);
61: 
62: 	DUCKDB_API bool SchemaInSearchPath(ClientContext &context, const string &catalog_name, const string &schema_name);
63: 
64: private:
65: 	//! Set paths without checking if they exist
66: 	void SetPathsInternal(vector<CatalogSearchEntry> new_paths);
67: 	string GetSetName(CatalogSetPathType set_type);
68: 
69: private:
70: 	ClientContext &context;
71: 	vector<CatalogSearchEntry> paths;
72: 	//! Only the paths that were explicitly set (minus the always included paths)
73: 	vector<CatalogSearchEntry> set_paths;
74: };
75: 
76: } // namespace duckdb
[end of src/include/duckdb/catalog/catalog_search_path.hpp]
[start of src/include/duckdb/execution/index/art/art.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/index/art/art.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/execution/index/bound_index.hpp"
12: #include "duckdb/execution/index/art/node.hpp"
13: #include "duckdb/common/array.hpp"
14: 
15: namespace duckdb {
16: 
17: enum class VerifyExistenceType : uint8_t { APPEND = 0, APPEND_FK = 1, DELETE_FK = 2 };
18: enum class ARTConflictType : uint8_t { NO_CONFLICT = 0, CONSTRAINT = 1, TRANSACTION = 2 };
19: enum class ARTAppendMode : uint8_t { DEFAULT = 0, IGNORE_DUPLICATES = 1, INSERT_DUPLICATES = 2 };
20: 
21: class ConflictManager;
22: class ARTKey;
23: class ARTKeySection;
24: class FixedSizeAllocator;
25: 
26: struct ARTIndexScanState;
27: 
28: class ART : public BoundIndex {
29: public:
30: 	friend class Leaf;
31: 
32: public:
33: 	//! Index type name for the ART.
34: 	static constexpr const char *TYPE_NAME = "ART";
35: 	//! FixedSizeAllocator count of the ART.
36: 	static constexpr uint8_t ALLOCATOR_COUNT = 9;
37: 	//! FixedSizeAllocator count of deprecated ARTs.
38: 	static constexpr uint8_t DEPRECATED_ALLOCATOR_COUNT = ALLOCATOR_COUNT - 3;
39: 
40: public:
41: 	ART(const string &name, const IndexConstraintType index_constraint_type, const vector<column_t> &column_ids,
42: 	    TableIOManager &table_io_manager, const vector<unique_ptr<Expression>> &unbound_expressions,
43: 	    AttachedDatabase &db,
44: 	    const shared_ptr<array<unsafe_unique_ptr<FixedSizeAllocator>, ALLOCATOR_COUNT>> &allocators_ptr = nullptr,
45: 	    const IndexStorageInfo &info = IndexStorageInfo());
46: 
47: 	//! Create a index instance of this type.
48: 	static unique_ptr<BoundIndex> Create(CreateIndexInput &input) {
49: 		auto art = make_uniq<ART>(input.name, input.constraint_type, input.column_ids, input.table_io_manager,
50: 		                          input.unbound_expressions, input.db, nullptr, input.storage_info);
51: 		return std::move(art);
52: 	}
53: 
54: 	//! Plan index construction.
55: 	static unique_ptr<PhysicalOperator> CreatePlan(PlanIndexInput &input);
56: 
57: 	//! Root of the tree.
58: 	Node tree = Node();
59: 	//! Fixed-size allocators holding the ART nodes.
60: 	shared_ptr<array<unsafe_unique_ptr<FixedSizeAllocator>, ALLOCATOR_COUNT>> allocators;
61: 	//! True, if the ART owns its data.
62: 	bool owns_data;
63: 	//! The number of bytes fitting in the prefix.
64: 	uint8_t prefix_count;
65: 	//! The append mode.
66: 	ARTAppendMode append_mode;
67: 
68: public:
69: 	//! Try to initialize a scan on the ART with the given expression and filter.
70: 	unique_ptr<IndexScanState> TryInitializeScan(const Expression &expr, const Expression &filter_expr);
71: 	//! Perform a lookup on the ART, fetching up to max_count row IDs.
72: 	//! If all row IDs were fetched, it return true, else false.
73: 	bool Scan(IndexScanState &state, idx_t max_count, unsafe_vector<row_t> &row_ids);
74: 
75: 	//! Appends data to the locked index.
76: 	ErrorData Append(IndexLock &l, DataChunk &chunk, Vector &row_ids) override;
77: 	//! Appends data to the locked index and verifies constraint violations against a delete index.
78: 	ErrorData AppendWithDeleteIndex(IndexLock &l, DataChunk &chunk, Vector &row_ids,
79: 	                                optional_ptr<BoundIndex> delete_index) override;
80: 
81: 	//! Internally inserts a chunk.
82: 	ARTConflictType Insert(Node &node, const ARTKey &key, idx_t depth, const ARTKey &row_id, const GateStatus status,
83: 	                       optional_ptr<ART> delete_art);
84: 	//! Insert a chunk.
85: 	ErrorData Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids) override;
86: 	//! Insert a chunk and verifies constraint violations against a delete index.
87: 	ErrorData Insert(IndexLock &l, DataChunk &data, Vector &row_ids, optional_ptr<BoundIndex> delete_index) override;
88: 
89: 	//! Verify that data can be appended to the index without a constraint violation.
90: 	void VerifyAppend(DataChunk &chunk, optional_ptr<BoundIndex> delete_index,
91: 	                  optional_ptr<ConflictManager> manager) override;
92: 
93: 	//! Delete a chunk from the ART.
94: 	void Delete(IndexLock &lock, DataChunk &entries, Vector &row_ids) override;
95: 	//! Drop the ART.
96: 	void CommitDrop(IndexLock &index_lock) override;
97: 
98: 	//! Construct an ART from a vector of sorted keys and their row IDs.
99: 	bool Construct(unsafe_vector<ARTKey> &keys, unsafe_vector<ARTKey> &row_ids, const idx_t row_count);
100: 
101: 	//! Merge another ART into this ART. Both must be locked.
102: 	bool MergeIndexes(IndexLock &state, BoundIndex &other_index) override;
103: 
104: 	//! Vacuums the ART storage.
105: 	void Vacuum(IndexLock &state) override;
106: 
107: 	//! Returns ART storage serialization information.
108: 	IndexStorageInfo GetStorageInfo(const case_insensitive_map_t<Value> &options, const bool to_wal) override;
109: 	//! Returns the in-memory usage of the ART.
110: 	idx_t GetInMemorySize(IndexLock &index_lock) override;
111: 
112: 	//! ART key generation.
113: 	template <bool IS_NOT_NULL = false>
114: 	static void GenerateKeys(ArenaAllocator &allocator, DataChunk &input, unsafe_vector<ARTKey> &keys);
115: 	static void GenerateKeyVectors(ArenaAllocator &allocator, DataChunk &input, Vector &row_ids,
116: 	                               unsafe_vector<ARTKey> &keys, unsafe_vector<ARTKey> &row_id_keys);
117: 
118: 	//! Verifies the nodes and optionally returns a string of the ART.
119: 	string VerifyAndToString(IndexLock &state, const bool only_verify) override;
120: 	//! Verifies that the node allocations match the node counts.
121: 	void VerifyAllocations(IndexLock &state) override;
122: 
123: private:
124: 	bool SearchEqual(ARTKey &key, idx_t max_count, unsafe_vector<row_t> &row_ids);
125: 	bool SearchGreater(ARTKey &key, bool equal, idx_t max_count, unsafe_vector<row_t> &row_ids);
126: 	bool SearchLess(ARTKey &upper_bound, bool equal, idx_t max_count, unsafe_vector<row_t> &row_ids);
127: 	bool SearchCloseRange(ARTKey &lower_bound, ARTKey &upper_bound, bool left_equal, bool right_equal, idx_t max_count,
128: 	                      unsafe_vector<row_t> &row_ids);
129: 	const unsafe_optional_ptr<const Node> Lookup(const Node &node, const ARTKey &key, idx_t depth);
130: 
131: 	void InsertIntoEmpty(Node &node, const ARTKey &key, const idx_t depth, const ARTKey &row_id,
132: 	                     const GateStatus status);
133: 	ARTConflictType InsertIntoInlined(Node &node, const ARTKey &key, const idx_t depth, const ARTKey &row_id,
134: 	                                  const GateStatus status, optional_ptr<ART> delete_art);
135: 	ARTConflictType InsertIntoNode(Node &node, const ARTKey &key, const idx_t depth, const ARTKey &row_id,
136: 	                               const GateStatus status, optional_ptr<ART> delete_art);
137: 
138: 	string GenerateErrorKeyName(DataChunk &input, idx_t row);
139: 	string GenerateConstraintErrorMessage(VerifyExistenceType verify_type, const string &key_name);
140: 	void VerifyLeaf(const Node &leaf, const ARTKey &key, optional_ptr<ART> delete_art, ConflictManager &manager,
141: 	                optional_idx &conflict_idx, idx_t i);
142: 	void VerifyConstraint(DataChunk &chunk, optional_ptr<BoundIndex> delete_index, ConflictManager &manager) override;
143: 	string GetConstraintViolationMessage(VerifyExistenceType verify_type, idx_t failed_index,
144: 	                                     DataChunk &input) override;
145: 
146: 	void Erase(Node &node, reference<const ARTKey> key, idx_t depth, reference<const ARTKey> row_id, GateStatus status);
147: 
148: 	bool ConstructInternal(const unsafe_vector<ARTKey> &keys, const unsafe_vector<ARTKey> &row_ids, Node &node,
149: 	                       ARTKeySection &section);
150: 
151: 	void InitializeMerge(unsafe_vector<idx_t> &upper_bounds);
152: 
153: 	void InitializeVacuum(unordered_set<uint8_t> &indexes);
154: 	void FinalizeVacuum(const unordered_set<uint8_t> &indexes);
155: 
156: 	void InitAllocators(const IndexStorageInfo &info);
157: 	void TransformToDeprecated();
158: 	void Deserialize(const BlockPointer &pointer);
159: 	void WritePartialBlocks(const bool v1_0_0_storage);
160: 	void SetPrefixCount(const IndexStorageInfo &info);
161: 
162: 	string VerifyAndToStringInternal(const bool only_verify);
163: 	void VerifyAllocationsInternal();
164: };
165: 
166: template <>
167: void ART::GenerateKeys<>(ArenaAllocator &allocator, DataChunk &input, unsafe_vector<ARTKey> &keys);
168: 
169: template <>
170: void ART::GenerateKeys<true>(ArenaAllocator &allocator, DataChunk &input, unsafe_vector<ARTKey> &keys);
171: 
172: } // namespace duckdb
[end of src/include/duckdb/execution/index/art/art.hpp]
[start of src/include/duckdb/execution/index/art/prefix.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/index/art/prefix.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: #pragma once
9: 
10: #include "duckdb/execution/index/fixed_size_allocator.hpp"
11: #include "duckdb/execution/index/art/art.hpp"
12: #include "duckdb/execution/index/art/node.hpp"
13: 
14: namespace duckdb {
15: 
16: class ARTKey;
17: 
18: //! Prefix is a wrapper class to access a prefix.
19: //! The prefix contains up to the ART's prefix size bytes and an additional byte for the count.
20: //! It also contains a Node pointer to a child node.
21: class Prefix {
22: public:
23: 	static constexpr NType PREFIX = NType::PREFIX;
24: 
25: 	static constexpr uint8_t ROW_ID_SIZE = sizeof(row_t);
26: 	static constexpr uint8_t ROW_ID_COUNT = ROW_ID_SIZE - 1;
27: 	static constexpr uint8_t DEPRECATED_COUNT = 15;
28: 	static constexpr uint8_t METADATA_SIZE = sizeof(Node) + 1;
29: 
30: public:
31: 	Prefix() = delete;
32: 	Prefix(const ART &art, const Node ptr_p, const bool is_mutable = false, const bool set_in_memory = false);
33: 	Prefix(unsafe_unique_ptr<FixedSizeAllocator> &allocator, const Node ptr_p, const idx_t count);
34: 
35: 	data_ptr_t data;
36: 	Node *ptr;
37: 	bool in_memory;
38: 
39: public:
40: 	static inline uint8_t Count(const ART &art) {
41: 		return art.prefix_count;
42: 	}
43: 	static idx_t GetMismatchWithOther(const Prefix &l_prefix, const Prefix &r_prefix, const idx_t max_count);
44: 	static idx_t GetMismatchWithKey(ART &art, const Node &node, const ARTKey &key, idx_t &depth);
45: 	static uint8_t GetByte(const ART &art, const Node &node, const uint8_t pos);
46: 
47: public:
48: 	//! Get a new list of prefix nodes. The node reference holds the last prefix of the list.
49: 	static void New(ART &art, reference<Node> &ref, const ARTKey &key, const idx_t depth, idx_t count);
50: 
51: 	//! Free the prefix and its child.
52: 	static void Free(ART &art, Node &node);
53: 
54: 	//! Initializes a merge by incrementing the buffer ID of the prefix and its child.
55: 	static void InitializeMerge(ART &art, Node &node, const unsafe_vector<idx_t> &upper_bounds);
56: 
57: 	//! Concatenates parent -> byte -> child. Special-handling, if
58: 	//! 1. the byte was in a gate node.
59: 	//! 2. the byte was in PREFIX_INLINED.
60: 	static void Concat(ART &art, Node &parent, uint8_t byte, const GateStatus old_status, const Node &child,
61: 	                   const GateStatus status);
62: 
63: 	//! Traverse a prefix and a key until
64: 	//! 1. a non-prefix node.
65: 	//! 2. a mismatching byte.
66: 	//! Early-out, if the next prefix is a gate node.
67: 	static idx_t Traverse(ART &art, reference<const Node> &node, const ARTKey &key, idx_t &depth);
68: 	static idx_t TraverseMutable(ART &art, reference<Node> &node, const ARTKey &key, idx_t &depth);
69: 
70: 	//! Traverse two prefixes to find
71: 	//! 1. that they match.
72: 	//! 2. that they mismatch.
73: 	//! 3. that one prefix contains the other prefix.
74: 	static bool Traverse(ART &art, reference<Node> &l_node, reference<Node> &r_node, idx_t &pos,
75: 	                     const GateStatus status);
76: 
77: 	//! Removes up to pos bytes from the prefix.
78: 	//! Shifts all subsequent bytes by pos. Frees empty nodes.
79: 	static void Reduce(ART &art, Node &node, const idx_t pos);
80: 	//! Splits the prefix at pos.
81: 	//! prefix_node points to the node that replaces the split byte.
82: 	//! child_node points to the remaining node after the split.
83: 	//! Returns INSIDE, if a gate node was freed, else OUTSIDE.
84: 	static GateStatus Split(ART &art, reference<Node> &node, Node &child, const uint8_t pos);
85: 
86: 	//! Insert a key into a prefix.
87: 	static ARTConflictType Insert(ART &art, Node &node, const ARTKey &key, idx_t depth, const ARTKey &row_id,
88: 	                              const GateStatus status, optional_ptr<ART> delete_art);
89: 
90: 	//! Returns the string representation of the node, or only traverses and verifies the node and its subtree
91: 	static string VerifyAndToString(ART &art, const Node &node, const bool only_verify);
92: 	//! Count the number of prefixes.
93: 	static void VerifyAllocations(ART &art, const Node &node, unordered_map<uint8_t, idx_t> &node_counts);
94: 
95: 	//! Vacuum the child of the node.
96: 	static void Vacuum(ART &art, Node &node, const unordered_set<uint8_t> &indexes);
97: 
98: 	//! Transform the child of the node.
99: 	static void TransformToDeprecated(ART &art, Node &node, unsafe_unique_ptr<FixedSizeAllocator> &allocator);
100: 
101: private:
102: 	static Prefix NewInternal(ART &art, Node &node, const data_ptr_t data, const uint8_t count, const idx_t offset,
103: 	                          const NType type);
104: 
105: 	static Prefix GetTail(ART &art, const Node &node);
106: 
107: 	static void ConcatGate(ART &art, Node &parent, uint8_t byte, const Node &child);
108: 	static void ConcatChildIsGate(ART &art, Node &parent, uint8_t byte, const Node &child);
109: 
110: 	Prefix Append(ART &art, const uint8_t byte);
111: 	void Append(ART &art, Node other);
112: 	Prefix TransformToDeprecatedAppend(ART &art, unsafe_unique_ptr<FixedSizeAllocator> &allocator, uint8_t byte);
113: 
114: private:
115: 	template <class F, class NODE>
116: 	static void Iterator(ART &art, reference<NODE> &ref, const bool exit_gate, const bool is_mutable, F &&lambda) {
117: 		while (ref.get().HasMetadata() && ref.get().GetType() == PREFIX) {
118: 			Prefix prefix(art, ref, is_mutable);
119: 			lambda(prefix);
120: 
121: 			ref = *prefix.ptr;
122: 			if (exit_gate && ref.get().GetGateStatus() == GateStatus::GATE_SET) {
123: 				break;
124: 			}
125: 		}
126: 	}
127: };
128: } // namespace duckdb
[end of src/include/duckdb/execution/index/art/prefix.hpp]
[start of src/include/duckdb/execution/operator/persistent/physical_insert.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/persistent/physical_insert.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/execution/physical_operator.hpp"
12: #include "duckdb/planner/expression.hpp"
13: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
14: #include "duckdb/common/index_vector.hpp"
15: #include "duckdb/parser/statement/insert_statement.hpp"
16: #include "duckdb/storage/table/append_state.hpp"
17: #include "duckdb/catalog/catalog_entry/duck_table_entry.hpp"
18: #include "duckdb/storage/table/delete_state.hpp"
19: 
20: namespace duckdb {
21: 
22: //===--------------------------------------------------------------------===//
23: // Sink
24: //===--------------------------------------------------------------------===//
25: class InsertGlobalState : public GlobalSinkState {
26: public:
27: 	explicit InsertGlobalState(ClientContext &context, const vector<LogicalType> &return_types, DuckTableEntry &table);
28: 
29: public:
30: 	mutex lock;
31: 	DuckTableEntry &table;
32: 	idx_t insert_count;
33: 	bool initialized;
34: 	LocalAppendState append_state;
35: 	ColumnDataCollection return_collection;
36: };
37: 
38: class InsertLocalState : public LocalSinkState {
39: public:
40: public:
41: 	InsertLocalState(ClientContext &context, const vector<LogicalType> &types_p,
42: 	                 const vector<unique_ptr<Expression>> &bound_defaults,
43: 	                 const vector<unique_ptr<BoundConstraint>> &bound_constraints);
44: 
45: public:
46: 	ConstraintState &GetConstraintState(DataTable &table, TableCatalogEntry &table_ref);
47: 	TableDeleteState &GetDeleteState(DataTable &table, TableCatalogEntry &table_ref, ClientContext &context);
48: 
49: public:
50: 	//! The to-be-inserted chunk.
51: 	//! We initialize it lazily, as we need to know which columns will be references and which will be set to their
52: 	//! default values.
53: 	DataChunk insert_chunk;
54: 	bool init_insert_chunk = true;
55: 	vector<LogicalType> types;
56: 	//! The chunk containing the tuples that become an update (if DO UPDATE)
57: 	DataChunk update_chunk;
58: 	ExpressionExecutor default_executor;
59: 	TableAppendState local_append_state;
60: 	unique_ptr<RowGroupCollection> local_collection;
61: 	optional_ptr<OptimisticDataWriter> writer;
62: 	// Rows that have been updated by a DO UPDATE conflict
63: 	unordered_set<row_t> updated_rows;
64: 	idx_t update_count = 0;
65: 	unique_ptr<ConstraintState> constraint_state;
66: 	const vector<unique_ptr<BoundConstraint>> &bound_constraints;
67: 	//! The delete state for ON CONFLICT handling that is rewritten into DELETE + INSERT.
68: 	unique_ptr<TableDeleteState> delete_state;
69: 	//! The append chunk for ON CONFLICT handling that is rewritting into DELETE + INSERT.
70: 	DataChunk append_chunk;
71: };
72: 
73: //! Physically insert a set of data into a table
74: class PhysicalInsert : public PhysicalOperator {
75: public:
76: 	static constexpr const PhysicalOperatorType TYPE = PhysicalOperatorType::INSERT;
77: 
78: public:
79: 	//! INSERT INTO
80: 	PhysicalInsert(vector<LogicalType> types, TableCatalogEntry &table, physical_index_vector_t<idx_t> column_index_map,
81: 	               vector<unique_ptr<Expression>> bound_defaults, vector<unique_ptr<BoundConstraint>> bound_constraints,
82: 	               vector<unique_ptr<Expression>> set_expressions, vector<PhysicalIndex> set_columns,
83: 	               vector<LogicalType> set_types, idx_t estimated_cardinality, bool return_chunk, bool parallel,
84: 	               OnConflictAction action_type, unique_ptr<Expression> on_conflict_condition,
85: 	               unique_ptr<Expression> do_update_condition, unordered_set<column_t> on_conflict_filter,
86: 	               vector<column_t> columns_to_fetch, bool update_is_del_and_insert);
87: 	//! CREATE TABLE AS
88: 	PhysicalInsert(LogicalOperator &op, SchemaCatalogEntry &schema, unique_ptr<BoundCreateTableInfo> info,
89: 	               idx_t estimated_cardinality, bool parallel);
90: 
91: 	//! The map from insert column index to table column index
92: 	physical_index_vector_t<idx_t> column_index_map;
93: 	//! The table to insert into
94: 	optional_ptr<TableCatalogEntry> insert_table;
95: 	//! The insert types
96: 	vector<LogicalType> insert_types;
97: 	//! The default expressions of the columns for which no value is provided
98: 	vector<unique_ptr<Expression>> bound_defaults;
99: 	//! The bound constraints for the table
100: 	vector<unique_ptr<BoundConstraint>> bound_constraints;
101: 	//! If the returning statement is present, return the whole chunk
102: 	bool return_chunk;
103: 	//! Table schema, in case of CREATE TABLE AS
104: 	optional_ptr<SchemaCatalogEntry> schema;
105: 	//! Create table info, in case of CREATE TABLE AS
106: 	unique_ptr<BoundCreateTableInfo> info;
107: 	//! Whether or not the INSERT can be executed in parallel
108: 	//! This insert is not order preserving if executed in parallel
109: 	bool parallel;
110: 	// Which action to perform on conflict
111: 	OnConflictAction action_type;
112: 
113: 	// The DO UPDATE set expressions, if 'action_type' is UPDATE
114: 	vector<unique_ptr<Expression>> set_expressions;
115: 	// Which columns are targeted by the set expressions
116: 	vector<PhysicalIndex> set_columns;
117: 	// The types of the columns targeted by a SET expression
118: 	vector<LogicalType> set_types;
119: 
120: 	// Condition for the ON CONFLICT clause
121: 	unique_ptr<Expression> on_conflict_condition;
122: 	// Condition for the DO UPDATE clause
123: 	unique_ptr<Expression> do_update_condition;
124: 	// The column ids to apply the ON CONFLICT on
125: 	unordered_set<column_t> conflict_target;
126: 	//! True, if the INSERT OR REPLACE requires delete + insert.
127: 	bool update_is_del_and_insert;
128: 
129: 	// Column ids from the original table to fetch
130: 	vector<StorageIndex> columns_to_fetch;
131: 	// Matching types to the column ids to fetch
132: 	vector<LogicalType> types_to_fetch;
133: 
134: public:
135: 	// Source interface
136: 	unique_ptr<GlobalSourceState> GetGlobalSourceState(ClientContext &context) const override;
137: 	SourceResultType GetData(ExecutionContext &context, DataChunk &chunk, OperatorSourceInput &input) const override;
138: 
139: 	bool IsSource() const override {
140: 		return true;
141: 	}
142: 
143: public:
144: 	// Sink interface
145: 	unique_ptr<GlobalSinkState> GetGlobalSinkState(ClientContext &context) const override;
146: 	unique_ptr<LocalSinkState> GetLocalSinkState(ExecutionContext &context) const override;
147: 	SinkResultType Sink(ExecutionContext &context, DataChunk &chunk, OperatorSinkInput &input) const override;
148: 	SinkCombineResultType Combine(ExecutionContext &context, OperatorSinkCombineInput &input) const override;
149: 	SinkFinalizeType Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
150: 	                          OperatorSinkFinalizeInput &input) const override;
151: 
152: 	bool IsSink() const override {
153: 		return true;
154: 	}
155: 
156: 	bool ParallelSink() const override {
157: 		return parallel;
158: 	}
159: 
160: 	bool SinkOrderDependent() const override {
161: 		return true;
162: 	}
163: 
164: public:
165: 	static void GetInsertInfo(const BoundCreateTableInfo &info, vector<LogicalType> &insert_types,
166: 	                          vector<unique_ptr<Expression>> &bound_defaults);
167: 	static void ResolveDefaults(const TableCatalogEntry &table, DataChunk &chunk,
168: 	                            const physical_index_vector_t<idx_t> &column_index_map,
169: 	                            ExpressionExecutor &defaults_executor, DataChunk &result);
170: 
171: protected:
172: 	void CombineExistingAndInsertTuples(DataChunk &result, DataChunk &scan_chunk, DataChunk &input_chunk,
173: 	                                    ClientContext &client) const;
174: 	//! Returns the amount of updated tuples
175: 	void CreateUpdateChunk(ExecutionContext &context, DataChunk &chunk, TableCatalogEntry &table, Vector &row_ids,
176: 	                       DataChunk &result) const;
177: 	idx_t OnConflictHandling(TableCatalogEntry &table, ExecutionContext &context, InsertLocalState &lstate) const;
178: };
179: 
180: } // namespace duckdb
[end of src/include/duckdb/execution/operator/persistent/physical_insert.hpp]
[start of src/include/duckdb/execution/reservoir_sample.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/reservoir_sample.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/allocator.hpp"
12: #include "duckdb/common/common.hpp"
13: #include "duckdb/common/random_engine.hpp"
14: #include "duckdb/common/types/data_chunk.hpp"
15: #include "duckdb/common/windows_undefs.hpp"
16: 
17: #include "duckdb/common/queue.hpp"
18: 
19: // Originally intended to be the vector size, but in order to run on
20: // vector size = 2, we had to change it.
21: #define FIXED_SAMPLE_SIZE 2048
22: 
23: namespace duckdb {
24: 
25: enum class SampleType : uint8_t { BLOCKING_SAMPLE = 0, RESERVOIR_SAMPLE = 1, RESERVOIR_PERCENTAGE_SAMPLE = 2 };
26: 
27: enum class SamplingState : uint8_t { RANDOM = 0, RESERVOIR = 1 };
28: 
29: class ReservoirRNG : public RandomEngine {
30: public:
31: 	// return type must be called result type to be a valid URNG
32: 	typedef uint32_t result_type;
33: 
34: 	explicit ReservoirRNG(int64_t seed) : RandomEngine(seed) {};
35: 
36: 	result_type operator()() {
37: 		return NextRandomInteger();
38: 	};
39: 
40: 	static constexpr result_type min() {
41: 		return NumericLimits<result_type>::Minimum();
42: 	};
43: 	static constexpr result_type max() {
44: 		return NumericLimits<result_type>::Maximum();
45: 	};
46: };
47: 
48: //! Resevoir sampling is based on the 2005 paper "Weighted Random Sampling" by Efraimidis and Spirakis
49: class BaseReservoirSampling {
50: public:
51: 	explicit BaseReservoirSampling(int64_t seed);
52: 	BaseReservoirSampling();
53: 
54: 	void InitializeReservoirWeights(idx_t cur_size, idx_t sample_size);
55: 
56: 	void SetNextEntry();
57: 
58: 	void ReplaceElementWithIndex(idx_t entry_index, double with_weight, bool pop = true);
59: 	void ReplaceElement(double with_weight = -1);
60: 
61: 	void UpdateMinWeightThreshold();
62: 
63: 	//! Go from the naive sampling to the reservoir sampling
64: 	//! Naive samping will not collect weights, but when we serialize
65: 	//! we need to serialize weights again.
66: 	void FillWeights(SelectionVector &sel, idx_t &sel_size);
67: 
68: 	unique_ptr<BaseReservoirSampling> Copy();
69: 
70: 	//! The random generator
71: 	ReservoirRNG random;
72: 
73: 	//! The next element to sample
74: 	idx_t next_index_to_sample;
75: 	//! The reservoir threshold of the current min entry
76: 	double min_weight_threshold;
77: 	//! The reservoir index of the current min entry
78: 	idx_t min_weighted_entry_index;
79: 	//! The current count towards next index (i.e. we will replace an entry in next_index - current_count tuples)
80: 	//! The number of entries "seen" before choosing one that will go in our reservoir sample.
81: 	idx_t num_entries_to_skip_b4_next_sample;
82: 	//! when collecting a sample in parallel, we want to know how many values each thread has seen
83: 	//! so we can collect the samples from the thread local states in a uniform manner
84: 	idx_t num_entries_seen_total;
85: 	//! Priority queue of [random element, index] for each of the elements in the sample
86: 	std::priority_queue<std::pair<double, idx_t>> reservoir_weights;
87: 
88: 	void Serialize(Serializer &serializer) const;
89: 	static unique_ptr<BaseReservoirSampling> Deserialize(Deserializer &deserializer);
90: 
91: 	static double GetMinWeightFromTuplesSeen(idx_t rows_seen_total);
92: 	// static unordered_map<idx_t, double> tuples_to_min_weight_map;
93: 	// Blocking sample is a virtual class. It should be allowed to see the weights and
94: 	// of tuples in the sample. The blocking sample can then easily maintain statisitcal properties
95: 	// from the sample point of view.
96: 	friend class BlockingSample;
97: };
98: 
99: class BlockingSample {
100: public:
101: 	static constexpr const SampleType TYPE = SampleType::BLOCKING_SAMPLE;
102: 
103: 	unique_ptr<BaseReservoirSampling> base_reservoir_sample;
104: 	//! The sample type
105: 	SampleType type;
106: 	//! has the sample been destroyed due to updates to the referenced table
107: 	bool destroyed;
108: 
109: public:
110: 	explicit BlockingSample(int64_t seed = -1)
111: 	    : base_reservoir_sample(make_uniq<BaseReservoirSampling>(seed)), type(SampleType::BLOCKING_SAMPLE),
112: 	      destroyed(false) {
113: 	}
114: 	virtual ~BlockingSample() {
115: 	}
116: 
117: 	//! Add a chunk of data to the sample
118: 	virtual void AddToReservoir(DataChunk &input) = 0;
119: 	virtual unique_ptr<BlockingSample> Copy() const = 0;
120: 	virtual void Finalize() = 0;
121: 	virtual void Destroy();
122: 
123: 	//! Fetches a chunk from the sample. destroy = true should only be used when
124: 	//! querying from a sample defined in a query and not a duckdb_table_sample.
125: 	virtual unique_ptr<DataChunk> GetChunk() = 0;
126: 
127: 	virtual void Serialize(Serializer &serializer) const;
128: 	static unique_ptr<BlockingSample> Deserialize(Deserializer &deserializer);
129: 
130: 	//! Helper functions needed to merge two reservoirs while respecting weights of sampled rows
131: 	std::pair<double, idx_t> PopFromWeightQueue();
132: 	double GetMinWeightThreshold();
133: 	idx_t GetPriorityQueueSize();
134: 
135: public:
136: 	template <class TARGET>
137: 	TARGET &Cast() {
138: 		if (type != TARGET::TYPE && TARGET::TYPE != SampleType::BLOCKING_SAMPLE) {
139: 			throw InternalException("Failed to cast sample to type - sample type mismatch");
140: 		}
141: 		return reinterpret_cast<TARGET &>(*this);
142: 	}
143: 
144: 	template <class TARGET>
145: 	const TARGET &Cast() const {
146: 		if (type != TARGET::TYPE && TARGET::TYPE != SampleType::BLOCKING_SAMPLE) {
147: 			throw InternalException("Failed to cast sample to type - sample type mismatch");
148: 		}
149: 		return reinterpret_cast<const TARGET &>(*this);
150: 	}
151: };
152: 
153: class ReservoirChunk {
154: public:
155: 	ReservoirChunk() {
156: 	}
157: 
158: 	DataChunk chunk;
159: 	void Serialize(Serializer &serializer) const;
160: 	static unique_ptr<ReservoirChunk> Deserialize(Deserializer &deserializer);
161: 
162: 	unique_ptr<ReservoirChunk> Copy() const;
163: };
164: 
165: struct SelectionVectorHelper {
166: 	SelectionVector sel;
167: 	uint32_t size;
168: };
169: 
170: class ReservoirSample : public BlockingSample {
171: public:
172: 	static constexpr const SampleType TYPE = SampleType::RESERVOIR_SAMPLE;
173: 
174: 	constexpr static idx_t FIXED_SAMPLE_SIZE_MULTIPLIER = 10;
175: 	constexpr static idx_t FAST_TO_SLOW_THRESHOLD = 60;
176: 
177: 	// If the table has less than 204800 rows, this is the percentage
178: 	// of values we save when serializing/returning a sample.
179: 	constexpr static double SAVE_PERCENTAGE = 0.01;
180: 
181: 	ReservoirSample(Allocator &allocator, idx_t sample_count, int64_t seed = 1);
182: 	explicit ReservoirSample(idx_t sample_count, unique_ptr<ReservoirChunk> = nullptr);
183: 
184: 	//! methods used to help with serializing and deserializing
185: 	void EvictOverBudgetSamples();
186: 	void ExpandSerializedSample();
187: 
188: 	SamplingState GetSamplingState() const;
189: 
190: 	//! Vacuum the Reservoir Sample so it throws away tuples that are not in the
191: 	//! reservoir weights or in the selection vector
192: 	void Vacuum();
193: 
194: 	//! Transform To sample based on reservoir sampling paper
195: 	void ConvertToReservoirSample();
196: 
197: 	//! Get the capactiy of the data chunk reserved for storing samples
198: 	idx_t GetReservoirChunkCapacity() const;
199: 
200: 	//! If for_serialization=true then the sample_chunk is not padded with extra spaces for
201: 	//! future sampling values
202: 	unique_ptr<BlockingSample> Copy() const override;
203: 
204: 	//! create the first chunk called by AddToReservoir()
205: 	idx_t FillReservoir(DataChunk &chunk);
206: 	//! Add a chunk of data to the sample
207: 	void AddToReservoir(DataChunk &input) override;
208: 	//! Merge two Reservoir Samples. Other must be a reservoir sample
209: 	void Merge(unique_ptr<BlockingSample> other);
210: 
211: 	void ShuffleSel(SelectionVector &sel, idx_t range, idx_t size) const;
212: 
213: 	//! Update the sample by pushing new sample rows to the end of the sample_chunk.
214: 	//! The new sample rows are the tuples rows resulting from applying sel to other
215: 	void UpdateSampleAppend(DataChunk &this_, DataChunk &other, SelectionVector &other_sel, idx_t append_count) const;
216: 
217: 	idx_t GetTuplesSeen() const;
218: 	idx_t NumSamplesCollected() const;
219: 	idx_t GetActiveSampleCount() const;
220: 	static bool ValidSampleType(const LogicalType &type);
221: 
222: 	// get the chunk from Reservoir chunk
223: 	DataChunk &Chunk();
224: 
225: 	//! Fetches a chunk from the sample. Note that this method is destructive and should only be used after the
226: 	//! sample is completely built.
227: 	// unique_ptr<DataChunk> GetChunkAndDestroy() override;
228: 	unique_ptr<DataChunk> GetChunk() override;
229: 	void Destroy() override;
230: 	void Finalize() override;
231: 	void Verify();
232: 
233: 	idx_t GetSampleCount();
234: 
235: 	// map is [index in input chunk] -> [index in sample chunk]. Both are zero-based
236: 	// [index in sample chunk] is incremented by 1
237: 	// index in input chunk have random values, however, they are increasing.
238: 	// The base_reservoir_sampling gets updated however, so the indexes point to (sample_chunk_offset +
239: 	// index_in_sample_chunk) this data is used to make a selection vector to copy samples from the input chunk to the
240: 	// sample chunk
241: 	//! Get indexes from current sample that can be replaced.
242: 	SelectionVectorHelper GetReplacementIndexes(idx_t sample_chunk_offset, idx_t theoretical_chunk_length);
243: 
244: 	void Serialize(Serializer &serializer) const override;
245: 	static unique_ptr<BlockingSample> Deserialize(Deserializer &deserializer);
246: 
247: private:
248: 	// when we serialize, we may have collected too many samples since we fill a standard vector size, then
249: 	// truncate if the table is still <=204800 values. The problem is, in our weights, we store indexes into
250: 	// the selection vector. If throw away values at selection vector index i = 5 , we need to update all indexes
251: 	// i > 5. Otherwise we will have indexes in the weights that are greater than the length of our sample.
252: 	void NormalizeWeights();
253: 
254: 	SelectionVectorHelper GetReplacementIndexesSlow(const idx_t sample_chunk_offset, const idx_t chunk_length);
255: 	SelectionVectorHelper GetReplacementIndexesFast(const idx_t sample_chunk_offset, const idx_t chunk_length);
256: 	void SimpleMerge(ReservoirSample &other);
257: 	void WeightedMerge(ReservoirSample &other_sample);
258: 
259: 	// Helper methods for Shrink().
260: 	// Shrink has different logic depending on if the Reservoir sample is still in
261: 	// "Random" mode or in "reservoir" mode. This function creates a new sample chunk
262: 	// to copy the old sample chunk into
263: 	unique_ptr<ReservoirChunk> CreateNewSampleChunk(vector<LogicalType> &types, idx_t size) const;
264: 
265: 	// Get a vector where each index is a random int in the range 0, size.
266: 	// This is used to shuffle selection vector indexes
267: 	vector<uint32_t> GetRandomizedVector(uint32_t range, uint32_t size) const;
268: 
269: 	idx_t sample_count;
270: 	Allocator &allocator;
271: 	unique_ptr<ReservoirChunk> reservoir_chunk;
272: 	bool stats_sample;
273: 	SelectionVector sel;
274: 	idx_t sel_size;
275: };
276: 
277: //! The reservoir sample sample_size class maintains a streaming sample of variable size
278: class ReservoirSamplePercentage : public BlockingSample {
279: 	constexpr static idx_t RESERVOIR_THRESHOLD = 100000;
280: 
281: public:
282: 	static constexpr const SampleType TYPE = SampleType::RESERVOIR_PERCENTAGE_SAMPLE;
283: 
284: 	ReservoirSamplePercentage(Allocator &allocator, double percentage, int64_t seed = -1);
285: 	ReservoirSamplePercentage(double percentage, int64_t seed, idx_t reservoir_sample_size);
286: 	explicit ReservoirSamplePercentage(double percentage, int64_t seed = -1);
287: 
288: 	//! Add a chunk of data to the sample
289: 	void AddToReservoir(DataChunk &input) override;
290: 
291: 	unique_ptr<BlockingSample> Copy() const override;
292: 
293: 	//! Fetches a chunk from the sample. If destory = true this method is descructive
294: 	unique_ptr<DataChunk> GetChunk() override;
295: 	void Finalize() override;
296: 
297: 	void Serialize(Serializer &serializer) const override;
298: 	static unique_ptr<BlockingSample> Deserialize(Deserializer &deserializer);
299: 
300: private:
301: 	Allocator &allocator;
302: 	//! The sample_size to sample
303: 	double sample_percentage;
304: 	//! The fixed sample size of the sub-reservoirs
305: 	idx_t reservoir_sample_size;
306: 
307: 	//! The current sample
308: 	unique_ptr<ReservoirSample> current_sample;
309: 
310: 	//! The set of finished samples of the reservoir sample
311: 	vector<unique_ptr<ReservoirSample>> finished_samples;
312: 
313: 	//! The amount of tuples that have been processed so far (not put in the reservoir, just processed)
314: 	idx_t current_count = 0;
315: 	//! Whether or not the stream is finalized. The stream is automatically finalized on the first call to
316: 	//! GetChunkAndShrink();
317: 	bool is_finalized;
318: };
319: 
320: } // namespace duckdb
[end of src/include/duckdb/execution/reservoir_sample.hpp]
[start of src/include/duckdb/storage/data_table.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/data_table.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/enums/index_constraint_type.hpp"
12: #include "duckdb/common/types/data_chunk.hpp"
13: #include "duckdb/common/unique_ptr.hpp"
14: #include "duckdb/storage/index.hpp"
15: #include "duckdb/storage/statistics/column_statistics.hpp"
16: #include "duckdb/storage/table/column_segment.hpp"
17: #include "duckdb/storage/table/data_table_info.hpp"
18: #include "duckdb/storage/table/persistent_table_data.hpp"
19: #include "duckdb/storage/table/row_group.hpp"
20: #include "duckdb/storage/table/row_group_collection.hpp"
21: #include "duckdb/storage/table/table_statistics.hpp"
22: #include "duckdb/transaction/local_storage.hpp"
23: 
24: namespace duckdb {
25: 
26: class BoundForeignKeyConstraint;
27: class ClientContext;
28: class ColumnDataCollection;
29: class ColumnDefinition;
30: class DataTable;
31: class DuckTransaction;
32: class OptimisticDataWriter;
33: class RowGroup;
34: class StorageManager;
35: class TableCatalogEntry;
36: class TableIOManager;
37: class Transaction;
38: class WriteAheadLog;
39: class TableDataWriter;
40: class ConflictManager;
41: class TableScanState;
42: struct TableDeleteState;
43: struct ConstraintState;
44: struct TableUpdateState;
45: enum class VerifyExistenceType : uint8_t;
46: 
47: //! DataTable represents a physical table on disk
48: class DataTable {
49: public:
50: 	//! Constructs a new data table from an (optional) set of persistent segments
51: 	DataTable(AttachedDatabase &db, shared_ptr<TableIOManager> table_io_manager, const string &schema,
52: 	          const string &table, vector<ColumnDefinition> column_definitions_p,
53: 	          unique_ptr<PersistentTableData> data = nullptr);
54: 	//! Constructs a DataTable as a delta on an existing data table with a newly added column
55: 	DataTable(ClientContext &context, DataTable &parent, ColumnDefinition &new_column, Expression &default_value);
56: 	//! Constructs a DataTable as a delta on an existing data table but with one column removed
57: 	DataTable(ClientContext &context, DataTable &parent, idx_t removed_column);
58: 	//! Constructs a DataTable as a delta on an existing data table but with one column changed type
59: 	DataTable(ClientContext &context, DataTable &parent, idx_t changed_idx, const LogicalType &target_type,
60: 	          const vector<StorageIndex> &bound_columns, Expression &cast_expr);
61: 	//! Constructs a DataTable as a delta on an existing data table but with one column added new constraint
62: 	DataTable(ClientContext &context, DataTable &parent, BoundConstraint &constraint);
63: 
64: 	//! A reference to the database instance
65: 	AttachedDatabase &db;
66: 
67: public:
68: 	AttachedDatabase &GetAttached();
69: 	TableIOManager &GetTableIOManager();
70: 
71: 	bool IsTemporary() const;
72: 
73: 	//! Returns a list of types of the table
74: 	vector<LogicalType> GetTypes();
75: 	const vector<ColumnDefinition> &Columns() const;
76: 
77: 	void InitializeScan(DuckTransaction &transaction, TableScanState &state, const vector<StorageIndex> &column_ids,
78: 	                    TableFilterSet *table_filters = nullptr);
79: 
80: 	//! Returns the maximum amount of threads that should be assigned to scan this data table
81: 	idx_t MaxThreads(ClientContext &context) const;
82: 	void InitializeParallelScan(ClientContext &context, ParallelTableScanState &state);
83: 	bool NextParallelScan(ClientContext &context, ParallelTableScanState &state, TableScanState &scan_state);
84: 
85: 	//! Scans up to STANDARD_VECTOR_SIZE elements from the table starting
86: 	//! from offset and store them in result. Offset is incremented with how many
87: 	//! elements were returned.
88: 	//! Returns true if all pushed down filters were executed during data fetching
89: 	void Scan(DuckTransaction &transaction, DataChunk &result, TableScanState &state);
90: 
91: 	//! Fetch data from the specific row identifiers from the base table
92: 	void Fetch(DuckTransaction &transaction, DataChunk &result, const vector<StorageIndex> &column_ids,
93: 	           const Vector &row_ids, idx_t fetch_count, ColumnFetchState &state);
94: 
95: 	//! Initializes appending to transaction-local storage
96: 	void InitializeLocalAppend(LocalAppendState &state, TableCatalogEntry &table, ClientContext &context,
97: 	                           const vector<unique_ptr<BoundConstraint>> &bound_constraints);
98: 	//! Initializes only the delete-indexes of the transaction-local storage
99: 	void InitializeLocalStorage(LocalAppendState &state, TableCatalogEntry &table, ClientContext &context,
100: 	                            const vector<unique_ptr<BoundConstraint>> &bound_constraints);
101: 	//! Append a DataChunk to the transaction-local storage of the table.
102: 	void LocalAppend(LocalAppendState &state, ClientContext &context, DataChunk &chunk, bool unsafe);
103: 	//! Finalizes a transaction-local append
104: 	void FinalizeLocalAppend(LocalAppendState &state);
105: 	//! Append a chunk to the transaction-local storage of this table and update the delete indexes.
106: 	void LocalAppend(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,
107: 	                 const vector<unique_ptr<BoundConstraint>> &bound_constraints, Vector &row_ids,
108: 	                 DataChunk &delete_chunk);
109: 	//! Append a chunk to the transaction-local storage of this table.
110: 	void LocalAppend(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,
111: 	                 const vector<unique_ptr<BoundConstraint>> &bound_constraints);
112: 	//! Append a column data collection with default values to the transaction-local storage of this table.
113: 	void LocalAppend(TableCatalogEntry &table, ClientContext &context, ColumnDataCollection &collection,
114: 	                 const vector<unique_ptr<BoundConstraint>> &bound_constraints,
115: 	                 optional_ptr<const vector<LogicalIndex>> column_ids);
116: 	//! Merge a row group collection into the transaction-local storage
117: 	void LocalMerge(ClientContext &context, RowGroupCollection &collection);
118: 	//! Creates an optimistic writer for this table - used for optimistically writing parallel appends
119: 	OptimisticDataWriter &CreateOptimisticWriter(ClientContext &context);
120: 	void FinalizeOptimisticWriter(ClientContext &context, OptimisticDataWriter &writer);
121: 
122: 	unique_ptr<TableDeleteState> InitializeDelete(TableCatalogEntry &table, ClientContext &context,
123: 	                                              const vector<unique_ptr<BoundConstraint>> &bound_constraints);
124: 	//! Delete the entries with the specified row identifier from the table
125: 	idx_t Delete(TableDeleteState &state, ClientContext &context, Vector &row_ids, idx_t count);
126: 
127: 	unique_ptr<TableUpdateState> InitializeUpdate(TableCatalogEntry &table, ClientContext &context,
128: 	                                              const vector<unique_ptr<BoundConstraint>> &bound_constraints);
129: 	//! Update the entries with the specified row identifier from the table
130: 	void Update(TableUpdateState &state, ClientContext &context, Vector &row_ids,
131: 	            const vector<PhysicalIndex> &column_ids, DataChunk &data);
132: 	//! Update a single (sub-)column along a column path
133: 	//! The column_path vector is a *path* towards a column within the table
134: 	//! i.e. if we have a table with a single column S STRUCT(A INT, B INT)
135: 	//! and we update the validity mask of "S.B"
136: 	//! the column path is:
137: 	//! 0 (first column of table)
138: 	//! -> 1 (second subcolumn of struct)
139: 	//! -> 0 (first subcolumn of INT)
140: 	//! This method should only be used from the WAL replay. It does not verify update constraints.
141: 	void UpdateColumn(TableCatalogEntry &table, ClientContext &context, Vector &row_ids,
142: 	                  const vector<column_t> &column_path, DataChunk &updates);
143: 
144: 	//! Fetches an append lock
145: 	void AppendLock(TableAppendState &state);
146: 	//! Begin appending structs to this table, obtaining necessary locks, etc
147: 	void InitializeAppend(DuckTransaction &transaction, TableAppendState &state);
148: 	//! Append a chunk to the table using the AppendState obtained from InitializeAppend
149: 	void Append(DataChunk &chunk, TableAppendState &state);
150: 	//! Finalize an append
151: 	void FinalizeAppend(DuckTransaction &transaction, TableAppendState &state);
152: 	//! Commit the append
153: 	void CommitAppend(transaction_t commit_id, idx_t row_start, idx_t count);
154: 	//! Write a segment of the table to the WAL
155: 	void WriteToLog(DuckTransaction &transaction, WriteAheadLog &log, idx_t row_start, idx_t count,
156: 	                optional_ptr<StorageCommitState> commit_state);
157: 	//! Revert a set of appends made by the given AppendState, used to revert appends in the event of an error during
158: 	//! commit (e.g. because of an I/O exception)
159: 	void RevertAppend(DuckTransaction &transaction, idx_t start_row, idx_t count);
160: 	void RevertAppendInternal(idx_t start_row);
161: 
162: 	void ScanTableSegment(DuckTransaction &transaction, idx_t start_row, idx_t count,
163: 	                      const std::function<void(DataChunk &chunk)> &function);
164: 
165: 	//! Merge a row group collection directly into this table - appending it to the end of the table without copying
166: 	void MergeStorage(RowGroupCollection &data, TableIndexList &indexes, optional_ptr<StorageCommitState> commit_state);
167: 
168: 	//! Append a chunk with the row ids [row_start, ..., row_start + chunk.size()] to all indexes of the table.
169: 	//! Returns empty ErrorData, if the append was successful.
170: 	ErrorData AppendToIndexes(optional_ptr<TableIndexList> delete_indexes, DataChunk &chunk, row_t row_start);
171: 	static ErrorData AppendToIndexes(TableIndexList &indexes, optional_ptr<TableIndexList> delete_indexes,
172: 	                                 DataChunk &chunk, row_t row_start);
173: 	//! Remove a chunk with the row ids [row_start, ..., row_start + chunk.size()] from all indexes of the table
174: 	void RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start);
175: 	//! Remove the chunk with the specified set of row identifiers from all indexes of the table
176: 	void RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, Vector &row_identifiers);
177: 	//! Remove the row identifiers from all the indexes of the table
178: 	void RemoveFromIndexes(Vector &row_identifiers, idx_t count);
179: 
180: 	void SetAsRoot() {
181: 		this->is_root = true;
182: 	}
183: 
184: 	bool IsRoot() {
185: 		return this->is_root;
186: 	}
187: 
188: 	//! Get statistics of a physical column within the table
189: 	unique_ptr<BaseStatistics> GetStatistics(ClientContext &context, column_t column_id);
190: 
191: 	//! Get table sample
192: 	unique_ptr<BlockingSample> GetSample();
193: 	//! Sets statistics of a physical column within the table
194: 	void SetDistinct(column_t column_id, unique_ptr<DistinctStatistics> distinct_stats);
195: 
196: 	//! Obtains a shared lock to prevent checkpointing while operations are running
197: 	unique_ptr<StorageLockKey> GetSharedCheckpointLock();
198: 	//! Obtains a lock during a checkpoint operation that prevents other threads from reading this table
199: 	unique_ptr<StorageLockKey> GetCheckpointLock();
200: 	//! Checkpoint the table to the specified table data writer
201: 	void Checkpoint(TableDataWriter &writer, Serializer &serializer);
202: 	void CommitDropTable();
203: 	void CommitDropColumn(idx_t index);
204: 
205: 	idx_t ColumnCount() const;
206: 	idx_t GetTotalRows() const;
207: 
208: 	vector<ColumnSegmentInfo> GetColumnSegmentInfo();
209: 
210: 	//! Scans the next chunk for the CREATE INDEX operator
211: 	bool CreateIndexScan(TableScanState &state, DataChunk &result, TableScanType type);
212: 	//! Returns true, if the index name is unique (i.e., no PK, UNIQUE, FK constraint has the same name)
213: 	//! FIXME: This is only necessary until we treat all indexes as catalog entries, allowing to alter constraints
214: 	bool IndexNameIsUnique(const string &name);
215: 
216: 	//! Initialize constraint verification state
217: 	unique_ptr<ConstraintState> InitializeConstraintState(TableCatalogEntry &table,
218: 	                                                      const vector<unique_ptr<BoundConstraint>> &bound_constraints);
219: 	//! Verify constraints with a chunk from the Append containing all columns of the table
220: 	void VerifyAppendConstraints(ConstraintState &constraint_state, ClientContext &context, DataChunk &chunk,
221: 	                             optional_ptr<LocalTableStorage> local_storage, optional_ptr<ConflictManager> manager);
222: 
223: 	shared_ptr<DataTableInfo> &GetDataTableInfo();
224: 
225: 	void InitializeIndexes(ClientContext &context);
226: 	bool HasIndexes() const;
227: 	bool HasUniqueIndexes() const;
228: 	bool HasForeignKeyIndex(const vector<PhysicalIndex> &keys, ForeignKeyType type);
229: 	void SetIndexStorageInfo(vector<IndexStorageInfo> index_storage_info);
230: 	void VacuumIndexes();
231: 	void CleanupAppend(transaction_t lowest_transaction, idx_t start, idx_t count);
232: 
233: 	string GetTableName() const;
234: 	void SetTableName(string new_name);
235: 
236: 	TableStorageInfo GetStorageInfo();
237: 
238: 	idx_t GetRowGroupSize() const;
239: 
240: 	static void VerifyUniqueIndexes(TableIndexList &indexes, optional_ptr<LocalTableStorage> storage, DataChunk &chunk,
241: 	                                optional_ptr<ConflictManager> manager);
242: 
243: 	//! AddIndex initializes an index and adds it to the table's index list.
244: 	//! It is either empty, or initialized via its index storage information.
245: 	void AddIndex(const ColumnList &columns, const vector<LogicalIndex> &column_indexes, const IndexConstraintType type,
246: 	              const IndexStorageInfo &info);
247: 	//! AddIndex moves an index to this table's index list.
248: 	void AddIndex(unique_ptr<Index> index);
249: 
250: 	//! Returns a list of the partition stats
251: 	vector<PartitionStatistics> GetPartitionStats(ClientContext &context);
252: 
253: private:
254: 	//! Verify the new added constraints against current persistent&local data
255: 	void VerifyNewConstraint(LocalStorage &local_storage, DataTable &parent, const BoundConstraint &constraint);
256: 
257: 	//! Verify constraints with a chunk from the Update containing only the specified column_ids
258: 	void VerifyUpdateConstraints(ConstraintState &state, ClientContext &context, DataChunk &chunk,
259: 	                             const vector<PhysicalIndex> &column_ids);
260: 	//! Verify constraints with a chunk from the Delete containing all columns of the table
261: 	void VerifyDeleteConstraints(optional_ptr<LocalTableStorage> storage, TableDeleteState &state,
262: 	                             ClientContext &context, DataChunk &chunk);
263: 
264: 	void InitializeScanWithOffset(DuckTransaction &transaction, TableScanState &state,
265: 	                              const vector<StorageIndex> &column_ids, idx_t start_row, idx_t end_row);
266: 
267: 	void VerifyForeignKeyConstraint(optional_ptr<LocalTableStorage> storage,
268: 	                                const BoundForeignKeyConstraint &bound_foreign_key, ClientContext &context,
269: 	                                DataChunk &chunk, VerifyExistenceType type);
270: 	void VerifyAppendForeignKeyConstraint(optional_ptr<LocalTableStorage> storage,
271: 	                                      const BoundForeignKeyConstraint &bound_foreign_key, ClientContext &context,
272: 	                                      DataChunk &chunk);
273: 	void VerifyDeleteForeignKeyConstraint(optional_ptr<LocalTableStorage> storage,
274: 	                                      const BoundForeignKeyConstraint &bound_foreign_key, ClientContext &context,
275: 	                                      DataChunk &chunk);
276: 
277: private:
278: 	//! The table info
279: 	shared_ptr<DataTableInfo> info;
280: 	//! The set of physical columns stored by this DataTable
281: 	vector<ColumnDefinition> column_definitions;
282: 	//! Lock for appending entries to the table
283: 	mutex append_lock;
284: 	//! The row groups of the table
285: 	shared_ptr<RowGroupCollection> row_groups;
286: 	//! Whether or not the data table is the root DataTable for this table; the root DataTable is the newest version
287: 	//! that can be appended to
288: 	atomic<bool> is_root;
289: };
290: } // namespace duckdb
[end of src/include/duckdb/storage/data_table.hpp]
[start of src/include/duckdb/transaction/local_storage.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/transaction/local_storage.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/storage/table/row_group_collection.hpp"
12: #include "duckdb/storage/table/table_index_list.hpp"
13: #include "duckdb/storage/table/table_statistics.hpp"
14: #include "duckdb/storage/optimistic_data_writer.hpp"
15: #include "duckdb/common/error_data.hpp"
16: #include "duckdb/common/reference_map.hpp"
17: 
18: namespace duckdb {
19: class AttachedDatabase;
20: class Catalog;
21: class DataTable;
22: class StorageCommitState;
23: class Transaction;
24: class WriteAheadLog;
25: struct LocalAppendState;
26: struct TableAppendState;
27: 
28: class LocalTableStorage : public enable_shared_from_this<LocalTableStorage> {
29: public:
30: 	// Create a new LocalTableStorage
31: 	explicit LocalTableStorage(ClientContext &context, DataTable &table);
32: 	// Create a LocalTableStorage from an ALTER TYPE
33: 	LocalTableStorage(ClientContext &context, DataTable &table, LocalTableStorage &parent, idx_t changed_idx,
34: 	                  const LogicalType &target_type, const vector<StorageIndex> &bound_columns, Expression &cast_expr);
35: 	// Create a LocalTableStorage from a DROP COLUMN
36: 	LocalTableStorage(DataTable &table, LocalTableStorage &parent, idx_t drop_idx);
37: 	// Create a LocalTableStorage from an ADD COLUMN
38: 	LocalTableStorage(ClientContext &context, DataTable &table, LocalTableStorage &parent, ColumnDefinition &new_column,
39: 	                  ExpressionExecutor &default_executor);
40: 	~LocalTableStorage();
41: 
42: 	reference<DataTable> table_ref;
43: 
44: 	Allocator &allocator;
45: 	//! The main chunk collection holding the data
46: 	shared_ptr<RowGroupCollection> row_groups;
47: 	//! The set of unique append indexes.
48: 	TableIndexList append_indexes;
49: 	//! The set of delete indexes.
50: 	TableIndexList delete_indexes;
51: 	//! The number of deleted rows
52: 	idx_t deleted_rows;
53: 	//! The main optimistic data writer
54: 	OptimisticDataWriter optimistic_writer;
55: 	//! The set of all optimistic data writers associated with this table
56: 	vector<unique_ptr<OptimisticDataWriter>> optimistic_writers;
57: 	//! Whether or not storage was merged
58: 	bool merged_storage = false;
59: 	//! Whether or not the storage was dropped
60: 	bool is_dropped = false;
61: 
62: public:
63: 	void InitializeScan(CollectionScanState &state, optional_ptr<TableFilterSet> table_filters = nullptr);
64: 	//! Write a new row group to disk (if possible)
65: 	void WriteNewRowGroup();
66: 	void FlushBlocks();
67: 	void Rollback();
68: 	idx_t EstimatedSize();
69: 
70: 	void AppendToIndexes(DuckTransaction &transaction, TableAppendState &append_state, bool append_to_table);
71: 	ErrorData AppendToIndexes(DuckTransaction &transaction, RowGroupCollection &source, TableIndexList &index_list,
72: 	                          const vector<LogicalType> &table_types, row_t &start_row);
73: 	void AppendToDeleteIndexes(Vector &row_ids, DataChunk &delete_chunk);
74: 
75: 	//! Creates an optimistic writer for this table
76: 	OptimisticDataWriter &CreateOptimisticWriter();
77: 	void FinalizeOptimisticWriter(OptimisticDataWriter &writer);
78: };
79: 
80: class LocalTableManager {
81: public:
82: 	shared_ptr<LocalTableStorage> MoveEntry(DataTable &table);
83: 	reference_map_t<DataTable, shared_ptr<LocalTableStorage>> MoveEntries();
84: 	optional_ptr<LocalTableStorage> GetStorage(DataTable &table) const;
85: 	LocalTableStorage &GetOrCreateStorage(ClientContext &context, DataTable &table);
86: 	idx_t EstimatedSize() const;
87: 	bool IsEmpty() const;
88: 	void InsertEntry(DataTable &table, shared_ptr<LocalTableStorage> entry);
89: 
90: private:
91: 	mutable mutex table_storage_lock;
92: 	reference_map_t<DataTable, shared_ptr<LocalTableStorage>> table_storage;
93: };
94: 
95: //! The LocalStorage class holds appends that have not been committed yet
96: class LocalStorage {
97: public:
98: 	struct CommitState {
99: 		CommitState();
100: 		~CommitState();
101: 
102: 		reference_map_t<DataTable, unique_ptr<TableAppendState>> append_states;
103: 	};
104: 
105: public:
106: 	explicit LocalStorage(ClientContext &context, DuckTransaction &transaction);
107: 
108: 	static LocalStorage &Get(DuckTransaction &transaction);
109: 	static LocalStorage &Get(ClientContext &context, AttachedDatabase &db);
110: 	static LocalStorage &Get(ClientContext &context, Catalog &catalog);
111: 
112: 	//! Initialize a scan of the local storage
113: 	void InitializeScan(DataTable &table, CollectionScanState &state, optional_ptr<TableFilterSet> table_filters);
114: 	//! Scan
115: 	void Scan(CollectionScanState &state, const vector<StorageIndex> &column_ids, DataChunk &result);
116: 
117: 	void InitializeParallelScan(DataTable &table, ParallelCollectionScanState &state);
118: 	bool NextParallelScan(ClientContext &context, DataTable &table, ParallelCollectionScanState &state,
119: 	                      CollectionScanState &scan_state);
120: 
121: 	//! Begin appending to the local storage
122: 	void InitializeAppend(LocalAppendState &state, DataTable &table);
123: 	//! Initialize the storage and its indexes, but no row groups.
124: 	void InitializeStorage(LocalAppendState &state, DataTable &table);
125: 	//! Append a chunk to the local storage
126: 	static void Append(LocalAppendState &state, DataChunk &chunk);
127: 	//! Finish appending to the local storage
128: 	static void FinalizeAppend(LocalAppendState &state);
129: 	//! Merge a row group collection into the transaction-local storage
130: 	void LocalMerge(DataTable &table, RowGroupCollection &collection);
131: 	//! Create an optimistic writer for the specified table
132: 	OptimisticDataWriter &CreateOptimisticWriter(DataTable &table);
133: 	void FinalizeOptimisticWriter(DataTable &table, OptimisticDataWriter &writer);
134: 
135: 	//! Delete a set of rows from the local storage
136: 	idx_t Delete(DataTable &table, Vector &row_ids, idx_t count);
137: 	//! Update a set of rows in the local storage
138: 	void Update(DataTable &table, Vector &row_ids, const vector<PhysicalIndex> &column_ids, DataChunk &data);
139: 
140: 	//! Commits the local storage, writing it to the WAL and completing the commit
141: 	void Commit(optional_ptr<StorageCommitState> commit_state);
142: 	//! Rollback the local storage
143: 	void Rollback();
144: 
145: 	bool ChangesMade() noexcept;
146: 	idx_t EstimatedSize();
147: 
148: 	void DropTable(DataTable &table);
149: 	bool Find(DataTable &table);
150: 
151: 	idx_t AddedRows(DataTable &table);
152: 	vector<PartitionStatistics> GetPartitionStats(DataTable &table) const;
153: 
154: 	void AddColumn(DataTable &old_dt, DataTable &new_dt, ColumnDefinition &new_column,
155: 	               ExpressionExecutor &default_executor);
156: 	void DropColumn(DataTable &old_dt, DataTable &new_dt, idx_t removed_column);
157: 	void ChangeType(DataTable &old_dt, DataTable &new_dt, idx_t changed_idx, const LogicalType &target_type,
158: 	                const vector<StorageIndex> &bound_columns, Expression &cast_expr);
159: 
160: 	void MoveStorage(DataTable &old_dt, DataTable &new_dt);
161: 	void FetchChunk(DataTable &table, Vector &row_ids, idx_t count, const vector<StorageIndex> &col_ids,
162: 	                DataChunk &chunk, ColumnFetchState &fetch_state);
163: 	TableIndexList &GetIndexes(DataTable &table);
164: 	optional_ptr<LocalTableStorage> GetStorage(DataTable &table);
165: 
166: 	void VerifyNewConstraint(DataTable &parent, const BoundConstraint &constraint);
167: 
168: private:
169: 	ClientContext &context;
170: 	DuckTransaction &transaction;
171: 	LocalTableManager table_manager;
172: 
173: 	void Flush(DataTable &table, LocalTableStorage &storage, optional_ptr<StorageCommitState> commit_state);
174: };
175: 
176: } // namespace duckdb
[end of src/include/duckdb/transaction/local_storage.hpp]
[start of src/main/extension/extension_helper.cpp]
1: #include "duckdb/main/extension_helper.hpp"
2: 
3: #include "duckdb/common/file_system.hpp"
4: #include "duckdb/common/serializer/binary_deserializer.hpp"
5: #include "duckdb/common/serializer/buffered_file_reader.hpp"
6: #include "duckdb/common/string_util.hpp"
7: #include "duckdb/common/windows.hpp"
8: #include "duckdb/main/client_context.hpp"
9: #include "duckdb/main/database.hpp"
10: #include "duckdb/main/extension.hpp"
11: #include "duckdb/main/extension_install_info.hpp"
12: 
13: // Note that c++ preprocessor doesn't have a nice way to clean this up so we need to set the defines we use to false
14: // explicitly when they are undefined
15: #ifndef DUCKDB_EXTENSION_CORE_FUNCTIONS_LINKED
16: #define DUCKDB_EXTENSION_CORE_FUNCTIONS_LINKED false
17: #endif
18: 
19: #ifndef DUCKDB_EXTENSION_ICU_LINKED
20: #define DUCKDB_EXTENSION_ICU_LINKED false
21: #endif
22: 
23: #ifndef DUCKDB_EXTENSION_EXCEL_LINKED
24: #define DUCKDB_EXTENSION_EXCEL_LINKED false
25: #endif
26: 
27: #ifndef DUCKDB_EXTENSION_PARQUET_LINKED
28: #define DUCKDB_EXTENSION_PARQUET_LINKED false
29: #endif
30: 
31: #ifndef DUCKDB_EXTENSION_TPCH_LINKED
32: #define DUCKDB_EXTENSION_TPCH_LINKED false
33: #endif
34: 
35: #ifndef DUCKDB_EXTENSION_TPCDS_LINKED
36: #define DUCKDB_EXTENSION_TPCDS_LINKED false
37: #endif
38: 
39: #ifndef DUCKDB_EXTENSION_HTTPFS_LINKED
40: #define DUCKDB_EXTENSION_HTTPFS_LINKED false
41: #endif
42: 
43: #ifndef DUCKDB_EXTENSION_JSON_LINKED
44: #define DUCKDB_EXTENSION_JSON_LINKED false
45: #endif
46: 
47: #ifndef DUCKDB_EXTENSION_JEMALLOC_LINKED
48: #define DUCKDB_EXTENSION_JEMALLOC_LINKED false
49: #endif
50: 
51: #ifndef DUCKDB_EXTENSION_AUTOCOMPLETE_LINKED
52: #define DUCKDB_EXTENSION_AUTOCOMPLETE_LINKED false
53: #endif
54: 
55: // Load the generated header file containing our list of extension headers
56: #if defined(GENERATED_EXTENSION_HEADERS) && GENERATED_EXTENSION_HEADERS && !defined(DUCKDB_AMALGAMATION)
57: #include "duckdb/main/extension/generated_extension_loader.hpp"
58: #else
59: // TODO: rewrite package_build.py to allow also loading out-of-tree extensions in non-cmake builds, after that
60: //		 these can be removed
61: #if DUCKDB_EXTENSION_CORE_FUNCTIONS_LINKED
62: #include "core_functions_extension.hpp"
63: #endif
64: 
65: #if DUCKDB_EXTENSION_ICU_LINKED
66: #include "icu_extension.hpp"
67: #endif
68: 
69: #if DUCKDB_EXTENSION_PARQUET_LINKED
70: #include "parquet_extension.hpp"
71: #endif
72: 
73: #if DUCKDB_EXTENSION_TPCH_LINKED
74: #include "tpch_extension.hpp"
75: #endif
76: 
77: #if DUCKDB_EXTENSION_TPCDS_LINKED
78: #include "tpcds_extension.hpp"
79: #endif
80: 
81: #if DUCKDB_EXTENSION_JSON_LINKED
82: #include "json_extension.hpp"
83: #endif
84: 
85: #if DUCKDB_EXTENSION_JEMALLOC_LINKED
86: #include "jemalloc_extension.hpp"
87: #endif
88: 
89: #if DUCKDB_EXTENSION_AUTOCOMPLETE_LINKED
90: #include "autocomplete_extension.hpp"
91: #endif
92: 
93: #endif
94: 
95: namespace duckdb {
96: 
97: //===--------------------------------------------------------------------===//
98: // Default Extensions
99: //===--------------------------------------------------------------------===//
100: static const DefaultExtension internal_extensions[] = {
101:     {"core_functions", "Core function library", DUCKDB_EXTENSION_CORE_FUNCTIONS_LINKED},
102:     {"icu", "Adds support for time zones and collations using the ICU library", DUCKDB_EXTENSION_ICU_LINKED},
103:     {"excel", "Adds support for Excel-like format strings", DUCKDB_EXTENSION_EXCEL_LINKED},
104:     {"parquet", "Adds support for reading and writing parquet files", DUCKDB_EXTENSION_PARQUET_LINKED},
105:     {"tpch", "Adds TPC-H data generation and query support", DUCKDB_EXTENSION_TPCH_LINKED},
106:     {"tpcds", "Adds TPC-DS data generation and query support", DUCKDB_EXTENSION_TPCDS_LINKED},
107:     {"httpfs", "Adds support for reading and writing files over a HTTP(S) connection", DUCKDB_EXTENSION_HTTPFS_LINKED},
108:     {"json", "Adds support for JSON operations", DUCKDB_EXTENSION_JSON_LINKED},
109:     {"jemalloc", "Overwrites system allocator with JEMalloc", DUCKDB_EXTENSION_JEMALLOC_LINKED},
110:     {"autocomplete", "Adds support for autocomplete in the shell", DUCKDB_EXTENSION_AUTOCOMPLETE_LINKED},
111:     {"motherduck", "Enables motherduck integration with the system", false},
112:     {"mysql_scanner", "Adds support for connecting to a MySQL database", false},
113:     {"sqlite_scanner", "Adds support for reading and writing SQLite database files", false},
114:     {"postgres_scanner", "Adds support for connecting to a Postgres database", false},
115:     {"inet", "Adds support for IP-related data types and functions", false},
116:     {"spatial", "Geospatial extension that adds support for working with spatial data and functions", false},
117:     {"aws", "Provides features that depend on the AWS SDK", false},
118:     {"arrow", "A zero-copy data integration between Apache Arrow and DuckDB", false},
119:     {"azure", "Adds a filesystem abstraction for Azure blob storage to DuckDB", false},
120:     {"iceberg", "Adds support for Apache Iceberg", false},
121:     {"vss", "Adds indexing support to accelerate Vector Similarity Search", false},
122:     {"delta", "Adds support for Delta Lake", false},
123:     {"fts", "Adds support for Full-Text Search Indexes", false},
124:     {nullptr, nullptr, false}};
125: 
126: idx_t ExtensionHelper::DefaultExtensionCount() {
127: 	idx_t index;
128: 	for (index = 0; internal_extensions[index].name != nullptr; index++) {
129: 	}
130: 	return index;
131: }
132: 
133: DefaultExtension ExtensionHelper::GetDefaultExtension(idx_t index) {
134: 	D_ASSERT(index < DefaultExtensionCount());
135: 	return internal_extensions[index];
136: }
137: 
138: //===--------------------------------------------------------------------===//
139: // Allow Auto-Install Extensions
140: //===--------------------------------------------------------------------===//
141: static const char *const auto_install[] = {"motherduck", "postgres_scanner", "mysql_scanner", "sqlite_scanner",
142:                                            "delta",      "iceberg",          "uc_catalog",    nullptr};
143: 
144: // TODO: unify with new autoload mechanism
145: bool ExtensionHelper::AllowAutoInstall(const string &extension) {
146: 	auto lcase = StringUtil::Lower(extension);
147: 	for (idx_t i = 0; auto_install[i]; i++) {
148: 		if (lcase == auto_install[i]) {
149: 			return true;
150: 		}
151: 	}
152: 	return false;
153: }
154: 
155: bool ExtensionHelper::CanAutoloadExtension(const string &ext_name) {
156: #ifdef DUCKDB_DISABLE_EXTENSION_LOAD
157: 	return false;
158: #endif
159: 
160: 	if (ext_name.empty()) {
161: 		return false;
162: 	}
163: 	for (const auto &ext : AUTOLOADABLE_EXTENSIONS) {
164: 		if (ext_name == ext) {
165: 			return true;
166: 		}
167: 	}
168: 	return false;
169: }
170: 
171: string ExtensionHelper::AddExtensionInstallHintToErrorMsg(ClientContext &context, const string &base_error,
172:                                                           const string &extension_name) {
173: 
174: 	return AddExtensionInstallHintToErrorMsg(DatabaseInstance::GetDatabase(context), base_error, extension_name);
175: }
176: string ExtensionHelper::AddExtensionInstallHintToErrorMsg(DatabaseInstance &db, const string &base_error,
177:                                                           const string &extension_name) {
178: 	string install_hint;
179: 
180: 	auto &config = db.config;
181: 
182: 	if (!ExtensionHelper::CanAutoloadExtension(extension_name)) {
183: 		install_hint = "Please try installing and loading the " + extension_name + " extension:\nINSTALL " +
184: 		               extension_name + ";\nLOAD " + extension_name + ";\n\n";
185: 	} else if (!config.options.autoload_known_extensions) {
186: 		install_hint =
187: 		    "Please try installing and loading the " + extension_name + " extension by running:\nINSTALL " +
188: 		    extension_name + ";\nLOAD " + extension_name +
189: 		    ";\n\nAlternatively, consider enabling auto-install "
190: 		    "and auto-load by running:\nSET autoinstall_known_extensions=1;\nSET autoload_known_extensions=1;";
191: 	} else if (!config.options.autoinstall_known_extensions) {
192: 		install_hint =
193: 		    "Please try installing the " + extension_name + " extension by running:\nINSTALL " + extension_name +
194: 		    ";\n\nAlternatively, consider enabling autoinstall by running:\nSET autoinstall_known_extensions=1;";
195: 	}
196: 
197: 	if (!install_hint.empty()) {
198: 		return base_error + "\n\n" + install_hint;
199: 	}
200: 
201: 	return base_error;
202: }
203: 
204: bool ExtensionHelper::TryAutoLoadExtension(ClientContext &context, const string &extension_name) noexcept {
205: 	if (context.db->ExtensionIsLoaded(extension_name)) {
206: 		return true;
207: 	}
208: 	auto &dbconfig = DBConfig::GetConfig(context);
209: 	try {
210: 		if (dbconfig.options.autoinstall_known_extensions) {
211: 			auto &config = DBConfig::GetConfig(context);
212: 			auto autoinstall_repo = ExtensionRepository::GetRepositoryByUrl(
213: 			    StringValue::Get(config.GetSetting<AutoinstallExtensionRepositorySetting>(context)));
214: 			ExtensionInstallOptions options;
215: 			options.repository = autoinstall_repo;
216: 			ExtensionHelper::InstallExtension(context, extension_name, options);
217: 		}
218: 		ExtensionHelper::LoadExternalExtension(context, extension_name);
219: 		return true;
220: 	} catch (...) {
221: 		return false;
222: 	}
223: }
224: 
225: bool ExtensionHelper::TryAutoLoadExtension(DatabaseInstance &instance, const string &extension_name) noexcept {
226: 	if (instance.ExtensionIsLoaded(extension_name)) {
227: 		return true;
228: 	}
229: 	auto &dbconfig = DBConfig::GetConfig(instance);
230: 	try {
231: 		auto &fs = FileSystem::GetFileSystem(instance);
232: 		if (dbconfig.options.autoinstall_known_extensions) {
233: 			auto autoinstall_repo =
234: 			    ExtensionRepository::GetRepositoryByUrl(dbconfig.options.autoinstall_extension_repo);
235: 			ExtensionInstallOptions options;
236: 			options.repository = autoinstall_repo;
237: 			ExtensionHelper::InstallExtension(instance, fs, extension_name, options);
238: 		}
239: 		ExtensionHelper::LoadExternalExtension(instance, fs, extension_name);
240: 		return true;
241: 	} catch (...) {
242: 		return false;
243: 	}
244: }
245: 
246: static ExtensionUpdateResult UpdateExtensionInternal(ClientContext &context, DatabaseInstance &db, FileSystem &fs,
247:                                                      const string &full_extension_path, const string &extension_name) {
248: 	ExtensionUpdateResult result;
249: 	result.extension_name = extension_name;
250: 
251: 	auto &config = DBConfig::GetConfig(db);
252: 
253: 	if (!fs.FileExists(full_extension_path)) {
254: 		result.tag = ExtensionUpdateResultTag::NOT_INSTALLED;
255: 		return result;
256: 	}
257: 
258: 	// Extension exists, check for .info file
259: 	const string info_file_path = full_extension_path + ".info";
260: 	if (!fs.FileExists(info_file_path)) {
261: 		result.tag = ExtensionUpdateResultTag::MISSING_INSTALL_INFO;
262: 		return result;
263: 	}
264: 
265: 	// Parse the version of the extension before updating
266: 	auto ext_binary_handle = fs.OpenFile(full_extension_path, FileOpenFlags::FILE_FLAGS_READ);
267: 	auto parsed_metadata = ExtensionHelper::ParseExtensionMetaData(*ext_binary_handle);
268: 	if (!parsed_metadata.AppearsValid() && !config.options.allow_extensions_metadata_mismatch) {
269: 		throw IOException(
270: 		    "Failed to update extension: '%s', the metadata of the extension appears invalid! To resolve this, either "
271: 		    "reinstall the extension using 'FORCE INSTALL %s', manually remove the file '%s', or enable '"
272: 		    "SET allow_extensions_metadata_mismatch=true'",
273: 		    extension_name, extension_name, full_extension_path);
274: 	}
275: 
276: 	result.prev_version = parsed_metadata.AppearsValid() ? parsed_metadata.extension_version : "";
277: 
278: 	auto extension_install_info = ExtensionInstallInfo::TryReadInfoFile(fs, info_file_path, extension_name);
279: 
280: 	// Early out: no info file found
281: 	if (extension_install_info->mode == ExtensionInstallMode::UNKNOWN) {
282: 		result.tag = ExtensionUpdateResultTag::MISSING_INSTALL_INFO;
283: 		return result;
284: 	}
285: 
286: 	// Early out: we can only update extensions from repositories
287: 	if (extension_install_info->mode != ExtensionInstallMode::REPOSITORY) {
288: 		result.tag = ExtensionUpdateResultTag::NOT_A_REPOSITORY;
289: 		result.installed_version = result.prev_version;
290: 		return result;
291: 	}
292: 
293: 	auto repository_from_info = ExtensionRepository::GetRepositoryByUrl(extension_install_info->repository_url);
294: 	result.repository = repository_from_info.ToReadableString();
295: 
296: 	// Force install the full url found in this file, enabling etags to ensure efficient updating
297: 	ExtensionInstallOptions options;
298: 	options.repository = repository_from_info;
299: 	options.force_install = true;
300: 	options.use_etags = true;
301: 
302: 	unique_ptr<ExtensionInstallInfo> install_result;
303: 	try {
304: 		install_result = ExtensionHelper::InstallExtension(context, extension_name, options);
305: 	} catch (std::exception &e) {
306: 		ErrorData error(e);
307: 		error.Throw("Extension updating failed when trying to install '" + extension_name + "', original error: ");
308: 	}
309: 
310: 	result.installed_version = install_result->version;
311: 
312: 	if (result.installed_version.empty()) {
313: 		result.tag = ExtensionUpdateResultTag::REDOWNLOADED;
314: 	} else if (result.installed_version != result.prev_version) {
315: 		result.tag = ExtensionUpdateResultTag::UPDATED;
316: 	} else {
317: 		result.tag = ExtensionUpdateResultTag::NO_UPDATE_AVAILABLE;
318: 	}
319: 
320: 	return result;
321: }
322: 
323: vector<ExtensionUpdateResult> ExtensionHelper::UpdateExtensions(ClientContext &context) {
324: 	auto &fs = FileSystem::GetFileSystem(context);
325: 
326: 	vector<ExtensionUpdateResult> result;
327: 	DatabaseInstance &db = DatabaseInstance::GetDatabase(context);
328: 
329: #ifndef WASM_LOADABLE_EXTENSIONS
330: 	case_insensitive_set_t seen_extensions;
331: 
332: 	// scan the install directory for installed extensions
333: 	auto ext_directory = ExtensionHelper::ExtensionDirectory(db, fs);
334: 	fs.ListFiles(ext_directory, [&](const string &path, bool is_directory) {
335: 		if (!StringUtil::EndsWith(path, ".duckdb_extension")) {
336: 			return;
337: 		}
338: 
339: 		auto extension_file_name = StringUtil::GetFileName(path);
340: 		auto extension_name = StringUtil::Split(extension_file_name, ".")[0];
341: 
342: 		seen_extensions.insert(extension_name);
343: 
344: 		result.push_back(UpdateExtensionInternal(context, db, fs, fs.JoinPath(ext_directory, path), extension_name));
345: 	});
346: #endif
347: 
348: 	return result;
349: }
350: 
351: ExtensionUpdateResult ExtensionHelper::UpdateExtension(ClientContext &context, const string &extension_name) {
352: 	auto &fs = FileSystem::GetFileSystem(context);
353: 	DatabaseInstance &db = DatabaseInstance::GetDatabase(context);
354: 	auto ext_directory = ExtensionHelper::ExtensionDirectory(db, fs);
355: 
356: 	auto full_extension_path = fs.JoinPath(ext_directory, extension_name + ".duckdb_extension");
357: 
358: 	auto update_result = UpdateExtensionInternal(context, db, fs, full_extension_path, extension_name);
359: 
360: 	if (update_result.tag == ExtensionUpdateResultTag::NOT_INSTALLED) {
361: 		throw InvalidInputException("Failed to update the extension '%s', the extension is not installed!",
362: 		                            extension_name);
363: 	} else if (update_result.tag == ExtensionUpdateResultTag::UNKNOWN) {
364: 		throw InternalException("Failed to update extension '%s', an unknown error occurred", extension_name);
365: 	}
366: 	return update_result;
367: }
368: 
369: void ExtensionHelper::AutoLoadExtension(ClientContext &context, const string &extension_name) {
370: 	return ExtensionHelper::AutoLoadExtension(*context.db, extension_name);
371: }
372: 
373: void ExtensionHelper::AutoLoadExtension(DatabaseInstance &db, const string &extension_name) {
374: 	if (db.ExtensionIsLoaded(extension_name)) {
375: 		// Avoid downloading again
376: 		return;
377: 	}
378: 	auto &dbconfig = DBConfig::GetConfig(db);
379: 	try {
380: 		auto fs = FileSystem::CreateLocal();
381: #ifndef DUCKDB_WASM
382: 		if (dbconfig.options.autoinstall_known_extensions) {
383: 			//! Get the autoloading repository
384: 			auto repository = ExtensionRepository::GetRepositoryByUrl(dbconfig.options.autoinstall_extension_repo);
385: 			ExtensionInstallOptions options;
386: 			options.repository = repository;
387: 			ExtensionHelper::InstallExtension(db, *fs, extension_name, options);
388: 		}
389: #endif
390: 		ExtensionHelper::LoadExternalExtension(db, *fs, extension_name);
391: 		DUCKDB_LOG_INFO(db, "duckdb.Extensions.ExtensionAutoloaded", extension_name);
392: 
393: 	} catch (std::exception &e) {
394: 		ErrorData error(e);
395: 		throw AutoloadException(extension_name, error.RawMessage());
396: 	}
397: }
398: 
399: //===--------------------------------------------------------------------===//
400: // Load Statically Compiled Extension
401: //===--------------------------------------------------------------------===//
402: void ExtensionHelper::LoadAllExtensions(DuckDB &db) {
403: 	// The in-tree extensions that we check. Non-cmake builds are currently limited to these for static linking
404: 	// TODO: rewrite package_build.py to allow also loading out-of-tree extensions in non-cmake builds, after that
405: 	//		 these can be removed
406: 	vector<string> extensions {"parquet", "icu",  "tpch",     "tpcds",        "httpfs",        "json",
407: 	                           "excel",   "inet", "jemalloc", "autocomplete", "core_functions"};
408: 	for (auto &ext : extensions) {
409: 		LoadExtensionInternal(db, ext, true);
410: 	}
411: 
412: #if defined(GENERATED_EXTENSION_HEADERS) && GENERATED_EXTENSION_HEADERS
413: 	for (const auto &ext : LinkedExtensions()) {
414: 		LoadExtensionInternal(db, ext, true);
415: 	}
416: #endif
417: }
418: 
419: ExtensionLoadResult ExtensionHelper::LoadExtension(DuckDB &db, const std::string &extension) {
420: 	return LoadExtensionInternal(db, extension, false);
421: }
422: 
423: ExtensionLoadResult ExtensionHelper::LoadExtensionInternal(DuckDB &db, const std::string &extension,
424:                                                            bool initial_load) {
425: #ifdef DUCKDB_TEST_REMOTE_INSTALL
426: 	if (!initial_load && StringUtil::Contains(DUCKDB_TEST_REMOTE_INSTALL, extension)) {
427: 		Connection con(db);
428: 		auto result = con.Query("INSTALL " + extension);
429: 		if (result->HasError()) {
430: 			result->Print();
431: 			return ExtensionLoadResult::EXTENSION_UNKNOWN;
432: 		}
433: 		result = con.Query("LOAD " + extension);
434: 		if (result->HasError()) {
435: 			result->Print();
436: 			return ExtensionLoadResult::EXTENSION_UNKNOWN;
437: 		}
438: 		return ExtensionLoadResult::LOADED_EXTENSION;
439: 	}
440: #endif
441: 
442: #ifdef DUCKDB_EXTENSIONS_TEST_WITH_LOADABLE
443: 	// Note: weird comma's are on purpose to do easy string contains on a list of extension names
444: 	if (!initial_load && StringUtil::Contains(DUCKDB_EXTENSIONS_TEST_WITH_LOADABLE, "," + extension + ",")) {
445: 		Connection con(db);
446: 		auto result = con.Query((string) "LOAD '" + DUCKDB_EXTENSIONS_BUILD_PATH + "/" + extension + "/" + extension +
447: 		                        ".duckdb_extension'");
448: 		if (result->HasError()) {
449: 			result->Print();
450: 			return ExtensionLoadResult::EXTENSION_UNKNOWN;
451: 		}
452: 		return ExtensionLoadResult::LOADED_EXTENSION;
453: 	}
454: #endif
455: 
456: 	// This is the main extension loading mechanism that loads the extension that are statically linked.
457: #if defined(GENERATED_EXTENSION_HEADERS) && GENERATED_EXTENSION_HEADERS
458: 	if (TryLoadLinkedExtension(db, extension)) {
459: 		return ExtensionLoadResult::LOADED_EXTENSION;
460: 	} else {
461: 		return ExtensionLoadResult::NOT_LOADED;
462: 	}
463: #endif
464: 
465: 	// This is the fallback to the "old" extension loading mechanism for non-cmake builds
466: 	// TODO: rewrite package_build.py to allow also loading out-of-tree extensions in non-cmake builds
467: 	if (extension == "parquet") {
468: #if DUCKDB_EXTENSION_PARQUET_LINKED
469: 		db.LoadStaticExtension<ParquetExtension>();
470: #else
471: 		// parquet extension required but not build: skip this test
472: 		return ExtensionLoadResult::NOT_LOADED;
473: #endif
474: 	} else if (extension == "icu") {
475: #if DUCKDB_EXTENSION_ICU_LINKED
476: 		db.LoadStaticExtension<IcuExtension>();
477: #else
478: 		// icu extension required but not build: skip this test
479: 		return ExtensionLoadResult::NOT_LOADED;
480: #endif
481: 	} else if (extension == "tpch") {
482: #if DUCKDB_EXTENSION_TPCH_LINKED
483: 		db.LoadStaticExtension<TpchExtension>();
484: #else
485: 		// icu extension required but not build: skip this test
486: 		return ExtensionLoadResult::NOT_LOADED;
487: #endif
488: 	} else if (extension == "tpcds") {
489: #if DUCKDB_EXTENSION_TPCDS_LINKED
490: 		db.LoadStaticExtension<TpcdsExtension>();
491: #else
492: 		// icu extension required but not build: skip this test
493: 		return ExtensionLoadResult::NOT_LOADED;
494: #endif
495: 	} else if (extension == "httpfs") {
496: #if DUCKDB_EXTENSION_HTTPFS_LINKED
497: 		db.LoadStaticExtension<HttpfsExtension>();
498: #else
499: 		return ExtensionLoadResult::NOT_LOADED;
500: #endif
501: 	} else if (extension == "json") {
502: #if DUCKDB_EXTENSION_JSON_LINKED
503: 		db.LoadStaticExtension<JsonExtension>();
504: #else
505: 		// json extension required but not build: skip this test
506: 		return ExtensionLoadResult::NOT_LOADED;
507: #endif
508: 	} else if (extension == "excel") {
509: #if DUCKDB_EXTENSION_EXCEL_LINKED
510: 		db.LoadStaticExtension<ExcelExtension>();
511: #else
512: 		// excel extension required but not build: skip this test
513: 		return ExtensionLoadResult::NOT_LOADED;
514: #endif
515: 	} else if (extension == "jemalloc") {
516: #if DUCKDB_EXTENSION_JEMALLOC_LINKED
517: 		db.LoadStaticExtension<JemallocExtension>();
518: #else
519: 		// jemalloc extension required but not build: skip this test
520: 		return ExtensionLoadResult::NOT_LOADED;
521: #endif
522: 	} else if (extension == "autocomplete") {
523: #if DUCKDB_EXTENSION_AUTOCOMPLETE_LINKED
524: 		db.LoadStaticExtension<AutocompleteExtension>();
525: #else
526: 		// autocomplete extension required but not build: skip this test
527: 		return ExtensionLoadResult::NOT_LOADED;
528: #endif
529: 	} else if (extension == "inet") {
530: #if DUCKDB_EXTENSION_INET_LINKED
531: 		db.LoadStaticExtension<InetExtension>();
532: #else
533: 		// inet extension required but not build: skip this test
534: 		return ExtensionLoadResult::NOT_LOADED;
535: #endif
536: 	} else if (extension == "core_functions") {
537: #if DUCKDB_EXTENSION_CORE_FUNCTIONS_LINKED
538: 		db.LoadStaticExtension<CoreFunctionsExtension>();
539: #else
540: 		// core_functions extension required but not build: skip this test
541: 		return ExtensionLoadResult::NOT_LOADED;
542: #endif
543: 	}
544: 
545: 	return ExtensionLoadResult::LOADED_EXTENSION;
546: }
547: 
548: static const char *const public_keys[] = {
549:     R"(
550: -----BEGIN PUBLIC KEY-----
551: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA6aZuHUa1cLR9YDDYaEfi
552: UDbWY8m2t7b71S+k1ZkXfHqu+5drAxm+dIDzdOHOKZSIdwnJbT3sSqwFoG6PlXF3
553: g3dsJjax5qESIhbVvf98nyipwNINxoyHCkcCIPkX17QP2xpnT7V59+CqcfDJXLqB
554: ymjqoFSlaH8dUCHybM4OXlWnAtVHW/nmw0khF8CetcWn4LxaTUHptByaBz8CasSs
555: gWpXgSfaHc3R9eArsYhtsVFGyL/DEWgkEHWolxY3Llenhgm/zOf3s7PsAMe7EJX4
556: qlSgiXE6OVBXnqd85z4k20lCw/LAOe5hoTMmRWXIj74MudWe2U91J6GrrGEZa7zT
557: 7QIDAQAB
558: -----END PUBLIC KEY-----
559: )",
560:     R"(
561: -----BEGIN PUBLIC KEY-----
562: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAq8Gg1S/LI6ApMAYsFc9m
563: PrkFIY+nc0LXSpxm77twU8D5M0Xkz/Av4f88DQmj1OE3164bEtR7sl7xDPZojFHj
564: YYyucJxEI97l5OU1d3Pc1BdKXL4+mnW5FlUGj218u8qD+G1hrkySXQkrUzIjPPNw
565: o6knF3G/xqQF+KI+tc7ajnTni8CAlnUSxfnstycqbVS86m238PLASVPK9/SmIRgO
566: XCEV+ZNMlerq8EwsW4cJPHH0oNVMcaG+QT4z79roW1rbJghn9ubAVdQU6VLUAikI
567: b8keUyY+D0XdY9DpDBeiorb1qPYt8BPLOAQrIUAw1CgpMM9KFp9TNvW47KcG4bcB
568: dQIDAQAB
569: -----END PUBLIC KEY-----
570: )",
571:     R"(
572: -----BEGIN PUBLIC KEY-----
573: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyYATA9KOQ0Azf97QAPfY
574: Jc/WeZyE4E1qlRgKWKqNtYSXZqk5At0V7w2ntAWtYSpczFrVepCJ0oPMDpZTigEr
575: NgOgfo5LEhPx5XmtCf62xY/xL3kgtfz9Mm5TBkuQy4KwY4z1npGr4NYYDXtF7kkf
576: LQE+FnD8Yr4E0wHBib7ey7aeeKWmwqvUjzDqG+TzaqwzO/RCUsSctqSS0t1oo2hv
577: 4q1ofanUXsV8MXk/ujtgxu7WkVvfiSpK1zRazgeZjcrQFO9qL/pla0vBUxa1U8He
578: GMLnL0oRfcMg7yKrbIMrvlEl2ZmiR9im44dXJWfY42quObwr1PuEkEoCMcMisSWl
579: jwIDAQAB
580: -----END PUBLIC KEY-----
581: )",
582:     R"(
583: -----BEGIN PUBLIC KEY-----
584: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4RvbWx3zLblDHH/lGUF5
585: Q512MT+v3YPriuibROMllv8WiCLAMeJ0QXbVaIzBOeHDeLx8yvoZZN+TENKxtT6u
586: IfMMneUzxHBqy0AQNfIsSsOnG5nqoeE/AwbS6VqCdH1aLfoCoPffacHYa0XvTcsi
587: aVlZfr+UzJS+ty8pRmFVi1UKSOADDdK8XfIovJl/zMP2TxYX2Y3fnjeLtl8Sqs2e
588: P+eHDoy7Wi4EPTyY7tNTCfxwKNHn1HQ5yrv5dgvMxFWIWXGz24yikFvtwLGHe8uJ
589: Wi+fBX+0PF0diZ6pIthZ149VU8qCqYAXjgpxZ0EZdrsiF6Ewz0cfg20SYApFcmW4
590: pwIDAQAB
591: -----END PUBLIC KEY-----
592: )",
593:     R"(
594: -----BEGIN PUBLIC KEY-----
595: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyhd5AfwrUohG3O4DE0K9
596: O3FmgB7zE4aDnkL8UUfGCh5kdP8q7ewMjekY+c6LwWOmpdJpSwqhfV1q5ZU1l6rk
597: 3hlt03LO3sgs28kcfOVH15hqfxts6Sg5KcRjxStE50ORmXGwXDcS9vqkJ60J1EHA
598: lcZqbCRSO73ZPLhdepfd0/C6tM0L7Ge6cAE62/MTmYNGv8fDzwQr/kYIJMdoS8Zp
599: thRpctFZJtPs3b0fffZA/TCLVKMvEVgTWs48751qKid7N/Lm/iEGx/tOf4o23Nec
600: Pz1IQaGLP+UOLVQbqQBHJWNOqigm7kWhDgs3N4YagWgxPEQ0WVLtFji/ZjlKZc7h
601: dwIDAQAB
602: -----END PUBLIC KEY-----
603: )",
604:     R"(
605: -----BEGIN PUBLIC KEY-----
606: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAnFDg3LhyV6BVE2Z3zQvN
607: 6urrKvPhygTa5+wIPGwYTzJ8DfGALqlsX3VOXMvcJTca6SbuwwkoXHuSU5wQxfcs
608: bt4jTXD3NIoRwQPl+D9IbgIMuX0ACl27rJmr/f9zkY7qui4k1X82pQkxBe+/qJ4r
609: TBwVNONVx1fekTMnSCEhwg5yU3TNbkObu0qlQeJfuMWLDQbW/8v/qfr/Nz0JqHDN
610: yYKfKvFMlORxyJYiOyeOsbzNGEhkGQGOmKhRUhS35kD+oA0jqwPwMCM9O4kFg/L8
611: iZbpBBX2By1K3msejWMRAewTOyPas6YMQOYq9BMmWQqzVtG5xcaSJwN/YnMpJyqb
612: sQIDAQAB
613: -----END PUBLIC KEY-----
614: )",
615:     R"(
616: -----BEGIN PUBLIC KEY-----
617: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA1z0RU8vGrfEkrscEoZKA
618: GiOcGh2EMcKwjQpl4nKuR9H4o/dg+CZregVSHg7MP2f8mhLZZyoFev49oWOV4Rmi
619: qs99UNxm7DyKW1fF1ovowsUW5lsDoKYLvpuzHo0s4laiV4AnIYP7tHGLdzsnK2Os
620: Cp5dSuMwKHPZ9N25hXxFB/dRrAdIiXHvbSqr4N29XzfQloQpL3bGHLKY6guFHluH
621: X5dJ9eirVakWWou7BR2rnD0k9vER6oRdVnJ6YKb5uhWEOQ3NmV961oyr+uiDTcep
622: qqtGHWuFhENixtiWGjFJJcACwqxEAW3bz9lyrfnPDsHSW/rlQVDIAkik+fOp+R7L
623: kQIDAQAB
624: -----END PUBLIC KEY-----
625: )",
626:     R"(
627: -----BEGIN PUBLIC KEY-----
628: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxwO27e1vnbNcpiDg7Wwx
629: K/w5aEGukXotu3529ieq+O39H0+Bak4vIbzGhDUh3/ElmxaFMAs4PYrWe/hc2WFD
630: H4JCOoFIn4y9gQeE855DGGFgeIVd1BnSs5S+5wUEMxLNyHdHSmINN6FsoZ535iUg
631: KdYjRh1iZevezg7ln8o/O36uthu925ehFBXSy6jLJgQlwmq0KxZJE0OAZhuDBM60
632: MtIunNa/e5y+Gw3GknFwtRLmn/nEckZx1nEtepYvvUa7UGy+8KuGuhOerCZTutbG
633: k8liCVgGenRve8unA2LrBbpL+AUf3CrZU/uAxxTqWmw6Z/S6TeW5ozeeyOCh8ii6
634: TwIDAQAB
635: -----END PUBLIC KEY-----
636: )",
637:     R"(
638: -----BEGIN PUBLIC KEY-----
639: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAsGIFOfIQ4RI5qu4klOxf
640: ge6eXwBMAkuTXyhyIIJDtE8CurnwQvUXVlt+Kf0SfuIFW6MY5ErcWE/vMFbc81IR
641: 9wByOAAV2CTyiLGZT63uE8pN6FSHd6yGYCLjXd3P3cnP3Qj5pBncpLuAUDfHG4wP
642: bs9jIADw3HysD+eCNja8p7ZC7CzWxTcO7HsEu9deAAU19YywdpagXvQ0pJ9zV5qU
643: jrHxBygl31t6TmmX+3d+azjGu9Hu36E+5wcSOOhuwAFXDejb40Ixv53ItJ3fZzzH
644: PF2nj9sQvQ8c5ptjyOvQCBRdqkEWXIVHClxqWb+o59pDIh1G0UGcmiDN7K9Gz5HA
645: ZQIDAQAB
646: -----END PUBLIC KEY-----
647: )",
648:     R"(
649: -----BEGIN PUBLIC KEY-----
650: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAt9uUnlW/CoUXT68yaZh9
651: SeXHzGRCPNEI98Tara+dgYxDX1z7nfOh8o15liT0QsAzx34EewZOxcKCNiV/dZX5
652: z4clCkD8uUbZut6IVx8Eu+7Qcd5jZthRc6hQrN9Ltv7ZQEh7KGXOHa53kT2K01ws
653: 4jbVmd/7Nx7y0Yyqhja01pIu/CUaTkODfQxBXwriLdIzp7y/iJeF/TLqCwZWHKQx
654: QOZnsPEveB1F00Va9MeAtTlXFUJ/TQXquqTjeLj4HuIRtbyuNgWoc0JyF+mcafAl
655: bnrNEBIfxZhAT81aUCIAzRJp6AqfdeZxnZ/WwohtZQZLXAxFQPTWCcP+Z9M7OIQL
656: WwIDAQAB
657: -----END PUBLIC KEY-----
658: )",
659:     R"(
660: -----BEGIN PUBLIC KEY-----
661: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA56NhfACkeCyZM07l2wmd
662: iTp24E2tLLKU3iByKlIRWRAvXsOejRMJTHTNHWa3cQ7uLP++Tf2St7ksNsyPMNZy
663: 9QRTLNCYr9rN9loLwdb2sMWxFBwwzCaAOTahGI7GJQy30UB7FEND0X/5U2rZvQij
664: Q6K+O4aa+K9M5qyOHNMmXywmTnAgWKNaNxQHPRtD2+dSj60T6zXdtIuCrPfcNGg5
665: gj07qWGEXX83V/L7nSqCiIVYg/wqds1x52Yjk1nhXYNBTqlnhmOd8LynGxz/sXC7
666: h2Q9XsHjXIChW4FHyLIOl6b4zPMBSxzCigYm3QZJWfAkZv5PBRtnq7vhYOLHzLQj
667: CwIDAQAB
668: -----END PUBLIC KEY-----
669: )",
670:     R"(
671: -----BEGIN PUBLIC KEY-----
672: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAmfPLe0IWGYC0MZC6YiM3
673: QGfhT6zSKB0I2DW44nlBlWUcF+32jW2bFJtgE76qGGKFeU4kJBWYr99ufHoAodNg
674: M1Ehl/JfQ5KmbC1WIqnFTrgbmqJde79jeCvCpbFLuqnzidwO1PbXDbfRFQcgWaXT
675: mDVLNNVmLxA0GkCv+kydE2gtcOD9BDceg7F/56TDvclyI5QqAnjE2XIRMPZlXQP4
676: oF2kgz4Cn7LxLHYmkU2sS9NYLzHoyUqFplWlxkQjA4eQ0neutV1Ydmc1IX8W7R38
677: A7nFtaT8iI8w6Vkv7ijYN6xf5cVBPKZ3Dv7AdwPet86JD5mf5v+r7iwg5xl3r77Z
678: iwIDAQAB
679: -----END PUBLIC KEY-----
680: )",
681:     R"(
682: -----BEGIN PUBLIC KEY-----
683: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAoB1kWsX8YmCcFOD9ilBY
684: xK076HmUAN026uJ8JpmU9Hz+QT1FNXOsnj1h2G6U6btYVIdHUTHy/BvAumrDKqRz
685: qcEAzCuhxUjPjss54a/Zqu6nQcoIPHuG/Er39oZHIVkPR1WCvWj8wmyYv6T//dPH
686: unO6tW29sXXxS+J1Gah6vpbtJw1pI/liah1DZzb13KWPDI6ZzviTNnW4S05r6js/
687: 30He+Yud6aywrdaP/7G90qcrteEFcjFy4Xf+5vG960oKoGoDplwX5poay1oCP9tb
688: g8AC8VSRAGi3oviTeSWZcrLXS8AtJhGvF48cXQj2q+8YeVKVDpH6fPQxJ9Sh9aeU
689: awIDAQAB
690: -----END PUBLIC KEY-----
691: )",
692:     R"(
693: -----BEGIN PUBLIC KEY-----
694: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4NTMAIYIlCMID00ufy/I
695: AZXc8pocDx9N1Q5x5/cL3aIpLmx02AKo9BvTJaJuHiTjlwYhPtlhIrHV4HUVTkOX
696: sISp8B8v9i2I1RIvCTAcvy3gcH6rdRWZ0cdTUiMEqnnxBX9zdzl8oMzZcyauv19D
697: BeqJvzflIT96b8g8K3mvgJHs9a1j9f0gN8FuTA0c52DouKnrh8UwH7mlrumYerJw
698: 6goJGQuK1HEOt6bcQuvogkbgJWOoEYwjNrPwQvIcP4wyrgSnOHg1yXOFE84oVynJ
699: czQEOz9ke42I3h8wrnQxilEYBVo2uX8MenqTyfGnE32lPRt3Wv1iEVQls8Cxiuy2
700: CQIDAQAB
701: -----END PUBLIC KEY-----
702: )",
703:     R"(
704: -----BEGIN PUBLIC KEY-----
705: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA3bUtfp66OtRyvIF/oucn
706: id8mo7gvbNEH04QMLO3Ok43dlWgWI3hekJAqOYc0mvoI5anqr98h8FI7aCYZm/bY
707: vpz0I1aXBaEPh3aWh8f/w9HME7ykBvmhMe3J+VFGWWL4eswfRl//GCtnSMBzDFhM
708: SaQOTvADWHkC0njeI5yXjf/lNm6fMACP1cnhuvCtnx7VP/DAtvUk9usDKG56MJnZ
709: UoVM3HHjbJeRwxCdlSWe12ilCdwMRKSDY92Hk38/zBLenH04C3HRQLjBGewACUmx
710: uvNInehZ4kSYFGa+7UxBxFtzJhlKzGR73qUjpWzZivCe1K0WfRVP5IWsKNCCESJ/
711: nQIDAQAB
712: -----END PUBLIC KEY-----
713: )",
714:     R"(
715: -----BEGIN PUBLIC KEY-----
716: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyV2dE/CRUAUE8ybq/DoS
717: Lc7QlYXh04K+McbhN724TbHahLTuDk5mR5TAunA8Nea4euRzknKdMFAz1eh9gyy3
718: 5x4UfXQW1fIZqNo6WNrGxYJgWAXU+pov+OvxsMQWzqS4jrTHDHbblCCLKp1akwJk
719: aFNyqgjAL373PcqXC+XAn8vHx4xHFoFP5lq4lLcJCOW5ee9v9El3w0USLwS+t1cF
720: RY3kuV6Njlr4zsRH9iM6/zaSuCALYWJ/JrPEurSJXzFZnWsvn6aQdeNeAn08+z0F
721: k2NwaauEo0xmLqzqTRGzjHqKKmeefN3/+M/FN2FrApDlxWQfhD2Y3USdAiN547Nj
722: 1wIDAQAB
723: -----END PUBLIC KEY-----
724: )",
725:     R"(
726: -----BEGIN PUBLIC KEY-----
727: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvm2+kTrEQWZXuxhWzBdl
728: PCbQGqbrukbeS6JKSlQLJDC8ayZIxFxatqg1Q8UPyv89MVRsHOGlG1OqFaOEtPjQ
729: Oo6j/moFwB4GPyJhJHOGpCKa4CLB5clhfDCLJw6ty7PcDU3T6yW4X4Qc5k4LRRWy
730: yzC8lVHfBdarN+1iEe0ALMOGoeiJjVn6i/AFxktRwgd8njqv/oWQyfjJZXkNMsb6
731: 7ZDxNVAUrp/WXpE4Kq694bB9xa/pWsqv7FjQJUgTnEzvbN+qXnVPtA7dHcOYYJ8Z
732: SbrJUfHrf8TS5B54AiopFpWG+hIbjqqdigqabBqFpmjiRDZgDy4zJJj52xJZMnrp
733: rwIDAQAB
734: -----END PUBLIC KEY-----
735: )",
736:     R"(
737: -----BEGIN PUBLIC KEY-----
738: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAwEAcVmY3589O02pLA22f
739: MlarLyJUgy0BeJDG5AUsi17ct8sHZzRiv9zKQVCBk1CtZY//jyqnrM7iCBLWsyby
740: TiTOtGYHHApaLnNjjtaHdQ6zplhbc3g2XLy+4ab8GNKG3zc8iXpsQM6r+JO5n9pm
741: V9vollz9dkFxS9l+1P17lZdIgCh9O3EIFJv5QCd5c9l2ezHAan2OhkWhiDtldnH/
742: MfRXbz7X5sqlwWLa/jhPtvY45x7dZaCHGqNzbupQZs0vHnAVdDu3vAWDmT/3sXHG
743: vmGxswKA9tPU0prSvQWLz4LUCnGi/cC5R+fiu+fovFM/BwvaGtqBFIF/1oWVq7bZ
744: 4wIDAQAB
745: -----END PUBLIC KEY-----
746: )",
747:     R"(
748: -----BEGIN PUBLIC KEY-----
749: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA25qGwNO1+qHygC8mjm8L
750: 3I66mV/IzslgBDHC91mE8YcI5Fq0sdrtsbUhK3z89wIN/zOhbHX0NEiXm2GxUnsI
751: vb5tDZXAh7AbTnXTMVbxO/e/8sPLUiObGjDvjVzyzrxOeG87yK/oIiilwk9wTsIb
752: wMn2Grj4ht9gVKx3oGHYV7STNdWBlzSaJj4Ou7+5M1InjPDRFZG1K31D2d3IHByX
753: lmcRPZtPFTa5C1uVJw00fI4F4uEFlPclZQlR5yA0G9v+0uDgLcjIUB4eqwMthUWc
754: dHhlmrPp04LI19eksWHCtG30RzmUaxDiIC7J2Ut0zHDqUe7aXn8tOVI7dE9tTKQD
755: KQIDAQAB
756: -----END PUBLIC KEY-----
757: )",
758:     R"(
759: -----BEGIN PUBLIC KEY-----
760: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA7EC2bx7aRnf3TcRg5gmw
761: QOKNCUheCelK8hoXLMsKSJqmufyJ+IHUejpXGOpvyYRbACiJ5GiNcww20MVpTBU7
762: YESWB2QSU2eEJJXMq84qsZSO8WGmAuKpUckI+hNHKQYJBEDOougV6/vVVEm5c5bc
763: SLWQo0+/ciQ21Zwz5SwimX8ep1YpqYirO04gcyGZzAfGboXRvdUwA+1bZvuUXdKC
764: 4zsCw2QALlcVpzPwjB5mqA/3a+SPgdLAiLOwWXFDRMnQw44UjsnPJFoXgEZiUpZm
765: EMS5gLv50CzQqJXK9mNzPuYXNUIc4Pw4ssVWe0OfN3Od90gl5uFUwk/G9lWSYnBN
766: 3wIDAQAB
767: -----END PUBLIC KEY-----
768: )", nullptr};
769: 
770: static const char *const community_public_keys[] = {
771:     R"(
772: -----BEGIN PUBLIC KEY-----
773: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAtXl28loGwAH3ZGQXXgJQ
774: 3omhIEiUb3z9Petjl+jmdtEQnMNUFEZiXkfJB02UFWBL1OoKKnjiGhcr5oGiIZKR
775: CoaL6SfmWe//7o8STM44stE0exzZcv8W4tWwjrzSWQnwh2JgSnHN64xoDQjdvG3X
776: 9uQ1xXMXghWOKqEpgArpJQkHoPW3CD5sCS2NLFrBG6KgX0W+GTV5HaKhTMr2754F
777: l260drcBJZhLFCeesze2DXtQC+R9D25Zwn2ehHHd2Fd1M10ZL/iKN8NeerB4Jnph
778: w6E3orA0DusDLDLtpJUHhmpLoU/1eYQFQOpGw2ce5I88Tkx7SKnCRy1UiE7BA82W
779: YQIDAQAB
780: -----END PUBLIC KEY-----
781: )",
782:     R"(
783: -----BEGIN PUBLIC KEY-----
784: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvTgQ+mJs8vG/TQTJ6sV+
785: tACTZTbmp8NkgTuwEyHZSNhX6W8FYwAqPzbePo7wudsUdBWV8j+kUYaBiqeiPUp0
786: 7neO/3oTUQkMJLq9FeIXfoYkS3+/5CIuvsfas6PJP9U2ge6MV1Ndgbd7a12cmX8V
787: 4eNwQRDv/H4zgL7YI2ZZSG1loxgMffZrpflNB87t/f0QYdmnwphMC5RqxiCkDZPA
788: a5/5KbmD6kjLh8RRRw3lAZbPQe5r7o2Xqqwg9gc6rQ/WFBB1Oj+Q5Bggqznl6dCB
789: JcLOA7rhYatv/mvt1h6ogQwQ9FGRM3PifV9boZxOQGBAkMD6ngpd5kVoOxdygC7v
790: twIDAQAB
791: -----END PUBLIC KEY-----
792: )",
793:     R"(
794: -----BEGIN PUBLIC KEY-----
795: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA7KvnA+Ixj4ZCLR+aXSFz
796: ICGbQdVrZ/hhjImDQcWgWY+z/bEbybslDvy5KEPrxTNxKZ0VfFFAVEUj2cw8B5KI
797: naK8U2VIpdD6LpEJvkOuWKg3bym4COhyAcRNqKKu/GPzS90wICJ2aaayF1mVoCIL
798: dsp2ZShSIVRJa55gVvfRN1ZEkqBnZryKNt/h3DNqqq2Sn3n3HIZ8H9oEO+L+2Efe
799: kyET7o9OHy6QZXhf4SJ8QlQAwxxe/L4bln8CBlBHKrUNNqxpjhC37EnY2jpuu3a9
800: EZcNFj8R4qIJx7hcltntZyKrEIXqc6I6x4oZ4qhZj3RQ5Lr+pJ++idoc1LmBS3k5
801: yQIDAQAB
802: -----END PUBLIC KEY-----
803: )",
804:     R"(
805: -----BEGIN PUBLIC KEY-----
806: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA7SF+5RZ9jXyruBkhxhk2
807: BSWPbohevxxv++7Uw0HXC/3Xw4jzii0tYaJ6O8QWXyggEAkvmONblAN1rfiz+h5M
808: oJUQwHjTTZ8BmKUmWrNayVokUXLu4IpCAHk4uSXfx4U/AINnNfWW7z8mUJf6nGsM
809: XePuKPBRUsw+JmTWOXEIVrkc/66B+gpgi+DwRFLUPh96D8XRAhp7QbHE9UMD3HpA
810: mPMX7ICVsVS+NGdCHNsdWfH4noaESjgmMdApKekgeeo8Zu1pvQ3y8iew1xOQVBoR
811: V+PCGWAJYB7ulqBBkRz+NhPLWw7wRA4yLNcZVlZuDFxH9EoavWdfIyYYUn4efSz9
812: tQIDAQAB
813: -----END PUBLIC KEY-----
814: )",
815:     R"(
816: -----BEGIN PUBLIC KEY-----
817: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAszmZ6Slv/oJvFpOLtSMx
818: 58AKMia9y+qcVfw77/Alb3b+Qi5L2uy6nHfJElT7RIeeXhJ8mFglZ70MecTfj0jl
819: 5WhW+yMg6jmPCJL2JMt/oeC4iY4Cf/3C9RHU4IO13VN4dnVQ5S+SEEmSbXnno9Pe
820: 06yyVgZeJ0REJMV1JZj9gOPc/wbeLHsx4UC5qsu32Ammy6J7tS+k7JvRc9CPOEpe
821: IhWoZmpONydcI6IRfyH2xl4uLY3hWDrRei0I2zGH45G2hPNeTtRh27t+SzXO7h9j
822: y072CgHytRgQBiH711i8fe4bHMmtVPhPjFrbuzbJSgE7SyikrWIHMDsnPz443bdR
823: cQIDAQAB
824: -----END PUBLIC KEY-----
825: )",
826:     R"(
827: -----BEGIN PUBLIC KEY-----
828: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAleywAb7xZKYTFE08gGA9
829: ffTeYPRcECl/J060fUziIiFu0NHTOZO+a4BH2X+E1WjjNNQkbn00g+op4nqg3/U+
830: UaKuXNjWY2Rvd8s91fUD0YOdRpPmsTm2QqhgmYYzO8Oh3YXBNRpXaqALbjL9Nahw
831: YEAsI3o5yenZGUIEk3JaZFHsAZPL5wGgDVpZgmVUHJ0EO8N5LQh01aHxnP5+ey2z
832: L5h6IdWLubb07wEBk5bnmIvdhd6dIBzUql27BAqvxKJbW0/okjrhIgcIANDCavfV
833: L8UP7MCGnfozK7VIl5DG85gCQVAD8+lGUDzOuhzZjl7XKpkFAIWaS8pl4AJbJuG8
834: nwIDAQAB
835: -----END PUBLIC KEY-----
836: )",
837:     R"(
838: -----BEGIN PUBLIC KEY-----
839: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxiKgcR7Kb1CGTNczbuX+
840: S7OFpnVLDD5XGVKvYWxL+2By2QRFPWtMs8c24omLIgZ/CWBFPraMiNKS4+V9ar2C
841: wJhToJnAOKyayA0Gw2wNZx1mgHAZ/5mT+ImfkmZu2HPwtzJmJDQlESD4p40BWBNa
842: ZpWFGPMKn4GqvOOSGevC/r9inXm6NaPkM+B/piVDEgiJ7g/kpoqImmNb/c2/3XG5
843: 3kbDIHdbd2m3A3jWCjNGSANKsR5C0/rZtvsA8tjDlNWIuKmkU3C2nfj3UduU4dNP
844: Cisod/pDY8ov0U9sdkM9XZsTXjtbAIGLzMshmOv4ajRFUueGnsZW0GRqp9DSnKmj
845: 2QIDAQAB
846: -----END PUBLIC KEY-----
847: )",
848:     R"(
849: -----BEGIN PUBLIC KEY-----
850: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAuh334hUmJcdDJUSmeXqE
851: GUfGnASD2QrnuoS+gsXgW5BQW8YMDFASvADQhoDUdcwZMlAF+p+CxKCX/gBp40nC
852: 5eyPXv1e0K6PFcCdHtJq8MhGYAr1sy+7cOpzv0r9whobYUykGoHjdwZeu3VbA3uz
853: go80oYQlwY+v4zZFafCz3cXw8u7n/9PlddgeqHuIPsNZLocICuBUxwg5rHTzycg2
854: Pa68CRselONGN12V0/wlOg+NZpKCym58CM9SS/0v4YZ6LnmINo8gdRYnGE2zhvey
855: pHR8IJ8WSJXbl8NwyIY1AmtT/Z0dbAclfD8Wt/w5KA/sttnQzrB7fPsLRyLP1Alq
856: iQIDAQAB
857: -----END PUBLIC KEY-----
858: )",
859:     R"(
860: -----BEGIN PUBLIC KEY-----
861: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvWuRMEbez/Ud2o/0KA04
862: K9u3HePWud9rEqMsPv2HlclH3k+cezoUJzVre0lopv3R4aG3LDoFETrgGgUVrfPG
863: z3Zh7vyk0kb4IGkv+kLQu/cWQXyNzigxV+WQnpIWQ28vrP45y5f+GhwwgzFaDAQR
864: u1o1HH1FEnP7SSzHVvisNTecY95+F5AOvtOOUg4VlegXdUeGZHEza/0D9V8gODPL
865: DzbOJDDiqX8ahhRnIZyGEg6y7QqftZFz7j0siCHTXXYJBOcPjD4TqTUNpGvBox44
866: wgLlLcDsZ/n2Ck4doLXxVz9F80VKOriHSk+qIwseykKVzWQDQTOMOsjCmQsDvram
867: RwIDAQAB
868: -----END PUBLIC KEY-----
869: )",
870:     R"(
871: -----BEGIN PUBLIC KEY-----
872: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyJmGd1GuBv/WD80IcVyr
873: dZcmuYe/7azLuV1wsgtH4gsUx+ifUwLZUhLFGOTAPFitbFYPPdhQKncO+BcbvOIo
874: 9FGKj9jGVpMU6C+0JQfi+koESevtO1tYzG8c2dMOGNUO0Hlj2Hezm3tZY4nAbo1J
875: DYqQSY7qvOYZPFvOS/zL+q2vMx93w9jDHJK4iU02ovAqK9xCWfTp4W7rtbDeTgiX
876: W/75rMG8DWI1ZHA2JXAOFPsiOHa0/yyvCvUIWvRuNHqTTN5NFiJRIcbTCKKbNwNM
877: xcNkBQCx4xwOqD9TkDbHpBOC/pfW7j3ygJdYRjFFqm10+KwPACYo/f0n4n4DI8Zz
878: twIDAQAB
879: -----END PUBLIC KEY-----
880: )",
881:     R"(
882: -----BEGIN PUBLIC KEY-----
883: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAnmxbunsK+2pL8Cva9F8E
884: 9o/dQ35TuIqcgpl9/oIc++x+6G5/8UT5mgGCQTITJRIAPnHsZ9XEnMxTAuSCDkYG
885: CA3JMl1MT7Zxu8TQJBPiXxOaAE1UmA13JuQ2Uu0v7T6TucQxR9KMvcdCxOZ5cBU4
886: uyJObnZVy/WjM2vWcWDUaYGfMss3eYxcDpavspBANdtSZfv11+8/VC+gEGBOe+oW
887: zDR+BlQx//MAzwSP5HVQcmLHsT073IvkoUWJUxSCCwlLe60ylpY16BLT6dB0RU8B
888: sxFcIwmYg0kq19EEPPvZLvRKjG/TJRm1MFzOE5LP2VxLGdMltWYEVsBZHTcWU7HR
889: 8wIDAQAB
890: -----END PUBLIC KEY-----
891: )",
892:     R"(
893: -----BEGIN PUBLIC KEY-----
894: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAlo7eDZOpCptanajUtDK3
895: q8Q/ykxmDDw6lVSiLBm54zwMxaqfM+tV/xqalvIVv3BrucRkCs6H+R0bpd7XhbE5
896: a7ZFSrWCBf1V6y/NZrEn4qcRbk/WsG4UFqu7CG4r+EgQ4nmoIH/A5+e8FUcur3Y8
897: 2ie9Foi1CUpZojWYZJeHKbb2yYn4MFHszEb5w9HVxY+i9jR1B8Rvn6OEK3OYDrtA
898: KnPXp4OiDx6CviYEmipX815PPj7Sv8KKL96JqGWjC4kYw6ALgV/GxiX++tv6rh2O
899: paW9MBv1y+5oZ8ls5S2T/LXbxDpjUEKC9guSSWmsPHRMxOumXsw0H43grC3Ce8Ui
900: CwIDAQAB
901: -----END PUBLIC KEY-----
902: )",
903:     R"(
904: -----BEGIN PUBLIC KEY-----
905: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA0ACgf0kJeQgDh+vHj2aj
906: K/6FQ794MknLxlwTARGlktoVwZgW/qc6vMZsILRUP1gb/gPXdpSTqqad/GLG4f5R
907: 1Ji1It6BniJOPWu1YyTz0C/BXzTGWbwPnIaawbpQE8n4A+tjGGvAoauPtzr0bWfV
908: XOXPfIW9XB51dcaVTZgHN55Y8Yd/Pcu9/lqXqXyE23tDLXR/QgGpwK9VxTSbRmuC
909: WspwqWY6L3MIw+3HIXERTM1uNhc9oHxMOCRbJmUghG0wCWB0ed3Xhbnl9mHlX+l1
910: rfCJAP4lVWKFjkKBNUejaf+WHxASMjrQubgHLZ2fpf3Ra8TfI3rgPABsAqEIFw3T
911: QwIDAQAB
912: -----END PUBLIC KEY-----
913: )",
914:     R"(
915: -----BEGIN PUBLIC KEY-----
916: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAt635/P50bMbEDTapjAQz
917: ARTb3y8jMHxVruX0tJU1tycmkX3J8tBALmc6TkSHNTJcQmR8L8Sj3h76l/vuL373
918: HFSGZ4xghBQqR1lUd2kVomoh+rzEte+0rHWm0JMhjmTQBx+AkDCOw4z3vi5AxWx0
919: 4EbYpQm2akVGKXQrQPyds0UirmdLACCH6WM6exgAXr75DB4PUpG85oI9Q+5ee1Km
920: +4atVJ4FNa6ZnjWccrlMYT0W7a0Y7feJPAPvfizrs2MG9/ijyBX34eCWA5dtUSIm
921: 2uqI6DxITZlLTvXVDSKQGlq5TEGMvRULWTatqWy4g+tOZ8rSbRuj32pcBnXlwuVu
922: 7QIDAQAB
923: -----END PUBLIC KEY-----
924: )",
925:     R"(
926: -----BEGIN PUBLIC KEY-----
927: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAwqO3yWSLKqz1uQ54iFd/
928: VcQzgT6chLVuhktt7EFvi3tKaQqz2h2KPkDR+MssRV/BZ/41GNlR6r6p5CaPVDDe
929: Cuj5IcxrIFZIOBMBi1YZ/bknF9edJacINxNfGK/lXBNEAdUvxcOxX8WeP69uvl2l
930: SKyO3yAdx6HOyL9if95bYQD19HYPZzbfccPX1aD4pjnej6uMfd7yZErH7i8y0oj4
931: eSKSe1CisjFlR9NzRGO42jU9rtqnAFH9sK5wU9xKQ7bQwlz7yKBF2RuuQweMpXb6
932: lSObI7ZqYN+7jkf9F5hKRx4kX3+MMBeYmFOy1aYZ08u6sdJ2ua/hFNSDRg7e/UCe
933: AwIDAQAB
934: -----END PUBLIC KEY-----
935: )",
936:     R"(
937: -----BEGIN PUBLIC KEY-----
938: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAkJihnfMECaa6YCg6avam
939: cb8Sy1GshJ7c7+EW6C4vnspSSvEi04AEBB29pnEF9+VO6VSUHLxunVCpbmKFaLH+
940: 5fDLnc/wCkjPQww49da9MEScCmVGjROlmog65cxQbv4lfxyw55sFV3s/5CPcGlVc
941: 1gojHRABrx4YocpeYies04mEVoOYg1DBG4Uf+aFd5+hm3ZtBa4mqTK2iQa4ILkHa
942: a0/Us1drRuDjjI4zSbgRzy9x0JVDvqDdLubHyaEf7d7SdrKzodhydG84qpsPFxIj
943: LK7Bu5v7P4ZTJmxMG3PBM2kB//hlYVR4vO4VEu66mQIM6km+vT9cwxz77qIJhLn3
944: ywIDAQAB
945: -----END PUBLIC KEY-----
946: )",
947:     R"(
948: -----BEGIN PUBLIC KEY-----
949: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA9NbP7ijUxZh4j0NVF6yO
950: IZ0rzROwl4pP4HGeN+Woyi9+qpdE874WlVoquGEpsshF4Ojzbu2BtXuihb783awa
951: GLx66MYPeID1FjTKmuCJ2aluOP+DkVo6K1EoqVJXyeIxZzVSqhSIuAdb/vmPlgLz
952: Fzdk3FgNNOERuGV363DRGz1YxZVnJeSs76g+/9ddhMk8cqIRup5S4YgTOSr0vKem
953: 1E6lyE8IbLoq9J7w5Ur8VjzE2cI+eLKGFqr46Q8pf0pJq72gd+Z3mH5D2LmvEtAR
954: 9jAQXVlLfHauQR2M0K6mqDy9GxL19OU4tGO+GY86VvDTU+wZppAZRz9AKoL1fwfI
955: BQIDAQAB
956: -----END PUBLIC KEY-----
957: )",
958:     R"(
959: -----BEGIN PUBLIC KEY-----
960: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAjrI16GdC2zJriLbyzcre
961: AqvckBSTMd4bdGaodUBNBTBVbITsOw/k7D62y2hSZHt2nHOyEVkJINJHADrpNZuY
962: ybS4ssEXxD8+NnjATqQxDMuSz8lUj/Jnf49uzLh84fep3DTksDcQX6Nvio5q8Xbh
963: HRgvl5I+tPfLtme0oW9cVuVja2i5lHB3SzYCW9Kk/V4/d2WiceYf91a1Nae6m7QV
964: 5bmbYoHmsxT8refTQq+5lAhzVXYU9QRgiKdbE8sSmkV+YiZEtGijefUXgmOxx3I9
965: B3y03796WBS/RHpSzdMNJw/xPWJcSEMqaUdSYr0DuPCnrn7ojFeF/EFC47CBq5DU
966: swIDAQAB
967: -----END PUBLIC KEY-----
968: )",
969:     R"(
970: -----BEGIN PUBLIC KEY-----
971: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAjS1+My6OhQCCD1DgrzKu
972: db4Fvc3aqqEhQyjqMLnalp0uoGFpSLoPsZiPGloTE8FSs1ZBFKQ8h2SsGwSdhRKF
973: xIqoOnS0B/ORjGJxTj7Q2YWjzkCZUD4Ul2AxIbv3TmZM2LeyHJL3A71tSuck8EQY
974: PE2aj1tLzXsSfRaByy5xwXiU6UpnwCY1xb8tK8QxavRCo5T9Si9tNsolStoNVXV0
975: k9EbTcRNnxCvab/oqjvgyRuSmIES00v8jZOGQZQUpw02RN6yCBeX2i8GPsGjj/T9
976: 6Gu1Z3G4zUjLlJxl8vjo8KIDaQ8NVWT0j7gx9Knvb5tWnAORI1aJA8AHQvaoOT1W
977: 1wIDAQAB
978: -----END PUBLIC KEY-----
979: )", nullptr};
980: 
981: const vector<string> ExtensionHelper::GetPublicKeys(bool allow_community_extensions) {
982: 	vector<string> keys;
983: 	for (idx_t i = 0; public_keys[i]; i++) {
984: 		keys.emplace_back(public_keys[i]);
985: 	}
986: 	if (allow_community_extensions) {
987: 		for (idx_t i = 0; community_public_keys[i]; i++) {
988: 			keys.emplace_back(community_public_keys[i]);
989: 		}
990: 	}
991: 	return keys;
992: }
993: 
994: } // namespace duckdb
[end of src/main/extension/extension_helper.cpp]
[start of src/optimizer/column_lifetime_analyzer.cpp]
1: #include "duckdb/optimizer/column_lifetime_analyzer.hpp"
2: 
3: #include "duckdb/main/client_context.hpp"
4: #include "duckdb/optimizer/column_binding_replacer.hpp"
5: #include "duckdb/optimizer/optimizer.hpp"
6: #include "duckdb/optimizer/topn_optimizer.hpp"
7: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
8: #include "duckdb/planner/expression/bound_constant_expression.hpp"
9: #include "duckdb/planner/expression_iterator.hpp"
10: #include "duckdb/planner/operator/logical_comparison_join.hpp"
11: #include "duckdb/planner/operator/logical_filter.hpp"
12: #include "duckdb/planner/operator/logical_order.hpp"
13: #include "duckdb/planner/operator/logical_projection.hpp"
14: 
15: namespace duckdb {
16: 
17: void ColumnLifetimeAnalyzer::ExtractUnusedColumnBindings(const vector<ColumnBinding> &bindings,
18:                                                          column_binding_set_t &unused_bindings) {
19: 	for (idx_t i = 0; i < bindings.size(); i++) {
20: 		if (column_references.find(bindings[i]) == column_references.end()) {
21: 			unused_bindings.insert(bindings[i]);
22: 		}
23: 	}
24: }
25: 
26: void ColumnLifetimeAnalyzer::GenerateProjectionMap(vector<ColumnBinding> bindings,
27:                                                    column_binding_set_t &unused_bindings,
28:                                                    vector<idx_t> &projection_map) {
29: 	projection_map.clear();
30: 	if (unused_bindings.empty()) {
31: 		return;
32: 	}
33: 	// now iterate over the result bindings of the child
34: 	for (idx_t i = 0; i < bindings.size(); i++) {
35: 		// if this binding does not belong to the unused bindings, add it to the projection map
36: 		if (unused_bindings.find(bindings[i]) == unused_bindings.end()) {
37: 			projection_map.push_back(i);
38: 		}
39: 	}
40: 	if (projection_map.size() == bindings.size()) {
41: 		projection_map.clear();
42: 	}
43: }
44: 
45: void ColumnLifetimeAnalyzer::StandardVisitOperator(LogicalOperator &op) {
46: 	VisitOperatorExpressions(op);
47: 	VisitOperatorChildren(op);
48: }
49: 
50: void ExtractColumnBindings(Expression &expr, vector<ColumnBinding> &bindings) {
51: 	if (expr.GetExpressionType() == ExpressionType::BOUND_COLUMN_REF) {
52: 		auto &bound_ref = expr.Cast<BoundColumnRefExpression>();
53: 		bindings.push_back(bound_ref.binding);
54: 	}
55: 	ExpressionIterator::EnumerateChildren(expr, [&](Expression &child) { ExtractColumnBindings(child, bindings); });
56: }
57: 
58: void ColumnLifetimeAnalyzer::VisitOperator(LogicalOperator &op) {
59: 	Verify(op);
60: 	if (TopN::CanOptimize(op) && op.children[0]->type == LogicalOperatorType::LOGICAL_ORDER_BY) {
61: 		// Let's not mess with this, TopN is more important than projection maps
62: 		// TopN does not support a projection map like Order does
63: 		VisitOperatorExpressions(op);                        // Visit LIMIT
64: 		VisitOperatorExpressions(*op.children[0]);           // Visit ORDER
65: 		StandardVisitOperator(*op.children[0]->children[0]); // Recurse into child of ORDER
66: 		return;
67: 	}
68: 	switch (op.type) {
69: 	case LogicalOperatorType::LOGICAL_AGGREGATE_AND_GROUP_BY: {
70: 		// FIXME: groups that are not referenced can be removed from projection
71: 		// recurse into the children of the aggregate
72: 		ColumnLifetimeAnalyzer analyzer(optimizer, root);
73: 		analyzer.StandardVisitOperator(op);
74: 		return;
75: 	}
76: 	case LogicalOperatorType::LOGICAL_ASOF_JOIN:
77: 	case LogicalOperatorType::LOGICAL_DELIM_JOIN:
78: 	case LogicalOperatorType::LOGICAL_COMPARISON_JOIN: {
79: 		auto &comp_join = op.Cast<LogicalComparisonJoin>();
80: 		if (everything_referenced) {
81: 			break;
82: 		}
83: 
84: 		// FIXME: for now, we only push into the projection map for equality (hash) joins
85: 		idx_t has_range = 0;
86: 		if (!comp_join.HasEquality(has_range) || optimizer.context.config.prefer_range_joins) {
87: 			return;
88: 		}
89: 
90: 		column_binding_set_t lhs_unused;
91: 		column_binding_set_t rhs_unused;
92: 		ExtractUnusedColumnBindings(op.children[0]->GetColumnBindings(), lhs_unused);
93: 		ExtractUnusedColumnBindings(op.children[1]->GetColumnBindings(), rhs_unused);
94: 
95: 		StandardVisitOperator(op);
96: 
97: 		// then generate the projection map
98: 		if (op.type != LogicalOperatorType::LOGICAL_ASOF_JOIN) {
99: 			// FIXME: left_projection_map in ASOF join
100: 			GenerateProjectionMap(op.children[0]->GetColumnBindings(), lhs_unused, comp_join.left_projection_map);
101: 		}
102: 		GenerateProjectionMap(op.children[1]->GetColumnBindings(), rhs_unused, comp_join.right_projection_map);
103: 		return;
104: 	}
105: 	case LogicalOperatorType::LOGICAL_UNION:
106: 	case LogicalOperatorType::LOGICAL_EXCEPT:
107: 	case LogicalOperatorType::LOGICAL_INTERSECT:
108: 	case LogicalOperatorType::LOGICAL_MATERIALIZED_CTE: {
109: 		// for set operations/materialized CTEs we don't remove anything, just recursively visit the children
110: 		// FIXME: for UNION we can remove unreferenced columns as long as everything_referenced is false (i.e. we
111: 		// encounter a UNION node that is not preceded by a DISTINCT)
112: 		ColumnLifetimeAnalyzer analyzer(optimizer, root, true);
113: 		analyzer.StandardVisitOperator(op);
114: 		return;
115: 	}
116: 	case LogicalOperatorType::LOGICAL_PROJECTION: {
117: 		// then recurse into the children of this projection
118: 		ColumnLifetimeAnalyzer analyzer(optimizer, root);
119: 		analyzer.StandardVisitOperator(op);
120: 		return;
121: 	}
122: 	case LogicalOperatorType::LOGICAL_ORDER_BY: {
123: 		auto &order = op.Cast<LogicalOrder>();
124: 		if (everything_referenced) {
125: 			break;
126: 		}
127: 
128: 		column_binding_set_t unused_bindings;
129: 		ExtractUnusedColumnBindings(op.children[0]->GetColumnBindings(), unused_bindings);
130: 
131: 		StandardVisitOperator(op);
132: 
133: 		GenerateProjectionMap(op.children[0]->GetColumnBindings(), unused_bindings, order.projection_map);
134: 		return;
135: 	}
136: 	case LogicalOperatorType::LOGICAL_DISTINCT: {
137: 		// distinct, all projected columns are used for the DISTINCT computation
138: 		// mark all columns as used and continue to the children
139: 		// FIXME: DISTINCT with expression list does not implicitly reference everything
140: 		everything_referenced = true;
141: 		break;
142: 	}
143: 	case LogicalOperatorType::LOGICAL_FILTER: {
144: 		auto &filter = op.Cast<LogicalFilter>();
145: 		if (everything_referenced) {
146: 			break;
147: 		}
148: 
149: 		// filter, figure out which columns are not needed after the filter
150: 		column_binding_set_t unused_bindings;
151: 		ExtractUnusedColumnBindings(op.children[0]->GetColumnBindings(), unused_bindings);
152: 
153: 		StandardVisitOperator(op);
154: 
155: 		// then generate the projection map
156: 		GenerateProjectionMap(op.children[0]->GetColumnBindings(), unused_bindings, filter.projection_map);
157: 
158: 		return;
159: 	}
160: 	default:
161: 		break;
162: 	}
163: 	StandardVisitOperator(op);
164: }
165: 
166: void ColumnLifetimeAnalyzer::Verify(LogicalOperator &op) {
167: #ifdef DEBUG
168: 	if (everything_referenced) {
169: 		return;
170: 	}
171: 	switch (op.type) {
172: 	case LogicalOperatorType::LOGICAL_ASOF_JOIN:
173: 	case LogicalOperatorType::LOGICAL_DELIM_JOIN:
174: 	case LogicalOperatorType::LOGICAL_COMPARISON_JOIN:
175: 		AddVerificationProjection(op.children[0]);
176: 		if (op.Cast<LogicalComparisonJoin>().join_type != JoinType::MARK) { // Can't mess up the mark_index
177: 			AddVerificationProjection(op.children[1]);
178: 		}
179: 		break;
180: 	case LogicalOperatorType::LOGICAL_ORDER_BY:
181: 	case LogicalOperatorType::LOGICAL_FILTER:
182: 		AddVerificationProjection(op.children[0]);
183: 		break;
184: 	default:
185: 		break;
186: 	}
187: #endif
188: }
189: 
190: void ColumnLifetimeAnalyzer::AddVerificationProjection(unique_ptr<LogicalOperator> &child) {
191: 	child->ResolveOperatorTypes();
192: 	const auto child_types = child->types;
193: 	const auto child_bindings = child->GetColumnBindings();
194: 	const auto column_count = child_bindings.size();
195: 
196: 	// If our child has columns [i, j], we will generate a projection like so [NULL, j, NULL, i, NULL]
197: 	const auto projection_column_count = column_count * 2 + 1;
198: 	vector<unique_ptr<Expression>> expressions;
199: 	expressions.reserve(projection_column_count);
200: 
201: 	// First fill with all NULLs
202: 	for (idx_t col_idx = 0; col_idx < projection_column_count; col_idx++) {
203: 		expressions.emplace_back(make_uniq<BoundConstantExpression>(Value(LogicalType::UTINYINT)));
204: 	}
205: 
206: 	// Now place the "real" columns in their respective positions, while keeping track of which column becomes which
207: 	const auto table_index = optimizer.binder.GenerateTableIndex();
208: 	ColumnBindingReplacer replacer;
209: 	for (idx_t col_idx = 0; col_idx < column_count; col_idx++) {
210: 		const auto &old_binding = child_bindings[col_idx];
211: 		const auto new_col_idx = projection_column_count - 2 - col_idx * 2;
212: 		expressions[new_col_idx] = make_uniq<BoundColumnRefExpression>(child_types[col_idx], old_binding);
213: 		replacer.replacement_bindings.emplace_back(old_binding, ColumnBinding(table_index, new_col_idx));
214: 	}
215: 
216: 	// Create a projection and swap the operators accordingly
217: 	auto projection = make_uniq<LogicalProjection>(table_index, std::move(expressions));
218: 	projection->children.emplace_back(std::move(child));
219: 	child = std::move(projection);
220: 
221: 	// Replace references to the old binding (higher up in the plan) with references to the new binding
222: 	replacer.stop_operator = child.get();
223: 	replacer.VisitOperator(root);
224: 
225: 	// Add new bindings to column_references, else they are considered "unused"
226: 	for (const auto &replacement_binding : replacer.replacement_bindings) {
227: 		if (column_references.find(replacement_binding.old_binding) != column_references.end()) {
228: 			column_references.insert(replacement_binding.new_binding);
229: 		}
230: 	}
231: }
232: 
233: unique_ptr<Expression> ColumnLifetimeAnalyzer::VisitReplace(BoundColumnRefExpression &expr,
234:                                                             unique_ptr<Expression> *expr_ptr) {
235: 	column_references.insert(expr.binding);
236: 	return nullptr;
237: }
238: 
239: unique_ptr<Expression> ColumnLifetimeAnalyzer::VisitReplace(BoundReferenceExpression &expr,
240:                                                             unique_ptr<Expression> *expr_ptr) {
241: 	// BoundReferenceExpression should not be used here yet, they only belong in the physical plan
242: 	throw InternalException("BoundReferenceExpression should not be used here yet!");
243: }
244: 
245: } // namespace duckdb
[end of src/optimizer/column_lifetime_analyzer.cpp]
[start of src/optimizer/filter_combiner.cpp]
1: #include "duckdb/optimizer/filter_combiner.hpp"
2: 
3: #include "duckdb/execution/expression_executor.hpp"
4: #include "duckdb/optimizer/optimizer.hpp"
5: #include "duckdb/planner/expression.hpp"
6: #include "duckdb/planner/expression/bound_between_expression.hpp"
7: #include "duckdb/planner/expression/bound_cast_expression.hpp"
8: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
9: #include "duckdb/planner/expression/bound_comparison_expression.hpp"
10: #include "duckdb/planner/expression/bound_conjunction_expression.hpp"
11: #include "duckdb/planner/expression/bound_constant_expression.hpp"
12: #include "duckdb/planner/expression/bound_function_expression.hpp"
13: #include "duckdb/planner/expression/bound_operator_expression.hpp"
14: #include "duckdb/planner/filter/constant_filter.hpp"
15: #include "duckdb/planner/filter/in_filter.hpp"
16: #include "duckdb/planner/filter/null_filter.hpp"
17: #include "duckdb/planner/filter/optional_filter.hpp"
18: #include "duckdb/planner/filter/struct_filter.hpp"
19: #include "duckdb/planner/table_filter.hpp"
20: #include "duckdb/common/operator/subtract.hpp"
21: 
22: namespace duckdb {
23: 
24: using ExpressionValueInformation = FilterCombiner::ExpressionValueInformation;
25: 
26: ValueComparisonResult CompareValueInformation(ExpressionValueInformation &left, ExpressionValueInformation &right);
27: 
28: FilterCombiner::FilterCombiner(ClientContext &context) : context(context) {
29: }
30: 
31: FilterCombiner::FilterCombiner(Optimizer &optimizer) : FilterCombiner(optimizer.context) {
32: }
33: 
34: Expression &FilterCombiner::GetNode(Expression &expr) {
35: 	auto entry = stored_expressions.find(expr);
36: 	if (entry != stored_expressions.end()) {
37: 		// expression already exists: return a reference to the stored expression
38: 		return *entry->second;
39: 	}
40: 	// expression does not exist yet: create a copy and store it
41: 	auto copy = expr.Copy();
42: 	auto &copy_ref = *copy;
43: 	D_ASSERT(stored_expressions.find(copy_ref) == stored_expressions.end());
44: 	stored_expressions[copy_ref] = std::move(copy);
45: 	return copy_ref;
46: }
47: 
48: idx_t FilterCombiner::GetEquivalenceSet(Expression &expr) {
49: 	D_ASSERT(stored_expressions.find(expr) != stored_expressions.end());
50: 	D_ASSERT(stored_expressions.find(expr)->second.get() == &expr);
51: 	auto entry = equivalence_set_map.find(expr);
52: 	if (entry == equivalence_set_map.end()) {
53: 		idx_t index = set_index++;
54: 		equivalence_set_map[expr] = index;
55: 		equivalence_map[index].push_back(expr);
56: 		constant_values.insert(make_pair(index, vector<ExpressionValueInformation>()));
57: 		return index;
58: 	} else {
59: 		return entry->second;
60: 	}
61: }
62: 
63: FilterResult FilterCombiner::AddConstantComparison(vector<ExpressionValueInformation> &info_list,
64:                                                    ExpressionValueInformation info) {
65: 	if (info.constant.IsNull()) {
66: 		return FilterResult::UNSATISFIABLE;
67: 	}
68: 	for (idx_t i = 0; i < info_list.size(); i++) {
69: 		auto comparison = CompareValueInformation(info_list[i], info);
70: 		switch (comparison) {
71: 		case ValueComparisonResult::PRUNE_LEFT:
72: 			// prune the entry from the info list
73: 			info_list.erase_at(i);
74: 			i--;
75: 			break;
76: 		case ValueComparisonResult::PRUNE_RIGHT:
77: 			// prune the current info
78: 			return FilterResult::SUCCESS;
79: 		case ValueComparisonResult::UNSATISFIABLE_CONDITION:
80: 			// combination of filters is unsatisfiable: prune the entire branch
81: 			return FilterResult::UNSATISFIABLE;
82: 		default:
83: 			// prune nothing, move to the next condition
84: 			break;
85: 		}
86: 	}
87: 	// finally add the entry to the list
88: 	info_list.push_back(info);
89: 	return FilterResult::SUCCESS;
90: }
91: 
92: FilterResult FilterCombiner::AddFilter(unique_ptr<Expression> expr) {
93: 	//	LookUpConjunctions(expr.get());
94: 	// try to push the filter into the combiner
95: 	auto result = AddFilter(*expr);
96: 	if (result == FilterResult::UNSUPPORTED) {
97: 		// unsupported filter, push into remaining filters
98: 		remaining_filters.push_back(std::move(expr));
99: 		return FilterResult::SUCCESS;
100: 	}
101: 	return result;
102: }
103: 
104: void FilterCombiner::GenerateFilters(const std::function<void(unique_ptr<Expression> filter)> &callback) {
105: 	// first loop over the remaining filters
106: 	for (auto &filter : remaining_filters) {
107: 		callback(std::move(filter));
108: 	}
109: 	remaining_filters.clear();
110: 	// now loop over the equivalence sets
111: 	for (auto &entry : equivalence_map) {
112: 		auto equivalence_set = entry.first;
113: 		auto &entries = entry.second;
114: 		auto &constant_list = constant_values.find(equivalence_set)->second;
115: 		// for each entry generate an equality expression comparing to each other
116: 		for (idx_t i = 0; i < entries.size(); i++) {
117: 			for (idx_t k = i + 1; k < entries.size(); k++) {
118: 				auto comparison = make_uniq<BoundComparisonExpression>(
119: 				    ExpressionType::COMPARE_EQUAL, entries[i].get().Copy(), entries[k].get().Copy());
120: 				callback(std::move(comparison));
121: 			}
122: 			// for each entry also create a comparison with each constant
123: 			auto lower_index = optional_idx::Invalid();
124: 			auto upper_index = optional_idx::Invalid();
125: 			bool lower_inclusive = false;
126: 			bool upper_inclusive = false;
127: 			for (idx_t k = 0; k < constant_list.size(); k++) {
128: 				auto &info = constant_list[k];
129: 				if (info.comparison_type == ExpressionType::COMPARE_GREATERTHAN ||
130: 				    info.comparison_type == ExpressionType::COMPARE_GREATERTHANOREQUALTO) {
131: 					lower_index = k;
132: 					lower_inclusive = info.comparison_type == ExpressionType::COMPARE_GREATERTHANOREQUALTO;
133: 				} else if (info.comparison_type == ExpressionType::COMPARE_LESSTHAN ||
134: 				           info.comparison_type == ExpressionType::COMPARE_LESSTHANOREQUALTO) {
135: 					upper_index = k;
136: 					upper_inclusive = info.comparison_type == ExpressionType::COMPARE_LESSTHANOREQUALTO;
137: 				} else {
138: 					auto constant = make_uniq<BoundConstantExpression>(info.constant);
139: 					auto comparison = make_uniq<BoundComparisonExpression>(
140: 					    info.comparison_type, entries[i].get().Copy(), std::move(constant));
141: 					callback(std::move(comparison));
142: 				}
143: 			}
144: 			if (lower_index.IsValid() && upper_index.IsValid()) {
145: 				// found both lower and upper index, create a BETWEEN expression
146: 				auto lower_constant =
147: 				    make_uniq<BoundConstantExpression>(constant_list[lower_index.GetIndex()].constant);
148: 				auto upper_constant =
149: 				    make_uniq<BoundConstantExpression>(constant_list[upper_index.GetIndex()].constant);
150: 				auto between =
151: 				    make_uniq<BoundBetweenExpression>(entries[i].get().Copy(), std::move(lower_constant),
152: 				                                      std::move(upper_constant), lower_inclusive, upper_inclusive);
153: 				callback(std::move(between));
154: 			} else if (lower_index.IsValid()) {
155: 				// only lower index found, create simple comparison expression
156: 				auto constant = make_uniq<BoundConstantExpression>(constant_list[lower_index.GetIndex()].constant);
157: 				auto comparison =
158: 				    make_uniq<BoundComparisonExpression>(constant_list[lower_index.GetIndex()].comparison_type,
159: 				                                         entries[i].get().Copy(), std::move(constant));
160: 				callback(std::move(comparison));
161: 			} else if (upper_index.IsValid()) {
162: 				// only upper index found, create simple comparison expression
163: 				auto constant = make_uniq<BoundConstantExpression>(constant_list[upper_index.GetIndex()].constant);
164: 				auto comparison =
165: 				    make_uniq<BoundComparisonExpression>(constant_list[upper_index.GetIndex()].comparison_type,
166: 				                                         entries[i].get().Copy(), std::move(constant));
167: 				callback(std::move(comparison));
168: 			}
169: 		}
170: 	}
171: 	stored_expressions.clear();
172: 	equivalence_set_map.clear();
173: 	constant_values.clear();
174: 	equivalence_map.clear();
175: }
176: 
177: bool FilterCombiner::HasFilters() {
178: 	bool has_filters = false;
179: 	GenerateFilters([&](unique_ptr<Expression> child) { has_filters = true; });
180: 	return has_filters;
181: }
182: 
183: // unordered_map<idx_t, std::pair<Value *, Value *>> MergeAnd(unordered_map<idx_t, std::pair<Value *, Value *>> &f_1,
184: //                                                            unordered_map<idx_t, std::pair<Value *, Value *>> &f_2) {
185: // 	unordered_map<idx_t, std::pair<Value *, Value *>> result;
186: // 	for (auto &f : f_1) {
187: // 		auto it = f_2.find(f.first);
188: // 		if (it == f_2.end()) {
189: // 			result[f.first] = f.second;
190: // 		} else {
191: // 			Value *min = nullptr, *max = nullptr;
192: // 			if (it->second.first && f.second.first) {
193: // 				if (*f.second.first > *it->second.first) {
194: // 					min = f.second.first;
195: // 				} else {
196: // 					min = it->second.first;
197: // 				}
198: 
199: // 			} else if (it->second.first) {
200: // 				min = it->second.first;
201: // 			} else if (f.second.first) {
202: // 				min = f.second.first;
203: // 			} else {
204: // 				min = nullptr;
205: // 			}
206: // 			if (it->second.second && f.second.second) {
207: // 				if (*f.second.second < *it->second.second) {
208: // 					max = f.second.second;
209: // 				} else {
210: // 					max = it->second.second;
211: // 				}
212: // 			} else if (it->second.second) {
213: // 				max = it->second.second;
214: // 			} else if (f.second.second) {
215: // 				max = f.second.second;
216: // 			} else {
217: // 				max = nullptr;
218: // 			}
219: // 			result[f.first] = {min, max};
220: // 			f_2.erase(f.first);
221: // 		}
222: // 	}
223: // 	for (auto &f : f_2) {
224: // 		result[f.first] = f.second;
225: // 	}
226: // 	return result;
227: // }
228: 
229: // unordered_map<idx_t, std::pair<Value *, Value *>> MergeOr(unordered_map<idx_t, std::pair<Value *, Value *>> &f_1,
230: //                                                           unordered_map<idx_t, std::pair<Value *, Value *>> &f_2) {
231: // 	unordered_map<idx_t, std::pair<Value *, Value *>> result;
232: // 	for (auto &f : f_1) {
233: // 		auto it = f_2.find(f.first);
234: // 		if (it != f_2.end()) {
235: // 			Value *min = nullptr, *max = nullptr;
236: // 			if (it->second.first && f.second.first) {
237: // 				if (*f.second.first < *it->second.first) {
238: // 					min = f.second.first;
239: // 				} else {
240: // 					min = it->second.first;
241: // 				}
242: // 			}
243: // 			if (it->second.second && f.second.second) {
244: // 				if (*f.second.second > *it->second.second) {
245: // 					max = f.second.second;
246: // 				} else {
247: // 					max = it->second.second;
248: // 				}
249: // 			}
250: // 			result[f.first] = {min, max};
251: // 			f_2.erase(f.first);
252: // 		}
253: // 	}
254: // 	return result;
255: // }
256: 
257: // unordered_map<idx_t, std::pair<Value *, Value *>>
258: // FilterCombiner::FindZonemapChecks(vector<idx_t> &column_ids, unordered_set<idx_t> &not_constants, Expression *filter)
259: // { 	unordered_map<idx_t, std::pair<Value *, Value *>> checks; 	switch (filter->type) { 	case
260: // ExpressionType::CONJUNCTION_OR: {
261: // 		//! For a filter to
262: // 		auto &or_exp = filter->Cast<BoundConjunctionExpression>();
263: // 		checks = FindZonemapChecks(column_ids, not_constants, or_exp.children[0].get());
264: // 		for (size_t i = 1; i < or_exp.children.size(); ++i) {
265: // 			auto child_check = FindZonemapChecks(column_ids, not_constants, or_exp.children[i].get());
266: // 			checks = MergeOr(checks, child_check);
267: // 		}
268: // 		return checks;
269: // 	}
270: // 	case ExpressionType::CONJUNCTION_AND: {
271: // 		auto &and_exp = filter->Cast<BoundConjunctionExpression>();
272: // 		checks = FindZonemapChecks(column_ids, not_constants, and_exp.children[0].get());
273: // 		for (size_t i = 1; i < and_exp.children.size(); ++i) {
274: // 			auto child_check = FindZonemapChecks(column_ids, not_constants, and_exp.children[i].get());
275: // 			checks = MergeAnd(checks, child_check);
276: // 		}
277: // 		return checks;
278: // 	}
279: // 	case ExpressionType::COMPARE_IN: {
280: // 		auto &comp_in_exp = filter->Cast<BoundOperatorExpression>();
281: // 		if (comp_in_exp.children[0]->type == ExpressionType::BOUND_COLUMN_REF) {
282: // 			Value *min = nullptr, *max = nullptr;
283: // 			auto &column_ref = comp_in_exp.children[0]->Cast<BoundColumnRefExpression>();
284: // 			for (size_t i {1}; i < comp_in_exp.children.size(); i++) {
285: // 				if (comp_in_exp.children[i]->type != ExpressionType::VALUE_CONSTANT) {
286: // 					//! This indicates the column has a comparison that is not with a constant
287: // 					not_constants.insert(column_ids[column_ref.binding.column_index]);
288: // 					break;
289: // 				} else {
290: // 					auto &const_value_expr = comp_in_exp.children[i]->Cast<BoundConstantExpression>();
291: // 					if (const_value_expr.value.IsNull()) {
292: // 						return checks;
293: // 					}
294: // 					if (!min && !max) {
295: // 						min = &const_value_expr.value;
296: // 						max = min;
297: // 					} else {
298: // 						if (*min > const_value_expr.value) {
299: // 							min = &const_value_expr.value;
300: // 						}
301: // 						if (*max < const_value_expr.value) {
302: // 							max = &const_value_expr.value;
303: // 						}
304: // 					}
305: // 				}
306: // 			}
307: // 			checks[column_ids[column_ref.binding.column_index]] = {min, max};
308: // 		}
309: // 		return checks;
310: // 	}
311: // 	case ExpressionType::COMPARE_EQUAL: {
312: // 		auto &comp_exp = filter->Cast<BoundComparisonExpression>();
313: // 		if ((comp_exp.left->expression_class == ExpressionClass::BOUND_COLUMN_REF &&
314: // 		     comp_exp.right->expression_class == ExpressionClass::BOUND_CONSTANT)) {
315: // 			auto &column_ref = comp_exp.left->Cast<BoundColumnRefExpression>();
316: // 			auto &constant_value_expr = comp_exp.right->Cast<BoundConstantExpression>();
317: // 			checks[column_ids[column_ref.binding.column_index]] = {&constant_value_expr.value,
318: // 			                                                       &constant_value_expr.value};
319: // 		}
320: // 		if ((comp_exp.left->expression_class == ExpressionClass::BOUND_CONSTANT &&
321: // 		     comp_exp.right->expression_class == ExpressionClass::BOUND_COLUMN_REF)) {
322: // 			auto &column_ref = comp_exp.right->Cast<BoundColumnRefExpression>();
323: // 			auto &constant_value_expr = comp_exp.left->Cast<BoundConstantExpression>();
324: // 			checks[column_ids[column_ref.binding.column_index]] = {&constant_value_expr.value,
325: // 			                                                       &constant_value_expr.value};
326: // 		}
327: // 		return checks;
328: // 	}
329: // 	case ExpressionType::COMPARE_LESSTHAN:
330: // 	case ExpressionType::COMPARE_LESSTHANOREQUALTO: {
331: // 		auto &comp_exp = filter->Cast<BoundComparisonExpression>();
332: // 		if ((comp_exp.left->expression_class == ExpressionClass::BOUND_COLUMN_REF &&
333: // 		     comp_exp.right->expression_class == ExpressionClass::BOUND_CONSTANT)) {
334: // 			auto &column_ref = comp_exp.left->Cast<BoundColumnRefExpression>();
335: // 			auto &constant_value_expr = comp_exp.right->Cast<BoundConstantExpression>();
336: // 			checks[column_ids[column_ref.binding.column_index]] = {nullptr, &constant_value_expr.value};
337: // 		}
338: // 		if ((comp_exp.left->expression_class == ExpressionClass::BOUND_CONSTANT &&
339: // 		     comp_exp.right->expression_class == ExpressionClass::BOUND_COLUMN_REF)) {
340: // 			auto &column_ref = comp_exp.right->Cast<BoundColumnRefExpression>();
341: // 			auto &constant_value_expr = comp_exp.left->Cast<BoundConstantExpression>();
342: // 			checks[column_ids[column_ref.binding.column_index]] = {&constant_value_expr.value, nullptr};
343: // 		}
344: // 		return checks;
345: // 	}
346: // 	case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
347: // 	case ExpressionType::COMPARE_GREATERTHAN: {
348: // 		auto &comp_exp = filter->Cast<BoundComparisonExpression>();
349: // 		if ((comp_exp.left->expression_class == ExpressionClass::BOUND_COLUMN_REF &&
350: // 		     comp_exp.right->expression_class == ExpressionClass::BOUND_CONSTANT)) {
351: // 			auto &column_ref = comp_exp.left->Cast<BoundColumnRefExpression>();
352: // 			auto &constant_value_expr = comp_exp.right->Cast<BoundConstantExpression>();
353: // 			checks[column_ids[column_ref.binding.column_index]] = {&constant_value_expr.value, nullptr};
354: // 		}
355: // 		if ((comp_exp.left->expression_class == ExpressionClass::BOUND_CONSTANT &&
356: // 		     comp_exp.right->expression_class == ExpressionClass::BOUND_COLUMN_REF)) {
357: // 			auto &column_ref = comp_exp.right->Cast<BoundColumnRefExpression>();
358: // 			auto &constant_value_expr = comp_exp.left->Cast<BoundConstantExpression>();
359: // 			checks[column_ids[column_ref.binding.column_index]] = {nullptr, &constant_value_expr.value};
360: // 		}
361: // 		return checks;
362: // 	}
363: // 	default:
364: // 		return checks;
365: // 	}
366: // }
367: 
368: // vector<TableFilter> FilterCombiner::GenerateZonemapChecks(vector<idx_t> &column_ids,
369: //                                                           vector<TableFilter> &pushed_filters) {
370: // 	vector<TableFilter> zonemap_checks;
371: // 	unordered_set<idx_t> not_constants;
372: // 	//! We go through the remaining filters and capture their min max
373: // 	if (remaining_filters.empty()) {
374: // 		return zonemap_checks;
375: // 	}
376: 
377: // 	auto checks = FindZonemapChecks(column_ids, not_constants, remaining_filters[0].get());
378: // 	for (size_t i = 1; i < remaining_filters.size(); ++i) {
379: // 		auto child_check = FindZonemapChecks(column_ids, not_constants, remaining_filters[i].get());
380: // 		checks = MergeAnd(checks, child_check);
381: // 	}
382: // 	//! We construct the equivalent filters
383: // 	for (auto not_constant : not_constants) {
384: // 		checks.erase(not_constant);
385: // 	}
386: // 	for (const auto &pushed_filter : pushed_filters) {
387: // 		checks.erase(column_ids[pushed_filter.column_index]);
388: // 	}
389: // 	for (const auto &check : checks) {
390: // 		if (check.second.first) {
391: // 			zonemap_checks.emplace_back(check.second.first->Copy(), ExpressionType::COMPARE_GREATERTHANOREQUALTO,
392: // 			                            check.first);
393: // 		}
394: // 		if (check.second.second) {
395: // 			zonemap_checks.emplace_back(check.second.second->Copy(), ExpressionType::COMPARE_LESSTHANOREQUALTO,
396: // 			                            check.first);
397: // 		}
398: // 	}
399: // 	return zonemap_checks;
400: // }
401: 
402: // Try to extract a column index from a bound column ref expression, or a column ref recursively nested
403: // inside of a struct_extract call. If the expression is not a column ref (or nested column ref), return false.
404: static bool TryGetBoundColumnIndex(const vector<ColumnIndex> &column_ids, const Expression &expr, ColumnIndex &result) {
405: 	switch (expr.GetExpressionType()) {
406: 	case ExpressionType::BOUND_COLUMN_REF: {
407: 		auto &ref = expr.Cast<BoundColumnRefExpression>();
408: 		result = column_ids[ref.binding.column_index];
409: 		return true;
410: 	}
411: 	case ExpressionType::BOUND_FUNCTION: {
412: 		auto &func = expr.Cast<BoundFunctionExpression>();
413: 		if (func.function.name == "struct_extract" || func.function.name == "struct_extract_at") {
414: 			auto &child_expr = func.children[0];
415: 			return TryGetBoundColumnIndex(column_ids, *child_expr, result);
416: 		}
417: 		return false;
418: 	}
419: 	default:
420: 		return false;
421: 	}
422: }
423: 
424: // Try to push down a filter into a expression by recursively wrapping any nested expressions in StructFilters.
425: // If the expression is not a struct_extract, return the inner_filter unchanged.
426: static unique_ptr<TableFilter> PushDownFilterIntoExpr(const Expression &expr, unique_ptr<TableFilter> inner_filter) {
427: 	if (expr.GetExpressionType() == ExpressionType::BOUND_FUNCTION) {
428: 		auto &func = expr.Cast<BoundFunctionExpression>();
429: 		auto &child_expr = func.children[0];
430: 		auto child_value = func.children[1]->Cast<BoundConstantExpression>().value;
431: 		if (func.function.name == "struct_extract") {
432: 			string child_name = child_value.GetValue<string>();
433: 			auto child_index = StructType::GetChildIndexUnsafe(func.children[0]->return_type, child_name);
434: 			inner_filter = make_uniq<StructFilter>(child_index, child_name, std::move(inner_filter));
435: 			return PushDownFilterIntoExpr(*child_expr, std::move(inner_filter));
436: 		} else if (func.function.name == "struct_extract_at") {
437: 			inner_filter = make_uniq<StructFilter>(child_value.GetValue<idx_t>() - 1, "", std::move(inner_filter));
438: 			return PushDownFilterIntoExpr(*child_expr, std::move(inner_filter));
439: 		}
440: 	}
441: 	return inner_filter;
442: }
443: 
444: bool FilterCombiner::ContainsNull(vector<Value> &in_list) {
445: 	for (idx_t i = 0; i < in_list.size(); i++) {
446: 		if (in_list[i].IsNull()) {
447: 			return true;
448: 		}
449: 	}
450: 	return false;
451: }
452: 
453: bool FilterCombiner::IsDenseRange(vector<Value> &in_list) {
454: 	if (in_list.empty()) {
455: 		return true;
456: 	}
457: 	if (!in_list[0].type().IsIntegral()) {
458: 		return false;
459: 	}
460: 	// sort the input list
461: 	sort(in_list.begin(), in_list.end());
462: 
463: 	// check if the gap between each value is exactly one
464: 	hugeint_t prev_value = in_list[0].GetValue<hugeint_t>();
465: 	for (idx_t i = 1; i < in_list.size(); i++) {
466: 		hugeint_t current_value = in_list[i].GetValue<hugeint_t>();
467: 		hugeint_t diff;
468: 		if (!TrySubtractOperator::Operation(current_value, prev_value, diff)) {
469: 			// if subtract would overflow then it's certainly not 1
470: 			return false;
471: 		}
472: 		if (diff != 1) {
473: 			// gap is not 1 - this is not a dense range
474: 			return false;
475: 		}
476: 		prev_value = current_value;
477: 	}
478: 	// dense range
479: 	return true;
480: }
481: 
482: TableFilterSet FilterCombiner::GenerateTableScanFilters(const vector<ColumnIndex> &column_ids) {
483: 	TableFilterSet table_filters;
484: 	//! First, we figure the filters that have constant expressions that we can push down to the table scan
485: 	for (auto &constant_value : constant_values) {
486: 		if (!constant_value.second.empty()) {
487: 			auto filter_exp = equivalence_map.end();
488: 			if ((constant_value.second[0].comparison_type == ExpressionType::COMPARE_EQUAL ||
489: 			     constant_value.second[0].comparison_type == ExpressionType::COMPARE_GREATERTHAN ||
490: 			     constant_value.second[0].comparison_type == ExpressionType::COMPARE_GREATERTHANOREQUALTO ||
491: 			     constant_value.second[0].comparison_type == ExpressionType::COMPARE_LESSTHAN ||
492: 			     constant_value.second[0].comparison_type == ExpressionType::COMPARE_LESSTHANOREQUALTO ||
493: 			     constant_value.second[0].comparison_type == ExpressionType::COMPARE_NOTEQUAL) &&
494: 			    (TypeIsNumeric(constant_value.second[0].constant.type().InternalType()) ||
495: 			     constant_value.second[0].constant.type().InternalType() == PhysicalType::VARCHAR ||
496: 			     constant_value.second[0].constant.type().InternalType() == PhysicalType::BOOL)) {
497: 				//! Here we check if these filters are column references
498: 				filter_exp = equivalence_map.find(constant_value.first);
499: 
500: 				if (filter_exp->second.size() != 1) {
501: 					continue;
502: 				}
503: 
504: 				auto &expr = filter_exp->second[0];
505: 				auto equiv_set = filter_exp->first;
506: 
507: 				// Try to get the column index, either from bound column ref, or a column ref nested inside of a
508: 				// struct_extract call
509: 				ColumnIndex column_index;
510: 				if (!TryGetBoundColumnIndex(column_ids, expr, column_index)) {
511: 					continue;
512: 				}
513: 
514: 				auto &constant_list = constant_values.find(equiv_set)->second;
515: 				for (auto &constant_cmp : constant_list) {
516: 					auto constant_filter =
517: 					    make_uniq<ConstantFilter>(constant_cmp.comparison_type, constant_cmp.constant);
518: 					table_filters.PushFilter(column_index, PushDownFilterIntoExpr(expr, std::move(constant_filter)));
519: 				}
520: 				equivalence_map.erase(filter_exp);
521: 			}
522: 		}
523: 	}
524: 	//! Here we look for LIKE or IN filters
525: 	for (idx_t rem_fil_idx = 0; rem_fil_idx < remaining_filters.size(); rem_fil_idx++) {
526: 		auto &remaining_filter = remaining_filters[rem_fil_idx];
527: 		if (remaining_filter->GetExpressionClass() == ExpressionClass::BOUND_FUNCTION) {
528: 			auto &func = remaining_filter->Cast<BoundFunctionExpression>();
529: 			if (func.function.name == "prefix" &&
530: 			    func.children[0]->GetExpressionClass() == ExpressionClass::BOUND_COLUMN_REF &&
531: 			    func.children[1]->GetExpressionType() == ExpressionType::VALUE_CONSTANT) {
532: 				//! This is a like function.
533: 				auto &column_ref = func.children[0]->Cast<BoundColumnRefExpression>();
534: 				auto &constant_value_expr = func.children[1]->Cast<BoundConstantExpression>();
535: 				auto like_string = StringValue::Get(constant_value_expr.value);
536: 				if (like_string.empty()) {
537: 					continue;
538: 				}
539: 				auto &column_index = column_ids[column_ref.binding.column_index];
540: 				//! Here the like must be transformed to a BOUND COMPARISON geq le
541: 				auto lower_bound =
542: 				    make_uniq<ConstantFilter>(ExpressionType::COMPARE_GREATERTHANOREQUALTO, Value(like_string));
543: 				like_string[like_string.size() - 1]++;
544: 				auto upper_bound = make_uniq<ConstantFilter>(ExpressionType::COMPARE_LESSTHAN, Value(like_string));
545: 				table_filters.PushFilter(column_index, std::move(lower_bound));
546: 				table_filters.PushFilter(column_index, std::move(upper_bound));
547: 			}
548: 			if (func.function.name == "~~" &&
549: 			    func.children[0]->GetExpressionClass() == ExpressionClass::BOUND_COLUMN_REF &&
550: 			    func.children[1]->GetExpressionType() == ExpressionType::VALUE_CONSTANT) {
551: 				//! This is a like function.
552: 				auto &column_ref = func.children[0]->Cast<BoundColumnRefExpression>();
553: 				auto &constant_value_expr = func.children[1]->Cast<BoundConstantExpression>();
554: 				auto &column_index = column_ids[column_ref.binding.column_index];
555: 				// constant value expr can sometimes be null. if so, push is not null filter, which will
556: 				// make the filter unsatisfiable and return no results.
557: 				if (constant_value_expr.value.IsNull()) {
558: 					auto is_not_null = make_uniq<IsNotNullFilter>();
559: 					table_filters.PushFilter(column_index, std::move(is_not_null));
560: 					continue;
561: 				}
562: 				auto &like_string = StringValue::Get(constant_value_expr.value);
563: 				if (like_string[0] == '%' || like_string[0] == '_') {
564: 					//! We have no prefix so nothing to pushdown
565: 					break;
566: 				}
567: 				string prefix;
568: 				bool equality = true;
569: 				for (char const &c : like_string) {
570: 					if (c == '%' || c == '_') {
571: 						equality = false;
572: 						break;
573: 					}
574: 					prefix += c;
575: 				}
576: 				if (equality) {
577: 					//! Here the like can be transformed to an equality query
578: 					auto equal_filter = make_uniq<ConstantFilter>(ExpressionType::COMPARE_EQUAL, Value(prefix));
579: 					table_filters.PushFilter(column_index, std::move(equal_filter));
580: 				} else {
581: 					//! Here the like must be transformed to a BOUND COMPARISON geq le
582: 					auto lower_bound =
583: 					    make_uniq<ConstantFilter>(ExpressionType::COMPARE_GREATERTHANOREQUALTO, Value(prefix));
584: 					prefix[prefix.size() - 1]++;
585: 					auto upper_bound = make_uniq<ConstantFilter>(ExpressionType::COMPARE_LESSTHAN, Value(prefix));
586: 					table_filters.PushFilter(column_index, std::move(lower_bound));
587: 					table_filters.PushFilter(column_index, std::move(upper_bound));
588: 				}
589: 			}
590: 		} else if (remaining_filter->GetExpressionType() == ExpressionType::COMPARE_IN) {
591: 			auto &func = remaining_filter->Cast<BoundOperatorExpression>();
592: 			D_ASSERT(func.children.size() > 1);
593: 			if (func.children[0]->GetExpressionClass() != ExpressionClass::BOUND_COLUMN_REF) {
594: 				continue;
595: 			}
596: 			auto &column_ref = func.children[0]->Cast<BoundColumnRefExpression>();
597: 			auto &column_index = column_ids[column_ref.binding.column_index];
598: 			if (column_index.IsRowIdColumn()) {
599: 				break;
600: 			}
601: 			//! check if all children are const expr
602: 			bool children_constant = true;
603: 			for (size_t i {1}; i < func.children.size(); i++) {
604: 				if (func.children[i]->GetExpressionType() != ExpressionType::VALUE_CONSTANT) {
605: 					children_constant = false;
606: 					break;
607: 				}
608: 				auto &const_value_expr = func.children[i]->Cast<BoundConstantExpression>();
609: 				if (const_value_expr.value.IsNull()) {
610: 					// cannot simplify NULL values
611: 					children_constant = false;
612: 					break;
613: 				}
614: 			}
615: 			if (!children_constant) {
616: 				continue;
617: 			}
618: 			auto &fst_const_value_expr = func.children[1]->Cast<BoundConstantExpression>();
619: 			auto &type = fst_const_value_expr.value.type();
620: 
621: 			if ((type.IsNumeric() || type.id() == LogicalTypeId::VARCHAR || type.id() == LogicalTypeId::BOOLEAN) &&
622: 			    func.children.size() == 2) {
623: 				auto bound_eq_comparison =
624: 				    make_uniq<ConstantFilter>(ExpressionType::COMPARE_EQUAL, fst_const_value_expr.value);
625: 				table_filters.PushFilter(column_index, std::move(bound_eq_comparison));
626: 				remaining_filters.erase_at(rem_fil_idx--); // decrement to stay on the same idx next iteration
627: 				continue;
628: 			}
629: 
630: 			//! Check if values are consecutive, if yes transform them to >= <= (only for integers)
631: 			// e.g. if we have x IN (1, 2, 3, 4, 5) we transform this into x >= 1 AND x <= 5
632: 			vector<Value> in_list;
633: 			for (idx_t i = 1; i < func.children.size(); i++) {
634: 				auto &const_value_expr = func.children[i]->Cast<BoundConstantExpression>();
635: 				D_ASSERT(!const_value_expr.value.IsNull());
636: 				in_list.push_back(const_value_expr.value);
637: 			}
638: 			if (type.IsIntegral() && IsDenseRange(in_list)) {
639: 				// dense range! turn this into x >= min AND x <= max
640: 				// IsDenseRange sorts in_list, so the front element is the min and the back element is the max
641: 				auto lower_bound =
642: 				    make_uniq<ConstantFilter>(ExpressionType::COMPARE_GREATERTHANOREQUALTO, std::move(in_list.front()));
643: 				auto upper_bound =
644: 				    make_uniq<ConstantFilter>(ExpressionType::COMPARE_LESSTHANOREQUALTO, std::move(in_list.back()));
645: 				table_filters.PushFilter(column_index, std::move(lower_bound));
646: 				table_filters.PushFilter(column_index, std::move(upper_bound));
647: 
648: 				remaining_filters.erase_at(rem_fil_idx--); // decrement to stay on the same idx next iteration
649: 			} else {
650: 				// if this is not a dense range we can push a zone-map filter
651: 				auto optional_filter = make_uniq<OptionalFilter>();
652: 				auto in_filter = make_uniq<InFilter>(std::move(in_list));
653: 				optional_filter->child_filter = std::move(in_filter);
654: 				table_filters.PushFilter(column_index, std::move(optional_filter));
655: 			}
656: 		}
657: 	}
658: 
659: 	for (idx_t rem_fil_idx = 0; rem_fil_idx < remaining_filters.size(); rem_fil_idx++) {
660: 		auto &remaining_filter = remaining_filters[rem_fil_idx];
661: 		if (remaining_filter->GetExpressionClass() == ExpressionClass::BOUND_CONJUNCTION) {
662: 			auto &conj = remaining_filter->Cast<BoundConjunctionExpression>();
663: 			if (conj.GetExpressionType() == ExpressionType::CONJUNCTION_OR) {
664: 				optional_idx column_id;
665: 				auto optional_filter = make_uniq<OptionalFilter>();
666: 				auto conj_filter = make_uniq<ConjunctionOrFilter>();
667: 				for (auto &child : conj.children) {
668: 					if (child->GetExpressionClass() != ExpressionClass::BOUND_COMPARISON) {
669: 						column_id.SetInvalid();
670: 						break;
671: 					}
672: 					optional_ptr<BoundColumnRefExpression> column_ref;
673: 					optional_ptr<BoundConstantExpression> const_val;
674: 					auto &comp = child->Cast<BoundComparisonExpression>();
675: 					bool invert = false;
676: 					if (comp.left->GetExpressionClass() == ExpressionClass::BOUND_COLUMN_REF &&
677: 					    comp.right->GetExpressionClass() == ExpressionClass::BOUND_CONSTANT) {
678: 						column_ref = comp.left->Cast<BoundColumnRefExpression>();
679: 						const_val = comp.right->Cast<BoundConstantExpression>();
680: 					} else if (comp.left->GetExpressionClass() == ExpressionClass::BOUND_CONSTANT &&
681: 					           comp.right->GetExpressionClass() == ExpressionClass::BOUND_COLUMN_REF) {
682: 						column_ref = comp.right->Cast<BoundColumnRefExpression>();
683: 						const_val = comp.left->Cast<BoundConstantExpression>();
684: 						invert = true;
685: 					} else {
686: 						// child of OR filter is not simple so we do not push the or filter down at all
687: 						column_id.SetInvalid();
688: 						break;
689: 					}
690: 
691: 					if (!column_id.IsValid()) {
692: 						auto &col_id = column_ids[column_ref->binding.column_index];
693: 						if (col_id.IsRowIdColumn()) {
694: 							break;
695: 						}
696: 						column_id = col_id.GetPrimaryIndex();
697: 					} else if (column_id.GetIndex() != column_ids[column_ref->binding.column_index].GetPrimaryIndex()) {
698: 						column_id.SetInvalid();
699: 						break;
700: 					}
701: 
702: 					if (const_val->value.type().IsTemporal()) {
703: 						column_id.SetInvalid();
704: 						break;
705: 					}
706: 					auto comparison_type =
707: 					    invert ? FlipComparisonExpression(comp.GetExpressionType()) : comp.GetExpressionType();
708: 					if (const_val->value.IsNull()) {
709: 						switch (comparison_type) {
710: 						case ExpressionType::COMPARE_DISTINCT_FROM: {
711: 							auto null_filter = make_uniq<IsNotNullFilter>();
712: 							conj_filter->child_filters.push_back(std::move(null_filter));
713: 							break;
714: 						}
715: 						case ExpressionType::COMPARE_NOT_DISTINCT_FROM: {
716: 							auto null_filter = make_uniq<IsNullFilter>();
717: 							conj_filter->child_filters.push_back(std::move(null_filter));
718: 							break;
719: 						}
720: 						// if any other comparison type (i.e EQUAL, NOT_EQUAL) do not push a table filter
721: 						default:
722: 							break;
723: 						}
724: 					} else {
725: 						auto const_filter = make_uniq<ConstantFilter>(comparison_type, const_val->value);
726: 						conj_filter->child_filters.push_back(std::move(const_filter));
727: 					}
728: 				}
729: 				if (column_id.IsValid()) {
730: 					optional_filter->child_filter = std::move(conj_filter);
731: 					table_filters.PushFilter(ColumnIndex(column_id.GetIndex()), std::move(optional_filter));
732: 				}
733: 			}
734: 		}
735: 	}
736: 
737: 	return table_filters;
738: }
739: 
740: static bool IsGreaterThan(ExpressionType type) {
741: 	return type == ExpressionType::COMPARE_GREATERTHAN || type == ExpressionType::COMPARE_GREATERTHANOREQUALTO;
742: }
743: 
744: static bool IsLessThan(ExpressionType type) {
745: 	return type == ExpressionType::COMPARE_LESSTHAN || type == ExpressionType::COMPARE_LESSTHANOREQUALTO;
746: }
747: 
748: FilterResult FilterCombiner::AddBoundComparisonFilter(Expression &expr) {
749: 	auto &comparison = expr.Cast<BoundComparisonExpression>();
750: 	if (comparison.GetExpressionType() != ExpressionType::COMPARE_LESSTHAN &&
751: 	    comparison.GetExpressionType() != ExpressionType::COMPARE_LESSTHANOREQUALTO &&
752: 	    comparison.GetExpressionType() != ExpressionType::COMPARE_GREATERTHAN &&
753: 	    comparison.GetExpressionType() != ExpressionType::COMPARE_GREATERTHANOREQUALTO &&
754: 	    comparison.GetExpressionType() != ExpressionType::COMPARE_EQUAL &&
755: 	    comparison.GetExpressionType() != ExpressionType::COMPARE_NOTEQUAL) {
756: 		// only support [>, >=, <, <=, ==, !=] expressions
757: 		return FilterResult::UNSUPPORTED;
758: 	}
759: 	// check if one of the sides is a scalar value
760: 	bool left_is_scalar = comparison.left->IsFoldable();
761: 	bool right_is_scalar = comparison.right->IsFoldable();
762: 	if (left_is_scalar || right_is_scalar) {
763: 		// comparison with scalar
764: 		auto &node = GetNode(left_is_scalar ? *comparison.right : *comparison.left);
765: 		idx_t equivalence_set = GetEquivalenceSet(node);
766: 		auto &scalar = left_is_scalar ? comparison.left : comparison.right;
767: 		Value constant_value;
768: 		if (!ExpressionExecutor::TryEvaluateScalar(context, *scalar, constant_value)) {
769: 			return FilterResult::UNSUPPORTED;
770: 		}
771: 		if (constant_value.IsNull()) {
772: 			// comparisons with null are always null (i.e. will never result in rows)
773: 			return FilterResult::UNSATISFIABLE;
774: 		}
775: 
776: 		// create the ExpressionValueInformation
777: 		ExpressionValueInformation info;
778: 		info.comparison_type =
779: 		    left_is_scalar ? FlipComparisonExpression(comparison.GetExpressionType()) : comparison.GetExpressionType();
780: 		info.constant = constant_value;
781: 
782: 		// get the current bucket of constant values
783: 		D_ASSERT(constant_values.find(equivalence_set) != constant_values.end());
784: 		auto &info_list = constant_values.find(equivalence_set)->second;
785: 		if (node.return_type != info.constant.type()) {
786: 			return FilterResult::UNSUPPORTED;
787: 		}
788: 		// check the existing constant comparisons to see if we can do any pruning
789: 		auto ret = AddConstantComparison(info_list, info);
790: 
791: 		auto &non_scalar = left_is_scalar ? *comparison.right : *comparison.left;
792: 		auto transitive_filter = FindTransitiveFilter(non_scalar);
793: 		if (transitive_filter != nullptr) {
794: 			// try to add transitive filters
795: 			if (AddTransitiveFilters(transitive_filter->Cast<BoundComparisonExpression>()) ==
796: 			    FilterResult::UNSUPPORTED) {
797: 				// in case of unsuccessful re-add filter into remaining ones
798: 				remaining_filters.push_back(std::move(transitive_filter));
799: 			}
800: 		}
801: 		return ret;
802: 	} else {
803: 		// comparison between two non-scalars
804: 		// only handle comparisons for now
805: 		if (expr.GetExpressionType() != ExpressionType::COMPARE_EQUAL) {
806: 			return FilterResult::UNSUPPORTED;
807: 		}
808: 		// get the LHS and RHS nodes
809: 		auto &left_node = GetNode(*comparison.left);
810: 		auto &right_node = GetNode(*comparison.right);
811: 		if (left_node.Equals(right_node)) {
812: 			return FilterResult::UNSUPPORTED;
813: 		}
814: 		// get the equivalence sets of the LHS and RHS
815: 		auto left_equivalence_set = GetEquivalenceSet(left_node);
816: 		auto right_equivalence_set = GetEquivalenceSet(right_node);
817: 		if (left_equivalence_set == right_equivalence_set) {
818: 			// this equality filter already exists, prune it
819: 			return FilterResult::SUCCESS;
820: 		}
821: 		// add the right bucket into the left bucket
822: 		D_ASSERT(equivalence_map.find(left_equivalence_set) != equivalence_map.end());
823: 		D_ASSERT(equivalence_map.find(right_equivalence_set) != equivalence_map.end());
824: 
825: 		auto &left_bucket = equivalence_map.find(left_equivalence_set)->second;
826: 		auto &right_bucket = equivalence_map.find(right_equivalence_set)->second;
827: 		for (auto &right_expr : right_bucket) {
828: 			// rewrite the equivalence set mapping for this node
829: 			equivalence_set_map[right_expr] = left_equivalence_set;
830: 			// add the node to the left bucket
831: 			left_bucket.push_back(right_expr);
832: 		}
833: 		// now add all constant values from the right bucket to the left bucket
834: 		D_ASSERT(constant_values.find(left_equivalence_set) != constant_values.end());
835: 		D_ASSERT(constant_values.find(right_equivalence_set) != constant_values.end());
836: 		auto &left_constant_bucket = constant_values.find(left_equivalence_set)->second;
837: 		auto &right_constant_bucket = constant_values.find(right_equivalence_set)->second;
838: 		for (auto &right_constant : right_constant_bucket) {
839: 			if (AddConstantComparison(left_constant_bucket, right_constant) == FilterResult::UNSATISFIABLE) {
840: 				return FilterResult::UNSATISFIABLE;
841: 			}
842: 		}
843: 	}
844: 	return FilterResult::SUCCESS;
845: }
846: 
847: FilterResult FilterCombiner::AddFilter(Expression &expr) {
848: 	if (expr.HasParameter()) {
849: 		return FilterResult::UNSUPPORTED;
850: 	}
851: 	if (expr.IsFoldable()) {
852: 		// scalar condition, evaluate it
853: 		Value result;
854: 		if (!ExpressionExecutor::TryEvaluateScalar(context, expr, result)) {
855: 			return FilterResult::UNSUPPORTED;
856: 		}
857: 		result = result.DefaultCastAs(LogicalType::BOOLEAN);
858: 		// check if the filter passes
859: 		if (result.IsNull() || !BooleanValue::Get(result)) {
860: 			// the filter does not pass the scalar test, create an empty result
861: 			return FilterResult::UNSATISFIABLE;
862: 		} else {
863: 			// the filter passes the scalar test, just remove the condition
864: 			return FilterResult::SUCCESS;
865: 		}
866: 	}
867: 	D_ASSERT(!expr.IsFoldable());
868: 	if (expr.GetExpressionClass() == ExpressionClass::BOUND_BETWEEN) {
869: 		auto &comparison = expr.Cast<BoundBetweenExpression>();
870: 		//! check if one of the sides is a scalar value
871: 		bool lower_is_scalar = comparison.lower->IsFoldable();
872: 		bool upper_is_scalar = comparison.upper->IsFoldable();
873: 		if (lower_is_scalar || upper_is_scalar) {
874: 			//! comparison with scalar - break apart
875: 			auto &node = GetNode(*comparison.input);
876: 			idx_t equivalence_set = GetEquivalenceSet(node);
877: 			auto result = FilterResult::UNSATISFIABLE;
878: 
879: 			if (lower_is_scalar) {
880: 				auto scalar = comparison.lower.get();
881: 				Value constant_value;
882: 				if (!ExpressionExecutor::TryEvaluateScalar(context, *scalar, constant_value)) {
883: 					return FilterResult::UNSUPPORTED;
884: 				}
885: 
886: 				// create the ExpressionValueInformation
887: 				ExpressionValueInformation info;
888: 				if (comparison.lower_inclusive) {
889: 					info.comparison_type = ExpressionType::COMPARE_GREATERTHANOREQUALTO;
890: 				} else {
891: 					info.comparison_type = ExpressionType::COMPARE_GREATERTHAN;
892: 				}
893: 				info.constant = constant_value;
894: 
895: 				// get the current bucket of constant values
896: 				D_ASSERT(constant_values.find(equivalence_set) != constant_values.end());
897: 				auto &info_list = constant_values.find(equivalence_set)->second;
898: 				// check the existing constant comparisons to see if we can do any pruning
899: 				result = AddConstantComparison(info_list, info);
900: 			} else {
901: 				D_ASSERT(upper_is_scalar);
902: 				const auto type = comparison.upper_inclusive ? ExpressionType::COMPARE_LESSTHANOREQUALTO
903: 				                                             : ExpressionType::COMPARE_LESSTHAN;
904: 				auto left = comparison.lower->Copy();
905: 				auto right = comparison.input->Copy();
906: 				auto lower_comp = make_uniq<BoundComparisonExpression>(type, std::move(left), std::move(right));
907: 				result = AddBoundComparisonFilter(*lower_comp);
908: 			}
909: 
910: 			//	 Stop if we failed
911: 			if (result != FilterResult::SUCCESS) {
912: 				return result;
913: 			}
914: 
915: 			if (upper_is_scalar) {
916: 				auto scalar = comparison.upper.get();
917: 				Value constant_value;
918: 				if (!ExpressionExecutor::TryEvaluateScalar(context, *scalar, constant_value)) {
919: 					return FilterResult::UNSUPPORTED;
920: 				}
921: 
922: 				// create the ExpressionValueInformation
923: 				ExpressionValueInformation info;
924: 				if (comparison.upper_inclusive) {
925: 					info.comparison_type = ExpressionType::COMPARE_LESSTHANOREQUALTO;
926: 				} else {
927: 					info.comparison_type = ExpressionType::COMPARE_LESSTHAN;
928: 				}
929: 				info.constant = constant_value;
930: 
931: 				// get the current bucket of constant values
932: 				D_ASSERT(constant_values.find(equivalence_set) != constant_values.end());
933: 				// check the existing constant comparisons to see if we can do any pruning
934: 				result = AddConstantComparison(constant_values.find(equivalence_set)->second, info);
935: 			} else {
936: 				D_ASSERT(lower_is_scalar);
937: 				const auto type = comparison.upper_inclusive ? ExpressionType::COMPARE_LESSTHANOREQUALTO
938: 				                                             : ExpressionType::COMPARE_LESSTHAN;
939: 				auto left = comparison.input->Copy();
940: 				auto right = comparison.upper->Copy();
941: 				auto upper_comp = make_uniq<BoundComparisonExpression>(type, std::move(left), std::move(right));
942: 				result = AddBoundComparisonFilter(*upper_comp);
943: 			}
944: 
945: 			return result;
946: 		}
947: 	} else if (expr.GetExpressionClass() == ExpressionClass::BOUND_COMPARISON) {
948: 		return AddBoundComparisonFilter(expr);
949: 	}
950: 	// only comparisons supported for now
951: 	return FilterResult::UNSUPPORTED;
952: }
953: 
954: /*
955:  * Create and add new transitive filters from a two non-scalar filter such as j > i, j >= i, j < i, and j <= i
956:  * It's missing to create another method to add transitive filters from scalar filters, e.g, i > 10
957:  */
958: FilterResult FilterCombiner::AddTransitiveFilters(BoundComparisonExpression &comparison, bool is_root) {
959: 	if (!IsGreaterThan(comparison.GetExpressionType()) && !IsLessThan(comparison.GetExpressionType())) {
960: 		return FilterResult::UNSUPPORTED;
961: 	}
962: 	// get the LHS and RHS nodes
963: 	auto &left_node = GetNode(*comparison.left);
964: 	reference<Expression> right_node = GetNode(*comparison.right);
965: 	// In case with filters like CAST(i) = j and i = 5 we replace the COLUMN_REF i with the constant 5
966: 	do {
967: 		if (right_node.get().GetExpressionType() != ExpressionType::OPERATOR_CAST) {
968: 			break;
969: 		}
970: 		auto &bound_cast_expr = right_node.get().Cast<BoundCastExpression>();
971: 		if (bound_cast_expr.child->GetExpressionType() != ExpressionType::BOUND_COLUMN_REF) {
972: 			break;
973: 		}
974: 		auto &col_ref = bound_cast_expr.child->Cast<BoundColumnRefExpression>();
975: 		for (auto &stored_exp : stored_expressions) {
976: 			reference<Expression> expr = stored_exp.first;
977: 			if (expr.get().GetExpressionType() == ExpressionType::OPERATOR_CAST) {
978: 				expr = *(right_node.get().Cast<BoundCastExpression>().child);
979: 			}
980: 			if (expr.get().GetExpressionType() != ExpressionType::BOUND_COLUMN_REF) {
981: 				continue;
982: 			}
983: 			auto &st_col_ref = expr.get().Cast<BoundColumnRefExpression>();
984: 			if (st_col_ref.binding != col_ref.binding) {
985: 				continue;
986: 			}
987: 			if (bound_cast_expr.return_type != stored_exp.second->return_type) {
988: 				continue;
989: 			}
990: 			bound_cast_expr.child = stored_exp.second->Copy();
991: 			right_node = GetNode(*bound_cast_expr.child);
992: 			break;
993: 		}
994: 	} while (false);
995: 
996: 	if (left_node.Equals(right_node)) {
997: 		return FilterResult::UNSUPPORTED;
998: 	}
999: 	// get the equivalence sets of the LHS and RHS
1000: 	idx_t left_equivalence_set = GetEquivalenceSet(left_node);
1001: 	idx_t right_equivalence_set = GetEquivalenceSet(right_node);
1002: 	if (left_equivalence_set == right_equivalence_set) {
1003: 		// this equality filter already exists, prune it
1004: 		return FilterResult::SUCCESS;
1005: 	}
1006: 
1007: 	vector<ExpressionValueInformation> &left_constants = constant_values.find(left_equivalence_set)->second;
1008: 	vector<ExpressionValueInformation> &right_constants = constant_values.find(right_equivalence_set)->second;
1009: 	bool is_successful = false;
1010: 	bool is_inserted = false;
1011: 	// read every constant filters already inserted for the right scalar variable
1012: 	// and see if we can create new transitive filters, e.g., there is already a filter i > 10,
1013: 	// suppose that we have now the j >= i, then we can infer a new filter j > 10
1014: 	for (const auto &right_constant : right_constants) {
1015: 		ExpressionValueInformation info;
1016: 		info.constant = right_constant.constant;
1017: 		// there is already an equality filter, e.g., i = 10
1018: 		if (right_constant.comparison_type == ExpressionType::COMPARE_EQUAL) {
1019: 			// create filter j [>, >=, <, <=] 10
1020: 			// suppose the new comparison is j >= i and we have already a filter i = 10,
1021: 			// then we create a new filter j >= 10
1022: 			// and the filter j >= i can be pruned by not adding it into the remaining filters
1023: 			info.comparison_type = comparison.GetExpressionType();
1024: 		} else if ((comparison.GetExpressionType() == ExpressionType::COMPARE_GREATERTHANOREQUALTO &&
1025: 		            IsGreaterThan(right_constant.comparison_type)) ||
1026: 		           (comparison.GetExpressionType() == ExpressionType::COMPARE_LESSTHANOREQUALTO &&
1027: 		            IsLessThan(right_constant.comparison_type))) {
1028: 			// filters (j >= i AND i [>, >=] 10) OR (j <= i AND i [<, <=] 10)
1029: 			// create filter j [>, >=] 10 and add the filter j [>=, <=] i into the remaining filters
1030: 			info.comparison_type = right_constant.comparison_type; // create filter j [>, >=, <, <=] 10
1031: 			if (!is_inserted) {
1032: 				// Add the filter j >= i in the remaing filters
1033: 				auto filter = make_uniq<BoundComparisonExpression>(comparison.GetExpressionType(),
1034: 				                                                   comparison.left->Copy(), comparison.right->Copy());
1035: 				remaining_filters.push_back(std::move(filter));
1036: 				is_inserted = true;
1037: 			}
1038: 		} else if ((comparison.GetExpressionType() == ExpressionType::COMPARE_GREATERTHAN &&
1039: 		            IsGreaterThan(right_constant.comparison_type)) ||
1040: 		           (comparison.GetExpressionType() == ExpressionType::COMPARE_LESSTHAN &&
1041: 		            IsLessThan(right_constant.comparison_type))) {
1042: 			// filters (j > i AND i [>, >=] 10) OR j < i AND i [<, <=] 10
1043: 			// create filter j [>, <] 10 and add the filter j [>, <] i into the remaining filters
1044: 			// the comparisons j > i and j < i are more restrictive
1045: 			info.comparison_type = comparison.GetExpressionType();
1046: 			if (!is_inserted) {
1047: 				// Add the filter j [>, <] i
1048: 				auto filter = make_uniq<BoundComparisonExpression>(comparison.GetExpressionType(),
1049: 				                                                   comparison.left->Copy(), comparison.right->Copy());
1050: 				remaining_filters.push_back(std::move(filter));
1051: 				is_inserted = true;
1052: 			}
1053: 		} else {
1054: 			// we cannot add a new filter
1055: 			continue;
1056: 		}
1057: 		// Add the new filer into the left set
1058: 		if (AddConstantComparison(left_constants, info) == FilterResult::UNSATISFIABLE) {
1059: 			return FilterResult::UNSATISFIABLE;
1060: 		}
1061: 		is_successful = true;
1062: 	}
1063: 	if (is_successful) {
1064: 		if (is_root) {
1065: 			// now check for remaining transitive filters from the left column
1066: 			auto transitive_filter = FindTransitiveFilter(*comparison.left);
1067: 			if (transitive_filter != nullptr) {
1068: 				// try to add transitive filters
1069: 				auto &transitive_cast = transitive_filter->Cast<BoundComparisonExpression>();
1070: 				if (AddTransitiveFilters(transitive_cast, false) == FilterResult::UNSUPPORTED) {
1071: 					// in case of unsuccessful re-add filter into remaining ones
1072: 					remaining_filters.push_back(std::move(transitive_filter));
1073: 				}
1074: 			}
1075: 		}
1076: 		return FilterResult::SUCCESS;
1077: 	}
1078: 
1079: 	return FilterResult::UNSUPPORTED;
1080: }
1081: 
1082: /*
1083:  * Find a transitive filter already inserted into the remaining filters
1084:  * Check for a match between the right column of bound comparisons and the expression,
1085:  * then removes the bound comparison from the remaining filters and returns it
1086:  */
1087: unique_ptr<Expression> FilterCombiner::FindTransitiveFilter(Expression &expr) {
1088: 	// We only check for bound column ref
1089: 	if (expr.GetExpressionType() != ExpressionType::BOUND_COLUMN_REF) {
1090: 		return nullptr;
1091: 	}
1092: 	for (idx_t i = 0; i < remaining_filters.size(); i++) {
1093: 		if (remaining_filters[i]->GetExpressionClass() == ExpressionClass::BOUND_COMPARISON) {
1094: 			auto &comparison = remaining_filters[i]->Cast<BoundComparisonExpression>();
1095: 			if (expr.Equals(*comparison.right) && comparison.GetExpressionType() != ExpressionType::COMPARE_NOTEQUAL) {
1096: 				auto filter = std::move(remaining_filters[i]);
1097: 				remaining_filters.erase_at(i);
1098: 				return filter;
1099: 			}
1100: 		}
1101: 	}
1102: 	return nullptr;
1103: }
1104: 
1105: ValueComparisonResult InvertValueComparisonResult(ValueComparisonResult result) {
1106: 	if (result == ValueComparisonResult::PRUNE_RIGHT) {
1107: 		return ValueComparisonResult::PRUNE_LEFT;
1108: 	}
1109: 	if (result == ValueComparisonResult::PRUNE_LEFT) {
1110: 		return ValueComparisonResult::PRUNE_RIGHT;
1111: 	}
1112: 	return result;
1113: }
1114: 
1115: ValueComparisonResult CompareValueInformation(ExpressionValueInformation &left, ExpressionValueInformation &right) {
1116: 	if (left.comparison_type == ExpressionType::COMPARE_EQUAL) {
1117: 		// left is COMPARE_EQUAL, we can either
1118: 		// (1) prune the right side or
1119: 		// (2) return UNSATISFIABLE
1120: 		bool prune_right_side = false;
1121: 		switch (right.comparison_type) {
1122: 		case ExpressionType::COMPARE_LESSTHAN:
1123: 			prune_right_side = left.constant < right.constant;
1124: 			break;
1125: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
1126: 			prune_right_side = left.constant <= right.constant;
1127: 			break;
1128: 		case ExpressionType::COMPARE_GREATERTHAN:
1129: 			prune_right_side = left.constant > right.constant;
1130: 			break;
1131: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
1132: 			prune_right_side = left.constant >= right.constant;
1133: 			break;
1134: 		case ExpressionType::COMPARE_NOTEQUAL:
1135: 			prune_right_side = left.constant != right.constant;
1136: 			break;
1137: 		default:
1138: 			D_ASSERT(right.comparison_type == ExpressionType::COMPARE_EQUAL);
1139: 			prune_right_side = left.constant == right.constant;
1140: 			break;
1141: 		}
1142: 		if (prune_right_side) {
1143: 			return ValueComparisonResult::PRUNE_RIGHT;
1144: 		} else {
1145: 			return ValueComparisonResult::UNSATISFIABLE_CONDITION;
1146: 		}
1147: 	} else if (right.comparison_type == ExpressionType::COMPARE_EQUAL) {
1148: 		// right is COMPARE_EQUAL
1149: 		return InvertValueComparisonResult(CompareValueInformation(right, left));
1150: 	} else if (left.comparison_type == ExpressionType::COMPARE_NOTEQUAL) {
1151: 		// left is COMPARE_NOTEQUAL, we can either
1152: 		// (1) prune the left side or
1153: 		// (2) not prune anything
1154: 		bool prune_left_side = false;
1155: 		switch (right.comparison_type) {
1156: 		case ExpressionType::COMPARE_LESSTHAN:
1157: 			prune_left_side = left.constant >= right.constant;
1158: 			break;
1159: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
1160: 			prune_left_side = left.constant > right.constant;
1161: 			break;
1162: 		case ExpressionType::COMPARE_GREATERTHAN:
1163: 			prune_left_side = left.constant <= right.constant;
1164: 			break;
1165: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
1166: 			prune_left_side = left.constant < right.constant;
1167: 			break;
1168: 		default:
1169: 			D_ASSERT(right.comparison_type == ExpressionType::COMPARE_NOTEQUAL);
1170: 			prune_left_side = left.constant == right.constant;
1171: 			break;
1172: 		}
1173: 		if (prune_left_side) {
1174: 			return ValueComparisonResult::PRUNE_LEFT;
1175: 		} else {
1176: 			return ValueComparisonResult::PRUNE_NOTHING;
1177: 		}
1178: 	} else if (right.comparison_type == ExpressionType::COMPARE_NOTEQUAL) {
1179: 		return InvertValueComparisonResult(CompareValueInformation(right, left));
1180: 	} else if (IsGreaterThan(left.comparison_type) && IsGreaterThan(right.comparison_type)) {
1181: 		// both comparisons are [>], we can either
1182: 		// (1) prune the left side or
1183: 		// (2) prune the right side
1184: 		if (left.constant > right.constant) {
1185: 			// left constant is more selective, prune right
1186: 			return ValueComparisonResult::PRUNE_RIGHT;
1187: 		} else if (left.constant < right.constant) {
1188: 			// right constant is more selective, prune left
1189: 			return ValueComparisonResult::PRUNE_LEFT;
1190: 		} else {
1191: 			// constants are equivalent
1192: 			// however we can still have the scenario where one is [>=] and the other is [>]
1193: 			// we want to prune the [>=] because [>] is more selective
1194: 			// if left is [>=] we prune the left, else we prune the right
1195: 			if (left.comparison_type == ExpressionType::COMPARE_GREATERTHANOREQUALTO) {
1196: 				return ValueComparisonResult::PRUNE_LEFT;
1197: 			} else {
1198: 				return ValueComparisonResult::PRUNE_RIGHT;
1199: 			}
1200: 		}
1201: 	} else if (IsLessThan(left.comparison_type) && IsLessThan(right.comparison_type)) {
1202: 		// both comparisons are [<], we can either
1203: 		// (1) prune the left side or
1204: 		// (2) prune the right side
1205: 		if (left.constant < right.constant) {
1206: 			// left constant is more selective, prune right
1207: 			return ValueComparisonResult::PRUNE_RIGHT;
1208: 		} else if (left.constant > right.constant) {
1209: 			// right constant is more selective, prune left
1210: 			return ValueComparisonResult::PRUNE_LEFT;
1211: 		} else {
1212: 			// constants are equivalent
1213: 			// however we can still have the scenario where one is [<=] and the other is [<]
1214: 			// we want to prune the [<=] because [<] is more selective
1215: 			// if left is [<=] we prune the left, else we prune the right
1216: 			if (left.comparison_type == ExpressionType::COMPARE_LESSTHANOREQUALTO) {
1217: 				return ValueComparisonResult::PRUNE_LEFT;
1218: 			} else {
1219: 				return ValueComparisonResult::PRUNE_RIGHT;
1220: 			}
1221: 		}
1222: 	} else if (IsLessThan(left.comparison_type)) {
1223: 		D_ASSERT(IsGreaterThan(right.comparison_type));
1224: 		// left is [<] and right is [>], in this case we can either
1225: 		// (1) prune nothing or
1226: 		// (2) return UNSATISFIABLE
1227: 		// the SMALLER THAN constant has to be greater than the BIGGER THAN constant
1228: 		if (left.constant >= right.constant) {
1229: 			return ValueComparisonResult::PRUNE_NOTHING;
1230: 		} else {
1231: 			return ValueComparisonResult::UNSATISFIABLE_CONDITION;
1232: 		}
1233: 	} else {
1234: 		// left is [>] and right is [<] or [!=]
1235: 		D_ASSERT(IsLessThan(right.comparison_type) && IsGreaterThan(left.comparison_type));
1236: 		return InvertValueComparisonResult(CompareValueInformation(right, left));
1237: 	}
1238: }
1239: //
1240: // void FilterCombiner::LookUpConjunctions(Expression *expr) {
1241: //	if (expr->GetExpressionType() == ExpressionType::CONJUNCTION_OR) {
1242: //		auto root_or_expr = (BoundConjunctionExpression *)expr;
1243: //		for (const auto &entry : map_col_conjunctions) {
1244: //			for (const auto &conjs_to_push : entry.second) {
1245: //				if (conjs_to_push->root_or->Equals(root_or_expr)) {
1246: //					return;
1247: //				}
1248: //			}
1249: //		}
1250: //
1251: //		cur_root_or = root_or_expr;
1252: //		cur_conjunction = root_or_expr;
1253: //		cur_colref_to_push = nullptr;
1254: //		if (!BFSLookUpConjunctions(cur_root_or)) {
1255: //			if (cur_colref_to_push) {
1256: //				auto entry = map_col_conjunctions.find(cur_colref_to_push);
1257: //				auto &vec_conjs_to_push = entry->second;
1258: //				if (vec_conjs_to_push.size() == 1) {
1259: //					map_col_conjunctions.erase(entry);
1260: //					return;
1261: //				}
1262: //				vec_conjs_to_push.pop_back();
1263: //			}
1264: //		}
1265: //		return;
1266: //	}
1267: //
1268: //	// Verify if the expression has a column already pushed down by other OR expression
1269: //	VerifyOrsToPush(*expr);
1270: //}
1271: //
1272: // bool FilterCombiner::BFSLookUpConjunctions(BoundConjunctionExpression *conjunction) {
1273: //	vector<BoundConjunctionExpression *> conjunctions_to_visit;
1274: //
1275: //	for (auto &child : conjunction->children) {
1276: //		switch (child->GetExpressionClass()) {
1277: //		case ExpressionClass::BOUND_CONJUNCTION: {
1278: //			auto child_conjunction = (BoundConjunctionExpression *)child.get();
1279: //			conjunctions_to_visit.emplace_back(child_conjunction);
1280: //			break;
1281: //		}
1282: //		case ExpressionClass::BOUND_COMPARISON: {
1283: //			if (!UpdateConjunctionFilter((BoundComparisonExpression *)child.get())) {
1284: //				return false;
1285: //			}
1286: //			break;
1287: //		}
1288: //		default: {
1289: //			return false;
1290: //		}
1291: //		}
1292: //	}
1293: //
1294: //	for (auto child_conjunction : conjunctions_to_visit) {
1295: //		cur_conjunction = child_conjunction;
1296: //		// traverse child conjunction
1297: //		if (!BFSLookUpConjunctions(child_conjunction)) {
1298: //			return false;
1299: //		}
1300: //	}
1301: //	return true;
1302: //}
1303: //
1304: // void FilterCombiner::VerifyOrsToPush(Expression &expr) {
1305: //	if (expr.type == ExpressionType::BOUND_COLUMN_REF) {
1306: //		auto colref = (BoundColumnRefExpression *)&expr;
1307: //		auto entry = map_col_conjunctions.find(colref);
1308: //		if (entry == map_col_conjunctions.end()) {
1309: //			return;
1310: //		}
1311: //		map_col_conjunctions.erase(entry);
1312: //	}
1313: //	ExpressionIterator::EnumerateChildren(expr, [&](Expression &child) { VerifyOrsToPush(child); });
1314: //}
1315: //
1316: // bool FilterCombiner::UpdateConjunctionFilter(BoundComparisonExpression *comparison_expr) {
1317: //	bool left_is_scalar = comparison_expr->left->IsFoldable();
1318: //	bool right_is_scalar = comparison_expr->right->IsFoldable();
1319: //
1320: //	Expression *non_scalar_expr;
1321: //	if (left_is_scalar || right_is_scalar) {
1322: //		// only support comparison with scalar
1323: //		non_scalar_expr = left_is_scalar ? comparison_expr->right.get() : comparison_expr->left.get();
1324: //
1325: //		if (non_scalar_expr->GetExpressionType() == ExpressionType::BOUND_COLUMN_REF) {
1326: //			return UpdateFilterByColumn((BoundColumnRefExpression *)non_scalar_expr, comparison_expr);
1327: //		}
1328: //	}
1329: //
1330: //	return false;
1331: //}
1332: //
1333: // bool FilterCombiner::UpdateFilterByColumn(BoundColumnRefExpression *column_ref,
1334: //                                          BoundComparisonExpression *comparison_expr) {
1335: //	if (cur_colref_to_push == nullptr) {
1336: //		cur_colref_to_push = column_ref;
1337: //
1338: //		auto or_conjunction = make_uniq<BoundConjunctionExpression>(ExpressionType::CONJUNCTION_OR);
1339: //		or_conjunction->children.emplace_back(comparison_expr->Copy());
1340: //
1341: //		unique_ptr<ConjunctionsToPush> conjs_to_push = make_uniq<ConjunctionsToPush>();
1342: //		conjs_to_push->conjunctions.emplace_back(std::move(or_conjunction));
1343: //		conjs_to_push->root_or = cur_root_or;
1344: //
1345: //		auto &&vec_col_conjs = map_col_conjunctions[column_ref];
1346: //		vec_col_conjs.emplace_back(std::move(conjs_to_push));
1347: //		vec_colref_insertion_order.emplace_back(column_ref);
1348: //		return true;
1349: //	}
1350: //
1351: //	auto entry = map_col_conjunctions.find(cur_colref_to_push);
1352: //	D_ASSERT(entry != map_col_conjunctions.end());
1353: //	auto &conjunctions_to_push = entry->second.back();
1354: //
1355: //	if (!cur_colref_to_push->Equals(column_ref)) {
1356: //		// check for multiple colunms in the same root OR node
1357: //		if (cur_root_or == cur_conjunction) {
1358: //			return false;
1359: //		}
1360: //		// found an AND using a different column, we should stop the look up
1361: //		if (cur_conjunction->GetExpressionType() == ExpressionType::CONJUNCTION_AND) {
1362: //			return false;
1363: //		}
1364: //
1365: //		// found a different column, AND conditions cannot be preserved anymore
1366: //		conjunctions_to_push->preserve_and = false;
1367: //		return true;
1368: //	}
1369: //
1370: //	auto &last_conjunction = conjunctions_to_push->conjunctions.back();
1371: //	if (cur_conjunction->GetExpressionType() == last_conjunction->GetExpressionType()) {
1372: //		last_conjunction->children.emplace_back(comparison_expr->Copy());
1373: //	} else {
1374: //		auto new_conjunction = make_uniq<BoundConjunctionExpression>(cur_conjunction->GetExpressionType());
1375: //		new_conjunction->children.emplace_back(comparison_expr->Copy());
1376: //		conjunctions_to_push->conjunctions.emplace_back(std::move(new_conjunction));
1377: //	}
1378: //	return true;
1379: //}
1380: //
1381: // void FilterCombiner::GenerateORFilters(TableFilterSet &table_filter, vector<idx_t> &column_ids) {
1382: //	for (const auto colref : vec_colref_insertion_order) {
1383: //		auto column_index = column_ids[colref->binding.column_index];
1384: //		if (column_index == COLUMN_IDENTIFIER_ROW_ID) {
1385: //			break;
1386: //		}
1387: //
1388: //		for (const auto &conjunctions_to_push : map_col_conjunctions[colref]) {
1389: //			// root OR filter to push into the TableFilter
1390: //			auto root_or_filter = make_uniq<ConjunctionOrFilter>();
1391: //			// variable to hold the last conjuntion filter pointer
1392: //			// the next filter will be added into it, i.e., we create a chain of conjunction filters
1393: //			ConjunctionFilter *last_conj_filter = root_or_filter.get();
1394: //
1395: //			for (auto &conjunction : conjunctions_to_push->conjunctions) {
1396: //				if (conjunction->GetExpressionType() == ExpressionType::CONJUNCTION_AND &&
1397: //				    conjunctions_to_push->preserve_and) {
1398: //					GenerateConjunctionFilter<ConjunctionAndFilter>(conjunction.get(), last_conj_filter);
1399: //				} else {
1400: //					GenerateConjunctionFilter<ConjunctionOrFilter>(conjunction.get(), last_conj_filter);
1401: //				}
1402: //			}
1403: //			table_filter.PushFilter(column_index, std::move(root_or_filter));
1404: //		}
1405: //	}
1406: //	map_col_conjunctions.clear();
1407: //	vec_colref_insertion_order.clear();
1408: //}
1409: 
1410: } // namespace duckdb
[end of src/optimizer/filter_combiner.cpp]
[start of src/parser/transform/expression/transform_columnref.cpp]
1: #include "duckdb/common/exception.hpp"
2: #include "duckdb/parser/expression/columnref_expression.hpp"
3: #include "duckdb/parser/expression/function_expression.hpp"
4: #include "duckdb/parser/expression/star_expression.hpp"
5: #include "duckdb/parser/transformer.hpp"
6: 
7: namespace duckdb {
8: 
9: QualifiedColumnName TransformQualifiedColumnName(duckdb_libpgquery::PGList &list) {
10: 	QualifiedColumnName result;
11: 	switch (list.length) {
12: 	case 1:
13: 		result.column = const_char_ptr_cast(list.head->data.ptr_value);
14: 		break;
15: 	case 2:
16: 		result.table = const_char_ptr_cast(list.head->data.ptr_value);
17: 		result.column = const_char_ptr_cast(list.head->next->data.ptr_value);
18: 		break;
19: 	case 3:
20: 		result.schema = const_char_ptr_cast(list.head->data.ptr_value);
21: 		result.table = const_char_ptr_cast(list.head->next->data.ptr_value);
22: 		result.column = const_char_ptr_cast(list.head->next->next->data.ptr_value);
23: 		break;
24: 	case 4:
25: 		result.catalog = const_char_ptr_cast(list.head->data.ptr_value);
26: 		result.schema = const_char_ptr_cast(list.head->next->data.ptr_value);
27: 		result.table = const_char_ptr_cast(list.head->next->next->data.ptr_value);
28: 		result.column = const_char_ptr_cast(list.head->next->next->next->data.ptr_value);
29: 		break;
30: 	default:
31: 		throw ParserException("Qualified column name must have between 1 and 4 elements");
32: 	}
33: 	return result;
34: }
35: 
36: unique_ptr<ParsedExpression> Transformer::TransformStarExpression(duckdb_libpgquery::PGAStar &star) {
37: 	auto result = make_uniq<StarExpression>(star.relation ? star.relation : string());
38: 	if (star.except_list) {
39: 		for (auto head = star.except_list->head; head; head = head->next) {
40: 			auto exclude_column_list = PGPointerCast<duckdb_libpgquery::PGList>(head->data.ptr_value);
41: 			auto exclude_column = TransformQualifiedColumnName(*exclude_column_list);
42: 			// qualified - add to exclude list
43: 			if (result->exclude_list.find(exclude_column) != result->exclude_list.end()) {
44: 				throw ParserException("Duplicate entry \"%s\" in EXCLUDE list", exclude_column.ToString());
45: 			}
46: 			result->exclude_list.insert(std::move(exclude_column));
47: 		}
48: 	}
49: 	if (star.replace_list) {
50: 		for (auto head = star.replace_list->head; head; head = head->next) {
51: 			auto list = PGPointerCast<duckdb_libpgquery::PGList>(head->data.ptr_value);
52: 			D_ASSERT(list->length == 2);
53: 			auto replace_expression =
54: 			    TransformExpression(PGPointerCast<duckdb_libpgquery::PGNode>(list->head->data.ptr_value));
55: 			auto value = PGPointerCast<duckdb_libpgquery::PGValue>(list->tail->data.ptr_value);
56: 			D_ASSERT(value->type == duckdb_libpgquery::T_PGString);
57: 			string replace_entry = value->val.str;
58: 			if (result->replace_list.find(replace_entry) != result->replace_list.end()) {
59: 				throw ParserException("Duplicate entry \"%s\" in REPLACE list", replace_entry);
60: 			}
61: 			if (result->exclude_list.find(QualifiedColumnName(replace_entry)) != result->exclude_list.end()) {
62: 				throw ParserException("Column \"%s\" cannot occur in both EXCLUDE and REPLACE list", replace_entry);
63: 			}
64: 			result->replace_list.insert(make_pair(std::move(replace_entry), std::move(replace_expression)));
65: 		}
66: 	}
67: 	if (star.rename_list) {
68: 		for (auto head = star.rename_list->head; head; head = head->next) {
69: 			auto list = PGPointerCast<duckdb_libpgquery::PGList>(head->data.ptr_value);
70: 			D_ASSERT(list->length == 2);
71: 			auto rename_column_list = PGPointerCast<duckdb_libpgquery::PGList>(list->head->data.ptr_value);
72: 			auto rename_column = TransformQualifiedColumnName(*rename_column_list);
73: 			string new_name = char_ptr_cast(list->tail->data.ptr_value);
74: 			if (result->rename_list.find(rename_column) != result->rename_list.end()) {
75: 				throw ParserException("Duplicate entry \"%s\" in EXCLUDE list", rename_column.ToString());
76: 			}
77: 			if (result->exclude_list.find(rename_column) != result->exclude_list.end()) {
78: 				throw ParserException("Column \"%s\" cannot occur in both EXCLUDE and RENAME list",
79: 				                      rename_column.ToString());
80: 			}
81: 			if (result->replace_list.find(rename_column.column) != result->replace_list.end()) {
82: 				throw ParserException("Column \"%s\" cannot occur in both REPLACE and RENAME list",
83: 				                      rename_column.ToString());
84: 			}
85: 			result->rename_list.insert(make_pair(std::move(rename_column), std::move(new_name)));
86: 		}
87: 	}
88: 	if (star.expr) {
89: 		D_ASSERT(star.columns);
90: 		D_ASSERT(result->relation_name.empty());
91: 		D_ASSERT(result->exclude_list.empty());
92: 		D_ASSERT(result->replace_list.empty());
93: 		result->expr = TransformExpression(star.expr);
94: 		if (StarExpression::IsStar(*result->expr)) {
95: 			auto &child_star = result->expr->Cast<StarExpression>();
96: 			result->relation_name = child_star.relation_name;
97: 			result->exclude_list = std::move(child_star.exclude_list);
98: 			result->replace_list = std::move(child_star.replace_list);
99: 			result->expr.reset();
100: 		} else if (result->expr->GetExpressionType() == ExpressionType::LAMBDA) {
101: 			vector<unique_ptr<ParsedExpression>> children;
102: 			children.push_back(make_uniq<StarExpression>());
103: 			children.push_back(std::move(result->expr));
104: 			auto list_filter = make_uniq<FunctionExpression>("list_filter", std::move(children));
105: 			result->expr = std::move(list_filter);
106: 		}
107: 	}
108: 	result->columns = star.columns;
109: 	result->unpacked = star.unpacked;
110: 	SetQueryLocation(*result, star.location);
111: 	return std::move(result);
112: }
113: 
114: unique_ptr<ParsedExpression> Transformer::TransformColumnRef(duckdb_libpgquery::PGColumnRef &root) {
115: 	auto fields = root.fields;
116: 	auto head_node = PGPointerCast<duckdb_libpgquery::PGNode>(fields->head->data.ptr_value);
117: 	switch (head_node->type) {
118: 	case duckdb_libpgquery::T_PGString: {
119: 		if (fields->length < 1) {
120: 			throw InternalException("Unexpected field length");
121: 		}
122: 		vector<string> column_names;
123: 		for (auto node = fields->head; node; node = node->next) {
124: 			column_names.emplace_back(PGPointerCast<duckdb_libpgquery::PGValue>(node->data.ptr_value)->val.str);
125: 		}
126: 		auto colref = make_uniq<ColumnRefExpression>(std::move(column_names));
127: 		SetQueryLocation(*colref, root.location);
128: 		return std::move(colref);
129: 	}
130: 	case duckdb_libpgquery::T_PGAStar: {
131: 		return TransformStarExpression(PGCast<duckdb_libpgquery::PGAStar>(*head_node));
132: 	}
133: 	default:
134: 		throw NotImplementedException("ColumnRef not implemented!");
135: 	}
136: }
137: 
138: } // namespace duckdb
[end of src/parser/transform/expression/transform_columnref.cpp]
[start of src/planner/binder/statement/bind_create.cpp]
1: #include "duckdb/catalog/catalog.hpp"
2: #include "duckdb/catalog/catalog_entry/duck_table_entry.hpp"
3: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
4: #include "duckdb/catalog/catalog_entry/type_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_search_path.hpp"
6: #include "duckdb/catalog/duck_catalog.hpp"
7: #include "duckdb/function/scalar_macro_function.hpp"
8: #include "duckdb/function/table/table_scan.hpp"
9: #include "duckdb/main/attached_database.hpp"
10: #include "duckdb/main/client_context.hpp"
11: #include "duckdb/main/client_data.hpp"
12: #include "duckdb/main/database.hpp"
13: #include "duckdb/main/database_manager.hpp"
14: #include "duckdb/main/secret/secret_manager.hpp"
15: #include "duckdb/parser/constraints/foreign_key_constraint.hpp"
16: #include "duckdb/parser/constraints/list.hpp"
17: #include "duckdb/parser/constraints/unique_constraint.hpp"
18: #include "duckdb/parser/expression/constant_expression.hpp"
19: #include "duckdb/parser/expression/function_expression.hpp"
20: #include "duckdb/parser/expression/subquery_expression.hpp"
21: #include "duckdb/parser/parsed_data/create_index_info.hpp"
22: #include "duckdb/parser/parsed_data/create_macro_info.hpp"
23: #include "duckdb/parser/parsed_data/create_secret_info.hpp"
24: #include "duckdb/parser/parsed_data/create_view_info.hpp"
25: #include "duckdb/parser/parsed_expression_iterator.hpp"
26: #include "duckdb/parser/statement/create_statement.hpp"
27: #include "duckdb/parser/tableref/basetableref.hpp"
28: #include "duckdb/parser/tableref/table_function_ref.hpp"
29: #include "duckdb/planner/binder.hpp"
30: #include "duckdb/planner/bound_query_node.hpp"
31: #include "duckdb/planner/expression/bound_cast_expression.hpp"
32: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
33: #include "duckdb/planner/expression_binder/index_binder.hpp"
34: #include "duckdb/planner/expression_binder/select_binder.hpp"
35: #include "duckdb/planner/operator/logical_create.hpp"
36: #include "duckdb/planner/operator/logical_create_table.hpp"
37: #include "duckdb/planner/operator/logical_get.hpp"
38: #include "duckdb/planner/operator/logical_projection.hpp"
39: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
40: #include "duckdb/planner/query_node/bound_select_node.hpp"
41: #include "duckdb/planner/tableref/bound_basetableref.hpp"
42: #include "duckdb/storage/storage_extension.hpp"
43: #include "duckdb/common/extension_type_info.hpp"
44: #include "duckdb/common/type_visitor.hpp"
45: 
46: namespace duckdb {
47: 
48: void Binder::BindSchemaOrCatalog(ClientContext &context, string &catalog, string &schema) {
49: 	if (catalog.empty() && !schema.empty()) {
50: 		// schema is specified - but catalog is not
51: 		// try searching for the catalog instead
52: 		auto &db_manager = DatabaseManager::Get(context);
53: 		auto database = db_manager.GetDatabase(context, schema);
54: 		if (database) {
55: 			// we have a database with this name
56: 			// check if there is a schema
57: 			auto &search_path = *context.client_data->catalog_search_path;
58: 			auto catalog_names = search_path.GetCatalogsForSchema(schema);
59: 			if (catalog_names.empty()) {
60: 				catalog_names.push_back(DatabaseManager::GetDefaultDatabase(context));
61: 			}
62: 			for (auto &catalog_name : catalog_names) {
63: 				auto &catalog = Catalog::GetCatalog(context, catalog_name);
64: 				if (catalog.CheckAmbiguousCatalogOrSchema(context, schema)) {
65: 					throw BinderException(
66: 					    "Ambiguous reference to catalog or schema \"%s\" - use a fully qualified path like \"%s.%s\"",
67: 					    schema, catalog_name, schema);
68: 				}
69: 			}
70: 			catalog = schema;
71: 			schema = string();
72: 		}
73: 	}
74: }
75: 
76: void Binder::BindSchemaOrCatalog(string &catalog, string &schema) {
77: 	BindSchemaOrCatalog(context, catalog, schema);
78: }
79: 
80: const string Binder::BindCatalog(string &catalog) {
81: 	auto &db_manager = DatabaseManager::Get(context);
82: 	optional_ptr<AttachedDatabase> database = db_manager.GetDatabase(context, catalog);
83: 	if (database) {
84: 		return db_manager.GetDatabase(context, catalog).get()->GetName();
85: 	} else {
86: 		return db_manager.GetDefaultDatabase(context);
87: 	}
88: }
89: 
90: SchemaCatalogEntry &Binder::BindSchema(CreateInfo &info) {
91: 	BindSchemaOrCatalog(info.catalog, info.schema);
92: 	if (IsInvalidCatalog(info.catalog) && info.temporary) {
93: 		info.catalog = TEMP_CATALOG;
94: 	}
95: 	auto &search_path = ClientData::Get(context).catalog_search_path;
96: 	if (IsInvalidCatalog(info.catalog) && IsInvalidSchema(info.schema)) {
97: 		auto &default_entry = search_path->GetDefault();
98: 		info.catalog = default_entry.catalog;
99: 		info.schema = default_entry.schema;
100: 	} else if (IsInvalidSchema(info.schema)) {
101: 		info.schema = search_path->GetDefaultSchema(info.catalog);
102: 	} else if (IsInvalidCatalog(info.catalog)) {
103: 		info.catalog = search_path->GetDefaultCatalog(info.schema);
104: 	}
105: 	if (IsInvalidCatalog(info.catalog)) {
106: 		info.catalog = DatabaseManager::GetDefaultDatabase(context);
107: 	}
108: 	if (!info.temporary) {
109: 		// non-temporary create: not read only
110: 		if (info.catalog == TEMP_CATALOG) {
111: 			throw ParserException("Only TEMPORARY table names can use the \"%s\" catalog", TEMP_CATALOG);
112: 		}
113: 	} else {
114: 		if (info.catalog != TEMP_CATALOG) {
115: 			throw ParserException("TEMPORARY table names can *only* use the \"%s\" catalog", TEMP_CATALOG);
116: 		}
117: 	}
118: 	// fetch the schema in which we want to create the object
119: 	auto &schema_obj = Catalog::GetSchema(context, info.catalog, info.schema);
120: 	D_ASSERT(schema_obj.type == CatalogType::SCHEMA_ENTRY);
121: 	info.schema = schema_obj.name;
122: 	if (!info.temporary) {
123: 		auto &properties = GetStatementProperties();
124: 		properties.RegisterDBModify(schema_obj.catalog, context);
125: 	}
126: 	return schema_obj;
127: }
128: 
129: SchemaCatalogEntry &Binder::BindCreateSchema(CreateInfo &info) {
130: 	auto &schema = BindSchema(info);
131: 	if (schema.catalog.IsSystemCatalog()) {
132: 		throw BinderException("Cannot create entry in system catalog");
133: 	}
134: 	return schema;
135: }
136: 
137: void Binder::SetCatalogLookupCallback(catalog_entry_callback_t callback) {
138: 	entry_retriever.SetCallback(std::move(callback));
139: }
140: 
141: void Binder::BindCreateViewInfo(CreateViewInfo &base) {
142: 	// bind the view as if it were a query so we can catch errors
143: 	// note that we bind the original, and replace the original with a copy
144: 	auto view_binder = Binder::CreateBinder(context);
145: 	auto &dependencies = base.dependencies;
146: 	auto &catalog = Catalog::GetCatalog(context, base.catalog);
147: 
148: 	auto &db_config = DBConfig::GetConfig(context);
149: 	bool should_create_dependencies = db_config.GetSetting<EnableViewDependenciesSetting>(context);
150: 	if (should_create_dependencies) {
151: 		view_binder->SetCatalogLookupCallback([&dependencies, &catalog](CatalogEntry &entry) {
152: 			if (&catalog != &entry.ParentCatalog()) {
153: 				// Don't register dependencies between catalogs
154: 				return;
155: 			}
156: 			dependencies.AddDependency(entry);
157: 		});
158: 	}
159: 	view_binder->can_contain_nulls = true;
160: 
161: 	auto copy = base.query->Copy();
162: 	auto query_node = view_binder->Bind(*base.query);
163: 	base.query = unique_ptr_cast<SQLStatement, SelectStatement>(std::move(copy));
164: 	if (base.aliases.size() > query_node.names.size()) {
165: 		throw BinderException("More VIEW aliases than columns in query result");
166: 	}
167: 	base.types = query_node.types;
168: 	base.names = query_node.names;
169: }
170: 
171: SchemaCatalogEntry &Binder::BindCreateFunctionInfo(CreateInfo &info) {
172: 	auto &base = info.Cast<CreateMacroInfo>();
173: 
174: 	auto &dependencies = base.dependencies;
175: 	auto &catalog = Catalog::GetCatalog(context, info.catalog);
176: 	auto &db_config = DBConfig::GetConfig(context);
177: 	// try to bind each of the included functions
178: 	unordered_set<idx_t> positional_parameters;
179: 	for (auto &function : base.macros) {
180: 		auto &scalar_function = function->Cast<ScalarMacroFunction>();
181: 		if (scalar_function.expression->HasParameter()) {
182: 			throw BinderException("Parameter expressions within macro's are not supported!");
183: 		}
184: 		vector<LogicalType> dummy_types;
185: 		vector<string> dummy_names;
186: 		auto parameter_count = function->parameters.size();
187: 		if (positional_parameters.find(parameter_count) != positional_parameters.end()) {
188: 			throw BinderException(
189: 			    "Ambiguity in macro overloads - macro \"%s\" has multiple definitions with %llu parameters", base.name,
190: 			    parameter_count);
191: 		}
192: 		positional_parameters.insert(parameter_count);
193: 
194: 		// positional parameters
195: 		for (auto &param_expr : function->parameters) {
196: 			auto param = param_expr->Cast<ColumnRefExpression>();
197: 			if (param.IsQualified()) {
198: 				throw BinderException("Invalid parameter name '%s': must be unqualified", param.ToString());
199: 			}
200: 			dummy_types.emplace_back(LogicalType::UNKNOWN);
201: 			dummy_names.push_back(param.GetColumnName());
202: 		}
203: 		// default parameters
204: 		for (auto &entry : function->default_parameters) {
205: 			auto &val = entry.second->Cast<ConstantExpression>();
206: 			dummy_types.push_back(val.value.type());
207: 			dummy_names.push_back(entry.first);
208: 		}
209: 		auto this_macro_binding = make_uniq<DummyBinding>(dummy_types, dummy_names, base.name);
210: 		macro_binding = this_macro_binding.get();
211: 
212: 		// create a copy of the expression because we do not want to alter the original
213: 		auto expression = scalar_function.expression->Copy();
214: 		ExpressionBinder::QualifyColumnNames(*this, expression);
215: 
216: 		// bind it to verify the function was defined correctly
217: 		BoundSelectNode sel_node;
218: 		BoundGroupInformation group_info;
219: 		SelectBinder binder(*this, context, sel_node, group_info);
220: 		bool should_create_dependencies = db_config.GetSetting<EnableMacroDependenciesSetting>(context);
221: 
222: 		if (should_create_dependencies) {
223: 			binder.SetCatalogLookupCallback([&dependencies, &catalog](CatalogEntry &entry) {
224: 				if (&catalog != &entry.ParentCatalog()) {
225: 					// Don't register any cross-catalog dependencies
226: 					return;
227: 				}
228: 				// Register any catalog entry required to bind the macro function
229: 				dependencies.AddDependency(entry);
230: 			});
231: 		}
232: 		ErrorData error;
233: 		try {
234: 			error = binder.Bind(expression, 0, false);
235: 			if (error.HasError()) {
236: 				error.Throw();
237: 			}
238: 		} catch (const std::exception &ex) {
239: 			error = ErrorData(ex);
240: 		}
241: 		// if we cannot resolve parameters we postpone binding until the macro function is used
242: 		if (error.HasError() && error.Type() != ExceptionType::PARAMETER_NOT_RESOLVED) {
243: 			error.Throw();
244: 		}
245: 	}
246: 
247: 	return BindCreateSchema(info);
248: }
249: 
250: static bool IsValidUserType(optional_ptr<CatalogEntry> entry) {
251: 	if (!entry) {
252: 		return false;
253: 	}
254: 	return entry->Cast<TypeCatalogEntry>().user_type.id() != LogicalTypeId::INVALID;
255: }
256: 
257: LogicalType Binder::BindLogicalTypeInternal(const LogicalType &type, optional_ptr<Catalog> catalog,
258:                                             const string &schema) {
259: 	if (type.id() != LogicalTypeId::USER) {
260: 		// Nested type, make sure to bind any nested user types recursively
261: 		LogicalType result;
262: 		switch (type.id()) {
263: 		case LogicalTypeId::LIST: {
264: 			auto child_type = BindLogicalTypeInternal(ListType::GetChildType(type), catalog, schema);
265: 			result = LogicalType::LIST(child_type);
266: 			break;
267: 		}
268: 		case LogicalTypeId::MAP: {
269: 			auto key_type = BindLogicalTypeInternal(MapType::KeyType(type), catalog, schema);
270: 			auto value_type = BindLogicalTypeInternal(MapType::ValueType(type), catalog, schema);
271: 			result = LogicalType::MAP(std::move(key_type), std::move(value_type));
272: 			break;
273: 		}
274: 		case LogicalTypeId::ARRAY: {
275: 			auto child_type = BindLogicalTypeInternal(ArrayType::GetChildType(type), catalog, schema);
276: 			auto array_size = ArrayType::GetSize(type);
277: 			result = LogicalType::ARRAY(child_type, array_size);
278: 			break;
279: 		}
280: 		case LogicalTypeId::STRUCT: {
281: 			auto child_types = StructType::GetChildTypes(type);
282: 			child_list_t<LogicalType> new_child_types;
283: 			for (auto &entry : child_types) {
284: 				new_child_types.emplace_back(entry.first, BindLogicalTypeInternal(entry.second, catalog, schema));
285: 			}
286: 			result = LogicalType::STRUCT(std::move(new_child_types));
287: 			break;
288: 		}
289: 		case LogicalTypeId::UNION: {
290: 			child_list_t<LogicalType> member_types;
291: 			for (idx_t i = 0; i < UnionType::GetMemberCount(type); i++) {
292: 				auto child_type = BindLogicalTypeInternal(UnionType::GetMemberType(type, i), catalog, schema);
293: 				member_types.emplace_back(UnionType::GetMemberName(type, i), std::move(child_type));
294: 			}
295: 			result = LogicalType::UNION(std::move(member_types));
296: 			break;
297: 		}
298: 		default:
299: 			return type;
300: 		}
301: 
302: 		// Set the alias and extension info back
303: 		result.SetAlias(type.GetAlias());
304: 		auto ext_info = type.HasExtensionInfo() ? make_uniq<ExtensionTypeInfo>(*type.GetExtensionInfo()) : nullptr;
305: 		result.SetExtensionInfo(std::move(ext_info));
306: 		return result;
307: 	}
308: 
309: 	// User type, bind the user type
310: 	auto user_type_name = UserType::GetTypeName(type);
311: 	auto user_type_schema = UserType::GetSchema(type);
312: 	auto user_type_mods = UserType::GetTypeModifiers(type);
313: 
314: 	bind_logical_type_function_t user_bind_modifiers_func = nullptr;
315: 
316: 	LogicalType result;
317: 	if (catalog) {
318: 		// The search order is:
319: 		// 1) In the explicitly set schema (my_schema.my_type)
320: 		// 2) In the same schema as the table
321: 		// 3) In the same catalog
322: 		// 4) System catalog
323: 
324: 		optional_ptr<CatalogEntry> entry = nullptr;
325: 		if (!user_type_schema.empty()) {
326: 			entry = entry_retriever.GetEntry(CatalogType::TYPE_ENTRY, *catalog, user_type_schema, user_type_name,
327: 			                                 OnEntryNotFound::RETURN_NULL);
328: 		}
329: 		if (!IsValidUserType(entry)) {
330: 			entry = entry_retriever.GetEntry(CatalogType::TYPE_ENTRY, *catalog, schema, user_type_name,
331: 			                                 OnEntryNotFound::RETURN_NULL);
332: 		}
333: 		if (!IsValidUserType(entry)) {
334: 			entry = entry_retriever.GetEntry(CatalogType::TYPE_ENTRY, *catalog, INVALID_SCHEMA, user_type_name,
335: 			                                 OnEntryNotFound::RETURN_NULL);
336: 		}
337: 		if (!IsValidUserType(entry)) {
338: 			entry = entry_retriever.GetEntry(CatalogType::TYPE_ENTRY, INVALID_CATALOG, INVALID_SCHEMA, user_type_name,
339: 			                                 OnEntryNotFound::THROW_EXCEPTION);
340: 		}
341: 		auto &type_entry = entry->Cast<TypeCatalogEntry>();
342: 		result = type_entry.user_type;
343: 		user_bind_modifiers_func = type_entry.bind_function;
344: 	} else {
345: 		string type_catalog = UserType::GetCatalog(type);
346: 		string type_schema = UserType::GetSchema(type);
347: 
348: 		BindSchemaOrCatalog(context, type_catalog, type_schema);
349: 		auto entry = entry_retriever.GetEntry(CatalogType::TYPE_ENTRY, type_catalog, type_schema, user_type_name);
350: 		auto &type_entry = entry->Cast<TypeCatalogEntry>();
351: 		result = type_entry.user_type;
352: 		user_bind_modifiers_func = type_entry.bind_function;
353: 	}
354: 
355: 	// Now we bind the inner user type
356: 	BindLogicalType(result, catalog, schema);
357: 
358: 	// Apply the type modifiers (if any)
359: 	if (user_bind_modifiers_func) {
360: 		// If an explicit bind_modifiers function was provided, use that to construct the type
361: 
362: 		BindLogicalTypeInput input {context, result, user_type_mods};
363: 		result = user_bind_modifiers_func(input);
364: 	} else {
365: 		if (!user_type_mods.empty()) {
366: 			throw BinderException("Type '%s' does not take any type modifiers", user_type_name);
367: 		}
368: 	}
369: 	return result;
370: }
371: 
372: void Binder::BindLogicalType(LogicalType &type, optional_ptr<Catalog> catalog, const string &schema) {
373: 	// check if we need to bind this type at all
374: 	if (!TypeVisitor::Contains(type, LogicalTypeId::USER)) {
375: 		return;
376: 	}
377: 	type = BindLogicalTypeInternal(type, catalog, schema);
378: }
379: 
380: unique_ptr<LogicalOperator> DuckCatalog::BindCreateIndex(Binder &binder, CreateStatement &stmt,
381:                                                          TableCatalogEntry &table, unique_ptr<LogicalOperator> plan) {
382: 	D_ASSERT(plan->type == LogicalOperatorType::LOGICAL_GET);
383: 	auto create_index_info = unique_ptr_cast<CreateInfo, CreateIndexInfo>(std::move(stmt.info));
384: 	IndexBinder index_binder(binder, binder.context);
385: 	return index_binder.BindCreateIndex(binder.context, std::move(create_index_info), table, std::move(plan), nullptr);
386: }
387: 
388: BoundStatement Binder::Bind(CreateStatement &stmt) {
389: 	BoundStatement result;
390: 	result.names = {"Count"};
391: 	result.types = {LogicalType::BIGINT};
392: 
393: 	auto catalog_type = stmt.info->type;
394: 	auto &properties = GetStatementProperties();
395: 	switch (catalog_type) {
396: 	case CatalogType::SCHEMA_ENTRY: {
397: 		auto &base = stmt.info->Cast<CreateInfo>();
398: 		auto catalog = BindCatalog(base.catalog);
399: 		properties.RegisterDBModify(Catalog::GetCatalog(context, catalog), context);
400: 		result.plan = make_uniq<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_SCHEMA, std::move(stmt.info));
401: 		break;
402: 	}
403: 	case CatalogType::VIEW_ENTRY: {
404: 		auto &base = stmt.info->Cast<CreateViewInfo>();
405: 		// bind the schema
406: 		auto &schema = BindCreateSchema(*stmt.info);
407: 		BindCreateViewInfo(base);
408: 		result.plan = make_uniq<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_VIEW, std::move(stmt.info), &schema);
409: 		break;
410: 	}
411: 	case CatalogType::SEQUENCE_ENTRY: {
412: 		auto &schema = BindCreateSchema(*stmt.info);
413: 		result.plan =
414: 		    make_uniq<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_SEQUENCE, std::move(stmt.info), &schema);
415: 		break;
416: 	}
417: 	case CatalogType::TABLE_MACRO_ENTRY: {
418: 		auto &schema = BindCreateSchema(*stmt.info);
419: 		result.plan =
420: 		    make_uniq<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_MACRO, std::move(stmt.info), &schema);
421: 		break;
422: 	}
423: 	case CatalogType::MACRO_ENTRY: {
424: 		auto &schema = BindCreateFunctionInfo(*stmt.info);
425: 		auto logical_create =
426: 		    make_uniq<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_MACRO, std::move(stmt.info), &schema);
427: 		result.plan = std::move(logical_create);
428: 		break;
429: 	}
430: 	case CatalogType::INDEX_ENTRY: {
431: 		auto &create_index_info = stmt.info->Cast<CreateIndexInfo>();
432: 
433: 		// Plan the table scan.
434: 		TableDescription table_description(create_index_info.catalog, create_index_info.schema,
435: 		                                   create_index_info.table);
436: 		auto table_ref = make_uniq<BaseTableRef>(table_description);
437: 		auto bound_table = Bind(*table_ref);
438: 		if (bound_table->type != TableReferenceType::BASE_TABLE) {
439: 			throw BinderException("can only create an index on a base table");
440: 		}
441: 
442: 		auto &table_binding = bound_table->Cast<BoundBaseTableRef>();
443: 		auto &table = table_binding.table;
444: 		if (table.temporary) {
445: 			stmt.info->temporary = true;
446: 		}
447: 		properties.RegisterDBModify(table.catalog, context);
448: 
449: 		// create a plan over the bound table
450: 		auto plan = CreatePlan(*bound_table);
451: 		if (plan->type != LogicalOperatorType::LOGICAL_GET) {
452: 			throw BinderException("Cannot create index on a view!");
453: 		}
454: 
455: 		result.plan = table.catalog.BindCreateIndex(*this, stmt, table, std::move(plan));
456: 		break;
457: 	}
458: 	case CatalogType::TABLE_ENTRY: {
459: 		auto bound_info = BindCreateTableInfo(std::move(stmt.info));
460: 		auto root = std::move(bound_info->query);
461: 
462: 		// create the logical operator
463: 		auto &schema = bound_info->schema;
464: 		auto create_table = make_uniq<LogicalCreateTable>(schema, std::move(bound_info));
465: 		if (root) {
466: 			// CREATE TABLE AS
467: 			properties.return_type = StatementReturnType::CHANGED_ROWS;
468: 			create_table->children.push_back(std::move(root));
469: 		}
470: 		result.plan = std::move(create_table);
471: 		break;
472: 	}
473: 	case CatalogType::TYPE_ENTRY: {
474: 		auto &schema = BindCreateSchema(*stmt.info);
475: 		auto &create_type_info = stmt.info->Cast<CreateTypeInfo>();
476: 		result.plan = make_uniq<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_TYPE, std::move(stmt.info), &schema);
477: 
478: 		auto &catalog = Catalog::GetCatalog(context, create_type_info.catalog);
479: 		auto &dependencies = create_type_info.dependencies;
480: 		auto dependency_callback = [&dependencies, &catalog](CatalogEntry &entry) {
481: 			if (&catalog != &entry.ParentCatalog()) {
482: 				// Don't register any cross-catalog dependencies
483: 				return;
484: 			}
485: 			dependencies.AddDependency(entry);
486: 		};
487: 		if (create_type_info.query) {
488: 			// CREATE TYPE mood AS ENUM (SELECT 'happy')
489: 			auto query_obj = Bind(*create_type_info.query);
490: 			auto query = std::move(query_obj.plan);
491: 			create_type_info.query.reset();
492: 
493: 			auto &sql_types = query_obj.types;
494: 			if (sql_types.size() != 1) {
495: 				// add cast expression?
496: 				throw BinderException("The query must return a single column");
497: 			}
498: 			if (sql_types[0].id() != LogicalType::VARCHAR) {
499: 				// push a projection casting to varchar
500: 				vector<unique_ptr<Expression>> select_list;
501: 				auto ref = make_uniq<BoundColumnRefExpression>(sql_types[0], query->GetColumnBindings()[0]);
502: 				auto cast_expr = BoundCastExpression::AddCastToType(context, std::move(ref), LogicalType::VARCHAR);
503: 				select_list.push_back(std::move(cast_expr));
504: 				auto proj = make_uniq<LogicalProjection>(GenerateTableIndex(), std::move(select_list));
505: 				proj->AddChild(std::move(query));
506: 				query = std::move(proj);
507: 			}
508: 
509: 			result.plan->AddChild(std::move(query));
510: 		} else if (create_type_info.type.id() == LogicalTypeId::USER) {
511: 			SetCatalogLookupCallback(dependency_callback);
512: 			// two cases:
513: 			// 1: create a type with a non-existent type as source, Binder::BindLogicalType(...) will throw exception.
514: 			// 2: create a type alias with a custom type.
515: 			// eg. CREATE TYPE a AS INT; CREATE TYPE b AS a;
516: 			// We set b to be an alias for the underlying type of a
517: 			auto type_entry_p = entry_retriever.GetEntry(CatalogType::TYPE_ENTRY, schema.catalog.GetName(), schema.name,
518: 			                                             UserType::GetTypeName(create_type_info.type));
519: 			D_ASSERT(type_entry_p);
520: 			auto &type_entry = type_entry_p->Cast<TypeCatalogEntry>();
521: 			create_type_info.type = type_entry.user_type;
522: 		} else {
523: 			SetCatalogLookupCallback(dependency_callback);
524: 			// This is done so that if the type contains a USER type,
525: 			// we register this dependency
526: 			auto preserved_type = create_type_info.type;
527: 			BindLogicalType(create_type_info.type);
528: 			create_type_info.type = preserved_type;
529: 		}
530: 		break;
531: 	}
532: 	case CatalogType::SECRET_ENTRY: {
533: 		CatalogTransaction transaction = CatalogTransaction(Catalog::GetSystemCatalog(context), context);
534: 		properties.return_type = StatementReturnType::QUERY_RESULT;
535: 		return SecretManager::Get(context).BindCreateSecret(transaction, stmt.info->Cast<CreateSecretInfo>());
536: 	}
537: 	default:
538: 		throw InternalException("Unrecognized type!");
539: 	}
540: 	properties.return_type = StatementReturnType::NOTHING;
541: 	properties.allow_stream_result = false;
542: 	return result;
543: }
544: 
545: } // namespace duckdb
[end of src/planner/binder/statement/bind_create.cpp]
[start of src/planner/binder/tableref/bind_table_function.cpp]
1: #include "duckdb/catalog/catalog.hpp"
2: #include "duckdb/catalog/catalog_entry/table_macro_catalog_entry.hpp"
3: #include "duckdb/common/algorithm.hpp"
4: #include "duckdb/execution/expression_executor.hpp"
5: #include "duckdb/parser/expression/columnref_expression.hpp"
6: #include "duckdb/parser/expression/comparison_expression.hpp"
7: #include "duckdb/parser/expression/function_expression.hpp"
8: #include "duckdb/parser/expression/subquery_expression.hpp"
9: #include "duckdb/parser/query_node/select_node.hpp"
10: #include "duckdb/parser/tableref/emptytableref.hpp"
11: #include "duckdb/parser/tableref/table_function_ref.hpp"
12: #include "duckdb/planner/binder.hpp"
13: #include "duckdb/planner/expression_binder/table_function_binder.hpp"
14: #include "duckdb/planner/expression_binder/select_binder.hpp"
15: #include "duckdb/planner/operator/logical_get.hpp"
16: #include "duckdb/planner/query_node/bound_select_node.hpp"
17: #include "duckdb/planner/tableref/bound_subqueryref.hpp"
18: #include "duckdb/planner/tableref/bound_table_function.hpp"
19: #include "duckdb/function/function_binder.hpp"
20: #include "duckdb/catalog/catalog_entry/table_function_catalog_entry.hpp"
21: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
22: #include "duckdb/function/table/read_csv.hpp"
23: 
24: namespace duckdb {
25: 
26: enum class TableFunctionBindType { STANDARD_TABLE_FUNCTION, TABLE_IN_OUT_FUNCTION, TABLE_PARAMETER_FUNCTION };
27: 
28: static TableFunctionBindType GetTableFunctionBindType(TableFunctionCatalogEntry &table_function,
29:                                                       vector<unique_ptr<ParsedExpression>> &expressions) {
30: 	// first check if all expressions are scalar
31: 	// if they are we always bind as a standard table function
32: 	bool all_scalar = true;
33: 	for (auto &expr : expressions) {
34: 		if (!expr->IsScalar()) {
35: 			all_scalar = false;
36: 			break;
37: 		}
38: 	}
39: 	if (all_scalar) {
40: 		return TableFunctionBindType::STANDARD_TABLE_FUNCTION;
41: 	}
42: 	// if we have non-scalar parameters - we need to look at the function definition to decide how to bind
43: 	// if a function does not have an in_out_function defined, we need to bind as a standard table function regardless
44: 	bool has_in_out_function = false;
45: 	bool has_standard_table_function = false;
46: 	bool has_table_parameter = false;
47: 	for (idx_t function_idx = 0; function_idx < table_function.functions.Size(); function_idx++) {
48: 		const auto &function = table_function.functions.GetFunctionReferenceByOffset(function_idx);
49: 		for (auto &arg : function.arguments) {
50: 			if (arg.id() == LogicalTypeId::TABLE) {
51: 				has_table_parameter = true;
52: 			}
53: 		}
54: 		if (function.in_out_function) {
55: 			has_in_out_function = true;
56: 		} else if (function.function || function.bind_replace) {
57: 			has_standard_table_function = true;
58: 		} else {
59: 			throw InternalException("Function \"%s\" has neither in_out_function nor function defined",
60: 			                        table_function.name);
61: 		}
62: 	}
63: 	if (has_table_parameter) {
64: 		if (table_function.functions.Size() != 1) {
65: 			throw InternalException(
66: 			    "Function \"%s\" has a TABLE parameter, and multiple function overloads - this is not supported",
67: 			    table_function.name);
68: 		}
69: 		return TableFunctionBindType::TABLE_PARAMETER_FUNCTION;
70: 	}
71: 	if (has_in_out_function && has_standard_table_function) {
72: 		throw InternalException("Function \"%s\" is both an in_out_function and a table function", table_function.name);
73: 	}
74: 	return has_in_out_function ? TableFunctionBindType::TABLE_IN_OUT_FUNCTION
75: 	                           : TableFunctionBindType::STANDARD_TABLE_FUNCTION;
76: }
77: 
78: void Binder::BindTableInTableOutFunction(vector<unique_ptr<ParsedExpression>> &expressions,
79:                                          unique_ptr<BoundSubqueryRef> &subquery) {
80: 	auto binder = Binder::CreateBinder(this->context, this);
81: 	unique_ptr<QueryNode> subquery_node;
82: 	// generate a subquery and bind that (i.e. UNNEST([1,2,3]) becomes UNNEST((SELECT [1,2,3]))
83: 	auto select_node = make_uniq<SelectNode>();
84: 	select_node->select_list = std::move(expressions);
85: 	select_node->from_table = make_uniq<EmptyTableRef>();
86: 	subquery_node = std::move(select_node);
87: 	binder->can_contain_nulls = true;
88: 	auto node = binder->BindNode(*subquery_node);
89: 	subquery = make_uniq<BoundSubqueryRef>(std::move(binder), std::move(node));
90: 	MoveCorrelatedExpressions(*subquery->binder);
91: }
92: 
93: bool Binder::BindTableFunctionParameters(TableFunctionCatalogEntry &table_function,
94:                                          vector<unique_ptr<ParsedExpression>> &expressions,
95:                                          vector<LogicalType> &arguments, vector<Value> &parameters,
96:                                          named_parameter_map_t &named_parameters,
97:                                          unique_ptr<BoundSubqueryRef> &subquery, ErrorData &error) {
98: 	auto bind_type = GetTableFunctionBindType(table_function, expressions);
99: 	if (bind_type == TableFunctionBindType::TABLE_IN_OUT_FUNCTION) {
100: 		// bind table in-out function
101: 		BindTableInTableOutFunction(expressions, subquery);
102: 		// fetch the arguments from the subquery
103: 		arguments = subquery->subquery->types;
104: 		return true;
105: 	}
106: 	bool seen_subquery = false;
107: 	for (auto &child : expressions) {
108: 		string parameter_name;
109: 
110: 		// hack to make named parameters work
111: 		if (child->GetExpressionType() == ExpressionType::COMPARE_EQUAL) {
112: 			// comparison, check if the LHS is a columnref
113: 			auto &comp = child->Cast<ComparisonExpression>();
114: 			if (comp.left->GetExpressionType() == ExpressionType::COLUMN_REF) {
115: 				auto &colref = comp.left->Cast<ColumnRefExpression>();
116: 				if (!colref.IsQualified()) {
117: 					parameter_name = colref.GetColumnName();
118: 					child = std::move(comp.right);
119: 				}
120: 			}
121: 		} else if (!child->GetAlias().empty()) {
122: 			// <name> => <expression> will set the alias of <expression> to <name>
123: 			parameter_name = child->GetAlias();
124: 		}
125: 		if (bind_type == TableFunctionBindType::TABLE_PARAMETER_FUNCTION &&
126: 		    child->GetExpressionType() == ExpressionType::SUBQUERY) {
127: 			D_ASSERT(table_function.functions.Size() == 1);
128: 			auto fun = table_function.functions.GetFunctionByOffset(0);
129: 			if (table_function.functions.Size() != 1 || fun.arguments.empty() ||
130: 			    fun.arguments[0].id() != LogicalTypeId::TABLE) {
131: 				throw BinderException(
132: 				    "Only table-in-out functions can have subquery parameters - %s only accepts constant parameters",
133: 				    fun.name);
134: 			}
135: 			if (seen_subquery) {
136: 				error = ErrorData("Table function can have at most one subquery parameter");
137: 				return false;
138: 			}
139: 			auto binder = Binder::CreateBinder(this->context, this);
140: 			binder->can_contain_nulls = true;
141: 			auto &se = child->Cast<SubqueryExpression>();
142: 			auto node = binder->BindNode(*se.subquery->node);
143: 			subquery = make_uniq<BoundSubqueryRef>(std::move(binder), std::move(node));
144: 			MoveCorrelatedExpressions(*subquery->binder);
145: 			seen_subquery = true;
146: 			arguments.emplace_back(LogicalTypeId::TABLE);
147: 			parameters.emplace_back(Value());
148: 			continue;
149: 		}
150: 
151: 		TableFunctionBinder binder(*this, context, table_function.name);
152: 		LogicalType sql_type;
153: 		auto expr = binder.Bind(child, &sql_type);
154: 		if (expr->HasParameter()) {
155: 			throw ParameterNotResolvedException();
156: 		}
157: 		if (!expr->IsScalar()) {
158: 			// should have been eliminated before
159: 			throw InternalException("Table function requires a constant parameter");
160: 		}
161: 		auto constant = ExpressionExecutor::EvaluateScalar(context, *expr, true);
162: 		if (parameter_name.empty()) {
163: 			// unnamed parameter
164: 			if (!named_parameters.empty()) {
165: 				error = ErrorData("Unnamed parameters cannot come after named parameters");
166: 				return false;
167: 			}
168: 			arguments.emplace_back(constant.IsNull() ? LogicalType::SQLNULL : sql_type);
169: 			parameters.emplace_back(std::move(constant));
170: 		} else {
171: 			named_parameters[parameter_name] = std::move(constant);
172: 		}
173: 	}
174: 	return true;
175: }
176: 
177: static string GetAlias(const TableFunctionRef &ref) {
178: 	if (!ref.alias.empty()) {
179: 		return ref.alias;
180: 	}
181: 	if (ref.function && ref.function->GetExpressionType() == ExpressionType::FUNCTION) {
182: 		auto &function_expr = ref.function->Cast<FunctionExpression>();
183: 		return function_expr.function_name;
184: 	}
185: 	return string();
186: }
187: 
188: unique_ptr<LogicalOperator> Binder::BindTableFunctionInternal(TableFunction &table_function,
189:                                                               const TableFunctionRef &ref, vector<Value> parameters,
190:                                                               named_parameter_map_t named_parameters,
191:                                                               vector<LogicalType> input_table_types,
192:                                                               vector<string> input_table_names) {
193: 	auto function_name = GetAlias(ref);
194: 	auto &column_name_alias = ref.column_name_alias;
195: 
196: 	auto bind_index = GenerateTableIndex();
197: 	// perform the binding
198: 	unique_ptr<FunctionData> bind_data;
199: 	vector<LogicalType> return_types;
200: 	vector<string> return_names;
201: 	if (table_function.bind || table_function.bind_replace) {
202: 		TableFunctionBindInput bind_input(parameters, named_parameters, input_table_types, input_table_names,
203: 		                                  table_function.function_info.get(), this, table_function, ref);
204: 		if (table_function.bind_replace) {
205: 			auto new_plan = table_function.bind_replace(context, bind_input);
206: 			if (new_plan) {
207: 				new_plan->alias = ref.alias;
208: 				new_plan->column_name_alias = ref.column_name_alias;
209: 				return CreatePlan(*Bind(*new_plan));
210: 			} else if (!table_function.bind) {
211: 				throw BinderException("Failed to bind \"%s\": nullptr returned from bind_replace without bind function",
212: 				                      table_function.name);
213: 			}
214: 		}
215: 		bind_data = table_function.bind(context, bind_input, return_types, return_names);
216: 	} else {
217: 		throw InvalidInputException("Cannot call function \"%s\" directly - it has no bind function",
218: 		                            table_function.name);
219: 	}
220: 	if (return_types.size() != return_names.size()) {
221: 		throw InternalException("Failed to bind \"%s\": return_types/names must have same size", table_function.name);
222: 	}
223: 	if (return_types.empty()) {
224: 		throw InternalException("Failed to bind \"%s\": Table function must return at least one column",
225: 		                        table_function.name);
226: 	}
227: 	// overwrite the names with any supplied aliases
228: 	for (idx_t i = 0; i < column_name_alias.size() && i < return_names.size(); i++) {
229: 		return_names[i] = column_name_alias[i];
230: 	}
231: 	for (idx_t i = 0; i < return_names.size(); i++) {
232: 		if (return_names[i].empty()) {
233: 			return_names[i] = "C" + to_string(i);
234: 		}
235: 	}
236: 
237: 	auto get = make_uniq<LogicalGet>(bind_index, table_function, std::move(bind_data), return_types, return_names);
238: 	get->parameters = parameters;
239: 	get->named_parameters = named_parameters;
240: 	get->input_table_types = input_table_types;
241: 	get->input_table_names = input_table_names;
242: 	if (table_function.in_out_function && !table_function.projection_pushdown) {
243: 		for (idx_t i = 0; i < return_types.size(); i++) {
244: 			get->AddColumnId(i);
245: 		}
246: 	}
247: 	// now add the table function to the bind context so its columns can be bound
248: 	bind_context.AddTableFunction(bind_index, function_name, return_names, return_types, get->GetMutableColumnIds(),
249: 	                              get->GetTable().get());
250: 	return std::move(get);
251: }
252: 
253: unique_ptr<LogicalOperator> Binder::BindTableFunction(TableFunction &function, vector<Value> parameters) {
254: 	named_parameter_map_t named_parameters;
255: 	vector<LogicalType> input_table_types;
256: 	vector<string> input_table_names;
257: 
258: 	TableFunctionRef ref;
259: 	ref.alias = function.name;
260: 	D_ASSERT(!ref.alias.empty());
261: 	return BindTableFunctionInternal(function, ref, std::move(parameters), std::move(named_parameters),
262: 	                                 std::move(input_table_types), std::move(input_table_names));
263: }
264: 
265: unique_ptr<BoundTableRef> Binder::Bind(TableFunctionRef &ref) {
266: 	QueryErrorContext error_context(ref.query_location);
267: 
268: 	D_ASSERT(ref.function->GetExpressionType() == ExpressionType::FUNCTION);
269: 	auto &fexpr = ref.function->Cast<FunctionExpression>();
270: 
271: 	string catalog = fexpr.catalog;
272: 	string schema = fexpr.schema;
273: 	Binder::BindSchemaOrCatalog(context, catalog, schema);
274: 
275: 	// fetch the function from the catalog
276: 	auto &func_catalog = *GetCatalogEntry(CatalogType::TABLE_FUNCTION_ENTRY, catalog, schema, fexpr.function_name,
277: 	                                      OnEntryNotFound::THROW_EXCEPTION, error_context);
278: 
279: 	if (func_catalog.type == CatalogType::TABLE_MACRO_ENTRY) {
280: 		auto &macro_func = func_catalog.Cast<TableMacroCatalogEntry>();
281: 		auto query_node = BindTableMacro(fexpr, macro_func, 0);
282: 		D_ASSERT(query_node);
283: 
284: 		auto binder = Binder::CreateBinder(context, this);
285: 		binder->can_contain_nulls = true;
286: 
287: 		binder->alias = ref.alias.empty() ? "unnamed_query" : ref.alias;
288: 		unique_ptr<BoundQueryNode> query;
289: 		try {
290: 			query = binder->BindNode(*query_node);
291: 		} catch (std::exception &ex) {
292: 			ErrorData error(ex);
293: 			error.AddQueryLocation(ref);
294: 			error.Throw();
295: 		}
296: 
297: 		idx_t bind_index = query->GetRootIndex();
298: 		// string alias;
299: 		string alias = (ref.alias.empty() ? "unnamed_query" + to_string(bind_index) : ref.alias);
300: 
301: 		auto result = make_uniq<BoundSubqueryRef>(std::move(binder), std::move(query));
302: 		// remember ref here is TableFunctionRef and NOT base class
303: 		bind_context.AddSubquery(bind_index, alias, ref, *result->subquery);
304: 		MoveCorrelatedExpressions(*result->binder);
305: 		return std::move(result);
306: 	}
307: 	D_ASSERT(func_catalog.type == CatalogType::TABLE_FUNCTION_ENTRY);
308: 	auto &function = func_catalog.Cast<TableFunctionCatalogEntry>();
309: 
310: 	// evaluate the input parameters to the function
311: 	vector<LogicalType> arguments;
312: 	vector<Value> parameters;
313: 	named_parameter_map_t named_parameters;
314: 	unique_ptr<BoundSubqueryRef> subquery;
315: 	ErrorData error;
316: 	if (!BindTableFunctionParameters(function, fexpr.children, arguments, parameters, named_parameters, subquery,
317: 	                                 error)) {
318: 		error.AddQueryLocation(ref);
319: 		error.Throw();
320: 	}
321: 
322: 	// select the function based on the input parameters
323: 	FunctionBinder function_binder(*this);
324: 	auto best_function_idx = function_binder.BindFunction(function.name, function.functions, arguments, error);
325: 	if (!best_function_idx.IsValid()) {
326: 		error.AddQueryLocation(ref);
327: 		error.Throw();
328: 	}
329: 	auto table_function = function.functions.GetFunctionByOffset(best_function_idx.GetIndex());
330: 
331: 	// now check the named parameters
332: 	BindNamedParameters(table_function.named_parameters, named_parameters, error_context, table_function.name);
333: 
334: 	vector<LogicalType> input_table_types;
335: 	vector<string> input_table_names;
336: 
337: 	if (subquery) {
338: 		input_table_types = subquery->subquery->types;
339: 		input_table_names = subquery->subquery->names;
340: 	} else if (table_function.in_out_function) {
341: 		for (auto &param : parameters) {
342: 			input_table_types.push_back(param.type());
343: 			input_table_names.push_back(string());
344: 		}
345: 	}
346: 	if (!parameters.empty()) {
347: 		// cast the parameters to the type of the function
348: 		for (idx_t i = 0; i < arguments.size(); i++) {
349: 			auto target_type =
350: 			    i < table_function.arguments.size() ? table_function.arguments[i] : table_function.varargs;
351: 
352: 			if (target_type != LogicalType::ANY && target_type != LogicalType::POINTER &&
353: 			    target_type.id() != LogicalTypeId::LIST && target_type != LogicalType::TABLE) {
354: 				parameters[i] = parameters[i].CastAs(context, target_type);
355: 			}
356: 		}
357: 	} else if (subquery) {
358: 		for (idx_t i = 0; i < arguments.size(); i++) {
359: 			auto target_type =
360: 			    i < table_function.arguments.size() ? table_function.arguments[i] : table_function.varargs;
361: 
362: 			if (target_type != LogicalType::ANY && target_type != LogicalType::POINTER &&
363: 			    target_type.id() != LogicalTypeId::LIST) {
364: 				input_table_types[i] = target_type;
365: 			}
366: 		}
367: 	}
368: 
369: 	auto get = BindTableFunctionInternal(table_function, ref, std::move(parameters), std::move(named_parameters),
370: 	                                     std::move(input_table_types), std::move(input_table_names));
371: 	auto table_function_ref = make_uniq<BoundTableFunction>(std::move(get));
372: 	table_function_ref->subquery = std::move(subquery);
373: 	return std::move(table_function_ref);
374: }
375: 
376: } // namespace duckdb
[end of src/planner/binder/tableref/bind_table_function.cpp]
[start of src/planner/subquery/flatten_dependent_join.cpp]
1: #include "duckdb/planner/subquery/flatten_dependent_join.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/aggregate_function_catalog_entry.hpp"
4: #include "duckdb/common/operator/add.hpp"
5: #include "duckdb/function/aggregate/distributive_functions.hpp"
6: #include "duckdb/function/aggregate/distributive_function_utils.hpp"
7: #include "duckdb/planner/binder.hpp"
8: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
9: #include "duckdb/planner/expression/list.hpp"
10: #include "duckdb/planner/logical_operator_visitor.hpp"
11: #include "duckdb/planner/operator/list.hpp"
12: #include "duckdb/planner/subquery/has_correlated_expressions.hpp"
13: #include "duckdb/planner/subquery/rewrite_correlated_expressions.hpp"
14: #include "duckdb/planner/subquery/rewrite_cte_scan.hpp"
15: #include "duckdb/planner/operator/logical_dependent_join.hpp"
16: #include "duckdb/execution/column_binding_resolver.hpp"
17: 
18: namespace duckdb {
19: 
20: FlattenDependentJoins::FlattenDependentJoins(Binder &binder, const vector<CorrelatedColumnInfo> &correlated,
21:                                              bool perform_delim, bool any_join)
22:     : binder(binder), delim_offset(DConstants::INVALID_INDEX), correlated_columns(correlated),
23:       perform_delim(perform_delim), any_join(any_join) {
24: 	for (idx_t i = 0; i < correlated_columns.size(); i++) {
25: 		auto &col = correlated_columns[i];
26: 		correlated_map[col.binding] = i;
27: 		delim_types.push_back(col.type);
28: 	}
29: }
30: 
31: bool FlattenDependentJoins::DetectCorrelatedExpressions(LogicalOperator &op, bool lateral, idx_t lateral_depth) {
32: 
33: 	bool is_lateral_join = false;
34: 
35: 	// check if this entry has correlated expressions
36: 	if (op.type == LogicalOperatorType::LOGICAL_DEPENDENT_JOIN) {
37: 		is_lateral_join = true;
38: 	}
39: 	HasCorrelatedExpressions visitor(correlated_columns, lateral, lateral_depth);
40: 	visitor.VisitOperator(op);
41: 	bool has_correlation = visitor.has_correlated_expressions;
42: 	int child_idx = 0;
43: 	// now visit the children of this entry and check if they have correlated expressions
44: 	for (auto &child : op.children) {
45: 		auto new_lateral_depth = lateral_depth;
46: 		if (is_lateral_join && child_idx == 1) {
47: 			new_lateral_depth = lateral_depth + 1;
48: 		}
49: 		// we OR the property with its children such that has_correlation is true if either
50: 		// (1) this node has a correlated expression or
51: 		// (2) one of its children has a correlated expression
52: 		if (DetectCorrelatedExpressions(*child, lateral, new_lateral_depth)) {
53: 			has_correlation = true;
54: 		}
55: 		child_idx++;
56: 	}
57: 	// set the entry in the map
58: 	has_correlated_expressions[op] = has_correlation;
59: 
60: 	// If we detect correlation in a materialized or recursive CTE, the entire right side of the operator
61: 	// needs to be marked as correlated. Otherwise, function PushDownDependentJoinInternal does not do the
62: 	// right thing.
63: 	if (op.type == LogicalOperatorType::LOGICAL_MATERIALIZED_CTE ||
64: 	    op.type == LogicalOperatorType::LOGICAL_RECURSIVE_CTE) {
65: 		if (has_correlation) {
66: 			MarkSubtreeCorrelated(*op.children[1].get());
67: 		}
68: 	}
69: 	return has_correlation;
70: }
71: 
72: bool FlattenDependentJoins::MarkSubtreeCorrelated(LogicalOperator &op) {
73: 	// Do not mark base table scans as correlated
74: 	auto entry = has_correlated_expressions.find(op);
75: 	D_ASSERT(entry != has_correlated_expressions.end());
76: 	bool has_correlation = entry->second;
77: 	for (auto &child : op.children) {
78: 		has_correlation |= MarkSubtreeCorrelated(*child.get());
79: 	}
80: 	if (op.type != LogicalOperatorType::LOGICAL_GET || op.children.size() == 1) {
81: 		if (op.type == LogicalOperatorType::LOGICAL_CTE_REF) {
82: 			has_correlated_expressions[op] = true;
83: 			return true;
84: 		} else {
85: 			has_correlated_expressions[op] = has_correlation;
86: 		}
87: 	}
88: 	return has_correlation;
89: }
90: 
91: unique_ptr<LogicalOperator> FlattenDependentJoins::PushDownDependentJoin(unique_ptr<LogicalOperator> plan,
92:                                                                          bool propagate_null_values) {
93: 	auto result = PushDownDependentJoinInternal(std::move(plan), propagate_null_values, 0);
94: 	if (!replacement_map.empty()) {
95: 		// check if we have to replace any COUNT aggregates into "CASE WHEN X IS NULL THEN 0 ELSE COUNT END"
96: 		RewriteCountAggregates aggr(replacement_map);
97: 		aggr.VisitOperator(*result);
98: 	}
99: 	return result;
100: }
101: 
102: bool SubqueryDependentFilter(Expression &expr) {
103: 	if (expr.GetExpressionClass() == ExpressionClass::BOUND_CONJUNCTION &&
104: 	    expr.GetExpressionType() == ExpressionType::CONJUNCTION_AND) {
105: 		auto &bound_conjunction = expr.Cast<BoundConjunctionExpression>();
106: 		for (auto &child : bound_conjunction.children) {
107: 			if (SubqueryDependentFilter(*child)) {
108: 				return true;
109: 			}
110: 		}
111: 	}
112: 	if (expr.GetExpressionClass() == ExpressionClass::BOUND_SUBQUERY) {
113: 		return true;
114: 	}
115: 	return false;
116: }
117: 
118: unique_ptr<LogicalOperator> FlattenDependentJoins::PushDownDependentJoinInternal(unique_ptr<LogicalOperator> plan,
119:                                                                                  bool &parent_propagate_null_values,
120:                                                                                  idx_t lateral_depth) {
121: 	// first check if the logical operator has correlated expressions
122: 	auto entry = has_correlated_expressions.find(*plan);
123: 	bool exit_projection = false;
124: 	unique_ptr<LogicalDelimGet> delim_scan;
125: 	D_ASSERT(entry != has_correlated_expressions.end());
126: 	if (!entry->second) {
127: 		// we reached a node without correlated expressions
128: 		// we can eliminate the dependent join now and create a simple cross product
129: 		// now create the duplicate eliminated scan for this node
130: 		if (plan->type == LogicalOperatorType::LOGICAL_CTE_REF) {
131: 			auto &op = plan->Cast<LogicalCTERef>();
132: 
133: 			auto rec_cte = binder.recursive_ctes.find(op.cte_index);
134: 			if (rec_cte != binder.recursive_ctes.end()) {
135: 				D_ASSERT(rec_cte->second->type == LogicalOperatorType::LOGICAL_RECURSIVE_CTE);
136: 				auto &rec_cte_op = rec_cte->second->Cast<LogicalRecursiveCTE>();
137: 				RewriteCTEScan cte_rewriter(op.cte_index, rec_cte_op.correlated_columns);
138: 				cte_rewriter.VisitOperator(*plan);
139: 			}
140: 		}
141: 
142: 		// create cross product with Delim Join
143: 		auto delim_index = binder.GenerateTableIndex();
144: 		base_binding = ColumnBinding(delim_index, 0);
145: 
146: 		auto left_columns = plan->GetColumnBindings().size();
147: 		delim_offset = left_columns;
148: 		data_offset = 0;
149: 		delim_scan = make_uniq<LogicalDelimGet>(delim_index, delim_types);
150: 		if (plan->type == LogicalOperatorType::LOGICAL_PROJECTION) {
151: 			// we want to keep the logical projection for positionality.
152: 			exit_projection = true;
153: 		} else {
154: 			auto cross_product = LogicalCrossProduct::Create(std::move(plan), std::move(delim_scan));
155: 			return cross_product;
156: 		}
157: 	}
158: 	switch (plan->type) {
159: 	case LogicalOperatorType::LOGICAL_UNNEST:
160: 	case LogicalOperatorType::LOGICAL_FILTER: {
161: 		// filter
162: 		// first we flatten the dependent join in the child of the filter
163: 		for (auto &expr : plan->expressions) {
164: 			any_join |= SubqueryDependentFilter(*expr);
165: 		}
166: 		plan->children[0] =
167: 		    PushDownDependentJoinInternal(std::move(plan->children[0]), parent_propagate_null_values, lateral_depth);
168: 
169: 		// then we replace any correlated expressions with the corresponding entry in the correlated_map
170: 		RewriteCorrelatedExpressions rewriter(base_binding, correlated_map, lateral_depth);
171: 		rewriter.VisitOperator(*plan);
172: 		return plan;
173: 	}
174: 	case LogicalOperatorType::LOGICAL_PROJECTION: {
175: 		// projection
176: 		// first we flatten the dependent join in the child of the projection
177: 		for (auto &expr : plan->expressions) {
178: 			parent_propagate_null_values &= expr->PropagatesNullValues();
179: 		}
180: 
181: 		// if the node has no correlated expressions,
182: 		// push the cross product with the delim get only below the projection.
183: 		// This will preserve positionality of the columns and prevent errors when reordering of
184: 		// delim gets is enabled.
185: 		if (exit_projection) {
186: 			auto cross_product = LogicalCrossProduct::Create(std::move(plan->children[0]), std::move(delim_scan));
187: 			plan->children[0] = std::move(cross_product);
188: 		} else {
189: 			plan->children[0] = PushDownDependentJoinInternal(std::move(plan->children[0]),
190: 			                                                  parent_propagate_null_values, lateral_depth);
191: 		}
192: 
193: 		// then we replace any correlated expressions with the corresponding entry in the correlated_map
194: 		RewriteCorrelatedExpressions rewriter(base_binding, correlated_map, lateral_depth);
195: 		rewriter.VisitOperator(*plan);
196: 		// now we add all the columns of the delim_scan to the projection list
197: 		auto &proj = plan->Cast<LogicalProjection>();
198: 		for (idx_t i = 0; i < correlated_columns.size(); i++) {
199: 			auto &col = correlated_columns[i];
200: 			auto colref = make_uniq<BoundColumnRefExpression>(
201: 			    col.name, col.type, ColumnBinding(base_binding.table_index, base_binding.column_index + i));
202: 			plan->expressions.push_back(std::move(colref));
203: 		}
204: 
205: 		base_binding.table_index = proj.table_index;
206: 		this->delim_offset = base_binding.column_index = plan->expressions.size() - correlated_columns.size();
207: 		this->data_offset = 0;
208: 		return plan;
209: 	}
210: 	case LogicalOperatorType::LOGICAL_AGGREGATE_AND_GROUP_BY: {
211: 		auto &aggr = plan->Cast<LogicalAggregate>();
212: 		// aggregate and group by
213: 		// first we flatten the dependent join in the child of the projection
214: 		for (auto &expr : plan->expressions) {
215: 			parent_propagate_null_values &= expr->PropagatesNullValues();
216: 		}
217: 		plan->children[0] =
218: 		    PushDownDependentJoinInternal(std::move(plan->children[0]), parent_propagate_null_values, lateral_depth);
219: 		// then we replace any correlated expressions with the corresponding entry in the correlated_map
220: 		RewriteCorrelatedExpressions rewriter(base_binding, correlated_map, lateral_depth);
221: 		rewriter.VisitOperator(*plan);
222: 		// now we add all the columns of the delim_scan to the grouping operators AND the projection list
223: 		idx_t delim_table_index;
224: 		idx_t delim_column_offset;
225: 		idx_t delim_data_offset;
226: 		auto new_group_count = perform_delim ? correlated_columns.size() : 1;
227: 		for (idx_t i = 0; i < new_group_count; i++) {
228: 			auto &col = correlated_columns[i];
229: 			auto colref = make_uniq<BoundColumnRefExpression>(
230: 			    col.name, col.type, ColumnBinding(base_binding.table_index, base_binding.column_index + i));
231: 			for (auto &set : aggr.grouping_sets) {
232: 				set.insert(aggr.groups.size());
233: 			}
234: 			aggr.groups.push_back(std::move(colref));
235: 		}
236: 		if (!perform_delim) {
237: 			// if we are not performing the duplicate elimination, we have only added the row_id column to the grouping
238: 			// operators in this case, we push a FIRST aggregate for each of the remaining expressions
239: 			delim_table_index = aggr.aggregate_index;
240: 			delim_column_offset = aggr.expressions.size();
241: 			delim_data_offset = aggr.groups.size();
242: 			for (idx_t i = 0; i < correlated_columns.size(); i++) {
243: 				auto &col = correlated_columns[i];
244: 				auto first_aggregate = FirstFunctionGetter::GetFunction(col.type);
245: 				auto colref = make_uniq<BoundColumnRefExpression>(
246: 				    col.name, col.type, ColumnBinding(base_binding.table_index, base_binding.column_index + i));
247: 				vector<unique_ptr<Expression>> aggr_children;
248: 				aggr_children.push_back(std::move(colref));
249: 				auto first_fun =
250: 				    make_uniq<BoundAggregateExpression>(std::move(first_aggregate), std::move(aggr_children), nullptr,
251: 				                                        nullptr, AggregateType::NON_DISTINCT);
252: 				aggr.expressions.push_back(std::move(first_fun));
253: 			}
254: 		} else {
255: 			delim_table_index = aggr.group_index;
256: 			delim_column_offset = aggr.groups.size() - correlated_columns.size();
257: 			delim_data_offset = aggr.groups.size();
258: 		}
259: 		bool ungrouped_join = false;
260: 		if (aggr.grouping_sets.empty()) {
261: 			ungrouped_join = aggr.groups.size() == new_group_count;
262: 		} else {
263: 			for (auto &grouping_set : aggr.grouping_sets) {
264: 				if (grouping_set.size() == new_group_count) {
265: 					ungrouped_join = true;
266: 				}
267: 			}
268: 		}
269: 		if (ungrouped_join) {
270: 			// we have to perform an INNER or LEFT OUTER JOIN between the result of this aggregate and the delim scan
271: 			// this does not always have to be a LEFT OUTER JOIN, depending on whether aggr.expressions return
272: 			// NULL or a value
273: 			JoinType join_type = JoinType::INNER;
274: 			if (any_join || !parent_propagate_null_values) {
275: 				join_type = JoinType::LEFT;
276: 			}
277: 			for (auto &aggr_exp : aggr.expressions) {
278: 				auto &b_aggr_exp = aggr_exp->Cast<BoundAggregateExpression>();
279: 				if (!b_aggr_exp.PropagatesNullValues()) {
280: 					join_type = JoinType::LEFT;
281: 					break;
282: 				}
283: 			}
284: 			unique_ptr<LogicalComparisonJoin> join = make_uniq<LogicalComparisonJoin>(join_type);
285: 			auto left_index = binder.GenerateTableIndex();
286: 			delim_scan = make_uniq<LogicalDelimGet>(left_index, delim_types);
287: 			join->children.push_back(std::move(delim_scan));
288: 			join->children.push_back(std::move(plan));
289: 			for (idx_t i = 0; i < new_group_count; i++) {
290: 				auto &col = correlated_columns[i];
291: 				JoinCondition cond;
292: 				cond.left = make_uniq<BoundColumnRefExpression>(col.name, col.type, ColumnBinding(left_index, i));
293: 				cond.right = make_uniq<BoundColumnRefExpression>(
294: 				    correlated_columns[i].type, ColumnBinding(delim_table_index, delim_column_offset + i));
295: 				cond.comparison = ExpressionType::COMPARE_NOT_DISTINCT_FROM;
296: 				join->conditions.push_back(std::move(cond));
297: 			}
298: 			// for any COUNT aggregate we replace references to the column with: CASE WHEN COUNT(*) IS NULL THEN 0
299: 			// ELSE COUNT(*) END
300: 			for (idx_t i = 0; i < aggr.expressions.size(); i++) {
301: 				D_ASSERT(aggr.expressions[i]->GetExpressionClass() == ExpressionClass::BOUND_AGGREGATE);
302: 				auto &bound = aggr.expressions[i]->Cast<BoundAggregateExpression>();
303: 				vector<LogicalType> arguments;
304: 				if (bound.function == CountFunctionBase::GetFunction() ||
305: 				    bound.function == CountStarFun::GetFunction()) {
306: 					// have to replace this ColumnBinding with the CASE expression
307: 					replacement_map[ColumnBinding(aggr.aggregate_index, i)] = i;
308: 				}
309: 			}
310: 			// now we update the delim_index
311: 			base_binding.table_index = left_index;
312: 			this->delim_offset = base_binding.column_index = 0;
313: 			this->data_offset = 0;
314: 			return std::move(join);
315: 		} else {
316: 			// update the delim_index
317: 			base_binding.table_index = delim_table_index;
318: 			this->delim_offset = base_binding.column_index = delim_column_offset;
319: 			this->data_offset = delim_data_offset;
320: 			return plan;
321: 		}
322: 	}
323: 	case LogicalOperatorType::LOGICAL_CROSS_PRODUCT: {
324: 		// cross product
325: 		// push into both sides of the plan
326: 		bool left_has_correlation = has_correlated_expressions.find(*plan->children[0])->second;
327: 		bool right_has_correlation = has_correlated_expressions.find(*plan->children[1])->second;
328: 		if (!right_has_correlation) {
329: 			// only left has correlation: push into left
330: 			plan->children[0] = PushDownDependentJoinInternal(std::move(plan->children[0]),
331: 			                                                  parent_propagate_null_values, lateral_depth);
332: 			return plan;
333: 		}
334: 		if (!left_has_correlation) {
335: 			// only right has correlation: push into right
336: 			plan->children[1] = PushDownDependentJoinInternal(std::move(plan->children[1]),
337: 			                                                  parent_propagate_null_values, lateral_depth);
338: 			return plan;
339: 		}
340: 		// both sides have correlation
341: 		// turn into an inner join
342: 		auto join = make_uniq<LogicalComparisonJoin>(JoinType::INNER);
343: 		plan->children[0] =
344: 		    PushDownDependentJoinInternal(std::move(plan->children[0]), parent_propagate_null_values, lateral_depth);
345: 		auto left_binding = this->base_binding;
346: 		plan->children[1] =
347: 		    PushDownDependentJoinInternal(std::move(plan->children[1]), parent_propagate_null_values, lateral_depth);
348: 		// add the correlated columns to the join conditions
349: 		for (idx_t i = 0; i < correlated_columns.size(); i++) {
350: 			JoinCondition cond;
351: 			cond.left = make_uniq<BoundColumnRefExpression>(
352: 			    correlated_columns[i].type, ColumnBinding(left_binding.table_index, left_binding.column_index + i));
353: 			cond.right = make_uniq<BoundColumnRefExpression>(
354: 			    correlated_columns[i].type, ColumnBinding(base_binding.table_index, base_binding.column_index + i));
355: 			cond.comparison = ExpressionType::COMPARE_NOT_DISTINCT_FROM;
356: 			join->conditions.push_back(std::move(cond));
357: 		}
358: 		join->children.push_back(std::move(plan->children[0]));
359: 		join->children.push_back(std::move(plan->children[1]));
360: 		return std::move(join);
361: 	}
362: 	case LogicalOperatorType::LOGICAL_DEPENDENT_JOIN: {
363: 		auto &dependent_join = plan->Cast<LogicalJoin>();
364: 		if (!((dependent_join.join_type == JoinType::INNER) || (dependent_join.join_type == JoinType::LEFT))) {
365: 			throw NotImplementedException("Dependent join can only be INNER or LEFT type");
366: 		}
367: 		D_ASSERT(plan->children.size() == 2);
368: 		// Push all the bindings down to the left side so the right side knows where to refer DELIM_GET from
369: 		plan->children[0] =
370: 		    PushDownDependentJoinInternal(std::move(plan->children[0]), parent_propagate_null_values, lateral_depth);
371: 
372: 		// Normal rewriter like in other joins
373: 		RewriteCorrelatedExpressions rewriter(this->base_binding, correlated_map, lateral_depth);
374: 		rewriter.VisitOperator(*plan);
375: 
376: 		// Recursive rewriter to visit right side of lateral join and update bindings from left
377: 		RewriteCorrelatedExpressions recursive_rewriter(this->base_binding, correlated_map, lateral_depth + 1, true);
378: 		recursive_rewriter.VisitOperator(*plan->children[1]);
379: 
380: 		return plan;
381: 	}
382: 	case LogicalOperatorType::LOGICAL_ANY_JOIN:
383: 	case LogicalOperatorType::LOGICAL_ASOF_JOIN:
384: 	case LogicalOperatorType::LOGICAL_COMPARISON_JOIN: {
385: 		auto &join = plan->Cast<LogicalJoin>();
386: 		D_ASSERT(plan->children.size() == 2);
387: 		// check the correlated expressions in the children of the join
388: 		bool left_has_correlation = has_correlated_expressions.find(*plan->children[0])->second;
389: 		bool right_has_correlation = has_correlated_expressions.find(*plan->children[1])->second;
390: 
391: 		if (join.join_type == JoinType::INNER) {
392: 			// inner join
393: 			if (!right_has_correlation) {
394: 				// only left has correlation: push into left
395: 				plan->children[0] = PushDownDependentJoinInternal(std::move(plan->children[0]),
396: 				                                                  parent_propagate_null_values, lateral_depth);
397: 				// Remove the correlated columns coming from outside for current join node
398: 				return plan;
399: 			}
400: 			if (!left_has_correlation) {
401: 				// only right has correlation: push into right
402: 				plan->children[1] = PushDownDependentJoinInternal(std::move(plan->children[1]),
403: 				                                                  parent_propagate_null_values, lateral_depth);
404: 				// Remove the correlated columns coming from outside for current join node
405: 				return plan;
406: 			}
407: 		} else if (join.join_type == JoinType::LEFT) {
408: 			// left outer join
409: 			if (!right_has_correlation) {
410: 				// only left has correlation: push into left
411: 				plan->children[0] = PushDownDependentJoinInternal(std::move(plan->children[0]),
412: 				                                                  parent_propagate_null_values, lateral_depth);
413: 				// Remove the correlated columns coming from outside for current join node
414: 				return plan;
415: 			}
416: 		} else if (join.join_type == JoinType::RIGHT) {
417: 			// right outer join
418: 			if (!left_has_correlation) {
419: 				// only right has correlation: push into right
420: 				plan->children[1] = PushDownDependentJoinInternal(std::move(plan->children[1]),
421: 				                                                  parent_propagate_null_values, lateral_depth);
422: 				return plan;
423: 			}
424: 		} else if (join.join_type == JoinType::MARK) {
425: 			if (right_has_correlation) {
426: 				throw NotImplementedException("MARK join with correlation in RHS not supported");
427: 			}
428: 			// push the child into the LHS
429: 			plan->children[0] = PushDownDependentJoinInternal(std::move(plan->children[0]),
430: 			                                                  parent_propagate_null_values, lateral_depth);
431: 			// rewrite expressions in the join conditions
432: 			RewriteCorrelatedExpressions rewriter(base_binding, correlated_map, lateral_depth);
433: 			rewriter.VisitOperator(*plan);
434: 			return plan;
435: 		} else {
436: 			throw NotImplementedException("Unsupported join type for flattening correlated subquery");
437: 		}
438: 		// both sides have correlation
439: 		// push into both sides
440: 		plan->children[0] =
441: 		    PushDownDependentJoinInternal(std::move(plan->children[0]), parent_propagate_null_values, lateral_depth);
442: 		auto left_binding = this->base_binding;
443: 		plan->children[1] =
444: 		    PushDownDependentJoinInternal(std::move(plan->children[1]), parent_propagate_null_values, lateral_depth);
445: 		auto right_binding = this->base_binding;
446: 		// NOTE: for OUTER JOINS it matters what the BASE BINDING is after the join
447: 		// for the LEFT OUTER JOIN, we want the LEFT side to be the base binding after we push
448: 		// because the RIGHT binding might contain NULL values
449: 		if (join.join_type == JoinType::LEFT) {
450: 			this->base_binding = left_binding;
451: 		} else if (join.join_type == JoinType::RIGHT) {
452: 			this->base_binding = right_binding;
453: 			delim_offset += plan->children[0]->GetColumnBindings().size();
454: 		}
455: 		// add the correlated columns to the join conditions
456: 		for (idx_t i = 0; i < correlated_columns.size(); i++) {
457: 			auto left = make_uniq<BoundColumnRefExpression>(
458: 			    correlated_columns[i].type, ColumnBinding(left_binding.table_index, left_binding.column_index + i));
459: 			auto right = make_uniq<BoundColumnRefExpression>(
460: 			    correlated_columns[i].type, ColumnBinding(right_binding.table_index, right_binding.column_index + i));
461: 
462: 			if (join.type == LogicalOperatorType::LOGICAL_COMPARISON_JOIN ||
463: 			    join.type == LogicalOperatorType::LOGICAL_ASOF_JOIN) {
464: 				JoinCondition cond;
465: 				cond.left = std::move(left);
466: 				cond.right = std::move(right);
467: 				cond.comparison = ExpressionType::COMPARE_NOT_DISTINCT_FROM;
468: 
469: 				auto &comparison_join = join.Cast<LogicalComparisonJoin>();
470: 				comparison_join.conditions.push_back(std::move(cond));
471: 			} else {
472: 				auto &logical_any_join = join.Cast<LogicalAnyJoin>();
473: 				auto comparison = make_uniq<BoundComparisonExpression>(ExpressionType::COMPARE_NOT_DISTINCT_FROM,
474: 				                                                       std::move(left), std::move(right));
475: 				auto conjunction = make_uniq<BoundConjunctionExpression>(
476: 				    ExpressionType::CONJUNCTION_AND, std::move(comparison), std::move(logical_any_join.condition));
477: 				logical_any_join.condition = std::move(conjunction);
478: 			}
479: 		}
480: 		// then we replace any correlated expressions with the corresponding entry in the correlated_map
481: 		RewriteCorrelatedExpressions rewriter(right_binding, correlated_map, lateral_depth);
482: 		rewriter.VisitOperator(*plan);
483: 		return plan;
484: 	}
485: 	case LogicalOperatorType::LOGICAL_LIMIT: {
486: 		auto &limit = plan->Cast<LogicalLimit>();
487: 		switch (limit.limit_val.Type()) {
488: 		case LimitNodeType::CONSTANT_PERCENTAGE:
489: 		case LimitNodeType::EXPRESSION_PERCENTAGE:
490: 			// NOTE: limit percent could be supported in a manner similar to the LIMIT above
491: 			// but instead of filtering by an exact number of rows, the limit should be expressed as
492: 			// COUNT computed over the partition multiplied by the percentage
493: 			throw ParserException("Limit percent operator not supported in correlated subquery");
494: 		case LimitNodeType::EXPRESSION_VALUE:
495: 			throw ParserException("Non-constant limit not supported in correlated subquery");
496: 		default:
497: 			break;
498: 		}
499: 		switch (limit.offset_val.Type()) {
500: 		case LimitNodeType::EXPRESSION_VALUE:
501: 			throw ParserException("Non-constant offset not supported in correlated subquery");
502: 		case LimitNodeType::CONSTANT_PERCENTAGE:
503: 		case LimitNodeType::EXPRESSION_PERCENTAGE:
504: 			throw InternalException("Percentage offset in FlattenDependentJoin");
505: 		default:
506: 			break;
507: 		}
508: 		auto rownum_alias = "limit_rownum";
509: 		unique_ptr<LogicalOperator> child;
510: 		unique_ptr<LogicalOrder> order_by;
511: 
512: 		// check if the direct child of this LIMIT node is an ORDER BY node, if so, keep it separate
513: 		// this is done for an optimization to avoid having to compute the total order
514: 		if (plan->children[0]->type == LogicalOperatorType::LOGICAL_ORDER_BY) {
515: 			order_by = unique_ptr_cast<LogicalOperator, LogicalOrder>(std::move(plan->children[0]));
516: 			child = PushDownDependentJoinInternal(std::move(order_by->children[0]), parent_propagate_null_values,
517: 			                                      lateral_depth);
518: 		} else {
519: 			child = PushDownDependentJoinInternal(std::move(plan->children[0]), parent_propagate_null_values,
520: 			                                      lateral_depth);
521: 		}
522: 		auto child_column_count = child->GetColumnBindings().size();
523: 		// we push a row_number() OVER (PARTITION BY [correlated columns])
524: 		auto window_index = binder.GenerateTableIndex();
525: 		auto window = make_uniq<LogicalWindow>(window_index);
526: 		auto row_number =
527: 		    make_uniq<BoundWindowExpression>(ExpressionType::WINDOW_ROW_NUMBER, LogicalType::BIGINT, nullptr, nullptr);
528: 		auto partition_count = perform_delim ? correlated_columns.size() : 1;
529: 		for (idx_t i = 0; i < partition_count; i++) {
530: 			auto &col = correlated_columns[i];
531: 			auto colref = make_uniq<BoundColumnRefExpression>(
532: 			    col.name, col.type, ColumnBinding(base_binding.table_index, base_binding.column_index + i));
533: 			row_number->partitions.push_back(std::move(colref));
534: 		}
535: 		if (order_by) {
536: 			// optimization: if there is an ORDER BY node followed by a LIMIT
537: 			// rather than computing the entire order, we push the ORDER BY expressions into the row_num computation
538: 			// this way, the order only needs to be computed per partition
539: 			row_number->orders = std::move(order_by->orders);
540: 		}
541: 		row_number->start = WindowBoundary::UNBOUNDED_PRECEDING;
542: 		row_number->end = WindowBoundary::CURRENT_ROW_ROWS;
543: 		window->expressions.push_back(std::move(row_number));
544: 		window->children.push_back(std::move(child));
545: 
546: 		// add a filter based on the row_number
547: 		// the filter we add is "row_number > offset AND row_number <= offset + limit"
548: 		auto filter = make_uniq<LogicalFilter>();
549: 		unique_ptr<Expression> condition;
550: 		auto row_num_ref =
551: 		    make_uniq<BoundColumnRefExpression>(rownum_alias, LogicalType::BIGINT, ColumnBinding(window_index, 0));
552: 
553: 		if (limit.limit_val.Type() == LimitNodeType::CONSTANT_VALUE) {
554: 			auto upper_bound_limit = NumericLimits<int64_t>::Maximum();
555: 			auto limit_val = int64_t(limit.limit_val.GetConstantValue());
556: 			if (limit.offset_val.Type() == LimitNodeType::CONSTANT_VALUE) {
557: 				// both offset and limit specified - upper bound is offset + limit
558: 				auto offset_val = int64_t(limit.offset_val.GetConstantValue());
559: 				TryAddOperator::Operation(limit_val, offset_val, upper_bound_limit);
560: 			} else {
561: 				// no offset - upper bound is only the limit
562: 				upper_bound_limit = limit_val;
563: 			}
564: 			auto upper_bound = make_uniq<BoundConstantExpression>(Value::BIGINT(upper_bound_limit));
565: 			condition = make_uniq<BoundComparisonExpression>(ExpressionType::COMPARE_LESSTHANOREQUALTO,
566: 			                                                 row_num_ref->Copy(), std::move(upper_bound));
567: 		}
568: 		// we only need to add "row_number >= offset + 1" if offset is bigger than 0
569: 		if (limit.offset_val.Type() == LimitNodeType::CONSTANT_VALUE) {
570: 			auto offset_val = int64_t(limit.offset_val.GetConstantValue());
571: 			auto lower_bound = make_uniq<BoundConstantExpression>(Value::BIGINT(offset_val));
572: 			auto lower_comp = make_uniq<BoundComparisonExpression>(ExpressionType::COMPARE_GREATERTHAN,
573: 			                                                       row_num_ref->Copy(), std::move(lower_bound));
574: 			if (condition) {
575: 				auto conj = make_uniq<BoundConjunctionExpression>(ExpressionType::CONJUNCTION_AND,
576: 				                                                  std::move(lower_comp), std::move(condition));
577: 				condition = std::move(conj);
578: 			} else {
579: 				condition = std::move(lower_comp);
580: 			}
581: 		}
582: 		filter->expressions.push_back(std::move(condition));
583: 		filter->children.push_back(std::move(window));
584: 		// we prune away the row_number after the filter clause using the projection map
585: 		for (idx_t i = 0; i < child_column_count; i++) {
586: 			filter->projection_map.push_back(i);
587: 		}
588: 		return std::move(filter);
589: 	}
590: 	case LogicalOperatorType::LOGICAL_WINDOW: {
591: 		auto &window = plan->Cast<LogicalWindow>();
592: 		// push into children
593: 		plan->children[0] =
594: 		    PushDownDependentJoinInternal(std::move(plan->children[0]), parent_propagate_null_values, lateral_depth);
595: 
596: 		// we replace any correlated expressions with the corresponding entry in the correlated_map
597: 		RewriteCorrelatedExpressions rewriter(base_binding, correlated_map, lateral_depth);
598: 		rewriter.VisitOperator(*plan);
599: 
600: 		// add the correlated columns to the PARTITION BY clauses in the Window
601: 		for (auto &expr : window.expressions) {
602: 			D_ASSERT(expr->GetExpressionClass() == ExpressionClass::BOUND_WINDOW);
603: 			auto &w = expr->Cast<BoundWindowExpression>();
604: 			for (idx_t i = 0; i < correlated_columns.size(); i++) {
605: 				w.partitions.push_back(make_uniq<BoundColumnRefExpression>(
606: 				    correlated_columns[i].type,
607: 				    ColumnBinding(base_binding.table_index, base_binding.column_index + i)));
608: 			}
609: 		}
610: 		return plan;
611: 	}
612: 	case LogicalOperatorType::LOGICAL_EXCEPT:
613: 	case LogicalOperatorType::LOGICAL_INTERSECT:
614: 	case LogicalOperatorType::LOGICAL_UNION: {
615: 		auto &setop = plan->Cast<LogicalSetOperation>();
616: 		// set operator, push into both children
617: #ifdef DEBUG
618: 		plan->children[0]->ResolveOperatorTypes();
619: 		plan->children[1]->ResolveOperatorTypes();
620: 		D_ASSERT(plan->children[0]->types == plan->children[1]->types);
621: #endif
622: 		plan->children[0] = PushDownDependentJoin(std::move(plan->children[0]));
623: 		plan->children[1] = PushDownDependentJoin(std::move(plan->children[1]));
624: 		for (idx_t i = 0; i < plan->children.size(); i++) {
625: 			if (plan->children[i]->type == LogicalOperatorType::LOGICAL_CROSS_PRODUCT) {
626: 				auto proj_index = binder.GenerateTableIndex();
627: 				auto bindings = plan->children[i]->GetColumnBindings();
628: 				plan->children[i]->ResolveOperatorTypes();
629: 				auto types = plan->children[i]->types;
630: 				vector<unique_ptr<Expression>> expressions;
631: 				expressions.reserve(bindings.size());
632: 				D_ASSERT(bindings.size() == types.size());
633: 
634: 				// No column binding replaceent is needed because the parent operator is
635: 				// a setop which will immediately assign new bindings.
636: 				for (idx_t col_idx = 0; col_idx < bindings.size(); col_idx++) {
637: 					expressions.push_back(make_uniq<BoundColumnRefExpression>(types[col_idx], bindings[col_idx]));
638: 				}
639: 				auto proj = make_uniq<LogicalProjection>(proj_index, std::move(expressions));
640: 				proj->children.push_back(std::move(plan->children[i]));
641: 				plan->children[i] = std::move(proj);
642: 			}
643: 		}
644: 
645: 		// here we need to check the children. If they have reorderable bindings, you need to plan a projection
646: 		// on top that will guarantee the order of the bindings.
647: #ifdef DEBUG
648: 		D_ASSERT(plan->children[0]->GetColumnBindings().size() == plan->children[1]->GetColumnBindings().size());
649: 		plan->children[0]->ResolveOperatorTypes();
650: 		plan->children[1]->ResolveOperatorTypes();
651: 		D_ASSERT(plan->children[0]->types == plan->children[1]->types);
652: #endif
653: 		// we have to refer to the setop index now
654: 		base_binding.table_index = setop.table_index;
655: 		base_binding.column_index = setop.column_count;
656: 		setop.column_count += correlated_columns.size();
657: 		return plan;
658: 	}
659: 	case LogicalOperatorType::LOGICAL_DISTINCT: {
660: 		auto &distinct = plan->Cast<LogicalDistinct>();
661: 		// push down into child
662: 		distinct.children[0] = PushDownDependentJoin(std::move(distinct.children[0]));
663: 		// add all correlated columns to the distinct targets
664: 		for (idx_t i = 0; i < correlated_columns.size(); i++) {
665: 			distinct.distinct_targets.push_back(make_uniq<BoundColumnRefExpression>(
666: 			    correlated_columns[i].type, ColumnBinding(base_binding.table_index, base_binding.column_index + i)));
667: 		}
668: 		return plan;
669: 	}
670: 	case LogicalOperatorType::LOGICAL_EXPRESSION_GET: {
671: 		// expression get
672: 		// first we flatten the dependent join in the child
673: 		plan->children[0] =
674: 		    PushDownDependentJoinInternal(std::move(plan->children[0]), parent_propagate_null_values, lateral_depth);
675: 		// then we replace any correlated expressions with the corresponding entry in the correlated_map
676: 		RewriteCorrelatedExpressions rewriter(base_binding, correlated_map, lateral_depth);
677: 		rewriter.VisitOperator(*plan);
678: 		// now we add all the correlated columns to each of the expressions of the expression scan
679: 		auto &expr_get = plan->Cast<LogicalExpressionGet>();
680: 		for (idx_t i = 0; i < correlated_columns.size(); i++) {
681: 			for (auto &expr_list : expr_get.expressions) {
682: 				auto colref = make_uniq<BoundColumnRefExpression>(
683: 				    correlated_columns[i].type, ColumnBinding(base_binding.table_index, base_binding.column_index + i));
684: 				expr_list.push_back(std::move(colref));
685: 			}
686: 			expr_get.expr_types.push_back(correlated_columns[i].type);
687: 		}
688: 
689: 		base_binding.table_index = expr_get.table_index;
690: 		this->delim_offset = base_binding.column_index = expr_get.expr_types.size() - correlated_columns.size();
691: 		this->data_offset = 0;
692: 		return plan;
693: 	}
694: 	case LogicalOperatorType::LOGICAL_PIVOT:
695: 		throw BinderException("PIVOT is not supported in correlated subqueries yet");
696: 	case LogicalOperatorType::LOGICAL_ORDER_BY:
697: 		plan->children[0] = PushDownDependentJoin(std::move(plan->children[0]));
698: 		return plan;
699: 	case LogicalOperatorType::LOGICAL_GET: {
700: 		auto &get = plan->Cast<LogicalGet>();
701: 		if (get.children.size() != 1) {
702: 			throw InternalException("Flatten dependent joins - logical get encountered without children");
703: 		}
704: 		plan->children[0] = PushDownDependentJoin(std::move(plan->children[0]));
705: 		for (idx_t i = 0; i < correlated_columns.size(); i++) {
706: 			get.projected_input.push_back(this->delim_offset + i);
707: 		}
708: 		this->delim_offset = get.returned_types.size();
709: 		this->data_offset = 0;
710: 
711: 		RewriteCorrelatedExpressions rewriter(base_binding, correlated_map, lateral_depth);
712: 		rewriter.VisitOperator(*plan);
713: 		return plan;
714: 	}
715: 	case LogicalOperatorType::LOGICAL_MATERIALIZED_CTE:
716: 	case LogicalOperatorType::LOGICAL_RECURSIVE_CTE: {
717: 
718: #ifdef DEBUG
719: 		plan->children[0]->ResolveOperatorTypes();
720: 		plan->children[1]->ResolveOperatorTypes();
721: #endif
722: 		idx_t table_index = 0;
723: 		plan->children[0] =
724: 		    PushDownDependentJoinInternal(std::move(plan->children[0]), parent_propagate_null_values, lateral_depth);
725: 		if (plan->type == LogicalOperatorType::LOGICAL_RECURSIVE_CTE) {
726: 			auto &setop = plan->Cast<LogicalRecursiveCTE>();
727: 			base_binding.table_index = setop.table_index;
728: 			base_binding.column_index = setop.column_count;
729: 			table_index = setop.table_index;
730: 			setop.correlated_columns = correlated_columns;
731: 		} else if (plan->type == LogicalOperatorType::LOGICAL_MATERIALIZED_CTE) {
732: 			auto &setop = plan->Cast<LogicalMaterializedCTE>();
733: 			base_binding.table_index = setop.table_index;
734: 			base_binding.column_index = setop.column_count;
735: 			table_index = setop.table_index;
736: 		}
737: 
738: 		RewriteCTEScan cte_rewriter(table_index, correlated_columns);
739: 		cte_rewriter.VisitOperator(*plan->children[1]);
740: 
741: 		parent_propagate_null_values = false;
742: 		plan->children[1] =
743: 		    PushDownDependentJoinInternal(std::move(plan->children[1]), parent_propagate_null_values, lateral_depth);
744: 		RewriteCorrelatedExpressions rewriter(this->base_binding, correlated_map, lateral_depth);
745: 		rewriter.VisitOperator(*plan);
746: 
747: 		RewriteCorrelatedExpressions recursive_rewriter(this->base_binding, correlated_map, lateral_depth, true);
748: 		recursive_rewriter.VisitOperator(*plan->children[0]);
749: 		recursive_rewriter.VisitOperator(*plan->children[1]);
750: 
751: #ifdef DEBUG
752: 		plan->children[0]->ResolveOperatorTypes();
753: 		plan->children[1]->ResolveOperatorTypes();
754: #endif
755: 		if (plan->type == LogicalOperatorType::LOGICAL_RECURSIVE_CTE) {
756: 			// we have to refer to the recursive CTE index now
757: 			auto &setop = plan->Cast<LogicalRecursiveCTE>();
758: 			base_binding.table_index = setop.table_index;
759: 			base_binding.column_index = setop.column_count;
760: 			setop.column_count += correlated_columns.size();
761: 		}
762: 
763: 		return plan;
764: 	}
765: 	case LogicalOperatorType::LOGICAL_CTE_REF: {
766: 		auto &cteref = plan->Cast<LogicalCTERef>();
767: 		// Read correlated columns from CTE_SCAN instead of from DELIM_SCAN
768: 		base_binding.table_index = cteref.table_index;
769: 		base_binding.column_index = cteref.chunk_types.size() - cteref.correlated_columns;
770: 		return plan;
771: 	}
772: 	case LogicalOperatorType::LOGICAL_DELIM_JOIN: {
773: 		throw BinderException("Nested lateral joins or lateral joins in correlated subqueries are not (yet) supported");
774: 	}
775: 	case LogicalOperatorType::LOGICAL_SAMPLE:
776: 		throw BinderException("Sampling in correlated subqueries is not (yet) supported");
777: 	case LogicalOperatorType::LOGICAL_POSITIONAL_JOIN:
778: 		throw BinderException("Positional join in correlated subqueries is not (yet) supported");
779: 	default:
780: 		throw InternalException("Logical operator type \"%s\" for dependent join", LogicalOperatorToString(plan->type));
781: 	}
782: }
783: 
784: } // namespace duckdb
[end of src/planner/subquery/flatten_dependent_join.cpp]
[start of src/storage/data_table.cpp]
1: #include "duckdb/storage/data_table.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/chrono.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/exception/transaction_exception.hpp"
7: #include "duckdb/common/helper.hpp"
8: #include "duckdb/common/types/conflict_manager.hpp"
9: #include "duckdb/common/types/constraint_conflict_info.hpp"
10: #include "duckdb/common/vector_operations/vector_operations.hpp"
11: #include "duckdb/execution/expression_executor.hpp"
12: #include "duckdb/main/attached_database.hpp"
13: #include "duckdb/main/client_context.hpp"
14: #include "duckdb/parser/constraints/list.hpp"
15: #include "duckdb/planner/constraints/list.hpp"
16: #include "duckdb/planner/expression/bound_constant_expression.hpp"
17: #include "duckdb/planner/expression/bound_reference_expression.hpp"
18: #include "duckdb/planner/expression_binder/check_binder.hpp"
19: #include "duckdb/planner/expression_binder/constant_binder.hpp"
20: #include "duckdb/planner/table_filter.hpp"
21: #include "duckdb/storage/checkpoint/table_data_writer.hpp"
22: #include "duckdb/storage/storage_manager.hpp"
23: #include "duckdb/storage/table/append_state.hpp"
24: #include "duckdb/storage/table/delete_state.hpp"
25: #include "duckdb/storage/table/persistent_table_data.hpp"
26: #include "duckdb/storage/table/row_group.hpp"
27: #include "duckdb/storage/table/scan_state.hpp"
28: #include "duckdb/storage/table/standard_column_data.hpp"
29: #include "duckdb/storage/table/update_state.hpp"
30: #include "duckdb/storage/table_storage_info.hpp"
31: #include "duckdb/transaction/duck_transaction.hpp"
32: 
33: namespace duckdb {
34: 
35: DataTableInfo::DataTableInfo(AttachedDatabase &db, shared_ptr<TableIOManager> table_io_manager_p, string schema,
36:                              string table)
37:     : db(db), table_io_manager(std::move(table_io_manager_p)), schema(std::move(schema)), table(std::move(table)) {
38: }
39: 
40: void DataTableInfo::InitializeIndexes(ClientContext &context, const char *index_type) {
41: 	indexes.InitializeIndexes(context, *this, index_type);
42: }
43: 
44: bool DataTableInfo::IsTemporary() const {
45: 	return db.IsTemporary();
46: }
47: 
48: DataTable::DataTable(AttachedDatabase &db, shared_ptr<TableIOManager> table_io_manager_p, const string &schema,
49:                      const string &table, vector<ColumnDefinition> column_definitions_p,
50:                      unique_ptr<PersistentTableData> data)
51:     : db(db), info(make_shared_ptr<DataTableInfo>(db, std::move(table_io_manager_p), schema, table)),
52:       column_definitions(std::move(column_definitions_p)), is_root(true) {
53: 	// initialize the table with the existing data from disk, if any
54: 	auto types = GetTypes();
55: 	auto &io_manager = TableIOManager::Get(*this);
56: 	this->row_groups = make_shared_ptr<RowGroupCollection>(info, io_manager, types, 0);
57: 	if (data && data->row_group_count > 0) {
58: 		this->row_groups->Initialize(*data);
59: 	} else {
60: 		this->row_groups->InitializeEmpty();
61: 		D_ASSERT(row_groups->GetTotalRows() == 0);
62: 	}
63: 	row_groups->Verify();
64: }
65: 
66: DataTable::DataTable(ClientContext &context, DataTable &parent, ColumnDefinition &new_column, Expression &default_value)
67:     : db(parent.db), info(parent.info), is_root(true) {
68: 	// add the column definitions from this DataTable
69: 	for (auto &column_def : parent.column_definitions) {
70: 		column_definitions.emplace_back(column_def.Copy());
71: 	}
72: 	column_definitions.emplace_back(new_column.Copy());
73: 
74: 	auto &local_storage = LocalStorage::Get(context, db);
75: 
76: 	ExpressionExecutor default_executor(context);
77: 	default_executor.AddExpression(default_value);
78: 
79: 	// prevent any new tuples from being added to the parent
80: 	lock_guard<mutex> parent_lock(parent.append_lock);
81: 
82: 	this->row_groups = parent.row_groups->AddColumn(context, new_column, default_executor);
83: 
84: 	// also add this column to client local storage
85: 	local_storage.AddColumn(parent, *this, new_column, default_executor);
86: 
87: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
88: 	parent.is_root = false;
89: }
90: 
91: DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t removed_column)
92:     : db(parent.db), info(parent.info), is_root(true) {
93: 	// prevent any new tuples from being added to the parent
94: 	auto &local_storage = LocalStorage::Get(context, db);
95: 	lock_guard<mutex> parent_lock(parent.append_lock);
96: 
97: 	for (auto &column_def : parent.column_definitions) {
98: 		column_definitions.emplace_back(column_def.Copy());
99: 	}
100: 
101: 	info->InitializeIndexes(context);
102: 
103: 	// first check if there are any indexes that exist that point to the removed column
104: 	info->indexes.Scan([&](Index &index) {
105: 		for (auto &column_id : index.GetColumnIds()) {
106: 			if (column_id == removed_column) {
107: 				throw CatalogException("Cannot drop this column: an index depends on it!");
108: 			} else if (column_id > removed_column) {
109: 				throw CatalogException("Cannot drop this column: an index depends on a column after it!");
110: 			}
111: 		}
112: 		return false;
113: 	});
114: 
115: 	// erase the column definitions from this DataTable
116: 	D_ASSERT(removed_column < column_definitions.size());
117: 	column_definitions.erase_at(removed_column);
118: 
119: 	storage_t storage_idx = 0;
120: 	for (idx_t i = 0; i < column_definitions.size(); i++) {
121: 		auto &col = column_definitions[i];
122: 		col.SetOid(i);
123: 		if (col.Generated()) {
124: 			continue;
125: 		}
126: 		col.SetStorageOid(storage_idx++);
127: 	}
128: 
129: 	// alter the row_groups and remove the column from each of them
130: 	this->row_groups = parent.row_groups->RemoveColumn(removed_column);
131: 
132: 	// scan the original table, and fill the new column with the transformed value
133: 	local_storage.DropColumn(parent, *this, removed_column);
134: 
135: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
136: 	parent.is_root = false;
137: }
138: 
139: DataTable::DataTable(ClientContext &context, DataTable &parent, BoundConstraint &constraint)
140:     : db(parent.db), info(parent.info), row_groups(parent.row_groups), is_root(true) {
141: 
142: 	// ALTER COLUMN to add a new constraint.
143: 
144: 	// Clone the storage info vector or the table.
145: 	for (const auto &index_info : parent.info->index_storage_infos) {
146: 		info->index_storage_infos.push_back(IndexStorageInfo(index_info.name));
147: 	}
148: 	info->InitializeIndexes(context);
149: 
150: 	auto &local_storage = LocalStorage::Get(context, db);
151: 	lock_guard<mutex> parent_lock(parent.append_lock);
152: 	for (auto &column_def : parent.column_definitions) {
153: 		column_definitions.emplace_back(column_def.Copy());
154: 	}
155: 
156: 	if (constraint.type != ConstraintType::UNIQUE) {
157: 		VerifyNewConstraint(local_storage, parent, constraint);
158: 	}
159: 	local_storage.MoveStorage(parent, *this);
160: 	parent.is_root = false;
161: }
162: 
163: DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t changed_idx, const LogicalType &target_type,
164:                      const vector<StorageIndex> &bound_columns, Expression &cast_expr)
165:     : db(parent.db), info(parent.info), is_root(true) {
166: 	auto &local_storage = LocalStorage::Get(context, db);
167: 	// prevent any tuples from being added to the parent
168: 	lock_guard<mutex> lock(append_lock);
169: 	for (auto &column_def : parent.column_definitions) {
170: 		column_definitions.emplace_back(column_def.Copy());
171: 	}
172: 
173: 	info->InitializeIndexes(context);
174: 
175: 	// first check if there are any indexes that exist that point to the changed column
176: 	info->indexes.Scan([&](Index &index) {
177: 		for (auto &column_id : index.GetColumnIds()) {
178: 			if (column_id == changed_idx) {
179: 				throw CatalogException("Cannot change the type of this column: an index depends on it!");
180: 			}
181: 		}
182: 		return false;
183: 	});
184: 
185: 	// change the type in this DataTable
186: 	column_definitions[changed_idx].SetType(target_type);
187: 
188: 	// set up the statistics for the table
189: 	// the column that had its type changed will have the new statistics computed during conversion
190: 	this->row_groups = parent.row_groups->AlterType(context, changed_idx, target_type, bound_columns, cast_expr);
191: 
192: 	// scan the original table, and fill the new column with the transformed value
193: 	local_storage.ChangeType(parent, *this, changed_idx, target_type, bound_columns, cast_expr);
194: 
195: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
196: 	parent.is_root = false;
197: }
198: 
199: vector<LogicalType> DataTable::GetTypes() {
200: 	vector<LogicalType> types;
201: 	for (auto &it : column_definitions) {
202: 		types.push_back(it.Type());
203: 	}
204: 	return types;
205: }
206: 
207: bool DataTable::IsTemporary() const {
208: 	return info->IsTemporary();
209: }
210: 
211: AttachedDatabase &DataTable::GetAttached() {
212: 	D_ASSERT(RefersToSameObject(db, info->db));
213: 	return db;
214: }
215: 
216: const vector<ColumnDefinition> &DataTable::Columns() const {
217: 	return column_definitions;
218: }
219: 
220: TableIOManager &DataTable::GetTableIOManager() {
221: 	return *info->table_io_manager;
222: }
223: 
224: TableIOManager &TableIOManager::Get(DataTable &table) {
225: 	return table.GetTableIOManager();
226: }
227: 
228: //===--------------------------------------------------------------------===//
229: // Scan
230: //===--------------------------------------------------------------------===//
231: void DataTable::InitializeScan(DuckTransaction &transaction, TableScanState &state,
232:                                const vector<StorageIndex> &column_ids, TableFilterSet *table_filters) {
233: 	state.checkpoint_lock = transaction.SharedLockTable(*info);
234: 	auto &local_storage = LocalStorage::Get(transaction);
235: 	state.Initialize(column_ids, table_filters);
236: 	row_groups->InitializeScan(state.table_state, column_ids, table_filters);
237: 	local_storage.InitializeScan(*this, state.local_state, table_filters);
238: }
239: 
240: void DataTable::InitializeScanWithOffset(DuckTransaction &transaction, TableScanState &state,
241:                                          const vector<StorageIndex> &column_ids, idx_t start_row, idx_t end_row) {
242: 	state.checkpoint_lock = transaction.SharedLockTable(*info);
243: 	state.Initialize(column_ids);
244: 	row_groups->InitializeScanWithOffset(state.table_state, column_ids, start_row, end_row);
245: }
246: 
247: idx_t DataTable::GetRowGroupSize() const {
248: 	return row_groups->GetRowGroupSize();
249: }
250: 
251: vector<PartitionStatistics> DataTable::GetPartitionStats(ClientContext &context) {
252: 	auto result = row_groups->GetPartitionStats();
253: 	auto &local_storage = LocalStorage::Get(context, db);
254: 	auto local_partitions = local_storage.GetPartitionStats(*this);
255: 	result.insert(result.end(), local_partitions.begin(), local_partitions.end());
256: 	return result;
257: }
258: 
259: idx_t DataTable::MaxThreads(ClientContext &context) const {
260: 	idx_t row_group_size = GetRowGroupSize();
261: 	idx_t parallel_scan_vector_count = row_group_size / STANDARD_VECTOR_SIZE;
262: 	if (ClientConfig::GetConfig(context).verify_parallelism) {
263: 		parallel_scan_vector_count = 1;
264: 	}
265: 	idx_t parallel_scan_tuple_count = STANDARD_VECTOR_SIZE * parallel_scan_vector_count;
266: 	return GetTotalRows() / parallel_scan_tuple_count + 1;
267: }
268: 
269: void DataTable::InitializeParallelScan(ClientContext &context, ParallelTableScanState &state) {
270: 	auto &local_storage = LocalStorage::Get(context, db);
271: 	auto &transaction = DuckTransaction::Get(context, db);
272: 	state.checkpoint_lock = transaction.SharedLockTable(*info);
273: 	row_groups->InitializeParallelScan(state.scan_state);
274: 
275: 	local_storage.InitializeParallelScan(*this, state.local_state);
276: }
277: 
278: bool DataTable::NextParallelScan(ClientContext &context, ParallelTableScanState &state, TableScanState &scan_state) {
279: 	if (row_groups->NextParallelScan(context, state.scan_state, scan_state.table_state)) {
280: 		return true;
281: 	}
282: 	auto &local_storage = LocalStorage::Get(context, db);
283: 	if (local_storage.NextParallelScan(context, *this, state.local_state, scan_state.local_state)) {
284: 		return true;
285: 	} else {
286: 		// finished all scans: no more scans remaining
287: 		return false;
288: 	}
289: }
290: 
291: void DataTable::Scan(DuckTransaction &transaction, DataChunk &result, TableScanState &state) {
292: 	// scan the persistent segments
293: 	if (state.table_state.Scan(transaction, result)) {
294: 		D_ASSERT(result.size() > 0);
295: 		return;
296: 	}
297: 
298: 	// scan the transaction-local segments
299: 	auto &local_storage = LocalStorage::Get(transaction);
300: 	local_storage.Scan(state.local_state, state.GetColumnIds(), result);
301: }
302: 
303: bool DataTable::CreateIndexScan(TableScanState &state, DataChunk &result, TableScanType type) {
304: 	return state.table_state.ScanCommitted(result, type);
305: }
306: 
307: //===--------------------------------------------------------------------===//
308: // Index Methods
309: //===--------------------------------------------------------------------===//
310: shared_ptr<DataTableInfo> &DataTable::GetDataTableInfo() {
311: 	return info;
312: }
313: 
314: void DataTable::InitializeIndexes(ClientContext &context) {
315: 	info->InitializeIndexes(context);
316: }
317: 
318: bool DataTable::HasIndexes() const {
319: 	return !info->indexes.Empty();
320: }
321: 
322: bool DataTable::HasUniqueIndexes() const {
323: 	if (!HasIndexes()) {
324: 		return false;
325: 	}
326: 	bool has_unique_index = false;
327: 	info->indexes.Scan([&](Index &index) {
328: 		if (index.IsUnique()) {
329: 			has_unique_index = true;
330: 			return true;
331: 		}
332: 		return false;
333: 	});
334: 	return has_unique_index;
335: }
336: 
337: void DataTable::AddIndex(unique_ptr<Index> index) {
338: 	info->indexes.AddIndex(std::move(index));
339: }
340: 
341: bool DataTable::HasForeignKeyIndex(const vector<PhysicalIndex> &keys, ForeignKeyType type) {
342: 	auto index = info->indexes.FindForeignKeyIndex(keys, type);
343: 	return index != nullptr;
344: }
345: 
346: void DataTable::SetIndexStorageInfo(vector<IndexStorageInfo> index_storage_info) {
347: 	info->index_storage_infos = std::move(index_storage_info);
348: }
349: 
350: void DataTable::VacuumIndexes() {
351: 	info->indexes.Scan([&](Index &index) {
352: 		if (index.IsBound()) {
353: 			index.Cast<BoundIndex>().Vacuum();
354: 		}
355: 		return false;
356: 	});
357: }
358: 
359: void DataTable::CleanupAppend(transaction_t lowest_transaction, idx_t start, idx_t count) {
360: 	row_groups->CleanupAppend(lowest_transaction, start, count);
361: }
362: 
363: bool DataTable::IndexNameIsUnique(const string &name) {
364: 	return info->indexes.NameIsUnique(name);
365: }
366: 
367: string DataTableInfo::GetSchemaName() {
368: 	return schema;
369: }
370: 
371: string DataTableInfo::GetTableName() {
372: 	lock_guard<mutex> l(name_lock);
373: 	return table;
374: }
375: 
376: void DataTableInfo::SetTableName(string name) {
377: 	lock_guard<mutex> l(name_lock);
378: 	table = std::move(name);
379: }
380: 
381: string DataTable::GetTableName() const {
382: 	return info->GetTableName();
383: }
384: 
385: void DataTable::SetTableName(string new_name) {
386: 	info->SetTableName(std::move(new_name));
387: }
388: 
389: TableStorageInfo DataTable::GetStorageInfo() {
390: 	TableStorageInfo result;
391: 	result.cardinality = GetTotalRows();
392: 	info->indexes.Scan([&](Index &index) {
393: 		IndexInfo index_info;
394: 		index_info.is_primary = index.IsPrimary();
395: 		index_info.is_unique = index.IsUnique() || index_info.is_primary;
396: 		index_info.is_foreign = index.IsForeign();
397: 		index_info.column_set = index.GetColumnIdSet();
398: 		result.index_info.push_back(std::move(index_info));
399: 		return false;
400: 	});
401: 	return result;
402: }
403: 
404: //===--------------------------------------------------------------------===//
405: // Fetch
406: //===--------------------------------------------------------------------===//
407: void DataTable::Fetch(DuckTransaction &transaction, DataChunk &result, const vector<StorageIndex> &column_ids,
408:                       const Vector &row_identifiers, idx_t fetch_count, ColumnFetchState &state) {
409: 	auto lock = info->checkpoint_lock.GetSharedLock();
410: 	row_groups->Fetch(transaction, result, column_ids, row_identifiers, fetch_count, state);
411: }
412: 
413: //===--------------------------------------------------------------------===//
414: // Append
415: //===--------------------------------------------------------------------===//
416: static void VerifyNotNullConstraint(TableCatalogEntry &table, Vector &vector, idx_t count, const string &col_name) {
417: 	if (!VectorOperations::HasNull(vector, count)) {
418: 		return;
419: 	}
420: 
421: 	throw ConstraintException("NOT NULL constraint failed: %s.%s", table.name, col_name);
422: }
423: 
424: // To avoid throwing an error at SELECT, instead this moves the error detection to INSERT
425: static void VerifyGeneratedExpressionSuccess(ClientContext &context, TableCatalogEntry &table, DataChunk &chunk,
426:                                              Expression &expr, column_t index) {
427: 	auto &col = table.GetColumn(LogicalIndex(index));
428: 	D_ASSERT(col.Generated());
429: 	ExpressionExecutor executor(context, expr);
430: 	Vector result(col.Type());
431: 	try {
432: 		executor.ExecuteExpression(chunk, result);
433: 	} catch (InternalException &ex) {
434: 		throw;
435: 	} catch (std::exception &ex) {
436: 		ErrorData error(ex);
437: 		throw ConstraintException("Incorrect value for generated column \"%s %s AS (%s)\" : %s", col.Name(),
438: 		                          col.Type().ToString(), col.GeneratedExpression().ToString(), error.RawMessage());
439: 	}
440: }
441: 
442: static void VerifyCheckConstraint(ClientContext &context, TableCatalogEntry &table, Expression &expr, DataChunk &chunk,
443:                                   CheckConstraint &check) {
444: 	ExpressionExecutor executor(context, expr);
445: 	Vector result(LogicalType::INTEGER);
446: 	try {
447: 		executor.ExecuteExpression(chunk, result);
448: 	} catch (std::exception &ex) {
449: 		ErrorData error(ex);
450: 		throw ConstraintException("CHECK constraint failed on table %s with expression %s (Error: %s)", table.name,
451: 		                          check.ToString(), error.RawMessage());
452: 	} catch (...) {
453: 		// LCOV_EXCL_START
454: 		throw ConstraintException("CHECK constraint failed on table %s with expression %s (Unknown Error)", table.name,
455: 		                          check.ToString());
456: 	} // LCOV_EXCL_STOP
457: 	UnifiedVectorFormat vdata;
458: 	result.ToUnifiedFormat(chunk.size(), vdata);
459: 
460: 	auto dataptr = UnifiedVectorFormat::GetData<int32_t>(vdata);
461: 	for (idx_t i = 0; i < chunk.size(); i++) {
462: 		auto idx = vdata.sel->get_index(i);
463: 		if (vdata.validity.RowIsValid(idx) && dataptr[idx] == 0) {
464: 			throw ConstraintException("CHECK constraint failed on table %s with expression %s", table.name,
465: 			                          check.ToString());
466: 		}
467: 	}
468: }
469: 
470: // Find the first index that is not null, and did not find a match
471: static idx_t FirstMissingMatch(const ManagedSelection &matches) {
472: 	idx_t match_idx = 0;
473: 
474: 	for (idx_t i = 0; i < matches.Size(); i++) {
475: 		auto match = matches.IndexMapsToLocation(match_idx, i);
476: 		match_idx += match;
477: 		if (!match) {
478: 			// This index is missing in the matches vector
479: 			return i;
480: 		}
481: 	}
482: 	return DConstants::INVALID_INDEX;
483: }
484: 
485: idx_t LocateErrorIndex(bool is_append, const ManagedSelection &matches) {
486: 	// We expected to find nothing, so the first error is the first match.
487: 	if (!is_append) {
488: 		return matches[0];
489: 	}
490: 	// We expected to find matches for all of them, so the first missing match is the first error.
491: 	return FirstMissingMatch(matches);
492: }
493: 
494: [[noreturn]] static void ThrowForeignKeyConstraintError(idx_t failed_index, bool is_append, Index &conflict_index,
495:                                                         DataChunk &input) {
496: 	// The index that caused the conflict has to be bound by this point (or we would not have gotten here)
497: 	D_ASSERT(conflict_index.IsBound());
498: 	auto &index = conflict_index.Cast<BoundIndex>();
499: 	auto verify_type = is_append ? VerifyExistenceType::APPEND_FK : VerifyExistenceType::DELETE_FK;
500: 	D_ASSERT(failed_index != DConstants::INVALID_INDEX);
501: 	auto message = index.GetConstraintViolationMessage(verify_type, failed_index, input);
502: 	throw ConstraintException(message);
503: }
504: 
505: bool IsForeignKeyConstraintError(bool is_append, idx_t input_count, const ManagedSelection &matches) {
506: 	if (is_append) {
507: 		// We need to find a match for all values
508: 		return matches.Count() != input_count;
509: 	} else {
510: 		// We should not find any matches
511: 		return matches.Count() != 0;
512: 	}
513: }
514: 
515: static bool IsAppend(VerifyExistenceType verify_type) {
516: 	return verify_type == VerifyExistenceType::APPEND_FK;
517: }
518: 
519: void DataTable::VerifyForeignKeyConstraint(optional_ptr<LocalTableStorage> storage,
520:                                            const BoundForeignKeyConstraint &bound_foreign_key, ClientContext &context,
521:                                            DataChunk &chunk, VerifyExistenceType verify_type) {
522: 	reference<const vector<PhysicalIndex>> src_keys_ptr = bound_foreign_key.info.fk_keys;
523: 	reference<const vector<PhysicalIndex>> dst_keys_ptr = bound_foreign_key.info.pk_keys;
524: 
525: 	bool is_append = IsAppend(verify_type);
526: 	if (!is_append) {
527: 		src_keys_ptr = bound_foreign_key.info.pk_keys;
528: 		dst_keys_ptr = bound_foreign_key.info.fk_keys;
529: 	}
530: 
531: 	// Get the column types in their physical order.
532: 	auto &table_entry = Catalog::GetEntry<TableCatalogEntry>(context, db.GetName(), bound_foreign_key.info.schema,
533: 	                                                         bound_foreign_key.info.table);
534: 	vector<LogicalType> types;
535: 	for (auto &col : table_entry.GetColumns().Physical()) {
536: 		types.emplace_back(col.Type());
537: 	}
538: 
539: 	// Create the data chunk that has to be verified.
540: 	DataChunk dst_chunk;
541: 	dst_chunk.InitializeEmpty(types);
542: 	for (idx_t i = 0; i < src_keys_ptr.get().size(); i++) {
543: 		auto &src_chunk = chunk.data[src_keys_ptr.get()[i].index];
544: 		dst_chunk.data[dst_keys_ptr.get()[i].index].Reference(src_chunk);
545: 	}
546: 
547: 	auto count = chunk.size();
548: 	dst_chunk.SetCardinality(count);
549: 	if (count <= 0) {
550: 		return;
551: 	}
552: 
553: 	// Record conflicts instead of throwing immediately.
554: 	unordered_set<column_t> empty_column_list;
555: 	ConflictInfo empty_conflict_info(empty_column_list, false);
556: 	ConflictManager global_conflicts(verify_type, count, &empty_conflict_info);
557: 	ConflictManager local_conflicts(verify_type, count, &empty_conflict_info);
558: 	global_conflicts.SetMode(ConflictManagerMode::SCAN);
559: 	local_conflicts.SetMode(ConflictManagerMode::SCAN);
560: 
561: 	// Global constraint verification.
562: 	auto &data_table = table_entry.GetStorage();
563: 	data_table.info->indexes.VerifyForeignKey(storage, dst_keys_ptr, dst_chunk, global_conflicts);
564: 	global_conflicts.Finalize();
565: 	auto &global_matches = global_conflicts.Conflicts();
566: 
567: 	// Check if we can insert the chunk into the local storage.
568: 	auto &local_storage = LocalStorage::Get(context, db);
569: 	bool local_error = false;
570: 	auto local_verification = local_storage.Find(data_table);
571: 
572: 	// Local constraint verification.
573: 	if (local_verification) {
574: 		auto &local_indexes = local_storage.GetIndexes(data_table);
575: 		local_indexes.VerifyForeignKey(storage, dst_keys_ptr, dst_chunk, local_conflicts);
576: 		local_conflicts.Finalize();
577: 		auto &local_matches = local_conflicts.Conflicts();
578: 		local_error = IsForeignKeyConstraintError(is_append, count, local_matches);
579: 	}
580: 
581: 	// No constraint violation.
582: 	auto global_error = IsForeignKeyConstraintError(is_append, count, global_matches);
583: 	if (!global_error && !local_error) {
584: 		return;
585: 	}
586: 
587: 	// Some error occurred, and we likely want to throw
588: 	optional_ptr<Index> index;
589: 	optional_ptr<Index> transaction_index;
590: 
591: 	auto fk_type = is_append ? ForeignKeyType::FK_TYPE_PRIMARY_KEY_TABLE : ForeignKeyType::FK_TYPE_FOREIGN_KEY_TABLE;
592: 	// check whether or not the chunk can be inserted or deleted into the referenced table' storage
593: 	index = data_table.info->indexes.FindForeignKeyIndex(dst_keys_ptr, fk_type);
594: 	if (local_verification) {
595: 		auto &transact_index = local_storage.GetIndexes(data_table);
596: 		// check whether or not the chunk can be inserted or deleted into the referenced table' storage
597: 		transaction_index = transact_index.FindForeignKeyIndex(dst_keys_ptr, fk_type);
598: 	}
599: 
600: 	if (!local_verification) {
601: 		// Only local state is checked, throw the error
602: 		D_ASSERT(global_error);
603: 		auto failed_index = LocateErrorIndex(is_append, global_matches);
604: 		D_ASSERT(failed_index != DConstants::INVALID_INDEX);
605: 		ThrowForeignKeyConstraintError(failed_index, is_append, *index, dst_chunk);
606: 	}
607: 	if (local_error && global_error && is_append) {
608: 		// When we want to do an append, we only throw if the foreign key does not exist in both transaction and local
609: 		// storage
610: 		auto &transaction_matches = local_conflicts.Conflicts();
611: 		idx_t failed_index = DConstants::INVALID_INDEX;
612: 		idx_t regular_idx = 0;
613: 		idx_t transaction_idx = 0;
614: 		for (idx_t i = 0; i < count; i++) {
615: 			bool in_regular = global_matches.IndexMapsToLocation(regular_idx, i);
616: 			regular_idx += in_regular;
617: 			bool in_transaction = transaction_matches.IndexMapsToLocation(transaction_idx, i);
618: 			transaction_idx += in_transaction;
619: 
620: 			if (!in_regular && !in_transaction) {
621: 				// We need to find a match for all of the input values
622: 				// The failed index is i, it does not show up in either regular or transaction storage
623: 				failed_index = i;
624: 				break;
625: 			}
626: 		}
627: 		if (failed_index == DConstants::INVALID_INDEX) {
628: 			// We don't throw, every value was present in either regular or transaction storage
629: 			return;
630: 		}
631: 		ThrowForeignKeyConstraintError(failed_index, true, *index, dst_chunk);
632: 	}
633: 	if (!is_append) {
634: 		D_ASSERT(local_verification);
635: 		auto &transaction_matches = local_conflicts.Conflicts();
636: 		if (global_error) {
637: 			auto failed_index = LocateErrorIndex(false, global_matches);
638: 			D_ASSERT(failed_index != DConstants::INVALID_INDEX);
639: 			ThrowForeignKeyConstraintError(failed_index, false, *index, dst_chunk);
640: 		} else {
641: 			D_ASSERT(local_error);
642: 			D_ASSERT(transaction_matches.Count() != DConstants::INVALID_INDEX);
643: 			auto failed_index = LocateErrorIndex(false, transaction_matches);
644: 			D_ASSERT(failed_index != DConstants::INVALID_INDEX);
645: 			ThrowForeignKeyConstraintError(failed_index, false, *transaction_index, dst_chunk);
646: 		}
647: 	}
648: }
649: 
650: void DataTable::VerifyAppendForeignKeyConstraint(optional_ptr<LocalTableStorage> storage,
651:                                                  const BoundForeignKeyConstraint &bound_foreign_key,
652:                                                  ClientContext &context, DataChunk &chunk) {
653: 	VerifyForeignKeyConstraint(storage, bound_foreign_key, context, chunk, VerifyExistenceType::APPEND_FK);
654: }
655: 
656: void DataTable::VerifyDeleteForeignKeyConstraint(optional_ptr<LocalTableStorage> storage,
657:                                                  const BoundForeignKeyConstraint &bound_foreign_key,
658:                                                  ClientContext &context, DataChunk &chunk) {
659: 	VerifyForeignKeyConstraint(storage, bound_foreign_key, context, chunk, VerifyExistenceType::DELETE_FK);
660: }
661: 
662: void DataTable::VerifyNewConstraint(LocalStorage &local_storage, DataTable &parent, const BoundConstraint &constraint) {
663: 	if (constraint.type != ConstraintType::NOT_NULL) {
664: 		throw NotImplementedException("FIXME: ALTER COLUMN with such constraint is not supported yet");
665: 	}
666: 
667: 	parent.row_groups->VerifyNewConstraint(parent, constraint);
668: 	local_storage.VerifyNewConstraint(parent, constraint);
669: }
670: 
671: void DataTable::VerifyUniqueIndexes(TableIndexList &indexes, optional_ptr<LocalTableStorage> storage, DataChunk &chunk,
672:                                     optional_ptr<ConflictManager> manager) {
673: 	// Verify the constraint without a conflict manager.
674: 	if (!manager) {
675: 		return indexes.ScanBound<ART>([&](ART &art) {
676: 			if (!art.IsUnique()) {
677: 				return false;
678: 			}
679: 
680: 			if (storage) {
681: 				auto delete_index = storage->delete_indexes.Find(art.GetIndexName());
682: 				art.VerifyAppend(chunk, delete_index, nullptr);
683: 			} else {
684: 				art.VerifyAppend(chunk, nullptr, nullptr);
685: 			}
686: 			return false;
687: 		});
688: 	}
689: 
690: 	// The conflict manager is only provided for statements containing ON CONFLICT.
691: 	auto &conflict_info = manager->GetConflictInfo();
692: 
693: 	// Find all indexes matching the conflict target.
694: 	indexes.ScanBound<ART>([&](ART &art) {
695: 		if (!art.IsUnique()) {
696: 			return false;
697: 		}
698: 		if (!conflict_info.ConflictTargetMatches(art)) {
699: 			return false;
700: 		}
701: 
702: 		if (storage) {
703: 			auto delete_index = storage->delete_indexes.Find(art.GetIndexName());
704: 			manager->AddIndex(art, delete_index);
705: 		} else {
706: 			manager->AddIndex(art, nullptr);
707: 		}
708: 		return false;
709: 	});
710: 
711: 	// Verify indexes matching the conflict target.
712: 	manager->SetMode(ConflictManagerMode::SCAN);
713: 	auto &matched_indexes = manager->MatchedIndexes();
714: 	auto &matched_delete_indexes = manager->MatchedDeleteIndexes();
715: 	for (idx_t i = 0; i < matched_indexes.size(); i++) {
716: 		matched_indexes[i].get().VerifyAppend(chunk, matched_delete_indexes[i], *manager);
717: 	}
718: 
719: 	// Scan the other indexes and throw, if there are any conflicts.
720: 	manager->SetMode(ConflictManagerMode::THROW);
721: 	indexes.ScanBound<ART>([&](ART &art) {
722: 		if (!art.IsUnique()) {
723: 			return false;
724: 		}
725: 		if (manager->MatchedIndex(art)) {
726: 			return false;
727: 		}
728: 
729: 		if (storage) {
730: 			auto delete_index = storage->delete_indexes.Find(art.GetIndexName());
731: 			art.VerifyAppend(chunk, delete_index, *manager);
732: 		} else {
733: 			art.VerifyAppend(chunk, nullptr, *manager);
734: 		}
735: 		return false;
736: 	});
737: }
738: 
739: void DataTable::VerifyAppendConstraints(ConstraintState &constraint_state, ClientContext &context, DataChunk &chunk,
740:                                         optional_ptr<LocalTableStorage> storage,
741:                                         optional_ptr<ConflictManager> manager) {
742: 
743: 	auto &table = constraint_state.table;
744: 	if (table.HasGeneratedColumns()) {
745: 		// Verify the generated columns against the inserted values.
746: 		auto binder = Binder::CreateBinder(context);
747: 		physical_index_set_t bound_columns;
748: 		CheckBinder generated_check_binder(*binder, context, table.name, table.GetColumns(), bound_columns);
749: 		for (auto &col : table.GetColumns().Logical()) {
750: 			if (!col.Generated()) {
751: 				continue;
752: 			}
753: 			D_ASSERT(col.Type().id() != LogicalTypeId::ANY);
754: 			generated_check_binder.target_type = col.Type();
755: 			auto to_be_bound_expression = col.GeneratedExpression().Copy();
756: 			auto bound_expression = generated_check_binder.Bind(to_be_bound_expression);
757: 			VerifyGeneratedExpressionSuccess(context, table, chunk, *bound_expression, col.Oid());
758: 		}
759: 	}
760: 
761: 	if (HasUniqueIndexes()) {
762: 		VerifyUniqueIndexes(info->indexes, storage, chunk, manager);
763: 	}
764: 
765: 	auto &constraints = table.GetConstraints();
766: 	for (idx_t i = 0; i < constraint_state.bound_constraints.size(); i++) {
767: 		auto &base_constraint = constraints[i];
768: 		auto &constraint = constraint_state.bound_constraints[i];
769: 		switch (base_constraint->type) {
770: 		case ConstraintType::NOT_NULL: {
771: 			auto &bound_not_null = constraint->Cast<BoundNotNullConstraint>();
772: 			auto &not_null = base_constraint->Cast<NotNullConstraint>();
773: 			auto &col = table.GetColumns().GetColumn(LogicalIndex(not_null.index));
774: 			VerifyNotNullConstraint(table, chunk.data[bound_not_null.index.index], chunk.size(), col.Name());
775: 			break;
776: 		}
777: 		case ConstraintType::CHECK: {
778: 			auto &check = base_constraint->Cast<CheckConstraint>();
779: 			auto &bound_check = constraint->Cast<BoundCheckConstraint>();
780: 			VerifyCheckConstraint(context, table, *bound_check.expression, chunk, check);
781: 			break;
782: 		}
783: 		case ConstraintType::UNIQUE: {
784: 			// These were handled earlier.
785: 			break;
786: 		}
787: 		case ConstraintType::FOREIGN_KEY: {
788: 			auto &bound_foreign_key = constraint->Cast<BoundForeignKeyConstraint>();
789: 			if (bound_foreign_key.info.IsAppendConstraint()) {
790: 				VerifyAppendForeignKeyConstraint(storage, bound_foreign_key, context, chunk);
791: 			}
792: 			break;
793: 		}
794: 		default:
795: 			throw InternalException("invalid constraint type");
796: 		}
797: 	}
798: }
799: 
800: unique_ptr<ConstraintState>
801: DataTable::InitializeConstraintState(TableCatalogEntry &table,
802:                                      const vector<unique_ptr<BoundConstraint>> &bound_constraints) {
803: 	return make_uniq<ConstraintState>(table, bound_constraints);
804: }
805: 
806: void DataTable::InitializeLocalAppend(LocalAppendState &state, TableCatalogEntry &table, ClientContext &context,
807:                                       const vector<unique_ptr<BoundConstraint>> &bound_constraints) {
808: 	if (!is_root) {
809: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
810: 	}
811: 	auto &local_storage = LocalStorage::Get(context, db);
812: 	local_storage.InitializeAppend(state, *this);
813: 	state.constraint_state = InitializeConstraintState(table, bound_constraints);
814: }
815: 
816: void DataTable::InitializeLocalStorage(LocalAppendState &state, TableCatalogEntry &table, ClientContext &context,
817:                                        const vector<unique_ptr<BoundConstraint>> &bound_constraints) {
818: 	if (!is_root) {
819: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
820: 	}
821: 
822: 	auto &local_storage = LocalStorage::Get(context, db);
823: 	local_storage.InitializeStorage(state, *this);
824: 	state.constraint_state = InitializeConstraintState(table, bound_constraints);
825: }
826: 
827: void DataTable::LocalAppend(LocalAppendState &state, ClientContext &context, DataChunk &chunk, bool unsafe) {
828: 	if (chunk.size() == 0) {
829: 		return;
830: 	}
831: 	if (!is_root) {
832: 		throw TransactionException("write conflict: adding entries to a table that has been altered");
833: 	}
834: 	chunk.Verify();
835: 
836: 	// Insert any row ids into the DELETE ART and verify constraints afterward.
837: 	// This happens only for the global indexes.
838: 	if (!unsafe) {
839: 		auto &constraint_state = *state.constraint_state;
840: 		VerifyAppendConstraints(constraint_state, context, chunk, *state.storage, nullptr);
841: 	}
842: 
843: 	// Append to the transaction-local data.
844: 	LocalStorage::Append(state, chunk);
845: }
846: 
847: void DataTable::FinalizeLocalAppend(LocalAppendState &state) {
848: 	LocalStorage::FinalizeAppend(state);
849: }
850: 
851: OptimisticDataWriter &DataTable::CreateOptimisticWriter(ClientContext &context) {
852: 	auto &local_storage = LocalStorage::Get(context, db);
853: 	return local_storage.CreateOptimisticWriter(*this);
854: }
855: 
856: void DataTable::FinalizeOptimisticWriter(ClientContext &context, OptimisticDataWriter &writer) {
857: 	auto &local_storage = LocalStorage::Get(context, db);
858: 	local_storage.FinalizeOptimisticWriter(*this, writer);
859: }
860: 
861: void DataTable::LocalMerge(ClientContext &context, RowGroupCollection &collection) {
862: 	auto &local_storage = LocalStorage::Get(context, db);
863: 	local_storage.LocalMerge(*this, collection);
864: }
865: 
866: void DataTable::LocalAppend(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,
867:                             const vector<unique_ptr<BoundConstraint>> &bound_constraints) {
868: 	LocalAppendState append_state;
869: 	auto &storage = table.GetStorage();
870: 	storage.InitializeLocalAppend(append_state, table, context, bound_constraints);
871: 
872: 	storage.LocalAppend(append_state, context, chunk, false);
873: 	storage.FinalizeLocalAppend(append_state);
874: }
875: 
876: void DataTable::LocalAppend(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,
877:                             const vector<unique_ptr<BoundConstraint>> &bound_constraints, Vector &row_ids,
878:                             DataChunk &delete_chunk) {
879: 	LocalAppendState append_state;
880: 	auto &storage = table.GetStorage();
881: 	storage.InitializeLocalAppend(append_state, table, context, bound_constraints);
882: 	append_state.storage->AppendToDeleteIndexes(row_ids, delete_chunk);
883: 
884: 	storage.LocalAppend(append_state, context, chunk, false);
885: 	storage.FinalizeLocalAppend(append_state);
886: }
887: 
888: void DataTable::LocalAppend(TableCatalogEntry &table, ClientContext &context, ColumnDataCollection &collection,
889:                             const vector<unique_ptr<BoundConstraint>> &bound_constraints,
890:                             optional_ptr<const vector<LogicalIndex>> column_ids) {
891: 
892: 	LocalAppendState append_state;
893: 	auto &storage = table.GetStorage();
894: 	storage.InitializeLocalAppend(append_state, table, context, bound_constraints);
895: 
896: 	if (!column_ids || column_ids->empty()) {
897: 		for (auto &chunk : collection.Chunks()) {
898: 			storage.LocalAppend(append_state, context, chunk, false);
899: 		}
900: 		storage.FinalizeLocalAppend(append_state);
901: 		return;
902: 	}
903: 
904: 	auto &column_list = table.GetColumns();
905: 	map<PhysicalIndex, unique_ptr<Expression>> active_expressions;
906: 	for (idx_t i = 0; i < column_ids->size(); i++) {
907: 		auto &col = column_list.GetColumn((*column_ids)[i]);
908: 		auto expr = make_uniq<BoundReferenceExpression>(col.Name(), col.Type(), i);
909: 		active_expressions[col.Physical()] = std::move(expr);
910: 	}
911: 
912: 	auto binder = Binder::CreateBinder(context);
913: 	ConstantBinder default_binder(*binder, context, "DEFAULT value");
914: 	vector<unique_ptr<Expression>> expressions;
915: 	for (idx_t i = 0; i < column_list.PhysicalColumnCount(); i++) {
916: 		auto expr = active_expressions.find(PhysicalIndex(i));
917: 		if (expr != active_expressions.end()) {
918: 			expressions.push_back(std::move(expr->second));
919: 			continue;
920: 		}
921: 
922: 		auto &col = column_list.GetColumn(PhysicalIndex(i));
923: 		if (!col.HasDefaultValue()) {
924: 			auto null_expr = make_uniq<BoundConstantExpression>(Value(col.Type()));
925: 			expressions.push_back(std::move(null_expr));
926: 			continue;
927: 		}
928: 
929: 		auto default_copy = col.DefaultValue().Copy();
930: 		default_binder.target_type = col.Type();
931: 		auto bound_default = default_binder.Bind(default_copy);
932: 		expressions.push_back(std::move(bound_default));
933: 	}
934: 
935: 	ExpressionExecutor expression_executor(context, expressions);
936: 	DataChunk result;
937: 	result.Initialize(context, table.GetTypes());
938: 
939: 	for (auto &chunk : collection.Chunks()) {
940: 		expression_executor.Execute(chunk, result);
941: 		storage.LocalAppend(append_state, context, result, false);
942: 		result.Reset();
943: 	}
944: 	storage.FinalizeLocalAppend(append_state);
945: }
946: 
947: void DataTable::AppendLock(TableAppendState &state) {
948: 	state.append_lock = unique_lock<mutex>(append_lock);
949: 	if (!is_root) {
950: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
951: 	}
952: 	state.row_start = NumericCast<row_t>(row_groups->GetTotalRows());
953: 	state.current_row = state.row_start;
954: }
955: 
956: void DataTable::InitializeAppend(DuckTransaction &transaction, TableAppendState &state) {
957: 	// obtain the append lock for this table
958: 	if (!state.append_lock) {
959: 		throw InternalException("DataTable::AppendLock should be called before DataTable::InitializeAppend");
960: 	}
961: 	row_groups->InitializeAppend(transaction, state);
962: }
963: 
964: void DataTable::Append(DataChunk &chunk, TableAppendState &state) {
965: 	D_ASSERT(is_root);
966: 	row_groups->Append(chunk, state);
967: }
968: 
969: void DataTable::FinalizeAppend(DuckTransaction &transaction, TableAppendState &state) {
970: 	row_groups->FinalizeAppend(transaction, state);
971: }
972: 
973: void DataTable::ScanTableSegment(DuckTransaction &transaction, idx_t row_start, idx_t count,
974:                                  const std::function<void(DataChunk &chunk)> &function) {
975: 	if (count == 0) {
976: 		return;
977: 	}
978: 	idx_t end = row_start + count;
979: 
980: 	vector<StorageIndex> column_ids;
981: 	vector<LogicalType> types;
982: 	for (idx_t i = 0; i < this->column_definitions.size(); i++) {
983: 		auto &col = this->column_definitions[i];
984: 		column_ids.emplace_back(i);
985: 		types.push_back(col.Type());
986: 	}
987: 	DataChunk chunk;
988: 	chunk.Initialize(Allocator::Get(db), types);
989: 
990: 	CreateIndexScanState state;
991: 
992: 	InitializeScanWithOffset(transaction, state, column_ids, row_start, row_start + count);
993: 	auto row_start_aligned = state.table_state.row_group->start + state.table_state.vector_index * STANDARD_VECTOR_SIZE;
994: 
995: 	idx_t current_row = row_start_aligned;
996: 	while (current_row < end) {
997: 		state.table_state.ScanCommitted(chunk, TableScanType::TABLE_SCAN_COMMITTED_ROWS);
998: 		if (chunk.size() == 0) {
999: 			break;
1000: 		}
1001: 		idx_t end_row = current_row + chunk.size();
1002: 		// start of chunk is current_row
1003: 		// end of chunk is end_row
1004: 		// figure out if we need to write the entire chunk or just part of it
1005: 		idx_t chunk_start = MaxValue<idx_t>(current_row, row_start);
1006: 		idx_t chunk_end = MinValue<idx_t>(end_row, end);
1007: 		D_ASSERT(chunk_start < chunk_end);
1008: 		idx_t chunk_count = chunk_end - chunk_start;
1009: 		if (chunk_count != chunk.size()) {
1010: 			D_ASSERT(chunk_count <= chunk.size());
1011: 			// need to slice the chunk before insert
1012: 			idx_t start_in_chunk;
1013: 			if (current_row >= row_start) {
1014: 				start_in_chunk = 0;
1015: 			} else {
1016: 				start_in_chunk = row_start - current_row;
1017: 			}
1018: 			SelectionVector sel(start_in_chunk, chunk_count);
1019: 			chunk.Slice(sel, chunk_count);
1020: 			chunk.Verify();
1021: 		}
1022: 		function(chunk);
1023: 		chunk.Reset();
1024: 		current_row = end_row;
1025: 	}
1026: }
1027: 
1028: void DataTable::MergeStorage(RowGroupCollection &data, TableIndexList &,
1029:                              optional_ptr<StorageCommitState> commit_state) {
1030: 	row_groups->MergeStorage(data, this, commit_state);
1031: 	row_groups->Verify();
1032: }
1033: 
1034: void DataTable::WriteToLog(DuckTransaction &transaction, WriteAheadLog &log, idx_t row_start, idx_t count,
1035:                            optional_ptr<StorageCommitState> commit_state) {
1036: 	log.WriteSetTable(info->schema, info->table);
1037: 	if (commit_state) {
1038: 		idx_t optimistic_count = 0;
1039: 		auto entry = commit_state->GetRowGroupData(*this, row_start, optimistic_count);
1040: 		if (entry) {
1041: 			D_ASSERT(optimistic_count > 0);
1042: 			log.WriteRowGroupData(*entry);
1043: 			if (optimistic_count > count) {
1044: 				throw InternalException(
1045: 				    "Optimistically written count cannot exceed actual count (got %llu, but expected count is %llu)",
1046: 				    optimistic_count, count);
1047: 			}
1048: 			// write any remaining (non-optimistically written) rows to the WAL normally
1049: 			row_start += optimistic_count;
1050: 			count -= optimistic_count;
1051: 			if (count == 0) {
1052: 				return;
1053: 			}
1054: 		}
1055: 	}
1056: 	ScanTableSegment(transaction, row_start, count, [&](DataChunk &chunk) { log.WriteInsert(chunk); });
1057: }
1058: 
1059: void DataTable::CommitAppend(transaction_t commit_id, idx_t row_start, idx_t count) {
1060: 	lock_guard<mutex> lock(append_lock);
1061: 	row_groups->CommitAppend(commit_id, row_start, count);
1062: }
1063: 
1064: void DataTable::RevertAppendInternal(idx_t start_row) {
1065: 	D_ASSERT(is_root);
1066: 	// revert appends made to row_groups
1067: 	row_groups->RevertAppendInternal(start_row);
1068: }
1069: 
1070: void DataTable::RevertAppend(DuckTransaction &transaction, idx_t start_row, idx_t count) {
1071: 	lock_guard<mutex> lock(append_lock);
1072: 
1073: 	// revert any appends to indexes
1074: 	if (!info->indexes.Empty()) {
1075: 		idx_t current_row_base = start_row;
1076: 		row_t row_data[STANDARD_VECTOR_SIZE];
1077: 		Vector row_identifiers(LogicalType::ROW_TYPE, data_ptr_cast(row_data));
1078: 		idx_t scan_count = MinValue<idx_t>(count, row_groups->GetTotalRows() - start_row);
1079: 		ScanTableSegment(transaction, start_row, scan_count, [&](DataChunk &chunk) {
1080: 			for (idx_t i = 0; i < chunk.size(); i++) {
1081: 				row_data[i] = NumericCast<row_t>(current_row_base + i);
1082: 			}
1083: 			info->indexes.Scan([&](Index &index) {
1084: 				// We cant add to unbound indexes anyways, so there is no need to revert them
1085: 				if (index.IsBound()) {
1086: 					index.Cast<BoundIndex>().Delete(chunk, row_identifiers);
1087: 				}
1088: 				return false;
1089: 			});
1090: 			current_row_base += chunk.size();
1091: 		});
1092: 	}
1093: 
1094: 	// we need to vacuum the indexes to remove any buffers that are now empty
1095: 	// due to reverting the appends
1096: 	info->indexes.Scan([&](Index &index) {
1097: 		// We cant add to unbound indexes anyway, so there is no need to vacuum them
1098: 		if (index.IsBound()) {
1099: 			index.Cast<BoundIndex>().Vacuum();
1100: 		}
1101: 		return false;
1102: 	});
1103: 
1104: 	// revert the data table append
1105: 	RevertAppendInternal(start_row);
1106: }
1107: 
1108: //===--------------------------------------------------------------------===//
1109: // Indexes
1110: //===--------------------------------------------------------------------===//
1111: ErrorData DataTable::AppendToIndexes(TableIndexList &indexes, optional_ptr<TableIndexList> delete_indexes,
1112:                                      DataChunk &chunk, row_t row_start) {
1113: 	ErrorData error;
1114: 	if (indexes.Empty()) {
1115: 		return error;
1116: 	}
1117: 
1118: 	// first generate the vector of row identifiers
1119: 	Vector row_ids(LogicalType::ROW_TYPE);
1120: 	VectorOperations::GenerateSequence(row_ids, chunk.size(), row_start, 1);
1121: 
1122: 	vector<BoundIndex *> already_appended;
1123: 	bool append_failed = false;
1124: 	// now append the entries to the indices
1125: 	indexes.Scan([&](Index &index_to_append) {
1126: 		if (!index_to_append.IsBound()) {
1127: 			throw InternalException("unbound index in DataTable::AppendToIndexes");
1128: 		}
1129: 		auto &index = index_to_append.Cast<BoundIndex>();
1130: 
1131: 		// Find the matching delete index.
1132: 		optional_ptr<BoundIndex> delete_index;
1133: 		if (index.IsUnique()) {
1134: 			if (delete_indexes) {
1135: 				delete_index = delete_indexes->Find(index.name);
1136: 			}
1137: 		}
1138: 
1139: 		try {
1140: 			error = index.AppendWithDeleteIndex(chunk, row_ids, delete_index);
1141: 		} catch (std::exception &ex) {
1142: 			error = ErrorData(ex);
1143: 		}
1144: 
1145: 		if (error.HasError()) {
1146: 			append_failed = true;
1147: 			return true;
1148: 		}
1149: 		already_appended.push_back(&index);
1150: 		return false;
1151: 	});
1152: 
1153: 	if (append_failed) {
1154: 		// constraint violation!
1155: 		// remove any appended entries from previous indexes (if any)
1156: 		for (auto *index : already_appended) {
1157: 			index->Delete(chunk, row_ids);
1158: 		}
1159: 	}
1160: 	return error;
1161: }
1162: 
1163: ErrorData DataTable::AppendToIndexes(optional_ptr<TableIndexList> delete_indexes, DataChunk &chunk, row_t row_start) {
1164: 	D_ASSERT(is_root);
1165: 	return AppendToIndexes(info->indexes, delete_indexes, chunk, row_start);
1166: }
1167: 
1168: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
1169: 	D_ASSERT(is_root);
1170: 	if (info->indexes.Empty()) {
1171: 		return;
1172: 	}
1173: 	// first generate the vector of row identifiers
1174: 	Vector row_identifiers(LogicalType::ROW_TYPE);
1175: 	VectorOperations::GenerateSequence(row_identifiers, chunk.size(), row_start, 1);
1176: 
1177: 	// now remove the entries from the indices
1178: 	RemoveFromIndexes(state, chunk, row_identifiers);
1179: }
1180: 
1181: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, Vector &row_identifiers) {
1182: 	D_ASSERT(is_root);
1183: 	info->indexes.Scan([&](Index &index) {
1184: 		if (!index.IsBound()) {
1185: 			throw InternalException("Unbound index found in DataTable::RemoveFromIndexes");
1186: 		}
1187: 		index.Cast<BoundIndex>().Delete(chunk, row_identifiers);
1188: 		return false;
1189: 	});
1190: }
1191: 
1192: void DataTable::RemoveFromIndexes(Vector &row_identifiers, idx_t count) {
1193: 	D_ASSERT(is_root);
1194: 	row_groups->RemoveFromIndexes(info->indexes, row_identifiers, count);
1195: }
1196: 
1197: //===--------------------------------------------------------------------===//
1198: // Delete
1199: //===--------------------------------------------------------------------===//
1200: static bool TableHasDeleteConstraints(TableCatalogEntry &table) {
1201: 	for (auto &constraint : table.GetConstraints()) {
1202: 		switch (constraint->type) {
1203: 		case ConstraintType::NOT_NULL:
1204: 		case ConstraintType::CHECK:
1205: 		case ConstraintType::UNIQUE:
1206: 			break;
1207: 		case ConstraintType::FOREIGN_KEY: {
1208: 			auto &foreign_key = constraint->Cast<ForeignKeyConstraint>();
1209: 			if (foreign_key.info.IsDeleteConstraint()) {
1210: 				return true;
1211: 			}
1212: 			break;
1213: 		}
1214: 		default:
1215: 			throw NotImplementedException("Constraint type not implemented!");
1216: 		}
1217: 	}
1218: 	return false;
1219: }
1220: 
1221: void DataTable::VerifyDeleteConstraints(optional_ptr<LocalTableStorage> storage, TableDeleteState &state,
1222:                                         ClientContext &context, DataChunk &chunk) {
1223: 	for (auto &constraint : state.constraint_state->bound_constraints) {
1224: 		switch (constraint->type) {
1225: 		case ConstraintType::NOT_NULL:
1226: 		case ConstraintType::CHECK:
1227: 		case ConstraintType::UNIQUE:
1228: 			break;
1229: 		case ConstraintType::FOREIGN_KEY: {
1230: 			auto &bound_foreign_key = constraint->Cast<BoundForeignKeyConstraint>();
1231: 			if (bound_foreign_key.info.IsDeleteConstraint()) {
1232: 				VerifyDeleteForeignKeyConstraint(storage, bound_foreign_key, context, chunk);
1233: 			}
1234: 			break;
1235: 		}
1236: 		default:
1237: 			throw NotImplementedException("Constraint type not implemented!");
1238: 		}
1239: 	}
1240: }
1241: 
1242: unique_ptr<TableDeleteState> DataTable::InitializeDelete(TableCatalogEntry &table, ClientContext &context,
1243:                                                          const vector<unique_ptr<BoundConstraint>> &bound_constraints) {
1244: 	// initialize indexes (if any)
1245: 	info->InitializeIndexes(context);
1246: 
1247: 	auto binder = Binder::CreateBinder(context);
1248: 	vector<LogicalType> types;
1249: 	auto result = make_uniq<TableDeleteState>();
1250: 	result->has_delete_constraints = TableHasDeleteConstraints(table);
1251: 	if (result->has_delete_constraints) {
1252: 		// initialize the chunk if there are any constraints to verify
1253: 		for (idx_t i = 0; i < column_definitions.size(); i++) {
1254: 			result->col_ids.emplace_back(column_definitions[i].StorageOid());
1255: 			types.emplace_back(column_definitions[i].Type());
1256: 		}
1257: 		result->verify_chunk.Initialize(Allocator::Get(context), types);
1258: 		result->constraint_state = make_uniq<ConstraintState>(table, bound_constraints);
1259: 	}
1260: 	return result;
1261: }
1262: 
1263: idx_t DataTable::Delete(TableDeleteState &state, ClientContext &context, Vector &row_identifiers, idx_t count) {
1264: 	D_ASSERT(row_identifiers.GetType().InternalType() == ROW_TYPE);
1265: 	if (count == 0) {
1266: 		return 0;
1267: 	}
1268: 
1269: 	auto &transaction = DuckTransaction::Get(context, db);
1270: 	auto &local_storage = LocalStorage::Get(transaction);
1271: 	auto storage = local_storage.GetStorage(*this);
1272: 
1273: 	row_identifiers.Flatten(count);
1274: 	auto ids = FlatVector::GetData<row_t>(row_identifiers);
1275: 
1276: 	idx_t pos = 0;
1277: 	idx_t delete_count = 0;
1278: 	while (pos < count) {
1279: 		idx_t start = pos;
1280: 		bool is_transaction_delete = ids[pos] >= MAX_ROW_ID;
1281: 		// figure out which batch of rows to delete now
1282: 		for (pos++; pos < count; pos++) {
1283: 			bool row_is_transaction_delete = ids[pos] >= MAX_ROW_ID;
1284: 			if (row_is_transaction_delete != is_transaction_delete) {
1285: 				break;
1286: 			}
1287: 		}
1288: 		idx_t current_offset = start;
1289: 		idx_t current_count = pos - start;
1290: 
1291: 		Vector offset_ids(row_identifiers, current_offset, pos);
1292: 
1293: 		// This is a transaction-local DELETE.
1294: 		if (is_transaction_delete) {
1295: 			if (state.has_delete_constraints) {
1296: 				// Verify any delete constraints.
1297: 				ColumnFetchState fetch_state;
1298: 				local_storage.FetchChunk(*this, offset_ids, current_count, state.col_ids, state.verify_chunk,
1299: 				                         fetch_state);
1300: 				VerifyDeleteConstraints(storage, state, context, state.verify_chunk);
1301: 			}
1302: 			delete_count += local_storage.Delete(*this, offset_ids, current_count);
1303: 			continue;
1304: 		}
1305: 
1306: 		// This is a regular DELETE.
1307: 		if (state.has_delete_constraints) {
1308: 			// Verify any delete constraints.
1309: 			ColumnFetchState fetch_state;
1310: 			Fetch(transaction, state.verify_chunk, state.col_ids, offset_ids, current_count, fetch_state);
1311: 			VerifyDeleteConstraints(storage, state, context, state.verify_chunk);
1312: 		}
1313: 		delete_count += row_groups->Delete(transaction, *this, ids + current_offset, current_count);
1314: 	}
1315: 	return delete_count;
1316: }
1317: 
1318: //===--------------------------------------------------------------------===//
1319: // Update
1320: //===--------------------------------------------------------------------===//
1321: static void CreateMockChunk(vector<LogicalType> &types, const vector<PhysicalIndex> &column_ids, DataChunk &chunk,
1322:                             DataChunk &mock_chunk) {
1323: 	// construct a mock DataChunk
1324: 	mock_chunk.InitializeEmpty(types);
1325: 	for (column_t i = 0; i < column_ids.size(); i++) {
1326: 		mock_chunk.data[column_ids[i].index].Reference(chunk.data[i]);
1327: 	}
1328: 	mock_chunk.SetCardinality(chunk.size());
1329: }
1330: 
1331: static bool CreateMockChunk(TableCatalogEntry &table, const vector<PhysicalIndex> &column_ids,
1332:                             physical_index_set_t &desired_column_ids, DataChunk &chunk, DataChunk &mock_chunk) {
1333: 	idx_t found_columns = 0;
1334: 	// check whether the desired columns are present in the UPDATE clause
1335: 	for (column_t i = 0; i < column_ids.size(); i++) {
1336: 		if (desired_column_ids.find(column_ids[i]) != desired_column_ids.end()) {
1337: 			found_columns++;
1338: 		}
1339: 	}
1340: 	if (found_columns == 0) {
1341: 		// no columns were found: no need to check the constraint again
1342: 		return false;
1343: 	}
1344: 	if (found_columns != desired_column_ids.size()) {
1345: 		// not all columns in UPDATE clause are present!
1346: 		// this should not be triggered at all as the binder should add these columns
1347: 		throw InternalException("Not all columns required for the CHECK constraint are present in the UPDATED chunk!");
1348: 	}
1349: 	// construct a mock DataChunk
1350: 	auto types = table.GetTypes();
1351: 	CreateMockChunk(types, column_ids, chunk, mock_chunk);
1352: 	return true;
1353: }
1354: 
1355: void DataTable::VerifyUpdateConstraints(ConstraintState &state, ClientContext &context, DataChunk &chunk,
1356:                                         const vector<PhysicalIndex> &column_ids) {
1357: 	auto &table = state.table;
1358: 	auto &constraints = table.GetConstraints();
1359: 	auto &bound_constraints = state.bound_constraints;
1360: 	for (idx_t constr_idx = 0; constr_idx < bound_constraints.size(); constr_idx++) {
1361: 		auto &base_constraint = constraints[constr_idx];
1362: 		auto &constraint = bound_constraints[constr_idx];
1363: 		switch (constraint->type) {
1364: 		case ConstraintType::NOT_NULL: {
1365: 			auto &bound_not_null = constraint->Cast<BoundNotNullConstraint>();
1366: 			auto &not_null = base_constraint->Cast<NotNullConstraint>();
1367: 			// check if the constraint is in the list of column_ids
1368: 			for (idx_t col_idx = 0; col_idx < column_ids.size(); col_idx++) {
1369: 				if (column_ids[col_idx] == bound_not_null.index) {
1370: 					// found the column id: check the data in
1371: 					auto &col = table.GetColumn(LogicalIndex(not_null.index));
1372: 					VerifyNotNullConstraint(table, chunk.data[col_idx], chunk.size(), col.Name());
1373: 					break;
1374: 				}
1375: 			}
1376: 			break;
1377: 		}
1378: 		case ConstraintType::CHECK: {
1379: 			auto &check = base_constraint->Cast<CheckConstraint>();
1380: 			auto &bound_check = constraint->Cast<BoundCheckConstraint>();
1381: 
1382: 			DataChunk mock_chunk;
1383: 			if (CreateMockChunk(table, column_ids, bound_check.bound_columns, chunk, mock_chunk)) {
1384: 				VerifyCheckConstraint(context, table, *bound_check.expression, mock_chunk, check);
1385: 			}
1386: 			break;
1387: 		}
1388: 		case ConstraintType::UNIQUE:
1389: 		case ConstraintType::FOREIGN_KEY:
1390: 			break;
1391: 		default:
1392: 			throw NotImplementedException("Constraint type not implemented!");
1393: 		}
1394: 	}
1395: 
1396: #ifdef DEBUG
1397: 	// Ensure that we never call UPDATE for indexed columns.
1398: 	// Instead, we must rewrite these updates into DELETE + INSERT.
1399: 	info->indexes.Scan([&](Index &index) {
1400: 		D_ASSERT(index.IsBound());
1401: 		D_ASSERT(!index.Cast<BoundIndex>().IndexIsUpdated(column_ids));
1402: 		return false;
1403: 	});
1404: #endif
1405: }
1406: 
1407: unique_ptr<TableUpdateState> DataTable::InitializeUpdate(TableCatalogEntry &table, ClientContext &context,
1408:                                                          const vector<unique_ptr<BoundConstraint>> &bound_constraints) {
1409: 	// check that there are no unknown indexes
1410: 	info->InitializeIndexes(context);
1411: 
1412: 	auto result = make_uniq<TableUpdateState>();
1413: 	result->constraint_state = InitializeConstraintState(table, bound_constraints);
1414: 	return result;
1415: }
1416: 
1417: void DataTable::Update(TableUpdateState &state, ClientContext &context, Vector &row_ids,
1418:                        const vector<PhysicalIndex> &column_ids, DataChunk &updates) {
1419: 	D_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);
1420: 	D_ASSERT(column_ids.size() == updates.ColumnCount());
1421: 	updates.Verify();
1422: 
1423: 	auto count = updates.size();
1424: 	if (count == 0) {
1425: 		return;
1426: 	}
1427: 
1428: 	if (!is_root) {
1429: 		throw TransactionException("Transaction conflict: cannot update a table that has been altered!");
1430: 	}
1431: 
1432: 	// first verify that no constraints are violated
1433: 	VerifyUpdateConstraints(*state.constraint_state, context, updates, column_ids);
1434: 
1435: 	// now perform the actual update
1436: 	Vector max_row_id_vec(Value::BIGINT(MAX_ROW_ID));
1437: 	Vector row_ids_slice(LogicalType::BIGINT);
1438: 	DataChunk updates_slice;
1439: 	updates_slice.InitializeEmpty(updates.GetTypes());
1440: 
1441: 	SelectionVector sel_local_update(count), sel_global_update(count);
1442: 	auto n_local_update = VectorOperations::GreaterThanEquals(row_ids, max_row_id_vec, nullptr, count,
1443: 	                                                          &sel_local_update, &sel_global_update);
1444: 	auto n_global_update = count - n_local_update;
1445: 
1446: 	// row id > MAX_ROW_ID? transaction-local storage
1447: 	if (n_local_update > 0) {
1448: 		updates_slice.Slice(updates, sel_local_update, n_local_update);
1449: 		updates_slice.Flatten();
1450: 		row_ids_slice.Slice(row_ids, sel_local_update, n_local_update);
1451: 		row_ids_slice.Flatten(n_local_update);
1452: 
1453: 		LocalStorage::Get(context, db).Update(*this, row_ids_slice, column_ids, updates_slice);
1454: 	}
1455: 
1456: 	// otherwise global storage
1457: 	if (n_global_update > 0) {
1458: 		auto &transaction = DuckTransaction::Get(context, db);
1459: 		updates_slice.Slice(updates, sel_global_update, n_global_update);
1460: 		updates_slice.Flatten();
1461: 		row_ids_slice.Slice(row_ids, sel_global_update, n_global_update);
1462: 		row_ids_slice.Flatten(n_global_update);
1463: 
1464: 		transaction.UpdateCollection(row_groups);
1465: 		row_groups->Update(transaction, FlatVector::GetData<row_t>(row_ids_slice), column_ids, updates_slice);
1466: 	}
1467: }
1468: 
1469: void DataTable::UpdateColumn(TableCatalogEntry &table, ClientContext &context, Vector &row_ids,
1470:                              const vector<column_t> &column_path, DataChunk &updates) {
1471: 	D_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);
1472: 	D_ASSERT(updates.ColumnCount() == 1);
1473: 	updates.Verify();
1474: 	if (updates.size() == 0) {
1475: 		return;
1476: 	}
1477: 
1478: 	if (!is_root) {
1479: 		throw TransactionException("Transaction conflict: cannot update a table that has been altered!");
1480: 	}
1481: 
1482: 	// now perform the actual update
1483: 	auto &transaction = DuckTransaction::Get(context, db);
1484: 
1485: 	updates.Flatten();
1486: 	row_ids.Flatten(updates.size());
1487: 	row_groups->UpdateColumn(transaction, row_ids, column_path, updates);
1488: }
1489: 
1490: //===--------------------------------------------------------------------===//
1491: // Statistics
1492: //===--------------------------------------------------------------------===//
1493: unique_ptr<BaseStatistics> DataTable::GetStatistics(ClientContext &context, column_t column_id) {
1494: 	if (column_id == COLUMN_IDENTIFIER_ROW_ID) {
1495: 		return nullptr;
1496: 	}
1497: 	return row_groups->CopyStats(column_id);
1498: }
1499: 
1500: void DataTable::SetDistinct(column_t column_id, unique_ptr<DistinctStatistics> distinct_stats) {
1501: 	D_ASSERT(column_id != COLUMN_IDENTIFIER_ROW_ID);
1502: 	row_groups->SetDistinct(column_id, std::move(distinct_stats));
1503: }
1504: 
1505: unique_ptr<BlockingSample> DataTable::GetSample() {
1506: 	return row_groups->GetSample();
1507: }
1508: 
1509: //===--------------------------------------------------------------------===//
1510: // Checkpoint
1511: //===--------------------------------------------------------------------===//
1512: unique_ptr<StorageLockKey> DataTable::GetSharedCheckpointLock() {
1513: 	return info->checkpoint_lock.GetSharedLock();
1514: }
1515: 
1516: unique_ptr<StorageLockKey> DataTable::GetCheckpointLock() {
1517: 	return info->checkpoint_lock.GetExclusiveLock();
1518: }
1519: 
1520: void DataTable::Checkpoint(TableDataWriter &writer, Serializer &serializer) {
1521: 	// checkpoint each individual row group
1522: 	TableStatistics global_stats;
1523: 	row_groups->CopyStats(global_stats);
1524: 	row_groups->Checkpoint(writer, global_stats);
1525: 	// The row group payload data has been written. Now write:
1526: 	//   sample
1527: 	//   column stats
1528: 	//   row-group pointers
1529: 	//   table pointer
1530: 	//   index data
1531: 	writer.FinalizeTable(global_stats, info.get(), serializer);
1532: }
1533: 
1534: void DataTable::CommitDropColumn(idx_t index) {
1535: 	row_groups->CommitDropColumn(index);
1536: }
1537: 
1538: idx_t DataTable::ColumnCount() const {
1539: 	return column_definitions.size();
1540: }
1541: 
1542: idx_t DataTable::GetTotalRows() const {
1543: 	return row_groups->GetTotalRows();
1544: }
1545: 
1546: void DataTable::CommitDropTable() {
1547: 	// commit a drop of this table: mark all blocks as modified, so they can be reclaimed later on
1548: 	row_groups->CommitDropTable();
1549: 
1550: 	// propagate dropping this table to its indexes: frees all index memory
1551: 	info->indexes.Scan([&](Index &index) {
1552: 		D_ASSERT(index.IsBound());
1553: 		index.Cast<BoundIndex>().CommitDrop();
1554: 		return false;
1555: 	});
1556: }
1557: 
1558: //===--------------------------------------------------------------------===//
1559: // Column Segment Info
1560: //===--------------------------------------------------------------------===//
1561: vector<ColumnSegmentInfo> DataTable::GetColumnSegmentInfo() {
1562: 	auto lock = GetSharedCheckpointLock();
1563: 	return row_groups->GetColumnSegmentInfo();
1564: }
1565: 
1566: //===--------------------------------------------------------------------===//
1567: // Index Constraint Creation
1568: //===--------------------------------------------------------------------===//
1569: void DataTable::AddIndex(const ColumnList &columns, const vector<LogicalIndex> &column_indexes,
1570:                          const IndexConstraintType type, const IndexStorageInfo &index_info) {
1571: 	if (!IsRoot()) {
1572: 		throw TransactionException("cannot add an index to a table that has been altered!");
1573: 	}
1574: 
1575: 	// Fetch the column types and create bound column reference expressions.
1576: 	vector<column_t> physical_ids;
1577: 	vector<unique_ptr<Expression>> expressions;
1578: 
1579: 	for (const auto column_index : column_indexes) {
1580: 		auto binding = ColumnBinding(0, physical_ids.size());
1581: 		auto &col = columns.GetColumn(column_index);
1582: 		auto ref = make_uniq<BoundColumnRefExpression>(col.Name(), col.Type(), binding);
1583: 		expressions.push_back(std::move(ref));
1584: 		physical_ids.push_back(col.Physical().index);
1585: 	}
1586: 
1587: 	// Create an ART around the expressions.
1588: 	auto &io_manager = TableIOManager::Get(*this);
1589: 	auto art = make_uniq<ART>(index_info.name, type, physical_ids, io_manager, std::move(expressions), db, nullptr,
1590: 	                          index_info);
1591: 	info->indexes.AddIndex(std::move(art));
1592: }
1593: 
1594: } // namespace duckdb
[end of src/storage/data_table.cpp]
[start of src/storage/local_storage.cpp]
1: #include "duckdb/transaction/local_storage.hpp"
2: #include "duckdb/execution/index/art/art.hpp"
3: #include "duckdb/storage/table/append_state.hpp"
4: #include "duckdb/storage/write_ahead_log.hpp"
5: #include "duckdb/common/vector_operations/vector_operations.hpp"
6: #include "duckdb/storage/table/row_group.hpp"
7: #include "duckdb/transaction/duck_transaction.hpp"
8: #include "duckdb/planner/table_filter.hpp"
9: #include "duckdb/storage/partial_block_manager.hpp"
10: 
11: #include "duckdb/storage/table/column_checkpoint_state.hpp"
12: #include "duckdb/storage/table_io_manager.hpp"
13: #include "duckdb/storage/table/scan_state.hpp"
14: 
15: namespace duckdb {
16: 
17: LocalTableStorage::LocalTableStorage(ClientContext &context, DataTable &table)
18:     : table_ref(table), allocator(Allocator::Get(table.db)), deleted_rows(0), optimistic_writer(table),
19:       merged_storage(false) {
20: 
21: 	auto types = table.GetTypes();
22: 	auto data_table_info = table.GetDataTableInfo();
23: 	auto &io_manager = TableIOManager::Get(table);
24: 	row_groups = make_shared_ptr<RowGroupCollection>(data_table_info, io_manager, types, MAX_ROW_ID, 0);
25: 	row_groups->InitializeEmpty();
26: 
27: 	data_table_info->GetIndexes().BindAndScan<ART>(context, *data_table_info, [&](ART &art) {
28: 		auto constraint_type = art.GetConstraintType();
29: 		if (constraint_type == IndexConstraintType::NONE) {
30: 			return false;
31: 		}
32: 
33: 		// UNIQUE constraint.
34: 		vector<unique_ptr<Expression>> expressions;
35: 		vector<unique_ptr<Expression>> delete_expressions;
36: 		for (auto &expr : art.unbound_expressions) {
37: 			expressions.push_back(expr->Copy());
38: 			delete_expressions.push_back(expr->Copy());
39: 		}
40: 
41: 		// Create a delete index and a local index.
42: 		auto delete_index = make_uniq<ART>(art.GetIndexName(), constraint_type, art.GetColumnIds(),
43: 		                                   art.table_io_manager, std::move(delete_expressions), art.db);
44: 		delete_index->append_mode = ARTAppendMode::IGNORE_DUPLICATES;
45: 		delete_indexes.AddIndex(std::move(delete_index));
46: 
47: 		auto index = make_uniq<ART>(art.GetIndexName(), constraint_type, art.GetColumnIds(), art.table_io_manager,
48: 		                            std::move(expressions), art.db);
49: 		append_indexes.AddIndex(std::move(index));
50: 		return false;
51: 	});
52: }
53: 
54: LocalTableStorage::LocalTableStorage(ClientContext &context, DataTable &new_dt, LocalTableStorage &parent,
55:                                      idx_t changed_idx, const LogicalType &target_type,
56:                                      const vector<StorageIndex> &bound_columns, Expression &cast_expr)
57:     : table_ref(new_dt), allocator(Allocator::Get(new_dt.db)), deleted_rows(parent.deleted_rows),
58:       optimistic_writer(new_dt, parent.optimistic_writer), optimistic_writers(std::move(parent.optimistic_writers)),
59:       merged_storage(parent.merged_storage) {
60: 	row_groups = parent.row_groups->AlterType(context, changed_idx, target_type, bound_columns, cast_expr);
61: 	parent.row_groups.reset();
62: 	append_indexes.Move(parent.append_indexes);
63: }
64: 
65: LocalTableStorage::LocalTableStorage(DataTable &new_dt, LocalTableStorage &parent, idx_t drop_idx)
66:     : table_ref(new_dt), allocator(Allocator::Get(new_dt.db)), deleted_rows(parent.deleted_rows),
67:       optimistic_writer(new_dt, parent.optimistic_writer), optimistic_writers(std::move(parent.optimistic_writers)),
68:       merged_storage(parent.merged_storage) {
69: 	row_groups = parent.row_groups->RemoveColumn(drop_idx);
70: 	parent.row_groups.reset();
71: 	append_indexes.Move(parent.append_indexes);
72: }
73: 
74: LocalTableStorage::LocalTableStorage(ClientContext &context, DataTable &new_dt, LocalTableStorage &parent,
75:                                      ColumnDefinition &new_column, ExpressionExecutor &default_executor)
76:     : table_ref(new_dt), allocator(Allocator::Get(new_dt.db)), deleted_rows(parent.deleted_rows),
77:       optimistic_writer(new_dt, parent.optimistic_writer), optimistic_writers(std::move(parent.optimistic_writers)),
78:       merged_storage(parent.merged_storage) {
79: 	row_groups = parent.row_groups->AddColumn(context, new_column, default_executor);
80: 	parent.row_groups.reset();
81: 	append_indexes.Move(parent.append_indexes);
82: }
83: 
84: LocalTableStorage::~LocalTableStorage() {
85: }
86: 
87: void LocalTableStorage::InitializeScan(CollectionScanState &state, optional_ptr<TableFilterSet> table_filters) {
88: 	if (row_groups->GetTotalRows() == 0) {
89: 		throw InternalException("No rows in LocalTableStorage row group for scan");
90: 	}
91: 	row_groups->InitializeScan(state, state.GetColumnIds(), table_filters.get());
92: }
93: 
94: idx_t LocalTableStorage::EstimatedSize() {
95: 	// count the appended rows
96: 	idx_t appended_rows = row_groups->GetTotalRows() - deleted_rows;
97: 
98: 	// get the (estimated) size of a row (no compressions, etc.)
99: 	idx_t row_size = 0;
100: 	auto &types = row_groups->GetTypes();
101: 	for (auto &type : types) {
102: 		row_size += GetTypeIdSize(type.InternalType());
103: 	}
104: 
105: 	// get the index size
106: 	idx_t index_sizes = 0;
107: 	append_indexes.Scan([&](Index &index) {
108: 		D_ASSERT(index.IsBound());
109: 		index_sizes += index.Cast<BoundIndex>().GetInMemorySize();
110: 		return false;
111: 	});
112: 
113: 	// return the size of the appended rows and the index size
114: 	return appended_rows * row_size + index_sizes;
115: }
116: 
117: void LocalTableStorage::WriteNewRowGroup() {
118: 	if (deleted_rows != 0) {
119: 		// we have deletes - we cannot merge row groups
120: 		return;
121: 	}
122: 	optimistic_writer.WriteNewRowGroup(*row_groups);
123: }
124: 
125: void LocalTableStorage::FlushBlocks() {
126: 	const idx_t row_group_size = row_groups->GetRowGroupSize();
127: 	if (!merged_storage && row_groups->GetTotalRows() > row_group_size) {
128: 		optimistic_writer.WriteLastRowGroup(*row_groups);
129: 	}
130: 	optimistic_writer.FinalFlush();
131: }
132: 
133: ErrorData LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, RowGroupCollection &source,
134:                                              TableIndexList &index_list, const vector<LogicalType> &table_types,
135:                                              row_t &start_row) {
136: 	// only need to scan for index append
137: 	// figure out which columns we need to scan for the set of indexes
138: 	auto index_columns = index_list.GetRequiredColumns();
139: 	vector<StorageIndex> required_columns;
140: 	for (auto &col : index_columns) {
141: 		required_columns.emplace_back(col);
142: 	}
143: 	// create an empty mock chunk that contains all the correct types for the table
144: 	DataChunk mock_chunk;
145: 	mock_chunk.InitializeEmpty(table_types);
146: 	ErrorData error;
147: 	source.Scan(transaction, required_columns, [&](DataChunk &chunk) -> bool {
148: 		// construct the mock chunk by referencing the required columns
149: 		for (idx_t i = 0; i < required_columns.size(); i++) {
150: 			auto col_id = required_columns[i].GetPrimaryIndex();
151: 			mock_chunk.data[col_id].Reference(chunk.data[i]);
152: 		}
153: 		mock_chunk.SetCardinality(chunk);
154: 		// append this chunk to the indexes of the table
155: 		error = DataTable::AppendToIndexes(index_list, nullptr, mock_chunk, start_row);
156: 		if (error.HasError()) {
157: 			return false;
158: 		}
159: 		start_row += UnsafeNumericCast<row_t>(chunk.size());
160: 		return true;
161: 	});
162: 	return error;
163: }
164: 
165: void LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, TableAppendState &append_state,
166:                                         bool append_to_table) {
167: 	auto &table = table_ref.get();
168: 	if (append_to_table) {
169: 		table.InitializeAppend(transaction, append_state);
170: 	}
171: 	ErrorData error;
172: 	if (append_to_table) {
173: 		// appending: need to scan entire
174: 		row_groups->Scan(transaction, [&](DataChunk &chunk) -> bool {
175: 			// append this chunk to the indexes of the table
176: 			error = table.AppendToIndexes(delete_indexes, chunk, append_state.current_row);
177: 			if (error.HasError()) {
178: 				return false;
179: 			}
180: 			// append to base table
181: 			table.Append(chunk, append_state);
182: 			return true;
183: 		});
184: 	} else {
185: 		auto data_table_info = table.GetDataTableInfo();
186: 		auto &index_list = data_table_info->GetIndexes();
187: 		error = AppendToIndexes(transaction, *row_groups, index_list, table.GetTypes(), append_state.current_row);
188: 	}
189: 	if (error.HasError()) {
190: 		// need to revert all appended row ids
191: 		row_t current_row = append_state.row_start;
192: 		// remove the data from the indexes, if there are any indexes
193: 		row_groups->Scan(transaction, [&](DataChunk &chunk) -> bool {
194: 			// append this chunk to the indexes of the table
195: 			try {
196: 				table.RemoveFromIndexes(append_state, chunk, current_row);
197: 			} catch (std::exception &ex) { // LCOV_EXCL_START
198: 				error = ErrorData(ex);
199: 				return false;
200: 			} // LCOV_EXCL_STOP
201: 
202: 			current_row += UnsafeNumericCast<row_t>(chunk.size());
203: 			if (current_row >= append_state.current_row) {
204: 				// finished deleting all rows from the index: abort now
205: 				return false;
206: 			}
207: 			return true;
208: 		});
209: 		if (append_to_table) {
210: 			table.RevertAppendInternal(NumericCast<idx_t>(append_state.row_start));
211: 		}
212: 
213: 		// we need to vacuum the indexes to remove any buffers that are now empty
214: 		// due to reverting the appends
215: 		table.VacuumIndexes();
216: 		error.Throw();
217: 	}
218: 	if (append_to_table) {
219: 		table.FinalizeAppend(transaction, append_state);
220: 	}
221: }
222: 
223: OptimisticDataWriter &LocalTableStorage::CreateOptimisticWriter() {
224: 	auto writer = make_uniq<OptimisticDataWriter>(table_ref.get());
225: 	optimistic_writers.push_back(std::move(writer));
226: 	return *optimistic_writers.back();
227: }
228: 
229: void LocalTableStorage::FinalizeOptimisticWriter(OptimisticDataWriter &writer) {
230: 	// remove the writer from the set of optimistic writers
231: 	unique_ptr<OptimisticDataWriter> owned_writer;
232: 	for (idx_t i = 0; i < optimistic_writers.size(); i++) {
233: 		if (optimistic_writers[i].get() == &writer) {
234: 			owned_writer = std::move(optimistic_writers[i]);
235: 			optimistic_writers.erase_at(i);
236: 			break;
237: 		}
238: 	}
239: 	if (!owned_writer) {
240: 		throw InternalException("Error in FinalizeOptimisticWriter - could not find writer");
241: 	}
242: 	optimistic_writer.Merge(*owned_writer);
243: }
244: 
245: void LocalTableStorage::Rollback() {
246: 	for (auto &writer : optimistic_writers) {
247: 		writer->Rollback();
248: 	}
249: 	optimistic_writers.clear();
250: 	optimistic_writer.Rollback();
251: }
252: 
253: //===--------------------------------------------------------------------===//
254: // LocalTableManager
255: //===--------------------------------------------------------------------===//
256: optional_ptr<LocalTableStorage> LocalTableManager::GetStorage(DataTable &table) const {
257: 	lock_guard<mutex> l(table_storage_lock);
258: 	auto entry = table_storage.find(table);
259: 	return entry == table_storage.end() ? nullptr : entry->second.get();
260: }
261: 
262: LocalTableStorage &LocalTableManager::GetOrCreateStorage(ClientContext &context, DataTable &table) {
263: 	lock_guard<mutex> l(table_storage_lock);
264: 	auto entry = table_storage.find(table);
265: 	if (entry == table_storage.end()) {
266: 		auto new_storage = make_shared_ptr<LocalTableStorage>(context, table);
267: 		auto storage = new_storage.get();
268: 		table_storage.insert(make_pair(reference<DataTable>(table), std::move(new_storage)));
269: 		return *storage;
270: 	} else {
271: 		return *entry->second.get();
272: 	}
273: }
274: 
275: bool LocalTableManager::IsEmpty() const {
276: 	lock_guard<mutex> l(table_storage_lock);
277: 	return table_storage.empty();
278: }
279: 
280: shared_ptr<LocalTableStorage> LocalTableManager::MoveEntry(DataTable &table) {
281: 	lock_guard<mutex> l(table_storage_lock);
282: 	auto entry = table_storage.find(table);
283: 	if (entry == table_storage.end()) {
284: 		return nullptr;
285: 	}
286: 	auto storage_entry = std::move(entry->second);
287: 	table_storage.erase(entry);
288: 	return storage_entry;
289: }
290: 
291: reference_map_t<DataTable, shared_ptr<LocalTableStorage>> LocalTableManager::MoveEntries() {
292: 	lock_guard<mutex> l(table_storage_lock);
293: 	return std::move(table_storage);
294: }
295: 
296: idx_t LocalTableManager::EstimatedSize() const {
297: 	lock_guard<mutex> l(table_storage_lock);
298: 	idx_t estimated_size = 0;
299: 	for (auto &storage : table_storage) {
300: 		estimated_size += storage.second->EstimatedSize();
301: 	}
302: 	return estimated_size;
303: }
304: 
305: void LocalTableManager::InsertEntry(DataTable &table, shared_ptr<LocalTableStorage> entry) {
306: 	lock_guard<mutex> l(table_storage_lock);
307: 	D_ASSERT(table_storage.find(table) == table_storage.end());
308: 	table_storage[table] = std::move(entry);
309: }
310: 
311: //===--------------------------------------------------------------------===//
312: // LocalStorage
313: //===--------------------------------------------------------------------===//
314: LocalStorage::LocalStorage(ClientContext &context, DuckTransaction &transaction)
315:     : context(context), transaction(transaction) {
316: }
317: 
318: LocalStorage::CommitState::CommitState() {
319: }
320: 
321: LocalStorage::CommitState::~CommitState() {
322: }
323: 
324: LocalStorage &LocalStorage::Get(DuckTransaction &transaction) {
325: 	return transaction.GetLocalStorage();
326: }
327: 
328: LocalStorage &LocalStorage::Get(ClientContext &context, AttachedDatabase &db) {
329: 	return DuckTransaction::Get(context, db).GetLocalStorage();
330: }
331: 
332: LocalStorage &LocalStorage::Get(ClientContext &context, Catalog &catalog) {
333: 	return LocalStorage::Get(context, catalog.GetAttached());
334: }
335: 
336: void LocalStorage::InitializeScan(DataTable &table, CollectionScanState &state,
337:                                   optional_ptr<TableFilterSet> table_filters) {
338: 	auto storage = table_manager.GetStorage(table);
339: 	if (storage == nullptr || storage->row_groups->GetTotalRows() == 0) {
340: 		return;
341: 	}
342: 	storage->InitializeScan(state, table_filters);
343: }
344: 
345: void LocalStorage::Scan(CollectionScanState &state, const vector<StorageIndex> &, DataChunk &result) {
346: 	state.Scan(transaction, result);
347: }
348: 
349: void LocalStorage::InitializeParallelScan(DataTable &table, ParallelCollectionScanState &state) {
350: 	auto storage = table_manager.GetStorage(table);
351: 	if (!storage) {
352: 		state.max_row = 0;
353: 		state.vector_index = 0;
354: 		state.current_row_group = nullptr;
355: 	} else {
356: 		storage->row_groups->InitializeParallelScan(state);
357: 	}
358: }
359: 
360: bool LocalStorage::NextParallelScan(ClientContext &context, DataTable &table, ParallelCollectionScanState &state,
361:                                     CollectionScanState &scan_state) {
362: 	auto storage = table_manager.GetStorage(table);
363: 	if (!storage) {
364: 		return false;
365: 	}
366: 	return storage->row_groups->NextParallelScan(context, state, scan_state);
367: }
368: 
369: void LocalStorage::InitializeAppend(LocalAppendState &state, DataTable &table) {
370: 	table.InitializeIndexes(context);
371: 	state.storage = &table_manager.GetOrCreateStorage(context, table);
372: 	state.storage->row_groups->InitializeAppend(TransactionData(transaction), state.append_state);
373: }
374: 
375: void LocalStorage::InitializeStorage(LocalAppendState &state, DataTable &table) {
376: 	table.InitializeIndexes(context);
377: 	state.storage = &table_manager.GetOrCreateStorage(context, table);
378: }
379: 
380: void LocalTableStorage::AppendToDeleteIndexes(Vector &row_ids, DataChunk &delete_chunk) {
381: 	if (delete_chunk.size() == 0) {
382: 		return;
383: 	}
384: 
385: 	delete_indexes.ScanBound<ART>([&](ART &art) {
386: 		if (!art.IsUnique()) {
387: 			return false;
388: 		}
389: 		auto result = art.Cast<BoundIndex>().Append(delete_chunk, row_ids);
390: 		if (result.HasError()) {
391: 			throw InternalException("unexpected constraint violation on delete ART: ", result.Message());
392: 		}
393: 		return false;
394: 	});
395: }
396: 
397: void LocalStorage::Append(LocalAppendState &state, DataChunk &chunk) {
398: 	// Append to any unique indexes.
399: 	auto storage = state.storage;
400: 	auto offset = NumericCast<idx_t>(MAX_ROW_ID) + storage->row_groups->GetTotalRows();
401: 	idx_t base_id = offset + state.append_state.total_append_count;
402: 
403: 	auto error = DataTable::AppendToIndexes(storage->append_indexes, storage->delete_indexes, chunk,
404: 	                                        NumericCast<row_t>(base_id));
405: 	if (error.HasError()) {
406: 		error.Throw();
407: 	}
408: 
409: 	// Append the chunk to the local storage.
410: 	auto new_row_group = storage->row_groups->Append(chunk, state.append_state);
411: 
412: 	// Check if we should pre-emptively flush blocks to disk.
413: 	if (new_row_group) {
414: 		storage->WriteNewRowGroup();
415: 	}
416: }
417: 
418: void LocalStorage::FinalizeAppend(LocalAppendState &state) {
419: 	state.storage->row_groups->FinalizeAppend(state.append_state.transaction, state.append_state);
420: }
421: 
422: void LocalStorage::LocalMerge(DataTable &table, RowGroupCollection &collection) {
423: 	auto &storage = table_manager.GetOrCreateStorage(context, table);
424: 	if (!storage.append_indexes.Empty()) {
425: 		// append data to indexes if required
426: 		row_t base_id = MAX_ROW_ID + NumericCast<row_t>(storage.row_groups->GetTotalRows());
427: 		auto error =
428: 		    storage.AppendToIndexes(transaction, collection, storage.append_indexes, table.GetTypes(), base_id);
429: 		if (error.HasError()) {
430: 			error.Throw();
431: 		}
432: 	}
433: 	storage.row_groups->MergeStorage(collection, nullptr, nullptr);
434: 	storage.merged_storage = true;
435: }
436: 
437: OptimisticDataWriter &LocalStorage::CreateOptimisticWriter(DataTable &table) {
438: 	auto &storage = table_manager.GetOrCreateStorage(context, table);
439: 	return storage.CreateOptimisticWriter();
440: }
441: 
442: void LocalStorage::FinalizeOptimisticWriter(DataTable &table, OptimisticDataWriter &writer) {
443: 	auto &storage = table_manager.GetOrCreateStorage(context, table);
444: 	storage.FinalizeOptimisticWriter(writer);
445: }
446: 
447: bool LocalStorage::ChangesMade() noexcept {
448: 	return !table_manager.IsEmpty();
449: }
450: 
451: bool LocalStorage::Find(DataTable &table) {
452: 	return table_manager.GetStorage(table) != nullptr;
453: }
454: 
455: idx_t LocalStorage::EstimatedSize() {
456: 	return table_manager.EstimatedSize();
457: }
458: 
459: idx_t LocalStorage::Delete(DataTable &table, Vector &row_ids, idx_t count) {
460: 	auto storage = table_manager.GetStorage(table);
461: 	D_ASSERT(storage);
462: 
463: 	// delete from unique indices (if any)
464: 	if (!storage->append_indexes.Empty()) {
465: 		storage->row_groups->RemoveFromIndexes(storage->append_indexes, row_ids, count);
466: 	}
467: 
468: 	auto ids = FlatVector::GetData<row_t>(row_ids);
469: 	idx_t delete_count = storage->row_groups->Delete(TransactionData(0, 0), table, ids, count);
470: 	storage->deleted_rows += delete_count;
471: 	return delete_count;
472: }
473: 
474: void LocalStorage::Update(DataTable &table, Vector &row_ids, const vector<PhysicalIndex> &column_ids,
475:                           DataChunk &updates) {
476: 	auto storage = table_manager.GetStorage(table);
477: 	D_ASSERT(storage);
478: 
479: 	auto ids = FlatVector::GetData<row_t>(row_ids);
480: 	storage->row_groups->Update(TransactionData(0, 0), ids, column_ids, updates);
481: }
482: 
483: void LocalStorage::Flush(DataTable &table, LocalTableStorage &storage, optional_ptr<StorageCommitState> commit_state) {
484: 	if (storage.is_dropped) {
485: 		return;
486: 	}
487: 	if (storage.row_groups->GetTotalRows() <= storage.deleted_rows) {
488: 		// all rows that we added were deleted
489: 		// rollback any partial blocks that are still outstanding
490: 		storage.Rollback();
491: 		return;
492: 	}
493: 	idx_t append_count = storage.row_groups->GetTotalRows() - storage.deleted_rows;
494: 	table.InitializeIndexes(context);
495: 
496: 	const idx_t row_group_size = storage.row_groups->GetRowGroupSize();
497: 
498: 	TableAppendState append_state;
499: 	table.AppendLock(append_state);
500: 	transaction.PushAppend(table, NumericCast<idx_t>(append_state.row_start), append_count);
501: 	if ((append_state.row_start == 0 || storage.row_groups->GetTotalRows() >= row_group_size) &&
502: 	    storage.deleted_rows == 0) {
503: 		// table is currently empty OR we are bulk appending: move over the storage directly
504: 		// first flush any outstanding blocks
505: 		storage.FlushBlocks();
506: 		// now append to the indexes (if there are any)
507: 		// FIXME: we should be able to merge the transaction-local index directly into the main table index
508: 		// as long we just rewrite some row-ids
509: 		if (table.HasIndexes()) {
510: 			storage.AppendToIndexes(transaction, append_state, false);
511: 		}
512: 		// finally move over the row groups
513: 		table.MergeStorage(*storage.row_groups, storage.append_indexes, commit_state);
514: 	} else {
515: 		// check if we have written data
516: 		// if we have, we cannot merge to disk after all
517: 		// so we need to revert the data we have already written
518: 		storage.Rollback();
519: 		// append to the indexes and append to the base table
520: 		storage.AppendToIndexes(transaction, append_state, true);
521: 	}
522: 
523: 	// possibly vacuum any excess index data
524: 	table.VacuumIndexes();
525: }
526: 
527: void LocalStorage::Commit(optional_ptr<StorageCommitState> commit_state) {
528: 	// commit local storage
529: 	// iterate over all entries in the table storage map and commit them
530: 	// after this, the local storage is no longer required and can be cleared
531: 	auto table_storage = table_manager.MoveEntries();
532: 	for (auto &entry : table_storage) {
533: 		auto table = entry.first;
534: 		auto storage = entry.second.get();
535: 		Flush(table, *storage, commit_state);
536: 		entry.second.reset();
537: 	}
538: }
539: 
540: void LocalStorage::Rollback() {
541: 	// rollback local storage
542: 	// after this, the local storage is no longer required and can be cleared
543: 	auto table_storage = table_manager.MoveEntries();
544: 	for (auto &entry : table_storage) {
545: 		auto storage = entry.second.get();
546: 		if (!storage) {
547: 			continue;
548: 		}
549: 		storage->Rollback();
550: 
551: 		entry.second.reset();
552: 	}
553: }
554: 
555: idx_t LocalStorage::AddedRows(DataTable &table) {
556: 	auto storage = table_manager.GetStorage(table);
557: 	if (!storage) {
558: 		return 0;
559: 	}
560: 	return storage->row_groups->GetTotalRows() - storage->deleted_rows;
561: }
562: 
563: vector<PartitionStatistics> LocalStorage::GetPartitionStats(DataTable &table) const {
564: 	auto storage = table_manager.GetStorage(table);
565: 	if (!storage) {
566: 		return vector<PartitionStatistics>();
567: 	}
568: 	return storage->row_groups->GetPartitionStats();
569: }
570: 
571: void LocalStorage::DropTable(DataTable &table) {
572: 	auto storage = table_manager.GetStorage(table);
573: 	if (!storage) {
574: 		return;
575: 	}
576: 	storage->is_dropped = true;
577: }
578: 
579: void LocalStorage::MoveStorage(DataTable &old_dt, DataTable &new_dt) {
580: 	// check if there are any pending appends for the old version of the table
581: 	auto new_storage = table_manager.MoveEntry(old_dt);
582: 	if (!new_storage) {
583: 		return;
584: 	}
585: 	// take over the storage from the old entry
586: 	new_storage->table_ref = new_dt;
587: 	table_manager.InsertEntry(new_dt, std::move(new_storage));
588: }
589: 
590: void LocalStorage::AddColumn(DataTable &old_dt, DataTable &new_dt, ColumnDefinition &new_column,
591:                              ExpressionExecutor &default_executor) {
592: 	// check if there are any pending appends for the old version of the table
593: 	auto storage = table_manager.MoveEntry(old_dt);
594: 	if (!storage) {
595: 		return;
596: 	}
597: 	auto new_storage = make_shared_ptr<LocalTableStorage>(context, new_dt, *storage, new_column, default_executor);
598: 	table_manager.InsertEntry(new_dt, std::move(new_storage));
599: }
600: 
601: void LocalStorage::DropColumn(DataTable &old_dt, DataTable &new_dt, idx_t removed_column) {
602: 	// check if there are any pending appends for the old version of the table
603: 	auto storage = table_manager.MoveEntry(old_dt);
604: 	if (!storage) {
605: 		return;
606: 	}
607: 	auto new_storage = make_shared_ptr<LocalTableStorage>(new_dt, *storage, removed_column);
608: 	table_manager.InsertEntry(new_dt, std::move(new_storage));
609: }
610: 
611: void LocalStorage::ChangeType(DataTable &old_dt, DataTable &new_dt, idx_t changed_idx, const LogicalType &target_type,
612:                               const vector<StorageIndex> &bound_columns, Expression &cast_expr) {
613: 	// check if there are any pending appends for the old version of the table
614: 	auto storage = table_manager.MoveEntry(old_dt);
615: 	if (!storage) {
616: 		return;
617: 	}
618: 	auto new_storage = make_shared_ptr<LocalTableStorage>(context, new_dt, *storage, changed_idx, target_type,
619: 	                                                      bound_columns, cast_expr);
620: 	table_manager.InsertEntry(new_dt, std::move(new_storage));
621: }
622: 
623: void LocalStorage::FetchChunk(DataTable &table, Vector &row_ids, idx_t count, const vector<StorageIndex> &col_ids,
624:                               DataChunk &chunk, ColumnFetchState &fetch_state) {
625: 	auto storage = table_manager.GetStorage(table);
626: 	if (!storage) {
627: 		throw InternalException("LocalStorage::FetchChunk - local storage not found");
628: 	}
629: 
630: 	storage->row_groups->Fetch(transaction, chunk, col_ids, row_ids, count, fetch_state);
631: }
632: 
633: TableIndexList &LocalStorage::GetIndexes(DataTable &table) {
634: 	auto storage = table_manager.GetStorage(table);
635: 	if (!storage) {
636: 		throw InternalException("LocalStorage::GetIndexes - local storage not found");
637: 	}
638: 	return storage->append_indexes;
639: }
640: 
641: optional_ptr<LocalTableStorage> LocalStorage::GetStorage(DataTable &table) {
642: 	return table_manager.GetStorage(table);
643: }
644: 
645: void LocalStorage::VerifyNewConstraint(DataTable &parent, const BoundConstraint &constraint) {
646: 	auto storage = table_manager.GetStorage(parent);
647: 	if (!storage) {
648: 		return;
649: 	}
650: 	storage->row_groups->VerifyNewConstraint(parent, constraint);
651: }
652: 
653: } // namespace duckdb
[end of src/storage/local_storage.cpp]
[start of src/storage/table_index_list.cpp]
1: #include "duckdb/storage/table/table_index_list.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/duck_table_entry.hpp"
4: #include "duckdb/common/types/conflict_manager.hpp"
5: #include "duckdb/execution/index/index_type_set.hpp"
6: #include "duckdb/execution/index/unbound_index.hpp"
7: #include "duckdb/main/config.hpp"
8: #include "duckdb/main/database.hpp"
9: #include "duckdb/planner/expression_binder/index_binder.hpp"
10: #include "duckdb/storage/data_table.hpp"
11: #include "duckdb/storage/table/data_table_info.hpp"
12: 
13: namespace duckdb {
14: 
15: void TableIndexList::AddIndex(unique_ptr<Index> index) {
16: 	D_ASSERT(index);
17: 	lock_guard<mutex> lock(indexes_lock);
18: 	indexes.push_back(std::move(index));
19: }
20: 
21: void TableIndexList::RemoveIndex(const string &name) {
22: 	lock_guard<mutex> lock(indexes_lock);
23: 
24: 	for (idx_t i = 0; i < indexes.size(); i++) {
25: 		auto &index = indexes[i];
26: 		if (index->GetIndexName() == name) {
27: 			indexes.erase_at(i);
28: 			break;
29: 		}
30: 	}
31: }
32: 
33: void TableIndexList::CommitDrop(const string &name) {
34: 	lock_guard<mutex> lock(indexes_lock);
35: 
36: 	for (auto &index : indexes) {
37: 		if (index->GetIndexName() == name) {
38: 			index->CommitDrop();
39: 		}
40: 	}
41: }
42: 
43: bool TableIndexList::NameIsUnique(const string &name) {
44: 	lock_guard<mutex> lock(indexes_lock);
45: 
46: 	// Only covers PK, FK, and UNIQUE indexes.
47: 	for (const auto &index : indexes) {
48: 		if (index->IsPrimary() || index->IsForeign() || index->IsUnique()) {
49: 			if (index->GetIndexName() == name) {
50: 				return false;
51: 			}
52: 		}
53: 	}
54: 	return true;
55: }
56: 
57: optional_ptr<BoundIndex> TableIndexList::Find(const string &name) {
58: 	for (auto &index : indexes) {
59: 		if (index->GetIndexName() == name) {
60: 			return index->Cast<BoundIndex>();
61: 		}
62: 	}
63: 	return nullptr;
64: }
65: 
66: void TableIndexList::InitializeIndexes(ClientContext &context, DataTableInfo &table_info, const char *index_type) {
67: 	// Fast path: do we have any unbound indexes?
68: 	bool needs_binding = false;
69: 	{
70: 		lock_guard<mutex> lock(indexes_lock);
71: 		for (auto &index : indexes) {
72: 			if (!index->IsBound() && (index_type == nullptr || index->GetIndexType() == index_type)) {
73: 				needs_binding = true;
74: 				break;
75: 			}
76: 		}
77: 	}
78: 	if (!needs_binding) {
79: 		return;
80: 	}
81: 
82: 	// Get the table from the catalog, so we can add it to the binder.
83: 	auto &catalog = table_info.GetDB().GetCatalog();
84: 	auto schema = table_info.GetSchemaName();
85: 	auto table_name = table_info.GetTableName();
86: 	auto &table_entry = catalog.GetEntry(context, CatalogType::TABLE_ENTRY, schema, table_name);
87: 	auto &table = table_entry.Cast<DuckTableEntry>();
88: 
89: 	vector<LogicalType> column_types;
90: 	vector<string> column_names;
91: 	for (auto &col : table.GetColumns().Logical()) {
92: 		column_types.push_back(col.Type());
93: 		column_names.push_back(col.Name());
94: 	}
95: 
96: 	lock_guard<mutex> lock(indexes_lock);
97: 	for (auto &index : indexes) {
98: 		if (!index->IsBound() && (index_type == nullptr || index->GetIndexType() == index_type)) {
99: 			// Create a binder to bind this index.
100: 			auto binder = Binder::CreateBinder(context);
101: 
102: 			// Add the table to the binder.
103: 			vector<ColumnIndex> dummy_column_ids;
104: 			binder->bind_context.AddBaseTable(0, string(), column_names, column_types, dummy_column_ids, table);
105: 
106: 			// Create an IndexBinder to bind the index
107: 			IndexBinder idx_binder(*binder, context);
108: 
109: 			// Replace the unbound index with a bound index.
110: 			auto bound_idx = idx_binder.BindIndex(index->Cast<UnboundIndex>());
111: 			index = std::move(bound_idx);
112: 		}
113: 	}
114: }
115: 
116: bool TableIndexList::Empty() {
117: 	lock_guard<mutex> lock(indexes_lock);
118: 	return indexes.empty();
119: }
120: 
121: idx_t TableIndexList::Count() {
122: 	lock_guard<mutex> lock(indexes_lock);
123: 	return indexes.size();
124: }
125: 
126: void TableIndexList::Move(TableIndexList &other) {
127: 	D_ASSERT(indexes.empty());
128: 	indexes = std::move(other.indexes);
129: }
130: 
131: bool IsForeignKeyIndex(const vector<PhysicalIndex> &fk_keys, Index &index, ForeignKeyType fk_type) {
132: 	if (fk_type == ForeignKeyType::FK_TYPE_PRIMARY_KEY_TABLE ? !index.IsUnique() : !index.IsForeign()) {
133: 		return false;
134: 	}
135: 	if (fk_keys.size() != index.GetColumnIds().size()) {
136: 		return false;
137: 	}
138: 
139: 	auto &column_ids = index.GetColumnIds();
140: 	for (auto &fk_key : fk_keys) {
141: 		bool found = false;
142: 		for (auto &index_key : column_ids) {
143: 			if (fk_key.index == index_key) {
144: 				found = true;
145: 				break;
146: 			}
147: 		}
148: 		if (!found) {
149: 			return false;
150: 		}
151: 	}
152: 	return true;
153: }
154: 
155: optional_ptr<Index> TableIndexList::FindForeignKeyIndex(const vector<PhysicalIndex> &fk_keys,
156:                                                         const ForeignKeyType fk_type) {
157: 	for (auto &index_elem : indexes) {
158: 		if (IsForeignKeyIndex(fk_keys, *index_elem, fk_type)) {
159: 			return index_elem;
160: 		}
161: 	}
162: 	return nullptr;
163: }
164: 
165: void TableIndexList::VerifyForeignKey(optional_ptr<LocalTableStorage> storage, const vector<PhysicalIndex> &fk_keys,
166:                                       DataChunk &chunk, ConflictManager &conflict_manager) {
167: 	auto fk_type = conflict_manager.LookupType() == VerifyExistenceType::APPEND_FK
168: 	                   ? ForeignKeyType::FK_TYPE_PRIMARY_KEY_TABLE
169: 	                   : ForeignKeyType::FK_TYPE_FOREIGN_KEY_TABLE;
170: 
171: 	// Check whether the chunk can be inserted in or deleted from the referenced table storage.
172: 	auto index = FindForeignKeyIndex(fk_keys, fk_type);
173: 	D_ASSERT(index && index->IsBound());
174: 	if (storage) {
175: 		auto delete_index = storage->delete_indexes.Find(index->GetIndexName());
176: 		index->Cast<BoundIndex>().VerifyConstraint(chunk, delete_index, conflict_manager);
177: 	} else {
178: 		index->Cast<BoundIndex>().VerifyConstraint(chunk, nullptr, conflict_manager);
179: 	}
180: }
181: 
182: unordered_set<column_t> TableIndexList::GetRequiredColumns() {
183: 	lock_guard<mutex> lock(indexes_lock);
184: 	unordered_set<column_t> column_ids;
185: 	for (auto &index : indexes) {
186: 		for (auto col_id : index->GetColumnIds()) {
187: 			column_ids.insert(col_id);
188: 		}
189: 	}
190: 	return column_ids;
191: }
192: 
193: vector<IndexStorageInfo> TableIndexList::GetStorageInfos(const case_insensitive_map_t<Value> &options) {
194: 	vector<IndexStorageInfo> infos;
195: 	for (auto &index : indexes) {
196: 		if (index->IsBound()) {
197: 			auto info = index->Cast<BoundIndex>().GetStorageInfo(options, false);
198: 			D_ASSERT(info.IsValid() && !info.name.empty());
199: 			infos.push_back(info);
200: 			continue;
201: 		}
202: 
203: 		auto info = index->Cast<UnboundIndex>().GetStorageInfo();
204: 		D_ASSERT(info.IsValid() && !info.name.empty());
205: 		infos.push_back(info);
206: 	}
207: 
208: 	return infos;
209: }
210: 
211: } // namespace duckdb
[end of src/storage/table_index_list.cpp]
[start of src/storage/wal_replay.cpp]
1: #include "duckdb/catalog/catalog_entry/duck_index_entry.hpp"
2: #include "duckdb/catalog/catalog_entry/duck_table_entry.hpp"
3: #include "duckdb/catalog/catalog_entry/scalar_macro_catalog_entry.hpp"
4: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/type_catalog_entry.hpp"
6: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
7: #include "duckdb/common/checksum.hpp"
8: #include "duckdb/common/printer.hpp"
9: #include "duckdb/common/serializer/binary_deserializer.hpp"
10: #include "duckdb/common/serializer/buffered_file_reader.hpp"
11: #include "duckdb/common/serializer/memory_stream.hpp"
12: #include "duckdb/common/string_util.hpp"
13: #include "duckdb/execution/index/art/art.hpp"
14: #include "duckdb/execution/index/index_type_set.hpp"
15: #include "duckdb/main/attached_database.hpp"
16: #include "duckdb/main/client_context.hpp"
17: #include "duckdb/main/config.hpp"
18: #include "duckdb/main/connection.hpp"
19: #include "duckdb/main/database.hpp"
20: #include "duckdb/parser/parsed_data/alter_table_info.hpp"
21: #include "duckdb/parser/parsed_data/create_schema_info.hpp"
22: #include "duckdb/parser/parsed_data/create_view_info.hpp"
23: #include "duckdb/parser/parsed_data/drop_info.hpp"
24: #include "duckdb/planner/binder.hpp"
25: #include "duckdb/planner/expression_binder/index_binder.hpp"
26: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
27: #include "duckdb/storage/storage_manager.hpp"
28: #include "duckdb/storage/table/delete_state.hpp"
29: #include "duckdb/storage/write_ahead_log.hpp"
30: #include "duckdb/transaction/meta_transaction.hpp"
31: #include "duckdb/storage/table/column_data.hpp"
32: 
33: namespace duckdb {
34: 
35: class ReplayState {
36: public:
37: 	ReplayState(AttachedDatabase &db, ClientContext &context) : db(db), context(context), catalog(db.GetCatalog()) {
38: 	}
39: 
40: 	AttachedDatabase &db;
41: 	ClientContext &context;
42: 	Catalog &catalog;
43: 	optional_ptr<TableCatalogEntry> current_table;
44: 	MetaBlockPointer checkpoint_id;
45: 	idx_t wal_version = 1;
46: };
47: 
48: class WriteAheadLogDeserializer {
49: public:
50: 	WriteAheadLogDeserializer(ReplayState &state_p, BufferedFileReader &stream_p, bool deserialize_only = false)
51: 	    : state(state_p), db(state.db), context(state.context), catalog(state.catalog), data(nullptr),
52: 	      stream(nullptr, 0), deserializer(stream_p), deserialize_only(deserialize_only) {
53: 		deserializer.Set<Catalog &>(catalog);
54: 	}
55: 	WriteAheadLogDeserializer(ReplayState &state_p, unique_ptr<data_t[]> data_p, idx_t size,
56: 	                          bool deserialize_only = false)
57: 	    : state(state_p), db(state.db), context(state.context), catalog(state.catalog), data(std::move(data_p)),
58: 	      stream(data.get(), size), deserializer(stream), deserialize_only(deserialize_only) {
59: 		deserializer.Set<Catalog &>(catalog);
60: 	}
61: 
62: 	static WriteAheadLogDeserializer Open(ReplayState &state_p, BufferedFileReader &stream,
63: 	                                      bool deserialize_only = false) {
64: 		if (state_p.wal_version == 1) {
65: 			// old WAL versions do not have checksums
66: 			return WriteAheadLogDeserializer(state_p, stream, deserialize_only);
67: 		}
68: 		if (state_p.wal_version != 2) {
69: 			throw IOException("Failed to read WAL of version %llu - can only read version 1 and 2",
70: 			                  state_p.wal_version);
71: 		}
72: 		// read the checksum and size
73: 		auto size = stream.Read<uint64_t>();
74: 		auto stored_checksum = stream.Read<uint64_t>();
75: 		auto offset = stream.CurrentOffset();
76: 		auto file_size = stream.FileSize();
77: 
78: 		if (offset + size > file_size) {
79: 			throw SerializationException(
80: 			    "Corrupt WAL file: entry size exceeded remaining data in file at byte position %llu "
81: 			    "(found entry with size %llu bytes, file size %llu bytes)",
82: 			    offset, size, file_size);
83: 		}
84: 
85: 		// allocate a buffer and read data into the buffer
86: 		auto buffer = unique_ptr<data_t[]>(new data_t[size]);
87: 		stream.ReadData(buffer.get(), size);
88: 
89: 		// compute and verify the checksum
90: 		auto computed_checksum = Checksum(buffer.get(), size);
91: 		if (stored_checksum != computed_checksum) {
92: 			throw IOException("Corrupt WAL file: entry at byte position %llu computed checksum %llu does not match "
93: 			                  "stored checksum %llu",
94: 			                  offset, computed_checksum, stored_checksum);
95: 		}
96: 		return WriteAheadLogDeserializer(state_p, std::move(buffer), size, deserialize_only);
97: 	}
98: 
99: 	bool ReplayEntry() {
100: 		deserializer.Begin();
101: 		auto wal_type = deserializer.ReadProperty<WALType>(100, "wal_type");
102: 		if (wal_type == WALType::WAL_FLUSH) {
103: 			deserializer.End();
104: 			return true;
105: 		}
106: 		ReplayEntry(wal_type);
107: 		deserializer.End();
108: 		return false;
109: 	}
110: 
111: 	bool DeserializeOnly() {
112: 		return deserialize_only;
113: 	}
114: 
115: protected:
116: 	void ReplayEntry(WALType wal_type);
117: 
118: 	void ReplayVersion();
119: 
120: 	void ReplayCreateTable();
121: 	void ReplayDropTable();
122: 	void ReplayAlter();
123: 
124: 	void ReplayCreateView();
125: 	void ReplayDropView();
126: 
127: 	void ReplayCreateSchema();
128: 	void ReplayDropSchema();
129: 
130: 	void ReplayCreateType();
131: 	void ReplayDropType();
132: 
133: 	void ReplayCreateSequence();
134: 	void ReplayDropSequence();
135: 	void ReplaySequenceValue();
136: 
137: 	void ReplayCreateMacro();
138: 	void ReplayDropMacro();
139: 
140: 	void ReplayCreateTableMacro();
141: 	void ReplayDropTableMacro();
142: 
143: 	void ReplayCreateIndex();
144: 	void ReplayDropIndex();
145: 
146: 	void ReplayUseTable();
147: 	void ReplayInsert();
148: 	void ReplayRowGroupData();
149: 	void ReplayDelete();
150: 	void ReplayUpdate();
151: 	void ReplayCheckpoint();
152: 
153: private:
154: 	ReplayState &state;
155: 	AttachedDatabase &db;
156: 	ClientContext &context;
157: 	Catalog &catalog;
158: 	unique_ptr<data_t[]> data;
159: 	MemoryStream stream;
160: 	BinaryDeserializer deserializer;
161: 	bool deserialize_only;
162: };
163: 
164: //===--------------------------------------------------------------------===//
165: // Replay
166: //===--------------------------------------------------------------------===//
167: unique_ptr<WriteAheadLog> WriteAheadLog::Replay(FileSystem &fs, AttachedDatabase &db, const string &wal_path) {
168: 	auto handle = fs.OpenFile(wal_path, FileFlags::FILE_FLAGS_READ | FileFlags::FILE_FLAGS_NULL_IF_NOT_EXISTS);
169: 	if (!handle) {
170: 		// WAL does not exist - instantiate an empty WAL
171: 		return make_uniq<WriteAheadLog>(db, wal_path);
172: 	}
173: 	auto wal_handle = ReplayInternal(db, std::move(handle));
174: 	if (wal_handle) {
175: 		return wal_handle;
176: 	}
177: 	// replay returning NULL indicates we can nuke the WAL entirely - but only if this is not a read-only connection
178: 	if (!db.IsReadOnly()) {
179: 		fs.RemoveFile(wal_path);
180: 	}
181: 	return make_uniq<WriteAheadLog>(db, wal_path);
182: }
183: unique_ptr<WriteAheadLog> WriteAheadLog::ReplayInternal(AttachedDatabase &database, unique_ptr<FileHandle> handle) {
184: 	Connection con(database.GetDatabase());
185: 	auto wal_path = handle->GetPath();
186: 	BufferedFileReader reader(FileSystem::Get(database), std::move(handle));
187: 	if (reader.Finished()) {
188: 		// WAL file exists but it is empty - we can delete the file
189: 		return nullptr;
190: 	}
191: 
192: 	con.BeginTransaction();
193: 	MetaTransaction::Get(*con.context).ModifyDatabase(database);
194: 
195: 	auto &config = DBConfig::GetConfig(database.GetDatabase());
196: 	// first deserialize the WAL to look for a checkpoint flag
197: 	// if there is a checkpoint flag, we might have already flushed the contents of the WAL to disk
198: 	ReplayState checkpoint_state(database, *con.context);
199: 	try {
200: 		while (true) {
201: 			// read the current entry (deserialize only)
202: 			auto deserializer = WriteAheadLogDeserializer::Open(checkpoint_state, reader, true);
203: 			if (deserializer.ReplayEntry()) {
204: 				// check if the file is exhausted
205: 				if (reader.Finished()) {
206: 					// we finished reading the file: break
207: 					break;
208: 				}
209: 			}
210: 		}
211: 	} catch (std::exception &ex) { // LCOV_EXCL_START
212: 		ErrorData error(ex);
213: 		// ignore serialization exceptions - they signal a torn WAL
214: 		if (error.Type() != ExceptionType::SERIALIZATION) {
215: 			error.Throw("Failure while replaying WAL file \"" + wal_path + "\": ");
216: 		}
217: 	} // LCOV_EXCL_STOP
218: 	if (checkpoint_state.checkpoint_id.IsValid()) {
219: 		// there is a checkpoint flag: check if we need to deserialize the WAL
220: 		auto &manager = database.GetStorageManager();
221: 		if (manager.IsCheckpointClean(checkpoint_state.checkpoint_id)) {
222: 			// the contents of the WAL have already been checkpointed
223: 			// we can safely truncate the WAL and ignore its contents
224: 			return nullptr;
225: 		}
226: 	}
227: 
228: 	// we need to recover from the WAL: actually set up the replay state
229: 	ReplayState state(database, *con.context);
230: 
231: 	// reset the reader - we are going to read the WAL from the beginning again
232: 	reader.Reset();
233: 
234: 	// replay the WAL
235: 	// note that everything is wrapped inside a try/catch block here
236: 	// there can be errors in WAL replay because of a corrupt WAL file
237: 	idx_t successful_offset = 0;
238: 	bool all_succeeded = false;
239: 	try {
240: 		while (true) {
241: 			// read the current entry
242: 			auto deserializer = WriteAheadLogDeserializer::Open(state, reader);
243: 			if (deserializer.ReplayEntry()) {
244: 				con.Commit();
245: 				successful_offset = reader.offset;
246: 				// check if the file is exhausted
247: 				if (reader.Finished()) {
248: 					// we finished reading the file: break
249: 					all_succeeded = true;
250: 					break;
251: 				}
252: 				con.BeginTransaction();
253: 				MetaTransaction::Get(*con.context).ModifyDatabase(database);
254: 			}
255: 		}
256: 	} catch (std::exception &ex) { // LCOV_EXCL_START
257: 		// exception thrown in WAL replay: rollback
258: 		con.Query("ROLLBACK");
259: 		ErrorData error(ex);
260: 		// serialization failure means a truncated WAL
261: 		// these failures are ignored unless abort_on_wal_failure is true
262: 		// other failures always result in an error
263: 		if (config.options.abort_on_wal_failure || error.Type() != ExceptionType::SERIALIZATION) {
264: 			error.Throw("Failure while replaying WAL file \"" + wal_path + "\": ");
265: 		}
266: 	} catch (...) {
267: 		// exception thrown in WAL replay: rollback
268: 		con.Query("ROLLBACK");
269: 		throw;
270: 	} // LCOV_EXCL_STOP
271: 	auto init_state = all_succeeded ? WALInitState::UNINITIALIZED : WALInitState::UNINITIALIZED_REQUIRES_TRUNCATE;
272: 	return make_uniq<WriteAheadLog>(database, wal_path, successful_offset, init_state);
273: }
274: 
275: //===--------------------------------------------------------------------===//
276: // Replay Entries
277: //===--------------------------------------------------------------------===//
278: void WriteAheadLogDeserializer::ReplayEntry(WALType entry_type) {
279: 	switch (entry_type) {
280: 	case WALType::WAL_VERSION:
281: 		ReplayVersion();
282: 		break;
283: 	case WALType::CREATE_TABLE:
284: 		ReplayCreateTable();
285: 		break;
286: 	case WALType::DROP_TABLE:
287: 		ReplayDropTable();
288: 		break;
289: 	case WALType::ALTER_INFO:
290: 		ReplayAlter();
291: 		break;
292: 	case WALType::CREATE_VIEW:
293: 		ReplayCreateView();
294: 		break;
295: 	case WALType::DROP_VIEW:
296: 		ReplayDropView();
297: 		break;
298: 	case WALType::CREATE_SCHEMA:
299: 		ReplayCreateSchema();
300: 		break;
301: 	case WALType::DROP_SCHEMA:
302: 		ReplayDropSchema();
303: 		break;
304: 	case WALType::CREATE_SEQUENCE:
305: 		ReplayCreateSequence();
306: 		break;
307: 	case WALType::DROP_SEQUENCE:
308: 		ReplayDropSequence();
309: 		break;
310: 	case WALType::SEQUENCE_VALUE:
311: 		ReplaySequenceValue();
312: 		break;
313: 	case WALType::CREATE_MACRO:
314: 		ReplayCreateMacro();
315: 		break;
316: 	case WALType::DROP_MACRO:
317: 		ReplayDropMacro();
318: 		break;
319: 	case WALType::CREATE_TABLE_MACRO:
320: 		ReplayCreateTableMacro();
321: 		break;
322: 	case WALType::DROP_TABLE_MACRO:
323: 		ReplayDropTableMacro();
324: 		break;
325: 	case WALType::CREATE_INDEX:
326: 		ReplayCreateIndex();
327: 		break;
328: 	case WALType::DROP_INDEX:
329: 		ReplayDropIndex();
330: 		break;
331: 	case WALType::USE_TABLE:
332: 		ReplayUseTable();
333: 		break;
334: 	case WALType::INSERT_TUPLE:
335: 		ReplayInsert();
336: 		break;
337: 	case WALType::ROW_GROUP_DATA:
338: 		ReplayRowGroupData();
339: 		break;
340: 	case WALType::DELETE_TUPLE:
341: 		ReplayDelete();
342: 		break;
343: 	case WALType::UPDATE_TUPLE:
344: 		ReplayUpdate();
345: 		break;
346: 	case WALType::CHECKPOINT:
347: 		ReplayCheckpoint();
348: 		break;
349: 	case WALType::CREATE_TYPE:
350: 		ReplayCreateType();
351: 		break;
352: 	case WALType::DROP_TYPE:
353: 		ReplayDropType();
354: 		break;
355: 	default:
356: 		throw InternalException("Invalid WAL entry type!");
357: 	}
358: }
359: 
360: //===--------------------------------------------------------------------===//
361: // Replay Version
362: //===--------------------------------------------------------------------===//
363: void WriteAheadLogDeserializer::ReplayVersion() {
364: 	state.wal_version = deserializer.ReadProperty<idx_t>(101, "version");
365: }
366: 
367: //===--------------------------------------------------------------------===//
368: // Replay Table
369: //===--------------------------------------------------------------------===//
370: void WriteAheadLogDeserializer::ReplayCreateTable() {
371: 	auto info = deserializer.ReadProperty<unique_ptr<CreateInfo>>(101, "table");
372: 	if (DeserializeOnly()) {
373: 		return;
374: 	}
375: 	// bind the constraints to the table again
376: 	auto binder = Binder::CreateBinder(context);
377: 	auto &schema = catalog.GetSchema(context, info->schema);
378: 	auto bound_info = Binder::BindCreateTableCheckpoint(std::move(info), schema);
379: 
380: 	catalog.CreateTable(context, *bound_info);
381: }
382: 
383: void WriteAheadLogDeserializer::ReplayDropTable() {
384: 	DropInfo info;
385: 
386: 	info.type = CatalogType::TABLE_ENTRY;
387: 	info.schema = deserializer.ReadProperty<string>(101, "schema");
388: 	info.name = deserializer.ReadProperty<string>(102, "name");
389: 	if (DeserializeOnly()) {
390: 		return;
391: 	}
392: 
393: 	catalog.DropEntry(context, info);
394: }
395: 
396: void ReplayWithoutIndex(ClientContext &context, Catalog &catalog, AlterInfo &info, const bool only_deserialize) {
397: 	if (only_deserialize) {
398: 		return;
399: 	}
400: 	catalog.Alter(context, info);
401: }
402: 
403: void ReplayIndexData(AttachedDatabase &db, BinaryDeserializer &deserializer, IndexStorageInfo &info,
404:                      const bool deserialize_only) {
405: 	D_ASSERT(info.IsValid() && !info.name.empty());
406: 
407: 	auto &storage_manager = db.GetStorageManager();
408: 	auto &single_file_sm = storage_manager.Cast<SingleFileStorageManager>();
409: 	auto &block_manager = single_file_sm.block_manager;
410: 	auto &buffer_manager = block_manager->buffer_manager;
411: 
412: 	deserializer.ReadList(103, "index_storage", [&](Deserializer::List &list, idx_t i) {
413: 		auto &data_info = info.allocator_infos[i];
414: 
415: 		// Read the data into buffer handles and convert them to blocks on disk.
416: 		for (idx_t j = 0; j < data_info.allocation_sizes.size(); j++) {
417: 
418: 			// Read the data into a buffer handle.
419: 			auto buffer_handle = buffer_manager.Allocate(MemoryTag::ART_INDEX, block_manager->GetBlockSize(), false);
420: 			auto block_handle = buffer_handle.GetBlockHandle();
421: 			auto data_ptr = buffer_handle.Ptr();
422: 
423: 			list.ReadElement<bool>(data_ptr, data_info.allocation_sizes[j]);
424: 
425: 			// Convert the buffer handle to a persistent block and store the block id.
426: 			if (!deserialize_only) {
427: 				auto block_id = block_manager->GetFreeBlockId();
428: 				block_manager->ConvertToPersistent(block_id, std::move(block_handle), std::move(buffer_handle));
429: 				data_info.block_pointers[j].block_id = block_id;
430: 			}
431: 		}
432: 	});
433: }
434: 
435: void WriteAheadLogDeserializer::ReplayAlter() {
436: 	auto info = deserializer.ReadProperty<unique_ptr<ParseInfo>>(101, "info");
437: 	auto &alter_info = info->Cast<AlterInfo>();
438: 	if (!alter_info.IsAddPrimaryKey()) {
439: 		return ReplayWithoutIndex(context, catalog, alter_info, DeserializeOnly());
440: 	}
441: 
442: 	auto index_storage_info = deserializer.ReadProperty<IndexStorageInfo>(102, "index_storage_info");
443: 	ReplayIndexData(db, deserializer, index_storage_info, DeserializeOnly());
444: 	if (DeserializeOnly()) {
445: 		return;
446: 	}
447: 
448: 	auto &table_info = alter_info.Cast<AlterTableInfo>();
449: 	auto &constraint_info = table_info.Cast<AddConstraintInfo>();
450: 	auto &unique_info = constraint_info.constraint->Cast<UniqueConstraint>();
451: 
452: 	auto &table =
453: 	    catalog.GetEntry<TableCatalogEntry>(context, table_info.schema, table_info.name).Cast<DuckTableEntry>();
454: 	auto &column_list = table.GetColumns();
455: 
456: 	// Add the table to the bind context to bind the parsed expressions.
457: 	auto binder = Binder::CreateBinder(context);
458: 	vector<LogicalType> column_types;
459: 	vector<string> column_names;
460: 	for (auto &col : column_list.Logical()) {
461: 		column_types.push_back(col.Type());
462: 		column_names.push_back(col.Name());
463: 	}
464: 
465: 	// Create a binder to bind the parsed expressions.
466: 	vector<ColumnIndex> column_indexes;
467: 	binder->bind_context.AddBaseTable(0, string(), column_names, column_types, column_indexes, table);
468: 	IndexBinder idx_binder(*binder, context);
469: 
470: 	// Bind the parsed expressions to create unbound expressions.
471: 	vector<unique_ptr<Expression>> unbound_expressions;
472: 	auto logical_indexes = unique_info.GetLogicalIndexes(column_list);
473: 	for (const auto &logical_index : logical_indexes) {
474: 		auto &col = column_list.GetColumn(logical_index);
475: 		unique_ptr<ParsedExpression> parsed = make_uniq<ColumnRefExpression>(col.GetName(), table_info.name);
476: 		unbound_expressions.push_back(idx_binder.Bind(parsed));
477: 	}
478: 
479: 	vector<column_t> column_ids;
480: 	for (auto &column_index : column_indexes) {
481: 		column_ids.push_back(column_index.GetPrimaryIndex());
482: 	}
483: 
484: 	auto &storage = table.GetStorage();
485: 	CreateIndexInput input(TableIOManager::Get(storage), storage.db, IndexConstraintType::PRIMARY,
486: 	                       index_storage_info.name, column_ids, unbound_expressions, index_storage_info,
487: 	                       index_storage_info.options);
488: 
489: 	auto index_type = context.db->config.GetIndexTypes().FindByName(ART::TYPE_NAME);
490: 	auto index_instance = index_type->create_instance(input);
491: 	storage.AddIndex(std::move(index_instance));
492: 
493: 	catalog.Alter(context, alter_info);
494: }
495: 
496: //===--------------------------------------------------------------------===//
497: // Replay View
498: //===--------------------------------------------------------------------===//
499: void WriteAheadLogDeserializer::ReplayCreateView() {
500: 	auto entry = deserializer.ReadProperty<unique_ptr<CreateInfo>>(101, "view");
501: 	if (DeserializeOnly()) {
502: 		return;
503: 	}
504: 	catalog.CreateView(context, entry->Cast<CreateViewInfo>());
505: }
506: 
507: void WriteAheadLogDeserializer::ReplayDropView() {
508: 	DropInfo info;
509: 	info.type = CatalogType::VIEW_ENTRY;
510: 	info.schema = deserializer.ReadProperty<string>(101, "schema");
511: 	info.name = deserializer.ReadProperty<string>(102, "name");
512: 	if (DeserializeOnly()) {
513: 		return;
514: 	}
515: 	catalog.DropEntry(context, info);
516: }
517: 
518: //===--------------------------------------------------------------------===//
519: // Replay Schema
520: //===--------------------------------------------------------------------===//
521: void WriteAheadLogDeserializer::ReplayCreateSchema() {
522: 	CreateSchemaInfo info;
523: 	info.schema = deserializer.ReadProperty<string>(101, "schema");
524: 	if (DeserializeOnly()) {
525: 		return;
526: 	}
527: 
528: 	catalog.CreateSchema(context, info);
529: }
530: 
531: void WriteAheadLogDeserializer::ReplayDropSchema() {
532: 	DropInfo info;
533: 
534: 	info.type = CatalogType::SCHEMA_ENTRY;
535: 	info.name = deserializer.ReadProperty<string>(101, "schema");
536: 	if (DeserializeOnly()) {
537: 		return;
538: 	}
539: 
540: 	catalog.DropEntry(context, info);
541: }
542: 
543: //===--------------------------------------------------------------------===//
544: // Replay Custom Type
545: //===--------------------------------------------------------------------===//
546: void WriteAheadLogDeserializer::ReplayCreateType() {
547: 	auto info = deserializer.ReadProperty<unique_ptr<CreateInfo>>(101, "type");
548: 	info->on_conflict = OnCreateConflict::IGNORE_ON_CONFLICT;
549: 	catalog.CreateType(context, info->Cast<CreateTypeInfo>());
550: }
551: 
552: void WriteAheadLogDeserializer::ReplayDropType() {
553: 	DropInfo info;
554: 
555: 	info.type = CatalogType::TYPE_ENTRY;
556: 	info.schema = deserializer.ReadProperty<string>(101, "schema");
557: 	info.name = deserializer.ReadProperty<string>(102, "name");
558: 	if (DeserializeOnly()) {
559: 		return;
560: 	}
561: 
562: 	catalog.DropEntry(context, info);
563: }
564: 
565: //===--------------------------------------------------------------------===//
566: // Replay Sequence
567: //===--------------------------------------------------------------------===//
568: void WriteAheadLogDeserializer::ReplayCreateSequence() {
569: 	auto entry = deserializer.ReadProperty<unique_ptr<CreateInfo>>(101, "sequence");
570: 	if (DeserializeOnly()) {
571: 		return;
572: 	}
573: 
574: 	catalog.CreateSequence(context, entry->Cast<CreateSequenceInfo>());
575: }
576: 
577: void WriteAheadLogDeserializer::ReplayDropSequence() {
578: 	DropInfo info;
579: 	info.type = CatalogType::SEQUENCE_ENTRY;
580: 	info.schema = deserializer.ReadProperty<string>(101, "schema");
581: 	info.name = deserializer.ReadProperty<string>(102, "name");
582: 	if (DeserializeOnly()) {
583: 		return;
584: 	}
585: 
586: 	catalog.DropEntry(context, info);
587: }
588: 
589: void WriteAheadLogDeserializer::ReplaySequenceValue() {
590: 	auto schema = deserializer.ReadProperty<string>(101, "schema");
591: 	auto name = deserializer.ReadProperty<string>(102, "name");
592: 	auto usage_count = deserializer.ReadProperty<uint64_t>(103, "usage_count");
593: 	auto counter = deserializer.ReadProperty<int64_t>(104, "counter");
594: 	if (DeserializeOnly()) {
595: 		return;
596: 	}
597: 
598: 	// fetch the sequence from the catalog
599: 	auto &seq = catalog.GetEntry<SequenceCatalogEntry>(context, schema, name);
600: 	seq.ReplayValue(usage_count, counter);
601: }
602: 
603: //===--------------------------------------------------------------------===//
604: // Replay Macro
605: //===--------------------------------------------------------------------===//
606: void WriteAheadLogDeserializer::ReplayCreateMacro() {
607: 	auto entry = deserializer.ReadProperty<unique_ptr<CreateInfo>>(101, "macro");
608: 	if (DeserializeOnly()) {
609: 		return;
610: 	}
611: 
612: 	catalog.CreateFunction(context, entry->Cast<CreateMacroInfo>());
613: }
614: 
615: void WriteAheadLogDeserializer::ReplayDropMacro() {
616: 	DropInfo info;
617: 	info.type = CatalogType::MACRO_ENTRY;
618: 	info.schema = deserializer.ReadProperty<string>(101, "schema");
619: 	info.name = deserializer.ReadProperty<string>(102, "name");
620: 	if (DeserializeOnly()) {
621: 		return;
622: 	}
623: 
624: 	catalog.DropEntry(context, info);
625: }
626: 
627: //===--------------------------------------------------------------------===//
628: // Replay Table Macro
629: //===--------------------------------------------------------------------===//
630: void WriteAheadLogDeserializer::ReplayCreateTableMacro() {
631: 	auto entry = deserializer.ReadProperty<unique_ptr<CreateInfo>>(101, "table_macro");
632: 	if (DeserializeOnly()) {
633: 		return;
634: 	}
635: 	catalog.CreateFunction(context, entry->Cast<CreateMacroInfo>());
636: }
637: 
638: void WriteAheadLogDeserializer::ReplayDropTableMacro() {
639: 	DropInfo info;
640: 	info.type = CatalogType::TABLE_MACRO_ENTRY;
641: 	info.schema = deserializer.ReadProperty<string>(101, "schema");
642: 	info.name = deserializer.ReadProperty<string>(102, "name");
643: 	if (DeserializeOnly()) {
644: 		return;
645: 	}
646: 
647: 	catalog.DropEntry(context, info);
648: }
649: 
650: //===--------------------------------------------------------------------===//
651: // Replay Index
652: //===--------------------------------------------------------------------===//
653: void WriteAheadLogDeserializer::ReplayCreateIndex() {
654: 	auto create_info = deserializer.ReadProperty<unique_ptr<CreateInfo>>(101, "index_catalog_entry");
655: 	auto index_info = deserializer.ReadProperty<IndexStorageInfo>(102, "index_storage_info");
656: 
657: 	ReplayIndexData(db, deserializer, index_info, DeserializeOnly());
658: 	if (DeserializeOnly()) {
659: 		return;
660: 	}
661: 
662: 	auto &info = create_info->Cast<CreateIndexInfo>();
663: 
664: 	// Ensure that the index type exists.
665: 	if (info.index_type.empty()) {
666: 		info.index_type = ART::TYPE_NAME;
667: 	}
668: 
669: 	auto index_type = context.db->config.GetIndexTypes().FindByName(info.index_type);
670: 	if (!index_type) {
671: 		throw InternalException("Index type \"%s\" not recognized", info.index_type);
672: 	}
673: 
674: 	// Create the index in the catalog.
675: 	auto &table = catalog.GetEntry<TableCatalogEntry>(context, create_info->schema, info.table).Cast<DuckTableEntry>();
676: 	auto &index = table.schema.CreateIndex(context, info, table)->Cast<DuckIndexEntry>();
677: 
678: 	// Add the table to the bind context to bind the parsed expressions.
679: 	auto binder = Binder::CreateBinder(context);
680: 	vector<LogicalType> column_types;
681: 	vector<string> column_names;
682: 	for (auto &col : table.GetColumns().Logical()) {
683: 		column_types.push_back(col.Type());
684: 		column_names.push_back(col.Name());
685: 	}
686: 
687: 	// create a binder to bind the parsed expressions
688: 	vector<ColumnIndex> column_ids;
689: 	binder->bind_context.AddBaseTable(0, string(), column_names, column_types, column_ids, table);
690: 	IndexBinder idx_binder(*binder, context);
691: 
692: 	// Bind the parsed expressions to create unbound expressions.
693: 	vector<unique_ptr<Expression>> unbound_expressions;
694: 	for (auto &expr : index.parsed_expressions) {
695: 		auto copy = expr->Copy();
696: 		unbound_expressions.push_back(idx_binder.Bind(copy));
697: 	}
698: 
699: 	auto &storage = table.GetStorage();
700: 	CreateIndexInput input(TableIOManager::Get(storage), storage.db, info.constraint_type, info.index_name,
701: 	                       info.column_ids, unbound_expressions, index_info, info.options);
702: 	auto index_instance = index_type->create_instance(input);
703: 	storage.AddIndex(std::move(index_instance));
704: }
705: 
706: void WriteAheadLogDeserializer::ReplayDropIndex() {
707: 	DropInfo info;
708: 	info.type = CatalogType::INDEX_ENTRY;
709: 	info.schema = deserializer.ReadProperty<string>(101, "schema");
710: 	info.name = deserializer.ReadProperty<string>(102, "name");
711: 	if (DeserializeOnly()) {
712: 		return;
713: 	}
714: 
715: 	catalog.DropEntry(context, info);
716: }
717: 
718: //===--------------------------------------------------------------------===//
719: // Replay Data
720: //===--------------------------------------------------------------------===//
721: void WriteAheadLogDeserializer::ReplayUseTable() {
722: 	auto schema_name = deserializer.ReadProperty<string>(101, "schema");
723: 	auto table_name = deserializer.ReadProperty<string>(102, "table");
724: 	if (DeserializeOnly()) {
725: 		return;
726: 	}
727: 	state.current_table = &catalog.GetEntry<TableCatalogEntry>(context, schema_name, table_name);
728: }
729: 
730: void WriteAheadLogDeserializer::ReplayInsert() {
731: 	DataChunk chunk;
732: 	deserializer.ReadObject(101, "chunk", [&](Deserializer &object) { chunk.Deserialize(object); });
733: 	if (DeserializeOnly()) {
734: 		return;
735: 	}
736: 	if (!state.current_table) {
737: 		throw InternalException("Corrupt WAL: insert without table");
738: 	}
739: 
740: 	// Append to the current table without constraint verification.
741: 	vector<unique_ptr<BoundConstraint>> bound_constraints;
742: 	auto &storage = state.current_table->GetStorage();
743: 	storage.LocalAppend(*state.current_table, context, chunk, bound_constraints);
744: }
745: 
746: static void MarkBlocksAsUsed(BlockManager &manager, const PersistentColumnData &col_data) {
747: 	for (auto &pointer : col_data.pointers) {
748: 		auto block_id = pointer.block_pointer.block_id;
749: 		if (block_id != INVALID_BLOCK) {
750: 			manager.MarkBlockAsUsed(block_id);
751: 		}
752: 		if (pointer.segment_state) {
753: 			for (auto &block : pointer.segment_state->blocks) {
754: 				manager.MarkBlockAsUsed(block);
755: 			}
756: 		}
757: 	}
758: 	for (auto &child_column : col_data.child_columns) {
759: 		MarkBlocksAsUsed(manager, child_column);
760: 	}
761: }
762: 
763: void WriteAheadLogDeserializer::ReplayRowGroupData() {
764: 	auto &block_manager = db.GetStorageManager().GetBlockManager();
765: 	PersistentCollectionData data;
766: 	deserializer.Set<DatabaseInstance &>(db.GetDatabase());
767: 	CompressionInfo compression_info(block_manager.GetBlockSize());
768: 	deserializer.Set<const CompressionInfo &>(compression_info);
769: 	deserializer.ReadProperty(101, "row_group_data", data);
770: 	deserializer.Unset<const CompressionInfo>();
771: 	deserializer.Unset<DatabaseInstance>();
772: 	if (DeserializeOnly()) {
773: 		// label blocks in data as used - they will be used after the WAL replay is finished
774: 		// we need to do this during the deserialization phase to ensure the blocks will not be overwritten
775: 		// by previous deserialization steps
776: 		for (auto &group : data.row_group_data) {
777: 			for (auto &col_data : group.column_data) {
778: 				MarkBlocksAsUsed(block_manager, col_data);
779: 			}
780: 		}
781: 		return;
782: 	}
783: 	if (!state.current_table) {
784: 		throw InternalException("Corrupt WAL: insert without table");
785: 	}
786: 	auto &storage = state.current_table->GetStorage();
787: 	auto &table_info = storage.GetDataTableInfo();
788: 	RowGroupCollection new_row_groups(table_info, table_info->GetIOManager(), storage.GetTypes(), 0);
789: 	new_row_groups.Initialize(data);
790: 	TableIndexList index_list;
791: 	storage.MergeStorage(new_row_groups, index_list, nullptr);
792: }
793: 
794: void WriteAheadLogDeserializer::ReplayDelete() {
795: 	DataChunk chunk;
796: 	deserializer.ReadObject(101, "chunk", [&](Deserializer &object) { chunk.Deserialize(object); });
797: 	if (DeserializeOnly()) {
798: 		return;
799: 	}
800: 	if (!state.current_table) {
801: 		throw InternalException("Corrupt WAL: delete without table");
802: 	}
803: 
804: 	D_ASSERT(chunk.ColumnCount() == 1 && chunk.data[0].GetType() == LogicalType::ROW_TYPE);
805: 	row_t row_ids[1];
806: 	Vector row_identifiers(LogicalType::ROW_TYPE, data_ptr_cast(row_ids));
807: 
808: 	auto source_ids = FlatVector::GetData<row_t>(chunk.data[0]);
809: 	// delete the tuples from the current table
810: 	TableDeleteState delete_state;
811: 	for (idx_t i = 0; i < chunk.size(); i++) {
812: 		row_ids[0] = source_ids[i];
813: 		state.current_table->GetStorage().Delete(delete_state, context, row_identifiers, 1);
814: 	}
815: }
816: 
817: void WriteAheadLogDeserializer::ReplayUpdate() {
818: 	auto column_path = deserializer.ReadProperty<vector<column_t>>(101, "column_indexes");
819: 
820: 	DataChunk chunk;
821: 	deserializer.ReadObject(102, "chunk", [&](Deserializer &object) { chunk.Deserialize(object); });
822: 
823: 	if (DeserializeOnly()) {
824: 		return;
825: 	}
826: 	if (!state.current_table) {
827: 		throw InternalException("Corrupt WAL: update without table");
828: 	}
829: 
830: 	if (column_path[0] >= state.current_table->GetColumns().PhysicalColumnCount()) {
831: 		throw InternalException("Corrupt WAL: column index for update out of bounds");
832: 	}
833: 
834: 	// remove the row id vector from the chunk
835: 	auto row_ids = std::move(chunk.data.back());
836: 	chunk.data.pop_back();
837: 
838: 	// now perform the update
839: 	state.current_table->GetStorage().UpdateColumn(*state.current_table, context, row_ids, column_path, chunk);
840: }
841: 
842: void WriteAheadLogDeserializer::ReplayCheckpoint() {
843: 	state.checkpoint_id = deserializer.ReadProperty<MetaBlockPointer>(101, "meta_block");
844: }
845: 
846: } // namespace duckdb
[end of src/storage/wal_replay.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: