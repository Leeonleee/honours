diff --git a/.github/config/out_of_tree_extensions.cmake b/.github/config/out_of_tree_extensions.cmake
index f3c49daae57f..2d628facb839 100644
--- a/.github/config/out_of_tree_extensions.cmake
+++ b/.github/config/out_of_tree_extensions.cmake
@@ -28,8 +28,7 @@ if (NOT MINGW AND NOT ${WASM_ENABLED} AND NOT ${MUSL_ENABLED})
     duckdb_extension_load(arrow
             LOAD_TESTS DONT_LINK
             GIT_URL https://github.com/duckdb/arrow
-            GIT_TAG c50862c82c065096722745631f4230832a3a04e8
-            APPLY_PATCHES
+            GIT_TAG cff2f0e21b1608e38640e15b4cf0693dd52dd0eb
             )
 endif()
 
diff --git a/.github/patches/extensions/arrow/arrow_extension_types.patch b/.github/patches/extensions/arrow/arrow_extension_types.patch
deleted file mode 100644
index f99754806186..000000000000
--- a/.github/patches/extensions/arrow/arrow_extension_types.patch
+++ /dev/null
@@ -1,57 +0,0 @@
-diff --git a/src/arrow_scan_ipc.cpp b/src/arrow_scan_ipc.cpp
-index a60d255..37e0e81 100644
---- a/src/arrow_scan_ipc.cpp
-+++ b/src/arrow_scan_ipc.cpp
-@@ -15,7 +15,6 @@ TableFunction ArrowIPCTableFunction::GetFunction() {
-       ArrowTableFunction::ArrowScanInitLocal);
-
-   scan_arrow_ipc_func.cardinality = ArrowTableFunction::ArrowScanCardinality;
--  scan_arrow_ipc_func.get_batch_index = nullptr; // TODO implement
-   scan_arrow_ipc_func.projection_pushdown = true;
-   scan_arrow_ipc_func.filter_pushdown = false;
-
-@@ -71,9 +70,12 @@ unique_ptr<FunctionData> ArrowIPCTableFunction::ArrowScanBind(
-     if (!schema.release) {
-       throw InvalidInputException("arrow_scan: released schema passed");
-     }
--    auto arrow_type = GetArrowLogicalType(schema);
-+    auto arrow_type =
-+       ArrowType::GetArrowLogicalType(DBConfig::GetConfig(context), schema);
-+
-     if (schema.dictionary) {
--      auto dictionary_type = GetArrowLogicalType(*schema.dictionary);
-+      auto dictionary_type = ArrowType::GetArrowLogicalType(
-+          DBConfig::GetConfig(context), *schema.dictionary);
-       return_types.emplace_back(dictionary_type->GetDuckType());
-       arrow_type->SetDictionary(std::move(dictionary_type));
-     } else {
-diff --git a/src/arrow_to_ipc.cpp b/src/arrow_to_ipc.cpp
-index c316d85..905df2b 100644
---- a/src/arrow_to_ipc.cpp
-+++ b/src/arrow_to_ipc.cpp
-@@ -76,9 +76,9 @@ ToArrowIPCFunction::Bind(ClientContext &context, TableFunctionBindInput &input,
-
-   // Create the Arrow schema
-   ArrowSchema schema;
-+  auto properties = context.GetClientProperties();
-   ArrowConverter::ToArrowSchema(&schema, input.input_table_types,
--                                input.input_table_names,
--                                context.GetClientProperties());
-+                                input.input_table_names, properties);
-   result->schema = arrow::ImportSchema(&schema).ValueOrDie();
-
-   return std::move(result);
-@@ -116,9 +116,10 @@ OperatorResultType ToArrowIPCFunction::Function(ExecutionContext &context,
-     output.data[1].SetValue(0, Value::BOOLEAN(1));
-   } else {
-     if (!local_state.appender) {
--      local_state.appender =
--          make_uniq<ArrowAppender>(input.GetTypes(), data.chunk_size,
--                                   context.client.GetClientProperties());
-+      local_state.appender = make_uniq<ArrowAppender>(input.GetTypes(), data.chunk_size,
-+                                   context.client.GetClientProperties(),
-+                                   ArrowTypeExtensionData::GetExtensionTypes(
-+                                       context.client, input.GetTypes()));
-     }
-
-     // Append input chunk
diff --git a/.github/workflows/Pyodide.yml b/.github/workflows/Pyodide.yml
index 0b6950ce89a5..32e14802065c 100644
--- a/.github/workflows/Pyodide.yml
+++ b/.github/workflows/Pyodide.yml
@@ -108,7 +108,7 @@ jobs:
       - name: install deps into environment
         run: |
           source .venv-pyodide/bin/activate
-          pip install pytest numpy pandas mypy
+          pip install pytest numpy pandas 'mypy<=1.13'
 
       - name: install duckdb wasm wheel into environment
         run: |
diff --git a/benchmark/tpch/startup.cpp b/benchmark/tpch/startup.cpp
index 88f678d22fdf..a468545476f2 100644
--- a/benchmark/tpch/startup.cpp
+++ b/benchmark/tpch/startup.cpp
@@ -1,6 +1,5 @@
 #include "benchmark_runner.hpp"
 #include "compare_result.hpp"
-#include "tpch_extension.hpp"
 #include "duckdb_benchmark_macro.hpp"
 
 using namespace duckdb;
@@ -70,6 +69,6 @@ TPCHStartup("PRAGMA tpch(1)") NormalConfig() string VerifyResult(QueryResult *re
 	if (result->HasError()) {
 		return result->GetError();
 	}
-	return compare_csv(*result, TpchExtension::GetAnswer(SF, 1), true);
+	return string();
 }
 FINISH_BENCHMARK(TPCHQ1)
diff --git a/extension/core_functions/scalar/list/flatten.cpp b/extension/core_functions/scalar/list/flatten.cpp
index 849c20d16c05..5f4361cdbb1a 100644
--- a/extension/core_functions/scalar/list/flatten.cpp
+++ b/extension/core_functions/scalar/list/flatten.cpp
@@ -7,44 +7,41 @@
 
 namespace duckdb {
 
-void ListFlattenFunction(DataChunk &args, ExpressionState &state, Vector &result) {
-	D_ASSERT(args.ColumnCount() == 1);
+static void ListFlattenFunction(DataChunk &args, ExpressionState &, Vector &result) {
 
-	Vector &input = args.data[0];
-	if (input.GetType().id() == LogicalTypeId::SQLNULL) {
-		result.Reference(input);
+	const auto flat_list_data = FlatVector::GetData<list_entry_t>(result);
+	auto &flat_list_mask = FlatVector::Validity(result);
+
+	UnifiedVectorFormat outer_format;
+	UnifiedVectorFormat inner_format;
+	UnifiedVectorFormat items_format;
+
+	// Setup outer vec;
+	auto &outer_vec = args.data[0];
+	const auto outer_count = args.size();
+	outer_vec.ToUnifiedFormat(outer_count, outer_format);
+
+	// Special case: outer list is all-null
+	if (outer_vec.GetType().id() == LogicalTypeId::SQLNULL) {
+		result.Reference(outer_vec);
 		return;
 	}
 
-	idx_t count = args.size();
-
-	// Prepare the result vector
-	result.SetVectorType(VectorType::FLAT_VECTOR);
-	// This holds the new offsets and lengths
-	auto result_entries = FlatVector::GetData<list_entry_t>(result);
-	auto &result_validity = FlatVector::Validity(result);
-
-	// The outermost list in each row
-	UnifiedVectorFormat row_data;
-	input.ToUnifiedFormat(count, row_data);
-	auto row_entries = UnifiedVectorFormat::GetData<list_entry_t>(row_data);
-
-	// The list elements in each row: [HERE, ...]
-	auto &row_lists = ListVector::GetEntry(input);
-	UnifiedVectorFormat row_lists_data;
-	idx_t total_row_lists = ListVector::GetListSize(input);
-	row_lists.ToUnifiedFormat(total_row_lists, row_lists_data);
-	auto row_lists_entries = UnifiedVectorFormat::GetData<list_entry_t>(row_lists_data);
-
-	if (row_lists.GetType().id() == LogicalTypeId::SQLNULL) {
-		for (idx_t row_cnt = 0; row_cnt < count; row_cnt++) {
-			auto row_idx = row_data.sel->get_index(row_cnt);
-			if (!row_data.validity.RowIsValid(row_idx)) {
-				result_validity.SetInvalid(row_cnt);
+	// Setup inner vec
+	auto &inner_vec = ListVector::GetEntry(outer_vec);
+	const auto inner_count = ListVector::GetListSize(outer_vec);
+	inner_vec.ToUnifiedFormat(inner_count, inner_format);
+
+	// Special case: inner list is all-null
+	if (inner_vec.GetType().id() == LogicalTypeId::SQLNULL) {
+		for (idx_t outer_raw_idx = 0; outer_raw_idx < outer_count; outer_raw_idx++) {
+			const auto outer_idx = outer_format.sel->get_index(outer_raw_idx);
+			if (!outer_format.validity.RowIsValid(outer_idx)) {
+				flat_list_mask.SetInvalid(outer_raw_idx);
 				continue;
 			}
-			result_entries[row_cnt].offset = 0;
-			result_entries[row_cnt].length = 0;
+			flat_list_data[outer_raw_idx].offset = 0;
+			flat_list_data[outer_raw_idx].length = 0;
 		}
 		if (args.AllConstant()) {
 			result.SetVectorType(VectorType::CONSTANT_VECTOR);
@@ -52,57 +49,90 @@ void ListFlattenFunction(DataChunk &args, ExpressionState &state, Vector &result
 		return;
 	}
 
-	// The actual elements inside each row list: [[HERE, ...], []]
-	// This one becomes the child vector of the result.
-	auto &elem_vector = ListVector::GetEntry(row_lists);
+	// Setup items vec
+	auto &items_vec = ListVector::GetEntry(inner_vec);
+	const auto items_count = ListVector::GetListSize(inner_vec);
+	items_vec.ToUnifiedFormat(items_count, items_format);
+
+	// First pass: Figure out the total amount of items.
+	// This can be more than items_count if the inner list reference the same item(s) multiple times.
+
+	idx_t total_items = 0;
+
+	const auto outer_data = UnifiedVectorFormat::GetData<list_entry_t>(outer_format);
+	const auto inner_data = UnifiedVectorFormat::GetData<list_entry_t>(inner_format);
+
+	for (idx_t outer_raw_idx = 0; outer_raw_idx < outer_count; outer_raw_idx++) {
+		const auto outer_idx = outer_format.sel->get_index(outer_raw_idx);
+
+		if (!outer_format.validity.RowIsValid(outer_idx)) {
+			continue;
+		}
+
+		const auto &outer_entry = outer_data[outer_idx];
+
+		for (idx_t inner_raw_idx = outer_entry.offset; inner_raw_idx < outer_entry.offset + outer_entry.length;
+		     inner_raw_idx++) {
+			const auto inner_idx = inner_format.sel->get_index(inner_raw_idx);
 
-	// We'll use this selection vector to slice the elem_vector.
-	idx_t child_elem_cnt = ListVector::GetListSize(row_lists);
-	SelectionVector sel(child_elem_cnt);
+			if (!inner_format.validity.RowIsValid(inner_idx)) {
+				continue;
+			}
+
+			const auto &inner_entry = inner_data[inner_idx];
+
+			total_items += inner_entry.length;
+		}
+	}
+
+	// Now we know the total amount of items, we can create our selection vector.
+	SelectionVector sel(total_items);
 	idx_t sel_idx = 0;
 
-	// HERE, [[]], ...
-	for (idx_t row_cnt = 0; row_cnt < count; row_cnt++) {
-		auto row_idx = row_data.sel->get_index(row_cnt);
+	// Second pass: Fill the selection vector (and the result list entries)
+
+	for (idx_t outer_raw_idx = 0; outer_raw_idx < outer_count; outer_raw_idx++) {
+		const auto outer_idx = outer_format.sel->get_index(outer_raw_idx);
 
-		if (!row_data.validity.RowIsValid(row_idx)) {
-			result_validity.SetInvalid(row_cnt);
+		if (!outer_format.validity.RowIsValid(outer_idx)) {
+			flat_list_mask.SetInvalid(outer_raw_idx);
 			continue;
 		}
 
-		idx_t list_offset = sel_idx;
-		idx_t list_length = 0;
+		const auto &outer_entry = outer_data[outer_idx];
+
+		list_entry_t list_entry = {sel_idx, 0};
 
-		// [HERE, [...], ...]
-		auto row_entry = row_entries[row_idx];
-		for (idx_t row_lists_cnt = 0; row_lists_cnt < row_entry.length; row_lists_cnt++) {
-			auto row_lists_idx = row_lists_data.sel->get_index(row_entry.offset + row_lists_cnt);
+		for (idx_t inner_raw_idx = outer_entry.offset; inner_raw_idx < outer_entry.offset + outer_entry.length;
+		     inner_raw_idx++) {
+			const auto inner_idx = inner_format.sel->get_index(inner_raw_idx);
 
-			// Skip invalid lists
-			if (!row_lists_data.validity.RowIsValid(row_lists_idx)) {
+			if (!inner_format.validity.RowIsValid(inner_idx)) {
 				continue;
 			}
 
-			// [[HERE, ...], [.., ...]]
-			auto list_entry = row_lists_entries[row_lists_idx];
-			list_length += list_entry.length;
+			const auto &inner_entry = inner_data[inner_idx];
+
+			list_entry.length += inner_entry.length;
+
+			for (idx_t elem_raw_idx = inner_entry.offset; elem_raw_idx < inner_entry.offset + inner_entry.length;
+			     elem_raw_idx++) {
+				const auto elem_idx = items_format.sel->get_index(elem_raw_idx);
 
-			for (idx_t elem_cnt = 0; elem_cnt < list_entry.length; elem_cnt++) {
-				// offset of the element in the elem_vector.
-				idx_t offset = list_entry.offset + elem_cnt;
-				sel.set_index(sel_idx, offset);
+				sel.set_index(sel_idx, elem_idx);
 				sel_idx++;
 			}
 		}
 
-		result_entries[row_cnt].offset = list_offset;
-		result_entries[row_cnt].length = list_length;
+		// Assign the result list entry
+		flat_list_data[outer_raw_idx] = list_entry;
 	}
 
+	// Now assing the result
 	ListVector::SetListSize(result, sel_idx);
 
 	auto &result_child_vector = ListVector::GetEntry(result);
-	result_child_vector.Slice(elem_vector, sel, sel_idx);
+	result_child_vector.Slice(items_vec, sel, sel_idx);
 	result_child_vector.Flatten(sel_idx);
 
 	if (args.AllConstant()) {
diff --git a/extension/parquet/column_writer.cpp b/extension/parquet/column_writer.cpp
index 2502484b50e8..5735036e6102 100644
--- a/extension/parquet/column_writer.cpp
+++ b/extension/parquet/column_writer.cpp
@@ -717,6 +717,7 @@ void BasicColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {
 	column_chunk.meta_data.total_compressed_size =
 	    UnsafeNumericCast<int64_t>(column_writer.GetTotalWritten() - start_offset);
 	column_chunk.meta_data.total_uncompressed_size = UnsafeNumericCast<int64_t>(total_uncompressed_size);
+	state.row_group.total_byte_size += column_chunk.meta_data.total_uncompressed_size;
 
 	if (state.bloom_filter) {
 		writer.BufferBloomFilter(state.col_idx, std::move(state.bloom_filter));
diff --git a/extension/parquet/parquet_writer.cpp b/extension/parquet/parquet_writer.cpp
index f7e3651ececa..f236362780b2 100644
--- a/extension/parquet/parquet_writer.cpp
+++ b/extension/parquet/parquet_writer.cpp
@@ -395,7 +395,6 @@ void ParquetWriter::PrepareRowGroup(ColumnDataCollection &buffer, PreparedRowGro
 	// set up a new row group for this chunk collection
 	auto &row_group = result.row_group;
 	row_group.num_rows = NumericCast<int64_t>(buffer.Count());
-	row_group.total_byte_size = NumericCast<int64_t>(buffer.SizeInBytes());
 	row_group.__isset.file_offset = true;
 
 	auto &states = result.states;
diff --git a/src/catalog/catalog.cpp b/src/catalog/catalog.cpp
index 0382b24a8b1d..cb2ea83bbbc4 100644
--- a/src/catalog/catalog.cpp
+++ b/src/catalog/catalog.cpp
@@ -426,7 +426,12 @@ vector<CatalogSearchEntry> GetCatalogEntries(CatalogEntryRetriever &retriever, c
 			entries.emplace_back(catalog, schema_name);
 		}
 		if (entries.empty()) {
-			entries.emplace_back(catalog, DEFAULT_SCHEMA);
+			auto catalog_entry = Catalog::GetCatalogEntry(context, catalog);
+			if (catalog_entry) {
+				entries.emplace_back(catalog, catalog_entry->GetDefaultSchema());
+			} else {
+				entries.emplace_back(catalog, DEFAULT_SCHEMA);
+			}
 		}
 	} else {
 		// specific catalog and schema provided
@@ -968,12 +973,16 @@ optional_ptr<SchemaCatalogEntry> Catalog::GetSchema(CatalogEntryRetriever &retri
 			// skip if it is not an attached database
 			continue;
 		}
-		auto on_not_found = i + 1 == entries.size() ? if_not_found : OnEntryNotFound::RETURN_NULL;
+		const auto on_not_found = i + 1 == entries.size() ? if_not_found : OnEntryNotFound::RETURN_NULL;
 		auto result = catalog->GetSchema(retriever.GetContext(), schema_name, on_not_found, error_context);
 		if (result) {
 			return result;
 		}
 	}
+	// Catalog has not been found.
+	if (if_not_found == OnEntryNotFound::THROW_EXCEPTION) {
+		throw CatalogException(error_context, "Catalog with name %s does not exist!", catalog_name);
+	}
 	return nullptr;
 }
 
@@ -1073,6 +1082,10 @@ optional_ptr<DependencyManager> Catalog::GetDependencyManager() {
 	return nullptr;
 }
 
+string Catalog::GetDefaultSchema() const {
+	return DEFAULT_SCHEMA;
+}
+
 //! Whether this catalog has a default table. Catalogs with a default table can be queries by their catalog name
 bool Catalog::HasDefaultTable() const {
 	return !default_table.empty();
diff --git a/src/catalog/catalog_search_path.cpp b/src/catalog/catalog_search_path.cpp
index 2f2a987519df..793de909b9f6 100644
--- a/src/catalog/catalog_search_path.cpp
+++ b/src/catalog/catalog_search_path.cpp
@@ -165,7 +165,7 @@ void CatalogSearchPath::Set(vector<CatalogSearchEntry> new_paths, CatalogSetPath
 		if (path.catalog.empty()) {
 			auto catalog = Catalog::GetCatalogEntry(context, path.schema);
 			if (catalog) {
-				auto schema = catalog->GetSchema(context, DEFAULT_SCHEMA, OnEntryNotFound::RETURN_NULL);
+				auto schema = catalog->GetSchema(context, catalog->GetDefaultSchema(), OnEntryNotFound::RETURN_NULL);
 				if (schema) {
 					path.catalog = std::move(path.schema);
 					path.schema = schema->name;
@@ -205,6 +205,22 @@ string CatalogSearchPath::GetDefaultSchema(const string &catalog) {
 	return DEFAULT_SCHEMA;
 }
 
+string CatalogSearchPath::GetDefaultSchema(ClientContext &context, const string &catalog) {
+	for (auto &path : paths) {
+		if (path.catalog == TEMP_CATALOG) {
+			continue;
+		}
+		if (StringUtil::CIEquals(path.catalog, catalog)) {
+			return path.schema;
+		}
+	}
+	auto catalog_entry = Catalog::GetCatalogEntry(context, catalog);
+	if (catalog_entry) {
+		return catalog_entry->GetDefaultSchema();
+	}
+	return DEFAULT_SCHEMA;
+}
+
 string CatalogSearchPath::GetDefaultCatalog(const string &schema) {
 	if (DefaultSchemaGenerator::IsDefaultSchema(schema)) {
 		return SYSTEM_CATALOG;
diff --git a/src/common/enum_util.cpp b/src/common/enum_util.cpp
index 0ac5bb0c4e7f..49e7bb2b9b77 100644
--- a/src/common/enum_util.cpp
+++ b/src/common/enum_util.cpp
@@ -83,6 +83,7 @@
 #include "duckdb/common/types/vector_buffer.hpp"
 #include "duckdb/execution/index/art/art.hpp"
 #include "duckdb/execution/index/art/node.hpp"
+#include "duckdb/execution/index/bound_index.hpp"
 #include "duckdb/execution/operator/csv_scanner/csv_option.hpp"
 #include "duckdb/execution/operator/csv_scanner/csv_state.hpp"
 #include "duckdb/execution/operator/csv_scanner/quote_rules.hpp"
@@ -150,25 +151,6 @@
 
 namespace duckdb {
 
-const StringUtil::EnumStringLiteral *GetARTAppendModeValues() {
-	static constexpr StringUtil::EnumStringLiteral values[] {
-		{ static_cast<uint32_t>(ARTAppendMode::DEFAULT), "DEFAULT" },
-		{ static_cast<uint32_t>(ARTAppendMode::IGNORE_DUPLICATES), "IGNORE_DUPLICATES" },
-		{ static_cast<uint32_t>(ARTAppendMode::INSERT_DUPLICATES), "INSERT_DUPLICATES" }
-	};
-	return values;
-}
-
-template<>
-const char* EnumUtil::ToChars<ARTAppendMode>(ARTAppendMode value) {
-	return StringUtil::EnumToString(GetARTAppendModeValues(), 3, "ARTAppendMode", static_cast<uint32_t>(value));
-}
-
-template<>
-ARTAppendMode EnumUtil::FromString<ARTAppendMode>(const char *value) {
-	return static_cast<ARTAppendMode>(StringUtil::StringToEnum(GetARTAppendModeValues(), 3, "ARTAppendMode", value));
-}
-
 const StringUtil::EnumStringLiteral *GetARTConflictTypeValues() {
 	static constexpr StringUtil::EnumStringLiteral values[] {
 		{ static_cast<uint32_t>(ARTConflictType::NO_CONFLICT), "NO_CONFLICT" },
@@ -1878,6 +1860,25 @@ HLLStorageType EnumUtil::FromString<HLLStorageType>(const char *value) {
 	return static_cast<HLLStorageType>(StringUtil::StringToEnum(GetHLLStorageTypeValues(), 2, "HLLStorageType", value));
 }
 
+const StringUtil::EnumStringLiteral *GetIndexAppendModeValues() {
+	static constexpr StringUtil::EnumStringLiteral values[] {
+		{ static_cast<uint32_t>(IndexAppendMode::DEFAULT), "DEFAULT" },
+		{ static_cast<uint32_t>(IndexAppendMode::IGNORE_DUPLICATES), "IGNORE_DUPLICATES" },
+		{ static_cast<uint32_t>(IndexAppendMode::INSERT_DUPLICATES), "INSERT_DUPLICATES" }
+	};
+	return values;
+}
+
+template<>
+const char* EnumUtil::ToChars<IndexAppendMode>(IndexAppendMode value) {
+	return StringUtil::EnumToString(GetIndexAppendModeValues(), 3, "IndexAppendMode", static_cast<uint32_t>(value));
+}
+
+template<>
+IndexAppendMode EnumUtil::FromString<IndexAppendMode>(const char *value) {
+	return static_cast<IndexAppendMode>(StringUtil::StringToEnum(GetIndexAppendModeValues(), 3, "IndexAppendMode", value));
+}
+
 const StringUtil::EnumStringLiteral *GetIndexConstraintTypeValues() {
 	static constexpr StringUtil::EnumStringLiteral values[] {
 		{ static_cast<uint32_t>(IndexConstraintType::NONE), "NONE" },
diff --git a/src/common/random_engine.cpp b/src/common/random_engine.cpp
index cf558ea7ad87..78403e0301af 100644
--- a/src/common/random_engine.cpp
+++ b/src/common/random_engine.cpp
@@ -21,7 +21,10 @@ RandomEngine::RandomEngine(int64_t seed) : random_state(make_uniq<RandomState>()
 	if (seed < 0) {
 #ifdef __linux__
 		idx_t random_seed = 0;
-		auto result = syscall(SYS_getrandom, &random_seed, sizeof(random_seed), 0);
+		int result = -1;
+#if defined(SYS_getrandom)
+		result = static_cast<int>(syscall(SYS_getrandom, &random_seed, sizeof(random_seed), 0));
+#endif
 		if (result == -1) {
 			// Something went wrong with the syscall, we use chrono
 			const auto now = std::chrono::high_resolution_clock::now();
diff --git a/src/common/serializer/memory_stream.cpp b/src/common/serializer/memory_stream.cpp
index e5f0455e3ee4..2b3d0bb1bf4e 100644
--- a/src/common/serializer/memory_stream.cpp
+++ b/src/common/serializer/memory_stream.cpp
@@ -1,14 +1,12 @@
 #include "duckdb/common/serializer/memory_stream.hpp"
 
+#include "duckdb/common/allocator.hpp"
+
 namespace duckdb {
 
 MemoryStream::MemoryStream(idx_t capacity) : position(0), capacity(capacity), owns_data(true) {
 	D_ASSERT(capacity != 0 && IsPowerOfTwo(capacity));
-	auto data_malloc_result = malloc(capacity);
-	if (!data_malloc_result) {
-		throw std::bad_alloc();
-	}
-	data = static_cast<data_ptr_t>(data_malloc_result);
+	data = Allocator::DefaultAllocatorReference()->AllocateData(capacity);
 }
 
 MemoryStream::MemoryStream(data_ptr_t buffer, idx_t capacity)
@@ -17,7 +15,7 @@ MemoryStream::MemoryStream(data_ptr_t buffer, idx_t capacity)
 
 MemoryStream::~MemoryStream() {
 	if (owns_data) {
-		free(data);
+		Allocator::DefaultAllocatorReference()->FreeData(data, capacity);
 	}
 }
 
@@ -39,7 +37,7 @@ MemoryStream &MemoryStream::operator=(MemoryStream &&other) noexcept {
 	if (this != &other) {
 		// Free the current data
 		if (owns_data) {
-			free(data);
+			Allocator::DefaultAllocatorReference()->FreeData(data, capacity);
 		}
 
 		// Move the data from the other stream into this stream
@@ -58,14 +56,17 @@ MemoryStream &MemoryStream::operator=(MemoryStream &&other) noexcept {
 }
 
 void MemoryStream::WriteData(const_data_ptr_t source, idx_t write_size) {
+	const auto old_capacity = capacity;
 	while (position + write_size > capacity) {
 		if (owns_data) {
 			capacity *= 2;
-			data = static_cast<data_ptr_t>(realloc(data, capacity));
 		} else {
 			throw SerializationException("Failed to serialize: not enough space in buffer to fulfill write request");
 		}
 	}
+	if (capacity != old_capacity) {
+		data = Allocator::DefaultAllocatorReference()->ReallocateData(data, old_capacity, capacity);
+	}
 	memcpy(data + position, source, write_size);
 	position += write_size;
 }
diff --git a/src/execution/index/art/art.cpp b/src/execution/index/art/art.cpp
index 137cc98811a4..1317fb08edd2 100644
--- a/src/execution/index/art/art.cpp
+++ b/src/execution/index/art/art.cpp
@@ -45,7 +45,7 @@ ART::ART(const string &name, const IndexConstraintType index_constraint_type, co
          const shared_ptr<array<unsafe_unique_ptr<FixedSizeAllocator>, ALLOCATOR_COUNT>> &allocators_ptr,
          const IndexStorageInfo &info)
     : BoundIndex(name, ART::TYPE_NAME, index_constraint_type, column_ids, table_io_manager, unbound_expressions, db),
-      allocators(allocators_ptr), owns_data(false), append_mode(ARTAppendMode::DEFAULT) {
+      allocators(allocators_ptr), owns_data(false) {
 
 	// FIXME: Use the new byte representation function to support nested types.
 	for (idx_t i = 0; i < types.size(); i++) {
@@ -480,10 +480,11 @@ bool ART::Construct(unsafe_vector<ARTKey> &keys, unsafe_vector<ARTKey> &row_ids,
 //===--------------------------------------------------------------------===//
 
 ErrorData ART::Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids) {
-	return Insert(l, chunk, row_ids, nullptr);
+	IndexAppendInfo info;
+	return Insert(l, chunk, row_ids, info);
 }
 
-ErrorData ART::Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids, optional_ptr<BoundIndex> delete_index) {
+ErrorData ART::Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids, IndexAppendInfo &info) {
 	D_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);
 	auto row_count = chunk.size();
 
@@ -493,8 +494,8 @@ ErrorData ART::Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids, optional_
 	GenerateKeyVectors(allocator, chunk, row_ids, keys, row_id_keys);
 
 	optional_ptr<ART> delete_art;
-	if (delete_index) {
-		delete_art = delete_index->Cast<ART>();
+	if (info.delete_index) {
+		delete_art = info.delete_index->Cast<ART>();
 	}
 
 	auto conflict_type = ARTConflictType::NO_CONFLICT;
@@ -506,7 +507,7 @@ ErrorData ART::Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids, optional_
 		if (keys[i].Empty()) {
 			continue;
 		}
-		conflict_type = Insert(tree, keys[i], 0, row_id_keys[i], tree.GetGateStatus(), delete_art);
+		conflict_type = Insert(tree, keys[i], 0, row_id_keys[i], tree.GetGateStatus(), delete_art, info.append_mode);
 		if (conflict_type != ARTConflictType::NO_CONFLICT) {
 			conflict_idx = i;
 			break;
@@ -557,27 +558,27 @@ ErrorData ART::Append(IndexLock &l, DataChunk &chunk, Vector &row_ids) {
 	ExecuteExpressions(chunk, expr_chunk);
 
 	// Now insert the data chunk.
-	return Insert(l, expr_chunk, row_ids, nullptr);
+	IndexAppendInfo info;
+	return Insert(l, expr_chunk, row_ids, info);
 }
 
-ErrorData ART::AppendWithDeleteIndex(IndexLock &l, DataChunk &chunk, Vector &row_ids,
-                                     optional_ptr<BoundIndex> delete_index) {
+ErrorData ART::Append(IndexLock &l, DataChunk &chunk, Vector &row_ids, IndexAppendInfo &info) {
 	// Execute all column expressions before inserting the data chunk.
 	DataChunk expr_chunk;
 	expr_chunk.Initialize(Allocator::DefaultAllocator(), logical_types);
 	ExecuteExpressions(chunk, expr_chunk);
 
 	// Now insert the data chunk.
-	return Insert(l, expr_chunk, row_ids, delete_index);
+	return Insert(l, expr_chunk, row_ids, info);
 }
 
-void ART::VerifyAppend(DataChunk &chunk, optional_ptr<BoundIndex> delete_index, optional_ptr<ConflictManager> manager) {
+void ART::VerifyAppend(DataChunk &chunk, IndexAppendInfo &info, optional_ptr<ConflictManager> manager) {
 	if (manager) {
 		D_ASSERT(manager->LookupType() == VerifyExistenceType::APPEND);
-		return VerifyConstraint(chunk, delete_index, *manager);
+		return VerifyConstraint(chunk, info, *manager);
 	}
 	ConflictManager local_manager(VerifyExistenceType::APPEND, chunk.size());
-	VerifyConstraint(chunk, delete_index, local_manager);
+	VerifyConstraint(chunk, info, local_manager);
 }
 
 void ART::InsertIntoEmpty(Node &node, const ARTKey &key, const idx_t depth, const ARTKey &row_id,
@@ -598,15 +599,16 @@ void ART::InsertIntoEmpty(Node &node, const ARTKey &key, const idx_t depth, cons
 }
 
 ARTConflictType ART::InsertIntoInlined(Node &node, const ARTKey &key, const idx_t depth, const ARTKey &row_id,
-                                       const GateStatus status, optional_ptr<ART> delete_art) {
+                                       const GateStatus status, optional_ptr<ART> delete_art,
+                                       const IndexAppendMode append_mode) {
 
-	if (!IsUnique() || append_mode == ARTAppendMode::INSERT_DUPLICATES) {
+	if (!IsUnique() || append_mode == IndexAppendMode::INSERT_DUPLICATES) {
 		Leaf::InsertIntoInlined(*this, node, row_id, depth, status);
 		return ARTConflictType::NO_CONFLICT;
 	}
 
 	if (!delete_art) {
-		if (append_mode == ARTAppendMode::IGNORE_DUPLICATES) {
+		if (append_mode == IndexAppendMode::IGNORE_DUPLICATES) {
 			return ARTConflictType::NO_CONFLICT;
 		}
 		return ARTConflictType::CONSTRAINT;
@@ -633,14 +635,15 @@ ARTConflictType ART::InsertIntoInlined(Node &node, const ARTKey &key, const idx_
 }
 
 ARTConflictType ART::InsertIntoNode(Node &node, const ARTKey &key, const idx_t depth, const ARTKey &row_id,
-                                    const GateStatus status, optional_ptr<ART> delete_art) {
+                                    const GateStatus status, optional_ptr<ART> delete_art,
+                                    const IndexAppendMode append_mode) {
 	D_ASSERT(depth < key.len);
 	auto child = node.GetChildMutable(*this, key[depth]);
 
 	// Recurse, if a child exists at key[depth].
 	if (child) {
 		D_ASSERT(child->HasMetadata());
-		auto conflict_type = Insert(*child, key, depth + 1, row_id, status, delete_art);
+		auto conflict_type = Insert(*child, key, depth + 1, row_id, status, delete_art, append_mode);
 		node.ReplaceChild(*this, key[depth], *child);
 		return conflict_type;
 	}
@@ -649,7 +652,7 @@ ARTConflictType ART::InsertIntoNode(Node &node, const ARTKey &key, const idx_t d
 	if (status == GateStatus::GATE_SET) {
 		Node remainder;
 		auto byte = key[depth];
-		auto conflict_type = Insert(remainder, key, depth + 1, row_id, status, delete_art);
+		auto conflict_type = Insert(remainder, key, depth + 1, row_id, status, delete_art, append_mode);
 		Node::InsertChild(*this, node, byte, remainder);
 		return conflict_type;
 	}
@@ -671,7 +674,7 @@ ARTConflictType ART::InsertIntoNode(Node &node, const ARTKey &key, const idx_t d
 }
 
 ARTConflictType ART::Insert(Node &node, const ARTKey &key, idx_t depth, const ARTKey &row_id, const GateStatus status,
-                            optional_ptr<ART> delete_art) {
+                            optional_ptr<ART> delete_art, const IndexAppendMode append_mode) {
 	if (!node.HasMetadata()) {
 		InsertIntoEmpty(node, key, depth, row_id, status);
 		return ARTConflictType::NO_CONFLICT;
@@ -688,17 +691,17 @@ ARTConflictType ART::Insert(Node &node, const ARTKey &key, idx_t depth, const AR
 			// incoming transaction must fail here.
 			return ARTConflictType::TRANSACTION;
 		}
-		return Insert(node, row_id, 0, row_id, GateStatus::GATE_SET, delete_art);
+		return Insert(node, row_id, 0, row_id, GateStatus::GATE_SET, delete_art, append_mode);
 	}
 
 	auto type = node.GetType();
 	switch (type) {
 	case NType::LEAF_INLINED: {
-		return InsertIntoInlined(node, key, depth, row_id, status, delete_art);
+		return InsertIntoInlined(node, key, depth, row_id, status, delete_art, append_mode);
 	}
 	case NType::LEAF: {
 		Leaf::TransformToNested(*this, node);
-		return Insert(node, key, depth, row_id, status, delete_art);
+		return Insert(node, key, depth, row_id, status, delete_art, append_mode);
 	}
 	case NType::NODE_7_LEAF:
 	case NType::NODE_15_LEAF:
@@ -712,9 +715,9 @@ ARTConflictType ART::Insert(Node &node, const ARTKey &key, idx_t depth, const AR
 	case NType::NODE_16:
 	case NType::NODE_48:
 	case NType::NODE_256:
-		return InsertIntoNode(node, key, depth, row_id, status, delete_art);
+		return InsertIntoNode(node, key, depth, row_id, status, delete_art, append_mode);
 	case NType::PREFIX:
-		return Prefix::Insert(*this, node, key, depth, row_id, status, delete_art);
+		return Prefix::Insert(*this, node, key, depth, row_id, status, delete_art, append_mode);
 	default:
 		throw InternalException("Invalid node type for ART::Insert.");
 	}
@@ -1119,7 +1122,7 @@ void ART::VerifyLeaf(const Node &leaf, const ARTKey &key, optional_ptr<ART> dele
 	}
 }
 
-void ART::VerifyConstraint(DataChunk &chunk, optional_ptr<BoundIndex> delete_index, ConflictManager &manager) {
+void ART::VerifyConstraint(DataChunk &chunk, IndexAppendInfo &info, ConflictManager &manager) {
 	// Lock the index during constraint checking.
 	lock_guard<mutex> l(lock);
 
@@ -1132,8 +1135,8 @@ void ART::VerifyConstraint(DataChunk &chunk, optional_ptr<BoundIndex> delete_ind
 	GenerateKeys<>(arena_allocator, expr_chunk, keys);
 
 	optional_ptr<ART> delete_art;
-	if (delete_index) {
-		delete_art = delete_index->Cast<ART>();
+	if (info.delete_index) {
+		delete_art = info.delete_index->Cast<ART>();
 	}
 
 	optional_idx conflict_idx;
diff --git a/src/execution/index/art/leaf.cpp b/src/execution/index/art/leaf.cpp
index 5cde0d5d042a..f5eadf178e9c 100644
--- a/src/execution/index/art/leaf.cpp
+++ b/src/execution/index/art/leaf.cpp
@@ -26,7 +26,7 @@ void Leaf::New(ART &art, reference<Node> &node, const unsafe_vector<ARTKey> &row
 	// We cannot recurse into the leaf during Construct(...) because row IDs are not sorted.
 	for (idx_t i = 0; i < count; i++) {
 		idx_t offset = start + i;
-		art.Insert(node, row_ids[offset], 0, row_ids[offset], GateStatus::GATE_SET, nullptr);
+		art.Insert(node, row_ids[offset], 0, row_ids[offset], GateStatus::GATE_SET, nullptr, IndexAppendMode::DEFAULT);
 	}
 	node.get().SetGateStatus(GateStatus::GATE_SET);
 }
@@ -36,7 +36,7 @@ void Leaf::MergeInlined(ART &art, Node &l_node, Node &r_node) {
 
 	ArenaAllocator arena_allocator(Allocator::Get(art.db));
 	auto key = ARTKey::CreateARTKey<row_t>(arena_allocator, r_node.GetRowId());
-	art.Insert(l_node, key, 0, key, l_node.GetGateStatus(), nullptr);
+	art.Insert(l_node, key, 0, key, l_node.GetGateStatus(), nullptr, IndexAppendMode::DEFAULT);
 	r_node.Clear();
 }
 
@@ -96,18 +96,14 @@ void Leaf::TransformToNested(ART &art, Node &node) {
 	ArenaAllocator allocator(Allocator::Get(art.db));
 	Node root = Node();
 
-	// Temporarily disable constraint checking.
-	if (art.IsUnique() && art.append_mode == ARTAppendMode::DEFAULT) {
-		art.append_mode = ARTAppendMode::INSERT_DUPLICATES;
-	}
-
 	// Move all row IDs into the nested leaf.
 	reference<const Node> leaf_ref(node);
 	while (leaf_ref.get().HasMetadata()) {
 		auto &leaf = Node::Ref<const Leaf>(art, leaf_ref, LEAF);
 		for (uint8_t i = 0; i < leaf.count; i++) {
 			auto row_id = ARTKey::CreateARTKey<row_t>(allocator, leaf.row_ids[i]);
-			auto conflict_type = art.Insert(root, row_id, 0, row_id, GateStatus::GATE_SET, nullptr);
+			auto conflict_type =
+			    art.Insert(root, row_id, 0, row_id, GateStatus::GATE_SET, nullptr, IndexAppendMode::INSERT_DUPLICATES);
 			if (conflict_type != ARTConflictType::NO_CONFLICT) {
 				throw InternalException("invalid conflict type in Leaf::TransformToNested");
 			}
@@ -115,7 +111,6 @@ void Leaf::TransformToNested(ART &art, Node &node) {
 		leaf_ref = leaf.ptr;
 	}
 
-	art.append_mode = ARTAppendMode::DEFAULT;
 	root.SetGateStatus(GateStatus::GATE_SET);
 	Node::Free(art, node);
 	node = root;
diff --git a/src/execution/index/art/node.cpp b/src/execution/index/art/node.cpp
index 25c1dd5ff6da..1a85ad921fd5 100644
--- a/src/execution/index/art/node.cpp
+++ b/src/execution/index/art/node.cpp
@@ -572,7 +572,7 @@ bool Node::MergeInternal(ART &art, Node &other, const GateStatus status) {
 		ArenaAllocator allocator(Allocator::Get(art.db));
 		for (idx_t i = 0; i < row_ids.size(); i++) {
 			auto row_id = ARTKey::CreateARTKey<row_t>(allocator, row_ids[i]);
-			art.Insert(*this, row_id, 0, row_id, GateStatus::GATE_SET, nullptr);
+			art.Insert(*this, row_id, 0, row_id, GateStatus::GATE_SET, nullptr, IndexAppendMode::DEFAULT);
 		}
 		return true;
 	}
diff --git a/src/execution/index/art/prefix.cpp b/src/execution/index/art/prefix.cpp
index f0821be040c3..551171819d16 100644
--- a/src/execution/index/art/prefix.cpp
+++ b/src/execution/index/art/prefix.cpp
@@ -292,7 +292,8 @@ GateStatus Prefix::Split(ART &art, reference<Node> &node, Node &child, const uin
 }
 
 ARTConflictType Prefix::Insert(ART &art, Node &node, const ARTKey &key, idx_t depth, const ARTKey &row_id,
-                               const GateStatus status, optional_ptr<ART> delete_art) {
+                               const GateStatus status, optional_ptr<ART> delete_art,
+                               const IndexAppendMode append_mode) {
 	reference<Node> next(node);
 	auto pos = TraverseMutable(art, next, key, depth);
 
@@ -301,7 +302,7 @@ ARTConflictType Prefix::Insert(ART &art, Node &node, const ARTKey &key, idx_t de
 	// (2) we reach a gate.
 	if (pos == DConstants::INVALID_INDEX) {
 		if (next.get().GetType() != NType::PREFIX || next.get().GetGateStatus() == GateStatus::GATE_SET) {
-			return art.Insert(next, key, depth, row_id, status, delete_art);
+			return art.Insert(next, key, depth, row_id, status, delete_art, append_mode);
 		}
 	}
 
diff --git a/src/execution/index/bound_index.cpp b/src/execution/index/bound_index.cpp
index 199cc4bddb49..26933680adc3 100644
--- a/src/execution/index/bound_index.cpp
+++ b/src/execution/index/bound_index.cpp
@@ -38,24 +38,22 @@ ErrorData BoundIndex::Append(DataChunk &chunk, Vector &row_ids) {
 	return Append(l, chunk, row_ids);
 }
 
-ErrorData BoundIndex::AppendWithDeleteIndex(IndexLock &l, DataChunk &chunk, Vector &row_ids,
-                                            optional_ptr<BoundIndex> delete_index) {
+ErrorData BoundIndex::Append(IndexLock &l, DataChunk &chunk, Vector &row_ids, IndexAppendInfo &info) {
 	// Fallback to the old Append.
 	return Append(l, chunk, row_ids);
 }
 
-ErrorData BoundIndex::AppendWithDeleteIndex(DataChunk &chunk, Vector &row_ids, optional_ptr<BoundIndex> delete_index) {
+ErrorData BoundIndex::Append(DataChunk &chunk, Vector &row_ids, IndexAppendInfo &info) {
 	IndexLock l;
 	InitializeLock(l);
-	return AppendWithDeleteIndex(l, chunk, row_ids, delete_index);
+	return Append(l, chunk, row_ids, info);
 }
 
-void BoundIndex::VerifyAppend(DataChunk &chunk, optional_ptr<BoundIndex> delete_index,
-                              optional_ptr<ConflictManager> manager) {
+void BoundIndex::VerifyAppend(DataChunk &chunk, IndexAppendInfo &info, optional_ptr<ConflictManager> manager) {
 	throw NotImplementedException("this implementation of VerifyAppend does not exist.");
 }
 
-void BoundIndex::VerifyConstraint(DataChunk &chunk, optional_ptr<BoundIndex> delete_index, ConflictManager &manager) {
+void BoundIndex::VerifyConstraint(DataChunk &chunk, IndexAppendInfo &info, ConflictManager &manager) {
 	throw NotImplementedException("this implementation of VerifyConstraint does not exist.");
 }
 
@@ -71,7 +69,7 @@ void BoundIndex::Delete(DataChunk &entries, Vector &row_identifiers) {
 	Delete(state, entries, row_identifiers);
 }
 
-ErrorData BoundIndex::Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids, optional_ptr<BoundIndex> delete_index) {
+ErrorData BoundIndex::Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids, IndexAppendInfo &info) {
 	throw NotImplementedException("this implementation of Insert does not exist.");
 }
 
diff --git a/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp b/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp
index c922f7294b2a..94ef37399510 100644
--- a/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp
+++ b/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp
@@ -126,6 +126,10 @@ StringValueResult::StringValueResult(CSVStates &states, CSVStateMachine &state_m
 			SkipBOM();
 		}
 	}
+	ignore_empty_values = state_machine.dialect_options.state_machine_options.delimiter.GetValue()[0] != ' ' &&
+	                      state_machine.dialect_options.state_machine_options.quote != ' ' &&
+	                      state_machine.dialect_options.state_machine_options.escape != ' ' &&
+	                      state_machine.dialect_options.state_machine_options.comment != ' ';
 }
 
 StringValueResult::~StringValueResult() {
@@ -503,8 +507,15 @@ void StringValueResult::AddQuotedValue(StringValueResult &result, const idx_t bu
 	if (!result.unquoted) {
 		result.current_errors.Insert(UNTERMINATED_QUOTES, result.cur_col_id, result.chunk_col_id, result.last_position);
 	}
-	AddPossiblyEscapedValue(result, buffer_pos, result.buffer_ptr + result.quoted_position + 1,
-	                        buffer_pos - result.quoted_position - 2, buffer_pos < result.last_position.buffer_pos + 2);
+	// remove potential empty values
+	idx_t length = buffer_pos - result.quoted_position - 1;
+	while (length > 0 && result.ignore_empty_values &&
+	       result.buffer_ptr[result.quoted_position + 1 + length - 1] == ' ') {
+		length--;
+	}
+	length--;
+	AddPossiblyEscapedValue(result, buffer_pos, result.buffer_ptr + result.quoted_position + 1, length,
+	                        buffer_pos < result.last_position.buffer_pos + 2);
 	result.quoted = false;
 }
 
@@ -1365,8 +1376,12 @@ void StringValueScanner::ProcessOverBufferValue() {
 	if (!skip_value) {
 		string_t value;
 		if (result.quoted && !result.comment) {
-			value = string_t(over_buffer_string.c_str() + result.quoted_position,
-			                 UnsafeNumericCast<uint32_t>(over_buffer_string.size() - 1 - result.quoted_position));
+			idx_t length = over_buffer_string.size() - 1 - result.quoted_position;
+			while (length > 0 && result.ignore_empty_values &&
+			       over_buffer_string.c_str()[result.quoted_position + length] == ' ') {
+				length--;
+			}
+			value = string_t(over_buffer_string.c_str() + result.quoted_position, UnsafeNumericCast<uint32_t>(length));
 			if (result.escaped) {
 				if (!result.HandleTooManyColumnsError(over_buffer_string.c_str(), over_buffer_string.size())) {
 					const auto str_ptr = over_buffer_string.c_str() + result.quoted_position;
diff --git a/src/execution/operator/persistent/physical_copy_database.cpp b/src/execution/operator/persistent/physical_copy_database.cpp
index 6ad354ea2edc..3479f7887ce5 100644
--- a/src/execution/operator/persistent/physical_copy_database.cpp
+++ b/src/execution/operator/persistent/physical_copy_database.cpp
@@ -1,6 +1,8 @@
 #include "duckdb/execution/operator/persistent/physical_copy_database.hpp"
+
 #include "duckdb/catalog/catalog.hpp"
 #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
+#include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
 #include "duckdb/planner/binder.hpp"
 #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
 #include "duckdb/parser/parsed_data/create_schema_info.hpp"
@@ -9,6 +11,8 @@
 #include "duckdb/parser/parsed_data/create_type_info.hpp"
 #include "duckdb/parser/parsed_data/create_view_info.hpp"
 #include "duckdb/parser/parsed_data/create_index_info.hpp"
+#include "duckdb/execution/index/unbound_index.hpp"
+#include "duckdb/storage/data_table.hpp"
 
 namespace duckdb {
 
@@ -52,7 +56,7 @@ SourceResultType PhysicalCopyDatabase::GetData(ExecutionContext &context, DataCh
 			break;
 		}
 		case CatalogType::INDEX_ENTRY: {
-			catalog.CreateIndex(context.client, create_info->Cast<CreateIndexInfo>());
+			// Skip for now.
 			break;
 		}
 		default:
@@ -60,6 +64,27 @@ SourceResultType PhysicalCopyDatabase::GetData(ExecutionContext &context, DataCh
 			                              CatalogTypeToString(create_info->type));
 		}
 	}
+
+	// Create the indexes after table creation.
+	for (auto &create_info : info->entries) {
+		if (!create_info || create_info->type != CatalogType::INDEX_ENTRY) {
+			continue;
+		}
+		catalog.CreateIndex(context.client, create_info->Cast<CreateIndexInfo>());
+
+		auto &create_index_info = create_info->Cast<CreateIndexInfo>();
+		auto &catalog_table = catalog.GetEntry(context.client, CatalogType::TABLE_ENTRY, create_index_info.schema,
+		                                       create_index_info.table);
+		auto &table_entry = catalog_table.Cast<TableCatalogEntry>();
+		auto &data_table = table_entry.GetStorage();
+
+		IndexStorageInfo storage_info(create_index_info.index_name);
+		storage_info.options.emplace("v1_0_0_storage", false);
+		auto unbound_index = make_uniq<UnboundIndex>(create_index_info.Copy(), storage_info,
+		                                             data_table.GetTableIOManager(), catalog.GetAttached());
+		data_table.AddIndex(std::move(unbound_index));
+	}
+
 	return SourceResultType::FINISHED;
 }
 
diff --git a/src/execution/operator/persistent/physical_delete.cpp b/src/execution/operator/persistent/physical_delete.cpp
index 7d494ac0c18f..abb705aa01be 100644
--- a/src/execution/operator/persistent/physical_delete.cpp
+++ b/src/execution/operator/persistent/physical_delete.cpp
@@ -133,12 +133,13 @@ SinkResultType PhysicalDelete::Sink(ExecutionContext &context, DataChunk &chunk,
 	if (g_state.has_unique_indexes && l_state.delete_chunk.size() != 0) {
 		auto &local_storage = LocalStorage::Get(context.client, table.db);
 		auto storage = local_storage.GetStorage(table);
+		IndexAppendInfo index_append_info(IndexAppendMode::IGNORE_DUPLICATES, nullptr);
 		storage->delete_indexes.Scan([&](Index &index) {
 			if (!index.IsBound() || !index.IsUnique()) {
 				return false;
 			}
 			auto &bound_index = index.Cast<BoundIndex>();
-			auto error = bound_index.Append(l_state.delete_chunk, row_ids);
+			auto error = bound_index.Append(l_state.delete_chunk, row_ids, index_append_info);
 			if (error.HasError()) {
 				throw InternalException("failed to update delete ART in physical delete: ", error.Message());
 			}
diff --git a/src/execution/operator/persistent/physical_insert.cpp b/src/execution/operator/persistent/physical_insert.cpp
index cb21cc26b91c..a3c6619ed5e9 100644
--- a/src/execution/operator/persistent/physical_insert.cpp
+++ b/src/execution/operator/persistent/physical_insert.cpp
@@ -80,17 +80,15 @@ InsertGlobalState::InsertGlobalState(ClientContext &context, const vector<Logica
     : table(table), insert_count(0), initialized(false), return_collection(context, return_types) {
 }
 
-InsertLocalState::InsertLocalState(ClientContext &context, const vector<LogicalType> &types_p,
+InsertLocalState::InsertLocalState(ClientContext &context, const vector<LogicalType> &types,
                                    const vector<unique_ptr<Expression>> &bound_defaults,
                                    const vector<unique_ptr<BoundConstraint>> &bound_constraints)
     : default_executor(context, bound_defaults), bound_constraints(bound_constraints) {
 
 	auto &allocator = Allocator::Get(context);
-
-	types = types_p;
-	auto initialize = vector<bool>(types.size(), false);
-	update_chunk.Initialize(allocator, types, initialize);
-	append_chunk.Initialize(allocator, types, initialize);
+	insert_chunk.Initialize(allocator, types);
+	update_chunk.Initialize(allocator, types);
+	append_chunk.Initialize(allocator, types);
 }
 
 ConstraintState &InsertLocalState::GetConstraintState(DataTable &table, TableCatalogEntry &table_ref) {
@@ -187,10 +185,8 @@ static void CombineExistingAndInsertTuples(DataChunk &result, DataChunk &scan_ch
 	auto &insert_types = op.insert_types;
 
 	if (types_to_fetch.empty()) {
-		// We have not scanned the initial table, so we duplicate the initial chunk.
-		const auto &types = input_chunk.GetTypes();
-		auto initialize = vector<bool>(types.size(), false);
-		result.Initialize(client, types, initialize, input_chunk.size());
+		// We have not scanned the initial table, so we can just duplicate the initial chunk
+		result.Initialize(client, input_chunk.GetTypes());
 		result.Reference(input_chunk);
 		result.SetCardinality(input_chunk);
 		return;
@@ -200,7 +196,7 @@ static void CombineExistingAndInsertTuples(DataChunk &result, DataChunk &scan_ch
 	combined_types.insert(combined_types.end(), insert_types.begin(), insert_types.end());
 	combined_types.insert(combined_types.end(), types_to_fetch.begin(), types_to_fetch.end());
 
-	result.Initialize(client, combined_types, input_chunk.size());
+	result.Initialize(client, combined_types);
 	result.Reset();
 	// Add the VALUES list
 	for (idx_t i = 0; i < insert_types.size(); i++) {
@@ -227,8 +223,8 @@ static void CombineExistingAndInsertTuples(DataChunk &result, DataChunk &scan_ch
 	result.SetCardinality(input_chunk.size());
 }
 
-static void CreateUpdateChunk(ExecutionContext &context, DataChunk &chunk, Vector &row_ids, DataChunk &update_chunk,
-                              const PhysicalInsert &op) {
+static void CreateUpdateChunk(ExecutionContext &context, DataChunk &chunk, TableCatalogEntry &table, Vector &row_ids,
+                              DataChunk &update_chunk, const PhysicalInsert &op) {
 
 	auto &do_update_condition = op.do_update_condition;
 	auto &set_types = op.set_types;
@@ -284,7 +280,7 @@ static idx_t PerformOnConflictAction(InsertLocalState &lstate, ExecutionContext
 
 	auto &set_columns = op.set_columns;
 	DataChunk update_chunk;
-	CreateUpdateChunk(context, chunk, row_ids, update_chunk, op);
+	CreateUpdateChunk(context, chunk, table, row_ids, update_chunk, op);
 	auto &data_table = table.GetStorage();
 
 	// Perform the UPDATE on the (global) storage.
@@ -488,9 +484,7 @@ static idx_t HandleInsertConflicts(TableCatalogEntry &table, ExecutionContext &c
 	DataChunk combined_chunk; // contains conflict_chunk + scan_chunk (wide)
 
 	// Filter out everything but the conflicting rows
-	const auto &types = tuples.GetTypes();
-	auto initialize = vector<bool>(types.size(), false);
-	conflict_chunk.Initialize(context.client, types, initialize, tuples.size());
+	conflict_chunk.Initialize(context.client, tuples.GetTypes());
 	conflict_chunk.Reference(tuples);
 	conflict_chunk.Slice(conflicts.Selection(), conflicts.Count());
 	conflict_chunk.SetCardinality(conflicts.Count());
@@ -501,7 +495,7 @@ static idx_t HandleInsertConflicts(TableCatalogEntry &table, ExecutionContext &c
 		D_ASSERT(scan_chunk.size() == 0);
 		// When these values are required for the conditions or the SET expressions,
 		// then we scan the existing table for the conflicting tuples, using the rowids
-		scan_chunk.Initialize(context.client, types_to_fetch, conflicts.Count());
+		scan_chunk.Initialize(context.client, types_to_fetch);
 		fetch_state = make_uniq<ColumnFetchState>();
 		if (GLOBAL) {
 			auto &transaction = DuckTransaction::Get(context.client, table.catalog);
@@ -534,7 +528,7 @@ static idx_t HandleInsertConflicts(TableCatalogEntry &table, ExecutionContext &c
 	return affected_tuples;
 }
 
-idx_t PhysicalInsert::OnConflictHandling(TableCatalogEntry &table, ExecutionContext &context,
+idx_t PhysicalInsert::OnConflictHandling(TableCatalogEntry &table, ExecutionContext &context, InsertGlobalState &gstate,
                                          InsertLocalState &lstate) const {
 	auto &data_table = table.GetStorage();
 	auto &local_storage = LocalStorage::Get(context.client, data_table.db);
@@ -634,21 +628,6 @@ SinkResultType PhysicalInsert::Sink(ExecutionContext &context, DataChunk &chunk,
 
 	auto &table = gstate.table;
 	auto &storage = table.GetStorage();
-	if (lstate.init_insert_chunk) {
-		auto initialize = vector<bool>(lstate.types.size(), false);
-		if (!column_index_map.empty()) {
-			for (auto &col : table.GetColumns().Physical()) {
-				auto storage_idx = col.StorageOid();
-				auto mapped_index = column_index_map[col.Physical()];
-				if (mapped_index == DConstants::INVALID_INDEX) {
-					initialize[storage_idx] = true;
-				}
-			}
-		}
-		auto &allocator = Allocator::Get(context.client);
-		lstate.insert_chunk.Initialize(allocator, lstate.types, initialize, chunk.size());
-		lstate.init_insert_chunk = false;
-	}
 	PhysicalInsert::ResolveDefaults(table, chunk, column_index_map, lstate.default_executor, lstate.insert_chunk);
 
 	if (!parallel) {
@@ -663,7 +642,7 @@ SinkResultType PhysicalInsert::Sink(ExecutionContext &context, DataChunk &chunk,
 			// so it should not be added to the RETURNING chunk
 			gstate.return_collection.Append(lstate.insert_chunk);
 		}
-		idx_t updated_tuples = OnConflictHandling(table, context, lstate);
+		idx_t updated_tuples = OnConflictHandling(table, context, gstate, lstate);
 		if (action_type == OnConflictAction::NOTHING && return_chunk) {
 			// Because we didn't add to the RETURNING chunk yet
 			// we add the tuples that did not get filtered out now
@@ -694,7 +673,7 @@ SinkResultType PhysicalInsert::Sink(ExecutionContext &context, DataChunk &chunk,
 			lstate.local_collection->InitializeAppend(lstate.local_append_state);
 			lstate.writer = &gstate.table.GetStorage().CreateOptimisticWriter(context.client);
 		}
-		OnConflictHandling(table, context, lstate);
+		OnConflictHandling(table, context, gstate, lstate);
 		D_ASSERT(action_type != OnConflictAction::UPDATE);
 
 		auto new_row_group = lstate.local_collection->Append(lstate.insert_chunk, lstate.local_append_state);
diff --git a/src/execution/operator/schema/physical_create_art_index.cpp b/src/execution/operator/schema/physical_create_art_index.cpp
index e34e49de475e..8f9b2657e0a8 100644
--- a/src/execution/operator/schema/physical_create_art_index.cpp
+++ b/src/execution/operator/schema/physical_create_art_index.cpp
@@ -88,7 +88,8 @@ SinkResultType PhysicalCreateARTIndex::SinkUnsorted(OperatorSinkInput &input) co
 	// Insert each key and its corresponding row ID.
 	for (idx_t i = 0; i < row_count; i++) {
 		auto status = art.tree.GetGateStatus();
-		auto conflict_type = art.Insert(art.tree, l_state.keys[i], 0, l_state.row_ids[i], status, nullptr);
+		auto conflict_type =
+		    art.Insert(art.tree, l_state.keys[i], 0, l_state.row_ids[i], status, nullptr, IndexAppendMode::DEFAULT);
 		D_ASSERT(conflict_type != ARTConflictType::TRANSACTION);
 		if (conflict_type == ARTConflictType::CONSTRAINT) {
 			throw ConstraintException("Data contains duplicates on indexed column(s)");
diff --git a/src/execution/sample/reservoir_sample.cpp b/src/execution/sample/reservoir_sample.cpp
index 334b613d1d0a..58056920eaf8 100644
--- a/src/execution/sample/reservoir_sample.cpp
+++ b/src/execution/sample/reservoir_sample.cpp
@@ -749,6 +749,7 @@ void ReservoirSample::AddToReservoir(DataChunk &chunk) {
 
 	if (chunk_sel.size == 0) {
 		// not adding any samples
+		base_reservoir_sample->num_entries_seen_total += chunk.size();
 		return;
 	}
 	idx_t size = chunk_sel.size;
diff --git a/src/include/duckdb/catalog/catalog.hpp b/src/include/duckdb/catalog/catalog.hpp
index 121b047cb781..96679f2a9153 100644
--- a/src/include/duckdb/catalog/catalog.hpp
+++ b/src/include/duckdb/catalog/catalog.hpp
@@ -315,7 +315,11 @@ class Catalog {
 		return CatalogLookupBehavior::STANDARD;
 	}
 
+	//! Returns the default schema of the catalog
+	virtual string GetDefaultSchema() const;
+
 	//! The default table is used for `SELECT * FROM <catalog_name>;`
+	//! FIXME: these should be virtual methods
 	DUCKDB_API bool HasDefaultTable() const;
 	DUCKDB_API void SetDefaultTable(const string &schema, const string &name);
 	DUCKDB_API string GetDefaultTable() const;
diff --git a/src/include/duckdb/catalog/catalog_search_path.hpp b/src/include/duckdb/catalog/catalog_search_path.hpp
index ff0def0fcd30..f128a4962e75 100644
--- a/src/include/duckdb/catalog/catalog_search_path.hpp
+++ b/src/include/duckdb/catalog/catalog_search_path.hpp
@@ -53,7 +53,9 @@ class CatalogSearchPath {
 		return set_paths;
 	}
 	DUCKDB_API const CatalogSearchEntry &GetDefault();
+	//! FIXME: this method is deprecated
 	DUCKDB_API string GetDefaultSchema(const string &catalog);
+	DUCKDB_API string GetDefaultSchema(ClientContext &context, const string &catalog);
 	DUCKDB_API string GetDefaultCatalog(const string &schema);
 
 	DUCKDB_API vector<string> GetSchemasForCatalog(const string &catalog);
diff --git a/src/include/duckdb/common/enum_util.hpp b/src/include/duckdb/common/enum_util.hpp
index accdea5d42eb..379a23969c55 100644
--- a/src/include/duckdb/common/enum_util.hpp
+++ b/src/include/duckdb/common/enum_util.hpp
@@ -32,8 +32,6 @@ struct EnumUtil {
     static string ToString(T value) { return string(ToChars<T>(value)); }
 };
 
-enum class ARTAppendMode : uint8_t;
-
 enum class ARTConflictType : uint8_t;
 
 enum class AccessMode : uint8_t;
@@ -188,6 +186,8 @@ enum class GateStatus : uint8_t;
 
 enum class HLLStorageType : uint8_t;
 
+enum class IndexAppendMode : uint8_t;
+
 enum class IndexConstraintType : uint8_t;
 
 enum class InsertColumnOrder : uint8_t;
@@ -391,9 +391,6 @@ enum class WindowBoundary : uint8_t;
 enum class WindowExcludeMode : uint8_t;
 
 
-template<>
-const char* EnumUtil::ToChars<ARTAppendMode>(ARTAppendMode value);
-
 template<>
 const char* EnumUtil::ToChars<ARTConflictType>(ARTConflictType value);
 
@@ -625,6 +622,9 @@ const char* EnumUtil::ToChars<GateStatus>(GateStatus value);
 template<>
 const char* EnumUtil::ToChars<HLLStorageType>(HLLStorageType value);
 
+template<>
+const char* EnumUtil::ToChars<IndexAppendMode>(IndexAppendMode value);
+
 template<>
 const char* EnumUtil::ToChars<IndexConstraintType>(IndexConstraintType value);
 
@@ -929,9 +929,6 @@ template<>
 const char* EnumUtil::ToChars<WindowExcludeMode>(WindowExcludeMode value);
 
 
-template<>
-ARTAppendMode EnumUtil::FromString<ARTAppendMode>(const char *value);
-
 template<>
 ARTConflictType EnumUtil::FromString<ARTConflictType>(const char *value);
 
@@ -1163,6 +1160,9 @@ GateStatus EnumUtil::FromString<GateStatus>(const char *value);
 template<>
 HLLStorageType EnumUtil::FromString<HLLStorageType>(const char *value);
 
+template<>
+IndexAppendMode EnumUtil::FromString<IndexAppendMode>(const char *value);
+
 template<>
 IndexConstraintType EnumUtil::FromString<IndexConstraintType>(const char *value);
 
diff --git a/src/include/duckdb/execution/index/art/art.hpp b/src/include/duckdb/execution/index/art/art.hpp
index 31a560ad5842..d6f41c1c4d92 100644
--- a/src/include/duckdb/execution/index/art/art.hpp
+++ b/src/include/duckdb/execution/index/art/art.hpp
@@ -16,7 +16,6 @@ namespace duckdb {
 
 enum class VerifyExistenceType : uint8_t { APPEND = 0, APPEND_FK = 1, DELETE_FK = 2 };
 enum class ARTConflictType : uint8_t { NO_CONFLICT = 0, CONSTRAINT = 1, TRANSACTION = 2 };
-enum class ARTAppendMode : uint8_t { DEFAULT = 0, IGNORE_DUPLICATES = 1, INSERT_DUPLICATES = 2 };
 
 class ConflictManager;
 class ARTKey;
@@ -62,8 +61,6 @@ class ART : public BoundIndex {
 	bool owns_data;
 	//! The number of bytes fitting in the prefix.
 	uint8_t prefix_count;
-	//! The append mode.
-	ARTAppendMode append_mode;
 
 public:
 	//! Try to initialize a scan on the ART with the given expression and filter.
@@ -74,21 +71,19 @@ class ART : public BoundIndex {
 
 	//! Appends data to the locked index.
 	ErrorData Append(IndexLock &l, DataChunk &chunk, Vector &row_ids) override;
-	//! Appends data to the locked index and verifies constraint violations against a delete index.
-	ErrorData AppendWithDeleteIndex(IndexLock &l, DataChunk &chunk, Vector &row_ids,
-	                                optional_ptr<BoundIndex> delete_index) override;
+	//! Appends data to the locked index and verifies constraint violations.
+	ErrorData Append(IndexLock &l, DataChunk &chunk, Vector &row_ids, IndexAppendInfo &info) override;
 
 	//! Internally inserts a chunk.
 	ARTConflictType Insert(Node &node, const ARTKey &key, idx_t depth, const ARTKey &row_id, const GateStatus status,
-	                       optional_ptr<ART> delete_art);
+	                       optional_ptr<ART> delete_art, const IndexAppendMode append_mode);
 	//! Insert a chunk.
 	ErrorData Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids) override;
-	//! Insert a chunk and verifies constraint violations against a delete index.
-	ErrorData Insert(IndexLock &l, DataChunk &data, Vector &row_ids, optional_ptr<BoundIndex> delete_index) override;
+	//! Insert a chunk and verifies constraint violations.
+	ErrorData Insert(IndexLock &l, DataChunk &data, Vector &row_ids, IndexAppendInfo &info) override;
 
 	//! Verify that data can be appended to the index without a constraint violation.
-	void VerifyAppend(DataChunk &chunk, optional_ptr<BoundIndex> delete_index,
-	                  optional_ptr<ConflictManager> manager) override;
+	void VerifyAppend(DataChunk &chunk, IndexAppendInfo &info, optional_ptr<ConflictManager> manager) override;
 
 	//! Delete a chunk from the ART.
 	void Delete(IndexLock &lock, DataChunk &entries, Vector &row_ids) override;
@@ -131,15 +126,17 @@ class ART : public BoundIndex {
 	void InsertIntoEmpty(Node &node, const ARTKey &key, const idx_t depth, const ARTKey &row_id,
 	                     const GateStatus status);
 	ARTConflictType InsertIntoInlined(Node &node, const ARTKey &key, const idx_t depth, const ARTKey &row_id,
-	                                  const GateStatus status, optional_ptr<ART> delete_art);
+	                                  const GateStatus status, optional_ptr<ART> delete_art,
+	                                  const IndexAppendMode append_mode);
 	ARTConflictType InsertIntoNode(Node &node, const ARTKey &key, const idx_t depth, const ARTKey &row_id,
-	                               const GateStatus status, optional_ptr<ART> delete_art);
+	                               const GateStatus status, optional_ptr<ART> delete_art,
+	                               const IndexAppendMode append_mode);
 
 	string GenerateErrorKeyName(DataChunk &input, idx_t row);
 	string GenerateConstraintErrorMessage(VerifyExistenceType verify_type, const string &key_name);
 	void VerifyLeaf(const Node &leaf, const ARTKey &key, optional_ptr<ART> delete_art, ConflictManager &manager,
 	                optional_idx &conflict_idx, idx_t i);
-	void VerifyConstraint(DataChunk &chunk, optional_ptr<BoundIndex> delete_index, ConflictManager &manager) override;
+	void VerifyConstraint(DataChunk &chunk, IndexAppendInfo &info, ConflictManager &manager) override;
 	string GetConstraintViolationMessage(VerifyExistenceType verify_type, idx_t failed_index,
 	                                     DataChunk &input) override;
 
diff --git a/src/include/duckdb/execution/index/art/prefix.hpp b/src/include/duckdb/execution/index/art/prefix.hpp
index 5d656c742e8e..38b57514c862 100644
--- a/src/include/duckdb/execution/index/art/prefix.hpp
+++ b/src/include/duckdb/execution/index/art/prefix.hpp
@@ -85,7 +85,8 @@ class Prefix {
 
 	//! Insert a key into a prefix.
 	static ARTConflictType Insert(ART &art, Node &node, const ARTKey &key, idx_t depth, const ARTKey &row_id,
-	                              const GateStatus status, optional_ptr<ART> delete_art);
+	                              const GateStatus status, optional_ptr<ART> delete_art,
+	                              const IndexAppendMode append_mode);
 
 	//! Returns the string representation of the node, or only traverses and verifies the node and its subtree
 	static string VerifyAndToString(ART &art, const Node &node, const bool only_verify);
diff --git a/src/include/duckdb/execution/index/bound_index.hpp b/src/include/duckdb/execution/index/bound_index.hpp
index 522c7c78e4e7..4ac5d4b25a0c 100644
--- a/src/include/duckdb/execution/index/bound_index.hpp
+++ b/src/include/duckdb/execution/index/bound_index.hpp
@@ -28,6 +28,19 @@ class ConflictManager;
 struct IndexLock;
 struct IndexScanState;
 
+enum class IndexAppendMode : uint8_t { DEFAULT = 0, IGNORE_DUPLICATES = 1, INSERT_DUPLICATES = 2 };
+
+class IndexAppendInfo {
+public:
+	IndexAppendInfo() : append_mode(IndexAppendMode::DEFAULT), delete_index(nullptr) {};
+	IndexAppendInfo(const IndexAppendMode append_mode, const optional_ptr<BoundIndex> delete_index)
+	    : append_mode(append_mode), delete_index(delete_index) {};
+
+public:
+	IndexAppendMode append_mode;
+	optional_ptr<BoundIndex> delete_index;
+};
+
 //! The index is an abstract base class that serves as the basis for indexes
 class BoundIndex : public Index {
 public:
@@ -71,17 +84,15 @@ class BoundIndex : public Index {
 	virtual ErrorData Append(IndexLock &l, DataChunk &chunk, Vector &row_ids) = 0;
 	//! Obtains a lock and calls Append while holding that lock.
 	ErrorData Append(DataChunk &chunk, Vector &row_ids);
-	//! Appends data to the locked index and verifies constraint violations against a delete index.
-	virtual ErrorData AppendWithDeleteIndex(IndexLock &l, DataChunk &chunk, Vector &row_ids,
-	                                        optional_ptr<BoundIndex> delete_index);
-	//! Obtains a lock and calls Append with an delete_index while holding that lock.
-	ErrorData AppendWithDeleteIndex(DataChunk &chunk, Vector &row_ids, optional_ptr<BoundIndex> delete_index);
+	//! Appends data to the locked index and verifies constraint violations.
+	virtual ErrorData Append(IndexLock &l, DataChunk &chunk, Vector &row_ids, IndexAppendInfo &info);
+	//! Obtains a lock and calls Append while holding that lock.
+	ErrorData Append(DataChunk &chunk, Vector &row_ids, IndexAppendInfo &info);
 
 	//! Verify that data can be appended to the index without a constraint violation.
-	virtual void VerifyAppend(DataChunk &chunk, optional_ptr<BoundIndex> delete_index,
-	                          optional_ptr<ConflictManager> manager);
+	virtual void VerifyAppend(DataChunk &chunk, IndexAppendInfo &info, optional_ptr<ConflictManager> manager);
 	//! Verifies the constraint for a chunk of data.
-	virtual void VerifyConstraint(DataChunk &chunk, optional_ptr<BoundIndex> delete_index, ConflictManager &manager);
+	virtual void VerifyConstraint(DataChunk &chunk, IndexAppendInfo &info, ConflictManager &manager);
 
 	//! Deletes all data from the index. The lock obtained from InitializeLock must be held
 	virtual void CommitDrop(IndexLock &index_lock) = 0;
@@ -94,8 +105,8 @@ class BoundIndex : public Index {
 
 	//! Insert a chunk.
 	virtual ErrorData Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids) = 0;
-	//! Insert a chunk and verifies constraint violations against a delete index.
-	virtual ErrorData Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids, optional_ptr<BoundIndex> delete_index);
+	//! Insert a chunk and verifies constraint violations.
+	virtual ErrorData Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids, IndexAppendInfo &info);
 
 	//! Merge another index into this index. The lock obtained from InitializeLock must be held, and the other
 	//! index must also be locked during the merge
diff --git a/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp b/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp
index 7ce7ab5776b4..6bddd944f2a5 100644
--- a/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp
+++ b/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp
@@ -217,6 +217,8 @@ class StringValueResult : public ScannerResult {
 	//! (i.e., non-comment) line.
 	bool first_line_is_comment = false;
 
+	bool ignore_empty_values = true;
+
 	//! Specialized code for quoted values, makes sure to remove quotes and escapes
 	static inline void AddQuotedValue(StringValueResult &result, const idx_t buffer_pos);
 	//! Specialized code for possibly escaped values, makes sure to remove escapes
diff --git a/src/include/duckdb/execution/operator/persistent/physical_insert.hpp b/src/include/duckdb/execution/operator/persistent/physical_insert.hpp
index 04d202af24d9..ccb113c4f939 100644
--- a/src/include/duckdb/execution/operator/persistent/physical_insert.hpp
+++ b/src/include/duckdb/execution/operator/persistent/physical_insert.hpp
@@ -38,7 +38,7 @@ class InsertGlobalState : public GlobalSinkState {
 class InsertLocalState : public LocalSinkState {
 public:
 public:
-	InsertLocalState(ClientContext &context, const vector<LogicalType> &types_p,
+	InsertLocalState(ClientContext &context, const vector<LogicalType> &types,
 	                 const vector<unique_ptr<Expression>> &bound_defaults,
 	                 const vector<unique_ptr<BoundConstraint>> &bound_constraints);
 
@@ -47,12 +47,8 @@ class InsertLocalState : public LocalSinkState {
 	TableDeleteState &GetDeleteState(DataTable &table, TableCatalogEntry &table_ref, ClientContext &context);
 
 public:
-	//! The to-be-inserted chunk.
-	//! We initialize it lazily, as we need to know which columns will be references and which will be set to their
-	//! default values.
+	//! The chunk that ends up getting inserted
 	DataChunk insert_chunk;
-	bool init_insert_chunk = true;
-	vector<LogicalType> types;
 	//! The chunk containing the tuples that become an update (if DO UPDATE)
 	DataChunk update_chunk;
 	ExpressionExecutor default_executor;
@@ -174,7 +170,8 @@ class PhysicalInsert : public PhysicalOperator {
 	//! Returns the amount of updated tuples
 	void CreateUpdateChunk(ExecutionContext &context, DataChunk &chunk, TableCatalogEntry &table, Vector &row_ids,
 	                       DataChunk &result) const;
-	idx_t OnConflictHandling(TableCatalogEntry &table, ExecutionContext &context, InsertLocalState &lstate) const;
+	idx_t OnConflictHandling(TableCatalogEntry &table, ExecutionContext &context, InsertGlobalState &gstate,
+	                         InsertLocalState &lstate) const;
 };
 
 } // namespace duckdb
diff --git a/src/include/duckdb/execution/reservoir_sample.hpp b/src/include/duckdb/execution/reservoir_sample.hpp
index b794328bb0fb..bc815b11f9de 100644
--- a/src/include/duckdb/execution/reservoir_sample.hpp
+++ b/src/include/duckdb/execution/reservoir_sample.hpp
@@ -172,7 +172,13 @@ class ReservoirSample : public BlockingSample {
 	static constexpr const SampleType TYPE = SampleType::RESERVOIR_SAMPLE;
 
 	constexpr static idx_t FIXED_SAMPLE_SIZE_MULTIPLIER = 10;
-	constexpr static idx_t FAST_TO_SLOW_THRESHOLD = 60;
+	// size is small enough, then the threshold to switch
+	// MinValue between std vec size and fixed sample size.
+	// During 'fast' sampling, we want every new vector to have the potential
+	// to add to the sample. If the threshold is too far below the standard vector size, then
+	// samples in the sample have a higher weight than new samples coming in.
+	// i.e during vector_size=2, 2 new samples will not be significant compared 2048 samples from 204800 tuples.
+	constexpr static idx_t FAST_TO_SLOW_THRESHOLD = MinValue<idx_t>(STANDARD_VECTOR_SIZE, 60);
 
 	// If the table has less than 204800 rows, this is the percentage
 	// of values we save when serializing/returning a sample.
diff --git a/src/include/duckdb/storage/data_table.hpp b/src/include/duckdb/storage/data_table.hpp
index 32418d911da0..6a0b97727384 100644
--- a/src/include/duckdb/storage/data_table.hpp
+++ b/src/include/duckdb/storage/data_table.hpp
@@ -107,8 +107,8 @@ class DataTable {
 	                 const vector<unique_ptr<BoundConstraint>> &bound_constraints, Vector &row_ids,
 	                 DataChunk &delete_chunk);
 	//! Append a chunk to the transaction-local storage of this table.
-	void LocalAppend(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,
-	                 const vector<unique_ptr<BoundConstraint>> &bound_constraints);
+	void LocalWALAppend(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,
+	                    const vector<unique_ptr<BoundConstraint>> &bound_constraints);
 	//! Append a column data collection with default values to the transaction-local storage of this table.
 	void LocalAppend(TableCatalogEntry &table, ClientContext &context, ColumnDataCollection &collection,
 	                 const vector<unique_ptr<BoundConstraint>> &bound_constraints,
@@ -167,9 +167,10 @@ class DataTable {
 
 	//! Append a chunk with the row ids [row_start, ..., row_start + chunk.size()] to all indexes of the table.
 	//! Returns empty ErrorData, if the append was successful.
-	ErrorData AppendToIndexes(optional_ptr<TableIndexList> delete_indexes, DataChunk &chunk, row_t row_start);
+	ErrorData AppendToIndexes(optional_ptr<TableIndexList> delete_indexes, DataChunk &chunk, row_t row_start,
+	                          const IndexAppendMode index_append_mode);
 	static ErrorData AppendToIndexes(TableIndexList &indexes, optional_ptr<TableIndexList> delete_indexes,
-	                                 DataChunk &chunk, row_t row_start);
+	                                 DataChunk &chunk, row_t row_start, const IndexAppendMode index_append_mode);
 	//! Remove a chunk with the row ids [row_start, ..., row_start + chunk.size()] from all indexes of the table
 	void RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start);
 	//! Remove the chunk with the specified set of row identifiers from all indexes of the table
diff --git a/src/include/duckdb/storage/index_storage_info.hpp b/src/include/duckdb/storage/index_storage_info.hpp
index d5e71ae48183..27312c8244c1 100644
--- a/src/include/duckdb/storage/index_storage_info.hpp
+++ b/src/include/duckdb/storage/index_storage_info.hpp
@@ -62,6 +62,10 @@ struct IndexStorageInfo {
 	BlockPointer root_block_ptr;
 
 	//! Returns true, if IndexStorageInfo holds information to deserialize an index.
+	//! Note that the name can be misleading - any index that is empty (no nodes, etc.) might
+	//! also have neither a root_block_ptr nor allocator_infos.
+	//! Ensure that your index constructor initializes an empty index correctly without the
+	//! need for these fields.
 	bool IsValid() const {
 		return root_block_ptr.IsValid() || !allocator_infos.empty();
 	}
diff --git a/src/include/duckdb/transaction/local_storage.hpp b/src/include/duckdb/transaction/local_storage.hpp
index 453a7ce440ab..4213fa0fadce 100644
--- a/src/include/duckdb/transaction/local_storage.hpp
+++ b/src/include/duckdb/transaction/local_storage.hpp
@@ -48,6 +48,8 @@ class LocalTableStorage : public enable_shared_from_this<LocalTableStorage> {
 	TableIndexList append_indexes;
 	//! The set of delete indexes.
 	TableIndexList delete_indexes;
+	//! Set to INSERT_DUPLICATES, if we are skipping constraint checking during, e.g., WAL replay.
+	IndexAppendMode index_append_mode = IndexAppendMode::DEFAULT;
 	//! The number of deleted rows
 	idx_t deleted_rows;
 	//! The main optimistic data writer
diff --git a/src/main/extension/extension_helper.cpp b/src/main/extension/extension_helper.cpp
index c7b613226a10..f42d43aa8571 100644
--- a/src/main/extension/extension_helper.cpp
+++ b/src/main/extension/extension_helper.cpp
@@ -143,9 +143,9 @@ static const char *const auto_install[] = {"motherduck", "postgres_scanner", "my
 
 // TODO: unify with new autoload mechanism
 bool ExtensionHelper::AllowAutoInstall(const string &extension) {
-	auto lcase = StringUtil::Lower(extension);
+	auto extension_name = ApplyExtensionAlias(extension);
 	for (idx_t i = 0; auto_install[i]; i++) {
-		if (lcase == auto_install[i]) {
+		if (extension_name == auto_install[i]) {
 			return true;
 		}
 	}
diff --git a/src/optimizer/column_lifetime_analyzer.cpp b/src/optimizer/column_lifetime_analyzer.cpp
index efb3e684d473..8b6f1152bf8c 100644
--- a/src/optimizer/column_lifetime_analyzer.cpp
+++ b/src/optimizer/column_lifetime_analyzer.cpp
@@ -102,6 +102,12 @@ void ColumnLifetimeAnalyzer::VisitOperator(LogicalOperator &op) {
 		GenerateProjectionMap(op.children[1]->GetColumnBindings(), rhs_unused, comp_join.right_projection_map);
 		return;
 	}
+	case LogicalOperatorType::LOGICAL_INSERT:
+	case LogicalOperatorType::LOGICAL_UPDATE:
+	case LogicalOperatorType::LOGICAL_DELETE:
+		//! When RETURNING is used, a PROJECTION is the top level operator for INSERTS, UPDATES, and DELETES
+		//! We still need to project all values from these operators so the projection
+		//! on top of them can select from only the table values being inserted.
 	case LogicalOperatorType::LOGICAL_UNION:
 	case LogicalOperatorType::LOGICAL_EXCEPT:
 	case LogicalOperatorType::LOGICAL_INTERSECT:
diff --git a/src/optimizer/filter_combiner.cpp b/src/optimizer/filter_combiner.cpp
index a72354d7df91..45d7c06a06f2 100644
--- a/src/optimizer/filter_combiner.cpp
+++ b/src/optimizer/filter_combiner.cpp
@@ -78,6 +78,7 @@ FilterResult FilterCombiner::AddConstantComparison(vector<ExpressionValueInforma
 			return FilterResult::SUCCESS;
 		case ValueComparisonResult::UNSATISFIABLE_CONDITION:
 			// combination of filters is unsatisfiable: prune the entire branch
+			info_list.push_back(info);
 			return FilterResult::UNSATISFIABLE;
 		default:
 			// prune nothing, move to the next condition
@@ -792,11 +793,15 @@ FilterResult FilterCombiner::AddBoundComparisonFilter(Expression &expr) {
 		auto transitive_filter = FindTransitiveFilter(non_scalar);
 		if (transitive_filter != nullptr) {
 			// try to add transitive filters
-			if (AddTransitiveFilters(transitive_filter->Cast<BoundComparisonExpression>()) ==
-			    FilterResult::UNSUPPORTED) {
+			auto transitive_result = AddTransitiveFilters(transitive_filter->Cast<BoundComparisonExpression>());
+			if (transitive_result == FilterResult::UNSUPPORTED) {
 				// in case of unsuccessful re-add filter into remaining ones
 				remaining_filters.push_back(std::move(transitive_filter));
 			}
+			if (transitive_result == FilterResult::UNSATISFIABLE) {
+				// in case transitive filter is unsatisfiable - abort filter pushdown
+				return FilterResult::UNSATISFIABLE;
+			}
 		}
 		return ret;
 	} else {
@@ -1067,10 +1072,15 @@ FilterResult FilterCombiner::AddTransitiveFilters(BoundComparisonExpression &com
 			if (transitive_filter != nullptr) {
 				// try to add transitive filters
 				auto &transitive_cast = transitive_filter->Cast<BoundComparisonExpression>();
-				if (AddTransitiveFilters(transitive_cast, false) == FilterResult::UNSUPPORTED) {
+				auto transitive_result = AddTransitiveFilters(transitive_cast, false);
+				if (transitive_result == FilterResult::UNSUPPORTED) {
 					// in case of unsuccessful re-add filter into remaining ones
 					remaining_filters.push_back(std::move(transitive_filter));
 				}
+				if (transitive_result == FilterResult::UNSATISFIABLE) {
+					// while adding transitive filters we discovered the filter is unsatisfisable - we can prune
+					return FilterResult::UNSATISFIABLE;
+				}
 			}
 		}
 		return FilterResult::SUCCESS;
diff --git a/src/optimizer/late_materialization.cpp b/src/optimizer/late_materialization.cpp
index 81d4881cd754..aa4e5c4c1fe4 100644
--- a/src/optimizer/late_materialization.cpp
+++ b/src/optimizer/late_materialization.cpp
@@ -1,5 +1,6 @@
 #include "duckdb/optimizer/late_materialization.hpp"
 #include "duckdb/planner/operator/logical_comparison_join.hpp"
+#include "duckdb/planner/operator/logical_filter.hpp"
 #include "duckdb/planner/operator/logical_get.hpp"
 #include "duckdb/planner/operator/logical_limit.hpp"
 #include "duckdb/planner/operator/logical_order.hpp"
@@ -61,6 +62,8 @@ ColumnBinding LateMaterialization::ConstructRHS(unique_ptr<LogicalOperator> &op)
 	// we have reached the logical get - now we need to push the row-id column (if it is not yet projected out)
 	auto &get = child.get().Cast<LogicalGet>();
 	auto row_id_idx = GetOrInsertRowId(get);
+	idx_t column_count = get.projection_ids.empty() ? get.GetColumnIds().size() : get.projection_ids.size();
+	D_ASSERT(column_count == get.GetColumnBindings().size());
 
 	// the row id has been projected - now project it up the stack
 	ColumnBinding row_id_binding(get.table_index, row_id_idx);
@@ -74,11 +77,18 @@ ColumnBinding LateMaterialization::ConstructRHS(unique_ptr<LogicalOperator> &op)
 			    make_uniq<BoundColumnRefExpression>("rowid", get.GetRowIdType(), row_id_binding));
 			// modify the row-id-binding to push to the new projection
 			row_id_binding = ColumnBinding(proj.table_index, proj.expressions.size() - 1);
+			column_count = proj.expressions.size();
 			break;
 		}
-		case LogicalOperatorType::LOGICAL_FILTER:
-			// column bindings pass-through this operator as-is
+		case LogicalOperatorType::LOGICAL_FILTER: {
+			auto &filter = op.Cast<LogicalFilter>();
+			// column bindings pass-through this operator as-is UNLESS the filter has a projection map
+			if (filter.HasProjectionMap()) {
+				// if the filter has a projection map, we need to project the new column
+				filter.projection_map.push_back(column_count - 1);
+			}
 			break;
+		}
 		default:
 			throw InternalException("Unsupported logical operator in LateMaterialization::ConstructRHS");
 		}
@@ -212,12 +222,13 @@ bool LateMaterialization::TryLateMaterialization(unique_ptr<LogicalOperator> &op
 			child = *child.get().children[0];
 			break;
 		}
-		case LogicalOperatorType::LOGICAL_FILTER:
+		case LogicalOperatorType::LOGICAL_FILTER: {
 			// visit filter expressions - we need these columns
 			VisitOperatorExpressions(child.get());
 			// continue into child
 			child = *child.get().children[0];
 			break;
+		}
 		default:
 			// unsupported operator for late materialization
 			return false;
diff --git a/src/parser/transform/expression/transform_columnref.cpp b/src/parser/transform/expression/transform_columnref.cpp
index a232219fa14c..dfbbc6f121fc 100644
--- a/src/parser/transform/expression/transform_columnref.cpp
+++ b/src/parser/transform/expression/transform_columnref.cpp
@@ -96,6 +96,7 @@ unique_ptr<ParsedExpression> Transformer::TransformStarExpression(duckdb_libpgqu
 			result->relation_name = child_star.relation_name;
 			result->exclude_list = std::move(child_star.exclude_list);
 			result->replace_list = std::move(child_star.replace_list);
+			result->rename_list = std::move(child_star.rename_list);
 			result->expr.reset();
 		} else if (result->expr->GetExpressionType() == ExpressionType::LAMBDA) {
 			vector<unique_ptr<ParsedExpression>> children;
diff --git a/src/planner/binder/statement/bind_copy_database.cpp b/src/planner/binder/statement/bind_copy_database.cpp
index f05445e906e4..d2c0a03fb8c7 100644
--- a/src/planner/binder/statement/bind_copy_database.cpp
+++ b/src/planner/binder/statement/bind_copy_database.cpp
@@ -43,7 +43,6 @@ unique_ptr<LogicalOperator> Binder::BindCopyDatabaseSchema(Catalog &from_databas
 		info->entries.push_back(std::move(create_info));
 	}
 
-	// FIXME: copy indexes
 	return make_uniq<LogicalCopyDatabase>(std::move(info));
 }
 
diff --git a/src/planner/binder/statement/bind_create.cpp b/src/planner/binder/statement/bind_create.cpp
index 4f1a43a308ba..053ceaef4f56 100644
--- a/src/planner/binder/statement/bind_create.cpp
+++ b/src/planner/binder/statement/bind_create.cpp
@@ -98,7 +98,7 @@ SchemaCatalogEntry &Binder::BindSchema(CreateInfo &info) {
 		info.catalog = default_entry.catalog;
 		info.schema = default_entry.schema;
 	} else if (IsInvalidSchema(info.schema)) {
-		info.schema = search_path->GetDefaultSchema(info.catalog);
+		info.schema = search_path->GetDefaultSchema(context, info.catalog);
 	} else if (IsInvalidCatalog(info.catalog)) {
 		info.catalog = search_path->GetDefaultCatalog(info.schema);
 	}
diff --git a/src/planner/binder/tableref/bind_table_function.cpp b/src/planner/binder/tableref/bind_table_function.cpp
index 26dd86c9dfd1..29d68f3fc3ab 100644
--- a/src/planner/binder/tableref/bind_table_function.cpp
+++ b/src/planner/binder/tableref/bind_table_function.cpp
@@ -126,8 +126,7 @@ bool Binder::BindTableFunctionParameters(TableFunctionCatalogEntry &table_functi
 		    child->GetExpressionType() == ExpressionType::SUBQUERY) {
 			D_ASSERT(table_function.functions.Size() == 1);
 			auto fun = table_function.functions.GetFunctionByOffset(0);
-			if (table_function.functions.Size() != 1 || fun.arguments.empty() ||
-			    fun.arguments[0].id() != LogicalTypeId::TABLE) {
+			if (table_function.functions.Size() != 1 || fun.arguments.empty()) {
 				throw BinderException(
 				    "Only table-in-out functions can have subquery parameters - %s only accepts constant parameters",
 				    fun.name);
diff --git a/src/planner/subquery/flatten_dependent_join.cpp b/src/planner/subquery/flatten_dependent_join.cpp
index a297a7855dae..bb5db60a98bd 100644
--- a/src/planner/subquery/flatten_dependent_join.cpp
+++ b/src/planner/subquery/flatten_dependent_join.cpp
@@ -401,6 +401,7 @@ unique_ptr<LogicalOperator> FlattenDependentJoins::PushDownDependentJoinInternal
 				// only right has correlation: push into right
 				plan->children[1] = PushDownDependentJoinInternal(std::move(plan->children[1]),
 				                                                  parent_propagate_null_values, lateral_depth);
+				delim_offset += plan->children[0]->GetColumnBindings().size();
 				// Remove the correlated columns coming from outside for current join node
 				return plan;
 			}
@@ -419,6 +420,7 @@ unique_ptr<LogicalOperator> FlattenDependentJoins::PushDownDependentJoinInternal
 				// only right has correlation: push into right
 				plan->children[1] = PushDownDependentJoinInternal(std::move(plan->children[1]),
 				                                                  parent_propagate_null_values, lateral_depth);
+				delim_offset += plan->children[0]->GetColumnBindings().size();
 				return plan;
 			}
 		} else if (join.join_type == JoinType::MARK) {
diff --git a/src/storage/data_table.cpp b/src/storage/data_table.cpp
index cf35519cca82..168ee3009b64 100644
--- a/src/storage/data_table.cpp
+++ b/src/storage/data_table.cpp
@@ -679,9 +679,11 @@ void DataTable::VerifyUniqueIndexes(TableIndexList &indexes, optional_ptr<LocalT
 
 			if (storage) {
 				auto delete_index = storage->delete_indexes.Find(art.GetIndexName());
-				art.VerifyAppend(chunk, delete_index, nullptr);
+				IndexAppendInfo index_append_info(IndexAppendMode::DEFAULT, delete_index);
+				art.VerifyAppend(chunk, index_append_info, nullptr);
 			} else {
-				art.VerifyAppend(chunk, nullptr, nullptr);
+				IndexAppendInfo index_append_info;
+				art.VerifyAppend(chunk, index_append_info, nullptr);
 			}
 			return false;
 		});
@@ -712,8 +714,10 @@ void DataTable::VerifyUniqueIndexes(TableIndexList &indexes, optional_ptr<LocalT
 	manager->SetMode(ConflictManagerMode::SCAN);
 	auto &matched_indexes = manager->MatchedIndexes();
 	auto &matched_delete_indexes = manager->MatchedDeleteIndexes();
+	IndexAppendInfo index_append_info(IndexAppendMode::DEFAULT, nullptr);
 	for (idx_t i = 0; i < matched_indexes.size(); i++) {
-		matched_indexes[i].get().VerifyAppend(chunk, matched_delete_indexes[i], *manager);
+		index_append_info.delete_index = matched_delete_indexes[i];
+		matched_indexes[i].get().VerifyAppend(chunk, index_append_info, *manager);
 	}
 
 	// Scan the other indexes and throw, if there are any conflicts.
@@ -728,9 +732,11 @@ void DataTable::VerifyUniqueIndexes(TableIndexList &indexes, optional_ptr<LocalT
 
 		if (storage) {
 			auto delete_index = storage->delete_indexes.Find(art.GetIndexName());
-			art.VerifyAppend(chunk, delete_index, *manager);
+			IndexAppendInfo index_append_info(IndexAppendMode::DEFAULT, delete_index);
+			art.VerifyAppend(chunk, index_append_info, *manager);
 		} else {
-			art.VerifyAppend(chunk, nullptr, *manager);
+			IndexAppendInfo index_append_info;
+			art.VerifyAppend(chunk, index_append_info, *manager);
 		}
 		return false;
 	});
@@ -863,13 +869,14 @@ void DataTable::LocalMerge(ClientContext &context, RowGroupCollection &collectio
 	local_storage.LocalMerge(*this, collection);
 }
 
-void DataTable::LocalAppend(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,
-                            const vector<unique_ptr<BoundConstraint>> &bound_constraints) {
+void DataTable::LocalWALAppend(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,
+                               const vector<unique_ptr<BoundConstraint>> &bound_constraints) {
 	LocalAppendState append_state;
 	auto &storage = table.GetStorage();
 	storage.InitializeLocalAppend(append_state, table, context, bound_constraints);
 
-	storage.LocalAppend(append_state, context, chunk, false);
+	storage.LocalAppend(append_state, context, chunk, true);
+	append_state.storage->index_append_mode = IndexAppendMode::INSERT_DUPLICATES;
 	storage.FinalizeLocalAppend(append_state);
 }
 
@@ -1109,7 +1116,7 @@ void DataTable::RevertAppend(DuckTransaction &transaction, idx_t start_row, idx_
 // Indexes
 //===--------------------------------------------------------------------===//
 ErrorData DataTable::AppendToIndexes(TableIndexList &indexes, optional_ptr<TableIndexList> delete_indexes,
-                                     DataChunk &chunk, row_t row_start) {
+                                     DataChunk &chunk, row_t row_start, const IndexAppendMode index_append_mode) {
 	ErrorData error;
 	if (indexes.Empty()) {
 		return error;
@@ -1137,7 +1144,8 @@ ErrorData DataTable::AppendToIndexes(TableIndexList &indexes, optional_ptr<Table
 		}
 
 		try {
-			error = index.AppendWithDeleteIndex(chunk, row_ids, delete_index);
+			IndexAppendInfo index_append_info(index_append_mode, delete_index);
+			error = index.Append(chunk, row_ids, index_append_info);
 		} catch (std::exception &ex) {
 			error = ErrorData(ex);
 		}
@@ -1160,9 +1168,10 @@ ErrorData DataTable::AppendToIndexes(TableIndexList &indexes, optional_ptr<Table
 	return error;
 }
 
-ErrorData DataTable::AppendToIndexes(optional_ptr<TableIndexList> delete_indexes, DataChunk &chunk, row_t row_start) {
+ErrorData DataTable::AppendToIndexes(optional_ptr<TableIndexList> delete_indexes, DataChunk &chunk, row_t row_start,
+                                     const IndexAppendMode index_append_mode) {
 	D_ASSERT(is_root);
-	return AppendToIndexes(info->indexes, delete_indexes, chunk, row_start);
+	return AppendToIndexes(info->indexes, delete_indexes, chunk, row_start, index_append_mode);
 }
 
 void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
diff --git a/src/storage/local_storage.cpp b/src/storage/local_storage.cpp
index 71b20bd10059..93326ed6726f 100644
--- a/src/storage/local_storage.cpp
+++ b/src/storage/local_storage.cpp
@@ -41,7 +41,6 @@ LocalTableStorage::LocalTableStorage(ClientContext &context, DataTable &table)
 		// Create a delete index and a local index.
 		auto delete_index = make_uniq<ART>(art.GetIndexName(), constraint_type, art.GetColumnIds(),
 		                                   art.table_io_manager, std::move(delete_expressions), art.db);
-		delete_index->append_mode = ARTAppendMode::IGNORE_DUPLICATES;
 		delete_indexes.AddIndex(std::move(delete_index));
 
 		auto index = make_uniq<ART>(art.GetIndexName(), constraint_type, art.GetColumnIds(), art.table_io_manager,
@@ -152,7 +151,7 @@ ErrorData LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, RowGr
 		}
 		mock_chunk.SetCardinality(chunk);
 		// append this chunk to the indexes of the table
-		error = DataTable::AppendToIndexes(index_list, nullptr, mock_chunk, start_row);
+		error = DataTable::AppendToIndexes(index_list, nullptr, mock_chunk, start_row, index_append_mode);
 		if (error.HasError()) {
 			return false;
 		}
@@ -173,7 +172,7 @@ void LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, TableAppen
 		// appending: need to scan entire
 		row_groups->Scan(transaction, [&](DataChunk &chunk) -> bool {
 			// append this chunk to the indexes of the table
-			error = table.AppendToIndexes(delete_indexes, chunk, append_state.current_row);
+			error = table.AppendToIndexes(delete_indexes, chunk, append_state.current_row, index_append_mode);
 			if (error.HasError()) {
 				return false;
 			}
@@ -186,6 +185,7 @@ void LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, TableAppen
 		auto &index_list = data_table_info->GetIndexes();
 		error = AppendToIndexes(transaction, *row_groups, index_list, table.GetTypes(), append_state.current_row);
 	}
+
 	if (error.HasError()) {
 		// need to revert all appended row ids
 		row_t current_row = append_state.row_start;
@@ -386,7 +386,8 @@ void LocalTableStorage::AppendToDeleteIndexes(Vector &row_ids, DataChunk &delete
 		if (!art.IsUnique()) {
 			return false;
 		}
-		auto result = art.Cast<BoundIndex>().Append(delete_chunk, row_ids);
+		IndexAppendInfo index_append_info(IndexAppendMode::IGNORE_DUPLICATES, nullptr);
+		auto result = art.Cast<BoundIndex>().Append(delete_chunk, row_ids, index_append_info);
 		if (result.HasError()) {
 			throw InternalException("unexpected constraint violation on delete ART: ", result.Message());
 		}
@@ -401,7 +402,7 @@ void LocalStorage::Append(LocalAppendState &state, DataChunk &chunk) {
 	idx_t base_id = offset + state.append_state.total_append_count;
 
 	auto error = DataTable::AppendToIndexes(storage->append_indexes, storage->delete_indexes, chunk,
-	                                        NumericCast<row_t>(base_id));
+	                                        NumericCast<row_t>(base_id), storage->index_append_mode);
 	if (error.HasError()) {
 		error.Throw();
 	}
diff --git a/src/storage/table_index_list.cpp b/src/storage/table_index_list.cpp
index b46ddaf1989e..abcb9a7d144a 100644
--- a/src/storage/table_index_list.cpp
+++ b/src/storage/table_index_list.cpp
@@ -173,9 +173,11 @@ void TableIndexList::VerifyForeignKey(optional_ptr<LocalTableStorage> storage, c
 	D_ASSERT(index && index->IsBound());
 	if (storage) {
 		auto delete_index = storage->delete_indexes.Find(index->GetIndexName());
-		index->Cast<BoundIndex>().VerifyConstraint(chunk, delete_index, conflict_manager);
+		IndexAppendInfo index_append_info(IndexAppendMode::DEFAULT, delete_index);
+		index->Cast<BoundIndex>().VerifyConstraint(chunk, index_append_info, conflict_manager);
 	} else {
-		index->Cast<BoundIndex>().VerifyConstraint(chunk, nullptr, conflict_manager);
+		IndexAppendInfo index_append_info;
+		index->Cast<BoundIndex>().VerifyConstraint(chunk, index_append_info, conflict_manager);
 	}
 }
 
@@ -201,10 +203,9 @@ vector<IndexStorageInfo> TableIndexList::GetStorageInfos(const case_insensitive_
 		}
 
 		auto info = index->Cast<UnboundIndex>().GetStorageInfo();
-		D_ASSERT(info.IsValid() && !info.name.empty());
+		D_ASSERT(!info.name.empty());
 		infos.push_back(info);
 	}
-
 	return infos;
 }
 
diff --git a/src/storage/wal_replay.cpp b/src/storage/wal_replay.cpp
index 0b6894854ea2..ac2a1bb57fed 100644
--- a/src/storage/wal_replay.cpp
+++ b/src/storage/wal_replay.cpp
@@ -740,7 +740,7 @@ void WriteAheadLogDeserializer::ReplayInsert() {
 	// Append to the current table without constraint verification.
 	vector<unique_ptr<BoundConstraint>> bound_constraints;
 	auto &storage = state.current_table->GetStorage();
-	storage.LocalAppend(*state.current_table, context, chunk, bound_constraints);
+	storage.LocalWALAppend(*state.current_table, context, chunk, bound_constraints);
 }
 
 static void MarkBlocksAsUsed(BlockManager &manager, const PersistentColumnData &col_data) {
diff --git a/tools/pythonpkg/duckdb/experimental/spark/sql/functions.py b/tools/pythonpkg/duckdb/experimental/spark/sql/functions.py
index 4d01c69e5993..f0cbf1dbfc12 100644
--- a/tools/pythonpkg/duckdb/experimental/spark/sql/functions.py
+++ b/tools/pythonpkg/duckdb/experimental/spark/sql/functions.py
@@ -5617,7 +5617,7 @@ def to_timestamp(col: "ColumnOrName", format: Optional[str] = None) -> Column:
     >>> df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect()
     [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]
     """
-    return _to_date_or_timestamp(col, _types.TimestampType(), format)
+    return _to_date_or_timestamp(col, _types.TimestampNTZType(), format)
 
 
 def to_timestamp_ltz(
@@ -5649,7 +5649,7 @@ def to_timestamp_ltz(
     ... # doctest: +SKIP
     [Row(r=datetime.datetime(2016, 12, 31, 0, 0))]
     """
-    return _to_date_or_timestamp(timestamp, _types.TimestampType(), format)
+    return _to_date_or_timestamp(timestamp, _types.TimestampNTZType(), format)
 
 
 def to_timestamp_ntz(
@@ -5731,7 +5731,7 @@ def substr(
 
 
 def _unix_diff(col: "ColumnOrName", part: str) -> Column:
-    return _invoke_function_over_columns("date_diff", lit(part), lit("1970-01-01 00:00:00+00:00").cast("timestamptz"), col)
+    return _invoke_function_over_columns("date_diff", lit(part), lit("1970-01-01 00:00:00+00:00").cast("timestamp"), col)
 
 def unix_date(col: "ColumnOrName") -> Column:
     """Returns the number of days since 1970-01-01.
diff --git a/tools/shell/shell.cpp b/tools/shell/shell.cpp
index 53880de8a55d..45252c86e95b 100644
--- a/tools/shell/shell.cpp
+++ b/tools/shell/shell.cpp
@@ -4004,7 +4004,7 @@ MetadataResult ToggleTimer(ShellState &state, const char **azArg, idx_t nArg) {
 }
 
 MetadataResult ShowVersion(ShellState &state, const char **azArg, idx_t nArg) {
-	utf8_printf(state.out, "SQLite %s %s
" /*extra-version-info*/, sqlite3_libversion(), sqlite3_sourceid());
+	utf8_printf(state.out, "DuckDB %s %s
" /*extra-version-info*/, sqlite3_libversion(), sqlite3_sourceid());
 #define CTIMEOPT_VAL_(opt) #opt
 #define CTIMEOPT_VAL(opt)  CTIMEOPT_VAL_(opt)
 #if defined(__clang__) && defined(__clang_major__)
