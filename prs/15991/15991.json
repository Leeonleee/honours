{
  "repo": "duckdb/duckdb",
  "pull_number": 15991,
  "instance_id": "duckdb__duckdb-15991",
  "issue_numbers": [
    "15921",
    "15443",
    "15443"
  ],
  "base_commit": "0959644c1d57409e78d2fae0262f792921a54c55",
  "patch": "diff --git a/.github/config/out_of_tree_extensions.cmake b/.github/config/out_of_tree_extensions.cmake\nindex f3c49daae57f..2d628facb839 100644\n--- a/.github/config/out_of_tree_extensions.cmake\n+++ b/.github/config/out_of_tree_extensions.cmake\n@@ -28,8 +28,7 @@ if (NOT MINGW AND NOT ${WASM_ENABLED} AND NOT ${MUSL_ENABLED})\n     duckdb_extension_load(arrow\n             LOAD_TESTS DONT_LINK\n             GIT_URL https://github.com/duckdb/arrow\n-            GIT_TAG c50862c82c065096722745631f4230832a3a04e8\n-            APPLY_PATCHES\n+            GIT_TAG cff2f0e21b1608e38640e15b4cf0693dd52dd0eb\n             )\n endif()\n \ndiff --git a/.github/patches/extensions/arrow/arrow_extension_types.patch b/.github/patches/extensions/arrow/arrow_extension_types.patch\ndeleted file mode 100644\nindex f99754806186..000000000000\n--- a/.github/patches/extensions/arrow/arrow_extension_types.patch\n+++ /dev/null\n@@ -1,57 +0,0 @@\n-diff --git a/src/arrow_scan_ipc.cpp b/src/arrow_scan_ipc.cpp\n-index a60d255..37e0e81 100644\n---- a/src/arrow_scan_ipc.cpp\n-+++ b/src/arrow_scan_ipc.cpp\n-@@ -15,7 +15,6 @@ TableFunction ArrowIPCTableFunction::GetFunction() {\n-       ArrowTableFunction::ArrowScanInitLocal);\n-\n-   scan_arrow_ipc_func.cardinality = ArrowTableFunction::ArrowScanCardinality;\n--  scan_arrow_ipc_func.get_batch_index = nullptr; // TODO implement\n-   scan_arrow_ipc_func.projection_pushdown = true;\n-   scan_arrow_ipc_func.filter_pushdown = false;\n-\n-@@ -71,9 +70,12 @@ unique_ptr<FunctionData> ArrowIPCTableFunction::ArrowScanBind(\n-     if (!schema.release) {\n-       throw InvalidInputException(\"arrow_scan: released schema passed\");\n-     }\n--    auto arrow_type = GetArrowLogicalType(schema);\n-+    auto arrow_type =\n-+       ArrowType::GetArrowLogicalType(DBConfig::GetConfig(context), schema);\n-+\n-     if (schema.dictionary) {\n--      auto dictionary_type = GetArrowLogicalType(*schema.dictionary);\n-+      auto dictionary_type = ArrowType::GetArrowLogicalType(\n-+          DBConfig::GetConfig(context), *schema.dictionary);\n-       return_types.emplace_back(dictionary_type->GetDuckType());\n-       arrow_type->SetDictionary(std::move(dictionary_type));\n-     } else {\n-diff --git a/src/arrow_to_ipc.cpp b/src/arrow_to_ipc.cpp\n-index c316d85..905df2b 100644\n---- a/src/arrow_to_ipc.cpp\n-+++ b/src/arrow_to_ipc.cpp\n-@@ -76,9 +76,9 @@ ToArrowIPCFunction::Bind(ClientContext &context, TableFunctionBindInput &input,\n-\n-   // Create the Arrow schema\n-   ArrowSchema schema;\n-+  auto properties = context.GetClientProperties();\n-   ArrowConverter::ToArrowSchema(&schema, input.input_table_types,\n--                                input.input_table_names,\n--                                context.GetClientProperties());\n-+                                input.input_table_names, properties);\n-   result->schema = arrow::ImportSchema(&schema).ValueOrDie();\n-\n-   return std::move(result);\n-@@ -116,9 +116,10 @@ OperatorResultType ToArrowIPCFunction::Function(ExecutionContext &context,\n-     output.data[1].SetValue(0, Value::BOOLEAN(1));\n-   } else {\n-     if (!local_state.appender) {\n--      local_state.appender =\n--          make_uniq<ArrowAppender>(input.GetTypes(), data.chunk_size,\n--                                   context.client.GetClientProperties());\n-+      local_state.appender = make_uniq<ArrowAppender>(input.GetTypes(), data.chunk_size,\n-+                                   context.client.GetClientProperties(),\n-+                                   ArrowTypeExtensionData::GetExtensionTypes(\n-+                                       context.client, input.GetTypes()));\n-     }\n-\n-     // Append input chunk\ndiff --git a/.github/workflows/Pyodide.yml b/.github/workflows/Pyodide.yml\nindex 0b6950ce89a5..32e14802065c 100644\n--- a/.github/workflows/Pyodide.yml\n+++ b/.github/workflows/Pyodide.yml\n@@ -108,7 +108,7 @@ jobs:\n       - name: install deps into environment\n         run: |\n           source .venv-pyodide/bin/activate\n-          pip install pytest numpy pandas mypy\n+          pip install pytest numpy pandas 'mypy<=1.13'\n \n       - name: install duckdb wasm wheel into environment\n         run: |\ndiff --git a/benchmark/tpch/startup.cpp b/benchmark/tpch/startup.cpp\nindex 88f678d22fdf..a468545476f2 100644\n--- a/benchmark/tpch/startup.cpp\n+++ b/benchmark/tpch/startup.cpp\n@@ -1,6 +1,5 @@\n #include \"benchmark_runner.hpp\"\n #include \"compare_result.hpp\"\n-#include \"tpch_extension.hpp\"\n #include \"duckdb_benchmark_macro.hpp\"\n \n using namespace duckdb;\n@@ -70,6 +69,6 @@ TPCHStartup(\"PRAGMA tpch(1)\") NormalConfig() string VerifyResult(QueryResult *re\n \tif (result->HasError()) {\n \t\treturn result->GetError();\n \t}\n-\treturn compare_csv(*result, TpchExtension::GetAnswer(SF, 1), true);\n+\treturn string();\n }\n FINISH_BENCHMARK(TPCHQ1)\ndiff --git a/extension/core_functions/scalar/list/flatten.cpp b/extension/core_functions/scalar/list/flatten.cpp\nindex 849c20d16c05..5f4361cdbb1a 100644\n--- a/extension/core_functions/scalar/list/flatten.cpp\n+++ b/extension/core_functions/scalar/list/flatten.cpp\n@@ -7,44 +7,41 @@\n \n namespace duckdb {\n \n-void ListFlattenFunction(DataChunk &args, ExpressionState &state, Vector &result) {\n-\tD_ASSERT(args.ColumnCount() == 1);\n+static void ListFlattenFunction(DataChunk &args, ExpressionState &, Vector &result) {\n \n-\tVector &input = args.data[0];\n-\tif (input.GetType().id() == LogicalTypeId::SQLNULL) {\n-\t\tresult.Reference(input);\n+\tconst auto flat_list_data = FlatVector::GetData<list_entry_t>(result);\n+\tauto &flat_list_mask = FlatVector::Validity(result);\n+\n+\tUnifiedVectorFormat outer_format;\n+\tUnifiedVectorFormat inner_format;\n+\tUnifiedVectorFormat items_format;\n+\n+\t// Setup outer vec;\n+\tauto &outer_vec = args.data[0];\n+\tconst auto outer_count = args.size();\n+\touter_vec.ToUnifiedFormat(outer_count, outer_format);\n+\n+\t// Special case: outer list is all-null\n+\tif (outer_vec.GetType().id() == LogicalTypeId::SQLNULL) {\n+\t\tresult.Reference(outer_vec);\n \t\treturn;\n \t}\n \n-\tidx_t count = args.size();\n-\n-\t// Prepare the result vector\n-\tresult.SetVectorType(VectorType::FLAT_VECTOR);\n-\t// This holds the new offsets and lengths\n-\tauto result_entries = FlatVector::GetData<list_entry_t>(result);\n-\tauto &result_validity = FlatVector::Validity(result);\n-\n-\t// The outermost list in each row\n-\tUnifiedVectorFormat row_data;\n-\tinput.ToUnifiedFormat(count, row_data);\n-\tauto row_entries = UnifiedVectorFormat::GetData<list_entry_t>(row_data);\n-\n-\t// The list elements in each row: [HERE, ...]\n-\tauto &row_lists = ListVector::GetEntry(input);\n-\tUnifiedVectorFormat row_lists_data;\n-\tidx_t total_row_lists = ListVector::GetListSize(input);\n-\trow_lists.ToUnifiedFormat(total_row_lists, row_lists_data);\n-\tauto row_lists_entries = UnifiedVectorFormat::GetData<list_entry_t>(row_lists_data);\n-\n-\tif (row_lists.GetType().id() == LogicalTypeId::SQLNULL) {\n-\t\tfor (idx_t row_cnt = 0; row_cnt < count; row_cnt++) {\n-\t\t\tauto row_idx = row_data.sel->get_index(row_cnt);\n-\t\t\tif (!row_data.validity.RowIsValid(row_idx)) {\n-\t\t\t\tresult_validity.SetInvalid(row_cnt);\n+\t// Setup inner vec\n+\tauto &inner_vec = ListVector::GetEntry(outer_vec);\n+\tconst auto inner_count = ListVector::GetListSize(outer_vec);\n+\tinner_vec.ToUnifiedFormat(inner_count, inner_format);\n+\n+\t// Special case: inner list is all-null\n+\tif (inner_vec.GetType().id() == LogicalTypeId::SQLNULL) {\n+\t\tfor (idx_t outer_raw_idx = 0; outer_raw_idx < outer_count; outer_raw_idx++) {\n+\t\t\tconst auto outer_idx = outer_format.sel->get_index(outer_raw_idx);\n+\t\t\tif (!outer_format.validity.RowIsValid(outer_idx)) {\n+\t\t\t\tflat_list_mask.SetInvalid(outer_raw_idx);\n \t\t\t\tcontinue;\n \t\t\t}\n-\t\t\tresult_entries[row_cnt].offset = 0;\n-\t\t\tresult_entries[row_cnt].length = 0;\n+\t\t\tflat_list_data[outer_raw_idx].offset = 0;\n+\t\t\tflat_list_data[outer_raw_idx].length = 0;\n \t\t}\n \t\tif (args.AllConstant()) {\n \t\t\tresult.SetVectorType(VectorType::CONSTANT_VECTOR);\n@@ -52,57 +49,90 @@ void ListFlattenFunction(DataChunk &args, ExpressionState &state, Vector &result\n \t\treturn;\n \t}\n \n-\t// The actual elements inside each row list: [[HERE, ...], []]\n-\t// This one becomes the child vector of the result.\n-\tauto &elem_vector = ListVector::GetEntry(row_lists);\n+\t// Setup items vec\n+\tauto &items_vec = ListVector::GetEntry(inner_vec);\n+\tconst auto items_count = ListVector::GetListSize(inner_vec);\n+\titems_vec.ToUnifiedFormat(items_count, items_format);\n+\n+\t// First pass: Figure out the total amount of items.\n+\t// This can be more than items_count if the inner list reference the same item(s) multiple times.\n+\n+\tidx_t total_items = 0;\n+\n+\tconst auto outer_data = UnifiedVectorFormat::GetData<list_entry_t>(outer_format);\n+\tconst auto inner_data = UnifiedVectorFormat::GetData<list_entry_t>(inner_format);\n+\n+\tfor (idx_t outer_raw_idx = 0; outer_raw_idx < outer_count; outer_raw_idx++) {\n+\t\tconst auto outer_idx = outer_format.sel->get_index(outer_raw_idx);\n+\n+\t\tif (!outer_format.validity.RowIsValid(outer_idx)) {\n+\t\t\tcontinue;\n+\t\t}\n+\n+\t\tconst auto &outer_entry = outer_data[outer_idx];\n+\n+\t\tfor (idx_t inner_raw_idx = outer_entry.offset; inner_raw_idx < outer_entry.offset + outer_entry.length;\n+\t\t     inner_raw_idx++) {\n+\t\t\tconst auto inner_idx = inner_format.sel->get_index(inner_raw_idx);\n \n-\t// We'll use this selection vector to slice the elem_vector.\n-\tidx_t child_elem_cnt = ListVector::GetListSize(row_lists);\n-\tSelectionVector sel(child_elem_cnt);\n+\t\t\tif (!inner_format.validity.RowIsValid(inner_idx)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tconst auto &inner_entry = inner_data[inner_idx];\n+\n+\t\t\ttotal_items += inner_entry.length;\n+\t\t}\n+\t}\n+\n+\t// Now we know the total amount of items, we can create our selection vector.\n+\tSelectionVector sel(total_items);\n \tidx_t sel_idx = 0;\n \n-\t// HERE, [[]], ...\n-\tfor (idx_t row_cnt = 0; row_cnt < count; row_cnt++) {\n-\t\tauto row_idx = row_data.sel->get_index(row_cnt);\n+\t// Second pass: Fill the selection vector (and the result list entries)\n+\n+\tfor (idx_t outer_raw_idx = 0; outer_raw_idx < outer_count; outer_raw_idx++) {\n+\t\tconst auto outer_idx = outer_format.sel->get_index(outer_raw_idx);\n \n-\t\tif (!row_data.validity.RowIsValid(row_idx)) {\n-\t\t\tresult_validity.SetInvalid(row_cnt);\n+\t\tif (!outer_format.validity.RowIsValid(outer_idx)) {\n+\t\t\tflat_list_mask.SetInvalid(outer_raw_idx);\n \t\t\tcontinue;\n \t\t}\n \n-\t\tidx_t list_offset = sel_idx;\n-\t\tidx_t list_length = 0;\n+\t\tconst auto &outer_entry = outer_data[outer_idx];\n+\n+\t\tlist_entry_t list_entry = {sel_idx, 0};\n \n-\t\t// [HERE, [...], ...]\n-\t\tauto row_entry = row_entries[row_idx];\n-\t\tfor (idx_t row_lists_cnt = 0; row_lists_cnt < row_entry.length; row_lists_cnt++) {\n-\t\t\tauto row_lists_idx = row_lists_data.sel->get_index(row_entry.offset + row_lists_cnt);\n+\t\tfor (idx_t inner_raw_idx = outer_entry.offset; inner_raw_idx < outer_entry.offset + outer_entry.length;\n+\t\t     inner_raw_idx++) {\n+\t\t\tconst auto inner_idx = inner_format.sel->get_index(inner_raw_idx);\n \n-\t\t\t// Skip invalid lists\n-\t\t\tif (!row_lists_data.validity.RowIsValid(row_lists_idx)) {\n+\t\t\tif (!inner_format.validity.RowIsValid(inner_idx)) {\n \t\t\t\tcontinue;\n \t\t\t}\n \n-\t\t\t// [[HERE, ...], [.., ...]]\n-\t\t\tauto list_entry = row_lists_entries[row_lists_idx];\n-\t\t\tlist_length += list_entry.length;\n+\t\t\tconst auto &inner_entry = inner_data[inner_idx];\n+\n+\t\t\tlist_entry.length += inner_entry.length;\n+\n+\t\t\tfor (idx_t elem_raw_idx = inner_entry.offset; elem_raw_idx < inner_entry.offset + inner_entry.length;\n+\t\t\t     elem_raw_idx++) {\n+\t\t\t\tconst auto elem_idx = items_format.sel->get_index(elem_raw_idx);\n \n-\t\t\tfor (idx_t elem_cnt = 0; elem_cnt < list_entry.length; elem_cnt++) {\n-\t\t\t\t// offset of the element in the elem_vector.\n-\t\t\t\tidx_t offset = list_entry.offset + elem_cnt;\n-\t\t\t\tsel.set_index(sel_idx, offset);\n+\t\t\t\tsel.set_index(sel_idx, elem_idx);\n \t\t\t\tsel_idx++;\n \t\t\t}\n \t\t}\n \n-\t\tresult_entries[row_cnt].offset = list_offset;\n-\t\tresult_entries[row_cnt].length = list_length;\n+\t\t// Assign the result list entry\n+\t\tflat_list_data[outer_raw_idx] = list_entry;\n \t}\n \n+\t// Now assing the result\n \tListVector::SetListSize(result, sel_idx);\n \n \tauto &result_child_vector = ListVector::GetEntry(result);\n-\tresult_child_vector.Slice(elem_vector, sel, sel_idx);\n+\tresult_child_vector.Slice(items_vec, sel, sel_idx);\n \tresult_child_vector.Flatten(sel_idx);\n \n \tif (args.AllConstant()) {\ndiff --git a/extension/parquet/column_writer.cpp b/extension/parquet/column_writer.cpp\nindex 2502484b50e8..5735036e6102 100644\n--- a/extension/parquet/column_writer.cpp\n+++ b/extension/parquet/column_writer.cpp\n@@ -717,6 +717,7 @@ void BasicColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {\n \tcolumn_chunk.meta_data.total_compressed_size =\n \t    UnsafeNumericCast<int64_t>(column_writer.GetTotalWritten() - start_offset);\n \tcolumn_chunk.meta_data.total_uncompressed_size = UnsafeNumericCast<int64_t>(total_uncompressed_size);\n+\tstate.row_group.total_byte_size += column_chunk.meta_data.total_uncompressed_size;\n \n \tif (state.bloom_filter) {\n \t\twriter.BufferBloomFilter(state.col_idx, std::move(state.bloom_filter));\ndiff --git a/extension/parquet/parquet_writer.cpp b/extension/parquet/parquet_writer.cpp\nindex f7e3651ececa..f236362780b2 100644\n--- a/extension/parquet/parquet_writer.cpp\n+++ b/extension/parquet/parquet_writer.cpp\n@@ -395,7 +395,6 @@ void ParquetWriter::PrepareRowGroup(ColumnDataCollection &buffer, PreparedRowGro\n \t// set up a new row group for this chunk collection\n \tauto &row_group = result.row_group;\n \trow_group.num_rows = NumericCast<int64_t>(buffer.Count());\n-\trow_group.total_byte_size = NumericCast<int64_t>(buffer.SizeInBytes());\n \trow_group.__isset.file_offset = true;\n \n \tauto &states = result.states;\ndiff --git a/src/catalog/catalog.cpp b/src/catalog/catalog.cpp\nindex 0382b24a8b1d..cb2ea83bbbc4 100644\n--- a/src/catalog/catalog.cpp\n+++ b/src/catalog/catalog.cpp\n@@ -426,7 +426,12 @@ vector<CatalogSearchEntry> GetCatalogEntries(CatalogEntryRetriever &retriever, c\n \t\t\tentries.emplace_back(catalog, schema_name);\n \t\t}\n \t\tif (entries.empty()) {\n-\t\t\tentries.emplace_back(catalog, DEFAULT_SCHEMA);\n+\t\t\tauto catalog_entry = Catalog::GetCatalogEntry(context, catalog);\n+\t\t\tif (catalog_entry) {\n+\t\t\t\tentries.emplace_back(catalog, catalog_entry->GetDefaultSchema());\n+\t\t\t} else {\n+\t\t\t\tentries.emplace_back(catalog, DEFAULT_SCHEMA);\n+\t\t\t}\n \t\t}\n \t} else {\n \t\t// specific catalog and schema provided\n@@ -968,12 +973,16 @@ optional_ptr<SchemaCatalogEntry> Catalog::GetSchema(CatalogEntryRetriever &retri\n \t\t\t// skip if it is not an attached database\n \t\t\tcontinue;\n \t\t}\n-\t\tauto on_not_found = i + 1 == entries.size() ? if_not_found : OnEntryNotFound::RETURN_NULL;\n+\t\tconst auto on_not_found = i + 1 == entries.size() ? if_not_found : OnEntryNotFound::RETURN_NULL;\n \t\tauto result = catalog->GetSchema(retriever.GetContext(), schema_name, on_not_found, error_context);\n \t\tif (result) {\n \t\t\treturn result;\n \t\t}\n \t}\n+\t// Catalog has not been found.\n+\tif (if_not_found == OnEntryNotFound::THROW_EXCEPTION) {\n+\t\tthrow CatalogException(error_context, \"Catalog with name %s does not exist!\", catalog_name);\n+\t}\n \treturn nullptr;\n }\n \n@@ -1073,6 +1082,10 @@ optional_ptr<DependencyManager> Catalog::GetDependencyManager() {\n \treturn nullptr;\n }\n \n+string Catalog::GetDefaultSchema() const {\n+\treturn DEFAULT_SCHEMA;\n+}\n+\n //! Whether this catalog has a default table. Catalogs with a default table can be queries by their catalog name\n bool Catalog::HasDefaultTable() const {\n \treturn !default_table.empty();\ndiff --git a/src/catalog/catalog_search_path.cpp b/src/catalog/catalog_search_path.cpp\nindex 2f2a987519df..793de909b9f6 100644\n--- a/src/catalog/catalog_search_path.cpp\n+++ b/src/catalog/catalog_search_path.cpp\n@@ -165,7 +165,7 @@ void CatalogSearchPath::Set(vector<CatalogSearchEntry> new_paths, CatalogSetPath\n \t\tif (path.catalog.empty()) {\n \t\t\tauto catalog = Catalog::GetCatalogEntry(context, path.schema);\n \t\t\tif (catalog) {\n-\t\t\t\tauto schema = catalog->GetSchema(context, DEFAULT_SCHEMA, OnEntryNotFound::RETURN_NULL);\n+\t\t\t\tauto schema = catalog->GetSchema(context, catalog->GetDefaultSchema(), OnEntryNotFound::RETURN_NULL);\n \t\t\t\tif (schema) {\n \t\t\t\t\tpath.catalog = std::move(path.schema);\n \t\t\t\t\tpath.schema = schema->name;\n@@ -205,6 +205,22 @@ string CatalogSearchPath::GetDefaultSchema(const string &catalog) {\n \treturn DEFAULT_SCHEMA;\n }\n \n+string CatalogSearchPath::GetDefaultSchema(ClientContext &context, const string &catalog) {\n+\tfor (auto &path : paths) {\n+\t\tif (path.catalog == TEMP_CATALOG) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tif (StringUtil::CIEquals(path.catalog, catalog)) {\n+\t\t\treturn path.schema;\n+\t\t}\n+\t}\n+\tauto catalog_entry = Catalog::GetCatalogEntry(context, catalog);\n+\tif (catalog_entry) {\n+\t\treturn catalog_entry->GetDefaultSchema();\n+\t}\n+\treturn DEFAULT_SCHEMA;\n+}\n+\n string CatalogSearchPath::GetDefaultCatalog(const string &schema) {\n \tif (DefaultSchemaGenerator::IsDefaultSchema(schema)) {\n \t\treturn SYSTEM_CATALOG;\ndiff --git a/src/common/enum_util.cpp b/src/common/enum_util.cpp\nindex 0ac5bb0c4e7f..49e7bb2b9b77 100644\n--- a/src/common/enum_util.cpp\n+++ b/src/common/enum_util.cpp\n@@ -83,6 +83,7 @@\n #include \"duckdb/common/types/vector_buffer.hpp\"\n #include \"duckdb/execution/index/art/art.hpp\"\n #include \"duckdb/execution/index/art/node.hpp\"\n+#include \"duckdb/execution/index/bound_index.hpp\"\n #include \"duckdb/execution/operator/csv_scanner/csv_option.hpp\"\n #include \"duckdb/execution/operator/csv_scanner/csv_state.hpp\"\n #include \"duckdb/execution/operator/csv_scanner/quote_rules.hpp\"\n@@ -150,25 +151,6 @@\n \n namespace duckdb {\n \n-const StringUtil::EnumStringLiteral *GetARTAppendModeValues() {\n-\tstatic constexpr StringUtil::EnumStringLiteral values[] {\n-\t\t{ static_cast<uint32_t>(ARTAppendMode::DEFAULT), \"DEFAULT\" },\n-\t\t{ static_cast<uint32_t>(ARTAppendMode::IGNORE_DUPLICATES), \"IGNORE_DUPLICATES\" },\n-\t\t{ static_cast<uint32_t>(ARTAppendMode::INSERT_DUPLICATES), \"INSERT_DUPLICATES\" }\n-\t};\n-\treturn values;\n-}\n-\n-template<>\n-const char* EnumUtil::ToChars<ARTAppendMode>(ARTAppendMode value) {\n-\treturn StringUtil::EnumToString(GetARTAppendModeValues(), 3, \"ARTAppendMode\", static_cast<uint32_t>(value));\n-}\n-\n-template<>\n-ARTAppendMode EnumUtil::FromString<ARTAppendMode>(const char *value) {\n-\treturn static_cast<ARTAppendMode>(StringUtil::StringToEnum(GetARTAppendModeValues(), 3, \"ARTAppendMode\", value));\n-}\n-\n const StringUtil::EnumStringLiteral *GetARTConflictTypeValues() {\n \tstatic constexpr StringUtil::EnumStringLiteral values[] {\n \t\t{ static_cast<uint32_t>(ARTConflictType::NO_CONFLICT), \"NO_CONFLICT\" },\n@@ -1878,6 +1860,25 @@ HLLStorageType EnumUtil::FromString<HLLStorageType>(const char *value) {\n \treturn static_cast<HLLStorageType>(StringUtil::StringToEnum(GetHLLStorageTypeValues(), 2, \"HLLStorageType\", value));\n }\n \n+const StringUtil::EnumStringLiteral *GetIndexAppendModeValues() {\n+\tstatic constexpr StringUtil::EnumStringLiteral values[] {\n+\t\t{ static_cast<uint32_t>(IndexAppendMode::DEFAULT), \"DEFAULT\" },\n+\t\t{ static_cast<uint32_t>(IndexAppendMode::IGNORE_DUPLICATES), \"IGNORE_DUPLICATES\" },\n+\t\t{ static_cast<uint32_t>(IndexAppendMode::INSERT_DUPLICATES), \"INSERT_DUPLICATES\" }\n+\t};\n+\treturn values;\n+}\n+\n+template<>\n+const char* EnumUtil::ToChars<IndexAppendMode>(IndexAppendMode value) {\n+\treturn StringUtil::EnumToString(GetIndexAppendModeValues(), 3, \"IndexAppendMode\", static_cast<uint32_t>(value));\n+}\n+\n+template<>\n+IndexAppendMode EnumUtil::FromString<IndexAppendMode>(const char *value) {\n+\treturn static_cast<IndexAppendMode>(StringUtil::StringToEnum(GetIndexAppendModeValues(), 3, \"IndexAppendMode\", value));\n+}\n+\n const StringUtil::EnumStringLiteral *GetIndexConstraintTypeValues() {\n \tstatic constexpr StringUtil::EnumStringLiteral values[] {\n \t\t{ static_cast<uint32_t>(IndexConstraintType::NONE), \"NONE\" },\ndiff --git a/src/common/random_engine.cpp b/src/common/random_engine.cpp\nindex cf558ea7ad87..78403e0301af 100644\n--- a/src/common/random_engine.cpp\n+++ b/src/common/random_engine.cpp\n@@ -21,7 +21,10 @@ RandomEngine::RandomEngine(int64_t seed) : random_state(make_uniq<RandomState>()\n \tif (seed < 0) {\n #ifdef __linux__\n \t\tidx_t random_seed = 0;\n-\t\tauto result = syscall(SYS_getrandom, &random_seed, sizeof(random_seed), 0);\n+\t\tint result = -1;\n+#if defined(SYS_getrandom)\n+\t\tresult = static_cast<int>(syscall(SYS_getrandom, &random_seed, sizeof(random_seed), 0));\n+#endif\n \t\tif (result == -1) {\n \t\t\t// Something went wrong with the syscall, we use chrono\n \t\t\tconst auto now = std::chrono::high_resolution_clock::now();\ndiff --git a/src/common/serializer/memory_stream.cpp b/src/common/serializer/memory_stream.cpp\nindex e5f0455e3ee4..2b3d0bb1bf4e 100644\n--- a/src/common/serializer/memory_stream.cpp\n+++ b/src/common/serializer/memory_stream.cpp\n@@ -1,14 +1,12 @@\n #include \"duckdb/common/serializer/memory_stream.hpp\"\n \n+#include \"duckdb/common/allocator.hpp\"\n+\n namespace duckdb {\n \n MemoryStream::MemoryStream(idx_t capacity) : position(0), capacity(capacity), owns_data(true) {\n \tD_ASSERT(capacity != 0 && IsPowerOfTwo(capacity));\n-\tauto data_malloc_result = malloc(capacity);\n-\tif (!data_malloc_result) {\n-\t\tthrow std::bad_alloc();\n-\t}\n-\tdata = static_cast<data_ptr_t>(data_malloc_result);\n+\tdata = Allocator::DefaultAllocatorReference()->AllocateData(capacity);\n }\n \n MemoryStream::MemoryStream(data_ptr_t buffer, idx_t capacity)\n@@ -17,7 +15,7 @@ MemoryStream::MemoryStream(data_ptr_t buffer, idx_t capacity)\n \n MemoryStream::~MemoryStream() {\n \tif (owns_data) {\n-\t\tfree(data);\n+\t\tAllocator::DefaultAllocatorReference()->FreeData(data, capacity);\n \t}\n }\n \n@@ -39,7 +37,7 @@ MemoryStream &MemoryStream::operator=(MemoryStream &&other) noexcept {\n \tif (this != &other) {\n \t\t// Free the current data\n \t\tif (owns_data) {\n-\t\t\tfree(data);\n+\t\t\tAllocator::DefaultAllocatorReference()->FreeData(data, capacity);\n \t\t}\n \n \t\t// Move the data from the other stream into this stream\n@@ -58,14 +56,17 @@ MemoryStream &MemoryStream::operator=(MemoryStream &&other) noexcept {\n }\n \n void MemoryStream::WriteData(const_data_ptr_t source, idx_t write_size) {\n+\tconst auto old_capacity = capacity;\n \twhile (position + write_size > capacity) {\n \t\tif (owns_data) {\n \t\t\tcapacity *= 2;\n-\t\t\tdata = static_cast<data_ptr_t>(realloc(data, capacity));\n \t\t} else {\n \t\t\tthrow SerializationException(\"Failed to serialize: not enough space in buffer to fulfill write request\");\n \t\t}\n \t}\n+\tif (capacity != old_capacity) {\n+\t\tdata = Allocator::DefaultAllocatorReference()->ReallocateData(data, old_capacity, capacity);\n+\t}\n \tmemcpy(data + position, source, write_size);\n \tposition += write_size;\n }\ndiff --git a/src/execution/index/art/art.cpp b/src/execution/index/art/art.cpp\nindex 137cc98811a4..1317fb08edd2 100644\n--- a/src/execution/index/art/art.cpp\n+++ b/src/execution/index/art/art.cpp\n@@ -45,7 +45,7 @@ ART::ART(const string &name, const IndexConstraintType index_constraint_type, co\n          const shared_ptr<array<unsafe_unique_ptr<FixedSizeAllocator>, ALLOCATOR_COUNT>> &allocators_ptr,\n          const IndexStorageInfo &info)\n     : BoundIndex(name, ART::TYPE_NAME, index_constraint_type, column_ids, table_io_manager, unbound_expressions, db),\n-      allocators(allocators_ptr), owns_data(false), append_mode(ARTAppendMode::DEFAULT) {\n+      allocators(allocators_ptr), owns_data(false) {\n \n \t// FIXME: Use the new byte representation function to support nested types.\n \tfor (idx_t i = 0; i < types.size(); i++) {\n@@ -480,10 +480,11 @@ bool ART::Construct(unsafe_vector<ARTKey> &keys, unsafe_vector<ARTKey> &row_ids,\n //===--------------------------------------------------------------------===//\n \n ErrorData ART::Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids) {\n-\treturn Insert(l, chunk, row_ids, nullptr);\n+\tIndexAppendInfo info;\n+\treturn Insert(l, chunk, row_ids, info);\n }\n \n-ErrorData ART::Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids, optional_ptr<BoundIndex> delete_index) {\n+ErrorData ART::Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids, IndexAppendInfo &info) {\n \tD_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);\n \tauto row_count = chunk.size();\n \n@@ -493,8 +494,8 @@ ErrorData ART::Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids, optional_\n \tGenerateKeyVectors(allocator, chunk, row_ids, keys, row_id_keys);\n \n \toptional_ptr<ART> delete_art;\n-\tif (delete_index) {\n-\t\tdelete_art = delete_index->Cast<ART>();\n+\tif (info.delete_index) {\n+\t\tdelete_art = info.delete_index->Cast<ART>();\n \t}\n \n \tauto conflict_type = ARTConflictType::NO_CONFLICT;\n@@ -506,7 +507,7 @@ ErrorData ART::Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids, optional_\n \t\tif (keys[i].Empty()) {\n \t\t\tcontinue;\n \t\t}\n-\t\tconflict_type = Insert(tree, keys[i], 0, row_id_keys[i], tree.GetGateStatus(), delete_art);\n+\t\tconflict_type = Insert(tree, keys[i], 0, row_id_keys[i], tree.GetGateStatus(), delete_art, info.append_mode);\n \t\tif (conflict_type != ARTConflictType::NO_CONFLICT) {\n \t\t\tconflict_idx = i;\n \t\t\tbreak;\n@@ -557,27 +558,27 @@ ErrorData ART::Append(IndexLock &l, DataChunk &chunk, Vector &row_ids) {\n \tExecuteExpressions(chunk, expr_chunk);\n \n \t// Now insert the data chunk.\n-\treturn Insert(l, expr_chunk, row_ids, nullptr);\n+\tIndexAppendInfo info;\n+\treturn Insert(l, expr_chunk, row_ids, info);\n }\n \n-ErrorData ART::AppendWithDeleteIndex(IndexLock &l, DataChunk &chunk, Vector &row_ids,\n-                                     optional_ptr<BoundIndex> delete_index) {\n+ErrorData ART::Append(IndexLock &l, DataChunk &chunk, Vector &row_ids, IndexAppendInfo &info) {\n \t// Execute all column expressions before inserting the data chunk.\n \tDataChunk expr_chunk;\n \texpr_chunk.Initialize(Allocator::DefaultAllocator(), logical_types);\n \tExecuteExpressions(chunk, expr_chunk);\n \n \t// Now insert the data chunk.\n-\treturn Insert(l, expr_chunk, row_ids, delete_index);\n+\treturn Insert(l, expr_chunk, row_ids, info);\n }\n \n-void ART::VerifyAppend(DataChunk &chunk, optional_ptr<BoundIndex> delete_index, optional_ptr<ConflictManager> manager) {\n+void ART::VerifyAppend(DataChunk &chunk, IndexAppendInfo &info, optional_ptr<ConflictManager> manager) {\n \tif (manager) {\n \t\tD_ASSERT(manager->LookupType() == VerifyExistenceType::APPEND);\n-\t\treturn VerifyConstraint(chunk, delete_index, *manager);\n+\t\treturn VerifyConstraint(chunk, info, *manager);\n \t}\n \tConflictManager local_manager(VerifyExistenceType::APPEND, chunk.size());\n-\tVerifyConstraint(chunk, delete_index, local_manager);\n+\tVerifyConstraint(chunk, info, local_manager);\n }\n \n void ART::InsertIntoEmpty(Node &node, const ARTKey &key, const idx_t depth, const ARTKey &row_id,\n@@ -598,15 +599,16 @@ void ART::InsertIntoEmpty(Node &node, const ARTKey &key, const idx_t depth, cons\n }\n \n ARTConflictType ART::InsertIntoInlined(Node &node, const ARTKey &key, const idx_t depth, const ARTKey &row_id,\n-                                       const GateStatus status, optional_ptr<ART> delete_art) {\n+                                       const GateStatus status, optional_ptr<ART> delete_art,\n+                                       const IndexAppendMode append_mode) {\n \n-\tif (!IsUnique() || append_mode == ARTAppendMode::INSERT_DUPLICATES) {\n+\tif (!IsUnique() || append_mode == IndexAppendMode::INSERT_DUPLICATES) {\n \t\tLeaf::InsertIntoInlined(*this, node, row_id, depth, status);\n \t\treturn ARTConflictType::NO_CONFLICT;\n \t}\n \n \tif (!delete_art) {\n-\t\tif (append_mode == ARTAppendMode::IGNORE_DUPLICATES) {\n+\t\tif (append_mode == IndexAppendMode::IGNORE_DUPLICATES) {\n \t\t\treturn ARTConflictType::NO_CONFLICT;\n \t\t}\n \t\treturn ARTConflictType::CONSTRAINT;\n@@ -633,14 +635,15 @@ ARTConflictType ART::InsertIntoInlined(Node &node, const ARTKey &key, const idx_\n }\n \n ARTConflictType ART::InsertIntoNode(Node &node, const ARTKey &key, const idx_t depth, const ARTKey &row_id,\n-                                    const GateStatus status, optional_ptr<ART> delete_art) {\n+                                    const GateStatus status, optional_ptr<ART> delete_art,\n+                                    const IndexAppendMode append_mode) {\n \tD_ASSERT(depth < key.len);\n \tauto child = node.GetChildMutable(*this, key[depth]);\n \n \t// Recurse, if a child exists at key[depth].\n \tif (child) {\n \t\tD_ASSERT(child->HasMetadata());\n-\t\tauto conflict_type = Insert(*child, key, depth + 1, row_id, status, delete_art);\n+\t\tauto conflict_type = Insert(*child, key, depth + 1, row_id, status, delete_art, append_mode);\n \t\tnode.ReplaceChild(*this, key[depth], *child);\n \t\treturn conflict_type;\n \t}\n@@ -649,7 +652,7 @@ ARTConflictType ART::InsertIntoNode(Node &node, const ARTKey &key, const idx_t d\n \tif (status == GateStatus::GATE_SET) {\n \t\tNode remainder;\n \t\tauto byte = key[depth];\n-\t\tauto conflict_type = Insert(remainder, key, depth + 1, row_id, status, delete_art);\n+\t\tauto conflict_type = Insert(remainder, key, depth + 1, row_id, status, delete_art, append_mode);\n \t\tNode::InsertChild(*this, node, byte, remainder);\n \t\treturn conflict_type;\n \t}\n@@ -671,7 +674,7 @@ ARTConflictType ART::InsertIntoNode(Node &node, const ARTKey &key, const idx_t d\n }\n \n ARTConflictType ART::Insert(Node &node, const ARTKey &key, idx_t depth, const ARTKey &row_id, const GateStatus status,\n-                            optional_ptr<ART> delete_art) {\n+                            optional_ptr<ART> delete_art, const IndexAppendMode append_mode) {\n \tif (!node.HasMetadata()) {\n \t\tInsertIntoEmpty(node, key, depth, row_id, status);\n \t\treturn ARTConflictType::NO_CONFLICT;\n@@ -688,17 +691,17 @@ ARTConflictType ART::Insert(Node &node, const ARTKey &key, idx_t depth, const AR\n \t\t\t// incoming transaction must fail here.\n \t\t\treturn ARTConflictType::TRANSACTION;\n \t\t}\n-\t\treturn Insert(node, row_id, 0, row_id, GateStatus::GATE_SET, delete_art);\n+\t\treturn Insert(node, row_id, 0, row_id, GateStatus::GATE_SET, delete_art, append_mode);\n \t}\n \n \tauto type = node.GetType();\n \tswitch (type) {\n \tcase NType::LEAF_INLINED: {\n-\t\treturn InsertIntoInlined(node, key, depth, row_id, status, delete_art);\n+\t\treturn InsertIntoInlined(node, key, depth, row_id, status, delete_art, append_mode);\n \t}\n \tcase NType::LEAF: {\n \t\tLeaf::TransformToNested(*this, node);\n-\t\treturn Insert(node, key, depth, row_id, status, delete_art);\n+\t\treturn Insert(node, key, depth, row_id, status, delete_art, append_mode);\n \t}\n \tcase NType::NODE_7_LEAF:\n \tcase NType::NODE_15_LEAF:\n@@ -712,9 +715,9 @@ ARTConflictType ART::Insert(Node &node, const ARTKey &key, idx_t depth, const AR\n \tcase NType::NODE_16:\n \tcase NType::NODE_48:\n \tcase NType::NODE_256:\n-\t\treturn InsertIntoNode(node, key, depth, row_id, status, delete_art);\n+\t\treturn InsertIntoNode(node, key, depth, row_id, status, delete_art, append_mode);\n \tcase NType::PREFIX:\n-\t\treturn Prefix::Insert(*this, node, key, depth, row_id, status, delete_art);\n+\t\treturn Prefix::Insert(*this, node, key, depth, row_id, status, delete_art, append_mode);\n \tdefault:\n \t\tthrow InternalException(\"Invalid node type for ART::Insert.\");\n \t}\n@@ -1119,7 +1122,7 @@ void ART::VerifyLeaf(const Node &leaf, const ARTKey &key, optional_ptr<ART> dele\n \t}\n }\n \n-void ART::VerifyConstraint(DataChunk &chunk, optional_ptr<BoundIndex> delete_index, ConflictManager &manager) {\n+void ART::VerifyConstraint(DataChunk &chunk, IndexAppendInfo &info, ConflictManager &manager) {\n \t// Lock the index during constraint checking.\n \tlock_guard<mutex> l(lock);\n \n@@ -1132,8 +1135,8 @@ void ART::VerifyConstraint(DataChunk &chunk, optional_ptr<BoundIndex> delete_ind\n \tGenerateKeys<>(arena_allocator, expr_chunk, keys);\n \n \toptional_ptr<ART> delete_art;\n-\tif (delete_index) {\n-\t\tdelete_art = delete_index->Cast<ART>();\n+\tif (info.delete_index) {\n+\t\tdelete_art = info.delete_index->Cast<ART>();\n \t}\n \n \toptional_idx conflict_idx;\ndiff --git a/src/execution/index/art/leaf.cpp b/src/execution/index/art/leaf.cpp\nindex 5cde0d5d042a..f5eadf178e9c 100644\n--- a/src/execution/index/art/leaf.cpp\n+++ b/src/execution/index/art/leaf.cpp\n@@ -26,7 +26,7 @@ void Leaf::New(ART &art, reference<Node> &node, const unsafe_vector<ARTKey> &row\n \t// We cannot recurse into the leaf during Construct(...) because row IDs are not sorted.\n \tfor (idx_t i = 0; i < count; i++) {\n \t\tidx_t offset = start + i;\n-\t\tart.Insert(node, row_ids[offset], 0, row_ids[offset], GateStatus::GATE_SET, nullptr);\n+\t\tart.Insert(node, row_ids[offset], 0, row_ids[offset], GateStatus::GATE_SET, nullptr, IndexAppendMode::DEFAULT);\n \t}\n \tnode.get().SetGateStatus(GateStatus::GATE_SET);\n }\n@@ -36,7 +36,7 @@ void Leaf::MergeInlined(ART &art, Node &l_node, Node &r_node) {\n \n \tArenaAllocator arena_allocator(Allocator::Get(art.db));\n \tauto key = ARTKey::CreateARTKey<row_t>(arena_allocator, r_node.GetRowId());\n-\tart.Insert(l_node, key, 0, key, l_node.GetGateStatus(), nullptr);\n+\tart.Insert(l_node, key, 0, key, l_node.GetGateStatus(), nullptr, IndexAppendMode::DEFAULT);\n \tr_node.Clear();\n }\n \n@@ -96,18 +96,14 @@ void Leaf::TransformToNested(ART &art, Node &node) {\n \tArenaAllocator allocator(Allocator::Get(art.db));\n \tNode root = Node();\n \n-\t// Temporarily disable constraint checking.\n-\tif (art.IsUnique() && art.append_mode == ARTAppendMode::DEFAULT) {\n-\t\tart.append_mode = ARTAppendMode::INSERT_DUPLICATES;\n-\t}\n-\n \t// Move all row IDs into the nested leaf.\n \treference<const Node> leaf_ref(node);\n \twhile (leaf_ref.get().HasMetadata()) {\n \t\tauto &leaf = Node::Ref<const Leaf>(art, leaf_ref, LEAF);\n \t\tfor (uint8_t i = 0; i < leaf.count; i++) {\n \t\t\tauto row_id = ARTKey::CreateARTKey<row_t>(allocator, leaf.row_ids[i]);\n-\t\t\tauto conflict_type = art.Insert(root, row_id, 0, row_id, GateStatus::GATE_SET, nullptr);\n+\t\t\tauto conflict_type =\n+\t\t\t    art.Insert(root, row_id, 0, row_id, GateStatus::GATE_SET, nullptr, IndexAppendMode::INSERT_DUPLICATES);\n \t\t\tif (conflict_type != ARTConflictType::NO_CONFLICT) {\n \t\t\t\tthrow InternalException(\"invalid conflict type in Leaf::TransformToNested\");\n \t\t\t}\n@@ -115,7 +111,6 @@ void Leaf::TransformToNested(ART &art, Node &node) {\n \t\tleaf_ref = leaf.ptr;\n \t}\n \n-\tart.append_mode = ARTAppendMode::DEFAULT;\n \troot.SetGateStatus(GateStatus::GATE_SET);\n \tNode::Free(art, node);\n \tnode = root;\ndiff --git a/src/execution/index/art/node.cpp b/src/execution/index/art/node.cpp\nindex 25c1dd5ff6da..1a85ad921fd5 100644\n--- a/src/execution/index/art/node.cpp\n+++ b/src/execution/index/art/node.cpp\n@@ -572,7 +572,7 @@ bool Node::MergeInternal(ART &art, Node &other, const GateStatus status) {\n \t\tArenaAllocator allocator(Allocator::Get(art.db));\n \t\tfor (idx_t i = 0; i < row_ids.size(); i++) {\n \t\t\tauto row_id = ARTKey::CreateARTKey<row_t>(allocator, row_ids[i]);\n-\t\t\tart.Insert(*this, row_id, 0, row_id, GateStatus::GATE_SET, nullptr);\n+\t\t\tart.Insert(*this, row_id, 0, row_id, GateStatus::GATE_SET, nullptr, IndexAppendMode::DEFAULT);\n \t\t}\n \t\treturn true;\n \t}\ndiff --git a/src/execution/index/art/prefix.cpp b/src/execution/index/art/prefix.cpp\nindex f0821be040c3..551171819d16 100644\n--- a/src/execution/index/art/prefix.cpp\n+++ b/src/execution/index/art/prefix.cpp\n@@ -292,7 +292,8 @@ GateStatus Prefix::Split(ART &art, reference<Node> &node, Node &child, const uin\n }\n \n ARTConflictType Prefix::Insert(ART &art, Node &node, const ARTKey &key, idx_t depth, const ARTKey &row_id,\n-                               const GateStatus status, optional_ptr<ART> delete_art) {\n+                               const GateStatus status, optional_ptr<ART> delete_art,\n+                               const IndexAppendMode append_mode) {\n \treference<Node> next(node);\n \tauto pos = TraverseMutable(art, next, key, depth);\n \n@@ -301,7 +302,7 @@ ARTConflictType Prefix::Insert(ART &art, Node &node, const ARTKey &key, idx_t de\n \t// (2) we reach a gate.\n \tif (pos == DConstants::INVALID_INDEX) {\n \t\tif (next.get().GetType() != NType::PREFIX || next.get().GetGateStatus() == GateStatus::GATE_SET) {\n-\t\t\treturn art.Insert(next, key, depth, row_id, status, delete_art);\n+\t\t\treturn art.Insert(next, key, depth, row_id, status, delete_art, append_mode);\n \t\t}\n \t}\n \ndiff --git a/src/execution/index/bound_index.cpp b/src/execution/index/bound_index.cpp\nindex 199cc4bddb49..26933680adc3 100644\n--- a/src/execution/index/bound_index.cpp\n+++ b/src/execution/index/bound_index.cpp\n@@ -38,24 +38,22 @@ ErrorData BoundIndex::Append(DataChunk &chunk, Vector &row_ids) {\n \treturn Append(l, chunk, row_ids);\n }\n \n-ErrorData BoundIndex::AppendWithDeleteIndex(IndexLock &l, DataChunk &chunk, Vector &row_ids,\n-                                            optional_ptr<BoundIndex> delete_index) {\n+ErrorData BoundIndex::Append(IndexLock &l, DataChunk &chunk, Vector &row_ids, IndexAppendInfo &info) {\n \t// Fallback to the old Append.\n \treturn Append(l, chunk, row_ids);\n }\n \n-ErrorData BoundIndex::AppendWithDeleteIndex(DataChunk &chunk, Vector &row_ids, optional_ptr<BoundIndex> delete_index) {\n+ErrorData BoundIndex::Append(DataChunk &chunk, Vector &row_ids, IndexAppendInfo &info) {\n \tIndexLock l;\n \tInitializeLock(l);\n-\treturn AppendWithDeleteIndex(l, chunk, row_ids, delete_index);\n+\treturn Append(l, chunk, row_ids, info);\n }\n \n-void BoundIndex::VerifyAppend(DataChunk &chunk, optional_ptr<BoundIndex> delete_index,\n-                              optional_ptr<ConflictManager> manager) {\n+void BoundIndex::VerifyAppend(DataChunk &chunk, IndexAppendInfo &info, optional_ptr<ConflictManager> manager) {\n \tthrow NotImplementedException(\"this implementation of VerifyAppend does not exist.\");\n }\n \n-void BoundIndex::VerifyConstraint(DataChunk &chunk, optional_ptr<BoundIndex> delete_index, ConflictManager &manager) {\n+void BoundIndex::VerifyConstraint(DataChunk &chunk, IndexAppendInfo &info, ConflictManager &manager) {\n \tthrow NotImplementedException(\"this implementation of VerifyConstraint does not exist.\");\n }\n \n@@ -71,7 +69,7 @@ void BoundIndex::Delete(DataChunk &entries, Vector &row_identifiers) {\n \tDelete(state, entries, row_identifiers);\n }\n \n-ErrorData BoundIndex::Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids, optional_ptr<BoundIndex> delete_index) {\n+ErrorData BoundIndex::Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids, IndexAppendInfo &info) {\n \tthrow NotImplementedException(\"this implementation of Insert does not exist.\");\n }\n \ndiff --git a/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp b/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp\nindex c922f7294b2a..94ef37399510 100644\n--- a/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp\n+++ b/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp\n@@ -126,6 +126,10 @@ StringValueResult::StringValueResult(CSVStates &states, CSVStateMachine &state_m\n \t\t\tSkipBOM();\n \t\t}\n \t}\n+\tignore_empty_values = state_machine.dialect_options.state_machine_options.delimiter.GetValue()[0] != ' ' &&\n+\t                      state_machine.dialect_options.state_machine_options.quote != ' ' &&\n+\t                      state_machine.dialect_options.state_machine_options.escape != ' ' &&\n+\t                      state_machine.dialect_options.state_machine_options.comment != ' ';\n }\n \n StringValueResult::~StringValueResult() {\n@@ -503,8 +507,15 @@ void StringValueResult::AddQuotedValue(StringValueResult &result, const idx_t bu\n \tif (!result.unquoted) {\n \t\tresult.current_errors.Insert(UNTERMINATED_QUOTES, result.cur_col_id, result.chunk_col_id, result.last_position);\n \t}\n-\tAddPossiblyEscapedValue(result, buffer_pos, result.buffer_ptr + result.quoted_position + 1,\n-\t                        buffer_pos - result.quoted_position - 2, buffer_pos < result.last_position.buffer_pos + 2);\n+\t// remove potential empty values\n+\tidx_t length = buffer_pos - result.quoted_position - 1;\n+\twhile (length > 0 && result.ignore_empty_values &&\n+\t       result.buffer_ptr[result.quoted_position + 1 + length - 1] == ' ') {\n+\t\tlength--;\n+\t}\n+\tlength--;\n+\tAddPossiblyEscapedValue(result, buffer_pos, result.buffer_ptr + result.quoted_position + 1, length,\n+\t                        buffer_pos < result.last_position.buffer_pos + 2);\n \tresult.quoted = false;\n }\n \n@@ -1365,8 +1376,12 @@ void StringValueScanner::ProcessOverBufferValue() {\n \tif (!skip_value) {\n \t\tstring_t value;\n \t\tif (result.quoted && !result.comment) {\n-\t\t\tvalue = string_t(over_buffer_string.c_str() + result.quoted_position,\n-\t\t\t                 UnsafeNumericCast<uint32_t>(over_buffer_string.size() - 1 - result.quoted_position));\n+\t\t\tidx_t length = over_buffer_string.size() - 1 - result.quoted_position;\n+\t\t\twhile (length > 0 && result.ignore_empty_values &&\n+\t\t\t       over_buffer_string.c_str()[result.quoted_position + length] == ' ') {\n+\t\t\t\tlength--;\n+\t\t\t}\n+\t\t\tvalue = string_t(over_buffer_string.c_str() + result.quoted_position, UnsafeNumericCast<uint32_t>(length));\n \t\t\tif (result.escaped) {\n \t\t\t\tif (!result.HandleTooManyColumnsError(over_buffer_string.c_str(), over_buffer_string.size())) {\n \t\t\t\t\tconst auto str_ptr = over_buffer_string.c_str() + result.quoted_position;\ndiff --git a/src/execution/operator/persistent/physical_copy_database.cpp b/src/execution/operator/persistent/physical_copy_database.cpp\nindex 6ad354ea2edc..3479f7887ce5 100644\n--- a/src/execution/operator/persistent/physical_copy_database.cpp\n+++ b/src/execution/operator/persistent/physical_copy_database.cpp\n@@ -1,6 +1,8 @@\n #include \"duckdb/execution/operator/persistent/physical_copy_database.hpp\"\n+\n #include \"duckdb/catalog/catalog.hpp\"\n #include \"duckdb/catalog/catalog_entry/schema_catalog_entry.hpp\"\n+#include \"duckdb/catalog/catalog_entry/table_catalog_entry.hpp\"\n #include \"duckdb/planner/binder.hpp\"\n #include \"duckdb/planner/parsed_data/bound_create_table_info.hpp\"\n #include \"duckdb/parser/parsed_data/create_schema_info.hpp\"\n@@ -9,6 +11,8 @@\n #include \"duckdb/parser/parsed_data/create_type_info.hpp\"\n #include \"duckdb/parser/parsed_data/create_view_info.hpp\"\n #include \"duckdb/parser/parsed_data/create_index_info.hpp\"\n+#include \"duckdb/execution/index/unbound_index.hpp\"\n+#include \"duckdb/storage/data_table.hpp\"\n \n namespace duckdb {\n \n@@ -52,7 +56,7 @@ SourceResultType PhysicalCopyDatabase::GetData(ExecutionContext &context, DataCh\n \t\t\tbreak;\n \t\t}\n \t\tcase CatalogType::INDEX_ENTRY: {\n-\t\t\tcatalog.CreateIndex(context.client, create_info->Cast<CreateIndexInfo>());\n+\t\t\t// Skip for now.\n \t\t\tbreak;\n \t\t}\n \t\tdefault:\n@@ -60,6 +64,27 @@ SourceResultType PhysicalCopyDatabase::GetData(ExecutionContext &context, DataCh\n \t\t\t                              CatalogTypeToString(create_info->type));\n \t\t}\n \t}\n+\n+\t// Create the indexes after table creation.\n+\tfor (auto &create_info : info->entries) {\n+\t\tif (!create_info || create_info->type != CatalogType::INDEX_ENTRY) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tcatalog.CreateIndex(context.client, create_info->Cast<CreateIndexInfo>());\n+\n+\t\tauto &create_index_info = create_info->Cast<CreateIndexInfo>();\n+\t\tauto &catalog_table = catalog.GetEntry(context.client, CatalogType::TABLE_ENTRY, create_index_info.schema,\n+\t\t                                       create_index_info.table);\n+\t\tauto &table_entry = catalog_table.Cast<TableCatalogEntry>();\n+\t\tauto &data_table = table_entry.GetStorage();\n+\n+\t\tIndexStorageInfo storage_info(create_index_info.index_name);\n+\t\tstorage_info.options.emplace(\"v1_0_0_storage\", false);\n+\t\tauto unbound_index = make_uniq<UnboundIndex>(create_index_info.Copy(), storage_info,\n+\t\t                                             data_table.GetTableIOManager(), catalog.GetAttached());\n+\t\tdata_table.AddIndex(std::move(unbound_index));\n+\t}\n+\n \treturn SourceResultType::FINISHED;\n }\n \ndiff --git a/src/execution/operator/persistent/physical_delete.cpp b/src/execution/operator/persistent/physical_delete.cpp\nindex 7d494ac0c18f..abb705aa01be 100644\n--- a/src/execution/operator/persistent/physical_delete.cpp\n+++ b/src/execution/operator/persistent/physical_delete.cpp\n@@ -133,12 +133,13 @@ SinkResultType PhysicalDelete::Sink(ExecutionContext &context, DataChunk &chunk,\n \tif (g_state.has_unique_indexes && l_state.delete_chunk.size() != 0) {\n \t\tauto &local_storage = LocalStorage::Get(context.client, table.db);\n \t\tauto storage = local_storage.GetStorage(table);\n+\t\tIndexAppendInfo index_append_info(IndexAppendMode::IGNORE_DUPLICATES, nullptr);\n \t\tstorage->delete_indexes.Scan([&](Index &index) {\n \t\t\tif (!index.IsBound() || !index.IsUnique()) {\n \t\t\t\treturn false;\n \t\t\t}\n \t\t\tauto &bound_index = index.Cast<BoundIndex>();\n-\t\t\tauto error = bound_index.Append(l_state.delete_chunk, row_ids);\n+\t\t\tauto error = bound_index.Append(l_state.delete_chunk, row_ids, index_append_info);\n \t\t\tif (error.HasError()) {\n \t\t\t\tthrow InternalException(\"failed to update delete ART in physical delete: \", error.Message());\n \t\t\t}\ndiff --git a/src/execution/operator/persistent/physical_insert.cpp b/src/execution/operator/persistent/physical_insert.cpp\nindex cb21cc26b91c..a3c6619ed5e9 100644\n--- a/src/execution/operator/persistent/physical_insert.cpp\n+++ b/src/execution/operator/persistent/physical_insert.cpp\n@@ -80,17 +80,15 @@ InsertGlobalState::InsertGlobalState(ClientContext &context, const vector<Logica\n     : table(table), insert_count(0), initialized(false), return_collection(context, return_types) {\n }\n \n-InsertLocalState::InsertLocalState(ClientContext &context, const vector<LogicalType> &types_p,\n+InsertLocalState::InsertLocalState(ClientContext &context, const vector<LogicalType> &types,\n                                    const vector<unique_ptr<Expression>> &bound_defaults,\n                                    const vector<unique_ptr<BoundConstraint>> &bound_constraints)\n     : default_executor(context, bound_defaults), bound_constraints(bound_constraints) {\n \n \tauto &allocator = Allocator::Get(context);\n-\n-\ttypes = types_p;\n-\tauto initialize = vector<bool>(types.size(), false);\n-\tupdate_chunk.Initialize(allocator, types, initialize);\n-\tappend_chunk.Initialize(allocator, types, initialize);\n+\tinsert_chunk.Initialize(allocator, types);\n+\tupdate_chunk.Initialize(allocator, types);\n+\tappend_chunk.Initialize(allocator, types);\n }\n \n ConstraintState &InsertLocalState::GetConstraintState(DataTable &table, TableCatalogEntry &table_ref) {\n@@ -187,10 +185,8 @@ static void CombineExistingAndInsertTuples(DataChunk &result, DataChunk &scan_ch\n \tauto &insert_types = op.insert_types;\n \n \tif (types_to_fetch.empty()) {\n-\t\t// We have not scanned the initial table, so we duplicate the initial chunk.\n-\t\tconst auto &types = input_chunk.GetTypes();\n-\t\tauto initialize = vector<bool>(types.size(), false);\n-\t\tresult.Initialize(client, types, initialize, input_chunk.size());\n+\t\t// We have not scanned the initial table, so we can just duplicate the initial chunk\n+\t\tresult.Initialize(client, input_chunk.GetTypes());\n \t\tresult.Reference(input_chunk);\n \t\tresult.SetCardinality(input_chunk);\n \t\treturn;\n@@ -200,7 +196,7 @@ static void CombineExistingAndInsertTuples(DataChunk &result, DataChunk &scan_ch\n \tcombined_types.insert(combined_types.end(), insert_types.begin(), insert_types.end());\n \tcombined_types.insert(combined_types.end(), types_to_fetch.begin(), types_to_fetch.end());\n \n-\tresult.Initialize(client, combined_types, input_chunk.size());\n+\tresult.Initialize(client, combined_types);\n \tresult.Reset();\n \t// Add the VALUES list\n \tfor (idx_t i = 0; i < insert_types.size(); i++) {\n@@ -227,8 +223,8 @@ static void CombineExistingAndInsertTuples(DataChunk &result, DataChunk &scan_ch\n \tresult.SetCardinality(input_chunk.size());\n }\n \n-static void CreateUpdateChunk(ExecutionContext &context, DataChunk &chunk, Vector &row_ids, DataChunk &update_chunk,\n-                              const PhysicalInsert &op) {\n+static void CreateUpdateChunk(ExecutionContext &context, DataChunk &chunk, TableCatalogEntry &table, Vector &row_ids,\n+                              DataChunk &update_chunk, const PhysicalInsert &op) {\n \n \tauto &do_update_condition = op.do_update_condition;\n \tauto &set_types = op.set_types;\n@@ -284,7 +280,7 @@ static idx_t PerformOnConflictAction(InsertLocalState &lstate, ExecutionContext\n \n \tauto &set_columns = op.set_columns;\n \tDataChunk update_chunk;\n-\tCreateUpdateChunk(context, chunk, row_ids, update_chunk, op);\n+\tCreateUpdateChunk(context, chunk, table, row_ids, update_chunk, op);\n \tauto &data_table = table.GetStorage();\n \n \t// Perform the UPDATE on the (global) storage.\n@@ -488,9 +484,7 @@ static idx_t HandleInsertConflicts(TableCatalogEntry &table, ExecutionContext &c\n \tDataChunk combined_chunk; // contains conflict_chunk + scan_chunk (wide)\n \n \t// Filter out everything but the conflicting rows\n-\tconst auto &types = tuples.GetTypes();\n-\tauto initialize = vector<bool>(types.size(), false);\n-\tconflict_chunk.Initialize(context.client, types, initialize, tuples.size());\n+\tconflict_chunk.Initialize(context.client, tuples.GetTypes());\n \tconflict_chunk.Reference(tuples);\n \tconflict_chunk.Slice(conflicts.Selection(), conflicts.Count());\n \tconflict_chunk.SetCardinality(conflicts.Count());\n@@ -501,7 +495,7 @@ static idx_t HandleInsertConflicts(TableCatalogEntry &table, ExecutionContext &c\n \t\tD_ASSERT(scan_chunk.size() == 0);\n \t\t// When these values are required for the conditions or the SET expressions,\n \t\t// then we scan the existing table for the conflicting tuples, using the rowids\n-\t\tscan_chunk.Initialize(context.client, types_to_fetch, conflicts.Count());\n+\t\tscan_chunk.Initialize(context.client, types_to_fetch);\n \t\tfetch_state = make_uniq<ColumnFetchState>();\n \t\tif (GLOBAL) {\n \t\t\tauto &transaction = DuckTransaction::Get(context.client, table.catalog);\n@@ -534,7 +528,7 @@ static idx_t HandleInsertConflicts(TableCatalogEntry &table, ExecutionContext &c\n \treturn affected_tuples;\n }\n \n-idx_t PhysicalInsert::OnConflictHandling(TableCatalogEntry &table, ExecutionContext &context,\n+idx_t PhysicalInsert::OnConflictHandling(TableCatalogEntry &table, ExecutionContext &context, InsertGlobalState &gstate,\n                                          InsertLocalState &lstate) const {\n \tauto &data_table = table.GetStorage();\n \tauto &local_storage = LocalStorage::Get(context.client, data_table.db);\n@@ -634,21 +628,6 @@ SinkResultType PhysicalInsert::Sink(ExecutionContext &context, DataChunk &chunk,\n \n \tauto &table = gstate.table;\n \tauto &storage = table.GetStorage();\n-\tif (lstate.init_insert_chunk) {\n-\t\tauto initialize = vector<bool>(lstate.types.size(), false);\n-\t\tif (!column_index_map.empty()) {\n-\t\t\tfor (auto &col : table.GetColumns().Physical()) {\n-\t\t\t\tauto storage_idx = col.StorageOid();\n-\t\t\t\tauto mapped_index = column_index_map[col.Physical()];\n-\t\t\t\tif (mapped_index == DConstants::INVALID_INDEX) {\n-\t\t\t\t\tinitialize[storage_idx] = true;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\tauto &allocator = Allocator::Get(context.client);\n-\t\tlstate.insert_chunk.Initialize(allocator, lstate.types, initialize, chunk.size());\n-\t\tlstate.init_insert_chunk = false;\n-\t}\n \tPhysicalInsert::ResolveDefaults(table, chunk, column_index_map, lstate.default_executor, lstate.insert_chunk);\n \n \tif (!parallel) {\n@@ -663,7 +642,7 @@ SinkResultType PhysicalInsert::Sink(ExecutionContext &context, DataChunk &chunk,\n \t\t\t// so it should not be added to the RETURNING chunk\n \t\t\tgstate.return_collection.Append(lstate.insert_chunk);\n \t\t}\n-\t\tidx_t updated_tuples = OnConflictHandling(table, context, lstate);\n+\t\tidx_t updated_tuples = OnConflictHandling(table, context, gstate, lstate);\n \t\tif (action_type == OnConflictAction::NOTHING && return_chunk) {\n \t\t\t// Because we didn't add to the RETURNING chunk yet\n \t\t\t// we add the tuples that did not get filtered out now\n@@ -694,7 +673,7 @@ SinkResultType PhysicalInsert::Sink(ExecutionContext &context, DataChunk &chunk,\n \t\t\tlstate.local_collection->InitializeAppend(lstate.local_append_state);\n \t\t\tlstate.writer = &gstate.table.GetStorage().CreateOptimisticWriter(context.client);\n \t\t}\n-\t\tOnConflictHandling(table, context, lstate);\n+\t\tOnConflictHandling(table, context, gstate, lstate);\n \t\tD_ASSERT(action_type != OnConflictAction::UPDATE);\n \n \t\tauto new_row_group = lstate.local_collection->Append(lstate.insert_chunk, lstate.local_append_state);\ndiff --git a/src/execution/operator/schema/physical_create_art_index.cpp b/src/execution/operator/schema/physical_create_art_index.cpp\nindex e34e49de475e..8f9b2657e0a8 100644\n--- a/src/execution/operator/schema/physical_create_art_index.cpp\n+++ b/src/execution/operator/schema/physical_create_art_index.cpp\n@@ -88,7 +88,8 @@ SinkResultType PhysicalCreateARTIndex::SinkUnsorted(OperatorSinkInput &input) co\n \t// Insert each key and its corresponding row ID.\n \tfor (idx_t i = 0; i < row_count; i++) {\n \t\tauto status = art.tree.GetGateStatus();\n-\t\tauto conflict_type = art.Insert(art.tree, l_state.keys[i], 0, l_state.row_ids[i], status, nullptr);\n+\t\tauto conflict_type =\n+\t\t    art.Insert(art.tree, l_state.keys[i], 0, l_state.row_ids[i], status, nullptr, IndexAppendMode::DEFAULT);\n \t\tD_ASSERT(conflict_type != ARTConflictType::TRANSACTION);\n \t\tif (conflict_type == ARTConflictType::CONSTRAINT) {\n \t\t\tthrow ConstraintException(\"Data contains duplicates on indexed column(s)\");\ndiff --git a/src/execution/sample/reservoir_sample.cpp b/src/execution/sample/reservoir_sample.cpp\nindex 334b613d1d0a..58056920eaf8 100644\n--- a/src/execution/sample/reservoir_sample.cpp\n+++ b/src/execution/sample/reservoir_sample.cpp\n@@ -749,6 +749,7 @@ void ReservoirSample::AddToReservoir(DataChunk &chunk) {\n \n \tif (chunk_sel.size == 0) {\n \t\t// not adding any samples\n+\t\tbase_reservoir_sample->num_entries_seen_total += chunk.size();\n \t\treturn;\n \t}\n \tidx_t size = chunk_sel.size;\ndiff --git a/src/include/duckdb/catalog/catalog.hpp b/src/include/duckdb/catalog/catalog.hpp\nindex 121b047cb781..96679f2a9153 100644\n--- a/src/include/duckdb/catalog/catalog.hpp\n+++ b/src/include/duckdb/catalog/catalog.hpp\n@@ -315,7 +315,11 @@ class Catalog {\n \t\treturn CatalogLookupBehavior::STANDARD;\n \t}\n \n+\t//! Returns the default schema of the catalog\n+\tvirtual string GetDefaultSchema() const;\n+\n \t//! The default table is used for `SELECT * FROM <catalog_name>;`\n+\t//! FIXME: these should be virtual methods\n \tDUCKDB_API bool HasDefaultTable() const;\n \tDUCKDB_API void SetDefaultTable(const string &schema, const string &name);\n \tDUCKDB_API string GetDefaultTable() const;\ndiff --git a/src/include/duckdb/catalog/catalog_search_path.hpp b/src/include/duckdb/catalog/catalog_search_path.hpp\nindex ff0def0fcd30..f128a4962e75 100644\n--- a/src/include/duckdb/catalog/catalog_search_path.hpp\n+++ b/src/include/duckdb/catalog/catalog_search_path.hpp\n@@ -53,7 +53,9 @@ class CatalogSearchPath {\n \t\treturn set_paths;\n \t}\n \tDUCKDB_API const CatalogSearchEntry &GetDefault();\n+\t//! FIXME: this method is deprecated\n \tDUCKDB_API string GetDefaultSchema(const string &catalog);\n+\tDUCKDB_API string GetDefaultSchema(ClientContext &context, const string &catalog);\n \tDUCKDB_API string GetDefaultCatalog(const string &schema);\n \n \tDUCKDB_API vector<string> GetSchemasForCatalog(const string &catalog);\ndiff --git a/src/include/duckdb/common/enum_util.hpp b/src/include/duckdb/common/enum_util.hpp\nindex accdea5d42eb..379a23969c55 100644\n--- a/src/include/duckdb/common/enum_util.hpp\n+++ b/src/include/duckdb/common/enum_util.hpp\n@@ -32,8 +32,6 @@ struct EnumUtil {\n     static string ToString(T value) { return string(ToChars<T>(value)); }\n };\n \n-enum class ARTAppendMode : uint8_t;\n-\n enum class ARTConflictType : uint8_t;\n \n enum class AccessMode : uint8_t;\n@@ -188,6 +186,8 @@ enum class GateStatus : uint8_t;\n \n enum class HLLStorageType : uint8_t;\n \n+enum class IndexAppendMode : uint8_t;\n+\n enum class IndexConstraintType : uint8_t;\n \n enum class InsertColumnOrder : uint8_t;\n@@ -391,9 +391,6 @@ enum class WindowBoundary : uint8_t;\n enum class WindowExcludeMode : uint8_t;\n \n \n-template<>\n-const char* EnumUtil::ToChars<ARTAppendMode>(ARTAppendMode value);\n-\n template<>\n const char* EnumUtil::ToChars<ARTConflictType>(ARTConflictType value);\n \n@@ -625,6 +622,9 @@ const char* EnumUtil::ToChars<GateStatus>(GateStatus value);\n template<>\n const char* EnumUtil::ToChars<HLLStorageType>(HLLStorageType value);\n \n+template<>\n+const char* EnumUtil::ToChars<IndexAppendMode>(IndexAppendMode value);\n+\n template<>\n const char* EnumUtil::ToChars<IndexConstraintType>(IndexConstraintType value);\n \n@@ -929,9 +929,6 @@ template<>\n const char* EnumUtil::ToChars<WindowExcludeMode>(WindowExcludeMode value);\n \n \n-template<>\n-ARTAppendMode EnumUtil::FromString<ARTAppendMode>(const char *value);\n-\n template<>\n ARTConflictType EnumUtil::FromString<ARTConflictType>(const char *value);\n \n@@ -1163,6 +1160,9 @@ GateStatus EnumUtil::FromString<GateStatus>(const char *value);\n template<>\n HLLStorageType EnumUtil::FromString<HLLStorageType>(const char *value);\n \n+template<>\n+IndexAppendMode EnumUtil::FromString<IndexAppendMode>(const char *value);\n+\n template<>\n IndexConstraintType EnumUtil::FromString<IndexConstraintType>(const char *value);\n \ndiff --git a/src/include/duckdb/execution/index/art/art.hpp b/src/include/duckdb/execution/index/art/art.hpp\nindex 31a560ad5842..d6f41c1c4d92 100644\n--- a/src/include/duckdb/execution/index/art/art.hpp\n+++ b/src/include/duckdb/execution/index/art/art.hpp\n@@ -16,7 +16,6 @@ namespace duckdb {\n \n enum class VerifyExistenceType : uint8_t { APPEND = 0, APPEND_FK = 1, DELETE_FK = 2 };\n enum class ARTConflictType : uint8_t { NO_CONFLICT = 0, CONSTRAINT = 1, TRANSACTION = 2 };\n-enum class ARTAppendMode : uint8_t { DEFAULT = 0, IGNORE_DUPLICATES = 1, INSERT_DUPLICATES = 2 };\n \n class ConflictManager;\n class ARTKey;\n@@ -62,8 +61,6 @@ class ART : public BoundIndex {\n \tbool owns_data;\n \t//! The number of bytes fitting in the prefix.\n \tuint8_t prefix_count;\n-\t//! The append mode.\n-\tARTAppendMode append_mode;\n \n public:\n \t//! Try to initialize a scan on the ART with the given expression and filter.\n@@ -74,21 +71,19 @@ class ART : public BoundIndex {\n \n \t//! Appends data to the locked index.\n \tErrorData Append(IndexLock &l, DataChunk &chunk, Vector &row_ids) override;\n-\t//! Appends data to the locked index and verifies constraint violations against a delete index.\n-\tErrorData AppendWithDeleteIndex(IndexLock &l, DataChunk &chunk, Vector &row_ids,\n-\t                                optional_ptr<BoundIndex> delete_index) override;\n+\t//! Appends data to the locked index and verifies constraint violations.\n+\tErrorData Append(IndexLock &l, DataChunk &chunk, Vector &row_ids, IndexAppendInfo &info) override;\n \n \t//! Internally inserts a chunk.\n \tARTConflictType Insert(Node &node, const ARTKey &key, idx_t depth, const ARTKey &row_id, const GateStatus status,\n-\t                       optional_ptr<ART> delete_art);\n+\t                       optional_ptr<ART> delete_art, const IndexAppendMode append_mode);\n \t//! Insert a chunk.\n \tErrorData Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids) override;\n-\t//! Insert a chunk and verifies constraint violations against a delete index.\n-\tErrorData Insert(IndexLock &l, DataChunk &data, Vector &row_ids, optional_ptr<BoundIndex> delete_index) override;\n+\t//! Insert a chunk and verifies constraint violations.\n+\tErrorData Insert(IndexLock &l, DataChunk &data, Vector &row_ids, IndexAppendInfo &info) override;\n \n \t//! Verify that data can be appended to the index without a constraint violation.\n-\tvoid VerifyAppend(DataChunk &chunk, optional_ptr<BoundIndex> delete_index,\n-\t                  optional_ptr<ConflictManager> manager) override;\n+\tvoid VerifyAppend(DataChunk &chunk, IndexAppendInfo &info, optional_ptr<ConflictManager> manager) override;\n \n \t//! Delete a chunk from the ART.\n \tvoid Delete(IndexLock &lock, DataChunk &entries, Vector &row_ids) override;\n@@ -131,15 +126,17 @@ class ART : public BoundIndex {\n \tvoid InsertIntoEmpty(Node &node, const ARTKey &key, const idx_t depth, const ARTKey &row_id,\n \t                     const GateStatus status);\n \tARTConflictType InsertIntoInlined(Node &node, const ARTKey &key, const idx_t depth, const ARTKey &row_id,\n-\t                                  const GateStatus status, optional_ptr<ART> delete_art);\n+\t                                  const GateStatus status, optional_ptr<ART> delete_art,\n+\t                                  const IndexAppendMode append_mode);\n \tARTConflictType InsertIntoNode(Node &node, const ARTKey &key, const idx_t depth, const ARTKey &row_id,\n-\t                               const GateStatus status, optional_ptr<ART> delete_art);\n+\t                               const GateStatus status, optional_ptr<ART> delete_art,\n+\t                               const IndexAppendMode append_mode);\n \n \tstring GenerateErrorKeyName(DataChunk &input, idx_t row);\n \tstring GenerateConstraintErrorMessage(VerifyExistenceType verify_type, const string &key_name);\n \tvoid VerifyLeaf(const Node &leaf, const ARTKey &key, optional_ptr<ART> delete_art, ConflictManager &manager,\n \t                optional_idx &conflict_idx, idx_t i);\n-\tvoid VerifyConstraint(DataChunk &chunk, optional_ptr<BoundIndex> delete_index, ConflictManager &manager) override;\n+\tvoid VerifyConstraint(DataChunk &chunk, IndexAppendInfo &info, ConflictManager &manager) override;\n \tstring GetConstraintViolationMessage(VerifyExistenceType verify_type, idx_t failed_index,\n \t                                     DataChunk &input) override;\n \ndiff --git a/src/include/duckdb/execution/index/art/prefix.hpp b/src/include/duckdb/execution/index/art/prefix.hpp\nindex 5d656c742e8e..38b57514c862 100644\n--- a/src/include/duckdb/execution/index/art/prefix.hpp\n+++ b/src/include/duckdb/execution/index/art/prefix.hpp\n@@ -85,7 +85,8 @@ class Prefix {\n \n \t//! Insert a key into a prefix.\n \tstatic ARTConflictType Insert(ART &art, Node &node, const ARTKey &key, idx_t depth, const ARTKey &row_id,\n-\t                              const GateStatus status, optional_ptr<ART> delete_art);\n+\t                              const GateStatus status, optional_ptr<ART> delete_art,\n+\t                              const IndexAppendMode append_mode);\n \n \t//! Returns the string representation of the node, or only traverses and verifies the node and its subtree\n \tstatic string VerifyAndToString(ART &art, const Node &node, const bool only_verify);\ndiff --git a/src/include/duckdb/execution/index/bound_index.hpp b/src/include/duckdb/execution/index/bound_index.hpp\nindex 522c7c78e4e7..4ac5d4b25a0c 100644\n--- a/src/include/duckdb/execution/index/bound_index.hpp\n+++ b/src/include/duckdb/execution/index/bound_index.hpp\n@@ -28,6 +28,19 @@ class ConflictManager;\n struct IndexLock;\n struct IndexScanState;\n \n+enum class IndexAppendMode : uint8_t { DEFAULT = 0, IGNORE_DUPLICATES = 1, INSERT_DUPLICATES = 2 };\n+\n+class IndexAppendInfo {\n+public:\n+\tIndexAppendInfo() : append_mode(IndexAppendMode::DEFAULT), delete_index(nullptr) {};\n+\tIndexAppendInfo(const IndexAppendMode append_mode, const optional_ptr<BoundIndex> delete_index)\n+\t    : append_mode(append_mode), delete_index(delete_index) {};\n+\n+public:\n+\tIndexAppendMode append_mode;\n+\toptional_ptr<BoundIndex> delete_index;\n+};\n+\n //! The index is an abstract base class that serves as the basis for indexes\n class BoundIndex : public Index {\n public:\n@@ -71,17 +84,15 @@ class BoundIndex : public Index {\n \tvirtual ErrorData Append(IndexLock &l, DataChunk &chunk, Vector &row_ids) = 0;\n \t//! Obtains a lock and calls Append while holding that lock.\n \tErrorData Append(DataChunk &chunk, Vector &row_ids);\n-\t//! Appends data to the locked index and verifies constraint violations against a delete index.\n-\tvirtual ErrorData AppendWithDeleteIndex(IndexLock &l, DataChunk &chunk, Vector &row_ids,\n-\t                                        optional_ptr<BoundIndex> delete_index);\n-\t//! Obtains a lock and calls Append with an delete_index while holding that lock.\n-\tErrorData AppendWithDeleteIndex(DataChunk &chunk, Vector &row_ids, optional_ptr<BoundIndex> delete_index);\n+\t//! Appends data to the locked index and verifies constraint violations.\n+\tvirtual ErrorData Append(IndexLock &l, DataChunk &chunk, Vector &row_ids, IndexAppendInfo &info);\n+\t//! Obtains a lock and calls Append while holding that lock.\n+\tErrorData Append(DataChunk &chunk, Vector &row_ids, IndexAppendInfo &info);\n \n \t//! Verify that data can be appended to the index without a constraint violation.\n-\tvirtual void VerifyAppend(DataChunk &chunk, optional_ptr<BoundIndex> delete_index,\n-\t                          optional_ptr<ConflictManager> manager);\n+\tvirtual void VerifyAppend(DataChunk &chunk, IndexAppendInfo &info, optional_ptr<ConflictManager> manager);\n \t//! Verifies the constraint for a chunk of data.\n-\tvirtual void VerifyConstraint(DataChunk &chunk, optional_ptr<BoundIndex> delete_index, ConflictManager &manager);\n+\tvirtual void VerifyConstraint(DataChunk &chunk, IndexAppendInfo &info, ConflictManager &manager);\n \n \t//! Deletes all data from the index. The lock obtained from InitializeLock must be held\n \tvirtual void CommitDrop(IndexLock &index_lock) = 0;\n@@ -94,8 +105,8 @@ class BoundIndex : public Index {\n \n \t//! Insert a chunk.\n \tvirtual ErrorData Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids) = 0;\n-\t//! Insert a chunk and verifies constraint violations against a delete index.\n-\tvirtual ErrorData Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids, optional_ptr<BoundIndex> delete_index);\n+\t//! Insert a chunk and verifies constraint violations.\n+\tvirtual ErrorData Insert(IndexLock &l, DataChunk &chunk, Vector &row_ids, IndexAppendInfo &info);\n \n \t//! Merge another index into this index. The lock obtained from InitializeLock must be held, and the other\n \t//! index must also be locked during the merge\ndiff --git a/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp b/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp\nindex 7ce7ab5776b4..6bddd944f2a5 100644\n--- a/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp\n+++ b/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp\n@@ -217,6 +217,8 @@ class StringValueResult : public ScannerResult {\n \t//! (i.e., non-comment) line.\n \tbool first_line_is_comment = false;\n \n+\tbool ignore_empty_values = true;\n+\n \t//! Specialized code for quoted values, makes sure to remove quotes and escapes\n \tstatic inline void AddQuotedValue(StringValueResult &result, const idx_t buffer_pos);\n \t//! Specialized code for possibly escaped values, makes sure to remove escapes\ndiff --git a/src/include/duckdb/execution/operator/persistent/physical_insert.hpp b/src/include/duckdb/execution/operator/persistent/physical_insert.hpp\nindex 04d202af24d9..ccb113c4f939 100644\n--- a/src/include/duckdb/execution/operator/persistent/physical_insert.hpp\n+++ b/src/include/duckdb/execution/operator/persistent/physical_insert.hpp\n@@ -38,7 +38,7 @@ class InsertGlobalState : public GlobalSinkState {\n class InsertLocalState : public LocalSinkState {\n public:\n public:\n-\tInsertLocalState(ClientContext &context, const vector<LogicalType> &types_p,\n+\tInsertLocalState(ClientContext &context, const vector<LogicalType> &types,\n \t                 const vector<unique_ptr<Expression>> &bound_defaults,\n \t                 const vector<unique_ptr<BoundConstraint>> &bound_constraints);\n \n@@ -47,12 +47,8 @@ class InsertLocalState : public LocalSinkState {\n \tTableDeleteState &GetDeleteState(DataTable &table, TableCatalogEntry &table_ref, ClientContext &context);\n \n public:\n-\t//! The to-be-inserted chunk.\n-\t//! We initialize it lazily, as we need to know which columns will be references and which will be set to their\n-\t//! default values.\n+\t//! The chunk that ends up getting inserted\n \tDataChunk insert_chunk;\n-\tbool init_insert_chunk = true;\n-\tvector<LogicalType> types;\n \t//! The chunk containing the tuples that become an update (if DO UPDATE)\n \tDataChunk update_chunk;\n \tExpressionExecutor default_executor;\n@@ -174,7 +170,8 @@ class PhysicalInsert : public PhysicalOperator {\n \t//! Returns the amount of updated tuples\n \tvoid CreateUpdateChunk(ExecutionContext &context, DataChunk &chunk, TableCatalogEntry &table, Vector &row_ids,\n \t                       DataChunk &result) const;\n-\tidx_t OnConflictHandling(TableCatalogEntry &table, ExecutionContext &context, InsertLocalState &lstate) const;\n+\tidx_t OnConflictHandling(TableCatalogEntry &table, ExecutionContext &context, InsertGlobalState &gstate,\n+\t                         InsertLocalState &lstate) const;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/execution/reservoir_sample.hpp b/src/include/duckdb/execution/reservoir_sample.hpp\nindex b794328bb0fb..bc815b11f9de 100644\n--- a/src/include/duckdb/execution/reservoir_sample.hpp\n+++ b/src/include/duckdb/execution/reservoir_sample.hpp\n@@ -172,7 +172,13 @@ class ReservoirSample : public BlockingSample {\n \tstatic constexpr const SampleType TYPE = SampleType::RESERVOIR_SAMPLE;\n \n \tconstexpr static idx_t FIXED_SAMPLE_SIZE_MULTIPLIER = 10;\n-\tconstexpr static idx_t FAST_TO_SLOW_THRESHOLD = 60;\n+\t// size is small enough, then the threshold to switch\n+\t// MinValue between std vec size and fixed sample size.\n+\t// During 'fast' sampling, we want every new vector to have the potential\n+\t// to add to the sample. If the threshold is too far below the standard vector size, then\n+\t// samples in the sample have a higher weight than new samples coming in.\n+\t// i.e during vector_size=2, 2 new samples will not be significant compared 2048 samples from 204800 tuples.\n+\tconstexpr static idx_t FAST_TO_SLOW_THRESHOLD = MinValue<idx_t>(STANDARD_VECTOR_SIZE, 60);\n \n \t// If the table has less than 204800 rows, this is the percentage\n \t// of values we save when serializing/returning a sample.\ndiff --git a/src/include/duckdb/storage/data_table.hpp b/src/include/duckdb/storage/data_table.hpp\nindex 32418d911da0..6a0b97727384 100644\n--- a/src/include/duckdb/storage/data_table.hpp\n+++ b/src/include/duckdb/storage/data_table.hpp\n@@ -107,8 +107,8 @@ class DataTable {\n \t                 const vector<unique_ptr<BoundConstraint>> &bound_constraints, Vector &row_ids,\n \t                 DataChunk &delete_chunk);\n \t//! Append a chunk to the transaction-local storage of this table.\n-\tvoid LocalAppend(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,\n-\t                 const vector<unique_ptr<BoundConstraint>> &bound_constraints);\n+\tvoid LocalWALAppend(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,\n+\t                    const vector<unique_ptr<BoundConstraint>> &bound_constraints);\n \t//! Append a column data collection with default values to the transaction-local storage of this table.\n \tvoid LocalAppend(TableCatalogEntry &table, ClientContext &context, ColumnDataCollection &collection,\n \t                 const vector<unique_ptr<BoundConstraint>> &bound_constraints,\n@@ -167,9 +167,10 @@ class DataTable {\n \n \t//! Append a chunk with the row ids [row_start, ..., row_start + chunk.size()] to all indexes of the table.\n \t//! Returns empty ErrorData, if the append was successful.\n-\tErrorData AppendToIndexes(optional_ptr<TableIndexList> delete_indexes, DataChunk &chunk, row_t row_start);\n+\tErrorData AppendToIndexes(optional_ptr<TableIndexList> delete_indexes, DataChunk &chunk, row_t row_start,\n+\t                          const IndexAppendMode index_append_mode);\n \tstatic ErrorData AppendToIndexes(TableIndexList &indexes, optional_ptr<TableIndexList> delete_indexes,\n-\t                                 DataChunk &chunk, row_t row_start);\n+\t                                 DataChunk &chunk, row_t row_start, const IndexAppendMode index_append_mode);\n \t//! Remove a chunk with the row ids [row_start, ..., row_start + chunk.size()] from all indexes of the table\n \tvoid RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start);\n \t//! Remove the chunk with the specified set of row identifiers from all indexes of the table\ndiff --git a/src/include/duckdb/storage/index_storage_info.hpp b/src/include/duckdb/storage/index_storage_info.hpp\nindex d5e71ae48183..27312c8244c1 100644\n--- a/src/include/duckdb/storage/index_storage_info.hpp\n+++ b/src/include/duckdb/storage/index_storage_info.hpp\n@@ -62,6 +62,10 @@ struct IndexStorageInfo {\n \tBlockPointer root_block_ptr;\n \n \t//! Returns true, if IndexStorageInfo holds information to deserialize an index.\n+\t//! Note that the name can be misleading - any index that is empty (no nodes, etc.) might\n+\t//! also have neither a root_block_ptr nor allocator_infos.\n+\t//! Ensure that your index constructor initializes an empty index correctly without the\n+\t//! need for these fields.\n \tbool IsValid() const {\n \t\treturn root_block_ptr.IsValid() || !allocator_infos.empty();\n \t}\ndiff --git a/src/include/duckdb/transaction/local_storage.hpp b/src/include/duckdb/transaction/local_storage.hpp\nindex 453a7ce440ab..4213fa0fadce 100644\n--- a/src/include/duckdb/transaction/local_storage.hpp\n+++ b/src/include/duckdb/transaction/local_storage.hpp\n@@ -48,6 +48,8 @@ class LocalTableStorage : public enable_shared_from_this<LocalTableStorage> {\n \tTableIndexList append_indexes;\n \t//! The set of delete indexes.\n \tTableIndexList delete_indexes;\n+\t//! Set to INSERT_DUPLICATES, if we are skipping constraint checking during, e.g., WAL replay.\n+\tIndexAppendMode index_append_mode = IndexAppendMode::DEFAULT;\n \t//! The number of deleted rows\n \tidx_t deleted_rows;\n \t//! The main optimistic data writer\ndiff --git a/src/main/extension/extension_helper.cpp b/src/main/extension/extension_helper.cpp\nindex c7b613226a10..f42d43aa8571 100644\n--- a/src/main/extension/extension_helper.cpp\n+++ b/src/main/extension/extension_helper.cpp\n@@ -143,9 +143,9 @@ static const char *const auto_install[] = {\"motherduck\", \"postgres_scanner\", \"my\n \n // TODO: unify with new autoload mechanism\n bool ExtensionHelper::AllowAutoInstall(const string &extension) {\n-\tauto lcase = StringUtil::Lower(extension);\n+\tauto extension_name = ApplyExtensionAlias(extension);\n \tfor (idx_t i = 0; auto_install[i]; i++) {\n-\t\tif (lcase == auto_install[i]) {\n+\t\tif (extension_name == auto_install[i]) {\n \t\t\treturn true;\n \t\t}\n \t}\ndiff --git a/src/optimizer/column_lifetime_analyzer.cpp b/src/optimizer/column_lifetime_analyzer.cpp\nindex efb3e684d473..8b6f1152bf8c 100644\n--- a/src/optimizer/column_lifetime_analyzer.cpp\n+++ b/src/optimizer/column_lifetime_analyzer.cpp\n@@ -102,6 +102,12 @@ void ColumnLifetimeAnalyzer::VisitOperator(LogicalOperator &op) {\n \t\tGenerateProjectionMap(op.children[1]->GetColumnBindings(), rhs_unused, comp_join.right_projection_map);\n \t\treturn;\n \t}\n+\tcase LogicalOperatorType::LOGICAL_INSERT:\n+\tcase LogicalOperatorType::LOGICAL_UPDATE:\n+\tcase LogicalOperatorType::LOGICAL_DELETE:\n+\t\t//! When RETURNING is used, a PROJECTION is the top level operator for INSERTS, UPDATES, and DELETES\n+\t\t//! We still need to project all values from these operators so the projection\n+\t\t//! on top of them can select from only the table values being inserted.\n \tcase LogicalOperatorType::LOGICAL_UNION:\n \tcase LogicalOperatorType::LOGICAL_EXCEPT:\n \tcase LogicalOperatorType::LOGICAL_INTERSECT:\ndiff --git a/src/optimizer/filter_combiner.cpp b/src/optimizer/filter_combiner.cpp\nindex a72354d7df91..45d7c06a06f2 100644\n--- a/src/optimizer/filter_combiner.cpp\n+++ b/src/optimizer/filter_combiner.cpp\n@@ -78,6 +78,7 @@ FilterResult FilterCombiner::AddConstantComparison(vector<ExpressionValueInforma\n \t\t\treturn FilterResult::SUCCESS;\n \t\tcase ValueComparisonResult::UNSATISFIABLE_CONDITION:\n \t\t\t// combination of filters is unsatisfiable: prune the entire branch\n+\t\t\tinfo_list.push_back(info);\n \t\t\treturn FilterResult::UNSATISFIABLE;\n \t\tdefault:\n \t\t\t// prune nothing, move to the next condition\n@@ -792,11 +793,15 @@ FilterResult FilterCombiner::AddBoundComparisonFilter(Expression &expr) {\n \t\tauto transitive_filter = FindTransitiveFilter(non_scalar);\n \t\tif (transitive_filter != nullptr) {\n \t\t\t// try to add transitive filters\n-\t\t\tif (AddTransitiveFilters(transitive_filter->Cast<BoundComparisonExpression>()) ==\n-\t\t\t    FilterResult::UNSUPPORTED) {\n+\t\t\tauto transitive_result = AddTransitiveFilters(transitive_filter->Cast<BoundComparisonExpression>());\n+\t\t\tif (transitive_result == FilterResult::UNSUPPORTED) {\n \t\t\t\t// in case of unsuccessful re-add filter into remaining ones\n \t\t\t\tremaining_filters.push_back(std::move(transitive_filter));\n \t\t\t}\n+\t\t\tif (transitive_result == FilterResult::UNSATISFIABLE) {\n+\t\t\t\t// in case transitive filter is unsatisfiable - abort filter pushdown\n+\t\t\t\treturn FilterResult::UNSATISFIABLE;\n+\t\t\t}\n \t\t}\n \t\treturn ret;\n \t} else {\n@@ -1067,10 +1072,15 @@ FilterResult FilterCombiner::AddTransitiveFilters(BoundComparisonExpression &com\n \t\t\tif (transitive_filter != nullptr) {\n \t\t\t\t// try to add transitive filters\n \t\t\t\tauto &transitive_cast = transitive_filter->Cast<BoundComparisonExpression>();\n-\t\t\t\tif (AddTransitiveFilters(transitive_cast, false) == FilterResult::UNSUPPORTED) {\n+\t\t\t\tauto transitive_result = AddTransitiveFilters(transitive_cast, false);\n+\t\t\t\tif (transitive_result == FilterResult::UNSUPPORTED) {\n \t\t\t\t\t// in case of unsuccessful re-add filter into remaining ones\n \t\t\t\t\tremaining_filters.push_back(std::move(transitive_filter));\n \t\t\t\t}\n+\t\t\t\tif (transitive_result == FilterResult::UNSATISFIABLE) {\n+\t\t\t\t\t// while adding transitive filters we discovered the filter is unsatisfisable - we can prune\n+\t\t\t\t\treturn FilterResult::UNSATISFIABLE;\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \t\treturn FilterResult::SUCCESS;\ndiff --git a/src/optimizer/late_materialization.cpp b/src/optimizer/late_materialization.cpp\nindex 81d4881cd754..aa4e5c4c1fe4 100644\n--- a/src/optimizer/late_materialization.cpp\n+++ b/src/optimizer/late_materialization.cpp\n@@ -1,5 +1,6 @@\n #include \"duckdb/optimizer/late_materialization.hpp\"\n #include \"duckdb/planner/operator/logical_comparison_join.hpp\"\n+#include \"duckdb/planner/operator/logical_filter.hpp\"\n #include \"duckdb/planner/operator/logical_get.hpp\"\n #include \"duckdb/planner/operator/logical_limit.hpp\"\n #include \"duckdb/planner/operator/logical_order.hpp\"\n@@ -61,6 +62,8 @@ ColumnBinding LateMaterialization::ConstructRHS(unique_ptr<LogicalOperator> &op)\n \t// we have reached the logical get - now we need to push the row-id column (if it is not yet projected out)\n \tauto &get = child.get().Cast<LogicalGet>();\n \tauto row_id_idx = GetOrInsertRowId(get);\n+\tidx_t column_count = get.projection_ids.empty() ? get.GetColumnIds().size() : get.projection_ids.size();\n+\tD_ASSERT(column_count == get.GetColumnBindings().size());\n \n \t// the row id has been projected - now project it up the stack\n \tColumnBinding row_id_binding(get.table_index, row_id_idx);\n@@ -74,11 +77,18 @@ ColumnBinding LateMaterialization::ConstructRHS(unique_ptr<LogicalOperator> &op)\n \t\t\t    make_uniq<BoundColumnRefExpression>(\"rowid\", get.GetRowIdType(), row_id_binding));\n \t\t\t// modify the row-id-binding to push to the new projection\n \t\t\trow_id_binding = ColumnBinding(proj.table_index, proj.expressions.size() - 1);\n+\t\t\tcolumn_count = proj.expressions.size();\n \t\t\tbreak;\n \t\t}\n-\t\tcase LogicalOperatorType::LOGICAL_FILTER:\n-\t\t\t// column bindings pass-through this operator as-is\n+\t\tcase LogicalOperatorType::LOGICAL_FILTER: {\n+\t\t\tauto &filter = op.Cast<LogicalFilter>();\n+\t\t\t// column bindings pass-through this operator as-is UNLESS the filter has a projection map\n+\t\t\tif (filter.HasProjectionMap()) {\n+\t\t\t\t// if the filter has a projection map, we need to project the new column\n+\t\t\t\tfilter.projection_map.push_back(column_count - 1);\n+\t\t\t}\n \t\t\tbreak;\n+\t\t}\n \t\tdefault:\n \t\t\tthrow InternalException(\"Unsupported logical operator in LateMaterialization::ConstructRHS\");\n \t\t}\n@@ -212,12 +222,13 @@ bool LateMaterialization::TryLateMaterialization(unique_ptr<LogicalOperator> &op\n \t\t\tchild = *child.get().children[0];\n \t\t\tbreak;\n \t\t}\n-\t\tcase LogicalOperatorType::LOGICAL_FILTER:\n+\t\tcase LogicalOperatorType::LOGICAL_FILTER: {\n \t\t\t// visit filter expressions - we need these columns\n \t\t\tVisitOperatorExpressions(child.get());\n \t\t\t// continue into child\n \t\t\tchild = *child.get().children[0];\n \t\t\tbreak;\n+\t\t}\n \t\tdefault:\n \t\t\t// unsupported operator for late materialization\n \t\t\treturn false;\ndiff --git a/src/parser/transform/expression/transform_columnref.cpp b/src/parser/transform/expression/transform_columnref.cpp\nindex a232219fa14c..dfbbc6f121fc 100644\n--- a/src/parser/transform/expression/transform_columnref.cpp\n+++ b/src/parser/transform/expression/transform_columnref.cpp\n@@ -96,6 +96,7 @@ unique_ptr<ParsedExpression> Transformer::TransformStarExpression(duckdb_libpgqu\n \t\t\tresult->relation_name = child_star.relation_name;\n \t\t\tresult->exclude_list = std::move(child_star.exclude_list);\n \t\t\tresult->replace_list = std::move(child_star.replace_list);\n+\t\t\tresult->rename_list = std::move(child_star.rename_list);\n \t\t\tresult->expr.reset();\n \t\t} else if (result->expr->GetExpressionType() == ExpressionType::LAMBDA) {\n \t\t\tvector<unique_ptr<ParsedExpression>> children;\ndiff --git a/src/planner/binder/statement/bind_copy_database.cpp b/src/planner/binder/statement/bind_copy_database.cpp\nindex f05445e906e4..d2c0a03fb8c7 100644\n--- a/src/planner/binder/statement/bind_copy_database.cpp\n+++ b/src/planner/binder/statement/bind_copy_database.cpp\n@@ -43,7 +43,6 @@ unique_ptr<LogicalOperator> Binder::BindCopyDatabaseSchema(Catalog &from_databas\n \t\tinfo->entries.push_back(std::move(create_info));\n \t}\n \n-\t// FIXME: copy indexes\n \treturn make_uniq<LogicalCopyDatabase>(std::move(info));\n }\n \ndiff --git a/src/planner/binder/statement/bind_create.cpp b/src/planner/binder/statement/bind_create.cpp\nindex 4f1a43a308ba..053ceaef4f56 100644\n--- a/src/planner/binder/statement/bind_create.cpp\n+++ b/src/planner/binder/statement/bind_create.cpp\n@@ -98,7 +98,7 @@ SchemaCatalogEntry &Binder::BindSchema(CreateInfo &info) {\n \t\tinfo.catalog = default_entry.catalog;\n \t\tinfo.schema = default_entry.schema;\n \t} else if (IsInvalidSchema(info.schema)) {\n-\t\tinfo.schema = search_path->GetDefaultSchema(info.catalog);\n+\t\tinfo.schema = search_path->GetDefaultSchema(context, info.catalog);\n \t} else if (IsInvalidCatalog(info.catalog)) {\n \t\tinfo.catalog = search_path->GetDefaultCatalog(info.schema);\n \t}\ndiff --git a/src/planner/binder/tableref/bind_table_function.cpp b/src/planner/binder/tableref/bind_table_function.cpp\nindex 26dd86c9dfd1..29d68f3fc3ab 100644\n--- a/src/planner/binder/tableref/bind_table_function.cpp\n+++ b/src/planner/binder/tableref/bind_table_function.cpp\n@@ -126,8 +126,7 @@ bool Binder::BindTableFunctionParameters(TableFunctionCatalogEntry &table_functi\n \t\t    child->GetExpressionType() == ExpressionType::SUBQUERY) {\n \t\t\tD_ASSERT(table_function.functions.Size() == 1);\n \t\t\tauto fun = table_function.functions.GetFunctionByOffset(0);\n-\t\t\tif (table_function.functions.Size() != 1 || fun.arguments.empty() ||\n-\t\t\t    fun.arguments[0].id() != LogicalTypeId::TABLE) {\n+\t\t\tif (table_function.functions.Size() != 1 || fun.arguments.empty()) {\n \t\t\t\tthrow BinderException(\n \t\t\t\t    \"Only table-in-out functions can have subquery parameters - %s only accepts constant parameters\",\n \t\t\t\t    fun.name);\ndiff --git a/src/planner/subquery/flatten_dependent_join.cpp b/src/planner/subquery/flatten_dependent_join.cpp\nindex a297a7855dae..bb5db60a98bd 100644\n--- a/src/planner/subquery/flatten_dependent_join.cpp\n+++ b/src/planner/subquery/flatten_dependent_join.cpp\n@@ -401,6 +401,7 @@ unique_ptr<LogicalOperator> FlattenDependentJoins::PushDownDependentJoinInternal\n \t\t\t\t// only right has correlation: push into right\n \t\t\t\tplan->children[1] = PushDownDependentJoinInternal(std::move(plan->children[1]),\n \t\t\t\t                                                  parent_propagate_null_values, lateral_depth);\n+\t\t\t\tdelim_offset += plan->children[0]->GetColumnBindings().size();\n \t\t\t\t// Remove the correlated columns coming from outside for current join node\n \t\t\t\treturn plan;\n \t\t\t}\n@@ -419,6 +420,7 @@ unique_ptr<LogicalOperator> FlattenDependentJoins::PushDownDependentJoinInternal\n \t\t\t\t// only right has correlation: push into right\n \t\t\t\tplan->children[1] = PushDownDependentJoinInternal(std::move(plan->children[1]),\n \t\t\t\t                                                  parent_propagate_null_values, lateral_depth);\n+\t\t\t\tdelim_offset += plan->children[0]->GetColumnBindings().size();\n \t\t\t\treturn plan;\n \t\t\t}\n \t\t} else if (join.join_type == JoinType::MARK) {\ndiff --git a/src/storage/data_table.cpp b/src/storage/data_table.cpp\nindex cf35519cca82..168ee3009b64 100644\n--- a/src/storage/data_table.cpp\n+++ b/src/storage/data_table.cpp\n@@ -679,9 +679,11 @@ void DataTable::VerifyUniqueIndexes(TableIndexList &indexes, optional_ptr<LocalT\n \n \t\t\tif (storage) {\n \t\t\t\tauto delete_index = storage->delete_indexes.Find(art.GetIndexName());\n-\t\t\t\tart.VerifyAppend(chunk, delete_index, nullptr);\n+\t\t\t\tIndexAppendInfo index_append_info(IndexAppendMode::DEFAULT, delete_index);\n+\t\t\t\tart.VerifyAppend(chunk, index_append_info, nullptr);\n \t\t\t} else {\n-\t\t\t\tart.VerifyAppend(chunk, nullptr, nullptr);\n+\t\t\t\tIndexAppendInfo index_append_info;\n+\t\t\t\tart.VerifyAppend(chunk, index_append_info, nullptr);\n \t\t\t}\n \t\t\treturn false;\n \t\t});\n@@ -712,8 +714,10 @@ void DataTable::VerifyUniqueIndexes(TableIndexList &indexes, optional_ptr<LocalT\n \tmanager->SetMode(ConflictManagerMode::SCAN);\n \tauto &matched_indexes = manager->MatchedIndexes();\n \tauto &matched_delete_indexes = manager->MatchedDeleteIndexes();\n+\tIndexAppendInfo index_append_info(IndexAppendMode::DEFAULT, nullptr);\n \tfor (idx_t i = 0; i < matched_indexes.size(); i++) {\n-\t\tmatched_indexes[i].get().VerifyAppend(chunk, matched_delete_indexes[i], *manager);\n+\t\tindex_append_info.delete_index = matched_delete_indexes[i];\n+\t\tmatched_indexes[i].get().VerifyAppend(chunk, index_append_info, *manager);\n \t}\n \n \t// Scan the other indexes and throw, if there are any conflicts.\n@@ -728,9 +732,11 @@ void DataTable::VerifyUniqueIndexes(TableIndexList &indexes, optional_ptr<LocalT\n \n \t\tif (storage) {\n \t\t\tauto delete_index = storage->delete_indexes.Find(art.GetIndexName());\n-\t\t\tart.VerifyAppend(chunk, delete_index, *manager);\n+\t\t\tIndexAppendInfo index_append_info(IndexAppendMode::DEFAULT, delete_index);\n+\t\t\tart.VerifyAppend(chunk, index_append_info, *manager);\n \t\t} else {\n-\t\t\tart.VerifyAppend(chunk, nullptr, *manager);\n+\t\t\tIndexAppendInfo index_append_info;\n+\t\t\tart.VerifyAppend(chunk, index_append_info, *manager);\n \t\t}\n \t\treturn false;\n \t});\n@@ -863,13 +869,14 @@ void DataTable::LocalMerge(ClientContext &context, RowGroupCollection &collectio\n \tlocal_storage.LocalMerge(*this, collection);\n }\n \n-void DataTable::LocalAppend(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,\n-                            const vector<unique_ptr<BoundConstraint>> &bound_constraints) {\n+void DataTable::LocalWALAppend(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,\n+                               const vector<unique_ptr<BoundConstraint>> &bound_constraints) {\n \tLocalAppendState append_state;\n \tauto &storage = table.GetStorage();\n \tstorage.InitializeLocalAppend(append_state, table, context, bound_constraints);\n \n-\tstorage.LocalAppend(append_state, context, chunk, false);\n+\tstorage.LocalAppend(append_state, context, chunk, true);\n+\tappend_state.storage->index_append_mode = IndexAppendMode::INSERT_DUPLICATES;\n \tstorage.FinalizeLocalAppend(append_state);\n }\n \n@@ -1109,7 +1116,7 @@ void DataTable::RevertAppend(DuckTransaction &transaction, idx_t start_row, idx_\n // Indexes\n //===--------------------------------------------------------------------===//\n ErrorData DataTable::AppendToIndexes(TableIndexList &indexes, optional_ptr<TableIndexList> delete_indexes,\n-                                     DataChunk &chunk, row_t row_start) {\n+                                     DataChunk &chunk, row_t row_start, const IndexAppendMode index_append_mode) {\n \tErrorData error;\n \tif (indexes.Empty()) {\n \t\treturn error;\n@@ -1137,7 +1144,8 @@ ErrorData DataTable::AppendToIndexes(TableIndexList &indexes, optional_ptr<Table\n \t\t}\n \n \t\ttry {\n-\t\t\terror = index.AppendWithDeleteIndex(chunk, row_ids, delete_index);\n+\t\t\tIndexAppendInfo index_append_info(index_append_mode, delete_index);\n+\t\t\terror = index.Append(chunk, row_ids, index_append_info);\n \t\t} catch (std::exception &ex) {\n \t\t\terror = ErrorData(ex);\n \t\t}\n@@ -1160,9 +1168,10 @@ ErrorData DataTable::AppendToIndexes(TableIndexList &indexes, optional_ptr<Table\n \treturn error;\n }\n \n-ErrorData DataTable::AppendToIndexes(optional_ptr<TableIndexList> delete_indexes, DataChunk &chunk, row_t row_start) {\n+ErrorData DataTable::AppendToIndexes(optional_ptr<TableIndexList> delete_indexes, DataChunk &chunk, row_t row_start,\n+                                     const IndexAppendMode index_append_mode) {\n \tD_ASSERT(is_root);\n-\treturn AppendToIndexes(info->indexes, delete_indexes, chunk, row_start);\n+\treturn AppendToIndexes(info->indexes, delete_indexes, chunk, row_start, index_append_mode);\n }\n \n void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {\ndiff --git a/src/storage/local_storage.cpp b/src/storage/local_storage.cpp\nindex 71b20bd10059..93326ed6726f 100644\n--- a/src/storage/local_storage.cpp\n+++ b/src/storage/local_storage.cpp\n@@ -41,7 +41,6 @@ LocalTableStorage::LocalTableStorage(ClientContext &context, DataTable &table)\n \t\t// Create a delete index and a local index.\n \t\tauto delete_index = make_uniq<ART>(art.GetIndexName(), constraint_type, art.GetColumnIds(),\n \t\t                                   art.table_io_manager, std::move(delete_expressions), art.db);\n-\t\tdelete_index->append_mode = ARTAppendMode::IGNORE_DUPLICATES;\n \t\tdelete_indexes.AddIndex(std::move(delete_index));\n \n \t\tauto index = make_uniq<ART>(art.GetIndexName(), constraint_type, art.GetColumnIds(), art.table_io_manager,\n@@ -152,7 +151,7 @@ ErrorData LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, RowGr\n \t\t}\n \t\tmock_chunk.SetCardinality(chunk);\n \t\t// append this chunk to the indexes of the table\n-\t\terror = DataTable::AppendToIndexes(index_list, nullptr, mock_chunk, start_row);\n+\t\terror = DataTable::AppendToIndexes(index_list, nullptr, mock_chunk, start_row, index_append_mode);\n \t\tif (error.HasError()) {\n \t\t\treturn false;\n \t\t}\n@@ -173,7 +172,7 @@ void LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, TableAppen\n \t\t// appending: need to scan entire\n \t\trow_groups->Scan(transaction, [&](DataChunk &chunk) -> bool {\n \t\t\t// append this chunk to the indexes of the table\n-\t\t\terror = table.AppendToIndexes(delete_indexes, chunk, append_state.current_row);\n+\t\t\terror = table.AppendToIndexes(delete_indexes, chunk, append_state.current_row, index_append_mode);\n \t\t\tif (error.HasError()) {\n \t\t\t\treturn false;\n \t\t\t}\n@@ -186,6 +185,7 @@ void LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, TableAppen\n \t\tauto &index_list = data_table_info->GetIndexes();\n \t\terror = AppendToIndexes(transaction, *row_groups, index_list, table.GetTypes(), append_state.current_row);\n \t}\n+\n \tif (error.HasError()) {\n \t\t// need to revert all appended row ids\n \t\trow_t current_row = append_state.row_start;\n@@ -386,7 +386,8 @@ void LocalTableStorage::AppendToDeleteIndexes(Vector &row_ids, DataChunk &delete\n \t\tif (!art.IsUnique()) {\n \t\t\treturn false;\n \t\t}\n-\t\tauto result = art.Cast<BoundIndex>().Append(delete_chunk, row_ids);\n+\t\tIndexAppendInfo index_append_info(IndexAppendMode::IGNORE_DUPLICATES, nullptr);\n+\t\tauto result = art.Cast<BoundIndex>().Append(delete_chunk, row_ids, index_append_info);\n \t\tif (result.HasError()) {\n \t\t\tthrow InternalException(\"unexpected constraint violation on delete ART: \", result.Message());\n \t\t}\n@@ -401,7 +402,7 @@ void LocalStorage::Append(LocalAppendState &state, DataChunk &chunk) {\n \tidx_t base_id = offset + state.append_state.total_append_count;\n \n \tauto error = DataTable::AppendToIndexes(storage->append_indexes, storage->delete_indexes, chunk,\n-\t                                        NumericCast<row_t>(base_id));\n+\t                                        NumericCast<row_t>(base_id), storage->index_append_mode);\n \tif (error.HasError()) {\n \t\terror.Throw();\n \t}\ndiff --git a/src/storage/table_index_list.cpp b/src/storage/table_index_list.cpp\nindex b46ddaf1989e..abcb9a7d144a 100644\n--- a/src/storage/table_index_list.cpp\n+++ b/src/storage/table_index_list.cpp\n@@ -173,9 +173,11 @@ void TableIndexList::VerifyForeignKey(optional_ptr<LocalTableStorage> storage, c\n \tD_ASSERT(index && index->IsBound());\n \tif (storage) {\n \t\tauto delete_index = storage->delete_indexes.Find(index->GetIndexName());\n-\t\tindex->Cast<BoundIndex>().VerifyConstraint(chunk, delete_index, conflict_manager);\n+\t\tIndexAppendInfo index_append_info(IndexAppendMode::DEFAULT, delete_index);\n+\t\tindex->Cast<BoundIndex>().VerifyConstraint(chunk, index_append_info, conflict_manager);\n \t} else {\n-\t\tindex->Cast<BoundIndex>().VerifyConstraint(chunk, nullptr, conflict_manager);\n+\t\tIndexAppendInfo index_append_info;\n+\t\tindex->Cast<BoundIndex>().VerifyConstraint(chunk, index_append_info, conflict_manager);\n \t}\n }\n \n@@ -201,10 +203,9 @@ vector<IndexStorageInfo> TableIndexList::GetStorageInfos(const case_insensitive_\n \t\t}\n \n \t\tauto info = index->Cast<UnboundIndex>().GetStorageInfo();\n-\t\tD_ASSERT(info.IsValid() && !info.name.empty());\n+\t\tD_ASSERT(!info.name.empty());\n \t\tinfos.push_back(info);\n \t}\n-\n \treturn infos;\n }\n \ndiff --git a/src/storage/wal_replay.cpp b/src/storage/wal_replay.cpp\nindex 0b6894854ea2..ac2a1bb57fed 100644\n--- a/src/storage/wal_replay.cpp\n+++ b/src/storage/wal_replay.cpp\n@@ -740,7 +740,7 @@ void WriteAheadLogDeserializer::ReplayInsert() {\n \t// Append to the current table without constraint verification.\n \tvector<unique_ptr<BoundConstraint>> bound_constraints;\n \tauto &storage = state.current_table->GetStorage();\n-\tstorage.LocalAppend(*state.current_table, context, chunk, bound_constraints);\n+\tstorage.LocalWALAppend(*state.current_table, context, chunk, bound_constraints);\n }\n \n static void MarkBlocksAsUsed(BlockManager &manager, const PersistentColumnData &col_data) {\ndiff --git a/tools/pythonpkg/duckdb/experimental/spark/sql/functions.py b/tools/pythonpkg/duckdb/experimental/spark/sql/functions.py\nindex 4d01c69e5993..f0cbf1dbfc12 100644\n--- a/tools/pythonpkg/duckdb/experimental/spark/sql/functions.py\n+++ b/tools/pythonpkg/duckdb/experimental/spark/sql/functions.py\n@@ -5617,7 +5617,7 @@ def to_timestamp(col: \"ColumnOrName\", format: Optional[str] = None) -> Column:\n     >>> df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect()\n     [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n     \"\"\"\n-    return _to_date_or_timestamp(col, _types.TimestampType(), format)\n+    return _to_date_or_timestamp(col, _types.TimestampNTZType(), format)\n \n \n def to_timestamp_ltz(\n@@ -5649,7 +5649,7 @@ def to_timestamp_ltz(\n     ... # doctest: +SKIP\n     [Row(r=datetime.datetime(2016, 12, 31, 0, 0))]\n     \"\"\"\n-    return _to_date_or_timestamp(timestamp, _types.TimestampType(), format)\n+    return _to_date_or_timestamp(timestamp, _types.TimestampNTZType(), format)\n \n \n def to_timestamp_ntz(\n@@ -5731,7 +5731,7 @@ def substr(\n \n \n def _unix_diff(col: \"ColumnOrName\", part: str) -> Column:\n-    return _invoke_function_over_columns(\"date_diff\", lit(part), lit(\"1970-01-01 00:00:00+00:00\").cast(\"timestamptz\"), col)\n+    return _invoke_function_over_columns(\"date_diff\", lit(part), lit(\"1970-01-01 00:00:00+00:00\").cast(\"timestamp\"), col)\n \n def unix_date(col: \"ColumnOrName\") -> Column:\n     \"\"\"Returns the number of days since 1970-01-01.\ndiff --git a/tools/shell/shell.cpp b/tools/shell/shell.cpp\nindex 53880de8a55d..45252c86e95b 100644\n--- a/tools/shell/shell.cpp\n+++ b/tools/shell/shell.cpp\n@@ -4004,7 +4004,7 @@ MetadataResult ToggleTimer(ShellState &state, const char **azArg, idx_t nArg) {\n }\n \n MetadataResult ShowVersion(ShellState &state, const char **azArg, idx_t nArg) {\n-\tutf8_printf(state.out, \"SQLite %s %s\\n\" /*extra-version-info*/, sqlite3_libversion(), sqlite3_sourceid());\n+\tutf8_printf(state.out, \"DuckDB %s %s\\n\" /*extra-version-info*/, sqlite3_libversion(), sqlite3_sourceid());\n #define CTIMEOPT_VAL_(opt) #opt\n #define CTIMEOPT_VAL(opt)  CTIMEOPT_VAL_(opt)\n #if defined(__clang__) && defined(__clang_major__)\n",
  "test_patch": "diff --git a/scripts/sqllogictest/__init__.py b/scripts/sqllogictest/__init__.py\nindex 02ae67a6f99c..6053ce836ec1 100644\n--- a/scripts/sqllogictest/__init__.py\n+++ b/scripts/sqllogictest/__init__.py\n@@ -20,6 +20,7 @@\n     Sleep,\n     SleepUnit,\n     Skip,\n+    Unzip,\n     Unskip,\n )\n from .decorator import SkipIf, OnlyIf\n@@ -50,6 +51,7 @@\n     Sleep,\n     SleepUnit,\n     Skip,\n+    Unzip,\n     Unskip,\n     SkipIf,\n     OnlyIf,\ndiff --git a/scripts/sqllogictest/parser/parser.py b/scripts/sqllogictest/parser/parser.py\nindex 349d63cb1e34..37049dc6a1fe 100644\n--- a/scripts/sqllogictest/parser/parser.py\n+++ b/scripts/sqllogictest/parser/parser.py\n@@ -23,6 +23,7 @@\n     Reconnect,\n     Sleep,\n     Skip,\n+    Unzip,\n     Unskip,\n     SortStyle,\n )\n@@ -86,6 +87,7 @@ def __init__(self):\n             TokenType.SQLLOGIC_RESTART: self.statement_restart,\n             TokenType.SQLLOGIC_RECONNECT: self.statement_reconnect,\n             TokenType.SQLLOGIC_SLEEP: self.statement_sleep,\n+            TokenType.SQLLOGIC_UNZIP: self.statement_unzip,\n             TokenType.SQLLOGIC_INVALID: None,\n         }\n         self.DECORATORS = {\n@@ -405,6 +407,29 @@ def statement_sleep(self, header: Token) -> Optional[BaseStatement]:\n             raise self.fail(f\"Unrecognized sleep mode - expected {create_formatted_list(options)}\")\n         return Sleep(header, self.current_line + 1, sleep_duration, sleep_unit)\n \n+    def statement_unzip(self, header: Token) -> Optional[BaseStatement]:\n+        params = header.parameters\n+        if len(params) != 1 and len(params) != 2:\n+            docs = \"\"\"\n+                unzip requires 1 parameter, the path to a (g)zipped file.\n+                Optionally a destination location can be provided, defaulting to '__TEST_DIR__/<base_name>'\n+            \"\"\"\n+            self.fail(docs)\n+\n+        source = params[0]\n+\n+        accepted_filetypes = {'.gz'}\n+\n+        basename = os.path.basename(source)\n+        stem, extension = os.path.splitext(basename)\n+        if extension not in accepted_filetypes:\n+            accepted_options = \", \".join(list(accepted_filetypes))\n+            self.fail(\n+                f\"unzip: input does not end in a valid file extension ({extension}), accepted options are: {accepted_options}\"\n+            )\n+        destination = params[1] if len(params) == 2 else f'__TEST_DIR__/{stem}'\n+        return Unzip(header, self.current_line + 1, source, destination)\n+\n     # Decorators\n \n     def decorator_skipif(self, token: Token) -> Optional[BaseDecorator]:\n@@ -531,6 +556,7 @@ def is_single_line_statement(self, token):\n             TokenType.SQLLOGIC_RESTART,\n             TokenType.SQLLOGIC_RECONNECT,\n             TokenType.SQLLOGIC_SLEEP,\n+            TokenType.SQLLOGIC_UNZIP,\n         ]\n \n         if token.type in single_line_statements:\n@@ -566,6 +592,7 @@ def command_to_token(self, token):\n             \"load\": TokenType.SQLLOGIC_LOAD,\n             \"restart\": TokenType.SQLLOGIC_RESTART,\n             \"reconnect\": TokenType.SQLLOGIC_RECONNECT,\n+            \"unzip\": TokenType.SQLLOGIC_UNZIP,\n             \"sleep\": TokenType.SQLLOGIC_SLEEP,\n         }\n \ndiff --git a/scripts/sqllogictest/result.py b/scripts/sqllogictest/result.py\nindex aa4e8158bd47..1893389b1d1a 100644\n--- a/scripts/sqllogictest/result.py\n+++ b/scripts/sqllogictest/result.py\n@@ -21,6 +21,7 @@\n     Sleep,\n     SleepUnit,\n     Skip,\n+    Unzip,\n     SortStyle,\n     Unskip,\n )\n@@ -764,6 +765,7 @@ def __init__(\n             Restart: self.execute_restart,\n             HashThreshold: self.execute_hash_threshold,\n             Set: self.execute_set,\n+            Unzip: self.execute_unzip,\n             Loop: self.execute_loop,\n             Foreach: self.execute_foreach,\n             Endloop: None,  # <-- should never be encountered outside of Loop/Foreach\n@@ -900,6 +902,18 @@ def is_query_result(sql_query, statement) -> bool:\n     def execute_skip(self, statement: Skip):\n         self.runner.skip()\n \n+    def execute_unzip(self, statement: Unzip):\n+        import gzip\n+        import shutil\n+\n+        source = self.replace_keywords(statement.source)\n+        destination = self.replace_keywords(statement.destination)\n+\n+        with gzip.open(source, 'rb') as f_in:\n+            with open(destination, 'wb') as f_out:\n+                shutil.copyfileobj(f_in, f_out)\n+        print(f\"Extracted to '{destination}'\")\n+\n     def execute_unskip(self, statement: Unskip):\n         self.runner.unskip()\n \ndiff --git a/scripts/sqllogictest/statement/__init__.py b/scripts/sqllogictest/statement/__init__.py\nindex daeeee0a1d52..2600fe110963 100644\n--- a/scripts/sqllogictest/statement/__init__.py\n+++ b/scripts/sqllogictest/statement/__init__.py\n@@ -14,6 +14,7 @@\n from .restart import Restart\n from .reconnect import Reconnect\n from .sleep import Sleep, SleepUnit\n+from .unzip import Unzip\n \n from .skip import Skip, Unskip\n \n@@ -35,6 +36,7 @@\n     Sleep,\n     SleepUnit,\n     Skip,\n+    Unzip,\n     Unskip,\n     SortStyle,\n ]\ndiff --git a/scripts/sqllogictest/statement/unzip.py b/scripts/sqllogictest/statement/unzip.py\nnew file mode 100644\nindex 000000000000..030e1b7b4e70\n--- /dev/null\n+++ b/scripts/sqllogictest/statement/unzip.py\n@@ -0,0 +1,9 @@\n+from sqllogictest.base_statement import BaseStatement\n+from sqllogictest.token import Token\n+\n+\n+class Unzip(BaseStatement):\n+    def __init__(self, header: Token, line: int, source: str, destination: str):\n+        super().__init__(header, line)\n+        self.source = source\n+        self.destination = destination\ndiff --git a/scripts/sqllogictest/token.py b/scripts/sqllogictest/token.py\nindex 20928735d950..88d42d156d38 100644\n--- a/scripts/sqllogictest/token.py\n+++ b/scripts/sqllogictest/token.py\n@@ -22,6 +22,7 @@ class TokenType(Enum):\n     SQLLOGIC_RESTART = auto()\n     SQLLOGIC_RECONNECT = auto()\n     SQLLOGIC_SLEEP = auto()\n+    SQLLOGIC_UNZIP = auto()\n \n \n class Token:\ndiff --git a/test/fuzzer/duckfuzz/late_materialization_filter.test b/test/fuzzer/duckfuzz/late_materialization_filter.test\nnew file mode 100644\nindex 000000000000..30bdea187cca\n--- /dev/null\n+++ b/test/fuzzer/duckfuzz/late_materialization_filter.test\n@@ -0,0 +1,17 @@\n+# name: test/fuzzer/duckfuzz/late_materialization_filter.test\n+# description: NULL input to json_extract_string - found by fuzzer\n+# group: [duckfuzz]\n+\n+statement ok\n+create table tbl(z int, i bool, j bool, k uhugeint);\n+\n+statement ok\n+insert into tbl (i, j, k) values (true, 'true', 3), (NULL, NULL, NULL), (false, 'false', 1);\n+\n+query I\n+select k\n+from tbl\n+where i and j\n+limit 30;\n+----\n+3\ndiff --git a/test/fuzzer/public/insert_returning.test b/test/fuzzer/public/insert_returning.test\nnew file mode 100644\nindex 000000000000..4f4cfb9acea5\n--- /dev/null\n+++ b/test/fuzzer/public/insert_returning.test\n@@ -0,0 +1,17 @@\n+# name: test/fuzzer/public/insert_returning.test\n+# description: Test INSERT OR REPLACE with DEFAULT VALUES\n+# group: [public]\n+\n+statement ok\n+pragma enable_verification\n+\n+statement ok\n+CREATE TABLE v00 (c01 INT, c02 STRING);\n+\n+statement ok\n+INSERT INTO v00 (c01, c02) VALUES (0, 'abc');\n+\n+query I\n+INSERT INTO v00 FROM v00 ORDER BY ALL DESC RETURNING 'abc';\n+----\n+abc\ndiff --git a/test/fuzzer/public/lateral_in_right_side_of_join.test b/test/fuzzer/public/lateral_in_right_side_of_join.test\nnew file mode 100644\nindex 000000000000..aab7706bef98\n--- /dev/null\n+++ b/test/fuzzer/public/lateral_in_right_side_of_join.test\n@@ -0,0 +1,28 @@\n+# name: test/fuzzer/public/lateral_in_right_side_of_join.test\n+# description: Lateral correlation in right side of join\n+# group: [public]\n+\n+statement ok\n+pragma enable_verification\n+\n+statement ok\n+CREATE TABLE t0(c0 INT , c1 VARCHAR);\n+\n+statement ok\n+CREATE TABLE t1( c1 INT);\n+\n+statement ok\n+INSERT INTO t0 VALUES(4, 3);\n+\n+statement ok\n+INSERT INTO t1 VALUES(2);\n+\n+query IIII\n+SELECT * FROM t1, t0 JOIN LATERAL (SELECT t1.c1 AS col0) _subq ON true;\n+----\n+2\t4\t3\t2\n+\n+query IIII\n+SELECT * FROM t1, t0 INNER JOIN (SELECT t1.c1 AS col0) ON true;\n+----\n+2\t4\t3\t2\ndiff --git a/test/fuzzer/public/unsatisfiable_filter_prune.test b/test/fuzzer/public/unsatisfiable_filter_prune.test\nnew file mode 100644\nindex 000000000000..b1f3e1a67f3e\n--- /dev/null\n+++ b/test/fuzzer/public/unsatisfiable_filter_prune.test\n@@ -0,0 +1,20 @@\n+# name: test/fuzzer/public/unsatisfiable_filter_prune.test\n+# description: Test SEMI JOIN with USING clause\n+# group: [public]\n+\n+statement ok\n+pragma enable_verification\n+\n+statement ok\n+CREATE TABLE  t0(c0 INT, c1 INT);\n+\n+statement ok\n+INSERT INTO t0( c0, c1) VALUES ( -1, -1);\n+\n+query I\n+SELECT c0 FROM t0 WHERE (((CASE t0.c0 WHEN t0.c0 THEN t0.c0 END ) BETWEEN 1 AND t0.c0)AND(t0.c0 <= 0)) ;\n+----\n+\n+query II\n+SELECT * FROM t0 WHERE c0 >= 1 AND c0 <= t0.c1 AND t0.c1 <= 0;\n+----\ndiff --git a/test/sql/catalog/sequence/test_sequence_google_fuzz.test b/test/sql/catalog/sequence/test_sequence_google_fuzz.test\nnew file mode 100644\nindex 000000000000..a439abbf30e3\n--- /dev/null\n+++ b/test/sql/catalog/sequence/test_sequence_google_fuzz.test\n@@ -0,0 +1,16 @@\n+# name: test/sql/catalog/sequence/test_sequence_google_fuzz.test\n+# description: Test Sequence on results produce by the google fuzzer\n+# group: [sequence]\n+\n+statement error\n+;creAte\n+sequence\n+uGe\u00f5\u00f3\u00b1\u00f5\u00f5\u00f5\u00f5.\u00f5\u00f5\u00f5.\u00f5;creAte\n+sequence\n+uGe\u00f5\u00f5\u00f5\u00f5\u00f5.\u0081.\u00f5;creAte\n+sequence\n+uGe\u00f5\u00f5\u00f5\u00f5\u00f5.\u0081.\u00f5;creAte\n+sequence\n+uGe\u00f5\u00f5\u00f5\u00f5\n+----\n+Catalog with name uGe\u00f5\u00f3\u00b1\u00f5\u00f5\u00f5\u00f5 does not exist!\n\\ No newline at end of file\ndiff --git a/test/sql/constraints/primarykey/test_pk_updel_multi_column.test b/test/sql/constraints/primarykey/test_pk_updel_multi_column.test\nindex 5357eaa69d46..077fa824f38c 100644\n--- a/test/sql/constraints/primarykey/test_pk_updel_multi_column.test\n+++ b/test/sql/constraints/primarykey/test_pk_updel_multi_column.test\n@@ -2,6 +2,9 @@\n # description: PRIMARY KEY and update/delete on multiple columns\n # group: [primarykey]\n \n+# See test/sql/index/art/constraints/test_art_tx_returning.test.\n+require vector_size 2048\n+\n statement ok\n PRAGMA enable_verification;\n \ndiff --git a/test/sql/copy/csv/14512.test b/test/sql/copy/csv/14512.test\nindex 97df786ccb28..79af7b8399c0 100644\n--- a/test/sql/copy/csv/14512.test\n+++ b/test/sql/copy/csv/14512.test\n@@ -8,7 +8,7 @@ PRAGMA enable_verification\n query II\n FROM read_csv('data/csv/14512.csv', strict_mode=TRUE);\n ----\n-onions \t,\n+onions\t,\n \n query I\n select columns FROM sniff_csv('data/csv/14512.csv')\n@@ -20,7 +20,7 @@ FROM 'data/csv/14512_og.csv';\n ----\n 00000579000098\t13.99\tEA\tPINE RIDGE CHENIN VOIGNIER\t750.0\tML\t1\t13\tNULL\t1\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tDEFAULT BRAND\tNULL\tNULL\tNULL\tNULL\tBEER & WINE\tNULL\tNULL\t7.25\t{\"sales_tax\":{ \"tax_type\": \"rate_percent\", \"value\" :0.0725}}\n 00000609082001\t3.99\tEA\tMADELAINE MINI MILK CHOCOLATE TURKEY\t1.0\tOZ\t1\t13\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tMADELEINE\tNULL\tNULL\tNULL\tNULL\tCANDY\tNULL\tNULL\t7.25\t{\"sales_tax\":{ \"tax_type\": \"rate_percent\", \"value\" :0.0725}}\n-00817566020096\t9.99\tEA\tCOTSWOLD EW\t5.3\tOZ\t1\t13\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tLONG CLAWSON\tNULL\tNULL\tNULL\tNULL\tDELI\tINGREDIENTS: DOUBLE GLOUCESTER CHEESE (PASTEURIZED MILK  SALT  ENZYMES  DAIRY CULTURES  ANNATTO  EXTRACT AS A COLOR)  RECONSTITUTED MINCED ONIONS (2%)  DRIED CHIVES. CONTAINS: MILK     THIS PRODUCT WAS PRODUCED IN AN ENVIRONMENT THAT ALSO USES PEANUTS  TREE NUTS  EGGS  MILK  WHEAT  SOY  FISH  SHELLFISH  AND SESAME. \tNULL\t2.0\t{\"sales_tax\":{ \"tax_type\": \"rate_percent\", \"value\" :0.02}}\n+00817566020096\t9.99\tEA\tCOTSWOLD EW\t5.3\tOZ\t1\t13\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tLONG CLAWSON\tNULL\tNULL\tNULL\tNULL\tDELI\tINGREDIENTS: DOUBLE GLOUCESTER CHEESE (PASTEURIZED MILK  SALT  ENZYMES  DAIRY CULTURES  ANNATTO  EXTRACT AS A COLOR)  RECONSTITUTED MINCED ONIONS (2%)  DRIED CHIVES. CONTAINS: MILK     THIS PRODUCT WAS PRODUCED IN AN ENVIRONMENT THAT ALSO USES PEANUTS  TREE NUTS  EGGS  MILK  WHEAT  SOY  FISH  SHELLFISH  AND SESAME.\tNULL\t2.0\t{\"sales_tax\":{ \"tax_type\": \"rate_percent\", \"value\" :0.02}}\n \n query I\n select columns FROM sniff_csv('data/csv/14512_og.csv')\ndiff --git a/test/sql/copy/csv/parallel/test_parallel_csv.test b/test/sql/copy/csv/parallel/test_parallel_csv.test\nindex b903844ccf41..ce3cfefede61 100644\n--- a/test/sql/copy/csv/parallel/test_parallel_csv.test\n+++ b/test/sql/copy/csv/parallel/test_parallel_csv.test\n@@ -5,6 +5,13 @@\n statement ok\n PRAGMA enable_verification\n \n+query IIIIIIIIIIIIIIIIIIIIIIIIII\n+FROM read_csv('data/csv/14512_og.csv', buffer_size = 473)\n+----\n+00000579000098\t13.99\tEA\tPINE RIDGE CHENIN VOIGNIER\t750.0\tML\t1\t13\tNULL\t1\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tDEFAULT BRAND\tNULL\tNULL\tNULL\tNULL\tBEER & WINE\tNULL\tNULL\t7.25\t{\"sales_tax\":{ \"tax_type\": \"rate_percent\", \"value\" :0.0725}}\n+00000609082001\t3.99\tEA\tMADELAINE MINI MILK CHOCOLATE TURKEY\t1.0\tOZ\t1\t13\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tMADELEINE\tNULL\tNULL\tNULL\tNULL\tCANDY\tNULL\tNULL\t7.25\t{\"sales_tax\":{ \"tax_type\": \"rate_percent\", \"value\" :0.0725}}\n+00817566020096\t9.99\tEA\tCOTSWOLD EW\t5.3\tOZ\t1\t13\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tLONG CLAWSON\tNULL\tNULL\tNULL\tNULL\tDELI\tINGREDIENTS: DOUBLE GLOUCESTER CHEESE (PASTEURIZED MILK  SALT  ENZYMES  DAIRY CULTURES  ANNATTO  EXTRACT AS A COLOR)  RECONSTITUTED MINCED ONIONS (2%)  DRIED CHIVES. CONTAINS: MILK     THIS PRODUCT WAS PRODUCED IN AN ENVIRONMENT THAT ALSO USES PEANUTS  TREE NUTS  EGGS  MILK  WHEAT  SOY  FISH  SHELLFISH  AND SESAME.\tNULL\t2.0\t{\"sales_tax\":{ \"tax_type\": \"rate_percent\", \"value\" :0.02}}\n+\n \n query III\n select * from read_csv_auto('data/csv/dirty_line.csv',  skip = 1)\ndiff --git a/test/sql/copy/csv/test_thijs_unquoted_file.test b/test/sql/copy/csv/test_thijs_unquoted_file.test\nindex b40e9af98897..64a2fc222192 100644\n--- a/test/sql/copy/csv/test_thijs_unquoted_file.test\n+++ b/test/sql/copy/csv/test_thijs_unquoted_file.test\n@@ -14,7 +14,7 @@ from read_csv('data/csv/thijs_unquoted.csv', quote='\"', sep='|', escape='\"', col\n query III\n from read_csv('data/csv/thijs_unquoted.csv', quote='\"', sep='|', escape='\"', columns={'a':'varchar', 'b': 'varchar', 'c': 'integer'}, auto_detect=false, strict_mode = False);\n ----\n-HYDRONIC GESELLSCHAFT F\u00dcR WASSERTECHNIK MBH                                                     \t \t2011\n+HYDRONIC GESELLSCHAFT F\u00dcR WASSERTECHNIK MBH\t \t2011\n ANTON SONNENSCHUTZSYSTEME GESELLSCHAFT MIT BESCHR\u00c4NKTER HAFTUN\t \t2012\n ENERGYS MAINTENANCE S\t \t2015\n SYSTEMAT BELGIUM  S\t \t2013\ndiff --git a/test/sql/copy_database/copy_database_index.test b/test/sql/copy_database/copy_database_index.test\nindex 0c7db26eafe9..e8f7351b7d60 100644\n--- a/test/sql/copy_database/copy_database_index.test\n+++ b/test/sql/copy_database/copy_database_index.test\n@@ -43,13 +43,10 @@ SELECT * FROM test WHERE a = 42;\n ----\n 42\t88\thello\n \n-# FIXME: We do not yet copy indexes.\n-# See issue 14909.\n-\n query II\n EXPLAIN ANALYZE SELECT * FROM test WHERE a = 42;\n ----\n-analyzed_plan\t<REGEX>:.*Type: Sequential Scan.*\n+analyzed_plan\t<REGEX>:.*Type: Index Scan.*\n \n statement ok\n DROP INDEX i_index;\ndiff --git a/test/sql/copy_database/copy_database_with_index.test b/test/sql/copy_database/copy_database_with_index.test\nnew file mode 100644\nindex 000000000000..b4daa7003bc3\n--- /dev/null\n+++ b/test/sql/copy_database/copy_database_with_index.test\n@@ -0,0 +1,42 @@\n+# name: test/sql/copy_database/copy_database_with_index.test\n+# description: Test the COPY DATABASE statement with an index.\n+# group: [copy_database]\n+\n+require noforcestorage\n+\n+statement ok\n+PRAGMA enable_verification;\n+\n+statement ok\n+ATTACH ':memory:' AS db1;\n+\n+statement ok\n+USE db1;\n+\n+statement ok\n+CREATE TABLE data AS\n+SELECT i, hash(i)::VARCHAR AS value FROM generate_series(1, 10000) s(i);\n+\n+statement ok\n+ALTER TABLE data ALTER COLUMN value SET NOT NULL;\n+\n+statement ok\n+CREATE INDEX data_value ON data(value);\n+\n+statement ok\n+ATTACH ':memory:' AS db2;\n+\n+statement ok\n+COPY FROM DATABASE db1 TO db2;\n+\n+query III\n+SELECT database_name, table_name, index_name FROM duckdb_indexes ORDER BY ALL;\n+----\n+db1\tdata\tdata_value\n+db2\tdata\tdata_value\n+\n+query III\n+SELECT database_name, table_name, index_count FROM duckdb_tables ORDER BY ALL;\n+----\n+db1\tdata\t1\n+db2\tdata\t1\n\\ No newline at end of file\ndiff --git a/test/sql/copy_database/copy_database_with_unique_index.test b/test/sql/copy_database/copy_database_with_unique_index.test\nnew file mode 100644\nindex 000000000000..9c0e00123cf8\n--- /dev/null\n+++ b/test/sql/copy_database/copy_database_with_unique_index.test\n@@ -0,0 +1,62 @@\n+# name: test/sql/copy_database/copy_database_with_unique_index.test\n+# description: Test the COPY DATABASE statement with unique indexes.\n+# group: [copy_database]\n+\n+require noforcestorage\n+\n+statement ok\n+PRAGMA enable_verification;\n+\n+statement ok\n+ATTACH '__TEST_DIR__/copy_db_old.db' AS old;\n+\n+statement ok\n+CREATE TABLE old.items (id INT, uniq INT UNIQUE);\n+\n+statement ok\n+INSERT INTO old.items VALUES (1, 1), (2, 2), (3, 3);\n+\n+statement ok\n+CREATE UNIQUE INDEX idx_id ON old.items(id);\n+\n+statement ok\n+ATTACH '__TEST_DIR__/copy_db_new1.db' AS new1;\n+\n+statement ok\n+COPY FROM DATABASE old TO new1 (SCHEMA);\n+\n+statement ok\n+COPY FROM DATABASE old TO new1 (DATA);\n+\n+query II\n+SELECT id, uniq FROM new1.items ORDER BY ALL;\n+----\n+1\t1\n+2\t2\n+3\t3\n+\n+statement error\n+INSERT INTO new1.items VALUES (1, 4);\n+----\n+<REGEX>:Constraint Error.*violates unique constraint.*\n+\n+statement error\n+INSERT INTO new1.items VALUES (4, 1);\n+----\n+<REGEX>:Constraint Error.*violates unique constraint.*\n+\n+statement ok\n+DETACH new1;\n+\n+statement ok\n+ATTACH '__TEST_DIR__/copy_db_new1.db' AS new1;\n+\n+statement error\n+INSERT INTO new1.items VALUES (1, 4);\n+----\n+<REGEX>:Constraint Error.*violates unique constraint.*\n+\n+statement error\n+INSERT INTO new1.items VALUES (4, 1);\n+----\n+<REGEX>:Constraint Error.*violates unique constraint.*\ndiff --git a/test/sql/function/list/flatten.test b/test/sql/function/list/flatten.test\nindex e0c894b802ae..4869cb2233ce 100644\n--- a/test/sql/function/list/flatten.test\n+++ b/test/sql/function/list/flatten.test\n@@ -150,3 +150,13 @@ query I\n select flatten(NULL);\n ----\n NULL\n+\n+query IIII rowsort\n+with v_data (col, list) as ( select * FROM (VALUES ('a', [1,2,3]), ('b', [4,5]), ('a', [2,6])) ),\n+        v_list_of_lists ( col, list, list_of_lists ) as ( select v.*, array_agg(v.list) over (partition by v.col) from v_data v )\n+select v.*, flatten(v.list_of_lists) from v_list_of_lists v;\n+----\n+a\t[1, 2, 3]\t[[1, 2, 3], [2, 6]]\t[1, 2, 3, 2, 6]\n+a\t[2, 6]\t[[1, 2, 3], [2, 6]]\t[1, 2, 3, 2, 6]\n+b\t[4, 5]\t[[4, 5]]\t[4, 5]\n+\ndiff --git a/test/sql/index/art/constraints/test_art_eager_with_wal.test b/test/sql/index/art/constraints/test_art_eager_with_wal.test\nnew file mode 100644\nindex 000000000000..4d211adf7734\n--- /dev/null\n+++ b/test/sql/index/art/constraints/test_art_eager_with_wal.test\n@@ -0,0 +1,39 @@\n+# name: test/sql/index/art/constraints/test_art_eager_with_wal.test\n+# description: Test eager constraint checking with WAL replay.\n+# group: [constraints]\n+\n+load __TEST_DIR__/pk_duplicate_key_wal.db\n+\n+statement ok\n+PRAGMA enable_verification;\n+\n+statement ok\n+SET checkpoint_threshold = '10.0 GB';\n+\n+statement ok\n+PRAGMA disable_checkpoint_on_shutdown;\n+\n+statement ok\n+CREATE TABLE tbl (id INT PRIMARY KEY);\n+\n+statement ok\n+INSERT INTO tbl VALUES (1);\n+\n+statement ok\n+BEGIN;\n+\n+statement ok\n+DELETE FROM tbl WHERE id = 1;\n+\n+statement ok\n+INSERT INTO tbl VALUES (1);\n+\n+statement ok\n+COMMIT;\n+\n+restart\n+\n+statement error\n+INSERT INTO tbl VALUES (1);\n+----\n+<REGEX>:Constraint Error.*violates primary key constraint.*\n\\ No newline at end of file\ndiff --git a/test/sql/index/art/constraints/test_art_tx_returning.test b/test/sql/index/art/constraints/test_art_tx_returning.test\nindex 318d68b08844..de9e8963f04c 100644\n--- a/test/sql/index/art/constraints/test_art_tx_returning.test\n+++ b/test/sql/index/art/constraints/test_art_tx_returning.test\n@@ -2,6 +2,13 @@\n # description: Test updates on the primary key containing RETURNING.\n # group: [constraints]\n \n+# For each incoming chunk, we add the row IDs to the delete index.\n+# For standard_vector_size = 2, we delete [0, 1], and then try to insert value [1, 2].\n+# This is expected to throw a constraint violation.\n+# The value 2 is not yet in the delete index, as the chunk that would add that value hasn't been processed, yet.\n+# This scenario is a known limitation (also in postgres).\n+require vector_size 2048\n+\n statement ok\n PRAGMA enable_verification\n \ndiff --git a/test/sql/projection/select_star_rename.test b/test/sql/projection/select_star_rename.test\nindex f52f4a9e7b2a..7f3be601c2e0 100644\n--- a/test/sql/projection/select_star_rename.test\n+++ b/test/sql/projection/select_star_rename.test\n@@ -17,6 +17,12 @@ SELECT renamed_col FROM (SELECT * RENAME i AS renamed_col FROM integers)\n ----\n 1\n \n+# rename with COLUMNS\n+query I\n+SELECT renamed_col FROM (SELECT COLUMNS(* RENAME i AS renamed_col) FROM integers)\n+----\n+1\n+\n # qualified\n query I\n SELECT renamed_col FROM (SELECT * RENAME integers.i AS renamed_col FROM integers)\ndiff --git a/test/sql/sample/table_samples/basic_sample_tests.test b/test/sql/sample/table_samples/basic_sample_tests.test\nindex 1bb07364c9c3..733b14b6827a 100644\n--- a/test/sql/sample/table_samples/basic_sample_tests.test\n+++ b/test/sql/sample/table_samples/basic_sample_tests.test\n@@ -1,6 +1,11 @@\n # name: test/sql/sample/table_samples/basic_sample_tests.test\n # group: [table_samples]\n \n+# currently require fixed vector size since the \"randomness\" of the sample depends on\n+# the vector size. If the vector size decreases, the randomness of the sample decreases\n+# This is especially noticeable for small tables and their samples\n+require vector_size 2048\n+\n statement ok\n PRAGMA enable_verification\n \ndiff --git a/test/sql/sample/table_samples/sample_stores_rows_from_later_on.test_slow b/test/sql/sample/table_samples/sample_stores_rows_from_later_on.test_slow\nindex c88b9cd74a67..7f883d9fa19b 100644\n--- a/test/sql/sample/table_samples/sample_stores_rows_from_later_on.test_slow\n+++ b/test/sql/sample/table_samples/sample_stores_rows_from_later_on.test_slow\n@@ -2,6 +2,7 @@\n # description: Test sampling of larger relations\n # group: [table_samples]\n \n+# required when testing table samples. See basic_sample_test.test\n require vector_size 2048\n \n require noforcestorage\ndiff --git a/test/sql/sample/table_samples/table_sample_converts_to_block_sample.test b/test/sql/sample/table_samples/table_sample_converts_to_block_sample.test\nindex 0ffca61f31ef..3e810c325a3b 100644\n--- a/test/sql/sample/table_samples/table_sample_converts_to_block_sample.test\n+++ b/test/sql/sample/table_samples/table_sample_converts_to_block_sample.test\n@@ -2,6 +2,7 @@\n # description: Test sampling of larger relations\n # group: [table_samples]\n \n+# required when testing table samples. See basic_sample_test.test\n require vector_size 2048\n \n require noforcestorage\ndiff --git a/test/sql/sample/table_samples/table_sample_is_stored.test_slow b/test/sql/sample/table_samples/table_sample_is_stored.test_slow\nindex f8370557d2a5..aef613355f64 100644\n--- a/test/sql/sample/table_samples/table_sample_is_stored.test_slow\n+++ b/test/sql/sample/table_samples/table_sample_is_stored.test_slow\n@@ -2,6 +2,7 @@\n # description: Test sampling of larger relations\n # group: [table_samples]\n \n+# required when testing table samples. See basic_sample_test.test\n require vector_size 2048\n \n require noforcestorage\ndiff --git a/test/sql/sample/table_samples/test_sample_is_destroyed_on_updates.test b/test/sql/sample/table_samples/test_sample_is_destroyed_on_updates.test\nindex 410d9004e87c..5041df626617 100644\n--- a/test/sql/sample/table_samples/test_sample_is_destroyed_on_updates.test\n+++ b/test/sql/sample/table_samples/test_sample_is_destroyed_on_updates.test\n@@ -2,6 +2,7 @@\n # description: Test sampling of larger relations\n # group: [table_samples]\n \n+# required when testing table samples. See basic_sample_test.test\n require vector_size 2048\n \n load __TEST_DIR__/test_sample_is_destroyed_on_update.db\ndiff --git a/tools/pythonpkg/tests/fast/spark/test_spark_functions_date.py b/tools/pythonpkg/tests/fast/spark/test_spark_functions_date.py\nindex fda95752a714..8a03fd68169c 100644\n--- a/tools/pythonpkg/tests/fast/spark/test_spark_functions_date.py\n+++ b/tools/pythonpkg/tests/fast/spark/test_spark_functions_date.py\n@@ -177,20 +177,13 @@ def test_to_date(self, spark):\n     def test_to_timestamp(self, spark):\n         df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n         res = df.select(F.to_timestamp(df.t).alias('dt')).collect()\n-        # FIXME: Fix difference between DuckDB and Spark\n-        if USE_ACTUAL_SPARK:\n-            assert res == [Row(dt=datetime(1997, 2, 28, 10, 30))]\n-        else:\n-            assert res == [Row(dt=datetime(1997, 2, 28, 10, 30, tzinfo=timezone.utc))]\n+        assert res == [Row(dt=datetime(1997, 2, 28, 10, 30))]\n \n     def test_to_timestamp_ltz(self, spark):\n         df = spark.createDataFrame([(\"2016-12-31\",)], [\"e\"])\n         res = df.select(F.to_timestamp_ltz(df.e).alias('r')).collect()\n-        # FIXME: Fix difference between DuckDB and Spark\n-        if USE_ACTUAL_SPARK:\n-            assert res == [Row(r=datetime(2016, 12, 31, 0, 0))]\n-        else:\n-            assert res == [Row(r=datetime(2016, 12, 31, 0, 0, tzinfo=timezone.utc))]\n+\n+        assert res == [Row(r=datetime(2016, 12, 31, 0, 0))]\n \n     def test_to_timestamp_ntz(self, spark):\n         df = spark.createDataFrame([(\"2016-04-08\",)], [\"e\"])\n",
  "problem_statement": "DuckDB Trigger Assertion Failure: result.data[storage_idx].GetType() == chunk.data[mapped_index].GetType()\n### What happens?\r\n\r\nThe latest version of the DuckDB (latest main: v1.1.4-dev3741 ab8c909857) triggers Internal Error when running the following SQL statement: \r\n\r\n```sql\r\nCREATE TABLE v00 (c01 INT, c02 STRING);\r\nINSERT INTO v00 BY POSITION ( c02 ) OVERRIDING USER VALUE FROM LATERAL ( SELECT 'simple_string' ) AS ta507400 GROUP BY ALL WINDOW EVENT AS ( GROUPS BETWEEN 'abc' IS NOT UNKNOWN IN CASE WHEN 'abc' THEN 0 ELSE 'abc' END FOLLOWING AND UNBOUNDED FOLLOWING ) ORDER BY ALL RETURNING 'anything';\r\n```\r\n\r\nThe code is working fine from the latest release: v1.1.3 19864453f7. Maybe just a faulty assertion? \r\n\r\nHere is the stack from ab8c909857:\r\n\r\n```\r\nAssertion triggered in file \"/home/duckdb/duckdb/src/execution/operator/persistent/physical_insert.cpp\" on line 150: result.data[storage_idx].GetType() == chunk.data[mapped_index].GetType()\r\n\r\n#0  duckdb::InternalException::InternalException (this=0x60d000018ba0, Python Exception <class 'gdb.error'> There is no member named _M_dataplus.:\r\nmsg=) at /home/duckdb/duckdb/src/common/exception.cpp:320\r\n#1  0x00000000020c1089 in duckdb::InternalException::InternalException<char const*, int, char const*> (this=0x60d000018ba0, msg=...,\r\n    params=<optimized out>, params=<optimized out>, params=<optimized out>) at ../../src/include/duckdb/common/exception.hpp:313\r\n#2  0x0000000001e672b1 in duckdb::DuckDBAssertInternal (condition=<optimized out>, condition_name=<optimized out>, file=<optimized out>,\r\n    linenr=<optimized out>) at /home/duckdb/duckdb/src/common/assert.cpp:13\r\n#3  0x000000000a69cf33 in duckdb::PhysicalInsert::ResolveDefaults (table=..., chunk=..., column_index_map=..., default_executor=..., result=...)\r\n    at /home/duckdb/duckdb/src/execution/operator/persistent/physical_insert.cpp:150\r\n#4  0x000000000a6f5aaa in duckdb::PhysicalInsert::Sink (this=0x6150000e4a00, context=..., chunk=..., input=...)\r\n    at /home/duckdb/duckdb/src/execution/operator/persistent/physical_insert.cpp:623\r\n#5  0x0000000003cf87a8 in duckdb::PipelineExecutor::ExecutePushInternal (this=<optimized out>, input=..., chunk_budget=..., initial_idx=<optimized out>)\r\n    at /home/duckdb/duckdb/src/parallel/pipeline_executor.cpp:332\r\n#6  0x0000000003cdbda6 in duckdb::PipelineExecutor::Execute (this=0x615000083980, max_chunks=<optimized out>)\r\n    at /home/duckdb/duckdb/src/parallel/pipeline_executor.cpp:250\r\n#7  0x0000000003cdd7e0 in duckdb::PipelineExecutor::Execute (this=<optimized out>) at /home/duckdb/duckdb/src/parallel/pipeline_executor.cpp:278\r\n#8  0x0000000003cd8c05 in duckdb::PipelineTask::ExecuteTask (this=0x60700008ace0, mode=<optimized out>) at /home/duckdb/duckdb/src/parallel/pipeline.cpp:51\r\n#9  0x0000000003ca595c in duckdb::ExecutorTask::Execute (this=0x60700008ace0, mode=<optimized out>)\r\n    at /home/duckdb/duckdb/src/parallel/executor_task.cpp:49\r\n#10 0x0000000003d0f68f in duckdb::TaskScheduler::ExecuteForever (this=<optimized out>, marker=<optimized out>)\r\n    at /home/duckdb/duckdb/src/parallel/task_scheduler.cpp:189\r\n#11 0x000079e0d73d5df4 in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6\r\n#12 0x000079e0d7195609 in start_thread (arg=<optimized out>) at pthread_create.c:477\r\n#13 0x000079e0d7093353 in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95\r\n```\r\n\r\n### To Reproduce\r\n\r\n1. Clone the DuckDB Git from the official repo.\r\n2. Checkout to the latest main (v1.1.4-dev3741 ab8c909857).\r\n3. Compile the DuckDB binary by using `make relassert` or `make debug`.\r\n4. Run the compiled DuckDB and input the following SQL:\r\n\r\n```sql\r\nCREATE TABLE v00 (c01 INT, c02 STRING);\r\nINSERT INTO v00 BY POSITION ( c02 ) OVERRIDING USER VALUE FROM LATERAL ( SELECT 'simple_string' ) AS ta507400 GROUP BY ALL WINDOW EVENT AS ( GROUPS BETWEEN 'abc' IS NOT UNKNOWN IN CASE WHEN 'abc' THEN 0 ELSE 'abc' END FOLLOWING AND UNBOUNDED FOLLOWING ) ORDER BY ALL RETURNING 'anything';\r\n```\r\n\r\n### OS:\r\n\r\nUbuntu 24.04 LTS\r\n\r\n### DuckDB Version:\r\n\r\nv1.1.4-dev3741 ab8c909857\r\n\r\n### DuckDB Client:\r\n\r\ncli\r\n\r\n### Hardware:\r\n\r\n_No response_\r\n\r\n### Full Name:\r\n\r\nYu Liang\r\n\r\n### Affiliation:\r\n\r\nPennsylvania State University\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a source build\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\nDuckDB Trigger Assertion Failure: result.data[storage_idx].GetType() == chunk.data[mapped_index].GetType()\n### What happens?\r\n\r\nThe latest version of the DuckDB (latest main: v1.1.4-dev3741 ab8c909857) triggers Internal Error when running the following SQL statement: \r\n\r\n```sql\r\nCREATE TABLE v00 (c01 INT, c02 STRING);\r\nINSERT INTO v00 BY POSITION ( c02 ) OVERRIDING USER VALUE FROM LATERAL ( SELECT 'simple_string' ) AS ta507400 GROUP BY ALL WINDOW EVENT AS ( GROUPS BETWEEN 'abc' IS NOT UNKNOWN IN CASE WHEN 'abc' THEN 0 ELSE 'abc' END FOLLOWING AND UNBOUNDED FOLLOWING ) ORDER BY ALL RETURNING 'anything';\r\n```\r\n\r\nThe code is working fine from the latest release: v1.1.3 19864453f7. Maybe just a faulty assertion? \r\n\r\nHere is the stack from ab8c909857:\r\n\r\n```\r\nAssertion triggered in file \"/home/duckdb/duckdb/src/execution/operator/persistent/physical_insert.cpp\" on line 150: result.data[storage_idx].GetType() == chunk.data[mapped_index].GetType()\r\n\r\n#0  duckdb::InternalException::InternalException (this=0x60d000018ba0, Python Exception <class 'gdb.error'> There is no member named _M_dataplus.:\r\nmsg=) at /home/duckdb/duckdb/src/common/exception.cpp:320\r\n#1  0x00000000020c1089 in duckdb::InternalException::InternalException<char const*, int, char const*> (this=0x60d000018ba0, msg=...,\r\n    params=<optimized out>, params=<optimized out>, params=<optimized out>) at ../../src/include/duckdb/common/exception.hpp:313\r\n#2  0x0000000001e672b1 in duckdb::DuckDBAssertInternal (condition=<optimized out>, condition_name=<optimized out>, file=<optimized out>,\r\n    linenr=<optimized out>) at /home/duckdb/duckdb/src/common/assert.cpp:13\r\n#3  0x000000000a69cf33 in duckdb::PhysicalInsert::ResolveDefaults (table=..., chunk=..., column_index_map=..., default_executor=..., result=...)\r\n    at /home/duckdb/duckdb/src/execution/operator/persistent/physical_insert.cpp:150\r\n#4  0x000000000a6f5aaa in duckdb::PhysicalInsert::Sink (this=0x6150000e4a00, context=..., chunk=..., input=...)\r\n    at /home/duckdb/duckdb/src/execution/operator/persistent/physical_insert.cpp:623\r\n#5  0x0000000003cf87a8 in duckdb::PipelineExecutor::ExecutePushInternal (this=<optimized out>, input=..., chunk_budget=..., initial_idx=<optimized out>)\r\n    at /home/duckdb/duckdb/src/parallel/pipeline_executor.cpp:332\r\n#6  0x0000000003cdbda6 in duckdb::PipelineExecutor::Execute (this=0x615000083980, max_chunks=<optimized out>)\r\n    at /home/duckdb/duckdb/src/parallel/pipeline_executor.cpp:250\r\n#7  0x0000000003cdd7e0 in duckdb::PipelineExecutor::Execute (this=<optimized out>) at /home/duckdb/duckdb/src/parallel/pipeline_executor.cpp:278\r\n#8  0x0000000003cd8c05 in duckdb::PipelineTask::ExecuteTask (this=0x60700008ace0, mode=<optimized out>) at /home/duckdb/duckdb/src/parallel/pipeline.cpp:51\r\n#9  0x0000000003ca595c in duckdb::ExecutorTask::Execute (this=0x60700008ace0, mode=<optimized out>)\r\n    at /home/duckdb/duckdb/src/parallel/executor_task.cpp:49\r\n#10 0x0000000003d0f68f in duckdb::TaskScheduler::ExecuteForever (this=<optimized out>, marker=<optimized out>)\r\n    at /home/duckdb/duckdb/src/parallel/task_scheduler.cpp:189\r\n#11 0x000079e0d73d5df4 in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6\r\n#12 0x000079e0d7195609 in start_thread (arg=<optimized out>) at pthread_create.c:477\r\n#13 0x000079e0d7093353 in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95\r\n```\r\n\r\n### To Reproduce\r\n\r\n1. Clone the DuckDB Git from the official repo.\r\n2. Checkout to the latest main (v1.1.4-dev3741 ab8c909857).\r\n3. Compile the DuckDB binary by using `make relassert` or `make debug`.\r\n4. Run the compiled DuckDB and input the following SQL:\r\n\r\n```sql\r\nCREATE TABLE v00 (c01 INT, c02 STRING);\r\nINSERT INTO v00 BY POSITION ( c02 ) OVERRIDING USER VALUE FROM LATERAL ( SELECT 'simple_string' ) AS ta507400 GROUP BY ALL WINDOW EVENT AS ( GROUPS BETWEEN 'abc' IS NOT UNKNOWN IN CASE WHEN 'abc' THEN 0 ELSE 'abc' END FOLLOWING AND UNBOUNDED FOLLOWING ) ORDER BY ALL RETURNING 'anything';\r\n```\r\n\r\n### OS:\r\n\r\nUbuntu 24.04 LTS\r\n\r\n### DuckDB Version:\r\n\r\nv1.1.4-dev3741 ab8c909857\r\n\r\n### DuckDB Client:\r\n\r\ncli\r\n\r\n### Hardware:\r\n\r\n_No response_\r\n\r\n### Full Name:\r\n\r\nYu Liang\r\n\r\n### Affiliation:\r\n\r\nPennsylvania State University\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a source build\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\n",
  "hints_text": "\n",
  "created_at": "2025-01-29T21:40:50Z"
}