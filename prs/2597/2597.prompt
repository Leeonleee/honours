You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Should `Date` types roundtrip to parquet?
**What does happen?**
When saving a Date to parquet, it looks like it's being stored as a datetime instead of a Date. I looked through the documentation and didn't see anything that mentioned this was intentional, but it may be. 

**What should happen?**
Store Dates as Dates 

**To Reproduce**
```r
library(duckdb)
#> Loading required package: DBI

con <- dbConnect(duckdb())

df <- data.frame(date = Sys.Date())

dbWriteTable(con, "df", df)
dbExecute(con, "COPY (SELECT * FROM df) TO 'df.parquet' (FORMAT 'parquet');")
#> [1] 1

df_rt <- dbGetQuery(con, "SELECT * FROM parquet_scan('df.parquet')")

waldo::compare(df, df_rt)
#> `class(old$date)`: "Date"            
#> `class(new$date)`: "POSIXct" "POSIXt"
#> 
#> `attr(old$date, 'tzone')` is absent
#> `attr(new$date, 'tzone')` is a character vector ('UTC')
#> 
#> `old$date`:      18919
#> `new$date`: 1634601600
```

**Environment (please complete the following information):**
 - OS: macOS 11.6
 - DuckDB Version: install from `master` branch

**Before submitting**
- [x] Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?
- [x] Have you tried this on the latest `master` branch? In case you cannot compile, you may find some binaries here: https://github.com/duckdb/duckdb/releases/tag/master-builds

Should `Date` types roundtrip to parquet?
**What does happen?**
When saving a Date to parquet, it looks like it's being stored as a datetime instead of a Date. I looked through the documentation and didn't see anything that mentioned this was intentional, but it may be. 

**What should happen?**
Store Dates as Dates 

**To Reproduce**
```r
library(duckdb)
#> Loading required package: DBI

con <- dbConnect(duckdb())

df <- data.frame(date = Sys.Date())

dbWriteTable(con, "df", df)
dbExecute(con, "COPY (SELECT * FROM df) TO 'df.parquet' (FORMAT 'parquet');")
#> [1] 1

df_rt <- dbGetQuery(con, "SELECT * FROM parquet_scan('df.parquet')")

waldo::compare(df, df_rt)
#> `class(old$date)`: "Date"            
#> `class(new$date)`: "POSIXct" "POSIXt"
#> 
#> `attr(old$date, 'tzone')` is absent
#> `attr(new$date, 'tzone')` is a character vector ('UTC')
#> 
#> `old$date`:      18919
#> `new$date`: 1634601600
```

**Environment (please complete the following information):**
 - OS: macOS 11.6
 - DuckDB Version: install from `master` branch

**Before submitting**
- [x] Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?
- [x] Have you tried this on the latest `master` branch? In case you cannot compile, you may find some binaries here: https://github.com/duckdb/duckdb/releases/tag/master-builds


</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16: </p>
17: 
18: ## DuckDB
19: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/docs/why_duckdb.html).
20: 
21: ## Installation
22: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
23: 
24: ## Data Import
25: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
26: 
27: ```sql
28: SELECT * FROM 'myfile.csv';
29: SELECT * FROM 'myfile.parquet';
30: ```
31: 
32: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
33: 
34: ## SQL Reference
35: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
36: 
37: ## Development
38: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
39: 
40: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
41: 
42: 
[end of README.md]
[start of extension/parquet/column_reader.cpp]
1: #include "column_reader.hpp"
2: #include "parquet_timestamp.hpp"
3: #include "utf8proc_wrapper.hpp"
4: #include "parquet_reader.hpp"
5: 
6: #include "boolean_column_reader.hpp"
7: #include "callback_column_reader.hpp"
8: #include "decimal_column_reader.hpp"
9: #include "list_column_reader.hpp"
10: #include "string_column_reader.hpp"
11: #include "struct_column_reader.hpp"
12: #include "templated_column_reader.hpp"
13: 
14: #include "snappy.h"
15: #include "miniz_wrapper.hpp"
16: #include "zstd.h"
17: #include <iostream>
18: 
19: #include "duckdb.hpp"
20: #ifndef DUCKDB_AMALGAMATION
21: #include "duckdb/common/types/blob.hpp"
22: #include "duckdb/common/types/chunk_collection.hpp"
23: #endif
24: 
25: namespace duckdb {
26: 
27: using duckdb_parquet::format::CompressionCodec;
28: using duckdb_parquet::format::ConvertedType;
29: using duckdb_parquet::format::Encoding;
30: using duckdb_parquet::format::PageType;
31: using duckdb_parquet::format::Type;
32: 
33: const uint32_t RleBpDecoder::BITPACK_MASKS[] = {
34:     0,       1,       3,        7,        15,       31,        63,        127,       255,        511,       1023,
35:     2047,    4095,    8191,     16383,    32767,    65535,     131071,    262143,    524287,     1048575,   2097151,
36:     4194303, 8388607, 16777215, 33554431, 67108863, 134217727, 268435455, 536870911, 1073741823, 2147483647};
37: 
38: const uint8_t RleBpDecoder::BITPACK_DLEN = 8;
39: 
40: ColumnReader::ColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
41:                            idx_t max_define_p, idx_t max_repeat_p)
42:     : schema(schema_p), file_idx(file_idx_p), max_define(max_define_p), max_repeat(max_repeat_p), reader(reader),
43:       type(move(type_p)), page_rows_available(0) {
44: 
45: 	// dummies for Skip()
46: 	none_filter.none();
47: 	dummy_define.resize(reader.allocator, STANDARD_VECTOR_SIZE);
48: 	dummy_repeat.resize(reader.allocator, STANDARD_VECTOR_SIZE);
49: }
50: 
51: ColumnReader::~ColumnReader() {
52: }
53: 
54: unique_ptr<ColumnReader> ColumnReader::CreateReader(ParquetReader &reader, const LogicalType &type_p,
55:                                                     const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,
56:                                                     idx_t max_repeat) {
57: 	switch (type_p.id()) {
58: 	case LogicalTypeId::BOOLEAN:
59: 		return make_unique<BooleanColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
60: 	case LogicalTypeId::UTINYINT:
61: 		return make_unique<TemplatedColumnReader<uint8_t, TemplatedParquetValueConversion<uint32_t>>>(
62: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
63: 	case LogicalTypeId::USMALLINT:
64: 		return make_unique<TemplatedColumnReader<uint16_t, TemplatedParquetValueConversion<uint32_t>>>(
65: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
66: 	case LogicalTypeId::UINTEGER:
67: 		return make_unique<TemplatedColumnReader<uint32_t, TemplatedParquetValueConversion<uint32_t>>>(
68: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
69: 	case LogicalTypeId::UBIGINT:
70: 		return make_unique<TemplatedColumnReader<uint64_t, TemplatedParquetValueConversion<uint64_t>>>(
71: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
72: 	case LogicalTypeId::INTEGER:
73: 		return make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<int32_t>>>(
74: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
75: 	case LogicalTypeId::BIGINT:
76: 		return make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<int64_t>>>(
77: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
78: 	case LogicalTypeId::FLOAT:
79: 		return make_unique<TemplatedColumnReader<float, TemplatedParquetValueConversion<float>>>(
80: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
81: 	case LogicalTypeId::DOUBLE:
82: 		return make_unique<TemplatedColumnReader<double, TemplatedParquetValueConversion<double>>>(
83: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
84: 	case LogicalTypeId::TIMESTAMP:
85: 		switch (schema_p.type) {
86: 		case Type::INT96:
87: 			return make_unique<CallbackColumnReader<Int96, timestamp_t, ImpalaTimestampToTimestamp>>(
88: 			    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
89: 		case Type::INT64:
90: 			switch (schema_p.converted_type) {
91: 			case ConvertedType::TIMESTAMP_MICROS:
92: 				return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMicrosToTimestamp>>(
93: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
94: 			case ConvertedType::TIMESTAMP_MILLIS:
95: 				return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMsToTimestamp>>(
96: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
97: 			default:
98: 				break;
99: 			}
100: 		default:
101: 			break;
102: 		}
103: 		break;
104: 	case LogicalTypeId::DATE:
105: 		return make_unique<CallbackColumnReader<int32_t, date_t, ParquetIntToDate>>(reader, type_p, schema_p,
106: 		                                                                            file_idx_p, max_define, max_repeat);
107: 	case LogicalTypeId::BLOB:
108: 	case LogicalTypeId::VARCHAR:
109: 		return make_unique<StringColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
110: 	case LogicalTypeId::DECIMAL:
111: 		// we have to figure out what kind of int we need
112: 		switch (type_p.InternalType()) {
113: 		case PhysicalType::INT16:
114: 			return make_unique<DecimalColumnReader<int16_t>>(reader, type_p, schema_p, file_idx_p, max_define,
115: 			                                                 max_repeat);
116: 		case PhysicalType::INT32:
117: 			return make_unique<DecimalColumnReader<int32_t>>(reader, type_p, schema_p, file_idx_p, max_define,
118: 			                                                 max_repeat);
119: 		case PhysicalType::INT64:
120: 			return make_unique<DecimalColumnReader<int64_t>>(reader, type_p, schema_p, file_idx_p, max_define,
121: 			                                                 max_repeat);
122: 		case PhysicalType::INT128:
123: 			return make_unique<DecimalColumnReader<hugeint_t>>(reader, type_p, schema_p, file_idx_p, max_define,
124: 			                                                   max_repeat);
125: 
126: 		default:
127: 			break;
128: 		}
129: 		break;
130: 	default:
131: 		break;
132: 	}
133: 	throw NotImplementedException(type_p.ToString());
134: }
135: 
136: void ColumnReader::PrepareRead(parquet_filter_t &filter) {
137: 	dict_decoder.reset();
138: 	defined_decoder.reset();
139: 	block.reset();
140: 
141: 	PageHeader page_hdr;
142: 	page_hdr.read(protocol);
143: 
144: 	//	page_hdr.printTo(std::cout);
145: 	//	std::cout << '\n';
146: 
147: 	PreparePage(page_hdr.compressed_page_size, page_hdr.uncompressed_page_size);
148: 
149: 	switch (page_hdr.type) {
150: 	case PageType::DATA_PAGE_V2:
151: 	case PageType::DATA_PAGE:
152: 		PrepareDataPage(page_hdr);
153: 		break;
154: 	case PageType::DICTIONARY_PAGE:
155: 		Dictionary(move(block), page_hdr.dictionary_page_header.num_values);
156: 		break;
157: 	default:
158: 		break; // ignore INDEX page type and any other custom extensions
159: 	}
160: }
161: 
162: void ColumnReader::PreparePage(idx_t compressed_page_size, idx_t uncompressed_page_size) {
163: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
164: 
165: 	block = make_shared<ResizeableBuffer>(reader.allocator, compressed_page_size + 1);
166: 	trans.read((uint8_t *)block->ptr, compressed_page_size);
167: 
168: 	shared_ptr<ResizeableBuffer> unpacked_block;
169: 	if (chunk->meta_data.codec != CompressionCodec::UNCOMPRESSED) {
170: 		unpacked_block = make_shared<ResizeableBuffer>(reader.allocator, uncompressed_page_size + 1);
171: 	}
172: 
173: 	switch (chunk->meta_data.codec) {
174: 	case CompressionCodec::UNCOMPRESSED:
175: 		break;
176: 	case CompressionCodec::GZIP: {
177: 		MiniZStream s;
178: 
179: 		s.Decompress((const char *)block->ptr, compressed_page_size, (char *)unpacked_block->ptr,
180: 		             uncompressed_page_size);
181: 		block = move(unpacked_block);
182: 
183: 		break;
184: 	}
185: 	case CompressionCodec::SNAPPY: {
186: 		auto res = snappy::RawUncompress((const char *)block->ptr, compressed_page_size, (char *)unpacked_block->ptr);
187: 		if (!res) {
188: 			throw std::runtime_error("Decompression failure");
189: 		}
190: 		block = move(unpacked_block);
191: 		break;
192: 	}
193: 	case CompressionCodec::ZSTD: {
194: 		auto res = duckdb_zstd::ZSTD_decompress((char *)unpacked_block->ptr, uncompressed_page_size,
195: 		                                        (const char *)block->ptr, compressed_page_size);
196: 		if (duckdb_zstd::ZSTD_isError(res) || res != (size_t)uncompressed_page_size) {
197: 			throw std::runtime_error("ZSTD Decompression failure");
198: 		}
199: 		block = move(unpacked_block);
200: 		break;
201: 	}
202: 
203: 	default: {
204: 		std::stringstream codec_name;
205: 		codec_name << chunk->meta_data.codec;
206: 		throw std::runtime_error("Unsupported compression codec \"" + codec_name.str() +
207: 		                         "\". Supported options are uncompressed, gzip or snappy");
208: 		break;
209: 	}
210: 	}
211: }
212: 
213: static uint8_t ComputeBitWidth(idx_t val) {
214: 	if (val == 0) {
215: 		return 0;
216: 	}
217: 	uint8_t ret = 1;
218: 	while (((idx_t)(1 << ret) - 1) < val) {
219: 		ret++;
220: 	}
221: 	return ret;
222: }
223: 
224: void ColumnReader::PrepareDataPage(PageHeader &page_hdr) {
225: 	if (page_hdr.type == PageType::DATA_PAGE && !page_hdr.__isset.data_page_header) {
226: 		throw std::runtime_error("Missing data page header from data page");
227: 	}
228: 	if (page_hdr.type == PageType::DATA_PAGE_V2 && !page_hdr.__isset.data_page_header_v2) {
229: 		throw std::runtime_error("Missing data page header from data page v2");
230: 	}
231: 
232: 	page_rows_available = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.num_values
233: 	                                                           : page_hdr.data_page_header_v2.num_values;
234: 	auto page_encoding = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.encoding
235: 	                                                          : page_hdr.data_page_header_v2.encoding;
236: 
237: 	if (HasRepeats()) {
238: 		uint32_t rep_length = page_hdr.type == PageType::DATA_PAGE
239: 		                          ? block->read<uint32_t>()
240: 		                          : page_hdr.data_page_header_v2.repetition_levels_byte_length;
241: 		block->available(rep_length);
242: 		repeated_decoder =
243: 		    make_unique<RleBpDecoder>((const uint8_t *)block->ptr, rep_length, ComputeBitWidth(max_repeat));
244: 		block->inc(rep_length);
245: 	}
246: 
247: 	if (HasDefines()) {
248: 		uint32_t def_length = page_hdr.type == PageType::DATA_PAGE
249: 		                          ? block->read<uint32_t>()
250: 		                          : page_hdr.data_page_header_v2.definition_levels_byte_length;
251: 		block->available(def_length);
252: 		defined_decoder =
253: 		    make_unique<RleBpDecoder>((const uint8_t *)block->ptr, def_length, ComputeBitWidth(max_define));
254: 		block->inc(def_length);
255: 	}
256: 
257: 	switch (page_encoding) {
258: 	case Encoding::RLE_DICTIONARY:
259: 	case Encoding::PLAIN_DICTIONARY: {
260: 		// TODO there seems to be some confusion whether this is in the bytes for v2
261: 		// where is it otherwise??
262: 		auto dict_width = block->read<uint8_t>();
263: 		// TODO somehow dict_width can be 0 ?
264: 		dict_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, block->len, dict_width);
265: 		block->inc(block->len);
266: 		break;
267: 	}
268: 	case Encoding::PLAIN:
269: 		// nothing to do here, will be read directly below
270: 		break;
271: 
272: 	default:
273: 		throw std::runtime_error("Unsupported page encoding");
274: 	}
275: }
276: 
277: idx_t ColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
278:                          Vector &result) {
279: 	// we need to reset the location because multiple column readers share the same protocol
280: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
281: 	trans.SetLocation(chunk_read_offset);
282: 
283: 	idx_t result_offset = 0;
284: 	auto to_read = num_values;
285: 
286: 	while (to_read > 0) {
287: 		while (page_rows_available == 0) {
288: 			PrepareRead(filter);
289: 		}
290: 
291: 		D_ASSERT(block);
292: 		auto read_now = MinValue<idx_t>(to_read, page_rows_available);
293: 
294: 		D_ASSERT(read_now <= STANDARD_VECTOR_SIZE);
295: 
296: 		if (HasRepeats()) {
297: 			D_ASSERT(repeated_decoder);
298: 			repeated_decoder->GetBatch<uint8_t>((char *)repeat_out + result_offset, read_now);
299: 		}
300: 
301: 		if (HasDefines()) {
302: 			D_ASSERT(defined_decoder);
303: 			defined_decoder->GetBatch<uint8_t>((char *)define_out + result_offset, read_now);
304: 		}
305: 
306: 		if (dict_decoder) {
307: 			// we need the null count because the offsets and plain values have no entries for nulls
308: 			idx_t null_count = 0;
309: 			if (HasDefines()) {
310: 				for (idx_t i = 0; i < read_now; i++) {
311: 					if (define_out[i + result_offset] != max_define) {
312: 						null_count++;
313: 					}
314: 				}
315: 			}
316: 
317: 			offset_buffer.resize(reader.allocator, sizeof(uint32_t) * (read_now - null_count));
318: 			dict_decoder->GetBatch<uint32_t>(offset_buffer.ptr, read_now - null_count);
319: 			DictReference(result);
320: 			Offsets((uint32_t *)offset_buffer.ptr, define_out, read_now, filter, result_offset, result);
321: 		} else {
322: 			PlainReference(block, result);
323: 			Plain(block, define_out, read_now, filter, result_offset, result);
324: 		}
325: 
326: 		result_offset += read_now;
327: 		page_rows_available -= read_now;
328: 		to_read -= read_now;
329: 	}
330: 	group_rows_available -= num_values;
331: 	chunk_read_offset = trans.GetLocation();
332: 
333: 	return num_values;
334: }
335: 
336: void ColumnReader::Skip(idx_t num_values) {
337: 	dummy_define.zero();
338: 	dummy_repeat.zero();
339: 
340: 	// TODO this can be optimized, for example we dont actually have to bitunpack offsets
341: 	Vector dummy_result(type, nullptr);
342: 	auto values_read =
343: 	    Read(num_values, none_filter, (uint8_t *)dummy_define.ptr, (uint8_t *)dummy_repeat.ptr, dummy_result);
344: 	if (values_read != num_values) {
345: 		throw std::runtime_error("Row count mismatch when skipping rows");
346: 	}
347: }
348: 
349: uint32_t StringColumnReader::VerifyString(const char *str_data, uint32_t str_len) {
350: 	if (Type() != LogicalTypeId::VARCHAR) {
351: 		return str_len;
352: 	}
353: 	// verify if a string is actually UTF8, and if there are no null bytes in the middle of the string
354: 	// technically Parquet should guarantee this, but reality is often disappointing
355: 	UnicodeInvalidReason reason;
356: 	size_t pos;
357: 	auto utf_type = Utf8Proc::Analyze(str_data, str_len, &reason, &pos);
358: 	if (utf_type == UnicodeType::INVALID) {
359: 		if (reason == UnicodeInvalidReason::NULL_BYTE) {
360: 			// for null bytes we just truncate the string
361: 			return pos;
362: 		}
363: 		throw InvalidInputException("Invalid string encoding found in Parquet file: value \"" +
364: 		                            Blob::ToString(string_t(str_data, str_len)) + "\" is not valid UTF8!");
365: 	}
366: 	return str_len;
367: }
368: 
369: void StringColumnReader::Dictionary(shared_ptr<ByteBuffer> data, idx_t num_entries) {
370: 	dict = move(data);
371: 	dict_strings = unique_ptr<string_t[]>(new string_t[num_entries]);
372: 	for (idx_t dict_idx = 0; dict_idx < num_entries; dict_idx++) {
373: 		uint32_t str_len = dict->read<uint32_t>();
374: 		dict->available(str_len);
375: 
376: 		auto actual_str_len = VerifyString(dict->ptr, str_len);
377: 		dict_strings[dict_idx] = string_t(dict->ptr, actual_str_len);
378: 		dict->inc(str_len);
379: 	}
380: }
381: 
382: class ParquetStringVectorBuffer : public VectorBuffer {
383: public:
384: 	explicit ParquetStringVectorBuffer(shared_ptr<ByteBuffer> buffer_p)
385: 	    : VectorBuffer(VectorBufferType::OPAQUE_BUFFER), buffer(move(buffer_p)) {
386: 	}
387: 
388: private:
389: 	shared_ptr<ByteBuffer> buffer;
390: };
391: 
392: void StringColumnReader::DictReference(Vector &result) {
393: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(dict));
394: }
395: void StringColumnReader::PlainReference(shared_ptr<ByteBuffer> plain_data, Vector &result) {
396: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(move(plain_data)));
397: }
398: 
399: string_t StringParquetValueConversion::DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
400: 	auto &dict_strings = ((StringColumnReader &)reader).dict_strings;
401: 	return dict_strings[offset];
402: }
403: 
404: string_t StringParquetValueConversion::PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
405: 	auto &scr = ((StringColumnReader &)reader);
406: 	uint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;
407: 	plain_data.available(str_len);
408: 	auto actual_str_len = ((StringColumnReader &)reader).VerifyString(plain_data.ptr, str_len);
409: 	auto ret_str = string_t(plain_data.ptr, actual_str_len);
410: 	plain_data.inc(str_len);
411: 	return ret_str;
412: }
413: 
414: void StringParquetValueConversion::PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
415: 	auto &scr = ((StringColumnReader &)reader);
416: 	uint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;
417: 	plain_data.available(str_len);
418: 	plain_data.inc(str_len);
419: }
420: 
421: idx_t ListColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
422:                              Vector &result_out) {
423: 	idx_t result_offset = 0;
424: 	auto result_ptr = FlatVector::GetData<list_entry_t>(result_out);
425: 	auto &result_mask = FlatVector::Validity(result_out);
426: 
427: 	D_ASSERT(ListVector::GetListSize(result_out) == 0);
428: 	// if an individual list is longer than STANDARD_VECTOR_SIZE we actually have to loop the child read to fill it
429: 	bool finished = false;
430: 	while (!finished) {
431: 		idx_t child_actual_num_values = 0;
432: 
433: 		// check if we have any overflow from a previous read
434: 		if (overflow_child_count == 0) {
435: 			// we don't: read elements from the child reader
436: 			child_defines.zero();
437: 			child_repeats.zero();
438: 			// we don't know in advance how many values to read because of the beautiful repetition/definition setup
439: 			// we just read (up to) a vector from the child column, and see if we have read enough
440: 			// if we have not read enough, we read another vector
441: 			// if we have read enough, we leave any unhandled elements in the overflow vector for a subsequent read
442: 			auto child_req_num_values =
443: 			    MinValue<idx_t>(STANDARD_VECTOR_SIZE, child_column_reader->GroupRowsAvailable());
444: 			read_vector.ResetFromCache(read_cache);
445: 			child_actual_num_values = child_column_reader->Read(child_req_num_values, child_filter, child_defines_ptr,
446: 			                                                    child_repeats_ptr, read_vector);
447: 		} else {
448: 			// we do: use the overflow values
449: 			child_actual_num_values = overflow_child_count;
450: 			overflow_child_count = 0;
451: 		}
452: 
453: 		if (child_actual_num_values == 0) {
454: 			// no more elements available: we are done
455: 			break;
456: 		}
457: 		read_vector.Verify(child_actual_num_values);
458: 		idx_t current_chunk_offset = ListVector::GetListSize(result_out);
459: 
460: 		// hard-won piece of code this, modify at your own risk
461: 		// the intuition is that we have to only collapse values into lists that are repeated *on this level*
462: 		// the rest is pretty much handed up as-is as a single-valued list or NULL
463: 		idx_t child_idx;
464: 		for (child_idx = 0; child_idx < child_actual_num_values; child_idx++) {
465: 			if (child_repeats_ptr[child_idx] == max_repeat) {
466: 				// value repeats on this level, append
467: 				D_ASSERT(result_offset > 0);
468: 				result_ptr[result_offset - 1].length++;
469: 				continue;
470: 			}
471: 
472: 			if (result_offset >= num_values) {
473: 				// we ran out of output space
474: 				finished = true;
475: 				break;
476: 			}
477: 			if (child_defines_ptr[child_idx] >= max_define) {
478: 				// value has been defined down the stack, hence its NOT NULL
479: 				result_ptr[result_offset].offset = child_idx + current_chunk_offset;
480: 				result_ptr[result_offset].length = 1;
481: 			} else {
482: 				// value is NULL somewhere up the stack
483: 				result_mask.SetInvalid(result_offset);
484: 				result_ptr[result_offset].offset = 0;
485: 				result_ptr[result_offset].length = 0;
486: 			}
487: 
488: 			repeat_out[result_offset] = child_repeats_ptr[child_idx];
489: 			define_out[result_offset] = child_defines_ptr[child_idx];
490: 
491: 			result_offset++;
492: 		}
493: 		// actually append the required elements to the child list
494: 		ListVector::Append(result_out, read_vector, child_idx);
495: 
496: 		// we have read more values from the child reader than we can fit into the result for this read
497: 		// we have to pass everything from child_idx to child_actual_num_values into the next call
498: 		if (child_idx < child_actual_num_values && result_offset == num_values) {
499: 			read_vector.Slice(read_vector, child_idx);
500: 			overflow_child_count = child_actual_num_values - child_idx;
501: 			read_vector.Verify(overflow_child_count);
502: 
503: 			// move values in the child repeats and defines *backward* by child_idx
504: 			for (idx_t repdef_idx = 0; repdef_idx < overflow_child_count; repdef_idx++) {
505: 				child_defines_ptr[repdef_idx] = child_defines_ptr[child_idx + repdef_idx];
506: 				child_repeats_ptr[repdef_idx] = child_repeats_ptr[child_idx + repdef_idx];
507: 			}
508: 		}
509: 	}
510: 	result_out.Verify(result_offset);
511: 	return result_offset;
512: }
513: 
514: ListColumnReader::ListColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
515:                                    idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p,
516:                                    unique_ptr<ColumnReader> child_column_reader_p)
517:     : ColumnReader(reader, move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p),
518:       child_column_reader(move(child_column_reader_p)), read_cache(ListType::GetChildType(Type())),
519:       read_vector(read_cache), overflow_child_count(0) {
520: 
521: 	child_defines.resize(reader.allocator, STANDARD_VECTOR_SIZE);
522: 	child_repeats.resize(reader.allocator, STANDARD_VECTOR_SIZE);
523: 	child_defines_ptr = (uint8_t *)child_defines.ptr;
524: 	child_repeats_ptr = (uint8_t *)child_repeats.ptr;
525: 
526: 	child_filter.set();
527: }
528: 
529: } // namespace duckdb
[end of extension/parquet/column_reader.cpp]
[start of extension/parquet/parquet_reader.cpp]
1: #include "parquet_reader.hpp"
2: #include "parquet_timestamp.hpp"
3: #include "parquet_statistics.hpp"
4: #include "column_reader.hpp"
5: 
6: #include "boolean_column_reader.hpp"
7: #include "callback_column_reader.hpp"
8: #include "decimal_column_reader.hpp"
9: #include "list_column_reader.hpp"
10: #include "string_column_reader.hpp"
11: #include "struct_column_reader.hpp"
12: #include "templated_column_reader.hpp"
13: 
14: #include "thrift_tools.hpp"
15: 
16: #include "parquet_file_metadata_cache.hpp"
17: 
18: #include "duckdb.hpp"
19: #ifndef DUCKDB_AMALGAMATION
20: #include "duckdb/planner/table_filter.hpp"
21: #include "duckdb/planner/filter/constant_filter.hpp"
22: #include "duckdb/planner/filter/null_filter.hpp"
23: #include "duckdb/planner/filter/conjunction_filter.hpp"
24: #include "duckdb/common/file_system.hpp"
25: #include "duckdb/common/string_util.hpp"
26: #include "duckdb/common/types/date.hpp"
27: #include "duckdb/common/pair.hpp"
28: 
29: #include "duckdb/storage/object_cache.hpp"
30: #endif
31: 
32: #include <sstream>
33: #include <cassert>
34: #include <chrono>
35: #include <cstring>
36: #include <iostream>
37: 
38: namespace duckdb {
39: 
40: using duckdb_parquet::format::ColumnChunk;
41: using duckdb_parquet::format::ConvertedType;
42: using duckdb_parquet::format::FieldRepetitionType;
43: using duckdb_parquet::format::FileMetaData;
44: using ParquetRowGroup = duckdb_parquet::format::RowGroup;
45: using duckdb_parquet::format::SchemaElement;
46: using duckdb_parquet::format::Statistics;
47: using duckdb_parquet::format::Type;
48: 
49: static unique_ptr<duckdb_apache::thrift::protocol::TProtocol> CreateThriftProtocol(Allocator &allocator,
50:                                                                                    FileHandle &file_handle) {
51: 	auto transport = make_shared<ThriftFileTransport>(allocator, file_handle);
52: 	return make_unique<duckdb_apache::thrift::protocol::TCompactProtocolT<ThriftFileTransport>>(move(transport));
53: }
54: 
55: static shared_ptr<ParquetFileMetadataCache> LoadMetadata(Allocator &allocator, FileHandle &file_handle) {
56: 	auto current_time = std::chrono::system_clock::to_time_t(std::chrono::system_clock::now());
57: 
58: 	auto proto = CreateThriftProtocol(allocator, file_handle);
59: 	auto &transport = ((ThriftFileTransport &)*proto->getTransport());
60: 	auto file_size = transport.GetSize();
61: 	if (file_size < 12) {
62: 		throw InvalidInputException("File '%s' too small to be a Parquet file", file_handle.path);
63: 	}
64: 
65: 	ResizeableBuffer buf;
66: 	buf.resize(allocator, 8);
67: 	buf.zero();
68: 
69: 	transport.SetLocation(file_size - 8);
70: 	transport.read((uint8_t *)buf.ptr, 8);
71: 
72: 	if (strncmp(buf.ptr + 4, "PAR1", 4) != 0) {
73: 		throw InvalidInputException("No magic bytes found at end of file '%s'", file_handle.path);
74: 	}
75: 	// read four-byte footer length from just before the end magic bytes
76: 	auto footer_len = *(uint32_t *)buf.ptr;
77: 	if (footer_len <= 0 || file_size < 12 + footer_len) {
78: 		throw InvalidInputException("Footer length error in file '%s'", file_handle.path);
79: 	}
80: 	auto metadata_pos = file_size - (footer_len + 8);
81: 	transport.SetLocation(metadata_pos);
82: 	transport.Prefetch(metadata_pos, footer_len);
83: 
84: 	auto metadata = make_unique<FileMetaData>();
85: 	metadata->read(proto.get());
86: 	return make_shared<ParquetFileMetadataCache>(move(metadata), current_time);
87: }
88: 
89: LogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele) {
90: 	// inner node
91: 	D_ASSERT(s_ele.__isset.type && s_ele.num_children == 0);
92: 	switch (s_ele.type) {
93: 	case Type::BOOLEAN:
94: 		return LogicalType::BOOLEAN;
95: 	case Type::INT32:
96: 		if (s_ele.__isset.converted_type) {
97: 			switch (s_ele.converted_type) {
98: 			case ConvertedType::DATE:
99: 				return LogicalType::DATE;
100: 			case ConvertedType::UINT_8:
101: 				return LogicalType::UTINYINT;
102: 			case ConvertedType::UINT_16:
103: 				return LogicalType::USMALLINT;
104: 			default:
105: 				return LogicalType::INTEGER;
106: 			}
107: 		}
108: 		return LogicalType::INTEGER;
109: 	case Type::INT64:
110: 		if (s_ele.__isset.converted_type) {
111: 			switch (s_ele.converted_type) {
112: 			case ConvertedType::TIMESTAMP_MICROS:
113: 			case ConvertedType::TIMESTAMP_MILLIS:
114: 				return LogicalType::TIMESTAMP;
115: 			case ConvertedType::UINT_32:
116: 				return LogicalType::UINTEGER;
117: 			case ConvertedType::UINT_64:
118: 				return LogicalType::UBIGINT;
119: 			default:
120: 				return LogicalType::BIGINT;
121: 			}
122: 		}
123: 		return LogicalType::BIGINT;
124: 
125: 	case Type::INT96: // always a timestamp it would seem
126: 		return LogicalType::TIMESTAMP;
127: 	case Type::FLOAT:
128: 		return LogicalType::FLOAT;
129: 	case Type::DOUBLE:
130: 		return LogicalType::DOUBLE;
131: 	case Type::BYTE_ARRAY:
132: 	case Type::FIXED_LEN_BYTE_ARRAY:
133: 		if (s_ele.type == Type::FIXED_LEN_BYTE_ARRAY && !s_ele.__isset.type_length) {
134: 			return LogicalType::INVALID;
135: 		}
136: 		if (s_ele.__isset.converted_type) {
137: 			switch (s_ele.converted_type) {
138: 			case ConvertedType::DECIMAL:
139: 				if (s_ele.type == Type::FIXED_LEN_BYTE_ARRAY && s_ele.__isset.scale && s_ele.__isset.type_length) {
140: 					return LogicalType::DECIMAL(s_ele.precision, s_ele.scale);
141: 				}
142: 				return LogicalType::INVALID;
143: 
144: 			case ConvertedType::UTF8:
145: 				return LogicalType::VARCHAR;
146: 			default:
147: 				return LogicalType::BLOB;
148: 			}
149: 		}
150: 		if (parquet_options.binary_as_string) {
151: 			return LogicalType::VARCHAR;
152: 		}
153: 		return LogicalType::BLOB;
154: 	default:
155: 		return LogicalType::INVALID;
156: 	}
157: }
158: 
159: unique_ptr<ColumnReader> ParquetReader::CreateReaderRecursive(const FileMetaData *file_meta_data, idx_t depth,
160:                                                               idx_t max_define, idx_t max_repeat,
161:                                                               idx_t &next_schema_idx, idx_t &next_file_idx) {
162: 	D_ASSERT(file_meta_data);
163: 	D_ASSERT(next_schema_idx < file_meta_data->schema.size());
164: 	auto &s_ele = file_meta_data->schema[next_schema_idx];
165: 	auto this_idx = next_schema_idx;
166: 
167: 	if (s_ele.__isset.repetition_type) {
168: 		if (s_ele.repetition_type != FieldRepetitionType::REQUIRED) {
169: 			max_define++;
170: 		}
171: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
172: 			max_repeat++;
173: 		}
174: 	}
175: 
176: 	if (!s_ele.__isset.type) { // inner node
177: 		if (s_ele.num_children == 0) {
178: 			throw std::runtime_error("Node has no children but should");
179: 		}
180: 		child_list_t<LogicalType> child_types;
181: 		vector<unique_ptr<ColumnReader>> child_readers;
182: 
183: 		idx_t c_idx = 0;
184: 		while (c_idx < (idx_t)s_ele.num_children) {
185: 			next_schema_idx++;
186: 
187: 			auto &child_ele = file_meta_data->schema[next_schema_idx];
188: 
189: 			auto child_reader = CreateReaderRecursive(file_meta_data, depth + 1, max_define, max_repeat,
190: 			                                          next_schema_idx, next_file_idx);
191: 			child_types.push_back(make_pair(child_ele.name, child_reader->Type()));
192: 			child_readers.push_back(move(child_reader));
193: 
194: 			c_idx++;
195: 		}
196: 		D_ASSERT(!child_types.empty());
197: 		unique_ptr<ColumnReader> result;
198: 		LogicalType result_type;
199: 		// if we only have a single child no reason to create a struct ay
200: 		if (child_types.size() > 1 || depth == 0) {
201: 			result_type = LogicalType::STRUCT(move(child_types));
202: 			result = make_unique<StructColumnReader>(*this, result_type, s_ele, this_idx, max_define, max_repeat,
203: 			                                         move(child_readers));
204: 		} else {
205: 			// if we have a struct with only a single type, pull up
206: 			result_type = child_types[0].second;
207: 			result = move(child_readers[0]);
208: 		}
209: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
210: 			result_type = LogicalType::LIST(result_type);
211: 			return make_unique<ListColumnReader>(*this, result_type, s_ele, this_idx, max_define, max_repeat,
212: 			                                     move(result));
213: 		}
214: 		return result;
215: 	} else { // leaf node
216: 		// TODO check return value of derive type or should we only do this on read()
217: 		return ColumnReader::CreateReader(*this, DeriveLogicalType(s_ele), s_ele, next_file_idx++, max_define,
218: 		                                  max_repeat);
219: 	}
220: }
221: 
222: // TODO we don't need readers for columns we are not going to read ay
223: unique_ptr<ColumnReader> ParquetReader::CreateReader(const duckdb_parquet::format::FileMetaData *file_meta_data) {
224: 	idx_t next_schema_idx = 0;
225: 	idx_t next_file_idx = 0;
226: 
227: 	auto ret = CreateReaderRecursive(file_meta_data, 0, 0, 0, next_schema_idx, next_file_idx);
228: 	D_ASSERT(next_schema_idx == file_meta_data->schema.size() - 1);
229: 	D_ASSERT(file_meta_data->row_groups.empty() || next_file_idx == file_meta_data->row_groups[0].columns.size());
230: 	return ret;
231: }
232: 
233: void ParquetReader::InitializeSchema(const vector<LogicalType> &expected_types_p, const string &initial_filename_p) {
234: 	auto file_meta_data = GetFileMetadata();
235: 
236: 	if (file_meta_data->__isset.encryption_algorithm) {
237: 		throw FormatException("Encrypted Parquet files are not supported");
238: 	}
239: 	// check if we like this schema
240: 	if (file_meta_data->schema.size() < 2) {
241: 		throw FormatException("Need at least one non-root column in the file");
242: 	}
243: 
244: 	bool has_expected_types = !expected_types_p.empty();
245: 	auto root_reader = CreateReader(file_meta_data);
246: 
247: 	auto &root_type = root_reader->Type();
248: 	auto &child_types = StructType::GetChildTypes(root_type);
249: 	D_ASSERT(root_type.id() == LogicalTypeId::STRUCT);
250: 	if (has_expected_types && child_types.size() != expected_types_p.size()) {
251: 		throw FormatException("column count mismatch");
252: 	}
253: 	idx_t col_idx = 0;
254: 	for (auto &type_pair : child_types) {
255: 		if (has_expected_types && expected_types_p[col_idx] != type_pair.second) {
256: 			if (initial_filename_p.empty()) {
257: 				throw FormatException("column \"%d\" in parquet file is of type %s, could not auto cast to "
258: 				                      "expected type %s for this column",
259: 				                      col_idx, type_pair.second, expected_types_p[col_idx].ToString());
260: 			} else {
261: 				throw FormatException("schema mismatch in Parquet glob: column \"%d\" in parquet file is of type "
262: 				                      "%s, but in the original file \"%s\" this column is of type \"%s\"",
263: 				                      col_idx, type_pair.second, initial_filename_p,
264: 				                      expected_types_p[col_idx].ToString());
265: 			}
266: 		} else {
267: 			names.push_back(type_pair.first);
268: 			return_types.push_back(type_pair.second);
269: 		}
270: 		col_idx++;
271: 	}
272: 	D_ASSERT(!names.empty());
273: 	D_ASSERT(!return_types.empty());
274: }
275: 
276: ParquetReader::ParquetReader(Allocator &allocator_p, unique_ptr<FileHandle> file_handle_p,
277:                              const vector<LogicalType> &expected_types_p, const string &initial_filename_p)
278:     : allocator(allocator_p) {
279: 	file_name = file_handle_p->path;
280: 	file_handle = move(file_handle_p);
281: 	metadata = LoadMetadata(allocator, *file_handle);
282: 	InitializeSchema(expected_types_p, initial_filename_p);
283: }
284: 
285: ParquetReader::ParquetReader(ClientContext &context_p, string file_name_p, const vector<LogicalType> &expected_types_p,
286:                              ParquetOptions parquet_options_p, const string &initial_filename_p)
287:     : allocator(Allocator::Get(context_p)), file_opener(FileSystem::GetFileOpener(context_p)),
288:       parquet_options(parquet_options_p) {
289: 	auto &fs = FileSystem::GetFileSystem(context_p);
290: 	file_name = move(file_name_p);
291: 	file_handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,
292: 	                          FileSystem::DEFAULT_COMPRESSION, file_opener);
293: 	// If object cached is disabled
294: 	// or if this file has cached metadata
295: 	// or if the cached version already expired
296: 	auto last_modify_time = fs.GetLastModifiedTime(*file_handle);
297: 	if (!ObjectCache::ObjectCacheEnabled(context_p)) {
298: 		metadata = LoadMetadata(allocator, *file_handle);
299: 	} else {
300: 		metadata =
301: 		    std::dynamic_pointer_cast<ParquetFileMetadataCache>(ObjectCache::GetObjectCache(context_p).Get(file_name));
302: 		if (!metadata || (last_modify_time + 10 >= metadata->read_time)) {
303: 			metadata = LoadMetadata(allocator, *file_handle);
304: 			ObjectCache::GetObjectCache(context_p).Put(file_name, metadata);
305: 		}
306: 	}
307: 	InitializeSchema(expected_types_p, initial_filename_p);
308: }
309: 
310: ParquetReader::~ParquetReader() {
311: }
312: 
313: const FileMetaData *ParquetReader::GetFileMetadata() {
314: 	D_ASSERT(metadata);
315: 	D_ASSERT(metadata->metadata);
316: 	return metadata->metadata.get();
317: }
318: 
319: // TODO also somewhat ugly, perhaps this can be moved to the column reader too
320: unique_ptr<BaseStatistics> ParquetReader::ReadStatistics(ParquetReader &reader, LogicalType &type,
321:                                                          column_t file_col_idx, const FileMetaData *file_meta_data) {
322: 	unique_ptr<BaseStatistics> column_stats;
323: 	auto root_reader = reader.CreateReader(file_meta_data);
324: 	auto column_reader = ((StructColumnReader *)root_reader.get())->GetChildReader(file_col_idx);
325: 
326: 	for (auto &row_group : file_meta_data->row_groups) {
327: 		auto chunk_stats = column_reader->Stats(row_group.columns);
328: 		if (!chunk_stats) {
329: 			return nullptr;
330: 		}
331: 		if (!column_stats) {
332: 			column_stats = move(chunk_stats);
333: 		} else {
334: 			column_stats->Merge(*chunk_stats);
335: 		}
336: 	}
337: 	return column_stats;
338: }
339: 
340: const ParquetRowGroup &ParquetReader::GetGroup(ParquetReaderScanState &state) {
341: 	auto file_meta_data = GetFileMetadata();
342: 	D_ASSERT(state.current_group >= 0 && (idx_t)state.current_group < state.group_idx_list.size());
343: 	D_ASSERT(state.group_idx_list[state.current_group] >= 0 &&
344: 	         state.group_idx_list[state.current_group] < file_meta_data->row_groups.size());
345: 	return file_meta_data->row_groups[state.group_idx_list[state.current_group]];
346: }
347: 
348: void ParquetReader::PrepareRowGroupBuffer(ParquetReaderScanState &state, idx_t out_col_idx) {
349: 	auto &group = GetGroup(state);
350: 
351: 	auto column_reader = ((StructColumnReader *)state.root_reader.get())->GetChildReader(state.column_ids[out_col_idx]);
352: 
353: 	// TODO move this to columnreader too
354: 	if (state.filters) {
355: 		auto stats = column_reader->Stats(group.columns);
356: 		// filters contain output chunk index, not file col idx!
357: 		auto filter_entry = state.filters->filters.find(out_col_idx);
358: 		if (stats && filter_entry != state.filters->filters.end()) {
359: 			bool skip_chunk = false;
360: 			auto &filter = *filter_entry->second;
361: 			auto prune_result = filter.CheckStatistics(*stats);
362: 			if (prune_result == FilterPropagateResult::FILTER_ALWAYS_FALSE) {
363: 				skip_chunk = true;
364: 			}
365: 			if (skip_chunk) {
366: 				state.group_offset = group.num_rows;
367: 				return;
368: 				// this effectively will skip this chunk
369: 			}
370: 		}
371: 	}
372: 
373: 	state.root_reader->InitializeRead(group.columns, *state.thrift_file_proto);
374: }
375: 
376: idx_t ParquetReader::NumRows() {
377: 	return GetFileMetadata()->num_rows;
378: }
379: 
380: idx_t ParquetReader::NumRowGroups() {
381: 	return GetFileMetadata()->row_groups.size();
382: }
383: 
384: void ParquetReader::InitializeScan(ParquetReaderScanState &state, vector<column_t> column_ids,
385:                                    vector<idx_t> groups_to_read, TableFilterSet *filters) {
386: 	state.current_group = -1;
387: 	state.finished = false;
388: 	state.column_ids = move(column_ids);
389: 	state.group_offset = 0;
390: 	state.group_idx_list = move(groups_to_read);
391: 	state.filters = filters;
392: 	state.sel.Initialize(STANDARD_VECTOR_SIZE);
393: 	state.file_handle =
394: 	    file_handle->file_system.OpenFile(file_handle->path, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,
395: 	                                      FileSystem::DEFAULT_COMPRESSION, file_opener);
396: 	state.thrift_file_proto = CreateThriftProtocol(allocator, *state.file_handle);
397: 	state.root_reader = CreateReader(GetFileMetadata());
398: 
399: 	state.define_buf.resize(allocator, STANDARD_VECTOR_SIZE);
400: 	state.repeat_buf.resize(allocator, STANDARD_VECTOR_SIZE);
401: }
402: 
403: void FilterIsNull(Vector &v, parquet_filter_t &filter_mask, idx_t count) {
404: 	auto &mask = FlatVector::Validity(v);
405: 	if (mask.AllValid()) {
406: 		filter_mask.reset();
407: 	} else {
408: 		for (idx_t i = 0; i < count; i++) {
409: 			filter_mask[i] = filter_mask[i] && !mask.RowIsValid(i);
410: 		}
411: 	}
412: }
413: 
414: void FilterIsNotNull(Vector &v, parquet_filter_t &filter_mask, idx_t count) {
415: 	auto &mask = FlatVector::Validity(v);
416: 	if (!mask.AllValid()) {
417: 		for (idx_t i = 0; i < count; i++) {
418: 			filter_mask[i] = filter_mask[i] && mask.RowIsValid(i);
419: 		}
420: 	}
421: }
422: 
423: template <class T, class OP>
424: void TemplatedFilterOperation(Vector &v, T constant, parquet_filter_t &filter_mask, idx_t count) {
425: 	D_ASSERT(v.GetVectorType() == VectorType::FLAT_VECTOR); // we just created the damn thing it better be
426: 
427: 	auto v_ptr = FlatVector::GetData<T>(v);
428: 	auto &mask = FlatVector::Validity(v);
429: 
430: 	if (!mask.AllValid()) {
431: 		for (idx_t i = 0; i < count; i++) {
432: 			if (mask.RowIsValid(i)) {
433: 				filter_mask[i] = filter_mask[i] && OP::Operation(v_ptr[i], constant);
434: 			}
435: 		}
436: 	} else {
437: 		for (idx_t i = 0; i < count; i++) {
438: 			filter_mask[i] = filter_mask[i] && OP::Operation(v_ptr[i], constant);
439: 		}
440: 	}
441: }
442: 
443: template <class OP>
444: static void FilterOperationSwitch(Vector &v, Value &constant, parquet_filter_t &filter_mask, idx_t count) {
445: 	if (filter_mask.none() || count == 0) {
446: 		return;
447: 	}
448: 	switch (v.GetType().id()) {
449: 	case LogicalTypeId::BOOLEAN:
450: 		TemplatedFilterOperation<bool, OP>(v, constant.value_.boolean, filter_mask, count);
451: 		break;
452: 	case LogicalTypeId::UTINYINT:
453: 		TemplatedFilterOperation<uint8_t, OP>(v, constant.value_.utinyint, filter_mask, count);
454: 		break;
455: 	case LogicalTypeId::USMALLINT:
456: 		TemplatedFilterOperation<uint16_t, OP>(v, constant.value_.usmallint, filter_mask, count);
457: 		break;
458: 	case LogicalTypeId::UINTEGER:
459: 		TemplatedFilterOperation<uint32_t, OP>(v, constant.value_.uinteger, filter_mask, count);
460: 		break;
461: 	case LogicalTypeId::UBIGINT:
462: 		TemplatedFilterOperation<uint64_t, OP>(v, constant.value_.ubigint, filter_mask, count);
463: 		break;
464: 	case LogicalTypeId::INTEGER:
465: 		TemplatedFilterOperation<int32_t, OP>(v, constant.value_.integer, filter_mask, count);
466: 		break;
467: 	case LogicalTypeId::BIGINT:
468: 		TemplatedFilterOperation<int64_t, OP>(v, constant.value_.bigint, filter_mask, count);
469: 		break;
470: 	case LogicalTypeId::FLOAT:
471: 		TemplatedFilterOperation<float, OP>(v, constant.value_.float_, filter_mask, count);
472: 		break;
473: 	case LogicalTypeId::DOUBLE:
474: 		TemplatedFilterOperation<double, OP>(v, constant.value_.double_, filter_mask, count);
475: 		break;
476: 	case LogicalTypeId::DATE:
477: 		TemplatedFilterOperation<date_t, OP>(v, constant.value_.date, filter_mask, count);
478: 		break;
479: 	case LogicalTypeId::TIMESTAMP:
480: 		TemplatedFilterOperation<timestamp_t, OP>(v, constant.value_.timestamp, filter_mask, count);
481: 		break;
482: 	case LogicalTypeId::BLOB:
483: 	case LogicalTypeId::VARCHAR:
484: 		TemplatedFilterOperation<string_t, OP>(v, string_t(constant.str_value), filter_mask, count);
485: 		break;
486: 	case LogicalTypeId::DECIMAL:
487: 		switch (v.GetType().InternalType()) {
488: 		case PhysicalType::INT16:
489: 			TemplatedFilterOperation<int16_t, OP>(v, constant.value_.smallint, filter_mask, count);
490: 			break;
491: 		case PhysicalType::INT32:
492: 			TemplatedFilterOperation<int32_t, OP>(v, constant.value_.integer, filter_mask, count);
493: 			break;
494: 		case PhysicalType::INT64:
495: 			TemplatedFilterOperation<int64_t, OP>(v, constant.value_.bigint, filter_mask, count);
496: 			break;
497: 		case PhysicalType::INT128:
498: 			TemplatedFilterOperation<hugeint_t, OP>(v, constant.value_.hugeint, filter_mask, count);
499: 			break;
500: 		default:
501: 			throw InternalException("Unsupported internal type for decimal");
502: 		}
503: 		break;
504: 	default:
505: 		throw NotImplementedException("Unsupported type for filter %s", v.ToString());
506: 	}
507: }
508: 
509: static void ApplyFilter(Vector &v, TableFilter &filter, parquet_filter_t &filter_mask, idx_t count) {
510: 	switch (filter.filter_type) {
511: 	case TableFilterType::CONJUNCTION_AND: {
512: 		auto &conjunction = (ConjunctionAndFilter &)filter;
513: 		for (auto &child_filter : conjunction.child_filters) {
514: 			ApplyFilter(v, *child_filter, filter_mask, count);
515: 		}
516: 		break;
517: 	}
518: 	case TableFilterType::CONJUNCTION_OR: {
519: 		auto &conjunction = (ConjunctionOrFilter &)filter;
520: 		for (auto &child_filter : conjunction.child_filters) {
521: 			parquet_filter_t child_mask = filter_mask;
522: 			ApplyFilter(v, *child_filter, child_mask, count);
523: 			filter_mask |= child_mask;
524: 		}
525: 		break;
526: 	}
527: 	case TableFilterType::CONSTANT_COMPARISON: {
528: 		auto &constant_filter = (ConstantFilter &)filter;
529: 		switch (constant_filter.comparison_type) {
530: 		case ExpressionType::COMPARE_EQUAL:
531: 			FilterOperationSwitch<Equals>(v, constant_filter.constant, filter_mask, count);
532: 			break;
533: 		case ExpressionType::COMPARE_LESSTHAN:
534: 			FilterOperationSwitch<LessThan>(v, constant_filter.constant, filter_mask, count);
535: 			break;
536: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
537: 			FilterOperationSwitch<LessThanEquals>(v, constant_filter.constant, filter_mask, count);
538: 			break;
539: 		case ExpressionType::COMPARE_GREATERTHAN:
540: 			FilterOperationSwitch<GreaterThan>(v, constant_filter.constant, filter_mask, count);
541: 			break;
542: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
543: 			FilterOperationSwitch<GreaterThanEquals>(v, constant_filter.constant, filter_mask, count);
544: 			break;
545: 		default:
546: 			D_ASSERT(0);
547: 		}
548: 		break;
549: 	}
550: 	case TableFilterType::IS_NOT_NULL:
551: 		FilterIsNotNull(v, filter_mask, count);
552: 		break;
553: 	case TableFilterType::IS_NULL:
554: 		FilterIsNull(v, filter_mask, count);
555: 		break;
556: 	default:
557: 		D_ASSERT(0);
558: 		break;
559: 	}
560: }
561: 
562: void ParquetReader::Scan(ParquetReaderScanState &state, DataChunk &result) {
563: 	while (ScanInternal(state, result)) {
564: 		if (result.size() > 0) {
565: 			break;
566: 		}
567: 		result.Reset();
568: 	}
569: }
570: 
571: bool ParquetReader::ScanInternal(ParquetReaderScanState &state, DataChunk &result) {
572: 	if (state.finished) {
573: 		return false;
574: 	}
575: 
576: 	// see if we have to switch to the next row group in the parquet file
577: 	if (state.current_group < 0 || (int64_t)state.group_offset >= GetGroup(state).num_rows) {
578: 		state.current_group++;
579: 		state.group_offset = 0;
580: 
581: 		if ((idx_t)state.current_group == state.group_idx_list.size()) {
582: 			state.finished = true;
583: 			return false;
584: 		}
585: 
586: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
587: 			// this is a special case where we are not interested in the actual contents of the file
588: 			if (state.column_ids[out_col_idx] == COLUMN_IDENTIFIER_ROW_ID) {
589: 				continue;
590: 			}
591: 
592: 			PrepareRowGroupBuffer(state, out_col_idx);
593: 		}
594: 		return true;
595: 	}
596: 
597: 	auto this_output_chunk_rows = MinValue<idx_t>(STANDARD_VECTOR_SIZE, GetGroup(state).num_rows - state.group_offset);
598: 	result.SetCardinality(this_output_chunk_rows);
599: 
600: 	if (this_output_chunk_rows == 0) {
601: 		state.finished = true;
602: 		return false; // end of last group, we are done
603: 	}
604: 
605: 	// we evaluate simple table filters directly in this scan so we can skip decoding column data that's never going to
606: 	// be relevant
607: 	parquet_filter_t filter_mask;
608: 	filter_mask.set();
609: 
610: 	state.define_buf.zero();
611: 	state.repeat_buf.zero();
612: 
613: 	auto define_ptr = (uint8_t *)state.define_buf.ptr;
614: 	auto repeat_ptr = (uint8_t *)state.repeat_buf.ptr;
615: 
616: 	auto root_reader = ((StructColumnReader *)state.root_reader.get());
617: 
618: 	if (state.filters) {
619: 		vector<bool> need_to_read(result.ColumnCount(), true);
620: 
621: 		// first load the columns that are used in filters
622: 		for (auto &filter_col : state.filters->filters) {
623: 			auto file_col_idx = state.column_ids[filter_col.first];
624: 
625: 			if (filter_mask.none()) { // if no rows are left we can stop checking filters
626: 				break;
627: 			}
628: 
629: 			root_reader->GetChildReader(file_col_idx)
630: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[filter_col.first]);
631: 
632: 			need_to_read[filter_col.first] = false;
633: 
634: 			ApplyFilter(result.data[filter_col.first], *filter_col.second, filter_mask, this_output_chunk_rows);
635: 		}
636: 
637: 		// we still may have to read some cols
638: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
639: 			if (!need_to_read[out_col_idx]) {
640: 				continue;
641: 			}
642: 			auto file_col_idx = state.column_ids[out_col_idx];
643: 
644: 			if (filter_mask.none()) {
645: 				root_reader->GetChildReader(file_col_idx)->Skip(result.size());
646: 				continue;
647: 			}
648: 			// TODO handle ROWID here, too
649: 			root_reader->GetChildReader(file_col_idx)
650: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);
651: 		}
652: 
653: 		idx_t sel_size = 0;
654: 		for (idx_t i = 0; i < this_output_chunk_rows; i++) {
655: 			if (filter_mask[i]) {
656: 				state.sel.set_index(sel_size++, i);
657: 			}
658: 		}
659: 
660: 		result.Slice(state.sel, sel_size);
661: 		result.Verify();
662: 
663: 	} else { // #nofilter, just fricking load the data
664: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
665: 			auto file_col_idx = state.column_ids[out_col_idx];
666: 
667: 			if (file_col_idx == COLUMN_IDENTIFIER_ROW_ID) {
668: 				Value constant_42 = Value::BIGINT(42);
669: 				result.data[out_col_idx].Reference(constant_42);
670: 				continue;
671: 			}
672: 
673: 			root_reader->GetChildReader(file_col_idx)
674: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);
675: 		}
676: 	}
677: 
678: 	state.group_offset += this_output_chunk_rows;
679: 	return true;
680: }
681: 
682: } // namespace duckdb
[end of extension/parquet/parquet_reader.cpp]
[start of extension/parquet/parquet_writer.cpp]
1: #include "parquet_writer.hpp"
2: #include "parquet_timestamp.hpp"
3: 
4: #include "duckdb.hpp"
5: #ifndef DUCKDB_AMALGAMATION
6: #include "duckdb/function/table_function.hpp"
7: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
8: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
9: #include "duckdb/main/client_context.hpp"
10: #include "duckdb/main/connection.hpp"
11: #include "duckdb/common/file_system.hpp"
12: #include "duckdb/common/string_util.hpp"
13: #include "duckdb/common/types/date.hpp"
14: #include "duckdb/common/types/time.hpp"
15: #include "duckdb/common/types/timestamp.hpp"
16: #include "duckdb/common/serializer/buffered_file_writer.hpp"
17: #include "duckdb/common/serializer/buffered_serializer.hpp"
18: #endif
19: 
20: #include "snappy.h"
21: #include "miniz_wrapper.hpp"
22: #include "zstd.h"
23: 
24: namespace duckdb {
25: 
26: using namespace duckdb_parquet;                   // NOLINT
27: using namespace duckdb_apache::thrift;            // NOLINT
28: using namespace duckdb_apache::thrift::protocol;  // NOLINT
29: using namespace duckdb_apache::thrift::transport; // NOLINT
30: using namespace duckdb_miniz;                     // NOLINT
31: 
32: using duckdb_parquet::format::CompressionCodec;
33: using duckdb_parquet::format::ConvertedType;
34: using duckdb_parquet::format::Encoding;
35: using duckdb_parquet::format::FieldRepetitionType;
36: using duckdb_parquet::format::FileMetaData;
37: using duckdb_parquet::format::PageHeader;
38: using duckdb_parquet::format::PageType;
39: using ParquetRowGroup = duckdb_parquet::format::RowGroup;
40: using duckdb_parquet::format::Type;
41: 
42: class MyTransport : public TTransport {
43: public:
44: 	explicit MyTransport(Serializer &serializer) : serializer(serializer) {
45: 	}
46: 
47: 	bool isOpen() const override {
48: 		return true;
49: 	}
50: 
51: 	void open() override {
52: 	}
53: 
54: 	void close() override {
55: 	}
56: 
57: 	void write_virt(const uint8_t *buf, uint32_t len) override {
58: 		serializer.WriteData((const_data_ptr_t)buf, len);
59: 	}
60: 
61: private:
62: 	Serializer &serializer;
63: };
64: 
65: static Type::type DuckDBTypeToParquetType(const LogicalType &duckdb_type) {
66: 	switch (duckdb_type.id()) {
67: 	case LogicalTypeId::BOOLEAN:
68: 		return Type::BOOLEAN;
69: 	case LogicalTypeId::TINYINT:
70: 	case LogicalTypeId::SMALLINT:
71: 	case LogicalTypeId::INTEGER:
72: 		return Type::INT32;
73: 	case LogicalTypeId::BIGINT:
74: 		return Type::INT64;
75: 	case LogicalTypeId::FLOAT:
76: 		return Type::FLOAT;
77: 	case LogicalTypeId::DECIMAL: // for now...
78: 	case LogicalTypeId::DOUBLE:
79: 		return Type::DOUBLE;
80: 	case LogicalTypeId::VARCHAR:
81: 	case LogicalTypeId::BLOB:
82: 		return Type::BYTE_ARRAY;
83: 	case LogicalTypeId::DATE:
84: 	case LogicalTypeId::TIMESTAMP:
85: 		return Type::INT96;
86: 	default:
87: 		throw NotImplementedException(duckdb_type.ToString());
88: 	}
89: }
90: 
91: static bool DuckDBTypeToConvertedType(const LogicalType &duckdb_type, ConvertedType::type &result) {
92: 	switch (duckdb_type.id()) {
93: 	case LogicalTypeId::VARCHAR:
94: 		result = ConvertedType::UTF8;
95: 		return true;
96: 	default:
97: 		return false;
98: 	}
99: }
100: 
101: static void VarintEncode(uint32_t val, Serializer &ser) {
102: 	do {
103: 		uint8_t byte = val & 127;
104: 		val >>= 7;
105: 		if (val != 0) {
106: 			byte |= 128;
107: 		}
108: 		ser.Write<uint8_t>(byte);
109: 	} while (val != 0);
110: }
111: 
112: static uint8_t GetVarintSize(uint32_t val) {
113: 	uint8_t res = 0;
114: 	do {
115: 		uint8_t byte = val & 127;
116: 		val >>= 7;
117: 		if (val != 0) {
118: 			byte |= 128;
119: 		}
120: 		res++;
121: 	} while (val != 0);
122: 	return res;
123: }
124: 
125: template <class SRC, class TGT>
126: static void TemplatedWritePlain(Vector &col, idx_t length, ValidityMask &mask, Serializer &ser) {
127: 	auto *ptr = FlatVector::GetData<SRC>(col);
128: 	for (idx_t r = 0; r < length; r++) {
129: 		if (mask.RowIsValid(r)) {
130: 			ser.Write<TGT>((TGT)ptr[r]);
131: 		}
132: 	}
133: }
134: 
135: ParquetWriter::ParquetWriter(FileSystem &fs, string file_name_p, FileOpener *file_opener_p, vector<LogicalType> types_p,
136:                              vector<string> names_p, CompressionCodec::type codec)
137:     : file_name(move(file_name_p)), sql_types(move(types_p)), column_names(move(names_p)), codec(codec) {
138: #if STANDARD_VECTOR_SIZE < 64
139: 	throw NotImplementedException("Parquet writer is not supported for vector sizes < 64");
140: #endif
141: 
142: 	// initialize the file writer
143: 	writer = make_unique<BufferedFileWriter>(
144: 	    fs, file_name.c_str(), FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE_NEW, file_opener_p);
145: 	// parquet files start with the string "PAR1"
146: 	writer->WriteData((const_data_ptr_t) "PAR1", 4);
147: 	TCompactProtocolFactoryT<MyTransport> tproto_factory;
148: 	protocol = tproto_factory.getProtocol(make_shared<MyTransport>(*writer));
149: 
150: 	file_meta_data.num_rows = 0;
151: 	file_meta_data.version = 1;
152: 
153: 	file_meta_data.__isset.created_by = true;
154: 	file_meta_data.created_by = "DuckDB";
155: 
156: 	file_meta_data.schema.resize(sql_types.size() + 1);
157: 
158: 	// populate root schema object
159: 	file_meta_data.schema[0].name = "duckdb_schema";
160: 	file_meta_data.schema[0].num_children = sql_types.size();
161: 	file_meta_data.schema[0].__isset.num_children = true;
162: 
163: 	for (idx_t i = 0; i < sql_types.size(); i++) {
164: 		auto &schema_element = file_meta_data.schema[i + 1];
165: 
166: 		schema_element.type = DuckDBTypeToParquetType(sql_types[i]);
167: 		schema_element.repetition_type = FieldRepetitionType::OPTIONAL;
168: 		schema_element.num_children = 0;
169: 		schema_element.__isset.num_children = true;
170: 		schema_element.__isset.type = true;
171: 		schema_element.__isset.repetition_type = true;
172: 		schema_element.name = column_names[i];
173: 		schema_element.__isset.converted_type = DuckDBTypeToConvertedType(sql_types[i], schema_element.converted_type);
174: 	}
175: }
176: 
177: void ParquetWriter::Flush(ChunkCollection &buffer) {
178: 	if (buffer.Count() == 0) {
179: 		return;
180: 	}
181: 	lock_guard<mutex> glock(lock);
182: 
183: 	// set up a new row group for this chunk collection
184: 	ParquetRowGroup row_group;
185: 	row_group.num_rows = 0;
186: 	row_group.file_offset = writer->GetTotalWritten();
187: 	row_group.__isset.file_offset = true;
188: 	row_group.columns.resize(buffer.ColumnCount());
189: 
190: 	// iterate over each of the columns of the chunk collection and write them
191: 	for (idx_t i = 0; i < buffer.ColumnCount(); i++) {
192: 		// we start off by writing everything into a temporary buffer
193: 		// this is necessary to (1) know the total written size, and (2) to compress it afterwards
194: 		BufferedSerializer temp_writer;
195: 
196: 		// set up some metadata
197: 		PageHeader hdr;
198: 		hdr.compressed_page_size = 0;
199: 		hdr.uncompressed_page_size = 0;
200: 		hdr.type = PageType::DATA_PAGE;
201: 		hdr.__isset.data_page_header = true;
202: 
203: 		hdr.data_page_header.num_values = buffer.Count();
204: 		hdr.data_page_header.encoding = Encoding::PLAIN;
205: 		hdr.data_page_header.definition_level_encoding = Encoding::RLE;
206: 		hdr.data_page_header.repetition_level_encoding = Encoding::BIT_PACKED;
207: 
208: 		// record the current offset of the writer into the file
209: 		// this is the starting position of the current page
210: 		auto start_offset = writer->GetTotalWritten();
211: 
212: 		// write the definition levels (i.e. the inverse of the nullmask)
213: 		// we always bit pack everything
214: 
215: 		// first figure out how many bytes we need (1 byte per 8 rows, rounded up)
216: 		auto define_byte_count = (buffer.Count() + 7) / 8;
217: 		// we need to set up the count as a varint, plus an added marker for the RLE scheme
218: 		// for this marker we shift the count left 1 and set low bit to 1 to indicate bit packed literals
219: 		uint32_t define_header = (define_byte_count << 1) | 1;
220: 		uint32_t define_size = GetVarintSize(define_header) + define_byte_count;
221: 
222: 		// we write the actual definitions into the temp_writer for now
223: 		temp_writer.Write<uint32_t>(define_size);
224: 		VarintEncode(define_header, temp_writer);
225: 
226: 		for (auto &chunk : buffer.Chunks()) {
227: 			auto &validity = FlatVector::Validity(chunk->data[i]);
228: 			auto validity_data = validity.GetData();
229: 			auto chunk_define_byte_count = (chunk->size() + 7) / 8;
230: 			if (!validity_data) {
231: 				ValidityMask nop_mask(chunk->size());
232: 				temp_writer.WriteData((const_data_ptr_t)nop_mask.GetData(), chunk_define_byte_count);
233: 			} else {
234: 				// write the bits of the nullmask
235: 				temp_writer.WriteData((const_data_ptr_t)validity_data, chunk_define_byte_count);
236: 			}
237: 		}
238: 
239: 		// now write the actual payload: we write this as PLAIN values (for now? possibly for ever?)
240: 		for (auto &chunk : buffer.Chunks()) {
241: 			auto &input = *chunk;
242: 			auto &input_column = input.data[i];
243: 			auto &mask = FlatVector::Validity(input_column);
244: 
245: 			// write actual payload data
246: 			switch (sql_types[i].id()) {
247: 			case LogicalTypeId::BOOLEAN: {
248: 				auto *ptr = FlatVector::GetData<bool>(input_column);
249: 				uint8_t byte = 0;
250: 				uint8_t byte_pos = 0;
251: 				for (idx_t r = 0; r < input.size(); r++) {
252: 					if (mask.RowIsValid(r)) { // only encode if non-null
253: 						byte |= (ptr[r] & 1) << byte_pos;
254: 						byte_pos++;
255: 
256: 						if (byte_pos == 8) {
257: 							temp_writer.Write<uint8_t>(byte);
258: 							byte = 0;
259: 							byte_pos = 0;
260: 						}
261: 					}
262: 				}
263: 				// flush last byte if req
264: 				if (byte_pos > 0) {
265: 					temp_writer.Write<uint8_t>(byte);
266: 				}
267: 				break;
268: 			}
269: 			case LogicalTypeId::TINYINT:
270: 				TemplatedWritePlain<int8_t, int32_t>(input_column, input.size(), mask, temp_writer);
271: 				break;
272: 			case LogicalTypeId::SMALLINT:
273: 				TemplatedWritePlain<int16_t, int32_t>(input_column, input.size(), mask, temp_writer);
274: 				break;
275: 			case LogicalTypeId::INTEGER:
276: 				TemplatedWritePlain<int32_t, int32_t>(input_column, input.size(), mask, temp_writer);
277: 				break;
278: 			case LogicalTypeId::BIGINT:
279: 				TemplatedWritePlain<int64_t, int64_t>(input_column, input.size(), mask, temp_writer);
280: 				break;
281: 			case LogicalTypeId::FLOAT:
282: 				TemplatedWritePlain<float, float>(input_column, input.size(), mask, temp_writer);
283: 				break;
284: 			case LogicalTypeId::DECIMAL: {
285: 				// FIXME: fixed length byte array...
286: 				Vector double_vec(LogicalType::DOUBLE);
287: 				VectorOperations::Cast(input_column, double_vec, input.size());
288: 				TemplatedWritePlain<double, double>(double_vec, input.size(), mask, temp_writer);
289: 				break;
290: 			}
291: 			case LogicalTypeId::DOUBLE:
292: 				TemplatedWritePlain<double, double>(input_column, input.size(), mask, temp_writer);
293: 				break;
294: 			case LogicalTypeId::DATE: {
295: 				auto *ptr = FlatVector::GetData<date_t>(input_column);
296: 				for (idx_t r = 0; r < input.size(); r++) {
297: 					if (mask.RowIsValid(r)) {
298: 						auto ts = Timestamp::FromDatetime(ptr[r], dtime_t(0));
299: 						temp_writer.Write<Int96>(TimestampToImpalaTimestamp(ts));
300: 					}
301: 				}
302: 				break;
303: 			}
304: 			case LogicalTypeId::TIMESTAMP: {
305: 				auto *ptr = FlatVector::GetData<timestamp_t>(input_column);
306: 				for (idx_t r = 0; r < input.size(); r++) {
307: 					if (mask.RowIsValid(r)) {
308: 						temp_writer.Write<Int96>(TimestampToImpalaTimestamp(ptr[r]));
309: 					}
310: 				}
311: 				break;
312: 			}
313: 			case LogicalTypeId::BLOB:
314: 			case LogicalTypeId::VARCHAR: {
315: 				auto *ptr = FlatVector::GetData<string_t>(input_column);
316: 				for (idx_t r = 0; r < input.size(); r++) {
317: 					if (mask.RowIsValid(r)) {
318: 						temp_writer.Write<uint32_t>(ptr[r].GetSize());
319: 						temp_writer.WriteData((const_data_ptr_t)ptr[r].GetDataUnsafe(), ptr[r].GetSize());
320: 					}
321: 				}
322: 				break;
323: 			}
324: 			default:
325: 				throw NotImplementedException((sql_types[i].ToString()));
326: 			}
327: 		}
328: 
329: 		// now that we have finished writing the data we know the uncompressed size
330: 		hdr.uncompressed_page_size = temp_writer.blob.size;
331: 
332: 		// compress the data based
333: 		size_t compressed_size;
334: 		data_ptr_t compressed_data;
335: 		unique_ptr<data_t[]> compressed_buf;
336: 		switch (codec) {
337: 		case CompressionCodec::UNCOMPRESSED:
338: 			compressed_size = temp_writer.blob.size;
339: 			compressed_data = temp_writer.blob.data.get();
340: 			break;
341: 		case CompressionCodec::SNAPPY: {
342: 			compressed_size = snappy::MaxCompressedLength(temp_writer.blob.size);
343: 			compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
344: 			snappy::RawCompress((const char *)temp_writer.blob.data.get(), temp_writer.blob.size,
345: 			                    (char *)compressed_buf.get(), &compressed_size);
346: 			compressed_data = compressed_buf.get();
347: 			break;
348: 		}
349: 		case CompressionCodec::GZIP: {
350: 			MiniZStream s;
351: 			compressed_size = s.MaxCompressedLength(temp_writer.blob.size);
352: 			compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
353: 			s.Compress((const char *)temp_writer.blob.data.get(), temp_writer.blob.size, (char *)compressed_buf.get(),
354: 			           &compressed_size);
355: 			compressed_data = compressed_buf.get();
356: 			break;
357: 		}
358: 		case CompressionCodec::ZSTD: {
359: 			compressed_size = duckdb_zstd::ZSTD_compressBound(temp_writer.blob.size);
360: 			compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
361: 			compressed_size = duckdb_zstd::ZSTD_compress((void *)compressed_buf.get(), compressed_size,
362: 			                                             (const void *)temp_writer.blob.data.get(),
363: 			                                             temp_writer.blob.size, ZSTD_CLEVEL_DEFAULT);
364: 			compressed_data = compressed_buf.get();
365: 			break;
366: 		}
367: 		default:
368: 			throw InternalException("Unsupported codec for Parquet Writer");
369: 		}
370: 
371: 		hdr.compressed_page_size = compressed_size;
372: 		// now finally write the data to the actual file
373: 		hdr.write(protocol.get());
374: 		writer->WriteData(compressed_data, compressed_size);
375: 
376: 		auto &column_chunk = row_group.columns[i];
377: 		column_chunk.__isset.meta_data = true;
378: 		column_chunk.meta_data.data_page_offset = start_offset;
379: 		column_chunk.meta_data.total_compressed_size = writer->GetTotalWritten() - start_offset;
380: 		column_chunk.meta_data.codec = codec;
381: 		column_chunk.meta_data.path_in_schema.push_back(file_meta_data.schema[i + 1].name);
382: 		column_chunk.meta_data.num_values = buffer.Count();
383: 		column_chunk.meta_data.type = file_meta_data.schema[i + 1].type;
384: 	}
385: 	row_group.num_rows += buffer.Count();
386: 
387: 	// append the row group to the file meta data
388: 	file_meta_data.row_groups.push_back(row_group);
389: 	file_meta_data.num_rows += buffer.Count();
390: }
391: 
392: void ParquetWriter::Finalize() {
393: 	auto start_offset = writer->GetTotalWritten();
394: 	file_meta_data.write(protocol.get());
395: 
396: 	writer->Write<uint32_t>(writer->GetTotalWritten() - start_offset);
397: 
398: 	// parquet files also end with the string "PAR1"
399: 	writer->WriteData((const_data_ptr_t) "PAR1", 4);
400: 
401: 	// flush to disk
402: 	writer->Sync();
403: 	writer.reset();
404: }
405: 
406: } // namespace duckdb
[end of extension/parquet/parquet_writer.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: