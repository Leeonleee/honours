{
  "repo": "duckdb/duckdb",
  "pull_number": 2597,
  "instance_id": "duckdb__duckdb-2597",
  "issue_numbers": [
    "2454",
    "2454"
  ],
  "base_commit": "89f5889ec118a340ad4600ccc3dedfb66f62051a",
  "patch": "diff --git a/extension/parquet/column_reader.cpp b/extension/parquet/column_reader.cpp\nindex 70deb57b08df..26f457f0d7ea 100644\n--- a/extension/parquet/column_reader.cpp\n+++ b/extension/parquet/column_reader.cpp\n@@ -51,6 +51,25 @@ ColumnReader::ColumnReader(ParquetReader &reader, LogicalType type_p, const Sche\n ColumnReader::~ColumnReader() {\n }\n \n+template <class T>\n+unique_ptr<ColumnReader> CreateDecimalReader(ParquetReader &reader, const LogicalType &type_p,\n+                                             const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,\n+                                             idx_t max_repeat) {\n+\tswitch (type_p.InternalType()) {\n+\tcase PhysicalType::INT16:\n+\t\treturn make_unique<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<T>>>(\n+\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase PhysicalType::INT32:\n+\t\treturn make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<T>>>(\n+\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase PhysicalType::INT64:\n+\t\treturn make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<T>>>(\n+\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tdefault:\n+\t\tthrow NotImplementedException(\"Unimplemented internal type for CreateDecimalReader\");\n+\t}\n+}\n+\n unique_ptr<ColumnReader> ColumnReader::CreateReader(ParquetReader &reader, const LogicalType &type_p,\n                                                     const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,\n                                                     idx_t max_repeat) {\n@@ -69,6 +88,12 @@ unique_ptr<ColumnReader> ColumnReader::CreateReader(ParquetReader &reader, const\n \tcase LogicalTypeId::UBIGINT:\n \t\treturn make_unique<TemplatedColumnReader<uint64_t, TemplatedParquetValueConversion<uint64_t>>>(\n \t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase LogicalTypeId::TINYINT:\n+\t\treturn make_unique<TemplatedColumnReader<int8_t, TemplatedParquetValueConversion<int32_t>>>(\n+\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase LogicalTypeId::SMALLINT:\n+\t\treturn make_unique<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<int32_t>>>(\n+\t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n \tcase LogicalTypeId::INTEGER:\n \t\treturn make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<int32_t>>>(\n \t\t    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n@@ -109,22 +134,30 @@ unique_ptr<ColumnReader> ColumnReader::CreateReader(ParquetReader &reader, const\n \t\treturn make_unique<StringColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n \tcase LogicalTypeId::DECIMAL:\n \t\t// we have to figure out what kind of int we need\n-\t\tswitch (type_p.InternalType()) {\n-\t\tcase PhysicalType::INT16:\n-\t\t\treturn make_unique<DecimalColumnReader<int16_t>>(reader, type_p, schema_p, file_idx_p, max_define,\n-\t\t\t                                                 max_repeat);\n-\t\tcase PhysicalType::INT32:\n-\t\t\treturn make_unique<DecimalColumnReader<int32_t>>(reader, type_p, schema_p, file_idx_p, max_define,\n-\t\t\t                                                 max_repeat);\n-\t\tcase PhysicalType::INT64:\n-\t\t\treturn make_unique<DecimalColumnReader<int64_t>>(reader, type_p, schema_p, file_idx_p, max_define,\n-\t\t\t                                                 max_repeat);\n-\t\tcase PhysicalType::INT128:\n-\t\t\treturn make_unique<DecimalColumnReader<hugeint_t>>(reader, type_p, schema_p, file_idx_p, max_define,\n-\t\t\t                                                   max_repeat);\n-\n+\t\tswitch (schema_p.type) {\n+\t\tcase Type::INT32:\n+\t\t\treturn CreateDecimalReader<int32_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\t\tcase Type::INT64:\n+\t\t\treturn CreateDecimalReader<int64_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\t\tcase Type::FIXED_LEN_BYTE_ARRAY:\n+\t\t\tswitch (type_p.InternalType()) {\n+\t\t\tcase PhysicalType::INT16:\n+\t\t\t\treturn make_unique<DecimalColumnReader<int16_t>>(reader, type_p, schema_p, file_idx_p, max_define,\n+\t\t\t\t                                                 max_repeat);\n+\t\t\tcase PhysicalType::INT32:\n+\t\t\t\treturn make_unique<DecimalColumnReader<int32_t>>(reader, type_p, schema_p, file_idx_p, max_define,\n+\t\t\t\t                                                 max_repeat);\n+\t\t\tcase PhysicalType::INT64:\n+\t\t\t\treturn make_unique<DecimalColumnReader<int64_t>>(reader, type_p, schema_p, file_idx_p, max_define,\n+\t\t\t\t                                                 max_repeat);\n+\t\t\tcase PhysicalType::INT128:\n+\t\t\t\treturn make_unique<DecimalColumnReader<hugeint_t>>(reader, type_p, schema_p, file_idx_p, max_define,\n+\t\t\t\t                                                   max_repeat);\n+\t\t\tdefault:\n+\t\t\t\tthrow InternalException(\"Unrecognized type for Decimal\");\n+\t\t\t}\n \t\tdefault:\n-\t\t\tbreak;\n+\t\t\tthrow NotImplementedException(\"Unrecognized type for Decimal\");\n \t\t}\n \t\tbreak;\n \tdefault:\ndiff --git a/extension/parquet/parquet_reader.cpp b/extension/parquet/parquet_reader.cpp\nindex ab3847f8f847..52c9dda92c2c 100644\n--- a/extension/parquet/parquet_reader.cpp\n+++ b/extension/parquet/parquet_reader.cpp\n@@ -89,70 +89,131 @@ static shared_ptr<ParquetFileMetadataCache> LoadMetadata(Allocator &allocator, F\n LogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele) {\n \t// inner node\n \tD_ASSERT(s_ele.__isset.type && s_ele.num_children == 0);\n-\tswitch (s_ele.type) {\n-\tcase Type::BOOLEAN:\n-\t\treturn LogicalType::BOOLEAN;\n-\tcase Type::INT32:\n-\t\tif (s_ele.__isset.converted_type) {\n-\t\t\tswitch (s_ele.converted_type) {\n-\t\t\tcase ConvertedType::DATE:\n-\t\t\t\treturn LogicalType::DATE;\n-\t\t\tcase ConvertedType::UINT_8:\n+\tif (s_ele.type == Type::FIXED_LEN_BYTE_ARRAY && !s_ele.__isset.type_length) {\n+\t\tthrow IOException(\"FIXED_LEN_BYTE_ARRAY requires length to be set\");\n+\t}\n+\tif (s_ele.__isset.converted_type) {\n+\t\tswitch (s_ele.converted_type) {\n+\t\tcase ConvertedType::INT_8:\n+\t\t\tif (s_ele.type == Type::INT32) {\n+\t\t\t\treturn LogicalType::TINYINT;\n+\t\t\t} else {\n+\t\t\t\tthrow IOException(\"INT8 converted type can only be set for value of Type::INT32\");\n+\t\t\t}\n+\t\tcase ConvertedType::INT_16:\n+\t\t\tif (s_ele.type == Type::INT32) {\n+\t\t\t\treturn LogicalType::SMALLINT;\n+\t\t\t} else {\n+\t\t\t\tthrow IOException(\"INT16 converted type can only be set for value of Type::INT32\");\n+\t\t\t}\n+\t\tcase ConvertedType::INT_32:\n+\t\t\tif (s_ele.type == Type::INT32) {\n+\t\t\t\treturn LogicalType::INTEGER;\n+\t\t\t} else {\n+\t\t\t\tthrow IOException(\"INT32 converted type can only be set for value of Type::INT32\");\n+\t\t\t}\n+\t\tcase ConvertedType::INT_64:\n+\t\t\tif (s_ele.type == Type::INT64) {\n+\t\t\t\treturn LogicalType::BIGINT;\n+\t\t\t} else {\n+\t\t\t\tthrow IOException(\"INT64 converted type can only be set for value of Type::INT32\");\n+\t\t\t}\n+\t\tcase ConvertedType::UINT_8:\n+\t\t\tif (s_ele.type == Type::INT32) {\n \t\t\t\treturn LogicalType::UTINYINT;\n-\t\t\tcase ConvertedType::UINT_16:\n+\t\t\t} else {\n+\t\t\t\tthrow IOException(\"UINT8 converted type can only be set for value of Type::INT32\");\n+\t\t\t}\n+\t\tcase ConvertedType::UINT_16:\n+\t\t\tif (s_ele.type == Type::INT32) {\n \t\t\t\treturn LogicalType::USMALLINT;\n-\t\t\tdefault:\n-\t\t\t\treturn LogicalType::INTEGER;\n+\t\t\t} else {\n+\t\t\t\tthrow IOException(\"UINT16 converted type can only be set for value of Type::INT32\");\n \t\t\t}\n-\t\t}\n-\t\treturn LogicalType::INTEGER;\n-\tcase Type::INT64:\n-\t\tif (s_ele.__isset.converted_type) {\n-\t\t\tswitch (s_ele.converted_type) {\n-\t\t\tcase ConvertedType::TIMESTAMP_MICROS:\n-\t\t\tcase ConvertedType::TIMESTAMP_MILLIS:\n-\t\t\t\treturn LogicalType::TIMESTAMP;\n-\t\t\tcase ConvertedType::UINT_32:\n+\t\tcase ConvertedType::UINT_32:\n+\t\t\tif (s_ele.type == Type::INT32) {\n \t\t\t\treturn LogicalType::UINTEGER;\n-\t\t\tcase ConvertedType::UINT_64:\n+\t\t\t} else {\n+\t\t\t\tthrow IOException(\"UINT32 converted type can only be set for value of Type::INT32\");\n+\t\t\t}\n+\t\tcase ConvertedType::UINT_64:\n+\t\t\tif (s_ele.type == Type::INT64) {\n \t\t\t\treturn LogicalType::UBIGINT;\n+\t\t\t} else {\n+\t\t\t\tthrow IOException(\"UINT64 converted type can only be set for value of Type::INT64\");\n+\t\t\t}\n+\t\tcase ConvertedType::DATE:\n+\t\t\tif (s_ele.type == Type::INT32) {\n+\t\t\t\treturn LogicalType::DATE;\n+\t\t\t} else {\n+\t\t\t\tthrow IOException(\"DATE converted type can only be set for value of Type::INT32\");\n+\t\t\t}\n+\t\tcase ConvertedType::TIMESTAMP_MICROS:\n+\t\tcase ConvertedType::TIMESTAMP_MILLIS:\n+\t\t\tif (s_ele.type == Type::INT64) {\n+\t\t\t\treturn LogicalType::TIMESTAMP;\n+\t\t\t} else {\n+\t\t\t\tthrow IOException(\"TIMESTAMP converted type can only be set for value of Type::INT64\");\n+\t\t\t}\n+\t\tcase ConvertedType::DECIMAL:\n+\t\t\tif (!s_ele.__isset.precision || !s_ele.__isset.scale) {\n+\t\t\t\tthrow IOException(\"DECIMAL requires a length and scale specifier!\");\n+\t\t\t}\n+\t\t\tswitch (s_ele.type) {\n+\t\t\tcase Type::BYTE_ARRAY:\n+\t\t\tcase Type::FIXED_LEN_BYTE_ARRAY:\n+\t\t\tcase Type::INT32:\n+\t\t\tcase Type::INT64:\n+\t\t\t\treturn LogicalType::DECIMAL(s_ele.precision, s_ele.scale);\n \t\t\tdefault:\n-\t\t\t\treturn LogicalType::BIGINT;\n+\t\t\t\tthrow IOException(\n+\t\t\t\t    \"DECIMAL converted type can only be set for value of Type::(FIXED_LEN_)BYTE_ARRAY/INT32/INT64\");\n \t\t\t}\n-\t\t}\n-\t\treturn LogicalType::BIGINT;\n-\n-\tcase Type::INT96: // always a timestamp it would seem\n-\t\treturn LogicalType::TIMESTAMP;\n-\tcase Type::FLOAT:\n-\t\treturn LogicalType::FLOAT;\n-\tcase Type::DOUBLE:\n-\t\treturn LogicalType::DOUBLE;\n-\tcase Type::BYTE_ARRAY:\n-\tcase Type::FIXED_LEN_BYTE_ARRAY:\n-\t\tif (s_ele.type == Type::FIXED_LEN_BYTE_ARRAY && !s_ele.__isset.type_length) {\n-\t\t\treturn LogicalType::INVALID;\n-\t\t}\n-\t\tif (s_ele.__isset.converted_type) {\n-\t\t\tswitch (s_ele.converted_type) {\n-\t\t\tcase ConvertedType::DECIMAL:\n-\t\t\t\tif (s_ele.type == Type::FIXED_LEN_BYTE_ARRAY && s_ele.__isset.scale && s_ele.__isset.type_length) {\n-\t\t\t\t\treturn LogicalType::DECIMAL(s_ele.precision, s_ele.scale);\n-\t\t\t\t}\n-\t\t\t\treturn LogicalType::INVALID;\n-\n-\t\t\tcase ConvertedType::UTF8:\n+\t\tcase ConvertedType::UTF8:\n+\t\t\tswitch (s_ele.type) {\n+\t\t\tcase Type::BYTE_ARRAY:\n+\t\t\tcase Type::FIXED_LEN_BYTE_ARRAY:\n \t\t\t\treturn LogicalType::VARCHAR;\n \t\t\tdefault:\n-\t\t\t\treturn LogicalType::BLOB;\n+\t\t\t\tthrow IOException(\"UTF8 converted type can only be set for Type::(FIXED_LEN_)BYTE_ARRAY\");\n \t\t\t}\n+\t\tcase ConvertedType::MAP:\n+\t\tcase ConvertedType::MAP_KEY_VALUE:\n+\t\tcase ConvertedType::LIST:\n+\t\tcase ConvertedType::ENUM:\n+\t\tcase ConvertedType::TIME_MILLIS:\n+\t\tcase ConvertedType::TIME_MICROS:\n+\t\tcase ConvertedType::JSON:\n+\t\tcase ConvertedType::BSON:\n+\t\tcase ConvertedType::INTERVAL:\n+\t\tdefault:\n+\t\t\tthrow IOException(\"Unsupported converted type\");\n \t\t}\n-\t\tif (parquet_options.binary_as_string) {\n-\t\t\treturn LogicalType::VARCHAR;\n+\t} else {\n+\t\t// no converted type set\n+\t\t// use default type for each physical type\n+\t\tswitch (s_ele.type) {\n+\t\tcase Type::BOOLEAN:\n+\t\t\treturn LogicalType::BOOLEAN;\n+\t\tcase Type::INT32:\n+\t\t\treturn LogicalType::INTEGER;\n+\t\tcase Type::INT64:\n+\t\t\treturn LogicalType::BIGINT;\n+\t\tcase Type::INT96: // always a timestamp it would seem\n+\t\t\treturn LogicalType::TIMESTAMP;\n+\t\tcase Type::FLOAT:\n+\t\t\treturn LogicalType::FLOAT;\n+\t\tcase Type::DOUBLE:\n+\t\t\treturn LogicalType::DOUBLE;\n+\t\tcase Type::BYTE_ARRAY:\n+\t\tcase Type::FIXED_LEN_BYTE_ARRAY:\n+\t\t\tif (parquet_options.binary_as_string) {\n+\t\t\t\treturn LogicalType::VARCHAR;\n+\t\t\t}\n+\t\t\treturn LogicalType::BLOB;\n+\t\tdefault:\n+\t\t\treturn LogicalType::INVALID;\n \t\t}\n-\t\treturn LogicalType::BLOB;\n-\tdefault:\n-\t\treturn LogicalType::INVALID;\n \t}\n }\n \n@@ -461,6 +522,12 @@ static void FilterOperationSwitch(Vector &v, Value &constant, parquet_filter_t &\n \tcase LogicalTypeId::UBIGINT:\n \t\tTemplatedFilterOperation<uint64_t, OP>(v, constant.value_.ubigint, filter_mask, count);\n \t\tbreak;\n+\tcase LogicalTypeId::TINYINT:\n+\t\tTemplatedFilterOperation<int8_t, OP>(v, constant.value_.tinyint, filter_mask, count);\n+\t\tbreak;\n+\tcase LogicalTypeId::SMALLINT:\n+\t\tTemplatedFilterOperation<int16_t, OP>(v, constant.value_.smallint, filter_mask, count);\n+\t\tbreak;\n \tcase LogicalTypeId::INTEGER:\n \t\tTemplatedFilterOperation<int32_t, OP>(v, constant.value_.integer, filter_mask, count);\n \t\tbreak;\ndiff --git a/extension/parquet/parquet_writer.cpp b/extension/parquet/parquet_writer.cpp\nindex 75aa18b3bb58..7f6ce5a8ea56 100644\n--- a/extension/parquet/parquet_writer.cpp\n+++ b/extension/parquet/parquet_writer.cpp\n@@ -11,6 +11,7 @@\n #include \"duckdb/common/file_system.hpp\"\n #include \"duckdb/common/string_util.hpp\"\n #include \"duckdb/common/types/date.hpp\"\n+#include \"duckdb/common/types/hugeint.hpp\"\n #include \"duckdb/common/types/time.hpp\"\n #include \"duckdb/common/types/timestamp.hpp\"\n #include \"duckdb/common/serializer/buffered_file_writer.hpp\"\n@@ -69,6 +70,7 @@ static Type::type DuckDBTypeToParquetType(const LogicalType &duckdb_type) {\n \tcase LogicalTypeId::TINYINT:\n \tcase LogicalTypeId::SMALLINT:\n \tcase LogicalTypeId::INTEGER:\n+\tcase LogicalTypeId::DATE:\n \t\treturn Type::INT32;\n \tcase LogicalTypeId::BIGINT:\n \t\treturn Type::INT64;\n@@ -76,13 +78,22 @@ static Type::type DuckDBTypeToParquetType(const LogicalType &duckdb_type) {\n \t\treturn Type::FLOAT;\n \tcase LogicalTypeId::DECIMAL: // for now...\n \tcase LogicalTypeId::DOUBLE:\n+\tcase LogicalTypeId::HUGEINT:\n \t\treturn Type::DOUBLE;\n \tcase LogicalTypeId::VARCHAR:\n \tcase LogicalTypeId::BLOB:\n \t\treturn Type::BYTE_ARRAY;\n-\tcase LogicalTypeId::DATE:\n \tcase LogicalTypeId::TIMESTAMP:\n-\t\treturn Type::INT96;\n+\tcase LogicalTypeId::TIMESTAMP_MS:\n+\tcase LogicalTypeId::TIMESTAMP_NS:\n+\tcase LogicalTypeId::TIMESTAMP_SEC:\n+\t\treturn Type::INT64;\n+\tcase LogicalTypeId::UTINYINT:\n+\tcase LogicalTypeId::USMALLINT:\n+\tcase LogicalTypeId::UINTEGER:\n+\t\treturn Type::INT32;\n+\tcase LogicalTypeId::UBIGINT:\n+\t\treturn Type::INT64;\n \tdefault:\n \t\tthrow NotImplementedException(duckdb_type.ToString());\n \t}\n@@ -90,9 +101,44 @@ static Type::type DuckDBTypeToParquetType(const LogicalType &duckdb_type) {\n \n static bool DuckDBTypeToConvertedType(const LogicalType &duckdb_type, ConvertedType::type &result) {\n \tswitch (duckdb_type.id()) {\n+\tcase LogicalTypeId::TINYINT:\n+\t\tresult = ConvertedType::INT_8;\n+\t\treturn true;\n+\tcase LogicalTypeId::SMALLINT:\n+\t\tresult = ConvertedType::INT_16;\n+\t\treturn true;\n+\tcase LogicalTypeId::INTEGER:\n+\t\tresult = ConvertedType::INT_32;\n+\t\treturn true;\n+\tcase LogicalTypeId::BIGINT:\n+\t\tresult = ConvertedType::INT_64;\n+\t\treturn true;\n+\tcase LogicalTypeId::UTINYINT:\n+\t\tresult = ConvertedType::UINT_8;\n+\t\treturn true;\n+\tcase LogicalTypeId::USMALLINT:\n+\t\tresult = ConvertedType::UINT_16;\n+\t\treturn true;\n+\tcase LogicalTypeId::UINTEGER:\n+\t\tresult = ConvertedType::UINT_32;\n+\t\treturn true;\n+\tcase LogicalTypeId::UBIGINT:\n+\t\tresult = ConvertedType::UINT_64;\n+\t\treturn true;\n+\tcase LogicalTypeId::DATE:\n+\t\tresult = ConvertedType::DATE;\n+\t\treturn true;\n \tcase LogicalTypeId::VARCHAR:\n \t\tresult = ConvertedType::UTF8;\n \t\treturn true;\n+\tcase LogicalTypeId::TIMESTAMP:\n+\tcase LogicalTypeId::TIMESTAMP_NS:\n+\tcase LogicalTypeId::TIMESTAMP_SEC:\n+\t\tresult = ConvertedType::TIMESTAMP_MICROS;\n+\t\treturn true;\n+\tcase LogicalTypeId::TIMESTAMP_MS:\n+\t\tresult = ConvertedType::TIMESTAMP_MILLIS;\n+\t\treturn true;\n \tdefault:\n \t\treturn false;\n \t}\n@@ -122,12 +168,40 @@ static uint8_t GetVarintSize(uint32_t val) {\n \treturn res;\n }\n \n-template <class SRC, class TGT>\n+struct ParquetCastOperator {\n+\ttemplate <class SRC, class TGT>\n+\tstatic TGT Operation(SRC input) {\n+\t\treturn TGT(input);\n+\t}\n+};\n+\n+struct ParquetTimestampNSOperator {\n+\ttemplate <class SRC, class TGT>\n+\tstatic TGT Operation(SRC input) {\n+\t\treturn Timestamp::FromEpochNanoSeconds(input).value;\n+\t}\n+};\n+\n+struct ParquetTimestampSOperator {\n+\ttemplate <class SRC, class TGT>\n+\tstatic TGT Operation(SRC input) {\n+\t\treturn Timestamp::FromEpochSeconds(input).value;\n+\t}\n+};\n+\n+struct ParquetHugeintOperator {\n+\ttemplate <class SRC, class TGT>\n+\tstatic TGT Operation(SRC input) {\n+\t\treturn Hugeint::Cast<double>(input);\n+\t}\n+};\n+\n+template <class SRC, class TGT, class OP = ParquetCastOperator>\n static void TemplatedWritePlain(Vector &col, idx_t length, ValidityMask &mask, Serializer &ser) {\n \tauto *ptr = FlatVector::GetData<SRC>(col);\n \tfor (idx_t r = 0; r < length; r++) {\n \t\tif (mask.RowIsValid(r)) {\n-\t\t\tser.Write<TGT>((TGT)ptr[r]);\n+\t\t\tser.Write<TGT>(OP::template Operation<SRC, TGT>(ptr[r]));\n \t\t}\n \t}\n }\n@@ -273,11 +347,38 @@ void ParquetWriter::Flush(ChunkCollection &buffer) {\n \t\t\t\tTemplatedWritePlain<int16_t, int32_t>(input_column, input.size(), mask, temp_writer);\n \t\t\t\tbreak;\n \t\t\tcase LogicalTypeId::INTEGER:\n+\t\t\tcase LogicalTypeId::DATE:\n \t\t\t\tTemplatedWritePlain<int32_t, int32_t>(input_column, input.size(), mask, temp_writer);\n \t\t\t\tbreak;\n \t\t\tcase LogicalTypeId::BIGINT:\n+\t\t\tcase LogicalTypeId::TIMESTAMP:\n+\t\t\tcase LogicalTypeId::TIMESTAMP_MS:\n \t\t\t\tTemplatedWritePlain<int64_t, int64_t>(input_column, input.size(), mask, temp_writer);\n \t\t\t\tbreak;\n+\t\t\tcase LogicalTypeId::HUGEINT:\n+\t\t\t\tTemplatedWritePlain<hugeint_t, double, ParquetHugeintOperator>(input_column, input.size(), mask,\n+\t\t\t\t                                                               temp_writer);\n+\t\t\t\tbreak;\n+\t\t\tcase LogicalTypeId::TIMESTAMP_NS:\n+\t\t\t\tTemplatedWritePlain<int64_t, int64_t, ParquetTimestampNSOperator>(input_column, input.size(), mask,\n+\t\t\t\t                                                                  temp_writer);\n+\t\t\t\tbreak;\n+\t\t\tcase LogicalTypeId::TIMESTAMP_SEC:\n+\t\t\t\tTemplatedWritePlain<int64_t, int64_t, ParquetTimestampSOperator>(input_column, input.size(), mask,\n+\t\t\t\t                                                                 temp_writer);\n+\t\t\t\tbreak;\n+\t\t\tcase LogicalTypeId::UTINYINT:\n+\t\t\t\tTemplatedWritePlain<uint8_t, int32_t>(input_column, input.size(), mask, temp_writer);\n+\t\t\t\tbreak;\n+\t\t\tcase LogicalTypeId::USMALLINT:\n+\t\t\t\tTemplatedWritePlain<uint16_t, int32_t>(input_column, input.size(), mask, temp_writer);\n+\t\t\t\tbreak;\n+\t\t\tcase LogicalTypeId::UINTEGER:\n+\t\t\t\tTemplatedWritePlain<uint32_t, uint32_t>(input_column, input.size(), mask, temp_writer);\n+\t\t\t\tbreak;\n+\t\t\tcase LogicalTypeId::UBIGINT:\n+\t\t\t\tTemplatedWritePlain<uint64_t, uint64_t>(input_column, input.size(), mask, temp_writer);\n+\t\t\t\tbreak;\n \t\t\tcase LogicalTypeId::FLOAT:\n \t\t\t\tTemplatedWritePlain<float, float>(input_column, input.size(), mask, temp_writer);\n \t\t\t\tbreak;\n@@ -291,25 +392,6 @@ void ParquetWriter::Flush(ChunkCollection &buffer) {\n \t\t\tcase LogicalTypeId::DOUBLE:\n \t\t\t\tTemplatedWritePlain<double, double>(input_column, input.size(), mask, temp_writer);\n \t\t\t\tbreak;\n-\t\t\tcase LogicalTypeId::DATE: {\n-\t\t\t\tauto *ptr = FlatVector::GetData<date_t>(input_column);\n-\t\t\t\tfor (idx_t r = 0; r < input.size(); r++) {\n-\t\t\t\t\tif (mask.RowIsValid(r)) {\n-\t\t\t\t\t\tauto ts = Timestamp::FromDatetime(ptr[r], dtime_t(0));\n-\t\t\t\t\t\ttemp_writer.Write<Int96>(TimestampToImpalaTimestamp(ts));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\t}\n-\t\t\tcase LogicalTypeId::TIMESTAMP: {\n-\t\t\t\tauto *ptr = FlatVector::GetData<timestamp_t>(input_column);\n-\t\t\t\tfor (idx_t r = 0; r < input.size(); r++) {\n-\t\t\t\t\tif (mask.RowIsValid(r)) {\n-\t\t\t\t\t\ttemp_writer.Write<Int96>(TimestampToImpalaTimestamp(ptr[r]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\t}\n \t\t\tcase LogicalTypeId::BLOB:\n \t\t\tcase LogicalTypeId::VARCHAR: {\n \t\t\t\tauto *ptr = FlatVector::GetData<string_t>(input_column);\n",
  "test_patch": "diff --git a/data/parquet-testing/broken/broken_bigint.parquet b/data/parquet-testing/broken/broken_bigint.parquet\nnew file mode 100644\nindex 000000000000..fc2e3859b6e8\nBinary files /dev/null and b/data/parquet-testing/broken/broken_bigint.parquet differ\ndiff --git a/data/parquet-testing/broken/broken_date.parquet b/data/parquet-testing/broken/broken_date.parquet\nnew file mode 100644\nindex 000000000000..2215cb1dc2ba\nBinary files /dev/null and b/data/parquet-testing/broken/broken_date.parquet differ\ndiff --git a/data/parquet-testing/broken/broken_int.parquet b/data/parquet-testing/broken/broken_int.parquet\nnew file mode 100644\nindex 000000000000..8cf309fca197\nBinary files /dev/null and b/data/parquet-testing/broken/broken_int.parquet differ\ndiff --git a/data/parquet-testing/broken/broken_smallint.parquet b/data/parquet-testing/broken/broken_smallint.parquet\nnew file mode 100644\nindex 000000000000..a44c096700a6\nBinary files /dev/null and b/data/parquet-testing/broken/broken_smallint.parquet differ\ndiff --git a/data/parquet-testing/broken/broken_timestamp.parquet b/data/parquet-testing/broken/broken_timestamp.parquet\nnew file mode 100644\nindex 000000000000..859fd9e7cee3\nBinary files /dev/null and b/data/parquet-testing/broken/broken_timestamp.parquet differ\ndiff --git a/data/parquet-testing/broken/broken_timestamp_ms.parquet b/data/parquet-testing/broken/broken_timestamp_ms.parquet\nnew file mode 100644\nindex 000000000000..433f7761a113\nBinary files /dev/null and b/data/parquet-testing/broken/broken_timestamp_ms.parquet differ\ndiff --git a/data/parquet-testing/broken/broken_tinyint.parquet b/data/parquet-testing/broken/broken_tinyint.parquet\nnew file mode 100644\nindex 000000000000..a20079c13874\nBinary files /dev/null and b/data/parquet-testing/broken/broken_tinyint.parquet differ\ndiff --git a/data/parquet-testing/broken/broken_ubigint.parquet b/data/parquet-testing/broken/broken_ubigint.parquet\nnew file mode 100644\nindex 000000000000..77a623aa0df9\nBinary files /dev/null and b/data/parquet-testing/broken/broken_ubigint.parquet differ\ndiff --git a/data/parquet-testing/broken/broken_uinteger.parquet b/data/parquet-testing/broken/broken_uinteger.parquet\nnew file mode 100644\nindex 000000000000..f019cbc4f280\nBinary files /dev/null and b/data/parquet-testing/broken/broken_uinteger.parquet differ\ndiff --git a/data/parquet-testing/broken/broken_usmallint.parquet b/data/parquet-testing/broken/broken_usmallint.parquet\nnew file mode 100644\nindex 000000000000..1bb946331a84\nBinary files /dev/null and b/data/parquet-testing/broken/broken_usmallint.parquet differ\ndiff --git a/data/parquet-testing/broken/broken_utinyint.parquet b/data/parquet-testing/broken/broken_utinyint.parquet\nnew file mode 100644\nindex 000000000000..81e36a64aba0\nBinary files /dev/null and b/data/parquet-testing/broken/broken_utinyint.parquet differ\ndiff --git a/test/parquet/test_parquet_reader.test b/test/parquet/test_parquet_reader.test\nindex 7864cce28b3d..1c154b760595 100644\n--- a/test/parquet/test_parquet_reader.test\n+++ b/test/parquet/test_parquet_reader.test\n@@ -8,7 +8,7 @@ statement ok\n PRAGMA enable_verification\n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/manyrowgroups.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/manyrowgroups.parquet') limit 50;\n ----\n 42\t\n 42\t\n@@ -64,7 +64,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/manyrowgroups.parquet') limit 5\n mode skip \n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/map.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/map.parquet') limit 50;\n ----\n {key: [Content-Encoding, X-Frame-Options, Connection, Via, X-Xss-Protection, Content-Type, Date, X-Cache, Vary, Server, X-Cache-Lookup, X-Content-Type-Options, Content-Length], value: [gzip, SAMEORIGIN, keep-alive, 1.1 ip-10-1-1-216.ec2.internal (squid/4.10-20200322-r358ad2fdf), 1; mode=block, text/html;charset=utf-8, Sat, 30 Jan 2021 16:19:57 GMT, MISS from ip-10-1-1-216.ec2.internal, Accept-Encoding, nginx/1.10.3, HIT from ip-10-1-1-216.ec2.internal:3128, nosniff, 921]}\t\n {key: [Content-Encoding, X-Frame-Options, Connection, Via, X-Xss-Protection, Content-Type, Date, X-Cache, Vary, Server, X-Cache-Lookup, X-Content-Type-Options, Content-Length], value: [gzip, SAMEORIGIN, keep-alive, 1.1 ip-10-1-1-216.ec2.internal (squid/4.10-20200322-r358ad2fdf), 1; mode=block, text/html;charset=utf-8, Sat, 30 Jan 2021 16:19:59 GMT, MISS from ip-10-1-1-216.ec2.internal, Accept-Encoding, nginx/1.10.3, HIT from ip-10-1-1-216.ec2.internal:3128, nosniff, 922]}\t\n@@ -119,10 +119,8 @@ SELECT * FROM parquet_scan('data/parquet-testing/map.parquet') limit 50\n \n mode unskip \n \n-mode skip \n-\n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/int32_decimal.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/int32_decimal.parquet') limit 50;\n ----\n 1.00\t\n 2.00\t\n@@ -149,19 +147,17 @@ SELECT * FROM parquet_scan('data/parquet-testing/int32_decimal.parquet') limit 5\n 23.00\t\n 24.00\t\n \n-mode unskip \n-\n mode skip \n \n query IIIIII\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/nonnullable.impala.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/nonnullable.impala.parquet') limit 50;\n ----\n 8\t[-1]\t[[-1, -2], []]\t{key: [k1], value: [-1]}\t[{key: [], value: []}, {key: [k1], value: [1]}, {key: [], value: []}, {key: [], value: []}]\t{a: -1, B: [-1], c: {D: [[{e: -1, f: nonnullable}]]}, G: {key: [], value: []}}\t\n \n mode unskip \n \n query IIIIIIIIII\n-SELECT * FROM parquet_scan('data/parquet-testing/bug687_nulls.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/bug687_nulls.parquet') limit 50;\n ----\n NULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\t\n 95\t42\t39\t49\t16\t34\t82\t7\t40\t82\t\n@@ -215,7 +211,7 @@ NULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\n 4\t94\t21\t52\t43\t32\t43\t51\t56\t60\t\n \n query IIII\n-SELECT * FROM parquet_scan('data/parquet-testing/bug1554.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/bug1554.parquet') limit 50;\n ----\n 1584883:SSuWRbZnFmIqCUBQYxk9+48fdIwywjfQUyfcKP+pbJhaqWS+UZh0Sua8VNJKlQpIlRzyWr57xyrqTh2ZgIQnxQ==\tFalse\tNULL\t200\t\n 1584883:VduFa/R/CL7CbbEUmdFKysh80R38hXdrfuDlFhsa5mU3G3vfUDiQdTR0H0LzJzWojUDGgUr+hKp55VRRXMxaaQ==\tFalse\tNULL\t200\t\n@@ -271,7 +267,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/bug1554.parquet') limit 50\n mode skip \n \n query IIIII\n-SELECT * FROM parquet_scan('data/parquet-testing/apkwan.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/apkwan.parquet') limit 50;\n ----\n 53e997b9b7602d9701f9f044\t[https://link.springer.com/10.1007/s00108-004-1229-0]\t[{name: M. Stoll, id: 56018d9645cedb3395e77641, org: Abteilung Klinische Immunologie Medizinische Hochschule Hannover}, {name: H. Heiken, id: 53f4d53adabfaef34ff814c8, org: Abteilung Klinische Immunologie Medizinische Hochschule Hannover}, {name: G. M. N. Behrens, id: 53f42afbdabfaec09f0ed4e0, org: Abteilung Klinische Immunologie Medizinische Hochschule Hannover}, {name: R. E. Schmidt, id: 56018d9645cedb3395e77644, org: Abteilung Klinische Immunologie Medizinische Hochschule Hannover}]\ten\tImmunrekonstitutionssyndrome (IRIS)\t\n 53e997b2b7602d9701f8fea5\t[]\t[{name: D. Barr, id: 5440d4cfdabfae805a6fd46c, org: Camborne School of Mines Redruth, Cornwall England}]\ten\tMineral bioprocessing\t\n@@ -327,7 +323,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/apkwan.parquet') limit 50\n mode unskip \n \n query II\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/nested_lists.snappy.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/nested_lists.snappy.parquet') limit 50;\n ----\n [[[a, b], [c]], [NULL, [d]]]\t1\n [[[a, b], [c, d]], [NULL, [e]]]\t1\n@@ -336,7 +332,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/nested_lists.snappy.parqu\n mode skip \n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/nulls.snappy.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/nulls.snappy.parquet') limit 50;\n ----\n {b_c_int: NULL}\t\n {b_c_int: NULL}\t\n@@ -350,14 +346,14 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/nulls.snappy.parquet') li\n mode unskip \n \n query III\n-SELECT * FROM parquet_scan('data/parquet-testing/nan-float.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/nan-float.parquet') limit 50;\n ----\n -1.0\tfoo\tTrue\t\n NULL\tbar\tFalse\t\n 2.5\tbaz\tTrue\t\n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/manyrowgroups2.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/manyrowgroups2.parquet') limit 50;\n ----\n 42\t\n 42\t\n@@ -411,7 +407,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/manyrowgroups2.parquet') limit\n 90\t\n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/struct.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/struct.parquet') limit 50;\n ----\n {'str_field': hello, 'f64_field': NULL}\n {'str_field': NULL, 'f64_field': 1.230000}\n@@ -419,7 +415,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/struct.parquet') limit 50\n mode skip \n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/byte_array_decimal.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/byte_array_decimal.parquet') limit 50;\n ----\n 1.00\t\n 2.00\t\n@@ -449,25 +445,25 @@ SELECT * FROM parquet_scan('data/parquet-testing/byte_array_decimal.parquet') li\n mode unskip \n \n query II\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/list_columns.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/list_columns.parquet') limit 50;\n ----\n [1, 2, 3]\t[abc, efg, hij]\t\n [NULL, 1]\tNULL\t\n [4]\t[efg, NULL, hij, xyz]\t\n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/timestamp-ms.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/timestamp-ms.parquet') limit 50;\n ----\n 2020-10-05 17:21:49\t\n \n query IIIIIIIIIII\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/alltypes_dictionary.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/alltypes_dictionary.parquet') limit 50;\n ----\n 0\tTrue\t0\t0\t0\t0\t0.0\t0.0\t01/01/09\t0\t2009-01-01 00:00:00\t\n 1\tFalse\t1\t1\t1\t10\t1.100000023841858\t10.1\t01/01/09\t1\t2009-01-01 00:01:00\t\n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/binary.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/binary.parquet') limit 50;\n ----\n \\x00\n \\x01\n@@ -483,7 +479,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/binary.parquet') limit 50\n \\x0B\n \n query IIII\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/nation.dict-malformed.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/nation.dict-malformed.parquet') limit 50;\n ----\n 0\tALGERIA\t0\t haggle. carefully final deposits detect slyly agai\t\n 1\tARGENTINA\t1\tal foxes promise slyly according to the regular accounts. bold requests alon\t\n@@ -512,7 +508,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/nation.dict-malformed.par\n 24\tUNITED STATES\t1\ty final packages. slow foxes cajole quickly. quickly silent platelets breach ironic accounts. unusual pinto be\t\n \n query IIIIIIIIIIIIIIII\n-SELECT * FROM parquet_scan('data/parquet-testing/lineitem-top10000.gzip.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/lineitem-top10000.gzip.parquet') limit 50;\n ----\n 1\t155190\t7706\t1\t17\t21168.23\t0.04\t0.02\tN\tO\t1996-03-13\t1996-02-12\t1996-03-22\tDELIVER IN PERSON\tTRUCK\tegular courts above the\t\n 1\t67310\t7311\t2\t36\t45983.16\t0.09\t0.06\tN\tO\t1996-04-12\t1996-02-28\t1996-04-20\tTAKE BACK RETURN\tMAIL\tly final dependencies: slyly bold \t\n@@ -568,7 +564,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/lineitem-top10000.gzip.parquet'\n mode skip \n \n query III\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/nested_maps.snappy.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/nested_maps.snappy.parquet') limit 50;\n ----\n {key: [a], value: [{key: [1, 2], value: [True, False]}]}\t1\t1.0\t\n {key: [], value: [{key: [1], value: [True]}]}\t1\t1.0\t\n@@ -580,7 +576,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/nested_maps.snappy.parque\n mode unskip \n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/dict-page-offset-zero.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/dict-page-offset-zero.parquet') limit 50;\n ----\n 1552\t\n 1552\t\n@@ -623,34 +619,32 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/dict-page-offset-zero.par\n 1552\t\n \n query III\n-SELECT * FROM parquet_scan('data/parquet-testing/silly-names.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/silly-names.parquet') limit 50;\n ----\n 1\tfoo\tTrue\t\n 2\tbar\tFalse\t\n 3\tbaz\tTrue\t\n \n query III\n-SELECT * FROM parquet_scan('data/parquet-testing/zstd.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/zstd.parquet') limit 50;\n ----\n -1.0\tfoo\tTrue\t\n NULL\tbar\tFalse\t\n 2.5\tbaz\tTrue\t\n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/bug1618_struct_strings.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/bug1618_struct_strings.parquet') limit 50;\n ----\n {'str_field': hello, 'f64_field': NULL}\n {'str_field': NULL, 'f64_field': 1.230000}\n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/single_nan.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/single_nan.parquet') limit 50;\n ----\n NULL\t\n \n-mode skip \n-\n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/int64_decimal.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/int64_decimal.parquet') limit 50;\n ----\n 1.00\t\n 2.00\t\n@@ -677,10 +671,8 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/int64_decimal.parquet') l\n 23.00\t\n 24.00\t\n \n-mode unskip \n-\n query IIIIIIIIIIIIIIIIIII\n-SELECT * FROM parquet_scan('data/parquet-testing/filter_bug1391.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/filter_bug1391.parquet') limit 50;\n ----\n 98\t1900-01-01 00:00:00.000\t9999-12-31 00:00:00.000\tXYZ\t1900-01-01 00:00:00.000\t9999-12-31 00:00:00.000\t275\tDDU Emergency & General Surgery\t1900-01-01 00:00:00.000\t9999-12-31 00:00:00.000\t7\tWard/Unit\t3\tDepartment\t1900-01-01 00:00:00.000\t9999-12-31 00:00:00.000\t1900-01-01 00:00:00.000\t9999-12-31 00:00:00.000\tCODE\t\n 13\t1900-01-01 00:00:00.000\t9999-12-31 00:00:00.000\tXYZ\t1900-01-01 00:00:00.000\t9999-12-31 00:00:00.000\t275\tDDU Emergency & General Surgery\t1900-01-01 00:00:00.000\t9999-12-31 00:00:00.000\t7\tWard/Unit\t3\tDepartment\t1900-01-01 00:00:00.000\t9999-12-31 00:00:00.000\t1900-01-01 00:00:00.000\t9999-12-31 00:00:00.000\tCODE\t\n@@ -734,7 +726,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/filter_bug1391.parquet') limit\n 257\t1900-01-01 00:00:00.000\t9999-12-31 00:00:00.000\tXYZ\t1900-01-01 00:00:00.000\t9999-12-31 00:00:00.000\t141\tTherapies CBU\t1900-01-01 00:00:00.000\t9999-12-31 00:00:00.000\t7\tWard/Unit\tNULL\tNULL\t1900-01-01 00:00:00.000\t9999-12-31 00:00:00.000\t1900-01-01 00:00:00.000\t9999-12-31 00:00:00.000\tCODE\t\n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/fixed_length_decimal_legacy.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/fixed_length_decimal_legacy.parquet') limit 50;\n ----\n 1.00\t\n 2.00\t\n@@ -762,12 +754,12 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/fixed_length_decimal_lega\n 24.00\t\n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/timestamp.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/timestamp.parquet') limit 50;\n ----\n 2020-10-05 17:21:49.48844\n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/fixed_length_decimal.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/fixed_length_decimal.parquet') limit 50;\n ----\n 1.00\t\n 2.00\t\n@@ -795,7 +787,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/fixed_length_decimal.parq\n 24.00\t\n \n query IIIIIIIIIIIIIIIIIIII\n-SELECT * FROM parquet_scan('data/parquet-testing/leftdate3_192_loop_1.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/leftdate3_192_loop_1.parquet') limit 50;\n ----\n 00080010\t10006\t22156\t0.0\t1.0\t3743.0\t1925-12-31\t109.0\t109.75\t109.0\t400.0\tNULL\t109.375\t109.5\t600.0\t7.412625\t7.26\t109.75\tNULL\tNULL\t\n 00299090\t10022\t22158\t0.0\t1.0\t3420.0\t1925-12-31\t55.0\t56.0\t56.0\t3400.0\tNULL\t56.0\t56.25\t200.0\t9.365437\t9.365437\t55.125\tNULL\tNULL\t\n@@ -849,14 +841,14 @@ SELECT * FROM parquet_scan('data/parquet-testing/leftdate3_192_loop_1.parquet')\n 04557310\t10559\t22202\t0.0\t1.0\t5311.0\t1925-12-31\t53.0\t54.125\t53.0\t3800.0\tNULL\t52.75\t53.0\t599.0\t18.0\t18.0\t53.5\tNULL\tNULL\t\n \n query III\n-SELECT * FROM parquet_scan('data/parquet-testing/blob.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/blob.parquet') limit 50;\n ----\n 1\t\\x04\\x00\tstr1\t\n 2\t\\x04\\x00\\x80\tstr2\t\n 3\t\\x03\\xFF\\x00\\xFF\tstr3\n \n query IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n-SELECT * FROM parquet_scan('data/parquet-testing/bug1588.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/bug1588.parquet') limit 50;\n ----\n 1621259:e1WMOfPKh7EnuBJ+dG3V8mksk2NSFL8m7vLi1NPB3xk6NzQI4Dfqs3Ok2GmTRXqMqo3oc7T3ckM0/uTs/e4nVg==\t1621259\te1WMOfPKh7EnuBJ+dG3V8mksk2NSFL8m7vLi1NPB3xk6NzQI4Dfqs3Ok2GmTRXqMqo3oc7T3ckM0/uTs/e4nVg==\ta\tYiX2OkkxZvSMMT5TmbyZjlE8gCQSSmvxUrNBtLw1rWrs5cmxQNdTwPJgzgXNB3nF+1vaazrHwH32rnq67T7cHg==\thttp://crawler-test.com/urls/page_url_length_n\t0.66\t200\tPage URL Length\twl8lmqkOqcTS8gbpoGaBf0ZxvleJXOCIrIDuQui6k7nhUPa5Xu/tupkmw0xYxSz8ByUNjMGgY0i0egvh6WNBBw==\thttp://crawler-test.com/urls/page_url_length/xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\t0.0\t200\tPage URL Length (400 characters\ta\tPage URL Length (400 characters)\tFalse\tFalse\tTrue\tFalse\tNULL\tNULL\thttp://crawler-test.com/urls/page_url_length_n\tYiX2OkkxZvSMMT5TmbyZjlE8gCQSSmvxUrNBtLw1rWrs5cmxQNdTwPJgzgXNB3nF+1vaazrHwH32rnq67T7cHg==\thttp://crawler-test.com/urls/page_url_length/xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\twl8lmqkOqcTS8gbpoGaBf0ZxvleJXOCIrIDuQui6k7nhUPa5Xu/tupkmw0xYxSz8ByUNjMGgY0i0egvh6WNBBw==\tFalse\tFalse\tFalse\t2\t2\tFalse\tFalse\tFalse\t1\tdoc\t0\t\n 1621259:fCt7H+I6nGxXkRaRSYXL5kW5lkuXmaXAXiZdcm5dejcpwITcdVYNeYY0JQAJbDAOX2mjEOQX0Z/vvq9OAQDFUQ==\t1621259\tfCt7H+I6nGxXkRaRSYXL5kW5lkuXmaXAXiZdcm5dejcpwITcdVYNeYY0JQAJbDAOX2mjEOQX0Z/vvq9OAQDFUQ==\ta\tXnZecm8bhAZFA327isPnIwA+v6xejmC5P8/iQ0ax8ZMBhM/mCxes6Ugj8WsvwK6qKJteGGM5pLvdyBDhcaisyw==\thttps://crawler-test.com/content/page_html_size_n\t1.89\t200\tPage HTML Size\thzYV7GhCYN6hhz7hXVTbyIct7YXRsyHzsCriciWYnQeMCzu9UKIsFAKdKf3yecmiDIVgYA/N8lLETj4AyprBmA==\thttps://crawler-test.com/content/page_html_size/5\t0.27\t200\tPage html Size (~5 KB)\ta\tPage HTML Size (~5 KB)\tFalse\tFalse\tTrue\tFalse\tNULL\tNULL\thttps://crawler-test.com/content/page_html_size_n\tXnZecm8bhAZFA327isPnIwA+v6xejmC5P8/iQ0ax8ZMBhM/mCxes6Ugj8WsvwK6qKJteGGM5pLvdyBDhcaisyw==\thttps://crawler-test.com/content/page_html_size/5\thzYV7GhCYN6hhz7hXVTbyIct7YXRsyHzsCriciWYnQeMCzu9UKIsFAKdKf3yecmiDIVgYA/N8lLETj4AyprBmA==\tTrue\tTrue\tFalse\t2\t2\tFalse\tFalse\tFalse\t1\tdoc\t0\t\n@@ -910,7 +902,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/bug1588.parquet') limit 50\n 1621259:QQ/wWoUYYMJETokJJxVZSlqTHUMifVE7YGZcGNvLQkWH4CpFqgXBlt/ABSfSJs3zBJL9gitT6ABEmV5MWFwOUQ==\t1621259\tQQ/wWoUYYMJETokJJxVZSlqTHUMifVE7YGZcGNvLQkWH4CpFqgXBlt/ABSfSJs3zBJL9gitT6ABEmV5MWFwOUQ==\ta\t0FgjuY6ilewP5/Tv2RHwt9A66vuidBx3jYV1+sEb5Rcld/ErHsWinoZyRnlrirqQiLdBjxv9zCe/9i9wj+FPkg==\thttps://crawler-test.com/titles/page_title_length_n\t1.89\t200\tPage Title Length\t/yh69joQhtpoZX9pPVfv7cbB3acvTzB22nfHIpCNP1cgfcZC3BnkcflsLTVeRY5xZ1qDpYf5fGlQMu72PhPOxQ==\thttps://crawler-test.com/titles/page_title_length/2\t0.27\t200\txx\ta\tPage Title Length (2 characters)\tFalse\tFalse\tTrue\tFalse\tNULL\tNULL\thttps://crawler-test.com/titles/page_title_length_n\t0FgjuY6ilewP5/Tv2RHwt9A66vuidBx3jYV1+sEb5Rcld/ErHsWinoZyRnlrirqQiLdBjxv9zCe/9i9wj+FPkg==\thttps://crawler-test.com/titles/page_title_length/2\t/yh69joQhtpoZX9pPVfv7cbB3acvTzB22nfHIpCNP1cgfcZC3BnkcflsLTVeRY5xZ1qDpYf5fGlQMu72PhPOxQ==\tTrue\tTrue\tFalse\t2\t2\tFalse\tFalse\tFalse\t1\tdoc\t0\t\n \n query II\n-SELECT * FROM parquet_scan('data/parquet-testing/bug1589.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/bug1589.parquet') limit 50;\n ----\n 200\tNULL\t\n 300\tNULL\t\n@@ -966,7 +958,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/bug1589.parquet') limit 50\n mode skip \n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/hadoop_lz4_compressed_larger.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/hadoop_lz4_compressed_larger.parquet') limit 50;\n ----\n c7ce6bef-d5b0-4863-b199-8ea8c7fb117b\t\n e8fb9197-cb9f-4118-b67f-fbfa65f61843\t\n@@ -1024,7 +1016,7 @@ mode unskip\n mode skip \n \n query III\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/non_hadoop_lz4_compressed.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/non_hadoop_lz4_compressed.parquet') limit 50;\n ----\n 1593604800\tabc\t42.0\t\n 1593604800\tdef\t7.7\t\n@@ -1034,7 +1026,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/non_hadoop_lz4_compressed\n mode unskip \n \n query IIIIIIIIIII\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/alltypes_plain.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/alltypes_plain.parquet') limit 50;\n ----\n 4\tTrue\t0\t0\t0\t0\t0.0\t0.0\t03/01/09\t0\t2009-03-01 00:00:00\t\n 5\tFalse\t1\t1\t1\t10\t1.100000023841858\t10.1\t03/01/09\t1\t2009-03-01 00:01:00\t\n@@ -1048,7 +1040,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/alltypes_plain.parquet')\n mode skip \n \n query II\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/repeated_no_annotation.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/repeated_no_annotation.parquet') limit 50;\n ----\n 1\tNULL\t\n 2\tNULL\t\n@@ -1062,7 +1054,7 @@ mode unskip\n mode skip \n \n query IIIIIIIIIIII\n-SELECT * FROM parquet_scan('data/parquet-testing/data-types.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/data-types.parquet') limit 50;\n ----\n NULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\t\n 42\t43\t44\t45\t4.599999904632568\t4.7\t4.80\t49\t50\tTrue\t2019-11-26 20:11:42.501000\t2020-01-10\t\n@@ -1073,7 +1065,7 @@ NULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\n mode unskip \n \n query IIII\n-SELECT * FROM parquet_scan('data/parquet-testing/unsigned.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/unsigned.parquet') limit 50;\n ----\n 1\t1\t1\t1\t\n 2\t2\t2\t2\t\n@@ -1083,13 +1075,13 @@ SELECT * FROM parquet_scan('data/parquet-testing/unsigned.parquet') limit 50\n 255\t65535\t4294967295\t18446744073709551615\t\n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/pandas-date.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/pandas-date.parquet') limit 50;\n ----\n 2021-01-12\t\n 1921-12-24\t\n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/date.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/date.parquet') limit 50;\n ----\n 1970-01-01\t\n 1971-01-01\t\n@@ -1131,7 +1123,7 @@ NULL\n mode skip \n \n query IIIIII\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/nullable.impala.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/nullable.impala.parquet') limit 50;\n ----\n 1\t[1, 2, 3]\t[[1, 2], [3, 4]]\t{key: [k1, k2], value: [1, 100]}\t[{key: [k1], value: [1]}]\t{A: 1, : [1], C: {d: [[{E: 10, F: aaa}, {E: -10, F: bb}], [{E: 11, F: c}]]}, g: {key: [foo], value: [{H: {i: [1.1]}}]}}\t\n 2\t[NULL, 1, 2, NULL, 3, NULL]\t[[NULL, 1, 2, NULL], [3, NULL, 4], [], NULL]\t{key: [k1, k2], value: [2, NULL]}\t[{key: [k3, k1], value: [NULL, 1]}, NULL, {key: [], value: []}]\t{A: NULL, : [NULL], C: {d: [[{E: NULL, F: NULL}, {E: 10, F: aaa}, {E: NULL, F: NULL}, {E: -10, F: bb}, {E: NULL, F: NULL}], [{E: 11, F: c}, NULL], [], NULL]}, g: {key: [g1, g2, g3, g4, g5], value: [{H: {i: [2.2, NULL]}}, {H: {i: []}}, NULL, {H: {i: NULL}}, {H: NULL}]}}\t\n@@ -1146,7 +1138,7 @@ mode unskip\n mode skip \n \n query III\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/hadoop_lz4_compressed.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/hadoop_lz4_compressed.parquet') limit 50;\n ----\n 1593604800\tabc\t42.0\t\n 1593604800\tdef\t7.7\t\n@@ -1156,20 +1148,18 @@ SELECT * FROM parquet_scan('data/parquet-testing/arrow/hadoop_lz4_compressed.par\n mode unskip \n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/fixed.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/fixed.parquet') limit 50;\n ----\n \\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x09\\x0A\\x0B\\x0C\\x0D\\x0E\\x0F\n \n query IIIIIIIIIII\n-SELECT * FROM parquet_scan('data/parquet-testing/arrow/alltypes_plain.snappy.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/arrow/alltypes_plain.snappy.parquet') limit 50;\n ----\n 6\tTrue\t0\t0\t0\t0\t0.0\t0.0\t04/01/09\t0\t2009-04-01 00:00:00\t\n 7\tFalse\t1\t1\t1\t10\t1.100000023841858\t10.1\t04/01/09\t1\t2009-04-01 00:01:00\t\n \n-mode skip \n-\n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/decimal/arrow/int32_decimal.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/decimal/int32_decimal.parquet') limit 50;\n ----\n 1.00\t\n 2.00\t\n@@ -1196,10 +1186,8 @@ SELECT * FROM parquet_scan('data/parquet-testing/decimal/arrow/int32_decimal.par\n 23.00\t\n 24.00\t\n \n-mode unskip \n-\n query IIIII\n-SELECT * FROM parquet_scan('data/parquet-testing/decimal/pandas_decimal.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/decimal/pandas_decimal.parquet') limit 50;\n ----\n 1234.0\t12.34\t12345.6789\t123456789.98765433\t922337203685477580700.92230685477500000\t\n -1234.0\t-12.34\t-9765.4321\t-987654321.12345680\t-922337236854775807.92233720306854775\t\n@@ -1211,7 +1199,7 @@ NULL\tNULL\tNULL\tNULL\t0E-17\n mode skip \n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/decimal/arrow/byte_array_decimal.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/decimal/arrow/byte_array_decimal.parquet') limit 50;\n ----\n 1.00\t\n 2.00\t\n@@ -1241,7 +1229,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/decimal/arrow/byte_array_decima\n mode unskip \n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/decimal/decimal_dc.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/decimal/decimal_dc.parquet') limit 50\n ----\n NULL\t\n NULL\t\n@@ -1294,10 +1282,8 @@ NULL\n NULL\t\n NULL\t\n \n-mode skip \n-\n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/decimal/int64_decimal.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/decimal/int64_decimal.parquet') limit 50\n ----\n 1.00\t\n 2.00\t\n@@ -1324,10 +1310,8 @@ SELECT * FROM parquet_scan('data/parquet-testing/decimal/int64_decimal.parquet')\n 23.00\t\n 24.00\t\n \n-mode unskip \n-\n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/decimal/fixed_length_decimal_legacy.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/decimal/fixed_length_decimal_legacy.parquet') limit 50;\n ----\n 1.00\t\n 2.00\t\n@@ -1355,7 +1339,7 @@ SELECT * FROM parquet_scan('data/parquet-testing/decimal/fixed_length_decimal_le\n 24.00\t\n \n query I\n-SELECT * FROM parquet_scan('data/parquet-testing/decimal/fixed_length_decimal.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/decimal/fixed_length_decimal.parquet') limit 50;\n ----\n 1.00\t\n 2.00\t\n@@ -1383,29 +1367,29 @@ SELECT * FROM parquet_scan('data/parquet-testing/decimal/fixed_length_decimal.pa\n 24.00\t\n \n query II\n-SELECT * FROM parquet_scan('data/parquet-testing/glob2/t1.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/glob2/t1.parquet') limit 50;\n ----\n 1\thello\t\n \n query II\n-SELECT * FROM parquet_scan('data/parquet-testing/cache/cache1.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/cache/cache1.parquet') limit 50;\n ----\n 1\thello\t\n \n query II\n-SELECT * FROM parquet_scan('data/parquet-testing/cache/cache2.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/cache/cache2.parquet') limit 50;\n ----\n 0\t10\t\n 1\t20\t\n 2\t30\t\n \n query II\n-SELECT * FROM parquet_scan('data/parquet-testing/glob/t2.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/glob/t2.parquet') limit 50;\n ----\n 1\thello\t\n \n query II\n-SELECT * FROM parquet_scan('data/parquet-testing/glob/t1.parquet') limit 50 \n+SELECT * FROM parquet_scan('data/parquet-testing/glob/t1.parquet') limit 50;\n ----\n 1\thello\t\n \ndiff --git a/test/sql/copy/parquet/incorrect_converted_type.test b/test/sql/copy/parquet/incorrect_converted_type.test\nnew file mode 100644\nindex 000000000000..7ab14fe672a7\n--- /dev/null\n+++ b/test/sql/copy/parquet/incorrect_converted_type.test\n@@ -0,0 +1,39 @@\n+# name: test/sql/copy/parquet/incorrect_converted_type.test\n+# description: Test parquet files with incorrect converted type annotations\n+# group: [parquet]\n+\n+require parquet\n+\n+statement error\n+SELECT * FROM 'data/parquet-testing/broken/broken_bigint.parquet';\n+\n+statement error\n+SELECT * FROM 'data/parquet-testing/broken/broken_date.parquet';\n+\n+statement error\n+SELECT * FROM 'data/parquet-testing/broken/broken_int.parquet';\n+\n+statement error\n+SELECT * FROM 'data/parquet-testing/broken/broken_smallint.parquet';\n+\n+statement error\n+SELECT * FROM 'data/parquet-testing/broken/broken_timestamp.parquet';\n+\n+statement error\n+SELECT * FROM 'data/parquet-testing/broken/broken_timestamp_ms.parquet';\n+\n+statement error\n+SELECT * FROM 'data/parquet-testing/broken/broken_tinyint.parquet';\n+\n+statement error\n+SELECT * FROM 'data/parquet-testing/broken/broken_ubigint.parquet';\n+\n+statement error\n+SELECT * FROM 'data/parquet-testing/broken/broken_uinteger.parquet';\n+\n+statement error\n+SELECT * FROM 'data/parquet-testing/broken/broken_usmallint.parquet';\n+\n+statement error\n+SELECT * FROM 'data/parquet-testing/broken/broken_utinyint.parquet';\n+\ndiff --git a/test/sql/copy/parquet/test_parquet_scan.test b/test/sql/copy/parquet/test_parquet_scan.test\nindex ea90fe4e8556..a81d3f234e35 100644\n--- a/test/sql/copy/parquet/test_parquet_scan.test\n+++ b/test/sql/copy/parquet/test_parquet_scan.test\n@@ -95,7 +95,7 @@ query IIIIRRITTTTI\n SELECT * FROM parquet_scan('data/parquet-testing/data-types.parquet')\n ----\n NULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\n-42\t43\t44\t45\t4.600000\t4.700000\t480\t49\t50\t1\t2019-11-26 20:11:42.501\t2020-01-10\n+42\t43\t44\t45\t4.600000\t4.700000\t4.80\t49\t50\t1\t2019-11-26 20:11:42.501\t2020-01-10\n -127\t-32767\t-2147483647\t-9223372036854775807\t-4.600000\t-4.700000\tNULL\tNULL\tNULL\t0\tNULL\tNULL\n 127\t32767\t2147483647\t9223372036854775807\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\n NULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\ndiff --git a/test/sql/copy/parquet/writer/parquet_write_date.test b/test/sql/copy/parquet/writer/parquet_write_date.test\nnew file mode 100644\nindex 000000000000..eb7f60ca2f7c\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/parquet_write_date.test\n@@ -0,0 +1,37 @@\n+# name: test/sql/copy/parquet/writer/parquet_write_date.test\n+# description: Parquet dates round trip\n+# group: [writer]\n+\n+require parquet\n+\n+require vector_size 64\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE dates(d DATE)\n+\n+statement ok\n+INSERT INTO dates VALUES (DATE '1992-01-01'), (DATE '1900-01-01'), (NULL), (DATE '2020-09-27')\n+\n+query I nosort date_scan\n+SELECT * FROM dates\n+----\n+\n+statement ok\n+COPY dates TO '__TEST_DIR__/dates.parquet' (FORMAT 'parquet');\n+\n+query I nosort date_scan\n+SELECT * FROM '__TEST_DIR__/dates.parquet'\n+----\n+\n+query I\n+SELECT typeof(d) FROM '__TEST_DIR__/dates.parquet' LIMIT 1\n+----\n+DATE\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/dates.parquet' WHERE d='1992-01-01'\n+----\n+1992-01-01\ndiff --git a/test/sql/copy/parquet/writer/parquet_write_hugeint.test b/test/sql/copy/parquet/writer/parquet_write_hugeint.test\nnew file mode 100644\nindex 000000000000..9e61b1f7fc14\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/parquet_write_hugeint.test\n@@ -0,0 +1,33 @@\n+# name: test/sql/copy/parquet/writer/parquet_write_hugeint.test\n+# description: Parquet hugeint round trip\n+# group: [writer]\n+\n+require parquet\n+\n+require vector_size 64\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE hugeints(h HUGEINT)\n+\n+statement ok\n+INSERT INTO hugeints VALUES (-1180591620717411303424), (0), (NULL), (1180591620717411303424)\n+\n+statement ok\n+COPY hugeints TO '__TEST_DIR__/hugeints.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/hugeints.parquet'\n+----\n+-1180591620717411303424\n+0\n+NULL\n+1180591620717411303424\n+\n+query I\n+SELECT typeof(h) FROM '__TEST_DIR__/hugeints.parquet' LIMIT 1\n+----\n+DOUBLE\n+\ndiff --git a/test/sql/copy/parquet/writer/parquet_write_signed.test b/test/sql/copy/parquet/writer/parquet_write_signed.test\nnew file mode 100644\nindex 000000000000..af5c14286962\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/parquet_write_signed.test\n@@ -0,0 +1,77 @@\n+# name: test/sql/copy/parquet/writer/parquet_write_signed.test\n+# description: Parquet signed types round trip\n+# group: [writer]\n+\n+require parquet\n+\n+require vector_size 64\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE values_TINYINT AS SELECT d::TINYINT d FROM (VALUES\n+    (-128), (42), (NULL), (127)) tbl (d);\n+\n+statement ok\n+CREATE TABLE values_SMALLINT AS SELECT d::SMALLINT d FROM (VALUES\n+    (-32768), (42), (NULL), (32767)) tbl (d);\n+\n+statement ok\n+CREATE TABLE values_INTEGER AS SELECT d::INTEGER d FROM (VALUES\n+    (-2147483648), (42), (NULL), (2147483647)) tbl (d);\n+\n+statement ok\n+CREATE TABLE values_BIGINT AS SELECT d::BIGINT d FROM (VALUES\n+    (-9223372036854775808), (42), (NULL), (9223372036854775807)) tbl (d);\n+\n+foreach type TINYINT SMALLINT INTEGER BIGINT\n+\n+statement ok\n+CREATE OR REPLACE TABLE signed(d ${type})\n+\n+statement ok\n+INSERT INTO signed SELECT * FROM values_${type}\n+\n+statement ok\n+COPY signed TO '__TEST_DIR__/signed.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/signed.parquet' EXCEPT SELECT * FROM signed\n+----\n+\n+query I\n+SELECT * FROM signed EXCEPT SELECT * FROM '__TEST_DIR__/signed.parquet'\n+----\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/signed.parquet' WHERE d=42\n+----\n+42\n+\n+query I\n+SELECT COUNT(*) FROM '__TEST_DIR__/signed.parquet' WHERE d>42\n+----\n+1\n+\n+query I\n+SELECT COUNT(*) FROM '__TEST_DIR__/signed.parquet' WHERE d>=42\n+----\n+2\n+\n+query I\n+SELECT COUNT(*) FROM '__TEST_DIR__/signed.parquet' WHERE d<42\n+----\n+1\n+\n+query I\n+SELECT COUNT(*) FROM '__TEST_DIR__/signed.parquet' WHERE d<=42\n+----\n+2\n+\n+query I\n+SELECT typeof(d)='${type}' FROM '__TEST_DIR__/signed.parquet' LIMIT 1\n+----\n+true\n+\n+endloop\n\\ No newline at end of file\ndiff --git a/test/sql/copy/parquet/writer/parquet_write_timestamp.test b/test/sql/copy/parquet/writer/parquet_write_timestamp.test\nnew file mode 100644\nindex 000000000000..415d5338db94\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/parquet_write_timestamp.test\n@@ -0,0 +1,45 @@\n+# name: test/sql/copy/parquet/writer/parquet_write_timestamp.test\n+# description: Parquet timestamp round trip\n+# group: [writer]\n+\n+require parquet\n+\n+require vector_size 64\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+foreach type TIMESTAMP TIMESTAMP_MS TIMESTAMP_NS TIMESTAMP_S\n+\n+statement ok\n+CREATE OR REPLACE TABLE timestamps(d ${type})\n+\n+statement ok\n+INSERT INTO timestamps VALUES\n+    (TIMESTAMP '1992-01-01 12:03:27'),\n+    (TIMESTAMP '1900-01-01 03:08:47'),\n+    (NULL),\n+    (TIMESTAMP '2020-09-27 13:12:01')\n+\n+query I nosort ts_scan\n+SELECT * FROM timestamps\n+----\n+\n+statement ok\n+COPY timestamps TO '__TEST_DIR__/timestamps.parquet' (FORMAT 'parquet');\n+\n+query I nosort ts_scan\n+SELECT * FROM '__TEST_DIR__/timestamps.parquet'\n+----\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/timestamps.parquet' WHERE d='1992-01-01 12:03:27'\n+----\n+1992-01-01 12:03:27\n+\n+query I\n+SELECT typeof(d) FROM '__TEST_DIR__/timestamps.parquet' LIMIT 1\n+----\n+TIMESTAMP\n+\n+endloop\n\\ No newline at end of file\ndiff --git a/test/sql/copy/parquet/writer/parquet_write_unsigned.test b/test/sql/copy/parquet/writer/parquet_write_unsigned.test\nnew file mode 100644\nindex 000000000000..c97dd5c22586\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/parquet_write_unsigned.test\n@@ -0,0 +1,77 @@\n+# name: test/sql/copy/parquet/writer/parquet_write_unsigned.test\n+# description: Parquet unsigned types round trip\n+# group: [writer]\n+\n+require parquet\n+\n+require vector_size 64\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE values_UTINYINT AS SELECT d::UTINYINT d FROM (VALUES\n+    (0), (42), (NULL), (255)) tbl (d);\n+\n+statement ok\n+CREATE TABLE values_USMALLINT AS SELECT d::USMALLINT d FROM (VALUES\n+    (0), (42), (NULL), (65535)) tbl (d);\n+\n+statement ok\n+CREATE TABLE values_UINTEGER AS SELECT d::UINTEGER d FROM (VALUES\n+    (0), (42), (NULL), (4294967295)) tbl (d);\n+\n+statement ok\n+CREATE TABLE values_UBIGINT AS SELECT d::UBIGINT d FROM (VALUES\n+    (0), (42), (NULL), (18446744073709551615)) tbl (d);\n+\n+foreach type UTINYINT USMALLINT UINTEGER UBIGINT\n+\n+statement ok\n+CREATE OR REPLACE TABLE unsigned(d ${type})\n+\n+statement ok\n+INSERT INTO unsigned SELECT * FROM values_${type}\n+\n+statement ok\n+COPY unsigned TO '__TEST_DIR__/unsigned.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/unsigned.parquet' EXCEPT SELECT * FROM unsigned\n+----\n+\n+query I\n+SELECT * FROM unsigned EXCEPT SELECT * FROM '__TEST_DIR__/unsigned.parquet'\n+----\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/unsigned.parquet' WHERE d=42\n+----\n+42\n+\n+query I\n+SELECT COUNT(*) FROM '__TEST_DIR__/unsigned.parquet' WHERE d>42\n+----\n+1\n+\n+query I\n+SELECT COUNT(*) FROM '__TEST_DIR__/unsigned.parquet' WHERE d>=42\n+----\n+2\n+\n+query I\n+SELECT COUNT(*) FROM '__TEST_DIR__/unsigned.parquet' WHERE d<42\n+----\n+1\n+\n+query I\n+SELECT COUNT(*) FROM '__TEST_DIR__/unsigned.parquet' WHERE d<=42\n+----\n+2\n+\n+query I\n+SELECT typeof(d)='${type}' FROM '__TEST_DIR__/unsigned.parquet' LIMIT 1\n+----\n+true\n+\n+endloop\n\\ No newline at end of file\ndiff --git a/test/sql/copy/parquet/test_parquet_write.test b/test/sql/copy/parquet/writer/test_parquet_write.test\nsimilarity index 86%\nrename from test/sql/copy/parquet/test_parquet_write.test\nrename to test/sql/copy/parquet/writer/test_parquet_write.test\nindex 120f40bea545..b7d715fdcd80 100644\n--- a/test/sql/copy/parquet/test_parquet_write.test\n+++ b/test/sql/copy/parquet/writer/test_parquet_write.test\n@@ -1,11 +1,10 @@\n-# name: test/sql/copy/parquet/test_parquet_write.test\n+# name: test/sql/copy/parquet/writer/test_parquet_write.test\n # description: Parquet basic write\n-# group: [parquet]\n+# group: [writer]\n \n require parquet\n \n # single scalar value\n-\n require vector_size 64\n \n statement ok\ndiff --git a/test/sql/copy/parquet/test_parquet_write_complex.test b/test/sql/copy/parquet/writer/test_parquet_write_complex.test\nsimilarity index 96%\nrename from test/sql/copy/parquet/test_parquet_write_complex.test\nrename to test/sql/copy/parquet/writer/test_parquet_write_complex.test\nindex 26f77396994f..d650ab3a1946 100644\n--- a/test/sql/copy/parquet/test_parquet_write_complex.test\n+++ b/test/sql/copy/parquet/writer/test_parquet_write_complex.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/copy/parquet/test_parquet_write_complex.test\n+# name: test/sql/copy/parquet/writer/test_parquet_write_complex.test\n # description: Parquet read and re-write various files\n-# group: [parquet]\n+# group: [writer]\n \n require parquet\n \ndiff --git a/tools/pythonpkg/tests/fast/arrow/parquet_write_roundtrip.py b/tools/pythonpkg/tests/fast/arrow/parquet_write_roundtrip.py\nnew file mode 100644\nindex 000000000000..65cfc1d7dbaa\n--- /dev/null\n+++ b/tools/pythonpkg/tests/fast/arrow/parquet_write_roundtrip.py\n@@ -0,0 +1,93 @@\n+import duckdb\n+import pytest\n+import tempfile\n+import numpy\n+import pandas\n+import datetime\n+try:\n+    import pyarrow as pa\n+    can_run = True\n+except:\n+    can_run = False\n+\n+def parquet_types_test(type_list):\n+    temp = tempfile.NamedTemporaryFile()\n+    temp_name = temp.name\n+    for type_pair in type_list:\n+        value_list = type_pair[0]\n+        numpy_type = type_pair[1]\n+        sql_type = type_pair[2]\n+        add_cast = len(type_pair) > 3 and type_pair[3]\n+        add_sql_cast = len(type_pair) > 4 and type_pair[4]\n+        df = pandas.DataFrame.from_dict({\n+            'val': numpy.array(value_list, dtype=numpy_type)\n+        })\n+        duckdb_cursor = duckdb.connect()\n+        duckdb_cursor.execute(f\"CREATE TABLE tmp AS SELECT val::{sql_type} val FROM df\")\n+        duckdb_cursor.execute(f\"COPY tmp TO '{temp_name}' (FORMAT PARQUET)\")\n+        read_df = pandas.read_parquet(temp_name)\n+        if add_cast:\n+            read_df['val'] = read_df['val'].astype(numpy_type)\n+        assert df.equals(read_df)\n+\n+        read_from_duckdb = duckdb_cursor.execute(f\"SELECT * FROM parquet_scan('{temp_name}')\").df()\n+        assert read_df.equals(read_from_duckdb)\n+\n+        df.to_parquet(temp_name)\n+        if add_sql_cast:\n+            read_from_arrow = duckdb_cursor.execute(f\"SELECT val::{sql_type} val FROM parquet_scan('{temp_name}')\").df()\n+        else:\n+            read_from_arrow = duckdb_cursor.execute(f\"SELECT * FROM parquet_scan('{temp_name}')\").df()\n+        assert read_df.equals(read_from_arrow)\n+\n+\n+class TestParquetRoundtrip(object):\n+    def test_roundtrip_numeric(self, duckdb_cursor):\n+        if not can_run:\n+            return\n+        type_list = [\n+            ([-2**7, 0, 2**7-1], numpy.int8, 'TINYINT'),\n+            ([-2**15, 0, 2**15-1], numpy.int16, 'SMALLINT'),\n+            ([-2**31, 0, 2**31-1], numpy.int32, 'INTEGER'),\n+            ([-2**63, 0, 2**63-1], numpy.int64, 'BIGINT'),\n+            ([0, 42, 2**8-1], numpy.uint8, 'UTINYINT'),\n+            ([0, 42, 2**16-1], numpy.uint16, 'USMALLINT'),\n+            ([0, 42, 2**32-1], numpy.uint32, 'UINTEGER', False, True),\n+            ([0, 42, 2**64-1], numpy.uint64, 'UBIGINT'),\n+            ([0, 0.5, -0.5], numpy.float32, 'REAL'),\n+            ([0, 0.5, -0.5], numpy.float64, 'DOUBLE'),\n+        ]\n+        parquet_types_test(type_list)\n+\n+    def test_roundtrip_timestamp(self, duckdb_cursor):\n+        if not can_run:\n+            return\n+        date_time_list = [\n+            datetime.datetime(2018, 3, 10, 11, 17, 54),\n+            datetime.datetime(1900, 12, 12, 23, 48, 42),\n+            None,\n+            datetime.datetime(1992, 7, 9, 7, 5, 33)\n+        ]\n+        type_list = [\n+            (date_time_list, 'datetime64[ns]', 'TIMESTAMP_NS'),\n+            (date_time_list, 'datetime64[us]', 'TIMESTAMP'),\n+            (date_time_list, 'datetime64[ms]', 'TIMESTAMP_MS'),\n+            (date_time_list, 'datetime64[s]', 'TIMESTAMP_S'),\n+            (date_time_list, 'datetime64[D]', 'DATE', True)\n+        ]\n+        parquet_types_test(type_list)\n+\n+    def test_roundtrip_varchar(self, duckdb_cursor):\n+        if not can_run:\n+            return\n+        varchar_list = [\n+            'hello',\n+            'this is a very long string',\n+            'hello',\n+            None\n+        ]\n+        type_list = [\n+            (varchar_list, object, 'VARCHAR')\n+        ]\n+        parquet_types_test(type_list)\n+\ndiff --git a/tools/pythonpkg/tests/fast/types/test_blob.py b/tools/pythonpkg/tests/fast/types/test_blob.py\nindex d681a9e152f1..40c748e77dc0 100644\n--- a/tools/pythonpkg/tests/fast/types/test_blob.py\n+++ b/tools/pythonpkg/tests/fast/types/test_blob.py\n@@ -9,4 +9,4 @@ def test_blob(self, duckdb_cursor):\n \n         duckdb_cursor.execute(\"SELECT BLOB 'hello' AS a\")\n         results = duckdb_cursor.fetchnumpy()\n-        assert results['a'] == numpy.array([b'hello'], dtype=numpy.object)\n+        assert results['a'] == numpy.array([b'hello'], dtype=object)\n",
  "problem_statement": "Should `Date` types roundtrip to parquet?\n**What does happen?**\r\nWhen saving a Date to parquet, it looks like it's being stored as a datetime instead of a Date. I looked through the documentation and didn't see anything that mentioned this was intentional, but it may be. \r\n\r\n**What should happen?**\r\nStore Dates as Dates \r\n\r\n**To Reproduce**\r\n```r\r\nlibrary(duckdb)\r\n#> Loading required package: DBI\r\n\r\ncon <- dbConnect(duckdb())\r\n\r\ndf <- data.frame(date = Sys.Date())\r\n\r\ndbWriteTable(con, \"df\", df)\r\ndbExecute(con, \"COPY (SELECT * FROM df) TO 'df.parquet' (FORMAT 'parquet');\")\r\n#> [1] 1\r\n\r\ndf_rt <- dbGetQuery(con, \"SELECT * FROM parquet_scan('df.parquet')\")\r\n\r\nwaldo::compare(df, df_rt)\r\n#> `class(old$date)`: \"Date\"            \r\n#> `class(new$date)`: \"POSIXct\" \"POSIXt\"\r\n#> \r\n#> `attr(old$date, 'tzone')` is absent\r\n#> `attr(new$date, 'tzone')` is a character vector ('UTC')\r\n#> \r\n#> `old$date`:      18919\r\n#> `new$date`: 1634601600\r\n```\r\n\r\n**Environment (please complete the following information):**\r\n - OS: macOS 11.6\r\n - DuckDB Version: install from `master` branch\r\n\r\n**Before submitting**\r\n- [x] Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n- [x] Have you tried this on the latest `master` branch? In case you cannot compile, you may find some binaries here: https://github.com/duckdb/duckdb/releases/tag/master-builds\r\n\nShould `Date` types roundtrip to parquet?\n**What does happen?**\r\nWhen saving a Date to parquet, it looks like it's being stored as a datetime instead of a Date. I looked through the documentation and didn't see anything that mentioned this was intentional, but it may be. \r\n\r\n**What should happen?**\r\nStore Dates as Dates \r\n\r\n**To Reproduce**\r\n```r\r\nlibrary(duckdb)\r\n#> Loading required package: DBI\r\n\r\ncon <- dbConnect(duckdb())\r\n\r\ndf <- data.frame(date = Sys.Date())\r\n\r\ndbWriteTable(con, \"df\", df)\r\ndbExecute(con, \"COPY (SELECT * FROM df) TO 'df.parquet' (FORMAT 'parquet');\")\r\n#> [1] 1\r\n\r\ndf_rt <- dbGetQuery(con, \"SELECT * FROM parquet_scan('df.parquet')\")\r\n\r\nwaldo::compare(df, df_rt)\r\n#> `class(old$date)`: \"Date\"            \r\n#> `class(new$date)`: \"POSIXct\" \"POSIXt\"\r\n#> \r\n#> `attr(old$date, 'tzone')` is absent\r\n#> `attr(new$date, 'tzone')` is a character vector ('UTC')\r\n#> \r\n#> `old$date`:      18919\r\n#> `new$date`: 1634601600\r\n```\r\n\r\n**Environment (please complete the following information):**\r\n - OS: macOS 11.6\r\n - DuckDB Version: install from `master` branch\r\n\r\n**Before submitting**\r\n- [x] Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n- [x] Have you tried this on the latest `master` branch? In case you cannot compile, you may find some binaries here: https://github.com/duckdb/duckdb/releases/tag/master-builds\r\n\n",
  "hints_text": "The issue here is that the DuckDB Parquet writer does not yet emit Parquet logical types (https://github.com/apache/parquet-format/blob/master/LogicalTypes.md), but it should do that.\nThe issue here is that the DuckDB Parquet writer does not yet emit Parquet logical types (https://github.com/apache/parquet-format/blob/master/LogicalTypes.md), but it should do that.",
  "created_at": "2021-11-14T12:50:43Z"
}