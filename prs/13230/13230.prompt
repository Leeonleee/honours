You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Segfault for simultaneous UPDATE and DROP on same table
### What happens?

When doing these simultaneously, the `UPDATE` query segfaults in `duckdb::StorageLock::GetExclusiveLock()`.

Stack trace:
```
         21:   threads.emplace_back([&]() {
         22:     duckdb::Connection con(db);
      >  23:     auto result = con.Query("UPDATE t1 SET i = 4 WHERE i = 2");
         24:     if (result->HasError()) {
         25:       std::cerr << result->GetError() << "\n";
         26:       return;
#14   Object "/opt/duckdb/libduckdb.so", at 0x7c96750ae046, in duckdb::Connection::Query(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)
#13   Object "/opt/duckdb/libduckdb.so", at 0x7c96750ada3f, in duckdb::ClientContext::Query(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool)
#12   Object "/opt/duckdb/libduckdb.so", at 0x7c96750ab213, in duckdb::ClientContext::ExecutePendingQueryInternal(duckdb::ClientContextLock&, duckdb::PendingQueryResult&)
#11   Object "/opt/duckdb/libduckdb.so", at 0x7c96750ab1a7, in duckdb::PendingQueryResult::ExecuteInternal(duckdb::ClientContextLock&)
#10   Object "/opt/duckdb/libduckdb.so", at 0x7c96750aa547, in duckdb::ClientContext::FetchResultInternal(duckdb::ClientContextLock&, duckdb::PendingQueryResult&)
#9    Object "/opt/duckdb/libduckdb.so", at 0x7c96750a9fe7, in duckdb::ClientContext::CleanupInternal(duckdb::ClientContextLock&, duckdb::BaseQueryResult*, bool)
#8    Object "/opt/duckdb/libduckdb.so", at 0x7c96750a9e1f, in duckdb::ClientContext::EndQueryInternal(duckdb::ClientContextLock&, bool, bool)
#7    Object "/opt/duckdb/libduckdb.so", at 0x7c9675235a77, in duckdb::TransactionContext::Commit()
#6    Object "/opt/duckdb/libduckdb.so", at 0x7c967523550c, in duckdb::MetaTransaction::Commit()
#5    Object "/opt/duckdb/libduckdb.so", at 0x7c9675236f36, in duckdb::DuckTransactionManager::CommitTransaction(duckdb::ClientContext&, duckdb::Transaction&)
#4    Object "/opt/duckdb/libduckdb.so", at 0x7c9675234ceb, in duckdb::DuckTransactionManager::RemoveTransaction(duckdb::DuckTransaction&, bool)
#3    Object "/opt/duckdb/libduckdb.so", at 0x7c96752336f3, in duckdb::UndoBuffer::Cleanup()
#2    Object "/opt/duckdb/libduckdb.so", at 0x7c96751cd0c4, in duckdb::UpdateSegment::CleanupUpdate(duckdb::UpdateInfo&)
#1    Object "/opt/duckdb/libduckdb.so", at 0x7c967520a4e5, in duckdb::StorageLock::GetExclusiveLock()
#0    Source "./nptl/pthread_mutex_lock.c", line 80, in ___pthread_mutex_lock [0x7c967309ffe4]
Segmentation fault (Signal sent by the kernel [(nil)])
Segmentation fault (core dumped)
```

Documentation I can find says that snapshot isolation is used for data, and specifies what forms of concurrent DML can cause conflicts, but doesn't say anything about DDL.

I understand if this is undefined and the segfault is the best that can be done, but the docs don't mention it so I figure maybe it's a bug and something more graceful can be done here.

### To Reproduce

```
#include <deque>
#include <iostream>
#include <thread>

#include "duckdb/main/appender.hpp"
#include "duckdb/main/connection.hpp"
#include "duckdb/main/database.hpp"

int main() {
  duckdb::DuckDB db(/*path=*/nullptr);
  duckdb::Connection con(db);
  con.Query("CREATE TABLE t1 (i INT)");
  duckdb::Appender appender(con, "t1");
  appender.AppendRow(1);
  appender.AppendRow(2);
  appender.AppendRow(3);
  appender.Close();

  std::deque<std::thread> threads;

  threads.emplace_back([&]() {
    duckdb::Connection con(db);
    auto result = con.Query("UPDATE t1 SET i = 4 WHERE i = 2");
    if (result->HasError()) {
      std::cerr << result->GetError() << "\n";
      return;
    }
  });

  threads.emplace_back([&]() {
    duckdb::Connection con(db);
    auto result = con.Query("DROP TABLE t1");
    if (result->HasError()) {
      std::cerr << result->GetError() << "\n";
      return;
    }
  });

  for (auto& thread : threads) {
    thread.join();
  }
  return 0;
}
```

### OS:

Ubuntu 24.04, x86

### DuckDB Version:

1.0.0

### DuckDB Client:

C++

### Full Name:

James Hill

### Affiliation:

Promoted.ai

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a source build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have
Segfault for simultaneous UPDATE and DROP on same table
### What happens?

When doing these simultaneously, the `UPDATE` query segfaults in `duckdb::StorageLock::GetExclusiveLock()`.

Stack trace:
```
         21:   threads.emplace_back([&]() {
         22:     duckdb::Connection con(db);
      >  23:     auto result = con.Query("UPDATE t1 SET i = 4 WHERE i = 2");
         24:     if (result->HasError()) {
         25:       std::cerr << result->GetError() << "\n";
         26:       return;
#14   Object "/opt/duckdb/libduckdb.so", at 0x7c96750ae046, in duckdb::Connection::Query(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)
#13   Object "/opt/duckdb/libduckdb.so", at 0x7c96750ada3f, in duckdb::ClientContext::Query(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool)
#12   Object "/opt/duckdb/libduckdb.so", at 0x7c96750ab213, in duckdb::ClientContext::ExecutePendingQueryInternal(duckdb::ClientContextLock&, duckdb::PendingQueryResult&)
#11   Object "/opt/duckdb/libduckdb.so", at 0x7c96750ab1a7, in duckdb::PendingQueryResult::ExecuteInternal(duckdb::ClientContextLock&)
#10   Object "/opt/duckdb/libduckdb.so", at 0x7c96750aa547, in duckdb::ClientContext::FetchResultInternal(duckdb::ClientContextLock&, duckdb::PendingQueryResult&)
#9    Object "/opt/duckdb/libduckdb.so", at 0x7c96750a9fe7, in duckdb::ClientContext::CleanupInternal(duckdb::ClientContextLock&, duckdb::BaseQueryResult*, bool)
#8    Object "/opt/duckdb/libduckdb.so", at 0x7c96750a9e1f, in duckdb::ClientContext::EndQueryInternal(duckdb::ClientContextLock&, bool, bool)
#7    Object "/opt/duckdb/libduckdb.so", at 0x7c9675235a77, in duckdb::TransactionContext::Commit()
#6    Object "/opt/duckdb/libduckdb.so", at 0x7c967523550c, in duckdb::MetaTransaction::Commit()
#5    Object "/opt/duckdb/libduckdb.so", at 0x7c9675236f36, in duckdb::DuckTransactionManager::CommitTransaction(duckdb::ClientContext&, duckdb::Transaction&)
#4    Object "/opt/duckdb/libduckdb.so", at 0x7c9675234ceb, in duckdb::DuckTransactionManager::RemoveTransaction(duckdb::DuckTransaction&, bool)
#3    Object "/opt/duckdb/libduckdb.so", at 0x7c96752336f3, in duckdb::UndoBuffer::Cleanup()
#2    Object "/opt/duckdb/libduckdb.so", at 0x7c96751cd0c4, in duckdb::UpdateSegment::CleanupUpdate(duckdb::UpdateInfo&)
#1    Object "/opt/duckdb/libduckdb.so", at 0x7c967520a4e5, in duckdb::StorageLock::GetExclusiveLock()
#0    Source "./nptl/pthread_mutex_lock.c", line 80, in ___pthread_mutex_lock [0x7c967309ffe4]
Segmentation fault (Signal sent by the kernel [(nil)])
Segmentation fault (core dumped)
```

Documentation I can find says that snapshot isolation is used for data, and specifies what forms of concurrent DML can cause conflicts, but doesn't say anything about DDL.

I understand if this is undefined and the segfault is the best that can be done, but the docs don't mention it so I figure maybe it's a bug and something more graceful can be done here.

### To Reproduce

```
#include <deque>
#include <iostream>
#include <thread>

#include "duckdb/main/appender.hpp"
#include "duckdb/main/connection.hpp"
#include "duckdb/main/database.hpp"

int main() {
  duckdb::DuckDB db(/*path=*/nullptr);
  duckdb::Connection con(db);
  con.Query("CREATE TABLE t1 (i INT)");
  duckdb::Appender appender(con, "t1");
  appender.AppendRow(1);
  appender.AppendRow(2);
  appender.AppendRow(3);
  appender.Close();

  std::deque<std::thread> threads;

  threads.emplace_back([&]() {
    duckdb::Connection con(db);
    auto result = con.Query("UPDATE t1 SET i = 4 WHERE i = 2");
    if (result->HasError()) {
      std::cerr << result->GetError() << "\n";
      return;
    }
  });

  threads.emplace_back([&]() {
    duckdb::Connection con(db);
    auto result = con.Query("DROP TABLE t1");
    if (result->HasError()) {
      std::cerr << result->GetError() << "\n";
      return;
    }
  });

  for (auto& thread : threads) {
    thread.join();
  }
  return 0;
}
```

### OS:

Ubuntu 24.04, x86

### DuckDB Version:

1.0.0

### DuckDB Client:

C++

### Full Name:

James Hill

### Affiliation:

Promoted.ai

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a source build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://www.duckdb.org/docs/installation) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/include/duckdb/transaction/duck_transaction.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/transaction/duck_transaction.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/transaction/transaction.hpp"
12: #include "duckdb/common/reference_map.hpp"
13: 
14: namespace duckdb {
15: class RowVersionManager;
16: class DuckTransactionManager;
17: class StorageLockKey;
18: class StorageCommitState;
19: struct UndoBufferProperties;
20: 
21: class DuckTransaction : public Transaction {
22: public:
23: 	DuckTransaction(DuckTransactionManager &manager, ClientContext &context, transaction_t start_time,
24: 	                transaction_t transaction_id, idx_t catalog_version);
25: 	~DuckTransaction() override;
26: 
27: 	//! The start timestamp of this transaction
28: 	transaction_t start_time;
29: 	//! The transaction id of this transaction
30: 	transaction_t transaction_id;
31: 	//! The commit id of this transaction, if it has successfully been committed
32: 	transaction_t commit_id;
33: 	//! Highest active query when the transaction finished, used for cleaning up
34: 	transaction_t highest_active_query;
35: 
36: 	atomic<idx_t> catalog_version;
37: 
38: public:
39: 	static DuckTransaction &Get(ClientContext &context, AttachedDatabase &db);
40: 	static DuckTransaction &Get(ClientContext &context, Catalog &catalog);
41: 	LocalStorage &GetLocalStorage();
42: 
43: 	void PushCatalogEntry(CatalogEntry &entry, data_ptr_t extra_data, idx_t extra_data_size);
44: 
45: 	void SetReadWrite() override;
46: 
47: 	bool ShouldWriteToWAL(AttachedDatabase &db);
48: 	ErrorData WriteToWAL(AttachedDatabase &db, unique_ptr<StorageCommitState> &commit_state) noexcept;
49: 	//! Commit the current transaction with the given commit identifier. Returns an error message if the transaction
50: 	//! commit failed, or an empty string if the commit was sucessful
51: 	ErrorData Commit(AttachedDatabase &db, transaction_t commit_id,
52: 	                 unique_ptr<StorageCommitState> commit_state) noexcept;
53: 	//! Returns whether or not a commit of this transaction should trigger an automatic checkpoint
54: 	bool AutomaticCheckpoint(AttachedDatabase &db, const UndoBufferProperties &properties);
55: 
56: 	//! Rollback
57: 	void Rollback() noexcept;
58: 	//! Cleanup the undo buffer
59: 	void Cleanup(transaction_t lowest_active_transaction);
60: 
61: 	bool ChangesMade();
62: 	UndoBufferProperties GetUndoProperties();
63: 
64: 	void PushDelete(DataTable &table, RowVersionManager &info, idx_t vector_idx, row_t rows[], idx_t count,
65: 	                idx_t base_row);
66: 	void PushSequenceUsage(SequenceCatalogEntry &entry, const SequenceData &data);
67: 	void PushAppend(DataTable &table, idx_t row_start, idx_t row_count);
68: 	UpdateInfo *CreateUpdateInfo(idx_t type_size, idx_t entries);
69: 
70: 	bool IsDuckTransaction() const override {
71: 		return true;
72: 	}
73: 
74: 	unique_ptr<StorageLockKey> TryGetCheckpointLock();
75: 	bool HasWriteLock() const {
76: 		return write_lock.get();
77: 	}
78: 
79: private:
80: 	DuckTransactionManager &transaction_manager;
81: 	//! The undo buffer is used to store old versions of rows that are updated
82: 	//! or deleted
83: 	UndoBuffer undo_buffer;
84: 	//! The set of uncommitted appends for the transaction
85: 	unique_ptr<LocalStorage> storage;
86: 	//! Write lock
87: 	unique_ptr<StorageLockKey> write_lock;
88: 	//! Lock for accessing sequence_usage
89: 	mutex sequence_lock;
90: 	//! Map of all sequences that were used during the transaction and the value they had in this transaction
91: 	reference_map_t<SequenceCatalogEntry, reference<SequenceValue>> sequence_usage;
92: };
93: 
94: } // namespace duckdb
[end of src/include/duckdb/transaction/duck_transaction.hpp]
[start of src/storage/data_table.cpp]
1: #include "duckdb/storage/data_table.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/chrono.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/helper.hpp"
7: #include "duckdb/common/vector_operations/vector_operations.hpp"
8: #include "duckdb/execution/expression_executor.hpp"
9: #include "duckdb/main/client_context.hpp"
10: #include "duckdb/parser/constraints/list.hpp"
11: #include "duckdb/planner/constraints/list.hpp"
12: #include "duckdb/planner/expression_binder/check_binder.hpp"
13: #include "duckdb/planner/table_filter.hpp"
14: #include "duckdb/storage/checkpoint/table_data_writer.hpp"
15: #include "duckdb/storage/storage_manager.hpp"
16: #include "duckdb/storage/table_storage_info.hpp"
17: #include "duckdb/storage/table/persistent_table_data.hpp"
18: #include "duckdb/storage/table/row_group.hpp"
19: #include "duckdb/storage/table/standard_column_data.hpp"
20: #include "duckdb/transaction/duck_transaction.hpp"
21: #include "duckdb/transaction/transaction_manager.hpp"
22: #include "duckdb/main/attached_database.hpp"
23: #include "duckdb/common/types/conflict_manager.hpp"
24: #include "duckdb/common/types/constraint_conflict_info.hpp"
25: #include "duckdb/storage/table/append_state.hpp"
26: #include "duckdb/storage/table/delete_state.hpp"
27: #include "duckdb/storage/table/scan_state.hpp"
28: #include "duckdb/storage/table/update_state.hpp"
29: #include "duckdb/common/exception/transaction_exception.hpp"
30: 
31: namespace duckdb {
32: 
33: DataTableInfo::DataTableInfo(AttachedDatabase &db, shared_ptr<TableIOManager> table_io_manager_p, string schema,
34:                              string table)
35:     : db(db), table_io_manager(std::move(table_io_manager_p)), schema(std::move(schema)), table(std::move(table)) {
36: }
37: 
38: void DataTableInfo::InitializeIndexes(ClientContext &context, const char *index_type) {
39: 	indexes.InitializeIndexes(context, *this, index_type);
40: }
41: 
42: bool DataTableInfo::IsTemporary() const {
43: 	return db.IsTemporary();
44: }
45: 
46: DataTable::DataTable(AttachedDatabase &db, shared_ptr<TableIOManager> table_io_manager_p, const string &schema,
47:                      const string &table, vector<ColumnDefinition> column_definitions_p,
48:                      unique_ptr<PersistentTableData> data)
49:     : db(db), info(make_shared_ptr<DataTableInfo>(db, std::move(table_io_manager_p), schema, table)),
50:       column_definitions(std::move(column_definitions_p)), is_root(true) {
51: 	// initialize the table with the existing data from disk, if any
52: 	auto types = GetTypes();
53: 	this->row_groups =
54: 	    make_shared_ptr<RowGroupCollection>(info, TableIOManager::Get(*this).GetBlockManagerForRowData(), types, 0);
55: 	if (data && data->row_group_count > 0) {
56: 		this->row_groups->Initialize(*data);
57: 	} else {
58: 		this->row_groups->InitializeEmpty();
59: 		D_ASSERT(row_groups->GetTotalRows() == 0);
60: 	}
61: 	row_groups->Verify();
62: }
63: 
64: DataTable::DataTable(ClientContext &context, DataTable &parent, ColumnDefinition &new_column, Expression &default_value)
65:     : db(parent.db), info(parent.info), is_root(true) {
66: 	// add the column definitions from this DataTable
67: 	for (auto &column_def : parent.column_definitions) {
68: 		column_definitions.emplace_back(column_def.Copy());
69: 	}
70: 	column_definitions.emplace_back(new_column.Copy());
71: 
72: 	auto &local_storage = LocalStorage::Get(context, db);
73: 
74: 	ExpressionExecutor default_executor(context);
75: 	default_executor.AddExpression(default_value);
76: 
77: 	// prevent any new tuples from being added to the parent
78: 	lock_guard<mutex> parent_lock(parent.append_lock);
79: 
80: 	this->row_groups = parent.row_groups->AddColumn(context, new_column, default_executor);
81: 
82: 	// also add this column to client local storage
83: 	local_storage.AddColumn(parent, *this, new_column, default_executor);
84: 
85: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
86: 	parent.is_root = false;
87: }
88: 
89: DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t removed_column)
90:     : db(parent.db), info(parent.info), is_root(true) {
91: 	// prevent any new tuples from being added to the parent
92: 	auto &local_storage = LocalStorage::Get(context, db);
93: 	lock_guard<mutex> parent_lock(parent.append_lock);
94: 
95: 	for (auto &column_def : parent.column_definitions) {
96: 		column_definitions.emplace_back(column_def.Copy());
97: 	}
98: 
99: 	info->InitializeIndexes(context);
100: 
101: 	// first check if there are any indexes that exist that point to the removed column
102: 	info->indexes.Scan([&](Index &index) {
103: 		for (auto &column_id : index.GetColumnIds()) {
104: 			if (column_id == removed_column) {
105: 				throw CatalogException("Cannot drop this column: an index depends on it!");
106: 			} else if (column_id > removed_column) {
107: 				throw CatalogException("Cannot drop this column: an index depends on a column after it!");
108: 			}
109: 		}
110: 		return false;
111: 	});
112: 
113: 	// erase the column definitions from this DataTable
114: 	D_ASSERT(removed_column < column_definitions.size());
115: 	column_definitions.erase_at(removed_column);
116: 
117: 	storage_t storage_idx = 0;
118: 	for (idx_t i = 0; i < column_definitions.size(); i++) {
119: 		auto &col = column_definitions[i];
120: 		col.SetOid(i);
121: 		if (col.Generated()) {
122: 			continue;
123: 		}
124: 		col.SetStorageOid(storage_idx++);
125: 	}
126: 
127: 	// alter the row_groups and remove the column from each of them
128: 	this->row_groups = parent.row_groups->RemoveColumn(removed_column);
129: 
130: 	// scan the original table, and fill the new column with the transformed value
131: 	local_storage.DropColumn(parent, *this, removed_column);
132: 
133: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
134: 	parent.is_root = false;
135: }
136: 
137: // Alter column to add new constraint
138: DataTable::DataTable(ClientContext &context, DataTable &parent, unique_ptr<BoundConstraint> constraint)
139:     : db(parent.db), info(parent.info), row_groups(parent.row_groups), is_root(true) {
140: 
141: 	auto &local_storage = LocalStorage::Get(context, db);
142: 	lock_guard<mutex> parent_lock(parent.append_lock);
143: 	for (auto &column_def : parent.column_definitions) {
144: 		column_definitions.emplace_back(column_def.Copy());
145: 	}
146: 
147: 	info->InitializeIndexes(context);
148: 
149: 	// Verify the new constraint against current persistent/local data
150: 	VerifyNewConstraint(local_storage, parent, *constraint);
151: 
152: 	// Get the local data ownership from old dt
153: 	local_storage.MoveStorage(parent, *this);
154: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
155: 	parent.is_root = false;
156: }
157: 
158: DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t changed_idx, const LogicalType &target_type,
159:                      const vector<column_t> &bound_columns, Expression &cast_expr)
160:     : db(parent.db), info(parent.info), is_root(true) {
161: 	auto &local_storage = LocalStorage::Get(context, db);
162: 	// prevent any tuples from being added to the parent
163: 	lock_guard<mutex> lock(append_lock);
164: 	for (auto &column_def : parent.column_definitions) {
165: 		column_definitions.emplace_back(column_def.Copy());
166: 	}
167: 
168: 	info->InitializeIndexes(context);
169: 
170: 	// first check if there are any indexes that exist that point to the changed column
171: 	info->indexes.Scan([&](Index &index) {
172: 		for (auto &column_id : index.GetColumnIds()) {
173: 			if (column_id == changed_idx) {
174: 				throw CatalogException("Cannot change the type of this column: an index depends on it!");
175: 			}
176: 		}
177: 		return false;
178: 	});
179: 
180: 	// change the type in this DataTable
181: 	column_definitions[changed_idx].SetType(target_type);
182: 
183: 	// set up the statistics for the table
184: 	// the column that had its type changed will have the new statistics computed during conversion
185: 	this->row_groups = parent.row_groups->AlterType(context, changed_idx, target_type, bound_columns, cast_expr);
186: 
187: 	// scan the original table, and fill the new column with the transformed value
188: 	local_storage.ChangeType(parent, *this, changed_idx, target_type, bound_columns, cast_expr);
189: 
190: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
191: 	parent.is_root = false;
192: }
193: 
194: vector<LogicalType> DataTable::GetTypes() {
195: 	vector<LogicalType> types;
196: 	for (auto &it : column_definitions) {
197: 		types.push_back(it.Type());
198: 	}
199: 	return types;
200: }
201: 
202: bool DataTable::IsTemporary() const {
203: 	return info->IsTemporary();
204: }
205: 
206: AttachedDatabase &DataTable::GetAttached() {
207: 	D_ASSERT(RefersToSameObject(db, info->db));
208: 	return db;
209: }
210: 
211: const vector<ColumnDefinition> &DataTable::Columns() const {
212: 	return column_definitions;
213: }
214: 
215: TableIOManager &DataTable::GetTableIOManager() {
216: 	return *info->table_io_manager;
217: }
218: 
219: TableIOManager &TableIOManager::Get(DataTable &table) {
220: 	return table.GetTableIOManager();
221: }
222: 
223: //===--------------------------------------------------------------------===//
224: // Scan
225: //===--------------------------------------------------------------------===//
226: void DataTable::InitializeScan(TableScanState &state, const vector<column_t> &column_ids,
227:                                TableFilterSet *table_filters) {
228: 	state.checkpoint_lock = info->checkpoint_lock.GetSharedLock();
229: 	state.Initialize(column_ids, table_filters);
230: 	row_groups->InitializeScan(state.table_state, column_ids, table_filters);
231: }
232: 
233: void DataTable::InitializeScan(DuckTransaction &transaction, TableScanState &state, const vector<column_t> &column_ids,
234:                                TableFilterSet *table_filters) {
235: 	auto &local_storage = LocalStorage::Get(transaction);
236: 	InitializeScan(state, column_ids, table_filters);
237: 	local_storage.InitializeScan(*this, state.local_state, table_filters);
238: }
239: 
240: void DataTable::InitializeScanWithOffset(TableScanState &state, const vector<column_t> &column_ids, idx_t start_row,
241:                                          idx_t end_row) {
242: 	state.checkpoint_lock = info->checkpoint_lock.GetSharedLock();
243: 	state.Initialize(column_ids);
244: 	row_groups->InitializeScanWithOffset(state.table_state, column_ids, start_row, end_row);
245: }
246: 
247: idx_t DataTable::MaxThreads(ClientContext &context) {
248: 	idx_t parallel_scan_vector_count = Storage::ROW_GROUP_VECTOR_COUNT;
249: 	if (ClientConfig::GetConfig(context).verify_parallelism) {
250: 		parallel_scan_vector_count = 1;
251: 	}
252: 	idx_t parallel_scan_tuple_count = STANDARD_VECTOR_SIZE * parallel_scan_vector_count;
253: 	return GetTotalRows() / parallel_scan_tuple_count + 1;
254: }
255: 
256: void DataTable::InitializeParallelScan(ClientContext &context, ParallelTableScanState &state) {
257: 	auto &local_storage = LocalStorage::Get(context, db);
258: 	state.checkpoint_lock = info->checkpoint_lock.GetSharedLock();
259: 	row_groups->InitializeParallelScan(state.scan_state);
260: 
261: 	local_storage.InitializeParallelScan(*this, state.local_state);
262: }
263: 
264: bool DataTable::NextParallelScan(ClientContext &context, ParallelTableScanState &state, TableScanState &scan_state) {
265: 	if (row_groups->NextParallelScan(context, state.scan_state, scan_state.table_state)) {
266: 		return true;
267: 	}
268: 	auto &local_storage = LocalStorage::Get(context, db);
269: 	if (local_storage.NextParallelScan(context, *this, state.local_state, scan_state.local_state)) {
270: 		return true;
271: 	} else {
272: 		// finished all scans: no more scans remaining
273: 		return false;
274: 	}
275: }
276: 
277: void DataTable::Scan(DuckTransaction &transaction, DataChunk &result, TableScanState &state) {
278: 	// scan the persistent segments
279: 	if (state.table_state.Scan(transaction, result)) {
280: 		D_ASSERT(result.size() > 0);
281: 		return;
282: 	}
283: 
284: 	// scan the transaction-local segments
285: 	auto &local_storage = LocalStorage::Get(transaction);
286: 	local_storage.Scan(state.local_state, state.GetColumnIds(), result);
287: }
288: 
289: bool DataTable::CreateIndexScan(TableScanState &state, DataChunk &result, TableScanType type) {
290: 	return state.table_state.ScanCommitted(result, type);
291: }
292: 
293: //===--------------------------------------------------------------------===//
294: // Index Methods
295: //===--------------------------------------------------------------------===//
296: shared_ptr<DataTableInfo> &DataTable::GetDataTableInfo() {
297: 	return info;
298: }
299: 
300: void DataTable::InitializeIndexes(ClientContext &context) {
301: 	info->InitializeIndexes(context);
302: }
303: 
304: bool DataTable::HasIndexes() const {
305: 	return !info->indexes.Empty();
306: }
307: 
308: void DataTable::AddIndex(unique_ptr<Index> index) {
309: 	info->indexes.AddIndex(std::move(index));
310: }
311: 
312: bool DataTable::HasForeignKeyIndex(const vector<PhysicalIndex> &keys, ForeignKeyType type) {
313: 	return info->indexes.FindForeignKeyIndex(keys, type) != nullptr;
314: }
315: 
316: void DataTable::SetIndexStorageInfo(vector<IndexStorageInfo> index_storage_info) {
317: 	info->index_storage_infos = std::move(index_storage_info);
318: }
319: 
320: void DataTable::VacuumIndexes() {
321: 	info->indexes.Scan([&](Index &index) {
322: 		if (index.IsBound()) {
323: 			index.Cast<BoundIndex>().Vacuum();
324: 		}
325: 		return false;
326: 	});
327: }
328: 
329: void DataTable::CleanupAppend(transaction_t lowest_transaction, idx_t start, idx_t count) {
330: 	row_groups->CleanupAppend(lowest_transaction, start, count);
331: }
332: 
333: bool DataTable::IndexNameIsUnique(const string &name) {
334: 	return info->indexes.NameIsUnique(name);
335: }
336: 
337: string DataTableInfo::GetSchemaName() {
338: 	return schema;
339: }
340: 
341: string DataTableInfo::GetTableName() {
342: 	lock_guard<mutex> l(name_lock);
343: 	return table;
344: }
345: 
346: void DataTableInfo::SetTableName(string name) {
347: 	lock_guard<mutex> l(name_lock);
348: 	table = std::move(name);
349: }
350: 
351: string DataTable::GetTableName() const {
352: 	return info->GetTableName();
353: }
354: 
355: void DataTable::SetTableName(string new_name) {
356: 	info->SetTableName(std::move(new_name));
357: }
358: 
359: TableStorageInfo DataTable::GetStorageInfo() {
360: 	TableStorageInfo result;
361: 	result.cardinality = GetTotalRows();
362: 	info->indexes.Scan([&](Index &index) {
363: 		IndexInfo index_info;
364: 		index_info.is_primary = index.IsPrimary();
365: 		index_info.is_unique = index.IsUnique() || index_info.is_primary;
366: 		index_info.is_foreign = index.IsForeign();
367: 		index_info.column_set = index.GetColumnIdSet();
368: 		result.index_info.push_back(std::move(index_info));
369: 		return false;
370: 	});
371: 	return result;
372: }
373: 
374: //===--------------------------------------------------------------------===//
375: // Fetch
376: //===--------------------------------------------------------------------===//
377: void DataTable::Fetch(DuckTransaction &transaction, DataChunk &result, const vector<column_t> &column_ids,
378:                       const Vector &row_identifiers, idx_t fetch_count, ColumnFetchState &state) {
379: 	auto lock = info->checkpoint_lock.GetSharedLock();
380: 	row_groups->Fetch(transaction, result, column_ids, row_identifiers, fetch_count, state);
381: }
382: 
383: //===--------------------------------------------------------------------===//
384: // Append
385: //===--------------------------------------------------------------------===//
386: static void VerifyNotNullConstraint(TableCatalogEntry &table, Vector &vector, idx_t count, const string &col_name) {
387: 	if (!VectorOperations::HasNull(vector, count)) {
388: 		return;
389: 	}
390: 
391: 	throw ConstraintException("NOT NULL constraint failed: %s.%s", table.name, col_name);
392: }
393: 
394: // To avoid throwing an error at SELECT, instead this moves the error detection to INSERT
395: static void VerifyGeneratedExpressionSuccess(ClientContext &context, TableCatalogEntry &table, DataChunk &chunk,
396:                                              Expression &expr, column_t index) {
397: 	auto &col = table.GetColumn(LogicalIndex(index));
398: 	D_ASSERT(col.Generated());
399: 	ExpressionExecutor executor(context, expr);
400: 	Vector result(col.Type());
401: 	try {
402: 		executor.ExecuteExpression(chunk, result);
403: 	} catch (InternalException &ex) {
404: 		throw;
405: 	} catch (std::exception &ex) {
406: 		ErrorData error(ex);
407: 		throw ConstraintException("Incorrect value for generated column \"%s %s AS (%s)\" : %s", col.Name(),
408: 		                          col.Type().ToString(), col.GeneratedExpression().ToString(), error.RawMessage());
409: 	}
410: }
411: 
412: static void VerifyCheckConstraint(ClientContext &context, TableCatalogEntry &table, Expression &expr,
413:                                   DataChunk &chunk) {
414: 	ExpressionExecutor executor(context, expr);
415: 	Vector result(LogicalType::INTEGER);
416: 	try {
417: 		executor.ExecuteExpression(chunk, result);
418: 	} catch (std::exception &ex) {
419: 		ErrorData error(ex);
420: 		throw ConstraintException("CHECK constraint failed: %s (Error: %s)", table.name, error.RawMessage());
421: 	} catch (...) { // LCOV_EXCL_START
422: 		throw ConstraintException("CHECK constraint failed: %s (Unknown Error)", table.name);
423: 	} // LCOV_EXCL_STOP
424: 	UnifiedVectorFormat vdata;
425: 	result.ToUnifiedFormat(chunk.size(), vdata);
426: 
427: 	auto dataptr = UnifiedVectorFormat::GetData<int32_t>(vdata);
428: 	for (idx_t i = 0; i < chunk.size(); i++) {
429: 		auto idx = vdata.sel->get_index(i);
430: 		if (vdata.validity.RowIsValid(idx) && dataptr[idx] == 0) {
431: 			throw ConstraintException("CHECK constraint failed: %s", table.name);
432: 		}
433: 	}
434: }
435: 
436: bool DataTable::IsForeignKeyIndex(const vector<PhysicalIndex> &fk_keys, Index &index, ForeignKeyType fk_type) {
437: 	if (fk_type == ForeignKeyType::FK_TYPE_PRIMARY_KEY_TABLE ? !index.IsUnique() : !index.IsForeign()) {
438: 		return false;
439: 	}
440: 	if (fk_keys.size() != index.GetColumnIds().size()) {
441: 		return false;
442: 	}
443: 	for (auto &fk_key : fk_keys) {
444: 		bool is_found = false;
445: 		for (auto &index_key : index.GetColumnIds()) {
446: 			if (fk_key.index == index_key) {
447: 				is_found = true;
448: 				break;
449: 			}
450: 		}
451: 		if (!is_found) {
452: 			return false;
453: 		}
454: 	}
455: 	return true;
456: }
457: 
458: // Find the first index that is not null, and did not find a match
459: static idx_t FirstMissingMatch(const ManagedSelection &matches) {
460: 	idx_t match_idx = 0;
461: 
462: 	for (idx_t i = 0; i < matches.Size(); i++) {
463: 		auto match = matches.IndexMapsToLocation(match_idx, i);
464: 		match_idx += match;
465: 		if (!match) {
466: 			// This index is missing in the matches vector
467: 			return i;
468: 		}
469: 	}
470: 	return DConstants::INVALID_INDEX;
471: }
472: 
473: idx_t LocateErrorIndex(bool is_append, const ManagedSelection &matches) {
474: 	idx_t failed_index = DConstants::INVALID_INDEX;
475: 	if (!is_append) {
476: 		// We expected to find nothing, so the first error is the first match
477: 		failed_index = matches[0];
478: 	} else {
479: 		// We expected to find matches for all of them, so the first missing match is the first error
480: 		return FirstMissingMatch(matches);
481: 	}
482: 	return failed_index;
483: }
484: 
485: [[noreturn]] static void ThrowForeignKeyConstraintError(idx_t failed_index, bool is_append, Index &conflict_index,
486:                                                         DataChunk &input) {
487: 	// The index that caused the conflict has to be bound by this point (or we would not have gotten here)
488: 	D_ASSERT(conflict_index.IsBound());
489: 	auto &index = conflict_index.Cast<BoundIndex>();
490: 	auto verify_type = is_append ? VerifyExistenceType::APPEND_FK : VerifyExistenceType::DELETE_FK;
491: 	D_ASSERT(failed_index != DConstants::INVALID_INDEX);
492: 	auto message = index.GetConstraintViolationMessage(verify_type, failed_index, input);
493: 	throw ConstraintException(message);
494: }
495: 
496: bool IsForeignKeyConstraintError(bool is_append, idx_t input_count, const ManagedSelection &matches) {
497: 	if (is_append) {
498: 		// We need to find a match for all values
499: 		return matches.Count() != input_count;
500: 	} else {
501: 		// We should not find any matches
502: 		return matches.Count() != 0;
503: 	}
504: }
505: 
506: static bool IsAppend(VerifyExistenceType verify_type) {
507: 	return verify_type == VerifyExistenceType::APPEND_FK;
508: }
509: 
510: void DataTable::VerifyForeignKeyConstraint(const BoundForeignKeyConstraint &bfk, ClientContext &context,
511:                                            DataChunk &chunk, VerifyExistenceType verify_type) {
512: 	const vector<PhysicalIndex> *src_keys_ptr = &bfk.info.fk_keys;
513: 	const vector<PhysicalIndex> *dst_keys_ptr = &bfk.info.pk_keys;
514: 
515: 	bool is_append = IsAppend(verify_type);
516: 	if (!is_append) {
517: 		src_keys_ptr = &bfk.info.pk_keys;
518: 		dst_keys_ptr = &bfk.info.fk_keys;
519: 	}
520: 
521: 	auto &table_entry_ptr =
522: 	    Catalog::GetEntry<TableCatalogEntry>(context, INVALID_CATALOG, bfk.info.schema, bfk.info.table);
523: 	// make the data chunk to check
524: 	vector<LogicalType> types;
525: 	for (auto &col : table_entry_ptr.GetColumns().Physical()) {
526: 		types.emplace_back(col.Type());
527: 	}
528: 	DataChunk dst_chunk;
529: 	dst_chunk.InitializeEmpty(types);
530: 	for (idx_t i = 0; i < src_keys_ptr->size(); i++) {
531: 		dst_chunk.data[(*dst_keys_ptr)[i].index].Reference(chunk.data[(*src_keys_ptr)[i].index]);
532: 	}
533: 	dst_chunk.SetCardinality(chunk.size());
534: 	auto &data_table = table_entry_ptr.GetStorage();
535: 
536: 	idx_t count = dst_chunk.size();
537: 	if (count <= 0) {
538: 		return;
539: 	}
540: 
541: 	// Set up a way to record conflicts, rather than directly throw on them
542: 	unordered_set<column_t> empty_column_list;
543: 	ConflictInfo empty_conflict_info(empty_column_list, false);
544: 	ConflictManager regular_conflicts(verify_type, count, &empty_conflict_info);
545: 	ConflictManager transaction_conflicts(verify_type, count, &empty_conflict_info);
546: 	regular_conflicts.SetMode(ConflictManagerMode::SCAN);
547: 	transaction_conflicts.SetMode(ConflictManagerMode::SCAN);
548: 
549: 	data_table.info->indexes.VerifyForeignKey(*dst_keys_ptr, dst_chunk, regular_conflicts);
550: 	regular_conflicts.Finalize();
551: 	auto &regular_matches = regular_conflicts.Conflicts();
552: 
553: 	// check if we can insert the chunk into the reference table's local storage
554: 	auto &local_storage = LocalStorage::Get(context, db);
555: 	bool error = IsForeignKeyConstraintError(is_append, count, regular_matches);
556: 	bool transaction_error = false;
557: 	bool transaction_check = local_storage.Find(data_table);
558: 
559: 	if (transaction_check) {
560: 		auto &transact_index = local_storage.GetIndexes(data_table);
561: 		transact_index.VerifyForeignKey(*dst_keys_ptr, dst_chunk, transaction_conflicts);
562: 		transaction_conflicts.Finalize();
563: 		auto &transaction_matches = transaction_conflicts.Conflicts();
564: 		transaction_error = IsForeignKeyConstraintError(is_append, count, transaction_matches);
565: 	}
566: 
567: 	if (!transaction_error && !error) {
568: 		// No error occurred;
569: 		return;
570: 	}
571: 
572: 	// Some error occurred, and we likely want to throw
573: 	optional_ptr<Index> index;
574: 	optional_ptr<Index> transaction_index;
575: 
576: 	auto fk_type = is_append ? ForeignKeyType::FK_TYPE_PRIMARY_KEY_TABLE : ForeignKeyType::FK_TYPE_FOREIGN_KEY_TABLE;
577: 	// check whether or not the chunk can be inserted or deleted into the referenced table' storage
578: 	index = data_table.info->indexes.FindForeignKeyIndex(*dst_keys_ptr, fk_type);
579: 	if (transaction_check) {
580: 		auto &transact_index = local_storage.GetIndexes(data_table);
581: 		// check whether or not the chunk can be inserted or deleted into the referenced table' storage
582: 		transaction_index = transact_index.FindForeignKeyIndex(*dst_keys_ptr, fk_type);
583: 	}
584: 
585: 	if (!transaction_check) {
586: 		// Only local state is checked, throw the error
587: 		D_ASSERT(error);
588: 		auto failed_index = LocateErrorIndex(is_append, regular_matches);
589: 		D_ASSERT(failed_index != DConstants::INVALID_INDEX);
590: 		ThrowForeignKeyConstraintError(failed_index, is_append, *index, dst_chunk);
591: 	}
592: 	if (transaction_error && error && is_append) {
593: 		// When we want to do an append, we only throw if the foreign key does not exist in both transaction and local
594: 		// storage
595: 		auto &transaction_matches = transaction_conflicts.Conflicts();
596: 		idx_t failed_index = DConstants::INVALID_INDEX;
597: 		idx_t regular_idx = 0;
598: 		idx_t transaction_idx = 0;
599: 		for (idx_t i = 0; i < count; i++) {
600: 			bool in_regular = regular_matches.IndexMapsToLocation(regular_idx, i);
601: 			regular_idx += in_regular;
602: 			bool in_transaction = transaction_matches.IndexMapsToLocation(transaction_idx, i);
603: 			transaction_idx += in_transaction;
604: 
605: 			if (!in_regular && !in_transaction) {
606: 				// We need to find a match for all of the input values
607: 				// The failed index is i, it does not show up in either regular or transaction storage
608: 				failed_index = i;
609: 				break;
610: 			}
611: 		}
612: 		if (failed_index == DConstants::INVALID_INDEX) {
613: 			// We don't throw, every value was present in either regular or transaction storage
614: 			return;
615: 		}
616: 		ThrowForeignKeyConstraintError(failed_index, true, *index, dst_chunk);
617: 	}
618: 	if (!is_append) {
619: 		D_ASSERT(transaction_check);
620: 		auto &transaction_matches = transaction_conflicts.Conflicts();
621: 		if (error) {
622: 			auto failed_index = LocateErrorIndex(false, regular_matches);
623: 			D_ASSERT(failed_index != DConstants::INVALID_INDEX);
624: 			ThrowForeignKeyConstraintError(failed_index, false, *index, dst_chunk);
625: 		} else {
626: 			D_ASSERT(transaction_error);
627: 			D_ASSERT(transaction_matches.Count() != DConstants::INVALID_INDEX);
628: 			auto failed_index = LocateErrorIndex(false, transaction_matches);
629: 			D_ASSERT(failed_index != DConstants::INVALID_INDEX);
630: 			ThrowForeignKeyConstraintError(failed_index, false, *transaction_index, dst_chunk);
631: 		}
632: 	}
633: }
634: 
635: void DataTable::VerifyAppendForeignKeyConstraint(const BoundForeignKeyConstraint &bfk, ClientContext &context,
636:                                                  DataChunk &chunk) {
637: 	VerifyForeignKeyConstraint(bfk, context, chunk, VerifyExistenceType::APPEND_FK);
638: }
639: 
640: void DataTable::VerifyDeleteForeignKeyConstraint(const BoundForeignKeyConstraint &bfk, ClientContext &context,
641:                                                  DataChunk &chunk) {
642: 	VerifyForeignKeyConstraint(bfk, context, chunk, VerifyExistenceType::DELETE_FK);
643: }
644: 
645: void DataTable::VerifyNewConstraint(LocalStorage &local_storage, DataTable &parent, const BoundConstraint &constraint) {
646: 	if (constraint.type != ConstraintType::NOT_NULL) {
647: 		throw NotImplementedException("FIXME: ALTER COLUMN with such constraint is not supported yet");
648: 	}
649: 
650: 	parent.row_groups->VerifyNewConstraint(parent, constraint);
651: 	local_storage.VerifyNewConstraint(parent, constraint);
652: }
653: 
654: bool HasUniqueIndexes(TableIndexList &list) {
655: 	bool has_unique_index = false;
656: 	list.Scan([&](Index &index) {
657: 		if (index.IsUnique()) {
658: 			has_unique_index = true;
659: 			return true;
660: 		}
661: 		return false;
662: 	});
663: 	return has_unique_index;
664: }
665: 
666: void DataTable::VerifyUniqueIndexes(TableIndexList &indexes, ClientContext &context, DataChunk &chunk,
667:                                     optional_ptr<ConflictManager> conflict_manager) {
668: 	//! check whether or not the chunk can be inserted into the indexes
669: 	if (!conflict_manager) {
670: 		// Only need to verify that no unique constraints are violated
671: 		indexes.Scan([&](Index &index) {
672: 			if (!index.IsUnique()) {
673: 				return false;
674: 			}
675: 			D_ASSERT(index.IsBound());
676: 			index.Cast<BoundIndex>().VerifyAppend(chunk);
677: 			return false;
678: 		});
679: 		return;
680: 	}
681: 
682: 	D_ASSERT(conflict_manager);
683: 	// The conflict manager is only provided when a ON CONFLICT clause was provided to the INSERT statement
684: 
685: 	idx_t matching_indexes = 0;
686: 	auto &conflict_info = conflict_manager->GetConflictInfo();
687: 	// First we figure out how many indexes match our conflict target
688: 	// So we can optimize accordingly
689: 	indexes.Scan([&](Index &index) {
690: 		matching_indexes += conflict_info.ConflictTargetMatches(index);
691: 		return false;
692: 	});
693: 	conflict_manager->SetMode(ConflictManagerMode::SCAN);
694: 	conflict_manager->SetIndexCount(matching_indexes);
695: 	// First we verify only the indexes that match our conflict target
696: 	unordered_set<Index *> checked_indexes;
697: 	indexes.Scan([&](Index &index) {
698: 		if (!index.IsUnique()) {
699: 			return false;
700: 		}
701: 		if (conflict_info.ConflictTargetMatches(index)) {
702: 			D_ASSERT(index.IsBound());
703: 			index.Cast<BoundIndex>().VerifyAppend(chunk, *conflict_manager);
704: 			checked_indexes.insert(&index);
705: 		}
706: 		return false;
707: 	});
708: 
709: 	conflict_manager->SetMode(ConflictManagerMode::THROW);
710: 	// Then we scan the other indexes, throwing if they cause conflicts on tuples that were not found during
711: 	// the scan
712: 	indexes.Scan([&](Index &index) {
713: 		if (!index.IsUnique()) {
714: 			return false;
715: 		}
716: 		if (checked_indexes.count(&index)) {
717: 			// Already checked this constraint
718: 			return false;
719: 		}
720: 		D_ASSERT(index.IsBound());
721: 		index.Cast<BoundIndex>().VerifyAppend(chunk, *conflict_manager);
722: 		return false;
723: 	});
724: }
725: 
726: void DataTable::VerifyAppendConstraints(ConstraintState &state, ClientContext &context, DataChunk &chunk,
727:                                         optional_ptr<ConflictManager> conflict_manager) {
728: 	auto &table = state.table;
729: 	if (table.HasGeneratedColumns()) {
730: 		// Verify that the generated columns expression work with the inserted values
731: 		auto binder = Binder::CreateBinder(context);
732: 		physical_index_set_t bound_columns;
733: 		CheckBinder generated_check_binder(*binder, context, table.name, table.GetColumns(), bound_columns);
734: 		for (auto &col : table.GetColumns().Logical()) {
735: 			if (!col.Generated()) {
736: 				continue;
737: 			}
738: 			D_ASSERT(col.Type().id() != LogicalTypeId::ANY);
739: 			generated_check_binder.target_type = col.Type();
740: 			auto to_be_bound_expression = col.GeneratedExpression().Copy();
741: 			auto bound_expression = generated_check_binder.Bind(to_be_bound_expression);
742: 			VerifyGeneratedExpressionSuccess(context, table, chunk, *bound_expression, col.Oid());
743: 		}
744: 	}
745: 
746: 	if (HasUniqueIndexes(info->indexes)) {
747: 		VerifyUniqueIndexes(info->indexes, context, chunk, conflict_manager);
748: 	}
749: 
750: 	auto &constraints = table.GetConstraints();
751: 	for (idx_t i = 0; i < state.bound_constraints.size(); i++) {
752: 		auto &base_constraint = constraints[i];
753: 		auto &constraint = state.bound_constraints[i];
754: 		switch (base_constraint->type) {
755: 		case ConstraintType::NOT_NULL: {
756: 			auto &bound_not_null = *reinterpret_cast<BoundNotNullConstraint *>(constraint.get());
757: 			auto &not_null = *reinterpret_cast<NotNullConstraint *>(base_constraint.get());
758: 			auto &col = table.GetColumns().GetColumn(LogicalIndex(not_null.index));
759: 			VerifyNotNullConstraint(table, chunk.data[bound_not_null.index.index], chunk.size(), col.Name());
760: 			break;
761: 		}
762: 		case ConstraintType::CHECK: {
763: 			auto &check = *reinterpret_cast<BoundCheckConstraint *>(constraint.get());
764: 			VerifyCheckConstraint(context, table, *check.expression, chunk);
765: 			break;
766: 		}
767: 		case ConstraintType::UNIQUE: {
768: 			// These were handled earlier on
769: 			break;
770: 		}
771: 		case ConstraintType::FOREIGN_KEY: {
772: 			auto &bfk = *reinterpret_cast<BoundForeignKeyConstraint *>(constraint.get());
773: 			if (bfk.info.type == ForeignKeyType::FK_TYPE_FOREIGN_KEY_TABLE ||
774: 			    bfk.info.type == ForeignKeyType::FK_TYPE_SELF_REFERENCE_TABLE) {
775: 				VerifyAppendForeignKeyConstraint(bfk, context, chunk);
776: 			}
777: 			break;
778: 		}
779: 		default:
780: 			throw NotImplementedException("Constraint type not implemented!");
781: 		}
782: 	}
783: }
784: 
785: unique_ptr<ConstraintState>
786: DataTable::InitializeConstraintState(TableCatalogEntry &table,
787:                                      const vector<unique_ptr<BoundConstraint>> &bound_constraints) {
788: 	return make_uniq<ConstraintState>(table, bound_constraints);
789: }
790: 
791: void DataTable::InitializeLocalAppend(LocalAppendState &state, TableCatalogEntry &table, ClientContext &context,
792:                                       const vector<unique_ptr<BoundConstraint>> &bound_constraints) {
793: 	if (!is_root) {
794: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
795: 	}
796: 	auto &local_storage = LocalStorage::Get(context, db);
797: 	local_storage.InitializeAppend(state, *this);
798: 
799: 	state.constraint_state = InitializeConstraintState(table, bound_constraints);
800: }
801: 
802: void DataTable::LocalAppend(LocalAppendState &state, TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,
803:                             bool unsafe) {
804: 	if (chunk.size() == 0) {
805: 		return;
806: 	}
807: 	D_ASSERT(chunk.ColumnCount() == table.GetColumns().PhysicalColumnCount());
808: 	if (!is_root) {
809: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
810: 	}
811: 
812: 	chunk.Verify();
813: 
814: 	// verify any constraints on the new chunk
815: 	if (!unsafe) {
816: 		VerifyAppendConstraints(*state.constraint_state, context, chunk);
817: 	}
818: 
819: 	// append to the transaction local data
820: 	LocalStorage::Append(state, chunk);
821: }
822: 
823: void DataTable::FinalizeLocalAppend(LocalAppendState &state) {
824: 	LocalStorage::FinalizeAppend(state);
825: }
826: 
827: OptimisticDataWriter &DataTable::CreateOptimisticWriter(ClientContext &context) {
828: 	auto &local_storage = LocalStorage::Get(context, db);
829: 	return local_storage.CreateOptimisticWriter(*this);
830: }
831: 
832: void DataTable::FinalizeOptimisticWriter(ClientContext &context, OptimisticDataWriter &writer) {
833: 	auto &local_storage = LocalStorage::Get(context, db);
834: 	local_storage.FinalizeOptimisticWriter(*this, writer);
835: }
836: 
837: void DataTable::LocalMerge(ClientContext &context, RowGroupCollection &collection) {
838: 	auto &local_storage = LocalStorage::Get(context, db);
839: 	local_storage.LocalMerge(*this, collection);
840: }
841: 
842: void DataTable::LocalAppend(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,
843:                             const vector<unique_ptr<BoundConstraint>> &bound_constraints) {
844: 	LocalAppendState append_state;
845: 	auto &storage = table.GetStorage();
846: 	storage.InitializeLocalAppend(append_state, table, context, bound_constraints);
847: 	storage.LocalAppend(append_state, table, context, chunk);
848: 	storage.FinalizeLocalAppend(append_state);
849: }
850: 
851: void DataTable::LocalAppend(TableCatalogEntry &table, ClientContext &context, ColumnDataCollection &collection,
852:                             const vector<unique_ptr<BoundConstraint>> &bound_constraints) {
853: 	LocalAppendState append_state;
854: 	auto &storage = table.GetStorage();
855: 	storage.InitializeLocalAppend(append_state, table, context, bound_constraints);
856: 	for (auto &chunk : collection.Chunks()) {
857: 		storage.LocalAppend(append_state, table, context, chunk);
858: 	}
859: 	storage.FinalizeLocalAppend(append_state);
860: }
861: 
862: void DataTable::AppendLock(TableAppendState &state) {
863: 	state.append_lock = unique_lock<mutex>(append_lock);
864: 	if (!is_root) {
865: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
866: 	}
867: 	state.row_start = NumericCast<row_t>(row_groups->GetTotalRows());
868: 	state.current_row = state.row_start;
869: }
870: 
871: void DataTable::InitializeAppend(DuckTransaction &transaction, TableAppendState &state) {
872: 	// obtain the append lock for this table
873: 	if (!state.append_lock) {
874: 		throw InternalException("DataTable::AppendLock should be called before DataTable::InitializeAppend");
875: 	}
876: 	row_groups->InitializeAppend(transaction, state);
877: }
878: 
879: void DataTable::Append(DataChunk &chunk, TableAppendState &state) {
880: 	D_ASSERT(is_root);
881: 	row_groups->Append(chunk, state);
882: }
883: 
884: void DataTable::FinalizeAppend(DuckTransaction &transaction, TableAppendState &state) {
885: 	row_groups->FinalizeAppend(transaction, state);
886: }
887: 
888: void DataTable::ScanTableSegment(idx_t row_start, idx_t count, const std::function<void(DataChunk &chunk)> &function) {
889: 	if (count == 0) {
890: 		return;
891: 	}
892: 	idx_t end = row_start + count;
893: 
894: 	vector<column_t> column_ids;
895: 	vector<LogicalType> types;
896: 	for (idx_t i = 0; i < this->column_definitions.size(); i++) {
897: 		auto &col = this->column_definitions[i];
898: 		column_ids.push_back(i);
899: 		types.push_back(col.Type());
900: 	}
901: 	DataChunk chunk;
902: 	chunk.Initialize(Allocator::Get(db), types);
903: 
904: 	CreateIndexScanState state;
905: 
906: 	InitializeScanWithOffset(state, column_ids, row_start, row_start + count);
907: 	auto row_start_aligned = state.table_state.row_group->start + state.table_state.vector_index * STANDARD_VECTOR_SIZE;
908: 
909: 	idx_t current_row = row_start_aligned;
910: 	while (current_row < end) {
911: 		state.table_state.ScanCommitted(chunk, TableScanType::TABLE_SCAN_COMMITTED_ROWS);
912: 		if (chunk.size() == 0) {
913: 			break;
914: 		}
915: 		idx_t end_row = current_row + chunk.size();
916: 		// start of chunk is current_row
917: 		// end of chunk is end_row
918: 		// figure out if we need to write the entire chunk or just part of it
919: 		idx_t chunk_start = MaxValue<idx_t>(current_row, row_start);
920: 		idx_t chunk_end = MinValue<idx_t>(end_row, end);
921: 		D_ASSERT(chunk_start < chunk_end);
922: 		idx_t chunk_count = chunk_end - chunk_start;
923: 		if (chunk_count != chunk.size()) {
924: 			D_ASSERT(chunk_count <= chunk.size());
925: 			// need to slice the chunk before insert
926: 			idx_t start_in_chunk;
927: 			if (current_row >= row_start) {
928: 				start_in_chunk = 0;
929: 			} else {
930: 				start_in_chunk = row_start - current_row;
931: 			}
932: 			SelectionVector sel(start_in_chunk, chunk_count);
933: 			chunk.Slice(sel, chunk_count);
934: 			chunk.Verify();
935: 		}
936: 		function(chunk);
937: 		chunk.Reset();
938: 		current_row = end_row;
939: 	}
940: }
941: 
942: void DataTable::MergeStorage(RowGroupCollection &data, TableIndexList &indexes) {
943: 	row_groups->MergeStorage(data);
944: 	row_groups->Verify();
945: }
946: 
947: void DataTable::WriteToLog(WriteAheadLog &log, idx_t row_start, idx_t count) {
948: 	log.WriteSetTable(info->schema, info->table);
949: 	ScanTableSegment(row_start, count, [&](DataChunk &chunk) { log.WriteInsert(chunk); });
950: }
951: 
952: void DataTable::CommitAppend(transaction_t commit_id, idx_t row_start, idx_t count) {
953: 	lock_guard<mutex> lock(append_lock);
954: 	row_groups->CommitAppend(commit_id, row_start, count);
955: }
956: 
957: void DataTable::RevertAppendInternal(idx_t start_row) {
958: 	D_ASSERT(is_root);
959: 	// revert appends made to row_groups
960: 	row_groups->RevertAppendInternal(start_row);
961: }
962: 
963: void DataTable::RevertAppend(idx_t start_row, idx_t count) {
964: 	lock_guard<mutex> lock(append_lock);
965: 
966: 	// revert any appends to indexes
967: 	if (!info->indexes.Empty()) {
968: 		idx_t current_row_base = start_row;
969: 		row_t row_data[STANDARD_VECTOR_SIZE];
970: 		Vector row_identifiers(LogicalType::ROW_TYPE, data_ptr_cast(row_data));
971: 		idx_t scan_count = MinValue<idx_t>(count, row_groups->GetTotalRows() - start_row);
972: 		ScanTableSegment(start_row, scan_count, [&](DataChunk &chunk) {
973: 			for (idx_t i = 0; i < chunk.size(); i++) {
974: 				row_data[i] = NumericCast<row_t>(current_row_base + i);
975: 			}
976: 			info->indexes.Scan([&](Index &index) {
977: 				// We cant add to unbound indexes anyways, so there is no need to revert them
978: 				if (index.IsBound()) {
979: 					index.Cast<BoundIndex>().Delete(chunk, row_identifiers);
980: 				}
981: 				return false;
982: 			});
983: 			current_row_base += chunk.size();
984: 		});
985: 	}
986: 
987: 	// we need to vacuum the indexes to remove any buffers that are now empty
988: 	// due to reverting the appends
989: 	info->indexes.Scan([&](Index &index) {
990: 		// We cant add to unbound indexes anyway, so there is no need to vacuum them
991: 		if (index.IsBound()) {
992: 			index.Cast<BoundIndex>().Vacuum();
993: 		}
994: 		return false;
995: 	});
996: 
997: 	// revert the data table append
998: 	RevertAppendInternal(start_row);
999: }
1000: 
1001: //===--------------------------------------------------------------------===//
1002: // Indexes
1003: //===--------------------------------------------------------------------===//
1004: ErrorData DataTable::AppendToIndexes(TableIndexList &indexes, DataChunk &chunk, row_t row_start) {
1005: 	ErrorData error;
1006: 	if (indexes.Empty()) {
1007: 		return error;
1008: 	}
1009: 	// first generate the vector of row identifiers
1010: 	Vector row_identifiers(LogicalType::ROW_TYPE);
1011: 	VectorOperations::GenerateSequence(row_identifiers, chunk.size(), row_start, 1);
1012: 
1013: 	vector<BoundIndex *> already_appended;
1014: 	bool append_failed = false;
1015: 	// now append the entries to the indices
1016: 	indexes.Scan([&](Index &index_to_append) {
1017: 		if (!index_to_append.IsBound()) {
1018: 			error = ErrorData("Unbound index found in DataTable::AppendToIndexes");
1019: 			append_failed = true;
1020: 			return true;
1021: 		}
1022: 		auto &index = index_to_append.Cast<BoundIndex>();
1023: 		try {
1024: 			error = index.Append(chunk, row_identifiers);
1025: 		} catch (std::exception &ex) {
1026: 			error = ErrorData(ex);
1027: 		}
1028: 		if (error.HasError()) {
1029: 			append_failed = true;
1030: 			return true;
1031: 		}
1032: 		already_appended.push_back(&index);
1033: 		return false;
1034: 	});
1035: 
1036: 	if (append_failed) {
1037: 		// constraint violation!
1038: 		// remove any appended entries from previous indexes (if any)
1039: 		for (auto *index : already_appended) {
1040: 			index->Delete(chunk, row_identifiers);
1041: 		}
1042: 	}
1043: 	return error;
1044: }
1045: 
1046: ErrorData DataTable::AppendToIndexes(DataChunk &chunk, row_t row_start) {
1047: 	D_ASSERT(is_root);
1048: 	return AppendToIndexes(info->indexes, chunk, row_start);
1049: }
1050: 
1051: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
1052: 	D_ASSERT(is_root);
1053: 	if (info->indexes.Empty()) {
1054: 		return;
1055: 	}
1056: 	// first generate the vector of row identifiers
1057: 	Vector row_identifiers(LogicalType::ROW_TYPE);
1058: 	VectorOperations::GenerateSequence(row_identifiers, chunk.size(), row_start, 1);
1059: 
1060: 	// now remove the entries from the indices
1061: 	RemoveFromIndexes(state, chunk, row_identifiers);
1062: }
1063: 
1064: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, Vector &row_identifiers) {
1065: 	D_ASSERT(is_root);
1066: 	info->indexes.Scan([&](Index &index) {
1067: 		if (!index.IsBound()) {
1068: 			throw InternalException("Unbound index found in DataTable::RemoveFromIndexes");
1069: 		}
1070: 		index.Cast<BoundIndex>().Delete(chunk, row_identifiers);
1071: 		return false;
1072: 	});
1073: }
1074: 
1075: void DataTable::RemoveFromIndexes(Vector &row_identifiers, idx_t count) {
1076: 	D_ASSERT(is_root);
1077: 	row_groups->RemoveFromIndexes(info->indexes, row_identifiers, count);
1078: }
1079: 
1080: //===--------------------------------------------------------------------===//
1081: // Delete
1082: //===--------------------------------------------------------------------===//
1083: static bool TableHasDeleteConstraints(TableCatalogEntry &table) {
1084: 	for (auto &constraint : table.GetConstraints()) {
1085: 		switch (constraint->type) {
1086: 		case ConstraintType::NOT_NULL:
1087: 		case ConstraintType::CHECK:
1088: 		case ConstraintType::UNIQUE:
1089: 			break;
1090: 		case ConstraintType::FOREIGN_KEY: {
1091: 			auto &bfk = constraint->Cast<ForeignKeyConstraint>();
1092: 			if (bfk.info.type == ForeignKeyType::FK_TYPE_PRIMARY_KEY_TABLE ||
1093: 			    bfk.info.type == ForeignKeyType::FK_TYPE_SELF_REFERENCE_TABLE) {
1094: 				return true;
1095: 			}
1096: 			break;
1097: 		}
1098: 		default:
1099: 			throw NotImplementedException("Constraint type not implemented!");
1100: 		}
1101: 	}
1102: 	return false;
1103: }
1104: 
1105: void DataTable::VerifyDeleteConstraints(TableDeleteState &state, ClientContext &context, DataChunk &chunk) {
1106: 	for (auto &constraint : state.constraint_state->bound_constraints) {
1107: 		switch (constraint->type) {
1108: 		case ConstraintType::NOT_NULL:
1109: 		case ConstraintType::CHECK:
1110: 		case ConstraintType::UNIQUE:
1111: 			break;
1112: 		case ConstraintType::FOREIGN_KEY: {
1113: 			auto &bfk = *reinterpret_cast<BoundForeignKeyConstraint *>(constraint.get());
1114: 			if (bfk.info.type == ForeignKeyType::FK_TYPE_PRIMARY_KEY_TABLE ||
1115: 			    bfk.info.type == ForeignKeyType::FK_TYPE_SELF_REFERENCE_TABLE) {
1116: 				VerifyDeleteForeignKeyConstraint(bfk, context, chunk);
1117: 			}
1118: 			break;
1119: 		}
1120: 		default:
1121: 			throw NotImplementedException("Constraint type not implemented!");
1122: 		}
1123: 	}
1124: }
1125: 
1126: unique_ptr<TableDeleteState> DataTable::InitializeDelete(TableCatalogEntry &table, ClientContext &context,
1127:                                                          const vector<unique_ptr<BoundConstraint>> &bound_constraints) {
1128: 	// initialize indexes (if any)
1129: 	info->InitializeIndexes(context);
1130: 
1131: 	auto binder = Binder::CreateBinder(context);
1132: 	vector<LogicalType> types;
1133: 	auto result = make_uniq<TableDeleteState>();
1134: 	result->has_delete_constraints = TableHasDeleteConstraints(table);
1135: 	if (result->has_delete_constraints) {
1136: 		// initialize the chunk if there are any constraints to verify
1137: 		for (idx_t i = 0; i < column_definitions.size(); i++) {
1138: 			result->col_ids.push_back(column_definitions[i].StorageOid());
1139: 			types.emplace_back(column_definitions[i].Type());
1140: 		}
1141: 		result->verify_chunk.Initialize(Allocator::Get(context), types);
1142: 		result->constraint_state = make_uniq<ConstraintState>(table, bound_constraints);
1143: 	}
1144: 	return result;
1145: }
1146: 
1147: idx_t DataTable::Delete(TableDeleteState &state, ClientContext &context, Vector &row_identifiers, idx_t count) {
1148: 	D_ASSERT(row_identifiers.GetType().InternalType() == ROW_TYPE);
1149: 	if (count == 0) {
1150: 		return 0;
1151: 	}
1152: 
1153: 	auto &transaction = DuckTransaction::Get(context, db);
1154: 	auto &local_storage = LocalStorage::Get(transaction);
1155: 
1156: 	row_identifiers.Flatten(count);
1157: 	auto ids = FlatVector::GetData<row_t>(row_identifiers);
1158: 
1159: 	idx_t pos = 0;
1160: 	idx_t delete_count = 0;
1161: 	while (pos < count) {
1162: 		idx_t start = pos;
1163: 		bool is_transaction_delete = ids[pos] >= MAX_ROW_ID;
1164: 		// figure out which batch of rows to delete now
1165: 		for (pos++; pos < count; pos++) {
1166: 			bool row_is_transaction_delete = ids[pos] >= MAX_ROW_ID;
1167: 			if (row_is_transaction_delete != is_transaction_delete) {
1168: 				break;
1169: 			}
1170: 		}
1171: 		idx_t current_offset = start;
1172: 		idx_t current_count = pos - start;
1173: 
1174: 		Vector offset_ids(row_identifiers, current_offset, pos);
1175: 		if (is_transaction_delete) {
1176: 			// transaction-local delete
1177: 			if (state.has_delete_constraints) {
1178: 				// perform the constraint verification
1179: 				ColumnFetchState fetch_state;
1180: 				local_storage.FetchChunk(*this, offset_ids, current_count, state.col_ids, state.verify_chunk,
1181: 				                         fetch_state);
1182: 				VerifyDeleteConstraints(state, context, state.verify_chunk);
1183: 			}
1184: 			delete_count += local_storage.Delete(*this, offset_ids, current_count);
1185: 		} else {
1186: 			// regular table delete
1187: 			if (state.has_delete_constraints) {
1188: 				// perform the constraint verification
1189: 				ColumnFetchState fetch_state;
1190: 				Fetch(transaction, state.verify_chunk, state.col_ids, offset_ids, current_count, fetch_state);
1191: 				VerifyDeleteConstraints(state, context, state.verify_chunk);
1192: 			}
1193: 			delete_count += row_groups->Delete(transaction, *this, ids + current_offset, current_count);
1194: 		}
1195: 	}
1196: 	return delete_count;
1197: }
1198: 
1199: //===--------------------------------------------------------------------===//
1200: // Update
1201: //===--------------------------------------------------------------------===//
1202: static void CreateMockChunk(vector<LogicalType> &types, const vector<PhysicalIndex> &column_ids, DataChunk &chunk,
1203:                             DataChunk &mock_chunk) {
1204: 	// construct a mock DataChunk
1205: 	mock_chunk.InitializeEmpty(types);
1206: 	for (column_t i = 0; i < column_ids.size(); i++) {
1207: 		mock_chunk.data[column_ids[i].index].Reference(chunk.data[i]);
1208: 	}
1209: 	mock_chunk.SetCardinality(chunk.size());
1210: }
1211: 
1212: static bool CreateMockChunk(TableCatalogEntry &table, const vector<PhysicalIndex> &column_ids,
1213:                             physical_index_set_t &desired_column_ids, DataChunk &chunk, DataChunk &mock_chunk) {
1214: 	idx_t found_columns = 0;
1215: 	// check whether the desired columns are present in the UPDATE clause
1216: 	for (column_t i = 0; i < column_ids.size(); i++) {
1217: 		if (desired_column_ids.find(column_ids[i]) != desired_column_ids.end()) {
1218: 			found_columns++;
1219: 		}
1220: 	}
1221: 	if (found_columns == 0) {
1222: 		// no columns were found: no need to check the constraint again
1223: 		return false;
1224: 	}
1225: 	if (found_columns != desired_column_ids.size()) {
1226: 		// not all columns in UPDATE clause are present!
1227: 		// this should not be triggered at all as the binder should add these columns
1228: 		throw InternalException("Not all columns required for the CHECK constraint are present in the UPDATED chunk!");
1229: 	}
1230: 	// construct a mock DataChunk
1231: 	auto types = table.GetTypes();
1232: 	CreateMockChunk(types, column_ids, chunk, mock_chunk);
1233: 	return true;
1234: }
1235: 
1236: void DataTable::VerifyUpdateConstraints(ConstraintState &state, ClientContext &context, DataChunk &chunk,
1237:                                         const vector<PhysicalIndex> &column_ids) {
1238: 	auto &table = state.table;
1239: 	auto &constraints = table.GetConstraints();
1240: 	auto &bound_constraints = state.bound_constraints;
1241: 	for (idx_t constr_idx = 0; constr_idx < bound_constraints.size(); constr_idx++) {
1242: 		auto &base_constraint = constraints[constr_idx];
1243: 		auto &constraint = bound_constraints[constr_idx];
1244: 		switch (constraint->type) {
1245: 		case ConstraintType::NOT_NULL: {
1246: 			auto &bound_not_null = *reinterpret_cast<BoundNotNullConstraint *>(constraint.get());
1247: 			auto &not_null = *reinterpret_cast<NotNullConstraint *>(base_constraint.get());
1248: 			// check if the constraint is in the list of column_ids
1249: 			for (idx_t col_idx = 0; col_idx < column_ids.size(); col_idx++) {
1250: 				if (column_ids[col_idx] == bound_not_null.index) {
1251: 					// found the column id: check the data in
1252: 					auto &col = table.GetColumn(LogicalIndex(not_null.index));
1253: 					VerifyNotNullConstraint(table, chunk.data[col_idx], chunk.size(), col.Name());
1254: 					break;
1255: 				}
1256: 			}
1257: 			break;
1258: 		}
1259: 		case ConstraintType::CHECK: {
1260: 			auto &check = *reinterpret_cast<BoundCheckConstraint *>(constraint.get());
1261: 
1262: 			DataChunk mock_chunk;
1263: 			if (CreateMockChunk(table, column_ids, check.bound_columns, chunk, mock_chunk)) {
1264: 				VerifyCheckConstraint(context, table, *check.expression, mock_chunk);
1265: 			}
1266: 			break;
1267: 		}
1268: 		case ConstraintType::UNIQUE:
1269: 		case ConstraintType::FOREIGN_KEY:
1270: 			break;
1271: 		default:
1272: 			throw NotImplementedException("Constraint type not implemented!");
1273: 		}
1274: 	}
1275: 	// update should not be called for indexed columns!
1276: 	// instead update should have been rewritten to delete + update on higher layer
1277: #ifdef DEBUG
1278: 	info->indexes.Scan([&](Index &index) {
1279: 		D_ASSERT(index.IsBound());
1280: 		D_ASSERT(!index.Cast<BoundIndex>().IndexIsUpdated(column_ids));
1281: 		return false;
1282: 	});
1283: 
1284: #endif
1285: }
1286: 
1287: unique_ptr<TableUpdateState> DataTable::InitializeUpdate(TableCatalogEntry &table, ClientContext &context,
1288:                                                          const vector<unique_ptr<BoundConstraint>> &bound_constraints) {
1289: 	// check that there are no unknown indexes
1290: 	info->InitializeIndexes(context);
1291: 
1292: 	auto result = make_uniq<TableUpdateState>();
1293: 	result->constraint_state = InitializeConstraintState(table, bound_constraints);
1294: 	return result;
1295: }
1296: 
1297: void DataTable::Update(TableUpdateState &state, ClientContext &context, Vector &row_ids,
1298:                        const vector<PhysicalIndex> &column_ids, DataChunk &updates) {
1299: 	D_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);
1300: 	D_ASSERT(column_ids.size() == updates.ColumnCount());
1301: 	updates.Verify();
1302: 
1303: 	auto count = updates.size();
1304: 	if (count == 0) {
1305: 		return;
1306: 	}
1307: 
1308: 	if (!is_root) {
1309: 		throw TransactionException("Transaction conflict: cannot update a table that has been altered!");
1310: 	}
1311: 
1312: 	// first verify that no constraints are violated
1313: 	VerifyUpdateConstraints(*state.constraint_state, context, updates, column_ids);
1314: 
1315: 	// now perform the actual update
1316: 	Vector max_row_id_vec(Value::BIGINT(MAX_ROW_ID));
1317: 	Vector row_ids_slice(LogicalType::BIGINT);
1318: 	DataChunk updates_slice;
1319: 	updates_slice.InitializeEmpty(updates.GetTypes());
1320: 
1321: 	SelectionVector sel_local_update(count), sel_global_update(count);
1322: 	auto n_local_update = VectorOperations::GreaterThanEquals(row_ids, max_row_id_vec, nullptr, count,
1323: 	                                                          &sel_local_update, &sel_global_update);
1324: 	auto n_global_update = count - n_local_update;
1325: 
1326: 	// row id > MAX_ROW_ID? transaction-local storage
1327: 	if (n_local_update > 0) {
1328: 		updates_slice.Slice(updates, sel_local_update, n_local_update);
1329: 		updates_slice.Flatten();
1330: 		row_ids_slice.Slice(row_ids, sel_local_update, n_local_update);
1331: 		row_ids_slice.Flatten(n_local_update);
1332: 
1333: 		LocalStorage::Get(context, db).Update(*this, row_ids_slice, column_ids, updates_slice);
1334: 	}
1335: 
1336: 	// otherwise global storage
1337: 	if (n_global_update > 0) {
1338: 		updates_slice.Slice(updates, sel_global_update, n_global_update);
1339: 		updates_slice.Flatten();
1340: 		row_ids_slice.Slice(row_ids, sel_global_update, n_global_update);
1341: 		row_ids_slice.Flatten(n_global_update);
1342: 
1343: 		row_groups->Update(DuckTransaction::Get(context, db), FlatVector::GetData<row_t>(row_ids_slice), column_ids,
1344: 		                   updates_slice);
1345: 	}
1346: }
1347: 
1348: void DataTable::UpdateColumn(TableCatalogEntry &table, ClientContext &context, Vector &row_ids,
1349:                              const vector<column_t> &column_path, DataChunk &updates) {
1350: 	D_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);
1351: 	D_ASSERT(updates.ColumnCount() == 1);
1352: 	updates.Verify();
1353: 	if (updates.size() == 0) {
1354: 		return;
1355: 	}
1356: 
1357: 	if (!is_root) {
1358: 		throw TransactionException("Transaction conflict: cannot update a table that has been altered!");
1359: 	}
1360: 
1361: 	// now perform the actual update
1362: 	auto &transaction = DuckTransaction::Get(context, db);
1363: 
1364: 	updates.Flatten();
1365: 	row_ids.Flatten(updates.size());
1366: 	row_groups->UpdateColumn(transaction, row_ids, column_path, updates);
1367: }
1368: 
1369: //===--------------------------------------------------------------------===//
1370: // Statistics
1371: //===--------------------------------------------------------------------===//
1372: unique_ptr<BaseStatistics> DataTable::GetStatistics(ClientContext &context, column_t column_id) {
1373: 	if (column_id == COLUMN_IDENTIFIER_ROW_ID) {
1374: 		return nullptr;
1375: 	}
1376: 	return row_groups->CopyStats(column_id);
1377: }
1378: 
1379: void DataTable::SetDistinct(column_t column_id, unique_ptr<DistinctStatistics> distinct_stats) {
1380: 	D_ASSERT(column_id != COLUMN_IDENTIFIER_ROW_ID);
1381: 	row_groups->SetDistinct(column_id, std::move(distinct_stats));
1382: }
1383: 
1384: //===--------------------------------------------------------------------===//
1385: // Checkpoint
1386: //===--------------------------------------------------------------------===//
1387: unique_ptr<StorageLockKey> DataTable::GetSharedCheckpointLock() {
1388: 	return info->checkpoint_lock.GetSharedLock();
1389: }
1390: 
1391: unique_ptr<StorageLockKey> DataTable::GetCheckpointLock() {
1392: 	return info->checkpoint_lock.GetExclusiveLock();
1393: }
1394: 
1395: void DataTable::Checkpoint(TableDataWriter &writer, Serializer &serializer) {
1396: 	// checkpoint each individual row group
1397: 	TableStatistics global_stats;
1398: 	row_groups->CopyStats(global_stats);
1399: 	row_groups->Checkpoint(writer, global_stats);
1400: 
1401: 	// The row group payload data has been written. Now write:
1402: 	//   column stats
1403: 	//   row-group pointers
1404: 	//   table pointer
1405: 	//   index data
1406: 	writer.FinalizeTable(global_stats, info.get(), serializer);
1407: }
1408: 
1409: void DataTable::CommitDropColumn(idx_t index) {
1410: 	row_groups->CommitDropColumn(index);
1411: }
1412: 
1413: idx_t DataTable::ColumnCount() const {
1414: 	return column_definitions.size();
1415: }
1416: 
1417: idx_t DataTable::GetTotalRows() const {
1418: 	return row_groups->GetTotalRows();
1419: }
1420: 
1421: void DataTable::CommitDropTable() {
1422: 	// commit a drop of this table: mark all blocks as modified, so they can be reclaimed later on
1423: 	row_groups->CommitDropTable();
1424: 
1425: 	// propagate dropping this table to its indexes: frees all index memory
1426: 	info->indexes.Scan([&](Index &index) {
1427: 		D_ASSERT(index.IsBound());
1428: 		index.Cast<BoundIndex>().CommitDrop();
1429: 		return false;
1430: 	});
1431: }
1432: 
1433: //===--------------------------------------------------------------------===//
1434: // GetColumnSegmentInfo
1435: //===--------------------------------------------------------------------===//
1436: vector<ColumnSegmentInfo> DataTable::GetColumnSegmentInfo() {
1437: 	auto lock = GetSharedCheckpointLock();
1438: 	return row_groups->GetColumnSegmentInfo();
1439: }
1440: 
1441: } // namespace duckdb
[end of src/storage/data_table.cpp]
[start of src/transaction/duck_transaction.cpp]
1: #include "duckdb/transaction/duck_transaction.hpp"
2: #include "duckdb/transaction/duck_transaction_manager.hpp"
3: #include "duckdb/main/client_context.hpp"
4: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/parser/column_definition.hpp"
7: #include "duckdb/storage/data_table.hpp"
8: #include "duckdb/storage/write_ahead_log.hpp"
9: #include "duckdb/storage/storage_manager.hpp"
10: 
11: #include "duckdb/transaction/append_info.hpp"
12: #include "duckdb/transaction/delete_info.hpp"
13: #include "duckdb/transaction/update_info.hpp"
14: #include "duckdb/transaction/local_storage.hpp"
15: #include "duckdb/main/config.hpp"
16: #include "duckdb/storage/table/column_data.hpp"
17: #include "duckdb/main/client_data.hpp"
18: #include "duckdb/main/attached_database.hpp"
19: #include "duckdb/storage/storage_lock.hpp"
20: 
21: namespace duckdb {
22: 
23: TransactionData::TransactionData(DuckTransaction &transaction_p) // NOLINT
24:     : transaction(&transaction_p), transaction_id(transaction_p.transaction_id), start_time(transaction_p.start_time) {
25: }
26: TransactionData::TransactionData(transaction_t transaction_id_p, transaction_t start_time_p)
27:     : transaction(nullptr), transaction_id(transaction_id_p), start_time(start_time_p) {
28: }
29: 
30: DuckTransaction::DuckTransaction(DuckTransactionManager &manager, ClientContext &context_p, transaction_t start_time,
31:                                  transaction_t transaction_id, idx_t catalog_version_p)
32:     : Transaction(manager, context_p), start_time(start_time), transaction_id(transaction_id), commit_id(0),
33:       highest_active_query(0), catalog_version(catalog_version_p), transaction_manager(manager), undo_buffer(context_p),
34:       storage(make_uniq<LocalStorage>(context_p, *this)) {
35: }
36: 
37: DuckTransaction::~DuckTransaction() {
38: }
39: 
40: DuckTransaction &DuckTransaction::Get(ClientContext &context, AttachedDatabase &db) {
41: 	return DuckTransaction::Get(context, db.GetCatalog());
42: }
43: 
44: DuckTransaction &DuckTransaction::Get(ClientContext &context, Catalog &catalog) {
45: 	auto &transaction = Transaction::Get(context, catalog);
46: 	if (!transaction.IsDuckTransaction()) {
47: 		throw InternalException("DuckTransaction::Get called on non-DuckDB transaction");
48: 	}
49: 	return transaction.Cast<DuckTransaction>();
50: }
51: 
52: LocalStorage &DuckTransaction::GetLocalStorage() {
53: 	return *storage;
54: }
55: 
56: void DuckTransaction::PushCatalogEntry(CatalogEntry &entry, data_ptr_t extra_data, idx_t extra_data_size) {
57: 	idx_t alloc_size = sizeof(CatalogEntry *);
58: 	if (extra_data_size > 0) {
59: 		alloc_size += extra_data_size + sizeof(idx_t);
60: 	}
61: 
62: 	auto baseptr = undo_buffer.CreateEntry(UndoFlags::CATALOG_ENTRY, alloc_size);
63: 	// store the pointer to the catalog entry
64: 	Store<CatalogEntry *>(&entry, baseptr);
65: 	if (extra_data_size > 0) {
66: 		// copy the extra data behind the catalog entry pointer (if any)
67: 		baseptr += sizeof(CatalogEntry *);
68: 		// first store the extra data size
69: 		Store<idx_t>(extra_data_size, baseptr);
70: 		baseptr += sizeof(idx_t);
71: 		// then copy over the actual data
72: 		memcpy(baseptr, extra_data, extra_data_size);
73: 	}
74: }
75: 
76: void DuckTransaction::PushDelete(DataTable &table, RowVersionManager &info, idx_t vector_idx, row_t rows[], idx_t count,
77:                                  idx_t base_row) {
78: 	bool is_consecutive = true;
79: 	// check if the rows are consecutive
80: 	for (idx_t i = 0; i < count; i++) {
81: 		if (rows[i] != row_t(i)) {
82: 			is_consecutive = false;
83: 			break;
84: 		}
85: 	}
86: 	idx_t alloc_size = sizeof(DeleteInfo);
87: 	if (!is_consecutive) {
88: 		// if rows are not consecutive we need to allocate row identifiers
89: 		alloc_size += sizeof(uint16_t) * count;
90: 	}
91: 
92: 	auto delete_info = reinterpret_cast<DeleteInfo *>(undo_buffer.CreateEntry(UndoFlags::DELETE_TUPLE, alloc_size));
93: 	delete_info->version_info = &info;
94: 	delete_info->vector_idx = vector_idx;
95: 	delete_info->table = &table;
96: 	delete_info->count = count;
97: 	delete_info->base_row = base_row;
98: 	delete_info->is_consecutive = is_consecutive;
99: 	if (!is_consecutive) {
100: 		// if rows are not consecutive
101: 		auto delete_rows = delete_info->GetRows();
102: 		for (idx_t i = 0; i < count; i++) {
103: 			delete_rows[i] = NumericCast<uint16_t>(rows[i]);
104: 		}
105: 	}
106: }
107: 
108: void DuckTransaction::PushAppend(DataTable &table, idx_t start_row, idx_t row_count) {
109: 	auto append_info =
110: 	    reinterpret_cast<AppendInfo *>(undo_buffer.CreateEntry(UndoFlags::INSERT_TUPLE, sizeof(AppendInfo)));
111: 	append_info->table = &table;
112: 	append_info->start_row = start_row;
113: 	append_info->count = row_count;
114: }
115: 
116: UpdateInfo *DuckTransaction::CreateUpdateInfo(idx_t type_size, idx_t entries) {
117: 	data_ptr_t base_info = undo_buffer.CreateEntry(
118: 	    UndoFlags::UPDATE_TUPLE, sizeof(UpdateInfo) + (sizeof(sel_t) + type_size) * STANDARD_VECTOR_SIZE);
119: 	auto update_info = reinterpret_cast<UpdateInfo *>(base_info);
120: 	update_info->max = STANDARD_VECTOR_SIZE;
121: 	update_info->tuples = reinterpret_cast<sel_t *>(base_info + sizeof(UpdateInfo));
122: 	update_info->tuple_data = base_info + sizeof(UpdateInfo) + sizeof(sel_t) * update_info->max;
123: 	update_info->version_number = transaction_id;
124: 	return update_info;
125: }
126: 
127: void DuckTransaction::PushSequenceUsage(SequenceCatalogEntry &sequence, const SequenceData &data) {
128: 	lock_guard<mutex> l(sequence_lock);
129: 	auto entry = sequence_usage.find(sequence);
130: 	if (entry == sequence_usage.end()) {
131: 		auto sequence_ptr = undo_buffer.CreateEntry(UndoFlags::SEQUENCE_VALUE, sizeof(SequenceValue));
132: 		auto sequence_info = reinterpret_cast<SequenceValue *>(sequence_ptr);
133: 		sequence_info->entry = &sequence;
134: 		sequence_info->usage_count = data.usage_count;
135: 		sequence_info->counter = data.counter;
136: 		sequence_usage.emplace(sequence, *sequence_info);
137: 	} else {
138: 		auto &sequence_info = entry->second.get();
139: 		D_ASSERT(RefersToSameObject(*sequence_info.entry, sequence));
140: 		sequence_info.usage_count = data.usage_count;
141: 		sequence_info.counter = data.counter;
142: 	}
143: }
144: 
145: bool DuckTransaction::ChangesMade() {
146: 	return undo_buffer.ChangesMade() || storage->ChangesMade();
147: }
148: 
149: UndoBufferProperties DuckTransaction::GetUndoProperties() {
150: 	return undo_buffer.GetProperties();
151: }
152: 
153: bool DuckTransaction::AutomaticCheckpoint(AttachedDatabase &db, const UndoBufferProperties &properties) {
154: 	if (!ChangesMade()) {
155: 		// read-only transactions cannot trigger an automated checkpoint
156: 		return false;
157: 	}
158: 	if (db.IsReadOnly()) {
159: 		// when attaching a database in read-only mode we cannot checkpoint
160: 		// note that attaching a database in read-only mode does NOT mean we never make changes
161: 		// WAL replay can make changes to the database - but only in the in-memory copy of the
162: 		return false;
163: 	}
164: 	auto &storage_manager = db.GetStorageManager();
165: 	return storage_manager.AutomaticCheckpoint(storage->EstimatedSize() + properties.estimated_size);
166: }
167: 
168: bool DuckTransaction::ShouldWriteToWAL(AttachedDatabase &db) {
169: 	if (!ChangesMade()) {
170: 		return false;
171: 	}
172: 	if (db.IsSystem()) {
173: 		return false;
174: 	}
175: 	auto &storage_manager = db.GetStorageManager();
176: 	auto log = storage_manager.GetWAL();
177: 	if (!log) {
178: 		return false;
179: 	}
180: 	return true;
181: }
182: 
183: ErrorData DuckTransaction::WriteToWAL(AttachedDatabase &db, unique_ptr<StorageCommitState> &commit_state) noexcept {
184: 	try {
185: 		D_ASSERT(ShouldWriteToWAL(db));
186: 		auto &storage_manager = db.GetStorageManager();
187: 		auto log = storage_manager.GetWAL();
188: 		storage->Commit();
189: 		commit_state = storage_manager.GenStorageCommitState(*log);
190: 		undo_buffer.WriteToWAL(*log);
191: 	} catch (std::exception &ex) {
192: 		if (commit_state) {
193: 			commit_state->RevertCommit();
194: 			commit_state.reset();
195: 		}
196: 		return ErrorData(ex);
197: 	}
198: 	return ErrorData();
199: }
200: 
201: ErrorData DuckTransaction::Commit(AttachedDatabase &db, transaction_t new_commit_id,
202:                                   unique_ptr<StorageCommitState> commit_state) noexcept {
203: 	// "checkpoint" parameter indicates if the caller will checkpoint. If checkpoint ==
204: 	//    true: Then this function will NOT write to the WAL or flush/persist.
205: 	//          This method only makes commit in memory, expecting caller to checkpoint/flush.
206: 	//    false: Then this function WILL write to the WAL and Flush/Persist it.
207: 	this->commit_id = new_commit_id;
208: 	if (!ChangesMade()) {
209: 		// no need to flush anything if we made no changes
210: 		return ErrorData();
211: 	}
212: 	D_ASSERT(db.IsSystem() || db.IsTemporary() || !IsReadOnly());
213: 
214: 	UndoBuffer::IteratorState iterator_state;
215: 	try {
216: 		storage->Commit();
217: 		undo_buffer.Commit(iterator_state, commit_id);
218: 		if (commit_state) {
219: 			// if we have written to the WAL - flush after the commit has been successful
220: 			commit_state->FlushCommit();
221: 		}
222: 		return ErrorData();
223: 	} catch (std::exception &ex) {
224: 		undo_buffer.RevertCommit(iterator_state, this->transaction_id);
225: 		if (commit_state) {
226: 			// if we have written to the WAL - truncate the WAL on failure
227: 			commit_state->RevertCommit();
228: 		}
229: 		return ErrorData(ex);
230: 	}
231: }
232: 
233: void DuckTransaction::Rollback() noexcept {
234: 	storage->Rollback();
235: 	undo_buffer.Rollback();
236: }
237: 
238: void DuckTransaction::Cleanup(transaction_t lowest_active_transaction) {
239: 	undo_buffer.Cleanup(lowest_active_transaction);
240: }
241: 
242: void DuckTransaction::SetReadWrite() {
243: 	Transaction::SetReadWrite();
244: 	// obtain a shared checkpoint lock to prevent concurrent checkpoints while this transaction is running
245: 	write_lock = transaction_manager.SharedCheckpointLock();
246: }
247: 
248: unique_ptr<StorageLockKey> DuckTransaction::TryGetCheckpointLock() {
249: 	if (!write_lock) {
250: 		throw InternalException("TryUpgradeCheckpointLock - but thread has no shared lock!?");
251: 	}
252: 	return transaction_manager.TryUpgradeCheckpointLock(*write_lock);
253: }
254: 
255: } // namespace duckdb
[end of src/transaction/duck_transaction.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: