{
  "repo": "duckdb/duckdb",
  "pull_number": 17031,
  "instance_id": "duckdb__duckdb-17031",
  "issue_numbers": [
    "17029"
  ],
  "base_commit": "7455c078f0834d870c3795fa3797ab96ef91ebb5",
  "patch": "diff --git a/extension/parquet/column_reader.cpp b/extension/parquet/column_reader.cpp\nindex aaa27bbb0879..6118653fcd80 100644\n--- a/extension/parquet/column_reader.cpp\n+++ b/extension/parquet/column_reader.cpp\n@@ -318,12 +318,14 @@ void ColumnReader::PreparePageV2(PageHeader &page_hdr) {\n \n \tauto compressed_bytes = page_hdr.compressed_page_size - uncompressed_bytes;\n \n-\tResizeableBuffer compressed_buffer;\n-\tcompressed_buffer.resize(GetAllocator(), compressed_bytes);\n-\treader.ReadData(*protocol, compressed_buffer.ptr, compressed_bytes);\n+\tif (compressed_bytes > 0) {\n+\t\tResizeableBuffer compressed_buffer;\n+\t\tcompressed_buffer.resize(GetAllocator(), compressed_bytes);\n+\t\treader.ReadData(*protocol, compressed_buffer.ptr, compressed_bytes);\n \n-\tDecompressInternal(chunk->meta_data.codec, compressed_buffer.ptr, compressed_bytes, block->ptr + uncompressed_bytes,\n-\t                   page_hdr.uncompressed_page_size - uncompressed_bytes);\n+\t\tDecompressInternal(chunk->meta_data.codec, compressed_buffer.ptr, compressed_bytes,\n+\t\t                   block->ptr + uncompressed_bytes, page_hdr.uncompressed_page_size - uncompressed_bytes);\n+\t}\n }\n \n void ColumnReader::AllocateBlock(idx_t size) {\n",
  "test_patch": "diff --git a/data/parquet-testing/compression/empty_datapage_v2.snappy.parquet b/data/parquet-testing/compression/empty_datapage_v2.snappy.parquet\nnew file mode 100644\nindex 000000000000..30d6fa7a687b\nBinary files /dev/null and b/data/parquet-testing/compression/empty_datapage_v2.snappy.parquet differ\ndiff --git a/test/parquet/test_parquet_reader_compression.test b/test/parquet/test_parquet_reader_compression.test\nindex b33f1596608e..dd2d146a5842 100644\n--- a/test/parquet/test_parquet_reader_compression.test\n+++ b/test/parquet/test_parquet_reader_compression.test\n@@ -78,4 +78,9 @@ SELECT * FROM parquet_scan('data/parquet-testing/compression/generated/data_page\n 28\t7\t{'string': foo, 'int': 34}\t[20, 1, 18, 20, 1, 3, 25, 2, 31, 22, NULL, 40, 23, 32, 40, 10]\n 29\t13\t{'string': bar, 'int': 8}\t[40, 32, 9, 2, 2, 40, 7, 0, 32, 31, 11, 14, 4, 14, 40, 20, 29, 17, 41]\n \n+query I\n+SELECT * FROM parquet_scan('data/parquet-testing/compression/empty_datapage_v2.snappy.parquet', hive_partitioning=0) limit 50\n+----\n+NULL\n+\n endloop\n\\ No newline at end of file\n",
  "problem_statement": "Handle Parquet with compressed empty DataPage v2\n### What happens?\n\nAn empty bytes buffer cannot be decompressed. Spark's Parquet writer stores a DataPage v2 with only `null` values as an empty byte buffer, rather than compressed bytes that decompress to zero bytes.\n\nThe code currently tries to decompress a 0 bytes buffer, which is not allowed. This causes an error:\n\n    Invalid Error: Snappy decompression failure\n\nThe issue is identical to this Apache Arrow issue: https://github.com/apache/arrow/issues/22459\nThe fix is identical to Apache Arrow fix: https://github.com/apache/arrow/pull/45252\n\n### To Reproduce\n\n```bash\n./spark-3.5.5-bin-hadoop3/bin/spark-shell --conf spark.hadoop.parquet.writer.version=\"v2\"\nSpark session available as 'spark'.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.5\n      /_/\n         \nUsing Scala version 2.12.18 (OpenJDK 64-Bit Server VM, Java 11.0.26)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> Seq(Option.empty[Float]).toDS.write.parquet(\"parquet-v2-example.parquet\")\n```\n\nDuckDB:\n```sql\nSELECT * FROM parquet_scan('parquet-v2-example.parquet/*.parquet', hive_partitioning=0) limit 50;\n```\n```\nInvalid Error: Snappy decompression failure\n```\n\n### OS:\n\nUbuntu x86_64\n\n### DuckDB Version:\n\nmain branch (451315955f969523e608e0b883a7fd58ffae0758)\n\n### DuckDB Client:\n\nmake unit\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nEnrico Minack\n\n### Affiliation:\n\nself-employed\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a source build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [x] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [x] Yes, I have\n",
  "hints_text": "",
  "created_at": "2025-04-08T10:21:40Z"
}