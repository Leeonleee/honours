{
  "repo": "duckdb/duckdb",
  "pull_number": 6437,
  "instance_id": "duckdb__duckdb-6437",
  "issue_numbers": [
    "6432",
    "6359"
  ],
  "base_commit": "185be5c8ad6009b92db95f2f923851ef9b884f0c",
  "patch": "diff --git a/extension/json/include/json_scan.hpp b/extension/json/include/json_scan.hpp\nindex d662a7d4344a..80ea0e72fbac 100644\n--- a/extension/json/include/json_scan.hpp\n+++ b/extension/json/include/json_scan.hpp\n@@ -296,6 +296,21 @@ struct JSONScan {\n \t\treturn lstate.GetBatchIndex();\n \t}\n \n+\tstatic unique_ptr<NodeStatistics> JSONScanCardinality(ClientContext &context, const FunctionData *bind_data) {\n+\t\tauto &data = (JSONScanData &)*bind_data;\n+\t\tidx_t per_file_cardinality;\n+\t\tif (data.stored_readers.empty()) {\n+\t\t\t// The cardinality of an unknown JSON file is the almighty number 42 except when it's not\n+\t\t\tper_file_cardinality = 42;\n+\t\t} else {\n+\t\t\t// If we multiply the almighty number 42 by 10, we get the exact average size of a JSON\n+\t\t\t// Not really, but the average size of a lineitem row in JSON is around 360 bytes\n+\t\t\tper_file_cardinality = data.stored_readers[0]->GetFileHandle().FileSize() / 420;\n+\t\t}\n+\t\t// Obviously this can be improved but this is better than defaulting to 0\n+\t\treturn make_unique<NodeStatistics>(per_file_cardinality * data.file_paths.size());\n+\t}\n+\n \tstatic void JSONScanSerialize(FieldWriter &writer, const FunctionData *bind_data_p, const TableFunction &function) {\n \t\tauto &bind_data = (JSONScanData &)*bind_data_p;\n \t\tbind_data.Serialize(writer);\n@@ -316,6 +331,7 @@ struct JSONScan {\n \n \t\ttable_function.table_scan_progress = JSONScanProgress;\n \t\ttable_function.get_batch_index = JSONScanGetBatchIndex;\n+\t\ttable_function.cardinality = JSONScanCardinality;\n \n \t\ttable_function.serialize = JSONScanSerialize;\n \t\ttable_function.deserialize = JSONScanDeserialize;\ndiff --git a/extension/json/json_functions.cpp b/extension/json/json_functions.cpp\nindex 96e6019c41af..ff4e73c9e2a3 100644\n--- a/extension/json/json_functions.cpp\n+++ b/extension/json/json_functions.cpp\n@@ -173,6 +173,7 @@ unique_ptr<TableRef> JSONFunctions::ReadJSONReplacement(ClientContext &context,\n \t\tlower_name = lower_name.substr(0, lower_name.size() - 4);\n \t}\n \tif (!StringUtil::EndsWith(lower_name, \".json\") && !StringUtil::Contains(lower_name, \".json?\") &&\n+\t    !StringUtil::EndsWith(lower_name, \".jsonl\") && !StringUtil::Contains(lower_name, \".jsonl?\") &&\n \t    !StringUtil::EndsWith(lower_name, \".ndjson\") && !StringUtil::Contains(lower_name, \".ndjson?\")) {\n \t\treturn nullptr;\n \t}\ndiff --git a/extension/json/json_functions/json_structure.cpp b/extension/json/json_functions/json_structure.cpp\nindex af17231115f7..00dd65d3e57a 100644\n--- a/extension/json/json_functions/json_structure.cpp\n+++ b/extension/json/json_functions/json_structure.cpp\n@@ -214,9 +214,6 @@ void JSONStructureNode::RefineCandidateTypesObject(yyjson_val *vals[], idx_t cou\n \t\t}\n \t}\n \n-\tif (count > STANDARD_VECTOR_SIZE) {\n-\t\tstring_vector.Initialize(false, count);\n-\t}\n \tfor (idx_t child_idx = 0; child_idx < child_count; child_idx++) {\n \t\tdesc.children[child_idx].RefineCandidateTypes(child_vals[child_idx], count, string_vector, allocator,\n \t\t                                              date_format_map);\n@@ -431,6 +428,10 @@ static inline yyjson_mut_val *ConvertStructureArray(const JSONStructureNode &nod\n static inline yyjson_mut_val *ConvertStructureObject(const JSONStructureNode &node, yyjson_mut_doc *doc) {\n \tD_ASSERT(node.descriptions.size() == 1 && node.descriptions[0].type == LogicalTypeId::STRUCT);\n \tauto &desc = node.descriptions[0];\n+\tif (desc.children.empty()) {\n+\t\t// Empty struct - let's do JSON instead\n+\t\treturn yyjson_mut_str(doc, JSONCommon::JSON_TYPE_NAME);\n+\t}\n \n \tauto obj = yyjson_mut_obj(doc);\n \tfor (auto &child : desc.children) {\n@@ -495,6 +496,10 @@ static LogicalType StructureToTypeObject(ClientContext &context, const JSONStruc\n                                          idx_t depth) {\n \tD_ASSERT(node.descriptions.size() == 1 && node.descriptions[0].type == LogicalTypeId::STRUCT);\n \tauto &desc = node.descriptions[0];\n+\tif (desc.children.empty()) {\n+\t\t// Empty struct - let's do JSON instead\n+\t\treturn JSONCommon::JSONType();\n+\t}\n \n \tchild_list_t<LogicalType> child_types;\n \tchild_types.reserve(desc.children.size());\ndiff --git a/extension/json/json_functions/json_transform.cpp b/extension/json/json_functions/json_transform.cpp\nindex 7d5d4561c3bc..4f69c234a5fc 100644\n--- a/extension/json/json_functions/json_transform.cpp\n+++ b/extension/json/json_functions/json_transform.cpp\n@@ -58,6 +58,9 @@ static LogicalType StructureToTypeObject(yyjson_val *obj, ClientContext &context\n \t\tchild_types.emplace_back(key_str, StructureStringToType(val, context));\n \t}\n \tD_ASSERT(yyjson_obj_size(obj) == names.size());\n+\tif (child_types.empty()) {\n+\t\tthrow InvalidInputException(\"Empty object in JSON structure\");\n+\t}\n \treturn LogicalType::STRUCT(child_types);\n }\n \n@@ -87,7 +90,7 @@ static unique_ptr<FunctionData> JSONTransformBind(ClientContext &context, Scalar\n \t} else {\n \t\tauto structure_val = ExpressionExecutor::EvaluateScalar(context, *arguments[1]);\n \t\tif (!structure_val.DefaultTryCastAs(JSONCommon::JSONType())) {\n-\t\t\tthrow InvalidInputException(\"cannot cast JSON structure to string\");\n+\t\t\tthrow InvalidInputException(\"Cannot cast JSON structure to string\");\n \t\t}\n \t\tauto structure_string = structure_val.GetValueUnsafe<string_t>();\n \t\tJSONAllocator json_allocator(Allocator::DefaultAllocator());\n@@ -251,7 +254,10 @@ static bool TransformDecimal(yyjson_val *vals[], Vector &result, const idx_t cou\n \n bool JSONTransform::GetStringVector(yyjson_val *vals[], const idx_t count, const LogicalType &target,\n                                     Vector &string_vector, JSONTransformOptions &options) {\n-\tauto data = (string_t *)FlatVector::GetData(string_vector);\n+\tif (count > STANDARD_VECTOR_SIZE) {\n+\t\tstring_vector.Initialize(false, count);\n+\t}\n+\tauto data = FlatVector::GetData<string_t>(string_vector);\n \tauto &validity = FlatVector::Validity(string_vector);\n \tvalidity.SetAllValid(count);\n \n",
  "test_patch": "diff --git a/test/sql/json/test_json_empty_object.test b/test/sql/json/test_json_empty_object.test\nnew file mode 100644\nindex 000000000000..cedbff22accd\n--- /dev/null\n+++ b/test/sql/json/test_json_empty_object.test\n@@ -0,0 +1,58 @@\n+# name: test/sql/json/test_json_empty_object.test\n+# description: Test empty objects in JSON (DuckDB cannot have empty STRUCT)\n+# group: [json]\n+\n+require json\n+\n+statement ok\n+pragma enable_verification\n+\n+# returns JSON instead of {}\n+query I\n+select json_structure('{}')\n+----\n+\"JSON\"\n+\n+statement error\n+select json_transform('{}', '{}')\n+----\n+Invalid Input Error: Empty object in JSON structure\n+\n+# create a JSON file with an empty struct\n+statement ok\n+copy (select '{\"a\": {}}') to '__TEST_DIR__/my.json' (FORMAT CSV, quote '')\n+\n+# auto-detection should not give back an empty struct\n+query T\n+select typeof(a) from '__TEST_DIR__/my.json'\n+----\n+JSON\n+\n+# can't force it to have an empty struct\n+statement error\n+select * from read_json('__TEST_DIR__/my.json', columns={a: 'STRUCT()')\n+\n+# test issue 6443\n+statement ok\n+copy (select unnest(['{\"c1\":\"val11\",\"c2\":{\"k1\":\"val11\",\"k2\":{}}}','{\"c1\":\"val21\",\"c2\":{\"k1\":\"val21\",\"k2\":{}}}'])) to '__TEST_DIR__/data.ndjson' (FORMAT CSV, quote '')\n+\n+statement ok\n+CREATE OR REPLACE TABLE tbl AS SELECT * FROM read_ndjson_auto('__TEST_DIR__/data.ndjson');\n+\n+# no empty struct here either\n+query II\n+select typeof(c1), typeof(c2) from tbl;\n+----\n+VARCHAR\tSTRUCT(k1 VARCHAR, k2 JSON)\n+VARCHAR\tSTRUCT(k1 VARCHAR, k2 JSON)\n+\n+require parquet\n+\n+statement ok\n+copy tbl to '__TEST_DIR__/data.parquet';\n+\n+query II\n+select * from '__TEST_DIR__/data.parquet';\n+----\n+val11\t{'k1': val11, 'k2': {}}\n+val21\t{'k1': val21, 'k2': {}}\n",
  "problem_statement": "unexpected exit on read_json_auto()\n### What happens?\r\n\r\nCli crashes when loading very big json files. I'm not sure what's the supported size limits for loading json files, so this could be also a feature request.\r\n\r\n### To Reproduce\r\n\r\nTest 1:\r\n\r\n```\r\nwget -O test.json \"https://www.kartenliebe.de/web-api/calendar-data\"\r\nls -lisah test.json\r\n# 2753933 1,3M -rw-rw-r-- 1 tb tb 1,3M Feb 22 21:53 test.json\r\n./duckdb\r\n# v0.7.1-dev318 c4dc292659\r\n# Enter \".help\" for usage hints.\r\n# Connected to a transient in-memory database.\r\n# Use \".open FILENAME\" to reopen on a persistent database.\r\nselect * from read_json_auto('test.json');\r\n# malloc_consolidate(): invalid chunk size\r\n# Aborted\r\necho $?\r\n# 134\r\n```\r\n\r\nTest 2:\r\n\r\n```\r\nwget -O test2.json \"https://www.kartenliebe.de/web-api/card-data?occasion=kommunion&themeId=2813&card=fotostreifen-90x210\"\r\nls -lisah test2.json\r\n# 2758495 1,3M -rw-rw-r-- 1 tb tb 1,3M Feb 22 21:58 test2.json\r\n./duckdb\r\n# v0.7.1-dev318 c4dc292659\r\n# Enter \".help\" for usage hints.\r\n# Connected to a transient in-memory database.\r\n# Use \".open FILENAME\" to reopen on a persistent database.\r\nselect * from read_json_auto('test2.json');\r\n# Segmentation fault\r\necho $?\r\n# 139\r\n```\r\n\r\n### OS:\r\n\r\nx64 amd\r\n\r\n### DuckDB Version:\r\n\r\nv0.7.1-dev318 c4dc292659\r\n\r\n### DuckDB Client:\r\n\r\ncli\r\n\r\n### Full Name:\r\n\r\nThomas Bley\r\n\r\n### Affiliation:\r\n\r\nmyself\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\n- [X] I agree\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] I agree\nDuckDB exits unexpectedly on large JSON import\n### What happens?\n\nI have a 43mb JSON file and I tried to read it like this:\r\n\r\n```bash\r\nduckdb -c \"select count(*) from read_json_auto('test_json2.json', maximum_object_size=64388587);\"\r\n```\r\n\r\nThe output looks like this:\r\n\r\n```bash\r\nmalloc(): corrupted top size\r\n```\n\n### To Reproduce\n\nThe file is private so I cannot upload it here, but I was curious if there are any known limits to how large a JSON file can be. If the core team has some large JSON files that have been tested I could look at those versus mine to see what might be different and share that instead.\n\n### OS:\n\nLinux Constitution 5.15.79.1-microsoft-standard-WSL2 #1 SMP Wed Nov 23 01:01:46 UTC 2022 x86_64 GNU/Linux\n\n### DuckDB Version:\n\nv0.7.0 f7827396d7\n\n### DuckDB Client:\n\nCLI\n\n### Full Name:\n\nShahid Shah\n\n### Affiliation:\n\nNetspective\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "\nCan you please try to make this reproducible, ie by creating a non-private JSON file that triggers the issue? Would help us tremendously in tracking this down. \nSure @hannes I'll try to deidentify the 43mb `test_json2.json` file and put up a copy as soon as I can. Meanwhile, could you point me to the largest JSON test fixtures the DuckDB team used for unit testing the `read_json` functions? Thanks!\n@shah I have stress-tested using the data over at https://www.gharchive.org\r\n\r\nIncreasing `maximum_object_size` directly increases the memory usage, so this could be the issue as well. I have made improvements in a very recent PR that should reduce memory usage slightly, maybe you could try again on the latest master?\nSure @lnkuiper I'm happy to try out master -- will need to setup my environment to build from source as the most current version (0.70.0) is what tested (via binary).\n@shah great! If you prefer, you can also wait for the 0.7.1 bugfix release. I hope it fixes the problem",
  "created_at": "2023-02-23T09:46:31Z"
}