You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Handle special characters from Python API
Hi All!
First off - DuckDB is flat out amazing - I love it. It's perfect for our use case.

I had a small issue while benchmarking it - a registered trademark symbol in my DuckDB caused it to fail upon executing a select statement with the error:
"Runtime error: Could not allocate string object!".

I was planning to adjust the collation, but I couldn't find a function in the Python API to do it. How can I change the collation through Python?

Many thanks!
-Alex
Handle special characters from Python API
Hi All!
First off - DuckDB is flat out amazing - I love it. It's perfect for our use case.

I had a small issue while benchmarking it - a registered trademark symbol in my DuckDB caused it to fail upon executing a select statement with the error:
"Runtime error: Could not allocate string object!".

I was planning to adjust the collation, but I couldn't find a function in the Python API to do it. How can I change the collation through Python?

Many thanks!
-Alex

</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="30">
2: 
3: [![Travis](https://api.travis-ci.org/cwida/duckdb.svg?branch=master)](https://travis-ci.org/cwida/duckdb)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
6: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3901452.svg)](https://zenodo.org/record/3901452)
7: 
8: 
9: ## Installation
10: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
11: 
12: ## Development
13: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
14: 
15: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
16: 
17: 
[end of README.md]
[start of scripts/amalgamation.py]
1: # this script creates a single header + source file combination out of the DuckDB sources
2: import os, re, sys, shutil
3: amal_dir = os.path.join('src', 'amalgamation')
4: header_file = os.path.join(amal_dir, "duckdb.hpp")
5: source_file = os.path.join(amal_dir, "duckdb.cpp")
6: temp_header = 'duckdb.hpp.tmp'
7: temp_source = 'duckdb.cpp.tmp'
8: 
9: src_dir = 'src'
10: include_dir = os.path.join('src', 'include')
11: fmt_dir = os.path.join('third_party', 'fmt')
12: fmt_include_dir = os.path.join('third_party', 'fmt', 'include')
13: hll_dir = os.path.join('third_party', 'hyperloglog')
14: miniz_dir = os.path.join('third_party', 'miniz')
15: re2_dir = os.path.join('third_party', 're2')
16: pg_query_dir = os.path.join('third_party', 'libpg_query')
17: pg_query_include_dir = os.path.join('third_party', 'libpg_query', 'include')
18: 
19: utf8proc_dir = os.path.join('third_party', 'utf8proc')
20: utf8proc_include_dir = os.path.join('third_party', 'utf8proc', 'include')
21: 
22: # files included in the amalgamated "duckdb.hpp" file
23: main_header_files = [os.path.join(include_dir, 'duckdb.hpp'), os.path.join(include_dir, 'duckdb.h'), os.path.join(include_dir, 'duckdb', 'common', 'types', 'date.hpp'), os.path.join(include_dir, 'duckdb', 'common', 'types', 'timestamp.hpp'), os.path.join(include_dir, 'duckdb', 'common', 'types', 'time.hpp'), os.path.join(include_dir, 'duckdb', 'main', 'appender.hpp'), os.path.join(include_dir, 'duckdb', 'main', 'client_context.hpp'), os.path.join(include_dir, 'duckdb', 'function', 'function.hpp'), os.path.join(include_dir, 'duckdb', 'function', 'table_function.hpp'), os.path.join(include_dir, 'duckdb', 'parser', 'parsed_data', 'create_table_function_info.hpp')]
24: 
25: # include paths for where to search for include files during amalgamation
26: include_paths = [include_dir, fmt_include_dir, hll_dir, re2_dir, miniz_dir, utf8proc_include_dir, utf8proc_dir, pg_query_include_dir, pg_query_dir]
27: # paths of where to look for files to compile and include to the final amalgamation
28: compile_directories = [src_dir, fmt_dir, hll_dir, miniz_dir, re2_dir, utf8proc_dir, pg_query_dir]
29: 
30: # files always excluded
31: always_excluded = ['src/amalgamation/duckdb.cpp', 'src/amalgamation/duckdb.hpp', 'src/amalgamation/parquet-extension.cpp', 'src/amalgamation/parquet-extension.hpp']
32: # files excluded from the amalgamation
33: excluded_files = ['grammar.cpp', 'grammar.hpp', 'symbols.cpp', 'file_system.cpp']
34: # files excluded from individual file compilation during test_compile
35: excluded_compilation_files = excluded_files + ['gram.hpp', 'kwlist.hpp', "duckdb-c.cpp"]
36: 
37: 
38: linenumbers = False
39: 
40: def get_includes(fpath, text):
41:     # find all the includes referred to in the directory
42:     include_statements = re.findall("(^[#]include[\t ]+[\"]([^\"]+)[\"])", text, flags=re.MULTILINE)
43:     include_files = []
44:     # figure out where they are located
45:     for included_file in [x[1] for x in include_statements]:
46:         included_file = os.sep.join(included_file.split('/'))
47:         found = False
48:         for include_path in include_paths:
49:             ipath = os.path.join(include_path, included_file)
50:             if os.path.isfile(ipath):
51:                 include_files.append(ipath)
52:                 found = True
53:                 break
54:         if not found:
55:             raise Exception('Could not find include file "' + included_file + '", included from file "' + fpath + '"')
56:     return ([x[0] for x in include_statements], include_files)
57: 
58: def cleanup_file(text):
59:     # remove all "#pragma once" notifications
60:     text = re.sub('#pragma once', '', text)
61:     return text
62: 
63: # recursively get all includes and write them
64: written_files = {}
65: 
66: def write_file(current_file, ignore_excluded = False):
67:     global linenumbers
68:     global written_files
69:     if current_file in always_excluded:
70:         return ""
71:     if current_file.split(os.sep)[-1] in excluded_files and not ignore_excluded:
72:         # file is in ignored files set
73:         return ""
74:     if current_file in written_files:
75:         # file is already written
76:         return ""
77:     written_files[current_file] = True
78: 
79:     # first read this file
80:     with open(current_file, 'r') as f:
81:         text = f.read()
82: 
83:     (statements, includes) = get_includes(current_file, text)
84:     # find the linenr of the final #include statement we parsed
85:     if len(statements) > 0:
86:         index = text.find(statements[-1])
87:         linenr = len(text[:index].split('\n'))
88: 
89:         # now write all the dependencies of this header first
90:         for i in range(len(includes)):
91:             include_text = write_file(includes[i])
92:             if linenumbers and i == len(includes) - 1:
93:                 # for the last include statement, we also include a #line directive
94:                 include_text += '\n#line %d "%s"\n' % (linenr, current_file)
95:             text = text.replace(statements[i], include_text)
96: 
97:     # add the initial line here
98:     if linenumbers:
99:         text = '\n#line 1 "%s"\n' % (current_file,) + text
100:     # print(current_file)
101:     # now read the header and write it
102:     return cleanup_file(text)
103: 
104: def write_dir(dir, sfile):
105:     files = os.listdir(dir)
106:     files.sort()
107:     for fname in files:
108:         if fname in excluded_files:
109:             continue
110:         fpath = os.path.join(dir, fname)
111:         if os.path.isdir(fpath):
112:             write_dir(fpath, sfile)
113:         elif fname.endswith('.cpp') or fname.endswith('.c') or fname.endswith('.cc'):
114:             sfile.write(write_file(fpath))
115: 
116: def copy_if_different(src, dest):
117:     if os.path.isfile(dest):
118:         # dest exists, check if the files are different
119:         with open(src, 'r') as f:
120:             source_text = f.read()
121:         with open(dest, 'r') as f:
122:             dest_text = f.read()
123:         if source_text == dest_text:
124:             return
125:     shutil.copyfile(src, dest)
126: 
127: def generate_amalgamation(source_file, header_file):
128:     # now construct duckdb.hpp from these headers
129:     print("-----------------------")
130:     print("-- Writing " + header_file + " --")
131:     print("-----------------------")
132:     with open(temp_header, 'w+') as hfile:
133:         hfile.write("#pragma once\n")
134:         hfile.write("#define DUCKDB_AMALGAMATION 1\n")
135:         for fpath in main_header_files:
136:             hfile.write(write_file(fpath))
137: 
138: 
139:     # now construct duckdb.cpp
140:     print("------------------------")
141:     print("-- Writing " + source_file + " --")
142:     print("------------------------")
143: 
144:     # scan all the .cpp files
145:     with open(temp_source, 'w+') as sfile:
146:         header_file_name = header_file.split(os.sep)[-1]
147:         sfile.write('#include "' + header_file_name + '"\n\n')
148:         for compile_dir in compile_directories:
149:             write_dir(compile_dir, sfile)
150:         # for windows we write file_system.cpp last
151:         # this is because it includes windows.h which contains a lot of #define statements that mess up the other code
152:         sfile.write(write_file(os.path.join('src', 'common', 'file_system.cpp'), True))
153: 
154:     copy_if_different(temp_header, header_file)
155:     copy_if_different(temp_source, source_file)
156: 
157: 
158: 
159: if __name__ == "__main__":
160:     for arg in sys.argv:
161:         if arg == '--linenumbers':
162:             linenumbers = True
163:         elif arg == '--no-linenumbers':
164:             linenumbers = False
165:         elif arg.startswith('--header='):
166:             header_file = os.path.join(*arg.split('=', 1)[1].split('/'))
167:         elif arg.startswith('--source='):
168:             source_file = os.path.join(*arg.split('=', 1)[1].split('/'))
169:     if not os.path.exists(amal_dir):
170:         os.makedirs(amal_dir)
171: 
172:     generate_amalgamation(source_file, header_file)
[end of scripts/amalgamation.py]
[start of tools/pythonpkg/duckdb_python.cpp]
1: #include <pybind11/pybind11.h>
2: #include <pybind11/numpy.h>
3: 
4: #include <unordered_map>
5: #include <vector>
6: 
7: #include "datetime.h" // from Python
8: 
9: #include "duckdb.hpp"
10: #include "parquet-extension.hpp"
11: 
12: namespace py = pybind11;
13: 
14: using namespace duckdb;
15: using namespace std;
16: 
17: namespace duckdb_py_convert {
18: 
19: struct RegularConvert {
20: 	template <class DUCKDB_T, class NUMPY_T> static NUMPY_T convert_value(DUCKDB_T val) {
21: 		return (NUMPY_T)val;
22: 	}
23: };
24: 
25: struct TimestampConvert {
26: 	template <class DUCKDB_T, class NUMPY_T> static int64_t convert_value(timestamp_t val) {
27: 		return Date::Epoch(Timestamp::GetDate(val)) * 1000 + (int64_t)(Timestamp::GetTime(val));
28: 	}
29: };
30: 
31: struct DateConvert {
32: 	template <class DUCKDB_T, class NUMPY_T> static int64_t convert_value(date_t val) {
33: 		return Date::Epoch(val);
34: 	}
35: };
36: 
37: struct TimeConvert {
38: 	template <class DUCKDB_T, class NUMPY_T> static py::str convert_value(time_t val) {
39: 		return py::str(duckdb::Time::ToString(val).c_str());
40: 	}
41: };
42: 
43: struct StringConvert {
44: 	template <class DUCKDB_T, class NUMPY_T> static py::str convert_value(string_t val) {
45: 		return py::str(val.GetData());
46: 	}
47: };
48: 
49: template <class DUCKDB_T, class NUMPY_T, class CONVERT>
50: static py::array fetch_column(string numpy_type, ChunkCollection &collection, idx_t column) {
51: 	auto out = py::array(py::dtype(numpy_type), collection.count);
52: 	auto out_ptr = (NUMPY_T *)out.mutable_data();
53: 
54: 	idx_t out_offset = 0;
55: 	for (auto &data_chunk : collection.chunks) {
56: 		auto &src = data_chunk->data[column];
57: 		auto src_ptr = FlatVector::GetData<DUCKDB_T>(src);
58: 		auto &nullmask = FlatVector::Nullmask(src);
59: 		for (idx_t i = 0; i < data_chunk->size(); i++) {
60: 			if (nullmask[i]) {
61: 				continue;
62: 			}
63: 			out_ptr[i + out_offset] = CONVERT::template convert_value<DUCKDB_T, NUMPY_T>(src_ptr[i]);
64: 		}
65: 		out_offset += data_chunk->size();
66: 	}
67: 	return out;
68: }
69: 
70: template <class T> static py::array fetch_column_regular(string numpy_type, ChunkCollection &collection, idx_t column) {
71: 	return fetch_column<T, T, RegularConvert>(numpy_type, collection, column);
72: }
73: 
74: }; // namespace duckdb_py_convert
75: // namespace duckdb_py_convert
76: 
77: namespace random_string {
78: static std::random_device rd;
79: static std::mt19937 gen(rd());
80: static std::uniform_int_distribution<> dis(0, 15);
81: 
82: std::string generate() {
83: 	std::stringstream ss;
84: 	int i;
85: 	ss << std::hex;
86: 	for (i = 0; i < 16; i++) {
87: 		ss << dis(gen);
88: 	}
89: 	return ss.str();
90: }
91: } // namespace random_string
92: 
93: struct PandasScanFunctionData : public TableFunctionData {
94: 	PandasScanFunctionData(py::handle df, idx_t row_count, vector<SQLType> sql_types)
95: 	    : df(df), row_count(row_count), sql_types(sql_types), position(0) {
96: 	}
97: 	py::handle df;
98: 	idx_t row_count;
99: 	vector<SQLType> sql_types;
100: 	idx_t position;
101: };
102: 
103: struct PandasScanFunction : public TableFunction {
104: 	PandasScanFunction()
105: 	    : TableFunction("pandas_scan", {SQLType::VARCHAR}, pandas_scan_bind, pandas_scan_function, nullptr){};
106: 
107: 	static unique_ptr<FunctionData> pandas_scan_bind(ClientContext &context, vector<Value> inputs,
108: 	                                                 vector<SQLType> &return_types, vector<string> &names) {
109: 		// Hey, it works (TM)
110: 		py::handle df((PyObject *)std::stoull(inputs[0].GetValue<string>(), nullptr, 16));
111: 
112: 		/* TODO this fails on Python2 for some reason
113: 		auto pandas_mod = py::module::import("pandas.core.frame");
114: 		auto df_class = pandas_mod.attr("DataFrame");
115: 
116: 		if (!df.get_type().is(df_class)) {
117: 		    throw Exception("parameter is not a DataFrame");
118: 		} */
119: 
120: 		auto df_names = py::list(df.attr("columns"));
121: 		auto df_types = py::list(df.attr("dtypes"));
122: 		// TODO support masked arrays as well
123: 		// TODO support dicts of numpy arrays as well
124: 		if (py::len(df_names) == 0 || py::len(df_types) == 0 || py::len(df_names) != py::len(df_types)) {
125: 			throw runtime_error("Need a DataFrame with at least one column");
126: 		}
127: 		for (idx_t col_idx = 0; col_idx < py::len(df_names); col_idx++) {
128: 			auto col_type = string(py::str(df_types[col_idx]));
129: 			names.push_back(string(py::str(df_names[col_idx])));
130: 			SQLType duckdb_col_type;
131: 			if (col_type == "bool") {
132: 				duckdb_col_type = SQLType::BOOLEAN;
133: 			} else if (col_type == "int8") {
134: 				duckdb_col_type = SQLType::TINYINT;
135: 			} else if (col_type == "int16") {
136: 				duckdb_col_type = SQLType::SMALLINT;
137: 			} else if (col_type == "int32") {
138: 				duckdb_col_type = SQLType::INTEGER;
139: 			} else if (col_type == "int64") {
140: 				duckdb_col_type = SQLType::BIGINT;
141: 			} else if (col_type == "float32") {
142: 				duckdb_col_type = SQLType::FLOAT;
143: 			} else if (col_type == "float64") {
144: 				duckdb_col_type = SQLType::DOUBLE;
145: 			} else if (col_type == "datetime64[ns]") {
146: 				duckdb_col_type = SQLType::TIMESTAMP;
147: 			} else if (col_type == "object") {
148: 				// this better be strings
149: 				duckdb_col_type = SQLType::VARCHAR;
150: 			} else {
151: 				throw runtime_error("unsupported python type " + col_type);
152: 			}
153: 			return_types.push_back(duckdb_col_type);
154: 		}
155: 		idx_t row_count = py::len(df.attr("__getitem__")(df_names[0]));
156: 		return make_unique<PandasScanFunctionData>(df, row_count, return_types);
157: 	}
158: 
159: 	template <class T> static void scan_pandas_column(py::array numpy_col, idx_t count, idx_t offset, Vector &out) {
160: 		auto src_ptr = (T *)numpy_col.data();
161: 		FlatVector::SetData(out, (data_ptr_t) (src_ptr + offset));
162: 	}
163: 
164: 	static void pandas_scan_function(ClientContext &context, vector<Value> &input, DataChunk &output,
165: 	                                 FunctionData *dataptr) {
166: 		auto &data = *((PandasScanFunctionData *)dataptr);
167: 
168: 		if (data.position >= data.row_count) {
169: 			return;
170: 		}
171: 		idx_t this_count = std::min((idx_t)STANDARD_VECTOR_SIZE, data.row_count - data.position);
172: 
173: 		auto df_names = py::list(data.df.attr("columns"));
174: 		auto get_fun = data.df.attr("__getitem__");
175: 
176: 		output.SetCardinality(this_count);
177: 		for (idx_t col_idx = 0; col_idx < output.column_count(); col_idx++) {
178: 			auto numpy_col = py::array(get_fun(df_names[col_idx]).attr("to_numpy")());
179: 
180: 			switch (data.sql_types[col_idx].id) {
181: 			case SQLTypeId::BOOLEAN:
182: 				scan_pandas_column<bool>(numpy_col, this_count, data.position, output.data[col_idx]);
183: 				break;
184: 			case SQLTypeId::TINYINT:
185: 				scan_pandas_column<int8_t>(numpy_col, this_count, data.position, output.data[col_idx]);
186: 				break;
187: 			case SQLTypeId::SMALLINT:
188: 				scan_pandas_column<int16_t>(numpy_col, this_count, data.position, output.data[col_idx]);
189: 				break;
190: 			case SQLTypeId::INTEGER:
191: 				scan_pandas_column<int32_t>(numpy_col, this_count, data.position, output.data[col_idx]);
192: 				break;
193: 			case SQLTypeId::BIGINT:
194: 				scan_pandas_column<int64_t>(numpy_col, this_count, data.position, output.data[col_idx]);
195: 				break;
196: 			case SQLTypeId::FLOAT:
197: 				scan_pandas_column<float>(numpy_col, this_count, data.position, output.data[col_idx]);
198: 				break;
199: 			case SQLTypeId::DOUBLE:
200: 				scan_pandas_column<double>(numpy_col, this_count, data.position, output.data[col_idx]);
201: 				break;
202: 			case SQLTypeId::TIMESTAMP: {
203: 				auto src_ptr = (int64_t *)numpy_col.data();
204: 				auto tgt_ptr = (timestamp_t *)FlatVector::GetData(output.data[col_idx]);
205: 
206: 				for (idx_t row = 0; row < this_count; row++) {
207: 					auto ms = src_ptr[row] / 1000000; // nanoseconds
208: 					auto ms_per_day = (int64_t)60 * 60 * 24 * 1000;
209: 					date_t date = Date::EpochToDate(ms / 1000);
210: 					dtime_t time = (dtime_t)(ms % ms_per_day);
211: 					tgt_ptr[row] = Timestamp::FromDatetime(date, time);
212: 				}
213: 				break;
214: 			} break;
215: 			case SQLTypeId::VARCHAR: {
216: 				auto src_ptr = (PyObject **)numpy_col.data();
217: 				auto tgt_ptr = (string_t *)FlatVector::GetData(output.data[col_idx]);
218: 
219: 				for (idx_t row = 0; row < this_count; row++) {
220: 					auto val = src_ptr[row + data.position];
221: 
222: #if PY_MAJOR_VERSION >= 3
223: 					if (!PyUnicode_Check(val)) {
224: 						FlatVector::SetNull(output.data[col_idx], row, true);
225: 						continue;
226: 					}
227: 					if (PyUnicode_READY(val) != 0) {
228: 						throw runtime_error("failure in PyUnicode_READY");
229: 					}
230: 					if (PyUnicode_KIND(val) == PyUnicode_1BYTE_KIND) {
231: 						auto ucs1 = PyUnicode_1BYTE_DATA(val);
232: 						auto length = PyUnicode_GET_LENGTH(val);
233: 						tgt_ptr[row] = string_t((const char*) ucs1, length);
234: 					} else {
235: 						tgt_ptr[row] = StringVector::AddString(output.data[col_idx], ((py::object*) &val)->cast<string>());
236: 					}
237: #else
238: 					if (!py::isinstance<py::str>(*((py::object*) &val))) {
239: 						FlatVector::SetNull(output.data[col_idx], row, true);
240: 						continue;
241: 					}
242: 
243: 					tgt_ptr[row] = StringVector::AddString(output.data[col_idx], ((py::object*) &val)->cast<string>());
244: #endif
245: 				}
246: 				break;
247: 			}
248: 			default:
249: 				throw runtime_error("Unsupported type " + SQLTypeToString(data.sql_types[col_idx]));
250: 			}
251: 		}
252: 		data.position += this_count;
253: 	}
254: };
255: 
256: struct DuckDBPyResult {
257: 
258: 	template <class SRC> static SRC fetch_scalar(Vector &src_vec, idx_t offset) {
259: 		auto src_ptr = FlatVector::GetData<SRC>(src_vec);
260: 		return src_ptr[offset];
261: 	}
262: 
263: 	py::object fetchone() {
264: 		if (!result) {
265: 			throw runtime_error("result closed");
266: 		}
267: 		if (!current_chunk || chunk_offset >= current_chunk->size()) {
268: 			current_chunk = result->Fetch();
269: 			chunk_offset = 0;
270: 		}
271: 		if (current_chunk->size() == 0) {
272: 			return py::none();
273: 		}
274: 		py::tuple res(result->types.size());
275: 
276: 		for (idx_t col_idx = 0; col_idx < result->types.size(); col_idx++) {
277: 			auto &nullmask = FlatVector::Nullmask(current_chunk->data[col_idx]);
278: 			if (nullmask[chunk_offset]) {
279: 				res[col_idx] = py::none();
280: 				continue;
281: 			}
282: 			auto val = current_chunk->data[col_idx].GetValue(chunk_offset);
283: 			switch (result->sql_types[col_idx].id) {
284: 			case SQLTypeId::BOOLEAN:
285: 				res[col_idx] = val.GetValue<bool>();
286: 				break;
287: 			case SQLTypeId::TINYINT:
288: 				res[col_idx] = val.GetValue<int8_t>();
289: 				break;
290: 			case SQLTypeId::SMALLINT:
291: 				res[col_idx] = val.GetValue<int16_t>();
292: 				break;
293: 			case SQLTypeId::INTEGER:
294: 				res[col_idx] = val.GetValue<int32_t>();
295: 				break;
296: 			case SQLTypeId::BIGINT:
297: 				res[col_idx] = val.GetValue<int64_t>();
298: 				break;
299: 			case SQLTypeId::FLOAT:
300: 				res[col_idx] = val.GetValue<float>();
301: 				break;
302: 			case SQLTypeId::DOUBLE:
303: 				res[col_idx] = val.GetValue<double>();
304: 				break;
305: 			case SQLTypeId::VARCHAR:
306: 				res[col_idx] = val.GetValue<string>();
307: 				break;
308: 
309: 			case SQLTypeId::TIMESTAMP: {
310: 				if (result->types[col_idx] != TypeId::INT64) {
311: 					throw runtime_error("expected int64 for timestamp");
312: 				}
313: 				auto timestamp = val.GetValue<int64_t>();
314: 				auto date = Timestamp::GetDate(timestamp);
315: 				res[col_idx] = PyDateTime_FromDateAndTime(
316: 				    Date::ExtractYear(date), Date::ExtractMonth(date), Date::ExtractDay(date),
317: 				    Timestamp::GetHours(timestamp), Timestamp::GetMinutes(timestamp), Timestamp::GetSeconds(timestamp),
318: 				    Timestamp::GetMilliseconds(timestamp) * 1000 - Timestamp::GetSeconds(timestamp) * 1000000);
319: 
320: 				break;
321: 			}
322: 			case SQLTypeId::TIME: {
323: 				if (result->types[col_idx] != TypeId::INT32) {
324: 					throw runtime_error("expected int32 for time");
325: 				}
326: 				int32_t hour, min, sec, msec;
327: 				auto time = val.GetValue<int32_t>();
328: 				duckdb::Time::Convert(time, hour, min, sec, msec);
329: 				res[col_idx] = PyTime_FromTime(hour, min, sec, msec * 1000);
330: 				break;
331: 			}
332: 			case SQLTypeId::DATE: {
333: 				if (result->types[col_idx] != TypeId::INT32) {
334: 					throw runtime_error("expected int32 for date");
335: 				}
336: 				auto date = val.GetValue<int32_t>();
337: 				res[col_idx] = PyDate_FromDate(duckdb::Date::ExtractYear(date), duckdb::Date::ExtractMonth(date),
338: 				                               duckdb::Date::ExtractDay(date));
339: 				break;
340: 			}
341: 
342: 			default:
343: 				throw runtime_error("unsupported type: " + SQLTypeToString(result->sql_types[col_idx]));
344: 			}
345: 		}
346: 		chunk_offset++;
347: 		return move(res);
348: 	}
349: 
350: 	py::list fetchall() {
351: 		py::list res;
352: 		while (true) {
353: 			auto fres = fetchone();
354: 			if (fres.is_none()) {
355: 				break;
356: 			}
357: 			res.append(fres);
358: 		}
359: 		return res;
360: 	}
361: 
362: 	py::dict fetchnumpy() {
363: 		if (!result) {
364: 			throw runtime_error("result closed");
365: 		}
366: 		// need to materialize the result if it was streamed because we need the count :/
367: 		MaterializedQueryResult *mres = nullptr;
368: 		unique_ptr<QueryResult> mat_res_holder;
369: 		if (result->type == QueryResultType::STREAM_RESULT) {
370: 			mat_res_holder = ((StreamQueryResult *)result.get())->Materialize();
371: 			mres = (MaterializedQueryResult *)mat_res_holder.get();
372: 		} else {
373: 			mres = (MaterializedQueryResult *)result.get();
374: 		}
375: 		assert(mres);
376: 
377: 		py::dict res;
378: 		for (idx_t col_idx = 0; col_idx < mres->types.size(); col_idx++) {
379: 			// convert the actual payload
380: 			py::array col_res;
381: 			switch (mres->sql_types[col_idx].id) {
382: 			case SQLTypeId::BOOLEAN:
383: 				col_res = duckdb_py_convert::fetch_column_regular<bool>("bool", mres->collection, col_idx);
384: 				break;
385: 			case SQLTypeId::TINYINT:
386: 				col_res = duckdb_py_convert::fetch_column_regular<int8_t>("int8", mres->collection, col_idx);
387: 				break;
388: 			case SQLTypeId::SMALLINT:
389: 				col_res = duckdb_py_convert::fetch_column_regular<int16_t>("int16", mres->collection, col_idx);
390: 				break;
391: 			case SQLTypeId::INTEGER:
392: 				col_res = duckdb_py_convert::fetch_column_regular<int32_t>("int32", mres->collection, col_idx);
393: 				break;
394: 			case SQLTypeId::BIGINT:
395: 				col_res = duckdb_py_convert::fetch_column_regular<int64_t>("int64", mres->collection, col_idx);
396: 				break;
397: 			case SQLTypeId::FLOAT:
398: 				col_res = duckdb_py_convert::fetch_column_regular<float>("float32", mres->collection, col_idx);
399: 				break;
400: 			case SQLTypeId::DOUBLE:
401: 				col_res = duckdb_py_convert::fetch_column_regular<double>("float64", mres->collection, col_idx);
402: 				break;
403: 			case SQLTypeId::TIMESTAMP:
404: 				col_res = duckdb_py_convert::fetch_column<timestamp_t, int64_t, duckdb_py_convert::TimestampConvert>(
405: 				    "datetime64[ms]", mres->collection, col_idx);
406: 				break;
407: 			case SQLTypeId::DATE:
408: 				col_res = duckdb_py_convert::fetch_column<date_t, int64_t, duckdb_py_convert::DateConvert>(
409: 				    "datetime64[s]", mres->collection, col_idx);
410: 				break;
411: 			case SQLTypeId::TIME:
412: 				col_res = duckdb_py_convert::fetch_column<time_t, py::str, duckdb_py_convert::TimeConvert>(
413: 				    "object", mres->collection, col_idx);
414: 				break;
415: 			case SQLTypeId::VARCHAR:
416: 				col_res = duckdb_py_convert::fetch_column<string_t, py::str, duckdb_py_convert::StringConvert>(
417: 				    "object", mres->collection, col_idx);
418: 				break;
419: 			default:
420: 				throw runtime_error("unsupported type " + SQLTypeToString(mres->sql_types[col_idx]));
421: 			}
422: 
423: 			// convert the nullmask
424: 			auto nullmask = py::array(py::dtype("bool"), mres->collection.count);
425: 			auto nullmask_ptr = (bool*) nullmask.mutable_data();
426: 			idx_t out_offset = 0;
427: 			for (auto &data_chunk : mres->collection.chunks) {
428: 				auto &src_nm = FlatVector::Nullmask(data_chunk->data[col_idx]);
429: 				for (idx_t i = 0; i < data_chunk->size(); i++) {
430: 					nullmask_ptr[i + out_offset] = src_nm[i];
431: 				}
432: 				out_offset += data_chunk->size();
433: 			}
434: 
435: 			// create masked array and assign to output
436: 			auto masked_array = py::module::import("numpy.ma").attr("masked_array")(col_res, nullmask);
437: 			res[mres->names[col_idx].c_str()] = masked_array;
438: 		}
439: 		return res;
440: 	}
441: 
442: 	py::object fetchdf() {
443: 		return py::module::import("pandas").attr("DataFrame").attr("from_dict")(fetchnumpy());
444: 	}
445: 
446: 	py::list description() {
447: 		py::list desc(result->names.size());
448: 		for (idx_t col_idx = 0; col_idx < result->names.size(); col_idx++) {
449: 			py::tuple col_desc(7);
450: 			col_desc[0] = py::str(result->names[col_idx]);
451: 			col_desc[1] = py::none();
452: 			col_desc[2] = py::none();
453: 			col_desc[3] = py::none();
454: 			col_desc[4] = py::none();
455: 			col_desc[5] = py::none();
456: 			col_desc[6] = py::none();
457: 			desc[col_idx] = col_desc;
458: 		}
459: 		return desc;
460: 	}
461: 
462: 	void close() {
463: 		result = nullptr;
464: 	}
465: 	idx_t chunk_offset = 0;
466: 
467: 	unique_ptr<QueryResult> result;
468: 	unique_ptr<DataChunk> current_chunk;
469: };
470: 
471: struct DuckDBPyRelation;
472: 
473: struct DuckDBPyConnection {
474: 
475: 	DuckDBPyConnection *executemany(string query, py::object params = py::list()) {
476: 		execute(query, params, true);
477: 		return this;
478: 	}
479: 
480: 	~DuckDBPyConnection() {
481: 		for (auto &element : registered_dfs) {
482: 			unregister_df(element.first);
483: 		}
484: 	}
485: 
486: 	DuckDBPyConnection *execute(string query, py::object params = py::list(), bool many = false) {
487: 		if (!connection) {
488: 			throw runtime_error("connection closed");
489: 		}
490: 		result = nullptr;
491: 
492: 		auto prep = connection->Prepare(query);
493: 		if (!prep->success) {
494: 			throw runtime_error(prep->error);
495: 		}
496: 
497: 		// this is a list of a list of parameters in executemany
498: 		py::list params_set;
499: 		if (!many) {
500: 			params_set = py::list(1);
501: 			params_set[0] = params;
502: 		} else {
503: 			params_set = params;
504: 		}
505: 
506: 		for (const auto &single_query_params : params_set) {
507: 			if (prep->n_param != py::len(single_query_params)) {
508: 				throw runtime_error("Prepared statments needs " + to_string(prep->n_param) + " parameters, " +
509: 				                    to_string(py::len(single_query_params)) + " given");
510: 			}
511: 			auto args = DuckDBPyConnection::transform_python_param_list(single_query_params);
512: 			auto res = make_unique<DuckDBPyResult>();
513: 			res->result = prep->Execute(args);
514: 			if (!res->result->success) {
515: 				throw runtime_error(res->result->error);
516: 			}
517: 			if (!many) {
518: 				result = move(res);
519: 			}
520: 		}
521: 		return this;
522: 	}
523: 
524: 	DuckDBPyConnection *append(string name, py::object value) {
525: 		register_df("__append_df", value);
526: 		return execute("INSERT INTO \"" + name + "\" SELECT * FROM __append_df");
527: 	}
528: 
529: 	static string ptr_to_string(void const *ptr) {
530: 		std::ostringstream address;
531: 		address << ptr;
532: 		return address.str();
533: 	}
534: 
535: 	DuckDBPyConnection *register_df(string name, py::object value) {
536: 		// hack alert: put the pointer address into the function call as a string
537: 		execute("CREATE OR REPLACE VIEW \"" + name + "\" AS SELECT * FROM pandas_scan('" + ptr_to_string(value.ptr()) +
538: 		        "')");
539: 
540: 		// try to bind
541: 		execute("SELECT * FROM \"" + name + "\" WHERE FALSE");
542: 
543: 		// keep a reference
544: 		registered_dfs[name] = value;
545: 		return this;
546: 	}
547: 
548: 	unique_ptr<DuckDBPyRelation> table(string tname) {
549: 		if (!connection) {
550: 			throw runtime_error("connection closed");
551: 		}
552: 		return make_unique<DuckDBPyRelation>(connection->Table(tname));
553: 	}
554: 
555: 	unique_ptr<DuckDBPyRelation> values(py::object params = py::list()) {
556: 		if (!connection) {
557: 			throw runtime_error("connection closed");
558: 		}
559: 		vector<vector<Value>> values {DuckDBPyConnection::transform_python_param_list(params)};
560: 		return make_unique<DuckDBPyRelation>(connection->Values(values));
561: 	}
562: 
563: 	unique_ptr<DuckDBPyRelation> view(string vname) {
564: 		if (!connection) {
565: 			throw runtime_error("connection closed");
566: 		}
567: 		return make_unique<DuckDBPyRelation>(connection->View(vname));
568: 	}
569: 
570: 	unique_ptr<DuckDBPyRelation> table_function(string fname, py::object params = py::list()) {
571: 		if (!connection) {
572: 			throw runtime_error("connection closed");
573: 		}
574: 
575: 		return make_unique<DuckDBPyRelation>(
576: 		    connection->TableFunction(fname, DuckDBPyConnection::transform_python_param_list(params)));
577: 	}
578: 
579: 	unique_ptr<DuckDBPyRelation> from_df(py::object value) {
580: 		if (!connection) {
581: 			throw runtime_error("connection closed");
582: 		};
583: 		string name = "df_" + random_string::generate();
584: 		registered_dfs[name] = value;
585: 		vector<Value> params;
586: 		params.push_back(Value(ptr_to_string(value.ptr())));
587: 		return make_unique<DuckDBPyRelation>(connection->TableFunction("pandas_scan", params)->Alias(name));
588: 	}
589: 
590: 	unique_ptr<DuckDBPyRelation> from_csv_auto(string filename) {
591: 		if (!connection) {
592: 			throw runtime_error("connection closed");
593: 		};
594: 		vector<Value> params;
595: 		params.push_back(Value(filename));
596: 		return make_unique<DuckDBPyRelation>(connection->TableFunction("read_csv_auto", params)->Alias(filename));
597: 	}
598: 
599: 
600: 	unique_ptr<DuckDBPyRelation> from_parquet(string filename) {
601: 		if (!connection) {
602: 			throw runtime_error("connection closed");
603: 		};
604: 		vector<Value> params;
605: 		params.push_back(Value(filename));
606: 		return make_unique<DuckDBPyRelation>(connection->TableFunction("parquet_scan", params)->Alias(filename));
607: 	}
608: 
609: 
610: 	DuckDBPyConnection *unregister_df(string name) {
611: 		registered_dfs[name] = py::none();
612: 		return this;
613: 	}
614: 
615: 	DuckDBPyConnection *begin() {
616: 		execute("BEGIN TRANSACTION");
617: 		return this;
618: 	}
619: 
620: 	DuckDBPyConnection *commit() {
621: 		if (connection->context->transaction.IsAutoCommit()) {
622: 			return this;
623: 		}
624: 		execute("COMMIT");
625: 		return this;
626: 	}
627: 
628: 	DuckDBPyConnection *rollback() {
629: 		execute("ROLLBACK");
630: 		return this;
631: 	}
632: 
633: 	py::object getattr(py::str key) {
634: 		if (key.cast<string>() == "description") {
635: 			if (!result) {
636: 				throw runtime_error("no open result set");
637: 			}
638: 			return result->description();
639: 		}
640: 		return py::none();
641: 	}
642: 
643: 	void close() {
644: 		connection = nullptr;
645: 		database = nullptr;
646: 	}
647: 
648: 	// cursor() is stupid
649: 	unique_ptr<DuckDBPyConnection> cursor() {
650: 		auto res = make_unique<DuckDBPyConnection>();
651: 		res->database = database;
652: 		res->connection = make_unique<Connection>(*res->database);
653: 		return res;
654: 	}
655: 
656: 	// these should be functions on the result but well
657: 	py::tuple fetchone() {
658: 		if (!result) {
659: 			throw runtime_error("no open result set");
660: 		}
661: 		return result->fetchone();
662: 	}
663: 
664: 	py::list fetchall() {
665: 		if (!result) {
666: 			throw runtime_error("no open result set");
667: 		}
668: 		return result->fetchall();
669: 	}
670: 
671: 	py::dict fetchnumpy() {
672: 		if (!result) {
673: 			throw runtime_error("no open result set");
674: 		}
675: 		return result->fetchnumpy();
676: 	}
677: 	py::object fetchdf() {
678: 		if (!result) {
679: 			throw runtime_error("no open result set");
680: 		}
681: 		return result->fetchdf();
682: 	}
683: 
684: 	static unique_ptr<DuckDBPyConnection> connect(string database, bool read_only) {
685: 		auto res = make_unique<DuckDBPyConnection>();
686: 		DBConfig config;
687: 		if (read_only)
688: 			config.access_mode = AccessMode::READ_ONLY;
689: 		res->database = make_unique<DuckDB>(database, &config);
690: 		res->database->LoadExtension<ParquetExtension>();
691: 		res->connection = make_unique<Connection>(*res->database);
692: 
693: 		PandasScanFunction scan_fun;
694: 		CreateTableFunctionInfo info(scan_fun);
695: 
696: 		auto &context = *res->connection->context;
697: 		context.transaction.BeginTransaction();
698: 		context.catalog.CreateTableFunction(context, &info);
699: 		context.transaction.Commit();
700: 
701: 		if (!read_only) {
702: 			res->connection->Query("CREATE OR REPLACE VIEW sqlite_master AS SELECT * FROM sqlite_master()");
703: 		}
704: 
705: 		return res;
706: 	}
707: 
708: 	shared_ptr<DuckDB> database;
709: 	unique_ptr<Connection> connection;
710: 	unordered_map<string, py::object> registered_dfs;
711: 	unique_ptr<DuckDBPyResult> result;
712: 
713: 	static vector<Value> transform_python_param_list(py::handle params) {
714: 		vector<Value> args;
715: 
716: 		auto datetime_mod = py::module::import("datetime");
717: 		auto datetime_date = datetime_mod.attr("datetime");
718: 		auto datetime_datetime = datetime_mod.attr("date");
719: 
720: 		for (auto &ele : params) {
721: 			if (ele.is_none()) {
722: 				args.push_back(Value());
723: 			} else if (py::isinstance<py::bool_>(ele)) {
724: 				args.push_back(Value::BOOLEAN(ele.cast<bool>()));
725: 			} else if (py::isinstance<py::int_>(ele)) {
726: 				args.push_back(Value::BIGINT(ele.cast<int64_t>()));
727: 			} else if (py::isinstance<py::float_>(ele)) {
728: 				args.push_back(Value::DOUBLE(ele.cast<double>()));
729: 			} else if (py::isinstance<py::str>(ele)) {
730: 				args.push_back(Value(ele.cast<string>()));
731: 			} else if (ele.get_type().is(datetime_date)) {
732: 				throw runtime_error("date parameters not supported yet :/");
733: 				// args.push_back(Value::DATE(1984, 4, 24));
734: 			} else if (ele.get_type().is(datetime_datetime)) {
735: 				throw runtime_error("datetime parameters not supported yet :/");
736: 				// args.push_back(Value::TIMESTAMP(1984, 4, 24, 14, 42, 0, 0));
737: 			} else {
738: 				throw runtime_error("unknown param type " + py::str(ele.get_type()).cast<string>());
739: 			}
740: 		}
741: 		return args;
742: 	}
743: };
744: 
745: static unique_ptr<DuckDBPyConnection> default_connection_ = nullptr;
746: 
747: static DuckDBPyConnection *default_connection() {
748: 	if (!default_connection_) {
749: 		default_connection_ = DuckDBPyConnection::connect(":memory:", false);
750: 	}
751: 	return default_connection_.get();
752: }
753: 
754: struct DuckDBPyRelation {
755: 
756: 	DuckDBPyRelation(shared_ptr<Relation> rel) : rel(rel) {
757: 	}
758: 
759: 	static unique_ptr<DuckDBPyRelation> from_df(py::object df) {
760: 		return default_connection()->from_df(df);
761: 	}
762: 
763: 	static unique_ptr<DuckDBPyRelation> from_csv_auto(string filename) {
764: 		return default_connection()->from_csv_auto(filename);
765: 	}
766: 
767: 	static unique_ptr<DuckDBPyRelation> from_parquet(string filename) {
768: 		return default_connection()->from_parquet(filename);
769: 	}
770: 
771: 	unique_ptr<DuckDBPyRelation> project(string expr) {
772: 		return make_unique<DuckDBPyRelation>(rel->Project(expr));
773: 	}
774: 
775: 	static unique_ptr<DuckDBPyRelation> project_df(py::object df, string expr) {
776: 		return default_connection()->from_df(df)->project(expr);
777: 	}
778: 
779: 	unique_ptr<DuckDBPyRelation> alias(string expr) {
780: 		return make_unique<DuckDBPyRelation>(rel->Alias(expr));
781: 	}
782: 
783: 	static unique_ptr<DuckDBPyRelation> alias_df(py::object df, string expr) {
784: 		return default_connection()->from_df(df)->alias(expr);
785: 	}
786: 
787: 	unique_ptr<DuckDBPyRelation> filter(string expr) {
788: 		return make_unique<DuckDBPyRelation>(rel->Filter(expr));
789: 	}
790: 
791: 	static unique_ptr<DuckDBPyRelation> filter_df(py::object df, string expr) {
792: 		return default_connection()->from_df(df)->filter(expr);
793: 	}
794: 
795: 	unique_ptr<DuckDBPyRelation> limit(int64_t n) {
796: 		return make_unique<DuckDBPyRelation>(rel->Limit(n));
797: 	}
798: 
799: 	static unique_ptr<DuckDBPyRelation> limit_df(py::object df, int64_t n) {
800: 		return default_connection()->from_df(df)->limit(n);
801: 	}
802: 
803: 	unique_ptr<DuckDBPyRelation> order(string expr) {
804: 		return make_unique<DuckDBPyRelation>(rel->Order(expr));
805: 	}
806: 
807: 	static unique_ptr<DuckDBPyRelation> order_df(py::object df, string expr) {
808: 		return default_connection()->from_df(df)->order(expr);
809: 	}
810: 
811: 	unique_ptr<DuckDBPyRelation> aggregate(string expr, string groups = "") {
812: 		if (groups.size() > 0) {
813: 			return make_unique<DuckDBPyRelation>(rel->Aggregate(expr, groups));
814: 		}
815: 		return make_unique<DuckDBPyRelation>(rel->Aggregate(expr));
816: 	}
817: 
818: 	static unique_ptr<DuckDBPyRelation> aggregate_df(py::object df, string expr, string groups = "") {
819: 		return default_connection()->from_df(df)->aggregate(expr, groups);
820: 	}
821: 
822: 	unique_ptr<DuckDBPyRelation> distinct() {
823: 		return make_unique<DuckDBPyRelation>(rel->Distinct());
824: 	}
825: 
826: 	static unique_ptr<DuckDBPyRelation> distinct_df(py::object df) {
827: 		return default_connection()->from_df(df)->distinct();
828: 	}
829: 
830: 	py::object to_df() {
831: 		auto res = make_unique<DuckDBPyResult>();
832: 		res->result = rel->Execute();
833: 		if (!res->result->success) {
834: 			throw runtime_error(res->result->error);
835: 		}
836: 		return res->fetchdf();
837: 	}
838: 
839: 	unique_ptr<DuckDBPyRelation> union_(DuckDBPyRelation *other) {
840: 		return make_unique<DuckDBPyRelation>(rel->Union(other->rel));
841: 	}
842: 
843: 	unique_ptr<DuckDBPyRelation> except(DuckDBPyRelation *other) {
844: 		return make_unique<DuckDBPyRelation>(rel->Except(other->rel));
845: 	}
846: 
847: 	unique_ptr<DuckDBPyRelation> intersect(DuckDBPyRelation *other) {
848: 		return make_unique<DuckDBPyRelation>(rel->Intersect(other->rel));
849: 	}
850: 
851: 	unique_ptr<DuckDBPyRelation> join(DuckDBPyRelation *other, string condition) {
852: 		return make_unique<DuckDBPyRelation>(rel->Join(other->rel, condition));
853: 	}
854: 
855: 	void write_csv(string file) {
856: 		rel->WriteCSV(file);
857: 	}
858: 
859: 	static void write_csv_df(py::object df, string file) {
860: 		return default_connection()->from_df(df)->write_csv(file);
861: 	}
862: 
863: 	// should this return a rel with the new view?
864: 	unique_ptr<DuckDBPyRelation> create_view(string view_name, bool replace = true) {
865: 		rel->CreateView(view_name, replace);
866: 		return make_unique<DuckDBPyRelation>(rel);
867: 	}
868: 
869: 	static unique_ptr<DuckDBPyRelation> create_view_df(py::object df, string view_name, bool replace = true) {
870: 		return default_connection()->from_df(df)->create_view(view_name, replace);
871: 	}
872: 
873: 	unique_ptr<DuckDBPyResult> query(string view_name, string sql_query) {
874: 		auto res = make_unique<DuckDBPyResult>();
875: 		res->result = rel->Query(view_name, sql_query);
876: 		if (!res->result->success) {
877: 			throw runtime_error(res->result->error);
878: 		}
879: 		return res;
880: 	}
881: 
882: 	unique_ptr<DuckDBPyResult> execute() {
883: 		auto res = make_unique<DuckDBPyResult>();
884: 		res->result = rel->Execute();
885: 		if (!res->result->success) {
886: 			throw runtime_error(res->result->error);
887: 		}
888: 		return res;
889: 	}
890: 
891: 	static unique_ptr<DuckDBPyResult> query_df(py::object df, string view_name, string sql_query) {
892: 		return default_connection()->from_df(df)->query(view_name, sql_query);
893: 	}
894: 
895: 	void insert_into(string table) {
896: 		rel->Insert(table);
897: 	}
898: 
899: 	void insert(py::object params = py::list()) {
900: 		vector<vector<Value>> values {DuckDBPyConnection::transform_python_param_list(params)};
901: 		rel->Insert(values);
902: 	}
903: 
904: 	void create(string table) {
905: 		rel->Create(table);
906: 	}
907: 
908: 	string print() {
909: 		return rel->ToString() + "\n---------------------\n-- Result Preview  --\n---------------------\n" +  rel->Limit(10)->Execute()->ToString() + "\n";
910: 	}
911: 
912: 	py::object getattr(py::str key) {
913: 		auto key_s = key.cast<string>();
914: 		if (key_s == "alias") {
915: 			return py::str(string(rel->GetAlias()));
916: 		} else if (key_s == "type") {
917: 			return py::str(RelationTypeToString(rel->type));
918: 		} else if (key_s == "columns") {
919: 			py::list res;
920: 			for (auto &col : rel->Columns()) {
921: 				res.append(col.name);
922: 			}
923: 			return move(res);
924: 		} else if (key_s == "types" || key_s == "dtypes") {
925: 			py::list res;
926: 			for (auto &col : rel->Columns()) {
927: 				res.append(SQLTypeToString(col.type));
928: 			}
929: 			return move(res);
930: 		}
931: 		return py::none();
932: 	}
933: 
934: 	shared_ptr<Relation> rel;
935: };
936: 
937: PYBIND11_MODULE(duckdb, m) {
938: 	m.def("connect", &DuckDBPyConnection::connect, "Create a DuckDB database instance. Can take a database file name to read/write persistent data and a read_only flag if no changes are desired",
939: 	      py::arg("database") = ":memory:", py::arg("read_only") = false);
940: 
941: 	auto conn_class =
942: 	    py::class_<DuckDBPyConnection>(m, "DuckDBPyConnection")
943: 	        .def("cursor", &DuckDBPyConnection::cursor, "Create a duplicate of the current connection")
944: 	        .def("duplicate", &DuckDBPyConnection::cursor, "Create a duplicate of the current connection")
945: 	        .def("execute", &DuckDBPyConnection::execute, "Execute the given SQL query, optionally using prepared statements with parameters set", py::arg("query"),
946: 	             py::arg("parameters") = py::list(), py::arg("multiple_parameter_sets") = false)
947: 	        .def("executemany", &DuckDBPyConnection::executemany, "Execute the given prepared statement multiple times using the list of parameter sets in parameters", py::arg("query"),
948: 	             py::arg("parameters") = py::list())
949: 	        .def("close", &DuckDBPyConnection::close, "Close the connection")
950: 	        .def("fetchone", &DuckDBPyConnection::fetchone, "Fetch a single row from a result following execute")
951: 	        .def("fetchall", &DuckDBPyConnection::fetchall, "Fetch all rows from a result following execute")
952: 	        .def("fetchnumpy", &DuckDBPyConnection::fetchnumpy, "Fetch a result as list of NumPy arrays following execute")
953: 	        .def("fetchdf", &DuckDBPyConnection::fetchdf, "Fetch a result as Data.Frame following execute()")
954: 	        .def("begin", &DuckDBPyConnection::begin, "Start a new transaction")
955: 	        .def("commit", &DuckDBPyConnection::commit, "Commit changes performed within a transaction")
956: 	        .def("rollback", &DuckDBPyConnection::rollback, "Roll back changes performed within a transaction")
957: 	        .def("append", &DuckDBPyConnection::append, "Append the passed Data.Frame to the named table", py::arg("table_name"), py::arg("df"))
958: 			.def("register", &DuckDBPyConnection::register_df, "Register the passed Data.Frame value for querying with a view", py::arg("view_name"), py::arg("df"))
959: 	        .def("unregister", &DuckDBPyConnection::unregister_df, "Unregister the view name",  py::arg("view_name"))
960: 			.def("table", &DuckDBPyConnection::table, "Create a relation object for the name'd table", py::arg("table_name"))
961: 	        .def("view", &DuckDBPyConnection::view, "Create a relation object for the name'd view", py::arg("view_name"))
962: 	        .def("values", &DuckDBPyConnection::values, "Create a relation object from the passed values", py::arg("values"))
963: 	        .def("table_function", &DuckDBPyConnection::table_function, "Create a relation object from the name'd table function with given parameters",
964: 	             py::arg("name"), py::arg("parameters") = py::list())
965: 	        .def("from_df", &DuckDBPyConnection::from_df, "Create a relation object from the Data.Frame in df", py::arg("df"))
966: 	        .def("df", &DuckDBPyConnection::from_df, "Create a relation object from the Data.Frame in df (alias of from_df)", py::arg("df"))
967: 			.def("from_csv_auto", &DuckDBPyConnection::from_csv_auto, "Create a relation object from the CSV file in file_name",
968: 	             py::arg("file_name"))
969: 	       	.def("from_parquet", &DuckDBPyConnection::from_parquet, "Create a relation object from the Parquet file in file_name",
970: 	             py::arg("file_name"))
971: 		    .def("__getattr__", &DuckDBPyConnection::getattr, "Get result set attributes, mainly column names");
972: 
973: 	py::class_<DuckDBPyResult>(m, "DuckDBPyResult")
974: 	    .def("close", &DuckDBPyResult::close)
975: 	    .def("fetchone", &DuckDBPyResult::fetchone)
976: 	    .def("fetchall", &DuckDBPyResult::fetchall)
977: 	    .def("fetchnumpy", &DuckDBPyResult::fetchnumpy)
978: 	    .def("fetchdf", &DuckDBPyResult::fetchdf)
979: 	    .def("fetch_df", &DuckDBPyResult::fetchdf)
980: 	    .def("df", &DuckDBPyResult::fetchdf);
981: 
982: 	py::class_<DuckDBPyRelation>(m, "DuckDBPyRelation")
983: 	    .def("filter", &DuckDBPyRelation::filter, "Filter the relation object by the filter in filter_expr", py::arg("filter_expr"))
984: 	    .def("project", &DuckDBPyRelation::project, "Project the relation object by the projection in project_expr", py::arg("project_expr"))
985: 	    .def("set_alias", &DuckDBPyRelation::alias, "Rename the relation object to new alias", py::arg("alias"))
986: 	    .def("order", &DuckDBPyRelation::order, "Reorder the relation object by order_expr", py::arg("order_expr"))
987: 	    .def("aggregate", &DuckDBPyRelation::aggregate, "Compute the aggregate aggr_expr by the optional groups group_expr on the relation", py::arg("aggr_expr"),
988: 	         py::arg("group_expr") = "")
989: 	    .def("union", &DuckDBPyRelation::union_, "Create the set union of this relation object with another relation object in other_rel")
990: 	    .def("except_", &DuckDBPyRelation::except, "Create the set except of this relation object with another relation object in other_rel", py::arg("other_rel"))
991: 	    .def("intersect", &DuckDBPyRelation::intersect, "Create the set intersection of this relation object with another relation object in other_rel", py::arg("other_rel"))
992: 	    .def("join", &DuckDBPyRelation::join, "Join the relation object with another relation object in other_rel using the join condition expression in join_condition", py::arg("other_rel"),
993: 	         py::arg("join_condition"))
994: 	    .def("distinct", &DuckDBPyRelation::distinct, "Retrieve distinct rows from this relation object")
995: 	    .def("limit", &DuckDBPyRelation::limit, "Only retrieve the first n rows from this relation object", py::arg("n"))
996: 	    .def("query", &DuckDBPyRelation::query, "Run the given SQL query in sql_query on the view named virtual_table_name that refers to the relation object", py::arg("virtual_table_name"),
997: 	         py::arg("sql_query"))
998: 	    .def("execute", &DuckDBPyRelation::execute, "Transform the relation into a result set")
999: 	    .def("write_csv", &DuckDBPyRelation::write_csv, "Write the relation object to a CSV file in file_name", py::arg("file_name"))
1000: 	    .def("insert_into", &DuckDBPyRelation::insert_into, "Inserts the relation object into an existing table named table_name", py::arg("table_name"))
1001: 	    .def("insert", &DuckDBPyRelation::insert, "Inserts the given values into the relation", py::arg("values"))
1002: 	    .def("create", &DuckDBPyRelation::create, "Creates a new table named table_name with the contents of the relation object", py::arg("table_name"))
1003: 	    .def("create_view", &DuckDBPyRelation::create_view, "Creates a view named view_name that refers to the relation object", py::arg("view_name"),
1004: 	         py::arg("replace") = true)
1005: 	    .def("to_df", &DuckDBPyRelation::to_df, "Transforms the relation object into a Data.Frame")
1006: 	    .def("df", &DuckDBPyRelation::to_df, "Transforms the relation object into a Data.Frame")
1007: 	    .def("__str__", &DuckDBPyRelation::print)
1008: 	    .def("__repr__", &DuckDBPyRelation::print)
1009: 	    .def("__getattr__", &DuckDBPyRelation::getattr);
1010: 
1011: 	m.def("from_csv_auto", &DuckDBPyRelation::from_csv_auto, "Creates a relation object from the CSV file in file_name", py::arg("file_name"));
1012: 	m.def("from_parquet", &DuckDBPyRelation::from_parquet, "Creates a relation object from the Parquet file in file_name", py::arg("file_name"));
1013: 	m.def("df", &DuckDBPyRelation::from_df, "Create a relation object from the Data.Frame df", py::arg("df"));
1014: 	m.def("from_df", &DuckDBPyRelation::from_df, "Create a relation object from the Data.Frame df", py::arg("df"));
1015: 	m.def("filter", &DuckDBPyRelation::filter_df, "Filter the Data.Frame df by the filter in filter_expr", py::arg("df"), py::arg("filter_expr"));
1016: 	m.def("project", &DuckDBPyRelation::project_df, "Project the Data.Frame df by the projection in project_expr", py::arg("df"),
1017: 	      py::arg("project_expr"));
1018: 	m.def("alias", &DuckDBPyRelation::alias_df, "Create a relation from Data.Frame df with the passed alias", py::arg("df"), py::arg("alias"));
1019: 	m.def("order", &DuckDBPyRelation::order_df, "Reorder the Data.Frame df by order_expr", py::arg("df"), py::arg("order_expr"));
1020: 	m.def("aggregate", &DuckDBPyRelation::aggregate_df, "Compute the aggregate aggr_expr by the optional groups group_expr on Data.frame df", py::arg("df"),
1021: 	      py::arg("aggr_expr"), py::arg("group_expr") = "");
1022: 	m.def("distinct", &DuckDBPyRelation::distinct_df, "Compute the distinct rows from Data.Frame df ", py::arg("df"));
1023: 	m.def("limit", &DuckDBPyRelation::limit_df, "Retrieve the first n rows from the Data.Frame df", py::arg("df"), py::arg("n"));
1024: 	m.def("query", &DuckDBPyRelation::query_df, "Run the given SQL query in sql_query on the view named virtual_table_name that contains the content of Data.Frame df", py::arg("df"),
1025: 	      py::arg("virtual_table_name"), py::arg("sql_query"));
1026: 	m.def("write_csv", &DuckDBPyRelation::write_csv_df, "Write the Data.Frame df to a CSV file in file_name", py::arg("df"),
1027: 	      py::arg("file_name"));
1028: 
1029: 	// we need this because otherwise we try to remove registered_dfs on shutdown when python is already dead
1030: 	auto clean_default_connection = []() { default_connection_ = nullptr; };
1031: 	m.add_object("_clean_default_connection", py::capsule(clean_default_connection));
1032: 	PyDateTime_IMPORT;
1033: }
[end of tools/pythonpkg/duckdb_python.cpp]
[start of tools/pythonpkg/setup.py]
1: #!/usr/bin/env python
2: # -*- coding: utf-8 -*-
3: 
4: import os
5: import sys
6: import subprocess
7: import shutil
8: import platform
9: 
10: 
11: import distutils.spawn
12: from setuptools import setup, Extension
13: from setuptools.command.sdist import sdist
14: 
15: 
16: # make sure we are in the right directory
17: os.chdir(os.path.dirname(os.path.realpath(__file__)))
18: 
19: # check if amalgamation exists
20: if os.path.isfile(os.path.join('..', '..', 'scripts', 'amalgamation.py')):
21:     prev_wd = os.getcwd()
22:     target_header = os.path.join(prev_wd, 'duckdb.hpp')
23:     target_source = os.path.join(prev_wd, 'duckdb.cpp')
24:     os.chdir(os.path.join('..', '..'))
25:     sys.path.append('scripts')
26:     import amalgamation
27:     amalgamation.generate_amalgamation(target_source, target_header)
28: 
29:     sys.path.append('extension/parquet')
30:     import parquet_amalgamation
31:     ext_target_header = os.path.join(prev_wd, 'parquet-extension.hpp')
32:     ext_target_source = os.path.join(prev_wd, 'parquet-extension.cpp')
33:     parquet_amalgamation.generate_amalgamation(ext_target_source, ext_target_header)
34: 
35:     os.chdir(prev_wd)
36: 
37: 
38: toolchain_args = ['-std=c++11']
39: #toolchain_args = ['-std=c++11', '-Wall', '-O0', '-g']
40: 
41: if platform.system() == 'Darwin':
42:     toolchain_args.extend(['-stdlib=libc++', '-mmacosx-version-min=10.7'])
43: 
44: class get_pybind_include(object):
45: 
46:     def __init__(self, user=False):
47:         self.user = user
48: 
49:     def __str__(self):
50:         import pybind11
51:         return pybind11.get_include(self.user)
52: 
53: class get_numpy_include(object):
54:     def __str__(self):
55:         import numpy
56:         return numpy.get_include()
57: 
58: 
59: libduckdb = Extension('duckdb',
60:     include_dirs=['.', get_numpy_include(), get_pybind_include(), get_pybind_include(user=True)],
61:     sources=['duckdb_python.cpp', 'duckdb.cpp', 'parquet-extension.cpp'],
62:     extra_compile_args=toolchain_args,
63:     extra_link_args=toolchain_args,
64:     language='c++')
65: 
66: # Only include pytest-runner in setup_requires if we're invoking tests
67: if {'pytest', 'test', 'ptr'}.intersection(sys.argv):
68:     setup_requires = ['pytest-runner']
69: else:
70:     setup_requires = []
71: 
72: setuptools_scm_conf = {"root": "../..", "relative_to": __file__}
73: if os.getenv('SETUPTOOLS_SCM_NO_LOCAL', 'no') != 'no':
74:     setuptools_scm_conf['local_scheme'] = 'no-local-version'
75: 
76: setup(
77:     name = "duckdb",
78:     description = 'DuckDB embedded database',
79:     keywords = 'DuckDB Database SQL OLAP',
80:     url="https://www.duckdb.org",
81:     long_description = 'See here for an introduction: https://duckdb.org/docs/api/python',
82:     install_requires=[ # these version is still available for Python 2, newer ones aren't
83:          'numpy>=1.14'
84:     ],
85:     packages=['duckdb_query_graph'],
86:     include_package_data=True,
87:     setup_requires=setup_requires + ["setuptools_scm"] + ['pybind11>=2.4'],
88:     use_scm_version = setuptools_scm_conf,
89:     tests_require=['pytest'],
90:     classifiers = [
91:         'Topic :: Database :: Database Engines/Servers',
92:         'Intended Audience :: Developers'
93:     ],
94:     ext_modules = [libduckdb],
95:     maintainer = "Hannes Muehleisen",
96:     maintainer_email = "hannes@cwi.nl"
97: )
[end of tools/pythonpkg/setup.py]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: