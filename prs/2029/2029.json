{
  "repo": "duckdb/duckdb",
  "pull_number": 2029,
  "instance_id": "duckdb__duckdb-2029",
  "issue_numbers": [
    "1835"
  ],
  "base_commit": "dfb1bf381ad1741c68cb2bd6872776b3c86648ff",
  "patch": "diff --git a/.github/workflows/main.yml b/.github/workflows/main.yml\nindex 5191b0a8142f..60256e959d15 100644\n--- a/.github/workflows/main.yml\n+++ b/.github/workflows/main.yml\n@@ -879,7 +879,7 @@ jobs:\n       CIBW_BUILD: 'cp36-* cp37-* cp38-* cp39-*'\n       CIBW_BEFORE_BUILD: 'pip install --prefer-binary \"pandas>=0.24\" \"pytest>=4.3\"'\n       CIBW_TEST_REQUIRES: 'pytest'\n-      CIBW_BEFORE_TEST: 'pip install --prefer-binary \"pandas>=0.24\" && pip install --prefer-binary \"requests>=2.26\" && (pip install --prefer-binary \"pyarrow>=4.0.0\" || true)'\n+      CIBW_BEFORE_TEST: 'pip install --prefer-binary \"pandas>=0.24\" && pip install --prefer-binary \"requests>=2.26\" && (pip install --extra-index-url https://pypi.fury.io/arrow-nightlies/ --prefer-binary --pre pyarrow || true)'\n       CIBW_TEST_COMMAND: 'python -m pytest {project}/tests'\n       SETUPTOOLS_SCM_NO_LOCAL: 'yes'\n       TWINE_USERNAME: 'hfmuehleisen'\n@@ -919,7 +919,7 @@ jobs:\n       CIBW_BUILD: 'cp36-* cp37-* cp38-* cp39-*'\n       CIBW_BEFORE_BUILD: 'yum install -y openssl-devel && pip install --prefer-binary \"pandas>=0.24\" \"pytest>=4.3\"'\n       CIBW_TEST_REQUIRES: 'pytest'\n-      CIBW_BEFORE_TEST: 'yum install -y openssl && pip install --prefer-binary \"pandas>=0.24\"  && pip install --prefer-binary \"requests>=2.26\" && (pip install --prefer-binary \"pyarrow>=4.0.0\" || true)'\n+      CIBW_BEFORE_TEST: 'yum install -y openssl && pip install --prefer-binary \"pandas>=0.24\"  && pip install --prefer-binary \"requests>=2.26\" && (pip install --extra-index-url https://pypi.fury.io/arrow-nightlies/ --prefer-binary --pre pyarrow || true)'\n       CIBW_TEST_COMMAND: 'python -m pytest {project}/tests'\n       CIBW_ENVIRONMENT: 'BUILD_HTTPFS=1'\n       SETUPTOOLS_SCM_NO_LOCAL: 'yes'\n@@ -953,7 +953,7 @@ jobs:\n       CIBW_BUILD: 'cp36-* cp37-* cp38-* cp39-*'\n       CIBW_BEFORE_BUILD: 'pip install --prefer-binary \"pandas>=0.24\" \"pytest>=4.3\"'\n       CIBW_TEST_REQUIRES: 'pytest'\n-      CIBW_BEFORE_TEST: 'pip install --prefer-binary \"pandas>=0.24\" \"requests>=2.26\" \"pyarrow>=4.0.0\"'\n+      CIBW_BEFORE_TEST: 'pip install --prefer-binary \"pandas>=0.24\" \"requests>=2.26\" && (pip install --extra-index-url https://pypi.fury.io/arrow-nightlies/ --prefer-binary --pre pyarrow || true)'\n       CIBW_TEST_COMMAND: 'python -m pytest {project}/tests'\n       CIBW_ARCHS_MACOS: 'x86_64 universal2 arm64'\n       SETUPTOOLS_SCM_NO_LOCAL: 'yes'\ndiff --git a/src/function/table/arrow.cpp b/src/function/table/arrow.cpp\nindex db2d43060c20..7f2757201d9f 100644\n--- a/src/function/table/arrow.cpp\n+++ b/src/function/table/arrow.cpp\n@@ -16,6 +16,7 @@\n #include \"utf8proc_wrapper.hpp\"\n \n #include \"duckdb/common/operator/multiply.hpp\"\n+#include \"duckdb/common/mutex.hpp\"\n namespace duckdb {\n \n LogicalType GetArrowLogicalType(ArrowSchema &schema,\n@@ -1034,9 +1035,12 @@ bool ArrowTableFunction::ArrowScanParallelStateNext(ClientContext &context, cons\n                                                     ParallelState *parallel_state_p) {\n \tauto &bind_data = (const ArrowScanFunctionData &)*bind_data_p;\n \tauto &state = (ArrowScanState &)*operator_state;\n-\n+\tauto &parallel_state = (ParallelArrowScanState &)*parallel_state_p;\n+\tlock_guard<mutex> parallel_lock(parallel_state.lock);\n \tstate.chunk_offset = 0;\n+\n \tstate.chunk = bind_data.stream->GetNextChunk();\n+\n \t//! have we run out of chunks? we are done\n \tif (!state.chunk->arrow_array.release) {\n \t\treturn false;\ndiff --git a/src/include/duckdb/function/table/arrow.hpp b/src/include/duckdb/function/table/arrow.hpp\nindex 96951f637b65..837e5b952ef7 100644\n--- a/src/include/duckdb/function/table/arrow.hpp\n+++ b/src/include/duckdb/function/table/arrow.hpp\n@@ -66,7 +66,7 @@ struct ArrowScanState : public FunctionOperatorData {\n struct ParallelArrowScanState : public ParallelState {\n \tParallelArrowScanState() {\n \t}\n-\tbool finished = false;\n+\tstd::mutex lock;\n };\n \n struct ArrowTableFunction {\ndiff --git a/tools/pythonpkg/src/arrow_array_stream.cpp b/tools/pythonpkg/src/arrow_array_stream.cpp\nindex 9a230b60ff01..c89c4a82b642 100644\n--- a/tools/pythonpkg/src/arrow_array_stream.cpp\n+++ b/tools/pythonpkg/src/arrow_array_stream.cpp\n@@ -4,98 +4,29 @@\n \n namespace duckdb {\n \n-PythonTableArrowArrayStream::PythonTableArrowArrayStream(PyObject *arrow_table_p,\n-                                                         PythonTableArrowArrayStreamFactory *factory)\n-    : factory(factory), arrow_table(arrow_table_p), chunk_idx(0) {\n-\tstream = make_unique<ArrowArrayStreamWrapper>();\n-\tInitializeFunctionPointers(&stream->arrow_array_stream);\n-\tpy::handle table_handle(arrow_table_p);\n-\tbatches = table_handle.attr(\"to_batches\")();\n-\tpy::int_ num_rows_func = -1;\n-\tif (py::hasattr(table_handle, \"num_rows\")) {\n-\t\tnum_rows_func = table_handle.attr(\"num_rows\");\n-\t}\n-\tstream->number_of_rows = num_rows_func;\n-\n-\tstream->arrow_array_stream.private_data = this;\n-}\n-\n-void PythonTableArrowArrayStream::InitializeFunctionPointers(ArrowArrayStream *stream) {\n-\tstream->get_schema = PythonTableArrowArrayStream::GetSchema;\n-\tstream->get_next = PythonTableArrowArrayStream::GetNext;\n-\tstream->release = PythonTableArrowArrayStream::Release;\n-\tstream->get_last_error = PythonTableArrowArrayStream::GetLastError;\n-}\n-\n unique_ptr<ArrowArrayStreamWrapper> PythonTableArrowArrayStreamFactory::Produce(uintptr_t factory_ptr) {\n \tpy::gil_scoped_acquire acquire;\n \tPythonTableArrowArrayStreamFactory *factory = (PythonTableArrowArrayStreamFactory *)factory_ptr;\n \tif (!factory->arrow_table) {\n \t\treturn nullptr;\n \t}\n-\t//! This is a bit hacky, but has to be this way to hide pybind from the main duckdb lib\n-\tauto table_stream = new PythonTableArrowArrayStream(factory->arrow_table, factory);\n-\treturn move(table_stream->stream);\n+\tpy::handle table(factory->arrow_table);\n+\tpy::object scanner;\n+\tpy::object arrow_scanner = py::module_::import(\"pyarrow.dataset\").attr(\"Scanner\").attr(\"from_dataset\");\n+\tauto py_object_type = string(py::str(table.get_type().attr(\"__name__\")));\n+\n+\tif (py_object_type == \"Table\") {\n+\t\tauto arrow_dataset = py::module_::import(\"pyarrow.dataset\").attr(\"dataset\");\n+\t\tauto dataset = arrow_dataset(table);\n+\t\tscanner = arrow_scanner(dataset);\n+\t} else {\n+\t\tscanner = arrow_scanner(table);\n+\t}\n+\tauto record_batches = scanner.attr(\"to_reader\")();\n+\tauto res = make_unique<ArrowArrayStreamWrapper>();\n+\tauto export_to_c = record_batches.attr(\"_export_to_c\");\n+\texport_to_c((uint64_t)&res->arrow_array_stream);\n+\treturn res;\n }\n \n-int PythonTableArrowArrayStream::PythonTableArrowArrayStream::GetSchema(ArrowArrayStream *stream,\n-                                                                        struct ArrowSchema *out) {\n-\tD_ASSERT(stream->private_data);\n-\tpy::gil_scoped_acquire acquire;\n-\tauto my_stream = (PythonTableArrowArrayStream *)stream->private_data;\n-\tif (!stream->release) {\n-\t\tmy_stream->last_error = \"stream was released\";\n-\t\treturn -1;\n-\t}\n-\tpy::handle table_handle(my_stream->arrow_table);\n-\tauto schema = table_handle.attr(\"schema\");\n-\tif (!py::hasattr(schema, \"_export_to_c\")) {\n-\t\tmy_stream->last_error = \"failed to acquire export_to_c function\";\n-\t\treturn -1;\n-\t}\n-\tauto export_to_c = schema.attr(\"_export_to_c\");\n-\texport_to_c((uint64_t)out);\n-\treturn 0;\n-}\n-\n-int PythonTableArrowArrayStream::GetNext(struct ArrowArrayStream *stream, struct ArrowArray *out) {\n-\tD_ASSERT(stream->private_data);\n-\tpy::gil_scoped_acquire acquire;\n-\tauto my_stream = (PythonTableArrowArrayStream *)stream->private_data;\n-\tif (!stream->release) {\n-\t\tmy_stream->last_error = \"stream was released\";\n-\t\treturn -1;\n-\t}\n-\tif (my_stream->chunk_idx >= py::len(my_stream->batches)) {\n-\t\tout->release = nullptr;\n-\t\treturn 0;\n-\t}\n-\tauto stream_batch = my_stream->batches[my_stream->chunk_idx++];\n-\tif (!py::hasattr(stream_batch, \"_export_to_c\")) {\n-\t\tmy_stream->last_error = \"failed to acquire export_to_c function\";\n-\t\treturn -1;\n-\t}\n-\tauto export_to_c = stream_batch.attr(\"_export_to_c\");\n-\texport_to_c((uint64_t)out);\n-\treturn 0;\n-}\n-\n-void PythonTableArrowArrayStream::Release(struct ArrowArrayStream *stream) {\n-\tpy::gil_scoped_acquire acquire;\n-\tif (!stream->release) {\n-\t\treturn;\n-\t}\n-\tstream->release = nullptr;\n-\tauto private_data = (PythonTableArrowArrayStream *)stream->private_data;\n-\tdelete (PythonTableArrowArrayStream *)stream->private_data;\n-}\n-\n-const char *PythonTableArrowArrayStream::GetLastError(struct ArrowArrayStream *stream) {\n-\tif (!stream->release) {\n-\t\treturn \"stream was released\";\n-\t}\n-\tD_ASSERT(stream->private_data);\n-\tauto my_stream = (PythonTableArrowArrayStream *)stream->private_data;\n-\treturn my_stream->last_error.c_str();\n-}\n } // namespace duckdb\n\\ No newline at end of file\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/arrow_array_stream.hpp b/tools/pythonpkg/src/include/duckdb_python/arrow_array_stream.hpp\nindex 443a2c2fe586..2befdef306c9 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/arrow_array_stream.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/arrow_array_stream.hpp\n@@ -20,24 +20,4 @@ class PythonTableArrowArrayStreamFactory {\n \tstatic unique_ptr<ArrowArrayStreamWrapper> Produce(uintptr_t factory);\n \tPyObject *arrow_table;\n };\n-\n-class PythonTableArrowArrayStream {\n-public:\n-\texplicit PythonTableArrowArrayStream(PyObject *arrow_table, PythonTableArrowArrayStreamFactory *factory);\n-\n-\tunique_ptr<ArrowArrayStreamWrapper> stream;\n-\tPythonTableArrowArrayStreamFactory *factory;\n-\n-private:\n-\tstatic void InitializeFunctionPointers(ArrowArrayStream *stream);\n-\tstatic int GetSchema(struct ArrowArrayStream *stream, struct ArrowSchema *out);\n-\tstatic int GetNext(struct ArrowArrayStream *stream, struct ArrowArray *out);\n-\tstatic void Release(struct ArrowArrayStream *stream);\n-\tstatic const char *GetLastError(struct ArrowArrayStream *stream);\n-\n-\tstd::string last_error;\n-\tPyObject *arrow_table;\n-\tpy::list batches;\n-\tstd::atomic<idx_t> chunk_idx;\n-};\n } // namespace duckdb\n\\ No newline at end of file\ndiff --git a/tools/pythonpkg/src/pyconnection.cpp b/tools/pythonpkg/src/pyconnection.cpp\nindex 5445aaf62eac..5aaf50803845 100644\n--- a/tools/pythonpkg/src/pyconnection.cpp\n+++ b/tools/pythonpkg/src/pyconnection.cpp\n@@ -167,10 +167,6 @@ DuckDBPyConnection *DuckDBPyConnection::RegisterArrow(const string &name, py::ob\n \tif (!connection) {\n \t\tthrow std::runtime_error(\"connection closed\");\n \t}\n-\tauto py_object_type = string(py::str(table.get_type().attr(\"__name__\")));\n-\tif (table.is_none() || (py_object_type != \"Table\" && py_object_type != \"FileSystemDataset\")) {\n-\t\tthrow std::runtime_error(\"Only arrow tables/datasets are supported\");\n-\t}\n \tauto stream_factory = make_unique<PythonTableArrowArrayStreamFactory>(table.ptr());\n \n \tauto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;\n@@ -270,13 +266,6 @@ unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromArrowTable(py::object &tabl\n \t\tthrow std::runtime_error(\"connection closed\");\n \t}\n \tpy::gil_scoped_acquire acquire;\n-\n-\t// the following is a careful dance around having to depend on pyarrow\n-\tauto py_object_type = string(py::str(table.get_type().attr(\"__name__\")));\n-\tif (table.is_none() || (py_object_type != \"Table\" && py_object_type != \"FileSystemDataset\")) {\n-\t\tthrow std::runtime_error(\"Only arrow tables/datasets are supported\");\n-\t}\n-\n \tstring name = \"arrow_table_\" + GenerateRandomName();\n \n \tauto stream_factory = make_unique<PythonTableArrowArrayStreamFactory>(table.ptr());\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/arrow/data/unsigned.parquet b/tools/pythonpkg/tests/arrow/data/unsigned.parquet\nindex 3a9bc2db2d6f..92a7378d21ca 100644\nBinary files a/tools/pythonpkg/tests/arrow/data/unsigned.parquet and b/tools/pythonpkg/tests/arrow/data/unsigned.parquet differ\ndiff --git a/tools/pythonpkg/tests/arrow/test_integration.py b/tools/pythonpkg/tests/arrow/test_integration.py\nindex 9e60c1009dea..4699ff08dec2 100644\n--- a/tools/pythonpkg/tests/arrow/test_integration.py\n+++ b/tools/pythonpkg/tests/arrow/test_integration.py\n@@ -37,98 +37,98 @@ def test_parquet_roundtrip(self, duckdb_cursor):\n             assert rel_from_arrow.equals(rel_from_arrow2, check_metadata=True)\n             assert rel_from_arrow.equals(rel_from_duckdb, check_metadata=True)\n \n-    def test_unsigned_roundtrip(self,duckdb_cursor):\n-        if not can_run:\n-            return\n-        parquet_filename = os.path.join(os.path.dirname(os.path.realpath(__file__)),'data','unsigned.parquet')\n-        data = (pyarrow.array([1,2,3,4,5,255], type=pyarrow.uint8()),pyarrow.array([1,2,3,4,5,65535], \\\n-            type=pyarrow.uint16()),pyarrow.array([1,2,3,4,5,4294967295], type=pyarrow.uint32()),\\\n-                pyarrow.array([1,2,3,4,5,18446744073709551615], type=pyarrow.uint64()))\n+    # def test_unsigned_roundtrip(self,duckdb_cursor):\n+    #     if not can_run:\n+    #         return\n+    #     parquet_filename = os.path.join(os.path.dirname(os.path.realpath(__file__)),'data','unsigned.parquet')\n+    #     data = (pyarrow.array([1,2,3,4,5,255], type=pyarrow.uint8()),pyarrow.array([1,2,3,4,5,65535], \\\n+    #         type=pyarrow.uint16()),pyarrow.array([1,2,3,4,5,4294967295], type=pyarrow.uint32()),\\\n+    #             pyarrow.array([1,2,3,4,5,18446744073709551615], type=pyarrow.uint64()))\n \n-        tbl = pyarrow.Table.from_arrays([data[0],data[1],data[2],data[3]],['a','b','c','d'])\n-        pyarrow.parquet.write_table(tbl, parquet_filename)\n+    #     tbl = pyarrow.Table.from_arrays([data[0],data[1],data[2],data[3]],['a','b','c','d'])\n+    #     pyarrow.parquet.write_table(tbl, parquet_filename)\n \n-        cols = 'a, b, c, d'\n+    #     cols = 'a, b, c, d'\n \n-        unsigned_parquet_table = pyarrow.parquet.read_table(parquet_filename)\n-        unsigned_parquet_table.validate(full=True)\n-        rel_from_arrow = duckdb.arrow(unsigned_parquet_table).project(cols).arrow()\n-        rel_from_arrow.validate(full=True)\n+    #     unsigned_parquet_table = pyarrow.parquet.read_table(parquet_filename)\n+    #     unsigned_parquet_table.validate(full=True)\n+    #     rel_from_arrow = duckdb.arrow(unsigned_parquet_table).project(cols).arrow()\n+    #     rel_from_arrow.validate(full=True)\n \n-        rel_from_duckdb = duckdb.from_parquet(parquet_filename).project(cols).arrow()\n-        rel_from_duckdb.validate(full=True)\n+    #     rel_from_duckdb = duckdb.from_parquet(parquet_filename).project(cols).arrow()\n+    #     rel_from_duckdb.validate(full=True)\n \n-        assert rel_from_arrow.equals(rel_from_duckdb, check_metadata=True)\n+    #     assert rel_from_arrow.equals(rel_from_duckdb, check_metadata=True)\n \n-        con = duckdb.connect()\n-        con.execute(\"select NULL c_null, (c % 4 = 0)::bool c_bool, (c%128)::tinyint c_tinyint, c::smallint*1000 c_smallint, c::integer*100000 c_integer, c::bigint*1000000000000 c_bigint, c::float c_float, c::double c_double, 'c_' || c::string c_string from (select case when range % 2 == 0 then range else null end as c from range(-10000, 10000)) sq\")\n-        arrow_result = con.fetch_arrow_table()\n-        arrow_result.validate(full=True)\n-        arrow_result.combine_chunks()\n-        arrow_result.validate(full=True)\n+    #     con = duckdb.connect()\n+    #     con.execute(\"select NULL c_null, (c % 4 = 0)::bool c_bool, (c%128)::tinyint c_tinyint, c::smallint*1000 c_smallint, c::integer*100000 c_integer, c::bigint*1000000000000 c_bigint, c::float c_float, c::double c_double, 'c_' || c::string c_string from (select case when range % 2 == 0 then range else null end as c from range(-10000, 10000)) sq\")\n+    #     arrow_result = con.fetch_arrow_table()\n+    #     arrow_result.validate(full=True)\n+    #     arrow_result.combine_chunks()\n+    #     arrow_result.validate(full=True)\n \n-        round_tripping = duckdb.from_arrow_table(arrow_result).to_arrow_table()\n-        round_tripping.validate(full=True)\n+    #     round_tripping = duckdb.from_arrow_table(arrow_result).to_arrow_table()\n+    #     round_tripping.validate(full=True)\n \n-        assert round_tripping.equals(arrow_result, check_metadata=True)\n+    #     assert round_tripping.equals(arrow_result, check_metadata=True)\n \n-    def test_decimals_roundtrip(self,duckdb_cursor):\n-        if not can_run:\n-            return\n+    # def test_decimals_roundtrip(self,duckdb_cursor):\n+    #     if not can_run:\n+    #         return\n \n-        duckdb_conn = duckdb.connect()\n+    #     duckdb_conn = duckdb.connect()\n \n-        duckdb_conn.execute(\"CREATE TABLE test (a DECIMAL(4,2), b DECIMAL(9,2), c DECIMAL (18,2), d DECIMAL (30,2))\")\n+    #     duckdb_conn.execute(\"CREATE TABLE test (a DECIMAL(4,2), b DECIMAL(9,2), c DECIMAL (18,2), d DECIMAL (30,2))\")\n \n-        duckdb_conn.execute(\"INSERT INTO  test VALUES (1.11,1.11,1.11,1.11),(NULL,NULL,NULL,NULL)\")\n+    #     duckdb_conn.execute(\"INSERT INTO  test VALUES (1.11,1.11,1.11,1.11),(NULL,NULL,NULL,NULL)\")\n \n-        true_result = duckdb_conn.execute(\"SELECT sum(a), sum(b), sum(c),sum(d) from test\").fetchall()\n+    #     true_result = duckdb_conn.execute(\"SELECT sum(a), sum(b), sum(c),sum(d) from test\").fetchall()\n \n-        duck_tbl = duckdb_conn.table(\"test\")\n+    #     duck_tbl = duckdb_conn.table(\"test\")\n \n-        duck_from_arrow = duckdb_conn.from_arrow_table(duck_tbl.arrow())\n+    #     duck_from_arrow = duckdb_conn.from_arrow_table(duck_tbl.arrow())\n \n-        duck_from_arrow.create(\"testarrow\")\n+    #     duck_from_arrow.create(\"testarrow\")\n \n-        arrow_result = duckdb_conn.execute(\"SELECT sum(a), sum(b), sum(c),sum(d) from testarrow\").fetchall()\n+    #     arrow_result = duckdb_conn.execute(\"SELECT sum(a), sum(b), sum(c),sum(d) from testarrow\").fetchall()\n \n-        assert(arrow_result == true_result)\n+    #     assert(arrow_result == true_result)\n \n-        arrow_result = duckdb_conn.execute(\"SELECT typeof(a), typeof(b), typeof(c),typeof(d) from testarrow\").fetchone()\n+    #     arrow_result = duckdb_conn.execute(\"SELECT typeof(a), typeof(b), typeof(c),typeof(d) from testarrow\").fetchone()\n \n-        assert (arrow_result[0] == 'DECIMAL(4,2)') \n-        assert (arrow_result[1] == 'DECIMAL(9,2)') \n-        assert (arrow_result[2] == 'DECIMAL(18,2)') \n-        assert (arrow_result[3] == 'DECIMAL(30,2)') \n+    #     assert (arrow_result[0] == 'DECIMAL(4,2)') \n+    #     assert (arrow_result[1] == 'DECIMAL(9,2)') \n+    #     assert (arrow_result[2] == 'DECIMAL(18,2)') \n+    #     assert (arrow_result[3] == 'DECIMAL(30,2)') \n \n-        #Lets also test big number comming from arrow land\n-        data = (pyarrow.array(np.array([9999999999999999999999999999999999]), type=pyarrow.decimal128(38,0)))\n-        arrow_tbl = pyarrow.Table.from_arrays([data],['a'])\n-        duckdb_conn = duckdb.connect()\n-        duckdb_conn.from_arrow_table(arrow_tbl).create(\"bigdecimal\")\n-        result = duckdb_conn.execute('select * from bigdecimal')\n-        assert (result.fetchone()[0] == 9999999999999999999999999999999999)\n+    #     #Lets also test big number comming from arrow land\n+    #     data = (pyarrow.array(np.array([9999999999999999999999999999999999]), type=pyarrow.decimal128(38,0)))\n+    #     arrow_tbl = pyarrow.Table.from_arrays([data],['a'])\n+    #     duckdb_conn = duckdb.connect()\n+    #     duckdb_conn.from_arrow_table(arrow_tbl).create(\"bigdecimal\")\n+    #     result = duckdb_conn.execute('select * from bigdecimal')\n+    #     assert (result.fetchone()[0] == 9999999999999999999999999999999999)\n \n-    def test_strings_roundtrip(self,duckdb_cursor):\n-        if not can_run:\n-            return\n+    # def test_strings_roundtrip(self,duckdb_cursor):\n+    #     if not can_run:\n+    #         return\n \n-        duckdb_conn = duckdb.connect()\n+    #     duckdb_conn = duckdb.connect()\n \n-        duckdb_conn.execute(\"CREATE TABLE test (a varchar)\")\n+    #     duckdb_conn.execute(\"CREATE TABLE test (a varchar)\")\n \n-        # Test Small, Null and Very Big String\n-        for i in range (0,1000):\n-            duckdb_conn.execute(\"INSERT INTO  test VALUES ('Matt Damon'),(NULL), ('Jeffffreeeey Jeeeeef Baaaaaaazos'), ('X-Content-Type-Options')\")\n+    #     # Test Small, Null and Very Big String\n+    #     for i in range (0,1000):\n+    #         duckdb_conn.execute(\"INSERT INTO  test VALUES ('Matt Damon'),(NULL), ('Jeffffreeeey Jeeeeef Baaaaaaazos'), ('X-Content-Type-Options')\")\n \n-        true_result = duckdb_conn.execute(\"SELECT * from test\").fetchall()\n+    #     true_result = duckdb_conn.execute(\"SELECT * from test\").fetchall()\n \n-        duck_tbl = duckdb_conn.table(\"test\")\n+    #     duck_tbl = duckdb_conn.table(\"test\")\n \n-        duck_from_arrow = duckdb_conn.from_arrow_table(duck_tbl.arrow())\n+    #     duck_from_arrow = duckdb_conn.from_arrow_table(duck_tbl.arrow())\n \n-        duck_from_arrow.create(\"testarrow\")\n+    #     duck_from_arrow.create(\"testarrow\")\n \n-        arrow_result = duckdb_conn.execute(\"SELECT * from testarrow\").fetchall()\n+    #     arrow_result = duckdb_conn.execute(\"SELECT * from testarrow\").fetchall()\n \n-        assert(arrow_result == true_result)\n+    #     assert(arrow_result == true_result)\ndiff --git a/tools/pythonpkg/tests/arrow/test_nested_arrow.py b/tools/pythonpkg/tests/arrow/test_nested_arrow.py\nindex 57132c5e8ad0..2ca5cb412da3 100644\n--- a/tools/pythonpkg/tests/arrow/test_nested_arrow.py\n+++ b/tools/pythonpkg/tests/arrow/test_nested_arrow.py\n@@ -162,12 +162,6 @@ def test_frankstein_nested(self,duckdb_cursor):\n         # Map with struct as key and/or value\n         compare_results(\"SELECT MAP(LIST_VALUE({'i':1,'j':2},{'i':3,'j':4}),LIST_VALUE({'i':1,'j':2},{'i':3,'j':4}))\")\n \n-        # Struct that is NULL entirely\n-        compare_results(\"SELECT * FROM (VALUES ({'i':1,'j':2}), (NULL), ({'i':1,'j':2}), (NULL)) as a\")\n-\n         # Null checks on lists with structs \n         compare_results(\"SELECT [{'i':1,'j':[2,3]},NULL,{'i':1,'j':[2,3]}]\")\n         \n-        # MAP that is NULL entirely \n-        compare_results(\"SELECT * FROM (VALUES (MAP(LIST_VALUE(1,2),LIST_VALUE(3,4))),(NULL), (MAP(LIST_VALUE(1,2),LIST_VALUE(3,4))), (NULL)) as a\")\n-\n",
  "problem_statement": "Arrow dataset + aggregation + multithreading\n**What does happen?**\r\nI'm testing out the new abilities to query an Arrow source from DuckDB. Things work well for Tables and record batches, however I've found that with datasets, when using aggregations, with multithreading I get hangs.\r\n\r\nThe following will (almost always) hang on the last line:\r\n```\r\nds <- arrow::open_dataset(\"userdata1.parquet\")\r\nduckdb::duckdb_register_arrow(con, \"mydatasetreader\", ds)\r\ndbExecute(con, \"PRAGMA threads=4\")\r\ndbGetQuery(con, \"SELECT count(*) FROM mydatasetreader\")\r\n```\r\n\r\nI've noticed that this will _sometimes_ succeed (with a correct result), though when it hangs it hangs forever. I've tried smaller numbers of threads (2, 3) and those seem to work slightly more frequently \u2014 1 thread always works. Higher numbers of threads (e.g. 8) always hang. \r\n\r\nI'm happy to dig more / debug on my end if there are obvious places I should look.\r\n\r\n**What should happen?**\r\nReturn the result of the query without hanging.\r\n\r\n**To Reproduce**\r\nThis code (in the style of / could be inserted into duckdb/tools/rpkg/tests/testthat/test_register_arrow.R)\r\n```\r\ntest_that(\"duckdb_register_arrow() works with datasets\", {\r\n    skip_on_os(\"windows\")\r\n    skip_if_not_installed(\"arrow\", \"4.0.1\")\r\n    con <- dbConnect(duckdb::duckdb())\r\n\r\n    # Registering a dataset + aggregation\r\n    ds <- arrow::open_dataset(\"userdata1.parquet\")\r\n    duckdb::duckdb_register_arrow(con, \"mydatasetreader\", ds)\r\n    res1 <- dbGetQuery(con, \"SELECT count(*) FROM mydatasetreader\")\r\n    res2 <- dbGetQuery(con, \"SELECT count(*) FROM parquet_scan('userdata1.parquet')\")\r\n    expect_true(identical(res1, res2))\r\n    # we can read with > 3 cores\r\n    dbExecute(con, \"PRAGMA threads=4\")\r\n    res3 <- dbGetQuery(con, \"SELECT count(*) FROM mydatasetreader\")\r\n    expect_true(identical(res2, res3))\r\n    duckdb::duckdb_unregister_arrow(con, \"mydatasetreader\")\r\n\r\n    dbDisconnect(con, shutdown = T)\r\n})\r\n```\r\n\r\n**Environment (please complete the following information):**\r\n - OS: macOS 11.3.1 on a Mac Pro 6-core Xeon E5\r\n - DuckDB Version: the `master` branch from today\r\n\r\n**Before submitting**\r\n- [x] Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n- [x] Have you tried this on the latest `master` branch? In case you cannot compile, you may find some binaries here: https://github.com/duckdb/duckdb/releases/tag/master-builds\r\n\n",
  "hints_text": "Thanks for opening the issue @jonkeane. I'll have a look at this soon!\nI'm actually surprised this somewhat works from R land.\r\n\r\nOn Python land, we explicitly only accept table objects, so the following code will give a runtime error.\r\n\r\n```python \r\nuserdata_parquet_dataset= pyarrow.dataset.dataset(parquet_filename, format=\"parquet\")\r\nduckdb_conn.register_arrow(\"arrow_dataset\", userdata_parquet_dataset)\r\n```\r\n\r\nI'm happy to support datasets though, but as a quick measure, I think a similar check should be performed on R-land until this is properly implemented (Maybe @hannesmuehleisen can add this?). Also would avoid the reading of unexpected objects.\r\n\r\n```cpp\r\nauto py_object_type = string(py::str(table.get_type().attr(\"__name__\")));\r\nif (table.is_none() || py_object_type != \"Table\" ) {\r\n\tthrow std::runtime_error(\"Only arrow tables are supported\");\r\n}\r\n```\r\n\r\nRelated to the support of the datasets, we could either internally transform them to arrow_tables, with an extra parameter of the partition number, which is necessary for parallelism, since our current implementation only parallelizes partitions.\nInteresting. From what I understand both datasets and tables in R land are having recordbatchreaders exposed + exported. I believe the interface for those two is the same or similar enough, but I can dig more and confirm that.\r\n\r\nI looked through the duckdb python code, but couldn't find this right away: what scanner are you using on the Python side? I don't see a recordbatchreader, but I might be looking in the wrong place!\nYou don't want to convert a Dataset to a Table: Datasets must not be assumed to fit into memory, as a Table does. Jon is right though, at least in R you aren't actually consuming a Dataset or a Table, you're consuming an ArrowArrayStream struct in both cases.\r\n\r\nAnd if it works to query a dataset with 1 or few threads, then it's not a problem with Dataset per se.",
  "created_at": "2021-07-20T11:28:42Z"
}