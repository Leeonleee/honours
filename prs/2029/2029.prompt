You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Arrow dataset + aggregation + multithreading
**What does happen?**
I'm testing out the new abilities to query an Arrow source from DuckDB. Things work well for Tables and record batches, however I've found that with datasets, when using aggregations, with multithreading I get hangs.

The following will (almost always) hang on the last line:
```
ds <- arrow::open_dataset("userdata1.parquet")
duckdb::duckdb_register_arrow(con, "mydatasetreader", ds)
dbExecute(con, "PRAGMA threads=4")
dbGetQuery(con, "SELECT count(*) FROM mydatasetreader")
```

I've noticed that this will _sometimes_ succeed (with a correct result), though when it hangs it hangs forever. I've tried smaller numbers of threads (2, 3) and those seem to work slightly more frequently â€” 1 thread always works. Higher numbers of threads (e.g. 8) always hang. 

I'm happy to dig more / debug on my end if there are obvious places I should look.

**What should happen?**
Return the result of the query without hanging.

**To Reproduce**
This code (in the style of / could be inserted into duckdb/tools/rpkg/tests/testthat/test_register_arrow.R)
```
test_that("duckdb_register_arrow() works with datasets", {
    skip_on_os("windows")
    skip_if_not_installed("arrow", "4.0.1")
    con <- dbConnect(duckdb::duckdb())

    # Registering a dataset + aggregation
    ds <- arrow::open_dataset("userdata1.parquet")
    duckdb::duckdb_register_arrow(con, "mydatasetreader", ds)
    res1 <- dbGetQuery(con, "SELECT count(*) FROM mydatasetreader")
    res2 <- dbGetQuery(con, "SELECT count(*) FROM parquet_scan('userdata1.parquet')")
    expect_true(identical(res1, res2))
    # we can read with > 3 cores
    dbExecute(con, "PRAGMA threads=4")
    res3 <- dbGetQuery(con, "SELECT count(*) FROM mydatasetreader")
    expect_true(identical(res2, res3))
    duckdb::duckdb_unregister_arrow(con, "mydatasetreader")

    dbDisconnect(con, shutdown = T)
})
```

**Environment (please complete the following information):**
 - OS: macOS 11.3.1 on a Mac Pro 6-core Xeon E5
 - DuckDB Version: the `master` branch from today

**Before submitting**
- [x] Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?
- [x] Have you tried this on the latest `master` branch? In case you cannot compile, you may find some binaries here: https://github.com/duckdb/duckdb/releases/tag/master-builds


</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
2: 
3: ![.github/workflows/main.yml](https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![codecov](https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN)](https://codecov.io/gh/duckdb/duckdb)
6: 
7: 
8: ## Installation
9: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
10: 
11: ## Development
12: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
13: 
14: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
15: 
16: 
[end of README.md]
[start of .github/workflows/main.yml]
1: on: [push, pull_request]
2: 
3: defaults:
4:   run:
5:     shell: bash
6: 
7: env:
8:   GH_TOKEN: ${{ secrets.GH_TOKEN }}
9:   TWINE_PASSWORD: ${{ secrets.TWINE_PASSWORD }}
10:   AWS_ACCESS_KEY_ID: AKIAVBLKPL2ZW2T7TYFQ
11:   AWS_SECRET_ACCESS_KEY: ${{ secrets.NODE_PRE_GYP_SECRETACCESSKEY }}
12:   NODE_AUTH_TOKEN: ${{secrets.NODE_AUTH_TOKEN}}
13: 
14: jobs:
15:   linux-debug:
16:     name: Linux Debug
17:     runs-on: ubuntu-20.04
18: 
19:     env:
20:       CC: gcc-10
21:       CXX: g++-10
22:       TREAT_WARNINGS_AS_ERRORS: 1
23:       GEN: ninja
24: 
25:     steps:
26:     - uses: actions/checkout@v2
27:       with:
28:         fetch-depth: 0
29: 
30:     - name: Install
31:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
32: 
33:     - name: Build
34:       run: BUILD_ARROW_ABI_TEST=1 make debug
35: 
36:     - name: Test
37:       run: make unittestci
38: 
39:   format-check:
40:     name: Format Check
41:     runs-on: ubuntu-20.04
42: 
43:     env:
44:       CC: gcc-10
45:       CXX: g++-10
46:       GEN: ninja
47: 
48:     steps:
49:     - uses: actions/checkout@v2
50:       with:
51:         fetch-depth: 0
52: 
53:     - name: Install
54:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build clang-format && sudo pip3 install cmake-format
55: 
56:     - name: Format Check
57:       run: |
58:         clang-format --version
59:         clang-format --dump-config
60:         make format-check-silent
61: 
62:   tidy-check:
63:     name: Tidy Check
64:     runs-on: ubuntu-20.04
65: 
66:     env:
67:       CC: gcc-10
68:       CXX: g++-10
69:       GEN: ninja
70:       TIDY_THREADS: 4
71: 
72:     steps:
73:     - uses: actions/checkout@v2
74:       with:
75:         fetch-depth: 0
76: 
77:     - name: Install
78:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build clang-tidy && sudo pip3 install pybind11[global]
79: 
80:     - name: Tidy Check
81:       run: make tidy-check
82: 
83:   win-release-64:
84:     name: Windows (64 Bit)
85:     runs-on: windows-latest
86:     needs: linux-debug
87: 
88:     steps:
89:     - uses: actions/checkout@v2
90:       with:
91:         fetch-depth: 0
92: 
93:     - uses: actions/setup-python@v2
94:       with:
95:         python-version: '3.7'
96: 
97:     - name: Build
98:       run: |
99:         python scripts/windows_ci.py
100:         cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_GENERATOR_PLATFORM=x64 -DBUILD_ICU_EXTENSION=1 -DBUILD_PARQUET_EXTENSION=1 -DBUILD_TPCH_EXTENSION=1 -DBUILD_TPCDS_EXTENSION=1 -DBUILD_FTS_EXTENSION=1 -DBUILD_REST=1 -DJDBC_DRIVER=1 -DBUILD_VISUALIZER_EXTENSION=1
101:         cmake --build . --config Release
102: 
103:     - name: Test
104:       run: test/Release/unittest.exe
105: 
106:     - name: Tools Test
107:       run: |
108:         python tools/shell/shell-test.py Release/duckdb.exe
109:         java -cp tools/jdbc/duckdb_jdbc.jar org.duckdb.test.TestDuckDBJDBC
110: 
111:     - name: Deploy
112:       run: |
113:         python scripts/amalgamation.py
114:         choco install zip -y --force
115:         zip -j duckdb_cli-windows-amd64.zip Release/duckdb.exe
116:         zip -j libduckdb-windows-amd64.zip src/Release/duckdb.dll src/amalgamation/duckdb.hpp src/include/duckdb.h
117:         python scripts/asset-upload-gha.py libduckdb-windows-amd64.zip duckdb_cli-windows-amd64.zip duckdb_jdbc-windows-amd64.jar=tools/jdbc/duckdb_jdbc.jar
118: 
119:     - uses: actions/upload-artifact@v2
120:       with:
121:         name: duckdb-binaries-windows
122:         path: |
123:           libduckdb-windows-amd64.zip
124:           duckdb_cli-windows-amd64.zip
125:           tools/jdbc/duckdb_jdbc.jar
126: 
127:     - uses: ilammy/msvc-dev-cmd@v1
128:     - name: Duckdb.dll export symbols with C++ on Windows
129:       run: cl -I src/include examples/embedded-c++-windows/cppintegration.cpp -link src/Release/duckdb.lib
130: 
131:   win-release-32:
132:     name: Windows (32 Bit)
133:     runs-on: windows-latest
134:     needs: linux-debug
135: 
136:     steps:
137:     - uses: actions/checkout@v2
138:       with:
139:         fetch-depth: 0
140: 
141:     - uses: actions/setup-python@v2
142:       with:
143:         python-version: '3.7'
144: 
145:     - name: Build
146:       run: |
147:         cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_GENERATOR_PLATFORM=Win32 -DBUILD_ICU_EXTENSION=1 -DBUILD_PARQUET_EXTENSION=1 -DBUILD_TPCH_EXTENSION=1 -DBUILD_TPCDS_EXTENSION=1 -DBUILD_FTS_EXTENSION=1 -DJDBC_DRIVER=1 -DBUILD_VISUALIZER_EXTENSION=1
148:         cmake --build . --config Release
149: 
150:     - name: Test
151:       run: test/Release/unittest.exe
152: 
153:     - name: Tools Test
154:       run: |
155:         python tools/shell/shell-test.py Release/duckdb.exe
156: 
157:     - name: Deploy
158:       run: |
159:         python scripts/amalgamation.py
160:         choco install zip -y --force
161:         zip -j duckdb_cli-windows-i386.zip Release/duckdb.exe
162:         zip -j libduckdb-windows-i386.zip src/Release/duckdb.dll src/amalgamation/duckdb.hpp src/include/duckdb.h
163:         python scripts/asset-upload-gha.py libduckdb-windows-i386.zip duckdb_cli-windows-i386.zip duckdb_jdbc-windows-i386.jar=tools/jdbc/duckdb_jdbc.jar
164: 
165:     - uses: actions/upload-artifact@v2
166:       with:
167:         name: duckdb-binaries-windows
168:         path: |
169:           libduckdb-windows-i386.zip
170:           duckdb_cli-windows-i386.zip
171:           tools/jdbc/duckdb_jdbc.jar
172: 
173:   mingw:
174:      name: MingW (64 Bit)
175:      runs-on: windows-latest
176:      needs: linux-debug
177:      defaults:
178:        run:
179:          shell: msys2 {0}
180:      steps:
181:        - uses: actions/checkout@v2
182:        - uses: msys2/setup-msys2@v2
183:          with:
184:            msystem: MINGW64
185:            update: true
186:            install: git mingw-w64-x86_64-toolchain mingw-w64-x86_64-cmake mingw-w64-x86_64-ninja git
187:        # see here: https://gist.github.com/scivision/1de4fd6abea9ba6b2d87dc1e86b5d2ce
188:        - name: Put MSYS2_MinGW64 on PATH
189:          # there is not yet an environment variable for this path from msys2/setup-msys2
190:          run: export PATH=D:/a/_temp/msys/msys64/mingw64/bin:$PATH
191: 
192:        - name: Build
193:          run: |
194:            cmake -G "Ninja" -DCMAKE_BUILD_TYPE=Release -DBUILD_PARQUET_EXTENSION=1
195:            cmake --build . --config Release
196: 
197:        - name: Test
198:          run: |
199:            cp src/libduckdb.dll .
200:            test/unittest.exe
201: 
202:   xcode-release:
203:     name: OSX Release
204:     runs-on: macos-latest
205:     needs: linux-debug
206: 
207:     env:
208:       BUILD_VISUALIZER: 1
209:       BUILD_ICU: 1
210:       BUILD_TPCH: 1
211:       BUILD_TPCDS: 1
212:       BUILD_FTS: 1
213:       BUILD_REST: 1
214:       BUILD_JDBC: 1
215:       BUILD_HTTPFS: 1
216:       OPENSSL_ROOT_DIR: /usr/local/opt/openssl/
217: 
218: 
219:     steps:
220:     - uses: actions/checkout@v2
221:       with:
222:         fetch-depth: 0
223: 
224:     - uses: actions/setup-python@v2
225:       with:
226:         python-version: '3.7'
227: 
228:     - name: Build
229:       run: make
230: 
231:     - name: Unit Test
232:       run: make allunit
233: 
234:     - name: Tools Tests
235:       run: |
236:         python tools/shell/shell-test.py build/release/duckdb
237:         java -cp build/release/tools/jdbc/duckdb_jdbc.jar org.duckdb.test.TestDuckDBJDBC
238: 
239:     - name: Examples
240:       run: |
241:         (cd examples/embedded-c; make)
242:         (cd examples/embedded-c++; make)
243:         (cd examples/jdbc; make; make maven)
244: 
245:     - name: Deploy
246:       run: |
247:         python scripts/amalgamation.py
248:         zip -j duckdb_cli-osx-amd64.zip build/release/duckdb
249:         zip -j libduckdb-osx-amd64.zip build/release/src/libduckdb*.dylib src/amalgamation/duckdb.hpp src/include/duckdb.h
250:         python scripts/asset-upload-gha.py libduckdb-osx-amd64.zip duckdb_cli-osx-amd64.zip duckdb_jdbc-osx-amd64.jar=build/release/tools/jdbc/duckdb_jdbc.jar
251: 
252:     - uses: actions/upload-artifact@v2
253:       with:
254:         name: duckdb-binaries-osx
255:         path: |
256:           libduckdb-osx-amd64.zip
257:           duckdb_cli-osx-amd64.zip
258:           build/release/tools/jdbc/duckdb_jdbc.jar
259: 
260: 
261:   xcode-debug:
262:     name: OSX Debug
263:     runs-on: macos-latest
264:     needs: linux-debug
265: 
266:     env:
267:       TREAT_WARNINGS_AS_ERRORS: 1
268: 
269:     steps:
270:     - uses: actions/checkout@v2
271:       with:
272:         fetch-depth: 0
273: 
274:     - name: Build
275:       run: make debug
276: 
277:     - name: Test
278:       run: make unittestci
279: 
280:     - name: Amalgamation
281:       run: |
282:         python scripts/amalgamation.py --extended
283:         python scripts/parquet_amalgamation.py
284:         cd src/amalgamation
285:         clang++ -std=c++11 -emit-llvm -S duckdb.cpp parquet-amalgamation.cpp
286: 
287: 
288:   linux-release-64:
289:     name: Linux (64 Bit)
290:     runs-on: ubuntu-16.04
291:     needs: linux-debug
292: 
293:     env:
294:       GEN: ninja
295:       BUILD_VISUALIZER: 1
296:       BUILD_BENCHMARK: 1
297:       BUILD_ICU: 1
298:       BUILD_TPCH: 1
299:       BUILD_TPCDS: 1
300:       BUILD_FTS: 1
301:       BUILD_REST: 1
302:       BUILD_JDBC: 1
303:       BUILD_HTTPFS: 1
304: 
305:     steps:
306:     - uses: actions/checkout@v2
307:       with:
308:         fetch-depth: 0
309: 
310:     - uses: actions/setup-python@v2
311:       with:
312:         python-version: '3.7'
313: 
314:     - name: Install
315:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
316: 
317:     - name: Build
318:       run: STATIC_LIBCPP=1 make
319: 
320:     - name: Test
321:       run: make allunit
322: 
323:     - name: Symbol Leakage Test
324:       run: python scripts/exported_symbols_check.py build/release/src/libduckdb*.so
325: 
326:     - name: Tools Tests
327:       run: |
328:         python tools/shell/shell-test.py build/release/duckdb
329:         pip install requests
330:         python tools/rest/test_the_rest.py build/release/tools/rest
331:         java -cp build/release/tools/jdbc/duckdb_jdbc.jar org.duckdb.test.TestDuckDBJDBC
332: 
333:     - name: Examples
334:       run: |
335:         (cd examples/embedded-c; make)
336:         (cd examples/embedded-c++; make)
337:         (cd examples/jdbc; make; make maven)
338:         build/release/benchmark/benchmark_runner benchmark/tpch/sf1/q01.benchmark
339: 
340:     - name: Deploy
341:       run: |
342:         python scripts/amalgamation.py
343:         zip -j duckdb_cli-linux-amd64.zip build/release/duckdb
344:         zip -j libduckdb-linux-amd64.zip build/release/src/libduckdb*.so src/amalgamation/duckdb.hpp src/include/duckdb.h
345:         zip -j libduckdb-src.zip src/amalgamation/duckdb.hpp src/amalgamation/duckdb.cpp src/include/duckdb.h
346:         zip -j duckdb_rest-linux-amd64.zip build/release/tools/rest/duckdb_rest_server
347:         python scripts/asset-upload-gha.py libduckdb-src.zip libduckdb-linux-amd64.zip duckdb_cli-linux-amd64.zip duckdb_rest-linux-amd64.zip duckdb_jdbc-linux-amd64.jar=build/release/tools/jdbc/duckdb_jdbc.jar
348: 
349:     - uses: actions/upload-artifact@v2
350:       with:
351:         name: duckdb-binaries-linux
352:         path: |
353:           libduckdb-linux-amd64.zip
354:           duckdb_cli-linux-amd64.zip
355:           build/release/tools/jdbc/duckdb_jdbc.jar
356: 
357: 
358: 
359:   linux-release-32:
360:     name: Linux (32 Bit)
361:     runs-on: ubuntu-16.04
362:     needs: linux-debug
363: 
364:     env:
365:       GEN: ninja
366: 
367:     steps:
368:     - uses: actions/checkout@v2
369:       with:
370:         fetch-depth: 0
371: 
372:     - uses: actions/setup-python@v2
373:       with:
374:         python-version: '3.7'
375: 
376:     - name: Install
377:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build libc6-dev-i386 gcc-multilib g++-multilib lib32readline6-dev
378: 
379:     - name: Build
380:       run: |
381:         mkdir -p build/release
382:         (cd build/release && cmake -DSTATIC_LIBCPP=1 -DJDBC_DRIVER=1 -DBUILD_ICU_EXTENSION=1 -DBUILD_PARQUET_EXTENSION=1 -DBUILD_FTS_EXTENSION=1 -DFORCE_32_BIT=1 -DCMAKE_BUILD_TYPE=Release ../.. && cmake --build .)
383: 
384:     - name: Test
385:       run: build/release/test/unittest "*"
386: 
387:     - name: Deploy
388:       run: |
389:         python scripts/amalgamation.py
390:         zip -j duckdb_cli-linux-i386.zip build/release/duckdb
391:         zip -j libduckdb-linux-i386.zip build/release/src/libduckdb*.so src/amalgamation/duckdb.hpp src/include/duckdb.h
392:         python scripts/asset-upload-gha.py libduckdb-linux-i386.zip duckdb_cli-linux-i386.zip duckdb_jdbc-linux-i386.jar=build/release/tools/jdbc/duckdb_jdbc.jar
393: 
394:     - uses: actions/upload-artifact@v2
395:       with:
396:         name: duckdb-binaries-linux
397:         path: |
398:           libduckdb-linux-i386.zip
399:           duckdb_cli-linux-i386.zip
400:           build/release/tools/jdbc/duckdb_jdbc.jar
401: 
402: 
403:   linux-rpi:
404:     name: Linux (Raspberry Pi)
405:     runs-on: ubuntu-20.04
406:     needs: linux-debug
407: 
408:     steps:
409:     - uses: actions/checkout@v2
410:       with:
411:         fetch-depth: 0
412: 
413:     - uses: actions/setup-python@v2
414:       with:
415:         python-version: '3.7'
416: 
417:     - name: Install
418:       run: |
419:         sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
420:         git clone https://github.com/raspberrypi/tools --depth=1 rpi-tools
421: 
422:     - name: Build
423:       run: |
424:         export TOOLCHAIN=`pwd`/rpi-tools
425:         mkdir -p build/release
426:         cd build/release
427:         cmake -G Ninja -DBUILD_TPCH_EXTENSION=1 -DBUILD_TPCDS_EXTENSION=1 -DDUCKDB_RPI_TOOLCHAIN_PREFIX=$TOOLCHAIN -DBUILD_UNITTESTS=0 -DCMAKE_TOOLCHAIN_FILE=../../scripts/raspberry-pi-cmake-toolchain.cmake ../../
428:         cmake --build .
429:         file duckdb
430: 
431:     - name: Deploy
432:       run: |
433:         python scripts/amalgamation.py
434:         zip -j duckdb_cli-linux-rpi.zip build/release/duckdb
435:         zip -j libduckdb-linux-rpi.zip build/release/src/libduckdb*.so src/amalgamation/duckdb.hpp src/include/duckdb.h
436:         python scripts/asset-upload-gha.py libduckdb-linux-rpi.zip duckdb_cli-linux-rpi.zip
437: 
438:     - uses: actions/upload-artifact@v2
439:       with:
440:         name: duckdb-binaries-rpi
441:         path: |
442:           libduckdb-linux-rpi.zip
443:           duckdb_cli-linux-rpi.zip
444: 
445: 
446:   old-gcc:
447:     name: GCC 4.8
448:     runs-on: ubuntu-18.04
449:     needs: linux-debug
450: 
451:     env:
452:       CC: gcc-4.8
453:       CXX: g++-4.8
454: 
455:     steps:
456:     - uses: actions/checkout@v2
457:       with:
458:         fetch-depth: 0
459: 
460:     - uses: actions/setup-python@v2
461:       with:
462:         python-version: '3.7'
463: 
464:     - name: Install
465:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq g++-4.8 binutils
466: 
467:     - name: Build
468:       run: make release
469: 
470:     - name: Test
471:       run: make allunit
472: 
473: 
474:   centos:
475:     name: CentOS 7
476:     runs-on: ubuntu-latest
477:     container: centos:7
478:     needs: linux-debug
479: 
480:     steps:
481:     - uses: actions/checkout@v2
482:       with:
483:         fetch-depth: 0
484: 
485:     - name: Install
486:       run: yum install -y gcc gcc-c++ git cmake make
487: 
488:     - name: Build
489:       run: make release
490: 
491:     - name: Test
492:       run: ./build/release/test/unittest
493: 
494: 
495:   release-assert:
496:     name: Release Assertions
497:     runs-on: ubuntu-20.04
498:     needs: linux-debug
499: 
500:     env:
501:       CC: gcc-10
502:       CXX: g++-10
503:       GEN: ninja
504:       BUILD_ICU: 1
505:       BUILD_TPCH: 1
506:       BUILD_TPCDS: 1
507:       BUILD_FTS: 1
508:       BUILD_VISUALIZER: 1
509:       DISABLE_SANITIZER: 1
510: 
511:     steps:
512:     - uses: actions/checkout@v2
513:       with:
514:         fetch-depth: 0
515: 
516:     - name: Install
517:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
518: 
519:     - name: Build
520:       run: make relassert
521: 
522:     - name: Test
523:       run: |
524:           python3 scripts/run_tests_one_by_one.py build/relassert/test/unittest "*"
525: 
526:   force-storage:
527:     name: Force Storage
528:     runs-on: ubuntu-20.04
529:     needs: linux-debug
530: 
531:     env:
532:       CC: gcc-10
533:       CXX: g++-10
534:       GEN: ninja
535:       BUILD_ICU: 1
536:       BUILD_PARQUET: 1
537:       BUILD_TPCH: 1
538:       BUILD_TPCDS: 1
539:       BUILD_FTS: 1
540:       BUILD_VISUALIZER: 1
541: 
542:     steps:
543:     - uses: actions/checkout@v2
544:       with:
545:         fetch-depth: 0
546: 
547:     - name: Install
548:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
549: 
550:     - name: Build
551:       run: make reldebug
552: 
553:     - name: Test
554:       run: build/reldebug/test/unittest "*" --force-storage
555: 
556: 
557:   threadsan:
558:     name: Thread Sanitizer
559:     runs-on: ubuntu-20.04
560:     needs: linux-debug
561: 
562:     env:
563:       CC: gcc-10
564:       CXX: g++-10
565:       GEN: ninja
566:       BUILD_ICU: 1
567:       BUILD_TPCH: 1
568:       BUILD_TPCDS: 1
569:       BUILD_FTS: 1
570:       BUILD_VISUALIZER: 1
571:       TSAN_OPTIONS: suppressions=.sanitizer-thread-suppressions.txt
572: 
573:     steps:
574:     - uses: actions/checkout@v2
575:       with:
576:         fetch-depth: 0
577: 
578:     - name: Install
579:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
580: 
581:     - name: Build
582:       run: THREADSAN=1 make reldebug
583: 
584:     - name: Test
585:       run: |
586:           python3 scripts/run_tests_one_by_one.py build/reldebug/test/unittest
587:           python3 scripts/run_tests_one_by_one.py build/reldebug/test/unittest "[intraquery]"
588:           python3 scripts/run_tests_one_by_one.py build/reldebug/test/unittest "[interquery]"
589:           python3 scripts/run_tests_one_by_one.py build/reldebug/test/unittest "[detailed_profiler]"
590:           python3 scripts/run_tests_one_by_one.py build/reldebug/test/unittest test/sql/tpch/tpch_sf01.test_slow
591: 
592:   valgrind:
593:     name: Valgrind
594:     runs-on: ubuntu-20.04
595:     needs: linux-debug
596: 
597:     env:
598:       CC: gcc-10
599:       CXX: g++-10
600:       DISABLE_SANITIZER: 1
601:       GEN: ninja
602: 
603:     steps:
604:     - uses: actions/checkout@v2
605:       with:
606:         fetch-depth: 0
607: 
608:     - name: Install
609:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build valgrind
610: 
611:     - name: Build
612:       run: make debug
613: 
614:     - name: Test
615:       run: valgrind ./build/debug/test/unittest test/sql/tpch/tpch_sf001.test_slow
616: 
617:   codecov:
618:     name: CodeCov
619:     runs-on: ubuntu-20.04
620:     needs: linux-debug
621:     env:
622:       GEN: ninja
623:     steps:
624:       - uses: actions/checkout@v2
625:         with:
626:           fetch-depth: 0
627: 
628:       - name: Install
629:         run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build lcov
630: 
631:       - name: Set up Python 3.9
632:         uses: actions/setup-python@v2
633:         with:
634:           python-version: '3.9'
635: 
636:       - name: Before Install
637:         run: |
638:           pip install requests
639:           sudo apt-get install g++
640: 
641:       - name: Coverage Reset
642:         run: |
643:           lcov --config-file .github/workflows/lcovrc --zerocounters --directory .
644:           lcov --config-file .github/workflows/lcovrc --capture --initial --directory . --base-directory . --no-external --output-file coverage.info
645: 
646:       - name: Run Tests
647:         run: |
648:           mkdir -p build/coverage
649:           (cd build/coverage && cmake -E env CXXFLAGS="--coverage" cmake -DBUILD_ARROW_ABI_TEST=1 -DBUILD_PARQUET_EXTENSION=1 -DENABLE_SANITIZER=0 -DCMAKE_BUILD_TYPE=Debug ../.. && make)
650:           build/coverage/test/unittest
651:           build/coverage/test/unittest "[intraquery]"
652:           build/coverage/test/unittest "[interquery]"
653:           build/coverage/test/unittest "[coverage]"
654:           build/coverage/test/unittest "[detailed_profiler]"
655:           build/coverage/test/unittest "[tpch]"
656:           build/coverage/tools/sqlite3_api_wrapper/test_sqlite3_api_wrapper
657:           python tools/shell/shell-test.py build/coverage/duckdb
658: 
659:       - name: Generate Coverage
660:         run: |
661:           lcov --config-file .github/workflows/lcovrc --directory . --base-directory . --no-external --capture --output-file coverage.info
662:           lcov --config-file .github/workflows/lcovrc --remove coverage.info '/usr*' '*/cl.hpp' '*/tools/*' '*/benchmark/*' '*/examples/*' '*/third_party/*' '*/test/*' -o lcov.info
663: 
664:       - name: CodeCov Upload
665:         uses: codecov/codecov-action@v1
666:         with:
667:           files: lcov.info
668:           fail_ci_if_error: true
669: 
670:   vector-sizes:
671:     name: Vector Sizes
672:     runs-on: ubuntu-20.04
673:     needs: linux-debug
674: 
675:     env:
676:       CC: gcc-10
677:       CXX: g++-10
678: 
679:     steps:
680:     - uses: actions/checkout@v2
681:       with:
682:         fetch-depth: 0
683: 
684:     - uses: actions/setup-python@v2
685:       with:
686:         python-version: '3.7'
687: 
688:     - name: Test
689:       run: python scripts/test_vector_sizes.py
690: 
691: 
692:   sqllogic:
693:     name: Sqllogic tests
694:     runs-on: ubuntu-20.04
695:     needs: linux-debug
696: 
697:     env:
698:       CC: gcc-10
699:       CXX: g++-10
700: 
701:     steps:
702:     - uses: actions/checkout@v2
703:       with:
704:         fetch-depth: 0
705: 
706:     - name: Test
707:       run: make sqlite
708: 
709: 
710:   expanded:
711:     name: Expanded
712:     runs-on: ubuntu-20.04
713:     needs: linux-debug
714: 
715:     env:
716:       CC: gcc-10
717:       CXX: g++-10
718:       TREAT_WARNINGS_AS_ERRORS: 1
719:       DISABLE_UNITY: 1
720:       GEN: ninja
721: 
722:     steps:
723:     - uses: actions/checkout@v2
724:       with:
725:         fetch-depth: 0
726: 
727:     - name: Install
728:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
729: 
730:     - name: Build
731:       run: make debug
732: 
733: 
734:   sqlancer:
735:     name: SQLancer
736:     runs-on: ubuntu-20.04
737:     needs: linux-debug
738: 
739:     env:
740:       BUILD_JDBC: 1
741:       FORCE_QUERY_LOG: sqlancer_log.tmp
742:       GEN: ninja
743: 
744:     steps:
745:     - uses: actions/checkout@v2
746:       with:
747:         fetch-depth: 0
748: 
749:     - name: Install
750:       run: |
751:         sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
752:         git clone https://github.com/hannesmuehleisen/sqlancer
753:         cd sqlancer
754:         git checkout persistent
755:         mvn package -q -DskipTests
756: 
757:     - name: Build
758:       run: make reldebug
759: 
760:     - name: Test
761:       run: |
762:         cp build/reldebug/tools/jdbc/duckdb_jdbc.jar sqlancer/target/lib/duckdb_jdbc-*.jar
763:         python3 scripts/run_sqlancer.py
764: 
765: 
766:   sqlancer_persistent:
767:     name: SQLancer (Persistent)
768:     runs-on: ubuntu-20.04
769:     needs: linux-debug
770: 
771:     env:
772:       BUILD_JDBC: 1
773:       FORCE_QUERY_LOG: sqlancer_log.tmp
774:       GEN: ninja
775: 
776:     steps:
777:     - uses: actions/checkout@v2
778:       with:
779:         fetch-depth: 0
780: 
781:     - name: Install
782:       run: |
783:         sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
784:         git clone https://github.com/hannesmuehleisen/sqlancer
785:         cd sqlancer
786:         git checkout persistent
787:         mvn package -q -DskipTests
788: 
789:     - name: Build
790:       run: make reldebug
791: 
792:     - name: Test
793:       run: |
794:         cp build/reldebug/tools/jdbc/duckdb_jdbc.jar sqlancer/target/lib/duckdb_jdbc-*.jar
795:         python3 scripts/run_sqlancer.py --persistent
796: 
797: 
798:   jdbc:
799:     name: JDBC Compliance
800:     runs-on: ubuntu-18.04
801:     needs: linux-debug
802: 
803:     env:
804:       CC: gcc-10
805:       CXX: g++-10
806:       BUILD_JDBC: 1
807:       GEN: ninja
808: 
809:     steps:
810:     - uses: actions/checkout@v2
811:       with:
812:         fetch-depth: 0
813: 
814:     - name: Install
815:       run: |
816:         sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
817:         git clone https://github.com/cwida/jdbccts.git
818: 
819:     - name: Build
820:       run: make release
821: 
822:     - name: Test
823:       run: (cd jdbccts && make DUCKDB_JAR=../build/release/tools/jdbc/duckdb_jdbc.jar test)
824: 
825: 
826:   odbc:
827:     name: ODBC
828:     runs-on: ubuntu-20.04
829:     needs: linux-debug
830: 
831:     env:
832:       BUILD_ODBC: 1
833:       GEN: ninja
834: 
835:     steps:
836:     - uses: actions/checkout@v2
837:       with:
838:         fetch-depth: 0
839: 
840:     - name: Dependencies
841:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build unixodbc-dev
842: 
843: 
844:     - name: Install nanodbc
845:       run: |
846:         wget https://github.com/nanodbc/nanodbc/archive/refs/tags/v2.13.0.tar.gz -O nanodbc.tgz
847:         (mkdir nanodbc && tar xvf nanodbc.tgz -C nanodbc --strip-components=1 && cd nanodbc && sed -i -e "s/set(test_list/set(test_list odbc/" test/CMakeLists.txt && mkdir build && cd build && cmake -DNANODBC_DISABLE_TESTS=OFF .. && cmake --build .)
848: 
849:     - name: Install psqlodbc
850:       run: |
851:         git clone https://github.com/Mytherin/psqlodbc.git
852:         (cd psqlodbc && make debug)
853: 
854:     - name: Build
855:       run: DISABLE_SANITIZER=1 make debug
856: 
857:     # - name: Test nanodbc
858:     #   run: NANODBC_TEST_CONNSTR_ODBC="DRIVER=./build/debug/tools/odbc/libduckdb_odbc.so" ./nanodbc/build/test/odbc_tests test_simple
859: 
860:     - name: Test psqlodbc
861:       run: |
862:         echo -e "[ODBC]\nTrace = yes\nTraceFile = /tmp/odbctrace\n\n[DuckDB Driver]\nDriver = "`pwd`"/build/debug/tools/odbc/libduckdb_odbc.so" > odbcinst.ini
863:         echo -e "[DuckDB]\nDriver = DuckDB Driver\nDatabase=:memory:\n" > odbc.ini
864:         cd psqlodbc
865:         export ODBCSYSINI=.. && export PSQLODBC_TEST_DSN="DuckDB"
866:         # creating contrib_regression database used by some tests
867:         (./build/debug/reset-db < sampletables.sql) || (cat /tmp/odbctrace; exit 1)
868:         # running supported tests
869:         (./build/debug/psql_odbc_test -f ../tools/odbc/supported_tests) || (cat /tmp/odbctrace; exit 1)
870: 
871: 
872: 
873:   linux-python3:
874:     name: Python 3 Linux
875:     runs-on: ubuntu-20.04
876:     needs: linux-debug
877: 
878:     env:
879:       CIBW_BUILD: 'cp36-* cp37-* cp38-* cp39-*'
880:       CIBW_BEFORE_BUILD: 'pip install --prefer-binary "pandas>=0.24" "pytest>=4.3"'
881:       CIBW_TEST_REQUIRES: 'pytest'
882:       CIBW_BEFORE_TEST: 'pip install --prefer-binary "pandas>=0.24" && pip install --prefer-binary "requests>=2.26" && (pip install --prefer-binary "pyarrow>=4.0.0" || true)'
883:       CIBW_TEST_COMMAND: 'python -m pytest {project}/tests'
884:       SETUPTOOLS_SCM_NO_LOCAL: 'yes'
885:       TWINE_USERNAME: 'hfmuehleisen'
886: 
887:     steps:
888:     - uses: actions/checkout@v2
889:       with:
890:         fetch-depth: 0
891: 
892:     - uses: actions/setup-python@v2
893:       with:
894:         python-version: '3.7'
895: 
896:     - name: Install
897:       run: pip install cibuildwheel twine
898: 
899:     - name: Build
900:       run: |
901:         cd tools/pythonpkg
902:         python setup.py sdist
903:         mkdir duckdb_tarball && tar xvf dist/duckdb-*.tar.gz --strip-components=1 -C duckdb_tarball
904:         cibuildwheel --output-dir wheelhouse duckdb_tarball
905: 
906:     - name: Deploy
907:       run: |
908:         python scripts/asset-upload-gha.py duckdb_python_src.tar.gz=tools/pythonpkg/dist/duckdb-*.tar.gz
909:         if [[ "$GITHUB_REF" =~ ^(refs/heads/master|refs/tags/v.+)$ ]] ; then
910:           twine upload --non-interactive --disable-progress-bar --skip-existing tools/pythonpkg/wheelhouse/*.whl tools/pythonpkg/dist/duckdb-*.tar.gz
911:         fi
912: 
913:   linux-python3-httpfs:
914:     name: Python 3 Linux with HTTPFS support
915:     runs-on: ubuntu-20.04
916:     needs: linux-debug
917: 
918:     env:
919:       CIBW_BUILD: 'cp36-* cp37-* cp38-* cp39-*'
920:       CIBW_BEFORE_BUILD: 'yum install -y openssl-devel && pip install --prefer-binary "pandas>=0.24" "pytest>=4.3"'
921:       CIBW_TEST_REQUIRES: 'pytest'
922:       CIBW_BEFORE_TEST: 'yum install -y openssl && pip install --prefer-binary "pandas>=0.24"  && pip install --prefer-binary "requests>=2.26" && (pip install --prefer-binary "pyarrow>=4.0.0" || true)'
923:       CIBW_TEST_COMMAND: 'python -m pytest {project}/tests'
924:       CIBW_ENVIRONMENT: 'BUILD_HTTPFS=1'
925:       SETUPTOOLS_SCM_NO_LOCAL: 'yes'
926:       TWINE_USERNAME: 'hfmuehleisen'
927: 
928:     steps:
929:       - uses: actions/checkout@v2
930:         with:
931:           fetch-depth: 0
932: 
933:       - uses: actions/setup-python@v2
934:         with:
935:           python-version: '3.7'
936: 
937:       - name: Install
938:         run: pip install cibuildwheel twine
939: 
940:       - name: Build
941:         run: |
942:           cd tools/pythonpkg
943:           BUILD_HTTPFS=1 python setup.py sdist
944:           mkdir duckdb_tarball && tar xvf dist/duckdb-*.tar.gz --strip-components=1 -C duckdb_tarball
945:           cibuildwheel --output-dir wheelhouse duckdb_tarball
946: 
947:   osx-python3:
948:     name: Python 3 OSX
949:     runs-on: macos-latest
950:     needs: linux-debug
951: 
952:     env:
953:       CIBW_BUILD: 'cp36-* cp37-* cp38-* cp39-*'
954:       CIBW_BEFORE_BUILD: 'pip install --prefer-binary "pandas>=0.24" "pytest>=4.3"'
955:       CIBW_TEST_REQUIRES: 'pytest'
956:       CIBW_BEFORE_TEST: 'pip install --prefer-binary "pandas>=0.24" "requests>=2.26" "pyarrow>=4.0.0"'
957:       CIBW_TEST_COMMAND: 'python -m pytest {project}/tests'
958:       CIBW_ARCHS_MACOS: 'x86_64 universal2 arm64'
959:       SETUPTOOLS_SCM_NO_LOCAL: 'yes'
960:       TWINE_USERNAME: 'hfmuehleisen'
961: 
962:     steps:
963:     - uses: actions/checkout@v2
964:       with:
965:         fetch-depth: 0
966: 
967:     - uses: actions/setup-python@v2
968:       with:
969:         python-version: '3.7'
970: 
971:     - name: Install
972:       run: pip install cibuildwheel twine
973: 
974:     - name: Build
975:       run: |
976:         cd tools/pythonpkg
977:         python setup.py sdist
978:         mkdir duckdb_tarball && tar xvf dist/duckdb-*.tar.gz --strip-components=1 -C duckdb_tarball
979:         cibuildwheel --output-dir wheelhouse duckdb_tarball
980: 
981:     - name: Deploy
982:       run: |
983:         if [[ "$GITHUB_REF" =~ ^(refs/heads/master|refs/tags/v.+)$ ]] ; then
984:           twine upload --non-interactive --disable-progress-bar --skip-existing tools/pythonpkg/wheelhouse/*.whl
985:         fi
986: 
987:   win-python3:
988:     name: Python 3 Windows
989:     runs-on: windows-latest
990:     needs: linux-debug
991: 
992:     env:
993:       CIBW_BUILD: 'cp36-* cp37-* cp38-* cp39-*'
994:       CIBW_BEFORE_BUILD: 'pip install --prefer-binary "pandas>=0.24" "pytest>=4.3"'
995:       CIBW_TEST_REQUIRES: 'pytest'
996:       CIBW_BEFORE_TEST: 'pip install --prefer-binary "pandas>=0.24" '
997:       CIBW_TEST_COMMAND: 'python -m pytest {project}/tests'
998:       SETUPTOOLS_SCM_NO_LOCAL: 'yes'
999:       TWINE_USERNAME: 'hfmuehleisen'
1000: 
1001:     steps:
1002:     - uses: actions/checkout@v2
1003:       with:
1004:         fetch-depth: 0
1005: 
1006:     - uses: actions/setup-python@v2
1007:       with:
1008:         python-version: '3.7'
1009: 
1010:     - name: Install
1011:       run: pip install cibuildwheel twine
1012: 
1013:     - name: Build
1014:       run: |
1015:         cd tools/pythonpkg
1016:         python setup.py sdist
1017:         mkdir duckdb_tarball && tar xvf dist/duckdb-*.tar.gz --strip-components=1 -C duckdb_tarball
1018:         cibuildwheel --output-dir wheelhouse duckdb_tarball
1019: 
1020:     - name: Deploy
1021:       run: |
1022:         if [[ "$GITHUB_REF" =~ ^(refs/heads/master|refs/tags/v.+)$ ]] ; then
1023:           twine upload --non-interactive --disable-progress-bar --skip-existing tools/pythonpkg/wheelhouse/*.whl
1024:         fi
1025: 
1026: 
1027:   rstats-linux:
1028:     name: R Package Linux
1029:     runs-on: ubuntu-20.04
1030:     needs: linux-debug
1031: 
1032:     steps:
1033:     - uses: actions/checkout@v2
1034:       with:
1035:         fetch-depth: 0
1036: 
1037:     - uses: actions/setup-python@v2
1038:       with:
1039:         python-version: '3.7'
1040: 
1041:     - uses: r-lib/actions/setup-r@v1
1042:       with:
1043:         r-version: 'devel'
1044: 
1045:     - name: Install
1046:       run: |
1047:         sudo apt-get update -y -qq && sudo apt-get install -y -qq texlive-latex-base texlive-fonts-extra libcurl4-openssl-dev
1048:         mkdir -p $HOME/.R
1049:         R -f tools/rpkg/dependencies.R
1050: 
1051:     - name: Build
1052:       run: |
1053:         cd tools/rpkg
1054:         ./configure
1055:         R CMD build .
1056:         R CMD INSTALL duckdb_*.tar.gz
1057:         (cd tests && R -f testthat.R)
1058:         R CMD check --as-cran -o /tmp duckdb_*.tar.gz
1059:         if grep WARNING /tmp/duckdb.Rcheck/00check.log ; then exit 1; fi
1060: 
1061:     - name: Deploy
1062:       run: python scripts/asset-upload-gha.py duckdb_r_src.tar.gz=tools/rpkg/duckdb_*.tar.gz
1063: 
1064: 
1065:   rstats-windows:
1066:     name: R Package Windows
1067:     runs-on: windows-latest
1068:     needs: linux-debug
1069: 
1070:     steps:
1071:     - uses: actions/checkout@v2
1072:       with:
1073:         fetch-depth: 0
1074: 
1075:     - uses: actions/setup-python@v2
1076:       with:
1077:         python-version: '3.7'
1078: 
1079:     - uses: r-lib/actions/setup-r@v1
1080:       with:
1081:         r-version: 'devel'
1082: 
1083:     - name: Install
1084:       run: |
1085:         R -f tools/rpkg/dependencies.R
1086: 
1087:     - name: Build
1088:       run: |
1089:         cd tools/rpkg
1090:         ./configure
1091:         R CMD build .
1092:         R CMD INSTALL duckdb_*.tar.gz
1093:         (cd tests && R -f testthat.R)
1094:         R CMD check --as-cran --no-manual -o /tmp duckdb_*.tar.gz
1095:         if grep WARNING /tmp/duckdb.Rcheck/00check.log ; then exit 1; fi
1096: 
1097: 
1098:   linux-tarball-v2:
1099:     name: Python 2 Tarball
1100:     runs-on: ubuntu-20.04
1101:     needs: linux-debug
1102: 
1103:     steps:
1104:     - uses: actions/checkout@v2
1105:       with:
1106:         fetch-depth: 0
1107: 
1108:     - uses: actions/setup-python@v2
1109:       with:
1110:         python-version: '2.7'
1111: 
1112:     - name: Install
1113:       run: |
1114:         pip install setuptools-scm==5.0.2
1115:         pip install numpy pytest pandas
1116: 
1117:     - name: Build
1118:       run: |
1119:         python --version
1120:         git archive --format zip --output test-tarball.zip HEAD
1121:         mkdir duckdb-test-tarball
1122:         mv test-tarball.zip duckdb-test-tarball
1123:         cd duckdb-test-tarball
1124:         unzip test-tarball.zip
1125:         cd tools/pythonpkg
1126:         export SETUPTOOLS_SCM_PRETEND_VERSION=0.2.2
1127:         python setup.py install --user
1128:         (cd tests/ && python -m pytest)
1129: 
1130:   linux-tarball:
1131:     name: Python 3 Tarball
1132:     runs-on: ubuntu-20.04
1133:     needs: linux-debug
1134: 
1135:     steps:
1136:     - uses: actions/checkout@v2
1137:       with:
1138:         fetch-depth: 0
1139: 
1140:     - uses: actions/setup-python@v2
1141:       with:
1142:         python-version: '3.7'
1143: 
1144:     - name: Install
1145:       run: pip install numpy pytest pandas
1146: 
1147:     - name: Build
1148:       run: |
1149:         python --version
1150:         git archive --format zip --output test-tarball.zip HEAD
1151:         mkdir duckdb-test-tarball
1152:         mv test-tarball.zip duckdb-test-tarball
1153:         cd duckdb-test-tarball
1154:         unzip test-tarball.zip
1155:         cd tools/pythonpkg
1156:         export SETUPTOOLS_SCM_PRETEND_VERSION=0.2.2
1157:         python setup.py install --user
1158:         (cd tests/ && python -m pytest)
1159: 
1160:   linux-nodejs:
1161:     name: node.js Linux
1162:     runs-on: ubuntu-20.04
1163:     needs: linux-debug
1164: 
1165: 
1166:     steps:
1167:     - uses: actions/checkout@v2
1168:       with:
1169:         fetch-depth: 0
1170: 
1171:     - name: Setup
1172:       run: ./scripts/node_version.sh upload
1173: 
1174:     - name: Node 10
1175:       run: ./scripts/node_build.sh 10
1176: 
1177:     - name: Node 12
1178:       run: ./scripts/node_build.sh 12
1179: 
1180:     - name: Node 14
1181:       run: ./scripts/node_build.sh 14
1182: 
1183:     - name: Node 15
1184:       run: ./scripts/node_build.sh 15
1185: 
1186: 
1187:   osx-nodejs:
1188:     name: node.js OSX
1189:     runs-on: macos-latest
1190:     needs: linux-debug
1191: 
1192:     steps:
1193:     - uses: actions/checkout@v2
1194:       with:
1195:         fetch-depth: 0
1196: 
1197:     - name: Setup
1198:       run: ./scripts/node_version.sh
1199: 
1200:     - name: Node 10
1201:       run: ./scripts/node_build.sh 10
1202: 
1203:     - name: Node 12
1204:       run: ./scripts/node_build.sh 12
1205: 
1206:     - name: Node 14
1207:       run: ./scripts/node_build.sh 14
1208: 
1209:     - name: Node 15
1210:       run: ./scripts/node_build.sh 15
1211: 
1212:   linux-wasm-release:
1213:     name: WebAssembly Release
1214:     runs-on: ubuntu-20.04
1215:     needs: linux-debug
1216: 
1217:     steps:
1218:     - uses: actions/checkout@v2
1219:       with:
1220:         fetch-depth: 0
1221: 
1222:     - name: Build Amalgamation
1223:       run: python scripts/amalgamation.py
1224: 
1225:     - name: Setup
1226:       run: ./scripts/wasm_configure.sh
1227: 
1228:     - name: Build Library Module
1229:       run: ./scripts/wasm_build_lib.sh Release
1230: 
1231:     - name: Build Test Module
1232:       run: ./scripts/wasm_build_test.sh Release
1233: 
1234:     - name: Test WASM Module
1235:       run: node ./test/wasm/hello_wasm_test.js
1236: 
1237:     - name: Package
1238:       run: |
1239:         zip -j duckdb-wasm32-nothreads.zip ./.wasm/build/duckdb.wasm
1240:         python scripts/asset-upload-gha.py duckdb-wasm32-nothreads.zip
1241: 
1242:     - uses: actions/upload-artifact@v2
1243:       with:
1244:         name: duckdb-wasm32-nothreads
1245:         path: |
1246:           duckdb-wasm32-nothreads.zip
[end of .github/workflows/main.yml]
[start of src/function/table/arrow.cpp]
1: #include "duckdb/common/arrow.hpp"
2: 
3: #include "duckdb.hpp"
4: #include "duckdb/common/arrow_wrapper.hpp"
5: #include "duckdb/common/limits.hpp"
6: #include "duckdb/common/to_string.hpp"
7: #include "duckdb/common/types/date.hpp"
8: #include "duckdb/common/types/hugeint.hpp"
9: #include "duckdb/common/types/time.hpp"
10: #include "duckdb/common/types/timestamp.hpp"
11: #include "duckdb/function/table/arrow.hpp"
12: #include "duckdb/function/table_function.hpp"
13: #include "duckdb/main/client_context.hpp"
14: #include "duckdb/main/connection.hpp"
15: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
16: #include "utf8proc_wrapper.hpp"
17: 
18: #include "duckdb/common/operator/multiply.hpp"
19: namespace duckdb {
20: 
21: LogicalType GetArrowLogicalType(ArrowSchema &schema,
22:                                 std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,
23:                                 idx_t col_idx) {
24: 	auto format = string(schema.format);
25: 	if (arrow_convert_data.find(col_idx) == arrow_convert_data.end()) {
26: 		arrow_convert_data[col_idx] = make_unique<ArrowConvertData>();
27: 	}
28: 	if (format == "n") {
29: 		return LogicalType::SQLNULL;
30: 	} else if (format == "b") {
31: 		return LogicalType::BOOLEAN;
32: 	} else if (format == "c") {
33: 		return LogicalType::TINYINT;
34: 	} else if (format == "s") {
35: 		return LogicalType::SMALLINT;
36: 	} else if (format == "i") {
37: 		return LogicalType::INTEGER;
38: 	} else if (format == "l") {
39: 		return LogicalType::BIGINT;
40: 	} else if (format == "C") {
41: 		return LogicalType::UTINYINT;
42: 	} else if (format == "S") {
43: 		return LogicalType::USMALLINT;
44: 	} else if (format == "I") {
45: 		return LogicalType::UINTEGER;
46: 	} else if (format == "L") {
47: 		return LogicalType::UBIGINT;
48: 	} else if (format == "f") {
49: 		return LogicalType::FLOAT;
50: 	} else if (format == "g") {
51: 		return LogicalType::DOUBLE;
52: 	} else if (format[0] == 'd') { //! this can be either decimal128 or decimal 256 (e.g., d:38,0)
53: 		std::string parameters = format.substr(format.find(':'));
54: 		uint8_t width = std::stoi(parameters.substr(1, parameters.find(',')));
55: 		uint8_t scale = std::stoi(parameters.substr(parameters.find(',') + 1));
56: 		if (width > 38) {
57: 			throw NotImplementedException("Unsupported Internal Arrow Type for Decimal %s", format);
58: 		}
59: 		return LogicalType::DECIMAL(width, scale);
60: 	} else if (format == "u") {
61: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::NORMAL, 0);
62: 		return LogicalType::VARCHAR;
63: 	} else if (format == "U") {
64: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::SUPER_SIZE, 0);
65: 		return LogicalType::VARCHAR;
66: 	} else if (format == "tsn:") {
67: 		return LogicalTypeId::TIMESTAMP_NS;
68: 	} else if (format == "tsu:") {
69: 		return LogicalTypeId::TIMESTAMP;
70: 	} else if (format == "tsm:") {
71: 		return LogicalTypeId::TIMESTAMP_MS;
72: 	} else if (format == "tss:") {
73: 		return LogicalTypeId::TIMESTAMP_SEC;
74: 	} else if (format == "tdD") {
75: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::DAYS);
76: 		return LogicalType::DATE;
77: 	} else if (format == "tdm") {
78: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MILLISECONDS);
79: 		return LogicalType::DATE;
80: 	} else if (format == "tts") {
81: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::SECONDS);
82: 		return LogicalType::TIME;
83: 	} else if (format == "ttm") {
84: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MILLISECONDS);
85: 		return LogicalType::TIME;
86: 	} else if (format == "ttu") {
87: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MICROSECONDS);
88: 		return LogicalType::TIME;
89: 	} else if (format == "ttn") {
90: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::NANOSECONDS);
91: 		return LogicalType::TIME;
92: 	} else if (format == "tDs") {
93: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::SECONDS);
94: 		return LogicalType::INTERVAL;
95: 	} else if (format == "tDm") {
96: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MILLISECONDS);
97: 		return LogicalType::INTERVAL;
98: 	} else if (format == "tDu") {
99: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MICROSECONDS);
100: 		return LogicalType::INTERVAL;
101: 	} else if (format == "tDn") {
102: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::NANOSECONDS);
103: 		return LogicalType::INTERVAL;
104: 	} else if (format == "tiD") {
105: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::DAYS);
106: 		return LogicalType::INTERVAL;
107: 	} else if (format == "tiM") {
108: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MONTHS);
109: 		return LogicalType::INTERVAL;
110: 	} else if (format == "+l") {
111: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::NORMAL, 0);
112: 		auto child_type = GetArrowLogicalType(*schema.children[0], arrow_convert_data, col_idx);
113: 		return LogicalType::LIST(child_type);
114: 	} else if (format == "+L") {
115: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::SUPER_SIZE, 0);
116: 		auto child_type = GetArrowLogicalType(*schema.children[0], arrow_convert_data, col_idx);
117: 		return LogicalType::LIST(child_type);
118: 	} else if (format[0] == '+' && format[1] == 'w') {
119: 		std::string parameters = format.substr(format.find(':') + 1);
120: 		idx_t fixed_size = std::stoi(parameters);
121: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::FIXED_SIZE, fixed_size);
122: 		auto child_type = GetArrowLogicalType(*schema.children[0], arrow_convert_data, col_idx);
123: 		return LogicalType::LIST(move(child_type));
124: 	} else if (format == "+s") {
125: 		child_list_t<LogicalType> child_types;
126: 		for (idx_t type_idx = 0; type_idx < (idx_t)schema.n_children; type_idx++) {
127: 			auto child_type = GetArrowLogicalType(*schema.children[type_idx], arrow_convert_data, col_idx);
128: 			child_types.push_back({schema.children[type_idx]->name, child_type});
129: 		}
130: 		return LogicalType::STRUCT(move(child_types));
131: 
132: 	} else if (format == "+m") {
133: 		child_list_t<LogicalType> child_types;
134: 		//! First type will be struct, so we skip it
135: 		auto &struct_schema = *schema.children[0];
136: 		for (idx_t type_idx = 0; type_idx < (idx_t)struct_schema.n_children; type_idx++) {
137: 			//! The other types must be added on lists
138: 			auto child_type = GetArrowLogicalType(*struct_schema.children[type_idx], arrow_convert_data, col_idx);
139: 
140: 			auto list_type = LogicalType::LIST(child_type);
141: 			child_types.push_back({struct_schema.children[type_idx]->name, list_type});
142: 		}
143: 		return LogicalType::MAP(move(child_types));
144: 	} else if (format == "z") {
145: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::NORMAL, 0);
146: 		return LogicalType::BLOB;
147: 	} else if (format == "Z") {
148: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::SUPER_SIZE, 0);
149: 		return LogicalType::BLOB;
150: 	} else if (format[0] == 'w') {
151: 		std::string parameters = format.substr(format.find(':') + 1);
152: 		idx_t fixed_size = std::stoi(parameters);
153: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::FIXED_SIZE, fixed_size);
154: 		return LogicalType::BLOB;
155: 	} else {
156: 		throw NotImplementedException("Unsupported Internal Arrow Type %s", format);
157: 	}
158: }
159: 
160: unique_ptr<FunctionData> ArrowTableFunction::ArrowScanBind(ClientContext &context, vector<Value> &inputs,
161:                                                            unordered_map<string, Value> &named_parameters,
162:                                                            vector<LogicalType> &input_table_types,
163:                                                            vector<string> &input_table_names,
164:                                                            vector<LogicalType> &return_types, vector<string> &names) {
165: 
166: 	auto stream_factory_ptr = inputs[0].GetPointer();
167: 	unique_ptr<ArrowArrayStreamWrapper> (*stream_factory_produce)(uintptr_t stream_factory_ptr) =
168: 	    (unique_ptr<ArrowArrayStreamWrapper>(*)(uintptr_t stream_factory_ptr))inputs[1].GetPointer();
169: 	auto rows_per_thread = inputs[2].GetValue<uint64_t>();
170: 
171: 	auto res = make_unique<ArrowScanFunctionData>(rows_per_thread);
172: 	auto &data = *res;
173: 	data.stream = stream_factory_produce(stream_factory_ptr);
174: 	if (!data.stream) {
175: 		throw InvalidInputException("arrow_scan: NULL pointer passed");
176: 	}
177: 
178: 	data.stream->GetSchema(data.schema_root);
179: 
180: 	for (idx_t col_idx = 0; col_idx < (idx_t)data.schema_root.arrow_schema.n_children; col_idx++) {
181: 		auto &schema = *data.schema_root.arrow_schema.children[col_idx];
182: 		if (!schema.release) {
183: 			throw InvalidInputException("arrow_scan: released schema passed");
184: 		}
185: 		if (schema.dictionary) {
186: 			res->arrow_convert_data[col_idx] =
187: 			    make_unique<ArrowConvertData>(GetArrowLogicalType(schema, res->arrow_convert_data, col_idx));
188: 			return_types.emplace_back(GetArrowLogicalType(*schema.dictionary, res->arrow_convert_data, col_idx));
189: 		} else {
190: 			return_types.emplace_back(GetArrowLogicalType(schema, res->arrow_convert_data, col_idx));
191: 		}
192: 		auto format = string(schema.format);
193: 		auto name = string(schema.name);
194: 		if (name.empty()) {
195: 			name = string("v") + to_string(col_idx);
196: 		}
197: 		names.push_back(name);
198: 	}
199: 	return move(res);
200: }
201: 
202: unique_ptr<FunctionOperatorData> ArrowTableFunction::ArrowScanInit(ClientContext &context,
203:                                                                    const FunctionData *bind_data,
204:                                                                    const vector<column_t> &column_ids,
205:                                                                    TableFilterCollection *filters) {
206: 	auto current_chunk = make_unique<ArrowArrayWrapper>();
207: 	auto result = make_unique<ArrowScanState>(move(current_chunk));
208: 	result->column_ids = column_ids;
209: 	return move(result);
210: }
211: 
212: void ShiftRight(unsigned char *ar, int size, int shift) {
213: 	int carry = 0;
214: 	while (shift--) {
215: 		for (int i = size - 1; i >= 0; --i) {
216: 			int next = (ar[i] & 1) ? 0x80 : 0;
217: 			ar[i] = carry | (ar[i] >> 1);
218: 			carry = next;
219: 		}
220: 	}
221: }
222: 
223: void SetValidityMask(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size, int64_t nested_offset,
224:                      bool add_null = false) {
225: 	auto &mask = FlatVector::Validity(vector);
226: 	if (array.null_count != 0 && array.buffers[0]) {
227: 		D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);
228: 		auto bit_offset = scan_state.chunk_offset + array.offset;
229: 		if (nested_offset != -1) {
230: 			bit_offset = nested_offset;
231: 		}
232: 		auto n_bitmask_bytes = (size + 8 - 1) / 8;
233: 		mask.EnsureWritable();
234: 		if (bit_offset % 8 == 0) {
235: 			//! just memcpy nullmask
236: 			memcpy((void *)mask.GetData(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes);
237: 		} else {
238: 			//! need to re-align nullmask
239: 			std::vector<uint8_t> temp_nullmask(n_bitmask_bytes + 1);
240: 			memcpy(temp_nullmask.data(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes + 1);
241: 			ShiftRight(temp_nullmask.data(), n_bitmask_bytes + 1,
242: 			           bit_offset % 8); //! why this has to be a right shift is a mystery to me
243: 			memcpy((void *)mask.GetData(), (data_ptr_t)temp_nullmask.data(), n_bitmask_bytes);
244: 		}
245: 	}
246: 	if (add_null) {
247: 		//! We are setting a validity mask of the data part of dictionary vector
248: 		//! For some reason, Nulls are allowed to be indexes, hence we need to set the last element here to be null
249: 		//! We might have to resize the mask
250: 		mask.Resize(size, size + 1);
251: 		mask.SetInvalid(size);
252: 	}
253: }
254: 
255: void GetValidityMask(ValidityMask &mask, ArrowArray &array, ArrowScanState &scan_state, idx_t size) {
256: 	if (array.null_count != 0 && array.buffers[0]) {
257: 		auto bit_offset = scan_state.chunk_offset + array.offset;
258: 		auto n_bitmask_bytes = (size + 8 - 1) / 8;
259: 		mask.EnsureWritable();
260: 		if (bit_offset % 8 == 0) {
261: 			//! just memcpy nullmask
262: 			memcpy((void *)mask.GetData(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes);
263: 		} else {
264: 			//! need to re-align nullmask
265: 			std::vector<uint8_t> temp_nullmask(n_bitmask_bytes + 1);
266: 			memcpy(temp_nullmask.data(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes + 1);
267: 			ShiftRight(temp_nullmask.data(), n_bitmask_bytes + 1,
268: 			           bit_offset % 8); //! why this has to be a right shift is a mystery to me
269: 			memcpy((void *)mask.GetData(), (data_ptr_t)temp_nullmask.data(), n_bitmask_bytes);
270: 		}
271: 	}
272: }
273: 
274: void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,
275:                          std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,
276:                          std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset = -1,
277:                          ValidityMask *parent_mask = nullptr);
278: 
279: void ArrowToDuckDBList(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,
280:                        std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,
281:                        std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset, ValidityMask *parent_mask) {
282: 	auto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.first++];
283: 	idx_t list_size = 0;
284: 	SetValidityMask(vector, array, scan_state, size, nested_offset);
285: 	idx_t start_offset = 0;
286: 	idx_t cur_offset = 0;
287: 	if (original_type.first == ArrowVariableSizeType::FIXED_SIZE) {
288: 		//! Have to check validity mask before setting this up
289: 		idx_t offset = (scan_state.chunk_offset + array.offset) * original_type.second;
290: 		if (nested_offset != -1) {
291: 			offset = original_type.second * nested_offset;
292: 		}
293: 		start_offset = offset;
294: 		auto list_data = FlatVector::GetData<list_entry_t>(vector);
295: 		for (idx_t i = 0; i < size; i++) {
296: 			auto &le = list_data[i];
297: 			le.offset = cur_offset;
298: 			le.length = original_type.second;
299: 			cur_offset += original_type.second;
300: 		}
301: 		list_size = cur_offset;
302: 	} else if (original_type.first == ArrowVariableSizeType::NORMAL) {
303: 		auto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
304: 		if (nested_offset != -1) {
305: 			offsets = (uint32_t *)array.buffers[1] + nested_offset;
306: 		}
307: 		start_offset = offsets[0];
308: 		auto list_data = FlatVector::GetData<list_entry_t>(vector);
309: 		for (idx_t i = 0; i < size; i++) {
310: 			auto &le = list_data[i];
311: 			le.offset = cur_offset;
312: 			le.length = offsets[i + 1] - offsets[i];
313: 			cur_offset += le.length;
314: 		}
315: 		list_size = offsets[size];
316: 	} else {
317: 		auto offsets = (uint64_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
318: 		if (nested_offset != -1) {
319: 			offsets = (uint64_t *)array.buffers[1] + nested_offset;
320: 		}
321: 		start_offset = offsets[0];
322: 		auto list_data = FlatVector::GetData<list_entry_t>(vector);
323: 		for (idx_t i = 0; i < size; i++) {
324: 			auto &le = list_data[i];
325: 			le.offset = cur_offset;
326: 			le.length = offsets[i + 1] - offsets[i];
327: 			cur_offset += le.length;
328: 		}
329: 		list_size = offsets[size];
330: 	}
331: 	list_size -= start_offset;
332: 	ListVector::Reserve(vector, list_size);
333: 	ListVector::SetListSize(vector, list_size);
334: 	auto &child_vector = ListVector::GetEntry(vector);
335: 	SetValidityMask(child_vector, *array.children[0], scan_state, list_size, start_offset);
336: 	auto &list_mask = FlatVector::Validity(vector);
337: 	if (parent_mask) {
338: 		//! Since this List is owned by a struct we must guarantee their validity map matches on Null
339: 		if (!parent_mask->AllValid()) {
340: 			for (idx_t i = 0; i < size; i++) {
341: 				if (!parent_mask->RowIsValid(i)) {
342: 					list_mask.SetInvalid(i);
343: 				}
344: 			}
345: 		}
346: 	}
347: 	if (list_size == 0 && start_offset == 0) {
348: 		ColumnArrowToDuckDB(child_vector, *array.children[0], scan_state, list_size, arrow_convert_data, col_idx,
349: 		                    arrow_convert_idx, -1);
350: 	} else {
351: 		ColumnArrowToDuckDB(child_vector, *array.children[0], scan_state, list_size, arrow_convert_data, col_idx,
352: 		                    arrow_convert_idx, start_offset);
353: 	}
354: }
355: 
356: void ArrowToDuckDBBlob(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,
357:                        std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,
358:                        std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset) {
359: 	auto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.first++];
360: 	SetValidityMask(vector, array, scan_state, size, nested_offset);
361: 	if (original_type.first == ArrowVariableSizeType::FIXED_SIZE) {
362: 		//! Have to check validity mask before setting this up
363: 		idx_t offset = (scan_state.chunk_offset + array.offset) * original_type.second;
364: 		if (nested_offset != -1) {
365: 			offset = original_type.second * nested_offset;
366: 		}
367: 		auto cdata = (char *)array.buffers[1];
368: 		for (idx_t row_idx = 0; row_idx < size; row_idx++) {
369: 			if (FlatVector::IsNull(vector, row_idx)) {
370: 				continue;
371: 			}
372: 			auto bptr = cdata + offset;
373: 			auto blob_len = original_type.second;
374: 			FlatVector::GetData<string_t>(vector)[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);
375: 			offset += blob_len;
376: 		}
377: 	} else if (original_type.first == ArrowVariableSizeType::NORMAL) {
378: 		auto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
379: 		if (nested_offset != -1) {
380: 			offsets = (uint32_t *)array.buffers[1] + array.offset + nested_offset;
381: 		}
382: 		auto cdata = (char *)array.buffers[2];
383: 		for (idx_t row_idx = 0; row_idx < size; row_idx++) {
384: 			if (FlatVector::IsNull(vector, row_idx)) {
385: 				continue;
386: 			}
387: 			auto bptr = cdata + offsets[row_idx];
388: 			auto blob_len = offsets[row_idx + 1] - offsets[row_idx];
389: 			FlatVector::GetData<string_t>(vector)[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);
390: 		}
391: 	} else {
392: 		//! Check if last offset is higher than max uint32
393: 		if (((uint64_t *)array.buffers[1])[array.length] > NumericLimits<uint32_t>::Maximum()) {
394: 			throw std::runtime_error("We do not support Blobs over 4GB");
395: 		}
396: 		auto offsets = (uint64_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
397: 		if (nested_offset != -1) {
398: 			offsets = (uint64_t *)array.buffers[1] + array.offset + nested_offset;
399: 		}
400: 		auto cdata = (char *)array.buffers[2];
401: 		for (idx_t row_idx = 0; row_idx < size; row_idx++) {
402: 			if (FlatVector::IsNull(vector, row_idx)) {
403: 				continue;
404: 			}
405: 			auto bptr = cdata + offsets[row_idx];
406: 			auto blob_len = offsets[row_idx + 1] - offsets[row_idx];
407: 			FlatVector::GetData<string_t>(vector)[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);
408: 		}
409: 	}
410: }
411: 
412: void ArrowToDuckDBMapList(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,
413:                           std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,
414:                           std::pair<idx_t, idx_t> &arrow_convert_idx, uint32_t *offsets, ValidityMask *parent_mask) {
415: 	idx_t list_size = offsets[size] - offsets[0];
416: 	ListVector::Reserve(vector, list_size);
417: 
418: 	auto &child_vector = ListVector::GetEntry(vector);
419: 	auto list_data = FlatVector::GetData<list_entry_t>(vector);
420: 	auto cur_offset = 0;
421: 	for (idx_t i = 0; i < size; i++) {
422: 		auto &le = list_data[i];
423: 		le.offset = cur_offset;
424: 		le.length = offsets[i + 1] - offsets[i];
425: 		cur_offset += le.length;
426: 	}
427: 	ListVector::SetListSize(vector, list_size);
428: 	if (list_size == 0 && offsets[0] == 0) {
429: 		SetValidityMask(child_vector, array, scan_state, list_size, -1);
430: 	} else {
431: 		SetValidityMask(child_vector, array, scan_state, list_size, offsets[0]);
432: 	}
433: 
434: 	auto &list_mask = FlatVector::Validity(vector);
435: 	if (parent_mask) {
436: 		//! Since this List is owned by a struct we must guarantee their validity map matches on Null
437: 		if (!parent_mask->AllValid()) {
438: 			for (idx_t i = 0; i < size; i++) {
439: 				if (!parent_mask->RowIsValid(i)) {
440: 					list_mask.SetInvalid(i);
441: 				}
442: 			}
443: 		}
444: 	}
445: 	if (list_size == 0 && offsets[0] == 0) {
446: 		ColumnArrowToDuckDB(child_vector, array, scan_state, list_size, arrow_convert_data, col_idx, arrow_convert_idx,
447: 		                    -1);
448: 	} else {
449: 		ColumnArrowToDuckDB(child_vector, array, scan_state, list_size, arrow_convert_data, col_idx, arrow_convert_idx,
450: 		                    offsets[0]);
451: 	}
452: }
453: template <class T>
454: static void SetVectorString(Vector &vector, idx_t size, char *cdata, T *offsets) {
455: 	auto strings = FlatVector::GetData<string_t>(vector);
456: 	for (idx_t row_idx = 0; row_idx < size; row_idx++) {
457: 		if (FlatVector::IsNull(vector, row_idx)) {
458: 			continue;
459: 		}
460: 		auto cptr = cdata + offsets[row_idx];
461: 		auto str_len = offsets[row_idx + 1] - offsets[row_idx];
462: 		strings[row_idx] = string_t(cptr, str_len);
463: 	}
464: }
465: 
466: void DirectConversion(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, int64_t nested_offset) {
467: 	auto internal_type = GetTypeIdSize(vector.GetType().InternalType());
468: 	auto data_ptr = (data_ptr_t)array.buffers[1] + internal_type * (scan_state.chunk_offset + array.offset);
469: 	if (nested_offset != -1) {
470: 		data_ptr = (data_ptr_t)array.buffers[1] + internal_type * (array.offset + nested_offset);
471: 	}
472: 	FlatVector::SetData(vector, data_ptr);
473: }
474: 
475: template <class T>
476: void TimeConversion(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, int64_t nested_offset, idx_t size,
477:                     int64_t conversion) {
478: 	auto tgt_ptr = (dtime_t *)FlatVector::GetData(vector);
479: 	auto src_ptr = (T *)array.buffers[1] + scan_state.chunk_offset + array.offset;
480: 	if (nested_offset != -1) {
481: 		src_ptr = (T *)array.buffers[1] + nested_offset + array.offset;
482: 	}
483: 	for (idx_t row = 0; row < size; row++) {
484: 		if (!TryMultiplyOperator::Operation((int64_t)src_ptr[row], conversion, tgt_ptr[row].micros)) {
485: 			throw ConversionException("Could not convert Interval to Microsecond");
486: 		}
487: 	}
488: }
489: 
490: void IntervalConversionUs(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, int64_t nested_offset,
491:                           idx_t size, int64_t conversion) {
492: 	auto tgt_ptr = (interval_t *)FlatVector::GetData(vector);
493: 	auto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
494: 	if (nested_offset != -1) {
495: 		src_ptr = (int64_t *)array.buffers[1] + nested_offset + array.offset;
496: 	}
497: 	for (idx_t row = 0; row < size; row++) {
498: 		tgt_ptr[row].days = 0;
499: 		tgt_ptr[row].months = 0;
500: 		if (!TryMultiplyOperator::Operation(src_ptr[row], conversion, tgt_ptr[row].micros)) {
501: 			throw ConversionException("Could not convert Interval to Microsecond");
502: 		}
503: 	}
504: }
505: 
506: void IntervalConversionMonths(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, int64_t nested_offset,
507:                               idx_t size) {
508: 	auto tgt_ptr = (interval_t *)FlatVector::GetData(vector);
509: 	auto src_ptr = (int32_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
510: 	if (nested_offset != -1) {
511: 		src_ptr = (int32_t *)array.buffers[1] + nested_offset + array.offset;
512: 	}
513: 	for (idx_t row = 0; row < size; row++) {
514: 		tgt_ptr[row].days = 0;
515: 		tgt_ptr[row].micros = 0;
516: 		tgt_ptr[row].months = src_ptr[row];
517: 	}
518: }
519: 
520: void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,
521:                          std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,
522:                          std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset, ValidityMask *parent_mask) {
523: 	switch (vector.GetType().id()) {
524: 	case LogicalTypeId::SQLNULL:
525: 		vector.Reference(Value());
526: 		break;
527: 	case LogicalTypeId::BOOLEAN: {
528: 		//! Arrow bit-packs boolean values
529: 		//! Lets first figure out where we are in the source array
530: 		auto src_ptr = (uint8_t *)array.buffers[1] + (scan_state.chunk_offset + array.offset) / 8;
531: 
532: 		if (nested_offset != -1) {
533: 			src_ptr = (uint8_t *)array.buffers[1] + (nested_offset + array.offset) / 8;
534: 		}
535: 		auto tgt_ptr = (uint8_t *)FlatVector::GetData(vector);
536: 		int src_pos = 0;
537: 		idx_t cur_bit = scan_state.chunk_offset % 8;
538: 		if (nested_offset != -1) {
539: 			cur_bit = nested_offset % 8;
540: 		}
541: 		for (idx_t row = 0; row < size; row++) {
542: 			if ((src_ptr[src_pos] & (1 << cur_bit)) == 0) {
543: 				tgt_ptr[row] = 0;
544: 			} else {
545: 				tgt_ptr[row] = 1;
546: 			}
547: 			cur_bit++;
548: 			if (cur_bit == 8) {
549: 				src_pos++;
550: 				cur_bit = 0;
551: 			}
552: 		}
553: 		break;
554: 	}
555: 	case LogicalTypeId::TINYINT:
556: 	case LogicalTypeId::SMALLINT:
557: 	case LogicalTypeId::INTEGER:
558: 	case LogicalTypeId::FLOAT:
559: 	case LogicalTypeId::UTINYINT:
560: 	case LogicalTypeId::USMALLINT:
561: 	case LogicalTypeId::UINTEGER:
562: 	case LogicalTypeId::UBIGINT:
563: 	case LogicalTypeId::BIGINT:
564: 	case LogicalTypeId::HUGEINT:
565: 	case LogicalTypeId::TIMESTAMP:
566: 	case LogicalTypeId::TIMESTAMP_SEC:
567: 	case LogicalTypeId::TIMESTAMP_MS:
568: 	case LogicalTypeId::TIMESTAMP_NS: {
569: 		DirectConversion(vector, array, scan_state, nested_offset);
570: 		break;
571: 	}
572: 	case LogicalTypeId::DOUBLE: {
573: 		DirectConversion(vector, array, scan_state, nested_offset);
574: 		//! Need to check if there are NaNs, if yes, must turn that to null
575: 		auto data = (double *)vector.GetData();
576: 		auto &mask = FlatVector::Validity(vector);
577: 		for (idx_t row_idx = 0; row_idx < size; row_idx++) {
578: 			if (!Value::DoubleIsValid(data[row_idx])) {
579: 				mask.SetInvalid(row_idx);
580: 			}
581: 		}
582: 		break;
583: 	}
584: 	case LogicalTypeId::VARCHAR: {
585: 		auto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.first++];
586: 		auto cdata = (char *)array.buffers[2];
587: 		if (original_type.first == ArrowVariableSizeType::SUPER_SIZE) {
588: 			if (((uint64_t *)array.buffers[1])[array.length] > NumericLimits<uint32_t>::Maximum()) {
589: 				throw std::runtime_error("We do not support Strings over 4GB");
590: 			}
591: 			auto offsets = (uint64_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
592: 			if (nested_offset != -1) {
593: 				offsets = (uint64_t *)array.buffers[1] + array.offset + nested_offset;
594: 			}
595: 			SetVectorString(vector, size, cdata, offsets);
596: 
597: 		} else {
598: 			auto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
599: 			if (nested_offset != -1) {
600: 				offsets = (uint32_t *)array.buffers[1] + array.offset + nested_offset;
601: 			}
602: 			SetVectorString(vector, size, cdata, offsets);
603: 		}
604: 
605: 		break;
606: 	}
607: 	case LogicalTypeId::DATE: {
608: 		auto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];
609: 		switch (precision) {
610: 		case ArrowDateTimeType::DAYS: {
611: 			DirectConversion(vector, array, scan_state, nested_offset);
612: 			break;
613: 		}
614: 		case ArrowDateTimeType::MILLISECONDS: {
615: 			//! convert date from nanoseconds to days
616: 			auto src_ptr = (uint64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
617: 			if (nested_offset != -1) {
618: 				src_ptr = (uint64_t *)array.buffers[1] + nested_offset + array.offset;
619: 			}
620: 			auto tgt_ptr = (date_t *)FlatVector::GetData(vector);
621: 			for (idx_t row = 0; row < size; row++) {
622: 				tgt_ptr[row] = date_t(int64_t(src_ptr[row]) / (1000 * 60 * 60 * 24));
623: 			}
624: 			break;
625: 		}
626: 		default:
627: 			throw std::runtime_error("Unsupported precision for Date Type ");
628: 		}
629: 		break;
630: 	}
631: 	case LogicalTypeId::TIME: {
632: 		auto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];
633: 		switch (precision) {
634: 		case ArrowDateTimeType::SECONDS: {
635: 			TimeConversion<int32_t>(vector, array, scan_state, nested_offset, size, 1000000);
636: 			break;
637: 		}
638: 		case ArrowDateTimeType::MILLISECONDS: {
639: 			TimeConversion<int32_t>(vector, array, scan_state, nested_offset, size, 1000);
640: 			break;
641: 		}
642: 		case ArrowDateTimeType::MICROSECONDS: {
643: 			TimeConversion<int64_t>(vector, array, scan_state, nested_offset, size, 1);
644: 			break;
645: 		}
646: 		case ArrowDateTimeType::NANOSECONDS: {
647: 			auto tgt_ptr = (dtime_t *)FlatVector::GetData(vector);
648: 			auto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
649: 			if (nested_offset != -1) {
650: 				src_ptr = (int64_t *)array.buffers[1] + nested_offset + array.offset;
651: 			}
652: 			for (idx_t row = 0; row < size; row++) {
653: 				tgt_ptr[row].micros = src_ptr[row] / 1000;
654: 			}
655: 			break;
656: 		}
657: 		default:
658: 			throw std::runtime_error("Unsupported precision for Time Type ");
659: 		}
660: 		break;
661: 	}
662: 	case LogicalTypeId::INTERVAL: {
663: 		auto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];
664: 		switch (precision) {
665: 		case ArrowDateTimeType::SECONDS: {
666: 			IntervalConversionUs(vector, array, scan_state, nested_offset, size, 1000000);
667: 			break;
668: 		}
669: 		case ArrowDateTimeType::DAYS:
670: 		case ArrowDateTimeType::MILLISECONDS: {
671: 			IntervalConversionUs(vector, array, scan_state, nested_offset, size, 1000);
672: 			break;
673: 		}
674: 		case ArrowDateTimeType::MICROSECONDS: {
675: 			IntervalConversionUs(vector, array, scan_state, nested_offset, size, 1);
676: 			break;
677: 		}
678: 		case ArrowDateTimeType::NANOSECONDS: {
679: 			auto tgt_ptr = (interval_t *)FlatVector::GetData(vector);
680: 			auto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
681: 			if (nested_offset != -1) {
682: 				src_ptr = (int64_t *)array.buffers[1] + nested_offset + array.offset;
683: 			}
684: 			for (idx_t row = 0; row < size; row++) {
685: 				tgt_ptr[row].micros = src_ptr[row] / 1000;
686: 				tgt_ptr[row].days = 0;
687: 				tgt_ptr[row].months = 0;
688: 			}
689: 			break;
690: 		}
691: 		case ArrowDateTimeType::MONTHS: {
692: 			IntervalConversionMonths(vector, array, scan_state, nested_offset, size);
693: 			break;
694: 		}
695: 		default:
696: 			throw std::runtime_error("Unsupported precision for Interval/Duration Type ");
697: 		}
698: 		break;
699: 	}
700: 	case LogicalTypeId::DECIMAL: {
701: 		auto val_mask = FlatVector::Validity(vector);
702: 		//! We have to convert from INT128
703: 		switch (vector.GetType().InternalType()) {
704: 		case PhysicalType::INT16: {
705: 			auto src_ptr = (hugeint_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
706: 			if (nested_offset != -1) {
707: 				src_ptr = (hugeint_t *)array.buffers[1] + nested_offset + array.offset;
708: 			}
709: 			auto tgt_ptr = (int16_t *)FlatVector::GetData(vector);
710: 			for (idx_t row = 0; row < size; row++) {
711: 				if (val_mask.RowIsValid(row)) {
712: 					auto result = Hugeint::TryCast(src_ptr[row], tgt_ptr[row]);
713: 					D_ASSERT(result);
714: 				}
715: 			}
716: 			break;
717: 		}
718: 		case PhysicalType::INT32: {
719: 			auto src_ptr = (hugeint_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
720: 			if (nested_offset != -1) {
721: 				src_ptr = (hugeint_t *)array.buffers[1] + nested_offset + array.offset;
722: 			}
723: 			auto tgt_ptr = (int32_t *)FlatVector::GetData(vector);
724: 			for (idx_t row = 0; row < size; row++) {
725: 				if (val_mask.RowIsValid(row)) {
726: 					auto result = Hugeint::TryCast(src_ptr[row], tgt_ptr[row]);
727: 					D_ASSERT(result);
728: 				}
729: 			}
730: 			break;
731: 		}
732: 		case PhysicalType::INT64: {
733: 			auto src_ptr = (hugeint_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
734: 			if (nested_offset != -1) {
735: 				src_ptr = (hugeint_t *)array.buffers[1] + nested_offset + array.offset;
736: 			}
737: 			auto tgt_ptr = (int64_t *)FlatVector::GetData(vector);
738: 			for (idx_t row = 0; row < size; row++) {
739: 				if (val_mask.RowIsValid(row)) {
740: 					auto result = Hugeint::TryCast(src_ptr[row], tgt_ptr[row]);
741: 					D_ASSERT(result);
742: 				}
743: 			}
744: 			break;
745: 		}
746: 		case PhysicalType::INT128: {
747: 			FlatVector::SetData(vector, (data_ptr_t)array.buffers[1] + GetTypeIdSize(vector.GetType().InternalType()) *
748: 			                                                               (scan_state.chunk_offset + array.offset));
749: 			break;
750: 		}
751: 		default:
752: 			throw std::runtime_error("Unsupported physical type for Decimal: " +
753: 			                         TypeIdToString(vector.GetType().InternalType()));
754: 		}
755: 		break;
756: 	}
757: 	case LogicalTypeId::BLOB: {
758: 		ArrowToDuckDBBlob(vector, array, scan_state, size, arrow_convert_data, col_idx, arrow_convert_idx,
759: 		                  nested_offset);
760: 		break;
761: 	}
762: 	case LogicalTypeId::LIST: {
763: 		ArrowToDuckDBList(vector, array, scan_state, size, arrow_convert_data, col_idx, arrow_convert_idx,
764: 		                  nested_offset, parent_mask);
765: 		break;
766: 	}
767: 	case LogicalTypeId::MAP: {
768: 		//! Since this is a map we skip first child, because its a struct
769: 		auto &struct_arrow = *array.children[0];
770: 		auto &child_entries = StructVector::GetEntries(vector);
771: 		D_ASSERT(child_entries.size() == 2);
772: 		auto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
773: 		if (nested_offset != -1) {
774: 			offsets = (uint32_t *)array.buffers[1] + nested_offset;
775: 		}
776: 		auto &struct_validity_mask = FlatVector::Validity(vector);
777: 		//! Fill the children
778: 		for (idx_t type_idx = 0; type_idx < (idx_t)struct_arrow.n_children; type_idx++) {
779: 			ArrowToDuckDBMapList(*child_entries[type_idx], *struct_arrow.children[type_idx], scan_state, size,
780: 			                     arrow_convert_data, col_idx, arrow_convert_idx, offsets, &struct_validity_mask);
781: 		}
782: 		break;
783: 	}
784: 	case LogicalTypeId::STRUCT: {
785: 		//! Fill the children
786: 		auto &child_entries = StructVector::GetEntries(vector);
787: 		auto &struct_validity_mask = FlatVector::Validity(vector);
788: 		for (idx_t type_idx = 0; type_idx < (idx_t)array.n_children; type_idx++) {
789: 			SetValidityMask(*child_entries[type_idx], *array.children[type_idx], scan_state, size, nested_offset);
790: 			ColumnArrowToDuckDB(*child_entries[type_idx], *array.children[type_idx], scan_state, size,
791: 			                    arrow_convert_data, col_idx, arrow_convert_idx, nested_offset, &struct_validity_mask);
792: 		}
793: 		break;
794: 	}
795: 	default:
796: 		throw std::runtime_error("Unsupported type " + vector.GetType().ToString());
797: 	}
798: }
799: 
800: template <class T>
801: static void SetSelectionVectorLoop(SelectionVector &sel, data_ptr_t indices_p, idx_t size) {
802: 	auto indices = (T *)indices_p;
803: 	for (idx_t row = 0; row < size; row++) {
804: 		sel.set_index(row, indices[row]);
805: 	}
806: }
807: 
808: template <class T>
809: static void SetSelectionVectorLoopWithChecks(SelectionVector &sel, data_ptr_t indices_p, idx_t size) {
810: 
811: 	auto indices = (T *)indices_p;
812: 	for (idx_t row = 0; row < size; row++) {
813: 		if (indices[row] > NumericLimits<uint32_t>::Maximum()) {
814: 			throw std::runtime_error("DuckDB only supports indices that fit on an uint32");
815: 		}
816: 		sel.set_index(row, indices[row]);
817: 	}
818: }
819: 
820: template <class T>
821: static void SetMaskedSelectionVectorLoop(SelectionVector &sel, data_ptr_t indices_p, idx_t size, ValidityMask &mask,
822:                                          idx_t last_element_pos) {
823: 	auto indices = (T *)indices_p;
824: 	for (idx_t row = 0; row < size; row++) {
825: 		if (mask.RowIsValid(row)) {
826: 			sel.set_index(row, indices[row]);
827: 		} else {
828: 			//! Need to point out to last element
829: 			sel.set_index(row, last_element_pos);
830: 		}
831: 	}
832: }
833: 
834: void SetSelectionVector(SelectionVector &sel, data_ptr_t indices_p, LogicalType &logical_type, idx_t size,
835:                         ValidityMask *mask = nullptr, idx_t last_element_pos = 0) {
836: 	sel.Initialize(size);
837: 
838: 	if (mask) {
839: 		switch (logical_type.id()) {
840: 		case LogicalTypeId::UTINYINT:
841: 			SetMaskedSelectionVectorLoop<uint8_t>(sel, indices_p, size, *mask, last_element_pos);
842: 			break;
843: 		case LogicalTypeId::TINYINT:
844: 			SetMaskedSelectionVectorLoop<int8_t>(sel, indices_p, size, *mask, last_element_pos);
845: 			break;
846: 		case LogicalTypeId::USMALLINT:
847: 			SetMaskedSelectionVectorLoop<uint16_t>(sel, indices_p, size, *mask, last_element_pos);
848: 			break;
849: 		case LogicalTypeId::SMALLINT:
850: 			SetMaskedSelectionVectorLoop<int16_t>(sel, indices_p, size, *mask, last_element_pos);
851: 			break;
852: 		case LogicalTypeId::UINTEGER:
853: 			if (last_element_pos > NumericLimits<uint32_t>::Maximum()) {
854: 				//! Its guaranteed that our indices will point to the last element, so just throw an error
855: 				throw std::runtime_error("DuckDB only supports indices that fit on an uint32");
856: 			}
857: 			SetMaskedSelectionVectorLoop<uint32_t>(sel, indices_p, size, *mask, last_element_pos);
858: 			break;
859: 		case LogicalTypeId::INTEGER:
860: 			SetMaskedSelectionVectorLoop<int32_t>(sel, indices_p, size, *mask, last_element_pos);
861: 			break;
862: 		case LogicalTypeId::UBIGINT:
863: 			if (last_element_pos > NumericLimits<uint32_t>::Maximum()) {
864: 				//! Its guaranteed that our indices will point to the last element, so just throw an error
865: 				throw std::runtime_error("DuckDB only supports indices that fit on an uint32");
866: 			}
867: 			SetMaskedSelectionVectorLoop<uint64_t>(sel, indices_p, size, *mask, last_element_pos);
868: 			break;
869: 		case LogicalTypeId::BIGINT:
870: 			if (last_element_pos > NumericLimits<uint32_t>::Maximum()) {
871: 				//! Its guaranteed that our indices will point to the last element, so just throw an error
872: 				throw std::runtime_error("DuckDB only supports indices that fit on an uint32");
873: 			}
874: 			SetMaskedSelectionVectorLoop<int64_t>(sel, indices_p, size, *mask, last_element_pos);
875: 			break;
876: 
877: 		default:
878: 			throw std::runtime_error("(Arrow) Unsupported type for selection vectors " + logical_type.ToString());
879: 		}
880: 
881: 	} else {
882: 		switch (logical_type.id()) {
883: 		case LogicalTypeId::UTINYINT:
884: 			SetSelectionVectorLoop<uint8_t>(sel, indices_p, size);
885: 			break;
886: 		case LogicalTypeId::TINYINT:
887: 			SetSelectionVectorLoop<int8_t>(sel, indices_p, size);
888: 			break;
889: 		case LogicalTypeId::USMALLINT:
890: 			SetSelectionVectorLoop<uint16_t>(sel, indices_p, size);
891: 			break;
892: 		case LogicalTypeId::SMALLINT:
893: 			SetSelectionVectorLoop<int16_t>(sel, indices_p, size);
894: 			break;
895: 		case LogicalTypeId::UINTEGER:
896: 			SetSelectionVectorLoop<uint32_t>(sel, indices_p, size);
897: 			break;
898: 		case LogicalTypeId::INTEGER:
899: 			SetSelectionVectorLoop<int32_t>(sel, indices_p, size);
900: 			break;
901: 		case LogicalTypeId::UBIGINT:
902: 			if (last_element_pos > NumericLimits<uint32_t>::Maximum()) {
903: 				//! We need to check if our indexes fit in a uint32_t
904: 				SetSelectionVectorLoopWithChecks<uint64_t>(sel, indices_p, size);
905: 			} else {
906: 				SetSelectionVectorLoop<uint64_t>(sel, indices_p, size);
907: 			}
908: 			break;
909: 		case LogicalTypeId::BIGINT:
910: 			if (last_element_pos > NumericLimits<uint32_t>::Maximum()) {
911: 				//! We need to check if our indexes fit in a uint32_t
912: 				SetSelectionVectorLoopWithChecks<int64_t>(sel, indices_p, size);
913: 			} else {
914: 				SetSelectionVectorLoop<int64_t>(sel, indices_p, size);
915: 			}
916: 			break;
917: 		default:
918: 			throw std::runtime_error("(Arrow) Unsupported type for selection vectors " + logical_type.ToString());
919: 		}
920: 	}
921: }
922: 
923: void ColumnArrowToDuckDBDictionary(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,
924:                                    std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,
925:                                    idx_t col_idx, std::pair<idx_t, idx_t> &arrow_convert_idx) {
926: 	SelectionVector sel;
927: 	auto &dict_vectors = scan_state.arrow_dictionary_vectors;
928: 	if (dict_vectors.find(col_idx) == dict_vectors.end()) {
929: 		//! We need to set the dictionary data for this column
930: 		auto base_vector = make_unique<Vector>(vector.GetType(), array.dictionary->length);
931: 		SetValidityMask(*base_vector, *array.dictionary, scan_state, array.dictionary->length, 0, array.null_count > 0);
932: 		ColumnArrowToDuckDB(*base_vector, *array.dictionary, scan_state, array.dictionary->length, arrow_convert_data,
933: 		                    col_idx, arrow_convert_idx);
934: 		dict_vectors[col_idx] = move(base_vector);
935: 	}
936: 	auto dictionary_type = arrow_convert_data[col_idx]->dictionary_type;
937: 	//! Get Pointer to Indices of Dictionary
938: 	auto indices = (data_ptr_t)array.buffers[1] +
939: 	               GetTypeIdSize(dictionary_type.InternalType()) * (scan_state.chunk_offset + array.offset);
940: 	if (array.null_count > 0) {
941: 		ValidityMask indices_validity;
942: 		GetValidityMask(indices_validity, array, scan_state, size);
943: 		SetSelectionVector(sel, indices, dictionary_type, size, &indices_validity, array.dictionary->length);
944: 	} else {
945: 		SetSelectionVector(sel, indices, dictionary_type, size);
946: 	}
947: 	vector.Slice(*dict_vectors[col_idx], sel, size);
948: }
949: void ArrowTableFunction::ArrowToDuckDB(ArrowScanState &scan_state,
950:                                        std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,
951:                                        DataChunk &output, idx_t start) {
952: 	for (idx_t idx = 0; idx < scan_state.column_ids.size(); idx++) {
953: 		auto col_idx = scan_state.column_ids[idx];
954: 		if (col_idx == COLUMN_IDENTIFIER_ROW_ID) {
955: 			output.data[idx].Sequence(start, start + output.size());
956: 		} else {
957: 			std::pair<idx_t, idx_t> arrow_convert_idx {0, 0};
958: 			auto &array = *scan_state.chunk->arrow_array.children[col_idx];
959: 			if (!array.release) {
960: 				throw InvalidInputException("arrow_scan: released array passed");
961: 			}
962: 			if (array.length != scan_state.chunk->arrow_array.length) {
963: 				throw InvalidInputException("arrow_scan: array length mismatch");
964: 			}
965: 			if (array.dictionary) {
966: 				ColumnArrowToDuckDBDictionary(output.data[idx], array, scan_state, output.size(), arrow_convert_data,
967: 				                              col_idx, arrow_convert_idx);
968: 			} else {
969: 				SetValidityMask(output.data[idx], array, scan_state, output.size(), -1);
970: 				ColumnArrowToDuckDB(output.data[idx], array, scan_state, output.size(), arrow_convert_data, col_idx,
971: 				                    arrow_convert_idx);
972: 			}
973: 		}
974: 	}
975: }
976: 
977: void ArrowTableFunction::ArrowScanFunction(ClientContext &context, const FunctionData *bind_data,
978:                                            FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
979: 	auto &data = (ArrowScanFunctionData &)*bind_data;
980: 	auto &state = (ArrowScanState &)*operator_state;
981: 
982: 	//! have we run out of data on the current chunk? move to next one
983: 	if (state.chunk_offset >= (idx_t)state.chunk->arrow_array.length) {
984: 		state.chunk_offset = 0;
985: 		state.arrow_dictionary_vectors.clear();
986: 		state.chunk = data.stream->GetNextChunk();
987: 	}
988: 
989: 	//! have we run out of chunks? we are done
990: 	if (!state.chunk->arrow_array.release) {
991: 		return;
992: 	}
993: 
994: 	int64_t output_size = MinValue<int64_t>(STANDARD_VECTOR_SIZE, state.chunk->arrow_array.length - state.chunk_offset);
995: 	data.lines_read += output_size;
996: 	output.SetCardinality(output_size);
997: 	ArrowToDuckDB(state, data.arrow_convert_data, output, data.lines_read - output_size);
998: 	output.Verify();
999: 	state.chunk_offset += output.size();
1000: }
1001: 
1002: void ArrowTableFunction::ArrowScanFunctionParallel(ClientContext &context, const FunctionData *bind_data,
1003:                                                    FunctionOperatorData *operator_state, DataChunk *input,
1004:                                                    DataChunk &output, ParallelState *parallel_state_p) {
1005: 	auto &data = (ArrowScanFunctionData &)*bind_data;
1006: 	auto &state = (ArrowScanState &)*operator_state;
1007: 	//! Out of tuples in this chunk
1008: 	if (state.chunk_offset >= (idx_t)state.chunk->arrow_array.length) {
1009: 		return;
1010: 	}
1011: 	int64_t output_size = MinValue<int64_t>(STANDARD_VECTOR_SIZE, state.chunk->arrow_array.length - state.chunk_offset);
1012: 	data.lines_read += output_size;
1013: 	output.SetCardinality(output_size);
1014: 	ArrowToDuckDB(state, data.arrow_convert_data, output, data.lines_read - output_size);
1015: 	output.Verify();
1016: 	state.chunk_offset += output.size();
1017: }
1018: 
1019: idx_t ArrowTableFunction::ArrowScanMaxThreads(ClientContext &context, const FunctionData *bind_data_p) {
1020: 	auto &bind_data = (const ArrowScanFunctionData &)*bind_data_p;
1021: 	if (bind_data.stream->number_of_rows == -1) {
1022: 		return context.db->NumberOfThreads();
1023: 	}
1024: 	return (bind_data.stream->number_of_rows + bind_data.rows_per_thread - 1) / bind_data.rows_per_thread;
1025: }
1026: 
1027: unique_ptr<ParallelState> ArrowTableFunction::ArrowScanInitParallelState(ClientContext &context,
1028:                                                                          const FunctionData *bind_data_p) {
1029: 	return make_unique<ParallelArrowScanState>();
1030: }
1031: 
1032: bool ArrowTableFunction::ArrowScanParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,
1033:                                                     FunctionOperatorData *operator_state,
1034:                                                     ParallelState *parallel_state_p) {
1035: 	auto &bind_data = (const ArrowScanFunctionData &)*bind_data_p;
1036: 	auto &state = (ArrowScanState &)*operator_state;
1037: 
1038: 	state.chunk_offset = 0;
1039: 	state.chunk = bind_data.stream->GetNextChunk();
1040: 	//! have we run out of chunks? we are done
1041: 	if (!state.chunk->arrow_array.release) {
1042: 		return false;
1043: 	}
1044: 	return true;
1045: }
1046: 
1047: unique_ptr<FunctionOperatorData>
1048: ArrowTableFunction::ArrowScanParallelInit(ClientContext &context, const FunctionData *bind_data_p, ParallelState *state,
1049:                                           const vector<column_t> &column_ids, TableFilterCollection *filters) {
1050: 	auto current_chunk = make_unique<ArrowArrayWrapper>();
1051: 	auto result = make_unique<ArrowScanState>(move(current_chunk));
1052: 	result->column_ids = column_ids;
1053: 	if (!ArrowScanParallelStateNext(context, bind_data_p, result.get(), state)) {
1054: 		return nullptr;
1055: 	}
1056: 	return move(result);
1057: }
1058: 
1059: unique_ptr<NodeStatistics> ArrowTableFunction::ArrowScanCardinality(ClientContext &context, const FunctionData *data) {
1060: 	auto &bind_data = (ArrowScanFunctionData &)*data;
1061: 	return make_unique<NodeStatistics>(bind_data.stream->number_of_rows, bind_data.stream->number_of_rows);
1062: }
1063: 
1064: int ArrowTableFunction::ArrowProgress(ClientContext &context, const FunctionData *bind_data_p) {
1065: 	auto &bind_data = (const ArrowScanFunctionData &)*bind_data_p;
1066: 	if (bind_data.stream->number_of_rows == 0) {
1067: 		return 100;
1068: 	}
1069: 	auto percentage = bind_data.lines_read * 100 / bind_data.stream->number_of_rows;
1070: 	return percentage;
1071: }
1072: 
1073: void ArrowTableFunction::RegisterFunction(BuiltinFunctions &set) {
1074: 	TableFunctionSet arrow("arrow_scan");
1075: 	arrow.AddFunction(TableFunction({LogicalType::POINTER, LogicalType::POINTER, LogicalType::UBIGINT},
1076: 	                                ArrowScanFunction, ArrowScanBind, ArrowScanInit, nullptr, nullptr, nullptr,
1077: 	                                ArrowScanCardinality, nullptr, nullptr, ArrowScanMaxThreads,
1078: 	                                ArrowScanInitParallelState, ArrowScanFunctionParallel, ArrowScanParallelInit,
1079: 	                                ArrowScanParallelStateNext, true, false, ArrowProgress));
1080: 	set.AddFunction(arrow);
1081: }
1082: 
1083: void BuiltinFunctions::RegisterArrowFunctions() {
1084: 	ArrowTableFunction::RegisterFunction(*this);
1085: }
1086: } // namespace duckdb
[end of src/function/table/arrow.cpp]
[start of src/include/duckdb/function/table/arrow.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/function/table/arrow.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/function/table_function.hpp"
12: #include "duckdb/parallel/parallel_state.hpp"
13: #include "duckdb/common/arrow_wrapper.hpp"
14: #include "duckdb/common/atomic.hpp"
15: 
16: namespace duckdb {
17: //===--------------------------------------------------------------------===//
18: // Arrow Variable Size Types
19: //===--------------------------------------------------------------------===//
20: enum class ArrowVariableSizeType : uint8_t { FIXED_SIZE = 0, NORMAL = 1, SUPER_SIZE = 2 };
21: 
22: //===--------------------------------------------------------------------===//
23: // Arrow Time/Date Types
24: //===--------------------------------------------------------------------===//
25: enum class ArrowDateTimeType : uint8_t {
26: 	MILLISECONDS = 0,
27: 	MICROSECONDS = 1,
28: 	NANOSECONDS = 2,
29: 	SECONDS = 3,
30: 	DAYS = 4,
31: 	MONTHS = 5
32: };
33: struct ArrowConvertData {
34: 	ArrowConvertData(LogicalType type) : dictionary_type(type) {};
35: 	ArrowConvertData() {};
36: 	//! Hold type of dictionary
37: 	LogicalType dictionary_type;
38: 	//! If its a variable size type (e.g., strings, blobs, lists) holds which type it is
39: 	vector<std::pair<ArrowVariableSizeType, idx_t>> variable_sz_type;
40: 	//! If this is a date/time holds its precision
41: 	vector<ArrowDateTimeType> date_time_precision;
42: };
43: 
44: struct ArrowScanFunctionData : public TableFunctionData {
45: 	ArrowScanFunctionData(idx_t rows_per_thread_p) : lines_read(0), rows_per_thread(rows_per_thread_p) {
46: 	}
47: 	unique_ptr<ArrowArrayStreamWrapper> stream;
48: 	//! This holds the original list type (col_idx, [ArrowListType,size])
49: 	std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> arrow_convert_data;
50: 	std::atomic<idx_t> lines_read;
51: 	ArrowSchemaWrapper schema_root;
52: 	idx_t rows_per_thread;
53: };
54: 
55: struct ArrowScanState : public FunctionOperatorData {
56: 	explicit ArrowScanState(unique_ptr<ArrowArrayWrapper> current_chunk) : chunk(move(current_chunk)) {
57: 	}
58: 	unique_ptr<ArrowArrayWrapper> chunk;
59: 	idx_t chunk_offset = 0;
60: 	idx_t chunk_idx = 0;
61: 	vector<column_t> column_ids;
62: 	//! Store child vectors for Arrow Dictionary Vectors (col-idx,vector)
63: 	unordered_map<idx_t, unique_ptr<Vector>> arrow_dictionary_vectors;
64: };
65: 
66: struct ParallelArrowScanState : public ParallelState {
67: 	ParallelArrowScanState() {
68: 	}
69: 	bool finished = false;
70: };
71: 
72: struct ArrowTableFunction {
73: public:
74: 	static void RegisterFunction(BuiltinFunctions &set);
75: 
76: private:
77: 	//! Binds an arrow table
78: 	static unique_ptr<FunctionData> ArrowScanBind(ClientContext &context, vector<Value> &inputs,
79: 	                                              unordered_map<string, Value> &named_parameters,
80: 	                                              vector<LogicalType> &input_table_types,
81: 	                                              vector<string> &input_table_names, vector<LogicalType> &return_types,
82: 	                                              vector<string> &names);
83: 	//! Actual conversion from Arrow to DuckDB
84: 	static void ArrowToDuckDB(ArrowScanState &scan_state,
85: 	                          std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,
86: 	                          DataChunk &output, idx_t start);
87: 
88: 	//! -----Single Thread Functions:-----
89: 	//! Initialize Single Thread Scan
90: 	static unique_ptr<FunctionOperatorData> ArrowScanInit(ClientContext &context, const FunctionData *bind_data,
91: 	                                                      const vector<column_t> &column_ids,
92: 	                                                      TableFilterCollection *filters);
93: 
94: 	//! Scan Function for Single Thread Execution
95: 	static void ArrowScanFunction(ClientContext &context, const FunctionData *bind_data,
96: 	                              FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output);
97: 
98: 	//! -----Multi Thread Functions:-----
99: 	//! Initialize Parallel State
100: 	static unique_ptr<ParallelState> ArrowScanInitParallelState(ClientContext &context,
101: 	                                                            const FunctionData *bind_data_p);
102: 	//! Initialize Parallel Scans
103: 	static unique_ptr<FunctionOperatorData> ArrowScanParallelInit(ClientContext &context,
104: 	                                                              const FunctionData *bind_data_p, ParallelState *state,
105: 	                                                              const vector<column_t> &column_ids,
106: 	                                                              TableFilterCollection *filters);
107: 	//! Defines Maximum Number of Threads
108: 	static idx_t ArrowScanMaxThreads(ClientContext &context, const FunctionData *bind_data_p);
109: 	//! Scan Function for Parallel Execution
110: 	static void ArrowScanFunctionParallel(ClientContext &context, const FunctionData *bind_data,
111: 	                                      FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output,
112: 	                                      ParallelState *parallel_state_p);
113: 	//! Get next chunk for the running thread
114: 	static bool ArrowScanParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,
115: 	                                       FunctionOperatorData *operator_state, ParallelState *parallel_state_p);
116: 
117: 	//! -----Utility Functions:-----
118: 	//! Gets Arrow Table's Cardinality
119: 	static unique_ptr<NodeStatistics> ArrowScanCardinality(ClientContext &context, const FunctionData *bind_data);
120: 	//! Gets the progress on the table scan, used for Progress Bars
121: 	static int ArrowProgress(ClientContext &context, const FunctionData *bind_data_p);
122: };
123: 
124: } // namespace duckdb
[end of src/include/duckdb/function/table/arrow.hpp]
[start of tools/pythonpkg/src/arrow_array_stream.cpp]
1: #include "duckdb/common/assert.hpp"
2: #include "include/duckdb_python/arrow_array_stream.hpp"
3: #include "duckdb/common/common.hpp"
4: 
5: namespace duckdb {
6: 
7: PythonTableArrowArrayStream::PythonTableArrowArrayStream(PyObject *arrow_table_p,
8:                                                          PythonTableArrowArrayStreamFactory *factory)
9:     : factory(factory), arrow_table(arrow_table_p), chunk_idx(0) {
10: 	stream = make_unique<ArrowArrayStreamWrapper>();
11: 	InitializeFunctionPointers(&stream->arrow_array_stream);
12: 	py::handle table_handle(arrow_table_p);
13: 	batches = table_handle.attr("to_batches")();
14: 	py::int_ num_rows_func = -1;
15: 	if (py::hasattr(table_handle, "num_rows")) {
16: 		num_rows_func = table_handle.attr("num_rows");
17: 	}
18: 	stream->number_of_rows = num_rows_func;
19: 
20: 	stream->arrow_array_stream.private_data = this;
21: }
22: 
23: void PythonTableArrowArrayStream::InitializeFunctionPointers(ArrowArrayStream *stream) {
24: 	stream->get_schema = PythonTableArrowArrayStream::GetSchema;
25: 	stream->get_next = PythonTableArrowArrayStream::GetNext;
26: 	stream->release = PythonTableArrowArrayStream::Release;
27: 	stream->get_last_error = PythonTableArrowArrayStream::GetLastError;
28: }
29: 
30: unique_ptr<ArrowArrayStreamWrapper> PythonTableArrowArrayStreamFactory::Produce(uintptr_t factory_ptr) {
31: 	py::gil_scoped_acquire acquire;
32: 	PythonTableArrowArrayStreamFactory *factory = (PythonTableArrowArrayStreamFactory *)factory_ptr;
33: 	if (!factory->arrow_table) {
34: 		return nullptr;
35: 	}
36: 	//! This is a bit hacky, but has to be this way to hide pybind from the main duckdb lib
37: 	auto table_stream = new PythonTableArrowArrayStream(factory->arrow_table, factory);
38: 	return move(table_stream->stream);
39: }
40: 
41: int PythonTableArrowArrayStream::PythonTableArrowArrayStream::GetSchema(ArrowArrayStream *stream,
42:                                                                         struct ArrowSchema *out) {
43: 	D_ASSERT(stream->private_data);
44: 	py::gil_scoped_acquire acquire;
45: 	auto my_stream = (PythonTableArrowArrayStream *)stream->private_data;
46: 	if (!stream->release) {
47: 		my_stream->last_error = "stream was released";
48: 		return -1;
49: 	}
50: 	py::handle table_handle(my_stream->arrow_table);
51: 	auto schema = table_handle.attr("schema");
52: 	if (!py::hasattr(schema, "_export_to_c")) {
53: 		my_stream->last_error = "failed to acquire export_to_c function";
54: 		return -1;
55: 	}
56: 	auto export_to_c = schema.attr("_export_to_c");
57: 	export_to_c((uint64_t)out);
58: 	return 0;
59: }
60: 
61: int PythonTableArrowArrayStream::GetNext(struct ArrowArrayStream *stream, struct ArrowArray *out) {
62: 	D_ASSERT(stream->private_data);
63: 	py::gil_scoped_acquire acquire;
64: 	auto my_stream = (PythonTableArrowArrayStream *)stream->private_data;
65: 	if (!stream->release) {
66: 		my_stream->last_error = "stream was released";
67: 		return -1;
68: 	}
69: 	if (my_stream->chunk_idx >= py::len(my_stream->batches)) {
70: 		out->release = nullptr;
71: 		return 0;
72: 	}
73: 	auto stream_batch = my_stream->batches[my_stream->chunk_idx++];
74: 	if (!py::hasattr(stream_batch, "_export_to_c")) {
75: 		my_stream->last_error = "failed to acquire export_to_c function";
76: 		return -1;
77: 	}
78: 	auto export_to_c = stream_batch.attr("_export_to_c");
79: 	export_to_c((uint64_t)out);
80: 	return 0;
81: }
82: 
83: void PythonTableArrowArrayStream::Release(struct ArrowArrayStream *stream) {
84: 	py::gil_scoped_acquire acquire;
85: 	if (!stream->release) {
86: 		return;
87: 	}
88: 	stream->release = nullptr;
89: 	auto private_data = (PythonTableArrowArrayStream *)stream->private_data;
90: 	delete (PythonTableArrowArrayStream *)stream->private_data;
91: }
92: 
93: const char *PythonTableArrowArrayStream::GetLastError(struct ArrowArrayStream *stream) {
94: 	if (!stream->release) {
95: 		return "stream was released";
96: 	}
97: 	D_ASSERT(stream->private_data);
98: 	auto my_stream = (PythonTableArrowArrayStream *)stream->private_data;
99: 	return my_stream->last_error.c_str();
100: }
101: } // namespace duckdb
[end of tools/pythonpkg/src/arrow_array_stream.cpp]
[start of tools/pythonpkg/src/include/duckdb_python/arrow_array_stream.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb_python/arrow/arrow_array_stream.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include <string>
12: #include "duckdb/common/atomic.hpp"
13: #include "duckdb/common/constants.hpp"
14: #include "duckdb/common/arrow_wrapper.hpp"
15: #include "pybind_wrapper.hpp"
16: namespace duckdb {
17: class PythonTableArrowArrayStreamFactory {
18: public:
19: 	explicit PythonTableArrowArrayStreamFactory(PyObject *arrow_table) : arrow_table(arrow_table) {};
20: 	static unique_ptr<ArrowArrayStreamWrapper> Produce(uintptr_t factory);
21: 	PyObject *arrow_table;
22: };
23: 
24: class PythonTableArrowArrayStream {
25: public:
26: 	explicit PythonTableArrowArrayStream(PyObject *arrow_table, PythonTableArrowArrayStreamFactory *factory);
27: 
28: 	unique_ptr<ArrowArrayStreamWrapper> stream;
29: 	PythonTableArrowArrayStreamFactory *factory;
30: 
31: private:
32: 	static void InitializeFunctionPointers(ArrowArrayStream *stream);
33: 	static int GetSchema(struct ArrowArrayStream *stream, struct ArrowSchema *out);
34: 	static int GetNext(struct ArrowArrayStream *stream, struct ArrowArray *out);
35: 	static void Release(struct ArrowArrayStream *stream);
36: 	static const char *GetLastError(struct ArrowArrayStream *stream);
37: 
38: 	std::string last_error;
39: 	PyObject *arrow_table;
40: 	py::list batches;
41: 	std::atomic<idx_t> chunk_idx;
42: };
43: } // namespace duckdb
[end of tools/pythonpkg/src/include/duckdb_python/arrow_array_stream.hpp]
[start of tools/pythonpkg/src/pyconnection.cpp]
1: #include "duckdb_python/pyconnection.hpp"
2: #include "duckdb_python/pyresult.hpp"
3: #include "duckdb_python/pyrelation.hpp"
4: #include "duckdb_python/pandas_scan.hpp"
5: #include "duckdb_python/map.hpp"
6: 
7: #include "duckdb/common/arrow.hpp"
8: #include "duckdb_python/arrow_array_stream.hpp"
9: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
10: #include "duckdb/main/client_context.hpp"
11: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
12: #include "duckdb/common/types/vector.hpp"
13: #include "duckdb/common/printer.hpp"
14: #include "duckdb/main/config.hpp"
15: #include "duckdb/parser/expression/constant_expression.hpp"
16: #include "duckdb/parser/expression/function_expression.hpp"
17: #include "duckdb/parser/tableref/table_function_ref.hpp"
18: 
19: #include "extension/extension_helper.hpp"
20: 
21: #include "datetime.h" // from Python
22: 
23: #include <random>
24: 
25: namespace duckdb {
26: 
27: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::default_connection = nullptr;
28: 
29: void DuckDBPyConnection::Initialize(py::handle &m) {
30: 	py::class_<DuckDBPyConnection, shared_ptr<DuckDBPyConnection>>(m, "DuckDBPyConnection")
31: 	    .def("cursor", &DuckDBPyConnection::Cursor, "Create a duplicate of the current connection")
32: 	    .def("duplicate", &DuckDBPyConnection::Cursor, "Create a duplicate of the current connection")
33: 	    .def("execute", &DuckDBPyConnection::Execute,
34: 	         "Execute the given SQL query, optionally using prepared statements with parameters set", py::arg("query"),
35: 	         py::arg("parameters") = py::list(), py::arg("multiple_parameter_sets") = false)
36: 	    .def("executemany", &DuckDBPyConnection::ExecuteMany,
37: 	         "Execute the given prepared statement multiple times using the list of parameter sets in parameters",
38: 	         py::arg("query"), py::arg("parameters") = py::list())
39: 	    .def("close", &DuckDBPyConnection::Close, "Close the connection")
40: 	    .def("fetchone", &DuckDBPyConnection::FetchOne, "Fetch a single row from a result following execute")
41: 	    .def("fetchall", &DuckDBPyConnection::FetchAll, "Fetch all rows from a result following execute")
42: 	    .def("fetchnumpy", &DuckDBPyConnection::FetchNumpy, "Fetch a result as list of NumPy arrays following execute")
43: 	    .def("fetchdf", &DuckDBPyConnection::FetchDF, "Fetch a result as Data.Frame following execute()")
44: 	    .def("fetch_df", &DuckDBPyConnection::FetchDF, "Fetch a result as Data.Frame following execute()")
45: 	    .def("fetch_df_chunk", &DuckDBPyConnection::FetchDFChunk,
46: 	         "Fetch a chunk of the result as Data.Frame following execute()", py::arg("vectors_per_chunk") = 1)
47: 	    .def("df", &DuckDBPyConnection::FetchDF, "Fetch a result as Data.Frame following execute()")
48: 	    .def("fetch_arrow_table", &DuckDBPyConnection::FetchArrow, "Fetch a result as Arrow table following execute()")
49: 	    .def("arrow", &DuckDBPyConnection::FetchArrow, "Fetch a result as Arrow table following execute()")
50: 	    .def("begin", &DuckDBPyConnection::Begin, "Start a new transaction")
51: 	    .def("commit", &DuckDBPyConnection::Commit, "Commit changes performed within a transaction")
52: 	    .def("rollback", &DuckDBPyConnection::Rollback, "Roll back changes performed within a transaction")
53: 	    .def("append", &DuckDBPyConnection::Append, "Append the passed Data.Frame to the named table",
54: 	         py::arg("table_name"), py::arg("df"))
55: 	    .def("register", &DuckDBPyConnection::RegisterDF,
56: 	         "Register the passed Data.Frame value for querying with a view", py::arg("view_name"), py::arg("df"))
57: 	    .def("unregister", &DuckDBPyConnection::UnregisterPythonObject, "Unregister the view name",
58: 	         py::arg("view_name"))
59: 	    .def("register_arrow", &DuckDBPyConnection::RegisterArrow,
60: 	         "Register the passed Arrow Table for querying with a view", py::arg("view_name"), py::arg("arrow_table"),
61: 	         py::arg("rows_per_thread") = 1000000)
62: 	    .def("table", &DuckDBPyConnection::Table, "Create a relation object for the name'd table",
63: 	         py::arg("table_name"))
64: 	    .def("view", &DuckDBPyConnection::View, "Create a relation object for the name'd view", py::arg("view_name"))
65: 	    .def("values", &DuckDBPyConnection::Values, "Create a relation object from the passed values",
66: 	         py::arg("values"))
67: 	    .def("table_function", &DuckDBPyConnection::TableFunction,
68: 	         "Create a relation object from the name'd table function with given parameters", py::arg("name"),
69: 	         py::arg("parameters") = py::list())
70: 	    .def("from_query", &DuckDBPyConnection::FromQuery, "Create a relation object from the given SQL query",
71: 	         py::arg("query"), py::arg("alias") = "query_relation")
72: 	    .def("query", &DuckDBPyConnection::FromQuery, "Create a relation object from the given SQL query",
73: 	         py::arg("query"), py::arg("alias") = "query_relation")
74: 	    .def("from_df", &DuckDBPyConnection::FromDF, "Create a relation object from the Data.Frame in df",
75: 	         py::arg("df") = py::none())
76: 	    .def("from_arrow_table", &DuckDBPyConnection::FromArrowTable, "Create a relation object from an Arrow table",
77: 	         py::arg("table"), py::arg("rows_per_thread") = 1000000)
78: 	    .def("df", &DuckDBPyConnection::FromDF, "Create a relation object from the Data.Frame in df (alias of from_df)",
79: 	         py::arg("df"))
80: 	    .def("from_csv_auto", &DuckDBPyConnection::FromCsvAuto,
81: 	         "Create a relation object from the CSV file in file_name", py::arg("file_name"))
82: 	    .def("from_parquet", &DuckDBPyConnection::FromParquet,
83: 	         "Create a relation object from the Parquet file in file_name", py::arg("file_name"))
84: 	    .def("__getattr__", &DuckDBPyConnection::GetAttr, "Get result set attributes, mainly column names");
85: 
86: 	PyDateTime_IMPORT;
87: }
88: 
89: DuckDBPyConnection *DuckDBPyConnection::ExecuteMany(const string &query, py::object params) {
90: 	Execute(query, std::move(params), true);
91: 	return this;
92: }
93: 
94: DuckDBPyConnection *DuckDBPyConnection::Execute(const string &query, py::object params, bool many) {
95: 	if (!connection) {
96: 		throw std::runtime_error("connection closed");
97: 	}
98: 	result = nullptr;
99: 
100: 	auto statements = connection->ExtractStatements(query);
101: 	if (statements.empty()) {
102: 		// no statements to execute
103: 		return this;
104: 	}
105: 	// if there are multiple statements, we directly execute the statements besides the last one
106: 	// we only return the result of the last statement to the user, unless one of the previous statements fails
107: 	for (idx_t i = 0; i + 1 < statements.size(); i++) {
108: 		auto res = connection->Query(move(statements[i]));
109: 		if (!res->success) {
110: 			throw std::runtime_error(res->error);
111: 		}
112: 	}
113: 
114: 	auto prep = connection->Prepare(move(statements.back()));
115: 	if (!prep->success) {
116: 		throw std::runtime_error(prep->error);
117: 	}
118: 
119: 	// this is a list of a list of parameters in executemany
120: 	py::list params_set;
121: 	if (!many) {
122: 		params_set = py::list(1);
123: 		params_set[0] = params;
124: 	} else {
125: 		params_set = params;
126: 	}
127: 
128: 	for (pybind11::handle single_query_params : params_set) {
129: 		if (prep->n_param != py::len(single_query_params)) {
130: 			throw std::runtime_error("Prepared statement needs " + to_string(prep->n_param) + " parameters, " +
131: 			                         to_string(py::len(single_query_params)) + " given");
132: 		}
133: 		auto args = DuckDBPyConnection::TransformPythonParamList(single_query_params);
134: 		auto res = make_unique<DuckDBPyResult>();
135: 		{
136: 			py::gil_scoped_release release;
137: 			res->result = prep->Execute(args);
138: 		}
139: 		if (!res->result->success) {
140: 			throw std::runtime_error(res->result->error);
141: 		}
142: 		if (!many) {
143: 			result = move(res);
144: 		}
145: 	}
146: 	return this;
147: }
148: 
149: DuckDBPyConnection *DuckDBPyConnection::Append(const string &name, py::object value) {
150: 	RegisterDF("__append_df", std::move(value));
151: 	return Execute("INSERT INTO \"" + name + "\" SELECT * FROM __append_df");
152: }
153: 
154: DuckDBPyConnection *DuckDBPyConnection::RegisterDF(const string &name, py::object value) {
155: 	if (!connection) {
156: 		throw std::runtime_error("connection closed");
157: 	}
158: 	connection->TableFunction("pandas_scan", {Value::POINTER((uintptr_t)value.ptr())})->CreateView(name, true, true);
159: 	// keep a reference
160: 	auto object = make_unique<RegisteredObject>(value);
161: 	registered_objects[name] = move(object);
162: 	return this;
163: }
164: 
165: DuckDBPyConnection *DuckDBPyConnection::RegisterArrow(const string &name, py::object &table,
166:                                                       const idx_t rows_per_tuple) {
167: 	if (!connection) {
168: 		throw std::runtime_error("connection closed");
169: 	}
170: 	auto py_object_type = string(py::str(table.get_type().attr("__name__")));
171: 	if (table.is_none() || (py_object_type != "Table" && py_object_type != "FileSystemDataset")) {
172: 		throw std::runtime_error("Only arrow tables/datasets are supported");
173: 	}
174: 	auto stream_factory = make_unique<PythonTableArrowArrayStreamFactory>(table.ptr());
175: 
176: 	auto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;
177: 	connection
178: 	    ->TableFunction("arrow_scan",
179: 	                    {Value::POINTER((uintptr_t)stream_factory.get()),
180: 	                     Value::POINTER((uintptr_t)stream_factory_produce), Value::UBIGINT(rows_per_tuple)})
181: 	    ->CreateView(name, true, true);
182: 	auto object = make_unique<RegisteredArrow>(move(stream_factory), move(table));
183: 	registered_objects[name] = move(object);
184: 	return this;
185: }
186: 
187: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromQuery(const string &query, const string &alias) {
188: 	if (!connection) {
189: 		throw std::runtime_error("connection closed");
190: 	}
191: 	return make_unique<DuckDBPyRelation>(connection->RelationFromQuery(query, alias));
192: }
193: 
194: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::Table(const string &tname) {
195: 	if (!connection) {
196: 		throw std::runtime_error("connection closed");
197: 	}
198: 	return make_unique<DuckDBPyRelation>(connection->Table(tname));
199: }
200: 
201: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::Values(py::object params) {
202: 	if (!connection) {
203: 		throw std::runtime_error("connection closed");
204: 	}
205: 	vector<vector<Value>> values {DuckDBPyConnection::TransformPythonParamList(std::move(params))};
206: 	return make_unique<DuckDBPyRelation>(connection->Values(values));
207: }
208: 
209: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::View(const string &vname) {
210: 	if (!connection) {
211: 		throw std::runtime_error("connection closed");
212: 	}
213: 	return make_unique<DuckDBPyRelation>(connection->View(vname));
214: }
215: 
216: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::TableFunction(const string &fname, py::object params) {
217: 	if (!connection) {
218: 		throw std::runtime_error("connection closed");
219: 	}
220: 
221: 	return make_unique<DuckDBPyRelation>(
222: 	    connection->TableFunction(fname, DuckDBPyConnection::TransformPythonParamList(std::move(params))));
223: }
224: 
225: static std::string GenerateRandomName() {
226: 	std::random_device rd;
227: 	std::mt19937 gen(rd());
228: 	std::uniform_int_distribution<> dis(0, 15);
229: 
230: 	std::stringstream ss;
231: 	int i;
232: 	ss << std::hex;
233: 	for (i = 0; i < 16; i++) {
234: 		ss << dis(gen);
235: 	}
236: 	return ss.str();
237: }
238: 
239: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromDF(py::object value) {
240: 	if (!connection) {
241: 		throw std::runtime_error("connection closed");
242: 	}
243: 	string name = "df_" + GenerateRandomName();
244: 	registered_objects[name] = make_unique<RegisteredObject>(value);
245: 	vector<Value> params;
246: 	params.emplace_back(Value::POINTER((uintptr_t)value.ptr()));
247: 	return make_unique<DuckDBPyRelation>(connection->TableFunction("pandas_scan", params)->Alias(name));
248: }
249: 
250: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromCsvAuto(const string &filename) {
251: 	if (!connection) {
252: 		throw std::runtime_error("connection closed");
253: 	}
254: 	vector<Value> params;
255: 	params.emplace_back(filename);
256: 	return make_unique<DuckDBPyRelation>(connection->TableFunction("read_csv_auto", params)->Alias(filename));
257: }
258: 
259: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquet(const string &filename) {
260: 	if (!connection) {
261: 		throw std::runtime_error("connection closed");
262: 	}
263: 	vector<Value> params;
264: 	params.emplace_back(filename);
265: 	return make_unique<DuckDBPyRelation>(connection->TableFunction("parquet_scan", params)->Alias(filename));
266: }
267: 
268: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromArrowTable(py::object &table, const idx_t rows_per_tuple) {
269: 	if (!connection) {
270: 		throw std::runtime_error("connection closed");
271: 	}
272: 	py::gil_scoped_acquire acquire;
273: 
274: 	// the following is a careful dance around having to depend on pyarrow
275: 	auto py_object_type = string(py::str(table.get_type().attr("__name__")));
276: 	if (table.is_none() || (py_object_type != "Table" && py_object_type != "FileSystemDataset")) {
277: 		throw std::runtime_error("Only arrow tables/datasets are supported");
278: 	}
279: 
280: 	string name = "arrow_table_" + GenerateRandomName();
281: 
282: 	auto stream_factory = make_unique<PythonTableArrowArrayStreamFactory>(table.ptr());
283: 
284: 	unique_ptr<ArrowArrayStreamWrapper> (*stream_factory_produce)(uintptr_t factory) =
285: 	    PythonTableArrowArrayStreamFactory::Produce;
286: 	auto rel = make_unique<DuckDBPyRelation>(
287: 	    connection
288: 	        ->TableFunction("arrow_scan",
289: 	                        {Value::POINTER((uintptr_t)stream_factory.get()),
290: 	                         Value::POINTER((uintptr_t)stream_factory_produce), Value::UBIGINT(rows_per_tuple)})
291: 	        ->Alias(name));
292: 	registered_objects[name] = make_unique<RegisteredArrow>(move(stream_factory), table);
293: 	return rel;
294: }
295: 
296: DuckDBPyConnection *DuckDBPyConnection::UnregisterPythonObject(const string &name) {
297: 	registered_objects.erase(name);
298: 
299: 	if (connection) {
300: 		connection->Query("DROP VIEW \"" + name + "\"");
301: 	}
302: 	return this;
303: }
304: 
305: DuckDBPyConnection *DuckDBPyConnection::Begin() {
306: 	Execute("BEGIN TRANSACTION");
307: 	return this;
308: }
309: 
310: DuckDBPyConnection *DuckDBPyConnection::Commit() {
311: 	if (connection->context->transaction.IsAutoCommit()) {
312: 		return this;
313: 	}
314: 	Execute("COMMIT");
315: 	return this;
316: }
317: 
318: DuckDBPyConnection *DuckDBPyConnection::Rollback() {
319: 	Execute("ROLLBACK");
320: 	return this;
321: }
322: 
323: py::object DuckDBPyConnection::GetAttr(const py::str &key) {
324: 	if (key.cast<string>() == "description") {
325: 		if (!result) {
326: 			return py::none();
327: 		}
328: 		return result->Description();
329: 	}
330: 	return py::none();
331: }
332: 
333: void DuckDBPyConnection::Close() {
334: 	result = nullptr;
335: 	connection = nullptr;
336: 	database = nullptr;
337: 	for (auto &cur : cursors) {
338: 		cur->Close();
339: 	}
340: 	cursors.clear();
341: }
342: 
343: // cursor() is stupid
344: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Cursor() {
345: 	auto res = make_shared<DuckDBPyConnection>();
346: 	res->database = database;
347: 	res->connection = make_unique<Connection>(*res->database);
348: 	cursors.push_back(res);
349: 	return res;
350: }
351: 
352: // these should be functions on the result but well
353: py::object DuckDBPyConnection::FetchOne() {
354: 	if (!result) {
355: 		throw std::runtime_error("no open result set");
356: 	}
357: 	return result->Fetchone();
358: }
359: 
360: py::list DuckDBPyConnection::FetchAll() {
361: 	if (!result) {
362: 		throw std::runtime_error("no open result set");
363: 	}
364: 	return result->Fetchall();
365: }
366: 
367: py::dict DuckDBPyConnection::FetchNumpy() {
368: 	if (!result) {
369: 		throw std::runtime_error("no open result set");
370: 	}
371: 	return result->FetchNumpy();
372: }
373: py::object DuckDBPyConnection::FetchDF() {
374: 	if (!result) {
375: 		throw std::runtime_error("no open result set");
376: 	}
377: 	return result->FetchDF();
378: }
379: 
380: py::object DuckDBPyConnection::FetchDFChunk(const idx_t vectors_per_chunk) const {
381: 	if (!result) {
382: 		throw std::runtime_error("no open result set");
383: 	}
384: 	return result->FetchDFChunk(vectors_per_chunk);
385: }
386: 
387: py::object DuckDBPyConnection::FetchArrow() {
388: 	if (!result) {
389: 		throw std::runtime_error("no open result set");
390: 	}
391: 	return result->FetchArrowTable();
392: }
393: 
394: static unique_ptr<TableFunctionRef> TryPandasReplacement(py::dict &dict, py::str &table_name) {
395: 	if (!dict.contains(table_name)) {
396: 		// not present in the globals
397: 		return nullptr;
398: 	}
399: 	auto entry = dict[table_name];
400: 
401: 	// check if there is a local or global variable
402: 	auto table_function = make_unique<TableFunctionRef>();
403: 	vector<unique_ptr<ParsedExpression>> children;
404: 	children.push_back(make_unique<ConstantExpression>(Value::POINTER((uintptr_t)entry.ptr())));
405: 	table_function->function = make_unique<FunctionExpression>("pandas_scan", move(children));
406: 	return table_function;
407: }
408: 
409: static unique_ptr<TableFunctionRef> PandasScanReplacement(const string &table_name, void *data) {
410: 	py::gil_scoped_acquire acquire;
411: 	// look in the locals first
412: 	PyObject *p = PyEval_GetLocals();
413: 	auto py_table_name = py::str(table_name);
414: 	if (p) {
415: 		auto local_dict = py::reinterpret_borrow<py::dict>(p);
416: 		auto result = TryPandasReplacement(local_dict, py_table_name);
417: 		if (result) {
418: 			return result;
419: 		}
420: 	}
421: 	// otherwise look in the globals
422: 	auto global_dict = py::globals();
423: 	return TryPandasReplacement(global_dict, py_table_name);
424: }
425: 
426: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Connect(const string &database, bool read_only,
427:                                                            const py::dict &config_dict) {
428: 	auto res = make_shared<DuckDBPyConnection>();
429: 	DBConfig config;
430: 	if (read_only) {
431: 		config.access_mode = AccessMode::READ_ONLY;
432: 	}
433: 	for (auto &kv : config_dict) {
434: 		string key = py::str(kv.first);
435: 		string val = py::str(kv.second);
436: 		auto config_property = DBConfig::GetOptionByName(key);
437: 		if (!config_property) {
438: 			throw InvalidInputException("Unrecognized configuration property \"%s\"", key);
439: 		}
440: 		config.SetOption(*config_property, Value(val));
441: 	}
442: 	if (config.enable_external_access) {
443: 		config.replacement_scans.emplace_back(PandasScanReplacement);
444: 	}
445: 
446: 	res->database = make_unique<DuckDB>(database, &config);
447: 	ExtensionHelper::LoadAllExtensions(*res->database);
448: 	res->connection = make_unique<Connection>(*res->database);
449: 
450: 	PandasScanFunction scan_fun;
451: 	CreateTableFunctionInfo scan_info(scan_fun);
452: 
453: 	MapFunction map_fun;
454: 	CreateTableFunctionInfo map_info(map_fun);
455: 
456: 	auto &context = *res->connection->context;
457: 	auto &catalog = Catalog::GetCatalog(context);
458: 	context.transaction.BeginTransaction();
459: 	catalog.CreateTableFunction(context, &scan_info);
460: 	catalog.CreateTableFunction(context, &map_info);
461: 
462: 	context.transaction.Commit();
463: 
464: 	return res;
465: }
466: 
467: vector<Value> DuckDBPyConnection::TransformPythonParamList(py::handle params) {
468: 	vector<Value> args;
469: 
470: 	auto datetime_mod = py::module::import("datetime");
471: 	auto datetime_date = datetime_mod.attr("date");
472: 	auto datetime_datetime = datetime_mod.attr("datetime");
473: 	auto datetime_time = datetime_mod.attr("time");
474: 	auto decimal_mod = py::module::import("decimal");
475: 	auto decimal_decimal = decimal_mod.attr("Decimal");
476: 
477: 	for (pybind11::handle ele : params) {
478: 		if (ele.is_none()) {
479: 			args.emplace_back();
480: 		} else if (py::isinstance<py::bool_>(ele)) {
481: 			args.push_back(Value::BOOLEAN(ele.cast<bool>()));
482: 		} else if (py::isinstance<py::int_>(ele)) {
483: 			args.push_back(Value::BIGINT(ele.cast<int64_t>()));
484: 		} else if (py::isinstance<py::float_>(ele)) {
485: 			args.push_back(Value::DOUBLE(ele.cast<double>()));
486: 		} else if (py::isinstance(ele, decimal_decimal)) {
487: 			args.emplace_back(py::str(ele).cast<string>());
488: 		} else if (py::isinstance(ele, datetime_datetime)) {
489: 			auto year = PyDateTime_GET_YEAR(ele.ptr());
490: 			auto month = PyDateTime_GET_MONTH(ele.ptr());
491: 			auto day = PyDateTime_GET_DAY(ele.ptr());
492: 			auto hour = PyDateTime_DATE_GET_HOUR(ele.ptr());
493: 			auto minute = PyDateTime_DATE_GET_MINUTE(ele.ptr());
494: 			auto second = PyDateTime_DATE_GET_SECOND(ele.ptr());
495: 			auto micros = PyDateTime_DATE_GET_MICROSECOND(ele.ptr());
496: 			args.push_back(Value::TIMESTAMP(year, month, day, hour, minute, second, micros));
497: 		} else if (py::isinstance(ele, datetime_time)) {
498: 			auto hour = PyDateTime_TIME_GET_HOUR(ele.ptr());
499: 			auto minute = PyDateTime_TIME_GET_MINUTE(ele.ptr());
500: 			auto second = PyDateTime_TIME_GET_SECOND(ele.ptr());
501: 			auto micros = PyDateTime_TIME_GET_MICROSECOND(ele.ptr());
502: 			args.push_back(Value::TIME(hour, minute, second, micros));
503: 		} else if (py::isinstance(ele, datetime_date)) {
504: 			auto year = PyDateTime_GET_YEAR(ele.ptr());
505: 			auto month = PyDateTime_GET_MONTH(ele.ptr());
506: 			auto day = PyDateTime_GET_DAY(ele.ptr());
507: 			args.push_back(Value::DATE(year, month, day));
508: 		} else if (py::isinstance<py::str>(ele)) {
509: 			args.emplace_back(ele.cast<string>());
510: 		} else if (py::isinstance<py::memoryview>(ele)) {
511: 			py::memoryview py_view = ele.cast<py::memoryview>();
512: 			PyObject *py_view_ptr = py_view.ptr();
513: 			Py_buffer *py_buf = PyMemoryView_GET_BUFFER(py_view_ptr);
514: 			args.emplace_back(Value::BLOB(const_data_ptr_t(py_buf->buf), idx_t(py_buf->len)));
515: 		} else if (py::isinstance<py::bytes>(ele)) {
516: 			const string &ele_string = ele.cast<string>();
517: 			args.emplace_back(Value::BLOB(const_data_ptr_t(ele_string.data()), ele_string.size()));
518: 		} else {
519: 			throw std::runtime_error("unknown param type " + py::str(ele.get_type()).cast<string>());
520: 		}
521: 	}
522: 	return args;
523: }
524: 
525: DuckDBPyConnection *DuckDBPyConnection::DefaultConnection() {
526: 	if (!default_connection) {
527: 		py::dict config_dict;
528: 		default_connection = DuckDBPyConnection::Connect(":memory:", false, config_dict);
529: 	}
530: 	return default_connection.get();
531: }
532: 
533: void DuckDBPyConnection::Cleanup() {
534: 	default_connection.reset();
535: }
536: 
537: } // namespace duckdb
[end of tools/pythonpkg/src/pyconnection.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: