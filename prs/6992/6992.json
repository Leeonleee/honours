{
  "repo": "duckdb/duckdb",
  "pull_number": 6992,
  "instance_id": "duckdb__duckdb-6992",
  "issue_numbers": [
    "6990",
    "6990"
  ],
  "base_commit": "29db5c7fe222c937529687724fc231e8f1d9fe5e",
  "patch": "diff --git a/extension/parquet/column_reader.cpp b/extension/parquet/column_reader.cpp\nindex f4540e4df72e..71543f294bd1 100644\n--- a/extension/parquet/column_reader.cpp\n+++ b/extension/parquet/column_reader.cpp\n@@ -736,22 +736,22 @@ void StringColumnReader::PlainReference(shared_ptr<ByteBuffer> plain_data, Vecto\n }\n \n string_t StringParquetValueConversion::DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {\n-\tauto &dict_strings = ((StringColumnReader &)reader).dict_strings;\n+\tauto &dict_strings = reader.Cast<StringColumnReader>().dict_strings;\n \treturn dict_strings[offset];\n }\n \n string_t StringParquetValueConversion::PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {\n-\tauto &scr = ((StringColumnReader &)reader);\n+\tauto &scr = reader.Cast<StringColumnReader>();\n \tuint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;\n \tplain_data.available(str_len);\n-\tauto actual_str_len = ((StringColumnReader &)reader).VerifyString(plain_data.ptr, str_len);\n+\tauto actual_str_len = reader.Cast<StringColumnReader>().VerifyString(plain_data.ptr, str_len);\n \tauto ret_str = string_t(plain_data.ptr, actual_str_len);\n \tplain_data.inc(str_len);\n \treturn ret_str;\n }\n \n void StringParquetValueConversion::PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {\n-\tauto &scr = ((StringColumnReader &)reader);\n+\tauto &scr = reader.Cast<StringColumnReader>();\n \tuint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;\n \tplain_data.inc(str_len);\n }\ndiff --git a/extension/parquet/include/boolean_column_reader.hpp b/extension/parquet/include/boolean_column_reader.hpp\nindex 6dcc96b514a6..4418edf095b9 100644\n--- a/extension/parquet/include/boolean_column_reader.hpp\n+++ b/extension/parquet/include/boolean_column_reader.hpp\n@@ -16,6 +16,9 @@ namespace duckdb {\n struct BooleanParquetValueConversion;\n \n class BooleanColumnReader : public TemplatedColumnReader<bool, BooleanParquetValueConversion> {\n+public:\n+\tstatic constexpr const PhysicalType TYPE = PhysicalType::BOOL;\n+\n public:\n \tBooleanColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t schema_idx_p,\n \t                    idx_t max_define_p, idx_t max_repeat_p)\n@@ -44,7 +47,7 @@ struct BooleanParquetValueConversion {\n \n \tstatic bool PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {\n \t\tplain_data.available(1);\n-\t\tauto &byte_pos = ((BooleanColumnReader &)reader).byte_pos;\n+\t\tauto &byte_pos = reader.Cast<BooleanColumnReader>().byte_pos;\n \t\tbool ret = (*plain_data.ptr >> byte_pos) & 1;\n \t\tbyte_pos++;\n \t\tif (byte_pos == 8) {\ndiff --git a/extension/parquet/include/callback_column_reader.hpp b/extension/parquet/include/callback_column_reader.hpp\nindex 9258e687cdb4..45c3e726ee0f 100644\n--- a/extension/parquet/include/callback_column_reader.hpp\n+++ b/extension/parquet/include/callback_column_reader.hpp\n@@ -23,6 +23,9 @@ class CallbackColumnReader\n \t    TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE,\n \t                          CallbackParquetValueConversion<PARQUET_PHYSICAL_TYPE, DUCKDB_PHYSICAL_TYPE, FUNC>>;\n \n+public:\n+\tstatic constexpr const PhysicalType TYPE = PhysicalType::INVALID;\n+\n public:\n \tCallbackColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,\n \t                     idx_t max_define_p, idx_t max_repeat_p)\ndiff --git a/extension/parquet/include/cast_column_reader.hpp b/extension/parquet/include/cast_column_reader.hpp\nindex 763eb3b24743..f5ab0c5aa7aa 100644\n--- a/extension/parquet/include/cast_column_reader.hpp\n+++ b/extension/parquet/include/cast_column_reader.hpp\n@@ -15,6 +15,9 @@ namespace duckdb {\n \n //! A column reader that represents a cast over a child reader\n class CastColumnReader : public ColumnReader {\n+public:\n+\tstatic constexpr const PhysicalType TYPE = PhysicalType::INVALID;\n+\n public:\n \tCastColumnReader(duckdb::unique_ptr<ColumnReader> child_reader, LogicalType target_type);\n \ndiff --git a/extension/parquet/include/column_reader.hpp b/extension/parquet/include/column_reader.hpp\nindex cf39ffb5a5ca..6382adaaa68a 100644\n--- a/extension/parquet/include/column_reader.hpp\n+++ b/extension/parquet/include/column_reader.hpp\n@@ -167,6 +167,23 @@ class ColumnReader {\n \tparquet_filter_t none_filter;\n \tResizeableBuffer dummy_define;\n \tResizeableBuffer dummy_repeat;\n+\n+public:\n+\ttemplate <class TARGET>\n+\tTARGET &Cast() {\n+\t\tif (TARGET::TYPE != PhysicalType::INVALID && type.InternalType() != TARGET::TYPE) {\n+\t\t\tthrow InternalException(\"Failed to cast column reader to type - type mismatch\");\n+\t\t}\n+\t\treturn (TARGET &)*this;\n+\t}\n+\n+\ttemplate <class TARGET>\n+\tconst TARGET &Cast() const {\n+\t\tif (TARGET::TYPE != PhysicalType::INVALID && type.InternalType() != TARGET::TYPE) {\n+\t\t\tthrow InternalException(\"Failed to cast column reader to type - type mismatch\");\n+\t\t}\n+\t\treturn (const TARGET &)*this;\n+\t}\n };\n \n } // namespace duckdb\ndiff --git a/extension/parquet/include/list_column_reader.hpp b/extension/parquet/include/list_column_reader.hpp\nindex e11f42f1891f..b1e9f559310d 100644\n--- a/extension/parquet/include/list_column_reader.hpp\n+++ b/extension/parquet/include/list_column_reader.hpp\n@@ -14,6 +14,9 @@\n namespace duckdb {\n \n class ListColumnReader : public ColumnReader {\n+public:\n+\tstatic constexpr const PhysicalType TYPE = PhysicalType::LIST;\n+\n public:\n \tListColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t schema_idx_p,\n \t                 idx_t max_define_p, idx_t max_repeat_p, duckdb::unique_ptr<ColumnReader> child_column_reader_p);\ndiff --git a/extension/parquet/include/row_number_column_reader.hpp b/extension/parquet/include/row_number_column_reader.hpp\nindex a27c42adb71f..8b5921386ccc 100644\n--- a/extension/parquet/include/row_number_column_reader.hpp\n+++ b/extension/parquet/include/row_number_column_reader.hpp\n@@ -18,6 +18,9 @@ namespace duckdb {\n \n //! Reads a file-absolute row number as a virtual column that's not actually stored in the file\n class RowNumberColumnReader : public ColumnReader {\n+public:\n+\tstatic constexpr const PhysicalType TYPE = PhysicalType::INT64;\n+\n public:\n \tRowNumberColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t schema_idx_p,\n \t                      idx_t max_define_p, idx_t max_repeat_p);\ndiff --git a/extension/parquet/include/string_column_reader.hpp b/extension/parquet/include/string_column_reader.hpp\nindex 98b45110973c..6086143fa3f3 100644\n--- a/extension/parquet/include/string_column_reader.hpp\n+++ b/extension/parquet/include/string_column_reader.hpp\n@@ -21,6 +21,9 @@ struct StringParquetValueConversion {\n };\n \n class StringColumnReader : public TemplatedColumnReader<string_t, StringParquetValueConversion> {\n+public:\n+\tstatic constexpr const PhysicalType TYPE = PhysicalType::VARCHAR;\n+\n public:\n \tStringColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t schema_idx_p,\n \t                   idx_t max_define_p, idx_t max_repeat_p);\ndiff --git a/extension/parquet/include/struct_column_reader.hpp b/extension/parquet/include/struct_column_reader.hpp\nindex fb4721f55174..2fcc3706acb0 100644\n--- a/extension/parquet/include/struct_column_reader.hpp\n+++ b/extension/parquet/include/struct_column_reader.hpp\n@@ -14,6 +14,9 @@\n namespace duckdb {\n \n class StructColumnReader : public ColumnReader {\n+public:\n+\tstatic constexpr const PhysicalType TYPE = PhysicalType::STRUCT;\n+\n public:\n \tStructColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t schema_idx_p,\n \t                   idx_t max_define_p, idx_t max_repeat_p,\ndiff --git a/extension/parquet/include/templated_column_reader.hpp b/extension/parquet/include/templated_column_reader.hpp\nindex b638af6b640a..59a1c13c4781 100644\n--- a/extension/parquet/include/templated_column_reader.hpp\n+++ b/extension/parquet/include/templated_column_reader.hpp\n@@ -30,6 +30,9 @@ struct TemplatedParquetValueConversion {\n \n template <class VALUE_TYPE, class VALUE_CONVERSION>\n class TemplatedColumnReader : public ColumnReader {\n+public:\n+\tstatic constexpr const PhysicalType TYPE = PhysicalType::INVALID;\n+\n public:\n \tTemplatedColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t schema_idx_p,\n \t                      idx_t max_define_p, idx_t max_repeat_p)\ndiff --git a/extension/parquet/parquet_reader.cpp b/extension/parquet/parquet_reader.cpp\nindex ebeeaf1756fa..39f76984963e 100644\n--- a/extension/parquet/parquet_reader.cpp\n+++ b/extension/parquet/parquet_reader.cpp\n@@ -91,7 +91,6 @@ static shared_ptr<ParquetFileMetadataCache> LoadMetadata(Allocator &allocator, F\n \n LogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele, bool binary_as_string) {\n \t// inner node\n-\tD_ASSERT(s_ele.__isset.type && s_ele.num_children == 0);\n \tif (s_ele.type == Type::FIXED_LEN_BYTE_ARRAY && !s_ele.__isset.type_length) {\n \t\tthrow IOException(\"FIXED_LEN_BYTE_ARRAY requires length to be set\");\n \t}\n@@ -271,11 +270,7 @@ unique_ptr<ColumnReader> ParquetReader::CreateReaderRecursive(idx_t depth, idx_t\n \tif (repetition_type == FieldRepetitionType::REPEATED) {\n \t\tmax_repeat++;\n \t}\n-\n-\tif (!s_ele.__isset.type) { // inner node\n-\t\tif (s_ele.num_children == 0) {\n-\t\t\tthrow InvalidInputException(\"Node has no children but should\");\n-\t\t}\n+\tif (s_ele.__isset.num_children && s_ele.num_children > 0) { // inner node\n \t\tchild_list_t<LogicalType> child_types;\n \t\tvector<duckdb::unique_ptr<ColumnReader>> child_readers;\n \n@@ -339,6 +334,10 @@ unique_ptr<ColumnReader> ParquetReader::CreateReaderRecursive(idx_t depth, idx_t\n \t\t}\n \t\treturn result;\n \t} else { // leaf node\n+\t\tif (!s_ele.__isset.type) {\n+\t\t\tthrow InvalidInputException(\n+\t\t\t    \"Node has neither num_children nor type set - this violates the Parquet spec (corrupted file)\");\n+\t\t}\n \t\tif (s_ele.repetition_type == FieldRepetitionType::REPEATED) {\n \t\t\tconst auto derived_type = DeriveLogicalType(s_ele);\n \t\t\tauto list_type = LogicalType::LIST(derived_type);\n@@ -368,10 +367,13 @@ unique_ptr<ColumnReader> ParquetReader::CreateReader() {\n \t\tthrow IOException(\"Parquet reader: root schema element has no children\");\n \t}\n \tauto ret = CreateReaderRecursive(0, 0, 0, next_schema_idx, next_file_idx);\n+\tif (ret->Type().id() != LogicalTypeId::STRUCT) {\n+\t\tthrow InvalidInputException(\"Root element of Parquet file must be a struct\");\n+\t}\n \tD_ASSERT(next_schema_idx == file_meta_data->schema.size() - 1);\n \tD_ASSERT(file_meta_data->row_groups.empty() || next_file_idx == file_meta_data->row_groups[0].columns.size());\n \n-\tauto &root_struct_reader = (StructColumnReader &)*ret;\n+\tauto &root_struct_reader = ret->Cast<StructColumnReader>();\n \t// add casts if required\n \tfor (auto &entry : reader_data.cast_map) {\n \t\tauto column_idx = entry.first;\n",
  "test_patch": "diff --git a/data/parquet-testing/issue6990.parquet b/data/parquet-testing/issue6990.parquet\nnew file mode 100644\nindex 000000000000..f003aac6843c\nBinary files /dev/null and b/data/parquet-testing/issue6990.parquet differ\ndiff --git a/test/sql/copy/parquet/parquet_6990.test b/test/sql/copy/parquet/parquet_6990.test\nnew file mode 100644\nindex 000000000000..320594e765e2\n--- /dev/null\n+++ b/test/sql/copy/parquet/parquet_6990.test\n@@ -0,0 +1,11 @@\n+# name: test/sql/copy/parquet/parquet_6990.test\n+# description: Issue #6990: Reading parquet file causes a segfault\n+# group: [parquet]\n+\n+require parquet\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+SELECT * FROM 'data/parquet-testing/issue6990.parquet';\n",
  "problem_statement": "Reading parquet file causes a segfault\n### What happens?\n\nI installed duckdb 0.71 and tried to read a parquet file which lead to a segfault that I cannot troubleshoot/debug.\r\n\n\n### To Reproduce\n\nThe issue can be reproduced with the following steps. The parquet file is attached as well.\r\n\r\n```\r\n$ duckdb                                         \r\nv0.7.1 b00b93f0b1\r\nEnter \".help\" for usage hints.\r\nConnected to a transient in-memory database.\r\nUse \".open FILENAME\" to reopen on a persistent database.\r\nD select count(*) FROM read_parquet('out/data.parquet');\r\nzsh: segmentation fault  duckdb\r\n```\r\n\r\nParquet file: [data.parquet.zip](https://github.com/duckdb/duckdb/files/11170831/data.parquet.zip).\r\n\n\n### OS:\n\nOS X, Apple M1 Pro\n\n### DuckDB Version:\n\n0.7.1\n\n### DuckDB Client:\n\nCLI\n\n### Full Name:\n\nFilip Petkovski\n\n### Affiliation:\n\nShopify\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\nReading parquet file causes a segfault\n### What happens?\n\nI installed duckdb 0.71 and tried to read a parquet file which lead to a segfault that I cannot troubleshoot/debug.\r\n\n\n### To Reproduce\n\nThe issue can be reproduced with the following steps. The parquet file is attached as well.\r\n\r\n```\r\n$ duckdb                                         \r\nv0.7.1 b00b93f0b1\r\nEnter \".help\" for usage hints.\r\nConnected to a transient in-memory database.\r\nUse \".open FILENAME\" to reopen on a persistent database.\r\nD select count(*) FROM read_parquet('out/data.parquet');\r\nzsh: segmentation fault  duckdb\r\n```\r\n\r\nParquet file: [data.parquet.zip](https://github.com/duckdb/duckdb/files/11170831/data.parquet.zip).\r\n\n\n### OS:\n\nOS X, Apple M1 Pro\n\n### DuckDB Version:\n\n0.7.1\n\n### DuckDB Client:\n\nCLI\n\n### Full Name:\n\nFilip Petkovski\n\n### Affiliation:\n\nShopify\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "```c++\r\nLogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele, bool binary_as_string) {\r\n\t// inner node\r\n\tD_ASSERT(s_ele.__isset.type && s_ele.num_children == 0);\r\n```\r\nIt hits this assertion, children is `412`\r\n```\r\n(lldb) p s_ele.__isset.type\r\n(const bool) $0 = true\r\n```\r\nSounds like we expect it to not be nested, and it is?\r\n\r\nPandas is also not happy with this file\r\n```py\r\n>>> df = pd.read_parquet('tmp/data.parquet')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/pandas/io/parquet.py\", line 503, in read_parquet\r\n    return impl.read(\r\n           ^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/pandas/io/parquet.py\", line 251, in read\r\n    result = self.api.parquet.read_table(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/pyarrow/parquet/core.py\", line 2973, in read_table\r\n    return dataset.read(columns=columns, use_threads=use_threads,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/pyarrow/parquet/core.py\", line 2601, in read\r\n    table = self._dataset.to_table(\r\n            ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"pyarrow/_dataset.pyx\", line 369, in pyarrow._dataset.Dataset.to_table\r\n  File \"pyarrow/_dataset.pyx\", line 2818, in pyarrow._dataset.Scanner.to_table\r\n  File \"pyarrow/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 115, in pyarrow.lib.check_status\r\nOSError: Not yet implemented: DecodeArrow for DeltaLengthByteArrayDecoder.\r\n```\r\n\r\nSeems relevant:\r\nhttps://parquet.apache.org/docs/file-format/data-pages/encodings/#delta-length-byte-array-delta_length_byte_array--6\r\n\r\nThough this PR suggests we do support delta-length byte array encoding\r\n<https://github.com/duckdb/duckdb/pull/5457>\nYea I'm convinced it's just corrupted\r\n`type` is set in `__isset`, which it shouldn't be if it's nested\r\n\r\nThis other piece of code confirms that invariant\r\n```c++\r\n\tif (!s_ele.__isset.type) { // inner node\r\n\t\tif (s_ele.num_children == 0) {\r\n\t\t\tthrow InvalidInputException(\"Node has no children but should\");\r\n\t\t}\r\n```\r\n\r\nChanging that condition to `!s_ele.__isset.type || s_ele.num_children != 0`\r\nmakes the query complete successfully\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 count_star() \u2502\r\n\u2502    int64     \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502            1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nBut that shouldn't be necessary, again, the file looks corrupt if type is set and it also has children\nThanks for troubleshooting this. Should DuckDB show an error message instead of exiting with a segfault?  I was able to read the file with the library used to create it and I would not have been able to figure out why DuckDB could not.\nI get that it's not very helpful, D_ASSERT is not checked on release (that's why it didn't get caught and resulted in a segfault later on), we might turn it into an if statement that throws an InternalException instead.\r\n\r\n@Mytherin you're more familiar with Parquet, is this a corruption or should we just also check if `num_children > 0` to recognize a nested type?\n```c++\r\nLogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele, bool binary_as_string) {\r\n\t// inner node\r\n\tD_ASSERT(s_ele.__isset.type && s_ele.num_children == 0);\r\n```\r\nIt hits this assertion, children is `412`\r\n```\r\n(lldb) p s_ele.__isset.type\r\n(const bool) $0 = true\r\n```\r\nSounds like we expect it to not be nested, and it is?\r\n\r\nPandas is also not happy with this file\r\n```py\r\n>>> df = pd.read_parquet('tmp/data.parquet')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/pandas/io/parquet.py\", line 503, in read_parquet\r\n    return impl.read(\r\n           ^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/pandas/io/parquet.py\", line 251, in read\r\n    result = self.api.parquet.read_table(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/pyarrow/parquet/core.py\", line 2973, in read_table\r\n    return dataset.read(columns=columns, use_threads=use_threads,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/pyarrow/parquet/core.py\", line 2601, in read\r\n    table = self._dataset.to_table(\r\n            ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"pyarrow/_dataset.pyx\", line 369, in pyarrow._dataset.Dataset.to_table\r\n  File \"pyarrow/_dataset.pyx\", line 2818, in pyarrow._dataset.Scanner.to_table\r\n  File \"pyarrow/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 115, in pyarrow.lib.check_status\r\nOSError: Not yet implemented: DecodeArrow for DeltaLengthByteArrayDecoder.\r\n```\r\n\r\nSeems relevant:\r\nhttps://parquet.apache.org/docs/file-format/data-pages/encodings/#delta-length-byte-array-delta_length_byte_array--6\r\n\r\nThough this PR suggests we do support delta-length byte array encoding\r\n<https://github.com/duckdb/duckdb/pull/5457>\nYea I'm convinced it's just corrupted\r\n`type` is set in `__isset`, which it shouldn't be if it's nested\r\n\r\nThis other piece of code confirms that invariant\r\n```c++\r\n\tif (!s_ele.__isset.type) { // inner node\r\n\t\tif (s_ele.num_children == 0) {\r\n\t\t\tthrow InvalidInputException(\"Node has no children but should\");\r\n\t\t}\r\n```\r\n\r\nChanging that condition to `!s_ele.__isset.type || s_ele.num_children != 0`\r\nmakes the query complete successfully\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 count_star() \u2502\r\n\u2502    int64     \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502            1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nBut that shouldn't be necessary, again, the file looks corrupt if type is set and it also has children\nThanks for troubleshooting this. Should DuckDB show an error message instead of exiting with a segfault?  I was able to read the file with the library used to create it and I would not have been able to figure out why DuckDB could not.\nI get that it's not very helpful, D_ASSERT is not checked on release (that's why it didn't get caught and resulted in a segfault later on), we might turn it into an if statement that throws an InternalException instead.\r\n\r\n@Mytherin you're more familiar with Parquet, is this a corruption or should we just also check if `num_children > 0` to recognize a nested type?",
  "created_at": "2023-04-06T15:49:30Z"
}