{
  "repo": "duckdb/duckdb",
  "pull_number": 14914,
  "instance_id": "duckdb__duckdb-14914",
  "issue_numbers": [
    "14600",
    "14563",
    "14626",
    "14600",
    "14542"
  ],
  "base_commit": "9b9bde57ca3f192c2fcb290377f3b0022e6cdb09",
  "patch": "diff --git a/extension/core_functions/aggregate/holistic/reservoir_quantile.cpp b/extension/core_functions/aggregate/holistic/reservoir_quantile.cpp\nindex e99912762d43..8c332500d6e5 100644\n--- a/extension/core_functions/aggregate/holistic/reservoir_quantile.cpp\n+++ b/extension/core_functions/aggregate/holistic/reservoir_quantile.cpp\n@@ -39,7 +39,7 @@ struct ReservoirQuantileState {\n \tvoid FillReservoir(idx_t sample_size, T element) {\n \t\tif (pos < sample_size) {\n \t\t\tv[pos++] = element;\n-\t\t\tr_samp->InitializeReservoir(pos, len);\n+\t\t\tr_samp->InitializeReservoirWeights(pos, len);\n \t\t} else {\n \t\t\tD_ASSERT(r_samp->next_index_to_sample >= r_samp->num_entries_to_skip_b4_next_sample);\n \t\t\tif (r_samp->next_index_to_sample == r_samp->num_entries_to_skip_b4_next_sample) {\ndiff --git a/src/catalog/catalog_entry/duck_table_entry.cpp b/src/catalog/catalog_entry/duck_table_entry.cpp\nindex e1d0c66a2ded..6b7f72c5bae6 100644\n--- a/src/catalog/catalog_entry/duck_table_entry.cpp\n+++ b/src/catalog/catalog_entry/duck_table_entry.cpp\n@@ -136,6 +136,10 @@ unique_ptr<BaseStatistics> DuckTableEntry::GetStatistics(ClientContext &context,\n \treturn storage->GetStatistics(context, column.StorageOid());\n }\n \n+unique_ptr<BlockingSample> DuckTableEntry::GetSample() {\n+\treturn storage->GetSample();\n+}\n+\n unique_ptr<CatalogEntry> DuckTableEntry::AlterEntry(CatalogTransaction transaction, AlterInfo &info) {\n \tif (transaction.HasContext()) {\n \t\treturn AlterEntry(transaction.GetContext(), info);\ndiff --git a/src/catalog/catalog_entry/table_catalog_entry.cpp b/src/catalog/catalog_entry/table_catalog_entry.cpp\nindex 9fc439e8d5d5..4be35415dc95 100644\n--- a/src/catalog/catalog_entry/table_catalog_entry.cpp\n+++ b/src/catalog/catalog_entry/table_catalog_entry.cpp\n@@ -43,6 +43,10 @@ LogicalIndex TableCatalogEntry::GetColumnIndex(string &column_name, bool if_exis\n \treturn entry;\n }\n \n+unique_ptr<BlockingSample> TableCatalogEntry::GetSample() {\n+\treturn nullptr;\n+}\n+\n bool TableCatalogEntry::ColumnExists(const string &name) const {\n \treturn columns.ColumnExists(name);\n }\ndiff --git a/src/common/enum_util.cpp b/src/common/enum_util.cpp\nindex e803649e7339..e118848f2ae9 100644\n--- a/src/common/enum_util.cpp\n+++ b/src/common/enum_util.cpp\n@@ -3124,6 +3124,24 @@ SampleType EnumUtil::FromString<SampleType>(const char *value) {\n \treturn static_cast<SampleType>(StringUtil::StringToEnum(GetSampleTypeValues(), 3, \"SampleType\", value));\n }\n \n+const StringUtil::EnumStringLiteral *GetSamplingStateValues() {\n+\tstatic constexpr StringUtil::EnumStringLiteral values[] {\n+\t\t{ static_cast<uint32_t>(SamplingState::RANDOM), \"RANDOM\" },\n+\t\t{ static_cast<uint32_t>(SamplingState::RESERVOIR), \"RESERVOIR\" }\n+\t};\n+\treturn values;\n+}\n+\n+template<>\n+const char* EnumUtil::ToChars<SamplingState>(SamplingState value) {\n+\treturn StringUtil::EnumToString(GetSamplingStateValues(), 2, \"SamplingState\", static_cast<uint32_t>(value));\n+}\n+\n+template<>\n+SamplingState EnumUtil::FromString<SamplingState>(const char *value) {\n+\treturn static_cast<SamplingState>(StringUtil::StringToEnum(GetSamplingStateValues(), 2, \"SamplingState\", value));\n+}\n+\n const StringUtil::EnumStringLiteral *GetScanTypeValues() {\n \tstatic constexpr StringUtil::EnumStringLiteral values[] {\n \t\t{ static_cast<uint32_t>(ScanType::TABLE), \"TABLE\" },\ndiff --git a/src/common/random_engine.cpp b/src/common/random_engine.cpp\nindex 704992f0dc9c..ebc0abd43e32 100644\n--- a/src/common/random_engine.cpp\n+++ b/src/common/random_engine.cpp\n@@ -55,6 +55,10 @@ uint32_t RandomEngine::NextRandomInteger(uint32_t min, uint32_t max) {\n \treturn min + static_cast<uint32_t>(NextRandom() * double(max - min));\n }\n \n+uint32_t RandomEngine::NextRandomInteger32(uint32_t min, uint32_t max) {\n+\treturn min + static_cast<uint32_t>(NextRandom32() * double(max - min));\n+}\n+\n void RandomEngine::SetSeed(uint32_t seed) {\n \trandom_state->pcg.seed(seed);\n }\ndiff --git a/src/execution/CMakeLists.txt b/src/execution/CMakeLists.txt\nindex ef0d0a3eb50e..87c2a3bcc16e 100644\n--- a/src/execution/CMakeLists.txt\n+++ b/src/execution/CMakeLists.txt\n@@ -3,6 +3,7 @@ add_subdirectory(nested_loop_join)\n add_subdirectory(operator)\n add_subdirectory(physical_plan)\n add_subdirectory(index)\n+add_subdirectory(sample)\n \n add_library_unity(\n   duckdb_execution\n@@ -17,8 +18,7 @@ add_library_unity(\n   perfect_aggregate_hashtable.cpp\n   physical_operator.cpp\n   physical_plan_generator.cpp\n-  radix_partitioned_hashtable.cpp\n-  reservoir_sample.cpp)\n+  radix_partitioned_hashtable.cpp)\n set(ALL_OBJECT_FILES\n     ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:duckdb_execution>\n     PARENT_SCOPE)\ndiff --git a/src/execution/reservoir_sample.cpp b/src/execution/reservoir_sample.cpp\ndeleted file mode 100644\nindex 284e03faef09..000000000000\n--- a/src/execution/reservoir_sample.cpp\n+++ /dev/null\n@@ -1,324 +0,0 @@\n-#include \"duckdb/execution/reservoir_sample.hpp\"\n-#include \"duckdb/common/types/data_chunk.hpp\"\n-#include \"duckdb/common/pair.hpp\"\n-\n-namespace duckdb {\n-\n-void ReservoirChunk::Serialize(Serializer &serializer) const {\n-\tchunk.Serialize(serializer);\n-}\n-\n-unique_ptr<ReservoirChunk> ReservoirChunk::Deserialize(Deserializer &deserializer) {\n-\tauto result = make_uniq<ReservoirChunk>();\n-\tresult->chunk.Deserialize(deserializer);\n-\treturn result;\n-}\n-\n-ReservoirSample::ReservoirSample(Allocator &allocator, idx_t sample_count, int64_t seed)\n-    : BlockingSample(seed), allocator(allocator), sample_count(sample_count), reservoir_initialized(false) {\n-}\n-\n-ReservoirSample::ReservoirSample(idx_t sample_count, int64_t seed)\n-    : ReservoirSample(Allocator::DefaultAllocator(), sample_count, seed) {\n-}\n-\n-void ReservoirSample::AddToReservoir(DataChunk &input) {\n-\tif (sample_count == 0) {\n-\t\t// sample count is 0, means no samples were requested\n-\t\treturn;\n-\t}\n-\told_base_reservoir_sample.num_entries_seen_total += input.size();\n-\t// Input: A population V of n weighted items\n-\t// Output: A reservoir R with a size m\n-\t// 1: The first m items of V are inserted into R\n-\t// first we need to check if the reservoir already has \"m\" elements\n-\tif (!reservoir_data_chunk || reservoir_data_chunk->size() < sample_count) {\n-\t\tif (FillReservoir(input) == 0) {\n-\t\t\t// entire chunk was consumed by reservoir\n-\t\t\treturn;\n-\t\t}\n-\t}\n-\tD_ASSERT(reservoir_data_chunk);\n-\tD_ASSERT(reservoir_data_chunk->size() == sample_count);\n-\t// Initialize the weights if they have not been already\n-\tif (old_base_reservoir_sample.reservoir_weights.empty()) {\n-\t\told_base_reservoir_sample.InitializeReservoir(reservoir_data_chunk->size(), sample_count);\n-\t}\n-\t// find the position of next_index_to_sample relative to number of seen entries (num_entries_to_skip_b4_next_sample)\n-\tidx_t remaining = input.size();\n-\tidx_t base_offset = 0;\n-\twhile (true) {\n-\t\tidx_t offset = old_base_reservoir_sample.next_index_to_sample -\n-\t\t               old_base_reservoir_sample.num_entries_to_skip_b4_next_sample;\n-\t\tif (offset >= remaining) {\n-\t\t\t// not in this chunk! increment current count and go to the next chunk\n-\t\t\told_base_reservoir_sample.num_entries_to_skip_b4_next_sample += remaining;\n-\t\t\treturn;\n-\t\t}\n-\t\t// in this chunk! replace the element\n-\t\tReplaceElement(input, base_offset + offset);\n-\t\t// shift the chunk forward\n-\t\tremaining -= offset;\n-\t\tbase_offset += offset;\n-\t}\n-}\n-\n-unique_ptr<DataChunk> ReservoirSample::GetChunk() {\n-\tif (!reservoir_data_chunk || reservoir_data_chunk->size() == 0) {\n-\t\treturn nullptr;\n-\t}\n-\tauto collected_sample_count = reservoir_data_chunk->size();\n-\tif (collected_sample_count > STANDARD_VECTOR_SIZE) {\n-\t\t// get from the back to avoid creating two selection vectors\n-\t\t// one to return the first STANDARD_VECTOR_SIZE\n-\t\t// another to replace the reservoir_data_chunk with the first STANDARD VECTOR SIZE missing\n-\t\tauto ret = make_uniq<DataChunk>();\n-\t\tauto samples_remaining = collected_sample_count - STANDARD_VECTOR_SIZE;\n-\t\tauto reservoir_types = reservoir_data_chunk->GetTypes();\n-\t\tSelectionVector sel(STANDARD_VECTOR_SIZE);\n-\t\tfor (idx_t i = samples_remaining; i < collected_sample_count; i++) {\n-\t\t\tsel.set_index(i - samples_remaining, i);\n-\t\t}\n-\t\tret->Initialize(allocator, reservoir_types);\n-\t\tret->Slice(*reservoir_data_chunk, sel, STANDARD_VECTOR_SIZE);\n-\t\tret->SetCardinality(STANDARD_VECTOR_SIZE);\n-\t\t// reduce capacity and cardinality of the sample data chunk\n-\t\treservoir_data_chunk->SetCardinality(samples_remaining);\n-\t\treturn ret;\n-\t}\n-\treturn std::move(reservoir_data_chunk);\n-}\n-\n-void ReservoirSample::ReplaceElement(DataChunk &input, idx_t index_in_chunk, double with_weight) {\n-\t// replace the entry in the reservoir\n-\t// 8. The item in R with the minimum key is replaced by item vi\n-\tD_ASSERT(input.ColumnCount() == reservoir_data_chunk->ColumnCount());\n-\tfor (idx_t col_idx = 0; col_idx < input.ColumnCount(); col_idx++) {\n-\t\treservoir_data_chunk->SetValue(col_idx, old_base_reservoir_sample.min_weighted_entry_index,\n-\t\t                               input.GetValue(col_idx, index_in_chunk));\n-\t}\n-\told_base_reservoir_sample.ReplaceElement(with_weight);\n-}\n-\n-void ReservoirSample::InitializeReservoir(DataChunk &input) {\n-\treservoir_data_chunk = make_uniq<DataChunk>();\n-\treservoir_data_chunk->Initialize(allocator, input.GetTypes(), sample_count);\n-\tfor (idx_t col_idx = 0; col_idx < reservoir_data_chunk->ColumnCount(); col_idx++) {\n-\t\tFlatVector::Validity(reservoir_data_chunk->data[col_idx]).Initialize(sample_count);\n-\t}\n-\treservoir_initialized = true;\n-}\n-\n-idx_t ReservoirSample::FillReservoir(DataChunk &input) {\n-\tidx_t chunk_count = input.size();\n-\tinput.Flatten();\n-\tauto num_added_samples = reservoir_data_chunk ? reservoir_data_chunk->size() : 0;\n-\tD_ASSERT(num_added_samples <= sample_count);\n-\n-\t// required count is what we still need to add to the reservoir\n-\tidx_t required_count;\n-\tif (num_added_samples + chunk_count >= sample_count) {\n-\t\t// have to limit the count of the chunk\n-\t\trequired_count = sample_count - num_added_samples;\n-\t} else {\n-\t\t// we copy the entire chunk\n-\t\trequired_count = chunk_count;\n-\t}\n-\tinput.SetCardinality(required_count);\n-\n-\t// initialize the reservoir\n-\tif (!reservoir_initialized) {\n-\t\tInitializeReservoir(input);\n-\t}\n-\treservoir_data_chunk->Append(input, false, nullptr, required_count);\n-\told_base_reservoir_sample.InitializeReservoir(required_count, sample_count);\n-\n-\t// check if there are still elements remaining in the Input data chunk that should be\n-\t// randomly sampled and potentially added. This happens if we are on a boundary\n-\t// for example, input.size() is 1024, but our sample size is 10\n-\tif (required_count == chunk_count) {\n-\t\t// we are done here\n-\t\treturn 0;\n-\t}\n-\t// we still need to process a part of the chunk\n-\t// create a selection vector of the remaining elements\n-\tSelectionVector sel(STANDARD_VECTOR_SIZE);\n-\tfor (idx_t i = required_count; i < chunk_count; i++) {\n-\t\tsel.set_index(i - required_count, i);\n-\t}\n-\t// slice the input vector and continue\n-\tinput.Slice(sel, chunk_count - required_count);\n-\treturn input.size();\n-}\n-\n-void ReservoirSample::Finalize() {\n-\treturn;\n-}\n-\n-ReservoirSamplePercentage::ReservoirSamplePercentage(Allocator &allocator, double percentage, int64_t seed)\n-    : BlockingSample(seed), allocator(allocator), sample_percentage(percentage / 100.0), current_count(0),\n-      is_finalized(false) {\n-\treservoir_sample_size = idx_t(sample_percentage * RESERVOIR_THRESHOLD);\n-\tcurrent_sample = make_uniq<ReservoirSample>(allocator, reservoir_sample_size, random.NextRandomInteger());\n-}\n-\n-ReservoirSamplePercentage::ReservoirSamplePercentage(double percentage, int64_t seed)\n-    : ReservoirSamplePercentage(Allocator::DefaultAllocator(), percentage, seed) {\n-}\n-\n-void ReservoirSamplePercentage::AddToReservoir(DataChunk &input) {\n-\told_base_reservoir_sample.num_entries_seen_total += input.size();\n-\tif (current_count + input.size() > RESERVOIR_THRESHOLD) {\n-\t\t// we don't have enough space in our current reservoir\n-\t\t// first check what we still need to append to the current sample\n-\t\tidx_t append_to_current_sample_count = RESERVOIR_THRESHOLD - current_count;\n-\t\tidx_t append_to_next_sample = input.size() - append_to_current_sample_count;\n-\t\tif (append_to_current_sample_count > 0) {\n-\t\t\t// we have elements remaining, first add them to the current sample\n-\t\t\tif (append_to_next_sample > 0) {\n-\t\t\t\t// we need to also add to the next sample\n-\t\t\t\tDataChunk new_chunk;\n-\t\t\t\tnew_chunk.InitializeEmpty(input.GetTypes());\n-\t\t\t\tnew_chunk.Slice(input, *FlatVector::IncrementalSelectionVector(), append_to_current_sample_count);\n-\t\t\t\tnew_chunk.Flatten();\n-\t\t\t\tcurrent_sample->AddToReservoir(new_chunk);\n-\t\t\t} else {\n-\t\t\t\tinput.Flatten();\n-\t\t\t\tinput.SetCardinality(append_to_current_sample_count);\n-\t\t\t\tcurrent_sample->AddToReservoir(input);\n-\t\t\t}\n-\t\t}\n-\t\tif (append_to_next_sample > 0) {\n-\t\t\t// slice the input for the remainder\n-\t\t\tSelectionVector sel(append_to_next_sample);\n-\t\t\tfor (idx_t i = append_to_current_sample_count; i < append_to_next_sample + append_to_current_sample_count;\n-\t\t\t     i++) {\n-\t\t\t\tsel.set_index(i - append_to_current_sample_count, i);\n-\t\t\t}\n-\t\t\tinput.Slice(sel, append_to_next_sample);\n-\t\t}\n-\t\t// now our first sample is filled: append it to the set of finished samples\n-\t\tfinished_samples.push_back(std::move(current_sample));\n-\n-\t\t// allocate a new sample, and potentially add the remainder of the current input to that sample\n-\t\tcurrent_sample = make_uniq<ReservoirSample>(allocator, reservoir_sample_size, random.NextRandomInteger());\n-\t\tif (append_to_next_sample > 0) {\n-\t\t\tcurrent_sample->AddToReservoir(input);\n-\t\t}\n-\t\tcurrent_count = append_to_next_sample;\n-\t} else {\n-\t\t// we can just append to the current sample\n-\t\tcurrent_count += input.size();\n-\t\tcurrent_sample->AddToReservoir(input);\n-\t}\n-}\n-\n-unique_ptr<DataChunk> ReservoirSamplePercentage::GetChunk() {\n-\tif (!is_finalized) {\n-\t\tFinalize();\n-\t}\n-\twhile (!finished_samples.empty()) {\n-\t\tauto &front = finished_samples.front();\n-\t\tauto chunk = front->GetChunk();\n-\t\tif (chunk && chunk->size() > 0) {\n-\t\t\treturn chunk;\n-\t\t}\n-\t\t// move to the next sample\n-\t\tfinished_samples.erase(finished_samples.begin());\n-\t}\n-\treturn nullptr;\n-}\n-\n-void ReservoirSamplePercentage::Finalize() {\n-\t// need to finalize the current sample, if any\n-\t// we are finializing, so we are starting to return chunks. Our last chunk has\n-\t// sample_percentage * RESERVOIR_THRESHOLD entries that hold samples.\n-\t// if our current count is less than the sample_percentage * RESERVOIR_THRESHOLD\n-\t// then we have sampled too much for the current_sample and we need to redo the sample\n-\t// otherwise we can just push the current sample back\n-\t// Imagine sampling 70% of 100 rows (so 70 rows). We allocate sample_percentage * RESERVOIR_THRESHOLD\n-\t// -----------------------------------------\n-\tauto sampled_more_than_required =\n-\t    static_cast<double>(current_count) > sample_percentage * RESERVOIR_THRESHOLD || finished_samples.empty();\n-\tif (current_count > 0 && sampled_more_than_required) {\n-\t\t// create a new sample\n-\t\tauto new_sample_size = idx_t(round(sample_percentage * static_cast<double>(current_count)));\n-\t\tauto new_sample = make_uniq<ReservoirSample>(allocator, new_sample_size, random.NextRandomInteger());\n-\t\twhile (true) {\n-\t\t\tauto chunk = current_sample->GetChunk();\n-\t\t\tif (!chunk || chunk->size() == 0) {\n-\t\t\t\tbreak;\n-\t\t\t}\n-\t\t\tnew_sample->AddToReservoir(*chunk);\n-\t\t}\n-\t\tfinished_samples.push_back(std::move(new_sample));\n-\t} else {\n-\t\tfinished_samples.push_back(std::move(current_sample));\n-\t}\n-\t// when finalizing, current_sample is null. All samples are now in finished samples.\n-\tcurrent_sample = nullptr;\n-\tis_finalized = true;\n-}\n-\n-BaseReservoirSampling::BaseReservoirSampling(int64_t seed) : random(seed) {\n-\tnext_index_to_sample = 0;\n-\tmin_weight_threshold = 0;\n-\tmin_weighted_entry_index = 0;\n-\tnum_entries_to_skip_b4_next_sample = 0;\n-\tnum_entries_seen_total = 0;\n-}\n-\n-BaseReservoirSampling::BaseReservoirSampling() : BaseReservoirSampling(-1) {\n-}\n-\n-void BaseReservoirSampling::InitializeReservoir(idx_t cur_size, idx_t sample_size) {\n-\t//! 1: The first m items of V are inserted into R\n-\t//! first we need to check if the reservoir already has \"m\" elements\n-\tif (cur_size == sample_size) {\n-\t\t//! 2. For each item vi \u2208 R: Calculate a key ki = random(0, 1)\n-\t\t//! we then define the threshold to enter the reservoir T_w as the minimum key of R\n-\t\t//! we use a priority queue to extract the minimum key in O(1) time\n-\t\tfor (idx_t i = 0; i < sample_size; i++) {\n-\t\t\tdouble k_i = random.NextRandom();\n-\t\t\treservoir_weights.emplace(-k_i, i);\n-\t\t}\n-\t\tSetNextEntry();\n-\t}\n-}\n-\n-void BaseReservoirSampling::SetNextEntry() {\n-\t//! 4. Let r = random(0, 1) and Xw = log(r) / log(T_w)\n-\tauto &min_key = reservoir_weights.top();\n-\tdouble t_w = -min_key.first;\n-\tdouble r = random.NextRandom();\n-\tdouble x_w = log(r) / log(t_w);\n-\t//! 5. From the current item vc skip items until item vi , such that:\n-\t//! 6. wc +wc+1 +\u00b7\u00b7\u00b7+wi\u22121 < Xw <= wc +wc+1 +\u00b7\u00b7\u00b7+wi\u22121 +wi\n-\t//! since all our weights are 1 (uniform sampling), we can just determine the amount of elements to skip\n-\tmin_weight_threshold = t_w;\n-\tmin_weighted_entry_index = min_key.second;\n-\tnext_index_to_sample = MaxValue<idx_t>(1, idx_t(round(x_w)));\n-\tnum_entries_to_skip_b4_next_sample = 0;\n-}\n-\n-void BaseReservoirSampling::ReplaceElement(double with_weight) {\n-\t//! replace the entry in the reservoir\n-\t//! pop the minimum entry\n-\treservoir_weights.pop();\n-\t//! now update the reservoir\n-\t//! 8. Let tw = Tw i , r2 = random(tw,1) and vi\u2019s key: ki = (r2)1/wi\n-\t//! 9. The new threshold Tw is the new minimum key of R\n-\t//! we generate a random number between (min_weight_threshold, 1)\n-\tdouble r2 = random.NextRandom(min_weight_threshold, 1);\n-\n-\t//! if we are merging two reservoir samples use the weight passed\n-\tif (with_weight >= 0) {\n-\t\tr2 = with_weight;\n-\t}\n-\t//! now we insert the new weight into the reservoir\n-\treservoir_weights.emplace(-r2, min_weighted_entry_index);\n-\t//! we update the min entry with the new min entry in the reservoir\n-\tSetNextEntry();\n-}\n-\n-} // namespace duckdb\ndiff --git a/src/execution/sample/CMakeLists.txt b/src/execution/sample/CMakeLists.txt\nnew file mode 100644\nindex 000000000000..6f69a205a0a9\n--- /dev/null\n+++ b/src/execution/sample/CMakeLists.txt\n@@ -0,0 +1,5 @@\n+add_library_unity(duckdb_sample OBJECT base_reservoir_sample.cpp\n+                  reservoir_sample.cpp)\n+set(ALL_OBJECT_FILES\n+    ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:duckdb_sample>\n+    PARENT_SCOPE)\ndiff --git a/src/execution/sample/base_reservoir_sample.cpp b/src/execution/sample/base_reservoir_sample.cpp\nnew file mode 100644\nindex 000000000000..0f0fcdf7a387\n--- /dev/null\n+++ b/src/execution/sample/base_reservoir_sample.cpp\n@@ -0,0 +1,136 @@\n+#include \"duckdb/execution/reservoir_sample.hpp\"\n+#include <math.h>\n+\n+namespace duckdb {\n+\n+double BaseReservoirSampling::GetMinWeightFromTuplesSeen(idx_t rows_seen_total) {\n+\t// this function was obtained using https://mycurvefit.com. Inputting multiple x, y values into\n+\t// The\n+\tswitch (rows_seen_total) {\n+\tcase 0:\n+\t\treturn 0;\n+\tcase 1:\n+\t\treturn 0.000161;\n+\tcase 2:\n+\t\treturn 0.530136;\n+\tcase 3:\n+\t\treturn 0.693454;\n+\tdefault: {\n+\t\treturn (0.99 - 0.355 * std::exp(-0.07 * static_cast<double>(rows_seen_total)));\n+\t}\n+\t}\n+}\n+\n+BaseReservoirSampling::BaseReservoirSampling(int64_t seed) : random(seed) {\n+\tnext_index_to_sample = 0;\n+\tmin_weight_threshold = 0;\n+\tmin_weighted_entry_index = 0;\n+\tnum_entries_to_skip_b4_next_sample = 0;\n+\tnum_entries_seen_total = 0;\n+}\n+\n+BaseReservoirSampling::BaseReservoirSampling() : BaseReservoirSampling(1) {\n+}\n+\n+unique_ptr<BaseReservoirSampling> BaseReservoirSampling::Copy() {\n+\tauto ret = make_uniq<BaseReservoirSampling>(1);\n+\tret->reservoir_weights = reservoir_weights;\n+\tret->next_index_to_sample = next_index_to_sample;\n+\tret->min_weight_threshold = min_weight_threshold;\n+\tret->min_weighted_entry_index = min_weighted_entry_index;\n+\tret->num_entries_to_skip_b4_next_sample = num_entries_to_skip_b4_next_sample;\n+\tret->num_entries_seen_total = num_entries_seen_total;\n+\treturn ret;\n+}\n+\n+void BaseReservoirSampling::InitializeReservoirWeights(idx_t cur_size, idx_t sample_size) {\n+\t//! 1: The first m items of V are inserted into R\n+\t//! first we need to check if the reservoir already has \"m\" elements\n+\t//! 2. For each item vi \u2208 R: Calculate a key ki = random(0, 1)\n+\t//! we then define the threshold to enter the reservoir T_w as the minimum key of R\n+\t//! we use a priority queue to extract the minimum key in O(1) time\n+\tif (cur_size == sample_size) {\n+\t\t//! 2. For each item vi \u2208 R: Calculate a key ki = random(0, 1)\n+\t\t//! we then define the threshold to enter the reservoir T_w as the minimum key of R\n+\t\t//! we use a priority queue to extract the minimum key in O(1) time\n+\t\tfor (idx_t i = 0; i < sample_size; i++) {\n+\t\t\tdouble k_i = random.NextRandom();\n+\t\t\treservoir_weights.emplace(-k_i, i);\n+\t\t}\n+\t\tSetNextEntry();\n+\t}\n+}\n+\n+void BaseReservoirSampling::SetNextEntry() {\n+\tD_ASSERT(!reservoir_weights.empty());\n+\t//! 4. Let r = random(0, 1) and Xw = log(r) / log(T_w)\n+\tauto &min_key = reservoir_weights.top();\n+\tdouble t_w = -min_key.first;\n+\tdouble r = random.NextRandom32();\n+\tdouble x_w = log(r) / log(t_w);\n+\t//! 5. From the current item vc skip items until item vi , such that:\n+\t//! 6. wc +wc+1 +\u00b7\u00b7\u00b7+wi\u22121 < Xw <= wc +wc+1 +\u00b7\u00b7\u00b7+wi\u22121 +wi\n+\t//! since all our weights are 1 (uniform sampling), we can just determine the amount of elements to skip\n+\tmin_weight_threshold = t_w;\n+\tmin_weighted_entry_index = min_key.second;\n+\tnext_index_to_sample = MaxValue<idx_t>(1, idx_t(round(x_w)));\n+\tnum_entries_to_skip_b4_next_sample = 0;\n+}\n+\n+void BaseReservoirSampling::ReplaceElementWithIndex(idx_t entry_index, double with_weight, bool pop) {\n+\n+\tif (pop) {\n+\t\treservoir_weights.pop();\n+\t}\n+\tdouble r2 = with_weight;\n+\t//! now we insert the new weight into the reservoir\n+\treservoir_weights.emplace(-r2, entry_index);\n+\t//! we update the min entry with the new min entry in the reservoir\n+\tSetNextEntry();\n+}\n+\n+void BaseReservoirSampling::ReplaceElement(double with_weight) {\n+\t//! replace the entry in the reservoir\n+\t//! pop the minimum entry\n+\treservoir_weights.pop();\n+\t//! now update the reservoir\n+\t//! 8. Let tw = Tw i , r2 = random(tw,1) and vi\u2019s key: ki = (r2)1/wi\n+\t//! 9. The new threshold Tw is the new minimum key of R\n+\t//! we generate a random number between (min_weight_threshold, 1)\n+\tdouble r2 = random.NextRandom(min_weight_threshold, 1);\n+\n+\t//! if we are merging two reservoir samples use the weight passed\n+\tif (with_weight >= 0) {\n+\t\tr2 = with_weight;\n+\t}\n+\t//! now we insert the new weight into the reservoir\n+\treservoir_weights.emplace(-r2, min_weighted_entry_index);\n+\t//! we update the min entry with the new min entry in the reservoir\n+\tSetNextEntry();\n+}\n+\n+void BaseReservoirSampling::UpdateMinWeightThreshold() {\n+\tif (!reservoir_weights.empty()) {\n+\t\tmin_weight_threshold = -reservoir_weights.top().first;\n+\t\tmin_weighted_entry_index = reservoir_weights.top().second;\n+\t\treturn;\n+\t}\n+\tmin_weight_threshold = 1;\n+}\n+\n+void BaseReservoirSampling::FillWeights(SelectionVector &sel, idx_t &sel_size) {\n+\tif (!reservoir_weights.empty()) {\n+\t\treturn;\n+\t}\n+\tD_ASSERT(reservoir_weights.empty());\n+\tauto num_entries_seen_normalized = num_entries_seen_total / FIXED_SAMPLE_SIZE;\n+\tauto min_weight = GetMinWeightFromTuplesSeen(num_entries_seen_normalized);\n+\tfor (idx_t i = 0; i < sel_size; i++) {\n+\t\tauto weight = random.NextRandom(min_weight, 1);\n+\t\treservoir_weights.emplace(-weight, i);\n+\t}\n+\tD_ASSERT(reservoir_weights.size() <= sel_size);\n+\tSetNextEntry();\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/execution/sample/reservoir_sample.cpp b/src/execution/sample/reservoir_sample.cpp\nnew file mode 100644\nindex 000000000000..eef10fca128b\n--- /dev/null\n+++ b/src/execution/sample/reservoir_sample.cpp\n@@ -0,0 +1,930 @@\n+#include \"duckdb/execution/reservoir_sample.hpp\"\n+#include \"duckdb/common/types/data_chunk.hpp\"\n+#include \"duckdb/common/vector_operations/vector_operations.hpp\"\n+#include <unordered_set>\n+\n+namespace duckdb {\n+\n+std::pair<double, idx_t> BlockingSample::PopFromWeightQueue() {\n+\tD_ASSERT(base_reservoir_sample && !base_reservoir_sample->reservoir_weights.empty());\n+\tauto ret = base_reservoir_sample->reservoir_weights.top();\n+\tbase_reservoir_sample->reservoir_weights.pop();\n+\n+\tbase_reservoir_sample->UpdateMinWeightThreshold();\n+\tD_ASSERT(base_reservoir_sample->min_weight_threshold > 0);\n+\treturn ret;\n+}\n+\n+double BlockingSample::GetMinWeightThreshold() {\n+\treturn base_reservoir_sample->min_weight_threshold;\n+}\n+\n+idx_t BlockingSample::GetPriorityQueueSize() {\n+\treturn base_reservoir_sample->reservoir_weights.size();\n+}\n+\n+void BlockingSample::Destroy() {\n+\tdestroyed = true;\n+}\n+\n+void ReservoirChunk::Serialize(Serializer &serializer) const {\n+\tchunk.Serialize(serializer);\n+}\n+\n+unique_ptr<ReservoirChunk> ReservoirChunk::Deserialize(Deserializer &deserializer) {\n+\tauto result = make_uniq<ReservoirChunk>();\n+\tresult->chunk.Deserialize(deserializer);\n+\treturn result;\n+}\n+\n+unique_ptr<ReservoirChunk> ReservoirChunk::Copy() const {\n+\tauto copy = make_uniq<ReservoirChunk>();\n+\tcopy->chunk.Initialize(Allocator::DefaultAllocator(), chunk.GetTypes());\n+\n+\tchunk.Copy(copy->chunk);\n+\treturn copy;\n+}\n+\n+ReservoirSample::ReservoirSample(idx_t sample_count, unique_ptr<ReservoirChunk> reservoir_chunk)\n+    : ReservoirSample(Allocator::DefaultAllocator(), sample_count, 1) {\n+\tif (reservoir_chunk) {\n+\t\tthis->reservoir_chunk = std::move(reservoir_chunk);\n+\t\tsel_size = this->reservoir_chunk->chunk.size();\n+\t\tsel = SelectionVector(0, sel_size);\n+\t\tExpandSerializedSample();\n+\t}\n+\tstats_sample = true;\n+}\n+\n+ReservoirSample::ReservoirSample(Allocator &allocator, idx_t sample_count, int64_t seed)\n+    : BlockingSample(seed), sample_count(sample_count), allocator(allocator) {\n+\tbase_reservoir_sample = make_uniq<BaseReservoirSampling>(seed);\n+\ttype = SampleType::RESERVOIR_SAMPLE;\n+\treservoir_chunk = nullptr;\n+\tstats_sample = false;\n+\tsel = SelectionVector(sample_count);\n+\tsel_size = 0;\n+}\n+\n+idx_t ReservoirSample::GetSampleCount() {\n+\treturn sample_count;\n+}\n+\n+idx_t ReservoirSample::NumSamplesCollected() const {\n+\tif (!reservoir_chunk) {\n+\t\treturn 0;\n+\t}\n+\treturn reservoir_chunk->chunk.size();\n+}\n+\n+SamplingState ReservoirSample::GetSamplingState() const {\n+\tif (base_reservoir_sample->reservoir_weights.empty()) {\n+\t\treturn SamplingState::RANDOM;\n+\t}\n+\treturn SamplingState::RESERVOIR;\n+}\n+\n+idx_t ReservoirSample::GetActiveSampleCount() const {\n+\tswitch (GetSamplingState()) {\n+\tcase SamplingState::RANDOM:\n+\t\treturn sel_size;\n+\tcase SamplingState::RESERVOIR:\n+\t\treturn base_reservoir_sample->reservoir_weights.size();\n+\tdefault:\n+\t\tthrow InternalException(\"Sampling State is INVALID\");\n+\t}\n+}\n+\n+idx_t ReservoirSample::GetTuplesSeen() const {\n+\treturn base_reservoir_sample->num_entries_seen_total;\n+}\n+\n+DataChunk &ReservoirSample::Chunk() {\n+\tD_ASSERT(reservoir_chunk);\n+\treturn reservoir_chunk->chunk;\n+}\n+\n+unique_ptr<DataChunk> ReservoirSample::GetChunk() {\n+\tif (destroyed || !reservoir_chunk || Chunk().size() == 0) {\n+\t\treturn nullptr;\n+\t}\n+\t// cannot destory internal samples.\n+\tauto ret = make_uniq<DataChunk>();\n+\n+\tSelectionVector ret_sel(STANDARD_VECTOR_SIZE);\n+\tidx_t collected_samples = GetActiveSampleCount();\n+\n+\tif (collected_samples == 0) {\n+\t\treturn nullptr;\n+\t}\n+\n+\tidx_t samples_remaining;\n+\tidx_t return_chunk_size;\n+\tif (collected_samples > STANDARD_VECTOR_SIZE) {\n+\t\tsamples_remaining = collected_samples - STANDARD_VECTOR_SIZE;\n+\t\treturn_chunk_size = STANDARD_VECTOR_SIZE;\n+\t} else {\n+\t\tsamples_remaining = 0;\n+\t\treturn_chunk_size = collected_samples;\n+\t}\n+\n+\tfor (idx_t i = samples_remaining; i < collected_samples; i++) {\n+\t\t// pop samples and reduce size of selection vector.\n+\t\tif (GetSamplingState() == SamplingState::RESERVOIR) {\n+\t\t\tauto top = PopFromWeightQueue();\n+\t\t\tret_sel.set_index(i - samples_remaining, sel.get_index(top.second));\n+\t\t} else {\n+\t\t\tret_sel.set_index(i - samples_remaining, sel.get_index(i));\n+\t\t}\n+\t\tsel_size -= 1;\n+\t}\n+\n+\tauto reservoir_types = Chunk().GetTypes();\n+\n+\tret->Initialize(allocator, reservoir_types, STANDARD_VECTOR_SIZE);\n+\tret->Slice(Chunk(), ret_sel, return_chunk_size);\n+\tret->SetCardinality(return_chunk_size);\n+\treturn ret;\n+}\n+\n+unique_ptr<ReservoirChunk> ReservoirSample::CreateNewSampleChunk(vector<LogicalType> &types, idx_t size) const {\n+\tauto new_sample_chunk = make_uniq<ReservoirChunk>();\n+\tnew_sample_chunk->chunk.Initialize(Allocator::DefaultAllocator(), types, size);\n+\n+\t// set the NULL columns correctly\n+\tfor (idx_t col_idx = 0; col_idx < types.size(); col_idx++) {\n+\t\tif (!ValidSampleType(types[col_idx]) && stats_sample) {\n+\t\t\tnew_sample_chunk->chunk.data[col_idx].SetVectorType(VectorType::CONSTANT_VECTOR);\n+\t\t\tConstantVector::SetNull(new_sample_chunk->chunk.data[col_idx], true);\n+\t\t}\n+\t}\n+\treturn new_sample_chunk;\n+}\n+\n+void ReservoirSample::Vacuum() {\n+\tVerify();\n+\tif (NumSamplesCollected() <= FIXED_SAMPLE_SIZE || !reservoir_chunk || destroyed) {\n+\t\t// sample is destroyed or too small to shrink\n+\t\treturn;\n+\t}\n+\n+\tauto ret = Copy();\n+\tauto ret_reservoir = duckdb::unique_ptr_cast<BlockingSample, ReservoirSample>(std::move(ret));\n+\treservoir_chunk = std::move(ret_reservoir->reservoir_chunk);\n+\tsel = std::move(ret_reservoir->sel);\n+\tsel_size = ret_reservoir->sel_size;\n+\n+\tVerify();\n+\t// We should only have one sample chunk now.\n+\tD_ASSERT(Chunk().size() > 0 && Chunk().size() <= sample_count);\n+}\n+\n+unique_ptr<BlockingSample> ReservoirSample::Copy() const {\n+\n+\tauto ret = make_uniq<ReservoirSample>(sample_count);\n+\tret->stats_sample = stats_sample;\n+\n+\tret->base_reservoir_sample = base_reservoir_sample->Copy();\n+\tret->destroyed = destroyed;\n+\n+\tif (!reservoir_chunk || destroyed) {\n+\t\treturn unique_ptr_cast<ReservoirSample, BlockingSample>(std::move(ret));\n+\t}\n+\n+\tD_ASSERT(reservoir_chunk);\n+\n+\t// create a new sample chunk to store new samples\n+\tauto types = reservoir_chunk->chunk.GetTypes();\n+\t// how many values should be copied\n+\tidx_t values_to_copy = MinValue<idx_t>(GetActiveSampleCount(), sample_count);\n+\n+\tauto new_sample_chunk = CreateNewSampleChunk(types, GetReservoirChunkCapacity());\n+\n+\tSelectionVector sel_copy(sel);\n+\n+\tret->reservoir_chunk = std::move(new_sample_chunk);\n+\tret->UpdateSampleAppend(ret->reservoir_chunk->chunk, reservoir_chunk->chunk, sel_copy, values_to_copy);\n+\tret->sel = SelectionVector(values_to_copy);\n+\tfor (idx_t i = 0; i < values_to_copy; i++) {\n+\t\tret->sel.set_index(i, i);\n+\t}\n+\tret->sel_size = sel_size;\n+\tD_ASSERT(ret->reservoir_chunk->chunk.size() <= sample_count);\n+\tret->Verify();\n+\treturn unique_ptr_cast<ReservoirSample, BlockingSample>(std::move(ret));\n+}\n+\n+void ReservoirSample::ConvertToReservoirSample() {\n+\tD_ASSERT(sel_size <= sample_count);\n+\tbase_reservoir_sample->FillWeights(sel, sel_size);\n+}\n+\n+vector<uint32_t> ReservoirSample::GetRandomizedVector(uint32_t range, uint32_t size) const {\n+\tvector<uint32_t> ret;\n+\tret.reserve(range);\n+\tfor (uint32_t i = 0; i < range; i++) {\n+\t\tret.push_back(i);\n+\t}\n+\tif (size == FIXED_SAMPLE_SIZE) {\n+\t\tstd::shuffle(ret.begin(), ret.end(), base_reservoir_sample->random);\n+\t\treturn ret;\n+\t}\n+\tfor (uint32_t i = 0; i < size; i++) {\n+\t\tuint32_t random_shuffle = base_reservoir_sample->random.NextRandomInteger32(i, range);\n+\t\tif (random_shuffle == i) {\n+\t\t\t// leave the value where it is\n+\t\t\tcontinue;\n+\t\t}\n+\t\tuint32_t tmp = ret[random_shuffle];\n+\t\t// basically replacing the tuple that was at index actual_sample_indexes[random_shuffle]\n+\t\tret[random_shuffle] = ret[i];\n+\t\tret[i] = tmp;\n+\t}\n+\treturn ret;\n+}\n+\n+void ReservoirSample::SimpleMerge(ReservoirSample &other) {\n+\tD_ASSERT(GetPriorityQueueSize() == 0);\n+\tD_ASSERT(other.GetPriorityQueueSize() == 0);\n+\tD_ASSERT(GetSamplingState() == SamplingState::RANDOM);\n+\tD_ASSERT(other.GetSamplingState() == SamplingState::RANDOM);\n+\n+\tif (other.GetActiveSampleCount() == 0 && other.GetTuplesSeen() == 0) {\n+\t\treturn;\n+\t}\n+\n+\tif (GetActiveSampleCount() == 0 && GetTuplesSeen() == 0) {\n+\t\tsel = SelectionVector(other.sel);\n+\t\tsel_size = other.sel_size;\n+\t\tbase_reservoir_sample->num_entries_seen_total = other.GetTuplesSeen();\n+\t\treturn;\n+\t}\n+\n+\tidx_t total_seen = GetTuplesSeen() + other.GetTuplesSeen();\n+\n+\tauto weight_tuples_this = static_cast<double>(GetTuplesSeen()) / static_cast<double>(total_seen);\n+\tauto weight_tuples_other = static_cast<double>(other.GetTuplesSeen()) / static_cast<double>(total_seen);\n+\n+\t// If weights don't add up to 1, most likely a simple merge occured and no new samples were added.\n+\t// if that is the case, add the missing weight to the lower weighted sample to adjust.\n+\t// this is to avoid cases where if you have a 20k row table and add another 20k rows row by row\n+\t// then eventually the missing weights will add up, and get you a more even distribution\n+\tif (weight_tuples_this + weight_tuples_other < 1) {\n+\t\tweight_tuples_other += 1 - (weight_tuples_other + weight_tuples_this);\n+\t}\n+\n+\tidx_t keep_from_this = 0;\n+\tidx_t keep_from_other = 0;\n+\tD_ASSERT(stats_sample);\n+\tD_ASSERT(sample_count == FIXED_SAMPLE_SIZE);\n+\tD_ASSERT(sample_count == other.sample_count);\n+\tauto sample_count_double = static_cast<double>(sample_count);\n+\n+\tif (weight_tuples_this > weight_tuples_other) {\n+\t\tkeep_from_this = MinValue<idx_t>(static_cast<idx_t>(round(sample_count_double * weight_tuples_this)),\n+\t\t                                 GetActiveSampleCount());\n+\t\tkeep_from_other = MinValue<idx_t>(sample_count - keep_from_this, other.GetActiveSampleCount());\n+\t} else {\n+\t\tkeep_from_other = MinValue<idx_t>(static_cast<idx_t>(round(sample_count_double * weight_tuples_other)),\n+\t\t                                  other.GetActiveSampleCount());\n+\t\tkeep_from_this = MinValue<idx_t>(sample_count - keep_from_other, GetActiveSampleCount());\n+\t}\n+\n+\tD_ASSERT(keep_from_this <= GetActiveSampleCount());\n+\tD_ASSERT(keep_from_other <= other.GetActiveSampleCount());\n+\tD_ASSERT(keep_from_other + keep_from_this <= FIXED_SAMPLE_SIZE);\n+\tidx_t size_after_merge = MinValue<idx_t>(keep_from_other + keep_from_this, FIXED_SAMPLE_SIZE);\n+\n+\t// Check if appending the other samples to this will go over the sample chunk size\n+\tif (reservoir_chunk->chunk.size() + keep_from_other > GetReservoirChunkCapacity()) {\n+\t\tVacuum();\n+\t}\n+\n+\tD_ASSERT(size_after_merge <= other.GetActiveSampleCount() + GetActiveSampleCount());\n+\tSelectionVector chunk_sel(keep_from_other);\n+\tauto offset = reservoir_chunk->chunk.size();\n+\tfor (idx_t i = keep_from_this; i < size_after_merge; i++) {\n+\t\tif (i >= GetActiveSampleCount()) {\n+\t\t\tsel.set_index(GetActiveSampleCount(), offset);\n+\t\t\tsel_size += 1;\n+\t\t} else {\n+\t\t\tsel.set_index(i, offset);\n+\t\t}\n+\t\tchunk_sel.set_index(i - keep_from_this, other.sel.get_index(i - keep_from_this));\n+\t\toffset += 1;\n+\t}\n+\n+\tD_ASSERT(GetActiveSampleCount() == size_after_merge);\n+\n+\t// Copy the rows that make it to the sample from other and put them into this.\n+\tUpdateSampleAppend(reservoir_chunk->chunk, other.reservoir_chunk->chunk, chunk_sel, keep_from_other);\n+\tbase_reservoir_sample->num_entries_seen_total += other.GetTuplesSeen();\n+\n+\t// if THIS has too many samples now, we conver it to a slower sample.\n+\tif (GetTuplesSeen() >= FIXED_SAMPLE_SIZE * FAST_TO_SLOW_THRESHOLD) {\n+\t\tConvertToReservoirSample();\n+\t}\n+\tVerify();\n+}\n+\n+void ReservoirSample::WeightedMerge(ReservoirSample &other_sample) {\n+\tD_ASSERT(GetSamplingState() == SamplingState::RESERVOIR);\n+\tD_ASSERT(other_sample.GetSamplingState() == SamplingState::RESERVOIR);\n+\n+\t// Find out how many samples we want to keep.\n+\tidx_t total_samples = GetActiveSampleCount() + other_sample.GetActiveSampleCount();\n+\tidx_t total_samples_seen =\n+\t    base_reservoir_sample->num_entries_seen_total + other_sample.base_reservoir_sample->num_entries_seen_total;\n+\tidx_t num_samples_to_keep = MinValue<idx_t>(total_samples, MinValue<idx_t>(sample_count, total_samples_seen));\n+\n+\tD_ASSERT(GetActiveSampleCount() <= num_samples_to_keep);\n+\tD_ASSERT(total_samples <= FIXED_SAMPLE_SIZE * 2);\n+\n+\t// pop from base base_reservoir weights until there are num_samples_to_keep left.\n+\tvector<idx_t> this_indexes_to_replace;\n+\tfor (idx_t i = num_samples_to_keep; i < total_samples; i++) {\n+\t\tauto min_weight_this = base_reservoir_sample->min_weight_threshold;\n+\t\tauto min_weight_other = other_sample.base_reservoir_sample->min_weight_threshold;\n+\t\t// min weight threshol is always positive\n+\t\tif (min_weight_this > min_weight_other) {\n+\t\t\t// pop from other\n+\t\t\tother_sample.base_reservoir_sample->reservoir_weights.pop();\n+\t\t\tother_sample.base_reservoir_sample->UpdateMinWeightThreshold();\n+\t\t} else {\n+\t\t\tauto top_this = PopFromWeightQueue();\n+\t\t\tthis_indexes_to_replace.push_back(top_this.second);\n+\t\t\tbase_reservoir_sample->UpdateMinWeightThreshold();\n+\t\t}\n+\t}\n+\n+\tD_ASSERT(other_sample.GetPriorityQueueSize() + GetPriorityQueueSize() <= FIXED_SAMPLE_SIZE);\n+\tD_ASSERT(other_sample.GetPriorityQueueSize() + GetPriorityQueueSize() == num_samples_to_keep);\n+\tD_ASSERT(other_sample.reservoir_chunk->chunk.GetTypes() == reservoir_chunk->chunk.GetTypes());\n+\n+\t// Prepare a selection vector to copy data from the other sample chunk to this sample chunk\n+\tSelectionVector sel_other(other_sample.GetPriorityQueueSize());\n+\tD_ASSERT(GetPriorityQueueSize() <= num_samples_to_keep);\n+\tD_ASSERT(other_sample.GetPriorityQueueSize() >= this_indexes_to_replace.size());\n+\tidx_t chunk_offset = 0;\n+\n+\t// Now push weights from other.base_reservoir_sample to this\n+\t// Depending on how many sample values \"this\" has, we either need to add to the selection vector\n+\t// Or replace values in \"this'\" selection vector\n+\tidx_t i = 0;\n+\twhile (other_sample.GetPriorityQueueSize() > 0) {\n+\t\tauto other_top = other_sample.PopFromWeightQueue();\n+\t\tidx_t index_for_new_pair = chunk_offset + reservoir_chunk->chunk.size();\n+\n+\t\t// update the sel used to copy values from other to this\n+\t\tsel_other.set_index(chunk_offset, other_top.second);\n+\t\tif (i < this_indexes_to_replace.size()) {\n+\t\t\tauto replacement_index = this_indexes_to_replace[i];\n+\t\t\tsel.set_index(replacement_index, index_for_new_pair);\n+\t\t\tother_top.second = replacement_index;\n+\t\t} else {\n+\t\t\tsel.set_index(sel_size, index_for_new_pair);\n+\t\t\tother_top.second = sel_size;\n+\t\t\tsel_size += 1;\n+\t\t}\n+\n+\t\t// make sure that the sample indexes are (this.sample_chunk.size() + chunk_offfset)\n+\t\tbase_reservoir_sample->reservoir_weights.push(other_top);\n+\t\tchunk_offset += 1;\n+\t\ti += 1;\n+\t}\n+\n+\tD_ASSERT(GetPriorityQueueSize() == num_samples_to_keep);\n+\n+\tbase_reservoir_sample->UpdateMinWeightThreshold();\n+\tD_ASSERT(base_reservoir_sample->min_weight_threshold > 0);\n+\tbase_reservoir_sample->num_entries_seen_total = GetTuplesSeen() + other_sample.GetTuplesSeen();\n+\n+\tUpdateSampleAppend(reservoir_chunk->chunk, other_sample.reservoir_chunk->chunk, sel_other, chunk_offset);\n+\tif (reservoir_chunk->chunk.size() > FIXED_SAMPLE_SIZE * (FIXED_SAMPLE_SIZE_MULTIPLIER - 3)) {\n+\t\tVacuum();\n+\t}\n+\n+\tVerify();\n+}\n+\n+void ReservoirSample::Merge(unique_ptr<BlockingSample> other) {\n+\tif (destroyed || other->destroyed) {\n+\t\tDestroy();\n+\t\treturn;\n+\t}\n+\n+\tD_ASSERT(other->type == SampleType::RESERVOIR_SAMPLE);\n+\tauto &other_sample = other->Cast<ReservoirSample>();\n+\n+\t// if the other sample has not collected anything yet return\n+\tif (!other_sample.reservoir_chunk || other_sample.reservoir_chunk->chunk.size() == 0) {\n+\t\treturn;\n+\t}\n+\n+\t// this has not collected samples, take over the other\n+\tif (!reservoir_chunk || reservoir_chunk->chunk.size() == 0) {\n+\t\tbase_reservoir_sample = std::move(other->base_reservoir_sample);\n+\t\treservoir_chunk = std::move(other_sample.reservoir_chunk);\n+\t\tsel = SelectionVector(other_sample.sel);\n+\t\tsel_size = other_sample.sel_size;\n+\t\tVerify();\n+\t\treturn;\n+\t}\n+\t//! Both samples are still in \"fast sampling\" method\n+\tif (GetSamplingState() == SamplingState::RANDOM && other_sample.GetSamplingState() == SamplingState::RANDOM) {\n+\t\tSimpleMerge(other_sample);\n+\t\treturn;\n+\t}\n+\n+\t// One or none of the samples are in \"Fast Sampling\" method.\n+\t// When this is the case, switch both to slow sampling\n+\tConvertToReservoirSample();\n+\tother_sample.ConvertToReservoirSample();\n+\tWeightedMerge(other_sample);\n+}\n+\n+void ReservoirSample::ShuffleSel(SelectionVector &sel, idx_t range, idx_t size) const {\n+\tauto randomized = GetRandomizedVector(static_cast<uint32_t>(range), static_cast<uint32_t>(size));\n+\tSelectionVector original_sel(range);\n+\tfor (idx_t i = 0; i < range; i++) {\n+\t\toriginal_sel.set_index(i, sel.get_index(i));\n+\t}\n+\tfor (idx_t i = 0; i < size; i++) {\n+\t\tsel.set_index(i, original_sel.get_index(randomized[i]));\n+\t}\n+}\n+\n+void ReservoirSample::NormalizeWeights() {\n+\tvector<std::pair<double, idx_t>> tmp_weights;\n+\twhile (!base_reservoir_sample->reservoir_weights.empty()) {\n+\t\tauto top = base_reservoir_sample->reservoir_weights.top();\n+\t\ttmp_weights.push_back(std::move(top));\n+\t\tbase_reservoir_sample->reservoir_weights.pop();\n+\t}\n+\tstd::sort(tmp_weights.begin(), tmp_weights.end(),\n+\t          [&](std::pair<double, idx_t> a, std::pair<double, idx_t> b) { return a.second < b.second; });\n+\tfor (idx_t i = 0; i < tmp_weights.size(); i++) {\n+\t\tbase_reservoir_sample->reservoir_weights.emplace(tmp_weights.at(i).first, i);\n+\t}\n+\tbase_reservoir_sample->SetNextEntry();\n+}\n+\n+void ReservoirSample::EvictOverBudgetSamples() {\n+\tVerify();\n+\tif (!reservoir_chunk || destroyed) {\n+\t\treturn;\n+\t}\n+\n+\t// since this is for serialization, we really need to make sure keep a\n+\t// minimum of 1% of the rows or 2048 rows\n+\tidx_t num_samples_to_keep =\n+\t    MinValue<idx_t>(FIXED_SAMPLE_SIZE, static_cast<idx_t>(SAVE_PERCENTAGE * static_cast<double>(GetTuplesSeen())));\n+\n+\tif (num_samples_to_keep <= 0) {\n+\t\treservoir_chunk->chunk.SetCardinality(0);\n+\t\treturn;\n+\t}\n+\n+\tif (num_samples_to_keep == sample_count) {\n+\t\treturn;\n+\t}\n+\n+\t// if we over sampled, make sure we only keep the highest percentage samples\n+\tstd::unordered_set<idx_t> selections_to_delete;\n+\n+\twhile (num_samples_to_keep < GetPriorityQueueSize()) {\n+\t\tauto top = PopFromWeightQueue();\n+\t\tD_ASSERT(top.second < sel_size);\n+\t\tselections_to_delete.emplace(top.second);\n+\t}\n+\n+\t// set up reservoir chunk for the reservoir sample\n+\tD_ASSERT(reservoir_chunk->chunk.size() <= sample_count);\n+\t// create a new sample chunk to store new samples\n+\tauto types = reservoir_chunk->chunk.GetTypes();\n+\tD_ASSERT(num_samples_to_keep <= sample_count);\n+\tD_ASSERT(stats_sample);\n+\tD_ASSERT(sample_count == STANDARD_VECTOR_SIZE);\n+\tauto new_reservoir_chunk = CreateNewSampleChunk(types, STANDARD_VECTOR_SIZE);\n+\n+\t// The current selection vector can potentially have 2048 valid mappings.\n+\t// If we need to save a sample with less rows than that, we need to do the following\n+\t// 1. Create a new selection vector that doesn't point to the rows we are evicting\n+\tSelectionVector new_sel(num_samples_to_keep);\n+\tidx_t offset = 0;\n+\tfor (idx_t i = 0; i < num_samples_to_keep + selections_to_delete.size(); i++) {\n+\t\tif (selections_to_delete.find(i) == selections_to_delete.end()) {\n+\t\t\tD_ASSERT(i - offset < num_samples_to_keep);\n+\t\t\tnew_sel.set_index(i - offset, sel.get_index(i));\n+\t\t} else {\n+\t\t\toffset += 1;\n+\t\t}\n+\t}\n+\t// 2. Update row_ids in our weights so that they don't store rows ids to\n+\t//    indexes in the selection vector that have been evicted.\n+\tif (!selections_to_delete.empty()) {\n+\t\tNormalizeWeights();\n+\t}\n+\n+\tD_ASSERT(reservoir_chunk->chunk.GetTypes() == new_reservoir_chunk->chunk.GetTypes());\n+\n+\tUpdateSampleAppend(new_reservoir_chunk->chunk, reservoir_chunk->chunk, new_sel, num_samples_to_keep);\n+\t// set the cardinality\n+\tnew_reservoir_chunk->chunk.SetCardinality(num_samples_to_keep);\n+\treservoir_chunk = std::move(new_reservoir_chunk);\n+\tsel_size = num_samples_to_keep;\n+\tbase_reservoir_sample->UpdateMinWeightThreshold();\n+}\n+\n+void ReservoirSample::ExpandSerializedSample() {\n+\tif (!reservoir_chunk) {\n+\t\treturn;\n+\t}\n+\n+\tauto types = reservoir_chunk->chunk.GetTypes();\n+\tauto new_res_chunk = CreateNewSampleChunk(types, GetReservoirChunkCapacity());\n+\tauto copy_count = reservoir_chunk->chunk.size();\n+\tSelectionVector tmp_sel = SelectionVector(0, copy_count);\n+\tUpdateSampleAppend(new_res_chunk->chunk, reservoir_chunk->chunk, tmp_sel, copy_count);\n+\tnew_res_chunk->chunk.SetCardinality(copy_count);\n+\tstd::swap(reservoir_chunk, new_res_chunk);\n+}\n+\n+idx_t ReservoirSample::GetReservoirChunkCapacity() const {\n+\treturn sample_count + (FIXED_SAMPLE_SIZE_MULTIPLIER * FIXED_SAMPLE_SIZE);\n+}\n+\n+idx_t ReservoirSample::FillReservoir(DataChunk &chunk) {\n+\n+\tidx_t ingested_count = 0;\n+\tif (!reservoir_chunk) {\n+\t\tif (chunk.size() > FIXED_SAMPLE_SIZE) {\n+\t\t\tthrow InternalException(\"Creating sample with DataChunk that is larger than the fixed sample size\");\n+\t\t}\n+\t\tauto types = chunk.GetTypes();\n+\t\t// create a new sample chunk to store new samples\n+\t\treservoir_chunk = CreateNewSampleChunk(types, GetReservoirChunkCapacity());\n+\t}\n+\n+\tidx_t actual_sample_index_start = GetActiveSampleCount();\n+\tD_ASSERT(reservoir_chunk->chunk.ColumnCount() == chunk.ColumnCount());\n+\n+\tif (reservoir_chunk->chunk.size() < sample_count) {\n+\t\tingested_count = MinValue<idx_t>(sample_count - reservoir_chunk->chunk.size(), chunk.size());\n+\t\tauto random_other_sel =\n+\t\t    GetRandomizedVector(static_cast<uint32_t>(ingested_count), static_cast<uint32_t>(ingested_count));\n+\t\tSelectionVector sel_for_input_chunk(ingested_count);\n+\t\tfor (idx_t i = 0; i < ingested_count; i++) {\n+\t\t\tsel.set_index(actual_sample_index_start + i, actual_sample_index_start + i);\n+\t\t\tsel_for_input_chunk.set_index(i, random_other_sel[i]);\n+\t\t}\n+\t\tUpdateSampleAppend(reservoir_chunk->chunk, chunk, sel_for_input_chunk, ingested_count);\n+\t\tsel_size += ingested_count;\n+\t}\n+\tD_ASSERT(GetActiveSampleCount() <= sample_count);\n+\tD_ASSERT(GetActiveSampleCount() >= ingested_count);\n+\t// always return how many tuples were ingested\n+\treturn ingested_count;\n+}\n+\n+void ReservoirSample::Destroy() {\n+\tdestroyed = true;\n+}\n+\n+SelectionVectorHelper ReservoirSample::GetReplacementIndexes(idx_t sample_chunk_offset,\n+                                                             idx_t theoretical_chunk_length) {\n+\tif (GetSamplingState() == SamplingState::RANDOM) {\n+\t\treturn GetReplacementIndexesFast(sample_chunk_offset, theoretical_chunk_length);\n+\t}\n+\treturn GetReplacementIndexesSlow(sample_chunk_offset, theoretical_chunk_length);\n+}\n+\n+SelectionVectorHelper ReservoirSample::GetReplacementIndexesFast(idx_t sample_chunk_offset, idx_t chunk_length) {\n+\n+\t// how much weight to the other tuples have compared to the ones in this chunk?\n+\tauto weight_tuples_other = static_cast<double>(chunk_length) / static_cast<double>(GetTuplesSeen() + chunk_length);\n+\tauto num_to_pop = static_cast<uint32_t>(round(weight_tuples_other * static_cast<double>(sample_count)));\n+\tD_ASSERT(num_to_pop <= sample_count);\n+\tD_ASSERT(num_to_pop <= sel_size);\n+\tSelectionVectorHelper ret;\n+\n+\tif (num_to_pop == 0) {\n+\t\tret.sel = SelectionVector(num_to_pop);\n+\t\tret.size = 0;\n+\t\treturn ret;\n+\t}\n+\tstd::unordered_map<idx_t, idx_t> replacement_indexes;\n+\tSelectionVector chunk_sel(num_to_pop);\n+\n+\tauto random_indexes_chunk = GetRandomizedVector(static_cast<uint32_t>(chunk_length), num_to_pop);\n+\tauto random_sel_indexes = GetRandomizedVector(static_cast<uint32_t>(sel_size), num_to_pop);\n+\tfor (idx_t i = 0; i < num_to_pop; i++) {\n+\t\t// update the selection vector for the reservoir sample\n+\t\tchunk_sel.set_index(i, random_indexes_chunk[i]);\n+\t\t// sel is not guaratneed to be random, so we update the indexes according to our\n+\t\t// random sel indexes.\n+\t\tsel.set_index(random_sel_indexes[i], sample_chunk_offset + i);\n+\t}\n+\n+\tD_ASSERT(sel_size == sample_count);\n+\n+\tret.sel = SelectionVector(chunk_sel);\n+\tret.size = num_to_pop;\n+\treturn ret;\n+}\n+\n+SelectionVectorHelper ReservoirSample::GetReplacementIndexesSlow(const idx_t sample_chunk_offset,\n+                                                                 const idx_t chunk_length) {\n+\tidx_t remaining = chunk_length;\n+\tstd::unordered_map<idx_t, idx_t> ret_map;\n+\tidx_t sample_chunk_index = 0;\n+\n+\tidx_t base_offset = 0;\n+\n+\twhile (true) {\n+\t\tidx_t offset =\n+\t\t    base_reservoir_sample->next_index_to_sample - base_reservoir_sample->num_entries_to_skip_b4_next_sample;\n+\t\tif (offset >= remaining) {\n+\t\t\t// not in this chunk! increment current count and go to the next chunk\n+\t\t\tbase_reservoir_sample->num_entries_to_skip_b4_next_sample += remaining;\n+\t\t\tbreak;\n+\t\t}\n+\t\t// in this chunk! replace the element\n+\t\t// ret[index_in_new_chunk] = index_in_sample_chunk (the sample chunk offset will be applied later)\n+\t\t// D_ASSERT(sample_chunk_index == ret.size());\n+\t\tret_map[base_offset + offset] = sample_chunk_index;\n+\t\tdouble r2 = base_reservoir_sample->random.NextRandom32(base_reservoir_sample->min_weight_threshold, 1);\n+\t\t// replace element in our max_heap\n+\t\t// first get the top most pair\n+\t\tconst auto top = PopFromWeightQueue();\n+\t\tconst auto index = top.second;\n+\t\tconst auto index_in_sample_chunk = sample_chunk_offset + sample_chunk_index;\n+\t\tsel.set_index(index, index_in_sample_chunk);\n+\t\tbase_reservoir_sample->ReplaceElementWithIndex(index, r2, false);\n+\n+\t\tsample_chunk_index += 1;\n+\t\t// shift the chunk forward\n+\t\tremaining -= offset;\n+\t\tbase_offset += offset;\n+\t}\n+\n+\t// create selection vector to return\n+\tSelectionVector ret_sel(ret_map.size());\n+\tD_ASSERT(sel_size == sample_count);\n+\tfor (auto &kv : ret_map) {\n+\t\tret_sel.set_index(kv.second, kv.first);\n+\t}\n+\tSelectionVectorHelper ret;\n+\tret.sel = SelectionVector(ret_sel);\n+\tret.size = static_cast<uint32_t>(ret_map.size());\n+\treturn ret;\n+}\n+\n+void ReservoirSample::Finalize() {\n+}\n+\n+bool ReservoirSample::ValidSampleType(const LogicalType &type) {\n+\treturn type.IsNumeric();\n+}\n+\n+void ReservoirSample::UpdateSampleAppend(DataChunk &this_, DataChunk &other, SelectionVector &other_sel,\n+                                         idx_t append_count) const {\n+\tidx_t new_size = this_.size() + append_count;\n+\tif (other.size() == 0) {\n+\t\treturn;\n+\t}\n+\tD_ASSERT(this_.GetTypes() == other.GetTypes());\n+\n+\t// UpdateSampleAppend(this_, other, other_sel, append_count);\n+\tD_ASSERT(this_.GetTypes() == other.GetTypes());\n+\tauto types = reservoir_chunk->chunk.GetTypes();\n+\n+\tfor (idx_t i = 0; i < reservoir_chunk->chunk.ColumnCount(); i++) {\n+\t\tauto col_type = types[i];\n+\t\tif (ValidSampleType(col_type) || !stats_sample) {\n+\t\t\tD_ASSERT(this_.data[i].GetVectorType() == VectorType::FLAT_VECTOR);\n+\t\t\tVectorOperations::Copy(other.data[i], this_.data[i], other_sel, append_count, 0, this_.size());\n+\t\t}\n+\t}\n+\tthis_.SetCardinality(new_size);\n+}\n+\n+void ReservoirSample::AddToReservoir(DataChunk &chunk) {\n+\tif (destroyed || chunk.size() == 0) {\n+\t\treturn;\n+\t}\n+\n+\tidx_t tuples_consumed = FillReservoir(chunk);\n+\tbase_reservoir_sample->num_entries_seen_total += tuples_consumed;\n+\tD_ASSERT(reservoir_chunk->chunk.size() >= 1);\n+\n+\tif (tuples_consumed == chunk.size()) {\n+\t\treturn;\n+\t}\n+\n+\t// the chunk filled the first FIXED_SAMPLE_SIZE chunk but still has tuples remaining\n+\t// slice the chunk and call AddToReservoir again.\n+\tif (tuples_consumed != chunk.size() && tuples_consumed != 0) {\n+\t\t// Fill reservoir consumed some of the chunk to reach FIXED_SAMPLE_SIZE\n+\t\t// now we need to\n+\t\t// So we slice it and call AddToReservoir\n+\t\tauto slice = make_uniq<DataChunk>();\n+\t\tauto samples_remaining = chunk.size() - tuples_consumed;\n+\t\tauto types = chunk.GetTypes();\n+\t\tSelectionVector input_sel(samples_remaining);\n+\t\tfor (idx_t i = 0; i < samples_remaining; i++) {\n+\t\t\tinput_sel.set_index(i, tuples_consumed + i);\n+\t\t}\n+\t\tslice->Initialize(Allocator::DefaultAllocator(), types, samples_remaining);\n+\t\tslice->Slice(chunk, input_sel, samples_remaining);\n+\t\tslice->SetCardinality(samples_remaining);\n+\t\tAddToReservoir(*slice);\n+\t\treturn;\n+\t}\n+\n+\t// at this point we should have collected at least sample count samples\n+\tD_ASSERT(GetActiveSampleCount() >= sample_count);\n+\n+\tauto chunk_sel = GetReplacementIndexes(reservoir_chunk->chunk.size(), chunk.size());\n+\n+\tif (chunk_sel.size == 0) {\n+\t\t// not adding any samples\n+\t\treturn;\n+\t}\n+\tidx_t size = chunk_sel.size;\n+\tD_ASSERT(size <= chunk.size());\n+\n+\tUpdateSampleAppend(reservoir_chunk->chunk, chunk, chunk_sel.sel, size);\n+\n+\tbase_reservoir_sample->num_entries_seen_total += chunk.size();\n+\tD_ASSERT(base_reservoir_sample->reservoir_weights.size() == 0 ||\n+\t         base_reservoir_sample->reservoir_weights.size() == sample_count);\n+\n+\tVerify();\n+\n+\t// if we are over the threshold, we ned to swith to slow sampling.\n+\tif (GetSamplingState() == SamplingState::RANDOM && GetTuplesSeen() >= FIXED_SAMPLE_SIZE * FAST_TO_SLOW_THRESHOLD) {\n+\t\tConvertToReservoirSample();\n+\t}\n+\tif (reservoir_chunk->chunk.size() >= (GetReservoirChunkCapacity() - (static_cast<idx_t>(FIXED_SAMPLE_SIZE) * 3))) {\n+\t\tVacuum();\n+\t}\n+}\n+\n+void ReservoirSample::Verify() {\n+#ifdef DEBUG\n+\tif (destroyed) {\n+\t\treturn;\n+\t}\n+\tif (GetPriorityQueueSize() == 0) {\n+\t\tD_ASSERT(GetActiveSampleCount() <= sample_count);\n+\t\tD_ASSERT(GetTuplesSeen() >= GetActiveSampleCount());\n+\t\treturn;\n+\t}\n+\tif (NumSamplesCollected() > sample_count) {\n+\t\tD_ASSERT(GetPriorityQueueSize() == sample_count);\n+\t} else if (NumSamplesCollected() <= sample_count && GetPriorityQueueSize() > 0) {\n+\t\t// it's possible to collect more samples than your priority queue size.\n+\t\t// see sample_converts_to_reservoir_sample.test\n+\t\tD_ASSERT(NumSamplesCollected() >= GetPriorityQueueSize());\n+\t}\n+\tauto base_reservoir_copy = base_reservoir_sample->Copy();\n+\tstd::unordered_map<idx_t, idx_t> index_count;\n+\twhile (!base_reservoir_copy->reservoir_weights.empty()) {\n+\t\tauto &pair = base_reservoir_copy->reservoir_weights.top();\n+\t\tif (index_count.find(pair.second) == index_count.end()) {\n+\t\t\tindex_count[pair.second] = 1;\n+\t\t\tbase_reservoir_copy->reservoir_weights.pop();\n+\t\t} else {\n+\t\t\tindex_count[pair.second] += 1;\n+\t\t\tbase_reservoir_copy->reservoir_weights.pop();\n+\t\t\tthrow InternalException(\"Duplicate selection index in reservoir weights\");\n+\t\t}\n+\t}\n+\t// TODO: Verify the Sel as well. No duplicate indices.\n+\n+\tif (reservoir_chunk) {\n+\t\treservoir_chunk->chunk.Verify();\n+\t}\n+#endif\n+}\n+\n+ReservoirSamplePercentage::ReservoirSamplePercentage(double percentage, int64_t seed, idx_t reservoir_sample_size)\n+    : BlockingSample(seed), allocator(Allocator::DefaultAllocator()), sample_percentage(percentage / 100.0),\n+      reservoir_sample_size(reservoir_sample_size), current_count(0), is_finalized(false) {\n+\tcurrent_sample = make_uniq<ReservoirSample>(allocator, reservoir_sample_size, base_reservoir_sample->random());\n+\ttype = SampleType::RESERVOIR_PERCENTAGE_SAMPLE;\n+}\n+\n+ReservoirSamplePercentage::ReservoirSamplePercentage(Allocator &allocator, double percentage, int64_t seed)\n+    : BlockingSample(seed), allocator(allocator), sample_percentage(percentage / 100.0), current_count(0),\n+      is_finalized(false) {\n+\treservoir_sample_size = (idx_t)(sample_percentage * RESERVOIR_THRESHOLD);\n+\tcurrent_sample = make_uniq<ReservoirSample>(allocator, reservoir_sample_size, base_reservoir_sample->random());\n+\ttype = SampleType::RESERVOIR_PERCENTAGE_SAMPLE;\n+}\n+\n+ReservoirSamplePercentage::ReservoirSamplePercentage(double percentage, int64_t seed)\n+    : ReservoirSamplePercentage(Allocator::DefaultAllocator(), percentage, seed) {\n+}\n+\n+void ReservoirSamplePercentage::AddToReservoir(DataChunk &input) {\n+\tbase_reservoir_sample->num_entries_seen_total += input.size();\n+\tif (current_count + input.size() > RESERVOIR_THRESHOLD) {\n+\t\t// we don't have enough space in our current reservoir\n+\t\t// first check what we still need to append to the current sample\n+\t\tidx_t append_to_current_sample_count = RESERVOIR_THRESHOLD - current_count;\n+\t\tidx_t append_to_next_sample = input.size() - append_to_current_sample_count;\n+\t\tif (append_to_current_sample_count > 0) {\n+\t\t\t// we have elements remaining, first add them to the current sample\n+\t\t\tif (append_to_next_sample > 0) {\n+\t\t\t\t// we need to also add to the next sample\n+\t\t\t\tDataChunk new_chunk;\n+\t\t\t\tnew_chunk.InitializeEmpty(input.GetTypes());\n+\t\t\t\tnew_chunk.Slice(input, *FlatVector::IncrementalSelectionVector(), append_to_current_sample_count);\n+\t\t\t\tnew_chunk.Flatten();\n+\t\t\t\tcurrent_sample->AddToReservoir(new_chunk);\n+\t\t\t} else {\n+\t\t\t\tinput.Flatten();\n+\t\t\t\tinput.SetCardinality(append_to_current_sample_count);\n+\t\t\t\tcurrent_sample->AddToReservoir(input);\n+\t\t\t}\n+\t\t}\n+\t\tif (append_to_next_sample > 0) {\n+\t\t\t// slice the input for the remainder\n+\t\t\tSelectionVector sel(append_to_next_sample);\n+\t\t\tfor (idx_t i = append_to_current_sample_count; i < append_to_next_sample + append_to_current_sample_count;\n+\t\t\t     i++) {\n+\t\t\t\tsel.set_index(i - append_to_current_sample_count, i);\n+\t\t\t}\n+\t\t\tinput.Slice(sel, append_to_next_sample);\n+\t\t}\n+\t\t// now our first sample is filled: append it to the set of finished samples\n+\t\tfinished_samples.push_back(std::move(current_sample));\n+\n+\t\t// allocate a new sample, and potentially add the remainder of the current input to that sample\n+\t\tcurrent_sample = make_uniq<ReservoirSample>(allocator, reservoir_sample_size, base_reservoir_sample->random());\n+\t\tif (append_to_next_sample > 0) {\n+\t\t\tcurrent_sample->AddToReservoir(input);\n+\t\t}\n+\t\tcurrent_count = append_to_next_sample;\n+\t} else {\n+\t\t// we can just append to the current sample\n+\t\tcurrent_count += input.size();\n+\t\tcurrent_sample->AddToReservoir(input);\n+\t}\n+}\n+\n+unique_ptr<DataChunk> ReservoirSamplePercentage::GetChunk() {\n+\t// reservoir sample percentage should never stay\n+\tif (!is_finalized) {\n+\t\tFinalize();\n+\t}\n+\twhile (!finished_samples.empty()) {\n+\t\tauto &front = finished_samples.front();\n+\t\tauto chunk = front->GetChunk();\n+\t\tif (chunk && chunk->size() > 0) {\n+\t\t\treturn chunk;\n+\t\t}\n+\t\t// move to the next sample\n+\t\tfinished_samples.erase(finished_samples.begin());\n+\t}\n+\treturn nullptr;\n+}\n+\n+unique_ptr<BlockingSample> ReservoirSamplePercentage::Copy() const {\n+\tthrow InternalException(\"Cannot call Copy on ReservoirSample Percentage\");\n+}\n+\n+void ReservoirSamplePercentage::Finalize() {\n+\t// need to finalize the current sample, if any\n+\t// we are finializing, so we are starting to return chunks. Our last chunk has\n+\t// sample_percentage * RESERVOIR_THRESHOLD entries that hold samples.\n+\t// if our current count is less than the sample_percentage * RESERVOIR_THRESHOLD\n+\t// then we have sampled too much for the current_sample and we need to redo the sample\n+\t// otherwise we can just push the current sample back\n+\t// Imagine sampling 70% of 100 rows (so 70 rows). We allocate sample_percentage * RESERVOIR_THRESHOLD\n+\t// -----------------------------------------\n+\tauto sampled_more_than_required =\n+\t    static_cast<double>(current_count) > sample_percentage * RESERVOIR_THRESHOLD || finished_samples.empty();\n+\tif (current_count > 0 && sampled_more_than_required) {\n+\t\t// create a new sample\n+\t\tauto new_sample_size = static_cast<idx_t>(round(sample_percentage * static_cast<double>(current_count)));\n+\t\tauto new_sample = make_uniq<ReservoirSample>(allocator, new_sample_size, base_reservoir_sample->random());\n+\t\twhile (true) {\n+\t\t\tauto chunk = current_sample->GetChunk();\n+\t\t\tif (!chunk || chunk->size() == 0) {\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tnew_sample->AddToReservoir(*chunk);\n+\t\t}\n+\t\tfinished_samples.push_back(std::move(new_sample));\n+\t} else {\n+\t\tfinished_samples.push_back(std::move(current_sample));\n+\t}\n+\t// when finalizing, current_sample is null. All samples are now in finished samples.\n+\tcurrent_sample = nullptr;\n+\tis_finalized = true;\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/function/table/system/CMakeLists.txt b/src/function/table/system/CMakeLists.txt\nindex 36bcfa029613..573895f2b4a2 100644\n--- a/src/function/table/system/CMakeLists.txt\n+++ b/src/function/table/system/CMakeLists.txt\n@@ -26,6 +26,7 @@ add_library_unity(\n   pragma_metadata_info.cpp\n   pragma_storage_info.cpp\n   pragma_table_info.cpp\n+  pragma_table_sample.cpp\n   pragma_user_agent.cpp\n   test_all_types.cpp\n   test_vector_types.cpp)\ndiff --git a/src/function/table/system/pragma_table_sample.cpp b/src/function/table/system/pragma_table_sample.cpp\nnew file mode 100644\nindex 000000000000..7f4122b92b62\n--- /dev/null\n+++ b/src/function/table/system/pragma_table_sample.cpp\n@@ -0,0 +1,95 @@\n+#include \"duckdb/function/table/system_functions.hpp\"\n+\n+#include \"duckdb/catalog/catalog.hpp\"\n+#include \"duckdb/catalog/catalog_entry/table_catalog_entry.hpp\"\n+#include \"duckdb/catalog/catalog_entry/view_catalog_entry.hpp\"\n+#include \"duckdb/parser/qualified_name.hpp\"\n+#include \"duckdb/parser/constraints/not_null_constraint.hpp\"\n+#include \"duckdb/parser/constraints/unique_constraint.hpp\"\n+#include \"duckdb/planner/expression/bound_parameter_expression.hpp\"\n+#include \"duckdb/planner/binder.hpp\"\n+\n+#include \"duckdb/common/exception.hpp\"\n+#include \"duckdb/common/limits.hpp\"\n+\n+#include <algorithm>\n+\n+namespace duckdb {\n+\n+struct DuckDBTableSampleFunctionData : public TableFunctionData {\n+\texplicit DuckDBTableSampleFunctionData(CatalogEntry &entry_p) : entry(entry_p) {\n+\t}\n+\tCatalogEntry &entry;\n+};\n+\n+struct DuckDBTableSampleOperatorData : public GlobalTableFunctionState {\n+\tDuckDBTableSampleOperatorData() : sample_offset(0) {\n+\t\tsample = nullptr;\n+\t}\n+\tidx_t sample_offset;\n+\tunique_ptr<BlockingSample> sample;\n+};\n+\n+static unique_ptr<FunctionData> DuckDBTableSampleBind(ClientContext &context, TableFunctionBindInput &input,\n+                                                      vector<LogicalType> &return_types, vector<string> &names) {\n+\n+\t// look up the table name in the catalog\n+\tauto qname = QualifiedName::Parse(input.inputs[0].GetValue<string>());\n+\tBinder::BindSchemaOrCatalog(context, qname.catalog, qname.schema);\n+\n+\tauto &entry = Catalog::GetEntry(context, CatalogType::TABLE_ENTRY, qname.catalog, qname.schema, qname.name);\n+\tif (entry.type != CatalogType::TABLE_ENTRY) {\n+\t\tthrow NotImplementedException(\"Invalid Catalog type passed to table_sample()\");\n+\t}\n+\tauto &table_entry = entry.Cast<TableCatalogEntry>();\n+\tauto types = table_entry.GetTypes();\n+\tfor (auto &type : types) {\n+\t\treturn_types.push_back(type);\n+\t}\n+\tfor (idx_t i = 0; i < types.size(); i++) {\n+\t\tauto logical_index = LogicalIndex(i);\n+\t\tauto &col = table_entry.GetColumn(logical_index);\n+\t\tnames.push_back(col.GetName());\n+\t}\n+\n+\treturn make_uniq<DuckDBTableSampleFunctionData>(entry);\n+}\n+\n+unique_ptr<GlobalTableFunctionState> DuckDBTableSampleInit(ClientContext &context, TableFunctionInitInput &input) {\n+\treturn make_uniq<DuckDBTableSampleOperatorData>();\n+}\n+\n+static void DuckDBTableSampleTable(ClientContext &context, DuckDBTableSampleOperatorData &data,\n+                                   TableCatalogEntry &table, DataChunk &output) {\n+\t// if table has statistics.\n+\t// copy the sample of statistics into the output chunk\n+\tif (!data.sample) {\n+\t\tdata.sample = table.GetSample();\n+\t}\n+\tif (data.sample) {\n+\t\tauto sample_chunk = data.sample->GetChunk();\n+\t\tif (sample_chunk) {\n+\t\t\tsample_chunk->Copy(output, 0);\n+\t\t\tdata.sample_offset += sample_chunk->size();\n+\t\t}\n+\t}\n+}\n+\n+static void DuckDBTableSampleFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &bind_data = data_p.bind_data->Cast<DuckDBTableSampleFunctionData>();\n+\tauto &state = data_p.global_state->Cast<DuckDBTableSampleOperatorData>();\n+\tswitch (bind_data.entry.type) {\n+\tcase CatalogType::TABLE_ENTRY:\n+\t\tDuckDBTableSampleTable(context, state, bind_data.entry.Cast<TableCatalogEntry>(), output);\n+\t\tbreak;\n+\tdefault:\n+\t\tthrow NotImplementedException(\"Unimplemented catalog type for pragma_table_sample\");\n+\t}\n+}\n+\n+void DuckDBTableSample::RegisterFunction(BuiltinFunctions &set) {\n+\tset.AddFunction(TableFunction(\"duckdb_table_sample\", {LogicalType::VARCHAR}, DuckDBTableSampleFunction,\n+\t                              DuckDBTableSampleBind, DuckDBTableSampleInit));\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/function/table/system_functions.cpp b/src/function/table/system_functions.cpp\nindex 12e8bcc35eff..7560221c587b 100644\n--- a/src/function/table/system_functions.cpp\n+++ b/src/function/table/system_functions.cpp\n@@ -34,6 +34,7 @@ void BuiltinFunctions::RegisterSQLiteFunctions() {\n \tDuckDBSequencesFun::RegisterFunction(*this);\n \tDuckDBSettingsFun::RegisterFunction(*this);\n \tDuckDBTablesFun::RegisterFunction(*this);\n+\tDuckDBTableSample::RegisterFunction(*this);\n \tDuckDBTemporaryFilesFun::RegisterFunction(*this);\n \tDuckDBTypesFun::RegisterFunction(*this);\n \tDuckDBVariablesFun::RegisterFunction(*this);\ndiff --git a/src/include/duckdb/catalog/catalog_entry/duck_table_entry.hpp b/src/include/duckdb/catalog/catalog_entry/duck_table_entry.hpp\nindex ce09c4fe54f1..fb9d5ae67d14 100644\n--- a/src/include/duckdb/catalog/catalog_entry/duck_table_entry.hpp\n+++ b/src/include/duckdb/catalog/catalog_entry/duck_table_entry.hpp\n@@ -35,6 +35,8 @@ class DuckTableEntry : public TableCatalogEntry {\n \t//! Get statistics of a column (physical or virtual) within the table\n \tunique_ptr<BaseStatistics> GetStatistics(ClientContext &context, column_t column_id) override;\n \n+\tunique_ptr<BlockingSample> GetSample() override;\n+\n \tunique_ptr<CatalogEntry> Copy(ClientContext &context) const override;\n \n \tvoid SetAsRoot() override;\ndiff --git a/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp b/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp\nindex d2e47e6d4ea4..5ce45af7ff2b 100644\n--- a/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp\n+++ b/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp\n@@ -13,6 +13,7 @@\n #include \"duckdb/parser/column_list.hpp\"\n #include \"duckdb/parser/constraint.hpp\"\n #include \"duckdb/planner/bound_constraint.hpp\"\n+#include \"duckdb/storage/table/table_statistics.hpp\"\n #include \"duckdb/planner/expression.hpp\"\n #include \"duckdb/common/case_insensitive_map.hpp\"\n #include \"duckdb/catalog/catalog_entry/table_column_type.hpp\"\n@@ -82,6 +83,8 @@ class TableCatalogEntry : public StandardEntry {\n \t//! Get statistics of a column (physical or virtual) within the table\n \tvirtual unique_ptr<BaseStatistics> GetStatistics(ClientContext &context, column_t column_id) = 0;\n \n+\tvirtual unique_ptr<BlockingSample> GetSample();\n+\n \t//! Returns the column index of the specified column name.\n \t//! If the column does not exist:\n \t//! If if_column_exists is true, returns DConstants::INVALID_INDEX\ndiff --git a/src/include/duckdb/common/enum_util.hpp b/src/include/duckdb/common/enum_util.hpp\nindex 855e153870a7..6057a89b2aa3 100644\n--- a/src/include/duckdb/common/enum_util.hpp\n+++ b/src/include/duckdb/common/enum_util.hpp\n@@ -282,6 +282,8 @@ enum class SampleMethod : uint8_t;\n \n enum class SampleType : uint8_t;\n \n+enum class SamplingState : uint8_t;\n+\n enum class ScanType : uint8_t;\n \n enum class SecretDisplayType : uint8_t;\n@@ -752,6 +754,9 @@ const char* EnumUtil::ToChars<SampleMethod>(SampleMethod value);\n template<>\n const char* EnumUtil::ToChars<SampleType>(SampleType value);\n \n+template<>\n+const char* EnumUtil::ToChars<SamplingState>(SamplingState value);\n+\n template<>\n const char* EnumUtil::ToChars<ScanType>(ScanType value);\n \n@@ -1269,6 +1274,9 @@ SampleMethod EnumUtil::FromString<SampleMethod>(const char *value);\n template<>\n SampleType EnumUtil::FromString<SampleType>(const char *value);\n \n+template<>\n+SamplingState EnumUtil::FromString<SamplingState>(const char *value);\n+\n template<>\n ScanType EnumUtil::FromString<ScanType>(const char *value);\n \ndiff --git a/src/include/duckdb/common/random_engine.hpp b/src/include/duckdb/common/random_engine.hpp\nindex 970db6ce4f45..59531e1d945e 100644\n--- a/src/include/duckdb/common/random_engine.hpp\n+++ b/src/include/duckdb/common/random_engine.hpp\n@@ -18,11 +18,11 @@ namespace duckdb {\n class ClientContext;\n struct RandomState;\n \n-struct RandomEngine {\n+class RandomEngine {\n+public:\n \texplicit RandomEngine(int64_t seed = -1);\n \t~RandomEngine();\n \n-public:\n \t//! Generate a random number between min and max\n \tdouble NextRandom(double min, double max);\n \n@@ -31,6 +31,7 @@ struct RandomEngine {\n \t//! Generate a random number between 0 and 1, using 32-bits as a base\n \tdouble NextRandom32();\n \tdouble NextRandom32(double min, double max);\n+\tuint32_t NextRandomInteger32(uint32_t min, uint32_t max);\n \tuint32_t NextRandomInteger();\n \tuint32_t NextRandomInteger(uint32_t min, uint32_t max);\n \tuint64_t NextRandomInteger64();\ndiff --git a/src/include/duckdb/common/serializer/serializer.hpp b/src/include/duckdb/common/serializer/serializer.hpp\nindex 60531fd1a3b3..54b994e4ba2d 100644\n--- a/src/include/duckdb/common/serializer/serializer.hpp\n+++ b/src/include/duckdb/common/serializer/serializer.hpp\n@@ -16,6 +16,7 @@\n #include \"duckdb/common/types/uhugeint.hpp\"\n #include \"duckdb/common/unordered_map.hpp\"\n #include \"duckdb/common/unordered_set.hpp\"\n+#include \"duckdb/common/queue.hpp\"\n #include \"duckdb/common/optional_idx.hpp\"\n #include \"duckdb/common/optionally_owned_ptr.hpp\"\n #include \"duckdb/common/value_operations/value_operations.hpp\"\ndiff --git a/src/include/duckdb/common/types/uuid.hpp b/src/include/duckdb/common/types/uuid.hpp\nindex 5573aac633c5..bf5ade17a15f 100644\n--- a/src/include/duckdb/common/types/uuid.hpp\n+++ b/src/include/duckdb/common/types/uuid.hpp\n@@ -13,7 +13,7 @@\n \n namespace duckdb {\n class ClientContext;\n-struct RandomEngine;\n+class RandomEngine;\n \n //! The UUID class contains static operations for the UUID type\n class UUID {\ndiff --git a/src/include/duckdb/execution/physical_operator.hpp b/src/include/duckdb/execution/physical_operator.hpp\nindex 822b55377b74..50529d594f67 100644\n--- a/src/include/duckdb/execution/physical_operator.hpp\n+++ b/src/include/duckdb/execution/physical_operator.hpp\n@@ -164,7 +164,7 @@ class PhysicalOperator {\n \tvirtual void PrepareFinalize(ClientContext &context, GlobalSinkState &sink_state) const;\n \t//! The finalize is called when ALL threads are finished execution. It is called only once per pipeline, and is\n \t//! entirely single threaded.\n-\t//! If Finalize returns SinkResultType::FINISHED, the sink is marked as finished\n+\t//! If Finalize returns SinkResultType::Finished, the sink is marked as finished\n \tvirtual SinkFinalizeType Finalize(Pipeline &pipeline, Event &event, ClientContext &context,\n \t                                  OperatorSinkFinalizeInput &input) const;\n \t//! For sinks with RequiresBatchIndex set to true, when a new batch starts being processed this method is called\ndiff --git a/src/include/duckdb/execution/reservoir_sample.hpp b/src/include/duckdb/execution/reservoir_sample.hpp\nindex 0edc7e073b9a..b794328bb0fb 100644\n--- a/src/include/duckdb/execution/reservoir_sample.hpp\n+++ b/src/include/duckdb/execution/reservoir_sample.hpp\n@@ -12,25 +12,64 @@\n #include \"duckdb/common/common.hpp\"\n #include \"duckdb/common/random_engine.hpp\"\n #include \"duckdb/common/types/data_chunk.hpp\"\n+#include \"duckdb/common/windows_undefs.hpp\"\n \n #include \"duckdb/common/queue.hpp\"\n \n+// Originally intended to be the vector size, but in order to run on\n+// vector size = 2, we had to change it.\n+#define FIXED_SAMPLE_SIZE 2048\n+\n namespace duckdb {\n \n enum class SampleType : uint8_t { BLOCKING_SAMPLE = 0, RESERVOIR_SAMPLE = 1, RESERVOIR_PERCENTAGE_SAMPLE = 2 };\n \n+enum class SamplingState : uint8_t { RANDOM = 0, RESERVOIR = 1 };\n+\n+class ReservoirRNG : public RandomEngine {\n+public:\n+\t// return type must be called result type to be a valid URNG\n+\ttypedef uint32_t result_type;\n+\n+\texplicit ReservoirRNG(int64_t seed) : RandomEngine(seed) {};\n+\n+\tresult_type operator()() {\n+\t\treturn NextRandomInteger();\n+\t};\n+\n+\tstatic constexpr result_type min() {\n+\t\treturn NumericLimits<result_type>::Minimum();\n+\t};\n+\tstatic constexpr result_type max() {\n+\t\treturn NumericLimits<result_type>::Maximum();\n+\t};\n+};\n+\n+//! Resevoir sampling is based on the 2005 paper \"Weighted Random Sampling\" by Efraimidis and Spirakis\n class BaseReservoirSampling {\n public:\n \texplicit BaseReservoirSampling(int64_t seed);\n \tBaseReservoirSampling();\n \n-\tvoid InitializeReservoir(idx_t cur_size, idx_t sample_size);\n+\tvoid InitializeReservoirWeights(idx_t cur_size, idx_t sample_size);\n \n \tvoid SetNextEntry();\n \n+\tvoid ReplaceElementWithIndex(idx_t entry_index, double with_weight, bool pop = true);\n \tvoid ReplaceElement(double with_weight = -1);\n+\n+\tvoid UpdateMinWeightThreshold();\n+\n+\t//! Go from the naive sampling to the reservoir sampling\n+\t//! Naive samping will not collect weights, but when we serialize\n+\t//! we need to serialize weights again.\n+\tvoid FillWeights(SelectionVector &sel, idx_t &sel_size);\n+\n+\tunique_ptr<BaseReservoirSampling> Copy();\n+\n \t//! The random generator\n-\tRandomEngine random;\n+\tReservoirRNG random;\n+\n \t//! The next element to sample\n \tidx_t next_index_to_sample;\n \t//! The reservoir threshold of the current min entry\n@@ -48,6 +87,13 @@ class BaseReservoirSampling {\n \n \tvoid Serialize(Serializer &serializer) const;\n \tstatic unique_ptr<BaseReservoirSampling> Deserialize(Deserializer &deserializer);\n+\n+\tstatic double GetMinWeightFromTuplesSeen(idx_t rows_seen_total);\n+\t// static unordered_map<idx_t, double> tuples_to_min_weight_map;\n+\t// Blocking sample is a virtual class. It should be allowed to see the weights and\n+\t// of tuples in the sample. The blocking sample can then easily maintain statisitcal properties\n+\t// from the sample point of view.\n+\tfriend class BlockingSample;\n };\n \n class BlockingSample {\n@@ -61,24 +107,31 @@ class BlockingSample {\n \tbool destroyed;\n \n public:\n-\texplicit BlockingSample(int64_t seed) : old_base_reservoir_sample(seed), random(old_base_reservoir_sample.random) {\n-\t\tbase_reservoir_sample = nullptr;\n+\texplicit BlockingSample(int64_t seed = -1)\n+\t    : base_reservoir_sample(make_uniq<BaseReservoirSampling>(seed)), type(SampleType::BLOCKING_SAMPLE),\n+\t      destroyed(false) {\n \t}\n \tvirtual ~BlockingSample() {\n \t}\n \n \t//! Add a chunk of data to the sample\n \tvirtual void AddToReservoir(DataChunk &input) = 0;\n-\n+\tvirtual unique_ptr<BlockingSample> Copy() const = 0;\n \tvirtual void Finalize() = 0;\n-\t//! Fetches a chunk from the sample. Note that this method is destructive and should only be used after the\n-\t//! sample is completely built.\n+\tvirtual void Destroy();\n+\n+\t//! Fetches a chunk from the sample. destroy = true should only be used when\n+\t//! querying from a sample defined in a query and not a duckdb_table_sample.\n \tvirtual unique_ptr<DataChunk> GetChunk() = 0;\n-\tBaseReservoirSampling old_base_reservoir_sample;\n \n \tvirtual void Serialize(Serializer &serializer) const;\n \tstatic unique_ptr<BlockingSample> Deserialize(Deserializer &deserializer);\n \n+\t//! Helper functions needed to merge two reservoirs while respecting weights of sampled rows\n+\tstd::pair<double, idx_t> PopFromWeightQueue();\n+\tdouble GetMinWeightThreshold();\n+\tidx_t GetPriorityQueueSize();\n+\n public:\n \ttemplate <class TARGET>\n \tTARGET &Cast() {\n@@ -95,8 +148,6 @@ class BlockingSample {\n \t\t}\n \t\treturn reinterpret_cast<const TARGET &>(*this);\n \t}\n-\t//! The reservoir sampling\n-\tRandomEngine &random;\n };\n \n class ReservoirChunk {\n@@ -107,45 +158,120 @@ class ReservoirChunk {\n \tDataChunk chunk;\n \tvoid Serialize(Serializer &serializer) const;\n \tstatic unique_ptr<ReservoirChunk> Deserialize(Deserializer &deserializer);\n+\n+\tunique_ptr<ReservoirChunk> Copy() const;\n+};\n+\n+struct SelectionVectorHelper {\n+\tSelectionVector sel;\n+\tuint32_t size;\n };\n \n-//! The reservoir sample class maintains a streaming sample of fixed size \"sample_count\"\n class ReservoirSample : public BlockingSample {\n public:\n \tstatic constexpr const SampleType TYPE = SampleType::RESERVOIR_SAMPLE;\n \n-public:\n+\tconstexpr static idx_t FIXED_SAMPLE_SIZE_MULTIPLIER = 10;\n+\tconstexpr static idx_t FAST_TO_SLOW_THRESHOLD = 60;\n+\n+\t// If the table has less than 204800 rows, this is the percentage\n+\t// of values we save when serializing/returning a sample.\n+\tconstexpr static double SAVE_PERCENTAGE = 0.01;\n+\n \tReservoirSample(Allocator &allocator, idx_t sample_count, int64_t seed = 1);\n-\texplicit ReservoirSample(idx_t sample_count, int64_t seed = 1);\n+\texplicit ReservoirSample(idx_t sample_count, unique_ptr<ReservoirChunk> = nullptr);\n+\n+\t//! methods used to help with serializing and deserializing\n+\tvoid EvictOverBudgetSamples();\n+\tvoid ExpandSerializedSample();\n+\n+\tSamplingState GetSamplingState() const;\n+\n+\t//! Vacuum the Reservoir Sample so it throws away tuples that are not in the\n+\t//! reservoir weights or in the selection vector\n+\tvoid Vacuum();\n+\n+\t//! Transform To sample based on reservoir sampling paper\n+\tvoid ConvertToReservoirSample();\n+\n+\t//! Get the capactiy of the data chunk reserved for storing samples\n+\tidx_t GetReservoirChunkCapacity() const;\n \n+\t//! If for_serialization=true then the sample_chunk is not padded with extra spaces for\n+\t//! future sampling values\n+\tunique_ptr<BlockingSample> Copy() const override;\n+\n+\t//! create the first chunk called by AddToReservoir()\n+\tidx_t FillReservoir(DataChunk &chunk);\n \t//! Add a chunk of data to the sample\n \tvoid AddToReservoir(DataChunk &input) override;\n+\t//! Merge two Reservoir Samples. Other must be a reservoir sample\n+\tvoid Merge(unique_ptr<BlockingSample> other);\n+\n+\tvoid ShuffleSel(SelectionVector &sel, idx_t range, idx_t size) const;\n+\n+\t//! Update the sample by pushing new sample rows to the end of the sample_chunk.\n+\t//! The new sample rows are the tuples rows resulting from applying sel to other\n+\tvoid UpdateSampleAppend(DataChunk &this_, DataChunk &other, SelectionVector &other_sel, idx_t append_count) const;\n+\n+\tidx_t GetTuplesSeen() const;\n+\tidx_t NumSamplesCollected() const;\n+\tidx_t GetActiveSampleCount() const;\n+\tstatic bool ValidSampleType(const LogicalType &type);\n+\n+\t// get the chunk from Reservoir chunk\n+\tDataChunk &Chunk();\n \n \t//! Fetches a chunk from the sample. Note that this method is destructive and should only be used after the\n \t//! sample is completely built.\n+\t// unique_ptr<DataChunk> GetChunkAndDestroy() override;\n \tunique_ptr<DataChunk> GetChunk() override;\n+\tvoid Destroy() override;\n \tvoid Finalize() override;\n+\tvoid Verify();\n+\n+\tidx_t GetSampleCount();\n+\n+\t// map is [index in input chunk] -> [index in sample chunk]. Both are zero-based\n+\t// [index in sample chunk] is incremented by 1\n+\t// index in input chunk have random values, however, they are increasing.\n+\t// The base_reservoir_sampling gets updated however, so the indexes point to (sample_chunk_offset +\n+\t// index_in_sample_chunk) this data is used to make a selection vector to copy samples from the input chunk to the\n+\t// sample chunk\n+\t//! Get indexes from current sample that can be replaced.\n+\tSelectionVectorHelper GetReplacementIndexes(idx_t sample_chunk_offset, idx_t theoretical_chunk_length);\n+\n \tvoid Serialize(Serializer &serializer) const override;\n \tstatic unique_ptr<BlockingSample> Deserialize(Deserializer &deserializer);\n \n private:\n-\t//! Replace a single element of the input\n-\tvoid ReplaceElement(DataChunk &input, idx_t index_in_chunk, double with_weight = -1);\n-\tvoid InitializeReservoir(DataChunk &input);\n-\t//! Fills the reservoir up until sample_count entries, returns how many entries are still required\n-\tidx_t FillReservoir(DataChunk &input);\n+\t// when we serialize, we may have collected too many samples since we fill a standard vector size, then\n+\t// truncate if the table is still <=204800 values. The problem is, in our weights, we store indexes into\n+\t// the selection vector. If throw away values at selection vector index i = 5 , we need to update all indexes\n+\t// i > 5. Otherwise we will have indexes in the weights that are greater than the length of our sample.\n+\tvoid NormalizeWeights();\n+\n+\tSelectionVectorHelper GetReplacementIndexesSlow(const idx_t sample_chunk_offset, const idx_t chunk_length);\n+\tSelectionVectorHelper GetReplacementIndexesFast(const idx_t sample_chunk_offset, const idx_t chunk_length);\n+\tvoid SimpleMerge(ReservoirSample &other);\n+\tvoid WeightedMerge(ReservoirSample &other_sample);\n+\n+\t// Helper methods for Shrink().\n+\t// Shrink has different logic depending on if the Reservoir sample is still in\n+\t// \"Random\" mode or in \"reservoir\" mode. This function creates a new sample chunk\n+\t// to copy the old sample chunk into\n+\tunique_ptr<ReservoirChunk> CreateNewSampleChunk(vector<LogicalType> &types, idx_t size) const;\n+\n+\t// Get a vector where each index is a random int in the range 0, size.\n+\t// This is used to shuffle selection vector indexes\n+\tvector<uint32_t> GetRandomizedVector(uint32_t range, uint32_t size) const;\n \n-public:\n-\tAllocator &allocator;\n-\t//! The size of the reservoir sample.\n-\t//! when calculating percentages, it is set to reservoir_threshold * percentage\n-\t//! when explicit number used, sample_count = number\n \tidx_t sample_count;\n-\tbool reservoir_initialized;\n-\n-\t//! The current reservoir\n-\tunique_ptr<DataChunk> reservoir_data_chunk;\n+\tAllocator &allocator;\n \tunique_ptr<ReservoirChunk> reservoir_chunk;\n+\tbool stats_sample;\n+\tSelectionVector sel;\n+\tidx_t sel_size;\n };\n \n //! The reservoir sample sample_size class maintains a streaming sample of variable size\n@@ -155,15 +281,16 @@ class ReservoirSamplePercentage : public BlockingSample {\n public:\n \tstatic constexpr const SampleType TYPE = SampleType::RESERVOIR_PERCENTAGE_SAMPLE;\n \n-public:\n \tReservoirSamplePercentage(Allocator &allocator, double percentage, int64_t seed = -1);\n+\tReservoirSamplePercentage(double percentage, int64_t seed, idx_t reservoir_sample_size);\n \texplicit ReservoirSamplePercentage(double percentage, int64_t seed = -1);\n \n \t//! Add a chunk of data to the sample\n \tvoid AddToReservoir(DataChunk &input) override;\n \n-\t//! Fetches a chunk from the sample. Note that this method is destructive and should only be used after the\n-\t//! sample is completely built.\n+\tunique_ptr<BlockingSample> Copy() const override;\n+\n+\t//! Fetches a chunk from the sample. If destory = true this method is descructive\n \tunique_ptr<DataChunk> GetChunk() override;\n \tvoid Finalize() override;\n \n@@ -182,9 +309,11 @@ class ReservoirSamplePercentage : public BlockingSample {\n \n \t//! The set of finished samples of the reservoir sample\n \tvector<unique_ptr<ReservoirSample>> finished_samples;\n+\n \t//! The amount of tuples that have been processed so far (not put in the reservoir, just processed)\n \tidx_t current_count = 0;\n-\t//! Whether or not the stream is finalized. The stream is automatically finalized on the first call to GetChunk();\n+\t//! Whether or not the stream is finalized. The stream is automatically finalized on the first call to\n+\t//! GetChunkAndShrink();\n \tbool is_finalized;\n };\n \ndiff --git a/src/include/duckdb/function/table/system_functions.hpp b/src/include/duckdb/function/table/system_functions.hpp\nindex f74dc466e067..689b55201d0e 100644\n--- a/src/include/duckdb/function/table/system_functions.hpp\n+++ b/src/include/duckdb/function/table/system_functions.hpp\n@@ -107,6 +107,10 @@ struct DuckDBTablesFun {\n \tstatic void RegisterFunction(BuiltinFunctions &set);\n };\n \n+struct DuckDBTableSample {\n+\tstatic void RegisterFunction(BuiltinFunctions &set);\n+};\n+\n struct DuckDBTemporaryFilesFun {\n \tstatic void RegisterFunction(BuiltinFunctions &set);\n };\ndiff --git a/src/include/duckdb/main/client_data.hpp b/src/include/duckdb/main/client_data.hpp\nindex 777859755e1a..b3e326267897 100644\n--- a/src/include/duckdb/main/client_data.hpp\n+++ b/src/include/duckdb/main/client_data.hpp\n@@ -27,7 +27,7 @@ class QueryProfiler;\n class PreparedStatementData;\n class SchemaCatalogEntry;\n class HTTPLogger;\n-struct RandomEngine;\n+class RandomEngine;\n \n struct ClientData {\n \texplicit ClientData(ClientContext &context);\ndiff --git a/src/include/duckdb/storage/data_table.hpp b/src/include/duckdb/storage/data_table.hpp\nindex 10e126e896ce..39795ed1b907 100644\n--- a/src/include/duckdb/storage/data_table.hpp\n+++ b/src/include/duckdb/storage/data_table.hpp\n@@ -187,6 +187,9 @@ class DataTable {\n \n \t//! Get statistics of a physical column within the table\n \tunique_ptr<BaseStatistics> GetStatistics(ClientContext &context, column_t column_id);\n+\n+\t//! Get table sample\n+\tunique_ptr<BlockingSample> GetSample();\n \t//! Sets statistics of a physical column within the table\n \tvoid SetDistinct(column_t column_id, unique_ptr<DistinctStatistics> distinct_stats);\n \ndiff --git a/src/include/duckdb/storage/serialization/nodes.json b/src/include/duckdb/storage/serialization/nodes.json\nindex dc4665959f36..59791a85969b 100644\n--- a/src/include/duckdb/storage/serialization/nodes.json\n+++ b/src/include/duckdb/storage/serialization/nodes.json\n@@ -281,7 +281,7 @@\n         \"type\": \"unique_ptr<ReservoirChunk>\"\n       }\n     ],\n-    \"constructor\": [\"sample_count\"]\n+    \"constructor\": [\"sample_count\", \"reservoir_chunk\"]\n   },\n   {\n     \"class\": \"ReservoirSamplePercentage\",\n@@ -331,7 +331,6 @@\n     ],\n     \"pointer_type\": \"none\"\n   },\n-\n   {\n     \"class\": \"PivotColumnEntry\",\n     \"members\": [\ndiff --git a/src/include/duckdb/storage/table/row_group_collection.hpp b/src/include/duckdb/storage/table/row_group_collection.hpp\nindex 5c47dcd62179..19aa6452038c 100644\n--- a/src/include/duckdb/storage/table/row_group_collection.hpp\n+++ b/src/include/duckdb/storage/table/row_group_collection.hpp\n@@ -124,6 +124,7 @@ class RowGroupCollection {\n \n \tvoid CopyStats(TableStatistics &stats);\n \tunique_ptr<BaseStatistics> CopyStats(column_t column_id);\n+\tunique_ptr<BlockingSample> GetSample();\n \tvoid SetDistinct(column_t column_id, unique_ptr<DistinctStatistics> distinct_stats);\n \n \tAttachedDatabase &GetAttached();\ndiff --git a/src/include/duckdb/storage/table/table_statistics.hpp b/src/include/duckdb/storage/table/table_statistics.hpp\nindex 633d469463c2..628023dfd4dc 100644\n--- a/src/include/duckdb/storage/table/table_statistics.hpp\n+++ b/src/include/duckdb/storage/table/table_statistics.hpp\n@@ -48,6 +48,14 @@ class TableStatistics {\n \t//! Get a reference to the stats - this requires us to hold the lock.\n \t//! The reference can only be safely accessed while the lock is held\n \tColumnStatistics &GetStats(TableStatisticsLock &lock, idx_t i);\n+\t//! Get a reference to the table sample - this requires us to hold the lock.\n+\tBlockingSample &GetTableSampleRef(TableStatisticsLock &lock);\n+\t//! Take ownership of the sample, needed for merging. Requires the lock\n+\tunique_ptr<BlockingSample> GetTableSample(TableStatisticsLock &lock);\n+\tvoid SetTableSample(TableStatisticsLock &lock, unique_ptr<BlockingSample> sample);\n+\n+\tvoid DestroyTableSample(TableStatisticsLock &lock) const;\n+\tvoid AppendToTableSample(TableStatisticsLock &lock, unique_ptr<BlockingSample> sample);\n \n \tbool Empty();\n \n@@ -62,7 +70,6 @@ class TableStatistics {\n \t//! Column statistics\n \tvector<shared_ptr<ColumnStatistics>> column_stats;\n \t//! The table sample\n-\t//! Sample for table\n \tunique_ptr<BlockingSample> table_sample;\n };\n \ndiff --git a/src/storage/data_table.cpp b/src/storage/data_table.cpp\nindex d8a2a7c00597..81bf50100b13 100644\n--- a/src/storage/data_table.cpp\n+++ b/src/storage/data_table.cpp\n@@ -1517,6 +1517,10 @@ void DataTable::SetDistinct(column_t column_id, unique_ptr<DistinctStatistics> d\n \trow_groups->SetDistinct(column_id, std::move(distinct_stats));\n }\n \n+unique_ptr<BlockingSample> DataTable::GetSample() {\n+\treturn row_groups->GetSample();\n+}\n+\n //===--------------------------------------------------------------------===//\n // Checkpoint\n //===--------------------------------------------------------------------===//\n@@ -1533,8 +1537,8 @@ void DataTable::Checkpoint(TableDataWriter &writer, Serializer &serializer) {\n \tTableStatistics global_stats;\n \trow_groups->CopyStats(global_stats);\n \trow_groups->Checkpoint(writer, global_stats);\n-\n \t// The row group payload data has been written. Now write:\n+\t//   sample\n \t//   column stats\n \t//   row-group pointers\n \t//   table pointer\ndiff --git a/src/storage/serialization/serialize_nodes.cpp b/src/storage/serialization/serialize_nodes.cpp\nindex 440976e8ec61..2c7230383850 100644\n--- a/src/storage/serialization/serialize_nodes.cpp\n+++ b/src/storage/serialization/serialize_nodes.cpp\n@@ -596,8 +596,8 @@ void ReservoirSample::Serialize(Serializer &serializer) const {\n \n unique_ptr<BlockingSample> ReservoirSample::Deserialize(Deserializer &deserializer) {\n \tauto sample_count = deserializer.ReadPropertyWithDefault<idx_t>(200, \"sample_count\");\n-\tauto result = duckdb::unique_ptr<ReservoirSample>(new ReservoirSample(sample_count));\n-\tdeserializer.ReadPropertyWithDefault<unique_ptr<ReservoirChunk>>(201, \"reservoir_chunk\", result->reservoir_chunk);\n+\tauto reservoir_chunk = deserializer.ReadPropertyWithDefault<unique_ptr<ReservoirChunk>>(201, \"reservoir_chunk\");\n+\tauto result = duckdb::unique_ptr<ReservoirSample>(new ReservoirSample(sample_count, std::move(reservoir_chunk)));\n \treturn std::move(result);\n }\n \ndiff --git a/src/storage/table/row_group_collection.cpp b/src/storage/table/row_group_collection.cpp\nindex f2ff23fe6afd..90a7236ff447 100644\n--- a/src/storage/table/row_group_collection.cpp\n+++ b/src/storage/table/row_group_collection.cpp\n@@ -1,5 +1,4 @@\n #include \"duckdb/storage/table/row_group_collection.hpp\"\n-\n #include \"duckdb/common/serializer/binary_deserializer.hpp\"\n #include \"duckdb/execution/expression_executor.hpp\"\n #include \"duckdb/execution/index/bound_index.hpp\"\n@@ -397,11 +396,20 @@ bool RowGroupCollection::Append(DataChunk &chunk, TableAppendState &state) {\n \t\t}\n \t}\n \tstate.current_row += row_t(total_append_count);\n+\n \tauto local_stats_lock = state.stats.GetLock();\n+\n \tfor (idx_t col_idx = 0; col_idx < types.size(); col_idx++) {\n \t\tauto &column_stats = state.stats.GetStats(*local_stats_lock, col_idx);\n \t\tcolumn_stats.UpdateDistinctStatistics(chunk.data[col_idx], chunk.size(), state.hashes);\n \t}\n+\n+\tauto &table_sample = state.stats.GetTableSampleRef(*local_stats_lock);\n+\tif (!table_sample.destroyed) {\n+\t\tD_ASSERT(table_sample.type == SampleType::RESERVOIR_SAMPLE);\n+\t\ttable_sample.AddToReservoir(chunk);\n+\t}\n+\n \treturn new_row_group;\n }\n \n@@ -421,8 +429,8 @@ void RowGroupCollection::FinalizeAppend(TransactionData transaction, TableAppend\n \tstate.total_append_count = 0;\n \tstate.start_row_group = nullptr;\n \n-\tauto global_stats_lock = stats.GetLock();\n \tauto local_stats_lock = state.stats.GetLock();\n+\tauto global_stats_lock = stats.GetLock();\n \tfor (idx_t col_idx = 0; col_idx < types.size(); col_idx++) {\n \t\tauto &global_stats = stats.GetStats(*global_stats_lock, col_idx);\n \t\tif (!global_stats.HasDistinctStats()) {\n@@ -435,6 +443,22 @@ void RowGroupCollection::FinalizeAppend(TransactionData transaction, TableAppend\n \t\tglobal_stats.DistinctStats().Merge(local_stats.DistinctStats());\n \t}\n \n+\tauto local_sample = state.stats.GetTableSample(*local_stats_lock);\n+\tauto global_sample = stats.GetTableSample(*global_stats_lock);\n+\n+\tif (local_sample && global_sample) {\n+\t\tD_ASSERT(global_sample->type == SampleType::RESERVOIR_SAMPLE);\n+\t\tauto &reservoir_sample = global_sample->Cast<ReservoirSample>();\n+\t\treservoir_sample.Merge(std::move(local_sample));\n+\t\t// initialize the thread local sample again\n+\t\tauto new_local_sample = make_uniq<ReservoirSample>(reservoir_sample.GetSampleCount());\n+\t\tstate.stats.SetTableSample(*local_stats_lock, std::move(new_local_sample));\n+\t\tstats.SetTableSample(*global_stats_lock, std::move(global_sample));\n+\t} else {\n+\t\tstate.stats.SetTableSample(*local_stats_lock, std::move(local_sample));\n+\t\tstats.SetTableSample(*global_stats_lock, std::move(global_sample));\n+\t}\n+\n \tVerify();\n }\n \n@@ -582,6 +606,11 @@ idx_t RowGroupCollection::Delete(TransactionData transaction, DataTable &table,\n \t\t}\n \t\tdelete_count += row_group->Delete(transaction, table, ids + start, pos - start);\n \t} while (pos < count);\n+\n+\t// When deleting destroy the sample.\n+\tauto stats_guard = stats.GetLock();\n+\tstats.DestroyTableSample(*stats_guard);\n+\n \treturn delete_count;\n }\n \n@@ -619,6 +648,9 @@ void RowGroupCollection::Update(TransactionData transaction, row_t *ids, const v\n \t\t\tstats.MergeStats(*l, column_id.index, *row_group->GetStatistics(column_id.index));\n \t\t}\n \t} while (pos < updates.size());\n+\t// on update destroy the sample\n+\tauto stats_guard = stats.GetLock();\n+\tstats.DestroyTableSample(*stats_guard);\n }\n \n void RowGroupCollection::RemoveFromIndexes(TableIndexList &indexes, Vector &row_identifiers, idx_t count) {\n@@ -1102,6 +1134,9 @@ shared_ptr<RowGroupCollection> RowGroupCollection::AddColumn(ClientContext &cont\n \n \t\tresult->row_groups->AppendSegment(std::move(new_row_group));\n \t}\n+\t// When adding a column destroy the sample\n+\tstats.DestroyTableSample(*lock);\n+\n \treturn result;\n }\n \n@@ -1114,6 +1149,9 @@ shared_ptr<RowGroupCollection> RowGroupCollection::RemoveColumn(idx_t col_idx) {\n \t                                                  total_rows.load(), row_group_size);\n \tresult->stats.InitializeRemoveColumn(stats, col_idx);\n \n+\tauto result_lock = result->stats.GetLock();\n+\tresult->stats.DestroyTableSample(*result_lock);\n+\n \tfor (auto &current_row_group : row_groups->Segments()) {\n \t\tauto new_row_group = current_row_group.RemoveColumn(*result, col_idx);\n \t\tresult->row_groups->AppendSegment(std::move(new_row_group));\n@@ -1160,7 +1198,6 @@ shared_ptr<RowGroupCollection> RowGroupCollection::AlterType(ClientContext &cont\n \t\tnew_row_group->MergeIntoStatistics(changed_idx, changed_stats.Statistics());\n \t\tresult->row_groups->AppendSegment(std::move(new_row_group));\n \t}\n-\n \treturn result;\n }\n \n@@ -1207,7 +1244,7 @@ void RowGroupCollection::VerifyNewConstraint(DataTable &parent, const BoundConst\n \n //===--------------------------------------------------------------------===//\n // Statistics\n-//===--------------------------------------------------------------------===//\n+//===---------------------------------------------------------------r-----===//\n void RowGroupCollection::CopyStats(TableStatistics &other_stats) {\n \tstats.CopyStats(other_stats);\n }\n@@ -1216,6 +1253,18 @@ unique_ptr<BaseStatistics> RowGroupCollection::CopyStats(column_t column_id) {\n \treturn stats.CopyStats(column_id);\n }\n \n+unique_ptr<BlockingSample> RowGroupCollection::GetSample() {\n+\tauto lock = stats.GetLock();\n+\tauto &sample = stats.GetTableSampleRef(*lock);\n+\tif (!sample.destroyed) {\n+\t\tD_ASSERT(sample.type == SampleType::RESERVOIR_SAMPLE);\n+\t\tauto ret = sample.Copy();\n+\t\tret->Cast<ReservoirSample>().EvictOverBudgetSamples();\n+\t\treturn ret;\n+\t}\n+\treturn nullptr;\n+}\n+\n void RowGroupCollection::SetDistinct(column_t column_id, unique_ptr<DistinctStatistics> distinct_stats) {\n \tD_ASSERT(column_id != COLUMN_IDENTIFIER_ROW_ID);\n \tauto stats_lock = stats.GetLock();\ndiff --git a/src/storage/table/table_statistics.cpp b/src/storage/table/table_statistics.cpp\nindex b51c445d0dc2..e56f98440022 100644\n--- a/src/storage/table/table_statistics.cpp\n+++ b/src/storage/table/table_statistics.cpp\n@@ -1,16 +1,23 @@\n #include \"duckdb/storage/table/table_statistics.hpp\"\n-#include \"duckdb/storage/table/persistent_table_data.hpp\"\n-#include \"duckdb/common/serializer/serializer.hpp\"\n+\n #include \"duckdb/common/serializer/deserializer.hpp\"\n+#include \"duckdb/common/serializer/serializer.hpp\"\n #include \"duckdb/execution/reservoir_sample.hpp\"\n+#include \"duckdb/storage/table/persistent_table_data.hpp\"\n \n namespace duckdb {\n \n void TableStatistics::Initialize(const vector<LogicalType> &types, PersistentTableData &data) {\n \tD_ASSERT(Empty());\n+\tD_ASSERT(!table_sample);\n \n \tstats_lock = make_shared_ptr<mutex>();\n \tcolumn_stats = std::move(data.table_stats.column_stats);\n+\tif (data.table_stats.table_sample) {\n+\t\ttable_sample = std::move(data.table_stats.table_sample);\n+\t} else {\n+\t\ttable_sample = make_uniq<ReservoirSample>(static_cast<idx_t>(FIXED_SAMPLE_SIZE));\n+\t}\n \tif (column_stats.size() != types.size()) { // LCOV_EXCL_START\n \t\tthrow IOException(\"Table statistics column count is not aligned with table column count. Corrupt file?\");\n \t} // LCOV_EXCL_STOP\n@@ -18,8 +25,10 @@ void TableStatistics::Initialize(const vector<LogicalType> &types, PersistentTab\n \n void TableStatistics::InitializeEmpty(const vector<LogicalType> &types) {\n \tD_ASSERT(Empty());\n+\tD_ASSERT(!table_sample);\n \n \tstats_lock = make_shared_ptr<mutex>();\n+\ttable_sample = make_uniq<ReservoirSample>(static_cast<idx_t>(FIXED_SAMPLE_SIZE));\n \tfor (auto &type : types) {\n \t\tcolumn_stats.push_back(ColumnStatistics::CreateEmptyStats(type));\n \t}\n@@ -35,6 +44,12 @@ void TableStatistics::InitializeAddColumn(TableStatistics &parent, const Logical\n \t\tcolumn_stats.push_back(parent.column_stats[i]);\n \t}\n \tcolumn_stats.push_back(ColumnStatistics::CreateEmptyStats(new_column_type));\n+\tif (parent.table_sample) {\n+\t\ttable_sample = std::move(parent.table_sample);\n+\t}\n+\tif (table_sample) {\n+\t\ttable_sample->Destroy();\n+\t}\n }\n \n void TableStatistics::InitializeRemoveColumn(TableStatistics &parent, idx_t removed_column) {\n@@ -48,6 +63,12 @@ void TableStatistics::InitializeRemoveColumn(TableStatistics &parent, idx_t remo\n \t\t\tcolumn_stats.push_back(parent.column_stats[i]);\n \t\t}\n \t}\n+\tif (parent.table_sample) {\n+\t\ttable_sample = std::move(parent.table_sample);\n+\t}\n+\tif (table_sample) {\n+\t\ttable_sample->Destroy();\n+\t}\n }\n \n void TableStatistics::InitializeAlterType(TableStatistics &parent, idx_t changed_idx, const LogicalType &new_type) {\n@@ -63,6 +84,12 @@ void TableStatistics::InitializeAlterType(TableStatistics &parent, idx_t changed\n \t\t\tcolumn_stats.push_back(parent.column_stats[i]);\n \t\t}\n \t}\n+\tif (parent.table_sample) {\n+\t\ttable_sample = std::move(parent.table_sample);\n+\t}\n+\tif (table_sample) {\n+\t\ttable_sample->Destroy();\n+\t}\n }\n \n void TableStatistics::InitializeAddConstraint(TableStatistics &parent) {\n@@ -79,6 +106,21 @@ void TableStatistics::InitializeAddConstraint(TableStatistics &parent) {\n void TableStatistics::MergeStats(TableStatistics &other) {\n \tauto l = GetLock();\n \tD_ASSERT(column_stats.size() == other.column_stats.size());\n+\tif (table_sample) {\n+\t\tif (other.table_sample) {\n+\t\t\tD_ASSERT(table_sample->type == SampleType::RESERVOIR_SAMPLE);\n+\t\t\tauto &this_reservoir = table_sample->Cast<ReservoirSample>();\n+\t\t\tD_ASSERT(other.table_sample->type == SampleType::RESERVOIR_SAMPLE);\n+\t\t\tthis_reservoir.Merge(std::move(other.table_sample));\n+\t\t}\n+\t\t// if no other.table sample, do nothig\n+\t} else {\n+\t\tif (other.table_sample) {\n+\t\t\tauto &other_reservoir = other.table_sample->Cast<ReservoirSample>();\n+\t\t\tauto other_table_sample_copy = other_reservoir.Copy();\n+\t\t\ttable_sample = std::move(other_table_sample_copy);\n+\t\t}\n+\t}\n \tfor (idx_t i = 0; i < column_stats.size(); i++) {\n \t\tif (column_stats[i]) {\n \t\t\tD_ASSERT(other.column_stats[i]);\n@@ -100,6 +142,25 @@ ColumnStatistics &TableStatistics::GetStats(TableStatisticsLock &lock, idx_t i)\n \treturn *column_stats[i];\n }\n \n+BlockingSample &TableStatistics::GetTableSampleRef(TableStatisticsLock &lock) {\n+\tD_ASSERT(table_sample);\n+\treturn *table_sample;\n+}\n+\n+unique_ptr<BlockingSample> TableStatistics::GetTableSample(TableStatisticsLock &lock) {\n+\treturn std::move(table_sample);\n+}\n+\n+void TableStatistics::SetTableSample(TableStatisticsLock &lock, unique_ptr<BlockingSample> sample) {\n+\ttable_sample = std::move(sample);\n+}\n+\n+void TableStatistics::DestroyTableSample(TableStatisticsLock &lock) const {\n+\tif (table_sample) {\n+\t\ttable_sample->Destroy();\n+\t}\n+}\n+\n unique_ptr<BaseStatistics> TableStatistics::CopyStats(idx_t i) {\n \tlock_guard<mutex> l(*stats_lock);\n \tauto result = column_stats[i]->Statistics().Copy();\n@@ -120,11 +181,25 @@ void TableStatistics::CopyStats(TableStatisticsLock &lock, TableStatistics &othe\n \tfor (auto &stats : column_stats) {\n \t\tother.column_stats.push_back(stats->Copy());\n \t}\n+\n+\tif (table_sample) {\n+\t\tD_ASSERT(table_sample->type == SampleType::RESERVOIR_SAMPLE);\n+\t\tauto &res = table_sample->Cast<ReservoirSample>();\n+\t\tother.table_sample = res.Copy();\n+\t}\n }\n \n void TableStatistics::Serialize(Serializer &serializer) const {\n \tserializer.WriteProperty(100, \"column_stats\", column_stats);\n-\tserializer.WritePropertyWithDefault<unique_ptr<BlockingSample>>(101, \"table_sample\", table_sample, nullptr);\n+\tunique_ptr<BlockingSample> to_serialize = nullptr;\n+\tif (table_sample) {\n+\t\tD_ASSERT(table_sample->type == SampleType::RESERVOIR_SAMPLE);\n+\t\tauto &reservoir_sample = table_sample->Cast<ReservoirSample>();\n+\t\tto_serialize = unique_ptr_cast<BlockingSample, ReservoirSample>(reservoir_sample.Copy());\n+\t\tauto &res_serialize = to_serialize->Cast<ReservoirSample>();\n+\t\tres_serialize.EvictOverBudgetSamples();\n+\t}\n+\tserializer.WritePropertyWithDefault<unique_ptr<BlockingSample>>(101, \"table_sample\", to_serialize, nullptr);\n }\n \n void TableStatistics::Deserialize(Deserializer &deserializer, ColumnList &columns) {\n@@ -142,8 +217,19 @@ void TableStatistics::Deserialize(Deserializer &deserializer, ColumnList &column\n \n \t\tdeserializer.Unset<LogicalType>();\n \t});\n-\ttable_sample =\n-\t    deserializer.ReadPropertyWithExplicitDefault<unique_ptr<BlockingSample>>(101, \"table_sample\", nullptr);\n+\ttable_sample = deserializer.ReadPropertyWithDefault<unique_ptr<BlockingSample>>(101, \"table_sample\");\n+\tif (table_sample) {\n+\t\tD_ASSERT(table_sample->type == SampleType::RESERVOIR_SAMPLE);\n+#ifdef DEBUG\n+\t\tif (table_sample) {\n+\t\t\tauto &reservoir_sample = table_sample->Cast<ReservoirSample>();\n+\t\t\treservoir_sample.Verify();\n+\t\t}\n+#endif\n+\t} else {\n+\t\ttable_sample = make_uniq<ReservoirSample>(static_cast<idx_t>(FIXED_SAMPLE_SIZE));\n+\t\ttable_sample->Destroy();\n+\t}\n }\n \n unique_ptr<TableStatisticsLock> TableStatistics::GetLock() {\n",
  "test_patch": "diff --git a/test/sql/sample/can_sample_from_ingested_files.test b/test/sql/sample/can_sample_from_ingested_files.test\nnew file mode 100644\nindex 000000000000..23c2ef50fd52\n--- /dev/null\n+++ b/test/sql/sample/can_sample_from_ingested_files.test\n@@ -0,0 +1,44 @@\n+# name: test/sql/sample/can_sample_from_ingested_files.test\n+# description: Test reservoir sample crash on large data sets\n+# group: [sample]\n+\n+require parquet\n+\n+statement ok\n+PRAGMA enable_verification;\n+\n+statement ok\n+create table all_types as select * exclude(small_enum, medium_enum, large_enum, \"union\", bit) from test_all_types();\n+\n+statement ok\n+copy all_types to '__TEST_DIR__/sample_all_types.csv' (FORMAT CSV);\n+\n+statement ok\n+Create table all_types_csv_1 as select * from read_csv_auto('__TEST_DIR__/sample_all_types.csv');\n+\n+statement ok\n+Create table all_types_csv_2 as select * from read_csv_auto('__TEST_DIR__/sample_all_types.csv');\n+\n+query T nosort result_1\n+select * from all_types_csv_1;\n+\n+query T nosort result_1\n+select * from all_types_csv_2;\n+\n+\n+statement ok\n+copy (SELECT * from all_types) to '__TEST_DIR__/sample_all_types.parquet' (FORMAT PARQUET);\n+\n+# test parquet\n+statement ok\n+Create table all_types_parquet_1 as select * from read_parquet('__TEST_DIR__/sample_all_types.parquet');\n+\n+statement ok\n+Create table all_types_parquet_2 as select * from read_parquet('__TEST_DIR__/sample_all_types.parquet');\n+\n+query T nosort result_parquet\n+select * from all_types_parquet_1;\n+\n+query T nosort result_paruet\n+select * from all_types_parquet_2;\n+\ndiff --git a/test/sql/sample/reservoir_testing_percentage.test b/test/sql/sample/reservoir_testing_percentage.test\nnew file mode 100644\nindex 000000000000..19217d635461\n--- /dev/null\n+++ b/test/sql/sample/reservoir_testing_percentage.test\n@@ -0,0 +1,85 @@\n+# name: test/sql/sample/reservoir_testing_percentage.test\n+# description: Test SAMPLE keyword\n+# group: [sample]\n+\n+loop i 1 8\n+\n+statement ok\n+pragma threads=${i};\n+\n+statement ok\n+CREATE or replace TABLE t1 as select range a from range(1000);\n+\n+query I\n+SELECT count(*) from t1 using sample 0 percent (reservoir);\n+----\n+0\n+\n+query I\n+SELECT count(*) from t1 using sample 10 percent (reservoir);\n+----\n+100\n+\n+query I\n+SELECT count(*) from t1 using sample 20 percent (reservoir);\n+----\n+200\n+\n+query I\n+SELECT count(*) from t1 using sample 80 percent (reservoir);\n+----\n+800\n+\n+query I\n+SELECT count(*) from t1 using sample 100 percent (reservoir);\n+----\n+1000\n+\n+\n+statement ok\n+Insert into t1 select range a from range(9000);\n+\n+query I\n+select count(*) from t1 using sample 80 percent (reservoir);\n+----\n+8000\n+\n+statement ok\n+Insert into t1 select range a from range(90000);\n+\n+\n+statement ok\n+Insert into t1 select range a from range(900000);\n+\n+query I\n+select count(*) from t1 using sample 20 percent (reservoir);\n+----\n+200000\n+\n+query I\n+select count(*) from t1 using sample 30 percent (reservoir);\n+----\n+300000\n+\n+query I\n+select count(*) from t1 using sample 40 percent (reservoir);\n+----\n+400000\n+\n+query I\n+select count(*) from t1 using sample 50 percent (reservoir);\n+----\n+500000\n+\n+\n+query I\n+select count(*) from t1 using sample 60 percent (reservoir);\n+----\n+600000\n+\n+query I\n+select count(*) from t1 using sample 70 percent (reservoir);\n+----\n+700000\n+\n+endloop\ndiff --git a/test/sql/sample/reservoir_testing_rows_value.test_slow b/test/sql/sample/reservoir_testing_rows_value.test_slow\nindex a4e355a69044..3355225b5b8e 100644\n--- a/test/sql/sample/reservoir_testing_rows_value.test_slow\n+++ b/test/sql/sample/reservoir_testing_rows_value.test_slow\n@@ -8,7 +8,7 @@ statement ok\n pragma threads=${i};\n \n statement ok\n-CREATE or replace TABLE t1 as select range a, [1, a, 2] b, a::VARCHAR || 'ducktastic' c, get_current_timestamp() c from range(1000);\n+CREATE or replace TABLE t1 as select range a from range(1000);\n \n query I\n SELECT count(*) from t1 using sample 0;\n@@ -30,13 +30,15 @@ SELECT count(*) from t1 using sample 800;\n ----\n 800\n \n+\n query I\n SELECT count(*) from t1 using sample 1000;\n ----\n 1000\n \n+\n statement ok\n-CREATE or replace TABLE t1 as select range a, [1, a, 2] b, a::VARCHAR || 'ducktastic' c, get_current_timestamp() c from range(10000);\n+create or replace table t1 as select * from range(10000);\n \n query I\n select count(*) from t1 using sample 1000;\n@@ -59,7 +61,7 @@ select count(*) from t1 using sample 8000;\n 8000\n \n statement ok\n-CREATE or replace TABLE t1 as select range a, [1, a, 2] b, a::VARCHAR || 'ducktastic' c, get_current_timestamp() c from range(1000000);\n+Create or replace table t1  as select range a from range(1000000);\n \n query I\n select count(*) from t1 using sample 200000;\ndiff --git a/test/sql/sample/same_seed_same_sample.test_slow b/test/sql/sample/same_seed_same_sample.test_slow\nindex c191fcb75e30..1970cf1c4d75 100644\n--- a/test/sql/sample/same_seed_same_sample.test_slow\n+++ b/test/sql/sample/same_seed_same_sample.test_slow\n@@ -23,15 +23,15 @@ SELECT * from t1 using sample reservoir(100) repeatable (1) order by a;\n \n endloop\n \n-# testing a table with greater cardinality than the standard vector size\n+# testing a table with greater cardinality than the standard vector size, and greater than a row group size.\n \n statement ok\n CREATE or replace TABLE t1 as select range a, [1, a, 2] b, a::VARCHAR || 'ducktastic' c, get_current_timestamp() c from range(100000);\n \n-loop i 1 8\n+# samples are only equal when threads = 1\n \n statement ok\n-pragma threads=${i};\n+set threads=1;\n \n query III nosort result_2\n SELECT * from t1 using sample reservoir(6000) repeatable (1) order by a;\n@@ -42,8 +42,6 @@ query III nosort result_2\n SELECT * from t1 using sample reservoir(6000) repeatable (1) order by a;\n ----\n \n-endloop\n-\n statement ok\n CREATE or replace TABLE t1 as select range a, [1, a, 2] b, a::VARCHAR || 'ducktastic' c, get_current_timestamp() c from range(1000000);\n \n@@ -73,5 +71,3 @@ select count(*) < 10 from (select * from sample1 intersect select * from sample2\n ----\n true\n \n-\n-\ndiff --git a/test/sql/sample/table_samples/basic_sample_tests.test b/test/sql/sample/table_samples/basic_sample_tests.test\nnew file mode 100644\nindex 000000000000..2107062f32c3\n--- /dev/null\n+++ b/test/sql/sample/table_samples/basic_sample_tests.test\n@@ -0,0 +1,81 @@\n+# name: test/sql/sample/table_samples/basic_sample_tests.test\n+# group: [table_samples]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+load __TEST_DIR__/test_samples_basic.db\n+\n+query I\n+select count(*) from range(100000) using sample (10000);\n+----\n+10000\n+\n+query I\n+select count(*) from range(100) using sample (10);\n+----\n+10\n+\n+query I\n+select count(*) from range(205000) using sample (10000);\n+----\n+10000\n+\n+statement ok\n+create table t1 as select range a from range(204800);\n+\n+statement ok\n+select * from duckdb_table_sample('t1');\n+\n+statement ok\n+create or replace table t1 as select range a from range(1000);\n+\n+query II\n+select avg(a) > 200, avg(a) < 800 from duckdb_table_sample('t1');\n+----\n+true\ttrue\n+\n+statement ok\n+create or replace table t1 as select range a from range(204800);\n+\n+# average is not skewed\n+query II\n+select avg(a) > (0.2*204800), avg(a) < (0.8*204800) from duckdb_table_sample('t1');\n+----\n+true\ttrue\n+\n+# about half the samples are below 102400 and half above\n+query I\n+select count(*) < 1060 from duckdb_table_sample('t1') where a < 102400;\n+----\n+true\n+\n+query I\n+select count(*) < 1060 from duckdb_table_sample('t1') where a > 102400;\n+----\n+true\n+\n+query I\n+select count(*) from t1 using sample (200000);\n+----\n+200000\n+\n+statement ok\n+create or replace table materialized_range as select * from range(100);\n+\n+statement ok\n+create or replace table integers_1 as (select range b from materialized_range);\n+\n+query I\n+select count(b) from duckdb_table_sample('integers_1') where b in (select * from materialized_range);\n+----\n+1\n+\n+# sample exists after restart\n+restart\n+\n+query I\n+select count(b) from duckdb_table_sample('integers_1') where b in (select * from materialized_range);\n+----\n+1\n+\ndiff --git a/test/sql/sample/table_samples/sample_stores_rows_from_later_on.test_slow b/test/sql/sample/table_samples/sample_stores_rows_from_later_on.test_slow\nnew file mode 100644\nindex 000000000000..c88b9cd74a67\n--- /dev/null\n+++ b/test/sql/sample/table_samples/sample_stores_rows_from_later_on.test_slow\n@@ -0,0 +1,39 @@\n+# name: test/sql/sample/table_samples/sample_stores_rows_from_later_on.test_slow\n+# description: Test sampling of larger relations\n+# group: [table_samples]\n+\n+require vector_size 2048\n+\n+require noforcestorage\n+\n+load __TEST_DIR__/test_sample_conversion.db\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+create table t1 as select 1 a from range(200000);\n+\n+loop i 1 4805\n+\n+statement ok\n+INSERT INTO t1 VALUES(${i} + 1);\n+\n+restart\n+\n+endloop\n+\n+query I\n+select count(*) from duckdb_table_sample('t1');\n+----\n+2048\n+\n+query I\n+select count(*) from duckdb_table_sample('t1') where a > 1;\n+----\n+48\n+\n+query I\n+select count(*) from (select (floor(range/200000))::INT a from range(204800) using sample reservoir (1%)) t1 where a >= 1;\n+----\n+48\ndiff --git a/test/sql/sample/table_samples/table_sample_converts_to_block_sample.test b/test/sql/sample/table_samples/table_sample_converts_to_block_sample.test\nnew file mode 100644\nindex 000000000000..0ffca61f31ef\n--- /dev/null\n+++ b/test/sql/sample/table_samples/table_sample_converts_to_block_sample.test\n@@ -0,0 +1,56 @@\n+# name: test/sql/sample/table_samples/table_sample_converts_to_block_sample.test\n+# description: Test sampling of larger relations\n+# group: [table_samples]\n+\n+require vector_size 2048\n+\n+require noforcestorage\n+\n+# table samples first collect only 1% of the table, until the table has a cardinality of 2048.\n+# then the sample stays at a fixed 2048 values.\n+\n+load __TEST_DIR__/test_sample_converts_after_load.db\n+\n+statement ok\n+create table materialized_range as select 1 a from range(102400);\n+\n+# only 1% of 102400\n+query I\n+select count(*) from duckdb_table_sample('materialized_range');\n+----\n+1024\n+\n+restart\n+\n+statement ok\n+insert into materialized_range select 2 a from range(102400);\n+\n+# collect another 1% of 102400\n+query I\n+select count(*) from duckdb_table_sample('materialized_range');\n+----\n+2048\n+\n+query II\n+select a, count(*) from duckdb_table_sample('materialized_range') group by all order by a;\n+----\n+1\t1024\n+2\t1024\n+\n+# insert another\n+statement ok\n+insert into materialized_range select 3 a from range(102400);\n+\n+# sample remains at 2048 values\n+query I\n+select count(*) from duckdb_table_sample('materialized_range');\n+----\n+2048\n+\n+# 2048 / 3 = 682. so each value should have at least >650\n+query II\n+select a, count(*) > 650 from duckdb_table_sample('materialized_range') group by all order by a;\n+----\n+1\t1\n+2\t1\n+3\t1\n\\ No newline at end of file\ndiff --git a/test/sql/sample/table_samples/table_sample_is_stored.test_slow b/test/sql/sample/table_samples/table_sample_is_stored.test_slow\nnew file mode 100644\nindex 000000000000..d2d5e8001245\n--- /dev/null\n+++ b/test/sql/sample/table_samples/table_sample_is_stored.test_slow\n@@ -0,0 +1,143 @@\n+# name: test/sql/sample/table_samples/table_sample_is_stored.test_slow\n+# description: Test sampling of larger relations\n+# group: [table_samples]\n+\n+require vector_size 2048\n+\n+require noforcestorage\n+\n+load __TEST_DIR__/test_samples.db\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+create table materialized_range as select * from range(5000000);\n+\n+statement ok\n+create table integers_1 as (select (range + 5) a, range b, get_current_time() as time from materialized_range);\n+\n+query II nosort result_1\n+select a::INT, b from duckdb_table_sample('integers_1') order by all;\n+----\n+\n+statement ok\n+create table integers_2 as (select (range + 5) a, range b, get_current_time() as time from materialized_range);\n+\n+## samples should be the same given the same table and the same contents.\n+query II nosort result_1\n+select a::INT, b from duckdb_table_sample('integers_2') order by all;\n+----\n+\n+statement ok\n+create or replace table integers_1 as (select (range + 5) a, range b from materialized_range);\n+\n+statement ok\n+create or replace table integers_2 as (select (range + 5) a, range b from materialized_range);\n+\n+# sample only has values in the table it was sampled from\n+query I\n+select count(*) from (select b from duckdb_table_sample('integers_1') intersect (select b from integers_1));\n+----\n+2048\n+\n+query I\n+select count(*) from (select b from duckdb_table_sample('integers_2') intersect (select b from integers_2));\n+----\n+2048\n+\n+# sample exists after restart\n+restart\n+\n+query I\n+select count(*) from duckdb_table_sample('integers_1');\n+----\n+2048\n+\n+query I\n+select count(*) from duckdb_table_sample('integers_2');\n+----\n+2048\n+\n+\n+query II\n+select floor(b / 1000000) as interval, count(*) as frequency from duckdb_table_sample('integers_1') group by interval order by all;\n+----\n+0.0\t453\n+1.0\t407\n+2.0\t406\n+3.0\t403\n+4.0\t379\n+\n+\n+# adding another interval should subtract an equal number from the rest of the intervals\n+statement ok\n+insert into integers_1 (select (range + 5) a, range b from range(5000000,6000000));\n+\n+query II\n+select floor(b / 1000000) as interval, count(*) as frequency from duckdb_table_sample('integers_1') group by interval order by all;\n+----\n+0.0\t374\n+1.0\t334\n+2.0\t332\n+3.0\t335\n+4.0\t313\n+5.0\t360\n+\n+# If double the table count is appended, around half the sample should account for the new values.\n+statement ok\n+insert into integers_1 (select -1, -1 from range(6000000));\n+\n+query I\n+select count(*) from integers_1;\n+----\n+12000000\n+\n+# about half of the samples should have the pair '-1', 1.\n+query I\n+select count(*) from duckdb_table_sample('integers_1') where a = -1 and b = -1;\n+----\n+1022\n+\n+\n+restart\n+\n+# updated sample is also newly serialized\n+query I\n+select count(*) from duckdb_table_sample('integers_1') where a = -1 and b = -1;\n+----\n+1022\n+\n+# create a view on top of the sample\n+statement ok\n+create view sample_view as select * from duckdb_table_sample('integers_1');\n+\n+# update the sample\n+statement ok\n+insert into integers_1 (select -2, -2 from range(6000000));\n+\n+# 2048 / 3 = 682 (706 is good)\n+query I\n+select count(*) from sample_view where a = -2 and b = -2;\n+----\n+624\n+\n+restart\n+\n+query I\n+select count(*) from sample_view where a = -2 and b = -2;\n+----\n+624\n+\n+# currently have 18_000_000 values in the table.\n+# to try and get 1 value in the sample, we should add\n+# 18000000 / 2048 = 8789 values to see 1\n+\n+statement ok\n+insert into integers_1 (select -3, -3 from range(7000));\n+\n+# 1 value makes it\n+query I\n+select count(*) from sample_view where a = -3 and b = -3;\n+----\n+1\n\\ No newline at end of file\ndiff --git a/test/sql/sample/table_samples/test_sample_is_destroyed_on_updates.test b/test/sql/sample/table_samples/test_sample_is_destroyed_on_updates.test\nnew file mode 100644\nindex 000000000000..410d9004e87c\n--- /dev/null\n+++ b/test/sql/sample/table_samples/test_sample_is_destroyed_on_updates.test\n@@ -0,0 +1,122 @@\n+# name: test/sql/sample/table_samples/test_sample_is_destroyed_on_updates.test\n+# description: Test sampling of larger relations\n+# group: [table_samples]\n+\n+require vector_size 2048\n+\n+load __TEST_DIR__/test_sample_is_destroyed_on_update.db\n+\n+statement ok\n+create or replace table integers_1 as select range a, range+1 b  from range(102400);\n+\n+# no sample collected yet. There are only 5\n+query I\n+select count(*) from duckdb_table_sample('integers_1') order by all;\n+----\n+1024\n+\n+statement ok\n+delete from integers_1 where a = 3;\n+\n+# sample no longer exists\n+query I\n+select count(*) from duckdb_table_sample('integers_1') order by all;\n+----\n+0\n+\n+statement ok\n+create or replace table integers_1 as select range a, range+1 b  from range(102400);\n+\n+query I\n+select count(*) from duckdb_table_sample('integers_1');\n+----\n+1024\n+\n+statement ok\n+update integers_1 set a = 5 where a = 1;\n+\n+query II\n+select * from duckdb_table_sample('integers_1');\n+----\n+\n+# test adding columns destroys the sample.\n+statement ok\n+create or replace table integers_1 as select range a, range+1 b  from range(204800);\n+\n+query I\n+select count(*) from duckdb_table_sample('integers_1');\n+----\n+2048\n+\n+statement ok\n+Alter table integers_1 add column c DOUBLE;\n+\n+query III\n+select * from duckdb_table_sample('integers_1');\n+----\n+\n+\n+# test altering types destroys the sample\n+statement ok\n+create or replace table integers_1 as select range a, range+1 b  from range(102400);\n+\n+\n+# don't have enough smaples yet.\n+query I\n+select count(*) from duckdb_table_sample('integers_1');\n+----\n+1024\n+\n+statement ok\n+Alter table integers_1 alter b TYPE VARCHAR\n+\n+query II\n+select * from duckdb_table_sample('integers_1');\n+----\n+\n+# test dropping a columns\n+statement ok\n+create or replace table integers_1 as select range a, range+1 b  from range(102400);\n+\n+query I\n+select count(*) from duckdb_table_sample('integers_1');\n+----\n+1024\n+\n+statement ok\n+Alter table integers_1 drop b;\n+\n+query I\n+select * from duckdb_table_sample('integers_1');\n+----\n+\n+# test sample is destroyed after a restart\n+statement ok\n+create or replace table integers_1 as select range a, range+1 b  from range(500);\n+\n+query I\n+select count(*) from duckdb_table_sample('integers_1');\n+----\n+5\n+\n+statement ok\n+Alter table integers_1 drop b;\n+\n+# sample is destroyed\n+query I\n+select * from duckdb_table_sample('integers_1');\n+----\n+\n+restart\n+\n+statement ok\n+insert into integers_1 select range a from range(500);\n+\n+# sample is still destroyed\n+query I\n+select * from duckdb_table_sample('integers_1');\n+----\n+\n+\n+\n+\ndiff --git a/test/sql/sample/table_samples/test_sample_types.test b/test/sql/sample/table_samples/test_sample_types.test\nnew file mode 100644\nindex 000000000000..22834abaa4f6\n--- /dev/null\n+++ b/test/sql/sample/table_samples/test_sample_types.test\n@@ -0,0 +1,78 @@\n+# name: test/sql/sample/table_samples/test_sample_types.test\n+# description: Test sampling of larger relations\n+# group: [table_samples]\n+\n+# test valid sampling types (for now only integral types)\n+\n+statement ok\n+pragma enable_verification;\n+\n+statement ok\n+create table string_samples as select range::Varchar a from range(204800);\n+\n+query I\n+select count(*) from duckdb_table_sample('string_samples') where a is NULL;\n+----\n+2048\n+\n+statement ok\n+create table struct_samples as select {'key1': 'quack-a-lack', 'key2': range} a from range(204800);\n+\n+query I\n+select count(*) from duckdb_table_sample('struct_samples') where a is null;\n+----\n+2048\n+\n+statement ok\n+create table blob_samples as select '\\xAA\\xAB\\xAC'::BLOB a from range(204800);\n+\n+query I\n+select count(*) from duckdb_table_sample('blob_samples') where a is NULL;\n+----\n+2048\n+\n+statement ok\n+create table integral_samples as select range::BIGINT a, range::DOUBLE b, range::FLOAT c, range::HUGEINT d, INTERVAL 1 YEAR e from range(204800);\n+\n+query I\n+select count(*) from duckdb_table_sample('integral_samples') where a NOT null;\n+----\n+2048\n+\n+query I\n+select count(*) from duckdb_table_sample('integral_samples') where b NOT null;\n+----\n+2048\n+\n+query I\n+select count(*) from duckdb_table_sample('integral_samples') where c NOT null;\n+----\n+2048\n+\n+query I\n+select count(*) from duckdb_table_sample('integral_samples') where d NOT null;\n+----\n+2048\n+\n+query I\n+select count(*) from duckdb_table_sample('integral_samples') where e IS null;\n+----\n+2048\n+\n+statement ok\n+CREATE or replace TABLE t1 as select range a, [1, a, 2] b, a::VARCHAR || 'ducktastic' c, get_current_time() d from range(1000000);\n+\n+query I\n+select count(*) from duckdb_table_sample('t1') where b is null;\n+----\n+2048\n+\n+query I\n+select count(*) from duckdb_table_sample('t1') where c is null;\n+----\n+2048\n+\n+query I\n+select count(*) from duckdb_table_sample('t1') where d is null;\n+----\n+2048\n\\ No newline at end of file\ndiff --git a/test/sql/sample/table_samples/test_table_sample_errors.test b/test/sql/sample/table_samples/test_table_sample_errors.test\nnew file mode 100644\nindex 000000000000..ca30dcfbf65c\n--- /dev/null\n+++ b/test/sql/sample/table_samples/test_table_sample_errors.test\n@@ -0,0 +1,19 @@\n+# name: test/sql/sample/table_samples/test_table_sample_errors.test\n+# description: test table sampl[e errors\n+# group: [table_samples]\n+\n+statement ok\n+create table t1 as select range a from range(204800);\n+\n+statement ok\n+create view v1 as select * from t1;\n+\n+statement error\n+select * from duckdb_table_sample('v1');\n+----\n+<REGEX>:.*Invalid Catalog type.*\n+\n+statement error\n+select * from duckdb_table_sample('a');\n+----\n+<REGEX>:.*Catalog Error:.*Table.*does not exist.*\n\\ No newline at end of file\ndiff --git a/test/sql/sample/test_sample.test_slow b/test/sql/sample/test_sample.test_slow\nindex 57a41394ea41..b3134e97c6ca 100644\n--- a/test/sql/sample/test_sample.test_slow\n+++ b/test/sql/sample/test_sample.test_slow\n@@ -75,7 +75,6 @@ SELECT COUNT(*) FROM range(2000000) USING SAMPLE 2\n ----\n 2\n \n-\n # test sample with multiple columns\n # we insert the same data in the entire column\n statement ok\n@@ -218,10 +217,10 @@ select * from integers using sample 10000%;\n query I\n select i from integers using sample (1 rows) repeatable (0);\n ----\n-152\n+79\n \n query I\n-select i from integers using sample reservoir(1%) repeatable (0);\n+select i from integers using sample reservoir(1%) repeatable (0) order by i;\n ----\n-51\n-78\n+87\n+164\ndiff --git a/test/sql/storage/checkpointed_self_append.test b/test/sql/storage/checkpointed_self_append.test\nindex cd1f0e806e7c..dc3162709d65 100644\n--- a/test/sql/storage/checkpointed_self_append.test\n+++ b/test/sql/storage/checkpointed_self_append.test\n@@ -4,7 +4,6 @@\n \n require skip_reload\n \n-\n # load the DB from disk\n load __TEST_DIR__/checkpointed_self_append.db\n \ndiff --git a/test/sql/storage/reclaim_space/reclaim_space_drop_column_overflow_strings.test_slow b/test/sql/storage/reclaim_space/reclaim_space_drop_column_overflow_strings.test_slow\nindex 2b6ae0d0fcac..b1b013597f1d 100644\n--- a/test/sql/storage/reclaim_space/reclaim_space_drop_column_overflow_strings.test_slow\n+++ b/test/sql/storage/reclaim_space/reclaim_space_drop_column_overflow_strings.test_slow\n@@ -90,4 +90,4 @@ SELECT AVG(STRLEN(s)), MIN(STRLEN(S)), MAX(STRLEN(S)), SUM(STRLEN(S)), MIN(S[1])\n ----\n 296.955\t0\t5000\t44543527\t(empty)\tX\n \n-endloop\n+endloop\n\\ No newline at end of file\ndiff --git a/test/sql/upsert/test_big_insert.test b/test/sql/upsert/test_big_insert.test\nindex 2de05a7d27b1..1bba1ba5fd17 100644\n--- a/test/sql/upsert/test_big_insert.test\n+++ b/test/sql/upsert/test_big_insert.test\n@@ -2,8 +2,6 @@\n # description: Test insert into statements\n # group: [upsert]\n \n-# TODO: remove this, behavior should be consistent at all vector sizes\n-\n statement ok\n pragma enable_verification;\n \n",
  "problem_statement": "Multiple pivots prior to create call have wrong/unexpected behavior\n### What happens?\r\n\r\nwhen you run multiple pivot statements before turning their relations into tables, all but the last get clobbered with nonsense.\r\nI'm running linux but the same problem occurs on windows.\r\n\r\ndetails in reproduction code.\r\n\r\n### To Reproduce\r\n\r\n```python\r\n# demonstrate bug\r\nimport numpy as np\r\nimport pandas as pd\r\nimport duckdb\r\nN = 1000\r\ndf = pd.DataFrame({\"a\":np.random.choice([\"u\", \"v\", \"w\"], N, True),\r\n                   \"b\":np.random.choice([\"x\", \"y\", \"z\"], N, True),\r\n                   \"c\":np.random.randn(N),})\r\nconn = duckdb.connect()\r\n\r\nddf = conn.from_df(df)\r\nddf.create(\"input_data\")\r\n\r\nxx = conn.query(\"pivot input_data on a using max(c) group by b;\")\r\nyy = conn.query(\"pivot input_data on b using max(c) group by a;\")\r\nxx.create(\"xx\")\r\nyy.create(\"yy\")\r\nprint(conn.query(\"SHOW ALL TABLES;\").df())\r\nxx_table = conn.table(\"xx\")\r\nyy_table = conn.table(\"yy\")\r\nassert set(yy.columns)==set(yy_table.columns) # Succeeds.\r\nassert set(xx.columns)==set(xx_table.columns)  # Fails. \r\n# xx_table shows up with cols [b, x, y, z] and all nans in the x,y,z cols\r\n# The first col should indeed by b, but the next 3 should be u,v,w and they shouldn't have all nans\r\n```\r\n\r\n### OS:\r\n\r\nUbuntu 22.04.5 LTS x86_64\r\n\r\n### DuckDB Version:\r\n\r\n1.1.1\r\n\r\n### DuckDB Client:\r\n\r\npython\r\n\r\n### Hardware:\r\n\r\n_No response_\r\n\r\n### Full Name:\r\n\r\nMichael Hankin\r\n\r\n### Affiliation:\r\n\r\nMothball Labs\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have not tested with any build\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\nScoping problem with function argument\n### What happens?\r\n\r\nSomehow, `z` seems to disappear:\r\n```sql\r\ncreate or replace function demo(n, z) as table (\r\n  select list_transform(range(0,n), x -> z) as row\r\n);  \r\n\r\nfrom demo(3,0);\r\n```\r\n```\r\nBinder Error: Referenced column \"z\" not found in FROM clause!\r\n```\r\n\r\n\r\n\r\n### To Reproduce\r\n\r\n```sql\r\ncreate or replace function demo(n, z) as table (\r\n  select list_transform(range(0,n), x -> z) as row\r\n);  \r\n\r\nfrom demo(3,0);\r\n```\r\n\r\n### OS:\r\n\r\nmacos\r\n\r\n### DuckDB Version:\r\n\r\nv1.0, v1.1.3-dev38 \u2502\r\n\r\n### DuckDB Client:\r\n\r\nCLI\r\n\r\n### Hardware:\r\n\r\n_No response_\r\n\r\n### Full Name:\r\n\r\nPeter Koppstein\r\n\r\n### Affiliation:\r\n\r\nPrinceton University\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a source build\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nNot applicable - the reproduction does not require a data set\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\n[CSV] INTERNAL Error: Attempted to dereference unique_ptr that is NULL!\n### What happens?\n\nINTERNAL Error: Attempted to dereference unique_ptr that is NULL!\r\nThis error signals an assertion failure within DuckDB. This usually occurs due to unexpected conditions or errors in the program's logic.\r\nFor more information, see https://duckdb.org/docs/dev/internal_errors\r\n\r\n===csv2tsv.csv===\r\n```\r\na,\"\r\na,\"b\"\r\n```\n\n### To Reproduce\n\n```\r\nselect version();\r\ncreate or replace table t as\r\nfrom read_csv('csv2tsv.csv',\r\n   header=false,\r\n   quote='\"',\r\n   escape = '\"',\r\n   sep=',',\r\n   ignore_errors=true);\r\n```\r\n\n\n### OS:\n\nMacOS\n\n### DuckDB Version:\n\nv1.1.3-dev119\n\n### DuckDB Client:\n\nCLI\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nPeter Koppstein\n\n### Affiliation:\n\nPrinceton University\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a nightly build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\nMultiple pivots prior to create call have wrong/unexpected behavior\n### What happens?\r\n\r\nwhen you run multiple pivot statements before turning their relations into tables, all but the last get clobbered with nonsense.\r\nI'm running linux but the same problem occurs on windows.\r\n\r\ndetails in reproduction code.\r\n\r\n### To Reproduce\r\n\r\n```python\r\n# demonstrate bug\r\nimport numpy as np\r\nimport pandas as pd\r\nimport duckdb\r\nN = 1000\r\ndf = pd.DataFrame({\"a\":np.random.choice([\"u\", \"v\", \"w\"], N, True),\r\n                   \"b\":np.random.choice([\"x\", \"y\", \"z\"], N, True),\r\n                   \"c\":np.random.randn(N),})\r\nconn = duckdb.connect()\r\n\r\nddf = conn.from_df(df)\r\nddf.create(\"input_data\")\r\n\r\nxx = conn.query(\"pivot input_data on a using max(c) group by b;\")\r\nyy = conn.query(\"pivot input_data on b using max(c) group by a;\")\r\nxx.create(\"xx\")\r\nyy.create(\"yy\")\r\nprint(conn.query(\"SHOW ALL TABLES;\").df())\r\nxx_table = conn.table(\"xx\")\r\nyy_table = conn.table(\"yy\")\r\nassert set(yy.columns)==set(yy_table.columns) # Succeeds.\r\nassert set(xx.columns)==set(xx_table.columns)  # Fails. \r\n# xx_table shows up with cols [b, x, y, z] and all nans in the x,y,z cols\r\n# The first col should indeed by b, but the next 3 should be u,v,w and they shouldn't have all nans\r\n```\r\n\r\n### OS:\r\n\r\nUbuntu 22.04.5 LTS x86_64\r\n\r\n### DuckDB Version:\r\n\r\n1.1.1\r\n\r\n### DuckDB Client:\r\n\r\npython\r\n\r\n### Hardware:\r\n\r\n_No response_\r\n\r\n### Full Name:\r\n\r\nMichael Hankin\r\n\r\n### Affiliation:\r\n\r\nMothball Labs\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have not tested with any build\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\nINTERNAL error when using a macro to expand json[] array (`json_each`)\n### What happens?\r\n\r\nWhen I use the macro defined below with a `json[]` column, it errors out, but text and json types convert correctly.\r\n\r\nINTERNAL Error: Failed to cast logical operator to type - logical operator type mismatch\r\nThis error signals an assertion failure within DuckDB. This usually occurs due to unexpected conditions or errors in the program's logic.\r\nFor more information, see https://duckdb.org/docs/dev/internal_errors\r\n\r\n### To Reproduce\r\n\r\nFrom the cli:\r\n```sh\r\nduckdb -c \"\r\n  CREATE OR REPLACE MACRO json_each(input) AS\r\n    TABLE (\r\n      SELECT\r\n          CASE json_type (val::json)\r\n          WHEN 'ARRAY' THEN\r\n              unnest(RANGE (json_array_length(val::json)::bigint)) ::varchar\r\n          ELSE\r\n              unnest(json_keys (val::json))\r\n          END AS key,\r\n          json_extract (val::json, key) AS value\r\n      FROM (SELECT input as val)\r\n    );\r\n\r\n  FROM (SELECT '[1,2,3]' as message) CROSS JOIN json_each(message);\r\n  FROM (SELECT '[1,2,3]'::json as message) CROSS JOIN json_each(message);\r\n  FROM (SELECT '[1,2,3]'::json[] as message) CROSS JOIN json_each(message);\r\n  \"\r\n```\r\n\r\nOutput:\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 message \u2502   key   \u2502 value \u2502\r\n\u2502 varchar \u2502 varchar \u2502 json  \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 [1,2,3] \u2502 0       \u2502 1     \u2502\r\n\u2502 [1,2,3] \u2502 1       \u2502 2     \u2502\r\n\u2502 [1,2,3] \u2502 2       \u2502 3     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 message \u2502   key   \u2502 value \u2502\r\n\u2502  json   \u2502 varchar \u2502 json  \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 [1,2,3] \u2502 0       \u2502 1     \u2502\r\n\u2502 [1,2,3] \u2502 1       \u2502 2     \u2502\r\n\u2502 [1,2,3] \u2502 2       \u2502 3     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nINTERNAL Error: Failed to cast logical operator to type - logical operator type mismatch\r\nThis error signals an assertion failure within DuckDB. This usually occurs due to unexpected conditions or errors in the program's logic.\r\nFor more information, see https://duckdb.org/docs/dev/internal_errors\r\n```\r\n\r\n### OS:\r\n\r\nlinux\r\n\r\n### DuckDB Version:\r\n\r\n1.0.0, 1.1.0\r\n\r\n### DuckDB Client:\r\n\r\nCLI\r\n\r\n### Hardware:\r\n\r\n_No response_\r\n\r\n### Full Name:\r\n\r\nAshkan Kiani\r\n\r\n### Affiliation:\r\n\r\nJane Street\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a stable release\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\n",
  "hints_text": "I'd be more than happy to help with this issue and https://github.com/duckdb/duckdb/issues/14601 if someone with some experience here would be willing to hold my hand a bit.\nIf it helps, this seems to work when adding the var within an expression (0+z, x-z, etc)\r\n```sql\r\ncreate or replace function demo(n, z) as table (\r\n    select list_transform(range(0,n), x -> 0+z) as row\r\n  );\r\nfrom demo(3,1);\r\n```\r\n```text\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502    row    \u2502\r\n\u2502  int32[]  \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 [1, 1, 1] \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n@lcostantino - Good to know, thanks. I was able to find another workaround too (adding a FROM clause).  Hopefully the problem will be easy for the gurus to fix. \nIf it's help, in debug the assertion is triggered\r\n\r\n/duckdb/src/execution/operator/csv_scanner/sniffer/type_detection.cpp\r\n```\r\n\tif (!best_candidate) {\r\n\t\tDialectCandidates dialect_candidates(options.dialect_options.state_machine_options);\r\n\t\tauto error = CSVError::SniffingError(options, dialect_candidates.Print());\r\n\t\terror_handler->Error(error);\r\n\t}\r\n\t// Assert that it's all good at this point.\r\n\tD_ASSERT(best_candidate && !best_format_candidates.empty());\r\n```\r\n\r\nI'm not sure how the error is handled in the entire CSV scanner, but  after TypeDetection continues with TypeRefinement(), etc.. and given the best_candidate is nullptr ,it fails.\r\n\r\n\nI'd be more than happy to help with this issue and https://github.com/duckdb/duckdb/issues/14601 if someone with some experience here would be willing to hold my hand a bit.\nI noticed that trying to cast it within the macro call, i.e. `json_each(obj::json)` also fails. Only casting it in a subquery before calling the macro seems to not error out.\r\n\r\nIt seems there's a conversion issue with json[]\nThanks for the issue and the additional context. I could reproduce this very easily.",
  "created_at": "2024-11-20T12:13:31Z"
}