diff --git a/extension/core_functions/aggregate/holistic/reservoir_quantile.cpp b/extension/core_functions/aggregate/holistic/reservoir_quantile.cpp
index e99912762d43..8c332500d6e5 100644
--- a/extension/core_functions/aggregate/holistic/reservoir_quantile.cpp
+++ b/extension/core_functions/aggregate/holistic/reservoir_quantile.cpp
@@ -39,7 +39,7 @@ struct ReservoirQuantileState {
 	void FillReservoir(idx_t sample_size, T element) {
 		if (pos < sample_size) {
 			v[pos++] = element;
-			r_samp->InitializeReservoir(pos, len);
+			r_samp->InitializeReservoirWeights(pos, len);
 		} else {
 			D_ASSERT(r_samp->next_index_to_sample >= r_samp->num_entries_to_skip_b4_next_sample);
 			if (r_samp->next_index_to_sample == r_samp->num_entries_to_skip_b4_next_sample) {
diff --git a/src/catalog/catalog_entry/duck_table_entry.cpp b/src/catalog/catalog_entry/duck_table_entry.cpp
index e1d0c66a2ded..6b7f72c5bae6 100644
--- a/src/catalog/catalog_entry/duck_table_entry.cpp
+++ b/src/catalog/catalog_entry/duck_table_entry.cpp
@@ -136,6 +136,10 @@ unique_ptr<BaseStatistics> DuckTableEntry::GetStatistics(ClientContext &context,
 	return storage->GetStatistics(context, column.StorageOid());
 }
 
+unique_ptr<BlockingSample> DuckTableEntry::GetSample() {
+	return storage->GetSample();
+}
+
 unique_ptr<CatalogEntry> DuckTableEntry::AlterEntry(CatalogTransaction transaction, AlterInfo &info) {
 	if (transaction.HasContext()) {
 		return AlterEntry(transaction.GetContext(), info);
diff --git a/src/catalog/catalog_entry/table_catalog_entry.cpp b/src/catalog/catalog_entry/table_catalog_entry.cpp
index 9fc439e8d5d5..4be35415dc95 100644
--- a/src/catalog/catalog_entry/table_catalog_entry.cpp
+++ b/src/catalog/catalog_entry/table_catalog_entry.cpp
@@ -43,6 +43,10 @@ LogicalIndex TableCatalogEntry::GetColumnIndex(string &column_name, bool if_exis
 	return entry;
 }
 
+unique_ptr<BlockingSample> TableCatalogEntry::GetSample() {
+	return nullptr;
+}
+
 bool TableCatalogEntry::ColumnExists(const string &name) const {
 	return columns.ColumnExists(name);
 }
diff --git a/src/common/enum_util.cpp b/src/common/enum_util.cpp
index e803649e7339..e118848f2ae9 100644
--- a/src/common/enum_util.cpp
+++ b/src/common/enum_util.cpp
@@ -3124,6 +3124,24 @@ SampleType EnumUtil::FromString<SampleType>(const char *value) {
 	return static_cast<SampleType>(StringUtil::StringToEnum(GetSampleTypeValues(), 3, "SampleType", value));
 }
 
+const StringUtil::EnumStringLiteral *GetSamplingStateValues() {
+	static constexpr StringUtil::EnumStringLiteral values[] {
+		{ static_cast<uint32_t>(SamplingState::RANDOM), "RANDOM" },
+		{ static_cast<uint32_t>(SamplingState::RESERVOIR), "RESERVOIR" }
+	};
+	return values;
+}
+
+template<>
+const char* EnumUtil::ToChars<SamplingState>(SamplingState value) {
+	return StringUtil::EnumToString(GetSamplingStateValues(), 2, "SamplingState", static_cast<uint32_t>(value));
+}
+
+template<>
+SamplingState EnumUtil::FromString<SamplingState>(const char *value) {
+	return static_cast<SamplingState>(StringUtil::StringToEnum(GetSamplingStateValues(), 2, "SamplingState", value));
+}
+
 const StringUtil::EnumStringLiteral *GetScanTypeValues() {
 	static constexpr StringUtil::EnumStringLiteral values[] {
 		{ static_cast<uint32_t>(ScanType::TABLE), "TABLE" },
diff --git a/src/common/random_engine.cpp b/src/common/random_engine.cpp
index 704992f0dc9c..ebc0abd43e32 100644
--- a/src/common/random_engine.cpp
+++ b/src/common/random_engine.cpp
@@ -55,6 +55,10 @@ uint32_t RandomEngine::NextRandomInteger(uint32_t min, uint32_t max) {
 	return min + static_cast<uint32_t>(NextRandom() * double(max - min));
 }
 
+uint32_t RandomEngine::NextRandomInteger32(uint32_t min, uint32_t max) {
+	return min + static_cast<uint32_t>(NextRandom32() * double(max - min));
+}
+
 void RandomEngine::SetSeed(uint32_t seed) {
 	random_state->pcg.seed(seed);
 }
diff --git a/src/execution/CMakeLists.txt b/src/execution/CMakeLists.txt
index ef0d0a3eb50e..87c2a3bcc16e 100644
--- a/src/execution/CMakeLists.txt
+++ b/src/execution/CMakeLists.txt
@@ -3,6 +3,7 @@ add_subdirectory(nested_loop_join)
 add_subdirectory(operator)
 add_subdirectory(physical_plan)
 add_subdirectory(index)
+add_subdirectory(sample)
 
 add_library_unity(
   duckdb_execution
@@ -17,8 +18,7 @@ add_library_unity(
   perfect_aggregate_hashtable.cpp
   physical_operator.cpp
   physical_plan_generator.cpp
-  radix_partitioned_hashtable.cpp
-  reservoir_sample.cpp)
+  radix_partitioned_hashtable.cpp)
 set(ALL_OBJECT_FILES
     ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:duckdb_execution>
     PARENT_SCOPE)
diff --git a/src/execution/reservoir_sample.cpp b/src/execution/reservoir_sample.cpp
deleted file mode 100644
index 284e03faef09..000000000000
--- a/src/execution/reservoir_sample.cpp
+++ /dev/null
@@ -1,324 +0,0 @@
-#include "duckdb/execution/reservoir_sample.hpp"
-#include "duckdb/common/types/data_chunk.hpp"
-#include "duckdb/common/pair.hpp"
-
-namespace duckdb {
-
-void ReservoirChunk::Serialize(Serializer &serializer) const {
-	chunk.Serialize(serializer);
-}
-
-unique_ptr<ReservoirChunk> ReservoirChunk::Deserialize(Deserializer &deserializer) {
-	auto result = make_uniq<ReservoirChunk>();
-	result->chunk.Deserialize(deserializer);
-	return result;
-}
-
-ReservoirSample::ReservoirSample(Allocator &allocator, idx_t sample_count, int64_t seed)
-    : BlockingSample(seed), allocator(allocator), sample_count(sample_count), reservoir_initialized(false) {
-}
-
-ReservoirSample::ReservoirSample(idx_t sample_count, int64_t seed)
-    : ReservoirSample(Allocator::DefaultAllocator(), sample_count, seed) {
-}
-
-void ReservoirSample::AddToReservoir(DataChunk &input) {
-	if (sample_count == 0) {
-		// sample count is 0, means no samples were requested
-		return;
-	}
-	old_base_reservoir_sample.num_entries_seen_total += input.size();
-	// Input: A population V of n weighted items
-	// Output: A reservoir R with a size m
-	// 1: The first m items of V are inserted into R
-	// first we need to check if the reservoir already has "m" elements
-	if (!reservoir_data_chunk || reservoir_data_chunk->size() < sample_count) {
-		if (FillReservoir(input) == 0) {
-			// entire chunk was consumed by reservoir
-			return;
-		}
-	}
-	D_ASSERT(reservoir_data_chunk);
-	D_ASSERT(reservoir_data_chunk->size() == sample_count);
-	// Initialize the weights if they have not been already
-	if (old_base_reservoir_sample.reservoir_weights.empty()) {
-		old_base_reservoir_sample.InitializeReservoir(reservoir_data_chunk->size(), sample_count);
-	}
-	// find the position of next_index_to_sample relative to number of seen entries (num_entries_to_skip_b4_next_sample)
-	idx_t remaining = input.size();
-	idx_t base_offset = 0;
-	while (true) {
-		idx_t offset = old_base_reservoir_sample.next_index_to_sample -
-		               old_base_reservoir_sample.num_entries_to_skip_b4_next_sample;
-		if (offset >= remaining) {
-			// not in this chunk! increment current count and go to the next chunk
-			old_base_reservoir_sample.num_entries_to_skip_b4_next_sample += remaining;
-			return;
-		}
-		// in this chunk! replace the element
-		ReplaceElement(input, base_offset + offset);
-		// shift the chunk forward
-		remaining -= offset;
-		base_offset += offset;
-	}
-}
-
-unique_ptr<DataChunk> ReservoirSample::GetChunk() {
-	if (!reservoir_data_chunk || reservoir_data_chunk->size() == 0) {
-		return nullptr;
-	}
-	auto collected_sample_count = reservoir_data_chunk->size();
-	if (collected_sample_count > STANDARD_VECTOR_SIZE) {
-		// get from the back to avoid creating two selection vectors
-		// one to return the first STANDARD_VECTOR_SIZE
-		// another to replace the reservoir_data_chunk with the first STANDARD VECTOR SIZE missing
-		auto ret = make_uniq<DataChunk>();
-		auto samples_remaining = collected_sample_count - STANDARD_VECTOR_SIZE;
-		auto reservoir_types = reservoir_data_chunk->GetTypes();
-		SelectionVector sel(STANDARD_VECTOR_SIZE);
-		for (idx_t i = samples_remaining; i < collected_sample_count; i++) {
-			sel.set_index(i - samples_remaining, i);
-		}
-		ret->Initialize(allocator, reservoir_types);
-		ret->Slice(*reservoir_data_chunk, sel, STANDARD_VECTOR_SIZE);
-		ret->SetCardinality(STANDARD_VECTOR_SIZE);
-		// reduce capacity and cardinality of the sample data chunk
-		reservoir_data_chunk->SetCardinality(samples_remaining);
-		return ret;
-	}
-	return std::move(reservoir_data_chunk);
-}
-
-void ReservoirSample::ReplaceElement(DataChunk &input, idx_t index_in_chunk, double with_weight) {
-	// replace the entry in the reservoir
-	// 8. The item in R with the minimum key is replaced by item vi
-	D_ASSERT(input.ColumnCount() == reservoir_data_chunk->ColumnCount());
-	for (idx_t col_idx = 0; col_idx < input.ColumnCount(); col_idx++) {
-		reservoir_data_chunk->SetValue(col_idx, old_base_reservoir_sample.min_weighted_entry_index,
-		                               input.GetValue(col_idx, index_in_chunk));
-	}
-	old_base_reservoir_sample.ReplaceElement(with_weight);
-}
-
-void ReservoirSample::InitializeReservoir(DataChunk &input) {
-	reservoir_data_chunk = make_uniq<DataChunk>();
-	reservoir_data_chunk->Initialize(allocator, input.GetTypes(), sample_count);
-	for (idx_t col_idx = 0; col_idx < reservoir_data_chunk->ColumnCount(); col_idx++) {
-		FlatVector::Validity(reservoir_data_chunk->data[col_idx]).Initialize(sample_count);
-	}
-	reservoir_initialized = true;
-}
-
-idx_t ReservoirSample::FillReservoir(DataChunk &input) {
-	idx_t chunk_count = input.size();
-	input.Flatten();
-	auto num_added_samples = reservoir_data_chunk ? reservoir_data_chunk->size() : 0;
-	D_ASSERT(num_added_samples <= sample_count);
-
-	// required count is what we still need to add to the reservoir
-	idx_t required_count;
-	if (num_added_samples + chunk_count >= sample_count) {
-		// have to limit the count of the chunk
-		required_count = sample_count - num_added_samples;
-	} else {
-		// we copy the entire chunk
-		required_count = chunk_count;
-	}
-	input.SetCardinality(required_count);
-
-	// initialize the reservoir
-	if (!reservoir_initialized) {
-		InitializeReservoir(input);
-	}
-	reservoir_data_chunk->Append(input, false, nullptr, required_count);
-	old_base_reservoir_sample.InitializeReservoir(required_count, sample_count);
-
-	// check if there are still elements remaining in the Input data chunk that should be
-	// randomly sampled and potentially added. This happens if we are on a boundary
-	// for example, input.size() is 1024, but our sample size is 10
-	if (required_count == chunk_count) {
-		// we are done here
-		return 0;
-	}
-	// we still need to process a part of the chunk
-	// create a selection vector of the remaining elements
-	SelectionVector sel(STANDARD_VECTOR_SIZE);
-	for (idx_t i = required_count; i < chunk_count; i++) {
-		sel.set_index(i - required_count, i);
-	}
-	// slice the input vector and continue
-	input.Slice(sel, chunk_count - required_count);
-	return input.size();
-}
-
-void ReservoirSample::Finalize() {
-	return;
-}
-
-ReservoirSamplePercentage::ReservoirSamplePercentage(Allocator &allocator, double percentage, int64_t seed)
-    : BlockingSample(seed), allocator(allocator), sample_percentage(percentage / 100.0), current_count(0),
-      is_finalized(false) {
-	reservoir_sample_size = idx_t(sample_percentage * RESERVOIR_THRESHOLD);
-	current_sample = make_uniq<ReservoirSample>(allocator, reservoir_sample_size, random.NextRandomInteger());
-}
-
-ReservoirSamplePercentage::ReservoirSamplePercentage(double percentage, int64_t seed)
-    : ReservoirSamplePercentage(Allocator::DefaultAllocator(), percentage, seed) {
-}
-
-void ReservoirSamplePercentage::AddToReservoir(DataChunk &input) {
-	old_base_reservoir_sample.num_entries_seen_total += input.size();
-	if (current_count + input.size() > RESERVOIR_THRESHOLD) {
-		// we don't have enough space in our current reservoir
-		// first check what we still need to append to the current sample
-		idx_t append_to_current_sample_count = RESERVOIR_THRESHOLD - current_count;
-		idx_t append_to_next_sample = input.size() - append_to_current_sample_count;
-		if (append_to_current_sample_count > 0) {
-			// we have elements remaining, first add them to the current sample
-			if (append_to_next_sample > 0) {
-				// we need to also add to the next sample
-				DataChunk new_chunk;
-				new_chunk.InitializeEmpty(input.GetTypes());
-				new_chunk.Slice(input, *FlatVector::IncrementalSelectionVector(), append_to_current_sample_count);
-				new_chunk.Flatten();
-				current_sample->AddToReservoir(new_chunk);
-			} else {
-				input.Flatten();
-				input.SetCardinality(append_to_current_sample_count);
-				current_sample->AddToReservoir(input);
-			}
-		}
-		if (append_to_next_sample > 0) {
-			// slice the input for the remainder
-			SelectionVector sel(append_to_next_sample);
-			for (idx_t i = append_to_current_sample_count; i < append_to_next_sample + append_to_current_sample_count;
-			     i++) {
-				sel.set_index(i - append_to_current_sample_count, i);
-			}
-			input.Slice(sel, append_to_next_sample);
-		}
-		// now our first sample is filled: append it to the set of finished samples
-		finished_samples.push_back(std::move(current_sample));
-
-		// allocate a new sample, and potentially add the remainder of the current input to that sample
-		current_sample = make_uniq<ReservoirSample>(allocator, reservoir_sample_size, random.NextRandomInteger());
-		if (append_to_next_sample > 0) {
-			current_sample->AddToReservoir(input);
-		}
-		current_count = append_to_next_sample;
-	} else {
-		// we can just append to the current sample
-		current_count += input.size();
-		current_sample->AddToReservoir(input);
-	}
-}
-
-unique_ptr<DataChunk> ReservoirSamplePercentage::GetChunk() {
-	if (!is_finalized) {
-		Finalize();
-	}
-	while (!finished_samples.empty()) {
-		auto &front = finished_samples.front();
-		auto chunk = front->GetChunk();
-		if (chunk && chunk->size() > 0) {
-			return chunk;
-		}
-		// move to the next sample
-		finished_samples.erase(finished_samples.begin());
-	}
-	return nullptr;
-}
-
-void ReservoirSamplePercentage::Finalize() {
-	// need to finalize the current sample, if any
-	// we are finializing, so we are starting to return chunks. Our last chunk has
-	// sample_percentage * RESERVOIR_THRESHOLD entries that hold samples.
-	// if our current count is less than the sample_percentage * RESERVOIR_THRESHOLD
-	// then we have sampled too much for the current_sample and we need to redo the sample
-	// otherwise we can just push the current sample back
-	// Imagine sampling 70% of 100 rows (so 70 rows). We allocate sample_percentage * RESERVOIR_THRESHOLD
-	// -----------------------------------------
-	auto sampled_more_than_required =
-	    static_cast<double>(current_count) > sample_percentage * RESERVOIR_THRESHOLD || finished_samples.empty();
-	if (current_count > 0 && sampled_more_than_required) {
-		// create a new sample
-		auto new_sample_size = idx_t(round(sample_percentage * static_cast<double>(current_count)));
-		auto new_sample = make_uniq<ReservoirSample>(allocator, new_sample_size, random.NextRandomInteger());
-		while (true) {
-			auto chunk = current_sample->GetChunk();
-			if (!chunk || chunk->size() == 0) {
-				break;
-			}
-			new_sample->AddToReservoir(*chunk);
-		}
-		finished_samples.push_back(std::move(new_sample));
-	} else {
-		finished_samples.push_back(std::move(current_sample));
-	}
-	// when finalizing, current_sample is null. All samples are now in finished samples.
-	current_sample = nullptr;
-	is_finalized = true;
-}
-
-BaseReservoirSampling::BaseReservoirSampling(int64_t seed) : random(seed) {
-	next_index_to_sample = 0;
-	min_weight_threshold = 0;
-	min_weighted_entry_index = 0;
-	num_entries_to_skip_b4_next_sample = 0;
-	num_entries_seen_total = 0;
-}
-
-BaseReservoirSampling::BaseReservoirSampling() : BaseReservoirSampling(-1) {
-}
-
-void BaseReservoirSampling::InitializeReservoir(idx_t cur_size, idx_t sample_size) {
-	//! 1: The first m items of V are inserted into R
-	//! first we need to check if the reservoir already has "m" elements
-	if (cur_size == sample_size) {
-		//! 2. For each item vi ∈ R: Calculate a key ki = random(0, 1)
-		//! we then define the threshold to enter the reservoir T_w as the minimum key of R
-		//! we use a priority queue to extract the minimum key in O(1) time
-		for (idx_t i = 0; i < sample_size; i++) {
-			double k_i = random.NextRandom();
-			reservoir_weights.emplace(-k_i, i);
-		}
-		SetNextEntry();
-	}
-}
-
-void BaseReservoirSampling::SetNextEntry() {
-	//! 4. Let r = random(0, 1) and Xw = log(r) / log(T_w)
-	auto &min_key = reservoir_weights.top();
-	double t_w = -min_key.first;
-	double r = random.NextRandom();
-	double x_w = log(r) / log(t_w);
-	//! 5. From the current item vc skip items until item vi , such that:
-	//! 6. wc +wc+1 +···+wi−1 < Xw <= wc +wc+1 +···+wi−1 +wi
-	//! since all our weights are 1 (uniform sampling), we can just determine the amount of elements to skip
-	min_weight_threshold = t_w;
-	min_weighted_entry_index = min_key.second;
-	next_index_to_sample = MaxValue<idx_t>(1, idx_t(round(x_w)));
-	num_entries_to_skip_b4_next_sample = 0;
-}
-
-void BaseReservoirSampling::ReplaceElement(double with_weight) {
-	//! replace the entry in the reservoir
-	//! pop the minimum entry
-	reservoir_weights.pop();
-	//! now update the reservoir
-	//! 8. Let tw = Tw i , r2 = random(tw,1) and vi’s key: ki = (r2)1/wi
-	//! 9. The new threshold Tw is the new minimum key of R
-	//! we generate a random number between (min_weight_threshold, 1)
-	double r2 = random.NextRandom(min_weight_threshold, 1);
-
-	//! if we are merging two reservoir samples use the weight passed
-	if (with_weight >= 0) {
-		r2 = with_weight;
-	}
-	//! now we insert the new weight into the reservoir
-	reservoir_weights.emplace(-r2, min_weighted_entry_index);
-	//! we update the min entry with the new min entry in the reservoir
-	SetNextEntry();
-}
-
-} // namespace duckdb
diff --git a/src/execution/sample/CMakeLists.txt b/src/execution/sample/CMakeLists.txt
new file mode 100644
index 000000000000..6f69a205a0a9
--- /dev/null
+++ b/src/execution/sample/CMakeLists.txt
@@ -0,0 +1,5 @@
+add_library_unity(duckdb_sample OBJECT base_reservoir_sample.cpp
+                  reservoir_sample.cpp)
+set(ALL_OBJECT_FILES
+    ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:duckdb_sample>
+    PARENT_SCOPE)
diff --git a/src/execution/sample/base_reservoir_sample.cpp b/src/execution/sample/base_reservoir_sample.cpp
new file mode 100644
index 000000000000..0f0fcdf7a387
--- /dev/null
+++ b/src/execution/sample/base_reservoir_sample.cpp
@@ -0,0 +1,136 @@
+#include "duckdb/execution/reservoir_sample.hpp"
+#include <math.h>
+
+namespace duckdb {
+
+double BaseReservoirSampling::GetMinWeightFromTuplesSeen(idx_t rows_seen_total) {
+	// this function was obtained using https://mycurvefit.com. Inputting multiple x, y values into
+	// The
+	switch (rows_seen_total) {
+	case 0:
+		return 0;
+	case 1:
+		return 0.000161;
+	case 2:
+		return 0.530136;
+	case 3:
+		return 0.693454;
+	default: {
+		return (0.99 - 0.355 * std::exp(-0.07 * static_cast<double>(rows_seen_total)));
+	}
+	}
+}
+
+BaseReservoirSampling::BaseReservoirSampling(int64_t seed) : random(seed) {
+	next_index_to_sample = 0;
+	min_weight_threshold = 0;
+	min_weighted_entry_index = 0;
+	num_entries_to_skip_b4_next_sample = 0;
+	num_entries_seen_total = 0;
+}
+
+BaseReservoirSampling::BaseReservoirSampling() : BaseReservoirSampling(1) {
+}
+
+unique_ptr<BaseReservoirSampling> BaseReservoirSampling::Copy() {
+	auto ret = make_uniq<BaseReservoirSampling>(1);
+	ret->reservoir_weights = reservoir_weights;
+	ret->next_index_to_sample = next_index_to_sample;
+	ret->min_weight_threshold = min_weight_threshold;
+	ret->min_weighted_entry_index = min_weighted_entry_index;
+	ret->num_entries_to_skip_b4_next_sample = num_entries_to_skip_b4_next_sample;
+	ret->num_entries_seen_total = num_entries_seen_total;
+	return ret;
+}
+
+void BaseReservoirSampling::InitializeReservoirWeights(idx_t cur_size, idx_t sample_size) {
+	//! 1: The first m items of V are inserted into R
+	//! first we need to check if the reservoir already has "m" elements
+	//! 2. For each item vi ∈ R: Calculate a key ki = random(0, 1)
+	//! we then define the threshold to enter the reservoir T_w as the minimum key of R
+	//! we use a priority queue to extract the minimum key in O(1) time
+	if (cur_size == sample_size) {
+		//! 2. For each item vi ∈ R: Calculate a key ki = random(0, 1)
+		//! we then define the threshold to enter the reservoir T_w as the minimum key of R
+		//! we use a priority queue to extract the minimum key in O(1) time
+		for (idx_t i = 0; i < sample_size; i++) {
+			double k_i = random.NextRandom();
+			reservoir_weights.emplace(-k_i, i);
+		}
+		SetNextEntry();
+	}
+}
+
+void BaseReservoirSampling::SetNextEntry() {
+	D_ASSERT(!reservoir_weights.empty());
+	//! 4. Let r = random(0, 1) and Xw = log(r) / log(T_w)
+	auto &min_key = reservoir_weights.top();
+	double t_w = -min_key.first;
+	double r = random.NextRandom32();
+	double x_w = log(r) / log(t_w);
+	//! 5. From the current item vc skip items until item vi , such that:
+	//! 6. wc +wc+1 +···+wi−1 < Xw <= wc +wc+1 +···+wi−1 +wi
+	//! since all our weights are 1 (uniform sampling), we can just determine the amount of elements to skip
+	min_weight_threshold = t_w;
+	min_weighted_entry_index = min_key.second;
+	next_index_to_sample = MaxValue<idx_t>(1, idx_t(round(x_w)));
+	num_entries_to_skip_b4_next_sample = 0;
+}
+
+void BaseReservoirSampling::ReplaceElementWithIndex(idx_t entry_index, double with_weight, bool pop) {
+
+	if (pop) {
+		reservoir_weights.pop();
+	}
+	double r2 = with_weight;
+	//! now we insert the new weight into the reservoir
+	reservoir_weights.emplace(-r2, entry_index);
+	//! we update the min entry with the new min entry in the reservoir
+	SetNextEntry();
+}
+
+void BaseReservoirSampling::ReplaceElement(double with_weight) {
+	//! replace the entry in the reservoir
+	//! pop the minimum entry
+	reservoir_weights.pop();
+	//! now update the reservoir
+	//! 8. Let tw = Tw i , r2 = random(tw,1) and vi’s key: ki = (r2)1/wi
+	//! 9. The new threshold Tw is the new minimum key of R
+	//! we generate a random number between (min_weight_threshold, 1)
+	double r2 = random.NextRandom(min_weight_threshold, 1);
+
+	//! if we are merging two reservoir samples use the weight passed
+	if (with_weight >= 0) {
+		r2 = with_weight;
+	}
+	//! now we insert the new weight into the reservoir
+	reservoir_weights.emplace(-r2, min_weighted_entry_index);
+	//! we update the min entry with the new min entry in the reservoir
+	SetNextEntry();
+}
+
+void BaseReservoirSampling::UpdateMinWeightThreshold() {
+	if (!reservoir_weights.empty()) {
+		min_weight_threshold = -reservoir_weights.top().first;
+		min_weighted_entry_index = reservoir_weights.top().second;
+		return;
+	}
+	min_weight_threshold = 1;
+}
+
+void BaseReservoirSampling::FillWeights(SelectionVector &sel, idx_t &sel_size) {
+	if (!reservoir_weights.empty()) {
+		return;
+	}
+	D_ASSERT(reservoir_weights.empty());
+	auto num_entries_seen_normalized = num_entries_seen_total / FIXED_SAMPLE_SIZE;
+	auto min_weight = GetMinWeightFromTuplesSeen(num_entries_seen_normalized);
+	for (idx_t i = 0; i < sel_size; i++) {
+		auto weight = random.NextRandom(min_weight, 1);
+		reservoir_weights.emplace(-weight, i);
+	}
+	D_ASSERT(reservoir_weights.size() <= sel_size);
+	SetNextEntry();
+}
+
+} // namespace duckdb
diff --git a/src/execution/sample/reservoir_sample.cpp b/src/execution/sample/reservoir_sample.cpp
new file mode 100644
index 000000000000..eef10fca128b
--- /dev/null
+++ b/src/execution/sample/reservoir_sample.cpp
@@ -0,0 +1,930 @@
+#include "duckdb/execution/reservoir_sample.hpp"
+#include "duckdb/common/types/data_chunk.hpp"
+#include "duckdb/common/vector_operations/vector_operations.hpp"
+#include <unordered_set>
+
+namespace duckdb {
+
+std::pair<double, idx_t> BlockingSample::PopFromWeightQueue() {
+	D_ASSERT(base_reservoir_sample && !base_reservoir_sample->reservoir_weights.empty());
+	auto ret = base_reservoir_sample->reservoir_weights.top();
+	base_reservoir_sample->reservoir_weights.pop();
+
+	base_reservoir_sample->UpdateMinWeightThreshold();
+	D_ASSERT(base_reservoir_sample->min_weight_threshold > 0);
+	return ret;
+}
+
+double BlockingSample::GetMinWeightThreshold() {
+	return base_reservoir_sample->min_weight_threshold;
+}
+
+idx_t BlockingSample::GetPriorityQueueSize() {
+	return base_reservoir_sample->reservoir_weights.size();
+}
+
+void BlockingSample::Destroy() {
+	destroyed = true;
+}
+
+void ReservoirChunk::Serialize(Serializer &serializer) const {
+	chunk.Serialize(serializer);
+}
+
+unique_ptr<ReservoirChunk> ReservoirChunk::Deserialize(Deserializer &deserializer) {
+	auto result = make_uniq<ReservoirChunk>();
+	result->chunk.Deserialize(deserializer);
+	return result;
+}
+
+unique_ptr<ReservoirChunk> ReservoirChunk::Copy() const {
+	auto copy = make_uniq<ReservoirChunk>();
+	copy->chunk.Initialize(Allocator::DefaultAllocator(), chunk.GetTypes());
+
+	chunk.Copy(copy->chunk);
+	return copy;
+}
+
+ReservoirSample::ReservoirSample(idx_t sample_count, unique_ptr<ReservoirChunk> reservoir_chunk)
+    : ReservoirSample(Allocator::DefaultAllocator(), sample_count, 1) {
+	if (reservoir_chunk) {
+		this->reservoir_chunk = std::move(reservoir_chunk);
+		sel_size = this->reservoir_chunk->chunk.size();
+		sel = SelectionVector(0, sel_size);
+		ExpandSerializedSample();
+	}
+	stats_sample = true;
+}
+
+ReservoirSample::ReservoirSample(Allocator &allocator, idx_t sample_count, int64_t seed)
+    : BlockingSample(seed), sample_count(sample_count), allocator(allocator) {
+	base_reservoir_sample = make_uniq<BaseReservoirSampling>(seed);
+	type = SampleType::RESERVOIR_SAMPLE;
+	reservoir_chunk = nullptr;
+	stats_sample = false;
+	sel = SelectionVector(sample_count);
+	sel_size = 0;
+}
+
+idx_t ReservoirSample::GetSampleCount() {
+	return sample_count;
+}
+
+idx_t ReservoirSample::NumSamplesCollected() const {
+	if (!reservoir_chunk) {
+		return 0;
+	}
+	return reservoir_chunk->chunk.size();
+}
+
+SamplingState ReservoirSample::GetSamplingState() const {
+	if (base_reservoir_sample->reservoir_weights.empty()) {
+		return SamplingState::RANDOM;
+	}
+	return SamplingState::RESERVOIR;
+}
+
+idx_t ReservoirSample::GetActiveSampleCount() const {
+	switch (GetSamplingState()) {
+	case SamplingState::RANDOM:
+		return sel_size;
+	case SamplingState::RESERVOIR:
+		return base_reservoir_sample->reservoir_weights.size();
+	default:
+		throw InternalException("Sampling State is INVALID");
+	}
+}
+
+idx_t ReservoirSample::GetTuplesSeen() const {
+	return base_reservoir_sample->num_entries_seen_total;
+}
+
+DataChunk &ReservoirSample::Chunk() {
+	D_ASSERT(reservoir_chunk);
+	return reservoir_chunk->chunk;
+}
+
+unique_ptr<DataChunk> ReservoirSample::GetChunk() {
+	if (destroyed || !reservoir_chunk || Chunk().size() == 0) {
+		return nullptr;
+	}
+	// cannot destory internal samples.
+	auto ret = make_uniq<DataChunk>();
+
+	SelectionVector ret_sel(STANDARD_VECTOR_SIZE);
+	idx_t collected_samples = GetActiveSampleCount();
+
+	if (collected_samples == 0) {
+		return nullptr;
+	}
+
+	idx_t samples_remaining;
+	idx_t return_chunk_size;
+	if (collected_samples > STANDARD_VECTOR_SIZE) {
+		samples_remaining = collected_samples - STANDARD_VECTOR_SIZE;
+		return_chunk_size = STANDARD_VECTOR_SIZE;
+	} else {
+		samples_remaining = 0;
+		return_chunk_size = collected_samples;
+	}
+
+	for (idx_t i = samples_remaining; i < collected_samples; i++) {
+		// pop samples and reduce size of selection vector.
+		if (GetSamplingState() == SamplingState::RESERVOIR) {
+			auto top = PopFromWeightQueue();
+			ret_sel.set_index(i - samples_remaining, sel.get_index(top.second));
+		} else {
+			ret_sel.set_index(i - samples_remaining, sel.get_index(i));
+		}
+		sel_size -= 1;
+	}
+
+	auto reservoir_types = Chunk().GetTypes();
+
+	ret->Initialize(allocator, reservoir_types, STANDARD_VECTOR_SIZE);
+	ret->Slice(Chunk(), ret_sel, return_chunk_size);
+	ret->SetCardinality(return_chunk_size);
+	return ret;
+}
+
+unique_ptr<ReservoirChunk> ReservoirSample::CreateNewSampleChunk(vector<LogicalType> &types, idx_t size) const {
+	auto new_sample_chunk = make_uniq<ReservoirChunk>();
+	new_sample_chunk->chunk.Initialize(Allocator::DefaultAllocator(), types, size);
+
+	// set the NULL columns correctly
+	for (idx_t col_idx = 0; col_idx < types.size(); col_idx++) {
+		if (!ValidSampleType(types[col_idx]) && stats_sample) {
+			new_sample_chunk->chunk.data[col_idx].SetVectorType(VectorType::CONSTANT_VECTOR);
+			ConstantVector::SetNull(new_sample_chunk->chunk.data[col_idx], true);
+		}
+	}
+	return new_sample_chunk;
+}
+
+void ReservoirSample::Vacuum() {
+	Verify();
+	if (NumSamplesCollected() <= FIXED_SAMPLE_SIZE || !reservoir_chunk || destroyed) {
+		// sample is destroyed or too small to shrink
+		return;
+	}
+
+	auto ret = Copy();
+	auto ret_reservoir = duckdb::unique_ptr_cast<BlockingSample, ReservoirSample>(std::move(ret));
+	reservoir_chunk = std::move(ret_reservoir->reservoir_chunk);
+	sel = std::move(ret_reservoir->sel);
+	sel_size = ret_reservoir->sel_size;
+
+	Verify();
+	// We should only have one sample chunk now.
+	D_ASSERT(Chunk().size() > 0 && Chunk().size() <= sample_count);
+}
+
+unique_ptr<BlockingSample> ReservoirSample::Copy() const {
+
+	auto ret = make_uniq<ReservoirSample>(sample_count);
+	ret->stats_sample = stats_sample;
+
+	ret->base_reservoir_sample = base_reservoir_sample->Copy();
+	ret->destroyed = destroyed;
+
+	if (!reservoir_chunk || destroyed) {
+		return unique_ptr_cast<ReservoirSample, BlockingSample>(std::move(ret));
+	}
+
+	D_ASSERT(reservoir_chunk);
+
+	// create a new sample chunk to store new samples
+	auto types = reservoir_chunk->chunk.GetTypes();
+	// how many values should be copied
+	idx_t values_to_copy = MinValue<idx_t>(GetActiveSampleCount(), sample_count);
+
+	auto new_sample_chunk = CreateNewSampleChunk(types, GetReservoirChunkCapacity());
+
+	SelectionVector sel_copy(sel);
+
+	ret->reservoir_chunk = std::move(new_sample_chunk);
+	ret->UpdateSampleAppend(ret->reservoir_chunk->chunk, reservoir_chunk->chunk, sel_copy, values_to_copy);
+	ret->sel = SelectionVector(values_to_copy);
+	for (idx_t i = 0; i < values_to_copy; i++) {
+		ret->sel.set_index(i, i);
+	}
+	ret->sel_size = sel_size;
+	D_ASSERT(ret->reservoir_chunk->chunk.size() <= sample_count);
+	ret->Verify();
+	return unique_ptr_cast<ReservoirSample, BlockingSample>(std::move(ret));
+}
+
+void ReservoirSample::ConvertToReservoirSample() {
+	D_ASSERT(sel_size <= sample_count);
+	base_reservoir_sample->FillWeights(sel, sel_size);
+}
+
+vector<uint32_t> ReservoirSample::GetRandomizedVector(uint32_t range, uint32_t size) const {
+	vector<uint32_t> ret;
+	ret.reserve(range);
+	for (uint32_t i = 0; i < range; i++) {
+		ret.push_back(i);
+	}
+	if (size == FIXED_SAMPLE_SIZE) {
+		std::shuffle(ret.begin(), ret.end(), base_reservoir_sample->random);
+		return ret;
+	}
+	for (uint32_t i = 0; i < size; i++) {
+		uint32_t random_shuffle = base_reservoir_sample->random.NextRandomInteger32(i, range);
+		if (random_shuffle == i) {
+			// leave the value where it is
+			continue;
+		}
+		uint32_t tmp = ret[random_shuffle];
+		// basically replacing the tuple that was at index actual_sample_indexes[random_shuffle]
+		ret[random_shuffle] = ret[i];
+		ret[i] = tmp;
+	}
+	return ret;
+}
+
+void ReservoirSample::SimpleMerge(ReservoirSample &other) {
+	D_ASSERT(GetPriorityQueueSize() == 0);
+	D_ASSERT(other.GetPriorityQueueSize() == 0);
+	D_ASSERT(GetSamplingState() == SamplingState::RANDOM);
+	D_ASSERT(other.GetSamplingState() == SamplingState::RANDOM);
+
+	if (other.GetActiveSampleCount() == 0 && other.GetTuplesSeen() == 0) {
+		return;
+	}
+
+	if (GetActiveSampleCount() == 0 && GetTuplesSeen() == 0) {
+		sel = SelectionVector(other.sel);
+		sel_size = other.sel_size;
+		base_reservoir_sample->num_entries_seen_total = other.GetTuplesSeen();
+		return;
+	}
+
+	idx_t total_seen = GetTuplesSeen() + other.GetTuplesSeen();
+
+	auto weight_tuples_this = static_cast<double>(GetTuplesSeen()) / static_cast<double>(total_seen);
+	auto weight_tuples_other = static_cast<double>(other.GetTuplesSeen()) / static_cast<double>(total_seen);
+
+	// If weights don't add up to 1, most likely a simple merge occured and no new samples were added.
+	// if that is the case, add the missing weight to the lower weighted sample to adjust.
+	// this is to avoid cases where if you have a 20k row table and add another 20k rows row by row
+	// then eventually the missing weights will add up, and get you a more even distribution
+	if (weight_tuples_this + weight_tuples_other < 1) {
+		weight_tuples_other += 1 - (weight_tuples_other + weight_tuples_this);
+	}
+
+	idx_t keep_from_this = 0;
+	idx_t keep_from_other = 0;
+	D_ASSERT(stats_sample);
+	D_ASSERT(sample_count == FIXED_SAMPLE_SIZE);
+	D_ASSERT(sample_count == other.sample_count);
+	auto sample_count_double = static_cast<double>(sample_count);
+
+	if (weight_tuples_this > weight_tuples_other) {
+		keep_from_this = MinValue<idx_t>(static_cast<idx_t>(round(sample_count_double * weight_tuples_this)),
+		                                 GetActiveSampleCount());
+		keep_from_other = MinValue<idx_t>(sample_count - keep_from_this, other.GetActiveSampleCount());
+	} else {
+		keep_from_other = MinValue<idx_t>(static_cast<idx_t>(round(sample_count_double * weight_tuples_other)),
+		                                  other.GetActiveSampleCount());
+		keep_from_this = MinValue<idx_t>(sample_count - keep_from_other, GetActiveSampleCount());
+	}
+
+	D_ASSERT(keep_from_this <= GetActiveSampleCount());
+	D_ASSERT(keep_from_other <= other.GetActiveSampleCount());
+	D_ASSERT(keep_from_other + keep_from_this <= FIXED_SAMPLE_SIZE);
+	idx_t size_after_merge = MinValue<idx_t>(keep_from_other + keep_from_this, FIXED_SAMPLE_SIZE);
+
+	// Check if appending the other samples to this will go over the sample chunk size
+	if (reservoir_chunk->chunk.size() + keep_from_other > GetReservoirChunkCapacity()) {
+		Vacuum();
+	}
+
+	D_ASSERT(size_after_merge <= other.GetActiveSampleCount() + GetActiveSampleCount());
+	SelectionVector chunk_sel(keep_from_other);
+	auto offset = reservoir_chunk->chunk.size();
+	for (idx_t i = keep_from_this; i < size_after_merge; i++) {
+		if (i >= GetActiveSampleCount()) {
+			sel.set_index(GetActiveSampleCount(), offset);
+			sel_size += 1;
+		} else {
+			sel.set_index(i, offset);
+		}
+		chunk_sel.set_index(i - keep_from_this, other.sel.get_index(i - keep_from_this));
+		offset += 1;
+	}
+
+	D_ASSERT(GetActiveSampleCount() == size_after_merge);
+
+	// Copy the rows that make it to the sample from other and put them into this.
+	UpdateSampleAppend(reservoir_chunk->chunk, other.reservoir_chunk->chunk, chunk_sel, keep_from_other);
+	base_reservoir_sample->num_entries_seen_total += other.GetTuplesSeen();
+
+	// if THIS has too many samples now, we conver it to a slower sample.
+	if (GetTuplesSeen() >= FIXED_SAMPLE_SIZE * FAST_TO_SLOW_THRESHOLD) {
+		ConvertToReservoirSample();
+	}
+	Verify();
+}
+
+void ReservoirSample::WeightedMerge(ReservoirSample &other_sample) {
+	D_ASSERT(GetSamplingState() == SamplingState::RESERVOIR);
+	D_ASSERT(other_sample.GetSamplingState() == SamplingState::RESERVOIR);
+
+	// Find out how many samples we want to keep.
+	idx_t total_samples = GetActiveSampleCount() + other_sample.GetActiveSampleCount();
+	idx_t total_samples_seen =
+	    base_reservoir_sample->num_entries_seen_total + other_sample.base_reservoir_sample->num_entries_seen_total;
+	idx_t num_samples_to_keep = MinValue<idx_t>(total_samples, MinValue<idx_t>(sample_count, total_samples_seen));
+
+	D_ASSERT(GetActiveSampleCount() <= num_samples_to_keep);
+	D_ASSERT(total_samples <= FIXED_SAMPLE_SIZE * 2);
+
+	// pop from base base_reservoir weights until there are num_samples_to_keep left.
+	vector<idx_t> this_indexes_to_replace;
+	for (idx_t i = num_samples_to_keep; i < total_samples; i++) {
+		auto min_weight_this = base_reservoir_sample->min_weight_threshold;
+		auto min_weight_other = other_sample.base_reservoir_sample->min_weight_threshold;
+		// min weight threshol is always positive
+		if (min_weight_this > min_weight_other) {
+			// pop from other
+			other_sample.base_reservoir_sample->reservoir_weights.pop();
+			other_sample.base_reservoir_sample->UpdateMinWeightThreshold();
+		} else {
+			auto top_this = PopFromWeightQueue();
+			this_indexes_to_replace.push_back(top_this.second);
+			base_reservoir_sample->UpdateMinWeightThreshold();
+		}
+	}
+
+	D_ASSERT(other_sample.GetPriorityQueueSize() + GetPriorityQueueSize() <= FIXED_SAMPLE_SIZE);
+	D_ASSERT(other_sample.GetPriorityQueueSize() + GetPriorityQueueSize() == num_samples_to_keep);
+	D_ASSERT(other_sample.reservoir_chunk->chunk.GetTypes() == reservoir_chunk->chunk.GetTypes());
+
+	// Prepare a selection vector to copy data from the other sample chunk to this sample chunk
+	SelectionVector sel_other(other_sample.GetPriorityQueueSize());
+	D_ASSERT(GetPriorityQueueSize() <= num_samples_to_keep);
+	D_ASSERT(other_sample.GetPriorityQueueSize() >= this_indexes_to_replace.size());
+	idx_t chunk_offset = 0;
+
+	// Now push weights from other.base_reservoir_sample to this
+	// Depending on how many sample values "this" has, we either need to add to the selection vector
+	// Or replace values in "this'" selection vector
+	idx_t i = 0;
+	while (other_sample.GetPriorityQueueSize() > 0) {
+		auto other_top = other_sample.PopFromWeightQueue();
+		idx_t index_for_new_pair = chunk_offset + reservoir_chunk->chunk.size();
+
+		// update the sel used to copy values from other to this
+		sel_other.set_index(chunk_offset, other_top.second);
+		if (i < this_indexes_to_replace.size()) {
+			auto replacement_index = this_indexes_to_replace[i];
+			sel.set_index(replacement_index, index_for_new_pair);
+			other_top.second = replacement_index;
+		} else {
+			sel.set_index(sel_size, index_for_new_pair);
+			other_top.second = sel_size;
+			sel_size += 1;
+		}
+
+		// make sure that the sample indexes are (this.sample_chunk.size() + chunk_offfset)
+		base_reservoir_sample->reservoir_weights.push(other_top);
+		chunk_offset += 1;
+		i += 1;
+	}
+
+	D_ASSERT(GetPriorityQueueSize() == num_samples_to_keep);
+
+	base_reservoir_sample->UpdateMinWeightThreshold();
+	D_ASSERT(base_reservoir_sample->min_weight_threshold > 0);
+	base_reservoir_sample->num_entries_seen_total = GetTuplesSeen() + other_sample.GetTuplesSeen();
+
+	UpdateSampleAppend(reservoir_chunk->chunk, other_sample.reservoir_chunk->chunk, sel_other, chunk_offset);
+	if (reservoir_chunk->chunk.size() > FIXED_SAMPLE_SIZE * (FIXED_SAMPLE_SIZE_MULTIPLIER - 3)) {
+		Vacuum();
+	}
+
+	Verify();
+}
+
+void ReservoirSample::Merge(unique_ptr<BlockingSample> other) {
+	if (destroyed || other->destroyed) {
+		Destroy();
+		return;
+	}
+
+	D_ASSERT(other->type == SampleType::RESERVOIR_SAMPLE);
+	auto &other_sample = other->Cast<ReservoirSample>();
+
+	// if the other sample has not collected anything yet return
+	if (!other_sample.reservoir_chunk || other_sample.reservoir_chunk->chunk.size() == 0) {
+		return;
+	}
+
+	// this has not collected samples, take over the other
+	if (!reservoir_chunk || reservoir_chunk->chunk.size() == 0) {
+		base_reservoir_sample = std::move(other->base_reservoir_sample);
+		reservoir_chunk = std::move(other_sample.reservoir_chunk);
+		sel = SelectionVector(other_sample.sel);
+		sel_size = other_sample.sel_size;
+		Verify();
+		return;
+	}
+	//! Both samples are still in "fast sampling" method
+	if (GetSamplingState() == SamplingState::RANDOM && other_sample.GetSamplingState() == SamplingState::RANDOM) {
+		SimpleMerge(other_sample);
+		return;
+	}
+
+	// One or none of the samples are in "Fast Sampling" method.
+	// When this is the case, switch both to slow sampling
+	ConvertToReservoirSample();
+	other_sample.ConvertToReservoirSample();
+	WeightedMerge(other_sample);
+}
+
+void ReservoirSample::ShuffleSel(SelectionVector &sel, idx_t range, idx_t size) const {
+	auto randomized = GetRandomizedVector(static_cast<uint32_t>(range), static_cast<uint32_t>(size));
+	SelectionVector original_sel(range);
+	for (idx_t i = 0; i < range; i++) {
+		original_sel.set_index(i, sel.get_index(i));
+	}
+	for (idx_t i = 0; i < size; i++) {
+		sel.set_index(i, original_sel.get_index(randomized[i]));
+	}
+}
+
+void ReservoirSample::NormalizeWeights() {
+	vector<std::pair<double, idx_t>> tmp_weights;
+	while (!base_reservoir_sample->reservoir_weights.empty()) {
+		auto top = base_reservoir_sample->reservoir_weights.top();
+		tmp_weights.push_back(std::move(top));
+		base_reservoir_sample->reservoir_weights.pop();
+	}
+	std::sort(tmp_weights.begin(), tmp_weights.end(),
+	          [&](std::pair<double, idx_t> a, std::pair<double, idx_t> b) { return a.second < b.second; });
+	for (idx_t i = 0; i < tmp_weights.size(); i++) {
+		base_reservoir_sample->reservoir_weights.emplace(tmp_weights.at(i).first, i);
+	}
+	base_reservoir_sample->SetNextEntry();
+}
+
+void ReservoirSample::EvictOverBudgetSamples() {
+	Verify();
+	if (!reservoir_chunk || destroyed) {
+		return;
+	}
+
+	// since this is for serialization, we really need to make sure keep a
+	// minimum of 1% of the rows or 2048 rows
+	idx_t num_samples_to_keep =
+	    MinValue<idx_t>(FIXED_SAMPLE_SIZE, static_cast<idx_t>(SAVE_PERCENTAGE * static_cast<double>(GetTuplesSeen())));
+
+	if (num_samples_to_keep <= 0) {
+		reservoir_chunk->chunk.SetCardinality(0);
+		return;
+	}
+
+	if (num_samples_to_keep == sample_count) {
+		return;
+	}
+
+	// if we over sampled, make sure we only keep the highest percentage samples
+	std::unordered_set<idx_t> selections_to_delete;
+
+	while (num_samples_to_keep < GetPriorityQueueSize()) {
+		auto top = PopFromWeightQueue();
+		D_ASSERT(top.second < sel_size);
+		selections_to_delete.emplace(top.second);
+	}
+
+	// set up reservoir chunk for the reservoir sample
+	D_ASSERT(reservoir_chunk->chunk.size() <= sample_count);
+	// create a new sample chunk to store new samples
+	auto types = reservoir_chunk->chunk.GetTypes();
+	D_ASSERT(num_samples_to_keep <= sample_count);
+	D_ASSERT(stats_sample);
+	D_ASSERT(sample_count == STANDARD_VECTOR_SIZE);
+	auto new_reservoir_chunk = CreateNewSampleChunk(types, STANDARD_VECTOR_SIZE);
+
+	// The current selection vector can potentially have 2048 valid mappings.
+	// If we need to save a sample with less rows than that, we need to do the following
+	// 1. Create a new selection vector that doesn't point to the rows we are evicting
+	SelectionVector new_sel(num_samples_to_keep);
+	idx_t offset = 0;
+	for (idx_t i = 0; i < num_samples_to_keep + selections_to_delete.size(); i++) {
+		if (selections_to_delete.find(i) == selections_to_delete.end()) {
+			D_ASSERT(i - offset < num_samples_to_keep);
+			new_sel.set_index(i - offset, sel.get_index(i));
+		} else {
+			offset += 1;
+		}
+	}
+	// 2. Update row_ids in our weights so that they don't store rows ids to
+	//    indexes in the selection vector that have been evicted.
+	if (!selections_to_delete.empty()) {
+		NormalizeWeights();
+	}
+
+	D_ASSERT(reservoir_chunk->chunk.GetTypes() == new_reservoir_chunk->chunk.GetTypes());
+
+	UpdateSampleAppend(new_reservoir_chunk->chunk, reservoir_chunk->chunk, new_sel, num_samples_to_keep);
+	// set the cardinality
+	new_reservoir_chunk->chunk.SetCardinality(num_samples_to_keep);
+	reservoir_chunk = std::move(new_reservoir_chunk);
+	sel_size = num_samples_to_keep;
+	base_reservoir_sample->UpdateMinWeightThreshold();
+}
+
+void ReservoirSample::ExpandSerializedSample() {
+	if (!reservoir_chunk) {
+		return;
+	}
+
+	auto types = reservoir_chunk->chunk.GetTypes();
+	auto new_res_chunk = CreateNewSampleChunk(types, GetReservoirChunkCapacity());
+	auto copy_count = reservoir_chunk->chunk.size();
+	SelectionVector tmp_sel = SelectionVector(0, copy_count);
+	UpdateSampleAppend(new_res_chunk->chunk, reservoir_chunk->chunk, tmp_sel, copy_count);
+	new_res_chunk->chunk.SetCardinality(copy_count);
+	std::swap(reservoir_chunk, new_res_chunk);
+}
+
+idx_t ReservoirSample::GetReservoirChunkCapacity() const {
+	return sample_count + (FIXED_SAMPLE_SIZE_MULTIPLIER * FIXED_SAMPLE_SIZE);
+}
+
+idx_t ReservoirSample::FillReservoir(DataChunk &chunk) {
+
+	idx_t ingested_count = 0;
+	if (!reservoir_chunk) {
+		if (chunk.size() > FIXED_SAMPLE_SIZE) {
+			throw InternalException("Creating sample with DataChunk that is larger than the fixed sample size");
+		}
+		auto types = chunk.GetTypes();
+		// create a new sample chunk to store new samples
+		reservoir_chunk = CreateNewSampleChunk(types, GetReservoirChunkCapacity());
+	}
+
+	idx_t actual_sample_index_start = GetActiveSampleCount();
+	D_ASSERT(reservoir_chunk->chunk.ColumnCount() == chunk.ColumnCount());
+
+	if (reservoir_chunk->chunk.size() < sample_count) {
+		ingested_count = MinValue<idx_t>(sample_count - reservoir_chunk->chunk.size(), chunk.size());
+		auto random_other_sel =
+		    GetRandomizedVector(static_cast<uint32_t>(ingested_count), static_cast<uint32_t>(ingested_count));
+		SelectionVector sel_for_input_chunk(ingested_count);
+		for (idx_t i = 0; i < ingested_count; i++) {
+			sel.set_index(actual_sample_index_start + i, actual_sample_index_start + i);
+			sel_for_input_chunk.set_index(i, random_other_sel[i]);
+		}
+		UpdateSampleAppend(reservoir_chunk->chunk, chunk, sel_for_input_chunk, ingested_count);
+		sel_size += ingested_count;
+	}
+	D_ASSERT(GetActiveSampleCount() <= sample_count);
+	D_ASSERT(GetActiveSampleCount() >= ingested_count);
+	// always return how many tuples were ingested
+	return ingested_count;
+}
+
+void ReservoirSample::Destroy() {
+	destroyed = true;
+}
+
+SelectionVectorHelper ReservoirSample::GetReplacementIndexes(idx_t sample_chunk_offset,
+                                                             idx_t theoretical_chunk_length) {
+	if (GetSamplingState() == SamplingState::RANDOM) {
+		return GetReplacementIndexesFast(sample_chunk_offset, theoretical_chunk_length);
+	}
+	return GetReplacementIndexesSlow(sample_chunk_offset, theoretical_chunk_length);
+}
+
+SelectionVectorHelper ReservoirSample::GetReplacementIndexesFast(idx_t sample_chunk_offset, idx_t chunk_length) {
+
+	// how much weight to the other tuples have compared to the ones in this chunk?
+	auto weight_tuples_other = static_cast<double>(chunk_length) / static_cast<double>(GetTuplesSeen() + chunk_length);
+	auto num_to_pop = static_cast<uint32_t>(round(weight_tuples_other * static_cast<double>(sample_count)));
+	D_ASSERT(num_to_pop <= sample_count);
+	D_ASSERT(num_to_pop <= sel_size);
+	SelectionVectorHelper ret;
+
+	if (num_to_pop == 0) {
+		ret.sel = SelectionVector(num_to_pop);
+		ret.size = 0;
+		return ret;
+	}
+	std::unordered_map<idx_t, idx_t> replacement_indexes;
+	SelectionVector chunk_sel(num_to_pop);
+
+	auto random_indexes_chunk = GetRandomizedVector(static_cast<uint32_t>(chunk_length), num_to_pop);
+	auto random_sel_indexes = GetRandomizedVector(static_cast<uint32_t>(sel_size), num_to_pop);
+	for (idx_t i = 0; i < num_to_pop; i++) {
+		// update the selection vector for the reservoir sample
+		chunk_sel.set_index(i, random_indexes_chunk[i]);
+		// sel is not guaratneed to be random, so we update the indexes according to our
+		// random sel indexes.
+		sel.set_index(random_sel_indexes[i], sample_chunk_offset + i);
+	}
+
+	D_ASSERT(sel_size == sample_count);
+
+	ret.sel = SelectionVector(chunk_sel);
+	ret.size = num_to_pop;
+	return ret;
+}
+
+SelectionVectorHelper ReservoirSample::GetReplacementIndexesSlow(const idx_t sample_chunk_offset,
+                                                                 const idx_t chunk_length) {
+	idx_t remaining = chunk_length;
+	std::unordered_map<idx_t, idx_t> ret_map;
+	idx_t sample_chunk_index = 0;
+
+	idx_t base_offset = 0;
+
+	while (true) {
+		idx_t offset =
+		    base_reservoir_sample->next_index_to_sample - base_reservoir_sample->num_entries_to_skip_b4_next_sample;
+		if (offset >= remaining) {
+			// not in this chunk! increment current count and go to the next chunk
+			base_reservoir_sample->num_entries_to_skip_b4_next_sample += remaining;
+			break;
+		}
+		// in this chunk! replace the element
+		// ret[index_in_new_chunk] = index_in_sample_chunk (the sample chunk offset will be applied later)
+		// D_ASSERT(sample_chunk_index == ret.size());
+		ret_map[base_offset + offset] = sample_chunk_index;
+		double r2 = base_reservoir_sample->random.NextRandom32(base_reservoir_sample->min_weight_threshold, 1);
+		// replace element in our max_heap
+		// first get the top most pair
+		const auto top = PopFromWeightQueue();
+		const auto index = top.second;
+		const auto index_in_sample_chunk = sample_chunk_offset + sample_chunk_index;
+		sel.set_index(index, index_in_sample_chunk);
+		base_reservoir_sample->ReplaceElementWithIndex(index, r2, false);
+
+		sample_chunk_index += 1;
+		// shift the chunk forward
+		remaining -= offset;
+		base_offset += offset;
+	}
+
+	// create selection vector to return
+	SelectionVector ret_sel(ret_map.size());
+	D_ASSERT(sel_size == sample_count);
+	for (auto &kv : ret_map) {
+		ret_sel.set_index(kv.second, kv.first);
+	}
+	SelectionVectorHelper ret;
+	ret.sel = SelectionVector(ret_sel);
+	ret.size = static_cast<uint32_t>(ret_map.size());
+	return ret;
+}
+
+void ReservoirSample::Finalize() {
+}
+
+bool ReservoirSample::ValidSampleType(const LogicalType &type) {
+	return type.IsNumeric();
+}
+
+void ReservoirSample::UpdateSampleAppend(DataChunk &this_, DataChunk &other, SelectionVector &other_sel,
+                                         idx_t append_count) const {
+	idx_t new_size = this_.size() + append_count;
+	if (other.size() == 0) {
+		return;
+	}
+	D_ASSERT(this_.GetTypes() == other.GetTypes());
+
+	// UpdateSampleAppend(this_, other, other_sel, append_count);
+	D_ASSERT(this_.GetTypes() == other.GetTypes());
+	auto types = reservoir_chunk->chunk.GetTypes();
+
+	for (idx_t i = 0; i < reservoir_chunk->chunk.ColumnCount(); i++) {
+		auto col_type = types[i];
+		if (ValidSampleType(col_type) || !stats_sample) {
+			D_ASSERT(this_.data[i].GetVectorType() == VectorType::FLAT_VECTOR);
+			VectorOperations::Copy(other.data[i], this_.data[i], other_sel, append_count, 0, this_.size());
+		}
+	}
+	this_.SetCardinality(new_size);
+}
+
+void ReservoirSample::AddToReservoir(DataChunk &chunk) {
+	if (destroyed || chunk.size() == 0) {
+		return;
+	}
+
+	idx_t tuples_consumed = FillReservoir(chunk);
+	base_reservoir_sample->num_entries_seen_total += tuples_consumed;
+	D_ASSERT(reservoir_chunk->chunk.size() >= 1);
+
+	if (tuples_consumed == chunk.size()) {
+		return;
+	}
+
+	// the chunk filled the first FIXED_SAMPLE_SIZE chunk but still has tuples remaining
+	// slice the chunk and call AddToReservoir again.
+	if (tuples_consumed != chunk.size() && tuples_consumed != 0) {
+		// Fill reservoir consumed some of the chunk to reach FIXED_SAMPLE_SIZE
+		// now we need to
+		// So we slice it and call AddToReservoir
+		auto slice = make_uniq<DataChunk>();
+		auto samples_remaining = chunk.size() - tuples_consumed;
+		auto types = chunk.GetTypes();
+		SelectionVector input_sel(samples_remaining);
+		for (idx_t i = 0; i < samples_remaining; i++) {
+			input_sel.set_index(i, tuples_consumed + i);
+		}
+		slice->Initialize(Allocator::DefaultAllocator(), types, samples_remaining);
+		slice->Slice(chunk, input_sel, samples_remaining);
+		slice->SetCardinality(samples_remaining);
+		AddToReservoir(*slice);
+		return;
+	}
+
+	// at this point we should have collected at least sample count samples
+	D_ASSERT(GetActiveSampleCount() >= sample_count);
+
+	auto chunk_sel = GetReplacementIndexes(reservoir_chunk->chunk.size(), chunk.size());
+
+	if (chunk_sel.size == 0) {
+		// not adding any samples
+		return;
+	}
+	idx_t size = chunk_sel.size;
+	D_ASSERT(size <= chunk.size());
+
+	UpdateSampleAppend(reservoir_chunk->chunk, chunk, chunk_sel.sel, size);
+
+	base_reservoir_sample->num_entries_seen_total += chunk.size();
+	D_ASSERT(base_reservoir_sample->reservoir_weights.size() == 0 ||
+	         base_reservoir_sample->reservoir_weights.size() == sample_count);
+
+	Verify();
+
+	// if we are over the threshold, we ned to swith to slow sampling.
+	if (GetSamplingState() == SamplingState::RANDOM && GetTuplesSeen() >= FIXED_SAMPLE_SIZE * FAST_TO_SLOW_THRESHOLD) {
+		ConvertToReservoirSample();
+	}
+	if (reservoir_chunk->chunk.size() >= (GetReservoirChunkCapacity() - (static_cast<idx_t>(FIXED_SAMPLE_SIZE) * 3))) {
+		Vacuum();
+	}
+}
+
+void ReservoirSample::Verify() {
+#ifdef DEBUG
+	if (destroyed) {
+		return;
+	}
+	if (GetPriorityQueueSize() == 0) {
+		D_ASSERT(GetActiveSampleCount() <= sample_count);
+		D_ASSERT(GetTuplesSeen() >= GetActiveSampleCount());
+		return;
+	}
+	if (NumSamplesCollected() > sample_count) {
+		D_ASSERT(GetPriorityQueueSize() == sample_count);
+	} else if (NumSamplesCollected() <= sample_count && GetPriorityQueueSize() > 0) {
+		// it's possible to collect more samples than your priority queue size.
+		// see sample_converts_to_reservoir_sample.test
+		D_ASSERT(NumSamplesCollected() >= GetPriorityQueueSize());
+	}
+	auto base_reservoir_copy = base_reservoir_sample->Copy();
+	std::unordered_map<idx_t, idx_t> index_count;
+	while (!base_reservoir_copy->reservoir_weights.empty()) {
+		auto &pair = base_reservoir_copy->reservoir_weights.top();
+		if (index_count.find(pair.second) == index_count.end()) {
+			index_count[pair.second] = 1;
+			base_reservoir_copy->reservoir_weights.pop();
+		} else {
+			index_count[pair.second] += 1;
+			base_reservoir_copy->reservoir_weights.pop();
+			throw InternalException("Duplicate selection index in reservoir weights");
+		}
+	}
+	// TODO: Verify the Sel as well. No duplicate indices.
+
+	if (reservoir_chunk) {
+		reservoir_chunk->chunk.Verify();
+	}
+#endif
+}
+
+ReservoirSamplePercentage::ReservoirSamplePercentage(double percentage, int64_t seed, idx_t reservoir_sample_size)
+    : BlockingSample(seed), allocator(Allocator::DefaultAllocator()), sample_percentage(percentage / 100.0),
+      reservoir_sample_size(reservoir_sample_size), current_count(0), is_finalized(false) {
+	current_sample = make_uniq<ReservoirSample>(allocator, reservoir_sample_size, base_reservoir_sample->random());
+	type = SampleType::RESERVOIR_PERCENTAGE_SAMPLE;
+}
+
+ReservoirSamplePercentage::ReservoirSamplePercentage(Allocator &allocator, double percentage, int64_t seed)
+    : BlockingSample(seed), allocator(allocator), sample_percentage(percentage / 100.0), current_count(0),
+      is_finalized(false) {
+	reservoir_sample_size = (idx_t)(sample_percentage * RESERVOIR_THRESHOLD);
+	current_sample = make_uniq<ReservoirSample>(allocator, reservoir_sample_size, base_reservoir_sample->random());
+	type = SampleType::RESERVOIR_PERCENTAGE_SAMPLE;
+}
+
+ReservoirSamplePercentage::ReservoirSamplePercentage(double percentage, int64_t seed)
+    : ReservoirSamplePercentage(Allocator::DefaultAllocator(), percentage, seed) {
+}
+
+void ReservoirSamplePercentage::AddToReservoir(DataChunk &input) {
+	base_reservoir_sample->num_entries_seen_total += input.size();
+	if (current_count + input.size() > RESERVOIR_THRESHOLD) {
+		// we don't have enough space in our current reservoir
+		// first check what we still need to append to the current sample
+		idx_t append_to_current_sample_count = RESERVOIR_THRESHOLD - current_count;
+		idx_t append_to_next_sample = input.size() - append_to_current_sample_count;
+		if (append_to_current_sample_count > 0) {
+			// we have elements remaining, first add them to the current sample
+			if (append_to_next_sample > 0) {
+				// we need to also add to the next sample
+				DataChunk new_chunk;
+				new_chunk.InitializeEmpty(input.GetTypes());
+				new_chunk.Slice(input, *FlatVector::IncrementalSelectionVector(), append_to_current_sample_count);
+				new_chunk.Flatten();
+				current_sample->AddToReservoir(new_chunk);
+			} else {
+				input.Flatten();
+				input.SetCardinality(append_to_current_sample_count);
+				current_sample->AddToReservoir(input);
+			}
+		}
+		if (append_to_next_sample > 0) {
+			// slice the input for the remainder
+			SelectionVector sel(append_to_next_sample);
+			for (idx_t i = append_to_current_sample_count; i < append_to_next_sample + append_to_current_sample_count;
+			     i++) {
+				sel.set_index(i - append_to_current_sample_count, i);
+			}
+			input.Slice(sel, append_to_next_sample);
+		}
+		// now our first sample is filled: append it to the set of finished samples
+		finished_samples.push_back(std::move(current_sample));
+
+		// allocate a new sample, and potentially add the remainder of the current input to that sample
+		current_sample = make_uniq<ReservoirSample>(allocator, reservoir_sample_size, base_reservoir_sample->random());
+		if (append_to_next_sample > 0) {
+			current_sample->AddToReservoir(input);
+		}
+		current_count = append_to_next_sample;
+	} else {
+		// we can just append to the current sample
+		current_count += input.size();
+		current_sample->AddToReservoir(input);
+	}
+}
+
+unique_ptr<DataChunk> ReservoirSamplePercentage::GetChunk() {
+	// reservoir sample percentage should never stay
+	if (!is_finalized) {
+		Finalize();
+	}
+	while (!finished_samples.empty()) {
+		auto &front = finished_samples.front();
+		auto chunk = front->GetChunk();
+		if (chunk && chunk->size() > 0) {
+			return chunk;
+		}
+		// move to the next sample
+		finished_samples.erase(finished_samples.begin());
+	}
+	return nullptr;
+}
+
+unique_ptr<BlockingSample> ReservoirSamplePercentage::Copy() const {
+	throw InternalException("Cannot call Copy on ReservoirSample Percentage");
+}
+
+void ReservoirSamplePercentage::Finalize() {
+	// need to finalize the current sample, if any
+	// we are finializing, so we are starting to return chunks. Our last chunk has
+	// sample_percentage * RESERVOIR_THRESHOLD entries that hold samples.
+	// if our current count is less than the sample_percentage * RESERVOIR_THRESHOLD
+	// then we have sampled too much for the current_sample and we need to redo the sample
+	// otherwise we can just push the current sample back
+	// Imagine sampling 70% of 100 rows (so 70 rows). We allocate sample_percentage * RESERVOIR_THRESHOLD
+	// -----------------------------------------
+	auto sampled_more_than_required =
+	    static_cast<double>(current_count) > sample_percentage * RESERVOIR_THRESHOLD || finished_samples.empty();
+	if (current_count > 0 && sampled_more_than_required) {
+		// create a new sample
+		auto new_sample_size = static_cast<idx_t>(round(sample_percentage * static_cast<double>(current_count)));
+		auto new_sample = make_uniq<ReservoirSample>(allocator, new_sample_size, base_reservoir_sample->random());
+		while (true) {
+			auto chunk = current_sample->GetChunk();
+			if (!chunk || chunk->size() == 0) {
+				break;
+			}
+			new_sample->AddToReservoir(*chunk);
+		}
+		finished_samples.push_back(std::move(new_sample));
+	} else {
+		finished_samples.push_back(std::move(current_sample));
+	}
+	// when finalizing, current_sample is null. All samples are now in finished samples.
+	current_sample = nullptr;
+	is_finalized = true;
+}
+
+} // namespace duckdb
diff --git a/src/function/table/system/CMakeLists.txt b/src/function/table/system/CMakeLists.txt
index 36bcfa029613..573895f2b4a2 100644
--- a/src/function/table/system/CMakeLists.txt
+++ b/src/function/table/system/CMakeLists.txt
@@ -26,6 +26,7 @@ add_library_unity(
   pragma_metadata_info.cpp
   pragma_storage_info.cpp
   pragma_table_info.cpp
+  pragma_table_sample.cpp
   pragma_user_agent.cpp
   test_all_types.cpp
   test_vector_types.cpp)
diff --git a/src/function/table/system/pragma_table_sample.cpp b/src/function/table/system/pragma_table_sample.cpp
new file mode 100644
index 000000000000..7f4122b92b62
--- /dev/null
+++ b/src/function/table/system/pragma_table_sample.cpp
@@ -0,0 +1,95 @@
+#include "duckdb/function/table/system_functions.hpp"
+
+#include "duckdb/catalog/catalog.hpp"
+#include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
+#include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
+#include "duckdb/parser/qualified_name.hpp"
+#include "duckdb/parser/constraints/not_null_constraint.hpp"
+#include "duckdb/parser/constraints/unique_constraint.hpp"
+#include "duckdb/planner/expression/bound_parameter_expression.hpp"
+#include "duckdb/planner/binder.hpp"
+
+#include "duckdb/common/exception.hpp"
+#include "duckdb/common/limits.hpp"
+
+#include <algorithm>
+
+namespace duckdb {
+
+struct DuckDBTableSampleFunctionData : public TableFunctionData {
+	explicit DuckDBTableSampleFunctionData(CatalogEntry &entry_p) : entry(entry_p) {
+	}
+	CatalogEntry &entry;
+};
+
+struct DuckDBTableSampleOperatorData : public GlobalTableFunctionState {
+	DuckDBTableSampleOperatorData() : sample_offset(0) {
+		sample = nullptr;
+	}
+	idx_t sample_offset;
+	unique_ptr<BlockingSample> sample;
+};
+
+static unique_ptr<FunctionData> DuckDBTableSampleBind(ClientContext &context, TableFunctionBindInput &input,
+                                                      vector<LogicalType> &return_types, vector<string> &names) {
+
+	// look up the table name in the catalog
+	auto qname = QualifiedName::Parse(input.inputs[0].GetValue<string>());
+	Binder::BindSchemaOrCatalog(context, qname.catalog, qname.schema);
+
+	auto &entry = Catalog::GetEntry(context, CatalogType::TABLE_ENTRY, qname.catalog, qname.schema, qname.name);
+	if (entry.type != CatalogType::TABLE_ENTRY) {
+		throw NotImplementedException("Invalid Catalog type passed to table_sample()");
+	}
+	auto &table_entry = entry.Cast<TableCatalogEntry>();
+	auto types = table_entry.GetTypes();
+	for (auto &type : types) {
+		return_types.push_back(type);
+	}
+	for (idx_t i = 0; i < types.size(); i++) {
+		auto logical_index = LogicalIndex(i);
+		auto &col = table_entry.GetColumn(logical_index);
+		names.push_back(col.GetName());
+	}
+
+	return make_uniq<DuckDBTableSampleFunctionData>(entry);
+}
+
+unique_ptr<GlobalTableFunctionState> DuckDBTableSampleInit(ClientContext &context, TableFunctionInitInput &input) {
+	return make_uniq<DuckDBTableSampleOperatorData>();
+}
+
+static void DuckDBTableSampleTable(ClientContext &context, DuckDBTableSampleOperatorData &data,
+                                   TableCatalogEntry &table, DataChunk &output) {
+	// if table has statistics.
+	// copy the sample of statistics into the output chunk
+	if (!data.sample) {
+		data.sample = table.GetSample();
+	}
+	if (data.sample) {
+		auto sample_chunk = data.sample->GetChunk();
+		if (sample_chunk) {
+			sample_chunk->Copy(output, 0);
+			data.sample_offset += sample_chunk->size();
+		}
+	}
+}
+
+static void DuckDBTableSampleFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {
+	auto &bind_data = data_p.bind_data->Cast<DuckDBTableSampleFunctionData>();
+	auto &state = data_p.global_state->Cast<DuckDBTableSampleOperatorData>();
+	switch (bind_data.entry.type) {
+	case CatalogType::TABLE_ENTRY:
+		DuckDBTableSampleTable(context, state, bind_data.entry.Cast<TableCatalogEntry>(), output);
+		break;
+	default:
+		throw NotImplementedException("Unimplemented catalog type for pragma_table_sample");
+	}
+}
+
+void DuckDBTableSample::RegisterFunction(BuiltinFunctions &set) {
+	set.AddFunction(TableFunction("duckdb_table_sample", {LogicalType::VARCHAR}, DuckDBTableSampleFunction,
+	                              DuckDBTableSampleBind, DuckDBTableSampleInit));
+}
+
+} // namespace duckdb
diff --git a/src/function/table/system_functions.cpp b/src/function/table/system_functions.cpp
index 12e8bcc35eff..7560221c587b 100644
--- a/src/function/table/system_functions.cpp
+++ b/src/function/table/system_functions.cpp
@@ -34,6 +34,7 @@ void BuiltinFunctions::RegisterSQLiteFunctions() {
 	DuckDBSequencesFun::RegisterFunction(*this);
 	DuckDBSettingsFun::RegisterFunction(*this);
 	DuckDBTablesFun::RegisterFunction(*this);
+	DuckDBTableSample::RegisterFunction(*this);
 	DuckDBTemporaryFilesFun::RegisterFunction(*this);
 	DuckDBTypesFun::RegisterFunction(*this);
 	DuckDBVariablesFun::RegisterFunction(*this);
diff --git a/src/include/duckdb/catalog/catalog_entry/duck_table_entry.hpp b/src/include/duckdb/catalog/catalog_entry/duck_table_entry.hpp
index ce09c4fe54f1..fb9d5ae67d14 100644
--- a/src/include/duckdb/catalog/catalog_entry/duck_table_entry.hpp
+++ b/src/include/duckdb/catalog/catalog_entry/duck_table_entry.hpp
@@ -35,6 +35,8 @@ class DuckTableEntry : public TableCatalogEntry {
 	//! Get statistics of a column (physical or virtual) within the table
 	unique_ptr<BaseStatistics> GetStatistics(ClientContext &context, column_t column_id) override;
 
+	unique_ptr<BlockingSample> GetSample() override;
+
 	unique_ptr<CatalogEntry> Copy(ClientContext &context) const override;
 
 	void SetAsRoot() override;
diff --git a/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp b/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp
index d2e47e6d4ea4..5ce45af7ff2b 100644
--- a/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp
+++ b/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp
@@ -13,6 +13,7 @@
 #include "duckdb/parser/column_list.hpp"
 #include "duckdb/parser/constraint.hpp"
 #include "duckdb/planner/bound_constraint.hpp"
+#include "duckdb/storage/table/table_statistics.hpp"
 #include "duckdb/planner/expression.hpp"
 #include "duckdb/common/case_insensitive_map.hpp"
 #include "duckdb/catalog/catalog_entry/table_column_type.hpp"
@@ -82,6 +83,8 @@ class TableCatalogEntry : public StandardEntry {
 	//! Get statistics of a column (physical or virtual) within the table
 	virtual unique_ptr<BaseStatistics> GetStatistics(ClientContext &context, column_t column_id) = 0;
 
+	virtual unique_ptr<BlockingSample> GetSample();
+
 	//! Returns the column index of the specified column name.
 	//! If the column does not exist:
 	//! If if_column_exists is true, returns DConstants::INVALID_INDEX
diff --git a/src/include/duckdb/common/enum_util.hpp b/src/include/duckdb/common/enum_util.hpp
index 855e153870a7..6057a89b2aa3 100644
--- a/src/include/duckdb/common/enum_util.hpp
+++ b/src/include/duckdb/common/enum_util.hpp
@@ -282,6 +282,8 @@ enum class SampleMethod : uint8_t;
 
 enum class SampleType : uint8_t;
 
+enum class SamplingState : uint8_t;
+
 enum class ScanType : uint8_t;
 
 enum class SecretDisplayType : uint8_t;
@@ -752,6 +754,9 @@ const char* EnumUtil::ToChars<SampleMethod>(SampleMethod value);
 template<>
 const char* EnumUtil::ToChars<SampleType>(SampleType value);
 
+template<>
+const char* EnumUtil::ToChars<SamplingState>(SamplingState value);
+
 template<>
 const char* EnumUtil::ToChars<ScanType>(ScanType value);
 
@@ -1269,6 +1274,9 @@ SampleMethod EnumUtil::FromString<SampleMethod>(const char *value);
 template<>
 SampleType EnumUtil::FromString<SampleType>(const char *value);
 
+template<>
+SamplingState EnumUtil::FromString<SamplingState>(const char *value);
+
 template<>
 ScanType EnumUtil::FromString<ScanType>(const char *value);
 
diff --git a/src/include/duckdb/common/random_engine.hpp b/src/include/duckdb/common/random_engine.hpp
index 970db6ce4f45..59531e1d945e 100644
--- a/src/include/duckdb/common/random_engine.hpp
+++ b/src/include/duckdb/common/random_engine.hpp
@@ -18,11 +18,11 @@ namespace duckdb {
 class ClientContext;
 struct RandomState;
 
-struct RandomEngine {
+class RandomEngine {
+public:
 	explicit RandomEngine(int64_t seed = -1);
 	~RandomEngine();
 
-public:
 	//! Generate a random number between min and max
 	double NextRandom(double min, double max);
 
@@ -31,6 +31,7 @@ struct RandomEngine {
 	//! Generate a random number between 0 and 1, using 32-bits as a base
 	double NextRandom32();
 	double NextRandom32(double min, double max);
+	uint32_t NextRandomInteger32(uint32_t min, uint32_t max);
 	uint32_t NextRandomInteger();
 	uint32_t NextRandomInteger(uint32_t min, uint32_t max);
 	uint64_t NextRandomInteger64();
diff --git a/src/include/duckdb/common/serializer/serializer.hpp b/src/include/duckdb/common/serializer/serializer.hpp
index 60531fd1a3b3..54b994e4ba2d 100644
--- a/src/include/duckdb/common/serializer/serializer.hpp
+++ b/src/include/duckdb/common/serializer/serializer.hpp
@@ -16,6 +16,7 @@
 #include "duckdb/common/types/uhugeint.hpp"
 #include "duckdb/common/unordered_map.hpp"
 #include "duckdb/common/unordered_set.hpp"
+#include "duckdb/common/queue.hpp"
 #include "duckdb/common/optional_idx.hpp"
 #include "duckdb/common/optionally_owned_ptr.hpp"
 #include "duckdb/common/value_operations/value_operations.hpp"
diff --git a/src/include/duckdb/common/types/uuid.hpp b/src/include/duckdb/common/types/uuid.hpp
index 5573aac633c5..bf5ade17a15f 100644
--- a/src/include/duckdb/common/types/uuid.hpp
+++ b/src/include/duckdb/common/types/uuid.hpp
@@ -13,7 +13,7 @@
 
 namespace duckdb {
 class ClientContext;
-struct RandomEngine;
+class RandomEngine;
 
 //! The UUID class contains static operations for the UUID type
 class UUID {
diff --git a/src/include/duckdb/execution/physical_operator.hpp b/src/include/duckdb/execution/physical_operator.hpp
index 822b55377b74..50529d594f67 100644
--- a/src/include/duckdb/execution/physical_operator.hpp
+++ b/src/include/duckdb/execution/physical_operator.hpp
@@ -164,7 +164,7 @@ class PhysicalOperator {
 	virtual void PrepareFinalize(ClientContext &context, GlobalSinkState &sink_state) const;
 	//! The finalize is called when ALL threads are finished execution. It is called only once per pipeline, and is
 	//! entirely single threaded.
-	//! If Finalize returns SinkResultType::FINISHED, the sink is marked as finished
+	//! If Finalize returns SinkResultType::Finished, the sink is marked as finished
 	virtual SinkFinalizeType Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
 	                                  OperatorSinkFinalizeInput &input) const;
 	//! For sinks with RequiresBatchIndex set to true, when a new batch starts being processed this method is called
diff --git a/src/include/duckdb/execution/reservoir_sample.hpp b/src/include/duckdb/execution/reservoir_sample.hpp
index 0edc7e073b9a..b794328bb0fb 100644
--- a/src/include/duckdb/execution/reservoir_sample.hpp
+++ b/src/include/duckdb/execution/reservoir_sample.hpp
@@ -12,25 +12,64 @@
 #include "duckdb/common/common.hpp"
 #include "duckdb/common/random_engine.hpp"
 #include "duckdb/common/types/data_chunk.hpp"
+#include "duckdb/common/windows_undefs.hpp"
 
 #include "duckdb/common/queue.hpp"
 
+// Originally intended to be the vector size, but in order to run on
+// vector size = 2, we had to change it.
+#define FIXED_SAMPLE_SIZE 2048
+
 namespace duckdb {
 
 enum class SampleType : uint8_t { BLOCKING_SAMPLE = 0, RESERVOIR_SAMPLE = 1, RESERVOIR_PERCENTAGE_SAMPLE = 2 };
 
+enum class SamplingState : uint8_t { RANDOM = 0, RESERVOIR = 1 };
+
+class ReservoirRNG : public RandomEngine {
+public:
+	// return type must be called result type to be a valid URNG
+	typedef uint32_t result_type;
+
+	explicit ReservoirRNG(int64_t seed) : RandomEngine(seed) {};
+
+	result_type operator()() {
+		return NextRandomInteger();
+	};
+
+	static constexpr result_type min() {
+		return NumericLimits<result_type>::Minimum();
+	};
+	static constexpr result_type max() {
+		return NumericLimits<result_type>::Maximum();
+	};
+};
+
+//! Resevoir sampling is based on the 2005 paper "Weighted Random Sampling" by Efraimidis and Spirakis
 class BaseReservoirSampling {
 public:
 	explicit BaseReservoirSampling(int64_t seed);
 	BaseReservoirSampling();
 
-	void InitializeReservoir(idx_t cur_size, idx_t sample_size);
+	void InitializeReservoirWeights(idx_t cur_size, idx_t sample_size);
 
 	void SetNextEntry();
 
+	void ReplaceElementWithIndex(idx_t entry_index, double with_weight, bool pop = true);
 	void ReplaceElement(double with_weight = -1);
+
+	void UpdateMinWeightThreshold();
+
+	//! Go from the naive sampling to the reservoir sampling
+	//! Naive samping will not collect weights, but when we serialize
+	//! we need to serialize weights again.
+	void FillWeights(SelectionVector &sel, idx_t &sel_size);
+
+	unique_ptr<BaseReservoirSampling> Copy();
+
 	//! The random generator
-	RandomEngine random;
+	ReservoirRNG random;
+
 	//! The next element to sample
 	idx_t next_index_to_sample;
 	//! The reservoir threshold of the current min entry
@@ -48,6 +87,13 @@ class BaseReservoirSampling {
 
 	void Serialize(Serializer &serializer) const;
 	static unique_ptr<BaseReservoirSampling> Deserialize(Deserializer &deserializer);
+
+	static double GetMinWeightFromTuplesSeen(idx_t rows_seen_total);
+	// static unordered_map<idx_t, double> tuples_to_min_weight_map;
+	// Blocking sample is a virtual class. It should be allowed to see the weights and
+	// of tuples in the sample. The blocking sample can then easily maintain statisitcal properties
+	// from the sample point of view.
+	friend class BlockingSample;
 };
 
 class BlockingSample {
@@ -61,24 +107,31 @@ class BlockingSample {
 	bool destroyed;
 
 public:
-	explicit BlockingSample(int64_t seed) : old_base_reservoir_sample(seed), random(old_base_reservoir_sample.random) {
-		base_reservoir_sample = nullptr;
+	explicit BlockingSample(int64_t seed = -1)
+	    : base_reservoir_sample(make_uniq<BaseReservoirSampling>(seed)), type(SampleType::BLOCKING_SAMPLE),
+	      destroyed(false) {
 	}
 	virtual ~BlockingSample() {
 	}
 
 	//! Add a chunk of data to the sample
 	virtual void AddToReservoir(DataChunk &input) = 0;
-
+	virtual unique_ptr<BlockingSample> Copy() const = 0;
 	virtual void Finalize() = 0;
-	//! Fetches a chunk from the sample. Note that this method is destructive and should only be used after the
-	//! sample is completely built.
+	virtual void Destroy();
+
+	//! Fetches a chunk from the sample. destroy = true should only be used when
+	//! querying from a sample defined in a query and not a duckdb_table_sample.
 	virtual unique_ptr<DataChunk> GetChunk() = 0;
-	BaseReservoirSampling old_base_reservoir_sample;
 
 	virtual void Serialize(Serializer &serializer) const;
 	static unique_ptr<BlockingSample> Deserialize(Deserializer &deserializer);
 
+	//! Helper functions needed to merge two reservoirs while respecting weights of sampled rows
+	std::pair<double, idx_t> PopFromWeightQueue();
+	double GetMinWeightThreshold();
+	idx_t GetPriorityQueueSize();
+
 public:
 	template <class TARGET>
 	TARGET &Cast() {
@@ -95,8 +148,6 @@ class BlockingSample {
 		}
 		return reinterpret_cast<const TARGET &>(*this);
 	}
-	//! The reservoir sampling
-	RandomEngine &random;
 };
 
 class ReservoirChunk {
@@ -107,45 +158,120 @@ class ReservoirChunk {
 	DataChunk chunk;
 	void Serialize(Serializer &serializer) const;
 	static unique_ptr<ReservoirChunk> Deserialize(Deserializer &deserializer);
+
+	unique_ptr<ReservoirChunk> Copy() const;
+};
+
+struct SelectionVectorHelper {
+	SelectionVector sel;
+	uint32_t size;
 };
 
-//! The reservoir sample class maintains a streaming sample of fixed size "sample_count"
 class ReservoirSample : public BlockingSample {
 public:
 	static constexpr const SampleType TYPE = SampleType::RESERVOIR_SAMPLE;
 
-public:
+	constexpr static idx_t FIXED_SAMPLE_SIZE_MULTIPLIER = 10;
+	constexpr static idx_t FAST_TO_SLOW_THRESHOLD = 60;
+
+	// If the table has less than 204800 rows, this is the percentage
+	// of values we save when serializing/returning a sample.
+	constexpr static double SAVE_PERCENTAGE = 0.01;
+
 	ReservoirSample(Allocator &allocator, idx_t sample_count, int64_t seed = 1);
-	explicit ReservoirSample(idx_t sample_count, int64_t seed = 1);
+	explicit ReservoirSample(idx_t sample_count, unique_ptr<ReservoirChunk> = nullptr);
+
+	//! methods used to help with serializing and deserializing
+	void EvictOverBudgetSamples();
+	void ExpandSerializedSample();
+
+	SamplingState GetSamplingState() const;
+
+	//! Vacuum the Reservoir Sample so it throws away tuples that are not in the
+	//! reservoir weights or in the selection vector
+	void Vacuum();
+
+	//! Transform To sample based on reservoir sampling paper
+	void ConvertToReservoirSample();
+
+	//! Get the capactiy of the data chunk reserved for storing samples
+	idx_t GetReservoirChunkCapacity() const;
 
+	//! If for_serialization=true then the sample_chunk is not padded with extra spaces for
+	//! future sampling values
+	unique_ptr<BlockingSample> Copy() const override;
+
+	//! create the first chunk called by AddToReservoir()
+	idx_t FillReservoir(DataChunk &chunk);
 	//! Add a chunk of data to the sample
 	void AddToReservoir(DataChunk &input) override;
+	//! Merge two Reservoir Samples. Other must be a reservoir sample
+	void Merge(unique_ptr<BlockingSample> other);
+
+	void ShuffleSel(SelectionVector &sel, idx_t range, idx_t size) const;
+
+	//! Update the sample by pushing new sample rows to the end of the sample_chunk.
+	//! The new sample rows are the tuples rows resulting from applying sel to other
+	void UpdateSampleAppend(DataChunk &this_, DataChunk &other, SelectionVector &other_sel, idx_t append_count) const;
+
+	idx_t GetTuplesSeen() const;
+	idx_t NumSamplesCollected() const;
+	idx_t GetActiveSampleCount() const;
+	static bool ValidSampleType(const LogicalType &type);
+
+	// get the chunk from Reservoir chunk
+	DataChunk &Chunk();
 
 	//! Fetches a chunk from the sample. Note that this method is destructive and should only be used after the
 	//! sample is completely built.
+	// unique_ptr<DataChunk> GetChunkAndDestroy() override;
 	unique_ptr<DataChunk> GetChunk() override;
+	void Destroy() override;
 	void Finalize() override;
+	void Verify();
+
+	idx_t GetSampleCount();
+
+	// map is [index in input chunk] -> [index in sample chunk]. Both are zero-based
+	// [index in sample chunk] is incremented by 1
+	// index in input chunk have random values, however, they are increasing.
+	// The base_reservoir_sampling gets updated however, so the indexes point to (sample_chunk_offset +
+	// index_in_sample_chunk) this data is used to make a selection vector to copy samples from the input chunk to the
+	// sample chunk
+	//! Get indexes from current sample that can be replaced.
+	SelectionVectorHelper GetReplacementIndexes(idx_t sample_chunk_offset, idx_t theoretical_chunk_length);
+
 	void Serialize(Serializer &serializer) const override;
 	static unique_ptr<BlockingSample> Deserialize(Deserializer &deserializer);
 
 private:
-	//! Replace a single element of the input
-	void ReplaceElement(DataChunk &input, idx_t index_in_chunk, double with_weight = -1);
-	void InitializeReservoir(DataChunk &input);
-	//! Fills the reservoir up until sample_count entries, returns how many entries are still required
-	idx_t FillReservoir(DataChunk &input);
+	// when we serialize, we may have collected too many samples since we fill a standard vector size, then
+	// truncate if the table is still <=204800 values. The problem is, in our weights, we store indexes into
+	// the selection vector. If throw away values at selection vector index i = 5 , we need to update all indexes
+	// i > 5. Otherwise we will have indexes in the weights that are greater than the length of our sample.
+	void NormalizeWeights();
+
+	SelectionVectorHelper GetReplacementIndexesSlow(const idx_t sample_chunk_offset, const idx_t chunk_length);
+	SelectionVectorHelper GetReplacementIndexesFast(const idx_t sample_chunk_offset, const idx_t chunk_length);
+	void SimpleMerge(ReservoirSample &other);
+	void WeightedMerge(ReservoirSample &other_sample);
+
+	// Helper methods for Shrink().
+	// Shrink has different logic depending on if the Reservoir sample is still in
+	// "Random" mode or in "reservoir" mode. This function creates a new sample chunk
+	// to copy the old sample chunk into
+	unique_ptr<ReservoirChunk> CreateNewSampleChunk(vector<LogicalType> &types, idx_t size) const;
+
+	// Get a vector where each index is a random int in the range 0, size.
+	// This is used to shuffle selection vector indexes
+	vector<uint32_t> GetRandomizedVector(uint32_t range, uint32_t size) const;
 
-public:
-	Allocator &allocator;
-	//! The size of the reservoir sample.
-	//! when calculating percentages, it is set to reservoir_threshold * percentage
-	//! when explicit number used, sample_count = number
 	idx_t sample_count;
-	bool reservoir_initialized;
-
-	//! The current reservoir
-	unique_ptr<DataChunk> reservoir_data_chunk;
+	Allocator &allocator;
 	unique_ptr<ReservoirChunk> reservoir_chunk;
+	bool stats_sample;
+	SelectionVector sel;
+	idx_t sel_size;
 };
 
 //! The reservoir sample sample_size class maintains a streaming sample of variable size
@@ -155,15 +281,16 @@ class ReservoirSamplePercentage : public BlockingSample {
 public:
 	static constexpr const SampleType TYPE = SampleType::RESERVOIR_PERCENTAGE_SAMPLE;
 
-public:
 	ReservoirSamplePercentage(Allocator &allocator, double percentage, int64_t seed = -1);
+	ReservoirSamplePercentage(double percentage, int64_t seed, idx_t reservoir_sample_size);
 	explicit ReservoirSamplePercentage(double percentage, int64_t seed = -1);
 
 	//! Add a chunk of data to the sample
 	void AddToReservoir(DataChunk &input) override;
 
-	//! Fetches a chunk from the sample. Note that this method is destructive and should only be used after the
-	//! sample is completely built.
+	unique_ptr<BlockingSample> Copy() const override;
+
+	//! Fetches a chunk from the sample. If destory = true this method is descructive
 	unique_ptr<DataChunk> GetChunk() override;
 	void Finalize() override;
 
@@ -182,9 +309,11 @@ class ReservoirSamplePercentage : public BlockingSample {
 
 	//! The set of finished samples of the reservoir sample
 	vector<unique_ptr<ReservoirSample>> finished_samples;
+
 	//! The amount of tuples that have been processed so far (not put in the reservoir, just processed)
 	idx_t current_count = 0;
-	//! Whether or not the stream is finalized. The stream is automatically finalized on the first call to GetChunk();
+	//! Whether or not the stream is finalized. The stream is automatically finalized on the first call to
+	//! GetChunkAndShrink();
 	bool is_finalized;
 };
 
diff --git a/src/include/duckdb/function/table/system_functions.hpp b/src/include/duckdb/function/table/system_functions.hpp
index f74dc466e067..689b55201d0e 100644
--- a/src/include/duckdb/function/table/system_functions.hpp
+++ b/src/include/duckdb/function/table/system_functions.hpp
@@ -107,6 +107,10 @@ struct DuckDBTablesFun {
 	static void RegisterFunction(BuiltinFunctions &set);
 };
 
+struct DuckDBTableSample {
+	static void RegisterFunction(BuiltinFunctions &set);
+};
+
 struct DuckDBTemporaryFilesFun {
 	static void RegisterFunction(BuiltinFunctions &set);
 };
diff --git a/src/include/duckdb/main/client_data.hpp b/src/include/duckdb/main/client_data.hpp
index 777859755e1a..b3e326267897 100644
--- a/src/include/duckdb/main/client_data.hpp
+++ b/src/include/duckdb/main/client_data.hpp
@@ -27,7 +27,7 @@ class QueryProfiler;
 class PreparedStatementData;
 class SchemaCatalogEntry;
 class HTTPLogger;
-struct RandomEngine;
+class RandomEngine;
 
 struct ClientData {
 	explicit ClientData(ClientContext &context);
diff --git a/src/include/duckdb/storage/data_table.hpp b/src/include/duckdb/storage/data_table.hpp
index 10e126e896ce..39795ed1b907 100644
--- a/src/include/duckdb/storage/data_table.hpp
+++ b/src/include/duckdb/storage/data_table.hpp
@@ -187,6 +187,9 @@ class DataTable {
 
 	//! Get statistics of a physical column within the table
 	unique_ptr<BaseStatistics> GetStatistics(ClientContext &context, column_t column_id);
+
+	//! Get table sample
+	unique_ptr<BlockingSample> GetSample();
 	//! Sets statistics of a physical column within the table
 	void SetDistinct(column_t column_id, unique_ptr<DistinctStatistics> distinct_stats);
 
diff --git a/src/include/duckdb/storage/serialization/nodes.json b/src/include/duckdb/storage/serialization/nodes.json
index dc4665959f36..59791a85969b 100644
--- a/src/include/duckdb/storage/serialization/nodes.json
+++ b/src/include/duckdb/storage/serialization/nodes.json
@@ -281,7 +281,7 @@
         "type": "unique_ptr<ReservoirChunk>"
       }
     ],
-    "constructor": ["sample_count"]
+    "constructor": ["sample_count", "reservoir_chunk"]
   },
   {
     "class": "ReservoirSamplePercentage",
@@ -331,7 +331,6 @@
     ],
     "pointer_type": "none"
   },
-
   {
     "class": "PivotColumnEntry",
     "members": [
diff --git a/src/include/duckdb/storage/table/row_group_collection.hpp b/src/include/duckdb/storage/table/row_group_collection.hpp
index 5c47dcd62179..19aa6452038c 100644
--- a/src/include/duckdb/storage/table/row_group_collection.hpp
+++ b/src/include/duckdb/storage/table/row_group_collection.hpp
@@ -124,6 +124,7 @@ class RowGroupCollection {
 
 	void CopyStats(TableStatistics &stats);
 	unique_ptr<BaseStatistics> CopyStats(column_t column_id);
+	unique_ptr<BlockingSample> GetSample();
 	void SetDistinct(column_t column_id, unique_ptr<DistinctStatistics> distinct_stats);
 
 	AttachedDatabase &GetAttached();
diff --git a/src/include/duckdb/storage/table/table_statistics.hpp b/src/include/duckdb/storage/table/table_statistics.hpp
index 633d469463c2..628023dfd4dc 100644
--- a/src/include/duckdb/storage/table/table_statistics.hpp
+++ b/src/include/duckdb/storage/table/table_statistics.hpp
@@ -48,6 +48,14 @@ class TableStatistics {
 	//! Get a reference to the stats - this requires us to hold the lock.
 	//! The reference can only be safely accessed while the lock is held
 	ColumnStatistics &GetStats(TableStatisticsLock &lock, idx_t i);
+	//! Get a reference to the table sample - this requires us to hold the lock.
+	BlockingSample &GetTableSampleRef(TableStatisticsLock &lock);
+	//! Take ownership of the sample, needed for merging. Requires the lock
+	unique_ptr<BlockingSample> GetTableSample(TableStatisticsLock &lock);
+	void SetTableSample(TableStatisticsLock &lock, unique_ptr<BlockingSample> sample);
+
+	void DestroyTableSample(TableStatisticsLock &lock) const;
+	void AppendToTableSample(TableStatisticsLock &lock, unique_ptr<BlockingSample> sample);
 
 	bool Empty();
 
@@ -62,7 +70,6 @@ class TableStatistics {
 	//! Column statistics
 	vector<shared_ptr<ColumnStatistics>> column_stats;
 	//! The table sample
-	//! Sample for table
 	unique_ptr<BlockingSample> table_sample;
 };
 
diff --git a/src/storage/data_table.cpp b/src/storage/data_table.cpp
index d8a2a7c00597..81bf50100b13 100644
--- a/src/storage/data_table.cpp
+++ b/src/storage/data_table.cpp
@@ -1517,6 +1517,10 @@ void DataTable::SetDistinct(column_t column_id, unique_ptr<DistinctStatistics> d
 	row_groups->SetDistinct(column_id, std::move(distinct_stats));
 }
 
+unique_ptr<BlockingSample> DataTable::GetSample() {
+	return row_groups->GetSample();
+}
+
 //===--------------------------------------------------------------------===//
 // Checkpoint
 //===--------------------------------------------------------------------===//
@@ -1533,8 +1537,8 @@ void DataTable::Checkpoint(TableDataWriter &writer, Serializer &serializer) {
 	TableStatistics global_stats;
 	row_groups->CopyStats(global_stats);
 	row_groups->Checkpoint(writer, global_stats);
-
 	// The row group payload data has been written. Now write:
+	//   sample
 	//   column stats
 	//   row-group pointers
 	//   table pointer
diff --git a/src/storage/serialization/serialize_nodes.cpp b/src/storage/serialization/serialize_nodes.cpp
index 440976e8ec61..2c7230383850 100644
--- a/src/storage/serialization/serialize_nodes.cpp
+++ b/src/storage/serialization/serialize_nodes.cpp
@@ -596,8 +596,8 @@ void ReservoirSample::Serialize(Serializer &serializer) const {
 
 unique_ptr<BlockingSample> ReservoirSample::Deserialize(Deserializer &deserializer) {
 	auto sample_count = deserializer.ReadPropertyWithDefault<idx_t>(200, "sample_count");
-	auto result = duckdb::unique_ptr<ReservoirSample>(new ReservoirSample(sample_count));
-	deserializer.ReadPropertyWithDefault<unique_ptr<ReservoirChunk>>(201, "reservoir_chunk", result->reservoir_chunk);
+	auto reservoir_chunk = deserializer.ReadPropertyWithDefault<unique_ptr<ReservoirChunk>>(201, "reservoir_chunk");
+	auto result = duckdb::unique_ptr<ReservoirSample>(new ReservoirSample(sample_count, std::move(reservoir_chunk)));
 	return std::move(result);
 }
 
diff --git a/src/storage/table/row_group_collection.cpp b/src/storage/table/row_group_collection.cpp
index f2ff23fe6afd..90a7236ff447 100644
--- a/src/storage/table/row_group_collection.cpp
+++ b/src/storage/table/row_group_collection.cpp
@@ -1,5 +1,4 @@
 #include "duckdb/storage/table/row_group_collection.hpp"
-
 #include "duckdb/common/serializer/binary_deserializer.hpp"
 #include "duckdb/execution/expression_executor.hpp"
 #include "duckdb/execution/index/bound_index.hpp"
@@ -397,11 +396,20 @@ bool RowGroupCollection::Append(DataChunk &chunk, TableAppendState &state) {
 		}
 	}
 	state.current_row += row_t(total_append_count);
+
 	auto local_stats_lock = state.stats.GetLock();
+
 	for (idx_t col_idx = 0; col_idx < types.size(); col_idx++) {
 		auto &column_stats = state.stats.GetStats(*local_stats_lock, col_idx);
 		column_stats.UpdateDistinctStatistics(chunk.data[col_idx], chunk.size(), state.hashes);
 	}
+
+	auto &table_sample = state.stats.GetTableSampleRef(*local_stats_lock);
+	if (!table_sample.destroyed) {
+		D_ASSERT(table_sample.type == SampleType::RESERVOIR_SAMPLE);
+		table_sample.AddToReservoir(chunk);
+	}
+
 	return new_row_group;
 }
 
@@ -421,8 +429,8 @@ void RowGroupCollection::FinalizeAppend(TransactionData transaction, TableAppend
 	state.total_append_count = 0;
 	state.start_row_group = nullptr;
 
-	auto global_stats_lock = stats.GetLock();
 	auto local_stats_lock = state.stats.GetLock();
+	auto global_stats_lock = stats.GetLock();
 	for (idx_t col_idx = 0; col_idx < types.size(); col_idx++) {
 		auto &global_stats = stats.GetStats(*global_stats_lock, col_idx);
 		if (!global_stats.HasDistinctStats()) {
@@ -435,6 +443,22 @@ void RowGroupCollection::FinalizeAppend(TransactionData transaction, TableAppend
 		global_stats.DistinctStats().Merge(local_stats.DistinctStats());
 	}
 
+	auto local_sample = state.stats.GetTableSample(*local_stats_lock);
+	auto global_sample = stats.GetTableSample(*global_stats_lock);
+
+	if (local_sample && global_sample) {
+		D_ASSERT(global_sample->type == SampleType::RESERVOIR_SAMPLE);
+		auto &reservoir_sample = global_sample->Cast<ReservoirSample>();
+		reservoir_sample.Merge(std::move(local_sample));
+		// initialize the thread local sample again
+		auto new_local_sample = make_uniq<ReservoirSample>(reservoir_sample.GetSampleCount());
+		state.stats.SetTableSample(*local_stats_lock, std::move(new_local_sample));
+		stats.SetTableSample(*global_stats_lock, std::move(global_sample));
+	} else {
+		state.stats.SetTableSample(*local_stats_lock, std::move(local_sample));
+		stats.SetTableSample(*global_stats_lock, std::move(global_sample));
+	}
+
 	Verify();
 }
 
@@ -582,6 +606,11 @@ idx_t RowGroupCollection::Delete(TransactionData transaction, DataTable &table,
 		}
 		delete_count += row_group->Delete(transaction, table, ids + start, pos - start);
 	} while (pos < count);
+
+	// When deleting destroy the sample.
+	auto stats_guard = stats.GetLock();
+	stats.DestroyTableSample(*stats_guard);
+
 	return delete_count;
 }
 
@@ -619,6 +648,9 @@ void RowGroupCollection::Update(TransactionData transaction, row_t *ids, const v
 			stats.MergeStats(*l, column_id.index, *row_group->GetStatistics(column_id.index));
 		}
 	} while (pos < updates.size());
+	// on update destroy the sample
+	auto stats_guard = stats.GetLock();
+	stats.DestroyTableSample(*stats_guard);
 }
 
 void RowGroupCollection::RemoveFromIndexes(TableIndexList &indexes, Vector &row_identifiers, idx_t count) {
@@ -1102,6 +1134,9 @@ shared_ptr<RowGroupCollection> RowGroupCollection::AddColumn(ClientContext &cont
 
 		result->row_groups->AppendSegment(std::move(new_row_group));
 	}
+	// When adding a column destroy the sample
+	stats.DestroyTableSample(*lock);
+
 	return result;
 }
 
@@ -1114,6 +1149,9 @@ shared_ptr<RowGroupCollection> RowGroupCollection::RemoveColumn(idx_t col_idx) {
 	                                                  total_rows.load(), row_group_size);
 	result->stats.InitializeRemoveColumn(stats, col_idx);
 
+	auto result_lock = result->stats.GetLock();
+	result->stats.DestroyTableSample(*result_lock);
+
 	for (auto &current_row_group : row_groups->Segments()) {
 		auto new_row_group = current_row_group.RemoveColumn(*result, col_idx);
 		result->row_groups->AppendSegment(std::move(new_row_group));
@@ -1160,7 +1198,6 @@ shared_ptr<RowGroupCollection> RowGroupCollection::AlterType(ClientContext &cont
 		new_row_group->MergeIntoStatistics(changed_idx, changed_stats.Statistics());
 		result->row_groups->AppendSegment(std::move(new_row_group));
 	}
-
 	return result;
 }
 
@@ -1207,7 +1244,7 @@ void RowGroupCollection::VerifyNewConstraint(DataTable &parent, const BoundConst
 
 //===--------------------------------------------------------------------===//
 // Statistics
-//===--------------------------------------------------------------------===//
+//===---------------------------------------------------------------r-----===//
 void RowGroupCollection::CopyStats(TableStatistics &other_stats) {
 	stats.CopyStats(other_stats);
 }
@@ -1216,6 +1253,18 @@ unique_ptr<BaseStatistics> RowGroupCollection::CopyStats(column_t column_id) {
 	return stats.CopyStats(column_id);
 }
 
+unique_ptr<BlockingSample> RowGroupCollection::GetSample() {
+	auto lock = stats.GetLock();
+	auto &sample = stats.GetTableSampleRef(*lock);
+	if (!sample.destroyed) {
+		D_ASSERT(sample.type == SampleType::RESERVOIR_SAMPLE);
+		auto ret = sample.Copy();
+		ret->Cast<ReservoirSample>().EvictOverBudgetSamples();
+		return ret;
+	}
+	return nullptr;
+}
+
 void RowGroupCollection::SetDistinct(column_t column_id, unique_ptr<DistinctStatistics> distinct_stats) {
 	D_ASSERT(column_id != COLUMN_IDENTIFIER_ROW_ID);
 	auto stats_lock = stats.GetLock();
diff --git a/src/storage/table/table_statistics.cpp b/src/storage/table/table_statistics.cpp
index b51c445d0dc2..e56f98440022 100644
--- a/src/storage/table/table_statistics.cpp
+++ b/src/storage/table/table_statistics.cpp
@@ -1,16 +1,23 @@
 #include "duckdb/storage/table/table_statistics.hpp"
-#include "duckdb/storage/table/persistent_table_data.hpp"
-#include "duckdb/common/serializer/serializer.hpp"
+
 #include "duckdb/common/serializer/deserializer.hpp"
+#include "duckdb/common/serializer/serializer.hpp"
 #include "duckdb/execution/reservoir_sample.hpp"
+#include "duckdb/storage/table/persistent_table_data.hpp"
 
 namespace duckdb {
 
 void TableStatistics::Initialize(const vector<LogicalType> &types, PersistentTableData &data) {
 	D_ASSERT(Empty());
+	D_ASSERT(!table_sample);
 
 	stats_lock = make_shared_ptr<mutex>();
 	column_stats = std::move(data.table_stats.column_stats);
+	if (data.table_stats.table_sample) {
+		table_sample = std::move(data.table_stats.table_sample);
+	} else {
+		table_sample = make_uniq<ReservoirSample>(static_cast<idx_t>(FIXED_SAMPLE_SIZE));
+	}
 	if (column_stats.size() != types.size()) { // LCOV_EXCL_START
 		throw IOException("Table statistics column count is not aligned with table column count. Corrupt file?");
 	} // LCOV_EXCL_STOP
@@ -18,8 +25,10 @@ void TableStatistics::Initialize(const vector<LogicalType> &types, PersistentTab
 
 void TableStatistics::InitializeEmpty(const vector<LogicalType> &types) {
 	D_ASSERT(Empty());
+	D_ASSERT(!table_sample);
 
 	stats_lock = make_shared_ptr<mutex>();
+	table_sample = make_uniq<ReservoirSample>(static_cast<idx_t>(FIXED_SAMPLE_SIZE));
 	for (auto &type : types) {
 		column_stats.push_back(ColumnStatistics::CreateEmptyStats(type));
 	}
@@ -35,6 +44,12 @@ void TableStatistics::InitializeAddColumn(TableStatistics &parent, const Logical
 		column_stats.push_back(parent.column_stats[i]);
 	}
 	column_stats.push_back(ColumnStatistics::CreateEmptyStats(new_column_type));
+	if (parent.table_sample) {
+		table_sample = std::move(parent.table_sample);
+	}
+	if (table_sample) {
+		table_sample->Destroy();
+	}
 }
 
 void TableStatistics::InitializeRemoveColumn(TableStatistics &parent, idx_t removed_column) {
@@ -48,6 +63,12 @@ void TableStatistics::InitializeRemoveColumn(TableStatistics &parent, idx_t remo
 			column_stats.push_back(parent.column_stats[i]);
 		}
 	}
+	if (parent.table_sample) {
+		table_sample = std::move(parent.table_sample);
+	}
+	if (table_sample) {
+		table_sample->Destroy();
+	}
 }
 
 void TableStatistics::InitializeAlterType(TableStatistics &parent, idx_t changed_idx, const LogicalType &new_type) {
@@ -63,6 +84,12 @@ void TableStatistics::InitializeAlterType(TableStatistics &parent, idx_t changed
 			column_stats.push_back(parent.column_stats[i]);
 		}
 	}
+	if (parent.table_sample) {
+		table_sample = std::move(parent.table_sample);
+	}
+	if (table_sample) {
+		table_sample->Destroy();
+	}
 }
 
 void TableStatistics::InitializeAddConstraint(TableStatistics &parent) {
@@ -79,6 +106,21 @@ void TableStatistics::InitializeAddConstraint(TableStatistics &parent) {
 void TableStatistics::MergeStats(TableStatistics &other) {
 	auto l = GetLock();
 	D_ASSERT(column_stats.size() == other.column_stats.size());
+	if (table_sample) {
+		if (other.table_sample) {
+			D_ASSERT(table_sample->type == SampleType::RESERVOIR_SAMPLE);
+			auto &this_reservoir = table_sample->Cast<ReservoirSample>();
+			D_ASSERT(other.table_sample->type == SampleType::RESERVOIR_SAMPLE);
+			this_reservoir.Merge(std::move(other.table_sample));
+		}
+		// if no other.table sample, do nothig
+	} else {
+		if (other.table_sample) {
+			auto &other_reservoir = other.table_sample->Cast<ReservoirSample>();
+			auto other_table_sample_copy = other_reservoir.Copy();
+			table_sample = std::move(other_table_sample_copy);
+		}
+	}
 	for (idx_t i = 0; i < column_stats.size(); i++) {
 		if (column_stats[i]) {
 			D_ASSERT(other.column_stats[i]);
@@ -100,6 +142,25 @@ ColumnStatistics &TableStatistics::GetStats(TableStatisticsLock &lock, idx_t i)
 	return *column_stats[i];
 }
 
+BlockingSample &TableStatistics::GetTableSampleRef(TableStatisticsLock &lock) {
+	D_ASSERT(table_sample);
+	return *table_sample;
+}
+
+unique_ptr<BlockingSample> TableStatistics::GetTableSample(TableStatisticsLock &lock) {
+	return std::move(table_sample);
+}
+
+void TableStatistics::SetTableSample(TableStatisticsLock &lock, unique_ptr<BlockingSample> sample) {
+	table_sample = std::move(sample);
+}
+
+void TableStatistics::DestroyTableSample(TableStatisticsLock &lock) const {
+	if (table_sample) {
+		table_sample->Destroy();
+	}
+}
+
 unique_ptr<BaseStatistics> TableStatistics::CopyStats(idx_t i) {
 	lock_guard<mutex> l(*stats_lock);
 	auto result = column_stats[i]->Statistics().Copy();
@@ -120,11 +181,25 @@ void TableStatistics::CopyStats(TableStatisticsLock &lock, TableStatistics &othe
 	for (auto &stats : column_stats) {
 		other.column_stats.push_back(stats->Copy());
 	}
+
+	if (table_sample) {
+		D_ASSERT(table_sample->type == SampleType::RESERVOIR_SAMPLE);
+		auto &res = table_sample->Cast<ReservoirSample>();
+		other.table_sample = res.Copy();
+	}
 }
 
 void TableStatistics::Serialize(Serializer &serializer) const {
 	serializer.WriteProperty(100, "column_stats", column_stats);
-	serializer.WritePropertyWithDefault<unique_ptr<BlockingSample>>(101, "table_sample", table_sample, nullptr);
+	unique_ptr<BlockingSample> to_serialize = nullptr;
+	if (table_sample) {
+		D_ASSERT(table_sample->type == SampleType::RESERVOIR_SAMPLE);
+		auto &reservoir_sample = table_sample->Cast<ReservoirSample>();
+		to_serialize = unique_ptr_cast<BlockingSample, ReservoirSample>(reservoir_sample.Copy());
+		auto &res_serialize = to_serialize->Cast<ReservoirSample>();
+		res_serialize.EvictOverBudgetSamples();
+	}
+	serializer.WritePropertyWithDefault<unique_ptr<BlockingSample>>(101, "table_sample", to_serialize, nullptr);
 }
 
 void TableStatistics::Deserialize(Deserializer &deserializer, ColumnList &columns) {
@@ -142,8 +217,19 @@ void TableStatistics::Deserialize(Deserializer &deserializer, ColumnList &column
 
 		deserializer.Unset<LogicalType>();
 	});
-	table_sample =
-	    deserializer.ReadPropertyWithExplicitDefault<unique_ptr<BlockingSample>>(101, "table_sample", nullptr);
+	table_sample = deserializer.ReadPropertyWithDefault<unique_ptr<BlockingSample>>(101, "table_sample");
+	if (table_sample) {
+		D_ASSERT(table_sample->type == SampleType::RESERVOIR_SAMPLE);
+#ifdef DEBUG
+		if (table_sample) {
+			auto &reservoir_sample = table_sample->Cast<ReservoirSample>();
+			reservoir_sample.Verify();
+		}
+#endif
+	} else {
+		table_sample = make_uniq<ReservoirSample>(static_cast<idx_t>(FIXED_SAMPLE_SIZE));
+		table_sample->Destroy();
+	}
 }
 
 unique_ptr<TableStatisticsLock> TableStatistics::GetLock() {
