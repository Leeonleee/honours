You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Memory errors happening during a parquet query over a range of files
**What does happen?**
A number of different errors happen during one particular query:
`duckdb::OutOfRangeException: what():  Out of Range Error: Not enough memory to complete operation: could not allocate block of 285751568 bytes`
`Error: IO Error: Could not write all bytes to file ".tmp/4611686018427424834.block": wanted=2687930368 wrote=2147479552`
`SegmentationFault`

Plus, a lot of memory being used (up to 30GB).

The query: `SELECT * FROM parquet_scan('<folder>/*.parquet') AS curr_crawl WHERE url <> '' AND ((css <> TRUE OR css IS NULL) AND (js <> TRUE OR js IS NULL) AND (is_image <> TRUE OR is_image IS NULL) AND internal = TRUE) ORDER BY deeprank DESC`.

The files total 777 mb in size.

**What should happen?**
The query should be executed successfully. 

**To Reproduce**
There are a couple of ways to reproduce.

Running the query with `PRAGMA threads=6;`:
1. `PRAGMA threads=6;`
2. `SELECT * FROM parquet_scan('<folder>/*.parquet') AS curr_crawl WHERE url <> '' AND ((css <> TRUE OR css IS NULL) AND (js <> TRUE OR js IS NULL) AND (is_image <> TRUE OR is_image IS NULL) AND internal = TRUE) ORDER BY deeprank DESC`
3.  One of the errors above happens, or execution takes 15+ mins and not ending and 30GB of memory being used or execution succeeding in adequate time but again with 30GB being used.

Not using `PRAGMA threads=6;`:
1. `SELECT * FROM parquet_scan('<folder>/*.parquet') AS curr_crawl WHERE url <> '' AND ((css <> TRUE OR css IS NULL) AND (js <> TRUE OR js IS NULL) AND (is_image <> TRUE OR is_image IS NULL) AND internal = TRUE) ORDER BY deeprank DESC`
2. The query might complete successfully or might get `Error: IO Error: Could not write all bytes to file ".tmp/4611686018427424834.block": wanted=2687930368 wrote=2147479552` or `SegmentationFault`

I'm sending the files to you guys.

**Environment (please complete the following information):**
 - OS: Amazon Linux 2
 - DuckDB Version: latest master/0.0.27

**Before submitting**
- [x] Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?
- [x] Have you tried this on the latest `master` branch? In case you cannot compile, you may find some binaries here: https://github.com/duckdb/duckdb/releases/tag/master-builds


</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
2: 
3: ![.github/workflows/main.yml](https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![codecov](https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN)](https://codecov.io/gh/duckdb/duckdb)
6: 
7: 
8: ## Installation
9: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
10: 
11: ## Development
12: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
13: 
14: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
15: 
16: 
[end of README.md]
[start of src/common/exception.cpp]
1: #include "duckdb/common/exception.hpp"
2: #include "duckdb/common/string_util.hpp"
3: #include "duckdb/common/types.hpp"
4: #include "duckdb/common/to_string.hpp"
5: 
6: namespace duckdb {
7: 
8: Exception::Exception(const string &msg) : std::exception(), type(ExceptionType::INVALID) {
9: 	exception_message_ = msg;
10: }
11: 
12: Exception::Exception(ExceptionType exception_type, const string &message) : std::exception(), type(exception_type) {
13: 	exception_message_ = ExceptionTypeToString(exception_type) + " Error: " + message;
14: }
15: 
16: const char *Exception::what() const noexcept {
17: 	return exception_message_.c_str();
18: }
19: 
20: string Exception::ConstructMessageRecursive(const string &msg, vector<ExceptionFormatValue> &values) {
21: 	return ExceptionFormatValue::Format(msg, values);
22: }
23: 
24: string Exception::ExceptionTypeToString(ExceptionType type) {
25: 	switch (type) {
26: 	case ExceptionType::INVALID:
27: 		return "Invalid";
28: 	case ExceptionType::OUT_OF_RANGE:
29: 		return "Out of Range";
30: 	case ExceptionType::CONVERSION:
31: 		return "Conversion";
32: 	case ExceptionType::UNKNOWN_TYPE:
33: 		return "Unknown Type";
34: 	case ExceptionType::DECIMAL:
35: 		return "Decimal";
36: 	case ExceptionType::MISMATCH_TYPE:
37: 		return "Mismatch Type";
38: 	case ExceptionType::DIVIDE_BY_ZERO:
39: 		return "Divide by Zero";
40: 	case ExceptionType::OBJECT_SIZE:
41: 		return "Object Size";
42: 	case ExceptionType::INVALID_TYPE:
43: 		return "Invalid type";
44: 	case ExceptionType::SERIALIZATION:
45: 		return "Serialization";
46: 	case ExceptionType::TRANSACTION:
47: 		return "TransactionContext";
48: 	case ExceptionType::NOT_IMPLEMENTED:
49: 		return "Not implemented";
50: 	case ExceptionType::EXPRESSION:
51: 		return "Expression";
52: 	case ExceptionType::CATALOG:
53: 		return "Catalog";
54: 	case ExceptionType::PARSER:
55: 		return "Parser";
56: 	case ExceptionType::BINDER:
57: 		return "Binder";
58: 	case ExceptionType::PLANNER:
59: 		return "Planner";
60: 	case ExceptionType::SCHEDULER:
61: 		return "Scheduler";
62: 	case ExceptionType::EXECUTOR:
63: 		return "Executor";
64: 	case ExceptionType::CONSTRAINT:
65: 		return "Constraint";
66: 	case ExceptionType::INDEX:
67: 		return "Index";
68: 	case ExceptionType::STAT:
69: 		return "Stat";
70: 	case ExceptionType::CONNECTION:
71: 		return "Connection";
72: 	case ExceptionType::SYNTAX:
73: 		return "Syntax";
74: 	case ExceptionType::SETTINGS:
75: 		return "Settings";
76: 	case ExceptionType::OPTIMIZER:
77: 		return "Optimizer";
78: 	case ExceptionType::NULL_POINTER:
79: 		return "NullPointer";
80: 	case ExceptionType::IO:
81: 		return "IO";
82: 	case ExceptionType::INTERRUPT:
83: 		return "INTERRUPT";
84: 	case ExceptionType::FATAL:
85: 		return "FATAL";
86: 	case ExceptionType::INTERNAL:
87: 		return "INTERNAL";
88: 	case ExceptionType::INVALID_INPUT:
89: 		return "Invalid Input";
90: 	default:
91: 		return "Unknown";
92: 	}
93: }
94: 
95: CastException::CastException(const PhysicalType orig_type, const PhysicalType new_type)
96:     : Exception(ExceptionType::CONVERSION,
97:                 "Type " + TypeIdToString(orig_type) + " can't be cast as " + TypeIdToString(new_type)) {
98: }
99: 
100: CastException::CastException(const LogicalType &orig_type, const LogicalType &new_type)
101:     : Exception(ExceptionType::CONVERSION,
102:                 "Type " + orig_type.ToString() + " can't be cast as " + new_type.ToString()) {
103: }
104: 
105: ValueOutOfRangeException::ValueOutOfRangeException(const int64_t value, const PhysicalType orig_type,
106:                                                    const PhysicalType new_type)
107:     : Exception(ExceptionType::CONVERSION, "Type " + TypeIdToString(orig_type) + " with value " +
108:                                                to_string((intmax_t)value) +
109:                                                " can't be cast because the value is out of range "
110:                                                "for the destination type " +
111:                                                TypeIdToString(new_type)) {
112: }
113: 
114: ValueOutOfRangeException::ValueOutOfRangeException(const double value, const PhysicalType orig_type,
115:                                                    const PhysicalType new_type)
116:     : Exception(ExceptionType::CONVERSION, "Type " + TypeIdToString(orig_type) + " with value " + to_string(value) +
117:                                                " can't be cast because the value is out of range "
118:                                                "for the destination type " +
119:                                                TypeIdToString(new_type)) {
120: }
121: 
122: ValueOutOfRangeException::ValueOutOfRangeException(const hugeint_t value, const PhysicalType orig_type,
123:                                                    const PhysicalType new_type)
124:     : Exception(ExceptionType::CONVERSION, "Type " + TypeIdToString(orig_type) + " with value " + value.ToString() +
125:                                                " can't be cast because the value is out of range "
126:                                                "for the destination type " +
127:                                                TypeIdToString(new_type)) {
128: }
129: 
130: ValueOutOfRangeException::ValueOutOfRangeException(const PhysicalType var_type, const idx_t length)
131:     : Exception(ExceptionType::OUT_OF_RANGE,
132:                 "The value is too long to fit into type " + TypeIdToString(var_type) + "(" + to_string(length) + ")") {
133: }
134: 
135: ConversionException::ConversionException(const string &msg) : Exception(ExceptionType::CONVERSION, msg) {
136: }
137: 
138: InvalidTypeException::InvalidTypeException(PhysicalType type, const string &msg)
139:     : Exception(ExceptionType::INVALID_TYPE, "Invalid Type [" + TypeIdToString(type) + "]: " + msg) {
140: }
141: 
142: InvalidTypeException::InvalidTypeException(const LogicalType &type, const string &msg)
143:     : Exception(ExceptionType::INVALID_TYPE, "Invalid Type [" + type.ToString() + "]: " + msg) {
144: }
145: 
146: TypeMismatchException::TypeMismatchException(const PhysicalType type_1, const PhysicalType type_2, const string &msg)
147:     : Exception(ExceptionType::MISMATCH_TYPE,
148:                 "Type " + TypeIdToString(type_1) + " does not match with " + TypeIdToString(type_2) + ". " + msg) {
149: }
150: 
151: TypeMismatchException::TypeMismatchException(const LogicalType &type_1, const LogicalType &type_2, const string &msg)
152:     : Exception(ExceptionType::MISMATCH_TYPE,
153:                 "Type " + type_1.ToString() + " does not match with " + type_2.ToString() + ". " + msg) {
154: }
155: 
156: TransactionException::TransactionException(const string &msg) : Exception(ExceptionType::TRANSACTION, msg) {
157: }
158: 
159: NotImplementedException::NotImplementedException(const string &msg) : Exception(ExceptionType::NOT_IMPLEMENTED, msg) {
160: }
161: 
162: OutOfRangeException::OutOfRangeException(const string &msg) : Exception(ExceptionType::OUT_OF_RANGE, msg) {
163: }
164: 
165: CatalogException::CatalogException(const string &msg) : StandardException(ExceptionType::CATALOG, msg) {
166: }
167: 
168: ParserException::ParserException(const string &msg) : StandardException(ExceptionType::PARSER, msg) {
169: }
170: 
171: SyntaxException::SyntaxException(const string &msg) : Exception(ExceptionType::SYNTAX, msg) {
172: }
173: 
174: ConstraintException::ConstraintException(const string &msg) : Exception(ExceptionType::CONSTRAINT, msg) {
175: }
176: 
177: BinderException::BinderException(const string &msg) : StandardException(ExceptionType::BINDER, msg) {
178: }
179: 
180: IOException::IOException(const string &msg) : Exception(ExceptionType::IO, msg) {
181: }
182: 
183: SerializationException::SerializationException(const string &msg) : Exception(ExceptionType::SERIALIZATION, msg) {
184: }
185: 
186: SequenceException::SequenceException(const string &msg) : Exception(ExceptionType::SERIALIZATION, msg) {
187: }
188: 
189: InterruptException::InterruptException() : Exception(ExceptionType::INTERRUPT, "Interrupted!") {
190: }
191: 
192: FatalException::FatalException(const string &msg) : Exception(ExceptionType::FATAL, msg) {
193: }
194: 
195: InternalException::InternalException(const string &msg) : Exception(ExceptionType::INTERNAL, msg) {
196: }
197: 
198: InvalidInputException::InvalidInputException(const string &msg) : Exception(ExceptionType::INVALID_INPUT, msg) {
199: }
200: 
201: } // namespace duckdb
[end of src/common/exception.cpp]
[start of src/common/row_operations/row_heap_scatter.cpp]
1: #include "duckdb/common/helper.hpp"
2: #include "duckdb/common/row_operations/row_operations.hpp"
3: #include "duckdb/common/types/vector.hpp"
4: 
5: namespace duckdb {
6: 
7: using ValidityBytes = TemplatedValidityMask<uint8_t>;
8: 
9: static void ComputeStringEntrySizes(VectorData &vdata, idx_t entry_sizes[], const idx_t ser_count,
10:                                     const SelectionVector &sel, const idx_t offset) {
11: 	auto strings = (string_t *)vdata.data;
12: 	for (idx_t i = 0; i < ser_count; i++) {
13: 		auto idx = sel.get_index(i);
14: 		auto str_idx = vdata.sel->get_index(idx) + offset;
15: 		if (vdata.validity.RowIsValid(str_idx)) {
16: 			entry_sizes[i] += sizeof(uint32_t) + strings[str_idx].GetSize();
17: 		}
18: 	}
19: }
20: 
21: static void ComputeStructEntrySizes(Vector &v, idx_t entry_sizes[], idx_t vcount, idx_t ser_count,
22:                                     const SelectionVector &sel, idx_t offset) {
23: 	// obtain child vectors
24: 	idx_t num_children;
25: 	vector<Vector> struct_vectors;
26: 	if (v.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
27: 		auto &child = DictionaryVector::Child(v);
28: 		auto &dict_sel = DictionaryVector::SelVector(v);
29: 		auto &children = StructVector::GetEntries(child);
30: 		num_children = children.size();
31: 		for (auto &struct_child : children) {
32: 			Vector struct_vector(*struct_child, dict_sel, vcount);
33: 			struct_vectors.push_back(move(struct_vector));
34: 		}
35: 	} else {
36: 		auto &children = StructVector::GetEntries(v);
37: 		num_children = children.size();
38: 		for (auto &struct_child : children) {
39: 			Vector struct_vector(*struct_child);
40: 			struct_vectors.push_back(move(struct_vector));
41: 		}
42: 	}
43: 	// add struct validitymask size
44: 	const idx_t struct_validitymask_size = (num_children + 7) / 8;
45: 	for (idx_t i = 0; i < ser_count; i++) {
46: 		entry_sizes[i] += struct_validitymask_size;
47: 	}
48: 	// compute size of child vectors
49: 	for (auto &struct_vector : struct_vectors) {
50: 		RowOperations::ComputeEntrySizes(struct_vector, entry_sizes, vcount, ser_count, sel, offset);
51: 	}
52: }
53: 
54: static void ComputeListEntrySizes(Vector &v, VectorData &vdata, idx_t entry_sizes[], idx_t ser_count,
55:                                   const SelectionVector &sel, idx_t offset) {
56: 	auto list_data = ListVector::GetData(v);
57: 	auto &child_vector = ListVector::GetEntry(v);
58: 	idx_t list_entry_sizes[STANDARD_VECTOR_SIZE];
59: 	for (idx_t i = 0; i < ser_count; i++) {
60: 		auto idx = sel.get_index(i);
61: 		auto source_idx = vdata.sel->get_index(idx) + offset;
62: 		if (vdata.validity.RowIsValid(source_idx)) {
63: 			auto list_entry = list_data[source_idx];
64: 
65: 			// make room for list length, list validitymask
66: 			entry_sizes[i] += sizeof(list_entry.length);
67: 			entry_sizes[i] += (list_entry.length + 7) / 8;
68: 
69: 			// serialize size of each entry (if non-constant size)
70: 			if (!TypeIsConstantSize(ListType::GetChildType(v.GetType()).InternalType())) {
71: 				entry_sizes[i] += list_entry.length * sizeof(list_entry.length);
72: 			}
73: 
74: 			// compute size of each the elements in list_entry and sum them
75: 			auto entry_remaining = list_entry.length;
76: 			auto entry_offset = list_entry.offset;
77: 			while (entry_remaining > 0) {
78: 				// the list entry can span multiple vectors
79: 				auto next = MinValue((idx_t)STANDARD_VECTOR_SIZE, entry_remaining);
80: 
81: 				// compute and add to the total
82: 				std::fill_n(list_entry_sizes, next, 0);
83: 				RowOperations::ComputeEntrySizes(child_vector, list_entry_sizes, next, next,
84: 				                                 FlatVector::INCREMENTAL_SELECTION_VECTOR, entry_offset);
85: 				for (idx_t list_idx = 0; list_idx < next; list_idx++) {
86: 					entry_sizes[i] += list_entry_sizes[list_idx];
87: 				}
88: 
89: 				// update for next iteration
90: 				entry_remaining -= next;
91: 				entry_offset += next;
92: 			}
93: 		}
94: 	}
95: }
96: 
97: void RowOperations::ComputeEntrySizes(Vector &v, VectorData &vdata, idx_t entry_sizes[], idx_t vcount, idx_t ser_count,
98:                                       const SelectionVector &sel, idx_t offset) {
99: 	const auto physical_type = v.GetType().InternalType();
100: 	if (TypeIsConstantSize(physical_type)) {
101: 		const auto type_size = GetTypeIdSize(physical_type);
102: 		for (idx_t i = 0; i < ser_count; i++) {
103: 			entry_sizes[i] += type_size;
104: 		}
105: 	} else {
106: 		switch (physical_type) {
107: 		case PhysicalType::VARCHAR:
108: 			ComputeStringEntrySizes(vdata, entry_sizes, ser_count, sel, offset);
109: 			break;
110: 		case PhysicalType::STRUCT:
111: 			ComputeStructEntrySizes(v, entry_sizes, vcount, ser_count, sel, offset);
112: 			break;
113: 		case PhysicalType::LIST:
114: 			ComputeListEntrySizes(v, vdata, entry_sizes, ser_count, sel, offset);
115: 			break;
116: 		default:
117: 			throw NotImplementedException("Column with variable size type %s cannot be serialized to row-format",
118: 			                              v.GetType().ToString());
119: 		}
120: 	}
121: }
122: 
123: void RowOperations::ComputeEntrySizes(Vector &v, idx_t entry_sizes[], idx_t vcount, idx_t ser_count,
124:                                       const SelectionVector &sel, idx_t offset) {
125: 	VectorData vdata;
126: 	v.Orrify(vcount, vdata);
127: 	ComputeEntrySizes(v, vdata, entry_sizes, vcount, ser_count, sel, offset);
128: }
129: 
130: template <class T>
131: static void TemplatedHeapScatter(VectorData &vdata, const SelectionVector &sel, idx_t count, idx_t col_idx,
132:                                  data_ptr_t *key_locations, data_ptr_t *validitymask_locations, idx_t offset) {
133: 	auto source = (T *)vdata.data;
134: 	if (!validitymask_locations) {
135: 		for (idx_t i = 0; i < count; i++) {
136: 			auto idx = sel.get_index(i);
137: 			auto source_idx = vdata.sel->get_index(idx) + offset;
138: 
139: 			auto target = (T *)key_locations[i];
140: 			Store<T>(source[source_idx], (data_ptr_t)target);
141: 			key_locations[i] += sizeof(T);
142: 		}
143: 	} else {
144: 		idx_t entry_idx;
145: 		idx_t idx_in_entry;
146: 		ValidityBytes::GetEntryIndex(col_idx, entry_idx, idx_in_entry);
147: 		const auto bit = ~(1UL << idx_in_entry);
148: 		for (idx_t i = 0; i < count; i++) {
149: 			auto idx = sel.get_index(i);
150: 			auto source_idx = vdata.sel->get_index(idx) + offset;
151: 
152: 			auto target = (T *)key_locations[i];
153: 			Store<T>(source[source_idx], (data_ptr_t)target);
154: 			key_locations[i] += sizeof(T);
155: 
156: 			// set the validitymask
157: 			if (!vdata.validity.RowIsValid(source_idx)) {
158: 				*(validitymask_locations[i] + entry_idx) &= bit;
159: 			}
160: 		}
161: 	}
162: }
163: 
164: static void HeapScatterStringVector(Vector &v, idx_t vcount, const SelectionVector &sel, idx_t ser_count, idx_t col_idx,
165:                                     data_ptr_t *key_locations, data_ptr_t *validitymask_locations, idx_t offset) {
166: 	VectorData vdata;
167: 	v.Orrify(vcount, vdata);
168: 
169: 	auto strings = (string_t *)vdata.data;
170: 	if (!validitymask_locations) {
171: 		for (idx_t i = 0; i < ser_count; i++) {
172: 			auto idx = sel.get_index(i);
173: 			auto source_idx = vdata.sel->get_index(idx) + offset;
174: 			if (vdata.validity.RowIsValid(source_idx)) {
175: 				auto &string_entry = strings[source_idx];
176: 				// store string size
177: 				Store<uint32_t>(string_entry.GetSize(), key_locations[i]);
178: 				key_locations[i] += sizeof(uint32_t);
179: 				// store the string
180: 				memcpy(key_locations[i], string_entry.GetDataUnsafe(), string_entry.GetSize());
181: 				key_locations[i] += string_entry.GetSize();
182: 			}
183: 		}
184: 	} else {
185: 		idx_t entry_idx;
186: 		idx_t idx_in_entry;
187: 		ValidityBytes::GetEntryIndex(col_idx, entry_idx, idx_in_entry);
188: 		const auto bit = ~(1UL << idx_in_entry);
189: 		for (idx_t i = 0; i < ser_count; i++) {
190: 			auto idx = sel.get_index(i);
191: 			auto source_idx = vdata.sel->get_index(idx) + offset;
192: 			if (vdata.validity.RowIsValid(source_idx)) {
193: 				auto &string_entry = strings[source_idx];
194: 				// store string size
195: 				Store<uint32_t>(string_entry.GetSize(), key_locations[i]);
196: 				key_locations[i] += sizeof(uint32_t);
197: 				// store the string
198: 				memcpy(key_locations[i], string_entry.GetDataUnsafe(), string_entry.GetSize());
199: 				key_locations[i] += string_entry.GetSize();
200: 			} else {
201: 				// set the validitymask
202: 				*(validitymask_locations[i] + entry_idx) &= bit;
203: 			}
204: 		}
205: 	}
206: }
207: 
208: static void HeapScatterStructVector(Vector &v, idx_t vcount, const SelectionVector &sel, idx_t ser_count, idx_t col_idx,
209:                                     data_ptr_t *key_locations, data_ptr_t *validitymask_locations, idx_t offset) {
210: 	VectorData vdata;
211: 	v.Orrify(vcount, vdata);
212: 
213: 	idx_t num_children;
214: 	vector<Vector> struct_vectors;
215: 	if (v.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
216: 		auto &child = DictionaryVector::Child(v);
217: 		auto &dict_sel = DictionaryVector::SelVector(v);
218: 		auto &children = StructVector::GetEntries(child);
219: 		num_children = children.size();
220: 		for (auto &struct_child : children) {
221: 			Vector struct_vector(*struct_child, dict_sel, vcount);
222: 			struct_vectors.push_back(move(struct_vector));
223: 		}
224: 	} else {
225: 		auto &children = StructVector::GetEntries(v);
226: 		num_children = children.size();
227: 		for (auto &struct_child : children) {
228: 			Vector struct_vector(*struct_child);
229: 			struct_vectors.push_back(move(struct_vector));
230: 		}
231: 	}
232: 
233: 	// the whole struct itself can be NULL
234: 	idx_t entry_idx;
235: 	idx_t idx_in_entry;
236: 	ValidityBytes::GetEntryIndex(col_idx, entry_idx, idx_in_entry);
237: 	const auto bit = ~(1UL << idx_in_entry);
238: 
239: 	// struct must have a validitymask for its fields
240: 	const idx_t struct_validitymask_size = (num_children + 7) / 8;
241: 	data_ptr_t struct_validitymask_locations[STANDARD_VECTOR_SIZE];
242: 	for (idx_t i = 0; i < ser_count; i++) {
243: 		// initialize the struct validity mask
244: 		struct_validitymask_locations[i] = key_locations[i];
245: 		memset(struct_validitymask_locations[i], -1, struct_validitymask_size);
246: 		key_locations[i] += struct_validitymask_size;
247: 
248: 		// set whether the whole struct is null
249: 		auto idx = sel.get_index(i);
250: 		auto source_idx = vdata.sel->get_index(idx) + offset;
251: 		if (validitymask_locations && !vdata.validity.RowIsValid(source_idx)) {
252: 			*(validitymask_locations[i] + entry_idx) &= bit;
253: 		}
254: 	}
255: 
256: 	// now serialize the struct vectors
257: 	for (idx_t i = 0; i < struct_vectors.size(); i++) {
258: 		auto &struct_vector = struct_vectors[i];
259: 		RowOperations::HeapScatter(struct_vector, vcount, sel, ser_count, i, key_locations,
260: 		                           struct_validitymask_locations, offset);
261: 	}
262: }
263: 
264: static void HeapScatterListVector(Vector &v, idx_t vcount, const SelectionVector &sel, idx_t ser_count, idx_t col_no,
265:                                   data_ptr_t *key_locations, data_ptr_t *validitymask_locations, idx_t offset) {
266: 	VectorData vdata;
267: 	v.Orrify(vcount, vdata);
268: 
269: 	idx_t entry_idx;
270: 	idx_t idx_in_entry;
271: 	ValidityBytes::GetEntryIndex(col_no, entry_idx, idx_in_entry);
272: 
273: 	auto list_data = ListVector::GetData(v);
274: 
275: 	auto &child_vector = ListVector::GetEntry(v);
276: 
277: 	VectorData list_vdata;
278: 	child_vector.Orrify(ListVector::GetListSize(v), list_vdata);
279: 	auto child_type = ListType::GetChildType(v.GetType()).InternalType();
280: 
281: 	idx_t list_entry_sizes[STANDARD_VECTOR_SIZE];
282: 	data_ptr_t list_entry_locations[STANDARD_VECTOR_SIZE];
283: 
284: 	for (idx_t i = 0; i < ser_count; i++) {
285: 		auto idx = sel.get_index(i);
286: 		auto source_idx = vdata.sel->get_index(idx) + offset;
287: 		if (!vdata.validity.RowIsValid(source_idx)) {
288: 			if (validitymask_locations) {
289: 				// set the row validitymask for this column to invalid
290: 				ValidityBytes row_mask(validitymask_locations[i]);
291: 				row_mask.SetInvalidUnsafe(entry_idx, idx_in_entry);
292: 			}
293: 			continue;
294: 		}
295: 		auto list_entry = list_data[source_idx];
296: 
297: 		// store list length
298: 		Store<uint64_t>(list_entry.length, key_locations[i]);
299: 		key_locations[i] += sizeof(list_entry.length);
300: 
301: 		// make room for the validitymask
302: 		data_ptr_t list_validitymask_location = key_locations[i];
303: 		idx_t entry_offset_in_byte = 0;
304: 		idx_t validitymask_size = (list_entry.length + 7) / 8;
305: 		memset(list_validitymask_location, -1, validitymask_size);
306: 		key_locations[i] += validitymask_size;
307: 
308: 		// serialize size of each entry (if non-constant size)
309: 		data_ptr_t var_entry_size_ptr = nullptr;
310: 		if (!TypeIsConstantSize(child_type)) {
311: 			var_entry_size_ptr = key_locations[i];
312: 			key_locations[i] += list_entry.length * sizeof(idx_t);
313: 		}
314: 
315: 		auto entry_remaining = list_entry.length;
316: 		auto entry_offset = list_entry.offset;
317: 		while (entry_remaining > 0) {
318: 			// the list entry can span multiple vectors
319: 			auto next = MinValue((idx_t)STANDARD_VECTOR_SIZE, entry_remaining);
320: 
321: 			// serialize list validity
322: 			for (idx_t entry_idx = 0; entry_idx < next; entry_idx++) {
323: 				auto list_idx = list_vdata.sel->get_index(entry_idx) + entry_offset;
324: 				if (!list_vdata.validity.RowIsValid(list_idx)) {
325: 					*(list_validitymask_location) &= ~(1UL << entry_offset_in_byte);
326: 				}
327: 				if (++entry_offset_in_byte == 8) {
328: 					list_validitymask_location++;
329: 					entry_offset_in_byte = 0;
330: 				}
331: 			}
332: 
333: 			if (TypeIsConstantSize(child_type)) {
334: 				// constant size list entries: set list entry locations
335: 				const idx_t type_size = GetTypeIdSize(child_type);
336: 				for (idx_t entry_idx = 0; entry_idx < next; entry_idx++) {
337: 					list_entry_locations[entry_idx] = key_locations[i];
338: 					key_locations[i] += type_size;
339: 				}
340: 			} else {
341: 				// variable size list entries: compute entry sizes and set list entry locations
342: 				std::fill_n(list_entry_sizes, next, 0);
343: 				RowOperations::ComputeEntrySizes(child_vector, list_entry_sizes, next, next,
344: 				                                 FlatVector::INCREMENTAL_SELECTION_VECTOR, entry_offset);
345: 				for (idx_t entry_idx = 0; entry_idx < next; entry_idx++) {
346: 					list_entry_locations[entry_idx] = key_locations[i];
347: 					key_locations[i] += list_entry_sizes[entry_idx];
348: 					Store<idx_t>(list_entry_sizes[entry_idx], var_entry_size_ptr);
349: 					var_entry_size_ptr += sizeof(idx_t);
350: 				}
351: 			}
352: 
353: 			// now serialize to the locations
354: 			RowOperations::HeapScatter(child_vector, ListVector::GetListSize(v),
355: 			                           FlatVector::INCREMENTAL_SELECTION_VECTOR, next, 0, list_entry_locations, nullptr,
356: 			                           entry_offset);
357: 
358: 			// update for next iteration
359: 			entry_remaining -= next;
360: 			entry_offset += next;
361: 		}
362: 	}
363: }
364: 
365: void RowOperations::HeapScatter(Vector &v, idx_t vcount, const SelectionVector &sel, idx_t ser_count, idx_t col_idx,
366:                                 data_ptr_t *key_locations, data_ptr_t *validitymask_locations, idx_t offset) {
367: 	if (TypeIsConstantSize(v.GetType().InternalType())) {
368: 		VectorData vdata;
369: 		v.Orrify(vcount, vdata);
370: 		RowOperations::HeapScatterVData(vdata, v.GetType().InternalType(), sel, ser_count, col_idx, key_locations,
371: 		                                validitymask_locations, offset);
372: 	} else {
373: 		switch (v.GetType().InternalType()) {
374: 		case PhysicalType::VARCHAR:
375: 			HeapScatterStringVector(v, vcount, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
376: 			break;
377: 		case PhysicalType::STRUCT:
378: 			HeapScatterStructVector(v, vcount, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
379: 			break;
380: 		case PhysicalType::LIST:
381: 			HeapScatterListVector(v, vcount, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
382: 			break;
383: 		default:
384: 			throw NotImplementedException("Serialization of variable length vector with type %s",
385: 			                              v.GetType().ToString());
386: 		}
387: 	}
388: }
389: 
390: void RowOperations::HeapScatterVData(VectorData &vdata, PhysicalType type, const SelectionVector &sel, idx_t ser_count,
391:                                      idx_t col_idx, data_ptr_t *key_locations, data_ptr_t *validitymask_locations,
392:                                      idx_t offset) {
393: 	switch (type) {
394: 	case PhysicalType::BOOL:
395: 	case PhysicalType::INT8:
396: 		TemplatedHeapScatter<int8_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
397: 		break;
398: 	case PhysicalType::INT16:
399: 		TemplatedHeapScatter<int16_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
400: 		break;
401: 	case PhysicalType::INT32:
402: 		TemplatedHeapScatter<int32_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
403: 		break;
404: 	case PhysicalType::INT64:
405: 		TemplatedHeapScatter<int64_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
406: 		break;
407: 	case PhysicalType::UINT8:
408: 		TemplatedHeapScatter<uint8_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
409: 		break;
410: 	case PhysicalType::UINT16:
411: 		TemplatedHeapScatter<uint16_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
412: 		break;
413: 	case PhysicalType::UINT32:
414: 		TemplatedHeapScatter<uint32_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
415: 		break;
416: 	case PhysicalType::UINT64:
417: 		TemplatedHeapScatter<uint64_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
418: 		break;
419: 	case PhysicalType::INT128:
420: 		TemplatedHeapScatter<hugeint_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
421: 		break;
422: 	case PhysicalType::FLOAT:
423: 		TemplatedHeapScatter<float>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
424: 		break;
425: 	case PhysicalType::DOUBLE:
426: 		TemplatedHeapScatter<double>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
427: 		break;
428: 	case PhysicalType::INTERVAL:
429: 		TemplatedHeapScatter<interval_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
430: 		break;
431: 	default:
432: 		throw NotImplementedException("FIXME: unimplemented serialize to of constant type column to row-format");
433: 	}
434: }
435: 
436: } // namespace duckdb
[end of src/common/row_operations/row_heap_scatter.cpp]
[start of src/common/row_operations/row_radix_scatter.cpp]
1: #include "duckdb/common/helper.hpp"
2: #include "duckdb/common/radix.hpp"
3: #include "duckdb/common/row_operations/row_operations.hpp"
4: #include "duckdb/common/types/vector.hpp"
5: 
6: namespace duckdb {
7: 
8: template <class T>
9: void TemplatedRadixScatter(VectorData &vdata, const SelectionVector &sel, idx_t add_count, data_ptr_t *key_locations,
10:                            const bool desc, const bool has_null, const bool nulls_first, const bool is_little_endian,
11:                            const idx_t offset) {
12: 	auto source = (T *)vdata.data;
13: 	if (has_null) {
14: 		auto &validity = vdata.validity;
15: 		const data_t valid = nulls_first ? 1 : 0;
16: 		const data_t invalid = 1 - valid;
17: 
18: 		for (idx_t i = 0; i < add_count; i++) {
19: 			auto idx = sel.get_index(i);
20: 			auto source_idx = vdata.sel->get_index(idx) + offset;
21: 			// write validity and according value
22: 			if (validity.RowIsValid(source_idx)) {
23: 				key_locations[i][0] = valid;
24: 				EncodeData<T>(key_locations[i] + 1, source[source_idx], is_little_endian);
25: 				// invert bits if desc
26: 				if (desc) {
27: 					for (idx_t s = 1; s < sizeof(T) + 1; s++) {
28: 						*(key_locations[i] + s) = ~*(key_locations[i] + s);
29: 					}
30: 				}
31: 			} else {
32: 				key_locations[i][0] = invalid;
33: 				memset(key_locations[i] + 1, '\0', sizeof(T));
34: 			}
35: 			key_locations[i] += sizeof(T) + 1;
36: 		}
37: 	} else {
38: 		for (idx_t i = 0; i < add_count; i++) {
39: 			auto idx = sel.get_index(i);
40: 			auto source_idx = vdata.sel->get_index(idx) + offset;
41: 			// write value
42: 			EncodeData<T>(key_locations[i], source[source_idx], is_little_endian);
43: 			// invert bits if desc
44: 			if (desc) {
45: 				for (idx_t s = 0; s < sizeof(T); s++) {
46: 					*(key_locations[i] + s) = ~*(key_locations[i] + s);
47: 				}
48: 			}
49: 			key_locations[i] += sizeof(T);
50: 		}
51: 	}
52: }
53: 
54: void RadixScatterStringVector(VectorData &vdata, const SelectionVector &sel, idx_t add_count, data_ptr_t *key_locations,
55:                               const bool desc, const bool has_null, const bool nulls_first, const idx_t prefix_len,
56:                               idx_t offset) {
57: 	auto source = (string_t *)vdata.data;
58: 	if (has_null) {
59: 		auto &validity = vdata.validity;
60: 		const data_t valid = nulls_first ? 1 : 0;
61: 		const data_t invalid = 1 - valid;
62: 
63: 		for (idx_t i = 0; i < add_count; i++) {
64: 			auto idx = sel.get_index(i);
65: 			auto source_idx = vdata.sel->get_index(idx) + offset;
66: 			// write validity and according value
67: 			if (validity.RowIsValid(source_idx)) {
68: 				key_locations[i][0] = valid;
69: 				EncodeStringDataPrefix(key_locations[i] + 1, source[source_idx], prefix_len);
70: 				// invert bits if desc
71: 				if (desc) {
72: 					for (idx_t s = 1; s < prefix_len + 1; s++) {
73: 						*(key_locations[i] + s) = ~*(key_locations[i] + s);
74: 					}
75: 				}
76: 			} else {
77: 				key_locations[i][0] = invalid;
78: 				memset(key_locations[i] + 1, '\0', prefix_len);
79: 			}
80: 			key_locations[i] += prefix_len + 1;
81: 		}
82: 	} else {
83: 		for (idx_t i = 0; i < add_count; i++) {
84: 			auto idx = sel.get_index(i);
85: 			auto source_idx = vdata.sel->get_index(idx) + offset;
86: 			// write value
87: 			EncodeStringDataPrefix(key_locations[i], source[source_idx], prefix_len);
88: 			// invert bits if desc
89: 			if (desc) {
90: 				for (idx_t s = 0; s < prefix_len; s++) {
91: 					*(key_locations[i] + s) = ~*(key_locations[i] + s);
92: 				}
93: 			}
94: 			key_locations[i] += prefix_len;
95: 		}
96: 	}
97: }
98: 
99: void RadixScatterListVector(Vector &v, VectorData &vdata, const SelectionVector &sel, idx_t add_count,
100:                             data_ptr_t *key_locations, const bool desc, const bool has_null, const bool nulls_first,
101:                             const idx_t prefix_len, const idx_t width, const idx_t offset) {
102: 	auto list_data = ListVector::GetData(v);
103: 	auto &child_vector = ListVector::GetEntry(v);
104: 	auto list_size = ListVector::GetListSize(v);
105: 
106: 	// serialize null values
107: 	if (has_null) {
108: 		auto &validity = vdata.validity;
109: 		const data_t valid = nulls_first ? 1 : 0;
110: 		const data_t invalid = 1 - valid;
111: 
112: 		for (idx_t i = 0; i < add_count; i++) {
113: 			auto idx = sel.get_index(i);
114: 			auto source_idx = vdata.sel->get_index(idx) + offset;
115: 			data_ptr_t key_location = key_locations[i] + 1;
116: 			// write validity and according value
117: 			if (validity.RowIsValid(source_idx)) {
118: 				key_locations[i][0] = valid;
119: 				key_locations[i]++;
120: 				auto &list_entry = list_data[source_idx];
121: 				if (list_entry.length > 0) {
122: 					// denote that the list is not empty with a 1
123: 					key_locations[i][0] = 1;
124: 					key_locations[i]++;
125: 					RowOperations::RadixScatter(child_vector, list_size, FlatVector::INCREMENTAL_SELECTION_VECTOR, 1,
126: 					                            key_locations + i, false, has_null, false, prefix_len, width - 1,
127: 					                            list_entry.offset);
128: 				} else {
129: 					// denote that the list is empty with a 0
130: 					key_locations[i][0] = 0;
131: 					key_locations[i]++;
132: 					memset(key_locations[i], '\0', width - 2);
133: 				}
134: 				// invert bits if desc
135: 				if (desc) {
136: 					for (idx_t s = 0; s < width - 1; s++) {
137: 						*(key_location + s) = ~*(key_location + s);
138: 					}
139: 				}
140: 			} else {
141: 				key_locations[i][0] = invalid;
142: 				memset(key_locations[i] + 1, '\0', width - 1);
143: 				key_locations[i] += width;
144: 			}
145: 		}
146: 	} else {
147: 		for (idx_t i = 0; i < add_count; i++) {
148: 			auto idx = sel.get_index(i);
149: 			auto source_idx = vdata.sel->get_index(idx) + offset;
150: 			auto &list_entry = list_data[source_idx];
151: 			data_ptr_t key_location = key_locations[i];
152: 			if (list_entry.length > 0) {
153: 				// denote that the list is not empty with a 1
154: 				key_locations[i][0] = 1;
155: 				key_locations[i]++;
156: 				RowOperations::RadixScatter(child_vector, list_size, FlatVector::INCREMENTAL_SELECTION_VECTOR, 1,
157: 				                            key_locations + i, false, has_null, false, prefix_len, width - 1,
158: 				                            list_entry.offset);
159: 			} else {
160: 				// denote that the list is empty with a 0
161: 				key_locations[i][0] = 0;
162: 				key_locations[i]++;
163: 				memset(key_locations[i], '\0', width - 1);
164: 			}
165: 			// invert bits if desc
166: 			if (desc) {
167: 				for (idx_t s = 0; s < width; s++) {
168: 					*(key_location + s) = ~*(key_location + s);
169: 				}
170: 			}
171: 		}
172: 	}
173: }
174: 
175: void RadixScatterStructVector(Vector &v, VectorData &vdata, idx_t vcount, const SelectionVector &sel, idx_t add_count,
176:                               data_ptr_t *key_locations, const bool desc, const bool has_null, const bool nulls_first,
177:                               const idx_t prefix_len, idx_t width, const idx_t offset) {
178: 	// serialize null values
179: 	if (has_null) {
180: 		auto &validity = vdata.validity;
181: 		const data_t valid = nulls_first ? 1 : 0;
182: 		const data_t invalid = 1 - valid;
183: 
184: 		for (idx_t i = 0; i < add_count; i++) {
185: 			auto idx = sel.get_index(i);
186: 			auto source_idx = vdata.sel->get_index(idx) + offset;
187: 			// write validity and according value
188: 			if (validity.RowIsValid(source_idx)) {
189: 				key_locations[i][0] = valid;
190: 			} else {
191: 				key_locations[i][0] = invalid;
192: 			}
193: 			key_locations[i]++;
194: 		}
195: 		width--;
196: 	}
197: 	// serialize the struct
198: 	auto &child_vector = *StructVector::GetEntries(v)[0];
199: 	RowOperations::RadixScatter(child_vector, vcount, FlatVector::INCREMENTAL_SELECTION_VECTOR, add_count,
200: 	                            key_locations, false, has_null, false, prefix_len, width, offset);
201: 	// invert bits if desc
202: 	if (desc) {
203: 		for (idx_t i = 0; i < add_count; i++) {
204: 			for (idx_t s = 0; s < width; s++) {
205: 				*(key_locations[i] - width + s) = ~*(key_locations[i] - width + s);
206: 			}
207: 		}
208: 	}
209: }
210: 
211: void RowOperations::RadixScatter(Vector &v, idx_t vcount, const SelectionVector &sel, idx_t ser_count,
212:                                  data_ptr_t *key_locations, bool desc, bool has_null, bool nulls_first,
213:                                  idx_t prefix_len, idx_t width, idx_t offset) {
214: 	auto is_little_endian = IsLittleEndian();
215: 
216: 	VectorData vdata;
217: 	v.Orrify(vcount, vdata);
218: 	switch (v.GetType().InternalType()) {
219: 	case PhysicalType::BOOL:
220: 	case PhysicalType::INT8:
221: 		TemplatedRadixScatter<int8_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first,
222: 		                              is_little_endian, offset);
223: 		break;
224: 	case PhysicalType::INT16:
225: 		TemplatedRadixScatter<int16_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first,
226: 		                               is_little_endian, offset);
227: 		break;
228: 	case PhysicalType::INT32:
229: 		TemplatedRadixScatter<int32_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first,
230: 		                               is_little_endian, offset);
231: 		break;
232: 	case PhysicalType::INT64:
233: 		TemplatedRadixScatter<int64_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first,
234: 		                               is_little_endian, offset);
235: 		break;
236: 	case PhysicalType::UINT8:
237: 		TemplatedRadixScatter<uint8_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first,
238: 		                               is_little_endian, offset);
239: 		break;
240: 	case PhysicalType::UINT16:
241: 		TemplatedRadixScatter<uint16_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first,
242: 		                                is_little_endian, offset);
243: 		break;
244: 	case PhysicalType::UINT32:
245: 		TemplatedRadixScatter<uint32_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first,
246: 		                                is_little_endian, offset);
247: 		break;
248: 	case PhysicalType::UINT64:
249: 		TemplatedRadixScatter<uint64_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first,
250: 		                                is_little_endian, offset);
251: 		break;
252: 	case PhysicalType::INT128:
253: 		TemplatedRadixScatter<hugeint_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first,
254: 		                                 is_little_endian, offset);
255: 		break;
256: 	case PhysicalType::FLOAT:
257: 		TemplatedRadixScatter<float>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first,
258: 		                             is_little_endian, offset);
259: 		break;
260: 	case PhysicalType::DOUBLE:
261: 		TemplatedRadixScatter<double>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first,
262: 		                              is_little_endian, offset);
263: 		break;
264: 	case PhysicalType::INTERVAL:
265: 		TemplatedRadixScatter<interval_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first,
266: 		                                  is_little_endian, offset);
267: 		break;
268: 	case PhysicalType::VARCHAR:
269: 		RadixScatterStringVector(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first, prefix_len, offset);
270: 		break;
271: 	case PhysicalType::LIST:
272: 		RadixScatterListVector(v, vdata, sel, ser_count, key_locations, desc, has_null, nulls_first, prefix_len, width,
273: 		                       offset);
274: 		break;
275: 	case PhysicalType::STRUCT:
276: 		RadixScatterStructVector(v, vdata, vcount, sel, ser_count, key_locations, desc, has_null, nulls_first,
277: 		                         prefix_len, width, offset);
278: 		break;
279: 	default:
280: 		throw NotImplementedException("Cannot ORDER BY column with type %s", v.GetType().ToString());
281: 	}
282: }
283: 
284: } // namespace duckdb
[end of src/common/row_operations/row_radix_scatter.cpp]
[start of src/common/row_operations/row_scatter.cpp]
1: //===--------------------------------------------------------------------===//
2: // row_scatter.cpp
3: // Description: This file contains the implementation of the row scattering
4: //              operators
5: //===--------------------------------------------------------------------===//
6: 
7: #include "duckdb/common/exception.hpp"
8: #include "duckdb/common/helper.hpp"
9: #include "duckdb/common/row_operations/row_operations.hpp"
10: #include "duckdb/common/types/null_value.hpp"
11: #include "duckdb/common/types/row_data_collection.hpp"
12: #include "duckdb/common/types/row_layout.hpp"
13: #include "duckdb/common/types/selection_vector.hpp"
14: #include "duckdb/common/types/vector.hpp"
15: 
16: namespace duckdb {
17: 
18: using ValidityBytes = RowLayout::ValidityBytes;
19: 
20: template <class T>
21: static void TemplatedScatter(VectorData &col, Vector &rows, const SelectionVector &sel, const idx_t count,
22:                              const idx_t col_offset, const idx_t col_no) {
23: 	auto data = (T *)col.data;
24: 	auto ptrs = FlatVector::GetData<data_ptr_t>(rows);
25: 
26: 	if (!col.validity.AllValid()) {
27: 		for (idx_t i = 0; i < count; i++) {
28: 			auto idx = sel.get_index(i);
29: 			auto col_idx = col.sel->get_index(idx);
30: 			auto row = ptrs[idx];
31: 
32: 			auto isnull = !col.validity.RowIsValid(col_idx);
33: 			T store_value = isnull ? NullValue<T>() : data[col_idx];
34: 			Store<T>(store_value, row + col_offset);
35: 			if (isnull) {
36: 				ValidityBytes col_mask(ptrs[idx]);
37: 				col_mask.SetInvalidUnsafe(col_no);
38: 			}
39: 		}
40: 	} else {
41: 		for (idx_t i = 0; i < count; i++) {
42: 			auto idx = sel.get_index(i);
43: 			auto col_idx = col.sel->get_index(idx);
44: 			auto row = ptrs[idx];
45: 
46: 			Store<T>(data[col_idx], row + col_offset);
47: 		}
48: 	}
49: }
50: 
51: static void ComputeStringEntrySizes(const VectorData &col, idx_t entry_sizes[], const SelectionVector &sel,
52:                                     const idx_t count, const idx_t offset = 0) {
53: 	auto data = (const string_t *)col.data;
54: 	for (idx_t i = 0; i < count; i++) {
55: 		auto idx = sel.get_index(i);
56: 		auto col_idx = col.sel->get_index(idx) + offset;
57: 		const auto &str = data[col_idx];
58: 		if (col.validity.RowIsValid(col_idx) && !str.IsInlined()) {
59: 			entry_sizes[i] += str.GetSize();
60: 		}
61: 	}
62: }
63: 
64: static void ScatterStringVector(VectorData &col, Vector &rows, data_ptr_t str_locations[], const SelectionVector &sel,
65:                                 const idx_t count, const idx_t col_offset, const idx_t col_no) {
66: 	auto string_data = (string_t *)col.data;
67: 	auto ptrs = FlatVector::GetData<data_ptr_t>(rows);
68: 
69: 	for (idx_t i = 0; i < count; i++) {
70: 		auto idx = sel.get_index(i);
71: 		auto col_idx = col.sel->get_index(idx);
72: 		auto row = ptrs[idx];
73: 		if (!col.validity.RowIsValid(col_idx)) {
74: 			ValidityBytes col_mask(row);
75: 			col_mask.SetInvalidUnsafe(col_no);
76: 			Store<string_t>(NullValue<string_t>(), row + col_offset);
77: 		} else if (string_data[col_idx].IsInlined()) {
78: 			Store<string_t>(string_data[col_idx], row + col_offset);
79: 		} else {
80: 			const auto &str = string_data[col_idx];
81: 			string_t inserted((const char *)str_locations[i], str.GetSize());
82: 			memcpy(inserted.GetDataWriteable(), str.GetDataUnsafe(), str.GetSize());
83: 			str_locations[i] += str.GetSize();
84: 			inserted.Finalize();
85: 			Store<string_t>(inserted, row + col_offset);
86: 		}
87: 	}
88: }
89: 
90: static void ScatterNestedVector(Vector &vec, VectorData &col, Vector &rows, data_ptr_t data_locations[],
91:                                 const SelectionVector &sel, const idx_t count, const idx_t col_offset,
92:                                 const idx_t col_no, const idx_t vcount) {
93: 	// Store pointers to the data in the row
94: 	// Do this first because SerializeVector destroys the locations
95: 	auto ptrs = FlatVector::GetData<data_ptr_t>(rows);
96: 	for (idx_t i = 0; i < count; i++) {
97: 		auto idx = sel.get_index(i);
98: 		auto row = ptrs[idx];
99: 
100: 		Store<data_ptr_t>(data_locations[i], row + col_offset);
101: 	}
102: 
103: 	// Serialise the data
104: 	RowOperations::HeapScatter(vec, vcount, sel, count, col_no, data_locations, ptrs);
105: }
106: 
107: void RowOperations::Scatter(DataChunk &columns, VectorData col_data[], const RowLayout &layout, Vector &rows,
108:                             RowDataCollection &string_heap, const SelectionVector &sel, idx_t count) {
109: 	if (count == 0) {
110: 		return;
111: 	}
112: 
113: 	// Set the validity mask for each row before inserting data
114: 	auto ptrs = FlatVector::GetData<data_ptr_t>(rows);
115: 	for (idx_t i = 0; i < count; ++i) {
116: 		auto row_idx = sel.get_index(i);
117: 		auto row = ptrs[row_idx];
118: 		ValidityBytes(row).SetAllValid(layout.ColumnCount());
119: 	}
120: 
121: 	const auto vcount = columns.size();
122: 	auto &offsets = layout.GetOffsets();
123: 	auto &types = layout.GetTypes();
124: 
125: 	// Compute the entry size of the variable size columns
126: 	data_ptr_t data_locations[STANDARD_VECTOR_SIZE];
127: 	if (!layout.AllConstant()) {
128: 		idx_t entry_sizes[STANDARD_VECTOR_SIZE];
129: 		std::fill_n(entry_sizes, count, sizeof(idx_t));
130: 		for (idx_t col_no = 0; col_no < types.size(); col_no++) {
131: 			if (TypeIsConstantSize(types[col_no].InternalType())) {
132: 				continue;
133: 			}
134: 
135: 			auto &vec = columns.data[col_no];
136: 			auto &col = col_data[col_no];
137: 			switch (types[col_no].InternalType()) {
138: 			case PhysicalType::VARCHAR:
139: 				ComputeStringEntrySizes(col, entry_sizes, sel, count);
140: 				break;
141: 			case PhysicalType::LIST:
142: 			case PhysicalType::MAP:
143: 			case PhysicalType::STRUCT:
144: 				RowOperations::ComputeEntrySizes(vec, col, entry_sizes, vcount, count, sel);
145: 				break;
146: 			default:
147: 				throw Exception("Unsupported type for RowOperations::Scatter");
148: 			}
149: 		}
150: 
151: 		// Build out the buffer space
152: 		string_heap.Build(count, data_locations, entry_sizes);
153: 
154: 		// Serialize information that is needed for swizzling if the computation goes out-of-core
155: 		const idx_t heap_pointer_offset = layout.GetHeapPointerOffset();
156: 		for (idx_t i = 0; i < count; i++) {
157: 			auto row_idx = sel.get_index(i);
158: 			auto row = ptrs[row_idx];
159: 			// Pointer to this row in the heap block
160: 			Store<data_ptr_t>(data_locations[i], row + heap_pointer_offset);
161: 			// Row size is stored in the heap in front of each row
162: 			Store<idx_t>(entry_sizes[i], data_locations[i]);
163: 			data_locations[i] += sizeof(idx_t);
164: 		}
165: 	}
166: 
167: 	for (idx_t col_no = 0; col_no < types.size(); col_no++) {
168: 		auto &vec = columns.data[col_no];
169: 		auto &col = col_data[col_no];
170: 		auto col_offset = offsets[col_no];
171: 
172: 		switch (types[col_no].InternalType()) {
173: 		case PhysicalType::BOOL:
174: 		case PhysicalType::INT8:
175: 			TemplatedScatter<int8_t>(col, rows, sel, count, col_offset, col_no);
176: 			break;
177: 		case PhysicalType::INT16:
178: 			TemplatedScatter<int16_t>(col, rows, sel, count, col_offset, col_no);
179: 			break;
180: 		case PhysicalType::INT32:
181: 			TemplatedScatter<int32_t>(col, rows, sel, count, col_offset, col_no);
182: 			break;
183: 		case PhysicalType::INT64:
184: 			TemplatedScatter<int64_t>(col, rows, sel, count, col_offset, col_no);
185: 			break;
186: 		case PhysicalType::UINT8:
187: 			TemplatedScatter<uint8_t>(col, rows, sel, count, col_offset, col_no);
188: 			break;
189: 		case PhysicalType::UINT16:
190: 			TemplatedScatter<uint16_t>(col, rows, sel, count, col_offset, col_no);
191: 			break;
192: 		case PhysicalType::UINT32:
193: 			TemplatedScatter<uint32_t>(col, rows, sel, count, col_offset, col_no);
194: 			break;
195: 		case PhysicalType::UINT64:
196: 			TemplatedScatter<uint64_t>(col, rows, sel, count, col_offset, col_no);
197: 			break;
198: 		case PhysicalType::INT128:
199: 			TemplatedScatter<hugeint_t>(col, rows, sel, count, col_offset, col_no);
200: 			break;
201: 		case PhysicalType::FLOAT:
202: 			TemplatedScatter<float>(col, rows, sel, count, col_offset, col_no);
203: 			break;
204: 		case PhysicalType::DOUBLE:
205: 			TemplatedScatter<double>(col, rows, sel, count, col_offset, col_no);
206: 			break;
207: 		case PhysicalType::INTERVAL:
208: 			TemplatedScatter<interval_t>(col, rows, sel, count, col_offset, col_no);
209: 			break;
210: 		case PhysicalType::VARCHAR:
211: 			ScatterStringVector(col, rows, data_locations, sel, count, col_offset, col_no);
212: 			break;
213: 		case PhysicalType::LIST:
214: 		case PhysicalType::MAP:
215: 		case PhysicalType::STRUCT:
216: 			ScatterNestedVector(vec, col, rows, data_locations, sel, count, col_offset, col_no, vcount);
217: 			break;
218: 		default:
219: 			throw InternalException("Unsupported type for RowOperations::Scatter");
220: 		}
221: 	}
222: }
223: 
224: } // namespace duckdb
[end of src/common/row_operations/row_scatter.cpp]
[start of src/common/types/row_data_collection.cpp]
1: #include "duckdb/common/types/row_data_collection.hpp"
2: 
3: #include "duckdb/common/types/chunk_collection.hpp"
4: 
5: namespace duckdb {
6: 
7: using ValidityBytes = TemplatedValidityMask<uint8_t>;
8: 
9: RowDataCollection::RowDataCollection(BufferManager &buffer_manager, idx_t block_capacity, idx_t entry_size,
10:                                      bool keep_pinned)
11:     : buffer_manager(buffer_manager), count(0), block_capacity(block_capacity), entry_size(entry_size),
12:       keep_pinned(keep_pinned) {
13: 	D_ASSERT(block_capacity * entry_size >= Storage::BLOCK_SIZE);
14: }
15: 
16: idx_t RowDataCollection::AppendToBlock(RowDataBlock &block, BufferHandle &handle,
17:                                        vector<BlockAppendEntry> &append_entries, idx_t remaining, idx_t entry_sizes[]) {
18: 	idx_t append_count = 0;
19: 	data_ptr_t dataptr;
20: 	if (entry_sizes) {
21: 		// compute how many entries fit if entry size if variable
22: 		dataptr = handle.node->buffer + block.byte_offset;
23: 		for (idx_t i = 0; i < remaining; i++) {
24: 			if (block.byte_offset + entry_sizes[i] > block_capacity * entry_size) {
25: 				while (entry_sizes[i] > block_capacity * entry_size) {
26: 					// if an entry does not fit, increase capacity until it does
27: 					block_capacity *= 2;
28: 				}
29: 				break;
30: 			}
31: 			append_count++;
32: 			block.byte_offset += entry_sizes[i];
33: 		}
34: 	} else {
35: 		append_count = MinValue<idx_t>(remaining, block.capacity - block.count);
36: 		dataptr = handle.node->buffer + block.count * entry_size;
37: 	}
38: 	append_entries.emplace_back(dataptr, append_count);
39: 	block.count += append_count;
40: 	return append_count;
41: }
42: 
43: vector<unique_ptr<BufferHandle>> RowDataCollection::Build(idx_t added_count, data_ptr_t key_locations[],
44:                                                           idx_t entry_sizes[], const SelectionVector *sel) {
45: 	vector<unique_ptr<BufferHandle>> handles;
46: 	vector<BlockAppendEntry> append_entries;
47: 
48: 	// first allocate space of where to serialize the keys and payload columns
49: 	idx_t remaining = added_count;
50: 	{
51: 		// first append to the last block (if any)
52: 		lock_guard<mutex> append_lock(rdc_lock);
53: 		count += added_count;
54: 
55: 		if (!blocks.empty()) {
56: 			auto &last_block = blocks.back();
57: 			if (last_block.count < last_block.capacity) {
58: 				// last block has space: pin the buffer of this block
59: 				auto handle = buffer_manager.Pin(last_block.block);
60: 				// now append to the block
61: 				idx_t append_count = AppendToBlock(last_block, *handle, append_entries, remaining, entry_sizes);
62: 				remaining -= append_count;
63: 				handles.push_back(move(handle));
64: 			}
65: 		}
66: 		while (remaining > 0) {
67: 			// now for the remaining data, allocate new buffers to store the data and append there
68: 			RowDataBlock new_block(buffer_manager, block_capacity, entry_size);
69: 			auto handle = buffer_manager.Pin(new_block.block);
70: 
71: 			// offset the entry sizes array if we have added entries already
72: 			idx_t *offset_entry_sizes = entry_sizes ? entry_sizes + added_count - remaining : nullptr;
73: 
74: 			idx_t append_count = AppendToBlock(new_block, *handle, append_entries, remaining, offset_entry_sizes);
75: 			remaining -= append_count;
76: 
77: 			if (new_block.count > 0) {
78: 				// in case 0 tuples fit the block (huge entry, e.g. large string) we do not add
79: 				blocks.push_back(move(new_block));
80: 				if (keep_pinned) {
81: 					pinned_blocks.push_back(move(handle));
82: 				} else {
83: 					handles.push_back(move(handle));
84: 				}
85: 			}
86: 		}
87: 	}
88: 	// now set up the key_locations based on the append entries
89: 	idx_t append_idx = 0;
90: 	for (auto &append_entry : append_entries) {
91: 		idx_t next = append_idx + append_entry.count;
92: 		if (entry_sizes) {
93: 			for (; append_idx < next; append_idx++) {
94: 				key_locations[append_idx] = append_entry.baseptr;
95: 				append_entry.baseptr += entry_sizes[append_idx];
96: 			}
97: 		} else {
98: 			for (; append_idx < next; append_idx++) {
99: 				auto idx = sel->get_index(append_idx);
100: 				key_locations[idx] = append_entry.baseptr;
101: 				append_entry.baseptr += entry_size;
102: 			}
103: 		}
104: 	}
105: 	// return the unique pointers to the handles because they must stay pinned
106: 	return handles;
107: }
108: 
109: void RowDataCollection::Merge(RowDataCollection &other) {
110: 	RowDataCollection temp(buffer_manager, Storage::BLOCK_SIZE, 1);
111: 	{
112: 		//	One lock at a time to avoid deadlocks
113: 		lock_guard<mutex> read_lock(other.rdc_lock);
114: 		temp.count = other.count;
115: 		temp.block_capacity = other.block_capacity;
116: 		temp.entry_size = other.entry_size;
117: 		temp.blocks = move(other.blocks);
118: 		other.count = 0;
119: 	}
120: 
121: 	lock_guard<mutex> write_lock(rdc_lock);
122: 	count += temp.count;
123: 	block_capacity = MaxValue(block_capacity, temp.block_capacity);
124: 	entry_size = MaxValue(entry_size, temp.entry_size);
125: 	for (auto &block : temp.blocks) {
126: 		blocks.emplace_back(move(block));
127: 	}
128: 	for (auto &handle : temp.pinned_blocks) {
129: 		pinned_blocks.emplace_back(move(handle));
130: 	}
131: }
132: 
133: } // namespace duckdb
[end of src/common/types/row_data_collection.cpp]
[start of src/execution/operator/order/physical_order.cpp]
1: #include "duckdb/execution/operator/order/physical_order.hpp"
2: 
3: #include "duckdb/common/row_operations/row_operations.hpp"
4: #include "duckdb/common/types.hpp"
5: #include "duckdb/common/types/row_data_collection.hpp"
6: #include "duckdb/common/types/row_layout.hpp"
7: #include "duckdb/execution/expression_executor.hpp"
8: #include "duckdb/main/client_context.hpp"
9: #include "duckdb/parallel/task_context.hpp"
10: #include "duckdb/planner/expression/bound_reference_expression.hpp"
11: #include "duckdb/storage/statistics/base_statistics.hpp"
12: #include "duckdb/storage/statistics/numeric_statistics.hpp"
13: #include "duckdb/storage/statistics/string_statistics.hpp"
14: 
15: #include <numeric>
16: 
17: namespace duckdb {
18: 
19: using ValidityBytes = RowLayout::ValidityBytes;
20: 
21: PhysicalOrder::PhysicalOrder(vector<LogicalType> types, vector<BoundOrderByNode> orders, idx_t estimated_cardinality)
22:     : PhysicalSink(PhysicalOperatorType::ORDER_BY, move(types), estimated_cardinality), orders(move(orders)) {
23: }
24: 
25: //===--------------------------------------------------------------------===//
26: // Sink
27: //===--------------------------------------------------------------------===//
28: static idx_t GetSortingColSize(const LogicalType &type) {
29: 	auto physical_type = type.InternalType();
30: 	if (TypeIsConstantSize(physical_type)) {
31: 		return GetTypeIdSize(physical_type);
32: 	} else {
33: 		switch (physical_type) {
34: 		case PhysicalType::VARCHAR:
35: 			// TODO: make use of statistics
36: 			return string_t::INLINE_LENGTH;
37: 		case PhysicalType::LIST:
38: 			// Lists get another byte to denote the empty list
39: 			return 2 + GetSortingColSize(ListType::GetChildType(type));
40: 		case PhysicalType::MAP:
41: 		case PhysicalType::STRUCT:
42: 			return 1 + GetSortingColSize(StructType::GetChildType(type, 0));
43: 		default:
44: 			throw NotImplementedException("Unable to order column with type %s", type.ToString());
45: 		}
46: 	}
47: }
48: 
49: struct SortingState {
50: 	explicit SortingState(const vector<BoundOrderByNode> &orders)
51: 	    : column_count(orders.size()), all_constant(true), comparison_size(0), entry_size(0) {
52: 		vector<LogicalType> blob_layout_types;
53: 		for (idx_t i = 0; i < orders.size(); i++) {
54: 			const auto &order = orders[i];
55: 
56: 			order_types.push_back(order.type);
57: 			order_by_null_types.push_back(order.null_order);
58: 			auto &expr = *order.expression;
59: 			logical_types.push_back(expr.return_type);
60: 
61: 			auto physical_type = expr.return_type.InternalType();
62: 			all_constant = all_constant && TypeIsConstantSize(physical_type);
63: 			constant_size.push_back(TypeIsConstantSize(physical_type));
64: 			column_sizes.push_back(0);
65: 			auto &col_size = column_sizes.back();
66: 
67: 			if (expr.stats) {
68: 				stats.push_back(expr.stats.get());
69: 				has_null.push_back(stats.back()->CanHaveNull());
70: 			} else {
71: 				stats.push_back(nullptr);
72: 				// No stats - we must assume that there are nulls
73: 				has_null.push_back(true);
74: 			}
75: 
76: 			col_size += has_null.back() ? 1 : 0;
77: 			if (TypeIsConstantSize(physical_type)) {
78: 				col_size += GetTypeIdSize(physical_type);
79: 			} else {
80: 				col_size += GetSortingColSize(expr.return_type);
81: 				sorting_to_blob_col[i] = blob_layout_types.size();
82: 				blob_layout_types.push_back(expr.return_type);
83: 			}
84: 			comparison_size += col_size;
85: 		}
86: 		entry_size = comparison_size + sizeof(idx_t);
87: 		blob_layout.Initialize(blob_layout_types, false);
88: 	}
89: 
90: 	idx_t column_count;
91: 	vector<OrderType> order_types;
92: 	vector<OrderByNullType> order_by_null_types;
93: 	vector<LogicalType> logical_types;
94: 
95: 	bool all_constant;
96: 	vector<bool> constant_size;
97: 	vector<idx_t> column_sizes;
98: 	vector<BaseStatistics *> stats;
99: 	vector<bool> has_null;
100: 
101: 	idx_t comparison_size;
102: 	idx_t entry_size;
103: 
104: 	RowLayout blob_layout;
105: 	unordered_map<idx_t, idx_t> sorting_to_blob_col;
106: };
107: 
108: class OrderGlobalState : public GlobalOperatorState {
109: public:
110: 	OrderGlobalState(SortingState sorting_state, RowLayout payload_layout)
111: 	    : sorting_state(move(sorting_state)), payload_layout(move(payload_layout)), total_count(0),
112: 	      sorting_heap_capacity(Storage::BLOCK_SIZE), payload_heap_capacity(Storage::BLOCK_SIZE), external(false) {
113: 		auto thinnest_row = MinValue(sorting_state.entry_size, payload_layout.GetRowWidth());
114: 		if (!sorting_state.all_constant) {
115: 			thinnest_row = MinValue(thinnest_row, sorting_state.blob_layout.GetRowWidth());
116: 		}
117: 		block_capacity = (Storage::BLOCK_SIZE + thinnest_row - 1) / thinnest_row;
118: 	}
119: 
120: 	~OrderGlobalState() override;
121: 
122: 	//! The lock for updating the order global state
123: 	std::mutex lock;
124: 	//! Constants concerning sorting and payload data
125: 	const SortingState sorting_state;
126: 	const RowLayout payload_layout;
127: 
128: 	//! Sorted data
129: 	vector<unique_ptr<SortedBlock>> sorted_blocks;
130: 	vector<vector<unique_ptr<SortedBlock>>> sorted_blocks_temp;
131: 	unique_ptr<SortedBlock> odd_one_out = nullptr;
132: 	//! Pinned heap data (if sorting in memory)
133: 	vector<RowDataBlock> heap_blocks;
134: 	vector<unique_ptr<BufferHandle>> pinned_blocks;
135: 
136: 	//! Total count - get set in PhysicalOrder::Finalize
137: 	idx_t total_count;
138: 	//! Capacity (number of rows) used to initialize blocks
139: 	idx_t block_capacity;
140: 	//! Capacity (number of bytes) used to initialize blocks
141: 	idx_t sorting_heap_capacity;
142: 	idx_t payload_heap_capacity;
143: 
144: 	//! Whether we are doing an external sort
145: 	bool external;
146: 	//! Progress in merge path stage
147: 	idx_t pair_idx;
148: 	idx_t l_start;
149: 	idx_t r_start;
150: };
151: 
152: class OrderLocalState : public LocalSinkState {
153: public:
154: 	OrderLocalState() : initialized(false) {
155: 	}
156: 
157: 	//! Whether this local state has been initialized
158: 	bool initialized;
159: 	//! Local copy of the sorting expression executor
160: 	ExpressionExecutor executor;
161: 	//! Holds a vector of incoming sorting columns
162: 	DataChunk sort;
163: 
164: 	//! Initialize the local state using the global state
165: 	void Initialize(ClientContext &context, OrderGlobalState &gstate) {
166: 		auto &buffer_manager = BufferManager::GetBufferManager(context);
167: 		auto &sorting_state = gstate.sorting_state;
168: 		auto &payload_layout = gstate.payload_layout;
169: 		// Radix sorting data
170: 		idx_t vectors_per_block =
171: 		    (Storage::BLOCK_SIZE / sorting_state.entry_size + STANDARD_VECTOR_SIZE) / STANDARD_VECTOR_SIZE;
172: 		radix_sorting_data = make_unique<RowDataCollection>(buffer_manager, vectors_per_block * STANDARD_VECTOR_SIZE,
173: 		                                                    sorting_state.entry_size);
174: 		// Blob sorting data
175: 		if (!sorting_state.all_constant) {
176: 			auto blob_row_width = sorting_state.blob_layout.GetRowWidth();
177: 			vectors_per_block = (Storage::BLOCK_SIZE / blob_row_width + STANDARD_VECTOR_SIZE) / STANDARD_VECTOR_SIZE;
178: 			blob_sorting_data = make_unique<RowDataCollection>(buffer_manager, vectors_per_block * STANDARD_VECTOR_SIZE,
179: 			                                                   blob_row_width);
180: 			blob_sorting_heap = make_unique<RowDataCollection>(buffer_manager, (idx_t)Storage::BLOCK_SIZE, 1, true);
181: 		}
182: 		// Payload data
183: 		auto payload_row_width = payload_layout.GetRowWidth();
184: 		vectors_per_block = (Storage::BLOCK_SIZE / payload_row_width + STANDARD_VECTOR_SIZE) / STANDARD_VECTOR_SIZE;
185: 		payload_data =
186: 		    make_unique<RowDataCollection>(buffer_manager, vectors_per_block * STANDARD_VECTOR_SIZE, payload_row_width);
187: 		payload_heap = make_unique<RowDataCollection>(buffer_manager, (idx_t)Storage::BLOCK_SIZE, 1, true);
188: 		// Init done
189: 		initialized = true;
190: 	}
191: 
192: 	//! Whether the localstate has collected enough data to perform an external sort
193: 	bool Full(ClientContext &context, const SortingState &sorting_state, const RowLayout &payload_layout) {
194: 		// Compute the size of the collected data (in bytes)
195: 		idx_t size_in_bytes = radix_sorting_data->count * sorting_state.entry_size;
196: 		if (!sorting_state.all_constant) {
197: 			size_in_bytes += blob_sorting_data->count * sorting_state.blob_layout.GetRowWidth();
198: 			for (auto &block : blob_sorting_heap->blocks) {
199: 				size_in_bytes += block.byte_offset;
200: 			}
201: 		}
202: 		size_in_bytes += payload_data->count * payload_layout.GetRowWidth();
203: 		if (!payload_layout.AllConstant()) {
204: 			for (auto &block : payload_data->blocks) {
205: 				size_in_bytes += block.byte_offset;
206: 			}
207: 		}
208: 		// Get the max memory and number of threads
209: 		auto &buffer_manager = BufferManager::GetBufferManager(context);
210: 		auto &task_scheduler = TaskScheduler::GetScheduler(context);
211: 		idx_t max_memory = buffer_manager.GetMaxMemory();
212: 		idx_t num_threads = task_scheduler.NumberOfThreads();
213: 		// Memory usage per thread should scale with max mem / num threads
214: 		// We take 15% of the max memory, to be VERY conservative
215: 		return size_in_bytes > (0.15 * max_memory / num_threads);
216: 	}
217: 
218: 	//! Radix/memcmp sortable data
219: 	unique_ptr<RowDataCollection> radix_sorting_data;
220: 	//! Variable sized sorting data and accompanying heap
221: 	unique_ptr<RowDataCollection> blob_sorting_data;
222: 	unique_ptr<RowDataCollection> blob_sorting_heap;
223: 	//! Payload data and accompanying heap
224: 	unique_ptr<RowDataCollection> payload_data;
225: 	unique_ptr<RowDataCollection> payload_heap;
226: 	//! Sorted data
227: 	vector<unique_ptr<SortedBlock>> sorted_blocks;
228: 	//! Constant buffers allocated for vector serialization
229: 	const SelectionVector &sel_ptr = FlatVector::INCREMENTAL_SELECTION_VECTOR;
230: 	Vector addresses = Vector(LogicalType::POINTER);
231: 	idx_t entry_sizes[STANDARD_VECTOR_SIZE];
232: };
233: 
234: unique_ptr<GlobalOperatorState> PhysicalOrder::GetGlobalState(ClientContext &context) {
235: 	RowLayout payload_layout;
236: 	payload_layout.Initialize(types, false);
237: 	auto state = make_unique<OrderGlobalState>(SortingState(orders), payload_layout);
238: 	state->external = context.force_external;
239: 	return move(state);
240: }
241: 
242: unique_ptr<LocalSinkState> PhysicalOrder::GetLocalSinkState(ExecutionContext &context) {
243: 	auto result = make_unique<OrderLocalState>();
244: 	vector<LogicalType> types;
245: 	for (auto &order : orders) {
246: 		types.push_back(order.expression->return_type);
247: 		result->executor.AddExpression(*order.expression);
248: 	}
249: 	result->sort.Initialize(types);
250: 	return move(result);
251: }
252: 
253: void PhysicalOrder::Sink(ExecutionContext &context, GlobalOperatorState &gstate_p, LocalSinkState &lstate_p,
254:                          DataChunk &input) const {
255: 	auto &gstate = (OrderGlobalState &)gstate_p;
256: 	auto &lstate = (OrderLocalState &)lstate_p;
257: 	const auto &sorting_state = gstate.sorting_state;
258: 	const auto &payload_layout = gstate.payload_layout;
259: 
260: 	if (!lstate.initialized) {
261: 		lstate.Initialize(context.client, gstate);
262: 	}
263: 
264: 	// Obtain sorting columns
265: 	auto &sort = lstate.sort;
266: 	lstate.executor.Execute(input, sort);
267: 
268: 	// Build and serialize sorting data to radix sortable rows
269: 	auto data_pointers = FlatVector::GetData<data_ptr_t>(lstate.addresses);
270: 	auto handles = lstate.radix_sorting_data->Build(sort.size(), data_pointers, nullptr);
271: 	for (idx_t sort_col = 0; sort_col < sort.ColumnCount(); sort_col++) {
272: 		bool has_null = sorting_state.has_null[sort_col];
273: 		bool nulls_first = sorting_state.order_by_null_types[sort_col] == OrderByNullType::NULLS_FIRST;
274: 		bool desc = sorting_state.order_types[sort_col] == OrderType::DESCENDING;
275: 		// TODO: use actual string statistics
276: 		RowOperations::RadixScatter(sort.data[sort_col], sort.size(), lstate.sel_ptr, sort.size(), data_pointers, desc,
277: 		                            has_null, nulls_first, string_t::INLINE_LENGTH,
278: 		                            sorting_state.column_sizes[sort_col]);
279: 	}
280: 
281: 	// Also fully serialize blob sorting columns (to be able to break ties
282: 	if (!sorting_state.all_constant) {
283: 		DataChunk blob_chunk;
284: 		blob_chunk.SetCardinality(sort.size());
285: 		for (idx_t sort_col = 0; sort_col < sort.ColumnCount(); sort_col++) {
286: 			if (!TypeIsConstantSize(sort.data[sort_col].GetType().InternalType())) {
287: 				blob_chunk.data.emplace_back(sort.data[sort_col]);
288: 			}
289: 		}
290: 		handles = lstate.blob_sorting_data->Build(blob_chunk.size(), data_pointers, nullptr);
291: 		auto blob_data = blob_chunk.Orrify();
292: 		RowOperations::Scatter(blob_chunk, blob_data.get(), sorting_state.blob_layout, lstate.addresses,
293: 		                       *lstate.blob_sorting_heap, lstate.sel_ptr, blob_chunk.size());
294: 	}
295: 
296: 	// Finally, serialize payload data
297: 	handles = lstate.payload_data->Build(input.size(), data_pointers, nullptr);
298: 	auto input_data = input.Orrify();
299: 	RowOperations::Scatter(input, input_data.get(), payload_layout, lstate.addresses, *lstate.payload_heap,
300: 	                       lstate.sel_ptr, input.size());
301: 
302: 	// When sorting data reaches a certain size, we sort it
303: 	if (lstate.Full(context.client, sorting_state, payload_layout)) {
304: 		SortLocalState(context.client, lstate, gstate);
305: 	}
306: }
307: 
308: void PhysicalOrder::Combine(ExecutionContext &context, GlobalOperatorState &gstate_p, LocalSinkState &lstate_p) {
309: 	auto &gstate = (OrderGlobalState &)gstate_p;
310: 	auto &lstate = (OrderLocalState &)lstate_p;
311: 	if (!lstate.radix_sorting_data) {
312: 		return;
313: 	}
314: 
315: 	SortLocalState(context.client, lstate, gstate);
316: 	lock_guard<mutex> append_lock(gstate.lock);
317: 	for (auto &cb : lstate.sorted_blocks) {
318: 		gstate.sorted_blocks.push_back(move(cb));
319: 	}
320: }
321: 
322: //! Object that holds sorted rows, and an accompanying heap if there are blobs
323: struct SortedData {
324: public:
325: 	SortedData(const RowLayout &layout, BufferManager &buffer_manager, OrderGlobalState &state)
326: 	    : layout(layout), block_idx(0), entry_idx(0), buffer_manager(buffer_manager), state(state) {
327: 	}
328: 	//! Number of rows that this object holds
329: 	idx_t Count() {
330: 		idx_t count = std::accumulate(data_blocks.begin(), data_blocks.end(), (idx_t)0,
331: 		                              [](idx_t a, const RowDataBlock &b) { return a + b.count; });
332: 		if (!layout.AllConstant() && state.external) {
333: 			D_ASSERT(count == std::accumulate(heap_blocks.begin(), heap_blocks.end(), (idx_t)0,
334: 			                                  [](idx_t a, const RowDataBlock &b) { return a + b.count; }));
335: 		}
336: 		return count;
337: 	}
338: 	//! Pin the current block such that it can be read
339: 	void Pin() {
340: 		PinData();
341: 		if (!layout.AllConstant() && state.external) {
342: 			PinHeap();
343: 		}
344: 	}
345: 	//! Pointer to the row that is currently being read from
346: 	inline data_ptr_t DataPtr() const {
347: 		D_ASSERT(data_blocks[block_idx].block->Readers() != 0 &&
348: 		         data_handle->handle->BlockId() == data_blocks[block_idx].block->BlockId());
349: 		return data_ptr + entry_idx * layout.GetRowWidth();
350: 	}
351: 	//! Pointer to the heap row that corresponds to the current row
352: 	inline data_ptr_t HeapPtr() const {
353: 		D_ASSERT(!layout.AllConstant() && state.external);
354: 		D_ASSERT(heap_blocks[block_idx].block->Readers() != 0 &&
355: 		         heap_handle->handle->BlockId() == heap_blocks[block_idx].block->BlockId());
356: 		return heap_ptr + Load<idx_t>(DataPtr() + layout.GetHeapPointerOffset());
357: 	}
358: 	//! Advance one row
359: 	inline void Advance(const bool &adv) {
360: 		entry_idx += adv;
361: 		if (entry_idx == data_blocks[block_idx].count) {
362: 			block_idx++;
363: 			entry_idx = 0;
364: 			if (block_idx < data_blocks.size()) {
365: 				Pin();
366: 			} else {
367: 				UnpinAndReset(block_idx, entry_idx);
368: 				return;
369: 			}
370: 		}
371: 	}
372: 	//! Initialize new block to write to
373: 	void CreateBlock() {
374: 		data_blocks.emplace_back(buffer_manager, state.block_capacity, layout.GetRowWidth());
375: 		if (!layout.AllConstant() && state.external) {
376: 			heap_blocks.emplace_back(buffer_manager, heap_capacity, 1);
377: 			D_ASSERT(data_blocks.size() == heap_blocks.size());
378: 		}
379: 	}
380: 	//! Unpin blocks and reset read indices to the given indices
381: 	void UnpinAndReset(idx_t block_idx_to, idx_t entry_idx_to) {
382: 		data_handle = nullptr;
383: 		heap_handle = nullptr;
384: 		data_ptr = nullptr;
385: 		heap_ptr = nullptr;
386: 		block_idx = block_idx_to;
387: 		entry_idx = entry_idx_to;
388: 	}
389: 	//! Create a slice that holds the rows between the start and end indices
390: 	unique_ptr<SortedData> CreateSlice(idx_t start_block_index, idx_t start_entry_index, idx_t end_block_index,
391: 	                                   idx_t end_entry_index) {
392: 		// Add the corresponding blocks to the result
393: 		auto result = make_unique<SortedData>(layout, buffer_manager, state);
394: 		for (idx_t i = start_block_index; i <= end_block_index; i++) {
395: 			result->data_blocks.push_back(data_blocks[i]);
396: 			if (!layout.AllConstant() && state.external) {
397: 				result->heap_blocks.push_back(heap_blocks[i]);
398: 			}
399: 		}
400: 		// Use start and end entry indices to set the boundaries
401: 		result->entry_idx = start_entry_index;
402: 		D_ASSERT(end_entry_index <= result->data_blocks.back().count);
403: 		result->data_blocks.back().count = end_entry_index;
404: 		if (!layout.AllConstant() && state.external) {
405: 			result->heap_blocks.back().count = end_entry_index;
406: 		}
407: 		return result;
408: 	}
409: 
410: 	void Unswizzle() {
411: 		if (layout.AllConstant()) {
412: 			return;
413: 		}
414: 		for (idx_t i = 0; i < data_blocks.size(); i++) {
415: 			auto &data_block = data_blocks[i];
416: 			auto &heap_block = heap_blocks[i];
417: 			auto data_handle_p = buffer_manager.Pin(data_block.block);
418: 			auto heap_handle_p = buffer_manager.Pin(heap_block.block);
419: 			RowOperations::UnswizzleHeapPointer(layout, data_handle_p->Ptr(), heap_handle_p->Ptr(), data_block.count);
420: 			RowOperations::UnswizzleColumns(layout, data_handle_p->Ptr(), data_block.count);
421: 			state.heap_blocks.push_back(move(heap_block));
422: 			state.pinned_blocks.push_back(move(heap_handle_p));
423: 		}
424: 		heap_blocks.clear();
425: 	}
426: 
427: 	//! Layout of this data
428: 	const RowLayout layout;
429: 	//! Data and heap blocks
430: 	vector<RowDataBlock> data_blocks;
431: 	vector<RowDataBlock> heap_blocks;
432: 	//! Capacity (in bytes) of the heap blocks
433: 	idx_t heap_capacity;
434: 	//! Read indices
435: 	idx_t block_idx;
436: 	idx_t entry_idx;
437: 
438: private:
439: 	//! Pin fixed-size row data
440: 	void PinData() {
441: 		D_ASSERT(block_idx < data_blocks.size());
442: 		data_handle = buffer_manager.Pin(data_blocks[block_idx].block);
443: 		data_ptr = data_handle->Ptr();
444: 	}
445: 	//! Pin the accompanying heap data (if any)
446: 	void PinHeap() {
447: 		D_ASSERT(!layout.AllConstant() && state.external);
448: 		heap_handle = buffer_manager.Pin(heap_blocks[block_idx].block);
449: 		heap_ptr = heap_handle->Ptr();
450: 	}
451: 
452: 	//! The buffer manager
453: 	BufferManager &buffer_manager;
454: 	//! The global state
455: 	OrderGlobalState &state;
456: 	//! Buffer handles to the data being currently read
457: 	unique_ptr<BufferHandle> data_handle;
458: 	unique_ptr<BufferHandle> heap_handle;
459: 	//! Pointers into the buffers being currently read
460: 	data_ptr_t data_ptr;
461: 	data_ptr_t heap_ptr;
462: };
463: 
464: //! Block that holds sorted rows: radix, blob and payload data
465: struct SortedBlock {
466: public:
467: 	SortedBlock(BufferManager &buffer_manager, OrderGlobalState &state)
468: 	    : block_idx(0), entry_idx(0), buffer_manager(buffer_manager), state(state), sorting_state(state.sorting_state),
469: 	      payload_layout(state.payload_layout) {
470: 		blob_sorting_data = make_unique<SortedData>(sorting_state.blob_layout, buffer_manager, state);
471: 		payload_data = make_unique<SortedData>(payload_layout, buffer_manager, state);
472: 	}
473: 	//! Number of rows that this object holds
474: 	idx_t Count() {
475: 		idx_t count = std::accumulate(radix_sorting_data.begin(), radix_sorting_data.end(), 0,
476: 		                              [](idx_t a, const RowDataBlock &b) { return a + b.count; });
477: 		if (!sorting_state.all_constant) {
478: 			D_ASSERT(count == blob_sorting_data->Count());
479: 		}
480: 		D_ASSERT(count == payload_data->Count());
481: 		return count;
482: 	}
483: 	//! The remaining number of rows to be read from this object
484: 	idx_t Remaining() {
485: 		idx_t remaining = 0;
486: 		if (block_idx < radix_sorting_data.size()) {
487: 			remaining += radix_sorting_data[block_idx].count - entry_idx;
488: 			for (idx_t i = block_idx + 1; i < radix_sorting_data.size(); i++) {
489: 				remaining += radix_sorting_data[i].count;
490: 			}
491: 		}
492: 		return remaining;
493: 	}
494: 	//! Initialize this block to write data to
495: 	void InitializeWrite() {
496: 		CreateBlock();
497: 		if (!sorting_state.all_constant) {
498: 			blob_sorting_data->heap_capacity = state.sorting_heap_capacity;
499: 			blob_sorting_data->CreateBlock();
500: 		}
501: 		payload_data->heap_capacity = state.payload_heap_capacity;
502: 		payload_data->CreateBlock();
503: 	}
504: 	//! Init new block to write to
505: 	void CreateBlock() {
506: 		radix_sorting_data.emplace_back(buffer_manager, state.block_capacity, sorting_state.entry_size);
507: 	}
508: 	//! Cleanup sorting data
509: 	void UnregisterSortingBlocks() {
510: 		for (auto &block : radix_sorting_data) {
511: 			buffer_manager.UnregisterBlock(block.block->BlockId(), true);
512: 		}
513: 		if (!sorting_state.all_constant) {
514: 			for (auto &block : blob_sorting_data->data_blocks) {
515: 				buffer_manager.UnregisterBlock(block.block->BlockId(), true);
516: 			}
517: 			if (state.external) {
518: 				for (auto &block : blob_sorting_data->heap_blocks) {
519: 					buffer_manager.UnregisterBlock(block.block->BlockId(), true);
520: 				}
521: 			}
522: 		}
523: 	}
524: 	//! Cleanup payload data
525: 	void UnregisterPayloadBlocks() {
526: 		for (auto &block : payload_data->data_blocks) {
527: 			buffer_manager.UnregisterBlock(block.block->BlockId(), true);
528: 		}
529: 		if (state.external) {
530: 			if (!payload_data->layout.AllConstant()) {
531: 				for (auto &block : payload_data->heap_blocks) {
532: 					buffer_manager.UnregisterBlock(block.block->BlockId(), true);
533: 				}
534: 			}
535: 		}
536: 	}
537: 	//! Fill this sorted block by appending the blocks held by a vector of sorted blocks
538: 	void AppendSortedBlocks(vector<unique_ptr<SortedBlock>> &sorted_blocks) {
539: 		D_ASSERT(Count() == 0);
540: 		for (auto &sb : sorted_blocks) {
541: 			for (auto &radix_block : sb->radix_sorting_data) {
542: 				radix_sorting_data.push_back(move(radix_block));
543: 			}
544: 			if (!sorting_state.all_constant) {
545: 				for (auto &blob_block : sb->blob_sorting_data->data_blocks) {
546: 					blob_sorting_data->data_blocks.push_back(move(blob_block));
547: 				}
548: 				for (auto &heap_block : sb->blob_sorting_data->heap_blocks) {
549: 					blob_sorting_data->heap_blocks.push_back(move(heap_block));
550: 				}
551: 			}
552: 			for (auto &payload_data_block : sb->payload_data->data_blocks) {
553: 				payload_data->data_blocks.push_back(move(payload_data_block));
554: 			}
555: 			if (!payload_data->layout.AllConstant()) {
556: 				for (auto &payload_heap_block : sb->payload_data->heap_blocks) {
557: 					payload_data->heap_blocks.push_back(move(payload_heap_block));
558: 				}
559: 			}
560: 		}
561: 	}
562: 	//! Locate the block and entry index of a row in this block,
563: 	//! given an index between 0 and the total number of rows in this block
564: 	void GlobalToLocalIndex(const idx_t &global_idx, idx_t &local_block_index, idx_t &local_entry_index) {
565: 		if (global_idx == Count()) {
566: 			local_block_index = radix_sorting_data.size() - 1;
567: 			local_entry_index = radix_sorting_data.back().count;
568: 			return;
569: 		}
570: 		D_ASSERT(global_idx < Count());
571: 		local_entry_index = global_idx;
572: 		for (local_block_index = 0; local_block_index < radix_sorting_data.size(); local_block_index++) {
573: 			const idx_t &block_count = radix_sorting_data[local_block_index].count;
574: 			if (local_entry_index >= block_count) {
575: 				local_entry_index -= block_count;
576: 			} else {
577: 				break;
578: 			}
579: 		}
580: 		D_ASSERT(local_entry_index < radix_sorting_data[local_block_index].count);
581: 	}
582: 	//! Create a slice that holds the rows between the start and end indices
583: 	unique_ptr<SortedBlock> CreateSlice(const idx_t start, const idx_t end) {
584: 		// Identify blocks/entry indices of this slice
585: 		idx_t start_block_index;
586: 		idx_t start_entry_index;
587: 		GlobalToLocalIndex(start, start_block_index, start_entry_index);
588: 		idx_t end_block_index;
589: 		idx_t end_entry_index;
590: 		GlobalToLocalIndex(end, end_block_index, end_entry_index);
591: 		// Add the corresponding blocks to the result
592: 		auto result = make_unique<SortedBlock>(buffer_manager, state);
593: 		for (idx_t i = start_block_index; i <= end_block_index; i++) {
594: 			result->radix_sorting_data.push_back(radix_sorting_data[i]);
595: 		}
596: 		// Use start and end entry indices to set the boundaries
597: 		result->entry_idx = start_entry_index;
598: 		D_ASSERT(end_entry_index <= result->radix_sorting_data.back().count);
599: 		result->radix_sorting_data.back().count = end_entry_index;
600: 		// Same for the var size sorting data
601: 		if (!sorting_state.all_constant) {
602: 			result->blob_sorting_data =
603: 			    blob_sorting_data->CreateSlice(start_block_index, start_entry_index, end_block_index, end_entry_index);
604: 		}
605: 		// And the payload data
606: 		result->payload_data =
607: 		    payload_data->CreateSlice(start_block_index, start_entry_index, end_block_index, end_entry_index);
608: 		D_ASSERT(result->Remaining() == end - start);
609: 		return result;
610: 	}
611: 
612: 	idx_t HeapSize() const {
613: 		idx_t result = 0;
614: 		if (!sorting_state.all_constant) {
615: 			for (auto &block : blob_sorting_data->heap_blocks) {
616: 				result += block.capacity;
617: 			}
618: 		}
619: 		if (!payload_layout.AllConstant()) {
620: 			for (auto &block : payload_data->heap_blocks) {
621: 				result += block.capacity;
622: 			}
623: 		}
624: 		return result;
625: 	}
626: 
627: public:
628: 	//! Radix/memcmp sortable data
629: 	vector<RowDataBlock> radix_sorting_data;
630: 	idx_t block_idx;
631: 	idx_t entry_idx;
632: 	//! Variable sized sorting data
633: 	unique_ptr<SortedData> blob_sorting_data;
634: 	//! Payload data
635: 	unique_ptr<SortedData> payload_data;
636: 
637: private:
638: 	//! Buffer manager, and sorting state constants
639: 	BufferManager &buffer_manager;
640: 	OrderGlobalState &state;
641: 	const SortingState &sorting_state;
642: 	const RowLayout &payload_layout;
643: 
644: 	//! Handle and ptr for sorting_blocks
645: 	unique_ptr<BufferHandle> sorting_handle;
646: };
647: 
648: OrderGlobalState::~OrderGlobalState() {
649: 	std::lock_guard<mutex> glock(lock);
650: 	for (auto &sb : sorted_blocks) {
651: 		sb->UnregisterPayloadBlocks();
652: 	}
653: 	sorted_blocks.clear();
654: }
655: 
656: //! Concatenates the blocks in a RowDataCollection into a single block
657: static RowDataBlock ConcatenateBlocks(BufferManager &buffer_manager, RowDataCollection &row_data) {
658: 	// Create block with the correct capacity
659: 	const idx_t &entry_size = row_data.entry_size;
660: 	idx_t capacity = MaxValue(Storage::BLOCK_SIZE / entry_size + 1, row_data.count);
661: 	RowDataBlock new_block(buffer_manager, capacity, entry_size);
662: 	new_block.count = row_data.count;
663: 	auto new_block_handle = buffer_manager.Pin(new_block.block);
664: 	data_ptr_t new_block_ptr = new_block_handle->Ptr();
665: 	// Copy the data of the blocks into a single block
666: 	for (auto &block : row_data.blocks) {
667: 		auto block_handle = buffer_manager.Pin(block.block);
668: 		memcpy(new_block_ptr, block_handle->Ptr(), block.count * entry_size);
669: 		new_block_ptr += block.count * entry_size;
670: 		buffer_manager.UnregisterBlock(block.block->BlockId(), true);
671: 	}
672: 	row_data.blocks.clear();
673: 	row_data.count = 0;
674: 	return new_block;
675: }
676: 
677: //! Whether a tie between two blobs can be broken
678: static inline bool TieIsBreakable(const idx_t &col_idx, const data_ptr_t row_ptr, const RowLayout &row_layout) {
679: 	// Check if the blob is NULL
680: 	ValidityBytes row_mask(row_ptr);
681: 	idx_t entry_idx;
682: 	idx_t idx_in_entry;
683: 	ValidityBytes::GetEntryIndex(col_idx, entry_idx, idx_in_entry);
684: 	if (!row_mask.RowIsValid(row_mask.GetValidityEntry(entry_idx), idx_in_entry)) {
685: 		// Can't break a NULL tie
686: 		return false;
687: 	}
688: 	if (row_layout.GetTypes()[col_idx].InternalType() == PhysicalType::VARCHAR) {
689: 		const auto &tie_col_offset = row_layout.GetOffsets()[col_idx];
690: 		string_t tie_string = Load<string_t>(row_ptr + tie_col_offset);
691: 		if (tie_string.GetSize() < string_t::INLINE_LENGTH) {
692: 			// No need to break the tie - we already compared the full string
693: 			return false;
694: 		}
695: 	}
696: 	return true;
697: }
698: 
699: template <class T>
700: static inline int TemplatedCompareVal(const data_ptr_t &left_ptr, const data_ptr_t &right_ptr) {
701: 	const auto left_val = Load<T>(left_ptr);
702: 	const auto right_val = Load<T>(right_ptr);
703: 	if (Equals::Operation<T>(left_val, right_val)) {
704: 		return 0;
705: 	} else if (LessThan::Operation<T>(left_val, right_val)) {
706: 		return -1;
707: 	} else {
708: 		return 1;
709: 	}
710: }
711: 
712: template <class T>
713: static inline int TemplatedCompareAndAdvance(data_ptr_t &left_ptr, data_ptr_t &right_ptr) {
714: 	auto result = TemplatedCompareVal<T>(left_ptr, right_ptr);
715: 	left_ptr += sizeof(T);
716: 	right_ptr += sizeof(T);
717: 	return result;
718: }
719: 
720: static inline int CompareStringAndAdvance(data_ptr_t &left_ptr, data_ptr_t &right_ptr) {
721: 	// Construct the string_t
722: 	uint32_t left_string_size = Load<uint32_t>(left_ptr);
723: 	uint32_t right_string_size = Load<uint32_t>(right_ptr);
724: 	left_ptr += sizeof(uint32_t);
725: 	right_ptr += sizeof(uint32_t);
726: 	string_t left_val((const char *)left_ptr, left_string_size);
727: 	string_t right_val((const char *)right_ptr, left_string_size);
728: 	left_ptr += left_string_size;
729: 	right_ptr += right_string_size;
730: 	// Compare
731: 	return TemplatedCompareVal<string_t>((data_ptr_t)&left_val, (data_ptr_t)&right_val);
732: }
733: 
734: template <class T>
735: static inline int TemplatedCompareListLoop(data_ptr_t &left_ptr, data_ptr_t &right_ptr,
736:                                            const ValidityBytes &left_validity, const ValidityBytes &right_validity,
737:                                            const idx_t &count) {
738: 	int comp_res = 0;
739: 	bool left_valid;
740: 	bool right_valid;
741: 	idx_t entry_idx;
742: 	idx_t idx_in_entry;
743: 	for (idx_t i = 0; i < count; i++) {
744: 		ValidityBytes::GetEntryIndex(i, entry_idx, idx_in_entry);
745: 		left_valid = left_validity.RowIsValid(left_validity.GetValidityEntry(entry_idx), idx_in_entry);
746: 		right_valid = right_validity.RowIsValid(right_validity.GetValidityEntry(entry_idx), idx_in_entry);
747: 		comp_res = TemplatedCompareAndAdvance<T>(left_ptr, right_ptr);
748: 		if (!left_valid && !right_valid) {
749: 			comp_res = 0;
750: 		} else if (!left_valid) {
751: 			comp_res = 1;
752: 		} else if (!right_valid) {
753: 			comp_res = -1;
754: 		}
755: 		if (comp_res != 0) {
756: 			break;
757: 		}
758: 	}
759: 	return comp_res;
760: }
761: 
762: //! Compares two struct values at the given pointers (recursive)
763: static inline int CompareStructAndAdvance(data_ptr_t &left_ptr, data_ptr_t &right_ptr,
764:                                           const child_list_t<LogicalType> &types) {
765: 	idx_t count = types.size();
766: 	// Load validity masks
767: 	ValidityBytes left_validity(left_ptr);
768: 	ValidityBytes right_validity(right_ptr);
769: 	left_ptr += (count + 7) / 8;
770: 	right_ptr += (count + 7) / 8;
771: 	// Initialize variables
772: 	bool left_valid;
773: 	bool right_valid;
774: 	idx_t entry_idx;
775: 	idx_t idx_in_entry;
776: 	// Compare
777: 	int comp_res = 0;
778: 	for (idx_t i = 0; i < count; i++) {
779: 		ValidityBytes::GetEntryIndex(i, entry_idx, idx_in_entry);
780: 		left_valid = left_validity.RowIsValid(left_validity.GetValidityEntry(entry_idx), idx_in_entry);
781: 		right_valid = right_validity.RowIsValid(right_validity.GetValidityEntry(entry_idx), idx_in_entry);
782: 		auto &type = types[i].second;
783: 		if ((left_valid && right_valid) || TypeIsConstantSize(type.InternalType())) {
784: 			comp_res = PhysicalOrder::CompareValAndAdvance(left_ptr, right_ptr, types[i].second);
785: 		}
786: 		if (!left_valid && !right_valid) {
787: 			comp_res = 0;
788: 		} else if (!left_valid) {
789: 			comp_res = 1;
790: 		} else if (!right_valid) {
791: 			comp_res = -1;
792: 		}
793: 		if (comp_res != 0) {
794: 			break;
795: 		}
796: 	}
797: 	return comp_res;
798: }
799: 
800: //! Compare two list values at the pointers (can be recursive if nested type)
801: static inline int CompareListAndAdvance(data_ptr_t &left_ptr, data_ptr_t &right_ptr, const LogicalType &type) {
802: 	// Load list lengths
803: 	auto left_len = Load<idx_t>(left_ptr);
804: 	auto right_len = Load<idx_t>(right_ptr);
805: 	left_ptr += sizeof(idx_t);
806: 	right_ptr += sizeof(idx_t);
807: 	// Load list validity masks
808: 	ValidityBytes left_validity(left_ptr);
809: 	ValidityBytes right_validity(right_ptr);
810: 	left_ptr += (left_len + 7) / 8;
811: 	right_ptr += (right_len + 7) / 8;
812: 	// Compare
813: 	int comp_res = 0;
814: 	idx_t count = MinValue(left_len, right_len);
815: 	if (TypeIsConstantSize(type.InternalType())) {
816: 		// Templated code for fixed-size types
817: 		switch (type.InternalType()) {
818: 		case PhysicalType::BOOL:
819: 		case PhysicalType::INT8:
820: 			comp_res = TemplatedCompareListLoop<int8_t>(left_ptr, right_ptr, left_validity, right_validity, count);
821: 			break;
822: 		case PhysicalType::INT16:
823: 			comp_res = TemplatedCompareListLoop<int16_t>(left_ptr, right_ptr, left_validity, right_validity, count);
824: 			break;
825: 		case PhysicalType::INT32:
826: 			comp_res = TemplatedCompareListLoop<int32_t>(left_ptr, right_ptr, left_validity, right_validity, count);
827: 			break;
828: 		case PhysicalType::INT64:
829: 			comp_res = TemplatedCompareListLoop<int64_t>(left_ptr, right_ptr, left_validity, right_validity, count);
830: 			break;
831: 		case PhysicalType::UINT8:
832: 			comp_res = TemplatedCompareListLoop<uint8_t>(left_ptr, right_ptr, left_validity, right_validity, count);
833: 			break;
834: 		case PhysicalType::UINT16:
835: 			comp_res = TemplatedCompareListLoop<uint16_t>(left_ptr, right_ptr, left_validity, right_validity, count);
836: 			break;
837: 		case PhysicalType::UINT32:
838: 			comp_res = TemplatedCompareListLoop<uint32_t>(left_ptr, right_ptr, left_validity, right_validity, count);
839: 			break;
840: 		case PhysicalType::UINT64:
841: 			comp_res = TemplatedCompareListLoop<uint64_t>(left_ptr, right_ptr, left_validity, right_validity, count);
842: 			break;
843: 		case PhysicalType::INT128:
844: 			comp_res = TemplatedCompareListLoop<hugeint_t>(left_ptr, right_ptr, left_validity, right_validity, count);
845: 			break;
846: 		case PhysicalType::FLOAT:
847: 			comp_res = TemplatedCompareListLoop<float>(left_ptr, right_ptr, left_validity, right_validity, count);
848: 			break;
849: 		case PhysicalType::DOUBLE:
850: 			comp_res = TemplatedCompareListLoop<double>(left_ptr, right_ptr, left_validity, right_validity, count);
851: 			break;
852: 		case PhysicalType::INTERVAL:
853: 			comp_res = TemplatedCompareListLoop<interval_t>(left_ptr, right_ptr, left_validity, right_validity, count);
854: 			break;
855: 		default:
856: 			throw NotImplementedException("CompareListAndAdvance for fixed-size type %s", type.ToString());
857: 		}
858: 	} else {
859: 		// Variable-sized list entries
860: 		bool left_valid;
861: 		bool right_valid;
862: 		idx_t entry_idx;
863: 		idx_t idx_in_entry;
864: 		// Size (in bytes) of all variable-sizes entries is stored before the entries begin,
865: 		// to make deserialization easier. We need to skip over them
866: 		left_ptr += left_len * sizeof(idx_t);
867: 		right_ptr += right_len * sizeof(idx_t);
868: 		for (idx_t i = 0; i < count; i++) {
869: 			ValidityBytes::GetEntryIndex(i, entry_idx, idx_in_entry);
870: 			left_valid = left_validity.RowIsValid(left_validity.GetValidityEntry(entry_idx), idx_in_entry);
871: 			right_valid = right_validity.RowIsValid(right_validity.GetValidityEntry(entry_idx), idx_in_entry);
872: 			if (left_valid && right_valid) {
873: 				switch (type.InternalType()) {
874: 				case PhysicalType::LIST:
875: 					comp_res = CompareListAndAdvance(left_ptr, right_ptr, ListType::GetChildType(type));
876: 					break;
877: 				case PhysicalType::VARCHAR:
878: 					comp_res = CompareStringAndAdvance(left_ptr, right_ptr);
879: 					break;
880: 				case PhysicalType::STRUCT:
881: 					comp_res = CompareStructAndAdvance(left_ptr, right_ptr, StructType::GetChildTypes(type));
882: 					break;
883: 				default:
884: 					throw NotImplementedException("CompareListAndAdvance for variable-size type %s", type.ToString());
885: 				}
886: 			} else if (!left_valid && !right_valid) {
887: 				comp_res = 0;
888: 			} else if (left_valid) {
889: 				comp_res = -1;
890: 			} else {
891: 				comp_res = 1;
892: 			}
893: 			if (comp_res != 0) {
894: 				break;
895: 			}
896: 		}
897: 	}
898: 	// All values that we looped over were equal
899: 	if (comp_res == 0 && left_len != right_len) {
900: 		// Smaller lists first
901: 		if (left_len < right_len) {
902: 			comp_res = -1;
903: 		} else {
904: 			comp_res = 1;
905: 		}
906: 	}
907: 	return comp_res;
908: }
909: 
910: int PhysicalOrder::CompareValAndAdvance(data_ptr_t &l_ptr, data_ptr_t &r_ptr, const LogicalType &type) {
911: 	switch (type.InternalType()) {
912: 	case PhysicalType::BOOL:
913: 	case PhysicalType::INT8:
914: 		return TemplatedCompareAndAdvance<int8_t>(l_ptr, r_ptr);
915: 	case PhysicalType::INT16:
916: 		return TemplatedCompareAndAdvance<int16_t>(l_ptr, r_ptr);
917: 	case PhysicalType::INT32:
918: 		return TemplatedCompareAndAdvance<int32_t>(l_ptr, r_ptr);
919: 	case PhysicalType::INT64:
920: 		return TemplatedCompareAndAdvance<int64_t>(l_ptr, r_ptr);
921: 	case PhysicalType::UINT8:
922: 		return TemplatedCompareAndAdvance<uint8_t>(l_ptr, r_ptr);
923: 	case PhysicalType::UINT16:
924: 		return TemplatedCompareAndAdvance<uint16_t>(l_ptr, r_ptr);
925: 	case PhysicalType::UINT32:
926: 		return TemplatedCompareAndAdvance<uint32_t>(l_ptr, r_ptr);
927: 	case PhysicalType::UINT64:
928: 		return TemplatedCompareAndAdvance<uint64_t>(l_ptr, r_ptr);
929: 	case PhysicalType::INT128:
930: 		return TemplatedCompareAndAdvance<hugeint_t>(l_ptr, r_ptr);
931: 	case PhysicalType::FLOAT:
932: 		return TemplatedCompareAndAdvance<float>(l_ptr, r_ptr);
933: 	case PhysicalType::DOUBLE:
934: 		return TemplatedCompareAndAdvance<double>(l_ptr, r_ptr);
935: 	case PhysicalType::INTERVAL:
936: 		return TemplatedCompareAndAdvance<interval_t>(l_ptr, r_ptr);
937: 	case PhysicalType::VARCHAR:
938: 		return CompareStringAndAdvance(l_ptr, r_ptr);
939: 	case PhysicalType::LIST:
940: 		return CompareListAndAdvance(l_ptr, r_ptr, ListType::GetChildType(type));
941: 	case PhysicalType::STRUCT:
942: 		return CompareStructAndAdvance(l_ptr, r_ptr, StructType::GetChildTypes(type));
943: 	default:
944: 		throw NotImplementedException("Unimplemented CompareValAndAdvance for type %s", type.ToString());
945: 	}
946: }
947: 
948: //! Compare two blob values
949: static inline int CompareVal(const data_ptr_t l_ptr, const data_ptr_t r_ptr, const LogicalType &type) {
950: 	switch (type.InternalType()) {
951: 	case PhysicalType::VARCHAR:
952: 		return TemplatedCompareVal<string_t>(l_ptr, r_ptr);
953: 	case PhysicalType::LIST:
954: 	case PhysicalType::STRUCT: {
955: 		auto l_nested_ptr = Load<data_ptr_t>(l_ptr);
956: 		auto r_nested_ptr = Load<data_ptr_t>(r_ptr);
957: 		return PhysicalOrder::CompareValAndAdvance(l_nested_ptr, r_nested_ptr, type);
958: 	}
959: 	default:
960: 		throw NotImplementedException("Unimplemented CompareVal for type %s", type.ToString());
961: 	}
962: }
963: 
964: //! Unwizzles an offset into a pointer
965: static inline void UnswizzleSingleValue(data_ptr_t data_ptr, const data_ptr_t &heap_ptr, const LogicalType &type) {
966: 	if (type.InternalType() == PhysicalType::VARCHAR) {
967: 		data_ptr += sizeof(uint32_t) + string_t::PREFIX_LENGTH;
968: 	}
969: 	Store<data_ptr_t>(heap_ptr + Load<idx_t>(data_ptr), data_ptr);
970: }
971: 
972: //! Swizzles a pointer into an offset
973: static inline void SwizzleSingleValue(data_ptr_t data_ptr, const data_ptr_t &heap_ptr, const LogicalType &type) {
974: 	if (type.InternalType() == PhysicalType::VARCHAR) {
975: 		data_ptr += sizeof(uint32_t) + string_t::PREFIX_LENGTH;
976: 	}
977: 	Store<idx_t>(Load<data_ptr_t>(data_ptr) - heap_ptr, data_ptr);
978: }
979: 
980: //! Compares two blob values that were initially tied by their prefix
981: static inline int BreakBlobTie(const idx_t &tie_col, const SortedData &left, const SortedData &right,
982:                                const SortingState &sorting_state, const bool &external) {
983: 	const idx_t &col_idx = sorting_state.sorting_to_blob_col.at(tie_col);
984: 	data_ptr_t l_data_ptr = left.DataPtr();
985: 	data_ptr_t r_data_ptr = right.DataPtr();
986: 	if (!TieIsBreakable(col_idx, l_data_ptr, sorting_state.blob_layout)) {
987: 		// Quick check to see if ties can be broken
988: 		return 0;
989: 	}
990: 	// Align the pointers
991: 	const auto &tie_col_offset = sorting_state.blob_layout.GetOffsets()[col_idx];
992: 	l_data_ptr += tie_col_offset;
993: 	r_data_ptr += tie_col_offset;
994: 	// Do the comparison
995: 	const int order = sorting_state.order_types[tie_col] == OrderType::DESCENDING ? -1 : 1;
996: 	const auto &type = left.layout.GetTypes()[col_idx];
997: 	int result;
998: 	if (external) {
999: 		// Store heap pointers
1000: 		data_ptr_t l_heap_ptr = left.HeapPtr();
1001: 		data_ptr_t r_heap_ptr = right.HeapPtr();
1002: 		// Unswizzle offset to pointer
1003: 		UnswizzleSingleValue(l_data_ptr, l_heap_ptr, type);
1004: 		UnswizzleSingleValue(r_data_ptr, r_heap_ptr, type);
1005: 		// Compare
1006: 		result = CompareVal(l_data_ptr, r_data_ptr, type);
1007: 		// Swizzle the pointers back to offsets
1008: 		SwizzleSingleValue(l_data_ptr, l_heap_ptr, type);
1009: 		SwizzleSingleValue(r_data_ptr, r_heap_ptr, type);
1010: 	} else {
1011: 		result = CompareVal(l_data_ptr, r_data_ptr, type);
1012: 	}
1013: 	return order * result;
1014: }
1015: 
1016: //! Calls std::sort on strings that are tied by their prefix after the radix sort
1017: static void SortTiedBlobs(BufferManager &buffer_manager, const data_ptr_t dataptr, const idx_t &start, const idx_t &end,
1018:                           const idx_t &tie_col, bool *ties, const data_ptr_t blob_ptr,
1019:                           const SortingState &sorting_state) {
1020: 	const auto row_width = sorting_state.blob_layout.GetRowWidth();
1021: 	const idx_t &col_idx = sorting_state.sorting_to_blob_col.at(tie_col);
1022: 	// Locate the first blob row in question
1023: 	data_ptr_t row_ptr = dataptr + start * sorting_state.entry_size;
1024: 	data_ptr_t blob_row_ptr = blob_ptr + Load<idx_t>(row_ptr + sorting_state.comparison_size) * row_width;
1025: 	if (!TieIsBreakable(col_idx, blob_row_ptr, sorting_state.blob_layout)) {
1026: 		// Quick check to see if ties can be broken
1027: 		return;
1028: 	}
1029: 	// Fill pointer array for sorting
1030: 	auto ptr_block = buffer_manager.Allocate(MaxValue((end - start) * sizeof(data_ptr_t), (idx_t)Storage::BLOCK_SIZE));
1031: 	auto entry_ptrs = (data_ptr_t *)ptr_block->Ptr();
1032: 	for (idx_t i = start; i < end; i++) {
1033: 		entry_ptrs[i - start] = row_ptr;
1034: 		row_ptr += sorting_state.entry_size;
1035: 	}
1036: 	// Slow pointer-based sorting
1037: 	const int order = sorting_state.order_types[tie_col] == OrderType::DESCENDING ? -1 : 1;
1038: 	const auto &tie_col_offset = sorting_state.blob_layout.GetOffsets()[col_idx];
1039: 	auto logical_type = sorting_state.blob_layout.GetTypes()[col_idx];
1040: 	std::sort(entry_ptrs, entry_ptrs + end - start,
1041: 	          [&blob_ptr, &order, &sorting_state, &tie_col_offset, &row_width, &logical_type](const data_ptr_t l,
1042: 	                                                                                          const data_ptr_t r) {
1043: 		          idx_t left_idx = Load<idx_t>(l + sorting_state.comparison_size);
1044: 		          idx_t right_idx = Load<idx_t>(r + sorting_state.comparison_size);
1045: 		          data_ptr_t left_ptr = blob_ptr + left_idx * row_width + tie_col_offset;
1046: 		          data_ptr_t right_ptr = blob_ptr + right_idx * row_width + tie_col_offset;
1047: 		          return order * CompareVal(left_ptr, right_ptr, logical_type) < 0;
1048: 	          });
1049: 	// Re-order
1050: 	auto temp_block =
1051: 	    buffer_manager.Allocate(MaxValue((end - start) * sorting_state.entry_size, (idx_t)Storage::BLOCK_SIZE));
1052: 	data_ptr_t temp_ptr = temp_block->Ptr();
1053: 	for (idx_t i = 0; i < end - start; i++) {
1054: 		memcpy(temp_ptr, entry_ptrs[i], sorting_state.entry_size);
1055: 		temp_ptr += sorting_state.entry_size;
1056: 	}
1057: 	memcpy(dataptr + start * sorting_state.entry_size, temp_block->Ptr(), (end - start) * sorting_state.entry_size);
1058: 	// Determine if there are still ties (if this is not the last column)
1059: 	if (tie_col < sorting_state.column_count - 1) {
1060: 		data_ptr_t idx_ptr = dataptr + start * sorting_state.entry_size + sorting_state.comparison_size;
1061: 		// Load current entry
1062: 		data_ptr_t current_ptr = blob_ptr + Load<idx_t>(idx_ptr) * row_width + tie_col_offset;
1063: 		for (idx_t i = 0; i < end - start - 1; i++) {
1064: 			// Load next entry and compare
1065: 			idx_ptr += sorting_state.entry_size;
1066: 			data_ptr_t next_ptr = blob_ptr + Load<idx_t>(idx_ptr) * row_width + tie_col_offset;
1067: 			ties[start + i] = CompareVal(current_ptr, next_ptr, logical_type) == 0;
1068: 			current_ptr = next_ptr;
1069: 		}
1070: 	}
1071: }
1072: 
1073: //! Identifies sequences of rows that are tied by the prefix of a blob column, and sorts them
1074: static void SortTiedBlobs(BufferManager &buffer_manager, SortedBlock &sb, bool *ties, data_ptr_t dataptr,
1075:                           const idx_t &count, const idx_t &tie_col, const SortingState &sorting_state) {
1076: 	D_ASSERT(!ties[count - 1]);
1077: 	auto &blob_block = sb.blob_sorting_data->data_blocks.back();
1078: 	auto blob_handle = buffer_manager.Pin(blob_block.block);
1079: 	const data_ptr_t blob_ptr = blob_handle->Ptr();
1080: 
1081: 	for (idx_t i = 0; i < count; i++) {
1082: 		if (!ties[i]) {
1083: 			continue;
1084: 		}
1085: 		idx_t j;
1086: 		for (j = i; j < count; j++) {
1087: 			if (!ties[j]) {
1088: 				break;
1089: 			}
1090: 		}
1091: 		SortTiedBlobs(buffer_manager, dataptr, i, j + 1, tie_col, ties, blob_ptr, sorting_state);
1092: 		i = j;
1093: 	}
1094: }
1095: 
1096: //! Returns whether there are any 'true' values in the ties[] array
1097: static bool AnyTies(bool ties[], const idx_t &count) {
1098: 	D_ASSERT(!ties[count - 1]);
1099: 	bool any_ties = false;
1100: 	for (idx_t i = 0; i < count - 1; i++) {
1101: 		any_ties = any_ties || ties[i];
1102: 	}
1103: 	return any_ties;
1104: }
1105: 
1106: //! Compares subsequent rows to check for ties
1107: static void ComputeTies(data_ptr_t dataptr, const idx_t &count, const idx_t &col_offset, const idx_t &tie_size,
1108:                         bool ties[], const SortingState &sorting_state) {
1109: 	D_ASSERT(!ties[count - 1]);
1110: 	D_ASSERT(col_offset + tie_size <= sorting_state.comparison_size);
1111: 	// Align dataptr
1112: 	dataptr += col_offset;
1113: 	for (idx_t i = 0; i < count - 1; i++) {
1114: 		ties[i] = ties[i] && memcmp(dataptr, dataptr + sorting_state.entry_size, tie_size) == 0;
1115: 		dataptr += sorting_state.entry_size;
1116: 	}
1117: 	ties[count - 1] = false;
1118: }
1119: 
1120: //! Textbook LSD radix sort
1121: static void RadixSort(BufferManager &buffer_manager, data_ptr_t dataptr, const idx_t &count, const idx_t &col_offset,
1122:                       const idx_t &sorting_size, const SortingState &sorting_state) {
1123: 	auto temp_block = buffer_manager.Allocate(MaxValue(count * sorting_state.entry_size, (idx_t)Storage::BLOCK_SIZE));
1124: 	data_ptr_t temp = temp_block->Ptr();
1125: 	bool swap = false;
1126: 
1127: 	idx_t counts[256];
1128: 	uint8_t byte;
1129: 	for (idx_t offset = col_offset + sorting_size - 1; offset + 1 > col_offset; offset--) {
1130: 		// Init counts to 0
1131: 		memset(counts, 0, sizeof(counts));
1132: 		// Collect counts
1133: 		for (idx_t i = 0; i < count; i++) {
1134: 			byte = *(dataptr + i * sorting_state.entry_size + offset);
1135: 			counts[byte]++;
1136: 		}
1137: 		// Compute offsets from counts
1138: 		for (idx_t val = 1; val < 256; val++) {
1139: 			counts[val] = counts[val] + counts[val - 1];
1140: 		}
1141: 		// Re-order the data in temporary array
1142: 		for (idx_t i = count; i > 0; i--) {
1143: 			byte = *(dataptr + (i - 1) * sorting_state.entry_size + offset);
1144: 			memcpy(temp + (counts[byte] - 1) * sorting_state.entry_size, dataptr + (i - 1) * sorting_state.entry_size,
1145: 			       sorting_state.entry_size);
1146: 			counts[byte]--;
1147: 		}
1148: 		std::swap(dataptr, temp);
1149: 		swap = !swap;
1150: 	}
1151: 	// Move data back to original buffer (if it was swapped)
1152: 	if (swap) {
1153: 		memcpy(temp, dataptr, count * sorting_state.entry_size);
1154: 	}
1155: }
1156: 
1157: //! Identifies sequences of rows that are tied, and calls radix sort on these
1158: static void SubSortTiedTuples(BufferManager &buffer_manager, const data_ptr_t dataptr, const idx_t &count,
1159:                               const idx_t &col_offset, const idx_t &sorting_size, bool ties[],
1160:                               const SortingState &sorting_state) {
1161: 	D_ASSERT(!ties[count - 1]);
1162: 	for (idx_t i = 0; i < count; i++) {
1163: 		if (!ties[i]) {
1164: 			continue;
1165: 		}
1166: 		idx_t j;
1167: 		for (j = i + 1; j < count; j++) {
1168: 			if (!ties[j]) {
1169: 				break;
1170: 			}
1171: 		}
1172: 		RadixSort(buffer_manager, dataptr + i * sorting_state.entry_size, j - i + 1, col_offset, sorting_size,
1173: 		          sorting_state);
1174: 		i = j;
1175: 	}
1176: }
1177: 
1178: //! Sorts the data in a SortedBlock
1179: static void SortInMemory(BufferManager &buffer_manager, SortedBlock &sb, const SortingState &sorting_state) {
1180: 	auto &block = sb.radix_sorting_data.back();
1181: 	const auto &count = block.count;
1182: 	auto handle = buffer_manager.Pin(block.block);
1183: 	const auto dataptr = handle->Ptr();
1184: 	// Assign an index to each row
1185: 	data_ptr_t idx_dataptr = dataptr + sorting_state.comparison_size;
1186: 	for (idx_t i = 0; i < count; i++) {
1187: 		Store<idx_t>(i, idx_dataptr);
1188: 		idx_dataptr += sorting_state.entry_size;
1189: 	}
1190: 	// Radix sort and break ties until no more ties, or until all columns are sorted
1191: 	idx_t sorting_size = 0;
1192: 	idx_t col_offset = 0;
1193: 	unique_ptr<BufferHandle> ties_handle;
1194: 	bool *ties = nullptr;
1195: 	for (idx_t i = 0; i < sorting_state.column_count; i++) {
1196: 		sorting_size += sorting_state.column_sizes[i];
1197: 		if (sorting_state.constant_size[i] && i < sorting_state.column_count - 1 && sorting_size < 32) {
1198: 			// Add columns to the sorting size until we reach a variable size column, or the last column
1199: 			continue;
1200: 		}
1201: 
1202: 		if (!ties) {
1203: 			// This is the first sort
1204: 			RadixSort(buffer_manager, dataptr, count, col_offset, sorting_size, sorting_state);
1205: 			ties_handle = buffer_manager.Allocate(MaxValue(count, (idx_t)Storage::BLOCK_SIZE));
1206: 			ties = (bool *)ties_handle->Ptr();
1207: 			std::fill_n(ties, count - 1, true);
1208: 			ties[count - 1] = false;
1209: 		} else {
1210: 			// For subsequent sorts, we only have to subsort the tied tuples
1211: 			SubSortTiedTuples(buffer_manager, dataptr, count, col_offset, sorting_size, ties, sorting_state);
1212: 		}
1213: 
1214: 		if (sorting_state.constant_size[i] && i == sorting_state.column_count - 1) {
1215: 			// All columns are sorted, no ties to break because last column is constant size
1216: 			break;
1217: 		}
1218: 
1219: 		ComputeTies(dataptr, count, col_offset, sorting_size, ties, sorting_state);
1220: 		if (!AnyTies(ties, count)) {
1221: 			// No ties, stop sorting
1222: 			break;
1223: 		}
1224: 
1225: 		if (!sorting_state.constant_size[i]) {
1226: 			SortTiedBlobs(buffer_manager, sb, ties, dataptr, count, i, sorting_state);
1227: 			if (!AnyTies(ties, count)) {
1228: 				// No more ties after tie-breaking, stop
1229: 				break;
1230: 			}
1231: 		}
1232: 
1233: 		col_offset += sorting_size;
1234: 		sorting_size = 0;
1235: 	}
1236: }
1237: 
1238: //! Reorders SortedData according to the sorted key columns
1239: static void ReOrder(BufferManager &buffer_manager, SortedData &sd, data_ptr_t sorting_ptr, RowDataCollection &heap,
1240:                     OrderGlobalState &gstate) {
1241: 	auto &unordered_data_block = sd.data_blocks.back();
1242: 	const idx_t &count = unordered_data_block.count;
1243: 	auto unordered_data_handle = buffer_manager.Pin(unordered_data_block.block);
1244: 	const data_ptr_t unordered_data_ptr = unordered_data_handle->Ptr();
1245: 	// Create new block that will hold re-ordered row data
1246: 	RowDataBlock ordered_data_block(buffer_manager, unordered_data_block.capacity, unordered_data_block.entry_size);
1247: 	ordered_data_block.count = count;
1248: 	auto ordered_data_handle = buffer_manager.Pin(ordered_data_block.block);
1249: 	data_ptr_t ordered_data_ptr = ordered_data_handle->Ptr();
1250: 	// Re-order fixed-size row layout
1251: 	const idx_t row_width = sd.layout.GetRowWidth();
1252: 	const idx_t sorting_entry_size = gstate.sorting_state.entry_size;
1253: 	for (idx_t i = 0; i < count; i++) {
1254: 		idx_t index = Load<idx_t>(sorting_ptr);
1255: 		memcpy(ordered_data_ptr, unordered_data_ptr + index * row_width, row_width);
1256: 		ordered_data_ptr += row_width;
1257: 		sorting_ptr += sorting_entry_size;
1258: 	}
1259: 	// Replace the unordered data block with the re-ordered data block
1260: 	buffer_manager.UnregisterBlock(unordered_data_block.block->BlockId(), true);
1261: 	sd.data_blocks.clear();
1262: 	sd.data_blocks.push_back(move(ordered_data_block));
1263: 	// Deal with the heap (if necessary)
1264: 	if (!sd.layout.AllConstant()) {
1265: 		// Swizzle the column pointers to offsets
1266: 		RowOperations::SwizzleColumns(sd.layout, ordered_data_handle->Ptr(), count);
1267: 		// Create a single heap block to store the ordered heap
1268: 		idx_t total_byte_offset = std::accumulate(heap.blocks.begin(), heap.blocks.end(), 0,
1269: 		                                          [](idx_t a, const RowDataBlock &b) { return a + b.byte_offset; });
1270: 		idx_t heap_block_size = MaxValue(total_byte_offset, (idx_t)Storage::BLOCK_SIZE);
1271: 		RowDataBlock ordered_heap_block(buffer_manager, heap_block_size, 1);
1272: 		ordered_heap_block.count = count;
1273: 		ordered_heap_block.byte_offset = total_byte_offset;
1274: 		auto ordered_heap_handle = buffer_manager.Pin(ordered_heap_block.block);
1275: 		data_ptr_t ordered_heap_ptr = ordered_heap_handle->Ptr();
1276: 		// Fill the heap in order
1277: 		ordered_data_ptr = ordered_data_handle->Ptr();
1278: 		const idx_t heap_pointer_offset = sd.layout.GetHeapPointerOffset();
1279: 		for (idx_t i = 0; i < count; i++) {
1280: 			auto heap_row_ptr = Load<data_ptr_t>(ordered_data_ptr + heap_pointer_offset);
1281: 			auto heap_row_size = Load<idx_t>(heap_row_ptr);
1282: 			memcpy(ordered_heap_ptr, heap_row_ptr, heap_row_size);
1283: 			ordered_heap_ptr += heap_row_size;
1284: 			ordered_data_ptr += row_width;
1285: 		}
1286: 		// Swizzle the base pointer to the offset of each row in the heap
1287: 		RowOperations::SwizzleHeapPointer(sd.layout, ordered_data_handle->Ptr(), ordered_heap_handle->Ptr(), count);
1288: 		// Move the re-ordered heap to the SortedData, and clear the local heap
1289: 		sd.heap_blocks.push_back(move(ordered_heap_block));
1290: 		for (auto &block : heap.blocks) {
1291: 			buffer_manager.UnregisterBlock(block.block->BlockId(), true);
1292: 		}
1293: 	}
1294: 	// Reset the localstate heap
1295: 	heap.pinned_blocks.clear();
1296: 	heap.blocks.clear();
1297: 	heap.count = 0;
1298: }
1299: 
1300: //! Use the ordered sorting data to re-order the rest of the data
1301: static void ReOrder(ClientContext &context, SortedBlock &sb, OrderLocalState &lstate, OrderGlobalState &gstate) {
1302: 	auto &buffer_manager = BufferManager::GetBufferManager(context);
1303: 	auto sorting_handle = buffer_manager.Pin(sb.radix_sorting_data.back().block);
1304: 	const data_ptr_t sorting_ptr = sorting_handle->Ptr() + gstate.sorting_state.comparison_size;
1305: 	// Re-order variable size sorting columns
1306: 	if (!gstate.sorting_state.all_constant) {
1307: 		ReOrder(buffer_manager, *sb.blob_sorting_data, sorting_ptr, *lstate.blob_sorting_heap, gstate);
1308: 	}
1309: 	// And the payload
1310: 	ReOrder(buffer_manager, *sb.payload_data, sorting_ptr, *lstate.payload_heap, gstate);
1311: }
1312: 
1313: //! Appends and sorts the data accumulated in a local sink state
1314: void PhysicalOrder::SortLocalState(ClientContext &context, OrderLocalState &lstate, OrderGlobalState &gstate) const {
1315: 	D_ASSERT(lstate.radix_sorting_data->count == lstate.payload_data->count);
1316: 	if (lstate.radix_sorting_data->count == 0) {
1317: 		return;
1318: 	}
1319: 	auto &buffer_manager = BufferManager::GetBufferManager(context);
1320: 	const auto &sorting_state = gstate.sorting_state;
1321: 	// Move all data to a single SortedBlock
1322: 	auto sb = make_unique<SortedBlock>(buffer_manager, gstate);
1323: 	// Fixed-size sorting data
1324: 	auto sorting_block = ConcatenateBlocks(buffer_manager, *lstate.radix_sorting_data);
1325: 	sb->radix_sorting_data.push_back(move(sorting_block));
1326: 	// Variable-size sorting data
1327: 	if (!sorting_state.blob_layout.AllConstant()) {
1328: 		auto &blob_data = *lstate.blob_sorting_data;
1329: 		auto new_block = ConcatenateBlocks(buffer_manager, blob_data);
1330: 		sb->blob_sorting_data->data_blocks.push_back(move(new_block));
1331: 	}
1332: 	// Payload data
1333: 	auto payload_block = ConcatenateBlocks(buffer_manager, *lstate.payload_data);
1334: 	sb->payload_data->data_blocks.push_back(move(payload_block));
1335: 	// Now perform the actual sort
1336: 	SortInMemory(buffer_manager, *sb, sorting_state);
1337: 	// Re-order before the merge sort
1338: 	ReOrder(context, *sb, lstate, gstate);
1339: 	// Add the sorted block to the local state
1340: 	lstate.sorted_blocks.push_back(move(sb));
1341: }
1342: 
1343: //! Compares the tuples that a being read from in the 'left' and 'right blocks during merge sort
1344: //! (only in case we cannot simply 'memcmp' - if there are blob columns)
1345: inline int CompareTuple(const SortedBlock &left, const SortedBlock &right, const data_ptr_t &l_ptr,
1346:                         const data_ptr_t &r_ptr, const SortingState &sorting_state, const bool &external_sort) {
1347: 	// Compare the sorting columns one by one
1348: 	int comp_res = 0;
1349: 	data_ptr_t l_ptr_offset = l_ptr;
1350: 	data_ptr_t r_ptr_offset = r_ptr;
1351: 	for (idx_t col_idx = 0; col_idx < sorting_state.column_count; col_idx++) {
1352: 		comp_res = memcmp(l_ptr_offset, r_ptr_offset, sorting_state.column_sizes[col_idx]);
1353: 		if (comp_res == 0 && !sorting_state.constant_size[col_idx]) {
1354: 			comp_res =
1355: 			    BreakBlobTie(col_idx, *left.blob_sorting_data, *right.blob_sorting_data, sorting_state, external_sort);
1356: 		}
1357: 		if (comp_res != 0) {
1358: 			break;
1359: 		}
1360: 		l_ptr_offset += sorting_state.column_sizes[col_idx];
1361: 		r_ptr_offset += sorting_state.column_sizes[col_idx];
1362: 	}
1363: 	return comp_res;
1364: }
1365: 
1366: class PhysicalOrderMergeTask : public Task {
1367: public:
1368: 	PhysicalOrderMergeTask(Pipeline &parent_p, ClientContext &context_p, OrderGlobalState &state_p)
1369: 	    : parent(parent_p), context(context_p), buffer_manager(BufferManager::GetBufferManager(context_p)),
1370: 	      state(state_p), sorting_state(state_p.sorting_state) {
1371: 	}
1372: 
1373: 	void Execute() override {
1374: 		ComputeWork();
1375: 		auto &left = *left_block;
1376: 		auto &right = *right_block;
1377: 		D_ASSERT(left.radix_sorting_data.size() == left.payload_data->data_blocks.size());
1378: 		D_ASSERT(right.radix_sorting_data.size() == right.payload_data->data_blocks.size());
1379: 		if (!state.payload_layout.AllConstant() && state.external) {
1380: 			D_ASSERT(left.payload_data->data_blocks.size() == left.payload_data->heap_blocks.size());
1381: 			D_ASSERT(right.payload_data->data_blocks.size() == right.payload_data->heap_blocks.size());
1382: 		}
1383: 		if (!sorting_state.all_constant) {
1384: 			D_ASSERT(left.radix_sorting_data.size() == left.blob_sorting_data->data_blocks.size());
1385: 			D_ASSERT(right.radix_sorting_data.size() == right.blob_sorting_data->data_blocks.size());
1386: 			if (state.external) {
1387: 				D_ASSERT(left.blob_sorting_data->data_blocks.size() == left.blob_sorting_data->heap_blocks.size());
1388: 				D_ASSERT(right.blob_sorting_data->data_blocks.size() == right.blob_sorting_data->heap_blocks.size());
1389: 			}
1390: 		}
1391: 		// Set up the write block
1392: 		result->InitializeWrite();
1393: 		// Initialize arrays to store merge data
1394: 		bool left_smaller[STANDARD_VECTOR_SIZE];
1395: 		idx_t next_entry_sizes[STANDARD_VECTOR_SIZE];
1396: 		// Merge loop
1397: #ifdef DEBUG
1398: 		auto l_count = left.Remaining();
1399: 		auto r_count = right.Remaining();
1400: #endif
1401: 		while (true) {
1402: 			auto l_remaining = left.Remaining();
1403: 			auto r_remaining = right.Remaining();
1404: 			if (l_remaining + r_remaining == 0) {
1405: 				// Done
1406: 				break;
1407: 			}
1408: 			const idx_t next = MinValue(l_remaining + r_remaining, (idx_t)STANDARD_VECTOR_SIZE);
1409: 			if (l_remaining != 0 && r_remaining != 0) {
1410: 				// Compute the merge (not needed if one side is exhausted)
1411: 				ComputeMerge(next, left_smaller);
1412: 			}
1413: 			// Actually merge the data (radix, blob, and payload)
1414: 			Merge(next, left_smaller);
1415: 			if (!sorting_state.all_constant) {
1416: 				Merge(*result->blob_sorting_data, *left.blob_sorting_data, *right.blob_sorting_data, next, left_smaller,
1417: 				      next_entry_sizes);
1418: 				D_ASSERT(left.block_idx == left.blob_sorting_data->block_idx &&
1419: 				         left.entry_idx == left.blob_sorting_data->entry_idx);
1420: 				D_ASSERT(right.block_idx == right.blob_sorting_data->block_idx &&
1421: 				         right.entry_idx == right.blob_sorting_data->entry_idx);
1422: 				D_ASSERT(result->radix_sorting_data.size() == result->blob_sorting_data->data_blocks.size());
1423: 			}
1424: 			Merge(*result->payload_data, *left.payload_data, *right.payload_data, next, left_smaller, next_entry_sizes);
1425: 			D_ASSERT(left.block_idx == left.payload_data->block_idx && left.entry_idx == left.payload_data->entry_idx);
1426: 			D_ASSERT(right.block_idx == right.payload_data->block_idx &&
1427: 			         right.entry_idx == right.payload_data->entry_idx);
1428: 			D_ASSERT(result->radix_sorting_data.size() == result->payload_data->data_blocks.size());
1429: 		}
1430: #ifdef DEBUG
1431: 		D_ASSERT(result->Count() == l_count + r_count);
1432: #endif
1433: 		lock_guard<mutex> glock(state.lock);
1434: 		parent.finished_tasks++;
1435: 		if (parent.finished_tasks == parent.total_tasks) {
1436: 			// Unregister processed data
1437: 			for (auto &sb : state.sorted_blocks) {
1438: 				sb->UnregisterSortingBlocks();
1439: 				sb->UnregisterPayloadBlocks();
1440: 			}
1441: 			state.sorted_blocks.clear();
1442: 			if (state.odd_one_out) {
1443: 				state.sorted_blocks.push_back(move(state.odd_one_out));
1444: 				state.odd_one_out = nullptr;
1445: 			}
1446: 			for (auto &sorted_block_vector : state.sorted_blocks_temp) {
1447: 				state.sorted_blocks.push_back(make_unique<SortedBlock>(buffer_manager, state));
1448: 				state.sorted_blocks.back()->AppendSortedBlocks(sorted_block_vector);
1449: 			}
1450: 			state.sorted_blocks_temp.clear();
1451: 			PhysicalOrder::ScheduleMergeTasks(parent, context, state);
1452: 		}
1453: 	}
1454: 
1455: 	//! Sets the left and right block that this task will merge
1456: 	void ComputeWork() {
1457: 		// Acquire global lock to compute next intersection
1458: 		lock_guard<mutex> glock(state.lock);
1459: 		// Create result block
1460: 		state.sorted_blocks_temp[state.pair_idx].push_back(make_unique<SortedBlock>(buffer_manager, state));
1461: 		result = state.sorted_blocks_temp[state.pair_idx].back().get();
1462: 		// Determine which blocks must be merged
1463: 		auto &left = *state.sorted_blocks[state.pair_idx * 2];
1464: 		auto &right = *state.sorted_blocks[state.pair_idx * 2 + 1];
1465: 		const idx_t l_count = left.Count();
1466: 		const idx_t r_count = right.Count();
1467: 		// Compute the work that this thread must do using Merge Path
1468: 		idx_t l_end;
1469: 		idx_t r_end;
1470: 		if (state.l_start + state.r_start + state.block_capacity < l_count + r_count) {
1471: 			const idx_t intersection = state.l_start + state.r_start + state.block_capacity;
1472: 			ComputeIntersection(left, right, intersection, l_end, r_end);
1473: 			D_ASSERT(l_end <= l_count);
1474: 			D_ASSERT(r_end <= r_count);
1475: 			D_ASSERT(intersection == l_end + r_end);
1476: 			// Unpin after finding the intersection
1477: 			if (!sorting_state.blob_layout.AllConstant()) {
1478: 				left.blob_sorting_data->UnpinAndReset(0, 0);
1479: 				right.blob_sorting_data->UnpinAndReset(0, 0);
1480: 			}
1481: 		} else {
1482: 			l_end = l_count;
1483: 			r_end = r_count;
1484: 		}
1485: 		// Create slices of the data that this thread must merge
1486: 		left_block = left.CreateSlice(state.l_start, l_end);
1487: 		right_block = right.CreateSlice(state.r_start, r_end);
1488: 		// Update global state
1489: 		state.l_start = l_end;
1490: 		state.r_start = r_end;
1491: 		if (state.l_start == l_count && state.r_start == r_count) {
1492: 			state.pair_idx++;
1493: 			state.l_start = 0;
1494: 			state.r_start = 0;
1495: 		}
1496: 	}
1497: 
1498: 	//! Compare values within SortedBlocks using a global index
1499: 	int CompareUsingGlobalIndex(SortedBlock &l, SortedBlock &r, const idx_t l_idx, const idx_t r_idx) {
1500: 		D_ASSERT(l_idx < l.Count());
1501: 		D_ASSERT(r_idx < r.Count());
1502: 
1503: 		idx_t l_block_idx;
1504: 		idx_t l_entry_idx;
1505: 		l.GlobalToLocalIndex(l_idx, l_block_idx, l_entry_idx);
1506: 
1507: 		idx_t r_block_idx;
1508: 		idx_t r_entry_idx;
1509: 		r.GlobalToLocalIndex(r_idx, r_block_idx, r_entry_idx);
1510: 
1511: 		auto l_block_handle = buffer_manager.Pin(l.radix_sorting_data[l_block_idx].block);
1512: 		auto r_block_handle = buffer_manager.Pin(r.radix_sorting_data[r_block_idx].block);
1513: 		data_ptr_t l_ptr = l_block_handle->Ptr() + l_entry_idx * sorting_state.entry_size;
1514: 		data_ptr_t r_ptr = r_block_handle->Ptr() + r_entry_idx * sorting_state.entry_size;
1515: 
1516: 		int comp_res;
1517: 		if (sorting_state.all_constant) {
1518: 			comp_res = memcmp(l_ptr, r_ptr, sorting_state.comparison_size);
1519: 		} else {
1520: 			l.blob_sorting_data->block_idx = l_block_idx;
1521: 			l.blob_sorting_data->entry_idx = l_entry_idx;
1522: 			l.blob_sorting_data->Pin();
1523: 			r.blob_sorting_data->block_idx = r_block_idx;
1524: 			r.blob_sorting_data->entry_idx = r_entry_idx;
1525: 			r.blob_sorting_data->Pin();
1526: 			comp_res = CompareTuple(l, r, l_ptr, r_ptr, sorting_state, state.external);
1527: 		}
1528: 		return comp_res;
1529: 	}
1530: 
1531: 	//! Merge path
1532: 	void ComputeIntersection(SortedBlock &l, SortedBlock &r, const idx_t sum, idx_t &l_idx, idx_t &r_idx) {
1533: 		const idx_t l_count = l.Count();
1534: 		const idx_t r_count = r.Count();
1535: 		// Cover some edge cases
1536: 		if (sum >= l_count + r_count) {
1537: 			l_idx = l_count;
1538: 			r_idx = r_count;
1539: 			return;
1540: 		} else if (sum == 0) {
1541: 			l_idx = 0;
1542: 			r_idx = 0;
1543: 			return;
1544: 		} else if (l_count == 0) {
1545: 			l_idx = 0;
1546: 			r_idx = sum;
1547: 			return;
1548: 		} else if (r_count == 0) {
1549: 			r_idx = 0;
1550: 			l_idx = sum;
1551: 			return;
1552: 		}
1553: 		// Determine offsets for the binary search
1554: 		const idx_t l_offset = MinValue(l_count, sum);
1555: 		const idx_t r_offset = sum > l_count ? sum - l_count : 0;
1556: 		D_ASSERT(l_offset + r_offset == sum);
1557: 		const idx_t search_space =
1558: 		    sum > MaxValue(l_count, r_count) ? l_count + r_count - sum : MinValue(sum, MinValue(l_count, r_count));
1559: 		// Double binary search
1560: 		idx_t left = 0;
1561: 		idx_t right = search_space - 1;
1562: 		idx_t middle;
1563: 		int comp_res;
1564: 		while (left <= right) {
1565: 			middle = (left + right) / 2;
1566: 			l_idx = l_offset - middle;
1567: 			r_idx = r_offset + middle;
1568: 			if (l_idx == l_count || r_idx == 0) {
1569: 				comp_res = CompareUsingGlobalIndex(l, r, l_idx - 1, r_idx);
1570: 				if (comp_res > 0) {
1571: 					l_idx--;
1572: 					r_idx++;
1573: 				} else {
1574: 					return;
1575: 				}
1576: 				if (l_idx == 0 || r_idx == r_count) {
1577: 					return;
1578: 				} else {
1579: 					break;
1580: 				}
1581: 			}
1582: 			comp_res = CompareUsingGlobalIndex(l, r, l_idx, r_idx);
1583: 			if (comp_res > 0) {
1584: 				left = middle + 1;
1585: 			} else {
1586: 				right = middle - 1;
1587: 			}
1588: 		}
1589: 		// Shift by one (if needed)
1590: 		if (l_idx == 0) {
1591: 			comp_res = CompareUsingGlobalIndex(l, r, l_idx, r_idx);
1592: 			if (comp_res > 0) {
1593: 				l_idx--;
1594: 				r_idx++;
1595: 			}
1596: 			return;
1597: 		}
1598: 		int l_r_min1 = CompareUsingGlobalIndex(l, r, l_idx, r_idx - 1);
1599: 		int l_min1_r = CompareUsingGlobalIndex(l, r, l_idx - 1, r_idx);
1600: 		if (l_r_min1 > 0 && l_min1_r < 0) {
1601: 			return;
1602: 		} else if (l_r_min1 > 0) {
1603: 			l_idx--;
1604: 			r_idx++;
1605: 		} else if (l_min1_r < 0) {
1606: 			l_idx++;
1607: 			r_idx--;
1608: 		}
1609: 	}
1610: 
1611: 	//! Computes how the next 'count' tuples should be merged by setting the 'left_smaller' array
1612: 	void ComputeMerge(const idx_t &count, bool *left_smaller) {
1613: 		auto &left = *left_block;
1614: 		auto &right = *right_block;
1615: 		// Store indices to restore after computing the merge
1616: 		idx_t l_block_idx = left.block_idx;
1617: 		idx_t r_block_idx = right.block_idx;
1618: 		idx_t l_entry_idx = left.entry_idx;
1619: 		idx_t r_entry_idx = right.entry_idx;
1620: 		// Data handles and pointers for both sides
1621: 		unique_ptr<BufferHandle> l_radix_handle;
1622: 		unique_ptr<BufferHandle> r_radix_handle;
1623: 		data_ptr_t l_radix_ptr;
1624: 		data_ptr_t r_radix_ptr;
1625: 		// Compute the merge of the next 'count' tuples
1626: 		idx_t compared = 0;
1627: 		while (compared < count) {
1628: 			// Move to the next block (if needed)
1629: 			if (l_block_idx < left.radix_sorting_data.size() &&
1630: 			    l_entry_idx == left.radix_sorting_data[l_block_idx].count) {
1631: 				l_block_idx++;
1632: 				l_entry_idx = 0;
1633: 				if (!sorting_state.all_constant) {
1634: 					left.blob_sorting_data->block_idx = l_block_idx;
1635: 					left.blob_sorting_data->entry_idx = l_entry_idx;
1636: 				}
1637: 			}
1638: 			if (r_block_idx < right.radix_sorting_data.size() &&
1639: 			    r_entry_idx == right.radix_sorting_data[r_block_idx].count) {
1640: 				r_block_idx++;
1641: 				r_entry_idx = 0;
1642: 				if (!sorting_state.all_constant) {
1643: 					right.blob_sorting_data->block_idx = r_block_idx;
1644: 					right.blob_sorting_data->entry_idx = r_entry_idx;
1645: 				}
1646: 			}
1647: 			const bool l_done = l_block_idx == left.radix_sorting_data.size();
1648: 			const bool r_done = r_block_idx == right.radix_sorting_data.size();
1649: 			if (l_done || r_done) {
1650: 				// One of the sides is exhausted, no need to compare
1651: 				break;
1652: 			}
1653: 			// Pin the radix sorting data
1654: 			if (!l_done) {
1655: 				l_radix_handle = buffer_manager.Pin(left.radix_sorting_data[l_block_idx].block);
1656: 				l_radix_ptr = l_radix_handle->Ptr() + l_entry_idx * sorting_state.entry_size;
1657: 			}
1658: 			if (!r_done) {
1659: 				r_radix_handle = buffer_manager.Pin(right.radix_sorting_data[r_block_idx].block);
1660: 				r_radix_ptr = r_radix_handle->Ptr() + r_entry_idx * sorting_state.entry_size;
1661: 			}
1662: 			const idx_t &l_count = !l_done ? left.radix_sorting_data[l_block_idx].count : 0;
1663: 			const idx_t &r_count = !r_done ? right.radix_sorting_data[r_block_idx].count : 0;
1664: 			// Compute the merge
1665: 			if (sorting_state.all_constant) {
1666: 				// All sorting columns are constant size
1667: 				for (; compared < count && l_entry_idx < l_count && r_entry_idx < r_count; compared++) {
1668: 					left_smaller[compared] = memcmp(l_radix_ptr, r_radix_ptr, sorting_state.comparison_size) < 0;
1669: 					const bool &l_smaller = left_smaller[compared];
1670: 					const bool r_smaller = !l_smaller;
1671: 					// Use comparison bool (0 or 1) to increment entries and pointers
1672: 					l_entry_idx += l_smaller;
1673: 					r_entry_idx += r_smaller;
1674: 					l_radix_ptr += l_smaller * sorting_state.entry_size;
1675: 					r_radix_ptr += r_smaller * sorting_state.entry_size;
1676: 				}
1677: 			} else {
1678: 				// Pin the blob data
1679: 				if (!l_done) {
1680: 					left.blob_sorting_data->Pin();
1681: 				}
1682: 				if (!r_done) {
1683: 					right.blob_sorting_data->Pin();
1684: 				}
1685: 				// Merge with variable size sorting columns
1686: 				for (; compared < count && l_entry_idx < l_count && r_entry_idx < r_count; compared++) {
1687: 					D_ASSERT(l_block_idx == left.blob_sorting_data->block_idx &&
1688: 					         l_entry_idx == left.blob_sorting_data->entry_idx);
1689: 					D_ASSERT(r_block_idx == right.blob_sorting_data->block_idx &&
1690: 					         r_entry_idx == right.blob_sorting_data->entry_idx);
1691: 					left_smaller[compared] =
1692: 					    CompareTuple(left, right, l_radix_ptr, r_radix_ptr, sorting_state, state.external) < 0;
1693: 					const bool &l_smaller = left_smaller[compared];
1694: 					const bool r_smaller = !l_smaller;
1695: 					// Use comparison bool (0 or 1) to increment entries and pointers
1696: 					l_entry_idx += l_smaller;
1697: 					r_entry_idx += r_smaller;
1698: 					l_radix_ptr += l_smaller * sorting_state.entry_size;
1699: 					r_radix_ptr += r_smaller * sorting_state.entry_size;
1700: 					left.blob_sorting_data->Advance(l_smaller);
1701: 					right.blob_sorting_data->Advance(r_smaller);
1702: 				}
1703: 			}
1704: 		}
1705: 		// Reset block indices before the actual merge
1706: 		if (!sorting_state.all_constant) {
1707: 			left.blob_sorting_data->UnpinAndReset(left.block_idx, left.entry_idx);
1708: 			right.blob_sorting_data->UnpinAndReset(right.block_idx, right.entry_idx);
1709: 		}
1710: 	}
1711: 
1712: 	//! Merges the radix sorting blocks according to the 'left_smaller' array
1713: 	void Merge(const idx_t &count, const bool left_smaller[]) {
1714: 		auto &left = *left_block;
1715: 		auto &right = *right_block;
1716: 		RowDataBlock *l_block;
1717: 		RowDataBlock *r_block;
1718: 
1719: 		unique_ptr<BufferHandle> l_block_handle;
1720: 		unique_ptr<BufferHandle> r_block_handle;
1721: 		data_ptr_t l_ptr;
1722: 		data_ptr_t r_ptr;
1723: 
1724: 		RowDataBlock *result_block = &result->radix_sorting_data.back();
1725: 		auto result_handle = buffer_manager.Pin(result_block->block);
1726: 		data_ptr_t result_ptr = result_handle->Ptr() + result_block->count * sorting_state.entry_size;
1727: 
1728: 		idx_t copied = 0;
1729: 		while (copied < count) {
1730: 			// Move to the next block (if needed)
1731: 			if (left.block_idx < left.radix_sorting_data.size() &&
1732: 			    left.entry_idx == left.radix_sorting_data[left.block_idx].count) {
1733: 				left.block_idx++;
1734: 				left.entry_idx = 0;
1735: 			}
1736: 			if (right.block_idx < right.radix_sorting_data.size() &&
1737: 			    right.entry_idx == right.radix_sorting_data[right.block_idx].count) {
1738: 				right.block_idx++;
1739: 				right.entry_idx = 0;
1740: 			}
1741: 			const bool l_done = left.block_idx == left.radix_sorting_data.size();
1742: 			const bool r_done = right.block_idx == right.radix_sorting_data.size();
1743: 			// Pin the radix sortable blocks
1744: 			if (!l_done) {
1745: 				l_block = &left.radix_sorting_data[left.block_idx];
1746: 				l_block_handle = buffer_manager.Pin(l_block->block);
1747: 				l_ptr = l_block_handle->Ptr() + left.entry_idx * sorting_state.entry_size;
1748: 			}
1749: 			if (!r_done) {
1750: 				r_block = &right.radix_sorting_data[right.block_idx];
1751: 				r_block_handle = buffer_manager.Pin(r_block->block);
1752: 				r_ptr = r_block_handle->Ptr() + right.entry_idx * sorting_state.entry_size;
1753: 			}
1754: 			const idx_t &l_count = !l_done ? l_block->count : 0;
1755: 			const idx_t &r_count = !r_done ? r_block->count : 0;
1756: 			// Create new result block (if needed)
1757: 			if (result_block->count == result_block->capacity) {
1758: 				result->CreateBlock();
1759: 				result_block = &result->radix_sorting_data.back();
1760: 				result_handle = buffer_manager.Pin(result_block->block);
1761: 				result_ptr = result_handle->Ptr();
1762: 			}
1763: 			// Copy using computed merge
1764: 			if (!l_done && !r_done) {
1765: 				// Both sides have data - merge
1766: 				MergeRows(l_ptr, left.entry_idx, l_count, r_ptr, right.entry_idx, r_count, result_block, result_ptr,
1767: 				          sorting_state.entry_size, left_smaller, copied, count);
1768: 			} else if (r_done) {
1769: 				// Right side is exhausted
1770: 				FlushRows(l_ptr, left.entry_idx, l_count, result_block, result_ptr, sorting_state.entry_size, copied,
1771: 				          count);
1772: 			} else {
1773: 				// Left side is exhausted
1774: 				FlushRows(r_ptr, right.entry_idx, r_count, result_block, result_ptr, sorting_state.entry_size, copied,
1775: 				          count);
1776: 			}
1777: 		}
1778: 	}
1779: 
1780: 	//! Merges SortedData according to the 'left_smaller' array
1781: 	void Merge(SortedData &result_data, SortedData &l_data, SortedData &r_data, const idx_t &count,
1782: 	           const bool left_smaller[], idx_t next_entry_sizes[]) {
1783: 		const auto &layout = result_data.layout;
1784: 		const idx_t row_width = layout.GetRowWidth();
1785: 		const idx_t heap_pointer_offset = layout.GetHeapPointerOffset();
1786: 
1787: 		// Left and right row data to merge
1788: 		unique_ptr<BufferHandle> l_data_block_handle;
1789: 		unique_ptr<BufferHandle> r_data_block_handle;
1790: 		data_ptr_t l_ptr;
1791: 		data_ptr_t r_ptr;
1792: 		// Accompanying left and right heap data (if needed)
1793: 		unique_ptr<BufferHandle> l_heap_handle;
1794: 		unique_ptr<BufferHandle> r_heap_handle;
1795: 		data_ptr_t l_heap_ptr;
1796: 		data_ptr_t r_heap_ptr;
1797: 
1798: 		// Result rows to write to
1799: 		RowDataBlock *result_data_block = &result_data.data_blocks.back();
1800: 		auto result_data_handle = buffer_manager.Pin(result_data_block->block);
1801: 		data_ptr_t result_data_ptr = result_data_handle->Ptr() + result_data_block->count * row_width;
1802: 		// Result heap to write to (if needed)
1803: 		RowDataBlock *result_heap_block;
1804: 		unique_ptr<BufferHandle> result_heap_handle;
1805: 		data_ptr_t result_heap_ptr;
1806: 		if (!layout.AllConstant() && state.external) {
1807: 			result_heap_block = &result_data.heap_blocks.back();
1808: 			result_heap_handle = buffer_manager.Pin(result_heap_block->block);
1809: 			result_heap_ptr = result_heap_handle->Ptr() + result_heap_block->byte_offset;
1810: 		}
1811: 
1812: 		idx_t copied = 0;
1813: 		while (copied < count) {
1814: 			// Move to new data blocks (if needed)
1815: 			if (l_data.block_idx < l_data.data_blocks.size() &&
1816: 			    l_data.entry_idx == l_data.data_blocks[l_data.block_idx].count) {
1817: 				l_data.block_idx++;
1818: 				l_data.entry_idx = 0;
1819: 			}
1820: 			if (r_data.block_idx < r_data.data_blocks.size() &&
1821: 			    r_data.entry_idx == r_data.data_blocks[r_data.block_idx].count) {
1822: 				r_data.block_idx++;
1823: 				r_data.entry_idx = 0;
1824: 			}
1825: 			const bool l_done = l_data.block_idx == l_data.data_blocks.size();
1826: 			const bool r_done = r_data.block_idx == r_data.data_blocks.size();
1827: 			// Pin the row data blocks
1828: 			if (!l_done) {
1829: 				l_data_block_handle = buffer_manager.Pin(l_data.data_blocks[l_data.block_idx].block);
1830: 				l_ptr = l_data_block_handle->Ptr() + l_data.entry_idx * row_width;
1831: 			}
1832: 			if (!r_done) {
1833: 				r_data_block_handle = buffer_manager.Pin(r_data.data_blocks[r_data.block_idx].block);
1834: 				r_ptr = r_data_block_handle->Ptr() + r_data.entry_idx * row_width;
1835: 			}
1836: 			const idx_t &l_count = !l_done ? l_data.data_blocks[l_data.block_idx].count : 0;
1837: 			const idx_t &r_count = !r_done ? r_data.data_blocks[r_data.block_idx].count : 0;
1838: 			// Create new result data block (if needed)
1839: 			if (result_data_block->count == result_data_block->capacity) {
1840: 				// Shrink down the last heap block to fit the data
1841: 				if (!layout.AllConstant() && state.external &&
1842: 				    result_heap_block->byte_offset < result_heap_block->capacity &&
1843: 				    result_heap_block->byte_offset >= Storage::BLOCK_SIZE) {
1844: 					buffer_manager.ReAllocate(result_heap_block->block, result_heap_block->byte_offset);
1845: 					result_heap_block->capacity = result_heap_block->byte_offset;
1846: 				}
1847: 				result_data.CreateBlock();
1848: 				result_data_block = &result_data.data_blocks.back();
1849: 				result_data_handle = buffer_manager.Pin(result_data_block->block);
1850: 				result_data_ptr = result_data_handle->Ptr();
1851: 				if (!layout.AllConstant() && state.external) {
1852: 					result_heap_block = &result_data.heap_blocks.back();
1853: 					result_heap_handle = buffer_manager.Pin(result_heap_block->block);
1854: 					result_heap_ptr = result_heap_handle->Ptr();
1855: 				}
1856: 			}
1857: 			// Perform the merge
1858: 			if (layout.AllConstant() || !state.external) {
1859: 				// If all constant size, or if we are doing an in-memory sort, we do not need to touch the heap
1860: 				if (!l_done && !r_done) {
1861: 					// Both sides have data - merge
1862: 					MergeRows(l_ptr, l_data.entry_idx, l_count, r_ptr, r_data.entry_idx, r_count, result_data_block,
1863: 					          result_data_ptr, row_width, left_smaller, copied, count);
1864: 				} else if (r_done) {
1865: 					// Right side is exhausted
1866: 					FlushRows(l_ptr, l_data.entry_idx, l_count, result_data_block, result_data_ptr, row_width, copied,
1867: 					          count);
1868: 				} else {
1869: 					// Left side is exhausted
1870: 					FlushRows(r_ptr, r_data.entry_idx, r_count, result_data_block, result_data_ptr, row_width, copied,
1871: 					          count);
1872: 				}
1873: 			} else {
1874: 				// External sorting with variable size data. Pin the heap blocks too
1875: 				if (!l_done) {
1876: 					l_heap_handle = buffer_manager.Pin(l_data.heap_blocks[l_data.block_idx].block);
1877: 					l_heap_ptr = l_heap_handle->Ptr() + Load<idx_t>(l_ptr + heap_pointer_offset);
1878: 					D_ASSERT(l_heap_ptr - l_heap_handle->Ptr() >= 0);
1879: 					D_ASSERT((idx_t)(l_heap_ptr - l_heap_handle->Ptr()) <
1880: 					         l_data.heap_blocks[l_data.block_idx].byte_offset);
1881: 				}
1882: 				if (!r_done) {
1883: 					r_heap_handle = buffer_manager.Pin(r_data.heap_blocks[r_data.block_idx].block);
1884: 					r_heap_ptr = r_heap_handle->Ptr() + Load<idx_t>(r_ptr + heap_pointer_offset);
1885: 					D_ASSERT(r_heap_ptr - r_heap_handle->Ptr() >= 0);
1886: 					D_ASSERT((idx_t)(r_heap_ptr - r_heap_handle->Ptr()) <
1887: 					         r_data.heap_blocks[r_data.block_idx].byte_offset);
1888: 				}
1889: 				// Both the row and heap data need to be dealt with
1890: 				if (!l_done && !r_done) {
1891: 					// Both sides have data - merge
1892: 					idx_t l_idx_copy = l_data.entry_idx;
1893: 					idx_t r_idx_copy = r_data.entry_idx;
1894: 					data_ptr_t result_data_ptr_copy = result_data_ptr;
1895: 					idx_t copied_copy = copied;
1896: 					// Merge row data
1897: 					MergeRows(l_ptr, l_idx_copy, l_count, r_ptr, r_idx_copy, r_count, result_data_block,
1898: 					          result_data_ptr_copy, row_width, left_smaller, copied_copy, count);
1899: 					const idx_t merged = copied_copy - copied;
1900: 					// Compute the entry sizes and number of heap bytes that will be copied
1901: 					idx_t copy_bytes = 0;
1902: 					data_ptr_t l_heap_ptr_copy = l_heap_ptr;
1903: 					data_ptr_t r_heap_ptr_copy = r_heap_ptr;
1904: 					for (idx_t i = 0; i < merged; i++) {
1905: 						// Store base heap offset in the row data
1906: 						Store<idx_t>(result_heap_block->byte_offset + copy_bytes,
1907: 						             result_data_ptr + heap_pointer_offset);
1908: 						result_data_ptr += row_width;
1909: 						// Compute entry size and add to total
1910: 						const bool &l_smaller = left_smaller[copied + i];
1911: 						const bool r_smaller = !l_smaller;
1912: 						auto &entry_size = next_entry_sizes[copied + i];
1913: 						entry_size =
1914: 						    l_smaller * Load<idx_t>(l_heap_ptr_copy) + r_smaller * Load<idx_t>(r_heap_ptr_copy);
1915: 						D_ASSERT(entry_size >= sizeof(idx_t));
1916: 						D_ASSERT(l_heap_ptr_copy - l_heap_handle->Ptr() + l_smaller * entry_size <=
1917: 						         l_data.heap_blocks[l_data.block_idx].byte_offset);
1918: 						D_ASSERT(r_heap_ptr_copy - r_heap_handle->Ptr() + r_smaller * entry_size <=
1919: 						         r_data.heap_blocks[r_data.block_idx].byte_offset);
1920: 						l_heap_ptr_copy += l_smaller * entry_size;
1921: 						r_heap_ptr_copy += r_smaller * entry_size;
1922: 						copy_bytes += entry_size;
1923: 					}
1924: 					// Reallocate result heap block size (if needed)
1925: 					if (result_heap_block->byte_offset + copy_bytes > result_heap_block->capacity) {
1926: 						idx_t new_capacity = result_heap_block->byte_offset + copy_bytes;
1927: 						buffer_manager.ReAllocate(result_heap_block->block, new_capacity);
1928: 						result_heap_block->capacity = new_capacity;
1929: 						result_heap_ptr = result_heap_handle->Ptr() + result_heap_block->byte_offset;
1930: 					}
1931: 					D_ASSERT(result_heap_block->byte_offset + copy_bytes <= result_heap_block->capacity);
1932: 					// Now copy the heap data
1933: 					for (idx_t i = 0; i < merged; i++) {
1934: 						const bool &l_smaller = left_smaller[copied + i];
1935: 						const bool r_smaller = !l_smaller;
1936: 						const auto &entry_size = next_entry_sizes[copied + i];
1937: 						memcpy(result_heap_ptr, l_heap_ptr, l_smaller * entry_size);
1938: 						memcpy(result_heap_ptr, r_heap_ptr, r_smaller * entry_size);
1939: 						D_ASSERT(Load<idx_t>(result_heap_ptr) == entry_size);
1940: 						result_heap_ptr += entry_size;
1941: 						l_heap_ptr += l_smaller * entry_size;
1942: 						r_heap_ptr += r_smaller * entry_size;
1943: 						l_data.entry_idx += l_smaller;
1944: 						r_data.entry_idx += r_smaller;
1945: 					}
1946: 					// Update result indices and pointers
1947: 					result_heap_block->count += merged;
1948: 					result_heap_block->byte_offset += copy_bytes;
1949: 					copied += merged;
1950: 				} else if (r_done) {
1951: 					// Right side is exhausted - flush left
1952: 					FlushBlobs(buffer_manager, layout, l_count, l_ptr, l_data.entry_idx, l_heap_ptr, result_data_block,
1953: 					           result_data_ptr, result_heap_block, *result_heap_handle, result_heap_ptr, copied, count);
1954: 				} else {
1955: 					// Left side is exhausted - flush right
1956: 					FlushBlobs(buffer_manager, layout, r_count, r_ptr, r_data.entry_idx, r_heap_ptr, result_data_block,
1957: 					           result_data_ptr, result_heap_block, *result_heap_handle, result_heap_ptr, copied, count);
1958: 				}
1959: 				D_ASSERT(result_data_block->count == result_heap_block->count);
1960: 			}
1961: 		}
1962: 	}
1963: 
1964: 	//! Merges constant size rows
1965: 	static void MergeRows(data_ptr_t &l_ptr, idx_t &l_entry_idx, const idx_t &l_count, data_ptr_t &r_ptr,
1966: 	                      idx_t &r_entry_idx, const idx_t &r_count, RowDataBlock *target_block, data_ptr_t &target_ptr,
1967: 	                      const idx_t &entry_size, const bool *left_smaller, idx_t &copied, const idx_t &count) {
1968: 		const idx_t next = MinValue(count - copied, target_block->capacity - target_block->count);
1969: 		idx_t i;
1970: 		for (i = 0; i < next && l_entry_idx < l_count && r_entry_idx < r_count; i++) {
1971: 			const bool &l_smaller = left_smaller[copied + i];
1972: 			const bool r_smaller = !l_smaller;
1973: 			// Use comparison bool (0 or 1) to copy an entry from either side
1974: 			memcpy(target_ptr, l_ptr, l_smaller * entry_size);
1975: 			memcpy(target_ptr, r_ptr, r_smaller * entry_size);
1976: 			target_ptr += entry_size;
1977: 			// Use the comparison bool to increment entries and pointers
1978: 			l_entry_idx += l_smaller;
1979: 			r_entry_idx += r_smaller;
1980: 			l_ptr += l_smaller * entry_size;
1981: 			r_ptr += r_smaller * entry_size;
1982: 		}
1983: 		// Update counts
1984: 		target_block->count += i;
1985: 		copied += i;
1986: 	}
1987: 
1988: 	//! Flushes constant size rows
1989: 	static void FlushRows(data_ptr_t &source_ptr, idx_t &source_entry_idx, const idx_t &source_count,
1990: 	                      RowDataBlock *target_block, data_ptr_t &target_ptr, const idx_t &entry_size, idx_t &copied,
1991: 	                      const idx_t &count) {
1992: 		// Compute how many entries we can fit
1993: 		idx_t next = MinValue(count - copied, target_block->capacity - target_block->count);
1994: 		next = MinValue(next, source_count - source_entry_idx);
1995: 		// Copy them all in a single memcpy
1996: 		const idx_t copy_bytes = next * entry_size;
1997: 		memcpy(target_ptr, source_ptr, copy_bytes);
1998: 		target_ptr += copy_bytes;
1999: 		source_ptr += copy_bytes;
2000: 		// Update counts
2001: 		source_entry_idx += next;
2002: 		target_block->count += next;
2003: 		copied += next;
2004: 	}
2005: 
2006: 	//! Flushes blob rows and accompanying heap
2007: 	static void FlushBlobs(BufferManager &buffer_manager, const RowLayout &layout, const idx_t &source_count,
2008: 	                       data_ptr_t &source_data_ptr, idx_t &source_entry_idx, data_ptr_t &source_heap_ptr,
2009: 	                       RowDataBlock *target_data_block, data_ptr_t &target_data_ptr,
2010: 	                       RowDataBlock *target_heap_block, BufferHandle &target_heap_handle,
2011: 	                       data_ptr_t &target_heap_ptr, idx_t &copied, const idx_t &count) {
2012: 		const idx_t row_width = layout.GetRowWidth();
2013: 		const idx_t heap_pointer_offset = layout.GetHeapPointerOffset();
2014: 		idx_t source_entry_idx_copy = source_entry_idx;
2015: 		data_ptr_t target_data_ptr_copy = target_data_ptr;
2016: 		idx_t copied_copy = copied;
2017: 		// Flush row data
2018: 		FlushRows(source_data_ptr, source_entry_idx_copy, source_count, target_data_block, target_data_ptr_copy,
2019: 		          row_width, copied_copy, count);
2020: 		const idx_t flushed = copied_copy - copied;
2021: 		// Compute the entry sizes and number of heap bytes that will be copied
2022: 		idx_t copy_bytes = 0;
2023: 		data_ptr_t source_heap_ptr_copy = source_heap_ptr;
2024: 		for (idx_t i = 0; i < flushed; i++) {
2025: 			// Store base heap offset in the row data
2026: 			Store<idx_t>(target_heap_block->byte_offset + copy_bytes, target_data_ptr + heap_pointer_offset);
2027: 			target_data_ptr += row_width;
2028: 			// Compute entry size and add to total
2029: 			auto entry_size = Load<idx_t>(source_heap_ptr_copy);
2030: 			D_ASSERT(entry_size >= sizeof(idx_t));
2031: 			source_heap_ptr_copy += entry_size;
2032: 			copy_bytes += entry_size;
2033: 		}
2034: 		// Reallocate result heap block size (if needed)
2035: 		if (target_heap_block->byte_offset + copy_bytes > target_heap_block->capacity) {
2036: 			idx_t new_capacity = target_heap_block->byte_offset + copy_bytes;
2037: 			buffer_manager.ReAllocate(target_heap_block->block, new_capacity);
2038: 			target_heap_block->capacity = new_capacity;
2039: 			target_heap_ptr = target_heap_handle.Ptr() + target_heap_block->byte_offset;
2040: 		}
2041: 		D_ASSERT(target_heap_block->byte_offset + copy_bytes <= target_heap_block->capacity);
2042: 		// Copy the heap data in one go
2043: 		memcpy(target_heap_ptr, source_heap_ptr, copy_bytes);
2044: 		target_heap_ptr += copy_bytes;
2045: 		source_heap_ptr += copy_bytes;
2046: 		source_entry_idx += flushed;
2047: 		copied += flushed;
2048: 		// Update result indices and pointers
2049: 		target_heap_block->count += flushed;
2050: 		target_heap_block->byte_offset += copy_bytes;
2051: 		D_ASSERT(target_heap_block->byte_offset <= target_heap_block->capacity);
2052: 	}
2053: 
2054: private:
2055: 	Pipeline &parent;
2056: 	ClientContext &context;
2057: 	BufferManager &buffer_manager;
2058: 	OrderGlobalState &state;
2059: 	const SortingState &sorting_state;
2060: 
2061: 	unique_ptr<SortedBlock> left_block;
2062: 	unique_ptr<SortedBlock> right_block;
2063: 	SortedBlock *result;
2064: };
2065: 
2066: bool PhysicalOrder::Finalize(Pipeline &pipeline, ClientContext &context, unique_ptr<GlobalOperatorState> state_p) {
2067: 	this->sink_state = move(state_p);
2068: 	auto &state = (OrderGlobalState &)*this->sink_state;
2069: 	if (state.sorted_blocks.empty()) {
2070: 		return true;
2071: 	}
2072: 	// Set total count
2073: 	for (auto &sb : state.sorted_blocks) {
2074: 		state.total_count += sb->radix_sorting_data.back().count;
2075: 	}
2076: 	// Determine if we need to use do an external sort
2077: 	idx_t total_heap_size =
2078: 	    std::accumulate(state.sorted_blocks.begin(), state.sorted_blocks.end(), (idx_t)0,
2079: 	                    [](idx_t a, const unique_ptr<SortedBlock> &b) { return a + b->HeapSize(); });
2080: 	if (total_heap_size > 0.25 * BufferManager::GetBufferManager(context).GetMaxMemory()) {
2081: 		state.external = true;
2082: 	}
2083: 	// Use the data that we have to determine which block size to use during the merge
2084: 	const auto &sorting_state = state.sorting_state;
2085: 	for (auto &sb : state.sorted_blocks) {
2086: 		auto &block = sb->radix_sorting_data.back();
2087: 		state.block_capacity = MaxValue(state.block_capacity, block.capacity);
2088: 	}
2089: 	// Sorting heap data
2090: 	if (!sorting_state.all_constant && state.external) {
2091: 		for (auto &sb : state.sorted_blocks) {
2092: 			auto &heap_block = sb->blob_sorting_data->heap_blocks.back();
2093: 			state.sorting_heap_capacity = MaxValue(state.sorting_heap_capacity, heap_block.capacity);
2094: 		}
2095: 	}
2096: 	// Payload heap data
2097: 	const auto &payload_layout = state.payload_layout;
2098: 	if (!payload_layout.AllConstant() && state.external) {
2099: 		for (auto &sb : state.sorted_blocks) {
2100: 			auto &heap_block = sb->payload_data->heap_blocks.back();
2101: 			state.payload_heap_capacity = MaxValue(state.sorting_heap_capacity, heap_block.capacity);
2102: 		}
2103: 	}
2104: 	// Unswizzle and pin heap blocks if we can fit everything in memory
2105: 	if (!state.external) {
2106: 		for (auto &sb : state.sorted_blocks) {
2107: 			sb->blob_sorting_data->Unswizzle();
2108: 			sb->payload_data->Unswizzle();
2109: 		}
2110: 	}
2111: 	// Start the merge or finish if a merge is not necessary
2112: 	if (state.sorted_blocks.size() > 1) {
2113: 		// More than one block - merge
2114: 		PhysicalOrder::ScheduleMergeTasks(pipeline, context, state);
2115: 		return false;
2116: 	} else {
2117: 		// Clean up sorting data - payload is sorted
2118: 		for (auto &sb : state.sorted_blocks) {
2119: 			sb->UnregisterSortingBlocks();
2120: 		}
2121: 		return true;
2122: 	}
2123: }
2124: 
2125: void PhysicalOrder::ScheduleMergeTasks(Pipeline &pipeline, ClientContext &context, OrderGlobalState &state) {
2126: 	D_ASSERT(state.sorted_blocks_temp.empty());
2127: 	if (state.sorted_blocks.size() == 1) {
2128: 		for (auto &sb : state.sorted_blocks) {
2129: 			sb->UnregisterSortingBlocks();
2130: 		}
2131: 		pipeline.Finish();
2132: 		return;
2133: 	}
2134: 	// Uneven amount of blocks - keep one on the side
2135: 	auto num_blocks = state.sorted_blocks.size();
2136: 	if (num_blocks % 2 == 1) {
2137: 		state.odd_one_out = move(state.sorted_blocks.back());
2138: 		state.sorted_blocks.pop_back();
2139: 		num_blocks--;
2140: 	}
2141: 	// Init merge path path indices
2142: 	state.pair_idx = 0;
2143: 	state.l_start = 0;
2144: 	state.r_start = 0;
2145: 	// Compute how many tasks there will be
2146: 	idx_t num_tasks = 0;
2147: 	const idx_t tuples_per_block = state.block_capacity;
2148: 	for (idx_t block_idx = 0; block_idx < num_blocks; block_idx += 2) {
2149: 		auto &left = *state.sorted_blocks[block_idx];
2150: 		auto &right = *state.sorted_blocks[block_idx + 1];
2151: 		const idx_t count = left.Count() + right.Count();
2152: 		num_tasks += (count + tuples_per_block - 1) / tuples_per_block;
2153: 		// Allocate room for merge results
2154: 		state.sorted_blocks_temp.emplace_back();
2155: 	}
2156: 	// Schedule the tasks
2157: 	pipeline.total_tasks += num_tasks;
2158: 	for (idx_t tnum = 0; tnum < num_tasks; tnum++) {
2159: 		auto new_task = make_unique<PhysicalOrderMergeTask>(pipeline, context, state);
2160: 		TaskScheduler::GetScheduler(context).ScheduleTask(pipeline.token, move(new_task));
2161: 	}
2162: }
2163: 
2164: //===--------------------------------------------------------------------===//
2165: // GetChunkInternal
2166: //===--------------------------------------------------------------------===//
2167: class PhysicalOrderOperatorState : public PhysicalOperatorState {
2168: public:
2169: 	PhysicalOrderOperatorState(PhysicalOperator &op, PhysicalOperator *child)
2170: 	    : PhysicalOperatorState(op, child), initialized(false), global_entry_idx(0), block_idx(0), entry_idx(0) {
2171: 	}
2172: 	bool initialized;
2173: 
2174: 	SortedData *payload_data;
2175: 	Vector addresses = Vector(LogicalType::POINTER);
2176: 
2177: 	idx_t global_entry_idx;
2178: 	idx_t block_idx;
2179: 	idx_t entry_idx;
2180: };
2181: 
2182: unique_ptr<PhysicalOperatorState> PhysicalOrder::GetOperatorState() {
2183: 	return make_unique<PhysicalOrderOperatorState>(*this, children[0].get());
2184: }
2185: 
2186: //! Scans the payload of the final SortedBlock result
2187: static void Scan(ClientContext &context, DataChunk &chunk, PhysicalOrderOperatorState &state, OrderGlobalState &gstate,
2188:                  const idx_t &scan_count) {
2189: 	auto &buffer_manager = BufferManager::GetBufferManager(context);
2190: 	auto &payload_data = *state.payload_data;
2191: 	const auto &layout = gstate.payload_layout;
2192: 	const idx_t &row_width = layout.GetRowWidth();
2193: 	vector<unique_ptr<BufferHandle>> handles;
2194: 	// Set up a batch of pointers to scan data from
2195: 	idx_t count = 0;
2196: 	auto data_pointers = FlatVector::GetData<data_ptr_t>(state.addresses);
2197: 	while (count < scan_count) {
2198: 		auto &data_block = payload_data.data_blocks[state.block_idx];
2199: 		idx_t next = MinValue(data_block.count - state.entry_idx, scan_count - count);
2200: 		auto data_handle = buffer_manager.Pin(data_block.block);
2201: 		const data_ptr_t payload_dataptr = data_handle->Ptr() + state.entry_idx * row_width;
2202: 		handles.push_back(move(data_handle));
2203: 		// Set up the next pointers
2204: 		data_ptr_t row_ptr = payload_dataptr;
2205: 		for (idx_t i = 0; i < next; i++) {
2206: 			data_pointers[count + i] = row_ptr;
2207: 			row_ptr += row_width;
2208: 		}
2209: 		// Unswizzle the offsets back to pointers (if needed)
2210: 		if (!layout.AllConstant() && gstate.external) {
2211: 			auto heap_handle = buffer_manager.Pin(payload_data.heap_blocks[state.block_idx].block);
2212: 			RowOperations::UnswizzleHeapPointer(layout, payload_dataptr, heap_handle->Ptr(), next);
2213: 			RowOperations::UnswizzleColumns(layout, payload_dataptr, next);
2214: 			handles.push_back(move(heap_handle));
2215: 		}
2216: 		// Update state indices
2217: 		state.entry_idx += next;
2218: 		if (state.entry_idx == data_block.count) {
2219: 			state.block_idx++;
2220: 			state.entry_idx = 0;
2221: 		}
2222: 		count += next;
2223: 	}
2224: 	D_ASSERT(count == scan_count);
2225: 	state.global_entry_idx += scan_count;
2226: 	// Deserialize the payload data
2227: 	for (idx_t col_idx = 0; col_idx < layout.ColumnCount(); col_idx++) {
2228: 		const auto col_offset = layout.GetOffsets()[col_idx];
2229: 		RowOperations::Gather(state.addresses, FlatVector::INCREMENTAL_SELECTION_VECTOR, chunk.data[col_idx],
2230: 		                      FlatVector::INCREMENTAL_SELECTION_VECTOR, scan_count, col_offset, col_idx);
2231: 	}
2232: }
2233: 
2234: void PhysicalOrder::GetChunkInternal(ExecutionContext &context, DataChunk &chunk,
2235:                                      PhysicalOperatorState *state_p) const {
2236: 	auto &state = *reinterpret_cast<PhysicalOrderOperatorState *>(state_p);
2237: 	auto &gstate = (OrderGlobalState &)*this->sink_state;
2238: 
2239: 	if (gstate.sorted_blocks.empty()) {
2240: 		return;
2241: 	}
2242: 
2243: 	if (!state.initialized) {
2244: 		D_ASSERT(gstate.sorted_blocks.back()->Count() == gstate.total_count);
2245: 		state.payload_data = gstate.sorted_blocks.back()->payload_data.get();
2246: 		state.initialized = true;
2247: 	}
2248: 
2249: 	auto next = MinValue((idx_t)STANDARD_VECTOR_SIZE, gstate.total_count - state.global_entry_idx);
2250: 	Scan(context.client, chunk, state, gstate, next);
2251: 	chunk.SetCardinality(next);
2252: 	chunk.Verify();
2253: }
2254: 
2255: string PhysicalOrder::ParamsToString() const {
2256: 	string result;
2257: 	for (idx_t i = 0; i < orders.size(); i++) {
2258: 		if (i > 0) {
2259: 			result += "\n";
2260: 		}
2261: 		result += orders[i].expression->ToString() + " ";
2262: 		result += orders[i].type == OrderType::DESCENDING ? "DESC" : "ASC";
2263: 	}
2264: 	return result;
2265: }
2266: 
2267: } // namespace duckdb
[end of src/execution/operator/order/physical_order.cpp]
[start of src/execution/physical_plan/plan_order.cpp]
1: #include "duckdb/execution/operator/order/physical_order.hpp"
2: #include "duckdb/execution/physical_plan_generator.hpp"
3: #include "duckdb/planner/operator/logical_order.hpp"
4: 
5: namespace duckdb {
6: 
7: unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalOrder &op) {
8: 	D_ASSERT(op.children.size() == 1);
9: 
10: 	auto plan = CreatePlan(*op.children[0]);
11: 	if (!op.orders.empty()) {
12: 		auto order = make_unique<PhysicalOrder>(op.types, move(op.orders), op.estimated_cardinality);
13: 		order->children.push_back(move(plan));
14: 		plan = move(order);
15: 	}
16: 	return plan;
17: }
18: 
19: } // namespace duckdb
[end of src/execution/physical_plan/plan_order.cpp]
[start of src/include/duckdb/common/exception.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/exception.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/assert.hpp"
12: #include "duckdb/common/common.hpp"
13: #include "duckdb/common/vector.hpp"
14: #include "duckdb/common/exception_format_value.hpp"
15: 
16: #include <stdexcept>
17: 
18: namespace duckdb {
19: enum class PhysicalType : uint8_t;
20: struct LogicalType;
21: struct hugeint_t;
22: 
23: inline void assert_restrict_function(void *left_start, void *left_end, void *right_start, void *right_end,
24:                                      const char *fname, int linenr) {
25: 	// assert that the two pointers do not overlap
26: #ifdef DEBUG
27: 	if (!(left_end <= right_start || right_end <= left_start)) {
28: 		printf("ASSERT RESTRICT FAILED: %s:%d\n", fname, linenr);
29: 		D_ASSERT(0);
30: 	}
31: #endif
32: }
33: 
34: #define ASSERT_RESTRICT(left_start, left_end, right_start, right_end)                                                  \
35: 	assert_restrict_function(left_start, left_end, right_start, right_end, __FILE__, __LINE__)
36: 
37: //===--------------------------------------------------------------------===//
38: // Exception Types
39: //===--------------------------------------------------------------------===//
40: 
41: enum class ExceptionType {
42: 	INVALID = 0,          // invalid type
43: 	OUT_OF_RANGE = 1,     // value out of range error
44: 	CONVERSION = 2,       // conversion/casting error
45: 	UNKNOWN_TYPE = 3,     // unknown type
46: 	DECIMAL = 4,          // decimal related
47: 	MISMATCH_TYPE = 5,    // type mismatch
48: 	DIVIDE_BY_ZERO = 6,   // divide by 0
49: 	OBJECT_SIZE = 7,      // object size exceeded
50: 	INVALID_TYPE = 8,     // incompatible for operation
51: 	SERIALIZATION = 9,    // serialization
52: 	TRANSACTION = 10,     // transaction management
53: 	NOT_IMPLEMENTED = 11, // method not implemented
54: 	EXPRESSION = 12,      // expression parsing
55: 	CATALOG = 13,         // catalog related
56: 	PARSER = 14,          // parser related
57: 	PLANNER = 15,         // planner related
58: 	SCHEDULER = 16,       // scheduler related
59: 	EXECUTOR = 17,        // executor related
60: 	CONSTRAINT = 18,      // constraint related
61: 	INDEX = 19,           // index related
62: 	STAT = 20,            // stat related
63: 	CONNECTION = 21,      // connection related
64: 	SYNTAX = 22,          // syntax related
65: 	SETTINGS = 23,        // settings related
66: 	BINDER = 24,          // binder related
67: 	NETWORK = 25,         // network related
68: 	OPTIMIZER = 26,       // optimizer related
69: 	NULL_POINTER = 27,    // nullptr exception
70: 	IO = 28,              // IO exception
71: 	INTERRUPT = 29,       // interrupt
72: 	FATAL = 30, // Fatal exception: fatal exceptions are non-recoverable, and render the entire DB in an unusable state
73: 	INTERNAL =
74: 	    31, // Internal exception: exception that indicates something went wrong internally (i.e. bug in the code base)
75: 	INVALID_INPUT = 32 // Input or arguments error
76: };
77: 
78: class Exception : public std::exception {
79: public:
80: 	explicit Exception(const string &msg);
81: 	Exception(ExceptionType exception_type, const string &message);
82: 
83: 	ExceptionType type;
84: 
85: public:
86: 	const char *what() const noexcept override;
87: 
88: 	string ExceptionTypeToString(ExceptionType type);
89: 
90: 	template <typename... Args>
91: 	static string ConstructMessage(const string &msg, Args... params) {
92: 		vector<ExceptionFormatValue> values;
93: 		return ConstructMessageRecursive(msg, values, params...);
94: 	}
95: 
96: 	static string ConstructMessageRecursive(const string &msg, vector<ExceptionFormatValue> &values);
97: 
98: 	template <class T, typename... Args>
99: 	static string ConstructMessageRecursive(const string &msg, vector<ExceptionFormatValue> &values, T param,
100: 	                                        Args... params) {
101: 		values.push_back(ExceptionFormatValue::CreateFormatValue<T>(param));
102: 		return ConstructMessageRecursive(msg, values, params...);
103: 	}
104: 
105: private:
106: 	string exception_message_;
107: };
108: 
109: //===--------------------------------------------------------------------===//
110: // Exception derived classes
111: //===--------------------------------------------------------------------===//
112: 
113: //! Exceptions that are StandardExceptions do NOT invalidate the current transaction when thrown
114: class StandardException : public Exception {
115: public:
116: 	StandardException(ExceptionType exception_type, string message) : Exception(exception_type, message) {
117: 	}
118: };
119: 
120: class CatalogException : public StandardException {
121: public:
122: 	explicit CatalogException(const string &msg);
123: 
124: 	template <typename... Args>
125: 	explicit CatalogException(const string &msg, Args... params) : CatalogException(ConstructMessage(msg, params...)) {
126: 	}
127: };
128: 
129: class ParserException : public StandardException {
130: public:
131: 	explicit ParserException(const string &msg);
132: 
133: 	template <typename... Args>
134: 	explicit ParserException(const string &msg, Args... params) : ParserException(ConstructMessage(msg, params...)) {
135: 	}
136: };
137: 
138: class BinderException : public StandardException {
139: public:
140: 	explicit BinderException(const string &msg);
141: 
142: 	template <typename... Args>
143: 	explicit BinderException(const string &msg, Args... params) : BinderException(ConstructMessage(msg, params...)) {
144: 	}
145: };
146: 
147: class ConversionException : public Exception {
148: public:
149: 	explicit ConversionException(const string &msg);
150: 
151: 	template <typename... Args>
152: 	explicit ConversionException(const string &msg, Args... params)
153: 	    : ConversionException(ConstructMessage(msg, params...)) {
154: 	}
155: };
156: 
157: class TransactionException : public Exception {
158: public:
159: 	explicit TransactionException(const string &msg);
160: 
161: 	template <typename... Args>
162: 	explicit TransactionException(const string &msg, Args... params)
163: 	    : TransactionException(ConstructMessage(msg, params...)) {
164: 	}
165: };
166: 
167: class NotImplementedException : public Exception {
168: public:
169: 	explicit NotImplementedException(const string &msg);
170: 
171: 	template <typename... Args>
172: 	explicit NotImplementedException(const string &msg, Args... params)
173: 	    : NotImplementedException(ConstructMessage(msg, params...)) {
174: 	}
175: };
176: 
177: class OutOfRangeException : public Exception {
178: public:
179: 	explicit OutOfRangeException(const string &msg);
180: 
181: 	template <typename... Args>
182: 	explicit OutOfRangeException(const string &msg, Args... params)
183: 	    : OutOfRangeException(ConstructMessage(msg, params...)) {
184: 	}
185: };
186: 
187: class SyntaxException : public Exception {
188: public:
189: 	explicit SyntaxException(const string &msg);
190: 
191: 	template <typename... Args>
192: 	explicit SyntaxException(const string &msg, Args... params) : SyntaxException(ConstructMessage(msg, params...)) {
193: 	}
194: };
195: 
196: class ConstraintException : public Exception {
197: public:
198: 	explicit ConstraintException(const string &msg);
199: 
200: 	template <typename... Args>
201: 	explicit ConstraintException(const string &msg, Args... params)
202: 	    : ConstraintException(ConstructMessage(msg, params...)) {
203: 	}
204: };
205: 
206: class IOException : public Exception {
207: public:
208: 	explicit IOException(const string &msg);
209: 
210: 	template <typename... Args>
211: 	explicit IOException(const string &msg, Args... params) : IOException(ConstructMessage(msg, params...)) {
212: 	}
213: };
214: 
215: class SerializationException : public Exception {
216: public:
217: 	explicit SerializationException(const string &msg);
218: 
219: 	template <typename... Args>
220: 	explicit SerializationException(const string &msg, Args... params)
221: 	    : SerializationException(ConstructMessage(msg, params...)) {
222: 	}
223: };
224: 
225: class SequenceException : public Exception {
226: public:
227: 	explicit SequenceException(const string &msg);
228: 
229: 	template <typename... Args>
230: 	explicit SequenceException(const string &msg, Args... params)
231: 	    : SequenceException(ConstructMessage(msg, params...)) {
232: 	}
233: };
234: 
235: class InterruptException : public Exception {
236: public:
237: 	InterruptException();
238: };
239: 
240: class FatalException : public Exception {
241: public:
242: 	explicit FatalException(const string &msg);
243: 
244: 	template <typename... Args>
245: 	explicit FatalException(const string &msg, Args... params) : FatalException(ConstructMessage(msg, params...)) {
246: 	}
247: };
248: 
249: class InternalException : public Exception {
250: public:
251: 	explicit InternalException(const string &msg);
252: 
253: 	template <typename... Args>
254: 	explicit InternalException(const string &msg, Args... params)
255: 	    : InternalException(ConstructMessage(msg, params...)) {
256: 	}
257: };
258: 
259: class InvalidInputException : public Exception {
260: public:
261: 	explicit InvalidInputException(const string &msg);
262: 
263: 	template <typename... Args>
264: 	explicit InvalidInputException(const string &msg, Args... params)
265: 	    : InvalidInputException(ConstructMessage(msg, params...)) {
266: 	}
267: };
268: 
269: class CastException : public Exception {
270: public:
271: 	CastException(const PhysicalType origType, const PhysicalType newType);
272: 	CastException(const LogicalType &origType, const LogicalType &newType);
273: };
274: 
275: class InvalidTypeException : public Exception {
276: public:
277: 	InvalidTypeException(PhysicalType type, const string &msg);
278: 	InvalidTypeException(const LogicalType &type, const string &msg);
279: };
280: 
281: class TypeMismatchException : public Exception {
282: public:
283: 	TypeMismatchException(const PhysicalType type_1, const PhysicalType type_2, const string &msg);
284: 	TypeMismatchException(const LogicalType &type_1, const LogicalType &type_2, const string &msg);
285: };
286: 
287: class ValueOutOfRangeException : public Exception {
288: public:
289: 	ValueOutOfRangeException(const int64_t value, const PhysicalType origType, const PhysicalType newType);
290: 	ValueOutOfRangeException(const hugeint_t value, const PhysicalType origType, const PhysicalType newType);
291: 	ValueOutOfRangeException(const double value, const PhysicalType origType, const PhysicalType newType);
292: 	ValueOutOfRangeException(const PhysicalType varType, const idx_t length);
293: };
294: 
295: } // namespace duckdb
[end of src/include/duckdb/common/exception.hpp]
[start of src/include/duckdb/common/types/row_data_collection.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/types/row_chunk.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/types/vector.hpp"
13: #include "duckdb/storage/buffer_manager.hpp"
14: 
15: namespace duckdb {
16: 
17: struct RowDataBlock {
18: 	RowDataBlock(BufferManager &buffer_manager, idx_t capacity, idx_t entry_size)
19: 	    : capacity(capacity), entry_size(entry_size), count(0), byte_offset(0) {
20: 		block = buffer_manager.RegisterMemory(capacity * entry_size, false);
21: 	}
22: 	//! The buffer block handle
23: 	shared_ptr<BlockHandle> block;
24: 	//! Capacity (number of entries) and entry size that fit in this block
25: 	idx_t capacity;
26: 	const idx_t entry_size;
27: 	//! Number of entries currently in this block
28: 	idx_t count;
29: 	//! Write offset (if variable size entries)
30: 	idx_t byte_offset;
31: };
32: 
33: struct BlockAppendEntry {
34: 	BlockAppendEntry(data_ptr_t baseptr, idx_t count) : baseptr(baseptr), count(count) {
35: 	}
36: 	data_ptr_t baseptr;
37: 	idx_t count;
38: };
39: 
40: class RowDataCollection {
41: public:
42: 	RowDataCollection(BufferManager &buffer_manager, idx_t block_capacity, idx_t entry_size, bool keep_pinned = false);
43: 
44: 	//! BufferManager
45: 	BufferManager &buffer_manager;
46: 	//! The total number of stored entries
47: 	idx_t count;
48: 	//! The number of entries per block
49: 	idx_t block_capacity;
50: 	//! Size of entries in the blocks
51: 	idx_t entry_size;
52: 	//! The blocks holding the main data
53: 	vector<RowDataBlock> blocks;
54: 	//! The blocks that this collection currently has pinned
55: 	vector<unique_ptr<BufferHandle>> pinned_blocks;
56: 
57: public:
58: 	idx_t AppendToBlock(RowDataBlock &block, BufferHandle &handle, vector<BlockAppendEntry> &append_entries,
59: 	                    idx_t remaining, idx_t entry_sizes[]);
60: 	vector<unique_ptr<BufferHandle>> Build(idx_t added_count, data_ptr_t key_locations[], idx_t entry_sizes[],
61: 	                                       const SelectionVector *sel = &FlatVector::INCREMENTAL_SELECTION_VECTOR);
62: 
63: 	void Merge(RowDataCollection &other);
64: 
65: private:
66: 	mutex rdc_lock;
67: 
68: 	//! Whether the blocks should stay pinned (necessary for e.g. a heap)
69: 	const bool keep_pinned;
70: };
71: 
72: } // namespace duckdb
[end of src/include/duckdb/common/types/row_data_collection.hpp]
[start of src/include/duckdb/execution/operator/order/physical_order.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/order/physical_order.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/chunk_collection.hpp"
12: #include "duckdb/execution/physical_sink.hpp"
13: #include "duckdb/parallel/pipeline.hpp"
14: #include "duckdb/planner/bound_query_node.hpp"
15: 
16: namespace duckdb {
17: 
18: struct SortingState;
19: struct SortedBlock;
20: class OrderLocalState;
21: class OrderGlobalState;
22: 
23: //! Physically re-orders the input data
24: class PhysicalOrder : public PhysicalSink {
25: public:
26: 	PhysicalOrder(vector<LogicalType> types, vector<BoundOrderByNode> orders, idx_t estimated_cardinality);
27: 
28: 	//! Input data
29: 	vector<BoundOrderByNode> orders;
30: 
31: public:
32: 	void Sink(ExecutionContext &context, GlobalOperatorState &gstate_p, LocalSinkState &lstate_p,
33: 	          DataChunk &input) const override;
34: 	void Combine(ExecutionContext &context, GlobalOperatorState &gstate_p, LocalSinkState &lstate_p) override;
35: 	bool Finalize(Pipeline &pipeline, ClientContext &context, unique_ptr<GlobalOperatorState> gstate_p) override;
36: 
37: 	unique_ptr<LocalSinkState> GetLocalSinkState(ExecutionContext &context) override;
38: 	unique_ptr<GlobalOperatorState> GetGlobalState(ClientContext &context) override;
39: 
40: 	void GetChunkInternal(ExecutionContext &context, DataChunk &chunk, PhysicalOperatorState *state) const override;
41: 	unique_ptr<PhysicalOperatorState> GetOperatorState() override;
42: 
43: 	string ParamsToString() const override;
44: 
45: 	//! Schedule merge tasks until all blocks are merged
46: 	static void ScheduleMergeTasks(Pipeline &pipeline, ClientContext &context, OrderGlobalState &state);
47: 	//! Compares two values that are serialized to row format at the given pointers (recursive if type is nested)
48: 	static inline int CompareValAndAdvance(data_ptr_t &l_ptr, data_ptr_t &r_ptr, const LogicalType &type);
49: 
50: private:
51: 	//! Sort and re-order local state data when the local state has aggregated SORTING_BLOCK_SIZE data
52: 	void SortLocalState(ClientContext &context, OrderLocalState &lstate, OrderGlobalState &state) const;
53: };
54: 
55: } // namespace duckdb
[end of src/include/duckdb/execution/operator/order/physical_order.hpp]
[start of src/include/duckdb/planner/expression_binder.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/expression_binder.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/exception.hpp"
12: #include "duckdb/parser/expression/bound_expression.hpp"
13: #include "duckdb/parser/parsed_expression.hpp"
14: #include "duckdb/parser/tokens.hpp"
15: #include "duckdb/planner/expression.hpp"
16: #include "duckdb/common/unordered_map.hpp"
17: 
18: namespace duckdb {
19: 
20: class Binder;
21: class ClientContext;
22: class QueryNode;
23: 
24: class ScalarFunctionCatalogEntry;
25: class AggregateFunctionCatalogEntry;
26: class MacroCatalogEntry;
27: class CatalogEntry;
28: class SimpleFunction;
29: 
30: struct MacroBinding;
31: 
32: struct BindResult {
33: 	explicit BindResult(string error) : error(error) {
34: 	}
35: 	explicit BindResult(unique_ptr<Expression> expr) : expression(move(expr)) {
36: 	}
37: 
38: 	bool HasError() {
39: 		return !error.empty();
40: 	}
41: 
42: 	unique_ptr<Expression> expression;
43: 	string error;
44: };
45: 
46: class ExpressionBinder {
47: public:
48: 	ExpressionBinder(Binder &binder, ClientContext &context, bool replace_binder = false);
49: 	virtual ~ExpressionBinder();
50: 
51: 	//! The target type that should result from the binder. If the result is not of this type, a cast to this type will
52: 	//! be added. Defaults to INVALID.
53: 	LogicalType target_type;
54: 
55: public:
56: 	unique_ptr<Expression> Bind(unique_ptr<ParsedExpression> &expr, LogicalType *result_type = nullptr,
57: 	                            bool root_expression = true);
58: 
59: 	//! Returns whether or not any columns have been bound by the expression binder
60: 	bool BoundColumns() {
61: 		return bound_columns;
62: 	}
63: 
64: 	string Bind(unique_ptr<ParsedExpression> *expr, idx_t depth, bool root_expression = false);
65: 
66: 	// Bind table names to ColumnRefExpressions
67: 	static void BindTableNames(Binder &binder, ParsedExpression &expr,
68: 	                           unordered_map<string, idx_t> *alias_map = nullptr);
69: 	static unique_ptr<Expression> PushCollation(ClientContext &context, unique_ptr<Expression> source,
70: 	                                            const string &collation, bool equality_only = false);
71: 	static void TestCollation(ClientContext &context, const string &collation);
72: 
73: 	bool BindCorrelatedColumns(unique_ptr<ParsedExpression> &expr);
74: 
75: 	void BindChild(unique_ptr<ParsedExpression> &expr, idx_t depth, string &error);
76: 	static void ExtractCorrelatedExpressions(Binder &binder, Expression &expr);
77: 
78: protected:
79: 	virtual BindResult BindExpression(unique_ptr<ParsedExpression> *expr_ptr, idx_t depth,
80: 	                                  bool root_expression = false);
81: 
82: 	BindResult BindExpression(BetweenExpression &expr, idx_t depth);
83: 	BindResult BindExpression(CaseExpression &expr, idx_t depth);
84: 	BindResult BindExpression(CollateExpression &expr, idx_t depth);
85: 	BindResult BindExpression(CastExpression &expr, idx_t depth);
86: 	BindResult BindExpression(ColumnRefExpression &expr, idx_t depth);
87: 	BindResult BindExpression(ComparisonExpression &expr, idx_t depth);
88: 	BindResult BindExpression(ConjunctionExpression &expr, idx_t depth);
89: 	BindResult BindExpression(ConstantExpression &expr, idx_t depth);
90: 	BindResult BindExpression(FunctionExpression &expr, idx_t depth, unique_ptr<ParsedExpression> *expr_ptr);
91: 	BindResult BindExpression(LambdaExpression &expr, idx_t depth);
92: 	BindResult BindExpression(OperatorExpression &expr, idx_t depth);
93: 	BindResult BindExpression(ParameterExpression &expr, idx_t depth);
94: 	BindResult BindExpression(PositionalReferenceExpression &ref, idx_t depth);
95: 	BindResult BindExpression(StarExpression &expr, idx_t depth);
96: 	BindResult BindExpression(SubqueryExpression &expr, idx_t depth);
97: 
98: protected:
99: 	virtual BindResult BindFunction(FunctionExpression &expr, ScalarFunctionCatalogEntry *function, idx_t depth);
100: 	virtual BindResult BindAggregate(FunctionExpression &expr, AggregateFunctionCatalogEntry *function, idx_t depth);
101: 	virtual BindResult BindUnnest(FunctionExpression &expr, idx_t depth);
102: 	virtual BindResult BindMacro(FunctionExpression &expr, MacroCatalogEntry *macro, idx_t depth,
103: 	                             unique_ptr<ParsedExpression> *expr_ptr);
104: 
105: 	virtual void ReplaceMacroParametersRecursive(unique_ptr<ParsedExpression> &expr);
106: 	virtual void ReplaceMacroParametersRecursive(ParsedExpression &expr, QueryNode &node);
107: 	virtual void ReplaceMacroParametersRecursive(ParsedExpression &expr, TableRef &ref);
108: 	virtual void CheckForSideEffects(FunctionExpression &function, idx_t depth, string &error);
109: 
110: 	virtual string UnsupportedAggregateMessage();
111: 	virtual string UnsupportedUnnestMessage();
112: 
113: 	Binder &binder;
114: 	ClientContext &context;
115: 	ExpressionBinder *stored_binder;
116: 	MacroBinding *macro_binding;
117: 	bool bound_columns = false;
118: };
119: 
120: } // namespace duckdb
[end of src/include/duckdb/planner/expression_binder.hpp]
[start of src/include/duckdb/planner/operator/logical_order.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/operator/logical_order.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/planner/bound_query_node.hpp"
12: #include "duckdb/planner/logical_operator.hpp"
13: 
14: namespace duckdb {
15: 
16: //! LogicalOrder represents an ORDER BY clause, sorting the data
17: class LogicalOrder : public LogicalOperator {
18: public:
19: 	explicit LogicalOrder(vector<BoundOrderByNode> orders)
20: 	    : LogicalOperator(LogicalOperatorType::LOGICAL_ORDER_BY), orders(move(orders)) {
21: 	}
22: 
23: 	vector<BoundOrderByNode> orders;
24: 
25: 	string ParamsToString() const override {
26: 		string result;
27: 		for (idx_t i = 0; i < orders.size(); i++) {
28: 			if (i > 0) {
29: 				result += "\n";
30: 			}
31: 			result += orders[i].expression->GetName();
32: 		}
33: 		return result;
34: 	}
35: 
36: public:
37: 	vector<ColumnBinding> GetColumnBindings() override {
38: 		return children[0]->GetColumnBindings();
39: 	}
40: 
41: protected:
42: 	void ResolveTypes() override {
43: 		types = children[0]->types;
44: 	}
45: };
46: } // namespace duckdb
[end of src/include/duckdb/planner/operator/logical_order.hpp]
[start of src/include/duckdb/storage/buffer/block_handle.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/buffer/block_handle.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/mutex.hpp"
13: #include "duckdb/common/atomic.hpp"
14: #include "duckdb/storage/storage_info.hpp"
15: 
16: namespace duckdb {
17: class BufferHandle;
18: class BufferManager;
19: class DatabaseInstance;
20: class FileBuffer;
21: 
22: enum class BlockState : uint8_t { BLOCK_UNLOADED = 0, BLOCK_LOADED = 1 };
23: 
24: class BlockHandle {
25: 	friend struct BufferEvictionNode;
26: 	friend class BufferHandle;
27: 	friend class BufferManager;
28: 
29: public:
30: 	BlockHandle(DatabaseInstance &db, block_id_t block_id);
31: 	BlockHandle(DatabaseInstance &db, block_id_t block_id, unique_ptr<FileBuffer> buffer, bool can_destroy,
32: 	            idx_t alloc_size);
33: 	~BlockHandle();
34: 
35: 	DatabaseInstance &db;
36: 
37: public:
38: 	block_id_t BlockId() {
39: 		return block_id;
40: 	}
41: 
42: 	int32_t Readers() const {
43: 		return readers;
44: 	}
45: 
46: private:
47: 	static unique_ptr<BufferHandle> Load(shared_ptr<BlockHandle> &handle);
48: 	void Unload();
49: 	bool CanUnload();
50: 
51: 	//! The block-level lock
52: 	mutex lock;
53: 	//! Whether or not the block is loaded/unloaded
54: 	BlockState state;
55: 	//! Amount of concurrent readers
56: 	atomic<int32_t> readers;
57: 	//! The block id of the block
58: 	const block_id_t block_id;
59: 	//! Pointer to loaded data (if any)
60: 	unique_ptr<FileBuffer> buffer;
61: 	//! Internal eviction timestamp
62: 	atomic<idx_t> eviction_timestamp;
63: 	//! Whether or not the buffer can be destroyed (only used for temporary buffers)
64: 	const bool can_destroy;
65: 	//! The memory usage of the block
66: 	idx_t memory_usage;
67: };
68: 
69: } // namespace duckdb
[end of src/include/duckdb/storage/buffer/block_handle.hpp]
[start of src/include/duckdb/storage/buffer_manager.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/buffer_manager.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/storage/buffer/buffer_handle.hpp"
12: #include "duckdb/storage/buffer/managed_buffer.hpp"
13: #include "duckdb/storage/block_manager.hpp"
14: #include "duckdb/common/file_system.hpp"
15: #include "duckdb/common/unordered_map.hpp"
16: #include "duckdb/storage/buffer/block_handle.hpp"
17: 
18: #include "duckdb/common/atomic.hpp"
19: #include "duckdb/common/mutex.hpp"
20: 
21: namespace duckdb {
22: class DatabaseInstance;
23: class TemporaryDirectoryHandle;
24: struct EvictionQueue;
25: 
26: //! The buffer manager is in charge of handling memory management for the database. It hands out memory buffers that can
27: //! be used by the database internally.
28: class BufferManager {
29: 	friend class BufferHandle;
30: 	friend class BlockHandle;
31: 
32: public:
33: 	BufferManager(DatabaseInstance &db, string temp_directory, idx_t maximum_memory);
34: 	~BufferManager();
35: 
36: 	//! Register a block with the given block id in the base file
37: 	shared_ptr<BlockHandle> RegisterBlock(block_id_t block_id);
38: 
39: 	//! Register an in-memory buffer of arbitrary size, as long as it is >= BLOCK_SIZE. can_destroy signifies whether or
40: 	//! not the buffer can be destroyed when unpinned, or whether or not it needs to be written to a temporary file so
41: 	//! it can be reloaded. The resulting buffer will already be allocated, but needs to be pinned in order to be used.
42: 	shared_ptr<BlockHandle> RegisterMemory(idx_t alloc_size, bool can_destroy);
43: 
44: 	//! Allocate an in-memory buffer with a single pin.
45: 	//! The allocated memory is released when the buffer handle is destroyed.
46: 	unique_ptr<BufferHandle> Allocate(idx_t alloc_size);
47: 
48: 	//! Reallocate an in-memory buffer that is pinned.
49: 	void ReAllocate(shared_ptr<BlockHandle> &handle, idx_t alloc_size);
50: 
51: 	unique_ptr<BufferHandle> Pin(shared_ptr<BlockHandle> &handle);
52: 	void Unpin(shared_ptr<BlockHandle> &handle);
53: 
54: 	void UnregisterBlock(block_id_t block_id, bool can_destroy);
55: 
56: 	//! Set a new memory limit to the buffer manager, throws an exception if the new limit is too low and not enough
57: 	//! blocks can be evicted
58: 	void SetLimit(idx_t limit = (idx_t)-1);
59: 
60: 	static BufferManager &GetBufferManager(ClientContext &context);
61: 	static BufferManager &GetBufferManager(DatabaseInstance &db);
62: 
63: 	idx_t GetUsedMemory() {
64: 		return current_memory;
65: 	}
66: 	idx_t GetMaxMemory() {
67: 		return maximum_memory;
68: 	}
69: 
70: 	const string &GetTemporaryDirectory() {
71: 		return temp_directory;
72: 	}
73: 
74: 	void SetTemporaryDirectory(string new_dir);
75: 
76: private:
77: 	//! Evict blocks until the currently used memory + extra_memory fit, returns false if this was not possible
78: 	//! (i.e. not enough blocks could be evicted)
79: 	bool EvictBlocks(idx_t extra_memory, idx_t memory_limit);
80: 
81: 	//! Write a temporary buffer to disk
82: 	void WriteTemporaryBuffer(ManagedBuffer &buffer);
83: 	//! Read a temporary buffer from disk
84: 	unique_ptr<FileBuffer> ReadTemporaryBuffer(block_id_t id);
85: 	//! Get the path of the temporary buffer
86: 	string GetTemporaryPath(block_id_t id);
87: 
88: 	void DeleteTemporaryFile(block_id_t id);
89: 
90: 	void RequireTemporaryDirectory();
91: 
92: private:
93: 	//! The database instance
94: 	DatabaseInstance &db;
95: 	//! The current amount of memory that is occupied by the buffer manager (in bytes)
96: 	atomic<idx_t> current_memory;
97: 	//! The maximum amount of memory that the buffer manager can keep (in bytes)
98: 	atomic<idx_t> maximum_memory;
99: 	//! The directory name where temporary files are stored
100: 	string temp_directory;
101: 	//! Lock for creating the temp handle
102: 	mutex temp_handle_lock;
103: 	//! Handle for the temporary directory
104: 	unique_ptr<TemporaryDirectoryHandle> temp_directory_handle;
105: 	//! The lock for the set of blocks
106: 	mutex manager_lock;
107: 	//! A mapping of block id -> BlockHandle
108: 	unordered_map<block_id_t, weak_ptr<BlockHandle>> blocks;
109: 	//! Eviction queue
110: 	unique_ptr<EvictionQueue> queue;
111: 	//! The temporary id used for managed buffers
112: 	atomic<block_id_t> temporary_id;
113: };
114: } // namespace duckdb
[end of src/include/duckdb/storage/buffer_manager.hpp]
[start of src/optimizer/statistics/operator/propagate_order.cpp]
1: #include "duckdb/optimizer/statistics_propagator.hpp"
2: #include "duckdb/planner/operator/logical_order.hpp"
3: 
4: namespace duckdb {
5: 
6: unique_ptr<NodeStatistics> StatisticsPropagator::PropagateStatistics(LogicalOrder &order,
7:                                                                      unique_ptr<LogicalOperator> *node_ptr) {
8: 	// propagate statistics in the child node
9: 	return PropagateStatistics(order.children[0]);
10: }
11: 
12: } // namespace duckdb
[end of src/optimizer/statistics/operator/propagate_order.cpp]
[start of src/optimizer/statistics_propagator.cpp]
1: #include "duckdb/optimizer/statistics_propagator.hpp"
2: #include "duckdb/planner/logical_operator.hpp"
3: #include "duckdb/planner/expression_iterator.hpp"
4: #include "duckdb/planner/operator/logical_empty_result.hpp"
5: #include "duckdb/main/client_context.hpp"
6: 
7: namespace duckdb {
8: 
9: StatisticsPropagator::StatisticsPropagator(ClientContext &context) : context(context) {
10: }
11: 
12: void StatisticsPropagator::ReplaceWithEmptyResult(unique_ptr<LogicalOperator> &node) {
13: 	node = make_unique<LogicalEmptyResult>(move(node));
14: }
15: 
16: unique_ptr<NodeStatistics> StatisticsPropagator::PropagateChildren(LogicalOperator &node,
17:                                                                    unique_ptr<LogicalOperator> *node_ptr) {
18: 	for (idx_t child_idx = 0; child_idx < node.children.size(); child_idx++) {
19: 		PropagateStatistics(node.children[child_idx]);
20: 	}
21: 	return nullptr;
22: }
23: 
24: unique_ptr<NodeStatistics> StatisticsPropagator::PropagateStatistics(LogicalOperator &node,
25:                                                                      unique_ptr<LogicalOperator> *node_ptr) {
26: 	switch (node.type) {
27: 	case LogicalOperatorType::LOGICAL_AGGREGATE_AND_GROUP_BY:
28: 		return PropagateStatistics((LogicalAggregate &)node, node_ptr);
29: 	case LogicalOperatorType::LOGICAL_CROSS_PRODUCT:
30: 		return PropagateStatistics((LogicalCrossProduct &)node, node_ptr);
31: 	case LogicalOperatorType::LOGICAL_FILTER:
32: 		return PropagateStatistics((LogicalFilter &)node, node_ptr);
33: 	case LogicalOperatorType::LOGICAL_GET:
34: 		return PropagateStatistics((LogicalGet &)node, node_ptr);
35: 	case LogicalOperatorType::LOGICAL_PROJECTION:
36: 		return PropagateStatistics((LogicalProjection &)node, node_ptr);
37: 	case LogicalOperatorType::LOGICAL_ANY_JOIN:
38: 	case LogicalOperatorType::LOGICAL_COMPARISON_JOIN:
39: 	case LogicalOperatorType::LOGICAL_JOIN:
40: 		return PropagateStatistics((LogicalJoin &)node, node_ptr);
41: 	case LogicalOperatorType::LOGICAL_UNION:
42: 	case LogicalOperatorType::LOGICAL_EXCEPT:
43: 	case LogicalOperatorType::LOGICAL_INTERSECT:
44: 		return PropagateStatistics((LogicalSetOperation &)node, node_ptr);
45: 	default:
46: 		return PropagateChildren(node, node_ptr);
47: 	}
48: }
49: 
50: unique_ptr<NodeStatistics> StatisticsPropagator::PropagateStatistics(unique_ptr<LogicalOperator> &node_ptr) {
51: 	return PropagateStatistics(*node_ptr, &node_ptr);
52: }
53: 
54: unique_ptr<BaseStatistics> StatisticsPropagator::PropagateExpression(Expression &expr,
55:                                                                      unique_ptr<Expression> *expr_ptr) {
56: 	switch (expr.GetExpressionClass()) {
57: 	case ExpressionClass::BOUND_AGGREGATE:
58: 		return PropagateExpression((BoundAggregateExpression &)expr, expr_ptr);
59: 	case ExpressionClass::BOUND_BETWEEN:
60: 		return PropagateExpression((BoundBetweenExpression &)expr, expr_ptr);
61: 	case ExpressionClass::BOUND_CASE:
62: 		return PropagateExpression((BoundCaseExpression &)expr, expr_ptr);
63: 	case ExpressionClass::BOUND_FUNCTION:
64: 		return PropagateExpression((BoundFunctionExpression &)expr, expr_ptr);
65: 	case ExpressionClass::BOUND_CAST:
66: 		return PropagateExpression((BoundCastExpression &)expr, expr_ptr);
67: 	case ExpressionClass::BOUND_COMPARISON:
68: 		return PropagateExpression((BoundComparisonExpression &)expr, expr_ptr);
69: 	case ExpressionClass::BOUND_CONSTANT:
70: 		return PropagateExpression((BoundConstantExpression &)expr, expr_ptr);
71: 	case ExpressionClass::BOUND_COLUMN_REF:
72: 		return PropagateExpression((BoundColumnRefExpression &)expr, expr_ptr);
73: 	case ExpressionClass::BOUND_OPERATOR:
74: 		return PropagateExpression((BoundOperatorExpression &)expr, expr_ptr);
75: 	default:
76: 		break;
77: 	}
78: 	ExpressionIterator::EnumerateChildren(expr, [&](unique_ptr<Expression> &child) { PropagateExpression(child); });
79: 	return nullptr;
80: }
81: 
82: unique_ptr<BaseStatistics> StatisticsPropagator::PropagateExpression(unique_ptr<Expression> &expr) {
83: 	auto stats = PropagateExpression(*expr, &expr);
84: 	if (context.query_verification_enabled && stats) {
85: 		expr->stats = stats->Copy();
86: 	}
87: 	return stats;
88: }
89: 
90: } // namespace duckdb
[end of src/optimizer/statistics_propagator.cpp]
[start of src/planner/binder/expression/bind_macro_expression.cpp]
1: #include "duckdb/catalog/catalog_entry/macro_catalog_entry.hpp"
2: #include "duckdb/parser/expression/function_expression.hpp"
3: #include "duckdb/parser/expression/subquery_expression.hpp"
4: #include "duckdb/parser/parsed_expression_iterator.hpp"
5: #include "duckdb/parser/query_node.hpp"
6: #include "duckdb/parser/query_node/recursive_cte_node.hpp"
7: #include "duckdb/parser/query_node/select_node.hpp"
8: #include "duckdb/parser/query_node/set_operation_node.hpp"
9: #include "duckdb/parser/tableref/list.hpp"
10: #include "duckdb/planner/expression_binder.hpp"
11: #include "duckdb/common/string_util.hpp"
12: 
13: namespace duckdb {
14: 
15: void ExpressionBinder::ReplaceMacroParametersRecursive(unique_ptr<ParsedExpression> &expr) {
16: 	switch (expr->GetExpressionClass()) {
17: 	case ExpressionClass::COLUMN_REF: {
18: 		// if expr is a parameter, replace it with its argument
19: 		auto &colref = (ColumnRefExpression &)*expr;
20: 		if (colref.table_name.empty() && macro_binding->HasMatchingBinding(colref.column_name)) {
21: 			expr = macro_binding->ParamToArg(colref);
22: 		}
23: 		return;
24: 	}
25: 	case ExpressionClass::SUBQUERY: {
26: 		// replacing parameters within a subquery is slightly different
27: 		auto &sq = ((SubqueryExpression &)*expr).subquery;
28: 		ReplaceMacroParametersRecursive(*expr, *sq->node);
29: 		break;
30: 	}
31: 	default: // fall through
32: 		break;
33: 	}
34: 	// unfold child expressions
35: 	ParsedExpressionIterator::EnumerateChildren(
36: 	    *expr, [&](unique_ptr<ParsedExpression> &child) { ReplaceMacroParametersRecursive(child); });
37: }
38: 
39: void ExpressionBinder::ReplaceMacroParametersRecursive(ParsedExpression &expr, TableRef &ref) {
40: 	switch (ref.type) {
41: 	case TableReferenceType::CROSS_PRODUCT: {
42: 		auto &cp_ref = (CrossProductRef &)ref;
43: 		ReplaceMacroParametersRecursive(expr, *cp_ref.left);
44: 		ReplaceMacroParametersRecursive(expr, *cp_ref.right);
45: 		break;
46: 	}
47: 	case TableReferenceType::EXPRESSION_LIST: {
48: 		auto &el_ref = (ExpressionListRef &)ref;
49: 		for (idx_t i = 0; i < el_ref.values.size(); i++) {
50: 			for (idx_t j = 0; j < el_ref.values[i].size(); j++) {
51: 				ReplaceMacroParametersRecursive(el_ref.values[i][j]);
52: 			}
53: 		}
54: 		break;
55: 	}
56: 	case TableReferenceType::JOIN: {
57: 		auto &j_ref = (JoinRef &)ref;
58: 		ReplaceMacroParametersRecursive(expr, *j_ref.left);
59: 		ReplaceMacroParametersRecursive(expr, *j_ref.right);
60: 		ReplaceMacroParametersRecursive(j_ref.condition);
61: 		break;
62: 	}
63: 	case TableReferenceType::SUBQUERY: {
64: 		auto &sq_ref = (SubqueryRef &)ref;
65: 		ReplaceMacroParametersRecursive(expr, *sq_ref.subquery->node);
66: 		break;
67: 	}
68: 	case TableReferenceType::TABLE_FUNCTION: {
69: 		auto &tf_ref = (TableFunctionRef &)ref;
70: 		ReplaceMacroParametersRecursive(tf_ref.function);
71: 		break;
72: 	}
73: 	case TableReferenceType::BASE_TABLE:
74: 	case TableReferenceType::EMPTY:
75: 		// these TableRefs do not need to be unfolded
76: 		break;
77: 	default:
78: 		throw NotImplementedException("TableRef type not implemented for macro's!");
79: 	}
80: }
81: 
82: void ExpressionBinder::ReplaceMacroParametersRecursive(ParsedExpression &expr, QueryNode &node) {
83: 	switch (node.type) {
84: 	case QueryNodeType::RECURSIVE_CTE_NODE: {
85: 		auto &rcte_node = (RecursiveCTENode &)node;
86: 		ReplaceMacroParametersRecursive(expr, *rcte_node.left);
87: 		ReplaceMacroParametersRecursive(expr, *rcte_node.right);
88: 		break;
89: 	}
90: 	case QueryNodeType::SELECT_NODE: {
91: 		auto &sel_node = (SelectNode &)node;
92: 		for (idx_t i = 0; i < sel_node.select_list.size(); i++) {
93: 			ReplaceMacroParametersRecursive(sel_node.select_list[i]);
94: 		}
95: 		for (idx_t i = 0; i < sel_node.groups.size(); i++) {
96: 			ReplaceMacroParametersRecursive(sel_node.groups[i]);
97: 		}
98: 		if (sel_node.where_clause != nullptr) {
99: 			ReplaceMacroParametersRecursive(sel_node.where_clause);
100: 		}
101: 		if (sel_node.having != nullptr) {
102: 			ReplaceMacroParametersRecursive(sel_node.having);
103: 		}
104: 
105: 		ReplaceMacroParametersRecursive(expr, *sel_node.from_table.get());
106: 		break;
107: 	}
108: 	case QueryNodeType::SET_OPERATION_NODE: {
109: 		auto &setop_node = (SetOperationNode &)node;
110: 		ReplaceMacroParametersRecursive(expr, *setop_node.left);
111: 		ReplaceMacroParametersRecursive(expr, *setop_node.right);
112: 		break;
113: 	}
114: 	default:
115: 		throw NotImplementedException("QueryNode type not implemented for macro's!");
116: 	}
117: 	for (auto &kv : node.cte_map) {
118: 		ReplaceMacroParametersRecursive(expr, *kv.second->query->node);
119: 	}
120: }
121: 
122: void ExpressionBinder::CheckForSideEffects(FunctionExpression &function, idx_t depth, string &error) {
123: 	for (idx_t i = 0; i < function.children.size(); i++) {
124: 		auto arg_copy = function.children[i]->Copy();
125: 		BindChild(arg_copy, depth, error);
126: 		if (!error.empty()) {
127: 			return;
128: 		}
129: 		auto &bound_expr = (BoundExpression &)*arg_copy;
130: 		if (bound_expr.expr->HasSideEffects()) {
131: 			QueryErrorContext error_context(binder.root_statement, function.query_location);
132: 			error = StringUtil::Format("Arguments with side-effects are not supported ('%s()' was supplied). As a "
133: 			                           "workaround, try creating a CTE that evaluates the argument with side-effects.",
134: 			                           arg_copy->ToString());
135: 			return;
136: 		}
137: 	}
138: }
139: 
140: BindResult ExpressionBinder::BindMacro(FunctionExpression &function, MacroCatalogEntry *macro_func, idx_t depth,
141:                                        unique_ptr<ParsedExpression> *expr) {
142: 	auto &macro_def = *macro_func->function;
143: 	// validate the arguments and separate positional and default arguments
144: 	vector<unique_ptr<ParsedExpression>> positionals;
145: 	unordered_map<string, unique_ptr<ParsedExpression>> defaults;
146: 	string error = MacroFunction::ValidateArguments(*macro_func, function, positionals, defaults);
147: 	if (!error.empty()) {
148: 		return BindResult(binder.FormatError(*expr->get(), error));
149: 	}
150: 
151: 	// check for arguments with side-effects TODO: to support this, a projection must be pushed
152: 	//    CheckForSideEffects(function, depth, error);
153: 	//    if (!error.empty()) {
154: 	//        return BindResult(error);
155: 	//    }
156: 
157: 	// create a MacroBinding to bind this macro's parameters to its arguments
158: 	vector<LogicalType> types;
159: 	vector<string> names;
160: 	// positional parameters
161: 	for (idx_t i = 0; i < macro_def.parameters.size(); i++) {
162: 		types.push_back(LogicalType::SQLNULL);
163: 		auto &param = (ColumnRefExpression &)*macro_def.parameters[i];
164: 		names.push_back(param.column_name);
165: 	}
166: 	// default parameters
167: 	for (auto it = macro_def.default_parameters.begin(); it != macro_def.default_parameters.end(); it++) {
168: 		types.push_back(LogicalType::SQLNULL);
169: 		names.push_back(it->first);
170: 		// now push the defaults into the positionals
171: 		positionals.push_back(move(defaults[it->first]));
172: 	}
173: 	auto new_macro_binding = make_unique<MacroBinding>(types, names, macro_func->name);
174: 	new_macro_binding->arguments = move(positionals);
175: 	macro_binding = new_macro_binding.get();
176: 
177: 	// replace current expression with stored macro expression, and replace params
178: 	*expr = macro_func->function->expression->Copy();
179: 	ReplaceMacroParametersRecursive(*expr);
180: 
181: 	// bind the unfolded macro
182: 	return BindExpression(expr, depth);
183: }
184: 
185: } // namespace duckdb
[end of src/planner/binder/expression/bind_macro_expression.cpp]
[start of src/storage/buffer_manager.cpp]
1: #include "duckdb/storage/buffer_manager.hpp"
2: 
3: #include "duckdb/common/allocator.hpp"
4: #include "duckdb/common/exception.hpp"
5: #include "duckdb/parallel/concurrentqueue.hpp"
6: #include "duckdb/storage/storage_manager.hpp"
7: 
8: namespace duckdb {
9: 
10: BlockHandle::BlockHandle(DatabaseInstance &db, block_id_t block_id_p)
11:     : db(db), readers(0), block_id(block_id_p), buffer(nullptr), eviction_timestamp(0), can_destroy(false) {
12: 	eviction_timestamp = 0;
13: 	state = BlockState::BLOCK_UNLOADED;
14: 	memory_usage = Storage::BLOCK_ALLOC_SIZE;
15: }
16: 
17: BlockHandle::BlockHandle(DatabaseInstance &db, block_id_t block_id_p, unique_ptr<FileBuffer> buffer_p,
18:                          bool can_destroy_p, idx_t alloc_size)
19:     : db(db), readers(0), block_id(block_id_p), eviction_timestamp(0), can_destroy(can_destroy_p) {
20: 	D_ASSERT(alloc_size >= Storage::BLOCK_SIZE);
21: 	buffer = move(buffer_p);
22: 	state = BlockState::BLOCK_LOADED;
23: 	memory_usage = alloc_size + Storage::BLOCK_HEADER_SIZE;
24: }
25: 
26: BlockHandle::~BlockHandle() {
27: 	auto &buffer_manager = BufferManager::GetBufferManager(db);
28: 	// no references remain to this block: erase
29: 	if (state == BlockState::BLOCK_LOADED) {
30: 		// the block is still loaded in memory: erase it
31: 		buffer.reset();
32: 		buffer_manager.current_memory -= memory_usage;
33: 	}
34: 	buffer_manager.UnregisterBlock(block_id, can_destroy);
35: }
36: 
37: unique_ptr<BufferHandle> BlockHandle::Load(shared_ptr<BlockHandle> &handle) {
38: 	if (handle->state == BlockState::BLOCK_LOADED) {
39: 		// already loaded
40: 		D_ASSERT(handle->buffer);
41: 		return make_unique<BufferHandle>(handle, handle->buffer.get());
42: 	}
43: 	handle->state = BlockState::BLOCK_LOADED;
44: 
45: 	auto &buffer_manager = BufferManager::GetBufferManager(handle->db);
46: 	auto &block_manager = BlockManager::GetBlockManager(handle->db);
47: 	if (handle->block_id < MAXIMUM_BLOCK) {
48: 		auto block = make_unique<Block>(Allocator::Get(handle->db), handle->block_id);
49: 		block_manager.Read(*block);
50: 		handle->buffer = move(block);
51: 	} else {
52: 		if (handle->can_destroy) {
53: 			return nullptr;
54: 		} else {
55: 			handle->buffer = buffer_manager.ReadTemporaryBuffer(handle->block_id);
56: 		}
57: 	}
58: 	return make_unique<BufferHandle>(handle, handle->buffer.get());
59: }
60: 
61: void BlockHandle::Unload() {
62: 	if (state == BlockState::BLOCK_UNLOADED) {
63: 		// already unloaded: nothing to do
64: 		return;
65: 	}
66: 	D_ASSERT(CanUnload());
67: 	D_ASSERT(memory_usage >= Storage::BLOCK_SIZE);
68: 	state = BlockState::BLOCK_UNLOADED;
69: 
70: 	auto &buffer_manager = BufferManager::GetBufferManager(db);
71: 	if (block_id >= MAXIMUM_BLOCK && !can_destroy) {
72: 		// temporary block that cannot be destroyed: write to temporary file
73: 		buffer_manager.WriteTemporaryBuffer((ManagedBuffer &)*buffer);
74: 	}
75: 	buffer.reset();
76: 	buffer_manager.current_memory -= memory_usage;
77: }
78: 
79: bool BlockHandle::CanUnload() {
80: 	if (state == BlockState::BLOCK_UNLOADED) {
81: 		// already unloaded
82: 		return false;
83: 	}
84: 	if (readers > 0) {
85: 		// there are active readers
86: 		return false;
87: 	}
88: 	auto &buffer_manager = BufferManager::GetBufferManager(db);
89: 	if (block_id >= MAXIMUM_BLOCK && !can_destroy && buffer_manager.temp_directory.empty()) {
90: 		// in order to unload this block we need to write it to a temporary buffer
91: 		// however, no temporary directory is specified!
92: 		// hence we cannot unload the block
93: 		return false;
94: 	}
95: 	return true;
96: }
97: 
98: struct BufferEvictionNode {
99: 	BufferEvictionNode(weak_ptr<BlockHandle> handle_p, idx_t timestamp_p)
100: 	    : handle(move(handle_p)), timestamp(timestamp_p) {
101: 		D_ASSERT(!handle.expired());
102: 	}
103: 
104: 	weak_ptr<BlockHandle> handle;
105: 	idx_t timestamp;
106: 
107: 	bool CanUnload(BlockHandle &handle) {
108: 		if (timestamp != handle.eviction_timestamp) {
109: 			// handle was used in between
110: 			return false;
111: 		}
112: 		return handle.CanUnload();
113: 	}
114: };
115: 
116: typedef duckdb_moodycamel::ConcurrentQueue<unique_ptr<BufferEvictionNode>> eviction_queue_t;
117: 
118: struct EvictionQueue {
119: 	eviction_queue_t q;
120: };
121: 
122: class TemporaryDirectoryHandle {
123: public:
124: 	TemporaryDirectoryHandle(DatabaseInstance &db, string path_p) : db(db), temp_directory(move(path_p)) {
125: 		auto &fs = FileSystem::GetFileSystem(db);
126: 		if (!temp_directory.empty()) {
127: 			fs.CreateDirectory(temp_directory);
128: 		}
129: 	}
130: 	~TemporaryDirectoryHandle() {
131: 		auto &fs = FileSystem::GetFileSystem(db);
132: 		if (!temp_directory.empty()) {
133: 			fs.RemoveDirectory(temp_directory);
134: 		}
135: 	}
136: 
137: private:
138: 	DatabaseInstance &db;
139: 	string temp_directory;
140: };
141: 
142: void BufferManager::SetTemporaryDirectory(string new_dir) {
143: 	if (temp_directory_handle) {
144: 		throw NotImplementedException("Cannot switch temporary directory after the current one has been used");
145: 	}
146: 	this->temp_directory = move(new_dir);
147: }
148: 
149: BufferManager::BufferManager(DatabaseInstance &db, string tmp, idx_t maximum_memory)
150:     : db(db), current_memory(0), maximum_memory(maximum_memory), temp_directory(move(tmp)),
151:       queue(make_unique<EvictionQueue>()), temporary_id(MAXIMUM_BLOCK) {
152: }
153: 
154: BufferManager::~BufferManager() {
155: }
156: 
157: shared_ptr<BlockHandle> BufferManager::RegisterBlock(block_id_t block_id) {
158: 	lock_guard<mutex> lock(manager_lock);
159: 	// check if the block already exists
160: 	auto entry = blocks.find(block_id);
161: 	if (entry != blocks.end()) {
162: 		// already exists: check if it hasn't expired yet
163: 		auto existing_ptr = entry->second.lock();
164: 		if (existing_ptr) {
165: 			//! it hasn't! return it
166: 			return existing_ptr;
167: 		}
168: 	}
169: 	// create a new block pointer for this block
170: 	auto result = make_shared<BlockHandle>(db, block_id);
171: 	// register the block pointer in the set of blocks as a weak pointer
172: 	blocks[block_id] = weak_ptr<BlockHandle>(result);
173: 	return result;
174: }
175: 
176: shared_ptr<BlockHandle> BufferManager::RegisterMemory(idx_t alloc_size, bool can_destroy) {
177: 	// first evict blocks until we have enough memory to store this buffer
178: 	if (!EvictBlocks(alloc_size + Storage::BLOCK_HEADER_SIZE, maximum_memory)) {
179: 		throw OutOfRangeException("Not enough memory to complete operation: could not allocate block of %lld bytes",
180: 		                          alloc_size);
181: 	}
182: 
183: 	// allocate the buffer
184: 	auto temp_id = ++temporary_id;
185: 	auto buffer = make_unique<ManagedBuffer>(db, alloc_size, can_destroy, temp_id);
186: 
187: 	// create a new block pointer for this block
188: 	return make_shared<BlockHandle>(db, temp_id, move(buffer), can_destroy, alloc_size);
189: }
190: 
191: unique_ptr<BufferHandle> BufferManager::Allocate(idx_t alloc_size) {
192: 	auto block = RegisterMemory(alloc_size, true);
193: 	return Pin(block);
194: }
195: 
196: void BufferManager::ReAllocate(shared_ptr<BlockHandle> &handle, idx_t alloc_size) {
197: 	D_ASSERT(alloc_size >= Storage::BLOCK_SIZE);
198: 	lock_guard<mutex> lock(handle->lock);
199: 	D_ASSERT(handle->readers == 1);
200: 	auto total_size = alloc_size + Storage::BLOCK_HEADER_SIZE;
201: 	int64_t required_memory = total_size - handle->memory_usage;
202: 	if (required_memory > 0) {
203: 		// evict blocks until we have space to increase the size of this block
204: 		if (!EvictBlocks(required_memory, maximum_memory)) {
205: 			throw OutOfRangeException("Not enough memory to complete operation: failed to increase block size");
206: 		}
207: 	}
208: 	// re-allocate the buffer size and update its memory usage
209: 	handle->buffer->Resize(alloc_size);
210: 	if (required_memory < 0) {
211: 		current_memory += required_memory;
212: 	}
213: 	handle->memory_usage = total_size;
214: }
215: 
216: unique_ptr<BufferHandle> BufferManager::Pin(shared_ptr<BlockHandle> &handle) {
217: 	idx_t required_memory;
218: 	{
219: 		// lock the block
220: 		lock_guard<mutex> lock(handle->lock);
221: 		// check if the block is already loaded
222: 		if (handle->state == BlockState::BLOCK_LOADED) {
223: 			// the block is loaded, increment the reader count and return a pointer to the handle
224: 			handle->readers++;
225: 			return handle->Load(handle);
226: 		}
227: 		required_memory = handle->memory_usage;
228: 	}
229: 	// evict blocks until we have space for the current block
230: 	if (!EvictBlocks(required_memory, maximum_memory)) {
231: 		throw OutOfRangeException("Not enough memory to complete operation: failed to pin block");
232: 	}
233: 	// lock the handle again and repeat the check (in case anybody loaded in the mean time)
234: 	lock_guard<mutex> lock(handle->lock);
235: 	// check if the block is already loaded
236: 	if (handle->state == BlockState::BLOCK_LOADED) {
237: 		// the block is loaded, increment the reader count and return a pointer to the handle
238: 		handle->readers++;
239: 		return handle->Load(handle);
240: 	}
241: 	// now we can actually load the current block
242: 	D_ASSERT(handle->readers == 0);
243: 	handle->readers = 1;
244: 	return handle->Load(handle);
245: }
246: 
247: void BufferManager::Unpin(shared_ptr<BlockHandle> &handle) {
248: 	lock_guard<mutex> lock(handle->lock);
249: 	D_ASSERT(handle->readers > 0);
250: 	handle->readers--;
251: 	if (handle->readers == 0) {
252: 		handle->eviction_timestamp++;
253: 		queue->q.enqueue(make_unique<BufferEvictionNode>(weak_ptr<BlockHandle>(handle), handle->eviction_timestamp));
254: 		// FIXME: do some house-keeping to prevent the queue from being flooded with many old blocks
255: 	}
256: }
257: 
258: bool BufferManager::EvictBlocks(idx_t extra_memory, idx_t memory_limit) {
259: 	unique_ptr<BufferEvictionNode> node;
260: 	current_memory += extra_memory;
261: 	while (current_memory > memory_limit) {
262: 		// get a block to unpin from the queue
263: 		if (!queue->q.try_dequeue(node)) {
264: 			current_memory -= extra_memory;
265: 			return false;
266: 		}
267: 		// get a reference to the underlying block pointer
268: 		auto handle = node->handle.lock();
269: 		if (!handle) {
270: 			continue;
271: 		}
272: 		if (!node->CanUnload(*handle)) {
273: 			// early out: we already know that we cannot unload this node
274: 			continue;
275: 		}
276: 		// we might be able to free this block: grab the mutex and check if we can free it
277: 		lock_guard<mutex> lock(handle->lock);
278: 		if (!node->CanUnload(*handle)) {
279: 			// something changed in the mean-time, bail out
280: 			continue;
281: 		}
282: 		// hooray, we can unload the block
283: 		// release the memory and mark the block as unloaded
284: 		handle->Unload();
285: 	}
286: 	return true;
287: }
288: 
289: void BufferManager::UnregisterBlock(block_id_t block_id, bool can_destroy) {
290: 	if (block_id >= MAXIMUM_BLOCK) {
291: 		// in-memory buffer: destroy the buffer
292: 		if (!can_destroy) {
293: 			// buffer could have been offloaded to disk: remove the file
294: 			DeleteTemporaryFile(block_id);
295: 		}
296: 	} else {
297: 		lock_guard<mutex> lock(manager_lock);
298: 		// on-disk block: erase from list of blocks in manager
299: 		blocks.erase(block_id);
300: 	}
301: }
302: void BufferManager::SetLimit(idx_t limit) {
303: 	lock_guard<mutex> buffer_lock(manager_lock);
304: 	// try to evict until the limit is reached
305: 	if (!EvictBlocks(0, limit)) {
306: 		throw OutOfRangeException(
307: 		    "Failed to change memory limit to new limit %lld: could not free up enough memory for the new limit",
308: 		    limit);
309: 	}
310: 	idx_t old_limit = maximum_memory;
311: 	// set the global maximum memory to the new limit if successful
312: 	maximum_memory = limit;
313: 	// evict again
314: 	if (!EvictBlocks(0, limit)) {
315: 		// failed: go back to old limit
316: 		maximum_memory = old_limit;
317: 		throw OutOfRangeException(
318: 		    "Failed to change memory limit to new limit %lld: could not free up enough memory for the new limit",
319: 		    limit);
320: 	}
321: }
322: 
323: string BufferManager::GetTemporaryPath(block_id_t id) {
324: 	auto &fs = FileSystem::GetFileSystem(db);
325: 	return fs.JoinPath(temp_directory, to_string(id) + ".block");
326: }
327: 
328: void BufferManager::RequireTemporaryDirectory() {
329: 	if (temp_directory.empty()) {
330: 		throw Exception(
331: 		    "Out-of-memory: cannot write buffer because no temporary directory is specified!\nTo enable "
332: 		    "temporary buffer eviction set a temporary directory using PRAGMA temp_directory='/path/to/tmp.tmp'");
333: 	}
334: 	lock_guard<mutex> temp_handle_guard(temp_handle_lock);
335: 	if (!temp_directory_handle) {
336: 		// temp directory has not been created yet: initialize it
337: 		temp_directory_handle = make_unique<TemporaryDirectoryHandle>(db, temp_directory);
338: 	}
339: }
340: 
341: void BufferManager::WriteTemporaryBuffer(ManagedBuffer &buffer) {
342: 	RequireTemporaryDirectory();
343: 
344: 	D_ASSERT(buffer.size >= Storage::BLOCK_SIZE);
345: 	// get the path to write to
346: 	auto path = GetTemporaryPath(buffer.id);
347: 	// create the file and write the size followed by the buffer contents
348: 	auto &fs = FileSystem::GetFileSystem(db);
349: 	auto handle = fs.OpenFile(path, FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE);
350: 	handle->Write(&buffer.size, sizeof(idx_t), 0);
351: 	buffer.Write(*handle, sizeof(idx_t));
352: }
353: 
354: unique_ptr<FileBuffer> BufferManager::ReadTemporaryBuffer(block_id_t id) {
355: 	D_ASSERT(!temp_directory.empty());
356: 	D_ASSERT(temp_directory_handle.get());
357: 	idx_t alloc_size;
358: 	// open the temporary file and read the size
359: 	auto path = GetTemporaryPath(id);
360: 	auto &fs = FileSystem::GetFileSystem(db);
361: 	auto handle = fs.OpenFile(path, FileFlags::FILE_FLAGS_READ);
362: 	handle->Read(&alloc_size, sizeof(idx_t), 0);
363: 
364: 	// now allocate a buffer of this size and read the data into that buffer
365: 	auto buffer = make_unique<ManagedBuffer>(db, alloc_size, false, id);
366: 	buffer->Read(*handle, sizeof(idx_t));
367: 	return move(buffer);
368: }
369: 
370: void BufferManager::DeleteTemporaryFile(block_id_t id) {
371: 	if (temp_directory.empty() || !temp_directory_handle) {
372: 		return;
373: 	}
374: 	auto &fs = FileSystem::GetFileSystem(db);
375: 	auto path = GetTemporaryPath(id);
376: 	if (fs.FileExists(path)) {
377: 		fs.RemoveFile(path);
378: 	}
379: }
380: 
381: } // namespace duckdb
[end of src/storage/buffer_manager.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: