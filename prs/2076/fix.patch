diff --git a/src/common/exception.cpp b/src/common/exception.cpp
index 11856e46d3de..965b2b18852c 100644
--- a/src/common/exception.cpp
+++ b/src/common/exception.cpp
@@ -1,7 +1,8 @@
 #include "duckdb/common/exception.hpp"
+
 #include "duckdb/common/string_util.hpp"
-#include "duckdb/common/types.hpp"
 #include "duckdb/common/to_string.hpp"
+#include "duckdb/common/types.hpp"
 
 namespace duckdb {
 
@@ -87,6 +88,8 @@ string Exception::ExceptionTypeToString(ExceptionType type) {
 		return "INTERNAL";
 	case ExceptionType::INVALID_INPUT:
 		return "Invalid Input";
+	case ExceptionType::OUT_OF_MEMORY:
+		return "Out of Memory";
 	default:
 		return "Unknown";
 	}
@@ -198,4 +201,7 @@ InternalException::InternalException(const string &msg) : Exception(ExceptionTyp
 InvalidInputException::InvalidInputException(const string &msg) : Exception(ExceptionType::INVALID_INPUT, msg) {
 }
 
+OutOfMemoryException::OutOfMemoryException(const string &msg) : Exception(ExceptionType::OUT_OF_MEMORY, msg) {
+}
+
 } // namespace duckdb
diff --git a/src/common/row_operations/row_heap_scatter.cpp b/src/common/row_operations/row_heap_scatter.cpp
index 0dafa9e9247b..ac43c32b1727 100644
--- a/src/common/row_operations/row_heap_scatter.cpp
+++ b/src/common/row_operations/row_heap_scatter.cpp
@@ -114,8 +114,10 @@ void RowOperations::ComputeEntrySizes(Vector &v, VectorData &vdata, idx_t entry_
 			ComputeListEntrySizes(v, vdata, entry_sizes, ser_count, sel, offset);
 			break;
 		default:
+			// LCOV_EXCL_START
 			throw NotImplementedException("Column with variable size type %s cannot be serialized to row-format",
 			                              v.GetType().ToString());
+			// LCOV_EXCL_STOP
 		}
 	}
 }
@@ -381,8 +383,10 @@ void RowOperations::HeapScatter(Vector &v, idx_t vcount, const SelectionVector &
 			HeapScatterListVector(v, vcount, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
 			break;
 		default:
+			// LCOV_EXCL_START
 			throw NotImplementedException("Serialization of variable length vector with type %s",
 			                              v.GetType().ToString());
+			// LCOV_EXCL_STOP
 		}
 	}
 }
@@ -429,7 +433,7 @@ void RowOperations::HeapScatterVData(VectorData &vdata, PhysicalType type, const
 		TemplatedHeapScatter<interval_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
 		break;
 	default:
-		throw NotImplementedException("FIXME: unimplemented serialize to of constant type column to row-format");
+		throw NotImplementedException("FIXME: Serialize to of constant type column to row-format");
 	}
 }
 
diff --git a/src/common/row_operations/row_radix_scatter.cpp b/src/common/row_operations/row_radix_scatter.cpp
index bd29d5bfe623..411150d66a0c 100644
--- a/src/common/row_operations/row_radix_scatter.cpp
+++ b/src/common/row_operations/row_radix_scatter.cpp
@@ -123,7 +123,7 @@ void RadixScatterListVector(Vector &v, VectorData &vdata, const SelectionVector
 					key_locations[i][0] = 1;
 					key_locations[i]++;
 					RowOperations::RadixScatter(child_vector, list_size, FlatVector::INCREMENTAL_SELECTION_VECTOR, 1,
-					                            key_locations + i, false, has_null, false, prefix_len, width - 1,
+					                            key_locations + i, false, true, false, prefix_len, width - 1,
 					                            list_entry.offset);
 				} else {
 					// denote that the list is empty with a 0
@@ -154,7 +154,7 @@ void RadixScatterListVector(Vector &v, VectorData &vdata, const SelectionVector
 				key_locations[i][0] = 1;
 				key_locations[i]++;
 				RowOperations::RadixScatter(child_vector, list_size, FlatVector::INCREMENTAL_SELECTION_VECTOR, 1,
-				                            key_locations + i, false, has_null, false, prefix_len, width - 1,
+				                            key_locations + i, false, true, false, prefix_len, width - 1,
 				                            list_entry.offset);
 			} else {
 				// denote that the list is empty with a 0
@@ -197,7 +197,7 @@ void RadixScatterStructVector(Vector &v, VectorData &vdata, idx_t vcount, const
 	// serialize the struct
 	auto &child_vector = *StructVector::GetEntries(v)[0];
 	RowOperations::RadixScatter(child_vector, vcount, FlatVector::INCREMENTAL_SELECTION_VECTOR, add_count,
-	                            key_locations, false, has_null, false, prefix_len, width, offset);
+	                            key_locations, false, true, false, prefix_len, width, offset);
 	// invert bits if desc
 	if (desc) {
 		for (idx_t i = 0; i < add_count; i++) {
diff --git a/src/common/row_operations/row_scatter.cpp b/src/common/row_operations/row_scatter.cpp
index 388ca5310c50..18ccfc88dafa 100644
--- a/src/common/row_operations/row_scatter.cpp
+++ b/src/common/row_operations/row_scatter.cpp
@@ -123,6 +123,7 @@ void RowOperations::Scatter(DataChunk &columns, VectorData col_data[], const Row
 	auto &types = layout.GetTypes();
 
 	// Compute the entry size of the variable size columns
+	vector<unique_ptr<BufferHandle>> handles;
 	data_ptr_t data_locations[STANDARD_VECTOR_SIZE];
 	if (!layout.AllConstant()) {
 		idx_t entry_sizes[STANDARD_VECTOR_SIZE];
@@ -144,7 +145,7 @@ void RowOperations::Scatter(DataChunk &columns, VectorData col_data[], const Row
 				RowOperations::ComputeEntrySizes(vec, col, entry_sizes, vcount, count, sel);
 				break;
 			default:
-				throw Exception("Unsupported type for RowOperations::Scatter");
+				throw InternalException("Unsupported type for RowOperations::Scatter");
 			}
 		}
 
diff --git a/src/common/types/row_data_collection.cpp b/src/common/types/row_data_collection.cpp
index 8f9d41d325b8..40f2323e0cb4 100644
--- a/src/common/types/row_data_collection.cpp
+++ b/src/common/types/row_data_collection.cpp
@@ -18,13 +18,19 @@ idx_t RowDataCollection::AppendToBlock(RowDataBlock &block, BufferHandle &handle
 	idx_t append_count = 0;
 	data_ptr_t dataptr;
 	if (entry_sizes) {
-		// compute how many entries fit if entry size if variable
-		dataptr = handle.node->buffer + block.byte_offset;
+		D_ASSERT(entry_size == 1);
+		// compute how many entries fit if entry size is variable
+		dataptr = handle.Ptr() + block.byte_offset;
 		for (idx_t i = 0; i < remaining; i++) {
-			if (block.byte_offset + entry_sizes[i] > block_capacity * entry_size) {
-				while (entry_sizes[i] > block_capacity * entry_size) {
-					// if an entry does not fit, increase capacity until it does
-					block_capacity *= 2;
+			if (block.byte_offset + entry_sizes[i] > block.capacity) {
+				if (block.count == 0 && append_count == 0 && entry_sizes[i] > block.capacity) {
+					// special case: single entry is bigger than block capacity
+					// resize current block to fit the entry, append it, and move to the next block
+					block.capacity = entry_sizes[i];
+					buffer_manager.ReAllocate(block.block, block.capacity);
+					dataptr = handle.Ptr();
+					append_count++;
+					block.byte_offset += entry_sizes[i];
 				}
 				break;
 			}
@@ -33,7 +39,7 @@ idx_t RowDataCollection::AppendToBlock(RowDataBlock &block, BufferHandle &handle
 		}
 	} else {
 		append_count = MinValue<idx_t>(remaining, block.capacity - block.count);
-		dataptr = handle.node->buffer + block.count * entry_size;
+		dataptr = handle.Ptr() + block.count * entry_size;
 	}
 	append_entries.emplace_back(dataptr, append_count);
 	block.count += append_count;
@@ -72,16 +78,14 @@ vector<unique_ptr<BufferHandle>> RowDataCollection::Build(idx_t added_count, dat
 			idx_t *offset_entry_sizes = entry_sizes ? entry_sizes + added_count - remaining : nullptr;
 
 			idx_t append_count = AppendToBlock(new_block, *handle, append_entries, remaining, offset_entry_sizes);
+			D_ASSERT(new_block.count > 0);
 			remaining -= append_count;
 
-			if (new_block.count > 0) {
-				// in case 0 tuples fit the block (huge entry, e.g. large string) we do not add
-				blocks.push_back(move(new_block));
-				if (keep_pinned) {
-					pinned_blocks.push_back(move(handle));
-				} else {
-					handles.push_back(move(handle));
-				}
+			blocks.push_back(move(new_block));
+			if (keep_pinned) {
+				pinned_blocks.push_back(move(handle));
+			} else {
+				handles.push_back(move(handle));
 			}
 		}
 	}
diff --git a/src/execution/operator/order/physical_order.cpp b/src/execution/operator/order/physical_order.cpp
index d832744886ec..ab0f02df6552 100644
--- a/src/execution/operator/order/physical_order.cpp
+++ b/src/execution/operator/order/physical_order.cpp
@@ -18,8 +18,10 @@ namespace duckdb {
 
 using ValidityBytes = RowLayout::ValidityBytes;
 
-PhysicalOrder::PhysicalOrder(vector<LogicalType> types, vector<BoundOrderByNode> orders, idx_t estimated_cardinality)
-    : PhysicalSink(PhysicalOperatorType::ORDER_BY, move(types), estimated_cardinality), orders(move(orders)) {
+PhysicalOrder::PhysicalOrder(vector<LogicalType> types, vector<BoundOrderByNode> orders,
+                             vector<unique_ptr<BaseStatistics>> statistics, idx_t estimated_cardinality)
+    : PhysicalSink(PhysicalOperatorType::ORDER_BY, move(types), estimated_cardinality), orders(move(orders)),
+      statistics(move(statistics)) {
 }
 
 //===--------------------------------------------------------------------===//
@@ -47,7 +49,7 @@ static idx_t GetSortingColSize(const LogicalType &type) {
 }
 
 struct SortingState {
-	explicit SortingState(const vector<BoundOrderByNode> &orders)
+	SortingState(const vector<BoundOrderByNode> &orders, const vector<unique_ptr<BaseStatistics>> &statistics)
 	    : column_count(orders.size()), all_constant(true), comparison_size(0), entry_size(0) {
 		vector<LogicalType> blob_layout_types;
 		for (idx_t i = 0; i < orders.size(); i++) {
@@ -64,12 +66,11 @@ struct SortingState {
 			column_sizes.push_back(0);
 			auto &col_size = column_sizes.back();
 
-			if (expr.stats) {
-				stats.push_back(expr.stats.get());
+			if (!statistics.empty() && statistics[i]) {
+				stats.push_back(statistics[i].get());
 				has_null.push_back(stats.back()->CanHaveNull());
 			} else {
 				stats.push_back(nullptr);
-				// No stats - we must assume that there are nulls
 				has_null.push_back(true);
 			}
 
@@ -107,18 +108,11 @@ struct SortingState {
 
 class OrderGlobalState : public GlobalOperatorState {
 public:
-	OrderGlobalState(SortingState sorting_state, RowLayout payload_layout)
-	    : sorting_state(move(sorting_state)), payload_layout(move(payload_layout)), total_count(0),
-	      sorting_heap_capacity(Storage::BLOCK_SIZE), payload_heap_capacity(Storage::BLOCK_SIZE), external(false) {
-		auto thinnest_row = MinValue(sorting_state.entry_size, payload_layout.GetRowWidth());
-		if (!sorting_state.all_constant) {
-			thinnest_row = MinValue(thinnest_row, sorting_state.blob_layout.GetRowWidth());
-		}
-		block_capacity = (Storage::BLOCK_SIZE + thinnest_row - 1) / thinnest_row;
+	OrderGlobalState(PhysicalOrder &order, RowLayout payload_layout)
+	    : sorting_state(SortingState(order.orders, order.statistics)), payload_layout(move(payload_layout)),
+	      total_count(0), external(false) {
 	}
 
-	~OrderGlobalState() override;
-
 	//! The lock for updating the order global state
 	std::mutex lock;
 	//! Constants concerning sorting and payload data
@@ -137,12 +131,12 @@ class OrderGlobalState : public GlobalOperatorState {
 	idx_t total_count;
 	//! Capacity (number of rows) used to initialize blocks
 	idx_t block_capacity;
-	//! Capacity (number of bytes) used to initialize blocks
-	idx_t sorting_heap_capacity;
-	idx_t payload_heap_capacity;
 
 	//! Whether we are doing an external sort
 	bool external;
+	//! Memory usage per thread
+	idx_t memory_per_thread;
+
 	//! Progress in merge path stage
 	idx_t pair_idx;
 	idx_t l_start;
@@ -154,67 +148,55 @@ class OrderLocalState : public LocalSinkState {
 	OrderLocalState() : initialized(false) {
 	}
 
-	//! Whether this local state has been initialized
-	bool initialized;
-	//! Local copy of the sorting expression executor
-	ExpressionExecutor executor;
-	//! Holds a vector of incoming sorting columns
-	DataChunk sort;
-
 	//! Initialize the local state using the global state
 	void Initialize(ClientContext &context, OrderGlobalState &gstate) {
 		auto &buffer_manager = BufferManager::GetBufferManager(context);
 		auto &sorting_state = gstate.sorting_state;
 		auto &payload_layout = gstate.payload_layout;
 		// Radix sorting data
-		idx_t vectors_per_block =
-		    (Storage::BLOCK_SIZE / sorting_state.entry_size + STANDARD_VECTOR_SIZE) / STANDARD_VECTOR_SIZE;
-		radix_sorting_data = make_unique<RowDataCollection>(buffer_manager, vectors_per_block * STANDARD_VECTOR_SIZE,
+		radix_sorting_data = make_unique<RowDataCollection>(buffer_manager, EntriesPerBlock(sorting_state.entry_size),
 		                                                    sorting_state.entry_size);
 		// Blob sorting data
 		if (!sorting_state.all_constant) {
 			auto blob_row_width = sorting_state.blob_layout.GetRowWidth();
-			vectors_per_block = (Storage::BLOCK_SIZE / blob_row_width + STANDARD_VECTOR_SIZE) / STANDARD_VECTOR_SIZE;
-			blob_sorting_data = make_unique<RowDataCollection>(buffer_manager, vectors_per_block * STANDARD_VECTOR_SIZE,
-			                                                   blob_row_width);
+			blob_sorting_data =
+			    make_unique<RowDataCollection>(buffer_manager, EntriesPerBlock(blob_row_width), blob_row_width);
 			blob_sorting_heap = make_unique<RowDataCollection>(buffer_manager, (idx_t)Storage::BLOCK_SIZE, 1, true);
 		}
 		// Payload data
 		auto payload_row_width = payload_layout.GetRowWidth();
-		vectors_per_block = (Storage::BLOCK_SIZE / payload_row_width + STANDARD_VECTOR_SIZE) / STANDARD_VECTOR_SIZE;
 		payload_data =
-		    make_unique<RowDataCollection>(buffer_manager, vectors_per_block * STANDARD_VECTOR_SIZE, payload_row_width);
+		    make_unique<RowDataCollection>(buffer_manager, EntriesPerBlock(payload_row_width), payload_row_width);
 		payload_heap = make_unique<RowDataCollection>(buffer_manager, (idx_t)Storage::BLOCK_SIZE, 1, true);
 		// Init done
 		initialized = true;
 	}
 
 	//! Whether the localstate has collected enough data to perform an external sort
-	bool Full(ClientContext &context, const SortingState &sorting_state, const RowLayout &payload_layout) {
-		// Compute the size of the collected data (in bytes)
-		idx_t size_in_bytes = radix_sorting_data->count * sorting_state.entry_size;
+	bool Full(idx_t memory_per_thread, const SortingState &sorting_state, const RowLayout &payload_layout) {
+		idx_t size_in_bytes = radix_sorting_data->SizeInBytes() + payload_data->SizeInBytes();
 		if (!sorting_state.all_constant) {
-			size_in_bytes += blob_sorting_data->count * sorting_state.blob_layout.GetRowWidth();
-			for (auto &block : blob_sorting_heap->blocks) {
-				size_in_bytes += block.byte_offset;
-			}
+			size_in_bytes += blob_sorting_data->SizeInBytes() + blob_sorting_heap->SizeInBytes();
 		}
-		size_in_bytes += payload_data->count * payload_layout.GetRowWidth();
 		if (!payload_layout.AllConstant()) {
-			for (auto &block : payload_data->blocks) {
-				size_in_bytes += block.byte_offset;
-			}
+			size_in_bytes += payload_heap->SizeInBytes();
 		}
-		// Get the max memory and number of threads
-		auto &buffer_manager = BufferManager::GetBufferManager(context);
-		auto &task_scheduler = TaskScheduler::GetScheduler(context);
-		idx_t max_memory = buffer_manager.GetMaxMemory();
-		idx_t num_threads = task_scheduler.NumberOfThreads();
-		// Memory usage per thread should scale with max mem / num threads
-		// We take 15% of the max memory, to be VERY conservative
-		return size_in_bytes > (0.15 * max_memory / num_threads);
+		return size_in_bytes >= memory_per_thread;
 	}
 
+private:
+	idx_t EntriesPerBlock(idx_t width) {
+		return (Storage::BLOCK_SIZE + width * STANDARD_VECTOR_SIZE - 1) / width;
+	}
+
+public:
+	//! Whether this local state has been initialized
+	bool initialized;
+	//! Local copy of the sorting expression executor
+	ExpressionExecutor executor;
+	//! Holds a vector of incoming sorting columns
+	DataChunk sort;
+
 	//! Radix/memcmp sortable data
 	unique_ptr<RowDataCollection> radix_sorting_data;
 	//! Variable sized sorting data and accompanying heap
@@ -234,7 +216,12 @@ class OrderLocalState : public LocalSinkState {
 unique_ptr<GlobalOperatorState> PhysicalOrder::GetGlobalState(ClientContext &context) {
 	RowLayout payload_layout;
 	payload_layout.Initialize(types, false);
-	auto state = make_unique<OrderGlobalState>(SortingState(orders), payload_layout);
+	auto state = make_unique<OrderGlobalState>(*this, payload_layout);
+	// Memory usage per thread should scale with max mem / num threads
+	// We take 1/6th of this, to be conservative
+	idx_t max_memory = BufferManager::GetBufferManager(context).GetMaxMemory();
+	idx_t num_threads = TaskScheduler::GetScheduler(context).NumberOfThreads();
+	state->memory_per_thread = (max_memory / num_threads) / 6;
 	state->external = context.force_external;
 	return move(state);
 }
@@ -300,7 +287,7 @@ void PhysicalOrder::Sink(ExecutionContext &context, GlobalOperatorState &gstate_
 	                       lstate.sel_ptr, input.size());
 
 	// When sorting data reaches a certain size, we sort it
-	if (lstate.Full(context.client, sorting_state, payload_layout)) {
+	if (lstate.Full(gstate.memory_per_thread, sorting_state, payload_layout)) {
 		SortLocalState(context.client, lstate, gstate);
 	}
 }
@@ -371,9 +358,11 @@ struct SortedData {
 	}
 	//! Initialize new block to write to
 	void CreateBlock() {
-		data_blocks.emplace_back(buffer_manager, state.block_capacity, layout.GetRowWidth());
+		auto capacity = MaxValue(((idx_t)Storage::BLOCK_SIZE + layout.GetRowWidth() - 1) / layout.GetRowWidth(),
+		                         state.block_capacity);
+		data_blocks.emplace_back(buffer_manager, capacity, layout.GetRowWidth());
 		if (!layout.AllConstant() && state.external) {
-			heap_blocks.emplace_back(buffer_manager, heap_capacity, 1);
+			heap_blocks.emplace_back(buffer_manager, (idx_t)Storage::BLOCK_SIZE, 1);
 			D_ASSERT(data_blocks.size() == heap_blocks.size());
 		}
 	}
@@ -429,8 +418,6 @@ struct SortedData {
 	//! Data and heap blocks
 	vector<RowDataBlock> data_blocks;
 	vector<RowDataBlock> heap_blocks;
-	//! Capacity (in bytes) of the heap blocks
-	idx_t heap_capacity;
 	//! Read indices
 	idx_t block_idx;
 	idx_t entry_idx;
@@ -471,7 +458,7 @@ struct SortedBlock {
 		payload_data = make_unique<SortedData>(payload_layout, buffer_manager, state);
 	}
 	//! Number of rows that this object holds
-	idx_t Count() {
+	idx_t Count() const {
 		idx_t count = std::accumulate(radix_sorting_data.begin(), radix_sorting_data.end(), 0,
 		                              [](idx_t a, const RowDataBlock &b) { return a + b.count; });
 		if (!sorting_state.all_constant) {
@@ -481,7 +468,7 @@ struct SortedBlock {
 		return count;
 	}
 	//! The remaining number of rows to be read from this object
-	idx_t Remaining() {
+	idx_t Remaining() const {
 		idx_t remaining = 0;
 		if (block_idx < radix_sorting_data.size()) {
 			remaining += radix_sorting_data[block_idx].count - entry_idx;
@@ -495,44 +482,15 @@ struct SortedBlock {
 	void InitializeWrite() {
 		CreateBlock();
 		if (!sorting_state.all_constant) {
-			blob_sorting_data->heap_capacity = state.sorting_heap_capacity;
 			blob_sorting_data->CreateBlock();
 		}
-		payload_data->heap_capacity = state.payload_heap_capacity;
 		payload_data->CreateBlock();
 	}
 	//! Init new block to write to
 	void CreateBlock() {
-		radix_sorting_data.emplace_back(buffer_manager, state.block_capacity, sorting_state.entry_size);
-	}
-	//! Cleanup sorting data
-	void UnregisterSortingBlocks() {
-		for (auto &block : radix_sorting_data) {
-			buffer_manager.UnregisterBlock(block.block->BlockId(), true);
-		}
-		if (!sorting_state.all_constant) {
-			for (auto &block : blob_sorting_data->data_blocks) {
-				buffer_manager.UnregisterBlock(block.block->BlockId(), true);
-			}
-			if (state.external) {
-				for (auto &block : blob_sorting_data->heap_blocks) {
-					buffer_manager.UnregisterBlock(block.block->BlockId(), true);
-				}
-			}
-		}
-	}
-	//! Cleanup payload data
-	void UnregisterPayloadBlocks() {
-		for (auto &block : payload_data->data_blocks) {
-			buffer_manager.UnregisterBlock(block.block->BlockId(), true);
-		}
-		if (state.external) {
-			if (!payload_data->layout.AllConstant()) {
-				for (auto &block : payload_data->heap_blocks) {
-					buffer_manager.UnregisterBlock(block.block->BlockId(), true);
-				}
-			}
-		}
+		auto capacity = MaxValue(((idx_t)Storage::BLOCK_SIZE + sorting_state.entry_size - 1) / sorting_state.entry_size,
+		                         state.block_capacity);
+		radix_sorting_data.emplace_back(buffer_manager, capacity, sorting_state.entry_size);
 	}
 	//! Fill this sorted block by appending the blocks held by a vector of sorted blocks
 	void AppendSortedBlocks(vector<unique_ptr<SortedBlock>> &sorted_blocks) {
@@ -609,6 +567,7 @@ struct SortedBlock {
 		return result;
 	}
 
+	//! Size (in bytes) of the heap of this block
 	idx_t HeapSize() const {
 		idx_t result = 0;
 		if (!sorting_state.all_constant) {
@@ -623,6 +582,22 @@ struct SortedBlock {
 		}
 		return result;
 	}
+	//! Total size (in bytes) of this block
+	idx_t SizeInBytes() const {
+		idx_t bytes = 0;
+		for (idx_t i = 0; i < radix_sorting_data.size(); i++) {
+			bytes += radix_sorting_data[i].capacity * sorting_state.entry_size;
+			if (!sorting_state.all_constant) {
+				bytes += blob_sorting_data->data_blocks[i].capacity * sorting_state.blob_layout.GetRowWidth();
+				bytes += blob_sorting_data->heap_blocks[i].capacity;
+			}
+			bytes += payload_data->data_blocks[i].capacity * payload_layout.GetRowWidth();
+			if (!payload_layout.AllConstant()) {
+				bytes += payload_data->heap_blocks[i].capacity;
+			}
+		}
+		return bytes;
+	}
 
 public:
 	//! Radix/memcmp sortable data
@@ -640,24 +615,13 @@ struct SortedBlock {
 	OrderGlobalState &state;
 	const SortingState &sorting_state;
 	const RowLayout &payload_layout;
-
-	//! Handle and ptr for sorting_blocks
-	unique_ptr<BufferHandle> sorting_handle;
 };
 
-OrderGlobalState::~OrderGlobalState() {
-	std::lock_guard<mutex> glock(lock);
-	for (auto &sb : sorted_blocks) {
-		sb->UnregisterPayloadBlocks();
-	}
-	sorted_blocks.clear();
-}
-
 //! Concatenates the blocks in a RowDataCollection into a single block
 static RowDataBlock ConcatenateBlocks(BufferManager &buffer_manager, RowDataCollection &row_data) {
 	// Create block with the correct capacity
 	const idx_t &entry_size = row_data.entry_size;
-	idx_t capacity = MaxValue(Storage::BLOCK_SIZE / entry_size + 1, row_data.count);
+	idx_t capacity = MaxValue(((idx_t)Storage::BLOCK_SIZE + entry_size - 1) / entry_size, row_data.count);
 	RowDataBlock new_block(buffer_manager, capacity, entry_size);
 	new_block.count = row_data.count;
 	auto new_block_handle = buffer_manager.Pin(new_block.block);
@@ -667,7 +631,6 @@ static RowDataBlock ConcatenateBlocks(BufferManager &buffer_manager, RowDataColl
 		auto block_handle = buffer_manager.Pin(block.block);
 		memcpy(new_block_ptr, block_handle->Ptr(), block.count * entry_size);
 		new_block_ptr += block.count * entry_size;
-		buffer_manager.UnregisterBlock(block.block->BlockId(), true);
 	}
 	row_data.blocks.clear();
 	row_data.count = 0;
@@ -1027,8 +990,8 @@ static void SortTiedBlobs(BufferManager &buffer_manager, const data_ptr_t datapt
 		return;
 	}
 	// Fill pointer array for sorting
-	auto ptr_block = buffer_manager.Allocate(MaxValue((end - start) * sizeof(data_ptr_t), (idx_t)Storage::BLOCK_SIZE));
-	auto entry_ptrs = (data_ptr_t *)ptr_block->Ptr();
+	auto ptr_block = unique_ptr<data_ptr_t[]>(new data_ptr_t[end - start]);
+	auto entry_ptrs = (data_ptr_t *)ptr_block.get();
 	for (idx_t i = start; i < end; i++) {
 		entry_ptrs[i - start] = row_ptr;
 		row_ptr += sorting_state.entry_size;
@@ -1114,7 +1077,6 @@ static void ComputeTies(data_ptr_t dataptr, const idx_t &count, const idx_t &col
 		ties[i] = ties[i] && memcmp(dataptr, dataptr + sorting_state.entry_size, tie_size) == 0;
 		dataptr += sorting_state.entry_size;
 	}
-	ties[count - 1] = false;
 }
 
 //! Textbook LSD radix sort
@@ -1190,6 +1152,7 @@ static void SortInMemory(BufferManager &buffer_manager, SortedBlock &sb, const S
 	// Radix sort and break ties until no more ties, or until all columns are sorted
 	idx_t sorting_size = 0;
 	idx_t col_offset = 0;
+	unique_ptr<bool[]> ties_ptr;
 	unique_ptr<BufferHandle> ties_handle;
 	bool *ties = nullptr;
 	for (idx_t i = 0; i < sorting_state.column_count; i++) {
@@ -1202,8 +1165,8 @@ static void SortInMemory(BufferManager &buffer_manager, SortedBlock &sb, const S
 		if (!ties) {
 			// This is the first sort
 			RadixSort(buffer_manager, dataptr, count, col_offset, sorting_size, sorting_state);
-			ties_handle = buffer_manager.Allocate(MaxValue(count, (idx_t)Storage::BLOCK_SIZE));
-			ties = (bool *)ties_handle->Ptr();
+			ties_ptr = unique_ptr<bool[]>(new bool[count]);
+			ties = ties_ptr.get();
 			std::fill_n(ties, count - 1, true);
 			ties[count - 1] = false;
 		} else {
@@ -1257,7 +1220,6 @@ static void ReOrder(BufferManager &buffer_manager, SortedData &sd, data_ptr_t so
 		sorting_ptr += sorting_entry_size;
 	}
 	// Replace the unordered data block with the re-ordered data block
-	buffer_manager.UnregisterBlock(unordered_data_block.block->BlockId(), true);
 	sd.data_blocks.clear();
 	sd.data_blocks.push_back(move(ordered_data_block));
 	// Deal with the heap (if necessary)
@@ -1287,14 +1249,10 @@ static void ReOrder(BufferManager &buffer_manager, SortedData &sd, data_ptr_t so
 		RowOperations::SwizzleHeapPointer(sd.layout, ordered_data_handle->Ptr(), ordered_heap_handle->Ptr(), count);
 		// Move the re-ordered heap to the SortedData, and clear the local heap
 		sd.heap_blocks.push_back(move(ordered_heap_block));
-		for (auto &block : heap.blocks) {
-			buffer_manager.UnregisterBlock(block.block->BlockId(), true);
-		}
+		heap.pinned_blocks.clear();
+		heap.blocks.clear();
+		heap.count = 0;
 	}
-	// Reset the localstate heap
-	heap.pinned_blocks.clear();
-	heap.blocks.clear();
-	heap.count = 0;
 }
 
 //! Use the ordered sorting data to re-order the rest of the data
@@ -1324,7 +1282,7 @@ void PhysicalOrder::SortLocalState(ClientContext &context, OrderLocalState &lsta
 	auto sorting_block = ConcatenateBlocks(buffer_manager, *lstate.radix_sorting_data);
 	sb->radix_sorting_data.push_back(move(sorting_block));
 	// Variable-size sorting data
-	if (!sorting_state.blob_layout.AllConstant()) {
+	if (!gstate.sorting_state.all_constant) {
 		auto &blob_data = *lstate.blob_sorting_data;
 		auto new_block = ConcatenateBlocks(buffer_manager, blob_data);
 		sb->blob_sorting_data->data_blocks.push_back(move(new_block));
@@ -1389,6 +1347,7 @@ class PhysicalOrderMergeTask : public Task {
 			}
 		}
 		// Set up the write block
+		// Each merge task produces a SortedBlock with exactly state.block_capacity rows or less
 		result->InitializeWrite();
 		// Initialize arrays to store merge data
 		bool left_smaller[STANDARD_VECTOR_SIZE];
@@ -1433,11 +1392,6 @@ class PhysicalOrderMergeTask : public Task {
 		lock_guard<mutex> glock(state.lock);
 		parent.finished_tasks++;
 		if (parent.finished_tasks == parent.total_tasks) {
-			// Unregister processed data
-			for (auto &sb : state.sorted_blocks) {
-				sb->UnregisterSortingBlocks();
-				sb->UnregisterPayloadBlocks();
-			}
 			state.sorted_blocks.clear();
 			if (state.odd_one_out) {
 				state.sorted_blocks.push_back(move(state.odd_one_out));
@@ -1500,6 +1454,14 @@ class PhysicalOrderMergeTask : public Task {
 		D_ASSERT(l_idx < l.Count());
 		D_ASSERT(r_idx < r.Count());
 
+		// Easy comparison using the previous result (intersections must increase monotonically)
+		if (l_idx < state.l_start) {
+			return -1;
+		}
+		if (r_idx < state.r_start) {
+			return 1;
+		}
+
 		idx_t l_block_idx;
 		idx_t l_entry_idx;
 		l.GlobalToLocalIndex(l_idx, l_block_idx, l_entry_idx);
@@ -1533,6 +1495,9 @@ class PhysicalOrderMergeTask : public Task {
 		const idx_t l_count = l.Count();
 		const idx_t r_count = r.Count();
 		// Cover some edge cases
+		// Code coverage off because these edge cases cannot happen unless other code changes
+		// Edge cases have been tested extensively while developing Merge Path in a script
+		// LCOV_EXCL_START
 		if (sum >= l_count + r_count) {
 			l_idx = l_count;
 			r_idx = r_count;
@@ -1550,6 +1515,7 @@ class PhysicalOrderMergeTask : public Task {
 			l_idx = sum;
 			return;
 		}
+		// LCOV_EXCL_STOP
 		// Determine offsets for the binary search
 		const idx_t l_offset = MinValue(l_count, sum);
 		const idx_t r_offset = sum > l_count ? sum - l_count : 0;
@@ -1574,7 +1540,11 @@ class PhysicalOrderMergeTask : public Task {
 					return;
 				}
 				if (l_idx == 0 || r_idx == r_count) {
+					// This case is incredibly difficult to cover as it is dependent on parallelism randomness
+					// But it has been tested extensively during development in a script
+					// LCOV_EXCL_START
 					return;
+					// LCOV_EXCL_STOP
 				} else {
 					break;
 				}
@@ -1586,15 +1556,6 @@ class PhysicalOrderMergeTask : public Task {
 				right = middle - 1;
 			}
 		}
-		// Shift by one (if needed)
-		if (l_idx == 0) {
-			comp_res = CompareUsingGlobalIndex(l, r, l_idx, r_idx);
-			if (comp_res > 0) {
-				l_idx--;
-				r_idx++;
-			}
-			return;
-		}
 		int l_r_min1 = CompareUsingGlobalIndex(l, r, l_idx, r_idx - 1);
 		int l_min1_r = CompareUsingGlobalIndex(l, r, l_idx - 1, r_idx);
 		if (l_r_min1 > 0 && l_min1_r < 0) {
@@ -1753,13 +1714,6 @@ class PhysicalOrderMergeTask : public Task {
 			}
 			const idx_t &l_count = !l_done ? l_block->count : 0;
 			const idx_t &r_count = !r_done ? r_block->count : 0;
-			// Create new result block (if needed)
-			if (result_block->count == result_block->capacity) {
-				result->CreateBlock();
-				result_block = &result->radix_sorting_data.back();
-				result_handle = buffer_manager.Pin(result_block->block);
-				result_ptr = result_handle->Ptr();
-			}
 			// Copy using computed merge
 			if (!l_done && !r_done) {
 				// Both sides have data - merge
@@ -1835,25 +1789,6 @@ class PhysicalOrderMergeTask : public Task {
 			}
 			const idx_t &l_count = !l_done ? l_data.data_blocks[l_data.block_idx].count : 0;
 			const idx_t &r_count = !r_done ? r_data.data_blocks[r_data.block_idx].count : 0;
-			// Create new result data block (if needed)
-			if (result_data_block->count == result_data_block->capacity) {
-				// Shrink down the last heap block to fit the data
-				if (!layout.AllConstant() && state.external &&
-				    result_heap_block->byte_offset < result_heap_block->capacity &&
-				    result_heap_block->byte_offset >= Storage::BLOCK_SIZE) {
-					buffer_manager.ReAllocate(result_heap_block->block, result_heap_block->byte_offset);
-					result_heap_block->capacity = result_heap_block->byte_offset;
-				}
-				result_data.CreateBlock();
-				result_data_block = &result_data.data_blocks.back();
-				result_data_handle = buffer_manager.Pin(result_data_block->block);
-				result_data_ptr = result_data_handle->Ptr();
-				if (!layout.AllConstant() && state.external) {
-					result_heap_block = &result_data.heap_blocks.back();
-					result_heap_handle = buffer_manager.Pin(result_heap_block->block);
-					result_heap_ptr = result_heap_handle->Ptr();
-				}
-			}
 			// Perform the merge
 			if (layout.AllConstant() || !state.external) {
 				// If all constant size, or if we are doing an in-memory sort, we do not need to touch the heap
@@ -2077,28 +2012,14 @@ bool PhysicalOrder::Finalize(Pipeline &pipeline, ClientContext &context, unique_
 	idx_t total_heap_size =
 	    std::accumulate(state.sorted_blocks.begin(), state.sorted_blocks.end(), (idx_t)0,
 	                    [](idx_t a, const unique_ptr<SortedBlock> &b) { return a + b->HeapSize(); });
-	if (total_heap_size > 0.25 * BufferManager::GetBufferManager(context).GetMaxMemory()) {
+	if (state.external || total_heap_size > 0.25 * BufferManager::GetBufferManager(context).GetMaxMemory()) {
 		state.external = true;
 	}
 	// Use the data that we have to determine which block size to use during the merge
-	const auto &sorting_state = state.sorting_state;
+	state.block_capacity = state.sorted_blocks[0]->Count();
 	for (auto &sb : state.sorted_blocks) {
-		auto &block = sb->radix_sorting_data.back();
-		state.block_capacity = MaxValue(state.block_capacity, block.capacity);
-	}
-	// Sorting heap data
-	if (!sorting_state.all_constant && state.external) {
-		for (auto &sb : state.sorted_blocks) {
-			auto &heap_block = sb->blob_sorting_data->heap_blocks.back();
-			state.sorting_heap_capacity = MaxValue(state.sorting_heap_capacity, heap_block.capacity);
-		}
-	}
-	// Payload heap data
-	const auto &payload_layout = state.payload_layout;
-	if (!payload_layout.AllConstant() && state.external) {
-		for (auto &sb : state.sorted_blocks) {
-			auto &heap_block = sb->payload_data->heap_blocks.back();
-			state.payload_heap_capacity = MaxValue(state.sorting_heap_capacity, heap_block.capacity);
+		if (sb->SizeInBytes() >= state.memory_per_thread) {
+			state.block_capacity = MinValue(state.block_capacity, sb->Count());
 		}
 	}
 	// Unswizzle and pin heap blocks if we can fit everything in memory
@@ -2114,10 +2035,6 @@ bool PhysicalOrder::Finalize(Pipeline &pipeline, ClientContext &context, unique_
 		PhysicalOrder::ScheduleMergeTasks(pipeline, context, state);
 		return false;
 	} else {
-		// Clean up sorting data - payload is sorted
-		for (auto &sb : state.sorted_blocks) {
-			sb->UnregisterSortingBlocks();
-		}
 		return true;
 	}
 }
@@ -2126,7 +2043,10 @@ void PhysicalOrder::ScheduleMergeTasks(Pipeline &pipeline, ClientContext &contex
 	D_ASSERT(state.sorted_blocks_temp.empty());
 	if (state.sorted_blocks.size() == 1) {
 		for (auto &sb : state.sorted_blocks) {
-			sb->UnregisterSortingBlocks();
+			sb->radix_sorting_data.clear();
+			if (!state.sorting_state.all_constant) {
+				sb->blob_sorting_data.reset();
+			}
 		}
 		pipeline.Finish();
 		return;
@@ -2143,6 +2063,7 @@ void PhysicalOrder::ScheduleMergeTasks(Pipeline &pipeline, ClientContext &contex
 	state.l_start = 0;
 	state.r_start = 0;
 	// Compute how many tasks there will be
+	// Each merge task produces a SortedBlock exactly state.block_capacity or less
 	idx_t num_tasks = 0;
 	const idx_t tuples_per_block = state.block_capacity;
 	for (idx_t block_idx = 0; block_idx < num_blocks; block_idx += 2) {
@@ -2241,7 +2162,6 @@ void PhysicalOrder::GetChunkInternal(ExecutionContext &context, DataChunk &chunk
 	}
 
 	if (!state.initialized) {
-		D_ASSERT(gstate.sorted_blocks.back()->Count() == gstate.total_count);
 		state.payload_data = gstate.sorted_blocks.back()->payload_data.get();
 		state.initialized = true;
 	}
diff --git a/src/execution/physical_plan/plan_order.cpp b/src/execution/physical_plan/plan_order.cpp
index 56e29833df60..2b22afcec343 100644
--- a/src/execution/physical_plan/plan_order.cpp
+++ b/src/execution/physical_plan/plan_order.cpp
@@ -9,7 +9,8 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalOrder &op)
 
 	auto plan = CreatePlan(*op.children[0]);
 	if (!op.orders.empty()) {
-		auto order = make_unique<PhysicalOrder>(op.types, move(op.orders), op.estimated_cardinality);
+		auto order =
+		    make_unique<PhysicalOrder>(op.types, move(op.orders), move(op.statistics), op.estimated_cardinality);
 		order->children.push_back(move(plan));
 		plan = move(order);
 	}
diff --git a/src/include/duckdb/common/exception.hpp b/src/include/duckdb/common/exception.hpp
index 3d592d3f3055..c850131f285a 100644
--- a/src/include/duckdb/common/exception.hpp
+++ b/src/include/duckdb/common/exception.hpp
@@ -10,8 +10,8 @@
 
 #include "duckdb/common/assert.hpp"
 #include "duckdb/common/common.hpp"
-#include "duckdb/common/vector.hpp"
 #include "duckdb/common/exception_format_value.hpp"
+#include "duckdb/common/vector.hpp"
 
 #include <stdexcept>
 
@@ -72,7 +72,8 @@ enum class ExceptionType {
 	FATAL = 30, // Fatal exception: fatal exceptions are non-recoverable, and render the entire DB in an unusable state
 	INTERNAL =
 	    31, // Internal exception: exception that indicates something went wrong internally (i.e. bug in the code base)
-	INVALID_INPUT = 32 // Input or arguments error
+	INVALID_INPUT = 32, // Input or arguments error
+	OUT_OF_MEMORY = 33  // out of memory
 };
 
 class Exception : public std::exception {
@@ -184,6 +185,16 @@ class OutOfRangeException : public Exception {
 	}
 };
 
+class OutOfMemoryException : public Exception {
+public:
+	explicit OutOfMemoryException(const string &msg);
+
+	template <typename... Args>
+	explicit OutOfMemoryException(const string &msg, Args... params)
+	    : OutOfMemoryException(ConstructMessage(msg, params...)) {
+	}
+};
+
 class SyntaxException : public Exception {
 public:
 	explicit SyntaxException(const string &msg);
diff --git a/src/include/duckdb/common/types/row_data_collection.hpp b/src/include/duckdb/common/types/row_data_collection.hpp
index 6736206ecd1e..630c4b836516 100644
--- a/src/include/duckdb/common/types/row_data_collection.hpp
+++ b/src/include/duckdb/common/types/row_data_collection.hpp
@@ -62,6 +62,19 @@ class RowDataCollection {
 
 	void Merge(RowDataCollection &other);
 
+	//! The size (in bytes) of this RowDataCollection if it were stored in a single block
+	idx_t SizeInBytes() const {
+		idx_t bytes = 0;
+		if (entry_size == 1) {
+			for (auto &block : blocks) {
+				bytes += block.byte_offset;
+			}
+		} else {
+			bytes = count * entry_size;
+		}
+		return MaxValue(bytes, (idx_t)Storage::BLOCK_SIZE);
+	}
+
 private:
 	mutex rdc_lock;
 
diff --git a/src/include/duckdb/execution/operator/order/physical_order.hpp b/src/include/duckdb/execution/operator/order/physical_order.hpp
index 95d96cf78515..c77e8ea10a87 100644
--- a/src/include/duckdb/execution/operator/order/physical_order.hpp
+++ b/src/include/duckdb/execution/operator/order/physical_order.hpp
@@ -23,10 +23,13 @@ class OrderGlobalState;
 //! Physically re-orders the input data
 class PhysicalOrder : public PhysicalSink {
 public:
-	PhysicalOrder(vector<LogicalType> types, vector<BoundOrderByNode> orders, idx_t estimated_cardinality);
+	PhysicalOrder(vector<LogicalType> types, vector<BoundOrderByNode> orders,
+	              vector<unique_ptr<BaseStatistics>> statistics, idx_t estimated_cardinality);
 
 	//! Input data
 	vector<BoundOrderByNode> orders;
+	//! Statistics of the order expressions
+	vector<unique_ptr<BaseStatistics>> statistics;
 
 public:
 	void Sink(ExecutionContext &context, GlobalOperatorState &gstate_p, LocalSinkState &lstate_p,
diff --git a/src/include/duckdb/planner/expression_binder.hpp b/src/include/duckdb/planner/expression_binder.hpp
index 44eece2124f1..737d2ece6543 100644
--- a/src/include/duckdb/planner/expression_binder.hpp
+++ b/src/include/duckdb/planner/expression_binder.hpp
@@ -105,7 +105,6 @@ class ExpressionBinder {
 	virtual void ReplaceMacroParametersRecursive(unique_ptr<ParsedExpression> &expr);
 	virtual void ReplaceMacroParametersRecursive(ParsedExpression &expr, QueryNode &node);
 	virtual void ReplaceMacroParametersRecursive(ParsedExpression &expr, TableRef &ref);
-	virtual void CheckForSideEffects(FunctionExpression &function, idx_t depth, string &error);
 
 	virtual string UnsupportedAggregateMessage();
 	virtual string UnsupportedUnnestMessage();
diff --git a/src/include/duckdb/planner/operator/logical_order.hpp b/src/include/duckdb/planner/operator/logical_order.hpp
index 07a2b8ca76db..80143c51220c 100644
--- a/src/include/duckdb/planner/operator/logical_order.hpp
+++ b/src/include/duckdb/planner/operator/logical_order.hpp
@@ -10,6 +10,7 @@
 
 #include "duckdb/planner/bound_query_node.hpp"
 #include "duckdb/planner/logical_operator.hpp"
+#include "duckdb/storage/statistics/base_statistics.hpp"
 
 namespace duckdb {
 
@@ -21,6 +22,7 @@ class LogicalOrder : public LogicalOperator {
 	}
 
 	vector<BoundOrderByNode> orders;
+	vector<unique_ptr<BaseStatistics>> statistics;
 
 	string ParamsToString() const override {
 		string result;
diff --git a/src/include/duckdb/storage/buffer/block_handle.hpp b/src/include/duckdb/storage/buffer/block_handle.hpp
index 5c331326507d..e2fccb479994 100644
--- a/src/include/duckdb/storage/buffer/block_handle.hpp
+++ b/src/include/duckdb/storage/buffer/block_handle.hpp
@@ -29,7 +29,7 @@ class BlockHandle {
 public:
 	BlockHandle(DatabaseInstance &db, block_id_t block_id);
 	BlockHandle(DatabaseInstance &db, block_id_t block_id, unique_ptr<FileBuffer> buffer, bool can_destroy,
-	            idx_t alloc_size);
+	            idx_t block_size);
 	~BlockHandle();
 
 	DatabaseInstance &db;
diff --git a/src/include/duckdb/storage/buffer_manager.hpp b/src/include/duckdb/storage/buffer_manager.hpp
index 7da09cd42264..91093f6a16f5 100644
--- a/src/include/duckdb/storage/buffer_manager.hpp
+++ b/src/include/duckdb/storage/buffer_manager.hpp
@@ -8,15 +8,14 @@
 
 #pragma once
 
-#include "duckdb/storage/buffer/buffer_handle.hpp"
-#include "duckdb/storage/buffer/managed_buffer.hpp"
-#include "duckdb/storage/block_manager.hpp"
+#include "duckdb/common/atomic.hpp"
 #include "duckdb/common/file_system.hpp"
+#include "duckdb/common/mutex.hpp"
 #include "duckdb/common/unordered_map.hpp"
+#include "duckdb/storage/block_manager.hpp"
 #include "duckdb/storage/buffer/block_handle.hpp"
-
-#include "duckdb/common/atomic.hpp"
-#include "duckdb/common/mutex.hpp"
+#include "duckdb/storage/buffer/buffer_handle.hpp"
+#include "duckdb/storage/buffer/managed_buffer.hpp"
 
 namespace duckdb {
 class DatabaseInstance;
@@ -39,14 +38,14 @@ class BufferManager {
 	//! Register an in-memory buffer of arbitrary size, as long as it is >= BLOCK_SIZE. can_destroy signifies whether or
 	//! not the buffer can be destroyed when unpinned, or whether or not it needs to be written to a temporary file so
 	//! it can be reloaded. The resulting buffer will already be allocated, but needs to be pinned in order to be used.
-	shared_ptr<BlockHandle> RegisterMemory(idx_t alloc_size, bool can_destroy);
+	shared_ptr<BlockHandle> RegisterMemory(idx_t block_size, bool can_destroy);
 
 	//! Allocate an in-memory buffer with a single pin.
 	//! The allocated memory is released when the buffer handle is destroyed.
-	unique_ptr<BufferHandle> Allocate(idx_t alloc_size);
+	unique_ptr<BufferHandle> Allocate(idx_t block_size);
 
 	//! Reallocate an in-memory buffer that is pinned.
-	void ReAllocate(shared_ptr<BlockHandle> &handle, idx_t alloc_size);
+	void ReAllocate(shared_ptr<BlockHandle> &handle, idx_t block_size);
 
 	unique_ptr<BufferHandle> Pin(shared_ptr<BlockHandle> &handle);
 	void Unpin(shared_ptr<BlockHandle> &handle);
diff --git a/src/optimizer/statistics/operator/propagate_order.cpp b/src/optimizer/statistics/operator/propagate_order.cpp
index dd8b069e6ef3..582a944db058 100644
--- a/src/optimizer/statistics/operator/propagate_order.cpp
+++ b/src/optimizer/statistics/operator/propagate_order.cpp
@@ -1,12 +1,25 @@
 #include "duckdb/optimizer/statistics_propagator.hpp"
 #include "duckdb/planner/operator/logical_order.hpp"
+#include "duckdb/storage/statistics/base_statistics.hpp"
 
 namespace duckdb {
 
 unique_ptr<NodeStatistics> StatisticsPropagator::PropagateStatistics(LogicalOrder &order,
                                                                      unique_ptr<LogicalOperator> *node_ptr) {
-	// propagate statistics in the child node
-	return PropagateStatistics(order.children[0]);
+	// first propagate to the child
+	node_stats = PropagateStatistics(order.children[0]);
+
+	// then propagate to each of the order expressions
+	for (idx_t i = 0; i < order.orders.size(); i++) {
+		auto &expr = order.orders[i].expression;
+		PropagateExpression(expr);
+		if (expr->stats) {
+			order.statistics.push_back(expr->stats->Copy());
+		} else {
+			order.statistics.push_back(nullptr);
+		}
+	}
+	return move(node_stats);
 }
 
 } // namespace duckdb
diff --git a/src/optimizer/statistics_propagator.cpp b/src/optimizer/statistics_propagator.cpp
index d9e753269cca..84880e22949d 100644
--- a/src/optimizer/statistics_propagator.cpp
+++ b/src/optimizer/statistics_propagator.cpp
@@ -1,8 +1,9 @@
 #include "duckdb/optimizer/statistics_propagator.hpp"
-#include "duckdb/planner/logical_operator.hpp"
+
+#include "duckdb/main/client_context.hpp"
 #include "duckdb/planner/expression_iterator.hpp"
+#include "duckdb/planner/logical_operator.hpp"
 #include "duckdb/planner/operator/logical_empty_result.hpp"
-#include "duckdb/main/client_context.hpp"
 
 namespace duckdb {
 
@@ -42,6 +43,8 @@ unique_ptr<NodeStatistics> StatisticsPropagator::PropagateStatistics(LogicalOper
 	case LogicalOperatorType::LOGICAL_EXCEPT:
 	case LogicalOperatorType::LOGICAL_INTERSECT:
 		return PropagateStatistics((LogicalSetOperation &)node, node_ptr);
+	case LogicalOperatorType::LOGICAL_ORDER_BY:
+		return PropagateStatistics((LogicalOrder &)node, node_ptr);
 	default:
 		return PropagateChildren(node, node_ptr);
 	}
diff --git a/src/planner/binder/expression/bind_macro_expression.cpp b/src/planner/binder/expression/bind_macro_expression.cpp
index 82f1dc0c9834..a3478b4e2314 100644
--- a/src/planner/binder/expression/bind_macro_expression.cpp
+++ b/src/planner/binder/expression/bind_macro_expression.cpp
@@ -119,24 +119,6 @@ void ExpressionBinder::ReplaceMacroParametersRecursive(ParsedExpression &expr, Q
 	}
 }
 
-void ExpressionBinder::CheckForSideEffects(FunctionExpression &function, idx_t depth, string &error) {
-	for (idx_t i = 0; i < function.children.size(); i++) {
-		auto arg_copy = function.children[i]->Copy();
-		BindChild(arg_copy, depth, error);
-		if (!error.empty()) {
-			return;
-		}
-		auto &bound_expr = (BoundExpression &)*arg_copy;
-		if (bound_expr.expr->HasSideEffects()) {
-			QueryErrorContext error_context(binder.root_statement, function.query_location);
-			error = StringUtil::Format("Arguments with side-effects are not supported ('%s()' was supplied). As a "
-			                           "workaround, try creating a CTE that evaluates the argument with side-effects.",
-			                           arg_copy->ToString());
-			return;
-		}
-	}
-}
-
 BindResult ExpressionBinder::BindMacro(FunctionExpression &function, MacroCatalogEntry *macro_func, idx_t depth,
                                        unique_ptr<ParsedExpression> *expr) {
 	auto &macro_def = *macro_func->function;
@@ -148,12 +130,6 @@ BindResult ExpressionBinder::BindMacro(FunctionExpression &function, MacroCatalo
 		return BindResult(binder.FormatError(*expr->get(), error));
 	}
 
-	// check for arguments with side-effects TODO: to support this, a projection must be pushed
-	//    CheckForSideEffects(function, depth, error);
-	//    if (!error.empty()) {
-	//        return BindResult(error);
-	//    }
-
 	// create a MacroBinding to bind this macro's parameters to its arguments
 	vector<LogicalType> types;
 	vector<string> names;
diff --git a/src/storage/buffer_manager.cpp b/src/storage/buffer_manager.cpp
index be3fd4b9a2f6..39d34b822bd7 100644
--- a/src/storage/buffer_manager.cpp
+++ b/src/storage/buffer_manager.cpp
@@ -15,12 +15,12 @@ BlockHandle::BlockHandle(DatabaseInstance &db, block_id_t block_id_p)
 }
 
 BlockHandle::BlockHandle(DatabaseInstance &db, block_id_t block_id_p, unique_ptr<FileBuffer> buffer_p,
-                         bool can_destroy_p, idx_t alloc_size)
+                         bool can_destroy_p, idx_t block_size)
     : db(db), readers(0), block_id(block_id_p), eviction_timestamp(0), can_destroy(can_destroy_p) {
-	D_ASSERT(alloc_size >= Storage::BLOCK_SIZE);
+	D_ASSERT(block_size >= Storage::BLOCK_SIZE);
 	buffer = move(buffer_p);
 	state = BlockState::BLOCK_LOADED;
-	memory_usage = alloc_size + Storage::BLOCK_HEADER_SIZE;
+	memory_usage = block_size + Storage::BLOCK_HEADER_SIZE;
 }
 
 BlockHandle::~BlockHandle() {
@@ -40,7 +40,6 @@ unique_ptr<BufferHandle> BlockHandle::Load(shared_ptr<BlockHandle> &handle) {
 		D_ASSERT(handle->buffer);
 		return make_unique<BufferHandle>(handle, handle->buffer.get());
 	}
-	handle->state = BlockState::BLOCK_LOADED;
 
 	auto &buffer_manager = BufferManager::GetBufferManager(handle->db);
 	auto &block_manager = BlockManager::GetBlockManager(handle->db);
@@ -55,6 +54,7 @@ unique_ptr<BufferHandle> BlockHandle::Load(shared_ptr<BlockHandle> &handle) {
 			handle->buffer = buffer_manager.ReadTemporaryBuffer(handle->block_id);
 		}
 	}
+	handle->state = BlockState::BLOCK_LOADED;
 	return make_unique<BufferHandle>(handle, handle->buffer.get());
 }
 
@@ -64,8 +64,7 @@ void BlockHandle::Unload() {
 		return;
 	}
 	D_ASSERT(CanUnload());
-	D_ASSERT(memory_usage >= Storage::BLOCK_SIZE);
-	state = BlockState::BLOCK_UNLOADED;
+	D_ASSERT(memory_usage >= Storage::BLOCK_ALLOC_SIZE);
 
 	auto &buffer_manager = BufferManager::GetBufferManager(db);
 	if (block_id >= MAXIMUM_BLOCK && !can_destroy) {
@@ -74,6 +73,7 @@ void BlockHandle::Unload() {
 	}
 	buffer.reset();
 	buffer_manager.current_memory -= memory_usage;
+	state = BlockState::BLOCK_UNLOADED;
 }
 
 bool BlockHandle::CanUnload() {
@@ -173,44 +173,47 @@ shared_ptr<BlockHandle> BufferManager::RegisterBlock(block_id_t block_id) {
 	return result;
 }
 
-shared_ptr<BlockHandle> BufferManager::RegisterMemory(idx_t alloc_size, bool can_destroy) {
+shared_ptr<BlockHandle> BufferManager::RegisterMemory(idx_t block_size, bool can_destroy) {
+	auto alloc_size = block_size + Storage::BLOCK_HEADER_SIZE;
 	// first evict blocks until we have enough memory to store this buffer
-	if (!EvictBlocks(alloc_size + Storage::BLOCK_HEADER_SIZE, maximum_memory)) {
-		throw OutOfRangeException("Not enough memory to complete operation: could not allocate block of %lld bytes",
-		                          alloc_size);
+	if (!EvictBlocks(alloc_size, maximum_memory)) {
+		throw OutOfMemoryException("could not allocate block of %lld bytes", alloc_size);
 	}
 
 	// allocate the buffer
 	auto temp_id = ++temporary_id;
-	auto buffer = make_unique<ManagedBuffer>(db, alloc_size, can_destroy, temp_id);
+	auto buffer = make_unique<ManagedBuffer>(db, block_size, can_destroy, temp_id);
 
 	// create a new block pointer for this block
-	return make_shared<BlockHandle>(db, temp_id, move(buffer), can_destroy, alloc_size);
+	auto result = make_shared<BlockHandle>(db, temp_id, move(buffer), can_destroy, block_size);
+	return result;
 }
 
-unique_ptr<BufferHandle> BufferManager::Allocate(idx_t alloc_size) {
-	auto block = RegisterMemory(alloc_size, true);
+unique_ptr<BufferHandle> BufferManager::Allocate(idx_t block_size) {
+	auto block = RegisterMemory(block_size, true);
 	return Pin(block);
 }
 
-void BufferManager::ReAllocate(shared_ptr<BlockHandle> &handle, idx_t alloc_size) {
-	D_ASSERT(alloc_size >= Storage::BLOCK_SIZE);
+void BufferManager::ReAllocate(shared_ptr<BlockHandle> &handle, idx_t block_size) {
+	D_ASSERT(block_size >= Storage::BLOCK_SIZE);
 	lock_guard<mutex> lock(handle->lock);
-	D_ASSERT(handle->readers == 1);
-	auto total_size = alloc_size + Storage::BLOCK_HEADER_SIZE;
-	int64_t required_memory = total_size - handle->memory_usage;
-	if (required_memory > 0) {
-		// evict blocks until we have space to increase the size of this block
+	D_ASSERT(handle->state == BlockState::BLOCK_LOADED);
+	auto alloc_size = block_size + Storage::BLOCK_HEADER_SIZE;
+	int64_t required_memory = alloc_size - handle->memory_usage;
+	if (required_memory == 0) {
+		return;
+	} else if (required_memory > 0) {
+		// evict blocks until we have space to resize this block
 		if (!EvictBlocks(required_memory, maximum_memory)) {
-			throw OutOfRangeException("Not enough memory to complete operation: failed to increase block size");
+			throw OutOfMemoryException("failed to resize block from %lld to %lld", handle->memory_usage, alloc_size);
 		}
+	} else {
+		// no need to evict blocks
+		current_memory -= idx_t(-required_memory);
 	}
-	// re-allocate the buffer size and update its memory usage
-	handle->buffer->Resize(alloc_size);
-	if (required_memory < 0) {
-		current_memory += required_memory;
-	}
-	handle->memory_usage = total_size;
+	// resize and adjust current memory
+	handle->buffer->Resize(block_size);
+	handle->memory_usage = alloc_size;
 }
 
 unique_ptr<BufferHandle> BufferManager::Pin(shared_ptr<BlockHandle> &handle) {
@@ -228,7 +231,7 @@ unique_ptr<BufferHandle> BufferManager::Pin(shared_ptr<BlockHandle> &handle) {
 	}
 	// evict blocks until we have space for the current block
 	if (!EvictBlocks(required_memory, maximum_memory)) {
-		throw OutOfRangeException("Not enough memory to complete operation: failed to pin block");
+		throw OutOfMemoryException("failed to pin block of size %lld", required_memory);
 	}
 	// lock the handle again and repeat the check (in case anybody loaded in the mean time)
 	lock_guard<mutex> lock(handle->lock);
@@ -236,6 +239,7 @@ unique_ptr<BufferHandle> BufferManager::Pin(shared_ptr<BlockHandle> &handle) {
 	if (handle->state == BlockState::BLOCK_LOADED) {
 		// the block is loaded, increment the reader count and return a pointer to the handle
 		handle->readers++;
+		current_memory -= required_memory;
 		return handle->Load(handle);
 	}
 	// now we can actually load the current block
@@ -303,9 +307,8 @@ void BufferManager::SetLimit(idx_t limit) {
 	lock_guard<mutex> buffer_lock(manager_lock);
 	// try to evict until the limit is reached
 	if (!EvictBlocks(0, limit)) {
-		throw OutOfRangeException(
-		    "Failed to change memory limit to new limit %lld: could not free up enough memory for the new limit",
-		    limit);
+		throw OutOfMemoryException(
+		    "Failed to change memory limit to %lld: could not free up enough memory for the new limit", limit);
 	}
 	idx_t old_limit = maximum_memory;
 	// set the global maximum memory to the new limit if successful
@@ -314,9 +317,8 @@ void BufferManager::SetLimit(idx_t limit) {
 	if (!EvictBlocks(0, limit)) {
 		// failed: go back to old limit
 		maximum_memory = old_limit;
-		throw OutOfRangeException(
-		    "Failed to change memory limit to new limit %lld: could not free up enough memory for the new limit",
-		    limit);
+		throw OutOfMemoryException(
+		    "Failed to change memory limit to %lld: could not free up enough memory for the new limit", limit);
 	}
 }
 
@@ -354,16 +356,19 @@ void BufferManager::WriteTemporaryBuffer(ManagedBuffer &buffer) {
 unique_ptr<FileBuffer> BufferManager::ReadTemporaryBuffer(block_id_t id) {
 	D_ASSERT(!temp_directory.empty());
 	D_ASSERT(temp_directory_handle.get());
-	idx_t alloc_size;
+	idx_t block_size;
 	// open the temporary file and read the size
 	auto path = GetTemporaryPath(id);
 	auto &fs = FileSystem::GetFileSystem(db);
 	auto handle = fs.OpenFile(path, FileFlags::FILE_FLAGS_READ);
-	handle->Read(&alloc_size, sizeof(idx_t), 0);
+	handle->Read(&block_size, sizeof(idx_t), 0);
 
 	// now allocate a buffer of this size and read the data into that buffer
-	auto buffer = make_unique<ManagedBuffer>(db, alloc_size, false, id);
+	auto buffer = make_unique<ManagedBuffer>(db, block_size, false, id);
 	buffer->Read(*handle, sizeof(idx_t));
+
+	handle.reset();
+	DeleteTemporaryFile(id);
 	return move(buffer);
 }
 
