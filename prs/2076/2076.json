{
  "repo": "duckdb/duckdb",
  "pull_number": 2076,
  "instance_id": "duckdb__duckdb-2076",
  "issue_numbers": [
    "2035"
  ],
  "base_commit": "5e6888344eefa5371e52e1cf15a612e4a39facc8",
  "patch": "diff --git a/src/common/exception.cpp b/src/common/exception.cpp\nindex 11856e46d3de..965b2b18852c 100644\n--- a/src/common/exception.cpp\n+++ b/src/common/exception.cpp\n@@ -1,7 +1,8 @@\n #include \"duckdb/common/exception.hpp\"\n+\n #include \"duckdb/common/string_util.hpp\"\n-#include \"duckdb/common/types.hpp\"\n #include \"duckdb/common/to_string.hpp\"\n+#include \"duckdb/common/types.hpp\"\n \n namespace duckdb {\n \n@@ -87,6 +88,8 @@ string Exception::ExceptionTypeToString(ExceptionType type) {\n \t\treturn \"INTERNAL\";\n \tcase ExceptionType::INVALID_INPUT:\n \t\treturn \"Invalid Input\";\n+\tcase ExceptionType::OUT_OF_MEMORY:\n+\t\treturn \"Out of Memory\";\n \tdefault:\n \t\treturn \"Unknown\";\n \t}\n@@ -198,4 +201,7 @@ InternalException::InternalException(const string &msg) : Exception(ExceptionTyp\n InvalidInputException::InvalidInputException(const string &msg) : Exception(ExceptionType::INVALID_INPUT, msg) {\n }\n \n+OutOfMemoryException::OutOfMemoryException(const string &msg) : Exception(ExceptionType::OUT_OF_MEMORY, msg) {\n+}\n+\n } // namespace duckdb\ndiff --git a/src/common/row_operations/row_heap_scatter.cpp b/src/common/row_operations/row_heap_scatter.cpp\nindex 0dafa9e9247b..ac43c32b1727 100644\n--- a/src/common/row_operations/row_heap_scatter.cpp\n+++ b/src/common/row_operations/row_heap_scatter.cpp\n@@ -114,8 +114,10 @@ void RowOperations::ComputeEntrySizes(Vector &v, VectorData &vdata, idx_t entry_\n \t\t\tComputeListEntrySizes(v, vdata, entry_sizes, ser_count, sel, offset);\n \t\t\tbreak;\n \t\tdefault:\n+\t\t\t// LCOV_EXCL_START\n \t\t\tthrow NotImplementedException(\"Column with variable size type %s cannot be serialized to row-format\",\n \t\t\t                              v.GetType().ToString());\n+\t\t\t// LCOV_EXCL_STOP\n \t\t}\n \t}\n }\n@@ -381,8 +383,10 @@ void RowOperations::HeapScatter(Vector &v, idx_t vcount, const SelectionVector &\n \t\t\tHeapScatterListVector(v, vcount, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);\n \t\t\tbreak;\n \t\tdefault:\n+\t\t\t// LCOV_EXCL_START\n \t\t\tthrow NotImplementedException(\"Serialization of variable length vector with type %s\",\n \t\t\t                              v.GetType().ToString());\n+\t\t\t// LCOV_EXCL_STOP\n \t\t}\n \t}\n }\n@@ -429,7 +433,7 @@ void RowOperations::HeapScatterVData(VectorData &vdata, PhysicalType type, const\n \t\tTemplatedHeapScatter<interval_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);\n \t\tbreak;\n \tdefault:\n-\t\tthrow NotImplementedException(\"FIXME: unimplemented serialize to of constant type column to row-format\");\n+\t\tthrow NotImplementedException(\"FIXME: Serialize to of constant type column to row-format\");\n \t}\n }\n \ndiff --git a/src/common/row_operations/row_radix_scatter.cpp b/src/common/row_operations/row_radix_scatter.cpp\nindex bd29d5bfe623..411150d66a0c 100644\n--- a/src/common/row_operations/row_radix_scatter.cpp\n+++ b/src/common/row_operations/row_radix_scatter.cpp\n@@ -123,7 +123,7 @@ void RadixScatterListVector(Vector &v, VectorData &vdata, const SelectionVector\n \t\t\t\t\tkey_locations[i][0] = 1;\n \t\t\t\t\tkey_locations[i]++;\n \t\t\t\t\tRowOperations::RadixScatter(child_vector, list_size, FlatVector::INCREMENTAL_SELECTION_VECTOR, 1,\n-\t\t\t\t\t                            key_locations + i, false, has_null, false, prefix_len, width - 1,\n+\t\t\t\t\t                            key_locations + i, false, true, false, prefix_len, width - 1,\n \t\t\t\t\t                            list_entry.offset);\n \t\t\t\t} else {\n \t\t\t\t\t// denote that the list is empty with a 0\n@@ -154,7 +154,7 @@ void RadixScatterListVector(Vector &v, VectorData &vdata, const SelectionVector\n \t\t\t\tkey_locations[i][0] = 1;\n \t\t\t\tkey_locations[i]++;\n \t\t\t\tRowOperations::RadixScatter(child_vector, list_size, FlatVector::INCREMENTAL_SELECTION_VECTOR, 1,\n-\t\t\t\t                            key_locations + i, false, has_null, false, prefix_len, width - 1,\n+\t\t\t\t                            key_locations + i, false, true, false, prefix_len, width - 1,\n \t\t\t\t                            list_entry.offset);\n \t\t\t} else {\n \t\t\t\t// denote that the list is empty with a 0\n@@ -197,7 +197,7 @@ void RadixScatterStructVector(Vector &v, VectorData &vdata, idx_t vcount, const\n \t// serialize the struct\n \tauto &child_vector = *StructVector::GetEntries(v)[0];\n \tRowOperations::RadixScatter(child_vector, vcount, FlatVector::INCREMENTAL_SELECTION_VECTOR, add_count,\n-\t                            key_locations, false, has_null, false, prefix_len, width, offset);\n+\t                            key_locations, false, true, false, prefix_len, width, offset);\n \t// invert bits if desc\n \tif (desc) {\n \t\tfor (idx_t i = 0; i < add_count; i++) {\ndiff --git a/src/common/row_operations/row_scatter.cpp b/src/common/row_operations/row_scatter.cpp\nindex 388ca5310c50..18ccfc88dafa 100644\n--- a/src/common/row_operations/row_scatter.cpp\n+++ b/src/common/row_operations/row_scatter.cpp\n@@ -123,6 +123,7 @@ void RowOperations::Scatter(DataChunk &columns, VectorData col_data[], const Row\n \tauto &types = layout.GetTypes();\n \n \t// Compute the entry size of the variable size columns\n+\tvector<unique_ptr<BufferHandle>> handles;\n \tdata_ptr_t data_locations[STANDARD_VECTOR_SIZE];\n \tif (!layout.AllConstant()) {\n \t\tidx_t entry_sizes[STANDARD_VECTOR_SIZE];\n@@ -144,7 +145,7 @@ void RowOperations::Scatter(DataChunk &columns, VectorData col_data[], const Row\n \t\t\t\tRowOperations::ComputeEntrySizes(vec, col, entry_sizes, vcount, count, sel);\n \t\t\t\tbreak;\n \t\t\tdefault:\n-\t\t\t\tthrow Exception(\"Unsupported type for RowOperations::Scatter\");\n+\t\t\t\tthrow InternalException(\"Unsupported type for RowOperations::Scatter\");\n \t\t\t}\n \t\t}\n \ndiff --git a/src/common/types/row_data_collection.cpp b/src/common/types/row_data_collection.cpp\nindex 8f9d41d325b8..40f2323e0cb4 100644\n--- a/src/common/types/row_data_collection.cpp\n+++ b/src/common/types/row_data_collection.cpp\n@@ -18,13 +18,19 @@ idx_t RowDataCollection::AppendToBlock(RowDataBlock &block, BufferHandle &handle\n \tidx_t append_count = 0;\n \tdata_ptr_t dataptr;\n \tif (entry_sizes) {\n-\t\t// compute how many entries fit if entry size if variable\n-\t\tdataptr = handle.node->buffer + block.byte_offset;\n+\t\tD_ASSERT(entry_size == 1);\n+\t\t// compute how many entries fit if entry size is variable\n+\t\tdataptr = handle.Ptr() + block.byte_offset;\n \t\tfor (idx_t i = 0; i < remaining; i++) {\n-\t\t\tif (block.byte_offset + entry_sizes[i] > block_capacity * entry_size) {\n-\t\t\t\twhile (entry_sizes[i] > block_capacity * entry_size) {\n-\t\t\t\t\t// if an entry does not fit, increase capacity until it does\n-\t\t\t\t\tblock_capacity *= 2;\n+\t\t\tif (block.byte_offset + entry_sizes[i] > block.capacity) {\n+\t\t\t\tif (block.count == 0 && append_count == 0 && entry_sizes[i] > block.capacity) {\n+\t\t\t\t\t// special case: single entry is bigger than block capacity\n+\t\t\t\t\t// resize current block to fit the entry, append it, and move to the next block\n+\t\t\t\t\tblock.capacity = entry_sizes[i];\n+\t\t\t\t\tbuffer_manager.ReAllocate(block.block, block.capacity);\n+\t\t\t\t\tdataptr = handle.Ptr();\n+\t\t\t\t\tappend_count++;\n+\t\t\t\t\tblock.byte_offset += entry_sizes[i];\n \t\t\t\t}\n \t\t\t\tbreak;\n \t\t\t}\n@@ -33,7 +39,7 @@ idx_t RowDataCollection::AppendToBlock(RowDataBlock &block, BufferHandle &handle\n \t\t}\n \t} else {\n \t\tappend_count = MinValue<idx_t>(remaining, block.capacity - block.count);\n-\t\tdataptr = handle.node->buffer + block.count * entry_size;\n+\t\tdataptr = handle.Ptr() + block.count * entry_size;\n \t}\n \tappend_entries.emplace_back(dataptr, append_count);\n \tblock.count += append_count;\n@@ -72,16 +78,14 @@ vector<unique_ptr<BufferHandle>> RowDataCollection::Build(idx_t added_count, dat\n \t\t\tidx_t *offset_entry_sizes = entry_sizes ? entry_sizes + added_count - remaining : nullptr;\n \n \t\t\tidx_t append_count = AppendToBlock(new_block, *handle, append_entries, remaining, offset_entry_sizes);\n+\t\t\tD_ASSERT(new_block.count > 0);\n \t\t\tremaining -= append_count;\n \n-\t\t\tif (new_block.count > 0) {\n-\t\t\t\t// in case 0 tuples fit the block (huge entry, e.g. large string) we do not add\n-\t\t\t\tblocks.push_back(move(new_block));\n-\t\t\t\tif (keep_pinned) {\n-\t\t\t\t\tpinned_blocks.push_back(move(handle));\n-\t\t\t\t} else {\n-\t\t\t\t\thandles.push_back(move(handle));\n-\t\t\t\t}\n+\t\t\tblocks.push_back(move(new_block));\n+\t\t\tif (keep_pinned) {\n+\t\t\t\tpinned_blocks.push_back(move(handle));\n+\t\t\t} else {\n+\t\t\t\thandles.push_back(move(handle));\n \t\t\t}\n \t\t}\n \t}\ndiff --git a/src/execution/operator/order/physical_order.cpp b/src/execution/operator/order/physical_order.cpp\nindex d832744886ec..ab0f02df6552 100644\n--- a/src/execution/operator/order/physical_order.cpp\n+++ b/src/execution/operator/order/physical_order.cpp\n@@ -18,8 +18,10 @@ namespace duckdb {\n \n using ValidityBytes = RowLayout::ValidityBytes;\n \n-PhysicalOrder::PhysicalOrder(vector<LogicalType> types, vector<BoundOrderByNode> orders, idx_t estimated_cardinality)\n-    : PhysicalSink(PhysicalOperatorType::ORDER_BY, move(types), estimated_cardinality), orders(move(orders)) {\n+PhysicalOrder::PhysicalOrder(vector<LogicalType> types, vector<BoundOrderByNode> orders,\n+                             vector<unique_ptr<BaseStatistics>> statistics, idx_t estimated_cardinality)\n+    : PhysicalSink(PhysicalOperatorType::ORDER_BY, move(types), estimated_cardinality), orders(move(orders)),\n+      statistics(move(statistics)) {\n }\n \n //===--------------------------------------------------------------------===//\n@@ -47,7 +49,7 @@ static idx_t GetSortingColSize(const LogicalType &type) {\n }\n \n struct SortingState {\n-\texplicit SortingState(const vector<BoundOrderByNode> &orders)\n+\tSortingState(const vector<BoundOrderByNode> &orders, const vector<unique_ptr<BaseStatistics>> &statistics)\n \t    : column_count(orders.size()), all_constant(true), comparison_size(0), entry_size(0) {\n \t\tvector<LogicalType> blob_layout_types;\n \t\tfor (idx_t i = 0; i < orders.size(); i++) {\n@@ -64,12 +66,11 @@ struct SortingState {\n \t\t\tcolumn_sizes.push_back(0);\n \t\t\tauto &col_size = column_sizes.back();\n \n-\t\t\tif (expr.stats) {\n-\t\t\t\tstats.push_back(expr.stats.get());\n+\t\t\tif (!statistics.empty() && statistics[i]) {\n+\t\t\t\tstats.push_back(statistics[i].get());\n \t\t\t\thas_null.push_back(stats.back()->CanHaveNull());\n \t\t\t} else {\n \t\t\t\tstats.push_back(nullptr);\n-\t\t\t\t// No stats - we must assume that there are nulls\n \t\t\t\thas_null.push_back(true);\n \t\t\t}\n \n@@ -107,18 +108,11 @@ struct SortingState {\n \n class OrderGlobalState : public GlobalOperatorState {\n public:\n-\tOrderGlobalState(SortingState sorting_state, RowLayout payload_layout)\n-\t    : sorting_state(move(sorting_state)), payload_layout(move(payload_layout)), total_count(0),\n-\t      sorting_heap_capacity(Storage::BLOCK_SIZE), payload_heap_capacity(Storage::BLOCK_SIZE), external(false) {\n-\t\tauto thinnest_row = MinValue(sorting_state.entry_size, payload_layout.GetRowWidth());\n-\t\tif (!sorting_state.all_constant) {\n-\t\t\tthinnest_row = MinValue(thinnest_row, sorting_state.blob_layout.GetRowWidth());\n-\t\t}\n-\t\tblock_capacity = (Storage::BLOCK_SIZE + thinnest_row - 1) / thinnest_row;\n+\tOrderGlobalState(PhysicalOrder &order, RowLayout payload_layout)\n+\t    : sorting_state(SortingState(order.orders, order.statistics)), payload_layout(move(payload_layout)),\n+\t      total_count(0), external(false) {\n \t}\n \n-\t~OrderGlobalState() override;\n-\n \t//! The lock for updating the order global state\n \tstd::mutex lock;\n \t//! Constants concerning sorting and payload data\n@@ -137,12 +131,12 @@ class OrderGlobalState : public GlobalOperatorState {\n \tidx_t total_count;\n \t//! Capacity (number of rows) used to initialize blocks\n \tidx_t block_capacity;\n-\t//! Capacity (number of bytes) used to initialize blocks\n-\tidx_t sorting_heap_capacity;\n-\tidx_t payload_heap_capacity;\n \n \t//! Whether we are doing an external sort\n \tbool external;\n+\t//! Memory usage per thread\n+\tidx_t memory_per_thread;\n+\n \t//! Progress in merge path stage\n \tidx_t pair_idx;\n \tidx_t l_start;\n@@ -154,67 +148,55 @@ class OrderLocalState : public LocalSinkState {\n \tOrderLocalState() : initialized(false) {\n \t}\n \n-\t//! Whether this local state has been initialized\n-\tbool initialized;\n-\t//! Local copy of the sorting expression executor\n-\tExpressionExecutor executor;\n-\t//! Holds a vector of incoming sorting columns\n-\tDataChunk sort;\n-\n \t//! Initialize the local state using the global state\n \tvoid Initialize(ClientContext &context, OrderGlobalState &gstate) {\n \t\tauto &buffer_manager = BufferManager::GetBufferManager(context);\n \t\tauto &sorting_state = gstate.sorting_state;\n \t\tauto &payload_layout = gstate.payload_layout;\n \t\t// Radix sorting data\n-\t\tidx_t vectors_per_block =\n-\t\t    (Storage::BLOCK_SIZE / sorting_state.entry_size + STANDARD_VECTOR_SIZE) / STANDARD_VECTOR_SIZE;\n-\t\tradix_sorting_data = make_unique<RowDataCollection>(buffer_manager, vectors_per_block * STANDARD_VECTOR_SIZE,\n+\t\tradix_sorting_data = make_unique<RowDataCollection>(buffer_manager, EntriesPerBlock(sorting_state.entry_size),\n \t\t                                                    sorting_state.entry_size);\n \t\t// Blob sorting data\n \t\tif (!sorting_state.all_constant) {\n \t\t\tauto blob_row_width = sorting_state.blob_layout.GetRowWidth();\n-\t\t\tvectors_per_block = (Storage::BLOCK_SIZE / blob_row_width + STANDARD_VECTOR_SIZE) / STANDARD_VECTOR_SIZE;\n-\t\t\tblob_sorting_data = make_unique<RowDataCollection>(buffer_manager, vectors_per_block * STANDARD_VECTOR_SIZE,\n-\t\t\t                                                   blob_row_width);\n+\t\t\tblob_sorting_data =\n+\t\t\t    make_unique<RowDataCollection>(buffer_manager, EntriesPerBlock(blob_row_width), blob_row_width);\n \t\t\tblob_sorting_heap = make_unique<RowDataCollection>(buffer_manager, (idx_t)Storage::BLOCK_SIZE, 1, true);\n \t\t}\n \t\t// Payload data\n \t\tauto payload_row_width = payload_layout.GetRowWidth();\n-\t\tvectors_per_block = (Storage::BLOCK_SIZE / payload_row_width + STANDARD_VECTOR_SIZE) / STANDARD_VECTOR_SIZE;\n \t\tpayload_data =\n-\t\t    make_unique<RowDataCollection>(buffer_manager, vectors_per_block * STANDARD_VECTOR_SIZE, payload_row_width);\n+\t\t    make_unique<RowDataCollection>(buffer_manager, EntriesPerBlock(payload_row_width), payload_row_width);\n \t\tpayload_heap = make_unique<RowDataCollection>(buffer_manager, (idx_t)Storage::BLOCK_SIZE, 1, true);\n \t\t// Init done\n \t\tinitialized = true;\n \t}\n \n \t//! Whether the localstate has collected enough data to perform an external sort\n-\tbool Full(ClientContext &context, const SortingState &sorting_state, const RowLayout &payload_layout) {\n-\t\t// Compute the size of the collected data (in bytes)\n-\t\tidx_t size_in_bytes = radix_sorting_data->count * sorting_state.entry_size;\n+\tbool Full(idx_t memory_per_thread, const SortingState &sorting_state, const RowLayout &payload_layout) {\n+\t\tidx_t size_in_bytes = radix_sorting_data->SizeInBytes() + payload_data->SizeInBytes();\n \t\tif (!sorting_state.all_constant) {\n-\t\t\tsize_in_bytes += blob_sorting_data->count * sorting_state.blob_layout.GetRowWidth();\n-\t\t\tfor (auto &block : blob_sorting_heap->blocks) {\n-\t\t\t\tsize_in_bytes += block.byte_offset;\n-\t\t\t}\n+\t\t\tsize_in_bytes += blob_sorting_data->SizeInBytes() + blob_sorting_heap->SizeInBytes();\n \t\t}\n-\t\tsize_in_bytes += payload_data->count * payload_layout.GetRowWidth();\n \t\tif (!payload_layout.AllConstant()) {\n-\t\t\tfor (auto &block : payload_data->blocks) {\n-\t\t\t\tsize_in_bytes += block.byte_offset;\n-\t\t\t}\n+\t\t\tsize_in_bytes += payload_heap->SizeInBytes();\n \t\t}\n-\t\t// Get the max memory and number of threads\n-\t\tauto &buffer_manager = BufferManager::GetBufferManager(context);\n-\t\tauto &task_scheduler = TaskScheduler::GetScheduler(context);\n-\t\tidx_t max_memory = buffer_manager.GetMaxMemory();\n-\t\tidx_t num_threads = task_scheduler.NumberOfThreads();\n-\t\t// Memory usage per thread should scale with max mem / num threads\n-\t\t// We take 15% of the max memory, to be VERY conservative\n-\t\treturn size_in_bytes > (0.15 * max_memory / num_threads);\n+\t\treturn size_in_bytes >= memory_per_thread;\n \t}\n \n+private:\n+\tidx_t EntriesPerBlock(idx_t width) {\n+\t\treturn (Storage::BLOCK_SIZE + width * STANDARD_VECTOR_SIZE - 1) / width;\n+\t}\n+\n+public:\n+\t//! Whether this local state has been initialized\n+\tbool initialized;\n+\t//! Local copy of the sorting expression executor\n+\tExpressionExecutor executor;\n+\t//! Holds a vector of incoming sorting columns\n+\tDataChunk sort;\n+\n \t//! Radix/memcmp sortable data\n \tunique_ptr<RowDataCollection> radix_sorting_data;\n \t//! Variable sized sorting data and accompanying heap\n@@ -234,7 +216,12 @@ class OrderLocalState : public LocalSinkState {\n unique_ptr<GlobalOperatorState> PhysicalOrder::GetGlobalState(ClientContext &context) {\n \tRowLayout payload_layout;\n \tpayload_layout.Initialize(types, false);\n-\tauto state = make_unique<OrderGlobalState>(SortingState(orders), payload_layout);\n+\tauto state = make_unique<OrderGlobalState>(*this, payload_layout);\n+\t// Memory usage per thread should scale with max mem / num threads\n+\t// We take 1/6th of this, to be conservative\n+\tidx_t max_memory = BufferManager::GetBufferManager(context).GetMaxMemory();\n+\tidx_t num_threads = TaskScheduler::GetScheduler(context).NumberOfThreads();\n+\tstate->memory_per_thread = (max_memory / num_threads) / 6;\n \tstate->external = context.force_external;\n \treturn move(state);\n }\n@@ -300,7 +287,7 @@ void PhysicalOrder::Sink(ExecutionContext &context, GlobalOperatorState &gstate_\n \t                       lstate.sel_ptr, input.size());\n \n \t// When sorting data reaches a certain size, we sort it\n-\tif (lstate.Full(context.client, sorting_state, payload_layout)) {\n+\tif (lstate.Full(gstate.memory_per_thread, sorting_state, payload_layout)) {\n \t\tSortLocalState(context.client, lstate, gstate);\n \t}\n }\n@@ -371,9 +358,11 @@ struct SortedData {\n \t}\n \t//! Initialize new block to write to\n \tvoid CreateBlock() {\n-\t\tdata_blocks.emplace_back(buffer_manager, state.block_capacity, layout.GetRowWidth());\n+\t\tauto capacity = MaxValue(((idx_t)Storage::BLOCK_SIZE + layout.GetRowWidth() - 1) / layout.GetRowWidth(),\n+\t\t                         state.block_capacity);\n+\t\tdata_blocks.emplace_back(buffer_manager, capacity, layout.GetRowWidth());\n \t\tif (!layout.AllConstant() && state.external) {\n-\t\t\theap_blocks.emplace_back(buffer_manager, heap_capacity, 1);\n+\t\t\theap_blocks.emplace_back(buffer_manager, (idx_t)Storage::BLOCK_SIZE, 1);\n \t\t\tD_ASSERT(data_blocks.size() == heap_blocks.size());\n \t\t}\n \t}\n@@ -429,8 +418,6 @@ struct SortedData {\n \t//! Data and heap blocks\n \tvector<RowDataBlock> data_blocks;\n \tvector<RowDataBlock> heap_blocks;\n-\t//! Capacity (in bytes) of the heap blocks\n-\tidx_t heap_capacity;\n \t//! Read indices\n \tidx_t block_idx;\n \tidx_t entry_idx;\n@@ -471,7 +458,7 @@ struct SortedBlock {\n \t\tpayload_data = make_unique<SortedData>(payload_layout, buffer_manager, state);\n \t}\n \t//! Number of rows that this object holds\n-\tidx_t Count() {\n+\tidx_t Count() const {\n \t\tidx_t count = std::accumulate(radix_sorting_data.begin(), radix_sorting_data.end(), 0,\n \t\t                              [](idx_t a, const RowDataBlock &b) { return a + b.count; });\n \t\tif (!sorting_state.all_constant) {\n@@ -481,7 +468,7 @@ struct SortedBlock {\n \t\treturn count;\n \t}\n \t//! The remaining number of rows to be read from this object\n-\tidx_t Remaining() {\n+\tidx_t Remaining() const {\n \t\tidx_t remaining = 0;\n \t\tif (block_idx < radix_sorting_data.size()) {\n \t\t\tremaining += radix_sorting_data[block_idx].count - entry_idx;\n@@ -495,44 +482,15 @@ struct SortedBlock {\n \tvoid InitializeWrite() {\n \t\tCreateBlock();\n \t\tif (!sorting_state.all_constant) {\n-\t\t\tblob_sorting_data->heap_capacity = state.sorting_heap_capacity;\n \t\t\tblob_sorting_data->CreateBlock();\n \t\t}\n-\t\tpayload_data->heap_capacity = state.payload_heap_capacity;\n \t\tpayload_data->CreateBlock();\n \t}\n \t//! Init new block to write to\n \tvoid CreateBlock() {\n-\t\tradix_sorting_data.emplace_back(buffer_manager, state.block_capacity, sorting_state.entry_size);\n-\t}\n-\t//! Cleanup sorting data\n-\tvoid UnregisterSortingBlocks() {\n-\t\tfor (auto &block : radix_sorting_data) {\n-\t\t\tbuffer_manager.UnregisterBlock(block.block->BlockId(), true);\n-\t\t}\n-\t\tif (!sorting_state.all_constant) {\n-\t\t\tfor (auto &block : blob_sorting_data->data_blocks) {\n-\t\t\t\tbuffer_manager.UnregisterBlock(block.block->BlockId(), true);\n-\t\t\t}\n-\t\t\tif (state.external) {\n-\t\t\t\tfor (auto &block : blob_sorting_data->heap_blocks) {\n-\t\t\t\t\tbuffer_manager.UnregisterBlock(block.block->BlockId(), true);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\t//! Cleanup payload data\n-\tvoid UnregisterPayloadBlocks() {\n-\t\tfor (auto &block : payload_data->data_blocks) {\n-\t\t\tbuffer_manager.UnregisterBlock(block.block->BlockId(), true);\n-\t\t}\n-\t\tif (state.external) {\n-\t\t\tif (!payload_data->layout.AllConstant()) {\n-\t\t\t\tfor (auto &block : payload_data->heap_blocks) {\n-\t\t\t\t\tbuffer_manager.UnregisterBlock(block.block->BlockId(), true);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t\tauto capacity = MaxValue(((idx_t)Storage::BLOCK_SIZE + sorting_state.entry_size - 1) / sorting_state.entry_size,\n+\t\t                         state.block_capacity);\n+\t\tradix_sorting_data.emplace_back(buffer_manager, capacity, sorting_state.entry_size);\n \t}\n \t//! Fill this sorted block by appending the blocks held by a vector of sorted blocks\n \tvoid AppendSortedBlocks(vector<unique_ptr<SortedBlock>> &sorted_blocks) {\n@@ -609,6 +567,7 @@ struct SortedBlock {\n \t\treturn result;\n \t}\n \n+\t//! Size (in bytes) of the heap of this block\n \tidx_t HeapSize() const {\n \t\tidx_t result = 0;\n \t\tif (!sorting_state.all_constant) {\n@@ -623,6 +582,22 @@ struct SortedBlock {\n \t\t}\n \t\treturn result;\n \t}\n+\t//! Total size (in bytes) of this block\n+\tidx_t SizeInBytes() const {\n+\t\tidx_t bytes = 0;\n+\t\tfor (idx_t i = 0; i < radix_sorting_data.size(); i++) {\n+\t\t\tbytes += radix_sorting_data[i].capacity * sorting_state.entry_size;\n+\t\t\tif (!sorting_state.all_constant) {\n+\t\t\t\tbytes += blob_sorting_data->data_blocks[i].capacity * sorting_state.blob_layout.GetRowWidth();\n+\t\t\t\tbytes += blob_sorting_data->heap_blocks[i].capacity;\n+\t\t\t}\n+\t\t\tbytes += payload_data->data_blocks[i].capacity * payload_layout.GetRowWidth();\n+\t\t\tif (!payload_layout.AllConstant()) {\n+\t\t\t\tbytes += payload_data->heap_blocks[i].capacity;\n+\t\t\t}\n+\t\t}\n+\t\treturn bytes;\n+\t}\n \n public:\n \t//! Radix/memcmp sortable data\n@@ -640,24 +615,13 @@ struct SortedBlock {\n \tOrderGlobalState &state;\n \tconst SortingState &sorting_state;\n \tconst RowLayout &payload_layout;\n-\n-\t//! Handle and ptr for sorting_blocks\n-\tunique_ptr<BufferHandle> sorting_handle;\n };\n \n-OrderGlobalState::~OrderGlobalState() {\n-\tstd::lock_guard<mutex> glock(lock);\n-\tfor (auto &sb : sorted_blocks) {\n-\t\tsb->UnregisterPayloadBlocks();\n-\t}\n-\tsorted_blocks.clear();\n-}\n-\n //! Concatenates the blocks in a RowDataCollection into a single block\n static RowDataBlock ConcatenateBlocks(BufferManager &buffer_manager, RowDataCollection &row_data) {\n \t// Create block with the correct capacity\n \tconst idx_t &entry_size = row_data.entry_size;\n-\tidx_t capacity = MaxValue(Storage::BLOCK_SIZE / entry_size + 1, row_data.count);\n+\tidx_t capacity = MaxValue(((idx_t)Storage::BLOCK_SIZE + entry_size - 1) / entry_size, row_data.count);\n \tRowDataBlock new_block(buffer_manager, capacity, entry_size);\n \tnew_block.count = row_data.count;\n \tauto new_block_handle = buffer_manager.Pin(new_block.block);\n@@ -667,7 +631,6 @@ static RowDataBlock ConcatenateBlocks(BufferManager &buffer_manager, RowDataColl\n \t\tauto block_handle = buffer_manager.Pin(block.block);\n \t\tmemcpy(new_block_ptr, block_handle->Ptr(), block.count * entry_size);\n \t\tnew_block_ptr += block.count * entry_size;\n-\t\tbuffer_manager.UnregisterBlock(block.block->BlockId(), true);\n \t}\n \trow_data.blocks.clear();\n \trow_data.count = 0;\n@@ -1027,8 +990,8 @@ static void SortTiedBlobs(BufferManager &buffer_manager, const data_ptr_t datapt\n \t\treturn;\n \t}\n \t// Fill pointer array for sorting\n-\tauto ptr_block = buffer_manager.Allocate(MaxValue((end - start) * sizeof(data_ptr_t), (idx_t)Storage::BLOCK_SIZE));\n-\tauto entry_ptrs = (data_ptr_t *)ptr_block->Ptr();\n+\tauto ptr_block = unique_ptr<data_ptr_t[]>(new data_ptr_t[end - start]);\n+\tauto entry_ptrs = (data_ptr_t *)ptr_block.get();\n \tfor (idx_t i = start; i < end; i++) {\n \t\tentry_ptrs[i - start] = row_ptr;\n \t\trow_ptr += sorting_state.entry_size;\n@@ -1114,7 +1077,6 @@ static void ComputeTies(data_ptr_t dataptr, const idx_t &count, const idx_t &col\n \t\tties[i] = ties[i] && memcmp(dataptr, dataptr + sorting_state.entry_size, tie_size) == 0;\n \t\tdataptr += sorting_state.entry_size;\n \t}\n-\tties[count - 1] = false;\n }\n \n //! Textbook LSD radix sort\n@@ -1190,6 +1152,7 @@ static void SortInMemory(BufferManager &buffer_manager, SortedBlock &sb, const S\n \t// Radix sort and break ties until no more ties, or until all columns are sorted\n \tidx_t sorting_size = 0;\n \tidx_t col_offset = 0;\n+\tunique_ptr<bool[]> ties_ptr;\n \tunique_ptr<BufferHandle> ties_handle;\n \tbool *ties = nullptr;\n \tfor (idx_t i = 0; i < sorting_state.column_count; i++) {\n@@ -1202,8 +1165,8 @@ static void SortInMemory(BufferManager &buffer_manager, SortedBlock &sb, const S\n \t\tif (!ties) {\n \t\t\t// This is the first sort\n \t\t\tRadixSort(buffer_manager, dataptr, count, col_offset, sorting_size, sorting_state);\n-\t\t\tties_handle = buffer_manager.Allocate(MaxValue(count, (idx_t)Storage::BLOCK_SIZE));\n-\t\t\tties = (bool *)ties_handle->Ptr();\n+\t\t\tties_ptr = unique_ptr<bool[]>(new bool[count]);\n+\t\t\tties = ties_ptr.get();\n \t\t\tstd::fill_n(ties, count - 1, true);\n \t\t\tties[count - 1] = false;\n \t\t} else {\n@@ -1257,7 +1220,6 @@ static void ReOrder(BufferManager &buffer_manager, SortedData &sd, data_ptr_t so\n \t\tsorting_ptr += sorting_entry_size;\n \t}\n \t// Replace the unordered data block with the re-ordered data block\n-\tbuffer_manager.UnregisterBlock(unordered_data_block.block->BlockId(), true);\n \tsd.data_blocks.clear();\n \tsd.data_blocks.push_back(move(ordered_data_block));\n \t// Deal with the heap (if necessary)\n@@ -1287,14 +1249,10 @@ static void ReOrder(BufferManager &buffer_manager, SortedData &sd, data_ptr_t so\n \t\tRowOperations::SwizzleHeapPointer(sd.layout, ordered_data_handle->Ptr(), ordered_heap_handle->Ptr(), count);\n \t\t// Move the re-ordered heap to the SortedData, and clear the local heap\n \t\tsd.heap_blocks.push_back(move(ordered_heap_block));\n-\t\tfor (auto &block : heap.blocks) {\n-\t\t\tbuffer_manager.UnregisterBlock(block.block->BlockId(), true);\n-\t\t}\n+\t\theap.pinned_blocks.clear();\n+\t\theap.blocks.clear();\n+\t\theap.count = 0;\n \t}\n-\t// Reset the localstate heap\n-\theap.pinned_blocks.clear();\n-\theap.blocks.clear();\n-\theap.count = 0;\n }\n \n //! Use the ordered sorting data to re-order the rest of the data\n@@ -1324,7 +1282,7 @@ void PhysicalOrder::SortLocalState(ClientContext &context, OrderLocalState &lsta\n \tauto sorting_block = ConcatenateBlocks(buffer_manager, *lstate.radix_sorting_data);\n \tsb->radix_sorting_data.push_back(move(sorting_block));\n \t// Variable-size sorting data\n-\tif (!sorting_state.blob_layout.AllConstant()) {\n+\tif (!gstate.sorting_state.all_constant) {\n \t\tauto &blob_data = *lstate.blob_sorting_data;\n \t\tauto new_block = ConcatenateBlocks(buffer_manager, blob_data);\n \t\tsb->blob_sorting_data->data_blocks.push_back(move(new_block));\n@@ -1389,6 +1347,7 @@ class PhysicalOrderMergeTask : public Task {\n \t\t\t}\n \t\t}\n \t\t// Set up the write block\n+\t\t// Each merge task produces a SortedBlock with exactly state.block_capacity rows or less\n \t\tresult->InitializeWrite();\n \t\t// Initialize arrays to store merge data\n \t\tbool left_smaller[STANDARD_VECTOR_SIZE];\n@@ -1433,11 +1392,6 @@ class PhysicalOrderMergeTask : public Task {\n \t\tlock_guard<mutex> glock(state.lock);\n \t\tparent.finished_tasks++;\n \t\tif (parent.finished_tasks == parent.total_tasks) {\n-\t\t\t// Unregister processed data\n-\t\t\tfor (auto &sb : state.sorted_blocks) {\n-\t\t\t\tsb->UnregisterSortingBlocks();\n-\t\t\t\tsb->UnregisterPayloadBlocks();\n-\t\t\t}\n \t\t\tstate.sorted_blocks.clear();\n \t\t\tif (state.odd_one_out) {\n \t\t\t\tstate.sorted_blocks.push_back(move(state.odd_one_out));\n@@ -1500,6 +1454,14 @@ class PhysicalOrderMergeTask : public Task {\n \t\tD_ASSERT(l_idx < l.Count());\n \t\tD_ASSERT(r_idx < r.Count());\n \n+\t\t// Easy comparison using the previous result (intersections must increase monotonically)\n+\t\tif (l_idx < state.l_start) {\n+\t\t\treturn -1;\n+\t\t}\n+\t\tif (r_idx < state.r_start) {\n+\t\t\treturn 1;\n+\t\t}\n+\n \t\tidx_t l_block_idx;\n \t\tidx_t l_entry_idx;\n \t\tl.GlobalToLocalIndex(l_idx, l_block_idx, l_entry_idx);\n@@ -1533,6 +1495,9 @@ class PhysicalOrderMergeTask : public Task {\n \t\tconst idx_t l_count = l.Count();\n \t\tconst idx_t r_count = r.Count();\n \t\t// Cover some edge cases\n+\t\t// Code coverage off because these edge cases cannot happen unless other code changes\n+\t\t// Edge cases have been tested extensively while developing Merge Path in a script\n+\t\t// LCOV_EXCL_START\n \t\tif (sum >= l_count + r_count) {\n \t\t\tl_idx = l_count;\n \t\t\tr_idx = r_count;\n@@ -1550,6 +1515,7 @@ class PhysicalOrderMergeTask : public Task {\n \t\t\tl_idx = sum;\n \t\t\treturn;\n \t\t}\n+\t\t// LCOV_EXCL_STOP\n \t\t// Determine offsets for the binary search\n \t\tconst idx_t l_offset = MinValue(l_count, sum);\n \t\tconst idx_t r_offset = sum > l_count ? sum - l_count : 0;\n@@ -1574,7 +1540,11 @@ class PhysicalOrderMergeTask : public Task {\n \t\t\t\t\treturn;\n \t\t\t\t}\n \t\t\t\tif (l_idx == 0 || r_idx == r_count) {\n+\t\t\t\t\t// This case is incredibly difficult to cover as it is dependent on parallelism randomness\n+\t\t\t\t\t// But it has been tested extensively during development in a script\n+\t\t\t\t\t// LCOV_EXCL_START\n \t\t\t\t\treturn;\n+\t\t\t\t\t// LCOV_EXCL_STOP\n \t\t\t\t} else {\n \t\t\t\t\tbreak;\n \t\t\t\t}\n@@ -1586,15 +1556,6 @@ class PhysicalOrderMergeTask : public Task {\n \t\t\t\tright = middle - 1;\n \t\t\t}\n \t\t}\n-\t\t// Shift by one (if needed)\n-\t\tif (l_idx == 0) {\n-\t\t\tcomp_res = CompareUsingGlobalIndex(l, r, l_idx, r_idx);\n-\t\t\tif (comp_res > 0) {\n-\t\t\t\tl_idx--;\n-\t\t\t\tr_idx++;\n-\t\t\t}\n-\t\t\treturn;\n-\t\t}\n \t\tint l_r_min1 = CompareUsingGlobalIndex(l, r, l_idx, r_idx - 1);\n \t\tint l_min1_r = CompareUsingGlobalIndex(l, r, l_idx - 1, r_idx);\n \t\tif (l_r_min1 > 0 && l_min1_r < 0) {\n@@ -1753,13 +1714,6 @@ class PhysicalOrderMergeTask : public Task {\n \t\t\t}\n \t\t\tconst idx_t &l_count = !l_done ? l_block->count : 0;\n \t\t\tconst idx_t &r_count = !r_done ? r_block->count : 0;\n-\t\t\t// Create new result block (if needed)\n-\t\t\tif (result_block->count == result_block->capacity) {\n-\t\t\t\tresult->CreateBlock();\n-\t\t\t\tresult_block = &result->radix_sorting_data.back();\n-\t\t\t\tresult_handle = buffer_manager.Pin(result_block->block);\n-\t\t\t\tresult_ptr = result_handle->Ptr();\n-\t\t\t}\n \t\t\t// Copy using computed merge\n \t\t\tif (!l_done && !r_done) {\n \t\t\t\t// Both sides have data - merge\n@@ -1835,25 +1789,6 @@ class PhysicalOrderMergeTask : public Task {\n \t\t\t}\n \t\t\tconst idx_t &l_count = !l_done ? l_data.data_blocks[l_data.block_idx].count : 0;\n \t\t\tconst idx_t &r_count = !r_done ? r_data.data_blocks[r_data.block_idx].count : 0;\n-\t\t\t// Create new result data block (if needed)\n-\t\t\tif (result_data_block->count == result_data_block->capacity) {\n-\t\t\t\t// Shrink down the last heap block to fit the data\n-\t\t\t\tif (!layout.AllConstant() && state.external &&\n-\t\t\t\t    result_heap_block->byte_offset < result_heap_block->capacity &&\n-\t\t\t\t    result_heap_block->byte_offset >= Storage::BLOCK_SIZE) {\n-\t\t\t\t\tbuffer_manager.ReAllocate(result_heap_block->block, result_heap_block->byte_offset);\n-\t\t\t\t\tresult_heap_block->capacity = result_heap_block->byte_offset;\n-\t\t\t\t}\n-\t\t\t\tresult_data.CreateBlock();\n-\t\t\t\tresult_data_block = &result_data.data_blocks.back();\n-\t\t\t\tresult_data_handle = buffer_manager.Pin(result_data_block->block);\n-\t\t\t\tresult_data_ptr = result_data_handle->Ptr();\n-\t\t\t\tif (!layout.AllConstant() && state.external) {\n-\t\t\t\t\tresult_heap_block = &result_data.heap_blocks.back();\n-\t\t\t\t\tresult_heap_handle = buffer_manager.Pin(result_heap_block->block);\n-\t\t\t\t\tresult_heap_ptr = result_heap_handle->Ptr();\n-\t\t\t\t}\n-\t\t\t}\n \t\t\t// Perform the merge\n \t\t\tif (layout.AllConstant() || !state.external) {\n \t\t\t\t// If all constant size, or if we are doing an in-memory sort, we do not need to touch the heap\n@@ -2077,28 +2012,14 @@ bool PhysicalOrder::Finalize(Pipeline &pipeline, ClientContext &context, unique_\n \tidx_t total_heap_size =\n \t    std::accumulate(state.sorted_blocks.begin(), state.sorted_blocks.end(), (idx_t)0,\n \t                    [](idx_t a, const unique_ptr<SortedBlock> &b) { return a + b->HeapSize(); });\n-\tif (total_heap_size > 0.25 * BufferManager::GetBufferManager(context).GetMaxMemory()) {\n+\tif (state.external || total_heap_size > 0.25 * BufferManager::GetBufferManager(context).GetMaxMemory()) {\n \t\tstate.external = true;\n \t}\n \t// Use the data that we have to determine which block size to use during the merge\n-\tconst auto &sorting_state = state.sorting_state;\n+\tstate.block_capacity = state.sorted_blocks[0]->Count();\n \tfor (auto &sb : state.sorted_blocks) {\n-\t\tauto &block = sb->radix_sorting_data.back();\n-\t\tstate.block_capacity = MaxValue(state.block_capacity, block.capacity);\n-\t}\n-\t// Sorting heap data\n-\tif (!sorting_state.all_constant && state.external) {\n-\t\tfor (auto &sb : state.sorted_blocks) {\n-\t\t\tauto &heap_block = sb->blob_sorting_data->heap_blocks.back();\n-\t\t\tstate.sorting_heap_capacity = MaxValue(state.sorting_heap_capacity, heap_block.capacity);\n-\t\t}\n-\t}\n-\t// Payload heap data\n-\tconst auto &payload_layout = state.payload_layout;\n-\tif (!payload_layout.AllConstant() && state.external) {\n-\t\tfor (auto &sb : state.sorted_blocks) {\n-\t\t\tauto &heap_block = sb->payload_data->heap_blocks.back();\n-\t\t\tstate.payload_heap_capacity = MaxValue(state.sorting_heap_capacity, heap_block.capacity);\n+\t\tif (sb->SizeInBytes() >= state.memory_per_thread) {\n+\t\t\tstate.block_capacity = MinValue(state.block_capacity, sb->Count());\n \t\t}\n \t}\n \t// Unswizzle and pin heap blocks if we can fit everything in memory\n@@ -2114,10 +2035,6 @@ bool PhysicalOrder::Finalize(Pipeline &pipeline, ClientContext &context, unique_\n \t\tPhysicalOrder::ScheduleMergeTasks(pipeline, context, state);\n \t\treturn false;\n \t} else {\n-\t\t// Clean up sorting data - payload is sorted\n-\t\tfor (auto &sb : state.sorted_blocks) {\n-\t\t\tsb->UnregisterSortingBlocks();\n-\t\t}\n \t\treturn true;\n \t}\n }\n@@ -2126,7 +2043,10 @@ void PhysicalOrder::ScheduleMergeTasks(Pipeline &pipeline, ClientContext &contex\n \tD_ASSERT(state.sorted_blocks_temp.empty());\n \tif (state.sorted_blocks.size() == 1) {\n \t\tfor (auto &sb : state.sorted_blocks) {\n-\t\t\tsb->UnregisterSortingBlocks();\n+\t\t\tsb->radix_sorting_data.clear();\n+\t\t\tif (!state.sorting_state.all_constant) {\n+\t\t\t\tsb->blob_sorting_data.reset();\n+\t\t\t}\n \t\t}\n \t\tpipeline.Finish();\n \t\treturn;\n@@ -2143,6 +2063,7 @@ void PhysicalOrder::ScheduleMergeTasks(Pipeline &pipeline, ClientContext &contex\n \tstate.l_start = 0;\n \tstate.r_start = 0;\n \t// Compute how many tasks there will be\n+\t// Each merge task produces a SortedBlock exactly state.block_capacity or less\n \tidx_t num_tasks = 0;\n \tconst idx_t tuples_per_block = state.block_capacity;\n \tfor (idx_t block_idx = 0; block_idx < num_blocks; block_idx += 2) {\n@@ -2241,7 +2162,6 @@ void PhysicalOrder::GetChunkInternal(ExecutionContext &context, DataChunk &chunk\n \t}\n \n \tif (!state.initialized) {\n-\t\tD_ASSERT(gstate.sorted_blocks.back()->Count() == gstate.total_count);\n \t\tstate.payload_data = gstate.sorted_blocks.back()->payload_data.get();\n \t\tstate.initialized = true;\n \t}\ndiff --git a/src/execution/physical_plan/plan_order.cpp b/src/execution/physical_plan/plan_order.cpp\nindex 56e29833df60..2b22afcec343 100644\n--- a/src/execution/physical_plan/plan_order.cpp\n+++ b/src/execution/physical_plan/plan_order.cpp\n@@ -9,7 +9,8 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalOrder &op)\n \n \tauto plan = CreatePlan(*op.children[0]);\n \tif (!op.orders.empty()) {\n-\t\tauto order = make_unique<PhysicalOrder>(op.types, move(op.orders), op.estimated_cardinality);\n+\t\tauto order =\n+\t\t    make_unique<PhysicalOrder>(op.types, move(op.orders), move(op.statistics), op.estimated_cardinality);\n \t\torder->children.push_back(move(plan));\n \t\tplan = move(order);\n \t}\ndiff --git a/src/include/duckdb/common/exception.hpp b/src/include/duckdb/common/exception.hpp\nindex 3d592d3f3055..c850131f285a 100644\n--- a/src/include/duckdb/common/exception.hpp\n+++ b/src/include/duckdb/common/exception.hpp\n@@ -10,8 +10,8 @@\n \n #include \"duckdb/common/assert.hpp\"\n #include \"duckdb/common/common.hpp\"\n-#include \"duckdb/common/vector.hpp\"\n #include \"duckdb/common/exception_format_value.hpp\"\n+#include \"duckdb/common/vector.hpp\"\n \n #include <stdexcept>\n \n@@ -72,7 +72,8 @@ enum class ExceptionType {\n \tFATAL = 30, // Fatal exception: fatal exceptions are non-recoverable, and render the entire DB in an unusable state\n \tINTERNAL =\n \t    31, // Internal exception: exception that indicates something went wrong internally (i.e. bug in the code base)\n-\tINVALID_INPUT = 32 // Input or arguments error\n+\tINVALID_INPUT = 32, // Input or arguments error\n+\tOUT_OF_MEMORY = 33  // out of memory\n };\n \n class Exception : public std::exception {\n@@ -184,6 +185,16 @@ class OutOfRangeException : public Exception {\n \t}\n };\n \n+class OutOfMemoryException : public Exception {\n+public:\n+\texplicit OutOfMemoryException(const string &msg);\n+\n+\ttemplate <typename... Args>\n+\texplicit OutOfMemoryException(const string &msg, Args... params)\n+\t    : OutOfMemoryException(ConstructMessage(msg, params...)) {\n+\t}\n+};\n+\n class SyntaxException : public Exception {\n public:\n \texplicit SyntaxException(const string &msg);\ndiff --git a/src/include/duckdb/common/types/row_data_collection.hpp b/src/include/duckdb/common/types/row_data_collection.hpp\nindex 6736206ecd1e..630c4b836516 100644\n--- a/src/include/duckdb/common/types/row_data_collection.hpp\n+++ b/src/include/duckdb/common/types/row_data_collection.hpp\n@@ -62,6 +62,19 @@ class RowDataCollection {\n \n \tvoid Merge(RowDataCollection &other);\n \n+\t//! The size (in bytes) of this RowDataCollection if it were stored in a single block\n+\tidx_t SizeInBytes() const {\n+\t\tidx_t bytes = 0;\n+\t\tif (entry_size == 1) {\n+\t\t\tfor (auto &block : blocks) {\n+\t\t\t\tbytes += block.byte_offset;\n+\t\t\t}\n+\t\t} else {\n+\t\t\tbytes = count * entry_size;\n+\t\t}\n+\t\treturn MaxValue(bytes, (idx_t)Storage::BLOCK_SIZE);\n+\t}\n+\n private:\n \tmutex rdc_lock;\n \ndiff --git a/src/include/duckdb/execution/operator/order/physical_order.hpp b/src/include/duckdb/execution/operator/order/physical_order.hpp\nindex 95d96cf78515..c77e8ea10a87 100644\n--- a/src/include/duckdb/execution/operator/order/physical_order.hpp\n+++ b/src/include/duckdb/execution/operator/order/physical_order.hpp\n@@ -23,10 +23,13 @@ class OrderGlobalState;\n //! Physically re-orders the input data\n class PhysicalOrder : public PhysicalSink {\n public:\n-\tPhysicalOrder(vector<LogicalType> types, vector<BoundOrderByNode> orders, idx_t estimated_cardinality);\n+\tPhysicalOrder(vector<LogicalType> types, vector<BoundOrderByNode> orders,\n+\t              vector<unique_ptr<BaseStatistics>> statistics, idx_t estimated_cardinality);\n \n \t//! Input data\n \tvector<BoundOrderByNode> orders;\n+\t//! Statistics of the order expressions\n+\tvector<unique_ptr<BaseStatistics>> statistics;\n \n public:\n \tvoid Sink(ExecutionContext &context, GlobalOperatorState &gstate_p, LocalSinkState &lstate_p,\ndiff --git a/src/include/duckdb/planner/expression_binder.hpp b/src/include/duckdb/planner/expression_binder.hpp\nindex 44eece2124f1..737d2ece6543 100644\n--- a/src/include/duckdb/planner/expression_binder.hpp\n+++ b/src/include/duckdb/planner/expression_binder.hpp\n@@ -105,7 +105,6 @@ class ExpressionBinder {\n \tvirtual void ReplaceMacroParametersRecursive(unique_ptr<ParsedExpression> &expr);\n \tvirtual void ReplaceMacroParametersRecursive(ParsedExpression &expr, QueryNode &node);\n \tvirtual void ReplaceMacroParametersRecursive(ParsedExpression &expr, TableRef &ref);\n-\tvirtual void CheckForSideEffects(FunctionExpression &function, idx_t depth, string &error);\n \n \tvirtual string UnsupportedAggregateMessage();\n \tvirtual string UnsupportedUnnestMessage();\ndiff --git a/src/include/duckdb/planner/operator/logical_order.hpp b/src/include/duckdb/planner/operator/logical_order.hpp\nindex 07a2b8ca76db..80143c51220c 100644\n--- a/src/include/duckdb/planner/operator/logical_order.hpp\n+++ b/src/include/duckdb/planner/operator/logical_order.hpp\n@@ -10,6 +10,7 @@\n \n #include \"duckdb/planner/bound_query_node.hpp\"\n #include \"duckdb/planner/logical_operator.hpp\"\n+#include \"duckdb/storage/statistics/base_statistics.hpp\"\n \n namespace duckdb {\n \n@@ -21,6 +22,7 @@ class LogicalOrder : public LogicalOperator {\n \t}\n \n \tvector<BoundOrderByNode> orders;\n+\tvector<unique_ptr<BaseStatistics>> statistics;\n \n \tstring ParamsToString() const override {\n \t\tstring result;\ndiff --git a/src/include/duckdb/storage/buffer/block_handle.hpp b/src/include/duckdb/storage/buffer/block_handle.hpp\nindex 5c331326507d..e2fccb479994 100644\n--- a/src/include/duckdb/storage/buffer/block_handle.hpp\n+++ b/src/include/duckdb/storage/buffer/block_handle.hpp\n@@ -29,7 +29,7 @@ class BlockHandle {\n public:\n \tBlockHandle(DatabaseInstance &db, block_id_t block_id);\n \tBlockHandle(DatabaseInstance &db, block_id_t block_id, unique_ptr<FileBuffer> buffer, bool can_destroy,\n-\t            idx_t alloc_size);\n+\t            idx_t block_size);\n \t~BlockHandle();\n \n \tDatabaseInstance &db;\ndiff --git a/src/include/duckdb/storage/buffer_manager.hpp b/src/include/duckdb/storage/buffer_manager.hpp\nindex 7da09cd42264..91093f6a16f5 100644\n--- a/src/include/duckdb/storage/buffer_manager.hpp\n+++ b/src/include/duckdb/storage/buffer_manager.hpp\n@@ -8,15 +8,14 @@\n \n #pragma once\n \n-#include \"duckdb/storage/buffer/buffer_handle.hpp\"\n-#include \"duckdb/storage/buffer/managed_buffer.hpp\"\n-#include \"duckdb/storage/block_manager.hpp\"\n+#include \"duckdb/common/atomic.hpp\"\n #include \"duckdb/common/file_system.hpp\"\n+#include \"duckdb/common/mutex.hpp\"\n #include \"duckdb/common/unordered_map.hpp\"\n+#include \"duckdb/storage/block_manager.hpp\"\n #include \"duckdb/storage/buffer/block_handle.hpp\"\n-\n-#include \"duckdb/common/atomic.hpp\"\n-#include \"duckdb/common/mutex.hpp\"\n+#include \"duckdb/storage/buffer/buffer_handle.hpp\"\n+#include \"duckdb/storage/buffer/managed_buffer.hpp\"\n \n namespace duckdb {\n class DatabaseInstance;\n@@ -39,14 +38,14 @@ class BufferManager {\n \t//! Register an in-memory buffer of arbitrary size, as long as it is >= BLOCK_SIZE. can_destroy signifies whether or\n \t//! not the buffer can be destroyed when unpinned, or whether or not it needs to be written to a temporary file so\n \t//! it can be reloaded. The resulting buffer will already be allocated, but needs to be pinned in order to be used.\n-\tshared_ptr<BlockHandle> RegisterMemory(idx_t alloc_size, bool can_destroy);\n+\tshared_ptr<BlockHandle> RegisterMemory(idx_t block_size, bool can_destroy);\n \n \t//! Allocate an in-memory buffer with a single pin.\n \t//! The allocated memory is released when the buffer handle is destroyed.\n-\tunique_ptr<BufferHandle> Allocate(idx_t alloc_size);\n+\tunique_ptr<BufferHandle> Allocate(idx_t block_size);\n \n \t//! Reallocate an in-memory buffer that is pinned.\n-\tvoid ReAllocate(shared_ptr<BlockHandle> &handle, idx_t alloc_size);\n+\tvoid ReAllocate(shared_ptr<BlockHandle> &handle, idx_t block_size);\n \n \tunique_ptr<BufferHandle> Pin(shared_ptr<BlockHandle> &handle);\n \tvoid Unpin(shared_ptr<BlockHandle> &handle);\ndiff --git a/src/optimizer/statistics/operator/propagate_order.cpp b/src/optimizer/statistics/operator/propagate_order.cpp\nindex dd8b069e6ef3..582a944db058 100644\n--- a/src/optimizer/statistics/operator/propagate_order.cpp\n+++ b/src/optimizer/statistics/operator/propagate_order.cpp\n@@ -1,12 +1,25 @@\n #include \"duckdb/optimizer/statistics_propagator.hpp\"\n #include \"duckdb/planner/operator/logical_order.hpp\"\n+#include \"duckdb/storage/statistics/base_statistics.hpp\"\n \n namespace duckdb {\n \n unique_ptr<NodeStatistics> StatisticsPropagator::PropagateStatistics(LogicalOrder &order,\n                                                                      unique_ptr<LogicalOperator> *node_ptr) {\n-\t// propagate statistics in the child node\n-\treturn PropagateStatistics(order.children[0]);\n+\t// first propagate to the child\n+\tnode_stats = PropagateStatistics(order.children[0]);\n+\n+\t// then propagate to each of the order expressions\n+\tfor (idx_t i = 0; i < order.orders.size(); i++) {\n+\t\tauto &expr = order.orders[i].expression;\n+\t\tPropagateExpression(expr);\n+\t\tif (expr->stats) {\n+\t\t\torder.statistics.push_back(expr->stats->Copy());\n+\t\t} else {\n+\t\t\torder.statistics.push_back(nullptr);\n+\t\t}\n+\t}\n+\treturn move(node_stats);\n }\n \n } // namespace duckdb\ndiff --git a/src/optimizer/statistics_propagator.cpp b/src/optimizer/statistics_propagator.cpp\nindex d9e753269cca..84880e22949d 100644\n--- a/src/optimizer/statistics_propagator.cpp\n+++ b/src/optimizer/statistics_propagator.cpp\n@@ -1,8 +1,9 @@\n #include \"duckdb/optimizer/statistics_propagator.hpp\"\n-#include \"duckdb/planner/logical_operator.hpp\"\n+\n+#include \"duckdb/main/client_context.hpp\"\n #include \"duckdb/planner/expression_iterator.hpp\"\n+#include \"duckdb/planner/logical_operator.hpp\"\n #include \"duckdb/planner/operator/logical_empty_result.hpp\"\n-#include \"duckdb/main/client_context.hpp\"\n \n namespace duckdb {\n \n@@ -42,6 +43,8 @@ unique_ptr<NodeStatistics> StatisticsPropagator::PropagateStatistics(LogicalOper\n \tcase LogicalOperatorType::LOGICAL_EXCEPT:\n \tcase LogicalOperatorType::LOGICAL_INTERSECT:\n \t\treturn PropagateStatistics((LogicalSetOperation &)node, node_ptr);\n+\tcase LogicalOperatorType::LOGICAL_ORDER_BY:\n+\t\treturn PropagateStatistics((LogicalOrder &)node, node_ptr);\n \tdefault:\n \t\treturn PropagateChildren(node, node_ptr);\n \t}\ndiff --git a/src/planner/binder/expression/bind_macro_expression.cpp b/src/planner/binder/expression/bind_macro_expression.cpp\nindex 82f1dc0c9834..a3478b4e2314 100644\n--- a/src/planner/binder/expression/bind_macro_expression.cpp\n+++ b/src/planner/binder/expression/bind_macro_expression.cpp\n@@ -119,24 +119,6 @@ void ExpressionBinder::ReplaceMacroParametersRecursive(ParsedExpression &expr, Q\n \t}\n }\n \n-void ExpressionBinder::CheckForSideEffects(FunctionExpression &function, idx_t depth, string &error) {\n-\tfor (idx_t i = 0; i < function.children.size(); i++) {\n-\t\tauto arg_copy = function.children[i]->Copy();\n-\t\tBindChild(arg_copy, depth, error);\n-\t\tif (!error.empty()) {\n-\t\t\treturn;\n-\t\t}\n-\t\tauto &bound_expr = (BoundExpression &)*arg_copy;\n-\t\tif (bound_expr.expr->HasSideEffects()) {\n-\t\t\tQueryErrorContext error_context(binder.root_statement, function.query_location);\n-\t\t\terror = StringUtil::Format(\"Arguments with side-effects are not supported ('%s()' was supplied). As a \"\n-\t\t\t                           \"workaround, try creating a CTE that evaluates the argument with side-effects.\",\n-\t\t\t                           arg_copy->ToString());\n-\t\t\treturn;\n-\t\t}\n-\t}\n-}\n-\n BindResult ExpressionBinder::BindMacro(FunctionExpression &function, MacroCatalogEntry *macro_func, idx_t depth,\n                                        unique_ptr<ParsedExpression> *expr) {\n \tauto &macro_def = *macro_func->function;\n@@ -148,12 +130,6 @@ BindResult ExpressionBinder::BindMacro(FunctionExpression &function, MacroCatalo\n \t\treturn BindResult(binder.FormatError(*expr->get(), error));\n \t}\n \n-\t// check for arguments with side-effects TODO: to support this, a projection must be pushed\n-\t//    CheckForSideEffects(function, depth, error);\n-\t//    if (!error.empty()) {\n-\t//        return BindResult(error);\n-\t//    }\n-\n \t// create a MacroBinding to bind this macro's parameters to its arguments\n \tvector<LogicalType> types;\n \tvector<string> names;\ndiff --git a/src/storage/buffer_manager.cpp b/src/storage/buffer_manager.cpp\nindex be3fd4b9a2f6..39d34b822bd7 100644\n--- a/src/storage/buffer_manager.cpp\n+++ b/src/storage/buffer_manager.cpp\n@@ -15,12 +15,12 @@ BlockHandle::BlockHandle(DatabaseInstance &db, block_id_t block_id_p)\n }\n \n BlockHandle::BlockHandle(DatabaseInstance &db, block_id_t block_id_p, unique_ptr<FileBuffer> buffer_p,\n-                         bool can_destroy_p, idx_t alloc_size)\n+                         bool can_destroy_p, idx_t block_size)\n     : db(db), readers(0), block_id(block_id_p), eviction_timestamp(0), can_destroy(can_destroy_p) {\n-\tD_ASSERT(alloc_size >= Storage::BLOCK_SIZE);\n+\tD_ASSERT(block_size >= Storage::BLOCK_SIZE);\n \tbuffer = move(buffer_p);\n \tstate = BlockState::BLOCK_LOADED;\n-\tmemory_usage = alloc_size + Storage::BLOCK_HEADER_SIZE;\n+\tmemory_usage = block_size + Storage::BLOCK_HEADER_SIZE;\n }\n \n BlockHandle::~BlockHandle() {\n@@ -40,7 +40,6 @@ unique_ptr<BufferHandle> BlockHandle::Load(shared_ptr<BlockHandle> &handle) {\n \t\tD_ASSERT(handle->buffer);\n \t\treturn make_unique<BufferHandle>(handle, handle->buffer.get());\n \t}\n-\thandle->state = BlockState::BLOCK_LOADED;\n \n \tauto &buffer_manager = BufferManager::GetBufferManager(handle->db);\n \tauto &block_manager = BlockManager::GetBlockManager(handle->db);\n@@ -55,6 +54,7 @@ unique_ptr<BufferHandle> BlockHandle::Load(shared_ptr<BlockHandle> &handle) {\n \t\t\thandle->buffer = buffer_manager.ReadTemporaryBuffer(handle->block_id);\n \t\t}\n \t}\n+\thandle->state = BlockState::BLOCK_LOADED;\n \treturn make_unique<BufferHandle>(handle, handle->buffer.get());\n }\n \n@@ -64,8 +64,7 @@ void BlockHandle::Unload() {\n \t\treturn;\n \t}\n \tD_ASSERT(CanUnload());\n-\tD_ASSERT(memory_usage >= Storage::BLOCK_SIZE);\n-\tstate = BlockState::BLOCK_UNLOADED;\n+\tD_ASSERT(memory_usage >= Storage::BLOCK_ALLOC_SIZE);\n \n \tauto &buffer_manager = BufferManager::GetBufferManager(db);\n \tif (block_id >= MAXIMUM_BLOCK && !can_destroy) {\n@@ -74,6 +73,7 @@ void BlockHandle::Unload() {\n \t}\n \tbuffer.reset();\n \tbuffer_manager.current_memory -= memory_usage;\n+\tstate = BlockState::BLOCK_UNLOADED;\n }\n \n bool BlockHandle::CanUnload() {\n@@ -173,44 +173,47 @@ shared_ptr<BlockHandle> BufferManager::RegisterBlock(block_id_t block_id) {\n \treturn result;\n }\n \n-shared_ptr<BlockHandle> BufferManager::RegisterMemory(idx_t alloc_size, bool can_destroy) {\n+shared_ptr<BlockHandle> BufferManager::RegisterMemory(idx_t block_size, bool can_destroy) {\n+\tauto alloc_size = block_size + Storage::BLOCK_HEADER_SIZE;\n \t// first evict blocks until we have enough memory to store this buffer\n-\tif (!EvictBlocks(alloc_size + Storage::BLOCK_HEADER_SIZE, maximum_memory)) {\n-\t\tthrow OutOfRangeException(\"Not enough memory to complete operation: could not allocate block of %lld bytes\",\n-\t\t                          alloc_size);\n+\tif (!EvictBlocks(alloc_size, maximum_memory)) {\n+\t\tthrow OutOfMemoryException(\"could not allocate block of %lld bytes\", alloc_size);\n \t}\n \n \t// allocate the buffer\n \tauto temp_id = ++temporary_id;\n-\tauto buffer = make_unique<ManagedBuffer>(db, alloc_size, can_destroy, temp_id);\n+\tauto buffer = make_unique<ManagedBuffer>(db, block_size, can_destroy, temp_id);\n \n \t// create a new block pointer for this block\n-\treturn make_shared<BlockHandle>(db, temp_id, move(buffer), can_destroy, alloc_size);\n+\tauto result = make_shared<BlockHandle>(db, temp_id, move(buffer), can_destroy, block_size);\n+\treturn result;\n }\n \n-unique_ptr<BufferHandle> BufferManager::Allocate(idx_t alloc_size) {\n-\tauto block = RegisterMemory(alloc_size, true);\n+unique_ptr<BufferHandle> BufferManager::Allocate(idx_t block_size) {\n+\tauto block = RegisterMemory(block_size, true);\n \treturn Pin(block);\n }\n \n-void BufferManager::ReAllocate(shared_ptr<BlockHandle> &handle, idx_t alloc_size) {\n-\tD_ASSERT(alloc_size >= Storage::BLOCK_SIZE);\n+void BufferManager::ReAllocate(shared_ptr<BlockHandle> &handle, idx_t block_size) {\n+\tD_ASSERT(block_size >= Storage::BLOCK_SIZE);\n \tlock_guard<mutex> lock(handle->lock);\n-\tD_ASSERT(handle->readers == 1);\n-\tauto total_size = alloc_size + Storage::BLOCK_HEADER_SIZE;\n-\tint64_t required_memory = total_size - handle->memory_usage;\n-\tif (required_memory > 0) {\n-\t\t// evict blocks until we have space to increase the size of this block\n+\tD_ASSERT(handle->state == BlockState::BLOCK_LOADED);\n+\tauto alloc_size = block_size + Storage::BLOCK_HEADER_SIZE;\n+\tint64_t required_memory = alloc_size - handle->memory_usage;\n+\tif (required_memory == 0) {\n+\t\treturn;\n+\t} else if (required_memory > 0) {\n+\t\t// evict blocks until we have space to resize this block\n \t\tif (!EvictBlocks(required_memory, maximum_memory)) {\n-\t\t\tthrow OutOfRangeException(\"Not enough memory to complete operation: failed to increase block size\");\n+\t\t\tthrow OutOfMemoryException(\"failed to resize block from %lld to %lld\", handle->memory_usage, alloc_size);\n \t\t}\n+\t} else {\n+\t\t// no need to evict blocks\n+\t\tcurrent_memory -= idx_t(-required_memory);\n \t}\n-\t// re-allocate the buffer size and update its memory usage\n-\thandle->buffer->Resize(alloc_size);\n-\tif (required_memory < 0) {\n-\t\tcurrent_memory += required_memory;\n-\t}\n-\thandle->memory_usage = total_size;\n+\t// resize and adjust current memory\n+\thandle->buffer->Resize(block_size);\n+\thandle->memory_usage = alloc_size;\n }\n \n unique_ptr<BufferHandle> BufferManager::Pin(shared_ptr<BlockHandle> &handle) {\n@@ -228,7 +231,7 @@ unique_ptr<BufferHandle> BufferManager::Pin(shared_ptr<BlockHandle> &handle) {\n \t}\n \t// evict blocks until we have space for the current block\n \tif (!EvictBlocks(required_memory, maximum_memory)) {\n-\t\tthrow OutOfRangeException(\"Not enough memory to complete operation: failed to pin block\");\n+\t\tthrow OutOfMemoryException(\"failed to pin block of size %lld\", required_memory);\n \t}\n \t// lock the handle again and repeat the check (in case anybody loaded in the mean time)\n \tlock_guard<mutex> lock(handle->lock);\n@@ -236,6 +239,7 @@ unique_ptr<BufferHandle> BufferManager::Pin(shared_ptr<BlockHandle> &handle) {\n \tif (handle->state == BlockState::BLOCK_LOADED) {\n \t\t// the block is loaded, increment the reader count and return a pointer to the handle\n \t\thandle->readers++;\n+\t\tcurrent_memory -= required_memory;\n \t\treturn handle->Load(handle);\n \t}\n \t// now we can actually load the current block\n@@ -303,9 +307,8 @@ void BufferManager::SetLimit(idx_t limit) {\n \tlock_guard<mutex> buffer_lock(manager_lock);\n \t// try to evict until the limit is reached\n \tif (!EvictBlocks(0, limit)) {\n-\t\tthrow OutOfRangeException(\n-\t\t    \"Failed to change memory limit to new limit %lld: could not free up enough memory for the new limit\",\n-\t\t    limit);\n+\t\tthrow OutOfMemoryException(\n+\t\t    \"Failed to change memory limit to %lld: could not free up enough memory for the new limit\", limit);\n \t}\n \tidx_t old_limit = maximum_memory;\n \t// set the global maximum memory to the new limit if successful\n@@ -314,9 +317,8 @@ void BufferManager::SetLimit(idx_t limit) {\n \tif (!EvictBlocks(0, limit)) {\n \t\t// failed: go back to old limit\n \t\tmaximum_memory = old_limit;\n-\t\tthrow OutOfRangeException(\n-\t\t    \"Failed to change memory limit to new limit %lld: could not free up enough memory for the new limit\",\n-\t\t    limit);\n+\t\tthrow OutOfMemoryException(\n+\t\t    \"Failed to change memory limit to %lld: could not free up enough memory for the new limit\", limit);\n \t}\n }\n \n@@ -354,16 +356,19 @@ void BufferManager::WriteTemporaryBuffer(ManagedBuffer &buffer) {\n unique_ptr<FileBuffer> BufferManager::ReadTemporaryBuffer(block_id_t id) {\n \tD_ASSERT(!temp_directory.empty());\n \tD_ASSERT(temp_directory_handle.get());\n-\tidx_t alloc_size;\n+\tidx_t block_size;\n \t// open the temporary file and read the size\n \tauto path = GetTemporaryPath(id);\n \tauto &fs = FileSystem::GetFileSystem(db);\n \tauto handle = fs.OpenFile(path, FileFlags::FILE_FLAGS_READ);\n-\thandle->Read(&alloc_size, sizeof(idx_t), 0);\n+\thandle->Read(&block_size, sizeof(idx_t), 0);\n \n \t// now allocate a buffer of this size and read the data into that buffer\n-\tauto buffer = make_unique<ManagedBuffer>(db, alloc_size, false, id);\n+\tauto buffer = make_unique<ManagedBuffer>(db, block_size, false, id);\n \tbuffer->Read(*handle, sizeof(idx_t));\n+\n+\thandle.reset();\n+\tDeleteTemporaryFile(id);\n \treturn move(buffer);\n }\n \n",
  "test_patch": "diff --git a/test/sql/catalog/function/test_simple_macro.test b/test/sql/catalog/function/test_simple_macro.test\nindex c21ccf04d9b2..0b4be7011361 100644\n--- a/test/sql/catalog/function/test_simple_macro.test\n+++ b/test/sql/catalog/function/test_simple_macro.test\n@@ -28,6 +28,58 @@ SELECT one(NULL)\n statement ok\n DROP MACRO one;\n \n+# HAVING in a macro\n+statement ok\n+CREATE MACRO having_macro(x) AS (SELECT * FROM integers GROUP BY a HAVING a = x)\n+\n+query T\n+SELECT having_macro(1)\n+----\n+1\n+\n+query T\n+SELECT having_macro(6)\n+----\n+NULL\n+\n+# UNION in a macro\n+statement ok\n+CREATE MACRO union_macro(x, y, z) AS (SELECT x IN (SELECT y UNION ALL SELECT z))\n+\n+query T\n+SELECT union_macro(1, 2, 3)\n+----\n+false\n+\n+query T\n+SELECT union_macro(1, 2, 1)\n+----\n+true\n+\n+query T\n+SELECT union_macro(1, 1, 2)\n+----\n+true\n+\n+# expression list\n+statement ok\n+CREATE MACRO in_expression_list(x, y, z) AS (SELECT x IN (VALUES (y), (z)))\n+\n+query T\n+SELECT in_expression_list(1, 2, 3)\n+----\n+false\n+\n+query T\n+SELECT in_expression_list(1, 2, 1)\n+----\n+true\n+\n+query T\n+SELECT in_expression_list(1, 1, 2)\n+----\n+true\n+\n # FUNCTION alias\n statement ok\n CREATE FUNCTION two() AS (SELECT 2);\ndiff --git a/test/sql/order/test_order_nested.test b/test/sql/order/test_order_nested.test\nindex 28ffea1b04c0..d5aae6947d65 100644\n--- a/test/sql/order/test_order_nested.test\n+++ b/test/sql/order/test_order_nested.test\n@@ -5,18 +5,45 @@\n statement ok\n PRAGMA enable_verification\n \n-# first iteration runs externally, second runs normally\n+# test all integral types and varchar, internal and external\n statement ok\n PRAGMA force_external\n \n loop i 0 2\n \n+foreach type <integral> varchar\n+\n # list tests\n statement ok\n-CREATE TABLE test0 (i INT[])\n+CREATE TABLE test0 (i ${type}[])\n+\n+statement ok\n+INSERT INTO test0 VALUES ([2]), ([1]), ([1, 2]), ([]), ([2, 2]), ([NULL]), ([2, 3])\n+\n+query T\n+SELECT * FROM test0 ORDER BY i\n+----\n+[]\n+[1]\n+[1, 2]\n+[2]\n+[2, 2]\n+[2, 3]\n+[NULL]\n+\n+query T\n+SELECT * FROM test0 ORDER BY i DESC\n+----\n+[NULL]\n+[2, 3]\n+[2, 2]\n+[2]\n+[1, 2]\n+[1]\n+[]\n \n statement ok\n-INSERT INTO test0 VALUES ([2]), ([1]), ([1, 2]), ([]), ([2, 2]), ([NULL]), (NULL), ([2, 3])\n+INSERT INTO test0 VALUES (NULL)\n \n query T\n SELECT * FROM test0 ORDER BY i\n@@ -67,37 +94,7 @@ SELECT * FROM test0 ORDER BY i DESC NULLS LAST\n NULL\n \n statement ok\n-CREATE TABLE test1 (i VARCHAR[])\n-\n-statement ok\n-INSERT INTO test1 VALUES (['2']), (['1']), (['1', '2']), ([]), (['2', '2']), ([NULL]), (NULL), (['2', '3'])\n-\n-query T\n-SELECT * FROM test1 ORDER BY i\n-----\n-NULL\n-[]\n-[1]\n-[1, 2]\n-[2]\n-[2, 2]\n-[2, 3]\n-[NULL]\n-\n-query T\n-SELECT * FROM test1 ORDER BY i DESC\n-----\n-NULL\n-[NULL]\n-[2, 3]\n-[2, 2]\n-[2]\n-[1, 2]\n-[1]\n-[]\n-\n-statement ok\n-CREATE TABLE test2 (i INT[][])\n+CREATE TABLE test2 (i ${type}[][])\n \n statement ok\n INSERT INTO test2 VALUES ([[2]]), ([[1]]), ([NULL, []]), ([[1], [2]]), ([]), ([[]]), ([[2], [2]]), ([NULL]), (NULL), ([[2], [3]])\n@@ -131,7 +128,7 @@ NULL\n []\n \n statement ok\n-CREATE TABLE test3 (i INT[][])\n+CREATE TABLE test3 (i ${type}[][])\n \n statement ok\n INSERT INTO test3 VALUES ([[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]]), ([[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5], []]), ([[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5], [5, 5, 5, 5, 1]]), ([[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5], [5, 5, 5, 5, NULL]]), ([[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5], [5, 5, 5, 5, 2]]), ([[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5], [NULL]]), ([[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5], NULL])\n@@ -159,10 +156,10 @@ SELECT * FROM test3 ORDER BY i DESC\n [[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]]\n \n statement ok\n-CREATE TABLE test4 (i VARCHAR[][])\n+CREATE TABLE test4 (i ${type}[][])\n \n statement ok\n-INSERT INTO test4 VALUES ([[NULL, NULL, '0'], ['1', NULL, '1']]), ([[NULL, NULL, '1']]), ([[NULL, NULL, '0'], ['1', NULL, '0']]), ([[0, NULL, NULL, 1]]), ([[0, NULL, NULL, 0]])\n+INSERT INTO test4 VALUES ([[NULL, NULL, 0], [1, NULL, 1]]), ([[NULL, NULL, 1]]), ([[NULL, NULL, 0], [1, NULL, 0]]), ([[0, NULL, NULL, 1]]), ([[0, NULL, NULL, 0]])\n \n query T\n SELECT * FROM test4 ORDER BY i\n@@ -184,10 +181,18 @@ SELECT * FROM test4 ORDER BY i DESC\n \n # struct tests\n statement ok\n-CREATE TABLE test10 (s STRUCT(i INT, j INT));\n+CREATE TABLE test10 (s STRUCT(i ${type}, j ${type}));\n \n statement ok\n-INSERT INTO test10 values ({'i': 2, 'j': NULL}), ({'i': 1, 'j': NULL}), ({'i': 2, 'j': 3}), ({'i': 1, 'j': 2}), ({'i': 2, 'j': 2}), ({'i': NULL, 'j': NULL}), (NULL), ({'i': 2, 'j': 3})\n+INSERT INTO test10 values\n+({'i': 2, 'j': NULL}),\n+({'i': 1, 'j': NULL}),\n+({'i': 2, 'j': 3}),\n+({'i': 1, 'j': 2}),\n+({'i': 2, 'j': 2}),\n+({'i': NULL, 'j': NULL}),\n+(NULL),\n+({'i': 2, 'j': 3})\n \n query T\n SELECT * FROM test10 ORDER BY s\n@@ -214,7 +219,7 @@ NULL\n {'i': 1, 'j': 2}\n \n statement ok\n-CREATE TABLE test11 (s STRUCT(s1 STRUCT(i INT, j INT)))\n+CREATE TABLE test11 (s STRUCT(s1 STRUCT(i ${type}, j ${type})))\n \n statement ok\n INSERT INTO test11 VALUES ({'s1': NULL}), (NULL), ({'s1': {'i': NULL, 'j': 1}}), ({'s1': {'i': 0, 'j': NULL}}), ({'s1': {'i': NULL, 'j': NULL}}), ({'s1': {'i': 0, 'j': 1}})\n@@ -240,98 +245,40 @@ NULL\n {'s1': {'i': 0, 'j': 1}}\n \n statement ok\n-CREATE TABLE test12 (s STRUCT(s1 STRUCT(i VARCHAR, j VARCHAR)))\n-\n-statement ok\n-INSERT INTO test12 VALUES ({'s1': NULL}), (NULL), ({'s1': {'i': NULL, 'j': '1'}}), ({'s1': {'i': '0', 'j': NULL}}), ({'s1': {'i': NULL, 'j': NULL}}), ({'s1': {'i': '0', 'j': '1'}})\n-\n-query T\n-SELECT * FROM test12 ORDER BY s\n-----\n-NULL\n-{'s1': {'i': 0, 'j': 1}}\n-{'s1': {'i': 0, 'j': NULL}}\n-{'s1': {'i': NULL, 'j': 1}}\n-{'s1': {'i': NULL, 'j': NULL}}\n-{'s1': NULL}\n-\n-query T\n-SELECT * FROM test12 ORDER BY s DESC\n-----\n-NULL\n-{'s1': NULL}\n-{'s1': {'i': NULL, 'j': NULL}}\n-{'s1': {'i': NULL, 'j': 1}}\n-{'s1': {'i': 0, 'j': NULL}}\n-{'s1': {'i': 0, 'j': 1}}\n-\n-statement ok\n-CREATE TABLE test13 (s STRUCT(a INT, b INT, c INT, d INT, e INT, f INT, g INT, h INT, i INT, j INT))\n+CREATE TABLE test13 (s STRUCT(a ${type}, b ${type}, c ${type}, d ${type}, e ${type}, f ${type}, g ${type}, h ${type}, i ${type}, j ${type}))\n \n statement ok\n INSERT INTO test13 VALUES\n+({'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 8}),\n ({'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9}),\n-({'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 10}),\n+({'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': NULL, 'f': NULL, 'g': NULL, 'h': NULL, 'i': 8, 'j': 8}),\n ({'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': NULL, 'f': NULL, 'g': NULL, 'h': NULL, 'i': 8, 'j': 9}),\n-({'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': NULL, 'f': NULL, 'g': NULL, 'h': NULL, 'i': 8, 'j': 10}),\n-({'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': NULL, 'i': NULL, 'j': 9}),\n-({'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': NULL, 'i': NULL, 'j': 10})\n+({'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': NULL, 'i': NULL, 'j': 8}),\n+({'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': NULL, 'i': NULL, 'j': 9})\n \n query T\n SELECT * FROM test13 ORDER BY s\n ----\n+{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 8}\n {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9}\n-{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 10}\n+{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': NULL, 'i': NULL, 'j': 8}\n {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': NULL, 'i': NULL, 'j': 9}\n-{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': NULL, 'i': NULL, 'j': 10}\n+{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': NULL, 'f': NULL, 'g': NULL, 'h': NULL, 'i': 8, 'j': 8}\n {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': NULL, 'f': NULL, 'g': NULL, 'h': NULL, 'i': 8, 'j': 9}\n-{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': NULL, 'f': NULL, 'g': NULL, 'h': NULL, 'i': 8, 'j': 10}\n \n query T\n SELECT * FROM test13 ORDER BY s DESC\n ----\n-{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': NULL, 'f': NULL, 'g': NULL, 'h': NULL, 'i': 8, 'j': 10}\n {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': NULL, 'f': NULL, 'g': NULL, 'h': NULL, 'i': 8, 'j': 9}\n-{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': NULL, 'i': NULL, 'j': 10}\n+{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': NULL, 'f': NULL, 'g': NULL, 'h': NULL, 'i': 8, 'j': 8}\n {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': NULL, 'i': NULL, 'j': 9}\n-{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 10}\n+{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': NULL, 'i': NULL, 'j': 8}\n {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9}\n-\n-statement ok\n-CREATE TABLE test14 (s STRUCT(a VARCHAR, b VARCHAR, c VARCHAR, d VARCHAR, e VARCHAR, f VARCHAR, g VARCHAR, h VARCHAR, i VARCHAR, j VARCHAR))\n-\n-statement ok\n-INSERT INTO test14 VALUES\n-({'a': '0', 'b': '1', 'c': '2', 'd': '3', 'e': '4', 'f': '5', 'g': '6', 'h': '7', 'i': '8', 'j': '9'}),\n-({'a': '0', 'b': '1', 'c': '2', 'd': '3', 'e': '4', 'f': '5', 'g': '6', 'h': '7', 'i': '8', 'j': '10'}),\n-({'a': '0', 'b': '1', 'c': '2', 'd': '3', 'e': NULL, 'f': NULL, 'g': NULL, 'h': NULL, 'i': '8', 'j': '9'}),\n-({'a': '0', 'b': '1', 'c': '2', 'd': '3', 'e': NULL, 'f': NULL, 'g': NULL, 'h': NULL, 'i': '8', 'j': '10'}),\n-({'a': '0', 'b': '1', 'c': '2', 'd': '3', 'e': '4', 'f': '5', 'g': '6', 'h': NULL, 'i': NULL, 'j': '9'}),\n-({'a': '0', 'b': '1', 'c': '2', 'd': '3', 'e': '4', 'f': '5', 'g': '6', 'h': NULL, 'i': NULL, 'j': '10'})\n-\n-query T\n-SELECT * FROM test14 ORDER BY s\n-----\n-{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 10}\n-{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9}\n-{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': NULL, 'i': NULL, 'j': 10}\n-{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': NULL, 'i': NULL, 'j': 9}\n-{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': NULL, 'f': NULL, 'g': NULL, 'h': NULL, 'i': 8, 'j': 10}\n-{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': NULL, 'f': NULL, 'g': NULL, 'h': NULL, 'i': 8, 'j': 9}\n-\n-query T\n-SELECT * FROM test14 ORDER BY s DESC\n-----\n-{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': NULL, 'f': NULL, 'g': NULL, 'h': NULL, 'i': 8, 'j': 9}\n-{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': NULL, 'f': NULL, 'g': NULL, 'h': NULL, 'i': 8, 'j': 10}\n-{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': NULL, 'i': NULL, 'j': 9}\n-{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': NULL, 'i': NULL, 'j': 10}\n-{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9}\n-{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 10}\n+{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 8}\n \n # combination of lists and structs\n statement ok\n-CREATE TABLE test20 (l STRUCT(i INT)[])\n+CREATE TABLE test20 (l STRUCT(i ${type})[])\n \n statement ok\n INSERT INTO test20 VALUES\n@@ -369,45 +316,7 @@ SELECT * FROM test20 ORDER BY l DESC\n [{'i': NULL}, {'i': NULL}, {'i': 0}, {'i': NULL}, {'i': NULL}, {'i': 0}]\n \n statement ok\n-CREATE TABLE test21 (l STRUCT(i VARCHAR)[])\n-\n-statement ok\n-INSERT INTO test21 VALUES\n-([{'i': NULL}, {'i': NULL}, {'i': '0'}, {'i': NULL}, {'i': NULL}, {'i': '0'}]),\n-([{'i': NULL}, {'i': NULL}, {'i': '0'}, {'i': NULL}, {'i': NULL}, {'i': '1'}]),\n-([{'i': NULL}, {'i': NULL}, {'i': NULL}, {'i': NULL}, {'i': NULL}, {'i': '1'}]),\n-([{'i': NULL}, {'i': NULL}, {'i': '0'}, {'i': NULL}, {'i': NULL}, {'i': NULL}]),\n-([{'i': NULL}, {'i': NULL}, {'i': '1'}, {'i': NULL}, {'i': NULL}, {'i': '0'}]),\n-([{'i': NULL}, {'i': NULL}, {'i': '1'}, {'i': NULL}, {'i': NULL}, {'i': '1'}]),\n-([{'i': NULL}, {'i': NULL}, {'i': NULL}, {'i': NULL}, {'i': NULL}, {'i': '0'}]),\n-([{'i': NULL}, {'i': NULL}, {'i': '1'}, {'i': NULL}, {'i': NULL}, {'i': NULL}])\n-\n-query T\n-SELECT * FROM test21 ORDER BY l\n-----\n-[{'i': NULL}, {'i': NULL}, {'i': 0}, {'i': NULL}, {'i': NULL}, {'i': 0}]\n-[{'i': NULL}, {'i': NULL}, {'i': 0}, {'i': NULL}, {'i': NULL}, {'i': 1}]\n-[{'i': NULL}, {'i': NULL}, {'i': 0}, {'i': NULL}, {'i': NULL}, {'i': NULL}]\n-[{'i': NULL}, {'i': NULL}, {'i': 1}, {'i': NULL}, {'i': NULL}, {'i': 0}]\n-[{'i': NULL}, {'i': NULL}, {'i': 1}, {'i': NULL}, {'i': NULL}, {'i': 1}]\n-[{'i': NULL}, {'i': NULL}, {'i': 1}, {'i': NULL}, {'i': NULL}, {'i': NULL}]\n-[{'i': NULL}, {'i': NULL}, {'i': NULL}, {'i': NULL}, {'i': NULL}, {'i': 0}]\n-[{'i': NULL}, {'i': NULL}, {'i': NULL}, {'i': NULL}, {'i': NULL}, {'i': 1}]\n-\n-query T\n-SELECT * FROM test21 ORDER BY l DESC\n-----\n-[{'i': NULL}, {'i': NULL}, {'i': NULL}, {'i': NULL}, {'i': NULL}, {'i': 1}]\n-[{'i': NULL}, {'i': NULL}, {'i': NULL}, {'i': NULL}, {'i': NULL}, {'i': 0}]\n-[{'i': NULL}, {'i': NULL}, {'i': 1}, {'i': NULL}, {'i': NULL}, {'i': NULL}]\n-[{'i': NULL}, {'i': NULL}, {'i': 1}, {'i': NULL}, {'i': NULL}, {'i': 1}]\n-[{'i': NULL}, {'i': NULL}, {'i': 1}, {'i': NULL}, {'i': NULL}, {'i': 0}]\n-[{'i': NULL}, {'i': NULL}, {'i': 0}, {'i': NULL}, {'i': NULL}, {'i': NULL}]\n-[{'i': NULL}, {'i': NULL}, {'i': 0}, {'i': NULL}, {'i': NULL}, {'i': 1}]\n-[{'i': NULL}, {'i': NULL}, {'i': 0}, {'i': NULL}, {'i': NULL}, {'i': 0}]\n-\n-statement ok\n-CREATE TABLE test22 (s STRUCT(i INT[], j INT[]))\n+CREATE TABLE test22 (s STRUCT(i ${type}[], j ${type}[]))\n \n statement ok\n INSERT INTO test22 VALUES\n@@ -445,45 +354,7 @@ SELECT * FROM test22 ORDER BY s DESC\n {'i': [0, 2], 'j': [2, NULL]}\n \n statement ok\n-CREATE TABLE test23 (s STRUCT(i VARCHAR[], j VARCHAR[]))\n-\n-statement ok\n-INSERT INTO test23 VALUES\n-({'i': ['0', NULL], 'j': [NULL, '0']}),\n-({'i': ['0', NULL], 'j': [NULL, '1']}),\n-({'i': ['0', NULL], 'j': ['1', '2']}),\n-({'i': ['1', NULL], 'j': ['2', NULL]}),\n-({'i': ['0', '2'], 'j': ['2', NULL]}),\n-({'i': NULL, 'j': ['2', NULL]}),\n-({'i': ['0', '2'], 'j': NULL}),\n-({'i': NULL, 'j': ['2', '3']})\n-\n-query T\n-SELECT * FROM test23 ORDER BY s\n-----\n-{'i': [0, 2], 'j': [2, NULL]}\n-{'i': [0, 2], 'j': NULL}\n-{'i': [0, NULL], 'j': [1, 2]}\n-{'i': [0, NULL], 'j': [NULL, 0]}\n-{'i': [0, NULL], 'j': [NULL, 1]}\n-{'i': [1, NULL], 'j': [2, NULL]}\n-{'i': NULL, 'j': [2, 3]}\n-{'i': NULL, 'j': [2, NULL]}\n-\n-query T\n-SELECT * FROM test23 ORDER BY s DESC\n-----\n-{'i': NULL, 'j': [2, NULL]}\n-{'i': NULL, 'j': [2, 3]}\n-{'i': [1, NULL], 'j': [2, NULL]}\n-{'i': [0, NULL], 'j': [NULL, 1]}\n-{'i': [0, NULL], 'j': [NULL, 0]}\n-{'i': [0, NULL], 'j': [1, 2]}\n-{'i': [0, 2], 'j': NULL}\n-{'i': [0, 2], 'j': [2, NULL]}\n-\n-statement ok\n-CREATE TABLE test24 (l STRUCT(l1 STRUCT(i INT)[])[])\n+CREATE TABLE test24 (l STRUCT(l1 STRUCT(i ${type})[])[])\n \n statement ok\n INSERT INTO test24 VALUES\n@@ -536,107 +407,337 @@ SELECT * FROM test24 ORDER BY l DESC\n [{'l1': [{'i': 0}]}, {'l1': [{'i': 0}]}]\n \n statement ok\n-CREATE TABLE test25 (l STRUCT(l1 STRUCT(i VARCHAR)[])[])\n+DROP TABLE test0\n+\n+statement ok\n+DROP TABLE test2\n \n statement ok\n-INSERT INTO test25 VALUES\n-([{'l1': [{'i': '0'}]}, {'l1': [{'i': '0'}]}]),\n-([{'l1': [{'i': '0'}]}, {'l1': [{'i': '1'}]}]),\n-([{'l1': [{'i': '0'}]}, {'l1': [{'i': '1'}, {'i': '2'}]}]),\n-([{'l1': [{'i': '0'}]}, {'l1': [{'i': '1'}, {'i': '1'}]}]),\n-([{'l1': [{'i': '1'}]}, {'l1': [{'i': '0'}]}]),\n-([{'l1': [{'i': '1'}]}, {'l1': [{'i': '1'}]}]),\n-([{'l1': [{'i': NULL}]}, {'l1': [{'i': '0'}]}]),\n-([{'l1': [{'i': NULL}]}, {'l1': [{'i': '1'}]}]),\n-([{'l1': [{'i': '0'}]}, {'l1': [{'i': NULL}]}, {'l1': [{'i': NULL}]}]),\n-([{'l1': [{'i': '1'}]}, {'l1': [{'i': NULL}]}, {'l1': [{'i': '1'}]}]),\n-([{'l1': [{'i': '0'}]}, {'l1': [{'i': NULL}]}]),\n-([{'l1': [{'i': '1'}]}, {'l1': [{'i': NULL}]}]),\n-([{'l1': [{'i': '1'}]}, {'l1': [{'i': NULL}]}, {'l1': [{'i': NULL}]}])\n+DROP TABLE test3\n+\n+statement ok\n+DROP TABLE test4\n+\n+statement ok\n+DROP TABLE test10\n+\n+statement ok\n+DROP TABLE test11\n+\n+statement ok\n+DROP TABLE test13\n+\n+statement ok\n+DROP TABLE test20\n+\n+statement ok\n+DROP TABLE test22\n+\n+statement ok\n+DROP TABLE test24\n+\n+statement ok\n+PRAGMA disable_force_external\n+\n+endloop\n+\n+# test floating point types, internal and external\n+statement ok\n+PRAGMA force_external\n+\n+loop i 0 2\n+\n+foreach type float double\n+\n+statement ok\n+CREATE TABLE test2 (i ${type}[][])\n+\n+statement ok\n+INSERT INTO test2 VALUES ([[1.2]]), ([[1.1]]), ([NULL, []]), ([[1.1], [1.2]]), ([]), ([[]]), ([[1.2], [1.2]]), ([NULL]), (NULL), ([[1.2], [1.3]])\n \n query T\n-SELECT * FROM test25 ORDER BY l\n+SELECT * FROM test2 ORDER BY i\n ----\n-[{'l1': [{'i': 0}]}, {'l1': [{'i': 0}]}]\n-[{'l1': [{'i': 0}]}, {'l1': [{'i': 1}]}]\n-[{'l1': [{'i': 0}]}, {'l1': [{'i': 1}, {'i': 1}]}]\n-[{'l1': [{'i': 0}]}, {'l1': [{'i': 1}, {'i': 2}]}]\n-[{'l1': [{'i': 0}]}, {'l1': [{'i': NULL}]}]\n-[{'l1': [{'i': 0}]}, {'l1': [{'i': NULL}]}, {'l1': [{'i': NULL}]}]\n-[{'l1': [{'i': 1}]}, {'l1': [{'i': 0}]}]\n-[{'l1': [{'i': 1}]}, {'l1': [{'i': 1}]}]\n-[{'l1': [{'i': 1}]}, {'l1': [{'i': NULL}]}]\n-[{'l1': [{'i': 1}]}, {'l1': [{'i': NULL}]}, {'l1': [{'i': 1}]}]\n-[{'l1': [{'i': 1}]}, {'l1': [{'i': NULL}]}, {'l1': [{'i': NULL}]}]\n-[{'l1': [{'i': NULL}]}, {'l1': [{'i': 0}]}]\n-[{'l1': [{'i': NULL}]}, {'l1': [{'i': 1}]}]\n+NULL\n+[]\n+[[]]\n+[[1.100000]]\n+[[1.100000], [1.200000]]\n+[[1.200000]]\n+[[1.200000], [1.200000]]\n+[[1.200000], [1.300000]]\n+[NULL]\n+[NULL, []]\n \n query T\n-SELECT * FROM test25 ORDER BY l DESC\n+SELECT * FROM test2 ORDER BY i DESC\n ----\n-[{'l1': [{'i': NULL}]}, {'l1': [{'i': 1}]}]\n-[{'l1': [{'i': NULL}]}, {'l1': [{'i': 0}]}]\n-[{'l1': [{'i': 1}]}, {'l1': [{'i': NULL}]}, {'l1': [{'i': NULL}]}]\n-[{'l1': [{'i': 1}]}, {'l1': [{'i': NULL}]}, {'l1': [{'i': 1}]}]\n-[{'l1': [{'i': 1}]}, {'l1': [{'i': NULL}]}]\n-[{'l1': [{'i': 1}]}, {'l1': [{'i': 1}]}]\n-[{'l1': [{'i': 1}]}, {'l1': [{'i': 0}]}]\n-[{'l1': [{'i': 0}]}, {'l1': [{'i': NULL}]}, {'l1': [{'i': NULL}]}]\n-[{'l1': [{'i': 0}]}, {'l1': [{'i': NULL}]}]\n-[{'l1': [{'i': 0}]}, {'l1': [{'i': 1}, {'i': 2}]}]\n-[{'l1': [{'i': 0}]}, {'l1': [{'i': 1}, {'i': 1}]}]\n-[{'l1': [{'i': 0}]}, {'l1': [{'i': 1}]}]\n-[{'l1': [{'i': 0}]}, {'l1': [{'i': 0}]}]\n+NULL\n+[NULL, []]\n+[NULL]\n+[[1.200000], [1.300000]]\n+[[1.200000], [1.200000]]\n+[[1.200000]]\n+[[1.100000], [1.200000]]\n+[[1.100000]]\n+[[]]\n+[]\n \n statement ok\n-DROP TABLE test0\n+CREATE TABLE test10 (s STRUCT(i ${type}, j ${type}));\n \n statement ok\n-DROP TABLE test1\n+INSERT INTO test10 values\n+({'i': 1.2, 'j': NULL}),\n+({'i': 1.1, 'j': NULL}),\n+({'i': 1.2, 'j': 1.3}),\n+({'i': 1.1, 'j': 1.2}),\n+({'i': 1.2, 'j': 1.2}),\n+({'i': NULL, 'j': NULL}),\n+(NULL),\n+({'i': 1.2, 'j': 1.3})\n+\n+query T\n+SELECT * FROM test10 ORDER BY s\n+----\n+NULL\n+{'i': 1.100000, 'j': 1.200000}\n+{'i': 1.100000, 'j': NULL}\n+{'i': 1.200000, 'j': 1.200000}\n+{'i': 1.200000, 'j': 1.300000}\n+{'i': 1.200000, 'j': 1.300000}\n+{'i': 1.200000, 'j': NULL}\n+{'i': NULL, 'j': NULL}\n+\n+query T\n+SELECT * FROM test10 ORDER BY s DESC\n+----\n+NULL\n+{'i': NULL, 'j': NULL}\n+{'i': 1.200000, 'j': NULL}\n+{'i': 1.200000, 'j': 1.300000}\n+{'i': 1.200000, 'j': 1.300000}\n+{'i': 1.200000, 'j': 1.200000}\n+{'i': 1.100000, 'j': NULL}\n+{'i': 1.100000, 'j': 1.200000}\n \n statement ok\n-DROP TABLE test2\n+CREATE TABLE test24 (l STRUCT(l1 STRUCT(i ${type})[])[])\n \n statement ok\n-DROP TABLE test3\n+INSERT INTO test24 VALUES\n+([{'l1': [{'i': 1.1}]}, {'l1': [{'i': 1.1}]}]),\n+([{'l1': [{'i': 1.1}]}, {'l1': [{'i': 1.2}]}]),\n+([{'l1': [{'i': 1.1}]}, {'l1': [{'i': 1.2}, {'i': 1.3}]}]),\n+([{'l1': [{'i': 1.1}]}, {'l1': [{'i': 1.2}, {'i': 1.2}]}]),\n+([{'l1': [{'i': 1.2}]}, {'l1': [{'i': 1.1}]}]),\n+([{'l1': [{'i': 1.2}]}, {'l1': [{'i': 1.2}]}]),\n+([{'l1': [{'i': NULL}]}, {'l1': [{'i': 1.1}]}]),\n+([{'l1': [{'i': NULL}]}, {'l1': [{'i': 1.2}]}]),\n+([{'l1': [{'i': 1.1}]}, {'l1': [{'i': NULL}]}, {'l1': [{'i': NULL}]}]),\n+([{'l1': [{'i': 1.2}]}, {'l1': [{'i': NULL}]}, {'l1': [{'i': 1.2}]}]),\n+([{'l1': [{'i': 1.1}]}, {'l1': [{'i': NULL}]}]),\n+([{'l1': [{'i': 1.2}]}, {'l1': [{'i': NULL}]}]),\n+([{'l1': [{'i': 1.2}]}, {'l1': [{'i': NULL}]}, {'l1': [{'i': NULL}]}])\n+\n+query T\n+SELECT * FROM test24 ORDER BY l\n+----\n+[{'l1': [{'i': 1.100000}]}, {'l1': [{'i': 1.100000}]}]\n+[{'l1': [{'i': 1.100000}]}, {'l1': [{'i': 1.200000}]}]\n+[{'l1': [{'i': 1.100000}]}, {'l1': [{'i': 1.200000}, {'i': 1.200000}]}]\n+[{'l1': [{'i': 1.100000}]}, {'l1': [{'i': 1.200000}, {'i': 1.300000}]}]\n+[{'l1': [{'i': 1.100000}]}, {'l1': [{'i': NULL}]}]\n+[{'l1': [{'i': 1.100000}]}, {'l1': [{'i': NULL}]}, {'l1': [{'i': NULL}]}]\n+[{'l1': [{'i': 1.200000}]}, {'l1': [{'i': 1.100000}]}]\n+[{'l1': [{'i': 1.200000}]}, {'l1': [{'i': 1.200000}]}]\n+[{'l1': [{'i': 1.200000}]}, {'l1': [{'i': NULL}]}]\n+[{'l1': [{'i': 1.200000}]}, {'l1': [{'i': NULL}]}, {'l1': [{'i': 1.200000}]}]\n+[{'l1': [{'i': 1.200000}]}, {'l1': [{'i': NULL}]}, {'l1': [{'i': NULL}]}]\n+[{'l1': [{'i': NULL}]}, {'l1': [{'i': 1.100000}]}]\n+[{'l1': [{'i': NULL}]}, {'l1': [{'i': 1.200000}]}]\n+\n+query T\n+SELECT * FROM test24 ORDER BY l DESC\n+----\n+[{'l1': [{'i': NULL}]}, {'l1': [{'i': 1.200000}]}]\n+[{'l1': [{'i': NULL}]}, {'l1': [{'i': 1.100000}]}]\n+[{'l1': [{'i': 1.200000}]}, {'l1': [{'i': NULL}]}, {'l1': [{'i': NULL}]}]\n+[{'l1': [{'i': 1.200000}]}, {'l1': [{'i': NULL}]}, {'l1': [{'i': 1.200000}]}]\n+[{'l1': [{'i': 1.200000}]}, {'l1': [{'i': NULL}]}]\n+[{'l1': [{'i': 1.200000}]}, {'l1': [{'i': 1.200000}]}]\n+[{'l1': [{'i': 1.200000}]}, {'l1': [{'i': 1.100000}]}]\n+[{'l1': [{'i': 1.100000}]}, {'l1': [{'i': NULL}]}, {'l1': [{'i': NULL}]}]\n+[{'l1': [{'i': 1.100000}]}, {'l1': [{'i': NULL}]}]\n+[{'l1': [{'i': 1.100000}]}, {'l1': [{'i': 1.200000}, {'i': 1.300000}]}]\n+[{'l1': [{'i': 1.100000}]}, {'l1': [{'i': 1.200000}, {'i': 1.200000}]}]\n+[{'l1': [{'i': 1.100000}]}, {'l1': [{'i': 1.200000}]}]\n+[{'l1': [{'i': 1.100000}]}, {'l1': [{'i': 1.100000}]}]\n \n statement ok\n-DROP TABLE test4\n+DROP TABLE test2\n \n statement ok\n DROP TABLE test10\n \n statement ok\n-DROP TABLE test11\n+DROP TABLE test24\n \n statement ok\n-DROP TABLE test12\n+PRAGMA disable_force_external\n+\n+endloop\n \n+# test bool\n statement ok\n-DROP TABLE test13\n+CREATE TABLE test0 (i BOOL[])\n \n statement ok\n-DROP TABLE test14\n+INSERT INTO test0 VALUES ([True]), ([False]), ([False, True]), ([]), ([True, True]), ([NULL]), (NULL), ([False, False])\n+\n+query T\n+SELECT * FROM test0 ORDER BY i\n+----\n+NULL\n+[]\n+[False]\n+[False, False]\n+[False, True]\n+[True]\n+[True, True]\n+[NULL]\n+\n+query T\n+SELECT * FROM test0 ORDER BY i DESC\n+----\n+NULL\n+[NULL]\n+[True, True]\n+[True]\n+[False, True]\n+[False, False]\n+[False]\n+[]\n \n statement ok\n-DROP TABLE test20\n+CREATE TABLE test10 (s STRUCT(i BOOL, j BOOL));\n+\n+statement ok\n+INSERT INTO test10 values\n+({'i': True, 'j': NULL}),\n+({'i': False, 'j': NULL}),\n+({'i': True, 'j': True}),\n+({'i': False, 'j': True}),\n+({'i': NULL, 'j': False}),\n+({'i': NULL, 'j': True}),\n+({'i': NULL, 'j': NULL}),\n+(NULL),\n+({'i': False, 'j': False})\n+\n+query T\n+SELECT * FROM test10 ORDER BY s\n+----\n+NULL\n+{'i': False, 'j': False}\n+{'i': False, 'j': True}\n+{'i': False, 'j': NULL}\n+{'i': True, 'j': True}\n+{'i': True, 'j': NULL}\n+{'i': NULL, 'j': False}\n+{'i': NULL, 'j': True}\n+{'i': NULL, 'j': NULL}\n+\n+query T\n+SELECT * FROM test10 ORDER BY s DESC\n+----\n+NULL\n+{'i': NULL, 'j': NULL}\n+{'i': NULL, 'j': True}\n+{'i': NULL, 'j': False}\n+{'i': True, 'j': NULL}\n+{'i': True, 'j': True}\n+{'i': False, 'j': NULL}\n+{'i': False, 'j': True}\n+{'i': False, 'j': False}\n \n statement ok\n-DROP TABLE test21\n+DROP TABLE test0\n \n statement ok\n-DROP TABLE test22\n+DROP TABLE test10\n \n+# test interval\n statement ok\n-DROP TABLE test23\n+CREATE TABLE test0 (i INTERVAL[])\n \n statement ok\n-DROP TABLE test24\n+INSERT INTO test0 VALUES (['2 years']), (['1 year']), (['1 year', '2 years']), ([]), (['2 years', '2 years']), ([NULL]), (NULL), (['1 year', '1 year'])\n+\n+query T\n+SELECT * FROM test0 ORDER BY i\n+----\n+NULL\n+[]\n+[1 year]\n+[1 year, 1 year]\n+[1 year, 2 years]\n+[2 years]\n+[2 years, 2 years]\n+[NULL]\n+\n+query T\n+SELECT * FROM test0 ORDER BY i DESC\n+----\n+NULL\n+[NULL]\n+[2 years, 2 years]\n+[2 years]\n+[1 year, 2 years]\n+[1 year, 1 year]\n+[1 year]\n+[]\n \n statement ok\n-DROP TABLE test25\n+CREATE TABLE test10 (s STRUCT(i INTERVAL, j INTERVAL));\n \n statement ok\n-PRAGMA disable_force_external\n+INSERT INTO test10 values\n+({'i': '2 years', 'j': NULL}),\n+({'i': '1 year', 'j': NULL}),\n+({'i': '2 years', 'j': '2 years'}),\n+({'i': '1 year', 'j': '2 years'}),\n+({'i': '2 years', 'j': '1 year'}),\n+({'i': NULL, 'j': '1 year'}),\n+({'i': NULL, 'j': '2 years'}),\n+({'i': NULL, 'j': NULL}),\n+(NULL),\n+({'i': '1 year', 'j': '1 year'})\n \n-endloop\n+query T\n+SELECT * FROM test10 ORDER BY s\n+----\n+NULL\n+{'i': 1 year, 'j': 1 year}\n+{'i': 1 year, 'j': 2 years}\n+{'i': 1 year, 'j': NULL}\n+{'i': 2 years, 'j': 1 year}\n+{'i': 2 years, 'j': 2 years}\n+{'i': 2 years, 'j': NULL}\n+{'i': NULL, 'j': 1 year}\n+{'i': NULL, 'j': 2 years}\n+{'i': NULL, 'j': NULL}\n+\n+\n+query T\n+SELECT * FROM test10 ORDER BY s DESC\n+----\n+NULL\n+{'i': NULL, 'j': NULL}\n+{'i': NULL, 'j': 2 years}\n+{'i': NULL, 'j': 1 year}\n+{'i': 2 years, 'j': NULL}\n+{'i': 2 years, 'j': 2 years}\n+{'i': 2 years, 'j': 1 year}\n+{'i': 1 year, 'j': NULL}\n+{'i': 1 year, 'j': 2 years}\n+{'i': 1 year, 'j': 1 year}\ndiff --git a/test/sql/order/test_order_parallel.test_coverage b/test/sql/order/test_order_parallel.test_coverage\nindex a98c02e6b5fb..e4cd2f6da4aa 100644\n--- a/test/sql/order/test_order_parallel.test_coverage\n+++ b/test/sql/order/test_order_parallel.test_coverage\n@@ -5,14 +5,33 @@\n statement ok\n PRAGMA force_parallelism\n \n+# we run this with an uneven amount of threads to force the merge sort to have an odd amount of blocks to merge\n statement ok\n-PRAGMA threads=4\n+PRAGMA threads=3\n \n-# first iteration runs externally, second runs normally\n+# do one asc test to hit some coverage, the rest is desc\n statement ok\n-PRAGMA force_external\n+create table test as (select range i from range(100000) order by i desc);\n \n-loop i 0 2\n+# all fixed-size\n+query T\n+select * from test order by i asc;\n+----\n+100000 values hashing to 1933b84f18ddb7545c63962be5d10bb5\n+\n+statement ok\n+drop table test\n+\n+# internal/external, different memory limits\n+foreach pragma force_external disable_force_external\n+\n+foreach mem 50 500\n+\n+statement ok\n+PRAGMA ${pragma}\n+\n+statement ok\n+PRAGMA memory_limit='${mem}MB'\n \n statement ok\n create table test as (select range i from range(100000));\n@@ -144,7 +163,4 @@ select * from test order by cast(i % 1000 + 1000000000000 as varchar), i % 100,\n statement ok\n drop table test\n \n-statement ok\n-PRAGMA disable_force_external\n-\n endloop\ndiff --git a/test/sql/order/test_order_variable_size_payload.test b/test/sql/order/test_order_variable_size_payload.test_coverage\nsimilarity index 97%\nrename from test/sql/order/test_order_variable_size_payload.test\nrename to test/sql/order/test_order_variable_size_payload.test_coverage\nindex 88e7cb0698ec..6bd2a526d21d 100644\n--- a/test/sql/order/test_order_variable_size_payload.test\n+++ b/test/sql/order/test_order_variable_size_payload.test_coverage\n@@ -1,4 +1,4 @@\n-# name: test/sql/order/test_order_variable_size_payload.test\n+# name: test/sql/order/test_order_variable_size_payload.test_coverage\n # description: Test ORDER BY keyword (variable size sorting/payload columns)\n # group: [order]\n \n@@ -13,7 +13,7 @@ loop i 0 2\n \n # string that is longer than Storage::BLOCK_SIZE - this tests whether RowDataCollection dynamically increases block capacity\n statement ok\n-select range % 2 i, string_agg('thisstringis200characterslong..........................................................................................................................................................................') s from range(100000) group by i order by i\n+select range % 2 i, string_agg('thisstringis200characterslong..........................................................................................................................................................................') s from range(5000) group by i order by i\n \n # weird one from SQLancer\n statement ok\n@@ -50,6 +50,16 @@ Shipping and Receiving Clerk\tBerndt\n Shipping and Receiving Clerk\tKuppa\n Shipping and Receiving Supervisor\tAckerman\n \n+query TT\n+SELECT * FROM test0 ORDER BY job DESC, name DESC\n+----\n+Shipping and Receiving Supervisor\tAckerman\n+Shipping and Receiving Clerk\tKuppa\n+Shipping and Receiving Clerk\tBerndt\n+Production Supervisor - WC60\tCampbell\n+Production Supervisor - WC60\tBrown\n+Production Supervisor - WC40\tDsa\n+\n statement ok\n CREATE TABLE test1 (s VARCHAR)\n \ndiff --git a/test/sql/storage/test_buffer_manager.cpp b/test/sql/storage/test_buffer_manager.cpp\nindex 6f8a1be9919c..314e6a93366c 100644\n--- a/test/sql/storage/test_buffer_manager.cpp\n+++ b/test/sql/storage/test_buffer_manager.cpp\n@@ -1,7 +1,8 @@\n #include \"catch.hpp\"\n #include \"duckdb/common/file_system.hpp\"\n-#include \"test_helpers.hpp\"\n+#include \"duckdb/storage/buffer_manager.hpp\"\n #include \"duckdb/storage/storage_info.hpp\"\n+#include \"test_helpers.hpp\"\n \n using namespace duckdb;\n using namespace std;\n@@ -226,3 +227,52 @@ TEST_CASE(\"Modifying the buffer manager limit at runtime for an in-memory databa\n \tREQUIRE_NO_FAIL(con.Query(\"DROP TABLE test\"));\n \tREQUIRE_NO_FAIL(con.Query(\"PRAGMA memory_limit='1MB'\"));\n }\n+\n+TEST_CASE(\"Test buffer reallocation\", \"[storage][.]\") {\n+\tauto storage_database = TestCreatePath(\"storage_test\");\n+\tauto config = GetTestConfig();\n+\t// make sure the database does not exist\n+\tDeleteDatabase(storage_database);\n+\tDuckDB db(storage_database, config.get());\n+\n+\t// 1GB limit\n+\tConnection con(db);\n+\tconst idx_t limit = 1000000000;\n+\tREQUIRE_NO_FAIL(con.Query(StringUtil::Format(\"PRAGMA memory_limit='%lldB'\", limit)));\n+\n+\tauto &buffer_manager = BufferManager::GetBufferManager(*con.context);\n+\tD_ASSERT(buffer_manager.GetUsedMemory() == 0);\n+\n+\tidx_t requested_size = Storage::BLOCK_SIZE;\n+\tauto block = buffer_manager.RegisterMemory(requested_size, false);\n+\tauto handle = buffer_manager.Pin(block);\n+\tD_ASSERT(buffer_manager.GetUsedMemory() == requested_size + Storage::BLOCK_HEADER_SIZE);\n+\n+\tfor (; requested_size < limit; requested_size *= 2) {\n+\t\t// increase size\n+\t\tbuffer_manager.ReAllocate(block, requested_size);\n+\t\tD_ASSERT(buffer_manager.GetUsedMemory() == requested_size + Storage::BLOCK_HEADER_SIZE);\n+\t\t// unpin and make sure it's evicted\n+\t\thandle.reset();\n+\t\tREQUIRE_NO_FAIL(con.Query(StringUtil::Format(\"PRAGMA memory_limit='%lldB'\", requested_size)));\n+\t\tD_ASSERT(buffer_manager.GetUsedMemory() == 0);\n+\t\t// re-pin\n+\t\tREQUIRE_NO_FAIL(con.Query(StringUtil::Format(\"PRAGMA memory_limit='%lldB'\", limit)));\n+\t\thandle = buffer_manager.Pin(block);\n+\t\tD_ASSERT(buffer_manager.GetUsedMemory() == requested_size + Storage::BLOCK_HEADER_SIZE);\n+\t}\n+\trequested_size /= 2;\n+\tfor (; requested_size > Storage::BLOCK_SIZE; requested_size /= 2) {\n+\t\t// decrease size\n+\t\tbuffer_manager.ReAllocate(block, requested_size);\n+\t\tD_ASSERT(buffer_manager.GetUsedMemory() == requested_size + Storage::BLOCK_HEADER_SIZE);\n+\t\t// unpin and make sure it's evicted\n+\t\thandle.reset();\n+\t\tREQUIRE_NO_FAIL(con.Query(StringUtil::Format(\"PRAGMA memory_limit='%lldB'\", requested_size)));\n+\t\tD_ASSERT(buffer_manager.GetUsedMemory() == 0);\n+\t\t// re-pin\n+\t\tREQUIRE_NO_FAIL(con.Query(StringUtil::Format(\"PRAGMA memory_limit='%lldB'\", limit)));\n+\t\thandle = buffer_manager.Pin(block);\n+\t\tD_ASSERT(buffer_manager.GetUsedMemory() == requested_size + Storage::BLOCK_HEADER_SIZE);\n+\t}\n+}\n",
  "problem_statement": "Memory errors happening during a parquet query over a range of files\n**What does happen?**\r\nA number of different errors happen during one particular query:\r\n`duckdb::OutOfRangeException: what():  Out of Range Error: Not enough memory to complete operation: could not allocate block of 285751568 bytes`\r\n`Error: IO Error: Could not write all bytes to file \".tmp/4611686018427424834.block\": wanted=2687930368 wrote=2147479552`\r\n`SegmentationFault`\r\n\r\nPlus, a lot of memory being used (up to 30GB).\r\n\r\nThe query: `SELECT * FROM parquet_scan('<folder>/*.parquet') AS curr_crawl WHERE url <> '' AND ((css <> TRUE OR css IS NULL) AND (js <> TRUE OR js IS NULL) AND (is_image <> TRUE OR is_image IS NULL) AND internal = TRUE) ORDER BY deeprank DESC`.\r\n\r\nThe files total 777 mb in size.\r\n\r\n**What should happen?**\r\nThe query should be executed successfully. \r\n\r\n**To Reproduce**\r\nThere are a couple of ways to reproduce.\r\n\r\nRunning the query with `PRAGMA threads=6;`:\r\n1. `PRAGMA threads=6;`\r\n2. `SELECT * FROM parquet_scan('<folder>/*.parquet') AS curr_crawl WHERE url <> '' AND ((css <> TRUE OR css IS NULL) AND (js <> TRUE OR js IS NULL) AND (is_image <> TRUE OR is_image IS NULL) AND internal = TRUE) ORDER BY deeprank DESC`\r\n3.  One of the errors above happens, or execution takes 15+ mins and not ending and 30GB of memory being used or execution succeeding in adequate time but again with 30GB being used.\r\n\r\nNot using `PRAGMA threads=6;`:\r\n1. `SELECT * FROM parquet_scan('<folder>/*.parquet') AS curr_crawl WHERE url <> '' AND ((css <> TRUE OR css IS NULL) AND (js <> TRUE OR js IS NULL) AND (is_image <> TRUE OR is_image IS NULL) AND internal = TRUE) ORDER BY deeprank DESC`\r\n2. The query might complete successfully or might get `Error: IO Error: Could not write all bytes to file \".tmp/4611686018427424834.block\": wanted=2687930368 wrote=2147479552` or `SegmentationFault`\r\n\r\nI'm sending the files to you guys.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Amazon Linux 2\r\n - DuckDB Version: latest master/0.0.27\r\n\r\n**Before submitting**\r\n- [x] Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n- [x] Have you tried this on the latest `master` branch? In case you cannot compile, you may find some binaries here: https://github.com/duckdb/duckdb/releases/tag/master-builds\r\n\n",
  "hints_text": "",
  "created_at": "2021-07-29T15:21:04Z"
}