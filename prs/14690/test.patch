diff --git a/.github/patches/extensions/spatial/random_test_fix.patch b/.github/patches/extensions/spatial/random_test_fix.patch
new file mode 100644
index 000000000000..36351bce70f8
--- /dev/null
+++ b/.github/patches/extensions/spatial/random_test_fix.patch
@@ -0,0 +1,92 @@
+diff --git a/spatial/src/spatial/core/functions/table/st_generatepoints.cpp b/spatial/src/spatial/core/functions/table/st_generatepoints.cpp
+index 007d386..8754619 100644
+--- a/spatial/src/spatial/core/functions/table/st_generatepoints.cpp
++++ b/spatial/src/spatial/core/functions/table/st_generatepoints.cpp
+@@ -79,8 +79,8 @@ static void Execute(ClientContext &context, TableFunctionInput &data_p, DataChun
+ 	const auto chunk_size = MinValue<idx_t>(STANDARD_VECTOR_SIZE, bind_data.count - state.current_idx);
+ 	for (idx_t i = 0; i < chunk_size; i++) {
+ 
+-		x_data[i] = state.rng.NextRandom(bind_data.bbox.min.x, bind_data.bbox.max.x);
+-		y_data[i] = state.rng.NextRandom(bind_data.bbox.min.y, bind_data.bbox.max.y);
++		x_data[i] = state.rng.NextRandom32(bind_data.bbox.min.x, bind_data.bbox.max.x);
++		y_data[i] = state.rng.NextRandom32(bind_data.bbox.min.y, bind_data.bbox.max.y);
+ 
+ 		state.current_idx++;
+ 	}
+diff --git a/spatial/src/spatial/core/index/rtree/rtree_index_scan.cpp b/spatial/src/spatial/core/index/rtree/rtree_index_scan.cpp
+index 9168790..7fd53a2 100644
+--- a/spatial/src/spatial/core/index/rtree/rtree_index_scan.cpp
++++ b/spatial/src/spatial/core/index/rtree/rtree_index_scan.cpp
+@@ -208,7 +208,6 @@ TableFunction RTreeIndexScanFunction::GetFunction() {
+ 	func.pushdown_complex_filter = nullptr;
+ 	func.to_string = RTreeIndexScanToString;
+ 	func.table_scan_progress = nullptr;
+-	func.get_batch_index = nullptr;
+ 	func.projection_pushdown = true;
+ 	func.filter_pushdown = false;
+ 	func.get_bind_info = RTreeIndexScanBindInfo;
+diff --git a/spatial/src/spatial/core/io/osm/st_read_osm.cpp b/spatial/src/spatial/core/io/osm/st_read_osm.cpp
+index 465cb87..5aa49dd 100644
+--- a/spatial/src/spatial/core/io/osm/st_read_osm.cpp
++++ b/spatial/src/spatial/core/io/osm/st_read_osm.cpp
+@@ -836,10 +836,12 @@ static double Progress(ClientContext &context, const FunctionData *bind_data,
+ 	return state.GetProgress();
+ }
+ 
+-static idx_t GetBatchIndex(ClientContext &context, const FunctionData *bind_data_p,
+-                           LocalTableFunctionState *local_state, GlobalTableFunctionState *global_state) {
+-	auto &state = (LocalState &)*local_state;
+-	return state.block->block_idx;
++static OperatorPartitionData GetPartitionData(ClientContext &context, TableFunctionGetPartitionInput &input) {
++	if (input.partition_info.RequiresPartitionColumns()) {
++		throw InternalException("ST_ReadOSM::GetPartitionData: partition columns not supported");
++	}
++	auto &state = input.local_state->Cast<LocalState>();
++	return OperatorPartitionData(state.block->block_idx);
+ }
+ 
+ static unique_ptr<TableRef> ReadOsmPBFReplacementScan(ClientContext &context, ReplacementScanInput &input,
+@@ -898,7 +900,7 @@ static constexpr const char *DOC_EXAMPLE = R"(
+ void CoreTableFunctions::RegisterOsmTableFunction(DatabaseInstance &db) {
+ 	TableFunction read("ST_ReadOSM", {LogicalType::VARCHAR}, Execute, Bind, InitGlobal, InitLocal);
+ 
+-	read.get_batch_index = GetBatchIndex;
++	read.get_partition_data = GetPartitionData;
+ 	read.table_scan_progress = Progress;
+ 
+ 	ExtensionUtil::RegisterFunction(db, read);
+diff --git a/spatial/src/spatial/gdal/functions/st_read.cpp b/spatial/src/spatial/gdal/functions/st_read.cpp
+index b730baa..8d08898 100644
+--- a/spatial/src/spatial/gdal/functions/st_read.cpp
++++ b/spatial/src/spatial/gdal/functions/st_read.cpp
+@@ -676,7 +676,7 @@ void GdalTableFunction::Register(DatabaseInstance &db) {
+ 	                   GdalTableFunction::InitGlobal, GdalTableFunction::InitLocal);
+ 
+ 	scan.cardinality = GdalTableFunction::Cardinality;
+-	scan.get_batch_index = ArrowTableFunction::ArrowGetBatchIndex;
++	scan.get_partition_data = ArrowTableFunction::ArrowGetPartitionData;
+ 
+ 	scan.projection_pushdown = true;
+ 	scan.filter_pushdown = true;
+diff --git a/spatial/src/spatial/geos/functions/aggregate.cpp b/spatial/src/spatial/geos/functions/aggregate.cpp
+index aacc668..c478786 100644
+--- a/spatial/src/spatial/geos/functions/aggregate.cpp
++++ b/spatial/src/spatial/geos/functions/aggregate.cpp
+@@ -197,7 +197,7 @@ void GeosAggregateFunctions::Register(DatabaseInstance &db) {
+ 
+ 	AggregateFunctionSet st_intersection_agg("ST_Intersection_Agg");
+ 	st_intersection_agg.AddFunction(
+-	    AggregateFunction::UnaryAggregateDestructor<GEOSAggState, geometry_t, geometry_t, IntersectionAggFunction>(
++	    AggregateFunction::UnaryAggregateDestructor<GEOSAggState, geometry_t, geometry_t, IntersectionAggFunction, AggregateDestructorType::LEGACY>(
+ 	        core::GeoTypes::GEOMETRY(), core::GeoTypes::GEOMETRY()));
+ 
+ 	ExtensionUtil::RegisterFunction(db, st_intersection_agg);
+@@ -206,7 +206,7 @@ void GeosAggregateFunctions::Register(DatabaseInstance &db) {
+ 
+ 	AggregateFunctionSet st_union_agg("ST_Union_Agg");
+ 	st_union_agg.AddFunction(
+-	    AggregateFunction::UnaryAggregateDestructor<GEOSAggState, geometry_t, geometry_t, UnionAggFunction>(
++	    AggregateFunction::UnaryAggregateDestructor<GEOSAggState, geometry_t, geometry_t, UnionAggFunction, AggregateDestructorType::LEGACY>(
+ 	        core::GeoTypes::GEOMETRY(), core::GeoTypes::GEOMETRY()));
+ 
+ 	ExtensionUtil::RegisterFunction(db, st_union_agg);
diff --git a/data/parquet-testing/bug14120-dict-nulls-only.parquet b/data/parquet-testing/bug14120-dict-nulls-only.parquet
new file mode 100644
index 000000000000..ad56f26d87cc
Binary files /dev/null and b/data/parquet-testing/bug14120-dict-nulls-only.parquet differ
diff --git a/data/parquet-testing/incorrect_index_page_offsets.parquet b/data/parquet-testing/incorrect_index_page_offsets.parquet
new file mode 100644
index 000000000000..44670bcd19af
Binary files /dev/null and b/data/parquet-testing/incorrect_index_page_offsets.parquet differ
diff --git a/scripts/regression/test_runner.py b/scripts/regression/test_runner.py
new file mode 100644
index 000000000000..979d93b80259
--- /dev/null
+++ b/scripts/regression/test_runner.py
@@ -0,0 +1,178 @@
+import os
+import math
+import functools
+import shutil
+from benchmark import BenchmarkRunner, BenchmarkRunnerConfig
+from dataclasses import dataclass
+from typing import Optional, List, Union
+
+print = functools.partial(print, flush=True)
+
+
+def is_number(s):
+    try:
+        float(s)
+        return True
+    except ValueError:
+        return False
+
+
+# Geometric mean of an array of numbers
+def geomean(xs):
+    if len(xs) == 0:
+        return 'EMPTY'
+    for entry in xs:
+        if not is_number(entry):
+            return entry
+    return math.exp(math.fsum(math.log(float(x)) for x in xs) / len(xs))
+
+
+# how many times we will run the experiment, to be sure of the regression
+NUMBER_REPETITIONS = 5
+# the threshold at which we consider something a regression (percentage)
+REGRESSION_THRESHOLD_PERCENTAGE = 0.1
+# minimal seconds diff for something to be a regression (for very fast benchmarks)
+REGRESSION_THRESHOLD_SECONDS = 0.05
+
+import argparse
+
+# Set up the argument parser
+parser = argparse.ArgumentParser(description="Benchmark script with old and new runners.")
+
+# Define the arguments
+parser.add_argument("--old", type=str, help="Path to the old runner.", required=True)
+parser.add_argument("--new", type=str, help="Path to the new runner.", required=True)
+parser.add_argument("--benchmarks", type=str, help="Path to the benchmark file.", required=True)
+parser.add_argument("--verbose", action="store_true", help="Enable verbose output.")
+parser.add_argument("--threads", type=int, help="Number of threads to use.")
+parser.add_argument("--nofail", action="store_true", help="Do not fail on regression.")
+parser.add_argument("--disable-timeout", action="store_true", help="Disable timeout.")
+parser.add_argument("--max-timeout", type=int, default=3600, help="Set maximum timeout in seconds (default: 3600).")
+parser.add_argument("--root-dir", type=str, default="", help="Root directory.")
+
+# Parse the arguments
+args = parser.parse_args()
+
+# Assign parsed arguments to variables
+old_runner_path = args.old
+new_runner_path = args.new
+benchmark_file = args.benchmarks
+verbose = args.verbose
+threads = args.threads
+no_regression_fail = args.nofail
+disable_timeout = args.disable_timeout
+max_timeout = args.max_timeout
+root_dir = args.root_dir
+
+if not os.path.isfile(old_runner_path):
+    print(f"Failed to find old runner {old_runner_path}")
+    exit(1)
+
+if not os.path.isfile(new_runner_path):
+    print(f"Failed to find new runner {new_runner_path}")
+    exit(1)
+
+config_dict = vars(args)
+old_runner = BenchmarkRunner(BenchmarkRunnerConfig.from_params(old_runner_path, benchmark_file, **config_dict))
+new_runner = BenchmarkRunner(BenchmarkRunnerConfig.from_params(new_runner_path, benchmark_file, **config_dict))
+
+benchmark_list = old_runner.benchmark_list
+
+
+@dataclass
+class BenchmarkResult:
+    benchmark: str
+    old_result: Union[float, str]
+    new_result: Union[float, str]
+
+
+multiply_percentage = 1.0 + REGRESSION_THRESHOLD_PERCENTAGE
+other_results: List[BenchmarkResult] = []
+error_list: List[BenchmarkResult] = []
+for i in range(NUMBER_REPETITIONS):
+    regression_list: List[BenchmarkResult] = []
+    if len(benchmark_list) == 0:
+        break
+    print(
+        f'''====================================================
+==============      ITERATION {i}        =============
+==============      REMAINING {len(benchmark_list)}        =============
+====================================================
+'''
+    )
+
+    old_results = old_runner.run_benchmarks()
+    new_results = new_runner.run_benchmarks()
+
+    for benchmark in benchmark_list:
+        old_res = old_results[benchmark]
+        new_res = new_results[benchmark]
+        if isinstance(old_res, str) or isinstance(new_res, str):
+            # benchmark failed to run - always a regression
+            error_list.append(BenchmarkResult(benchmark, old_res, new_res))
+        elif (no_regression_fail == False) and (
+            (old_res + REGRESSION_THRESHOLD_SECONDS) * multiply_percentage < new_res
+        ):
+            regression_list.append(BenchmarkResult(benchmark, old_res, new_res))
+        else:
+            other_results.append(BenchmarkResult(benchmark, old_res, new_res))
+    benchmark_list = [res.benchmark for res in regression_list]
+
+exit_code = 0
+regression_list.extend(error_list)
+if len(regression_list) > 0:
+    exit_code = 1
+    print(
+        '''====================================================
+==============  REGRESSIONS DETECTED   =============
+====================================================
+'''
+    )
+    for regression in regression_list:
+        print(f"{regression.benchmark}")
+        print(f"Old timing: {regression.old_result}")
+        print(f"New timing: {regression.new_result}")
+        print("")
+    print(
+        '''====================================================
+==============     OTHER TIMINGS       =============
+====================================================
+'''
+    )
+else:
+    print(
+        '''====================================================
+============== NO REGRESSIONS DETECTED  =============
+====================================================
+'''
+    )
+
+other_results.sort(key=lambda x: x.benchmark)
+for res in other_results:
+    print(f"{res.benchmark}")
+    print(f"Old timing: {res.old_result}")
+    print(f"New timing: {res.new_result}")
+    print("")
+
+time_a = geomean(old_runner.complete_timings)
+time_b = geomean(new_runner.complete_timings)
+
+
+print("")
+if isinstance(time_a, str) or isinstance(time_b, str):
+    print(f"Old: {time_a}")
+    print(f"New: {time_b}")
+elif time_a > time_b * 1.01:
+    print(f"Old timing geometric mean: {time_a}")
+    print(f"New timing geometric mean: {time_b}, roughly {int((time_a - time_b) * 100.0 / time_a)}% faster")
+elif time_b > time_a * 1.01:
+    print(f"Old timing geometric mean: {time_a}, roughly {int((time_b - time_a) * 100.0 / time_b)}% faster")
+    print(f"New timing geometric mean: {time_b}")
+else:
+    print(f"Old timing geometric mean: {time_a}")
+    print(f"New timing geometric mean: {time_b}")
+
+# nuke cached benchmark data between runs
+if os.path.isdir("duckdb_benchmark_data"):
+    shutil.rmtree('duckdb_benchmark_data')
+exit(exit_code)
diff --git a/scripts/regression_test_runner.py b/scripts/regression_test_runner.py
deleted file mode 100644
index 97ef94b16710..000000000000
--- a/scripts/regression_test_runner.py
+++ /dev/null
@@ -1,249 +0,0 @@
-import os
-import sys
-import subprocess
-from io import StringIO
-import csv
-import statistics
-import math
-import functools
-import shutil
-
-print = functools.partial(print, flush=True)
-
-
-def is_number(s):
-    try:
-        float(s)
-        return True
-    except ValueError:
-        return False
-
-
-# Geometric mean of an array of numbers
-def geomean(xs):
-    if len(xs) == 0:
-        return 'EMPTY'
-    for entry in xs:
-        if not is_number(entry):
-            return entry
-    return math.exp(math.fsum(math.log(float(x)) for x in xs) / len(xs))
-
-
-# how many times we will run the experiment, to be sure of the regression
-number_repetitions = 5
-# the threshold at which we consider something a regression (percentage)
-regression_threshold_percentage = 0.1
-# minimal seconds diff for something to be a regression (for very fast benchmarks)
-regression_threshold_seconds = 0.05
-
-old_runner = None
-new_runner = None
-benchmark_file = None
-verbose = False
-threads = None
-no_regression_fail = False
-disable_timeout = False
-max_timeout = 3600
-root_dir = ""
-for arg in sys.argv:
-    if arg.startswith("--old="):
-        old_runner = arg.replace("--old=", "")
-    elif arg.startswith("--new="):
-        new_runner = arg.replace("--new=", "")
-    elif arg.startswith("--benchmarks="):
-        benchmark_file = arg.replace("--benchmarks=", "")
-    elif arg == "--verbose":
-        verbose = True
-    elif arg.startswith("--threads="):
-        threads = int(arg.replace("--threads=", ""))
-    elif arg.startswith("--nofail"):
-        no_regression_fail = True
-    elif arg == "--disable-timeout":
-        disable_timeout = True
-    elif arg.startswith("--root-dir="):
-        root_dir = arg.replace("--root-dir=", "")
-
-if old_runner is None or new_runner is None or benchmark_file is None:
-    print(
-        "Expected usage: python3 scripts/regression_test_runner.py --old=/old/benchmark_runner --new=/new/benchmark_runner --benchmarks=/benchmark/list.csv"
-    )
-    exit(1)
-
-if not os.path.isfile(old_runner):
-    print(f"Failed to find old runner {old_runner}")
-    exit(1)
-
-if not os.path.isfile(new_runner):
-    print(f"Failed to find new runner {new_runner}")
-    exit(1)
-
-complete_timings = {old_runner: [], new_runner: []}
-
-
-def run_benchmark(runner, benchmark):
-    benchmark_args = [runner, benchmark]
-
-    if root_dir:
-        benchmark_args += [f"--root-dir"]
-        benchmark_args += [root_dir]
-
-    if threads is not None:
-        benchmark_args += ["--threads=%d" % (threads,)]
-    if disable_timeout:
-        benchmark_args += ["--disable-timeout"]
-        timeout_seconds = max_timeout
-    else:
-        timeout_seconds = 600
-    try:
-        proc = subprocess.run(benchmark_args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout_seconds)
-        out = proc.stdout.decode('utf8')
-        err = proc.stderr.decode('utf8')
-        returncode = proc.returncode
-    except subprocess.TimeoutExpired:
-        print("Failed to run benchmark " + benchmark)
-        print(f"Aborted due to exceeding the limit of {timeout_seconds} seconds")
-        return 'Failed to run benchmark ' + benchmark
-    if returncode != 0:
-        print("Failed to run benchmark " + benchmark)
-        print(
-            '''====================================================
-==============         STDERR          =============
-====================================================
-'''
-        )
-        print(err)
-        print(
-            '''====================================================
-==============         STDOUT          =============
-====================================================
-'''
-        )
-        print(out)
-        if 'HTTP' in err:
-            print("Ignoring HTTP error and terminating the running of the regression tests")
-            exit(0)
-        return 'Failed to run benchmark ' + benchmark
-    if verbose:
-        print(err)
-    # read the input CSV
-    f = StringIO(err)
-    csv_reader = csv.reader(f, delimiter='\t')
-    header = True
-    timings = []
-    try:
-        for row in csv_reader:
-            if len(row) == 0:
-                continue
-            if header:
-                header = False
-            else:
-                timings.append(row[2])
-                complete_timings[runner].append(row[2])
-        return float(statistics.median(timings))
-    except:
-        print("Failed to run benchmark " + benchmark)
-        print(err)
-        return 'Failed to run benchmark ' + benchmark
-
-
-def run_benchmarks(runner, benchmark_list):
-    results = {}
-    for benchmark in benchmark_list:
-        results[benchmark] = run_benchmark(runner, benchmark)
-    return results
-
-
-# read the initial benchmark list
-with open(benchmark_file, 'r') as f:
-    benchmark_list = [x.strip() for x in f.read().split('
') if len(x) > 0]
-
-multiply_percentage = 1.0 + regression_threshold_percentage
-other_results = []
-error_list = []
-for i in range(number_repetitions):
-    regression_list = []
-    if len(benchmark_list) == 0:
-        break
-    print(
-        f'''====================================================
-==============      ITERATION {i}        =============
-==============      REMAINING {len(benchmark_list)}        =============
-====================================================
-'''
-    )
-
-    old_results = run_benchmarks(old_runner, benchmark_list)
-    new_results = run_benchmarks(new_runner, benchmark_list)
-
-    for benchmark in benchmark_list:
-        old_res = old_results[benchmark]
-        new_res = new_results[benchmark]
-        if isinstance(old_res, str) or isinstance(new_res, str):
-            # benchmark failed to run - always a regression
-            error_list.append([benchmark, old_res, new_res])
-        elif (no_regression_fail == False) and (
-            (old_res + regression_threshold_seconds) * multiply_percentage < new_res
-        ):
-            regression_list.append([benchmark, old_res, new_res])
-        else:
-            other_results.append([benchmark, old_res, new_res])
-    benchmark_list = [x[0] for x in regression_list]
-
-exit_code = 0
-regression_list += error_list
-if len(regression_list) > 0:
-    exit_code = 1
-    print(
-        '''====================================================
-==============  REGRESSIONS DETECTED   =============
-====================================================
-'''
-    )
-    for regression in regression_list:
-        print(f"{regression[0]}")
-        print(f"Old timing: {regression[1]}")
-        print(f"New timing: {regression[2]}")
-        print("")
-    print(
-        '''====================================================
-==============     OTHER TIMINGS       =============
-====================================================
-'''
-    )
-else:
-    print(
-        '''====================================================
-============== NO REGRESSIONS DETECTED  =============
-====================================================
-'''
-    )
-
-other_results.sort()
-for res in other_results:
-    print(f"{res[0]}")
-    print(f"Old timing: {res[1]}")
-    print(f"New timing: {res[2]}")
-    print("")
-
-time_a = geomean(complete_timings[old_runner])
-time_b = geomean(complete_timings[new_runner])
-
-
-print("")
-if isinstance(time_a, str) or isinstance(time_b, str):
-    print(f"Old: {time_a}")
-    print(f"New: {time_b}")
-elif time_a > time_b * 1.01:
-    print(f"Old timing geometric mean: {time_a}")
-    print(f"New timing geometric mean: {time_b}, roughly {int((time_a - time_b) * 100.0 / time_a)}% faster")
-elif time_b > time_a * 1.01:
-    print(f"Old timing geometric mean: {time_a}, roughly {int((time_b - time_a) * 100.0 / time_b)}% faster")
-    print(f"New timing geometric mean: {time_b}")
-else:
-    print(f"Old timing geometric mean: {time_a}")
-    print(f"New timing geometric mean: {time_b}")
-
-# nuke cached benchmark data between runs
-if os.path.isdir("duckdb_benchmark_data"):
-    shutil.rmtree('duckdb_benchmark_data')
-exit(exit_code)
diff --git a/test/api/capi/CMakeLists.txt b/test/api/capi/CMakeLists.txt
index b66716654d5a..1f2851e947dd 100644
--- a/test/api/capi/CMakeLists.txt
+++ b/test/api/capi/CMakeLists.txt
@@ -7,6 +7,7 @@ add_library_unity(
   capi_table_functions.cpp
   test_capi.cpp
   test_capi_any_invalid_type.cpp
+  test_capi_append_data_chunk.cpp
   test_starting_database.cpp
   test_capi_appender.cpp
   test_capi_arrow.cpp
diff --git a/test/api/capi/test_capi_append_data_chunk.cpp b/test/api/capi/test_capi_append_data_chunk.cpp
new file mode 100644
index 000000000000..2d203b47076a
--- /dev/null
+++ b/test/api/capi/test_capi_append_data_chunk.cpp
@@ -0,0 +1,140 @@
+#include "capi_tester.hpp"
+
+using namespace duckdb;
+using namespace std;
+
+TEST_CASE("Test casting columns in AppendDataChunk in C API", "[capi]") {
+	duckdb::vector<string> tables;
+	tables.push_back("CREATE TABLE test(i BIGINT, j VARCHAR);");
+	tables.push_back("CREATE TABLE test(i BIGINT, j BOOLEAN);");
+
+	for (idx_t i = 0; i < tables.size(); i++) {
+		CAPITester tester;
+		REQUIRE(tester.OpenDatabase(nullptr));
+		REQUIRE(duckdb_vector_size() == STANDARD_VECTOR_SIZE);
+
+		tester.Query(tables[i]);
+
+		duckdb_logical_type types[2];
+		types[0] = duckdb_create_logical_type(DUCKDB_TYPE_SMALLINT);
+		types[1] = duckdb_create_logical_type(DUCKDB_TYPE_BOOLEAN);
+
+		auto data_chunk = duckdb_create_data_chunk(types, 2);
+		REQUIRE(data_chunk);
+
+		auto smallint_col = duckdb_data_chunk_get_vector(data_chunk, 0);
+		auto boolean_col = duckdb_data_chunk_get_vector(data_chunk, 1);
+
+		auto smallint_data = reinterpret_cast<int16_t *>(duckdb_vector_get_data(smallint_col));
+		smallint_data[0] = 15;
+		smallint_data[1] = -15;
+
+		auto boolean_data = reinterpret_cast<bool *>(duckdb_vector_get_data(boolean_col));
+		boolean_data[0] = false;
+		boolean_data[1] = true;
+
+		duckdb_data_chunk_set_size(data_chunk, 2);
+
+		duckdb_appender appender;
+		auto status = duckdb_appender_create(tester.connection, nullptr, "test", &appender);
+		REQUIRE(status == DuckDBSuccess);
+
+		REQUIRE(duckdb_append_data_chunk(appender, data_chunk) == DuckDBSuccess);
+		duckdb_appender_close(appender);
+
+		auto result = tester.Query("SELECT i, j FROM test;");
+		REQUIRE(result->Fetch<int64_t>(0, 0) == 15);
+		REQUIRE(result->Fetch<int64_t>(0, 1) == -15);
+		auto str = result->Fetch<string>(1, 0);
+		REQUIRE(str.compare("false") == 0);
+		str = result->Fetch<string>(1, 1);
+		REQUIRE(str.compare("true") == 0);
+
+		duckdb_appender_destroy(&appender);
+		duckdb_destroy_data_chunk(&data_chunk);
+		duckdb_destroy_logical_type(&types[0]);
+		duckdb_destroy_logical_type(&types[1]);
+	}
+}
+
+TEST_CASE("Test casting error in AppendDataChunk in C API", "[capi]") {
+	CAPITester tester;
+	REQUIRE(tester.OpenDatabase(nullptr));
+	REQUIRE(duckdb_vector_size() == STANDARD_VECTOR_SIZE);
+
+	tester.Query("CREATE TABLE test(i BIGINT, j BOOLEAN[]);");
+
+	duckdb_logical_type types[2];
+	types[0] = duckdb_create_logical_type(DUCKDB_TYPE_SMALLINT);
+	types[1] = duckdb_create_logical_type(DUCKDB_TYPE_BOOLEAN);
+
+	auto data_chunk = duckdb_create_data_chunk(types, 2);
+	REQUIRE(data_chunk);
+
+	auto smallint_col = duckdb_data_chunk_get_vector(data_chunk, 0);
+	auto boolean_col = duckdb_data_chunk_get_vector(data_chunk, 1);
+
+	auto smallint_data = reinterpret_cast<int16_t *>(duckdb_vector_get_data(smallint_col));
+	smallint_data[0] = 15;
+	smallint_data[1] = -15;
+
+	auto boolean_data = reinterpret_cast<bool *>(duckdb_vector_get_data(boolean_col));
+	boolean_data[0] = false;
+	boolean_data[1] = true;
+
+	duckdb_data_chunk_set_size(data_chunk, 2);
+
+	duckdb_appender appender;
+	auto status = duckdb_appender_create(tester.connection, nullptr, "test", &appender);
+	REQUIRE(status == DuckDBSuccess);
+
+	REQUIRE(duckdb_append_data_chunk(appender, data_chunk) == DuckDBError);
+	auto error_msg = duckdb_appender_error(appender);
+	REQUIRE(string(error_msg) == "type mismatch in AppendDataChunk, expected BOOLEAN[], got BOOLEAN for column 1");
+
+	duckdb_appender_close(appender);
+	duckdb_appender_destroy(&appender);
+	duckdb_destroy_data_chunk(&data_chunk);
+	duckdb_destroy_logical_type(&types[0]);
+	duckdb_destroy_logical_type(&types[1]);
+}
+
+TEST_CASE("Test casting timestamps in AppendDataChunk in C API", "[capi]") {
+	CAPITester tester;
+	REQUIRE(tester.OpenDatabase(nullptr));
+	REQUIRE(duckdb_vector_size() == STANDARD_VECTOR_SIZE);
+
+	tester.Query("CREATE TABLE test(i TIMESTAMP, j DATE);");
+
+	duckdb_logical_type types[2];
+	types[0] = duckdb_create_logical_type(DUCKDB_TYPE_VARCHAR);
+	types[1] = duckdb_create_logical_type(DUCKDB_TYPE_VARCHAR);
+
+	auto data_chunk = duckdb_create_data_chunk(types, 2);
+	REQUIRE(data_chunk);
+
+	auto ts_column = duckdb_data_chunk_get_vector(data_chunk, 0);
+	auto date_column = duckdb_data_chunk_get_vector(data_chunk, 1);
+
+	duckdb_vector_assign_string_element(ts_column, 0, "2017-07-23 13:10:11");
+	duckdb_vector_assign_string_element(date_column, 0, "1993-08-14");
+	duckdb_data_chunk_set_size(data_chunk, 1);
+
+	duckdb_appender appender;
+	auto status = duckdb_appender_create(tester.connection, nullptr, "test", &appender);
+	REQUIRE(status == DuckDBSuccess);
+
+	REQUIRE(duckdb_append_data_chunk(appender, data_chunk) == DuckDBSuccess);
+	duckdb_appender_close(appender);
+
+	auto result = tester.Query("SELECT i::VARCHAR, j::VARCHAR FROM test;");
+	auto str = result->Fetch<string>(0, 0);
+	REQUIRE(str.compare("2017-07-23 13:10:11") == 0);
+	str = result->Fetch<string>(1, 0);
+	REQUIRE(str.compare("1993-08-14") == 0);
+
+	duckdb_appender_destroy(&appender);
+	duckdb_destroy_data_chunk(&data_chunk);
+	duckdb_destroy_logical_type(&types[0]);
+	duckdb_destroy_logical_type(&types[1]);
+}
diff --git a/test/api/capi/test_capi_appender.cpp b/test/api/capi/test_capi_appender.cpp
index 2c232b5e4807..fa7687fa8e44 100644
--- a/test/api/capi/test_capi_appender.cpp
+++ b/test/api/capi/test_capi_appender.cpp
@@ -649,50 +649,60 @@ TEST_CASE("Test append DEFAULT in C API", "[capi]") {
 TEST_CASE("Test append timestamp in C API", "[capi]") {
 	CAPITester tester;
 	duckdb::unique_ptr<CAPIResult> result;
-	duckdb_state status;
-
-	// open the database in in-memory mode
 	REQUIRE(tester.OpenDatabase(nullptr));
 
 	tester.Query("CREATE TABLE test (t timestamp)");
 	duckdb_appender appender;
 
-	status = duckdb_appender_create(tester.connection, nullptr, "test", &appender);
+	auto status = duckdb_appender_create_ext(tester.connection, nullptr, nullptr, "test", &appender);
 	REQUIRE(status == DuckDBSuccess);
 	REQUIRE(duckdb_appender_error(appender) == nullptr);
 
-	// successful append
-	status = duckdb_appender_begin_row(appender);
-	REQUIRE(status == DuckDBSuccess);
+	REQUIRE(duckdb_appender_begin_row(appender) == DuckDBSuccess);
+	REQUIRE(duckdb_append_varchar(appender, "2022-04-09 15:56:37.544") == DuckDBSuccess);
+	REQUIRE(duckdb_appender_end_row(appender) == DuckDBSuccess);
 
-	// status = duckdb_append_timestamp(appender, duckdb_timestamp{1649519797544000});
-	status = duckdb_append_varchar(appender, "2022-04-09 15:56:37.544");
-	REQUIRE(status == DuckDBSuccess);
+	REQUIRE(duckdb_appender_begin_row(appender) == DuckDBSuccess);
+	REQUIRE(duckdb_append_varchar(appender, "XXXXX") == DuckDBError);
+	REQUIRE(duckdb_appender_error(appender) != nullptr);
+	REQUIRE(duckdb_appender_end_row(appender) == DuckDBError);
 
-	status = duckdb_appender_end_row(appender);
-	REQUIRE(status == DuckDBSuccess);
+	REQUIRE(duckdb_appender_flush(appender) == DuckDBSuccess);
+	REQUIRE(duckdb_appender_close(appender) == DuckDBSuccess);
 
-	// append failure
-	status = duckdb_appender_begin_row(appender);
-	REQUIRE(status == DuckDBSuccess);
+	REQUIRE(duckdb_appender_destroy(&appender) == DuckDBSuccess);
 
-	status = duckdb_append_varchar(appender, "XXXXX");
-	REQUIRE(status == DuckDBError);
-	REQUIRE(duckdb_appender_error(appender) != nullptr);
+	result = tester.Query("SELECT * FROM test");
+	REQUIRE_NO_FAIL(*result);
+	REQUIRE(result->Fetch<string>(0, 0) == "2022-04-09 15:56:37.544");
+}
 
-	status = duckdb_appender_end_row(appender);
-	REQUIRE(status == DuckDBError);
+TEST_CASE("Test append to different catalog in C API") {
+	CAPITester tester;
+	REQUIRE(tester.OpenDatabase(nullptr));
 
-	status = duckdb_appender_flush(appender);
-	REQUIRE(status == DuckDBSuccess);
+	auto test_dir = GetTestDirectory();
+	auto attach_query = "ATTACH '" + test_dir + "/append_to_other.db'";
+	REQUIRE(tester.Query(attach_query)->success);
 
-	status = duckdb_appender_close(appender);
-	REQUIRE(status == DuckDBSuccess);
+	auto result = tester.Query("CREATE OR REPLACE TABLE append_to_other.tbl(i INTEGER)");
+	REQUIRE(result->success);
 
-	status = duckdb_appender_destroy(&appender);
+	duckdb_appender appender;
+	auto status = duckdb_appender_create_ext(tester.connection, "append_to_other", "main", "tbl", &appender);
 	REQUIRE(status == DuckDBSuccess);
+	REQUIRE(duckdb_appender_error(appender) == nullptr);
 
-	result = tester.Query("SELECT * FROM test");
-	REQUIRE_NO_FAIL(*result);
-	REQUIRE(result->Fetch<string>(0, 0) == "2022-04-09 15:56:37.544");
+	for (idx_t i = 0; i < 200; i++) {
+		REQUIRE(duckdb_appender_begin_row(appender) == DuckDBSuccess);
+		REQUIRE(duckdb_append_int32(appender, 2) == DuckDBSuccess);
+		REQUIRE(duckdb_appender_end_row(appender) == DuckDBSuccess);
+	}
+	REQUIRE(duckdb_appender_close(appender) == DuckDBSuccess);
+
+	result = tester.Query("SELECT SUM(i)::BIGINT FROM append_to_other.tbl");
+	REQUIRE(result->Fetch<int64_t>(0, 0) == 400);
+
+	REQUIRE(duckdb_appender_destroy(&appender) == DuckDBSuccess);
+	tester.Cleanup();
 }
diff --git a/test/api/capi/test_capi_data_chunk.cpp b/test/api/capi/test_capi_data_chunk.cpp
index 308675b14b98..8b4c8b361b52 100644
--- a/test/api/capi/test_capi_data_chunk.cpp
+++ b/test/api/capi/test_capi_data_chunk.cpp
@@ -217,46 +217,6 @@ TEST_CASE("Test DataChunk C API", "[capi]") {
 	}
 }
 
-TEST_CASE("Test DataChunk appending incorrect types in C API", "[capi]") {
-	CAPITester tester;
-	duckdb::unique_ptr<CAPIResult> result;
-	duckdb_state status;
-
-	REQUIRE(tester.OpenDatabase(nullptr));
-
-	REQUIRE(duckdb_vector_size() == STANDARD_VECTOR_SIZE);
-
-	tester.Query("CREATE TABLE test(i BIGINT, j SMALLINT)");
-
-	duckdb_logical_type types[2];
-	types[0] = duckdb_create_logical_type(DUCKDB_TYPE_BIGINT);
-	types[1] = duckdb_create_logical_type(DUCKDB_TYPE_BOOLEAN);
-
-	auto data_chunk = duckdb_create_data_chunk(types, 2);
-	REQUIRE(data_chunk);
-
-	auto col1_ptr = (int64_t *)duckdb_vector_get_data(duckdb_data_chunk_get_vector(data_chunk, 0));
-	*col1_ptr = 42;
-	auto col2_ptr = (bool *)duckdb_vector_get_data(duckdb_data_chunk_get_vector(data_chunk, 1));
-	*col2_ptr = false;
-
-	duckdb_appender appender;
-	status = duckdb_appender_create(tester.connection, nullptr, "test", &appender);
-	REQUIRE(status == DuckDBSuccess);
-
-	REQUIRE(duckdb_append_data_chunk(appender, data_chunk) == DuckDBError);
-
-	auto error = duckdb_appender_error(appender);
-	REQUIRE(duckdb::StringUtil::Contains(error, "expected SMALLINT but got BOOLEAN for column 2"));
-
-	duckdb_appender_destroy(&appender);
-
-	duckdb_destroy_data_chunk(&data_chunk);
-
-	duckdb_destroy_logical_type(&types[0]);
-	duckdb_destroy_logical_type(&types[1]);
-}
-
 TEST_CASE("Test DataChunk varchar result fetch in C API", "[capi]") {
 	if (duckdb_vector_size() < 64) {
 		return;
diff --git a/test/api/capi/test_capi_table_description.cpp b/test/api/capi/test_capi_table_description.cpp
index 67d24b00ce1f..0f8399b9c5bf 100644
--- a/test/api/capi/test_capi_table_description.cpp
+++ b/test/api/capi/test_capi_table_description.cpp
@@ -4,23 +4,35 @@
 using namespace duckdb;
 using namespace std;
 
-TEST_CASE("Test table description in C API", "[capi]") {
+TEST_CASE("Test the table description in the C API", "[capi]") {
 	CAPITester tester;
-	duckdb_state status;
+	REQUIRE(tester.OpenDatabase(nullptr));
 	duckdb_table_description table_description = nullptr;
+	tester.Query("SET threads=1;");
 
-	// open the database in in-memory mode
-	REQUIRE(tester.OpenDatabase(nullptr));
+	// Test a non-existent table.
+	auto status = duckdb_table_description_create(tester.connection, nullptr, "test", &table_description);
+	REQUIRE(status == DuckDBError);
+	duckdb_table_description_destroy(&table_description);
 
-	// Table doesn't exist yet
-	status = duckdb_table_description_create(tester.connection, nullptr, "test", &table_description);
+	status = duckdb_table_description_create_ext(tester.connection, "hello", "world", "test", &table_description);
 	REQUIRE(status == DuckDBError);
 	duckdb_table_description_destroy(&table_description);
 
-	tester.Query("CREATE TABLE test (i integer, j integer default 5)");
+	// Create an in-memory table and a table in an external file.
+	tester.Query("CREATE TABLE test (i INTEGER, j INTEGER default 5)");
+	auto test_dir = GetTestDirectory();
+	auto attach_query = "ATTACH '" + test_dir + "/ext_description.db'";
+	tester.Query(attach_query);
+	tester.Query("CREATE TABLE ext_description.test(my_column INTEGER)");
 
-	// The table was not created in this schema
-	status = duckdb_table_description_create(tester.connection, "non-existant", "test", &table_description);
+	// Test invalid catalog and schema.
+	status =
+	    duckdb_table_description_create_ext(tester.connection, "non-existent", nullptr, "test", &table_description);
+	REQUIRE(status == DuckDBError);
+	duckdb_table_description_destroy(&table_description);
+
+	status = duckdb_table_description_create(tester.connection, "non-existent", "test", &table_description);
 	REQUIRE(status == DuckDBError);
 	duckdb_table_description_destroy(&table_description);
 
@@ -29,19 +41,40 @@ TEST_CASE("Test table description in C API", "[capi]") {
 	REQUIRE(duckdb_table_description_error(table_description) == nullptr);
 
 	bool has_default;
-	SECTION("Out of range column") {
-		status = duckdb_column_has_default(table_description, 2, &has_default);
-		REQUIRE(status == DuckDBError);
+	SECTION("Passing nullptr to has_default") {
+		REQUIRE(duckdb_column_has_default(table_description, 2, nullptr) == DuckDBError);
+		REQUIRE(duckdb_column_has_default(nullptr, 2, &has_default) == DuckDBError);
+	}
+	SECTION("Out of range column for has_default") {
+		REQUIRE(duckdb_column_has_default(table_description, 2, &has_default) == DuckDBError);
 	}
 	SECTION("In range column - not default") {
-		status = duckdb_column_has_default(table_description, 0, &has_default);
-		REQUIRE(status == DuckDBSuccess);
+		REQUIRE(duckdb_column_has_default(table_description, 0, &has_default) == DuckDBSuccess);
 		REQUIRE(has_default == false);
 	}
 	SECTION("In range column - default") {
-		status = duckdb_column_has_default(table_description, 1, &has_default);
-		REQUIRE(status == DuckDBSuccess);
+		REQUIRE(duckdb_column_has_default(table_description, 1, &has_default) == DuckDBSuccess);
 		REQUIRE(has_default == true);
 	}
 	duckdb_table_description_destroy(&table_description);
+
+	// Let's get information about the external table.
+	status =
+	    duckdb_table_description_create_ext(tester.connection, "ext_description", nullptr, "test", &table_description);
+	REQUIRE(status == DuckDBSuccess);
+	REQUIRE(duckdb_table_description_error(table_description) == nullptr);
+
+	SECTION("Passing nullptr to get_name") {
+		REQUIRE(duckdb_table_description_get_column_name(nullptr, 0) == nullptr);
+	}
+	SECTION("Out of range column for get_name") {
+		REQUIRE(duckdb_table_description_get_column_name(table_description, 1) == nullptr);
+	}
+	SECTION("In range column - get the name") {
+		auto column_name = duckdb_table_description_get_column_name(table_description, 0);
+		string expected = "my_column";
+		REQUIRE(!expected.compare(column_name));
+		duckdb_free(column_name);
+	}
+	duckdb_table_description_destroy(&table_description);
 }
diff --git a/test/api/test_api.cpp b/test/api/test_api.cpp
index da7136af22bb..1e7c458f6573 100644
--- a/test/api/test_api.cpp
+++ b/test/api/test_api.cpp
@@ -450,6 +450,35 @@ TEST_CASE("Test fetch API with big results", "[api][.]") {
 	VerifyStreamResult(std::move(result));
 }
 
+TEST_CASE("Test TryFlushCachingOperators interrupted ExecutePushInternal", "[api][.]") {
+	DuckDB db;
+	Connection con(db);
+
+	con.Query("create table tbl as select 100000 a from range(2) t(a);");
+	con.Query("pragma threads=1");
+
+	// Use PhysicalCrossProduct with a very low amount of produced tuples, this caches the result in the
+	// CachingOperatorState This gets flushed with FinalExecute in PipelineExecutor::TryFlushCachingOperator
+	auto pending_query = con.PendingQuery("select unnest(range(a.a)) from tbl a, tbl b;");
+
+	// Through `unnest(range(a.a.))` this FinalExecute multiple chunks, more than the ExecutionBudget can handle with
+	// PROCESS_PARTIAL
+	pending_query->ExecuteTask();
+
+	// query the connection as normal after
+	auto res = pending_query->Execute();
+	REQUIRE(!res->HasError());
+	auto &materialized_res = res->Cast<MaterializedQueryResult>();
+	idx_t initial_tuples = 2 * 2;
+	REQUIRE(materialized_res.RowCount() == initial_tuples * 100000);
+	for (idx_t i = 0; i < initial_tuples; i++) {
+		for (idx_t j = 0; j < 100000; j++) {
+			auto value = static_cast<idx_t>(materialized_res.GetValue<int64_t>(0, (i * 100000) + j));
+			REQUIRE(value == j);
+		}
+	}
+}
+
 TEST_CASE("Test streaming query during stack unwinding", "[api]") {
 	DuckDB db;
 	Connection con(db);
@@ -651,7 +680,7 @@ TEST_CASE("Issue #6284: CachingPhysicalOperator in pull causes issues", "[api][.
 		count += chunk->size();
 	}
 
-	REQUIRE(951468 - count == 0);
+	REQUIRE(951698 == count);
 }
 
 TEST_CASE("Fuzzer 50 - Alter table heap-use-after-free", "[api]") {
diff --git a/test/api/test_reset.cpp b/test/api/test_reset.cpp
index 3e58d3fa75af..f978210a9789 100644
--- a/test/api/test_reset.cpp
+++ b/test/api/test_reset.cpp
@@ -43,7 +43,7 @@ struct OptionValueSet {
 void RequireValueEqual(ConfigurationOption *op, const Value &left, const Value &right, int line);
 #define REQUIRE_VALUE_EQUAL(op, lhs, rhs) RequireValueEqual(op, lhs, rhs, __LINE__)
 
-OptionValueSet GetValueForOption(const string &name, LogicalTypeId type) {
+OptionValueSet GetValueForOption(const string &name, const LogicalType &type) {
 	static unordered_map<string, OptionValueSet> value_map = {
 	    {"threads", {Value::BIGINT(42), Value::BIGINT(42)}},
 	    {"checkpoint_threshold", {"4.0 GiB"}},
@@ -109,7 +109,7 @@ OptionValueSet GetValueForOption(const string &name, LogicalTypeId type) {
 	    {"allocator_bulk_deallocation_flush_threshold", {"4.0 GiB"}}};
 	// Every option that's not excluded has to be part of this map
 	if (!value_map.count(name)) {
-		switch (type) {
+		switch (type.id()) {
 		case LogicalTypeId::BOOLEAN:
 			return OptionValueSet(Value::BOOLEAN(true));
 		case LogicalTypeId::TINYINT:
@@ -132,6 +132,8 @@ OptionValueSet GetValueForOption(const string &name, LogicalTypeId type) {
 bool OptionIsExcludedFromTest(const string &name) {
 	static unordered_set<string> excluded_options = {
 	    "access_mode",
+	    "allowed_directories",
+	    "allowed_paths",
 	    "schema",
 	    "search_path",
 	    "debug_window_mode",
@@ -197,8 +199,9 @@ TEST_CASE("Test RESET statement for ClientConfig options", "[api]") {
 
 		// Get the current value of the option
 		auto original_value = op->get_setting(*con.context);
+		auto parameter_type = DBConfig::ParseLogicalType(option.parameter_type);
 
-		auto value_set = GetValueForOption(option.name, option.parameter_type);
+		auto value_set = GetValueForOption(option.name, parameter_type);
 		// verify that at least one value is different
 		bool any_different = false;
 		string options;
@@ -221,7 +224,7 @@ TEST_CASE("Test RESET statement for ClientConfig options", "[api]") {
 		}
 		for (auto &value_pair : value_set.pairs) {
 			// Get the new value for the option
-			auto input = value_pair.input.DefaultCastAs(op->parameter_type);
+			auto input = value_pair.input.DefaultCastAs(parameter_type);
 			// Set the new option
 			if (op->set_local) {
 				op->set_local(*con.context, input);
diff --git a/test/api/test_threads.cpp b/test/api/test_threads.cpp
index 72e453e7c1ff..f67f7b2662e1 100644
--- a/test/api/test_threads.cpp
+++ b/test/api/test_threads.cpp
@@ -99,7 +99,7 @@ TEST_CASE("Test external threads", "[api]") {
 
 	auto res = con.Query("SET external_threads=-1");
 	REQUIRE(res->HasError());
-	REQUIRE(res->GetError() == "Syntax Error: Must have a non-negative number of external threads!");
+	REQUIRE(StringUtil::Contains(res->GetError(), "out of range"));
 
 	res = con.Query("SET external_threads=14");
 	REQUIRE(res->HasError());
diff --git a/test/appender/test_appender.cpp b/test/appender/test_appender.cpp
index e20d498c8833..aabb5845bb10 100644
--- a/test/appender/test_appender.cpp
+++ b/test/appender/test_appender.cpp
@@ -21,7 +21,7 @@ TEST_CASE("Basic appender tests", "[appender]") {
 	// append a bunch of values
 	{
 		Appender appender(con, "integers");
-		for (size_t i = 0; i < 2000; i++) {
+		for (idx_t i = 0; i < 2000; i++) {
 			appender.BeginRow();
 			appender.Append<int32_t>(1);
 			appender.EndRow();
@@ -39,7 +39,7 @@ TEST_CASE("Basic appender tests", "[appender]") {
 	{
 		Appender appender2(con, "integers");
 		// now append a bunch of values
-		for (size_t i = 0; i < 2000; i++) {
+		for (idx_t i = 0; i < 2000; i++) {
 			appender2.BeginRow();
 			appender2.Append<int32_t>(1);
 			appender2.EndRow();
@@ -59,7 +59,7 @@ TEST_CASE("Basic appender tests", "[appender]") {
 	{
 		Appender appender(con, "vals");
 
-		for (size_t i = 0; i < 2000; i++) {
+		for (idx_t i = 0; i < 2000; i++) {
 			appender.BeginRow();
 			appender.Append<int8_t>(1);
 			appender.Append<int16_t>(1);
@@ -102,7 +102,7 @@ TEST_CASE("Test AppendRow", "[appender]") {
 	// append a bunch of values
 	{
 		Appender appender(con, "integers");
-		for (size_t i = 0; i < 2000; i++) {
+		for (idx_t i = 0; i < 2000; i++) {
 			appender.AppendRow(1);
 		}
 		appender.Close();
@@ -123,7 +123,7 @@ TEST_CASE("Test AppendRow", "[appender]") {
 	// now append a bunch of values
 	{
 		Appender appender(con, "vals");
-		for (size_t i = 0; i < 2000; i++) {
+		for (idx_t i = 0; i < 2000; i++) {
 			appender.AppendRow(1, 1, 1, "hello", 3.33);
 			// append null values
 			appender.AppendRow(nullptr, nullptr, nullptr, nullptr, nullptr);
@@ -479,3 +479,95 @@ TEST_CASE("Test alter table in the middle of append", "[appender]") {
 		REQUIRE_THROWS(appender.Close());
 	}
 }
+
+TEST_CASE("Test appending to a different database file", "[appender]") {
+	duckdb::unique_ptr<QueryResult> result;
+	DuckDB db(nullptr);
+	Connection con(db);
+
+	auto test_dir = GetTestDirectory();
+	auto attach_query = "ATTACH '" + test_dir + "/append_to_other.db'";
+	REQUIRE_NO_FAIL(con.Query(attach_query));
+	REQUIRE_NO_FAIL(con.Query("CREATE OR REPLACE TABLE append_to_other.tbl(i INTEGER)"));
+
+	Appender appender(con, "append_to_other", "main", "tbl");
+	for (idx_t i = 0; i < 200; i++) {
+		appender.BeginRow();
+		appender.Append<int32_t>(2);
+		appender.EndRow();
+	}
+	appender.Close();
+
+	result = con.Query("SELECT SUM(i) FROM append_to_other.tbl");
+	REQUIRE(CHECK_COLUMN(result, 0, {400}));
+	bool failed;
+
+	try {
+		Appender appender_invalid(con, "invalid_database", "main", "tbl");
+		failed = false;
+	} catch (std::exception &ex) {
+		ErrorData error(ex);
+		REQUIRE(error.Message().find("Catalog Error") != std::string::npos);
+		failed = true;
+	}
+	REQUIRE(failed);
+
+	try {
+		Appender appender_invalid(con, "append_to_other", "invalid_schema", "tbl");
+		failed = false;
+	} catch (std::exception &ex) {
+		ErrorData error(ex);
+		REQUIRE(error.Message().find("Catalog Error") != std::string::npos);
+		failed = true;
+	}
+	REQUIRE(failed);
+
+	// Attach as readonly.
+	REQUIRE_NO_FAIL(con.Query("DETACH append_to_other"));
+	REQUIRE_NO_FAIL(con.Query(attach_query + " (readonly)"));
+
+	try {
+		Appender appender_readonly(con, "append_to_other", "main", "tbl");
+		failed = false;
+	} catch (std::exception &ex) {
+		ErrorData error(ex);
+		REQUIRE(error.Message().find("Cannot append to a readonly database") != std::string::npos);
+		failed = true;
+	}
+	REQUIRE(failed);
+}
+
+TEST_CASE("Test appending to different database files", "[appender]") {
+	duckdb::unique_ptr<QueryResult> result;
+	DuckDB db(nullptr);
+	Connection con(db);
+
+	auto test_dir = GetTestDirectory();
+	auto attach_db1 = "ATTACH '" + test_dir + "/db1.db'";
+	auto attach_db2 = "ATTACH '" + test_dir + "/db2.db'";
+	REQUIRE_NO_FAIL(con.Query(attach_db1));
+	REQUIRE_NO_FAIL(con.Query(attach_db2));
+	REQUIRE_NO_FAIL(con.Query("CREATE OR REPLACE TABLE db1.tbl(i INTEGER)"));
+	REQUIRE_NO_FAIL(con.Query("CREATE OR REPLACE TABLE db2.tbl(i INTEGER)"));
+
+	REQUIRE_NO_FAIL(con.Query("START TRANSACTION"));
+	REQUIRE_NO_FAIL(con.Query("INSERT INTO db1.tbl VALUES (1)"));
+
+	Appender appender(con, "db2", "main", "tbl");
+	appender.BeginRow();
+	appender.Append<int32_t>(2);
+	appender.EndRow();
+
+	bool failed;
+	try {
+		appender.Close();
+		failed = false;
+	} catch (std::exception &ex) {
+		ErrorData error(ex);
+		REQUIRE(error.Message().find("a single transaction can only write to a single attached database") !=
+		        std::string::npos);
+		failed = true;
+	}
+	REQUIRE(failed);
+	REQUIRE_NO_FAIL(con.Query("COMMIT TRANSACTION"));
+}
diff --git a/test/extension/test_tags.test b/test/extension/test_tags.test
index 581a02097880..f61e35acc4c2 100644
--- a/test/extension/test_tags.test
+++ b/test/extension/test_tags.test
@@ -13,12 +13,12 @@ statement ok
 LOAD '__BUILD_DIRECTORY__/test/extension/loadable_extension_demo.duckdb_extension';
 
 query II
-SELECT function_name, tags['ext:author'] FROM duckdb_functions() WHERE tags['ext:name'] = ['loadable_extension_demo'] ORDER BY function_name;
+SELECT function_name, tags['ext:author'] FROM duckdb_functions() WHERE tags['ext:name'] = 'loadable_extension_demo' ORDER BY function_name;
 ----
-add_point	[DuckDB Labs]
-sub_point	[DuckDB Labs]
+add_point	DuckDB Labs
+sub_point	DuckDB Labs
 
 query II
-SELECT type_name, tags['ext:author'] FROM duckdb_types() WHERE tags['ext:name'] = ['loadable_extension_demo'] ORDER BY type_name;
+SELECT type_name, tags['ext:author'] FROM duckdb_types() WHERE tags['ext:name'] = 'loadable_extension_demo' ORDER BY type_name;
 ----
-POINT	[DuckDB Labs]
\ No newline at end of file
+POINT	DuckDB Labs
\ No newline at end of file
diff --git a/test/fuzzer/duckfuzz/duck_fuzz_column_binding_tests.test b/test/fuzzer/duckfuzz/duck_fuzz_column_binding_tests.test
index 977f430a7487..9d27ebd296b7 100644
--- a/test/fuzzer/duckfuzz/duck_fuzz_column_binding_tests.test
+++ b/test/fuzzer/duckfuzz/duck_fuzz_column_binding_tests.test
@@ -4,6 +4,8 @@
 
 require tpch
 
+require icu
+
 statement ok
 create table all_types as select * exclude(small_enum, medium_enum, large_enum) from test_all_types() limit 0;
 
@@ -20,20 +22,18 @@ ON        ( ref_10.dec38_10 IS NOT NULL)
                                  ( SELECT ref_9."float" FROM main.all_types)
 ON         ((ref_9."smallint" = ref_8."smallint"))
 
-# original query from fuzzer
-statement ok
-SELECT ref_8.uint AS c0, CASE  WHEN ((min_by(CAST(ref_8."varchar" AS VARCHAR), CAST(ref_3."varchar" AS VARCHAR)) OVER (PARTITION BY subq_3.c1, ref_10.double_array ORDER BY ref_7."varchar") ~~* ref_10."varchar")) THEN (argmin(CAST(ref_6."timestamp" AS TIMESTAMP), CAST(ref_10."timestamp" AS TIMESTAMP)) OVER (PARTITION BY ref_10.nested_int_array, ref_10."timestamp" ORDER BY ref_8.hugeint)) ELSE argmin(CAST(ref_6."timestamp" AS TIMESTAMP), CAST(ref_10."timestamp" AS TIMESTAMP)) OVER (PARTITION BY ref_10.nested_int_array, ref_10."timestamp" ORDER BY ref_8.hugeint) END AS c1 FROM main.all_types AS ref_0 INNER JOIN (SELECT ref_1.timestamp_array AS c1 FROM main.all_types AS ref_1 INNER JOIN main.all_types AS ref_2 ON (NULL) WHERE (((CAST(NULL AS VARCHAR) ~~~ CAST(NULL AS VARCHAR)) OR (ref_1.blob IS NOT NULL) OR 0) AND (ref_1.timestamp_ms IS NULL) AND ((SELECT histogram("varchar") FROM main.all_types) IS NOT NULL) AND ((ref_2.timestamp_s IS NULL) OR (((ref_2."varchar" ~~~ ref_1."varchar") OR 1 OR (ref_2."varchar" ~~ (SELECT "varchar" FROM main.all_types LIMIT 1 OFFSET 5))) AND (ref_2."varchar" ~~ ref_2."varchar")) OR ((SELECT "varchar" FROM main.all_types LIMIT 1 OFFSET 6) !~~* ref_1."varchar")))) AS subq_0 ON (ref_0."varchar") LEFT JOIN main.all_types AS ref_3 LEFT JOIN (SELECT ref_4.ubigint AS c0, 13 AS c1, ref_4."time" AS c2, ref_4."float" AS c3, (SELECT double_array FROM main.all_types LIMIT 1 OFFSET 6) AS c4 FROM main.all_types AS ref_4 WHERE NULL) AS subq_1 ON ((ref_3."float" = subq_1.c3)) ON (NULL) INNER JOIN (SELECT ref_5.timestamp_ms AS c4 FROM main.all_types AS ref_5) AS subq_2 LEFT JOIN main.all_types AS ref_6 INNER JOIN main.all_types AS ref_7 ON (((SELECT "varchar" FROM main.all_types LIMIT 1 OFFSET 1) ~~* ref_7."varchar")) INNER JOIN main.all_types AS ref_8 INNER JOIN main.all_types AS ref_9 INNER JOIN main.all_types AS ref_10 ON (((ref_9."varchar" !~~* ref_9."varchar") OR (ref_10.dec38_10 IS NOT NULL) OR EXISTS(SELECT ref_9."float" AS c0, ref_9.usmallint AS c1, ref_10."bigint" AS c2, (SELECT bool FROM main.all_types LIMIT 1 OFFSET 5) AS c3, ref_10.nested_int_array AS c4, ref_9.timestamp_ms AS c5, 4 AS c6, ref_11."map" AS c7, (SELECT uint FROM main.all_types LIMIT 1 OFFSET 3) AS c8, ref_9.dec_4_1 AS c9 FROM main.all_types AS ref_11 WHERE (1 AND EXISTS(SELECT ref_10.varchar_array AS c0 FROM main.all_types AS ref_12 WHERE 0 LIMIT 149) AND 1) LIMIT 180))) ON (EXISTS(SELECT ref_8.double_array AS c0, ref_10."timestamp" AS c1, ref_8.uuid AS c2, ref_13.dec_9_4 AS c3, ref_13."int" AS c4, ref_13.timestamp_ns AS c5, ref_8."float" AS c6, 63 AS c7 FROM main.all_types AS ref_13 WHERE (ref_13."varchar" ~~ ref_9."varchar"))) ON ((ref_7."smallint" = ref_8."smallint")) INNER JOIN (SELECT ref_14."interval" AS c1 FROM main.all_types AS ref_14 WHERE ref_14."varchar") AS subq_3 ON (EXISTS(SELECT ref_15.utinyint AS c0 FROM main.all_types AS ref_15 WHERE (ref_9."varchar" ~~* (SELECT "varchar" FROM main.all_types LIMIT 1 OFFSET 4)))) ON ((ref_9."varchar" !~~ ref_6."varchar")) ON ((ref_3.timestamp_array = ref_6.timestamp_array))
 
-# https://github.com/duckdb/duckdb-fuzzer/issues/1358
 statement ok
-SELECT subq_0.c6 AS c1, subq_0.c14 AS c2, subq_0.c7 AS c4, subq_0.c4 AS c5 FROM (SELECT (SELECT date FROM main.all_types LIMIT 1 OFFSET 6) AS c3, ref_2."time" AS c4, (SELECT uuid FROM main.all_types LIMIT 1 OFFSET 1) AS c5, ref_3.array_of_structs AS c6, CASE  WHEN (((ref_0."varchar" !~~* ref_1."varchar") OR (ref_5."varchar" ~~~ ref_6."varchar"))) THEN (ref_2."bigint") ELSE ref_2."bigint" END AS c7, rtrim(CAST(CASE  WHEN ((((ref_8."varchar" ~~~ ref_2."varchar") AND (ref_2."varchar" ~~~ ref_0."varchar")) OR (1 AND (ref_7."varchar" ~~ ref_2."varchar")))) THEN (ref_8."varchar") ELSE ref_8."varchar" END AS VARCHAR), CAST(ref_2."varchar" AS VARCHAR)) AS c9, ref_8.ubigint AS c14 FROM main.all_types AS ref_0 INNER JOIN main.all_types AS ref_1 INNER JOIN main.all_types AS ref_2 ON ((ref_1.int_array = ref_2.int_array)) ON (((ref_2."varchar" !~~ ref_1."varchar") OR (ref_1.blob IS NOT NULL))) INNER JOIN main.all_types AS ref_3 INNER JOIN main.all_types AS ref_4 RIGHT JOIN main.all_types AS ref_5 ON ((ref_4.dec_18_6 = ref_5.dec_18_6)) ON (ref_5."varchar") LEFT JOIN main.all_types AS ref_6 RIGHT JOIN main.all_types AS ref_7 INNER JOIN main.all_types AS ref_8 ON ((ref_7.timestamp_array = ref_8.timestamp_array)) ON (1) ON (NULL) ON ((ref_0.dec_18_6 = ref_3.dec_18_6)) WHERE (ref_5."varchar" ^@ (SELECT "varchar" FROM main.all_types LIMIT 1 OFFSET 6)) LIMIT 96) AS subq_0 WHERE subq_0.c9
+SELECT subq_0.c6 AS c1, subq_0.c14 AS c2, subq_0.c7 AS c4, subq_0.c4 AS c5 FROM (SELECT (SELECT date FROM main.all_types LIMIT 1 OFFSET 6) AS c3, ref_2."time" AS c4, (SELECT uuid FROM main.all_types LIMIT 1 OFFSET 1) AS c5, ref_3.array_of_structs AS c6, CASE  WHEN (((ref_0."varchar" !~~* ref_1."varchar") OR (ref_5."varchar" ~~~ ref_6."varchar"))) THEN (ref_2."bigint") ELSE ref_2."bigint" END AS c7, rtrim(CAST(CASE  WHEN ((((ref_8."varchar" ~~~ ref_2."varchar") AND (ref_2."varchar" ~~~ ref_0."varchar")) OR (1 AND (ref_7."varchar" ~~ ref_2."varchar")))) THEN (ref_8."varchar") ELSE ref_8."varchar" END AS VARCHAR), CAST(ref_2."varchar" AS VARCHAR)) AS c9, ref_8.ubigint AS c14 FROM main.all_types AS ref_0 INNER JOIN main.all_types AS ref_1 INNER JOIN main.all_types AS ref_2 ON ((ref_1.int_array = ref_2.int_array)) ON (((ref_2."varchar" !~~ ref_1."varchar") OR (ref_1.blob IS NOT NULL))) INNER JOIN main.all_types AS ref_3 INNER JOIN main.all_types AS ref_4 RIGHT JOIN main.all_types AS ref_5 ON ((ref_4.dec_18_6 = ref_5.dec_18_6)) ON (ref_5."varchar") LEFT JOIN main.all_types AS ref_6 RIGHT JOIN main.all_types AS ref_7 INNER JOIN main.all_types AS ref_8 ON ((ref_7.timestamp_array = ref_8.timestamp_array)) ON (1) ON (NULL) ON ((ref_0.dec_18_6 = ref_3.dec_18_6)) WHERE (ref_5."varchar" ^@ (SELECT "varchar" FROM main.all_types LIMIT 1 OFFSET 6)) LIMIT 96) AS subq_0 WHERE subq_0.c9;
 
 statement ok
 call dbgen(sf=0.1);
 
-
 # https://github.com/duckdb/duckdb-fuzzer/issues/3240
-statement error
+query I
 SELECT NULL FROM (SELECT CAST(COALESCE(ref_3.ps_partkey, ref_4.p_partkey) AS BIGINT) AS c0, ref_0.n_regionkey AS c1, ref_5.r_regionkey AS c2, ref_0.n_comment AS c3 FROM main.nation AS ref_0 LEFT JOIN main.part AS ref_1 ON ((CAST(NULL AS VARCHAR) !~~ ref_0.n_comment)) RIGHT JOIN main.part AS ref_2 ON (((ref_1.p_size IS NOT NULL) AND 1)) INNER JOIN main.partsupp AS ref_3 LEFT JOIN main.part AS ref_4 RIGHT JOIN main.region AS ref_5 RIGHT JOIN main.part AS ref_6 INNER JOIN main.part AS ref_7 ON (EXISTS(SELECT ref_6.p_comment AS c0, ref_8.c_phone AS c1, ref_8.c_phone AS c2, ref_6.p_mfgr AS c3, ref_7.p_partkey AS c4 FROM main.customer AS ref_8 WHERE (((ref_6.p_container ^@ ref_8.c_mktsegment) AND 0) OR (0 AND (ref_8.c_comment ~~ ref_6.p_mfgr) AND EXISTS(SELECT ref_9.l_linestatus AS c0, 30 AS c1 FROM main.lineitem AS ref_9 WHERE ((ref_7.p_type ~~~ ref_9.l_shipmode) AND (ref_6.p_comment !~~* ref_6.p_container)) LIMIT 50))) LIMIT 37)) ON (1) ON ((ref_4.p_type = ref_7.p_name)) ON ((ref_5.r_name ^@ ref_4.p_brand)) ON ((ref_2.p_type ~~* ref_7.p_comment)) WHERE (ref_5.r_name IS NULL) LIMIT 145) AS subq_0 WHERE (("map"() IS NOT NULL) OR ((SELECT p_name FROM main.part LIMIT 1 OFFSET 2) !~~* subq_0.c3) OR (dayofweek(CAST(now() AS TIMESTAMP WITH TIME ZONE)) IS NULL))
 ----
-<REGEX>:.*Binder Error.*
+
+
+statement ok
+SELECT NULL FROM (SELECT ref_1.o_totalprice AS c0, ref_1.o_totalprice AS c1, 11 AS c2, ref_0.n_regionkey AS c3, ref_0.n_name AS c4, ref_1.o_shippriority AS c5, ref_1.o_orderpriority AS c6 FROM main.nation AS ref_0 INNER JOIN main.orders AS ref_1 ON ((ref_1.o_orderpriority ^@ ref_1.o_comment)) WHERE ((ref_1.o_orderstatus ^@ ref_0.n_name) AND (ref_1.o_comment !~~ (SELECT ps_comment FROM main.partsupp LIMIT 1 OFFSET 68)) AND 1) LIMIT 148) AS subq_0 INNER JOIN main.nation AS ref_2 INNER JOIN main.partsupp AS ref_3 RIGHT JOIN main.customer AS ref_4 ON ((ref_3.ps_comment IS NOT NULL)) LEFT JOIN main.orders AS ref_5 LEFT JOIN main.lineitem AS ref_6 ON ((1 OR (ref_6.l_partkey IS NOT NULL) OR 1)) ON ((ref_3.ps_supplycost = ref_5.o_totalprice)) ON (EXISTS(SELECT ref_4.c_name AS c0, ref_5.o_orderpriority AS c1, ref_5.o_orderpriority AS c2, ref_5.o_orderdate AS c3, subq_1.c0 AS c4, ref_3.ps_suppkey AS c5, 36 AS c6, ref_7.l_commitdate AS c7, ref_3.ps_comment AS c8, subq_1.c2 AS c9 FROM main.lineitem AS ref_7 , (SELECT ref_4.c_phone AS c0, ref_4.c_custkey AS c1, ref_2.n_nationkey AS c2 FROM main.customer AS ref_8 WHERE 0 LIMIT 111) AS subq_1 WHERE (ref_2.n_comment !~~ subq_1.c0) LIMIT 91)) ON ((subq_0.c1 = ref_4.c_acctbal)) RIGHT JOIN main.partsupp AS ref_9 ON ((ref_4.c_mktsegment = ref_9.ps_comment))
diff --git a/test/fuzzer/duckfuzz/semi_join_has_correct_left_right_relations.test b/test/fuzzer/duckfuzz/semi_join_has_correct_left_right_relations.test
index e5f9ca15e06e..0b0435a35559 100644
--- a/test/fuzzer/duckfuzz/semi_join_has_correct_left_right_relations.test
+++ b/test/fuzzer/duckfuzz/semi_join_has_correct_left_right_relations.test
@@ -17,6 +17,9 @@ select * from t1, t2 where a = b and a in (select c from t3);
 statement ok
 create table all_types as select * exclude(small_enum, medium_enum, large_enum) from test_all_types();
 
+statement ok
+pragma disabled_optimizers='empty_result_pullup';
+
 statement error
 SELECT NULL
 FROM   all_types
@@ -44,9 +47,9 @@ SELECT NULL FROM main.all_types AS ref_0 INNER JOIN main.all_types AS ref_1 INNE
 ----
 
 
-# When we disable FilterPullup, we get the expected error
+# When we disable FilterPullup and EmptyResultPullup we get the expected error
 statement ok
-set disabled_optimizers to 'filter_pullup'
+set disabled_optimizers to 'filter_pullup,empty_result_pullup'
 
 statement error
 SELECT NULL FROM main.all_types AS ref_0 INNER JOIN main.all_types AS ref_1 INNER JOIN main.all_types AS ref_2 ON ((SELECT NULL)) INNER JOIN main.all_types ON ((SELECT NULL)) ON (NULL) INNER JOIN main.all_types AS ref_4 ON (ref_4."varchar") INNER JOIN main.all_types AS ref_5 ON (EXISTS(SELECT 79 AS c0 FROM main.all_types AS ref_6 INNER JOIN main.all_types AS ref_7 INNER JOIN main.all_types ON (NULL) ON ((EXISTS(SELECT ref_2.uint AS c0, ref_4."time" AS c1 FROM main.all_types AS ref_9 WHERE (ref_2."varchar" ~~~ ref_5."varchar")) AND (ref_1."varchar" !~~* ref_7."varchar"))) , (SELECT ref_6.utinyint AS c0, (SELECT bit_or(ubigint) FROM main.all_types) AS c1, ref_5.fixed_varchar_array AS c2, ref_10."union" AS c3 FROM main.all_types AS ref_10 WHERE (ref_2."varchar" !~~* ref_0."varchar") LIMIT 159) AS subq_0 WHERE 0 LIMIT 67)) LEFT JOIN main.all_types AS ref_11 ON ((ref_1."time" = ref_11."time")) LIMIT 115
diff --git a/test/fuzzer/pedro/concurrent_catalog_usage.test b/test/fuzzer/pedro/concurrent_catalog_usage.test
index 9d2beaecad67..84127b180df7 100644
--- a/test/fuzzer/pedro/concurrent_catalog_usage.test
+++ b/test/fuzzer/pedro/concurrent_catalog_usage.test
@@ -2,6 +2,8 @@
 # description: Concurrent catalog usage
 # group: [pedro]
 
+# FIXME: add a dependency to 't2' to see how the DependencyManager behaves in that case
+
 statement ok
 CREATE TABLE t2 AS (SELECT 42);
 
diff --git a/test/fuzzer/pedro/foreign_key_binding_issue.test b/test/fuzzer/pedro/foreign_key_binding_issue.test
index 67a18fb62de8..37e0b42f0de8 100644
--- a/test/fuzzer/pedro/foreign_key_binding_issue.test
+++ b/test/fuzzer/pedro/foreign_key_binding_issue.test
@@ -1,5 +1,5 @@
 # name: test/fuzzer/pedro/foreign_key_binding_issue.test
-# description: foreign key binding issue
+# description: Referencing the same column multiple twice in constraint definition.
 # group: [pedro]
 
 statement ok
@@ -8,17 +8,17 @@ PRAGMA enable_verification
 statement error
 CREATE TABLE t0 (c0 INT, UNIQUE (c0, c0));
 ----
-appears twice in primary key constraint
+<REGEX>:Parser Error.*appears twice in primary key constraint.*
 
 statement error
 CREATE TABLE t0 (c0 INT, FOREIGN KEY (c0, c0) REFERENCES t0(c0, c0), UNIQUE (c0, c0));
 ----
-Duplicate primary
+<REGEX>:Parser Error.*duplicate primary key referenced in FOREIGN KEY constraint.*
 
 statement error
 CREATE TABLE t0 (c0 INT, c1 INT, FOREIGN KEY (c0, c0) REFERENCES t0(c0, c1), UNIQUE (c0, c1));
 ----
-Duplicate key
+<REGEX>:Parser Error.*duplicate key specified in FOREIGN KEY constraint.*
 
 statement ok
 CREATE TABLE t0 (c0 INT, c1 INT, FOREIGN KEY (c0, c1) REFERENCES t0(c0, c1), UNIQUE (c0, c1));
diff --git a/test/fuzzer/pedro/index_on_altered_table.test b/test/fuzzer/pedro/index_on_altered_table.test
index cddc405fcfe7..db8883e5eff2 100644
--- a/test/fuzzer/pedro/index_on_altered_table.test
+++ b/test/fuzzer/pedro/index_on_altered_table.test
@@ -16,7 +16,7 @@ ALTER TABLE t4 ADD c1 BLOB;
 statement error
 CREATE INDEX i3 ON t4 (c3);
 ----
-<REGEX>:TransactionContext Error.*Transaction conflict.*cannot add.*index.*table.*altered.*
+<REGEX>:TransactionContext Error.*cannot add.*index.*table.*altered.*
 
 statement ok con1
 COMMIT;
diff --git a/test/fuzzer/sqlsmith/window-leadlag-overflow.test b/test/fuzzer/sqlsmith/window-leadlag-overflow.test
index b50646169201..2328f6627246 100644
--- a/test/fuzzer/sqlsmith/window-leadlag-overflow.test
+++ b/test/fuzzer/sqlsmith/window-leadlag-overflow.test
@@ -9,7 +9,7 @@ WITH all_types AS (
 SELECT lag(c16, COLUMNS(*)) OVER (ROWS BETWEEN 1768 FOLLOWING AND UNBOUNDED FOLLOWING) 
 FROM all_types AS t42(c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16, c17, c18, c19, c20, c21, c22, c23, c24, c25, c26, c27, c28, c29, c30, c31, c32, c33, c34, c35, c36, c37, c38, c39, c40, c41)
 ----
-Overflow in subtraction
+the value is out of range for the destination type
 
 statement ok
 SELECT lead('1e668c84-6cbc-4d41-843e-970c17446f9e', 8479, 3087) OVER (PARTITION BY 9136)
diff --git a/test/issues/general/test_14232.test b/test/issues/general/test_14232.test
index 0a6798dd92d0..633a8f27dfae 100644
--- a/test/issues/general/test_14232.test
+++ b/test/issues/general/test_14232.test
@@ -42,9 +42,15 @@ INSERT INTO t2 (t2a, t2b, t2c, t2d, t2e, t2f, t2g, t2h, t2i) VALUES
 ('t1b', NULL, 16, 19, 17.0, 25, 26.00, '2014-05-04 01:01:00', NULL)
 
 query II rowsort
-SELECT t1a, t1b FROM t1 WHERE t1c IN (
-	SELECT t2c FROM t2 WHERE t1a = t2a)
-GROUP BY t1a, t1b;
+SELECT t1a,
+t1b
+FROM t1
+WHERE t1c IN (SELECT t2c
+FROM t2
+WHERE t1a = t2a)
+GROUP BY t1a,
+t1b,
+ORDER BY t1a
 ----
 t1b	8
 t1c	8
\ No newline at end of file
diff --git a/test/issues/general/test_9399.test_slow b/test/issues/general/test_9399.test_slow
index 86dac620f801..5327aa648c49 100644
--- a/test/issues/general/test_9399.test_slow
+++ b/test/issues/general/test_9399.test_slow
@@ -29,4 +29,4 @@ FROM test GROUP BY c0)
 SELECT * FROM cte
 ORDER BY c0
 ----
-299445 values hashing to 851e651ae21c938efff67bf148818639
+299406 values hashing to e9b6d718674413df6f4d2c3c3c5bd853
diff --git a/test/issues/general/test_9795.test b/test/issues/general/test_9795.test
index 1a094d6933d9..a4557523a1b7 100644
--- a/test/issues/general/test_9795.test
+++ b/test/issues/general/test_9795.test
@@ -28,6 +28,11 @@ select arg_max(a, a collate noaccent) from tbl;
 ----
 p
 
+query I
+select arg_max([a], a collate noaccent) from tbl;
+----
+[p]
+
 query I
 select min(a) from tbl;
 ----
diff --git a/test/issues/rigger/test_589.test b/test/issues/rigger/test_589.test
index 3a8700bcedca..0d27f5ae41ab 100644
--- a/test/issues/rigger/test_589.test
+++ b/test/issues/rigger/test_589.test
@@ -12,4 +12,4 @@ CREATE TABLE t0(c0 INT);
 statement error
 CREATE INDEX i0 ON t0(rowid, c0);
 ----
-<REGEX>:Binder Error:.*Cannot create an index.*
\ No newline at end of file
+<REGEX>:Binder Error:.*cannot create an index.*
\ No newline at end of file
diff --git a/test/optimizer/compare_blob.test b/test/optimizer/compare_blob.test
index 814ff7841057..3ababb58e333 100644
--- a/test/optimizer/compare_blob.test
+++ b/test/optimizer/compare_blob.test
@@ -121,35 +121,35 @@ statement ok
 INSERT INTO t1(c0) VALUES (CAST('' AS BLOB));
 
 query III
-SELECT t0.c0, t1.c0, (t1.c0)>=(CAST(t0.c0 AS BLOB)) FROM t0, v0, t1 WHERE ((t1.c0)>=(CAST(t0.c0 AS BLOB)));
+SELECT t0.c0, t1.c0, (t1.c0)>=(CAST(t0.c0 AS BLOB)) FROM t0, v0, t1 WHERE ((t1.c0)>=(CAST(t0.c0 AS BLOB))) ORDER BY ALL
 ----
-00000000000000000000000000000001	\xABcd	true
-00000000000000000000000000000001	x48656C6C6F	true
-00000000000000000000000000000001	x41	true
-00000000000000000000000000000001	x123456	true
-00000000000000000000000000000001	2119350449	true
-00000000000000000000000000000001	\xABcd	true
-00000000000000000000000000000001	x48656C6C6F	true
-00000000000000000000000000000001	x41	true
-00000000000000000000000000000001	x123456	true
-00000000000000000000000000000001	2119350449	true
-00000000000000000000000000000001	\xABcd	true
-00000000000000000000000000000001	x48656C6C6F	true
-00000000000000000000000000000001	x41	true
-00000000000000000000000000000001	x123456	true
-00000000000000000000000000000001	2119350449	true
-00000000000000000000000000000000	\xABcd	true
-00000000000000000000000000000000	x48656C6C6F	true
-00000000000000000000000000000000	x41	true
-00000000000000000000000000000000	x123456	true
 00000000000000000000000000000000	2119350449	true
-00000000000000000000000000000000	\xABcd	true
-00000000000000000000000000000000	x48656C6C6F	true
-00000000000000000000000000000000	x41	true
-00000000000000000000000000000000	x123456	true
 00000000000000000000000000000000	2119350449	true
-00000000000000000000000000000000	\xABcd	true
-00000000000000000000000000000000	x48656C6C6F	true
-00000000000000000000000000000000	x41	true
+00000000000000000000000000000000	2119350449	true
 00000000000000000000000000000000	x123456	true
-00000000000000000000000000000000	2119350449	true
\ No newline at end of file
+00000000000000000000000000000000	x123456	true
+00000000000000000000000000000000	x123456	true
+00000000000000000000000000000000	x41	true
+00000000000000000000000000000000	x41	true
+00000000000000000000000000000000	x41	true
+00000000000000000000000000000000	x48656C6C6F	true
+00000000000000000000000000000000	x48656C6C6F	true
+00000000000000000000000000000000	x48656C6C6F	true
+00000000000000000000000000000000	\xABcd	true
+00000000000000000000000000000000	\xABcd	true
+00000000000000000000000000000000	\xABcd	true
+00000000000000000000000000000001	2119350449	true
+00000000000000000000000000000001	2119350449	true
+00000000000000000000000000000001	2119350449	true
+00000000000000000000000000000001	x123456	true
+00000000000000000000000000000001	x123456	true
+00000000000000000000000000000001	x123456	true
+00000000000000000000000000000001	x41	true
+00000000000000000000000000000001	x41	true
+00000000000000000000000000000001	x41	true
+00000000000000000000000000000001	x48656C6C6F	true
+00000000000000000000000000000001	x48656C6C6F	true
+00000000000000000000000000000001	x48656C6C6F	true
+00000000000000000000000000000001	\xABcd	true
+00000000000000000000000000000001	\xABcd	true
+00000000000000000000000000000001	\xABcd	true
diff --git a/test/optimizer/perfect_ht.test b/test/optimizer/perfect_ht.test
index 9ce52eb56eb9..ec1d651dcf58 100644
--- a/test/optimizer/perfect_ht.test
+++ b/test/optimizer/perfect_ht.test
@@ -51,9 +51,9 @@ physical_plan	<REGEX>:.*PERFECT_HASH_GROUP_BY.*
 statement error
 PRAGMA perfect_ht_threshold=-1;
 ----
-<REGEX>:Parser Error:.*out of range.*
+<REGEX>:.*out of range.*
 
 statement error
 PRAGMA perfect_ht_threshold=100;
 ----
-<REGEX>:Parser Error:.*out of range.*
\ No newline at end of file
+<REGEX>:.*out of range.*
diff --git a/test/optimizer/pushdown/pushdown_unnest_into_cte.test b/test/optimizer/pushdown/pushdown_unnest_into_cte.test
new file mode 100644
index 000000000000..ea0a56bac144
--- /dev/null
+++ b/test/optimizer/pushdown/pushdown_unnest_into_cte.test
@@ -0,0 +1,31 @@
+# name: test/optimizer/pushdown/pushdown_unnest_into_cte.test
+# description: Parquet filter of IN with 1 argument can be converted to =
+# group: [pushdown]
+
+statement ok
+CREATE TABLE tbl2 as SELECT i as id1, [i-3, i+1, i+2] as somelist FROM generate_series(1, 10_000) s(i);
+
+statement ok
+pragma explain_output='OPTIMIZED_ONLY';
+
+query II
+EXPLAIN SELECT id1, element
+FROM (
+      SELECT id1, UNNEST(somelist) AS element
+      FROM tbl2
+    ) tmp
+WHERE id1=10;
+----
+logical_opt	<REGEX>:.*UNNEST.*SEQ_SCAN.*Filters.*
+
+
+query II
+EXPLAIN WITH tmp AS (
+      SELECT id1, generate_subscripts(somelist, 1) AS index, UNNEST(somelist) AS element
+      FROM tbl2
+)
+SELECT id1, index, element
+FROM tmp
+WHERE id1=10;
+----
+logical_opt	<REGEX>:.*UNNEST.*SEQ_SCAN.*Filters.*
\ No newline at end of file
diff --git a/test/optimizer/pushdown/test_pushdown_or.test b/test/optimizer/pushdown/test_pushdown_or.test
new file mode 100644
index 000000000000..dae544a28691
--- /dev/null
+++ b/test/optimizer/pushdown/test_pushdown_or.test
@@ -0,0 +1,53 @@
+# name: test/optimizer/pushdown/test_pushdown_or.test
+# group: [pushdown]
+
+require tpch
+
+statement ok
+create table t1 as select range a, range b from range(0,1000000) ;
+
+statement ok
+create table t2(a int);
+
+statement ok
+insert into t2 from range(10) t(a);
+
+#statement ok
+#select * from t1 join t2 using (a);
+
+query I
+select a from t1 where a = 30000 or a = 50000 or a = 500;
+----
+500
+30000
+50000
+
+
+statement ok
+call dbgen(sf=1);
+
+query I
+select l_orderkey from lineitem where l_orderkey = 6 or l_orderkey = 5999971;
+----
+6
+5999971
+5999971
+5999971
+5999971
+5999971
+5999971
+
+query II
+select l_orderkey, l_partkey from lineitem where l_orderkey = 6 or l_partkey = 4991;
+----
+6	139636
+403456	4991
+535522	4991
+981987	4991
+2593475	4991
+3237285	4991
+3695110	4991
+4093507	4991
+4437666	4991
+4734181	4991
+5552582	4991
diff --git a/test/optimizer/sampling_pushdown.test b/test/optimizer/sampling_pushdown.test
new file mode 100644
index 000000000000..1113395d80c0
--- /dev/null
+++ b/test/optimizer/sampling_pushdown.test
@@ -0,0 +1,52 @@
+# name: test/optimizer/sampling_pushdown.test
+# description: Test Sampling Pushdown optimization
+# group: [optimizer]
+
+statement ok
+CREATE TABLE integers1(i INTEGER, j INTEGER)
+
+statement ok
+CREATE TABLE integers2(i INTEGER, j INTEGER)
+
+statement ok
+INSERT INTO integers1 VALUES (1,1), (2,2), (3, 3), (4,4)
+
+statement ok
+INSERT INTO integers2 VALUES (1,1), (2,2), (3, 3), (4,4)
+
+
+# tablesample system + seq scan becomes sample scan
+query II
+EXPLAIN SELECT i FROM integers1 tablesample system(0.1%)
+----
+physical_plan	<REGEX>:.*SEQ_SCAN.*System: 0.1%.*
+
+# using sample system + seq scan becomes sample scan
+query II
+EXPLAIN SELECT i FROM integers1 using sample system(0.1%)
+----
+physical_plan	<REGEX>:.*SEQ_SCAN.*System: 0.1%.*
+
+# tablesample system + seq scan with join becomes sample scan with join
+query II
+EXPLAIN SELECT * FROM integers1 tablesample system(0.1%), integers2 tablesample system(0.1%)
+----
+physical_plan	<REGEX>:.*SEQ_SCAN.*System: 0.1%.*
+
+# tablesample bernoulli: no pushdown
+query II
+EXPLAIN SELECT i FROM integers1 tablesample bernoulli(0.1%)
+----
+physical_plan	<REGEX>:.*Bernoulli.*SEQ_SCAN.*
+
+# tablesample reservoir: no pushdown
+query II
+EXPLAIN SELECT i FROM integers1 tablesample reservoir(0.1%)
+----
+physical_plan	<REGEX>:.*RESERVOIR_SAMPLE.*SEQ_SCAN.*
+
+# tablesample system after a derived table: no pushdown
+query II
+EXPLAIN SELECT * FROM integers1, integers2 where integers1.i = integers2.i USING SAMPLE SYSTEM(25%)
+----
+physical_plan	<REGEX>:.*System.*SEQ_SCAN.*
\ No newline at end of file
diff --git a/test/optimizer/sampling_pushdown.test_slow b/test/optimizer/sampling_pushdown.test_slow
new file mode 100644
index 000000000000..8627fb8cc439
--- /dev/null
+++ b/test/optimizer/sampling_pushdown.test_slow
@@ -0,0 +1,44 @@
+# name: test/optimizer/sampling_pushdown.test_slow
+# description: Test the performance of Sampling Pushdown optimization
+# group: [optimizer]
+
+require tpch
+
+statement ok
+CALL DBGEN(sf=0.1);
+
+# tablesample system + seq scan becomes sample scan
+query II
+EXPLAIN ANALYZE SELECT count(*) FROM lineitem tablesample system(0.1%)
+----
+analyzed_plan	<REGEX>:.*TABLE_SCAN.*System: 0.1%.*
+
+# using sample system + seq scan becomes sample scan
+query II
+EXPLAIN ANALYZE SELECT count(*) FROM lineitem using sample system(0.1%)
+----
+analyzed_plan	<REGEX>:.*TABLE_SCAN.*System: 0.1%.*
+
+# tablesample system + seq scan with join becomes sample scan with join
+query II
+EXPLAIN ANALYZE SELECT count(*) FROM lineitem tablesample system(0.1%), orders tablesample system(0.1%)
+----
+analyzed_plan	<REGEX>:.*TABLE_SCAN.*System: 0.1%.*
+
+# tablesample bernoulli: no pushdown
+query II
+EXPLAIN ANALYZE SELECT count(*) FROM lineitem tablesample bernoulli(0.1%)
+----
+analyzed_plan	<REGEX>:.*Bernoulli.*TABLE_SCAN.*
+
+# tablesample reservoir: no pushdown
+query II
+EXPLAIN ANALYZE SELECT count(*) FROM lineitem tablesample reservoir(0.1%)
+----
+analyzed_plan	<REGEX>:.*RESERVOIR_SAMPLE.*TABLE_SCAN.*
+
+# tablesample system after a derived table: no pushdown
+query II
+EXPLAIN ANALYZE SELECT count(*) FROM lineitem, orders where l_orderkey = o_orderkey USING SAMPLE SYSTEM(25%)
+----
+analyzed_plan	<REGEX>:.*System.*TABLE_SCAN.*
\ No newline at end of file
diff --git a/test/optimizer/topn/complex_top_n.test b/test/optimizer/topn/complex_top_n.test
index 6e81978f7c5f..6a366b4fd911 100644
--- a/test/optimizer/topn/complex_top_n.test
+++ b/test/optimizer/topn/complex_top_n.test
@@ -44,7 +44,7 @@ LEFT JOIN (
   LEFT JOIN ORDERITEMVIEW ON ORDERVIEW.ORDER_ID = ORDERITEM_ORDERID
 ) AS Q3J ON (Q3J.Q3P = CTE.CUSTOMER_ID) order by all;
 ----
-285 values hashing to 9b1c4d195c0e3c4de5b190b1ab7b357b
+261 values hashing to 5a4c1b428d36bcb30b04222a764fbafc
 
 query II
 explain WITH CTE AS (
diff --git a/test/parquet/prefetching.test b/test/parquet/prefetching.test
new file mode 100644
index 000000000000..ae1a9a0c2a2b
--- /dev/null
+++ b/test/parquet/prefetching.test
@@ -0,0 +1,33 @@
+# name: test/parquet/prefetching.test
+# description: Test parquet files using the prefetching mechanism
+# group: [parquet]
+
+require parquet
+
+# Normally, local files do not use the prefetching mechanism, however this debugging options will force the mechanism
+statement ok
+set prefetch_all_parquet_files=true;
+
+# With default settings, this query will fail: the incorrectly set index page offsets mess with duckdb's prefetching mechanism
+statement error
+FROM 'data/parquet-testing/incorrect_index_page_offsets.parquet'
+----
+IO Error: The parquet file 'data/parquet-testing/incorrect_index_page_offsets.parquet' seems to have incorrectly set page offsets. This interferes with DuckDB's prefetching optimization. DuckDB may still be able to scan this file by manually disabling the prefetching mechanism using: 'SET disable_parquet_prefetching=true'.
+
+# Now we disable prefetching
+statement ok
+set disable_parquet_prefetching=true;
+
+query IIIIIIIIIII
+FROM 'data/parquet-testing/incorrect_index_page_offsets.parquet'
+----
+0.23	Ideal	E	SI2	61.5	55.0	326	3.95	3.98	2.43	0
+0.21	Premium	E	SI1	59.8	61.0	326	3.89	3.84	2.31	1
+0.23	Good	E	VS1	56.9	65.0	327	4.05	4.07	2.31	2
+0.29	Premium	I	VS2	62.4	58.0	334	4.2	4.23	2.63	3
+0.31	Good	J	SI2	63.3	58.0	335	4.34	4.35	2.75	4
+0.24	Very Good	J	VVS2	62.8	57.0	336	3.94	3.96	2.48	5
+0.24	Very Good	I	VVS1	62.3	57.0	336	3.95	3.98	2.47	6
+0.26	Very Good	H	SI1	61.9	55.0	337	4.07	4.11	2.53	7
+0.22	Fair	E	VS2	65.1	61.0	337	3.87	3.78	2.49	8
+0.23	Very Good	H	VS1	59.4	61.0	338	4.0	4.05	2.39	9
diff --git a/test/parquet/test_parquet_reader.test_slow b/test/parquet/test_parquet_reader.test_slow
index 3ecbc229269f..b366434dd400 100644
--- a/test/parquet/test_parquet_reader.test_slow
+++ b/test/parquet/test_parquet_reader.test_slow
@@ -1317,3 +1317,5 @@ SELECT * FROM parquet_scan('data/parquet-testing/bug2557.parquet') limit 10
 [Lorem, ipsum, dolor, sit, amet, consectetur, adipiscing, elit]	[597, 719]	[1218.803740]
 
 
+statement ok
+from 'data/parquet-testing/bug14120-dict-nulls-only.parquet';
\ No newline at end of file
diff --git a/test/sql/aggregate/aggregates/first_memory_usage.test_slow b/test/sql/aggregate/aggregates/first_memory_usage.test_slow
new file mode 100644
index 000000000000..43c91afe90b6
--- /dev/null
+++ b/test/sql/aggregate/aggregates/first_memory_usage.test_slow
@@ -0,0 +1,14 @@
+# name: test/sql/aggregate/aggregates/first_memory_usage.test_slow
+# description: Issue 14132 - Out of memory on basic hash aggregations with large values/aggregates
+# group: [aggregates]
+
+load __TEST_DIR__/first_memory_usage.db
+
+statement ok
+set memory_limit='500mb';
+
+# this query uses the first() aggregate, which used to use too much memory (it did redundant allocation in Combine)
+# we also limit the number of threads in RadixPartitionedHashtable to limit memory usage when close to the limit
+# we can now easily complete this query
+statement ok
+select distinct on (a) b from (select s a, md5(s::text) b from generate_series(1,10_000_000) as g(s)) limit 10;
diff --git a/test/sql/aggregate/aggregates/histogram_test_all_types.test_slow b/test/sql/aggregate/aggregates/histogram_test_all_types.test_slow
index ff4305945c47..e339057cd9a8 100644
--- a/test/sql/aggregate/aggregates/histogram_test_all_types.test_slow
+++ b/test/sql/aggregate/aggregates/histogram_test_all_types.test_slow
@@ -16,7 +16,7 @@ SELECT histogram[min], histogram[max] FROM (
 	FROM all_types
 )
 ----
-[1]	[1]
+1	1
 
 # binned histogram
 query II
@@ -25,6 +25,6 @@ SELECT histogram[min], histogram[max] FROM (
 	FROM all_types
 )
 ----
-[1]	[1]
+1	1
 
 endloop
diff --git a/test/sql/aggregate/aggregates/test_corr.test b/test/sql/aggregate/aggregates/test_corr.test
index 8b391312e384..4504f2c570b8 100644
--- a/test/sql/aggregate/aggregates/test_corr.test
+++ b/test/sql/aggregate/aggregates/test_corr.test
@@ -18,7 +18,7 @@ NULL
 query I
 select corr(1,1)
 ----
-NULL
+NAN
 
 statement error
 select corr(*)
diff --git a/test/sql/aggregate/aggregates/test_entropy.test b/test/sql/aggregate/aggregates/test_entropy.test
index dad1384ac34b..aad778cff0e6 100644
--- a/test/sql/aggregate/aggregates/test_entropy.test
+++ b/test/sql/aggregate/aggregates/test_entropy.test
@@ -60,6 +60,24 @@ select entropy(name) from names;
 ----
 1.459148
 
+# arrays
+statement ok
+create table array_names as select case when name is null then null else [name] end l from names
+
+query I
+select entropy(l) from array_names;
+----
+1.459148
+
+# array of structs
+statement ok
+create table array_of_structs as select case when name is null then null else [{'name': name}] end l from names
+
+query I
+select entropy(l) from array_of_structs;
+----
+1.459148
+
 query I rowsort
 select entropy(k) over (partition by k%2)
     from aggr;
diff --git a/test/sql/aggregate/aggregates/test_regression.test b/test/sql/aggregate/aggregates/test_regression.test
index 196526459df6..4a807c063446 100644
--- a/test/sql/aggregate/aggregates/test_regression.test
+++ b/test/sql/aggregate/aggregates/test_regression.test
@@ -72,7 +72,7 @@ NULL
 query I
 select regr_slope(1,1)
 ----
-NULL
+NAN
 
 statement error
 select regr_slope(*)
diff --git a/test/sql/aggregate/aggregates/test_skewness.test b/test/sql/aggregate/aggregates/test_skewness.test
index ad3cf4c8a352..3a21a4673269 100644
--- a/test/sql/aggregate/aggregates/test_skewness.test
+++ b/test/sql/aggregate/aggregates/test_skewness.test
@@ -28,7 +28,7 @@ select skewness(*)
 query I
 select skewness (10) from range (5)
 ----
-NULL
+NAN
 
 #Empty Table
 query I
diff --git a/test/sql/aggregate/external/simple_external_aggregate.test_slow b/test/sql/aggregate/external/simple_external_aggregate.test_slow
index 653e6ae93aa3..0bb729b035d7 100644
--- a/test/sql/aggregate/external/simple_external_aggregate.test_slow
+++ b/test/sql/aggregate/external/simple_external_aggregate.test_slow
@@ -38,7 +38,7 @@ set disabled_optimizers to 'compressed_materialization'
 # we can aggregate much more than fits in memory, but at low memory limits this is a bit more tricky
 # however, a large data set will make this test much slower
 statement ok
-pragma memory_limit='300MB'
+pragma memory_limit='350MB'
 
 query I
 select count(*) from (select distinct * from t1)
diff --git a/test/sql/aggregate/having/having_alias.test b/test/sql/aggregate/having/having_alias.test
new file mode 100644
index 000000000000..0b5f3fbedd37
--- /dev/null
+++ b/test/sql/aggregate/having/having_alias.test
@@ -0,0 +1,39 @@
+# name: test/sql/aggregate/having/having_alias.test
+# description: Test aliases in the HAVING clause
+# group: [having]
+
+statement ok
+PRAGMA enable_verification
+
+query II
+SELECT b, sum(a) AS a
+FROM (VALUES (1, 0), (1, 1)) t(a, b)
+GROUP BY b
+HAVING a > 0
+ORDER BY ALL
+----
+0	1
+1	1
+
+# if a reference is both a group and an alias, we prefer to bind to the group
+statement ok
+create table t1(a int);
+
+statement ok
+insert into t1 values (42), (84);
+
+query I
+select a+1 as a from t1 group by a having a=42;
+----
+43
+
+statement ok
+create table t2(a int);
+
+statement ok
+insert into t2 values (42), (84), (42);
+
+query II
+select a as b, sum(a) as a from t2 group by b having a=42;
+----
+42	84
diff --git a/test/sql/alter/add_col/test_add_col_default.test b/test/sql/alter/add_col/test_add_col_default.test
index 73a63ffcfd6c..7b1145713878 100644
--- a/test/sql/alter/add_col/test_add_col_default.test
+++ b/test/sql/alter/add_col/test_add_col_default.test
@@ -1,5 +1,5 @@
 # name: test/sql/alter/add_col/test_add_col_default.test
-# description: Test ALTER TABLE ADD COLUMN: ADD COLUMN with default value
+# description: Test ALTER TABLE ADD COLUMN: ADD COLUMN with a default value.
 # group: [add_col]
 
 statement ok
@@ -12,12 +12,7 @@ statement ok
 ALTER TABLE test ADD COLUMN l INTEGER DEFAULT 3
 
 query III
-SELECT * FROM test
+SELECT i, j, l FROM test
 ----
-1
-1
-3
-2
-2
-3
-
+1	1	3
+2	2	3
diff --git a/test/sql/alter/add_pk/test_add_multi_column_pk.test b/test/sql/alter/add_pk/test_add_multi_column_pk.test
new file mode 100644
index 000000000000..f05cf63b9e1b
--- /dev/null
+++ b/test/sql/alter/add_pk/test_add_multi_column_pk.test
@@ -0,0 +1,28 @@
+# name: test/sql/alter/add_pk/test_add_multi_column_pk.test
+# description: Test adding a multi-column PRIMARY KEY to a table.
+# group: [add_pk]
+
+statement ok
+PRAGMA enable_verification;
+
+statement ok
+CREATE TABLE test (i INTEGER, j INTEGER, d TEXT);
+
+statement ok
+INSERT INTO test VALUES (3, 4, 'hello'), (44, 45, '56');
+
+statement ok
+ALTER TABLE test ADD PRIMARY KEY (i, j);
+
+statement ok
+INSERT INTO test VALUES (1, 1, 'foo'), (1, 2, 'bar');
+
+statement error
+INSERT INTO test VALUES (1, 2, 'oops');
+----
+<REGEX>:Constraint Error.*Duplicate key "i: 1, j: 2" violates primary key constraint.*
+
+statement error
+INSERT INTO test VALUES (NULL, 2, 'nada');
+----
+<REGEX>:Constraint Error.*NOT NULL constraint failed: test.i.*
\ No newline at end of file
diff --git a/test/sql/alter/add_pk/test_add_pk.test b/test/sql/alter/add_pk/test_add_pk.test
new file mode 100644
index 000000000000..ff5e37574ad7
--- /dev/null
+++ b/test/sql/alter/add_pk/test_add_pk.test
@@ -0,0 +1,82 @@
+# name: test/sql/alter/add_pk/test_add_pk.test
+# description: Test adding a PRIMARY KEY to a table.
+# group: [add_pk]
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE test (i INTEGER, j INTEGER)
+
+statement ok
+INSERT INTO test VALUES (1, 1)
+
+statement ok
+ALTER TABLE test ADD PRIMARY KEY (j)
+
+query II
+SELECT * FROM test
+----
+1	1
+
+statement ok
+INSERT INTO test VALUES (1, 2)
+
+statement error
+INSERT INTO test VALUES (2, 1)
+----
+<REGEX>:Constraint Error.*Duplicate key "j: 1" violates primary key constraint.*
+
+statement error
+INSERT INTO test VALUES (2, NULL)
+----
+<REGEX>:Constraint Error.*NOT NULL constraint failed.*
+
+# FIXME: Perform duplicate verification before building the index.
+
+statement error
+ALTER TABLE test ADD PRIMARY KEY (i)
+----
+<REGEX>:Constraint Error.*contains duplicates on indexed column.*
+
+statement error
+ALTER TABLE test ADD PRIMARY KEY (j)
+----
+<REGEX>:Catalog Error.*an index with that name already exists for this table: PRIMARY_test_j.*
+
+# Reverse the column order in the index.
+
+statement ok
+CREATE TABLE reverse (i INTEGER, j INTEGER)
+
+statement ok
+INSERT INTO reverse VALUES (1, 2)
+
+statement ok
+ALTER TABLE reverse ADD PRIMARY KEY (j, i)
+
+statement error
+INSERT INTO reverse VALUES (1, 2)
+----
+<REGEX>:Constraint Error.*Duplicate key "j: 2, i: 1" violates primary key constraint.*
+
+statement error
+INSERT INTO reverse (j, i) VALUES (2, 1)
+----
+<REGEX>:Constraint Error.*Duplicate key "j: 2, i: 1" violates primary key constraint.*
+
+# Use an index scan.
+
+statement ok
+CREATE TABLE scan (i INTEGER, j INTEGER)
+
+statement ok
+INSERT INTO scan SELECT range, range + 1 FROM range(30000);
+
+statement ok
+ALTER TABLE scan ADD PRIMARY KEY (i)
+
+query II
+SELECT * FROM scan WHERE i = 2
+----
+2	3
\ No newline at end of file
diff --git a/test/sql/alter/add_pk/test_add_pk_alter_in_tx.test b/test/sql/alter/add_pk/test_add_pk_alter_in_tx.test
new file mode 100644
index 000000000000..935ba8684187
--- /dev/null
+++ b/test/sql/alter/add_pk/test_add_pk_alter_in_tx.test
@@ -0,0 +1,36 @@
+# name: test/sql/alter/add_pk/test_add_pk_alter_in_tx.test
+# description: Test altering a different constraint in the transaction.
+# group: [add_pk]
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE test (i INTEGER, j INTEGER)
+
+statement ok
+ALTER TABLE test ADD PRIMARY KEY (j);
+
+statement ok
+INSERT INTO test VALUES (1, 1)
+
+statement ok
+BEGIN TRANSACTION
+
+statement ok
+ALTER TABLE test ALTER COLUMN j SET NOT NULL;
+
+# Throw a constraint violation on the transaction-local storage.
+
+statement error
+INSERT INTO test VALUES (2, 1)
+----
+<REGEX>:Constraint Error.*violates primary key constraint.*
+
+statement ok
+ROLLBACK
+
+statement error
+INSERT INTO test VALUES (2, 1)
+----
+<REGEX>:Constraint Error.*violates primary key constraint.*
\ No newline at end of file
diff --git a/test/sql/alter/add_pk/test_add_pk_attach.test b/test/sql/alter/add_pk/test_add_pk_attach.test
new file mode 100644
index 000000000000..6b38d0cf4fd0
--- /dev/null
+++ b/test/sql/alter/add_pk/test_add_pk_attach.test
@@ -0,0 +1,46 @@
+# name: test/sql/alter/add_pk/test_add_pk_attach.test
+# description: Test adding a PRIMARY KEY combined with ATTACH/DETACH.
+# group: [add_pk]
+
+require noforcestorage
+
+require skip_reload
+
+load __TEST_DIR__/test_add_pk_attach.db
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE test (i INTEGER, j INTEGER)
+
+statement ok
+ALTER TABLE test ADD PRIMARY KEY (j)
+
+statement ok
+INSERT INTO test VALUES (1, 1)
+
+statement ok
+ATTACH ':memory:' as memory
+
+statement ok
+USE memory
+
+statement ok
+DETACH test_add_pk_attach
+
+statement ok
+ATTACH '__TEST_DIR__/test_add_pk_attach.db' as test_add_pk_attach
+
+statement ok
+USE test_add_pk_attach
+
+statement error
+ALTER TABLE test ADD PRIMARY KEY (i)
+----
+<REGEX>:Catalog Error.*table "test" can have only one primary key.*
+
+statement error
+INSERT INTO test VALUES (2, 1)
+----
+<REGEX>:Constraint Error.*Duplicate key "j: 1" violates primary key constraint.*
\ No newline at end of file
diff --git a/test/sql/alter/add_pk/test_add_pk_catalog_error.test b/test/sql/alter/add_pk/test_add_pk_catalog_error.test
new file mode 100644
index 000000000000..d14d74a60b05
--- /dev/null
+++ b/test/sql/alter/add_pk/test_add_pk_catalog_error.test
@@ -0,0 +1,45 @@
+# name: test/sql/alter/add_pk/test_add_pk_catalog_error.test
+# description: Test adding a PRIMARY KEY to an invalid table and column.
+# group: [add_pk]
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE test (i INTEGER, j INTEGER)
+
+statement error
+ALTER TABLE test ADD PRIMARY KEY (i_do_not_exist)
+----
+<REGEX>:Catalog Error.*table "test" does not have a column named "i_do_not_exist".*
+
+statement error
+ALTER TABLE i_do_not_exist ADD PRIMARY KEY (i, j)
+----
+<REGEX>:Catalog Error.*Table with name i_do_not_exist does not exist!.*
+
+# Try to add it to a column that already has a UNIQUE index.
+
+statement ok
+CREATE TABLE uniq (i INTEGER UNIQUE, j INTEGER)
+
+statement ok
+INSERT INTO uniq VALUES (1, 10), (2, 20), (3, 30)
+
+statement error
+INSERT INTO uniq VALUES (1, 100)
+----
+<REGEX>:Constraint Error.*Duplicate key "i: 1" violates unique constraint.*
+
+statement ok
+ALTER TABLE uniq ADD PRIMARY KEY (i)
+
+statement error
+INSERT INTO uniq VALUES (1, 101)
+----
+<REGEX>:Constraint Error.*Duplicate key "i: 1" violates.*constraint.*
+
+statement error
+INSERT INTO uniq VALUES (NULL, 100)
+----
+<REGEX>:Constraint Error.*NOT NULL constraint failed: uniq.i.*
\ No newline at end of file
diff --git a/test/sql/alter/add_pk/test_add_pk_commit.test b/test/sql/alter/add_pk/test_add_pk_commit.test
new file mode 100644
index 000000000000..029ed0b6360e
--- /dev/null
+++ b/test/sql/alter/add_pk/test_add_pk_commit.test
@@ -0,0 +1,27 @@
+# name: test/sql/alter/add_pk/test_add_pk_commit.test
+# description: Test committing the transaction-local ADD PRIMARY KEY command.
+# group: [add_pk]
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE test (i INTEGER, j INTEGER)
+
+statement ok
+BEGIN TRANSACTION
+
+statement ok
+INSERT INTO test VALUES (1, 1)
+
+statement ok
+ALTER TABLE test ADD PRIMARY KEY (j)
+
+statement ok
+COMMIT
+
+statement error
+INSERT INTO test VALUES (2, 1)
+----
+<REGEX>:Constraint Error.*Duplicate key "j: 1" violates primary key constraint.*
+
diff --git a/test/sql/alter/add_pk/test_add_pk_drop_and_reload.test b/test/sql/alter/add_pk/test_add_pk_drop_and_reload.test
new file mode 100644
index 000000000000..263a59c633bf
--- /dev/null
+++ b/test/sql/alter/add_pk/test_add_pk_drop_and_reload.test
@@ -0,0 +1,31 @@
+# name: test/sql/alter/add_pk/test_add_pk_drop_and_reload.test
+# description: Test adding a PRIMARY KEY, dropping everything, restarting, and recreating the PRIMARY KEY.
+# group: [add_pk]
+
+load __TEST_DIR__/test_add_pk_drop_reload.db
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE test (i INTEGER, j INTEGER)
+
+statement ok
+ALTER TABLE test ADD PRIMARY KEY (j)
+
+statement ok
+INSERT INTO test VALUES (1, 1)
+
+statement ok
+DROP TABLE test
+
+restart
+
+statement ok
+CREATE TABLE test (i INTEGER, j INTEGER)
+
+statement ok
+INSERT INTO test VALUES (1, 1)
+
+statement ok
+ALTER TABLE test ADD PRIMARY KEY (j)
\ No newline at end of file
diff --git a/test/sql/alter/add_pk/test_add_pk_gaps_in_rowids.test b/test/sql/alter/add_pk/test_add_pk_gaps_in_rowids.test
new file mode 100644
index 000000000000..df87c7bfbc1c
--- /dev/null
+++ b/test/sql/alter/add_pk/test_add_pk_gaps_in_rowids.test
@@ -0,0 +1,28 @@
+# name: test/sql/alter/add_pk/test_add_pk_gaps_in_rowids.test
+# description: Test adding a PRIMARY KEY to a table with gaps in its row IDs.
+# group: [add_pk]
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE integers(i integer)
+
+statement ok
+INSERT INTO integers SELECT * FROM range(50000);
+
+query I
+SELECT i FROM integers WHERE i = 100
+----
+100
+
+statement ok
+DELETE FROM integers WHERE i = 42;
+
+statement ok
+ALTER TABLE integers ADD PRIMARY KEY (i);
+
+query I
+SELECT i FROM integers WHERE i = 100
+----
+100
\ No newline at end of file
diff --git a/test/sql/alter/add_pk/test_add_pk_invalid_data.test b/test/sql/alter/add_pk/test_add_pk_invalid_data.test
new file mode 100644
index 000000000000..fe37f7988039
--- /dev/null
+++ b/test/sql/alter/add_pk/test_add_pk_invalid_data.test
@@ -0,0 +1,57 @@
+# name: test/sql/alter/add_pk/test_add_pk_invalid_data.test
+# description: Test adding a PRIMARY KEY to a table with invalid data.
+# group: [add_pk]
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE duplicates (i INTEGER, j INTEGER)
+
+statement ok
+INSERT INTO duplicates VALUES (1, 10), (2, 20), (3, 30), (1, 100)
+
+statement error
+ALTER TABLE duplicates ADD PRIMARY KEY (i)
+----
+<REGEX>:Constraint Error.*contains duplicates on indexed column.*
+
+# Now test with NULLs.
+
+statement ok
+CREATE TABLE nulls (i INTEGER, j INTEGER);
+
+statement ok
+INSERT INTO nulls VALUES (1, 10), (2, NULL), (3, 30), (4, 40);
+
+statement error
+ALTER TABLE nulls ADD PRIMARY KEY (i, j);
+----
+<REGEX>:Constraint Error.*NOT NULL constraint failed.*
+
+statement ok
+DROP TABLE nulls;
+
+statement ok
+CREATE TABLE nulls (i INTEGER, j INTEGER);
+
+statement ok
+INSERT INTO nulls VALUES (5, 10), (NULL, 20), (7, 30), (8, 100);
+
+statement error
+ALTER TABLE nulls ADD PRIMARY KEY (i)
+----
+<REGEX>:Constraint Error.*NOT NULL constraint failed.*
+
+# Validate NULL on a compound index.
+
+statement ok
+CREATE TABLE nulls_compound (i INTEGER, j INTEGER, k VARCHAR)
+
+statement ok
+INSERT INTO nulls_compound VALUES (1, 10, 'hello'), (2, 20, 'world'), (NULL, NULL, NULL), (3, 100, 'yay');
+
+statement error
+ALTER TABLE nulls_compound ADD PRIMARY KEY (k, i)
+----
+<REGEX>:Constraint Error.*NOT NULL constraint failed.*
diff --git a/test/sql/alter/add_pk/test_add_pk_invalid_type.test b/test/sql/alter/add_pk/test_add_pk_invalid_type.test
new file mode 100644
index 000000000000..8ba9918048a3
--- /dev/null
+++ b/test/sql/alter/add_pk/test_add_pk_invalid_type.test
@@ -0,0 +1,19 @@
+# name: test/sql/alter/add_pk/test_add_pk_invalid_type.test
+# description: Test adding a PRIMARY KEY with an invalid type.
+# group: [add_pk]
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE test (a INTEGER[], b INTEGER)
+
+statement error
+ALTER TABLE test ADD PRIMARY KEY (a)
+----
+<REGEX>:Invalid type Error.*Invalid Type.*Invalid type for index key.*
+
+statement error
+ALTER TABLE test ADD PRIMARY KEY (a, b)
+----
+<REGEX>:Invalid type Error.*Invalid Type.*Invalid type for index key.*
\ No newline at end of file
diff --git a/test/sql/alter/add_pk/test_add_pk_naming_conflict.test b/test/sql/alter/add_pk/test_add_pk_naming_conflict.test
new file mode 100644
index 000000000000..1f74e6b01156
--- /dev/null
+++ b/test/sql/alter/add_pk/test_add_pk_naming_conflict.test
@@ -0,0 +1,51 @@
+# name: test/sql/alter/add_pk/test_add_pk_naming_conflict.test
+# description: Test adding a PRIMARY KEY to a table that already has an index with that name.
+# group: [add_pk]
+
+# There is also a catalog dependency conflict,
+# which prevents altering a table with a dependency (index) on it.
+
+load __TEST_DIR__/test_add_pk_naming_conflict.db
+
+statement ok
+PRAGMA enable_verification;
+
+statement ok
+CREATE TABLE tbl (i INTEGER);
+
+statement ok
+INSERT INTO tbl VALUES (1);
+
+statement ok
+CREATE INDEX PRIMARY_tbl_i ON tbl(i);
+
+statement error
+ALTER TABLE tbl ADD PRIMARY KEY (i);
+----
+<REGEX>:Catalog Error.*an index with that name already exists for this table: PRIMARY_tbl_i.*
+
+restart
+
+statement ok
+PRAGMA enable_verification;
+
+statement error
+ALTER TABLE tbl ADD PRIMARY KEY (i);
+----
+<REGEX>:Catalog Error.*an index with that name already exists for this table: PRIMARY_tbl_i.*
+
+# Let's do it the other way around now.
+
+statement ok
+CREATE TABLE test (i INTEGER)
+
+statement ok
+INSERT INTO test VALUES (1);
+
+statement ok
+ALTER TABLE test ADD PRIMARY KEY (i);
+
+statement error
+CREATE INDEX PRIMARY_test_i ON test(i);
+----
+<REGEX>:Catalog Error.*index with the name PRIMARY_test_i already exists.*
diff --git a/test/sql/alter/add_pk/test_add_pk_reclaim_storage.test_slow b/test/sql/alter/add_pk/test_add_pk_reclaim_storage.test_slow
new file mode 100644
index 000000000000..e0ca8de6539f
--- /dev/null
+++ b/test/sql/alter/add_pk/test_add_pk_reclaim_storage.test_slow
@@ -0,0 +1,44 @@
+# name: test/sql/alter/add_pk/test_add_pk_reclaim_storage.test_slow
+# description: Test that we reclaim the storage after adding and dropping a PK.
+# group: [add_pk]
+
+load __TEST_DIR__/test_add_pk_reclaim_storage.db
+
+statement ok
+PRAGMA disable_checkpoint_on_shutdown
+
+statement ok
+PRAGMA wal_autocheckpoint='1TB';
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+SET force_compression='uncompressed';
+
+loop i 0 10
+
+statement ok
+CREATE TABLE tbl${i} AS SELECT range AS i FROM range(500000)
+
+statement ok
+ALTER TABLE tbl${i} ADD PRIMARY KEY (i);
+
+query I
+SELECT i FROM tbl${i} WHERE i = 500;
+----
+500
+
+statement ok
+CHECKPOINT;
+
+statement ok
+DROP TABLE tbl${i};
+
+statement ok
+CHECKPOINT;
+
+query I nosort expected_blocks
+SELECT round(total_blocks / 100.0) FROM pragma_database_size();
+
+endloop
\ No newline at end of file
diff --git a/test/sql/alter/add_pk/test_add_pk_rollback.test b/test/sql/alter/add_pk/test_add_pk_rollback.test
new file mode 100644
index 000000000000..9c600ed55446
--- /dev/null
+++ b/test/sql/alter/add_pk/test_add_pk_rollback.test
@@ -0,0 +1,57 @@
+# name: test/sql/alter/add_pk/test_add_pk_rollback.test
+# description: Test invalidating the constraint with uncommitted changes.
+# group: [add_pk]
+
+statement ok
+PRAGMA enable_verification;
+
+statement ok
+CREATE TABLE test (i INTEGER, j INTEGER);
+
+statement ok
+BEGIN TRANSACTION
+
+statement ok
+INSERT INTO test VALUES (1, 1);
+
+statement ok
+ALTER TABLE test ADD PRIMARY KEY (j);
+
+statement ok
+INSERT INTO test VALUES (2, 1);
+
+# Throw a constraint violation on the transaction-local storage.
+
+statement error
+COMMIT
+----
+<REGEX>:TransactionContext Error.*Failed to commit: PRIMARY KEY or UNIQUE constraint violated: duplicate key "1".*
+
+# Inserting duplicate values must work after rolling back.
+
+statement ok
+INSERT INTO test VALUES (1, 1), (2, 1), (2, NULL);
+
+# Invalidate the transaction with a constraint violation.
+
+statement ok
+CREATE TABLE other (i INTEGER, j INTEGER)
+
+statement ok
+BEGIN TRANSACTION
+
+statement ok
+INSERT INTO other VALUES (1, 1), (2, 1)
+
+statement ok
+ALTER TABLE other ADD PRIMARY KEY (j)
+
+statement error
+COMMIT
+----
+<REGEX>:TransactionContext Error.*Failed to commit: PRIMARY KEY or UNIQUE constraint violated: duplicate key "1".*
+
+# Inserting duplicate values must work after rolling back.
+
+statement ok
+INSERT INTO test VALUES (1, 1), (2, 1), (2, NULL);
\ No newline at end of file
diff --git a/test/sql/alter/add_pk/test_add_pk_storage.test b/test/sql/alter/add_pk/test_add_pk_storage.test
new file mode 100644
index 000000000000..d5910de8ff15
--- /dev/null
+++ b/test/sql/alter/add_pk/test_add_pk_storage.test
@@ -0,0 +1,29 @@
+# name: test/sql/alter/add_pk/test_add_pk_storage.test
+# description: Test adding and persisting a PRIMARY KEY.
+# group: [add_pk]
+
+load __TEST_DIR__/test_add_pk.db
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE test (i INTEGER, j INTEGER)
+
+statement ok
+ALTER TABLE test ADD PRIMARY KEY (j)
+
+statement ok
+INSERT INTO test VALUES (1, 1)
+
+restart
+
+statement error
+ALTER TABLE test ADD PRIMARY KEY (i)
+----
+<REGEX>:Catalog Error.*table "test" can have only one primary key.*
+
+statement error
+INSERT INTO test VALUES (2, 1)
+----
+<REGEX>:Constraint Error.*Duplicate key "j: 1" violates primary key constraint.*
diff --git a/test/sql/alter/add_pk/test_add_pk_wal.test b/test/sql/alter/add_pk/test_add_pk_wal.test
new file mode 100644
index 000000000000..48f874e4923c
--- /dev/null
+++ b/test/sql/alter/add_pk/test_add_pk_wal.test
@@ -0,0 +1,37 @@
+# name: test/sql/alter/add_pk/test_add_pk_wal.test
+# description: Test persisting the ALTER TABLE ... ADD PRIMARY KEY statement to the WAL.
+# group: [add_pk]
+
+load __TEST_DIR__/test_add_pk_wal.db
+
+statement ok
+PRAGMA disable_checkpoint_on_shutdown
+
+statement ok
+PRAGMA wal_autocheckpoint='1TB';
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE test (i INTEGER, j INTEGER);
+
+statement ok
+INSERT INTO test VALUES (1, 2), (3, 4);
+
+statement ok
+ALTER TABLE test ADD PRIMARY KEY (j)
+
+restart
+
+statement error
+ALTER TABLE test ADD PRIMARY KEY (i)
+----
+<REGEX>:Catalog Error.*table "test" can have only one primary key.*
+
+statement error
+INSERT INTO test VALUES (2, 2)
+----
+<REGEX>:Constraint Error.*Duplicate key "j: 2" violates primary key constraint.*
+
+
diff --git a/test/sql/alter/add_pk/test_add_pk_with_generated_column.test b/test/sql/alter/add_pk/test_add_pk_with_generated_column.test
new file mode 100644
index 000000000000..0b5a8a732fb9
--- /dev/null
+++ b/test/sql/alter/add_pk/test_add_pk_with_generated_column.test
@@ -0,0 +1,39 @@
+# name: test/sql/alter/add_pk/test_add_pk_with_generated_column.test
+# description: Test adding a PRIMARY KEY to a table with a generated column.
+# group: [add_pk]
+
+statement ok
+PRAGMA enable_verification;
+
+statement ok
+CREATE TABLE test (
+	a INT NOT NULL,
+	b INT GENERATED ALWAYS AS (a) VIRTUAL,
+	c INT,
+);
+
+statement ok
+INSERT INTO test VALUES (5, 4);
+
+# Cannot add a PK to generated columns.
+
+statement error
+ALTER TABLE test ADD PRIMARY KEY (b);
+----
+<REGEX>:Binder Error.*cannot create a PRIMARY KEY on a generated column: b.*
+
+statement error
+ALTER TABLE test ADD PRIMARY KEY (b, c);
+----
+<REGEX>:Binder Error.*cannot create a PRIMARY KEY on a generated column: b.*
+
+# Can add a PK to a non-generated column.
+
+statement ok
+ALTER TABLE test ADD PRIMARY KEY (c);
+
+statement error
+INSERT INTO test VALUES (1, 4);
+----
+<REGEX>:Constraint Error.*violates primary key constraint.*
+
diff --git a/test/sql/alter/add_pk/test_add_same_pk_simultaneously.test b/test/sql/alter/add_pk/test_add_same_pk_simultaneously.test
new file mode 100644
index 000000000000..d91906a676dd
--- /dev/null
+++ b/test/sql/alter/add_pk/test_add_same_pk_simultaneously.test
@@ -0,0 +1,47 @@
+# name: test/sql/alter/add_pk/test_add_same_pk_simultaneously.test
+# description: Test adding the same PRIMARY KEY in two different transactions.
+# group: [add_pk]
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE test (i INTEGER, j INTEGER)
+
+statement ok tran1
+BEGIN TRANSACTION
+
+statement ok tran1
+ALTER TABLE test ADD PRIMARY KEY (j)
+
+statement ok tran2
+BEGIN TRANSACTION
+
+# We trigger a Catalog write-write conflict.
+
+statement error tran2
+ALTER TABLE test ADD PRIMARY KEY (j)
+----
+<REGEX>:TransactionContext Error.*cannot add an index to a table that has been altered.*
+
+statement ok tran3
+BEGIN TRANSACTION
+
+statement ok tran1
+ROLLBACK
+
+statement ok tran2
+ROLLBACK
+
+statement ok tran3
+ALTER TABLE test ADD PRIMARY KEY (j)
+
+statement ok tran3
+COMMIT
+
+statement error
+INSERT INTO test VALUES (1, 1), (1, 1)
+----
+<REGEX>:Constraint Error.*PRIMARY KEY or UNIQUE constraint violated.*
+
+
diff --git a/test/sql/alter/add_pk/test_add_same_pk_twice.test b/test/sql/alter/add_pk/test_add_same_pk_twice.test
new file mode 100644
index 000000000000..af77beceae12
--- /dev/null
+++ b/test/sql/alter/add_pk/test_add_same_pk_twice.test
@@ -0,0 +1,43 @@
+# name: test/sql/alter/add_pk/test_add_same_pk_twice.test
+# description: Test adding the same PRIMARY KEY twice.
+# group: [add_pk]
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE test (i INTEGER, j INTEGER)
+
+statement ok
+BEGIN TRANSACTION
+
+statement ok
+ALTER TABLE test ADD PRIMARY KEY (j)
+
+statement error
+ALTER TABLE test ADD PRIMARY KEY (i)
+----
+<REGEX>:Catalog Error.*table "test" can have only one primary key.*
+
+statement ok
+COMMIT
+
+# Without a transaction.
+
+statement ok
+CREATE TABLE other (i INTEGER PRIMARY KEY, j INTEGER)
+
+statement error
+ALTER TABLE other ADD PRIMARY KEY (i, j)
+----
+<REGEX>:Catalog Error.*table "other" can have only one primary key.*
+
+statement error
+ALTER TABLE other ADD PRIMARY KEY (i)
+----
+<REGEX>:Catalog Error.*table "other" can have only one primary key.*
+
+statement error
+ALTER TABLE other ADD PRIMARY KEY (j)
+----
+<REGEX>:Catalog Error.*table "other" can have only one primary key.*
\ No newline at end of file
diff --git a/test/sql/alter/alter_type/test_alter_type.test b/test/sql/alter/alter_type/test_alter_type.test
index e9df4a97bc88..05de46684802 100644
--- a/test/sql/alter/alter_type/test_alter_type.test
+++ b/test/sql/alter/alter_type/test_alter_type.test
@@ -1,7 +1,10 @@
 # name: test/sql/alter/alter_type/test_alter_type.test
-# description: Test ALTER TABLE ALTER TYPE
+# description: Test ALTER TABLE ALTER TYPE.
 # group: [alter_type]
 
+statement ok
+PRAGMA enable_verification
+
 statement ok
 CREATE TABLE test(i INTEGER, j INTEGER)
 
@@ -12,14 +15,14 @@ statement ok
 ALTER TABLE test ALTER i SET DATA TYPE VARCHAR
 
 query TI
-SELECT * FROM test
+SELECT * FROM test ORDER BY ALL
 ----
 1	1
 2	2
 
-# filter on an altered column
+# Filter on an altered column.
 query TI
-SELECT * FROM test WHERE i='1'
+SELECT * FROM test WHERE i = '1'
 ----
 1	1
 
@@ -27,16 +30,49 @@ statement ok
 ALTER TABLE test ALTER i SET DATA TYPE INTEGER
 
 query II
-SELECT * FROM test WHERE i=1
+SELECT * FROM test WHERE i = 1
 ----
 1	1
 
+statement ok
+PRAGMA disable_verification
+
 query I
 SELECT stats(i) FROM test LIMIT 1
 ----
 <REGEX>:.*1.*2.*
 
-# fail when column does not exist
+statement ok
+PRAGMA enable_verification
+
+# Fail on non-existent column.
+statement error
+ALTER TABLE test ALTER not_a_column SET DATA TYPE INTEGER
+----
+<REGEX>:Binder Error.*does not have a column with name.*
+
+# Test ALTER TYPE USING
+
+statement ok
+CREATE TABLE tbl (col STRUCT(i INT));
+
+statement ok
+INSERT INTO tbl SELECT {'i': range} FROM range(5000);
+
+statement ok
+ALTER TABLE tbl ALTER col TYPE USING struct_insert(col, a := 42, b := NULL::VARCHAR);
+
+statement ok
+INSERT INTO tbl VALUES ({'i': 10000, 'a': NULL, 'b': 'hello'});
+
+query I
+SELECT col FROM tbl ORDER BY col DESC LIMIT 3;
+----
+{'i': 10000, 'a': NULL, 'b': hello}
+{'i': 4999, 'a': 42, 'b': NULL}
+{'i': 4998, 'a': 42, 'b': NULL}
+
 statement error
-ALTER TABLE test ALTER nonexistingcolumn SET DATA TYPE INTEGER
+ALTER TABLE tbl ALTER col TYPE;
 ----
+<REGEX>:Parser Error.*Omitting the type is only possible in combination with USING.*
diff --git a/test/sql/attach/attach_default_table.test b/test/sql/attach/attach_default_table.test
new file mode 100644
index 000000000000..4765be1dc921
--- /dev/null
+++ b/test/sql/attach/attach_default_table.test
@@ -0,0 +1,107 @@
+# name: test/sql/attach/attach_default_table.test
+# description: Test ATTACH of a database with a default table
+# group: [attach]
+
+require parquet
+
+require noforcestorage
+
+statement ok
+attach '__TEST_DIR__/test.db' as ddb (default_table 'my_table')
+
+statement error
+FROM ddb
+----
+Catalog Error: Table with name ddb does not exist!
+
+# Now we create the default table
+statement ok
+CREATE OR REPLACE TABLE ddb.my_table AS (SELECT 1337 as value);
+
+# We can query the table by the catalog name
+query I
+from ddb
+----
+1337
+
+# We can query the table using the catalog name plus the table name
+query I
+from ddb.my_table
+----
+1337
+
+# We can query the table using the catalog name, default schema name and table name
+query I
+from ddb.main.my_table
+----
+1337
+
+# Now we create a different table that is actually called my_table in the default catalog
+statement ok
+create table ddb as select 42 as value
+
+# This creates ambiguity: however we can provide the solution to the user in the error message
+statement error
+from ddb
+----
+Catalog Error: Ambiguity detected for 'ddb': this could either refer to the 'Table' 'ddb', or the attached catalog 'ddb' which has a default table. To avoid this error, either detach the catalog and reattach under a different name, or use a fully qualified name for the 'Table': 'memory.main.ddb' or for the Catalog Default Table: 'ddb.main.my_table'.
+
+# Ambiguous no more!
+query I
+from memory.main.ddb
+----
+42
+
+# Join the two tables
+query II
+SELECT
+    t1.value,
+    t2.value
+FROM
+    memory.main.ddb as t1
+JOIN
+    ddb.main.my_table as t2
+ON
+    t1.value != t2.value
+----
+42	1337
+
+statement ok
+use ddb
+
+# We can still query the delta catalog default table by its name
+query I
+from ddb
+----
+1337
+
+# Or by the default delta table name (`delta_table`)
+query I
+from my_table
+----
+1337
+
+# Or by specifying the default schema
+query I
+from main.my_table
+----
+1337
+
+# Swith back to main catalog
+statement ok
+use memory
+
+statement ok
+DROP TABLE memory.main.ddb
+
+statement ok
+CREATE VIEW ddb as SELECT 1
+
+statement error
+FROM ddb
+----
+Catalog Error: Ambiguity detected for 'ddb': this could either refer to the 'View' 'ddb', or the attached catalog 'ddb' which has a default table. To avoid this error, either detach the catalog and reattach under a different name, or use a fully qualified name for the 'View': 'memory.main.ddb' or for the Catalog Default Table: 'ddb.main.my_table'.
+
+# view can be dropped using only the name because the default table is a table not a view
+statement ok
+DROP VIEW ddb;
diff --git a/test/sql/attach/attach_enable_external_access.test b/test/sql/attach/attach_enable_external_access.test
new file mode 100644
index 000000000000..8af9c500c5cd
--- /dev/null
+++ b/test/sql/attach/attach_enable_external_access.test
@@ -0,0 +1,34 @@
+# name: test/sql/attach/attach_enable_external_access.test
+# description: enable_external_access with attached  databases
+# group: [attach]
+
+require skip_reload
+
+# attach multiple different databases
+statement ok
+ATTACH '__TEST_DIR__/attach_access1.db' AS a1
+
+statement ok
+ATTACH '__TEST_DIR__/attach_access2.db' AS a2
+
+statement ok
+SET enable_external_access=false
+
+# we can modify any database that was attached prior to
+statement ok
+CREATE TABLE a1.test (a INTEGER PRIMARY KEY, b INTEGER);
+
+statement ok
+CHECKPOINT a1
+
+# however, we cannot attach new database files
+statement ok
+CREATE TABLE a2.test (a INTEGER PRIMARY KEY, b INTEGER);
+
+statement ok
+CHECKPOINT a2
+
+statement error
+ATTACH '__TEST_DIR__/attach_access3.db' AS a2
+----
+Permission Error
diff --git a/test/sql/attach/attach_export_import.test b/test/sql/attach/attach_export_import.test
index b139699d15c9..129bdcd966d9 100644
--- a/test/sql/attach/attach_export_import.test
+++ b/test/sql/attach/attach_export_import.test
@@ -7,16 +7,42 @@ require skip_reload
 statement ok
 ATTACH ':memory:' AS db1
 
+statement ok
+ATTACH ':memory:' as other
+
+statement ok
+USE db1;
+
 statement ok
 CREATE TABLE db1.integers(i INTEGER);
 
 statement ok
 INSERT INTO db1.integers VALUES (1), (2), (3), (NULL);
 
+# FIXME: when we don't use 'USE' then we have to refer to 'integers' as 'db1.integers'
+# this breaks when re-imported, because the table will be created as just 'integers', not 'db1.integers'
+
+# Create a view that references the integers table
+statement ok
+CREATE VIEW db1.integers_view AS SELECT * FROM integers;
+
+statement ok
+BEGIN TRANSACTION;
+
+# Create a table that should not be exported
+statement ok
+CREATE TABLE other.dont_export_me (i integer);
+
 # now export the db
 statement ok
 EXPORT DATABASE db1 TO '__TEST_DIR__/export_test' (FORMAT CSV)
 
+statement ok
+rollback;
+
+statement ok
+drop table db1.integers CASCADE;
+
 statement error
 SELECT * FROM integers
 ----
@@ -25,10 +51,20 @@ does not exist
 statement ok
 IMPORT DATABASE '__TEST_DIR__/export_test'
 
-query I
+query I nosort q1
 SELECT * FROM integers ORDER BY i NULLS LAST
 ----
-1
-2
-3
-NULL
+
+# FIXME: this seems to be bugged
+# the view doesn't seem to get exported/imported correctly:
+
+# Catalog Error: Table with name integers_view does not exist!
+query I nosort q1
+SELECT * FROM integers_view order by i NULLS LAST;
+----
+
+statement error
+SELECT * FROM other.dont_export_me;
+----
+Catalog Error: Table with name dont_export_me does not exist!
+
diff --git a/test/sql/attach/attach_external_access.test b/test/sql/attach/attach_external_access.test
index 5e451583e477..aa50bf54a30e 100644
--- a/test/sql/attach/attach_external_access.test
+++ b/test/sql/attach/attach_external_access.test
@@ -14,4 +14,4 @@ ATTACH ':memory:' AS db1
 statement error
 ATTACH 'mydb.db' AS db2
 ----
-Attaching on-disk databases is disabled through configuration
+Permission Error
diff --git a/test/sql/attach/attach_index.test b/test/sql/attach/attach_index.test
index a10c33612e93..00bf11246d82 100644
--- a/test/sql/attach/attach_index.test
+++ b/test/sql/attach/attach_index.test
@@ -14,7 +14,10 @@ statement ok
 USE attach_index_db
 
 statement ok
-CREATE TABLE tbl_a (a_id INTEGER PRIMARY KEY, value VARCHAR NOT NULL)
+CREATE TABLE tbl_a (
+	a_id INTEGER PRIMARY KEY,
+	value VARCHAR NOT NULL
+)
 
 statement ok
 CREATE INDEX idx_tbl_a ON tbl_a (value)
diff --git a/test/sql/attach/attach_multi_identifiers.test b/test/sql/attach/attach_multi_identifiers.test
new file mode 100644
index 000000000000..caae69a6ffff
--- /dev/null
+++ b/test/sql/attach/attach_multi_identifiers.test
@@ -0,0 +1,125 @@
+# name: test/sql/attach/attach_multi_identifiers.test
+# description: Test ATTACH with complex identifiers
+# group: [attach]
+
+require skip_reload
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+ATTACH ':memory:' AS db1;
+
+statement ok
+ATTACH ':memory:' AS db2;
+
+statement ok
+CREATE SCHEMA db1.s1;
+
+statement ok
+CREATE SCHEMA db2.s1;
+
+statement ok
+CREATE TABLE db1.s1.t(c INT);
+
+statement ok
+CREATE TABLE db2.s1.t(c INT);
+
+statement ok
+INSERT INTO db1.s1.t VALUES (42);
+
+statement ok
+INSERT INTO db2.s1.t SELECT c * 2 FROM db1.s1.t
+
+query II
+SELECT * FROM db1.s1.t, db2.s1.t
+----
+42	84
+
+query II
+SELECT db1.t.c, db2.t.c FROM db1.s1.t, db2.s1.t
+----
+42	84
+
+query II
+SELECT db1.s1.t.c, db2.s1.t.c FROM db1.s1.t, db2.s1.t
+----
+42	84
+
+query I
+SELECT * EXCLUDE (db1.s1.t.c) FROM db1.s1.t, db2.s1.t
+----
+84
+
+query I
+SELECT * EXCLUDE (DB1.S1.T.C) FROM db1.s1.t, db2.s1.t
+----
+84
+
+query I
+SELECT * EXCLUDE (s1.t.c) FROM db1.s1.t, (SELECT 42) t
+----
+42
+
+# rename
+query I
+SELECT * EXCLUDE (new_col) FROM (SELECT * RENAME (db1.s1.t.c AS new_col) FROM db1.s1.t, db2.s1.t)
+----
+84
+
+query I
+SELECT * EXCLUDE (new_col) FROM (SELECT * RENAME (DB1.S1.T.C AS new_col) FROM db1.s1.t, db2.s1.t)
+----
+84
+
+query I
+SELECT * EXCLUDE (new_col) FROM (SELECT * RENAME (s1.t.c AS new_col) FROM db1.s1.t, (SELECT 42) t)
+----
+42
+
+# struct pack
+query II
+SELECT db1.s1.t, db2.s1.t FROM db1.s1.t, db2.s1.t
+----
+{'c': 42}	{'c': 84}
+
+query II
+SELECT db1.t, db2.t FROM db1.s1.t, db2.s1.t
+----
+{'c': 42}	{'c': 84}
+
+# conflicting identifiers
+statement error
+SELECT c FROM db1.s1.t, db2.s1.t
+----
+<REGEX>:.*Ambiguous reference to column name.*db1.s1.t.c.*db2.s1.t.c.*
+
+statement error
+SELECT t.c FROM db1.s1.t, db2.s1.t
+----
+Ambiguous reference to table
+
+statement error
+SELECT s1.t.c FROM db1.s1.t, db2.s1.t
+----
+Ambiguous reference to table
+
+query I
+SELECT db1.s1.t.c FROM db1.s1.t, db2.s1.t
+----
+42
+
+# generated columns
+statement ok
+CREATE OR REPLACE TABLE db1.s1.t (
+	c INT,
+	c_squared AS (c * c),
+);
+
+statement ok
+INSERT INTO db1.s1.t VALUES (42);
+
+query III
+SELECT * FROM db1.s1.t, db2.s1.t
+----
+42	1764	84
diff --git a/test/sql/attach/attach_view_search_path.test b/test/sql/attach/attach_view_search_path.test
new file mode 100644
index 000000000000..01a1f6eeddd8
--- /dev/null
+++ b/test/sql/attach/attach_view_search_path.test
@@ -0,0 +1,78 @@
+# name: test/sql/attach/attach_view_search_path.test
+# description: Test ATTACH with search path
+# group: [attach]
+
+# avoid loading a storage database because it changes the initial database name
+require noforcestorage
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+ATTACH DATABASE '__TEST_DIR__/view_search_path.db' AS view_search_path;
+
+statement ok
+USE view_search_path
+
+statement ok
+CREATE TABLE my_tbl(i INTEGER)
+
+statement ok
+INSERT INTO my_tbl VALUES (42)
+
+statement ok
+CREATE VIEW my_view AS FROM my_tbl
+
+query I
+FROM my_view
+----
+42
+
+statement ok
+CREATE SCHEMA my_schema
+
+statement ok
+USE my_schema
+
+statement ok
+CREATE TABLE my_tbl(i INTEGER)
+
+statement ok
+INSERT INTO my_tbl VALUES (84)
+
+statement ok
+CREATE VIEW my_view AS FROM my_tbl
+
+query I
+FROM my_view
+----
+84
+
+statement ok
+USE memory
+
+query I
+FROM view_search_path.my_view
+----
+42
+
+query I
+FROM view_search_path.my_schema.my_view
+----
+84
+
+statement ok
+DETACH view_search_path
+
+statement ok
+ATTACH DATABASE '__TEST_DIR__/view_search_path.db' AS view_search_path;
+
+query I
+FROM view_search_path.my_view
+----
+42
+
+query I
+FROM view_search_path.my_schema.my_view
+----
+84
diff --git a/test/sql/attach/attach_views.test b/test/sql/attach/attach_views.test
index 6311e9e702fb..c64c0fbf6505 100644
--- a/test/sql/attach/attach_views.test
+++ b/test/sql/attach/attach_views.test
@@ -2,7 +2,7 @@
 # description: Test views in an attached database
 # group: [attach]
 
-require skip_reload
+require noforcestorage
 
 statement ok
 PRAGMA enable_verification
@@ -46,7 +46,7 @@ SELECT * FROM ${prefix}.v1
 
 # reference tables from different databases in view
 statement ok
-CREATE OR REPLACE VIEW ${prefix}.v1 AS SELECT * FROM ${prefix}.t1 UNION ALL FROM t1 ORDER BY ALL
+CREATE OR REPLACE VIEW ${prefix}.v1 AS SELECT * FROM ${prefix}.t1 UNION ALL FROM memory.t1 ORDER BY ALL
 
 query I
 SELECT * FROM ${prefix}.v1
diff --git a/test/sql/binder/duplicate_alias.test b/test/sql/binder/duplicate_alias.test
new file mode 100644
index 000000000000..040d1010429b
--- /dev/null
+++ b/test/sql/binder/duplicate_alias.test
@@ -0,0 +1,40 @@
+# name: test/sql/binder/duplicate_alias.test
+# description: Duplicate table aliases
+# group: [binder]
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+create table t(i int);
+
+statement ok
+INSERT INTO t VALUES (42);
+
+# this works - since no column is referenced there is no ambiguity
+query I
+SELECT COUNT(*) FROM t, t
+----
+1
+
+# this works - all columns can be uniquely identified - no ambiguity
+query II
+SELECT * FROM (SELECT 42 x) t, (SELECT 84 y) t
+----
+42	84
+
+query II
+SELECT t.x, t.y FROM (SELECT 42 x) t, (SELECT 84 y) t
+----
+42	84
+
+statement error
+SELECT t.z FROM (SELECT 42 x) t, (SELECT 84 y) t
+----
+does not have a column named
+
+# this does not work - "t" is ambiguous
+statement error
+SELECT t.i FROM t, t
+----
+duplicate alias "t"
diff --git a/test/sql/binder/group_by_qualification.test b/test/sql/binder/group_by_qualification.test
new file mode 100644
index 000000000000..74cd9ebcad4c
--- /dev/null
+++ b/test/sql/binder/group_by_qualification.test
@@ -0,0 +1,30 @@
+# name: test/sql/binder/group_by_qualification.test
+# description: Issue #8740 - Error when not using fully qualified field name on select AND group by
+# group: [binder]
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE SCHEMA IF NOT EXISTS prd;
+
+statement ok
+CREATE TABLE prd.ad_stats_v(event_date DATE, app VARCHAR, country VARCHAR, platform VARCHAR);
+
+# mix of column, table.column, schema.table.column qualifications
+statement ok
+select event_date, country
+FROM prd.ad_stats_v
+GROUP by event_date, prd.ad_stats_v.country
+order by country
+
+statement ok
+select event_date, ad_stats_v.country
+FROM prd.ad_stats_v
+GROUP by event_date, prd.ad_stats_v.country
+order by ad_stats_v.country
+
+statement ok
+select event_date, prd.ad_stats_v.country
+FROM prd.ad_stats_v
+GROUP by event_date, prd.ad_stats_v.country;
diff --git a/test/sql/binder/separate_schema_tables.test b/test/sql/binder/separate_schema_tables.test
new file mode 100644
index 000000000000..accd677ebc0b
--- /dev/null
+++ b/test/sql/binder/separate_schema_tables.test
@@ -0,0 +1,163 @@
+# name: test/sql/binder/separate_schema_tables.test
+# description: Test tables in different schemas with the same name
+# group: [binder]
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE SCHEMA IF NOT EXISTS s1;
+
+statement ok
+CREATE SCHEMA IF NOT EXISTS s2;
+
+statement ok
+CREATE SCHEMA IF NOT EXISTS s3;
+
+statement ok
+CREATE TABLE s1.tbl(i INT);
+
+statement ok
+CREATE TABLE s2.tbl(i INT);
+
+statement ok
+CREATE TABLE s3.tbl(i INT);
+
+statement ok
+CREATE TABLE tbl(i INT);
+
+statement ok
+INSERT INTO s1.tbl VALUES (10);
+
+statement ok
+INSERT INTO s2.tbl VALUES (100);
+
+statement ok
+INSERT INTO s3.tbl VALUES (1000);
+
+statement ok
+INSERT INTO tbl VALUES (1);
+
+query III
+SELECT * FROM tbl, s1.tbl, s2.tbl
+----
+1	10	100
+
+query IIII
+SELECT * FROM tbl, s1.tbl, s2.tbl, s3.tbl
+----
+1	10	100	1000
+
+statement error
+SELECT tbl.i FROM s1.tbl, s2.tbl
+----
+s1.tbl or s2.tbl
+
+statement error
+SELECT tbl.i FROM s1.tbl, s2.tbl, s3.tbl
+----
+s1.tbl, s2.tbl or s3.tbl
+
+# struct pack
+query III
+SELECT s1.tbl, s2.tbl, s3.tbl FROM s1.tbl, s2.tbl, s3.tbl
+----
+{'i': 10}	{'i': 100}	{'i': 1000}
+
+# test joins
+statement ok
+CREATE TABLE s1.t AS SELECT 1 id, 's1.t' payload UNION ALL SELECT 10 id, 'AAA' payload
+
+statement ok
+CREATE TABLE s2.t AS SELECT 1 id, 's2.t' payload2 UNION ALL SELECT 100 id, 'BBB' payload2
+
+statement ok
+CREATE TABLE s3.t AS SELECT 1 id, 's3.t' payload3 UNION ALL SELECT 1000 id, 'CCC' payload3
+
+# USING
+query IIII
+SELECT * FROM s1.t JOIN s2.t USING (id) JOIN s3.t USING (id)
+----
+1	s1.t	s2.t	s3.t
+
+# explicit column reference to using column
+query I
+SELECT id FROM s1.t JOIN s2.t USING (id) JOIN s3.t USING (id)
+----
+1
+
+# natural join
+query IIII
+SELECT * FROM s1.t NATURAL JOIN s2.t NATURAL JOIN s3.t
+----
+1	s1.t	s2.t	s3.t
+
+# left join
+query IIIIIII
+SELECT id, s1.t.id, s2.t.id, s3.t.id, s1.t.payload, s2.t.payload2, s3.t.payload3
+FROM s1.t LEFT JOIN s2.t USING (id) LEFT JOIN s3.t USING (id)
+ORDER BY ALL
+----
+1	1	1	1	s1.t	s2.t	s3.t
+10	10	NULL	NULL	AAA	NULL	NULL
+
+# right join
+query IIIIIII
+SELECT id, s1.t.id, s2.t.id, s3.t.id, s1.t.payload, s2.t.payload2, s3.t.payload3
+FROM s1.t RIGHT JOIN s2.t USING (id) RIGHT JOIN s3.t USING (id)
+ORDER BY ALL
+----
+1	1	1	1	s1.t	s2.t	s3.t
+1000	NULL	NULL	1000	NULL	NULL	CCC
+
+# full outer join
+query IIIIIII
+SELECT id, s1.t.id, s2.t.id, s3.t.id, s1.t.payload, s2.t.payload2, s3.t.payload3
+FROM s1.t FULL OUTER JOIN s2.t USING (id) FULL OUTER JOIN s3.t USING (id)
+ORDER BY ALL
+----
+1	1	1	1	s1.t	s2.t	s3.t
+10	10	NULL	NULL	AAA	NULL	NULL
+100	NULL	100	NULL	NULL	BBB	NULL
+1000	NULL	NULL	1000	NULL	NULL	CCC
+
+
+# now do the same with identifiers that differ in case only
+statement ok
+CREATE OR REPLACE TABLE s1.tbl(col INT);
+
+statement ok
+CREATE OR REPLACE TABLE s2.TBL(COL INT);
+
+statement ok
+CREATE OR REPLACE TABLE s3.Tbl(Col INT);
+
+statement ok
+INSERT INTO s1.tbl VALUES (10);
+
+statement ok
+INSERT INTO s2.tbl VALUES (100);
+
+statement ok
+INSERT INTO s3.tbl VALUES (1000);
+
+query IIII
+SELECT * FROM tbl, s1.tbl, s2.tbl, s3.tbl
+----
+1	10	100	1000
+
+statement error
+SELECT tbl.col FROM s1.tbl, s2.tbl
+----
+s1.tbl or s2.TBL
+
+statement error
+SELECT tbl.col FROM s1.tbl, s2.tbl, s3.tbl
+----
+s1.tbl, s2.TBL or s3.Tbl
+
+# struct pack
+query III
+SELECT s1.tbl, s2.tbl, s3.tbl FROM s1.tbl, s2.tbl, s3.tbl
+----
+{'col': 10}	{'COL': 100}	{'Col': 1000}
diff --git a/test/sql/binder/table_view_alias.test b/test/sql/binder/table_view_alias.test
new file mode 100644
index 000000000000..f54e89e9723b
--- /dev/null
+++ b/test/sql/binder/table_view_alias.test
@@ -0,0 +1,43 @@
+# name: test/sql/binder/table_view_alias.test
+# description: Test table/view aliasing
+# group: [binder]
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE SCHEMA s1;
+
+statement ok
+CREATE VIEW s1.v AS SELECT 42 c;
+
+statement ok
+CREATE TABLE s1.t AS SELECT 42 c
+
+query III
+SELECT s1.v.c, v.c, c FROM s1.v
+----
+42	42	42
+
+query III
+SELECT s1.t.c, t.c, c FROM s1.t
+----
+42	42	42
+
+# explicitly aliasing the table, even if it is to the same name, should make the schema reference no longer work
+statement error
+SELECT s1.t.c, t.c, c FROM s1.t AS t
+----
+Referenced table "s1" not found
+
+statement error
+SELECT s1.x.c FROM s1.v AS x
+----
+
+statement error
+SELECT s1.v.c FROM s1.v AS x
+----
+
+statement error
+SELECT s1.v.c FROM s1.v AS v
+----
diff --git a/test/sql/binder/test_having_alias.test b/test/sql/binder/test_having_alias.test
index bfb0f2e06aa5..573ae895c1b8 100644
--- a/test/sql/binder/test_having_alias.test
+++ b/test/sql/binder/test_having_alias.test
@@ -46,15 +46,20 @@ SELECT COUNT(i) AS j FROM integers HAVING j=5;
 ----
 5
 
-# These don't work since i in HAVING refers the column instead of the alias
-# (SQLite says: 'Error: a GROUP BY clause is required before HAVING')
-statement error
+
+query I
 SELECT COUNT(i) AS i FROM integers HAVING i=5;
 ----
+5
 
-statement error
+query I
+SELECT COUNT(i) AS i FROM integers GROUP BY i HAVING i=5;
+----
+
+query I
 SELECT COUNT(i) AS i FROM integers HAVING i=5 ORDER BY i;
 ----
+5
 
 # use the same alias multiple times
 query I
diff --git a/test/sql/cast/string_to_map_cast.test_slow b/test/sql/cast/string_to_map_cast.test_slow
index bced678b844f..3a5cd142860e 100644
--- a/test/sql/cast/string_to_map_cast.test_slow
+++ b/test/sql/cast/string_to_map_cast.test_slow
@@ -220,7 +220,7 @@ statement ok
 INSERT INTO Duck_tbl VALUES ('{1=Duck, 3=DB}'), ('{12=DuckDB}'), ('{3=DB, 5=🦆 4=Ducky, 7=Duckster}'), ('{0=DuckParty}'), ('{5=DBDuck, 7=Duckster, 1=🦆}'), ('{1="final Quack"}');
 
 query I
-SELECT * FROM Duck_tbl WHERE list_contains(cast(col1 as MAP(INT, VARCHAR))[7], 'Duckster') ;
+SELECT * FROM Duck_tbl WHERE cast(col1 as MAP(INT, VARCHAR))[7] = 'Duckster' ;
 ----
 {3=DB, 5=🦆 4=Ducky, 7=Duckster}
 {5=DBDuck, 7=Duckster, 1=🦆}
diff --git a/test/sql/catalog/did_you_mean.test b/test/sql/catalog/did_you_mean.test
index bac86754556f..92d25f7f20ab 100644
--- a/test/sql/catalog/did_you_mean.test
+++ b/test/sql/catalog/did_you_mean.test
@@ -22,3 +22,35 @@ statement error
 SELECT * FROM bye;
 ----
 Did you mean "test.bye
+
+statement ok
+CREATE SCHEMA a;
+CREATE TABLE a.foo(name text);
+
+statement error
+SELECT * FROM foo;
+----
+Did you mean "a.foo"?
+
+statement ok
+CREATE SCHEMA b;
+CREATE TABLE b.foo(name text);
+
+statement error
+SELECT * FROM foo;
+----
+Did you mean "a.foo or b.foo"?
+
+statement ok
+CREATE SCHEMA c;
+CREATE TABLE c.foo(name text);
+
+statement error
+SELECT * FROM foo;
+----
+Did you mean "a.foo, b.foo, or c.foo"?
+
+statement error
+SELECT * FROM a.fooo;
+----
+Did you mean "foo"?
diff --git a/test/sql/constraints/foreignkey/test_fk_export.test b/test/sql/constraints/foreignkey/test_fk_export.test
index 208f03b67628..e280dbc25309 100644
--- a/test/sql/constraints/foreignkey/test_fk_export.test
+++ b/test/sql/constraints/foreignkey/test_fk_export.test
@@ -66,3 +66,4 @@ DROP TABLE fk_integers;
 
 statement ok
 DROP TABLE pk_integers;
+
diff --git a/test/sql/constraints/foreignkey/test_foreignkey.test b/test/sql/constraints/foreignkey/test_foreignkey.test
index 9c4a2ccce7ce..700c18922d07 100644
--- a/test/sql/constraints/foreignkey/test_foreignkey.test
+++ b/test/sql/constraints/foreignkey/test_foreignkey.test
@@ -182,7 +182,11 @@ statement ok
 INSERT INTO pkt VALUES (1, 11), (2, 12), (3, 13)
 
 statement ok
-CREATE TABLE fkt(j INTEGER, l INTEGER UNIQUE, FOREIGN KEY (j) REFERENCES pkt(i))
+CREATE TABLE fkt(
+	j INTEGER,
+	l INTEGER UNIQUE,
+	FOREIGN KEY (j) REFERENCES pkt(i)
+)
 
 statement ok
 CREATE INDEX k_index ON pkt(k)
diff --git a/test/sql/copy/csv/auto/test_auto_8231.test b/test/sql/copy/csv/auto/test_auto_8231.test
index 199fb8a9303d..d781745cf232 100644
--- a/test/sql/copy/csv/auto/test_auto_8231.test
+++ b/test/sql/copy/csv/auto/test_auto_8231.test
@@ -11,7 +11,7 @@ create view locations_header_trailing_comma as SELECT * from read_csv_auto('data
 query IIIII
 SELECT * from locations_header_trailing_comma
 ----
-1	name	0	0	NULL
+1	name	0	0	value
 
 query IIIIII
 describe locations_header_trailing_comma;
diff --git a/test/sql/copy/csv/auto/test_auto_8860.test b/test/sql/copy/csv/auto/test_auto_8860.test
index f746db251080..7a1d421ee84a 100644
--- a/test/sql/copy/csv/auto/test_auto_8860.test
+++ b/test/sql/copy/csv/auto/test_auto_8860.test
@@ -5,11 +5,10 @@
 statement ok
 PRAGMA enable_verification
 
-
 statement ok
 PRAGMA verify_parallelism
 
 query I
-SELECT count(*) FROM read_csv_auto("data/csv/auto/product_codes_HS17_V202301.csv.gz") ;
+SELECT count(*) FROM read_csv_auto("data/csv/auto/product_codes_HS17_V202301.csv.gz", quote = '"', comment='', delim = ',') ;
 ----
 5384
diff --git a/test/sql/copy/csv/auto/test_csv_auto.test b/test/sql/copy/csv/auto/test_csv_auto.test
index 50e782312fd9..60e64cc678e1 100644
--- a/test/sql/copy/csv/auto/test_csv_auto.test
+++ b/test/sql/copy/csv/auto/test_csv_auto.test
@@ -8,6 +8,21 @@ PRAGMA enable_verification
 statement ok 
 PRAGMA verify_parallelism
 
+query I
+FROM read_csv('data/csv/nullterm.csv')
+----
+world
+
+query I
+FROM read_csv('data/csv/nullterm.csv', quote = '"', escape = '"')
+----
+\0world\0
+
+query I
+FROM read_csv('data/csv/single_quote.csv', quote = '"')
+----
+'Doc'
+
 query I
 select columns FROM sniff_csv('data/csv/auto/mock_duckdb_test_data.csv', ignore_errors = true);
 ----
diff --git a/test/sql/copy/csv/auto/test_early_out.test b/test/sql/copy/csv/auto/test_early_out.test
new file mode 100644
index 000000000000..e447427cf0ce
--- /dev/null
+++ b/test/sql/copy/csv/auto/test_early_out.test
@@ -0,0 +1,11 @@
+# name: test/sql/copy/csv/auto/test_early_out.test
+# group: [auto]
+
+statement ok
+PRAGMA enable_verification
+
+statement error
+SELECT *
+FROM read_csv('data/csv/auto/early_out_error.csv', buffer_size = 8, maximum_line_size = 8, auto_detect = false, columns = {'a': 'integer','b': 'integer','c': 'integer'}, header = true)
+----
+Error when converting column "b". Could not convert string "
" to 'INTEGER'
\ No newline at end of file
diff --git a/test/sql/copy/csv/csv_external_access.test b/test/sql/copy/csv/csv_external_access.test
index 03813a4e1684..9753303df7d2 100644
--- a/test/sql/copy/csv/csv_external_access.test
+++ b/test/sql/copy/csv/csv_external_access.test
@@ -17,22 +17,22 @@ SET enable_external_access=false;
 statement error
 SELECT * FROM read_csv('data/csv/test/date.csv', columns = {'d': 'DATE'});
 ----
-Scanning read_csv files is disabled through configuration
+Permission Error
 
 statement error
 SELECT * FROM read_csv_auto('data/csv/test/date.csv');
 ----
-Scanning read_csv_auto files is disabled through configuration
+Permission Error
 
 statement error
 COPY date_test FROM 'data/csv/test/date.csv';
 ----
-COPY FROM is disabled by configuration
+Permission Error
 
 statement error
 COPY date_test TO '__TEST_DIR__/date.csv'
 ----
-COPY TO is disabled by configuration
+Permission Error
 
 # we also can't just enable external access again
 statement error
@@ -44,4 +44,4 @@ Cannot change enable_external_access setting while database is running
 statement error
 FROM sniff_csv('data/csv/test/date.csv');
 ----
-Permission Error: sniff_csv is disabled through configuration
+Permission Error
diff --git a/test/sql/copy/csv/csv_quoted_newline_incorrect.test b/test/sql/copy/csv/csv_quoted_newline_incorrect.test
index d670330ea9e2..edc70a3a7967 100644
--- a/test/sql/copy/csv/csv_quoted_newline_incorrect.test
+++ b/test/sql/copy/csv/csv_quoted_newline_incorrect.test
@@ -10,7 +10,6 @@ statement ok
 PRAGMA verify_parallelism
 
 # CSV reader skips malformed lines
-query II
-select * from 'data/csv/csv_quoted_newline_odd.csv';
-----
-84	hello world
+statement ok
+from 'data/csv/csv_quoted_newline_odd.csv';
+
diff --git a/test/sql/copy/csv/parallel/csv_parallel_buffer_size.test b/test/sql/copy/csv/parallel/csv_parallel_buffer_size.test
index 9e8e140f38aa..466758becc6f 100644
--- a/test/sql/copy/csv/parallel/csv_parallel_buffer_size.test
+++ b/test/sql/copy/csv/parallel/csv_parallel_buffer_size.test
@@ -6,7 +6,6 @@
 statement ok
 PRAGMA verify_parallelism
 
-
 query III
 SELECT sum(a), sum(b), sum(c) FROM read_csv('data/csv/test/multi_column_integer.csv',  COLUMNS=STRUCT_PACK(a := 'INTEGER', b := 'INTEGER', c := 'INTEGER'), auto_detect='true', delim = '|', buffer_size=30)
 ----
diff --git a/test/sql/copy/csv/parallel/test_parallel_csv.test b/test/sql/copy/csv/parallel/test_parallel_csv.test
index fb95a7cf8da7..b903844ccf41 100644
--- a/test/sql/copy/csv/parallel/test_parallel_csv.test
+++ b/test/sql/copy/csv/parallel/test_parallel_csv.test
@@ -139,12 +139,12 @@ require httpfs
 query II
 select * from read_csv_auto("https://duckdb-public-gzip-test.s3.us-east-2.amazonaws.com/test.csv", header = 0);
 ----
-'foo'	'bar'
+foo	bar
 foo	bar
 
 
 query II
 from read_csv_auto("https://duckdb-public-gzip-test.s3.us-east-2.amazonaws.com/test.csv.gz", header = 0);
 ----
-'foo'	'bar'
+foo	bar
 foo	bar
diff --git a/test/sql/copy/csv/rejects/csv_buffer_size_rejects.test_slow b/test/sql/copy/csv/rejects/csv_buffer_size_rejects.test_slow
index 0d0fac0ffbfa..49f93049c2fa 100644
--- a/test/sql/copy/csv/rejects/csv_buffer_size_rejects.test_slow
+++ b/test/sql/copy/csv/rejects/csv_buffer_size_rejects.test_slow
@@ -7,6 +7,31 @@ require skip_reload
 # Test fails on windows because byte_position is slightly different due to \r
 instead of 
.
 require notwindows
 
+loop buffer_size 7 11
+
+query IIIII
+SELECT typeof(first(column0)), typeof(first(column1)), COUNT(*), SUM(column0), MAX(len(column1)) FROM read_csv_auto(
+    'data/csv/small_bad.csv',
+    buffer_size=${buffer_size},
+    store_rejects = true,
+    columns = {'column0':'INTEGER', 'column1':'VARCHAR'});
+----
+INTEGER	VARCHAR	3	3	1
+
+query IIIIIIIII rowsort
+SELECT * EXCLUDE (scan_id) FROM reject_errors order by all;
+----
+0	3	9	9	1	column0	CAST	C,A	Error when converting column "column0". Could not convert string "C" to 'INTEGER'
+
+statement ok
+DROP TABLE reject_errors;
+
+statement ok
+DROP TABLE reject_scans;
+
+endloop
+
+
 loop buffer_size 5 10
 
 # Ensure that we can get the schema if we reduce the sample size and ignore errors
@@ -22,8 +47,8 @@ BIGINT	VARCHAR	11044	11044	2
 query IIIIIIIIIII rowsort
 SELECT * EXCLUDE (scan_id, user_arguments) FROM reject_scans order by all;
 ----
-0	data/csv/error/mismatch/big_bad.csv	,	"	"	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL
-1	data/csv/error/mismatch/big_bad2.csv	,	"	"	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL
+0	data/csv/error/mismatch/big_bad.csv	,	\0	\0	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL
+1	data/csv/error/mismatch/big_bad2.csv	,	\0	\0	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL
 
 query IIIIIIIII rowsort
 SELECT * EXCLUDE (scan_id) FROM reject_errors order by all;
diff --git a/test/sql/copy/csv/rejects/csv_rejects_read.test b/test/sql/copy/csv/rejects/csv_rejects_read.test
index 0eaeca597967..2a33a2e779b4 100644
--- a/test/sql/copy/csv/rejects/csv_rejects_read.test
+++ b/test/sql/copy/csv/rejects/csv_rejects_read.test
@@ -181,8 +181,8 @@ ON L.num = R.num;
 query IIIIIIIIIII
 SELECT * EXCLUDE (scan_id, file_id) FROM reject_scans ORDER BY ALL;
 ----
-data/csv/error/mismatch/small1.csv	,	"	"	
	0	true	{'num': 'INTEGER','str': 'VARCHAR'}	NULL	NULL	store_rejects=true
-data/csv/error/mismatch/small2.csv	,	"	"	
	0	true	{'num': 'INTEGER','str': 'VARCHAR'}	NULL	NULL	store_rejects=true
+data/csv/error/mismatch/small1.csv	,	\0	\0	
	0	1	{'num': 'INTEGER','str': 'VARCHAR'}	NULL	NULL	store_rejects=true
+data/csv/error/mismatch/small2.csv	,	\0	\0	
	0	1	{'num': 'INTEGER','str': 'VARCHAR'}	NULL	NULL	store_rejects=true
 
 
 query IIIIIIII
diff --git a/test/sql/copy/csv/rejects/csv_rejects_two_tables.test b/test/sql/copy/csv/rejects/csv_rejects_two_tables.test
index d308545706f3..4ad0f15b754f 100644
--- a/test/sql/copy/csv/rejects/csv_rejects_two_tables.test
+++ b/test/sql/copy/csv/rejects/csv_rejects_two_tables.test
@@ -19,8 +19,8 @@ BIGINT	VARCHAR	11044	11044	2
 query IIIIIIIIIIII
 SELECT * EXCLUDE (scan_id) FROM reject_scans order by all;
 ----
-0	data/csv/error/mismatch/big_bad.csv	,	"	"	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL	sample_size=1, store_rejects=true
-1	data/csv/error/mismatch/big_bad2.csv	,	"	"	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL	sample_size=1, store_rejects=true
+0	data/csv/error/mismatch/big_bad.csv	,	\0	\0	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL	sample_size=1, store_rejects=true
+1	data/csv/error/mismatch/big_bad2.csv	,	\0	\0	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL	sample_size=1, store_rejects=true
 
 query IIIIIIIII
 SELECT * EXCLUDE (scan_id) FROM reject_errors order by all;
@@ -54,8 +54,8 @@ BIGINT	VARCHAR	11044	11044	2
 query IIIIIIIIIIII
 SELECT * EXCLUDE (scan_id) FROM reject_scans order by all;
 ----
-0	data/csv/error/mismatch/big_bad.csv	,	"	"	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL	rejects_table='rejects_errors_2', sample_size=1
-1	data/csv/error/mismatch/big_bad2.csv	,	"	"	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL	rejects_table='rejects_errors_2', sample_size=1
+0	data/csv/error/mismatch/big_bad.csv	,	\0	\0	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL	rejects_table='rejects_errors_2', sample_size=1
+1	data/csv/error/mismatch/big_bad2.csv	,	\0	\0	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL	rejects_table='rejects_errors_2', sample_size=1
 
 query IIIIIIIII
 SELECT * EXCLUDE (scan_id) FROM rejects_errors_2 order by all;
@@ -80,8 +80,8 @@ BIGINT	VARCHAR	11044	11044	2
 query IIIIIIIIIIII
 SELECT * EXCLUDE (scan_id) FROM rejects_scan_2 order by all;
 ----
-0	data/csv/error/mismatch/big_bad.csv	,	"	"	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL	rejects_scan='rejects_scan_2', sample_size=1
-1	data/csv/error/mismatch/big_bad2.csv	,	"	"	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL	rejects_scan='rejects_scan_2', sample_size=1
+0	data/csv/error/mismatch/big_bad.csv	,	\0	\0	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL	rejects_scan='rejects_scan_2', sample_size=1
+1	data/csv/error/mismatch/big_bad2.csv	,	\0	\0	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL	rejects_scan='rejects_scan_2', sample_size=1
 
 query IIIIIIIII
 SELECT * EXCLUDE (scan_id) FROM reject_errors order by all;
@@ -106,8 +106,8 @@ query IIIIIIIIIIII
 SELECT * EXCLUDE (scan_id)
 FROM rejects_scan_3 order by all;
 ----
-0	data/csv/error/mismatch/big_bad.csv	,	"	"	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL	rejects_scan='rejects_scan_3', rejects_table='rejects_errors_3', sample_size=1
-1	data/csv/error/mismatch/big_bad2.csv	,	"	"	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL	rejects_scan='rejects_scan_3', rejects_table='rejects_errors_3', sample_size=1
+0	data/csv/error/mismatch/big_bad.csv	,	\0	\0	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL	rejects_scan='rejects_scan_3', rejects_table='rejects_errors_3', sample_size=1
+1	data/csv/error/mismatch/big_bad2.csv	,	\0	\0	
	0	0	{'column0': 'BIGINT','column1': 'VARCHAR'}	NULL	NULL	rejects_scan='rejects_scan_3', rejects_table='rejects_errors_3', sample_size=1
 
 query IIIIIIIII
 SELECT * EXCLUDE (scan_id) FROM rejects_errors_3 order by all;
diff --git a/test/sql/copy/csv/test_bug_9952.test_slow b/test/sql/copy/csv/test_bug_9952.test_slow
index 04e6cbc45995..d191209ae84d 100644
--- a/test/sql/copy/csv/test_bug_9952.test_slow
+++ b/test/sql/copy/csv/test_bug_9952.test_slow
@@ -5,12 +5,10 @@
 statement ok
 PRAGMA enable_verification
 
-# This will fail because by default we will use " as quote if none is found.
 statement error
-FROM 'data/csv/num.tsv.gz'
+FROM read_csv('data/csv/num.tsv.gz',quote = '"')
 ----
 Value with unterminated quote found.
 
-# This works because we are sampling the whole thing
 statement ok
-FROM read_csv('data/csv/num.tsv.gz', sample_size=-1)
+FROM read_csv('data/csv/num.tsv.gz')
diff --git a/test/sql/copy/csv/test_csv_remote.test b/test/sql/copy/csv/test_csv_remote.test
index 162e61c43a70..ae65c70fcaa0 100644
--- a/test/sql/copy/csv/test_csv_remote.test
+++ b/test/sql/copy/csv/test_csv_remote.test
@@ -33,4 +33,4 @@ PRAGMA enable_verification
 query IIIIIIIIIIII
 FROM sniff_csv('https://github.com/duckdb/duckdb/raw/main/data/csv/customer.csv?v=1')
 ----
-,	"	"	
	\0	0	0	[{'name': column00, 'type': BIGINT}, {'name': column01, 'type': VARCHAR}, {'name': column02, 'type': BIGINT}, {'name': column03, 'type': BIGINT}, {'name': column04, 'type': BIGINT}, {'name': column05, 'type': BIGINT}, {'name': column06, 'type': BIGINT}, {'name': column07, 'type': VARCHAR}, {'name': column08, 'type': VARCHAR}, {'name': column09, 'type': VARCHAR}, {'name': column10, 'type': VARCHAR}, {'name': column11, 'type': BIGINT}, {'name': column12, 'type': BIGINT}, {'name': column13, 'type': BIGINT}, {'name': column14, 'type': VARCHAR}, {'name': column15, 'type': VARCHAR}, {'name': column16, 'type': VARCHAR}, {'name': column17, 'type': BIGINT}]	NULL	NULL	NULL	FROM read_csv('https://github.com/duckdb/duckdb/raw/main/data/csv/customer.csv?v=1', auto_detect=false, delim=',', quote='"', escape='"', new_line='
', skip=0, comment='', header=false, columns={'column00': 'BIGINT', 'column01': 'VARCHAR', 'column02': 'BIGINT', 'column03': 'BIGINT', 'column04': 'BIGINT', 'column05': 'BIGINT', 'column06': 'BIGINT', 'column07': 'VARCHAR', 'column08': 'VARCHAR', 'column09': 'VARCHAR', 'column10': 'VARCHAR', 'column11': 'BIGINT', 'column12': 'BIGINT', 'column13': 'BIGINT', 'column14': 'VARCHAR', 'column15': 'VARCHAR', 'column16': 'VARCHAR', 'column17': 'BIGINT'});
+,	"	\	
	\0	0	0	[{'name': column00, 'type': BIGINT}, {'name': column01, 'type': VARCHAR}, {'name': column02, 'type': BIGINT}, {'name': column03, 'type': BIGINT}, {'name': column04, 'type': BIGINT}, {'name': column05, 'type': BIGINT}, {'name': column06, 'type': BIGINT}, {'name': column07, 'type': VARCHAR}, {'name': column08, 'type': VARCHAR}, {'name': column09, 'type': VARCHAR}, {'name': column10, 'type': VARCHAR}, {'name': column11, 'type': BIGINT}, {'name': column12, 'type': BIGINT}, {'name': column13, 'type': BIGINT}, {'name': column14, 'type': VARCHAR}, {'name': column15, 'type': VARCHAR}, {'name': column16, 'type': VARCHAR}, {'name': column17, 'type': BIGINT}]	NULL	NULL	NULL	FROM read_csv('https://github.com/duckdb/duckdb/raw/main/data/csv/customer.csv?v=1', auto_detect=false, delim=',', quote='"', escape='\', new_line='
', skip=0, comment='', header=false, columns={'column00': 'BIGINT', 'column01': 'VARCHAR', 'column02': 'BIGINT', 'column03': 'BIGINT', 'column04': 'BIGINT', 'column05': 'BIGINT', 'column06': 'BIGINT', 'column07': 'VARCHAR', 'column08': 'VARCHAR', 'column09': 'VARCHAR', 'column10': 'VARCHAR', 'column11': 'BIGINT', 'column12': 'BIGINT', 'column13': 'BIGINT', 'column14': 'VARCHAR', 'column15': 'VARCHAR', 'column16': 'VARCHAR', 'column17': 'BIGINT'});
diff --git a/test/sql/copy/csv/test_ignore_errors.test b/test/sql/copy/csv/test_ignore_errors.test
index 3ec5b0d82291..a6c36c9e38f0 100644
--- a/test/sql/copy/csv/test_ignore_errors.test
+++ b/test/sql/copy/csv/test_ignore_errors.test
@@ -5,7 +5,6 @@
 statement ok
 PRAGMA enable_verification
 
-
 statement ok
 CREATE TABLE integers(i INTEGER, j INTEGER);
 
@@ -133,9 +132,11 @@ FROM read_csv('data/csv/titanic.csv', ignore_errors=1) limit 10
 9	1	3	Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)	female	27.0	0	2	347742	11.1333	NULL	S
 10	1	2	Nasser, Mrs. Nicholas (Adele Achem)	female	14.0	1	0	237736	30.0708	NULL	C
 
-query I
-SELECT * FROM read_csv('data/csv/test_ignore_errors.csv', columns = {'Order ref ID': 'VARCHAR'}, ignore_errors=true);
+# If we can't parse even one row, we can't sniff it.
+statement error
+SELECT * FROM read_csv('data/csv/test_ignore_errors.csv', columns = {'Order ref ID': 'VARCHAR'}, delim = ',',  ignore_errors=true);
 ----
+It was not possible to automatically detect the CSV Parsing dialect/types
 
 query IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII
 SELECT * FROM read_csv('data/csv/test_ignore_errors.csv', types = {'Order ref ID': 'VARCHAR'}, ignore_errors=true);
diff --git a/test/sql/copy/csv/test_mixed_line_endings.test b/test/sql/copy/csv/test_mixed_line_endings.test
index 9f7cad5ef3f1..be54cbc20dbd 100644
--- a/test/sql/copy/csv/test_mixed_line_endings.test
+++ b/test/sql/copy/csv/test_mixed_line_endings.test
@@ -10,28 +10,7 @@ PRAGMA enable_verification
 statement ok
 CREATE TABLE test (a INTEGER, b VARCHAR, c INTEGER);
 
-
 query I
 insert into test select * from read_csv_auto('data/csv/test/mixed_line_endings.csv');
 ----
 3
-
-query I
-SELECT LENGTH(b) FROM test ORDER BY a;
-----
-5
-5
-4
-
-query III
-select * from test;
-----
-10	hello	20
-20	world	30
-30	test	30
-
-query RR
-SELECT SUM(a), SUM(c) FROM test;
-----
-60.000000	80.000000
-
diff --git a/test/sql/copy/csv/test_quote_default.test b/test/sql/copy/csv/test_quote_default.test
index d785b1e0ef60..e67a614cb14c 100644
--- a/test/sql/copy/csv/test_quote_default.test
+++ b/test/sql/copy/csv/test_quote_default.test
@@ -5,12 +5,12 @@
 statement ok
 PRAGMA enable_verification
 
-query III
-select quote,escape,delimiter from sniff_csv('data/csv/test_default_option.csv')
+query II
+from read_csv('data/csv/test_default_option.csv', columns = {'a':'varchar', 'b':'integer'}, auto_detect = false, header = true) where b = 1
 ----
-"	"	,
+x,y	1
 
-query III
-select quote,escape,delimiter from sniff_csv('data/csv/test_default_option_2.csv')
+query II
+from read_csv('data/csv/test_default_option_2.csv', columns = {'a':'varchar', 'b':'integer'}, auto_detect = false, header = true, delim = '|') where b = 1
 ----
-"	"	|
\ No newline at end of file
+x|y	1
\ No newline at end of file
diff --git a/test/sql/copy/csv/test_segfault.test b/test/sql/copy/csv/test_segfault.test
index fef0d2520655..ff29a3454206 100644
--- a/test/sql/copy/csv/test_segfault.test
+++ b/test/sql/copy/csv/test_segfault.test
@@ -5,198 +5,11 @@
 statement ok
 PRAGMA enable_verification
 
-query II
-from read_csv('data/csv/segfault.csv',
-   header=false,
-   quote='"',
-   escape = '"',
-   sep=',',
-   ignore_errors=true);
-----
-
-# Also try with multiple ignored lines
-query I
-from read_csv('data/csv/venue_pipe.csv',
-   header=false,
-   quote='"',
-   escape = '"',
-   sep='|',
-   ignore_errors=true, columns = {'a':'varchar'});
-----
-
-statement error
-from 'data/csv/fuzzing/0.csv'
-----
-It was not possible to automatically detect the CSV Parsing
-
-statement ok
-from 'data/csv/fuzzing/1.csv'
-
-
-statement error
-from 'data/csv/fuzzing/2.csv'
-----
-It was not possible to automatically detect the CSV Parsing dialect/types
-
-statement ok
-from 'data/csv/fuzzing/3.csv'
-
-statement ok
-from 'data/csv/fuzzing/4.csv'
-
-statement error
-from 'data/csv/fuzzing/5.csv'
-----
-It was not possible to automatically detect the CSV Parsing dialect/types
-
-statement error
-from 'data/csv/fuzzing/6.csv'
-----
-It was not possible to automatically detect the CSV Parsing dialect/types
-
-statement error
-from 'data/csv/fuzzing/7.csv'
-----
-It was not possible to automatically detect the CSV Parsing dialect/types
-
-statement error
-from 'data/csv/fuzzing/8.csv'
-----
-Invalid unicode (byte sequence mismatch) detected.
-
-statement error
-from 'data/csv/fuzzing/9.csv'
-----
-It was not possible to automatically detect the CSV Parsing dialect/types
-
-statement error
-from 'data/csv/fuzzing/10.csv'
-----
-It was not possible to automatically detect the CSV Parsing dialect/types
-
-statement error
-from 'data/csv/fuzzing/11.csv'
-----
-It was not possible to automatically detect the CSV Parsing dialect/types
-
-statement error
-from 'data/csv/fuzzing/12.csv'
-----
-Invalid unicode (byte sequence mismatch) detected.
-
-statement ok
-from 'data/csv/fuzzing/13.csv'
-
-statement error
-from 'data/csv/fuzzing/14.csv'
-----
-Invalid unicode (byte sequence mismatch) detected.
-
-statement error
-from 'data/csv/fuzzing/15.csv'
-----
-Invalid unicode (byte sequence mismatch) detected.
-
-statement error
-from 'data/csv/fuzzing/16.csv'
-----
-Invalid unicode (byte sequence mismatch) detected.
-
-statement ok
-from 'data/csv/fuzzing/17.csv'
-
-statement error
-from 'data/csv/fuzzing/18.csv'
-----
-It was not possible to automatically detect the CSV Parsing dialect/types
-
-statement ok
-from 'data/csv/fuzzing/19.csv'
+loop i 0 39
+# We don't really care if these pass or fail, as long as they don't segfault
 
-statement error
-from 'data/csv/fuzzing/20.csv'
+statement maybe
+from 'data/csv/fuzzing/{i}.csv'
 ----
-Invalid unicode (byte sequence mismatch) detected.
-
-statement ok
-from 'data/csv/fuzzing/21.csv'
-
-
-statement error
-from 'data/csv/fuzzing/22.csv'
-----
-Invalid unicode (byte sequence mismatch) detected.
-
-statement error
-from 'data/csv/fuzzing/23.csv'
-----
-Invalid unicode (byte sequence mismatch) detected.
-
-statement error
-from 'data/csv/fuzzing/24.csv'
-----
-Invalid unicode (byte sequence mismatch) detected.
-
-statement error
-from 'data/csv/fuzzing/25.csv'
-----
-It was not possible to automatically detect the CSV Parsing dialect/types
-
-statement error
-from 'data/csv/fuzzing/26.csv'
-----
-Invalid unicode (byte sequence mismatch) detected.
-
-statement error
-from 'data/csv/fuzzing/27.csv'
-----
-Invalid unicode (byte sequence mismatch) detected.
-
-statement error
-from 'data/csv/fuzzing/28.csv'
-----
-It was not possible to automatically detect the CSV Parsing dialect/types
-
-statement error
-from 'data/csv/fuzzing/29.csv'
-----
-It was not possible to automatically detect the CSV Parsing dialect/types
-
-statement error
-from 'data/csv/fuzzing/30.csv'
-----
-It was not possible to automatically detect the CSV Parsing dialect/types
-
-statement ok
-from 'data/csv/fuzzing/31.csv'
-
-statement error
-from 'data/csv/fuzzing/32.csv'
-----
-It was not possible to automatically detect the CSV Parsing dialect/types
-
-statement error
-from 'data/csv/fuzzing/33.csv'
-----
-It was not possible to automatically detect the CSV Parsing dialect/types
-
-statement error
-from 'data/csv/fuzzing/34.csv'
-----
-It was not possible to automatically detect the CSV Parsing dialect/types
-
-statement ok
-from 'data/csv/fuzzing/35.csv'
-
-statement error
-from 'data/csv/fuzzing/36.csv'
-----
-It was not possible to automatically detect the CSV Parsing dialect/types
-
-statement ok
-from 'data/csv/fuzzing/37.csv'
-
-statement ok
-from 'data/csv/fuzzing/38.csv'
-
 
+endloop
diff --git a/test/sql/copy/csv/test_skip.test_slow b/test/sql/copy/csv/test_skip.test_slow
index 29522b24dde2..d6f6e6fa8792 100644
--- a/test/sql/copy/csv/test_skip.test_slow
+++ b/test/sql/copy/csv/test_skip.test_slow
@@ -63,12 +63,12 @@ endloop
 query IIIIIIIIIII
 SELECT * EXCLUDE (prompt) from sniff_csv('__TEST_DIR__/skip.csv',skip=3000)
 ----
-,	"	"	
	\0	3000	false	[{'name': column0, 'type': BIGINT}]	NULL	NULL	skip=3000
+,	\0	\0	
	\0	3000	0	[{'name': column0, 'type': BIGINT}]	NULL	NULL	skip=3000
 
 query IIIIIIIIIII
 SELECT * EXCLUDE (prompt) from sniff_csv('__TEST_DIR__/skip.csv',skip=11000)
 ----
-,	"	"	
	\0	11000	false	[{'name': column0, 'type': BIGINT}]	NULL	NULL	skip=11000
+,	\0	\0	
	\0	11000	0	[{'name': column0, 'type': BIGINT}]	NULL	NULL	skip=11000
 
 # Test with different buffer sizes
 
@@ -153,7 +153,7 @@ from read_csv('__TEST_DIR__/skip_2.csv',skip=10000, buffer_size = 26)
 query IIIIIIIIIII
 SELECT * EXCLUDE (prompt) from sniff_csv('__TEST_DIR__/skip_2.csv',skip=10000)
 ----
-,	"	"	
	\0	10000	false	[{'name': column0, 'type': BIGINT}, {'name': column1, 'type': BIGINT}, {'name': column2, 'type': BIGINT}]	NULL	NULL	skip=10000
+,	\0	\0	
	\0	10000	0	[{'name': column0, 'type': BIGINT}, {'name': column1, 'type': BIGINT}, {'name': column2, 'type': BIGINT}]	NULL	NULL	skip=10000
 
 query III
 from read_csv('__TEST_DIR__/skip_2.csv',skip=10000)
diff --git a/test/sql/copy/csv/test_sniff_csv.test b/test/sql/copy/csv/test_sniff_csv.test
index 872162e68735..86983588b57b 100644
--- a/test/sql/copy/csv/test_sniff_csv.test
+++ b/test/sql/copy/csv/test_sniff_csv.test
@@ -11,7 +11,7 @@ require notwindows
 query I
 SELECT Prompt FROM sniff_csv('data/csv/real/lineitem_sample.csv');
 ----
-FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='"', escape='"', new_line='
', skip=0, comment='', header=false, columns={'column00': 'BIGINT', 'column01': 'BIGINT', 'column02': 'BIGINT', 'column03': 'BIGINT', 'column04': 'BIGINT', 'column05': 'DOUBLE', 'column06': 'DOUBLE', 'column07': 'DOUBLE', 'column08': 'VARCHAR', 'column09': 'VARCHAR', 'column10': 'DATE', 'column11': 'DATE', 'column12': 'DATE', 'column13': 'VARCHAR', 'column14': 'VARCHAR', 'column15': 'VARCHAR'}, dateformat='%Y-%m-%d');
+FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='', escape='', new_line='
', skip=0, comment='', header=false, columns={'column00': 'BIGINT', 'column01': 'BIGINT', 'column02': 'BIGINT', 'column03': 'BIGINT', 'column04': 'BIGINT', 'column05': 'DOUBLE', 'column06': 'DOUBLE', 'column07': 'DOUBLE', 'column08': 'VARCHAR', 'column09': 'VARCHAR', 'column10': 'DATE', 'column11': 'DATE', 'column12': 'DATE', 'column13': 'VARCHAR', 'column14': 'VARCHAR', 'column15': 'VARCHAR'}, dateformat='%Y-%m-%d');
 
 query IIIIIIIIIIIIIIII
 FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='"', escape='"', new_line='
', skip=0, comment='', header=false, columns={'column00': 'BIGINT', 'column01': 'BIGINT', 'column02': 'BIGINT', 'column03': 'BIGINT', 'column04': 'BIGINT', 'column05': 'DOUBLE', 'column06': 'DOUBLE', 'column07': 'DOUBLE', 'column08': 'VARCHAR', 'column09': 'VARCHAR', 'column10': 'DATE', 'column11': 'DATE', 'column12': 'DATE', 'column13': 'VARCHAR', 'column14': 'VARCHAR', 'column15': 'VARCHAR'}, dateformat='%Y-%m-%d')  limit 1;
@@ -21,7 +21,7 @@ FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|',
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/real/lineitem_sample.csv');
 ----
-|	"	"	
	\0	0	0	[{'name': column00, 'type': BIGINT}, {'name': column01, 'type': BIGINT}, {'name': column02, 'type': BIGINT}, {'name': column03, 'type': BIGINT}, {'name': column04, 'type': BIGINT}, {'name': column05, 'type': DOUBLE}, {'name': column06, 'type': DOUBLE}, {'name': column07, 'type': DOUBLE}, {'name': column08, 'type': VARCHAR}, {'name': column09, 'type': VARCHAR}, {'name': column10, 'type': DATE}, {'name': column11, 'type': DATE}, {'name': column12, 'type': DATE}, {'name': column13, 'type': VARCHAR}, {'name': column14, 'type': VARCHAR}, {'name': column15, 'type': VARCHAR}]	%Y-%m-%d	NULL	NULL	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='"', escape='"', new_line='
', skip=0, comment='', header=false, columns={'column00': 'BIGINT', 'column01': 'BIGINT', 'column02': 'BIGINT', 'column03': 'BIGINT', 'column04': 'BIGINT', 'column05': 'DOUBLE', 'column06': 'DOUBLE', 'column07': 'DOUBLE', 'column08': 'VARCHAR', 'column09': 'VARCHAR', 'column10': 'DATE', 'column11': 'DATE', 'column12': 'DATE', 'column13': 'VARCHAR', 'column14': 'VARCHAR', 'column15': 'VARCHAR'}, dateformat='%Y-%m-%d');
+|	\0	\0	
	\0	0	0	[{'name': column00, 'type': BIGINT}, {'name': column01, 'type': BIGINT}, {'name': column02, 'type': BIGINT}, {'name': column03, 'type': BIGINT}, {'name': column04, 'type': BIGINT}, {'name': column05, 'type': DOUBLE}, {'name': column06, 'type': DOUBLE}, {'name': column07, 'type': DOUBLE}, {'name': column08, 'type': VARCHAR}, {'name': column09, 'type': VARCHAR}, {'name': column10, 'type': DATE}, {'name': column11, 'type': DATE}, {'name': column12, 'type': DATE}, {'name': column13, 'type': VARCHAR}, {'name': column14, 'type': VARCHAR}, {'name': column15, 'type': VARCHAR}]	%Y-%m-%d	NULL	NULL	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='', escape='', new_line='
', skip=0, comment='', header=false, columns={'column00': 'BIGINT', 'column01': 'BIGINT', 'column02': 'BIGINT', 'column03': 'BIGINT', 'column04': 'BIGINT', 'column05': 'DOUBLE', 'column06': 'DOUBLE', 'column07': 'DOUBLE', 'column08': 'VARCHAR', 'column09': 'VARCHAR', 'column10': 'DATE', 'column11': 'DATE', 'column12': 'DATE', 'column13': 'VARCHAR', 'column14': 'VARCHAR', 'column15': 'VARCHAR'}, dateformat='%Y-%m-%d');
 
 # Test Invalid Path
 statement error
@@ -34,7 +34,7 @@ No files found that match the pattern "data/csv/real/non_ecziste.csv"
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/error/mismatch/big_bad.csv', sample_size=1);
 ----
-,	"	"	
	\0	0	0	[{'name': column0, 'type': BIGINT}, {'name': column1, 'type': VARCHAR}]	NULL	NULL	sample_size=1	FROM read_csv('data/csv/error/mismatch/big_bad.csv', auto_detect=false, delim=',', quote='"', escape='"', new_line='
', skip=0, comment='', header=false, columns={'column0': 'BIGINT', 'column1': 'VARCHAR'}, sample_size=1);
+,	\0	\0	
	\0	0	0	[{'name': column0, 'type': BIGINT}, {'name': column1, 'type': VARCHAR}]	NULL	NULL	sample_size=1	FROM read_csv('data/csv/error/mismatch/big_bad.csv', auto_detect=false, delim=',', quote='', escape='', new_line='
', skip=0, comment='', header=false, columns={'column0': 'BIGINT', 'column1': 'VARCHAR'}, sample_size=1);
 
 statement error
 FROM read_csv('data/csv/error/mismatch/big_bad.csv', auto_detect=false, delim=',', quote='"', escape='"', new_line='
', skip=0, header=0, columns={'column0': 'BIGINT', 'column1': 'VARCHAR'}, sample_size=1);
@@ -44,18 +44,18 @@ Conversion Error: CSV Error on Line: 2176
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/error/mismatch/big_bad.csv', sample_size=10000);
 ----
-,	"	"	
	\0	0	1	[{'name': 1, 'type': VARCHAR}, {'name': A, 'type': VARCHAR}]	NULL	NULL	sample_size=10000	FROM read_csv('data/csv/error/mismatch/big_bad.csv', auto_detect=false, delim=',', quote='"', escape='"', new_line='
', skip=0, comment='', header=true, columns={'1': 'VARCHAR', 'A': 'VARCHAR'}, sample_size=10000);
+,	\0	\0	
	\0	0	1	[{'name': 1, 'type': VARCHAR}, {'name': A, 'type': VARCHAR}]	NULL	NULL	sample_size=10000	FROM read_csv('data/csv/error/mismatch/big_bad.csv', auto_detect=false, delim=',', quote='', escape='', new_line='
', skip=0, comment='', header=true, columns={'1': 'VARCHAR', 'A': 'VARCHAR'}, sample_size=10000);
 
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/error/mismatch/big_bad.csv', sample_size=-1);
 ----
-,	"	"	
	\0	0	1	[{'name': 1, 'type': VARCHAR}, {'name': A, 'type': VARCHAR}]	NULL	NULL	sample_size=-1	FROM read_csv('data/csv/error/mismatch/big_bad.csv', auto_detect=false, delim=',', quote='"', escape='"', new_line='
', skip=0, comment='', header=true, columns={'1': 'VARCHAR', 'A': 'VARCHAR'}, sample_size=-1);
+,	\0	\0	
	\0	0	1	[{'name': 1, 'type': VARCHAR}, {'name': A, 'type': VARCHAR}]	NULL	NULL	sample_size=-1	FROM read_csv('data/csv/error/mismatch/big_bad.csv', auto_detect=false, delim=',', quote='', escape='', new_line='
', skip=0, comment='', header=true, columns={'1': 'VARCHAR', 'A': 'VARCHAR'}, sample_size=-1);
 
 # Test with defined time and timestamp
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/test/dateformat.csv')
 ----
-,	"	"	
	\0	0	0	[{'name': column0, 'type': DATE}]	%d/%m/%Y	NULL	NULL	FROM read_csv('data/csv/test/dateformat.csv', auto_detect=false, delim=',', quote='"', escape='"', new_line='
', skip=0, comment='', header=false, columns={'column0': 'DATE'}, dateformat='%d/%m/%Y');
+,	\0	\0	
	\0	0	0	[{'name': column0, 'type': DATE}]	%d/%m/%Y	NULL	NULL	FROM read_csv('data/csv/test/dateformat.csv', auto_detect=false, delim=',', quote='', escape='', new_line='
', skip=0, comment='', header=false, columns={'column0': 'DATE'}, dateformat='%d/%m/%Y');
 
 query I
 FROM read_csv('data/csv/test/dateformat.csv', auto_detect=false, delim=',', quote='"', escape='"', new_line='
', skip=0, header=false, columns={'column0': 'DATE'}, dateformat='%d/%m/%Y');
@@ -66,7 +66,7 @@ FROM read_csv('data/csv/test/dateformat.csv', auto_detect=false, delim=',', quot
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/auto/time_date_timestamp_yyyy.mm.dd.csv')
 ----
-,	"	"	
	\0	0	1	[{'name': a, 'type': BIGINT}, {'name': b, 'type': VARCHAR}, {'name': t, 'type': TIME}, {'name': d, 'type': DATE}, {'name': ts, 'type': TIMESTAMP}]	%Y.%m.%d	%Y.%m.%d %H:%M:%S	NULL	FROM read_csv('data/csv/auto/time_date_timestamp_yyyy.mm.dd.csv', auto_detect=false, delim=',', quote='"', escape='"', new_line='
', skip=0, comment='', header=true, columns={'a': 'BIGINT', 'b': 'VARCHAR', 't': 'TIME', 'd': 'DATE', 'ts': 'TIMESTAMP'}, dateformat='%Y.%m.%d', timestampformat='%Y.%m.%d %H:%M:%S');
+,	\0	\0	
	\0	0	1	[{'name': a, 'type': BIGINT}, {'name': b, 'type': VARCHAR}, {'name': t, 'type': TIME}, {'name': d, 'type': DATE}, {'name': ts, 'type': TIMESTAMP}]	%Y.%m.%d	%Y.%m.%d %H:%M:%S	NULL	FROM read_csv('data/csv/auto/time_date_timestamp_yyyy.mm.dd.csv', auto_detect=false, delim=',', quote='', escape='', new_line='
', skip=0, comment='', header=true, columns={'a': 'BIGINT', 'b': 'VARCHAR', 't': 'TIME', 'd': 'DATE', 'ts': 'TIMESTAMP'}, dateformat='%Y.%m.%d', timestampformat='%Y.%m.%d %H:%M:%S');
 
 query IIIII
 FROM read_csv('data/csv/auto/time_date_timestamp_yyyy.mm.dd.csv', auto_detect=false, delim=',', quote='"', escape='"', new_line='
', skip=0, header=true, columns={'a': 'BIGINT', 'b': 'VARCHAR', 't': 'TIME', 'd': 'DATE', 'ts': 'TIMESTAMP'}, dateformat='%Y.%m.%d', timestampformat='%Y.%m.%d %H:%M:%S');
@@ -79,7 +79,7 @@ FROM read_csv('data/csv/auto/time_date_timestamp_yyyy.mm.dd.csv', auto_detect=fa
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/inconsistent_cells.csv')
 ----
-,	"	"	
	\0	3	0	[{'name': column0, 'type': BIGINT}, {'name': column1, 'type': BIGINT}, {'name': column2, 'type': BIGINT}, {'name': column3, 'type': BIGINT}, {'name': column4, 'type': BIGINT}]	NULL	NULL	NULL	FROM read_csv('data/csv/inconsistent_cells.csv', auto_detect=false, delim=',', quote='"', escape='"', new_line='
', skip=3, comment='', header=false, columns={'column0': 'BIGINT', 'column1': 'BIGINT', 'column2': 'BIGINT', 'column3': 'BIGINT', 'column4': 'BIGINT'});
+,	\0	\0	
	\0	3	0	[{'name': column0, 'type': BIGINT}, {'name': column1, 'type': BIGINT}, {'name': column2, 'type': BIGINT}, {'name': column3, 'type': BIGINT}, {'name': column4, 'type': BIGINT}]	NULL	NULL	NULL	FROM read_csv('data/csv/inconsistent_cells.csv', auto_detect=false, delim=',', quote='', escape='', new_line='
', skip=3, comment='', header=false, columns={'column0': 'BIGINT', 'column1': 'BIGINT', 'column2': 'BIGINT', 'column3': 'BIGINT', 'column4': 'BIGINT'});
 
 query IIIII
 FROM read_csv('data/csv/inconsistent_cells.csv', auto_detect=false, delim=',', quote='"', escape='"', new_line='
', skip=3, header=false, columns={'column0': 'BIGINT', 'column1': 'BIGINT', 'column2': 'BIGINT', 'column3': 'BIGINT', 'column4': 'BIGINT'});
@@ -91,7 +91,7 @@ FROM read_csv('data/csv/inconsistent_cells.csv', auto_detect=false, delim=',', q
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/timings.csv')
 ----
-|	"	"	
	\0	0	1	[{'name': tool, 'type': VARCHAR}, {'name': sf, 'type': BIGINT}, {'name': day, 'type': DATE}, {'name': batch_type, 'type': VARCHAR}, {'name': q, 'type': VARCHAR}, {'name': parameters, 'type': VARCHAR}, {'name': time, 'type': DOUBLE}]	%Y-%m-%d	NULL	NULL	FROM read_csv('data/csv/timings.csv', auto_detect=false, delim='|', quote='"', escape='"', new_line='
', skip=0, comment='', header=true, columns={'tool': 'VARCHAR', 'sf': 'BIGINT', 'day': 'DATE', 'batch_type': 'VARCHAR', 'q': 'VARCHAR', 'parameters': 'VARCHAR', 'time': 'DOUBLE'}, dateformat='%Y-%m-%d');
+|	\0	\0	
	\0	0	1	[{'name': tool, 'type': VARCHAR}, {'name': sf, 'type': BIGINT}, {'name': day, 'type': DATE}, {'name': batch_type, 'type': VARCHAR}, {'name': q, 'type': VARCHAR}, {'name': parameters, 'type': VARCHAR}, {'name': time, 'type': DOUBLE}]	%Y-%m-%d	NULL	NULL	FROM read_csv('data/csv/timings.csv', auto_detect=false, delim='|', quote='', escape='', new_line='
', skip=0, comment='', header=true, columns={'tool': 'VARCHAR', 'sf': 'BIGINT', 'day': 'DATE', 'batch_type': 'VARCHAR', 'q': 'VARCHAR', 'parameters': 'VARCHAR', 'time': 'DOUBLE'}, dateformat='%Y-%m-%d');
 
 query IIIIIII
 FROM read_csv('data/csv/timings.csv', auto_detect=false, delim='|', quote='"', escape='\', new_line='
', skip=0, header=true, columns={'tool': 'VARCHAR', 'sf': 'BIGINT', 'day': 'DATE', 'batch_type': 'VARCHAR', 'q': 'VARCHAR', 'parameters': 'VARCHAR', 'time': 'DOUBLE'}, dateformat='%Y-%m-%d') order by all limit 1;
@@ -115,7 +115,7 @@ FROM read_csv('data/csv/auto/backslash_escape.csv', auto_detect=false, delim='|'
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/comments/simple.csv');
 ----
-;	"	"	
	#	2	1	[{'name': a, 'type': BIGINT}, {'name': b, 'type': BIGINT}]	NULL	NULL	NULL	FROM read_csv('data/csv/comments/simple.csv', auto_detect=false, delim=';', quote='"', escape='"', new_line='
', skip=2, comment='#', header=true, columns={'a': 'BIGINT', 'b': 'BIGINT'});
+;	\0	\0	
	#	2	1	[{'name': a, 'type': BIGINT}, {'name': b, 'type': BIGINT}]	NULL	NULL	NULL	FROM read_csv('data/csv/comments/simple.csv', auto_detect=false, delim=';', quote='', escape='', new_line='
', skip=2, comment='#', header=true, columns={'a': 'BIGINT', 'b': 'BIGINT'});
 
 # Test Prompt
 query II
diff --git a/test/sql/copy/csv/test_sniff_csv_options.test b/test/sql/copy/csv/test_sniff_csv_options.test
index 677e7a383c39..3491cbcb0216 100644
--- a/test/sql/copy/csv/test_sniff_csv_options.test
+++ b/test/sql/copy/csv/test_sniff_csv_options.test
@@ -20,13 +20,13 @@ It was not possible to automatically detect the CSV Parsing dialect
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/real/lineitem_sample.csv', delim='|');
 ----
-|	"	"	
	\0	0	0	[{'name': column00, 'type': BIGINT}, {'name': column01, 'type': BIGINT}, {'name': column02, 'type': BIGINT}, {'name': column03, 'type': BIGINT}, {'name': column04, 'type': BIGINT}, {'name': column05, 'type': DOUBLE}, {'name': column06, 'type': DOUBLE}, {'name': column07, 'type': DOUBLE}, {'name': column08, 'type': VARCHAR}, {'name': column09, 'type': VARCHAR}, {'name': column10, 'type': DATE}, {'name': column11, 'type': DATE}, {'name': column12, 'type': DATE}, {'name': column13, 'type': VARCHAR}, {'name': column14, 'type': VARCHAR}, {'name': column15, 'type': VARCHAR}]	%Y-%m-%d	NULL	delim='|'	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, quote='"', escape='"', new_line='
', skip=0, comment='', header=false, columns={'column00': 'BIGINT', 'column01': 'BIGINT', 'column02': 'BIGINT', 'column03': 'BIGINT', 'column04': 'BIGINT', 'column05': 'DOUBLE', 'column06': 'DOUBLE', 'column07': 'DOUBLE', 'column08': 'VARCHAR', 'column09': 'VARCHAR', 'column10': 'DATE', 'column11': 'DATE', 'column12': 'DATE', 'column13': 'VARCHAR', 'column14': 'VARCHAR', 'column15': 'VARCHAR'}, dateformat='%Y-%m-%d', delim='|');
+|	\0	\0	
	\0	0	0	[{'name': column00, 'type': BIGINT}, {'name': column01, 'type': BIGINT}, {'name': column02, 'type': BIGINT}, {'name': column03, 'type': BIGINT}, {'name': column04, 'type': BIGINT}, {'name': column05, 'type': DOUBLE}, {'name': column06, 'type': DOUBLE}, {'name': column07, 'type': DOUBLE}, {'name': column08, 'type': VARCHAR}, {'name': column09, 'type': VARCHAR}, {'name': column10, 'type': DATE}, {'name': column11, 'type': DATE}, {'name': column12, 'type': DATE}, {'name': column13, 'type': VARCHAR}, {'name': column14, 'type': VARCHAR}, {'name': column15, 'type': VARCHAR}]	%Y-%m-%d	NULL	delim='|'	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, quote='', escape='', new_line='
', skip=0, comment='', header=false, columns={'column00': 'BIGINT', 'column01': 'BIGINT', 'column02': 'BIGINT', 'column03': 'BIGINT', 'column04': 'BIGINT', 'column05': 'DOUBLE', 'column06': 'DOUBLE', 'column07': 'DOUBLE', 'column08': 'VARCHAR', 'column09': 'VARCHAR', 'column10': 'DATE', 'column11': 'DATE', 'column12': 'DATE', 'column13': 'VARCHAR', 'column14': 'VARCHAR', 'column15': 'VARCHAR'}, dateformat='%Y-%m-%d', delim='|');
 
 # quote
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/real/lineitem_sample.csv', quote='"');
 ----
-|	"	"	
	\0	0	0	[{'name': column00, 'type': BIGINT}, {'name': column01, 'type': BIGINT}, {'name': column02, 'type': BIGINT}, {'name': column03, 'type': BIGINT}, {'name': column04, 'type': BIGINT}, {'name': column05, 'type': DOUBLE}, {'name': column06, 'type': DOUBLE}, {'name': column07, 'type': DOUBLE}, {'name': column08, 'type': VARCHAR}, {'name': column09, 'type': VARCHAR}, {'name': column10, 'type': DATE}, {'name': column11, 'type': DATE}, {'name': column12, 'type': DATE}, {'name': column13, 'type': VARCHAR}, {'name': column14, 'type': VARCHAR}, {'name': column15, 'type': VARCHAR}]	%Y-%m-%d	NULL	quote='"'	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', escape='"', new_line='
', skip=0, comment='', header=false, columns={'column00': 'BIGINT', 'column01': 'BIGINT', 'column02': 'BIGINT', 'column03': 'BIGINT', 'column04': 'BIGINT', 'column05': 'DOUBLE', 'column06': 'DOUBLE', 'column07': 'DOUBLE', 'column08': 'VARCHAR', 'column09': 'VARCHAR', 'column10': 'DATE', 'column11': 'DATE', 'column12': 'DATE', 'column13': 'VARCHAR', 'column14': 'VARCHAR', 'column15': 'VARCHAR'}, dateformat='%Y-%m-%d', quote='"');
+|	"	\0	
	\0	0	0	[{'name': column00, 'type': BIGINT}, {'name': column01, 'type': BIGINT}, {'name': column02, 'type': BIGINT}, {'name': column03, 'type': BIGINT}, {'name': column04, 'type': BIGINT}, {'name': column05, 'type': DOUBLE}, {'name': column06, 'type': DOUBLE}, {'name': column07, 'type': DOUBLE}, {'name': column08, 'type': VARCHAR}, {'name': column09, 'type': VARCHAR}, {'name': column10, 'type': DATE}, {'name': column11, 'type': DATE}, {'name': column12, 'type': DATE}, {'name': column13, 'type': VARCHAR}, {'name': column14, 'type': VARCHAR}, {'name': column15, 'type': VARCHAR}]	%Y-%m-%d	NULL	quote='"'	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', escape='', new_line='
', skip=0, comment='', header=false, columns={'column00': 'BIGINT', 'column01': 'BIGINT', 'column02': 'BIGINT', 'column03': 'BIGINT', 'column04': 'BIGINT', 'column05': 'DOUBLE', 'column06': 'DOUBLE', 'column07': 'DOUBLE', 'column08': 'VARCHAR', 'column09': 'VARCHAR', 'column10': 'DATE', 'column11': 'DATE', 'column12': 'DATE', 'column13': 'VARCHAR', 'column14': 'VARCHAR', 'column15': 'VARCHAR'}, dateformat='%Y-%m-%d', quote='"');
 
 # escape
 query IIIIIIIIIIII
@@ -44,7 +44,7 @@ FROM sniff_csv('data/csv/real/lineitem_sample.csv', escape='"');
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/real/lineitem_sample.csv', names=['c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9', 'c10', 'c11', 'c12', 'c13', 'c14', 'c15', 'c16']);
 ----
-|	"	"	
	\0	0	0	[{'name': c1, 'type': BIGINT}, {'name': c2, 'type': BIGINT}, {'name': c3, 'type': BIGINT}, {'name': c4, 'type': BIGINT}, {'name': c5, 'type': BIGINT}, {'name': c6, 'type': DOUBLE}, {'name': c7, 'type': DOUBLE}, {'name': c8, 'type': DOUBLE}, {'name': c9, 'type': VARCHAR}, {'name': c10, 'type': VARCHAR}, {'name': c11, 'type': DATE}, {'name': c12, 'type': DATE}, {'name': c13, 'type': DATE}, {'name': c14, 'type': VARCHAR}, {'name': c15, 'type': VARCHAR}, {'name': c16, 'type': VARCHAR}]	%Y-%m-%d	NULL	NULL	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='"', escape='"', new_line='
', skip=0, comment='', header=false, columns={'c1': 'BIGINT', 'c2': 'BIGINT', 'c3': 'BIGINT', 'c4': 'BIGINT', 'c5': 'BIGINT', 'c6': 'DOUBLE', 'c7': 'DOUBLE', 'c8': 'DOUBLE', 'c9': 'VARCHAR', 'c10': 'VARCHAR', 'c11': 'DATE', 'c12': 'DATE', 'c13': 'DATE', 'c14': 'VARCHAR', 'c15': 'VARCHAR', 'c16': 'VARCHAR'}, dateformat='%Y-%m-%d');
+|	\0	\0	
	\0	0	0	[{'name': c1, 'type': BIGINT}, {'name': c2, 'type': BIGINT}, {'name': c3, 'type': BIGINT}, {'name': c4, 'type': BIGINT}, {'name': c5, 'type': BIGINT}, {'name': c6, 'type': DOUBLE}, {'name': c7, 'type': DOUBLE}, {'name': c8, 'type': DOUBLE}, {'name': c9, 'type': VARCHAR}, {'name': c10, 'type': VARCHAR}, {'name': c11, 'type': DATE}, {'name': c12, 'type': DATE}, {'name': c13, 'type': DATE}, {'name': c14, 'type': VARCHAR}, {'name': c15, 'type': VARCHAR}, {'name': c16, 'type': VARCHAR}]	%Y-%m-%d	NULL	NULL	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='', escape='', new_line='
', skip=0, comment='', header=false, columns={'c1': 'BIGINT', 'c2': 'BIGINT', 'c3': 'BIGINT', 'c4': 'BIGINT', 'c5': 'BIGINT', 'c6': 'DOUBLE', 'c7': 'DOUBLE', 'c8': 'DOUBLE', 'c9': 'VARCHAR', 'c10': 'VARCHAR', 'c11': 'DATE', 'c12': 'DATE', 'c13': 'DATE', 'c14': 'VARCHAR', 'c15': 'VARCHAR', 'c16': 'VARCHAR'}, dateformat='%Y-%m-%d');
 
 
 query IIIIIIIIIIIIIIII
@@ -55,37 +55,37 @@ FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|',
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/real/lineitem_sample.csv', columns={'c1': 'BIGINT', 'c2': 'BIGINT', 'c3': 'BIGINT', 'c4': 'BIGINT', 'c5': 'BIGINT', 'c6': 'DOUBLE', 'c7': 'DOUBLE', 'c8': 'DOUBLE', 'c9': 'VARCHAR', 'c10': 'VARCHAR', 'c11': 'DATE', 'c12': 'DATE', 'c13': 'DATE', 'c14': 'VARCHAR', 'c15': 'VARCHAR', 'c16': 'VARCHAR'});
 ----
-|	"	"	
	\0	0	0	[{'name': c1, 'type': BIGINT}, {'name': c2, 'type': BIGINT}, {'name': c3, 'type': BIGINT}, {'name': c4, 'type': BIGINT}, {'name': c5, 'type': BIGINT}, {'name': c6, 'type': DOUBLE}, {'name': c7, 'type': DOUBLE}, {'name': c8, 'type': DOUBLE}, {'name': c9, 'type': VARCHAR}, {'name': c10, 'type': VARCHAR}, {'name': c11, 'type': DATE}, {'name': c12, 'type': DATE}, {'name': c13, 'type': DATE}, {'name': c14, 'type': VARCHAR}, {'name': c15, 'type': VARCHAR}, {'name': c16, 'type': VARCHAR}]	%Y-%m-%d	NULL	NULL	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='"', escape='"', new_line='
', skip=0, comment='', header=false, columns={'c1': 'BIGINT', 'c2': 'BIGINT', 'c3': 'BIGINT', 'c4': 'BIGINT', 'c5': 'BIGINT', 'c6': 'DOUBLE', 'c7': 'DOUBLE', 'c8': 'DOUBLE', 'c9': 'VARCHAR', 'c10': 'VARCHAR', 'c11': 'DATE', 'c12': 'DATE', 'c13': 'DATE', 'c14': 'VARCHAR', 'c15': 'VARCHAR', 'c16': 'VARCHAR'}, dateformat='%Y-%m-%d');
+|	\0	\0	
	\0	0	0	[{'name': c1, 'type': BIGINT}, {'name': c2, 'type': BIGINT}, {'name': c3, 'type': BIGINT}, {'name': c4, 'type': BIGINT}, {'name': c5, 'type': BIGINT}, {'name': c6, 'type': DOUBLE}, {'name': c7, 'type': DOUBLE}, {'name': c8, 'type': DOUBLE}, {'name': c9, 'type': VARCHAR}, {'name': c10, 'type': VARCHAR}, {'name': c11, 'type': DATE}, {'name': c12, 'type': DATE}, {'name': c13, 'type': DATE}, {'name': c14, 'type': VARCHAR}, {'name': c15, 'type': VARCHAR}, {'name': c16, 'type': VARCHAR}]	%Y-%m-%d	NULL	NULL	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='', escape='', new_line='
', skip=0, comment='', header=false, columns={'c1': 'BIGINT', 'c2': 'BIGINT', 'c3': 'BIGINT', 'c4': 'BIGINT', 'c5': 'BIGINT', 'c6': 'DOUBLE', 'c7': 'DOUBLE', 'c8': 'DOUBLE', 'c9': 'VARCHAR', 'c10': 'VARCHAR', 'c11': 'DATE', 'c12': 'DATE', 'c13': 'DATE', 'c14': 'VARCHAR', 'c15': 'VARCHAR', 'c16': 'VARCHAR'}, dateformat='%Y-%m-%d');
 
 # skip rows
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/real/lineitem_sample.csv', skip=1);
 ----
-|	"	"	
	\0	1	0	[{'name': column00, 'type': BIGINT}, {'name': column01, 'type': BIGINT}, {'name': column02, 'type': BIGINT}, {'name': column03, 'type': BIGINT}, {'name': column04, 'type': BIGINT}, {'name': column05, 'type': DOUBLE}, {'name': column06, 'type': DOUBLE}, {'name': column07, 'type': DOUBLE}, {'name': column08, 'type': VARCHAR}, {'name': column09, 'type': VARCHAR}, {'name': column10, 'type': DATE}, {'name': column11, 'type': DATE}, {'name': column12, 'type': DATE}, {'name': column13, 'type': VARCHAR}, {'name': column14, 'type': VARCHAR}, {'name': column15, 'type': VARCHAR}]	%Y-%m-%d	NULL	skip=1	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='"', escape='"', new_line='
', comment='', header=false, columns={'column00': 'BIGINT', 'column01': 'BIGINT', 'column02': 'BIGINT', 'column03': 'BIGINT', 'column04': 'BIGINT', 'column05': 'DOUBLE', 'column06': 'DOUBLE', 'column07': 'DOUBLE', 'column08': 'VARCHAR', 'column09': 'VARCHAR', 'column10': 'DATE', 'column11': 'DATE', 'column12': 'DATE', 'column13': 'VARCHAR', 'column14': 'VARCHAR', 'column15': 'VARCHAR'}, dateformat='%Y-%m-%d', skip=1);
+|	\0	\0	
	\0	1	0	[{'name': column00, 'type': BIGINT}, {'name': column01, 'type': BIGINT}, {'name': column02, 'type': BIGINT}, {'name': column03, 'type': BIGINT}, {'name': column04, 'type': BIGINT}, {'name': column05, 'type': DOUBLE}, {'name': column06, 'type': DOUBLE}, {'name': column07, 'type': DOUBLE}, {'name': column08, 'type': VARCHAR}, {'name': column09, 'type': VARCHAR}, {'name': column10, 'type': DATE}, {'name': column11, 'type': DATE}, {'name': column12, 'type': DATE}, {'name': column13, 'type': VARCHAR}, {'name': column14, 'type': VARCHAR}, {'name': column15, 'type': VARCHAR}]	%Y-%m-%d	NULL	skip=1	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='', escape='', new_line='
', comment='', header=false, columns={'column00': 'BIGINT', 'column01': 'BIGINT', 'column02': 'BIGINT', 'column03': 'BIGINT', 'column04': 'BIGINT', 'column05': 'DOUBLE', 'column06': 'DOUBLE', 'column07': 'DOUBLE', 'column08': 'VARCHAR', 'column09': 'VARCHAR', 'column10': 'DATE', 'column11': 'DATE', 'column12': 'DATE', 'column13': 'VARCHAR', 'column14': 'VARCHAR', 'column15': 'VARCHAR'}, dateformat='%Y-%m-%d', skip=1);
 
 # header exists
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/real/lineitem_sample.csv', header=true);
 ----
-|	"	"	
	\0	0	1	[{'name': 1, 'type': BIGINT}, {'name': 15519, 'type': BIGINT}, {'name': 785, 'type': BIGINT}, {'name': 1_1, 'type': BIGINT}, {'name': 17, 'type': BIGINT}, {'name': 24386.670000, 'type': DOUBLE}, {'name': 0.040000, 'type': DOUBLE}, {'name': 0.020000, 'type': DOUBLE}, {'name': N, 'type': VARCHAR}, {'name': O, 'type': VARCHAR}, {'name': 1996-03-13, 'type': DATE}, {'name': 1996-02-12, 'type': DATE}, {'name': 1996-03-22, 'type': DATE}, {'name': DELIVER IN PERSON, 'type': VARCHAR}, {'name': TRUCK, 'type': VARCHAR}, {'name': egular courts above the, 'type': VARCHAR}]	%Y-%m-%d	NULL	header=true	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='"', escape='"', new_line='
', skip=0, comment='', columns={'1': 'BIGINT', '15519': 'BIGINT', '785': 'BIGINT', '1_1': 'BIGINT', '17': 'BIGINT', '24386.670000': 'DOUBLE', '0.040000': 'DOUBLE', '0.020000': 'DOUBLE', 'N': 'VARCHAR', 'O': 'VARCHAR', '1996-03-13': 'DATE', '1996-02-12': 'DATE', '1996-03-22': 'DATE', 'DELIVER IN PERSON': 'VARCHAR', 'TRUCK': 'VARCHAR', 'egular courts above the': 'VARCHAR'}, dateformat='%Y-%m-%d', header=true);
+|	\0	\0	
	\0	0	1	[{'name': 1, 'type': BIGINT}, {'name': 15519, 'type': BIGINT}, {'name': 785, 'type': BIGINT}, {'name': 1_1, 'type': BIGINT}, {'name': 17, 'type': BIGINT}, {'name': 24386.670000, 'type': DOUBLE}, {'name': 0.040000, 'type': DOUBLE}, {'name': 0.020000, 'type': DOUBLE}, {'name': N, 'type': VARCHAR}, {'name': O, 'type': VARCHAR}, {'name': 1996-03-13, 'type': DATE}, {'name': 1996-02-12, 'type': DATE}, {'name': 1996-03-22, 'type': DATE}, {'name': DELIVER IN PERSON, 'type': VARCHAR}, {'name': TRUCK, 'type': VARCHAR}, {'name': egular courts above the, 'type': VARCHAR}]	%Y-%m-%d	NULL	header=true	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='', escape='', new_line='
', skip=0, comment='', columns={'1': 'BIGINT', '15519': 'BIGINT', '785': 'BIGINT', '1_1': 'BIGINT', '17': 'BIGINT', '24386.670000': 'DOUBLE', '0.040000': 'DOUBLE', '0.020000': 'DOUBLE', 'N': 'VARCHAR', 'O': 'VARCHAR', '1996-03-13': 'DATE', '1996-02-12': 'DATE', '1996-03-22': 'DATE', 'DELIVER IN PERSON': 'VARCHAR', 'TRUCK': 'VARCHAR', 'egular courts above the': 'VARCHAR'}, dateformat='%Y-%m-%d', header=true);
 
 # timestampformat
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/auto/time_date_timestamp_yyyy.mm.dd.csv', dateformat='%Y.%m.%d')
 ----
-,	"	"	
	\0	0	1	[{'name': a, 'type': BIGINT}, {'name': b, 'type': VARCHAR}, {'name': t, 'type': TIME}, {'name': d, 'type': DATE}, {'name': ts, 'type': TIMESTAMP}]	%Y.%m.%d	%Y.%m.%d %H:%M:%S	dateformat='%Y.%m.%d'	FROM read_csv('data/csv/auto/time_date_timestamp_yyyy.mm.dd.csv', auto_detect=false, delim=',', quote='"', escape='"', new_line='
', skip=0, comment='', header=true, columns={'a': 'BIGINT', 'b': 'VARCHAR', 't': 'TIME', 'd': 'DATE', 'ts': 'TIMESTAMP'}, timestampformat='%Y.%m.%d %H:%M:%S', dateformat='%Y.%m.%d');
+,	\0	\0	
	\0	0	1	[{'name': a, 'type': BIGINT}, {'name': b, 'type': VARCHAR}, {'name': t, 'type': TIME}, {'name': d, 'type': DATE}, {'name': ts, 'type': TIMESTAMP}]	%Y.%m.%d	%Y.%m.%d %H:%M:%S	dateformat='%Y.%m.%d'	FROM read_csv('data/csv/auto/time_date_timestamp_yyyy.mm.dd.csv', auto_detect=false, delim=',', quote='', escape='', new_line='
', skip=0, comment='', header=true, columns={'a': 'BIGINT', 'b': 'VARCHAR', 't': 'TIME', 'd': 'DATE', 'ts': 'TIMESTAMP'}, timestampformat='%Y.%m.%d %H:%M:%S', dateformat='%Y.%m.%d');
 
 # dateformat
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/auto/time_date_timestamp_yyyy.mm.dd.csv', timestampformat='%Y.%m.%d %H:%M:%S')
 ----
-,	"	"	
	\0	0	1	[{'name': a, 'type': BIGINT}, {'name': b, 'type': VARCHAR}, {'name': t, 'type': TIME}, {'name': d, 'type': DATE}, {'name': ts, 'type': TIMESTAMP}]	%Y.%m.%d	%Y.%m.%d %H:%M:%S	timestampformat='%Y.%m.%d %H:%M:%S'	FROM read_csv('data/csv/auto/time_date_timestamp_yyyy.mm.dd.csv', auto_detect=false, delim=',', quote='"', escape='"', new_line='
', skip=0, comment='', header=true, columns={'a': 'BIGINT', 'b': 'VARCHAR', 't': 'TIME', 'd': 'DATE', 'ts': 'TIMESTAMP'}, dateformat='%Y.%m.%d', timestampformat='%Y.%m.%d %H:%M:%S');
+,	\0	\0	
	\0	0	1	[{'name': a, 'type': BIGINT}, {'name': b, 'type': VARCHAR}, {'name': t, 'type': TIME}, {'name': d, 'type': DATE}, {'name': ts, 'type': TIMESTAMP}]	%Y.%m.%d	%Y.%m.%d %H:%M:%S	timestampformat='%Y.%m.%d %H:%M:%S'	FROM read_csv('data/csv/auto/time_date_timestamp_yyyy.mm.dd.csv', auto_detect=false, delim=',', quote='', escape='', new_line='
', skip=0, comment='', header=true, columns={'a': 'BIGINT', 'b': 'VARCHAR', 't': 'TIME', 'd': 'DATE', 'ts': 'TIMESTAMP'}, dateformat='%Y.%m.%d', timestampformat='%Y.%m.%d %H:%M:%S');
 
 # Test a combination
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/auto/time_date_timestamp_yyyy.mm.dd.csv', dateformat='%Y.%m.%d', timestampformat='%Y.%m.%d %H:%M:%S')
 ----
-,	"	"	
	\0	0	1	[{'name': a, 'type': BIGINT}, {'name': b, 'type': VARCHAR}, {'name': t, 'type': TIME}, {'name': d, 'type': DATE}, {'name': ts, 'type': TIMESTAMP}]	%Y.%m.%d	%Y.%m.%d %H:%M:%S	dateformat='%Y.%m.%d', timestampformat='%Y.%m.%d %H:%M:%S'	FROM read_csv('data/csv/auto/time_date_timestamp_yyyy.mm.dd.csv', auto_detect=false, delim=',', quote='"', escape='"', new_line='
', skip=0, comment='', header=true, columns={'a': 'BIGINT', 'b': 'VARCHAR', 't': 'TIME', 'd': 'DATE', 'ts': 'TIMESTAMP'}, dateformat='%Y.%m.%d', timestampformat='%Y.%m.%d %H:%M:%S');
+,	\0	\0	
	\0	0	1	[{'name': a, 'type': BIGINT}, {'name': b, 'type': VARCHAR}, {'name': t, 'type': TIME}, {'name': d, 'type': DATE}, {'name': ts, 'type': TIMESTAMP}]	%Y.%m.%d	%Y.%m.%d %H:%M:%S	dateformat='%Y.%m.%d', timestampformat='%Y.%m.%d %H:%M:%S'	FROM read_csv('data/csv/auto/time_date_timestamp_yyyy.mm.dd.csv', auto_detect=false, delim=',', quote='', escape='', new_line='
', skip=0, comment='', header=true, columns={'a': 'BIGINT', 'b': 'VARCHAR', 't': 'TIME', 'd': 'DATE', 'ts': 'TIMESTAMP'}, dateformat='%Y.%m.%d', timestampformat='%Y.%m.%d %H:%M:%S');
 
 query IIIII
 FROM read_csv('data/csv/auto/time_date_timestamp_yyyy.mm.dd.csv', auto_detect=false, delim=',', quote='"', escape='"', new_line='
', skip=0, header=true, columns={'a': 'BIGINT', 'b': 'VARCHAR', 't': 'TIME', 'd': 'DATE', 'ts': 'TIMESTAMP'}, timestampformat='%Y.%m.%d %H:%M:%S', dateformat='%Y.%m.%d') order by all limit 1;
@@ -101,13 +101,13 @@ sniff_csv function does not accept auto_detect variable set to false
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/real/lineitem_sample.csv', columns={'c1': 'BIGINT', 'c2': 'BIGINT', 'c3': 'BIGINT', 'c4': 'BIGINT', 'c5': 'BIGINT', 'c6': 'DOUBLE', 'c7': 'DOUBLE', 'c8': 'DOUBLE', 'c9': 'VARCHAR', 'c10': 'VARCHAR', 'c11': 'DATE', 'c12': 'DATE', 'c13': 'DATE', 'c14': 'VARCHAR', 'c15': 'VARCHAR', 'c16': 'VARCHAR'}, auto_detect = true);
 ----
-|	"	"	
	\0	0	0	[{'name': c1, 'type': BIGINT}, {'name': c2, 'type': BIGINT}, {'name': c3, 'type': BIGINT}, {'name': c4, 'type': BIGINT}, {'name': c5, 'type': BIGINT}, {'name': c6, 'type': DOUBLE}, {'name': c7, 'type': DOUBLE}, {'name': c8, 'type': DOUBLE}, {'name': c9, 'type': VARCHAR}, {'name': c10, 'type': VARCHAR}, {'name': c11, 'type': DATE}, {'name': c12, 'type': DATE}, {'name': c13, 'type': DATE}, {'name': c14, 'type': VARCHAR}, {'name': c15, 'type': VARCHAR}, {'name': c16, 'type': VARCHAR}]	%Y-%m-%d	NULL	NULL	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='"', escape='"', new_line='
', skip=0, comment='', header=false, columns={'c1': 'BIGINT', 'c2': 'BIGINT', 'c3': 'BIGINT', 'c4': 'BIGINT', 'c5': 'BIGINT', 'c6': 'DOUBLE', 'c7': 'DOUBLE', 'c8': 'DOUBLE', 'c9': 'VARCHAR', 'c10': 'VARCHAR', 'c11': 'DATE', 'c12': 'DATE', 'c13': 'DATE', 'c14': 'VARCHAR', 'c15': 'VARCHAR', 'c16': 'VARCHAR'}, dateformat='%Y-%m-%d');
+|	\0	\0	
	\0	0	0	[{'name': c1, 'type': BIGINT}, {'name': c2, 'type': BIGINT}, {'name': c3, 'type': BIGINT}, {'name': c4, 'type': BIGINT}, {'name': c5, 'type': BIGINT}, {'name': c6, 'type': DOUBLE}, {'name': c7, 'type': DOUBLE}, {'name': c8, 'type': DOUBLE}, {'name': c9, 'type': VARCHAR}, {'name': c10, 'type': VARCHAR}, {'name': c11, 'type': DATE}, {'name': c12, 'type': DATE}, {'name': c13, 'type': DATE}, {'name': c14, 'type': VARCHAR}, {'name': c15, 'type': VARCHAR}, {'name': c16, 'type': VARCHAR}]	%Y-%m-%d	NULL	NULL	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='', escape='', new_line='
', skip=0, comment='', header=false, columns={'c1': 'BIGINT', 'c2': 'BIGINT', 'c3': 'BIGINT', 'c4': 'BIGINT', 'c5': 'BIGINT', 'c6': 'DOUBLE', 'c7': 'DOUBLE', 'c8': 'DOUBLE', 'c9': 'VARCHAR', 'c10': 'VARCHAR', 'c11': 'DATE', 'c12': 'DATE', 'c13': 'DATE', 'c14': 'VARCHAR', 'c15': 'VARCHAR', 'c16': 'VARCHAR'}, dateformat='%Y-%m-%d');
 
 # auto_type_candidates
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/autotypecandidates.csv', auto_type_candidates=['SMALLINT','BIGINT', 'DOUBLE', 'FLOAT','VARCHAR']);
 ----
-|	"	"	
	\0	0	0	[{'name': column0, 'type': SMALLINT}, {'name': column1, 'type': FLOAT}, {'name': column2, 'type': VARCHAR}]	NULL	NULL	NULL	FROM read_csv('data/csv/autotypecandidates.csv', auto_detect=false, delim='|', quote='"', escape='"', new_line='
', skip=0, comment='', header=false, columns={'column0': 'SMALLINT', 'column1': 'FLOAT', 'column2': 'VARCHAR'});
+|	\0	\0	
	\0	0	0	[{'name': column0, 'type': SMALLINT}, {'name': column1, 'type': FLOAT}, {'name': column2, 'type': VARCHAR}]	NULL	NULL	NULL	FROM read_csv('data/csv/autotypecandidates.csv', auto_detect=false, delim='|', quote='', escape='', new_line='
', skip=0, comment='', header=false, columns={'column0': 'SMALLINT', 'column1': 'FLOAT', 'column2': 'VARCHAR'});
 
 query III
 FROM read_csv('data/csv/autotypecandidates.csv', auto_detect=false, delim='|', quote='"', escape='"', new_line='
', skip=0, header=false, columns={'column0': 'SMALLINT', 'column1': 'FLOAT', 'column2': 'VARCHAR'});
@@ -130,27 +130,27 @@ Invalid named parameter "oop" for function sniff_csv
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/autotypecandidates.csv', HIVE_PARTITIONING=1);
 ----
-|	"	"	
	\0	0	0	[{'name': column0, 'type': BIGINT}, {'name': column1, 'type': DOUBLE}, {'name': column2, 'type': VARCHAR}]	NULL	NULL	NULL	FROM read_csv('data/csv/autotypecandidates.csv', auto_detect=false, delim='|', quote='"', escape='"', new_line='
', skip=0, comment='', header=false, columns={'column0': 'BIGINT', 'column1': 'DOUBLE', 'column2': 'VARCHAR'});
+|	\0	\0	
	\0	0	0	[{'name': column0, 'type': BIGINT}, {'name': column1, 'type': DOUBLE}, {'name': column2, 'type': VARCHAR}]	NULL	NULL	NULL	FROM read_csv('data/csv/autotypecandidates.csv', auto_detect=false, delim='|', quote='', escape='', new_line='
', skip=0, comment='', header=false, columns={'column0': 'BIGINT', 'column1': 'DOUBLE', 'column2': 'VARCHAR'});
 
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/real/lineitem_sample.csv', types=['INTEGER','BIGINT','BIGINT','BIGINT','BIGINT', 'DOUBLE','DOUBLE','DOUBLE','VARCHAR', 'VARCHAR','DATE', 'DATE',  'DATE', 'VARCHAR',  'VARCHAR', 'VARCHAR']);
 ----
-|	"	"	
	\0	0	0	[{'name': column00, 'type': INTEGER}, {'name': column01, 'type': BIGINT}, {'name': column02, 'type': BIGINT}, {'name': column03, 'type': BIGINT}, {'name': column04, 'type': BIGINT}, {'name': column05, 'type': DOUBLE}, {'name': column06, 'type': DOUBLE}, {'name': column07, 'type': DOUBLE}, {'name': column08, 'type': VARCHAR}, {'name': column09, 'type': VARCHAR}, {'name': column10, 'type': DATE}, {'name': column11, 'type': DATE}, {'name': column12, 'type': DATE}, {'name': column13, 'type': VARCHAR}, {'name': column14, 'type': VARCHAR}, {'name': column15, 'type': VARCHAR}]	%Y-%m-%d	NULL	NULL	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='"', escape='"', new_line='
', skip=0, comment='', header=false, columns={'column00': 'INTEGER', 'column01': 'BIGINT', 'column02': 'BIGINT', 'column03': 'BIGINT', 'column04': 'BIGINT', 'column05': 'DOUBLE', 'column06': 'DOUBLE', 'column07': 'DOUBLE', 'column08': 'VARCHAR', 'column09': 'VARCHAR', 'column10': 'DATE', 'column11': 'DATE', 'column12': 'DATE', 'column13': 'VARCHAR', 'column14': 'VARCHAR', 'column15': 'VARCHAR'}, dateformat='%Y-%m-%d');
+|	\0	\0	
	\0	0	0	[{'name': column00, 'type': INTEGER}, {'name': column01, 'type': BIGINT}, {'name': column02, 'type': BIGINT}, {'name': column03, 'type': BIGINT}, {'name': column04, 'type': BIGINT}, {'name': column05, 'type': DOUBLE}, {'name': column06, 'type': DOUBLE}, {'name': column07, 'type': DOUBLE}, {'name': column08, 'type': VARCHAR}, {'name': column09, 'type': VARCHAR}, {'name': column10, 'type': DATE}, {'name': column11, 'type': DATE}, {'name': column12, 'type': DATE}, {'name': column13, 'type': VARCHAR}, {'name': column14, 'type': VARCHAR}, {'name': column15, 'type': VARCHAR}]	%Y-%m-%d	NULL	NULL	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='', escape='', new_line='
', skip=0, comment='', header=false, columns={'column00': 'INTEGER', 'column01': 'BIGINT', 'column02': 'BIGINT', 'column03': 'BIGINT', 'column04': 'BIGINT', 'column05': 'DOUBLE', 'column06': 'DOUBLE', 'column07': 'DOUBLE', 'column08': 'VARCHAR', 'column09': 'VARCHAR', 'column10': 'DATE', 'column11': 'DATE', 'column12': 'DATE', 'column13': 'VARCHAR', 'column14': 'VARCHAR', 'column15': 'VARCHAR'}, dateformat='%Y-%m-%d');
 
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/real/lineitem_sample.csv', dtypes=['INTEGER','BIGINT','BIGINT','BIGINT','BIGINT', 'DOUBLE','DOUBLE','DOUBLE','VARCHAR', 'VARCHAR','DATE', 'DATE',  'DATE', 'VARCHAR',  'VARCHAR', 'VARCHAR']);
 ----
-|	"	"	
	\0	0	0	[{'name': column00, 'type': INTEGER}, {'name': column01, 'type': BIGINT}, {'name': column02, 'type': BIGINT}, {'name': column03, 'type': BIGINT}, {'name': column04, 'type': BIGINT}, {'name': column05, 'type': DOUBLE}, {'name': column06, 'type': DOUBLE}, {'name': column07, 'type': DOUBLE}, {'name': column08, 'type': VARCHAR}, {'name': column09, 'type': VARCHAR}, {'name': column10, 'type': DATE}, {'name': column11, 'type': DATE}, {'name': column12, 'type': DATE}, {'name': column13, 'type': VARCHAR}, {'name': column14, 'type': VARCHAR}, {'name': column15, 'type': VARCHAR}]	%Y-%m-%d	NULL	NULL	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='"', escape='"', new_line='
', skip=0, comment='', header=false, columns={'column00': 'INTEGER', 'column01': 'BIGINT', 'column02': 'BIGINT', 'column03': 'BIGINT', 'column04': 'BIGINT', 'column05': 'DOUBLE', 'column06': 'DOUBLE', 'column07': 'DOUBLE', 'column08': 'VARCHAR', 'column09': 'VARCHAR', 'column10': 'DATE', 'column11': 'DATE', 'column12': 'DATE', 'column13': 'VARCHAR', 'column14': 'VARCHAR', 'column15': 'VARCHAR'}, dateformat='%Y-%m-%d');
+|	\0	\0	
	\0	0	0	[{'name': column00, 'type': INTEGER}, {'name': column01, 'type': BIGINT}, {'name': column02, 'type': BIGINT}, {'name': column03, 'type': BIGINT}, {'name': column04, 'type': BIGINT}, {'name': column05, 'type': DOUBLE}, {'name': column06, 'type': DOUBLE}, {'name': column07, 'type': DOUBLE}, {'name': column08, 'type': VARCHAR}, {'name': column09, 'type': VARCHAR}, {'name': column10, 'type': DATE}, {'name': column11, 'type': DATE}, {'name': column12, 'type': DATE}, {'name': column13, 'type': VARCHAR}, {'name': column14, 'type': VARCHAR}, {'name': column15, 'type': VARCHAR}]	%Y-%m-%d	NULL	NULL	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='', escape='', new_line='
', skip=0, comment='', header=false, columns={'column00': 'INTEGER', 'column01': 'BIGINT', 'column02': 'BIGINT', 'column03': 'BIGINT', 'column04': 'BIGINT', 'column05': 'DOUBLE', 'column06': 'DOUBLE', 'column07': 'DOUBLE', 'column08': 'VARCHAR', 'column09': 'VARCHAR', 'column10': 'DATE', 'column11': 'DATE', 'column12': 'DATE', 'column13': 'VARCHAR', 'column14': 'VARCHAR', 'column15': 'VARCHAR'}, dateformat='%Y-%m-%d');
 
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/real/lineitem_sample.csv', column_types=['INTEGER','BIGINT','BIGINT','BIGINT','BIGINT', 'DOUBLE','DOUBLE','DOUBLE','VARCHAR', 'VARCHAR','DATE', 'DATE',  'DATE', 'VARCHAR',  'VARCHAR', 'VARCHAR']);
 ----
-|	"	"	
	\0	0	0	[{'name': column00, 'type': INTEGER}, {'name': column01, 'type': BIGINT}, {'name': column02, 'type': BIGINT}, {'name': column03, 'type': BIGINT}, {'name': column04, 'type': BIGINT}, {'name': column05, 'type': DOUBLE}, {'name': column06, 'type': DOUBLE}, {'name': column07, 'type': DOUBLE}, {'name': column08, 'type': VARCHAR}, {'name': column09, 'type': VARCHAR}, {'name': column10, 'type': DATE}, {'name': column11, 'type': DATE}, {'name': column12, 'type': DATE}, {'name': column13, 'type': VARCHAR}, {'name': column14, 'type': VARCHAR}, {'name': column15, 'type': VARCHAR}]	%Y-%m-%d	NULL	NULL	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='"', escape='"', new_line='
', skip=0, comment='', header=false, columns={'column00': 'INTEGER', 'column01': 'BIGINT', 'column02': 'BIGINT', 'column03': 'BIGINT', 'column04': 'BIGINT', 'column05': 'DOUBLE', 'column06': 'DOUBLE', 'column07': 'DOUBLE', 'column08': 'VARCHAR', 'column09': 'VARCHAR', 'column10': 'DATE', 'column11': 'DATE', 'column12': 'DATE', 'column13': 'VARCHAR', 'column14': 'VARCHAR', 'column15': 'VARCHAR'}, dateformat='%Y-%m-%d');
+|	\0	\0	
	\0	0	0	[{'name': column00, 'type': INTEGER}, {'name': column01, 'type': BIGINT}, {'name': column02, 'type': BIGINT}, {'name': column03, 'type': BIGINT}, {'name': column04, 'type': BIGINT}, {'name': column05, 'type': DOUBLE}, {'name': column06, 'type': DOUBLE}, {'name': column07, 'type': DOUBLE}, {'name': column08, 'type': VARCHAR}, {'name': column09, 'type': VARCHAR}, {'name': column10, 'type': DATE}, {'name': column11, 'type': DATE}, {'name': column12, 'type': DATE}, {'name': column13, 'type': VARCHAR}, {'name': column14, 'type': VARCHAR}, {'name': column15, 'type': VARCHAR}]	%Y-%m-%d	NULL	NULL	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='', escape='', new_line='
', skip=0, comment='', header=false, columns={'column00': 'INTEGER', 'column01': 'BIGINT', 'column02': 'BIGINT', 'column03': 'BIGINT', 'column04': 'BIGINT', 'column05': 'DOUBLE', 'column06': 'DOUBLE', 'column07': 'DOUBLE', 'column08': 'VARCHAR', 'column09': 'VARCHAR', 'column10': 'DATE', 'column11': 'DATE', 'column12': 'DATE', 'column13': 'VARCHAR', 'column14': 'VARCHAR', 'column15': 'VARCHAR'}, dateformat='%Y-%m-%d');
 
 query IIIIIIIIIIII
 FROM sniff_csv('data/csv/real/lineitem_sample.csv', names=['c01','c02','c03','c04','c5', 'c06','c07','c08','c09', 'c10','c11', 'c12',  'c13', 'c14',  'c15', 'c16']);
 ----
-|	"	"	
	\0	0	0	[{'name': c01, 'type': BIGINT}, {'name': c02, 'type': BIGINT}, {'name': c03, 'type': BIGINT}, {'name': c04, 'type': BIGINT}, {'name': c5, 'type': BIGINT}, {'name': c06, 'type': DOUBLE}, {'name': c07, 'type': DOUBLE}, {'name': c08, 'type': DOUBLE}, {'name': c09, 'type': VARCHAR}, {'name': c10, 'type': VARCHAR}, {'name': c11, 'type': DATE}, {'name': c12, 'type': DATE}, {'name': c13, 'type': DATE}, {'name': c14, 'type': VARCHAR}, {'name': c15, 'type': VARCHAR}, {'name': c16, 'type': VARCHAR}]	%Y-%m-%d	NULL	NULL	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='"', escape='"', new_line='
', skip=0, comment='', header=false, columns={'c01': 'BIGINT', 'c02': 'BIGINT', 'c03': 'BIGINT', 'c04': 'BIGINT', 'c5': 'BIGINT', 'c06': 'DOUBLE', 'c07': 'DOUBLE', 'c08': 'DOUBLE', 'c09': 'VARCHAR', 'c10': 'VARCHAR', 'c11': 'DATE', 'c12': 'DATE', 'c13': 'DATE', 'c14': 'VARCHAR', 'c15': 'VARCHAR', 'c16': 'VARCHAR'}, dateformat='%Y-%m-%d');
+|	\0	\0	
	\0	0	0	[{'name': c01, 'type': BIGINT}, {'name': c02, 'type': BIGINT}, {'name': c03, 'type': BIGINT}, {'name': c04, 'type': BIGINT}, {'name': c5, 'type': BIGINT}, {'name': c06, 'type': DOUBLE}, {'name': c07, 'type': DOUBLE}, {'name': c08, 'type': DOUBLE}, {'name': c09, 'type': VARCHAR}, {'name': c10, 'type': VARCHAR}, {'name': c11, 'type': DATE}, {'name': c12, 'type': DATE}, {'name': c13, 'type': DATE}, {'name': c14, 'type': VARCHAR}, {'name': c15, 'type': VARCHAR}, {'name': c16, 'type': VARCHAR}]	%Y-%m-%d	NULL	NULL	FROM read_csv('data/csv/real/lineitem_sample.csv', auto_detect=false, delim='|', quote='', escape='', new_line='
', skip=0, comment='', header=false, columns={'c01': 'BIGINT', 'c02': 'BIGINT', 'c03': 'BIGINT', 'c04': 'BIGINT', 'c5': 'BIGINT', 'c06': 'DOUBLE', 'c07': 'DOUBLE', 'c08': 'DOUBLE', 'c09': 'VARCHAR', 'c10': 'VARCHAR', 'c11': 'DATE', 'c12': 'DATE', 'c13': 'DATE', 'c14': 'VARCHAR', 'c15': 'VARCHAR', 'c16': 'VARCHAR'}, dateformat='%Y-%m-%d');
 
 # Test that rejects returs correct values
 
diff --git a/test/sql/copy/csv/test_validator.test b/test/sql/copy/csv/test_validator.test
new file mode 100644
index 000000000000..66a444809520
--- /dev/null
+++ b/test/sql/copy/csv/test_validator.test
@@ -0,0 +1,61 @@
+# name: test/sql/copy/csv/test_validator.test
+# description: Test that the validator throws on parallel files that do not properly fit
+# group: [csv]
+
+statement ok
+PRAGMA enable_verification
+
+# We need to do some precise byte math here
+require notwindows
+
+statement error
+FROM read_csv('data/csv/evil_nullpadding.csv', buffer_size=20, quote = '"')
+----
+The Parallel CSV Reader currently does not support a full read on this file.
+
+statement ok
+FROM read_csv('data/csv/evil_nullpadding.csv', buffer_size=20)
+
+query I
+FROM read_csv('data/csv/validator/single_column.csv', header = 0)
+----
+123
+123
+123
+one
+123
+123
+123
+123
+123
+123
+
+statement error
+FROM read_csv('data/csv/validator/single_column.csv', header = 0, columns = {'a': 'integer'}, auto_detect = false)
+----
+Error when converting column "a". Could not convert string "one" to 'INTEGER'
+
+statement error
+FROM read_csv('data/csv/validator/single_column.csv', header = 0, columns = {'a': 'integer'}, auto_detect = false, buffer_size = 11)
+----
+Error when converting column "a". Could not convert string "one" to 'INTEGER'
+
+statement error
+FROM read_csv('data/csv/validator/single_column.csv', header = 0, columns = {'a': 'integer'}, auto_detect = false, buffer_size = 11, parallel = false)
+----
+Error when converting column "a". Could not convert string "one" to 'INTEGER'
+
+statement ok
+FROM read_csv('data/csv/validator/quoted_new_value.csv')
+
+statement ok
+FROM read_csv('data/csv/validator/quoted_new_value.csv', columns = {'band': 'varchar', 'album': 'varchar', 'release': 'varchar'}, quote = '''', delim = ';', header = 0)
+
+statement ok
+FROM read_csv('data/csv/validator/quoted_new_value.csv', columns = {'band': 'varchar', 'album': 'varchar', 'release': 'varchar'}, quote = '''', delim = ';', header = 0, buffer_size = 46)
+
+statement ok
+FROM read_csv('data/csv/validator/single_column_quoted_newline.csv', columns = {'Raffaella Carrà': 'varchar'}, quote = '"',  buffer_size = 24)
+
+statement ok
+FROM read_csv('data/csv/validator/single_column_notquoted_newline.csv', columns = {'Raffaella Carrà': 'varchar'}, quote = '"',  buffer_size = 22)
\ No newline at end of file
diff --git a/test/sql/copy/csv/test_wrong_newline_delimiter.test b/test/sql/copy/csv/test_wrong_newline_delimiter.test
index 3066590d0a0c..5539ef248a24 100644
--- a/test/sql/copy/csv/test_wrong_newline_delimiter.test
+++ b/test/sql/copy/csv/test_wrong_newline_delimiter.test
@@ -16,7 +16,7 @@ FROM read_csv('data/csv/timestamp.csv', columns = {'a': 'BIGINT'}, new_line= '

 new_line = 
 (Set By User)
 
 statement error
-FROM read_csv('data/csv/timestamp.csv', columns = {'a': 'BIGINT'}, new_line= '\r
')
+FROM read_csv('data/csv/timestamp.csv', columns = {'a': 'BIGINT'}, new_line= '\r
', auto_detect = false)
 ----
 new_line = \r
 (Set By User)
 
diff --git a/test/sql/copy/file_size_bytes.test b/test/sql/copy/file_size_bytes.test
index a16e53897863..e0436518d27d 100644
--- a/test/sql/copy/file_size_bytes.test
+++ b/test/sql/copy/file_size_bytes.test
@@ -61,8 +61,9 @@ SELECT count(*) FROM glob('__TEST_DIR__/file_size_bytes_json/*.json')
 require parquet
 
 # we can trigger early flushes with Parquet by setting a low row group size
+# also, we have to use DOUBLE for Parquet because we compress integers by A LOT which affects the file_size_bytes
 statement ok
-COPY (FROM bigdata) TO '__TEST_DIR__/file_size_bytes_parquet' (FORMAT PARQUET, ROW_GROUP_SIZE 2000, FILE_SIZE_BYTES '1kb');
+COPY (SELECT col_a::DOUBLE, col_b::DOUBLE FROM bigdata) TO '__TEST_DIR__/file_size_bytes_parquet' (FORMAT PARQUET, ROW_GROUP_SIZE 2000, FILE_SIZE_BYTES '1kb');
 
 query I
 SELECT COUNT(*) FROM read_parquet('__TEST_DIR__/file_size_bytes_parquet/*.parquet')
diff --git a/test/sql/copy/parquet/afl.test b/test/sql/copy/parquet/afl.test
index c538f992a9dd..8b8e05b798c1 100644
--- a/test/sql/copy/parquet/afl.test
+++ b/test/sql/copy/parquet/afl.test
@@ -25,6 +25,6 @@ foreach i 3 4 5 7
 statement error
 select * from parquet_scan('data/parquet-testing/afl/3.parquet')
 ----
-Parquet file is likely corrupted, cannot have dictionary offsets without seeing a non-empty dictionary first.
+Invalid Error: Parquet file is likely corrupted, missing dictionary
 
 endloop
\ No newline at end of file
diff --git a/test/sql/copy/parquet/describe_parquet.test b/test/sql/copy/parquet/describe_parquet.test
new file mode 100644
index 000000000000..f8b7eb7b3900
--- /dev/null
+++ b/test/sql/copy/parquet/describe_parquet.test
@@ -0,0 +1,14 @@
+# name: test/sql/copy/parquet/describe_parquet.test
+# description: Test DESCRIBE on a parquet file
+# group: [parquet]
+
+require parquet
+
+query IIIIII nosort describeresult
+DESCRIBE 'data/parquet-testing/delta_byte_array.parquet'
+
+query IIIIII nosort describeresult
+DESCRIBE "data/parquet-testing/delta_byte_array.parquet"
+
+query IIIIII nosort describeresult
+DESCRIBE FROM read_parquet("data/parquet-testing/delta_byte_array.parquet")
diff --git a/test/sql/copy/parquet/hive_timestamps.test b/test/sql/copy/parquet/hive_timestamps.test
index 6e29e0d9bc1e..533c8eb3499a 100644
--- a/test/sql/copy/parquet/hive_timestamps.test
+++ b/test/sql/copy/parquet/hive_timestamps.test
@@ -39,11 +39,11 @@ FROM timeseries
 ORDER BY ALL
 LIMIT 5
 ----
-2023-11-01 00:00:00	15134
-2023-11-01 01:00:00	16968
-2023-11-01 02:00:00	13882
-2023-11-01 03:00:00	14317
-2023-11-01 04:00:00	14905
+2023-11-01 00:00:00	15848
+2023-11-01 01:00:00	13946
+2023-11-01 02:00:00	14636
+2023-11-01 03:00:00	13540
+2023-11-01 04:00:00	16269
 
 statement ok
 COPY (
@@ -58,8 +58,8 @@ FROM read_parquet('__TEST_DIR__/hive/*/*.parquet')
 ORDER BY ALL
 LIMIT 5
 ----
-2023-11-01 00:00:00	15134
-2023-11-01 01:00:00	16968
-2023-11-01 02:00:00	13882
-2023-11-01 03:00:00	14317
-2023-11-01 04:00:00	14905
+2023-11-01 00:00:00	15848
+2023-11-01 01:00:00	13946
+2023-11-01 02:00:00	14636
+2023-11-01 03:00:00	13540
+2023-11-01 04:00:00	16269
diff --git a/test/sql/copy/parquet/parquet_5209.test b/test/sql/copy/parquet/parquet_5209.test
index 317f656e11d3..33811fd54467 100644
--- a/test/sql/copy/parquet/parquet_5209.test
+++ b/test/sql/copy/parquet/parquet_5209.test
@@ -9,8 +9,9 @@ require vector_size 2048
 statement ok
 SET threads=1;
 
+# we cast range to double because we compress integers now which messes up the test
 statement ok
-CREATE TABLE test_5209 AS FROM range(10000);
+CREATE TABLE test_5209 AS SELECT range::DOUBLE FROM range(10000);
 
 statement ok
 COPY test_5209 TO '__TEST_DIR__/test_5209.parquet' (ROW_GROUP_SIZE 1000);
@@ -19,4 +20,4 @@ query III
 SELECT SUM(total_compressed_size) > 10000, SUM(total_uncompressed_size) > 10000, SUM(total_uncompressed_size) > SUM(total_compressed_size)
 FROM parquet_metadata('__TEST_DIR__/test_5209.parquet');
 ----
-1	1	1
\ No newline at end of file
+1	1	1
diff --git a/test/sql/copy/parquet/parquet_hive.test b/test/sql/copy/parquet/parquet_hive.test
index df40e3de494c..e78dde338553 100644
--- a/test/sql/copy/parquet/parquet_hive.test
+++ b/test/sql/copy/parquet/parquet_hive.test
@@ -158,8 +158,8 @@ SELECT a, b, replace(filename, '\', '/') filename FROM parquet_scan('data/parque
 
 # Test handling missing files
 query IIII
-select id, value, part, date 
-from parquet_scan('data/parquet-testing/hive-partitioning/missing/*/*/test.parquet', HIVE_PARTITIONING=1) 
+select id, value, part, date
+from parquet_scan('data/parquet-testing/hive-partitioning/missing/*/*/test.parquet', HIVE_PARTITIONING=1)
 order by id
 ----
 3	value3	c	2014-01-01
diff --git a/test/sql/copy/parquet/replacement_scan_custom_extension.test b/test/sql/copy/parquet/replacement_scan_custom_extension.test
new file mode 100644
index 000000000000..843e4a7458c1
--- /dev/null
+++ b/test/sql/copy/parquet/replacement_scan_custom_extension.test
@@ -0,0 +1,21 @@
+# name: test/sql/copy/parquet/replacement_scan_custom_extension.test
+# description: Test unknown extensions in replacement scans
+# group: [parquet]
+
+require parquet
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+COPY (SELECT 42 a) to '__TEST_DIR__/lists.my_file_extension' (FORMAT PARQUET);
+
+statement error
+FROM '__TEST_DIR__/lists.my_file_extension';
+----
+No extension found that is capable of reading the file
+
+query I
+FROM read_parquet('__TEST_DIR__/lists.my_file_extension')
+----
+42
diff --git a/test/sql/copy/parquet/struct_column_reader_skip.test b/test/sql/copy/parquet/struct_column_reader_skip.test
index f94e4c53d00c..02ae1dbb698a 100644
--- a/test/sql/copy/parquet/struct_column_reader_skip.test
+++ b/test/sql/copy/parquet/struct_column_reader_skip.test
@@ -10,4 +10,4 @@ PRAGMA enable_verification
 query III
 SELECT my_map['A'], * FROM parquet_scan('data/parquet-testing/struct_skip_test.parquet') where filter == '0'
 ----
-[NULL]	{A=NULL}	0
+NULL	{A=NULL}	0
diff --git a/test/sql/copy/parquet/test_parquet_filter_pushdown.test b/test/sql/copy/parquet/test_parquet_filter_pushdown.test
index 52e698cfeae5..d172756ea335 100644
--- a/test/sql/copy/parquet/test_parquet_filter_pushdown.test
+++ b/test/sql/copy/parquet/test_parquet_filter_pushdown.test
@@ -186,6 +186,11 @@ SELECT COUNT(*) FROM parquet_scan('data/parquet-testing/date.parquet') where d <
 ----
 21
 
+query I
+SELECT COUNT(*) FROM parquet_scan('data/parquet-testing/date.parquet') where d < cast('1975-01-01' as date) or d > cast('1976-01-01' as date)
+----
+21
+
 query I
 SELECT COUNT(*) FROM parquet_scan('data/parquet-testing/date.parquet') where d < cast('1975-01-01' as date) or d >= cast('1976-01-01' as date)
 ----
diff --git a/test/sql/copy/parquet/writer/parquet_write_compression_level.test b/test/sql/copy/parquet/writer/parquet_write_compression_level.test
index 8ba2a3391fdd..9bb277e477e3 100644
--- a/test/sql/copy/parquet/writer/parquet_write_compression_level.test
+++ b/test/sql/copy/parquet/writer/parquet_write_compression_level.test
@@ -4,6 +4,9 @@
 
 require parquet
 
+# NOTE: since updating ZSTD, compression levels between -131072 and 22 are now supported
+# We now also support this, and this test has been updated accordingly
+
 statement ok
 PRAGMA enable_verification
 
@@ -15,15 +18,21 @@ COPY integers TO '__TEST_DIR__/compress_level.parquet' (FORMAT 'parquet', COMPRE
 ----
 only supported
 
-statement error
+statement ok
 COPY integers TO '__TEST_DIR__/compress_level.parquet' (FORMAT 'parquet', CODEC ZSTD, COMPRESSION_LEVEL 0);
-----
-must be between 1 and 22
 
 statement error
 COPY integers TO '__TEST_DIR__/compress_level.parquet' (FORMAT 'parquet', CODEC ZSTD, COMPRESSION_LEVEL 23);
 ----
-must be between 1 and 22
+level must be between
+
+statement ok
+COPY integers TO '__TEST_DIR__/compress_level.parquet' (FORMAT 'parquet', CODEC ZSTD, COMPRESSION_LEVEL -131072);
+
+statement error
+COPY integers TO '__TEST_DIR__/compress_level.parquet' (FORMAT 'parquet', CODEC ZSTD, COMPRESSION_LEVEL -131073);
+----
+level must be between
 
 statement ok
 COPY integers TO '__TEST_DIR__/compress_level.parquet' (FORMAT 'parquet', CODEC ZSTD, COMPRESSION_LEVEL 1);
diff --git a/test/sql/copy/partitioned/partitioned_group_by.test b/test/sql/copy/partitioned/partitioned_group_by.test
new file mode 100644
index 000000000000..8b5ac2612514
--- /dev/null
+++ b/test/sql/copy/partitioned/partitioned_group_by.test
@@ -0,0 +1,157 @@
+# name: test/sql/copy/partitioned/partitioned_group_by.test
+# description: Test partitioned aggregates
+# group: [partitioned]
+
+require parquet
+
+statement ok
+CREATE TABLE partitioned_tbl AS SELECT i%2 AS partition, i col1, i // 7 col2, (i%3)::VARCHAR col3 FROM range(10000) t(i)
+
+statement ok
+COPY partitioned_tbl TO '__TEST_DIR__/partition_group_by' (FORMAT parquet, PARTITION_BY (partition))
+
+statement ok
+DROP TABLE partitioned_tbl
+
+statement ok
+CREATE VIEW partitioned_tbl AS FROM '__TEST_DIR__/partition_group_by/**/*.parquet'
+
+query II
+SELECT partition, SUM(col1)
+FROM partitioned_tbl
+GROUP BY partition
+ORDER BY ALL
+----
+0	24995000
+1	25000000
+
+# make sure the partitioned aggregate is used
+# all of these are identical, i.e. this gets folded into a single count aggregate
+query II
+EXPLAIN SELECT partition, SUM(col1)
+FROM partitioned_tbl
+GROUP BY partition
+ORDER BY ALL
+----
+physical_plan	<REGEX>:.*PARTITIONED_AGGREGATE.*
+
+# distinct aggregate
+query II
+SELECT partition, COUNT(DISTINCT col2)
+FROM partitioned_tbl
+GROUP BY partition
+ORDER BY ALL
+----
+0	1429
+1	1429
+
+# grouping sets
+query II
+SELECT partition, SUM(col1)
+FROM partitioned_tbl
+GROUP BY GROUPING SETS ((), (partition))
+ORDER BY ALL
+----
+0	24995000
+1	25000000
+NULL	49995000
+
+# filtered aggregate
+query II
+SELECT partition, SUM(col1) FILTER (col2%7>2)
+FROM partitioned_tbl
+GROUP BY partition
+ORDER BY ALL
+----
+0	14302848
+1	14302848
+
+query II
+SELECT SUM(col1), partition
+FROM partitioned_tbl
+GROUP BY partition
+ORDER BY ALL
+----
+24995000	0
+25000000	1
+
+# filter
+query II
+SELECT partition, SUM(col1)
+FROM partitioned_tbl
+WHERE col2 > 100
+GROUP BY partition
+ORDER BY ALL
+----
+0	24870038
+1	24875391
+
+# partition on multiple columns
+statement ok
+CREATE TABLE partitioned_tbl2 AS SELECT i%2 AS partition1, i%3 AS partition2, i col1, i + 1 col2 FROM range(10000) t(i)
+
+statement ok
+COPY partitioned_tbl2 TO '__TEST_DIR__/partition_group_by_multiple' (FORMAT parquet, PARTITION_BY (partition1, partition2))
+
+statement ok
+DROP TABLE partitioned_tbl2
+
+statement ok
+CREATE VIEW partitioned_tbl2 AS FROM '__TEST_DIR__/partition_group_by_multiple/**/*.parquet'
+
+query III
+SELECT partition1, partition2, SUM(col1)
+FROM partitioned_tbl2
+GROUP BY partition1, partition2
+ORDER BY ALL
+----
+0	0	8331666
+0	1	8328334
+0	2	8335000
+1	0	8336667
+1	1	8333333
+1	2	8330000
+
+# partition on a subset of the columns
+query II
+SELECT partition1, SUM(col1)
+FROM partitioned_tbl2
+GROUP BY partition1
+ORDER BY ALL
+----
+0	24995000
+1	25000000
+
+query II
+SELECT partition2, SUM(col1)
+FROM partitioned_tbl2
+GROUP BY partition2
+ORDER BY ALL
+----
+0	16668333
+1	16661667
+2	16665000
+
+# with a filter
+query II
+SELECT partition1, SUM(col1)
+FROM partitioned_tbl2
+WHERE partition2=0
+GROUP BY partition1
+ORDER BY ALL
+----
+0	8331666
+1	8336667
+
+# grouping sets
+query III
+SELECT partition1, partition2, SUM(col1)
+FROM partitioned_tbl2
+GROUP BY GROUPING SETS ((partition1), (partition2))
+ORDER BY ALL
+----
+0	NULL	24995000
+1	NULL	25000000
+NULL	0	16668333
+NULL	1	16661667
+NULL	2	16665000
diff --git a/test/sql/create/create_as.test b/test/sql/create/create_as.test
index c122eefada60..2ea97b748932 100644
--- a/test/sql/create/create_as.test
+++ b/test/sql/create/create_as.test
@@ -64,3 +64,54 @@ CREATE TABLE tbl4 IF NOT EXISTS AS SELECT 4;
 statement error
 CREATE OR REPLACE TABLE tbl4 IF NOT EXISTS AS SELECT 4;
 ----
+
+
+### CREATE TABLE t(col1, col2) AS SELECT ... 
+statement ok
+CREATE TABLE tbl4(col1, col2) AS SELECT 1, 'hello';
+
+query II
+SELECT * FROM tbl4;
+----
+1	hello
+
+
+statement ok
+CREATE OR REPLACE TABLE tbl4(col1, col2) AS SELECT 2, 'duck';
+
+query II
+SELECT * FROM tbl4;
+----
+2	duck
+
+
+statement ok
+CREATE TABLE IF NOT EXISTS tbl5(col1, col2) AS SELECT 3, 'database';
+
+query II
+SELECT * FROM tbl5;
+----
+3	database
+
+# define a column name need quote
+statement ok
+CREATE OR REPLACE TABLE tbl5(col1, "col need ' quote") AS SELECT 3.5, 'quote';
+
+query II
+SELECT * FROM tbl5;
+----
+3.5	quote
+
+#colname and query mismatch
+statement ok
+CREATE TABLE tbl6(col1) AS SELECT 4 ,'mismatch';
+
+query II
+SELECT * FROM tbl6;
+----
+4	mismatch
+
+statement error
+CREATE TABLE tbl7(col1, col2) AS SELECT 5;
+----
+Binder Error: Target table has more colum names than query result.
\ No newline at end of file
diff --git a/test/sql/explain/explain_execute.test b/test/sql/explain/explain_execute.test
new file mode 100644
index 000000000000..1b1df1ff8ec4
--- /dev/null
+++ b/test/sql/explain/explain_execute.test
@@ -0,0 +1,20 @@
+# name: test/sql/explain/explain_execute.test
+# description: Test explain
+# group: [explain]
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE integers(i INTEGER, j INTEGER)
+
+statement ok
+INSERT INTO integers VALUES (1, 1), (2, 2), (3, 3), (NULL, NULL)
+
+statement ok
+PREPARE query AS SELECT * FROM integers
+
+query II
+EXPLAIN EXECUTE query
+----
+physical_plan	<REGEX>:.*SEQ_SCAN.*
diff --git a/test/sql/explain/test_explain_analyze.test b/test/sql/explain/test_explain_analyze.test
index 6af71a8a7c09..c14f1fa5786a 100644
--- a/test/sql/explain/test_explain_analyze.test
+++ b/test/sql/explain/test_explain_analyze.test
@@ -80,3 +80,13 @@ statement error
 SELECT * FROM read_csv('__TEST_DIR__/test.json', columns={'json': 'VARCHAR'}, sep='🦆');
 ----
 The delimiter option cannot exceed a size of 1 byte.
+
+query II
+EXPLAIN (ANALYZE, FORMAT graphviz) SELECT SUM(i) FROM integers
+----
+analyzed_plan	<REGEX>:.+EXPLAIN_ANALYZE.*
+
+query II
+EXPLAIN (ANALYZE, FORMAT html) SELECT SUM(i) FROM integers
+----
+analyzed_plan	<REGEX>:.+<div class="title">EXPLAIN_ANALYZE</div>.*
\ No newline at end of file
diff --git a/test/sql/export/export_generated_columns.test b/test/sql/export/export_generated_columns.test
index b385456cd5c3..abf16033b65c 100644
--- a/test/sql/export/export_generated_columns.test
+++ b/test/sql/export/export_generated_columns.test
@@ -2,16 +2,23 @@
 # description: Test export of generated columns
 # group: [export]
 
+statement ok
+pragma storage_compatibility_version='v1.1.0'
+
 statement ok
 PRAGMA enable_verification
 
 statement ok
 BEGIN TRANSACTION
 
+# Create a macro that the generated column uses
+statement ok
+CREATE MACRO my_macro(b) AS b + 5;
+
 statement ok
 CREATE TABLE tbl (
 	x INTEGER,
-	gen_x AS (x + 5)
+	gen_x AS (my_macro(x))
 );
 
 statement ok
@@ -47,6 +54,11 @@ statement error
 INSERT INTO tbl VALUES(2,3)
 ----
 
+statement error
+drop macro my_macro;
+----
+Dependency Error: Cannot drop entry "my_macro" because there are entries that depend on it
+
 # 'x' can not be removed, as 'gen_x' depends on it
 statement error
 ALTER TABLE tbl DROP COLUMN x;
diff --git a/test/sql/export/export_indexes.test b/test/sql/export/export_indexes.test
new file mode 100644
index 000000000000..ac5e36412bd6
--- /dev/null
+++ b/test/sql/export/export_indexes.test
@@ -0,0 +1,52 @@
+# name: test/sql/export/export_indexes.test
+# description: Test export of macro's
+# group: [export]
+
+# FIXME: see duckdb-internal/146
+require skip_reload
+
+statement ok
+BEGIN TRANSACTION
+
+# scalar macro
+statement ok
+CREATE MACRO elaborate_macro(x, y := 7) AS x + y;
+
+statement ok
+CREATE TABLE tbl (x integer, y varchar);
+
+# Index that depends on the 'elaborate_macro' macro function
+statement ok
+CREATE UNIQUE INDEX my_index on tbl (elaborate_macro(tbl.x));
+
+query I
+select index_name from duckdb_indexes();
+----
+my_index
+
+statement ok
+EXPORT DATABASE '__TEST_DIR__/export_macros' (FORMAT CSV);
+
+statement ok
+ROLLBACK
+
+statement ok
+IMPORT DATABASE '__TEST_DIR__/export_macros'
+
+query I
+select index_name from duckdb_indexes();
+----
+my_index
+
+query T
+SELECT elaborate_macro(28, y := 5)
+----
+33
+
+statement ok
+insert into tbl VALUES (10, 'hello');
+
+statement error
+insert into tbl VALUES (10, 'world');
+----
+Constraint Error: Duplicate key "(x + 7): 17" violates unique constraint.
diff --git a/test/sql/export/export_macros.test b/test/sql/export/export_macros.test
index 0cf4ae8231a2..8c2bb087d7c3 100644
--- a/test/sql/export/export_macros.test
+++ b/test/sql/export/export_macros.test
@@ -2,6 +2,9 @@
 # description: Test export of macro's
 # group: [export]
 
+statement ok
+set enable_macro_dependencies=true;
+
 statement ok
 PRAGMA enable_verification
 
diff --git a/test/sql/filter/test_or_pushdown.test b/test/sql/filter/test_or_pushdown.test
new file mode 100644
index 000000000000..a18d298e1268
--- /dev/null
+++ b/test/sql/filter/test_or_pushdown.test
@@ -0,0 +1,71 @@
+# name: test/sql/filter/test_or_pushdown.test
+# description: Test or filter pushdown into zone maps
+# group: [filter]
+
+# no force storage otherwise test to check scanned tupls fails
+require noforcestorage
+
+statement ok
+PRAGMA explain_output = PHYSICAL_ONLY;
+
+statement ok
+create table t1 as select range a, 'foo' || (range%100)::VARCHAR b, 100000-range c from range(900000);
+
+query I
+select count(*) from t1 where a < 5 or b = 'foo8';
+----
+9005
+
+query II
+explain select * from t1 where a<5 or b = 'foo8';
+----
+physical_plan	<!REGEX>:.*optional.*
+
+# columns are different
+query II
+explain select * from t1 where a<5 or c>8000;
+----
+physical_plan	<!REGEX>:.*optional.*
+
+# two different row groups can be propagated here.
+query I
+select a from t1 where a=5 or a=899999;
+----
+5
+899999
+
+# check that multiple rows from mulitple row groups are being projected and not just 2.
+query II
+explain analyze select * from t1 where a=5 or a=899999;
+----
+analyzed_plan	<REGEX>:.*TABLE_SCAN.*19360.*
+
+query II
+explain select * from t1 where a<5 or a>10;
+----
+physical_plan	<REGEX>:.*optional.*
+
+query II
+explain select * from t1 where a in (1, 5, 10);
+----
+physical_plan	<REGEX>:.*optional.*
+
+query II
+explain select * from t1 where a = 1 or a = 5 or a = 10;
+----
+physical_plan	<REGEX>:.*optional.*
+
+query II
+explain select * from t1 where a in (1, 2, 3);
+----
+physical_plan	<!REGEX>:.*optional.*
+
+query II
+explain select * from t1 where a in (1);
+----
+physical_plan	<!REGEX>:.*optional.*
+
+query II
+explain select b from t1 where b = 'foo9' or b = 'foo10';
+----
+physical_plan	<!REGEX>:.*optional.*
diff --git a/test/sql/function/blob/test_blob_array_slice.test b/test/sql/function/blob/test_blob_array_slice.test
new file mode 100644
index 000000000000..3b0d16a02756
--- /dev/null
+++ b/test/sql/function/blob/test_blob_array_slice.test
@@ -0,0 +1,71 @@
+# name: test/sql/function/blob/test_blob_array_slice.test
+# description: Blob slicing test
+# group: [blob]
+
+statement ok
+PRAGMA enable_verification
+
+query I
+select array_slice(blob '\x00\x01\x02\x03\x04\x05', 2, 4)
+----
+\x01\x02\x03
+
+# zero-offset works fine, too
+query I
+select array_slice(blob '\x00\x01\x02\x03\x04\x05', 0, 2)
+----
+\x00\x01
+
+# other syntax works, too
+query I
+select (blob '\x00\x01\x02\x03\x04\x05'::BLOB)[2:4]
+----
+\x01\x02\x03
+
+# we can have offsets bigger than length
+query I
+select array_slice(blob '\x00\x01\x02\x03\x04\x05', 4, 10)
+----
+\x03\x04\x05
+
+# nonsensical offsets lead to empty BLOB
+query I
+select octet_length(array_slice(blob '\x00\x01\x02\x03\x04\x05', 4, 3))
+----
+0
+
+# we can have negative offsets from back, this is somehow consistent with strings
+query I
+select array_slice(blob  '\x00\x01\x02\x03\x04\x05', 2,-2)
+----
+\x01\x02\x03\x04
+
+# both can be negative
+# we can have negative offsets from back, this is somehow consistent with strings
+query I
+select array_slice(blob '\x00\x01\x02\x03\x04\x05', -4, -2)
+----
+\x02\x03\x04
+
+# we can subset utf characters when they're blobs
+query I
+select array_slice(blob '\x00\xF0\x9F\xA6\x86\x00', 2, 3)
+----
+\xF0\x9F
+
+# we can subset utf characters when they're blobs
+query I
+select array_slice(blob '\x00\xF0\x9F\xA6\x86\x00', 4, 6)
+----
+\xA6\x86\x00
+
+# we can slice blob NULL
+query I
+select array_slice(NULL::BLOB, 4, 6)
+----
+NULL
+
+statement error
+select array_slice('hello world', 1, 8, 2);
+----
+Slice with steps has not been implemented for string types
\ No newline at end of file
diff --git a/test/sql/function/list/aggregates/skewness.test b/test/sql/function/list/aggregates/skewness.test
index 70d75e65ce26..5be729491cc1 100644
--- a/test/sql/function/list/aggregates/skewness.test
+++ b/test/sql/function/list/aggregates/skewness.test
@@ -19,6 +19,11 @@ CREATE TABLE skew AS SELECT LIST(10) AS i FROM range(5) t1(i)
 query I
 select list_skewness (i) from skew
 ----
+NAN
+
+query I
+select list_skewness ([1,2])
+----
 NULL
 
 # out of range
diff --git a/test/sql/function/list/list_concat.test b/test/sql/function/list/list_concat.test
index ce170468d49d..a7a73702c61f 100644
--- a/test/sql/function/list/list_concat.test
+++ b/test/sql/function/list/list_concat.test
@@ -187,6 +187,22 @@ SELECT array_push_front(NULL, 1);
 ----
 [1]
 
+# concat operator
+query T
+SELECT [1, 2] || NULL
+----
+NULL
+
+query T
+SELECT [1, 2] || b FROM (VALUES (NULL::INT[])) t(b)
+----
+NULL
+
+query T
+SELECT a || b FROM (VALUES ([1,2,3], NULL::INT[])) t(a,b)
+----
+NULL
+
 # type mismatch
 statement error
 SELECT concat([42], [84], 'str')
diff --git a/test/sql/function/list/list_grade_up.test_slow b/test/sql/function/list/list_grade_up.test_slow
index 7e05d37789a4..993a254f5a3b 100644
--- a/test/sql/function/list/list_grade_up.test_slow
+++ b/test/sql/function/list/list_grade_up.test_slow
@@ -293,7 +293,7 @@ select k, list_grade_up(k,'DESC') from (values ([1,2,3,4])) as t(k);
 query IIII
 select k, v, map(k,v), map(k,v)[(list_grade_up(k,'DESC'))[1]] from (values ([1,2,3,4],[2,3,4,5])) as t(k,v);
 ----
-[1, 2, 3, 4]	[2, 3, 4, 5]	{1=2, 2=3, 3=4, 4=5}	[5]
+[1, 2, 3, 4]	[2, 3, 4, 5]	{1=2, 2=3, 3=4, 4=5}	5
 
 # bug fixes test for #5694
 
diff --git a/test/sql/function/list/list_sort.test b/test/sql/function/list/list_sort.test
index 9dcd2715846c..8389e025cb5a 100644
--- a/test/sql/function/list/list_sort.test
+++ b/test/sql/function/list/list_sort.test
@@ -407,7 +407,7 @@ select k, array_sort(k,'DESC') from (values ([1,2,3,4])) as t(k);
 query IIII
 select k, v, map(k,v), map(k,v)[(array_sort(k,'DESC'))[1]] from (values ([1,2,3,4],[2,3,4,5])) as t(k,v);
 ----
-[1, 2, 3, 4]	[2, 3, 4, 5]	{1=2, 2=3, 3=4, 4=5}	[5]
+[1, 2, 3, 4]	[2, 3, 4, 5]	{1=2, 2=3, 3=4, 4=5}	5
 
 # bug fixes test for #7614
 
diff --git a/test/sql/function/nested/test_struct_insert.test b/test/sql/function/nested/test_struct_insert.test
index b732c91b972c..3054a1c07330 100644
--- a/test/sql/function/nested/test_struct_insert.test
+++ b/test/sql/function/nested/test_struct_insert.test
@@ -1,44 +1,68 @@
 # name: test/sql/function/nested/test_struct_insert.test
-# description: Test the struct_insert function
+# description: Test the struct_insert function.
 # group: [nested]
 
 statement ok
 pragma enable_verification
 
-# test basic insertion into a struct
+# Test basic insertion into a STRUCT.
 query T
-SELECT struct_insert ({a: 1, b: 2}, c := 3)
+SELECT struct_insert ({a: 1, b: 2}, c := 3);
 ----
 {'a': 1, 'b': 2, 'c': 3}
 
-# insertion into a struct generated by a row of data
+# Insertion into a STRUCT generated by a row of data.
 query T
-WITH data AS (
-    SELECT 1 as a, 2 as b, 3 as c
-)
-SELECT struct_insert (data, d := 4) FROM data
+WITH data AS (SELECT 1 AS a, 2 AS b, 3 AS c)
+SELECT struct_insert (data, d := 4) FROM data;
 ----
 {'a': 1, 'b': 2, 'c': 3, 'd': 4}
 
-# insertion of a nested struct into an existing struct
+# Insertion of a nested STRUCT into an existing STRUCT.
 query T
-select struct_insert({'a': 1, 'b': 'abc', 'c': true}, d := {'a': 'new stuff'})
+SELECT struct_insert({'a': 1, 'b': 'abc', 'c': true}, d := {'a': 'new stuff'});
 ----
 {'a': 1, 'b': abc, 'c': true, 'd': {'a': new stuff}}
 
-# test incorrect usage
+# Test incorrect usage.
 statement error
-select struct_insert()
+SELECT struct_insert();
 ----
+<REGEX>:Invalid Input Error.*Missing required arguments for struct_insert function.*
 
 statement error
-select struct_insert({a: 1, b: 2})
+SELECT struct_insert({a: 1, b: 2});
 ----
+<REGEX>:Invalid Input Error.*insert nothing into a STRUCT.*
 
 statement error
-select struct_insert(123, a := 1)
+SELECT struct_insert(123, a := 1);
 ----
+<REGEX>:Invalid Input Error.*The first argument to struct_insert must be a STRUCT.*
 
 statement error
-select struct_insert({a: 1, b: 2}, a := 2)
+SELECT struct_insert({a: 1, b: 2}, a := 2);
 ----
+<REGEX>:Binder Error.*Duplicate struct entry name.*
+
+# Test inserting NULL as the default value.
+
+statement ok
+CREATE TABLE tbl (col STRUCT(i INT));
+
+statement ok
+INSERT INTO tbl SELECT {'i': range} FROM range(3);
+
+query I
+SELECT struct_insert(col, a := col.i + 1, b := NULL::VARCHAR) FROM tbl ORDER BY ALL;
+----
+{'i': 0, 'a': 1, 'b': NULL}
+{'i': 1, 'a': 2, 'b': NULL}
+{'i': 2, 'a': 3, 'b': NULL}
+
+query I
+SELECT struct_insert(col, a := NULL, b := NULL::VARCHAR, c := [NULL]) FROM tbl ORDER BY ALL;
+----
+{'i': 0, 'a': NULL, 'b': NULL, 'c': [NULL]}
+{'i': 1, 'a': NULL, 'b': NULL, 'c': [NULL]}
+{'i': 2, 'a': NULL, 'b': NULL, 'c': [NULL]}
\ No newline at end of file
diff --git a/test/sql/function/numeric/set_seed_for_sample.test b/test/sql/function/numeric/set_seed_for_sample.test
new file mode 100644
index 000000000000..771c23192498
--- /dev/null
+++ b/test/sql/function/numeric/set_seed_for_sample.test
@@ -0,0 +1,24 @@
+# name: test/sql/function/numeric/set_seed_for_sample.test
+# description: Test setseed for samples
+# group: [numeric]
+
+require skip_reload
+
+statement ok
+create table t1 as select * from generate_series(1,50) as t(number);
+
+statement ok
+select setseed(0.1);
+
+query I rowsort result_1
+select * from t1 using sample 5;
+----
+
+statement ok
+select setseed(0.1);
+
+query I rowsort result_1
+select * from t1 using sample 5;
+----
+
+
diff --git a/test/sql/function/operator/test_conjunction.test b/test/sql/function/operator/test_conjunction.test
index 363ec1a6b776..e90329323d05 100644
--- a/test/sql/function/operator/test_conjunction.test
+++ b/test/sql/function/operator/test_conjunction.test
@@ -10,13 +10,18 @@ statement ok
 CREATE TABLE a (i integer, j integer);
 
 statement ok
-INSERT INTO a VALUES (3, 4), (4, 5), (5, 6)
+INSERT INTO a VALUES (3, 4), (4, 5), (5, 6);
 
 query II
-SELECT * FROM a WHERE (i > 3 AND j < 5) OR (i > 3 AND j > 5)
+SELECT * FROM a WHERE (i > 3 AND j < 5) OR (i > 3 AND j > 5);
 ----
 5	6
 
+query II
+explain SELECT * FROM a WHERE (i > 3 AND j < 5) OR (i > 3 AND j > 5);
+----
+physical_plan	<REGEX>:.*optional.*
+
 # test boolean logic in conjunctions
 query T
 SELECT true AND true
diff --git a/test/sql/function/string/test_jaro_winkler.test b/test/sql/function/string/test_jaro_winkler.test
index 31f7127cc3b8..8f82ca768097 100644
--- a/test/sql/function/string/test_jaro_winkler.test
+++ b/test/sql/function/string/test_jaro_winkler.test
@@ -171,6 +171,27 @@ select jaro_winkler_similarity('PENNSYLVANIA', 'PENNCISYLVNIA')
 ----
 0.8980186480186481
 
+# test score cutoff
+query T
+select jaro_winkler_similarity('CRATE', 'TRACE', 0.7)
+----
+0.733333
+
+query T
+select jaro_winkler_similarity('CRATE', 'TRACE', 0.75)
+----
+0.0
+
+query T
+select jaro_winkler_similarity('000000000000000000000000000000000000000000000000000000000000000', '00000000000000000000000000000000000000000000000000000000000000000', 0.9)
+----
+0.9938
+
+query T
+select jaro_winkler_similarity('000000000000000000000000000000000000000000000000000000000000000', '00000000000000000000000000000000000000000000000000000000000000000', 0.995)
+----
+0.0
+
 # test with table just in case
 statement ok
 create table test as select '0000' || range::varchar s from range(10000);
diff --git a/test/sql/index/art/memory/test_art_linear.test_slow b/test/sql/index/art/memory/test_art_linear.test_slow
index fd6fbec3b489..d8590df0bb7d 100644
--- a/test/sql/index/art/memory/test_art_linear.test_slow
+++ b/test/sql/index/art/memory/test_art_linear.test_slow
@@ -48,8 +48,11 @@ statement ok
 UPDATE empty SET usage = (SELECT mem_to_bytes(current.memory_usage) FROM pragma_database_size() AS current);
 
 query I
-SELECT mem_to_bytes(current.memory_usage) == empty.usage
-FROM pragma_database_size() current, empty;
+SELECT case when base.usage >= empty.usage * 5
+	then true
+	else concat('Memory usage ', base.usage, ' is >= than 5 * empty usage ', empty.usage)::UNION(error VARCHAR, b BOOLEAN)
+	end
+FROM base, empty;
 ----
 true
 
@@ -83,12 +86,6 @@ true
 statement ok
 DROP TABLE t;
 
-query I
-SELECT mem_to_bytes(current.memory_usage) == empty.usage
-FROM pragma_database_size() current, empty;
-----
-true
-
 # create a table with a primary key and store the memory usage
 # now verify that the memory drops, but this time drop the whole table instead of deleting entries from it
 
@@ -108,12 +105,6 @@ FROM base, pragma_database_size() current;
 statement ok
 DROP TABLE t
 
-query I
-SELECT mem_to_bytes(current.memory_usage) == empty.usage
-FROM pragma_database_size() current, empty;
-----
-true
-
 # create a table with a primary key and store the memory usage
 # verify that the memory decreases by approximately half when deleting half the entries
 
diff --git a/test/sql/index/art/storage/test_art_names.test b/test/sql/index/art/storage/test_art_names.test
index 6ef33de92d6b..cdb7aa6853d1 100644
--- a/test/sql/index/art/storage/test_art_names.test
+++ b/test/sql/index/art/storage/test_art_names.test
@@ -7,22 +7,22 @@ load __TEST_DIR__/test_art_names.db
 statement ok
 PRAGMA enable_verification;
 
-# test PKs and UNIQUE
+# Test PKs and UNIQUE constraints.
 
 statement ok
 CREATE TABLE tbl (i INTEGER PRIMARY KEY, j INTEGER UNIQUE);
 
-# names match the internal constraint names
+# Test that index names match the internal constraints.
 
 statement error
 CREATE INDEX PRIMARY_tbl_0 ON tbl(i);
 ----
-already exists
+<REGEX>:Catalog Error.*An index with the name PRIMARY_tbl_0 already exists.*
 
 statement error
 CREATE INDEX UNIQUE_tbl_1 ON tbl(j);
 ----
-already exists
+<REGEX>:Catalog Error.*An index with the name UNIQUE_tbl_1 already exists.*
 
 statement ok
 INSERT INTO tbl SELECT range, range FROM range (3000);
@@ -30,46 +30,46 @@ INSERT INTO tbl SELECT range, range FROM range (3000);
 statement error
 INSERT INTO tbl VALUES (4000, 20);
 ----
-constraint violation
+<REGEX>:Constraint Error.*violates unique constraint.*
 
 statement error
 INSERT INTO tbl VALUES (20, 4000);
 ----
-constraint violation
+<REGEX>:Constraint Error.*violates primary key constraint.*
 
 restart
 
-# test that we deserialized the index names correctly
+# Test index name deserialization.
 
 statement error
 INSERT INTO tbl VALUES (4000, 20);
 ----
-constraint violation
+<REGEX>:Constraint Error.*violates unique constraint.*
 
 statement error
 INSERT INTO tbl VALUES (20, 4000);
 ----
-constraint violation
+<REGEX>:Constraint Error.*violates primary key constraint.*
 
 statement error
 CREATE INDEX PRIMARY_tbl_0 ON tbl(i);
 ----
-already exists
+<REGEX>:Catalog Error.*An index with the name PRIMARY_tbl_0 already exists.*
 
 statement error
 CREATE INDEX UNIQUE_tbl_1 ON tbl(j);
 ----
-already exists
+<REGEX>:Catalog Error.*An index with the name UNIQUE_tbl_1 already exists.*
 
 statement error
 INSERT INTO tbl VALUES (20, 4000);
 ----
-constraint violation
+<REGEX>:Constraint Error.*violates primary key constraint.*
 
 statement error
 INSERT INTO tbl VALUES (4000, 20);
 ----
-constraint violation
+<REGEX>:Constraint Error.*violates unique constraint.*
 
 restart
 
@@ -78,7 +78,7 @@ DROP TABLE tbl;
 
 restart
 
-# test PKs, FKs, and UNIQUE
+# Test PKs, FKs, and UNIQUE constraints.
 
 statement ok
 CREATE TABLE tbl (i INTEGER PRIMARY KEY, j INTEGER UNIQUE);
@@ -94,124 +94,124 @@ INSERT INTO tbl SELECT range, range FROM range (3000);
 statement ok
 INSERT INTO fk_tbl SELECT range, range FROM range (3000);
 
-# check all constraints
+# Check all constraint violations and catalog errors.
 
 statement error
 INSERT INTO tbl VALUES (4000, 20);
 ----
-constraint violation
+<REGEX>:Constraint Error.*violates unique constraint.*
 
 statement error
 INSERT INTO tbl VALUES (20, 4000);
 ----
-constraint violation
+<REGEX>:Constraint Error.*violates primary key constraint.*
 
 statement error
 INSERT INTO fk_tbl VALUES (4000, 20);
 ----
-foreign key constraint
+<REGEX>:Constraint Error.*Violates foreign key constraint because key.*does not exist in the referenced table.*
 
 statement error
 INSERT INTO fk_tbl VALUES (20, 4000);
 ----
-foreign key constraint
+<REGEX>:Constraint Error.*Violates foreign key constraint because key.*does not exist in the referenced table.*
 
 statement error
 CREATE INDEX PRIMARY_tbl_0 ON tbl(i);
 ----
-already exists
+<REGEX>:Catalog Error.*index with the name PRIMARY_tbl_0 already exists.*
 
 statement error
 CREATE INDEX UNIQUE_tbl_1 ON tbl(j);
 ----
-already exists
+<REGEX>:Catalog Error.*index with the name UNIQUE_tbl_1 already exists.*
 
-# fails on the FK table
+# Fails on the FK table.
 
 statement error
 CREATE INDEX FOREIGN_fk_tbl_0 ON fk_tbl(i);
 ----
-already exists
+<REGEX>:Catalog Error.*index with the name FOREIGN_fk_tbl_0 already exists.*
 
 statement error
 CREATE INDEX FOREIGN_fk_tbl_1 ON fk_tbl(j);
 ----
-already exists
+<REGEX>:Catalog Error.*index with the name FOREIGN_fk_tbl_1 already exists.*
 
-# check all constraints
+# Check all constraint violations and catalog errors.
 
 statement error
 INSERT INTO tbl VALUES (4000, 20);
 ----
-constraint violation
+<REGEX>:Constraint Error.*violates unique constraint.*
 
 statement error
 INSERT INTO tbl VALUES (20, 4000);
 ----
-constraint violation
+<REGEX>:Constraint Error.*violates primary key constraint.*
 
 statement error
 INSERT INTO fk_tbl VALUES (4000, 20);
 ----
-foreign key constraint
+<REGEX>:Constraint Error.*Violates foreign key constraint because key.*does not exist in the referenced table.*
 
 statement error
 INSERT INTO fk_tbl VALUES (20, 4000);
 ----
-foreign key constraint
+<REGEX>:Constraint Error.*Violates foreign key constraint because key.*does not exist in the referenced table.*
 
 statement error
 CREATE INDEX PRIMARY_tbl_0 ON tbl(i);
 ----
-already exists
+<REGEX>:Catalog Error.*index with the name PRIMARY_tbl_0 already exists.*
 
 statement error
 CREATE INDEX UNIQUE_tbl_1 ON tbl(j);
 ----
-already exists
+<REGEX>:Catalog Error.*index with the name UNIQUE_tbl_1 already exists.*
 
 restart
 
 statement error
 CREATE INDEX PRIMARY_tbl_0 ON tbl(i);
 ----
-already exists
+<REGEX>:Catalog Error.*index with the name PRIMARY_tbl_0 already exists.*
 
 statement error
 CREATE INDEX UNIQUE_tbl_1 ON tbl(j);
 ----
-already exists
+<REGEX>:Catalog Error.*index with the name UNIQUE_tbl_1 already exists.*
 
-# fails on the FK table
+# Fails on the FK table.
 
 statement error
 CREATE INDEX FOREIGN_fk_tbl_0 ON fk_tbl(i);
 ----
-already exists
+<REGEX>:Catalog Error.*index with the name FOREIGN_fk_tbl_0 already exists.*
 
 statement error
 CREATE INDEX FOREIGN_fk_tbl_1 ON fk_tbl(j);
 ----
-already exists
+<REGEX>:Catalog Error.*index with the name FOREIGN_fk_tbl_1 already exists.*
 
-# check all constraints
+# Check all constraint errors.
 
 statement error
 INSERT INTO tbl VALUES (4000, 20);
 ----
-constraint violation
+<REGEX>:Constraint Error.*Duplicate key "j: 20" violates unique constraint.*
 
 statement error
 INSERT INTO tbl VALUES (20, 4000);
 ----
-constraint violation
+<REGEX>:Constraint Error.*Duplicate key "i: 20" violates primary key constraint.*
 
 statement error
 INSERT INTO fk_tbl VALUES (4000, 20);
 ----
-foreign key constraint
+<REGEX>:Constraint Error.*Violates foreign key constraint because key "i: 4000" does not exist in the referenced table.*
 
 statement error
 INSERT INTO fk_tbl VALUES (20, 4000);
 ----
-foreign key constraint
+<REGEX>:Constraint Error.*Violates foreign key constraint because key "j: 4000" does not exist in the referenced table.*
diff --git a/test/sql/index/art/vacuum/test_art_vacuum_strings.test_slow b/test/sql/index/art/vacuum/test_art_vacuum_strings.test_slow
index 1d89e1c000b9..f81c1fd9de6b 100644
--- a/test/sql/index/art/vacuum/test_art_vacuum_strings.test_slow
+++ b/test/sql/index/art/vacuum/test_art_vacuum_strings.test_slow
@@ -68,10 +68,16 @@ statement ok
 INSERT INTO t SELECT (range * 4) || 'I am' || (range * 4) || 'a long not' || (range * 4) || 'inlined string' || (range * 4) FROM range(100000) AS range;
 
 query I
-SELECT mem_to_bytes(current.memory_usage) > 4 * base.usage AND mem_to_bytes(current.memory_usage) < 7 * base.usage
+SELECT
+case
+when mem_to_bytes(current.memory_usage) > 4 * base.usage AND mem_to_bytes(current.memory_usage) <= 8 * base.usage
+then true
+else
+concat('current mem usage not between 4X and 7X base (current ', current.memory_usage, ', base ', base.usage, ')')::union(err varchar, b bool)
+end
 FROM base, pragma_database_size() current;
 ----
-1
+true
 
 # store the current size of the DB
 statement ok
diff --git a/test/sql/join/external/tpch_all_tables.test_slow b/test/sql/join/external/tpch_all_tables.test_slow
new file mode 100644
index 000000000000..554a1597fbb1
--- /dev/null
+++ b/test/sql/join/external/tpch_all_tables.test_slow
@@ -0,0 +1,37 @@
+# name: test/sql/join/external/tpch_all_tables.test_slow
+# description: Join all tables in TPC-H together under a tight memory limit
+# group: [external]
+
+load __TEST_DIR__/tpch_all_tables.db
+
+require tpch
+
+statement ok
+CALL dbgen(sf=1)
+
+statement ok
+SET threads=4;
+
+statement ok
+SET memory_limit='1GB';
+
+query I
+SELECT min(COLUMNS(*))
+FROM customer c,
+     lineitem l,
+     nation n,
+     orders o,
+     part p,
+     partsupp ps,
+     region r,
+     supplier s
+WHERE c.c_custkey = o.o_custkey
+  AND n.n_nationkey = c.c_nationkey
+  AND o.o_orderkey = l.l_orderkey
+  AND p.p_partkey = ps.ps_partkey
+  AND ps.ps_partkey = l.l_partkey
+  AND ps.ps_suppkey = l.l_suppkey
+  AND r.r_regionkey = n.n_regionkey
+  AND s.s_suppkey = ps.ps_suppkey
+----
+61 values hashing to 75e39ab1e96c3069839f38729b0ed427
diff --git a/test/sql/join/iejoin/iejoin_projection_maps.test b/test/sql/join/iejoin/iejoin_projection_maps.test
index c4090b320968..ea3c7a771e21 100644
--- a/test/sql/join/iejoin/iejoin_projection_maps.test
+++ b/test/sql/join/iejoin/iejoin_projection_maps.test
@@ -27,7 +27,7 @@ query IIIIII
 SELECT SUM(id) AS id, SUM(id2) AS id2, SUM(id3) AS id3, SUM(value) AS sum_value, SUM(one_min_value) AS sum_one_min_value, sum_value + sum_one_min_value AS sum
 FROM df
 ----
-256630	29499	17606	2500.532	2499.468	5000.000
+255978	29751	17482	2504.293	2495.704	4999.997
 
 statement ok
 PRAGMA enable_verification
@@ -51,7 +51,7 @@ FROM (
 GROUP BY ALL
 ORDER BY ALL
 ----
-660 values hashing to 0acad7a6a360d48246479ab35572ecce
+660 values hashing to 7a9e98649bddd1ba74c34324caf4674c
 
 endloop
 
diff --git a/test/sql/join/iejoin/test_iejoin_events.test b/test/sql/join/iejoin/test_iejoin_events.test
index 8876036d5505..86de26c9a45b 100644
--- a/test/sql/join/iejoin/test_iejoin_events.test
+++ b/test/sql/join/iejoin/test_iejoin_events.test
@@ -43,4 +43,4 @@ SELECT COUNT(*) FROM (
 	  AND r.id <> s.id
 ) q2;
 ----
-6
+4
diff --git a/test/sql/join/pushdown/pushdown_join_subquery.test b/test/sql/join/pushdown/pushdown_join_subquery.test
index e3555d75e170..9466784772f5 100644
--- a/test/sql/join/pushdown/pushdown_join_subquery.test
+++ b/test/sql/join/pushdown/pushdown_join_subquery.test
@@ -55,3 +55,80 @@ query III
 SELECT * FROM (SELECT i + 2 AS i, i AS k FROM integers) JOIN (SELECT MAX(i) AS max_i FROM integers) ON i=max_i
 ----
 999	997	999
+
+# group by with join on grouping column
+query II
+SELECT * FROM (SELECT i FROM integers GROUP BY i) JOIN (SELECT MAX(i) AS max_i FROM integers) ON i=max_i
+----
+999	999
+
+# group by with join on aggregate
+query III
+SELECT * FROM (SELECT (1000 - i) AS grp, SUM(i) AS i FROM integers GROUP BY grp) JOIN (SELECT MAX(i) AS max_i FROM integers) ON i=max_i
+----
+1	999	999
+
+# distinct
+query II
+SELECT * FROM (SELECT DISTINCT i FROM integers) JOIN (SELECT MAX(i) AS max_i FROM integers) ON i=max_i
+----
+999	999
+
+# window
+query IIII
+SELECT * FROM (SELECT i + 2 AS i, i AS k, row_number() over () as rownum FROM integers) JOIN (SELECT MAX(i) AS max_i FROM integers) ON i=max_i
+----
+999	997	998	999
+
+statement ok
+CREATE TABLE lists AS SELECT CASE WHEN i%2=0 THEN NULL ELSE i END AS i, [500 + i, 500 + i + 1, 500 + i + 2] l FROM range(1000) t(i)
+
+# unnest where filter applies to non-unnest column
+query III
+SELECT * FROM (SELECT i, UNNEST(l) AS l FROM lists) JOIN (SELECT MAX(i) AS max_i FROM lists) ON i=max_i
+----
+999	1499	999
+999	1500	999
+999	1501	999
+
+# unnest where filter applies to unnest column
+query III
+SELECT * FROM (SELECT i AS k, UNNEST(l) AS i FROM lists) JOIN (SELECT MAX(i) AS max_i FROM lists) ON i=max_i
+----
+497	999	999
+NULL	999	999
+499	999	999
+
+# set operations
+# union
+query II
+SELECT * FROM (FROM integers UNION ALL FROM integers) JOIN (SELECT MAX(i) AS max_i FROM integers) ON i=max_i
+----
+999	999
+999	999
+
+query II
+SELECT * FROM (FROM integers UNION FROM integers) JOIN (SELECT MAX(i) AS max_i FROM integers) ON i=max_i
+----
+999	999
+
+# intersect
+query II
+SELECT * FROM (FROM integers INTERSECT FROM integers) JOIN (SELECT MAX(i) AS max_i FROM integers) ON i=max_i
+----
+999	999
+
+query II
+SELECT * FROM (FROM integers INTERSECT ALL FROM integers) JOIN (SELECT MAX(i) AS max_i FROM integers) ON i=max_i
+----
+999	999
+
+# except
+query II
+SELECT * FROM (FROM integers EXCEPT (SELECT 42)) JOIN (SELECT MAX(i) AS max_i FROM integers) ON i=max_i
+----
+999	999
+
+query II
+SELECT * FROM ((SELECT 999 i) EXCEPT FROM integers) JOIN (SELECT MAX(i) AS max_i FROM integers) ON i=max_i
+----
diff --git a/test/sql/json/issues/issue14167.test b/test/sql/json/issues/issue14167.test
new file mode 100644
index 000000000000..52083a2965bb
--- /dev/null
+++ b/test/sql/json/issues/issue14167.test
@@ -0,0 +1,11 @@
+# name: test/sql/json/issues/issue14167.test
+# description: Test issue 14167 - Dot notation for json field extraction is no longer working in v1.1.*
+# group: [issues]
+
+require json
+
+# the auto-detected type is a MAP, but we can still extract using the dot syntax because we rewrite to map_extract
+query I
+select columns.v4_c6 from read_ndjson_auto('data/json/14167.json');
+----
+{'statistics': {'nonNullCount': 0}}
diff --git a/test/sql/optimizer/test_no_pushdown_cast_into_cte.test b/test/sql/optimizer/test_no_pushdown_cast_into_cte.test
new file mode 100644
index 000000000000..121240a812ec
--- /dev/null
+++ b/test/sql/optimizer/test_no_pushdown_cast_into_cte.test
@@ -0,0 +1,74 @@
+# name: test/sql/optimizer/test_no_pushdown_cast_into_cte.test
+# description: No Pushdown cast into cte
+# group: [optimizer]
+
+statement ok
+pragma explain_output='optimized_only';
+
+query II
+WITH t(a, b) AS (
+  SELECT a :: int, b :: int
+  FROM (VALUES
+    ('1', '4'),
+    ('5', '3'),
+    ('2', '*'),
+    ('3', '8'),
+    ('7', '*')) AS _(a, b)
+  WHERE position('*' in b) = 0
+)
+SELECT a, b
+FROM   t
+WHERE  a < b;
+----
+1	4
+3	8
+
+
+# check filter is above projection that casts the varchar to int
+query II
+EXPLAIN WITH t(a, b) AS (
+  SELECT a :: int, b :: int
+  FROM (VALUES
+    ('1', '4'),
+    ('5', '3'),
+    ('2', '*'),
+    ('3', '8'),
+    ('7', '*')) AS _(a, b)
+  WHERE position('*' in b) = 0
+)
+SELECT a, b
+FROM   t
+WHERE  a < b;
+----
+logical_opt	<REGEX>:.*FILTER.*CAST\(a AS INTEGER.*<.*b AS INTEGER\).*PROJECTION.*FILTER.*position.*
+
+
+# INT can always be cast to varchar, so the filter a[1] = '1'
+# can be pushed down
+query II
+with t(a, b) as  (
+	select a :: varchar, b :: varchar
+	FROM VALUES
+	(1, 2),
+	(3, 3),
+	(5, 6),
+	(7, 6) as
+	_(a, b) where a <= b
+) select a, b from t where a[1] = '1';
+----
+1	2
+
+
+# we should not see two filters, since the filter can be pushed to just above the column data scan
+query II
+explain with t(a, b) as  (
+	select a :: varchar, b :: varchar
+	FROM VALUES
+	(1, 2),
+	(3, 3),
+	(5, 6),
+	(7, 6) as
+	_(a, b) where a <= b
+) select a, b from t where a[1] = '1';
+----
+logical_opt	<!REGEX>:.*FILTER.*PROJECTION.*FILTER.*
diff --git a/test/sql/parser/star_expression.test b/test/sql/parser/star_expression.test
index 67088ec0bf33..edaca57c74e3 100644
--- a/test/sql/parser/star_expression.test
+++ b/test/sql/parser/star_expression.test
@@ -51,7 +51,7 @@ SELECT * FROM integers ORDER BY *, *
 statement error
 SELECT * FROM integers ORDER BY * + 42
 ----
-only allowed as the root element
+cannot be applied to a star expression
 
 statement error
 INSERT INTO integers VALUES (*, *)
diff --git a/test/sql/parser/test_columns_lists.test b/test/sql/parser/test_columns_lists.test
index a95e253b4456..39621709750a 100644
--- a/test/sql/parser/test_columns_lists.test
+++ b/test/sql/parser/test_columns_lists.test
@@ -80,7 +80,7 @@ COLUMNS expression is not allowed inside another COLUMNS expression
 statement error
 SELECT * + 42 FROM integers
 ----
-STAR expression is only allowed as the root element of an expression
+cannot be applied to a star expression
 
 # empty lambda
 statement error
diff --git a/test/sql/pivot/pivot_example.test b/test/sql/pivot/pivot_example.test
index 5f128bb01957..64a009e0e5df 100644
--- a/test/sql/pivot/pivot_example.test
+++ b/test/sql/pivot/pivot_example.test
@@ -201,6 +201,6 @@ UNPIVOT PivotedCities ON 2000, 2010, 2020 ORDER BY ALL LIMIT 1
 NL	Amsterdam	2000	1005
 
 query IIII
-UNPIVOT PivotedCities ON 2000, 2010, 2020 ORDER BY 1 LIMIT 1 OFFSET 1
+UNPIVOT PivotedCities ON 2000, 2010, 2020 ORDER BY 1, 3 LIMIT 1 OFFSET 1
 ----
 NL	Amsterdam	2010	1065
diff --git a/test/sql/projection/select_star_exclude.test b/test/sql/projection/select_star_exclude.test
index 2687bbbbbd1e..7b9757223350 100644
--- a/test/sql/projection/select_star_exclude.test
+++ b/test/sql/projection/select_star_exclude.test
@@ -48,6 +48,11 @@ SELECT integers.* EXCLUDE (i) FROM integers
 ----
 2	3
 
+query II
+SELECT integers.* EXCLUDE ('i') FROM integers
+----
+2	3
+
 query I
 SELECT integers.* EXCLUDE (i, j) FROM integers
 ----
@@ -88,10 +93,52 @@ SELECT * EXCLUDE (i) FROM integers i1 JOIN integers i2 USING (i)
 ----
 2	3	2	3
 
+# qualified exclude entries
+query II
+SELECT * EXCLUDE integers.i FROM integers
+----
+2	3
+
+query I
+SELECT * EXCLUDE (integers.i, integers.j) FROM integers
+----
+3
+
+query I
+SELECT integers.* EXCLUDE (integers.i, integers.j) FROM integers
+----
+3
+
+query I
+SELECT * EXCLUDE (INTEGERS.i, integers.J) FROM integers
+----
+3
+
+statement error
+SELECT * EXCLUDE (integers.i, integers.j, integers2.i) FROM integers
+----
+not found in FROM clause
+
+statement error
+SELECT * EXCLUDE (integers2.i) FROM integers
+----
+not found in FROM clause
+
+query IIII
+SELECT * EXCLUDE (i1.i, i2.i) FROM integers i1 JOIN integers i2 USING (i)
+----
+2	3	2	3
+
 # duplicate entry in exclude list
 statement error
 SELECT * EXCLUDE (i, i) FROM integers
 ----
+Duplicate entry
+
+statement error
+SELECT * EXCLUDE (integers.i, i) FROM integers
+----
+Duplicate entry
 
 # column name that does not exist
 statement error
diff --git a/test/sql/projection/select_star_like.test b/test/sql/projection/select_star_like.test
new file mode 100644
index 000000000000..fcb3a552632f
--- /dev/null
+++ b/test/sql/projection/select_star_like.test
@@ -0,0 +1,64 @@
+# name: test/sql/projection/select_star_like.test
+# description: SELECT * LIKE
+# group: [projection]
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE integers(col1 INTEGER, col2 INTEGER, k INTEGER)
+
+statement ok
+INSERT INTO integers VALUES (1, 2, 3)
+
+query II
+SELECT COLUMNS(x -> x LIKE 'col%') FROM integers
+----
+1	2
+
+query II
+SELECT * LIKE 'col%' FROM integers
+----
+1	2
+
+# not like
+query I
+SELECT * NOT LIKE 'col%' FROM integers
+----
+3
+
+# ilike
+query II
+SELECT * ILIKE 'COL%' FROM integers
+----
+1	2
+
+# regex
+query II
+SELECT * SIMILAR TO '.*col.*' FROM integers
+----
+1	2
+
+# exclude
+query I
+SELECT * EXCLUDE (col1) SIMILAR TO '.*col.*' FROM integers
+----
+2
+
+# non-constant pattern
+statement error
+SELECT * SIMILAR TO pattern FROM integers, (SELECT '.*col.*') t(pattern)
+----
+must be a constant
+
+# unsupported function
+statement error
+SELECT * + 42 FROM integers
+----
+cannot be applied to a star expression
+
+# replace
+statement error
+SELECT * REPLACE (col1 + 42 AS col1) SIMILAR TO '.*col.*' FROM integers
+----
+Replace list cannot be combined with a filtering operation
diff --git a/test/sql/projection/select_star_rename.test b/test/sql/projection/select_star_rename.test
new file mode 100644
index 000000000000..f52f4a9e7b2a
--- /dev/null
+++ b/test/sql/projection/select_star_rename.test
@@ -0,0 +1,87 @@
+# name: test/sql/projection/select_star_rename.test
+# description: SELECT * RENAME
+# group: [projection]
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE integers(i INTEGER, j INTEGER, k INTEGER);
+
+statement ok
+INSERT INTO integers VALUES (1, 2, 3);
+
+# basic rename
+query I
+SELECT renamed_col FROM (SELECT * RENAME i AS renamed_col FROM integers)
+----
+1
+
+# qualified
+query I
+SELECT renamed_col FROM (SELECT * RENAME integers.i AS renamed_col FROM integers)
+----
+1
+
+# qualified star
+query I
+SELECT renamed_col FROM (SELECT integers.* RENAME integers.i AS renamed_col FROM integers)
+----
+1
+
+# multiple renames
+query II
+SELECT r1, r2 FROM (SELECT * RENAME (integers.i AS r1, j AS r2) FROM integers)
+----
+1	2
+
+# trailing comma
+query II
+SELECT r1, r2 FROM (SELECT * RENAME (integers.i AS r1, j AS r2,) FROM integers)
+----
+1	2
+
+# exclude and rename to that same column
+query I
+SELECT i FROM (SELECT * EXCLUDE (i) RENAME (j AS i) FROM integers)
+----
+2
+
+# struct
+query I
+SELECT r FROM (SELECT struct.* RENAME (i AS r) FROM (SELECT {'i': 42} AS struct))
+----
+42
+
+# using
+query I
+SELECT new_col FROM (SELECT * RENAME (i AS new_col) FROM integers i1 JOIN integers i2 USING (i))
+----
+1
+
+query II
+SELECT new_col, new_col2 FROM (SELECT * RENAME (i1.i AS new_col, i1.j AS new_col2) FROM integers i1 JOIN integers i2 USING (i))
+----
+1	2
+
+query I
+SELECT new_col FROM (SELECT * RENAME (i AS new_col) FROM integers i1 LEFT JOIN integers i2 USING (i))
+----
+1
+
+query I
+SELECT new_col FROM (SELECT * RENAME (i1.i AS new_col) FROM integers i1 FULL OUTER JOIN integers i2 USING (i))
+----
+1
+
+# conflict with exclude
+statement error
+SELECT * EXCLUDE (i) RENAME (i AS renamed_col) FROM integers
+----
+Column "i" cannot occur in both EXCLUDE and RENAME list
+
+# conflict with replace
+statement error
+SELECT * REPLACE (i + 1 AS i) RENAME (i AS renamed_col) FROM integers
+----
+cannot occur in both REPLACE and RENAME list
diff --git a/test/sql/sample/test_sample.test_slow b/test/sql/sample/test_sample.test_slow
index e56ebc09eacb..57a41394ea41 100644
--- a/test/sql/sample/test_sample.test_slow
+++ b/test/sql/sample/test_sample.test_slow
@@ -218,10 +218,10 @@ select * from integers using sample 10000%;
 query I
 select i from integers using sample (1 rows) repeatable (0);
 ----
-130
+152
 
 query I
 select i from integers using sample reservoir(1%) repeatable (0);
 ----
-83
-21
+51
+78
diff --git a/test/sql/select/test_multi_column_reference.test b/test/sql/select/test_multi_column_reference.test
index 3feb30af18b0..5fda1670450f 100644
--- a/test/sql/select/test_multi_column_reference.test
+++ b/test/sql/select/test_multi_column_reference.test
@@ -142,7 +142,6 @@ statement ok
 DROP SCHEMA t CASCADE
 
 # test multiple tables with the same name but a different schema
-# we don't allow this (duplicate alias in query)
 statement ok
 CREATE SCHEMA s1
 
@@ -155,9 +154,25 @@ CREATE TABLE s1.t1 AS SELECT 42 t
 statement ok
 CREATE TABLE s2.t1 AS SELECT 84 t
 
-statement error
+query I
 SELECT s1.t1.t FROM s1.t1, s2.t1
 ----
+42
+
+query I
+SELECT * EXCLUDE (s1.t1.t) FROM s1.t1, s2.t1
+----
+84
+
+query I
+SELECT * EXCLUDE (S1.T1.T) FROM s1.t1, s2.t1
+----
+84
+
+query I
+SELECT s2.t1.t FROM s1.t1, s2.t1
+----
+84
 
 # test various failures
 statement error
diff --git a/test/sql/select/test_select_alias_prefix_colon.test b/test/sql/select/test_select_alias_prefix_colon.test
new file mode 100644
index 000000000000..f3a7d3093515
--- /dev/null
+++ b/test/sql/select/test_select_alias_prefix_colon.test
@@ -0,0 +1,114 @@
+# name: test/sql/select/test_select_alias_prefix_colon.test
+# description: Test selecting a view through a qualified reference
+# group: [select]
+
+statement ok
+PRAGMA enable_verification
+
+query I
+SELECT j : 42;
+----
+42
+
+query I
+select column_name from (describe SELECT j : 42)
+----
+j
+
+query I
+SELECT "j" : 42;
+----
+42
+
+statement error
+SELECT 'j': 42
+----
+Parser Error
+
+query I
+SELECT "hel lo" : 42;
+----
+42
+
+query I
+select column_name from (describe SELECT "hel lo" : 42)
+----
+hel lo
+
+query III
+SELECT j1 : 42, 42 AS j2, 42 j3;
+----
+42	42	42
+
+statement ok
+CREATE TABLE a (i INTEGER);
+
+statement ok
+INSERT INTO a VALUES (42);
+
+query I
+SELECT j : i FROM a
+----
+42
+
+query I
+SELECT "j" : "i" FROM a
+----
+42
+
+query I
+SELECT * FROM b : a
+----
+42
+
+query I
+SELECT * FROM "b" : a
+----
+42
+
+query I
+SELECT i FROM b : a
+----
+42
+
+query I
+SELECT b.i FROM b : a
+----
+42
+
+statement error
+SELECT a.i FROM b : a
+----
+Binder Error: Referenced table "a" not found!
+Candidate tables: "b"
+
+statement error
+SELECT a : 42 AS b
+----
+syntax error
+
+query I
+from my_wonderful_values : (values(42))
+----
+42
+
+query I
+from 'my_wonderful_values' : (values(42))
+----
+42
+
+query I
+from "my_wonderful_values" : (values(42))
+----
+42
+
+query I
+from r : range(1)
+----
+0
+
+query I
+from "r" : range(1)
+----
+0
+
diff --git a/test/sql/setops/test_union_all_by_name.test b/test/sql/setops/test_union_all_by_name.test
index fa311692cfb6..569f5637af16 100644
--- a/test/sql/setops/test_union_all_by_name.test
+++ b/test/sql/setops/test_union_all_by_name.test
@@ -197,7 +197,7 @@ NULL	4	4
 statement error
 SELECT x, x FROM t1 UNION ALL BY NAME SELECT y FROM t2;
 ----
-<REGEX>:Binder Error:.*doesn't support same name.*
+<REGEX>:Binder Error:.*doesn't support duplicate names.*
 
 query III
 SELECT x, x as a FROM t1 UNION ALL BY NAME SELECT y FROM t2;
diff --git a/test/sql/settings/allowed_directories.test b/test/sql/settings/allowed_directories.test
new file mode 100644
index 000000000000..de8423308b8e
--- /dev/null
+++ b/test/sql/settings/allowed_directories.test
@@ -0,0 +1,197 @@
+# name: test/sql/settings/allowed_directories.test
+# description: Test allowed_directories setting together with enable_external_access = false
+# group: [settings]
+
+require skip_reload
+
+# enable_external_access = false disables extension loading
+require no_extension_autoloading
+
+require parquet
+
+require json
+
+# we can set allowed_directories as much as we want
+statement ok
+SET allowed_directories=['data/csv/glob']
+
+statement ok
+RESET allowed_directories
+
+statement ok
+SET allowed_directories=['data/csv/glob', 'data/parquet-testing/glob', 'data/json', '__TEST_DIR__']
+
+statement ok
+SET enable_external_access=false
+
+# ...until enable_external_access is false
+statement error
+RESET allowed_directories
+----
+Cannot change allowed_directories when enable_external_access is disabled
+
+statement error
+SET allowed_directories=[]
+----
+Cannot change allowed_directories when enable_external_access is disabled
+
+# we can read CSV files from the allowed_directories
+query III
+SELECT * FROM 'data/csv/glob/f_1.csv'
+----
+1	alice	alice@email.com
+2	eve	eve@email.com
+3r	bob	NULL
+
+# also within contained directories
+query I
+SELECT * FROM 'data/csv/glob/a1/a1.csv'
+----
+2019-06-05
+2019-06-15
+2019-06-25
+
+# we can also use "..", as long as we remain inside our directory
+query III
+SELECT * FROM 'data/csv/glob/a1/../f_1.csv'
+----
+1	alice	alice@email.com
+2	eve	eve@email.com
+3r	bob	NULL
+
+# and we can use ./
+query III
+SELECT * FROM 'data/csv/glob/./f_1.csv'
+----
+1	alice	alice@email.com
+2	eve	eve@email.com
+3r	bob	NULL
+
+# we cannot read files that are not in the allowed directories
+statement error
+SELECT * FROM 'data/csv/all_quotes.csv'
+----
+Permission Error
+
+# also not through usage of ".."
+statement error
+SELECT * FROM 'data/csv/glob/../all_quotes.csv'
+----
+Permission Error
+
+# //.. edge case
+statement error
+SELECT * FROM 'data/csv/glob//../all_quotes.csv'
+----
+Permission Error
+
+statement error
+SELECT * FROM 'data/csv/glob/a1/../../all_quotes.csv'
+----
+Permission Error
+
+# we can also sniff csv files
+statement ok
+SELECT * FROM sniff_csv('data/csv/glob/f_1.csv')
+
+# but not outside of allowed directories
+statement error
+SELECT * FROM sniff_csv('data/csv/all_quotes.csv')
+----
+Permission Error
+
+# we can also glob allowed directories
+query I
+SELECT replace(fname, '\', '/') as fname FROM glob('data/csv/glob/*.csv') t(fname)
+----
+data/csv/glob/f_1.csv
+data/csv/glob/f_2.csv
+data/csv/glob/f_3.csv
+
+statement error
+SELECT * FROM glob('data/csv/**.csv')
+----
+Permission Error
+
+# we can write to our test dir
+statement ok
+COPY (SELECT 42 i) TO '__TEST_DIR__/permission_test.csv' (FORMAT csv)
+
+statement ok
+CREATE TABLE integers(i INT);
+
+statement ok
+COPY integers FROM '__TEST_DIR__/permission_test.csv'
+
+query I
+FROM integers
+----
+42
+
+# but not to other directories
+statement error
+COPY (SELECT 42 i) TO 'permission_test.csv' (FORMAT csv)
+----
+Permission Error
+
+statement error
+COPY integers FROM 'permission_test.csv'
+----
+Permission Error
+
+# we can attach databases in allowed directories
+statement ok
+ATTACH '__TEST_DIR__/attached_dir.db' AS a1
+
+statement ok
+CREATE TABLE a1.integers(i INTEGER);
+
+# but not in other directories
+statement error
+ATTACH 'test.db'
+----
+Permission Error
+
+# we cannot load or install extensions with enable_external access
+statement error
+LOAD my_ext
+----
+Permission Error
+
+statement error
+INSTALL my_ext
+----
+Permission Error
+
+# export/import also work with allowed_directories
+statement ok
+EXPORT DATABASE a1 TO '__TEST_DIR__/export_test'
+
+statement error
+EXPORT DATABASE a1 TO 'export_test'
+----
+Permission Error
+
+statement ok
+IMPORT DATABASE '__TEST_DIR__/export_test'
+
+statement error
+IMPORT DATABASE 'export_test'
+----
+Permission Error
+
+# we can read parquet/json files
+query II
+SELECT * FROM 'data/parquet-testing/glob/t1.parquet'
+----
+1	a
+
+statement error
+SELECT * FROM 'data/parquet-testing/aws2.parquet'
+----
+Permission Error
+
+query II
+SELECT * FROM 'data/parquet-testing/glob/t1.parquet'
+----
+1	a
diff --git a/test/sql/settings/allowed_paths.test b/test/sql/settings/allowed_paths.test
new file mode 100644
index 000000000000..c50a1d3b8116
--- /dev/null
+++ b/test/sql/settings/allowed_paths.test
@@ -0,0 +1,57 @@
+# name: test/sql/settings/allowed_paths.test
+# description: Test allowed_paths setting together with enable_external_access = false
+# group: [settings]
+
+require skip_reload
+
+# enable_external_access = false disables extension loading
+require no_extension_autoloading
+
+# we can set allowed_directories as much as we want
+statement ok
+SET allowed_paths=['data/csv/glob/f_1.csv']
+
+statement ok
+RESET allowed_paths
+
+statement ok
+SET allowed_paths=['data/csv/glob/f_1.csv', '__TEST_DIR__/allowed_file.csv']
+
+statement ok
+SET enable_external_access=false
+
+# ...until enable_external_access is false
+statement error
+RESET allowed_paths
+----
+Cannot change allowed_paths when enable_external_access is disabled
+
+statement error
+SET allowed_paths=[]
+----
+Cannot change allowed_paths when enable_external_access is disabled
+
+# we can read our allowed files
+query III
+SELECT * FROM 'data/csv/glob/f_1.csv'
+----
+1	alice	alice@email.com
+2	eve	eve@email.com
+3r	bob	NULL
+
+# but not files that are not allowed
+statement error
+SELECT * FROM 'data/csv/glob/a1/a1.csv'
+----
+Permission Error
+
+# we can also write to our allowed file
+statement ok
+COPY (SELECT 42 i) TO '__TEST_DIR__/allowed_file.csv'
+
+# but not to not-allowed files
+statement error
+COPY (SELECT 42 i) TO '__TEST_DIR__/not_allowed_file.csv'
+----
+Permission Error
+
diff --git a/test/sql/settings/setting_exhaustive.test b/test/sql/settings/setting_exhaustive.test
index 38bee2e53130..66379ec3040f 100644
--- a/test/sql/settings/setting_exhaustive.test
+++ b/test/sql/settings/setting_exhaustive.test
@@ -6,7 +6,7 @@
 statement error
 SET debug_window_mode='unknown';
 ----
-<REGEX>:Parser Error.*Unrecognized option for PRAGMA.*
+<REGEX>:.*unrecognized value.*
 
 # default_order
 foreach default_order ASC DESC
@@ -75,4 +75,4 @@ endloop
 statement error
 SET explain_output='unknown';
 ----
-<REGEX>:Parser Error.*Unrecognized output type.*
\ No newline at end of file
+<REGEX>:.*unrecognized value.*
\ No newline at end of file
diff --git a/test/sql/show_select/show_quote_identifier.test b/test/sql/show_select/show_quote_identifier.test
new file mode 100644
index 000000000000..f5300d53d3e3
--- /dev/null
+++ b/test/sql/show_select/show_quote_identifier.test
@@ -0,0 +1,14 @@
+# name: test/sql/show_select/show_quote_identifier.test
+# description: Test show with a quote in an identifier
+# group: [show_select]
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE "s1"".tbl"(i INT);
+
+query II
+SELECT column_name, column_type FROM (DESCRIBE "s1"".tbl")
+----
+i	INTEGER
diff --git a/test/sql/show_select/show_select_constraints.test b/test/sql/show_select/show_select_constraints.test
new file mode 100644
index 000000000000..162f484bf0a3
--- /dev/null
+++ b/test/sql/show_select/show_select_constraints.test
@@ -0,0 +1,84 @@
+# name: test/sql/show_select/show_select_constraints.test
+# description: Test show select constraints
+# group: [show_select]
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE integers(c1 INTEGER PRIMARY KEY, c2 INT NOT NULL, c3 INT DEFAULT 42, c4 INT UNIQUE, c5 INT);
+
+statement ok
+INSERT INTO integers VALUES (42, 42, 42, 42, 42)
+
+statement ok
+INSERT INTO integers VALUES (84, 84, 84, 84, 84)
+
+# show on the table directly vs show is identical
+query IIIIII nosort describe_result
+DESCRIBE SELECT * FROM integers
+----
+
+query IIIIII nosort describe_result
+DESCRIBE integers
+----
+
+# projection
+query II
+SELECT column_name, key FROM (DESCRIBE SELECT c4, c1 FROM integers)
+----
+c4	UNI
+c1	PRI
+
+# filter
+query II
+SELECT column_name, key FROM (DESCRIBE SELECT c4 FROM integers WHERE c1=42)
+----
+c4	UNI
+
+# limit
+query II
+SELECT column_name, key FROM (DESCRIBE SELECT c4 FROM integers LIMIT 5)
+----
+c4	UNI
+
+# top n
+query II
+SELECT column_name, key FROM (DESCRIBE SELECT c4 FROM integers ORDER BY c1 LIMIT 5)
+----
+c4	UNI
+
+# sample
+query II
+SELECT column_name, key FROM (DESCRIBE SELECT c4 FROM integers USING SAMPLE 5)
+----
+c4	UNI
+
+# cross product
+query II
+SELECT column_name, key FROM (DESCRIBE SELECT c4 FROM integers, (SELECT 84))
+----
+c4	UNI
+
+# join
+query II
+SELECT column_name, key FROM (DESCRIBE SELECT t.c1, integers.c1 FROM integers JOIN (SELECT 84 c1) t USING (c1))
+----
+c1	NULL
+c1	PRI
+
+# views
+statement ok
+CREATE VIEW my_view AS SELECT * FROM integers
+
+query II
+SELECT column_name, key FROM (DESCRIBE SELECT c4, c1 FROM my_view)
+----
+c4	UNI
+c1	PRI
+
+# constraint information does not persist through expressions
+query II
+SELECT column_name, key FROM (DESCRIBE SELECT c4 + 1 AS expr FROM integers LIMIT 5)
+----
+expr	NULL
diff --git a/test/sql/storage/optimistic_write/optimistic_write_custom_row_group_size.test_slow b/test/sql/storage/optimistic_write/optimistic_write_custom_row_group_size.test_slow
new file mode 100644
index 000000000000..889875b733a7
--- /dev/null
+++ b/test/sql/storage/optimistic_write/optimistic_write_custom_row_group_size.test_slow
@@ -0,0 +1,56 @@
+# name: test/sql/storage/optimistic_write/optimistic_write_custom_row_group_size.test_slow
+# description: Test large appends within individual transactions
+# group: [optimistic_write]
+
+statement ok
+ATTACH '__TEST_DIR__/optimistic_write_custom_row_group_size.db' AS attached_db (ROW_GROUP_SIZE 204800)
+
+statement ok
+USE attached_db
+
+statement ok
+SET debug_skip_checkpoint_on_commit=true
+
+statement ok
+PRAGMA disable_checkpoint_on_shutdown
+
+statement ok
+CREATE OR REPLACE TABLE test (a INTEGER);
+
+statement ok
+INSERT INTO test SELECT * FROM range(1000000)
+
+query I
+SELECT SUM(a) FROM test
+----
+499999500000
+
+# re-attach, this time with a different row group size
+statement ok
+ATTACH ':memory:' AS mem
+
+statement ok
+USE mem
+
+statement ok
+DETACH attached_db
+
+statement ok
+ATTACH '__TEST_DIR__/optimistic_write_custom_row_group_size.db' AS attached_db (ROW_GROUP_SIZE 2048)
+
+statement ok
+USE attached_db
+
+query I
+SELECT SUM(a) FROM test
+----
+499999500000
+
+statement ok
+INSERT INTO test SELECT * FROM range(1000000)
+
+query I
+SELECT SUM(a) FROM test
+----
+999999000000
+
diff --git a/test/sql/storage/parallel/custom_row_group_size.test_slow b/test/sql/storage/parallel/custom_row_group_size.test_slow
new file mode 100644
index 000000000000..8925f9cdfb48
--- /dev/null
+++ b/test/sql/storage/parallel/custom_row_group_size.test_slow
@@ -0,0 +1,165 @@
+# name: test/sql/storage/parallel/custom_row_group_size.test_slow
+# description: Test batch insert with small batches
+# group: [parallel]
+
+require parquet
+
+statement ok
+ATTACH '__TEST_DIR__/custom_row_group_size.db' AS custom_row_group_size (ROW_GROUP_SIZE 204800)
+
+statement ok
+USE custom_row_group_size
+
+statement ok
+COPY (FROM range(100000) tbl(i)) TO '__TEST_DIR__/mix_batches_small.parquet' (ROW_GROUP_SIZE 5000)
+
+statement ok
+COPY (FROM range(100000, 400000) tbl(i)) TO '__TEST_DIR__/mix_batches_large.parquet' (ROW_GROUP_SIZE 200000)
+
+statement ok
+COPY (FROM range(400000, 700000) tbl(i)) TO '__TEST_DIR__/mix_batches_odd.parquet' (ROW_GROUP_SIZE 999)
+
+statement ok
+COPY (FROM range(700000, 1000000) tbl(i)) TO '__TEST_DIR__/mix_batches_odd_again.parquet' (ROW_GROUP_SIZE 99979)
+
+
+# create views that read the batches
+statement ok
+CREATE VIEW v1 AS SELECT * FROM parquet_scan(['__TEST_DIR__/mix_batches_small.parquet', '__TEST_DIR__/mix_batches_large.parquet', '__TEST_DIR__/mix_batches_odd.parquet',  '__TEST_DIR__/mix_batches_odd_again.parquet'])
+
+statement ok
+CREATE VIEW v2 AS FROM v1 WHERE (i//10000)%2=0;
+
+statement ok
+CREATE VIEW v3 AS FROM v1 WHERE (i//10000)%2=0 OR (i>200000 AND i < 400000) OR (i>600000 AND i < 800000);
+
+query I
+CREATE TABLE integers AS FROM v1;
+----
+1000000
+
+# verify we are actually creating larger row groups
+query I
+SELECT MAX(count) FROM pragma_storage_info('integers')
+----
+204800
+
+# we have a total of 1M values - this should not be more than 10 row groups (ideally it is 5)
+query I
+select count(distinct row_group_id) < 10 from pragma_storage_info('integers');
+----
+true
+
+query IIIII
+SELECT SUM(i), MIN(i), MAX(i), COUNT(i), COUNT(*) FROM v1
+----
+499999500000	0	999999	1000000	1000000
+
+query IIIII
+SELECT SUM(i), MIN(i), MAX(i), COUNT(i), COUNT(*) FROM integers
+----
+499999500000	0	999999	1000000	1000000
+
+# now do the same, but filter out half of the values
+query I
+CREATE TABLE integers2 AS FROM v2
+----
+500000
+
+# also test deletions
+query I
+DELETE FROM integers WHERE (i//10000)%2<>0;
+----
+500000
+
+query IIIII
+SELECT SUM(i), MIN(i), MAX(i), COUNT(i), COUNT(*) FROM integers
+----
+247499750000	0	989999	500000	500000
+
+query IIIII
+SELECT SUM(i), MIN(i), MAX(i), COUNT(i), COUNT(*) FROM integers2
+----
+247499750000	0	989999	500000	500000
+
+# test updates
+query I
+UPDATE integers SET i=i+1 WHERE i%2=0
+----
+250000
+
+query IIIII
+SELECT SUM(i), MIN(i), MAX(i), COUNT(i), COUNT(*) FROM integers
+----
+247500000000	1	989999	500000	500000
+
+query I
+CREATE TABLE integers3 AS FROM v3
+----
+700000
+
+# verify that we are not consuming an unnecessarily giant amount of blocks
+# we have a total of 750K values - this should not be more than 10 row groups (ideally it is 4)
+query I
+select count(distinct row_group_id) < 10 from pragma_storage_info('integers3');
+----
+true
+
+query IIIII
+SELECT SUM(i), MIN(i), MAX(i), COUNT(i), COUNT(*) FROM integers3
+----
+348499650000	0	989999	700000	700000
+
+# non-batch insert
+statement ok
+SET preserve_insertion_order = false
+
+statement ok
+CREATE TABLE integers4 AS FROM integers
+
+query IIIII
+SELECT SUM(i), MIN(i), MAX(i), COUNT(i), COUNT(*) FROM integers4
+----
+247500000000	1	989999	500000	500000
+
+# re-attach without the parameter
+statement ok
+ATTACH ':memory:' AS mem
+
+statement ok
+USE mem
+
+statement ok
+DETACH custom_row_group_size
+
+statement ok
+ATTACH '__TEST_DIR__/custom_row_group_size.db' AS custom_row_group_size
+
+statement ok
+USE custom_row_group_size
+
+query IIIII
+SELECT SUM(i), MIN(i), MAX(i), COUNT(i), COUNT(*) FROM integers
+----
+247500000000	1	989999	500000	500000
+
+query IIIII
+SELECT SUM(i), MIN(i), MAX(i), COUNT(i), COUNT(*) FROM integers2
+----
+247499750000	0	989999	500000	500000
+
+query IIIII
+SELECT SUM(i), MIN(i), MAX(i), COUNT(i), COUNT(*) FROM integers3
+----
+348499650000	0	989999	700000	700000
+
+# invalid row group size parameters
+statement error
+ATTACH '__TEST_DIR__/custom_row_group_size_xx.db' AS custom_row_group_size_x1 (ROW_GROUP_SIZE 0)
+----
+row group size must be bigger than 0
+
+statement error
+ATTACH '__TEST_DIR__/custom_row_group_size_xx.db' AS custom_row_group_size_x2 (ROW_GROUP_SIZE 77)
+----
+row group size must be divisible by the vector size
diff --git a/test/sql/storage/parallel/tiny_row_group_size.test_slow b/test/sql/storage/parallel/tiny_row_group_size.test_slow
new file mode 100644
index 000000000000..61d44612ed77
--- /dev/null
+++ b/test/sql/storage/parallel/tiny_row_group_size.test_slow
@@ -0,0 +1,42 @@
+# name: test/sql/storage/parallel/tiny_row_group_size.test_slow
+# description: Test tiny row group size
+# group: [parallel]
+
+statement ok
+ATTACH '__TEST_DIR__/tiny_row_group_size.db' (ROW_GROUP_SIZE 2048)
+
+statement ok
+USE tiny_row_group_size
+
+statement ok
+CREATE TABLE t AS FROM range(1000000) t(i)
+
+query IIIII
+SELECT SUM(i), MIN(i), MAX(i), COUNT(i), COUNT(*) FROM t
+----
+499999500000	0	999999	1000000	1000000
+
+# we have a total of 1M values, ideally this is 488 row groups
+query II
+select count(distinct row_group_id) < 1000, max(count) from pragma_storage_info('t');
+----
+true	2048
+
+query I
+SELECT * FROM t OFFSET 77777 LIMIT 5
+----
+77777
+77778
+77779
+77780
+77781
+
+query II
+SELECT i, row_number() OVER () FROM t OFFSET 777776 LIMIT 5
+----
+777776	777777
+777777	777778
+777778	777779
+777779	777780
+777780	777781
+
diff --git a/test/sql/storage/reclaim_space/reclaim_space_lists.test_slow b/test/sql/storage/reclaim_space/reclaim_space_lists.test_slow
index e84f14265829..9bd93d9f7ba9 100644
--- a/test/sql/storage/reclaim_space/reclaim_space_lists.test_slow
+++ b/test/sql/storage/reclaim_space/reclaim_space_lists.test_slow
@@ -8,7 +8,7 @@ statement ok
 PRAGMA force_checkpoint;
 
 statement ok
-SET force_compression='uncompressed'
+SET force_compression='uncompressed';
 
 statement ok
 CREATE TABLE lists AS SELECT [i] l FROM range(10000000) tbl(i);
@@ -20,7 +20,7 @@ statement ok
 CHECKPOINT;
 
 query III
-SELECT MIN(l[1]), MAX(l[1]), COUNT(*) FROM lists
+SELECT MIN(l[1]), MAX(l[1]), COUNT(*) FROM lists;
 ----
 0	9999999	10000000
 
@@ -33,7 +33,7 @@ statement ok
 CREATE TABLE lists${i} AS SELECT [i] l FROM range(10000000) tbl(i);
 
 query III
-SELECT MIN(l[1]), MAX(l[1]), COUNT(*) FROM lists${i}
+SELECT MIN(l[1]), MAX(l[1]), COUNT(*) FROM lists${i};
 ----
 0	9999999	10000000
 
@@ -41,12 +41,12 @@ statement ok
 CHECKPOINT;
 
 statement ok
-DROP TABLE lists${i}
+DROP TABLE lists${i};
 
 statement ok
-CHECKPOINT
+CHECKPOINT;
 
 query I nosort expected_blocks
-select round(total_blocks / 100.0) from pragma_database_size();
+SELECT round(total_blocks / 100.0) FROM pragma_database_size();
 
 endloop
diff --git a/test/sql/storage/storage_enable_external_access.test b/test/sql/storage/storage_enable_external_access.test
new file mode 100644
index 000000000000..a366965dda60
--- /dev/null
+++ b/test/sql/storage/storage_enable_external_access.test
@@ -0,0 +1,27 @@
+# name: test/sql/storage/storage_enable_external_access.test
+# description: enable_external_access with persistent databases
+# group: [storage]
+
+require skip_reload
+
+# load the DB from disk
+load __TEST_DIR__/external_access_test.db
+
+statement ok
+SET enable_external_access=false
+
+# verify that we can still modify the currently attached database if enable_external_access is disabled
+statement ok
+CREATE TABLE test (a INTEGER PRIMARY KEY, b INTEGER);
+
+statement ok
+INSERT INTO test VALUES (11, 22), (13, 22);
+
+query II
+SELECT * FROM test ORDER BY a
+----
+11	22
+13	22
+
+statement ok
+CHECKPOINT
diff --git a/test/sql/storage/store_group_order_all.test b/test/sql/storage/store_group_order_all.test
index 61cb846d77ff..de753c18efd4 100644
--- a/test/sql/storage/store_group_order_all.test
+++ b/test/sql/storage/store_group_order_all.test
@@ -12,20 +12,39 @@ statement ok
 PRAGMA wal_autocheckpoint='1TB';
 
 statement ok
-CREATE TABLE integers(g integer, i integer);
+CREATE TABLE integers(
+	g integer,
+	i integer
+);
 
 statement ok
-INSERT INTO integers values (0, 1), (0, 2), (1, 3), (1, NULL);
+INSERT INTO integers values
+	(0, 1),
+	(0, 2),
+	(1, 3),
+	(1, NULL);
 
 statement ok
-CREATE VIEW v1 AS SELECT g, i, g%2, SUM(i), SUM(g) FROM integers GROUP BY ALL ORDER BY ALL
+CREATE VIEW v1 AS SELECT
+	g,
+	i,
+	g%2,
+	SUM(i),
+	SUM(g)
+FROM integers GROUP BY ALL ORDER BY ALL;
 
 query IIIII nosort v1
 SELECT * FROM v1
 ----
 
 statement ok
-CREATE VIEW v2 AS SELECT g, i, g%2, SUM(i), SUM(g) FROM integers GROUP BY ALL ORDER BY ALL DESC NULLS LAST
+CREATE VIEW v2 AS SELECT
+	g,
+	i,
+	g%2,
+	SUM(i),
+	SUM(g)
+FROM integers GROUP BY ALL ORDER BY ALL DESC NULLS LAST;
 
 query IIIII nosort v2
 SELECT * FROM v2
diff --git a/test/sql/storage/temp_directory/max_swap_space_error.test b/test/sql/storage/temp_directory/max_swap_space_error.test
index 9249b246a54c..27464e0367e9 100644
--- a/test/sql/storage/temp_directory/max_swap_space_error.test
+++ b/test/sql/storage/temp_directory/max_swap_space_error.test
@@ -25,7 +25,7 @@ statement ok
 set max_temp_directory_size='0KiB'
 
 statement error
-CREATE OR REPLACE TABLE t2 AS SELECT * FROM range(1000000);
+CREATE OR REPLACE TABLE t2 AS SELECT random() FROM range(1000000);
 ----
 failed to offload data block
 
@@ -39,12 +39,12 @@ statement ok
 set max_temp_directory_size='256KiB'
 
 statement error
-CREATE OR REPLACE TABLE t2 AS SELECT * FROM range(1000000);
+CREATE OR REPLACE TABLE t2 AS SELECT random() FROM range(1000000);
 ----
 failed to offload data block
 
 statement error
-CREATE OR REPLACE TABLE t2 AS SELECT * FROM range(1000000);
+CREATE OR REPLACE TABLE t2 AS SELECT random() FROM range(1000000);
 ----
 failed to offload data block
 
@@ -52,9 +52,8 @@ query I
 select "size" from duckdb_temporary_files()
 ----
 
-# 6 blocks
 statement ok
-set max_temp_directory_size='1536KiB'
+set max_temp_directory_size='4MB'
 
 statement ok
 pragma threads=2;
@@ -62,38 +61,38 @@ pragma threads=2;
 statement ok
 set preserve_insertion_order=true;
 
-# This is 1600000 bytes of BIGINT data, which is roughly 6.1 blocks
+# This is 1600000 bytes of DOUBLE data, which is roughly 6.1 blocks
 # Because our memory limit is set at 1MiB (4 blocks) this works
 statement ok
-CREATE OR REPLACE TABLE t2 AS SELECT * FROM range(200000);
+CREATE OR REPLACE TABLE t2 AS SELECT random() FROM range(200000);
 
-# When we increase the size to 2400000 bytes of BIGINT data (9.1 blocks) this errors
+# When we increase the size to 2400000 bytes of DOUBLE data (9.1 blocks) this errors
 statement error
-CREATE OR REPLACE TABLE t2 AS SELECT * FROM range(300000);
+CREATE OR REPLACE TABLE t2 AS SELECT * FROM range(1000000);
 ----
 failed to offload data block
 
 query I
-SELECT "size" FROM duckdb_temporary_files();
+SELECT CASE WHEN sum("size") > 1000000 THEN true ELSE CONCAT('Expected size 1000000, but got ', sum("size"))::UNION(msg VARCHAR, b BOOLEAN) END FROM duckdb_temporary_files();
 ----
-1572864
+true
 
-# Lower the limit
+# Cannot lower the limit
 statement error
 set max_temp_directory_size='256KiB'
 ----
-failed to adjust the 'max_temp_directory_size', currently used space (1.5 MiB) exceeds the new limit (256.0 KiB)
+exceeds the new limit
 
-# Lower the limit
+# Cannot lower the limit
 statement error
 set max_temp_directory_size='256KiB'
 ----
-failed to adjust the 'max_temp_directory_size', currently used space (1.5 MiB) exceeds the new limit (256.0 KiB)
+exceeds the new limit
 
 query I
 select current_setting('max_temp_directory_size')
 ----
-1.5 MiB
+3.8 MiB
 
 statement ok
 set max_temp_directory_size='2550KiB'
diff --git a/test/sql/storage/temp_directory/temp_directory_enable_external_access.test b/test/sql/storage/temp_directory/temp_directory_enable_external_access.test
new file mode 100644
index 000000000000..f42158fedb4b
--- /dev/null
+++ b/test/sql/storage/temp_directory/temp_directory_enable_external_access.test
@@ -0,0 +1,28 @@
+# name: test/sql/storage/temp_directory/temp_directory_enable_external_access.test
+# group: [temp_directory]
+
+require block_size 262144
+
+statement ok
+SET temp_directory='__TEST_DIR__/test_temp_dir'
+
+statement ok
+SET memory_limit = '8MB';
+
+statement ok
+SET enable_external_access=false
+
+# cannot modify the temp directory when enable_external_access is set
+statement error
+SET temp_directory='__TEST_DIR__/new_temp_dir'
+----
+disabled by configuration
+
+statement error
+RESET temp_directory
+----
+disabled by configuration
+
+# 10M row table with 8B rows -> 80MB uncompressed - this requires the temp directory
+statement ok
+CREATE TEMPORARY TABLE tbl AS FROM range(10_000_000)
diff --git a/test/sql/storage/update/larger_than_memory_update.test_slow b/test/sql/storage/update/larger_than_memory_update.test_slow
new file mode 100644
index 000000000000..0acac4f5ce7b
--- /dev/null
+++ b/test/sql/storage/update/larger_than_memory_update.test_slow
@@ -0,0 +1,51 @@
+# name: test/sql/storage/update/larger_than_memory_update.test_slow
+# description: Test larger than memory updates
+# group: [update]
+
+load __TEST_DIR__/larger_than_memory_update.db
+
+statement ok
+CREATE TABLE integers AS FROM range(10000000) t(i);
+
+statement ok
+SET threads=1
+
+# 10M bigints is ~75MB uncompressed
+statement ok
+SET memory_limit='8MB';
+
+query II
+SELECT COUNT(*), SUM(i) FROM integers
+----
+10000000	49999995000000
+
+query I
+UPDATE integers SET i=i+1;
+----
+10000000
+
+query II
+SELECT COUNT(*), SUM(i) FROM integers
+----
+10000000	50000005000000
+
+query I
+UPDATE integers SET i=i+1 WHERE i%2=0;
+----
+5000000
+
+query II
+SELECT COUNT(*), SUM(i) FROM integers
+----
+10000000	50000010000000
+
+statement ok
+BEGIN
+
+query I
+UPDATE integers SET i=i+1;
+----
+10000000
+
+statement ok
+ROLLBACK
diff --git a/test/sql/storage/update/larger_than_memory_update_transactions.test_slow b/test/sql/storage/update/larger_than_memory_update_transactions.test_slow
new file mode 100644
index 000000000000..60796a7fe7c0
--- /dev/null
+++ b/test/sql/storage/update/larger_than_memory_update_transactions.test_slow
@@ -0,0 +1,58 @@
+# name: test/sql/storage/update/larger_than_memory_update_transactions.test_slow
+# description: Test larger than memory updates with transactions
+# group: [update]
+
+require skip_reload
+
+load __TEST_DIR__/larger_than_memory_update_transactions.db
+
+statement ok
+SET threads=1
+
+statement ok
+CREATE TABLE integers AS FROM range(10000000) t(i);
+
+# 10M bigints is ~75MB uncompressed
+statement ok
+SET memory_limit='16MB';
+
+statement ok con1
+BEGIN
+
+query II con1
+SELECT COUNT(*), SUM(i) FROM integers
+----
+10000000	49999995000000
+
+query I
+UPDATE integers SET i=i+1;
+----
+10000000
+
+query II con1
+SELECT COUNT(*), SUM(i) FROM integers
+----
+10000000	49999995000000
+
+query II
+SELECT COUNT(*), SUM(i) FROM integers
+----
+10000000	50000005000000
+
+query I
+UPDATE integers SET i=i+1 WHERE i%2=0;
+----
+5000000
+
+query II con1
+SELECT COUNT(*), SUM(i) FROM integers
+----
+10000000	49999995000000
+
+query II
+SELECT COUNT(*), SUM(i) FROM integers
+----
+10000000	50000010000000
+
+statement ok con1
+ROLLBACK
diff --git a/test/sql/storage/wal/wal_create_index.test b/test/sql/storage/wal/wal_create_index.test
index 05dcf837f8ee..04299e8fc4b8 100644
--- a/test/sql/storage/wal/wal_create_index.test
+++ b/test/sql/storage/wal/wal_create_index.test
@@ -2,7 +2,6 @@
 # description: Test serialization of index to WAL
 # group: [wal]
 
-# load the DB from disk
 load __TEST_DIR__/test_wal_create_index.db
 
 statement ok
diff --git a/test/sql/storage/wal/wal_store_alter_type.test b/test/sql/storage/wal/wal_store_alter_type.test
index 2b851f3e3779..2b7676e1680e 100644
--- a/test/sql/storage/wal/wal_store_alter_type.test
+++ b/test/sql/storage/wal/wal_store_alter_type.test
@@ -4,7 +4,6 @@
 
 require skip_reload
 
-
 load __TEST_DIR__/test_store_alter_type.db
 
 statement ok
diff --git a/test/sql/table_function/duckdb_functions.test_slow b/test/sql/table_function/duckdb_functions.test_slow
index 04b96d276266..386cc0d9ee8e 100644
--- a/test/sql/table_function/duckdb_functions.test_slow
+++ b/test/sql/table_function/duckdb_functions.test_slow
@@ -8,6 +8,10 @@ PRAGMA enable_verification
 statement ok
 SELECT * FROM duckdb_functions();
 
+query I
+select function_name from duckdb_functions() where not internal order by 1;
+----
+
 statement ok
 CREATE MACRO add_default1(a := 3, b := 5) AS a + b
 
@@ -20,6 +24,9 @@ SELECT * FROM duckdb_functions();
 statement ok
 SELECT * FROM duckdb_functions() WHERE function_type='table';
 
+statement ok
+SELECT sqrt(4)
+
 query I
 select distinct function_name from duckdb_functions() where function_name='sqrt';
 ----
diff --git a/test/sql/table_function/duckdb_memory.test b/test/sql/table_function/duckdb_memory.test
index a88acdf6b134..c068252569ec 100644
--- a/test/sql/table_function/duckdb_memory.test
+++ b/test/sql/table_function/duckdb_memory.test
@@ -23,11 +23,11 @@ endloop
 # These should both report only 140mb is used
 
 query I
-select temporary_storage_bytes < 150_000_000 from duckdb_memory() where tag = 'IN_MEMORY_TABLE';
+select sum(temporary_storage_bytes) < 150_000_000 from duckdb_memory() where tag = 'IN_MEMORY_TABLE';
 ----
 true
 
 query I
-select size < 150_000_000 FROM duckdb_temporary_files();
+select sum(size) < 150_000_000 FROM duckdb_temporary_files();
 ----
 true
diff --git a/test/sql/topn/test_top_n.test b/test/sql/topn/test_top_n.test
index c1ae67f42154..a2eea03f9b75 100644
--- a/test/sql/topn/test_top_n.test
+++ b/test/sql/topn/test_top_n.test
@@ -24,6 +24,21 @@ SELECT b FROM test ORDER BY b LIMIT 1 OFFSET 1;
 ----
 7
 
+# Top N optimization: Limit greater than number of rows
+query I
+SELECT b FROM test ORDER BY b LIMIT 10
+----
+2
+7
+22
+
+query I
+SELECT b FROM test ORDER BY b DESC LIMIT 10
+----
+22
+7
+2
+
 # Top N optimization: Limit greater than number of rows
 query I
 SELECT b FROM test ORDER BY b LIMIT 10 OFFSET 1;
diff --git a/test/sql/tpch/tpch_hive_partition.test_slow b/test/sql/tpch/tpch_hive_partition.test_slow
new file mode 100644
index 000000000000..f706ebaa7bca
--- /dev/null
+++ b/test/sql/tpch/tpch_hive_partition.test_slow
@@ -0,0 +1,52 @@
+# name: test/sql/tpch/tpch_hive_partition.test_slow
+# description: Test TPC-H SF1 over hive partitions
+# group: [tpch]
+
+require parquet
+
+require tpch
+
+statement ok
+CALL dbgen(sf=1);
+
+statement ok
+COPY lineitem TO '__TEST_DIR__/lineitem_aggregate_partitioned' (FORMAT parquet, PARTITION_BY (l_returnflag, l_linestatus));
+
+statement ok
+DROP TABLE lineitem
+
+statement ok
+CREATE VIEW lineitem AS FROM '__TEST_DIR__/lineitem_aggregate_partitioned/**/*.parquet'
+
+query I
+PRAGMA tpch(1)
+----
+<FILE>:extension/tpch/dbgen/answers/sf1/q01.csv
+
+query III
+SELECT COUNT(*), SUM(l_extendedprice), l_returnflag
+FROM lineitem
+GROUP BY ALL
+ORDER BY ALL
+----
+1478493	56586554400.73	A
+1478870	56568041380.90	R
+3043852	116422715119.57	N
+
+query III
+SELECT COUNT(*), SUM(l_extendedprice), l_returnflag
+FROM lineitem
+WHERE l_linestatus='O'
+GROUP BY ALL
+ORDER BY ALL
+----
+3004998	114935210409.19	N
+
+query III
+SELECT COUNT(*), SUM(l_extendedprice), l_linestatus
+FROM lineitem
+GROUP BY ALL
+ORDER BY ALL
+----
+2996217	114642100492.01	F
+3004998	114935210409.19	O
diff --git a/test/sql/types/nested/map/map_from_entries/data_types.test b/test/sql/types/nested/map/map_from_entries/data_types.test
index 116b38a942f3..1c1db4b6dbc6 100644
--- a/test/sql/types/nested/map/map_from_entries/data_types.test
+++ b/test/sql/types/nested/map/map_from_entries/data_types.test
@@ -20,7 +20,7 @@ select * from string_key;
 query I
 select col['a'] from string_key;
 ----
-[x]
+x
 
 # Invalid
 statement error
@@ -40,7 +40,7 @@ select * from tinyint_key;
 query I
 select col[123] from tinyint_key;
 ----
-[x]
+x
 
 # Invalid
 statement error
@@ -60,7 +60,7 @@ select * from smallint_key;
 query I
 select col[123] from smallint_key;
 ----
-[x]
+x
 
 # Invalid
 statement error
@@ -80,7 +80,7 @@ select * from integer_key;
 query I
 select col[123] from integer_key;
 ----
-[x]
+x
 
 # Invalid
 statement error
@@ -100,7 +100,7 @@ select * from bigint_key;
 query I
 select col[123] from bigint_key;
 ----
-[x]
+x
 
 # Invalid
 statement error
@@ -120,7 +120,7 @@ select * from hugeint_key;
 query I
 select col[123] from hugeint_key;
 ----
-[x]
+x
 
 # Invalid
 statement error
@@ -140,7 +140,7 @@ select * from boolean_key;
 query I
 select col[True] from boolean_key;
 ----
-[x]
+x
 
 # Invalid
 statement error
@@ -160,7 +160,7 @@ select * from blob_key;
 query I
 select col['\xF0\x9F\xA6\x86'::BLOB] from blob_key;
 ----
-[x]
+x
 
 # Invalid
 statement error
@@ -180,7 +180,7 @@ select * from date_key;
 query I
 select col['1992-09-20'::DATE] from date_key;
 ----
-[x]
+x
 
 # Invalid
 statement error
@@ -200,7 +200,7 @@ select * from double_key;
 query I
 select col['12.3'::DOUBLE] from double_key;
 ----
-[x]
+x
 
 # Invalid
 statement error
@@ -220,7 +220,7 @@ select * from real_key;
 query I
 select col['12.3'::REAL] from real_key;
 ----
-[x]
+x
 
 # Invalid
 statement error
@@ -240,7 +240,7 @@ select * from decimal_key;
 query I
 select col['12.3'::DECIMAL] from decimal_key;
 ----
-[x]
+x
 
 # Invalid
 statement error
@@ -260,7 +260,7 @@ select * from interval_key;
 query I
 select col[('2022-01-02 01:00:00'::TIMESTAMP - '2022-01-01'::TIMESTAMP)] from interval_key;
 ----
-[x]
+x
 
 # Invalid
 statement error
@@ -280,7 +280,7 @@ select * from time_key;
 query I
 select col['12:30:00'::TIME] from time_key;
 ----
-[x]
+x
 
 # Invalid
 statement error
@@ -300,7 +300,7 @@ select * from timestamp_key;
 query I
 select col['1992-09-20 11:30:00'::TIMESTAMP] from timestamp_key;
 ----
-[x]
+x
 
 # Invalid
 statement error
@@ -320,7 +320,7 @@ select * from uuid_key;
 query I
 select col['eeccb8c5-9943-b2bb-bb5e-222f4e14b687'::UUID] from uuid_key;
 ----
-[x]
+x
 
 # Invalid
 statement error
diff --git a/test/sql/types/nested/map/test_map_subscript.test b/test/sql/types/nested/map/test_map_subscript.test
index 4aa03da877d2..5ab23d6a9942 100644
--- a/test/sql/types/nested/map/test_map_subscript.test
+++ b/test/sql/types/nested/map/test_map_subscript.test
@@ -9,13 +9,13 @@ pragma enable_verification
 query I
 select m[1] from (select MAP(LIST_VALUE(1, 2, 3, 4),LIST_VALUE(10, 9, 8, 7)) as m) as T
 ----
-[10]
+10
 
 #Element not present on MAP
 query I
 select m[0] from (select MAP(LIST_VALUE(1, 2, 3, 4,5),LIST_VALUE(10, 9, 8, 7,11)) as m) as T
 ----
-[]
+NULL
 
 # Keys of a MAP can not be NULL
 statement error
@@ -26,7 +26,7 @@ select m[NULL] from (select MAP(LIST_VALUE(1, 2, 3, 4,5, NULL),LIST_VALUE(10, 9,
 query I
 select m[NULL] from (select MAP(LIST_VALUE(1, 2, 3, 4,5),LIST_VALUE(10, 9, 8, 7,11)) as m) as T
 ----
-[]
+NULL
 
 # Keys of a MAP have to be unique
 statement error
@@ -37,18 +37,18 @@ select m[2] from (select MAP(LIST_VALUE(1, 2, 3, 4,2),LIST_VALUE(10, 9, 8, 7,11)
 query I
 select m[2] from (select MAP(LIST_VALUE(),LIST_VALUE()) as m) as T
 ----
-[]
+NULL
 
 query I
 select m[2] from (select MAP() as m) as T
 ----
-[]
+NULL
 
 # Check autocast
 query I
 select m[2::TINYINT] from (select MAP(LIST_VALUE(1, 2, 3, 4,5),LIST_VALUE(10, 9, 8, 7,11)) as m) as T
 ----
-[9]
+9
 
 
 # Keys of a MAP have to be unique
@@ -59,48 +59,48 @@ select m['Jon Lajoie'] from (select MAP(LIST_VALUE('Jon Lajoie', 'Backstreet Boy
 query I
 select m['Spice Girls'] from (select MAP(LIST_VALUE('Jon Lajoie', 'Backstreet Boys', 'Tenacious D' ),LIST_VALUE(10,9,10)) as m) as T
 ----
-[]
+NULL
 
 query I
 select m[NULL] from (select MAP(LIST_VALUE('Jon Lajoie', 'Backstreet Boys', 'Tenacious D' ),LIST_VALUE(10,9,10)) as m) as T
 ----
-[]
+NULL
 
 query I
 select m['Tenacious D'] from (select MAP(LIST_VALUE('Jon Lajoie', 'Backstreet Boys', 'Tenacious D'),LIST_VALUE(10,9,1)) as m) as T
 ----
-[1]
+1
 
 query I
 select m[0] from (select MAP(LIST_VALUE('Jon Lajoie', 'Backstreet Boys', 'Tenacious D' ),LIST_VALUE(10,9,10)) as m) as T
 ----
-[]
+NULL
 
 query I
 select map_extract(m,1) from (select MAP(LIST_VALUE(1, 2, 3, 4),LIST_VALUE(10, 9, 8, 7)) as m) as T
 ----
-[10]
+10
 
 query I
 select map_extract(m,3) from (select MAP(LIST_VALUE(1, 2, 3, 4),LIST_VALUE(10, 9, 8, 7)) as m) as T
 ----
-[8]
+8
 
 query I
 select element_at(m,1) from (select MAP(LIST_VALUE(1, 2, 3, 4),LIST_VALUE(10, 9, 8, 7)) as m) as T
 ----
-[10]
+10
 
 query I
 select element_at(m,3) from (select MAP(LIST_VALUE(1, 2, 3, 4),LIST_VALUE(10, 9, 8, 7)) as m) as T
 ----
-[8]
+8
 
 #Map with larger-than-vector-size cardinalities
 query I
 select m[10] from (select MAP(lst,lst) as m from (SELECT LIST(i) as lst FROM range(10000) tbl(i)) as lst_tbl) as T
 ----
-[10]
+10
 
 #Duplicate keys in a map
 statement error
@@ -115,13 +115,13 @@ select m[NULL] from (select MAP(LIST_VALUE('Jon Lajoie', NULL, 'Tenacious D',NUL
 query I
 select m['Jon Lajoie'] from (select MAP(LIST_VALUE('Jon Lajoie', 'Tenacious D'),LIST_VALUE(10,1)) as m) as T
 ----
-[10]
+10
 
 #A map with only NULL values as key or value (or both)
 query I
 select m[10] from (select MAP(LIST_VALUE(10,9,1,11,13),LIST_VALUE(NULL, NULL, NULL,NULL,NULL)) as m) as T
 ----
-[NULL]
+NULL
 
 statement error
 select m[NULL] from (select MAP(LIST_VALUE(NULL, NULL, NULL,NULL,NULL ),LIST_VALUE(10,9,10,11,13)) as m) as T
@@ -146,7 +146,7 @@ select m from (select MAP(lsta,lstb) as m from (SELECT list(a) as lsta, list(b)
 query I
 select m[2] from (select MAP(lsta,lstb) as m from (SELECT list(a) as lsta, list(b) as lstb FROM ints where a < 4 and b > 1) as lst_tbl) as T
 ----
-[2]
+2
 
 #What about multiple rows each with a map, instead of just a single row?
 statement ok
@@ -163,10 +163,10 @@ select m from (select MAP(lsta,lstb) as m from (SELECT list(a) as lsta, list(b)
 query I
 select m[1] from (select MAP(lsta,lstb) as m from (SELECT list(a) as lsta, list(b) as lstb FROM ints group by b) as lst_tbl) as T ORDER BY ALL
 ----
-[]
-[]
-[1]
-[2]
+1
+2
+NULL
+NULL
 
 #What about multiple rows each with a map, with a selection vector on top of them (e.g. because of a WHERE clause)?
 statement ok
@@ -175,58 +175,58 @@ insert into ints values (1,4)
 query I
 select m[1] from (select MAP(lsta,lstb) as m from (SELECT list(a) as lsta, list(b) as lstb FROM ints where b <4 group by b) as lst_tbl) as T ORDER BY ALL
 ----
-[]
-[1]
-[2]
+1
+2
+NULL
 
 #map_extract directly on a constant map (SELECT map_extract(MAP(..)), i.e. no FROM clause).
 query I
 SELECT map_extract(MAP(LIST_VALUE(10,9,12,11,13),LIST_VALUE(10,9,10,11,13)),10)
 ----
-[10]
+10
 
 #Multiple constants
 query I
 select m[1] from (select MAP(list_value(1), list_value(2)) from range(5) tbl(i)) tbl(m);
 ----
-[2]
-[2]
-[2]
-[2]
-[2]
+2
+2
+2
+2
+2
 
 #Test dictionary vectors
 query III rowsort
-select grp, m, case when grp>1 then m[0] else list_value(null) end
+select grp, m, case when grp>1 then m[0] else null end
 from (select grp, MAP(lsta,lstb) as m
 from (SELECT a%4 as grp, list(a) as lsta, list(a) as lstb FROM range(7) tbl(a) group by grp) as lst_tbl) as T;
 ----
-0	{0=0, 4=4}	[NULL]
-1	{1=1, 5=5}	[NULL]
-2	{2=2, 6=6}	[]
-3	{3=3}	[]
+0	{0=0, 4=4}	NULL
+1	{1=1, 5=5}	NULL
+2	{2=2, 6=6}	NULL
+3	{3=3}	NULL
 
 query I
 select MAP_EXTRACT(MAP([],[]), NULL)
 ----
-[]
+NULL
 
 query I
 select MAP_EXTRACT(MAP(NULL, NULL), NULL)
 ----
-[]
+NULL
 
 query I
 select MAP_EXTRACT(NULL, NULL)
 ----
-[]
+NULL
 
 query I
 select MAP_EXTRACT(NULL::MAP("NULL", "NULL"), NULL)
 ----
-[]
+NULL
 
 query I
 select MAP_EXTRACT(NULL::MAP(INT, BIGINT), NULL)
 ----
-[]
+NULL
diff --git a/test/sql/types/nested/map/test_map_subscript_all_types.test_slow b/test/sql/types/nested/map/test_map_subscript_all_types.test_slow
index c1c3528dfed7..e6cd72ededfb 100644
--- a/test/sql/types/nested/map/test_map_subscript_all_types.test_slow
+++ b/test/sql/types/nested/map/test_map_subscript_all_types.test_slow
@@ -8,7 +8,7 @@ statement ok
 PRAGMA enable_verification;
 
 query I nosort all_types
-SELECT [columns(*)]
+SELECT columns(*)
 FROM test_all_types()
 WHERE bool IS NOT NULL;
 ----
diff --git a/test/sql/types/nested/map/test_map_subscript_composite.test b/test/sql/types/nested/map/test_map_subscript_composite.test
index d6c320b9ebc5..0bf247c0442d 100644
--- a/test/sql/types/nested/map/test_map_subscript_composite.test
+++ b/test/sql/types/nested/map/test_map_subscript_composite.test
@@ -6,13 +6,13 @@
 query I
 select m[[2,0]] from (select MAP(LIST_VALUE([0], [1], [2,0], [3]),LIST_VALUE(10, 9, 8, 7)) as m) as T
 ----
-[8]
+8
 
 #Element not present on MAP
 query I
 select m[[2,3]] from (select MAP(LIST_VALUE([0], [1], [2,0], [3], [5]),LIST_VALUE(10, 9, 8, 7,11)) as m) as T
 ----
-[]
+NULL
 
 # Keys of a MAP can not be NULL
 statement error
@@ -23,7 +23,7 @@ select m[NULL] from (select MAP(LIST_VALUE([2], [NULL], [3,0], [NULL,NULL],[5,4]
 query I
 select m[NULL] from (select MAP(LIST_VALUE({a:3}, {a:4}, {a:5}, {a:6},{a:7}),LIST_VALUE(10, 9, 8, 7,11)) as m) as T
 ----
-[]
+NULL
 
 # Keys of a MAP have to be unique
 statement error
@@ -35,13 +35,13 @@ Map keys must be unique
 query I
 select m[[2::TINYINT,3::BIGINT]] from (select MAP(LIST_VALUE([1], [2,3], [3], [2],[3,2]),LIST_VALUE(10, 9, 8, 7,11)) as m) as T
 ----
-[9]
+9
 
 #Map with larger-than-vector-size cardinalities
 query I
 select m[[10,11]] from (select MAP(lst,lst) as m from (SELECT LIST([i,i+1]) as lst FROM range(10000) tbl(i)) as lst_tbl) as T
 ----
-[[10, 11]]
+[10, 11]
 
 #Duplicate keys in a map
 statement error
@@ -52,12 +52,12 @@ Map keys must be unique
 query I
 select m[['Tenacious D', 'test']] from (select MAP(LIST_VALUE(['Jon Lajoie'], ['test', NULL], ['Tenacious D', 'test'], ['test', 'Tenacious D']),LIST_VALUE(5,10,9,11)) as m) as T
 ----
-[9]
+9
 
 query I
 select m[['Jon Lajoie']] from (select MAP(LIST_VALUE(['Jon Lajoie'], ['Tenacious D', 'a', 'b', 'c']),LIST_VALUE(10,1)) as m) as T
 ----
-[10]
+10
 
 #Searching using a non-constant value (e.g. select m[a] from ...)
 statement ok
@@ -74,7 +74,7 @@ select m from (select MAP(lsta,lstb) as m from (SELECT list(a) as lsta, list(b)
 query I
 select m[[2]] from (select MAP(lsta,lstb) as m from (SELECT list(a) as lsta, list(b) as lstb FROM ints where a[1] < 4 and b[1] > 1) as lst_tbl) as T
 ----
-[[2]]
+[2]
 
 #What about multiple rows each with a map, instead of just a single row?
 statement ok
@@ -91,10 +91,10 @@ select m from (select MAP(lsta,lstb) as m from (SELECT list(a) as lsta, list(b)
 query I rowsort
 select m[[1]] from (select MAP(lsta,lstb) as m from (SELECT list(a) as lsta, list(b) as lstb FROM ints group by b) as lst_tbl) as T
 ----
-[[1]]
-[[2]]
-[]
-[]
+NULL
+NULL
+[1]
+[2]
 
 #What about multiple rows each with a map, with a selection vector on top of them (e.g. because of a WHERE clause)?
 statement ok
@@ -103,36 +103,36 @@ insert into ints values ([1],[4])
 query I rowsort
 select m[[1]] from (select MAP(lsta,lstb) as m from (SELECT list(a) as lsta, list(b) as lstb FROM ints where b[1] <4 group by b) as lst_tbl) as T
 ----
-[[1]]
-[[2]]
-[]
+NULL
+[1]
+[2]
 
 #map_extract directly on a constant map (SELECT map_extract(MAP(..)), i.e. no FROM clause).
 query I
 SELECT map_extract(MAP(LIST_VALUE([10],[9],[12],[11],[13]),LIST_VALUE(10,9,10,11,13)),[10])
 ----
-[10]
+10
 
 #Multiple constants
 query I
 select m[{a:1, b:2, c:3}] from (select MAP(list_value({a:1, b:2, c:3}), list_value(2)) from range(5) tbl(i)) tbl(m);
 ----
-[2]
-[2]
-[2]
-[2]
-[2]
+2
+2
+2
+2
+2
 
 #Test dictionary vectors
 query III rowsort
-select grp, m, case when grp>1 then m[[0]] else list_value(null) end
+select grp, m, case when grp>1 then m[[0]] else null end
 from (select grp, MAP(lsta,lstb) as m
 from (SELECT a%4 as grp, list([a]) as lsta, list(a) as lstb FROM range(7) tbl(a) group by grp) as lst_tbl) as T;
 ----
-0	{[0]=0, [4]=4}	[NULL]
-1	{[1]=1, [5]=5}	[NULL]
-2	{[2]=2, [6]=6}	[]
-3	{[3]=3}	[]
+0	{[0]=0, [4]=4}	NULL
+1	{[1]=1, [5]=5}	NULL
+2	{[2]=2, [6]=6}	NULL
+3	{[3]=3}	NULL
 
 # Doubly nested lists
 query I
@@ -152,4 +152,4 @@ select map([3, 2, 1],
 	]
 ])[1]
 ----
-[[[3, 3, 3], [2], [NULL, 3, 2]]]
+[[3, 3, 3], [2], [NULL, 3, 2]]
diff --git a/test/sql/types/nested/map/test_map_subscript_from_column.test b/test/sql/types/nested/map/test_map_subscript_from_column.test
index 1c1057ab7875..39b8b63bf1c0 100644
--- a/test/sql/types/nested/map/test_map_subscript_from_column.test
+++ b/test/sql/types/nested/map/test_map_subscript_from_column.test
@@ -32,13 +32,13 @@ insert into t2 select id, map(k,v), k from t1;
 query I
 select v_map[array_sort(k, 'DESC', 'NULLS LAST')[1]] from t2 limit 10;
 ----
-[4.000]
-[8.000]
-[12.000]
-[16.000]
-[20.000]
-[24.000]
-[28.000]
-[32.000]
-[36.000]
-[40.000]
+4.000
+8.000
+12.000
+16.000
+20.000
+24.000
+28.000
+32.000
+36.000
+40.000
diff --git a/test/sql/types/nested/map/test_map_subscript_vector_types.test b/test/sql/types/nested/map/test_map_subscript_vector_types.test
index 0a309aa116a0..2e22b1386161 100644
--- a/test/sql/types/nested/map/test_map_subscript_vector_types.test
+++ b/test/sql/types/nested/map/test_map_subscript_vector_types.test
@@ -13,7 +13,7 @@ WHERE c IS NOT NULL;
 ----
 
 query I nosort correct
-SELECT map([c], [c])[c] IS NOT DISTINCT FROM [c] as equal
+SELECT map([c], [c])[c] IS NOT DISTINCT FROM c as equal
 FROM test_vector_types(NULL::INT[]) t(c)
 WHERE c IS NOT NULL;
 ----
@@ -47,7 +47,7 @@ SELECT
 	filtered,
 	last_element,
 	pos,
-	expected_result = result OR (result = [] AND expected_result = [NULL]) as equal
+	expected_result IS NOT DISTINCT FROM result as equal
 from
 (
 	SELECT
@@ -61,8 +61,8 @@ from
 		END as pos,
 		CASE
 			WHEN last_element IS NULL
-				THEN []
-			ELSE [list_position(filtered, last_element)]
+				THEN NULL
+			ELSE list_position(filtered, last_element)
 		END as expected_result,
 		mymap[last_element] as result,
 	FROM test_vector_types(NULL::INT[]) t(c)
diff --git a/test/sql/types/nested/map/test_map_subscript_where.test b/test/sql/types/nested/map/test_map_subscript_where.test
index 00b934bb878e..afd43a9c3451 100644
--- a/test/sql/types/nested/map/test_map_subscript_where.test
+++ b/test/sql/types/nested/map/test_map_subscript_where.test
@@ -32,12 +32,12 @@ insert into t2 select id, map(k,v), k from t1;
 query II
 select id, v_map[array_sort(k, 'DESC', 'NULLS LAST')[1]] from t2 where id > 3 limit 10;
 ----
-4	[20.000]
-5	[24.000]
-6	[28.000]
-7	[32.000]
-8	[36.000]
-9	[40.000]
+4	20.000
+5	24.000
+6	28.000
+7	32.000
+8	36.000
+9	40.000
 
 
 statement ok
@@ -50,5 +50,5 @@ query I
 SELECT MAP([key], [val])[key] FROM tbl
 WHERE key <> '2';
 ----
-[duck]
-[duckDB]
+duck
+duckDB
diff --git a/test/sql/types/nested/map/test_map_vector_types.test b/test/sql/types/nested/map/test_map_vector_types.test
index f1ca0c4d04a7..e135a61bd944 100644
--- a/test/sql/types/nested/map/test_map_vector_types.test
+++ b/test/sql/types/nested/map/test_map_vector_types.test
@@ -54,7 +54,7 @@ select
 	map(keys, vals)[key] as first_map_entry,
 		from tbl
 	where
-		not_filtered and first_map_entry[1] != val;
+		not_filtered and first_map_entry != val;
 ----
 
 statement ok
diff --git a/test/sql/types/nested/map/test_null_map_interaction.test b/test/sql/types/nested/map/test_null_map_interaction.test
index f4e73f573469..7ae4df2eab77 100644
--- a/test/sql/types/nested/map/test_null_map_interaction.test
+++ b/test/sql/types/nested/map/test_null_map_interaction.test
@@ -37,9 +37,9 @@ STRUCT("key" "NULL", "value" "NULL")[]
 query I
 SELECT TYPEOF(MAP_EXTRACT(NULL::MAP(TEXT, BIGINT), 'a'));
 ----
-BIGINT[]
+BIGINT
 
 query I
 SELECT TYPEOF(MAP_EXTRACT(NULL, 'a'));
 ----
-"NULL"[]
+"NULL"
diff --git a/test/sql/upsert/postgres/planner_preprocessing.test b/test/sql/upsert/postgres/planner_preprocessing.test
index 70892f412ca1..5dcea51e113d 100644
--- a/test/sql/upsert/postgres/planner_preprocessing.test
+++ b/test/sql/upsert/postgres/planner_preprocessing.test
@@ -146,7 +146,7 @@ insert into excluded AS target values(1, '2') on conflict (key) do update set da
 statement error
 insert into excluded values(1, '2') on conflict (key) do update set data = 3 RETURNING excluded.*;
 ----
-Binder Error: Duplicate alias "excluded" in query!
+Ambiguous reference to table "excluded"
 
 # clean up
 statement ok
diff --git a/test/sql/upsert/upsert_aliased.test b/test/sql/upsert/upsert_aliased.test
index e11165e48c8b..248f61d6200e 100644
--- a/test/sql/upsert/upsert_aliased.test
+++ b/test/sql/upsert/upsert_aliased.test
@@ -87,4 +87,4 @@ select * from tbl;
 statement error
 insert into tbl as excluded values (8,3) on conflict (j) do update set i = 5;
 ----
-Binder Error: Duplicate alias "excluded" in query!
+Ambiguous reference to table "excluded"
diff --git a/test/sql/window/test_custom_spooling.test_slow b/test/sql/window/test_custom_spooling.test_slow
new file mode 100644
index 000000000000..f6d2fd9e0fd9
--- /dev/null
+++ b/test/sql/window/test_custom_spooling.test_slow
@@ -0,0 +1,37 @@
+# name: test/sql/window/test_custom_spooling.test_slow
+# description: Validate spooling of custom window functions.
+# group: [window]
+
+require tpch
+
+statement ok
+call dbgen(sf=0.1);
+
+statement ok
+PRAGMA temp_directory='__TEST_DIR__/window_spooling'
+
+# Force paging
+statement ok
+PRAGMA memory_limit='50MB'
+
+statement ok
+PRAGMA enable_verification
+
+# MODE is very unstable on this data
+query IIII
+SELECT SUM(s), SUM(q), SUM(m), ROUND(SUM(d), -4)
+FROM (
+	SELECT
+		SUM(l_extendedprice) OVER w AS s,
+		QUANTILE(l_extendedprice, 0.5) OVER w AS q,
+		MAD(l_extendedprice) OVER w AS m,
+		MODE(l_quantity) OVER w AS d,
+	FROM lineitem
+	WINDOW w AS (
+		PARTITION BY (l_linenumber = 1) 
+		ORDER BY l_orderkey, l_linenumber 
+		ROWS BETWEEN 20 PRECEDING AND 20 FOLLOWING
+	)
+) t;
+----
+886221884732.62	20709502135.61	9898471727.97	15330000
diff --git a/test/sql/window/test_ignore_nulls.test b/test/sql/window/test_ignore_nulls.test
index b879a7a4e3ec..27f4d6b704fe 100644
--- a/test/sql/window/test_ignore_nulls.test
+++ b/test/sql/window/test_ignore_nulls.test
@@ -303,6 +303,28 @@ ORDER BY id
 ----
 40000 values hashing to c302c8b0f3c10c1e5cc7211c4af7a8d6
 
+# Independent INGORE/RESPECT NULLS with shared input.
+query III
+SELECT 
+	v, 
+	lead(v) OVER (ORDER BY id), 
+	lead(v IGNORE NULLS) OVER (ORDER BY id) 
+FROM (VALUES 
+	(1, 1), 
+	(2, NULL), 
+	(3, 2), 
+	(4, NULL), 
+	(5, 3), 
+	(6, NULL)
+) tbl(id, v);
+----
+1	NULL	2
+NULL	2	2
+2	NULL	3
+NULL	3	3
+3	NULL	NULL
+NULL	NULL	NULL
+
 #
 # Unsupported
 #
diff --git a/test/sql/window/test_volatile_independence.test b/test/sql/window/test_volatile_independence.test
new file mode 100644
index 000000000000..faa6fa5bf1b3
--- /dev/null
+++ b/test/sql/window/test_volatile_independence.test
@@ -0,0 +1,18 @@
+# name: test/sql/window/test_volatile_independence.test
+# description: Window Sharing should distiguish identical volatile expressions
+# group: [window]
+
+require skip_reload
+
+statement ok
+SELECT SETSEED(0.8675309);
+
+query II
+SELECT 
+	list(random()) OVER (ORDER BY id), 
+	max(random()) OVER (ORDER BY id) 
+FROM range(3) t(id);
+----
+[0.23450047366512405]	0.772592377754351
+[0.23450047366512405, 0.4512114749041355]	0.772592377754351
+[0.23450047366512405, 0.4512114749041355, 0.5199990716061366]	0.9123614504583814
diff --git a/test/sql/window/test_window_distinct.test_slow b/test/sql/window/test_window_distinct.test_slow
index 12ced0116b97..1eb8cf88cbca 100644
--- a/test/sql/window/test_window_distinct.test_slow
+++ b/test/sql/window/test_window_distinct.test_slow
@@ -2,6 +2,9 @@
 # description: Windowed distinct aggregates at scale
 # group: [window]
 
+# FIXME - deadlock causing CI issues
+mode skip
+
 require tpch
 
 statement ok
@@ -32,8 +35,10 @@ PRAGMA disable_verification
 statement ok
 PRAGMA threads=4
 
+# The actual memory consumption of this query is 1.5GB
+# So this is a serious level of stress.
 statement ok
-PRAGMA memory_limit='20MB'
+PRAGMA memory_limit='500MB'
 
 query I
 WITH t AS (
diff --git a/test/sql/window/test_window_exclude.test b/test/sql/window/test_window_exclude.test
index 948e0b2cde85..36eb8d347644 100644
--- a/test/sql/window/test_window_exclude.test
+++ b/test/sql/window/test_window_exclude.test
@@ -593,67 +593,78 @@ ORDER BY i;
 query II
 SELECT i, mode(i) OVER  w
 FROM (
-	SELECT * FROM generate_series(1,5)
-    UNION ALL
-    SELECT * FROM generate_series(1,5) 
+	SELECT i FROM generate_series(1,3) t(i), range(4)
 ) AS _(i)
 WINDOW w AS (ORDER BY i ROWS BETWEEN 1 PRECEDING AND 2 FOLLOWING EXCLUDE CURRENT ROW)
-ORDER BY i;
+ORDER BY ALL;
 ----
 1	1
+1	1
+1	1
 1	2
-2	1
+2	2
+2	2
+2	2
 2	3
-3	2
-3	4
-4	3
-4	5
-5	4
-5	5
+3	3
+3	3
+3	3
+3	3
 
 # MODE and GROUP
 query II
 SELECT i, mode(i) OVER  w
 FROM (
-	SELECT * FROM generate_series(1,5)
-    UNION ALL
-    SELECT * FROM generate_series(1,5) 
+	SELECT i FROM generate_series(1,3) t(i), range(4)
 ) AS _(i)
 WINDOW w AS (ORDER BY i ROWS BETWEEN 1 PRECEDING AND 2 FOLLOWING EXCLUDE GROUP)
-ORDER BY i;
+ORDER BY ALL;
 ----
 1	2
 1	2
+1	NULL
+1	NULL
 2	1
 2	3
+2	3
+2	NULL
 3	2
-3	4
-4	3
-4	5
-5	4
-5	NULL
+3	NULL
+3	NULL
+3	NULL
 
 # MODE and TIES
 query II
-SELECT i, mode(i) OVER w 
+SELECT i, mode(i) OVER w
 FROM (
-	SELECT * FROM generate_series(1,5)
-    UNION ALL
-    SELECT * FROM generate_series(1,5) 
+	FROM repeat(1, 10)
+	UNION ALL
+	FROM repeat(2, 4)
+	UNION ALL
+	FROM repeat(3, 5)
 ) AS _(i)
-WINDOW w AS (ORDER BY i ROWS BETWEEN 1 PRECEDING AND 2 FOLLOWING EXCLUDE TIES)
-ORDER BY i;
+WINDOW w AS (ORDER BY i ROWS BETWEEN CURRENT ROW AND 100 FOLLOWING EXCLUDE TIES)
+ORDER BY ALL;
 ----
-1	1
-1	2
-2	1
+1	3
+1	3
+1	3
+1	3
+1	3
+1	3
+1	3
+1	3
+1	3
+1	3
 2	3
-3	2
-3	4
-4	3
-4	5
-5	4
-5	5
+2	3
+2	3
+2	3
+3	3
+3	3
+3	3
+3	3
+3	3
 
 # MEDIAN and CURRENT ROW
 query II
diff --git a/test/sql/window/test_window_wide_frame.test_slow b/test/sql/window/test_window_wide_frame.test_slow
index a9907fce83e9..9199a6540ded 100644
--- a/test/sql/window/test_window_wide_frame.test_slow
+++ b/test/sql/window/test_window_wide_frame.test_slow
@@ -25,4 +25,4 @@ window w as (order by timestamp asc range between interval 55 seconds preceding
 order by 3 desc
 limit 1;
 ----
-2020-10-15 16:58:10.318385	50.000000	3363
+2020-10-15 16:47:25.764708	49.0	3437
diff --git a/test/sqlite/sqllogic_test_runner.cpp b/test/sqlite/sqllogic_test_runner.cpp
index daf384750bee..529f19de59bf 100644
--- a/test/sqlite/sqllogic_test_runner.cpp
+++ b/test/sqlite/sqllogic_test_runner.cpp
@@ -20,11 +20,14 @@ namespace duckdb {
 SQLLogicTestRunner::SQLLogicTestRunner(string dbpath) : dbpath(std::move(dbpath)), finished_processing_file(false) {
 	config = GetTestConfig();
 	config->options.allow_unredacted_secrets = true;
+	config->options.load_extensions = false;
 
 	auto env_var = std::getenv("LOCAL_EXTENSION_REPO");
 	if (!env_var) {
-		config->options.load_extensions = false;
 		config->options.autoload_known_extensions = false;
+	} else {
+		local_extension_repo = env_var;
+		config->options.autoload_known_extensions = true;
 	}
 }
 
@@ -90,6 +93,8 @@ void SQLLogicTestRunner::LoadDatabase(string dbpath, bool load_extensions) {
 
 	try {
 		db = make_uniq<DuckDB>(dbpath, config.get());
+		// always load core functions
+		ExtensionHelper::LoadExtension(*db, "core_functions");
 	} catch (std::exception &ex) {
 		ErrorData err(ex);
 		SQLLogicTestLogger::LoadDatabaseFail(dbpath, err.Message());
@@ -121,10 +126,8 @@ void SQLLogicTestRunner::Reconnect() {
 		con->EnableQueryVerification();
 	}
 	// Set the local extension repo for autoinstalling extensions
-	auto env_var = std::getenv("LOCAL_EXTENSION_REPO");
-	if (env_var) {
-		config->options.autoload_known_extensions = true;
-		auto res1 = con->Query("SET autoinstall_extension_repository='" + string(env_var) + "'");
+	if (!local_extension_repo.empty()) {
+		auto res1 = con->Query("SET autoinstall_extension_repository='" + local_extension_repo + "'");
 	}
 }
 
diff --git a/test/sqlite/sqllogic_test_runner.hpp b/test/sqlite/sqllogic_test_runner.hpp
index cac1f6a0ec08..e94107ee1958 100644
--- a/test/sqlite/sqllogic_test_runner.hpp
+++ b/test/sqlite/sqllogic_test_runner.hpp
@@ -44,6 +44,7 @@ class SQLLogicTestRunner {
 	bool enable_verification = false;
 	bool skip_reload = false;
 	unordered_map<string, string> environment_variables;
+	string local_extension_repo;
 
 	// If these error msgs occur in a test, the test will abort but still count as passed
 	unordered_set<string> ignore_error_messages = {"HTTP", "Unable to connect"};
diff --git a/tools/pythonpkg/tests/conftest.py b/tools/pythonpkg/tests/conftest.py
index 5843c71092fa..7deb53163523 100644
--- a/tools/pythonpkg/tests/conftest.py
+++ b/tools/pythonpkg/tests/conftest.py
@@ -224,12 +224,15 @@ def _require(extension_name, db_name=''):
 # By making the scope 'function' we ensure that a new connection gets created for every function that uses the fixture
 @pytest.fixture(scope='function')
 def spark():
+    from spark_namespace import USE_ACTUAL_SPARK
+
     if not hasattr(spark, 'session'):
         # Cache the import
-        from duckdb.experimental.spark.sql import SparkSession as session
+        from spark_namespace.sql import SparkSession as session
 
         spark.session = session
-    return spark.session.builder.master(':memory:').appName('pyspark').getOrCreate()
+
+    return spark.session.builder.appName('pyspark').getOrCreate()
 
 
 @pytest.fixture(scope='function')
diff --git a/tools/pythonpkg/tests/fast/api/test_read_csv.py b/tools/pythonpkg/tests/fast/api/test_read_csv.py
index d379ec1a009d..bb9718af81ec 100644
--- a/tools/pythonpkg/tests/fast/api/test_read_csv.py
+++ b/tools/pythonpkg/tests/fast/api/test_read_csv.py
@@ -584,12 +584,9 @@ def test_read_csv_list_invalid_path(self, tmp_path):
     @pytest.mark.parametrize(
         'options',
         [
-            {'lineterminator': '\\r\
'},
             {'lineterminator': '\
'},
             {'lineterminator': 'LINE_FEED'},
-            {'lineterminator': 'CARRIAGE_RETURN_LINE_FEED'},
             {'lineterminator': CSVLineTerminator.LINE_FEED},
-            {'lineterminator': CSVLineTerminator.CARRIAGE_RETURN_LINE_FEED},
             {'columns': {'id': 'INTEGER', 'name': 'INTEGER', 'c': 'integer', 'd': 'INTEGER'}},
             {'auto_type_candidates': ['INTEGER', 'INTEGER']},
             {'max_line_size': 10000},
diff --git a/tools/pythonpkg/tests/fast/relational_api/test_joins.py b/tools/pythonpkg/tests/fast/relational_api/test_joins.py
index 827a536e48e0..8eb365d584e3 100644
--- a/tools/pythonpkg/tests/fast/relational_api/test_joins.py
+++ b/tools/pythonpkg/tests/fast/relational_api/test_joins.py
@@ -78,3 +78,10 @@ def test_semi_join(self, con):
         rel = a.join(b, expr, 'semi')
         res = rel.fetchall()
         assert res == [(1, 1), (2, 1)]
+
+    def test_cross_join(self, con):
+        a = con.table('tbl_a')
+        b = con.table('tbl_b')
+        rel = a.cross(b)
+        res = rel.fetchall()
+        assert res == [(1, 1, 1, 4), (2, 1, 1, 4), (3, 2, 1, 4), (1, 1, 3, 5), (2, 1, 3, 5), (3, 2, 3, 5)]
diff --git a/tools/pythonpkg/tests/fast/relational_api/test_rapi_query.py b/tools/pythonpkg/tests/fast/relational_api/test_rapi_query.py
index 9707dc368321..7d04b04b1ce4 100644
--- a/tools/pythonpkg/tests/fast/relational_api/test_rapi_query.py
+++ b/tools/pythonpkg/tests/fast/relational_api/test_rapi_query.py
@@ -6,17 +6,27 @@
 
 @pytest.fixture()
 def tbl_table():
-    con = duckdb.default_connection
+    con = duckdb.default_connection()
     con.execute("drop table if exists tbl CASCADE")
     con.execute("create table tbl (i integer)")
     yield
     con.execute('drop table tbl CASCADE')
 
 
+@pytest.fixture()
+def scoped_default(duckdb_cursor):
+    default = duckdb.connect(':default:')
+    duckdb.set_default_connection(duckdb_cursor)
+    # Overwrite the default connection
+    yield
+    # Set it back on finalizing of the function
+    duckdb.set_default_connection(default)
+
+
 class TestRAPIQuery(object):
     @pytest.mark.parametrize('steps', [1, 2, 3, 4])
     def test_query_chain(self, steps):
-        con = duckdb.default_connection
+        con = duckdb.default_connection()
         amount = int(1000000)
         rel = None
         for _ in range(steps):
@@ -28,7 +38,7 @@ def test_query_chain(self, steps):
 
     @pytest.mark.parametrize('input', [[5, 4, 3], [], [1000]])
     def test_query_table(self, tbl_table, input):
-        con = duckdb.default_connection
+        con = duckdb.default_connection()
         rel = con.table("tbl")
         for row in input:
             rel.insert([row])
@@ -38,7 +48,7 @@ def test_query_table(self, tbl_table, input):
         assert result.fetchall() == [tuple([x]) for x in input]
 
     def test_query_table_basic(self, tbl_table):
-        con = duckdb.default_connection
+        con = duckdb.default_connection()
         rel = con.table("tbl")
         # Querying a table relation
         rel = rel.query("x", "select 5")
@@ -46,7 +56,7 @@ def test_query_table_basic(self, tbl_table):
         assert result.fetchall() == [(5,)]
 
     def test_query_table_qualified(self, duckdb_cursor):
-        con = duckdb.default_connection
+        con = duckdb.default_connection()
         con.execute("create schema fff")
 
         # Create table in fff schema
@@ -54,7 +64,7 @@ def test_query_table_qualified(self, duckdb_cursor):
         assert con.table("fff.t2").fetchall() == [(1,)]
 
     def test_query_insert_into_relation(self, tbl_table):
-        con = duckdb.default_connection
+        con = duckdb.default_connection()
         rel = con.query("select i from range(1000) tbl(i)")
         # Can't insert into this, not a table relation
         with pytest.raises(duckdb.InvalidInputException):
@@ -79,7 +89,7 @@ def test_query_non_select_fail(self, duckdb_cursor):
             rel.query("relation", "create table tbl as select * from not_a_valid_view")
 
     def test_query_table_unrelated(self, tbl_table):
-        con = duckdb.default_connection
+        con = duckdb.default_connection()
         rel = con.table("tbl")
         # Querying a table relation
         rel = rel.query("x", "select 5")
@@ -131,3 +141,52 @@ def test_replacement_scan_recursion(self, duckdb_cursor):
         other_rel = duckdb_cursor.sql('select a from rel')
         res = other_rel.fetchall()
         assert res == [(84,)]
+
+    def test_set_default_connection(self, scoped_default):
+        duckdb.sql("create table t as select 42")
+        assert duckdb.table('t').fetchall() == [(42,)]
+        con = duckdb.connect(':default:')
+
+        # Uses the same db as the module
+        assert con.table('t').fetchall() == [(42,)]
+
+        con2 = duckdb.connect()
+        con2.sql("create table t as select 21")
+        assert con2.table('t').fetchall() == [(21,)]
+        # Change the db used by the module
+        duckdb.set_default_connection(con2)
+
+        with pytest.raises(duckdb.CatalogException, match='Table with name d does not exist'):
+            con2.table('d').fetchall()
+
+        assert duckdb.table('t').fetchall() == [(21,)]
+
+        duckdb.sql("create table d as select [1,2,3]")
+
+        assert duckdb.table('d').fetchall() == [([1, 2, 3],)]
+        assert con2.table('d').fetchall() == [([1, 2, 3],)]
+
+    def test_set_default_connection_error(self, scoped_default):
+        with pytest.raises(TypeError, match='Invoked with: None'):
+            # set_default_connection does not allow None
+            duckdb.set_default_connection(None)
+
+        with pytest.raises(TypeError, match='Invoked with: 5'):
+            duckdb.set_default_connection(5)
+
+        assert duckdb.sql("select 42").fetchall() == [(42,)]
+        duckdb.close()
+
+        with pytest.raises(duckdb.ConnectionException, match='Connection Error: Connection already closed!'):
+            duckdb.sql("select 42").fetchall()
+
+        con2 = duckdb.connect()
+        duckdb.set_default_connection(con2)
+        assert duckdb.sql("select 42").fetchall() == [(42,)]
+
+        con3 = duckdb.connect()
+        con3.close()
+        duckdb.set_default_connection(con3)
+
+        with pytest.raises(duckdb.ConnectionException, match='Connection Error: Connection already closed!'):
+            duckdb.sql("select 42").fetchall()
diff --git a/tools/pythonpkg/tests/fast/spark/test_replace_column_value.py b/tools/pythonpkg/tests/fast/spark/test_replace_column_value.py
index 84e6fd395a04..339406161384 100644
--- a/tools/pythonpkg/tests/fast/spark/test_replace_column_value.py
+++ b/tools/pythonpkg/tests/fast/spark/test_replace_column_value.py
@@ -1,7 +1,7 @@
 import pytest
 
 _ = pytest.importorskip("duckdb.experimental.spark")
-from duckdb.experimental.spark.sql.types import Row
+from spark_namespace.sql.types import Row
 
 
 class TestReplaceValue(object):
@@ -11,12 +11,12 @@ def test_replace_value(self, spark):
         df = spark.createDataFrame(address, ["id", "address", "state"])
 
         # Replace part of string with another string
-        from duckdb.experimental.spark.sql.functions import regexp_replace
+        from spark_namespace.sql.functions import regexp_replace
 
         df2 = df.withColumn('address', regexp_replace('address', 'Rd', 'Road'))
 
         # Replace string column value conditionally
-        from duckdb.experimental.spark.sql.functions import when
+        from spark_namespace.sql.functions import when
 
         res = df2.collect()
         print(res)
diff --git a/tools/pythonpkg/tests/fast/spark/test_replace_empty_value.py b/tools/pythonpkg/tests/fast/spark/test_replace_empty_value.py
index 34bdcd1a6c99..fd3dc092149e 100644
--- a/tools/pythonpkg/tests/fast/spark/test_replace_empty_value.py
+++ b/tools/pythonpkg/tests/fast/spark/test_replace_empty_value.py
@@ -1,7 +1,9 @@
 import pytest
 
 _ = pytest.importorskip("duckdb.experimental.spark")
-from duckdb.experimental.spark.sql.types import Row
+
+from spark_namespace import USE_ACTUAL_SPARK
+from spark_namespace.sql.types import Row
 
 
 # https://sparkbyexamples.com/pyspark/pyspark-replace-empty-value-with-none-on-dataframe-2/?expand_article=1
@@ -17,7 +19,7 @@ def test_replace_empty(self, spark):
 
         # Replace name
         # CASE WHEN "name" == '' THEN NULL ELSE "name" END
-        from duckdb.experimental.spark.sql.functions import col, when
+        from spark_namespace.sql.functions import col, when
 
         df2 = df.withColumn("name", when(col("name") == "", None).otherwise(col("name")))
         assert df2.columns == ['name', 'state']
@@ -25,29 +27,45 @@ def test_replace_empty(self, spark):
         assert res == [Row(name=None), Row(name='Julia'), Row(name='Robert'), Row(name=None)]
 
         # Replace state + name
-        from duckdb.experimental.spark.sql.functions import col, when
+        from spark_namespace.sql.functions import col, when
 
         df2 = df.select([when(col(c) == "", None).otherwise(col(c)).alias(c) for c in df.columns])
         assert df2.columns == ['name', 'state']
-        res = df2.collect()
-        assert res == [
-            Row(name=None, state='CA'),
-            Row(name='Julia', state=None),
-            Row(name='Robert', state=None),
-            Row(name=None, state='NJ'),
-        ]
+        key_f = lambda x: x.name or x.state
+        res = df2.sort("name", "state").collect()
+        # FIXME: Null-handling in sort is different in DuckDB and Spark
+        if USE_ACTUAL_SPARK:
+            expected_res = [
+                Row(name=None, state='CA'),
+                Row(name=None, state='NJ'),
+                Row(name='Julia', state=None),
+                Row(name='Robert', state=None),
+            ]
+        else:
+            expected_res = [
+                Row(name='Julia', state=None),
+                Row(name='Robert', state=None),
+                Row(name=None, state='CA'),
+                Row(name=None, state='NJ'),
+            ]
+        assert res == expected_res
 
         # On selection of columns
         # Replace empty string with None on selected columns
-        from duckdb.experimental.spark.sql.functions import col, when
+        from spark_namespace.sql.functions import col, when
 
         replaceCols = ["state"]
         df2 = df.select([when(col(c) == "", None).otherwise(col(c)).alias(c) for c in replaceCols]).sort(col('state'))
         assert df2.columns == ['state']
+
+        key_f = lambda x: x.state or ""
         res = df2.collect()
-        assert res == [
-            Row(state='CA'),
-            Row(state='NJ'),
-            Row(state=None),
-            Row(state=None),
-        ]
+        assert sorted(res, key=key_f) == sorted(
+            [
+                Row(state='CA'),
+                Row(state='NJ'),
+                Row(state=None),
+                Row(state=None),
+            ],
+            key=key_f,
+        )
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_arrow_table.py b/tools/pythonpkg/tests/fast/spark/test_spark_arrow_table.py
new file mode 100644
index 000000000000..da1d1366690f
--- /dev/null
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_arrow_table.py
@@ -0,0 +1,20 @@
+import pytest
+
+_ = pytest.importorskip("duckdb.experimental.spark")
+pa = pytest.importorskip("pyarrow")
+from spark_namespace import USE_ACTUAL_SPARK
+
+
+class TestArrowTable:
+    def test_spark_to_arrow_table(self, spark):
+        if USE_ACTUAL_SPARK:
+            return
+        data = [
+            ("firstRowFirstColumn",),
+            ("2ndRowFirstColumn",),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn"])
+        arrow_table = df.toArrow()
+        assert arrow_table.num_columns == 1
+        assert arrow_table.num_rows == 2
+        assert arrow_table.column_names == ["firstColumn"]
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_catalog.py b/tools/pythonpkg/tests/fast/spark/test_spark_catalog.py
index 7a8b637bc72a..7f523abda59b 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_catalog.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_catalog.py
@@ -1,35 +1,44 @@
 import pytest
 
 _ = pytest.importorskip("duckdb.experimental.spark")
-from duckdb.experimental.spark.sql.catalog import Table, Database, Column
+
+from spark_namespace import USE_ACTUAL_SPARK
+from spark_namespace.sql.catalog import Table, Database, Column
 
 
 class TestSparkCatalog(object):
     def test_list_databases(self, spark):
         dbs = spark.catalog.listDatabases()
-        assert dbs == [
-            Database(name='memory', description=None, locationUri=''),
-            Database(name='system', description=None, locationUri=''),
-            Database(name='temp', description=None, locationUri=''),
-        ]
+        if USE_ACTUAL_SPARK:
+            assert all(isinstance(db, Database) for db in dbs)
+        else:
+            assert dbs == [
+                Database(name='memory', description=None, locationUri=''),
+                Database(name='system', description=None, locationUri=''),
+                Database(name='temp', description=None, locationUri=''),
+            ]
 
     def test_list_tables(self, spark):
         # empty
         tbls = spark.catalog.listTables()
         assert tbls == []
 
-        spark.sql('create table tbl(a varchar)')
-        tbls = spark.catalog.listTables()
-        assert tbls == [
-            Table(
-                name='tbl',
-                database='memory',
-                description='CREATE TABLE tbl(a VARCHAR);',
-                tableType='',
-                isTemporary=False,
-            )
-        ]
-
+        if not USE_ACTUAL_SPARK:
+            # Skip this if we're using actual Spark because we can't create tables
+            # with our setup.
+            spark.sql('create table tbl(a varchar)')
+            tbls = spark.catalog.listTables()
+            assert tbls == [
+                Table(
+                    name='tbl',
+                    database='memory',
+                    description='CREATE TABLE tbl(a VARCHAR);',
+                    tableType='',
+                    isTemporary=False,
+                )
+            ]
+
+    @pytest.mark.skipif(USE_ACTUAL_SPARK, reason="We can't create tables with our Spark test setup")
     def test_list_columns(self, spark):
         spark.sql('create table tbl(a varchar, b bool)')
         columns = spark.catalog.listColumns('tbl')
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_column.py b/tools/pythonpkg/tests/fast/spark/test_spark_column.py
index 5367076bd98a..84e2d3c256a7 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_column.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_column.py
@@ -2,10 +2,11 @@
 
 _ = pytest.importorskip("duckdb.experimental.spark")
 
-from duckdb.experimental.spark.sql.column import Column
-from duckdb.experimental.spark.sql.functions import struct
-from duckdb.experimental.spark.sql.types import Row
-from duckdb.experimental.spark.errors import PySparkTypeError
+from spark_namespace import USE_ACTUAL_SPARK
+from spark_namespace.sql.column import Column
+from spark_namespace.sql.functions import struct
+from spark_namespace.sql.types import Row
+from spark_namespace.errors import PySparkTypeError
 
 import duckdb
 import re
@@ -15,11 +16,20 @@ class TestSparkColumn(object):
     def test_struct_column(self, spark):
         df = spark.createDataFrame([Row(a=1, b=2, c=3, d=4)])
 
-        df = df.withColumn('struct', struct(df.col0, df.col1))
-        assert 'struct' in df
-        new_col = df.schema['struct']
-        assert 'col0' in new_col.dataType
-        assert 'col1' in new_col.dataType
+        # FIXME: column names should be set explicitly using the Row, rather than letting duckdb assign defaults (col0, col1, etc..)
+        if USE_ACTUAL_SPARK:
+            df = df.withColumn('struct', struct(df.a, df.b))
+        else:
+            df = df.withColumn('struct', struct(df.col0, df.col1))
+            assert 'struct' in df
+            new_col = df.schema['struct']
+
+        if USE_ACTUAL_SPARK:
+            assert 'a' in df.schema['struct'].dataType.fieldNames()
+            assert 'b' in df.schema['struct'].dataType.fieldNames()
+        else:
+            assert 'col0' in new_col.dataType
+            assert 'col1' in new_col.dataType
 
         with pytest.raises(
             PySparkTypeError, match=re.escape("[NOT_COLUMN] Argument `col` should be a Column, got str.")
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_dataframe.py b/tools/pythonpkg/tests/fast/spark/test_spark_dataframe.py
index 6272dda74a8d..5b7492d71d66 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_dataframe.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_dataframe.py
@@ -2,7 +2,8 @@
 
 _ = pytest.importorskip("duckdb.experimental.spark")
 
-from duckdb.experimental.spark.sql.types import (
+from spark_namespace import USE_ACTUAL_SPARK
+from spark_namespace.sql.types import (
     LongType,
     StructType,
     BooleanType,
@@ -14,11 +15,18 @@
     ArrayType,
     MapType,
 )
-from duckdb.experimental.spark.sql.functions import col, struct, when
+from spark_namespace.sql.functions import col, struct, when
+from spark_namespace.sql.column import Column
 import duckdb
 import re
 
-from duckdb.experimental.spark.errors import PySparkValueError, PySparkTypeError
+from spark_namespace.errors import PySparkValueError, PySparkTypeError
+
+
+def assert_column_objects_equal(col1: Column, col2: Column):
+    assert type(col1) == type(col2)
+    if not USE_ACTUAL_SPARK:
+        assert col1.expr == col2.expr
 
 
 class TestDataFrame(object):
@@ -35,8 +43,16 @@ def test_dataframe_from_list_of_tuples(self, spark):
 
         # Tuples of different sizes
         address = [(1, "14851 Jeffrey Rd", "DE"), (2, "43421 Margarita St", "NY"), (3, "13111 Siemon Ave")]
-        with pytest.raises(PySparkTypeError, match="LENGTH_SHOULD_BE_THE_SAME"):
-            df = spark.createDataFrame(address, ["id", "address", "state"])
+
+        if USE_ACTUAL_SPARK:
+            from py4j.protocol import Py4JJavaError
+
+            with pytest.raises(Py4JJavaError):
+                df = spark.createDataFrame(address, ["id", "address", "state"])
+                df.collect()
+        else:
+            with pytest.raises(PySparkTypeError, match="LENGTH_SHOULD_BE_THE_SAME"):
+                df = spark.createDataFrame(address, ["id", "address", "state"])
 
         # Dataframe instead of list
         with pytest.raises(PySparkTypeError, match="SHOULD_NOT_DATAFRAME"):
@@ -47,22 +63,29 @@ def test_dataframe_from_list_of_tuples(self, spark):
             df = spark.createDataFrame(5, ["id", "address", "test"])
 
         # Empty list
-        df = spark.createDataFrame([], ["id", "address", "test"])
-        res = df.collect()
-        assert res == []
+        if not USE_ACTUAL_SPARK:
+            # FIXME: Spark raises PySparkValueError [CANNOT_INFER_EMPTY_SCHEMA]
+            df = spark.createDataFrame([], ["id", "address", "test"])
+            res = df.collect()
+            assert res == []
 
         # Duplicate column names
         address = [(1, "14851 Jeffrey Rd", "DE"), (2, "43421 Margarita St", "NY"), (3, "13111 Siemon Ave", "DE")]
         df = spark.createDataFrame(address, ["id", "address", "id"])
         res = df.collect()
-        assert (
-            str(res)
-            == "[Row(id=1, address='14851 Jeffrey Rd', id='DE'), Row(id=2, address='43421 Margarita St', id='NY'), Row(id=3, address='13111 Siemon Ave', id='DE')]"
-        )
+        exptected_res_str = "[Row(id=1, address='14851 Jeffrey Rd', id='DE'), Row(id=2, address='43421 Margarita St', id='NY'), Row(id=3, address='13111 Siemon Ave', id='DE')]"
+        if USE_ACTUAL_SPARK:
+            # Spark uses string for both ID columns. DuckDB correctly infers the types.
+            exptected_res_str = (
+                exptected_res_str.replace("id=1", "id='1'").replace("id=2", "id='2'").replace("id=3", "id='3'")
+            )
+        assert str(res) == exptected_res_str
 
         # Not enough column names
-        with pytest.raises(PySparkValueError, match="number of columns in the DataFrame don't match"):
-            df = spark.createDataFrame(address, ["id", "address"])
+        if not USE_ACTUAL_SPARK:
+            # FIXME: Spark does not raise this error
+            with pytest.raises(PySparkValueError, match="number of columns in the DataFrame don't match"):
+                df = spark.createDataFrame(address, ["id", "address"])
 
         # Empty column names list
         # Columns are filled in with default names
@@ -76,24 +99,28 @@ def test_dataframe_from_list_of_tuples(self, spark):
         ]
 
         # Too many column names
-        with pytest.raises(PySparkValueError, match="number of columns in the DataFrame don't match"):
-            df = spark.createDataFrame(address, ["id", "address", "one", "two", "three"])
+        if not USE_ACTUAL_SPARK:
+            # In Spark, this leads to an IndexError
+            with pytest.raises(PySparkValueError, match="number of columns in the DataFrame don't match"):
+                df = spark.createDataFrame(address, ["id", "address", "one", "two", "three"])
 
         # Column names is not a list (but is iterable)
-        df = spark.createDataFrame(address, {'a': 5, 'b': 6, 'c': 42})
-        res = df.collect()
-        assert res == [
-            Row(a=1, b='14851 Jeffrey Rd', c='DE'),
-            Row(a=2, b='43421 Margarita St', c='NY'),
-            Row(a=3, b='13111 Siemon Ave', c='DE'),
-        ]
+        if not USE_ACTUAL_SPARK:
+            # These things do not work in Spark or throw different errors
+            df = spark.createDataFrame(address, {'a': 5, 'b': 6, 'c': 42})
+            res = df.collect()
+            assert res == [
+                Row(a=1, b='14851 Jeffrey Rd', c='DE'),
+                Row(a=2, b='43421 Margarita St', c='NY'),
+                Row(a=3, b='13111 Siemon Ave', c='DE'),
+            ]
 
-        # Column names is not a list (string, becomes a single column name)
-        with pytest.raises(PySparkValueError, match="number of columns in the DataFrame don't match"):
-            df = spark.createDataFrame(address, 'a')
+            # Column names is not a list (string, becomes a single column name)
+            with pytest.raises(PySparkValueError, match="number of columns in the DataFrame don't match"):
+                df = spark.createDataFrame(address, 'a')
 
-        with pytest.raises(TypeError, match="must be an iterable, not int"):
-            df = spark.createDataFrame(address, 5)
+            with pytest.raises(TypeError, match="must be an iterable, not int"):
+                df = spark.createDataFrame(address, 5)
 
     def test_dataframe(self, spark):
         # Create DataFrame
@@ -101,6 +128,7 @@ def test_dataframe(self, spark):
         res = df.collect()
         assert res == [Row(col0='Scala', col1=25000), Row(col0='Spark', col1=35000), Row(col0='PHP', col1=21000)]
 
+    @pytest.mark.skipif(USE_ACTUAL_SPARK, reason="We can't create tables with our Spark test setup")
     def test_writing_to_table(self, spark):
         # Create Hive table & query it.
         spark.sql(
@@ -166,7 +194,7 @@ def test_df_from_name_list(self, spark):
         assert res == [Row(a=42, b=True), Row(a=21, b=False)]
 
     def test_df_creation_coverage(self, spark):
-        from duckdb.experimental.spark.sql.types import StructType, StructField, StringType, IntegerType
+        from spark_namespace.sql.types import StructType, StructField, StringType, IntegerType
 
         data2 = [
             ("James", "", "Smith", "36636", "M", 3000),
@@ -225,7 +253,7 @@ def test_df_nested_struct(self, spark):
 
         df2 = spark.createDataFrame(data=structureData, schema=structureSchema)
         res = df2.collect()
-        assert res == [
+        expected_res = [
             Row(
                 name={'firstname': 'James', 'middlename': '', 'lastname': 'Smith'}, id='36636', gender='M', salary=3100
             ),
@@ -246,6 +274,9 @@ def test_df_nested_struct(self, spark):
             ),
             Row(name={'firstname': 'Jen', 'middlename': 'Mary', 'lastname': 'Brown'}, id='', gender='F', salary=-1),
         ]
+        if USE_ACTUAL_SPARK:
+            expected_res = [Row(name=Row(**r.name), id=r.id, gender=r.gender, salary=r.salary) for r in expected_res]
+        assert res == expected_res
         schema = df2.schema
         assert schema == StructType(
             [
@@ -267,7 +298,7 @@ def test_df_nested_struct(self, spark):
         )
 
     def test_df_columns(self, spark):
-        from duckdb.experimental.spark.sql.functions import col, struct, when
+        from spark_namespace.sql.functions import col, struct, when
 
         structureData = [
             (("James", "", "Smith"), "36636", "M", 3100),
@@ -308,7 +339,7 @@ def test_df_columns(self, spark):
             ),
         ).drop("id", "gender", "salary")
 
-        assert 'OtherInfo' in updatedDF
+        assert 'OtherInfo' in updatedDF.columns
 
     def test_array_and_map_type(self, spark):
         """Array & Map"""
@@ -345,7 +376,7 @@ def test_getitem_column(self, spark):
         data = [(56, "Carol"), (20, "Alice")]
         df = spark.createDataFrame(data, ["age", "name"])
 
-        assert df["age"] == col("age")
+        assert_column_objects_equal(df["age"], col("age"))
 
     def test_getitem_dataframe(self, spark):
         data = [(56, "Ben", "Street1"), (20, "Tom", "Street2")]
@@ -360,7 +391,7 @@ def test_getattr_dataframe(self, spark):
         data = [(56, "Ben", "Street1"), (20, "Tom", "Street2")]
         df = spark.createDataFrame(data, ["age", "name", "address"])
 
-        assert df.age == col("age")
+        assert_column_objects_equal(df.age, col("age"))
 
     def test_head_first(self, spark):
         data = [(56, "Carol"), (20, "Alice"), (3, "Dave"), (3, "Anna"), (1, "Ben")]
@@ -382,3 +413,11 @@ def test_head_take_n(self, spark):
         rows = df.head(2)
         take = df.take(2)
         assert rows == take == expected
+
+    def test_drop(self, spark):
+        data = [(1, 2, 3, 4)]
+        df = spark.createDataFrame(data, ["one", "two", "three", "four"])
+        expected = ["one", "four"]
+        assert df.drop("two", "three").columns == expected
+        assert df.drop("two", col("three")).columns == expected
+        assert df.drop("two", col("three"), col("missing")).columns == expected
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_dataframe_sort.py b/tools/pythonpkg/tests/fast/spark/test_spark_dataframe_sort.py
index 2808ec52b121..8242a00d6ab8 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_dataframe_sort.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_dataframe_sort.py
@@ -1,10 +1,12 @@
 import pytest
 
-from duckdb.experimental.spark.sql.types import Row
-from duckdb.experimental.spark.errors import PySparkTypeError, PySparkValueError, PySparkIndexError
-
 _ = pytest.importorskip("duckdb.experimental.spark")
 
+import spark_namespace.errors
+from spark_namespace.sql.types import Row
+from spark_namespace.errors import PySparkTypeError, PySparkValueError
+from spark_namespace import USE_ACTUAL_SPARK
+
 
 class TestDataFrameSort(object):
     data = [(56, "Carol"), (20, "Alice"), (3, "Dave"), (3, "Anna"), (1, "Ben")]
@@ -25,8 +27,10 @@ def test_sort_ascending(self, spark):
         df = df.sort("age", "name")
         assert df.collect() == expected
 
-        df = df.sort(1, 2)
-        assert df.collect() == expected
+        if not USE_ACTUAL_SPARK:
+            # Spark does not support passing integers
+            df = df.sort(1, 2)
+            assert df.collect() == expected
 
     def test_sort_descending(self, spark):
         df = spark.createDataFrame(self.data, ["age", "name"])
@@ -44,8 +48,10 @@ def test_sort_descending(self, spark):
         df = df.sort("name", "age")
         assert df.collect() == expected
 
-        df = df.sort(2, 1)
-        assert df.collect() == expected
+        if not USE_ACTUAL_SPARK:
+            # Spark does not support passing integers
+            df = df.sort(2, 1)
+            assert df.collect() == expected
 
     def test_sort_wrong_asc_params(self, spark):
         df = spark.createDataFrame(self.data, ["age", "name"])
@@ -59,9 +65,17 @@ def test_sort_empty_params(self, spark):
         with pytest.raises(PySparkValueError):
             df = df.sort()
 
+    # See https://github.com/apache/spark/commit/0193d0f88a953063c41c41042fb58bd0badc155c
+    # for the PR which added that error to PySpark
+    @pytest.mark.skipif(
+        USE_ACTUAL_SPARK and not hasattr(spark_namespace.errors, "PySparkIndexError"),
+        reason="PySparkIndexError is only introduced in PySpark 4.0.0",
+    )
     def test_sort_zero_index(self, spark):
         df = spark.createDataFrame(self.data, ["age", "name"])
 
+        from spark_namespace.errors import PySparkIndexError
+
         with pytest.raises(PySparkIndexError):
             df = df.sort(0)
 
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_drop_duplicates.py b/tools/pythonpkg/tests/fast/spark/test_spark_drop_duplicates.py
index 465af91cd66a..6dc7f5734262 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_drop_duplicates.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_drop_duplicates.py
@@ -1,7 +1,7 @@
 import pytest
 
 
-from duckdb.experimental.spark.sql.types import (
+from spark_namespace.sql.types import (
     Row,
 )
 
@@ -9,7 +9,8 @@
 
 
 class TestDataFrameDropDuplicates(object):
-    def test_spark_drop_duplicates(self, spark):
+    @pytest.mark.parametrize("method", ["dropDuplicates", "drop_duplicates"])
+    def test_spark_drop_duplicates(self, method, spark):
         # Prepare Data
         data = [
             ("James", "Sales", 3000),
@@ -45,7 +46,7 @@ def test_spark_drop_duplicates(self, spark):
         ]
         assert res == expected
 
-        df2 = df.dropDuplicates().sort("employee_name")
+        df2 = getattr(df, method)().sort("employee_name")
         assert df2.count() == 9
         res2 = df2.collect()
         assert res2 == res
@@ -61,7 +62,7 @@ def test_spark_drop_duplicates(self, spark):
             Row(department='Sales', salary=4600),
         ]
 
-        dropDisDF = df.dropDuplicates(["department", "salary"]).sort("department", "salary")
+        dropDisDF = getattr(df, method)(["department", "salary"]).sort("department", "salary")
         assert dropDisDF.columns == ["employee_name", "department", "salary"]
         assert dropDisDF.count() == len(expected_subset)
         res = dropDisDF.select("department", "salary").collect()
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_except.py b/tools/pythonpkg/tests/fast/spark/test_spark_except.py
new file mode 100644
index 000000000000..434ac613e1ad
--- /dev/null
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_except.py
@@ -0,0 +1,31 @@
+import platform
+import pytest
+
+_ = pytest.importorskip("duckdb.experimental.spark")
+
+from duckdb.experimental.spark.sql.types import Row
+from duckdb.experimental.spark.sql.functions import col
+
+
+@pytest.fixture
+def df(spark):
+    return spark.createDataFrame([("a", 1), ("a", 1), ("a", 1), ("a", 2), ("b", 3), ("c", 4)], ["C1", "C2"])
+
+
+@pytest.fixture
+def df2(spark):
+    return spark.createDataFrame([("a", 1), ("b", 3)], ["C1", "C2"])
+
+
+class TestDataFrameIntersect:
+    def test_exceptAll(self, spark, df, df2):
+
+        df3 = df.exceptAll(df2).sort(*df.columns)
+        res = df3.collect()
+
+        assert res == [
+            Row(C1="a", C2=1),
+            Row(C1="a", C2=1),
+            Row(C1="a", C2=2),
+            Row(C1="c", C2=4),
+        ]
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_filter.py b/tools/pythonpkg/tests/fast/spark/test_spark_filter.py
index b15dcbffe6ce..fb6f0b1af174 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_filter.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_filter.py
@@ -2,7 +2,8 @@
 
 _ = pytest.importorskip("duckdb.experimental.spark")
 
-from duckdb.experimental.spark.sql.types import (
+from spark_namespace import USE_ACTUAL_SPARK
+from spark_namespace.sql.types import (
     LongType,
     StructType,
     BooleanType,
@@ -14,8 +15,8 @@
     ArrayType,
     MapType,
 )
-from duckdb.experimental.spark.sql.functions import col, struct, when, lit, array_contains
-from duckdb.experimental.spark.errors import PySparkTypeError
+from spark_namespace.sql.functions import col, struct, when, lit, array_contains
+from spark_namespace.errors import PySparkTypeError
 import duckdb
 import re
 
@@ -140,15 +141,21 @@ def test_dataframe_filter(self, spark):
 
         df2 = df.filter(array_contains(df.languages, "Java"))
         res = df2.collect()
+
+        james_name = {'firstname': 'James', 'middlename': '', 'lastname': 'Smith'}
+        anna_name = {'firstname': 'Anna', 'middlename': 'Rose', 'lastname': ''}
+        if USE_ACTUAL_SPARK:
+            james_name = Row(**james_name)
+            anna_name = Row(**anna_name)
         assert res == [
             Row(
-                name={'firstname': 'James', 'middlename': '', 'lastname': 'Smith'},
+                name=james_name,
                 languages=['Java', 'Scala', 'C++'],
                 state='OH',
                 gender='M',
             ),
             Row(
-                name={'firstname': 'Anna', 'middlename': 'Rose', 'lastname': ''},
+                name=anna_name,
                 languages=['Spark', 'Java', 'C++'],
                 state='CA',
                 gender='F',
@@ -157,15 +164,20 @@ def test_dataframe_filter(self, spark):
 
         df2 = df.filter(df.name.lastname == "Williams")
         res = df2.collect()
+        julia_name = {'firstname': 'Julia', 'middlename': '', 'lastname': 'Williams'}
+        mike_name = {'firstname': 'Mike', 'middlename': 'Mary', 'lastname': 'Williams'}
+        if USE_ACTUAL_SPARK:
+            julia_name = Row(**julia_name)
+            mike_name = Row(**mike_name)
         assert res == [
             Row(
-                name={'firstname': 'Julia', 'middlename': '', 'lastname': 'Williams'},
+                name=julia_name,
                 languages=['CSharp', 'VB'],
                 state='OH',
                 gender='F',
             ),
             Row(
-                name={'firstname': 'Mike', 'middlename': 'Mary', 'lastname': 'Williams'},
+                name=mike_name,
                 languages=['Python', 'VB'],
                 state='OH',
                 gender='M',
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_function_concat_ws.py b/tools/pythonpkg/tests/fast/spark/test_spark_function_concat_ws.py
index 83c92d209ffb..82f19cd13d62 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_function_concat_ws.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_function_concat_ws.py
@@ -1,8 +1,8 @@
 import pytest
 
 _ = pytest.importorskip("duckdb.experimental.spark")
-from duckdb.experimental.spark.sql.types import Row
-from duckdb.experimental.spark.sql.functions import concat_ws, col
+from spark_namespace.sql.types import Row
+from spark_namespace.sql.functions import concat_ws, col
 
 
 class TestReplaceEmpty(object):
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_functions_array.py b/tools/pythonpkg/tests/fast/spark/test_spark_functions_array.py
new file mode 100644
index 000000000000..51af25c6ccc0
--- /dev/null
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_functions_array.py
@@ -0,0 +1,68 @@
+import pytest
+
+_ = pytest.importorskip("duckdb.experimental.spark")
+from spark_namespace.sql import functions as F
+from spark_namespace.sql.types import Row
+
+
+class TestSparkFunctionsArray:
+    def test_array_distinct(self, spark):
+        data = [
+            ([1, 2, 2], 2),
+            ([2, 4, 5], 3),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn", "secondColumn"])
+        df = df.withColumn("distinct_values", F.array_distinct(F.col("firstColumn")))
+        res = df.select("distinct_values").collect()
+        # Output order can vary across platforms which is why we sort it first
+        assert len(res) == 2
+        assert sorted(res[0].distinct_values) == [1, 2]
+        assert sorted(res[1].distinct_values) == [2, 4, 5]
+
+    def test_array_intersect(self, spark):
+        data = [
+            (["b", "a", "c"], ["c", "d", "a", "f"]),
+        ]
+        df = spark.createDataFrame(data, ["c1", "c2"])
+        df = df.withColumn("intersect_values", F.array_intersect(F.col("c1"), F.col("c2")))
+        res = df.select("intersect_values").collect()
+        # Output order can vary across platforms which is why we sort it first
+        assert len(res) == 1
+        assert sorted(res[0].intersect_values) == ["a", "c"]
+
+    def test_array_union(self, spark):
+        data = [
+            (["b", "a", "c"], ["c", "d", "a", "f"]),
+        ]
+        df = spark.createDataFrame(data, ["c1", "c2"])
+        df = df.withColumn("union_values", F.array_union(F.col("c1"), F.col("c2")))
+        res = df.select("union_values").collect()
+        # Output order can vary across platforms which is why we sort it first
+        assert len(res) == 1
+        assert sorted(res[0].union_values) == ["a", "b", "c", "d", "f"]
+
+    def test_array_max(self, spark):
+        data = [
+            ([1, 2, 3], 3),
+            ([4, 2, 5], 5),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn", "secondColumn"])
+        df = df.withColumn("max_value", F.array_max(F.col("firstColumn")))
+        res = df.select("max_value").collect()
+        assert res == [
+            Row(max_value=3),
+            Row(max_value=5),
+        ]
+
+    def test_array_min(self, spark):
+        data = [
+            ([2, 1, 3], 3),
+            ([2, 4, 5], 5),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn", "secondColumn"])
+        df = df.withColumn("min_value", F.array_min(F.col("firstColumn")))
+        res = df.select("min_value").collect()
+        assert res == [
+            Row(max_value=1),
+            Row(max_value=2),
+        ]
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_functions_base64.py b/tools/pythonpkg/tests/fast/spark/test_spark_functions_base64.py
new file mode 100644
index 000000000000..734a527589af
--- /dev/null
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_functions_base64.py
@@ -0,0 +1,43 @@
+import pytest
+
+_ = pytest.importorskip("duckdb.experimental.spark")
+
+from spark_namespace.sql import functions as F
+
+
+class TestSparkFunctionsBase64(object):
+    def test_base64(self, spark):
+        data = [
+            ("quack",),
+        ]
+        res = (
+            spark.createDataFrame(data, ["firstColumn"])
+            .withColumn("encoded_value", F.base64(F.col("firstColumn")))
+            .select("encoded_value")
+            .collect()
+        )
+        assert res[0].encoded_value == "cXVhY2s="
+
+    def test_base64ColString(self, spark):
+        data = [
+            ("quack",),
+        ]
+        res = (
+            spark.createDataFrame(data, ["firstColumn"])
+            .withColumn("encoded_value", F.base64("firstColumn"))
+            .select("encoded_value")
+            .collect()
+        )
+        assert res[0].encoded_value == "cXVhY2s="
+
+    def test_unbase64(self, spark):
+        data = [
+            ("cXVhY2s=",),
+        ]
+        res = (
+            spark.createDataFrame(data, ["firstColumn"])
+            .withColumn("decoded_value", F.unbase64(F.col("firstColumn")))
+            .select("decoded_value")
+            .collect()
+        )
+        assert res[0].decoded_value == b'quack'
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_functions_date.py b/tools/pythonpkg/tests/fast/spark/test_spark_functions_date.py
index 1da7c73bb07a..05ecb13d2cf2 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_functions_date.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_functions_date.py
@@ -3,8 +3,8 @@
 _ = pytest.importorskip("duckdb.experimental.spark")
 from datetime import date, datetime
 
-from duckdb.experimental.spark.sql import functions as F
-from duckdb.experimental.spark.sql.types import Row
+from spark_namespace.sql import functions as F
+from spark_namespace.sql.types import Row
 
 
 class TestsSparkFunctionsDate(object):
@@ -36,7 +36,9 @@ def test_date_trunc(self, spark):
         gen_record = df.select(*[F.date_trunc(fmt, "dt_ref").alias(fmt) for fmt in cols]).collect()[0]
 
         expected_record = spark.createDataFrame(
-            [r.values() for r in expected],
+            # Need to convert to a list for Spark which otherwise throws a TypeError.
+            # It would work without it for DuckDB.
+            [list(r.values()) for r in expected],
             cols,
         ).collect()[0]
 
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_functions_hash.py b/tools/pythonpkg/tests/fast/spark/test_spark_functions_hash.py
index 2f6cff1d06d1..7b14f29eb543 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_functions_hash.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_functions_hash.py
@@ -1,7 +1,7 @@
 import pytest
 
 _ = pytest.importorskip("duckdb.experimental.spark")
-from duckdb.experimental.spark.sql import functions as F
+from spark_namespace.sql import functions as F
 
 
 class TestSparkFunctionsHash(object):
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_functions_hex.py b/tools/pythonpkg/tests/fast/spark/test_spark_functions_hex.py
new file mode 100644
index 000000000000..e5cbf12f3301
--- /dev/null
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_functions_hex.py
@@ -0,0 +1,68 @@
+import pytest
+import sys
+
+_ = pytest.importorskip("duckdb.experimental.spark")
+from spark_namespace.sql import functions as F
+
+
+class TestSparkFunctionsHex(object):
+    def test_hex_string_col(self, spark):
+        data = [
+            ("quack",),
+        ]
+        res = (
+            spark.createDataFrame(data, ["firstColumn"])
+            .withColumn("hex_value", F.hex(F.col("firstColumn")))
+            .select("hex_value")
+            .collect()
+        )
+        assert res[0].hex_value == "717561636B"
+
+    def test_hex_binary_col(self, spark):
+        data = [
+            (b'quack',),
+        ]
+        res = (
+            spark.createDataFrame(data, ["firstColumn"])
+            .withColumn("hex_value", F.hex(F.col("firstColumn")))
+            .select("hex_value")
+            .collect()
+        )
+        assert res[0].hex_value == "717561636B"
+
+    def test_hex_integer_col(self, spark):
+        data = [
+            (int(42),),
+        ]
+        res = (
+            spark.createDataFrame(data, ["firstColumn"])
+            .withColumn("hex_value", F.hex(F.col("firstColumn")))
+            .select("hex_value")
+            .collect()
+        )
+        assert res[0].hex_value == "2A"
+
+    # def test_hex_long_col(self, spark):
+    #     long_value = sys.maxsize + 1
+    #     data = [
+    #         (long_value,),
+    #     ]
+    #     res = (
+    #         spark.createDataFrame(data, ["firstColumn"])
+    #         .withColumn("hex_value", F.hex(F.col("firstColumn")))
+    #         .select("hex_value")
+    #         .collect()
+    #     )
+    #     assert res[0].hex_value == hex(long_value)[2:]
+
+    def test_unhex(self, spark):
+        data = [
+            ("717561636B",),
+        ]
+        res = (
+            spark.createDataFrame(data, ["firstColumn"])
+            .withColumn("unhex_value", F.unhex(F.col("firstColumn")))
+            .select("unhex_value")
+            .collect()
+        )
+        assert res[0].unhex_value == b'quack'
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_functions_miscellaneous.py b/tools/pythonpkg/tests/fast/spark/test_spark_functions_miscellaneous.py
new file mode 100644
index 000000000000..18a90364074e
--- /dev/null
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_functions_miscellaneous.py
@@ -0,0 +1,30 @@
+import pytest
+
+_ = pytest.importorskip("duckdb.experimental.spark")
+from spark_namespace.sql import functions as F
+from spark_namespace.sql.types import Row
+
+
+class TestsSparkFunctionsMiscellaneous:
+    def test_call_function(self, spark):
+        data = [
+            (-1, 2),
+            (4, 3),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn", "secondColumn"])
+
+        # Test with 2 columns as arguments
+        df = df.withColumn("greatest_value", F.call_function("greatest", F.col("firstColumn"), F.col("secondColumn")))
+        res = df.select("greatest_value").collect()
+        assert res == [
+            Row(greatest_value=2),
+            Row(greatest_value=4),
+        ]
+
+        # Test with 1 column as argument
+        df = df.withColumn("abs_value", F.call_function("abs", F.col("firstColumn")))
+        res = df.select("abs_value").collect()
+        assert res == [
+            Row(abs_value=1),
+            Row(abs_value=4),
+        ]
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_functions_null.py b/tools/pythonpkg/tests/fast/spark/test_spark_functions_null.py
index 941678e422ad..e321c6153ead 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_functions_null.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_functions_null.py
@@ -1,8 +1,8 @@
 import pytest
 
 _ = pytest.importorskip("duckdb.experimental.spark")
-from duckdb.experimental.spark.sql import functions as F
-from duckdb.experimental.spark.sql.types import Row
+from spark_namespace.sql import functions as F
+from spark_namespace.sql.types import Row
 
 
 class TestsSparkFunctionsNull(object):
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_functions_numeric.py b/tools/pythonpkg/tests/fast/spark/test_spark_functions_numeric.py
index 71922512e934..afce98b9026e 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_functions_numeric.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_functions_numeric.py
@@ -1,8 +1,8 @@
 import pytest
 
 _ = pytest.importorskip("duckdb.experimental.spark")
-from duckdb.experimental.spark.sql import functions as F
-from duckdb.experimental.spark.sql.types import Row
+from spark_namespace.sql import functions as F
+from spark_namespace.sql.types import Row
 
 
 class TestSparkFunctionsNumeric(object):
@@ -83,3 +83,184 @@ def test_sqrt(self, spark):
             Row(sqrt_value=2.0),
             Row(sqrt_value=3.0),
         ]
+
+    def test_cos(self, spark):
+        data = [
+            (0.0,),
+            (3.14159,),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn"])
+        df = df.withColumn("cos_value", F.cos(F.col("firstColumn")))
+        res = df.select("cos_value").collect()
+        assert len(res) == 2
+        assert res[0].cos_value == pytest.approx(1.0)
+        assert res[1].cos_value == pytest.approx(-1.0)
+
+    def test_acos(self, spark):
+        data = [
+            (1,),
+            (-1,),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn"])
+        df = df.withColumn("acos_value", F.acos(F.col("firstColumn")))
+        res = df.select("acos_value").collect()
+        assert len(res) == 2
+        assert res[0].acos_value == pytest.approx(0.0)
+        assert res[1].acos_value == pytest.approx(3.141592653589793)
+
+    def test_exp(self, spark):
+        data = [
+            (0.693,),
+            (0.0,),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn"])
+        df = df.withColumn("exp_value", F.exp(F.col("firstColumn")))
+        res = df.select("exp_value").collect()
+        round(res[0].exp_value, 2) == 2
+        res[1].exp_value == 1
+
+    def test_factorial(self, spark):
+        data = [
+            (4,),
+            (5,),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn"])
+        df = df.withColumn("factorial_value", F.factorial(F.col("firstColumn")))
+        res = df.select("factorial_value").collect()
+        assert res == [
+            Row(factorial_value=24),
+            Row(factorial_value=120),
+        ]
+
+    def test_log2(self, spark):
+        data = [
+            (4,),
+            (8,),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn"])
+        df = df.withColumn("log2_value", F.log2(F.col("firstColumn")))
+        res = df.select("log2_value").collect()
+        assert res == [
+            Row(log2_value=2.0),
+            Row(log2_value=3.0),
+        ]
+
+    def test_ln(self, spark):
+        data = [
+            (2.718,),
+            (1.0,),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn"])
+        df = df.withColumn("ln_value", F.ln(F.col("firstColumn")))
+        res = df.select("ln_value").collect()
+        round(res[0].ln_value, 2) == 1
+        res[1].ln_value == 0
+
+    def test_degrees(self, spark):
+        data = [
+            (3.14159,),
+            (0.0,),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn"])
+        df = df.withColumn("degrees_value", F.degrees(F.col("firstColumn")))
+        res = df.select("degrees_value").collect()
+        round(res[0].degrees_value, 2) == 180
+        res[1].degrees_value == 0
+
+    def test_radians(self, spark):
+        data = [
+            (180,),
+            (0,),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn"])
+        df = df.withColumn("radians_value", F.radians(F.col("firstColumn")))
+        res = df.select("radians_value").collect()
+        round(res[0].radians_value, 2) == 3.14
+        res[1].radians_value == 0
+
+    def test_atan(self, spark):
+        data = [
+            (1,),
+            (0,),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn"])
+        df = df.withColumn("atan_value", F.atan(F.col("firstColumn")))
+        res = df.select("atan_value").collect()
+        round(res[0].atan_value, 2) == 0.79
+        res[1].atan_value == 0
+
+    def test_atan2(self, spark):
+        data = [
+            (1, 1),
+            (0, 0),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn", "secondColumn"])
+
+        # Both columns
+        df2 = df.withColumn("atan2_value", F.atan2(F.col("firstColumn"), "secondColumn"))
+        res = df2.select("atan2_value").collect()
+        round(res[0].atan2_value, 2) == 0.79
+        res[1].atan2_value == 0
+
+        # Both literals
+        df2 = df.withColumn("atan2_value_lit", F.atan2(1, 1))
+        res = df2.select("atan2_value_lit").collect()
+        round(res[0].atan2_value_lit, 2) == 0.79
+        round(res[1].atan2_value_lit, 2) == 0.79
+
+        # One literal, one column
+        df2 = df.withColumn("atan2_value_lit_col", F.atan2(1.0, F.col("secondColumn")))
+        res = df2.select("atan2_value_lit_col").collect()
+        round(res[0].atan2_value_lit_col, 2) == 0.79
+        res[1].atan2_value_lit_col == 0
+
+    def test_tan(self, spark):
+        data = [
+            (0,),
+            (1,),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn"])
+        df = df.withColumn("tan_value", F.tan(F.col("firstColumn")))
+        res = df.select("tan_value").collect()
+        res[0].tan_value == 0
+        round(res[1].tan_value, 2) == 1.56
+
+    def test_round(self, spark):
+        data = [
+            (11.15,),
+            (2.9,),
+            # Test with this that HALF_UP rounding method is used and not HALF_EVEN
+            (2.5,),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn"])
+        df = (
+            df.withColumn("round_value", F.round("firstColumn"))
+            .withColumn("round_value_1", F.round(F.col("firstColumn"), 1))
+            .withColumn("round_value_minus_1", F.round("firstColumn", -1))
+        )
+        res = df.select("round_value", "round_value_1", "round_value_minus_1").collect()
+        assert res == [
+            Row(round_value=11, round_value_1=11.2, round_value_minus_1=10),
+            Row(round_value=3, round_value_1=2.9, round_value_minus_1=0),
+            Row(round_value=3, round_value_1=2.5, round_value_minus_1=0),
+        ]
+
+    def test_bround(self, spark):
+        data = [
+            (11.15,),
+            (2.9,),
+            # Test with this that HALF_EVEN rounding method is used and not HALF_UP
+            (2.5,),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn"])
+        df = (
+            df.withColumn("round_value", F.bround(F.col("firstColumn")))
+            .withColumn("round_value_1", F.bround(F.col("firstColumn"), 1))
+            .withColumn("round_value_minus_1", F.bround(F.col("firstColumn"), -1))
+        )
+        res = df.select("round_value", "round_value_1", "round_value_minus_1").collect()
+        assert res == [
+            Row(round_value=11, round_value_1=11.2, round_value_minus_1=10),
+            Row(round_value=3, round_value_1=2.9, round_value_minus_1=0),
+            Row(round_value=2, round_value_1=2.5, round_value_minus_1=0),
+        ]
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_functions_string.py b/tools/pythonpkg/tests/fast/spark/test_spark_functions_string.py
index 791b48f01643..694ed89f8aa6 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_functions_string.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_functions_string.py
@@ -1,8 +1,8 @@
 import pytest
 
 _ = pytest.importorskip("duckdb.experimental.spark")
-from duckdb.experimental.spark.sql import functions as F
-from duckdb.experimental.spark.sql.types import Row
+from spark_namespace.sql import functions as F
+from spark_namespace.sql.types import Row
 
 
 class TestSparkFunctionsString(object):
@@ -83,3 +83,29 @@ def test_rtrim(self, spark):
             Row(rtrimmed=" firstRowFirstColumn"),
             Row(rtrimmed=" 2ndRowFirstColumn"),
         ]
+
+    def test_endswith(self, spark):
+        data = [
+            ("firstRowFirstColumn", "Column"),
+            ("2ndRowFirstColumn", "column"),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn", "secondColumn"])
+        df = df.withColumn("endswith", F.endswith(F.col("firstColumn"), F.col("secondColumn")))
+        res = df.select("endswith").collect()
+        assert res == [
+            Row(endswith=True),
+            Row(endswith=False),
+        ]
+
+    def test_startswith(self, spark):
+        data = [
+            ("firstRowFirstColumn", "irst"),
+            ("2ndRowFirstColumn", "2nd"),
+        ]
+        df = spark.createDataFrame(data, ["firstColumn", "secondColumn"])
+        df = df.withColumn("startswith", F.startswith(F.col("firstColumn"), F.col("secondColumn")))
+        res = df.select("startswith").collect()
+        assert res == [
+            Row(startswith=False),
+            Row(startswith=True),
+        ]
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_group_by.py b/tools/pythonpkg/tests/fast/spark/test_spark_group_by.py
index 5b0fe0305291..25cb64a12a27 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_group_by.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_group_by.py
@@ -2,7 +2,8 @@
 
 _ = pytest.importorskip("duckdb.experimental.spark")
 
-from duckdb.experimental.spark.sql.types import (
+from spark_namespace import USE_ACTUAL_SPARK
+from spark_namespace.sql.types import (
     LongType,
     StructType,
     BooleanType,
@@ -14,8 +15,19 @@
     ArrayType,
     MapType,
 )
-from duckdb.experimental.spark.sql.functions import col, struct, when, lit, array_contains
-from duckdb.experimental.spark.sql.functions import sum, avg, max, min, mean, count
+from spark_namespace.sql.functions import col, struct, when, lit, array_contains
+from spark_namespace.sql.functions import (
+    sum,
+    avg,
+    max,
+    min,
+    mean,
+    count,
+    any_value,
+    approx_count_distinct,
+    covar_pop,
+    covar_samp,
+)
 
 
 class TestDataFrameGroupBy(object):
@@ -70,10 +82,10 @@ def test_group_by(self, spark):
 
         df2 = df.groupBy("department").mean("salary").sort("department")
         res = df2.collect()
-        assert (
-            str(res)
-            == "[Row(department='Finance', mean(salary)=87750.0), Row(department='Marketing', mean(salary)=85500.0), Row(department='Sales', mean(salary)=85666.66666666667)]"
-        )
+        expected_res_str = "[Row(department='Finance', mean(salary)=87750.0), Row(department='Marketing', mean(salary)=85500.0), Row(department='Sales', mean(salary)=85666.66666666667)]"
+        if USE_ACTUAL_SPARK:
+            expected_res_str = expected_res_str.replace("mean(", "avg(")
+        assert str(res) == expected_res_str
 
         df2 = df.groupBy("department", "state").sum("salary", "bonus").sort("department", "state")
         res = df2.collect()
@@ -89,13 +101,15 @@ def test_group_by(self, spark):
                 avg("salary").alias("avg_salary"),
                 sum("bonus").alias("sum_bonus"),
                 max("bonus").alias("max_bonus"),
+                any_value("state").alias("any_state"),
+                approx_count_distinct("state").alias("distinct_state"),
             )
             .sort("department")
         )
         res = df2.collect()
         assert (
             str(res)
-            == "[Row(department='Finance', sum_salary=351000, avg_salary=87750.0, sum_bonus=81000, max_bonus=24000), Row(department='Marketing', sum_salary=171000, avg_salary=85500.0, sum_bonus=39000, max_bonus=21000), Row(department='Sales', sum_salary=257000, avg_salary=85666.66666666667, sum_bonus=53000, max_bonus=23000)]"
+            == "[Row(department='Finance', sum_salary=351000, avg_salary=87750.0, sum_bonus=81000, max_bonus=24000, any_state='CA', distinct_state=2), Row(department='Marketing', sum_salary=171000, avg_salary=85500.0, sum_bonus=39000, max_bonus=21000, any_state='CA', distinct_state=2), Row(department='Sales', sum_salary=257000, avg_salary=85666.66666666667, sum_bonus=53000, max_bonus=23000, any_state='NY', distinct_state=2)]"
         )
 
         df2 = (
@@ -105,6 +119,7 @@ def test_group_by(self, spark):
                 avg("salary").alias("avg_salary"),
                 sum("bonus").alias("sum_bonus"),
                 max("bonus").alias("max_bonus"),
+                any_value("state").alias("any_state"),
             )
             .where(col("sum_bonus") >= 50000)
             .sort("department")
@@ -113,9 +128,25 @@ def test_group_by(self, spark):
         print(str(res))
         assert (
             str(res)
-            == "[Row(department='Finance', sum_salary=351000, avg_salary=87750.0, sum_bonus=81000, max_bonus=24000), Row(department='Sales', sum_salary=257000, avg_salary=85666.66666666667, sum_bonus=53000, max_bonus=23000)]"
+            == "[Row(department='Finance', sum_salary=351000, avg_salary=87750.0, sum_bonus=81000, max_bonus=24000, any_state='CA'), Row(department='Sales', sum_salary=257000, avg_salary=85666.66666666667, sum_bonus=53000, max_bonus=23000, any_state='NY')]"
         )
 
+        df = spark.createDataFrame(
+            [
+                (1, 1, "a"),
+                (2, 2, "a"),
+                (2, 3, "a"),
+                (5, 4, "a"),
+            ],
+            schema=["age", "other_variable", "group"],
+        )
+        df2 = df.groupBy("group").agg(
+            covar_pop("age", "other_variable").alias("covar_pop"),
+            covar_samp("age", "other_variable").alias("covar_samp"),
+        )
+        res = df2.collect()
+        assert str(res) == "[Row(group='a', covar_pop=1.5, covar_samp=2.0)]"
+
     def test_group_by_empty(self, spark):
         df = spark.createDataFrame(
             [(2, 1.0, "1"), (2, 2.0, "2"), (2, 3.0, "3"), (5, 4.0, "4")], schema=["age", "extra", "name"]
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_intersect.py b/tools/pythonpkg/tests/fast/spark/test_spark_intersect.py
new file mode 100644
index 000000000000..7fd97d40ab8b
--- /dev/null
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_intersect.py
@@ -0,0 +1,40 @@
+import platform
+import pytest
+
+_ = pytest.importorskip("duckdb.experimental.spark")
+
+from duckdb.experimental.spark.sql.types import Row
+from duckdb.experimental.spark.sql.functions import col
+
+
+@pytest.fixture
+def df(spark):
+    return spark.createDataFrame([("a", 1), ("a", 1), ("b", 3), ("c", 4)], ["C1", "C2"])
+
+
+@pytest.fixture
+def df2(spark):
+    return spark.createDataFrame([("a", 1), ("a", 1), ("b", 3)], ["C1", "C2"])
+
+
+class TestDataFrameIntersect:
+    def test_intersect(self, spark, df, df2):
+
+        df3 = df.intersect(df2).sort(df.C1)
+        res = df3.collect()
+
+        assert res == [
+            Row(C1="a", C2=1),
+            Row(C1="b", C2=3),
+        ]
+
+    def test_intersect_all(self, spark, df, df2):
+
+        df3 = df.intersectAll(df2).sort(df.C1)
+        res = df3.collect()
+
+        assert res == [
+            Row(C1="a", C2=1),
+            Row(C1="a", C2=1),
+            Row(C1="b", C2=3),
+        ]
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_join.py b/tools/pythonpkg/tests/fast/spark/test_spark_join.py
index f1a4a6758aa4..ef028e4420df 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_join.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_join.py
@@ -2,7 +2,7 @@
 
 _ = pytest.importorskip("duckdb.experimental.spark")
 
-from duckdb.experimental.spark.sql.types import (
+from spark_namespace.sql.types import (
     LongType,
     StructType,
     BooleanType,
@@ -14,8 +14,8 @@
     ArrayType,
     MapType,
 )
-from duckdb.experimental.spark.sql.functions import col, struct, when, lit, array_contains
-from duckdb.experimental.spark.sql.functions import sum, avg, max, min, mean, count
+from spark_namespace.sql.functions import col, struct, when, lit, array_contains
+from spark_namespace.sql.functions import sum, avg, max, min, mean, count
 
 
 @pytest.fixture
@@ -103,199 +103,227 @@ def test_inner_join(self, dataframe_a, dataframe_b):
                 dept_id=40,
             ),
         ]
-        assert res == expected
+        assert sorted(res) == sorted(expected)
 
     @pytest.mark.parametrize('how', ['outer', 'fullouter', 'full', 'full_outer'])
     def test_outer_join(self, dataframe_a, dataframe_b, how):
         df = dataframe_a.join(dataframe_b, dataframe_a.emp_dept_id == dataframe_b.dept_id, how)
         df = df.sort(*df.columns)
         res1 = df.collect()
-        assert res1 == [
-            Row(
-                emp_id=1,
-                name='Smith',
-                superior_emp_id=-1,
-                year_joined='2018',
-                emp_dept_id='10',
-                gender='M',
-                salary=3000,
-                dept_name='Finance',
-                dept_id=10,
-            ),
-            Row(
-                emp_id=2,
-                name='Rose',
-                superior_emp_id=1,
-                year_joined='2010',
-                emp_dept_id='20',
-                gender='M',
-                salary=4000,
-                dept_name='Marketing',
-                dept_id=20,
-            ),
-            Row(
-                emp_id=3,
-                name='Williams',
-                superior_emp_id=1,
-                year_joined='2010',
-                emp_dept_id='10',
-                gender='M',
-                salary=1000,
-                dept_name='Finance',
-                dept_id=10,
-            ),
-            Row(
-                emp_id=4,
-                name='Jones',
-                superior_emp_id=2,
-                year_joined='2005',
-                emp_dept_id='10',
-                gender='F',
-                salary=2000,
-                dept_name='Finance',
-                dept_id=10,
-            ),
-            Row(
-                emp_id=5,
-                name='Brown',
-                superior_emp_id=2,
-                year_joined='2010',
-                emp_dept_id='40',
-                gender='',
-                salary=-1,
-                dept_name='IT',
-                dept_id=40,
-            ),
-            Row(
-                emp_id=6,
-                name='Brown',
-                superior_emp_id=2,
-                year_joined='2010',
-                emp_dept_id='50',
-                gender='',
-                salary=-1,
-                dept_name=None,
-                dept_id=None,
-            ),
-            Row(
-                emp_id=None,
-                name=None,
-                superior_emp_id=None,
-                year_joined=None,
-                emp_dept_id=None,
-                gender=None,
-                salary=None,
-                dept_name='Sales',
-                dept_id=30,
-            ),
-        ]
+        assert sorted(res1, key=lambda x: x.emp_id or 0) == sorted(
+            [
+                Row(
+                    emp_id=1,
+                    name='Smith',
+                    superior_emp_id=-1,
+                    year_joined='2018',
+                    emp_dept_id='10',
+                    gender='M',
+                    salary=3000,
+                    dept_name='Finance',
+                    dept_id=10,
+                ),
+                Row(
+                    emp_id=2,
+                    name='Rose',
+                    superior_emp_id=1,
+                    year_joined='2010',
+                    emp_dept_id='20',
+                    gender='M',
+                    salary=4000,
+                    dept_name='Marketing',
+                    dept_id=20,
+                ),
+                Row(
+                    emp_id=3,
+                    name='Williams',
+                    superior_emp_id=1,
+                    year_joined='2010',
+                    emp_dept_id='10',
+                    gender='M',
+                    salary=1000,
+                    dept_name='Finance',
+                    dept_id=10,
+                ),
+                Row(
+                    emp_id=4,
+                    name='Jones',
+                    superior_emp_id=2,
+                    year_joined='2005',
+                    emp_dept_id='10',
+                    gender='F',
+                    salary=2000,
+                    dept_name='Finance',
+                    dept_id=10,
+                ),
+                Row(
+                    emp_id=5,
+                    name='Brown',
+                    superior_emp_id=2,
+                    year_joined='2010',
+                    emp_dept_id='40',
+                    gender='',
+                    salary=-1,
+                    dept_name='IT',
+                    dept_id=40,
+                ),
+                Row(
+                    emp_id=6,
+                    name='Brown',
+                    superior_emp_id=2,
+                    year_joined='2010',
+                    emp_dept_id='50',
+                    gender='',
+                    salary=-1,
+                    dept_name=None,
+                    dept_id=None,
+                ),
+                Row(
+                    emp_id=None,
+                    name=None,
+                    superior_emp_id=None,
+                    year_joined=None,
+                    emp_dept_id=None,
+                    gender=None,
+                    salary=None,
+                    dept_name='Sales',
+                    dept_id=30,
+                ),
+            ],
+            key=lambda x: x.emp_id or 0,
+        )
 
     @pytest.mark.parametrize('how', ['right', 'rightouter', 'right_outer'])
     def test_right_join(self, dataframe_a, dataframe_b, how):
         df = dataframe_a.join(dataframe_b, dataframe_a.emp_dept_id == dataframe_b.dept_id, how)
         df = df.sort(*df.columns)
         res = df.collect()
-        assert res == [
-            Row(
-                emp_id=1,
-                name='Smith',
-                superior_emp_id=-1,
-                year_joined='2018',
-                emp_dept_id='10',
-                gender='M',
-                salary=3000,
-                dept_name='Finance',
-                dept_id=10,
-            ),
-            Row(
-                emp_id=2,
-                name='Rose',
-                superior_emp_id=1,
-                year_joined='2010',
-                emp_dept_id='20',
-                gender='M',
-                salary=4000,
-                dept_name='Marketing',
-                dept_id=20,
-            ),
-            Row(
-                emp_id=3,
-                name='Williams',
-                superior_emp_id=1,
-                year_joined='2010',
-                emp_dept_id='10',
-                gender='M',
-                salary=1000,
-                dept_name='Finance',
-                dept_id=10,
-            ),
-            Row(
-                emp_id=4,
-                name='Jones',
-                superior_emp_id=2,
-                year_joined='2005',
-                emp_dept_id='10',
-                gender='F',
-                salary=2000,
-                dept_name='Finance',
-                dept_id=10,
-            ),
-            Row(
-                emp_id=5,
-                name='Brown',
-                superior_emp_id=2,
-                year_joined='2010',
-                emp_dept_id='40',
-                gender='',
-                salary=-1,
-                dept_name='IT',
-                dept_id=40,
-            ),
-            Row(
-                emp_id=None,
-                name=None,
-                superior_emp_id=None,
-                year_joined=None,
-                emp_dept_id=None,
-                gender=None,
-                salary=None,
-                dept_name='Sales',
-                dept_id=30,
-            ),
-        ]
+        assert sorted(res, key=lambda x: x.emp_id or 0) == sorted(
+            [
+                Row(
+                    emp_id=1,
+                    name='Smith',
+                    superior_emp_id=-1,
+                    year_joined='2018',
+                    emp_dept_id='10',
+                    gender='M',
+                    salary=3000,
+                    dept_name='Finance',
+                    dept_id=10,
+                ),
+                Row(
+                    emp_id=2,
+                    name='Rose',
+                    superior_emp_id=1,
+                    year_joined='2010',
+                    emp_dept_id='20',
+                    gender='M',
+                    salary=4000,
+                    dept_name='Marketing',
+                    dept_id=20,
+                ),
+                Row(
+                    emp_id=3,
+                    name='Williams',
+                    superior_emp_id=1,
+                    year_joined='2010',
+                    emp_dept_id='10',
+                    gender='M',
+                    salary=1000,
+                    dept_name='Finance',
+                    dept_id=10,
+                ),
+                Row(
+                    emp_id=4,
+                    name='Jones',
+                    superior_emp_id=2,
+                    year_joined='2005',
+                    emp_dept_id='10',
+                    gender='F',
+                    salary=2000,
+                    dept_name='Finance',
+                    dept_id=10,
+                ),
+                Row(
+                    emp_id=5,
+                    name='Brown',
+                    superior_emp_id=2,
+                    year_joined='2010',
+                    emp_dept_id='40',
+                    gender='',
+                    salary=-1,
+                    dept_name='IT',
+                    dept_id=40,
+                ),
+                Row(
+                    emp_id=None,
+                    name=None,
+                    superior_emp_id=None,
+                    year_joined=None,
+                    emp_dept_id=None,
+                    gender=None,
+                    salary=None,
+                    dept_name='Sales',
+                    dept_id=30,
+                ),
+            ],
+            key=lambda x: x.emp_id or 0,
+        )
 
     @pytest.mark.parametrize('how', ['semi', 'leftsemi', 'left_semi'])
     def test_semi_join(self, dataframe_a, dataframe_b, how):
         df = dataframe_a.join(dataframe_b, dataframe_a.emp_dept_id == dataframe_b.dept_id, how)
         df = df.sort(*df.columns)
         res = df.collect()
-        assert res == [
-            Row(
-                emp_id=1,
-                name='Smith',
-                superior_emp_id=-1,
-                year_joined='2018',
-                emp_dept_id='10',
-                gender='M',
-                salary=3000,
-            ),
-            Row(
-                emp_id=2, name='Rose', superior_emp_id=1, year_joined='2010', emp_dept_id='20', gender='M', salary=4000
-            ),
-            Row(
-                emp_id=3,
-                name='Williams',
-                superior_emp_id=1,
-                year_joined='2010',
-                emp_dept_id='10',
-                gender='M',
-                salary=1000,
-            ),
-            Row(
-                emp_id=4, name='Jones', superior_emp_id=2, year_joined='2005', emp_dept_id='10', gender='F', salary=2000
-            ),
-            Row(emp_id=5, name='Brown', superior_emp_id=2, year_joined='2010', emp_dept_id='40', gender='', salary=-1),
-        ]
+        assert sorted(res) == sorted(
+            [
+                Row(
+                    emp_id=1,
+                    name='Smith',
+                    superior_emp_id=-1,
+                    year_joined='2018',
+                    emp_dept_id='10',
+                    gender='M',
+                    salary=3000,
+                ),
+                Row(
+                    emp_id=2,
+                    name='Rose',
+                    superior_emp_id=1,
+                    year_joined='2010',
+                    emp_dept_id='20',
+                    gender='M',
+                    salary=4000,
+                ),
+                Row(
+                    emp_id=3,
+                    name='Williams',
+                    superior_emp_id=1,
+                    year_joined='2010',
+                    emp_dept_id='10',
+                    gender='M',
+                    salary=1000,
+                ),
+                Row(
+                    emp_id=4,
+                    name='Jones',
+                    superior_emp_id=2,
+                    year_joined='2005',
+                    emp_dept_id='10',
+                    gender='F',
+                    salary=2000,
+                ),
+                Row(
+                    emp_id=5,
+                    name='Brown',
+                    superior_emp_id=2,
+                    year_joined='2010',
+                    emp_dept_id='40',
+                    gender='',
+                    salary=-1,
+                ),
+            ]
+        )
 
     @pytest.mark.parametrize('how', ['anti', 'leftanti', 'left_anti'])
     def test_anti_join(self, dataframe_a, dataframe_b, how):
@@ -321,10 +349,34 @@ def test_self_join(self, dataframe_a):
         )
         df = df.orderBy(*df.columns)
         res = df.collect()
-        assert res == [
-            Row(emp_id=2, name='Rose', superior_emp_id=1, superior_emp_name='Smith'),
-            Row(emp_id=3, name='Williams', superior_emp_id=1, superior_emp_name='Smith'),
-            Row(emp_id=4, name='Jones', superior_emp_id=2, superior_emp_name='Rose'),
-            Row(emp_id=5, name='Brown', superior_emp_id=2, superior_emp_name='Rose'),
-            Row(emp_id=6, name='Brown', superior_emp_id=2, superior_emp_name='Rose'),
-        ]
+        assert sorted(res, key=lambda x: x.emp_id) == sorted(
+            [
+                Row(emp_id=2, name='Rose', superior_emp_id=1, superior_emp_name='Smith'),
+                Row(emp_id=3, name='Williams', superior_emp_id=1, superior_emp_name='Smith'),
+                Row(emp_id=4, name='Jones', superior_emp_id=2, superior_emp_name='Rose'),
+                Row(emp_id=5, name='Brown', superior_emp_id=2, superior_emp_name='Rose'),
+                Row(emp_id=6, name='Brown', superior_emp_id=2, superior_emp_name='Rose'),
+            ],
+            key=lambda x: x.emp_id,
+        )
+
+    def test_cross_join(self, spark):
+        data1 = [(1, "Carol"), (2, "Alice"), (3, "Dave")]
+        data2 = [(4, "A"), (5, "B")]
+        df1 = spark.createDataFrame(data1, ["age", "name"])
+        df2 = spark.createDataFrame(data2, ["id", "rank"])
+
+        df = df1.crossJoin(df2)
+
+        res = df.orderBy("rank", "age").collect()
+
+        assert sorted(res) == sorted(
+            [
+                Row(age=1, name="Carol", id=4, rank="A"),
+                Row(age=2, name="Alice", id=4, rank="A"),
+                Row(age=3, name="Dave", id=4, rank="A"),
+                Row(age=1, name="Carol", id=5, rank="B"),
+                Row(age=2, name="Alice", id=5, rank="B"),
+                Row(age=3, name="Dave", id=5, rank="B"),
+            ]
+        )
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_limit.py b/tools/pythonpkg/tests/fast/spark/test_spark_limit.py
index 2d40ea21a2f0..c00496a04ab5 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_limit.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_limit.py
@@ -2,7 +2,7 @@
 
 _ = pytest.importorskip("duckdb.experimental.spark")
 
-from duckdb.experimental.spark.sql.types import (
+from spark_namespace.sql.types import (
     Row,
 )
 
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_order_by.py b/tools/pythonpkg/tests/fast/spark/test_spark_order_by.py
index 26f4211609ab..4cf83493eea7 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_order_by.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_order_by.py
@@ -2,7 +2,7 @@
 
 _ = pytest.importorskip("duckdb.experimental.spark")
 
-from duckdb.experimental.spark.sql.types import (
+from spark_namespace.sql.types import (
     LongType,
     StructType,
     BooleanType,
@@ -14,7 +14,7 @@
     ArrayType,
     MapType,
 )
-from duckdb.experimental.spark.sql.functions import col, struct, when, lit, array_contains
+from spark_namespace.sql.functions import col, struct, when, lit, array_contains
 import duckdb
 import re
 
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_pandas_dataframe.py b/tools/pythonpkg/tests/fast/spark/test_spark_pandas_dataframe.py
index 356b12d02ecb..dcec77a8656e 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_pandas_dataframe.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_pandas_dataframe.py
@@ -3,7 +3,7 @@
 _ = pytest.importorskip("duckdb.experimental.spark")
 pd = pytest.importorskip("pandas")
 
-from duckdb.experimental.spark.sql.types import (
+from spark_namespace.sql.types import (
     LongType,
     StructType,
     BooleanType,
@@ -15,7 +15,7 @@
     ArrayType,
     MapType,
 )
-from duckdb.experimental.spark.sql.functions import col, struct, when
+from spark_namespace.sql.functions import col, struct, when
 import duckdb
 import re
 from pandas.testing import assert_frame_equal
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_readcsv.py b/tools/pythonpkg/tests/fast/spark/test_spark_readcsv.py
index 5c8d159a9e68..8e6c0515c5c0 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_readcsv.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_readcsv.py
@@ -2,7 +2,8 @@
 
 _ = pytest.importorskip("duckdb.experimental.spark")
 
-from duckdb.experimental.spark.sql.types import Row
+from spark_namespace.sql.types import Row
+from spark_namespace import USE_ACTUAL_SPARK
 import textwrap
 
 
@@ -22,4 +23,9 @@ def test_read_csv(self, spark, tmp_path):
         file_path = file_path.as_posix()
         df = spark.read.csv(file_path)
         res = df.collect()
-        assert res == [Row(column0=1, column1=2), Row(column0=3, column1=4), Row(column0=5, column1=6)]
+
+        expected_res = sorted([Row(column0=1, column1=2), Row(column0=3, column1=4), Row(column0=5, column1=6)])
+        if USE_ACTUAL_SPARK:
+            # Convert all values to strings as this is how Spark reads them by default
+            expected_res = [Row(column0=str(row.column0), column1=str(row.column1)) for row in expected_res]
+        assert sorted(res) == expected_res
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_readjson.py b/tools/pythonpkg/tests/fast/spark/test_spark_readjson.py
index e9019be68ad6..a6ad05f0ea9c 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_readjson.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_readjson.py
@@ -2,7 +2,7 @@
 
 _ = pytest.importorskip("duckdb.experimental.spark")
 
-from duckdb.experimental.spark.sql.types import Row
+from spark_namespace.sql.types import Row
 import textwrap
 import duckdb
 
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_readparquet.py b/tools/pythonpkg/tests/fast/spark/test_spark_readparquet.py
index 6c8d68cd66a8..a08ab16de8af 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_readparquet.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_readparquet.py
@@ -2,7 +2,7 @@
 
 _ = pytest.importorskip("duckdb.experimental.spark")
 
-from duckdb.experimental.spark.sql.types import Row
+from spark_namespace.sql.types import Row
 import textwrap
 import duckdb
 
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_runtime_config.py b/tools/pythonpkg/tests/fast/spark/test_spark_runtime_config.py
index b152d2d71957..5e93ed631f59 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_runtime_config.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_runtime_config.py
@@ -2,12 +2,17 @@
 
 _ = pytest.importorskip("duckdb.experimental.spark")
 
+from spark_namespace import USE_ACTUAL_SPARK
+
 
 class TestSparkRuntimeConfig(object):
     def test_spark_runtime_config(self, spark):
         # This fetches the internal runtime config from the session
         spark.conf
 
+    @pytest.mark.skipif(
+        USE_ACTUAL_SPARK, reason="Getting an error with our local PySpark setup. Unclear why but not a priority."
+    )
     def test_spark_runtime_config_set(self, spark):
         # Set Config
         with pytest.raises(NotImplementedError):
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_session.py b/tools/pythonpkg/tests/fast/spark/test_spark_session.py
index a4a75a4fa0a3..7c338898fe7a 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_session.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_session.py
@@ -2,10 +2,11 @@
 from duckdb.experimental.spark.exception import (
     ContributionsAcceptedError,
 )
-from duckdb.experimental.spark.sql.types import Row
+from spark_namespace.sql.types import Row
+from spark_namespace import USE_ACTUAL_SPARK
 
 _ = pytest.importorskip("duckdb.experimental.spark")
-from duckdb.experimental.spark.sql import SparkSession
+from spark_namespace.sql import SparkSession
 
 
 class TestSparkSession(object):
@@ -45,6 +46,7 @@ def test_hive_support(self):
             .getOrCreate()
         )
 
+    @pytest.mark.skipif(USE_ACTUAL_SPARK, reason="Different version numbers")
     def test_version(self, spark):
         version = spark.version
         assert version == '1.0.0'
@@ -72,6 +74,9 @@ def test_stop_context(self, spark):
         context = spark.sparkContext
         spark.stop()
 
+    @pytest.mark.skipif(
+        USE_ACTUAL_SPARK, reason="Can't create table with the local PySpark setup in the CI/CD pipeline"
+    )
     def test_table(self, spark):
         spark.sql('create table tbl(a varchar(10))')
         df = spark.table('tbl')
@@ -85,9 +90,10 @@ def test_range(self, spark):
         assert res_2 == [Row(id=3), Row(id=5), Row(id=7), Row(id=9)]
         assert res_3 == [Row(id=3), Row(id=4), Row(id=5)]
 
-        with pytest.raises(ContributionsAcceptedError):
-            # partition size is not supported
-            spark.range(0, 10, 2, 2)
+        if not USE_ACTUAL_SPARK:
+            with pytest.raises(ContributionsAcceptedError):
+                # partition size is not supported
+                spark.range(0, 10, 2, 2)
 
     def test_udf(self, spark):
         udf_registration = spark.udf
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_to_csv.py b/tools/pythonpkg/tests/fast/spark/test_spark_to_csv.py
index e51384e59081..5048e5797d4d 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_to_csv.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_to_csv.py
@@ -5,9 +5,19 @@
 
 _ = pytest.importorskip("duckdb.experimental.spark")
 
-from duckdb.experimental.spark.sql import SparkSession as session
+from spark_namespace import USE_ACTUAL_SPARK
+
+if USE_ACTUAL_SPARK:
+    pytest.skip(
+        "Skipping these tests as right now,"
+        + " there are too many differences between DuckDB and PySpark when reading and writing CSV files."
+        + " For example, PySpark does read back numbers as strings and has the option 'headers' set to False.",
+        allow_module_level=True,
+    )
+
 from duckdb import connect, InvalidInputException, read_csv
 from conftest import NumpyPandas, ArrowPandas, getTimeSeriesData
+from spark_namespace import USE_ACTUAL_SPARK
 import pandas._testing as tm
 import datetime
 import csv
@@ -186,6 +196,10 @@ def test_to_csv_encoding_correct(self, pandas_df_strings, spark, tmp_path):
         csv_rel = spark.read.csv(temp_file_name)
         assert df.collect() == csv_rel.collect()
 
+    @pytest.mark.skipif(
+        USE_ACTUAL_SPARK,
+        reason="This test uses DuckDB to read the CSV. However, this does not work if Spark created it as Spark creates a folder instead of a single file.",
+    )
     def test_compression_gzip(self, pandas_df_strings, spark, tmp_path):
         temp_file_name = os.path.join(tmp_path, "temp_file.csv")
         df = spark.createDataFrame(pandas_df_strings)
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_to_parquet.py b/tools/pythonpkg/tests/fast/spark/test_spark_to_parquet.py
index 313bff391ea4..68a10f65805d 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_to_parquet.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_to_parquet.py
@@ -28,7 +28,7 @@ def test_basic_to_parquet(self, df, spark, tmp_path):
 
         csv_rel = spark.read.parquet(temp_file_name)
 
-        assert df.collect() == csv_rel.collect()
+        assert sorted(df.collect()) == sorted(csv_rel.collect())
 
     def test_compressed_to_parquet(self, df, spark, tmp_path):
         temp_file_name = os.path.join(tmp_path, "temp_file.parquet")
@@ -37,4 +37,4 @@ def test_compressed_to_parquet(self, df, spark, tmp_path):
 
         csv_rel = spark.read.parquet(temp_file_name)
 
-        assert df.collect() == csv_rel.collect()
+        assert sorted(df.collect()) == sorted(csv_rel.collect())
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_transform.py b/tools/pythonpkg/tests/fast/spark/test_spark_transform.py
index 622a6102fa12..83e219a51640 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_transform.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_transform.py
@@ -2,7 +2,7 @@
 
 _ = pytest.importorskip("duckdb.experimental.spark")
 
-from duckdb.experimental.spark.sql.types import (
+from spark_namespace.sql.types import (
     LongType,
     StructType,
     BooleanType,
@@ -14,8 +14,8 @@
     ArrayType,
     MapType,
 )
-from duckdb.experimental.spark.sql.functions import col, struct, when, lit, array_contains
-from duckdb.experimental.spark.sql.functions import sum, avg, max, min, mean, count
+from spark_namespace.sql.functions import col, struct, when, lit, array_contains
+from spark_namespace.sql.functions import sum, avg, max, min, mean, count
 
 
 @pytest.fixture
@@ -46,7 +46,7 @@ def df(spark):
 class TestDataFrameUnion(object):
     def test_transform(self, spark, df):
         # Custom transformation 1
-        from duckdb.experimental.spark.sql.functions import upper
+        from spark_namespace.sql.functions import upper
 
         def to_upper_str_columns(df):
             return df.withColumn("CourseName", upper(df.CourseName))
@@ -72,6 +72,6 @@ def apply_discount(df):
     # https://sparkbyexamples.com/pyspark/pyspark-transform-function/
     @pytest.mark.skip(reason='LambdaExpressions are currently under development, waiting til that is finished')
     def test_transform_function(self, spark, array_df):
-        from duckdb.experimental.spark.sql.functions import upper, transform
+        from spark_namespace.sql.functions import upper, transform
 
         df.select(transform("Languages1", lambda x: upper(x)).alias("languages1")).show()
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_types.py b/tools/pythonpkg/tests/fast/spark/test_spark_types.py
index e975f0dcab1c..980e24a258a9 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_types.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_types.py
@@ -1,8 +1,16 @@
 import pytest
 
 _ = pytest.importorskip("duckdb.experimental.spark")
-from duckdb.experimental.spark.sql.types import Row
-from duckdb.experimental.spark.sql.types import (
+
+from spark_namespace import USE_ACTUAL_SPARK
+
+if USE_ACTUAL_SPARK:
+    pytest.skip(
+        "Skipping these tests as they use test_all_types() which is specific to DuckDB", allow_module_level=True
+    )
+
+from spark_namespace.sql.types import Row
+from spark_namespace.sql.types import (
     StringType,
     BinaryType,
     BitstringType,
@@ -47,12 +55,12 @@ def test_all_types_schema(self, spark):
 				medium_enum,
 				large_enum,
 				'union',
-				fixed_int_array, 
-				fixed_varchar_array, 
+				fixed_int_array,
+				fixed_varchar_array,
 				fixed_nested_int_array,
-            	fixed_nested_varchar_array, 
-            	fixed_struct_array, 
-            	struct_of_fixed_array, 
+            	fixed_nested_varchar_array,
+            	fixed_struct_array,
+            	struct_of_fixed_array,
             	fixed_array_of_int_list,
                 list_of_fixed_int_array,
                 varint
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_union.py b/tools/pythonpkg/tests/fast/spark/test_spark_union.py
index 2399785fafaf..ea889e05141d 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_union.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_union.py
@@ -3,8 +3,8 @@
 
 _ = pytest.importorskip("duckdb.experimental.spark")
 
-from duckdb.experimental.spark.sql.types import Row
-from duckdb.experimental.spark.sql.functions import col
+from spark_namespace.sql.types import Row
+from spark_namespace.sql.functions import col
 
 
 @pytest.fixture
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_union_by_name.py b/tools/pythonpkg/tests/fast/spark/test_spark_union_by_name.py
index 70b8d59abcbb..08f3c62b682b 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_union_by_name.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_union_by_name.py
@@ -2,7 +2,8 @@
 
 _ = pytest.importorskip("duckdb.experimental.spark")
 
-from duckdb.experimental.spark.sql.types import (
+
+from spark_namespace.sql.types import (
     LongType,
     StructType,
     BooleanType,
@@ -14,8 +15,8 @@
     ArrayType,
     MapType,
 )
-from duckdb.experimental.spark.sql.functions import col, struct, when, lit, array_contains
-from duckdb.experimental.spark.sql.functions import sum, avg, max, min, mean, count
+from spark_namespace.sql.functions import col, struct, when, lit, array_contains
+from spark_namespace.sql.functions import sum, avg, max, min, mean, count
 
 
 @pytest.fixture
@@ -32,10 +33,33 @@ def df2(spark):
     yield dataframe
 
 
-# https://sparkbyexamples.com/pyspark/pyspark-unionbyname/
-@pytest.mark.skip(reason="union_by_name is not supported in the Relation API yet")
 class TestDataFrameUnion(object):
     def test_union_by_name(self, df1, df2):
         rel = df1.unionByName(df2)
         res = rel.collect()
-        print(res)
+        expected = [
+            Row(name='James', id=34),
+            Row(name='Michael', id=56),
+            Row(name='Robert', id=30),
+            Row(name='Maria', id=24),
+            Row(name='James', id=34),
+            Row(name='Maria', id=45),
+            Row(name='Jen', id=45),
+            Row(name='Jeff', id=34),
+        ]
+        assert res == expected
+
+    def test_union_by_name_allow_missing_cols(self, df1, df2):
+        rel = df1.unionByName(df2.drop("id"), allowMissingColumns=True)
+        res = rel.collect()
+        expected = [
+            Row(name='James', id=34),
+            Row(name='Michael', id=56),
+            Row(name='Robert', id=30),
+            Row(name='Maria', id=24),
+            Row(name='James', id=None),
+            Row(name='Maria', id=None),
+            Row(name='Jen', id=None),
+            Row(name='Jeff', id=None),
+        ]
+        assert res == expected
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_with_column.py b/tools/pythonpkg/tests/fast/spark/test_spark_with_column.py
index 6515d2c5c74e..80da34c3164b 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_with_column.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_with_column.py
@@ -2,7 +2,7 @@
 
 _ = pytest.importorskip("duckdb.experimental.spark")
 
-from duckdb.experimental.spark.sql.types import (
+from spark_namespace.sql.types import (
     LongType,
     StructType,
     BooleanType,
@@ -14,7 +14,8 @@
     ArrayType,
     MapType,
 )
-from duckdb.experimental.spark.sql.functions import col, struct, when, lit
+from spark_namespace.sql.functions import col, struct, when, lit
+from spark_namespace import USE_ACTUAL_SPARK
 import duckdb
 import re
 
@@ -31,7 +32,7 @@ def test_with_column(self, spark):
 
         columns = ["firstname", "middlename", "lastname", "dob", "gender", "salary"]
         df = spark.createDataFrame(data=data, schema=columns)
-        assert df.schema['salary'].dataType.typeName() == 'integer'
+        assert df.schema['salary'].dataType.typeName() == ('long' if USE_ACTUAL_SPARK else 'integer')
 
         # The type of 'salary' has been cast to Bigint
         new_df = df.withColumn("salary", col("salary").cast("BIGINT"))
@@ -57,8 +58,8 @@ def test_with_column(self, spark):
         assert res[1].anotherColumn == 'anotherValue'
 
         df2 = df.withColumnRenamed("gender", "sex")
-        assert 'gender' not in df2
-        assert 'sex' in df2
+        assert 'gender' not in df2.columns
+        assert 'sex' in df2.columns
 
         df2 = df.drop("salary")
-        assert 'salary' not in df2
+        assert 'salary' not in df2.columns
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_with_column_renamed.py b/tools/pythonpkg/tests/fast/spark/test_spark_with_column_renamed.py
index 24e7a32ad5cc..168ff23a8660 100644
--- a/tools/pythonpkg/tests/fast/spark/test_spark_with_column_renamed.py
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_with_column_renamed.py
@@ -2,7 +2,7 @@
 
 _ = pytest.importorskip("duckdb.experimental.spark")
 
-from duckdb.experimental.spark.sql.types import (
+from spark_namespace.sql.types import (
     LongType,
     StructType,
     BooleanType,
@@ -14,7 +14,7 @@
     ArrayType,
     MapType,
 )
-from duckdb.experimental.spark.sql.functions import col, struct, when, lit
+from spark_namespace.sql.functions import col, struct, when, lit
 import duckdb
 import re
 
@@ -28,7 +28,7 @@ def test_with_column_renamed(self, spark):
             (('Maria', 'Anne', 'Jones'), '1967-12-01', 'F', 4000),
             (('Jen', 'Mary', 'Brown'), '1980-02-17', 'F', -1),
         ]
-        from duckdb.experimental.spark.sql.types import StructType, StructField, StringType, IntegerType
+        from spark_namespace.sql.types import StructType, StructField, StringType, IntegerType
 
         schema = StructType(
             [
@@ -51,10 +51,10 @@ def test_with_column_renamed(self, spark):
         df = spark.createDataFrame(data=dataDF, schema=schema)
 
         df2 = df.withColumnRenamed("dob", "DateOfBirth").withColumnRenamed("salary", "salary_amount")
-        assert 'dob' not in df2
-        assert 'salary' not in df2
-        assert 'DateOfBirth' in df2
-        assert 'salary_amount' in df2
+        assert 'dob' not in df2.columns
+        assert 'salary' not in df2.columns
+        assert 'DateOfBirth' in df2.columns
+        assert 'salary_amount' in df2.columns
 
         schema2 = StructType(
             [
@@ -72,9 +72,9 @@ def test_with_column_renamed(self, spark):
         )
 
         df2 = df.withColumnRenamed("name", "full name")
-        assert 'name' not in df2
-        assert 'full name' in df2
-        assert 'firstname' in df2.schema['full name'].dataType
+        assert 'name' not in df2.columns
+        assert 'full name' in df2.columns
+        assert 'firstname' in df2.schema['full name'].dataType.fieldNames()
 
         df2 = df.select(
             col("name").alias("full name"),
@@ -82,9 +82,9 @@ def test_with_column_renamed(self, spark):
             col("gender"),
             col("salary"),
         )
-        assert 'name' not in df2
-        assert 'full name' in df2
-        assert 'firstname' in df2.schema['full name'].dataType
+        assert 'name' not in df2.columns
+        assert 'full name' in df2.columns
+        assert 'firstname' in df2.schema['full name'].dataType.fieldNames()
 
         df2 = df.select(
             col("name.firstname").alias("fname"),
@@ -94,5 +94,5 @@ def test_with_column_renamed(self, spark):
             col("gender"),
             col("salary"),
         )
-        assert 'firstname' not in df2
-        assert 'fname' in df2
+        assert 'firstname' not in df2.columns
+        assert 'fname' in df2.columns
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_with_columns.py b/tools/pythonpkg/tests/fast/spark/test_spark_with_columns.py
new file mode 100644
index 000000000000..6e1bedea62ba
--- /dev/null
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_with_columns.py
@@ -0,0 +1,45 @@
+import pytest
+
+_ = pytest.importorskip("duckdb.experimental.spark")
+
+
+from spark_namespace.sql.functions import col, lit
+from spark_namespace import USE_ACTUAL_SPARK
+
+
+class TestWithColumns:
+    def test_with_columns(self, spark):
+        data = [
+            ('James', '', 'Smith', '1991-04-01', 'M', 3000),
+            ('Michael', 'Rose', '', '2000-05-19', 'M', 4000),
+            ('Robert', '', 'Williams', '1978-09-05', 'M', 4000),
+            ('Maria', 'Anne', 'Jones', '1967-12-01', 'F', 4000),
+            ('Jen', 'Mary', 'Brown', '1980-02-17', 'F', -1),
+        ]
+
+        columns = ["firstname", "middlename", "lastname", "dob", "gender", "salary"]
+        df = spark.createDataFrame(data=data, schema=columns)
+        assert df.schema['salary'].dataType.typeName() == ('long' if USE_ACTUAL_SPARK else 'integer')
+
+        # The type of 'salary' has been cast to Bigint
+        new_df = df.withColumns({"salary": col("salary").cast("BIGINT")})
+        assert new_df.schema['salary'].dataType.typeName() == 'long'
+
+        # Replace the 'salary' column with '(salary * 100)' and add a new column
+        # from an existing column
+        df2 = df.withColumns({"salary": col("salary") * 100, "CopiedColumn": col("salary") * -1})
+        res = df2.collect()
+        assert res[0].salary == 300_000
+        assert res[0].CopiedColumn == -3000
+
+        df2 = df.withColumns({"Country": lit("USA")})
+        res = df2.collect()
+        assert res[0].Country == 'USA'
+
+        df2 = df.withColumns({"Country": lit("USA")}).withColumns({"anotherColumn": lit("anotherValue")})
+        res = df2.collect()
+        assert res[0].Country == 'USA'
+        assert res[1].anotherColumn == 'anotherValue'
+
+        df2 = df.drop("salary")
+        assert 'salary' not in df2.columns
diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_with_columns_renamed.py b/tools/pythonpkg/tests/fast/spark/test_spark_with_columns_renamed.py
new file mode 100644
index 000000000000..99c4ce63d413
--- /dev/null
+++ b/tools/pythonpkg/tests/fast/spark/test_spark_with_columns_renamed.py
@@ -0,0 +1,54 @@
+import re
+import pytest
+
+_ = pytest.importorskip("duckdb.experimental.spark")
+
+from spark_namespace import USE_ACTUAL_SPARK
+
+
+class TestWithColumnsRenamed(object):
+    def test_with_columns_renamed(self, spark):
+        dataDF = [
+            (('James', '', 'Smith'), '1991-04-01', 'M', 3000),
+            (('Michael', 'Rose', ''), '2000-05-19', 'M', 4000),
+            (('Robert', '', 'Williams'), '1978-09-05', 'M', 4000),
+            (('Maria', 'Anne', 'Jones'), '1967-12-01', 'F', 4000),
+            (('Jen', 'Mary', 'Brown'), '1980-02-17', 'F', -1),
+        ]
+        from spark_namespace.sql.types import StructType, StructField, StringType, IntegerType
+
+        schema = StructType(
+            [
+                StructField(
+                    'name',
+                    StructType(
+                        [
+                            StructField('firstname', StringType(), True),
+                            StructField('middlename', StringType(), True),
+                            StructField('lastname', StringType(), True),
+                        ]
+                    ),
+                ),
+                StructField('dob', StringType(), True),
+                StructField('gender', StringType(), True),
+                StructField('salary', IntegerType(), True),
+            ]
+        )
+
+        df = spark.createDataFrame(data=dataDF, schema=schema)
+
+        df2 = df.withColumnsRenamed({"dob": "DateOfBirth", "salary": "salary_amount"})
+        assert 'dob' not in df2.columns
+        assert 'salary' not in df2.columns
+        assert 'DateOfBirth' in df2.columns
+        assert 'salary_amount' in df2.columns
+
+        df2 = df.withColumnsRenamed({"name": "full name"})
+        assert 'name' not in df2.columns
+        assert 'full name' in df2.columns
+        assert 'firstname' in df2.schema['full name'].dataType.fieldNames()
+
+        # PySpark does not raise an error. This is a convenience we provide in DuckDB.
+        if not USE_ACTUAL_SPARK:
+            with pytest.raises(ValueError, match=re.escape("DataFrame does not contain column(s): unknown col")):
+                df2.withColumnsRenamed({"unknown col": "new name", "full name": "name"})
diff --git a/tools/pythonpkg/tests/fast/test_filesystem.py b/tools/pythonpkg/tests/fast/test_filesystem.py
index 3d3e25e2e443..eaa86398341d 100644
--- a/tools/pythonpkg/tests/fast/test_filesystem.py
+++ b/tools/pythonpkg/tests/fast/test_filesystem.py
@@ -111,7 +111,7 @@ def test_null_bytes(self, duckdb_cursor: DuckDBPyConnection, memory: AbstractFil
             fh.write(b'hello
\0world\0')
         duckdb_cursor.register_filesystem(memory)
 
-        duckdb_cursor.execute('select * from read_csv("memory://test.csv", header = 0)')
+        duckdb_cursor.execute('select * from read_csv("memory://test.csv", header = 0, quote = \'"\', escape = \'"\')')
 
         assert duckdb_cursor.fetchall() == [('hello',), ('\0world\0',)]
 
diff --git a/tools/pythonpkg/tests/fast/test_parameter_list.py b/tools/pythonpkg/tests/fast/test_parameter_list.py
index 2421c76e08e1..6db0325c42be 100644
--- a/tools/pythonpkg/tests/fast/test_parameter_list.py
+++ b/tools/pythonpkg/tests/fast/test_parameter_list.py
@@ -25,6 +25,6 @@ def test_exception(self, duckdb_cursor, pandas):
             res = conn.execute("select count(*) from bool_table where a =?", [df_in])
 
     def test_explicit_nan_param(self):
-        con = duckdb.default_connection
+        con = duckdb.default_connection()
         res = con.execute('select isnan(cast(? as double))', (float("nan"),))
         assert res.fetchone()[0] == True
diff --git a/tools/pythonpkg/tests/fast/test_parquet.py b/tools/pythonpkg/tests/fast/test_parquet.py
index 498fe53ef3c2..51d8d27677f7 100644
--- a/tools/pythonpkg/tests/fast/test_parquet.py
+++ b/tools/pythonpkg/tests/fast/test_parquet.py
@@ -130,7 +130,7 @@ def test_parquet_binary_as_string_pragma(self, duckdb_cursor):
         assert res[0] == (b'foo',)
 
     def test_from_parquet_binary_as_string_default_conn(self, duckdb_cursor):
-        duckdb.default_connection.execute("PRAGMA binary_as_string=1")
+        duckdb.execute("PRAGMA binary_as_string=1")
 
         rel = duckdb.from_parquet(filename, True)
         assert rel.types == [VARCHAR]
diff --git a/tools/pythonpkg/tests/pytest.ini b/tools/pythonpkg/tests/pytest.ini
index 72625f4e6940..5dd3c30634c1 100644
--- a/tools/pythonpkg/tests/pytest.ini
+++ b/tools/pythonpkg/tests/pytest.ini
@@ -3,5 +3,8 @@
 filterwarnings =
     error
     ignore::UserWarning
+    # Jupyter is throwing DeprecationWarnings
     ignore:function ham\(\) is deprecated:DeprecationWarning
-# Jupyter is throwing DeprecationWarnings
+    # Pyspark is throwing these warnings
+    ignore:distutils Version classes are deprecated:DeprecationWarning
+    ignore:is_datetime64tz_dtype is deprecated:DeprecationWarning
diff --git a/tools/pythonpkg/tests/spark_namespace/__init__.py b/tools/pythonpkg/tests/spark_namespace/__init__.py
new file mode 100644
index 000000000000..11f91af22acd
--- /dev/null
+++ b/tools/pythonpkg/tests/spark_namespace/__init__.py
@@ -0,0 +1,4 @@
+import os
+import sys
+
+USE_ACTUAL_SPARK = os.getenv("USE_ACTUAL_SPARK") == "true"
diff --git a/tools/pythonpkg/tests/spark_namespace/errors.py b/tools/pythonpkg/tests/spark_namespace/errors.py
new file mode 100644
index 000000000000..74d10e3fe2c7
--- /dev/null
+++ b/tools/pythonpkg/tests/spark_namespace/errors.py
@@ -0,0 +1,6 @@
+from . import USE_ACTUAL_SPARK
+
+if USE_ACTUAL_SPARK:
+    from pyspark.errors import *
+else:
+    from duckdb.experimental.spark.errors import *
diff --git a/tools/pythonpkg/tests/spark_namespace/sql/__init__.py b/tools/pythonpkg/tests/spark_namespace/sql/__init__.py
new file mode 100644
index 000000000000..67075e3ce2cc
--- /dev/null
+++ b/tools/pythonpkg/tests/spark_namespace/sql/__init__.py
@@ -0,0 +1,6 @@
+from .. import USE_ACTUAL_SPARK
+
+if USE_ACTUAL_SPARK:
+    from pyspark.sql import SparkSession
+else:
+    from duckdb.experimental.spark.sql import SparkSession
diff --git a/tools/pythonpkg/tests/spark_namespace/sql/catalog.py b/tools/pythonpkg/tests/spark_namespace/sql/catalog.py
new file mode 100644
index 000000000000..abc6aa2f97e4
--- /dev/null
+++ b/tools/pythonpkg/tests/spark_namespace/sql/catalog.py
@@ -0,0 +1,6 @@
+from .. import USE_ACTUAL_SPARK
+
+if USE_ACTUAL_SPARK:
+    from pyspark.sql.catalog import *
+else:
+    from duckdb.experimental.spark.sql.catalog import *
diff --git a/tools/pythonpkg/tests/spark_namespace/sql/column.py b/tools/pythonpkg/tests/spark_namespace/sql/column.py
new file mode 100644
index 000000000000..2af9e2199f9a
--- /dev/null
+++ b/tools/pythonpkg/tests/spark_namespace/sql/column.py
@@ -0,0 +1,6 @@
+from .. import USE_ACTUAL_SPARK
+
+if USE_ACTUAL_SPARK:
+    from pyspark.sql.column import *
+else:
+    from duckdb.experimental.spark.sql.column import *
diff --git a/tools/pythonpkg/tests/spark_namespace/sql/functions.py b/tools/pythonpkg/tests/spark_namespace/sql/functions.py
new file mode 100644
index 000000000000..1822b0b1efda
--- /dev/null
+++ b/tools/pythonpkg/tests/spark_namespace/sql/functions.py
@@ -0,0 +1,6 @@
+from .. import USE_ACTUAL_SPARK
+
+if USE_ACTUAL_SPARK:
+    from pyspark.sql.functions import *
+else:
+    from duckdb.experimental.spark.sql.functions import *
diff --git a/tools/pythonpkg/tests/spark_namespace/sql/types.py b/tools/pythonpkg/tests/spark_namespace/sql/types.py
new file mode 100644
index 000000000000..89da334662bb
--- /dev/null
+++ b/tools/pythonpkg/tests/spark_namespace/sql/types.py
@@ -0,0 +1,6 @@
+from .. import USE_ACTUAL_SPARK
+
+if USE_ACTUAL_SPARK:
+    from pyspark.sql.types import *
+else:
+    from duckdb.experimental.spark.sql.types import *
diff --git a/tools/shell/tests/conftest.py b/tools/shell/tests/conftest.py
index 84dba9b4e37a..a630a4177443 100644
--- a/tools/shell/tests/conftest.py
+++ b/tools/shell/tests/conftest.py
@@ -47,11 +47,11 @@ def check_stderr(self, expected: str):
 
 
 class ShellTest:
-    def __init__(self, shell):
+    def __init__(self, shell, arguments=[]):
         if not shell:
             raise ValueError("Please provide a shell binary")
         self.shell = shell
-        self.arguments = [shell, '--batch', '--init', '/dev/null']
+        self.arguments = [shell, '--batch', '--init', '/dev/null'] + arguments
         self.statements: List[str] = []
         self.input = None
         self.output = None
diff --git a/tools/shell/tests/shell_rendering.py b/tools/shell/tests/shell_rendering.py
new file mode 100644
index 000000000000..4223afeac439
--- /dev/null
+++ b/tools/shell/tests/shell_rendering.py
@@ -0,0 +1,42 @@
+# fmt: off
+
+import pytest
+import subprocess
+import sys
+from typing import List
+from conftest import ShellTest
+import os
+from pathlib import Path
+
+
+def test_left_align(shell):
+    test = (
+        ShellTest(shell)
+        .statement(".mode box")
+        .statement(".width 5")
+        .statement(f'select 100 AS r')
+    )
+
+    result = test.run()
+    result.check_stdout("│ 100   │")
+
+def test_right_align(shell):
+    test = (
+        ShellTest(shell)
+        .statement(".mode box")
+        .statement(".width -5")
+        .statement(f'select 100 AS r')
+    )
+
+    result = test.run()
+    result.check_stdout("│   100 │")
+
+def test_markdown(shell):
+    test = (
+        ShellTest(shell)
+        .statement(".mode markdown")
+        .statement("select 42 a, 'hello' str")
+    )
+
+    result = test.run()
+    result.check_stdout("| a  |  str  |")
diff --git a/tools/shell/tests/test_safe_mode.py b/tools/shell/tests/test_safe_mode.py
new file mode 100644
index 000000000000..fea1dd6551ee
--- /dev/null
+++ b/tools/shell/tests/test_safe_mode.py
@@ -0,0 +1,53 @@
+# fmt: off
+
+import pytest
+import subprocess
+import sys
+from typing import List
+from conftest import ShellTest
+from tools.shell.tests.conftest import random_filepath
+
+
+@pytest.mark.parametrize("command", [".sh ls", ".cd ..", ".log file", ".import file.csv tbl", ".open new_file", ".output out", ".once out", ".excel out", ".read myfile.sql"])
+def test_safe_mode_command(shell, command):
+    test = (
+        ShellTest(shell, ['-safe'])
+        .statement(command)
+    )
+    result = test.run()
+    result.check_stderr('cannot be used in -safe mode')
+
+
+def test_safe_mode_database_basic(shell, random_filepath):
+    test = (
+        ShellTest(shell, [random_filepath, '-safe'])
+        .statement('CREATE TABLE integers(i INT)')
+        .statement('INSERT INTO integers VALUES (1), (2), (3)')
+        .statement('SELECT SUM(i) FROM integers')
+    )
+    result = test.run()
+    result.check_stdout("6")
+
+@pytest.mark.parametrize("command", [".sh ls", ".cd ..", ".log file", ".import file.csv tbl", ".open new_file", ".output out", ".once out", ".excel out", ".read myfile.sql"])
+@pytest.mark.parametrize("persistent", [False, True])
+def test_safe_mode_database_commands(shell, random_filepath, command, persistent):
+    arguments = ['-safe'] if not persistent else [random_filepath, '-safe']
+    test = (
+        ShellTest(shell, arguments)
+        .statement(command)
+    )
+    result = test.run()
+    result.check_stderr('cannot be used in -safe mode')
+
+@pytest.mark.parametrize("sql", ["COPY (SELECT 42) TO 'test.csv'", "LOAD spatial", "INSTALL spatial", "ATTACH 'file.db' AS file"])
+@pytest.mark.parametrize("persistent", [False, True])
+def test_safe_mode_query(shell, random_filepath, sql, persistent):
+    arguments = ['-safe'] if not persistent else [random_filepath, '-safe']
+    test = (
+        ShellTest(shell, arguments)
+        .statement(sql)
+    )
+    result = test.run()
+    result.check_stderr('disabled')
+
+# fmt: on
diff --git a/tools/shell/tests/test_shell_basics.py b/tools/shell/tests/test_shell_basics.py
index e36edbb582b0..f8e0b5d06194 100644
--- a/tools/shell/tests/test_shell_basics.py
+++ b/tools/shell/tests/test_shell_basics.py
@@ -92,24 +92,10 @@ def test_invalid_cast(shell):
     result = test.run()
     result.check_stderr("Could not convert")
 
-
-@pytest.mark.parametrize(
-    ["input", "error"],
-    [
-        (".auth ON", "sqlite3_set_authorizer"),
-        (".auth OFF", "sqlite3_set_authorizer"),
-    ],
-)
-def test_invalid_shell_commands(shell, input, error):
-    test = ShellTest(shell).statement(input)
-    result = test.run()
-    result.check_stderr(error)
-
-
 def test_invalid_backup(shell, random_filepath):
     test = ShellTest(shell).statement(f'.backup {random_filepath.as_posix()}')
     result = test.run()
-    result.check_stderr("sqlite3_backup_init")
+    result.check_stderr("unsupported in the current version of the CLI")
 
 def test_newline_in_value(shell):
     test = (
@@ -300,15 +286,6 @@ def test_execute_display(shell):
     result = test.run()
     result.check_stdout("42")
 
-# this should be fixed
-def test_selftest(shell):
-    test = (
-        ShellTest(shell)
-        .statement(".selftest")
-    )
-    result = test.run()
-    result.check_stderr("sqlite3_table_column_metadata")
-
 @pytest.mark.parametrize('generated_file', ["select 42"], indirect=True)
 def test_read(shell, generated_file):
     test = (
@@ -332,7 +309,7 @@ def test_timeout(shell):
         .statement(".timeout")
     )
     result = test.run()
-    result.check_stderr("sqlite3_busy_timeout")
+    result.check_stderr("unsupported in the current version of the CLI")
 
 
 def test_save(shell, random_filepath):
@@ -341,7 +318,7 @@ def test_save(shell, random_filepath):
         .statement(f".save {random_filepath.as_posix()}")
     )
     result = test.run()
-    result.check_stderr("sqlite3_backup_init")
+    result.check_stderr("unsupported in the current version of the CLI")
 
 def test_restore(shell, random_filepath):
     test = (
@@ -349,7 +326,7 @@ def test_restore(shell, random_filepath):
         .statement(f".restore {random_filepath.as_posix()}")
     )
     result = test.run()
-    result.check_stderr("sqlite3_backup_init")
+    result.check_stderr("unsupported in the current version of the CLI")
 
 @pytest.mark.parametrize("cmd", [
     ".vfsinfo",
@@ -365,26 +342,6 @@ def test_volatile_commands(shell, cmd):
     result = test.run()
     result.check_stderr("")
 
-def test_stats_error(shell):
-    test = (
-        ShellTest(shell)
-        .statement(".stats")
-    )
-    result = test.run()
-    result.check_stderr("sqlite3_status64")
-
-@pytest.mark.parametrize("param", [
-    "off",
-    "on"
-])
-def test_stats(shell, param):
-    test = (
-        ShellTest(shell)
-        .statement(f".stats {param}")
-    )
-    result = test.run()
-    result.check_stdout("")
-
 @pytest.mark.parametrize("pattern", [
     "test",
     "tes%",
@@ -401,6 +358,15 @@ def test_schema(shell, pattern):
     result = test.run()
     result.check_stdout("CREATE TABLE test(a INTEGER, b VARCHAR);")
 
+def test_schema_indent(shell):
+    test = (
+        ShellTest(shell)
+        .statement("create table test (a int, b varchar, c int, d int, k int, primary key(a, b));")
+        .statement(f".schema -indent")
+    )
+    result = test.run()
+    result.check_stdout("CREATE TABLE test(")
+
 def test_tables(shell):
     test = (
         ShellTest(shell)
@@ -519,25 +485,6 @@ def test_timer(shell):
     result = test.run()
     result.check_stdout('Run Time (s):')
 
-
-def test_scanstats(shell):
-    test = (
-        ShellTest(shell)
-        .statement(".scanstats on")
-        .statement("SELECT NULL;")
-    )
-    result = test.run()
-    result.check_stderr('scanstats')
-
-def test_trace(shell, random_filepath):
-    test = (
-        ShellTest(shell)
-        .statement(f".trace {random_filepath.as_posix()}")
-        .statement("SELECT 42;")
-    )
-    result = test.run()
-    result.check_stderr('sqlite3_trace_v2')
-    
 def test_output_csv_mode(shell, random_filepath):
     test = (
         ShellTest(shell)
@@ -614,6 +561,42 @@ def test_mode_html(shell):
     result = test.run()
     result.check_stdout('<td>fourty-two</td>')
 
+def test_mode_html_escapes(shell):
+    test = (
+        ShellTest(shell)
+        .statement(".mode html")
+        .statement("SELECT '<&>\"\'\'' AS \"&><\"\"\'\";")
+    )
+    result = test.run()
+    result.check_stdout('<tr><th>&amp;&gt;&lt;&quot;&#39;</th>')
+
+def test_mode_tcl_escapes(shell):
+    test = (
+        ShellTest(shell)
+        .statement(".mode tcl")
+        .statement("SELECT '<&>\"\'\'' AS \"&><\"\"\'\";")
+    )
+    result = test.run()
+    result.check_stdout('"&><\\"\'"')
+
+def test_mode_csv_escapes(shell):
+    test = (
+        ShellTest(shell)
+        .statement(".mode csv")
+        .statement("SELECT 'BEGINVAL,
\"ENDVAL' AS \"BEGINHEADER\"\",
ENDHEADER\";")
+    )
+    result = test.run()
+    result.check_stdout('"BEGINHEADER"",
ENDHEADER"\r
"BEGINVAL,
""ENDVAL"')
+
+def test_mode_json_infinity(shell):
+    test = (
+        ShellTest(shell)
+        .statement(".mode json")
+        .statement("SELECT 'inf'::DOUBLE AS inf, '-inf'::DOUBLE AS ninf, 'nan'::DOUBLE AS nan;")
+    )
+    result = test.run()
+    result.check_stdout('[{"inf":1e999,"ninf":-1e999,"nan":nan}]')
+
 # Original comment: FIXME sqlite3_column_blob
 def test_mode_insert(shell):
     test = (
@@ -915,42 +898,6 @@ def test_sqlite_comments(shell):
     result = test.run()
     result.check_stdout('42')
 
-def test_sqlite_udfs_error(shell):
-    test = (
-        ShellTest(shell)
-        .statement("SELECT writefile()")
-    )
-    result = test.run()
-    result.check_stderr('wrong number of arguments to function writefile')
-
-    test = (
-        ShellTest(shell)
-        .statement("SELECT writefile('hello')")
-    )
-    result = test.run()
-    result.check_stderr('wrong number of arguments to function writefile')
-
-def test_sqlite_udfs_correct(shell, random_filepath):
-    import os
-    test = (
-        ShellTest(shell)
-        .statement(f"SELECT writefile('{random_filepath.as_posix()}', 'hello');")
-    )
-    result = test.run()
-    if not os.path.exists(random_filepath):
-        raise Exception(f"Failed to write file {random_filepath.as_posix()}")
-    with open(random_filepath, 'r') as f:
-        result.stdout = f.read()
-    result.check_stdout('hello')
-
-def test_lsmode(shell):
-    test = (
-        ShellTest(shell)
-        .statement("SELECT lsmode(1) AS lsmode;")
-    )
-    result = test.run()
-    result.check_stdout('lsmode')
-
 def test_duckbox(shell):
     test = (
         ShellTest(shell)
@@ -1036,31 +983,4 @@ def test_nullbyte_error_rendering(shell):
     result = test.run()
     result.check_stderr('INT32')
 
-@pytest.mark.parametrize("stmt", [
-	"select sha3(NULL);"
-])
-def test_sqlite_udf_null(shell, stmt):
-    test = (
-        ShellTest(shell)
-        .statement(stmt)
-    )
-    result = test.run()
-    result.check_stdout('NULL')
-
-def test_sqlite_udf_sha3_int(shell):
-    test = (
-        ShellTest(shell)
-        .statement("select sha3(256)")
-    )
-    result = test.run()
-    result.check_stdout('A7')
-
-def test_sqlite_udf_sha3_non_inlined_string(shell):
-    test = (
-        ShellTest(shell)
-        .statement("select sha3('hello world this is a long string');")
-    )
-    result = test.run()
-    result.check_stdout('D4')
-
 # fmt: on
