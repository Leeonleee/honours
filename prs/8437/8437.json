{
  "repo": "duckdb/duckdb",
  "pull_number": 8437,
  "instance_id": "duckdb__duckdb-8437",
  "issue_numbers": [
    "7406",
    "7406"
  ],
  "base_commit": "d34a26b2c08b771b8c286b6de273ab0add85051e",
  "patch": "diff --git a/.github/config/uncovered_files.csv b/.github/config/uncovered_files.csv\nindex 787c69e8fd3a..aa7c2a601117 100644\n--- a/.github/config/uncovered_files.csv\n+++ b/.github/config/uncovered_files.csv\n@@ -156,9 +156,11 @@ execution/expression_executor_state.cpp\t2\n execution/index/art/art.cpp\t3\n execution/index/art/art_key.cpp\t6\n execution/index/art/iterator.cpp\t2\n-execution/index/art/node.cpp\t6\n+execution/index/art/leaf.cpp\t3\n+execution/index/art/node.cpp\t11\n execution/index/art/node256.cpp\t1\n-execution/index/art/prefix.cpp\t1\n+execution/index/art/node48.cpp\t5\n+execution/index/art/prefix.cpp\t2\n execution/join_hashtable.cpp\t23\n execution/nested_loop_join/nested_loop_join_inner.cpp\t24\n execution/nested_loop_join/nested_loop_join_mark.cpp\t30\ndiff --git a/benchmark/micro/index/create/create_art_varchar.benchmark b/benchmark/micro/index/create/create_art_varchar.benchmark\nnew file mode 100644\nindex 000000000000..4f5dee3c5095\n--- /dev/null\n+++ b/benchmark/micro/index/create/create_art_varchar.benchmark\n@@ -0,0 +1,20 @@\n+# name: benchmark/micro/index/create/create_art_varchar.benchmark\n+# description: Create ART on 28.8M VARCHARs with 7.2M unique values\n+# group: [create]\n+\n+name Create ART Varchar\n+group art\n+\n+load\n+CREATE TABLE art AS\n+    SELECT rpad(((i * 95823983533) % 86000000)::VARCHAR, 10, '-') AS id\n+        FROM range(7200000) tbl(i);\n+INSERT INTO art (SELECT * FROM art);\n+INSERT INTO art (SELECT * FROM art);\n+SET memory_limit='8GB';\n+\n+run\n+CREATE INDEX idx ON art USING ART(id);\n+\n+cleanup\n+DROP INDEX idx;\ndiff --git a/benchmark/micro/index/delete/delete_art.benchmark b/benchmark/micro/index/delete/delete_art.benchmark\nindex 6d7706474142..367fa7c759f5 100644\n--- a/benchmark/micro/index/delete/delete_art.benchmark\n+++ b/benchmark/micro/index/delete/delete_art.benchmark\n@@ -6,8 +6,13 @@ name Delete ART\n group art\n \n load\n-CREATE TABLE art AS SELECT (random() * 1000000)::INT AS id FROM range(10000000);\n+CREATE TABLE temp AS SELECT (range * 9876983769044::INT128 % 10000000)::INT64 AS id FROM range(10000000);\n+CREATE TABLE art AS (SELECT id FROM temp);\n CREATE INDEX idx ON art USING ART(id);\n \n run\n-DELETE FROM art WHERE id < 500000;\n+DELETE FROM art WHERE id < 5000000;\n+\n+cleanup\n+DELETE FROM art;\n+INSERT INTO art (SELECT id FROM temp);\ndiff --git a/benchmark/micro/index/insert/insert_art.benchmark b/benchmark/micro/index/insert/insert_art.benchmark\nindex d80ebf61234a..ce5ecd146057 100644\n--- a/benchmark/micro/index/insert/insert_art.benchmark\n+++ b/benchmark/micro/index/insert/insert_art.benchmark\n@@ -6,9 +6,12 @@ name Insert ART\n group art\n \n load\n-CREATE TABLE temp AS SELECT (random() * 1000000)::INT AS id FROM range(10000000);\n+CREATE TABLE temp AS SELECT (range * 9876983769044::INT128 % 10000000)::INT64 AS id FROM range(10000000);\n CREATE TABLE art (id INTEGER);\n CREATE INDEX idx ON art USING ART(id);\n \n run\n-INSERT INTO art (SELECT id + ((random() * 1000000)::INT) FROM temp);\n+INSERT INTO art (SELECT id FROM temp);\n+\n+cleanup\n+DELETE FROM art;\ndiff --git a/src/catalog/catalog_entry/duck_table_entry.cpp b/src/catalog/catalog_entry/duck_table_entry.cpp\nindex 3a0296fa66d6..51d93495e7e1 100644\n--- a/src/catalog/catalog_entry/duck_table_entry.cpp\n+++ b/src/catalog/catalog_entry/duck_table_entry.cpp\n@@ -43,7 +43,7 @@ void AddDataTableIndex(DataTable &storage, const ColumnList &columns, const vect\n \t// create an adaptive radix tree around the expressions\n \tif (index_block) {\n \t\tart = make_uniq<ART>(column_ids, TableIOManager::Get(storage), std::move(unbound_expressions), constraint_type,\n-\t\t                     storage.db, index_block->block_id, index_block->offset);\n+\t\t                     storage.db, nullptr, index_block->block_id, index_block->offset);\n \t} else {\n \t\tart = make_uniq<ART>(column_ids, TableIOManager::Get(storage), std::move(unbound_expressions), constraint_type,\n \t\t                     storage.db);\ndiff --git a/src/execution/index/art/art.cpp b/src/execution/index/art/art.cpp\nindex fb7cb731a52e..af94e9b14d75 100644\n--- a/src/execution/index/art/art.cpp\n+++ b/src/execution/index/art/art.cpp\n@@ -33,21 +33,27 @@ struct ARTIndexScanState : public IndexScanState {\n \n ART::ART(const vector<column_t> &column_ids, TableIOManager &table_io_manager,\n          const vector<unique_ptr<Expression>> &unbound_expressions, const IndexConstraintType constraint_type,\n-         AttachedDatabase &db, const idx_t block_id, const idx_t block_offset)\n+         AttachedDatabase &db, const shared_ptr<vector<FixedSizeAllocator>> &allocators_ptr, const idx_t block_id,\n+         const idx_t block_offset)\n \n-    : Index(db, IndexType::ART, table_io_manager, column_ids, unbound_expressions, constraint_type) {\n+    : Index(db, IndexType::ART, table_io_manager, column_ids, unbound_expressions, constraint_type),\n+      allocators(allocators_ptr), owns_data(false) {\n \n \tif (!Radix::IsLittleEndian()) {\n \t\tthrow NotImplementedException(\"ART indexes are not supported on big endian architectures\");\n \t}\n \n \t// initialize all allocators\n-\tallocators.emplace_back(make_uniq<FixedSizeAllocator>(sizeof(Prefix), buffer_manager.GetBufferAllocator()));\n-\tallocators.emplace_back(make_uniq<FixedSizeAllocator>(sizeof(Leaf), buffer_manager.GetBufferAllocator()));\n-\tallocators.emplace_back(make_uniq<FixedSizeAllocator>(sizeof(Node4), buffer_manager.GetBufferAllocator()));\n-\tallocators.emplace_back(make_uniq<FixedSizeAllocator>(sizeof(Node16), buffer_manager.GetBufferAllocator()));\n-\tallocators.emplace_back(make_uniq<FixedSizeAllocator>(sizeof(Node48), buffer_manager.GetBufferAllocator()));\n-\tallocators.emplace_back(make_uniq<FixedSizeAllocator>(sizeof(Node256), buffer_manager.GetBufferAllocator()));\n+\tif (!allocators) {\n+\t\towns_data = true;\n+\t\tallocators = make_shared<vector<FixedSizeAllocator>>();\n+\t\tallocators->emplace_back(FixedSizeAllocator(sizeof(Prefix), buffer_manager.GetBufferAllocator()));\n+\t\tallocators->emplace_back(FixedSizeAllocator(sizeof(Leaf), buffer_manager.GetBufferAllocator()));\n+\t\tallocators->emplace_back(FixedSizeAllocator(sizeof(Node4), buffer_manager.GetBufferAllocator()));\n+\t\tallocators->emplace_back(FixedSizeAllocator(sizeof(Node16), buffer_manager.GetBufferAllocator()));\n+\t\tallocators->emplace_back(FixedSizeAllocator(sizeof(Node48), buffer_manager.GetBufferAllocator()));\n+\t\tallocators->emplace_back(FixedSizeAllocator(sizeof(Node256), buffer_manager.GetBufferAllocator()));\n+\t}\n \n \t// set the root node of the tree\n \ttree = make_uniq<Node>();\n@@ -986,26 +992,28 @@ BlockPointer ART::Serialize(MetaBlockWriter &writer) {\n \n void ART::InitializeVacuum(ARTFlags &flags) {\n \n-\tflags.vacuum_flags.reserve(allocators.size());\n-\tfor (auto &allocator : allocators) {\n-\t\tflags.vacuum_flags.push_back(allocator->InitializeVacuum());\n+\tflags.vacuum_flags.reserve(allocators->size());\n+\tfor (auto &allocator : *allocators) {\n+\t\tflags.vacuum_flags.push_back(allocator.InitializeVacuum());\n \t}\n }\n \n void ART::FinalizeVacuum(const ARTFlags &flags) {\n \n-\tfor (idx_t i = 0; i < allocators.size(); i++) {\n+\tfor (idx_t i = 0; i < allocators->size(); i++) {\n \t\tif (flags.vacuum_flags[i]) {\n-\t\t\tallocators[i]->FinalizeVacuum();\n+\t\t\t(*allocators)[i].FinalizeVacuum();\n \t\t}\n \t}\n }\n \n void ART::Vacuum(IndexLock &state) {\n \n+\tD_ASSERT(owns_data);\n+\n \tif (!tree->IsSet()) {\n-\t\tfor (auto &allocator : allocators) {\n-\t\t\tallocator->Reset();\n+\t\tfor (auto &allocator : *allocators) {\n+\t\t\tallocator.Reset();\n \t\t}\n \t\treturn;\n \t}\n@@ -1032,8 +1040,8 @@ void ART::Vacuum(IndexLock &state) {\n \t// finalize the vacuum operation\n \tFinalizeVacuum(flags);\n \n-\tfor (auto &allocator : allocators) {\n-\t\tallocator->Verify();\n+\tfor (auto &allocator : *allocators) {\n+\t\tallocator.Verify();\n \t}\n }\n \n@@ -1043,9 +1051,11 @@ void ART::Vacuum(IndexLock &state) {\n \n void ART::InitializeMerge(ARTFlags &flags) {\n \n-\tflags.merge_buffer_counts.reserve(allocators.size());\n-\tfor (auto &allocator : allocators) {\n-\t\tflags.merge_buffer_counts.emplace_back(allocator->buffers.size());\n+\tD_ASSERT(owns_data);\n+\n+\tflags.merge_buffer_counts.reserve(allocators->size());\n+\tfor (auto &allocator : *allocators) {\n+\t\tflags.merge_buffer_counts.emplace_back(allocator.buffers.size());\n \t}\n }\n \n@@ -1056,16 +1066,18 @@ bool ART::MergeIndexes(IndexLock &state, Index &other_index) {\n \t\treturn true;\n \t}\n \n-\tif (tree->IsSet()) {\n-\t\t//  fully deserialize other_index, and traverse it to increment its buffer IDs\n-\t\tARTFlags flags;\n-\t\tInitializeMerge(flags);\n-\t\tother_art.tree->InitializeMerge(other_art, flags);\n-\t}\n+\tif (other_art.owns_data) {\n+\t\tif (tree->IsSet()) {\n+\t\t\t//  fully deserialize other_index, and traverse it to increment its buffer IDs\n+\t\t\tARTFlags flags;\n+\t\t\tInitializeMerge(flags);\n+\t\t\tother_art.tree->InitializeMerge(other_art, flags);\n+\t\t}\n \n-\t// merge the node storage\n-\tfor (idx_t i = 0; i < allocators.size(); i++) {\n-\t\tallocators[i]->Merge(*other_art.allocators[i]);\n+\t\t// merge the node storage\n+\t\tfor (idx_t i = 0; i < allocators->size(); i++) {\n+\t\t\t(*allocators)[i].Merge((*other_art.allocators)[i]);\n+\t\t}\n \t}\n \n \t// merge the ARTs\n@@ -1073,8 +1085,8 @@ bool ART::MergeIndexes(IndexLock &state, Index &other_index) {\n \t\treturn false;\n \t}\n \n-\tfor (auto &allocator : allocators) {\n-\t\tallocator->Verify();\n+\tfor (auto &allocator : *allocators) {\n+\t\tallocator.Verify();\n \t}\n \treturn true;\n }\ndiff --git a/src/execution/index/art/leaf.cpp b/src/execution/index/art/leaf.cpp\nindex 730d01704d71..a1177e5a1c54 100644\n--- a/src/execution/index/art/leaf.cpp\n+++ b/src/execution/index/art/leaf.cpp\n@@ -42,19 +42,30 @@ void Leaf::New(ART &art, reference<Node> &node, const row_t *row_ids, idx_t coun\n \n void Leaf::Free(ART &art, Node &node) {\n \n-\tD_ASSERT(node.IsSet() && !node.IsSerialized());\n-\tauto &child = Leaf::Get(art, node).ptr;\n-\tNode::Free(art, child);\n+\tNode current_node = node;\n+\tNode next_node;\n+\twhile (current_node.IsSet() && !current_node.IsSerialized()) {\n+\t\tnext_node = Leaf::Get(art, current_node).ptr;\n+\t\tNode::GetAllocator(art, NType::LEAF).Free(current_node);\n+\t\tcurrent_node = next_node;\n+\t}\n+\n+\tnode.Reset();\n }\n \n void Leaf::InitializeMerge(ART &art, Node &node, const ARTFlags &flags) {\n \n-\tD_ASSERT(node.IsSet() && !node.IsSerialized());\n-\tD_ASSERT(node.GetType() == NType::LEAF);\n+\tauto merge_buffer_count = flags.merge_buffer_counts[(uint8_t)NType::LEAF - 1];\n \n-\tauto &leaf = Leaf::Get(art, node);\n-\tif (leaf.ptr.IsSet()) {\n-\t\tleaf.ptr.InitializeMerge(art, flags);\n+\tNode next_node = node;\n+\tnode.AddToBufferID(merge_buffer_count);\n+\n+\twhile (next_node.IsSet()) {\n+\t\tauto &leaf = Leaf::Get(art, next_node);\n+\t\tnext_node = leaf.ptr;\n+\t\tif (leaf.ptr.IsSet()) {\n+\t\t\tleaf.ptr.AddToBufferID(merge_buffer_count);\n+\t\t}\n \t}\n }\n \n@@ -290,7 +301,6 @@ string Leaf::VerifyAndToString(ART &art, Node &node) {\n \t\treturn \"Leaf [count: 1, row ID: \" + to_string(node.GetRowId()) + \"]\";\n \t}\n \n-\t// NOTE: we could do this recursively, but the function-call overhead can become kinda crazy\n \tstring str = \"\";\n \n \treference<Node> node_ref(node);\n@@ -322,46 +332,51 @@ BlockPointer Leaf::Serialize(ART &art, Node &node, MetaBlockWriter &writer) {\n \t\treturn block_pointer;\n \t}\n \n-\t// recurse into the child and retrieve its block pointer\n-\tauto &leaf = Leaf::Get(art, node);\n-\tauto child_block_pointer = leaf.ptr.Serialize(art, writer);\n-\n-\t// get pointer and write fields\n \tauto block_pointer = writer.GetBlockPointer();\n \twriter.Write(NType::LEAF);\n-\twriter.Write<uint8_t>(leaf.count);\n+\tidx_t total_count = Leaf::TotalCount(art, node);\n+\twriter.Write<idx_t>(total_count);\n \n-\t// write row IDs\n-\tfor (idx_t i = 0; i < leaf.count; i++) {\n-\t\twriter.Write(leaf.row_ids[i]);\n-\t}\n+\t// iterate all leaves and write their row IDs\n+\treference<Node> ref_node(node);\n+\twhile (ref_node.get().IsSet()) {\n+\t\tD_ASSERT(!ref_node.get().IsSerialized());\n+\t\tauto &leaf = Leaf::Get(art, ref_node);\n \n-\t// write child block pointer\n-\twriter.Write(child_block_pointer.block_id);\n-\twriter.Write(child_block_pointer.offset);\n+\t\t// write row IDs\n+\t\tfor (idx_t i = 0; i < leaf.count; i++) {\n+\t\t\twriter.Write(leaf.row_ids[i]);\n+\t\t}\n+\t\tref_node = leaf.ptr;\n+\t}\n \n \treturn block_pointer;\n }\n \n void Leaf::Deserialize(ART &art, Node &node, MetaBlockReader &reader) {\n \n-\tD_ASSERT(node.GetType() == NType::LEAF);\n+\tauto total_count = reader.Read<idx_t>();\n+\treference<Node> ref_node(node);\n \n-\tauto &leaf = Leaf::Get(art, node);\n-\tleaf.count = reader.Read<uint8_t>();\n+\twhile (total_count) {\n+\t\tref_node.get() = Node::GetAllocator(art, NType::LEAF).New();\n+\t\tref_node.get().SetType((uint8_t)NType::LEAF);\n \n-\t// read row IDs\n-\tfor (idx_t i = 0; i < leaf.count; i++) {\n-\t\tleaf.row_ids[i] = reader.Read<row_t>();\n-\t}\n+\t\tauto &leaf = Leaf::Get(art, ref_node);\n+\n+\t\tleaf.count = MinValue((idx_t)Node::LEAF_SIZE, total_count);\n+\t\tfor (idx_t i = 0; i < leaf.count; i++) {\n+\t\t\tleaf.row_ids[i] = reader.Read<row_t>();\n+\t\t}\n \n-\t// read child block pointer\n-\tleaf.ptr = Node(reader);\n+\t\ttotal_count -= leaf.count;\n+\t\tref_node = leaf.ptr;\n+\t\tleaf.ptr.Reset();\n+\t}\n }\n \n void Leaf::Vacuum(ART &art, Node &node) {\n \n-\t// NOTE: we could do this recursively, but the function-call overhead can become kinda crazy\n \tauto &allocator = Node::GetAllocator(art, NType::LEAF);\n \n \treference<Node> node_ref(node);\n@@ -373,7 +388,6 @@ void Leaf::Vacuum(ART &art, Node &node) {\n \t\tauto &leaf = Leaf::Get(art, node_ref);\n \t\tnode_ref = leaf.ptr;\n \t}\n-\treturn;\n }\n \n void Leaf::MoveInlinedToLeaf(ART &art, Node &node) {\ndiff --git a/src/execution/index/art/node.cpp b/src/execution/index/art/node.cpp\nindex 36f5d0b6c5a2..71965d8b80a6 100644\n--- a/src/execution/index/art/node.cpp\n+++ b/src/execution/index/art/node.cpp\n@@ -61,7 +61,6 @@ void Node::New(ART &art, Node &node, const NType type) {\n \n void Node::Free(ART &art, Node &node) {\n \n-\t// recursively free all nodes that are in-memory, and skip serialized and empty nodes\n \tif (!node.IsSet()) {\n \t\treturn;\n \t}\n@@ -72,11 +71,11 @@ void Node::Free(ART &art, Node &node) {\n \t\tauto type = node.GetType();\n \t\tswitch (type) {\n \t\tcase NType::PREFIX:\n-\t\t\tPrefix::Free(art, node);\n-\t\t\tbreak;\n+\t\t\t// iterative\n+\t\t\treturn Prefix::Free(art, node);\n \t\tcase NType::LEAF:\n-\t\t\tLeaf::Free(art, node);\n-\t\t\tbreak;\n+\t\t\t// iterative\n+\t\t\treturn Leaf::Free(art, node);\n \t\tcase NType::NODE_4:\n \t\t\tNode4::Free(art, node);\n \t\t\tbreak;\n@@ -90,8 +89,7 @@ void Node::Free(ART &art, Node &node) {\n \t\t\tNode256::Free(art, node);\n \t\t\tbreak;\n \t\tcase NType::LEAF_INLINED:\n-\t\t\tnode.Reset();\n-\t\t\treturn;\n+\t\t\treturn node.Reset();\n \t\t}\n \n \t\tNode::GetAllocator(art, type).Free(node);\n@@ -236,8 +234,10 @@ BlockPointer Node::Serialize(ART &art, MetaBlockWriter &writer) {\n \n \tswitch (GetType()) {\n \tcase NType::PREFIX:\n-\t\treturn Prefix::Get(art, *this).Serialize(art, writer);\n+\t\t// iterative\n+\t\treturn Prefix::Serialize(art, *this, writer);\n \tcase NType::LEAF:\n+\t\t// iterative\n \t\treturn Leaf::Serialize(art, *this, writer);\n \tcase NType::NODE_4:\n \t\treturn Node4::Get(art, *this).Serialize(art, writer);\n@@ -263,19 +263,23 @@ void Node::Deserialize(ART &art) {\n \tSetType(reader.Read<uint8_t>());\n \n \tauto decoded_type = GetType();\n+\n+\t// iterative functions\n+\tif (decoded_type == NType::PREFIX) {\n+\t\treturn Prefix::Deserialize(art, *this, reader);\n+\t}\n \tif (decoded_type == NType::LEAF_INLINED) {\n-\t\tSetRowId(reader.Read<row_t>());\n-\t\treturn;\n+\t\treturn SetRowId(reader.Read<row_t>());\n+\t}\n+\tif (decoded_type == NType::LEAF) {\n+\t\treturn Leaf::Deserialize(art, *this, reader);\n \t}\n \n \t*this = Node::GetAllocator(art, decoded_type).New();\n \tSetType((uint8_t)decoded_type);\n \n+\t// recursive functions\n \tswitch (decoded_type) {\n-\tcase NType::PREFIX:\n-\t\treturn Prefix::Get(art, *this).Deserialize(reader);\n-\tcase NType::LEAF:\n-\t\treturn Leaf::Deserialize(art, *this, reader);\n \tcase NType::NODE_4:\n \t\treturn Node4::Get(art, *this).Deserialize(reader);\n \tcase NType::NODE_16:\n@@ -363,7 +367,7 @@ NType Node::GetARTNodeTypeByCount(const idx_t count) {\n }\n \n FixedSizeAllocator &Node::GetAllocator(const ART &art, NType type) {\n-\treturn *art.allocators[(uint8_t)type - 1];\n+\treturn (*art.allocators)[(uint8_t)type - 1];\n }\n \n //===--------------------------------------------------------------------===//\n@@ -377,11 +381,11 @@ void Node::InitializeMerge(ART &art, const ARTFlags &flags) {\n \n \tswitch (GetType()) {\n \tcase NType::PREFIX:\n-\t\tPrefix::Get(art, *this).InitializeMerge(art, flags);\n-\t\tbreak;\n+\t\t// iterative\n+\t\treturn Prefix::InitializeMerge(art, *this, flags);\n \tcase NType::LEAF:\n-\t\tLeaf::InitializeMerge(art, *this, flags);\n-\t\tbreak;\n+\t\t// iterative\n+\t\treturn Leaf::InitializeMerge(art, *this, flags);\n \tcase NType::NODE_4:\n \t\tNode4::Get(art, *this).InitializeMerge(art, flags);\n \t\tbreak;\n@@ -398,8 +402,7 @@ void Node::InitializeMerge(ART &art, const ARTFlags &flags) {\n \t\treturn;\n \t}\n \n-\t// NOTE: this works because the rightmost 32 bits contain the buffer ID\n-\tdata += flags.merge_buffer_counts[(uint8_t)GetType() - 1];\n+\tAddToBufferID(flags.merge_buffer_counts[(uint8_t)GetType() - 1]);\n }\n \n bool Node::Merge(ART &art, Node &other) {\n@@ -572,11 +575,16 @@ void Node::Vacuum(ART &art, const ARTFlags &flags) {\n \t}\n \n \tauto node_type = GetType();\n+\n+\t// iterative functions\n+\tif (node_type == NType::PREFIX) {\n+\t\treturn Prefix::Vacuum(art, *this, flags);\n+\t}\n \tif (node_type == NType::LEAF_INLINED) {\n \t\treturn;\n \t}\n \tif (node_type == NType::LEAF) {\n-\t\tif (flags.vacuum_flags[(uint8_t)GetType() - 1]) {\n+\t\tif (flags.vacuum_flags[(uint8_t)node_type - 1]) {\n \t\t\tLeaf::Vacuum(art, *this);\n \t\t}\n \t\treturn;\n@@ -589,9 +597,8 @@ void Node::Vacuum(ART &art, const ARTFlags &flags) {\n \t\tSetType((uint8_t)node_type);\n \t}\n \n+\t// recursive functions\n \tswitch (node_type) {\n-\tcase NType::PREFIX:\n-\t\treturn Prefix::Get(art, *this).Vacuum(art, flags);\n \tcase NType::NODE_4:\n \t\treturn Node4::Get(art, *this).Vacuum(art, flags);\n \tcase NType::NODE_16:\ndiff --git a/src/execution/index/art/prefix.cpp b/src/execution/index/art/prefix.cpp\nindex 49ed40e554ee..fe237b8e2385 100644\n--- a/src/execution/index/art/prefix.cpp\n+++ b/src/execution/index/art/prefix.cpp\n@@ -55,9 +55,35 @@ void Prefix::New(ART &art, reference<Node> &node, const ARTKey &key, const uint3\n \n void Prefix::Free(ART &art, Node &node) {\n \n-\tD_ASSERT(node.IsSet() && !node.IsSerialized());\n-\tauto &child = Prefix::Get(art, node).ptr;\n-\tNode::Free(art, child);\n+\tNode current_node = node;\n+\tNode next_node;\n+\twhile (current_node.IsSet() && !current_node.IsSerialized() && current_node.GetType() == NType::PREFIX) {\n+\t\tnext_node = Prefix::Get(art, current_node).ptr;\n+\t\tNode::GetAllocator(art, NType::PREFIX).Free(current_node);\n+\t\tcurrent_node = next_node;\n+\t}\n+\n+\tNode::Free(art, current_node);\n+\tnode.Reset();\n+}\n+\n+void Prefix::InitializeMerge(ART &art, Node &node, const ARTFlags &flags) {\n+\n+\tauto merge_buffer_count = flags.merge_buffer_counts[(uint8_t)NType::PREFIX - 1];\n+\n+\tNode next_node = node;\n+\treference<Prefix> prefix = Prefix::Get(art, next_node);\n+\n+\twhile (next_node.GetType() == NType::PREFIX) {\n+\t\tnext_node = prefix.get().ptr;\n+\t\tif (prefix.get().ptr.GetType() == NType::PREFIX) {\n+\t\t\tprefix.get().ptr.AddToBufferID(merge_buffer_count);\n+\t\t\tprefix = Prefix::Get(art, next_node);\n+\t\t}\n+\t}\n+\n+\tnode.AddToBufferID(merge_buffer_count);\n+\tprefix.get().ptr.InitializeMerge(art, flags);\n }\n \n void Prefix::Concatenate(ART &art, Node &prefix_node, const uint8_t byte, Node &child_prefix_node) {\n@@ -280,19 +306,28 @@ string Prefix::VerifyAndToString(ART &art, Node &node, const bool only_verify) {\n \treturn str + node_ref.get().VerifyAndToString(art, only_verify);\n }\n \n-BlockPointer Prefix::Serialize(ART &art, MetaBlockWriter &writer) {\n+BlockPointer Prefix::Serialize(ART &art, Node &node, MetaBlockWriter &writer) {\n \n-\t// recurse into the child and retrieve its block pointer\n-\tauto child_block_pointer = ptr.Serialize(art, writer);\n+\treference<Node> first_non_prefix(node);\n+\tidx_t total_count = Prefix::TotalCount(art, first_non_prefix);\n+\tauto child_block_pointer = first_non_prefix.get().Serialize(art, writer);\n \n \t// get pointer and write fields\n \tauto block_pointer = writer.GetBlockPointer();\n \twriter.Write(NType::PREFIX);\n-\twriter.Write<uint8_t>(data[Node::PREFIX_SIZE]);\n+\twriter.Write<idx_t>(total_count);\n+\n+\treference<Node> current_node(node);\n+\twhile (current_node.get().GetType() == NType::PREFIX) {\n \n-\t// write prefix bytes\n-\tfor (idx_t i = 0; i < data[Node::PREFIX_SIZE]; i++) {\n-\t\twriter.Write(data[i]);\n+\t\t// write prefix bytes\n+\t\tD_ASSERT(!current_node.get().IsSerialized());\n+\t\tauto &prefix = Prefix::Get(art, current_node);\n+\t\tfor (idx_t i = 0; i < prefix.data[Node::PREFIX_SIZE]; i++) {\n+\t\t\twriter.Write(prefix.data[i]);\n+\t\t}\n+\n+\t\tcurrent_node = prefix.ptr;\n \t}\n \n \t// write child block pointer\n@@ -302,17 +337,48 @@ BlockPointer Prefix::Serialize(ART &art, MetaBlockWriter &writer) {\n \treturn block_pointer;\n }\n \n-void Prefix::Deserialize(MetaBlockReader &reader) {\n+void Prefix::Deserialize(ART &art, Node &node, MetaBlockReader &reader) {\n+\n+\tauto total_count = reader.Read<idx_t>();\n+\treference<Node> current_node(node);\n+\n+\twhile (total_count) {\n+\t\tcurrent_node.get() = Node::GetAllocator(art, NType::PREFIX).New();\n+\t\tcurrent_node.get().SetType((uint8_t)NType::PREFIX);\n \n-\tdata[Node::PREFIX_SIZE] = reader.Read<uint8_t>();\n+\t\tauto &prefix = Prefix::Get(art, current_node);\n+\t\tprefix.data[Node::PREFIX_SIZE] = MinValue((idx_t)Node::PREFIX_SIZE, total_count);\n \n-\t// read bytes\n-\tfor (idx_t i = 0; i < data[Node::PREFIX_SIZE]; i++) {\n-\t\tdata[i] = reader.Read<uint8_t>();\n+\t\t// read bytes\n+\t\tfor (idx_t i = 0; i < prefix.data[Node::PREFIX_SIZE]; i++) {\n+\t\t\tprefix.data[i] = reader.Read<uint8_t>();\n+\t\t}\n+\n+\t\ttotal_count -= prefix.data[Node::PREFIX_SIZE];\n+\t\tcurrent_node = prefix.ptr;\n+\t\tprefix.ptr.Reset();\n \t}\n \n \t// read child block pointer\n-\tptr = Node(reader);\n+\tcurrent_node.get() = Node(reader);\n+}\n+\n+void Prefix::Vacuum(ART &art, Node &node, const ARTFlags &flags) {\n+\n+\tbool flag_set = flags.vacuum_flags[(uint8_t)NType::PREFIX - 1];\n+\tauto &allocator = Node::GetAllocator(art, NType::PREFIX);\n+\n+\treference<Node> node_ref(node);\n+\twhile (!node_ref.get().IsSerialized() && node_ref.get().GetType() == NType::PREFIX) {\n+\t\tif (flag_set && allocator.NeedsVacuum(node_ref)) {\n+\t\t\tnode_ref.get() = allocator.VacuumPointer(node_ref);\n+\t\t\tnode_ref.get().SetType((uint8_t)NType::PREFIX);\n+\t\t}\n+\t\tauto &prefix = Prefix::Get(art, node_ref);\n+\t\tnode_ref = prefix.ptr;\n+\t}\n+\n+\tnode_ref.get().Vacuum(art, flags);\n }\n \n Prefix &Prefix::Append(ART &art, const uint8_t byte) {\n@@ -356,4 +422,22 @@ void Prefix::Append(ART &art, Node other_prefix) {\n \tD_ASSERT(prefix.get().ptr.GetType() != NType::PREFIX);\n }\n \n+idx_t Prefix::TotalCount(ART &art, reference<Node> &node) {\n+\n+\t// NOTE: first prefix in the prefix chain is already deserialized\n+\tD_ASSERT(node.get().IsSet() && !node.get().IsSerialized());\n+\n+\tidx_t count = 0;\n+\twhile (node.get().GetType() == NType::PREFIX) {\n+\t\tauto &prefix = Prefix::Get(art, node);\n+\t\tcount += prefix.data[Node::PREFIX_SIZE];\n+\n+\t\tif (prefix.ptr.IsSerialized()) {\n+\t\t\tprefix.ptr.Deserialize(art);\n+\t\t}\n+\t\tnode = prefix.ptr;\n+\t}\n+\treturn count;\n+}\n+\n } // namespace duckdb\ndiff --git a/src/execution/operator/schema/physical_create_index.cpp b/src/execution/operator/schema/physical_create_index.cpp\nindex 5cf03c2ae56b..f4e5f2aead83 100644\n--- a/src/execution/operator/schema/physical_create_index.cpp\n+++ b/src/execution/operator/schema/physical_create_index.cpp\n@@ -4,7 +4,9 @@\n #include \"duckdb/catalog/catalog_entry/table_catalog_entry.hpp\"\n #include \"duckdb/catalog/catalog_entry/duck_index_entry.hpp\"\n #include \"duckdb/main/client_context.hpp\"\n+#include \"duckdb/storage/index.hpp\"\n #include \"duckdb/storage/storage_manager.hpp\"\n+#include \"duckdb/storage/table/append_state.hpp\"\n #include \"duckdb/main/database_manager.hpp\"\n #include \"duckdb/execution/index/art/art_key.hpp\"\n #include \"duckdb/execution/index/art/node.hpp\"\n@@ -15,10 +17,10 @@ namespace duckdb {\n PhysicalCreateIndex::PhysicalCreateIndex(LogicalOperator &op, TableCatalogEntry &table_p,\n                                          const vector<column_t> &column_ids, unique_ptr<CreateIndexInfo> info,\n                                          vector<unique_ptr<Expression>> unbound_expressions,\n-                                         idx_t estimated_cardinality)\n+                                         idx_t estimated_cardinality, const bool sorted)\n     : PhysicalOperator(PhysicalOperatorType::CREATE_INDEX, op.types, estimated_cardinality),\n-      table(table_p.Cast<DuckTableEntry>()), info(std::move(info)),\n-      unbound_expressions(std::move(unbound_expressions)) {\n+      table(table_p.Cast<DuckTableEntry>()), info(std::move(info)), unbound_expressions(std::move(unbound_expressions)),\n+      sorted(sorted) {\n \t// convert virtual column ids to storage column ids\n \tfor (auto &column_id : column_ids) {\n \t\tstorage_ids.push_back(table.GetColumns().LogicalToPhysical(LogicalIndex(column_id)).index);\n@@ -86,43 +88,65 @@ unique_ptr<LocalSinkState> PhysicalCreateIndex::GetLocalSinkState(ExecutionConte\n \treturn std::move(state);\n }\n \n-SinkResultType PhysicalCreateIndex::Sink(ExecutionContext &context, DataChunk &chunk, OperatorSinkInput &input) const {\n+SinkResultType PhysicalCreateIndex::SinkUnsorted(Vector &row_identifiers, OperatorSinkInput &input) const {\n \n-\tD_ASSERT(chunk.ColumnCount() >= 2);\n-\tauto &lstate = input.local_state.Cast<CreateIndexLocalSinkState>();\n-\tauto &row_identifiers = chunk.data[chunk.ColumnCount() - 1];\n+\tauto &l_state = input.local_state.Cast<CreateIndexLocalSinkState>();\n+\tauto count = l_state.key_chunk.size();\n \n-\t// generate the keys for the given input\n-\tlstate.key_chunk.ReferenceColumns(chunk, lstate.key_column_ids);\n-\tlstate.arena_allocator.Reset();\n-\tART::GenerateKeys(lstate.arena_allocator, lstate.key_chunk, lstate.keys);\n+\t// get the corresponding row IDs\n+\trow_identifiers.Flatten(count);\n+\tauto row_ids = FlatVector::GetData<row_t>(row_identifiers);\n+\n+\t// insert the row IDs\n+\tauto &art = l_state.local_index->Cast<ART>();\n+\tfor (idx_t i = 0; i < count; i++) {\n+\t\tif (!art.Insert(*art.tree, l_state.keys[i], 0, row_ids[i])) {\n+\t\t\tthrow ConstraintException(\"Data contains duplicates on indexed column(s)\");\n+\t\t}\n+\t}\n \n+\treturn SinkResultType::NEED_MORE_INPUT;\n+}\n+\n+SinkResultType PhysicalCreateIndex::SinkSorted(Vector &row_identifiers, OperatorSinkInput &input) const {\n+\n+\tauto &l_state = input.local_state.Cast<CreateIndexLocalSinkState>();\n \tauto &storage = table.GetStorage();\n-\tauto art = make_uniq<ART>(lstate.local_index->column_ids, lstate.local_index->table_io_manager,\n-\t                          lstate.local_index->unbound_expressions, lstate.local_index->constraint_type, storage.db);\n-\tif (!art->ConstructFromSorted(lstate.key_chunk.size(), lstate.keys, row_identifiers)) {\n+\tauto &l_index = l_state.local_index;\n+\n+\t// create an ART from the chunk\n+\tauto art = make_uniq<ART>(l_index->column_ids, l_index->table_io_manager, l_index->unbound_expressions,\n+\t                          l_index->constraint_type, storage.db, l_index->Cast<ART>().allocators);\n+\tif (!art->ConstructFromSorted(l_state.key_chunk.size(), l_state.keys, row_identifiers)) {\n \t\tthrow ConstraintException(\"Data contains duplicates on indexed column(s)\");\n \t}\n \n \t// merge into the local ART\n-\tif (!lstate.local_index->MergeIndexes(*art)) {\n+\tif (!l_index->MergeIndexes(*art)) {\n \t\tthrow ConstraintException(\"Data contains duplicates on indexed column(s)\");\n \t}\n \n-#ifdef DEBUG\n-\t// ensure that all row IDs of this chunk exist in the ART\n-\tauto &local_art = lstate.local_index->Cast<ART>();\n-\tauto row_ids = FlatVector::GetData<row_t>(row_identifiers);\n-\tfor (idx_t i = 0; i < lstate.key_chunk.size(); i++) {\n-\t\tauto leaf = local_art.Lookup(*local_art.tree, lstate.keys[i], 0);\n-\t\tD_ASSERT(leaf.IsSet());\n-\t\tD_ASSERT(Leaf::ContainsRowId(local_art, leaf, row_ids[i]));\n-\t}\n-#endif\n-\n \treturn SinkResultType::NEED_MORE_INPUT;\n }\n \n+SinkResultType PhysicalCreateIndex::Sink(ExecutionContext &context, DataChunk &chunk, OperatorSinkInput &input) const {\n+\n+\tD_ASSERT(chunk.ColumnCount() >= 2);\n+\n+\t// generate the keys for the given input\n+\tauto &l_state = input.local_state.Cast<CreateIndexLocalSinkState>();\n+\tl_state.key_chunk.ReferenceColumns(chunk, l_state.key_column_ids);\n+\tl_state.arena_allocator.Reset();\n+\tART::GenerateKeys(l_state.arena_allocator, l_state.key_chunk, l_state.keys);\n+\n+\t// insert the keys and their corresponding row IDs\n+\tauto &row_identifiers = chunk.data[chunk.ColumnCount() - 1];\n+\tif (sorted) {\n+\t\treturn SinkSorted(row_identifiers, input);\n+\t}\n+\treturn SinkUnsorted(row_identifiers, input);\n+}\n+\n SinkCombineResultType PhysicalCreateIndex::Combine(ExecutionContext &context, OperatorSinkCombineInput &input) const {\n \n \tauto &gstate = input.global_state.Cast<CreateIndexGlobalSinkState>();\n@@ -133,18 +157,17 @@ SinkCombineResultType PhysicalCreateIndex::Combine(ExecutionContext &context, Op\n \t\tthrow ConstraintException(\"Data contains duplicates on indexed column(s)\");\n \t}\n \n-\t// vacuum excess memory\n-\tgstate.global_index->Vacuum();\n-\n \treturn SinkCombineResultType::FINISHED;\n }\n \n SinkFinalizeType PhysicalCreateIndex::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,\n                                                OperatorSinkFinalizeInput &input) const {\n \n-\t// here, we just set the resulting global index as the newly created index of the table\n-\n+\t// here, we set the resulting global index as the newly created index of the table\n \tauto &state = input.global_state.Cast<CreateIndexGlobalSinkState>();\n+\n+\t// vacuum excess memory and verify\n+\tstate.global_index->Vacuum();\n \tD_ASSERT(!state.global_index->VerifyAndToString(true).empty());\n \n \tauto &storage = table.GetStorage();\ndiff --git a/src/execution/physical_plan/plan_create_index.cpp b/src/execution/physical_plan/plan_create_index.cpp\nindex c346a7b9754c..2e3b85cf9e25 100644\n--- a/src/execution/physical_plan/plan_create_index.cpp\n+++ b/src/execution/physical_plan/plan_create_index.cpp\n@@ -68,27 +68,44 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalCreateInde\n \tnull_filter->types.emplace_back(LogicalType::ROW_TYPE);\n \tnull_filter->children.push_back(std::move(projection));\n \n-\t// order operator\n-\n-\tvector<BoundOrderByNode> orders;\n-\tvector<idx_t> projections;\n-\tfor (idx_t i = 0; i < new_column_types.size() - 1; i++) {\n-\t\tauto col_expr = make_uniq_base<Expression, BoundReferenceExpression>(new_column_types[i], i);\n-\t\torders.emplace_back(OrderType::ASCENDING, OrderByNullType::NULLS_FIRST, std::move(col_expr));\n-\t\tprojections.emplace_back(i);\n+\t// determine if we sort the data prior to index creation\n+\t// we don't sort, if either VARCHAR or compound key\n+\tauto perform_sorting = true;\n+\tif (op.unbound_expressions.size() > 1) {\n+\t\tperform_sorting = false;\n+\t} else if (op.unbound_expressions[0]->return_type.InternalType() == PhysicalType::VARCHAR) {\n+\t\tperform_sorting = false;\n \t}\n-\tprojections.emplace_back(new_column_types.size() - 1);\n-\n-\tauto physical_order =\n-\t    make_uniq<PhysicalOrder>(new_column_types, std::move(orders), std::move(projections), op.estimated_cardinality);\n-\tphysical_order->children.push_back(std::move(null_filter));\n \n \t// actual physical create index operator\n \n \tauto physical_create_index =\n \t    make_uniq<PhysicalCreateIndex>(op, op.table, op.info->column_ids, std::move(op.info),\n-\t                                   std::move(op.unbound_expressions), op.estimated_cardinality);\n-\tphysical_create_index->children.push_back(std::move(physical_order));\n+\t                                   std::move(op.unbound_expressions), op.estimated_cardinality, perform_sorting);\n+\n+\tif (perform_sorting) {\n+\n+\t\t// optional order operator\n+\t\tvector<BoundOrderByNode> orders;\n+\t\tvector<idx_t> projections;\n+\t\tfor (idx_t i = 0; i < new_column_types.size() - 1; i++) {\n+\t\t\tauto col_expr = make_uniq_base<Expression, BoundReferenceExpression>(new_column_types[i], i);\n+\t\t\torders.emplace_back(OrderType::ASCENDING, OrderByNullType::NULLS_FIRST, std::move(col_expr));\n+\t\t\tprojections.emplace_back(i);\n+\t\t}\n+\t\tprojections.emplace_back(new_column_types.size() - 1);\n+\n+\t\tauto physical_order = make_uniq<PhysicalOrder>(new_column_types, std::move(orders), std::move(projections),\n+\t\t                                               op.estimated_cardinality);\n+\t\tphysical_order->children.push_back(std::move(null_filter));\n+\n+\t\tphysical_create_index->children.push_back(std::move(physical_order));\n+\t} else {\n+\n+\t\t// no ordering\n+\t\tphysical_create_index->children.push_back(std::move(null_filter));\n+\t}\n+\n \treturn std::move(physical_create_index);\n }\n \ndiff --git a/src/include/duckdb/execution/index/art/art.hpp b/src/include/duckdb/execution/index/art/art.hpp\nindex 15cff52aa111..714b7278c305 100644\n--- a/src/include/duckdb/execution/index/art/art.hpp\n+++ b/src/include/duckdb/execution/index/art/art.hpp\n@@ -35,14 +35,16 @@ class ART : public Index {\n \t//! Constructs an ART\n \tART(const vector<column_t> &column_ids, TableIOManager &table_io_manager,\n \t    const vector<unique_ptr<Expression>> &unbound_expressions, const IndexConstraintType constraint_type,\n-\t    AttachedDatabase &db, const idx_t block_id = DConstants::INVALID_INDEX,\n-\t    const idx_t block_offset = DConstants::INVALID_INDEX);\n+\t    AttachedDatabase &db, const shared_ptr<vector<FixedSizeAllocator>> &allocators_ptr = nullptr,\n+\t    const idx_t block_id = DConstants::INVALID_INDEX, const idx_t block_offset = DConstants::INVALID_INDEX);\n \t~ART() override;\n \n \t//! Root of the tree\n \tunique_ptr<Node> tree;\n \t//! Fixed-size allocators holding the ART nodes\n-\tvector<unique_ptr<FixedSizeAllocator>> allocators;\n+\tshared_ptr<vector<FixedSizeAllocator>> allocators;\n+\t//! True, if the ART owns its data\n+\tbool owns_data;\n \n public:\n \t//! Initialize a single predicate scan on the index with the given expression and column IDs\n@@ -102,12 +104,12 @@ class ART : public Index {\n \n \t//! Find the node with a matching key, or return nullptr if not found\n \tNode Lookup(Node node, const ARTKey &key, idx_t depth);\n+\t//! Insert a key into the tree\n+\tbool Insert(Node &node, const ARTKey &key, idx_t depth, const row_t &row_id);\n \n private:\n \t//! Insert a row ID into a leaf\n \tbool InsertToLeaf(Node &leaf, const row_t &row_id);\n-\t//! Insert a key into the tree\n-\tbool Insert(Node &node, const ARTKey &key, idx_t depth, const row_t &row_id);\n \t//! Erase a key from the tree (if a leaf has more than one value) or erase the leaf itself\n \tvoid Erase(Node &node, const ARTKey &key, idx_t depth, const row_t &row_id);\n \ndiff --git a/src/include/duckdb/execution/index/art/leaf.hpp b/src/include/duckdb/execution/index/art/leaf.hpp\nindex f754c865721e..fb8e88f5118a 100644\n--- a/src/include/duckdb/execution/index/art/leaf.hpp\n+++ b/src/include/duckdb/execution/index/art/leaf.hpp\n@@ -49,9 +49,9 @@ class Leaf {\n \t\treturn *Node::GetAllocator(art, NType::LEAF).Get<Leaf>(ptr);\n \t}\n \n-\t//! Initializes a merge by incrementing the buffer IDs of the leaf\n+\t//! Initializes a merge by incrementing the buffer IDs of the leaf (chain)\n \tstatic void InitializeMerge(ART &art, Node &node, const ARTFlags &flags);\n-\t//! Merge leaves and free all copied leaf nodes\n+\t//! Merge leaves (chains) and free all copied leaf nodes\n \tstatic void Merge(ART &art, Node &l_node, Node &r_node);\n \n \t//! Insert a row ID into a leaf\n@@ -66,15 +66,15 @@ class Leaf {\n \t//! Returns whether the leaf contains the row ID\n \tstatic bool ContainsRowId(ART &art, Node &node, const row_t row_id);\n \n-\t//! Returns the string representation of the leaf, or only traverses and verifies the leaf\n+\t//! Returns the string representation of the leaf (chain), or only traverses and verifies the leaf (chain)\n \tstatic string VerifyAndToString(ART &art, Node &node);\n \n-\t//! Serialize the leaf\n+\t//! Serialize the leaf (chain)\n \tstatic BlockPointer Serialize(ART &art, Node &node, MetaBlockWriter &writer);\n-\t//! Deserialize the leaf\n+\t//! Deserialize the leaf (chain)\n \tstatic void Deserialize(ART &art, Node &node, MetaBlockReader &reader);\n \n-\t//! Vacuum the leaf\n+\t//! Vacuum the leaf (chain)\n \tstatic void Vacuum(ART &art, Node &node);\n \n private:\ndiff --git a/src/include/duckdb/execution/index/art/node.hpp b/src/include/duckdb/execution/index/art/node.hpp\nindex ca1bdbfbe0cd..ad671f3f6c5c 100644\n--- a/src/include/duckdb/execution/index/art/node.hpp\n+++ b/src/include/duckdb/execution/index/art/node.hpp\n@@ -180,6 +180,12 @@ class Node {\n \t\tdata = 0;\n \t}\n \n+\t//! Adds an idx_t to a buffer ID, the rightmost 32 bits contain the buffer ID\n+\tinline void AddToBufferID(const idx_t summand) {\n+\t\tD_ASSERT(summand < NumericLimits<uint32_t>().Maximum());\n+\t\tdata += summand;\n+\t}\n+\n \t//! Comparison operator\n \tinline bool operator==(const Node &node) const {\n \t\treturn data == node.data;\ndiff --git a/src/include/duckdb/execution/index/art/prefix.hpp b/src/include/duckdb/execution/index/art/prefix.hpp\nindex df2689599007..1aaac5148bd0 100644\n--- a/src/include/duckdb/execution/index/art/prefix.hpp\n+++ b/src/include/duckdb/execution/index/art/prefix.hpp\n@@ -42,10 +42,8 @@ class Prefix {\n \t\treturn *Node::GetAllocator(art, NType::PREFIX).Get<Prefix>(ptr);\n \t}\n \n-\t//! Initializes a merge by incrementing the buffer ID of the child node(s)\n-\tinline void InitializeMerge(ART &art, const ARTFlags &flags) {\n-\t\tptr.InitializeMerge(art, flags);\n-\t}\n+\t//! Initializes a merge by incrementing the buffer ID of the prefix and its child node(s)\n+\tstatic void InitializeMerge(ART &art, Node &node, const ARTFlags &flags);\n \n \t//! Appends a byte and a child_prefix to prefix. If there is no prefix, than it pushes the\n \t//! byte on top of child_prefix. If there is no child_prefix, then it creates a new\n@@ -75,15 +73,13 @@ class Prefix {\n \t//! Returns the string representation of the node, or only traverses and verifies the node and its subtree\n \tstatic string VerifyAndToString(ART &art, Node &node, const bool only_verify);\n \n-\t//! Serialize this node\n-\tBlockPointer Serialize(ART &art, MetaBlockWriter &writer);\n-\t//! Deserialize this node\n-\tvoid Deserialize(MetaBlockReader &reader);\n+\t//! Serialize this node and all subsequent nodes\n+\tstatic BlockPointer Serialize(ART &art, Node &node, MetaBlockWriter &writer);\n+\t//! Deserialize this node and all subsequent prefix nodes\n+\tstatic void Deserialize(ART &art, Node &node, MetaBlockReader &reader);\n \n \t//! Vacuum the child of the node\n-\tinline void Vacuum(ART &art, const ARTFlags &flags) {\n-\t\tptr.Vacuum(art, flags);\n-\t}\n+\tstatic void Vacuum(ART &art, Node &node, const ARTFlags &flags);\n \n private:\n \t//! Appends the byte to this prefix node, or creates a subsequent prefix node,\n@@ -92,6 +88,8 @@ class Prefix {\n \t//! Appends the other_prefix and all its subsequent prefix nodes to this prefix node.\n \t//! Also frees all copied/appended nodes\n \tvoid Append(ART &art, Node other_prefix);\n+\t//! Get the total count of bytes in the chain of prefixes, with the node reference pointing to first non-prefix node\n+\tstatic idx_t TotalCount(ART &art, reference<Node> &node);\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/execution/operator/schema/physical_create_index.hpp b/src/include/duckdb/execution/operator/schema/physical_create_index.hpp\nindex b28fff51c90f..d85f7fa3b6ca 100644\n--- a/src/include/duckdb/execution/operator/schema/physical_create_index.hpp\n+++ b/src/include/duckdb/execution/operator/schema/physical_create_index.hpp\n@@ -27,7 +27,7 @@ class PhysicalCreateIndex : public PhysicalOperator {\n public:\n \tPhysicalCreateIndex(LogicalOperator &op, TableCatalogEntry &table, const vector<column_t> &column_ids,\n \t                    unique_ptr<CreateIndexInfo> info, vector<unique_ptr<Expression>> unbound_expressions,\n-\t                    idx_t estimated_cardinality);\n+\t                    idx_t estimated_cardinality, const bool sorted);\n \n \t//! The table to create the index for\n \tDuckTableEntry &table;\n@@ -37,6 +37,8 @@ class PhysicalCreateIndex : public PhysicalOperator {\n \tunique_ptr<CreateIndexInfo> info;\n \t//! Unbound expressions to be used in the optimizer\n \tvector<unique_ptr<Expression>> unbound_expressions;\n+\t//! Whether the pipeline sorts the data prior to index creation\n+\tconst bool sorted;\n \n public:\n \t//! Source interface, NOP for this operator\n@@ -52,6 +54,11 @@ class PhysicalCreateIndex : public PhysicalOperator {\n \t//! Sink interface, global sink state\n \tunique_ptr<GlobalSinkState> GetGlobalSinkState(ClientContext &context) const override;\n \n+\t//! Sink for unsorted data: insert iteratively\n+\tSinkResultType SinkUnsorted(Vector &row_identifiers, OperatorSinkInput &input) const;\n+\t//! Sink for sorted data: build + merge\n+\tSinkResultType SinkSorted(Vector &row_identifiers, OperatorSinkInput &input) const;\n+\n \tSinkResultType Sink(ExecutionContext &context, DataChunk &chunk, OperatorSinkInput &input) const override;\n \tSinkCombineResultType Combine(ExecutionContext &context, OperatorSinkCombineInput &input) const override;\n \tSinkFinalizeType Finalize(Pipeline &pipeline, Event &event, ClientContext &context,\ndiff --git a/src/storage/checkpoint_manager.cpp b/src/storage/checkpoint_manager.cpp\nindex c633fb00cf0c..8872877f44c9 100644\n--- a/src/storage/checkpoint_manager.cpp\n+++ b/src/storage/checkpoint_manager.cpp\n@@ -394,7 +394,7 @@ void CheckpointReader::ReadIndex(ClientContext &context, MetaBlockReader &reader\n \tcase IndexType::ART: {\n \t\tauto &storage = table_catalog.GetStorage();\n \t\tauto art = make_uniq<ART>(index_info.column_ids, TableIOManager::Get(storage), std::move(unbound_expressions),\n-\t\t                          index_info.constraint_type, storage.db, root_block_id, root_offset);\n+\t\t                          index_info.constraint_type, storage.db, nullptr, root_block_id, root_offset);\n \t\tindex_catalog.index = art.get();\n \t\tstorage.info->indexes.AddIndex(std::move(art));\n \t\tbreak;\n",
  "test_patch": "diff --git a/test/sql/copy/csv/auto/test_fallback_all_varchar.test b/test/sql/copy/csv/auto/test_fallback_all_varchar.test\nindex 73c27592635b..38f731eafd99 100644\n--- a/test/sql/copy/csv/auto/test_fallback_all_varchar.test\n+++ b/test/sql/copy/csv/auto/test_fallback_all_varchar.test\n@@ -5,7 +5,6 @@\n statement ok\n PRAGMA verify_parallelism\n \n-\n # CSV file with irregularity in first column and default sample size\n statement ok\n CREATE TABLE test AS SELECT * FROM read_csv_auto ('test/sql/copy/csv/data/auto/test_fallback.csv');\ndiff --git a/test/sql/index/art/create_drop/test_art_create_unique.test b/test/sql/index/art/create_drop/test_art_create_unique.test\nindex afffc934e7ff..907e12b66ed0 100644\n--- a/test/sql/index/art/create_drop/test_art_create_unique.test\n+++ b/test/sql/index/art/create_drop/test_art_create_unique.test\n@@ -21,3 +21,18 @@ query I\n SELECT * FROM t0 WHERE t0.c0 = 1;\n ----\n 1\n+\n+statement ok\n+CREATE TABLE merge_violation (id INT);\n+\n+statement ok\n+INSERT INTO merge_violation SELECT range FROM range(2048);\n+\n+statement ok\n+INSERT INTO merge_violation SELECT range + 10000 FROM range(2048);\n+\n+statement ok\n+INSERT INTO merge_violation VALUES (2047);\n+\n+statement error\n+CREATE UNIQUE INDEX idx ON merge_violation(id);\ndiff --git a/test/sql/index/art/issues/test_art_issue_8066.test b/test/sql/index/art/issues/test_art_issue_8066.test\nnew file mode 100644\nindex 000000000000..78a099b0e5b4\n--- /dev/null\n+++ b/test/sql/index/art/issues/test_art_issue_8066.test\n@@ -0,0 +1,16 @@\n+# name: test/sql/index/art/issues/test_art_issue_8066.test\n+# description: Test CREATE INDEX on a lot of duplicate values with a persistent DB\n+# group: [issues]\n+\n+load __TEST_DIR__/test_index.db\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE t AS SELECT now() AS d FROM generate_series(1, 218165);\n+\n+statement ok\n+CREATE INDEX i ON t(d);\n+\n+restart\ndiff --git a/test/sql/index/art/memory/test_art_varchar.test_slow b/test/sql/index/art/memory/test_art_varchar.test_slow\nnew file mode 100644\nindex 000000000000..2325e6da9919\n--- /dev/null\n+++ b/test/sql/index/art/memory/test_art_varchar.test_slow\n@@ -0,0 +1,74 @@\n+# name: test/sql/index/art/memory/test_art_varchar.test_slow\n+# description: Test the memory usage of the ART for a big table with a VARCHAR column\n+# group: [memory]\n+\n+# test issue #7760\n+\n+require vector_size 2048\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE FUNCTION mem_to_bytes(x) AS CASE\n+    WHEN CONTAINS(x, 'KB') THEN REPLACE(x, 'KB', '')::BIGINT * 1000\n+    WHEN CONTAINS(x, 'MB') THEN REPLACE(x, 'MB', '')::BIGINT * 1000 * 1000\n+    WHEN CONTAINS(x, 'GB') THEN REPLACE(x, 'GB', '')::BIGINT * 1000 * 1000 * 1000\n+    WHEN CONTAINS(x, 'TB') THEN REPLACE(x, 'TB', '')::BIGINT * 1000 * 1000 * 1000 * 1000\n+    WHEN x = '0 bytes' THEN 0::BIGINT\n+    ELSE x::BIGINT END;\n+\n+# 7200000 unique strings\n+\n+statement ok\n+CREATE TABLE art AS\n+    SELECT rpad(((i * 95823983533) % 86000000)::VARCHAR, 10, '-') AS id\n+        FROM range(7200000) tbl(i);\n+\n+# 2 * 7200k entries\n+statement ok\n+INSERT INTO art (SELECT * FROM art);\n+\n+# 4 * 7200k entries\n+statement ok\n+INSERT INTO art (SELECT * FROM art);\n+\n+# 8 * 7200k entries\n+statement ok\n+INSERT INTO art (SELECT * FROM art);\n+\n+# 86M entries\n+statement ok\n+INSERT INTO art (SELECT * FROM art LIMIT 28400000);\n+\n+query I\n+SELECT count(*) FROM art;\n+----\n+86000000\n+\n+query I\n+SELECT COUNT(DISTINCT id) FROM art;\n+----\n+7200000\n+\n+query II\n+SELECT MIN(length(id)), MAX(length(id)) FROM art;\n+----\n+10\t10\n+\n+statement ok\n+CREATE TABLE base AS\n+SELECT mem_to_bytes(memory_usage)::BIGINT AS usage FROM pragma_database_size();\n+\n+statement ok\n+SET memory_limit='12GB';\n+\n+statement ok\n+CREATE INDEX idx ON art USING ART(id);\n+\n+query I\n+SELECT mem_to_bytes(current.memory_usage) > base.usage AND\n+       \tmem_to_bytes(current.memory_usage) < 4 * base.usage\n+FROM base, pragma_database_size() current;\n+----\n+1\n\\ No newline at end of file\ndiff --git a/test/sql/index/art/nodes/test_art_leaf.test b/test/sql/index/art/nodes/test_art_leaf.test\nindex 01bcf678449d..3331cf8d3658 100644\n--- a/test/sql/index/art/nodes/test_art_leaf.test\n+++ b/test/sql/index/art/nodes/test_art_leaf.test\n@@ -233,3 +233,32 @@ query I\n SELECT sum(id_int) FROM tbl_grow_shrink;\n ----\n NULL\n+\n+# merging leaves\n+\n+statement ok\n+CREATE TABLE merge_leaf_tbl (id int);\n+\n+statement ok\n+INSERT INTO merge_leaf_tbl SELECT range FROM range(2048);\n+\n+statement ok\n+INSERT INTO merge_leaf_tbl VALUES (2047);\n+\n+statement ok\n+CREATE INDEX idx_merge_leaf_tbl ON merge_leaf_tbl(id);\n+\n+statement ok\n+DROP INDEX idx_merge_leaf_tbl;\n+\n+statement ok\n+INSERT INTO merge_leaf_tbl VALUES (2047);\n+\n+statement ok\n+INSERT INTO merge_leaf_tbl SELECT range + 10000 FROM range(2048);\n+\n+statement ok\n+INSERT INTO merge_leaf_tbl SELECT range + 10000 FROM range(2048);\n+\n+statement ok\n+CREATE INDEX idx_merge_leaf_tbl ON merge_leaf_tbl(id);\ndiff --git a/test/sql/index/art/nodes/test_art_prefixes.test b/test/sql/index/art/nodes/test_art_prefixes.test\nindex a7fc06c695de..642cdf2bb57d 100644\n--- a/test/sql/index/art/nodes/test_art_prefixes.test\n+++ b/test/sql/index/art/nodes/test_art_prefixes.test\n@@ -5,8 +5,6 @@\n statement ok\n PRAGMA enable_verification\n \n-require vector_size 2048\n-\n # very mixed-length prefixes\n \n statement ok\ndiff --git a/test/sql/index/art/storage/test_art_storage.test b/test/sql/index/art/storage/test_art_storage.test\nindex 3154be4cd202..ad37609a4a43 100644\n--- a/test/sql/index/art/storage/test_art_storage.test\n+++ b/test/sql/index/art/storage/test_art_storage.test\n@@ -2,8 +2,6 @@\n # description: Test ART storage\n # group: [storage]\n \n-require vector_size 2048\n-\n statement ok\n PRAGMA enable_verification\n \ndiff --git a/test/sql/index/art/vacuum/test_art_vacuum.test b/test/sql/index/art/vacuum/test_art_vacuum.test\nnew file mode 100644\nindex 000000000000..20ef1643dcd8\n--- /dev/null\n+++ b/test/sql/index/art/vacuum/test_art_vacuum.test\n@@ -0,0 +1,27 @@\n+# name: test/sql/index/art/vacuum/test_art_vacuum.test\n+# description: Test vacuuming leaves\n+# group: [vacuum]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE delete_vacuum (id INT);\n+\n+statement ok\n+INSERT INTO delete_vacuum SELECT 10 FROM range(10000);\n+\n+statement ok\n+INSERT INTO delete_vacuum SELECT 11 FROM range(10000);\n+\n+statement ok\n+INSERT INTO delete_vacuum SELECT 12 FROM range(10000);\n+\n+statement ok\n+INSERT INTO delete_vacuum SELECT 13 FROM range(10000);\n+\n+statement ok\n+CREATE INDEX idx ON delete_vacuum(id);\n+\n+statement ok\n+DELETE FROM delete_vacuum WHERE id = 11 OR id = 10;\n\\ No newline at end of file\ndiff --git a/test/sql/parallelism/interquery/test_concurrent_index.cpp b/test/sql/parallelism/interquery/test_concurrent_index.cpp\nindex 82562552292a..81d998ff5c60 100644\n--- a/test/sql/parallelism/interquery/test_concurrent_index.cpp\n+++ b/test/sql/parallelism/interquery/test_concurrent_index.cpp\n@@ -81,8 +81,6 @@ static void append_to_integers(DuckDB *db, idx_t threadnr) {\n }\n \n TEST_CASE(\"Concurrent writes during index creation\", \"[index][.]\") {\n-\t// FIXME: this is extremely slow due to an overhead in calls to the index vacuum operation (#7406)\n-\treturn;\n \n \tduckdb::unique_ptr<QueryResult> result;\n \tDuckDB db(nullptr);\n@@ -356,8 +354,6 @@ static void join_integers(Connection *con, bool *index_join_success, idx_t threa\n }\n \n TEST_CASE(\"Concurrent appends during index join\", \"[interquery][.]\") {\n-\t// FIXME: this is extremely slow due to an overhead in calls to the index vacuum operation (#7406)\n-\treturn;\n \n \tduckdb::unique_ptr<QueryResult> result;\n \tDuckDB db(nullptr);\n",
  "problem_statement": "Avoid excessively calling vacuum during concurrent index operations\n### What happens?\n\nCurrently, we disable two tests focusing on concurrent index operations, `\"Concurrent writes during index creation\"` and `\"Concurrent appends during index join\"` (`test/sql/parallelism/interquery/test_concurrent_index.cpp`). #7372 fixes them; however, they take excessive time to finish.\r\n\r\nWhen profiling the tests, it becomes apparent that most time is spent in the `vacuum` operation of the ART while flushing the changes to storage. This should not be the case, i.e., we first check if a vacuum is necessary and only perform it when passing a threshold. I suspect that the local state in the separate threads either triggers the vacuum for each thread independently (isolation) or that there's a bug in the threshold check.\r\n\r\n![Screenshot 2023-05-08 at 14 04 42](https://user-images.githubusercontent.com/44262898/236823369-7681fdc3-2c95-4702-8d2c-9a831f711d5f.png)\r\n\n\n### To Reproduce\n\nBy enabling and running any of the two tests: `\"Concurrent writes during index creation\"` or `\"Concurrent appends during index join\"`.\n\n### OS:\n\niOS\n\n### DuckDB Version:\n\nnewest\n\n### DuckDB Client:\n\nCLI / test runner\n\n### Full Name:\n\nTania Bogatsch\n\n### Affiliation:\n\nDuckDB Labs\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\nAvoid excessively calling vacuum during concurrent index operations\n### What happens?\n\nCurrently, we disable two tests focusing on concurrent index operations, `\"Concurrent writes during index creation\"` and `\"Concurrent appends during index join\"` (`test/sql/parallelism/interquery/test_concurrent_index.cpp`). #7372 fixes them; however, they take excessive time to finish.\r\n\r\nWhen profiling the tests, it becomes apparent that most time is spent in the `vacuum` operation of the ART while flushing the changes to storage. This should not be the case, i.e., we first check if a vacuum is necessary and only perform it when passing a threshold. I suspect that the local state in the separate threads either triggers the vacuum for each thread independently (isolation) or that there's a bug in the threshold check.\r\n\r\n![Screenshot 2023-05-08 at 14 04 42](https://user-images.githubusercontent.com/44262898/236823369-7681fdc3-2c95-4702-8d2c-9a831f711d5f.png)\r\n\n\n### To Reproduce\n\nBy enabling and running any of the two tests: `\"Concurrent writes during index creation\"` or `\"Concurrent appends during index join\"`.\n\n### OS:\n\niOS\n\n### DuckDB Version:\n\nnewest\n\n### DuckDB Client:\n\nCLI / test runner\n\n### Full Name:\n\nTania Bogatsch\n\n### Affiliation:\n\nDuckDB Labs\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "\n",
  "created_at": "2023-08-01T11:55:36Z"
}