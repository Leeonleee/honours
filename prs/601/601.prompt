You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
BETWEEN with COLLATE NOACCENT.NOCASE expression results in a segfault/ASan failure
Consider the following statements:
```sql
CREATE TABLE t0(c0 DATE, c1 VARCHAR);
INSERT INTO t0(c0) VALUES (NULL), ('2000-01-01');
SELECT * FROM t0 WHERE 'a' BETWEEN c0 AND c1 COLLATE NOACCENT.NOCASE; -- segfault
```
Unexpectedly, the `SELECT` causes an ASan failure (the initial test case crashed the JVM when using the JDBC driver):
```
AddressSanitizer:DEADLYSIGNAL
=================================================================
==27945==ERROR: AddressSanitizer: SEGV on unknown address 0x000000000000 (pc 0x561f1a6c2d18 bp 0x7ffccd196060 sp 0x7ffccd196040 T0)
==27945==The signal is caused by a READ memory access.
==27945==Hint: address points to the zero page.
    #0 0x561f1a6c2d17 in duckdb::Utf8Proc::Analyze(char const*, unsigned long) /duckdb/third_party/utf8proc/utf8proc_wrapper.cpp:28
    #1 0x561f1a6d60b8 in duckdb::string_t::Verify() /duckdb/src/common/types/string_type.cpp:13
    #2 0x561f1a6edcf7 in duckdb::Vector::UTFVerify(duckdb::SelectionVector const&, unsigned long) /duckdb/src/common/types/vector.cpp:605
    #3 0x561f1ae3a88a in duckdb::ExpressionExecutor::Execute(duckdb::BoundFunctionExpression&, duckdb::ExpressionState*, duckdb::SelectionVector const*, unsigned long, duckdb::Vector&) /duckdb/src/execution/expression_executor/execute_function.cpp:40
    #4 0x561f1a97fccc in duckdb::ExpressionExecutor::Execute(duckdb::Expression&, duckdb::ExpressionState*, duckdb::SelectionVector const*, unsigned long, duckdb::Vector&) /duckdb/src/execution/expression_executor.cpp:146
    #5 0x561f1ae38070 in duckdb::ExpressionExecutor::Select(duckdb::BoundComparisonExpression&, duckdb::ExpressionState*, duckdb::SelectionVector const*, unsigned long, duckdb::SelectionVector*, duckdb::SelectionVector*) /duckdb/src/execution/expression_executor/execute_comparison.cpp:82
    #6 0x561f1a98008d in duckdb::ExpressionExecutor::Select(duckdb::Expression&, duckdb::ExpressionState*, duckdb::SelectionVector const*, unsigned long, duckdb::SelectionVector*, duckdb::SelectionVector*) /duckdb/src/execution/expression_executor.cpp:171
    #7 0x561f1ae39719 in duckdb::ExpressionExecutor::Select(duckdb::BoundConjunctionExpression&, duckdb::ExpressionState*, duckdb::SelectionVector const*, unsigned long, duckdb::SelectionVector*, duckdb::SelectionVector*) /duckdb/src/execution/expression_executor/execute_conjunction.cpp:80
    #8 0x561f1a9800c0 in duckdb::ExpressionExecutor::Select(duckdb::Expression&, duckdb::ExpressionState*, duckdb::SelectionVector const*, unsigned long, duckdb::SelectionVector*, duckdb::SelectionVector*) /duckdb/src/execution/expression_executor.cpp:173
    #9 0x561f1a97ed10 in duckdb::ExpressionExecutor::SelectExpression(duckdb::DataChunk&, duckdb::SelectionVector&) /duckdb/src/execution/expression_executor.cpp:60
    #10 0x561f1b1d62f1 in duckdb::PhysicalFilter::GetChunkInternal(duckdb::ClientContext&, duckdb::DataChunk&, duckdb::PhysicalOperatorState*) /duckdb/src/execution/operator/filter/physical_filter.cpp:44
    #11 0x561f1a98a87b in duckdb::PhysicalOperator::GetChunk(duckdb::ClientContext&, duckdb::DataChunk&, duckdb::PhysicalOperatorState*) /duckdb/src/execution/physical_operator.cpp:45
    #12 0x561f1b219a54 in duckdb::PhysicalProjection::GetChunkInternal(duckdb::ClientContext&, duckdb::DataChunk&, duckdb::PhysicalOperatorState*) /duckdb/src/execution/operator/projection/physical_projection.cpp:22
    #13 0x561f1a98a87b in duckdb::PhysicalOperator::GetChunk(duckdb::ClientContext&, duckdb::DataChunk&, duckdb::PhysicalOperatorState*) /duckdb/src/execution/physical_operator.cpp:45
    #14 0x561f1b1d70f6 in duckdb::PhysicalExecute::GetChunkInternal(duckdb::ClientContext&, duckdb::DataChunk&, duckdb::PhysicalOperatorState*) /duckdb/src/execution/operator/helper/physical_execute.cpp:8
    #15 0x561f1a98a87b in duckdb::PhysicalOperator::GetChunk(duckdb::ClientContext&, duckdb::DataChunk&, duckdb::PhysicalOperatorState*) /duckdb/src/execution/physical_operator.cpp:45
    #16 0x561f1a9d84fd in duckdb::ClientContext::FetchInternal() /duckdb/src/main/client_context.cpp:153
    #17 0x561f1a9d9d98 in duckdb::ClientContext::ExecutePreparedStatement(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, duckdb::PreparedStatementData&, std::vector<duckdb::Value, std::allocator<duckdb::Value> >, bool) /duckdb/src/main/client_context.cpp:232
    #18 0x561f1a9dc751 in duckdb::ClientContext::RunStatementInternal(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unique_ptr<duckdb::SQLStatement, std::default_delete<duckdb::SQLStatement> >, bool) /duckdb/src/main/client_context.cpp:340
    #19 0x561f1a9dd0d7 in duckdb::ClientContext::RunStatement(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unique_ptr<duckdb::SQLStatement, std::default_delete<duckdb::SQLStatement> >, bool) /duckdb/src/main/client_context.cpp:367
    #20 0x561f1a9dbb49 in duckdb::ClientContext::Execute(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::vector<duckdb::Value, std::allocator<duckdb::Value> >&, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) /duckdb/src/main/client_context.cpp:311
    #21 0x561f1a9f542b in duckdb::PreparedStatement::Execute(std::vector<duckdb::Value, std::allocator<duckdb::Value> >&, bool) /duckdb/src/main/prepared_statement.cpp:37
    #22 0x561f1a6a2810 in sqlite3_step /duckdb/tools/sqlite3_api_wrapper/sqlite3_api_wrapper.cpp:195
    #23 0x561f1a67c1d7 in exec_prepared_stmt /duckdb/tools/shell/shell.c:9899
    #24 0x561f1a67dfb7 in shell_exec /duckdb/tools/shell/shell.c:10187
    #25 0x561f1a69739d in runOneSqlLine /duckdb/tools/shell/shell.c:15047
    #26 0x561f1a697cd7 in process_input /duckdb/tools/shell/shell.c:15149
    #27 0x561f1a69a3e4 in main /duckdb/tools/shell/shell.c:15817
    #28 0x7f6e102a7b6a in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x26b6a)
    #29 0x561f1a6577c9 in _start (/duckdb/build/debug/duckdb_cli+0x9067c9)
```
I found this based on commit 22a5f36ee41542c915c73dc44d90d29fcbe9cbcb.

</issue>
<code>
[start of README.md]
1: <img align="left" src="logo/duckdb-logo.png" height="120">
2: 
3: # DuckDB, the SQLite for Analytics
4: [![Travis](https://api.travis-ci.org/cwida/duckdb.svg?branch=master)](https://travis-ci.org/cwida/duckdb)
5: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
6: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
7: 
8: <br>
9: 
10: 
11: # Requirements
12: DuckDB requires [CMake](https://cmake.org) to be installed and a `C++11` compliant compiler. GCC 4.9 and newer, Clang 3.9 and newer and VisualStudio 2017 are tested on each revision.
13: 
14: ## Compiling
15: Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You may run `make unit` and `make allunit` to verify that your version works properly after making changes.
16: 
17: # Usage
18: A command line utility based on `sqlite3` can be found in either `build/release/duckdb_cli` (release, the default) or `build/debug/duckdb_cli` (debug).
19: 
20: # Embedding
21: As DuckDB is an embedded database, there is no database server to launch or client to connect to a running server. However, the database server can be embedded directly into an application using the C or C++ bindings. The main build process creates the shared library `build/release/src/libduckdb.[so|dylib|dll]` that can be linked against. A static library is built as well.
22: 
23: For examples on how to embed DuckDB into your application, see the [examples](https://github.com/cwida/duckdb/tree/master/examples) folder.
24: 
25: ## Benchmarks
26: After compiling, benchmarks can be executed from the root directory by executing `./build/release/benchmark/benchmark_runner`.
27: 
28: ## Standing on the Shoulders of Giants
29: DuckDB is implemented in C++ 11, should compile with GCC and clang, uses CMake to build and [Catch2](https://github.com/catchorg/Catch2) for testing. DuckDB uses some components from various Open-Source databases and draws inspiration from scientific publications. Here is an overview:
30: 
31: * Parser: We use the PostgreSQL parser that was [repackaged as a stand-alone library](https://github.com/lfittl/libpg_query). The translation to our own parse tree is inspired by [Peloton](https://pelotondb.io).
32: * Shell: We have adapted the [SQLite shell](https://sqlite.org/cli.html) to work with DuckDB.
33: * Tests: We use the [SQL Logic Tests from SQLite](https://www.sqlite.org/sqllogictest/doc/trunk/about.wiki) to test DuckDB.
34: * Query fuzzing: We use [SQLsmith](https://github.com/anse1/sqlsmith) to generate random queries for additional testing.
35: * Date Math: We use the date math component from [MonetDB](https://www.monetdb.org).
36: * SQL Window Functions: DuckDB's window functions implementation uses Segment Tree Aggregation as described in the paper "Efficient Processing of Window Functions in Analytical SQL Queries" by Viktor Leis, Kan Kundhikanjana, Alfons Kemper and Thomas Neumann.
37: * Execution engine: The vectorized execution engine is inspired by the paper "MonetDB/X100: Hyper-Pipelining Query Execution" by Peter Boncz, Marcin Zukowski and Niels Nes.
38: * Optimizer: DuckDB's optimizer draws inspiration from the papers "Dynamic programming strikes back" by Guido Moerkotte and Thomas Neumman as well as "Unnesting Arbitrary Queries" by Thomas Neumann and Alfons Kemper.
39: * Concurrency control: Our MVCC implementation is inspired by the paper "Fast Serializable Multi-Version Concurrency Control for Main-Memory Database Systems" by Thomas Neumann, Tobias Mühlbauer and Alfons Kemper.
40: * Regular Expression: DuckDB uses Google's [RE2](https://github.com/google/re2) regular expression engine.
41: 
42: ## Other pages
43: * [Continuous Benchmarking (CB™)](https://www.duckdb.org/benchmarks/index.html), runs TPC-H, TPC-DS and some microbenchmarks on every commit
[end of README.md]
[start of extension/icu/icu-extension.cpp]
1: #include "icu-extension.hpp"
2: #include "icu-collate.hpp"
3: 
4: #include "duckdb/main/database.hpp"
5: #include "duckdb/main/connection.hpp"
6: 
7: #include "duckdb/common/string_util.hpp"
8: #include "duckdb/planner/expression/bound_function_expression.hpp"
9: #include "duckdb/function/scalar_function.hpp"
10: #include "duckdb/common/vector_operations/unary_executor.hpp"
11: #include "duckdb/parser/parsed_data/create_collation_info.hpp"
12: #include "duckdb/catalog/catalog.hpp"
13: 
14: namespace duckdb {
15: 
16: struct IcuBindData : public FunctionData {
17: 	std::unique_ptr<icu::Collator> collator;
18: 	string language;
19: 	string country;
20: 
21: 	IcuBindData(string language_p, string country_p) :
22: 		language(move(language_p)), country(move(country_p)) {
23: 		UErrorCode status = U_ZERO_ERROR;
24: 		this->collator = std::unique_ptr<icu::Collator>(icu::Collator::createInstance(icu::Locale(language.c_str(), country.c_str()), status));
25: 		if (U_FAILURE(status)) {
26: 			throw Exception("Failed to create ICU collator!");
27: 		}
28: 	}
29: 
30: 	unique_ptr<FunctionData> Copy() override {
31: 		return make_unique<IcuBindData>(language.c_str(), country.c_str());
32: 	}
33: };
34: 
35: static void icu_collate_function(DataChunk &args, ExpressionState &state, Vector &result) {
36: 	auto &func_expr = (BoundFunctionExpression &)state.expr;
37: 	auto &info = (IcuBindData &)*func_expr.bind_info;
38: 	auto &collator = *info.collator;
39: 
40: 	unique_ptr<char[]> buffer;
41: 	int32_t buffer_size = 0;
42: 	UnaryExecutor::Execute<string_t, string_t, true>(args.data[0], result, args.size(), [&](string_t input) {
43: 		// create a sort key from the string
44: 		int32_t string_size = collator.getSortKey(icu::UnicodeString::fromUTF8(icu::StringPiece(input.GetData(), input.GetSize())), (uint8_t*) buffer.get(), buffer_size);
45: 		if (string_size > buffer_size) {
46: 			// have to resize the buffer
47: 			buffer_size = string_size + 1;
48: 			buffer = unique_ptr<char[]>(new char[buffer_size]);
49: 
50: 			string_size = collator.getSortKey(icu::UnicodeString::fromUTF8(icu::StringPiece(input.GetData(), input.GetSize())), (uint8_t*) buffer.get(), buffer_size);
51: 		}
52: 		return StringVector::AddBlob(result, string_t(buffer.get(), buffer_size));
53: 	});
54: }
55: 
56: static unique_ptr<FunctionData> icu_collate_bind(BoundFunctionExpression &expr, ClientContext &context) {
57: 	auto splits = StringUtil::Split(expr.function.name, "_");
58: 	if (splits.size() == 1) {
59: 		return make_unique<IcuBindData>(splits[0], "");
60: 	} else if (splits.size() == 2) {
61: 		return make_unique<IcuBindData>(splits[0], splits[1]);
62: 	} else {
63: 		throw InternalException("Expected one or two splits");
64: 	}
65: }
66: 
67: static ScalarFunction get_icu_function(string collation) {
68: 	return ScalarFunction(collation, {SQLType::VARCHAR}, SQLType::BIGINT, icu_collate_function, false, icu_collate_bind);
69: }
70: 
71: void ICUExtension::Load(DuckDB &db) {
72: 	// load the collations
73: 	Connection con(db);
74: 	con.BeginTransaction();
75: 
76: 	// iterate over all the collations
77: 	int32_t count;
78: 	auto locales = icu::Collator::getAvailableLocales(count);
79: 	for(int32_t i = 0; i < count; i++) {
80: 		string collation;
81: 		if (string(locales[i].getCountry()).empty()) {
82: 			// language only
83: 			collation = locales[i].getLanguage();
84: 		} else {
85: 			// language + country
86: 			collation = locales[i].getLanguage() + string("_") + locales[i].getCountry();
87: 		}
88: 		collation = StringUtil::Lower(collation);
89: 
90: 		CreateCollationInfo info(collation, get_icu_function(collation), false);
91: 		info.on_conflict = OnCreateConflict::IGNORE;
92: 		db.catalog->CreateCollation(*con.context, &info);
93: 	}
94: 
95: 	con.Commit();
96: }
97: 
98: }
[end of extension/icu/icu-extension.cpp]
[start of src/execution/expression_executor/execute_function.cpp]
1: #include "duckdb/execution/expression_executor.hpp"
2: #include "duckdb/planner/expression/bound_function_expression.hpp"
3: 
4: using namespace duckdb;
5: using namespace std;
6: 
7: struct FunctionState : public ExpressionState {
8: 	FunctionState(Expression &expr, ExpressionExecutorState &root) : ExpressionState(expr, root) {
9: 		auto &func = (BoundFunctionExpression &)expr;
10: 		for (auto &child : func.children) {
11: 			child_types.push_back(child->return_type);
12: 		}
13: 	}
14: 
15: 	vector<TypeId> child_types;
16: };
17: 
18: unique_ptr<ExpressionState> ExpressionExecutor::InitializeState(BoundFunctionExpression &expr,
19:                                                                 ExpressionExecutorState &root) {
20: 	auto result = make_unique<FunctionState>(expr, root);
21: 	for (auto &child : expr.children) {
22: 		result->AddChild(child.get());
23: 	}
24: 	return move(result);
25: }
26: 
27: void ExpressionExecutor::Execute(BoundFunctionExpression &expr, ExpressionState *state_, const SelectionVector *sel,
28:                                  idx_t count, Vector &result) {
29: 	auto state = (FunctionState *)state_;
30: 	DataChunk arguments;
31: 	arguments.SetCardinality(count);
32: 	if (state->child_types.size() > 0) {
33: 		arguments.Initialize(state->child_types);
34: 		for (idx_t i = 0; i < expr.children.size(); i++) {
35: 			assert(state->child_types[i] == expr.children[i]->return_type);
36: 			Execute(*expr.children[i], state->child_states[i].get(), sel, count, arguments.data[i]);
37: #ifdef DEBUG
38: 			if (expr.arguments[i].id == SQLTypeId::VARCHAR) {
39: 				if (sel) {
40: 					arguments.data[i].UTFVerify(*sel, count);
41: 				} else {
42: 					arguments.data[i].UTFVerify(count);
43: 				}
44: 			}
45: #endif
46: 		}
47: 		arguments.Verify();
48: 	}
49: 	expr.function.function(arguments, *state, result);
50: 	if (result.type != expr.return_type) {
51: 		throw TypeMismatchException(expr.return_type, result.type,
52: 		                            "expected function to return the former "
53: 		                            "but the function returned the latter");
54: 	}
55: }
[end of src/execution/expression_executor/execute_function.cpp]
[start of src/function/aggregate/algebraic/stddev.cpp]
1: #include "duckdb/function/aggregate/algebraic_functions.hpp"
2: #include "duckdb/common/vector_operations/vector_operations.hpp"
3: #include "duckdb/function/function_set.hpp"
4: 
5: #include <cmath>
6: 
7: using namespace duckdb;
8: using namespace std;
9: 
10: struct stddev_state_t {
11: 	uint64_t count;  //  n
12: 	double mean;     //  M1
13: 	double dsquared; //  M2
14: };
15: 
16: // Streaming approximate standard deviation using Welford's
17: // method, DOI: 10.2307/1266577
18: struct STDDevBaseOperation {
19: 	template <class STATE> static void Initialize(STATE *state) {
20: 		state->count = 0;
21: 		state->mean = 0;
22: 		state->dsquared = 0;
23: 	}
24: 
25: 	template <class INPUT_TYPE, class STATE, class OP>
26: 	static void Operation(STATE *state, INPUT_TYPE *input_data, nullmask_t &nullmask, idx_t idx) {
27: 		// update running mean and d^2
28: 		state->count++;
29: 		const double input = input_data[idx];
30: 		const double mean_differential = (input - state->mean) / state->count;
31: 		const double new_mean = state->mean + mean_differential;
32: 		const double dsquared_increment = (input - new_mean) * (input - state->mean);
33: 		const double new_dsquared = state->dsquared + dsquared_increment;
34: 
35: 		state->mean = new_mean;
36: 		state->dsquared = new_dsquared;
37: 	}
38: 
39: 	template <class INPUT_TYPE, class STATE, class OP>
40: 	static void ConstantOperation(STATE *state, INPUT_TYPE *input_data, nullmask_t &nullmask, idx_t count) {
41: 		for (idx_t i = 0; i < count; i++) {
42: 			Operation<INPUT_TYPE, STATE, OP>(state, input_data, nullmask, 0);
43: 		}
44: 	}
45: 
46: 	template <class STATE, class OP> static void Combine(STATE source, STATE *target) {
47: 		if (target->count == 0) {
48: 			*target = source;
49: 		} else if (source.count > 0) {
50: 			const auto count = target->count + source.count;
51: 			const auto mean = (source.count * source.mean + target->count * target->mean) / count;
52: 			const auto delta = source.mean - target->mean;
53: 			target->dsquared =
54: 			    source.dsquared + target->dsquared + delta * delta * source.count * target->count / count;
55: 			target->mean = mean;
56: 			target->count = count;
57: 		}
58: 	}
59: 
60: 	static bool IgnoreNull() {
61: 		return true;
62: 	}
63: };
64: 
65: struct VarSampOperation : public STDDevBaseOperation {
66: 	template <class T, class STATE>
67: 	static void Finalize(Vector &result, STATE *state, T *target, nullmask_t &nullmask, idx_t idx) {
68: 		if (state->count == 0) {
69: 			nullmask[idx] = true;
70: 		} else {
71: 			target[idx] = state->count > 1 ? (state->dsquared / (state->count - 1)) : 0;
72: 		}
73: 	}
74: };
75: 
76: struct VarPopOperation : public STDDevBaseOperation {
77: 	template <class T, class STATE>
78: 	static void Finalize(Vector &result, STATE *state, T *target, nullmask_t &nullmask, idx_t idx) {
79: 		if (state->count == 0) {
80: 			nullmask[idx] = true;
81: 		} else {
82: 			target[idx] = state->count > 1 ? (state->dsquared / state->count) : 0;
83: 		}
84: 	}
85: };
86: 
87: struct STDDevSampOperation : public STDDevBaseOperation {
88: 	template <class T, class STATE>
89: 	static void Finalize(Vector &result, STATE *state, T *target, nullmask_t &nullmask, idx_t idx) {
90: 		if (state->count == 0) {
91: 			nullmask[idx] = true;
92: 		} else {
93: 			target[idx] = state->count > 1 ? sqrt(state->dsquared / (state->count - 1)) : 0;
94: 		}
95: 	}
96: };
97: 
98: struct STDDevPopOperation : public STDDevBaseOperation {
99: 	template <class T, class STATE>
100: 	static void Finalize(Vector &result, STATE *state, T *target, nullmask_t &nullmask, idx_t idx) {
101: 		if (state->count == 0) {
102: 			nullmask[idx] = true;
103: 		} else {
104: 			target[idx] = state->count > 1 ? sqrt(state->dsquared / state->count) : 0;
105: 		}
106: 	}
107: };
108: 
109: void StdDevSampFun::RegisterFunction(BuiltinFunctions &set) {
110: 	AggregateFunctionSet stddev_samp("stddev_samp");
111: 	stddev_samp.AddFunction(AggregateFunction::UnaryAggregate<stddev_state_t, double, double, STDDevSampOperation>(
112: 	    SQLType::DOUBLE, SQLType::DOUBLE));
113: 	set.AddFunction(stddev_samp);
114: }
115: 
116: void StdDevPopFun::RegisterFunction(BuiltinFunctions &set) {
117: 	AggregateFunctionSet stddev_pop("stddev_pop");
118: 	stddev_pop.AddFunction(AggregateFunction::UnaryAggregate<stddev_state_t, double, double, STDDevPopOperation>(
119: 	    SQLType::DOUBLE, SQLType::DOUBLE));
120: 	set.AddFunction(stddev_pop);
121: }
122: 
123: void VarPopFun::RegisterFunction(BuiltinFunctions &set) {
124: 	AggregateFunctionSet var_pop("var_pop");
125: 	var_pop.AddFunction(AggregateFunction::UnaryAggregate<stddev_state_t, double, double, VarPopOperation>(
126: 	    SQLType::DOUBLE, SQLType::DOUBLE));
127: 	set.AddFunction(var_pop);
128: }
129: 
130: void VarSampFun::RegisterFunction(BuiltinFunctions &set) {
131: 	AggregateFunctionSet var_samp("var_samp");
132: 	var_samp.AddFunction(AggregateFunction::UnaryAggregate<stddev_state_t, double, double, VarSampOperation>(
133: 	    SQLType::DOUBLE, SQLType::DOUBLE));
134: 	set.AddFunction(var_samp);
135: }
[end of src/function/aggregate/algebraic/stddev.cpp]
[start of src/function/function.cpp]
1: #include "duckdb/function/function.hpp"
2: #include "duckdb/function/aggregate_function.hpp"
3: #include "duckdb/function/scalar_function.hpp"
4: #include "duckdb/function/cast_rules.hpp"
5: 
6: #include "duckdb/catalog/catalog_entry/scalar_function_catalog_entry.hpp"
7: #include "duckdb/planner/expression/bound_cast_expression.hpp"
8: #include "duckdb/planner/expression/bound_function_expression.hpp"
9: 
10: #include "duckdb/common/string_util.hpp"
11: #include "duckdb/catalog/catalog.hpp"
12: #include "duckdb/parser/parsed_data/create_aggregate_function_info.hpp"
13: #include "duckdb/parser/parsed_data/create_collation_info.hpp"
14: #include "duckdb/parser/parsed_data/create_scalar_function_info.hpp"
15: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
16: #include "duckdb/function/scalar/string_functions.hpp"
17: 
18: using namespace duckdb;
19: using namespace std;
20: 
21: // add your initializer for new functions here
22: void BuiltinFunctions::Initialize() {
23: 	RegisterSQLiteFunctions();
24: 	RegisterReadFunctions();
25: 
26: 	RegisterAlgebraicAggregates();
27: 	RegisterDistributiveAggregates();
28: 	RegisterNestedAggregates();
29: 
30: 	RegisterDateFunctions();
31: 	RegisterMathFunctions();
32: 	RegisterOperators();
33: 	RegisterSequenceFunctions();
34: 	RegisterStringFunctions();
35: 	RegisterNestedFunctions();
36: 	RegisterTrigonometricsFunctions();
37: 
38: 	// initialize collations
39: 	AddCollation("nocase", LowerFun::GetFunction(), true);
40: 	AddCollation("noaccent", StripAccentsFun::GetFunction());
41: }
42: 
43: BuiltinFunctions::BuiltinFunctions(ClientContext &context, Catalog &catalog) : context(context), catalog(catalog) {
44: }
45: 
46: void BuiltinFunctions::AddCollation(string name, ScalarFunction function, bool combinable) {
47: 	CreateCollationInfo info(move(name), move(function), combinable);
48: 	catalog.CreateCollation(context, &info);
49: }
50: 
51: void BuiltinFunctions::AddFunction(AggregateFunctionSet set) {
52: 	CreateAggregateFunctionInfo info(set);
53: 	catalog.CreateFunction(context, &info);
54: }
55: 
56: void BuiltinFunctions::AddFunction(AggregateFunction function) {
57: 	CreateAggregateFunctionInfo info(function);
58: 	catalog.CreateFunction(context, &info);
59: }
60: 
61: void BuiltinFunctions::AddFunction(ScalarFunction function) {
62: 	CreateScalarFunctionInfo info(function);
63: 	catalog.CreateFunction(context, &info);
64: }
65: 
66: void BuiltinFunctions::AddFunction(vector<string> names, ScalarFunction function) {
67: 	for (auto &name : names) {
68: 		function.name = name;
69: 		AddFunction(function);
70: 	}
71: }
72: 
73: void BuiltinFunctions::AddFunction(ScalarFunctionSet set) {
74: 	CreateScalarFunctionInfo info(set);
75: 	catalog.CreateFunction(context, &info);
76: }
77: 
78: void BuiltinFunctions::AddFunction(TableFunction function) {
79: 	CreateTableFunctionInfo info(function);
80: 	catalog.CreateTableFunction(context, &info);
81: }
82: 
83: string Function::CallToString(string name, vector<SQLType> arguments) {
84: 	string result = name + "(";
85: 	result += StringUtil::Join(arguments, arguments.size(), ", ",
86: 	                           [](const SQLType &argument) { return SQLTypeToString(argument); });
87: 	return result + ")";
88: }
89: 
90: string Function::CallToString(string name, vector<SQLType> arguments, SQLType return_type) {
91: 	string result = CallToString(name, arguments);
92: 	result += " -> " + SQLTypeToString(return_type);
93: 	return result;
94: }
95: 
96: static int64_t BindVarArgsFunctionCost(SimpleFunction &func, vector<SQLType> &arguments) {
97: 	if (arguments.size() < func.arguments.size()) {
98: 		// not enough arguments to fulfill the non-vararg part of the function
99: 		return -1;
100: 	}
101: 	int64_t cost = 0;
102: 	for (idx_t i = 0; i < arguments.size(); i++) {
103: 		SQLType arg_type = i < func.arguments.size() ? func.arguments[i] : func.varargs;
104: 		if (arguments[i] == arg_type) {
105: 			// arguments match: do nothing
106: 			continue;
107: 		}
108: 		int64_t cast_cost = CastRules::ImplicitCast(arguments[i], arg_type);
109: 		if (cast_cost >= 0) {
110: 			// we can implicitly cast, add the cost to the total cost
111: 			cost += cast_cost;
112: 		} else {
113: 			// we can't implicitly cast: throw an error
114: 			return -1;
115: 		}
116: 	}
117: 	return cost;
118: }
119: 
120: static int64_t BindFunctionCost(SimpleFunction &func, vector<SQLType> &arguments) {
121: 	if (func.HasVarArgs()) {
122: 		// special case varargs function
123: 		return BindVarArgsFunctionCost(func, arguments);
124: 	}
125: 	if (func.arguments.size() != arguments.size()) {
126: 		// invalid argument count: check the next function
127: 		return -1;
128: 	}
129: 	int64_t cost = 0;
130: 	for (idx_t i = 0; i < arguments.size(); i++) {
131: 		if (arguments[i] == func.arguments[i]) {
132: 			// arguments match: do nothing
133: 			continue;
134: 		}
135: 		int64_t cast_cost = CastRules::ImplicitCast(arguments[i], func.arguments[i]);
136: 		if (cast_cost >= 0) {
137: 			// we can implicitly cast, add the cost to the total cost
138: 			cost += cast_cost;
139: 		} else {
140: 			// we can't implicitly cast: throw an error
141: 			return -1;
142: 		}
143: 	}
144: 	return cost;
145: }
146: 
147: template <class T>
148: static idx_t BindFunctionFromArguments(string name, vector<T> &functions, vector<SQLType> &arguments) {
149: 	idx_t best_function = INVALID_INDEX;
150: 	int64_t lowest_cost = numeric_limits<int64_t>::max();
151: 	vector<idx_t> conflicting_functions;
152: 	for (idx_t f_idx = 0; f_idx < functions.size(); f_idx++) {
153: 		auto &func = functions[f_idx];
154: 		// check the arguments of the function
155: 		int64_t cost = BindFunctionCost(func, arguments);
156: 		if (cost < 0) {
157: 			// auto casting was not possible
158: 			continue;
159: 		}
160: 		if (cost == lowest_cost) {
161: 			conflicting_functions.push_back(f_idx);
162: 			continue;
163: 		}
164: 		if (cost > lowest_cost) {
165: 			continue;
166: 		}
167: 		conflicting_functions.clear();
168: 		lowest_cost = cost;
169: 		best_function = f_idx;
170: 	}
171: 	if (conflicting_functions.size() > 0) {
172: 		// there are multiple possible function definitions
173: 		// throw an exception explaining which overloads are there
174: 		conflicting_functions.push_back(best_function);
175: 		string call_str = Function::CallToString(name, arguments);
176: 		string candidate_str = "";
177: 		for (auto &conf : conflicting_functions) {
178: 			auto &f = functions[conf];
179: 			candidate_str += "\t" + f.ToString() + "\n";
180: 		}
181: 		throw BinderException("Could not choose a best candidate function for the function call \"%s\". In order to "
182: 		                      "select one, please add explicit type casts.\n\tCandidate functions:\n%s",
183: 		                      call_str.c_str(), candidate_str.c_str());
184: 	}
185: 	if (best_function == INVALID_INDEX) {
186: 		// no matching function was found, throw an error
187: 		string call_str = Function::CallToString(name, arguments);
188: 		string candidate_str = "";
189: 		for (auto &f : functions) {
190: 			candidate_str += "\t" + f.ToString() + "\n";
191: 		}
192: 		throw BinderException("No function matches the given name and argument types '%s'. You might need to add "
193: 		                      "explicit type casts.\n\tCandidate functions:\n%s",
194: 		                      call_str.c_str(), candidate_str.c_str());
195: 	}
196: 	return best_function;
197: }
198: 
199: idx_t Function::BindFunction(string name, vector<ScalarFunction> &functions, vector<SQLType> &arguments) {
200: 	return BindFunctionFromArguments(name, functions, arguments);
201: }
202: 
203: idx_t Function::BindFunction(string name, vector<AggregateFunction> &functions, vector<SQLType> &arguments) {
204: 	return BindFunctionFromArguments(name, functions, arguments);
205: }
206: 
207: void SimpleFunction::CastToFunctionArguments(vector<unique_ptr<Expression>> &children, vector<SQLType> &types) {
208: 	for (idx_t i = 0; i < types.size(); i++) {
209: 		auto target_type = i < this->arguments.size() ? this->arguments[i] : this->varargs;
210: 		if (target_type.id != SQLTypeId::ANY && types[i] != target_type) {
211: 			// type of child does not match type of function argument: add a cast
212: 			children[i] = BoundCastExpression::AddCastToType(move(children[i]), types[i], target_type);
213: 		}
214: 	}
215: }
216: 
217: unique_ptr<BoundFunctionExpression> ScalarFunction::BindScalarFunction(ClientContext &context, string schema,
218:                                                                        string name, vector<SQLType> &arguments,
219:                                                                        vector<unique_ptr<Expression>> children,
220:                                                                        bool is_operator) {
221: 	// bind the function
222: 	auto function = Catalog::GetCatalog(context).GetEntry(context, CatalogType::SCALAR_FUNCTION, schema, name);
223: 	assert(function && function->type == CatalogType::SCALAR_FUNCTION);
224: 	return ScalarFunction::BindScalarFunction(context, (ScalarFunctionCatalogEntry &)*function, arguments,
225: 	                                          move(children), is_operator);
226: }
227: 
228: unique_ptr<BoundFunctionExpression>
229: ScalarFunction::BindScalarFunction(ClientContext &context, ScalarFunctionCatalogEntry &func, vector<SQLType> &arguments,
230:                                    vector<unique_ptr<Expression>> children, bool is_operator) {
231: 	// bind the function
232: 	idx_t best_function = Function::BindFunction(func.name, func.functions, arguments);
233: 	// found a matching function!
234: 	auto &bound_function = func.functions[best_function];
235: 	// check if we need to add casts to the children
236: 	bound_function.CastToFunctionArguments(children, arguments);
237: 
238: 	// now create the function
239: 	auto result =
240: 	    make_unique<BoundFunctionExpression>(GetInternalType(bound_function.return_type), bound_function, is_operator);
241: 	result->children = move(children);
242: 	result->arguments = arguments;
243: 	result->sql_return_type = bound_function.return_type;
244: 	if (bound_function.bind) {
245: 		result->bind_info = bound_function.bind(*result, context);
246: 	}
247: 	return result;
248: }
[end of src/function/function.cpp]
[start of src/include/duckdb/catalog/catalog_entry/collate_catalog_entry.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/catalog/catalog_entry/collate_catalog_entry.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/catalog/standard_entry.hpp"
12: #include "duckdb/function/function.hpp"
13: #include "duckdb/parser/parsed_data/create_collation_info.hpp"
14: 
15: namespace duckdb {
16: 
17: //! A collation catalog entry
18: class CollateCatalogEntry : public StandardEntry {
19: public:
20: 	CollateCatalogEntry(Catalog *catalog, SchemaCatalogEntry *schema, CreateCollationInfo *info)
21: 	    : StandardEntry(CatalogType::COLLATION, schema, catalog, info->name), function(info->function),
22: 	      combinable(info->combinable) {
23: 	}
24: 
25: 	ScalarFunction function;
26: 	bool combinable;
27: };
28: } // namespace duckdb
[end of src/include/duckdb/catalog/catalog_entry/collate_catalog_entry.hpp]
[start of src/include/duckdb/function/function.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/function/function.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/data_chunk.hpp"
12: #include "duckdb/common/unordered_set.hpp"
13: #include "duckdb/parser/column_definition.hpp"
14: 
15: namespace duckdb {
16: class CatalogEntry;
17: class Catalog;
18: class ClientContext;
19: class Expression;
20: class ExpressionExecutor;
21: class Transaction;
22: 
23: class AggregateFunction;
24: class AggregateFunctionSet;
25: class ScalarFunctionSet;
26: class ScalarFunction;
27: class TableFunction;
28: 
29: struct FunctionData {
30: 	virtual ~FunctionData() {
31: 	}
32: 
33: 	virtual unique_ptr<FunctionData> Copy() = 0;
34: };
35: 
36: struct TableFunctionData : public FunctionData {
37: 	unique_ptr<FunctionData> Copy() override {
38: 		throw NotImplementedException("Copy not required for table-producing function");
39: 	}
40: };
41: 
42: //! Function is the base class used for any type of function (scalar, aggregate or simple function)
43: class Function {
44: public:
45: 	Function(string name) : name(name) {
46: 	}
47: 	virtual ~Function() {
48: 	}
49: 
50: 	//! The name of the function
51: 	string name;
52: 
53: public:
54: 	//! Returns the formatted string name(arg1, arg2, ...)
55: 	static string CallToString(string name, vector<SQLType> arguments);
56: 	//! Returns the formatted string name(arg1, arg2..) -> return_type
57: 	static string CallToString(string name, vector<SQLType> arguments, SQLType return_type);
58: 
59: 	//! Bind a scalar function from the set of functions and input arguments. Returns the index of the chosen function,
60: 	//! or throws an exception if none could be found.
61: 	static idx_t BindFunction(string name, vector<ScalarFunction> &functions, vector<SQLType> &arguments);
62: 	//! Bind an aggregate function from the set of functions and input arguments. Returns the index of the chosen
63: 	//! function, or throws an exception if none could be found.
64: 	static idx_t BindFunction(string name, vector<AggregateFunction> &functions, vector<SQLType> &arguments);
65: };
66: 
67: class SimpleFunction : public Function {
68: public:
69: 	SimpleFunction(string name, vector<SQLType> arguments, SQLType return_type, bool has_side_effects)
70: 	    : Function(name), arguments(move(arguments)), return_type(return_type), varargs(SQLTypeId::INVALID),
71: 	      has_side_effects(has_side_effects) {
72: 	}
73: 	virtual ~SimpleFunction() {
74: 	}
75: 
76: 	//! The set of arguments of the function
77: 	vector<SQLType> arguments;
78: 	//! Return type of the function
79: 	SQLType return_type;
80: 	//! The type of varargs to support, or SQLTypeId::INVALID if the function does not accept variable length arguments
81: 	SQLType varargs;
82: 	//! Whether or not the function has side effects (e.g. sequence increments, random() functions, NOW()). Functions
83: 	//! with side-effects cannot be constant-folded.
84: 	bool has_side_effects;
85: 
86: public:
87: 	//! Cast a set of expressions to the arguments of this function
88: 	void CastToFunctionArguments(vector<unique_ptr<Expression>> &children, vector<SQLType> &types);
89: 
90: 	string ToString() {
91: 		return Function::CallToString(name, arguments, return_type);
92: 	}
93: 
94: 	bool HasVarArgs() {
95: 		return varargs.id != SQLTypeId::INVALID;
96: 	}
97: };
98: 
99: class BuiltinFunctions {
100: public:
101: 	BuiltinFunctions(ClientContext &transaction, Catalog &catalog);
102: 
103: 	//! Initialize a catalog with all built-in functions
104: 	void Initialize();
105: 
106: public:
107: 	void AddFunction(AggregateFunctionSet set);
108: 	void AddFunction(AggregateFunction function);
109: 	void AddFunction(ScalarFunctionSet set);
110: 	void AddFunction(ScalarFunction function);
111: 	void AddFunction(vector<string> names, ScalarFunction function);
112: 	void AddFunction(TableFunction function);
113: 
114: 	void AddCollation(string name, ScalarFunction function, bool combinable = false);
115: 
116: private:
117: 	ClientContext &context;
118: 	Catalog &catalog;
119: 
120: private:
121: 	template <class T> void Register() {
122: 		T::RegisterFunction(*this);
123: 	}
124: 
125: 	// table-producing functions
126: 	void RegisterSQLiteFunctions();
127: 	void RegisterReadFunctions();
128: 
129: 	// aggregates
130: 	void RegisterAlgebraicAggregates();
131: 	void RegisterDistributiveAggregates();
132: 	void RegisterNestedAggregates();
133: 
134: 	// scalar functions
135: 	void RegisterDateFunctions();
136: 	void RegisterMathFunctions();
137: 	void RegisterOperators();
138: 	void RegisterStringFunctions();
139: 	void RegisterNestedFunctions();
140: 	void RegisterSequenceFunctions();
141: 	void RegisterTrigonometricsFunctions();
142: };
143: 
144: } // namespace duckdb
[end of src/include/duckdb/function/function.hpp]
[start of src/include/duckdb/parser/parsed_data/create_collation_info.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/parser/parsed_data/create_collation_info.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/parser/parsed_data/create_info.hpp"
12: #include "duckdb/function/scalar_function.hpp"
13: 
14: namespace duckdb {
15: 
16: struct CreateCollationInfo : public CreateInfo {
17: 	CreateCollationInfo(string name_p, ScalarFunction function_p, bool combinable_p)
18: 	    : CreateInfo(CatalogType::COLLATION), function(move(function_p)), combinable(combinable_p) {
19: 		this->name = move(name_p);
20: 	}
21: 
22: 	string name;
23: 	ScalarFunction function;
24: 	bool combinable;
25: };
26: 
27: } // namespace duckdb
[end of src/include/duckdb/parser/parsed_data/create_collation_info.hpp]
[start of src/include/duckdb/planner/expression_binder.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/expression_binder.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/exception.hpp"
12: #include "duckdb/parser/parsed_expression.hpp"
13: #include "duckdb/parser/expression/bound_expression.hpp"
14: #include "duckdb/parser/tokens.hpp"
15: #include "duckdb/planner/expression.hpp"
16: 
17: namespace duckdb {
18: 
19: class Binder;
20: class ClientContext;
21: class SelectNode;
22: 
23: class AggregateFunctionCatalogEntry;
24: class ScalarFunctionCatalogEntry;
25: class SimpleFunction;
26: 
27: struct BindResult {
28: 	BindResult(string error) : error(error) {
29: 	}
30: 	BindResult(unique_ptr<Expression> expr, SQLType sql_type) : expression(move(expr)), sql_type(sql_type) {
31: 	}
32: 
33: 	bool HasError() {
34: 		return !error.empty();
35: 	}
36: 
37: 	unique_ptr<Expression> expression;
38: 	SQLType sql_type;
39: 	string error;
40: };
41: 
42: class ExpressionBinder {
43: public:
44: 	ExpressionBinder(Binder &binder, ClientContext &context, bool replace_binder = false);
45: 	virtual ~ExpressionBinder();
46: 
47: 	unique_ptr<Expression> Bind(unique_ptr<ParsedExpression> &expr, SQLType *result_type = nullptr,
48: 	                            bool root_expression = true);
49: 
50: 	//! Returns whether or not any columns have been bound by the expression binder
51: 	bool BoundColumns() {
52: 		return bound_columns;
53: 	}
54: 
55: 	string Bind(unique_ptr<ParsedExpression> *expr, idx_t depth, bool root_expression = false);
56: 
57: 	// Bind table names to ColumnRefExpressions
58: 	static void BindTableNames(Binder &binder, ParsedExpression &expr);
59: 	static unique_ptr<Expression> PushCollation(ClientContext &context, unique_ptr<Expression> source,
60: 	                                            string collation);
61: 
62: 	bool BindCorrelatedColumns(unique_ptr<ParsedExpression> &expr);
63: 
64: 	//! The target type that should result from the binder. If the result is not of this type, a cast to this type will
65: 	//! be added. Defaults to INVALID.
66: 	SQLType target_type;
67: 
68: protected:
69: 	virtual BindResult BindExpression(ParsedExpression &expr, idx_t depth, bool root_expression = false);
70: 
71: 	BindResult BindExpression(CaseExpression &expr, idx_t depth);
72: 	BindResult BindExpression(CollateExpression &expr, idx_t depth);
73: 	BindResult BindExpression(CastExpression &expr, idx_t depth);
74: 	BindResult BindExpression(ColumnRefExpression &expr, idx_t depth);
75: 	BindResult BindExpression(ComparisonExpression &expr, idx_t depth);
76: 	BindResult BindExpression(ConjunctionExpression &expr, idx_t depth);
77: 	BindResult BindExpression(ConstantExpression &expr, idx_t depth);
78: 	BindResult BindExpression(FunctionExpression &expr, idx_t depth);
79: 	BindResult BindExpression(OperatorExpression &expr, idx_t depth);
80: 	BindResult BindExpression(ParameterExpression &expr, idx_t depth);
81: 	BindResult BindExpression(StarExpression &expr, idx_t depth);
82: 	BindResult BindExpression(SubqueryExpression &expr, idx_t depth);
83: 
84: 	void BindChild(unique_ptr<ParsedExpression> &expr, idx_t depth, string &error);
85: 
86: protected:
87: 	static void ExtractCorrelatedExpressions(Binder &binder, Expression &expr);
88: 
89: 	virtual BindResult BindFunction(FunctionExpression &expr, ScalarFunctionCatalogEntry *function, idx_t depth);
90: 	virtual BindResult BindAggregate(FunctionExpression &expr, AggregateFunctionCatalogEntry *function, idx_t depth);
91: 	virtual BindResult BindUnnest(FunctionExpression &expr, idx_t depth);
92: 
93: 	virtual string UnsupportedAggregateMessage();
94: 	virtual string UnsupportedUnnestMessage();
95: 
96: 	Binder &binder;
97: 	ClientContext &context;
98: 	ExpressionBinder *stored_binder;
99: 	bool bound_columns = false;
100: };
101: 
102: } // namespace duckdb
[end of src/include/duckdb/planner/expression_binder.hpp]
[start of src/planner/binder/expression/bind_comparison_expression.cpp]
1: #include "duckdb/parser/expression/comparison_expression.hpp"
2: #include "duckdb/planner/expression/bound_cast_expression.hpp"
3: #include "duckdb/planner/expression/bound_comparison_expression.hpp"
4: #include "duckdb/planner/expression/bound_function_expression.hpp"
5: #include "duckdb/planner/expression_binder.hpp"
6: #include "duckdb/catalog/catalog_entry/collate_catalog_entry.hpp"
7: 
8: #include "duckdb/function/scalar/string_functions.hpp"
9: 
10: #include "duckdb/main/client_context.hpp"
11: #include "duckdb/main/database.hpp"
12: 
13: using namespace duckdb;
14: using namespace std;
15: 
16: unique_ptr<Expression> ExpressionBinder::PushCollation(ClientContext &context, unique_ptr<Expression> source,
17:                                                        string collation) {
18: 	// replace default collation with system collation
19: 	if (collation.empty()) {
20: 		collation = context.db.collation;
21: 	}
22: 	// bind the collation
23: 	if (collation.empty() || collation == "binary" || collation == "c" || collation == "posix") {
24: 		// binary collation: just skip
25: 		return move(source);
26: 	}
27: 	auto &catalog = Catalog::GetCatalog(context);
28: 	auto splits = StringUtil::Split(StringUtil::Lower(collation), ".");
29: 	vector<CollateCatalogEntry *> entries;
30: 	for (auto &collation_argument : splits) {
31: 		auto collation_entry = catalog.GetEntry<CollateCatalogEntry>(context, DEFAULT_SCHEMA, collation_argument);
32: 		if (collation_entry->combinable) {
33: 			entries.insert(entries.begin(), collation_entry);
34: 		} else {
35: 			if (entries.size() > 0 && !entries.back()->combinable) {
36: 				throw BinderException("Cannot combine collation types \"%s\" and \"%s\"", entries.back()->name.c_str(),
37: 				                      collation_entry->name.c_str());
38: 			}
39: 			entries.push_back(collation_entry);
40: 		}
41: 	}
42: 	for (auto &collation_entry : entries) {
43: 		auto function = make_unique<BoundFunctionExpression>(TypeId::VARCHAR, collation_entry->function);
44: 		function->children.push_back(move(source));
45: 		function->arguments.push_back({SQLType::VARCHAR});
46: 		function->sql_return_type = SQLType::VARCHAR;
47: 		if (collation_entry->function.bind) {
48: 			function->bind_info = collation_entry->function.bind(*function, context);
49: 		}
50: 		source = move(function);
51: 	}
52: 	return source;
53: }
54: 
55: BindResult ExpressionBinder::BindExpression(ComparisonExpression &expr, idx_t depth) {
56: 	// first try to bind the children of the case expression
57: 	string error;
58: 	BindChild(expr.left, depth, error);
59: 	BindChild(expr.right, depth, error);
60: 	if (!error.empty()) {
61: 		return BindResult(error);
62: 	}
63: 	// the children have been successfully resolved
64: 	auto &left = (BoundExpression &)*expr.left;
65: 	auto &right = (BoundExpression &)*expr.right;
66: 	// cast the input types to the same type
67: 	// now obtain the result type of the input types
68: 	auto input_type = MaxSQLType(left.sql_type, right.sql_type);
69: 	if (input_type.id == SQLTypeId::VARCHAR) {
70: 		// for comparison with strings, we prefer to bind to the numeric types
71: 		if (left.sql_type.IsNumeric()) {
72: 			input_type = left.sql_type;
73: 		} else if (right.sql_type.IsNumeric()) {
74: 			input_type = right.sql_type;
75: 		} else {
76: 			// else: check if collations are compatible
77: 			if (!left.sql_type.collation.empty() && !right.sql_type.collation.empty() &&
78: 			    left.sql_type.collation != right.sql_type.collation) {
79: 				throw BinderException("Cannot combine types with different collation!");
80: 			}
81: 		}
82: 	}
83: 	if (input_type.id == SQLTypeId::UNKNOWN) {
84: 		throw BinderException("Could not determine type of parameters: try adding explicit type casts");
85: 	}
86: 	// add casts (if necessary)
87: 	left.expr = BoundCastExpression::AddCastToType(move(left.expr), left.sql_type, input_type);
88: 	right.expr = BoundCastExpression::AddCastToType(move(right.expr), right.sql_type, input_type);
89: 	if (input_type.id == SQLTypeId::VARCHAR) {
90: 		// handle collation
91: 		left.expr = PushCollation(context, move(left.expr), input_type.collation);
92: 		right.expr = PushCollation(context, move(right.expr), input_type.collation);
93: 	}
94: 	// now create the bound comparison expression
95: 	return BindResult(make_unique<BoundComparisonExpression>(expr.type, move(left.expr), move(right.expr)),
96: 	                  SQLType(SQLTypeId::BOOLEAN));
97: }
[end of src/planner/binder/expression/bind_comparison_expression.cpp]
[start of src/planner/binder/query_node/bind_select_node.cpp]
1: #include "duckdb/parser/expression/columnref_expression.hpp"
2: #include "duckdb/parser/expression/constant_expression.hpp"
3: #include "duckdb/parser/query_node/select_node.hpp"
4: #include "duckdb/parser/tableref/joinref.hpp"
5: #include "duckdb/planner/binder.hpp"
6: #include "duckdb/execution/expression_executor.hpp"
7: #include "duckdb/planner/expression_binder/constant_binder.hpp"
8: #include "duckdb/planner/expression_binder/group_binder.hpp"
9: #include "duckdb/planner/expression_binder/having_binder.hpp"
10: #include "duckdb/planner/expression_binder/order_binder.hpp"
11: #include "duckdb/planner/expression_binder/select_binder.hpp"
12: #include "duckdb/planner/expression_binder/where_binder.hpp"
13: #include "duckdb/planner/query_node/bound_select_node.hpp"
14: #include "duckdb/parser/expression/table_star_expression.hpp"
15: 
16: using namespace std;
17: 
18: namespace duckdb {
19: 
20: static int64_t BindConstant(Binder &binder, ClientContext &context, string clause, unique_ptr<ParsedExpression> &expr) {
21: 	ConstantBinder constant_binder(binder, context, clause);
22: 	auto bound_expr = constant_binder.Bind(expr);
23: 	Value value = ExpressionExecutor::EvaluateScalar(*bound_expr);
24: 	if (!TypeIsNumeric(value.type)) {
25: 		throw BinderException("LIMIT clause can only contain numeric constants!");
26: 	}
27: 	int64_t limit_value = value.GetValue<int64_t>();
28: 	if (limit_value < 0) {
29: 		throw BinderException("LIMIT must not be negative");
30: 	}
31: 	return limit_value;
32: }
33: 
34: unique_ptr<Expression> Binder::BindFilter(unique_ptr<ParsedExpression> condition) {
35: 	WhereBinder where_binder(*this, context);
36: 	return where_binder.Bind(condition);
37: }
38: 
39: unique_ptr<Expression> Binder::BindOrderExpression(OrderBinder &order_binder, unique_ptr<ParsedExpression> expr) {
40: 	// we treat the Distinct list as a order by
41: 	auto bound_expr = order_binder.Bind(move(expr));
42: 	if (!bound_expr) {
43: 		// DISTINCT ON non-integer constant
44: 		// remove the expression from the DISTINCT ON list
45: 		return nullptr;
46: 	}
47: 	assert(bound_expr->type == ExpressionType::BOUND_COLUMN_REF);
48: 	return bound_expr;
49: }
50: 
51: unique_ptr<BoundResultModifier> Binder::BindLimit(LimitModifier &limit_mod) {
52: 	auto result = make_unique<BoundLimitModifier>();
53: 	if (limit_mod.limit) {
54: 		result->limit = BindConstant(*this, context, "LIMIT clause", limit_mod.limit);
55: 		result->offset = 0;
56: 	}
57: 	if (limit_mod.offset) {
58: 		result->offset = BindConstant(*this, context, "OFFSET clause", limit_mod.offset);
59: 		if (!limit_mod.limit) {
60: 			result->limit = std::numeric_limits<int64_t>::max();
61: 		}
62: 	}
63: 	return move(result);
64: }
65: 
66: void Binder::BindModifiers(OrderBinder &order_binder, QueryNode &statement, BoundQueryNode &result) {
67: 	for (auto &mod : statement.modifiers) {
68: 		unique_ptr<BoundResultModifier> bound_modifier;
69: 		switch (mod->type) {
70: 		case ResultModifierType::DISTINCT_MODIFIER: {
71: 			auto &distinct = (DistinctModifier &)*mod;
72: 			auto bound_distinct = make_unique<BoundDistinctModifier>();
73: 			for (idx_t i = 0; i < distinct.distinct_on_targets.size(); i++) {
74: 				auto expr = BindOrderExpression(order_binder, move(distinct.distinct_on_targets[i]));
75: 				if (!expr) {
76: 					continue;
77: 				}
78: 				bound_distinct->target_distincts.push_back(move(expr));
79: 			}
80: 			bound_modifier = move(bound_distinct);
81: 			break;
82: 		}
83: 		case ResultModifierType::ORDER_MODIFIER: {
84: 			auto &order = (OrderModifier &)*mod;
85: 			auto bound_order = make_unique<BoundOrderModifier>();
86: 			for (idx_t i = 0; i < order.orders.size(); i++) {
87: 				BoundOrderByNode node;
88: 				node.type = order.orders[i].type;
89: 				node.expression = BindOrderExpression(order_binder, move(order.orders[i].expression));
90: 				if (!node.expression) {
91: 					continue;
92: 				}
93: 				bound_order->orders.push_back(move(node));
94: 			}
95: 			if (bound_order->orders.size() > 0) {
96: 				bound_modifier = move(bound_order);
97: 			}
98: 			break;
99: 		}
100: 		case ResultModifierType::LIMIT_MODIFIER:
101: 			bound_modifier = BindLimit((LimitModifier &)*mod);
102: 			break;
103: 		default:
104: 			throw Exception("Unsupported result modifier");
105: 		}
106: 		if (bound_modifier) {
107: 			result.modifiers.push_back(move(bound_modifier));
108: 		}
109: 	}
110: }
111: 
112: void Binder::BindModifierTypes(BoundQueryNode &result, const vector<SQLType> &sql_types, idx_t projection_index) {
113: 	for (auto &bound_mod : result.modifiers) {
114: 		switch (bound_mod->type) {
115: 		case ResultModifierType::DISTINCT_MODIFIER: {
116: 			auto &distinct = (BoundDistinctModifier &)*bound_mod;
117: 			if (distinct.target_distincts.size() == 0) {
118: 				// DISTINCT without a target: push references to the standard select list
119: 				for (idx_t i = 0; i < sql_types.size(); i++) {
120: 					distinct.target_distincts.push_back(
121: 					    make_unique<BoundColumnRefExpression>(GetInternalType(sql_types[i]), ColumnBinding(projection_index, i)));
122: 				}
123: 			} else {
124: 				// DISTINCT with target list: set types
125: 				for (idx_t i = 0; i < distinct.target_distincts.size(); i++) {
126: 					auto &expr = distinct.target_distincts[i];
127: 					assert(expr->type == ExpressionType::BOUND_COLUMN_REF);
128: 					auto &bound_colref = (BoundColumnRefExpression &)*expr;
129: 					if (bound_colref.binding.column_index == INVALID_INDEX) {
130: 						throw BinderException("Ambiguous name in DISTINCT ON!");
131: 					}
132: 					assert(bound_colref.binding.column_index < sql_types.size());
133: 					bound_colref.return_type = GetInternalType(sql_types[bound_colref.binding.column_index]);
134: 				}
135: 			}
136: 			for(idx_t i = 0; i < distinct.target_distincts.size(); i++) {
137: 				auto &bound_colref = (BoundColumnRefExpression &)*distinct.target_distincts[i];
138: 				auto sql_type = sql_types[bound_colref.binding.column_index];
139: 				if (sql_type.id == SQLTypeId::VARCHAR) {
140: 					distinct.target_distincts[i] = ExpressionBinder::PushCollation(context, move(distinct.target_distincts[i]), sql_type.collation);
141: 				}
142: 			}
143: 			break;
144: 		}
145: 		case ResultModifierType::ORDER_MODIFIER: {
146: 			auto &order = (BoundOrderModifier &)*bound_mod;
147: 			for (idx_t i = 0; i < order.orders.size(); i++) {
148: 				auto &expr = order.orders[i].expression;
149: 				assert(expr->type == ExpressionType::BOUND_COLUMN_REF);
150: 				auto &bound_colref = (BoundColumnRefExpression &)*expr;
151: 				if (bound_colref.binding.column_index == INVALID_INDEX) {
152: 					throw BinderException("Ambiguous name in ORDER BY!");
153: 				}
154: 				assert(bound_colref.binding.column_index < sql_types.size());
155: 				auto sql_type = sql_types[bound_colref.binding.column_index];
156: 				bound_colref.return_type = GetInternalType(sql_types[bound_colref.binding.column_index]);
157: 				if (sql_type.id == SQLTypeId::VARCHAR) {
158: 					order.orders[i].expression = ExpressionBinder::PushCollation(context, move(order.orders[i].expression), sql_type.collation);
159: 				}
160: 			}
161: 			break;
162: 		}
163: 		default:
164: 			break;
165: 		}
166: 	}
167: }
168: 
169: unique_ptr<BoundQueryNode> Binder::BindNode(SelectNode &statement) {
170: 	auto result = make_unique<BoundSelectNode>();
171: 	result->projection_index = GenerateTableIndex();
172: 	result->group_index = GenerateTableIndex();
173: 	result->aggregate_index = GenerateTableIndex();
174: 	result->window_index = GenerateTableIndex();
175: 	result->unnest_index = GenerateTableIndex();
176: 	result->prune_index = GenerateTableIndex();
177: 
178: 	// first bind the FROM table statement
179: 	result->from_table = Bind(*statement.from_table);
180: 
181: 	// visit the select list and expand any "*" statements
182: 	vector<unique_ptr<ParsedExpression>> new_select_list;
183: 	for (auto &select_element : statement.select_list) {
184: 		if (select_element->GetExpressionType() == ExpressionType::STAR) {
185: 			// * statement, expand to all columns from the FROM clause
186: 			bind_context.GenerateAllColumnExpressions(new_select_list);
187: 		} else if (select_element->GetExpressionType() == ExpressionType::TABLE_STAR) {
188: 			auto table_star =
189: 			    (TableStarExpression *)select_element.get(); // TODO this cast to explicit class is a bit dirty?
190: 			bind_context.GenerateAllColumnExpressions(new_select_list, table_star->relation_name);
191: 		} else {
192: 			// regular statement, add it to the list
193: 			new_select_list.push_back(move(select_element));
194: 		}
195: 	}
196: 	statement.select_list = move(new_select_list);
197: 
198: 	// create a mapping of (alias -> index) and a mapping of (Expression -> index) for the SELECT list
199: 	unordered_map<string, idx_t> alias_map;
200: 	expression_map_t<idx_t> projection_map;
201: 	for (idx_t i = 0; i < statement.select_list.size(); i++) {
202: 		auto &expr = statement.select_list[i];
203: 		result->names.push_back(expr->GetName());
204: 		if (!expr->alias.empty()) {
205: 			alias_map[expr->alias] = i;
206: 		}
207: 		ExpressionBinder::BindTableNames(*this, *expr);
208: 		projection_map[expr.get()] = i;
209: 		result->original_expressions.push_back(expr->Copy());
210: 	}
211: 	result->column_count = statement.select_list.size();
212: 
213: 	// first visit the WHERE clause
214: 	// the WHERE clause happens before the GROUP BY, PROJECTION or HAVING clauses
215: 	if (statement.where_clause) {
216: 		result->where_clause = BindFilter(move(statement.where_clause));
217: 	}
218: 
219: 	// now bind all the result modifiers; including DISTINCT and ORDER BY targets
220: 	OrderBinder order_binder({this}, result->projection_index, statement, alias_map, projection_map);
221: 	BindModifiers(order_binder, statement, *result);
222: 
223: 	vector<unique_ptr<ParsedExpression>> unbound_groups;
224: 	BoundGroupInformation info;
225: 	if (statement.groups.size() > 0) {
226: 		// the statement has a GROUP BY clause, bind it
227: 		unbound_groups.resize(statement.groups.size());
228: 		GroupBinder group_binder(*this, context, statement, result->group_index, alias_map, info.alias_map);
229: 		for (idx_t i = 0; i < statement.groups.size(); i++) {
230: 
231: 			// we keep a copy of the unbound expression;
232: 			// we keep the unbound copy around to check for group references in the SELECT and HAVING clause
233: 			// the reason we want the unbound copy is because we want to figure out whether an expression
234: 			// is a group reference BEFORE binding in the SELECT/HAVING binder
235: 			group_binder.unbound_expression = statement.groups[i]->Copy();
236: 			group_binder.bind_index = i;
237: 
238: 			// bind the groups
239: 			SQLType group_type;
240: 			auto bound_expr = group_binder.Bind(statement.groups[i], &group_type);
241: 			assert(bound_expr->return_type != TypeId::INVALID);
242: 			info.group_types.push_back(group_type);
243: 			result->groups.push_back(move(bound_expr));
244: 
245: 			// in the unbound expression we DO bind the table names of any ColumnRefs
246: 			// we do this to make sure that "table.a" and "a" are treated the same
247: 			// if we wouldn't do this then (SELECT test.a FROM test GROUP BY a) would not work because "test.a" <> "a"
248: 			// hence we convert "a" -> "test.a" in the unbound expression
249: 			unbound_groups[i] = move(group_binder.unbound_expression);
250: 			ExpressionBinder::BindTableNames(*this, *unbound_groups[i]);
251: 			info.map[unbound_groups[i].get()] = i;
252: 		}
253: 	}
254: 
255: 	// bind the HAVING clause, if any
256: 	if (statement.having) {
257: 		HavingBinder having_binder(*this, context, *result, info);
258: 		ExpressionBinder::BindTableNames(*this, *statement.having);
259: 		result->having = having_binder.Bind(statement.having);
260: 	}
261: 
262: 	// after that, we bind to the SELECT list
263: 	SelectBinder select_binder(*this, context, *result, info);
264: 	vector<SQLType> internal_sql_types;
265: 	for (idx_t i = 0; i < statement.select_list.size(); i++) {
266: 		SQLType result_type;
267: 		auto expr = select_binder.Bind(statement.select_list[i], &result_type);
268: 		if (statement.aggregate_handling == AggregateHandling::FORCE_AGGREGATES && select_binder.BoundColumns()) {
269: 			if (select_binder.BoundAggregates()) {
270: 				throw BinderException("Cannot mix aggregates with non-aggregated columns!");
271: 			}
272: 			// we are forcing aggregates, and the node has columns bound
273: 			// this entry becomes a group
274: 			auto group_type = expr->return_type;
275: 			auto group_ref = make_unique<BoundColumnRefExpression>(
276: 			    group_type, ColumnBinding(result->group_index, result->groups.size()));
277: 			result->groups.push_back(move(expr));
278: 			expr = move(group_ref);
279: 		}
280: 		result->select_list.push_back(move(expr));
281: 		if (i < result->column_count) {
282: 			result->types.push_back(result_type);
283: 		}
284: 		internal_sql_types.push_back(result_type);
285: 		if (statement.aggregate_handling == AggregateHandling::FORCE_AGGREGATES) {
286: 			select_binder.ResetBindings();
287: 		}
288: 	}
289: 	result->need_prune = result->select_list.size() > result->column_count;
290: 
291: 	// in the normal select binder, we bind columns as if there is no aggregation
292: 	// i.e. in the query [SELECT i, SUM(i) FROM integers;] the "i" will be bound as a normal column
293: 	// since we have an aggregation, we need to either (1) throw an error, or (2) wrap the column in a FIRST() aggregate
294: 	// we choose the former one [CONTROVERSIAL: this is the PostgreSQL behavior]
295: 	if (result->groups.size() > 0 || result->aggregates.size() > 0 || statement.having) {
296: 		if (statement.aggregate_handling == AggregateHandling::NO_AGGREGATES_ALLOWED) {
297: 			throw BinderException("Aggregates cannot be present in a Project relation!");
298: 		} else if (statement.aggregate_handling == AggregateHandling::STANDARD_HANDLING) {
299: 			if (select_binder.BoundColumns()) {
300: 				throw BinderException("column must appear in the GROUP BY clause or be used in an aggregate function");
301: 			}
302: 		}
303: 	}
304: 
305: 	// now that the SELECT list is bound, we set the types of DISTINCT/ORDER BY expressions
306: 	BindModifierTypes(*result, internal_sql_types, result->projection_index);
307: 	return move(result);
308: }
309: 
310: } // namespace duckdb
[end of src/planner/binder/query_node/bind_select_node.cpp]
[start of src/storage/string_segment.cpp]
1: #include "duckdb/storage/string_segment.hpp"
2: #include "duckdb/storage/buffer_manager.hpp"
3: #include "duckdb/storage/numeric_segment.hpp"
4: #include "duckdb/transaction/update_info.hpp"
5: #include "duckdb/common/vector_operations/vector_operations.hpp"
6: #include "duckdb/storage/data_table.hpp"
7: #include "duckdb/common/operator/comparison_operators.hpp"
8: 
9: using namespace duckdb;
10: using namespace std;
11: 
12: StringSegment::StringSegment(BufferManager &manager, idx_t row_start, block_id_t block)
13:     : UncompressedSegment(manager, TypeId::VARCHAR, row_start) {
14: 	this->max_vector_count = 0;
15: 	this->dictionary_offset = 0;
16: 	// the vector_size is given in the size of the dictionary offsets
17: 	this->vector_size = STANDARD_VECTOR_SIZE * sizeof(int32_t) + sizeof(nullmask_t);
18: 	this->string_updates = nullptr;
19: 
20: 	this->block_id = block;
21: 	if (block_id == INVALID_BLOCK) {
22: 		// start off with an empty string segment: allocate space for it
23: 		auto handle = manager.Allocate(Storage::BLOCK_ALLOC_SIZE);
24: 		this->block_id = handle->block_id;
25: 
26: 		ExpandStringSegment(handle->node->buffer);
27: 	}
28: }
29: 
30: StringSegment::~StringSegment() {
31: 	while (head) {
32: 		manager.DestroyBuffer(head->block_id);
33: 		head = move(head->next);
34: 	}
35: }
36: 
37: void StringSegment::ExpandStringSegment(data_ptr_t baseptr) {
38: 	// clear the nullmask for this vector
39: 	auto mask = (nullmask_t *)(baseptr + (max_vector_count * vector_size));
40: 	mask->reset();
41: 
42: 	max_vector_count++;
43: 	if (versions) {
44: 		auto new_versions = unique_ptr<UpdateInfo *[]>(new UpdateInfo *[max_vector_count]);
45: 		memcpy(new_versions.get(), versions.get(), (max_vector_count - 1) * sizeof(UpdateInfo *));
46: 		new_versions[max_vector_count - 1] = nullptr;
47: 		versions = move(new_versions);
48: 	}
49: 
50: 	if (string_updates) {
51: 		auto new_string_updates = unique_ptr<string_update_info_t[]>(new string_update_info_t[max_vector_count]);
52: 		for (idx_t i = 0; i < max_vector_count - 1; i++) {
53: 			new_string_updates[i] = move(string_updates[i]);
54: 		}
55: 		new_string_updates[max_vector_count - 1] = 0;
56: 		string_updates = move(new_string_updates);
57: 	}
58: }
59: 
60: //===--------------------------------------------------------------------===//
61: // Scan
62: //===--------------------------------------------------------------------===//
63: void StringSegment::InitializeScan(ColumnScanState &state) {
64: 	// pin the primary buffer
65: 	state.primary_handle = manager.Pin(block_id);
66: }
67: 
68: //===--------------------------------------------------------------------===//
69: // Filter base data
70: //===--------------------------------------------------------------------===//
71: void StringSegment::read_string(string_t *result_data, buffer_handle_set_t &handles, data_ptr_t baseptr,
72:                                 int32_t *dict_offset, idx_t src_idx, idx_t res_idx, idx_t &update_idx,
73:                                 size_t vector_index) {
74: 	if (string_updates && string_updates[vector_index]) {
75: 		auto &info = *string_updates[vector_index];
76: 		while (info.ids[update_idx] < src_idx) {
77: 			//! We need to catch the update_idx up to the src_idx
78: 			update_idx++;
79: 		}
80: 		if (update_idx < info.count && info.ids[update_idx] == src_idx) {
81: 			result_data[res_idx] = ReadString(handles, info.block_ids[update_idx], info.offsets[update_idx]);
82: 		} else {
83: 			result_data[res_idx] = FetchStringFromDict(handles, baseptr, dict_offset[src_idx]);
84: 		}
85: 	} else {
86: 		result_data[res_idx] = FetchStringFromDict(handles, baseptr, dict_offset[src_idx]);
87: 	}
88: }
89: 
90: void StringSegment::Select(ColumnScanState &state, Vector &result, SelectionVector &sel, idx_t &approved_tuple_count,
91:                            vector<TableFilter> &tableFilter) {
92: 	auto vector_index = state.vector_index;
93: 	assert(vector_index < max_vector_count);
94: 	assert(vector_index * STANDARD_VECTOR_SIZE <= tuple_count);
95: 
96: 	auto handle = state.primary_handle.get();
97: 	state.handles.clear();
98: 	auto baseptr = handle->node->buffer;
99: 	// fetch the data from the base segment
100: 	auto base = baseptr + state.vector_index * vector_size;
101: 	auto base_data = (int32_t *)(base + sizeof(nullmask_t));
102: 	auto base_nullmask = (nullmask_t *)base;
103: 
104: 	if (tableFilter.size() == 1) {
105: 		switch (tableFilter[0].comparison_type) {
106: 		case ExpressionType::COMPARE_EQUAL: {
107: 			Select_String<Equals>(state.handles, result, baseptr, base_data, sel, tableFilter[0].constant.str_value,
108: 			                      approved_tuple_count, base_nullmask, vector_index);
109: 			break;
110: 		}
111: 		case ExpressionType::COMPARE_LESSTHAN: {
112: 			Select_String<LessThan>(state.handles, result, baseptr, base_data, sel, tableFilter[0].constant.str_value,
113: 			                        approved_tuple_count, base_nullmask, vector_index);
114: 			break;
115: 		}
116: 		case ExpressionType::COMPARE_GREATERTHAN: {
117: 			Select_String<GreaterThan>(state.handles, result, baseptr, base_data, sel,
118: 			                           tableFilter[0].constant.str_value, approved_tuple_count, base_nullmask,
119: 			                           vector_index);
120: 			break;
121: 		}
122: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO: {
123: 			Select_String<LessThanEquals>(state.handles, result, baseptr, base_data, sel,
124: 			                              tableFilter[0].constant.str_value, approved_tuple_count, base_nullmask,
125: 			                              vector_index);
126: 			break;
127: 		}
128: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO: {
129: 			Select_String<GreaterThanEquals>(state.handles, result, baseptr, base_data, sel,
130: 			                                 tableFilter[0].constant.str_value, approved_tuple_count, base_nullmask,
131: 			                                 vector_index);
132: 
133: 			break;
134: 		}
135: 		default:
136: 			throw NotImplementedException("Unknown comparison type for filter pushed down to table!");
137: 		}
138: 	} else {
139: 	    bool isFirstGreater = tableFilter[0].comparison_type == ExpressionType::COMPARE_GREATERTHAN ||
140: 		       tableFilter[0].comparison_type == ExpressionType::COMPARE_GREATERTHANOREQUALTO;
141:         auto less = isFirstGreater?tableFilter[1]:tableFilter[0];
142:         auto greater = isFirstGreater?tableFilter[0]:tableFilter[1];
143: 		if (greater.comparison_type == ExpressionType::COMPARE_GREATERTHAN) {
144: 			if (less.comparison_type == ExpressionType::COMPARE_LESSTHAN) {
145: 				Select_String_Between<GreaterThan, LessThan>(
146: 				    state.handles, result, baseptr, base_data, sel, greater.constant.str_value,
147: 				    less.constant.str_value, approved_tuple_count, base_nullmask, vector_index);
148: 			} else {
149: 				Select_String_Between<GreaterThan, LessThanEquals>(
150: 				    state.handles, result, baseptr, base_data, sel, greater.constant.str_value,
151: 				    less.constant.str_value, approved_tuple_count, base_nullmask, vector_index);
152: 			}
153: 		} else {
154: 			if (less.comparison_type == ExpressionType::COMPARE_LESSTHAN) {
155: 				Select_String_Between<GreaterThanEquals, LessThan>(
156: 				    state.handles, result, baseptr, base_data, sel, greater.constant.str_value,
157: 				    less.constant.str_value, approved_tuple_count, base_nullmask, vector_index);
158: 			} else {
159: 				Select_String_Between<GreaterThanEquals, LessThanEquals>(
160: 				    state.handles, result, baseptr, base_data, sel, greater.constant.str_value,
161: 				    less.constant.str_value, approved_tuple_count, base_nullmask, vector_index);
162: 			}
163: 		}
164: 	}
165: }
166: 
167: //===--------------------------------------------------------------------===//
168: // Fetch base data
169: //===--------------------------------------------------------------------===//
170: void StringSegment::FetchBaseData(ColumnScanState &state, idx_t vector_index, Vector &result) {
171: 	// clear any previously locked buffers and get the primary buffer handle
172: 	auto handle = state.primary_handle.get();
173: 	state.handles.clear();
174: 
175: 	// fetch the data from the base segment
176: 	FetchBaseData(state, handle->node->buffer, vector_index, result, GetVectorCount(vector_index));
177: }
178: 
179: void StringSegment::FetchBaseData(ColumnScanState &state, data_ptr_t baseptr, idx_t vector_index, Vector &result,
180:                                   idx_t count) {
181: 	auto base = baseptr + vector_index * vector_size;
182: 
183: 	auto &base_nullmask = *((nullmask_t *)base);
184: 	auto base_data = (int32_t *)(base + sizeof(nullmask_t));
185: 	auto result_data = FlatVector::GetData<string_t>(result);
186: 
187: 	if (string_updates && string_updates[vector_index]) {
188: 		// there are updates: merge them in
189: 		auto &info = *string_updates[vector_index];
190: 		idx_t update_idx = 0;
191: 		for (idx_t i = 0; i < count; i++) {
192: 			if (update_idx < info.count && info.ids[update_idx] == i) {
193: 				// use update info
194: 				result_data[i] = ReadString(state.handles, info.block_ids[update_idx], info.offsets[update_idx]);
195: 				update_idx++;
196: 			} else {
197: 				// use base table info
198: 				result_data[i] = FetchStringFromDict(state.handles, baseptr, base_data[i]);
199: 			}
200: 		}
201: 	} else {
202: 		// no updates: fetch only from the string dictionary
203: 		for (idx_t i = 0; i < count; i++) {
204: 			result_data[i] = FetchStringFromDict(state.handles, baseptr, base_data[i]);
205: 		}
206: 	}
207: 	FlatVector::SetNullmask(result, base_nullmask);
208: }
209: 
210: void StringSegment::FilterFetchBaseData(ColumnScanState &state, Vector &result, SelectionVector &sel,
211:                                         idx_t &approved_tuple_count) {
212: 	// clear any previously locked buffers and get the primary buffer handle
213: 	auto handle = state.primary_handle.get();
214: 	state.handles.clear();
215: 	auto baseptr = handle->node->buffer;
216: 	// fetch the data from the base segment
217: 	auto base = baseptr + state.vector_index * vector_size;
218: 	auto &base_nullmask = *((nullmask_t *)base);
219: 	auto base_data = (int32_t *)(base + sizeof(nullmask_t));
220: 	result.vector_type = VectorType::FLAT_VECTOR;
221: 	auto result_data = FlatVector::GetData<string_t>(result);
222: 	nullmask_t result_nullmask;
223: 	idx_t update_idx = 0;
224: 	if (base_nullmask.any()) {
225: 		for (idx_t i = 0; i < approved_tuple_count; i++) {
226: 			idx_t src_idx = sel.get_index(i);
227: 			if (base_nullmask[src_idx]) {
228: 				result_nullmask.set(i, true);
229: 				read_string(result_data, state.handles, baseptr, base_data, src_idx, i, update_idx, state.vector_index);
230: 			} else {
231: 				read_string(result_data, state.handles, baseptr, base_data, src_idx, i, update_idx, state.vector_index);
232: 			}
233: 		}
234: 	} else {
235: 		for (idx_t i = 0; i < approved_tuple_count; i++) {
236: 			idx_t src_idx = sel.get_index(i);
237: 			read_string(result_data, state.handles, baseptr, base_data, src_idx, i, update_idx, state.vector_index);
238: 		}
239: 	}
240: 	FlatVector::SetNullmask(result, result_nullmask);
241: }
242: 
243: //===--------------------------------------------------------------------===//
244: // Fetch update data
245: //===--------------------------------------------------------------------===//
246: void StringSegment::FetchUpdateData(ColumnScanState &state, Transaction &transaction, UpdateInfo *info,
247:                                     Vector &result) {
248: 	// fetch data from updates
249: 	auto handle = state.primary_handle.get();
250: 
251: 	auto result_data = FlatVector::GetData<string_t>(result);
252: 	auto &result_mask = FlatVector::Nullmask(result);
253: 	UpdateInfo::UpdatesForTransaction(info, transaction, [&](UpdateInfo *current) {
254: 		auto info_data = (string_location_t *)current->tuple_data;
255: 		for (idx_t i = 0; i < current->N; i++) {
256: 			auto string = FetchString(state.handles, handle->node->buffer, info_data[i]);
257: 			result_data[current->tuples[i]] = string;
258: 			result_mask[current->tuples[i]] = current->nullmask[current->tuples[i]];
259: 		}
260: 	});
261: }
262: 
263: //===--------------------------------------------------------------------===//
264: // Fetch strings
265: //===--------------------------------------------------------------------===//
266: void StringSegment::FetchStringLocations(data_ptr_t baseptr, row_t *ids, idx_t vector_index, idx_t vector_offset,
267:                                          idx_t count, string_location_t result[]) {
268: 	auto base = baseptr + vector_index * vector_size;
269: 	auto base_data = (int32_t *)(base + sizeof(nullmask_t));
270: 
271: 	if (string_updates && string_updates[vector_index]) {
272: 		// there are updates: merge them in
273: 		auto &info = *string_updates[vector_index];
274: 		idx_t update_idx = 0;
275: 		for (idx_t i = 0; i < count; i++) {
276: 			auto id = ids[i] - vector_offset;
277: 			while (update_idx < info.count && info.ids[update_idx] < id) {
278: 				update_idx++;
279: 			}
280: 			if (update_idx < info.count && info.ids[update_idx] == id) {
281: 				// use update info
282: 				result[i].block_id = info.block_ids[update_idx];
283: 				result[i].offset = info.offsets[update_idx];
284: 				update_idx++;
285: 			} else {
286: 				// use base table info
287: 				result[i] = FetchStringLocation(baseptr, base_data[id]);
288: 			}
289: 		}
290: 	} else {
291: 		// no updates: fetch strings from base vector
292: 		for (idx_t i = 0; i < count; i++) {
293: 			auto id = ids[i] - vector_offset;
294: 			result[i] = FetchStringLocation(baseptr, base_data[id]);
295: 		}
296: 	}
297: }
298: 
299: string_location_t StringSegment::FetchStringLocation(data_ptr_t baseptr, int32_t dict_offset) {
300: 	if (dict_offset == 0) {
301: 		return string_location_t(INVALID_BLOCK, 0);
302: 	}
303: 	// look up result in dictionary
304: 	auto dict_end = baseptr + Storage::BLOCK_SIZE;
305: 	auto dict_pos = dict_end - dict_offset;
306: 	auto string_length = *((uint16_t *)dict_pos);
307: 	string_location_t result;
308: 	if (string_length == BIG_STRING_MARKER) {
309: 		ReadStringMarker(dict_pos, result.block_id, result.offset);
310: 	} else {
311: 		result.block_id = INVALID_BLOCK;
312: 		result.offset = dict_offset;
313: 	}
314: 	return result;
315: }
316: 
317: string_t StringSegment::FetchStringFromDict(buffer_handle_set_t &handles, data_ptr_t baseptr, int32_t dict_offset) {
318: 	// fetch base data
319: 	assert(dict_offset <= Storage::BLOCK_SIZE);
320: 	string_location_t location = FetchStringLocation(baseptr, dict_offset);
321: 	return FetchString(handles, baseptr, location);
322: }
323: 
324: string_t StringSegment::FetchString(buffer_handle_set_t &handles, data_ptr_t baseptr, string_location_t location) {
325: 	if (location.block_id != INVALID_BLOCK) {
326: 		// big string marker: read from separate block
327: 		return ReadString(handles, location.block_id, location.offset);
328: 	} else {
329: 		if (location.offset == 0) {
330: 			return string_t(nullptr, 0);
331: 		}
332: 		// normal string: read string from this block
333: 		auto dict_end = baseptr + Storage::BLOCK_SIZE;
334: 		auto dict_pos = dict_end - location.offset;
335: 		auto string_length = *((uint16_t *)dict_pos);
336: 
337: 		auto str_ptr = (char *)(dict_pos + sizeof(uint16_t));
338: 		return string_t(str_ptr, string_length);
339: 	}
340: }
341: 
342: void StringSegment::FetchRow(ColumnFetchState &state, Transaction &transaction, row_t row_id, Vector &result,
343:                              idx_t result_idx) {
344: 	auto read_lock = lock.GetSharedLock();
345: 
346: 	idx_t vector_index = row_id / STANDARD_VECTOR_SIZE;
347: 	idx_t id_in_vector = row_id - vector_index * STANDARD_VECTOR_SIZE;
348: 	assert(vector_index < max_vector_count);
349: 
350: 	data_ptr_t baseptr;
351: 
352: 	// fetch a single row from the string segment
353: 	// first pin the main buffer if it is not already pinned
354: 	auto entry = state.handles.find(block_id);
355: 	if (entry == state.handles.end()) {
356: 		// not pinned yet: pin it
357: 		auto handle = manager.Pin(block_id);
358: 		baseptr = handle->node->buffer;
359: 		state.handles[block_id] = move(handle);
360: 	} else {
361: 		// already pinned: use the pinned handle
362: 		baseptr = entry->second->node->buffer;
363: 	}
364: 
365: 	auto base = baseptr + vector_index * vector_size;
366: 	auto &base_nullmask = *((nullmask_t *)base);
367: 	auto base_data = (int32_t *)(base + sizeof(nullmask_t));
368: 	auto result_data = FlatVector::GetData<string_t>(result);
369: 	auto &result_mask = FlatVector::Nullmask(result);
370: 
371: 	bool found_data = false;
372: 	// first see if there is any updated version of this tuple we must fetch
373: 	if (versions && versions[vector_index]) {
374: 		UpdateInfo::UpdatesForTransaction(versions[vector_index], transaction, [&](UpdateInfo *current) {
375: 			auto info_data = (string_location_t *)current->tuple_data;
376: 			// loop over the tuples in this UpdateInfo
377: 			for (idx_t i = 0; i < current->N; i++) {
378: 				if (current->tuples[i] == row_id) {
379: 					// found the relevant tuple
380: 					found_data = true;
381: 					result_data[result_idx] = FetchString(state.handles, baseptr, info_data[i]);
382: 					result_mask[result_idx] = current->nullmask[current->tuples[i]];
383: 					break;
384: 				} else if (current->tuples[i] > row_id) {
385: 					// tuples are sorted: so if the current tuple is > row_id we will not find it anymore
386: 					break;
387: 				}
388: 			}
389: 		});
390: 	}
391: 	if (!found_data && string_updates && string_updates[vector_index]) {
392: 		// there are updates: check if we should use them
393: 		auto &info = *string_updates[vector_index];
394: 		for (idx_t i = 0; i < info.count; i++) {
395: 			if (info.ids[i] == id_in_vector) {
396: 				// use the update
397: 				result_data[result_idx] = ReadString(state.handles, info.block_ids[i], info.offsets[i]);
398: 				found_data = true;
399: 				break;
400: 			} else if (info.ids[i] > id_in_vector) {
401: 				break;
402: 			}
403: 		}
404: 	}
405: 	if (!found_data) {
406: 		// no version was found yet: fetch base table version
407: 		result_data[result_idx] = FetchStringFromDict(state.handles, baseptr, base_data[id_in_vector]);
408: 	}
409: 	result_mask[result_idx] = base_nullmask[id_in_vector];
410: }
411: 
412: //===--------------------------------------------------------------------===//
413: // Append
414: //===--------------------------------------------------------------------===//
415: idx_t StringSegment::Append(SegmentStatistics &stats, Vector &data, idx_t offset, idx_t count) {
416: 	assert(data.type == TypeId::VARCHAR);
417: 	auto handle = manager.Pin(block_id);
418: 	idx_t initial_count = tuple_count;
419: 	while (count > 0) {
420: 		// get the vector index of the vector to append to and see how many tuples we can append to that vector
421: 		idx_t vector_index = tuple_count / STANDARD_VECTOR_SIZE;
422: 		if (vector_index == max_vector_count) {
423: 			// we are at the maximum vector, check if there is space to increase the maximum vector count
424: 			// as a heuristic, we only allow another vector to be added if we have at least 32 bytes per string
425: 			// remaining (32KB out of a 256KB block, or around 12% empty)
426: 			if (RemainingSpace() >= STANDARD_VECTOR_SIZE * 32) {
427: 				// we have enough remaining space to add another vector
428: 				ExpandStringSegment(handle->node->buffer);
429: 			} else {
430: 				break;
431: 			}
432: 		}
433: 		idx_t current_tuple_count = tuple_count - vector_index * STANDARD_VECTOR_SIZE;
434: 		idx_t append_count = std::min(STANDARD_VECTOR_SIZE - current_tuple_count, count);
435: 
436: 		// now perform the actual append
437: 		AppendData(stats, handle->node->buffer + vector_size * vector_index, handle->node->buffer + Storage::BLOCK_SIZE,
438: 		           current_tuple_count, data, offset, append_count);
439: 
440: 		count -= append_count;
441: 		offset += append_count;
442: 		tuple_count += append_count;
443: 	}
444: 	return tuple_count - initial_count;
445: }
446: 
447: static void update_min_max(string value, char *__restrict min, char *__restrict max) {
448: 	//! we can only fit 8 bytes, so we might need to trim our string
449: 	size_t value_size = value.size() > 7 ? 7 : value.size();
450: 	//! This marks the min/max was not initialized
451: 	char marker = '1';
452: 	if (min[0] == '\0' && min[1] == marker && max[0] == '\0' && max[1] == marker) {
453: 		size_t min_end = value.copy(min, value_size);
454: 		size_t max_end = value.copy(max, value_size);
455: 		for (size_t i = min_end; i < 8; i++) {
456: 			min[i] = '\0';
457: 		}
458: 		for (size_t i = max_end; i < 8; i++) {
459: 			max[i] = '\0';
460: 		}
461: 	}
462: 	if (strcmp(value.data(), min) < 0) {
463: 		size_t min_end = value.copy(min, value_size);
464: 		for (size_t i = min_end; i < 8; i++) {
465: 			min[i] = '\0';
466: 		}
467: 	}
468: 	if (strcmp(value.data(), max) > 0) {
469: 		size_t max_end = value.copy(max, value_size);
470: 		for (size_t i = max_end; i < 8; i++) {
471: 			max[i] = '\0';
472: 		}
473: 	}
474: }
475: 
476: void StringSegment::AppendData(SegmentStatistics &stats, data_ptr_t target, data_ptr_t end, idx_t target_offset,
477:                                Vector &source, idx_t offset, idx_t count) {
478: 	VectorData adata;
479: 	source.Orrify(count, adata);
480: 
481: 	auto sdata = (string_t *)adata.data;
482: 	auto &result_nullmask = *((nullmask_t *)target);
483: 	auto result_data = (int32_t *)(target + sizeof(nullmask_t));
484: 	auto min = (char *)stats.minimum.get();
485: 	auto max = (char *)stats.maximum.get();
486: 
487: 	idx_t remaining_strings = STANDARD_VECTOR_SIZE - (this->tuple_count % STANDARD_VECTOR_SIZE);
488: 	for (idx_t i = 0; i < count; i++) {
489: 		auto source_idx = adata.sel->get_index(offset + i);
490: 		auto target_idx = target_offset + i;
491: 		if ((*adata.nullmask)[source_idx]) {
492: 			// null value is stored as -1
493: 			result_data[target_idx] = 0;
494: 			result_nullmask[target_idx] = true;
495: 			stats.has_null = true;
496: 		} else {
497: 			assert(dictionary_offset < Storage::BLOCK_SIZE);
498: 			// non-null value, check if we can fit it within the block
499: 			idx_t string_length = sdata[source_idx].GetSize();
500: 			idx_t total_length = string_length + 1 + sizeof(uint16_t);
501: 
502: 			if (string_length > stats.max_string_length) {
503: 				stats.max_string_length = string_length;
504: 			}
505: 			// determine whether or not the string needs to be stored in an overflow block
506: 			// we never place small strings in the overflow blocks: the pointer would take more space than the
507: 			// string itself we always place big strings (>= STRING_BLOCK_LIMIT) in the overflow blocks we also have
508: 			// to always leave enough room for BIG_STRING_MARKER_SIZE for each of the remaining strings
509: 			if (total_length > BIG_STRING_MARKER_BASE_SIZE &&
510: 			    (total_length >= STRING_BLOCK_LIMIT ||
511: 			     total_length + (remaining_strings * BIG_STRING_MARKER_SIZE) > RemainingSpace())) {
512: 				assert(RemainingSpace() >= BIG_STRING_MARKER_SIZE);
513: 				// string is too big for block: write to overflow blocks
514: 				block_id_t block;
515: 				int32_t offset;
516: 				//! Update min/max of column segment
517: 				update_min_max(sdata[source_idx].GetData(), min, max);
518: 				// write the string into the current string block
519: 				WriteString(sdata[source_idx], block, offset);
520: 				dictionary_offset += BIG_STRING_MARKER_SIZE;
521: 				auto dict_pos = end - dictionary_offset;
522: 
523: 				// write a big string marker into the dictionary
524: 				WriteStringMarker(dict_pos, block, offset);
525: 
526: 				stats.has_overflow_strings = true;
527: 			} else {
528: 				// string fits in block, append to dictionary and increment dictionary position
529: 				assert(string_length < std::numeric_limits<uint16_t>::max());
530: 				dictionary_offset += total_length;
531: 				auto dict_pos = end - dictionary_offset;
532: 				//! Update min/max of column segment
533: 				update_min_max(sdata[source_idx].GetData(), min, max);
534: 				// first write the length as u16
535: 				uint16_t string_length_u16 = string_length;
536: 				memcpy(dict_pos, &string_length_u16, sizeof(uint16_t));
537: 				// now write the actual string data into the dictionary
538: 				memcpy(dict_pos + sizeof(uint16_t), sdata[source_idx].GetData(), string_length + 1);
539: 			}
540: 			// place the dictionary offset into the set of vectors
541: 			assert(dictionary_offset <= Storage::BLOCK_SIZE);
542: 			result_data[target_idx] = dictionary_offset;
543: 		}
544: 		remaining_strings--;
545: 	}
546: }
547: 
548: void StringSegment::WriteString(string_t string, block_id_t &result_block, int32_t &result_offset) {
549: 	assert(strlen(string.GetData()) == string.GetSize());
550: 	if (overflow_writer) {
551: 		// overflow writer is set: write string there
552: 		overflow_writer->WriteString(string, result_block, result_offset);
553: 	} else {
554: 		// default overflow behavior: use in-memory buffer to store the overflow string
555: 		WriteStringMemory(string, result_block, result_offset);
556: 	}
557: }
558: 
559: void StringSegment::WriteStringMemory(string_t string, block_id_t &result_block, int32_t &result_offset) {
560: 	uint32_t total_length = string.GetSize() + 1 + sizeof(uint32_t);
561: 	unique_ptr<BufferHandle> handle;
562: 	// check if the string fits in the current block
563: 	if (!head || head->offset + total_length >= head->size) {
564: 		// string does not fit, allocate space for it
565: 		// create a new string block
566: 		idx_t alloc_size = std::max((idx_t)total_length, (idx_t)Storage::BLOCK_ALLOC_SIZE);
567: 		auto new_block = make_unique<StringBlock>();
568: 		new_block->offset = 0;
569: 		new_block->size = alloc_size;
570: 		// allocate an in-memory buffer for it
571: 		handle = manager.Allocate(alloc_size);
572: 		new_block->block_id = handle->block_id;
573: 		new_block->next = move(head);
574: 		head = move(new_block);
575: 	} else {
576: 		// string fits, copy it into the current block
577: 		handle = manager.Pin(head->block_id);
578: 	}
579: 
580: 	result_block = head->block_id;
581: 	result_offset = head->offset;
582: 
583: 	// copy the string and the length there
584: 	auto ptr = handle->node->buffer + head->offset;
585: 	memcpy(ptr, &string.length, sizeof(uint32_t));
586: 	ptr += sizeof(uint32_t);
587: 	memcpy(ptr, string.GetData(), string.length + 1);
588: 	head->offset += total_length;
589: }
590: 
591: string_t StringSegment::ReadString(buffer_handle_set_t &handles, block_id_t block, int32_t offset) {
592: 	assert(offset < Storage::BLOCK_SIZE);
593: 	if (block == INVALID_BLOCK) {
594: 		return string_t(nullptr, 0);
595: 	}
596: 	if (block < MAXIMUM_BLOCK) {
597: 		// read the overflow string from disk
598: 		// pin the initial handle and read the length
599: 		auto handle = manager.Pin(block);
600: 		uint32_t length = *((uint32_t *)(handle->node->buffer + offset));
601: 		uint32_t remaining = length + 1;
602: 		offset += sizeof(uint32_t);
603: 
604: 		// allocate a buffer to store the string
605: 		auto alloc_size = std::max((idx_t)Storage::BLOCK_ALLOC_SIZE, (idx_t)length + 1 + sizeof(uint32_t));
606: 		auto target_handle = manager.Allocate(alloc_size, true);
607: 		auto target_ptr = target_handle->node->buffer;
608: 		// write the length in this block as well
609: 		*((uint32_t *)target_ptr) = length;
610: 		target_ptr += sizeof(uint32_t);
611: 		// now append the string to the single buffer
612: 		while (remaining > 0) {
613: 			idx_t to_write = std::min((idx_t)remaining, (idx_t)(Storage::BLOCK_SIZE - sizeof(block_id_t) - offset));
614: 			memcpy(target_ptr, handle->node->buffer + offset, to_write);
615: 
616: 			remaining -= to_write;
617: 			offset += to_write;
618: 			target_ptr += to_write;
619: 			if (remaining > 0) {
620: 				// read the next block
621: 				block_id_t next_block = *((block_id_t *)(handle->node->buffer + offset));
622: 				handle = manager.Pin(next_block);
623: 				offset = 0;
624: 			}
625: 		}
626: 
627: 		auto final_buffer = target_handle->node->buffer;
628: 		handles.insert(make_pair(target_handle->block_id, move(target_handle)));
629: 		return ReadString(final_buffer, 0);
630: 	} else {
631: 		// read the overflow string from memory
632: 		// first pin the handle, if it is not pinned yet
633: 		BufferHandle *handle;
634: 		auto entry = handles.find(block);
635: 		if (entry == handles.end()) {
636: 			auto pinned_handle = manager.Pin(block);
637: 			handle = pinned_handle.get();
638: 
639: 			handles.insert(make_pair(block, move(pinned_handle)));
640: 		} else {
641: 			handle = entry->second.get();
642: 		}
643: 		return ReadString(handle->node->buffer, offset);
644: 	}
645: }
646: 
647: string_t StringSegment::ReadString(data_ptr_t target, int32_t offset) {
648: 	auto ptr = target + offset;
649: 	auto str_length = *((uint32_t *)ptr);
650: 	auto str_ptr = (char *)(ptr + sizeof(uint32_t));
651: 	return string_t(str_ptr, str_length);
652: }
653: 
654: void StringSegment::WriteStringMarker(data_ptr_t target, block_id_t block_id, int32_t offset) {
655: 	uint16_t length = BIG_STRING_MARKER;
656: 	memcpy(target, &length, sizeof(uint16_t));
657: 	target += sizeof(uint16_t);
658: 	memcpy(target, &block_id, sizeof(block_id_t));
659: 	target += sizeof(block_id_t);
660: 	memcpy(target, &offset, sizeof(int32_t));
661: }
662: 
663: void StringSegment::ReadStringMarker(data_ptr_t target, block_id_t &block_id, int32_t &offset) {
664: 	target += sizeof(uint16_t);
665: 	memcpy(&block_id, target, sizeof(block_id_t));
666: 	target += sizeof(block_id_t);
667: 	memcpy(&offset, target, sizeof(int32_t));
668: }
669: 
670: //===--------------------------------------------------------------------===//
671: // String Update
672: //===--------------------------------------------------------------------===//
673: string_update_info_t StringSegment::CreateStringUpdate(SegmentStatistics &stats, Vector &update, row_t *ids,
674:                                                        idx_t count, idx_t vector_offset) {
675: 	auto info = make_unique<StringUpdateInfo>();
676: 	info->count = count;
677: 	auto strings = FlatVector::GetData<string_t>(update);
678: 	auto &update_nullmask = FlatVector::Nullmask(update);
679: 	for (idx_t i = 0; i < count; i++) {
680: 		info->ids[i] = ids[i] - vector_offset;
681: 		// copy the string into the block
682: 		if (!update_nullmask[i]) {
683: 			auto min = (char *)stats.minimum.get();
684: 			auto max = (char *)stats.maximum.get();
685: 			for (idx_t i = 0; i < count; i++) {
686: 				update_min_max(strings[i].GetData(), min, max);
687: 			}
688: 			WriteString(strings[i], info->block_ids[i], info->offsets[i]);
689: 		} else {
690: 			info->block_ids[i] = INVALID_BLOCK;
691: 			info->offsets[i] = 0;
692: 		}
693: 	}
694: 	return info;
695: }
696: 
697: string_update_info_t StringSegment::MergeStringUpdate(SegmentStatistics &stats, Vector &update, row_t *ids,
698:                                                       idx_t update_count, idx_t vector_offset,
699:                                                       StringUpdateInfo &update_info) {
700: 	auto info = make_unique<StringUpdateInfo>();
701: 
702: 	// perform a merge between the new and old indexes
703: 	auto strings = FlatVector::GetData<string_t>(update);
704: 	auto &update_nullmask = FlatVector::Nullmask(update);
705: 	//! Check if we need to update the segment's nullmask
706: 	for (idx_t i = 0; i < update_count; i++) {
707: 		if (!update_nullmask[i]) {
708: 			auto min = (char *)stats.minimum.get();
709: 			auto max = (char *)stats.maximum.get();
710: 			update_min_max(strings[i].GetData(), min, max);
711: 		}
712: 	}
713: 	auto pick_new = [&](idx_t id, idx_t idx, idx_t count) {
714: 		info->ids[count] = id;
715: 		if (!update_nullmask[idx]) {
716: 			WriteString(strings[idx], info->block_ids[count], info->offsets[count]);
717: 		} else {
718: 			info->block_ids[count] = INVALID_BLOCK;
719: 			info->offsets[count] = 0;
720: 		}
721: 	};
722: 	auto merge = [&](idx_t id, idx_t aidx, idx_t bidx, idx_t count) {
723: 		// merge: only pick new entry
724: 		pick_new(id, aidx, count);
725: 	};
726: 	auto pick_old = [&](idx_t id, idx_t bidx, idx_t count) {
727: 		// pick old entry
728: 		info->ids[count] = id;
729: 		info->block_ids[count] = update_info.block_ids[bidx];
730: 		info->offsets[count] = update_info.offsets[bidx];
731: 	};
732: 
733: 	info->count =
734: 	    merge_loop(ids, update_info.ids, update_count, update_info.count, vector_offset, merge, pick_new, pick_old);
735: 	return info;
736: }
737: 
738: //===--------------------------------------------------------------------===//
739: // Update Info
740: //===--------------------------------------------------------------------===//
741: void StringSegment::MergeUpdateInfo(UpdateInfo *node, row_t *ids, idx_t update_count, idx_t vector_offset,
742:                                     string_location_t base_data[], nullmask_t base_nullmask) {
743: 	auto info_data = (string_location_t *)node->tuple_data;
744: 
745: 	// first we copy the old update info into a temporary structure
746: 	sel_t old_ids[STANDARD_VECTOR_SIZE];
747: 	string_location_t old_data[STANDARD_VECTOR_SIZE];
748: 
749: 	memcpy(old_ids, node->tuples, node->N * sizeof(sel_t));
750: 	memcpy(old_data, node->tuple_data, node->N * sizeof(string_location_t));
751: 
752: 	// now we perform a merge of the new ids with the old ids
753: 	auto merge = [&](idx_t id, idx_t aidx, idx_t bidx, idx_t count) {
754: 		// new_id and old_id are the same, insert the old data in the UpdateInfo
755: 		assert(old_data[bidx].IsValid());
756: 		info_data[count] = old_data[bidx];
757: 		node->tuples[count] = id;
758: 	};
759: 	auto pick_new = [&](idx_t id, idx_t aidx, idx_t count) {
760: 		// new_id comes before the old id, insert the base table data into the update info
761: 		assert(base_data[aidx].IsValid());
762: 		info_data[count] = base_data[aidx];
763: 		node->nullmask[id] = base_nullmask[aidx];
764: 
765: 		node->tuples[count] = id;
766: 	};
767: 	auto pick_old = [&](idx_t id, idx_t bidx, idx_t count) {
768: 		// old_id comes before new_id, insert the old data
769: 		assert(old_data[bidx].IsValid());
770: 		info_data[count] = old_data[bidx];
771: 		node->tuples[count] = id;
772: 	};
773: 	// perform the merge
774: 	node->N = merge_loop(ids, old_ids, update_count, node->N, vector_offset, merge, pick_new, pick_old);
775: }
776: 
777: //===--------------------------------------------------------------------===//
778: // Update
779: //===--------------------------------------------------------------------===//
780: void StringSegment::Update(ColumnData &column_data, SegmentStatistics &stats, Transaction &transaction, Vector &update,
781:                            row_t *ids, idx_t count, idx_t vector_index, idx_t vector_offset, UpdateInfo *node) {
782: 	if (!string_updates) {
783: 		string_updates = unique_ptr<string_update_info_t[]>(new string_update_info_t[max_vector_count]);
784: 	}
785: 
786: 	// first pin the base block
787: 	auto handle = manager.Pin(block_id);
788: 	auto baseptr = handle->node->buffer;
789: 	auto base = baseptr + vector_index * vector_size;
790: 	auto &base_nullmask = *((nullmask_t *)base);
791: 
792: 	// fetch the original string locations and copy the original nullmask
793: 	string_location_t string_locations[STANDARD_VECTOR_SIZE];
794: 	nullmask_t original_nullmask = base_nullmask;
795: 	FetchStringLocations(baseptr, ids, vector_index, vector_offset, count, string_locations);
796: 
797: 	string_update_info_t new_update_info;
798: 	// next up: create the updates
799: 	if (!string_updates[vector_index]) {
800: 		// no string updates yet, allocate a block and place the updates there
801: 		new_update_info = CreateStringUpdate(stats, update, ids, count, vector_offset);
802: 	} else {
803: 		// string updates already exist, merge the string updates together
804: 		new_update_info = MergeStringUpdate(stats, update, ids, count, vector_offset, *string_updates[vector_index]);
805: 	}
806: 
807: 	// now update the original nullmask
808: 	auto &update_nullmask = FlatVector::Nullmask(update);
809: 	for (idx_t i = 0; i < count; i++) {
810: 		base_nullmask[ids[i] - vector_offset] = update_nullmask[i];
811: 	}
812: 
813: 	// now that the original strings are placed in the undo buffer and the updated strings are placed in the base table
814: 	// create the update node
815: 	if (!node) {
816: 		// create a new node in the undo buffer for this update
817: 		node = CreateUpdateInfo(column_data, transaction, ids, count, vector_index, vector_offset,
818: 		                        sizeof(string_location_t));
819: 
820: 		// copy the string location data into the undo buffer
821: 		node->nullmask = original_nullmask;
822: 		memcpy(node->tuple_data, string_locations, sizeof(string_location_t) * count);
823: 	} else {
824: 		// node in the update info already exists, merge the new updates in
825: 		MergeUpdateInfo(node, ids, count, vector_offset, string_locations, original_nullmask);
826: 	}
827: 	// finally move the string updates in place
828: 	string_updates[vector_index] = move(new_update_info);
829: }
830: 
831: void StringSegment::RollbackUpdate(UpdateInfo *info) {
832: 	auto lock_handle = lock.GetExclusiveLock();
833: 
834: 	idx_t new_count = 0;
835: 	auto &update_info = *string_updates[info->vector_index];
836: 	auto string_locations = (string_location_t *)info->tuple_data;
837: 
838: 	// put the previous NULL values back
839: 	auto handle = manager.Pin(block_id);
840: 	auto baseptr = handle->node->buffer;
841: 	auto base = baseptr + info->vector_index * vector_size;
842: 	auto &base_nullmask = *((nullmask_t *)base);
843: 	for (idx_t i = 0; i < info->N; i++) {
844: 		base_nullmask[info->tuples[i]] = info->nullmask[info->tuples[i]];
845: 	}
846: 
847: 	// now put the original values back into the update info
848: 	idx_t old_idx = 0;
849: 	for (idx_t i = 0; i < update_info.count; i++) {
850: 		if (old_idx >= info->N || update_info.ids[i] != info->tuples[old_idx]) {
851: 			assert(old_idx >= info->N || update_info.ids[i] < info->tuples[old_idx]);
852: 			// this entry is not rolled back: insert entry directly
853: 			update_info.ids[new_count] = update_info.ids[i];
854: 			update_info.block_ids[new_count] = update_info.block_ids[i];
855: 			update_info.offsets[new_count] = update_info.offsets[i];
856: 			new_count++;
857: 		} else {
858: 			// this entry is being rolled back
859: 			auto &old_location = string_locations[old_idx];
860: 			if (old_location.block_id != INVALID_BLOCK) {
861: 				// not rolled back to base table: insert entry again
862: 				update_info.ids[new_count] = update_info.ids[i];
863: 				update_info.block_ids[new_count] = old_location.block_id;
864: 				update_info.offsets[new_count] = old_location.offset;
865: 				new_count++;
866: 			}
867: 			old_idx++;
868: 		}
869: 	}
870: 
871: 	if (new_count == 0) {
872: 		// all updates are rolled back: delete the string update vector
873: 		string_updates[info->vector_index].reset();
874: 	} else {
875: 		// set the count of the new string update vector
876: 		update_info.count = new_count;
877: 	}
878: 	CleanupUpdate(info);
879: }
[end of src/storage/string_segment.cpp]
[start of third_party/libpg_query/pg_functions.cpp]
1: #include <stdexcept>
2: #include <string>
3: #include <thread>
4: #include <mutex>
5: #include "pg_functions.hpp"
6: #include "parser/parser.hpp"
7: #include <stdarg.h>
8: #include <mutex>
9: #include <cstring>
10: 
11: 
12: // max parse tree size approx 100 MB, should be enough
13: #define PG_MALLOC_SIZE 10240
14: #define PG_MALLOC_LIMIT 1000
15: 
16: typedef struct pg_parser_state_str parser_state;
17: struct pg_parser_state_str {
18: 	int pg_err_code;
19: 	int pg_err_pos;
20: 	char pg_err_msg[BUFSIZ];
21: 
22: 	size_t malloc_pos;
23: 	size_t malloc_ptr_idx;
24: 	char *malloc_ptrs[PG_MALLOC_LIMIT];
25: };
26: 
27: static __thread parser_state pg_parser_state;
28: 
29: #ifndef __GNUC__
30: __thread PGNode *newNodeMacroHolder;
31: #endif
32: 
33: static void allocate_new(parser_state* state, size_t n) {
34: 	if (state->malloc_ptr_idx + 1 >= PG_MALLOC_LIMIT) {
35: 		throw std::runtime_error("Memory allocation failure");
36: 	}
37: 	if (n < PG_MALLOC_SIZE) {
38: 		n = PG_MALLOC_SIZE;
39: 	}
40: 	char* base_ptr = (char*) malloc(n);
41: 	if (!base_ptr) {
42: 		throw std::runtime_error("Memory allocation failure");
43: 	}
44: 	state->malloc_ptrs[state->malloc_ptr_idx] = base_ptr;
45: 	state->malloc_ptr_idx++;
46: 	state->malloc_pos = 0;
47: }
48: 
49: 
50: void* palloc(size_t n) {
51: 	if (pg_parser_state.malloc_pos + n > PG_MALLOC_SIZE) {
52: 		allocate_new(&pg_parser_state, n);
53: 	}
54: 
55: 	void *ptr = pg_parser_state.malloc_ptrs[pg_parser_state.malloc_ptr_idx - 1] + pg_parser_state.malloc_pos;
56: 	memset(ptr, 0, n);
57: 	pg_parser_state.malloc_pos += n;
58: 	return ptr;
59: }
60: 
61: 
62: 
63: void pg_parser_init() {
64: 	pg_parser_state.pg_err_code = PGUNDEFINED;
65: 	pg_parser_state.pg_err_msg[0] = '\0';
66: 
67: 	pg_parser_state.malloc_ptr_idx = 0;
68: 	allocate_new(&pg_parser_state, 1);
69: }
70: 
71: void pg_parser_parse(const char* query, parse_result *res) {
72: 
73: 	res->parse_tree = nullptr;
74: 	try{
75: 		res->parse_tree = raw_parser(query);
76: 		res->success = pg_parser_state.pg_err_code == PGUNDEFINED;
77: 	} catch (...) {
78: 		res->success = false;
79: 
80: 	}
81: 	res->error_message = pg_parser_state.pg_err_msg;
82: 	res->error_location = pg_parser_state.pg_err_pos;
83: }
84: 
85: 
86: void pg_parser_cleanup() {
87: 	for (size_t ptr_idx = 0; ptr_idx < pg_parser_state.malloc_ptr_idx; ptr_idx++) {
88: 		char *ptr = pg_parser_state.malloc_ptrs[ptr_idx];
89: 		if (ptr) {
90: 			free(ptr);
91: 			pg_parser_state.malloc_ptrs[ptr_idx] = nullptr;
92: 		}
93: 	}
94: }
95: 
96: int ereport(int code, ...) {
97: 	std::string err = "parser error : " + std::string(pg_parser_state.pg_err_msg);
98:     throw std::runtime_error(err);
99: }
100: void elog(int code, const char* fmt,...) {
101:     throw std::runtime_error("elog NOT IMPLEMENTED");
102: }
103: int errcode(int sqlerrcode) {
104: 	pg_parser_state.pg_err_code = sqlerrcode;
105: 	return 1;
106: }
107: int errmsg(const char* fmt, ...) {
108: 	 va_list argptr;
109: 	 va_start(argptr, fmt);
110: 	 vsnprintf(pg_parser_state.pg_err_msg, BUFSIZ, fmt, argptr);
111: 	 va_end(argptr);
112: 	 return 1;
113: }
114: int errhint(const char* msg) {
115:     throw std::runtime_error("errhint NOT IMPLEMENTED");
116: }
117: int	errmsg_internal(const char *fmt,...) {
118:     throw std::runtime_error("errmsg_internal NOT IMPLEMENTED");
119: }
120: int	errdetail(const char *fmt,...) {
121:     throw std::runtime_error("errdetail NOT IMPLEMENTED");
122: }
123: int	errposition(int cursorpos) {
124: 	pg_parser_state.pg_err_pos = cursorpos;
125: 	return 1;
126: }
127: 
128: 
129: char *
130: psprintf(const char *fmt,...) {
131: 	char buf[BUFSIZ];
132: 	va_list		args;
133: 	size_t newlen;
134: 
135: 	// attempt one: use stack buffer and determine length
136: 	va_start(args, fmt);
137: 	newlen = vsnprintf(buf, BUFSIZ, fmt, args);
138: 	va_end(args);
139: 	if (newlen < BUFSIZ) {
140: 		return pstrdup(buf);
141: 	}
142: 
143: 	// attempt two, malloc
144: 	char* mbuf = (char*) palloc(newlen);
145: 	va_start(args, fmt);
146: 	vsnprintf(mbuf, newlen, fmt, args);
147: 	va_end(args);
148: 	return mbuf;
149: }
150: 
151: char *pstrdup(const char *in) {
152: 	char* new_str = (char*) palloc(strlen(in)+1);
153: 	memcpy(new_str, in, strlen(in));
154: 	return new_str;
155: }
156: 
157: void pfree(void* ptr) {
158:     // nop, we free up entire context on parser cleanup
159: }
160: void* palloc0fast(size_t n) { // very fast
161:     return palloc(n);
162: }
163: void* repalloc(void* ptr, size_t n) {
164:     throw std::runtime_error("repalloc NOT IMPLEMENTED");
165: }
166: char *NameListToString(PGList *names) {
167:     throw std::runtime_error("NameListToString NOT IMPLEMENTED");
168: }
169: void * copyObject(const void *from) {
170:     throw std::runtime_error("copyObject NOT IMPLEMENTED");
171: }
172: bool equal(const void *a, const void *b) {
173:     throw std::runtime_error("equal NOT IMPLEMENTED");
174: }
175: int exprLocation(const PGNode *expr) {
176:     throw std::runtime_error("exprLocation NOT IMPLEMENTED");
177: }
178: bool pg_verifymbstr(const char *mbstr, int len, bool noError) {
179:     throw std::runtime_error("pg_verifymbstr NOT IMPLEMENTED");
180: }
181: 
182: int pg_database_encoding_max_length(void) {
183: 	return 4; // UTF8
184: }
185: 
186: static int
187: pg_utf_mblen(const unsigned char *s)
188: {
189: 	int	len;
190: 
191: 	if ((*s & 0x80) == 0)
192: 		len = 1;
193: 	else if ((*s & 0xe0) == 0xc0)
194: 		len = 2;
195: 	else if ((*s & 0xf0) == 0xe0)
196: 		len = 3;
197: 	else if ((*s & 0xf8) == 0xf0)
198: 		len = 4;
199: #ifdef NOT_USED
200: 	else if ((*s & 0xfc) == 0xf8)
201: 		len = 5;
202: 	else if ((*s & 0xfe) == 0xfc)
203: 		len = 6;
204: #endif
205: 	else
206: 		len = 1;
207: 	return len;
208: }
209: 
210: 
211: int pg_mbstrlen_with_len(const char *mbstr, int limit) {
212: 	int	len = 0;
213: 	while (limit > 0 && *mbstr)	{
214: 		int	l = pg_utf_mblen((const unsigned char*) mbstr);
215: 		limit -= l;
216: 		mbstr += l;
217: 		len++;
218: 	}
219: 	return len;
220: }
221: 
222: 
223: int pg_mbcliplen(const char *mbstr, int len, int limit) {
224:     throw std::runtime_error("pg_mbcliplen NOT IMPLEMENTED");
225: }
226: int pg_mblen(const char *mbstr) {
227:     throw std::runtime_error("pg_mblen NOT IMPLEMENTED");
228: }
229: PGDefElem * defWithOids(bool value) {
230:     throw std::runtime_error("defWithOids NOT IMPLEMENTED");
231: }
232: unsigned char *unicode_to_utf8(pg_wchar c, unsigned char *utf8string) {
233:     throw std::runtime_error("unicode_to_utf8 NOT IMPLEMENTED");
234: }
235: 
[end of third_party/libpg_query/pg_functions.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: