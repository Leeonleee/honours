{
  "repo": "duckdb/duckdb",
  "pull_number": 2504,
  "instance_id": "duckdb__duckdb-2504",
  "issue_numbers": [
    "2295"
  ],
  "base_commit": "cb0d00d5fb1efbcf1ddf5413392c22926276fb9b",
  "patch": "diff --git a/src/common/types/hugeint.cpp b/src/common/types/hugeint.cpp\nindex 59ea3cf1daa4..e6ba24c4e752 100644\n--- a/src/common/types/hugeint.cpp\n+++ b/src/common/types/hugeint.cpp\n@@ -460,20 +460,30 @@ bool Hugeint::TryCast(hugeint_t input, float &result) {\n \treturn true;\n }\n \n-template <>\n-bool Hugeint::TryCast(hugeint_t input, double &result) {\n+template <class REAL_T>\n+bool CastBigintToFloating(hugeint_t input, REAL_T &result) {\n \tswitch (input.upper) {\n \tcase -1:\n \t\t// special case for upper = -1 to avoid rounding issues in small negative numbers\n-\t\tresult = -double(NumericLimits<uint64_t>::Maximum() - input.lower) - 1;\n+\t\tresult = -REAL_T(NumericLimits<uint64_t>::Maximum() - input.lower) - 1;\n \t\tbreak;\n \tdefault:\n-\t\tresult = double(input.lower) + double(input.upper) * double(NumericLimits<uint64_t>::Maximum());\n+\t\tresult = REAL_T(input.lower) + REAL_T(input.upper) * REAL_T(NumericLimits<uint64_t>::Maximum());\n \t\tbreak;\n \t}\n \treturn true;\n }\n \n+template <>\n+bool Hugeint::TryCast(hugeint_t input, double &result) {\n+\treturn CastBigintToFloating<double>(input, result);\n+}\n+\n+template <>\n+bool Hugeint::TryCast(hugeint_t input, long double &result) {\n+\treturn CastBigintToFloating<long double>(input, result);\n+}\n+\n template <class DST>\n hugeint_t HugeintConvertInteger(DST input) {\n \thugeint_t result;\n@@ -531,8 +541,8 @@ bool Hugeint::TryConvert(float value, hugeint_t &result) {\n \treturn Hugeint::TryConvert(double(value), result);\n }\n \n-template <>\n-bool Hugeint::TryConvert(double value, hugeint_t &result) {\n+template <class REAL_T>\n+bool ConvertFloatingToBigint(REAL_T value, hugeint_t &result) {\n \tif (value <= -170141183460469231731687303715884105728.0 || value >= 170141183460469231731687303715884105727.0) {\n \t\treturn false;\n \t}\n@@ -540,14 +550,24 @@ bool Hugeint::TryConvert(double value, hugeint_t &result) {\n \tif (negative) {\n \t\tvalue = -value;\n \t}\n-\tresult.lower = (uint64_t)fmod(value, double(NumericLimits<uint64_t>::Maximum()));\n-\tresult.upper = (uint64_t)(value / double(NumericLimits<uint64_t>::Maximum()));\n+\tresult.lower = (uint64_t)fmod(value, REAL_T(NumericLimits<uint64_t>::Maximum()));\n+\tresult.upper = (uint64_t)(value / REAL_T(NumericLimits<uint64_t>::Maximum()));\n \tif (negative) {\n-\t\tNegateInPlace(result);\n+\t\tHugeint::NegateInPlace(result);\n \t}\n \treturn true;\n }\n \n+template <>\n+bool Hugeint::TryConvert(double value, hugeint_t &result) {\n+\treturn ConvertFloatingToBigint<double>(value, result);\n+}\n+\n+template <>\n+bool Hugeint::TryConvert(long double value, hugeint_t &result) {\n+\treturn ConvertFloatingToBigint<long double>(value, result);\n+}\n+\n //===--------------------------------------------------------------------===//\n // hugeint_t operators\n //===--------------------------------------------------------------------===//\ndiff --git a/src/function/aggregate/algebraic/avg.cpp b/src/function/aggregate/algebraic/avg.cpp\nindex d718f22fb81c..0536a58ae089 100644\n--- a/src/function/aggregate/algebraic/avg.cpp\n+++ b/src/function/aggregate/algebraic/avg.cpp\n@@ -9,8 +9,35 @@ namespace duckdb {\n \n template <class T>\n struct AvgState {\n+\tuint64_t count;\n \tT value;\n+\n+\tvoid Initialize() {\n+\t\tthis->count = 0;\n+\t}\n+\n+\tvoid Combine(const AvgState<T> &other) {\n+\t\tthis->count += other.count;\n+\t\tthis->value += other.value;\n+\t}\n+};\n+\n+template <>\n+struct AvgState<double> {\n \tuint64_t count;\n+\tdouble value;\n+\tdouble err;\n+\n+\tvoid Initialize() {\n+\t\tthis->count = 0;\n+\t\tthis->err = 0.0;\n+\t}\n+\n+\tvoid Combine(const AvgState<double> &other) {\n+\t\tthis->count += other.count;\n+\t\tKahanAdd(other.value, this->value, this->err);\n+\t\tKahanAdd(other.err, this->value, this->err);\n+\t}\n };\n \n struct AverageDecimalBindData : public FunctionData {\n@@ -28,12 +55,11 @@ struct AverageDecimalBindData : public FunctionData {\n struct AverageSetOperation {\n \ttemplate <class STATE>\n \tstatic void Initialize(STATE *state) {\n-\t\tstate->count = 0;\n+\t\tstate->Initialize();\n \t}\n \ttemplate <class STATE>\n \tstatic void Combine(const STATE &source, STATE *target) {\n-\t\ttarget->count += source.count;\n-\t\ttarget->value += source.value;\n+\t\ttarget->Combine(source);\n \t}\n \ttemplate <class STATE>\n \tstatic void AddValues(STATE *state, idx_t count) {\n@@ -41,8 +67,9 @@ struct AverageSetOperation {\n \t}\n };\n \n-static double GetAverageDivident(uint64_t count, FunctionData *bind_data) {\n-\tdouble divident = double(count);\n+template <class T>\n+static T GetAverageDivident(uint64_t count, FunctionData *bind_data) {\n+\tT divident = T(count);\n \tif (bind_data) {\n \t\tauto &avg_bind_data = (AverageDecimalBindData &)*bind_data;\n \t\tdivident *= avg_bind_data.scale;\n@@ -57,7 +84,7 @@ struct IntegerAverageOperation : public BaseSumOperation<AverageSetOperation, Re\n \t\tif (state->count == 0) {\n \t\t\tmask.SetInvalid(idx);\n \t\t} else {\n-\t\t\tdouble divident = GetAverageDivident(state->count, bind_data);\n+\t\t\tdouble divident = GetAverageDivident<double>(state->count, bind_data);\n \t\t\ttarget[idx] = double(state->value) / divident;\n \t\t}\n \t}\n@@ -70,8 +97,8 @@ struct IntegerAverageOperationHugeint : public BaseSumOperation<AverageSetOperat\n \t\tif (state->count == 0) {\n \t\t\tmask.SetInvalid(idx);\n \t\t} else {\n-\t\t\tdouble divident = GetAverageDivident(state->count, bind_data);\n-\t\t\ttarget[idx] = Hugeint::Cast<double>(state->value) / divident;\n+\t\t\tlong double divident = GetAverageDivident<long double>(state->count, bind_data);\n+\t\t\ttarget[idx] = Hugeint::Cast<long double>(state->value) / divident;\n \t\t}\n \t}\n };\n@@ -83,13 +110,13 @@ struct HugeintAverageOperation : public BaseSumOperation<AverageSetOperation, Re\n \t\tif (state->count == 0) {\n \t\t\tmask.SetInvalid(idx);\n \t\t} else {\n-\t\t\tdouble divident = GetAverageDivident(state->count, bind_data);\n-\t\t\ttarget[idx] = Hugeint::Cast<double>(state->value) / divident;\n+\t\t\tlong double divident = GetAverageDivident<long double>(state->count, bind_data);\n+\t\t\ttarget[idx] = Hugeint::Cast<long double>(state->value) / divident;\n \t\t}\n \t}\n };\n \n-struct NumericAverageOperation : public BaseSumOperation<AverageSetOperation, RegularAdd> {\n+struct NumericAverageOperation : public BaseSumOperation<AverageSetOperation, DoubleAdd> {\n \ttemplate <class T, class STATE>\n \tstatic void Finalize(Vector &result, FunctionData *, STATE *state, T *target, ValidityMask &mask, idx_t idx) {\n \t\tif (state->count == 0) {\n@@ -98,7 +125,7 @@ struct NumericAverageOperation : public BaseSumOperation<AverageSetOperation, Re\n \t\t\tif (!Value::DoubleIsValid(state->value)) {\n \t\t\t\tthrow OutOfRangeException(\"AVG is out of range!\");\n \t\t\t}\n-\t\t\ttarget[idx] = state->value / state->count;\n+\t\t\ttarget[idx] = (state->value / state->count) + (state->err / state->count);\n \t\t}\n \t}\n };\ndiff --git a/src/function/aggregate/distributive/sum.cpp b/src/function/aggregate/distributive/sum.cpp\nindex 14cd6c62ce91..eba8d4d8db9d 100644\n--- a/src/function/aggregate/distributive/sum.cpp\n+++ b/src/function/aggregate/distributive/sum.cpp\n@@ -8,21 +8,14 @@\n \n namespace duckdb {\n \n-template <class T>\n-struct SumState {\n-\tT value;\n-\tbool isset;\n-};\n-\n struct SumSetOperation {\n \ttemplate <class STATE>\n \tstatic void Initialize(STATE *state) {\n-\t\tstate->isset = false;\n+\t\tstate->Initialize();\n \t}\n \ttemplate <class STATE>\n \tstatic void Combine(const STATE &source, STATE *target) {\n-\t\ttarget->isset = source.isset || target->isset;\n-\t\ttarget->value += source.value;\n+\t\ttarget->Combine(source);\n \t}\n \ttemplate <class STATE>\n \tstatic void AddValues(STATE *state, idx_t count) {\n@@ -52,7 +45,7 @@ struct SumToHugeintOperation : public BaseSumOperation<SumSetOperation, HugeintA\n \t}\n };\n \n-struct NumericSumOperation : public BaseSumOperation<SumSetOperation, RegularAdd> {\n+struct NumericSumOperation : public BaseSumOperation<SumSetOperation, DoubleAdd> {\n \ttemplate <class T, class STATE>\n \tstatic void Finalize(Vector &result, FunctionData *, STATE *state, T *target, ValidityMask &mask, idx_t idx) {\n \t\tif (!state->isset) {\ndiff --git a/src/include/duckdb/common/types/hugeint.hpp b/src/include/duckdb/common/types/hugeint.hpp\nindex b766c77bb9be..6d3ff11d4227 100644\n--- a/src/include/duckdb/common/types/hugeint.hpp\n+++ b/src/include/duckdb/common/types/hugeint.hpp\n@@ -139,6 +139,8 @@ template <>\n bool Hugeint::TryCast(hugeint_t input, float &result);\n template <>\n bool Hugeint::TryCast(hugeint_t input, double &result);\n+template <>\n+bool Hugeint::TryCast(hugeint_t input, long double &result);\n \n template <>\n bool Hugeint::TryConvert(int8_t value, hugeint_t &result);\n@@ -160,5 +162,7 @@ template <>\n bool Hugeint::TryConvert(float value, hugeint_t &result);\n template <>\n bool Hugeint::TryConvert(double value, hugeint_t &result);\n+template <>\n+bool Hugeint::TryConvert(long double value, hugeint_t &result);\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/function/aggregate/sum_helpers.hpp b/src/include/duckdb/function/aggregate/sum_helpers.hpp\nindex 045f596e689c..4babf49c413f 100644\n--- a/src/include/duckdb/function/aggregate/sum_helpers.hpp\n+++ b/src/include/duckdb/function/aggregate/sum_helpers.hpp\n@@ -14,6 +14,47 @@\n \n namespace duckdb {\n \n+template <class T>\n+static inline void KahanAdd(T input, double &summed, double &err) {\n+\tdouble diff = input - err;\n+\tdouble newval = summed + diff;\n+\terr = (newval - summed) - diff;\n+\tsummed = newval;\n+}\n+\n+template <class T>\n+struct SumState {\n+\tbool isset;\n+\tT value;\n+\n+\tvoid Initialize() {\n+\t\tthis->isset = false;\n+\t}\n+\n+\tvoid Combine(const SumState<T> &other) {\n+\t\tthis->isset = other.isset || this->isset;\n+\t\tthis->value += other.value;\n+\t}\n+};\n+\n+template <>\n+struct SumState<double> {\n+\tbool isset;\n+\tdouble value;\n+\tdouble err;\n+\n+\tvoid Initialize() {\n+\t\tthis->isset = false;\n+\t\tthis->err = 0.0;\n+\t}\n+\n+\tvoid Combine(const SumState<double> &other) {\n+\t\tthis->isset = other.isset || this->isset;\n+\t\tKahanAdd(other.value, this->value, this->err);\n+\t\tKahanAdd(other.err, this->value, this->err);\n+\t}\n+};\n+\n struct RegularAdd {\n \ttemplate <class STATE, class T>\n \tstatic void AddNumber(STATE &state, T input) {\n@@ -26,6 +67,18 @@ struct RegularAdd {\n \t}\n };\n \n+struct DoubleAdd {\n+\ttemplate <class STATE, class T>\n+\tstatic void AddNumber(STATE &state, T input) {\n+\t\tKahanAdd(input, state.value, state.err);\n+\t}\n+\n+\ttemplate <class STATE, class T>\n+\tstatic void AddConstant(STATE &state, T input, idx_t count) {\n+\t\tKahanAdd(input * count, state.value, state.err);\n+\t}\n+};\n+\n struct HugeintAdd {\n \tstatic void AddValue(hugeint_t &result, uint64_t value, int positive) {\n \t\t// integer summation taken from Tim Gubner et al. - Efficient Query Processing\n",
  "test_patch": "diff --git a/test/sql/aggregate/aggregates/test_bigint_avg.test b/test/sql/aggregate/aggregates/test_bigint_avg.test\nnew file mode 100644\nindex 000000000000..5768f5daa609\n--- /dev/null\n+++ b/test/sql/aggregate/aggregates/test_bigint_avg.test\n@@ -0,0 +1,17 @@\n+# name: test/sql/aggregate/aggregates/test_bigint_avg.test\n+# description: Test AVG on integers with no exact float64 representation\n+# group: [aggregates]\n+\n+statement ok\n+CREATE TABLE bigints(n HUGEINT);\n+\n+statement ok\n+INSERT INTO bigints (n) VALUES ('9007199254740992'::HUGEINT), (1::HUGEINT), (0::HUGEINT);\n+\n+# this would give the wrong result with 'double' precision\n+require longdouble\n+\n+query R\n+SELECT AVG(n)::DOUBLE - '3002399751580331'::DOUBLE FROM bigints;\n+----\n+0\ndiff --git a/test/sql/aggregate/aggregates/test_kahan_avg.test b/test/sql/aggregate/aggregates/test_kahan_avg.test\nnew file mode 100644\nindex 000000000000..700d3ca21cb3\n--- /dev/null\n+++ b/test/sql/aggregate/aggregates/test_kahan_avg.test\n@@ -0,0 +1,15 @@\n+# name: test/sql/aggregate/aggregates/test_kahan_avg.test\n+# description: Test averages in which the intermediate sums are not exact\n+# group: [aggregates]\n+\n+statement ok\n+CREATE TABLE doubles(n DOUBLE);\n+\n+statement ok\n+INSERT INTO doubles (n) VALUES ('9007199254740992'::DOUBLE), (1::DOUBLE), (1::DOUBLE), (0::DOUBLE);\n+\n+# this would give the wrong result with a simple sum-and-divide\n+query R\n+SELECT AVG(n) - '2251799813685248.5'::DOUBLE FROM doubles;\n+----\n+0\ndiff --git a/test/sql/aggregate/aggregates/test_kahan_sum.test b/test/sql/aggregate/aggregates/test_kahan_sum.test\nnew file mode 100644\nindex 000000000000..911b0710cb0f\n--- /dev/null\n+++ b/test/sql/aggregate/aggregates/test_kahan_sum.test\n@@ -0,0 +1,15 @@\n+# name: test/sql/aggregate/aggregates/test_kahan_sum.test\n+# description: Test sums in which temporary results are not exact\n+# group: [aggregates]\n+\n+statement ok\n+CREATE TABLE doubles(n DOUBLE);\n+\n+statement ok\n+INSERT INTO doubles (n) VALUES ('9007199254740992'::DOUBLE), (1::DOUBLE), (1::DOUBLE), (0::DOUBLE);\n+\n+# this would give the wrong result with a simple sum\n+query I\n+SELECT SUM(n)::BIGINT FROM doubles;\n+----\n+9007199254740994\ndiff --git a/test/sqlite/test_sqllogictest.cpp b/test/sqlite/test_sqllogictest.cpp\nindex a50427abc0cb..1dab26bb4828 100644\n--- a/test/sqlite/test_sqllogictest.cpp\n+++ b/test/sqlite/test_sqllogictest.cpp\n@@ -51,6 +51,7 @@\n #include <vector>\n #include <iostream>\n #include <thread>\n+#include <cfloat>\n \n using namespace duckdb;\n using namespace std;\n@@ -1553,6 +1554,10 @@ void SQLLogicTestRunner::ExecuteFile(string script) {\n \t\t\t} else if (param == \"windows\") {\n #ifndef _WIN32\n \t\t\t\treturn;\n+#endif\n+\t\t\t} else if (param == \"longdouble\") {\n+#if LDBL_MANT_DIG < 54\n+\t\t\t\treturn;\n #endif\n \t\t\t} else if (param == \"noforcestorage\") {\n \t\t\t\tif (TestForceStorage()) {\n",
  "problem_statement": "`AVG` result has low precision\nFunction `avg` computes an average that might not be precise if dealing with a large amount of numbers or with numbers in different scales. I am guessing it might be calculating a simple sum with `double` types and dividing by the number of elements, which can make the results inaccurace.\r\n\r\nExample in R:\r\n```r\r\nlibrary(DBI)\r\nlibrary(duckdb)\r\ndf = data.frame(col1 = c(2^53, 1, 1, 0))\r\nconn = dbConnect(duckdb::duckdb(), dbdir=\":memory:\", read_only=FALSE)\r\nduckdb_register(conn, \"df\", df)\r\nres = dbGetQuery(conn, \"SELECT avg(col1) from df\")\r\nsprintf(\"%.2f\", res)\r\n```\r\n```\r\n[1] \"2251799813685248.00\"\r\n```\r\n\r\nCorrect result:\r\n```r\r\nsprintf(\"%.2f\", mean(df$col1))\r\n```\r\n```\r\n[1] \"2251799813685248.50\"\r\n```\n",
  "hints_text": "Ah, that's a fun one; you can obviously fix it by having the data frame sorted differently (i.e., smallest to largest) but there's got to be a good way to solve this in general; do other databases implement https://en.wikipedia.org/wiki/Kahan_summation_algorithm or some such thing, even at the cost of the extra additions? Or is this the sort of thing that Numpy/R get right and databases like Postgres don't?\nI'm not so sure what other DBs use. From what I've read, at least some of them use pairwise summation (https://en.wikipedia.org/wiki/Pairwise_summation), which is less exact that compensated summation but more exact than a plain sum-and-divide.\r\n\r\nAccording to that wikipedia article, numpy uses the same pairwise summation technique, but I find the codebase hard to navigate so cannot tell for sure it's still in use in 2021.\r\n\r\nThere's also other more precise methods that can be used - for example:\r\n```c++\r\ndouble calc_mean(double x[], size_t n)\r\n{\r\n    double out = 0;\r\n    for (size_t ix = 0; ix < n; ix++)\r\n        out += (x[ix] - out) / (double)(ix+1);\r\n    return out;\r\n}\r\n```\r\n(although in this particular example, neither this nor pairwise summation would give the correct result)\r\n\r\nAmong popular software, I guess R is an outlier as it does it by a compensated summation in long double precision, plus adding the compensated error term divided by the number elements to the results, which is the most accurate method I know of but slower than others.\r\n\r\nIn any case, I think a compensated summation in regular double precision such as Kahan's or the other examples in wikipedia would be appropriate for `SUM` and `AVG`.\nPairwise summation would be difficult with our implementation (it essentially turns it into a holistic aggregate - where all data must be stored and read at once) but Kahan should be feasible. The question is simply whether it is worth the computational tradeoff.\r\n\r\nThe code also has a reference to [this paper](http://ic.ese.upenn.edu/pdf/parallel_fpaccum_tc2016.pdf), but that seems like overkill for now.\n> Pairwise summation would be difficult with our implementation (it essentially turns it into a holistic aggregate - where all data must be stored and read at once) but Kahan should be feasible. The question is simply whether it is worth the computational tradeoff.\r\n\r\nAgreed; it's roughly 4x more computation to do it, right? Was curious if this was a thing e.g. Postgres chose to handle explicitly, and found they are doing something even more expensive than Kahan, apparently (?!) https://github.com/postgres/postgres/blob/3ed2005ff595d349276e5b2edeca1a8100b08c87/src/backend/utils/adt/float.c#L2723 so maybe Kahan is worth considering.\nI believe we are using Youngs-Cramer for `VAR(_P)`. Unlike PG, however, we do not use it for simple accumulation. Quite apart from the additional computation overhead, it looks to me like PG can fail with overflow of `Sxx` even when `Sx` is fine.\nI know of a 'fairly expensive' technique that performs summation with perfect accuracy. It involves essentially keeping a separate integer counter for each exponent in the IEEE-754 floating point format. In Python-ish pseudocode:\r\n\r\n```python\r\ninfinity = 0\r\naggregates = [0] * 2048 # 11-bit exponent = 2048 possible exponents, 64-bit signed ints, 16 KiB aggregate state\r\nfor val in values:\r\n    sign, exponent, mantissa = parse_double(val)\r\n\r\n    bucket = exponent + EXPONENT_BIAS\r\n    aggregates[bucket] += sign * mantissa\r\n\r\n    # This branch is ~1/512.\r\n    if UNLIKELY(abs(aggregates[bucket]) >= 2**62):\r\n        if bucket == 1023:\r\n            return infinity * sign(aggregates[1023])\r\n        else:\r\n            next_bucket = min(1023, bucket + 62)\r\n            reduction = 1 << (next_bucket - bucket)\r\n\r\n            if aggregates[bucket] > 0:\r\n                aggregates[bucket] -= reduction\r\n                aggregates[next_bucket] += 1\r\n            else:\r\n                aggregates[bucket] += reduction\r\n                aggregates[next_bucket] -= 1\r\n\r\n\r\nreturn sum(agg * 2**(exp - EXPONENT_BIAS)\r\n           for exp, agg in enumerate(aggregates))\r\n```\r\n\r\nI actually think that in the grand scheme of things this really isn't all that expensive (at least in the number of instructions, 16KiB aggregate state might form cache pressure when doing many aggregates simultaneously), and being able to guarantee aggregates with perfect accuracy is quite valuable for some applications.\r\n\n> I know of a 'fairly expensive' technique that performs summation with perfect accuracy. It involves essentially keeping a separate integer counter for each exponent in the IEEE-754 floating point format. In Python-ish pseudocode:\r\n> \r\n> ```python\r\n> infinity = 0\r\n> aggregates = [0] * 2048 # 11-bit exponent = 2048 possible exponents, 64-bit signed ints, 16 KiB aggregate state\r\n> for val in values:\r\n>     sign, exponent, mantissa = parse_double(val)\r\n> \r\n>     bucket = exponent + EXPONENT_BIAS\r\n>     aggregates[bucket] += sign * mantissa\r\n> \r\n>     # This branch is ~1/512.\r\n>     if UNLIKELY(abs(aggregates[bucket]) >= 2**62):\r\n>         if bucket == 1023:\r\n>             return infinity * sign(aggregates[1023])\r\n>         else:\r\n>             next_bucket = min(1023, bucket + 62)\r\n>             aggregates[next_bucket] += 1\r\n> \r\n>             reduction = 1 << (next_bucket - bucket)\r\n>             if aggregates[bucket] > 0:\r\n>                 aggregates[bucket] -= reduction\r\n>             else:\r\n>                 aggregates[bucket] += reduction\r\n> \r\n> \r\n> return sum(agg * 2**(exp - EXPONENT_BIAS)\r\n>            for exp, agg in enumerate(aggregates))\r\n> ```\r\n> \r\n> I actually think that in the grand scheme of things this really isn't all that expensive (at least in the number of instructions, 16KiB aggregate state might form cache pressure when doing many aggregates simultaneously), and being able to guarantee aggregates with perfect accuracy is quite valuable for some applications.\r\n\r\nLooks quite good, but indeed very heavy on memory usage, which would make this impossible to use in scenarios where you have many groups (e.g. for 1M groups you are already looking at 15GB of memory usage). Perhaps we could provide this as an alternative aggregate (`accurate_fp_sum` or so).\nIf there's interest I can do a bit of a literature search tomorrow, I recall that there are other techniques that use less space at the cost of having near-perfect accuracy rather than perfect accuracy. I just remembered this one because it's 'stupidly simple' and works.\nOne issue that slips under the radar with new aggregate functions is the way memory usage grows when you try to use segment trees for moving aggregates with large state. So one can implement an alternative function like `accurate_fp_sum`, with large state, but it should include a moving window implementation that updates the bins by adding and removing values.",
  "created_at": "2021-10-30T00:42:26Z"
}