{
  "repo": "duckdb/duckdb",
  "pull_number": 10822,
  "instance_id": "duckdb__duckdb-10822",
  "issue_numbers": [
    "10803",
    "10803"
  ],
  "base_commit": "cd98e7da6fac0f0ec6d26a1a1331e9025c7a5a07",
  "patch": "diff --git a/src/storage/table/list_column_data.cpp b/src/storage/table/list_column_data.cpp\nindex b805497a596c..418a4bd4644e 100644\n--- a/src/storage/table/list_column_data.cpp\n+++ b/src/storage/table/list_column_data.cpp\n@@ -303,6 +303,7 @@ void ListColumnData::FetchRow(TransactionData transaction, ColumnFetchState &sta\n }\n \n void ListColumnData::CommitDropColumn() {\n+\tColumnData::CommitDropColumn();\n \tvalidity.CommitDropColumn();\n \tchild_column->CommitDropColumn();\n }\n",
  "test_patch": "diff --git a/test/sql/storage/reclaim_space/reclaim_space_lists.test_slow b/test/sql/storage/reclaim_space/reclaim_space_lists.test_slow\nnew file mode 100644\nindex 000000000000..e84f14265829\n--- /dev/null\n+++ b/test/sql/storage/reclaim_space/reclaim_space_lists.test_slow\n@@ -0,0 +1,52 @@\n+# name: test/sql/storage/reclaim_space/reclaim_space_lists.test_slow\n+# description: Test that we reclaim space of LIST tables\n+# group: [reclaim_space]\n+\n+load __TEST_DIR__/test_reclaim_space.db\n+\n+statement ok\n+PRAGMA force_checkpoint;\n+\n+statement ok\n+SET force_compression='uncompressed'\n+\n+statement ok\n+CREATE TABLE lists AS SELECT [i] l FROM range(10000000) tbl(i);\n+\n+statement ok\n+CHECKPOINT;\n+\n+statement ok\n+CHECKPOINT;\n+\n+query III\n+SELECT MIN(l[1]), MAX(l[1]), COUNT(*) FROM lists\n+----\n+0\t9999999\t10000000\n+\n+statement ok\n+DROP TABLE lists;\n+\n+loop i 0 10\n+\n+statement ok\n+CREATE TABLE lists${i} AS SELECT [i] l FROM range(10000000) tbl(i);\n+\n+query III\n+SELECT MIN(l[1]), MAX(l[1]), COUNT(*) FROM lists${i}\n+----\n+0\t9999999\t10000000\n+\n+statement ok\n+CHECKPOINT;\n+\n+statement ok\n+DROP TABLE lists${i}\n+\n+statement ok\n+CHECKPOINT\n+\n+query I nosort expected_blocks\n+select round(total_blocks / 100.0) from pragma_database_size();\n+\n+endloop\n",
  "problem_statement": "Lists leak disk space in persistent databases.\n### What happens?\n\nUsing lists of various data types seems to be leaking disk space when using persistent DBs.\r\n\r\nWhen a table containing lists is created and immediately dropped, not all disk space seems to be freed. This can be reproduced by doing a simple script which repeatedly creates table with some dummy data and immediately drops it. (See the reproduction steps for the complete script). The script produces a csv [sizes.csv](https://github.com/duckdb/duckdb/files/14374387/sizes.csv) with the evolution of the size of the DB file and the size of the WAL.\r\n\r\nWhen plotted, the sizes produce the following plot:\r\n\r\n![sizes](https://github.com/duckdb/duckdb/assets/58769376/af54b726-d0b7-45a8-8c36-8e6cd00e61d2)\r\n\r\nIt seems that every now and then, some disk space gets claimed if a table contains a list of things. More over, the frequency of the disk claims is the same as the frequency of WAL syncs. However, if the table contains just regular text column, the file size stays the same.\r\n\r\nAfter the script is finished, we end up with databases which are empty (contain no tables), but have several megabytes each. On my machine, this resulted in the following sizes:\r\n\r\n| db | size|\r\n|-|-|\r\n| lists_of_ints.duckdb | 2.6M |\r\n| lists_of_texts.duckdb | 7.3M |\r\n| plain_texts.duckdb | 268K |\r\n\r\nThis makes working with long-lived persistent databases somewhat difficult. \r\n\r\nAs a workaround a the issue can be sidestepped by storing a JSON instead of a list. JSONs do not seem to suffer from this problem, so if one has a datatype serializable to JSON and back it can be used as an intermediate storage format.\r\n\n\n### To Reproduce\n\nUse the following script to obtain empty dabatase files of _very_ non-empty sizes; and a csv with the progress of said sizes.\r\n\r\n```python\r\nimport csv\r\nfrom pathlib import Path\r\n\r\nimport duckdb\r\n\r\nREPEATS = 100_000\r\n\r\nlists_of_texts_db_path = Path(\"lists_of_texts.duckdb\")\r\nlists_of_texts_wal_path = Path(\"lists_of_texts.duckdb.wal\")\r\nlists_of_texts_db_path.unlink(missing_ok=True)\r\nlists_of_texts_wal_path.unlink(missing_ok=True)\r\nlists_of_texts_db_conn = duckdb.connect(str(lists_of_texts_db_path))\r\n\r\nlists_of_ints_db_path = Path(\"lists_of_ints.duckdb\")\r\nlists_of_ints_wal_path = Path(\"lists_of_ints.duckdb.wal\")\r\nlists_of_ints_db_path.unlink(missing_ok=True)\r\nlists_of_ints_wal_path.unlink(missing_ok=True)\r\nlists_of_ints_db_conn = duckdb.connect(str(lists_of_ints_db_path))\r\n\r\nplain_texts_db_path = Path(\"plain_texts.duckdb\")\r\nplain_texts_wal_path = Path(\"plain_texts.duckdb.wal\")\r\nplain_texts_db_path.unlink(missing_ok=True)\r\nplain_texts_wal_path.unlink(missing_ok=True)\r\nplain_texts_db_conn = duckdb.connect(str(plain_texts_db_path))\r\n\r\nwith Path(\"sizes.csv\").open(\"w\", newline=\"\") as size_csv:\r\n    size_writer = csv.writer(size_csv)\r\n    size_writer.writerow([\"iteration\", \"kind\", \"size\"])\r\n\r\n    for i in range(REPEATS):\r\n        lists_of_texts_db_conn.execute(f\"create table foo_{i} as select [uuid()]::text[] from generate_series(1, 100);\")\r\n        lists_of_texts_db_conn.execute(f\"drop table foo_{i};\")\r\n        size_writer.writerow([i, \"lists_of_texts_db\", lists_of_texts_db_path.stat().st_size])\r\n        size_writer.writerow([i, \"lists_of_texts_wal\", lists_of_texts_wal_path.stat().st_size])\r\n\r\n        lists_of_ints_db_conn.execute(f\"create table foo_{i} as select [random()]::int[] from generate_series(1, 100);\")\r\n        lists_of_ints_db_conn.execute(f\"drop table foo_{i};\")\r\n        size_writer.writerow([i, \"lists_of_ints_db\", lists_of_ints_db_path.stat().st_size])\r\n        size_writer.writerow([i, \"lists_of_ints_wal\", lists_of_ints_wal_path.stat().st_size])\r\n\r\n        plain_texts_db_conn.execute(f\"create table foo_{i} as select uuid()::text from generate_series(1, 100);\")\r\n        plain_texts_db_conn.execute(f\"drop table foo_{i};\")\r\n        size_writer.writerow([i, \"plain_texts_db\", plain_texts_db_path.stat().st_size])\r\n        size_writer.writerow([i, \"plain_texts_wal\", plain_texts_wal_path.stat().st_size])\r\n```\n\n### OS:\n\nUbuntu x64 in WSL on Windows 11\n\n### DuckDB Version:\n\n0.10.0\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nM\u00edma Hlav\u00e1\u010dek\n\n### Affiliation:\n\nBlindspot.ai\n\n### Have you tried this on the latest [nightly build](https://duckdb.org/docs/installation/?version=main)?\n\nI have tested with a release build (and could not test with a nightly build)\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] Yes, I have\nLists leak disk space in persistent databases.\n### What happens?\n\nUsing lists of various data types seems to be leaking disk space when using persistent DBs.\r\n\r\nWhen a table containing lists is created and immediately dropped, not all disk space seems to be freed. This can be reproduced by doing a simple script which repeatedly creates table with some dummy data and immediately drops it. (See the reproduction steps for the complete script). The script produces a csv [sizes.csv](https://github.com/duckdb/duckdb/files/14374387/sizes.csv) with the evolution of the size of the DB file and the size of the WAL.\r\n\r\nWhen plotted, the sizes produce the following plot:\r\n\r\n![sizes](https://github.com/duckdb/duckdb/assets/58769376/af54b726-d0b7-45a8-8c36-8e6cd00e61d2)\r\n\r\nIt seems that every now and then, some disk space gets claimed if a table contains a list of things. More over, the frequency of the disk claims is the same as the frequency of WAL syncs. However, if the table contains just regular text column, the file size stays the same.\r\n\r\nAfter the script is finished, we end up with databases which are empty (contain no tables), but have several megabytes each. On my machine, this resulted in the following sizes:\r\n\r\n| db | size|\r\n|-|-|\r\n| lists_of_ints.duckdb | 2.6M |\r\n| lists_of_texts.duckdb | 7.3M |\r\n| plain_texts.duckdb | 268K |\r\n\r\nThis makes working with long-lived persistent databases somewhat difficult. \r\n\r\nAs a workaround a the issue can be sidestepped by storing a JSON instead of a list. JSONs do not seem to suffer from this problem, so if one has a datatype serializable to JSON and back it can be used as an intermediate storage format.\r\n\n\n### To Reproduce\n\nUse the following script to obtain empty dabatase files of _very_ non-empty sizes; and a csv with the progress of said sizes.\r\n\r\n```python\r\nimport csv\r\nfrom pathlib import Path\r\n\r\nimport duckdb\r\n\r\nREPEATS = 100_000\r\n\r\nlists_of_texts_db_path = Path(\"lists_of_texts.duckdb\")\r\nlists_of_texts_wal_path = Path(\"lists_of_texts.duckdb.wal\")\r\nlists_of_texts_db_path.unlink(missing_ok=True)\r\nlists_of_texts_wal_path.unlink(missing_ok=True)\r\nlists_of_texts_db_conn = duckdb.connect(str(lists_of_texts_db_path))\r\n\r\nlists_of_ints_db_path = Path(\"lists_of_ints.duckdb\")\r\nlists_of_ints_wal_path = Path(\"lists_of_ints.duckdb.wal\")\r\nlists_of_ints_db_path.unlink(missing_ok=True)\r\nlists_of_ints_wal_path.unlink(missing_ok=True)\r\nlists_of_ints_db_conn = duckdb.connect(str(lists_of_ints_db_path))\r\n\r\nplain_texts_db_path = Path(\"plain_texts.duckdb\")\r\nplain_texts_wal_path = Path(\"plain_texts.duckdb.wal\")\r\nplain_texts_db_path.unlink(missing_ok=True)\r\nplain_texts_wal_path.unlink(missing_ok=True)\r\nplain_texts_db_conn = duckdb.connect(str(plain_texts_db_path))\r\n\r\nwith Path(\"sizes.csv\").open(\"w\", newline=\"\") as size_csv:\r\n    size_writer = csv.writer(size_csv)\r\n    size_writer.writerow([\"iteration\", \"kind\", \"size\"])\r\n\r\n    for i in range(REPEATS):\r\n        lists_of_texts_db_conn.execute(f\"create table foo_{i} as select [uuid()]::text[] from generate_series(1, 100);\")\r\n        lists_of_texts_db_conn.execute(f\"drop table foo_{i};\")\r\n        size_writer.writerow([i, \"lists_of_texts_db\", lists_of_texts_db_path.stat().st_size])\r\n        size_writer.writerow([i, \"lists_of_texts_wal\", lists_of_texts_wal_path.stat().st_size])\r\n\r\n        lists_of_ints_db_conn.execute(f\"create table foo_{i} as select [random()]::int[] from generate_series(1, 100);\")\r\n        lists_of_ints_db_conn.execute(f\"drop table foo_{i};\")\r\n        size_writer.writerow([i, \"lists_of_ints_db\", lists_of_ints_db_path.stat().st_size])\r\n        size_writer.writerow([i, \"lists_of_ints_wal\", lists_of_ints_wal_path.stat().st_size])\r\n\r\n        plain_texts_db_conn.execute(f\"create table foo_{i} as select uuid()::text from generate_series(1, 100);\")\r\n        plain_texts_db_conn.execute(f\"drop table foo_{i};\")\r\n        size_writer.writerow([i, \"plain_texts_db\", plain_texts_db_path.stat().st_size])\r\n        size_writer.writerow([i, \"plain_texts_wal\", plain_texts_wal_path.stat().st_size])\r\n```\n\n### OS:\n\nUbuntu x64 in WSL on Windows 11\n\n### DuckDB Version:\n\n0.10.0\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nM\u00edma Hlav\u00e1\u010dek\n\n### Affiliation:\n\nBlindspot.ai\n\n### Have you tried this on the latest [nightly build](https://duckdb.org/docs/installation/?version=main)?\n\nI have tested with a release build (and could not test with a nightly build)\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] Yes, I have\n",
  "hints_text": "Hi @michael-hlavacek, thanks for reporting this with a detailed reproducer. Currently `DROP` does not necessarily reclaim space, see the documentation: https://duckdb.org/docs/sql/statements/drop#limitations-on-reclaiming-disk-space\r\nThat said, I'll keep this issue open for future investigations.\nWhat I get from the docs is that the regions get marked as free space, which gets potentially reused later. This is what happens in all usual cases. However, in the reproduction case with lists we have data of constant size which get dropped and re-inserted. I have cases in our app where an empty DB grows to over 1GB in size even though we keep rotating a couple of kB worth of data. Which leads me to believe that what happens underneath is something else than a simple free region marking.\nThanks for clarifying, we'll take a look!\nThanks very much! Just wanted to add that I _really_ appreciate the work you all are putting into DuckDB \ud83e\udd86 \ud83e\udd47 \u2764\ufe0f\nHi @michael-hlavacek, thanks for reporting this with a detailed reproducer. Currently `DROP` does not necessarily reclaim space, see the documentation: https://duckdb.org/docs/sql/statements/drop#limitations-on-reclaiming-disk-space\r\nThat said, I'll keep this issue open for future investigations.\nWhat I get from the docs is that the regions get marked as free space, which gets potentially reused later. This is what happens in all usual cases. However, in the reproduction case with lists we have data of constant size which get dropped and re-inserted. I have cases in our app where an empty DB grows to over 1GB in size even though we keep rotating a couple of kB worth of data. Which leads me to believe that what happens underneath is something else than a simple free region marking.\nThanks for clarifying, we'll take a look!\nThanks very much! Just wanted to add that I _really_ appreciate the work you all are putting into DuckDB \ud83e\udd86 \ud83e\udd47 \u2764\ufe0f",
  "created_at": "2024-02-23T13:38:36Z"
}