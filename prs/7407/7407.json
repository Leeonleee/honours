{
  "repo": "duckdb/duckdb",
  "pull_number": 7407,
  "instance_id": "duckdb__duckdb-7407",
  "issue_numbers": [
    "6738"
  ],
  "base_commit": "d467627253231dbcd4e70b6245709106351d5811",
  "patch": "diff --git a/src/execution/operator/persistent/physical_insert.cpp b/src/execution/operator/persistent/physical_insert.cpp\nindex fc9eb521972e..82eb031b9157 100644\n--- a/src/execution/operator/persistent/physical_insert.cpp\n+++ b/src/execution/operator/persistent/physical_insert.cpp\n@@ -101,7 +101,9 @@ class InsertLocalState : public LocalSinkState {\n \tunique_ptr<RowGroupCollection> local_collection;\n \toptional_ptr<OptimisticDataWriter> writer;\n \t// Rows that have been updated by a DO UPDATE conflict\n-\tunordered_set<row_t> updated_rows;\n+\tunordered_set<row_t> updated_global_rows;\n+\t// Rows in the transaction-local storage that have been updated by a DO UPDATE conflict\n+\tunordered_set<row_t> updated_local_rows;\n \tidx_t update_count = 0;\n };\n \n@@ -177,8 +179,11 @@ void CheckOnConflictCondition(ExecutionContext &context, DataChunk &conflicts, c\n \tresult.SetCardinality(conflicts.size());\n }\n \n-void PhysicalInsert::CombineExistingAndInsertTuples(DataChunk &result, DataChunk &scan_chunk, DataChunk &input_chunk,\n-                                                    ClientContext &client) const {\n+static void CombineExistingAndInsertTuples(DataChunk &result, DataChunk &scan_chunk, DataChunk &input_chunk,\n+                                           ClientContext &client, const PhysicalInsert &op) {\n+\tauto &types_to_fetch = op.types_to_fetch;\n+\tauto &insert_types = op.insert_types;\n+\n \tif (types_to_fetch.empty()) {\n \t\t// We have not scanned the initial table, so we can just duplicate the initial chunk\n \t\tresult.Initialize(client, input_chunk.GetTypes());\n@@ -218,14 +223,12 @@ void PhysicalInsert::CombineExistingAndInsertTuples(DataChunk &result, DataChunk\n \tresult.SetCardinality(input_chunk.size());\n }\n \n-idx_t PhysicalInsert::PerformOnConflictAction(ExecutionContext &context, DataChunk &chunk, TableCatalogEntry &table,\n-                                              Vector &row_ids) const {\n-\tif (action_type == OnConflictAction::NOTHING) {\n-\t\treturn 0;\n-\t}\n-\n-\tDataChunk update_chunk; // contains only the to-update columns\n+static void CreateUpdateChunk(ExecutionContext &context, DataChunk &chunk, TableCatalogEntry &table, Vector &row_ids,\n+                              DataChunk &update_chunk, const PhysicalInsert &op) {\n \n+\tauto &do_update_condition = op.do_update_condition;\n+\tauto &set_types = op.set_types;\n+\tauto &set_expressions = op.set_expressions;\n \t// Check the optional condition for the DO UPDATE clause, to filter which rows will be updated\n \tif (do_update_condition) {\n \t\tDataChunk do_update_filter_result;\n@@ -256,19 +259,43 @@ idx_t PhysicalInsert::PerformOnConflictAction(ExecutionContext &context, DataChu\n \tExpressionExecutor executor(context.client, set_expressions);\n \texecutor.Execute(chunk, update_chunk);\n \tupdate_chunk.SetCardinality(chunk);\n+}\n+\n+template <bool GLOBAL>\n+static idx_t PerformOnConflictAction(ExecutionContext &context, DataChunk &chunk, TableCatalogEntry &table,\n+                                     Vector &row_ids, const PhysicalInsert &op) {\n+\n+\tif (op.action_type == OnConflictAction::NOTHING) {\n+\t\treturn 0;\n+\t}\n+\tauto &set_columns = op.set_columns;\n+\n+\tDataChunk update_chunk;\n+\tCreateUpdateChunk(context, chunk, table, row_ids, update_chunk, op);\n \n \tauto &data_table = table.GetStorage();\n \t// Perform the update, using the results of the SET expressions\n-\tdata_table.Update(table, context.client, row_ids, set_columns, update_chunk);\n+\tif (GLOBAL) {\n+\t\tdata_table.Update(table, context.client, row_ids, set_columns, update_chunk);\n+\t} else {\n+\t\tauto &local_storage = LocalStorage::Get(context.client, data_table.db);\n+\t\t// Perform the update, using the results of the SET expressions\n+\t\tlocal_storage.Update(data_table, row_ids, set_columns, update_chunk);\n+\t}\n \treturn update_chunk.size();\n }\n \n // TODO: should we use a hash table to keep track of this instead?\n-void PhysicalInsert::RegisterUpdatedRows(InsertLocalState &lstate, const Vector &row_ids, idx_t count) const {\n+template <bool GLOBAL>\n+static void RegisterUpdatedRows(InsertLocalState &lstate, const Vector &row_ids, idx_t count) {\n \t// Insert all rows, if any of the rows has already been updated before, we throw an error\n \tauto data = FlatVector::GetData<row_t>(row_ids);\n+\n+\t// The rowids in the transaction-local ART aren't final yet so we have to separately keep track of the two sets of\n+\t// rowids\n+\tunordered_set<row_t> &updated_rows = GLOBAL ? lstate.updated_global_rows : lstate.updated_local_rows;\n \tfor (idx_t i = 0; i < count; i++) {\n-\t\tauto result = lstate.updated_rows.insert(data[i]);\n+\t\tauto result = updated_rows.insert(data[i]);\n \t\tif (result.second == false) {\n \t\t\tthrow InvalidInputException(\n \t\t\t    \"ON CONFLICT DO UPDATE can not update the same row twice in the same command, Ensure that no rows \"\n@@ -277,20 +304,25 @@ void PhysicalInsert::RegisterUpdatedRows(InsertLocalState &lstate, const Vector\n \t}\n }\n \n-idx_t PhysicalInsert::OnConflictHandling(TableCatalogEntry &table, ExecutionContext &context,\n-                                         InsertLocalState &lstate) const {\n-\tauto &data_table = table.GetStorage();\n-\tif (action_type == OnConflictAction::THROW) {\n-\t\tdata_table.VerifyAppendConstraints(table, context.client, lstate.insert_chunk, nullptr);\n-\t\treturn 0;\n-\t}\n-\t// Check whether any conflicts arise, and if they all meet the conflict_target + condition\n-\t// If that's not the case - We throw the first error\n+template <bool GLOBAL>\n+static idx_t HandleInsertConflicts(TableCatalogEntry &table, ExecutionContext &context, InsertLocalState &lstate,\n+                                   DataTable &data_table, const PhysicalInsert &op) {\n+\tauto &types_to_fetch = op.types_to_fetch;\n+\tauto &on_conflict_condition = op.on_conflict_condition;\n+\tauto &conflict_target = op.conflict_target;\n+\tauto &columns_to_fetch = op.columns_to_fetch;\n+\n+\tauto &local_storage = LocalStorage::Get(context.client, data_table.db);\n \n \t// We either want to do nothing, or perform an update when conflicts arise\n \tConflictInfo conflict_info(conflict_target);\n \tConflictManager conflict_manager(VerifyExistenceType::APPEND, lstate.insert_chunk.size(), &conflict_info);\n-\tdata_table.VerifyAppendConstraints(table, context.client, lstate.insert_chunk, &conflict_manager);\n+\tif (GLOBAL) {\n+\t\tdata_table.VerifyAppendConstraints(table, context.client, lstate.insert_chunk, &conflict_manager);\n+\t} else {\n+\t\tDataTable::VerifyUniqueIndexes(local_storage.GetIndexes(data_table), context.client, lstate.insert_chunk,\n+\t\t                               &conflict_manager);\n+\t}\n \tconflict_manager.Finalize();\n \tif (conflict_manager.ConflictCount() == 0) {\n \t\t// No conflicts found, 0 updates performed\n@@ -309,18 +341,25 @@ idx_t PhysicalInsert::OnConflictHandling(TableCatalogEntry &table, ExecutionCont\n \tconflict_chunk.Slice(conflicts.Selection(), conflicts.Count());\n \tconflict_chunk.SetCardinality(conflicts.Count());\n \n+\t// Holds the pins for the fetched rows\n+\tunique_ptr<ColumnFetchState> fetch_state;\n \tif (!types_to_fetch.empty()) {\n \t\tD_ASSERT(scan_chunk.size() == 0);\n \t\t// When these values are required for the conditions or the SET expressions,\n \t\t// then we scan the existing table for the conflicting tuples, using the rowids\n \t\tscan_chunk.Initialize(context.client, types_to_fetch);\n-\t\tauto fetch_state = make_uniq<ColumnFetchState>();\n-\t\tauto &transaction = DuckTransaction::Get(context.client, table.catalog);\n-\t\tdata_table.Fetch(transaction, scan_chunk, columns_to_fetch, row_ids, conflicts.Count(), *fetch_state);\n+\t\tfetch_state = make_uniq<ColumnFetchState>();\n+\t\tif (GLOBAL) {\n+\t\t\tauto &transaction = DuckTransaction::Get(context.client, table.catalog);\n+\t\t\tdata_table.Fetch(transaction, scan_chunk, columns_to_fetch, row_ids, conflicts.Count(), *fetch_state);\n+\t\t} else {\n+\t\t\tlocal_storage.FetchChunk(data_table, row_ids, conflicts.Count(), columns_to_fetch, scan_chunk,\n+\t\t\t                         *fetch_state);\n+\t\t}\n \t}\n \n \t// Splice the Input chunk and the fetched chunk together\n-\tCombineExistingAndInsertTuples(combined_chunk, scan_chunk, conflict_chunk, context.client);\n+\tCombineExistingAndInsertTuples(combined_chunk, scan_chunk, conflict_chunk, context.client, op);\n \n \tif (on_conflict_condition) {\n \t\tDataChunk conflict_condition_result;\n@@ -338,14 +377,19 @@ idx_t PhysicalInsert::OnConflictHandling(TableCatalogEntry &table, ExecutionCont\n \t\t\t}\n \t\t\tcombined_chunk.Slice(sel.Selection(), sel.Count());\n \t\t\trow_ids.Slice(sel.Selection(), sel.Count());\n-\t\t\tdata_table.VerifyAppendConstraints(table, context.client, combined_chunk, nullptr);\n+\t\t\tif (GLOBAL) {\n+\t\t\t\tdata_table.VerifyAppendConstraints(table, context.client, combined_chunk, nullptr);\n+\t\t\t} else {\n+\t\t\t\tDataTable::VerifyUniqueIndexes(local_storage.GetIndexes(data_table), context.client,\n+\t\t\t\t                               lstate.insert_chunk, nullptr);\n+\t\t\t}\n \t\t\tthrow InternalException(\"The previous operation was expected to throw but didn't\");\n \t\t}\n \t}\n \n-\tRegisterUpdatedRows(lstate, row_ids, combined_chunk.size());\n+\tRegisterUpdatedRows<GLOBAL>(lstate, row_ids, combined_chunk.size());\n \n-\tidx_t updated_tuples = PerformOnConflictAction(context, combined_chunk, table, row_ids);\n+\tidx_t updated_tuples = PerformOnConflictAction<GLOBAL>(context, combined_chunk, table, row_ids, op);\n \n \t// Remove the conflicting tuples from the insert chunk\n \tSelectionVector sel_vec(lstate.insert_chunk.size());\n@@ -356,6 +400,23 @@ idx_t PhysicalInsert::OnConflictHandling(TableCatalogEntry &table, ExecutionCont\n \treturn updated_tuples;\n }\n \n+idx_t PhysicalInsert::OnConflictHandling(TableCatalogEntry &table, ExecutionContext &context,\n+                                         InsertLocalState &lstate) const {\n+\tauto &data_table = table.GetStorage();\n+\tif (action_type == OnConflictAction::THROW) {\n+\t\tdata_table.VerifyAppendConstraints(table, context.client, lstate.insert_chunk, nullptr);\n+\t\treturn 0;\n+\t}\n+\t// Check whether any conflicts arise, and if they all meet the conflict_target + condition\n+\t// If that's not the case - We throw the first error\n+\tidx_t updated_tuples = 0;\n+\tupdated_tuples += HandleInsertConflicts<true>(table, context, lstate, data_table, *this);\n+\t// Also check the transaction-local storage+ART so we can detect conflicts within this transaction\n+\tupdated_tuples += HandleInsertConflicts<false>(table, context, lstate, data_table, *this);\n+\n+\treturn updated_tuples;\n+}\n+\n SinkResultType PhysicalInsert::Sink(ExecutionContext &context, DataChunk &chunk, OperatorSinkInput &input) const {\n \tauto &gstate = input.global_state.Cast<InsertGlobalState>();\n \tauto &lstate = input.local_state.Cast<InsertLocalState>();\ndiff --git a/src/execution/operator/schema/physical_create_index.cpp b/src/execution/operator/schema/physical_create_index.cpp\nindex 470951dd3aaf..dd15e7075038 100644\n--- a/src/execution/operator/schema/physical_create_index.cpp\n+++ b/src/execution/operator/schema/physical_create_index.cpp\n@@ -17,7 +17,6 @@ PhysicalCreateIndex::PhysicalCreateIndex(LogicalOperator &op, TableCatalogEntry\n     : PhysicalOperator(PhysicalOperatorType::CREATE_INDEX, op.types, estimated_cardinality),\n       table(table_p.Cast<DuckTableEntry>()), info(std::move(info)),\n       unbound_expressions(std::move(unbound_expressions)) {\n-\tD_ASSERT(table_p.IsDuckTable());\n \t// convert virtual column ids to storage column ids\n \tfor (auto &column_id : column_ids) {\n \t\tstorage_ids.push_back(table.GetColumns().LogicalToPhysical(LogicalIndex(column_id)).index);\n@@ -136,6 +135,7 @@ SinkFinalizeType PhysicalCreateIndex::Finalize(Pipeline &pipeline, Event &event,\n \tauto &schema = table.schema;\n \tauto index_entry = schema.CreateIndex(context, *info, table).get();\n \tif (!index_entry) {\n+\t\tD_ASSERT(info->on_conflict == OnCreateConflict::IGNORE_ON_CONFLICT);\n \t\t// index already exists, but error ignored because of IF NOT EXISTS\n \t\treturn SinkFinalizeType::READY;\n \t}\ndiff --git a/src/include/duckdb/execution/operator/persistent/physical_insert.hpp b/src/include/duckdb/execution/operator/persistent/physical_insert.hpp\nindex 9f274c117832..ca4a3f0eb7c1 100644\n--- a/src/include/duckdb/execution/operator/persistent/physical_insert.hpp\n+++ b/src/include/duckdb/execution/operator/persistent/physical_insert.hpp\n@@ -115,10 +115,9 @@ class PhysicalInsert : public PhysicalOperator {\n \tvoid CombineExistingAndInsertTuples(DataChunk &result, DataChunk &scan_chunk, DataChunk &input_chunk,\n \t                                    ClientContext &client) const;\n \t//! Returns the amount of updated tuples\n+\tvoid CreateUpdateChunk(ExecutionContext &context, DataChunk &chunk, TableCatalogEntry &table, Vector &row_ids,\n+\t                       DataChunk &result) const;\n \tidx_t OnConflictHandling(TableCatalogEntry &table, ExecutionContext &context, InsertLocalState &lstate) const;\n-\tidx_t PerformOnConflictAction(ExecutionContext &context, DataChunk &chunk, TableCatalogEntry &table,\n-\t                              Vector &row_ids) const;\n-\tvoid RegisterUpdatedRows(InsertLocalState &lstate, const Vector &row_ids, idx_t count) const;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/storage/data_table.hpp b/src/include/duckdb/storage/data_table.hpp\nindex 568cade6fc5e..b7e9949f18a3 100644\n--- a/src/include/duckdb/storage/data_table.hpp\n+++ b/src/include/duckdb/storage/data_table.hpp\n@@ -193,6 +193,10 @@ class DataTable {\n \tvoid VerifyAppendConstraints(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,\n \t                             ConflictManager *conflict_manager = nullptr);\n \n+public:\n+\tstatic void VerifyUniqueIndexes(TableIndexList &indexes, ClientContext &context, DataChunk &chunk,\n+\t                                ConflictManager *conflict_manager);\n+\n private:\n \t//! Verify the new added constraints against current persistent&local data\n \tvoid VerifyNewConstraint(ClientContext &context, DataTable &parent, const BoundConstraint *constraint);\ndiff --git a/src/planner/expression_binder/index_binder.cpp b/src/planner/expression_binder/index_binder.cpp\nindex c5a220484fe8..e7af2b42753d 100644\n--- a/src/planner/expression_binder/index_binder.cpp\n+++ b/src/planner/expression_binder/index_binder.cpp\n@@ -40,7 +40,7 @@ BindResult IndexBinder::BindExpression(unique_ptr<ParsedExpression> &expr_ptr, i\n \t\t\t\tthrow InternalException(\"failed to replay CREATE INDEX statement - column id not found\");\n \t\t\t}\n \t\t\treturn BindResult(\n-\t\t\t    make_uniq<BoundColumnRefExpression>(col_ref.alias, col_type, ColumnBinding(0, col_id_idx)));\n+\t\t\t    make_uniq<BoundColumnRefExpression>(col_ref.GetColumnName(), col_type, ColumnBinding(0, col_id_idx)));\n \t\t}\n \t\treturn ExpressionBinder::BindExpression(expr_ptr, depth);\n \t}\ndiff --git a/src/storage/data_table.cpp b/src/storage/data_table.cpp\nindex 930445aae78b..78be26c1a6ce 100644\n--- a/src/storage/data_table.cpp\n+++ b/src/storage/data_table.cpp\n@@ -530,6 +530,75 @@ void DataTable::VerifyNewConstraint(ClientContext &context, DataTable &parent, c\n \tlocal_storage.VerifyNewConstraint(parent, *constraint);\n }\n \n+bool HasUniqueIndexes(TableIndexList &list) {\n+\tbool has_unique_index = false;\n+\tlist.Scan([&](Index &index) {\n+\t\tif (index.IsUnique()) {\n+\t\t\treturn has_unique_index = true;\n+\t\t\treturn true;\n+\t\t}\n+\t\treturn false;\n+\t});\n+\treturn has_unique_index;\n+}\n+\n+void DataTable::VerifyUniqueIndexes(TableIndexList &indexes, ClientContext &context, DataChunk &chunk,\n+                                    ConflictManager *conflict_manager) {\n+\t//! check whether or not the chunk can be inserted into the indexes\n+\tif (!conflict_manager) {\n+\t\t// Only need to verify that no unique constraints are violated\n+\t\tindexes.Scan([&](Index &index) {\n+\t\t\tif (!index.IsUnique()) {\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t\tindex.VerifyAppend(chunk);\n+\t\t\treturn false;\n+\t\t});\n+\t\treturn;\n+\t}\n+\n+\tD_ASSERT(conflict_manager);\n+\t// The conflict manager is only provided when a ON CONFLICT clause was provided to the INSERT statement\n+\n+\tidx_t matching_indexes = 0;\n+\tauto &conflict_info = conflict_manager->GetConflictInfo();\n+\t// First we figure out how many indexes match our conflict target\n+\t// So we can optimize accordingly\n+\tindexes.Scan([&](Index &index) {\n+\t\tmatching_indexes += conflict_info.ConflictTargetMatches(index);\n+\t\treturn false;\n+\t});\n+\tconflict_manager->SetMode(ConflictManagerMode::SCAN);\n+\tconflict_manager->SetIndexCount(matching_indexes);\n+\t// First we verify only the indexes that match our conflict target\n+\tunordered_set<Index *> checked_indexes;\n+\tindexes.Scan([&](Index &index) {\n+\t\tif (!index.IsUnique()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (conflict_info.ConflictTargetMatches(index)) {\n+\t\t\tindex.VerifyAppend(chunk, *conflict_manager);\n+\t\t\tchecked_indexes.insert(&index);\n+\t\t}\n+\t\treturn false;\n+\t});\n+\n+\tconflict_manager->SetMode(ConflictManagerMode::THROW);\n+\t// Then we scan the other indexes, throwing if they cause conflicts on tuples that were not found during\n+\t// the scan\n+\tindexes.Scan([&](Index &index) {\n+\t\tif (!index.IsUnique()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (checked_indexes.count(&index)) {\n+\t\t\t// Already checked this constraint\n+\t\t\treturn false;\n+\t\t}\n+\t\tindex.VerifyAppend(chunk, *conflict_manager);\n+\t\treturn false;\n+\t});\n+}\n+\n void DataTable::VerifyAppendConstraints(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,\n                                         ConflictManager *conflict_manager) {\n \tif (table.HasGeneratedColumns()) {\n@@ -548,6 +617,11 @@ void DataTable::VerifyAppendConstraints(TableCatalogEntry &table, ClientContext\n \t\t\tVerifyGeneratedExpressionSuccess(context, table, chunk, *bound_expression, col.Oid());\n \t\t}\n \t}\n+\n+\tif (HasUniqueIndexes(info->indexes)) {\n+\t\tVerifyUniqueIndexes(info->indexes, context, chunk, conflict_manager);\n+\t}\n+\n \tauto &constraints = table.GetConstraints();\n \tauto &bound_constraints = table.GetBoundConstraints();\n \tfor (idx_t i = 0; i < bound_constraints.size(); i++) {\n@@ -567,50 +641,7 @@ void DataTable::VerifyAppendConstraints(TableCatalogEntry &table, ClientContext\n \t\t\tbreak;\n \t\t}\n \t\tcase ConstraintType::UNIQUE: {\n-\t\t\t//! check whether or not the chunk can be inserted into the indexes\n-\t\t\tif (conflict_manager) {\n-\t\t\t\t// This is only provided when a ON CONFLICT clause was provided\n-\t\t\t\tidx_t matching_indexes = 0;\n-\t\t\t\tauto &conflict_info = conflict_manager->GetConflictInfo();\n-\t\t\t\t// First we figure out how many indexes match our conflict target\n-\t\t\t\t// So we can optimize accordingly\n-\t\t\t\tinfo->indexes.Scan([&](Index &index) {\n-\t\t\t\t\tmatching_indexes += conflict_info.ConflictTargetMatches(index);\n-\t\t\t\t\treturn false;\n-\t\t\t\t});\n-\t\t\t\tconflict_manager->SetMode(ConflictManagerMode::SCAN);\n-\t\t\t\tconflict_manager->SetIndexCount(matching_indexes);\n-\t\t\t\t// First we verify only the indexes that match our conflict target\n-\t\t\t\tinfo->indexes.Scan([&](Index &index) {\n-\t\t\t\t\tif (!index.IsUnique()) {\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\tif (conflict_info.ConflictTargetMatches(index)) {\n-\t\t\t\t\t\tindex.VerifyAppend(chunk, *conflict_manager);\n-\t\t\t\t\t}\n-\t\t\t\t\treturn false;\n-\t\t\t\t});\n-\n-\t\t\t\tconflict_manager->SetMode(ConflictManagerMode::THROW);\n-\t\t\t\t// Then we scan the other indexes, throwing if they cause conflicts on tuples that were not found during\n-\t\t\t\t// the scan\n-\t\t\t\tinfo->indexes.Scan([&](Index &index) {\n-\t\t\t\t\tif (!index.IsUnique()) {\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\tindex.VerifyAppend(chunk, *conflict_manager);\n-\t\t\t\t\treturn false;\n-\t\t\t\t});\n-\t\t\t} else {\n-\t\t\t\t// Only need to verify that no unique constraints are violated\n-\t\t\t\tinfo->indexes.Scan([&](Index &index) {\n-\t\t\t\t\tif (!index.IsUnique()) {\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\tindex.VerifyAppend(chunk);\n-\t\t\t\t\treturn false;\n-\t\t\t\t});\n-\t\t\t}\n+\t\t\t// These were handled earlier on\n \t\t\tbreak;\n \t\t}\n \t\tcase ConstraintType::FOREIGN_KEY: {\n",
  "test_patch": "diff --git a/test/fuzzer/pedro/buffer_manager_resize_issue.test b/test/fuzzer/pedro/buffer_manager_resize_issue.test\nindex 0ccf6de7120a..a4e554d94fe4 100644\n--- a/test/fuzzer/pedro/buffer_manager_resize_issue.test\n+++ b/test/fuzzer/pedro/buffer_manager_resize_issue.test\n@@ -32,4 +32,4 @@ INSERT INTO t2(c1,c0) VALUES (235,36),(43,81),(246,187),(28,149),(206,20),(135,1\n statement error\n INSERT INTO t2(c1,c0) VALUES (86,98),(96,107),(237,190),(253,242),(229,9),(6,147);\n ----\n-Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key\n\\ No newline at end of file\n+Constraint Error: Duplicate key \"c1: 6\" violates unique constraint\ndiff --git a/test/sql/index/art/art_issue_7349.test b/test/sql/index/art/art_issue_7349.test\nindex 845f30a23764..958bb4cbf650 100644\n--- a/test/sql/index/art/art_issue_7349.test\n+++ b/test/sql/index/art/art_issue_7349.test\n@@ -32,13 +32,12 @@ INSERT INTO td VALUES('2006-12-25');\n statement ok\n INSERT INTO tab0 VALUES('2006-12-25');\n \n-statement ok\n+statement error\n INSERT INTO td VALUES (date '2008-02-29');\n+Constraint Error: Failed to commit: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key \"2008-02-29\"\n \n-statement error\n+statement ok\n COMMIT TRANSACTION;\n-----\n-TransactionContext Error: Failed to commit: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key \"2008-02-29\"\n \n # three tables\n \n@@ -48,20 +47,17 @@ START TRANSACTION;\n statement ok\n INSERT INTO tab0 VALUES('2006-12-25');\n \n-statement ok\n+statement error\n INSERT INTO td VALUES (date '2008-02-29');\n+----\n+Constraint Error: Duplicate key \"tz: 2008-02-29\" violates unique constraint. If this is an unexpected constraint violation please double check with the known index limitations section in our documentation (docs - sql - indexes).\n \n statement ok\n-INSERT INTO td VALUES('2006-12-25');\n+COMMIT TRANSACTION;\n \n statement ok\n INSERT INTO tab1 VALUES('2006-12-25');\n \n-statement error\n-COMMIT TRANSACTION;\n-----\n-TransactionContext Error: Failed to commit: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key \"2008-02-29\"\n-\n # other table first\n \n statement ok\n@@ -70,13 +66,10 @@ START TRANSACTION;\n statement ok\n INSERT INTO tab0 VALUES('2006-12-25');\n \n-statement ok\n+statement error\n INSERT INTO td VALUES('2006-12-25');\n+----\n+Constraint Error: Duplicate key \"tz: 2006-12-25\" violates unique constraint. If this is an unexpected constraint violation please double check with the known index limitations section in our documentation (docs - sql - indexes).\n \n statement ok\n-INSERT INTO td VALUES (date '2008-02-29');\n-\n-statement error\n COMMIT TRANSACTION;\n-----\n-TransactionContext Error: Failed to commit: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key \"2008-02-29\"\n\\ No newline at end of file\ndiff --git a/test/sql/index/art/test_art_import_export.test b/test/sql/index/art/test_art_import_export.test\nindex 20476a18be2f..6bc4f881d5aa 100644\n--- a/test/sql/index/art/test_art_import_export.test\n+++ b/test/sql/index/art/test_art_import_export.test\n@@ -33,4 +33,4 @@ IMPORT DATABASE '__TEST_DIR__/export_index_db'\n statement error\n INSERT INTO raw VALUES (1, 1, 1, 1);\n ----\n-Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key\n\\ No newline at end of file\n+Constraint Error: Duplicate key \"customer_ID: 1, year: 1, month: 1\" violates unique constraint\ndiff --git a/test/sql/storage/test_unique_index_checkpoint.test b/test/sql/storage/test_unique_index_checkpoint.test\nindex 8a0868a98439..3638d129da12 100644\n--- a/test/sql/storage/test_unique_index_checkpoint.test\n+++ b/test/sql/storage/test_unique_index_checkpoint.test\n@@ -20,7 +20,7 @@ restart\n statement error\n INSERT INTO test VALUES (1,101),(2,201);\n ----\n-Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key\n+Constraint Error: Duplicate key \"i: 1\" violates unique constraint\n \n restart\n \n@@ -38,4 +38,4 @@ restart\n statement error\n INSERT INTO unique_index_test VALUES (1,101),(2,201);\n ----\n-Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key\n\\ No newline at end of file\n+Constraint Error: Duplicate key \"ordernumber: 1\" violates unique constraint\n\\ No newline at end of file\ndiff --git a/test/sql/storage/wal/wal_create_index.test b/test/sql/storage/wal/wal_create_index.test\nindex aafbbebddd75..d16b3562aad4 100644\n--- a/test/sql/storage/wal/wal_create_index.test\n+++ b/test/sql/storage/wal/wal_create_index.test\n@@ -45,7 +45,7 @@ logical_opt\t<REGEX>:.*INDEX_SCAN.*\n statement error\n INSERT INTO integers VALUES (1, 1);\n ----\n-Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key\n+Constraint Error: Duplicate key \"i: 1\" violates unique constraint\n \n restart\n \n@@ -97,7 +97,7 @@ SELECT i FROM integers WHERE i + j = 2\n statement error\n INSERT INTO integers VALUES (1, 1);\n ----\n-Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key\n+Constraint Error: Duplicate key \"(i + j): 2\" violates unique constraint\n \n statement ok\n DROP INDEX i_index;\n@@ -141,7 +141,7 @@ SELECT i FROM integers WHERE j + i = 2\n statement error\n INSERT INTO integers VALUES (1, 1);\n ----\n-Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key\n+Constraint Error: Duplicate key \"(j + i): 2\" violates unique constraint\n \n statement ok\n DROP INDEX i_index;\n@@ -164,7 +164,7 @@ restart\n statement error\n INSERT INTO integers VALUES (1, 1);\n ----\n-Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key\n+Constraint Error: Duplicate key \"(j + i): 2, j: 1, i: 1\" violates unique constraint\n \n statement ok\n DROP INDEX i_index;\n\\ No newline at end of file\ndiff --git a/test/sql/upsert/upsert_explicit_index.test b/test/sql/upsert/upsert_explicit_index.test\nnew file mode 100644\nindex 000000000000..02e1899e6ff8\n--- /dev/null\n+++ b/test/sql/upsert/upsert_explicit_index.test\n@@ -0,0 +1,23 @@\n+# name: test/sql/upsert/upsert_explicit_index.test\n+# group: [upsert]\n+\n+statement ok\n+pragma enable_verification;\n+\n+statement ok\n+create table tbl (i integer, j integer);\n+\n+statement ok\n+insert into tbl values (5,3), (3,2);\n+\n+statement ok\n+create unique index my_index on tbl(i);\n+\n+statement ok\n+insert into tbl values (5,2) on conflict (i) do update set j = 10;\n+\n+query II\n+select * from tbl;\n+----\n+5\t10\n+3\t2\ndiff --git a/test/sql/upsert/upsert_transaction.test b/test/sql/upsert/upsert_transaction.test\nnew file mode 100644\nindex 000000000000..a5e8ae3da477\n--- /dev/null\n+++ b/test/sql/upsert/upsert_transaction.test\n@@ -0,0 +1,128 @@\n+# name: test/sql/upsert/upsert_transaction.test\n+# group: [upsert]\n+\n+# This tests the desired behavior when UPSERT is used on a conflict that only exists within the transaction local storage.\n+# NOTE: This does not test behavior of conflicts that arise between transactions\n+\n+# DO UPDATE\n+\n+statement ok\n+BEGIN TRANSACTION;\n+\n+statement ok\n+CREATE TABLE tbl (a SHORT PRIMARY KEY, b SHORT);\n+\n+statement ok\n+INSERT INTO tbl VALUES (1, 2) ON CONFLICT (a) DO UPDATE SET b = excluded.b;\n+\n+# Create a conflict within the transaction\n+statement ok\n+INSERT INTO tbl VALUES (1, 3) ON CONFLICT (a) DO UPDATE SET b = excluded.b;\n+\n+query II\n+select * from tbl;\n+----\n+1\t3\n+\n+statement ok\n+COMMIT;\n+\n+query II\n+select * from tbl;\n+----\n+1\t3\n+\n+# DO NOTHING\n+\n+statement ok\n+BEGIN TRANSACTION;\n+\n+statement ok\n+INSERT INTO tbl VALUES\n+\t(2, 1),\n+\t(3, 1),\n+\t(4, 1);\n+\n+statement ok\n+INSERT INTO tbl VALUES\n+\t(2, 1),\n+\t(3, 1),\n+\t(4, 1) ON CONFLICT (a) DO NOTHING;\n+\n+statement ok\n+COMMIT;\n+\n+# Attempt to update the same row twice within the same UPSERT\n+statement ok\n+BEGIN TRANSACTION;\n+\n+statement ok\n+INSERT INTO tbl VALUES\n+\t(5, 0)\n+\n+statement error\n+insert into tbl VALUES\n+\t(5, 0),\n+\t(5, 1)\n+ON CONFLICT (a) DO UPDATE SET\n+\tb = excluded.b;\n+----\n+ON CONFLICT DO UPDATE can not update the same row twice in the same command, Ensure that no rows proposed for insertion within the same command have duplicate constrained values\n+\n+statement ok\n+COMMIT;\n+\n+# DO UPDATE 'affected_tuples' return value\n+\n+statement ok\n+BEGIN TRANSACTION;\n+\n+statement ok\n+INSERT INTO tbl VALUES (6,0);\n+\n+# Both of these inserts turn into updates\n+# The first affected tuple (5) is an update in the global storage\n+# The second affected tuple (6) is an update in the local storage\n+# The third affected tuple is an insert into the local storage\n+query I\n+INSERT INTO tbl VALUES (5,0), (6,0), (7,0) ON CONFLICT (a) DO UPDATE set b = excluded.b;\n+----\n+3\n+\n+# The only affected tuple is the insert into the local storage\n+# (5,0) causes a constraint error in the global storage\n+# (3,0) causes a constraint error in the local storage\n+query I\n+INSERT INTO tbl VALUES (-1, 0), (5,0), (6,0) ON CONFLICT (a) DO NOTHING;\n+----\n+1\n+\n+statement ok\n+COMMIT;\n+\n+# DO UPDATE > STANDARD_VECTOR_SIZE\n+\n+statement ok\n+BEGIN TRANSACTION;\n+\n+statement ok\n+CREATE OR REPLACE TABLE tbl (a SHORT PRIMARY KEY, b SHORT);\n+\n+statement ok\n+INSERT INTO tbl (select i, 0 from range(2500) tbl(i));\n+\n+query I\n+select max(b) from tbl;\n+----\n+0\n+\n+statement ok\n+INSERT INTO tbl (select i, i from range(2500) tbl(i)) ON CONFLICT (a) DO UPDATE SET b = excluded.b;\n+\n+query I\n+select max(b) from tbl;\n+----\n+2499\n+\n+statement ok\n+COMMIT;\n",
  "problem_statement": "Cannot UPSERT record created in the same transaction.  \n### What happens?\n\nWhen I create an entity and then try to upsert it in the same transaction, it results in `Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicated key`. When I try to upsert a record created in a different transaction, it succeeds. Expected behavior: upsert should succeed in both scenarios. \n\n### To Reproduce\n\nUpsert in the same transaction fails: \r\n```SQL\r\nCREATE TABLE test (a INTEGER PRIMARY KEY, b INTEGER);\r\nBEGIN;\r\nINSERT OR REPLACE INTO test (a, b) VALUES (1, 2);\r\nINSERT OR REPLACE INTO test (a, b) VALUES (1, 3);\r\n```\r\n```\r\nError: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicated key\r\n```\r\n\r\nBut if the record is created in another transaction, upsert succeeds: \r\n\r\n```SQL\r\nINSERT OR REPLACE INTO test (a, b) VALUES (1, 2);\r\nBEGIN;\r\nINSERT OR REPLACE INTO test (a, b) VALUES (1, 3);\r\nCOMMIT;\r\nSELECT * FROM test;\r\n```\r\n\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   a   \u2502   b   \u2502\r\n\u2502 int32 \u2502 int32 \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502     1 \u2502     3 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\n\n### OS:\n\nLinux\n\n### DuckDB Version:\n\nv0.7.1 b00b93f0b1\n\n### DuckDB Client:\n\nCLI\n\n### Full Name:\n\nStepan Anokhin\n\n### Affiliation:\n\nAtlan\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "Hi, thanks for raising the issue, but currently that's expected behavior.\r\nWe hope to extend the functionality of UPSERT later \ud83d\udc4d \nGot it! Thanks @Tishj! Is there a feature-request I can reference to? ",
  "created_at": "2023-05-08T13:37:15Z"
}