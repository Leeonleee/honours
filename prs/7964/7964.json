{
  "repo": "duckdb/duckdb",
  "pull_number": 7964,
  "instance_id": "duckdb__duckdb-7964",
  "issue_numbers": [
    "7937",
    "7937"
  ],
  "base_commit": "eefa9c921312ec3dabfbeae5367b39ded2724b98",
  "patch": "diff --git a/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_scan.hpp b/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_scan.hpp\nindex 5424cc15efc2..575cebb9f170 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_scan.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_scan.hpp\n@@ -9,7 +9,7 @@ struct PandasColumnBindData;\n \n struct NumpyScan {\n \tstatic void Scan(PandasColumnBindData &bind_data, idx_t count, idx_t offset, Vector &out);\n-\tstatic void ScanObjectColumn(PyObject **col, idx_t count, idx_t offset, Vector &out);\n+\tstatic void ScanObjectColumn(PyObject **col, idx_t stride, idx_t count, idx_t offset, Vector &out);\n };\n \n } // namespace duckdb\ndiff --git a/tools/pythonpkg/src/numpy/numpy_scan.cpp b/tools/pythonpkg/src/numpy/numpy_scan.cpp\nindex 7cbd68186c69..d1c420877fa9 100644\n--- a/tools/pythonpkg/src/numpy/numpy_scan.cpp\n+++ b/tools/pythonpkg/src/numpy/numpy_scan.cpp\n@@ -173,14 +173,21 @@ void VerifyTypeConstraints(Vector &vec, idx_t count) {\n \t}\n }\n \n-void NumpyScan::ScanObjectColumn(PyObject **col, idx_t count, idx_t offset, Vector &out) {\n+void NumpyScan::ScanObjectColumn(PyObject **col, idx_t stride, idx_t count, idx_t offset, Vector &out) {\n \t// numpy_col is a sequential list of objects, that make up one \"column\" (Vector)\n \tout.SetVectorType(VectorType::FLAT_VECTOR);\n-\t{\n-\t\tPythonGILWrapper gil; // We're creating python objects here, so we need the GIL\n+\tauto &mask = FlatVector::Validity(out);\n+\tPythonGILWrapper gil; // We're creating python objects here, so we need the GIL\n+\n+\tif (stride == sizeof(PyObject *)) {\n+\t\tauto src_ptr = col + offset;\n+\t\tfor (idx_t i = 0; i < count; i++) {\n+\t\t\tScanNumpyObject(src_ptr[i], i, out);\n+\t\t}\n+\t} else {\n \t\tfor (idx_t i = 0; i < count; i++) {\n-\t\t\tidx_t source_idx = offset + i;\n-\t\t\tScanNumpyObject(col[source_idx], i, out);\n+\t\t\tauto src_ptr = col[stride / sizeof(PyObject *) * (i + offset)];\n+\t\t\tScanNumpyObject(src_ptr, i, out);\n \t\t}\n \t}\n \tVerifyTypeConstraints(out, count);\n@@ -274,7 +281,7 @@ void NumpyScan::Scan(PandasColumnBindData &bind_data, idx_t count, idx_t offset,\n \t\t// Get the source pointer of the numpy array\n \t\tauto src_ptr = (PyObject **)array.data(); // NOLINT\n \t\tif (out.GetType().id() != LogicalTypeId::VARCHAR) {\n-\t\t\treturn NumpyScan::ScanObjectColumn(src_ptr, count, offset, out);\n+\t\t\treturn NumpyScan::ScanObjectColumn(src_ptr, numpy_col.stride, count, offset, out);\n \t\t}\n \n \t\t// Get the data pointer and the validity mask of the result vector\ndiff --git a/tools/pythonpkg/src/python_udf.cpp b/tools/pythonpkg/src/python_udf.cpp\nindex c7057b9c99e8..363bee7b7dd9 100644\n--- a/tools/pythonpkg/src/python_udf.cpp\n+++ b/tools/pythonpkg/src/python_udf.cpp\n@@ -194,9 +194,7 @@ static scalar_function_t CreateNativeFunction(PyObject *function, PythonExceptio\n \t\t\tpython_results.push_back(ret);\n \t\t}\n \n-\t\t// Cast the resulting native python to DuckDB, using the return type\n-\t\t// result.Resize(input.size());\n-\t\tNumpyScan::ScanObjectColumn(python_results.data(), input.size(), 0, result);\n+\t\tNumpyScan::ScanObjectColumn(python_results.data(), sizeof(PyObject *), input.size(), 0, result);\n \t\tif (input.size() == 1) {\n \t\t\tresult.SetVectorType(VectorType::CONSTANT_VECTOR);\n \t\t}\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py b/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py\nindex 5193b967b542..367bae598163 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py\n@@ -401,6 +401,35 @@ def test_double_object_conversion(self, pandas):\n         double_dtype = np.dtype('float64')\n         assert isinstance(converted_col['0'].dtype, double_dtype.__class__) == True\n \n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_numpy_object_with_stride(self, pandas):\n+        con = duckdb.connect()\n+        df = pandas.DataFrame(columns=[\"idx\", \"evens\", \"zeros\"])\n+\n+        df[\"idx\"] = list(range(10))\n+        for col in df.columns[1:]:\n+            df[col].values[:] = 0\n+\n+        counter = 0\n+        for i in range(10):\n+            df.loc[df[\"idx\"] == i, \"evens\"] += counter\n+            counter += 2\n+\n+        res = con.sql(\"select * from df\").fetchall()\n+        assert res == [\n+            (0, 0, 0),\n+            (1, 2, 0),\n+            (2, 4, 0),\n+            (3, 6, 0),\n+            (4, 8, 0),\n+            (5, 10, 0),\n+            (6, 12, 0),\n+            (7, 14, 0),\n+            (8, 16, 0),\n+            (9, 18, 0)\n+        ]\n+\n+\n     @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n     def test_numpy_stringliterals(self, pandas):\n         con = duckdb.connect()\n",
  "problem_statement": "Correctness issues reading unconsolidated Pandas DataFrames\n### What happens?\r\n\r\nWhen querying a Pandas DataFrame whose internal BlockManager has not \"consolidated\" its blocks, you get garbled results where values for columns within a block may be interleaved.\r\n\r\nThis was kind of elusive to track down because it depends on Pandas internals that I don't really understand, but it was encountered by one of our end users using duckdb against a manually constructed DataFrame. It's particularly tricky because the concept of \"blocks\" and \"consolidation\" doesn't appear to be part of the public API and consolidation can be triggered easily by a variety of operations - for example taking a shallow `.copy()` will consolidate and fix the issue (even merely accessing the `_values` property on a DataFrame triggers an in-place consolidation and stops the issue).\r\n\r\n### To Reproduce\r\n\r\nHere's a minimal reproduction I was able to isolate. I can't say I endorse this code - it's kind of a weird way to build a DataFrame, but it seems to be valid Pandas and it results in a value that duckdb is unable to read correctly.\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame(columns=[\"idx\", \"evens\", \"zeros\"])\r\n\r\ndf[\"idx\"] = list(range(10))\r\nfor col in df.columns[1:]:\r\n    df[col].values[:] = 0\r\n\r\ncounter = 0\r\nfor i in range(10):\r\n    df.loc[df[\"idx\"] == i, \"evens\"] += counter\r\n    counter += 2\r\n```\r\n\r\nPrinting this value looks as you would expect:\r\n\r\n```\r\n> print(df)\r\n   idx evens zeros\r\n0    0     0     0\r\n1    1     2     0\r\n2    2     4     0\r\n3    3     6     0\r\n4    4     8     0\r\n5    5    10     0\r\n6    6    12     0\r\n7    7    14     0\r\n8    8    16     0\r\n9    9    18     0\r\n```\r\n\r\nBut - ~~and this seems critical somehow~~ EDIT: actually see below - if you inspect the value of `df._mgr._is_consolidated` you will see that it is `False`. And querying the DataFrame with duckdb seems to interleave values of the `evens` and `zeros` columns:\r\n\r\n```python\r\nimport duckdb\r\n\r\nwith duckdb.connect(database=':memory:') as conn:\r\n    print(conn.execute(\"select * from df\").df())\r\n```\r\n\r\nwhich prints\r\n\r\n```\r\n   idx  evens  zeros\r\n0    0      0      0\r\n1    1      0      2\r\n2    2      2      0\r\n3    3      0      4\r\n4    4      4      0\r\n5    5      0      6\r\n6    6      6      0\r\n7    7      0      8\r\n8    8      8      0\r\n9    9      0     10\r\n```\r\n\r\n~~As I said anything that triggers \"consolidation\" stops the issue, e.g.~~\r\n\r\nEDIT: Actually see the comment below - I'm not sure consolidation is what if fixing per se. It turns out `.copy()` is deep by default and doing a shallow copy with `deep=False` produces a similarly broken DataFrame and even calling `df2._mgr._consolidate_inplace()` doesn't fix it - you get `df2._mgr._is_consolidated=True` after but the output of querying with duckdb is still mangled so the block consolidation stuff may be a red herring\r\n\r\n```python\r\ndf2 = df.copy()\r\nprint(df2._mgr._is_consolidated)\r\nwith duckdb.connect(database=':memory:') as conn:\r\n    print(conn.execute(\"select * from df2\").df())\r\n```\r\n\r\n\u27a1\ufe0f \r\n\r\n```\r\nTrue\r\n   idx  evens  zeros\r\n0    0      0      0\r\n1    1      2      0\r\n2    2      4      0\r\n3    3      6      0\r\n4    4      8      0\r\n5    5     10      0\r\n6    6     12      0\r\n7    7     14      0\r\n8    8     16      0\r\n9    9     18      0\r\n```\r\n\r\n### OS:\r\n\r\nmacOS, Linux\r\n\r\n### DuckDB Version:\r\n\r\n0.7.1, 0.8.1\r\n\r\n### DuckDB Client:\r\n\r\nPython\r\n\r\n### Full Name:\r\n\r\nDylan Scott\r\n\r\n### Affiliation:\r\n\r\nHex Technologies\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\n- [X] I agree\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] I agree\nCorrectness issues reading unconsolidated Pandas DataFrames\n### What happens?\r\n\r\nWhen querying a Pandas DataFrame whose internal BlockManager has not \"consolidated\" its blocks, you get garbled results where values for columns within a block may be interleaved.\r\n\r\nThis was kind of elusive to track down because it depends on Pandas internals that I don't really understand, but it was encountered by one of our end users using duckdb against a manually constructed DataFrame. It's particularly tricky because the concept of \"blocks\" and \"consolidation\" doesn't appear to be part of the public API and consolidation can be triggered easily by a variety of operations - for example taking a shallow `.copy()` will consolidate and fix the issue (even merely accessing the `_values` property on a DataFrame triggers an in-place consolidation and stops the issue).\r\n\r\n### To Reproduce\r\n\r\nHere's a minimal reproduction I was able to isolate. I can't say I endorse this code - it's kind of a weird way to build a DataFrame, but it seems to be valid Pandas and it results in a value that duckdb is unable to read correctly.\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame(columns=[\"idx\", \"evens\", \"zeros\"])\r\n\r\ndf[\"idx\"] = list(range(10))\r\nfor col in df.columns[1:]:\r\n    df[col].values[:] = 0\r\n\r\ncounter = 0\r\nfor i in range(10):\r\n    df.loc[df[\"idx\"] == i, \"evens\"] += counter\r\n    counter += 2\r\n```\r\n\r\nPrinting this value looks as you would expect:\r\n\r\n```\r\n> print(df)\r\n   idx evens zeros\r\n0    0     0     0\r\n1    1     2     0\r\n2    2     4     0\r\n3    3     6     0\r\n4    4     8     0\r\n5    5    10     0\r\n6    6    12     0\r\n7    7    14     0\r\n8    8    16     0\r\n9    9    18     0\r\n```\r\n\r\nBut - ~~and this seems critical somehow~~ EDIT: actually see below - if you inspect the value of `df._mgr._is_consolidated` you will see that it is `False`. And querying the DataFrame with duckdb seems to interleave values of the `evens` and `zeros` columns:\r\n\r\n```python\r\nimport duckdb\r\n\r\nwith duckdb.connect(database=':memory:') as conn:\r\n    print(conn.execute(\"select * from df\").df())\r\n```\r\n\r\nwhich prints\r\n\r\n```\r\n   idx  evens  zeros\r\n0    0      0      0\r\n1    1      0      2\r\n2    2      2      0\r\n3    3      0      4\r\n4    4      4      0\r\n5    5      0      6\r\n6    6      6      0\r\n7    7      0      8\r\n8    8      8      0\r\n9    9      0     10\r\n```\r\n\r\n~~As I said anything that triggers \"consolidation\" stops the issue, e.g.~~\r\n\r\nEDIT: Actually see the comment below - I'm not sure consolidation is what if fixing per se. It turns out `.copy()` is deep by default and doing a shallow copy with `deep=False` produces a similarly broken DataFrame and even calling `df2._mgr._consolidate_inplace()` doesn't fix it - you get `df2._mgr._is_consolidated=True` after but the output of querying with duckdb is still mangled so the block consolidation stuff may be a red herring\r\n\r\n```python\r\ndf2 = df.copy()\r\nprint(df2._mgr._is_consolidated)\r\nwith duckdb.connect(database=':memory:') as conn:\r\n    print(conn.execute(\"select * from df2\").df())\r\n```\r\n\r\n\u27a1\ufe0f \r\n\r\n```\r\nTrue\r\n   idx  evens  zeros\r\n0    0      0      0\r\n1    1      2      0\r\n2    2      4      0\r\n3    3      6      0\r\n4    4      8      0\r\n5    5     10      0\r\n6    6     12      0\r\n7    7     14      0\r\n8    8     16      0\r\n9    9     18      0\r\n```\r\n\r\n### OS:\r\n\r\nmacOS, Linux\r\n\r\n### DuckDB Version:\r\n\r\n0.7.1, 0.8.1\r\n\r\n### DuckDB Client:\r\n\r\nPython\r\n\r\n### Full Name:\r\n\r\nDylan Scott\r\n\r\n### Affiliation:\r\n\r\nHex Technologies\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\n- [X] I agree\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] I agree\n",
  "hints_text": "btw I forgot to mention but I suspect the issue might have to do with object dtype columns. In the example the `idx` column gets a proper numeric dtype but the remaining columns have `object` dtypes. And this can cause the issue to manifest in even weirder ways. In our customer's example there were a bunch of (more than the number of rows) all zero-valued columns after the one with nonzero values and if you selected just that initial column it would return `[$correctFirst, 0, 0, ... 0]` which seemed also to apply to the logic that sampled that object column to determine its type, and it ended up inferring a narrower type than could actually hold it (int8 vs int16) and so if you did a `select *` you'd get a type cast error when it tried to cast real subsequent values of that column to a too narrow type. I can make a repro of that case if that would be helpful but I suspect it is being caused by whatever underlying issue is making it see these interleaved values.\r\n\r\nEDIT: Though actually it may be a bit more subtle. Calling `df._mgr._consolidate_inplace()` actually doesn't fix my example, though it fixed the original (customer) repro where the two blocks in the BlockManager had the same type (one `ObjectBlock` containing a date column, another `ObjectBlock` containing all of the initially zeroed but nonetheless object columns) and consolidating merged them into a single `ObjectBlock`. With my example you still end up with two blocks because `idx` is a `NumericBlock` rather than an `ObjectBlock` so they don't get merged. `.copy()` still fixes but I didn't realize it's  a deep copy by default. Doing `.copy(deep=False)` produces a shallow copy that is still broken in the same way\nThanks for the detailed info, I'll look into it soon \ud83d\udc4d \nbtw I forgot to mention but I suspect the issue might have to do with object dtype columns. In the example the `idx` column gets a proper numeric dtype but the remaining columns have `object` dtypes. And this can cause the issue to manifest in even weirder ways. In our customer's example there were a bunch of (more than the number of rows) all zero-valued columns after the one with nonzero values and if you selected just that initial column it would return `[$correctFirst, 0, 0, ... 0]` which seemed also to apply to the logic that sampled that object column to determine its type, and it ended up inferring a narrower type than could actually hold it (int8 vs int16) and so if you did a `select *` you'd get a type cast error when it tried to cast real subsequent values of that column to a too narrow type. I can make a repro of that case if that would be helpful but I suspect it is being caused by whatever underlying issue is making it see these interleaved values.\r\n\r\nEDIT: Though actually it may be a bit more subtle. Calling `df._mgr._consolidate_inplace()` actually doesn't fix my example, though it fixed the original (customer) repro where the two blocks in the BlockManager had the same type (one `ObjectBlock` containing a date column, another `ObjectBlock` containing all of the initially zeroed but nonetheless object columns) and consolidating merged them into a single `ObjectBlock`. With my example you still end up with two blocks because `idx` is a `NumericBlock` rather than an `ObjectBlock` so they don't get merged. `.copy()` still fixes but I didn't realize it's  a deep copy by default. Doing `.copy(deep=False)` produces a shallow copy that is still broken in the same way\nThanks for the detailed info, I'll look into it soon \ud83d\udc4d ",
  "created_at": "2023-06-16T10:55:35Z"
}