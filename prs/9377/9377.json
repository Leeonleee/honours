{
  "repo": "duckdb/duckdb",
  "pull_number": 9377,
  "instance_id": "duckdb__duckdb-9377",
  "issue_numbers": [
    "9371"
  ],
  "base_commit": "2646836f6fa6b47b41b2db0a96f4e9f95a4c6449",
  "patch": "diff --git a/src/function/table/arrow.cpp b/src/function/table/arrow.cpp\nindex 9601e41d28c8..306e57d30dcc 100644\n--- a/src/function/table/arrow.cpp\n+++ b/src/function/table/arrow.cpp\n@@ -266,6 +266,7 @@ unique_ptr<ArrowArrayStreamWrapper> ProduceArrowScan(const ArrowScanFunctionData\n \t\t\tauto &schema = *function.schema_root.arrow_schema.children[col_idx];\n \t\t\tparameters.projected_columns.projection_map[idx] = schema.name;\n \t\t\tparameters.projected_columns.columns.emplace_back(schema.name);\n+\t\t\tparameters.projected_columns.filter_to_col[idx] = col_idx;\n \t\t}\n \t}\n \tparameters.filters = filters;\ndiff --git a/src/include/duckdb/function/table/arrow.hpp b/src/include/duckdb/function/table/arrow.hpp\nindex fae51810aae2..df6e49953835 100644\n--- a/src/include/duckdb/function/table/arrow.hpp\n+++ b/src/include/duckdb/function/table/arrow.hpp\n@@ -33,6 +33,8 @@ struct ArrowInterval {\n struct ArrowProjectedColumns {\n \tunordered_map<idx_t, string> projection_map;\n \tvector<string> columns;\n+\t// Map from filter index to column index\n+\tunordered_map<idx_t, idx_t> filter_to_col;\n };\n \n struct ArrowStreamParameters {\ndiff --git a/tools/pythonpkg/src/arrow/arrow_array_stream.cpp b/tools/pythonpkg/src/arrow/arrow_array_stream.cpp\nindex baf5ee1be856..f7093df8a126 100644\n--- a/tools/pythonpkg/src/arrow/arrow_array_stream.cpp\n+++ b/tools/pythonpkg/src/arrow/arrow_array_stream.cpp\n@@ -66,11 +66,12 @@ py::object PythonTableArrowArrayStreamFactory::ProduceScanner(py::object &arrow_\n \n \tauto filters = parameters.filters;\n \tauto &column_list = parameters.projected_columns.columns;\n+\tauto &filter_to_col = parameters.projected_columns.filter_to_col;\n \tbool has_filter = filters && !filters->filters.empty();\n \tpy::list projection_list = py::cast(column_list);\n \tif (has_filter) {\n-\t\tauto filter =\n-\t\t    TransformFilter(*filters, parameters.projected_columns.projection_map, client_properties, arrow_table);\n+\t\tauto filter = TransformFilter(*filters, parameters.projected_columns.projection_map, filter_to_col,\n+\t\t                              client_properties, arrow_table);\n \t\tif (column_list.empty()) {\n \t\t\treturn arrow_scanner(arrow_obj_handle, py::arg(\"filter\") = filter);\n \t\t} else {\n@@ -176,7 +177,7 @@ string ConvertTimestampUnit(ArrowDateTimeType unit) {\n \tcase ArrowDateTimeType::SECONDS:\n \t\treturn \"s\";\n \tdefault:\n-\t\tthrow NotImplementedException(\"DatetimeType not recognized in ConvertTimestampUnit\");\n+\t\tthrow NotImplementedException(\"DatetimeType not recognized in ConvertTimestampUnit: %d\", (int)unit);\n \t}\n }\n \n@@ -365,12 +366,13 @@ py::object TransformFilterRecursive(TableFilter *filter, const string &column_na\n \n py::object PythonTableArrowArrayStreamFactory::TransformFilter(TableFilterSet &filter_collection,\n                                                                std::unordered_map<idx_t, string> &columns,\n+                                                               unordered_map<idx_t, idx_t> filter_to_col,\n                                                                const ClientProperties &config,\n                                                                const ArrowTableType &arrow_table) {\n \tauto filters_map = &filter_collection.filters;\n \tauto it = filters_map->begin();\n \tD_ASSERT(columns.find(it->first) != columns.end());\n-\tauto &arrow_type = *arrow_table.GetColumns().at(it->first);\n+\tauto &arrow_type = *arrow_table.GetColumns().at(filter_to_col.at(it->first));\n \tpy::object expression =\n \t    TransformFilterRecursive(it->second.get(), columns[it->first], config.time_zone, arrow_type);\n \twhile (it != filters_map->end()) {\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/arrow/arrow_array_stream.hpp b/tools/pythonpkg/src/include/duckdb_python/arrow/arrow_array_stream.hpp\nindex 4ad36b6e606a..e6769f63b1b3 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/arrow/arrow_array_stream.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/arrow/arrow_array_stream.hpp\n@@ -76,6 +76,7 @@ class PythonTableArrowArrayStreamFactory {\n private:\n \t//! We transform a TableFilterSet to an Arrow Expression Object\n \tstatic py::object TransformFilter(TableFilterSet &filters, std::unordered_map<idx_t, string> &columns,\n+\t                                  unordered_map<idx_t, idx_t> filter_to_col,\n \t                                  const ClientProperties &client_properties, const ArrowTableType &arrow_table);\n \n \tstatic py::object ProduceScanner(py::object &arrow_scanner, py::handle &arrow_obj_handle,\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/arrow/test_filter_pushdown.py b/tools/pythonpkg/tests/fast/arrow/test_filter_pushdown.py\nindex 21aaab9e56c2..74630f4a74cf 100644\n--- a/tools/pythonpkg/tests/fast/arrow/test_filter_pushdown.py\n+++ b/tools/pythonpkg/tests/fast/arrow/test_filter_pushdown.py\n@@ -504,6 +504,32 @@ def test_filter_pushdown_integers(self, duckdb_cursor, data_type, value, create_\n         actual = duckdb_cursor.execute(\"select * from arrow_table where i = ?\", (value,)).fetchall()\n         assert expected == actual\n \n+    def test_9371(self, duckdb_cursor, tmp_path):\n+        import datetime\n+        import pathlib\n+\n+        # connect to an in-memory database\n+        duckdb_cursor.execute(\"SET TimeZone='UTC';\")\n+        base_path = tmp_path / \"parquet_folder\"\n+        base_path.mkdir(exist_ok=True)\n+        file_path = base_path / \"test.parquet\"\n+\n+        duckdb_cursor.execute(\"SET TimeZone='UTC';\")\n+\n+        # Example data\n+        dt = datetime.datetime(2023, 8, 29, 1, tzinfo=datetime.timezone.utc)\n+\n+        my_arrow_table = pa.Table.from_pydict({'ts': [dt, dt, dt], 'value': [1, 2, 3]})\n+        df = my_arrow_table.to_pandas()\n+        df = df.set_index(\"ts\")  # SET INDEX! (It all works correctly when the index is not set)\n+        df.to_parquet(str(file_path))\n+\n+        my_arrow_dataset = ds.dataset(str(file_path))\n+        res = duckdb_cursor.execute(\"SELECT * FROM my_arrow_dataset WHERE ts = ?\", parameters=[dt]).arrow()\n+        output = duckdb_cursor.sql(\"select * from res\").fetchall()\n+        expected = [(1, dt), (2, dt), (3, dt)]\n+        assert output == expected\n+\n     @pytest.mark.parametrize('create_table', [create_pyarrow_pandas, create_pyarrow_table])\n     def test_filter_pushdown_date(self, duckdb_cursor, create_table):\n         duckdb_cursor.execute(\n",
  "problem_statement": "SQL query filter not working with pyarrow dataset with timestamp index\n### What happens?\r\n\r\n- Create pandas dataframe with timestamp index (i.e. ts)\r\n- Write to parquet file\r\n- Create pyarrow dataset using this parquet file\r\n- Run filter query on dataset and select specific ts: \"SELECT * FROM my_arrow_dataset WHERE ts = ?\"\r\n- The filter query does not return any rows.\r\n\r\nThis query does work when you query the parquet file directly (see reproducible). Only when you make a dataset out of it it doesn't work. It's possible that this is inherent to dataset + parquet files with timestamp indices, however, I could find any documentation suggesting this.\r\n\r\n### To Reproduce\r\n```\r\n# python==3.10.9\r\n# duckdb==0.9.1\r\n# pandas==2.1.1\r\n# pyarrow==13.0.0\r\n\r\nimport datetime\r\nimport pathlib\r\nimport tempfile\r\n\r\nimport duckdb.duckdb\r\nimport pyarrow as pa\r\nimport pyarrow.dataset as ds\r\n\r\n# connect to an in-memory database\r\ncon = duckdb.connect()\r\ncon.execute(\"SET TimeZone='UTC';\")\r\n\r\n# Example data\r\ndt = datetime.datetime(2023, 8, 29, 1, tzinfo=datetime.timezone.utc)\r\nmy_arrow_table = pa.Table.from_pydict(\r\n    {'ts': [dt, dt, dt],\r\n      'value': [1, 2, 3]})\r\ndf = my_arrow_table.to_pandas()\r\ndf = df.set_index(\"ts\")  # SET INDEX! (It all works correctly when the index is not set)\r\n\r\n# create example parquet files and save in a folder\r\nbase_path = pathlib.Path(tempfile.gettempdir()) / \"parquet_folder\"\r\nbase_path.mkdir(exist_ok=True)\r\nfile_path = base_path / \"test.parquet\"\r\ndf.to_parquet(str(file_path))\r\nmy_arrow_dataset = ds.dataset(str(base_path))\r\n\r\n\r\ntry:\r\n    # Fails with `duckdb.duckdb.NotImplementedException:\r\n    # Not implemented Error: DatetimeType not recognized in ConvertTimestampUnit`\r\n    con.execute(\"SELECT * FROM my_arrow_dataset WHERE ts = ?\", parameters=[dt]).arrow()\r\nexcept duckdb.duckdb.NotImplementedException:\r\n    pass  # Ok let's try timestamp\r\n\r\n# Doesn't fail but doesn't return anything\r\ntimestamp = pa.scalar(dt, type=pa.timestamp(\"us\", tz='UTC'))\r\nresults_dataset = con.execute(\"SELECT * FROM my_arrow_dataset WHERE ts = ?\", parameters=[timestamp.value]).arrow()\r\n\r\n# Works correctly\r\nresult_parquet_file = con.execute(\r\n    f\"SELECT * prediction_moment_ts FROM read_parquet('{str(file_path)}') WHERE ts = ?\", parameters=[dt]\r\n).arrow()\r\n\r\nassert results_dataset == result_parquet_file, f\"{repr(results_dataset)} != {repr(result_parquet_file)}\"\r\n```\r\n### OS:\r\n\r\nOSX aarch64\r\n\r\n### DuckDB Version:\r\n\r\n0.9.0\r\n\r\n### DuckDB Client:\r\n\r\npython\r\n\r\n### Full Name:\r\n\r\nSam VL\r\n\r\n### Affiliation:\r\n\r\nsource.ag\r\n\r\n### Have you tried this on the latest `main` branch?\r\n\r\nI have tested with a main build\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] Yes, I have\n",
  "hints_text": "Probably similar to #8856 \r\nThe issue is explained there, I haven't run your example yet though\nIndeed it does look like it's the same issue (or very similar at least).\r\n\r\nI tried running the reproducible script with latest main `duckdb-0.9.2.dev48`.\r\nIt still has the same issue that the datetime in the filter cannot be cast to the column type:\r\n```\r\nduckdb.duckdb.NotImplementedException: Not implemented Error: DatetimeType not recognized in ConvertTimestampUnit\r\n```\r\n\r\nPerhaps that is the real bug (instead of working around and passing `TIMESTAMP_TZ(us)` as the filter argument). \nYea the PR I mentioned is included in 0.9.1, I didn't mean it was already fixed, sorry for the confusion.\r\n\r\nI had a look and it seems the ArrowDateTimeType is DAYS, which is not something we're expecting there currently\r\n\r\nAn unfortunate detail about your reproduction is that it only works on pyarrow 13+, and that makes it almost impossible to properly debug because when I attach lldb and pyarrow13+ gets imported it causes lldb to crash\r\n```\r\n* thread #2, queue = 'com.apple.main-thread', stop reason = EXC_BAD_INSTRUCTION (code=1, subcode=0x4a03000)\r\n    frame #0: 0x0000000171abb568 libarrow.1300.dylib`_armv8_sve_probe\r\nlibarrow.1300.dylib`:\r\n->  0x171abb568 <+0>: eor    z0.d, z0.d, z0.d\r\n    0x171abb56c <+4>: ret    \r\n\r\nlibarrow.1300.dylib`:\r\n    0x171abb570 <+0>: xar    z0.d, z0.d, z0.d, #0x20\r\n    0x171abb574 <+4>: ret    \r\nTarget 0: (Python) stopped.\r\n```\r\n\r\n\nActually, got a debugger attached to it with pyarrow 12 and reproducing the issue \ud83d\udc4d ",
  "created_at": "2023-10-17T15:07:34Z"
}