{
  "repo": "duckdb/duckdb",
  "pull_number": 16729,
  "instance_id": "duckdb__duckdb-16729",
  "issue_numbers": [
    "16572",
    "16572"
  ],
  "base_commit": "d89d5fc44b4424bdd4adbdc921ab459b3eb5ec16",
  "patch": "diff --git a/data/json/sample_utf8_bom.json b/data/json/sample_utf8_bom.json\nnew file mode 100644\nindex 000000000000..e94bf1147245\n--- /dev/null\n+++ b/data/json/sample_utf8_bom.json\n@@ -0,0 +1,24 @@\n+\ufeff{\n+    \"users\": [\n+        {\n+            \"id\": 1,\n+            \"name\": \"Alice\",\n+            \"email\": \"alice@example.com\"\n+        },\n+        {\n+            \"id\": 2,\n+            \"name\": \"Bob\",\n+            \"email\": \"bob@example.com\"\n+        },\n+        {\n+            \"id\": 3,\n+            \"name\": \"Charlie\",\n+            \"email\": \"charlie@example.com\"\n+        }\n+    ],\n+    \"metadata\": {\n+        \"description\": \"Sample JSON file with UTF-8 BOM\",\n+        \"version\": \"1.0\",\n+        \"generated\": \"2025-03-09T12:00:00Z\"\n+    }\n+}\n\\ No newline at end of file\ndiff --git a/extension/core_functions/scalar/list/array_slice.cpp b/extension/core_functions/scalar/list/array_slice.cpp\nindex 0962b3c2eeff..92f1c1efb53a 100644\n--- a/extension/core_functions/scalar/list/array_slice.cpp\n+++ b/extension/core_functions/scalar/list/array_slice.cpp\n@@ -416,7 +416,12 @@ static unique_ptr<FunctionData> ArraySliceBind(ClientContext &context, ScalarFun\n \t\t\t    \"Slice with steps has not been implemented for string types, you can consider rewriting your query as \"\n \t\t\t    \"follows:\\n SELECT array_to_string((str_split(string, '')[begin:end:step], '');\");\n \t\t}\n-\t\tbound_function.return_type = arguments[0]->return_type;\n+\t\tif (arguments[0]->return_type.IsJSONType()) {\n+\t\t\t// This is needed to avoid producing invalid JSON\n+\t\t\tbound_function.return_type = LogicalType::VARCHAR;\n+\t\t} else {\n+\t\t\tbound_function.return_type = arguments[0]->return_type;\n+\t\t}\n \t\tfor (idx_t i = 1; i < 3; i++) {\n \t\t\tif (arguments[i]->return_type.id() != LogicalTypeId::LIST) {\n \t\t\t\tbound_function.arguments[i] = LogicalType::BIGINT;\ndiff --git a/extension/json/json_functions/copy_json.cpp b/extension/json/json_functions/copy_json.cpp\nindex 7cb196576907..0cc42cb2d5e8 100644\n--- a/extension/json/json_functions/copy_json.cpp\n+++ b/extension/json/json_functions/copy_json.cpp\n@@ -120,7 +120,7 @@ CopyFunction JSONFunctions::GetJSONCopyFunction() {\n \n \tfunction.copy_from_bind = MultiFileReaderFunction<JSONMultiFileInfo>::MultiFileBindCopy;\n \tfunction.copy_from_function = JSONFunctions::GetReadJSONTableFunction(make_shared_ptr<JSONScanInfo>(\n-\t    JSONScanType::READ_JSON, JSONFormat::NEWLINE_DELIMITED, JSONRecordType::RECORDS, false));\n+\t    JSONScanType::READ_JSON, JSONFormat::AUTO_DETECT, JSONRecordType::RECORDS, false));\n \n \treturn function;\n }\ndiff --git a/extension/json/json_multi_file_info.cpp b/extension/json/json_multi_file_info.cpp\nindex ca91fc941b55..58e539686af2 100644\n--- a/extension/json/json_multi_file_info.cpp\n+++ b/extension/json/json_multi_file_info.cpp\n@@ -21,8 +21,10 @@ unique_ptr<BaseFileReaderOptions> JSONMultiFileInfo::InitializeOptions(ClientCon\n \t\t}\n \t} else {\n \t\t// COPY\n+\t\toptions.type = JSONScanType::READ_JSON;\n \t\toptions.record_type = JSONRecordType::RECORDS;\n-\t\toptions.format = JSONFormat::NEWLINE_DELIMITED;\n+\t\toptions.format = JSONFormat::AUTO_DETECT;\n+\t\toptions.auto_detect = false;\n \t}\n \treturn std::move(reader_options);\n }\n@@ -76,6 +78,9 @@ bool JSONMultiFileInfo::ParseOption(ClientContext &context, const string &key, c\n \t\tfor (idx_t i = 0; i < struct_children.size(); i++) {\n \t\t\tauto &name = StructType::GetChildName(child_type, i);\n \t\t\tauto &val = struct_children[i];\n+\t\t\tif (val.IsNull()) {\n+\t\t\t\tthrow BinderException(\"read_json \\\"columns\\\" parameter type specification cannot be NULL.\");\n+\t\t\t}\n \t\t\toptions.name_list.push_back(name);\n \t\t\tif (val.type().id() != LogicalTypeId::VARCHAR) {\n \t\t\t\tthrow BinderException(\"read_json \\\"columns\\\" parameter type specification must be VARCHAR.\");\n@@ -222,6 +227,7 @@ bool JSONMultiFileInfo::ParseCopyOption(ClientContext &context, const string &ke\n \t\t} else {\n \t\t\tJSONCheckSingleParameter(key, values);\n \t\t\toptions.auto_detect = BooleanValue::Get(values.back().DefaultCastAs(LogicalTypeId::BOOLEAN));\n+\t\t\toptions.format = JSONFormat::NEWLINE_DELIMITED;\n \t\t}\n \t\treturn true;\n \t}\n@@ -238,6 +244,9 @@ bool JSONMultiFileInfo::ParseCopyOption(ClientContext &context, const string &ke\n \t\t\tJSONCheckSingleParameter(key, values);\n \t\t\tif (BooleanValue::Get(values.back().DefaultCastAs(LogicalTypeId::BOOLEAN))) {\n \t\t\t\toptions.format = JSONFormat::ARRAY;\n+\t\t\t} else {\n+\t\t\t\t// Default to newline-delimited otherwise\n+\t\t\t\toptions.format = JSONFormat::NEWLINE_DELIMITED;\n \t\t\t}\n \t\t}\n \t\treturn true;\ndiff --git a/extension/json/json_reader.cpp b/extension/json/json_reader.cpp\nindex 1ee91452a4d6..241ea1509c66 100644\n--- a/extension/json/json_reader.cpp\n+++ b/extension/json/json_reader.cpp\n@@ -903,8 +903,11 @@ void JSONReader::FinalizeBuffer(JSONReaderScanState &scan_state) {\n \t// we read something\n \t// skip over the array start if required\n \tif (!scan_state.is_last) {\n-\t\tif (scan_state.buffer_index.GetIndex() == 0 && GetFormat() == JSONFormat::ARRAY) {\n-\t\t\tSkipOverArrayStart(scan_state);\n+\t\tif (scan_state.buffer_index.GetIndex() == 0) {\n+\t\t\tStringUtil::SkipBOM(scan_state.buffer_ptr, scan_state.buffer_size, scan_state.buffer_offset);\n+\t\t\tif (GetFormat() == JSONFormat::ARRAY) {\n+\t\t\t\tSkipOverArrayStart(scan_state);\n+\t\t\t}\n \t\t}\n \t}\n \t// then finalize the buffer\ndiff --git a/src/common/string_util.cpp b/src/common/string_util.cpp\nindex b0d4b8731ca8..f7a817b5065f 100644\n--- a/src/common/string_util.cpp\n+++ b/src/common/string_util.cpp\n@@ -846,6 +846,13 @@ void StringUtil::URLDecodeBuffer(const char *input, idx_t input_size, char *outp\n \t}\n }\n \n+void StringUtil::SkipBOM(const char *buffer_ptr, const idx_t &buffer_size, idx_t &buffer_pos) {\n+\tif (buffer_size >= 3 && buffer_ptr[0] == '\\xEF' && buffer_ptr[1] == '\\xBB' && buffer_ptr[2] == '\\xBF' &&\n+\t    buffer_pos == 0) {\n+\t\tbuffer_pos = 3;\n+\t}\n+}\n+\n string StringUtil::URLDecode(const string &input, bool plus_to_space) {\n \tidx_t result_size = URLDecodeSize(input.c_str(), input.size(), plus_to_space);\n \tauto result_data = make_uniq_array<char>(result_size);\ndiff --git a/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp b/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp\nindex 476e4f4bb468..51d508834fe4 100644\n--- a/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp\n+++ b/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp\n@@ -1578,10 +1578,7 @@ bool StringValueScanner::MoveToNextBuffer() {\n }\n \n void StringValueResult::SkipBOM() const {\n-\tif (buffer_size >= 3 && buffer_ptr[0] == '\\xEF' && buffer_ptr[1] == '\\xBB' && buffer_ptr[2] == '\\xBF' &&\n-\t    iterator.pos.buffer_pos == 0) {\n-\t\titerator.pos.buffer_pos = 3;\n-\t}\n+\tStringUtil::SkipBOM(buffer_ptr, buffer_size, iterator.pos.buffer_pos);\n }\n \n void StringValueResult::RemoveLastLine() {\ndiff --git a/src/execution/operator/set/physical_union.cpp b/src/execution/operator/set/physical_union.cpp\nindex de2a6282d60a..0071de2513e4 100644\n--- a/src/execution/operator/set/physical_union.cpp\n+++ b/src/execution/operator/set/physical_union.cpp\n@@ -17,6 +17,18 @@ PhysicalUnion::PhysicalUnion(vector<LogicalType> types, PhysicalOperator &top, P\n //===--------------------------------------------------------------------===//\n // Pipeline Construction\n //===--------------------------------------------------------------------===//\n+static bool ContainsSink(PhysicalOperator &op) {\n+\tif (op.IsSink()) {\n+\t\treturn true;\n+\t}\n+\tfor (auto &child : op.children) {\n+\t\tif (ContainsSink(child)) {\n+\t\t\treturn true;\n+\t\t}\n+\t}\n+\treturn false;\n+}\n+\n void PhysicalUnion::BuildPipelines(Pipeline &current, MetaPipeline &meta_pipeline) {\n \top_state.reset();\n \tsink_state.reset();\n@@ -52,7 +64,11 @@ void PhysicalUnion::BuildPipelines(Pipeline &current, MetaPipeline &meta_pipelin\n \n \tvector<shared_ptr<Pipeline>> dependencies;\n \toptional_ptr<MetaPipeline> last_child_ptr;\n-\tconst auto can_saturate_threads = children[0].get().CanSaturateThreads(current.GetClientContext());\n+\t// users commonly UNION ALL together a bunch of cheap scan pipelines (e.g., instead of a multi file list)\n+\t// in these cases, we don't want to avoid breadth-first plan evaluation,\n+\t// as it doesn't pose a threat to memory usage (it's just a bunch of straight scans)\n+\tconst auto can_saturate_threads =\n+\t    ContainsSink(children[0]) && children[0].get().CanSaturateThreads(current.GetClientContext());\n \tif (order_matters || can_saturate_threads) {\n \t\t// we add dependencies if order matters: union_pipeline comes after all pipelines created by building current\n \t\tdependencies = meta_pipeline.AddDependenciesFrom(union_pipeline, union_pipeline, false);\ndiff --git a/src/include/duckdb/common/string_util.hpp b/src/include/duckdb/common/string_util.hpp\nindex c46453952ad7..8c0c19bef93a 100644\n--- a/src/include/duckdb/common/string_util.hpp\n+++ b/src/include/duckdb/common/string_util.hpp\n@@ -165,6 +165,9 @@ class StringUtil {\n \tDUCKDB_API static void URLDecodeBuffer(const char *input, idx_t input_size, char *output,\n \t                                       bool plus_to_space = false);\n \n+\t//! BOM skipping (https://en.wikipedia.org/wiki/Byte_order_mark)\n+\tDUCKDB_API static void SkipBOM(const char *buffer_ptr, const idx_t &buffer_size, idx_t &buffer_pos);\n+\n \tDUCKDB_API static idx_t ToUnsigned(const string &str);\n \n \ttemplate <class T>\n",
  "test_patch": "diff --git a/test/sql/json/issues/internal_issue4389.test b/test/sql/json/issues/internal_issue4389.test\nnew file mode 100644\nindex 000000000000..dea3a10afe59\n--- /dev/null\n+++ b/test/sql/json/issues/internal_issue4389.test\n@@ -0,0 +1,20 @@\n+# name: test/sql/json/issues/internal_issue4389.test\n+# description: Test internal issue 4389 - auto_detect is false for COPY + JSON\n+# group: [issues]\n+\n+require json\n+\n+statement ok\n+pragma enable_verification\n+\n+statement ok\n+CREATE TABLE todos (userId UBIGINT, id UBIGINT, title VARCHAR, completed BOOLEAN);\n+\n+statement ok\n+insert into todos values (42, 42, 'duck', true)\n+\n+statement ok\n+copy todos to '__TEST_DIR__/todos.json' (ARRAY)\n+\n+statement ok\n+copy todos from '__TEST_DIR__/todos.json'\ndiff --git a/test/sql/json/issues/internal_issue4403.test b/test/sql/json/issues/internal_issue4403.test\nnew file mode 100644\nindex 000000000000..8507d2ff27d7\n--- /dev/null\n+++ b/test/sql/json/issues/internal_issue4403.test\n@@ -0,0 +1,13 @@\n+# name: test/sql/json/issues/internal_issue4403.test\n+# description: Test internal issue 4403 - AFL fuzzer crash (NULL type specification)\n+# group: [issues]\n+\n+require json\n+\n+statement ok\n+pragma enable_verification\n+\n+statement error\n+SELECT * FROM read_json('data/json/example_n.ndjson', columns={id: NULL::VARCHAR, name: NULL::VARCHAR})\n+----\n+Binder Error\ndiff --git a/test/sql/json/issues/issue16568.test b/test/sql/json/issues/issue16568.test\nnew file mode 100644\nindex 000000000000..a884d637c6a9\n--- /dev/null\n+++ b/test/sql/json/issues/issue16568.test\n@@ -0,0 +1,13 @@\n+# name: test/sql/json/issues/issue16568.test\n+# description: Test issue 16568 - Error when loading JSON files with UTF-8 Byte Order Mark (BOM)\n+# group: [issues]\n+\n+require json\n+\n+statement ok\n+pragma enable_verification\n+\n+query I\n+select count(*) from 'data/json/sample_utf8_bom.json'\n+----\n+1\ndiff --git a/test/sql/json/issues/issue16570.test b/test/sql/json/issues/issue16570.test\nnew file mode 100644\nindex 000000000000..f4caf2f548e7\n--- /dev/null\n+++ b/test/sql/json/issues/issue16570.test\n@@ -0,0 +1,17 @@\n+# name: test/sql/json/issues/issue16570.test\n+# description: Test issue 16570 - JSON type: string slice operation results in result value with JSON type, expected VARCHAR\n+# group: [issues]\n+\n+require json\n+\n+statement ok\n+pragma enable_verification\n+\n+query II\n+with cte as (\n+    select '{\"a\":1}'::JSON as j\n+)\n+select typeof(j[2:3]), typeof(substring(j, 2, 3))\n+from cte\n+----\n+VARCHAR\tVARCHAR\ndiff --git a/test/sql/json/test_json_copy.test_slow b/test/sql/json/test_json_copy.test_slow\nindex b1beee17c385..0c3f07291712 100644\n--- a/test/sql/json/test_json_copy.test_slow\n+++ b/test/sql/json/test_json_copy.test_slow\n@@ -160,14 +160,15 @@ select * from '__TEST_DIR__/out.json'\n statement ok\n create table conclusions (conclusion varchar)\n \n-# this doesn't work because we assume NDJSON records\n-statement error\n+# works because we auto-detect by default\n+statement ok\n copy conclusions from 'data/json/top_level_array.json'\n-----\n \n-# but works if we tell it to auto-detect\n-statement ok\n-copy conclusions from 'data/json/top_level_array.json' (AUTO_DETECT TRUE)\n+# doesn't work if we disable auto-detection\n+statement error\n+copy conclusions from 'data/json/top_level_array.json' (AUTO_DETECT FALSE)\n+----\n+Invalid Input Error\n \n statement ok\n delete from conclusions;\n",
  "problem_statement": "Performance issue: No multi-threading unless I partition and union manually\n### What happens?\n\nI have an aggregation that I need to do per customer. \nFor each customer, I have a number of files scattered around my file-system; the mapping from customer to files is provided by a third party tool that spits out strings of the form:\n\n```sql\nSELECT customer1 AS customer, * FROM '02q340234.parquet'\nUNION ALL\nSELECT customer1 AS customer, * FROM '014afe123.parquet'\nUNION ALL \nSELECT customer2 AS customer, * FROM '39fefe934.parquet'\nUNION ALL\nSELECT customer2 AS customer, * FROM '93feas023.parquet'\nUNION ALL\n...\n```\n\n\nIf I run \n\n```sql\nSELECT\n  customer,\n  <someaggregation>\nFROM <third_party_tool_output>\nGROUP BY customer\n```\n\nDuckDB doesn't seem to use multi-threading. If I manually run that same query separately on the files pertaining to each customer (by splitting the third party output manually), and take the union of results afterwards, all my 4 CPU cores are busy and I get the results 4x faster.\n\n### To Reproduce\n\n```python\nimport pandas as pd\nimport numpy as np\nimport duckdb\nimport itertools\nfrom collections import defaultdict\n\nm = 40\nn_customers = 8\nrows = 1_000_000\n\nraw_data = defaultdict(list)\nfor i in range(m):\n    path = f\"/tmp/test{i}.parquet\"\n    pd.DataFrame(np.random.rand(rows, 1), columns=['value']).reset_index().to_parquet(path)\n    customer = i % n_customers\n    raw_data[customer].append(f\"\"\"SELECT '{customer}' AS customer, * FROM '{path}'\"\"\")\n    \ndef my_query(inputs):\n    q = \"\\nUNION ALL\\n\".join(inputs)\n    return f\"\"\"SELECT df1.customer, sum(df1.value * df2.value) FROM ({q}) df1 JOIN ({q}) df2 USING (customer, index) GROUP BY ALL\"\"\"\n\nq1 = my_query(itertools.chain(*raw_data.values()))\nduckdb.query(q1) # slow \n\nq2 = \"\\nUNION ALL\\n\".join([my_query(inputs) for inputs in raw_data.values()])\nduckdb.query(q2) # fast but ugly\n```\n\n### OS:\n\nLinux\n\n### DuckDB Version:\n\n1.2.2-dev15\n\n### DuckDB Client:\n\nPython\n\n### Hardware:\n\nAMD64, Intel, 4 physical cores, 8 logical cores\n\n### Full Name:\n\nSoeren Wolfers\n\n### Affiliation:\n\nG-Research\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a nightly build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nNot applicable - the reproduction does not require a data set\n\n### Did you include all code required to reproduce the issue?\n\n- [x] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [x] Yes, I have\nPerformance issue: No multi-threading unless I partition and union manually\n### What happens?\n\nI have an aggregation that I need to do per customer. \nFor each customer, I have a number of files scattered around my file-system; the mapping from customer to files is provided by a third party tool that spits out strings of the form:\n\n```sql\nSELECT customer1 AS customer, * FROM '02q340234.parquet'\nUNION ALL\nSELECT customer1 AS customer, * FROM '014afe123.parquet'\nUNION ALL \nSELECT customer2 AS customer, * FROM '39fefe934.parquet'\nUNION ALL\nSELECT customer2 AS customer, * FROM '93feas023.parquet'\nUNION ALL\n...\n```\n\n\nIf I run \n\n```sql\nSELECT\n  customer,\n  <someaggregation>\nFROM <third_party_tool_output>\nGROUP BY customer\n```\n\nDuckDB doesn't seem to use multi-threading. If I manually run that same query separately on the files pertaining to each customer (by splitting the third party output manually), and take the union of results afterwards, all my 4 CPU cores are busy and I get the results 4x faster.\n\n### To Reproduce\n\n```python\nimport pandas as pd\nimport numpy as np\nimport duckdb\nimport itertools\nfrom collections import defaultdict\n\nm = 40\nn_customers = 8\nrows = 1_000_000\n\nraw_data = defaultdict(list)\nfor i in range(m):\n    path = f\"/tmp/test{i}.parquet\"\n    pd.DataFrame(np.random.rand(rows, 1), columns=['value']).reset_index().to_parquet(path)\n    customer = i % n_customers\n    raw_data[customer].append(f\"\"\"SELECT '{customer}' AS customer, * FROM '{path}'\"\"\")\n    \ndef my_query(inputs):\n    q = \"\\nUNION ALL\\n\".join(inputs)\n    return f\"\"\"SELECT df1.customer, sum(df1.value * df2.value) FROM ({q}) df1 JOIN ({q}) df2 USING (customer, index) GROUP BY ALL\"\"\"\n\nq1 = my_query(itertools.chain(*raw_data.values()))\nduckdb.query(q1) # slow \n\nq2 = \"\\nUNION ALL\\n\".join([my_query(inputs) for inputs in raw_data.values()])\nduckdb.query(q2) # fast but ugly\n```\n\n### OS:\n\nLinux\n\n### DuckDB Version:\n\n1.2.2-dev15\n\n### DuckDB Client:\n\nPython\n\n### Hardware:\n\nAMD64, Intel, 4 physical cores, 8 logical cores\n\n### Full Name:\n\nSoeren Wolfers\n\n### Affiliation:\n\nG-Research\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a nightly build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nNot applicable - the reproduction does not require a data set\n\n### Did you include all code required to reproduce the issue?\n\n- [x] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [x] Yes, I have\n",
  "hints_text": "we can confirm this as well. we have a lot of queries we need to union at the end. and it seems like every query is processed in serial with one thread. would be great to have multithreaded unions.\n@OneCyrus I'm not sure you have the same problem as described in this issue. In my case, both versions of the query contain unions and the one where the unions are the outer operation is actually faster. Thus, from my perspective, this seems to be a missed optimization of some aspect in the `GROUP BY` or `JOIN` clause that would allow DuckDB to logically do the same thing as the second query, i.e. run independent subqueries by customer, rather than creating one big hash map for the joins / group bys over all customers. \nwe can confirm this as well. we have a lot of queries we need to union at the end. and it seems like every query is processed in serial with one thread. would be great to have multithreaded unions.\n@OneCyrus I'm not sure you have the same problem as described in this issue. In my case, both versions of the query contain unions and the one where the unions are the outer operation is actually faster. Thus, from my perspective, this seems to be a missed optimization of some aspect in the `GROUP BY` or `JOIN` clause that would allow DuckDB to logically do the same thing as the second query, i.e. run independent subqueries by customer, rather than creating one big hash map for the joins / group bys over all customers. ",
  "created_at": "2025-03-19T14:19:37Z"
}