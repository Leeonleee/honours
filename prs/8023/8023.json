{
  "repo": "duckdb/duckdb",
  "pull_number": 8023,
  "instance_id": "duckdb__duckdb-8023",
  "issue_numbers": [
    "7988"
  ],
  "base_commit": "3a48ba1822e396e1aaf4d66f705073e8f0879e2f",
  "patch": "diff --git a/src/execution/physical_operator.cpp b/src/execution/physical_operator.cpp\nindex 42344f05fb11..427f214dc120 100644\n--- a/src/execution/physical_operator.cpp\n+++ b/src/execution/physical_operator.cpp\n@@ -124,6 +124,22 @@ idx_t PhysicalOperator::GetMaxThreadMemory(ClientContext &context) {\n \treturn (max_memory / num_threads) / 4;\n }\n \n+bool PhysicalOperator::OperatorCachingAllowed(ExecutionContext &context) {\n+\tif (!context.client.config.enable_caching_operators) {\n+\t\treturn false;\n+\t} else if (!context.pipeline) {\n+\t\treturn false;\n+\t} else if (!context.pipeline->GetSink()) {\n+\t\treturn false;\n+\t} else if (context.pipeline->GetSink()->RequiresBatchIndex()) {\n+\t\treturn false;\n+\t} else if (context.pipeline->IsOrderDependent()) {\n+\t\treturn false;\n+\t}\n+\n+\treturn true;\n+}\n+\n //===--------------------------------------------------------------------===//\n // Pipeline Construction\n //===--------------------------------------------------------------------===//\n@@ -239,20 +255,7 @@ OperatorResultType CachingPhysicalOperator::Execute(ExecutionContext &context, D\n #if STANDARD_VECTOR_SIZE >= 128\n \tif (!state.initialized) {\n \t\tstate.initialized = true;\n-\t\tstate.can_cache_chunk = true;\n-\n-\t\tif (!context.client.config.enable_caching_operators) {\n-\t\t\tstate.can_cache_chunk = false;\n-\t\t} else if (!context.pipeline || !caching_supported) {\n-\t\t\tstate.can_cache_chunk = false;\n-\t\t} else if (!context.pipeline->GetSink()) {\n-\t\t\t// Disabling for pipelines without Sink, i.e. when pulling\n-\t\t\tstate.can_cache_chunk = false;\n-\t\t} else if (context.pipeline->GetSink()->RequiresBatchIndex()) {\n-\t\t\tstate.can_cache_chunk = false;\n-\t\t} else if (context.pipeline->IsOrderDependent()) {\n-\t\t\tstate.can_cache_chunk = false;\n-\t\t}\n+\t\tstate.can_cache_chunk = caching_supported && PhysicalOperator::OperatorCachingAllowed(context);\n \t}\n \tif (!state.can_cache_chunk) {\n \t\treturn child_result;\ndiff --git a/src/include/duckdb/execution/physical_operator.hpp b/src/include/duckdb/execution/physical_operator.hpp\nindex e26d44fe13b8..048fe1e40320 100644\n--- a/src/include/duckdb/execution/physical_operator.hpp\n+++ b/src/include/duckdb/execution/physical_operator.hpp\n@@ -156,6 +156,9 @@ class PhysicalOperator {\n \t//! The maximum amount of memory the operator should use per thread.\n \tstatic idx_t GetMaxThreadMemory(ClientContext &context);\n \n+\t//! Whether operator caching is allowed in the current execution context\n+\tstatic bool OperatorCachingAllowed(ExecutionContext &context);\n+\n \tvirtual bool IsSink() const {\n \t\treturn false;\n \t}\ndiff --git a/src/parallel/pipeline_executor.cpp b/src/parallel/pipeline_executor.cpp\nindex 1bd1fde7b7ee..73831a85fae1 100644\n--- a/src/parallel/pipeline_executor.cpp\n+++ b/src/parallel/pipeline_executor.cpp\n@@ -79,6 +79,7 @@ bool PipelineExecutor::TryFlushCachingOperators() {\n \t\tOperatorResultType push_result;\n \n \t\tif (in_process_operators.empty()) {\n+\t\t\tcurr_chunk.Reset();\n \t\t\tStartOperator(current_operator);\n \t\t\tfinalize_result = current_operator.FinalExecute(context, curr_chunk, *current_operator.op_state,\n \t\t\t                                                *intermediate_states[flushing_idx]);\n",
  "test_patch": "diff --git a/test/sql/function/table/table_in_out.cpp b/test/sql/function/table/table_in_out.cpp\nindex e79f4179368f..0dac7996ba5d 100644\n--- a/test/sql/function/table/table_in_out.cpp\n+++ b/test/sql/function/table/table_in_out.cpp\n@@ -11,25 +11,35 @@ using namespace std;\n // - during flushing of caching operators still emits only 1 row sum per call, meaning that multiple flushes are\n // required to correctly process this operator\n struct ThrottlingSum {\n-\tstruct CustomFunctionData : public TableFunctionData {\n-\t\tCustomFunctionData() {\n+\tstruct ThrottlingSumLocalData : public LocalTableFunctionState {\n+\t\tThrottlingSumLocalData() {\n \t\t}\n \t\tduckdb::vector<int> row_sums;\n \t\tidx_t current_idx = 0;\n \t};\n \n+\tstatic duckdb::unique_ptr<GlobalTableFunctionState> ThrottlingSumGlobalInit(ClientContext &context,\n+\t                                                                            TableFunctionInitInput &input) {\n+\t\treturn make_uniq<GlobalTableFunctionState>();\n+\t}\n+\n+\tstatic duckdb::unique_ptr<LocalTableFunctionState> ThrottlingSumLocalInit(ExecutionContext &context,\n+\t                                                                          TableFunctionInitInput &input,\n+\t                                                                          GlobalTableFunctionState *global_state) {\n+\t\treturn make_uniq<ThrottlingSumLocalData>();\n+\t}\n+\n \tstatic duckdb::unique_ptr<FunctionData> Bind(ClientContext &context, TableFunctionBindInput &input,\n \t                                             duckdb::vector<LogicalType> &return_types,\n \t                                             duckdb::vector<string> &names) {\n-\t\tauto result = make_uniq<ThrottlingSum::CustomFunctionData>();\n \t\treturn_types.emplace_back(LogicalType::INTEGER);\n \t\tnames.emplace_back(\"total\");\n-\t\treturn std::move(result);\n+\t\treturn make_uniq<TableFunctionData>();\n \t}\n \n \tstatic OperatorResultType Function(ExecutionContext &context, TableFunctionInput &data_p, DataChunk &input,\n \t                                   DataChunk &output) {\n-\t\tauto &state = data_p.bind_data->CastNoConst<ThrottlingSum::CustomFunctionData>();\n+\t\tauto &local_state = data_p.local_state->Cast<ThrottlingSum::ThrottlingSumLocalData>();\n \n \t\tfor (idx_t row_idx = 0; row_idx < input.size(); row_idx++) {\n \t\t\tint sum = 0;\n@@ -38,14 +48,25 @@ struct ThrottlingSum {\n \t\t\t\t\tsum += input.data[col_idx].GetValue(row_idx).GetValue<int>();\n \t\t\t\t}\n \t\t\t}\n-\t\t\tstate.row_sums.push_back(sum);\n+\t\t\tlocal_state.row_sums.push_back(sum);\n \t\t}\n \n-\t\tif (state.current_idx < state.row_sums.size()) {\n-\t\t\toutput.SetCardinality(1);\n-\t\t\toutput.SetValue(0, 0, Value(state.row_sums[state.current_idx++]));\n+\t\tif (PhysicalOperator::OperatorCachingAllowed(context)) {\n+\t\t\t// Caching is allowed\n+\t\t\tif (local_state.current_idx < local_state.row_sums.size()) {\n+\t\t\t\toutput.SetCardinality(1);\n+\t\t\t\toutput.SetValue(0, 0, Value(local_state.row_sums[local_state.current_idx++]));\n+\t\t\t} else {\n+\t\t\t\toutput.SetCardinality(0);\n+\t\t\t}\n \t\t} else {\n-\t\t\toutput.SetCardinality(0);\n+\t\t\t// Caching is not allowed, we should emit everything!\n+\t\t\tauto to_emit = local_state.row_sums.size() - local_state.current_idx;\n+\t\t\tfor (idx_t i = 0; i < to_emit; i++) {\n+\t\t\t\toutput.SetValue(0, i, Value(local_state.row_sums[local_state.current_idx + i]));\n+\t\t\t}\n+\t\t\tlocal_state.current_idx += to_emit;\n+\t\t\toutput.SetCardinality(to_emit);\n \t\t}\n \n \t\treturn OperatorResultType::NEED_MORE_INPUT;\n@@ -53,14 +74,13 @@ struct ThrottlingSum {\n \n \tstatic OperatorFinalizeResultType Finalize(ExecutionContext &context, TableFunctionInput &data_p,\n \t                                           DataChunk &output) {\n-\t\tauto &state = data_p.bind_data->CastNoConst<ThrottlingSum::CustomFunctionData>();\n+\t\tauto &local_state = data_p.local_state->Cast<ThrottlingSum::ThrottlingSumLocalData>();\n \n-\t\tif (state.current_idx < state.row_sums.size()) {\n+\t\tif (local_state.current_idx < local_state.row_sums.size()) {\n \t\t\toutput.SetCardinality(1);\n-\t\t\toutput.SetValue(0, 0, Value(state.row_sums[state.current_idx++]));\n+\t\t\toutput.SetValue(0, 0, Value(local_state.row_sums[local_state.current_idx++]));\n \t\t\treturn OperatorFinalizeResultType::HAVE_MORE_OUTPUT;\n \t\t} else {\n-\t\t\toutput.SetCardinality(0);\n \t\t\treturn OperatorFinalizeResultType::FINISHED;\n \t\t}\n \t}\n@@ -70,7 +90,9 @@ struct ThrottlingSum {\n \t\tcon.BeginTransaction();\n \t\tauto &client_context = *con.context;\n \t\tauto &catalog = Catalog::GetSystemCatalog(client_context);\n-\t\tTableFunction caching_table_in_out(\"throttling_sum\", {LogicalType::TABLE}, nullptr, ThrottlingSum::Bind);\n+\t\tTableFunction caching_table_in_out(\"throttling_sum\", {LogicalType::TABLE}, nullptr, ThrottlingSum::Bind,\n+\t\t                                   ThrottlingSum::ThrottlingSumGlobalInit,\n+\t\t                                   ThrottlingSum::ThrottlingSumLocalInit);\n \t\tcaching_table_in_out.in_out_function = ThrottlingSum::Function;\n \t\tcaching_table_in_out.in_out_function_final = ThrottlingSum::Finalize;\n \t\tCreateTableFunctionInfo caching_table_in_out_info(caching_table_in_out);\n@@ -99,3 +121,17 @@ TEST_CASE(\"Caching TableInOutFunction\", \"[filter][.]\") {\n \tREQUIRE(result3->ColumnCount() == 1);\n \tREQUIRE(CHECK_COLUMN(result3, 0, {Value::BIGINT(16900000000)}));\n }\n+\n+TEST_CASE(\"Parallel execution with caching table in out functions\", \"[filter][.]\") {\n+\tDuckDB db(nullptr);\n+\tConnection con(db);\n+\n+\tThrottlingSum::Register(con);\n+\n+\tauto result = con.Query(\"CREATE TABLE test_data as select i::INTEGER from range(0,200000) tbl(i);\");\n+\tauto result2 = con.Query(\"SELECT * FROM throttling_sum((select * from test_data));\");\n+\n+\tREQUIRE(result2->ColumnCount() == 1);\n+\tREQUIRE(result2->RowCount() == 200000);\n+\tREQUIRE(CHECK_COLUMN(result2, 0, {0, 1, 2, 3, 4, 5}));\n+}\n",
  "problem_statement": "arrowIPCAll crashes with some queries\n### What happens?\r\n\r\nWhen querying for an Arrow buffer in the Node.js API, I'm seeing some queries fail with a complete DuckDB crash. This error appears to be new as of DuckDB v0.8.1.\r\n\r\nThe resulting error is:\r\n\r\n```\r\n[Error: INTERNAL Error: BatchedDataCollection::Merge error - batch index 9223372036854775807 is present in both collections. This occurs when batch indexes are not uniquely distributed over threads] {\r\n  errno: -1,\r\n  code: 'DUCKDB_NODEJS_ERROR',\r\n  errorType: 'INTERNAL'\r\n}\r\n```\r\n\r\n### To Reproduce\r\n\r\nThe following JavaScript code should reproduce the error. Note that the error does not occur when calling `con.all` instead of `con.arrowIPCAll`.\r\n\r\n```js\r\nimport duckdb from 'duckdb';\r\n\r\nconst db = new duckdb.Database(':memory:');\r\nconst con = db.connect();\r\n\r\n// load extensions and base data table\r\nconst initSQL = `\r\n  INSTALL arrow; INSTALL httpfs; LOAD arrow; LOAD httpfs;\r\n  CREATE TABLE stocks AS (\r\n    SELECT\r\n      row_number() OVER (PARTITION BY Symbol ORDER BY Date) AS Index,\r\n      Close,\r\n      Symbol\r\n    FROM 'https://uwdata.github.io/mosaic-datasets/data/stocks.csv'\r\n  );\r\n`;\r\n\r\n// query for time series with AM4 optimization (simplified for bug report)\r\nconst querySQL = `\r\n SELECT MIN(Index) AS Index, ARG_MIN(Close, Index) AS Close, Symbol FROM stocks GROUP BY Index, Symbol\r\n UNION\r\n  SELECT MAX(Index) AS Index, ARG_MAX(Close, Index) AS Close, Symbol FROM stocks GROUP BY Index, Symbol\r\n UNION\r\n  SELECT ARG_MIN(Index, Close) AS Index, MIN(Close) AS Close, Symbol FROM stocks GROUP BY Index, Symbol\r\n UNION\r\n  SELECT ARG_MAX(Index, Close) AS Index, MAX(Close) AS Close, Symbol FROM stocks GROUP BY Index, Symbol\r\n ORDER BY Symbol, Index\r\n`;\r\n\r\n// issue queries\r\ncon.exec(initSQL, (err) => {\r\n  if (err) {\r\n    console.error(err);\r\n  } else {\r\n    con.arrowIPCAll(querySQL, (err, result) => {\r\n      if (err) {\r\n        console.error(err);\r\n      } else {\r\n        console.log('SUCCESS!', result);\r\n      }\r\n    });\r\n  }\r\n});\r\n```\r\n\r\n### OS:\r\n\r\nMacOS\r\n\r\n### DuckDB Version:\r\n\r\n0.8.1\r\n\r\n### DuckDB Client:\r\n\r\nnodejs\r\n\r\n### Full Name:\r\n\r\nJeffrey Heer\r\n\r\n### Affiliation:\r\n\r\nUniversity of Washington\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\n- [x] I agree\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] I agree\n",
  "hints_text": "Update: I found that this simpler query also replicates the issue, so a `UNION` does not seem to be part of the issue. **If you remove the `ORDER BY` clause, the query completes successfully.**\r\n\r\n``` js\r\nconst querySQL = `\r\n SELECT MIN(Index) as Index, MAX(Close), Symbol FROM stocks\r\n GROUP BY Symbol, Index / 4.0\r\n ORDER BY Symbol, Index\r\n`;\r\n```",
  "created_at": "2023-06-21T09:03:17Z"
}