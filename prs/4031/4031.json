{
  "repo": "duckdb/duckdb",
  "pull_number": 4031,
  "instance_id": "duckdb__duckdb-4031",
  "issue_numbers": [
    "3989"
  ],
  "base_commit": "de79099670ab1a10de9ef60565b35a48b8a996a6",
  "patch": "diff --git a/extension/parquet/column_reader.cpp b/extension/parquet/column_reader.cpp\nindex e7486e9d5c38..0eb71e5a85b1 100644\n--- a/extension/parquet/column_reader.cpp\n+++ b/extension/parquet/column_reader.cpp\n@@ -698,16 +698,26 @@ ListColumnReader::ListColumnReader(ParquetReader &reader, LogicalType type_p, co\n \tchild_filter.set();\n }\n \n-// ListColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,\n-//                             Vector &result_out)\n void ListColumnReader::ApplyPendingSkips(idx_t num_values) {\n \tpending_skips -= num_values;\n \n-\tparquet_filter_t filter;\n \tauto define_out = unique_ptr<uint8_t[]>(new uint8_t[num_values]);\n \tauto repeat_out = unique_ptr<uint8_t[]>(new uint8_t[num_values]);\n-\tVector result_out(Type());\n-\tRead(num_values, filter, define_out.get(), repeat_out.get(), result_out);\n+\n+\tidx_t remaining = num_values;\n+\tidx_t read = 0;\n+\n+\twhile (remaining) {\n+\t\tVector result_out(Type());\n+\t\tparquet_filter_t filter;\n+\t\tidx_t to_read = MinValue<idx_t>(remaining, STANDARD_VECTOR_SIZE);\n+\t\tread += Read(to_read, filter, define_out.get(), repeat_out.get(), result_out);\n+\t\tremaining -= to_read;\n+\t}\n+\n+\tif (read != num_values) {\n+\t\tthrow InternalException(\"Not all skips done!\");\n+\t}\n }\n //===--------------------------------------------------------------------===//\n // Cast Column Reader\n",
  "test_patch": "diff --git a/test/sql/copy/parquet/parquet_3989.test b/test/sql/copy/parquet/parquet_3989.test\nnew file mode 100644\nindex 000000000000..719b21d7f3dd\n--- /dev/null\n+++ b/test/sql/copy/parquet/parquet_3989.test\n@@ -0,0 +1,16 @@\n+# name: test/sql/copy/parquet/parquet_3989.test\n+# description: Issue #3989: Skipping more than 1024 values on list column fails\n+# group: [parquet]\n+\n+require parquet\n+\n+statement ok\n+CREATE TABLE lists as SELECT i as id, [i] as list from range(0,10000) tbl(i);\n+\n+statement ok\n+COPY lists to '__TEST_DIR__/list_bug_test.parquet';\n+\n+query I\n+SELECT list from '__TEST_DIR__/list_bug_test.parquet' where id = 5000;\n+----\n+[5000]\n",
  "problem_statement": "heap-buffer-overflow with query against imdb parquet files\n#### What happens?\r\nIn 0.3.4, [this query](https://gist.github.com/whscullin/558a3ef92574958ceddd2b6efe9a2cab) executed without issue on both Linux and MacOS. After 0.4.0 and continue to the latest master as of this morning, it dies with (using a debug build):\r\n\r\n```\r\n==99888==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x6290002e8200 at pc 0x00011511fec0 bp 0x70000ea18600 sp 0x70000ea185f8\r\nWRITE of size 8 at 0x6290002e8200 thread T5\r\n    #0 0x11511febf in duckdb::ListColumnReader::Read(unsigned long long, std::__1::bitset<1024ul>&, unsigned char*, unsigned char*, duckdb::Vector&) column_reader.cpp:647\r\n    #1 0x1151237e9 in duckdb::ListColumnReader::ApplyPendingSkips(unsigned long long) column_reader.cpp:710\r\n    #2 0x11511e5a8 in duckdb::ListColumnReader::Read(unsigned long long, std::__1::bitset<1024ul>&, unsigned char*, unsigned char*, duckdb::Vector&) column_reader.cpp:592\r\n    #3 0x114f8beaf in duckdb::ParquetReader::ScanInternal(duckdb::ParquetReaderScanState&, duckdb::DataChunk&) parquet_reader.cpp:949\r\n    #4 0x114f877d5 in duckdb::ParquetReader::Scan(duckdb::ParquetReaderScanState&, duckdb::DataChunk&) parquet_reader.cpp:798\r\n    #5 0x114e9f76b in duckdb::ParquetScanFunction::ParquetScanImplementation(duckdb::ClientContext&, duckdb::TableFunctionInput&, duckdb::DataChunk&) parquet-extension.cpp:295\r\n    #6 0x112ce6708 in duckdb::PhysicalTableScan::GetData(duckdb::ExecutionContext&, duckdb::DataChunk&, duckdb::GlobalSourceState&, duckdb::LocalSourceState&) const physical_table_scan.cpp:72\r\n    #7 0x113998870 in duckdb::PipelineExecutor::FetchFromSource(duckdb::DataChunk&) pipeline_executor.cpp:334\r\n    #8 0x1139981d2 in duckdb::PipelineExecutor::Execute(unsigned long long) pipeline_executor.cpp:59\r\n    #9 0x11399b5c0 in duckdb::PipelineExecutor::Execute() pipeline_executor.cpp:78\r\n    #10 0x113a44006 in duckdb::PipelineTask::ExecuteTask(duckdb::TaskExecutionMode) pipeline.cpp:43\r\n    #11 0x11395fc46 in duckdb::ExecutorTask::Execute(duckdb::TaskExecutionMode) executor_task.cpp:17\r\n    #12 0x1139a2660 in duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) task_scheduler.cpp:135\r\n    #13 0x1139a43c8 in duckdb::ThreadExecuteTasks(duckdb::TaskScheduler*, std::__1::atomic<bool>*) task_scheduler.cpp:166\r\n    #14 0x113aa1b72 in decltype(static_cast<void (*>(fp)(static_cast<duckdb::TaskScheduler*>(fp0), static_cast<std::__1::atomic<bool>*>(fp0))) std::__1::__invoke<void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>(void (*&&)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*&&, std::__1::atomic<bool>*&&) type_traits:3918\r\n    #15 0x113aa1968 in void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*, 2ul, 3ul>(std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>&, std::__1::__tuple_indices<2ul, 3ul>) thread:287\r\n    #16 0x113a9ef06 in void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*> >(void*) thread:298\r\n    #17 0x7ff808bf34e0 in _pthread_start+0x7c (libsystem_pthread.dylib:x86_64+0x64e0)\r\n    #18 0x7ff808beef6a in thread_start+0xe (libsystem_pthread.dylib:x86_64+0x1f6a)\r\n    ...\r\n```\r\n\r\n#### To Reproduce\r\nSteps to reproduce the behavior. Bonus points if those are only SQL queries.\r\n\r\n1.  Create a directory with `names.parquet`, `principals.parquet` and `titles.parquet` from the imdb dataset.\r\n1. On MacOS, run [this query](https://gist.github.com/whscullin/558a3ef92574958ceddd2b6efe9a2cab) in that directory using the duckdb cli.\r\n1. Interestingly, removing the ` and(movies_plus.\"startYear\"<'2020')` from line 50 will prevent the crash.\r\n\r\n#### Environment (please complete the following information):\r\n - OS: x86 MacOS\r\n - DuckDB Version: 0.40 - latest master\r\n - DuckDB Client: Both duckdb CLI and node\r\n\r\n#### Identity Disclosure:\r\n - Full Name: Will Scullin\r\n - Affiliation: Google\r\n\r\n#### Before Submitting\r\n\r\n- [x] **Have you tried this on the latest `master` branch?**\r\n\n",
  "hints_text": "Thanks for the report! I suspect this is related to the changes made to the filter pushdown for lists in the latest release. Could you perhaps share the Parquet files (e.g. email to mark@duckdblabs.com) or share where we can find them? I suspect we will need the exact Parquet files as this is likely metadata dependent. \nYou are correct, they were built with 0.3.3 or 0.3.4, rebuilding them with 0.4.0 they work again. I emailed you a download link.\nThanks! I will have a look.",
  "created_at": "2022-07-05T08:08:58Z"
}