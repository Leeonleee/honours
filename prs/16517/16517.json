{
  "repo": "duckdb/duckdb",
  "pull_number": 16517,
  "instance_id": "duckdb__duckdb-16517",
  "issue_numbers": [
    "16339"
  ],
  "base_commit": "d0c7224b40408132d221f3424c4dc9c4dfc8b366",
  "patch": "diff --git a/.github/config/out_of_tree_extensions.cmake b/.github/config/out_of_tree_extensions.cmake\nindex 938bbe5f3f83..461442a9d3ba 100644\n--- a/.github/config/out_of_tree_extensions.cmake\n+++ b/.github/config/out_of_tree_extensions.cmake\n@@ -100,7 +100,7 @@ if (NOT MINGW AND NOT ${WASM_ENABLED})\n     duckdb_extension_load(postgres_scanner\n             DONT_LINK\n             GIT_URL https://github.com/duckdb/duckdb-postgres\n-            GIT_TAG 79fcce4a7478d245189d851ce289def2b42f4f93\n+            GIT_TAG 8461ed8b6f726564934e9c831cdc88d431e3148f\n             )\n endif()\n \n@@ -134,7 +134,7 @@ duckdb_extension_load(sqlite_scanner\n duckdb_extension_load(sqlsmith\n         DONT_LINK LOAD_TESTS\n         GIT_URL https://github.com/duckdb/duckdb-sqlsmith\n-        GIT_TAG b13723fe701f1e38d2cd65b3b6eb587c6553a251\n+        GIT_TAG e1eb0ae02a258f176d6e06b84c0d6c7a09c6b4da\n         )\n \n ################# VSS\n@@ -162,6 +162,6 @@ duckdb_extension_load(fts\n         LOAD_TESTS\n         DONT_LINK\n         GIT_URL https://github.com/duckdb/duckdb-fts\n-        GIT_TAG 0477abaf2484aa7b9aabf8ace9dc0bde80a15554\n+        GIT_TAG 3aa6a180b9c101d78070f5f7214c27552bb091c8\n         TEST_DIR test/sql\n )\ndiff --git a/.github/workflows/CodeQuality.yml b/.github/workflows/CodeQuality.yml\nindex 7094970312d0..7a54054e02f7 100644\n--- a/.github/workflows/CodeQuality.yml\n+++ b/.github/workflows/CodeQuality.yml\n@@ -36,7 +36,7 @@ env:\n jobs:\n   format-check:\n     name: Format Check\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n \n     env:\n       CC: gcc-10\n@@ -50,7 +50,7 @@ jobs:\n \n       - name: Install\n         shell: bash\n-        run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build clang-format-11 && sudo pip3 install cmake-format black cxxheaderparser pcpp\n+        run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build clang-format-11 && sudo pip3 install cmake-format 'black==24.*' cxxheaderparser pcpp 'clang_format==11.0.1'\n \n       - name: List Installed Packages\n         shell: bash\n@@ -73,7 +73,7 @@ jobs:\n   enum-check:\n     name: C Enum Integrity Check\n     needs: format-check\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n \n     env:\n       CC: gcc-10\ndiff --git a/.github/workflows/ExtraTests.yml b/.github/workflows/ExtraTests.yml\nindex d91653df9e73..7d1818e1f2ee 100644\n--- a/.github/workflows/ExtraTests.yml\n+++ b/.github/workflows/ExtraTests.yml\n@@ -12,7 +12,7 @@ env:\n jobs:\n  regression-test-all:\n   name: All Regression Tests\n-  runs-on: ubuntu-20.04\n+  runs-on: ubuntu-22.04\n   env:\n     CC: gcc-10\n     CXX: g++-10\ndiff --git a/.github/workflows/LinuxRelease.yml b/.github/workflows/LinuxRelease.yml\nindex 9cd5a448fc8d..15d0cfd1970b 100644\n--- a/.github/workflows/LinuxRelease.yml\n+++ b/.github/workflows/LinuxRelease.yml\n@@ -51,7 +51,6 @@ env:\n jobs:\n  linux-release-cli:\n     needs: linux-extensions-64\n-    if: ${{ github.ref == 'refs/heads/main' || github.ref == 'refs/heads/feature' }}\n \n     strategy:\n       fail-fast: false\n@@ -128,7 +127,6 @@ jobs:\n         build/release/benchmark/benchmark_runner benchmark/micro/update/update_with_join.benchmark\n         build/release/duckdb -c \"COPY (SELECT 42) TO '/dev/stdout' (FORMAT PARQUET)\" | cat\n \n-\n  # Linux extensions for builds that use C++11 ABI, currently these are all linux builds based on ubuntu >= 18 (e.g. NodeJS)\n  # note that the linux-release-64 is based on the manylinux-based extensions, which are built in .github/workflows/Python.yml\n  linux-extensions-64:\n@@ -258,7 +256,7 @@ jobs:\n  check-load-install-extensions:\n     name: Checks extension entries\n     if: ${{ inputs.skip_tests != 'true' }}\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n     needs: linux-extensions-64\n     env:\n       CC: gcc-10\n@@ -307,6 +305,7 @@ jobs:\n         python scripts/generate_extensions_function.py\n         pip install \"black>=24\"\n         pip install cmake-format\n+        pip install \"clang_format==11.0.1\"\n         make format-fix\n \n     - uses: actions/upload-artifact@v4\n@@ -328,7 +327,7 @@ jobs:\n  symbol-leakage:\n     name: Symbol Leakage\n     if: ${{ inputs.skip_tests != 'true' }}\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n     needs: linux-extensions-64\n \n     steps:\n@@ -359,7 +358,7 @@ jobs:\n     name: Amalgamation Tests\n     if: ${{ inputs.skip_tests != 'true' }}\n     needs: linux-extensions-64\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n     env:\n       CC: clang\n       CXX: clang++\n@@ -377,7 +376,7 @@ jobs:\n     - name: Install LLVM and Clang\n       uses: KyleMayes/install-llvm-action@v1\n       with:\n-        version: \"10.0\"\n+        version: \"14.0\"\n \n     - name: Generate Amalgamation\n       shell: bash\ndiff --git a/.github/workflows/Main.yml b/.github/workflows/Main.yml\nindex 2f78f4743e76..f12270172436 100644\n--- a/.github/workflows/Main.yml\n+++ b/.github/workflows/Main.yml\n@@ -37,7 +37,7 @@ jobs:\n  linux-debug:\n     name: Linux Debug (${{ matrix.tag }})\n     if: ${{ !startsWith(github.ref, 'refs/tags/v') }}\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n     strategy:\n       fail-fast: false\n       matrix:\n@@ -124,7 +124,7 @@ jobs:\n  force-storage:\n     name: Force Storage\n     if: ${{ !startsWith(github.ref, 'refs/tags/v') }}\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n     env:\n       CC: gcc-10\n       CXX: g++-10\n@@ -161,7 +161,7 @@ jobs:\n  force-restart:\n     name: Force Restart\n     if: ${{ !startsWith(github.ref, 'refs/tags/v') }}\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n     needs: force-storage\n     env:\n       CC: gcc-10\n@@ -198,7 +198,7 @@ jobs:\n  valgrind:\n     name: Valgrind\n     if: ${{ !startsWith(github.ref, 'refs/tags/v') }}\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n     needs: force-storage\n     env:\n       CC: gcc-10\ndiff --git a/.github/workflows/NightlyTests.yml b/.github/workflows/NightlyTests.yml\nindex 8bdb4c96c0f7..ed8d9ee3a2e5 100644\n--- a/.github/workflows/NightlyTests.yml\n+++ b/.github/workflows/NightlyTests.yml\n@@ -65,7 +65,7 @@ jobs:\n \n   release-assert:\n     name: Release Assertions\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n     needs: linux-memory-leaks\n     env:\n       CC: gcc-10\n@@ -140,7 +140,7 @@ jobs:\n \n   linux-clang:\n     name: Clang 14\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n     needs: linux-memory-leaks\n     env:\n       CC: /home/runner/work/llvm/bin/clang\n@@ -235,7 +235,7 @@ jobs:\n \n   linux-tarball:\n      name: Python 3 Tarball\n-     runs-on: ubuntu-20.04\n+     runs-on: ubuntu-22.04\n      needs: linux-memory-leaks\n      steps:\n      - uses: actions/checkout@v3\n@@ -274,7 +274,7 @@ jobs:\n \n   python-sqllogic:\n      name: Python SQLLogicTest Library (Linux)\n-     runs-on: ubuntu-20.04\n+     runs-on: ubuntu-22.04\n      env:\n         GEN: ninja\n         DUCKDEBUG: 1\n@@ -333,7 +333,7 @@ jobs:\n \n   storage-initialization:\n      name: Storage Initialization Verification\n-     runs-on: ubuntu-20.04\n+     runs-on: ubuntu-22.04\n      needs: linux-memory-leaks\n      env:\n        CC: gcc-10\n@@ -365,7 +365,7 @@ jobs:\n \n   extension-updating:\n     name: Extension updating test\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n     needs: linux-memory-leaks\n     env:\n       CC: gcc-10\n@@ -451,7 +451,7 @@ jobs:\n \n   latest-storage:\n     name: Latest Storage\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n     needs: linux-memory-leaks\n     env:\n       CC: gcc-10\n@@ -488,7 +488,7 @@ jobs:\n \n   vector-verification:\n     name: Vector Verification Tests (${{ matrix.vector_type }})\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n     needs: linux-memory-leaks\n     strategy:\n       fail-fast: false\n@@ -568,7 +568,7 @@ jobs:\n \n   regression-test-memory-safety:\n    name: Regression Tests between safe and unsafe builds\n-   runs-on: ubuntu-20.04\n+   runs-on: ubuntu-22.04\n    needs: linux-memory-leaks\n    env:\n      CC: gcc-10\n@@ -693,7 +693,7 @@ jobs:\n \n   vector-sizes:\n     name: Vector Sizes\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n     needs: linux-memory-leaks\n     env:\n       CC: gcc-10\n@@ -845,7 +845,7 @@ jobs:\n \n   codecov:\n     name: Code Coverage\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n     needs: linux-memory-leaks\n     strategy:\n       fail-fast: false\n@@ -893,7 +893,7 @@ jobs:\n \n   linux-httpfs:\n     name: Linux HTTPFS\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n     env:\n       CORE_EXTENSIONS: \"json;parquet;tpch;tpcds;httpfs\"\n       S3_TEST_SERVER_AVAILABLE: 1\ndiff --git a/.github/workflows/Python.yml b/.github/workflows/Python.yml\nindex b63b0b82caf7..b3608dc25498 100644\n--- a/.github/workflows/Python.yml\n+++ b/.github/workflows/Python.yml\n@@ -65,7 +65,7 @@ jobs:\n # This is just a sanity check of Python 3.10 running with Arrow\n   linux-python3-10:\n     name: Python 3.10 Linux\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n \n     env:\n       CIBW_BUILD: 'cp39-manylinux_x86_64'\n@@ -435,7 +435,7 @@ jobs:\n      if: startsWith(github.ref, 'refs/tags/v') || github.ref == 'refs/heads/main'\n      name: PyPi Release Cleanup\n      needs: twine-upload\n-     runs-on: ubuntu-20.04\n+     runs-on: ubuntu-22.04\n      env:\n        PYPI_CLEANUP_USERNAME: 'mytherin'\n        PYPI_CLEANUP_PASSWORD: ${{secrets.PYPI_CLEANUP_PASSWORD}}\ndiff --git a/.github/workflows/Regression.yml b/.github/workflows/Regression.yml\nindex 9d840701299e..39b475d21d1e 100644\n--- a/.github/workflows/Regression.yml\n+++ b/.github/workflows/Regression.yml\n@@ -45,7 +45,7 @@ env:\n jobs:\n   regression-test-benchmark-runner:\n     name: Regression Tests\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n     env:\n       CC: gcc-10\n       CXX: g++-10\n@@ -182,7 +182,7 @@ jobs:\n \n   regression-test-storage:\n     name: Storage Size Regression Test\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n     env:\n       CC: gcc-10\n       CXX: g++-10\n@@ -262,7 +262,7 @@ jobs:\n \n   regression-test-binary-size:\n     name: Regression test binary size\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n     env:\n       CC: gcc-10\n       CXX: g++-10\n@@ -304,7 +304,7 @@ jobs:\n \n   regression-test-plan-cost:\n     name: Regression Test Join Order Plan Cost\n-    runs-on: ubuntu-20.04\n+    runs-on: ubuntu-22.04\n     env:\n       CC: gcc-10\n       CXX: g++-10\ndiff --git a/extension/json/json_multi_file_info.cpp b/extension/json/json_multi_file_info.cpp\nindex 01da28d18a61..ca91fc941b55 100644\n--- a/extension/json/json_multi_file_info.cpp\n+++ b/extension/json/json_multi_file_info.cpp\n@@ -316,8 +316,32 @@ void JSONMultiFileInfo::BindReader(ClientContext &context, vector<LogicalType> &\n \t\t\tname_collision_count[col_name] = 0;\n \t\t}\n \t}\n-\t// FIXME: re-use readers created during auto-detection instead of clearing here\n-\tbind_data.union_readers.clear();\n+\tbool reuse_readers = true;\n+\tfor (auto &union_reader : bind_data.union_readers) {\n+\t\tif (!union_reader || !union_reader->reader) {\n+\t\t\t// not all readers have been initialized - don't re-use any\n+\t\t\treuse_readers = false;\n+\t\t\tbreak;\n+\t\t}\n+\t\tauto &json_reader = union_reader->reader->Cast<JSONReader>();\n+\t\tif (!json_reader.IsOpen()) {\n+\t\t\t// no open file-handle - close\n+\t\t\treuse_readers = false;\n+\t\t}\n+\t}\n+\tif (!reuse_readers) {\n+\t\tbind_data.union_readers.clear();\n+\t} else {\n+\t\t// re-use readers\n+\t\tfor (auto &union_reader : bind_data.union_readers) {\n+\t\t\tauto &json_reader = union_reader->reader->Cast<JSONReader>();\n+\t\t\tunion_reader->names = names;\n+\t\t\tunion_reader->types = return_types;\n+\t\t\tunion_reader->reader->columns =\n+\t\t\t    MultiFileReaderColumnDefinition::ColumnsFromNamesAndTypes(names, return_types);\n+\t\t\tjson_reader.Reset();\n+\t\t}\n+\t}\n }\n \n void JSONMultiFileInfo::FinalizeBindData(MultiFileBindData &multi_file_data) {\n@@ -427,8 +451,10 @@ bool JSONMultiFileInfo::TryInitializeScan(ClientContext &context, shared_ptr<Bas\n                                           GlobalTableFunctionState &gstate_p, LocalTableFunctionState &lstate_p) {\n \tauto &gstate = gstate_p.Cast<JSONGlobalTableFunctionState>().state;\n \tauto &lstate = lstate_p.Cast<JSONLocalTableFunctionState>().state;\n+\tauto &json_reader = reader->Cast<JSONReader>();\n+\n \tlstate.GetScanState().ResetForNextBuffer();\n-\treturn lstate.TryInitializeScan(gstate, reader->Cast<JSONReader>());\n+\treturn lstate.TryInitializeScan(gstate, json_reader);\n }\n \n void ReadJSONFunction(ClientContext &context, JSONReader &json_reader, JSONScanGlobalState &gstate,\ndiff --git a/extension/json/json_reader.cpp b/extension/json/json_reader.cpp\nindex bf3f705de769..dc63a7a3d641 100644\n--- a/extension/json/json_reader.cpp\n+++ b/extension/json/json_reader.cpp\n@@ -37,7 +37,7 @@ void JSONFileHandle::Reset() {\n \trequested_reads = 0;\n \tactual_reads = 0;\n \tlast_read_requested = false;\n-\tif (IsOpen() && !file_handle->IsPipe()) {\n+\tif (IsOpen() && !IsPipe()) {\n \t\tfile_handle->Reset();\n \t}\n }\n@@ -90,25 +90,12 @@ bool JSONFileHandle::GetPositionAndSize(idx_t &position, idx_t &size, idx_t requ\n \n void JSONFileHandle::ReadAtPosition(char *pointer, idx_t size, idx_t position,\n                                     optional_ptr<FileHandle> override_handle) {\n+\tif (IsPipe()) {\n+\t\tthrow InternalException(\"ReadAtPosition is not supported for pipes\");\n+\t}\n \tif (size != 0) {\n \t\tauto &handle = override_handle ? *override_handle.get() : *file_handle.get();\n-\t\tif (can_seek) {\n-\t\t\thandle.Read(pointer, size, position);\n-\t\t} else if (file_handle->IsPipe()) { // Cache the buffer\n-\t\t\thandle.Read(pointer, size, position);\n-\n-\t\t\tcached_buffers.emplace_back(allocator.Allocate(size));\n-\t\t\tmemcpy(cached_buffers.back().get(), pointer, size);\n-\t\t\tcached_size += size;\n-\t\t} else {\n-\t\t\tif (!cached_buffers.empty() || position < cached_size) {\n-\t\t\t\tReadFromCache(pointer, size, position);\n-\t\t\t}\n-\n-\t\t\tif (size != 0) {\n-\t\t\t\thandle.Read(pointer, size, position);\n-\t\t\t}\n-\t\t}\n+\t\thandle.Read(pointer, size, position);\n \t}\n \n \tconst auto incremented_actual_reads = ++actual_reads;\n@@ -127,26 +114,19 @@ bool JSONFileHandle::Read(char *pointer, idx_t &read_size, idx_t requested_size)\n \t\treturn false;\n \t}\n \n-\tif (can_seek) {\n-\t\tread_size = ReadInternal(pointer, requested_size);\n-\t\tread_position += read_size;\n-\t} else if (file_handle->IsPipe()) { // Cache the buffer\n-\t\tread_size = ReadInternal(pointer, requested_size);\n-\t\tif (read_size > 0) {\n-\t\t\tcached_buffers.emplace_back(allocator.Allocate(read_size));\n-\t\t\tmemcpy(cached_buffers.back().get(), pointer, read_size);\n-\t\t}\n-\t\tcached_size += read_size;\n-\t\tread_position += read_size;\n-\t} else {\n-\t\tread_size = 0;\n-\t\tif (!cached_buffers.empty() || read_position < cached_size) {\n-\t\t\tread_size += ReadFromCache(pointer, requested_size, read_position);\n-\t\t}\n-\t\tif (requested_size != 0) {\n-\t\t\tread_size += ReadInternal(pointer, requested_size);\n-\t\t}\n+\tread_size = 0;\n+\tif (!cached_buffers.empty() || read_position < cached_size) {\n+\t\tread_size += ReadFromCache(pointer, requested_size, read_position);\n+\t}\n+\n+\tauto temp_read_size = ReadInternal(pointer, requested_size);\n+\tif (IsPipe() && temp_read_size != 0) { // Cache the buffer\n+\t\tcached_buffers.emplace_back(allocator.Allocate(temp_read_size));\n+\t\tmemcpy(cached_buffers.back().get(), pointer, temp_read_size);\n+\t\tcached_size += temp_read_size;\n \t}\n+\tread_position += temp_read_size;\n+\tread_size += temp_read_size;\n \n \tif (read_size == 0) {\n \t\tlast_read_requested = true;\n@@ -724,8 +704,12 @@ void JSONReader::AutoDetect(Allocator &allocator, idx_t buffer_capacity) {\n \t\t    \"JSON auto-detection error in file \\\"%s\\\": Expected records, detected non-record JSON instead\", file_name);\n \t}\n \t// store the buffer in the file so it can be re-used by the first reader of the file\n-\tauto_detect_data = std::move(read_buffer);\n-\tauto_detect_data_size = read_size;\n+\tif (!file_handle->IsPipe()) {\n+\t\tauto_detect_data = std::move(read_buffer);\n+\t\tauto_detect_data_size = read_size;\n+\t} else {\n+\t\tfile_handle->Reset();\n+\t}\n }\n \n void JSONReader::ThrowObjectSizeError(const idx_t object_size) {\ndiff --git a/scripts/exported_symbols_check.py b/scripts/exported_symbols_check.py\nindex 984662e12b9e..ede58c3bf4e9 100644\n--- a/scripts/exported_symbols_check.py\n+++ b/scripts/exported_symbols_check.py\n@@ -13,8 +13,8 @@\n culprits = []\n \n whitelist = [\n-    '@@GLIBC',\n-    '@@CXXABI',\n+    '@GLIBC',\n+    '@CXXABI',\n     '__gnu_cxx::',\n     'std::',\n     'N6duckdb',\ndiff --git a/src/execution/operator/csv_scanner/buffer_manager/csv_buffer.cpp b/src/execution/operator/csv_scanner/buffer_manager/csv_buffer.cpp\nindex 8fcfad10340e..575d69d7d9bf 100644\n--- a/src/execution/operator/csv_scanner/buffer_manager/csv_buffer.cpp\n+++ b/src/execution/operator/csv_scanner/buffer_manager/csv_buffer.cpp\n@@ -4,7 +4,7 @@\n namespace duckdb {\n \n CSVBuffer::CSVBuffer(ClientContext &context, idx_t buffer_size_p, CSVFileHandle &file_handle,\n-                     idx_t &global_csv_current_position)\n+                     const idx_t &global_csv_current_position)\n     : context(context), requested_size(buffer_size_p), can_seek(file_handle.CanSeek()), is_pipe(file_handle.IsPipe()) {\n \tAllocateBuffer(buffer_size_p);\n \tauto buffer = Ptr();\n@@ -31,7 +31,7 @@ CSVBuffer::CSVBuffer(CSVFileHandle &file_handle, ClientContext &context, idx_t b\n \tlast_buffer = file_handle.FinishedReading();\n }\n \n-shared_ptr<CSVBuffer> CSVBuffer::Next(CSVFileHandle &file_handle, idx_t buffer_size, bool &has_seaked) {\n+shared_ptr<CSVBuffer> CSVBuffer::Next(CSVFileHandle &file_handle, idx_t buffer_size, bool &has_seaked) const {\n \tif (has_seaked) {\n \t\t// This means that at some point a reload was done, and we are currently on the incorrect position in our file\n \t\t// handle\ndiff --git a/src/execution/operator/csv_scanner/encode/csv_encoder.cpp b/src/execution/operator/csv_scanner/encode/csv_encoder.cpp\nindex 8a6c08032597..d57e95732467 100644\n--- a/src/execution/operator/csv_scanner/encode/csv_encoder.cpp\n+++ b/src/execution/operator/csv_scanner/encode/csv_encoder.cpp\n@@ -36,7 +36,7 @@ void CSVEncoderBuffer::Reset() {\n \tactual_encoded_buffer_size = 0;\n }\n \n-CSVEncoder::CSVEncoder(DBConfig &config, const string &encoding_name_to_find, idx_t buffer_size) {\n+CSVEncoder::CSVEncoder(const DBConfig &config, const string &encoding_name_to_find, idx_t buffer_size) {\n \tencoding_name = StringUtil::Lower(encoding_name_to_find);\n \tauto function = config.GetEncodeFunction(encoding_name_to_find);\n \tif (!function) {\ndiff --git a/src/execution/operator/csv_scanner/scanner/base_scanner.cpp b/src/execution/operator/csv_scanner/scanner/base_scanner.cpp\nindex 757598e140ee..41f5d50cfa4c 100644\n--- a/src/execution/operator/csv_scanner/scanner/base_scanner.cpp\n+++ b/src/execution/operator/csv_scanner/scanner/base_scanner.cpp\n@@ -11,9 +11,10 @@ ScannerResult::ScannerResult(CSVStates &states_p, CSVStateMachine &state_machine\n \n BaseScanner::BaseScanner(shared_ptr<CSVBufferManager> buffer_manager_p, shared_ptr<CSVStateMachine> state_machine_p,\n                          shared_ptr<CSVErrorHandler> error_handler_p, bool sniffing_p,\n-                         shared_ptr<CSVFileScan> csv_file_scan_p, CSVIterator iterator_p)\n+                         shared_ptr<CSVFileScan> csv_file_scan_p, const CSVIterator &iterator_p)\n     : csv_file_scan(std::move(csv_file_scan_p)), sniffing(sniffing_p), error_handler(std::move(error_handler_p)),\n-      state_machine(std::move(state_machine_p)), buffer_manager(std::move(buffer_manager_p)), iterator(iterator_p) {\n+      state_machine(std::move(state_machine_p)), states(), buffer_manager(std::move(buffer_manager_p)),\n+      iterator(iterator_p) {\n \tD_ASSERT(buffer_manager);\n \tD_ASSERT(state_machine);\n \t// Initialize current buffer handle\ndiff --git a/src/execution/operator/csv_scanner/scanner/csv_schema.cpp b/src/execution/operator/csv_scanner/scanner/csv_schema.cpp\nindex f99e46a9a614..866c9c1fdffa 100644\n--- a/src/execution/operator/csv_scanner/scanner/csv_schema.cpp\n+++ b/src/execution/operator/csv_scanner/scanner/csv_schema.cpp\n@@ -76,8 +76,8 @@ void CSVSchema::MergeSchemas(CSVSchema &other, bool null_padding) {\n \t}\n }\n \n-CSVSchema::CSVSchema(vector<string> &names, vector<LogicalType> &types, const string &file_path, idx_t rows_read_p,\n-                     const bool empty_p)\n+CSVSchema::CSVSchema(const vector<string> &names, const vector<LogicalType> &types, const string &file_path,\n+                     idx_t rows_read_p, const bool empty_p)\n     : rows_read(rows_read_p), empty(empty_p) {\n \tInitialize(names, types, file_path);\n }\ndiff --git a/src/execution/operator/csv_scanner/scanner/scanner_boundary.cpp b/src/execution/operator/csv_scanner/scanner/scanner_boundary.cpp\nindex b0511bec64ab..bdb324d61057 100644\n--- a/src/execution/operator/csv_scanner/scanner/scanner_boundary.cpp\n+++ b/src/execution/operator/csv_scanner/scanner/scanner_boundary.cpp\n@@ -13,7 +13,7 @@ CSVBoundary::CSVBoundary(idx_t buffer_idx_p, idx_t buffer_pos_p, idx_t boundary_\n CSVBoundary::CSVBoundary() : buffer_idx(0), buffer_pos(0), boundary_idx(0), end_pos(NumericLimits<idx_t>::Maximum()) {\n }\n \n-CSVIterator::CSVIterator() : is_set(false) {\n+CSVIterator::CSVIterator() : buffer_size(0), is_set(false) {\n }\n \n void CSVBoundary::Print() const {\ndiff --git a/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp b/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp\nindex 770afa62ed20..0d6e2c9eb3f0 100644\n--- a/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp\n+++ b/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp\n@@ -970,7 +970,8 @@ StringValueScanner::StringValueScanner(idx_t scanner_idx_p, const shared_ptr<CSV\n       result(states, *state_machine, cur_buffer_handle, BufferAllocator::Get(buffer_manager->context), result_size,\n              iterator.pos.buffer_pos, *error_handler, iterator,\n              buffer_manager->context.client_data->debug_set_max_line_length, csv_file_scan, lines_read, sniffing,\n-             buffer_manager->GetFilePath(), scanner_idx_p) {\n+             buffer_manager->GetFilePath(), scanner_idx_p),\n+      start_pos(0) {\n \titerator.buffer_size = state_machine->options.buffer_size_option.GetValue();\n }\n \n@@ -982,7 +983,8 @@ StringValueScanner::StringValueScanner(const shared_ptr<CSVBufferManager> &buffe\n       result(states, *state_machine, cur_buffer_handle, Allocator::DefaultAllocator(), result_size,\n              iterator.pos.buffer_pos, *error_handler, iterator,\n              buffer_manager->context.client_data->debug_set_max_line_length, csv_file_scan, lines_read, sniffing,\n-             buffer_manager->GetFilePath(), 0) {\n+             buffer_manager->GetFilePath(), 0),\n+      start_pos(0) {\n \titerator.buffer_size = state_machine->options.buffer_size_option.GetValue();\n }\n \ndiff --git a/src/execution/operator/csv_scanner/sniffer/type_refinement.cpp b/src/execution/operator/csv_scanner/sniffer/type_refinement.cpp\nindex 8d3e268450c4..98d668661dc7 100644\n--- a/src/execution/operator/csv_scanner/sniffer/type_refinement.cpp\n+++ b/src/execution/operator/csv_scanner/sniffer/type_refinement.cpp\n@@ -2,7 +2,7 @@\n #include \"duckdb/execution/operator/csv_scanner/csv_casting.hpp\"\n \n namespace duckdb {\n-bool CSVSniffer::TryCastVector(Vector &parse_chunk_col, idx_t size, const LogicalType &sql_type) {\n+bool CSVSniffer::TryCastVector(Vector &parse_chunk_col, idx_t size, const LogicalType &sql_type) const {\n \tauto &sniffing_state_machine = best_candidate->GetStateMachine();\n \t// try vector-cast from string to sql_type\n \tVector dummy_result(sql_type, size);\ndiff --git a/src/include/duckdb/execution/operator/csv_scanner/base_scanner.hpp b/src/include/duckdb/execution/operator/csv_scanner/base_scanner.hpp\nindex b2d9dae68a39..85c37db87b66 100644\n--- a/src/include/duckdb/execution/operator/csv_scanner/base_scanner.hpp\n+++ b/src/include/duckdb/execution/operator/csv_scanner/base_scanner.hpp\n@@ -116,7 +116,7 @@ class BaseScanner {\n public:\n \texplicit BaseScanner(shared_ptr<CSVBufferManager> buffer_manager, shared_ptr<CSVStateMachine> state_machine,\n \t                     shared_ptr<CSVErrorHandler> error_handler, bool sniffing = false,\n-\t                     shared_ptr<CSVFileScan> csv_file_scan = nullptr, CSVIterator iterator = {});\n+\t                     shared_ptr<CSVFileScan> csv_file_scan = nullptr, const CSVIterator &iterator = {});\n \n \tvirtual ~BaseScanner() = default;\n \ndiff --git a/src/include/duckdb/execution/operator/csv_scanner/csv_buffer.hpp b/src/include/duckdb/execution/operator/csv_scanner/csv_buffer.hpp\nindex 586867043291..7ee7d60a3202 100644\n--- a/src/include/duckdb/execution/operator/csv_scanner/csv_buffer.hpp\n+++ b/src/include/duckdb/execution/operator/csv_scanner/csv_buffer.hpp\n@@ -44,14 +44,14 @@ class CSVBuffer {\n public:\n \t//! Constructor for Initial Buffer\n \tCSVBuffer(ClientContext &context, idx_t buffer_size_p, CSVFileHandle &file_handle,\n-\t          idx_t &global_csv_current_position);\n+\t          const idx_t &global_csv_current_position);\n \n \t//! Constructor for `Next()` Buffers\n \tCSVBuffer(CSVFileHandle &file_handle, ClientContext &context, idx_t buffer_size, idx_t global_csv_current_position,\n \t          idx_t buffer_idx);\n \n \t//! Creates a new buffer with the next part of the CSV File\n-\tshared_ptr<CSVBuffer> Next(CSVFileHandle &file_handle, idx_t buffer_size, bool &has_seeked);\n+\tshared_ptr<CSVBuffer> Next(CSVFileHandle &file_handle, idx_t buffer_size, bool &has_seeked) const;\n \n \t//! Gets the buffer actual size\n \tidx_t GetBufferSize() const;\n@@ -66,7 +66,7 @@ class CSVBuffer {\n \t//! Wrapper for the Pin Function, if it can seek, it means that the buffer might have been destroyed, hence we must\n \t//! Scan it from the disk file again.\n \tshared_ptr<CSVBufferHandle> Pin(CSVFileHandle &file_handle, bool &has_seeked);\n-\t//! Wrapper for the unpin\n+\t//! Wrapper for unpin\n \tvoid Unpin();\n \tchar *Ptr() {\n \t\treturn char_ptr_cast(handle.Ptr());\ndiff --git a/src/include/duckdb/execution/operator/csv_scanner/csv_schema.hpp b/src/include/duckdb/execution/operator/csv_scanner/csv_schema.hpp\nindex 3e7a90c99116..6048185c1b24 100644\n--- a/src/include/duckdb/execution/operator/csv_scanner/csv_schema.hpp\n+++ b/src/include/duckdb/execution/operator/csv_scanner/csv_schema.hpp\n@@ -14,7 +14,7 @@\n namespace duckdb {\n //! Basic CSV Column Info\n struct CSVColumnInfo {\n-\tCSVColumnInfo(string &name_p, LogicalType &type_p) : name(name_p), type(type_p) {\n+\tCSVColumnInfo(const string &name_p, const LogicalType &type_p) : name(name_p), type(type_p) {\n \t}\n \tstring name;\n \tLogicalType type;\n@@ -25,7 +25,7 @@ struct CSVSchema {\n \texplicit CSVSchema(const bool empty = false) : empty(empty) {\n \t}\n \n-\tCSVSchema(vector<string> &names, vector<LogicalType> &types, const string &file_path, idx_t rows_read,\n+\tCSVSchema(const vector<string> &names, const vector<LogicalType> &types, const string &file_path, idx_t rows_read,\n \t          const bool empty = false);\n \n \t//! Initializes the schema based on names and types\ndiff --git a/src/include/duckdb/execution/operator/csv_scanner/encode/csv_encoder.hpp b/src/include/duckdb/execution/operator/csv_scanner/encode/csv_encoder.hpp\nindex 764d9694dc64..1ced26fca470 100644\n--- a/src/include/duckdb/execution/operator/csv_scanner/encode/csv_encoder.hpp\n+++ b/src/include/duckdb/execution/operator/csv_scanner/encode/csv_encoder.hpp\n@@ -46,7 +46,7 @@ struct CSVEncoderBuffer {\n class CSVEncoder {\n public:\n \t//! Constructor, basically takes an encoding and the output buffer size\n-\tCSVEncoder(DBConfig &config, const string &encoding_name, idx_t buffer_size);\n+\tCSVEncoder(const DBConfig &config, const string &encoding_name, idx_t buffer_size);\n \t//! Main encode function, it reads the file into an encoded buffer and converts it to the output buffer\n \tidx_t Encode(FileHandle &file_handle_input, char *output_buffer, const idx_t decoded_buffer_size);\n \tstring encoding_name;\ndiff --git a/src/include/duckdb/execution/operator/csv_scanner/sniffer/csv_sniffer.hpp b/src/include/duckdb/execution/operator/csv_scanner/sniffer/csv_sniffer.hpp\nindex b1f9df8332e8..8b42cdeab924 100644\n--- a/src/include/duckdb/execution/operator/csv_scanner/sniffer/csv_sniffer.hpp\n+++ b/src/include/duckdb/execution/operator/csv_scanner/sniffer/csv_sniffer.hpp\n@@ -225,7 +225,7 @@ class CSVSniffer {\n \t//! ------------------ Type Refinement ------------------ //\n \t//! ------------------------------------------------------//\n \tvoid RefineTypes();\n-\tbool TryCastVector(Vector &parse_chunk_col, idx_t size, const LogicalType &sql_type);\n+\tbool TryCastVector(Vector &parse_chunk_col, idx_t size, const LogicalType &sql_type) const;\n \tvector<LogicalType> detected_types;\n \t//! If when finding a SQLNULL type in type detection we default it to varchar\n \tconst bool default_null_to_varchar;\ndiff --git a/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp b/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp\nindex 6d196fc666bd..a18a0eb5f0bd 100644\n--- a/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp\n+++ b/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp\n@@ -121,8 +121,8 @@ class LineError {\n };\n \n struct ParseTypeInfo {\n-\tParseTypeInfo() {};\n-\tParseTypeInfo(const LogicalType &type, bool validate_utf_8_p) : validate_utf8(validate_utf_8_p) {\n+\tParseTypeInfo() : validate_utf8(false), type_id(), internal_type(), scale(0), width(0) {};\n+\tParseTypeInfo(const LogicalType &type, const bool validate_utf_8_p) : validate_utf8(validate_utf_8_p) {\n \t\ttype_id = type.id();\n \t\tinternal_type = type.InternalType();\n \t\tif (type.id() == LogicalTypeId::DECIMAL) {\ndiff --git a/src/include/duckdb/main/client_context_state.hpp b/src/include/duckdb/main/client_context_state.hpp\nindex e7a10593f3e2..80cdc8b8f33c 100644\n--- a/src/include/duckdb/main/client_context_state.hpp\n+++ b/src/include/duckdb/main/client_context_state.hpp\n@@ -80,6 +80,10 @@ class ClientContextState {\n \t}\n \tvirtual void WriteProfilingInformation(std::ostream &ss) {\n \t}\n+\tvirtual void OnTaskStart(ClientContext &context) {\n+\t}\n+\tvirtual void OnTaskStop(ClientContext &context) {\n+\t}\n \n public:\n \ttemplate <class TARGET>\ndiff --git a/src/include/duckdb/parallel/executor_task.hpp b/src/include/duckdb/parallel/executor_task.hpp\nindex d4d483b88b3f..1790b5014cb5 100644\n--- a/src/include/duckdb/parallel/executor_task.hpp\n+++ b/src/include/duckdb/parallel/executor_task.hpp\n@@ -9,10 +9,10 @@\n #pragma once\n \n #include \"duckdb/parallel/task.hpp\"\n-#include \"duckdb/parallel/event.hpp\"\n #include \"duckdb/common/optional_ptr.hpp\"\n \n namespace duckdb {\n+class Event;\n class PhysicalOperator;\n class ThreadContext;\n \n@@ -34,6 +34,9 @@ class ExecutorTask : public Task {\n \tunique_ptr<ThreadContext> thread_context;\n \toptional_ptr<const PhysicalOperator> op;\n \n+private:\n+\tClientContext &context;\n+\n public:\n \tvirtual TaskExecutionResult ExecuteTask(TaskExecutionMode mode) = 0;\n \tTaskExecutionResult Execute(TaskExecutionMode mode) override;\ndiff --git a/src/include/duckdb/parallel/pipeline.hpp b/src/include/duckdb/parallel/pipeline.hpp\nindex 34dcdd4474b3..dd1614145e18 100644\n--- a/src/include/duckdb/parallel/pipeline.hpp\n+++ b/src/include/duckdb/parallel/pipeline.hpp\n@@ -20,7 +20,6 @@\n namespace duckdb {\n \n class Executor;\n-class Event;\n class MetaPipeline;\n class PipelineExecutor;\n class Pipeline;\ndiff --git a/src/include/duckdb/parallel/task_executor.hpp b/src/include/duckdb/parallel/task_executor.hpp\nindex 191b6df9d3e6..912ad80b3f6f 100644\n--- a/src/include/duckdb/parallel/task_executor.hpp\n+++ b/src/include/duckdb/parallel/task_executor.hpp\n@@ -10,6 +10,7 @@\n \n #include \"duckdb/common/common.hpp\"\n #include \"duckdb/common/atomic.hpp\"\n+#include \"duckdb/common/optional_ptr.hpp\"\n #include \"duckdb/parallel/task.hpp\"\n #include \"duckdb/execution/task_error_manager.hpp\"\n \n@@ -47,6 +48,8 @@ class TaskExecutor {\n \tunique_ptr<ProducerToken> token;\n \tatomic<idx_t> completed_tasks;\n \tatomic<idx_t> total_tasks;\n+\tfriend class BaseExecutorTask;\n+\toptional_ptr<ClientContext> context;\n };\n \n class BaseExecutorTask : public Task {\ndiff --git a/src/include/duckdb/parallel/task_notifier.hpp b/src/include/duckdb/parallel/task_notifier.hpp\nnew file mode 100644\nindex 000000000000..6bf34b3c043f\n--- /dev/null\n+++ b/src/include/duckdb/parallel/task_notifier.hpp\n@@ -0,0 +1,27 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/parallel/task_notifier.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/common/optional_ptr.hpp\"\n+\n+namespace duckdb {\n+class ClientContext;\n+\n+//! The TaskNotifier notifies ClientContextState listener about started / stopped tasks\n+class TaskNotifier {\n+public:\n+\texplicit TaskNotifier(optional_ptr<ClientContext> context_p);\n+\n+\t~TaskNotifier();\n+\n+private:\n+\toptional_ptr<ClientContext> context;\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/parallel/task_scheduler.hpp b/src/include/duckdb/parallel/task_scheduler.hpp\nindex c40e8e10d34d..ae1b52a956e7 100644\n--- a/src/include/duckdb/parallel/task_scheduler.hpp\n+++ b/src/include/duckdb/parallel/task_scheduler.hpp\n@@ -68,6 +68,10 @@ class TaskScheduler {\n \t//! Returns the number of threads\n \tDUCKDB_API int32_t NumberOfThreads();\n \n+\tidx_t GetNumberOfTasks() const;\n+\tidx_t GetProducerCount() const;\n+\tidx_t GetTaskCountForProducer(ProducerToken &token) const;\n+\n \t//! Send signals to n threads, signalling for them to wake up and attempt to execute a task\n \tvoid Signal(idx_t n);\n \ndiff --git a/src/include/duckdb/storage/checkpoint/table_data_writer.hpp b/src/include/duckdb/storage/checkpoint/table_data_writer.hpp\nindex a606b5615197..90b5e753261d 100644\n--- a/src/include/duckdb/storage/checkpoint/table_data_writer.hpp\n+++ b/src/include/duckdb/storage/checkpoint/table_data_writer.hpp\n@@ -23,7 +23,7 @@ class TableStatistics;\n //! Abstraction will support, for example: tiering, versioning, or splitting into multiple block managers.\n class TableDataWriter {\n public:\n-\texplicit TableDataWriter(TableCatalogEntry &table);\n+\texplicit TableDataWriter(TableCatalogEntry &table, optional_ptr<ClientContext> client_context);\n \tvirtual ~TableDataWriter();\n \n public:\n@@ -39,9 +39,11 @@ class TableDataWriter {\n \n \tTaskScheduler &GetScheduler();\n \tDatabaseInstance &GetDatabase();\n+\toptional_ptr<ClientContext> GetClientContext();\n \n protected:\n \tDuckTableEntry &table;\n+\toptional_ptr<ClientContext> client_context;\n \t//! Pointers to the start of each row group.\n \tvector<RowGroupPointer> row_group_pointers;\n };\ndiff --git a/src/include/duckdb/storage/checkpoint_manager.hpp b/src/include/duckdb/storage/checkpoint_manager.hpp\nindex cb0455deb258..3bb11912854b 100644\n--- a/src/include/duckdb/storage/checkpoint_manager.hpp\n+++ b/src/include/duckdb/storage/checkpoint_manager.hpp\n@@ -97,7 +97,8 @@ class SingleFileCheckpointWriter final : public CheckpointWriter {\n \tfriend class SingleFileTableDataWriter;\n \n public:\n-\tSingleFileCheckpointWriter(AttachedDatabase &db, BlockManager &block_manager, CheckpointType checkpoint_type);\n+\tSingleFileCheckpointWriter(optional_ptr<ClientContext> client_context, AttachedDatabase &db,\n+\t                           BlockManager &block_manager, CheckpointType checkpoint_type);\n \n \t//! Checkpoint the current state of the WAL and flush it to the main storage. This should be called BEFORE any\n \t//! connection is available because right now the checkpointing cannot be done online. (TODO)\n@@ -112,10 +113,15 @@ class SingleFileCheckpointWriter final : public CheckpointWriter {\n \t\treturn checkpoint_type;\n \t}\n \n+\toptional_ptr<ClientContext> GetClientContext() const {\n+\t\treturn client_context;\n+\t}\n+\n public:\n \tvoid WriteTable(TableCatalogEntry &table, Serializer &serializer) override;\n \n private:\n+\toptional_ptr<ClientContext> client_context;\n \t//! The metadata writer is responsible for writing schema information\n \tunique_ptr<MetadataWriter> metadata_writer;\n \t//! The table data writer is responsible for writing the DataPointers used by the table chunks\ndiff --git a/src/include/duckdb/storage/storage_manager.hpp b/src/include/duckdb/storage/storage_manager.hpp\nindex de820437c395..e209ca95a7c4 100644\n--- a/src/include/duckdb/storage/storage_manager.hpp\n+++ b/src/include/duckdb/storage/storage_manager.hpp\n@@ -100,7 +100,8 @@ class StorageManager {\n \tvirtual bool AutomaticCheckpoint(idx_t estimated_wal_bytes) = 0;\n \tvirtual unique_ptr<StorageCommitState> GenStorageCommitState(WriteAheadLog &wal) = 0;\n \tvirtual bool IsCheckpointClean(MetaBlockPointer checkpoint_id) = 0;\n-\tvirtual void CreateCheckpoint(CheckpointOptions options = CheckpointOptions()) = 0;\n+\tvirtual void CreateCheckpoint(optional_ptr<ClientContext> client_context,\n+\t                              CheckpointOptions options = CheckpointOptions()) = 0;\n \tvirtual DatabaseSize GetDatabaseSize() = 0;\n \tvirtual vector<MetadataBlockInfo> GetMetadataInfo() = 0;\n \tvirtual shared_ptr<TableIOManager> GetTableIOManager(BoundCreateTableInfo *info) = 0;\n@@ -159,7 +160,7 @@ class SingleFileStorageManager : public StorageManager {\n \tbool AutomaticCheckpoint(idx_t estimated_wal_bytes) override;\n \tunique_ptr<StorageCommitState> GenStorageCommitState(WriteAheadLog &wal) override;\n \tbool IsCheckpointClean(MetaBlockPointer checkpoint_id) override;\n-\tvoid CreateCheckpoint(CheckpointOptions options) override;\n+\tvoid CreateCheckpoint(optional_ptr<ClientContext> client_context, CheckpointOptions options) override;\n \tDatabaseSize GetDatabaseSize() override;\n \tvector<MetadataBlockInfo> GetMetadataInfo() override;\n \tshared_ptr<TableIOManager> GetTableIOManager(BoundCreateTableInfo *info) override;\ndiff --git a/src/main/attached_database.cpp b/src/main/attached_database.cpp\nindex 5fb160e842f2..64c80df560e2 100644\n--- a/src/main/attached_database.cpp\n+++ b/src/main/attached_database.cpp\n@@ -247,7 +247,7 @@ void AttachedDatabase::Close() {\n \t\t\t}\n \t\t\tCheckpointOptions options;\n \t\t\toptions.wal_action = CheckpointWALAction::DELETE_WAL;\n-\t\t\tstorage->CreateCheckpoint(options);\n+\t\t\tstorage->CreateCheckpoint(nullptr, options);\n \t\t}\n \t} catch (...) { // NOLINT\n \t}\ndiff --git a/src/main/capi/duckdb-c.cpp b/src/main/capi/duckdb-c.cpp\nindex ad47d7448faa..f8718913a363 100644\n--- a/src/main/capi/duckdb-c.cpp\n+++ b/src/main/capi/duckdb-c.cpp\n@@ -35,7 +35,11 @@ duckdb_state duckdb_open_internal(DBInstanceCacheWrapper *cache, const char *pat\n \t\t}\n \n \t\tif (cache) {\n-\t\t\twrapper->database = cache->instance_cache->GetOrCreateInstance(path, *db_config, true);\n+\t\t\tduckdb::string path_str;\n+\t\t\tif (path) {\n+\t\t\t\tpath_str = path;\n+\t\t\t}\n+\t\t\twrapper->database = cache->instance_cache->GetOrCreateInstance(path_str, *db_config, true);\n \t\t} else {\n \t\t\twrapper->database = duckdb::make_shared_ptr<DuckDB>(path, db_config);\n \t\t}\ndiff --git a/src/optimizer/join_order/query_graph_manager.cpp b/src/optimizer/join_order/query_graph_manager.cpp\nindex 6850508a6997..9b3568245ab7 100644\n--- a/src/optimizer/join_order/query_graph_manager.cpp\n+++ b/src/optimizer/join_order/query_graph_manager.cpp\n@@ -265,7 +265,6 @@ GenerateJoinRelation QueryGraphManager::GenerateJoins(vector<unique_ptr<LogicalO\n \t\t\t\t\tbreak;\n \t\t\t\t}\n \t\t\t}\n-\n \t\t\tauto join = make_uniq<LogicalComparisonJoin>(chosen_filter->join_type);\n \t\t\t// Here we optimize build side probe side. Our build side is the right side\n \t\t\t// So the right plans should have lower cardinalities.\n@@ -288,8 +287,9 @@ GenerateJoinRelation QueryGraphManager::GenerateJoins(vector<unique_ptr<LogicalO\n \t\t\t\tbool invert = !JoinRelationSet::IsSubset(*left.set, *f->left_set);\n \t\t\t\t// If the left and right set are inverted AND it is a semi or anti join\n \t\t\t\t// swap left and right children back.\n+\n \t\t\t\tif (invert && (f->join_type == JoinType::SEMI || f->join_type == JoinType::ANTI)) {\n-\t\t\t\t\tstd::swap(left, right);\n+\t\t\t\t\tstd::swap(join->children[0], join->children[1]);\n \t\t\t\t\tinvert = false;\n \t\t\t\t}\n \ndiff --git a/src/parallel/CMakeLists.txt b/src/parallel/CMakeLists.txt\nindex 61e84816eec6..ee4d7f865324 100644\n--- a/src/parallel/CMakeLists.txt\n+++ b/src/parallel/CMakeLists.txt\n@@ -15,6 +15,7 @@ add_library_unity(\n   pipeline_initialize_event.cpp\n   pipeline_prepare_finish_event.cpp\n   task_executor.cpp\n+  task_notifier.cpp\n   task_scheduler.cpp\n   thread_context.cpp)\n set(ALL_OBJECT_FILES\ndiff --git a/src/parallel/executor_task.cpp b/src/parallel/executor_task.cpp\nindex ebd462f93dd2..9c0db7609143 100644\n--- a/src/parallel/executor_task.cpp\n+++ b/src/parallel/executor_task.cpp\n@@ -1,4 +1,5 @@\n-#include \"duckdb/parallel/task.hpp\"\n+#include \"duckdb/parallel/executor_task.hpp\"\n+#include \"duckdb/parallel/task_notifier.hpp\"\n #include \"duckdb/execution/executor.hpp\"\n #include \"duckdb/main/client_context.hpp\"\n #include \"duckdb/parallel/thread_context.hpp\"\n@@ -6,13 +7,13 @@\n namespace duckdb {\n \n ExecutorTask::ExecutorTask(Executor &executor_p, shared_ptr<Event> event_p)\n-    : executor(executor_p), event(std::move(event_p)) {\n+    : executor(executor_p), event(std::move(event_p)), context(executor_p.context) {\n \texecutor.RegisterTask();\n }\n \n-ExecutorTask::ExecutorTask(ClientContext &context, shared_ptr<Event> event_p, const PhysicalOperator &op_p)\n-    : executor(Executor::Get(context)), event(std::move(event_p)), op(&op_p) {\n-\tthread_context = make_uniq<ThreadContext>(context);\n+ExecutorTask::ExecutorTask(ClientContext &context_p, shared_ptr<Event> event_p, const PhysicalOperator &op_p)\n+    : executor(Executor::Get(context_p)), event(std::move(event_p)), op(&op_p), context(context_p) {\n+\tthread_context = make_uniq<ThreadContext>(context_p);\n \texecutor.RegisterTask();\n }\n \n@@ -38,6 +39,7 @@ TaskExecutionResult ExecutorTask::Execute(TaskExecutionMode mode) {\n \t\tif (thread_context) {\n \t\t\tTaskExecutionResult result;\n \t\t\tdo {\n+\t\t\t\tTaskNotifier task_notifier {context};\n \t\t\t\tthread_context->profiler.StartOperator(op);\n \t\t\t\t// to allow continuous profiling, always execute in small steps\n \t\t\t\tresult = ExecuteTask(TaskExecutionMode::PROCESS_PARTIAL);\n@@ -46,7 +48,9 @@ TaskExecutionResult ExecutorTask::Execute(TaskExecutionMode mode) {\n \t\t\t} while (mode == TaskExecutionMode::PROCESS_ALL && result == TaskExecutionResult::TASK_NOT_FINISHED);\n \t\t\treturn result;\n \t\t} else {\n-\t\t\treturn ExecuteTask(mode);\n+\t\t\tTaskNotifier task_notifier {context};\n+\t\t\tauto result = ExecuteTask(mode);\n+\t\t\treturn result;\n \t\t}\n \t} catch (std::exception &ex) {\n \t\texecutor.PushError(ErrorData(ex));\ndiff --git a/src/parallel/task_executor.cpp b/src/parallel/task_executor.cpp\nindex eef4d28ab142..10c4a9e9240d 100644\n--- a/src/parallel/task_executor.cpp\n+++ b/src/parallel/task_executor.cpp\n@@ -1,4 +1,5 @@\n #include \"duckdb/parallel/task_executor.hpp\"\n+#include \"duckdb/parallel/task_notifier.hpp\"\n #include \"duckdb/parallel/task_scheduler.hpp\"\n \n namespace duckdb {\n@@ -7,7 +8,8 @@ TaskExecutor::TaskExecutor(TaskScheduler &scheduler)\n     : scheduler(scheduler), token(scheduler.CreateProducer()), completed_tasks(0), total_tasks(0) {\n }\n \n-TaskExecutor::TaskExecutor(ClientContext &context) : TaskExecutor(TaskScheduler::GetScheduler(context)) {\n+TaskExecutor::TaskExecutor(ClientContext &context_p) : TaskExecutor(TaskScheduler::GetScheduler(context_p)) {\n+\tcontext = context_p;\n }\n \n TaskExecutor::~TaskExecutor() {\n@@ -69,6 +71,7 @@ TaskExecutionResult BaseExecutorTask::Execute(TaskExecutionMode mode) {\n \t\treturn TaskExecutionResult::TASK_FINISHED;\n \t}\n \ttry {\n+\t\tTaskNotifier task_notifier {executor.context};\n \t\tExecuteTask();\n \t\texecutor.FinishTask();\n \t\treturn TaskExecutionResult::TASK_FINISHED;\ndiff --git a/src/parallel/task_notifier.cpp b/src/parallel/task_notifier.cpp\nnew file mode 100644\nindex 000000000000..012afb548c41\n--- /dev/null\n+++ b/src/parallel/task_notifier.cpp\n@@ -0,0 +1,23 @@\n+#include \"duckdb/parallel/task_notifier.hpp\"\n+#include \"duckdb/main/client_context.hpp\"\n+#include \"duckdb/main/client_context_state.hpp\"\n+\n+namespace duckdb {\n+\n+TaskNotifier::TaskNotifier(optional_ptr<ClientContext> context_p) : context(context_p) {\n+\tif (context) {\n+\t\tfor (auto &state : context->registered_state->States()) {\n+\t\t\tstate->OnTaskStart(*context);\n+\t\t}\n+\t}\n+}\n+\n+TaskNotifier::~TaskNotifier() {\n+\tif (context) {\n+\t\tfor (auto &state : context->registered_state->States()) {\n+\t\t\tstate->OnTaskStop(*context);\n+\t\t}\n+\t}\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/parallel/task_scheduler.cpp b/src/parallel/task_scheduler.cpp\nindex fd6671a3e161..22bba36a08d4 100644\n--- a/src/parallel/task_scheduler.cpp\n+++ b/src/parallel/task_scheduler.cpp\n@@ -284,6 +284,39 @@ int32_t TaskScheduler::NumberOfThreads() {\n \treturn current_thread_count.load();\n }\n \n+idx_t TaskScheduler::GetNumberOfTasks() const {\n+#ifndef DUCKDB_NO_THREADS\n+\treturn queue->q.size_approx();\n+#else\n+\tidx_t task_count = 0;\n+\tfor (auto &producer : queue->q) {\n+\t\ttask_count += producer.second.size();\n+\t}\n+\treturn task_count;\n+#endif\n+}\n+\n+idx_t TaskScheduler::GetProducerCount() const {\n+#ifndef DUCKDB_NO_THREADS\n+\treturn queue->q.size_producers_approx();\n+#else\n+\treturn queue->q.size();\n+#endif\n+}\n+\n+idx_t TaskScheduler::GetTaskCountForProducer(ProducerToken &token) const {\n+#ifndef DUCKDB_NO_THREADS\n+\tlock_guard<mutex> producer_lock(token.producer_lock);\n+\treturn queue->q.size_producer_approx(token.token->queue_token);\n+#else\n+\tconst auto it = queue->q.find(std::ref(*token.token));\n+\tif (it == queue->q.end()) {\n+\t\treturn 0;\n+\t}\n+\treturn it->second.size();\n+#endif\n+}\n+\n void TaskScheduler::SetThreads(idx_t total_threads, idx_t external_threads) {\n \tif (total_threads == 0) {\n \t\tthrow SyntaxException(\"Number of threads must be positive!\");\ndiff --git a/src/storage/checkpoint/table_data_writer.cpp b/src/storage/checkpoint/table_data_writer.cpp\nindex 07bf9781ad07..d60ac7fa11a9 100644\n--- a/src/storage/checkpoint/table_data_writer.cpp\n+++ b/src/storage/checkpoint/table_data_writer.cpp\n@@ -10,7 +10,8 @@\n \n namespace duckdb {\n \n-TableDataWriter::TableDataWriter(TableCatalogEntry &table_p) : table(table_p.Cast<DuckTableEntry>()) {\n+TableDataWriter::TableDataWriter(TableCatalogEntry &table_p, optional_ptr<ClientContext> client_context_p)\n+    : table(table_p.Cast<DuckTableEntry>()), client_context(client_context_p) {\n \tD_ASSERT(table_p.IsDuckTable());\n }\n \n@@ -40,7 +41,8 @@ DatabaseInstance &TableDataWriter::GetDatabase() {\n \n SingleFileTableDataWriter::SingleFileTableDataWriter(SingleFileCheckpointWriter &checkpoint_manager,\n                                                      TableCatalogEntry &table, MetadataWriter &table_data_writer)\n-    : TableDataWriter(table), checkpoint_manager(checkpoint_manager), table_data_writer(table_data_writer) {\n+    : TableDataWriter(table, checkpoint_manager.GetClientContext()), checkpoint_manager(checkpoint_manager),\n+      table_data_writer(table_data_writer) {\n }\n \n unique_ptr<RowGroupWriter> SingleFileTableDataWriter::GetRowGroupWriter(RowGroup &row_group) {\ndiff --git a/src/storage/checkpoint_manager.cpp b/src/storage/checkpoint_manager.cpp\nindex ab2da7e51128..4dee9e0e8597 100644\n--- a/src/storage/checkpoint_manager.cpp\n+++ b/src/storage/checkpoint_manager.cpp\n@@ -36,10 +36,11 @@ namespace duckdb {\n \n void ReorderTableEntries(catalog_entry_vector_t &tables);\n \n-SingleFileCheckpointWriter::SingleFileCheckpointWriter(AttachedDatabase &db, BlockManager &block_manager,\n+SingleFileCheckpointWriter::SingleFileCheckpointWriter(optional_ptr<ClientContext> client_context_p,\n+                                                       AttachedDatabase &db, BlockManager &block_manager,\n                                                        CheckpointType checkpoint_type)\n-    : CheckpointWriter(db), partial_block_manager(block_manager, PartialBlockType::FULL_CHECKPOINT),\n-      checkpoint_type(checkpoint_type) {\n+    : CheckpointWriter(db), client_context(client_context_p),\n+      partial_block_manager(block_manager, PartialBlockType::FULL_CHECKPOINT), checkpoint_type(checkpoint_type) {\n }\n \n BlockManager &SingleFileCheckpointWriter::GetBlockManager() {\ndiff --git a/src/storage/storage_manager.cpp b/src/storage/storage_manager.cpp\nindex cb6c654e5fe2..a4109987c50b 100644\n--- a/src/storage/storage_manager.cpp\n+++ b/src/storage/storage_manager.cpp\n@@ -355,7 +355,7 @@ bool SingleFileStorageManager::IsCheckpointClean(MetaBlockPointer checkpoint_id)\n \treturn block_manager->IsRootBlock(checkpoint_id);\n }\n \n-void SingleFileStorageManager::CreateCheckpoint(CheckpointOptions options) {\n+void SingleFileStorageManager::CreateCheckpoint(optional_ptr<ClientContext> client_context, CheckpointOptions options) {\n \tif (InMemory() || read_only || !load_complete) {\n \t\treturn;\n \t}\n@@ -366,7 +366,7 @@ void SingleFileStorageManager::CreateCheckpoint(CheckpointOptions options) {\n \tif (GetWALSize() > 0 || config.options.force_checkpoint || options.action == CheckpointAction::ALWAYS_CHECKPOINT) {\n \t\t// we only need to checkpoint if there is anything in the WAL\n \t\ttry {\n-\t\t\tSingleFileCheckpointWriter checkpointer(db, *block_manager, options.type);\n+\t\t\tSingleFileCheckpointWriter checkpointer(client_context, db, *block_manager, options.type);\n \t\t\tcheckpointer.CreateCheckpoint();\n \t\t} catch (std::exception &ex) {\n \t\t\tErrorData error(ex);\ndiff --git a/src/transaction/duck_transaction_manager.cpp b/src/transaction/duck_transaction_manager.cpp\nindex 15248dabd776..68b2ff4e18b5 100644\n--- a/src/transaction/duck_transaction_manager.cpp\n+++ b/src/transaction/duck_transaction_manager.cpp\n@@ -194,7 +194,7 @@ void DuckTransactionManager::Checkpoint(ClientContext &context, bool force) {\n \t\t// we cannot do a full checkpoint if any transaction needs to read old data\n \t\toptions.type = CheckpointType::CONCURRENT_CHECKPOINT;\n \t}\n-\tstorage_manager.CreateCheckpoint(options);\n+\tstorage_manager.CreateCheckpoint(context, options);\n }\n \n unique_ptr<StorageLockKey> DuckTransactionManager::SharedCheckpointLock() {\n@@ -295,7 +295,7 @@ ErrorData DuckTransactionManager::CommitTransaction(ClientContext &context, Tran\n \t\toptions.action = CheckpointAction::ALWAYS_CHECKPOINT;\n \t\toptions.type = checkpoint_decision.type;\n \t\tauto &storage_manager = db.GetStorageManager();\n-\t\tstorage_manager.CreateCheckpoint(options);\n+\t\tstorage_manager.CreateCheckpoint(context, options);\n \t}\n \treturn error;\n }\ndiff --git a/third_party/concurrentqueue/concurrentqueue.h b/third_party/concurrentqueue/concurrentqueue.h\nindex 0f5ad0a4d625..b62b637bfb1d 100644\n--- a/third_party/concurrentqueue/concurrentqueue.h\n+++ b/third_party/concurrentqueue/concurrentqueue.h\n@@ -1254,6 +1254,23 @@ class ConcurrentQueue\n \t\treturn size;\n \t}\n \t\n+\n+\t// Returns the number of producers currently associated with the queue.\n+\tsize_t size_producers_approx() const\n+\t{\n+\t\tsize_t size = 0;\n+\t\tfor (auto ptr = producerListTail.load(std::memory_order_acquire); ptr != nullptr; ptr = ptr->next_prod()) {\n+\t\t\tsize += 1;\n+\t\t}\n+\t\treturn size;\n+\t}\n+\n+\t// Returns the number of elements currently in the queue for a specific producer.\n+\tsize_t size_producer_approx(producer_token_t const& producer) const\n+\t{\n+\t\treturn static_cast<ExplicitProducer*>(producer.producer)->size_approx();\n+\t}\n+\n \t\n \t// Returns true if the underlying atomic variables used by\n \t// the queue are lock-free (they should be on most platforms).\ndiff --git a/tools/pythonpkg/pyproject.toml b/tools/pythonpkg/pyproject.toml\nindex 076c0fa167ce..67acc3107f39 100644\n--- a/tools/pythonpkg/pyproject.toml\n+++ b/tools/pythonpkg/pyproject.toml\n@@ -15,7 +15,7 @@ local_scheme = \"no-local-version\"\n # Default config runs all tests and requires at least one extension to be tested against\n [tool.cibuildwheel]\n dependency-versions = \"latest\"\n-before-build = 'pip install oldest-supported-numpy'\n+before-build = 'pip install numpy>=2.0'\n before-test = 'python scripts/optional_requirements.py'\n test-requires = 'pytest'\n test-command = 'DUCKDB_PYTHON_TEST_EXTENSION_PATH={project} DUCKDB_PYTHON_TEST_EXTENSION_REQUIRED=1 python -m pytest {project}/tests --verbose'\ndiff --git a/tools/pythonpkg/scripts/cache_data.json b/tools/pythonpkg/scripts/cache_data.json\nindex 44408d142a75..685499fbf636 100644\n--- a/tools/pythonpkg/scripts/cache_data.json\n+++ b/tools/pythonpkg/scripts/cache_data.json\n@@ -48,10 +48,13 @@\n         \"name\": \"pandas\",\n         \"children\": [\n             \"pandas.DataFrame\",\n-            \"pandas.isnull\",\n-            \"pandas.ArrowDtype\",\n+            \"pandas.Categorical\",\n+            \"pandas.CategoricalDtype\",\n+            \"pandas.Series\",\n             \"pandas.NaT\",\n             \"pandas.NA\",\n+            \"pandas.isnull\",\n+            \"pandas.ArrowDtype\",\n             \"pandas.BooleanDtype\",\n             \"pandas.UInt8Dtype\",\n             \"pandas.UInt16Dtype\",\n@@ -170,8 +173,8 @@\n             \"datetime.date\",\n             \"datetime.time\",\n             \"datetime.timedelta\",\n-            \"datetime.timezone\",\n-            \"datetime.datetime\"\n+            \"datetime.datetime\",\n+            \"datetime.timezone\"\n         ]\n     },\n     \"datetime.date\": {\n@@ -671,5 +674,23 @@\n         \"full_path\": \"collections.abc.Mapping\",\n         \"name\": \"Mapping\",\n         \"children\": []\n+    },\n+    \"pandas.Categorical\": {\n+        \"type\": \"attribute\",\n+        \"full_path\": \"pandas.Categorical\",\n+        \"name\": \"Categorical\",\n+        \"children\": []\n+    },\n+    \"pandas.CategoricalDtype\": {\n+        \"type\": \"attribute\",\n+        \"full_path\": \"pandas.CategoricalDtype\",\n+        \"name\": \"CategoricalDtype\",\n+        \"children\": []\n+    },\n+    \"pandas.Series\": {\n+        \"type\": \"attribute\",\n+        \"full_path\": \"pandas.Series\",\n+        \"name\": \"Series\",\n+        \"children\": []\n     }\n }\n\\ No newline at end of file\ndiff --git a/tools/pythonpkg/scripts/generate_import_cache_json.py b/tools/pythonpkg/scripts/generate_import_cache_json.py\nindex 54426943d861..7a59e6b76093 100644\n--- a/tools/pythonpkg/scripts/generate_import_cache_json.py\n+++ b/tools/pythonpkg/scripts/generate_import_cache_json.py\n@@ -162,19 +162,26 @@ def to_json(self):\n     pass\n \n \n-def update_json(existing: dict, new: dict):\n-    for item in new:\n-        if item not in existing:\n-            continue\n-        object = new[item]\n-        if isinstance(object, dict):\n-            object.update(existing[item])\n-            update_json(object, existing[item])\n+def update_json(existing: dict, new: dict) -> dict:\n+    # Iterate over keys in the new dictionary.\n+    for key in new:\n+        new_value = new[key]\n+        old_value = existing[key] if key in existing else None\n+\n+        # If both values are dictionaries, update recursively.\n+        if isinstance(new_value, dict) and isinstance(old_value, dict):\n+            print(key)\n+            updated = update_json(old_value, new_value)\n+            existing[key] = updated\n+        else:\n+            # Otherwise, overwrite the existing value.\n+            existing[key] = new_value\n+    return existing\n \n \n # Merge the existing JSON data with the new data\n json_data = generator.to_json()\n-update_json(existing_json_data, json_data)\n+json_data = update_json(existing_json_data, json_data)\n \n # Save the merged JSON data back to the file\n with open(json_cache_path, \"w\") as file:\ndiff --git a/tools/pythonpkg/scripts/imports.py b/tools/pythonpkg/scripts/imports.py\nindex cfa45d65a13c..d23200daf7b2 100644\n--- a/tools/pythonpkg/scripts/imports.py\n+++ b/tools/pythonpkg/scripts/imports.py\n@@ -9,6 +9,9 @@\n import pandas\n \n pandas.DataFrame\n+pandas.Categorical\n+pandas.CategoricalDtype\n+pandas.Series\n pandas.NaT\n pandas.NA\n pandas.isnull\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/datetime_module.hpp b/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/datetime_module.hpp\nindex bda586be15b5..898262d537b5 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/datetime_module.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/datetime_module.hpp\n@@ -48,7 +48,7 @@ struct DatetimeCacheItem : public PythonImportCacheItem {\n public:\n \tDatetimeCacheItem()\n \t    : PythonImportCacheItem(\"datetime\"), date(this), time(\"time\", this), timedelta(\"timedelta\", this),\n-\t      timezone(\"timezone\", this), datetime(this) {\n+\t      datetime(this), timezone(\"timezone\", this) {\n \t}\n \t~DatetimeCacheItem() override {\n \t}\n@@ -56,8 +56,8 @@ struct DatetimeCacheItem : public PythonImportCacheItem {\n \tDatetimeDateCacheItem date;\n \tPythonImportCacheItem time;\n \tPythonImportCacheItem timedelta;\n-\tPythonImportCacheItem timezone;\n \tDatetimeDatetimeCacheItem datetime;\n+\tPythonImportCacheItem timezone;\n };\n \n } // namespace duckdb\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/pandas_module.hpp b/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/pandas_module.hpp\nindex dc13c129a2d9..117604221c25 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/pandas_module.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/pandas_module.hpp\n@@ -20,8 +20,9 @@ struct PandasCacheItem : public PythonImportCacheItem {\n \n public:\n \tPandasCacheItem()\n-\t    : PythonImportCacheItem(\"pandas\"), DataFrame(\"DataFrame\", this), isnull(\"isnull\", this),\n-\t      ArrowDtype(\"ArrowDtype\", this), NaT(\"NaT\", this), NA(\"NA\", this), BooleanDtype(\"BooleanDtype\", this),\n+\t    : PythonImportCacheItem(\"pandas\"), DataFrame(\"DataFrame\", this), Categorical(\"Categorical\", this),\n+\t      CategoricalDtype(\"CategoricalDtype\", this), Series(\"Series\", this), NaT(\"NaT\", this), NA(\"NA\", this),\n+\t      isnull(\"isnull\", this), ArrowDtype(\"ArrowDtype\", this), BooleanDtype(\"BooleanDtype\", this),\n \t      UInt8Dtype(\"UInt8Dtype\", this), UInt16Dtype(\"UInt16Dtype\", this), UInt32Dtype(\"UInt32Dtype\", this),\n \t      UInt64Dtype(\"UInt64Dtype\", this), Int8Dtype(\"Int8Dtype\", this), Int16Dtype(\"Int16Dtype\", this),\n \t      Int32Dtype(\"Int32Dtype\", this), Int64Dtype(\"Int64Dtype\", this), Float32Dtype(\"Float32Dtype\", this),\n@@ -31,10 +32,13 @@ struct PandasCacheItem : public PythonImportCacheItem {\n \t}\n \n \tPythonImportCacheItem DataFrame;\n-\tPythonImportCacheItem isnull;\n-\tPythonImportCacheItem ArrowDtype;\n+\tPythonImportCacheItem Categorical;\n+\tPythonImportCacheItem CategoricalDtype;\n+\tPythonImportCacheItem Series;\n \tPythonImportCacheItem NaT;\n \tPythonImportCacheItem NA;\n+\tPythonImportCacheItem isnull;\n+\tPythonImportCacheItem ArrowDtype;\n \tPythonImportCacheItem BooleanDtype;\n \tPythonImportCacheItem UInt8Dtype;\n \tPythonImportCacheItem UInt16Dtype;\ndiff --git a/tools/pythonpkg/src/map.cpp b/tools/pythonpkg/src/map.cpp\nindex cd724779a3d7..9864f2de3e36 100644\n--- a/tools/pythonpkg/src/map.cpp\n+++ b/tools/pythonpkg/src/map.cpp\n@@ -8,6 +8,7 @@\n #include \"duckdb_python/pybind11/dataframe.hpp\"\n #include \"duckdb_python/pytype.hpp\"\n #include \"duckdb_python/pybind11/dataframe.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n \n namespace duckdb {\n \n@@ -30,7 +31,10 @@ static py::object FunctionCall(NumpyResultConversion &conversion, const vector<s\n \tfor (idx_t col_idx = 0; col_idx < names.size(); col_idx++) {\n \t\tin_numpy_dict[names[col_idx].c_str()] = conversion.ToArray(col_idx);\n \t}\n-\tauto in_df = py::module::import(\"pandas\").attr(\"DataFrame\").attr(\"from_dict\")(in_numpy_dict);\n+\n+\tauto &import_cache = *DuckDBPyConnection::ImportCache();\n+\tauto pandas_df = import_cache.pandas.DataFrame();\n+\tauto in_df = pandas_df(in_numpy_dict);\n \tD_ASSERT(in_df.ptr());\n \n \tD_ASSERT(function);\ndiff --git a/tools/pythonpkg/src/numpy/numpy_bind.cpp b/tools/pythonpkg/src/numpy/numpy_bind.cpp\nindex 0ffef850312f..e2a4a83fae14 100644\n--- a/tools/pythonpkg/src/numpy/numpy_bind.cpp\n+++ b/tools/pythonpkg/src/numpy/numpy_bind.cpp\n@@ -4,6 +4,7 @@\n #include \"duckdb_python/pandas/column/pandas_numpy_column.hpp\"\n #include \"duckdb_python/pandas/pandas_bind.hpp\"\n #include \"duckdb_python/numpy/numpy_type.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n \n namespace duckdb {\n \ndiff --git a/tools/pythonpkg/src/pandas/analyzer.cpp b/tools/pythonpkg/src/pandas/analyzer.cpp\nindex 8c16e99becba..98fa25cc634c 100644\n--- a/tools/pythonpkg/src/pandas/analyzer.cpp\n+++ b/tools/pythonpkg/src/pandas/analyzer.cpp\n@@ -467,12 +467,11 @@ LogicalType PandasAnalyzer::InnerAnalyze(py::object column, bool &can_convert, i\n \tif (rows == 0) {\n \t\treturn LogicalType::SQLNULL;\n \t}\n+\tauto &import_cache = *DuckDBPyConnection::ImportCache();\n+\tauto pandas_series = import_cache.pandas.Series();\n \n \t// Keys are not guaranteed to start at 0 for Series, use the internal __array__ instead\n-\tauto pandas_module = py::module::import(\"pandas\");\n-\tauto pandas_series = pandas_module.attr(\"core\").attr(\"series\").attr(\"Series\");\n-\n-\tif (py::isinstance(column, pandas_series)) {\n+\tif (pandas_series && py::isinstance(column, pandas_series)) {\n \t\t// TODO: check if '_values' is more portable, and behaves the same as '__array__()'\n \t\tcolumn = column.attr(\"__array__\")();\n \t}\n@@ -503,6 +502,13 @@ bool PandasAnalyzer::Analyze(py::object column) {\n \tif (sample_size == 0) {\n \t\treturn false;\n \t}\n+\tauto &import_cache = *DuckDBPyConnection::ImportCache();\n+\tauto pandas = import_cache.pandas();\n+\tif (!pandas) {\n+\t\t//! Pandas is not installed, no need to analyze\n+\t\treturn false;\n+\t}\n+\n \tbool can_convert = true;\n \tidx_t increment = GetSampleIncrement(py::len(column));\n \tLogicalType type = InnerAnalyze(column, can_convert, increment);\ndiff --git a/tools/pythonpkg/src/pyresult.cpp b/tools/pythonpkg/src/pyresult.cpp\nindex 8444421dcf1f..aa84cee57d7f 100644\n--- a/tools/pythonpkg/src/pyresult.cpp\n+++ b/tools/pythonpkg/src/pyresult.cpp\n@@ -171,15 +171,21 @@ py::dict DuckDBPyResult::FetchNumpy() {\n \n void DuckDBPyResult::FillNumpy(py::dict &res, idx_t col_idx, NumpyResultConversion &conversion, const char *name) {\n \tif (result->types[col_idx].id() == LogicalTypeId::ENUM) {\n+\t\tauto &import_cache = *DuckDBPyConnection::ImportCache();\n+\t\tauto pandas_categorical = import_cache.pandas.Categorical();\n+\t\tauto categorical_dtype = import_cache.pandas.CategoricalDtype();\n+\t\tif (!pandas_categorical || !categorical_dtype) {\n+\t\t\tthrow InvalidInputException(\"'pandas' is required for this operation but it was not installed\");\n+\t\t}\n+\n \t\t// first we (might) need to create the categorical type\n \t\tif (categories_type.find(col_idx) == categories_type.end()) {\n \t\t\t// Equivalent to: pandas.CategoricalDtype(['a', 'b'], ordered=True)\n-\t\t\tcategories_type[col_idx] = py::module::import(\"pandas\").attr(\"CategoricalDtype\")(categories[col_idx], true);\n+\t\t\tcategories_type[col_idx] = categorical_dtype(categories[col_idx], true);\n \t\t}\n \t\t// Equivalent to: pandas.Categorical.from_codes(codes=[0, 1, 0, 1], dtype=dtype)\n-\t\tres[name] = py::module::import(\"pandas\")\n-\t\t                .attr(\"Categorical\")\n-\t\t                .attr(\"from_codes\")(conversion.ToArray(col_idx), py::arg(\"dtype\") = categories_type[col_idx]);\n+\t\tres[name] = pandas_categorical.attr(\"from_codes\")(conversion.ToArray(col_idx),\n+\t\t                                                  py::arg(\"dtype\") = categories_type[col_idx]);\n \t\tif (!conversion.ToPandas()) {\n \t\t\tres[name] = res[name].attr(\"to_numpy\")();\n \t\t}\n@@ -339,6 +345,9 @@ PandasDataFrame DuckDBPyResult::FrameFromNumpy(bool date_as_object, const py::ha\n \tD_ASSERT(py::gil_check());\n \tauto &import_cache = *DuckDBPyConnection::ImportCache();\n \tauto pandas = import_cache.pandas();\n+\tif (!pandas) {\n+\t\tthrow InvalidInputException(\"'pandas' is required for this operation but it was not installed\");\n+\t}\n \n \tpy::object items = o.attr(\"items\")();\n \tfor (const py::handle &item : items) {\n",
  "test_patch": "diff --git a/scripts/sqllogictest/result.py b/scripts/sqllogictest/result.py\nindex 1893389b1d1a..a03319493e26 100644\n--- a/scripts/sqllogictest/result.py\n+++ b/scripts/sqllogictest/result.py\n@@ -1092,6 +1092,15 @@ def check_require(self, statement: Require) -> RequireResult:\n                 return RequireResult.MISSING\n             return RequireResult.PRESENT\n \n+        allow_unsigned_extensions = connection.execute(\n+            \"select value::BOOLEAN from duckdb_settings() where name == 'allow_unsigned_extensions'\"\n+        ).fetchone()[0]\n+        if param == \"allow_unsigned_extensions\":\n+            if allow_unsigned_extensions == False:\n+                # If extension validation is turned on (that is allow_unsigned_extensions=False), skip test\n+                return RequireResult.MISSING\n+            return RequireResult.PRESENT\n+\n         excluded_from_autoloading = True\n         for ext in self.runner.AUTOLOADABLE_EXTENSIONS:\n             if ext == param:\ndiff --git a/test/api/capi/test_capi_instance_cache.cpp b/test/api/capi/test_capi_instance_cache.cpp\nindex 7d07225a3b18..dd3b40df17b8 100644\n--- a/test/api/capi/test_capi_instance_cache.cpp\n+++ b/test/api/capi/test_capi_instance_cache.cpp\n@@ -38,3 +38,30 @@ TEST_CASE(\"Test the database instance cache in the C API\", \"[api][.]\") {\n \n \tduckdb_destroy_instance_cache(&instance_cache);\n }\n+\n+TEST_CASE(\"Test the database instance cache in the C API with a null path\", \"[capi]\") {\n+\tauto instance_cache = duckdb_create_instance_cache();\n+\tduckdb_database db;\n+\tauto state = duckdb_get_or_create_from_cache(instance_cache, nullptr, &db, nullptr, nullptr);\n+\tREQUIRE(state == DuckDBSuccess);\n+\tduckdb_close(&db);\n+\tduckdb_destroy_instance_cache(&instance_cache);\n+}\n+\n+TEST_CASE(\"Test the database instance cache in the C API with an empty path\", \"[capi]\") {\n+\tauto instance_cache = duckdb_create_instance_cache();\n+\tduckdb_database db;\n+\tauto state = duckdb_get_or_create_from_cache(instance_cache, \"\", &db, nullptr, nullptr);\n+\tREQUIRE(state == DuckDBSuccess);\n+\tduckdb_close(&db);\n+\tduckdb_destroy_instance_cache(&instance_cache);\n+}\n+\n+TEST_CASE(\"Test the database instance cache in the C API with a memory path\", \"[capi]\") {\n+\tauto instance_cache = duckdb_create_instance_cache();\n+\tduckdb_database db;\n+\tauto state = duckdb_get_or_create_from_cache(instance_cache, \":memory:\", &db, nullptr, nullptr);\n+\tREQUIRE(state == DuckDBSuccess);\n+\tduckdb_close(&db);\n+\tduckdb_destroy_instance_cache(&instance_cache);\n+}\ndiff --git a/test/extension/load_extension.test b/test/extension/load_extension.test\nindex c40b06f929e8..420f80e6256c 100644\n--- a/test/extension/load_extension.test\n+++ b/test/extension/load_extension.test\n@@ -6,6 +6,8 @@ require notmingw\n \n require skip_reload\n \n+require allow_unsigned_extensions\n+\n statement ok\n PRAGMA enable_verification\n \ndiff --git a/test/extension/load_test_alias.test b/test/extension/load_test_alias.test\nindex 51b9ad54fcd0..ae885789fa85 100644\n--- a/test/extension/load_test_alias.test\n+++ b/test/extension/load_test_alias.test\n@@ -6,6 +6,8 @@ require skip_reload\n \n require notmingw\n \n+require allow_unsigned_extensions\n+\n statement ok\n PRAGMA enable_verification\n \ndiff --git a/test/extension/test_alias_point.test b/test/extension/test_alias_point.test\nindex 1b95a9360b43..75e02c81808e 100644\n--- a/test/extension/test_alias_point.test\n+++ b/test/extension/test_alias_point.test\n@@ -6,6 +6,8 @@ require skip_reload\n \n require notmingw\n \n+require allow_unsigned_extensions\n+\n statement ok\n PRAGMA enable_verification\n \ndiff --git a/test/extension/test_custom_type_modifier_cast.test b/test/extension/test_custom_type_modifier_cast.test\nindex 8a19071a84d0..1d4c886d8e14 100644\n--- a/test/extension/test_custom_type_modifier_cast.test\n+++ b/test/extension/test_custom_type_modifier_cast.test\n@@ -6,6 +6,8 @@ require skip_reload\n \n require notmingw\n \n+require allow_unsigned_extensions\n+\n statement ok\n PRAGMA enable_verification\n \ndiff --git a/test/extension/test_tags.test b/test/extension/test_tags.test\nindex f61e35acc4c2..a98f686142ef 100644\n--- a/test/extension/test_tags.test\n+++ b/test/extension/test_tags.test\n@@ -6,6 +6,8 @@ require skip_reload\n \n require notmingw\n \n+require allow_unsigned_extensions\n+\n statement ok\n PRAGMA enable_verification\n \ndiff --git a/test/optimizer/column_binding_error.test b/test/optimizer/column_binding_error.test\nnew file mode 100644\nindex 000000000000..534aa213f962\n--- /dev/null\n+++ b/test/optimizer/column_binding_error.test\n@@ -0,0 +1,28 @@\n+# name: test/optimizer/column_binding_error.test\n+# description: column binding error test inspired by #16426,\n+# group: [optimizer]\n+\n+require tpch\n+\n+statement ok\n+CREATE TABLE stats(num_docs) AS SELECT 1;\n+\n+statement ok\n+CREATE TABLE postings(docid, termid, tf) AS SELECT range, range, 1 FROM range(30);\n+\n+statement ok\n+CREATE TABLE docs(docid) AS FROM range(2);\n+\n+statement ok\n+WITH termids(termid) AS (SELECT 1)\n+SELECT\n+  (SELECT num_docs FROM stats),\n+  (SELECT num_docs FROM stats),\n+  (SELECT num_docs FROM stats),\n+  (SELECT num_docs FROM stats),\n+  (SELECT num_docs FROM stats),\n+  (SELECT num_docs FROM stats)\n+FROM postings\n+JOIN docs USING (docid)\n+JOIN termids USING (termid)\n+WHERE termid IN (SELECT termid FROM termids);\n\\ No newline at end of file\ndiff --git a/test/sql/copy/csv/test_copy.test b/test/sql/copy/csv/test_copy.test\nindex 8ae494956b5a..dee00e0f13d3 100644\n--- a/test/sql/copy/csv/test_copy.test\n+++ b/test/sql/copy/csv/test_copy.test\n@@ -121,11 +121,17 @@ COPY test4 (a,c) FROM '__TEST_DIR__/test4.csv' (SEP 1);\n ----\n \"sep\" expects a string argument!\n \n-# multiple format options\n+# multiple format options/1, last one wins\n statement error\n-COPY test4 (a,c) FROM '__TEST_DIR__/test4.csv' (FORMAT 'csv', FORMAT 'json');\n+COPY test4 (a,c) FROM '__TEST_DIR__/test4.csv' (FORMAT 'csv', FORMAT 'some_other_copy_function');\n ----\n-Copy Function with name \"json\" is not in the catalog, but it exists in the json extension.\n+Catalog Error: Copy Function with name some_other_copy_function does not exist\n+\n+# multiple format options/2, last one wins\n+query I\n+COPY test4 (a,c) FROM '__TEST_DIR__/test4.csv' (FORMAT 'some_other_copy_function', FORMAT 'csv');\n+----\n+5000\n \n # number as escape string\n statement error\ndiff --git a/test/sql/sample/bernoulli_sampling.test b/test/sql/sample/bernoulli_sampling.test\nindex 95b3e3796c8f..ce6cdf302e38 100644\n--- a/test/sql/sample/bernoulli_sampling.test\n+++ b/test/sql/sample/bernoulli_sampling.test\n@@ -2,6 +2,9 @@\n # description: Test reservoir sample crash on large data sets\n # group: [sample]\n \n+# seed does not persist across restarts\n+require skip_reload\n+\n statement ok\n create table output (num_rows INT);\n \ndiff --git a/test/sqlite/sqllogic_test_runner.cpp b/test/sqlite/sqllogic_test_runner.cpp\nindex 346dc72524db..18a1a5a03306 100644\n--- a/test/sqlite/sqllogic_test_runner.cpp\n+++ b/test/sqlite/sqllogic_test_runner.cpp\n@@ -524,6 +524,12 @@ RequireResult SQLLogicTestRunner::CheckRequire(SQLLogicParser &parser, const vec\n \t\t}\n \t\treturn RequireResult::PRESENT;\n \t}\n+\tif (param == \"allow_unsigned_extensions\") {\n+\t\tif (config->options.allow_unsigned_extensions) {\n+\t\t\treturn RequireResult::PRESENT;\n+\t\t}\n+\t\treturn RequireResult::MISSING;\n+\t}\n \n \tbool excluded_from_autoloading = true;\n \tfor (const auto &ext : AUTOLOADABLE_EXTENSIONS) {\ndiff --git a/tools/shell/tests/test_read_from_stdin.py b/tools/shell/tests/test_read_from_stdin.py\nindex 31f0629f7603..2cd570a44698 100644\n--- a/tools/shell/tests/test_read_from_stdin.py\n+++ b/tools/shell/tests/test_read_from_stdin.py\n@@ -179,6 +179,8 @@ def test_read_stdin_json_array(self, shell, json_extension):\n         ])\n \n     def test_read_stdin_json_auto_recursive_cte(self, shell, json_extension):\n+        # FIXME: disabled for now\n+        return\n         test = (\n             ShellTest(shell)\n             .input_file('data/json/filter_keystage.ndjson')\n",
  "problem_statement": "Python API: .to_df() segfaults when NumPy is installed but Pandas is missing\n### What happens?\n\nCalling `.to_df()` in the Python API causes a segmentation fault if NumPy is installed but Pandas is not. Ideally, the function should raise a clear error indicating that Pandas is required.\n\n### To Reproduce\n\n```\n% uv venv venv\nUsing CPython 3.13.2 interpreter at: /usr/bin/python3\nCreating virtual environment at: venv\nActivate with: source venv/bin/activate\n% . venv/bin/activate\n% uv pip install duckdb numpy\nUsing Python 3.13.2 environment at: venv\nResolved 2 packages in 24ms\nInstalled 2 packages in 16ms\n + duckdb==1.2.0\n + numpy==2.2.3\n% python3\nPython 3.13.2 (main, Feb  5 2025, 01:23:35) [GCC 14.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import duckdb\n>>> r = duckdb.read_csv('/etc/hostname')\n>>> r.to_df()\nzsh: segmentation fault  python3\n```\n\n```\n% python3\nPython 3.12.8 (main, Dec  3 2024, 18:42:41) [Clang 16.0.0 (clang-1600.0.26.4)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import duckdb\n>>> r = duckdb.read_csv('/etc/xtab')\n>>> r.to_df()\nzsh: segmentation fault  python3\n```\n\n### OS:\n\nDebian Linux testing aarch64, macOS Sonama 14.6.1 M3 Pro\n\n### DuckDB Version:\n\n1.2.0\n\n### DuckDB Client:\n\nPython\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nNODA Kai\n\n### Affiliation:\n\nIndependent\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [x] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [x] Yes, I have\n",
  "hints_text": "Hi @nodakai thanks for reporting this. Interestingly, it does not reproduce on macOS with a random dataframe I created. We'll take a closer look.\n\n<img width=\"1038\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/1e5c0009-9ff0-4e1a-834a-f99e51361da1\" />\nHmm??\n```\n% uv venv venv               \nUsing CPython 3.12.8 interpreter at: /opt/homebrew/opt/python@3.12/bin/python3.12\nCreating virtual environment at: venv\nActivate with: source venv/bin/activate\n% . venv/bin/activate\n% uv pip install duckdb numpy\nUsing Python 3.12.8 environment at: venv\nResolved 2 packages in 64ms\nInstalled 2 packages in 51ms\n + duckdb==1.2.0\n + numpy==2.2.3\n% python3\nPython 3.12.8 (main, Dec  3 2024, 18:42:41) [Clang 16.0.0 (clang-1600.0.26.4)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import duckdb\n>>> r = duckdb.sql('select 42 as x')\n>>> r.to_df()\nzsh: segmentation fault  python3\n```\n\nBy the way your transcript mentions two different CPython versions, 3.11.11 and 3.12.9. Not sure if that matters\nRight! I typed `python3` when I should have typed `python` :)",
  "created_at": "2025-03-04T19:56:38Z"
}