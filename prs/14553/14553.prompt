You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
[Python Arrow Scan] cannot join blob columns
### What happens?

when you join by binary column you get an error, duckdb v1.1.1.  works in v1.0.0


### To Reproduce

```
import duckdb
import pyarrow as pa
import hashlib
my_table = pa.Table.from_pydict({"foo": pa.array([hashlib.sha256("foo".encode()).digest()], type=pa.binary())})
my_table2 = pa.Table.from_pydict({"foo": pa.array([hashlib.sha256("foo".encode()).digest()], type=pa.binary()),"a": ["123"]})
duckdb.sql(f"""SELECT my_table2.* EXCLUDE (foo) FROM my_table LEFT JOIN my_table2 USING (foo)""").show()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
duckdb.duckdb.Error: UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb4 in position 2: invalid start byte
```

### OS:

mac

### DuckDB Version:

1..1.1

### DuckDB Client:

Python

### Hardware:

_No response_

### Full Name:

Jack Zhao

### Affiliation:

Delphina.ai

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://duckdb.org/docs/installation/) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/include/duckdb/planner/table_filter.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/table_filter.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/enums/filter_propagate_result.hpp"
13: #include "duckdb/common/mutex.hpp"
14: #include "duckdb/common/reference_map.hpp"
15: #include "duckdb/common/types.hpp"
16: #include "duckdb/common/unordered_map.hpp"
17: #include "duckdb/planner/column_binding.hpp"
18: 
19: namespace duckdb {
20: class BaseStatistics;
21: class Expression;
22: class PhysicalOperator;
23: class PhysicalTableScan;
24: 
25: enum class TableFilterType : uint8_t {
26: 	CONSTANT_COMPARISON = 0, // constant comparison (e.g. =C, >C, >=C, <C, <=C)
27: 	IS_NULL = 1,
28: 	IS_NOT_NULL = 2,
29: 	CONJUNCTION_OR = 3,
30: 	CONJUNCTION_AND = 4,
31: 	STRUCT_EXTRACT = 5
32: };
33: 
34: //! TableFilter represents a filter pushed down into the table scan.
35: class TableFilter {
36: public:
37: 	explicit TableFilter(TableFilterType filter_type_p) : filter_type(filter_type_p) {
38: 	}
39: 	virtual ~TableFilter() {
40: 	}
41: 
42: 	TableFilterType filter_type;
43: 
44: public:
45: 	//! Returns true if the statistics indicate that the segment can contain values that satisfy that filter
46: 	virtual FilterPropagateResult CheckStatistics(BaseStatistics &stats) = 0;
47: 	virtual string ToString(const string &column_name) = 0;
48: 	virtual unique_ptr<TableFilter> Copy() const = 0;
49: 	virtual bool Equals(const TableFilter &other) const {
50: 		return filter_type != other.filter_type;
51: 	}
52: 	virtual unique_ptr<Expression> ToExpression(const Expression &column) const = 0;
53: 
54: 	virtual void Serialize(Serializer &serializer) const;
55: 	static unique_ptr<TableFilter> Deserialize(Deserializer &deserializer);
56: 
57: public:
58: 	template <class TARGET>
59: 	TARGET &Cast() {
60: 		if (filter_type != TARGET::TYPE) {
61: 			throw InternalException("Failed to cast table to type - table filter type mismatch");
62: 		}
63: 		return reinterpret_cast<TARGET &>(*this);
64: 	}
65: 
66: 	template <class TARGET>
67: 	const TARGET &Cast() const {
68: 		if (filter_type != TARGET::TYPE) {
69: 			throw InternalException("Failed to cast table to type - table filter type mismatch");
70: 		}
71: 		return reinterpret_cast<const TARGET &>(*this);
72: 	}
73: };
74: 
75: class TableFilterSet {
76: public:
77: 	unordered_map<idx_t, unique_ptr<TableFilter>> filters;
78: 
79: public:
80: 	void PushFilter(idx_t column_index, unique_ptr<TableFilter> filter);
81: 
82: 	bool Equals(TableFilterSet &other) {
83: 		if (filters.size() != other.filters.size()) {
84: 			return false;
85: 		}
86: 		for (auto &entry : filters) {
87: 			auto other_entry = other.filters.find(entry.first);
88: 			if (other_entry == other.filters.end()) {
89: 				return false;
90: 			}
91: 			if (!entry.second->Equals(*other_entry->second)) {
92: 				return false;
93: 			}
94: 		}
95: 		return true;
96: 	}
97: 	static bool Equals(TableFilterSet *left, TableFilterSet *right) {
98: 		if (left == right) {
99: 			return true;
100: 		}
101: 		if (!left || !right) {
102: 			return false;
103: 		}
104: 		return left->Equals(*right);
105: 	}
106: 
107: 	void Serialize(Serializer &serializer) const;
108: 	static TableFilterSet Deserialize(Deserializer &deserializer);
109: };
110: 
111: class DynamicTableFilterSet {
112: public:
113: 	void ClearFilters(const PhysicalOperator &op);
114: 	void PushFilter(const PhysicalOperator &op, idx_t column_index, unique_ptr<TableFilter> filter);
115: 
116: 	bool HasFilters() const;
117: 	unique_ptr<TableFilterSet> GetFinalTableFilters(const PhysicalTableScan &scan,
118: 	                                                optional_ptr<TableFilterSet> existing_filters) const;
119: 
120: private:
121: 	mutable mutex lock;
122: 	reference_map_t<const PhysicalOperator, unique_ptr<TableFilterSet>> filters;
123: };
124: 
125: } // namespace duckdb
[end of src/include/duckdb/planner/table_filter.hpp]
[start of src/planner/table_filter.cpp]
1: #include "duckdb/planner/table_filter.hpp"
2: 
3: #include "duckdb/planner/filter/conjunction_filter.hpp"
4: #include "duckdb/planner/filter/constant_filter.hpp"
5: #include "duckdb/planner/filter/null_filter.hpp"
6: #include "duckdb/execution/operator/scan/physical_table_scan.hpp"
7: 
8: namespace duckdb {
9: 
10: void TableFilterSet::PushFilter(idx_t column_index, unique_ptr<TableFilter> filter) {
11: 	auto entry = filters.find(column_index);
12: 	if (entry == filters.end()) {
13: 		// no filter yet: push the filter directly
14: 		filters[column_index] = std::move(filter);
15: 	} else {
16: 		// there is already a filter: AND it together
17: 		if (entry->second->filter_type == TableFilterType::CONJUNCTION_AND) {
18: 			auto &and_filter = entry->second->Cast<ConjunctionAndFilter>();
19: 			and_filter.child_filters.push_back(std::move(filter));
20: 		} else {
21: 			auto and_filter = make_uniq<ConjunctionAndFilter>();
22: 			and_filter->child_filters.push_back(std::move(entry->second));
23: 			and_filter->child_filters.push_back(std::move(filter));
24: 			filters[column_index] = std::move(and_filter);
25: 		}
26: 	}
27: }
28: 
29: void DynamicTableFilterSet::ClearFilters(const PhysicalOperator &op) {
30: 	lock_guard<mutex> l(lock);
31: 	filters.erase(op);
32: }
33: 
34: void DynamicTableFilterSet::PushFilter(const PhysicalOperator &op, idx_t column_index, unique_ptr<TableFilter> filter) {
35: 	lock_guard<mutex> l(lock);
36: 	auto entry = filters.find(op);
37: 	optional_ptr<TableFilterSet> filter_ptr;
38: 	if (entry == filters.end()) {
39: 		auto filter_set = make_uniq<TableFilterSet>();
40: 		filter_ptr = filter_set.get();
41: 		filters[op] = std::move(filter_set);
42: 	} else {
43: 		filter_ptr = entry->second.get();
44: 	}
45: 	filter_ptr->PushFilter(column_index, std::move(filter));
46: }
47: 
48: bool DynamicTableFilterSet::HasFilters() const {
49: 	lock_guard<mutex> l(lock);
50: 	return !filters.empty();
51: }
52: 
53: unique_ptr<TableFilterSet>
54: DynamicTableFilterSet::GetFinalTableFilters(const PhysicalTableScan &scan,
55:                                             optional_ptr<TableFilterSet> existing_filters) const {
56: 	D_ASSERT(HasFilters());
57: 	auto result = make_uniq<TableFilterSet>();
58: 	if (existing_filters) {
59: 		for (auto &entry : existing_filters->filters) {
60: 			result->PushFilter(entry.first, entry.second->Copy());
61: 		}
62: 	}
63: 	for (auto &entry : filters) {
64: 		for (auto &filter : entry.second->filters) {
65: 			if (IsRowIdColumnId(scan.column_ids[filter.first])) {
66: 				// skip row id filters
67: 				continue;
68: 			}
69: 			result->PushFilter(filter.first, filter.second->Copy());
70: 		}
71: 	}
72: 	if (result->filters.empty()) {
73: 		return nullptr;
74: 	}
75: 	return result;
76: }
77: 
78: } // namespace duckdb
[end of src/planner/table_filter.cpp]
[start of tools/pythonpkg/src/arrow/arrow_array_stream.cpp]
1: #include "duckdb_python/arrow/arrow_array_stream.hpp"
2: 
3: #include "duckdb/common/assert.hpp"
4: #include "duckdb/common/common.hpp"
5: #include "duckdb/common/limits.hpp"
6: #include "duckdb/main/client_config.hpp"
7: #include "duckdb/planner/filter/conjunction_filter.hpp"
8: #include "duckdb/planner/filter/constant_filter.hpp"
9: #include "duckdb/planner/filter/struct_filter.hpp"
10: #include "duckdb/planner/table_filter.hpp"
11: 
12: #include "duckdb_python/pyconnection/pyconnection.hpp"
13: #include "duckdb_python/pyrelation.hpp"
14: #include "duckdb_python/pyresult.hpp"
15: #include "duckdb/function/table/arrow.hpp"
16: 
17: namespace duckdb {
18: 
19: void TransformDuckToArrowChunk(ArrowSchema &arrow_schema, ArrowArray &data, py::list &batches) {
20: 	py::gil_assert();
21: 	auto pyarrow_lib_module = py::module::import("pyarrow").attr("lib");
22: 	auto batch_import_func = pyarrow_lib_module.attr("RecordBatch").attr("_import_from_c");
23: 	batches.append(batch_import_func(reinterpret_cast<uint64_t>(&data), reinterpret_cast<uint64_t>(&arrow_schema)));
24: }
25: 
26: void VerifyArrowDatasetLoaded() {
27: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
28: 	if (!import_cache.pyarrow.dataset() || !ModuleIsLoaded<PyarrowDatasetCacheItem>()) {
29: 		throw InvalidInputException("Optional module 'pyarrow.dataset' is required to perform this action");
30: 	}
31: }
32: 
33: py::object PythonTableArrowArrayStreamFactory::ProduceScanner(py::object &arrow_scanner, py::handle &arrow_obj_handle,
34:                                                               ArrowStreamParameters &parameters,
35:                                                               const ClientProperties &client_properties) {
36: 	D_ASSERT(!py::isinstance<py::capsule>(arrow_obj_handle));
37: 	ArrowSchemaWrapper schema;
38: 	PythonTableArrowArrayStreamFactory::GetSchemaInternal(arrow_obj_handle, schema);
39: 	vector<string> unused_names;
40: 	vector<LogicalType> unused_types;
41: 	ArrowTableType arrow_table;
42: 	ArrowTableFunction::PopulateArrowTableType(arrow_table, schema, unused_names, unused_types);
43: 
44: 	auto filters = parameters.filters;
45: 	auto &column_list = parameters.projected_columns.columns;
46: 	auto &filter_to_col = parameters.projected_columns.filter_to_col;
47: 	bool has_filter = filters && !filters->filters.empty();
48: 	py::list projection_list = py::cast(column_list);
49: 	if (has_filter) {
50: 		auto filter = TransformFilter(*filters, parameters.projected_columns.projection_map, filter_to_col,
51: 		                              client_properties, arrow_table);
52: 		if (column_list.empty()) {
53: 			return arrow_scanner(arrow_obj_handle, py::arg("filter") = filter);
54: 		} else {
55: 			return arrow_scanner(arrow_obj_handle, py::arg("columns") = projection_list, py::arg("filter") = filter);
56: 		}
57: 	} else {
58: 		if (column_list.empty()) {
59: 			return arrow_scanner(arrow_obj_handle);
60: 		} else {
61: 			return arrow_scanner(arrow_obj_handle, py::arg("columns") = projection_list);
62: 		}
63: 	}
64: }
65: unique_ptr<ArrowArrayStreamWrapper> PythonTableArrowArrayStreamFactory::Produce(uintptr_t factory_ptr,
66:                                                                                 ArrowStreamParameters &parameters) {
67: 	py::gil_scoped_acquire acquire;
68: 	auto factory = static_cast<PythonTableArrowArrayStreamFactory *>(reinterpret_cast<void *>(factory_ptr)); // NOLINT
69: 	D_ASSERT(factory->arrow_object);
70: 	py::handle arrow_obj_handle(factory->arrow_object);
71: 	auto arrow_object_type = DuckDBPyConnection::GetArrowType(arrow_obj_handle);
72: 
73: 	if (arrow_object_type == PyArrowObjectType::PyCapsule) {
74: 		auto res = make_uniq<ArrowArrayStreamWrapper>();
75: 		auto capsule = py::reinterpret_borrow<py::capsule>(arrow_obj_handle);
76: 		auto stream = capsule.get_pointer<struct ArrowArrayStream>();
77: 		if (!stream->release) {
78: 			throw InternalException("ArrowArrayStream was released by another thread/library");
79: 		}
80: 		res->arrow_array_stream = *stream;
81: 		stream->release = nullptr;
82: 		return res;
83: 	}
84: 
85: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
86: 	py::object scanner;
87: 	py::object arrow_batch_scanner = import_cache.pyarrow.dataset.Scanner().attr("from_batches");
88: 	switch (arrow_object_type) {
89: 	case PyArrowObjectType::Table: {
90: 		auto arrow_dataset = import_cache.pyarrow.dataset().attr("dataset");
91: 		auto dataset = arrow_dataset(arrow_obj_handle);
92: 		py::object arrow_scanner = dataset.attr("__class__").attr("scanner");
93: 		scanner = ProduceScanner(arrow_scanner, dataset, parameters, factory->client_properties);
94: 		break;
95: 	}
96: 	case PyArrowObjectType::RecordBatchReader: {
97: 		scanner = ProduceScanner(arrow_batch_scanner, arrow_obj_handle, parameters, factory->client_properties);
98: 		break;
99: 	}
100: 	case PyArrowObjectType::Scanner: {
101: 		// If it's a scanner we have to turn it to a record batch reader, and then a scanner again since we can't stack
102: 		// scanners on arrow Otherwise pushed-down projections and filters will disappear like tears in the rain
103: 		auto record_batches = arrow_obj_handle.attr("to_reader")();
104: 		scanner = ProduceScanner(arrow_batch_scanner, record_batches, parameters, factory->client_properties);
105: 		break;
106: 	}
107: 	case PyArrowObjectType::Dataset: {
108: 		py::object arrow_scanner = arrow_obj_handle.attr("__class__").attr("scanner");
109: 		scanner = ProduceScanner(arrow_scanner, arrow_obj_handle, parameters, factory->client_properties);
110: 		break;
111: 	}
112: 	default: {
113: 		auto py_object_type = string(py::str(arrow_obj_handle.get_type().attr("__name__")));
114: 		throw InvalidInputException("Object of type '%s' is not a recognized Arrow object", py_object_type);
115: 	}
116: 	}
117: 
118: 	auto record_batches = scanner.attr("to_reader")();
119: 	auto res = make_uniq<ArrowArrayStreamWrapper>();
120: 	auto export_to_c = record_batches.attr("_export_to_c");
121: 	export_to_c(reinterpret_cast<uint64_t>(&res->arrow_array_stream));
122: 	return res;
123: }
124: 
125: void PythonTableArrowArrayStreamFactory::GetSchemaInternal(py::handle arrow_obj_handle, ArrowSchemaWrapper &schema) {
126: 	if (py::isinstance<py::capsule>(arrow_obj_handle)) {
127: 		auto capsule = py::reinterpret_borrow<py::capsule>(arrow_obj_handle);
128: 		auto stream = capsule.get_pointer<struct ArrowArrayStream>();
129: 		if (!stream->release) {
130: 			throw InternalException("ArrowArrayStream was released by another thread/library");
131: 		}
132: 		stream->get_schema(stream, &schema.arrow_schema);
133: 		return;
134: 	}
135: 
136: 	auto table_class = py::module::import("pyarrow").attr("Table");
137: 	if (py::isinstance(arrow_obj_handle, table_class)) {
138: 		auto obj_schema = arrow_obj_handle.attr("schema");
139: 		auto export_to_c = obj_schema.attr("_export_to_c");
140: 		export_to_c(reinterpret_cast<uint64_t>(&schema.arrow_schema));
141: 		return;
142: 	}
143: 
144: 	VerifyArrowDatasetLoaded();
145: 
146: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
147: 	auto scanner_class = import_cache.pyarrow.dataset.Scanner();
148: 
149: 	if (py::isinstance(arrow_obj_handle, scanner_class)) {
150: 		auto obj_schema = arrow_obj_handle.attr("projected_schema");
151: 		auto export_to_c = obj_schema.attr("_export_to_c");
152: 		export_to_c(reinterpret_cast<uint64_t>(&schema));
153: 	} else {
154: 		auto obj_schema = arrow_obj_handle.attr("schema");
155: 		auto export_to_c = obj_schema.attr("_export_to_c");
156: 		export_to_c(reinterpret_cast<uint64_t>(&schema));
157: 	}
158: }
159: 
160: void PythonTableArrowArrayStreamFactory::GetSchema(uintptr_t factory_ptr, ArrowSchemaWrapper &schema) {
161: 	py::gil_scoped_acquire acquire;
162: 	auto factory = static_cast<PythonTableArrowArrayStreamFactory *>(reinterpret_cast<void *>(factory_ptr)); // NOLINT
163: 	D_ASSERT(factory->arrow_object);
164: 	py::handle arrow_obj_handle(factory->arrow_object);
165: 	GetSchemaInternal(arrow_obj_handle, schema);
166: }
167: 
168: string ConvertTimestampUnit(ArrowDateTimeType unit) {
169: 	switch (unit) {
170: 	case ArrowDateTimeType::MICROSECONDS:
171: 		return "us";
172: 	case ArrowDateTimeType::MILLISECONDS:
173: 		return "ms";
174: 	case ArrowDateTimeType::NANOSECONDS:
175: 		return "ns";
176: 	case ArrowDateTimeType::SECONDS:
177: 		return "s";
178: 	default:
179: 		throw NotImplementedException("DatetimeType not recognized in ConvertTimestampUnit: %d", (int)unit);
180: 	}
181: }
182: 
183: int64_t ConvertTimestampTZValue(int64_t base_value, ArrowDateTimeType datetime_type) {
184: 	auto input = timestamp_t(base_value);
185: 	if (!Timestamp::IsFinite(input)) {
186: 		return base_value;
187: 	}
188: 
189: 	switch (datetime_type) {
190: 	case ArrowDateTimeType::MICROSECONDS:
191: 		return Timestamp::GetEpochMicroSeconds(input);
192: 	case ArrowDateTimeType::MILLISECONDS:
193: 		return Timestamp::GetEpochMs(input);
194: 	case ArrowDateTimeType::NANOSECONDS:
195: 		return Timestamp::GetEpochNanoSeconds(input);
196: 	case ArrowDateTimeType::SECONDS:
197: 		return Timestamp::GetEpochSeconds(input);
198: 	default:
199: 		throw NotImplementedException("DatetimeType not recognized in ConvertTimestampTZValue");
200: 	}
201: }
202: 
203: py::object GetScalar(Value &constant, const string &timezone_config, const ArrowType &type) {
204: 	py::object scalar = py::module_::import("pyarrow").attr("scalar");
205: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
206: 	py::object dataset_scalar = import_cache.pyarrow.dataset().attr("scalar");
207: 	py::object scalar_value;
208: 	switch (constant.type().id()) {
209: 	case LogicalTypeId::BOOLEAN:
210: 		return dataset_scalar(constant.GetValue<bool>());
211: 	case LogicalTypeId::TINYINT:
212: 		return dataset_scalar(constant.GetValue<int8_t>());
213: 	case LogicalTypeId::SMALLINT:
214: 		return dataset_scalar(constant.GetValue<int16_t>());
215: 	case LogicalTypeId::INTEGER:
216: 		return dataset_scalar(constant.GetValue<int32_t>());
217: 	case LogicalTypeId::BIGINT:
218: 		return dataset_scalar(constant.GetValue<int64_t>());
219: 	case LogicalTypeId::DATE: {
220: 		py::object date_type = py::module_::import("pyarrow").attr("date32");
221: 		return dataset_scalar(scalar(constant.GetValue<int32_t>(), date_type()));
222: 	}
223: 	case LogicalTypeId::TIME: {
224: 		py::object date_type = py::module_::import("pyarrow").attr("time64");
225: 		return dataset_scalar(scalar(constant.GetValue<int64_t>(), date_type("us")));
226: 	}
227: 	case LogicalTypeId::TIMESTAMP: {
228: 		py::object date_type = py::module_::import("pyarrow").attr("timestamp");
229: 		return dataset_scalar(scalar(constant.GetValue<int64_t>(), date_type("us")));
230: 	}
231: 	case LogicalTypeId::TIMESTAMP_MS: {
232: 		py::object date_type = py::module_::import("pyarrow").attr("timestamp");
233: 		return dataset_scalar(scalar(constant.GetValue<int64_t>(), date_type("ms")));
234: 	}
235: 	case LogicalTypeId::TIMESTAMP_NS: {
236: 		py::object date_type = py::module_::import("pyarrow").attr("timestamp");
237: 		return dataset_scalar(scalar(constant.GetValue<int64_t>(), date_type("ns")));
238: 	}
239: 	case LogicalTypeId::TIMESTAMP_SEC: {
240: 		py::object date_type = py::module_::import("pyarrow").attr("timestamp");
241: 		return dataset_scalar(scalar(constant.GetValue<int64_t>(), date_type("s")));
242: 	}
243: 	case LogicalTypeId::TIMESTAMP_TZ: {
244: 		auto &datetime_info = type.GetTypeInfo<ArrowDateTimeInfo>();
245: 		auto base_value = constant.GetValue<int64_t>();
246: 		auto arrow_datetime_type = datetime_info.GetDateTimeType();
247: 		auto time_unit_string = ConvertTimestampUnit(arrow_datetime_type);
248: 		auto converted_value = ConvertTimestampTZValue(base_value, arrow_datetime_type);
249: 		py::object date_type = py::module_::import("pyarrow").attr("timestamp");
250: 		return dataset_scalar(scalar(converted_value, date_type(time_unit_string, py::arg("tz") = timezone_config)));
251: 	}
252: 	case LogicalTypeId::UTINYINT: {
253: 		py::object integer_type = py::module_::import("pyarrow").attr("uint8");
254: 		return dataset_scalar(scalar(constant.GetValue<uint8_t>(), integer_type()));
255: 	}
256: 	case LogicalTypeId::USMALLINT: {
257: 		py::object integer_type = py::module_::import("pyarrow").attr("uint16");
258: 		return dataset_scalar(scalar(constant.GetValue<uint16_t>(), integer_type()));
259: 	}
260: 	case LogicalTypeId::UINTEGER: {
261: 		py::object integer_type = py::module_::import("pyarrow").attr("uint32");
262: 		return dataset_scalar(scalar(constant.GetValue<uint32_t>(), integer_type()));
263: 	}
264: 	case LogicalTypeId::UBIGINT: {
265: 		py::object integer_type = py::module_::import("pyarrow").attr("uint64");
266: 		return dataset_scalar(scalar(constant.GetValue<uint64_t>(), integer_type()));
267: 	}
268: 	case LogicalTypeId::FLOAT:
269: 		return dataset_scalar(constant.GetValue<float>());
270: 	case LogicalTypeId::DOUBLE:
271: 		return dataset_scalar(constant.GetValue<double>());
272: 	case LogicalTypeId::VARCHAR:
273: 		return dataset_scalar(constant.ToString());
274: 	case LogicalTypeId::BLOB:
275: 		return dataset_scalar(constant.GetValueUnsafe<string>());
276: 	case LogicalTypeId::DECIMAL: {
277: 		py::object date_type = py::module_::import("pyarrow").attr("decimal128");
278: 		uint8_t width;
279: 		uint8_t scale;
280: 		constant.type().GetDecimalProperties(width, scale);
281: 		switch (constant.type().InternalType()) {
282: 		case PhysicalType::INT16:
283: 			return dataset_scalar(scalar(constant.GetValue<int16_t>(), date_type(width, scale)));
284: 		case PhysicalType::INT32:
285: 			return dataset_scalar(scalar(constant.GetValue<int32_t>(), date_type(width, scale)));
286: 		case PhysicalType::INT64:
287: 			return dataset_scalar(scalar(constant.GetValue<int64_t>(), date_type(width, scale)));
288: 		default: {
289: 			auto hugeint_value = constant.GetValue<hugeint_t>();
290: 			auto hugeint_value_py = py::cast(hugeint_value.upper);
291: 			hugeint_value_py = hugeint_value_py.attr("__mul__")(NumericLimits<uint64_t>::Maximum());
292: 			hugeint_value_py = hugeint_value_py.attr("__add__")(hugeint_value.lower);
293: 			return dataset_scalar(scalar(hugeint_value_py, date_type(width, scale)));
294: 		}
295: 		}
296: 	}
297: 	default:
298: 		throw NotImplementedException("Unimplemented type \"%s\" for Arrow Filter Pushdown",
299: 		                              constant.type().ToString());
300: 	}
301: }
302: 
303: py::object TransformFilterRecursive(TableFilter *filter, vector<string> &column_ref, const string &timezone_config,
304:                                     const ArrowType &type) {
305: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
306: 	py::object field = import_cache.pyarrow.dataset().attr("field");
307: 	switch (filter->filter_type) {
308: 	case TableFilterType::CONSTANT_COMPARISON: {
309: 		auto &constant_filter = filter->Cast<ConstantFilter>();
310: 		auto constant_field = field(py::tuple(py::cast(column_ref)));
311: 		auto constant_value = GetScalar(constant_filter.constant, timezone_config, type);
312: 		switch (constant_filter.comparison_type) {
313: 		case ExpressionType::COMPARE_EQUAL: {
314: 			return constant_field.attr("__eq__")(constant_value);
315: 		}
316: 		case ExpressionType::COMPARE_LESSTHAN: {
317: 			return constant_field.attr("__lt__")(constant_value);
318: 		}
319: 		case ExpressionType::COMPARE_GREATERTHAN: {
320: 			return constant_field.attr("__gt__")(constant_value);
321: 		}
322: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO: {
323: 			return constant_field.attr("__le__")(constant_value);
324: 		}
325: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO: {
326: 			return constant_field.attr("__ge__")(constant_value);
327: 		}
328: 		default:
329: 			throw NotImplementedException("Comparison Type can't be an Arrow Scan Pushdown Filter");
330: 		}
331: 	}
332: 	//! We do not pushdown is null yet
333: 	case TableFilterType::IS_NULL: {
334: 		auto constant_field = field(py::tuple(py::cast(column_ref)));
335: 		return constant_field.attr("is_null")();
336: 	}
337: 	case TableFilterType::IS_NOT_NULL: {
338: 		auto constant_field = field(py::tuple(py::cast(column_ref)));
339: 		return constant_field.attr("is_valid")();
340: 	}
341: 	//! We do not pushdown or conjunctions yet
342: 	case TableFilterType::CONJUNCTION_OR: {
343: 		idx_t i = 0;
344: 		auto &or_filter = filter->Cast<ConjunctionOrFilter>();
345: 		//! Get first non null filter type
346: 		auto child_filter = or_filter.child_filters[i++].get();
347: 		py::object expression = TransformFilterRecursive(child_filter, column_ref, timezone_config, type);
348: 		while (i < or_filter.child_filters.size()) {
349: 			child_filter = or_filter.child_filters[i++].get();
350: 			py::object child_expression = TransformFilterRecursive(child_filter, column_ref, timezone_config, type);
351: 			expression = expression.attr("__or__")(child_expression);
352: 		}
353: 		return expression;
354: 	}
355: 	case TableFilterType::CONJUNCTION_AND: {
356: 		idx_t i = 0;
357: 		auto &and_filter = filter->Cast<ConjunctionAndFilter>();
358: 		auto child_filter = and_filter.child_filters[i++].get();
359: 		py::object expression = TransformFilterRecursive(child_filter, column_ref, timezone_config, type);
360: 		while (i < and_filter.child_filters.size()) {
361: 			child_filter = and_filter.child_filters[i++].get();
362: 			py::object child_expression = TransformFilterRecursive(child_filter, column_ref, timezone_config, type);
363: 			expression = expression.attr("__and__")(child_expression);
364: 		}
365: 		return expression;
366: 	}
367: 	case TableFilterType::STRUCT_EXTRACT: {
368: 		auto &struct_filter = filter->Cast<StructFilter>();
369: 		auto &child_type = StructType::GetChildType(type.GetDuckType(), struct_filter.child_idx);
370: 		auto &child_name = struct_filter.child_name;
371: 		auto &struct_type_info = type.GetTypeInfo<ArrowStructInfo>();
372: 		auto &struct_child_type = struct_type_info.GetChild(struct_filter.child_idx);
373: 
374: 		column_ref.push_back(child_name);
375: 		auto child_expr =
376: 		    TransformFilterRecursive(struct_filter.child_filter.get(), column_ref, timezone_config, struct_child_type);
377: 		column_ref.pop_back();
378: 
379: 		return child_expr;
380: 	}
381: 	default:
382: 		throw NotImplementedException("Pushdown Filter Type not supported in Arrow Scans");
383: 	}
384: }
385: 
386: py::object PythonTableArrowArrayStreamFactory::TransformFilter(TableFilterSet &filter_collection,
387:                                                                std::unordered_map<idx_t, string> &columns,
388:                                                                unordered_map<idx_t, idx_t> filter_to_col,
389:                                                                const ClientProperties &config,
390:                                                                const ArrowTableType &arrow_table) {
391: 	auto filters_map = &filter_collection.filters;
392: 	auto it = filters_map->begin();
393: 	D_ASSERT(columns.find(it->first) != columns.end());
394: 	auto arrow_type = &arrow_table.GetColumns().at(filter_to_col.at(it->first));
395: 
396: 	vector<string> column_ref;
397: 	column_ref.push_back(columns[it->first]);
398: 	py::object expression = TransformFilterRecursive(it->second.get(), column_ref, config.time_zone, **arrow_type);
399: 	while (it != filters_map->end()) {
400: 		arrow_type = &arrow_table.GetColumns().at(filter_to_col.at(it->first));
401: 		column_ref.clear();
402: 		column_ref.push_back(columns[it->first]);
403: 		py::object child_expression =
404: 		    TransformFilterRecursive(it->second.get(), column_ref, config.time_zone, **arrow_type);
405: 		expression = expression.attr("__and__")(child_expression);
406: 		it++;
407: 	}
408: 	return expression;
409: }
410: 
411: } // namespace duckdb
[end of tools/pythonpkg/src/arrow/arrow_array_stream.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: